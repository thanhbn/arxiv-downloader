# 2110.15343.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2110.15343.pdf
# File size: 3277983 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Scatterbrain: Unifying Sparse and Low-rank Attention
Approximation
Beidi Chen‚àóy, Tri Dao‚àóy, Eric Winsory, Zhao Songx, Atri Rudraz, and Christopher R√©y
yDepartment of Computer Science, Stanford University
xAdobe Research
zDepartment of Computer Science and Engineering, University at BuÔ¨Äalo, SUNY
{beidic,trid,winsor}@stanford.edu ,zsong@adobe.com ,atri@buffalo.edu ,
chrismre@cs.stanford.edu
October 29, 2021
Abstract
Recent advances in eÔ¨Écient Transformers have exploited either the sparsity or low-rank properties
of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences.
However, it is still challenging to balance the trade-oÔ¨Ä between model quality and eÔ¨Éciency to perform a
one-size-Ô¨Åts-all approximation for diÔ¨Äerent tasks. To better understand this trade-oÔ¨Ä, we observe that
sparse and low-rank approximations excel in diÔ¨Äerent regimes, determined by the softmax temperature in
attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA
algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse
(via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and eÔ¨Écient
approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain
can achieve 2:1lower error than baselines when serving as a drop-in replacement in BigGAN image
generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without Ô¨Åne-tuning,
Scatterbrain can reduce 98%of attention memory at the cost of only 1%drop in accuracy. We demonstrate
Scatterbrain for end-to-end training with up to 4points better perplexity and 5 points better average
accuracy than sparse or low-rank eÔ¨Écient transformers on language modeling and long-range-arena tasks.
1 Introduction
Transformer models [ 63] have been adapted in a wide variety of applications, including natural language
processing [ 7,26,50], image processing [ 10,47], and speech recognition [ 42]. Training large Transformers
requires extensive computational and memory resources, especially when modeling long sequences, mainly
due to the quadratic complexity (w.r.t. sequence length) in attention layers. Recent advances in eÔ¨Écient
transformers [ 17,22,35,36,65] leverage attention approximation to overcome the bottleneck by approximating
the attention matrices. However, it is challenging to Ô¨Ånd a robust approximation method that balances the
eÔ¨Éciency-accuracy trade-oÔ¨Ä on a wide variety of tasks [57, 58].
We categorize most of the existing approaches for eÔ¨Écient attention matrix computation into two major
groups: exploiting either the sparsity, e.g., Reformer [ 36], SMYRF [ 22], or low-rank properties of the attention
matrices, e.g., Linformer [ 65], Linear Transformer [ 35], and Performer [ 17]. However, these techniques usually
have diÔ¨Äerent strengths and focus on the performance of speciÔ¨Åc tasks, so their approximations still cause
accuracy degradation on many other tasks. For instance, according to a recent benchmark paper [ 57] and our
experiments, low-rank-based attention might be less eÔ¨Äective on hierarchically structured data or language
modeling tasks, while sparse-based variants do not perform well on classiÔ¨Åcation tasks.
‚àóEqual contribution. Order determined by coin Ô¨Çip.
1arXiv:2110.15343v1  [cs.LG]  28 Oct 2021

--- PAGE 2 ---
85%95%
Kernel & Hash Construction
  , ,CA TEGORIZA TION Input SP ARSE
LOWRANKSCA TTERBRAINFigure 1: Left: regimes that sparse+low-rank approximation is more accurate, based on the entropy of the
attention matrices. Right: Scatterbrain WorkÔ¨Çow. For the attention layer in Transformers, after computing
QueryQ, KeyK, and Value Vmatrices, we approximate softmax (QK>)Vwith two components: (i) sparse
SV(ii) low-rank (Q)((K)>V).
We observe that sparse and low-rank approximations are complementary for many attention matrices in
practice, and sparse+low-rank could outperform each individually (Figure 1 left). We empirically categorize
the regimes in which sparse or low-rank approximation achieves better error based on the softmax temperature
of attention (of which the entropy of softmax distribution can be used as a proxy). We expect that sparse
methods perform well if the attention depends on a few entries (low entropy softmax). In contrast, low-rank
methods do better if the attention depends on a mixture of many components (high entropy softmax). This
explains the phenomenon that current sparse and low-rank-based approaches excel on diÔ¨Äerent kinds of
tasks. A natural question is whether one could understand and unify the strength of both approaches. While
it is NP-hard to Ô¨Ånd the optimal combination of sparse and low-rank approximations, Robust PCA [ 9] is
a polynomial-time solution with tight approximation error. We observe that Robust PCA achieves lower
approximation error than sparse or low-rank alone on attention matrices. The diÔ¨Äerence is most pronounced
for ‚Äúmid-range‚Äù entropy, where we observe that up to 95%error reduction is possible.
The connection between Robust PCA and attention matrix estimation provides an opportunity to realize a
more robust approximation. SpeciÔ¨Åcally, given an attention matrix, one could adaptively perform sparse+low-
rank approximation to obtain a low error. However, it comes with three challenges: (i) How to decompose
the attention matrices into sparse and low-rank components and estimate them eÔ¨Éciently and accurately;
Robust PCA is accurate but slow and requires materializing the full attention, while straightforward addition
of sparse and low-rank attention will be inaccurate due to double counting. (ii) It is not clear if there is a
theoretical guarantee that sparse + low-rank approximation is strictly better than sparse or low-rank in some
regimes, though we observe the separation empirically. (iii) How does the lower approximation error transfer
to end-to-end performance in real tasks.
In this paper, we propose Scatterbrain, an accurate and eÔ¨Écient robust estimation of attention matrices
with theoretical guarantees to address the above challenges. SpeciÔ¨Åcally:
‚Ä¢In Section 3, we observe that sparse and low-rank approximation are complementary and demonstrate
that sparse + low-rank structure arises naturally when elements in the input sequence form clusters. We
theoretically characterize and analyze the regimes where sparse, low-rank, and sparse+low-rank excel,
dictated by the softmax temperature of attention.
‚Ä¢In Section 4, inspired by the classical Robust PCA algorithm, we propose Scatterbrain, which eÔ¨Éciently
combines sparse and low-rank matrices to approximate attention. In particular, we use Locality Sensitive
Hashing (LSH) to identify large entries of the attention matrix (after softmax) without materializing the
full matrix and then leverage kernel approximation to parameterize the low-rank part. We prove that our
method has a strictly lower approximation error than the low-rank baseline.
‚Ä¢In Section 5, we empirically validate our theory and the proposed method, showing that Scatterbrain
accurately approximates the attention matrix, is memory eÔ¨Écient for long sequences, and works well across
diÔ¨Äerent tasks. First, we show that its approximation accuracy is close to our oracle Robust PCA and
2

--- PAGE 3 ---
achieves 2.1lower error compared to other eÔ¨Écient baselines on real benchmarks. This leads to a direct
application of Scatterbrain as a drop-in replacement to pre-trained full attention, thus reducing up to 98%
of the memory required for attention computations in pre-trained T2T-ViT and BigGAN while maintaining
similar quality. Last we show that its superior accuracy and eÔ¨Éciency can improve the eÔ¨Éciency-accuracy
trade-oÔ¨Äs of Transformer end-to-end training. On the WikiText-103 language modeling task, Scatterbrain
achieves up to 1 point better perplexity compared to Reformer and Performer. On 5 benchmark long-range
tasks, Scatterbrain improves the average accuracy by up to 5 points.1
2 Problem Setting and Related Work
We Ô¨Årst deÔ¨Åne the approximation problem we aim to solve in this paper. Then we discuss the applications of
sparse and low-rank techniques in eÔ¨Écient Transformers and introduce robust PCA algorithm.
Problem Formulation: In the attention matrix approximation problem, we are given three matrices,
query, key, and value, Q;K;V2Rndto compute softmax (QK>)V. We seek to reduce the quadratic
complexity of softmax (QK>)(applied row-wise) with low approximation error. More precisely, for an approx-
imation procedure f, we minimize two objectives, the approximation error E
kf(Q;K ) softmax(QK>)k2
F
,
and the computation/memory cost C(f()).
Sparse, Low-rank Approximation for Attention Matrices: Recent work exploits the sparsity
patterns or Ô¨Ånds a low-rank mapping of the original attention matrices to overcome the computational and
memory bottlenecks in Transformers [ 17,22,35,36,53,65]. Generally, we can divide most of the techniques
into two categories ‚Äì sparse and low-rank approximations. Reformer [ 36] is a representative sparse variant
that uses LSH [ 3] to retrieve or detect the locations of the attention matrices with large values and reduce
the computation from O(n2)toO(nlogn). Performer [ 17] is an example of the low-rank variant, which
uses kernelization to avoid explicit O(n2d)computation. One problem of either the sparse or low-rank
approximation is that the structure of the attention matrices varies in practice, and it is challenging to
perform robust approximation on a wide range of attention matrices. For example, Wang et al. [65]observes
that attentions tend to have more low-rank structures in lower layers and Ramsauer et al. [51]shows that
they are sparser in the later stage of the training. Ideally, we want to unify the strength of both techniques,
but it is NP-hard to Ô¨Ånd the best combination of sparse and low-rank approximation.
Sparse + Low-rank and Robust PCA: Fortunately, classical Robust PCA [ 9] presents a polynomial
algorithm to Ô¨Ånd the approximately optimal or good combinations of sparse and low-rank approximation of
the matrices. The sparse + low-rank matrix structure has been well studied in statistics and signal processing
since the late 2000s [ 9]. This structure naturally generalizes low-rank [ 33,62], and sparse [ 60] matrices.
Scatterbrain is built on a line of work, e.g., Bigbird [ 70], Longformer [ 5] with the theme of combining multiple
types of attention. However, despite the multitude of papers, this sparse + low-rank matrix approximation
has not been rigorously studied in the context of attention matrices. We undertake this study and show how
we can relax the sparse + low-rank approximation from robust PCA, making it eÔ¨Écient while still retaining
PCA‚Äôs accuracy. In fact, our results shed further light on why Bigbird or Longformer work, as they are
special cases of a single principled structure. An extended discussion of related work is in Appendix A.
3 Characterization of Sparse + Low-rank Approx. to Attention
Matrices
We motivate the use of sparse + low-rank approximation of the attention matrices with the key observation
that for many attention matrices, sparse and low-rank approximation are complementary, and their ideal
combination (via Robust PCA) can outperform both (Section 3.1). Furthermore, we argue that the sparse +
low-rank structure can arise naturally when elements in the input sequence form clusters, as dictated by the
softmax temperature (Section 3.2).
1Scatterbrain code is available at https://github.com/HazyResearch/scatterbrain
3

--- PAGE 4 ---
3.1 Motivating Observations: Low-rank and Sparse Structures of Attention
Matrices
We empirically characterize regimes where sparse and low-rank approximation are well-suited, based on the
softmax temperature (for which we use the softmax distribution entropy is a proxy). SpeciÔ¨Åcally, in Fig. 1
(left), we present the approximation error of the original attention matrices and the approximation (sparse or
low-rank) of matrices sampled from a 4-layer Transformer trained on IMDb reviews classiÔ¨Åcation [ 57]. We
make two observations:
1.Sparse and low-rank approximation are complementary: sparse excels when the softmax temperature scale
is low (i.e., low entropy), and low-rank excels when the softmax temperature is high (i.e., high entropy).
2.An ideal combination of sparse and low-rank (orange line in Fig. 1 left), obtained with robust PCA, can
achieve lower error than both.
Similar observations on other benchmarks and details are presented in Appendix B.
3.2 A Generative Model of How Sparse + Low-rank Structure Can Arise
Figure 2: Visualization of the generative process, for
three diÔ¨Äerent values of the intra-cluster distance 
(small, medium, and large). The vectors from the input
sequence (rows of Q) form clusters that lie approxi-
mately on the unit sphere. DiÔ¨Äerent colors represent
diÔ¨Äerent clusters.Sparse + low-rank parameterization is more expres-
sive than either sparse or low-rank alone. Indeed,
in the Appendix, we construct a family of attention
matrices to show the separation between the approx-
imation capability of sparse + low-rank vs. sparse or
low-rank alone: for an nnattention matrix, sparse
or low-rank alone requires a O(n2)parameters to
getapproximation error in Frobenius norm, while
sparse + low-rank only requires O(n)parameters.
Moreover, weargueherethatsparse+low-rankis
a natural candidate to approximate generic attention
matrices. We describe a generative model of how the
sparse + low-rank structure in attention matrices
could arise when the elements of the input sequence
form clusters. Under this process, we characterize how the softmax temperature dictates when we would need
sparse, low-rank, or sparse + low-rank matrices to approximate the attention matrix. This result corroborates
the observation in Section 3.1.
Generative process of clustered elements in input sequence We describe here a generative model
of an input sequence to attention, parameterized by the inverse temperature 2Rand the intra-cluster
distance 2R.
Process 1. LetQ2Rnd, whered
(log3=2(n)), with every row of Qgenerated randomly as follows:
1. ForC= 
(n), sampleCnumber of cluster centers c1;:::;cC2Rdindependently from N(0;Id=p
d).
2.For each cluster around ci, sampleni=O(1)number of elements around ci, of the form zij=ci+rijfor
j= 1;:::;niwhererijN(0;Id=p
d). Assume that the total number of elements is n=n1++nC
andO(1=log1=4n).
LetQbe the matrix whose rows are the vectors zijwherei= 1;:::;Candj= 1;:::;ni. LetA=QQ>and
let the attention matrix be M= exp(A).
We visualize this generative process in Fig. 2.
Softmax temperature and approx. error We characterize when to use sparse, low-rank, or sparse +
low-rank to approximate the attention matrices in Process 1, depending on the inverse temperature . The
intuition here is that the inverse temperature corresponds to the strength of interaction between the clusters.
Ifis large, intra-cluster interaction dominates the attention matrix, the softmax distribution is peaked, and
so we only need a sparse matrix to approximate the attention. If is small, then the inter-cluster attention
is similar to intra-cluster attention, the softmax distribution is diÔ¨Äuse, and we can approximate it with a
4

--- PAGE 5 ---
low-rank matrix. In the middle regime of , we need the sparse part to cover the intra-cluster attention and
the low-rank part to approximate the inter-cluster attention.
We formalize this intuition in Theorem 1 (in bounds below we think of as a constant). All the proofs
are in Appendix D.
Theorem 1. LetM, be the attention matrix in Process 1. Fix 2(0;1). LetR2Rnnbe a matrix.
Consider low-rank, sparse, and sparse + low-rank approximations to M.
1.High temperature : Assume=o(logn=logd).
(a)Low-rank : There exists Rwithno(1)rank (and hence n1+o(1)parameters) such that kM RkFn.
(b)Sparse: IfRhas sparsity o(n2), thenkM RkF
(n).
2.Mid temperature : Assume (1 2) lognO(logn).
(a)Sparse + low-rank : There exists a sparse + low-rank Rwithn1+o(1)parameters withkM RkF
n.
(b)Low-rank : IfRis such that n rank(R) = 
(n), thenkM RkF
(n).
(c)Sparse: IfRhas sparsity o(n2), thenkM RkF
(n).
3.Low temperature : Assume= 
(logn).
(a)Low-rank : IfRis such that n rank(R) = 
(n), thenkM RkF
(e(1 2)).
(b)Sparse: There exists Rwith sparsity O(n)such thatkM RkFe(1 2)
4 Scatterbrain: Unifying Sparse and Low-rank Attention
Accuracy
Ef ficiency
Low HighHigh Scatterbrain
(sparse+low-rank)
Performer
(low-rank)
Reformer
(sparse)Robust- PCA
Figure 3: Qualitative comparison of approx.
accuracy and eÔ¨Éciency, among Robust PCA,
sparse (Reformer) and low-rank (Performer)
attention, and Scatterbrain. Scatterbrain is
more accurate while being eÔ¨Écient.We present Scatterbrain, and show that it approximates at-
tention accurately and eÔ¨Éciently. Section 4.1 describes the
challenges of designing an accurate and eÔ¨Écient approximation,
and how obvious baselines such as Robust PCA or a simple
combination of sparse attention and low-rank attention fail to
meet both criteria. Section 4.2 demonstrates how Scatterbrain
address the challenges (Fig. 1 contains a schematic of Scatter-
brain). In Section 4.3, we show that Scatterbrain is unbiased
with provably lower variance than low-rank baselines such as
Performer.
Fig. 3 shows a qualitative comparison between diÔ¨Äerent
methods of approximating the attention matrix: Robust PCA
is accurate but slow, sparse (e.g., Reformer), and low-rank (e.g.,
Performer) attention are fast and memory-eÔ¨Écient but may
not be very accurate, while Scatterbrain is more accurate than
its sparse and low-rank counterparts while remaining just as
eÔ¨Écient.
More details about the eÔ¨Écient implementation of Scatter-
brain are in Appendix C.
4.1 Challenges of Designing an Accurate and
EÔ¨Écient Sparse + Low-rank Approximation
We seek a sparse + low-rank approximation of the attention matrix2Athat isbothaccurate and eÔ¨Écient.
The natural theoretical baseline of Robust PCA is too slow and requires too much memory, while the most
straightforward way of combining sparse attention and low-rank attention fails due to double counting on the
support of the sparse attention.
2For simplicity of discussion, we consider the unnormalized attention matrix A=exp(QK>), omitting the usual scaling ofp
dand the softmax normalization constant.
5

--- PAGE 6 ---
1.If the goal is accuracy, Robust PCA is the most studied algorithm to Ô¨Ånd a sparse + low-rank approximation
to a given matrix. It relaxes the NP-hard problem of Ô¨Ånding the best sparse + low-rank approximation
into a convex optimization problem, with the nuclear norm and `1constraints. Even though it can be
solved in polynomial time, it is orders of magnitude too slow to be used in each iteration of a training
loop. Moreover, it requires materializing the attention matrix, which defeats the main purpose of reducing
compute and memory requirements.
2.On the other hand, one eÔ¨Écient way to get sparse + low-rank approximation of an attention matrix is to
simply add the entries of a sparse approximation S(say, from Reformer) and a low-rank approximation
eQeK>foreQ;eK2Rnm(say, from Performer). The sparse matrix Stypically has support determined
randomly [ 16], by LSH [ 22,36], or by clustering [ 53]. On the support of S, which likely includes the
locations of the large entries of the attention matrix A, the entries of Smatch those of A. One can multiply
(S+eQeK>)V=SV+eQ(eK>V)eÔ¨Éciently because Sis sparse, and grouping eQ(eK>V)reduces the matrix
multiplication complexity when mn, fromO(n2m)toO(nmd). The approximation S+eQeK>matches
eQeK>outside supp(S), hence it could be accurate there if eQeK>is accurate. However, S+eQeK>will
not be accurate on the support of Sdue to the contributions from both Sand fromeQeK>. Adjusting
eQeK>to discount the contribution from Sis diÔ¨Écult, especially if we want to avoid materializing eQeK>
for eÔ¨Éciency.
4.2 Scatterbrain: Algorithm Intuition and Description
The simple insight behind our method is that on the support of the sparse matrix S, instead of trying to
match the entries of the attention matrix A, we can set the entries of Sto discount the contribution from the
low-rank part eQeK>. This way, the approximation S+eQeK>will match Aexactly on the support of S, and
will matcheQeK>outside supp(S), which means it will still be accurate there if eQeK>is accurate. We do not
need to materialize the full matrix eQeK>as need a subset of its entries is required, hence our approximation
will be compute and memory eÔ¨Écient.
Scatterbrain thus proceeds in three steps: we construct a low-rank approximation eQeK>A, and construct
a sparse matrix Ssuch thatS+eQeK>matchesAon the support of S, then Ô¨Ånally multiply SVandeQ(eK>V)
and combine the result. More speciÔ¨Åcally:
1.Low-rank Approximation . We deÔ¨Åne a procedure LowRank that returns two matrices eQ;eK2Rnm
such thateQeK>approximates A. In particular, we use a randomized kernel feature map :Rd!Rm
where(x) =1pmexp(Wx kxk2=2)withW2Rmdrandomly sampled, entry-wise, from the standard
normal distribution N(0;1). We apply to each row vector of Q;Kmatrices, and denote eQ=(Q)and
eK=(K)(row-wise). Note that we do not materialize eQeK>.
2.Sparse Approximation . We deÔ¨Åne a procedure Sparsethat returns a sparse matrix Sthat matches
A eQeK>onsupp(S). In particular, using a family of locality sensitive hash functions, compute the hash
codes of each query and key vectors in Q;Kmatrices (row-wise). Let Sbe the set of locations (i;j)where
qiandkjhave the same hash codes (i.e, fall into the same hash bucket). Let Sbe the sparse matrix whose
support isS, and for each (i;j)2S, deÔ¨Åne
Si;j= exp(q>
ikj) (qi)>(kj) = exp(q>
ikj) eq>
iekj; (1)
whereqi;kj;eqi;ekjare thei-th andj-th rows of Q;K;eQ;eKrespectively. Note that we do not materialize
eQeK>.
3.Scatterbrain Approximation . WitheQ;eKreturned from LowRank andSfromSparse, we compute
the (unnormalized) attention output with
eO= (eQeK>+S)V=eQ(eK>V) +SV: (2)
The precise algorithm, including the normalization step, as well as the causal/unidirectional variant, is
described in Appendix C. We also note Scatterbrain‚Äôs Ô¨Çexibility: it can use diÔ¨Äerent kinds of low-rank and
sparse approximation as its sub-components. The combination of Reformer and Performer is simply one
instance of Scatterbrain. Instead of using Reformer as a sparse component, we could use local attention [ 5] or
random block-sparse attention [ 16]. Instead of using Performer [ 17] as a low-rank component, we could also
use Linear attention [35] or global tokens as in BigBird [70].
6

--- PAGE 7 ---
‚àí1.0‚àí0.5 0.0 0.5 1.0
q/latticetopk0246810MSEReformer (sparse)
Performer (low-rank)
Scatterbrain
(sparse+low-rank)Figure 4: Per-entry MSE for diÔ¨Äer-
ent approximations, across a range
of magnitude of q>k.Scatterbrain has
low MSE for both small and large en-
tries, thus outperforming its sparse
(Reformer) and low-rank (Performer)
counterparts.The Scatterbrain method would work exactly the same way. As
long as the low-rank component is unbiased (e.g., Performer), its
combination with any sparse component in Scatterbrain would yield
an unbiased estimator of the attention matrix as shown below.
4.3 Scatterbrain: Analysis
Our method combines a low-rank approximation eQeK>(which has
rankmn) with a sparse approximation S. We argue that it is
accurate (lower approximation error than baselines) and eÔ¨Écient
(scaling the same as sparse or low-rank alone). The main insight
of the analysis is that our approximation is exact for entries on
the support of S(picked by LSH), which are likely to be large. For
entries not in the support of S(likely to be small), our approximation
matches the low-rank part (Performer) eQeK>, which is unbiased and
has low variance for these entries. As a result, Scatterbrain retains
the unbiasedness of Performer [17] but with strictly lower variance.
We compare Scatterbrain to its low-rank baseline (Performer)
and sparse baseline (Reformer). Performer is also based on the kernel
approximation , and simply uses eQeK>to approximate the attention
matrixA. Reformer uses LSH to identify large entries of A, then
compute a sparse matrix Ssuch thatSij= exp(q>
ikj)forij2supp(S).
Accuracy : Because of the way Sis deÔ¨Åned in Eq. (1), eQeK>+SmatchesA=exp(QK>)exactly on
locations (i;j)2S, which are locations with likely large values. This addresses a weakness of low-rank
methods (e.g., Performer) where the low-rank estimate is not accurate for locations with large values. We
analyze the expectation and variance per entry of our estimator below (proof in Appendix D).
Theorem 2. DeÔ¨Åne(q;k) =exp(q>k),bpfeas Performer‚Äôs estimator and bsbeas Scatterbrain estimator.
DenoteSd 1Rdas the unit sphere. Suppose q;k2Sd 1are such thatkq kk<. Then:
E[bsbe(q;k)] =(q;k);Var[bsbe(q;k)] = (1 p)Var[bpfe(q;k)]<Var[bpfe(q;k)] (3)
wherep= exp( 2
4 2lnd O(ln lnd)).
Hence Scatterbrain is unbiased, similar to Performer [ 17], but with strictly lower variance. The variance
is small if exp(q>k)is small (since Var(bpfe(q;k))will be small), or if exp(q>k)is large (since the probability
of not being selected by LSH, 1 p, will be small). In Fig. 4, we plot the per-entry MSE of diÔ¨Äerent methods
from Theorem 2 when approximating the unnormalized softmax attention exp(QK>). Scatterbrain can
approximate well both small entries (similar to the low-rank baseline, Performer), as well as large entries
(similar to the sparse baseline, Reformer). Thus Scatterbrain has much lower MSE than Performer for large
entries, and lower MSE than Reformer for small entries.
EÔ¨Éciency : In Eq. (2), the computation SVis eÔ¨Écient because Sis sparse, and eQ(eK>V)is eÔ¨Écient
because of the way we associate matrix multiplication (scaling as O(nmd)instead ofO(n2d), which is much
bigger ifmn).
We validate these two properties of our approach in Section 5.
5 Experiments
We validate three claims that suggest Scatterbrain provides an accurate and eÔ¨Écient approximation to
attention matrices, allowing it to outperform its sparse and low-rank baselines on benchmark datasets.
‚Ä¢In Section 5.1, we evaluate the approximation error and testing accuracy of diÔ¨Äerent approximation methods
on pre-trained models such as BigGAN and Vision Transformer. We show that the approximation by
Scatterbrain is close to the Robust PCA oracle and up to 2:1lower approximation error than other
eÔ¨Écient baselines.
7

--- PAGE 8 ---
1 2 3 4 5 6
Entropy0.00.10.20.30.40.5Approx. ErrorScatterbrain
Robust-PCA
1 2 3 4 5 6
Entropy0.00.10.20.30.40.5Approx. ErrorSmyrf
Performer
Scatterbrain
10‚àí1100
Memory2030405060708090100InceptionSmyrf
Performer
Scatterbrain
Full
10‚àí1100
Memory1020304050FIDSmyrf
Performer
Scatterbrain
FullFigure 5: First: approximation comparison between Scatterbrain and its ‚Äúlowerbound" Robust PCA. Second:
comparison of error vs. entropy among SMYRF, Performer and Scatterbrain, three representatives of sparse,
low-rank and sparse+low-rank approximations. Third and forth: Inception score (higher is better) and FID
score (lower is better) of diÔ¨Äerent attention variants for pretrained BigGAN.
‚Ä¢In Section 5.2, we validate that when trained end-to-end, Scatterbrain outperforms baselines (sparse or
low-rank attention) on a wide variety of benchmark tasks, including language modeling, classiÔ¨Åcation, and
the Long-range Arena (LRA) benchmarks. Scatterbrain achieves up to 5 points higher average accuracy on
the LRA benchmark compared to Performer and Reformer.
‚Ä¢In Section 5.3, we demonstrate the scalability of Scatterbrain, showing that it has comparable memory
and time usage with simpler baselines (sparse or low-rank alone) across a range of input sequence lengths
(Section 5.3), while requiring up to 12smaller memory than full attention.
All details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix E.
5.1 Scatterbrain‚Äôs Approximation Accuracy
Table 1: Top-1 Accuracy of pre-trained T2T
Vision Transformer on ImageNet with diÔ¨Äerent
attention replacements. Error represents the
average normalized approximation error to full
attention.
Attention Top-1 Acc Error (avg)
Full Attention 81:7% -
SMYRF 79:8% 11 :4%
Performer 80:1% 7 :5%
Baseline SMYRF + Performer 79:7% 12 :6%
Scatterbrain 80.7% 5.3%WeevaluateScatterbrain‚Äôsapproximationaccuracyinthree
steps: (1) compare it with of Robust PCA (sparse+low-
rank), our theoretical foundation and oracle (2) compare
it with SMYRF3[22], Performer [ 17], which are popular
variants of sparse and low-rank approximation to attention
respectively and a naive baseline that directly adds SMYRF
and Performer, (3) evaluate the inference accuracy when
replacing full attention with Scatterbrain approximation.
Scatterbrain achieves error within 20% of the oracle robust
PCA, and up to 2:1lower error than SMYRF and Per-
former. When serving as a drop-in replacement for full
attention, even without training, Scatterbrain can reduce
the attention memory of Vision Transformer by 98% at the
cost of only 0.8% drop of accuracy.
Setup:We use the attention matrices from pre-trained BigGAN and T2T-ViT. BigGAN is a state-of-the-
art model in Image Generation for ImageNet. BigGAN has a single attention layer at resolution 64 √ó64
(4096 queries). T2T-ViT has 14 attention layers. Scatterbrain sets the ratio between SMYRF and Performer
based on the entropy of an observed subset of attention matrices in diÔ¨Äerent layers. We allocate more memory
to the low-rank component compared to the sparse part if the entropy is high.
Scatterbrain and Robust PCA: We Ô¨Årst show that Scatterbrain approximates pre-trained attention
matrices 105faster while its approximation error is within 20% on average. We also provide an example
visualization on 100 attention matrices from the BigGAN generation process in Figure 5 (left).
Scatterbrain vs. SMYRF and Performer: We show that Scatterbrain approximates pre-trained dense
attention matrices with very low error compared to sparse (Reformer) or low-rank (Performer). Measuring
Frobenius approx. error on the BigGAN image generation task, Scatterbrain achieves 2lower error compared
to Performer.
3SMYRF is a variant of Reformer that does not require the key and query to be the same, which is necessary for experiments
in this section.
8

--- PAGE 9 ---
Table 2: The performance of Scatterbrain, Reformer, Performer and Full-Attention on Long-Range-Arena
benchmarks and 2 popular language modeling tasks. We Ô¨Åx the same number of parameters ( 1=8of the full)
used for approximating the attention matrix for each method.
Attention Copy (ppl) WikiText-103 (ppl)
Full Attention 1 25.258
Reformer 6.8 27.68
Performer 49 66
Scatterbrain 2.58 26.72Attention ListOps TextRetrieval ImagePathÔ¨Ånder Avg
Full Attention 38.263.29 80.85 41.78 73.98 59.62
Reformer 36.85 58.12 78.36 28.3 67.95 53.9
Performer 35.75 62.36 78.83 39.71 68.6 57.05
Scatterbrain 38.6 64.55 80.22 43.65 69.91 59.38
Drop-in replacement for full attention: We show that accurate approximation directly leads to
eÔ¨Écient Inference. We replace BigGAN‚Äôs dense attention with a Scatterbrain layer without other modiÔ¨Åcations.
In 5 (right two), we show Inception and FID scores for Scatterbrain and other baselines under diÔ¨Äerent
memory budgets. Similarly, we use T2T-ViT [ 69], which is a token-to-token vision Transformer pre-trained
on ImageNet [ 25]. In Table 1, we show the average approximation error of Scatterbrain for each layer and the
end-to-end testing accuracy after replacing full attention with Scatterbrain and other baselines. Notably,
Scatterbrain achieves 80:7%Top-1 accuracy, which is only 1%drop from the original 81:7%by full attention
reducing up to 98%of the memory usage.
5.2 End-to-end Training Performance
Scatterbrain‚Äôs accurate approximation of attention matrices allows it to outperform other eÔ¨Écient Trans-
former methods on benchmark tasks. Across a range of diverse tasks, both commonly used autoregressive
tasks (sequence modeling) and benchmark long-range classiÔ¨Åcation tasks (Long-Range Arena), Scatterbrain
outperforms Performer (low-rank baseline) and Reformer (sparse baseline) by up to 4 points.
5.2.1 Auto-regressive Tasks
On the standard language modeling task of Wikitext-103, Scatterbrain obtains 1 point better perplexity than
Reformer (sparse baseline), coming within 1.5 points of full attention.
Settings: We compare the performance of Scatterbrain against Reformer and Performer on one popular
synthetic task, Copy, and one large language modeling task: WikiText103 [ 45]. Reformer is a representative
sparse-approximation-based variant and Performer is a low-rank-approximation-based variant. The base
model is vanilla Transformer [ 63]. We observed that generally allocating more memory budget to sparse
tends to perform better, so Scatterbrain sets the ratio to 3:1 (sparse: low-rank component) for simplicity.
The statistics of each dataset and model hyper-parameters are in Appendix E. We report the best results of
each method in perplexity.
Results: Table 2 shows the testing perplexity for Scatterbrain and other baselines under the same
parameter budget (each approximation is only allowed to compute1
8of the full computation). Scatterbrain
achieves comparable perplexity compared to the full attention Transformer model on Copy, and WikiText-103.
Notably, Scatterbrain achieves 4 points lower perplexity on Copy and 1 point lower on WikiText-103 compared
to Reformer, while Performer does not train stably on auto-regressive tasks (loss does not go down).
Analysis: We also analyze the results by visualizing the error of Reformer (sparse), Performer (low-rank),
and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating the full
attention matrices for each attention layer during training (Appendix E). The conclusion is for language
modeling tasks, sparse+low-rank has the smallest approximation error in most of the cases, and sparse
has the largest error, which matches with the end-to-end results. It also conÔ¨Årms the observation in the
popular benchmark paper [ 57] that kernel or low-rank based approximations are less eÔ¨Äective for hierarchical
structured data.
9

--- PAGE 10 ---
5.2.2 ClassiÔ¨Åcation Tasks
On a suite of long-range benchmark tasks (Long Range Area), Scatterbrain outperforms Reformer (sparse
baseline) and Performer (low-rank baseline) by up to 5 points on average.
Settings: We compare the performance of Scatterbrain against Reformer and Performer on ListOps, two
classiÔ¨Åcations: byte-level IMDb reviews text classiÔ¨Åcation, image classiÔ¨Åcation on sequences of pixels, a text
retrieval, and pathÔ¨Ånder tasks. The datasets are obtained from the Long Range Arena (LRA) Benchmark [ 57],
which is a recent popular benchmark designed for testing eÔ¨Écient Transformers. Similar to the auto-regressive
tasks above, we use Reformer and Performer as baselines. The base model is also a vanilla Transformer. We
follow the evaluation protocol from [57]. We report the best accuracy of each method.
Results: Table 2 shows the individual and average accuracy of each task for Scatterbrain and other
baselines under the same parameters budget. Specially, each approximation is only allowed to use 12:5%of
the full computation. We can see Scatterbrain is very close to full attention even with a large reduction in
computation and memory. Further more, it outperforms all the other baselines consistently on every task
and achieves more than 5 point average accuracy improvement than sparse-based approximation Reformer
and more than 2 point average accuracy improvement than low-rank-based variant Performer.
1024 4096 16384
Sequence Length010203040506070Time (ms)Reformer
Performer
Scatterbrain
Full Attn
1024 4096 16384
Sequence Length024681012Memory (GB)Reformer
Performer
Scatterbrain
Full Attn
Figure 6: Speed and memory required by diÔ¨Äerent eÔ¨Écient attention
methods. Scatterbrain is competitive with SMYRF (sparse baseline)
and Performer (low-rank baseline), while up to 3faster and 12more
memory eÔ¨Écient than full attention for sequence length 4096.Analysis: Similarly, in or-
der to analyze the performance of
Reformer, Performer and Scatter-
brain, we visualize their approxima-
tion error given the same number
of parameters when approximating
the full attention matrices for each
attention layer during training (Ap-
pendix E). We again Ô¨Ånd that Scat-
terbrain has the smallest approx-
imation error, while Performer is
the worst on ListOps and Reformer
has the largest error on classiÔ¨Å-
cation tasks, which matches with
the end-to-end results and conÔ¨Årms
our observations earlier (sparse and
low-rank approximation excel in
diÔ¨Äerent regimes).
5.3 Scatterbrain‚Äôs EÔ¨Éciency, Scaling with Input Sequence Length
We include ablation studies on the scalability of Scatterbrain in Fig. 6, showing that it is as computation and
memory-eÔ¨Écient as simpler baselines such as SMYRF and Performer, while up to 3faster and 12more
memory eÔ¨Écient than full attention for sequence length 4096. This demonstrates that our combination of
sparse and low-rank inherits their eÔ¨Éciency.
We report run times and memory consumption of the sequence lengths ranging from 512 to 32768. We
use a batch size of 16 for all runs and conduct experiments a V100 GPU. Since the eÔ¨Éciency would be largely
conditioned on hardware and implementation details, we perform best-eÔ¨Äort fair comparisons. We adapt
the Pytorch implementation from pytorch-fast-transformers library for our baselines and implement
Scatterbrain similarly without any customized cuda kernels.
6 Discussion
Limitations. As Scatterbrain has sparse attention as a component, it is not yet as hardware friendly (on
GPUs and TPUs) as the low-rank component, which uses the very optimized dense matrix multiplication.
This is the same limitation suÔ¨Äered by other sparse attention methods, but we are excited that more eÔ¨Écient
sparse GPU kernels are being developed [29, 31].
10

--- PAGE 11 ---
Potential negative societal impacts. Our work seeks to understand the role of matrix approximation
(and potentially energy savings) in the attention layer, which may improve a wide range of applications, each
with their own potential beneÔ¨Åts and harms. For example, making it language modeling more compute and
memory eÔ¨Écient might facilitate spreading misinformation, and better image and video processing may make
automatic surveillance easier. To mitigate these risks, one needs to address application-speciÔ¨Åc issues such as
privacy and fairness, going beyond the error metrics we considered. Specially, for language models (LMs),
while our work partially addresses the issue of environmental cost of LMs raised in [ 6], it does not address
other issues such as unfathomable training data [6].
Discussion and future work. In this work, we make an observation on the sparse + low-rank structure
of the attentions in Transformer models and theoretically characterize the regimes where sparse, low-rank
and sparse + low-rank excel, based on the softmax temperature of the attention matrices. Motivated by
this observation, we present Scatterbrain, a novel way to unify the strengths of both sparse and low-rank
methods for accurate and eÔ¨Écient attention approximation with provable guarantees. We empirically verify
the eÔ¨Äectiveness of Scatterbrain on pretrained BigGAN, vision transformers, as well as end-to-end training of
vanilla transformer. We anticipate that the study of this core approximation problem can prove useful in
other contexts, such as generalized attention layers with other non-linearity beside softmax, and wide output
layer in language modeling or extreme-classiÔ¨Åcation.
Acknowledgments
We thank Xun Huang, Sarah Hooper, Albert Gu, Ananya Kumar, Sen Wu, Trenton Chang, Megan Leszczynski,
and Karan Goel for their helpful discussions and feedback on early drafts of the paper.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under
No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying
Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP,
Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson,
Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce,
Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI),
and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is
a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging
and Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions,
Ô¨Åndings, and conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reÔ¨Çect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S.
Government. Atri Rudra‚Äôs research is supported by NSF grant CCF-1763481.
References
[1]Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. ButterÔ¨Çy transform: An eÔ¨Écient FFT based
neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR) ,
2020.
[2]Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and
optimal lsh for angular distance. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors,Advances in Neural Information Processing Systems (NeurIPS) , pages 1225‚Äì1233. 2015.
[3]Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical
and optimal LSH for angular distance. In Proceedings of the 28th International Conference on Neural
Information Processing Systems-Volume 1 , pages 1225‚Äì1233, 2015.
[4]R Artusi, P Verderio, and E Marubini. Bravais-pearson and spearman correlation coeÔ¨Écients: meaning,
test of hypothesis and conÔ¨Ådence interval. The International journal of biological markers , 17(2):148‚Äì151,
2002.
11

--- PAGE 12 ---
[5]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 , 2020.
[6]Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. On the dangers
of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency , New York, NY, USA, 2021. Association for Computing
Machinery.
[7]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
[8]Emmanuel J Cand√®s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations
of Computational mathematics , 9(6):717‚Äì772, 2009.
[9]Emmanuel J Cand√®s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
Journal of the ACM (JACM) , 58(3):1‚Äì37, 2011.
[10]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision ,
pages 213‚Äì229. Springer, 2020.
[11]Beidi Chen and Anshumali Shrivastava. DensiÔ¨Åed winner take all (wta) hashing for sparse datasets. In
Uncertainty in artiÔ¨Åcial intelligence , 2018.
[12]Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with application
to the syrian conÔ¨Çict. The Annals of Applied Statistics , 12(2):1039‚Äì1067, 2018.
[13]Beidi Chen, Yingchen Xu, and Anshumali Shrivastava. Fast and accurate stochastic gradient estimation.
2019.
[14]Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. SLIDE: In defense
of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of
Machine Learning and Systems , 2:291‚Äì306, 2020.
[15]Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song,
Anshumali Shrivastava, and Christopher R√©. Mongoose: A learnable lsh framework for eÔ¨Écient neural
network training. In The International Conference on Learning Representations (ICLR) , 2021.
[16]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
[17]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with
performers. arXiv preprint arXiv:2009.14794 , 2020.
[18]Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen, Mengnan Zhao, and Anshumali
Shrivastava. A tale of two eÔ¨Écient and informative negative sampling distributions. In International
Conference on Machine Learning , pages 2319‚Äì2329. PMLR, 2021.
[19]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a Ô¨Åxed-length context. arXiv preprint
arXiv:1901.02860 , 2019.
[20]Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R√©. Learning fast algorithms for
linear transforms using butterÔ¨Çy factorizations. In The International Conference on Machine Learning
(ICML), 2019.
12

--- PAGE 13 ---
[21]Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,
and Christopher R√©. Kaleidoscope: An eÔ¨Écient, learnable representation for all structured linear maps.
InThe International Conference on Learning Representations (ICLR) , 2020.
[22]Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf: EÔ¨Écient attention
using asymmetric clustering. arXiv preprint arXiv:2010.05315 , 2020.
[23]Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gradient
descent for some non-convex matrix problems. In International Conference on Machine Learning , pages
2332‚Äì2341. PMLR, 2015.
[24]Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R√©, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms , pages 1060‚Äì1079. SIAM, 2018.
[25]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 248‚Äì255,
2009. doi: 10.1109/CVPR.2009.5206848.
[26]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[27]Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. In International Conference on Learning Representations (ICLR) , 2019.
[28]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
[29]Trevor Gale, Matei Zaharia, CliÔ¨Ä Young, and Erich Elsen. Sparse GPU kernels for deep learning. In
Supercomputing , 2020.
[30]Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing.
InVldb, volume 99, pages 518‚Äì529, 1999.
[31]Scott Gray, Alec Radford, and Diederik P Kingma. GPU kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224 , 3, 2017.
[32]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R√©. Hippo: Recurrent memory with
optimal polynomial projections. In Advances in neural information processing systems (NeurIPS) , 2020.
[33]Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of
educational psychology , 24(6):417, 1933.
[34]Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing , pages
604‚Äì613, 1998.
[35]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International Conference on Machine Learning ,
pages 5156‚Äì5165. PMLR, 2020.
[36]Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The eÔ¨Écient transformer. In The
International Conference on Machine Learning (ICML) , 2020.
[37] Alex Krizhevsky, GeoÔ¨Ärey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[38]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite bert for self-supervised learning of language representations. In The International
Conference on Learning Representations (ICLR) , 2020.
13

--- PAGE 14 ---
[39]Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear
memory: How to make performers slim. arXiv preprint arXiv:2012.11346 , 2020.
[40]Drew Linsley, Junkyung Kim, Vijay Veerabadran, and Thomas Serre. Learning long-range spatial
dependencies with horizontal gated-recurrent units. arXiv preprint arXiv:1805.08315 , 2018.
[41]Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, and Anshumali Shrivastava. Climbing the
wol: Training for cheaper inference. arXiv preprint arXiv:2007.01230 , 2020.
[42]Haoneng Luo, Shiliang Zhang, Ming Lei, and Lei Xie. SimpliÔ¨Åed self-attention for transformer-based
end-to-end speech recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT) , pages 75‚Äì81.
IEEE, 2021.
[43]Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.
Luna: Linear uniÔ¨Åed nested attention. arXiv preprint arXiv:2106.01540 , 2021.
[44]Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association
for computational linguistics: Human language technologies , pages 142‚Äì150, 2011.
[45]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
arXiv preprint arXiv:1609.07843 , 2016.
[46]Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv
preprint arXiv:1804.06028 , 2018.
[47]Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin
Tran. Image transformer. In International Conference on Machine Learning , pages 4055‚Äì4064. PMLR,
2018.
[48]Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology
network corpus. Language Resources and Evaluation , 47(4):919‚Äì944, 2013.
[49]Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive trans-
formers for long-range sequence modelling. In The International Conference on Learning Representations
(ICLR), 2020.
[50]Colin RaÔ¨Äel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text
transformer. arXiv preprint arXiv:1910.10683 , 2019.
[51]Hubert Ramsauer, Bernhard Sch√§Ô¨Ç, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,
Lukas Gruber, Markus Holzleitner, Milena Pavloviƒá, Geir Kjetil Sandve, et al. HopÔ¨Åeld networks is all
you need. arXiv preprint arXiv:2008.02217 , 2020.
[52]Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research , 12
(12), 2011.
[53]Aurko Roy, Mohammad SaÔ¨Äar, Ashish Vaswani, and David Grangier. EÔ¨Écient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics , 9:
53‚Äì68, 2021.
[54]Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product
search (mips). In Advances in Neural Information Processing Systems (NeurIPS) , pages 2321‚Äì2329,
2014.
[55]Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems , pages 3088‚Äì3096, 2015.
14

--- PAGE 15 ---
[56]Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span
in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics ,
2019.
[57]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu
Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eÔ¨Écient transformers.
arXiv preprint arXiv:2011.04006 , 2020.
[58]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. EÔ¨Écient transformers: A survey. arXiv
preprint arXiv:2009.06732 , 2020.
[59]Richard Taylor. Interpretation of the correlation coeÔ¨Écient: a basic review. Journal of diagnostic medical
sonography , 6(1):35‚Äì39, 1990.
[60]Reginald P Tewarson and Reginald P Tewarson. Sparse matrices , volume 69. Academic Press New York,
1973.
[61]Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher R√©. Learning compressed transforms
with low displacement rank. In Advances in neural information processing systems (NeurIPS) , pages
9052‚Äì9060, 2018.
[62]Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal
on Mathematics of Data Science , 1(1):144‚Äì160, 2019.
[63]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.
[64]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461 , 2018.
[65]Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear
complexity. arXiv preprint arXiv:2006.04768 , 2020.
[66]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In The International Conference on Learning Representations
(ICLR), 2019.
[67]Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint
arXiv:2102.03902 , 2021.
[68]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:
Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 ,
2019.
[69]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint
arXiv:2101.11986 , 2021.
[70]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. Advances in Neural Information Processing Systems , 33, 2020.
[71]Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and
Bryan Catanzaro. Long-short transformer: EÔ¨Écient transformers for language and vision. arXiv preprint
arXiv:2107.02192 , 2021.
15

--- PAGE 16 ---
Appendix
Table of Contents
A Extended Related Work 17
A.1 Robust PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 EÔ¨Écient Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Locality Sensitive Hashing for EÔ¨Écient Neural Network Training . . . . . . . . . . . . . . 18
A.4 Structured Matrices for EÔ¨Écient Machine Learning Models . . . . . . . . . . . . . . . . . 18
B Motivating Observations: Low-rank and Sparse Structures of Attention Matrices 19
B.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Observation 1: Sparse and low-rank approximation errors are negatively correlated . . . . 19
B.3Observation 2: Sparse approximation error is lower when softmax entropy is low and
low-rank approximation error is lower error when entropy is high . . . . . . . . . . . . . . 19
B.4Observation 3: Sparse + Low-rank achieves better approximation error than sparse or
low-rank alone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C Scatterbrain Algorithm and Implementation Details 22
D Proofs 23
D.1 Expressiveness of Sparse + Low-rank Matrices . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 Generative Model, Softmax Temperature, and Matrix Approximation . . . . . . . . . . . 26
D.3 Scatterbrain: Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E Additional Experiments and Details 31
E.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.2 Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.3 More Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E.5 Additional Experiments of Fine-tuning Bert on GLUE . . . . . . . . . . . . . . . . . . . . 34
F Further Discussions and Future Work 34
16

--- PAGE 17 ---
A Extended Related Work
A.1 Robust PCA
Robust Principle Component Analysis (robust PCA) is the problem of Ô¨Ånding a composition of a matrix M
into a sum of sparse and low-rank components: M=S+L. It is a modiÔ¨Åcation of PCA to accommodate
corrupted observations (aka, noise). The sparse part covers the noise, while the low-rank part recovers the
principle components. The most popular method to solve the problem is convex relaxation [ 8], where one
minimizes the error kM S Lk2
Fsubject to`1constraint onkSk1and nuclear norm constraint on kLk, in
order to promote the sparsity of Sand the low-rankness of L. This convex problem can be solved with a
variety of methods, such as interior point methods or the method of Augmented Lagrange Multipliers.
In our context, to Ô¨Ånd a sparse + low-rank decomposition of the attention matrix, one can also heuristically
‚Äúpeel oÔ¨Ä‚Äù the sparsepartbyÔ¨Ånding thelargeentriesof theattentionmatrix, then Ô¨Åndalow-rankdecomposition
of the remainder. To avoid materializing the full attention matrix, one can use LSH to Ô¨Ånd potential locations
of large entries, and use matrix completion [ 52] to Ô¨Ånd a low-rank decomposition. Gradient descent can Ô¨Ånd
global optimum for this matrix completion problem [ 23]. However, it still requires too many iterations to be
used in each training step.
A.2 EÔ¨Écient Transformers
Sparse, Low-rank Approx.: Transformer-based model such as BERT [ 38] has achieved unprecedented
performance in natural language processing. Recently, Vision Transformers [ 28,69] has also achieved
comparable performance to the traditional convolutional neural network in computer vision tasks [ 66].
However, the quadratic computation of the attention layers constrains the scalability of Transformers. There
are many existing directions to overcome this bottleneck, including attention matrix approximation such
as Reformer [ 36], Performer [ 17], leveraging a side memory module that can access multiple tokens at
once [38,39,56] such as Longformer [ 5] and BigBird [ 70], segment-based recurrence such as Transformer-
XL [19] and Compressive Transformer [ 49]. Please refer to a recent survey [ 58] for more details. In this paper,
we mainly explore within the scope of approximating dense or full attention matrices.
Existing combination of Sparse and Low-rank Attention: Our focus on the classical and well-
deÔ¨Åned problem of matrix approximation, as opposed to simply designing an eÔ¨Écient model that performs
well on downstream tasks (e.g., Longformer, Luna, Long-short transformer, etc.) aÔ¨Äords us several advantages:
(i) Easier understanding and theoretical analysis (Section 3, 4). We see that Scatterbrain yields an unbiased
estimate of the attention matrix, and we can also understand how its variance changes. (ii) Clear-cut
evaluation based on approximation error, as well as the ability to directly replace a full attention layer with
Scatterbrain attention without re-training (Section 5). This setting is increasingly important as transformer
models are getting larger and training them from scratch has become prohibitively costly. Other methods
such as Luna and Long-short transformer are not backward compatible with pre-trained models.
Here we compare Scatterbrain with other work mentioned by the reviewer, showing how most of them are
special cases of Scatterbrain. We will also add this discussion in the updated version of the manuscript.
‚Ä¢Longformer [ 5]: a special case of Scatterbrain where the sparse component is local attention, and the
low-rank component is the global tokens. Global tokens can be considered a restricted form of low-rank
approximation.
‚Ä¢BigBird [ 70]: a special case of Scatterbrain where the sparse component is local + random sparse attention,
and the low-rank component is the global tokens. The use of global tokens makes the model unsuited
for autoregressive modeling. On the other hand, Scatterbrain‚Äôs generality allows it to use other kinds of
low-rank attention (e.g., Performer), and thus Scatterbrain works on both the causal/autoregressive and
the bidirectional/non-causal attention settings. BigBird‚Äôs motivation is also quite diÔ¨Äerent from ours: they
aim to design eÔ¨Écient attention such that the whole Transformer model is still a universal approximator
and is Turing complete. Our goal is more concrete and easier to evaluate: we approximate the attention
matrices, to get a small Frobenius error between the Scatterbrain attention and the full attention matrices.
‚Ä¢Luna [43] (concurrent work): they use a Ô¨Åxed-length extra sequence and two consecutive attention steps:
the context sequence attends to the extra sequence, and then the query sequence attends to the extra
sequence. This is similar in spirit to low-rank attention (Linformer) and global tokens, but it is not a
17

--- PAGE 18 ---
low-rank approximation due to the non-linearity between the two attention steps. It is not clear to us that
it combines diÔ¨Äerent kinds of attention.
‚Ä¢Long-short transformer[ 71] (concurrent work): a special case of Scatterbrain where the sparse component
is local attention and the low-rank component is Linformer.
A.3 Locality Sensitive Hashing for EÔ¨Écient Neural Network Training
Locality Sensitive Hashing (LSH) has been well-studied in approximate nearest-neighbor search [ 2,11,27,30,
34,54]. Since the brute-force approach for similarity search is computationally expensive, researchers have
come up with various indexing structures to expedite the search process. Usually this comes with trade-oÔ¨Äs
on the search quality. Based on these indexing structures, one can achieve sub-linear search time. LSH has
been used in estimation problem as well [12, 13].
Recently, there has been several work taking advantage of LSH data structures for eÔ¨Écient neural network
training. During training process, the weight matrices are slowly modiÔ¨Åed via gradients derived from objective
functions. If we consider the weights as the search data and input as queries, we can view neural network
training as a similarity search problem. For example, [ 14,18,41] proposes an algorithm which performs
sparse forward and backward computations via maximum inner product search during training. It is based
on the observation that the model is usually over-parameterized so the activation for a given input could be
sparse and LSH is used to Ô¨Ånd or impose the sparse structure. Similarly, LSH based algorithms have also been
used in Transformers [ 14,15], where LSH is used to capture the sparse structure of the attention matrices.
They can largely reduce the memory bottleneck of self-attention modules especially over long sequences in
Transformer. Though [ 15] has done some exploration to improve LSH accuracy-eÔ¨Éciency trade-oÔ¨Äs through
learnable LSH, most of the above works have limited understanding on when and where LSH can perform
well.
A.4 Structured Matrices for EÔ¨Écient Machine Learning Models
Sparse + low-rank is an example of a class of structured matrices: those with asymptotically fast matrix-vector
multiplication algorithm ( o(n2)time complexity) and few parameters ( o(n2)space complexity). Common
examples include sparse, low-rank matrices, and matrices based on fast transforms (e.g., Fourier transform,
circulant, Toeplitz, Legendre transform, Chebyshev transform, and more generally orthogonal polynomial
transforms). These classes of matrices, and their generalization, have been used in machine learning to replace
dense matrices in fully connected, convolutional, and recurrent layers [ 32,55,61]. De Sa et al. [24]shows that
any structured matrix can be written as product of sparse matrices, and products of sparse matrices even
with Ô¨Åxed sparsity pattern have been shown to be eÔ¨Äective at parameterizing compressed models [ 1,20,21].
In our setting, it remains diÔ¨Écult to approximate the attention matrix with these more general classes of
structured matrices. This is because many of them are Ô¨Åxed (e.g., Fourier transform, orthogonal polynomial
transforms), and there lacks eÔ¨Écient algorithms to Ô¨Ånd the closest structured matrix to a given attention
matrix.
18

--- PAGE 19 ---
B Motivating Observations: Low-rank and Sparse Structures of
Attention Matrices
We aim to build a deeper understanding of sparse and low-rank structures in real attention matrices: where
each of them excel, and the potential for their combination. SpeciÔ¨Åcally, we
‚Ä¢show that sparse and low-rank approximation errors are negatively correlated (through statistical tests),
‚Ä¢characterize regimes where each of sparse and low-rank approximation are well-suited, as dictated by the
entropy of the softmax attention distribution, and
‚Ä¢demonstrate that sparse + low-rank has the potential to achieve better approximation than either.
B.1 Setup
DenoteMas the attention matrix (after softmax) and Has entropy. We measure approximation error by
the Frobenius norm or the original matrix and the approximation (sparse or low-rank). All the observed
attention matrices in this section are from (1) a 4-layer vanilla Transformer trained from scratch on char-level
IMDb reviews classiÔ¨Åcation [ 57] (2) a 16-layer vanilla Transformer trained from scratch on WikiText103 [ 45]
(3) a 1-layer (attention) pre-trained BigGAN on ImageNet [ 25]. To collect attention matrices for IMDb and
WikiText103, we Ô¨Årst save checkpoint of the models in every epoch; then evaluate 100 samples from validate
data for each checkpoint and collect attention matrices from each layer each head. Note we take the median of
the stats (error) for those 100 samples if it is diÔ¨Écult to visualize. To collect attention matrices for BigGAN,
we generate 100 samples and collect the attention on the Ô¨Çy.
B.2 Observation 1: Sparse and low-rank approximation errors are negatively
correlated
Table 3: The Spearman‚Äôs rank, Pearson and Kendall‚Äôs Tau correlation coeÔ¨Écients between Sparse and Low-rank
approx. error on IMDb, WikiText-103, and BigGAN-ImageNet. P-values of <0:05indicate statistical signiÔ¨Åcance.
The two errors are negatively correlated.
IMDb WikiText103 BigGAN-ImageNet
Coef p-value Coef p-value Coef p-value
Spearman‚Äôs rank -0.89<.00001 -0.63<.00001 -0.21<.00001
Pearson -0.78<.00001 -0.61<.00001 -0.31<.00001
Kendall‚Äôs Tau -0.74<.00001 -0.51<.00001 -0.18<.00001
We Ô¨Åxed the number of parameters, K, allowed for each attention matrix approximation and collect
the errors from ideal sparse and low-rank approximations: top  Kentries for each row of the matrix for
sparse and top Keigenvalues for low-rank. Then we run three standard statistical correlation tests [ 4,59],
Spearman, Pearson and Kendall‚Äôs Tau on sparse and low-rank approximation error for all the matrices. We
can see from Table 3 that errors are signiÔ¨Åcantly negatively correlated (p-value <0:05). Further more, the
left three plots on Figure 7 visualizes the correlation between the two errors on three datasets.
This negative correlation suggests that there is some property of the softmax attention distribution which
determines when sparse or low-rank excels. We validate this claim in the next observation.
B.3 Observation 2: Sparse approximation error is lower when softmax entropy
is low and low-rank approximation error is lower error when entropy is
high
We visualize the sparse and low-rank approximation error against the entropy of attention matrices H(M)
(applied to each row, then averaged) on the right plot in Figure 7. The attention matrices are 2R10241024
(padded) so the x-axis has range from [0;ln(1024)]. For high-entropy distributions (more diÔ¨Äused) low-rank
19

--- PAGE 20 ---
0.0 0.2 0.4 0.6 0.8
Sparse Approx. Error0.00.10.20.30.40.50.60.70.8Low-rank Approx. Error
4 5 6 7
Entropy0.00.10.20.30.40.50.60.7Approx. ErrorSparse
Low-rank
Sparse+Lowrank
0.0 0.2 0.4 0.6 0.8 1.0
Sparse Approx. Error0.00.20.40.60.81.0Low-rank Approx. Error
0 2 4 6
Entropy0.00.20.40.60.81.0Approx. ErrorSparse
Low-rank
Sparse+Lowrank
0.0 0.1 0.2 0.3
Sparse Approx. Error0.000.050.100.150.200.250.300.35Low-rank Approx. Error
2 4 6
Entropy0.000.050.100.150.200.250.30Approx. ErrorSparse
Low-rank
Sparse+LowrankFigure 7: Characterization of the relationship between the softmax distribution of each attention matrix row and
approximation error of sparse, low-rank and sparse+low-rank. The top, middle and bottom plots are for IMDb,
WikiText103 and BigGAN-ImageNet respectively. Left: The approximation error of sparse and low-rank are negatively
correlated. Sparse performs well when low-rank does not, and vice versa. Right: Entropy of the softmax attention
distribution (i.e., scale of logits) determines the regimes where sparse, low-rank, or sparse + low-rank perform well.
Sparse + low-rank yields better approximation than sparse or low-rank alone, across the board. 20

--- PAGE 21 ---
matrices approximates the attention matrix well. For low-entropy distributions (more peaked), sparse matrices
are better-suited.
This implies that sparse and low-rank approximations could be complementary: if we can combine
the strength of both, it is possible to come up with a better approximation across more general scenarios.
Therefore, in the next observation, we try to combine sparse and low-rank approximations.
B.4 Observation 3: Sparse + Low-rank achieves better approximation error
than sparse or low-rank alone
We Ô¨Ånd an approximation of the attention matrix of the form S+L, whereSis sparse and Lis low-rank.
This problem has a rich history and is commonly solved with Robust PCA. As shown in 7, across the range
of entropy, sparse + low-rank approximation can achieve lower error than either sparse or low-rank when
choosing the correct mix ratio of sparse and low rank approximation ideally (with robust-PCA).
Motivated by the fact that sparse and low-rank approximations of attention matrices have complementary
strengths (Observations 1 and 2), one might want to combine them (Observation 3) in hope of yielding a
more robust approximation that works well across diÔ¨Äerent kinds of attention matrices. The above introduces
three main challenges that we have addressed in the main paper:
‚Ä¢how to Ô¨Ånd sparse + low-rank decomposition of an attention matrix that is compute eÔ¨Écient (the most
studied algorithm, robust PCA, is orders of magnitude too slow to be done at each training iteration) and
memory eÔ¨Écient (i.e., without materializing the full matrix) (Section 4),
‚Ä¢if we can Ô¨Ånd such a sparse + low-rank decomposition, how accurate is the approximation (Section 4.3),
‚Ä¢how expressive is the sparse + low-rank parameterization, i.e., are there natural classes of matrices where
sparse + low-rank yields asymptotically better approximation than sparse or low-rank alone) (Section 3)?
21

--- PAGE 22 ---
C Scatterbrain Algorithm and Implementation Details
LetQ;K2Rndbe the query and key matrices respectively, and V2Rndbe the value matrix. Let the
rows ofQbeq1;:::;qn, and the rows of Kbek1;:::;kn. The attention computes:
softmax(QK>)V;
with softmax applied row-wise, where for each vector v2Rn,softmax (v) =1Pn
j=1evjev1;:::;evn>:Here
we omit the usual scaling ofQK>
p
dfor simplicity since that could be folded into QorK. Note that
softmax (QK>) =D 1exp(QK>), where the exponential function is applied element-wise and Dis a diagonal
matrix containing the softmax normalization constants ( Di;i=Pn
j=1exp(q>
ikj)). Then attention has the
formD 1exp(QK>)V.
We describe the Scatterbrain approximation algorithm in Algorithm 1. This includes the normalization
step.
Algorithm 1 Scatterbrain Approximation of Attention
1:Input:Q;K;V2Rnd, hyper-parameters m;k;l
2:procedure Init(m;k;l)
3:SampleW2RmdwhereWiN(0;1)i.i.d.
4:Kernels:Rd7!Rm,(x) =exp(Wx kxk2=2)pm
5:Hash8l2[L],Hl=fhl;kgk2[K],H=[l2[L]Hl
6:end procedure
7:procedure LowRankApprox (Q;K;V; )
8: eQ=(Q),eK=(K) .applied to each row
9: return eQ(eK>V),eQ(eK>)1n.
10:end procedure
11:procedure SparseApprox (Q;K;V;;H)
12:S=f(i;j)jH(Qi) =H(Kj)g
13:S sparse matrix whose support is S
14: for(i;j)2Sdo
15:Sij= exp(q>
ikj) (qi)>(kj).
16: end for
17: returnSV,S1n.
18:end procedure
19:procedure ScatterbrainApprox (Q;K;V)
20:,h Init(m;k;l).
21:Olr;Dlr LowRankApprox (Q;K;V; ).
22:Os;Ds SparseApprox (Q;K;V;;h ).
23: return diag(Dlr+Ds) 1(Olr+Os).
24:end procedure
Autoregressive / Causal / Unidirectional Attention To approximate autoregressive attention, we
simply use the autoregressive variant of low-rank attention, and apply the autoregressive mask to the sparse
attention. In particular, let M2Rnnbe the autoregressive mask, whose lower triangle is all ones and the
rest of the entries are zero. The unnormalized attention matrix is exp((QK>)M), and the unnormalized
output is exp((QK>)M)V, whereis elementwise multiplication.
The low-rank autoregressive variant computes ((eQeK>)M)V, though with a custom GPU kernel /
implementation so as not to materialize the nnmatrix. For the sparse component, we simply mask out
locationsSijwherei > j. That is, we can perform SMeÔ¨Éciently. As a result, we can compute the
Scatterbrain output ((eQeK>)M)V+ (SM)VeÔ¨Éciently.
22

--- PAGE 23 ---
D Proofs
D.1 Expressiveness of Sparse + Low-rank Matrices
To motivate the use of sparse + low-rank matrices, we describe a family of attention matrices where sparse
+ low-rank matrices need asymptotically fewer parameters to approximate the attention matrix, compared
to sparse or low-rank matrices alone. For there cases, either sparse or low-rank alone requires a quadratic
number of parameters ( O(n2), wherennis the dimension of the attention matrix) to get approximation
error in Frobenius norm, while sparse + low-rank only requires O(n)parameters.
We construct a matrix family that shows the separation between the approximation capability of sparse +
low-rank vs. sparse or low-rank alone. More speciÔ¨Åcally, we will use diagonal + low-rank (a special case of
sparse + low-rank).
Example 1. Letdenote a parameter that satisÔ¨Åes 2(0;1=2]. Consider the following randomized
construction of a matrix Q2Rndwithd6 2lognandd= ( 2logn), where each entry of Qis
picked independently and uniformly at random from f1=p
dg. LetM=(QQ>)whereis the elementwise
exponential function (we Ô¨Årst ignore the normalization term of softmax here).
It can be shown (e.g. by HoeÔ¨Äding‚Äôs inequality) that with high probability
(QQ>)i;j=(
1; ifi=j;
2[ ;];otherwise:
SinceM=(QQ>)whereis the elementwise exponential function,
Mi;j=(
e; ifi=j;
2[1 O();1 +O()];otherwise:
Intuitively, as the attention matrix Mhas large diagonal entries, low-rank matrices will not be able to
approximate it well. However, the oÔ¨Ä-diagonals are also of reasonable size, thus making sparse approximation
diÔ¨Écult. With sparse + low-rank, we can use the sparse part to represent the diagonal, and the low-rank
part to represent the remaining elements, allowing it to approximate this matrix well. We formalize this
separation in the theorem below.
Theorem 3. LetMbe the attention matrix from Example 1. For any 2[0;1], with probability at least
1 n 1, there exists a sparse + low-rank estimator with O( 1n3=2logn)parameters that achieve pn
Frobenius error. For any matrix R2Rnnwith rank such that n rank = 
(n)(e.g.,Rhaso(n2)parameters),
with probability at least 1 n 1, we havekM RkF
(pn). Moreover, any matrix ESthat has row sparsity
k(each row has less than knon-zeros) such that n k=!(1)(e.g.,EShaso(n2)parameters) will have error
kM ESkF
(pn)with probability at least 1 n 1.
We see that for any 2[0;1], any low-rank or sparse estimator for Mwith (n2)parameters has 
( 1)
times the error of the sparse + low-rank estimator with O( 1n1:5logn)parameters.
Proof of Theorem 3. For eachi2[n], letqidenote thei-th row ofQ2Rnd. DeÔ¨ÅneJ2Rnnto be the all
1s matrix. DeÔ¨Åne T=M J QQ>. Therefore,
Ti;j=(
e 2 ifi=j
eq>
iqj 1 q>
iqjotherwise:
By HoeÔ¨Äding‚Äôs inequality, for a pair i6=j, we have that
P q>
iqj E[q>
iqj]
2 exp0
B@ 22

1p
d  1p
d21
CA= 2 exp( d2=2):
Note that E[q>
iqj] = 0.
23

--- PAGE 24 ---
By a union bound over all pairs i6=j(there are n(n 1)=2such pairs), with probability at least
1 n2exp 
 d2=2
, we have that
q>
iqj2[ ;]for alli6=j:
Since we assume that d6 2logn, we have that
n2exp( d2=2)n2exp( 3 logn) =n 1:
Henceq>
iqj2[ ;]for alli6=jwith probability at least 1 n 1. For the rest of the proof, we only consider
this case (where q>
iqj2[ ;]for alli6=j).
Since 1 +xex1 +x+x2forjxj<1, we can bound the oÔ¨Ä diagonal elements jTi;jj2. In particular,
for alli6=j,
jTijj=eq>
iqj 1 q>
iqj 
q>
iqj
2: (4)
Sparse + low-rank estimator: We use the following sparse + low-rank estimator:
ESL= (e 2)I|{z}
sparse+J+QQ>
|{z}
low rank;
where (e 2)Ihas row sparsity 1 and rank(J+QQ>)d+ 1 =O 
 2logn
.
Notice that the ESLestimate matches Mexactly on the diagonal, and on the oÔ¨Ä-diagonal it diÔ¨Äers from
MbyTij. Thus, the Frobenious error of the sparse + low-rank estimator is
kM ESLkF2p
n(n 1)2n:
Set=p
n1=4for01, Then
(i) The sparse + low-rank parameter count is n+nranknO( 2logn)O( 1n1:5logn).
(ii) The Frobenius error is pn.
Low-rank estimator: We want to argue that low-rank approximation would require more parameters. If
we approximate the matrix (e 2)Iby a matrix Rwith rankr, then the diÔ¨Äerence matrix will have at least
n dsingular values of magnitude e 21=2. As a result, by the Eckart‚ÄìYoung‚ÄìMirsky theorem,
k(e 2)I RkF1
2p
n r:
DeÔ¨ÅneT0=T (e 2)I, thenT0is all 0 on the diagonal and has absolute value 2on oÔ¨Ä-diagonal
entries. ThuskT0kF2n=pn.
We want to show that if R0is a rankr0matrix, thenkM R0kF1
2p
n r0 d 1 kT0kF. We argue
by contradiction. Suppose that there exists some matrix R0with rankr0such that
kM R0kF1
2p
n r0 d 1 kT0kF:
DeÔ¨ÅneR=R0 J QQ>, soM R0= (e 2)I R+T0. We see that:
k(e 2)I RkF=kM R0 T0kF
kM R0kF+kT0kF
1
2p
n r0 d 1
1
2p
n rank(R):
This contradicts the result above, which states that k(e 2)I RkF1
2p
n rank(R).
Therefore any low-rank estimator with rank rsuch thatn r= 
(n), which has 
(n2)parameters, will
have error at least 
(p
n r d 1) kT0kF= 
(pn), which is 
( 1)times the error of the sparse +
low-rank estimator above.
24

--- PAGE 25 ---
Sparse estimator: For our sparse estimator, it is easy to see that for any ES2Rnnthat has row sparsity
k(each row has fewer than knon-zeros),
kM ESkF
(p
n(n k)):
This implies that in order to achieve error O(pn), we would need n k=O(1), which requires 
(n2)
parameters.
Now we construct a matrix that shows better separation between the approximation capability of sparse
+ low-rank vs sparse or low-rank alone.
Example 2. Consider the following randomized construction of matrix Q2Rndwithd6 2rlognand
d= ( 2rlogn)(2(0;1]and close to 0 and ris(logn)): each entry of Qis picked independently and
uniformly at random from fp
r=dg. LetM=(QQ>)whereis the elementwise exponential function.
Similar to Example 1, with high probability, we have:
(QQ>)i;j=(
r; ifi=j;
2[ ;];otherwise:
We also have:
Mi;j=(
er; ifi=j;
2[1 O();1 +O()];otherwise:
By settingrappropriately, we can formalize the separation between the approximation ability of sparse,
low-rank, and sparse + low-rank matrices:
Theorem 4. LetMbe the attention matrix from Example 2. Any sparse or low-rank estimator of Mneeds

(n2)parameters for O(n)error with probability at least 1 n 1while a sparse + low-rank estimator needs
O(n)parameters for O(n)error with probability at least 1 n 1.
Proof of Theorem 4. Similar to the proof of Theorem 3, by HoeÔ¨Äding‚Äôs inequality, for a pair i6=j, we have
that
P q>
iqj E[q>
iqj]
2 exp0
B@ 22

rp
d  rp
d21
CA= 2 exp
 d2
2r
:
Note that E[q>
iqj] = 0. By a union bound over all pairs i6=j(there aren(n 1)=2such pairs), with
probability at least 1 n 1(sinced6 2rlogn), we have that
q>
iqj2[ ;]for alli6=j:
Since we assume that d6 2logn, we have that For the rest of the proof, we only consider this case (where
q>
iqj2[ ;]for alli6=j).
LetT=M (er 1)I+J, whereJis the all one matrix. We see that Tis zero on the diagonal.
Moreover, using the fact that ex1 + 2jxjfor allx2[ 1;1], the oÔ¨Ä-diagonal entries of Thave of magnitude
at most 2.
We consider 3 diÔ¨Äerent estimators.
Sparse + low-rank estimator: Our estimator is
ESL= (er 1)I|{z}
sparse+J|{z}
low rank;
where (e 1)Ihas row sparsity 1 and rank(J) = 1.
25

--- PAGE 26 ---
The Frobenious error of sparse + low-rank approximation is
kM ESLkFO(p
2n(n 1))O(n):
We have that:
(i) Sparse + low-rank parameter count is n(1 + 1)O(n).
(ii) Its Frobenius error is O(n).
Low-rank estimator: We want to argue that low-rank approximation would require more parameters.
From a similar observation that any matrix Rwith rank that n rank = 
(1) ,
k(er 1)I RkF
(er);
(by Eckart‚ÄìYoung‚ÄìMirsky theorem), we obtain a similar result to the proof of Theorem 3.
IfR0is a matrix with rank such that n rank = 
(1), thenkM R0kF
(n) kTkF
(n) O(n)

(n). Hence any low-rank matrix with O(n2)parameters would have error 
(n).
Sparse estimator: Similar to the proof of Theorem 3, for our sparse estimator, it is easy to see that for
anyES2Rnnthat has row sparsity k(each row has fewer than knon-zeros),
kM ESkF
(p
n(n k)):
This implies that to get O(n)error, we would need 
(n2)parameters.
D.2 Generative Model, Softmax Temperature, and Matrix Approximation
Here we show 3 cases where depending on the softmax temperature, either we‚Äôll need low-rank, low-rank +
sparse, or sparse to approximate the attention matrix.
We start with some notation Ô¨Årst. Given a matrix B, letB[i;j]be the entry in the ith row and jth
column. For a range [l;r], we deÔ¨Åne a matrix B[l;r]whereB[l;r][i;j] =B[i;j]ifB[i;j]2[l;r]andB[l;r]= 0
otherwise (that is, B[l;r]only keep entries for Bthat are in the range [l;r], with other entries zeroed out).
We write supp(C)for the set of locations of non-zeros in C. We leti(D)be thei-th largest (in absolute
value) eigenvalue of D.
To prove Theorem 1, we Ô¨Årst deÔ¨Åne a more general matrix class, prove that the attention matrix in Process 1
is a subset of this class (with high probability), and then show that Theorem 1 holds for this more general
class. We introduce an extra parameter l2R, in addition to the inverse temperature and the intro-cluster
distance .
Matrix Class 1. LetQ2Rndwith every row of Qhaving`2-norm in [1 O();1 +O()], and let
A=QQ>. Further:
1.LetH=A[1=l;2 1=l]for somel
(1). Assume that His block diagonal with 
(n)blocks, and supp(H)
iso(n2). That is, the large entries of QQ>form a block diagonal matrix.
2.LetL=A HthenL=A[ ;]where  =o(1=logd). Assume that there is a constant fraction of
elements in supp(L)falling in [0;]. Assume that supp(A[0;])is
(n2).
LetM= exp(A).
We now show that Process 1 is a subset of Matrix Class 1, with high probability.
Lemma 5. The matrix Min Process 1 is a subset of Matrix Class 1, where l=1
1 2.
Proof.We Ô¨Årst bound the norm of each row in Qin Process 1. For any i;j, we have
kzijk2=kci+rijk2=kcik2+ 2c>
irij+krijk2:
SinceciN(0;Id=p
d),kcik22[1 2;1 + 2]with probability at least 1 2e d2=8(by the standard
argument using the fact that 2-random variables are sub-exponential). Similarly, krijk22[2 4;2+4]
with probability at least 1 2e d2=8. By concentration of measure, we can also bound 2c>
irij2[2 
23;2 + 23]as well. Therefore, we have that kzijk22[1 O();1 +O()].
26

--- PAGE 27 ---
Now we show that the large entries of QQ>form a block diagonal matrix. With high probability, the
large entries come from intra-cluster dot product, and the small entries come from inter-cluster dot product.
We bound the intra-cluster dot product:
z>
ijzik= (ci+rij)>(ci+rik)
=kcik2+c>
irij+c>
irik+r>
ijrik:
Similar to the argument above, by concentration of measure, kcik22[1 +;1 ]with high probability
(we will pick =()). The cross terms c>
irijandc>
irikcan be bounded using Cauchy-Schwarz inequality
to be in [ ;]with high probability. And the fourth term r>
ijrikis in [ 2;2]with high probability.
Therefore, the inner product is in 1O()with high probability. This satisÔ¨Åes the Ô¨Årst condition in Matrix
Class 1, for l=1
1 2, assuming .
We use a similar argument to bound the inter-cluster dot product. For i6=i0
z>
ijzi0k= (ci+rij)>(ci0+ri0k)
=c>
ic>
i0+c>
iri0k+c>
i0rij+r>
ijri0k:
By concentration of measure, c>
ici02[ ;]. Similar to the argument in the intra-cluster case, we can bound
the other three terms, so this dot product is in [ O();O()]. This satisÔ¨Åes the second condition in Matrix
Class 1.
To prove Theorem 1 for Matrix Class 1, we start with some technical lemmas.
Lemma 6. LetF2RNN
0be a symmetric matrix. Let maxbe the largest eigenvalue of F. Assuming
N2, we have that
maxmin
i6=jF[i;j]:
Proof.SinceFis symmetric, maxis real and
max= max
u6=0u>Fu
uTu:
Letube the all 1‚Äôs vector, then
max1
NX
i=jF[i;j]
1
NX
i6=jF[i;j]
1
NN(N 1) min
i6=jF[i;j]
min
i6=jF[i;j];
where the second step follows from all the diagonal entries are non-negative, the last step follows from
N2
The above implies the following result:
Corollary 7. LetF2RNN
0be a block diagonal matrix. Let rbe the number of mmblocks inFfor
somem2. Ther(F)is at least the smallest non-diagonal element in any mmblock (m2) inF.
Proof.By Lemma 6, each mmblockB(m2) by itself has max eigenvalue at least mini6=j2[m]B[i;j].
The claim then follows from the fact that any eigenvalue of Bis also an eigenvalue of F.
27

--- PAGE 28 ---
We‚Äôll need the following function for our low-rank argument:
fk(x) =kX
i=0xi
i!:
Note thatf1(x) =ex.
DeÔ¨Ånition 1. Let2(0;1=10)andL>0. We say a function f:R!Ris(;L)-close toeyif
jey f(y)jfor anyy2[ L;L]:
Lemma 8. For any2(0;1=10)andL>0. If
D10(L+ log(1=))
then function fD(y)is(;L)-close toey.
Proof.Recall the deÔ¨Ånition of function fD,
ex=fD(x) +1X
i=D+1xi
i!;
It is suÔ¨Écient to show that jey f(y)j<if we have
xD+1
(D+ 1)!
2;
We can show that
yD
D!LD
D!
LD
(D=4)D
= (4L
D)D
(1=2)D
=10
where the Ô¨Årst step follows from jyjL, the second step follows n!(n=4)n, the forth step follows from
D10L, the last step follows D10 log(1=)and2(0;1=10).
We‚Äôll also use the following fact:
Lemma 9. For anyD=o(logn=logd), we have
rank(fD)no(1):
Proof.We can upper bound rank(fD(A))in the following sense:
rank(fD(A))(rank(A))D
dD
= 2Dlogd
= 2o(logn)
=no(1):
where the second step follows from rank(A)d, the forth step follows from D=o(logn
logd).
Finally we‚Äôre ready to prove the theorem:
Proof.The basic idea is: (i) Use fk(bA)to get the low-rank approximation (ii) Use exp(bH)to get the
sparse part.
28

--- PAGE 29 ---
Smallrange, i.e.,iso
logn
logd
.
Low rank approximation: R=fk(bA).
Since each entry of Ais in [ 1;1], each entry of Ais in [ ;]. But note that in this case is
o
logn
d
=O(logn). By the deÔ¨Ånition of k, each entry of exp(A) fk(A)has absolute value .
Therefore the overall error is n.
For sparse only: By assumption, m= 
(kLk0)entries inAare0, which are exactly the entries in
exp(A)that are1. Hence any (say)m
2sparse approximation has error pm
2
(p
kLk0). By our
assumption,kLk0= 
(n2).
Mid-range ,i.e.,1
llognandisO(logn).
Sparse only: the argument is the same as in the low range.
Sparse + low-rank: The low-rank part R=fst(A). By Lemma 9, this has rank no(1), so it hasn(1+o(1))
parameters.
The sparse part is S=eH Rsupp(H). Clearly this needs jsupp(H)jparameters.
LetE=M (S+R). Then (i) in supp(H),Eis all 0. (ii) output of supp(H), by deÔ¨Ånition, entries of
Aare in [ ;], which in the current range of is[ O(logn);O(logn)]. Therefore all the entries
ofEhave absolute value . By the deÔ¨Ånition of k, we have thatkEkFn.
Low-rank only: Let eRbe rankr no(1) 1that approximates M. Then using the same argument as our
existing lower bound argument, we get that eR RES(this means that the error kEkF+M eR
F).
Now note that S=eH (fk(A))suppHis a symmetric, block diagonal matrix with r= 
(n)blocks.
Corollary 7 implies that r(S)is at least the smallest non-diagonal value in S. Now the smallest non-diagonal
value ineHise1
llogn=n. On the other hand, the largest value in (fk(A))suppHis
kk
k!e
k 1k 1
.lognelogn
lognO(logn)
.logneO(lognlog1
)
.lognno(1)
=no(1):
Hencer(S)is
(n). The claimed result then follows since kEkFnandrankeR Rr 1(Eckart-
Young-Mirsky theorem).
Largerange, i.e.,!(logn).
Sparse only: S=eH. Note that each entry in E=M Sis upper bounded by eeo(
logd). Then
kEkFneo(
logd)
elogn
+o(
logd)
eo()+o(
logd)
eo()
e=l:
Low-rank only: since kEkFise=l, it is enough to argue that any rank r-approximation to Shas error
e=l. But the latter follows since r(S)e=l. This is because ebHis symmetric and each entry in His
1
. Then we can use Corollary 7. Eckart-Young-Mirsky then completes the proof.
29

--- PAGE 30 ---
D.3 Scatterbrain: Analysis
Here we prove Theorem 2, which shows that Scatterbrain approximation is unbiased and analyses its variance.
We restate the theorem here for the reader‚Äôs convenience.
Theorem. DeÔ¨Åne(q;k) =exp(q>k),bpfeas Performer‚Äôs estimator and bsbeas Scatterbrain estimator.
DenoteSd 1Rdas the unit sphere. Suppose q;k2Sd 1are such thatkq kk<. Then:
E[bsbe(q;k)] =(q;k);Var[bsbe(q;k)] = (1 p)Var[bpfe(q;k)]<Var[bpfe(q;k)]
wherep= exp( 2
4 2lnd O(ln lnd)).
Proof.LetAij=exp(q>
kkj)beij-entry of the unnormalized attention matrix, Alr
ij=(qi)>(kj)the entry
of the low-rank approximation (Performer), and let Asb
ijbe the entry of the Scatterbrain (sparse + low-rank)
approximation. By the construction of the Scatterbrain attention matrix (Eq. (1)), if ij2S, whereSis the
set of indices selected by the LSH, then:
Asb
ij= (eQeK>+S)ij=(qi)>(kj) + exp(q>
ikj) (qi)>(kj) = exp(q>
ikj):
Ifij =2S, then
Asb
ij= (eQeK>+S)ij=(qi)>(kj) + 0 =(qi)>(kj):
In other words, AsbmatchesAon the indices in S, and matches Alron the indices not in S.
To show that Asbis an unbiased estimator of A, we simply use the fact that Alris also an unbiased
estimator of A[17, Lemma 1]:
E[Asb
ij] =P(ij2S)E[Aijjij2S] +P(ij =2S)E[Alr
ijjij =2S]
=P(ij2S)Aij+P(ij =2S)Aij
=Aij:
In other words, E[bsbe(q;k)] =(q;k).
Now we analyze the per-entry variance of Asb. SinceAsbis an unbiased estimator of A, by the law of
total variance,
Var (Asb
ij) =P(ij2S)Var (Aijjij2S) +P(ij =2S)Var (Alr
ijjij =2S)
=P(ij2S)0 +P(ij =2S)Var (Alr
ij)
=P(ij =2S)Var (Alr
ij):
To compute the probability that the index ijis not inS(i.e., not selected by LSH), we use the standard
bound on cross-polytope LSH [3, Theorem 1]:
p:=P(ij2S) = exp( 2
4 2lnd O(ln lnd)):
Therefore,
Var (Asb
ij) = (1 p)Var (Alr
ij)<Var (Alr
ij):
In other words, Var [bsbe(q;k)] = (1 p)Var [bpfe(q;k)]<Var [bpfe(q;k)].
More explicitly, by plugging in the variance of Alr[17, Lemma 2], we have
Var (Asb
ij) = (1 p)1
mexp
kqi+kjk2
exp(2q>
ikj)
1 exp
 kqi+kjk2
;
wherep= exp( 2
4 2lnd O(ln lnd))
30

--- PAGE 31 ---
E Additional Experiments and Details
E.1 Datasets
ImageNet[25]: ImageNetisoneofthemostwidely-usedimageclassiÔ¨Åcationbenchmarks. Inourexperiments
in Section 5.1 of evaluating the approximation accuracy of Scatterbrain, both BigGAN and Vision Transformer
are pre-trained on this dataset. It has roughly 1.2 million training images and 50,000 validation images.
WikiText103 [45] and Copy [36]: WikiText103 is a popular dataset for auto-regressive models. It is
from a collection of over 100 million tokens extracted from the set of veriÔ¨Åed good and featured articles on
Wikipedia. It has 28,475 training articles, 60 for validation and 60 for testing.
Copy is a synthetic a synthetic sequence duplication task where inputs are of the form 0w0wand
w2f0;:::;Ng. It is previously used in [ 15,36]. This task is useful for demonstrating the eÔ¨Äectiveness of
long range attention: it requires non-local attention lookups. It cannot be solved by any model relying on
sparse attention with a limited range such as, local attention.
Long Range Arena (LRA) [57]: This is a recent benchmark for evaluating eÔ¨Écient transformers
with long input sequence. We used ListOps [ 46], byte-level IMDb reviews text classiÔ¨Åcation [ 44], byte-level
document retrieval [ 48], image classiÔ¨Åcation on sequences of pixels [ 37] and PathÔ¨Ånder [ 40]. We follow the
same evaluation mechanism from [57] but implement our own version in Pytorch (like data loader).
GlUE [64]: GLUE is a standard multi-task benchmark in NLP. It has single-sentence tasks, CoLA and
SST-2; similarity and paraphrase tasks, MRPC, STS-B, QQP; and inference tasks, MNLI, QNLI, RTE and
WNLI. For our additional experiments below (not enough space to be included in the main paper), we follow
the tradition from [22, 26, 68] and truncate all the input sequences to 128 tokens.
E.2 Settings
BigGAN: We adapt the same pre-trained BigGAN model from [ 22] with no additional training. The model
has a single attention layer at resolution 6464(4096). Similar to the prior work, we also replace its full
attention layer with Scatterbrain at the same resolution. Figure 5 in the main paper shows the best-eÔ¨Äort
comparison with [1/32, 1/16, 1/8, 1/4, 1/2] of the parameter budget. For example, if given parameter budget
1/2, we report the best performance of Smyrf from choice of 32/64/128 hash round 64/32/16 cluster size.
T2-ViT: We use the pre-trained vision transformer model T2T-ViT-14 from [ 69] with 224224image
size. Without any additional training, we just replace the attention layer with Scatterbrain and other baselines
and evaluate the approximation error and classiÔ¨Åcation accuracy on ImageNet testings. Again, we report the
best-eÔ¨Äort best performance of each approximation given the certain parameter budget.
Auto-regressive Model: We follow the settings from the popular repo https://github.com/NVIDIA/
DeepLearningExamples for training vanilla Transformer from scratch on WikiText103, except for chunking
WikiText103 into sequence length 1024 in order to simulate long input sequences. The model is 16 layer
with 8 head and 512 model dimension. We train all the models for 30 epochs and report the best Testing
Perplexity. The model we use for Copy task is simply a 2-layer-4-head transformer and sequence length is
also 1024. We make 5 runs and report average. Table 4 presents the results with standard deviation.
ClassiÔ¨Åcation Model: We follow the model setting from [ 57,67]. We share the same Ô¨Ånding with [ 67]
that the acuracy for the Retrieval tasks is actually higher than reported in [57].
Ratio between Sparse and Low-rank components: There are some rules that we used in our
experiments to set this ratio. For inference, we set this ratio based on the entropy of an observed subset
of attention matrices in diÔ¨Äerent layers: we allocate more memory to the low-rank component compared
to the sparse component if the entropy is high. For training, generally allocating more memory budget to
sparse tends to perform better, so in the experiment, we set the ratio to 3:1 (sparse: low-rank component)
for simplicity. Moreover, in future work, it could be useful to make this ratio adaptive during training. For
example, in the early stage of the training and early layers, attention matrices are usually more uniform
(higher entropy). Thus, the approximation error could be even lower if the ratio favors low-rank-based
components. One approach could be to monitor the approximation error of sparse and low-rank components
compared to full attention regularly and adjust the memory budget accordingly. We will add the above
discussion to the updated manuscript.
31

--- PAGE 32 ---
Table 4: The performance of Scatterbrain, reformer ,performer and Full-Attention on Long-Range-Arena
benchmarks and 2 popular language modeling tasks. We Ô¨Åx the same number of parameters ( 1=8of the full)
used for approximating the attention matrix for each method.
Attention Copy (ppl) WikiText-103 (ppl)
Full Attention 1 25.2580.37
Reformer 6.80.64 27.680.53
Performer 492.7 665.8
Scatterbrain 2.580.21 26.720.44Attention ListOps Text Retrieval Image PathÔ¨Ånder Avg
Full Attention 38.20.1763.290.3880.850.1241.780.2673.980.3159.62
Reformer 36.850.3758.120.4278.360.2928.30.3967.950.2853.9
Performer 35.750.2962.360.4978.830.3339.710.4868.60.3657.05
Scatterbrain 38.60.22 64.550.34 80.220.31 43.650.46 69.910.25 59.38
0 2 4 6
Entropy0.00.20.40.60.81.0Approx. ErrorReformer
Performer
Scatterbrain
0 2 4 6
Entropy0.00.20.40.60.81.0Approx. ErrorReformer
Performer
Scatterbrain
4 5 6 7
Entropy0.00.20.40.60.81.0Approx. ErrorReformer
Performer
Scatterbrain
5.0 5.5 6.0 6.5
Entropy0.000.050.100.150.200.250.300.350.40Approx. ErrorReformer
Performer
Scatterbrain
Figure 8: Top two plots present Approximation Error vs. Entropy of attention matrices for reformer ,performer
and Scatterbrain on Copy (left) and WikiText103 (right). Bottom two plots present Approximation Error vs. Entropy
of attention matrices for reformer ,performer and Scatterbrain on Text-IMDb (left) and Image-Cifar10 (right).
Recall we observe that entropy of the softmax attention distribution (i.e., scale of logits) determines the regimes
where sparse, low-rank, or sparse + low-rank perform well. Scatterbrain yields better approximation than reformer
orperformer in most of the cases; performer performs the worst on language modeling tasks while reformer
performs the worst on classiÔ¨Åcation tasks. These plots for approximation error analysis match with their performance
on downstream tasks.
32

--- PAGE 33 ---
E.3 More Ablation Studies
E.3.1 Memory Budget
We present an ablation study on the parameter budget for the WikiText-103 language modeling task. We
show that Scatterbrain outperforms its sparse and low-rank baselines across a range of parameter budgets.
The results are presented in Table 5.
Analysis: We have observed that Scatterbrain outperforms its sparse and low-rank baselines under
diÔ¨Äerent memory budgets. Similar to what we found in Section 5.2, Performer does not train stably even
with1
4of the full attention memory. However, under the Scatterbrain framework, Performer can be combined
with Reformer in an elegant way to achieve the same accuracy while using only half of the memory and faster
than Reformer by exploiting the sparse+low-rank structure in attention matrices.
Table 5: We run WikiText-103 LM with a sweep of 1/4, 1/8, 1/16 memory budget. We show the validation perplexity
and speed-up with respect to the full attention with diÔ¨Äerent eÔ¨Écient Attention layers.
1
4Mem1
8Mem1
16Mem
Perplexity (Speed-up) Perplexity Perplexity
Smyrf 26.76 (1.6) 27.68 (1.39) 28.7(1.85)
Performer 58(2.13) 66 (2.01) 85(1.77)
Scatterbrain 26.26(1.58) 26.72 (1.87) 27.74(2.03)
E.3.2 DiÔ¨Äerent Sparse and Low-rank baselines
Scatterbrain is general enough to accommodate diÔ¨Äerent kinds of sparse and low-rank approximations as its
sub-components. In particular, we can combine Local attention or block sparse (from Sparse Transformer
and BigBird) + Performer (instead of Reformer + Performer) in a similar fashion. The support of the sparse
matrix S will thus be Ô¨Åxed and not adaptive to input, but all the other steps are exactly the same.
We have run additional experiments on the Local attention + Performer combination and BigBird. Recall
that in Appendix E, we have shown Scatterbrain can reduce the attention memory of Vision Transformer
by 98% at the cost of only 0.8% drop of accuracy when serving as a drop-in replacement for full attention
without training on ImageNet. We show the results for local+performer variation with the same memory
budget in Table 6.
We have also run additional experiments on Local attention on Copy and Wikitext-103 language modeling
task ( Table 7). We see that Local attention is reasonably competitive on Wikitext-103 but does not perform
well on Copy. The results are not surprising as noted in the Reformer paper that Copy requires non-local
attention lookups.
E.3.3 DiÔ¨Äerent Sparse and Low-rank baselines
E.4 Analysis
RecallinSection5, wehavereportedtheanalysisaftervisualizingtheerrorof reformer (sparse), performer
(low-rank), and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating
the full attention matrices for each attention layer during training. In Figure 8, we present the visualization.
Table 6: Top-1 Accuracy of pre-trained T2T Vision Transformer on ImageNet with diÔ¨Äerent attention replacements.
Error represents the average normalized approximation error to full attention.
Attention Top-1 Acc
Full Attention 81:7%
SMYRF 79:8%
Local 79:6%
Performer 80:1%
BigBird 80:3%
Scatterbrain (Local + Performer) 80:3%
Scatterbrain (SMYRF + Performer) 80.7%
33

--- PAGE 34 ---
Table 7: Additional experiments for Local attention on the Copy and Wikitext-103 language modeling task.
Attention Copy (ppl) WikiText-103 (ppl)
Full Attention 1 25.258
Reformer 6.8 27.68
Performer 49 66
Local 53 30.72
Scatterbrain 2.58 26.72
The conclusion for language modeling tasks is that sparse+low-rank has the smallest approximation error
in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also
conÔ¨Årms the observation in the popular benchmark paper [ 57] that kernel or low-rank based approximations
are less eÔ¨Äective for hierarchical structured data. For classiÔ¨Åcation tasks, we again Ô¨Ånd that Scatterbrain has
the smallest approximation error, while performer is the worst on ListOps and reformer has the largest
error on classiÔ¨Åcation tasks, which matches with the end-to-end results and conÔ¨Årms our observations earlier
(sparse and low-rank approximation excel in diÔ¨Äerent regimes).
E.5 Additional Experiments of Fine-tuning Bert on GLUE
We provide additional experiments of Ô¨Åne-tuning Bert on GLUE in Table 8. We follow the similar setting
as [22]. We replace all the attention layers in Bert base model with Scatterbrain and other baselines. Then we
Ô¨Åne-tune Bert on 9 downstream tasks for 3 epochs with batch size 32 and learning rate 3e-5. The parameter
budget is 1/2 of the full attention because sequence length 128 is not very long. We can see Scatterbrain
outperforms all the other baselines in most of the downstream tasks.
Table 8: Results of GLUE when replacing dense attention matrices with smyrf,performer and Scatterbrain in
BERT base model. We Ô¨Åx the same number ofparameters (1/2 of the full) used for approximating the attention
matrix for each method.
CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI
mcc acc acc corr acc acc acc acc acc
Full 0.576 0.934 0.874 0.879 0.905 0.813 0.916 0.668 0.43
Smyrf 0.538 0.912 0.833 0.856 0.898 0.775 0.879 0.626 0.412
Performer 0.508 0.838 0.782 0.203 0.831 0.563 0.763 0.556 0.449
Scatterbrain 0.569 0.927 0.863 0.867 0.902 0.813 0.893 0.619 0.428
F Further Discussions and Future Work
In this paper, we present Scatterbrain, unifying the strength of sparse and low-rank approximation. It is
inspired by the observations on the attention matrix structures induced by the data and softmax function as
well as the classical robust-PCA algorithm. In our implementation and analysis, we have reformer /Smyrf
andperformer as the back-bone for sparse and low-rank approximations because of their properties, e.g.
Performer is unbiased. Scatterbrain is fundamentally a framework for combining the strength of sparse
and low-rank variants, so it can be easily extended to other variants, such as Routing Transformer [ 53] or
Nystromformer [ 67]. Further more, our observations on the connection between entropy and low-rank/sparse
approximation error also provide an opportunity for eÔ¨Éciently detecting the approximation or compression
method to choose for diÔ¨Äerent architectures or benchmarks.
34
