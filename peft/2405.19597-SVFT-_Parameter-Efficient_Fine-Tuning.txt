# 2405.19597.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2405.19597.pdf
# File size: 942627 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SVFT: Parameter-Efficient Fine-Tuning
with Singular Vectors
Vijay Lingam†∗Atula Tejaswi†∗Aditya Vavre†∗Aneesh Shetty†∗
Gautham Krishna Gudur†∗Joydeep Ghosh†Alex Dimakis†Eunsol Choi†
Aleksandar Bojchevski‡Sujay Sanghavi†
†University of Texas at Austin‡University of Cologne
Abstract
Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its
variants, freeze pre-trained model weights Wand inject learnable matrices ∆W .
These ∆W matrices are structured for efficient parameterization, often using
techniques like low-rank approximations or scaling vectors. However, these
methods typically show a performance gap compared to full fine-tuning. Although
recent PEFT methods have narrowed this gap, they do so at the cost of additional
learnable parameters. We propose SVFT , asimple approach that fundamentally
differs from existing methods: the structure imposed on ∆W depends on the
specific weight matrix W. Specifically, SVFT updates Was a sparse combination
of outer products of its singular vectors, training only the coefficients (scales)
of these sparse combinations. This approach allows fine-grained control over
expressivity through the number of coefficients. Extensive experiments on
language and vision benchmarks show that SVFT2recovers up to 96% of full
fine-tuning performance while training only 0.006 to 0.25 % of parameters,
outperforming existing methods that only recover up to 85% performance using
0.03 to 0.8% of the trainable parameter budget.
1 Introduction
Large-scale foundation models are often adapted for specific downstream tasks after pre-training.
Parameter-efficient fine-tuning (PEFT) facilitates this adaptation efficiently by learning a minimal set
of new parameters, thus creating an "expert" model. For instance, Large Language Models (LLMs)
pre-trained on vast training corpora are fine-tuned for specialized tasks such as text summarization [ 12,
34], sentiment analysis [ 25,20], and code completion [ 26] using instruction fine-tuning datasets.
Although full fine-tuning (Full-FT) is a viable method to achieve this, it requires re-training and
storing all model weights, making it impractical for deployment with large foundation models.
To address these challenges, PEFT techniques [ 13] (e.g., LoRA [ 14]) were introduced to significantly
reduce the number of learnable parameters compared to Full-FT, though often at the cost of perfor-
mance. DoRA [ 18] bridges this performance gap by adding more learnable parameters and being
more expressive than LoRA. Almost all these methods apply a low-rank update additively to the
frozen pre-trained weights, potentially limiting their expressivity. Furthermore, these adapters are
agnostic to the structure and geometry of the weight matrices they modify. Finally, more expressive
PEFT methods (e.g., LoRA, DoRA, BOFT [ 19]) still accumulate a considerable portion of learnable
parameters even in their most efficient configuration (e.g., setting rank=1 in LoRA and DoRA). The
∗indicates equal contribution.
2code is available at https://github.com/VijayLingam95/SVFT/
Preprint. Under review.arXiv:2405.19597v1  [cs.LG]  30 May 2024

--- PAGE 2 ---
0.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35
Number of Trainable Params (M)32.535.037.540.042.545.047.550.052.555.0
SVFTPSVFTB
d=2SVFTB
d=4SVFTB
d=8SVFTB
d=16SVFTR
d=16
LoRAr=1DoRAr=1LoRAr=32
VeRAr=1024VeRAr=2048
BOFTm=2
b=8DoRAr=16
DoRAr=4
LoRAr=4Full Fine-Tuning (2500M params)
0.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35
Number of Trainable Params (M)50.052.555.057.560.062.565.067.570.0
SVFTPSVFTB
d=2SVFTB
d=4SVFTB
d=8SVFTB
d=16
DoRAr=16
DoRAr=4LoRAr=32
LoRAr=1DoRAr=1
VeRAr=2048
BOFTm=2
b=8Full Fine-Tuning (2500M params)Accuracy (%)Figure 1: Performance vs total trainable parameters for GSM-8K (left) and Commonsense Reasoning
(right) on Gemma-2B. SVFTB/R
d=16outperforms DoRA r=8/16with 75% less trainable parameters.
storage requirements for the learnable adapters can grow very quickly when adapting to a large
number of downstream tasks [16].
Is it possible to narrow the performance gap between SVFT and Full-FT while being highly parameter-
efficient? We propose SVFT : Singular Vectors guided Fine-Tuning — a simple approach that involves
updating an existing weight matrix by adding to it a sparse weighted combination of its own singular
vectors . The structure of the induced perturbation in SVFT depends on the specific matrix being per-
turbed, setting it apart from all previous approaches. Our contributions can be summarized as follows:
•We introduce SVFT , a new PEFT method. Given a weight matrix W,SVFT involves adapting
it with a matrix ∆W:=P
(i,j)∈ΩmijuivT
jwhere the {ui}and{vj}are the left and right
singular vectors of W,Ωis an a-priori fixed sparsity pattern, and mijfor(i, j)∈Ωare learnable
parameters. By controlling |Ω|we can efficiently explore the accuracy vs parameters trade-off.
•SVFT achieves higher downstream accuracy, as a function of the number of trainable parameters,
as compared to several popular PEFT methods (see Figure 1) and over several downstream
tasks across both vision and language tasks. Our method recovers up to 96% of full fine-tuning
performance while training only 0.006 to 0.25 % of parameters, outperforming existing methods
that only recover up to 85% performance using 0.03 to 0.8% the trainable parameter budget.
We introduce four variants for parameterizing weight updates, namely: Plain ,Random ,Banded , and
Top-kinSVFT (which differ in their choices of the fixed sparsity pattern Ω) and validate these design
choices empirically. Additionally, we theoretically show that for any fixed parameters budget, SVFT
can induce a higher rank perturbation compared to previous PEFT techniques.
2 Related Work
Recent advancements in large language models (LLMs) have emphasized the development of PEFT
techniques to enhance the adaptability and efficiency of large pre-trained language models.
LoRA. A notable contribution in this field is Low-Rank Adaptation (LoRA) [ 14], which freezes the
weights of pre-trained models and integrates trainable low-rank matrices into each transformer layer.
For a pre-trained weight matrix W0∈Rd×n, LoRA constraints the weight update ∆Wto a low-rank
decomposition: h=W0x+ ∆Wx =W0x+BAx, where B∈Rd×r,A∈Rr×nand rank
r≪min(d, n). We underline the (trainable) parameters that are updated via gradient descent.
LoRA variants. We highlight some recent approaches that further improve the vanilla LoRA
architecture. Vector-based Random Matrix Adaptation (VeRA) [ 16] minimizes the number of
trainable parameters by utilizing a pair of low-rank random matrices shared between layers and
learning compact scaling vectors while maintaining performance comparable to LoRA. Formally,
2

--- PAGE 3 ---
Figure 2: Schematic comparison of LoRA, VeRA, DoRA, and SVFT (left to right).
VeRA can be expressed as: h=W0x+∆Wx =W0x+ΛbBΛdAx, where AandBare initialized
randomly, frozen, and shared across layers, while ΛbandΛdare trainable diagonal matrices.
An alternative approach, Weight-Decomposed Low-Rank Adaptation (DoRA) [ 18], decomposes pre-
trained weight matrices into magnitude and direction components, and applies low-rank updates for
directional updates, reducing trainable parameters and enhancing learning capacity and training sta-
bility. DoRA can be expressed as: h=mW0+∆W
∥W0+∆W∥cx=mW0+BA
∥W0+BA∥cx, where ∥·∥cdenotes the
vector-wise norm of a matrix across each column. Similar to LoRA, W0remains frozen, whereas the
magnitude vector m(initialized to ∥W0∥c) and low-rank matrices A,Bcontain trainable parameters.
AdaLoRA [ 35] adaptively distributes the parameter budget across weight matrices based on their
importance scores and modulates the rank of incremental matrices to manage this allocation
effectively. PiSSA (Principal Singular Values and Singular Vectors Adaptation) [ 21] is another variant
of LoRA, where matrices A,Bare initialized with principal components of SVD and the remaining
components are used to initialize W0. FLoRA [ 31] enhances LoRA by enabling each example in a
mini-batch to utilize distinct low-rank weights, preserving expressive power and facilitating efficient
batching, thereby extending the domain adaptation benefits of LoRA without batching limitations.
Other PEFT variants. Orthogonal Fine-tuning (OFT) [ 24] modifies pre-trained weight matrices
through orthogonal reparameterization to preserve essential information. However, it still requires
a considerable number of trainable parameters due to the high dimensionality of these matrices.
Butterfly Orthogonal Fine-tuning (BOFT) [ 19] extends OFT’s methodology by incorporating Butterfly
factorization thereby positioning OFT as a special case of BOFT. Unlike the additive low-rank weight
updates utilized in LoRA, BOFT applies multiplicative orthogonal weight updates, marking a
significant divergence in the approach but claims to improve parameter efficiency and fine-tuning
flexibility. BOFT can be formally expressed as: h= (R(m, b)·W0)x, where the orthogonal matrix
R(m, b)∈Rd×dis composed of a product of multiple orthogonal butterfly components. When
m= 1, BOFT reduces to block-diagonal OFT with block size b. When m= 1andb=d, BOFT
reduces to the original OFT with an unconstrained full orthogonal matrix.
3 Method
In this section, we introduce Singular Vectors guided Fine-Tuning (SVFT). The main innovation in
SVFT lies in applying structure/geometry-aware weight updates.
3.1 SVFT Formulation
We now formally describe our method, SVFT for parameter-efficient fine-tuning of a pre-trained
model. Let W0∈Rd1×d2denote a weight matrix in the pre-trained model. For instance, in a
transformer block, this could be the key matrix, the query matrix, a matrix in the MLP, etc. We add a
structured, learned ∆Wto this matrix as follows.
As a first step, we compute the Singular Value Decomposition (SVD) of the given matrix: W0=
UΣVT. That is, Uis the d1×d1matrix of left singular vectors (i.e., its columns are orthonormal),
VTis the d2×d2matrix of right singular vectors (i.e., its rows are orthonormal), and Σis ad1×d2
diagonal matrix. Then, we parameterize our weight update as ∆W=UMVT, where U,Vare
3

--- PAGE 4 ---
Figure 3: An Overview of SVFT . The original weights Ware decomposed into U,Σ,V. Here, M
contains all the trainable parameters, which can be configured into patterns such as Plain, Random,
Banded, and Top- k, represented by patterns of trainable (orange) and zero (gray) elements.
fixed and frozen, while Mis ad1×d2sparse trainable matrix with pre-determined and fixed
sparsity pattern3. That is, we first pre-determine a small fixed set of elements in Mthat will be
allowed to be non-zero and train only those elements. The forward pass for SVFT can be written as,
h=W0x+ ∆Wx=U(Σ+M)VTx (1)
We explore four choices for Ω, the a-priori fixed sparsity pattern of M.
Plain 
SVFTP
. In this variant, we constrain Mto be a diagonal matrix, which can be interpreted
as adapting singular values and reweighting the frozen singular vectors. Since only the diagonal
elements are learned, this is the most parameter-efficient SVFT variant.
Banded 
SVFTB
d
. In this approach, we populate Musing a banded matrix, progressively making
off-diagonals learnable. Specifically, for constants z1andz2,Mij= 0ifj < i−z1orj > i +z2,
where z1, z2≥0. In our experiments, we set z1=z2=dto induce off-diagonal elements that
capture additional interactions beyond those represented by singular values. This banded perturbation
induces local interactions, allowing specific singular values to interact with their immediate neighbors,
ensuring smoother transitions. This method, although deviating from the canonical form of SVD,
provides a mechanism to capture localized interactions.
Random 
SVFTR
d
. A straightforward heuristic for populating Minvolves randomly selecting
kelements to be learnable.
Top-k 
SVFTT
d
. The final design choice we explore involves computing the alignment between
the left and right singular vectors as uT
ivj. We then select the top- kelements and make them
learnable. However, note that this only works when left and right singular vectors have the same
size. A possible interpretation of this is we make only the top- kstrong interactions between singular
vector directions learnable.
We illustrate these SVFT design choices in Figure 3. Our empirical results demonstrate that these
simple design choices significantly enhance performance compared to state-of-the-art PEFT methods.
Note that SVFTPhas a fixed number of learnable parameters, while the remaining variants are
configurable. We hypothesize that further innovation is likely achievable through optimizing the
sparsity pattern of M, including efficient learned-sparsity methods. In this paper, we explore these
four choices to validate the overall idea: determining a perturbation using the singular vectors of the
matrix that is being perturbed.
3.2 Properties of SVFT
We highlight some properties of SVFT in the following lemma and provide insights into how its
specific algebraic structure compares and contrasts with baseline PEFT methods.
Lemma: LetW0be a matrix of size d1×d2with SVD given by UΣVT. Consider an updated
final matrix W0+UMVT, where Mis a matrix of the same size as Σ, which may or may not be
diagonal. Then, the following holds:
3Learnable parameters are underlined.
4

--- PAGE 5 ---
(a)Structure: IfMis also diagonal (i.e. the plain SVFT ), then the final matrix W0+UMVT
hasUas its left singular vectors and sign(Σ+M)VTas its right singular vectors. That
is, its singular vectors are unchanged, except for possible sign flips. Conversely, if Mis
notdiagonal (i.e., variants of SVFT other than plain), then UandVmay no longer be the
singular directions of the final matrix W0+UMVT.
(b)Expressivity: Given anytarget matrix Pof size d1×d2, there exists an Msuch that
P=W0+UMVT. That is, if Mis fully trainable, any target matrix can be realized
using this method.
(c)Rank: IfMhasknon-zero elements, then the rank of the update UMVTis at most
min{k,min{d1, d2}}. For the same number of trainable parameters, SVFT can produce
a much higher rank perturbation than LoRA (eventually becoming full rank), but in a
constrained structured subspace.
We provide our proofs in Appendix A. Building on this lemma, we now compare the form of the
SVFT update with LoRA and VeRA.
SVFT’s ∆Wcan be written as a sum of rank-one matrices:
∆W=X
(i,j)∈ΩmijuivT
j (2)
where uiis the ithleft singular vector, vjis the jthright singular vector, and Ωis the set of non-zero
elements in M.
Thus, our method involves adding a weighted combination of specific rank-one perturbations of the
formuivT
j.
LoRA and VeRA updates can also be expressed as sums of rank-one matrices.
∆WLoRA =rX
i=1aibiTand ∆WVeRA =rX
i=1αi(ˆai⊙β)ˆbT
i (3)
where aiandbjare the trainable columns of AandBmatrices in LoRA. In VeRA, ˆaiandˆbiare
random and fixed vectors, while αandβrepresent the diagonal elements of ΛdandΛbrespectively.
Note that LoRA requires d1+d2trainable parameters per rank-one matrix, while SVFT and VeRA
require only one. Although LoRA can potentially capture directions different from those achievable
by the fixed {ui,vT
j}pairs, each of these directions incurs a significantly higher parameter cost.
VeRA captures new directions at a parameter cost similar to SVFT ; however, there is a key distinction:
in VeRA, each vector ˆaiorˆbiappears in only one of the rank-one matrices. In contrast, in SVFT ,
the same vector uican appear in multiple terms in the summation, depending on the sparsity pattern
ofM. This results in an important difference: unlike SVFT , VeRA is not universally expressive – it
cannot represent any target matrix P. Moreover, ˆai,ˆbiare random, while ui,vjdepend on W0.
Note. SVFT requires storing both left and right singular vectors due to its computation of the SVD
on pre-trained weights. While this increases memory usage compared to LoRA (which is roughly
double), it remains lower than BOFT. We partially address this through system-level optimizations
like mixed-precision weights (e.g., bfloat16). Further exploration of memory-reduction techniques,
such as quantization, is planned as future work. Importantly, inference time and memory consumption
remain the same across all methods, including SVFT, as the weights can be fused.
4 Experiments
4.1 Base Models
We adapt widely-used language models, encoder-only model (DeBERTaV3 base[10]) and two decoder-
only models (Gemma-2B/7B [ 29], LLaMA-3-8B [ 1]). We also experiment with vision transformer
models (ViT-B/16 and ViT-L/16) [ 9]) pre-trained on ImageNet-21k [ 8], following prior work [ 16].
5

--- PAGE 6 ---
The complete details of our experimental setup and hyperparameter configurations are provided in
Appendix C.
Baselines. We compare with Full Fine-Tuning (FT) updating all learnable parameters in all layers,
along with LoRA [14], DoRA [18], BOFT [19] and VeRA [16].4
4.2 Datasets
Language. For natural language generation (NLG) tasks, we evaluate on GSM-8K [ 7] and
MATH [ 11] by fine-tuning on MetaMathQA-40K [ 32], following [ 19]. We also evaluate on 8
commonsense reasoning benchmarks (BoolQ [ 5], PIQA [ 3], SIQA [ 28], HellaSwag [ 33], Wino-
grande [ 27], ARC-easy/challenge [ 6], and OpenBookQA [ 22]). We follow the setting outlined in
prior work [ 18,15], where the training sets of all benchmarks are amalgamated for fine-tuning.
We fine-tune on 15K examples from this training set. For natural language understanding (NLU),
we evaluate on the General Language Understanding Evaluation (GLUE) benchmark consisting of
classification and regression tasks, in line with [16, 14].
Vision. Our experiments on vision tasks consist of 4 benchmarks: CIFAR-100 [ 17], Food101 [ 4],
RESISC45 [ 30], and Flowers102 [ 23]. We follow the setup from [ 16], and fine-tune on a subset
comprising 10 samples from each class.
Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params
denote the number of trainable parameters. bold andunderline represent best and second best
performing PEFT method, respectively. SVFT offers superior/competitive performance at much
lower #Params. For SVFTR
d, we set d= 16 for Gemma and d= 12 for LLaMA-3 models.
MethodGemma-2B Gemma-7B LLaMA-3-8B
#Params GSM-8K MATH #Params GSM-8K MATH #Params GSM-8K MATH
Full-FT 2.5B 52.69 17.94 8.5B 74.67 25.70 8.0B 64.13 16.24
LoRA r=32 26.2M 43.06 15.50 68.8M 76.57 29.34 56.6M 75.89 24.74
DoRA r=16 13.5M 44.27 16.18 35.5M 74.52 29.84 29.1M 75.66 24.72
BOFTb=8
m=2 1.22M 36.01 12.13 2.90M 71.79 28.98 4.35M 67.09 21.64
DoRA r=1 1.19M 35.25 13.04 3.26M 74.37 26.28 2.55M 68.30 21.96
LoRA r=1 0.82M 32.97 13.04 0.82M 72.4 26.28 1.77M 68.84 20.94
VeRA r=1024 0.63M 36.77 14.12 0.43M 71.11 27.04 0.98M 63.76 20.28
SVFTP0.19M 40.34 14.38 0.43M 73.50 27.30 0.48M 69.22 20.44
SVFTR
d 6.35M 50.03 15.56 19.8M 76.81 29.98 13.1M 75.90 24.22
5 Results
5.1 Performance on Language Tasks
Natural Language Generation. We present results on mathematical question answering against
baseline PEFT techniques across three base models – varying from 2B to 8B parameters in Table 1.
To ensure a comprehensive comparison, we test baseline techniques (LoRA, DoRA) with different
configurations, and varying hyper-parameters like rank to cover a range of learnable parameters
from low to high. Note that even when the rank is as low as 1, both methods yield more trainable
parameters than SVFTP.SVFTP(∼0.2M) shows as much as 18% relative improvement over
techniques that use 6 ×more trainable parameters ( BOFTb=8
m=2,LoRA r=1). Against techniques of
comparable sizes (VeRA), SVFTPachieves 15.5% relative improvement on average. Even in the
default regime, SVFTR
dmatches techniques with at least 3×more trainable parameters. Notably,
4BOFT is approximately three times slower than LoRA. The shared matrices in VERA can become a limiting
factor for models with non-uniform internal dimensions, such as LLaMA-3.
6

--- PAGE 7 ---
Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We
follow [ 18] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote
HellaSwag [ 33] and WinoGrande [ 27], respectively. SVFTPoffers competitive performance at a
fraction of #Params. SVFTB
d=8can match LoRA r=32with∼7x fewer parameters.
Method #Params BoolQ PIQA SIQA HS WG ARC-e ARC-c OBQA Average
Full-FT 8.5B 72.32 87.32 76.86 91.07 81.76 92.46 82.76 89.00 84.19
LoRA r=32 68.8M 71.55 87.95 77.27 91.80 79.71 92.67 82.16 86.40 83.69
DoRA r=16 35.5M 71.46 87.59 76.35 92.11 78.29 92.00 80.63 85.60 83.00
DoRA r=1 3.31M 68.22 86.72 75.23 91.14 78.13 91.87 83.19 86.20 82.59
VeRA r=2048 1.49M 64.25 86.28 74.04 86.96 69.00 92.76 82.33 82.00 79.70
LoRA r=1 0.82M 65.44 86.28 75.02 89.91 75.92 91.79 81.91 85.40 81.46
SVFTP0.51M 67.92 86.45 75.47 86.92 74.03 91.80 81.23 83.00 80.85
SVFTB
d=8 9.80M 71.90 86.98 76.28 91.55 78.76 92.80 83.11 85.40 83.35
Table 3: DeBERTaV3 basewith different adaptation methods on the GLUE benchmark. We report
matched accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and
accuracy for other tasks. Higher is better for all tasks. * indicates numbers published in prior work.
Method #Params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.
Full-FT* 184M 89.90 95.63 89.46 69.19 94.03 92.40 83.75 91.60 88.25
LoRA* r=8 1.33M 90.65 94.95 89.95 69.82 93.87 91.99 85.20 91.60 88.50
DoRA r=4 0.75M 89.92 95.41 89.10 69.37 94.14 91.53 87.00 91.80 88.53
BOFT*b=8
m=2 0.75M 90.25 96.44 92.40 72.95 94.23 92.10 88.81 91.92 89.89
LoRA r=1 0.17M 90.12 95.64 86.43 69.13 94.18 91.43 87.36 91.52 88.23
VeRA r=1024 0.09M 89.93 95.53 87.94 69.06 93.24 90.4 87.00 88.71 87.73
SVFTP0.06M 89.69 95.41 88.77 70.95 94.27 90.16 87.24 91.80 88.54
SVFTR
d=2 0.28M 89.97 95.99 88.99 72.61 93.90 91.50 88.09 91.73 89.10
on GSM-8K, SVFTR
dagain achieves 96% of the full fine-tuning performance, while DoRA r=16
recovers 86% with 2×more parameters than SVFTR
d.
Commonsense Reasoning. In Table 2, we compare performance on commonsense reasoning
benchmarks with Gemma-7B, and observe similar trends. In the lower and moderately parameter-
ized regime ( ∼0.43M), SVFTPshows competitive performance in comparison to LORAr=1and
DoRA r=1, which have 1.9 ×and 7.7 ×more parameters, respectively. Against VeRA, which trains
3.5×more parameters, SVFTPshows a relative improvement of ∼1.16%. Similarly, SVFTB
d=8also
matches or exceeds methods that use up to 7 ×more trainable parameters. For instance, SVFTB
d=8
attains an average performance of 83.35% with only 9.8M parameters, closely matching LoRA r=16
(83.69%, 68.8M parameters). We observe similar trends with Gemma-2B (refer Table 8).
Natural Language Understanding. Results on the GLUE benchmark are summarized in Table 3.
SVFT matches LoRA r=8andDoRA r=4which use 12-22×more trainable parameters. Similarly,
when compared to OFT and BOFT, SVFTPmaintains a comparable average performance despite
being 12 ×smaller. These results highlight SVFT ’s ability to strike a balance between parameter
efficiency and performance, making it an attractive PEFT choice for simple classification tasks.
Parameter efficiency. In Figure 1, we plot the performance of SVFT on mathematical reasoning
and commonsense reasoning against other PEFT techniques across a range of configurations. Across
7

--- PAGE 8 ---
Table 4: Performance on image classification benchmarks. For LoRA, DoRA and SVFTB, we adapt
{Q, K, V , U, D} modules of the transformer. For SVFTP, we adapt only {Q, V} to keep it comparable
with VeRA. We report accuracy for all tasks.
MethodViT-B ViT-L
#Params CIFAR100 Flowers102 #Params Food101 Resisc45
Head - 78.25 98.42 - 75.57 64.10
Full-FT 85.8M 85.35 98.37 303.3M 77.83 76.83
LoRA r=8 1.32M 84.10 99.23 3.54M 77.13 79.62
DoRA r=8 1.41M 85.03 99.30 3.76M 76.41 78.32
BOFTb=4
m=4 0.11M 85.54 98.59 2.95M 78.42 74.70
LoRA r=1 0.16M 84.86 96.88 0.44M 75.97 78.02
DoRA r=1 0.25M 84.46 99.15 0.66M 75.90 78.02
VeRA r=256 24.6K 83.38 98.59 0.06M 75.97 72.44
SVFTP18.5K 83.85 98.93 0.05M 75.95 71.97
SVFTB
d=2 0.27M 84.72 99.28 0.74M 77.94 79.70
SVFTB
d=8 0.93M 85.69 98.88 2.5M 78.36 73.83
trainable parameter budgets ranging from lowest to highest, SVFT obtains the best overall perfor-
mance, matching methods that require significantly more trainable parameters. These results establish
SVFT as a Pareto-dominant approach for parameter-efficient fine-tuning.
5.2 Performance on Vision Tasks
0.05 0.1 0.2 0.4 0.8 1.6 3 5.5
Number of Trainable Params (M)30323436384042444648Accuracy (%)Weight Types
Q,V
Q,K,V
U,D
Q,K,V,U,D
Q,K,V,U,D,G,O
Configuration
P
d=2
d=4
d=8
Figure 4: Performance variation with
SVFTB
dbased on the adapted weight
matrices – GSM-8K with Gemma-2B.
Adapting more target weight types re-
sults in greater gains in performance. In-
terestingly, for a fixed parameter budget,
adapting UandDweight types gives
greater lifts than adapting QandV.Table 4 contrasts SVFT against other PEFT techniques on
image classification benchmarks using ViT-B and ViT-L
models. For ViT-B, SVFTB
d=8surpasses full fine-tuning
performance along with LoRA r=8and DoRA r=8on
CIFAR-100. SVFTB
d=2matches LoRA r=8andDoRA r=8
on Flowers102 with up to 5×fewer parameters. For ViT-L,
SVFTB
dalso demonstrates superior or competitive perfor-
mance on both Food101 and Resisc45, with significantly
lower trainable parameters compared to both fully fine-
tuned models and other state-of-the-art PEFT approaches.
5.3 Contribution of Each Weight Type
In Figure 4, we investigate the contribution of each weight
type. Starting with the base configuration, we apply
SVFTB
dto theQandVweights in each transformer block
and report the performance. We then incrementally add
the remaining weight modules ( K,U,D,O,G) and ob-
serve the changes in performance. For each configuration,
we also vary the trainable parameters by incrementing the
total learnable off-diagonals.
Note that applying SVFTB
dtoU,D,O,andGdoes not increase trainable parameters as much as
applying LoRA/DoRA to these modules (Table 7). For example, for a large matrix of shape d1×d2,
LoRA r=1learns d1+d2parameters, while SVFTPlearns min(d1, d2)parameters. We observe
that adapting only UandDwith SVFT yields up to a 10% relative improvement over adapting
8

--- PAGE 9 ---
QandVfor the same parameter budget ( ∼0.8M). Our findings indicate that adapting more weight
types enhances performance.
Table 5: Results on fine-tuning Gemma-2B with
SVFT using different Mparameterizations.
Structure #Params GSM-8K MATH
Plain 0.2M 40.34 14.38
Banded3.3M 46.47 16.04
6.4M 47.84 15.68
Random3.3M 47.76 15.98
6.4M 50.03 15.56
Top-k3.3M 48.00 15.80
6.4M 49.65 15.32Table 6: Impact of pre-trained weight qual-
ity. Results on GSM-8K after fine-tuning on
Pythia-2.8B checkpoints at different stages of
pre-training (PT). Compared to LoRA, SVFT
benefits more from better pre-trained weights.
SVFT outperforms LoRA in both cases.
Method #ParamsPT Steps∆Perf
39K 143K
Full-FT 2.5B 21.00 30.09 9.09
LoRA 5.24M 11.22 18.95 7.73
SVFT 5.56M 15.08 23.19 8.11
5.4 Impact of M’s Structure on Performance
We analyze the impact of different parameterizations of M(Plain, Banded, Random, Top- k) on
downstream performance. To ensure a fair comparison, we match the number of trainable coefficients
across all variants. As shown in Table 5, both Random and Top- kvariants outperform Banded on the
GSM-8K dataset. However, this improvement comes at the cost of performance on MATH. This ob-
servation suggests that the choice of parameterization has a significant impact on model performance,
and the effectiveness of a particular structure may vary depending on the downstream task.
5.5 Impact of Pre-trained Weight Quality
A key feature of SVFT is that the weight update depends on the pre-trained weights W. We therefore
ask the following question: Does the quality of pre-trained weights have a disproportionate impact
onSVFT ?To answer this, we consider two checkpoints from the Pythia suite [ 2] at different stages
of training, i.e., 39K steps and 143K steps, respectively. We fine-tune each of these checkpoints
independently with Full-FT, LoRA, and SVFT . We then compare the increase in performance ( ∆Perf).
As shown in Table 6, compared to LoRA, SVFT benefits more from better pre-trained weights. We
also note that SVFT outperforms LoRA in both settings, suggesting that the benefits of inducing a
∆Wthat explicitly depends on Ware beneficial even when Wis sub-optimal.
6 Discussion
Limitations. Despite significantly reducing learnable parameters and boosting performance, SVFT
incurs some additional GPU memory usage. Unlike LoRA and its variants, SVFT necessitates
computing the SVD and storing both left and right singular vectors. While memory consumption
remains lower than BOFT, it’s roughly double that of LoRA. We mitigate this in our work by
employing system-level optimizations like mixed-precision weights (e.g., bfloat16). However, similar
to the scaling explored in [ 31], memory usage should amortize with the increasing scale of adaptation
tasks. In future work we will explore quantization and other techniques to address memory concerns.
Broader Impact. Our work enables easier personalization of foundational models, which can have
both positive and negative societal impacts. Since our method provides computational efficiency
(smaller parameter footprint), it will be less expensive to enable personalization.
9

--- PAGE 10 ---
7 Conclusion
This work introduces SVFT , a novel and efficient PEFT approach that leverages the structure of pre-
trained weights to determine weight update perturbations. We propose four simple yet effective sparse
parameterization patterns, offering flexibility in controlling the model’s expressivity and the number
of learnable parameters. Extensive experiments on language and vision tasks demonstrate SVFT ’s
effectiveness as a PEFT method across diverse parameter budgets. Furthermore, we theoretically
show that SVFT can induce higher-rank perturbation updates compared to existing methods, for a
fixed parameter budget. In future work, we aim to develop principled methods to generate sparsity
patterns, potentially leading to further performance improvements.
Acknowledgements
We thank CISPA Helmholtz Center for Information Security and Greg Kuhlmann for their invaluable
support in facilitating this research. We also appreciate Anubhav Goel for his helpful discussions and
support.
References
[1]Meta AI. Introducing meta llama 3: The most capable openly available llm to date. April 2024.
[2]Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,
Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large
language models across training and scaling, 2023.
[3]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning
about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on
Artificial Intelligence , 2020.
[4]Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative
components with random forests. In European Conference on Computer Vision , 2014.
[5]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions,
2019.
[6]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge, 2018.
[7]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
[8]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations , 2021.
[10] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-
style pre-training with gradient-disentangled embedding sharing, 2023.
[11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset,
2021.
10

--- PAGE 11 ---
[12] Karl Moritz Hermann, Tomáš Ko ˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of
the 28th International Conference on Neural Information Processing Systems , NIPS’15, page
1693–1701. MIT Press, 2015.
[13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for
NLP. In Proceedings of the 36th International Conference on Machine Learning , Proceedings
of Machine Learning Research. PMLR, 2019.
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
International Conference on Learning Representations , 2022.
[15] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya
Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning
of large language models, 2023.
[16] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. ELoRA: Efficient low-rank
adaptation with random matrices. In The Twelfth International Conference on Learning Repre-
sentations , 2024.
[17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[18] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang,
Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation,
2024.
[19] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen
Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard
Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth
International Conference on Learning Representations , 2024.
[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach, 2019.
[21] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular
vectors adaptation of large language models. arXiv preprint arXiv:2404.02948 , 2024.
[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering, 2018.
[23] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing ,
Dec 2008.
[24] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian
Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning.
InThirty-seventh Conference on Neural Information Processing Systems , volume 36, pages
79320–79362, 2023.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020.
[26] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov,
Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan
Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for
code, 2024.
11

--- PAGE 12 ---
[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale, 2019.
[28] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-
monsense reasoning about social interactions, 2019.
[29] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
models based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.
[30] Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle M Guyon, Mike Huisman, Felix Mohr,
Jan N van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multi-
domain meta-dataset for few-shot image classification. In Thirty-sixth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track , 2022.
[31] Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. In
The Twelfth International Conference on Learning Representations , 2024.
[32] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,
Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical
questions for large language models, 2023.
[33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can
a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , 2019.
[34] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with
extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International
Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pages 11328–11339. PMLR, 13–18 Jul 2020.
[35] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,
and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh
International Conference on Learning Representations , 2023.
12

--- PAGE 13 ---
Appendix
The appendix is organized as follows.
• In Appendix A, we give proofs for the lemmas outlined in 3.2.
•In Appendix B, we compare how the trainable parameters count for different PEFT tech-
niques (LoRA, DoRA, VeRA) versus our method SVFT.
•In Appendix C, we describe results for additional experiments and provide implementation
details for all the experiments.
A Proofs
We provide brief proofs for the Structure ,Expressivity and the Rank lemmas for SVFT:
(a) Structure: IfMis diagonal, then the final matrix W0+UMVTcan be written as
U(Σ +M)VTsinceW0=UΣVT, where (Σ +M)is also a diagonal matrix. Thus,
U(Σ +M)VTis a valid and unique SVD of W0+UMVTup to sign flips in the singular
vectors.
(b)Expressivity: Finding Mfor any target matrix Pof size d1×d2such that P=W0+
UMVTis the same as finding Mfor a new target matrix P′=P−W0such that
P′=UMVT. For a full SVD, the dimension of Misd1×d2and since the dimension of
P′is also d1×d2,P′=UMVTis a bijection and M=UT(P−W0)V(since UandV
are orthogonal).
(c)Rank: IfMhasknon-zero elements, then the rank of the update UMVTwill be upper
bounded by k(since by Gaussian elimination, kor less elements will remain, the best case
being all kelements in the diagonal). We also know that the rank is upper bounded by
min{d1, d2}, giving an achievable upper bound on the rank as min{k,min{d1, d2}}.
B Parameter Count Analysis
Table 7: Parameter count analysis. Ltuned,Dmodel,r,kdenote total layers being adapted, hidden
dimension, rank, and additional off-diagonals respectively.
Method Trainable Parameter Count
LoRA 2×Ltuned×Dmodel×r
DoRA Ltuned×Dmodel×(2r+ 1)
VeRA Ltuned×(Dmodel +r)
SVFTPLtuned×Dmodel
SVFTB
d=kLtuned×(Dmodel×k+ (Dmodel−k)(k+ 1))
C Additional Experiments and Implementation Details
All of our experiments are conducted on a Linux machine (Debian GNU) with the following specifi-
cations: 2xA100 80 GB, Intel Xeon CPU @ 2.20GHz with 12 cores, and 192 GB RAM. For all our
experiments (including baseline experiments), we utilize hardware-level optimizations like mixed
weight precision (e.g., bfloat16) whenever possible.
C.1 Commonsense Reasoning Gemma-2B
We evaluate and compare SVFT variants against baseline PEFT methods on commonsense reasoning
tasks with Gemma-2B model and tabulate results in Table 8.
13

--- PAGE 14 ---
Table 8: Results with Gemma-2B on eight commonsense reasoning benchmarks. We follow [ 18] for
hyperparameter configurations, and report accuracy for all tasks.
Method #Params BOOLQ PIQA SIQA HellaSwag Winogrande ARC-E ARC-C OBQA Average
Full-FT 2.5B 63.57 74.1 65.86 70.00 61.95 75.36 59.72 69 67.45
LoRA r=32 26.2M 63.11 73.44 63.20 47.79 52.95 74.78 57.16 67.00 62.43
LoRA r=16 13.5M 62.87 73.93 65.34 53.16 55.51 76.43 59.55 68.4 64.40
BOFTb=8
m=2 1.22M 59.23 63.65 47.90 29.93 50.35 59.04 42.66 41.00 49.22
VeRA r=2048 0.66M 62.11 64.31 49.18 32.00 50.74 58.08 42.83 42.6 50.23
LoRA r=1 0.82M 62.2 69.31 56.24 32.47 51.53 69.52 48.8 56.4 55.81
DoRA r=1 1.19M 62.17 68.77 55.93 32.95 51.22 68.81 48.72 55.6 55.52
SVFTP0.19M 62.26 70.18 56.7 32.47 47.04 69.31 50.08 58.4 55.81
SVFTB
d=16 6.35M 63.42 73.72 63.86 71.21 59.58 73.69 54.77 66.6 65.86
Table 9: Performance on image classification benchmarks. For LoRA, DoRA and SVFTB
d, we adapt
{Q, K, V , U, D} modules of the transformer. For SVFTP, we adapt only {Q, V} to keep it comparable
with VeRA. We report accuracy for all tasks.
MethodViT-B ViT-L
#Params CIFAR100 Flowers102 Food101 Resisc45 #Params CIFAR100 Flowers102 Food101 Resisc45
Head - 78.25 98.42 74.93 59.95 - 82.95 98.75 75.57 64.10
Full-FT 85.8M 85.35 98.37 76.32 68.03 303.3M 86.56 97.87 77.83 76.83
LoRA r=8 1.32M 84.41 99.23 76.02 76.86 0.35M 86.00 97.93 77.13 79.62
DoRA r=8 1.41M 85.03 99.30 75.88 76.95 3.76M 83.55 98.00 76.41 78.32
BOFTb=2
m=2 0.07M 85.55 98.54 76.06 67.70 0.20M 87.84 97.95 77.90 73.97
BOFTb=4
m=4 0.11M 85.54 98.59 76.51 69.44 0.30M 87.72 97.95 78.42 74.70
LoRA r=1 0.16M 84.86 96.88 73.35 76.33 0.44M 85.97 98.28 75.97 78.02
DoRA r=1 0.25M 84.46 99.15 74.80 77.06 0.66M 84.06 98.11 75.90 78.02
VeRA 24.6K 83.38 98.59 75.99 70.43 61.4K 86.77 98.94 75.97 72.44
SVFTP18.5K 83.85 98.93 75.68 67.19 49.2K 86.74 97.56 75.95 71.97
SVFTB
d=2 0.28M 84.72 99.28 75.64 72.49 0.74M 86.59 98.24 77.94 79.70
SVFTB
d=4 0.50M 83.17 98.52 76.54 66.65 1.32M 87.10 97.71 76.67 71.10
SVFTB
d=8 0.94M 85.69 98.88 76.70 70.41 2.50M 87.26 97.89 78.36 73.83
C.2 Additional Vision Experiments
For vision tasks, we compare the SVFT banded variants and SVFT plain with baseline PEFT
methods on classification vision tasks using ViT-Base and ViT-Large models in Table 9.
C.3 Are All Singular Vectors Important?
To determine the importance of considering all singular vectors and singular values during fine-
tuning, we reduce the rank of UandV, and truncate ΣandMto an effective rank of r. If the
original weight matrix W∈Rm×n, then after truncation, U∈Rm×r,V∈Rn×r. This truncation
significantly reduces the number of trainable parameters, so we compensate by increasing the number
of off-diagonal coefficients ( d) inM.
Our results, with four different configurations of randd, are presented in Table 10. The findings show
that a very low rank ( r= 128 ) leads to poor performance, even when parameters are matched. A
reasonably high rank of r= 1536 , which is 75% of the full rank, still fails to match the performance
of the full-rank variant that has 0.25 ×the trainable parameters. This indicates that all singular vectors
14

--- PAGE 15 ---
significantly contribute to the end task performance when fine-tuning with SVFT , and that important
information is lost even when truncating sparingly.
Table 10: Performance with varying rank ( r) and the off-diagonal elements ( d) ofM. When
r= 2048 , the update is full-rank.
Rank ( r) Diags ( d) #Params GSM-8K MATH
128 64 1.55M 0.98 0.21
1536 - 0.15M 16.37 3.64
1536 2 0.74M 25.01 6.04
2048 - 0.19M 40.34 14.38
C.4 Performance vs Total Trainable Parameters
In addition to the experiments performed in Figure 1 for Gemma-2B on challenging natural language
generation (NLG) tasks like GSM-8K and Commonsense Reasoning, we also plot the performance
vs total trainable parameters for larger state-of-the-art models like Gemma-7B and LLaMA-3-8B on
GSM-8K. Figure 5 further demonstrates SVFT’s Pereto-dominance. On larger models, we observe
that full-finetuning overfits, leading to sub-optimal performance in comparison to PEFT methods.
0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 84
Number of Trainable Params (M)707172737475767778
SVFTPSVFTB
d=2SVFTR
d=16
DoRAr=16DoRAr=4LoRAr=32
LoRAr=1DoRAr=1
VeRAr=1024BOFTm=2
b=8LoRAr=4Full Fine-Tuning (8500M params)
0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 81
Number of Trainable Params (M)62646668707274767880
SVFTPSVFTB
d=2SVFTB
d=8SVFTB
d=12
DoRAr=16LoRAr=32
LoRAr=1DoRAr=1
VeRAr=1024BOFTm=2
b=8LoRAr=4
Full Fine-Tuning (2500M params)Accuracy (%)
Figure 5: Performance versus total trainable parameters for GSM-8K on Gemma-7B (left) and
LLaMA-3-8B (right).
C.5 Settings for Language Tasks
Natural Language Understanding. We fine-tune the DeBERTaV3 base[10] model and apply SVFT
to all linear layers in every transformer block of the model. We only moderately tune the batch size,
learning rate, and number of training epochs. We use the same model sequence lengths used by [ 19]
to keep our comparisons fair. The hyperparameters used in our experiments can be found in Table 11.
Natural Language Generation. See the hyperparameters used in our experiments in Table 12. For
LoRA, DoRA, we adapt Q, K, V, U, D matrices. We apply BOFT on Q, V matrices since applying
on multiple modules is computationally expensive. For VeRA, which enforces a constraint of uniform
internal dimensions for shared matrices, we apply on G, U projection matrices as it yields the highest
number of learnable parameters. We apply SVFT on Q, K, V, U, D, O, G for the Gemma family of
models, and U, D, O, G for LLaMA-3-8B. Note that applying SVFT on these modules does not
increase trainable parameters at the same rate as applying LoRA or DoRA on them would. We adopt
the code base from https://github.com/meta-math/MetaMath.git for training scripts and
evaluation setups and use the fine-tuning data available at https://huggingface.co/datasets/
meta-math/MetaMathQA-40K .
15

--- PAGE 16 ---
Table 11: Hyperparameter setup used for DeBERTaV3 baseon the GLUE benchmark.
Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
Optimizer AdamW
Warmup Ratio 0.1
LR Schedule Linear
Learning Rate (Head) 6E-03
Max Seq. Len. 256 128 320 64 512 320 320 128
# Epochs 10 10 30 20 10 6 15 15
SVFTPBatch Size 32 32 16 16 32 16 4 32
Learning Rate 5E-02 5E-02 5E-02 8E-02 8E-02 5E-02 5E-02 5E-02
SVFTR
d=2Batch Size 32 32 16 16 32 32 16 32
Learning Rate 1E-02 1E-02 1E-02 1E-02 3E-02 1E-02 3E-02 1E-02
Table 12: Hyperparameter setup used for fine-tuning on MetaMathQA-40K.
HyperparameterGemma-2B Gemma-7B LLaMA-3-8B
SVFTPSVFTR
d=16 SVFTPSVFTR
d=16 SVFTPSVFTR
d=12
Optimizer AdamW
Warmup Ratio 0.1
LR Schedule Cosine
Learning Rate 5E-02 1E-03 5E-02 1E-03 5E-02 1E-03
Max Seq. Len. 512
# Epochs 2
Batch Size 64
Commonsense Reasoning. See the hyperparameters used in our experiments in Table 13. We
adopt the same set of matrices as that of natural language generation tasks. We use the code base from
https://github.com/AGI-Edgerunners/LLM-Adapters , which also contains the training and
evaluation data.
Table 13: Hyperparameter setup used for fine-tuning on commonsense-15K.
HyperparameterGemma-2B Gemma-7B
SVFTPSVFTB
d=8 SVFTPSVFTB
d=8
Optimizer AdamW
Warmup Steps 100
LR Schedule Linear
Max Seq. Len. 512
# Epochs 3
Batch Size 64
Learning Rate 5E-02 5E-03 5E-02 1E-03
16

--- PAGE 17 ---
Table 14: Hyperparameter setup used for fine-tuning on all vision tasks.
Hyperparameter ViT-B ViT-L
Optimizer AdamW
Warmup Ratio 0.1
Weight Decay 0.01
LR Schedule Linear
# Epochs 10
Batch Size 64
SVFTPLearning Rate (Head) 4E-03
SVFTPLearning Rate 5E-02
SVFTB
d=2Learning Rate (Head) 4E-03
SVFTB
d=2Learning Rate 5E-02
SVFTB
d=8Learning Rate (Head) 4E-03
SVFTB
d=8Learning Rate 5E-03
C.6 Settings for Vision Tasks
For each dataset in the vision tasks, we train on 10 samples per class, using 2 examples per class for
validation, and test on the full test set. Similar to previous literature, we always train the classifier
head for these methods since the number of classes is large. The parameter counts do not include the
number of parameters in the classification head. The hyperparameters are mentioned in Table 14.
We tune the learning rates for SVFT and BOFT select learning rates for other methods from [ 16], run
training for 10 epochs, and report test accuracy for the best validation model. For all methods, since
classification head has to be fully trained, we report the parameter count other than the classification
head.
17
