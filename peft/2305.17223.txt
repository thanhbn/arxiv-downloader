# 2305.17223.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.17223.pdf
# File size: 896596 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Do We Really Need a Large Number of Visual Prompts?
Youngeun Kim
Yale University
youngeun.kim@yale.eduYuhang Li
Yale University
yuhang.li@yale.eduAbhishek Moitra
Yale University
abhishek.moitra@yale.edu
Ruokai Yin
Yale University
ruokai.yin@yale.eduPriyadarshini Panda
Yale University
priya.panda@yale.edu
Abstract
Due to increasing interest in adapting models on resource-
constrained edges, parameter-efficient transfer learning has
been widely explored. Among various methods, Visual
Prompt Tuning (VPT), prepending learnable prompts to input
space, shows competitive fine-tuning performance compared
to training of full network parameters. However, VPT in-
creases the number of input tokens, resulting in additional
computational overhead. In this paper, we analyze the im-
pact of the number of prompts on fine-tuning performance
and self-attention operation in a vision transformer architec-
ture. Through theoretical and empirical analysis we show
that adding more prompts does not lead to linear perfor-
mance improvement. Further, we propose a Prompt Con-
densation (PC) technique that aims to prevent performance
degradation from using a small number of prompts. We val-
idate our methods on FGVC and VTAB-1k tasks and show
that our approach reduces the number of prompts by ∼70%
while maintaining accuracy.
1. Introduction
Parameter-Efficient Transfer Learning (PETL) has be-
come a popular approach in various domains as it enables
fine-tuning pre-trained models with minimal memory usage
on resource-constrained edge devices [ 31,45,46,48,14,16].
In PETL, a large model with billions of parameters, such
as a transformer [ 8,38], is first trained on a massive dataset
on a cloud server, and then fine-tuned with limited computa-
tional/memory resources on edge devices. Among various
PETL methods, Visual Prompt Tuning (VPT) [ 17] is promis-
ing due to its ability to update a small subset of parameters
while achieving higher accuracy than other methods. Tech-
nically, VPT introduces learnable prompt tokens, which are
prepended to the input or intermediate image patch tokens.
While VPT can induce memory efficiency, the use of# Prompts (ViT-B/16 [8]) 0 50 100 150 200
GFLOPs 17.6 22.2 26.9 31.8 36.7
Computational Overhead 0% 26.1% 52.8% 80.6% 108.5%
# Prompts (Swin-B [26]) 0 5 10 25 50
GFLOPs 15.4 16.3 17.2 19.8 24.3
Computational Overhead 0% 5.8% 11.6% 28.5% 57.8%
Table 1: The increase of floating-point operations (FLOPs)
with respect to the number of prompts in VPT [17].
50 100
#Prompt (%)8081828384Accuracy (%)<1% Acc. dropStandford_Cars
50 100
#Prompt (%)434445464748Accuracy (%)<1% Acc. dropDMlab
50 100
#Prompt (%)323334353637Accuracy (%)<1% Acc. dropSmallNORB_elevation
Figure 1: Accuracy depending on the number of prompts
used for VPT training. We transfer an ImageNet-22k pre-
trained ViT-B/16 [ 8] to three downstream tasks. The x-
axis shows the relative number of prompts compared to the
original number reported in [ 17]. The vertical dotted line
indicates the point where there is < 1% drop in accuracy
from 100% number of prompts.
additional prompt tokens leads to increased computational
costs from self-attention and linear layers [ 26,42,8]. We
report FLOPs with respect to the number of prompts in Table
1, which shows that the computational cost of VPT signifi-
cantly increases as the number of prompts increases. If 200
prompts are prepended to the input space of ViT-B, the com-
putational overhead ( i.e. FLOPs) almost doubles compared
to the model without any prompts. This indicates there is
an inevitable trade-off between the number of prompts and
computational cost in VPT.
Given such a trade-off, it is natural to ask: How does the
fine-tuning performance change with respect to the number
of prompts? To find the answer, we measure the test accuracy
with respect to the number of prompts. Interestingly, asarXiv:2305.17223v2  [cs.CV]  12 May 2024

--- PAGE 2 ---
shown in Fig. 1, we found that reducing the number of
prompts for VPT training by approximately 50% does not
lead to a significant drop, and most of the performance drop
happens in the 10%∼40% range. The results imply that the
correlation between the number of prompts and fine-tuning
accuracy is not linear.
To further provide a better understanding of the prompts
in VPT, we analyze the impact of the number of prompt to-
kens on fine-tuning accuracy by addressing several questions:
Why does the number of prompts and the fine-tuning perfor-
mance have a non-linear correlation? How does the number
of prompts affect the self-attention operation? If there is a
performance drop with less number of prompts, how can we
recover the accuracy drop? We provide both empirical and
mathematical analysis to answer such questions. This can
provide insights into the behavior of the VPT model and its
self-attention mechanism, which can help researchers better
understand VPT and potentially improve the prompt design.
At the same time, it is essential to analyze this impact on
the computational cost to ensure that the method remains
practical for deployment on extremely resource-constrained
edge devices.
A noteworthy observation from Fig. 1 is that the perfor-
mance degradation in <50% number of prompts regime is
non-trivial. To address this, we propose Prompt Condensa-
tion(PC), a technique that reduces the number of prompt
tokens with minimal accuracy drop. The PC consists of three
steps: (1) Computing the importance score for each prompt.
Here, we propose a global metric for measuring the impor-
tance score of each prompt, which provides better accuracy
compared to the local attention-based metrics [ 25,30,10].
(2) Selecting the top k%prompts based on the importance
score, and discard the remaining prompts. (3) Fine-tuning
the selected prompts while freezing other parameters.
In summary, our contributions can be as follows:
•In a first-of-its-kind study, we analyze the impact of
the number of visual prompt tokens on the fine-tuning
accuracy and self-attention operation in VPT.
• We find that the number of prompts is not linearly pro-
portional to performance improvement. To support this,
we provide empirical and mathematical analysis.
•To recover the performance drop with a small number of
prompts, we propose Prompt Condensation (PC). Our
method can reduce the number of prompts by ∼70%
while maintaining performance.
2. Related Work
2.1. Parameter Efficient Transfer Learning (PETL)
Efficient fine-tuning of large pre-trained models on edge
devices has become a popular research topic due to itspracticality and high performance [ 31,45,46,48,14,16].
Rather than training the entire set of parameters in neural
networks, researchers focus on how to use a small percent-
age of weights to maximize transfer performance. To this
end, several approaches [ 32,35,15,3] insert a lightweight
bottleneck module into the transformer model, allowing gra-
dients to be calculated only for a small number of parameters.
TinyTL [ 3] and BitFit [ 43] propose to update the bias term to
fine-tune the model. Other approaches [ 45,34] add side net-
works that can be optimized while keeping the original large
model frozen. Another effective method to reduce mem-
ory requirements is to sparsify [ 18] or quantize activation
[4,5,11,9] during backward gradient calculation. Recently,
VPT [ 17] prepends trainable parameters to the input space
of the pre-trained model, achieving similar (and sometimes
even better) accuracy compared to full fine-tuning while opti-
mizing only about 1%of the parameters. However, adding a
large number of prompts can significantly increase the com-
putational overhead of the model. In this work, we analyze
how the number of prompts affects fine-tuning performance.
Importance of our work. Prompt tuning is one of the major
research directions to fine-tune the large-scale pre-trained
model. Considering that prompt learning is applied to var-
ious applications, we aim to improve the efficiency of the
prompt tuning approach. Our objective differentiates from
prior works [ 7,1,6,47,19], such as adapter-based or partial
training methods, which primarily seek to enhance perfor-
mance on downstream tasks with different approaches. Fur-
thermore, given that our technique does not necessitate any
modifications to the model architecture, it offers promising
potential for extension in future prompt learning approaches.
2.2. Token Sparsification
The computational cost of ViT [ 8] increases as the number
of tokens given to the model increases [ 36]. To alleviate this
issue, previous works aim to reduce the number of tokens
[10,30,27,23,13,42,21,22,33]. Liang et al. [25] define
the importance score of each token based on its similarity to
a[CLS]token. Rao et al. [30] propose a prediction module
with Gumbel-Softmax to sparsify tokens, which is jointly
trained with the model parameters. Meng et al. [27] propose
a decision network that can turn on/off heads and blocks in
a transformer architecture. The authors of [ 41] propose an
adaptive halting module that calculates a probability for each
token to determine when to halt processing. However, these
methods require updating the weight parameters inside the
transformer or an additional module, which is challenging to
apply to the PETL scenario. Recently, [ 2] proposed a token
merging technique without training, gradually reducing the
number of tokens in each block of vision transformers to
speed up inference. However, their method will be diffi-
cult to apply for prompt tokens because prompt tokens are
introduced at every layer.

--- PAGE 3 ---
#Prompts \Data CUB-200 NABirds Stanford Dog Stanford Car CIFAR100 SVHN EuroSAT Resisc45
100% 88.52±0.09 84.20±0.05 90.22±0.09 83.42±0.11 78.51±0.71 80.64±1.28 96.41±0.39 82.66±1.54
50% 88.45±0.04 84.21±0.06 90.25±0.05 83.22±0.09 78.23±0.84 80.66±0.39 96.09±0.08 82.18±0.11
40% 88.45±0.10 84.18±0.02 90.21±0.06 83.16±0.04 77.87±0.49 80.58±0.42 95.88±0.40 82.10±0.97
30% 88.49±0.10 84.16±0.04 90.22±0.06 81.90±0.08 78.18±0.98 78.49±1.65 95.88±0.40 82.53±0.81
20% 88.47±0.09 84.11±0.04 90.22±0.09 81.42±0.12 78.08±0.68 79.08±1.46 95.98±0.27 82.36±0.40
10% 88.13±0.11 84.13±0.03 90.20±0.07 80.76±0.14 77.62±0.23 77.56±0.89 95.90±0.12 81.21±0.57
#Prompts \Data Clevr/count Clevr/dist DMLab KITTI/dist dSprites/loc dSprites/ori SmallNORB/azi SmallNORB/ele
100% 68.65±1.24 59.05±0.32 46.05±0.33 72.89±2.20 74.35±2.80 48.09±1.77 32.86±0.84 36.46±0.19
50% 68.49±2.12 59.68±0.60 46.21±0.87 72.26±1.38 72.26±3.11 47.50±1.36 32.43±0.28 36.34±0.42
40% 68.88±1.70 59.45±0.38 45.32±0.50 72.32±1.40 69.02±4.05 47.18±0.41 32.22±0.43 36.19±0.65
30% 66.40±0.83 58.94±0.20 44.58±1.10 72.33±1.29 67.48±5.75 47.37±1.17 31.24±0.76 35.94±0.64
20% 65.62±3.23 58.94±0.57 44.57±0.88 72.14±1.12 58.14±6.26 47.22±0.73 29.29±1.86 35.43±0.43
10% 60.49±3.08 58.83±0.21 44.22±0.89 72.39±1.30 52.26±6.06 44.46±2.57 29.03±1.50 32.63±0.50
Table 2: The test accuracy of VPT-Deep on FGVC and VTAB-1k tasks with respect to the number of prompts. We report 6
different prompt settings, where k%represents how many prompts we use for VPT training compared to the original number
of prompts reported in [ 17]. We run the same configuration 3 times and report the mean and standard deviation. We transfer an
ImageNet pre-trained ViT-B/16 [8] backbone.
3. Preliminary
Vision Transformer. Our work is based on ViT [ 8] which
processes image tokens with multiple attention operations.
The input image is sliced into multiple patches ( i.e. tokens).
Then, in each layer, the self-attention operation is applied to
the image tokens. Let’s assume we have token embedding
X∈Rn×d, Query Q=XW q, Key K=XW k, Value V=
XW vwith linear projection. Then, the attention operation
can be formulated as follows.
Attention (Q, K, V ) =Softmax (QKT
√
d)
| {z }
AV, (1)
where Ais the self-attention matrix after the Softmax func-
tion. We utilize Multi-Head Self-Attention (MHSA) in our
approach, which takes the outputs of multiple single-head at-
tention blocks, and then projects the combined output using
an additional parameter matrix.
head i=Attention (XWi
q, XWi
k, XWi
v). (2)
MHSA (X) =Concat [head 1, ..., head H]Wo+X. (3)
The output tokens generated by the MHSA block are fed
into a Feed-Forward Network (FFN), which is composed
of two fully-connected layers with a GELU activation layer
in between. In the last encoder layer of the Transformer,
the[CLS]token is extracted from the output tokens and
employed to predict the class.
Visual Prompt Tuning. Visual Prompt Tuning (VPT) [ 17]
suggests a memory-efficient fine-tuning technique by adding
a set of learnable prompts at the input/intermediate layers.
Depending on where the prompts are added, VPT has two
versions: VPT-Shallow and VPT-Deep.
LetXi∈Rn×dbe the token embedding at layer i∈
{1,2, ..., L}, andFi(·)be the operations inside layer i. VPT-
0 200 400
Eigenvalue index0.00.20.40.60.81.0Normalized cum. eigenvalueStanford_Cars
without prompt
with prompt
with 50% prompt
with 20% prompt
0 100 200 300
Eigenvalue index0.20.40.60.81.0Normalized cum. eigenvalueDMLabFigure 2: The normalized cumulative eigenvalue of self-
attention matrix Ain Eq. 1 on Stanford Cars and DMLab.
We report the mean and standard deviation across all layers.
Shallow prepends mprompts P1∈Rm×dto the input token
embedding X1.
[Z2, X2] =F1([P1;X1]). (4)
[Zi+1, Xi+1] =Fi([Zi;Xi])for1< i≤L. (5)
Here, Ziis the output tokens from the layer i. Note that only
P1and the classification head are trained.
On the other hand, VPT-Deep introduces prompts Pi∈
Rm×din every layer.
[__, Xi+1] =Fi([Pi;Xi])for1≤i≤L. (6)
VPT-Deep shows higher performance than VPT-Shallow by
using more prompts. In our work, we focus on VPT-Deep
due to its superior performance. Although VPT requires sig-
nificantly less memory usage for training, the computational
overhead increases as the total number of tokens increases.
4. Analysis on the Number of Visual Prompts
In this section, we analyze the impact of prompts on
self-attention operation and fine-tuning accuracy. We first

--- PAGE 4 ---
demonstrate two observations, and then we provide the math-
ematical support why the performance does not linearly im-
prove as we use more prompts.
Observation1 :Reducing the number of prompts does not
linearly decrease the accuracy. In addition to Fig. 1, we pro-
vide further empirical evidence on the correlation between
the number of prompts and the fine-tuning accuracy. We
evaluate the test accuracy of our approach on FGVC and
VTAB-1k [ 44] tasks with varying the number of prompts.
It is worth noting that each dataset requires a specific num-
ber of prompts to achieve optimal performance, as reported
in [17]. We focus on datasets that require more than 10
prompts for both VPT-Shallow and VPT-Deep since using
fewer than 10 prompts does not result in significant com-
putational overhead. We present the performance change
according to the number of prompts in Table 2. Our analysis
shows that for the majority of the datasets, decreasing the
number of prompts by about 50% does not result in a sig-
nificant decline in performance. Additionally, most of the
performance decrease occurred in the range of 10% to 40%,
indicating that the relation between accuracy and the number
of prompts is not linear.
Observation 2 :Self-attention matrix is low-rank be-
fore/after adding prompts. The previous work [ 40] shows
that the self-attention matrix in ViT is low-rank. In a similar
line of thought, we investigate the rank of the self-attention
matrix when we add prompts. In Fig. 2, we compare the
cumulative eigenvalue of the self-attention matrix Awith-
outprompts and with prompts. Our results show that the
self-attention matrix remains low-rank even when prompts
are added to the self-attention matrix. Especially, for the
Stanford Cars dataset, we add 200 prompts which is a large
number of tokens than the original image tokens ( i.e. 197),
but the cumulative eigenvalue trend does not change. Over-
all, the results imply that only a few prompts affect the
self-attention operation.
To understand why the number of prompts is not linearly
correlated to the self-attention operation and the accuracy,
we provide a mathematical analysis here. We use the rank of
the approximated low-rank matrix of the attention matrix as
a surrogate metric to evaluate the impact of the prompt on
the self-attention operation.
Theorem 1 (Self-attention is low rank. Proved in [ 40]).
LetA∈Rn×nbe a self-attention matrix, and v∈Rnbe
a column vector of value matrix V. Then, there exists a
low-rank matrix ˜A∈Rn×nsatisfying
Pr(∥˜AvT−AvT∥< ϵ∥AvT∥)>1−o(1), (7)
where the rank of ˜Ais bounded, i.e.,rank (˜A) = Θ( log(n)).
Proposition 1. For any low-rank matrices ˜An∈Rn×nand
˜An+m∈R(n+m)×(n+m)satisfying Pr(∥˜AvT−AvT∥<
123456789101112
Layer index60708090Accuracy(%)Original AccuracyStandford_Cars
123456789101112
Layer index304050Accuracy(%)DMLab
123456789101112
Layer index10203040Accuracy(%)SmallNORB-azimuth
123456789101112
Layer index306090Accuracy(%)Clevr-countFigure 3: Accuracy changes by removing whole prompts in
one layer. We report the original accuracy with a dotted line.
Each dataset shows a different trend in accuracy degradation.
ϵ∥AvT∥)>1−o(1), we have
rank (˜An+m)−rank (˜An) =O(log(m)), (8)
where mis the number of prompts.
Proof. Based on Theorem 1, given a bounded error
Pr(∥˜AvT−AvT∥< ϵ∥AvT∥)>1−o(1), the rank of
˜Anand˜An+mcan be:
αlog(n)≤rank (˜An)≤βlog(n), (9)
αlog(n+m)≤rank (˜An+m)≤βlog(n+m),(10)
where αandβare the constants for the lower and upper
bound respectively. Then, we have
log(n+m)α
nβ
≤rank (˜An+m)−rank (˜An)≤log(n+m)β
nα
.
(11)
We obtain Eq. 8 with respect to the variable m. Additional
details can be found in the Supplementary.
Proposition 1 demonstrates that the increase of the rank
of the low-rank self-attention matrix follows a logarithmic
trend. As the logarithmic function is concave, the effect of
adding new prompts on the attention operation diminishes
as the number of prompts increases. For instance, increasing
the number of prompts from 0to50has a greater impact
than increasing the number of prompts from 150to200.
This analysis is aligned with our Observation 1 where re-
ducing the number of prompts by approximately 50% does
not lead to a significant performance drop, but most of the
performance drop exists in the 10%∼40% range.
5. Prompt Condensation
Although decreasing the number of prompts up to 50%
shows slight performance degradation, the performance drop

--- PAGE 5 ---
is non-trivial in the small number of prompts regime. In
Table 2, the major performance drop happens under 40%
of prompts on most datasets. To address this, we propose a
technique called Prompt Condensation (PC).
Problem Statement. Our objective is to minimize the
number of prompts while maintaining accuracy. Let P=
{p1, p2, ..., p N}be the set of prompts, and P′be the con-
densed prompt set which has a smaller number of elements.
Then our goal can be written as:
min
P′|L(θ, P)− L(θ, P′)|, (12)
where L(·)is the objective function of a task, θis the model
parameters. At the same time, we also aim to minimize the
number of prompts inside P′.
In designing our model for the Parameter Efficient Trans-
fer Learning (PETL) scenario, we consider the following
principles: (1) Model parameters cannot be updated due to
memory constraints. Therefore, only prompts can be train-
able. (2) Additional modules such as those proposed in
[30,27,41] cannot be utilized. Given these constraints, most
token sparsification methods are difficult to be applied in our
case. Instead, our method focuses on identifying important
prompts and fine-tuning them without updating/adding any
model parameters.
Are all prompts equally important? The important design
choice for PC is whether to condense the same number of
prompts for each layer or not. To figure this out, we measure
the accuracy change with respect to the prompts in each layer.
We remove prompts in layer lwhile other layers keep the
same number of prompts. As shown in Fig. 3, we observe
that prompts in different layers have varying contributions
to the accuracy, and the trend varies across different datasets.
This observation leads us to leverage a global score across
all layers, which is unlike the layer-wise score ( i.e. using the
row similarity in self-attention) widely used in the previous
work [25, 30, 10].
Prompt Scoring. We define the impact of prompt piby
computing the difference of the objective function from the
fine-tuned VPT model.
∥∆L(θ, pi)∥2=∥L(θ, P)− L(θ, P′
i)∥2, (13)
where P′
iis the modified prompt set by zeroizing pi∈P.
With Taylor approximation, we can approximate L(θ, P′)at
pi= 0as
L(θ, P′
i)≈ L(θ, P)−dL(θ)
dpipi. (14)
We only use the first-order term since the beyond second-
order term requires huge memory storage. If we substitute
Eq. 14 to Eq. 13, we obtain
∥∆L(θ, pi)∥2≈ ∥dL(θ)
dpipi∥2. (15)Algorithm 1 Prompt Condensation
Input : Training data D; Neural network f(·;P), Original prompt
setP, Condensed prompt set P′, Condensation ratio k%
Output : Trained f(·;P′)
1:Train f(·;P)on dataset DforNvepochs ▷VPT training
2:Compute importance score spiacross all prompts ▷Eq. 16
3:P′←Select the prompts with top k%ofspi
4:Finetune f(·;P′)on dataset DforNpepochs
We average Eq. 15 across all data samples to compute the
importance score.
spi=1
|D|X
d∈D∥dL(θ, d)
dpipi∥2, (16)
where Dis the input data set. Note that, calculating the im-
portance score does not bring huge computational overhead
since we only need to compute the backward gradient for
the prompts.
Once we calculate the importance score for each prompt,
we select the prompts with the highest k% scores across
all layers. This global prompt selection method inherently
allocates the optimal number of prompts for each layer. On
the other hand, with a local layer-wise prompt selection, we
would enforce top k% prompt selection uniformly across all
layers that may inhibit the representation power within the
model. In our experiments, we show the global score pro-
vides better performance than the local layer-wise metrics.
Our approach is similar to filter pruning in CNNs [ 24,28]
in the aspect of utilizing Taylor expansion. However, we
have innovatively adapted this concept to the token level,
presenting a fundamentally distinct granularity in pruning
strategy. To our knowledge, our work is the first to employ
gradient information directly for token pruning within the
context of Vision Transformer (ViT) architectures. As a
result, we believe our research paves the way for the potential
application of existing channel pruning techniques to token
pruning in ViTs.
Overall Training Process. Algorithm 1 illustrates the over-
all process of Prompt Condensation. We first train the origi-
nal prompt set P(Line 1). Then we compute the importance
score of each prompt inside P(Line 2). After that, we sort
the importance score and select the prompts with the top
k%of the importance score (Line 3). This provides the con-
densed prompt set P′. We discard the remaining (100−k)%
of prompts. Finally, the prompts within P′are fine-tuned
(Line 4). For fine-tuning, we use less number of epochs
Npcompared to the original VPT training epochs Nv. We
analyze the effect of Npin Section 6.3. Note that the entire
training process freezes weight parameters across the model
except for the last classifier.

--- PAGE 6 ---
10 20 30 40 50
#Prompt (%)808590Accuracy (%)Stanford Cars
VPT (Baseline)
PC w/o fine-tuning
PC
10 20 30 40 50
#Prompt (%)50607080Accuracy (%)Clevr-count
10 20 30 40 50
#Prompt (%)404550Accuracy (%)DMLab
10 20 30 40 50
#Prompt (%)406080Accuracy (%)dSprites-location
10 20 30 40 50
#Prompt (%)404550Accuracy (%)dSprites-orientation
10 20 30 40 50
#Prompt (%)2025303540Accuracy (%)SmallNORB-azimuth
10 20 30 40 50
#Prompt (%)303540Accuracy (%)SmallNORB-elevation
10 20 30 40 50
#Prompt (%)60708090Accuracy (%)SVHNFigure 4: The test accuracy of VPT-Deep [ 17],PC w/o fine-tuning , and our proposed PCwith respect to the number of prompts.
We use the ViT-B/16 backbone. A dotted line represents the accuracy with 100% prompts.
10 20 30 40 50
#Prompt (%)25303540Accuracy (%)dSprites-orientation
VPT (Baseline)
PC w/o fine-tuning
PC
10 20 30 40 50
#Prompt (%)10152025Accuracy (%)SmallNORB-azimuth
10 20 30 40 50
#Prompt (%)2025303540Accuracy (%)SmallNORB-elevation
10 20 30 40 50
#Prompt (%)6065707580Accuracy (%)SVHN
Figure 5: The test accuracy of VPT-Shallow [ 17],PC w/o fine-tuning , and our proposed PCwith respect to the number of
prompts. We use the ViT-B/16 backbone. A dotted line represents the accuracy with 100% prompts.
6. Experiments
6.1. Experiment Setting
Architecture. We conduct experiments using two trans-
former architectures pre-trained on ImageNet-22k, i.e., Vi-
sion Transformer (ViT-B/16) [ 8] and Swin Transformer
(Swin-B) [26].
Dataset. We use the FGVC and VTAB-1k tasks as our
datasets. FGVC consists of 5 datasets, including CUB-200-
2011 [ 39], NABirds [ 37], Oxford Flowers [ 29], Stanford
Dogs [ 20], and Stanford Cars [ 12]. VTAB-1k [ 44] contains
19 datasets with various visual domains. Following previous
work [ 44,17], we use the provided 800-200 split of the train-
set for training and report the average accuracy score on tests
within three runs. For both FGVC and VTAB-1k datasets,
we select datasets that show a non-trivial ( i.e.≥1%) ac-
curacy drop with 10% prompts compared to the original
VPT. As a result, we have 8 datasets for ViT: {Stanford Cars,
Clevr-count, DMLab, dSprites-location, dSprites-orientation,
smallNORB-azimuth, smallNORB-elevation, SVHN}, and
5 datasets for Swin: {Clevr-count, Clevr-distance, dSprites-
location, smallNORB-azimuth, SVHN}. The details of
dataset selection are provided in the Supplementary.
We observed that non-trivial performance drops tend tooccur in more challenging downstream tasks. To illustrate
this, we calculated the mean and standard deviation of test ac-
curacies across downstream tasks, separating them into those
with non-trivial ( ≥1%) and trivial ( <1%) performance
drops. These tasks were derived from the FGVC and VTAB-
1k datasets. Our results show that datasets with non-trivial
accuracy drops exhibit an average accuracy of 58.91±20.23%,
while those with trivial accuracy drops demonstrate a higher
average accuracy of 81.96±11.54%.
Hyperparameters. We follow the hyperparameters ( e.g.,
weight decay, learning rate) reported in [ 17]. Each dataset
had a different number of prompts, determined by the pre-
vious work [ 17] that reported the best-performing number
of prompts for each dataset. During the prompt fine-tuning
stage, we turn off dropout and used a ×0.1of the original
VPT learning rate. For the prompt condensation, we retrain
the selected prompts for 20 epochs, which is shorter than the
original VPT training process. In Algorithm 1, we set the
number of epochs Nvfor training VPT to 100, following the
original paper [17].
6.2. Performance Comparison
We first evaluate the effectiveness of Prompt Condensa-
tion (PC) with a limited number of prompts. Specifically,

--- PAGE 7 ---
10 20 30 40 50
#Prompt (%)50607080Accuracy (%)Clevr-count
VPT (Baseline)
PC w/o fine-tuning
PC
10 20 30 40 50
#Prompt (%)5060Accuracy (%)Clevr-distance
10 20 30 40 50
#Prompt (%)5060708090Accuracy (%)dSprites-location
10 20 30 40 50
#Prompt (%)102030Accuracy (%)SmallNORB-azimuth
10 20 30 40 50
#Prompt (%)8590Accuracy (%)SVHNFigure 6: The test accuracy of VPT-Deep [ 17],PC w/o fine-tuning , and our proposed PCwith respect to the number of prompts.
We use the Swin-B backbone. A dotted line represents the accuracy with 100% prompts.
we vary the number of prompts from 10% to50%, where
the notation of k%denotes the use of k%of the number of
prompts reported in [ 17]. We compare the performance of
PC with the following models:
•VPT (baseline): We train the ImageNet pre-trained model
withk%of original prompts.
•PC w/o fine-tuning: From the trained VPT with 100% of
prompts, we compute the importance score (Eq. 16) of each
prompt and select top k%of prompts based on the score and
discard the remainder.
Fig. 4 and 5 present a comparison of the performance of
VPT-Deep and VPT-Shallow, respectively. From the results,
we make the following observations: (1) For VPT-Deep, PC
maintains the performance with only 20∼30% number of
prompts, demonstrating its effectiveness compared to the
naive VPT baseline. (2) The performance gain achieved by
applying PC to VPT-Shallow is comparatively lower than
that of VPT-Deep. This can be attributed to VPT-Shallow
having a smaller number of original prompts, which results
in less room for performance improvement. At the same
time, VPT-Deep yields higher performance than the VPT-
Shallow model. Therefore, we focus on VPT-Deep in this
paper. (3) Interestingly, PC w/o fine-tuning with VPT-Deep
does not demonstrate a significant performance drop with
40∼50% of the original prompts. This suggests that our
prompt importance score accurately reflects the impact of
each prompt on the overall accuracy. (4) For the 10∼30%
regime, there is considerable performance degradation with-
out fine-tuning. However, this can be fully recovered by
fine-tuning prompts, demonstrating fine-tuning is an essen-
tial stage for PC. (5) The results of Swin also provide a
similar trend as ViT, as shown in Fig. 6.
6.3. Experimental Analysis
Design Choice for Prompt Scoring. In our method, we
compute the gradient to evaluate the importance of each
prompt (Eq. 16). Based on this score, we select the top- k%
highest-scored prompts across all layers. To investigate the
effectiveness of our prompt scoring technique, we compare
it with several variants.
•Global Prompt Condensation (ours-Global): Our proposed
method where the top- k%highest-scored prompts are se-
lected across all layers.Datasets Methods 10% 20% 30% 40% 50%
StanfordCars[CLS]-Sim 81.67 82.63 83.35 83.95 84.08
Ours-Local 81.79 82.88 83.51 84.01 84.05
Ours-Global 82.79 84.08 84.10 84.09 84.10
Clevr-count[CLS]-Sim 57.25 63.02 67.51 67.40 67.92
Ours-Local 57.95 67.09 66.04 66.58 66.75
Ours-Global 65.06 67.00 67.26 69.30 69.70
DMLab[CLS]-Sim 45.44 45.02 46.25 46.81 46.74
Ours-Local 45.33 45.25 46.45 45.91 47.06
Ours-Global 45.74 45.34 46.69 46.58 47.12
dSprites-loc[CLS]-Sim 49.84 50.30 67.33 68.54 72.28
Ours-Local 57.95 60.26 63.26 71.30 72.87
Ours-Global 59.62 66.25 73.23 76.99 78.88
dSprites-ori[CLS]-Sim 40.95 45.64 46.68 46.69 46.62
Ours-Local 44.09 45.08 46.71 46.33 46.76
Ours-Global 45.59 47.58 47.36 47.36 47.05
SmallNORB-azi[CLS]-Sim 28.93 29.48 32.52 31.87 32.33
Ours-Local 30.29 30.60 31.24 31.80 32.29
Ours-Global 30.96 32.03 31.59 31.15 32.31
SmallNORB-ele[CLS]-Sim 35.83 36.47 37.90 37.62 37.31
Ours-Local 35.66 36.22 36.04 38.41 38.66
Ours-Global 36.24 36.70 36.88 37.83 37.81
SVHN[CLS]-Sim 74.26 75.87 78.06 79.63 80.00
Ours-Local 76.75 78.06 79.18 79.67 79.77
Ours-Global 78.22 78.31 78.43 80.88 80.39
Average[CLS]-Sim 51.77 53.55 57.45 57.81 58.41
Ours-Local 53.72 55.68 56.55 58.00 58.52
Ours-Global 55.52 57.16 58.19 59.27 59.67
Table 3: Test accuracy is evaluated using different prompt
scoring techniques on VPT-Deep with ViT-B/16, with the
best performance highlighted in bold .
•Local Prompt Condensation (ours-Local): Instead of con-
sidering whole layers, we select top- k%scored prompts in
one layer. This approach ensures that the number of selected
prompts is the same across all layers.
•[CLS]-Sim: We adopt the self-attention similarity between
prompt tokens and a [CLS] token as a scoring technique
inspired by a line of previous works [ 25,30,10]. Here, we
also select top- k%highest-scored prompts in one layer.
In Table 3, we present a comparison of the performance
achieved using three different prompt scoring techniques.
The results demonstrate that our proposed global scoring
method, which considers the importance of prompts across
all layers, outperforms the other two scoring techniques, par-
ticularly for lower percentages of PC ( e.g. 10%). Therefore,
our findings suggest that a global scoring metric is necessary
for PC, given that the significance of prompts varies across

--- PAGE 8 ---
123456789101112
Layer index020406080100#Prompts (%)
Prompt Condensation 50%
123456789101112
Layer index020406080100#Prompts (%)
Prompt Condensation 30%
123456789101112
Layer index020406080100#Prompts (%)
Prompt Condensation 10%Figure 7: Layer-wise prompt distribution with three different
prompt condensation levels.
different layers.
Layer-wise prompt distribution. We present the visual-
ization of the layer-wise prompt distribution for PC with
different percentages of prompts (50%, 30%, and 10%) in
Fig. 7. The average number of prompts in each layer is
computed across all datasets, and the mean and standard
deviation are reported. The results indicate that prompts
in the early layers have a minimal impact on the accuracy
for most datasets. Furthermore, reducing the percentage of
prompts leads to higher standard deviation, implying that the
optimal number of prompts varies across datasets. Therefore,
a global PC method is necessary to determine the optimal
number of prompts at each layer across different datasets.
GPU Latency Analysis. We analyze the practical latency
time of VPT with PC on GPUs. Theoretically, the complex-
ity of self-attention operation increases quadratically as the
input token length increases. However, this may not hold
true in practice due to factors such as hardware specifica-
tions [ 17,8]. To investigate the advantage of PC on GPU
latency, we measure the GPU latency for 64 images on three
different GPU environments: Quadro RTX5000, V100, and
A100. We performed experiments on three datasets with
different original numbers of prompts (StanfordCar, DMLab,
and SVHN datasets originally had 200, 100, and 50 prompts,
respectively). In Table 4, we observe that the proposed PC
reduces GPU latency for all configurations. As we expected,
the effectiveness of PC is higher in the case with a larger
number of prompts such as Stanford Cars. Moreover, the
global PC yields similar latency as that of the local PC. This
further corroborates the use of the global importance score
which achieves higher accuracy with negligible computa-
tional overhead. We measure the FLOPs of VPT with PC
in Table 5 to support our observation of the GPU latency,
20 40 60
#Fine-tuning Epochs8090100Accuracy (%) 
VPT Scoring Fine-tuning
Components050100Relative TimeLess than 25% overhead 
Compared to the original 
VPT training(a) (b)
Figure 8: (a) The validation accuracy change with respect
to the number of fine-tuning epochs. (b) Relative time of
original VPT training, scoring, and prompt fine-tuning.
where the results show a similar trend.
Analysis on the Number of Fine-tuning Epochs. One cru-
cial hyperparameter in our method is the number of prompt
fine-tuning epochs ( Npin Algorithm 1). However, longer
fine-tuning periods come with a higher computational cost,
which is incompatible with on-device training scenarios. To
determine the optimal number of fine-tuning epochs, we
measure the average validation accuracy across all down-
stream datasets of VTAB-1K with 10% prompts. As shown
in Fig. 8(a), the accuracy plateaus around epoch 20. Based
on this observation, we set the number of fine-tuning epochs
to 20 for all experiments. In addition, Fig. 8(b) illustrates the
relative computational time between the original VPT train-
ing, prompt scoring, and prompt fine-tuning (line 1, line 2,
and line 4 in Algorithm 1, respectively). The results demon-
strate that our PC method (prompt scoring + fine-tuning)
requires less than 25% of the computational time needed for
the original VPT training. These results indicate that our
method is well-suited for on-device training scenarios.
Practical Implementation of Prompt Condensation. In
practical applications, it may not always be evident if there
is a non-trivial performance drop with a small number of
prompts. In such cases, we can use a relative computa-
tional cost metric (i.e., the ratio of [original image tokens]
to [prompt + original image tokens]) to decide whether to
apply Prompt Condensation (PC). For instance, consider a
scenario with 197 original tokens (196 + [CLS] token) and
100 prompt tokens. In this case, the addition of prompts
results in a computational cost increase of100
197= 50.76%. If
the inclusion of prompts leads to an additional computational
cost of ≥K%, we can opt to implement PC. If not, it would
be more beneficial to skip PC.
7. Conclusion
In this study, our aim is to investigate the influence of
the number of prompts on VPT and its impact on both com-
putational cost and fine-tuning performance. Our findings
show that reducing the number of prompts by approximately
50% does not significantly affect fine-tuned accuracy, with

--- PAGE 9 ---
GPU typeDataset Local Prompt Condensation Global Prompt Condensation
(Unit: ms) 10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 100%
Rtx5000Stanford Cars 356 387 432 473 502 358 391 434 473 501 697
DMLab 334 360 375 386 410 338 352 375 390 410 493
SVHN 329 334 347 356 360 328 336 348 356 360 404
V100Stanford Cars 243 256 281 316 332 245 269 296 320 348 462
DMLab 223 245 256 266 271 227 240 251 264 278 331
SVHN 218 223 227 245 249 218 225 229 248 250 269
A100Stanford Cars 55 59 70 74 80 53 59 65 74 80 118
DMLab 51 55 57 58 60 51 54 56 59 63 80
SVHN 50 52 53 54 56 50 51 52 54 56 60
Table 4: Latency with respect to the number of prompts in VPT [17]. We use ViT-B/16 as a baseline model.
Dataset Local Prompt Condensation Global Prompt Condensation
(Unit: GFLOPs) 10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 100%
Stanford Cars 19.43 21.30 23.18 25.08 26.99 19.43 21.31 23.21 25.12 27.04 36.77
DMLab 18.50 19.43 20.36 21.30 22.24 18.49 19.43 20.36 21.30 22.25 26.99
SVHN 18.04 18.50 18.97 19.43 19.90 18.02 18.50 18.99 19.42 19.90 22.24
Table 5: FLOPs with respect to the number of prompts in VPT [17]. We use ViT-B/16 as a baseline model.
the majority of the performance drop occurring in the 10%
to 40% range. Additionally, we demonstrated that increas-
ing the number of prompts does not linearly enhance the
maximum rank of approximated self-attention matrices. At
the same time, we proposed Prompt Condensation (PC), a
condensation technique that can effectively recover the per-
formance degradation caused by using a small number of
prompts. Overall, we hope our analysis and observations can
provide insight to researchers in designing visual prompts.
References
[1]Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and
Phillip Isola. Exploring visual prompts for adapting large-
scale models. arXiv preprint arXiv:2203.17274 , 1(3):4, 2022.
2
[2]Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
Christoph Feichtenhofer, and Judy Hoffman. Token merging:
Your vit but faster. arXiv preprint arXiv:2210.09461 , 2022. 2
[3]Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl:
Reduce memory, not parameters for efficient on-device learn-
ing. Advances in Neural Information Processing Systems ,
33:11285–11297, 2020. 2
[4]Ayan Chakrabarti and Benjamin Moseley. Backprop with
approximate activations for memory-efficient network train-
ing. Advances in Neural Information Processing Systems , 32,
2019. 2
[5]Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang,
Ion Stoica, Michael Mahoney, and Joseph Gonzalez. Actnn:
Reducing training memory footprint via 2-bit activation com-
pressed training. In International Conference on Machine
Learning , pages 1803–1813. PMLR, 2021. 2
[6]Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib-
ing Song, Jue Wang, and Ping Luo. Adaptformer: Adaptingvision transformers for scalable visual recognition. arXiv
preprint arXiv:2205.13535 , 2022. 2
[7]Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for
dense predictions. arXiv preprint arXiv:2205.08534 , 2022. 2
[8]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 2, 3, 6, 8
[9]R David Evans and Tor Aamodt. Ac-gc: Lossy activation
compression with guaranteed convergence. Advances in Neu-
ral Information Processing Systems , 34:27434–27448, 2021.
2
[10] Mohsen Fayyaz, Soroush Abbasi Koohpayegani,
Farnoush Rezaei Jafari, Sunando Sengupta, Hamid
Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash,
and Jürgen Gall. Adaptive token sampling for efficient
vision transformers. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XI , pages 396–414. Springer, 2022. 2, 5, 7
[11] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia
Shao, Ce Zhang, and Bin Cui. Don’t waste your bits!
squeeze activations and gradients for deep neural networks via
tinyscript. In International Conference on Machine Learning ,
pages 3304–3314. PMLR, 2020. 2
[12] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen,
Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual
census estimation. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 31, 2017. 6
[13] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje,
Venkatesan Chakaravarthy, Yogish Sabharwal, and Ashish
Verma. Power-bert: Accelerating bert inference via progres-

--- PAGE 10 ---
sive word-vector elimination. In International Conference on
Machine Learning , pages 3690–3699. PMLR, 2020. 2
[14] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang,
and Xin Eric Wang. Parameter-efficient fine-tuning for vision
transformers. arXiv preprint arXiv:2203.16329 , 2022. 1, 2
[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 2
[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:
Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021. 1, 2
[17] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXIII , pages 709–727. Springer, 2022. 1,
2, 3, 4, 6, 7, 8, 9
[18] Ziyu Jiang, Xuxi Chen, Xueqin Huang, Xianzhi Du, Denny
Zhou, and Zhangyang Wang. Back razor: Memory-efficient
transfer learning by self-sparsified backpropagation. In Ad-
vances in Neural Information Processing Systems . 2
[19] Shibo Jie and Zhi-Hong Deng. Convolutional bypasses
are better vision transformer adapters. arXiv preprint
arXiv:2207.07039 , 2022. 2
[20] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao,
and Fei-Fei Li. Novel dataset for fine-grained image cat-
egorization: Stanford dogs. In Proc. CVPR workshop on
fine-grained visual categorization (FGVC) , volume 2. Cite-
seer, 2011. 6
[21] Gyuwan Kim and Kyunghyun Cho. Length-adaptive trans-
former: Train once with length drop, use anytime with search.
arXiv preprint arXiv:2010.07003 , 2020. 2
[22] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gho-
lami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer.
Learned token pruning for transformers. arXiv preprint
arXiv:2107.00910 , 2021. 2
[23] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei
Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao
Tang, et al. Spvit: Enabling faster vision transformers via
latency-aware soft token pruning. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, October
23–27, 2022, Proceedings, Part XI , pages 620–640. Springer,
2022. 2
[24] Yann LeCun, John Denker, and Sara Solla. Optimal brain
damage. Advances in neural information processing systems ,
2, 1989. 5
[25] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue
Wang, and Pengtao Xie. Not all patches are what you need:
Expediting vision transformers via token reorganizations.
arXiv preprint arXiv:2202.07800 , 2022. 2, 5, 7
[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 1, 6[27] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,
Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit:
Adaptive vision transformers for efficient image recognition.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12309–12318, 2022.
2, 5
[28] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for
resource efficient inference. arXiv preprint arXiv:1611.06440 ,
2016. 5
[29] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing , pages 722–729. IEEE, 2008. 6
[30] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision trans-
formers with dynamic token sparsification. Advances in neu-
ral information processing systems , 34:13937–13949, 2021.
2, 5, 7
[31] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Efficient parametrization of multi-domain deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 8119–8127, 2018. 1, 2
[32] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671 , 2016. 2
[33] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng
Jing, and Xiaoyao Liang. Cp-vit: Cascade vision transformer
pruning via progressive sparsity prediction. arXiv preprint
arXiv:2203.04570 , 2022. 2
[34] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder
side-tuning for parameter and memory efficient transfer learn-
ing. arXiv preprint arXiv:2206.06522 , 2022. 2
[35] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter:
Parameter-efficient transfer learning for vision-and-language
tasks. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5227–5237, 2022.
2
[36] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
Efficient transformers: A survey. ACM Computing Surveys ,
55(6):1–28, 2022. 2
[37] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber,
Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Be-
longie. Building a bird recognition app and large scale dataset
with citizen scientists: The fine print in fine-grained dataset
collection. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 595–604, 2015.
6
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1
[39] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona,
and Serge Belongie. The caltech-ucsd birds-200-2011 dataset.
2011. 6

--- PAGE 11 ---
[40] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 4
[41] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya,
Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for
efficient vision transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10809–10818, 2022. 2, 5
[42] Hao Yu and Jianxin Wu. A unified pruning framework for
vision transformers. arXiv preprint arXiv:2111.15127 , 2021.
1, 2
[43] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:
Simple parameter-efficient fine-tuning for transformer-based
masked language-models. arXiv preprint arXiv:2106.10199 ,
2021. 2
[44] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre
Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, An-
dre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al.
A large-scale study of representation learning with the visual
task adaptation benchmark. arXiv preprint arXiv:1910.04867 ,
2019. 4, 6
[45] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas
Guibas, and Jitendra Malik. Side-tuning: a baseline for net-
work adaptation via additive side networks. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part III 16 , pages
698–714. Springer, 2020. 1, 2
[46] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kun-
chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-
adapter: Training-free clip-adapter for better vision-language
modeling. arXiv preprint arXiv:2111.03930 , 2021. 1, 2
[47] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural
prompt search. arXiv preprint arXiv:2206.04673 , 2022. 2
[48] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. Interna-
tional Journal of Computer Vision , 130(9):2337–2348, 2022.
1, 2
