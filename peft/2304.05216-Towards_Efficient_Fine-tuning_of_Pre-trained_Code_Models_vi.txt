# 2304.05216.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2304.05216.pdf
# Kích thước tệp: 1080193 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện:
Một Nghiên cứu Thực nghiệm và Hơn thế nữa
Ensheng Shia,†Yanlin Wangb,§,†Hongyu Zhangc
Lun DudShi HandDongmei ZhangdHongbin Suna,§
aĐại học Giao thông Tây AnbTrường Kỹ thuật Phần mềm, Đại học Trung Sơn
cĐại học Trùng KhànhdMicrosoft
s1530129650@stu.xjtu.edu.cn, wangylin36@mail.sysu.edu.cn, hyzhang@cqu.edu.cn
{lun.du, shihan, dongmeiz}@microsoft.com, hsun@mail.xjtu.edu.cn
TÓM TẮT
Gần đây, fine-tuning các mô hình code được tiền huấn luyện như CodeBERT trên các tác vụ downstream đã đạt được thành công lớn trong nhiều tác vụ kiểm thử và phân tích phần mềm. Mặc dù hiệu quả và phổ biến, fine-tuning các tham số được tiền huấn luyện gây ra chi phí tính toán lớn. Trong bài báo này, chúng tôi tiến hành một nghiên cứu thực nghiệm mở rộng để khám phá điều gì xảy ra với các biểu diễn được tiền huấn luyện theo từng lớp và kiến thức code được mã hóa của chúng trong quá trình fine-tuning. Sau đó chúng tôi đề xuất các phương án thay thế hiệu quả để fine-tune mô hình code được tiền huấn luyện lớn dựa trên các phát hiện trên. Nghiên cứu thực nghiệm của chúng tôi cho thấy rằng (1) các thuộc tính từ vựng, cú pháp và cấu trúc của mã nguồn được mã hóa trong các lớp thấp hơn, trung gian và cao hơn tương ứng, trong khi thuộc tính ngữ nghĩa trải rộng trên toàn bộ mô hình. (2) Quá trình fine-tuning bảo tồn hầu hết các thuộc tính code. Cụ thể, các thuộc tính code cơ bản được nắm bắt bởi các lớp thấp hơn và trung gian vẫn được bảo tồn trong quá trình fine-tuning. Hơn nữa, chúng tôi thấy rằng chỉ có các biểu diễn của hai lớp trên cùng thay đổi nhiều nhất trong quá trình fine-tuning cho các tác vụ downstream khác nhau. (3) Dựa trên các phát hiện trên, chúng tôi đề xuất Telly để fine-tune hiệu quả các mô hình code được tiền huấn luyện thông qua đóng băng lớp. Các kết quả thực nghiệm mở rộng trên năm tác vụ downstream khác nhau chứng minh rằng các tham số huấn luyện và chi phí thời gian tương ứng được giảm đáng kể, trong khi hiệu suất tương tự hoặc tốt hơn.
KHÁI NIỆM CCS
•Phần mềm và kỹ thuật của nó →Kỹ thuật phát triển phần mềm; Khả năng tái sử dụng.
TỪ KHÓA
Nghiên cứu thực nghiệm, Mô hình Ngôn ngữ Được Tiền huấn luyện, Fine-tuning Hiệu quả, Kỹ thuật Probing, Phân tích Tương đồng Biểu diễn
Được phép tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này cho mục đích cá nhân hoặc lớp học mà không mất phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao mang thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của những người khác ngoài ACM phải được tôn trọng. Trích dẫn có ghi tín được phép. Để sao chép theo cách khác, hoặc xuất bản lại, để đăng trên máy chủ hoặc để phân phối lại danh sách, yêu cầu sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA
©2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX1 GIỚI THIỆU
Gần đây, paradigm tiền huấn luyện với fine-tuning [2,15,17,34] đã đạt được cải thiện đáng kể trong nhiều tác vụ kiểm thử và phân tích phần mềm như phát hiện lỗ hổng [13,17], tạo patch [9,18], sửa chữa chương trình tự động [27], đánh giá code [42, 55], tạo code [2,9,17], và phát hiện clone [13,17,18]. Chúng đầu tiên tiền huấn luyện các mô hình lớn dựa trên Transformer để học các biểu diễn code mục đích chung trên một lượng lớn dữ liệu. Sau đó, để thích ứng các mô hình này với các tác vụ downstream, chúng thường fine-tune chúng trên các tác vụ mục tiêu [2, 15, 17, 18, 40, 55, 63].

Trong paradigm này, fine-tuning các mô hình code được tiền huấn luyện thường đạt được kết quả tốt hơn rất nhiều trên các tác vụ downstream. Mặc dù hiệu quả, fine-tuning các tham số được tiền huấn luyện gây ra chi phí tính toán lớn với mức tiêu thụ năng lượng tương tự lớn. Như được báo cáo trong CodeXGLUE [34], chúng thường yêu cầu hơn 10 giờ để fine-tune mô hình được tiền huấn luyện trên một máy có hai card P100 cho các tác vụ downstream. Đặc biệt, khi các mô hình được tiền huấn luyện hoặc bộ dữ liệu được fine-tuned trở nên lớn hơn, chi phí tính toán trở nên đắt đỏ hơn. Ví dụ, CodeT5 [63] có khoảng 220MB tham số mất hơn 40 giờ fine-tuning nó trên bộ dữ liệu CONCODE [24] để tạo code. Trên thực tế, những hành động này trái ngược với deep learning carbon thấp [48]. Trong lĩnh vực kỹ thuật phần mềm, chỉ có một số ít nghiên cứu khám phá điều gì sẽ xảy ra với các mô hình code được tiền huấn luyện trong quá trình fine-tuning.

Hầu hết các nghiên cứu liên quan [15,21,30,54,58] nhằm hiểu những gì các mô hình code được tiền huấn luyện biết về mã nguồn. Có một nhu cầu rõ ràng để hiểu điều gì xảy ra với các mô hình code được tiền huấn luyện trong quá trình fine-tuning và hơn nữa thích ứng hiệu quả các mô hình được tiền huấn luyện với các tác vụ downstream với chi phí tính toán ít hơn.

Trong bài báo này, trước tiên chúng tôi khám phá các thuộc tính code nào được mã hóa trong các biểu diễn theo từng lớp của các mô hình code được tiền huấn luyện và điều gì xảy ra với các biểu diễn này trong quá trình fine-tuning. Sau đó, chúng tôi đề xuất một số phương án thay thế hiệu quả cho fine-tuning cho các mô hình code được tiền huấn luyện dựa trên các phát hiện trên. Cụ thể, đầu tiên, được truyền cảm hứng từ quá trình biên dịch [3] và các kỹ thuật phân tích chương trình tĩnh [38], chúng tôi đề xuất bốn tác vụ probing (được giới thiệu trong Phần 3.2.1) liên quan đến các thuộc tính code từ vựng, cú pháp, ngữ nghĩa và cấu trúc. Tiếp theo, chúng tôi tiến hành một nghiên cứu thực nghiệm để khám phá các thuộc tính code nào được mã hóa trong các mô hình code được tiền huấn luyện và những đóng góp của các lớp khác nhau là gì đối với việc hiểu

§Yanlin Wang và Hongbin Sun là các tác giả tương ứng.
†Công việc được thực hiện trong thời gian tác giả làm việc tại Microsoft Research Asia.arXiv:2304.05216v1 [cs.SE] 11 Tháng 4 2023

--- TRANG 2 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

các thuộc tính được mã hóa. Hơn nữa, chúng tôi tiến hành một nghiên cứu thực nghiệm mở rộng để đi sâu vào điều gì xảy ra với các biểu diễn theo từng lớp trong quá trình fine-tuning trên năm tác vụ downstream đa dạng (được hiển thị trong Bảng 1) bao gồm tìm kiếm code [16, 46], phát hiện clone [11, 49], tóm tắt code [47,61], tạo code [25], và hoàn thành code cấp dòng [34]. Thông qua các thí nghiệm mở rộng, chúng tôi có được các phát hiện chính sau đây về các mô hình code được tiền huấn luyện.

Phát hiện chính đầu tiên là các mô hình code được tiền huấn luyện mã hóa thuộc tính từ vựng của mã nguồn chủ yếu trong các lớp thấp hơn, nhận biết thuộc tính cú pháp chủ yếu trong các lớp trung gian, và hiểu thuộc tính cấu trúc chủ yếu trong các lớp cao hơn. Các thuộc tính ngữ nghĩa được nhận thức trên các lớp trong toàn bộ mô hình. Phát hiện chính thứ hai là quá trình fine-tuning bảo tồn hầu hết các thuộc tính code. Nghĩa là, trong quá trình fine-tuning, kiến thức code cơ bản (hoặc thuộc tính) được mã hóa trong các lớp thấp hơn và trung gian vẫn được bảo tồn. Chỉ có kiến thức được nắm bắt bởi các lớp cao hơn thay đổi nhiều nhất. Ngoài ra, nghiên cứu thực nghiệm của chúng tôi chứng minh rằng, khi fine-tuning các mô hình được tiền huấn luyện trên năm tác vụ downstream đa dạng, các biểu diễn của các lớp thấp hơn thay đổi nhẹ, chỉ có hai lớp trên cùng hiển thị những thay đổi đáng kể.

Dựa trên các phát hiện trên, chúng tôi đề xuất Telly-𝐾, để fine-tuning hiệu quả các mô hình code được tiền huấn luyện thông qua đóng băng lớp. Các giá trị 𝐾 khác nhau có nghĩa là các biến thể khác nhau của phương pháp của chúng tôi. Cụ thể, chúng tôi giảm các tham số được huấn luyện thông qua đóng băng các tham số được tiền huấn luyện của 𝐾 lớp dưới cùng thay đổi không đáng kể trong quá trình fine-tuning, trong đó 𝐾∈[0,1,2,3,...,𝐿-1], lớp thứ 0 là lớp embedding, và 𝐿 là số lượng tối đa (thường là 12) của các lớp của mô hình code được tiền huấn luyện. Do đó, Telly-1 có nghĩa là đóng băng embedding và lớp encoder thứ 1. Chúng tôi tiến hành các thí nghiệm mở rộng trên năm tác vụ downstream khác nhau từ ba khía cạnh bao gồm tham số huấn luyện, chi phí thời gian và hiệu suất. Các mô hình code được tiền huấn luyện được đánh giá có 12 lớp ẩn. Các kết quả thực nghiệm cho thấy rằng (1) đối với hầu hết tất cả Telly-𝐾 (0≤𝐾≤11), chi phí thời gian huấn luyện và tham số được giảm đáng kể, mà không có thay đổi đáng kể trong hiệu suất mô hình. (2) Khi đóng băng 𝐾 lớp dưới cùng (0≤𝐾≤5), tham số huấn luyện được giảm khoảng 30% đến 65%, và thời gian huấn luyện được tiết kiệm tương ứng khoảng 10% đến 75%. Hiệu suất mô hình thường tăng 1% đến 4% cho các tác vụ downstream khác nhau. (3) Khi đóng băng 𝐾 lớp dưới cùng (6≤𝐾≤9), tham số huấn luyện được giảm 65% đến 80%, tương ứng tiết kiệm khoảng 50% đến 80% thời gian huấn luyện, trong khi hiệu suất mô hình chỉ thay đổi nhẹ. (4) Khi số lượng lớp bị đóng băng lớn hơn chín (10≤𝐾≤11), tham số huấn luyện và chi phí thời gian huấn luyện tương ứng được giảm rất nhiều, trong khi hiệu suất mô hình cũng giảm đáng kể.

Các đóng góp chính của chúng tôi được tóm tắt như sau:
•Chúng tôi đề xuất bốn tác vụ probing liên quan đến các thuộc tính code từ vựng, cú pháp, ngữ nghĩa và cấu trúc. Chúng tôi khám phá cái gì và làm thế nào các thuộc tính code được mã hóa trong các biểu diễn theo từng lớp thông qua các tác vụ probing trên.
•Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên tiến hành một nghiên cứu thực nghiệm mở rộng để phân tích điều gì xảy ra với các biểu diễn theo từng lớp và các thuộc tính code được mã hóa của chúng trong quá trình fine-tuning của các mô hình code được tiền huấn luyện.
•Chúng tôi đề xuất một phương pháp hiệu quả để fine-tune các mô hình code được tiền huấn luyện cho các tác vụ downstream thông qua đóng băng lớp. Ngoài ra, chúng tôi tiến hành các thí nghiệm mở rộng trên năm tác vụ downstream khác nhau để chứng minh hiệu quả của phương pháp của chúng tôi.

Phần còn lại của bài báo này được tổ chức như sau. Phần 2 giới thiệu kiến thức nền tảng liên quan. Sau đó, chúng tôi tiến hành một nghiên cứu thực nghiệm để hiểu điều gì xảy ra với các mô hình code được tiền huấn luyện trong quá trình fine-tuning trong Phần 3. Dựa trên các phát hiện trên, trong Phần 4, chúng tôi đề xuất Telly-𝐾 và tiến hành các thí nghiệm mở rộng trên năm tác vụ downstream khác nhau để hiển thị sự ưu việt của nó. Phần 5 thảo luận về tầm quan trọng của việc giảm thời gian fine-tuning, hướng dẫn có thể thực hiện để fine-tuning tốt hơn, và tính tổng quát của các phát hiện thực nghiệm của chúng tôi, và xác định một số mối đe dọa đối với tính hợp lệ. Phần 6 trình bày công việc liên quan. Cuối cùng, chúng tôi tóm tắt bài báo của mình và thảo luận về công việc tương lai trong Phần 7.

2 KIẾN THỨC NỀN TẢNG
2.1 Các Mô hình Code Được Tiền huấn luyện

Các mô hình được tiền huấn luyện lớn đã đạt được kết quả đáng kể trong nhiều lĩnh vực bao gồm xử lý ngôn ngữ tự nhiên [12,33], thị giác máy tính [7,19] và kỹ thuật phần mềm [2,15,17,18,64]. Trong cộng đồng kỹ thuật phần mềm, nói chung, chúng đầu tiên tiền huấn luyện các mô hình lớn trên lượng dữ liệu liên quan đến mã nguồn, và sau đó fine-tune chúng trên các tác vụ downstream để cải thiện hiệu suất của chúng. Gần đây, nhiều mô hình code được tiền huấn luyện [2,15,17,18,40,63] đã được đề xuất và hiển thị các kết quả hứa hẹn đáng ngạc nhiên trên nhiều tác vụ kỹ thuật phần mềm liên quan đến kiểm thử phần mềm, bảo mật, bảo trì và phát triển [13,17,34,42,55]. Chúng tôi giới thiệu các mô hình này từ ba khía cạnh như sau.

Kiến trúc cơ bản. Hầu hết các mô hình code được tiền huấn luyện gần đây áp dụng mô hình Transformer đa lớp [56] (thường là một encoder Transformer) làm kiến trúc cơ bản. Một encoder Transformer về cơ bản được cấu tạo từ một lớp embedding, một bộ mã hóa vị trí, và một chồng các lớp encoder. Nói chung, với một đoạn code đầu vào, nó đầu tiên được nhúng bởi lớp embedding và bộ mã hóa vị trí để có được các embedding từ ban đầu. Tiếp theo, chúng được đưa vào nhiều lớp encoder được xếp chồng để mã hóa thông tin đầu vào theo từng lớp.

Về mặt toán học, chúng tôi ký hiệu các token đầu vào là 𝑇=[𝑡1,𝑡2,...,𝑡𝑛], trong đó 𝑛 là độ dài của chuỗi token đầu vào. Lớp embedding ánh xạ mỗi token vào một không gian ngữ nghĩa đa chiều cao. Bộ mã hóa vị trí được sử dụng để mã hóa thông tin vị trí và sau đó tiêm nó vào embedding đầu vào bằng:

𝑤𝑖=𝑒𝑚𝑏𝑒𝑑(𝑡𝑖)+𝑝𝑜𝑠(𝑡𝑖), 𝑖=1,2,...,𝑛 (1)

trong đó 𝑒𝑚𝑏𝑒𝑑(∗) và 𝑝𝑜𝑠(∗) ký hiệu lớp embedding và bộ mã hóa vị trí tương ứng. 𝑊=[𝑤1,𝑤2,...,𝑤𝑛] là các embedding từ ban đầu. Tiếp theo, nhiều lớp encoder được xếp chồng tạo ra một tập hợp các biểu diễn ngữ cảnh theo từng lớp 𝐻0,𝐻1,...,𝐻𝐿 bằng:

𝐻0=[𝑤1,𝑤2,...,𝑤𝑛]
𝐻𝑙=𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑙(𝐻𝑙−1), 𝑙=1,2,...,𝐿(2)

trong đó 𝐿 là số lượng lớp được xếp chồng, và 𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑙(∗) ký hiệu lớp encoder thứ 𝑙. 𝐻𝑙=[ℎ𝑙1,ℎ𝑙2,...,ℎ𝑙𝑛] ký hiệu các biểu diễn ngữ cảnh của lớp thứ 𝑙, và 𝐻0 là các embedding từ ban đầu.

--- TRANG 3 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

Tiền huấn luyện. Các kỹ thuật tiền huấn luyện, như một trong những phương pháp học tự giám sát, có thể tận dụng một mô hình lớn để học các biểu diễn tổng quát với lượng bộ dữ liệu không được gán nhãn [12,19,33,43]. Thường thì, các kỹ thuật như vậy thường tự động tạo ra các nhãn ảo từ các mẫu không được gán nhãn để tái cấu trúc một vấn đề học không giám sát thành một vấn đề học có giám sát. Các tác vụ có giám sát tương ứng được đặt tên là các tác vụ được tiền huấn luyện. Ví dụ, CodeBERT [15] sử dụng một encoder Transformer 12 lớp với embedding 768 chiều và tiền huấn luyện tất cả các tham số trên một bộ dữ liệu quy mô lớn có tên CodeSearchNet [23] (chứa 2.1M dữ liệu đa phương thức (các hàm code được ghép nối với các bình luận ngôn ngữ tự nhiên) và 6.4M code đơn phương thức trên sáu ngôn ngữ lập trình (Ruby, JavaScript, Go, Python, Java, PHP) với hai tác vụ được tiền huấn luyện, cụ thể là mô hình hóa ngôn ngữ có mặt nạ và phát hiện token được thay thế. Tác vụ đầu tiên là dự đoán các token gốc của các vị trí bị che, trong khi tác vụ sau là xác định một token có phải là token gốc hay không. UniXcoder [17] lấy chuỗi code/text làm đầu vào và được tiền huấn luyện trên bộ dữ liệu C4 từ T5 [43] và 4.1M code đơn phương thức từ CodeSearchNet với năm tác vụ được tiền huấn luyện khác nhau.

Fine-tuning. Sau khi tiền huấn luyện trên bộ dữ liệu khổng lồ, chúng thích ứng các mô hình được tiền huấn luyện với các tác vụ downstream bằng cách fine-tuning tất cả các tham số được tiền huấn luyện trên bộ dữ liệu mục tiêu [2,14,15,17, 18,40,63]. Ví dụ, trong tìm kiếm code, các nghiên cứu trước đây [17,34] lấy trung bình các biểu diễn ngữ cảnh lớp cuối cùng (ví dụ 𝐻𝐿) của các mô hình làm biểu diễn tổng thể, đo độ tương tự giữa các biểu diễn của mã nguồn và truy vấn bằng khoảng cách vector, và fine-tune chúng bằng cách kéo lại gần nhau code và truy vấn được ghép nối và đẩy xa code và truy vấn không được ghép nối. So với các mô hình được tiền huấn luyện, các giá trị MRR của các mô hình được fine-tuned trên tìm kiếm code được cải thiện từ 0.001 và 0.156 lên 0.694 và 0.713 cho CodeBERT và GraphCodeBERT tương ứng.

2.2 Các Kỹ thuật Probing

Các kỹ thuật probing đã được sử dụng rộng rãi trong cộng đồng NLP để nghiên cứu những thuộc tính ngôn ngữ nào được nắm bắt bởi các mô hình ngôn ngữ được tiền huấn luyện. Cụ thể, chúng trích xuất các biểu diễn ngữ cảnh (như 𝐻1,𝐻2) từ mô hình được tiền huấn luyện làm các đặc trưng bị đóng băng, đưa chúng vào một bộ phân loại tuyến tính, và chỉ huấn luyện nó để dự đoán các tác vụ probing liên quan đến các thuộc tính ngôn ngữ. Chúng cũng lấy các biểu diễn ngẫu nhiên làm đường cơ sở để chứng minh khả năng của các biểu diễn được tiền huấn luyện để mã hóa các thuộc tính ngôn ngữ để so sánh. Ví dụ, Tenney et al. [53] sử dụng các tác vụ probing khác nhau như gắn thẻ từ loại, phân tích cú pháp phụ thuộc, gắn nhãn vai trò ngữ nghĩa, và gắn nhãn đồng tham chiếu để kiểm tra khả năng của các mô hình được tiền huấn luyện để hiểu các thuộc tính ngôn ngữ, như từ loại, phụ thuộc, ngữ nghĩa, và đồng tham chiếu. Trong lĩnh vực kỹ thuật phần mềm, hầu hết các nghiên cứu liên quan [21,30,58] nhằm hiểu những gì các mô hình code được tiền huấn luyện biết về mã nguồn. Ví dụ, Wan et al. [58] đề xuất một tác vụ probing mới để điều tra liệu cấu trúc của AST có được mã hóa trong các biểu diễn của các mô hình code được tiền huấn luyện không. Cụ thể, chúng trích xuất các biểu diễn được tiền huấn luyện theo từng lớp của hai token code đầu vào từ các mô hình code được tiền huấn luyện, đưa chúng vào một ma trận, và huấn luyện ma trận để tái tạo khoảng cách giữa hai nút đầu cuối tương ứng trong AST, trong đó khoảng cách được sử dụng để đại diện cho cấu trúc cú pháp. López et al [21] đề xuất một tác vụ probing mới có tên AST-Probe, phục hồi AST từ các biểu diễn ẩn của các mô hình ngôn ngữ được tiền huấn luyện. Cụ thể, AST-probe đầu tiên ánh xạ các biểu diễn theo từng lớp của các mô hình code được tiền huấn luyện vào một không gian tiềm ẩn, được gọi là không gian con cú pháp, bằng cách sử dụng phép chiếu trực giao và sau đó sử dụng hình học của không gian này để dự đoán AST của đoạn code đầu vào. Karmakar et al [30] cũng đề xuất một số tác vụ probing mới để điều tra liệu các mô hình ngôn ngữ được tiền huấn luyện có thể hiểu một số thuộc tính code đơn giản không. Được thúc đẩy bởi quá trình biên dịch [3] và các kỹ thuật phân tích tĩnh [38], chúng tôi đề xuất bốn tác vụ probing (Phần 3.2.1) liên quan đến các thuộc tính code khác nhau. Ngoài việc probing các mô hình code được tiền huấn luyện, chúng tôi cũng nghiên cứu các biểu diễn theo từng lớp của chúng trong quá trình fine-tuning.

2.3 Phân tích Tương đồng Biểu diễn

Phân tích tương đồng biểu diễn (RSA) ban đầu được sử dụng trong khoa học thần kinh nhận thức [31] để nghiên cứu mối quan hệ giữa các mẫu kích hoạt thần kinh trong não người và các biểu diễn của một mô hình tính toán cho một tập hợp các kích thích. Gần đây, nó được áp dụng để đo độ tương tự giữa hai không gian biểu diễn [1,10,37]. Ví dụ, với một tập hợp đầu vào, các mô hình khác nhau tạo ra các không gian biểu diễn khác nhau sẽ tạo ra các biểu diễn khác nhau. Merchant et al. [37] xây dựng hai ma trận khoảng cách. Mỗi cái ghi lại các khoảng cách vector (như độ tương tự cosine) giữa các biểu diễn trong một không gian biểu diễn. Sau đó độ tương tự biểu diễn của hai không gian biểu diễn được đo bằng hệ số tương quan Pearson [45] của hai ma trận khoảng cách này. Nói chung, một giá trị của hệ số tương quan giữa 0.8 và 1 cho biết rằng hai không gian biểu diễn khá tương tự, trong khi một giá trị thấp hơn 0.5 có nghĩa là hai không gian biểu diễn không tương tự [4,45]. Trong bài báo này, chúng tôi tiến hành phân tích tương đồng biểu diễn để nghiên cứu sự tương tự của các biểu diễn theo từng lớp giữa các mô hình được tiền huấn luyện và fine-tuned trong Phần 3.3 và 3.5

3 MỘT NGHIÊN CỨU THỰC NGHIỆM VỀ MÔ HÌNH CODE ĐƯỢC TIỀN HUẤN LUYỆN
3.1 Các Câu hỏi Nghiên cứu

Mặc dù hiệu quả và phổ biến, fine-tuning các mô hình code được tiền huấn luyện gây ra chi phí tính toán lớn. Trong công việc này, trước tiên chúng tôi tiến hành một nghiên cứu thực nghiệm để điều tra các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp và điều gì xảy ra với các biểu diễn này trong quá trình fine-tuning. Các câu hỏi nghiên cứu về nghiên cứu thực nghiệm được giới thiệu chi tiết như sau.

RQ1: Các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp? Các kỹ thuật probing đã được sử dụng rộng rãi trong cộng đồng NLP để phân tích và diễn giải các mô hình ngôn ngữ được tiền huấn luyện. Được thúc đẩy bởi quá trình biên dịch [3] và các kỹ thuật phân tích tĩnh [38], trước tiên chúng tôi đề xuất bốn tác vụ probing liên quan đến các thuộc tính từ vựng, cú pháp, ngữ nghĩa và cấu trúc của mã nguồn. Chúng được giới thiệu chi tiết trong Phần 3.2.1. Sau đó, chúng tôi điều tra các thuộc tính code nào được mã hóa bởi các biểu diễn được tiền huấn luyện theo từng lớp thông qua các tác vụ probing trên trong Phần 3.2.2. Đồng thời, chúng tôi nghiên cứu mức độ đóng góp của các biểu diễn của mỗi lớp đối với việc hiểu các thuộc tính code này. Hơn nữa, chúng tôi thực hiện các thí nghiệm probing tương tự cho mô hình được fine-tuned trong một tác vụ downstream trong Phần 3.5, và hiểu một cách trực quan điều gì xảy ra với các thuộc tính code được nắm bắt bởi mô hình được tiền huấn luyện trong quá trình fine-tuning.

RQ2: Điều gì xảy ra với các biểu diễn theo từng lớp trong quá trình fine-tuning? Trong RQ1, chúng tôi nhằm hiểu một cách thô về điều gì xảy ra với một mô hình code được tiền huấn luyện khi fine-tuning với sự giúp đỡ của các tác vụ probing. Chúng tôi tiếp tục tiến hành phân tích tương đồng biểu diễn (RSA) mở rộng để nghiên cứu điều gì xảy ra với các biểu diễn được tiền huấn luyện từng lớp khi fine-tuning chúng trên các tác vụ downstream. RSA được giới thiệu trong Phần 2.3 là một kỹ thuật bất khả tri tác vụ và không yêu cầu kiến thức trước về các tác vụ probing. Cách áp dụng RSA cho các mô hình được tiền huấn luyện và fine-tuned được mô tả trong Phần 3.3. Để đảm bảo tính tổng quát của các phát hiện thực nghiệm của chúng tôi, chúng tôi tiến hành các thí nghiệm trên năm tác vụ downstream đa dạng bao gồm tìm kiếm code, phát hiện clone, tóm tắt code, tạo code, và hoàn thành code cấp dòng.

3.2 Probing các Mô hình Code Được Tiền huấn luyện
Chúng tôi giới thiệu bốn tác vụ probing liên quan đến code và pipeline probing như sau.

3.2.1 Bốn tác vụ probing. Chúng tôi thiết kế và hiển thị bốn tác vụ probing trong Hình 1. Chúng liên quan đến các thuộc tính code từ vựng, cú pháp, ngữ nghĩa và cấu trúc. Chúng tôi giới thiệu chúng từng cái một chi tiết.

Probing từ vựng. Probing từ vựng nhằm đo mức độ tốt mà các biểu diễn ngữ cảnh mã hóa các thuộc tính từ vựng của mã nguồn. Như chúng ta đều biết, khi mã nguồn được biên dịch, bước đầu tiên là phân tích từ vựng, tokenize chuỗi mã nguồn và xác định loại (như Định danh, Từ khóa) của mỗi token code. Các loại khác nhau đóng các vai trò ngữ nghĩa hoặc cú pháp rất khác nhau trong phân tích chương trình và biên dịch tiếp theo. Do đó, điều quan trọng là hiểu liệu các mô hình code được tiền huấn luyện có nắm bắt được thông tin từ vựng của mã nguồn bằng các biểu diễn ngữ cảnh không. Để đạt được điều này, trước tiên chúng tôi sử dụng các biểu diễn ngữ cảnh của các mô hình code được tiền huấn luyện làm các đặc trưng đóng băng, sau đó đưa chúng vào một bộ phân loại tuyến tính, và cuối cùng huấn luyện nó để dự đoán loại của mỗi token code. Như được hiển thị trong Hình 1(d), mỗi token thuộc về một trong năm loại bao gồm Định danh, Từ khóa, Toán tử, Số, và Chuỗi. Do hạn chế về không gian, định nghĩa chi tiết của mỗi loại và mô tả của probing từ vựng có thể được tìm thấy trong Phụ lục trực tuyến của gói sao chép [51].

Probing cú pháp. Phân tích cú pháp thường đến sau phân tích từ vựng trong quá trình biên dịch chương trình [3], trong đó một parser lấy chuỗi token được tạo ra bởi lexer làm đầu vào và tạo ra các cấu trúc dữ liệu như cây phân tích cú pháp hoặc cây cú pháp trừu tượng (AST). Tương tự, probing cú pháp được thiết kế để điều tra mức độ tốt mà các biểu diễn ngữ cảnh nhận thức các thuộc tính cú pháp của mã nguồn. Ý tưởng cơ bản là xác định liệu một code và một AST ẩn danh (được đặt tên AST-Only được hiển thị trong Hình 1(b)) có được ghép nối không. Cụ thể, trước tiên chúng tôi phân tích mã nguồn để có được AST tương ứng, bao gồm các nút không đầu cuối và đầu cuối. Các nút không đầu cuối đại diện cho thông tin cú pháp, trong khi các nút đầu cuối bao gồm các loại tương ứng với các phần tử cú pháp và các giá trị tương ứng với các token code trong mã nguồn. Thật dễ dàng cho một mô hình xác định liệu AST có được phân tích bởi một đoạn code theo sự chồng chéo của các token code. Do đó, như được hiển thị trong Hình 1(b), chúng tôi xây dựng AST-Only bằng cách loại bỏ các giá trị của các nút đầu cuối. Tiếp theo, chúng tôi huấn luyện một bộ phân loại tuyến tính để xác định liệu AST-Only đã cho có được phân tích từ đoạn code đã cho hay không. Các cặp đúng được xây dựng bằng cách ghép nối code với AST-Only được phân tích tương ứng, trong khi các cặp sai được xây dựng bằng cách ghép nối code với AST-Only được phân tích bởi các code khác nhau khác. Mô tả chi tiết của probing cú pháp có thể được tìm thấy trong Phụ lục trực tuyến [51].

Probing ngữ nghĩa. Để hiểu mức độ mà các mô hình code được tiền huấn luyện nhận thức về ngữ nghĩa code, chúng tôi thực hiện probing ngữ nghĩa (Hình 1(e)), kiểm tra khả năng xác định các đoạn code có cùng ngữ nghĩa nhưng các triển khai khác nhau. Cụ thể, chúng tôi sử dụng bộ dữ liệu POJ-104 [39], bao gồm 104 bài toán và 500 triển khai C/C++ cho mỗi bài toán, làm bộ dữ liệu được đánh giá. Chúng tôi huấn luyện một mapper tuyến tính lấy các biểu diễn được tiền huấn luyện làm đầu vào để ánh xạ các đoạn code tương tự về ngữ nghĩa vào các embedding tương tự. Do đó, các triển khai có cùng ngữ nghĩa có thể dễ dàng được thu hồi bằng khoảng cách vector của chúng. Mô tả chi tiết của probing ngữ nghĩa có thể được tìm thấy trong Phụ lục trực tuyến [51].

Probing cấu trúc. Ngoài các thuộc tính từ vựng, cú pháp và ngữ nghĩa, các thuộc tính cấu trúc cũng quan trọng đối với phân tích code. Độ phức tạp cyclomatic [36], cho biết độ phức tạp của một chương trình và có thể được tham chiếu đến đồ thị luồng điều khiển (CFG), có thể được sử dụng như một thuộc tính cấu trúc của code. Về mặt toán học, độ phức tạp cyclomatic 𝑀 có thể được tính toán dựa trên CFG của mã nguồn bằng:

𝑀=𝐸−𝑁+2𝑃 (3)

trong đó 𝐸 và 𝑁 là số lượng cạnh và nút của đồ thị tương ứng. 𝑃 là số lượng thành phần được kết nối. Giá trị của nó thường là 1 vì CFG là một đồ thị được kết nối. Như được hiển thị trong Hình 1(c), CFG có 7 nút và 7 cạnh, do đó độ phức tạp cyclomatic của đoạn code là 7−7+2=2. Chúng tôi sử dụng dự đoán độ phức tạp cyclomatic làm tác vụ probing cấu trúc để điều tra mức độ tốt mà các biểu diễn ngữ cảnh hiểu thuộc tính cấu trúc của mã nguồn. Mô tả chi tiết của probing cấu trúc có thể được tìm thấy trong Phụ lục trực tuyến [51].

3.2.2 Pipeline probing. Theo các nghiên cứu trước đây [37,52], để điều tra các thuộc tính code nào được mã hóa trong các biểu diễn theo từng lớp của mô hình được tiền huấn luyện, chúng tôi huấn luyện một bộ phân loại lấy các biểu diễn theo từng lớp này làm đầu vào để dự đoán tác vụ probing liên quan đến một trong các thuộc tính code. Đồng thời, chúng tôi học một tổ hợp tuyến tính của các biểu diễn ngữ cảnh của tất cả các lớp để nghiên cứu mức độ đóng góp của các biểu diễn của mỗi lớp đối với việc hiểu các thuộc tính code này. Về mặt toán học, đối với các biểu diễn được tiền huấn luyện theo từng lớp 𝐻0,𝐻1,...,𝐻𝐋, chúng tôi kết hợp chúng bằng:

𝐹=𝐿∑︁𝑙=1𝜆𝑙𝐻𝑙, 𝜆𝑙=exp𝑎𝑙Í𝐿𝑖=0exp𝑎𝑖(4)

trong đó các trọng số theo từng lớp 𝑎𝑙 được học chung với bộ phân loại probing. Một mặt, chúng tôi so sánh hiệu suất giữa các biểu diễn kết hợp 𝐹 và các biểu diễn được khởi tạo ngẫu nhiên để nghiên cứu mức độ tốt mà các biểu diễn ngữ cảnh được tiền huấn luyện

--- TRANG 4 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

Bảng 1: Tổng quan về các tác vụ downstream, bao gồm mô tả, và các bộ dữ liệu được đánh giá, ngôn ngữ lập trình và các chỉ số. #Size hiển thị kích thước của các tập train, validation và test theo thứ tự. Đối với các chỉ số, P, R, F1 là viết tắt của precision, recall và F1-score tương ứng. EM và Edit sim là viết tắt của Exact Match accuracy và Levenshtein edit similarity tương ứng.

Tác vụ Mô tả Tên Bộ dữ liệu Ngôn ngữ #Size Chỉ số
Tìm kiếm code Tìm kiếm các đoạn code có liên quan về ngữ nghĩa cho một truy vấn ngôn ngữ tự nhiên đã cho.CodeSearchNet [23]Python 251K/9.6K/1K MRR, R@1
Ruby 24.9K/1.4K/1.3K R@5, R@10
Phát hiện clone Phát hiện liệu hai đoạn code có tương đương về chức năng không.BigCloneBench [49] Java 901K/416K/416K P, R, F1
Tóm tắt code Tạo mô tả ngôn ngữ tự nhiên ngắn gọn cho đoạn code đã cho.CodeSearchNet [23]Python 251K/9.6K/1K BLEU, Meteor,
Ruby 24.9K/1.4K/1.3K Rouge-L,Cider
Tạo code Tạo một đoạn code cấp hàm cho mô tả ngôn ngữ tự nhiên đã cho.CONCODE [24] Java 100K/2K/2K BLEU, EM
Hoàn thành code Dự đoán dòng code tiếp theo cho ngữ cảnh code trước đó đã cho.Github Java Corpus [5] Java 12K/1.5K/1.5K Edit sim, EM

mã hóa các thuộc tính của mã nguồn. Chúng tôi cũng so sánh hiệu suất giữa các biểu diễn được tiền huấn luyện và fine-tuned để nghiên cứu điều gì xảy ra với các thuộc tính code trong quá trình fine-tuned. Mặt khác, để điều tra mức độ đóng góp của biểu diễn của mỗi lớp đối với việc mã hóa một thuộc tính code và để khám phá sự khác biệt giữa các mô hình được tiền huấn luyện và fine-tuned, chúng tôi trình bày trọng số theo từng lớp 𝑎𝑙 của các mô hình được tiền huấn luyện và fine-tuned cho mỗi tác vụ probing và phân tích thêm các kết quả thực nghiệm Phần 3.5.1.

3.3 Phân tích Tương đồng Biểu diễn

Theo nghiên cứu trước đây [37], chúng tôi lấy mẫu ngẫu nhiên 𝑁 đoạn code và thu được các biểu diễn theo từng lớp của mô hình được tiền huấn luyện và fine-tuned. Sau đó, đối với mỗi lớp, chúng tôi thu được ma trận khoảng cách 𝐴𝑙 (được giới thiệu trong Phần 2.3 và kích thước là 𝑁×𝑁) cho lớp thứ 𝑙 bằng cách tính toán độ tương tự cosine giữa các vector biểu diễn của lớp này của bất kỳ hai đoạn code nào. Các vector biểu diễn được thu được bằng cách lấy trung bình các biểu diễn ngữ cảnh của lớp đó. Về mặt toán học, chúng tôi ký hiệu các biểu diễn ngữ cảnh thứ 𝑙 của mô hình code được tiền huấn luyện hoặc fine-tuned cho đoạn code thứ 𝑘 là 𝐻𝑘𝑙. Ma trận khoảng cách 𝐴𝑙 được tính toán bằng:

𝐴𝑙𝑖,𝑗=𝑣𝑙𝑖·𝑣𝑙𝑗∥𝑣𝑙𝑖∥∥𝑣𝑙𝑗∥, 𝑣𝑙𝑘=𝑚𝑒𝑎𝑛(𝐻𝑘𝑙), 𝑖,𝑗,𝑘∈[1,2,...,𝑁] (5)

Tiếp theo, đối với lớp thứ 𝑙, chúng tôi tính toán các hệ số tương quan Pearson 𝜌𝑙 giữa hai ma trận khoảng cách thu được từ các mô hình được tiền huấn luyện và fine-tuned tương ứng. Đặc biệt, chúng tôi tiến hành các thí nghiệm trên năm tác vụ downstream đa dạng bao gồm tìm kiếm code, phát hiện clone, tóm tắt code, tạo code, và hoàn thành code cấp dòng. Tổng quan về chúng được thể hiện trong Bảng 1. Các kết quả thực nghiệm được hiển thị trong Phần 3.5.2.

3.4 Cài đặt Thực nghiệm

Trong nghiên cứu này, chúng tôi phân tích các mô hình code được tiền huấn luyện hiện đại UniXcoder [17] và GraphCodeBERT [18]. Cả hai đều là Transformer 12 lớp với 768 chiều và tổng số tham số khoảng 120 MB. UniXcoder là một mô hình code được tiền huấn luyện thống nhất và có thể được sử dụng như một encoder, một decoder, hoặc một kiến trúc encoder-decoder bằng một token chỉ định đặc biệt. GraphCodeBERT xem xét thông tin luồng dữ liệu và tiền huấn luyện một mô hình lớn sử dụng nhiều dữ liệu đa phương thức (các hàm code được ghép nối với các bình luận ngôn ngữ tự nhiên) và dữ liệu code đơn phương thức. Chúng tôi tiến hành thí nghiệm trên UniXcoder và GraphCodeBERT vì chúng đều đạt được kết quả hứa hẹn trên nhiều tác vụ trí tuệ code.

Đối với các thí nghiệm về probing, chúng tôi xây dựng các bộ dữ liệu đánh giá thông qua CodeSeachNet và POJ-104 [39] được hiển thị trong Bảng 1. Các probing từ vựng, cú pháp và cấu trúc sử dụng bộ dữ liệu CodeSeachNet với Python, và probing ngữ nghĩa sử dụng POJ-104 [39]. Theo các cài đặt thực nghiệm fine-tuning của UniXcoder/GraphCodeBERT trên tìm kiếm code, Chúng tôi fine-tune mô hình được tiền huấn luyện trên bộ dữ liệu CodeSeachNet với Python, và probe mô hình được tiền huấn luyện và fine-tuned với bốn tác vụ probing. Khi probing, độ dài tối đa của các đoạn code được đặt thành 512. Epoch tối đa và kích thước batch được đặt thành 30 và 32 tương ứng. Chúng tôi áp dụng bộ tối ưu Adam với tốc độ học 1e-4 và thực hiện early stopping trên tập validation. Chúng tôi chạy các thí nghiệm 3 lần với các seed ngẫu nhiên 0,1,2 và hiển thị giá trị trung bình trong bài báo.

Đối với phân tích tương đồng biểu diễn, theo nghiên cứu trước đây [37], 𝑁 được đặt thành 5,000. Theo các cài đặt thực nghiệm fine-tuning của UniXcoder/GraphCodeBERT, chúng tôi fine-tune nó trên năm tác vụ downstream được hiển thị trong Bảng 1.

3.5 Các Phát hiện Thực nghiệm

Trong phần này, chúng tôi trình bày và phân tích kết quả của hai câu hỏi nghiên cứu trên. Chúng tôi trình bày kết quả của Telly-𝐾 dựa trên UniXcoder chỉ do hạn chế về không gian và đặt kết quả của Telly-𝐾 dựa trên GraphCodeBERT trong Phụ lục trực tuyến [51]. Các kết luận và phát hiện áp dụng cho UniXcoder nói chung cũng áp dụng cho GraphCodeBERT.

3.5.1 RQ1: Các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp? Chúng tôi sử dụng bốn tác vụ probing liên quan đến các thuộc tính từ vựng, cú pháp, ngữ nghĩa và cấu trúc để khám phá các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp và mức độ đóng góp của các biểu diễn của mỗi lớp đối với việc hiểu các thuộc tính code này. Đồng thời, chúng tôi cũng so sánh các biểu diễn được tiền huấn luyện và fine-tuned theo từng lớp trong cùng một cài đặt. Hiệu suất trên các tác vụ probing được hiển thị trong Bảng 2 và các đóng góp theo từng lớp được trình bày trong Hình 2.

Trong Bảng 2, kết quả của probing từ vựng, cú pháp và cấu trúc được đo bằng độ chính xác. Kết quả của probing ngữ nghĩa được đo bằng mean average precision (MAP) [44]. Để biết mô tả chi tiết hơn về các chỉ số độ chính xác và MAP, vui lòng tham khảo Phụ lục trực tuyến [51]. Từ Bảng 2, chúng ta có thể thấy rằng (1) các biểu diễn được tiền huấn luyện và fine-tuned hiểu các thuộc tính code tốt hơn các biểu diễn ngẫu nhiên; (2) sau fine-tuning, các thuộc tính code từ vựng, cú pháp và ngữ nghĩa vẫn được nắm bắt tốt, trong khi khả năng nắm bắt thuộc tính cấu trúc giảm đáng kể. Phát hiện đầu tiên được mong đợi vì các mô hình code được tiền huấn luyện có thể tận dụng các bộ dữ liệu lớn hơn và kích thước mô hình để mã hóa kiến thức code cơ bản vào các biểu diễn của chúng. Sau fine-tuning, một số thuộc tính code vẫn được bảo tồn. Điều này có thể liên quan đến đặc điểm của các tác vụ downstream vì tìm kiếm code chủ yếu dựa vào thông tin từ vựng, cú pháp và ngữ nghĩa của code hơn là thông tin cấu trúc. Nó cũng có thể là kết quả của vanishing gradients [8] vì các thuộc tính code được mã hóa trong lớp thấp hơn thay đổi rất ít, và các thuộc tính code được mã hóa trong lớp cao hơn thay đổi rõ ràng. Thực tế, vanishing gradients có ít ảnh hưởng đến việc tối ưu hóa mô hình code được tiền huấn luyện vì kiến trúc cơ bản được sử dụng bởi mô hình sử dụng kết nối dư [20,56] có thể tránh hiệu quả vấn đề vanishing.

Hình 2 hiển thị các đóng góp theo từng lớp (𝜆𝑙 trong Eq. 4) cho mô hình được tiền huấn luyện và fine-tuned. Từ Hình 2, một mặt, chúng ta có thể quan sát thấy rằng đối với các mô hình được tiền huấn luyện hoặc fine-tuned, các thuộc tính từ vựng, cú pháp và cấu trúc của mã nguồn chủ yếu được nắm bắt trong các lớp thấp hơn, trung gian và cao hơn tương ứng, trong khi thuộc tính ngữ nghĩa gần như trải rộng trên toàn bộ mô hình. Chúng tôi tiến hành kiểm định ý nghĩa thống kê¹ để kiểm tra ý nghĩa của sự khác biệt đóng góp. Kết quả thực nghiệm cho thấy rằng đối với probing từ vựng, các đóng góp của lớp thứ 1, thứ 2 và thứ 4 lớn hơn đáng kể so với các lớp khác. Đối với probing cú pháp, các đóng góp của lớp thứ 4 đến thứ 7 lớn hơn đáng kể so với các lớp khác. Đối với probing ngữ nghĩa, các đóng góp giữa các lớp khác nhau không khác biệt đáng kể. Đối với probing cấu trúc, đóng góp của các lớp cuối của mô hình được tiền huấn luyện và fine-tuned lớn hơn đáng kể so với các lớp khác. Mặt khác, chúng tôi thấy rằng quá trình fine-tuning bảo tồn hầu hết các thuộc tính code. Cụ thể, các thuộc tính code cơ bản được nắm bắt bởi các lớp thấp hơn và trung gian vẫn được bảo tồn trong quá trình fine-tuning. Chỉ có hiệu suất của tác vụ probing cấu trúc thay đổi rõ ràng.

¹Kết quả được đặt trong Phụ lục trực tuyến [51] do hạn chế về không gian

--- TRANG 5 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

Bảng 2: Hiệu suất của các tác vụ probing cho các biểu diễn ngẫu nhiên, được tiền huấn luyện và fine-tuned.

Tác vụ Probing Hiệu suất
Ngẫu nhiên Được tiền huấn luyện Fine-tuned
Probing từ vựng 76.14 99.98 99.95
Probing cú pháp 64.70 95.60 95.10
Probing ngữ nghĩa 46.10 73.25 71.55
Probing cấu trúc 34.80 89.80 62.20

Tóm tắt. Đối với các biểu diễn được tiền huấn luyện theo từng lớp, các thuộc tính từ vựng, cú pháp và cấu trúc của mã nguồn chủ yếu được nắm bắt bởi các lớp thấp hơn, trung gian và cao hơn tương ứng, trong khi thuộc tính ngữ nghĩa gần như trải rộng trên toàn bộ mô hình. Đồng thời, các thuộc tính code cơ bản được nắm bắt bởi các lớp thấp hơn và trung gian vẫn được bảo tồn trong quá trình fine-tuning.

3.5.2 RQ2: Điều gì xảy ra với các biểu diễn theo từng lớp trong quá trình fine-tuning? Chúng tôi cũng tiến hành các thí nghiệm mở rộng về phân tích tương đồng biểu diễn (RSA) để nghiên cứu điều gì xảy ra với các biểu diễn theo từng lớp của mô hình được tiền huấn luyện trong quá trình fine-tuning cho năm tác vụ downstream đa dạng mà không cần sự giúp đỡ của các tác vụ probing. Kết quả được hiển thị trong Hình 3. Từ các kết quả được trình bày, chúng ta có thể thấy rằng các hệ số tương quan (điểm tương đồng trong Hình 3) của 9 lớp dưới cùng đều lớn hơn 0.8. Điều này có nghĩa là các biểu diễn của 9 lớp dưới cùng tương tự giữa các mô hình được tiền huấn luyện và fine-tuned cho năm tác vụ downstream. Các biểu diễn của lớp trên cùng không tương tự (𝜌≤0.5) ngoại trừ hoàn thành code. Điều này là vì lớp trên cùng của mô hình được tiền huấn luyện UniXcoder được sử dụng để dự đoán các token bị che, tương tự như cài đặt thực nghiệm của hoàn thành code. Hơn nữa, chúng tôi thấy rằng biểu diễn của 7 lớp dưới cùng (𝜌≥0.9) có liên quan lớn và 5 lớp dưới cùng (𝜌≥0.95) rất tương tự.

Tóm tắt. Các biểu diễn của chín lớp dưới cùng tương tự giữa các mô hình được tiền huấn luyện và fine-tuned cho năm tác vụ downstream. Chỉ có các biểu diễn của hai lớp trên cùng thay đổi lớn trong quá trình fine-tuning.

4 FINE-TUNING HIỆU QUẢ CÁC MÔ HÌNH CODE ĐƯỢC TIỀN HUẤN LUYỆN
4.1 Câu hỏi Nghiên cứu

RQ3: Có những phương án thay thế hiệu quả cho fine-tuning không? Dựa trên kết quả của nghiên cứu thực nghiệm, chúng tôi điều tra các phương án thay thế hiệu quả hơn để fine-tune các mô hình code được tiền huấn luyện. Động lực chính của chúng tôi là đóng băng các tham số được tiền huấn luyện của những lớp chỉ thay đổi nhẹ trong quá trình fine-tuning của các tác vụ downstream. Chúng tôi đề xuất Telly-𝐾, có nghĩa là fine-tuning hiệu quả các mô hình code được tiền huấn luyện thông qua đóng băng lớp. Telly-𝐾 có nghĩa là đóng băng các tham số được tiền huấn luyện của 𝐾 lớp dưới cùng và 𝐾 khác nhau có nghĩa là các biến thể khác nhau của phương pháp của chúng tôi. Lớp thứ 0 là lớp embedding, và số lượng lớp tối đa của mô hình code được tiền huấn luyện được nghiên cứu của chúng tôi là 12. Nếu 𝐾 được đặt thành 12, thì Telly-12 sẽ đóng băng tất cả các tham số và mô hình sẽ được giảm thành mô hình được tiền huấn luyện vanilla. Do đó, để tính toàn diện của các thí nghiệm, chúng tôi thay đổi 𝐾 từ 0 đến 11 và tiến hành các thí nghiệm mở rộng trên năm tác vụ downstream cho 12 biến thể mô hình này. Tiếp theo, chúng tôi giới thiệu các cài đặt thực nghiệm, kết quả và phân tích.

4.2 Cài đặt Thực nghiệm

Các thí nghiệm của chúng tôi được tiến hành trên năm tác vụ downstream đa dạng bao gồm tìm kiếm code, phát hiện code, tóm tắt code, tạo code, và hoàn thành code cấp dòng. Tổng quan về các tác vụ này được trình bày trong Bảng 1. Tìm kiếm code được đánh giá trên bộ dữ liệu CodeSeachNet được sử dụng rộng rãi với Python và Ruby và hiệu suất được đo bằng mean reciprocal rank (MRR) và top-k recall (R@k, k=1,5,10) [15,18,23]. Đối với Phát hiện clone, chúng tôi thí nghiệm trên bộ dữ liệu BigCloneBench được sử dụng phổ biến và sử dụng precision (P), recall (R), và F1-score (F1) làm các chỉ số đánh giá [17,18]. Đối với tóm tắt code, chúng tôi fine-tune các mô hình được tiền huấn luyện [17,18] trên bộ dữ liệu CodeSearchNet với Python và Ruby. Các chỉ số đánh giá là sentence-level smoothing BLEU [41], Meteor [6], Rouge-L [32] và Cider [57]. Tạo code được đánh giá trên bộ dữ liệu CONCODE được sử dụng rộng rãi, và hiệu suất được đo bằng sentence-level smoothing BLEU [41] và Exact Match accuracy (EM). Đối với hoàn thành code cấp dòng, Chúng tôi tiến hành các thí nghiệm trên bộ dữ liệu lớn (có tên GitHub Java Corpus) trong CodeXGLUE. Tương tự như công việc trước đây [34], hiệu suất được đo bằng EM và Levenshtein edit similarity (Edit sim) [50]. Fine-tuning của hoàn thành code cấp dòng theo các cài đặt thực nghiệm trong CodeXGLUE [34]. Các tác vụ khác theo các cài đặt của các nghiên cứu trước đây [17,18]. Chúng tôi áp dụng bộ tối ưu Adam với epoch tối đa là 30 và thực hiện early stopping trên tập validation. Chúng tôi chạy các thí nghiệm 3 lần với các seed ngẫu nhiên 0,1,2 và hiển thị giá trị trung bình trong bài báo. Tất cả các thí nghiệm được tiến hành trên một máy có Tesla A100 GPU. Các cài đặt thực nghiệm chi tiết có thể được tìm thấy trong Phụ lục trực tuyến của gói sao chép [51].

4.3 Kết quả Thực nghiệm

Chúng tôi tiến hành các thí nghiệm với Telly-𝐾, đóng băng 𝐾 lớp dưới cùng của mô hình code được tiền huấn luyện khi fine-tuning nó trên năm tác vụ downstream. Trước tiên chúng tôi trình bày và phân tích các kết quả thực nghiệm bao gồm tham số huấn luyện, chi phí thời gian huấn luyện và hiệu suất từng tác vụ, và sau đó tóm tắt các phát hiện trên các tác vụ downstream. Do hạn chế về không gian, chúng tôi chỉ trình bày kết quả của Telly-𝐾 dựa trên UniXcoder và chúng tôi đặt kết quả của Telly-𝐾 dựa trên GraphCodeBERT trong Phụ lục trực tuyến [51]. Các kết luận và phát hiện áp dụng cho UniXcoder cũng nói chung áp dụng cho GraphCodeBERT.

4.3.1 Tìm kiếm code. Chúng tôi nghiên cứu hiệu suất của 12 biến thể Telly-𝐾 trên tìm kiếm code. Kết quả được hiển thị trong Bảng 3. Chúng tôi chỉ hiển thị kết quả trên bộ dữ liệu Python và đặt kết quả trên Ruby (tương tự như kết quả trên Python) trong Phụ lục trực tuyến [51].

Trong Bảng 3, chúng tôi trình bày số lượng tham số được huấn luyện, thời gian huấn luyện và hiệu suất cho mô hình cơ sở và 12 biến thể mô hình. Mô hình cơ sở là fine tune tất cả các tham số được tiền huấn luyện, trong khi các biến thể khác nhau sẽ đóng băng một phần tham số. Chúng tôi báo cáo cả thời gian huấn luyện của mỗi epoch và thời gian cho đến khi mô hình hội tụ.

--- TRANG 6 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

0123456789101112
Lớp Khác nhauCấu trúc
 ProbingNgữ nghĩa
 ProbingCú pháp
 ProbingTừ vựng
 ProbingĐược tiền huấn luyện Fine-tuned

Hình 2: Các đóng góp theo từng lớp trên các tác vụ probing khác nhau cho mô hình code được tiền huấn luyện và fine-tuned.

Tỷ lệ thay đổi của các biến thể mô hình khác nhau so với mô hình cơ sở được hiển thị trong ngoặc đơn. Từ kết quả của Bảng 3, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện (đặc biệt là chi phí thời gian hội tụ) và các tham số huấn luyện đều được giảm đáng kể so với mô hình cơ sở, trong khi hiệu suất không thay đổi nhiều trên bốn chỉ số. Đặc biệt, đối với Telly–11 đóng băng 11 lớp dưới cùng, chi phí thời gian hội tụ mô hình và các tham số được huấn luyện được giảm 88% và 94% tương ứng, trong khi hiệu suất chỉ giảm khoảng 3%.

•Đối với Telly-𝐾(0≤𝐾≤8), chúng giảm các tham số huấn luyện 32% đến 77%, tương ứng tiết kiệm khoảng 18% đến 81% thời gian huấn luyện, với việc tăng hiệu suất 0% đến 3% trên tất cả các chỉ số.

•Khi đóng băng 9 lớp dưới cùng, có sự giảm 83% về các tham số huấn luyện và việc tiết kiệm thời gian huấn luyện tương ứng 84% với sự thay đổi nhẹ về hiệu suất. Ví dụ, so với mô hình cơ sở, các giá trị MRR và R@10 cho Telly-9 giảm ít hơn 1%, R@1 tăng khoảng 1%, và R@5 không thay đổi.

•Đối với Telly-𝐾(𝐾≥10), hiệu suất giảm liên tục trên bốn chỉ số. Tuy nhiên, ngay cả khi đóng băng 11 lớp dưới cùng, hiệu suất không giảm đáng kể, trong khi các tham số huấn luyện và thời gian huấn luyện tương ứng được giảm rất nhiều.

Tóm tắt. Trong tác vụ tìm kiếm code, hiệu suất của Telly-𝐾 tăng cho 0≤𝐾≤8, thay đổi nhẹ cho 𝐾=9, và giảm nhẹ cho 𝐾≥10 so với mô hình cơ sở.

4.3.2 Phát hiện clone. Chúng tôi tiến hành các thí nghiệm với các biến thể Telly-𝐾 khác nhau trên phát hiện clone và kết quả được hiển thị trong nửa bên trái của Bảng 4. Vì mô hình code được tiền huấn luyện áp dụng MLP 2 lớp làm bộ phân loại để xác định liệu hai code có phải là clone hay không, tổng số tham số khoảng 127.1 triệu. Hiệu suất được đánh giá bằng precision (P), recall (R), và F1-score (F1) và chúng nằm trong khoảng [0, 1]. Từ Bảng 4, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, chi phí huấn luyện và tham số được giảm đáng kể so với mô hình cơ sở, trong khi không có thay đổi đáng kể về hiệu suất. Đặc biệt, khi đóng băng 11 lớp dưới cùng, chi phí thời gian hội tụ và các tham số huấn luyện được giảm 88% và 94% tương ứng, trong khi hiệu suất chỉ thay đổi 1-3%.

•Đối với Telly-𝐾(0≤𝐾≤8), các tham số huấn luyện được giảm 32% đến 77% và tương ứng khoảng 23% đến 51% chi phí thời gian huấn luyện được tiết kiệm. Ngoài ra, hiệu suất thường ổn định cho F1-score và thay đổi nhẹ cho precision và recall.

•Khi (𝐾≥9), hiệu suất của các biến thể khác nhau giảm liên tục về precision và F1-score nhưng tăng về recall. Tuy nhiên, ngay cả đối với Telly-11, tất cả các chỉ số đánh giá đều có điểm số cao (lớn hơn 0.9), trong khi các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều.

Tóm tắt. Trong tác vụ phát hiện clone, hiệu suất của Telly-𝐾 thường ổn định cho 0≤𝐾≤8 nhưng thay đổi nhẹ cho 𝐾≥9 so với mô hình cơ sở.

4.3.3 Tóm tắt code. Chúng tôi tiến hành các thí nghiệm với Telly-𝐾 khác nhau trên tóm tắt code và kết quả được hiển thị trong nửa bên phải của Bảng 4. Kết quả của Rouge-L và Cider trên Python và tất cả kết quả thực nghiệm trên Ruby được đặt trong Phụ lục trực tuyến [51] do hạn chế về không gian. Các chỉ số được báo cáo bao gồm BLEU và METEOR nằm trong khoảng [0, 100]. Từ Bảng 4, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm đáng kể so với mô hình cơ sở, trong khi hiệu suất không thay đổi nhiều cho tất cả các chỉ số.

•Đối với Telly-𝐾(0≤𝐾≤5), chúng giảm các tham số huấn luyện 32% đến 61%, tương ứng tiết kiệm khoảng 5% đến 57% thời gian huấn luyện, với hiệu suất ổn định. Đặc biệt, thay đổi hiệu suất nhỏ hơn 1% cho tất cả các chỉ số đánh giá.

•Đối với Telly-𝐾(6≤𝐾≤9), các tham số huấn luyện được giảm 66% đến 83%, tương ứng với việc tiết kiệm 60% đến 80% thời gian huấn luyện với sự tăng hiệu suất thường nhẹ.

•Khi Telly-𝐾(𝐾≥10), hiệu suất giảm nhẹ về bốn chỉ số. Tuy nhiên, ngay cả khi đóng băng 11 lớp dưới cùng, hiệu suất không giảm đáng kể, trong khi cả tham số huấn luyện và chi phí thời gian tương ứng đều được giảm cực kỳ nhiều.

Tóm tắt. Trên tác vụ tóm tắt code, hiệu suất của Telly-𝐾 ổn định cho (0≤𝐾≤5), tăng nhẹ cho 6≤𝐾≤9, nhưng giảm nhẹ khi 𝐾≥10 so với mô hình cơ sở.

4.3.4 Tạo code. Chúng tôi tiến hành các thí nghiệm với Telly-𝐾 khác nhau trên tạo code và kết quả được hiển thị trong nửa bên trái của Bảng 5. Các chỉ số đánh giá bao gồm BLEU và EM nằm trong khoảng [0, 100]. Từ Bảng 5, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm đáng kể so với mô hình cơ sở, trong khi hiệu suất không thay đổi nhiều cho tất cả các chỉ số ngoại trừ Telly-𝐾(𝐾≥8).

•Đối với Telly-𝐾(0≤𝐾≤5), chúng giảm các tham số huấn luyện 32% đến 61%, tương ứng tiết kiệm khoảng 10% đến 55% thời gian huấn luyện, và hiệu suất được cải thiện nhẹ. Cụ thể, điểm BLEU tăng 0% đến 2%, và điểm EM tăng 3% đến 10%.

•Đối với Telly-𝐾(6≤𝐾≤9), các tham số huấn luyện được giảm 66% đến 83%, tương ứng với việc tiết kiệm 57% đến 65% thời gian huấn luyện. Hiệu suất của các biến thể này giảm đáng kể dưới BLEU nhưng thường tăng dưới EM. Điều này là vì BLEU kết hợp precision n-gram (n=1,2,3,4) giữa đoạn code được tạo và ground truth cho một mẫu, trong khi EM là 1 nếu chúng hoàn toàn giống nhau, 0 nếu ngược lại. Tuy nhiên, đối với toàn bộ tập hợp, chỉ khoảng 18% các đoạn code có thể được tạo ra hoàn toàn giống như ground truth. 82% mẫu còn lại cũng ảnh hưởng đến kết quả cuối cùng của điểm BLEU. Do đó, các thay đổi hiệu suất cho BLEU và EM hoạt động khác nhau.

•Khi Telly-𝐾(𝐾≥10), các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều, và hiệu suất của các biến thể cũng giảm đáng kể trên cả hai chỉ số.

Tóm tắt. Trên tác vụ tạo code, hiệu suất của Telly-𝐾 được cải thiện nhẹ cho Telly-𝐾(0≤𝐾≤5), thay đổi rõ ràng cho 6≤𝐾≤9, và giảm đáng kể khi 𝐾≥10 so với mô hình cơ sở.

4.3.5 Hoàn thành code cấp dòng. Chúng tôi tiến hành các thí nghiệm với tất cả Telly-𝐾 trên hoàn thành code cấp dòng và kết quả được hiển thị trong nửa bên phải của Bảng 5. Các chỉ số được đánh giá bao gồm Edit Sim và EM nằm trong khoảng [0, 100]. Từ Bảng 5, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm rất nhiều so với mô hình cơ sở, trong khi hiệu suất không thay đổi đáng kể ngoại trừ Telly-11.

•Đối với Telly-𝐾(0≤𝐾≤7), chúng giảm các tham số huấn luyện 32% đến 72%, tương ứng tiết kiệm khoảng 13% đến 75% thời gian huấn luyện. Ngoài ra, hiệu suất của Telly-𝐾(0≤𝐾≤3) được cải thiện nhẹ và hiệu suất của Telly-𝐾(4≤𝐾≤7) giảm nhẹ.

•Đối với Telly-𝐾(𝐾≥8), các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều. Hiệu suất của các biến thể thường giảm cho hai chỉ số. Đặc biệt, khi 𝐾=11, hiệu suất giảm đáng kể. Do đó, các tham số được tiền huấn luyện của các lớp cuối cần được fine-tuned để học cách dự đoán dòng code tiếp theo.

Tóm tắt. Trên tác vụ hoàn thành code, hiệu suất của Telly-𝐾 được cải thiện nhẹ cho (0≤𝐾≤10 và giảm đáng kể khi 𝐾=11 so với mô hình cơ sở.

4.3.6 Các phát hiện trên tất cả các tác vụ downstream. Sau khi phân tích các kết quả thực nghiệm từng tác vụ, chúng tôi tóm tắt các phát hiện tổng quát trên các tác vụ khác nhau như sau.

•Đối với tất cả Telly-𝐾, cả chi phí thời gian huấn luyện (đặc biệt là chi phí thời gian hội tụ) và các tham số huấn luyện đều được giảm đáng kể so với mô hình cơ sở. Hiệu suất không thay đổi nhiều, ngoại trừ Telly-10 và Telly-11 trên tạo code và Telly-11 trên hoàn thành code.

•Khi 0≤𝐾≤5, Telly-𝐾 thường giảm các tham số huấn luyện 30% đến 65%, tương ứng tiết kiệm khoảng 10% đến 70% thời gian huấn luyện, với hiệu suất thường tăng thay đổi

--- TRANG 7 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

0 1 2 3 4 5 6 7 8 9 10 11 12
Lớp Khác nhau0.20.40.60.81.0Điểm Tương đồng 
1 2 3 4 5 6 70.951.00

tìm kiếm
clonetóm tắt
tạo codethoàn thành_code

Hình 3: Điểm tương đồng giữa các mô hình được tiền huấn luyện và fine-tuned cho năm tác vụ downstream.

từ 1% đến 4% về các tác vụ downstream khác nhau. Do đó, Telly-5 thường là lựa chọn tốt nhất trong số các biến thể giảm tiêu thụ tài nguyên và cải thiện hiệu suất.

•Khi 6≤𝐾≤9, Telly-𝐾 thường giảm các tham số huấn luyện 65% đến 80%, tương ứng tiết kiệm khoảng 50% đến 80% thời gian huấn luyện, với hiệu suất thường thay đổi nhỏ cho các tác vụ downstream khác nhau. Cụ thể, hiệu suất của tìm kiếm code và tạo code thay đổi từ 1% đến 5%, phát hiện clone và tóm tắt code thay đổi từ 0% đến 1%, và hoàn thành code thay đổi từ 0% đến 3%. Do đó, Telly-9 thường là lựa chọn tốt nhất trong số các biến thể giảm tiêu thụ tài nguyên với thay đổi nhỏ về hiệu suất.

•Khi 𝐾≥10, cả tham số huấn luyện và chi phí thời gian đều được giảm rất nhiều. Hiệu suất của Telly-𝐾 giảm nhưng không đáng kể ngoại trừ tạo code và hoàn thành code.

Hiệu suất của các mô hình tăng mặc dù đóng băng một số lớp trong các thí nghiệm trên. Một lời giải thích có thể là việc giảm số lượng tham số làm giảm overfitting, cho phép các tham số được fine-tuned trên tập huấn luyện tổng quát hóa tốt hơn cho tập kiểm tra. Tuy nhiên, cơ chế chính xác đằng sau hiện tượng này vẫn chưa được hiểu rõ và cần điều tra thêm, như nghiên cứu thêm các bộ dữ liệu với các phân phối khác nhau giữa tập huấn luyện và kiểm tra, để xác minh phỏng đoán của chúng tôi.

Tóm tắt. Đối với Telly-𝐾 trên các tác vụ downstream khác nhau, cả tham số huấn luyện và chi phí thời gian đều được giảm cực kỳ nhiều so với mô hình cơ sở. Nói chung, hiệu suất của Telly-𝐾 tăng 1% đến 4% cho 0≤𝐾≤5, thay đổi nhẹ cho 6≤𝐾≤9, và giảm rõ ràng khi 𝐾≥10 so với mô hình cơ sở.

5 THẢO LUẬN VÀ MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
5.1 Tầm quan trọng của Việc Giảm Thời gian Fine-tuning, Chi phí và Ưu điểm của Telly-𝐾.

Việc giảm chi phí fine-tuning đặc biệt là chi phí thời gian rất quan trọng vì (1) paradigm tiền huấn luyện với fine-tuning cho thấy sự áp dụng ngày càng tăng trong nhiều tác vụ kỹ thuật phần mềm [2,15,17,34]. Việc giảm chi phí thời gian quan trọng vì thường mất nhiều thời gian để fine-tune một mô hình, đặc biệt trên các bộ dữ liệu lớn hơn. Việc giảm chi phí thời gian fine-tuning cũng có thể cải thiện hiệu quả phát triển của các mô hình code được tiền huấn luyện, đặc biệt khi các nhà phát triển cần đáp ứng thời hạn ra mắt sản phẩm, từ đó tiết kiệm chi phí và giảm thiệt hại bất ngờ. (2) GPU thường là tài nguyên tính toán đắt đỏ. Chúng ta có thể tiết kiệm tài nguyên GPU với việc giảm chi phí thời gian. Hơn nữa. Huấn luyện các mô hình dựa trên deep-learning, bao gồm fine-tuning các mô hình được tiền huấn luyện, có thể phát thải hơn 600K tấn CO2 mỗi năm [48]. Bằng cách giảm chi phí thời gian, lượng khí thải carbon có thể được giảm.

Trên thực tế, nhiều phương pháp [22,26,29,35,59,60] đã được đề xuất để tiết kiệm chi phí thời gian fine-tuning. Trong số chúng, các kỹ thuật chưng cất [29,35,60] có triển vọng và phổ biến. Cụ thể, chúng đề xuất các phương pháp khác nhau để nén một mô hình được tiền huấn luyện lớn thành một mô hình nhỏ hơn, và fine-tune mô hình nhỏ hơn để thực hiện các tác vụ downstream. Tuy nhiên, các kỹ thuật này cần thiết kế và điều chỉnh cẩn thận các kiến trúc của các mô hình nhỏ và các hàm mất mát để chưng cất kiến thức từ các mô hình lớn. Do đó, nói chung quá trình chưng cất đòi hỏi thiết kế thủ công nhiều hơn và tốn công hơn. Phương pháp Telly-𝐾 của chúng tôi đơn giản và có thể trực tiếp giảm chi phí huấn luyện thông qua đóng băng lớp.

5.2 Làm thế nào để giúp fine-tuning tốt hơn trong tương lai

Trong bài báo này, chúng tôi chưa kết luận một gợi ý "một-phương-pháp-thống-trị-tất-cả" cho các tác vụ khác nhau. Tuy nhiên, nghiên cứu của chúng tôi về RQ2 và RQ3 cho thấy rằng Telly-7, đóng băng 7 lớp dưới cùng, đạt được sự cân bằng tốt giữa hiệu quả và hiệu suất. Cụ thể, (1) từ Hình 3, chúng ta có thể thấy rằng các biểu diễn của 7 lớp dưới cùng giữa các mô hình được tiền huấn luyện và fine-tuned rất tương tự; (2) từ Bảng 3, 4 và 5, chúng ta có thể thấy rằng Telly–7 giảm đáng kể thời gian huấn luyện và tham số trong khi duy trì hiệu suất tương tự cho tất cả các tác vụ downstream. Do đó, khuyến nghị sử dụng Telly–7 trong thực tế. Trong công việc tương lai, chúng tôi nhằm điều tra các chiến lược đóng băng lớp tự động để sử dụng các biểu diễn theo từng lớp hiệu quả hơn. Ví dụ, chúng tôi dự định thiết kế các thuật toán để tự động chọn các đặc trưng từ các lớp khác nhau và tập hợp chúng để thực hiện các tác vụ downstream khác nhau.

5.3 Mối đe dọa đối với Tính hợp lệ

Chúng tôi đã xác định các mối đe dọa sau đây đối với nghiên cứu của chúng tôi:

--- TRANG 8 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

Bảng 3: Kết quả thực nghiệm trên tìm kiếm code. #Params là viết tắt của số lượng tham số huấn luyện. M là viết tắt của triệu. Tỷ lệ thay đổi so với mô hình cơ sở được hiển thị trong ngoặc đơn.

Mô hình Ghi chú #ParamsThời gian huấn luyện Hiệu suất
Mỗi epoch Hội tụ MRR R@1 R@5 R@10
Cơ sở Fine-tuning tất cả tham số 125.93M 17m14s 2h35m06s 0.720 0.612 0.838 0.889
Telly-0 Đóng băng 0 lớp dưới cùng 85.0M( ↓32%) 15m49s(↓8%) 2h06m32s(↓18%) 0.727(↑1%) 0.630(↑3%) 0.846(↑1%) 0.895(↑1%)
Telly-1 Đóng băng 1 lớp dưới cùng 78.0M( ↓38%) 14m53s(↓14%) 1h59m04s(↓23%) 0.727(↑1%) 0.629(↑3%) 0.848(↑1%) 0.895(↑1%)
Telly-2 Đóng băng 2 lớp dưới cùng 70.9M( ↓44%) 14m06s(↓18%) 1h52m48s(↓27%) 0.727(↑1%) 0.630(↑3%) 0.849(↑1%) 0.896(↑1%)
Telly-3 Đóng băng 3 lớp dưới cùng 63.8M( ↓49%) 13m12s(↓23%) 0h39m36s(↓74%) 0.724(↑1%) 0.626(↑2%) 0.842(↑0%) 0.895(↑1%)
Telly-4 Đóng băng 4 lớp dưới cùng 56.7M( ↓55%) 12m12s(↓29%) 0h48m48s(↓69%) 0.724(↑1%) 0.626(↑2%) 0.844(↑1%) 0.896(↑1%)
Telly-5 Đóng băng 5 lớp dưới cùng 49.6M( ↓61%) 11m42s(↓32%) 0h35m06s(↓77%) 0.726(↑1%) 0.628(↑3%) 0.848(↑1%) 0.896(↑1%)
Telly-6 Đóng băng 6 lớp dưới cùng 42.5M( ↓66%) 10m50s(↓37%) 0h32m30s(↓79%) 0.727(↑1%) 0.629(↑3%) 0.848(↑1%) 0.896(↑1%)
Telly-7 Đóng băng 7 lớp dưới cùng 35.4M( ↓72%) 9m57s(↓42%) 0h29m51s( ↓81%) 0.725(↑1%) 0.627(↑2%) 0.847(↑1%) 0.896(↑1%)
Telly-8 Đóng băng 8 lớp dưới cùng 28.4M( ↓77%) 9m04s(↓47%) 0h36m16s( ↓77%) 0.723(↑0%) 0.625(↑2%) 0.844(↑1%) 0.894(↑1%)
Telly-9 Đóng băng 9 lớp dưới cùng 21.3M( ↓83%) 8m14s(↓52%) 0h24m42s( ↓84%) 0.718(↓0%) 0.620(↑1%) 0.838( 0%) 0.888( ↓0%)
Telly-10 Đóng băng 10 lớp dưới cùng 14.2M( ↓89%) 7m10s(↓58%) 0h21m30s( ↓86%) 0.710(↓1%) 0.612(↓0%) 0.829(↓1%) 0.882(↓1%)
Telly-11 Đóng băng 11 lớp dưới cùng 7.1M( ↓94%) 6m21s(↓63%) 0h19m03s( ↓88%) 0.694(↓4%) 0.593(↓3%) 0.815(↓3%) 0.871(↓2%)

Bảng 4: Kết quả thực nghiệm trên phát hiện clone và tóm tắt code. #Params là viết tắt của số lượng tham số huấn luyện. M là viết tắt của triệu. Tỷ lệ thay đổi so với mô hình cơ sở được hiển thị trong ngoặc đơn.

MôHìnhPhát hiện Clone Tóm tắt Code
#ParamsThời gian huấn luyện Hiệu suất#ParamsThời gian huấn luyện Hiệu suất
Mỗi epoch Hội tụ Recall Precision F1-score Mỗi epoch Hội tụ BLEU METEOR
Cơ sở 127.1M 20m21s 1h41m45s 0.95 0.95 0.95 125.93M 22m44s 1h53m40s 19.15 17.26
Telly-0 86.2M(↓32%) 19m29s(↓4%) 1h17m56s(↓23%) 0.96(↑1%) 0.94(↓1%) 0.95( 0%) 85.0M( ↓32%) 21m30s(↓5%) 1h47m30s(↓5%) 19.19(↑0%) 17.32(↑0%)
Telly-1 79.2M(↓38%) 18m36s(↓9%) 1h14m24s(↓27%) 0.95( 0%) 0.95( 0%) 0.95( 0%) 78.0M( ↓38%) 20m15s(↓11%) 1h41m15s(↓11%) 19.21(↑0%) 17.33(↑0%)
Telly-2 72.1M(↓43%) 17m34s(↓14%) 1h10m16s(↓31%) 0.95( 0%) 0.95( 0%) 0.95( 0%) 70.9M( ↓44%) 19m11s(↓16%) 1h35m55s(↓16%) 19.17(↑0%) 17.30(↑0%)
Telly-3 65.0M(↓49%) 15m20s(↓25%) 1h01m20s(↓40%) 0.95( 0%) 0.95( 0%) 0.95( 0%) 63.8M( ↓49%) 17m59s(↓21%) 0h53m57s(↓53%) 19.16(↑0%) 17.26( 0%)
Telly-4 57.9M(↓54%) 14m28s(↓29%) 0h57m52s(↓43%) 0.94(↓1%) 0.96(↑1%) 0.95( 0%) 56.7M( ↓55%) 17m07s(↓25%) 0h51m21s(↓55%) 19.13(↓0%) 17.26( 0%)
Telly-5 50.8M(↓60%) 13m25s(↓34%) 1h07m05s(↓34%) 0.96(↑1%) 0.94(↓1%) 0.95( 0%) 49.6M( ↓61%) 16m18s(↓28%) 0h48m54s(↓57%) 19.18(↑0%) 17.26( 0%)
Telly-6 43.7M(↓66%) 12m35s(↓38%) 0h50m20s(↓51%) 0.96(↑1%) 0.95( 0%) 0.95( 0%) 42.5M( ↓66%) 15m10s(↓33%) 0h45m30s(↓60%) 19.36(↑1%) 17.35(↑1%)
Telly-7 36.6M(↓71%) 11m44s(↓42%) 0h58m40s(↓42%) 0.95( 0%) 0.93( ↓2%) 0.94(↓1%) 35.4M(↓72%) 14m08s(↓38%) 0h28m16s(↓75%) 19.37(↑1%) 17.28(↑0%)
Telly-8 29.5M(↓77%) 10m41s(↓48%) 0h53m25s(↓48%) 0.95( 0%) 0.94( ↓1%) 0.95( 0%) 28.4M( ↓77%) 12m59s(↓43%) 0h25m58s(↓77%) 19.34(↑1%) 17.26( 0%)
Telly-9 22.4M(↓82%) 9m55s(↓51%) 0h29m45s(↓71%) 0.95( 0%) 0.92( ↓3%) 0.93(↓2%) 21.3M(↓83%) 11m28s(↓50%) 0h22m56s(↓80%) 19.18(↑0%) 17.22(↓0%)
Telly-10 15.4M(↓88%) 8m51s(↓57%) 0h35m24s(↓65%) 0.97(↑2%) 0.92(↓3%) 0.94(↓1%) 14.2M(↓89%) 10m19s(↓55%) 0h10m19s(↓91%) 19.11(↓0%) 17.18(↓0%)
Telly-11 8.3M(↓93%) 8m00s(↓61%) 0h32m00s(↓69%) 0.96(↑1%) 0.92(↓3%) 0.94(↓1%) 7.1M(↓94%) 09m14s(↓59%) 0h09m14s(↓92%) 19.10(↓0%) 17.20(↓0%)

Ngôn ngữ Lập trình. Chúng tôi tiến hành thí nghiệm trên năm ngôn ngữ lập trình (Python, Java, Ruby, C, và C++). Mặc dù về nguyên tắc, các mô hình được nghiên cứu của chúng tôi không được thiết kế cụ thể cho các ngôn ngữ nhất định, các mô hình có thể hoạt động khác nhau trên các ngôn ngữ lập trình khác nhau. Do đó, cần nhiều thí nghiệm hơn để xác nhận tính tổng quát của các phát hiện và kết luận của chúng tôi. Trong tương lai, chúng tôi sẽ mở rộng nghiên cứu của mình sang nhiều ngôn ngữ lập trình hơn.

Bộ dữ liệu Đánh giá. Chúng tôi tiến hành các thí nghiệm trên các bộ dữ liệu được sử dụng rộng rãi. Bên cạnh đó, có các bộ dữ liệu khác cho mỗi tác vụ downstream. Chúng khác nhau ở một số khía cạnh như phương pháp xây dựng và kích thước corpus. Mô hình có thể hoạt động khác nhau trên các bộ dữ liệu khác nhau. Do đó, chúng tôi sẽ tiến hành thí nghiệm trên nhiều bộ dữ liệu hơn để xác nhận tính tổng quát của các phát hiện và kết luận của chúng tôi.

Chỉ số Đánh giá. Chúng tôi sử dụng càng nhiều chỉ số được sử dụng phổ biến càng tốt để đánh giá hiệu suất mô hình trong nghiên cứu này. Tuy nhiên, các chỉ số này có thể có những hạn chế vốn có của chúng. Ví dụ, BLEU và METEOR là các chỉ số dựa trên độ tương tự văn bản và không thể đo độ tương tự ngữ nghĩa của hai câu. Trong tương lai, chúng tôi sẽ sử dụng nhiều chỉ số hơn và đánh giá con người để xác nhận các phát hiện và kết luận trong nghiên cứu này.

Các Mô hình Code Được Tiền huấn luyện. Do hạn chế về tài nguyên tính toán, chúng tôi tập trung vào các mô hình code được tiền huấn luyện hiện đại UniXcoder và GraphCodeBERT trong nghiên cứu này. Các mô hình code được tiền huấn luyện khác như CodeGPT [34] và CodeT5 [63] chưa được nghiên cứu.

6 CÔNG VIỆC LIÊN QUAN
6.1 Probing các Mô hình Được Tiền huấn luyện

Trong cộng đồng xử lý ngôn ngữ tự nhiên, nhiều nghiên cứu [1, 10,37,52,53] đã điều tra cách các mô hình ngôn ngữ được tiền huấn luyện hiểu ngôn ngữ tự nhiên và điều gì xảy ra khi fine-tuning chúng. Chúng thường được chia thành hai loại. Loại đầu tiên sử dụng các kỹ thuật probing để nghiên cứu các thuộc tính ngôn ngữ nào được nắm bắt bởi các mô hình ngôn ngữ được tiền huấn luyện [52,53]. Loại thứ hai là phân tích tương đồng biểu diễn [1,10,37]. Đó là một phân tích bất khả tri tác vụ và được sử dụng để đo độ tương tự giữa hai không gian biểu diễn khác nhau. Tuy nhiên, trong lĩnh vực kỹ thuật phần mềm, ít nghiên cứu khám phá điều gì xảy ra với các mô hình code được tiền huấn luyện trong quá trình fine-tuning. Hầu hết các nghiên cứu liên quan [15,21,30,54,58] nhằm hiểu những gì các mô hình code được tiền huấn luyện biết về mã nguồn. Ví dụ, Wan et al. [58] tiến hành phân tích cấu trúc để chứng minh rằng các mô hình được tiền huấn luyện nhận thức về cấu trúc cú pháp. López et al [21] phục hồi AST từ các biểu diễn ẩn của các mô hình ngôn ngữ được tiền huấn luyện. Karmakar et al [30] cũng đề xuất một số tác vụ probing mới để điều tra những gì các mô hình code được tiền huấn luyện biết về code. Được truyền cảm hứng từ quá trình biên dịch và phân tích tĩnh, trước tiên chúng tôi đề xuất bốn tác vụ probing liên quan đến các thuộc tính code từ vựng, cú pháp, ngữ nghĩa và cấu trúc. Sau đó chúng tôi điều tra các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp và điều gì xảy ra với các biểu diễn này trong quá trình fine-tuning.

6.2 Tăng tốc Quá trình Fine-tuning

Có nhiều nghiên cứu về tăng tốc quá trình fine-tuning [22,26, 29,35,59,60]. Các nghiên cứu này có thể được phân loại thô thành hai loại. Loại đầu tiên là sử dụng kỹ thuật chưng cất kiến thức để nén các mô hình ngôn ngữ được tiền huấn luyện quy mô lớn [29,35,60]. Ví dụ, Jiao et al. [29] đề xuất TinyBERT để chưng cất BERT và chỉ sử dụng khoảng 28% tham số để hiểu ngôn ngữ tự nhiên. Loại thứ hai là phương pháp fine-tuning dựa trên adapter [22,59], trong đó các adapter là các module có thể huấn luyện mới được thêm vào giữa các lớp của các mô hình được tiền huấn luyện. Ví dụ, Houlsby et al.l [22] thiết kế một số adapter với số lượng tham số ít hơn hai bậc độ lớn để fine-tune so với các mô hình đầy đủ và đạt được hiệu suất tương tự với fine-tuning tất cả các tham số của mô hình được tiền huấn luyện. Ngoài ra, có một số nghiên cứu về huấn luyện mạng thần kinh hiệu quả từ đầu với đóng băng lớp [28,62]. Ví dụ, Wang et al. [62] tận dụng kiến thức từ một mô hình tham chiếu để đánh giá chính xác tính linh hoạt huấn luyện của các lớp riêng lẻ, đóng băng những lớp đã hội tụ và mở đóng băng các lớp bị đóng băng để tiếp tục huấn luyện. Nghiên cứu của chúng tôi có thể thúc đẩy các nhà nghiên cứu đưa ra các phương pháp fine-tuning hiệu quả hơn.

7 KẾT LUẬN

Trong bài báo này, trước tiên chúng tôi tiến hành nghiên cứu thực nghiệm mở rộng để khám phá điều gì xảy ra với kiến thức code theo từng lớp và các biểu diễn được tiền huấn luyện trong quá trình fine-tuning. Sau đó chúng tôi đề xuất các phương án thay thế hiệu quả để fine-tune mô hình code được tiền huấn luyện lớn dựa trên các phát hiện trên. Nghiên cứu thực nghiệm của chúng tôi cho thấy rằng các thuộc tính từ vựng, cú pháp và cấu trúc của mã nguồn chủ yếu được nắm bắt trong các lớp thấp hơn, trung gian và cao hơn tương ứng, trong khi thuộc tính ngữ nghĩa trải rộng trên toàn bộ mô hình. Các thuộc tính code cơ bản được nắm bắt bởi các lớp thấp hơn và trung gian vẫn được bảo tồn trong quá trình fine-tuning. Hơn nữa, chúng tôi thấy rằng chỉ có các biểu diễn của hai lớp trên cùng thay đổi nhiều nhất trong quá trình fine-tuning cho các tác vụ downstream khác nhau. Dựa trên các phát hiện trên, chúng tôi đề xuất Telly-𝐾 để fine-tune hiệu quả các mô hình code được tiền huấn luyện thông qua đóng băng lớp có chọn lọc. Các thí nghiệm mở rộng trên năm tác vụ downstream khác nhau chứng minh rằng cả tham số huấn luyện và chi phí thời gian có thể được giảm rất nhiều, trong khi hiệu suất tương tự hoặc thậm chí tốt hơn.

Hơn nữa, nghiên cứu thực nghiệm của chúng tôi cho thấy nhiều phát hiện hữu ích và hướng nghiên cứu hứa hẹn cho fine-tuning hiệu quả của các mô hình code được tiền huấn luyện. Ví dụ, chúng tôi thấy rằng các mô hình code được tiền huấn luyện mã hóa các thuộc tính cú pháp vào các lớp trung gian. Do đó, khi các tác vụ downstream nhận thức về cú pháp như tạo code, chúng ta có thể thiết kế một số module để tận dụng các biểu diễn ngữ cảnh của các lớp trung gian. Trong tương lai, chúng tôi sẽ tiếp tục khám phá các hướng nghiên cứu hứa hẹn khác nhau và đề xuất các phương pháp fine-tuning hiệu quả hơn.

Gói sao chép bao gồm mã nguồn, bộ dữ liệu, và Phụ lục trực tuyến có sẵn tại: https://github.com/ DeepSoftwareAnalytics/Telly.

LỜI CẢM ỞN

Chúng tôi cảm ơn các nhà đánh giá đã có những bình luận quý giá về công việc này. Nghiên cứu này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc (Số 2017YFA0700800) và Quỹ Nghiên cứu Cơ bản cho các Đại học Trung ương theo Grant xtr072022001.

TÀI LIỆU THAM KHẢO

[1]Samira Abnar, Lisa Beinborn, Rochelle Choenni, và Willem H. Zuidema. 2019. Blackbox Meets Blackbox: Representational Similarity & Stability Analysis of

--- TRANG 9 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm đáng kể so với mô hình cơ sở, trong khi không có thay đổi đáng kể về hiệu suất. Đặc biệt, khi đóng băng 11 lớp dưới cùng, chi phí thời gian hội tụ và các tham số huấn luyện được giảm 88% và 94% tương ứng, trong khi hiệu suất chỉ thay đổi 1-3%.

•Đối với Telly-𝐾(0≤𝐾≤8), các tham số huấn luyện được giảm 32% đến 77% và tương ứng khoảng 23% đến 51% chi phí thời gian huấn luyện được tiết kiệm. Ngoài ra, hiệu suất thường ổn định cho F1-score và thay đổi nhẹ cho precision và recall.

•Khi (𝐾≥9), hiệu suất của các biến thể khác nhau giảm liên tục về precision và F1-score nhưng tăng về recall. Tuy nhiên, ngay cả đối với Telly-11, tất cả các chỉ số đánh giá đều có điểm số cao (lớn hơn 0.9), trong khi các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều.

Tóm tắt. Trong tác vụ phát hiện clone, hiệu suất của Telly-𝐾 thường ổn định cho 0≤𝐾≤8 nhưng thay đổi nhẹ cho 𝐾≥9 so với mô hình cơ sở.

4.3.3 Tóm tắt code. Chúng tôi tiến hành các thí nghiệm với Telly-𝐾 khác nhau trên tóm tắt code và kết quả được hiển thị trong nửa bên phải của Bảng 4. Kết quả của Rouge-L và Cider trên Python và tất cả kết quả thực nghiệm trên Ruby được đặt trong Phụ lục trực tuyến [51] do hạn chế về không gian. Các chỉ số được báo cáo bao gồm BLEU và METEOR nằm trong khoảng [0, 100]. Từ Bảng 4, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm đáng kể so với mô hình cơ sở, trong khi hiệu suất không thay đổi nhiều cho tất cả các chỉ số.

•Đối với Telly-𝐾(0≤𝐾≤5), chúng giảm các tham số huấn luyện 32% đến 61%, tương ứng tiết kiệm khoảng 5% đến 57% thời gian huấn luyện, với hiệu suất ổn định. Đặc biệt, thay đổi hiệu suất nhỏ hơn 1% cho tất cả các chỉ số đánh giá.

•Đối với Telly-𝐾(6≤𝐾≤9), các tham số huấn luyện được giảm 66% đến 83%, tương ứng với việc tiết kiệm 60% đến 80% thời gian huấn luyện với sự tăng hiệu suất thường nhẹ.

•Khi Telly-𝐾(𝐾≥10), hiệu suất giảm nhẹ về bốn chỉ số. Tuy nhiên, ngay cả khi đóng băng 11 lớp dưới cùng, hiệu suất không giảm đáng kể, trong khi cả tham số huấn luyện và chi phí thời gian tương ứng đều được giảm cực kỳ nhiều.

Tóm tắt. Trên tác vụ tóm tắt code, hiệu suất của Telly-𝐾 ổn định cho (0≤𝐾≤5), tăng nhẹ cho 6≤𝐾≤9, nhưng giảm nhẹ khi 𝐾≥10 so với mô hình cơ sở.

4.3.4 Tạo code. Chúng tôi tiến hành các thí nghiệm với Telly-𝐾 khác nhau trên tạo code và kết quả được hiển thị trong nửa bên trái của Bảng 5. Các chỉ số đánh giá bao gồm BLEU và EM nằm trong khoảng [0, 100]. Từ Bảng 5, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm đáng kể so với mô hình cơ sở, trong khi hiệu suất không thay đổi nhiều cho tất cả các chỉ số ngoại trừ Telly-𝐾(𝐾≥8).

•Đối với Telly-𝐾(0≤𝐾≤5), chúng giảm các tham số huấn luyện 32% đến 61%, tương ứng tiết kiệm khoảng 10% đến 55% thời gian huấn luyện, và hiệu suất được cải thiện nhẹ. Cụ thể, điểm BLEU tăng 0% đến 2%, và điểm EM tăng 3% đến 10%.

•Đối với Telly-𝐾(6≤𝐾≤9), các tham số huấn luyện được giảm 66% đến 83%, tương ứng với việc tiết kiệm 57% đến 65% thời gian huấn luyện. Hiệu suất của các biến thể này giảm đáng kể dưới BLEU nhưng thường tăng dưới EM. Điều này là vì BLEU kết hợp precision n-gram (n=1,2,3,4) giữa đoạn code được tạo và ground truth cho một mẫu, trong khi EM là 1 nếu chúng hoàn toàn giống nhau, 0 nếu ngược lại. Tuy nhiên, đối với toàn bộ tập hợp, chỉ khoảng 18% các đoạn code có thể được tạo ra hoàn toàn giống như ground truth. 82% mẫu còn lại cũng ảnh hưởng đến kết quả cuối cùng của điểm BLEU. Do đó, các thay đổi hiệu suất cho BLEU và EM hoạt động khác nhau.

•Khi Telly-𝐾(𝐾≥10), các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều, và hiệu suất của các biến thể cũng giảm đáng kể trên cả hai chỉ số.

Tóm tắt. Trên tác vụ tạo code, hiệu suất của Telly-𝐾 được cải thiện nhẹ cho Telly-𝐾(0≤𝐾≤5), thay đổi rõ ràng cho 6≤𝐾≤9, và giảm đáng kể khi 𝐾≥10 so với mô hình cơ sở.

4.3.5 Hoàn thành code cấp dòng. Chúng tôi tiến hành các thí nghiệm với tất cả Telly-𝐾 trên hoàn thành code cấp dòng và kết quả được hiển thị trong nửa bên phải của Bảng 5. Các chỉ số được đánh giá bao gồm Edit Sim và EM nằm trong khoảng [0, 100]. Từ Bảng 5, chúng ta có thể thấy rằng:

•Đối với tất cả các biến thể mô hình, cả chi phí thời gian huấn luyện và tham số đều được giảm rất nhiều so với mô hình cơ sở, trong khi hiệu suất không thay đổi đáng kể ngoại trừ Telly-11.

•Đối với Telly-𝐾(0≤𝐾≤7), chúng giảm các tham số huấn luyện 32% đến 72%, tương ứng tiết kiệm khoảng 13% đến 75% thời gian huấn luyện. Ngoài ra, hiệu suất của Telly-𝐾(0≤𝐾≤3) được cải thiện nhẹ và hiệu suất của Telly-𝐾(4≤𝐾≤7) giảm nhẹ.

•Đối với Telly-𝐾(𝐾≥8), các tham số huấn luyện và chi phí thời gian tương ứng được giảm rất nhiều. Hiệu suất của các biến thể thường giảm cho hai chỉ số. Đặc biệt, khi 𝐾=11, hiệu suất giảm đáng kể. Do đó, các tham số được tiền huấn luyện của các lớp cuối cần được fine-tuned để học cách dự đoán dòng code tiếp theo.

Tóm tắt. Trên tác vụ hoàn thành code, hiệu suất của Telly-𝐾 được cải thiện nhẹ cho (0≤𝐾≤10 và giảm đáng kể khi 𝐾=11 so với mô hình cơ sở.

4.3.6 Các phát hiện trên tất cả các tác vụ downstream. Sau khi phân tích các kết quả thực nghiệm từng tác vụ, chúng tôi tóm tắt các phát hiện tổng quát trên các tác vụ khác nhau như sau.

•Đối với tất cả Telly-𝐾, cả chi phí thời gian huấn luyện (đặc biệt là chi phí thời gian hội tụ) và các tham số huấn luyện đều được giảm đáng kể so với mô hình cơ sở. Hiệu suất không thay đổi nhiều, ngoại trừ Telly-10 và Telly-11 trên tạo code và Telly-11 trên hoàn thành code.

•Khi 0≤𝐾≤5, Telly-𝐾 thường giảm các tham số huấn luyện 30% đến 65%, tương ứng tiết kiệm khoảng 10% đến 70% thời gian huấn luyện, với hiệu suất thường tăng thay đổi

--- TRANG 10 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

Bảng 5: Kết quả thực nghiệm trên tạo code và hoàn thành code cấp dòng. #Params là viết tắt của số lượng tham số huấn luyện. M là viết tắt của triệu. Tỷ lệ thay đổi so với mô hình cơ sở được hiển thị trong ngoặc đơn.

Mô hìnhTạo Code Hoàn thành Code Cấp dòng
#ParamsThời gian huấn luyện Hiệu suất#ParamsThời gian huấn luyện Hiệu suất
Mỗi epoch Hội tụ BLEU EM Mỗi epoch Hội tụ Edit sim EM
Cơ sở 125.93M 12m25s 4h08m20s 33.82 17.4 125.93M 04m12s 0h37m48s 51.92 20.40
Telly-0 85.0M(↓32%) 11m43s(↓6%) 3h42m37s(↓10%) 33.88(↑0%) 18.1(↑4%) 85.0M(↓32%) 03m59s(↓5%) 0h31m52s(↓16%) 52.58(↑1%) 21.07(↑3%)
Telly-1 78.0M(↓38%) 11m07s(↓10%) 3h20m06s(↓19%) 34.43(↑2%) 17.9(↑3%) 78.0M(↓38%) 03m47s(↓10%) 0h18m55s(↓50%) 52.35(↑1%) 20.87(↑2%)
Telly-2 70.9M(↓44%) 10m32s(↓15%) 2h06m24s(↓49%) 33.85(↑0%) 19.1(↑10%) 70.9M(↓44%) 03m38s(↓13%) 0h32m42s(↓13%) 52.31(↑1%) 20.93(↑3%)
Telly-3 63.8M(↓49%) 09m52s(↓21%) 2h08m16s(↓48%) 34.24(↑1%) 19.0(↑9%) 63.8M(↓49%) 03m29s(↓17%) 0h20m54s(↓45%) 51.81(↓0%) 20.73(↑2%)
Telly-4 56.7M(↓55%) 09m15s(↓26%) 1h51m00s(↓55%) 34.02(↑1%) 18.6(↑7%) 56.7M(↓55%) 03m18s(↓21%) 0h26m24s(↓30%) 51.62(↓1%) 20.27(↓1%)
Telly-5 49.6M(↓61%) 08m44s(↓30%) 1h53m32s(↓54%) 34.36(↑2%) 18.6(↑7%) 49.6M(↓61%) 03m07s(↓26%) 0h09m21s(↓75%) 51.66(↓0%) 20.40(0%)
Telly-6 42.5M(↓66%) 08m13s(↓34%) 1h46m49s(↓57%) 32.90(↓3%) 18.1(↑4%) 42.5M(↓66%) 02m57s(↓30%) 0h11m48s(↓69%) 51.36(↓1%) 20.27(↓1%)
Telly-7 35.4M(↓72%) 07m41s(↓38%) 1h39m53s(↓60%) 32.92(↓3%) 18.0(↑3%) 35.4M(↓72%) 02m58s(↓29%) 0h14m50s(↓63%) 51.32(↓1%) 20.40( 0%)
Telly-8 28.4M(↓77%) 07m08s(↓43%) 1h32m44s(↓63%) 32.12(↓5%) 17.4( 0%) 28.4M( ↓77%) 02m59s(↓29%) 0h14m55s(↓65%) 50.95(↓2%) 20.27(↓1%)
Telly-9 21.3M(↓83%) 06m42s(↓46%) 1h27m06s(↓65%) 31.33(↓7%) 18.1(↑4%) 21.3M(↓83%) 02m58s(↓29%) 0h14m50s(↓67%) 51.13(↓2%) 20.73(↑2%)
Telly-10 14.2M(↓89%) 06m03s(↓51%) 1h12m36s(↓71%) 30.43(↓10%) 16.6(↓5%) 14.2M(↓89%) 02m57s(↓30%) 0h11m48s(↓76%) 50.70(↓2%) 20.53(↑1%)
Telly-11 7.1M(↓94%) 05m22s(↓57%) 1h09m46s(↓72%) 27.71(↓18%) 14.3(↓18%) 7.1M(↓94%) 02m06s(↓50%) 0h06m18s(↓83%) 49.49(↓5%) 19.67(↓4%)

từ 1% đến 4% về các tác vụ downstream khác nhau. Do đó, Telly-5 thường là lựa chọn tốt nhất trong số các biến thể giảm tiêu thụ tài nguyên và cải thiện hiệu suất.

•Khi 6≤𝐾≤9, Telly-𝐾 thường giảm các tham số huấn luyện 65% đến 80%, tương ứng tiết kiệm khoảng 50% đến 80% thời gian huấn luyện, với hiệu suất thường thay đổi nhỏ cho các tác vụ downstream khác nhau. Cụ thể, hiệu suất của tìm kiếm code và tạo code thay đổi từ 1% đến 5%, phát hiện clone và tóm tắt code thay đổi từ 0% đến 1%, và hoàn thành code thay đổi từ 0% đến 3%. Do đó, Telly-9 thường là lựa chọn tốt nhất trong số các biến thể giảm tiêu thụ tài nguyên với thay đổi nhỏ về hiệu suất.

•Khi 𝐾≥10, cả tham số huấn luyện và chi phí thời gian đều được giảm rất nhiều. Hiệu suất của Telly-𝐾 giảm nhưng không đáng kể ngoại trừ tạo code và hoàn thành code.

Hiệu suất của các mô hình tăng mặc dù đóng băng một số lớp trong các thí nghiệm trên. Một lời giải thích có thể là việc giảm số lượng tham số làm giảm overfitting, cho phép các tham số được fine-tuned trên tập huấn luyện tổng quát hóa tốt hơn cho tập kiểm tra. Tuy nhiên, cơ chế chính xác đằng sau hiện tượng này vẫn chưa được hiểu rõ và cần điều tra thêm, như nghiên cứu thêm các bộ dữ liệu với các phân phối khác nhau giữa tập huấn luyện và kiểm tra, để xác minh phỏng đoán của chúng tôi.

Tóm tắt. Đối với Telly-𝐾 trên các tác vụ downstream khác nhau, cả tham số huấn luyện và chi phí thời gian đều được giảm cực kỳ nhiều so với mô hình cơ sở. Nói chung, hiệu suất của Telly-𝐾 tăng 1% đến 4% cho 0≤𝐾≤5, thay đổi nhẹ cho 6≤𝐾≤9, và giảm rõ ràng khi 𝐾≥10 so với mô hình cơ sở.

5 THẢO LUẬN VÀ MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ

5.1 Tầm quan trọng của Việc Giảm Thời gian Fine-tuning, Chi phí và Ưu điểm của Telly-𝐾.

Việc giảm chi phí fine-tuning đặc biệt là chi phí thời gian rất quan trọng vì (1) paradigm tiền huấn luyện với fine-tuning cho thấy sự áp dụng ngày càng tăng trong nhiều tác vụ kỹ thuật phần mềm [2,15,17,34]. Việc giảm chi phí thời gian quan trọng vì thường mất nhiều thời gian để fine-tune một mô hình, đặc biệt trên các bộ dữ liệu lớn hơn. Việc giảm chi phí thời gian fine-tuning cũng có thể cải thiện hiệu quả phát triển của các mô hình code được tiền huấn luyện, đặc biệt khi các nhà phát triển cần đáp ứng thời hạn ra mắt sản phẩm, từ đó tiết kiệm chi phí và giảm thiệt hại bất ngờ. (2) GPU thường là tài nguyên tính toán đắt đỏ. Chúng ta có thể tiết kiệm tài nguyên GPU với việc giảm chi phí thời gian. Hơn nữa. Huấn luyện các mô hình dựa trên deep-learning, bao gồm fine-tuning các mô hình được tiền huấn luyện, có thể phát thải hơn 600K tấn CO2 mỗi năm [48]. Bằng cách giảm chi phí thời gian, lượng khí thải carbon có thể được giảm.

Trên thực tế, nhiều phương pháp [22,26,29,35,59,60] đã được đề xuất để tiết kiệm chi phí thời gian fine-tuning. Trong số chúng, các kỹ thuật chưng cất [29,35,60] có triển vọng và phổ biến. Cụ thể, chúng đề xuất các phương pháp khác nhau để nén một mô hình được tiền huấn luyện lớn thành một mô hình nhỏ hơn, và fine-tune mô hình nhỏ hơn để thực hiện các tác vụ downstream. Tuy nhiên, các kỹ thuật này cần thiết kế và điều chỉnh cẩn thận các kiến trúc của các mô hình nhỏ và các hàm mất mát để chưng cất kiến thức từ các mô hình lớn. Do đó, nói chung quá trình chưng cất đòi hỏi thiết kế thủ công nhiều hơn và tốn công hơn. Phương pháp Telly-𝐾 của chúng tôi đơn giản và có thể trực tiếp giảm chi phí huấn luyện thông qua đóng băng lớp.

5.2 Làm thế nào để giúp fine-tuning tốt hơn trong tương lai

Trong bài báo này, chúng tôi chưa kết luận một gợi ý "một-phương-pháp-thống-trị-tất-cả" cho các tác vụ khác nhau. Tuy nhiên, nghiên cứu của chúng tôi về RQ2 và RQ3 cho thấy rằng Telly-7, đóng băng 7 lớp dưới cùng, đạt được sự cân bằng tốt giữa hiệu quả và hiệu suất. Cụ thể, (1) từ Hình 3, chúng ta có thể thấy rằng các biểu diễn của 7 lớp dưới cùng giữa các mô hình được tiền huấn luyện và fine-tuned rất tương tự; (2) từ Bảng 3, 4 và 5, chúng ta có thể thấy rằng Telly–7 giảm đáng kể thời gian huấn luyện và tham số trong khi duy trì hiệu suất tương tự cho tất cả các tác vụ downstream. Do đó, khuyến nghị sử dụng Telly–7 trong thực tế. Trong công việc tương lai, chúng tôi nhằm điều tra các chiến lược đóng băng lớp tự động để sử dụng các biểu diễn theo từng lớp hiệu quả hơn. Ví dụ, chúng tôi dự định thiết kế các thuật toán để tự động chọn các đặc trưng từ các lớp khác nhau và tập hợp chúng để thực hiện các tác vụ downstream khác nhau.

5.3 Mối đe dọa đối với Tính hợp lệ

Chúng tôi đã xác định các mối đe dọa sau đây đối với nghiên cứu của chúng tôi:

--- TRANG 11 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

Ngôn ngữ Lập trình. Chúng tôi tiến hành thí nghiệm trên năm ngôn ngữ lập trình (Python, Java, Ruby, C, và C++). Mặc dù về nguyên tắc, các mô hình được nghiên cứu của chúng tôi không được thiết kế cụ thể cho các ngôn ngữ nhất định, các mô hình có thể hoạt động khác nhau trên các ngôn ngữ lập trình khác nhau. Do đó, cần nhiều thí nghiệm hơn để xác nhận tính tổng quát của các phát hiện và kết luận của chúng tôi. Trong tương lai, chúng tôi sẽ mở rộng nghiên cứu của mình sang nhiều ngôn ngữ lập trình hơn.

Bộ dữ liệu Đánh giá. Chúng tôi tiến hành các thí nghiệm trên các bộ dữ liệu được sử dụng rộng rãi. Bên cạnh đó, có các bộ dữ liệu khác cho mỗi tác vụ downstream. Chúng khác nhau ở một số khía cạnh như phương pháp xây dựng và kích thước corpus. Mô hình có thể hoạt động khác nhau trên các bộ dữ liệu khác nhau. Do đó, chúng tôi sẽ tiến hành thí nghiệm trên nhiều bộ dữ liệu hơn để xác nhận tính tổng quát của các phát hiện và kết luận của chúng tôi.

Chỉ số Đánh giá. Chúng tôi sử dụng càng nhiều chỉ số được sử dụng phổ biến càng tốt để đánh giá hiệu suất mô hình trong nghiên cứu này. Tuy nhiên, các chỉ số này có thể có những hạn chế vốn có của chúng. Ví dụ, BLEU và METEOR là các chỉ số dựa trên độ tương tự văn bản và không thể đo độ tương tự ngữ nghĩa của hai câu. Trong tương lai, chúng tôi sẽ sử dụng nhiều chỉ số hơn và đánh giá con người để xác nhận các phát hiện và kết luận trong nghiên cứu này.

Các Mô hình Code Được Tiền huấn luyện. Do hạn chế về tài nguyên tính toán, chúng tôi tập trung vào các mô hình code được tiền huấn luyện hiện đại UniXcoder và GraphCodeBERT trong nghiên cứu này. Các mô hình code được tiền huấn luyện khác như CodeGPT [34] và CodeT5 [63] chưa được nghiên cứu.

6 CÔNG VIỆC LIÊN QUAN

6.1 Probing các Mô hình Được Tiền huấn luyện

Trong cộng đồng xử lý ngôn ngữ tự nhiên, nhiều nghiên cứu [1, 10,37,52,53] đã điều tra cách các mô hình ngôn ngữ được tiền huấn luyện hiểu ngôn ngữ tự nhiên và điều gì xảy ra khi fine-tuning chúng. Chúng thường được chia thành hai loại. Loại đầu tiên sử dụng các kỹ thuật probing để nghiên cứu các thuộc tính ngôn ngữ nào được nắm bắt bởi các mô hình ngôn ngữ được tiền huấn luyện [52,53]. Loại thứ hai là phân tích tương đồng biểu diễn [1,10,37]. Đó là một phân tích bất khả tri tác vụ và được sử dụng để đo độ tương tự giữa hai không gian biểu diễn khác nhau. Tuy nhiên, trong lĩnh vực kỹ thuật phần mềm, ít nghiên cứu khám phá điều gì xảy ra với các mô hình code được tiền huấn luyện trong quá trình fine-tuning. Hầu hết các nghiên cứu liên quan [15,21,30,54,58] nhằm hiểu những gì các mô hình code được tiền huấn luyện biết về mã nguồn. Ví dụ, Wan et al. [58] tiến hành phân tích cấu trúc để chứng minh rằng các mô hình được tiền huấn luyện nhận thức về cấu trúc cú pháp. López et al [21] phục hồi AST từ các biểu diễn ẩn của các mô hình ngôn ngữ được tiền huấn luyện. Karmakar et al [30] cũng đề xuất một số tác vụ probing mới để điều tra những gì các mô hình code được tiền huấn luyện biết về code. Được truyền cảm hứng từ quá trình biên dịch và phân tích tĩnh, trước tiên chúng tôi đề xuất bốn tác vụ probing liên quan đến các thuộc tính code từ vựng, cú pháp, ngữ nghĩa và cấu trúc. Sau đó chúng tôi điều tra các thuộc tính code nào được mã hóa trong các biểu diễn được tiền huấn luyện theo từng lớp và điều gì xảy ra với các biểu diễn này trong quá trình fine-tuning.

6.2 Tăng tốc Quá trình Fine-tuning

Có nhiều nghiên cứu về tăng tốc quá trình fine-tuning [22,26, 29,35,59,60]. Các nghiên cứu này có thể được phân loại thô thành hai loại. Loại đầu tiên là sử dụng kỹ thuật chưng cất kiến thức để nén các mô hình ngôn ngữ được tiền huấn luyện quy mô lớn [29,35,60]. Ví dụ, Jiao et al. [29] đề xuất TinyBERT để chưng cất BERT và chỉ sử dụng khoảng 28% tham số để hiểu ngôn ngữ tự nhiên. Loại thứ hai là phương pháp fine-tuning dựa trên adapter [22,59], trong đó các adapter là các module có thể huấn luyện mới được thêm vào giữa các lớp của các mô hình được tiền huấn luyện. Ví dụ, Houlsby et al.l [22] thiết kế một số adapter với số lượng tham số ít hơn hai bậc độ lớn để fine-tune so với các mô hình đầy đủ và đạt được hiệu suất tương tự với fine-tuning tất cả các tham số của mô hình được tiền huấn luyện. Ngoài ra, có một số nghiên cứu về huấn luyện mạng thần kinh hiệu quả từ đầu với đóng băng lớp [28,62]. Ví dụ, Wang et al. [62] tận dụng kiến thức từ một mô hình tham chiếu để đánh giá chính xác tính linh hoạt huấn luyện của các lớp riêng lẻ, đóng băng những lớp đã hội tụ và mở đóng băng các lớp bị đóng băng để tiếp tục huấn luyện. Nghiên cứu của chúng tôi có thể thúc đẩy các nhà nghiên cứu đưa ra các phương pháp fine-tuning hiệu quả hơn.

7 KẾT LUẬN

Trong bài báo này, trước tiên chúng tôi tiến hành nghiên cứu thực nghiệm mở rộng để khám phá điều gì xảy ra với kiến thức code theo từng lớp và các biểu diễn được tiền huấn luyện trong quá trình fine-tuning. Sau đó chúng tôi đề xuất các phương án thay thế hiệu quả để fine-tune mô hình code được tiền huấn luyện lớn dựa trên các phát hiện trên. Nghiên cứu thực nghiệm của chúng tôi cho thấy rằng các thuộc tính từ vựng, cú pháp và cấu trúc của mã nguồn chủ yếu được nắm bắt trong các lớp thấp hơn, trung gian và cao hơn tương ứng, trong khi thuộc tính ngữ nghĩa trải rộng trên toàn bộ mô hình. Các thuộc tính code cơ bản được nắm bắt bởi các lớp thấp hơn và trung gian vẫn được bảo tồn trong quá trình fine-tuning. Hơn nữa, chúng tôi thấy rằng chỉ có các biểu diễn của hai lớp trên cùng thay đổi nhiều nhất trong quá trình fine-tuning cho các tác vụ downstream khác nhau. Dựa trên các phát hiện trên, chúng tôi đề xuất Telly-𝐾 để fine-tune hiệu quả các mô hình code được tiền huấn luyện thông qua đóng băng lớp có chọn lọc. Các thí nghiệm mở rộng trên năm tác vụ downstream khác nhau chứng minh rằng cả tham số huấn luyện và chi phí thời gian có thể được giảm rất nhiều, trong khi hiệu suất tương tự hoặc thậm chí tốt hơn.

Hơn nữa, nghiên cứu thực nghiệm của chúng tôi cho thấy nhiều phát hiện hữu ích và hướng nghiên cứu hứa hẹn cho fine-tuning hiệu quả của các mô hình code được tiền huấn luyện. Ví dụ, chúng tôi thấy rằng các mô hình code được tiền huấn luyện mã hóa các thuộc tính cú pháp vào các lớp trung gian. Do đó, khi các tác vụ downstream nhận thức về cú pháp như tạo code, chúng ta có thể thiết kế một số module để tận dụng các biểu diễn ngữ cảnh của các lớp trung gian. Trong tương lai, chúng tôi sẽ tiếp tục khám phá các hướng nghiên cứu hứa hẹn khác nhau và đề xuất các phương pháp fine-tuning hiệu quả hơn.

Gói sao chép bao gồm mã nguồn, bộ dữ liệu, và Phụ lục trực tuyến có sẵn tại: https://github.com/ DeepSoftwareAnalytics/Telly.

LỜI CẢM ỞN

Chúng tôi cảm ơn các nhà đánh giá đã có những bình luận quý giá về công việc này. Nghiên cứu này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc (Số 2017YFA0700800) và Quỹ Nghiên cứu Cơ bản cho các Đại học Trung ương theo Grant xtr072022001.

TÀI LIỆU THAM KHẢO

[1]Samira Abnar, Lisa Beinborn, Rochelle Choenni, và Willem H. Zuidema. 2019. Blackbox Meets Blackbox: Representational Similarity & Stability Analysis of

--- TRANG 12 ---
ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA Shi, et al.

Neural Language Models and Brains. Trong BlackboxNLP@ACL. Association for Computational Linguistics, 191–203.
[2]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong NAACL-HLT. Association for Computational Linguistics, 2655–2668.
[3]Alfred V Aho, Monica S Lam, Ravi Sethi, và Jeffrey D Ullman. 2007. Compilers: principles, techniques, & tools. Pearson Education India.
[4]Haldun Akoglu. 2018. User's guide to correlation coefficients. Turkish journal of emergency medicine 18, 3 (2018), 91–93.
[5]Miltiadis Allamanis và Charles Sutton. 2013. Mining Source Code Repositories at Massive Scale using Language Modeling. Trong 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 207–216.
[6]Satanjeev Banerjee và Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Trong IEEvaluation@ACL.
[7]Hangbo Bao, Li Dong, Songhao Piao, và Furu Wei. 2022. BEiT: BERT Pre-Training of Image Transformers. Trong ICLR. OpenReview.net.
[8]Yoshua Bengio, Patrice Simard, và Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks 5, 2 (1994), 157–166.
[9]Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu, và Baishakhi Ray. 2022. NatGen: Generative pre-training by" Naturalizing" source code. (2022).
[10] Grzegorz Chrupala và Afra Alishahi. 2019. Correlating Neural and Symbolic Representations of Language. Trong ACL (1). Association for Computational Linguistics, 2952–2962.
[11] Yingnong Dang, Song Ge, Ray Huang, và Dongmei Zhang. 2011. Code clone detection experience at Microsoft. Trong Proceedings of the 5th International Workshop on Software Clones. 63–64.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong NAACL-HLT (1). Association for Computational Linguistics, 4171–4186.
[13] Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray, và Saikat Chakraborty. 2022. Towards Learning (Dis)-Similarity of Source Code from Program Contrasts. Trong ACL (1). Association for Computational Linguistics, 6300–6312.
[14] Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, và Dongmei Zhang. 2021. Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning Approach for Semantic Code Search. Trong Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2994–2998.
[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong EMNLP (Findings).
[16] Xiaodong Gu, Hongyu Zhang, và Sunghun Kim. 2018. Deep code search. Trong ICSE. ACM, 933–944.
[17] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, và Jian Yin. 2022. UniXcoder: Unified Cross-Modal Pre-training for Code Representation. Trong ACL (1). Association for Computational Linguistics, 7212–7225.
[18] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, và Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/ forum?id=jLoC4ez43PZ
[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross B. Girshick. 2022. Masked Autoencoders Are Scalable Vision Learners. Trong CVPR. IEEE, 15979–15988.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.
[21] José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado, và Houari Sahraoui. 2022. AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models. Trong 37th IEEE/ACM International Conference on Automated Software Engineering. 1–11.
[22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. Trong ICML (Proceedings of Machine Learning Research, Vol. 97). PMLR, 2790–2799.
[23] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http://arxiv.org/abs/ 1909.09436
[24] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. 2018. Mapping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018).
[25] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. Trong EMNLP. Association for Computational Linguistics, 1643–1652.
[26] Junguang Jiang, Yang Shu, Jianmin Wang, và Mingsheng Long. 2022. Transferability in Deep Learning: A Survey. arXiv preprint arXiv:2201.05867 (2022).
[27] Nan Jiang, Thibaud Lutellier, và Lin Tan. 2021. CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. Trong ICSE. IEEE, 1161–1173.
[28] Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, và Chuanxiong Guo. 2020. A unified architecture for accelerating distributed {DNN}training in heterogeneous{GPU/CPU}clusters. Trong 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). 463–479.
[29] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language Understanding. Trong EMNLP (Findings) (Findings of ACL, Vol. EMNLP 2020). Association for Computational Linguistics, 4163–4174.
[30] Anjan Karmakar và Romain Robbes. 2021. What do pre-trained code models know about code?. Trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1332–1336.
[31] Nikolaus Kriegeskorte, Marieke Mur, và Peter A Bandettini. 2008. Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience (2008), 4.
[32] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. Trong ACL.
[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).
[34] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. CoRR abs/2102.04664 (2021).
[35] Wenhao Lu, Jian Jiao, và Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. Trong Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2645–2652.
[36] Thomas J McCabe. 1976. A complexity measure. IEEE Transactions on software Engineering 4 (1976), 308–320.
[37] Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, và Ian Tenney. 2020. What Happens To BERT Embeddings During Fine-tuning?. Trong BlackboxNLP@EMNLP. Association for Computational Linguistics, 33–44.
[38] Anders Møller và Michael I Schwartzbach. 2012. Static program analysis. Notes. Feb(2012).
[39] Lili Mou, Ge Li, Lu Zhang, Tao Wang, và Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing. Trong Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. 1287–1293.
[40] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, và Bin Luo. 2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations. Trong ICSE. ACM, 1–13.
[41] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. Trong ACL. ACL, 311–318.
[42] Julian Aron Prenner và Romain Robbes. 2021. Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs. arXiv preprint arXiv:2111.03922 (2021).
[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2020), 140:1–140:67.
[44] Mark Sanderson. 2010. Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. ISBN-13 978-0-521-86571-5, xxi+ 482 pages. Natural Language Engineering 16, 1 (2010), 100–103.
[45] Patrick Schober, Christa Boer, và Lothar A Schwarte. 2018. Correlation coefficients: appropriate use and interpretation. Anesthesia & Analgesia 126, 5 (2018), 1763–1768.
[46] Ensheng Shi, Wenchao Gub, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, và Hongbin Sun. 2022. Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation. arXiv preprint arXiv:2204.03293 (2022).
[47] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, và Hongbin Sun. 2022. On the Evaluation of Neural Code Summarization. Trong ICSE.
[48] Emma Strubell, Ananya Ganesh, và Andrew McCallum. 2020. Energy and Policy Considerations for Modern Deep Learning Research. Trong AAAI. AAAI Press, 13693–13696.
[49] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, và Mohammad Mamun Mia. 2014. Towards a big data curated benchmark of inter-project code clones. Trong 2014 IEEE International Conference on Software Maintenance and Evolution. IEEE, 476–480.

--- TRANG 13 ---
Hướng tới Fine-tuning Hiệu quả các Mô hình Code Được Tiền huấn luyện ISSTA 2023, 17-21 Tháng 7, 2023, Seattle, USA

[50] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, và Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. Trong Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1433–1443.
[51] Telly. 2023. Replication Package. ISSTA (2023). https://github.com/ DeepSoftwareAnalytics/Telly
[52] Ian Tenney, Dipanjan Das, và Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. Trong ACL (1). Association for Computational Linguistics, 4593–4601.
[53] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, và Ellie Pavlick. 2019. What do you learn from context? Probing for sentence structure in contextualized word representations. Trong ICLR (Poster). OpenReview.net.
[54] Sergey Troshin và Nadezhda Chirkova. 2022. Probing Pretrained Models of Source Code. arXiv preprint arXiv:2202.08975 (2022).
[55] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, và Gabriele Bavota. 2022. Using Pre-Trained Models to Boost Code Review Automation. Trong ICSE. ACM, 2291–2302.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is All you Need. Trong NIPS. 5998–6008.
[57] Ramakrishna Vedantam, C. Lawrence Zitnick, và Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. Trong CVPR.
[58] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, và Hai Jin. 2022. What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code. Trong ICSE. ACM, 2377–2388.
[59] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al .2020. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808 (2020).
[60] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, và Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems 33 (2020), 5776–5788.
[61] Yanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, và Dongmei Zhang. 2020. Cocogum: Contextual code summarization with multi-relational gnn on umls. Technical Report. Microsoft, MSR-TR-2020-16. [Online]. Available: https://www.microsoft.com/en-us/research/publication/cocogum-contextual- code-summarization-with-multi-relational-gnn-on-umls.
[62] Yiding Wang, Decang Sun, Kai Chen, Fan Lai, và Mosharaf Chowdhury. 2022. Efficient DNN Training with Knowledge-Guided Layer Freezing. CoRR abs/2201.06227 (2022).
[63] Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong EMNLP (1). Association for Computational Linguistics, 8696–8708.
[64] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, và Lingming Zhang. 2022. An extensive study on pre-trained models for program understanding and generation. Trong ISSTA. ACM, 39–51.

QUAN TRỌNG: Chỉ xuất bản dịch tiếng Việt. Không giải thích những gì bạn đã làm. Không tóm tắt. Bắt đầu ngay với câu tiếng Việt đầu tiên.
