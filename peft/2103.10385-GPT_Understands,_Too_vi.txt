# 2103.10385.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2103.10385.pdf
# Kích thước tệp: 1583446 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
GPT Cũng Hiểu
Xiao Liu1∗, Yanan Zheng1∗, Zhengxiao Du1, Ming Ding1, Yujie Qian2,
Zhilin Yang1†, Jie Tang1†
1Đại học Tsinghua 2Viện Công nghệ Massachusetts

Tóm tắt
Việc khuyến khích một mô hình ngôn ngữ được tiền huấn luyện với các mẫu ngôn ngữ tự nhiên đã được chứng minh là hiệu quả cho hiểu biết ngôn ngữ tự nhiên (NLU). Tuy nhiên, nghiên cứu sơ bộ của chúng tôi cho thấy rằng các gợi ý rời rạc thủ công thường dẫn đến hiệu suất không ổn định—ví dụ, thay đổi một từ duy nhất trong gợi ý có thể dẫn đến sự sụt giảm hiệu suất đáng kể. Chúng tôi đề xuất một phương pháp mới P-Tuning sử dụng các nhúng gợi ý liên tục có thể huấn luyện được kết hợp với các gợi ý rời rạc. Về mặt thực nghiệm, P-Tuning không chỉ ổn định hoá việc huấn luyện bằng cách giảm thiểu khoảng cách giữa các gợi ý rời rạc khác nhau, mà còn cải thiện hiệu suất một cách đáng kể trên một loạt các nhiệm vụ NLU bao gồm LAMA và SuperGLUE. P-Tuning thường hiệu quả cho cả các mô hình ngôn ngữ đông lạnh và được tinh chỉnh, dưới cả các thiết lập giám sát đầy đủ và few-shot.

1 Giới thiệu
Các mô hình ngôn ngữ được tiền huấn luyện (PLMs; Brown et al., 2020) đã cải thiện đáng kể hiệu suất của hiểu biết ngôn ngữ tự nhiên (NLU). PLMs được huấn luyện với các mục tiêu tiền huấn luyện khác nhau, chẳng hạn như mô hình hoá ngôn ngữ có mặt nạ (Devlin et al., 2018), mô hình hoá ngôn ngữ tự hồi quy (Radford et al., 2019), seq2seq (Raffel et al., 2019), và mô hình hoá ngôn ngữ hoán vị (Yang et al., 2019).

PLMs có thể được tăng cường thêm với việc khuyến khích (Brown et al., 2020; Schick và Schütze, 2020), sử dụng các mẫu gợi ý được viết thủ công làm đầu vào bổ sung cho một mô hình ngôn ngữ. Với việc khuyến khích trong khi PLMs hoặc được tinh chỉnh trên một tập dữ liệu có nhãn nhỏ hoặc được đông lạnh để suy luận trực tiếp trên các nhiệm vụ hạ lưu. Việc khuyến khích đã cải thiện đáng kể hiệu suất của nhiều nhiệm vụ NLU (Brown et al., 2020; Schick và Schütze, 2020).

†tác giả liên hệ: Zhilin Yang (zhiliny@tsinghua.edu.cn) và Jie Tang (jietang@tsinghua.edu.cn)
∗đóng góp bằng nhau.

Hình 1: Điểm số trung bình trên 7 tập dữ liệu dev của SuperGLUE sử dụng P-Tuning.

Prompt P@1
w/o PT P@1
w/ PT
[X] is located in [Y]. (gốc) 31.3 57.8
[X] is located in which country or state? [Y]. 19.8 57.8
[X] is located in which country? [Y]. 31.4 58.1
[X] is located in which country? In [Y]. 51.1 58.1

Bảng 1: Các gợi ý rời rạc gặp phải tình trạng không ổn định (phương sai cao), trong khi P-Tuning ổn định hoá và cải thiện hiệu suất. Kết quả là precision@1 trên LAMA-TREx P17 với BERT-base-cased. "PT" đề cập đến P-Tuning, huấn luyện các gợi ý liên tục bổ sung kết hợp với các gợi ý rời rạc.

Tuy nhiên, chúng tôi quan sát thấy rằng các gợi ý rời rạc thủ công gặp phải một mức độ không ổn định lớn. Như được hiển thị trong Bảng 1, với một mô hình ngôn ngữ đông lạnh, việc thay đổi một từ duy nhất trong gợi ý có thể dẫn đến sự sụt giảm hiệu suất đáng kể. Như chúng tôi sẽ chỉ ra trong Phần 3, khi mô hình ngôn ngữ được tinh chỉnh, vấn đề không ổn định được giảm bớt nhưng sự khác biệt về hiệu suất giữa các gợi ý khác nhau vẫn đáng kể, đặc biệt là trong thiết lập few-shot. Vấn đề không ổn định như vậy của các gợi ý rời rạc đặt ra một thách thức quan trọng trong thực tế. Các cách tiếp cận gần đây về việc khuyến khích tự động đã cố gắng tìm kiếm một gợi ý có hiệu suất tốt hơn cho một nhiệm vụ cụ thể (Shin et al., 2020; Gao et al., 2020; Jiang et al., 2020b), nhưng các phương pháp này không thay đổi bản chất không ổn định của các gợi ý rời rạc.

Để giảm tình trạng không ổn định của các gợi ý rời rạc, chúng tôi đề xuất một phương pháp mới P-Tuning sử dụng các nhúng gợi ý liên tục có thể huấn luyện được kết hợp với các gợi ý rời rạc. Cụ thể, arXiv:2103.10385v2 [cs.CL] 25 Oct 2023

--- TRANG 2 ---
khi có một gợi ý rời rạc làm đầu vào, P-Tuning kết hợp các nhúng gợi ý liên tục với các token gợi ý rời rạc và đưa chúng làm đầu vào cho mô hình ngôn ngữ. Các gợi ý liên tục được cập nhật bởi lan truyền ngược để tối ưu hoá mục tiêu nhiệm vụ. Trực quan là các gợi ý liên tục tích hợp một mức độ khả năng học nhất định vào đầu vào, có thể học cách bù đắp tác động của những thay đổi nhỏ trong các gợi ý rời rạc để cải thiện tính ổn định của việc huấn luyện. Để cải thiện hiệu suất hơn nữa, chúng tôi sử dụng một bộ mã hoá gợi ý sử dụng LSTMs hoặc MLPs để mô hình hoá sự phụ thuộc giữa các nhúng gợi ý liên tục.

Chúng tôi thử nghiệm với hai chuẩn mực NLU: LAMA (Petroni et al., 2019) để thăm dò kiến thức và SuperGLUE (Wang et al., 2019a). Trên LAMA, với mô hình ngôn ngữ đông lạnh, P-Tuning vượt trội hơn các gợi ý rời rạc thủ công và các gợi ý được tìm kiếm tương ứng 20+ điểm và 9 điểm với cùng các mô hình được tiền huấn luyện. Trên SuperGLUE, với mô hình ngôn ngữ được tinh chỉnh, P-Tuning vượt trội hơn PET (Schick và Schütze, 2020) với các gợi ý rời rạc tốt nhất dưới cả thiết lập giám sát đầy đủ và few-shot. Ngoài việc cải thiện hiệu suất, kết quả của chúng tôi cho thấy rằng trên một loạt các nhiệm vụ và thiết lập, P-Tuning giảm đáng kể khoảng cách hiệu suất giữa các gợi ý rời rạc khác nhau, dẫn đến cải thiện tính ổn định cho việc thích ứng mô hình ngôn ngữ.

2 Phương pháp

2.1 Vấn đề với các gợi ý rời rạc

Việc khuyến khích sử dụng các mẫu ngôn ngữ tự nhiên làm đầu vào bổ sung cho các mô hình ngôn ngữ được tiền huấn luyện để thích ứng với các nhiệm vụ hạ lưu (Brown et al., 2020; Schick và Schütze, 2020). Nghiên cứu trước đây (Zheng et al., 2021) đã chỉ ra rằng việc khuyến khích đã đạt được những cải thiện nhất quán và đáng kể trên một số nhiệm vụ NLP. Tuy nhiên, vẫn còn là một vấn đề thách thức về cách viết các gợi ý rời rạc có hiệu suất cao.

Chúng tôi đã thực hiện các thí nghiệm sơ bộ sử dụng các gợi ý thủ công khác nhau trên nhiệm vụ thăm dò kiến thức LAMA (Petroni et al., 2019), nhằm trích xuất kiến thức triplet từ một mô hình ngôn ngữ bằng cách dự đoán các thực thể đuôi. Kết quả trong Bảng 1 cho thấy rằng các gợi ý rời rạc thủ công dẫn đến hiệu suất không ổn định. Ví dụ, nếu chúng ta so sánh hai gợi ý cuối cùng trong bảng, việc thay đổi một từ duy nhất trong gợi ý gây ra sự giảm mạnh 20 điểm trong hiệu suất.

Trước thách thức này, các nghiên cứu gần đây đề xuất tự động hoá quy trình tìm kiếm các gợi ý rời rạc bằng cách khai thác kho dữ liệu huấn luyện (Jiang et al., 2020b), tìm kiếm dựa trên gradient (Shin et al., 2020), và sử dụng các mô hình sinh được tiền huấn luyện (Gao et al., 2020). Tuy nhiên, các nghiên cứu này nhằm tìm kiếm các gợi ý có hiệu suất tốt hơn nhưng không thay đổi bản chất không ổn định của các gợi ý rời rạc. Ngoài vấn đề không ổn định, việc tìm kiếm trong không gian rời rạc có thể không khai thác được đầy đủ các gradient từ lan truyền ngược, có thể dẫn đến các giải pháp không tối ưu. Vì vậy, chúng tôi khám phá khả năng huấn luyện các gợi ý liên tục để ổn định hoá và cải thiện hiệu suất của việc thích ứng mô hình ngôn ngữ.

2.2 P-Tuning

Chính thức, gọi M là một mô hình ngôn ngữ được tiền huấn luyện với kích thước ẩn h và kích thước từ vựng |V|. Gọi {(xi,yi)} là một tập dữ liệu có nhãn cho một nhiệm vụ NLU, trong đó x0:n={x0, x1, ..., xn} là một đầu vào bao gồm một chuỗi các token rời rạc, và y∈Y là một nhãn. Mục tiêu của chúng tôi là ước lượng xác suất có điều kiện để phân loại fM(x) = p̂(y|x) với các tham số của M hoặc được tinh chỉnh hoặc đông lạnh.

Việc khuyến khích được đề xuất dưới dạng các token rời rạc (Schick và Schütze, 2020). Gọi [Di] là một token gợi ý rời rạc. Mỗi gợi ý có thể được mô tả như một mẫu T={[D0:i],x,[D(i+1):j],y,[D(j+1):k]}, có thể tổ chức dữ liệu có nhãn (bao gồm đầu vào x và nhãn y) thành một chuỗi các token văn bản, sao cho nhiệm vụ có thể được tái công thức hoá như việc điền vào chỗ trống của văn bản đầu vào. Ví dụ, đối với nhiệm vụ dự đoán thủ đô của một quốc gia (LAMA-TREx P36), một gợi ý có thể là "The capital of [INPUT] is [LABEL]." Với một phần dữ liệu có nhãn "(Britain, London)", văn bản được tái công thức hoá sẽ là "The capital of Britain is [MASK].", trong đó "[MASK]" sẽ dự đoán nhãn đã cho "London". Cả gợi ý rời rạc và dữ liệu rời rạc đều được ánh xạ thành các nhúng đầu vào:
{e(D0)...e(Di),e(x0), ...,e(xn), ...,e(Dk)}
thông qua lớp nhúng được tiền huấn luyện, trong đó e∈R|V|×d.

Tuy nhiên, như đã thảo luận trong Phần 2.1, các gợi ý rời rạc như vậy có xu hướng cực kỳ không ổn định và có thể không tối ưu với lan truyền ngược. Vì vậy, chúng tôi đề xuất P-Tuning sử dụng các nhúng gợi ý liên tục để cải thiện và ổn định hoá

--- TRANG 3 ---
[Hình ảnh mô tả kiến trúc P-Tuning với các thành phần: Mô hình ngôn ngữ được tiền huấn luyện (GPT, BERT, ...), Bộ mã hoá gợi ý, các token gợi ý giả [P0], [Pi], [Pi+1], [Pm], các vector ẩn h0, hi, hi+1, hm, và ví dụ với "The capital of Britain is [MASK]"]

Hình 2: Một ví dụ về tìm kiếm gợi ý cho "The capital of Britain is [MASK]". Cho trước ngữ cảnh (vùng màu xanh, "Britain") và mục tiêu (vùng màu đỏ, "[MASK]"), vùng màu cam đề cập đến gợi ý. Trong (a), bộ tạo gợi ý chỉ nhận được các phần thưởng rời rạc; ngược lại, trong (b) các nhúng gợi ý liên tục và bộ mã hoá gợi ý có thể được tối ưu hoá một cách có thể vi phân.

việc khuyến khích. Gọi [Pi] là nhúng gợi ý liên tục thứ i. Mẫu gợi ý cho P-Tuning như sau:
T={[P0:i],x,[P(i+1):j],y,[P(j+1):k]}

P-Tuning tận dụng một hàm nhúng bổ sung f: [Pi]→hi để ánh xạ mẫu thành
{h0, ..., hi,e(x), hi+1, ..., hj,e(y), hj+1, ..., hk}

Cuối cùng, chúng tôi cập nhật các nhúng {Pi}k i=1 để tối ưu hoá một hàm mất mát nhiệm vụ.

Đáng chú ý là chúng ta cũng có thể kết hợp các gợi ý rời rạc với các gợi ý liên tục, điều này hoạt động tốt hơn và được áp dụng xuyên suốt các thí nghiệm của chúng tôi. P-Tuning áp dụng được cho cả các mô hình ngôn ngữ đông lạnh và được tinh chỉnh.

2.3 Bộ mã hoá gợi ý

Trong khung work nêu trên, chúng tôi sử dụng một hàm ánh xạ f để ánh xạ các nhúng có thể huấn luyện {Pi} thành các đầu vào mô hình {hi}. Trực quan là bằng cách sử dụng một hàm ánh xạ, việc mô hình hoá sự phụ thuộc giữa các nhúng gợi ý khác nhau sẽ thuận tiện hơn, so với việc sử dụng các nhúng học được độc lập. Trong việc triển khai của chúng tôi, chúng tôi sử dụng một mạng neural nhẹ để công thức hoá hàm f. Cụ thể, chúng tôi thử nghiệm với việc sử dụng các mạng bộ nhớ dài-ngắn hạn (LSTM), các perceptron đa lớp (MLPs), và hàm ánh xạ đồng nhất trong Phần 3.

3 Thí nghiệm

Chúng tôi bao gồm hai chuẩn mực NLU: LAMA (Petroni et al., 2019) để thăm dò kiến thức (§ 3.1) và SuperGLUE (Wang et al., 2019a) để hiểu biết ngôn ngữ tự nhiên tổng quát. Trên SuperGLUE, chúng tôi xem xét cả thiết lập học có giám sát đầy đủ (§ 3.2) và học few-shot (§ 3.3).

LAMA Full SG Few SG
đông lạnh tinh chỉnh tinh chỉnh
Cải thiện ✓ ✓ ✓
Ổn định hoá ✓ ✗ ✓

Bảng 2: Các thiết lập nhiệm vụ và tóm tắt kết quả trong các thí nghiệm của chúng tôi. P-tuning cho thấy sự cải thiện so với các đường cơ sở trên tất cả các thiết lập nhiệm vụ, và có thể ổn định hoá hiệu suất trên LAMA và Few SG. Đối với Full SG, khoảng cách giữa các gợi ý rời rạc không lớn và việc huấn luyện ổn định ngay cả khi không có P-Tuning. (Full SG: học có giám sát đầy đủ trên SuperGLUE; Few SG: few-shot SuperGLUE; Cải thiện: hiệu suất tổng thể được cải thiện; Ổn định hoá: việc huấn luyện được ổn định hoá bằng cách giảm thiểu sự khác biệt giữa các gợi ý rời rạc).

Trên LAMA, theo Shin et al. (2020); Jiang et al. (2020b), các mô hình ngôn ngữ được đông lạnh và chỉ các gợi ý rời rạc hoặc liên tục được tinh chỉnh. Đối với SuperGLUE, theo Schick và Schütze (2020); Zheng et al. (2021), các mô hình ngôn ngữ được tinh chỉnh. Trong thiết lập của chúng tôi, chúng tôi tối ưu hoá đồng thời các tham số mô hình ngôn ngữ và các gợi ý liên tục. Thiết lập này không chỉ tuân theo các thiết lập tiêu chuẩn, phổ biến trong các nghiên cứu trước đây, mà còn cho phép đánh giá P-Tuning với cả các mô hình ngôn ngữ được tinh chỉnh và đông lạnh.

Thiết lập nhiệm vụ tổng thể và tóm tắt kết quả được hiển thị trong Bảng 2.

3.1 Thăm dò kiến thức

3.1.1 Thiết lập

Thăm dò kiến thức, hay còn gọi là truy xuất sự kiện, đánh giá mức độ kiến thức thế giới thực mà các mô hình ngôn ngữ đã có được từ việc tiền huấn luyện. Tập dữ liệu LAMA (Petroni et al., 2019) đánh giá nó với các bài kiểm tra điền từ được tạo từ các triplet được chọn trong các cơ sở kiến thức.

Tập dữ liệu và từ vựng. LAMA yêu cầu tất cả

--- TRANG 4 ---
Loại gợi ý Model P@1
Gốc
(MP) BERT-base 31.1
BERT-large 32.3
E-BERT 36.2
Rời rạc LPAQA (BERT-base) 34.1
LPAQA (BERT-large) 39.4
AutoPrompt (BERT-base) 43.3
P-tuning BERT-base 48.3
BERT-large 50.6

Model MP P-tuning
BERT-base (109M) 31.7 52.3 (+20.6)
-AutoPrompt (Shin et al., 2020) - 45.2
BERT-large (335M) 33.5 54.6 (+21.1)
RoBERTa-base (125M) 18.4 49.3 (+30.9)
-AutoPrompt (Shin et al., 2020) - 40.0
RoBERTa-large (355M) 22.1 53.5 (+31.4)
GPT2-medium (345M) 20.3 46.5 (+26.2)
GPT2-xl (1.5B) 22.8 54.4 (+31.6)
MegatronLM (11B) 23.1 64.2 (+41.1)

Bảng 3: Thăm dó kiến thức Precision@1 trên LAMA-34k (trái) và LAMA-29k (phải). P-tuning vượt trội hơn tất cả các đường cơ sở tìm kiếm gợi ý rời rạc. (MP: Gợi ý thủ công; PT: P-tuning).

các câu trả lời ở định dạng token đơn. Đầu tiên chúng tôi áp dụng tập dữ liệu LAMA-TREx gốc, bao gồm 41 quan hệ Wikidata và tổng cộng 34,039 triplet kiểm tra (tức là LAMA-34k, bao phủ tất cả từ vựng BERT). Vì các mô hình được tiền huấn luyện khác nhau chia sẻ các từ vựng riêng biệt, để cho phép so sánh trực tiếp, chúng tôi theo nghiên cứu trước đây (Shin et al., 2020) để áp dụng một tập con bao phủ giao điểm của từ vựng GPT và BERT. Điều này được gọi là LAMA-29k. Chúng tôi một lần nữa theo Shin et al. (2020) để xây dựng dữ liệu huấn luyện, phát triển, và kiểm tra để cho phép so sánh công bằng.

Thiết lập. LAMA đã cung cấp một gợi ý thủ công cho mỗi quan hệ, như được hiển thị trong Bảng 1, chúng hiệu quả nhưng có thể không tối ưu. Đối với các mô hình ngôn ngữ có mặt nạ hai chiều, chúng tôi chỉ cần thay thế "[X]" bằng thực thể chủ và "[Y]" bằng token [MASK]; đối với các mô hình ngôn ngữ một chiều như GPT, theo thiết lập gốc của LAMA trên Transformer-XL (Dai et al., 2019), chúng tôi sử dụng đầu ra mạng ngay trước vị trí mục tiêu.

Số lượng token gợi ý và vị trí được chọn dựa trên các tập phát triển, và để đơn giản chúng tôi chọn mẫu (3, sub, org_prompt, 3, obj, 3) cho các mô hình hai chiều và (3, sub, org_prompt, 3, obj) cho các mô hình một chiều vì cấu hình này hoạt động tốt cho hầu hết các quan hệ (trong đó số chỉ ra số lượng token gợi ý liên tục). Các gợi ý liên tục được kết hợp với các gợi ý rời rạc gốc. Trong quá trình huấn luyện gợi ý, chúng tôi đặt tốc độ học là 1e-5 và sử dụng bộ tối ưu Adam.

3.1.2 Kết quả chính

Kết quả được trình bày trong Bảng 3. P-tuning cải thiện đáng kể kết quả tốt nhất của thăm dò kiến thức từ 43.3% lên 50.6% trên LAMA-34k và từ 45.2% lên 64.2% trên LAMA-29k. Hơn nữa, P-tuning vượt trội hơn các phương pháp tìm kiếm gợi ý rời rạc trước đây như AutoPrompt (Shin et al., 2020) và LPAQA (Jiang et al., 2020b) trên các mô hình cùng kích thước. Điều này xác nhận trực quan của chúng tôi trong Phần 2 rằng các gợi ý rời rạc có thể không tối ưu.

3.2 Học có giám sát đầy đủ

3.2.1 Thiết lập

Tập dữ liệu. Để đánh giá P-tuning trên các nhiệm vụ học có giám sát đầy đủ, chúng tôi áp dụng chuẩn mực SuperGLUE (Wang et al., 2019b), bao gồm 8 nhiệm vụ hiểu biết ngôn ngữ tự nhiên (NLU) thách thức. Chúng tôi tập trung vào 7 trong số đó vì nhiệm vụ ReCoRD (Zhang et al., 2018) không áp dụng các gợi ý rời rạc, do đó P-tuning không trực tiếp áp dụng được. Các nhiệm vụ bao gồm trả lời câu hỏi (BoolQ (Clark et al., 2019a) & MultiRC (Khashabi et al., 2018)), suy luận văn bản (CB (De Marneffe et al., 2019) & RTE (Dagan et al., 2005)), giải quyết đồng tham chiếu (WiC (Pilehvar và Camacho-Collados, 2018)), lý luận nhân quả (COPA (Roemmele et al., 2011)), và phân biệt nghĩa từ (WSC (Levesque et al., 2012)).

Phương pháp so sánh. Chúng tôi thử nghiệm P-tuning trên cả các mô hình được tiền huấn luyện một chiều và hai chiều, tức là GPT và BERT. Chúng tôi bao gồm bốn biến thể BERT-Base, BERT-Large, GPT2-Base, và GPT-medium. Đối với mỗi mô hình, chúng tôi so sánh việc tinh chỉnh phân loại tiêu chuẩn, PET (Schick và Schütze, 2020) (một phương pháp tinh chỉnh điển hình dựa trên các gợi ý rời rạc thủ công) và P-tuning của chúng tôi.

Cấu hình. Chúng tôi sử dụng cùng các chỉ số như trong (Wang et al., 2019b). Đối với học có giám sát đầy đủ, chúng tôi sử dụng một tập huấn luyện lớn để tinh chỉnh các mô hình được tiền huấn luyện và sử dụng một tập phát triển cho siêu

--- TRANG 5 ---
(a) Hiệu suất có giám sát đầy đủ với các mô hình quy mô cơ sở.

Phương pháp BoolQ CB WiC RTE MultiRC WSC COPA Trung bình
(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)

BERT-Base
(109M) CLS-FT 72.9 85.1 73.9 71.1 68.4 16.2 66.3 63.5 67.0 66.2
PET-FT 73.7 87.5 90.8 67.9 70.4 13.7 62.5 60.6 70.0 67.1
P-tuning 73.9 89.2 92.1 68.8 71.1 14.8 63.3 63.5 72.0 68.4

GPT2-Base
(117M) CLS-FT 71.2 78.6 55.8 65.5 67.8 17.4 65.8 63.0 64.4 63.0
PET-FT 74.8 87.5 88.1 68.0 70.0 23.5 69.7 66.3 78.0 70.2
P-tuning 75.0 91.1 93.2 68.3 70.8 23.5 69.8 63.5 76.0 70.4

(b) Hiệu suất có giám sát đầy đủ với các mô hình quy mô lớn.

Phương pháp BoolQ CB WiC RTE MultiRC WSC COPA Trung bình
(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)

BERT-Large
(335M) CLS-FT¹ 77.7 94.6 93.7 74.9 75.8 24.7 70.5 68.3 69.0 72.5
PET-FT 77.2 91.1 93.5 70.5 73.6 17.7 67.0 80.8 75.0 73.1
P-tuning 77.8 96.4 97.4 72.7 75.5 17.1 65.6 81.7 76.0 74.6

GPT2-Med.
(345M) CLS-FT 71.0 73.2 51.2 65.2 72.2 19.2 65.8 62.5 66.0 63.1
PET-FT 78.3 96.4 97.4 70.4 72.6 32.1 74.4 73.0 80.0 74.9
P-tuning 78.9 98.2 98.7 69.4 75.5 29.3 74.2 74.0 81.0 75.6

¹Chúng tôi báo cáo cùng kết quả được lấy từ SuperGLUE (Wang et al., 2019a).

Bảng 4: Hiệu suất có giám sát đầy đủ trên tập phát triển SuperGLUE.

tham số và lựa chọn mô hình. Cụ thể, bộ tối ưu AdamW với tốc độ học giảm tuyến tính được sử dụng để huấn luyện. Chúng tôi sử dụng tốc độ học {1e−5,2e−5,3e−5}, kích thước batch {16,32}, và tỷ lệ khởi động {0.0,0.05,0.1}. Đối với các tập dữ liệu nhỏ (tức là COPA, WSC, CB, RTE), chúng tôi tinh chỉnh các mô hình được tiền huấn luyện trong 20 epochs. Đối với các tập dữ liệu lớn hơn (tức là WiC, BoolQ, MultiRC), chúng tôi giảm số lượng epochs huấn luyện xuống 10 vì mô hình hội tụ sớm hơn. Dừng sớm được sử dụng để tránh overfitting dữ liệu huấn luyện.

3.2.2 Kết quả chính

Kết quả chính của học có giám sát đầy đủ được hiển thị trong Bảng 4. Chúng tôi quan sát rằng P-tuning có thể cải thiện hiệu suất học có giám sát đầy đủ trên cả BERTs và GPTs. (1) Cụ thể, trên mô hình BERT-Base, P-tuning đạt hiệu suất tốt nhất trên 5/7 nhiệm vụ, trong khi với BERT-Large, P-tuning vượt trội hơn các phương pháp khác trên 4/7 nhiệm vụ. Các ngoại lệ là WiC và MultiRC, cả hai đều có tập huấn luyện tương đối lớn. Chúng tôi thấy rằng P-tuning có thể không có lợi ích lớn so với CLS-FT trên các nhiệm vụ có nhiều tài nguyên như vậy, trong khi có lợi ích hơn trên các nhiệm vụ ít tài nguyên. Trung bình, P-tuning cải thiện so với các đường cơ sở được xem xét. (2) Trên các mô hình GPT2-Base và GPT2-Medium, P-tuning nhất quán đạt hiệu suất tốt nhất trên tất cả các nhiệm vụ.

3.3 Học Few-Shot

Trong khi GPT-3 đã cho thấy tiềm năng học few-shot khá tốt với các gợi ý được tạo thủ công, nó vẫn gặp khó khăn trên một số nhiệm vụ thách thức (ví dụ, suy luận ngôn ngữ tự nhiên) (Brown et al., 2020). Chúng tôi có động lực nghiên cứu xem P-tuning có thể cải thiện hiệu suất học few-shot của các mô hình được tiền huấn luyện trên các nhiệm vụ thách thức hay không.

3.3.1 Thiết lập

Đánh giá Few-shot. Hiệu suất few-shot nhạy cảm với nhiều yếu tố (ví dụ, thứ tự của các ví dụ huấn luyện, seed ngẫu nhiên, và mẫu gợi ý), và do đó gặp phải phương sai cao (Zhao et al., 2021a; Lu et al., 2021; Zhang et al., 2020). Vì vậy, chiến lược đánh giá few-shot nên đảm bảo rằng các cải thiện thực sự từ một phương pháp được cải thiện thay vì từ phương sai. Vì vậy, chúng tôi theo quy trình đánh giá FewNLU (Zheng et al., 2021) đã giải quyết và xử lý vấn đề này. Cụ thể, chúng tôi sử dụng các phân chia dữ liệu ngẫu nhiên để thực hiện lựa chọn mô hình chỉ trên một tập có nhãn nhỏ để ngăn chặn overfitting một tập dev lớn.

Tập dữ liệu. Chúng tôi sử dụng chuẩn mực SuperGLUE few-shot (còn được gọi là FewGLUE) (Schick và Schütze, 2020) và theo thiết lập trong nghiên cứu trước đây (Zheng et al., 2021) về mặt xây dựng phân chia dữ liệu.

Đường cơ sở và siêu tham số. Trong học few-shot,

--- TRANG 6 ---
chúng tôi một lần nữa so sánh P-tuning với PET (Schick và Schütze, 2020), đã được chứng minh là vượt trội hơn GPT-3 trên một số nhiệm vụ. Tương tự như (Schick và Schütze, 2020), chúng tôi sử dụng ALBERT-xxLarge làm mô hình cơ sở. Đối với các siêu tham số được chia sẻ bởi PET và P-tuning (ví dụ, tốc độ học, bước huấn luyện tối đa, tần suất đánh giá), chúng tôi sử dụng cùng không gian tìm kiếm để so sánh công bằng. Cụ thể, chúng tôi tìm kiếm tốc độ học trong {1e−5,2e−5}, bước huấn luyện tối đa trong {250,500}, và tần suất đánh giá trong {0.02,0.04}.

Xây dựng mẫu gợi ý. Đối với PET, chúng tôi sử dụng cùng các gợi ý thủ công được báo cáo bởi Schick và Schütze (2020). Khi xây dựng mẫu gợi ý cho P-tuning, dựa trên cùng các gợi ý thủ công như PET, chúng tôi chèn số lượng khác nhau của token gợi ý liên tục vào các vị trí khác nhau, do đó tạo ra một số ứng viên mẫu. Sau đó chúng tôi chọn mẫu tốt nhất cho P-tuning sử dụng chiến lược xác thực của FewNLU (Zheng et al., 2021). Chúng tôi cũng tiến hành phân tích thêm về số lượng và vị trí của các token gợi ý liên tục trong §3.3.3.

3.3.2 Kết quả chính

Hiệu suất Few-Shot. Bảng 5 hiển thị kết quả chính của học few-shot. Chúng tôi thấy rằng, trên ALBERT, P-tuning nhất quán vượt trội hơn PET trung bình hơn 1 điểm. Nó vượt trội hơn PromptTuning hơn 13 điểm. Điều này chứng minh rằng bằng cách tự động học các token gợi ý liên tục, các mô hình được tiền huấn luyện có thể đạt hiệu suất few-shot tốt hơn trên các nhiệm vụ NLU.

3.3.3 Nghiên cứu Ablation

Loại bộ mã hoá gợi ý. Nghiên cứu trước đây (Shin et al., 2020) đề xuất đơn giản sử dụng một MLP làm bộ mã hoá gợi ý, chúng tôi thực hiện phân tích ablation thêm cho việc lựa chọn bộ mã hoá gợi ý, và kết quả được hiển thị trong Bảng 8. Chúng tôi xem xét LSTM, MLP, và EMB (tức là chúng tôi trực tiếp tối ưu hoá các nhúng từ mà không sử dụng tham số bổ sung). Từ kết quả, chúng ta có thể thấy rằng LSTM, MLP, và EMB đều hoạt động như một bộ mã hoá gợi ý. Kết quả cho thấy rằng cả LSTM và MLP đều hoạt động tốt trên các nhiệm vụ này, trong khi EMB không ổn định và có thể hoạt động kém hơn đáng kể so với hai loại kia trên một số nhiệm vụ (ví dụ, WiC và CB). Tóm lại, cả LSTM và MLP đều có thể được tính đến khi làm việc trên các nhiệm vụ mới.

Vị trí của các token gợi ý. Để nghiên cứu vị trí nào để chèn các token gợi ý liên tục, chúng tôi thực hiện các thí nghiệm như Bảng 7 hiển thị. Từ kết quả, chúng tôi có các phát hiện sau.

1. Bằng cách so sánh #1 (hoặc #2) với #3 (hoặc #4), chúng tôi thấy rằng sẽ tốt hơn nếu chúng ta chèn các token gợi ý liên tục tại vị trí mà nó không phân đoạn các câu. Ví dụ, trong trường hợp #1, "[P]" phá vỡ tính hoàn chỉnh của câu "[Hypothesis]?" trong khi ở trường hợp #3, "[P]" được đặt giữa các câu.

2. Bằng cách so sánh #2 (hoặc #3) với #4, chúng tôi thấy rằng không có sự ưu tiên đặc biệt nào cho việc đặt ở rìa hay giữa các đầu vào.

3. Được khuyến nghị viết một số ứng viên mẫu và sau đó tìm kiếm chúng để tìm ra cái tốt nhất cho mỗi nhiệm vụ.

Số lượng token gợi ý. Chúng tôi cũng nghiên cứu ảnh hưởng của số lượng token gợi ý và hiển thị kết quả trong Bảng 7. Bằng cách so sánh #3, #6, #7, và #8, chúng ta có thể kết luận rằng số lượng token gợi ý có tác động lớn đến hiệu suất few-shot. Tuy nhiên, không phải là số lượng token gợi ý lớn hơn sẽ luôn tốt hơn. Chúng tôi suy đoán rằng có thể do dữ liệu huấn luyện hạn chế, việc học các tham số trở nên khó khăn khi tăng quá mức số lượng token gợi ý liên tục. Trong thực tế, được khuyến nghị tìm kiếm số lượng token gợi ý tốt nhất thông qua lựa chọn mô hình.

3.3.4 So sánh với tìm kiếm gợi ý rời rạc

Nghiên cứu trước đây (Gao et al., 2020) đề xuất tự động tìm kiếm các gợi ý rời rạc và đạt kết quả tốt hơn so với các gợi ý thủ công. Bây giờ chúng tôi tiến hành so sánh P-Tuning với các gợi ý rời rạc được tìm kiếm tự động. Để so sánh công bằng, chúng tôi theo thiết lập của LM-BFF (Gao et al., 2020) để cũng tiến hành thí nghiệm trên một số nhiệm vụ GLUE (Wang et al., 2018) với mô hình RoBERTa-Large (Liu et al., 2019). Vì các giao thức đánh giá có tác động lớn đến hiệu suất few-shot, chúng tôi sử dụng top-3 gợi ý rời rạc được tìm kiếm bởi LM-BFF và thí nghiệm chỉ sử dụng các gợi ý rời rạc và áp dụng thêm P-Tuning. Đối với P-Tuning, các mẫu gợi ý được xây dựng bằng cách kết hợp cùng các gợi ý rời rạc cũng như các gợi ý liên tục. Kết quả trong Bảng 9 cho thấy rằng việc kết hợp thêm các gợi ý liên tục có thể cải thiện hiệu suất few-shot hơn nữa. P-Tuning

--- TRANG 7 ---
Phương pháp BoolQ
(Acc.) RTE
(Acc.) WiC
(Acc.) CB
(Acc.) (F1.) MultiRC
(F1a.) (EM.) WSC
(Acc.) COPA
(Acc.) Trung bình

Prompt Tuning 58.47 ±1.00 54.42 ±3.05 52.74 ±2.36 75.45 ±2.25 67.73 ±5.70 59.28 ±4.73 15.03 ±4.11 74.04 ±2.99 61.50 ±4.36 58.56
PET-FT 76.70 ±1.85 72.83 ±1.30 53.87 ±4.47 84.38 ±4.47 62.56 ±7.66 76.51 ±1.52 36.46 ±2.13 80.05 ±2.53 81.75 ±4.03 70.74
P-tuning 76.55 ±2.68 63.27 ±3.63 55.49 ±1.21 88.39 ±3.72 84.24 ±5.15 75.91 ±1.74 38.01 ±0.78 78.85 ±1.76 85.25 ±3.30 71.81

Bảng 5: Hiệu suất few-shot của PET (Schick và Schütze, 2020), Prompt Tuning (Lester et al., 2021) và P-tuning của chúng tôi trên bảy nhiệm vụ dựa trên ALBERT. Mỗi kết quả được tính trung bình trên 4 lần chạy với các phân chia dữ liệu khác nhau. Kết quả cho thấy P-tuning nhất quán cải thiện hiệu suất few-shot trung bình hơn 1 điểm so với PET và hơn 13 điểm so với Prompt Tuning.

Phương pháp P#0 P#1 P#2 P#3 P#4 P#5 STD
FSL
(BoolQ) PET-FT 77.10 67.96 74.14 72.48 71.77 60.86 5.68
±2.21 ±2.69 ±1.38 ±4.31 ±2.56 ±3.99
P-tuning 75.41 75.11 73.43 71.35 71.31 65.86 3.52
±3.09 ±1.61 ±2.60 ±4.57 ±8.58 ±3.80

LAMA
(P17) MP 31.3 19.8 31.4 51.1 34.0 32.7 10.1
P-tuning 57.8 57.8 58.1 58.1 58.9 58.7 0.46

Bảng 6: Bảng trên: Học few-shot (FSL) của PET và P-tuning theo từng mẫu trên SuperGLUE với ALBERT; Bảng dưới: Hiệu suất gợi ý thủ công (MP) và P-tuning trên LAMA-P17 với BERT-base-cased. Đối với mỗi cột, P-tuning và các phương pháp so sánh chia sẻ cùng các gợi ý thủ công, trong khi P-tuning kết hợp thêm các token gợi ý liên tục. Chúng tôi báo cáo độ lệch chuẩn trên nhiều kết quả của các mẫu khác nhau. Kết quả cho thấy P-tuning đạt độ lệch chuẩn nhỏ hơn, chứng minh rằng P-tuning có thể cải thiện tính ổn định w.r.t. việc lựa chọn mẫu rời rạc.

dễ dàng được kết hợp với các gợi ý rời rạc hiện có, đồng thời cải thiện thêm tính ổn định như được thảo luận trong Phần 3.4.

3.4 Ổn định hoá việc thích ứng mô hình ngôn ngữ

Trong các phần trên, chúng tôi đã cho thấy rằng P-Tuning cải thiện hiệu suất trên nhiều thiết lập. Bây giờ chúng tôi trình bày kết quả để chứng minh rằng P-Tuning cũng ổn định hoá việc thích ứng mô hình ngôn ngữ; tức là giảm sự khác biệt giữa các gợi ý khác nhau. Như chúng tôi đã hiển thị trong Bảng 1, các gợi ý thủ công có tác động lớn đến hiệu suất. Khi đến với học few-shot, khoảng cách hiệu suất của các gợi ý khác nhau nổi bật do tính nhạy cảm của học few-shot (Zheng et al., 2021). Kết quả trong Bảng 6 cho thấy rằng P-tuning cải thiện hiệu suất của các mẫu có hiệu suất kém nhất (ví dụ, P#5), và đạt độ lệch chuẩn nhỏ hơn trên nhiều mẫu. So với PET-FT, P-tuning tăng tính ổn định w.r.t. việc lựa chọn mẫu.

Trên LAMA, chúng tôi quan sát hiện tượng tương tự rằng trong khi các gợi ý thủ công thường mang lại kết quả khá biến động, việc thêm các gợi ý liên tục có thể huấn luyện được lên trên các gợi ý thủ công có thể ổn định hoá hiệu suất của chúng, giảm độ lệch chuẩn từ 10.1 xuống 0.46.

4 Nghiên cứu liên quan

Khuyến khích mô hình ngôn ngữ. GPT-3 (Brown et al., 2020) sử dụng các ví dụ trong ngữ cảnh (Liu et al., 2021; Zhao et al., 2021b) như một cách khuyến khích để chuyển giao kiến thức từ tiền huấn luyện sang các nhiệm vụ hạ lưu. Schick và Schütze (2020) đề xuất sử dụng các mẫu cloze, loại bỏ ràng buộc rằng token bị che phải là token cuối cùng của câu. Điều này giảm thiểu thêm khoảng cách giữa tiền huấn luyện và các nhiệm vụ hạ lưu. Để cải thiện việc khuyến khích cho NLU, các nghiên cứu gần đây đã đề xuất các phương pháp tự động tìm kiếm các gợi ý hiệu suất cao bằng cách khai thác kho dữ liệu huấn luyện (Jiang et al., 2020b), tìm kiếm dựa trên gradient (Shin et al., 2020), hoặc sử dụng các mô hình sinh được tiền huấn luyện (Gao et al., 2020). Cách tiếp cận của chúng tôi khác với các nghiên cứu trước đây này ở chỗ chúng tôi sử dụng các nhúng gợi ý liên tục, được tìm thấy là bổ sung cho các gợi ý rời rạc trong các thí nghiệm của chúng tôi.

Gần đây, một số nghiên cứu đồng thời cũng đề xuất sử dụng các gợi ý liên tục. Prefix-tuning (Li và Liang, 2021) thêm các gợi ý liên tục vào đầu chuỗi cho mỗi lớp. Trái ngược với nghiên cứu của chúng tôi, prefix-tuning nhắm vào các nhiệm vụ sinh ngôn ngữ tự nhiên.

Trong lĩnh vực NLU, một số phương pháp đồng thời được đề xuất dựa trên các gợi ý liên tục, tập trung

--- TRANG 8 ---
ID Mẫu gợi ý của P-tuning Seg. Pos. #[P] Acc. F1. Avg.
1 [Premise] Question: [Hypothesis] [P] ? Answer: [M]. Yes Mid 1 87.95 76.70 82.33
2 [Premise] Question [P]: [Hypothesis] ? Answer: [M]. Yes Mid 1 88.39 78.57 83.48
3 [Premise] Question: [Hypothesis] ? [P] Answer: [M]. No Mid 1 89.29 79.86 84.58
4 [Premise] [P] Question: [Hypothesis] ? Answer: [M]. No Mid 1 89.73 82.15 85.94
5 [Premise] Question: [Hypothesis] ? Answer: [M]. [P] No Edge 1 87.50 83.39 85.45
6 [Premise] Question: [Hypothesis] ? [P][P] Answer: [M]. No Mid 2 88.39 84.74 86.57
7 [Premise] Question: [Hypothesis] ? [P][P][P][P] Answer: [M]. No Mid 4 88.39 85.14 86.76
8 [Premise] Question: [Hypothesis] ? [P][P][P][P][P][P][P][P] Answer: [M]. No Mid 8 83.48 73.32 78.40

Bảng 7: Hiệu suất few-shot của P-tuning trên nhiệm vụ CB trên ALBERT với các mẫu gợi ý khác nhau. "Seg." có nghĩa là các token gợi ý được chèn có phân đoạn các câu hoàn chỉnh hay không. "Pos." chỉ ra việc chèn các token gợi ý ở rìa hay giữa các đầu vào. "[P]" là token gợi ý liên tục. "[M]" là token mặt nạ.

Nhiệm vụ LSTM MLP EMB
WiC-ACC 56.27±1.54 55.25±3.09 53.96±3.23
CB-ACC. 81.70±7.49 88.39±3.72 82.59±3.69
CB-F1. 77.41±9.15 84.24±5.15 67.27±6.78
BoolQ-ACC. 75.41±3.09 76.46±2.84 76.87±1.69

Bảng 8: Hiệu suất few-shot trên các nhiệm vụ WiC, CB và BoolQ với ALBERT sử dụng các bộ mã hoá gợi ý khác nhau. Kết quả cho thấy rằng cả LSTM và MLP đều hoạt động tốt trên các nhiệm vụ này, trong khi EMB không ổn định và có thể hoạt động kém hơn đáng kể so với hai loại kia trên một số nhiệm vụ (ví dụ, WiC và CB). "EMB" có nghĩa là sử dụng ánh xạ đồng nhất cho bộ mã hoá gợi ý.

Nhiệm vụ LM-BFF (Auto) P-Tuning
SST-2 92.89 92.78
MNLI 57.53 58.70
MRPC 68.26 69.49

Bảng 9: Hiệu suất few-shot của các gợi ý được tìm kiếm tự động và P-Tuning. Chúng tôi đánh giá LM-BFF (Auto) sử dụng top-3 mẫu được tìm kiếm được báo cáo dưới quy trình đánh giá của chúng tôi. P-Tuning cũng sử dụng cùng các gợi ý rời rạc, kết hợp với các gợi ý liên tục. Kết quả cho thấy rằng P-Tuning có thể được kết hợp hiệu quả với các mẫu rời rạc hiện có và đạt cải thiện hiệu suất hơn nữa.

vào cải thiện thăm dò kiến thức (Qin và Eisner, 2021; Zhong et al., 2021). Lester et al. (2021) cho thấy rằng với các mô hình được tiền huấn luyện lớn, chỉ tinh chỉnh các gợi ý liên tục với một mô hình ngôn ngữ đông lạnh đạt hiệu suất tương đương với việc tinh chỉnh toàn bộ mô hình.

So với các nghiên cứu đồng thời này về NLU, P-Tuning đạt được một kết luận độc đáo rằng các gợi ý liên tục cải thiện hiệu suất và ổn định hoá việc huấn luyện với các mô hình đông lạnh hoặc được tinh chỉnh dưới cả thiết lập few-shot và có giám sát đầy đủ. Ví dụ, không có nghiên cứu đồng thời nào cho thấy rằng các gợi ý liên tục có thể cải thiện hiệu suất với một mô hình ngôn ngữ được tinh chỉnh. Về mặt kỹ thuật, P-Tuning cũng có một số thiết kế độc đáo như sử dụng các gợi ý liên tục-rời rạc kết hợp và sử dụng một bộ mã hoá gợi ý.

Kiến thức trong các mô hình ngôn ngữ. Các mô hình ngôn ngữ được tiền huấn luyện tự giám sát (Liu et al., 2020) (Han et al., 2021) bao gồm GPT (Radford et al., 2019), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) đã được quan sát thấy học không chỉ các biểu diễn văn bản được bối cảnh hoá mà còn cả kiến thức ngôn ngữ và thế giới. (Hewitt và Manning, 2019) chứng minh rằng các biểu diễn được bối cảnh hoá được tạo ra bởi các mô hình ngôn ngữ có thể tạo thành một cây phân tích cú pháp trong không gian nhúng. (Vig, 2019; Clark et al., 2019b) xem xét các mẫu chú ý đa đầu trong transformers và khám phá rằng một số đầu chú ý nhất định có thể tương ứng với một số chức năng ngữ pháp, bao gồm đồng tham chiếu và các từ bổ nghĩa cho danh từ. LAMA (Petroni et al., 2019, 2020) đề xuất nhiệm vụ LAMA tận dụng các bài kiểm tra cloze để dự đoán các triplet sự kiện của cơ sở kiến thức để kiểm tra khả năng ghi nhớ sự kiện của mô hình ngôn ngữ với các câu trả lời ở định dạng token đơn. Trong (Wang et al., 2020), các tác giả điều tra các ma trận chú ý để tìm bằng chứng về các triplet kiến thức có trong ngữ cảnh. (Jiang et al., 2020a) phát triển một tập dữ liệu truy xuất sự kiện đa token dựa trên LAMA.

5 Kết luận

Trong bài báo này, chúng tôi trình bày một phương pháp P-Tuning sử dụng các gợi ý liên tục kết hợp với các gợi ý rời rạc. P-Tuning cải thiện hiệu suất và ổn định hoá việc huấn luyện cho việc thích ứng mô hình ngôn ngữ được tiền huấn luyện. P-Tuning hiệu quả với cả các mô hình ngôn ngữ được tinh chỉnh và đông lạnh dưới cả thiết lập few-shot và có giám sát đầy đủ.

--- TRANG 9 ---
Tài liệu tham khảo

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019a. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019b. What does bert look at? an analysis of bert's attention. arXiv preprint arXiv:1906.04341.

Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177–190. Springer.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.

Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107–124.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.

Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, et al. 2021. Pre-trained models: Past, present and future. AI Open.

John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL). Association for Computational Linguistics.

Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham Neubig. 2020a. X-factr: Multilingual factual knowledge retrieval from pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5943–5959.

Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252–262.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. ArXiv, abs/2104.08691.

Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer.

Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.

Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. 2020. Self-supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218, 1(2).

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. CoRR, abs/2104.08786.

Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. How context affects language models' factual predictions. arXiv preprint arXiv:2005.04611.

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.

Mohammad Taher Pilehvar and José Camacho-Collados. 2018. Wic: 10,000 example pairs for evaluating context-sensitive representations. CoRR, abs/1808.09121.

--- TRANG 10 ---
Guanghui Qin and J. Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. ArXiv, abs/2104.06599.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.

Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pages 90–95.

Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. Computing Research Repository, arXiv:2009.07118.

Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.

Jesse Vig. 2019. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In NeurIPS 2019, pages 3261–3275.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. ArXiv, abs/1804.07461.

Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.

Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.

Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. 2020. Revisiting few-sample BERT fine-tuning. CoRR, abs/2006.05987.

Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021a. Calibrate before use: Improving few-shot performance of language models. CoRR, abs/2102.09690.

Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021b. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.

Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, and Zhilin Yang. 2021. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding.

Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]: Learning vs. learning to recall. ArXiv, abs/2104.05240.
