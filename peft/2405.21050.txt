# 2405.21050.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2405.21050.pdf
# File size: 20895484 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SODA : Spectrum-Aware Parameter-Efficient
Fine-Tuning for Diffusion Models
Xinxi Zhang1,*Song Wen1,*Ligong Han1,*,‚Ä†Felix Juefei-Xu2Akash Srivastava3
Junzhou Huang4Hao Wang1Molei Tao5Dimitris Metaxas1
1Rutgers University2New York University3MIT-IBM Watson AI Lab4UT Arlington5Georgia Tech
*Equal contribution‚Ä†Project lead, Corresponding author
Input ImagesSubject PersonlizationStyle Mixing
LegoTransparentOrigami
SleepingStickerMinecraft
Space SuitWooden Sculpture
Origami
DogCatTeapotDriving a CarCherry BlossomWearing a Hat
Input ImagesSubject PersonlizationStyle Mixing
LegoTransparentOrigami
SleepingStickerMinecraft
Space SuitWooden Sculpture
Origami
DogCatTeapotDriving a CarCherry BlossomWearing a Hat
Figure 1: SODA achieves superior image quality and text alignment across diverse input images
and prompts, such as changing the background, altering the texture, and synthesizing new poses.
Additionally, SODA can generate prompt-aligned images in a given style specified by an input style
image.
Abstract
Adapting large-scale pre-trained generative models in a parameter-efficient manner
is gaining traction. Traditional methods like low rank adaptation achieve parameter
efficiency by imposing constraints but may not be optimal for tasks requiring
high representation capacity. We propose a novel spectrum-aware adaptation
framework for generative models. Our method adjusts both singular values and
their basis vectors of pretrained weights. Using the Kronecker product and efficient
Stiefel optimizers, we achieve parameter-efficient adaptation of orthogonal matrices.
Specifically, we introduce Spectral Orthogonal Decomposition Adaptation (SODA),
which balances computational efficiency and representation capacity. Extensive
evaluations on text-to-image diffusion models demonstrate SODA‚Äôs effectiveness,
offering a spectrum-aware alternative to existing fine-tuning methods.
Preprint. Under review.arXiv:2405.21050v1  [cs.CV]  31 May 2024

--- PAGE 2 ---
1 Introduction
Adapting large-scale pre-trained vision generative foundation models, such as Stable Diffusion [ 35,
10,5], in a parameter-efficient manner, is increasingly gaining traction within the research community.
These generative models, which have demonstrated remarkable capabilities in generating high-quality
images, can be computationally intensive and require substantial memory resources. To make these
models more accessible and adaptable to various applications [ 19,47,4,44], researchers are focusing
on methods that fine-tune these models efficiently without necessitating retraining the entire network.
Parameter-efficient adaptation [ 28,8,42] not only reduces computational overhead but also enables
quicker and more flexible model deployment across different tasks and datasets.
The potential for parameter-efficient fine-tuning has been highlighted through extensive validations,
demonstrating the ability to adapt base models to various data, enabling enhancements and customiza-
tions tailored to specific tasks and user characteristics. These methods allow the underlying model
architecture to remain largely unchanged while inserting or adjusting a small subset of parameters.
This approach is advantageous because it preserves the pre-trained knowledge while introducing
task-specific adjustments. The lightweight nature of the optimized parameters also facilitates their
seamless integration, making it possible to achieve high performance without the computational costs
associated with full model retraining.
The efficiency of these methods is generally achieved by introducing structures or constraints
into the parameter space. For instance, Low Rank Adaptation (LoRA) [ 17] constrains parameter
changes to be low-rank, while Kronecker Adapter (KronA) [ 9] constrains weight matrix changes
to be a Kronecker product. By imposing these constraints, the adaptation process becomes more
manageable and computationally efficient. LoRA, for example, operates under the assumption that the
necessary adjustments to the model‚Äôs parameters are intrinsically low-dimensional, which simplifies
the optimization process. However, this low-rank constraint might not always be optimal, especially
for tasks requiring higher representation capacity, as it might limit the model‚Äôs ability to capture
complex patterns in the data.
Despite the simplicity and effectiveness of LoRA, its low-rank constraint may not be optimal for
tasks requiring high representation capacity. Specifically, for a rank rapproximation of a matrix W,
the optimal solution corresponds to the largest rsingular values and their associated singular vectors,
which LoRA does not explicitly utilize. This limitation suggests that there are potentially valuable
directions in the parameter space, represented by the singular vectors, that are not being exploited.
Recognizing this gap, we propose to leverage the full spectral information of the pretrained weight
matrices during the fine-tuning process, thereby enhancing the model‚Äôs adaptability and performance.
In this paper, we propose a novel approach for spectrum-aware adaptation of generative models.
Our method leverages the spectral space of pretrained weights, adjusting both singular values and
singular vectors during fine-tuning. By focusing on both the magnitude and direction of these spectral
components, we can achieve a more nuanced and effective adaptation. To ensure parameter efficiency,
we employ a Kronecker product to rotate the singular vectors, thus modifying both their magnitude
and direction. This approach allows us to maintain a balance between computational efficiency and
the ability to capture complex data representations, making our method particularly suitable for
high-dimensional tasks. Our contributions are as follows:
‚Ä¢We propose a fine-tuning framework for personalization of text-to-image diffusion models
by utilizing the spectrum of the pretrained parameters.
‚Ä¢We introduce SODA, Spectral Orthogonal Decomposition Adaptation, a parameter-efficient
formulation of spectrum aware fine-tuning framework for generative models that leverages
Kronecker product and jointly adjusts the magnitude and orientation of the parameter‚Äôs
singular vectors during fine-tuning.
‚Ä¢We conduct extensive evaluations for our method on customization of text-to-image diffusion
models, and demonstrate it serves as an attractive alternative to traditional parameter-efficient
fine-tuning methods that are not spectrum-aware.
2 Related Work
Diffusion personalization. Diffusion [ 39,16,41,38,33,35] personalization [ 12,36] aims to learn
or reproduce concepts or subjects using pre-trained diffusion models, given one or a few images.
2

--- PAGE 3 ---
ùëä
ùëä
!ùëÖ=√ó
ùëä
ùëä
!
ùêµ
ùê¥=+√ó
ùëÖ
!
ùëÖ
"
ùëÖ
#‚Ä¶(a) Low-Rank Adaptation (LoRA)
‚ùÑ
‚ùÑ
üî•
üî•
üî•
ùëä
ùëà
!=√ó√ó
‚ùÑ
ùëâ
"
üî•‚Ä¶‚Ä¶‚Ä¶‚Ä¶
üî•
ùõ¥
ùëä
ùêø
!=√ó
0
ùëÑ
üî•‚Ä¶‚Ä¶‚Ä¶‚Ä¶
üî•(c) SODA-SVD (Ours)(b) Orthogonal Fine-Tuning (OFT)(d) SODA-QR (Ours): frozen: tunable: zeroFigure 2: Comparison of difference PEFT approaches. ( ‚ñ†: frozen parameters; ‚ñ†: tunable parameters;
‚ñ†: zeros.)
Some works [ 36,23,24] fine-tune the pre-trained diffusion models on images containing the desired
concepts or subjects. DreamBooth [ 36] proposes fine-tuning the entire set of weights to represent the
subjects or concepts as unique identifiers, which can be used for synthesizing images of different
scenarios or styles. CustomDiffusion [ 23] suggests that fine-tuning only the cross-attention layers is
sufficient to learn new concepts, leading to better performance on multiple-concept compositional
generation. Lee et al. propose DCO [ 24], which fine-tunes the diffusion models without losing
the composition ability of the pre-trained models by using implicit reward models. Another line
of work [ 12,11] focuses on optimizing the word embeddings. Specifically, Textual Inversion [ 12]
proposes optimizing word embeddings to capture unique concepts while freezing the pre-trained
diffusion model weights. Additionally, some works [ 13,43,2] combine the optimization of word
embeddings and diffusion model weights. However, model fine-tuning methods typically involve a
large number of parameters, which can be inefficient and prone to overfitting.
Parameter efficient fine-tuning. The rapid development of foundation models, which contain a
large number of parameters, has made fine-tuning these models on small datasets challenging due
to the numerous parameters involved. To address the efficiency and overfitting issues in model
fine-tuning, Parameter Efficient Fine-Tuning (PEFT) techniques have been proposed. One line of
PEFT research focuses on Adapter tuning [ 34,18,6], which involves inserting trainable layers
within the layers of pre-trained models. Another line of research explores Low-Rank Adaptation
(LoRA) [ 17,48,7,22,49,15]. LoRA [ 17,37] proposes learning residual weights by constructing
them through the multiplication of two low-rank matrices, thereby significantly reducing the number
of learned parameters. Other methods have also been developed, such as SVDiff [ 14], which performs
singular value decomposition on the pre-trained weight matrices and only fine-tunes the singular
values, and OFT [ 32,25], which maintains the hyperspherical energy of the pre-trained model by
multiplying a trainable orthogonal matrix. Additionally, KronA [ 9,27] constructs the residual weight
matrices using the Kronecker product of two small-size matrices. However, the aforementioned
methods do not fully leverage the prior knowledge embedded in the pre-trained weights. In this work,
we enhance existing PEFT methods by introducing Spectral Orthogonal Decomposition Adaptation.
While a concurrent study [ 46] also employs a spectrum-aware approach, it differs from ours as it
solely focuses on fine-tuning the top spectral space.
3 Methodology
3.1 Preliminary
Low-Rank Adaptation (LoRA). Text-to-image diffusion models consist of numerous large pre-
trained weights. We follow LoRA‚Äôs Stable Diffusion implementation [ 37] and only fine-tune the
linear projection matrices in cross-attention layers, one of which is denoted as W0‚ààRm√ón. The
weight change during the fine-tuning process is denoted as ‚àÜW. Low-Rank Adaptation (LoRA)
assumes the low rank of the network‚Äôs weight increments and decomposes each increment matrix into
the product of two low-rank matrices ‚àÜW=BA, where A‚ààRr√ónandB‚ààRm√ór. Therefore,
we can derive the following forward propagation:
h=W0x+ ‚àÜWx =W0x+BAx , (1)
3

--- PAGE 4 ---
where handxrespectively represent the output and input of W.
Orthogonal Fine-Tuning (OFT). Finetuning diffusion models usually requires efficiency and prior
knowledge preservation. To enhance prior knowledge, OFT [ 32] proposes to retain hyperspherical
energy in pairwise relational structure among neurons. In detail, it learns an orthogonal matrix to
conduct the same transformation for all neurons in each layer, which keeps the angle among all
neurons in each layer unchanged. The weight update is represented by
W=W0R, (2)
where Ris an orthogonal matrix and W0is the pre-trained weight. To decrease the number of
trainable parameters, the original OFT uses a block-diagonal structure to make it parameter efficient,
where R:=diag(R1,R2, . . . ,Rr)and each Riis a small-size orthogonal matrix.
3.2 Optimization on Stiefel Manifold
A Stiefel manifold St(n, m) :={V‚ààRn√óm:V‚ä§V=Im√óm}is the set of n√ómmatrices
(n‚â§m) with each column orthogonal to all other columns. Consider the optimization problem
minV‚ààSt(n,m )f(V), which is to find a matrix Vthat minimizes a given objective function f(V)
subject to the constraint that Vlies on a Stiefel manifold. This optimization problem has plenty of
applications, such as the OFT above. To make sure that the parameter being optimized stays on the
Stiefel manifold, the original OFT use Cayley parameterization, R= (I+S)(I‚àíS)‚àí1where Sis
a skew-symmetric matrix. Then Ris an orthogonal matrix and OFT only needs to optimize S. Here
we utilize the Stiefel optimizer introduced in [ 21], which preserves the manifold structure and keeps
momentum in the cotangent space. Given an orthogonal matrix V, we can use a Stiefel optimizer to
keep it in the Stiefel manifold. However, directly optimizing Vis not parameter efficient. Here we
utilize Kronecker product, we leverage the following remark:
Remark. IfV1,V2, . . . ,Vrare orthogonal matrices, then their Kronecker productNr
i=1Vi=
V1‚äóV2‚äó ¬∑¬∑¬∑ ‚äó Vris also orthogonal.
Then, the Kronecker product of several small-size orthogonal matrices can generate a relatively
large-size orthogonal matrix, so the number of parameters is reduced. Such formulation is more
efficient than OFT, we call it Kronecker Orthogonal Fine-Tuning (KOFT). However, it is not always
possible to find a Kronecker decomposition for any given orthogonal matrix. Thus, inspired by OFT,
we learn a rotation matrix Rto adjust V0,VR:=V0R. Now, since Ris initialized as identity I,
we can always parameterize it using Kronecker product, VR=V0(R1‚äóR2). The shared block
diagonal structure as adopted by the original OFT corresponds to Ir√ór‚äóR. Comparing with ours,
this is more sparse. So why do we want to consider optimizing in Stiefel manifold? On the one
hand, orthogonal matrices naturally arise in numerical decomposition. On the other hand, previous
works [ 1,3] have shown that imposing orthogonality on model parameters facilitates the learning by
limiting the exploding/vanishing gradients and improves the robustness.
3.3 Spectrum Aware Fine-Tuning
LoRA and OFT have shown promising performance in adapting pre-trained models to downstream
tasks. However, LoRA neglects the prior knowledge in the pre-trained weights, while OFT only
utilizes the angle information among neurons, failing to explore the knowledge in the pre-trained
weights fully. To better utilize the spectrum of pre-trained weights, we propose Spectral Orthogonal
Decomposition Adaptation (SODA). We first decompose the pre-trained weight matrix W0in
each layer into a spectral component Wspec
0 and a basis matrix Wbasis
0 , formulated as W0=
Wspec
0Wbasis
0 . We then update the spectrum in the spectral component Wspec
0 and the basis matrix
Wbasis
0 separately. The spectrum in the spectral component Wspec
0 is optimized using a gradient
descent optimizer. As the basis matrix is always orthogonal, we could use a Stiefel optimizer to
optimize it on the Stiefel manifold. However, directly optimizing Wbasis
0 would result in a number
of trainable parameters as large as full-weight tuning. Considering that both the matrix product and
Kronecker product of orthogonal matrices are also orthogonal, we construct an orthogonal matrix
R=Nr
i=1Ri, where Riis a small-size orthogonal matrix. Then our updated weight can be
formulated as:
W= (Wspec
0‚äï‚àÜS)Wbasis
0R, (3)
4

--- PAGE 5 ---
where the parameters with underlines are trainable. ‚àÜSdenotes the incremental spectrum, where
the operator ‚äïrepresents the addition of the incremental spectrum to the spectrum in the spectral
component matrix Wspec
0. The incremental spectrum ‚àÜSis updated using a gradient descent
optimizer, while each orthogonal matrix Riis updated using a Stiefel optimizer, which ensures that
the orthogonality constraint is maintained during the optimization process. Here we consider two
decomposition methods, SVD and LQ/QR decomposition:
Singular Value Decomposition (SVD). If we decompose W0=U0Œ£0V‚ä§
0where Œ£0=diag(œÉ)
is singular values and V‚ä§
0is an orthogonal matrix, SVDiff [ 14] proposes to fine-tune the singular
values, or tuning the spectral shifts Œ¥,
W=U0Œ£Œ¥V‚ä§
0withŒ£Œ¥:=diag(ReLU (œÉ+Œ¥)). (4)
We propose to fine-tune the singular vectors Vas well. However, directly tuning Vwould not be
parameter-efficient. We thus leverage Kronecker product VR:=V0R=V0(Nr
i=1Ri),
WSODA-SVD =U0Œ£Œ¥V‚ä§
RwithŒ£Œ¥=diag(ReLU (œÉ+Œ¥)),VR:=V0(rO
i=1Ri). (5)
LQ/QR Decomposition (QR). We can alternatively decompose W0=L0Q0, where L0is a lower
triangle matrix and Q0is an orthonormal matrix. Similar to SVD, the diagonal of L0are eigenvalues
ofL0and we propose to fine-tune both L0andQ0,
WSODA-QR =LŒ¥QRwithLŒ¥:=L0+diag(Œ¥),QR:=Q0(rO
i=1Ri). (6)
3.4 Analysis
Number of parameters. A comparison of number of tunable parameters for different approaches
is shown in Table 1. To simplify notation, here we assume m=nand small rotation blocks are
evenly divided ( n/rfor OFT, and n1/rfor SODA). For KOFT and SODA, we can observe that the
parameter count decreases drastically as rgrows (we use r= 3in our experiments).
Gradient of singular values. Given a matrix W‚ààRm√ónand its singular value decomposition
W=UŒ£V‚ä§. If we denote the derivative of loss lw.r.t. output hasŒ¥h=‚àÇl
‚àÇh, then the gradient of
Wequals to Œ¥W=Œ¥hx‚ä§, and
Œ¥Œ£=U‚ä§(Œ¥hx‚ä§)V= (U‚ä§Œ¥h)(V‚ä§x)‚ä§,andŒ¥Œ£ii=‚ü®ui, Œ¥h‚ü©‚ü®vi,x‚ü©, (7)
where ‚ü®¬∑,¬∑‚ü©denotes inner product, and uiandviare the i-th column of UandV, respectively. As
can be seen, the gradient of singular values is composed of two parts: a)‚Äúfrom left to right‚Äù, the
gradient Œ¥his projected onto columns of U;b)‚Äúfrom right to left‚Äù, the input xis projected onto
columns of Vand only the i-th component influences œÉi.
If we consider the change of weight matrix after a small step as ‚àÜW, then if we are only tuning
singular values œÉ, we can compute the effective change of the weight matrix ‚àÜW‚Ä≤,
‚àÜW‚Ä≤=U‚àÜŒ£V‚ä§and (8)
‚àÜŒ£= (U‚ä§‚àÜWV )‚äôIm√ón. (9)
We can observe that
||‚àÜW‚Ä≤||2
F=||‚àÜŒ£||2
F=||(U‚ä§‚àÜWV )‚äôIm√ón||2
F‚â§ ||U‚ä§‚àÜWV||2
F=||‚àÜW||2
F. (10)
The‚â§sign comes from the fact that the Hadamard product ‚äômasks out non-diagonal elements thus
shrinks the Frobenius norm. In fact, since we will mask out most of the elements, ||‚àÜW‚Ä≤||2
Ftends to
be much smaller than ||‚àÜW||2
Fand this is why we set a large learning rate to œÉ.
Table 1: Comparison of Parameter Counts for Different Methods.
Method LoRA OFT KOFT SODA
Number of parameters 2n¬∑rn2
rorn2
r2(shared) r¬∑n2/rn+r¬∑n2/r
5

--- PAGE 6 ---
fancy_boot
(a)‚ÄúA transparent [] fancy_boot‚ÄùVbackpack_dog
(b)‚ÄúA [] backpack_dog featuring a design inspired by the Arsenal Football Club‚ÄùVberry_bowl
(c)‚ÄúA [] berry_bowl made of lego‚ÄùVdog
(d)‚ÄúA [] dog in Minecraft style‚ÄùVteapot
(e)‚ÄúA cube shape [] teapot‚ÄùVInput ImagesSODA-SVD (Ours)SODA-QR (Ours)OFTLoRA
cat
(f)‚ÄúA [] cat in a wizard costume casting spells‚ÄùVmonster_toy
(g)‚ÄúA [] monster_toy playing a violin in sticker style‚ÄùVFigure 3: Results for Subject Personalization . Each subfigure consists of 3 samples: a large one
on the left and two smaller ones on the right. The text under the input images indicates the class of
the personalized subject, while the text prompt under the sample images is used for inference. Our
observations indicate that SODA outperforms both LoRA and OFT in generating prompt-aligned
images while preserving subject identities at a similar level.
4 Experiments
We use Stable Diffusion XL (SDXL) [ 31] as the pre-trained T2I diffusion model. We conduct
experiments on subject personalization (Sec. 4.1), style personalization (Sec. 4.1), and ablation
studies (Sec. 4.3). In all experiments, we train the text encoders and UNet of the SDXL model by
replacing all linear modules in the attention and cross-attention layers with corresponding PEFT.
4.1 Subject Personalization
6

--- PAGE 7 ---
(a)‚ÄúA {butterfly, house, piano} in melting golden 3D rendering style‚ÄùSODA-SVD (Ours) 
SODA-QR (Ours) 
SVD 
LoRA 
Input Image
(b)‚ÄúA {F1 race car, gril, robot} in melting golden 3D rendering style‚Äù
Figure 4: Personalized style generation. We show curated samples of ours (SODA-SVD, SODA-
QR), SVDiff [ 14] (SVD), and LoRA [ 17]. Independently trained subject and style weights are merged
without joint training. SVDiff tends to overfit to the subject and fail to preserve the style well.
Input StyleInput Subject
Playing Guiter
Wearing a Hat
Skateboarding
SODA-SVD (Ours)SODA-QR (Ours)
Reading a Book
Figure 5: Compositional generation of my subject in my style. We show visual samples of
my subject in my style with different actions or visual attributes specified by the text prompts.
Independently trained subject and style weights are merged without joint training.
0.40 0.42 0.44 0.46 0.48 0.50
Image Similarity0.550.600.650.700.75Image-T ext Similarity
SODA-SVD (Ours)
SODA-QR (Ours)
OFT
LoRA
Figure 6: Pareto curve between subject fidelity
(image similarity) and compositionality (image-
text similarity) on subject personalization task of
T2I diffusion models. Scores of each point on the
curve are measured with different learning rates .
Top-right corner is preferred.Experimental setting. For Subject Personal-
ization, we fine-tuned the SDXL model on the
DreamBooth dataset [ 36] following the Direct
Consistency Optimization (DCO) framework
[24]. For a fair comparison, we tuned the best
learning rate for each method and tested each
method with three different learning rates. This
allows us to evaluate their performance compre-
hensively. Detailed experiment setting can be
found in Appendix.
Baselines. We compared our methods with
strong baselines including LoRA [ 37] and
OFT [ 32]. For a fair comparison and to keep the
number of parameters approximately the same,
we set rank r= 1 for LoRA and r= 3 for
KOFT and SODA, i.e., R=R1‚äóR2‚äóR3.
Quantitative results. We report Image-Text
Similarity ( ‚Üë, using SigLIP [ 45]) to measure
the fidelity and Image Similarity ( ‚Üë, using DI-
NOv2 [ 30]) to measure the faithfulness or iden-
tity preservation. Detailed information on the evaluation prompts and metrics can be found in
Appendix. We plot the Pareto curve consists of scores at varying learning rates . This curve illustrates
the trade-off between the fidelity and faithfulness for the evaluated method. The upper right of the
7

--- PAGE 8 ---
curve is ideal, indicating that the method can achieve prompt-aligned compositional generation while
preserving the subject‚Äôs identity.
Fig. 6 shows comparison of LoRA ( ‚òÖ), our methods, SODA-SVD ( ‚óè) and SODA-QR ( ‚ñ≤). Interest-
ingly, LoRA cannot push the frontier to the upper right, indicating that LoRA tends to overfit to the
subject and struggles to generate prompt-aligned images while preserving the subject identity. This
suggests that jointly adjusting the magnitude and orientation of the decomposed pretrained weight
can better utilize model priors when adapting to new concepts without overfitting. Compared to OFT
(‚ñ†), our methods SODA-SVD ( ‚óè) and SODA-QR ( ‚ñ≤) depict the upper-right frontier in both image-
text similarity and image similarity, demonstrating their effectiveness. This suggests incorporating
spectral tuning can further enhance performance. Interestingly, our methods, SODA-SVD ( ‚óè) and
SODA-QR ( ‚ñ≤), overlap with each other, suggesting that tuning Œ£has a very similar effect to tuning
the diagonal of R.
Qualitative results. In Fig. 3, we provide qualitative comparisons between our approaches (SODA-
SVD and SODA-QR) and the baselines (OFT and LoRA). We observe that prompts involving
background changes (f) or style changes (g) are handled well by all methods. These prompts are
likely easier because they don‚Äôt require a deep understanding of the subject; even overfitting can still
produce prompt-aligned images. However, for prompts requiring changes in shape (e) or texture (a-d),
LoRA struggles with compositional generation due to overfitting the object. Our methods and OFT
perform significantly better than LoRA on prompts requiring texture or shape changes. For example,
in (a), (c), and (e), our methods demonstrate superior compositional generation. This suggests the
benefit of leveraging the spectrum of the pretrained weights, not just adjusting the basis orthogonally.
These observations are also supported by the quantitative results discussed above. Interestingly,
with objects like dogs (d), LoRA sometimes succeeds in generating good compositional images that
changes the subject‚Äôs texture. We hypothesize this is because pretrained models have a strong prior
for such objects, making it easier for LoRA to find optimization points that represent the object well
without overfitting.
4.2 Style Personalization
Style ImageSODA-SVD (Ours) 
SODA-QR (Ours) SVDLoRA
SneakerSunglassesDogPlushie
Figure 7: Comparison of my subject in my style.Experimental setting. For style personaliza-
tion we experiment on style images from Style-
Drop dataset [ 40], we finetune all peft meth-
ods on 10 style reference images and gener-
ate comopositional images for the corresponded
style. This result is displayed in Fig. 4, And
we follow [ 40] to mix the personalized subject
and style model to generate images of a per-
sonalized subject in a personalized style. we
randomly picked 10 subjects from Dreambooth
dataset [ 36] and merge their residual fine-tuned
peft weight ‚àÜW1with the residual fine-tuned
style peft weight ‚àÜW2. For merging, we use an
arithmetic merge (Merge) [ 40], i.e., ‚àÜW=
‚àÜW1+ ‚àÜW2, result is displayed in Fig. 7.
Other experiment settings follows Sec. 4.1.
Baselines. We compared our methods with
LoRA with rank r= 1, and SVDiff [ 14]. For
LoRA and SVDiff, we can directly get the
merged residual weight by merging the resid-
ual weight of the subject personalization model
and the style model. For our Methods, we calculate the residual weight by subtracting the fine-tuned
SVD or QR weight by the pretrained weight and merge the residual weight.
Results. Fig. 4 shows results comparisons between LoRA, SVDiff and Ours (SODA-SVD and
SODA-QR). Our methods can generate prompt-aligned images in the reference style, while SVDiff
observed over-fit to the images, and LoRA generates artifacts. Fig. 7 shows results of generated
images by merging the subject model and the style model. Our models can generate style match
images while preserving the identity of the personalized subject. while SVDiff tends to overfit to the
8

--- PAGE 9 ---
subject and fail to preserve the style well, and lora also overfit to the subject and generate artifact.
Additionally, we show novel compositional generation of the combined subject and style in Fig. 5
4.3 Ablation Study
We also conduct ablation studies on spectral awareness and optimization on Stiefel Manifold to
validate our design and choice.
0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58
Image Similarity0.450.500.550.600.650.700.750.800.85Image-T ext Similarity
SODA-SVD
SVD
KOFT
SODA-SVD-Res
LoRA
(a) Effect of spectral and orthogonal
tuning.
0.50 0.52 0.54 0.56 0.58 0.60 0.62
Image Similarity0.40.50.60.7Image-T ext Similarity
SVD
LQ-diag(b) Choice of spectrum tuning strate-
gies.
0.46 0.48 0.50 0.52 0.54 0.56
Image Similarity0.550.600.650.700.750.80Image-T ext Similarity
OFT
KOFT
KOFT-Cayley
OFT-Stiefel(c) Choice of optimization on Siefel
Manifold.
Figure 8: Ablation studies on spectral awareness and optimization on Stiefel Manifold.
Input Image
SVDSVD-noReLUSVD-softplus‚ÄúA [] dog made of lego‚ÄùV
‚ÄúA [] berry_bowl on Mars‚ÄùVdogberry_bowl
Figure 9: Results for spectral tuning on SVD.Spectral Awareness. a) We study the effect of
spectral tuning and orthogonal tuning. We select
10 subjects from the DreamBooth dataset [ 36]
and compare our method (SODA-SVD), which
combines spectral tuning and orthogonal tuning,
and only spectral tuning (SVD), only orthogonal
tuning (Kronecker orthogonal Adapter), and a
residual version of SODA-SVD. Fig. 8a shows
orthogonal only and spectral only have similar
performance to each other while ours performs
better. b)We also study the choice of spec-
tral tuning method. In Fig. 8b, we compare the
performance of spectral tuning using SVD and
LQ/QR decomposition. The results demonstrate that SVD spectral tuning slightly outperforms
LQ/QR. Fig. 9 visualizes the impact of different output constraint choices (no ReLU, softplus, and
ReLU) on SVD spectral tuning. The results show that using ReLU achieves the best performance.
Optimization on the Stiefel manifold. We conduct ablations on different optimization methods
on the Stiefel Manifold. We compare the original OFT, KOFT (OFT with Kronecker product),
KOFT-Cayley (OFT with Kronecker product and Cayley parameterization), and OFT-Stiefel (OFT
with Stiefel optimizer). From Fig. 8c, we observe that the Stiefel optimizer [ 21] outperforms the
other methods when using a small learning rate, while the other methods perform similarly to each
other. This demonstrates that the Stiefel optimizer can achieve comparable performance to Cayley
parameterization, with the added flexibility of allowing a non-square matrix. Furthermore, the Stiefel
optimizer exhibits greater robustness, as it achieves better performance with smaller learning rates
compared to the other methods and achieves comparable performance with larger learning rates.
5 Conclusion and Discussion
In this paper, we first identify the limitations of previous PEFT methods, which are not designed
to fully utilize the prior knowledge in the pre-trained weights. To address this issue, we propose
spectrum-aware parameter-efficient fine-tuning, a novel approach that leverages the spectrum of the
pre-trained parameters. Specifically, we introduce Spectral Orthogonal Decomposition Adaptation
(SODA), which jointly performs spectral and orthogonal tuning. Experiments on diffusion personal-
ization demonstrate that our method outperforms previous PEFT methods. Furthermore, ablation
studies validate the effectiveness of the individual components of our proposed SODA approach.
9

--- PAGE 10 ---
Limitations. Our proposed fine-tuning method runs slower during training compared to LoRA due
to the Stiefel optimizer. Future work will focus on accelerating the optimization algorithms and
applying them to large language models.
Acknowledgments. We would like to thank Haizhou Shi for valuable discussions.
References
[1]E. M. Achour, F. Malgouyres, and F. Mamalet. Existence, stability and scalability of orthogonal
convolutional neural networks. Journal of Machine Learning Research , 23(347):1‚Äì56, 2022.
[2]O. Avrahami, K. Aberman, O. Fried, D. Cohen-Or, and D. Lischinski. Break-a-scene: Extracting
multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers , pages
1‚Äì12, 2023.
[3]N. Bansal, X. Chen, and Z. Wang. Can we gain more from orthogonality regularizations in
training deep networks? Advances in Neural Information Processing Systems , 31, 2018.
[4]K. Black, M. Janner, Y . Du, I. Kostrikov, and S. Levine. Training diffusion models with
reinforcement learning. In ICML 2023 Workshop on Structured Probabilistic Inference {\&}
Generative Modeling , 2023.
[5]A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y . Levi, Z. English,
V . V oleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large
datasets. arXiv preprint arXiv:2311.15127 , 2023.
[6]S. Chen, C. Ge, Z. Tong, J. Wang, Y . Song, J. Wang, and P. Luo. Adaptformer: Adapting
vision transformers for scalable visual recognition. Advances in Neural Information Processing
Systems , 35:16664‚Äì16678, 2022.
[7]T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of
quantized llms. Advances in Neural Information Processing Systems , 36, 2024.
[8]N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen, C.-M. Chan, W. Chen, et al.
Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine
Intelligence , 5(3):220‚Äì235, 2023.
[9]A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona:
Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650 , 2022.
[10] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M√ºller, H. Saini, Y . Levi, D. Lorenz, A. Sauer,
F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv
preprint arXiv:2403.03206 , 2024.
[11] Z. Fei, M. Fan, and J. Huang. Gradient-free textual inversion. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 1364‚Äì1373, 2023.
[12] R. Gal, Y . Alaluf, Y . Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-or. An
image is worth one word: Personalizing text-to-image generation using textual inversion. In
The Eleventh International Conference on Learning Representations , 2022.
[13] R. Gal, M. Arar, Y . Atzmon, A. H. Bermano, G. Chechik, and D. Cohen-Or. Encoder-based
domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics
(TOG) , 42(4):1‚Äì13, 2023.
[14] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter
space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7323‚Äì7334, 2023.
[15] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models. arXiv
preprint arXiv:2402.12354 , 2024.
[16] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840‚Äì6851, 2020.
[17] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank
adaptation of large language models. In International Conference on Learning Representations ,
2021.
10

--- PAGE 11 ---
[18] Z. Hu, L. Wang, Y . Lan, W. Xu, E.-P. Lim, L. Bing, X. Xu, S. Poria, and R. K.-W. Lee. Llm-
adapters: An adapter family for parameter-efficient fine-tuning of large language models. In
The 2023 Conference on Empirical Methods in Natural Language Processing , 2023.
[19] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani. Imagic: Text-
based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6007‚Äì6017, 2023.
[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.
[21] L. Kong, Y . Wang, and M. Tao. Momentum stiefel optimizer, with applications to suitably-
orthogonal attention, and optimal transport. In The Eleventh International Conference on
Learning Representations , 2022.
[22] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation.
arXiv preprint arXiv:2310.11454 , 2023.
[23] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y . Zhu. Multi-concept customization of
text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1931‚Äì1941, 2023.
[24] K. Lee, S. Kwak, K. Sohn, and J. Shin. Direct consistency optimization for compositional
text-to-image personalization, 2024.
[25] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, et al.
Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth International
Conference on Learning Representations , 2023.
[26] I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.
[27] S. Marjit, H. Singh, N. Mathur, S. Paul, C.-M. Yu, and P.-Y . Chen. Diffusekrona: A parameter
efficient fine-tuning method for personalized diffusion model. arXiv preprint arXiv:2402.17412 ,
2024.
[28] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, and Y . Shan. T2i-adapter: Learning adapters
to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 38, pages 4296‚Äì4304, 2024.
[29] OpenAI. Gpt-4 technical report, 2024.
[30] M. Oquab, T. Darcet, T. Moutakanni, H. V o, M. Szafraniec, V . Khalidov, P. Fernandez, D. Haziza,
F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023.
[31] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M√ºller, J. Penna, and R. Rombach.
Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.
[32] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Sch√∂lkopf.
Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information
Processing Systems , 36:79320‚Äì79362, 2023.
[33] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
[34] S.-A. Rebuffi, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters.
Advances in neural information processing systems , 30, 2017.
[35] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis
with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 10684‚Äì10695, 2022.
[36] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven generation, 2023.
[37] S. Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github.
com/cloneofsimo/lora .
[38] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan,
S. S. Mahdavi, R. G. Lopes, et al. Photorealistic text-to-image diffusion models with deep
language understanding. arXiv preprint arXiv:2205.11487 , 2022.
11

--- PAGE 12 ---
[39] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning ,
pages 2256‚Äì2265. PMLR, 2015.
[40] K. Sohn, L. Jiang, J. Barber, K. Lee, N. Ruiz, D. Krishnan, H. Chang, Y . Li, I. Essa, M. Rubin-
stein, et al. Styledrop: Text-to-image synthesis of any style. Advances in Neural Information
Processing Systems , 36, 2024.
[41] Y . Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based
generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 ,
2020.
[42] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning approach for large
language models. In The Twelfth International Conference on Learning Representations , 2023.
[43] Y . Wei, Y . Zhang, Z. Ji, J. Bai, L. Zhang, and W. Zuo. Elite: Encoding visual concepts into
textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 15943‚Äì15953, 2023.
[44] Z. Xue, G. Song, Q. Guo, B. Liu, Z. Zong, Y . Liu, and P. Luo. Raphael: Text-to-image
generation via large mixture of diffusion paths. Advances in Neural Information Processing
Systems , 36, 2024.
[45] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-
training. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
11975‚Äì11986, 2023.
[46] F. Zhang and M. Pilanci. Spectral adapter: Fine-tuning in spectral space. arXiv preprint
arXiv:2405.13952 , 2024.
[47] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) ,
pages 3836‚Äì3847, October 2023.
[48] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and T. Zhao. Adaptive budget
allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on
Learning Representations , 2023.
[49] J. Zhu, K. Greenewald, K. Nadjahi, H. S. d. O. Borde, R. B. Gabrielsson, L. Choshen, M. Ghas-
semi, M. Yurochkin, and J. Solomon. Asymmetry in low-rank adapters of foundation models.
arXiv preprint arXiv:2402.16842 , 2024.
12

--- PAGE 13 ---
Appendix
A Derivations
Remark. IfV1,V2, . . . ,Vrare orthogonal matrices, then their Kronecker productNr
i=1Vi=
V1‚äóV2‚äó ¬∑¬∑¬∑ ‚äó Vris also orthogonal.
Proof. Without loss of generality, let‚Äôs assume Vi‚ààRmi√óniandV‚ä§
iVi=Ini√óni.
We begin by verify V=Vi‚äóVjis orthogonal,
V‚ä§V= (Vi‚äóVj)‚ä§(Vi‚äóVj) (11)
= (V‚ä§
i‚äóV‚ä§
j)(Vi‚äóVj) (12)
= (V‚ä§
iVi)‚äó(V‚ä§
jVj) (13)
=Ini√óni‚äóInj√ónj (14)
=Ininj√óninj (15)
When both ViandVjare square matrices, similarly, we have V V‚ä§=I. Also, the determinant
|V|=|Vi|ni|Vj|nj= 1.
The final result can be obtained by iteratively applying the above.
Derivation of gradient of gradients:
Gradient of singular values. We have h=Wx andW=UŒ£V‚ä§. Then
vec(Œ¥Œ£) =vec(Œ¥W)‚ä§(V‚äóU) (16)
=vec(Œ¥hx‚ä§)‚ä§(V‚äóU) (17)
= (x‚äóŒ¥h)‚ä§(V‚äóU) (18)
= (x‚ä§V)‚äó(Œ¥h‚ä§U) (19)
= ((x‚ä§V)‚ä§‚äó(Œ¥h‚ä§U)‚ä§)‚ä§(20)
=vec((V‚ä§xŒ¥h‚ä§U)‚ä§) (21)
=vec(U‚ä§(Œ¥hx‚ä§)V). (22)
Thus Œ¥Œ£=U‚ä§Œ¥hx‚ä§V. Here vec (¬∑)means staking the rows of a matrix.
Frobenius norm of weight change.
||‚àÜW‚Ä≤||2
F=trace(‚àÜW‚Ä≤‚àÜW‚Ä≤‚ä§) (23)
=trace(U‚àÜŒ£V‚ä§V‚àÜŒ£‚ä§U‚ä§) (24)
=trace(U‚àÜŒ£‚àÜŒ£‚ä§U‚ä§) (25)
=trace(‚àÜŒ£‚àÜŒ£‚ä§U‚ä§U) (26)
=||‚àÜŒ£||2
F (27)
=||(U‚ä§‚àÜWV )‚äôIm√ón||2
F (28)
‚â§ ||U‚ä§‚àÜWV||2
F (29)
=trace(U‚ä§‚àÜWV V‚ä§‚àÜW‚ä§U) (30)
=||‚àÜW||2
F. (31)
B Implementation Details
B.1 Dataset
For Subject Personalization, we fine-tuned the SDXL model on the DreamBooth dataset [ 36] follow-
ing the Direct Consistency Optimization (DCO) framework [ 24]. The SDXL model was fine-tuned
13

--- PAGE 14 ---
Input Images 
Comprehensive Caption Generated by GPT-4 "An outdoor shot of a dog on a sandy path with green trees and a pond in the background." 
"A photo of a duck_toy on a soft blue carpet, with hints of a dark corner of a room in the background." 
"A photo of a monster_toy, positioned on a windowsill with an expansive view of modern office buildings and a city park below.‚Äù 
"A photo of a poop_emoji toy in front of a large tree in a park, highlighting its playful character against the backdrop of a serene natural setting.‚Äù Figure 10: Results for Examples of comprehensive captions generated by GPT-4 . The class tokens
are marked in bold (e.g., dog, duck_toy, monster_toy).
on 30 subjects, with each subject having 3-5 images and corresponding comprehensive captions
generated by GPT-4 [ 29]. The model was trained with all PEFT methods for 1000 steps with a
batch size of 1. For evaluation, we sampled 16 images for 10 prompts per subject. Seven of these
prompts were designed to alter the texture of the original subject, such as ‚Äúa [ V] [berry_bowl] made
of lego.‚Äù These prompts were chosen because we observe that they present a significant challenge for
personalization, requiring the fine-tuned model to learn new concepts without overfitting the subject.
Examples of input images and their corresponding captions generated by GPT-4 is provided in Fig.
10. And we use ‚Äú pll‚Äù as the placeholder [ V] during subject personalization (e.g., An outdoor shot of
a [V] dog on a sandy path with green trees and a pond in the background.).
For style personalization, we fine-tuned the SDXL model on the Styledrop dataset [ 40], with a single
reference image for 10 different styles.
B.2 Hyperparameters
SODA-SVD (Ours) SODA-QR (Ours) OFT LoRA (r=1)0.000.250.500.751.001.251.501.75Parameter Numbers1e6Parameter Numbers of Different Methods in Subject Personalization
Figure 11: Parameters numbers for different methods
14

--- PAGE 15 ---
Budget. We conducted all our experiments on a single GPU (e.g., A100) using a batch size of
1. We fine-tuned all the methods with 1000 optimization steps and evaluated each methods at this
iteration count. To ensure fair comparisons, we hardcoded the architecture of OFT (shared orthogonal
blocks) and our methods (with the orthogonal matrix constructed by the Kronecker Product of 3
small matrices). The parameter numbers for each method are detailed in Tab. 11.
n0
20
40
60
80
100r
2.55.07.510.012.515.017.520.0Number of Parameters
5001000150020002500300035004000LoRA: 2nr
n0
20
40
60
80
100r
2.55.07.510.012.515.017.520.0Number of Parameters
200040006000800010000OFT: n2
r
n0
20
40
60
80
100r
2.55.07.510.012.515.017.520.0Number of Parameters
200040006000800010000OFT (shared): n2
r2
n0
20
40
60
80
100r
2.55.07.510.012.515.017.520.0Number of Parameters
200040006000800010000KOFT: rn2/r
n0
20
40
60
80
100r
2.55.07.510.012.515.017.520.0Number of Parameters
200040006000800010000SODA: n+rn2/r
Figure 12: Number of parameters for different methods.
Other training settings. With all the training without orthogonal requirements, we use the AdamW
[26] optimizer. For fair comparisons between experiments that require orthogonality, we use Adam
Optimizer [ 20] for OFT and Stiefel optimizer [ 21] for our methods. For the DCO loss [ 24], we use
Œ≤= 1.0.
C Additional Visual Results
We show additional visual results in Fig. 13 14 15 16.
15

--- PAGE 16 ---
Input Images
‚ÄúA side view of a [] fancy_boot lying on Mars‚ÄùVSODA-SVD (Ours)SODA-QR (Ours)
‚ÄúA [] fancy_boot in Origami style‚ÄùV
‚ÄúA [] fancy_boot in Minecraft style‚ÄùV‚ÄúA [] fancy_boot made of lego‚ÄùVFigure 13: Results of Our Methods for the subject fancy_boot .
Input Images
‚ÄúA [] monster_toy playing guitar in sticker style‚ÄùVSODA-SVD (Ours)SODA-QR (Ours)
‚ÄúA [] monster_toy in Origami style‚ÄùV
‚ÄúA [] monster_toy in Minecraft style‚ÄùV‚ÄúA [] monster_toy made of lego‚ÄùV
Figure 14: Results of Our Methods for the subject monster_toy .
16

--- PAGE 17 ---
Input Images
‚ÄúA [] cat as an astronaut in a space suit, walking on the surface of Mars‚ÄùVSODA-SVD (Ours)SODA-QR (Ours)
‚ÄúA [] cat carved as a knight  in Wooden sculpture‚ÄùV
‚ÄúA [] cat in Origami style‚ÄùV‚Äúa side view of a [] cat in times square‚ÄùVFigure 15: Results of Our Methods for the subject cat.
Input Images
‚ÄúA [] dog as an astronaut in a space suit, walking on the surface of Mars‚ÄùVSODA-SVD (Ours)SODA-QR (Ours)
‚ÄúA [] dog carved as a knight  in Wooden sculpture‚ÄùV
‚ÄúA [] dog in Origami style‚ÄùV‚Äúa side view of a [] dog in times square‚ÄùV
Figure 16: Results of Our Methods for the subject dog.
17
