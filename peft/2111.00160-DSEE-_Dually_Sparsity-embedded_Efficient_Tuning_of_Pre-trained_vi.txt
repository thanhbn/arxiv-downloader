# 2111.00160.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2111.00160.pdf
# Kích thước file: 1018725 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
DSEE: Điều chỉnh Hiệu quả Nhúng Độ Thưa Thớt Kép của Mô hình Ngôn ngữ Tiền huấn luyện

Xuxi Chen1, Tianlong Chen1, Weizhu Chen2, Ahmed Hassan Awadallah2
Zhangyang Wang1, Yu Cheng2
1Đại học Texas tại Austin, 2Tập đoàn Microsoft
{xxchen,tianlong.chen,atlaswang}@utexas.edu
{wzchen,hassanam,yu.cheng}@microsoft.com

Tóm tắt
Các mô hình tiền huấn luyện khổng lồ đã trở thành trung tâm của xử lý ngôn ngữ tự nhiên (NLP), đóng vai trò như điểm khởi đầu cho việc tinh chỉnh hướng tới một loạt các tác vụ phụ thuộc. Tuy nhiên, hai điểm đau vẫn tồn tại đối với paradigm này: (a) khi các mô hình tiền huấn luyện lớn hơn (ví dụ: 175B tham số cho GPT-3), thậm chí quá trình tinh chỉnh cũng có thể tốn thời gian và tốn kém về mặt tính toán; (b) mô hình được tinh chỉnh có cùng kích thước với điểm khởi đầu theo mặc định, điều này không hợp lý do chức năng chuyên biệt hơn của nó, cũng không thực tế vì nhiều mô hình được tinh chỉnh sẽ được triển khai trong các môi trường bị hạn chế tài nguyên. Để giải quyết các điểm đau này, chúng tôi đề xuất một khung cho việc tinh chỉnh hiệu quả về tài nguyên và tham số bằng cách tận dụng tiên nghiệm độ thưa thớt trong cả cập nhật trọng số và trọng số mô hình cuối cùng. Khung được đề xuất của chúng tôi, được gọi là Điều chỉnh Hiệu quả Nhúng Độ Thưa Thớt Kép (DSEE), nhằm đạt được hai mục tiêu chính: (i) tinh chỉnh hiệu quả tham số - bằng cách thực thi cập nhật thứ hạng thấp có nhận biết độ thưa thớt trên các trọng số tiền huấn luyện; và (ii) suy luận hiệu quả tài nguyên - bằng cách khuyến khích cấu trúc trọng số thưa thớt hướng tới mô hình tinh chỉnh cuối cùng. Chúng tôi tận dụng độ thưa thớt trong hai hướng này bằng cách khai thác cả các mẫu thưa thớt không có cấu trúc và có cấu trúc trong các mô hình ngôn ngữ tiền huấn luyện thông qua một cách tiếp cận thống nhất. Các thí nghiệm rộng rãi và điều tra sâu, với các backbone mạng đa dạng (tức là BERT, RoBERTa, và GPT-2) trên hàng chục tập dữ liệu, liên tục chứng minh hiệu quả tham số/suy luận ấn tượng, trong khi duy trì hiệu suất phụ thuộc cạnh tranh. Ví dụ, DSEE tiết kiệm khoảng 25% FLOP suy luận trong khi đạt được hiệu suất so sánh, với 0.5% tham số có thể huấn luyện trên BERT. Mã nguồn có sẵn tại https://github.com/VITA-Group/DSEE.

1 Giới thiệu
Hầu hết các ứng dụng NLP gần đây đã theo paradigm tiền huấn luyện rồi tinh chỉnh, bắt đầu từ một mô hình tiền huấn luyện khổng lồ và tinh chỉnh nó hướng tới các tác vụ phụ thuộc. Tinh chỉnh thông thường hoạt động thông qua việc cập nhật tất cả các tham số trong mô hình tiền huấn luyện. Tuy nhiên, khi kích thước của các mô hình tiền huấn luyện tăng lên, việc cập nhật tất cả tham số trở nên ít khả thi hơn trong hầu hết các tình huống thực tế, do yêu cầu bộ nhớ và tính toán đắt đỏ. Ví dụ, BERT BASE (Devlin et al., 2019) có 110M tham số có thể huấn luyện, trong khi GPT-2 (Radford et al., 2019) có tới 1.5B và phiên bản lớn nhất của GPT-3 (Radford et al., 2019) có 175B tham số có thể huấn luyện đáng kinh ngạc. Do đó, tinh chỉnh thông thường của các mô hình lớn hơn có thể yêu cầu hàng trăm giờ GPU. Một nhược điểm khác của paradigm này là nó yêu cầu lưu trữ nhiều tham số như trong các mô hình tiền huấn luyện quy mô lớn cho mỗi tác vụ phụ thuộc, điều này tạo ra trở ngại cho việc triển khai trong các môi trường thực tế bị hạn chế tài nguyên.

Một giải pháp để giải quyết yêu cầu tài nguyên rộng rãi của tinh chỉnh thông thường là tỉa cành mô hình (LeCun et al., 1990; Han et al., 2015; Ren et al., 2018; He et al., 2017; Liu et al., 2017), nơi các trọng số không cần thiết bị loại bỏ để thu nhỏ kích thước mô hình. Ví dụ, Chen et al. (2021b) tận dụng regularization ℓ1 để loại bỏ các đầu attention không đáng kể và đạt được 35∼45% thời gian huấn luyện với hiệu suất so sánh; Chen et al. (2021a); Dao et al. (2022) tận dụng các ma trận thưa thớt với cấu trúc cố định để giảm kích thước của các mô hình tiền huấn luyện. Tất cả các nghiên cứu này chỉ ra sự gia tăng của độ thưa thớt một cách tự nhiên trong quá trình tinh chỉnh một mô hình tiền huấn luyện mục đích chung, thành một số chức năng phụ thuộc chuyên biệt. Một diễn giải tiềm năng về lý do tại sao độ thưa thớt xuất hiện là các tập con khác nhau của các tham số có thể chịu trách nhiệm cho các tác vụ phụ thuộc và domain dữ liệu khác nhau (Sanh et al., 2020). Tuy nhiên, việc xác định các mặt nạ thưa thớt phù hợp có thể khó khăn: tinh chỉnh một mô hình ngôn ngữ tiền huấn luyện lớn như GPT-3 chỉ một bước tiêu thụ ít nhất 1.2TB VRAM và yêu cầu 96 thiết bị Tesla NVIDIA (Hu et al., 2021), và các phương pháp này hoặc yêu cầu truy cập vào trọng số tiền huấn luyện hoặc giới thiệu các hệ số học được bổ sung (chẳng hạn như điểm số tầm quan trọng của các đầu attention).

Một lựa chọn thay thế song song là thiết kế các thuật toán tinh chỉnh hiệu quả tham số, nhằm tối ưu hóa một phần nhỏ trọng số trong khi cố định hầu hết chúng khi tinh chỉnh trên các tác vụ phụ thuộc. Các công trình tiên phong theo hướng này, sử dụng adapter (Houlsby et al., 2019), embedding có thể học (Li and Liang, 2021; Liu et al., 2021), phân tích thứ hạng thấp (Hu et al., 2021) hoặc kết hợp của chúng (He et al., 2021), có thể giảm đáng kể số lượng tham số có thể huấn luyện trong khi duy trì hiệu suất tinh chỉnh tốt. Mặc dù các phương pháp này có thể cải thiện đáng kể hiệu quả lưu trữ và triển khai của mô hình, có hai rào cản chính: (i) chúng không mang lại bất kỳ lợi ích hiệu quả suy luận nào vì các trọng số tiền huấn luyện đầy đủ vẫn được yêu cầu để tính toán đầu ra; và (ii) các phương pháp hiện tại giả định rằng các cập nhật trên trọng số tiền huấn luyện là thưa thớt (Guo et al., 2020) hoặc thứ hạng thấp (Hu et al., 2021), nhưng những giả định đó có thể quá đơn giản (Yu et al., 2017) và bị hạn chế quá mức để cho phép các cập nhật hiệu quả. Những quan sát này đã truyền cảm hứng cho chúng tôi khám phá các phương pháp hiệu quả tham số tốt hơn.

Để cải thiện cả hiệu quả tài nguyên và tham số trong quá trình tinh chỉnh mô hình, chúng tôi rõ ràng dựa trên tiên nghiệm của độ thưa thớt cho cả cập nhật trọng số và trọng số cuối cùng, và thiết lập một khung điều chỉnh hiệu quả nhúng độ thưa thớt kép (DSEE). Bắt đầu từ một mô hình tiền huấn luyện, DSEE trước tiên áp dụng cập nhật trọng số thứ hạng thấp có nhận biết độ thưa thớt để đạt được hiệu quả tham số của quá trình tinh chỉnh; và sau đó thực thi cấu trúc trọng số thưa thớt trực tiếp từ các cập nhật trọng số bằng cách che mặt nạ để đạt được hiệu quả tài nguyên của mô hình tinh chỉnh tại thời điểm suy luận. Đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi đề xuất điều chỉnh hiệu quả nhúng độ thưa thớt kép, thống nhất cập nhật trọng số hiệu quả tham số có nhận biết độ thưa thớt và trọng số tiền huấn luyện thưa thớt trong việc tinh chỉnh các mô hình tiền huấn luyện khổng lồ. Đây là nỗ lực đầu tiên hướng tới việc tối ưu hóa đồng thời cả hiệu quả tham số của quá trình tinh chỉnh và hiệu quả tài nguyên của mô hình tinh chỉnh.

• Cả tiên nghiệm thưa thớt không có cấu trúc và có cấu trúc đều được điều tra trong thuật toán DSEE được đề xuất của chúng tôi. Đối với cập nhật trọng số, tiên nghiệm độ thưa thớt được tiêm vào tăng cường các lược đồ cập nhật hiệu quả tham số hiện có (ví dụ: phân tích thứ hạng thấp). Đối với trọng số cuối cùng, chúng tôi vẽ ra các mặt nạ thưa thớt vượt trội, không có cấu trúc hoặc có cấu trúc, trực tiếp từ các cập nhật trọng số, không yêu cầu tham số bổ sung cũng như không cần truy cập vào trọng số tiền huấn luyện và tiết kiệm chi phí thưa thớt hóa.

• Các thí nghiệm rộng rãi chứng minh hiệu quả của đề xuất của chúng tôi trên các mô hình ngôn ngữ tiền huấn luyện đại diện khác nhau (BERT, GPT-2, và RoBERTa) và trên các benchmark đánh giá đa dạng (E2E, DART, WebNLG, và GLUE). Trên GPT-2, các phương pháp của chúng tôi có thể đạt được điểm BLUE của {69.5, 54.9, 47.5} với 0.1% tham số có thể huấn luyện trên {E2E, WebNLG, DART} với 20% tham số đã loại bỏ trong trọng số tiền huấn luyện. Trên BERT, DSEE có thể tinh chỉnh chỉ 0.5% tham số và tiết kiệm khoảng 25% FLOP suy luận, trong khi mất ít hơn 2% hiệu suất.

2 Công trình liên quan
Tỉa cành và Thưa thớt hóa Tỉa cành là một kỹ thuật nén mô hình cổ điển có thể giảm số lượng tham số, có thể mang lại hiệu quả huấn luyện và suy luận. Các nhà nghiên cứu đã đề xuất một số phương pháp tỉa cành cho các mô hình ngôn ngữ tiền huấn luyện: McCarley et al. (2019); Chen et al. (2021b) tỉa cành các đầu attention có đóng góp ít hơn trong quá trình tinh chỉnh; Sanh et al. (2020) đề xuất một tiêu chí tỉa cành nhắm mục tiêu thay đổi trọng số sau huấn luyện, phù hợp với học chuyển giao tốt hơn; Wang et al. (2020) kết hợp phân tích thứ hạng thấp và regularization ℓ0 cho tỉa cành. Gần đây, có một loạt công trình thưa thớt hóa sử dụng các mặt nạ thưa thớt với cấu trúc cụ thể, được gọi là Butterflies, và đạt được hiệu quả cao trong việc tiền huấn luyện mô hình (Chen et al., 2021a) hoặc tinh chỉnh trên các tác vụ phụ thuộc (Dao et al., 2022). Tuy nhiên, các phương pháp này không cho phép cập nhật hiệu quả tham số.

Phân tích thứ hạng thấp Xấp xỉ thứ hạng thấp (Ye, 2005) có ứng dụng rộng rãi trong cộng đồng học máy và được nghiên cứu rộng rãi. Một kịch bản cổ điển là phân tích thành phần chính mạnh mẽ (Candès et al., 2011), phân tích một ma trận thành một thành phần thứ hạng thấp cộng với một thành phần thưa thớt. Tài liệu hiện có cho thấy rằng trong học sâu, các mô hình được học có quá nhiều tham số thường tự nhiên mang các cấu trúc trọng số thứ hạng thấp xấp xỉ (Oymak et al., 2019; Yu et al., 2017).

--- TRANG 2 ---
Một số nghiên cứu (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) đã áp đặt ràng buộc thứ hạng thấp một cách rõ ràng trong quá trình huấn luyện. Wang et al. (2020); Hu et al. (2021) sử dụng phân tích thứ hạng thấp để thu nhỏ kích thước mô hình và cắt giảm các tham số có thể huấn luyện trong quá trình tinh chỉnh. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, việc tích hợp độ thưa thớt và cấu trúc thứ hạng thấp chưa bao giờ được nghiên cứu trước đây cho việc tinh chỉnh hiệu quả của các mô hình ngôn ngữ tiền huấn luyện.

Thích ứng hiệu quả tham số. Tinh chỉnh hiệu quả tham số nhằm giảm số lượng tham số có thể huấn luyện khi tinh chỉnh các mô hình trên các domain phụ thuộc khác nhau. Không giống như tỉa cành, nó nhằm thích ứng mô hình với ít tham số hơn thay vì xây dựng mô hình thưa thớt. Các cách tiếp cận khác nhau được đề xuất để đạt được mục tiêu này: Rebuffi et al. (2017); Houlsby et al. (2019) chèn và chỉ huấn luyện các adapter giữa các lớp hiện có, có tham số ít hơn nhiều so với các mô hình tiền huấn luyện. Guo et al. (2020) tận dụng regularization ℓ0 để hạn chế số lượng phần tử khác không trong các vector cập nhật. Lester et al. (2021); Li and Liang (2021); Liu et al. (2021) giới thiệu điều chỉnh prompt hiệu quả chỉ tối ưu hóa một vector liên tục nhỏ cụ thể cho tác vụ. Zaken et al. (2021) chỉ tinh chỉnh các thuật ngữ bias bên trong mô hình. Hu et al. (2021) đề xuất một phương pháp dựa trên phân tích thứ hạng thấp, và He et al. (2021) kết hợp các phương pháp dựa trên thứ hạng thấp và adapter để tinh chỉnh hiệu quả. Tuy nhiên, các mô hình tinh chỉnh được tạo ra bởi các phương pháp này có cùng số lượng trọng số như các mô hình tiền huấn luyện; do đó chúng không đóng góp hiệu quả tài nguyên.

3 Phương pháp luận
Trong phần này, chúng tôi mô tả ký hiệu và định nghĩa của chúng tôi về sinh độ thưa thớt và tinh chỉnh hiệu quả tham số trong Phần 3.1, sau đó giới thiệu các thuật toán tinh chỉnh hiệu quả nhúng độ thưa thớt kép trong Phần 3.2 và 3.3.

3.1 Tiền đề
Thưa thớt hóa và tinh chỉnh hiệu quả tài nguyên.
Chúng tôi áp dụng cả phương pháp tỉa cành không có cấu trúc và có cấu trúc để tạo ra độ thưa thớt. Chúng có thể dẫn đến hiệu quả tài nguyên bao gồm tiết kiệm bộ nhớ và tính toán.

Cho trước W ∈ Rm×n là một ma trận trọng số, tỉa cành nhằm tìm một mặt nạ nhị phân M ∈ {0,1}m×n được áp dụng cho W và tạo ra một trọng số thưa thớt

[Hình 1: Tổng quan về đề xuất của chúng tôi. Các mặt nạ thưa thớt có thể có mẫu không có cấu trúc hoặc có cấu trúc, dẫn đến hiệu quả tài nguyên. Trong quá trình tinh chỉnh, chúng tôi chỉ huấn luyện các ma trận phân tích U, V và các phần tử khác không trong S2.]

W ⊙ M. Các trọng số tại các vị trí mà M có giá trị "0" được coi là đã tỉa cành. Phương pháp tỉa cành có thể được phân loại thành hai lớp theo cấu trúc của M: Đối với tỉa cành không có cấu trúc nơi M không có cấu trúc thưa thớt như hàng và cột, chi phí bộ nhớ được tiết kiệm do ít tham số khác không hơn; đối với tỉa cành có cấu trúc, nó cũng giúp tiết kiệm chi phí tính toán vì các trọng số thưa thớt có thể nhỏ hơn về kích thước. Một trong những phương pháp tỉa cành không có cấu trúc được sử dụng rộng rãi nhất là độ lớn trọng số (Han et al., 2015), tức là loại bỏ các trọng số có giá trị tuyệt đối nhỏ nhất. Một phương pháp tỉa cành có cấu trúc phổ biến trong lĩnh vực NLP là tỉa cành đầu (McCarley et al., 2019), cố gắng loại bỏ các đầu attention không quan trọng khỏi mô hình.

Tinh chỉnh hiệu quả tham số. Để tận dụng kiến thức trong trọng số tiền huấn luyện W, các mô hình phụ thuộc học cập nhật trọng số ∆W cụ thể cho tác vụ thông qua tinh chỉnh và tạo ra dự đoán với trọng số W + ∆W, nơi đầu ra của mô hình được tính toán là (W + ∆W)x với x là đầu vào. Vì ∆W có cùng kích thước với W, việc học các ma trận cập nhật thường yêu cầu tài nguyên khổng lồ khi kích thước của mô hình tiền huấn luyện tăng lên. Tinh chỉnh hiệu quả tham số cố gắng giải quyết vấn đề này bằng cách sử dụng ít tham số có thể huấn luyện nhất có thể để biểu diễn ∆W, trong khi duy trì hiệu suất tinh chỉnh phụ thuộc cạnh tranh. Tài liệu trước đây đạt được mục tiêu thông qua việc thưa thớt hóa ma trận cập nhật trọng số ∆W (Guo et al., 2020) hoặc tận dụng ma trận phân tích thứ hạng thấp để tính toán ∆W (Hu et al., 2021), trong khi trong công trình của chúng tôi, chúng tôi kết hợp cả hai.

3.2 Tinh chỉnh Hiệu quả Tham số Nhúng Độ Thưa Thớt
Một nghiên cứu gần đây (Hu et al., 2021) thực thi ràng buộc thứ hạng thấp lên các tensor cập nhật trọng số ∆W, và đạt được sự cân bằng thỏa mãn giữa hiệu quả tham số và chất lượng mô hình. Tuy nhiên, như được tiết lộ thực nghiệm bởi (Yu et al., 2017), một phần thông tin quan trọng trong các trọng số được huấn luyện phân tán bên ngoài không gian con thứ hạng thấp, tạo ra "phần dư" thưa thớt. Được truyền cảm hứng từ quan sát này, chúng tôi điều tra một không gian con thứ hạng thấp có nhận biết độ thưa thớt mới của ∆W, và giới thiệu thành phần đầu tiên của đề xuất chúng tôi trong Hình 1, tức là tinh chỉnh hiệu quả tham số nhúng độ thưa thớt.

Cụ thể, các cập nhật trọng số ∆W bao gồm hai thành phần như được minh họa trong Hình 1: (1) một thành phần thứ hạng thấp ∆Wl được xây dựng bằng phép nhân của hai ma trận U ∈ Rm×r và V ∈ Rr×n; và (2) một phần dư thưa thớt ∆Ws = PΩ(S) nơi S ∈ Rm×n là một ma trận có thể học, PΩ(S) = (si,j, (i,j) ∈ Ω; 0, (i,j) ∈ ΩC, i = 1,2,...,m, j = 1,2,...,n, wi,j là tham số của S tại vị trí (i,j), và Ω là một tập chỉ số chứa các vị trí của các phần tử khác không trong S. Ma trận cập nhật ∆W được biểu diễn là ∆Wl + ∆Ws, với U, V và S là các tham số có thể học trong khi Ω được cố định một khi được xác định. So với tinh chỉnh đầy đủ có m×n tham số có thể huấn luyện cho một ma trận có kích thước m×n, phương pháp của chúng tôi chỉ có (m+n)×r + card(Ω) tham số có thể huấn luyện. Nếu r nhỏ hơn m×n−card(Ω)/(m+n) ≪ 0.5 min{m,n}, phương pháp của chúng tôi có khả năng giảm tham số có thể huấn luyện cho tinh chỉnh phụ thuộc. Trong thực tế, giá trị của r rất nhỏ so với m và n nên việc tiết kiệm là đáng kể.

Một câu hỏi cho pipeline trên là làm thế nào để tìm một tập chỉ số Ω chất lượng cao. Được truyền cảm hứng từ quan sát rằng thành phần thứ hạng thấp ∆Wl có tương quan cao với cấu trúc thứ hạng thấp của W (Hu et al., 2021), chúng tôi giả định rằng tập chỉ số Ω cũng nên có tương quan cao. Cụ thể hơn, chúng tôi giả định rằng các phần dư thưa thớt không nằm trong không gian con chiều thấp của W cũng có thể nằm bên ngoài ∆Wl, điều này thúc đẩy thiết kế cập nhật thưa thớt ∆Ws. Chúng tôi hình thành vấn đề khám phá các phần dư thưa thớt của W như một Phân tích Thành phần Chính Mạnh mẽ (Candès et al., 2011). Chính thức, chúng tôi nhằm giải quyết vấn đề tối ưu hóa sau:

min(U,V,S) 1/2‖W−UV−S‖²F
s.t. rank(U) ≤ r, rank(V) ≤ r,
card(S) ≤ c.                                    (1)

nơi rank(·) và card(·) chỉ ra thứ hạng và số lượng phần tử khác không của một ma trận, tương ứng. S' đại diện cho các phần dư thưa thớt không thể phù hợp trong thành phần thứ hạng thấp AB, và chúng tôi thu được các vị trí của các phần tử có độ lớn khác không vào Ω. Để giải quyết Vấn đề 1 một cách hiệu quả, chúng tôi áp dụng thuật toán không SVD được gọi là GreBsmo (Zhou và Tao, 2013) (tham khảo Phần A.2). Thuật toán 1 tóm tắt quy trình chi tiết của việc xây dựng tập chỉ số thưa thớt Ω. Theo kinh nghiệm, chúng tôi đặt kích thước của Ω (tức là c) là 16 vì nó mang lại hiệu suất thử nghiệm cao (tham khảo Phần 4.2) trong khi áp đặt ít chi phí trên tham số. Các giá trị ban đầu của V và S được đặt là 0 nên các ma trận này không ảnh hưởng đến đầu ra khi bắt đầu huấn luyện.

3.3 Điều chỉnh Hiệu quả Nhúng Độ Thưa Thớt Kép (DSEE)
Thích ứng các mô hình tiền huấn luyện với ∆Wl và ∆Ws có thể mang lại hiệu quả tham số đáng kể, nhưng không trực tiếp mang lại bất kỳ hiệu quả tài nguyên nào như bộ nhớ hoặc chi phí tính toán. Được thúc đẩy bởi điều đó, chúng tôi đề xuất một khung thống nhất được gọi là DSEE theo đuổi cả hiệu quả tham số và tài nguyên đồng thời. Chúng tôi tận dụng độ thưa thớt trong trọng số của các mô hình tiền huấn luyện để tăng cường hiệu quả tài nguyên, như được chứng minh trong Hình 1. Cụ thể hơn, chúng tôi dẫn xuất các mặt nạ thưa thớt M trực tiếp từ các cập nhật hiệu quả tham số ∆W, và áp dụng các mặt nạ thưa thớt bằng cách tỉa cành các trọng số tiền huấn luyện W để tìm kiếm hiệu quả tài nguyên. Nó không yêu cầu tham số bổ sung nào để thưa thớt hóa mô hình và không cần truy cập vào trọng số tiền huấn luyện cơ bản, điều này thuận lợi do chi phí thưa thớt hóa thấp hơn.

--- TRANG 3 ---
Như được thể hiện trong Thuật toán 2, DSEE xử lý tỉa cành không có cấu trúc và có cấu trúc cùng lúc: đối với tỉa cành không có cấu trúc, chúng tôi sắp xếp độ lớn của ∆W, tạo ra một mặt nạ thưa thớt M bằng cách gán "1" cho vị trí mà ∆W có độ lớn lớn nhất và "0" cho phần còn lại; đối với tỉa cành có cấu trúc, chúng tôi tính tổng độ lớn của ∆W của mỗi đầu và loại bỏ những đầu có điểm số ít nhất. Chúng tôi cũng thu nhỏ ∆W tương ứng bằng cách loại bỏ các cột trọng số tương ứng trong V và ∆Ws để khớp hình dạng trong khi giữ nguyên U. Một so sánh các tiêu chí tỉa cành khác nhau được thể hiện trong Phần 4.2.1, chứng minh rằng ∆W là một lựa chọn vượt trội do hiệu suất tác vụ phụ thuộc cao và không cần truy cập vào trọng số tiền huấn luyện W.

Cho trước một ngân sách tham số, số lượng tham số trên mỗi module giảm nếu chúng ta chọn thích ứng nhiều module hơn, điều này áp đặt một sự cân bằng. Chúng tôi nghiên cứu các lựa chọn khác nhau của module để thích ứng trong Phần 4.2.2, và chúng tôi thấy rằng các module tối ưu để thích ứng là Wq và Wv, nơi Wq và Wv đại diện cho trọng số chiếu cho query và value trong các đầu attention. Vì một số module không được thích ứng trong quá trình tinh chỉnh (tức là ∆W = 0), chúng tôi tỉa cành chúng riêng biệt theo độ lớn của trọng số tiền huấn luyện tương ứng. Sau khi áp dụng mặt nạ M cho trọng số tiền huấn luyện W, chúng tôi thông thường điều chỉnh ∆Wl (= UV) và ∆Ws (= PΩ(S)) trong vài epoch để khôi phục hiệu suất (Han et al., 2015).

4 Kết quả Thí nghiệm
Tập dữ liệu và mô hình. Chúng tôi sử dụng ba mô hình ngôn ngữ tiền huấn luyện cổ điển trong các thí nghiệm của chúng tôi: BERT BASE (Devlin et al., 2019), RoBERTa LARGE (Liu et al., 2019) và GPT-2 (Radford et al., 2019), có 12/24/24 lớp với kích thước ẩn 768/1024/1024 và 110/380/354M tham số có thể huấn luyện, tương ứng. Đối với BERT và RoBERTa, chúng tôi đánh giá trên các benchmark GLUE (Wang et al., 2018), và đối với GPT-2, chúng tôi sử dụng E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017) và DART (Nan et al., 2021).

Chi tiết huấn luyện và đánh giá. Đối với BERT và RoBERTa, chúng tôi tuân theo các cài đặt mặc định trong Wolf et al. (2019); Devlin et al. (2019). Chúng tôi sử dụng trình tối ưu hóa AdamW (Loshchilov và Hutter, 2017) cho tinh chỉnh phụ thuộc, và kích thước batch là 32 cho BERT và RoBERTa, và kích thước batch là 2 cho GPT-2. Các siêu tham số còn lại cho huấn luyện được báo cáo trong Bảng 11.

Thuật toán 1: Phân tích Nhúng Độ Thưa Thớt Thứ hạng thấp
Đầu vào: Trọng số tiền huấn luyện W, số lượng phần tử khác không N, số lượng trọng số để phân tích n.
Đầu ra: Tập chỉ số Ωi, i = 1,2,...,n.
1 Khởi tạo mỗi Ωi là một tập rỗng.
2 foreach ma trận trọng số Wi trong W do
/*Phân tích*/
3   Thực hiện phân tích ma trận
    Wi ≈ AB + S' bằng cách giải quyết
    vấn đề tối ưu hóa 1.
/*Trích xuất các phần tử quan trọng từ S'
vào Ωi.*/
4   Thực hiện ngưỡng trên S': Giữ N
    phần tử trong S' với độ lớn hàng đầu,
    và nối vị trí của chúng vào Ωi.
5 end

Thuật toán 2: DSEE
Đầu vào: Trọng số tiền huấn luyện W, số lượng phần tử khác không N, độ thưa thớt mong muốn s, hàm mất mát L.
Đầu ra: Mặt nạ thưa thớt M, ma trận U, V, S.
Dẫn xuất Ω từ trọng số tiền huấn luyện W.
Khởi tạo: U = 0, V ~ N(0,0.02), và S = 0.
/*I: huấn luyện trước tỉa cành*/
Huấn luyện U, V, S theo L dưới
ràng buộc PΩC(S) = 0.
/*II: tỉa cành mô hình*/
if sử dụng tỉa cành không có cấu trúc then
    Tỉa cành (1-s%) trọng số trong W bằng cách
    sắp xếp độ lớn của ∆W.
else
    Tỉa cành (1-s%) đầu bằng cách sắp xếp
    độ lớn tổng hợp của ∆W của các đầu.
    Thu nhỏ V và S tương ứng để khớp
    hình dạng.
end if
/*III: điều chỉnh sau tỉa cành*/
Tinh chỉnh U, V, S để khôi phục hiệu suất.

Chỉ số Đánh giá. Đối với benchmark GLUE, chúng tôi báo cáo độ chính xác, tương quan Matthew's, và tương quan Pearson trong đánh giá. Trên GPT-2, chúng tôi sử dụng BLEU (Papineni et al., 2002), METEOR (Denkowski và Lavie, 2014), TER (Snover et al., 2006) và NIST (Doddington, 2002) làm chỉ số đánh giá. Để đánh giá hiệu quả của mô hình, chúng tôi báo cáo số lượng tham số có thể huấn luyện để đo lường hiệu quả tham số, số lượng tham số tổng (số lượng tham số khác không trong mô hình) để đo lường hiệu quả tài nguyên, và FLOP cho hiệu quả tính toán.

Baseline. Trên BERT và RoBERTa, chúng tôi tiến hành các thí nghiệm toàn diện với các phương pháp baseline sau: ❶Fine-tune: tinh chỉnh trực tiếp mô hình đầy đủ; ❷EarlyBERT (Chen et al., 2021b): học điểm số tầm quan trọng cho các đầu và thực hiện tỉa cành dựa trên chúng sau đó; ❸BERT Tickets (Chen et al., 2020): tỉa cành không có cấu trúc dựa trên IMP; ❹P-Tuning v2 (Liu et al., 2021); ❺Bitfit (Zaken et al., 2021): chỉ tinh chỉnh các thuật ngữ bias; và ❻LoRA: phân tích thứ hạng thấp, chỉ học ∆Wl (Hu et al., 2021). Trên GPT-2, chúng tôi tiến hành so sánh với nhiều phương pháp baseline: ❶Adapters (Houlsby et al., 2019): chèn

--- TRANG 4 ---
Bảng 1: So sánh hiệu suất với BERT BASE trên SST-2, RTE, CoLA, và MRPC. Chúng tôi báo cáo cả trung vị và độ lệch chuẩn từ năm lần chạy.

∆W = # Tham số    SST-2           RTE            CoLA           MRPC
     Có thể huấn luyện
∆Wl  589.8K      92.55 (0.35)    68.95 (2.02)   60.34 (1.69)   86.27 (0.88)
∆Wl+ ∆Ws 590.2K  92.78 (0.34)    70.04 (1.35)   60.31 (1.04)   86.52 (0.57)
∆Wl  294.9K      92.32 (0.36)    68.23 (1.43)   58.48 (1.61)   86.52 (0.72)
∆Wl+ ∆Ws 295.3K  92.66 (0.06)    69.31 (2.08)   58.85 (0.92)   87.01 (0.79)

adapter sau các lớp tuyến tính; ❷FT-Top2: chỉ tinh chỉnh 2 lớp hàng đầu; ❸: Prefix: điều chỉnh tiền tố được giới thiệu bởi Li và Liang (2021); và ❹LoRA.

4.1 Điều chỉnh Hiệu quả với DSEE
Hiệu quả tham số với phần dư thưa thớt.
Để xác minh rằng việc sử dụng thành phần thứ hạng thấp đơn giản ∆Wl có hạn chế, chúng tôi so sánh hiệu suất của nó với tinh chỉnh hiệu quả nhúng độ thưa thớt của chúng tôi. Bảng 1 cho thấy rằng trên bốn benchmark (tức là SST-2, RTE, CoLA, và MRPC), việc thêm một phần dư thưa thớt trong các cập nhật trọng số có thể mang lại tăng hiệu suất: ở mức khoảng 600K tham số có thể huấn luyện, việc thêm phần dư thưa thớt chỉ với 384 phần tử khác không (12×2×16 = 384) có thể tăng hiệu suất xác thực trên tất cả benchmark ngoại trừ CoLA từ 0.23%∼1.09%; ở mức khoảng 300K tham số có thể huấn luyện, việc thêm phần dư thưa thớt có thể mang lại tăng hiệu suất từ 0.34% đến 1.08% trên tất cả bốn benchmark.

Chúng tôi tiếp tục xác minh rằng việc thêm phần dư thưa thớt ∆Ws có thể có lợi cho các tác vụ NLG với GPT-2. Bảng 2 cho thấy rằng dưới các mức tham số khác nhau, việc thêm phần dư thưa thớt ∆Ws mang lại hiệu suất cao hơn cho hầu hết các chỉ số trên ba tác vụ. Ở mức 0.39M tham số, việc thêm phần dư thưa thớt có thể cải thiện tất cả chỉ số trên WebNLG và DART, và tăng nhẹ điểm NIST trên E2E. Ở mức 0.20M tham số, ∆Ws giúp tăng tất cả chỉ số trên ba tác vụ. Chúng tôi cũng thể hiện độ lệch chuẩn trong Bảng 10.

Hiệu quả tài nguyên và tham số với mặt nạ thưa thớt không có cấu trúc. Chúng tôi xác minh rằng DSEE có khả năng tăng cường cả hiệu quả tham số và tài nguyên, trong khi bảo tồn hiệu suất trên các tác vụ phụ thuộc, trên các kiến trúc khác nhau. Bảng 3 tóm tắt kết quả thí nghiệm trên BERT BASE, và chúng tôi quan sát thấy rằng việc giới thiệu các mẫu độ thưa thớt không có cấu trúc bên trong trọng số tiền huấn luyện không chỉ mang lại hiệu quả tài nguyên (được thể hiện bằng số lượng tham số tổng ít hơn) mà còn có thể cải thiện hiệu suất trên các tác vụ phụ thuộc. Cụ thể, ở 80% và 70% tham số tổng, DSEE có thể duy trì hiệu suất so sánh trên các tác vụ phụ thuộc, và thậm chí còn thể hiện sự tăng hiệu suất trên QQP, RTE, và SST-2 so với LoRA. Ở mức 50% tham số, hiệu suất trên các tập dữ liệu nhỏ hơn như CoLA và RTE giảm với biên độ rộng hơn; nhưng trên các tập dữ liệu lớn hơn như QQP, DSEE có thể duy trì hiệu suất so sánh (<1.5% khoảng cách) sau thưa thớt hóa.

Trên GPT-2, chúng tôi quan sát xu hướng tương tự như được thể hiện trong Bảng 4. DSEE có thể đạt được hiệu suất vượt trội với các mẫu thưa thớt không có cấu trúc với 80% tham số tổng so với tinh chỉnh toàn bộ mô hình, và vẫn có tính cạnh tranh cao với các baseline khác với ít tham số hơn trong mô hình. Chỉ sử dụng 50% tham số trong trọng số tiền huấn luyện, DSEE có thể đạt được hiệu suất so sánh với tinh chỉnh đầy đủ trên E2E và DART.

Cuối cùng, chúng tôi xác thực nếu DSEE có thể hoạt động trên mô hình lớn hơn RoBERTa LARGE. Chúng tôi tiến hành thí nghiệm trên bốn tập dữ liệu (CoLA, SST-2, QNLI, và RTE), và trình bày kết quả trong Bảng 5. So với tinh chỉnh đầy đủ, LoRA, và Adapter, phương pháp của chúng tôi đạt hiệu suất so sánh trên bốn tác vụ phụ thuộc này và tiết kiệm tài nguyên cùng lúc. Khoảng cách hiệu suất tối đa là 1% nhưng 30% tham số trong mô hình bị loại bỏ.

Hiệu quả tài nguyên và tham số với mặt nạ thưa thớt có cấu trúc. DSEE có thể trực tiếp thực hiện tỉa cành có cấu trúc trên trọng số mà không cần tham số bổ sung như điểm số tầm quan trọng của các đầu. Trong Bảng 6 chúng tôi thể hiện hiệu suất của BERT BASE được tỉa cành có cấu trúc trên một số tác vụ trong benchmark GLUE, nơi chúng tôi nghiên cứu độ chính xác thử nghiệm sau khi loại bỏ 3, 6 và 9 đầu attention trên SST-2, MNLI, QNLI và QQP, cũng như tỷ lệ FLOP suy luận của mô hình. Thứ nhất, việc loại bỏ 3 đầu khỏi mô hình đạt hiệu suất so sánh với tinh chỉnh đầy đủ (cải thiện trên SST-2, MNLI, và QNLI) và LoRA (cải thiện trên SST-2 và QQP), trong khi tận dụng lợi thế của việc giảm FLOP suy luận. Thứ hai, việc loại bỏ 6 đầu khỏi mô hình sẽ dẫn đến hiệu suất thấp hơn vì một nửa tham số trong các ma trận chiếu bị loại bỏ. Tuy nhiên, hiệu suất của DSEE vẫn cao hơn EarlyBERT. Cuối cùng, DSEE với 9 đầu được loại bỏ khỏi mô hình dẫn đến hiệu suất so sánh với EarlyBERT, nhưng số lượng tham số có thể huấn luyện nhỏ hơn đáng kể (0.6M so với 66M).

--- TRANG 5 ---
Bảng 2: So sánh hiệu suất của phân tích khác nhau trên GPT-2 với các thuật ngữ cập nhật trọng số khác nhau. Chúng tôi báo cáo giá trị trung vị của BLEU, MET, NIST và TER từ năm lần chạy.

Dạng thức # Tham số    E2E                     WebNLG                  DART
         Có thể huấn luyện BLEU  MET   NIST   BLEU  MET   TER   BLEU  MET   TER
∆W = ∆Wl     0.39M         70.38 46.89 8.844  55.29 0.414 0.394 48.23 0.392 0.469
∆W = ∆Wl+ ∆Ws 0.39M         70.29 46.65 8.858  55.50 0.416 0.392 48.17 0.397 0.467
∆W = ∆Wl     0.20M         69.17 45.90 8.741  55.23 0.413 0.396 46.49 0.387 0.477
∆W = ∆Wl+ ∆Ws 0.20M         69.70 46.85 8.824  55.56 0.413 0.392 47.47 0.393 0.475

Bảng 3: So sánh hiệu suất của các phương pháp khác nhau trên benchmark GLUE với BERT BASE. Chúng tôi sử dụng tỉa cành không có cấu trúc và báo cáo giá trị trung vị từ năm lần chạy. †: kết quả lấy từ Chen et al. (2020).

Phương pháp  # Tham số    # Tham số  Tập dữ liệu
            Có thể huấn luyện Tổng      CoLA  STS-B MNLI  QQP   QNLI  MRPC  RTE   SST-2
Fine-tune†  110M          100%       54.5  88.4  82.4  90.2  89.1  85.2  66.2  92.1
BERT Tickets† 33∼55M      30∼50%     53.8  88.2  82.6  90.0  88.9  84.9  66.0  91.9
P-Tuning v2 0.3M          100%       59.37 89.36 82.15 88.50 90.59 84.80 67.51 92.20
Bitfit      0.1M          100%       58.61 88.74 78.80 85.93 89.22 87.55 72.20 92.07
LoRA        0.6M          100%       59.99 89.09 83.32 89.48 90.72 86.27 68.95 92.32
DSEE        0.6M          80%        59.94 89.22 83.29 90.00 90.46 86.27 70.76 92.66
DSEE        0.6M          70%        58.69 89.08 83.09 89.97 90.68 86.27 71.48 91.97
DSEE        0.6M          50%        48.49 87.72 81.84 89.55 90.12 81.13 63.90 91.17

4.2 Phân tích Ablation và Trực quan hóa
Chúng tôi nghiên cứu một số lựa chọn tham số và cung cấp trực quan hóa trong phần này.

4.2.1 Các tiêu chí khác nhau cho mặt nạ thưa thớt
Chúng tôi thấy rằng độ lớn của các cập nhật trọng số (tức là |∆W|) là một giải pháp hiệu quả để bảo tồn hiệu suất với cả tỉa cành không có cấu trúc và có cấu trúc. Chúng tôi tiến hành thí nghiệm trên các trọng số được thích ứng (tức là Wq và Wv), và so sánh với hai baseline: ❶Random: thực hiện tỉa cành ngẫu nhiên trên các module được thích ứng; ❷|W + ∆W|: thực hiện tỉa cành dựa trên độ lớn của trọng số được thích ứng cuối cùng. Bảng 7 thể hiện kết quả trên RTE và SST-2 với BERT BASE. Chúng ta có thể thấy từ bảng rằng: ❶thực hiện tỉa cành không có cấu trúc mà không cần truy cập vào trọng số tiền huấn luyện có thể đạt được hiệu suất so sánh trên RTE và SST-2, chỉ yếu hơn một chút so với tỉa cành với trọng số được thích ứng cuối cùng; ❷thực hiện tỉa cành có cấu trúc theo ∆W mang lại hiệu suất cao nhất trên cả hai tập dữ liệu sau huấn luyện. Những quan sát này xác minh tính hiệu quả của đề xuất chúng tôi.

4.2.2 Các lựa chọn khác nhau của module để thích ứng
Chúng tôi nghiên cứu các lựa chọn của module để thích ứng cho DSEE trên RTE. Chúng tôi chọn các module có thể thích ứng trong Wq, Wk, Wv, và Wo, đại diện cho ma trận chiếu cho query, key, value, và output, tương ứng. Chúng tôi giữ số lượng tham số có thể huấn luyện ở cùng mức và đặt mức độ thưa thớt ở 30%. Bảng 9 tóm tắt hiệu suất với các trọng số được thích ứng khác nhau, chứng minh rằng việc thích ứng Wq và Wv mang lại hiệu suất cao nhất. Mỗi module sẽ được cung cấp ít tham số hơn khi thích ứng nhiều module hơn và mô hình có thể không được tinh chỉnh đầy đủ khi thích ứng ít module hơn và dẫn đến hiệu suất kém.

Các phương pháp khác nhau để xác định Ω. Chúng tôi so sánh đề xuất của chúng tôi với các phương pháp khác nhau để xác định Ω từ trọng số tiền huấn luyện W: ❶Magnitude, chọn vị trí của các phần tử có độ lớn cao nhất vào Ω; ❷Random, ngẫu nhiên lấy mẫu các vị trí vào Ω. Kết quả được thể hiện trong Hình 2. Chúng ta có thể quan sát rằng đề xuất của chúng tôi có thể xác định Ω chất lượng cao để tinh chỉnh trên các tác vụ phụ thuộc, được thể hiện bằng hiệu suất liên tục cao hơn với các kích thước khác nhau của tập chỉ số Ω.

Các kích thước khác nhau của Ω. Chúng tôi tìm kiếm từ 8∼256 để tìm kích thước tối ưu của Ω. Ω với kích thước nhỏ hơn mang lại ít tăng hiệu suất hơn, và Ω với kích thước lớn hơn có thể làm hại hiệu quả. Hình 2 thể hiện mối quan hệ giữa kích thước của Ω và hiệu suất trên SST-2. Chúng tôi thấy rằng lựa chọn tối ưu cho tác vụ này là 16 nơi mô hình đạt được hiệu suất cao nhất. Do đó, chúng tôi theo mặc định đặt kích thước của Ω là 16 cho đơn giản.

--- TRANG 6 ---
Bảng 4: So sánh hiệu suất của các phương pháp khác nhau trên GPT-2 trên E2E, WebNLG và DART. ‡: Kết quả lấy từ Hu et al. (2021).

Phương pháp  # Tham số     # Tham số  E2E                     WebNLG                  DART
            Có thể huấn luyện Tổng      BLEU  MET   NIST   BLEU  MET   TER   BLEU  MET   TER
Fine-tune‡  354.92M       100%       68.2  0.462 8.62   47.6  0.39  0.50  46.0  0.39  0.46
Adapters‡   11.48M        100%       68.9  0.461 8.71   55.2  0.41  0.39  45.4  0.38  0.46
FT-Top2‡    25.19M        100%       68.1  0.460 8.59   33.5  0.26  0.75  38.1  0.34  0.56
Prefix‡     0.35M         100%       69.7  0.461 8.81   54.4  0.41  0.41  45.7  0.38  0.46
LoRA‡       0.39M         100%       70.4  0.468 8.85   55.3  0.41  0.39  47.5  0.39  0.45
DSEE        0.39M         80%        69.4  0.465 8.78   54.9  0.44  0.39  47.5  0.39  0.46
DSEE        0.39M         50%        69.5  0.466 8.74   42.0  0.33  0.53  43.4  0.37  0.51

Bảng 5: So sánh hiệu suất của các phương pháp khác nhau trên RoBERTa LARGE trên CoLA, SST-2, MRPC và RTE. ‡: Kết quả lấy từ Hu et al. (2021).

Phương pháp  # Tham số     # Tham số  Tập dữ liệu
            Có thể huấn luyện Tổng      CoLA  SST-2 QNLI  RTE
Fine-tune‡  355.0M        100%       68.0  95.1  94.7  86.6
Adapter‡    0.8M          100%       66.3  96.3  94.7  72.9
LoRA‡       0.8M          100%       68.2  96.2  94.8  85.2
DSEE        0.8M          70%        67.2  96.1  94.4  84.9

Bảng 6: So sánh hiệu suất của các phương pháp khác nhau trên benchmark GLUE với BERT BASE. Chúng tôi thực hiện tỉa cành có cấu trúc và báo cáo giá trị trung vị từ năm lần chạy. †: kết quả lấy từ Chen et al. (2020).

Phương pháp      FLOPs  # Tham số      SST-2  MNLI  QNLI  QQP
                        Có thể huấn luyện
Fine-tune†       1.0×   110M          92.1   82.4  89.1  90.2
LoRA            1.01×   0.6M          92.32  83.32 90.72 89.48
EarlyBERT       0.63×   ∼66M          90.71  81.81 89.18 90.06
DSEE (3 đầu)    0.92×   0.6M          92.55  83.25 90.65 89.84
DSEE (6 đầu)    0.84×   0.6M          92.32  82.32 90.01 89.11
DSEE (9 đầu)    0.75×   0.6M          91.63  80.02 88.39 88.56

[Hình 2: Hiệu suất thử nghiệm trên SST-2 với các kích thước khác nhau của Ω. Chúng tôi báo cáo độ chính xác trung bình và khoảng tin cậy 90% từ năm lần chạy.]

5 Kết luận
Bài báo này dựa trên tiên nghiệm của độ thưa thớt và thiết lập khung DSEE. Đây là nỗ lực đầu tiên hướng tới việc tối ưu hóa đồng thời cả hiệu quả tham số của quá trình tinh chỉnh, và hiệu quả tài nguyên của mô hình tinh chỉnh. Trên

Bảng 7: Hiệu suất của việc sử dụng các tiêu chí tỉa cành khác nhau để tạo ra mặt nạ không có cấu trúc. Chúng tôi chỉ thực hiện tỉa cành trên Wq và Wv. Phần đầu áp dụng tỉa cành không có cấu trúc và phần sau áp dụng tỉa cành có cấu trúc.

Tiêu chí         RTE            SST-2
|∆W|            69.68 (1.37)    91.97 (0.26)
|W + ∆W|        70.76 (2.09)    92.78 (0.39)
Random          64.62 (2.28)    91.63 (0.25)
|∆W|            70.40 (1.05)    92.55 (0.43)
|W + ∆W|        68.59 (1.60)    92.20 (0.60)
Random          68.23 (1.29)    91.97 (0.14)

các mô hình ngôn ngữ quy mô lớn tiên tiến (ví dụ: BERT, GPT, và RoBERTa) và trên một số tập dữ liệu, DSEE liên tục chứng minh hiệu quả tham số và suy luận rất ấn tượng, ngoài việc bảo tồn hiệu suất chuyển giao phụ thuộc cạnh tranh trên các tác vụ khác nhau. Công việc tương lai của chúng tôi nhắm mục tiêu mở rộng DSEE cho việc tinh chỉnh các mô hình tiền huấn luyện tầm nhìn máy tính quy mô lớn và/hoặc đa phương thức.

Hạn chế Các mẫu thưa thớt không có cấu trúc mà chúng tôi giới thiệu không thân thiện với phần cứng bằng các mẫu có cấu trúc, cho thấy tăng tốc của việc sử dụng các mẫu không có cấu trúc có thể bị hạn chế do việc triển khai. Số lượng tham số của các mô hình mà chúng tôi đang nghiên cứu chỉ ở mức 100∼300M, và các tập dữ liệu tập trung vào GLUE, E2E, WebNLG, và DART. Chúng tôi sẽ tổng quát hóa cho các lựa chọn tập dữ liệu rộng hơn trong các công trình tương lai.

6 Tác động Đạo đức và Rộng hơn
DSEE nhằm giảm số lượng tham số có thể huấn luyện khi tinh chỉnh mô hình, có thể giúp tiết kiệm chi phí lưu trữ trọng số mới. Điều này có thể hữu ích cho các công ty đang tinh chỉnh các mô hình ngôn ngữ quy mô lớn

--- TRANG 7 ---
trên các tác vụ phụ thuộc khác nhau, cho thấy công việc của chúng tôi có tác động rộng hơn có thể tích cực. Mặt khác, công việc của chúng tôi không có tác động đạo đức rõ ràng, vì chúng tôi tập trung vào việc điều chỉnh mô hình.

--- TRANG 8 ---
Tài liệu tham khảo
Emmanuel J Candès, Xiaodong Li, Yi Ma, và John Wright. 2011. Phân tích thành phần chính mạnh mẽ? Journal of the ACM (JACM), 58(3):1–37.

Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher Re. 2021a. Pixelated butterfly: Huấn luyện thưa thớt đơn giản và hiệu quả cho các mô hình mạng neural. arXiv preprint arXiv:2112.00029.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. 2020. Giả thuyết vé số may mắn cho các mạng bert tiền huấn luyện. arXiv preprint arXiv:2007.12223.

Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, và Jingjing Liu. 2021b. Earlybert: Huấn luyện bert hiệu quả thông qua vé số may mắn early-bird. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.

Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, và Christopher Ré. 2022. Monarch: Ma trận có cấu trúc biểu cảm để huấn luyện hiệu quả và chính xác. arXiv preprint arXiv:2204.00595.

Michael Denkowski và Alon Lavie. 2014. Meteor universal: Đánh giá dịch thuật cụ thể cho ngôn ngữ cho bất kỳ ngôn ngữ đích nào. In Proceedings of the ninth workshop on statistical machine translation, trang 376–380.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. Bert: Tiền huấn luyện các transformer hai chiều sâu để hiểu ngôn ngữ. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186.

George Doddington. 2002. Đánh giá tự động chất lượng dịch máy sử dụng thống kê đồng xuất hiện n-gram. In Proceedings of the second international conference on Human Language Technology Research, trang 138–145.

Claire Gardent, Anastasia Shimorina, Shashi Narayan, và Laura Perez-Beltrachini. 2017. Thách thức webnlg: Tạo văn bản từ dữ liệu rdf. In Proceedings of the 10th International Conference on Natural Language Generation, trang 124–133.

Demi Guo, Alexander M Rush, và Yoon Kim. 2020. Học chuyển giao hiệu quả tham số với tỉa cành diff. arXiv preprint arXiv:2012.07463.

Song Han, Huizi Mao, và William J Dally. 2015. Nén sâu: Nén mạng neural sâu với tỉa cành, lượng tử hóa được huấn luyện và mã hóa huffman. arXiv preprint arXiv:1510.00149.

Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. 2021. Hướng tới cái nhìn thống nhất về học chuyển giao hiệu quả tham số. arXiv preprint arXiv:2110.04366.

Yihui He, Xiangyu Zhang, và Jian Sun. 2017. Tỉa cành kênh để tăng tốc các mạng neural rất sâu. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), trang 1389–1397.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Học chuyển giao hiệu quả tham số cho nlp. In International Conference on Machine Learning, trang 2790–2799. PMLR.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, và Weizhu Chen. 2021. Lora: Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2106.09685.

Max Jaderberg, Andrea Vedaldi, và Andrew Zisserman. 2014. Tăng tốc mạng neural tích chập với mở rộng thứ hạng thấp. In Proceedings of the British Machine Vision Conference. BMVA Press.

Yann LeCun, John S Denker, và Sara A Solla. 1990. Tổn thương não tối ưu. In Advances in neural information processing systems, trang 598–605.

Brian Lester, Rami Al-Rfou, và Noah Constant. 2021. Sức mạnh của quy mô cho điều chỉnh prompt hiệu quả tham số. arXiv preprint arXiv:2104.08691.

Xiang Lisa Li và Percy Liang. 2021. Prefix-tuning: Tối ưu hóa prompt liên tục để sinh. arXiv preprint arXiv:2101.00190.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, và Jie Tang. 2021. P-tuning v2: Điều chỉnh prompt có thể so sánh với tinh chỉnh một cách phổ quát qua các quy mô và tác vụ. arXiv preprint arXiv:2110.07602.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: Một cách tiếp cận tiền huấn luyện bert được tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692.

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. 2017. Học các mạng tích chập hiệu quả thông qua thu nhỏ mạng. In Proceedings of the IEEE international conference on computer vision, trang 2736–2744.

Ilya Loshchilov và Frank Hutter. 2017. Regularization phân tách trọng số giảm. arXiv preprint arXiv:1711.05101.

JS McCarley, Rishav Chakravarti, và Avirup Sil. 2019. Tỉa cành có cấu trúc của mô hình trả lời câu hỏi dựa trên bert. arXiv preprint arXiv:1910.06360.

--- TRANG 9 ---
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, và Nazneen Fatema Rajani. 2021. Dart: Sinh văn bản từ bản ghi dữ liệu có cấu trúc domain mở.

Jekaterina Novikova, Ondřej Dušek, và Verena Rieser. 2017. Tập dữ liệu e2e: Thách thức mới cho sinh end-to-end. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, trang 201–206.

Samet Oymak, Zalan Fabian, Mingchen Li, và Mahdi Soltanolkotabi. 2019. Đảm bảo tổng quát hóa cho mạng neural thông qua khai thác cấu trúc thứ hạng thấp của jacobian. arXiv preprint arXiv:1906.05392.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: một phương pháp đánh giá tự động dịch máy. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, trang 311–318.

Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, và Sanjeev Khudanpur. 2018. Phân tích nhân tử ma trận thứ hạng thấp bán trực giao cho mạng neural sâu. In Interspeech, trang 3743–3747.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Các mô hình ngôn ngữ là những người học đa tác vụ không giám sát. OpenAI blog, 1(8):9.

Sylvestre-Alvise Rebuffi, Hakan Bilen, và Andrea Vedaldi. 2017. Học nhiều domain thị giác với adapter dư thừa. In Proceedings of the 31st International Conference on Neural Information Processing Systems, trang 506–516.

Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian, Xue Lin, và Yanzhi Wang. 2018. Admm-nn: Một khung đồng thiết kế thuật toán-phần cứng của dnn sử dụng phương pháp hướng xen kẽ của multiplier.

Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, và Bhuvana Ramabhadran. 2013. Phân tích nhân tử ma trận thứ hạng thấp cho huấn luyện mạng neural sâu với mục tiêu đầu ra chiều cao. In 2013 IEEE international conference on acoustics, speech and signal processing, trang 6655–6659. IEEE.

Victor Sanh, Thomas Wolf, và Alexander M Rush. 2020. Tỉa cành chuyển động: Độ thưa thớt thích ứng bằng tinh chỉnh. In NeurIPS.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, và John Makhoul. 2006. Một nghiên cứu về tỷ lệ chỉnh sửa dịch thuật với chú thích của con người có mục tiêu. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, trang 223–231.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. 2018. Glue: Một nền tảng benchmark và phân tích đa tác vụ để hiểu ngôn ngữ tự nhiên. In International Conference on Learning Representations.

Ziheng Wang, Jeremy Wohlwend, và Tao Lei. 2020. Tỉa cành có cấu trúc của các mô hình ngôn ngữ lớn. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 6151–6162.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Transformers của huggingface: Xử lý ngôn ngữ tự nhiên tiên tiến nhất. arXiv preprint arXiv:1910.03771.

Jieping Ye. 2005. Xấp xỉ thứ hạng thấp tổng quát của ma trận. Machine Learning, 61(1-3):167–191.

Xiyu Yu, Tongliang Liu, Xinchao Wang, và Dacheng Tao. 2017. Về nén các mô hình sâu bằng phân tích thứ hạng thấp và thưa thớt. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 7370–7379.

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. 2021. Bitfit: Tinh chỉnh hiệu quả tham số đơn giản cho các mô hình ngôn ngữ có mặt nạ dựa trên transformer. arXiv preprint arXiv:2106.10199.

Yu Zhang, Ekapol Chuangsuwanich, và James Glass. 2014. Trích xuất các đặc trưng thắt cổ chai mạng neural sâu sử dụng phân tích nhân tử ma trận thứ hạng thấp. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), trang 185–189. IEEE.

Yong Zhao, Jinyu Li, và Yifan Gong. 2016. Thích ứng thứ hạng thấp cộng đường chéo cho mạng neural sâu. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), trang 5005–5009. IEEE.

Tianyi Zhou và Dacheng Tao. 2013. Sketch song phương tham lam, hoàn thiện & làm mượt. In Artificial Intelligence and Statistics, trang 650–658. PMLR.

--- TRANG 10 ---
A Chi tiết Triển khai Thêm
A.1 Siêu tham số
Chúng tôi báo cáo tỷ lệ học, kích thước batch, và độ dài chuỗi tối đa cho DSEE trong Bảng 11. Thiết bị chúng tôi sử dụng cho thí nghiệm là đa dạng, bao gồm NVIDIA GeForce GTX 1080 Ti, GeForce RTX 2080 Ti, Titan RTX, và A6000. Chúng tôi tuân theo (Hu et al., 2021) để đặt giao thức đánh giá trên E2E, WebNLG, và DART.

A.2 Phương pháp Phân tích
GreBsmo (Zhou và Tao, 2013) là một thuật toán để giải quyết các phương pháp giống Robust PCA. Tối ưu hóa của U, V và S tuân theo các quy tắc lặp sau:

Uk = Q, QR(X−Sk−1)VTk−1 = QR
Vk = QT(X−Sk−1)
Sk = Sλ(X−UkVk),                                    (2)

nơi X là ma trận dày đặc gốc, QR(·) có nghĩa là phân tích QR, Sλ(·) chỉ ra hàm ngưỡng mềm (tức là Sλ(x) = x1|x|≥λ), và chỉ số dưới k chỉ ra bước tối ưu hóa.

A.3 Thống kê và Sử dụng Tập dữ liệu
Chúng tôi báo cáo thống kê của các tập dữ liệu trong Bảng 8. Đối với các tác vụ GLUE, chúng tôi báo cáo kích thước của tập huấn luyện, tập phát triển và tập thử nghiệm, và đối với các tác vụ không phải GLUE, chúng tôi báo cáo kích thước của tập huấn luyện, xác thực (dev), và tập thử nghiệm. Chúng tôi tuân theo việc sử dụng thông thường của các tập dữ liệu này (Hu et al., 2021) và không sửa đổi các phần chia thông thường.

B Kết quả Thí nghiệm Thêm
B.1 Nghiên cứu Ablation
Bảng 9 tóm tắt hiệu suất với các trọng số được thích ứng khác nhau, chứng minh rằng việc thích ứng Wq và Wv dẫn đến hiệu suất cao nhất.

Hiệu suất của Ω khác nhau. Chúng tôi tiến hành nghiên cứu ablation bổ sung (ba lần chạy cho mỗi thí nghiệm) về kích thước của Ω trên ba tập dữ liệu trong GLUE (tức là STSB, QNLI và MRPC). Kết quả được thể hiện trong Bảng 12 dưới đây xác minh rằng phương pháp của chúng tôi có thể tổng quát hóa cho các tập dữ liệu khác. Trên STSB và QNLI, việc sử dụng kích thước 16 có thể đạt được hiệu suất tốt nhất, trong khi trên MRPC nó có thể đạt được độ chính xác thử nghiệm so sánh.

So sánh với các phương pháp gần đây. Chúng tôi đã tiến hành một bộ thí nghiệm để so sánh các phương pháp của chúng tôi với MAM Adapter (He et al., 2021). Chúng tôi huấn luyện một RoBERTa-large với phương pháp của họ trên SST-2, QNLI, RTE, bằng cách tuân theo các siêu tham số giống như được sử dụng trong công trình gốc. Kết quả được thể hiện trong Bảng 13. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi, ngay cả với các mô hình thưa thớt, đạt được hiệu suất cùng mức với LoRA và MAM Adapter.

Các Phương pháp Tỉa cành Khác. Chúng tôi áp dụng phương pháp tỉa cành độ lớn lặp trên RTE. Cụ thể, chúng tôi huấn luyện mô hình trong 10 epoch, tỉa cành 10% trọng số còn lại, và tinh chỉnh trong 10 epoch trước khi tỉa cành tiếp theo. Bảng 14 cho thấy rằng việc áp dụng trực tiếp tỉa cành độ lớn lặp không mang lại cải thiện hiệu suất so với baseline tỉa cành một lần.

--- TRANG 11 ---
Bảng 8: Thống kê của các tập dữ liệu chúng tôi sử dụng cho thí nghiệm.

Tên        Huấn luyện    Dev      Thử nghiệm
GLUE
CoLA       8,551         1,043    -
SST-2      67,349        872      -
MNLI       392,702       9,815    -
QNLI       104,743       5,463    -
QQP        363,846       40,430   -
STS-B      5,749         1,500    -
RTE        2,490         277      -
MRPC       3,668         408      -
không phải GLUE
E2E        42,061        4,672    4,693
WebNLG     18,025        2,258    4,928
DART       30,526        2,768    6,959

Bảng 9: Hiệu suất thử nghiệm của BERT BASE trên RTE với các module được thích ứng khác nhau. Chúng tôi báo cáo các giá trị trung vị và độ lệch chuẩn từ ba lần chạy.

Trọng số    Độ chính xác thử nghiệm    Trọng số    Độ chính xác thử nghiệm
Wq          68.59 (0.21)              Wk          67.87 (0.21)
Wv          68.23 (1.82)              Wo          68.23 (1.05)
Wq,Wk       68.95 (1.11)              Wq,Wv       71.48 (2.16)
Wk,Wv       70.04 (0.75)              Wq,Wk,Wv    69.31 (2.56)

Bảng 10: So sánh hiệu suất của phân tích khác nhau trên GPT-2 với các thuật ngữ cập nhật trọng số khác nhau. Chúng tôi báo cáo độ lệch chuẩn của BLEU, MET, NIST và TER từ năm lần chạy.

Dạng thức    # Tham số        E2E                     WebNLG                  DART
             Có thể huấn luyện BLEU  MET   NIST   BLEU  MET   TER   BLEU  MET   TER
∆W = ∆Wl     0.39M           0.43  0.13  0.037  0.37  0.005 0.003 0.23  0.001 0.001
∆W = ∆Wl+ ∆Ws 0.39M          0.07  0.26  0.047  0.48  0.005 0.004 0.40  0.003 0.002
∆W = ∆Wl     0.20M           0.23  0.03  0.043  0.26  0.005 0.007 0.06  0.002 0.001
∆W = ∆Wl+ ∆Ws 0.20M          0.61  0.19  0.029  0.52  0.006 0.004 0.15  0.001 0.001

Bảng 11: Siêu tham số chúng tôi sử dụng trên các tập dữ liệu và kiến trúc khác nhau.

Kiến trúc    Phương pháp    Tham số    Tập dữ liệu
                                      MNLI  QNLI  QQP   SST-2 CoLA  MRPC  RTE   STS-B
BERT BASE   DSEE (trước tỉa cành)  Tỷ lệ học  2e-4  2e-4  2e-4  2e-4  1e-3  8e-4  6e-4  8e-4
BERT BASE   DSEE (sau tỉa cành)   Tỷ lệ học  2e-4  2e-4  2e-4  2e-4  1e-3  8e-4  6e-4  8e-4
BERT BASE   DSEE              Kích thước batch  32
BERT BASE   DSEE              Độ dài chuỗi tối đa  128
RoBERTa LARGE DSEE (trước tỉa cành)  Tỷ lệ học  -    2e-4  -    4e-4  3e-4  -    4e-4  -
RoBERTa LARGE DSEE (sau tỉa cành)   Tỷ lệ học  -    2e-4  -    4e-4  3e-4  -    4e-4  -
RoBERTa LARGE DSEE              Kích thước batch  -    32    -    32    16    -    32    -
RoBERTa LARGE DSEE              Độ dài chuỗi tối đa  -    512   -    512   128   -    512   -

Bảng 12: Hiệu suất trên ba tập dữ liệu sử dụng các kích thước khác nhau của Ω.

Tập dữ liệu    Ω = 8    Ω = 16   Ω = 32   Ω = 48
STSB           89.26    89.28    89.10    89.16
MRPC           85.38    86.27    86.36    86.27
QNLI           91.01    91.06    91.00    90.87

Bảng 13: So sánh với nhiều phương pháp hơn.

Phương pháp    Tham số tổng    SST-2    QNLI    RTE
LoRA           100%           96.2     94.8    85.2
MAM Adapter    100%           96.1     94.7    80.4
Của chúng tôi  70%            96.1     94.4    84.9

Bảng 14: Áp dụng tỉa cành độ lớn lặp (IMP) để tỉa cành mô hình.

Trọng số còn lại    Độ chính xác
90%                 70.02%
81%                 70.76%
72.9%               63.90%
65.6%               61.01%
