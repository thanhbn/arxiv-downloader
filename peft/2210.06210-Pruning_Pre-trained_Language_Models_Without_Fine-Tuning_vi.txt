# 2210.06210.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2210.06210.pdf
# Kích thước tệp: 1992400 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa Mô hình Ngôn ngữ Tiền huấn luyện mà không cần Tinh chỉnh
Ting Jiang1, Deqing Wang13y, Fuzhen Zhuang123, Ruobing Xie4, Feng Xia4
1SKLSDE Lab, Trường Máy tính, Đại học Beihang, Bắc Kinh, Trung Quốc
2Viện Trí tuệ Nhân tạo, Đại học Beihang, Bắc Kinh, Trung Quốc
3Phòng thí nghiệm Zhongguancun, Bắc Kinh, Trung Quốc4WeChat, Tencent, Bắc Kinh, Trung Quốc
{royokong, dqwang, zhuangfuzhen}@buaa.edu.cn
Tóm tắt
Để khắc phục vấn đề quá tham số hóa
trong Mô hình Ngôn ngữ Tiền huấn luyện (PLM), cắt
tỉa được sử dụng rộng rãi như một phương pháp nén
đơn giản và trực tiếp bằng cách loại bỏ trực tiếp
các trọng số không quan trọng. Các phương pháp
bậc nhất trước đây đã thành công trong việc nén PLM
đến độ thưa cực cao với ít sụt giảm hiệu suất.
Những phương pháp này, như cắt tỉa theo chuyển động,
sử dụng thông tin bậc nhất để cắt tỉa PLM
trong khi tinh chỉnh các trọng số còn lại. Trong
công trình này, chúng tôi lập luận rằng tinh chỉnh là
thừa cho cắt tỉa bậc nhất, vì cắt tỉa
bậc nhất đủ để hội tụ PLM đến các
nhiệm vụ hạ nguồn mà không cần tinh chỉnh. Dưới
động lực này, chúng tôi đề xuất Cắt tỉa Mô hình
Tĩnh (SMP), chỉ sử dụng cắt tỉa bậc nhất để
thích nghi PLM với các nhiệm vụ hạ nguồn trong khi
đạt được mức độ thưa mục tiêu. Ngoài ra,
chúng tôi cũng thiết kế một hàm che mặt nạ mới và
mục tiêu huấn luyện để cải thiện SMP hơn nữa. Các
thí nghiệm rộng rãi ở nhiều mức độ thưa khác nhau
cho thấy SMP có cải thiện đáng kể so với
các phương pháp bậc nhất và bậc không. Không giống
các phương pháp bậc nhất trước đây, SMP cũng có thể
áp dụng cho độ thưa thấp và vượt trội hơn các phương pháp
bậc không. Đồng thời, SMP hiệu quả tham số hơn
các phương pháp khác do không yêu cầu
tinh chỉnh. Mã nguồn của chúng tôi có sẵn tại
https://github.com/kongds/SMP .
1 Giới thiệu
Các Mô hình Ngôn ngữ Tiền huấn luyện (PLM) như
BERT (Devlin et al., 2019) đã cho thấy hiệu suất
mạnh mẽ trong xử lý ngôn ngữ tự nhiên
bằng cách chuyển giao kiến thức từ kho ngữ liệu
quy mô lớn đến các nhiệm vụ hạ nguồn. Những mô hình này
cũng yêu cầu tham số quy mô lớn để đối phó với
kho ngữ liệu quy mô lớn trong tiền huấn luyện. Tuy nhiên,
những tham số quy mô lớn này quá tải cho hầu hết
các nhiệm vụ hạ nguồn (Chen et al., 2020), điều này
yTác giả Liên hệ.dẫn đến chi phí đáng kể cho việc chuyển giao và
lưu trữ chúng.
Để nén PLM, cắt tỉa được sử dụng rộng rãi bằng
cách loại bỏ các trọng số không quan trọng và đặt chúng
thành số không. Bằng cách sử dụng các mạng con thưa
thay vì mạng hoàn chỉnh ban đầu, các phương pháp
cắt tỉa hiện có có thể duy trì độ chính xác ban đầu bằng
cách loại bỏ hầu hết các trọng số. Cắt tỉa theo độ lớn
(Han et al., 2015) như một phương pháp phổ biến sử dụng
thông tin bậc không để đưa ra quyết định cắt tỉa dựa
trên giá trị tuyệt đối của trọng số. Tuy nhiên, trong
quá trình thích nghi với các nhiệm vụ hạ nguồn, các giá trị
trọng số trong PLM đã được xác định trước từ
các giá trị ban đầu. Để khắc phục nhược điểm này,
cắt tỉa theo chuyển động (Sanh et al., 2020) sử dụng
thông tin bậc nhất để chọn trọng số dựa trên cách
chúng thay đổi trong huấn luyện thay vì giá trị tuyệt đối
của chúng. Để thích nghi PLM cho các nhiệm vụ hạ nguồn,
hầu hết các phương pháp như cắt tỉa theo chuyển động thực hiện
cắt tỉa và tinh chỉnh cùng nhau bằng cách tăng dần
độ thưa trong quá trình huấn luyện. Với sự phát triển
của Giả thuyết Vé số Trúng thưởng (LTH) (Frankle
and Carbin, 2018) trong PLM, một số phương pháp (Chen
et al., 2020; Liang et al., 2021) tìm các mạng con
nhất định từ PLM bằng cắt tỉa, và sau đó tinh chỉnh
những mạng con này từ trọng số tiền huấn luyện. Hơn
nữa, nếu mạng con được tinh chỉnh có thể khớp với
hiệu suất của PLM đầy đủ, mạng con này được
gọi là vé số trúng thưởng (Chen et al., 2020).
Trong công trình này, chúng tôi đề xuất một phương pháp
bậc nhất đơn giản nhưng hiệu quả. Trái ngược với
phương pháp cắt tỉa trước đây, phương pháp của chúng tôi
thích nghi PLM chỉ bằng cắt tỉa, không có tinh chỉnh.
Nó đưa ra quyết định cắt tỉa dựa trên xu hướng chuyển động
của trọng số, thay vì chuyển động thực tế trong cắt tỉa
theo chuyển động. Để cải thiện hiệu suất của phương pháp,
chúng tôi đề xuất một hàm che mặt nạ mới để căn chỉnh
tốt hơn các trọng số còn lại theo kiến trúc của
PLM. Chúng tôi cũng tránh tinh chỉnh trọng số trong
đầu đặc trưng nhiệm vụ bằng cách sử dụng phương pháp
khởi tạo đầu của chúng tôi. Bằng cách giữ PLM đóng băng,
chúng tôi có thể tiết kiệmarXiv:2210.06210v2  [cs.CL]  16 May 2023

--- TRANG 2 ---
một nửa số tham số có thể huấn luyện so với các
phương pháp bậc nhất khác, và chỉ giới thiệu một mặt nạ
nhị phân như tham số mới cho mỗi nhiệm vụ hạ nguồn
ở nhiều mức độ thưa khác nhau. Các thí nghiệm rộng rãi
trên nhiều độ thưa cho thấy phương pháp của chúng tôi
vượt trội đáng kể so với các phương pháp cắt tỉa
tiên tiến. Trái ngược với các phương pháp bậc nhất
trước đây (Sanh et al., 2020), cho thấy hiệu suất kém
ở độ thưa thấp, phương pháp của chúng tôi cũng được
áp dụng cho độ thưa thấp và đạt hiệu suất tốt hơn
các phương pháp bậc không.
2 Công trình Liên quan
Nén PLM cho học chuyển giao là một lĩnh vực
nghiên cứu phổ biến. Nhiều phương pháp nén được
đề xuất để giải quyết vấn đề quá tham số hóa
trong PLM, như cắt tỉa mô hình (Han et al., 2015;
Molchanov et al., 2017; Xia et al., 2022), chưng cất
kiến thức (Jiao et al., 2020; Wang et al.,
2020), lượng tử hóa (Shen et al., 2020; Qin et al.,
2022), và phân tích ma trận (Lan et al., 2020).
Trong số đó, các phương pháp cắt tỉa đã được nghiên cứu
rộng rãi như cách tiếp cận trực quan nhất.
Các phương pháp cắt tỉa tập trung vào việc xác định và
loại bỏ các trọng số không quan trọng khỏi mô hình.
Các phương pháp bậc không và bậc nhất được sử dụng
rộng rãi để cắt tỉa PLM. Đối với các phương pháp bậc không,
cắt tỉa theo độ lớn (Han et al., 2015) đơn giản cắt tỉa
dựa trên giá trị tuyệt đối của trọng số. Đối với
các phương pháp bậc nhất, dựa trên khai triển Taylor
bậc nhất để đưa ra quyết định cắt tỉa, chính quy L0
(Louizos et al., 2017) thêm chính quy chuẩn L0
để giảm các trọng số còn lại bằng cách lấy mẫu chúng
với phân phối bê tông cứng. Cắt tỉa theo chuyển động
(Sanh et al., 2020) sử dụng ước lượng thẳng
(Bengio et al., 2013) để tính thông tin bậc nhất.
Dựa trên các phương pháp cắt tỉa, Frankle và
Carbin (2018) đề xuất Giả thuyết Vé số Trúng thưởng
(LTH). LTH làm rõ sự tồn tại của các mạng con
thưa (tức là, vé số trúng thưởng) có thể đạt được
hiệu suất gần như tương tự mô hình đầy đủ khi được
huấn luyện riêng lẻ. Với sự phát triển của LTH,
nhiều công trình tập trung vào PLM đã xuất hiện.
Chen et al. (2020) phát hiện rằng BERT chứa
vé số trúng thưởng với độ thưa từ 40% đến 90%,
và vé số trúng thưởng trong nhiệm vụ mô hình hóa
ngôn ngữ có mặt nạ có thể được chuyển giao đến
các nhiệm vụ hạ nguồn khác. Các công trình gần đây
cũng cố gắng tận dụng LTH để cải thiện hiệu suất
và hiệu quả của PLM. Liang et al. (2021) phát hiện
hiệu suất tổng quát hóa của các vé số trúng thưởng trước tiên cải thiện và sau đó
xấu đi sau một ngưỡng nhất định. Bằng cách tận dụng
hiện tượng này, họ cho thấy LTH có thể thành công
cải thiện hiệu suất của các nhiệm vụ hạ nguồn.
3 Nền tảng
Cho a=Wx tham chiếu đến một lớp kết nối đầy đủ trong
PLM, trong đó W2Rnnis ma trận trọng số,
x2Rnvàa2Rnlần lượt là đầu vào và đầu ra
. Cắt tỉa có thể được biểu diễn bằng
a= (WM)x, trong đó M2f0;1gnnis
mặt nạ nhị phân.
Chúng tôi trước tiên xem xét hai phương pháp cắt tỉa
phổ biến trong PLM: cắt tỉa theo độ lớn (Han et al., 2015)
và cắt tỉa theo chuyển động (Sanh et al., 2020). Cắt tỉa
theo độ lớn dựa vào thông tin bậc không để quyết định
Mbằng cách giữ vpercent hàng đầu của trọng số
theo giá trị tuyệt đối M=Topv(S).
Điểm quan trọng S2Rnnis:
S(T)
i;j=W(T)
i;j
=Wi;jwX
t<T@L
@Wi;j(t)(1)
trong đóS(T)
i;jlà điểm quan trọng tương ứng
vớiW(T)
i;jsauTbước cập nhật,Lvàware
mục tiêu học và tốc độ học của Wi;j. Cắt tỉa
theo độ lớn chọn trọng số có giá trị tuyệt đối cao
trong quá trình tinh chỉnh.
Đối với cắt tỉa theo chuyển động, nó dựa vào thông tin
bậc nhất bằng cách học điểm quan trọng S
với gradient. Gradient của Sđược xấp xỉ
với ước lượng thẳng (Bengio et al.,
2013), trực tiếp sử dụng gradient từ M.
Theo (Sanh et al., 2020), điểm quan trọng Slà:
S(T)
i;j=sX
t<T@L
@Wi;j(t)
W(t)
i;j (2)
trong đóslà tốc độ học của S. So với
cắt tỉa theo độ lớn, cắt tỉa theo chuyển động chọn
trọng số đang tăng giá trị tuyệt đối của chúng.
Để đạt được độ thưa mục tiêu, một phương pháp phổ biến
làcắt tỉa dần tự động (Michael H. Zhu,
2018). Mức độ thưa vlà tăng dần
với bộ lập lịch độ thưa khối bắt đầu từ
bước huấn luyện t0:vt=vf+ (v0vf)
1tt0
Nt3,
trong đóv0vàvflần lượt là độ thưa ban đầu và mục tiêu,
Nlà tổng số bước cắt tỉa, và tlà tần suất
cắt tỉa.

--- TRANG 3 ---
Trong quá trình huấn luyện, những phương pháp này cập nhật
cả WvàSđể thực hiện cắt tỉa và tinh chỉnh đồng
thời. Vì các trọng số được tinh chỉnh vẫn gần với
giá trị tiền huấn luyện của chúng (Sanh et al., 2020),
điểm quan trọng của cắt tỉa theo độ lớn bị ảnh hưởng
bởi giá trị tiền huấn luyện, điều này hạn chế hiệu suất
ở độ thưa cao. Tuy nhiên, cắt tỉa theo độ lớn vẫn
vượt trội hơn cắt tỉa theo chuyển động ở độ thưa thấp.
4 Cắt tỉa Mô hình Tĩnh
Trong công trình này, chúng tôi đề xuất một phương pháp
cắt tỉa bậc nhất đơn giản gọi là Cắt tỉa Mô hình Tĩnh
(SMP). Nó đóng băng Wđể làm cho cắt tỉa trên PLM
hiệu quả và có thể chuyển giao hơn. Dựa trên cắt tỉa
theo chuyển động (Sanh et al., 2020), điểm quan trọng
Scủa chúng tôi là:
S(T)
i;j=sWi;jX
t<T 
@L
@W0
i;j!(t)
(3)
trong đóW0
i;jlàWi;jMi;j. Vì phương pháp của chúng tôi
đóng băng Wi;j, chúng tôi cũng giữ thuật ngữ che mặt nạ
nhị phân Mi;j. Si;jtăng khi Wi;j@L
@W0
i;j<0. Đối với
trọng số còn lạiW0
i;j=Wi;j, có nghĩa là xu hướng
chuyển động@L
@W0
i;jtăng giá trị tuyệt đối của
Wi;j. Đối với trọng số được loại bỏ W0
i;j= 0, có nghĩa là
xu hướng chuyển động khuyến khích 0gần với Wi;j.
4.1 Hàm Che mặt nạ
Để có được mặt nạ Mdựa trên S, chúng tôi xem xét hai
hàm che mặt nạ theo cấu trúc cắt tỉa:
cục bộ và toàn cục.
Đối với hàm che mặt nạ cục bộ, chúng tôi đơn giản áp dụng
hàm Topvcho mỗi ma trận: M=Topv(S),
chọn v%trọng số quan trọng nhất theo
ma trận Stheo từng ma trận.
Đối với hàm che mặt nạ toàn cục, xếp hạng tất cả
điểm quan trọng cùng nhau (khoảng 85M trong BERT
base) không hiệu quả về mặt tính toán, thậm chí
làm hại hiệu suất cuối cùng trong phần 6.1. Vì vậy,
chúng tôi đề xuất một hàm che mặt nạ toàn cục mới
gán mức độ thưa dựa trên điểm tổng thể của
mỗi ma trận trọng số. Xem xét kiến trúc của
BERT, có Llớp transformer, mỗi lớp
chứa một lớp tự chú ý và một lớp feed-forward.
Trong khối tự chú ý lthl, Wl
Q,Wl
K,Wl
V,
vàWl
Olà các ma trận trọng số chúng tôi cần cắt tỉa.
Tương tự, Wl
UvàWl
Dlà các ma trận được
cắt tỉa trong lớp feed-forward lth. Chúng tôi trước tiên
tính mức độ thưa của mỗi ma trận trọng số
thay vì xếp hạng tất cả tham số của mạng. Mức độ thưa của mỗi ma trận trọng số vl
()được
tính như sau:
vl
()=R
Sl
()
L
PL
l0=1R
Sl0
()v (4)
trong đóR(S) =P
i;j(Si;j)là thuật ngữ chính quy
của Svới sigmoid ,Sl
()là điểm quan trọng
của trọng số Wl
(), và ()có thể là một trong
fQ;K;V;O;U;Dg. Mức độ thưa được xác định
bởi tỷ lệ điểm quan trọng với cùng loại ma trận
trong các lớp khác nhau.
4.2 Đầu Đặc trưng Nhiệm vụ
Thay vì huấn luyện đầu đặc trưng nhiệm vụ từ đầu,
chúng tôi khởi tạo nó từ embedding token BERT
và giữ nó đóng băng trong quá trình huấn luyện. Được
truyền cảm hứng bởi các phương pháp prompt tuning hiện tại,
chúng tôi khởi tạo đầu đặc trưng nhiệm vụ theo
embedding token BERT của các từ nhãn tương ứng
theo (Gao et al., 2021). Ví dụ, chúng tôi sử dụng
embedding token của "great" và "terrible" để khởi tạo
đầu phân loại trong SST2, và điểm nhãn tích cực
dự đoán là h[CLS]eT
great , trong đóh[CLS] là
trạng thái ẩn cuối cùng của token đặc biệt [CLS]
vàegreat là embedding token của "great".
4.3 Mục tiêu Huấn luyện
Để cắt tỉa mô hình, chúng tôi sử dụng lập lịch độ thưa
khối (Michael H. Zhu, 2018) không có bước
khởi động. Độ thưa vttại bước t là:
vt=(
vfvf
1t
N3t<N
vf o.w.(5)
chúng tôi tăng dần độ thưa từ 0 đến độ thưa mục tiêu
vftrong Nbước đầu tiên. Sau Nbước, chúng tôi
giữ độ thưa vt=vf. Trong giai đoạn này, số lượng
trọng số còn lại vẫn như cũ, nhưng những trọng số này
cũng có thể được thay thế bằng các trọng số đã loại bỏ
theo điểm quan trọng.
Chúng tôi đánh giá phương pháp của mình có và không có
chưng cất kiến thức. Đối với các cài đặt không có
chưng cất kiến thức, chúng tôi tối ưu hóa hàm mất mát
sau:
L=LCE+Rvt
vfR(S) (6)
trong đóLCElà mất mát phân loại tương ứng
với nhiệm vụ và R(S)là thuật ngữ chính quy

--- TRANG 4 ---
với siêu tham số R. Được truyền cảm hứng bởi soft-
movement (Sanh et al., 2020), nó sử dụng một thuật ngữ
chính quy để giảm Sđể tăng độ thưa với
hàm che mặt nạ ngưỡng. Chúng tôi thấy thuật ngữ
chính quy cũng quan trọng trong phương pháp của chúng tôi.
Vì Rlớn đủ trong phương pháp của chúng tôi, các điểm
quan trọng nhất trong Snhỏ hơn không khi
mức độ thưa hiện tại vtgần vfvà . Do gradient@R(S)
@Si;j=@(Si;j)
@Si;jtăng với sự
tăng của Si;jkhi Si;j<0, điểm tương ứng với
trọng số còn lại sẽ có hình phạt lớn hơn
trọng số đã loại bỏ. Nó khuyến khích Mthay đổi
khi vtgần đạt hoặc đạt vf.
Đối với các cài đặt có chưng cất kiến thức, chúng tôi
đơn giản thêm mất mát chưng cất LKDvàoLtheo
(Sanh et al., 2020; Xu et al., 2022):
LKD=DKL(pskpt) (7)
trong đóDKLlà phân kỳ KL. psvàpt
là phân phối đầu ra của mô hình học sinh và
mô hình giáo viên.
5 Thí nghiệm
5.1 Tập dữ liệu
Để cho thấy hiệu quả của phương pháp, chúng tôi sử dụng
ba tiêu chuẩn phổ biến: suy luận ngôn ngữ tự nhiên
(MNLI) (Williams et al., 2018), tương tự câu hỏi
(QQP) (Aghaebrahimian, 2017) và trả lời câu hỏi
(SQuAD) (Rajpurkar et al., 2016) theo
Sanh et al. Hơn nữa, chúng tôi cũng sử dụng tiêu chuẩn
GLUE (Wang et al., 2019) để xác thực hiệu suất
của phương pháp ở độ thưa thấp.
5.2 Thiết lập Thí nghiệm
Theo các phương pháp cắt tỉa trước đây, chúng tôi sử dụng
bert-base-uncasedđể thực hiện
cắt tỉa đặc trưng nhiệm vụ và báo cáo tỷ lệ trọng số
còn lại trong bộ mã hóa. Đối với đầu đặc trưng nhiệm vụ,
chúng tôi khởi tạo nó theo các từ nhãn của mỗi nhiệm vụ
theo (Gao et al., 2021). Đối với SQuAD, chúng tôi sử dụng
embedding token "yes" và "no" làm trọng số cho
việc bắt đầu và kết thúc phân loại câu trả lời. Chúng tôi
đóng băng tất cả trọng số của BERT bao gồm đầu
đặc trưng nhiệm vụ và chỉ tinh chỉnh mặt nạ. Bộ tối ưu
là Adam với tốc độ học 2e-2. Siêu tham số
Rcủa thuật ngữ chính quy là 400. Chúng tôi
đặt 12 epoch cho MNLI và QQP, và 10 epoch
cho SQuAD với kích thước lô 64. Đối với các nhiệm vụ
ở độ thưa thấp (hơn 70% trọng số còn lại), chúng tôi
đặt Ntrong lập lịch độ thưa khối thành 7 epoch. Đối với
các nhiệm vụ ở độ thưa cao, chúng tôi đặt Nthành 3500 bước. Chúng tôi cũng báo cáo hiệu suất của
bert-base-uncasedvàroberta-base
với 80% trọng số còn lại cho tất cả nhiệm vụ trên
GLUE với cùng kích thước lô và tốc độ học
như trên. Đối với lập lịch độ thưa, chúng tôi sử dụng
cùng lập lịch cho bert-base-uncasedvà
lập lịch tuyến tính cho roberta-base .N
trong lập lịch độ thưa là 3500. Đối với các nhiệm vụ lớn:
MNLI, QQP, SST2 và QNLI, chúng tôi sử dụng 12 epoch.
Đối với các nhiệm vụ nhỏ: MRPC, RTE, STS-B và
COLA, chúng tôi sử dụng 60 epoch. Lưu ý rằng các epoch
trên đã bao gồm các bước cắt tỉa. Ví dụ,
chúng tôi sử dụng khoảng 43 epoch để đạt độ thưa mục tiêu
trong MRPC. Chúng tôi tìm kiếm cấu trúc cắt tỉa từ
cục bộ và toàn cục.
5.3 Đường cơ sở
Chúng tôi so sánh phương pháp của mình với cắt tỉa theo
độ lớn (Han et al., 2015), chính quy L 0 (Louizos
et al., 2018), cắt tỉa theo chuyển động (Sanh et al., 2020)
và CAP (Xu et al., 2022). Chúng tôi cũng so sánh phương pháp
của mình với tinh chỉnh trực tiếp và super tick-
ets (Liang et al., 2021) trên GLUE. Đối với super tick-
ets, nó phát hiện rằng PLM chứa một số mạng con,
có thể vượt trội hơn mô hình đầy đủ bằng cách tinh chỉnh
chúng.
5.4 Kết quả Thí nghiệm
Bảng 1 cho thấy kết quả của SMP và các phương pháp
cắt tỉa khác ở độ thưa cao. Chúng tôi triển khai SMP
với hàm che mặt nạ cục bộ (SMP-L) và hàm che mặt nạ
đề xuất của chúng tôi (SMP-S).
SMP-S và SMP-L liên tục đạt hiệu suất tốt hơn
các phương pháp cắt tỉa khác không có chưng cất kiến thức.
Mặc dù cắt tỉa theo chuyển động và SMP-L sử dụng
cùng hàm che mặt nạ cục bộ, SMP-L có thể đạt được
cải thiện hơn 2.0 trên tất cả nhiệm vụ và mức độ thưa
trong Bảng 1. Hơn nữa, lợi ích càng đáng kể hơn
ở 3% trọng số còn lại. Đối với cắt tỉa soft-movement,
gán trọng số còn lại của ma trận không đều như SMP-S,
thậm chí còn kém hơn SMP-L.
Theo các công trình trước đây, chúng tôi cũng báo cáo
kết quả với chưng cất kiến thức trong Bảng 1. Cải thiện
do chưng cất kiến thức mang lại cũng rõ ràng trong SMP-L
và SMP-S. Ví dụ, nó cải thiện F1 của SQuAD 3.3 và 4.1
cho SMP-L và SMP-S. Chỉ với 3% trọng số còn lại,
SMP-S thậm chí vượt trội hơn cắt tỉa soft-movement
ở 10% trong MNLI và QQP. So với CAP,
thêm mục tiêu học tương phản từ

--- TRANG 5 ---
Phương pháp Trọng số còn lại Tham số mới mỗi nhiệm vụ Tham số có thể huấn luyện MNLI QQP SQuAD
MACC=MM ACC ACC=F1 EM =F1
BERT base 100% 110M 110M 84.5/84.9 91.4/88.4 80.4/88.1
Không có Chưng cất Kiến thức
Movement (Sanh et al., 2020) 10% 8.5M + M 170M 79.3/79.5 89.1/85.5 71.9/81.7
Soft-Movement (Sanh et al., 2020) 10% 8.5M + M 170M 80.7/81.1 90.5/87.1 71.3/81.5
SMP-L (Của chúng tôi) 10% M 85M 82.0/82.3 90.8/87.7 75.0/84.3
SMP-S (Của chúng tôi) 10% M 85M 82.5/82.3 90.8/87.6 75.1/84.6
Movement (Sanh et al., 2020) 3% 2.6M+ M 170M 76.1/76.7 85.6/81.0 65.2/76.3
Soft-Movement (Sanh et al., 2020) 3% 2.6M+ M 170M 79.0/79.6 89.3/85.6 69.5/79.9
SMP-L (Của chúng tôi) 3% M 85M 80.6/81.0 90.2/87.0 70.7/81.0
SMP-S (Của chúng tôi) 3% M 85M 80.9/81.1 90.3/87.1 70.9/81.4
Có Chưng cất Kiến thức
Movement (Sanh et al., 2020) 50% 42.5M+ M 170M 82.5/82.9 91.0/87.8 79.8/87.6
CAP (Xu et al., 2022) 50% 42.5M+ M 170M 83.8/84.2 91.6/88.6 80.9/88.2
SMP-L (Của chúng tôi) 50% M 85M 85.3/ 85.6 91.6/88.7 82.2/89.4
SMP-S (Của chúng tôi) 50% M 85M 85.7/85.5 91.7/88.8 82.8/89.8
Magnitude (Han et al., 2015) 10% 8.5M+ M 85M 78.3/79.3 79.8/75.9 70.2/80.1
L0-regularization (Louizos et al., 2018) 10% 8.5M+ M 170M 78.7/79.7 88.1/82.8 72.4/81.9
Movement (Sanh et al., 2020) 10% 8.5M+ M 170M 80.1/80.4 89.7/86.2 75.6/84.3
Soft-Movement (Sanh et al., 2020) 10% 8.5M+ M 170M 81.2/81.8 90.2/86.8 76.6/84.9
CAP (Xu et al., 2022) 10% 8.5M+ M 170M 82.0/82.9 90.7/87.4 77.1/85.6
SMP-L (Của chúng tôi) 10% M 85M 83.1/83.1 91.0/87.9 78.9/86.9
SMP-S (Của chúng tôi) 10% M 85M 83.7/83.6 91.0/87.9 79.3/87.2
Movement (Sanh et al., 2020) 3% 2.6M+ M 170M 76.5/77.4 86.1/81.5 67.5/78.0
Soft-Movement (Sanh et al., 2020) 3% 2.6M+ M 170M 79.5/80.1 89.1/85.5 72.7/82.3
CAP (Xu et al., 2022) 3% 2.6M+ M 170M 80.1/81.3 90.2/86.7 73.8/83.0
SMP-L (Của chúng tôi) 3% M 85M 80.8/81.2 90.1/87.0 74.0/83.4
SMP-S (Của chúng tôi) 3% M 85M 81.8/82.0 90.5/87.4 75.0/84.1
Bảng 1: Hiệu suất ở độ thưa cao. SMP-L và SMP-S tham chiếu đến phương pháp của chúng tôi với hàm che mặt nạ cục bộ
và hàm che mặt nạ của chúng tôi. Mlà kích thước của mặt nạ nhị phân M, khoảng 2.7M tham số và có thể được nén
thêm.1Vì các phương pháp cắt tỉa khác đóng băng các module embedding của BERT (Sanh et al., 2020), tham số có thể huấn luyện
của các phương pháp bậc nhất là tổng của bộ mã hóa BERT (85M), điểm quan trọng S(85M) và đầu
đặc trưng nhiệm vụ (ít hơn 0.01M). Đối với các phương pháp cắt tỉa bậc không như cắt tỉa theo độ lớn, tham số có thể huấn luyện
là 85M, loại trừ S. Kết quả của chúng tôi được tính trung bình từ năm hạt giống ngẫu nhiên.
mô hình giáo viên, phương pháp của chúng tôi liên tục mang lại
cải thiện đáng kể mà không có mục tiêu học phụ.
Đối với 50% trọng số còn lại, SMP-
S trong MNLI đạt độ chính xác 85.7 so với
84.5 với tinh chỉnh mô hình đầy đủ, trong khi nó giữ tất cả
trọng số của BERT không đổi.
Phương pháp của chúng tôi cũng hiệu quả tham số. So với
các phương pháp bậc nhất khác, chúng tôi có thể tiết kiệm
một nửa tham số có thể huấn luyện bằng cách giữ
toàn bộ BERT và đầu đặc trưng nhiệm vụ đóng băng. Đối với
tham số mới của mỗi nhiệm vụ, nó cũng là một yếu tố
quan trọng ảnh hưởng đến chi phí chuyển giao và lưu trữ
mạng con. Phương pháp của chúng tôi chỉ giới thiệu một mặt nạ
nhị phân Mlàm tham số mới cho mỗi nhiệm vụ ở
các mức độ thưa khác nhau, trong khi các phương pháp khác cần
lưu cả Mvà mạng con. Với trọng số còn lại
50%, 10%, và 3%, chúng tôi có thể tiết kiệm 42.5M,
8.5M, và 2.6M tham số tương ứng so với các phương pháp cắt tỉa khác.
Hình 1 cho thấy thêm kết quả từ 3% trọng số
còn lại đến 80% bằng cách so sánh phương pháp của chúng tôi với
các phương pháp bậc nhất: cắt tỉa theo chuyển động và cắt tỉa
soft-movement, và phương pháp cắt tỉa bậc không: cắt tỉa
theo độ lớn. Chúng tôi báo cáo kết quả của phương pháp
ở 3%, 10%, 30%, 50% và 80% trọng số còn lại. Các phương pháp
bậc nhất trước đây như cắt tỉa theo chuyển động kém hơn
cắt tỉa theo độ lớn ở trọng số còn lại hơn 25%
trong MNLI và SQuAD. Ngay cả ở mức độ thưa cao như
20% trọng số còn lại, cắt tỉa theo độ lớn vẫn mạnh mẽ
vượt trội hơn cả cắt tỉa theo chuyển động và cắt tỉa
soft-movement trong Hình 1
1Ví dụ ở 3% trọng số còn lại, chúng tôi có thể giảm
kích thước của Mxuống khoảng 20% kích thước ban đầu
thông qua nén. Điều này có nghĩa là chỉ khoảng 0.55M
tham số mới được giới thiệu ở 3% trọng số còn lại.
Ngoài ra, Mnén có thể được tìm thấy tại https:
//github.com/kongds/SMP/releases .

--- TRANG 6 ---
(a) MNLI
 (b) QQP
 (c) SQuAD
(d) MNLI + KD
 (e) QQP + KD
 (f) SQuAD + KD
Hình 1: So sánh các phương pháp cắt tỉa khác nhau từ 3% trọng số còn lại đến 80%. Đường đứt nét màu đen
trong các hình chỉ ra kết quả của BERT được tinh chỉnh. SMvP, MvP và MaP tham chiếu đến cắt tỉa soft-movement,
cắt tỉa theo chuyển động và cắt tỉa theo độ lớn, tương ứng. KD đại diện cho kết quả với chưng cất kiến thức.
Chúng tôi báo cáo kết quả của phương pháp ở 3%, 10%, 30%, 50%, 70%, và 80% trọng số còn lại. Phương pháp của chúng tôi
liên tục vượt trội hơn các phương pháp khác từ độ thưa thấp đến cao.
Trọng số còn lại Tham số mới mỗi nhiệm vụ MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
MACC ACC ACC MCC ACC ACC ACC P Corr Trung bình
BERT 100% 110M 84.5 92.9 87.7 58.1 92.0 91.4 71.1 91.2 83.6
SuperT 86.8% 98M + M 84.5 93.4 86.2 58.8 91.3 91.3 72.5 89.8 83.5
SMP (Của chúng tôi) 80% M 85.0 92.9 87.0 61.5 91.5 91.4 72.3 89.6 83.9
RoBERTa 100% 125M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4
SMP (Của chúng tôi) 80% M 87.6 94.9 89.9 65.4 92.8 91.9 81.5 91.1 86.9
Bảng 2: Hiệu suất trên tập phát triển GLUE. Kết quả của chúng tôi được tính trung bình từ năm hạt giống ngẫu nhiên. Kết quả
của SuperT từ (Liang et al., 2021), và trọng số còn lại và tham số mới mỗi nhiệm vụ trong SuperT được
tính trung bình trên tất cả nhiệm vụ. Lưu ý tất cả kết quả từ cài đặt không có chưng cất kiến thức để so sánh công bằng.
(c). Điều này cho thấy hạn chế của các phương pháp
bậc nhất hiện tại chỉ hoạt động lý tưởng ở độ thưa
rất cao so với các phương pháp cắt tỉa bậc không.
Tuy nhiên, SMP-L và SMP-S như các phương pháp bậc nhất
có thể liên tục cho thấy hiệu suất tốt hơn cắt tỉa
theo độ lớn ở độ thưa thấp. Đối với kết quả không có
chưng cất kiến thức, SMP-S và SMP-L đạt hiệu suất
tương tự của cắt tỉa soft-movement với ít trọng số
còn lại hơn nhiều. Xem xét LTH trước đây trong BERT,
chúng tôi thấy SMP-S có thể vượt trội hơn tinh chỉnh
mô hình đầy đủ ở một tỷ lệ nhất định của trọng số còn lại
trong Hình 1 (a), (b) và (c), cho thấy rằng BERT chứa
một số mạng con vượt trội hơn hiệu suất ban đầu mà không cần
tinh chỉnh. Đối với kết quả có chưng cất kiến thức, SMP-S và SMP-L hưởng lợi từ chưng cất
kiến thức ở tất cả mức độ thưa. Sau khi loại bỏ
ngay cả 70% trọng số từ bộ mã hóa, phương pháp của chúng tôi
vẫn mạnh mẽ vượt trội hơn tinh chỉnh mô hình đầy đủ.
Chúng tôi cũng xác thực phương pháp trên GLUE và báo cáo
kết quả ở 80% trọng số còn lại trong Bảng 2. So với
tinh chỉnh mô hình đầy đủ, phương pháp của chúng tôi đạt hiệu suất
tốt hơn trên hai PLM bằng cách chỉ loại bỏ 20% tham số
trong bộ mã hóa trong khi giữ các tham số còn lại không đổi.
So với SuperT, tìm kiếm 8 mức độ thưa khác nhau
cho mỗi nhiệm vụ, phương pháp của chúng tôi đạt hiệu suất
tốt hơn bằng cách sử dụng cùng mức độ thưa. Ngoài ra,
phương pháp của chúng tôi cũng tiết kiệm hơn 98M tham số mới
mỗi nhiệm vụ so với SuperT.

--- TRANG 7 ---
Hàm che mặt nạ MNLI SQuAD
80% 10% 3% 80% 10% 3%
T  (S()l)> N/A N/A N/A N/A N/A N/A
G S()lSv85.0 81.0 80.1 88.2 83.1 79.3
L Topv(S()l) 84.8 82.0 80.6 88.0 84.3 81.0
STopvl
()(S()l)85.0 82.5 80.9 88.3 84.6 81.4
Bảng 3: Ảnh hưởng của các hàm che mặt nạ khác nhau. Chúng tôi
báo cáo kết quả trong MNLI và SQuAD với 80%,
10% và 3% trọng số còn lại. N/A có nghĩa là phương pháp
của chúng tôi với hàm che mặt nạ tương ứng không thể
hội tụ trong cài đặt của chúng tôi. Hàm che mặt nạ là để chuyển đổi
S()thành mặt nạ nhị phân Ml
()củaWl
().Ttham chiếu
đến hàm che mặt nạ ngưỡng theo (Sanh
et al., 2020), và là ngưỡng. GvàLlà hàm che mặt nạ
toàn cục và cục bộ, và Svlà giá trị nhỏ nhất
trong v% hàng đầu sau khi sắp xếp tất cả Scùng nhau.S
tham chiếu đến hàm che mặt nạ đề xuất của chúng tôi, và vl
()từ
Phương trình 4.
6 Phân tích
6.1 Hàm Che mặt nạ
Trong phần này, chúng tôi thảo luận về ảnh hưởng của
các hàm che mặt nạ khác nhau. Bảng 3 cho thấy kết quả
của các hàm che mặt nạ khác nhau trên phương pháp của chúng tôi
không có chưng cất kiến thức. Trái ngược với các phương pháp
cắt tỉa trước đây, hàm che mặt nạ ngưỡng Tkhông thể
hội tụ trong phương pháp của chúng tôi do khó khăn
trong việc kiểm soát độ thưa trong quá trình huấn luyện.
Đối với hàm che mặt nạ toàn cục G, chúng tôi sắp xếp tất cả
85M trọng số bộ mã hóa BERT và giữ lại v% trọng số
hàng đầu trong mỗi bước huấn luyện. So với hàm che mặt nạ
cục bộ L,Gmất hơn gấp đôi thời gian huấn luyện
do chi phí tính toán của việc sắp xếp 85M trọng số.
Mặc dù mất thời gian lâu nhất để huấn luyện, nó vẫn
kém hơn Lở 10% và 3% trọng số còn lại. Trái ngược
với G, hàm che mặt nạ đề xuất Scủa chúng tôi vượt trội
hơn Lmà không cần thời gian huấn luyện bổ sung vì S
trực tiếp gán trọng số còn lại của mỗi ma trận. Thêm
kết quả của hàm che mặt nạ SvàLcũng có sẵn trong
Bảng 1 và Hình 1.
Hình 2 hiển thị phân phối trọng số còn lại
trong các lớp khác nhau trong MNLI với 10% trọng số
còn lại. Chúng tôi thấy Ggán quá nhiều trọng số
còn lại cho WUvàWV, gấp bốn lần
các ma trận khác. Nó khiến các ma trận trọng số khác
như WQthưa hơn SvàL. Theo các nghiên cứu
trước đây (Sanh et al., 2020; Mallya and Lazebnik, 2018),
chúng tôi cũng
(a)WQ
 (b)WK
(c)WV
 (d)WO
(e)WU
 (f)WD
(g) Tổng thể
Hình 2: Phân phối trọng số còn lại tương ứng
với mỗi lớp. Tổng thể tham chiếu đến trọng số còn lại
tổng thể của mỗi lớp. W()là trọng số
còn lại cho mỗi ma trận trọng số trong bộ mã hóa BERT.
L,GvàStrong các hình tham chiếu đến các hàm che mặt nạ
theo Bảng 3.
thấy rằng độ thưa tổng thể có xu hướng tăng theo
độ sâu của lớp. Tuy nhiên, chỉ WUvàWV
theo mẫu này trong tất cả ba ma trận. Vì WU
vàWVchiếm hơn 60% trọng số trong
mỗi lớp, nó khiến phân phối tổng thể của mỗi
lớp cũng theo xu hướng của chúng.
Để hiểu hành vi của các đầu chú ý, chúng tôi
cũng hiển thị tỷ lệ trọng số còn lại của mỗi
đầu trong Hình 3. Mỗi hàng đại diện cho một ma trận
chứa 12 đầu. Do hạn chế không gian và
phân phối tương tự giữa WQvàWK, chúng tôi
chỉ hiển thị WQvàWV. Thay vì gán độ thưa
đều cho mỗi đầu, độ thưa của mỗi
đầu không đều trong ba hàm che mặt nạ,

--- TRANG 8 ---
Hình 3: Tỷ lệ trọng số còn lại mỗi đầu chú ý của
WQvàWVtrong MNLI với 10% trọng số còn lại.
Mỗi ô tham chiếu đến tỷ lệ trọng số còn lại của
đầu chú ý tương ứng. Màu càng tối, tỷ lệ
trọng số còn lại càng cao. L,GvàStrong các hình
tham chiếu đến các hàm che mặt nạ theo Bảng 3.
với hầu hết các đầu chỉ có dưới 1% hoặc ít hơn
trọng số còn lại. Hơn nữa, ba hàm che mặt nạ
cho thấy mẫu tương tự ngay cả với các cách
khác nhau gán trọng số còn lại. Đối với hàm che mặt nạ
Scủa chúng tôi,Scó thể gán nhiều trọng số
còn lại hơn cho các đầu quan trọng so với L, và
một số đầu trong WQđạt hơn 60% trọng số
còn lại ở lớp thứ 9. Đối với hàm che mặt nạ toàn cục
G, do hầu hết trọng số còn lại được gán cho
WUvàWD, tỷ lệ trọng số còn lại trung bình
của WQvàWVtrong Gchỉ là 3.2%
và 2.8%, khiến Gkém hơn các
hàm che mặt nạ khác.
6.2 Đầu Đặc trưng Nhiệm vụ
Để xác thực hiệu quả của phương pháp khởi tạo
đầu đặc trưng nhiệm vụ, chúng tôi so sánh nó với huấn luyện từ đầu.
MNLI SQuAD
80% 10% 3% 80% 10% 3%
Từ đầu 84.6 81.7 80.5 87.5 84.2 80.7
Khởi tạo 84.8 82.0 80.6 88.0 84.3 81.0
Bảng 4: Ảnh hưởng của các phương pháp đầu đặc trưng
nhiệm vụ khác nhau. "Từ đầu" tham chiếu đến huấn luyện đầu
từ đầu theo các phương pháp cắt tỉa trước đây. "Khởi tạo"
tham chiếu đến phương pháp khởi tạo của chúng tôi.
Bảng 4 cho thấy kết quả của SMP-L trong MNLI và
SQuAD với 80%, 10% và 3% trọng số còn lại.
Đối với huấn luyện từ đầu, chúng tôi khởi tạo ngẫu nhiên
đầu và tinh chỉnh nó với tốc độ học 3e-
5 theo các phương pháp cắt tỉa trước đây. Kết quả
cho thấy phương pháp của chúng tôi đạt hiệu suất tốt hơn với
đầu đặc trưng nhiệm vụ đóng băng.
6.3 Mục tiêu Huấn luyện
Thuật ngữ chính quy trong mục tiêu huấn luyện là yếu tố
quan trọng cho phương pháp của chúng tôi. Chúng tôi thấy rằng
phương pháp của chúng tôi khó hội tụ ở độ thưa cao
không có thuật ngữ chính quy Rtrong Bảng 5. Với sự
tăng của độ thưa, khoảng cách hiệu suất giữa có và
không có Rtăng mạnh. SMP-L không có R
thậm chí không thể hội tụ ở 10% và 3% trọng số
còn lại trong SQuAD.
MNLI SQuAD
80% 10% 3% 80% 10% 3%
SMP-L 84.8 82.0 80.6 88.0 84.3 81.0
w/oR 84.2 80.1 69.2 86.6 N/A N/A
Bảng 5: Ảnh hưởng của thuật ngữ chính quy. Rtham chiếu
đến thuật ngữ chính quy. N/A tham chiếu đến không thể
hội tụ.
Như đã phân tích trong phần 4.3, chúng tôi thấy trọng số
còn lại trong các đầu chú ý đều hơn không có R.
Ví dụ, độ lệch chuẩn của trọng số còn lại trong
mỗi đầu chú ý là 3.75 so với 12.4 trong SMP-L
với Rtrong MNLI với 10% trọng số còn lại. Nói cách
khác, không có R, nó không thể gán nhiều trọng số
còn lại hơn cho các đầu quan trọng như trong Hình 3.
7 Kết luận
Trong bài báo này, chúng tôi đề xuất một phương pháp
cắt tỉa đặc trưng nhiệm vụ đơn giản nhưng hiệu quả
gọi là Cắt tỉa Mô hình Tĩnh (SMP). Xem xét các phương pháp
trước đây, thực hiện cả cắt tỉa và tinh chỉnh để

--- TRANG 9 ---
thích nghi PLM với các nhiệm vụ hạ nguồn, chúng tôi thấy
tinh chỉnh có thể thừa vì cắt tỉa bậc nhất đã
hội tụ PLM. Dựa trên điều này, phương pháp của chúng tôi
tập trung vào việc sử dụng cắt tỉa bậc nhất để thay thế
tinh chỉnh. Không có tinh chỉnh, phương pháp của chúng tôi mạnh mẽ
vượt trội hơn các phương pháp bậc nhất khác. Các thí nghiệm
rộng rãi cũng cho thấy rằng phương pháp của chúng tôi đạt
hiệu suất tiên tiến ở nhiều độ thưa khác nhau.
Đối với giả thuyết vé số trúng thưởng trong BERT, chúng tôi
thấy nó chứa các mạng con độ thưa đạt hiệu suất
ban đầu mà không cần huấn luyện chúng, và những
mạng con này ở 80% trọng số còn lại thậm chí vượt trội
hơn BERT được tinh chỉnh trên GLUE.
8 Hạn chế
Giống như tất cả các phương pháp cắt tỉa không có cấu trúc,
SMP khó đạt được tăng tốc suy luận so với
các phương pháp cắt tỉa có cấu trúc. Vì SMP cắt tỉa
mô hình mà không có tinh chỉnh, điều này cũng hạn chế
việc mở rộng SMP cho các phương pháp cắt tỉa có cấu trúc.
Tuy nhiên, chúng tôi thấy rằng hầu hết các hàng của
ma trận độ thưa trong SMP được cắt tỉa hoàn toàn
ở mức độ thưa cao. Điều này cho phép chúng tôi trực tiếp
nén kích thước của ma trận, dẫn đến suy luận nhanh hơn.
Ví dụ, mô hình 3% trọng số còn lại của MNLI
có thể được nén xuống 47.43% kích thước thực tế của mô hình
(dẫn đến khoảng 1.37 lần tăng tốc suy luận) mà không cần
huấn luyện lại hoặc mất hiệu suất. Bằng cách loại bỏ
các hàng của ma trận chứa ít hơn 10 trọng số
còn lại, chúng tôi có thể nén thêm xuống 25.19%
kích thước thực tế (1.76 lần tăng tốc suy luận) với
0.9 sụt giảm độ chính xác. Chúng tôi hy vọng rằng một
hàm mất mát được thiết kế cẩn thận trong quá trình huấn luyện
có thể dẫn đến kích thước mô hình thực tế nhỏ hơn và
tăng tốc suy luận nhanh hơn, điều mà chúng tôi để lại
trong tương lai.
9 Lời cảm ơn
Công trình nghiên cứu được hỗ trợ bởi Chương trình
Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc
theo Số hiệu 2021ZD0113602, Quỹ Khoa học Tự nhiên
Quốc gia Trung Quốc theo Số hiệu 62276015, 62176014,
Quỹ Nghiên cứu Cơ bản cho các Trường Đại học Trung ương.
Tài liệu tham khảo
Ahmad Aghaebrahimian. 2017. Quora question an-
swer dataset. In International Conference on Text,
Speech, and Dialogue , pages 66–73. Springer.
Yoshua Bengio, Nicholas Léonard, and Aaron
Courville. 2013. Estimating or propagating gradi-ents through stochastic neurons for conditional com-
putation. arXiv preprint arXiv:1308.3432 .
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Si-
jia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. 2020. The lottery ticket hypoth-
esis for pre-trained BERT networks. Advances
in Neural Information Processing Systems , 2020-
December(NeurIPS):1–13.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Jonathan Frankle and Michael Carbin. 2018. The lot-
tery ticket hypothesis: Finding sparse, trainable neu-
ral networks. arXiv preprint arXiv:1803.03635 .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. ACL-IJCNLP 2021 - 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing, Proceedings of the Con-
ference , pages 3816–3830.
Song Han, Jeff Pool, John Tran, and William Dally.
2015. Learning both weights and connections for
efﬁcient neural network. In Advances in Neural In-
formation Processing Systems (NeurIPS) .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2020. TinyBERT: Distilling BERT for natural lan-
guage understanding. In Findings of the Association
for Computational Linguistics: EMNLP 2020 .
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. Albert: A lite bert for self-supervised learning
of language representations. In International Con-
ference on Learning Representations (ICLR) .
Chen Liang, Simiao Zuo, Minshuo Chen, Haoming
Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and
Weizhu Chen. 2021. Super tickets in pre-trained lan-
guage models: From model compression to improv-
ing generalization. ACL-IJCNLP 2021 - 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing, Proceedings
of the Conference , (Figure 1):6524–6538.
Christos Louizos, Max Welling, and Diederik P
Kingma. 2017. Learning sparse neural net-
works through l_0regularization. arXiv preprint
arXiv:1712.01312 .

--- TRANG 10 ---
Christos Louizos, Max Welling, and Diederik P.
Kingma. 2018. Learning sparse neural networks
through l 0regularization. In International Confer-
ence on Learning Representations (ICLR) .
Arun Mallya and Svetlana Lazebnik. 2018. Piggyback:
Adding multiple tasks to a single, ﬁxed network by
learning to mask. ArXiv , abs/1801.06519.
Suyog Gupta Michael H. Zhu. 2018. To prune, or not to
prune: Exploring the efﬁcacy of pruning for model
compression. In International Conference on Learn-
ing Representations (ICLR) .
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo
Aila, and Jan Kautz. 2017. Pruning convolutional
neural networks for resource efﬁcient inference. In
International Conference on Learning Representa-
tions (ICLR) .
Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua
Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xi-
anglong Liu. 2022. BiBERT: Accurate Fully Bina-
rized BERT. arXiv preprint arXiv , pages 1–24.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In EMNLP .
Victor Sanh, Thomas Wolf, and Alexander M. Rush.
2020. Movement pruning: Adaptive sparsity by ﬁne-
tuning. Advances in Neural Information Processing
Systems , 2020-Decem(NeurIPS):1–14.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W. Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low pre-
cision quantization of bert. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence (AAAI) .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Inter-
national Conference on Learning Representations
(ICLR) .
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Advances in Neural
Information Processing Systems (NeurIPS) .
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In NAACL .
Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.
Structured pruning learns compact and accurate
models. arXiv preprint arXiv:2204.00408 .
Runxin Xu, Fuli Luo, Chengyu Wang, Baobao Chang,
Jun Huang, Songfang Huang, and Fei Huang. 2022.
From dense to sparse: Contrastive pruning for bet-
ter pre-trained language model compression. In
Thirty-Sixth AAAI Conference on Artiﬁcial Intelli-
gence (AAAI) .A Độ lệch chuẩn của Nhiệm vụ
Chúng tôi cũng báo cáo độ lệch chuẩn của các nhiệm vụ
từ 5 lần chạy ngẫu nhiên trong Bảng 6 và 7.
có KD không có KD
50% 10% 3% 10% 3%
MNLI SMP-L 0.17 0.26 0.19 0.27 0.20
MACCstd:SMP-S 0.13 0.24 0.30 0.25 0.28
QQP SMP-L 0.04 0.01 0.08 0.06 0.01
ACC std:SMP-S 0.02 0.03 0.02 0.01 0.02
SQuAD SMP-L 0.17 0.09 0.03 0.36 0.01
F1 std: SMP-S 0.10 0.07 0.02 0.42 0.07
Bảng 6: Độ lệch chuẩn của Bảng 1
SMP(BERT) SMP(RoBERTa)
MNLI 0.15 0.12
QNLI 0.15 0.11
QQP 0.03 0.14
SST2 0.36 0.28
MRPC 1.21 0.44
COLA 0.69 0.65
STSB 0.14 0.16
RTE 1.59 0.74
Bảng 7: Độ lệch chuẩn của Bảng 2
