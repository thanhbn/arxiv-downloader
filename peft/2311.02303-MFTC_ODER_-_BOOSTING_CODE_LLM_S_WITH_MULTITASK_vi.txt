# 2311.02303.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2311.02303.pdf
# Kích thước tệp: 1084615 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MFTCODER: TĂNG CƯỜNG CODE LLM VỚI TINH CHỈNH ĐA NHIỆM VỤ
BẢN THẢO

Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang,
Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu∗, Jianguo Li∗
Ant Group, Trung Quốc
7 tháng 11, 2023

TÓM TẮT
Code LLM đã xuất hiện như một lĩnh vực nghiên cứu chuyên biệt, với các nghiên cứu đáng chú ý dành riêng cho việc nâng cao khả năng lập trình của mô hình thông qua tinh chỉnh trên các mô hình được huấn luyện trước. Các phương pháp tinh chỉnh trước đây thường được thiết kế riêng cho các nhiệm vụ hoặc kịch bản phía dưới cụ thể, có nghĩa là tinh chỉnh riêng biệt cho từng nhiệm vụ, đòi hỏi tài nguyên đào tạo rộng lớn và đặt ra thách thức về triển khai và bảo trì. Hơn nữa, các phương pháp này không tận dụng được mối liên kết vốn có giữa các nhiệm vụ liên quan đến mã. Để khắc phục những hạn chế này, chúng tôi trình bày một khung tinh chỉnh đa nhiệm vụ, MFTCoder, cho phép tinh chỉnh đồng thời và song song trên nhiều nhiệm vụ. Bằng cách kết hợp các hàm mất mát khác nhau, chúng tôi giải quyết hiệu quả các thách thức phổ biến trong học đa nhiệm vụ, như mất cân bằng dữ liệu, mức độ khó khác nhau và tốc độ hội tụ không nhất quán. Các thí nghiệm rộng rãi đã chứng minh một cách thuyết phục rằng phương pháp tinh chỉnh đa nhiệm vụ của chúng tôi vượt trội hơn cả tinh chỉnh riêng lẻ trên từng nhiệm vụ và tinh chỉnh trên tập hợp hỗn hợp các nhiệm vụ. Hơn nữa, MFTCoder cung cấp khả năng đào tạo hiệu quả, bao gồm các chế độ token hóa dữ liệu hiệu quả và tinh chỉnh PEFT, dẫn đến cải thiện tốc độ đáng kể so với các phương pháp tinh chỉnh truyền thống. MFTCoder tích hợp liền mạch với một số LLM mã nguồn mở chính như CodeLLama và Qwen. Tận dụng nền tảng CodeLLama, mô hình tinh chỉnh MFTCoder của chúng tôi, CODEFUSE-CODELLAMA-34B, đạt được điểm pass@1 ấn tượng 74.4% trên benchmark HumanEval, vượt qua hiệu suất GPT-4 (67%, zero-shot). MFTCoder được mã nguồn mở tại https://github.com/codefuse-ai/MFTCOder

Từ khóa: Mô hình Ngôn ngữ Lớn · Sinh mã · Học đa nhiệm vụ

1 Giới thiệu
Sự xuất hiện thay đổi paradigm của ChatGPT², được hỗ trợ bởi cả GPT-3.5 và GPT-4 OpenAI (2023), đã châm ngòi cho bối cảnh nghiên cứu và phát triển trong lĩnh vực mô hình ngôn ngữ lớn (LLM). Đột phá này đã thúc đẩy thêm sự quan tâm đến việc tận dụng LLM để hiểu và sinh mã, thường được gọi là Code LLM. Bằng cách huấn luyện trước trên các nguồn dữ liệu mã rộng lớn như dữ liệu công khai Github, các Code LLM này có thể thu được các biểu diễn ngữ cảnh toàn diện có thể được áp dụng cho các nhiệm vụ liên quan đến mã khác nhau.

Trong khi giai đoạn huấn luyện trước của (Code) LLM tìm cách đảm bảo khả năng tổng quát hóa của chúng đối với các nhiệm vụ phía dưới khác nhau, giai đoạn tinh chỉnh tiếp theo thường chỉ thích ứng (Code) LLM với một nhiệm vụ hoặc kịch bản cụ thể. Tuy nhiên, phương pháp này bỏ qua hai thách thức quan trọng. Thứ nhất, nó liên quan đến việc tinh chỉnh riêng lẻ tốn kém tài nguyên của các LLM lớn cho từng nhiệm vụ, điều này cản trở việc triển khai hiệu quả trong sản xuất. Thứ hai, bản chất liên quan của các nhiệm vụ trong lĩnh vực mã gợi ý rằng tinh chỉnh chung có thể nâng cao hiệu suất so với tinh chỉnh riêng biệt. Do đó, việc thực hiện tinh chỉnh đa nhiệm vụ là bắt buộc, cho phép xử lý đồng thời tất cả các nhiệm vụ trong khi tận dụng điểm mạnh của các nhiệm vụ liên quan để nâng cao hiệu suất.

∗Tác giả liên hệ: {hyu.hugo, lijg.zero}@antgroup.com
²https://openai.com/blog/chatgpt

--- TRANG 2 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Như một ví dụ minh họa, giả sử chúng ta có hai nhiệm vụ liên quan: hoàn thành mã và tóm tắt mã. Hoàn thành mã liên quan đến việc dự đoán dòng mã tiếp theo dựa trên một đoạn mã một phần, trong khi tóm tắt mã nhằm tạo ra một tóm tắt súc tích có thể đọc được của con người cho một đoạn mã đã cho. Theo truyền thống, các mô hình riêng biệt sẽ được tinh chỉnh cho từng nhiệm vụ, dẫn đến sự trùng lặp tốn kém tài nguyên. Tuy nhiên, hoàn thành mã và tóm tắt mã có những kết nối vốn có. Việc hoàn thành một đoạn mã dựa vào việc hiểu chức năng và mục đích tổng thể, trong khi tạo ra một tóm tắt chính xác đòi hỏi hiểu biết về cấu trúc, phụ thuộc và chức năng dự định. Bằng cách sử dụng học đa nhiệm vụ, một mô hình duy nhất có thể được đào tạo để học cùng lúc cả hai nhiệm vụ, tận dụng kiến thức và mẫu chung, dẫn đến cải thiện hiệu suất trên cả hai nhiệm vụ. Mô hình hiểu các phụ thuộc ngữ cảnh giữa các yếu tố mã, hỗ trợ việc dự đoán đoạn tiếp theo và tạo ra các tóm tắt thông tin. Hơn nữa, học đa nhiệm vụ mang lại những lợi ích bổ sung ngoài hiệu suất nhiệm vụ riêng lẻ: biểu diễn chung giữa các nhiệm vụ giúp giảm thiểu overfitting, thúc đẩy tổng quát hóa tốt hơn và nâng cao khả năng xử lý khan hiếm dữ liệu của mô hình cho các nhiệm vụ cụ thể. Nếu hoàn thành mã có bộ dữ liệu đào tạo lớn hơn tóm tắt mã, mô hình có thể tận dụng sự phong phú của dữ liệu hoàn thành để nâng cao hiệu suất trong tóm tắt, giải quyết hiệu quả các thách thức khan hiếm dữ liệu. Học đa nhiệm vụ thậm chí cho phép mô hình xử lý các nhiệm vụ chưa thấy nhưng liên quan mà không cần dữ liệu đào tạo cụ thể. Nhìn chung, học đa nhiệm vụ cho phép các mô hình học cùng lúc nhiều nhiệm vụ liên quan, được hưởng lợi từ kiến thức chung, cải thiện hiệu suất, nâng cao tổng quát hóa và xử lý khan hiếm dữ liệu.

Bất chấp tầm quan trọng của học đa nhiệm vụ cho tinh chỉnh, chỉ có một số ít nghiên cứu hiện có đã khám phá phương pháp này trong lĩnh vực NLP Raffel et al. (2023); Aghajanyan et al. (2021); Aribandi et al. (2022). Các nghiên cứu này kết hợp dữ liệu đa nhiệm vụ và hợp nhất nó để học mô hình quy mô lớn, mà không tách biệt rõ ràng các nhiệm vụ. Thật không may, các nghiên cứu này có xu hướng ưu tiên các nhiệm vụ có kích thước mẫu lớn hơn, bỏ qua các nhiệm vụ có kích thước mẫu nhỏ hơn. Hơn nữa, chúng không đảm bảo tốc độ hội tụ bằng nhau giữa các nhiệm vụ, dẫn đến tối ưu hóa quá mức của một số nhiệm vụ và tối ưu hóa thiếu của những nhiệm vụ khác.

Trong bài báo này, chúng tôi tập trung vào tinh chỉnh đa nhiệm vụ (MFT) của (Code) LLM, nhằm đảm bảo sự chú ý công bằng đối với các nhiệm vụ có kích thước mẫu khác nhau và tiến trình tối ưu hóa tương tự gần như tương đồng. Cụ thể, sự chú ý của chúng tôi là trên Code LLM, vì các nhiệm vụ trong lĩnh vực mã thường thể hiện sự tương quan, và vì vậy chúng tôi đặt tên phương pháp của mình là MFTCoder. Chúng tôi nhấn mạnh rằng MFTCoder có thể được mở rộng cho một tập hợp tùy ý các nhiệm vụ NLP liên quan theo cách đơn giản. Để nâng cao hiệu quả của MFTCoder, chúng tôi kết hợp các kỹ thuật tinh chỉnh hiệu quả tham số, bao gồm LoRA Hu et al. (2021) và QLoRA Dettmers et al. (2023). Kết quả thí nghiệm cho thấy các mô hình đa nhiệm vụ được đào tạo bằng phương pháp MFT vượt trội hơn những mô hình được tinh chỉnh riêng lẻ cho từng nhiệm vụ hoặc bằng cách hợp nhất dữ liệu từ nhiều nhiệm vụ. Chúng tôi tiếp tục xác nhận hiệu quả của MFTCoder trên các LLM được huấn luyện trước cơ sở khác nhau, như Qwen Bai et al. (2023), Baichuan Baichuan (2023), Llama Touvron et al. (2023a), Llama 2 Touvron et al. (2023b), StarCoder Li et al. (2023a), CodeLLama Rozière et al. (2023), và CodeGeex2 Zheng et al. (2023). Đáng chú ý, khi áp dụng MFTCoder cho mô hình cơ sở CodeLlama-34B-Python Rozière et al. (2023), nó đạt được điểm pass@1 xuất sắc 74.4% trên bộ dữ liệu đánh giá humanEval, thậm chí vượt qua hiệu suất của GPT-4 (67%, zero-shot) OpenAI (2023).

Các đóng góp chính của bài báo này có thể được tóm tắt như sau:
• Chúng tôi đề xuất MFTCoder, một phương pháp tinh chỉnh đa nhiệm vụ mới để đồng thời thích ứng LLM cho nhiều nhiệm vụ liên quan đến mã. Trọng tâm của chúng tôi là giải quyết các vấn đề cân bằng dữ liệu và tốc độ hội tụ thường phát sinh trong các phương pháp tinh chỉnh đa nhiệm vụ trước đây.
• Chúng tôi xác nhận MFTCoder trên các mô hình được huấn luyện trước cơ sở khác nhau, bao gồm Qwen Bai et al. (2023), Baichuan Baichuan (2023), Llama Touvron et al. (2023a), Llama 2 Touvron et al. (2023b), StarCoder Li et al. (2023a), CodeLLama Rozière et al. (2023), CodeFuse Di et al. (2023), và CodeGeex2 Zheng et al. (2023), chứng minh khả năng tương thích của nó với các mô hình cơ sở khác nhau.
• Các thí nghiệm rộng rãi cho thấy phương pháp MFT vượt trội hơn việc tinh chỉnh riêng lẻ cho từng nhiệm vụ hoặc hợp nhất dữ liệu từ nhiều nhiệm vụ. Đáng chú ý, khi triển khai MFTCoder với mô hình cơ sở CodeLlama-34B-Python Rozière et al. (2023), nó đạt được điểm pass@1 ấn tượng 74.4% trên bộ dữ liệu đánh giá humanEval, vượt qua hiệu suất của GPT-4 (67%, zero-shot) OpenAI (2023).

2 Công trình Liên quan

2.1 Code LLM
Khả năng lập trình đóng vai trò như một tiêu chí quan trọng để đánh giá các mô hình ngôn ngữ lớn tổng quát (LLM) trong các nhiệm vụ liên quan đến mã. Hiệu suất đáng chú ý trên bộ dữ liệu HumanEval được sử dụng rộng rãi Chen et al. (2021), một benchmark cho sinh mã, đã được quan sát trên các mô hình khác nhau, bao gồm LaMDA Thoppilan et al. (2022), PaLM Chowdhery et al. (2022), PaLM 2 Anil et al. (2023), ChatGPT, và GPT-4 OpenAI (2023). Cụ thể, GPT-4 đã thiết lập một kỷ lục đáng chú ý với điểm pass@1 là 67.0%. Tuy nhiên, bản chất mã nguồn đóng của chúng hạn chế tính khả dụng và cản trở những tiến bộ hợp tác tiếp theo. Ngược lại, các LLM mã nguồn mở gần đây, bao gồm LLaMA Touvron et al. (2023a), LLaMA 2 Touvron et al. (2023b), Qwen Bai et al. (2023), và Phi-1.5 Li et al. (2023b), đã thể hiện tiến bộ đáng chú ý trong các nhiệm vụ liên quan đến mã, với điểm số đáng khen là 23.7%, 29.9%, 32.3%, và 41.4% tương ứng. Bất chấp tiến bộ này, hiệu suất của chúng vẫn tụt hậu so với các mô hình mã nguồn đóng tiên tiến.

Mặt khác, các LLM được thiết kế đặc biệt cho các nhiệm vụ liên quan đến mã, thường được gọi là code LLM, cũng đã trải qua những phát triển đáng kể. Cùng với các Code LLM mã nguồn đóng như Codex Chen et al. (2021), Code-Davinci Chen et al. (2021), AlphaCode Li et al. (2022), PaLM-Coder Chowdhery et al. (2022), và PanGu-Coder Christopoulou et al. (2022), các lựa chọn thay thế mã nguồn mở như SantaCoder Allal et al. (2023), Phi-1.0 Gunasekar et al. (2023), CodeGeeX-2 Zheng et al. (2023), StarCoder Li et al. (2023a), Code LLaMA Rozière et al. (2023) đã thể hiện hiệu suất cạnh tranh với các đối tác mã nguồn đóng của chúng. Đáng chú ý, CodeLLama-34B-Python Rozière et al. (2023) đạt được điểm số 53.7% trên HumanEval. Ngoài huấn luyện trước, một phương pháp thú vị khác để tiếp tục nâng cao Code LLM là tinh chỉnh theo hướng dẫn, như được thể hiện bởi CodeT5+ Wang et al. (2023), Phi-1.0 Gunasekar et al. (2023), OctoPack Muennighoff et al. (2023), và WizardCoder Luo et al. (2023). Bằng cách tận dụng các bộ dữ liệu hướng dẫn chất lượng cao được tuyển chọn cẩn thận, các phương pháp này thể hiện tiềm năng của tinh chỉnh để nâng cao khả năng sinh mã.

2.2 Học Đa nhiệm vụ
Học đa nhiệm vụ (MTL) Caruana (1997); Crawshaw (2020) là một phương pháp mạnh mẽ trong học máy có tiềm năng đáng kể để nâng cao hiệu suất mô hình và giải quyết các thách thức đa dạng Crawshaw (2020). Bằng cách đào tạo một mô hình duy nhất trên nhiều nhiệm vụ liên quan, MTL cho phép mô hình tận dụng kiến thức và mẫu chung, dẫn đến tổng quát hóa tăng cường và độ chính xác cải thiện. Các phương pháp MTL có thể được phân loại thành hai nhóm: chia sẻ tham số cứng Zhao et al. (2018); Liu et al. (2019b,a); Kendall et al. (2018); Liu et al. (2019c); Chen et al. (2018); Jean et al. (2019) và chia sẻ tham số mềm Duong et al. (2015); Yang and Hospedales (2017); Long et al. (2017); Lee et al. (2018); Sun et al. (2020); Pascal et al. (2021). Chia sẻ tham số cứng liên quan đến việc chia sẻ trọng số mô hình giữa các nhiệm vụ, trong khi chia sẻ tham số mềm kết hợp các mô hình riêng cho từng nhiệm vụ với trọng số riêng biệt. Trong bối cảnh của các mô hình ngôn ngữ lớn (LLM), chia sẻ tham số cứng đặc biệt phù hợp, vì số lượng tham số lớn trong LLM tạo thuận lợi cho khả năng xử lý nhiều nhiệm vụ liên quan với một tập hợp tham số chung. Kết quả là, việc tối ưu hóa một LLM để giải quyết hiệu quả nhiều nhiệm vụ nằm ở trung tâm của MTL cho LLM. Trong những năm gần đây, những tiến bộ đáng chú ý đã được thực hiện trong các kỹ thuật MTL. Google đã giới thiệu T5 Raffel et al. (2023) vào năm 2020, nơi họ khám phá việc ứng dụng các kỹ thuật MTL. Meta tiếp tục giới thiệu Mupper Aghajanyan et al. (2021) vào năm 2021, áp dụng học đa nhiệm vụ giữa huấn luyện trước và tinh chỉnh, được gọi là tiền tinh chỉnh (PFT). Họ phát hiện rằng việc kết hợp bước này nâng cao hiệu suất của mô hình được huấn luyện trước trên các nhiệm vụ phía dưới khác nhau và cải thiện đáng kể tốc độ tinh chỉnh. Tuy nhiên, nếu số lượng nhiệm vụ trong PFT quá nhỏ, nó có thể có tác động tiêu cực đến hiệu suất của mô hình. Do đó, nó được khuyến nghị có tối thiểu 15 nhiệm vụ để đạt kết quả tối ưu. Dựa trên T5, Google đã giới thiệu ExT5 Aribandi et al. (2022), tăng số lượng nhiệm vụ lên 107. Họ phát hiện rằng miễn là số lượng nhiệm vụ trong huấn luyện trước đủ lớn, ngay cả khi có thể có sự can thiệp lẫn nhau giữa các nhiệm vụ, kết quả cuối cùng vẫn rất tốt. Cuối cùng, ExT5 đã vượt trội hơn T5 trên nhiều chỉ số. Đáng chú ý là các nghiên cứu này chủ yếu tập trung vào việc kết hợp dữ liệu đa nhiệm vụ và hợp nhất nó để mô hình lớn học, mà không tách biệt rõ ràng các nhiệm vụ. Mặc dù các phương pháp này đã cho thấy kết quả đầy hứa hẹn, chúng có xu hướng bỏ qua các vấn đề về mất cân bằng dữ liệu và tốc độ hội tụ thường phát sinh trong MTL. Trong bài báo này, chúng tôi giải quyết những thách thức này và đề xuất MFTCoder, một phương pháp tinh chỉnh đa nhiệm vụ cho LLM giải quyết các vấn đề này một cách hiệu quả.

--- TRANG 4 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Hình 2: Phương pháp Sinh dữ liệu cho Bộ dữ liệu Bài tập Mã sử dụng Sơ đồ Hội thoại Một lượt.

3 Phương pháp

Trong phần này, chúng tôi sẽ giới thiệu khung tinh chỉnh đa nhiệm vụ của chúng tôi, MFTCoder³, cùng với thiết kế các thành phần chính của nó.

3.1 Khung MFT

MFTCoder nhằm thích ứng liền mạch LLM với các tình huống mới đa dạng trong khi tối đa hóa hiệu suất của chúng trong một bối cảnh cụ thể. Khi áp dụng MFTCoder cho một tình huống mới, bước ban đầu liên quan đến việc phân tách tình huống thành các nhiệm vụ nhỏ hơn tương ứng với các khả năng mục tiêu. Ví dụ, trong lĩnh vực code LLM, mục tiêu tổng thể của việc nâng cao khả năng mã của mô hình có thể được chia thành các nhiệm vụ cụ thể như hoàn thành mã, sinh mã từ văn bản, sinh test case đơn vị, sửa mã, debug mã, và thậm chí dịch thuật ngôn ngữ chéo. Kinh nghiệm thực tế rộng rãi của chúng tôi đã chứng minh rằng MFTCoder xử lý hiệu quả quy mô đa nhiệm vụ từ đơn lẻ đến hàng chục hoặc thậm chí hàng trăm nhiệm vụ. Mỗi nhiệm vụ đòi hỏi việc thu thập và tổ chức các bộ dữ liệu tinh chỉnh. Tuy nhiên, việc thu thập dữ liệu cho một số nhiệm vụ có thể gây ra thách thức. Để khắc phục điều này, MFTCoder tận dụng các kỹ thuật Self-Instruct Wang et al. (2022) và Agents để sinh các bộ dữ liệu hướng dẫn. Với khả năng tinh chỉnh đồng thời nhiều nhiệm vụ phía dưới, MFTCoder xử lý hiệu quả khối lượng lớn dữ liệu tinh chỉnh, đảm bảo đào tạo hiệu quả. Nó kết hợp hai chế độ token hóa dữ liệu hiệu quả và triển khai các kỹ thuật PEFT (Tinh chỉnh Hiệu quả Tham số) để nâng cao hiệu quả đào tạo. Trong lĩnh vực học đa nhiệm vụ, MFTCoder đối mặt với vấn đề mất cân bằng nhiệm vụ, bao gồm phân phối dữ liệu mất cân bằng, độ khó nhiệm vụ khác nhau, và tỷ lệ hội tụ khác nhau. Để giảm thiểu những thách thức này, MFTCoder giới thiệu hoặc thích ứng các hàm mất mát khác nhau để đạt được sự cân bằng nhiệm vụ. Nhận thức rằng các mô hình quy mô lớn khác nhau sở hữu điểm mạnh và khả năng riêng biệt, MFTCoder tạo thuận lợi cho việc lựa chọn kiến trúc mô hình phù hợp dựa trên các tình huống cụ thể để đạt được hiệu suất tối ưu. Nó đã được thích ứng với các LLM phổ biến, bao gồm LLama Touvron et al. (2023a), LLama 2 Touvron et al. (2023b), CodeLLama Rozière et al. (2023), Qwen Bai et al. (2023), Baichuan 1/2 Baichuan (2023), ChatGLM 2 Du et al. (2022), CodeGeeX 2 Zheng et al. (2023), GPT-NEOX Black et al. (2022), CodeFuse Di et al. (2023), StarCoder Li et al. (2023a), AntLLM, và nhiều hơn nữa. Chúng tôi liên tục cập nhật và mở rộng khả năng tương thích với các mô hình bổ sung.

Khung tổng thể của MFTCoder được minh họa trong Hình 1. Trong các phần tiếp theo, chúng tôi sẽ cung cấp một khám phá chi tiết hơn về các thành phần này, bao gồm xây dựng bộ dữ liệu hướng dẫn, chế độ token hóa hiệu quả, tinh chỉnh PEFT và các hàm mất mát cân bằng.

3.2 Xây dựng Bộ dữ liệu Hướng dẫn

Đối với các nhiệm vụ có việc thu thập dữ liệu thách thức, chúng tôi sử dụng kỹ thuật Self-Instruct Wang et al. (2022) để sinh dữ liệu tinh chỉnh cho các nhiệm vụ liên quan đến mã phía dưới trong MFTCoder. Điều này liên quan đến việc cung cấp các prompt tùy chỉnh cho GPT-3.5 hoặc GPT-4 mô tả rõ ràng yêu cầu sinh hướng dẫn của chúng tôi, từ đó sinh ra dữ liệu hướng dẫn. Hơn nữa, chúng tôi lấy cảm hứng từ phương pháp Textbook được sử dụng trong công trình PHI Gunasekar et al. (2023), kết hợp kỹ thuật self-instruct để sinh các bộ dữ liệu Code Exercises cho các nhiệm vụ liên quan đến mã phía dưới.

Về triển khai cụ thể, chúng tôi có hai lựa chọn. Một là phương pháp hội thoại đa lượt Agents đạt được thông qua Camel Li et al. (2023c), và cái khác là phương pháp hội thoại đơn lượt bằng cách gọi trực tiếp API ChatGPT.

³https://github.com/codefuse-ai/MFTCoder

--- TRANG 5 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Trong phương pháp đa lượt của chúng tôi, chúng tôi sử dụng Camel để khởi chạy hai agent, mỗi agent được gán vai trò và chủ đề nhiệm vụ cụ thể, tạo thuận lợi cho cuộc đối thoại giữa chúng để sinh dữ liệu hướng dẫn phù hợp với chủ đề đã cho. Ví dụ, khi sinh dữ liệu bài tập Python, chúng tôi chỉ định vai trò 'giáo viên' (mô phỏng vai trò user của ChatGPT) và 'học sinh' (mô phỏng vai trò assistant của ChatGPT) cho các agent. Trách nhiệm của giáo viên là cung cấp hướng dẫn bài tập cho học sinh, trong khi nhiệm vụ của học sinh là đưa ra các giải pháp tương ứng cho những hướng dẫn đó. Quá trình lặp đi lặp lại này tiếp tục, sinh ra nhiều câu hỏi bài tập, cho đến khi các yêu cầu nhiệm vụ được đáp ứng hoặc độ dài đầu vào tối đa của ChatGPT được đạt tới. Để thích ứng với giới hạn độ dài đầu vào của ChatGPT, chúng tôi không thể sử dụng trực tiếp một câu hỏi lớn làm chủ đề nhiệm vụ. Ví dụ, khi tạo câu hỏi bài tập Python để đánh giá thành thạo của học sinh, chúng tôi chia chủ đề chính thành các điểm kiến thức Python nhỏ hơn (ví dụ: cây tìm kiếm nhị phân) và khởi động các phiên Camel riêng biệt cho từng điểm kiến thức. Để biết ví dụ cụ thể, vui lòng tham khảo Phụ lục A.

Phương pháp đa lượt cung cấp tự động hóa cao nhưng có thể tốn kém do nhu cầu duy trì hai agent, mỗi agent thực hiện các cuộc gọi đa lượt đến API ChatGPT. Để giảm thiểu điều này, chúng tôi đề xuất một phương pháp sinh hội thoại đơn lượt hiệu quả về chi phí hơn, và quy trình tổng thể được minh họa trong Hình 2. Chúng tôi bắt đầu bằng việc tạo một tập hợp các seed ban đầu, như hàng trăm điểm kiến thức Python. Các seed này sau đó được kết hợp với các template prompt cố định đã chuẩn bị để sinh ra một tập hợp các prompt nhiệm vụ có mẫu. Để giải quyết vấn đề giảm tính đa dạng do các template cố định và đảm bảo mô tả prompt chính xác, chúng tôi sử dụng tính năng tinh chỉnh prompt nhiệm vụ của Camel để có được các prompt nhiệm vụ chính xác và đa dạng. Mỗi prompt nhiệm vụ được sử dụng để sinh ra một tập hợp các hướng dẫn liên quan đến seed tương ứng (ví dụ: các bài toán bài tập liên quan đến cây tìm kiếm nhị phân). Sử dụng ChatGPT, chúng tôi sinh ra các giải pháp tương ứng cho các hướng dẫn được sinh ra. Cuối cùng, chúng tôi lắp ráp và loại bỏ trùng lặp các hướng dẫn với các giải pháp tương ứng của chúng để có được một bộ dữ liệu bài tập. Chúng tôi đã mã nguồn mở một bộ dữ liệu Python Code Exercises⁴ được xây dựng bằng phương pháp này.

3.3 Chế độ Token hóa Hiệu quả

Token hóa là một bước thiết yếu trong quá trình huấn luyện trước và tinh chỉnh của các mô hình LLM, nơi văn bản đầu vào và đầu ra được chia thành các đơn vị nhỏ hơn để được xử lý. Nó, cùng với hàm mất mát, xác định hiệu quả cách sử dụng dữ liệu trong quá trình đào tạo, do đó đóng vai trò quan trọng trong cả hiệu quả và hiệu suất đào tạo của mô hình. Trong sơ đồ token hóa SFT điển hình, các mẫu trong cùng một batch được căn chỉnh với độ dài đầu vào tối đa (seq-length) của mô hình bằng các token padding bổ sung, được hiển thị như Hình 3a. Tuy nhiên, trong thực tế, chúng tôi phát hiện rằng phương pháp này dẫn đến tỷ lệ cao các token padding. Ví dụ, khi sử dụng tokenizer CodeFuse-13B Di et al. (2023) để xử lý 35 nhiệm vụ phía dưới, tỷ lệ trung bình của các token padding là 92.22% (với seq-length được đặt thành 4096). Điều này có nghĩa là một số lượng đáng kể token chỉ được sử dụng để căn chỉnh, không cung cấp giá trị nào cho quá trình đào tạo. Điều này dẫn đến hiệu quả đào tạo thấp hơn và lãng phí không gian lưu trữ token hóa offline. Để giải quyết vấn đề này, chúng tôi đã áp dụng và tối ưu hóa hai chế độ token hóa, cụ thể là chế độ dynamic padding và pack.

Trong chế độ dynamic padding, kích thước cửa sổ micro batch của mỗi GPU được xác định bởi độ dài mẫu tối đa trong micro batch. Các mẫu ngắn hơn được bổ sung các token padding bổ sung để khớp với kích thước này, như được hiển thị trong Hình 3b. Mặc dù các token padding không ảnh hưởng đến hiệu quả đào tạo của mô hình, chúng thêm chi phí tính toán trong quá trình đào tạo, ảnh hưởng đến tốc độ đào tạo. Chế độ dynamic padding giảm hiệu quả tỷ lệ các token padding được sử dụng, dẫn đến đào tạo nhanh hơn. Theo kinh nghiệm của chúng tôi, phương pháp này có thể mang lại cải thiện tốc độ khoảng gấp đôi so với chế độ token hóa SFT truyền thống (cải thiện thực tế phụ thuộc vào bộ dữ liệu). Điều quan trọng cần lưu ý là chế độ này chỉ phù hợp cho các tình huống token hóa trực tuyến.

Trong khi chế độ dynamic padding giảm kích thước cửa sổ micro batch, chế độ pack, tương tự như chế độ token hóa SFT của Llama 2 Touvron et al. (2023b), tối đa hóa việc sử dụng độ dài cửa sổ đầu vào tối đa (seq-length) của mô hình. Trong chế độ pack, nhiều mẫu tinh chỉnh được đóng gói tuần tự vào một cửa sổ seq-length, được phân tách bởi các token eos, như được hiển thị trong Hình 3c. Trong hình, các mẫu 1-4 của Hình 3a được kết hợp và đặt trong một cửa sổ nối tiếp nhau. Nếu một mẫu không thể vừa trong cửa sổ hiện tại, nó được đặt trong cửa sổ tiếp theo với các token padding lấp đầy không gian còn lại. Ví dụ, trong Hình 3c, mẫu 5 được đặt trong cửa sổ thứ hai với các token padding, trong khi mẫu 6 được chứa trong cửa sổ thứ ba. Chế độ pack, so với chế độ dynamic padding, cung cấp tỷ lệ token padding thậm chí thấp hơn, dẫn đến cải thiện tốc độ đào tạo. Kinh nghiệm thực tế của chúng tôi chứng minh rằng phương pháp này giảm tỷ lệ trung bình của các token padding xuống dưới 10% trong số 35 nhiệm vụ được đề cập trước đó, dẫn đến sự tăng tốc đáng kể trong tốc độ đào tạo trong khi duy trì hiệu quả đào tạo. Điều quan trọng cần nhấn mạnh là MFTCoder hỗ trợ cả tình huống token hóa pack trực tuyến và ngoại tuyến, phục vụ không chỉ giai đoạn SFT mà còn cả giai đoạn huấn luyện trước.

⁴https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k

--- TRANG 6 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

(a) Chế độ SFT Bình thường
(b) Chế độ Dynamic Padding
(c) Chế độ Pack SFT

Hình 3: Minh họa sự khác biệt trong tổ chức mẫu trong một batch giữa SFT bình thường, dynamic padding và chế độ token hóa Pack SFT. Các ô vuông màu nhạt trong hình đại diện cho phần Prompt của các mẫu, trong khi các ô vuông màu tối đại diện cho phần Label (tham gia tính toán loss). Các ô vuông trống đại diện cho phần padding.

3.4 Tinh chỉnh Hiệu quả PEFT

Các mô hình quy mô lớn phổ biến thường chứa hàng tỷ tham số, trong khi các tình huống học đa nhiệm vụ thường liên quan đến nhiều nhiệm vụ, dẫn đến tổng số lượng mẫu tinh chỉnh đáng kể. Nếu chúng ta lựa chọn tinh chỉnh toàn diện các mô hình lớn này sử dụng lượng lớn dữ liệu, hai thách thức sẽ phát sinh: thứ nhất, nhu cầu về tài nguyên lưu trữ và tính toán rộng lớn; thứ hai, rủi ro tiềm ẩn của việc quên thảm khốc trong quá trình đào tạo. Để giải quyết những vấn đề này, MFTCoder kết hợp kỹ thuật PEFT (Tinh chỉnh hiệu quả tham số) Houlsby et al. (2019), cho phép tinh chỉnh hiệu quả được hoàn thành trong thời gian ngắn và với yêu cầu tài nguyên tối thiểu. Cụ thể, MFTCoder hỗ trợ hai phương pháp PEFT: Lora (Thích ứng Thứ hạng Thấp Mô hình Ngôn ngữ Quy mô Lớn) Hu et al. (2021) và QLora (Thích ứng Thứ hạng Thấp Mô hình Ngôn ngữ Quy mô Lớn Lượng tử hóa) Dettmers et al. (2023). Khái niệm cơ bản của Lora khá đơn giản, như được mô tả trong Hình 4. Nó liên quan đến việc thêm một nhánh phụ vào mô hình ngôn ngữ được huấn luyện trước ban đầu. Trong quá trình đào tạo, các tham số W∈R^(d×d) của mô hình được huấn luyện trước ban đầu vẫn cố định, trong khi chỉ ma trận mở rộng chiều A∈R^(d×r) và ma trận giảm chiều B∈R^(r×d) trong nhánh phụ được đào tạo. Tích ma trận BA sau đó được thêm vào mô hình gốc W, dẫn đến mô hình mới được đào tạo. Do độ lớn của r nhỏ hơn đáng kể so với d, số lượng tham số có thể đào tạo có thể được giảm đáng kể. Dựa trên LoRA, QLoRA kết hợp một kỹ thuật lượng tử hóa độ chính xác cao mới gọi là NF4 và lượng tử hóa kép để lượng tử hóa mô hình được huấn luyện trước thành 4 bit. Nó cũng giới thiệu một tập hợp nhỏ các trọng số adapter thứ hạng thấp có thể học được. Các trọng số này được tinh chỉnh bằng cách tối ưu hóa gradient thông qua lan truyền ngược của các trọng số được lượng tử hóa. Kết quả là, QLoRA cho phép tinh chỉnh các mô hình lớn hơn bằng cách sử dụng ít tài nguyên GPU hơn. Ví dụ, MFTCoder có thể tinh chỉnh một mô hình 70B trên một A100 với 80GB VRAM.

--- TRANG 7 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Hình 4: Trực quan hóa Bản chất Ý tưởng Cơ bản của Lora.

3.5 Tinh chỉnh Đa nhiệm vụ với Các Hàm Mất mát Cân bằng

Là một khung học đa nhiệm vụ, MFTCoder, như được mô tả trong Phần 2, đối mặt với thách thức đáng kể về mất cân bằng dữ liệu, tính không đồng nhất nhiệm vụ, và tốc độ hội tụ khác nhau. Để giải quyết những thách thức này, MFTCoder kết hợp một tập hợp các hàm mất mát được thiết kế đặc biệt để giảm thiểu những mất cân bằng này.

Để giải quyết vấn đề mất cân bằng dữ liệu, trước tiên chúng tôi đảm bảo rằng tất cả mẫu từ tất cả nhiệm vụ được sử dụng đúng một lần trong một epoch duy nhất. Để tránh mô hình thiên vị các nhiệm vụ có lượng dữ liệu lớn hơn, chúng tôi giới thiệu một chiến lược gán trọng số trong quá trình tính toán loss. Cụ thể, chúng tôi hỗ trợ hai sơ đồ tính toán trọng số: một dựa trên số lượng mẫu nhiệm vụ và cái khác dựa trên số lượng token hợp lệ tham gia tính toán loss. Cái trước đơn giản hơn, nhưng có thể hoạt động kém khi xử lý các nhiệm vụ có sự khác biệt cực lớn về số lượng token hợp lệ, như các nhiệm vụ phân loại nhị phân như trả lời "có" hoặc "không" hoặc các nhiệm vụ thi trắc nghiệm một lựa chọn. Mặt khác, sơ đồ gán trọng số sau dựa trên số lượng thực tế các token hợp lệ tham gia tính toán loss có thể giảm thiểu những vấn đề này. Công thức cụ thể cho tính toán loss có trọng số được hiển thị trong Phương trình 1. Trong Phương trình 1, N đại diện cho tổng số nhiệm vụ, M_i biểu thị số lượng mẫu cho nhiệm vụ thứ i, T_ij biểu thị số lượng token hợp lệ (tức là, các token tham gia tính toán loss) cho mẫu thứ j của nhiệm vụ thứ i, và t_ijk đề cập đến token hợp lệ thứ k của mẫu thứ j cho nhiệm vụ thứ i.

L(θ) = min_θ (1/N) ∑_{i=1}^N (∑_{j=1}^{M_i} ∑_{k=1}^{T_ij} -log(p_θ(t_ijk))) / (∑_{j=1}^{M_i} T_ij)   (1)

Để giải quyết vấn đề tính không đồng nhất nhiệm vụ, chúng tôi lấy cảm hứng từ phương pháp focal loss và kết hợp nó vào MFTCoder. Chúng tôi triển khai hai mức độ khác nhau của các hàm focal loss để phục vụ các độ chi tiết khác nhau. Một hoạt động ở mức mẫu, như được hiển thị trong Phương trình 2, trong khi cái khác hoạt động ở mức nhiệm vụ, như được hiển thị trong Phương trình 3.

L_2(θ) = min_θ (∑_{i=1}^N ∑_{j=1}^{M_i} -α_i * (1-P_ij)^γ * Q_ij) / (∑_{i=1}^N M_i), P_ij = (1/T_ij) ∑_{k=1}^{T_ij} P_ijk, Q_ij = (1/T_ij) ∑_{k=1}^{T_ij} log(P_ijk)   (2)

L_3(θ) = min_θ (1/N) ∑_{i=1}^N -α_i * (1-P_i)^γ * Q_i, P_i = (1/M_i) ∑_{j=1}^{M_i} (1/T_ij) ∑_{k=1}^{T_ij} P_ijk, Q_i = (1/M_i) ∑_{j=1}^{M_i} (1/T_ij) ∑_{k=1}^{T_ij} log(P_ijk)   (3)

Để giải quyết vấn đề tốc độ hội tụ không nhất quán, chúng tôi lấy cảm hứng từ phương pháp FAMO Liu et al. (2023) và áp dụng nó một cách sáng tạo để tính toán loss xác thực. Đầu tiên, chúng tôi giả định rằng mỗi nhiệm vụ, được đánh chỉ số bởi i, có loss gốc riêng của nó L_i(θ). Trong lần lặp thứ t, chúng tôi cập nhật trọng số của mỗi nhiệm vụ dựa trên gradient của loss xác thực tương ứng của chúng, nhằm tối đa hóa trọng số w_i cho nhiệm vụ có tốc độ hội tụ chậm nhất, được hiển thị như Phương trình 4. Ở đây, g_t đại diện cho gradient của loss xác thực có trọng số cho tất cả các nhiệm vụ, c_i(α, g_t) biểu thị độ dốc (gradient) của loss xác thực cho nhiệm vụ thứ i, θ_t biểu thị các tham số của mạng trong lần lặp thứ t, α là tỷ lệ học, và ε là một hằng số nhỏ để ngăn chặn chia cho zero. Hơn nữa, chúng tôi muốn cung cấp giải thích thêm về cách chúng tôi đạt được sự hội tụ cân bằng. Để đảm bảo rằng các nhiệm vụ hội tụ với tốc độ tương tự, chúng tôi giới thiệu một

--- TRANG 8 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 1: Các mô hình thí nghiệm khác nhau và dữ liệu đào tạo tương ứng của chúng.

Mô hình Thí nghiệm | Nhiệm vụ | Khả năng Mong muốn | #Mẫu | #Mẫu sau đóng gói
SFT-S-CODECOMPLETION | CODE-COMPLETION | Hoàn thành Mã | 192,547 | 18,811
SFT-S-TEXT2CODE | TEXT2CODE | Sinh mã từ Văn bản | 94,086 | 14,399
SFT-S-CODECOMMENT | CODE-COMMENT | Sinh Chú thích Mã | 645,711 | 134,775
SFT-S-CODETRANS | CODE-TRANS | Dịch Mã | 307,585 | 71,573
SFT-S-UNITTEST | UNIT-TEST | Sinh test case đơn vị | 390,393 | 77,681
SFT-MIXED | Hỗn hợp 5 nhiệm vụ trên | Tất cả các nhiệm vụ trên | 1,630,322 | 317,239
MFT-5TASKS | 5 nhiệm vụ trên | Tất cả các nhiệm vụ trên | 1,630,322 | 317,239

cơ chế cân bằng động. Tại mỗi lần lặp, chúng tôi cập nhật trọng số cụ thể cho từng nhiệm vụ dựa trên gradient của loss xác thực của chúng. Phương pháp này nhằm tạo ra tầm quan trọng lớn hơn cho các nhiệm vụ có tốc độ hội tụ chậm hơn, cho phép chúng có ảnh hưởng lớn hơn đến quá trình tối ưu hóa tổng thể. Bằng cách điều chỉnh động trọng số nhiệm vụ, chúng tôi tạo ra một tình huống hội tụ cân bằng, nơi tất cả các nhiệm vụ tiến về phía giải pháp tối ưu của chúng với tốc độ tương tự. Cơ chế này giải quyết hiệu quả vấn đề tốc độ hội tụ khác nhau và nâng cao tính ổn định và hiệu suất tổng thể của khung MFTCoder.

L_4(θ) = max_{g_t} min_i (1/α)c_i(α, g_t) - (1/2)||g_t||^2, g_t = Σ_i w_i^t ∇L_i(θ_t), c_i(α, g_t) = (L_i(θ_t) - L_i(θ_t - αd_t))/(L_i(θ_t) + ε)   (4)

Bằng cách kết hợp những hàm mất mát khác nhau này, MFTCoder giải quyết hiệu quả các yêu cầu đa dạng của các tình huống đa nhiệm vụ khác nhau và giảm thiểu các thách thức về mất cân bằng dữ liệu, tính không đồng nhất nhiệm vụ, và tốc độ hội tụ không nhất quán thường gặp trong nghiên cứu MTL quy mô lớn hiện có. Khung linh hoạt của MFTCoder cung cấp một giải pháp mạnh mẽ cho những vấn đề này, trao quyền cho việc phát triển các mô hình đa nhiệm vụ hiệu quả và chính xác hơn.

4 Đánh giá

Trong phần này, chúng tôi sẽ thực hiện nhiều bộ thí nghiệm sử dụng MFTCoder để xác thực hiệu quả và tính ưu việt của phương pháp MFT. Cụ thể, chúng tôi nhằm giải quyết ba câu hỏi nghiên cứu sau:

RQ1: Liệu mô hình MFT, thu được bằng cách tinh chỉnh nhiều nhiệm vụ sử dụng phương pháp MFT, có vượt trội hơn các mô hình SFT-S(ingle), nơi mỗi nhiệm vụ được tinh chỉnh riêng lẻ?

RQ2: Liệu mô hình MFT có vượt trội hơn mô hình SFT-Mixed, nơi nhiều nhiệm vụ được kết hợp và tinh chỉnh như một?

RQ3: Về tổng quát hóa cho các nhiệm vụ chưa thấy, liệu mô hình MFT có vượt trội hơn mô hình SFT-Mixed?

Tiếp theo, chúng tôi sẽ bắt đầu bằng việc trình bày thiết lập thí nghiệm. Sau đó, chúng tôi sẽ thể hiện và khám phá sâu vào kết quả thí nghiệm. Cuối cùng, chúng tôi sẽ kết thúc bằng cách tóm tắt và giải quyết các câu hỏi nghiên cứu được đặt ra trong phần này.

4.1 Thiết lập Đánh giá

Để giải quyết ba câu hỏi nghiên cứu này, chúng tôi chọn 5 nhiệm vụ liên quan đến mã phía dưới và chuẩn bị dữ liệu tinh chỉnh tương ứng, như được hiển thị trong Bảng 1. Bảng 1 trình bày các cải tiến mong muốn (Cột III) và số lượng mẫu (Cột IV) cho từng nhiệm vụ. Ví dụ, CODECOMPLETION-TASK nhằm cải thiện khả năng hoàn thành mã của mô hình và bao gồm 192,547 mẫu tinh chỉnh. CODETRANS-TASK nhằm nâng cao khả năng dịch mã của mô hình và bao gồm 307,585 mẫu tinh chỉnh. Do đó, chúng tôi đào tạo 7 mô hình (Cột I), bao gồm các mô hình SFT-S-* riêng lẻ được đào tạo cho mỗi nhiệm vụ phía dưới, một mô hình SFT-MIXED kết hợp cho dữ liệu 5 nhiệm vụ, và một mô hình MFT-5TASKS được đào tạo bằng phương pháp MFT.

Trong thí nghiệm, tất cả các mô hình được cấu hình giống hệt nhau ngoại trừ dữ liệu đào tạo. Mô hình cơ sở cho tất cả các mô hình là CodeLlama-13B-Python Rozière et al. (2023). Mỗi mô hình được đào tạo sử dụng 16 GPU A100 (với 80GB VRAM), kích thước micro batch là 8, và kích thước global batch là 128. Bộ tối ưu hóa Adam Kingma và Ba (2017) được sử dụng với tỷ lệ học ban đầu là 2e-4, và tỷ lệ học tối thiểu là 1e-5. Chúng tôi sử dụng chế độ QLora-INT4 của MFTCoder để tinh chỉnh, với tỷ lệ tham số tinh chỉnh nhất quán là 2.52%. Vị trí và giá trị ban đầu của các tham số có thể đào tạo cũng giống nhau. Tất cả các mô hình kết hợp Data-Balance Loss (tức là, Phương trình 1) và sử dụng chế độ token hóa pack. Đáng chú ý, khi chỉ có một nhiệm vụ, hàm loss này phù hợp với loss thông thường được sử dụng trong huấn luyện trước mô hình GPT tiêu chuẩn. Để xác định điểm hội tụ cho mỗi mô hình, chúng tôi đào tạo chúng cho đến khi loss xác thực vượt qua loss từ epoch hiện tại trong hai epoch liên tiếp tiếp theo. Cơ chế này, được biết đến như chiến lược dừng sớm, được sử dụng để đảm bảo hội tụ tối ưu cho mỗi mô hình.

4.2 Bộ dữ liệu Đánh giá

Trong bài báo này, chúng tôi sử dụng các benchmark đánh giá mã có sẵn công khai và đại diện để đánh giá so sánh. Các benchmark này bao gồm:

• HumanEval Chen et al. (2021) là một bộ dữ liệu đánh giá hoàn thành mã Python được sử dụng rộng rãi, được tuyển chọn tỉ mỉ bởi các nhà nghiên cứu tại OpenAI.

• HumanEval-X Zheng et al. (2023) là một phần mở rộng của HumanEval, được dịch sang nhiều ngôn ngữ lập trình, cho phép đánh giá hoàn thành mã đa ngôn ngữ.

• DS-1000 Lai et al. (2022) tập trung vào việc đánh giá khả năng thực hiện phân tích khoa học dữ liệu của mô hình sử dụng mã Python, bao gồm các thư viện thiết yếu như Numpy, Pandas, TensorFlow, Pytorch, Scipy, Sklearn, và Matplotlib.

• MBPP Austin et al. (2021) bao gồm 1000 bài toán lập trình Python, được xây dựng thông qua crowdsourcing, chủ yếu nhắm vào khả năng thành thạo Python cơ bản của mô hình. Trong nghiên cứu này, chúng tôi chọn 500 bài toán với ID 11-510 từ MBPP để đánh giá khả năng sinh mã từ văn bản, cụ thể là sinh mã dựa trên mô tả bài toán.

• CodeFuseEval Di et al. (2023), dựa trên HumanEval và HumanEval-X, mở rộng thêm đánh giá để bao gồm khả năng hoàn thành mã tiếng Trung (với docstring tiếng Trung), dịch mã, và sinh test case đơn vị, được gọi tương ứng là CodeFuseEval-CN, CodeFuseEval-CodeTrans, và CodeFuseEval-UnitTest.

Trong suốt các bộ dữ liệu đánh giá này, chúng tôi sử dụng "pass@1" làm chỉ số đánh giá trong bài báo này.

4.3 Kết quả Đánh giá

Trong phần này, chúng tôi sẽ thể hiện kết quả đánh giá của bảy mô hình được đào tạo. Đối với các mô hình SFT-S-*, được đào tạo riêng lẻ cho từng nhiệm vụ, chúng tôi sẽ tập trung vào việc kiểm tra khả năng mục tiêu cụ thể của chúng. Ví dụ, chúng tôi sẽ đánh giá độc quyền hiệu suất của mô hình SFT-S-CODECOMPLETION trong nhiệm vụ hoàn thành mã. Mặt khác, đối với các mô hình SFT-MIXED và MFT-5TASKS, chúng tôi sẽ đánh giá hiệu suất của chúng trên từng nhiệm vụ và so sánh với các mô hình SFT-S-* tương ứng. Cụ thể, chúng tôi sẽ thực hiện các kiểm tra để đánh giá khả năng hoàn thành mã, sinh mã từ văn bản, sinh chú thích mã, dịch mã, và sinh test case đơn vị.

4.3.1 Hoàn thành Mã

Đối với hoàn thành mã, chúng tôi sử dụng các bộ dữ liệu đánh giá HumanEval Chen et al. (2021) và HumanEval-X Zheng et al. (2023) để đánh giá hiệu suất của mô hình. HumanEval là một bộ dữ liệu benchmark được sử dụng rộng rãi được phát hành bởi OpenAI được thiết kế đặc biệt để đánh giá khả năng hoàn thành mã Python của các mô hình ngôn ngữ lớn. HumanEval-X, mặt khác, là một phần mở rộng của HumanEval cho phép đánh giá hiệu suất hoàn thành mã của các mô hình lớn trên nhiều ngôn ngữ lập trình khác nhau. Phù hợp với các nghiên cứu khác, chúng tôi sử dụng chỉ số pass@1 làm thước đo đánh giá.

Chúng tôi đánh giá ba mô hình: SFT-S-CODECOMPLETION, SFT-MIXED, và MFT-5TASKS. Hiệu suất của các mô hình này trên bộ dữ liệu HumanEval được tóm tắt trong Bảng 2 (Cột III). Kết quả cho thấy mô hình MFT-5TASKS, được đào tạo bằng phương pháp MFT, vượt trội hơn hai mô hình khác. Nó đạt được hiệu suất cao hơn 2.44% so với mô hình SFT-MIXED, được tinh chỉnh với dữ liệu nhiệm vụ hỗn hợp. Đáng chú ý là mô hình SFT-MIXED không hoạt động tốt bằng mô hình SFT-S-CODECOMPLETION, được đào tạo riêng lẻ cho nhiệm vụ hoàn thành mã.

Hơn nữa, chúng tôi thực hiện đánh giá đa ngôn ngữ trên bộ dữ liệu HumanEval-X cho ba mô hình, như được trình bày trong Bảng 3. Mô hình MFT-5TASKS thể hiện hiệu suất vượt trội trong Java và Golang, trong khi mô hình SFT-MIXED xuất sắc trong C++ và JavaScript. Nhìn chung, đánh giá khẳng định rằng mô hình MFT-5TASKS vượt trội hơn những mô hình khác, với cải thiện trung bình 1.22% so với mô hình SFT-MIXED.

Nhìn chung, về các nhiệm vụ hoàn thành mã, các mô hình được đào tạo bằng phương pháp MFT vượt trội hơn cả các mô hình được tinh chỉnh riêng lẻ và các mô hình được tinh chỉnh sau khi kết hợp nhiều nhiệm vụ.

--- TRANG 10 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 2: Hiệu suất pass@1 trên HumanEval (Hoàn thành Mã) và MBPP (Sinh mã từ Văn bản). Chúng tôi sử dụng chiến lược giải mã tham lam với zero-shot. Các giá trị của CodeLlama-Python-base được lấy từ Rozière et al. (2023).

Mô hình | Kích thước | HumanEval pass@1 | MBPP pass@1 | Trung bình
CodeLlama-Python-base Rozière et al. (2023) | 13B | 43.3% | 49.0% | 46.15%
SFT-S-CODECOMPLETION | 13B | 59.76% | NA | NA
SFT-S-TEXT2CODE | 13B | NA | 54.2% | NA
SFT-MIXED | 13B | 57.93% | 53.6% | 55.765%
MFT-5TASKS | 13B | 60.37% | 56.0% | 58.185%

Bảng 3: So sánh hiệu suất chỉ số pass@1 trên HumanEval-X đa ngôn ngữ (zero-shot, greedy-decoding)

Mô hình Được đào tạo | Java | C++ | JavaScript | Golang | Trung bình
CodeLlama-13B-Py-base | 43.3% | 41.46% | 34.76% | 38.41% | 29.27%
SFT-S-CODECOMPLETION | 50.0% | 39.02% | 47.56% | 40.23% | 44.20%
SFT-MIXED | 56.1% | 48.17% | 56.10% | 37.80% | 49.54%
MFT-5TASKS | 57.32% | 46.34% | 54.27% | 45.12% | 50.76%

4.3.2 Sinh mã từ Văn bản

Để đánh giá khả năng sinh mã của các mô hình dựa trên mô tả, chúng tôi chọn bộ dữ liệu đánh giá MBPP Austin et al. (2021) và sử dụng chỉ số pass@1. MBPP được thiết kế đặc biệt để đánh giá khả năng tổng hợp các chương trình Python ngắn gọn từ mô tả ngôn ngữ tự nhiên của các mô hình.

Chúng tôi kiểm tra ba mô hình, cụ thể là SFT-S-TEXT2CODE, SFT-MIXED, và MFT-5TASKS, trên bộ dữ liệu MBPP, đo hiệu suất pass@1 của chúng như được hiển thị trong Bảng 2 (Cột IV). Trong số các mô hình này, MFT-5TASKS thể hiện hiệu suất cao nhất, vượt qua mô hình SFT-MIXED 2.4%. Tương tự, về nhiệm vụ sinh mã từ văn bản, các mô hình được tinh chỉnh sau khi kết hợp nhiều nhiệm vụ cho thấy hiệu suất kém hơn so với các mô hình được tinh chỉnh đặc biệt cho nhiệm vụ riêng lẻ này.

Nhìn chung, về các nhiệm vụ sinh mã từ văn bản, các mô hình được đào tạo bằng phương pháp MFT vượt trội hơn cả các mô hình được tinh chỉnh riêng lẻ và các mô hình được tinh chỉnh sau khi kết hợp nhiều nhiệm vụ.

4.3.3 Sinh Chú thích Mã

Mục tiêu của nhiệm vụ sinh chú thích mã là có các mô hình thêm các chú thích cần thiết vào mã mà không sửa đổi chính mã đầu vào. Điều này bao gồm cả chú thích dòng và chú thích giao diện, làm cho mã dễ đọc và thân thiện với người dùng hơn.

Để đánh giá khả năng này, chúng tôi xây dựng một tập hợp đánh giá dựa trên 500 câu hỏi kiểm tra MBPP (id xx-xx). Đối với mỗi câu hỏi trong tập hợp đánh giá, chúng tôi có các mô hình SFT-S-CODECOMMENT, SFT-MIXED và MFT-5TASKS sinh chú thích cho nó. Sau đó, chúng tôi sử dụng GPT-4 làm trọng tài, được hướng dẫn với tiêu chí cho chú thích tốt, để xác định mô hình nào hoạt động tốt nhất. Nếu không thể xác định được, đầu ra được gán nhãn là UNKNOWN. Cuối cùng, chúng tôi đếm số lượng câu hỏi mà mỗi mô hình được xác định là hoạt động tốt nhất và tính toán tỷ lệ tương ứng, được hiển thị trong Bảng 4.

Có thể quan sát thấy rằng 38.8% câu hỏi được xác định là được thực hiện tốt nhất bởi mô hình MFT-5TASKS, vượt qua mô hình SFT-MIXED đứng thứ hai 7.4% và mô hình SFT-S-CODECOMMENT đứng thứ ba 10.8%. Thêm vào đó, 1.8% câu hỏi được đánh dấu là không thể xác định bởi GPT-4. Tóm lại, đối với nhiệm vụ này, các mô hình được đào tạo bằng phương pháp MFT thể hiện hiệu suất tốt nhất.

4.3.4 Dịch Mã

Mục tiêu của nhiệm vụ dịch mã là dịch một cách chính xác và chính xác một đoạn mã đã cho được triển khai trong ngôn ngữ nguồn thành một đoạn mã tương đương được triển khai trong ngôn ngữ đích trong khi đảm bảo rằng cả hai triển khai đều có chức năng giống hệt nhau. Ở đây, chúng tôi sử dụng tập con dịch mã của các bộ dữ liệu đánh giá CODEFUSE EVAL⁵ Di et al. (2023) để hỗ trợ dịch thuật hai chiều giữa Java, Python, và C++. Để đánh giá độ chính xác và tương đương chức năng của kết quả dịch thuật, chúng tôi sử dụng các test case tương đương về mặt ngữ nghĩa với những test case của chương trình nguồn cho mỗi nhiệm vụ. Các test case này được sử dụng để xác minh liệu mã kết quả có thể chạy và vượt qua thành công hay không, như được chỉ ra bởi tiêu chí pass@1.

Kết quả kiểm tra của ba mô hình được trình bày trong Bảng 5. Mô hình MFT-5TASKS hoạt động tốt nhất trong dịch thuật Python-sang-Java, Python-sang-C++, và C++-sang-Java. Mô hình SFT-MIXED xuất sắc trong dịch thuật C++-sang-Python, trong khi mô hình SFT-S-CODETRANS hoạt động tốt nhất trong dịch thuật Java-sang-Python và Java-sang-C++. Nhìn chung, mô hình MFT-5TASKS thể hiện hiệu suất vượt trội, với cải thiện trung bình 0.93% so với SFT-MIXED và 10.9% so với SFT-S-CODETRANS. Nhiệm vụ này cũng nhấn mạnh hiện tượng rằng các mô hình được đào tạo bằng phương pháp MFT vượt trội hơn hai phương pháp đào tạo khác.

4.3.5 Sinh Test Case Đơn vị

Nhiệm vụ đang xem xét là sinh test case đơn vị bằng cách đào tạo một mô hình để tạo ra một tập hợp test case cho một đoạn mã đã cho, như một phương thức hoặc lớp, và xác minh liệu triển khai mã được cung cấp có đúng hay không. Chúng tôi đã lựa chọn sử dụng tập con unittest từ các bộ dữ liệu đánh giá CODEFUSE EVAL Di et al. (2023) làm bộ kiểm tra của chúng tôi. Chúng tôi đánh giá các test case sử dụng chỉ số pass@1, có nghĩa là nếu một mô hình sinh test case cho một chương trình mẫu và chương trình mẫu vượt qua tất cả các test case, số lượng mẫu được sinh chính xác tăng lên 1. Tương tự như CodeLLama Rozière et al. (2023), chúng tôi sử dụng chiến lược giải mã tham lam trong quá trình đánh giá.

Chúng tôi so sánh ba mô hình trên Python, Java, và JavaScript về khả năng sinh test của chúng. Kết quả trong Bảng 6 cho thấy mô hình MFT-5TASKS vượt trội hơn những mô hình khác trong sinh test cho Python, với 5.73% dẫn đầu so với mô hình SFT-MIXED đứng thứ hai, và 10.19% dẫn đầu đáng kể so với mô hình SFT-S-UNITTEST đứng thứ ba. Trong JavaScript, mô hình MFT-5TASKS cũng xuất sắc, với 7.93% lợi thế so với các mô hình khác. Tuy nhiên, trong Java, mô hình MFT-5TASKS hoạt động tốt hơn 5.37% so với SFT-S-UNITTEST, nhưng hơi tụt hậu so với SFT-MIXED 5.44%. Nhìn chung, mô hình MFT-5TASKS nhất quán thể hiện hiệu suất cao nhất, với cải thiện trung bình 2.74% so với SFT-MIXED, và cải thiện đáng kể 7.83% so với SFT-S-UNITTEST.

Tóm lại, việc đào tạo các mô hình bằng phương pháp MFT mang lại hiệu suất tốt hơn so với các mô hình được tinh chỉnh với cùng dữ liệu được trộn lẫn với nhau. Hơn nữa, hiệu suất của các mô hình được đào tạo MFT vượt qua hiệu suất của mô hình được tinh chỉnh riêng lẻ cho nhiệm vụ UNIT-TEST.

Bảng 6: So sánh hiệu suất chỉ số pass@1 trên codefuseEVAL-TestcaseGeneration Di et al. (2023) (zero-shot, greedy-decoding)

Mô hình Được đào tạo | Python | Java | JavaScript | Trung bình
SFT-S-UNITTEST | 33.76% | 32.43% | 41.46% | 35.88%
SFT-MIXED | 38.22% | 43.24% | 41.46% | 40.97%
MFT-5TASKS | 43.95% | 37.8% | 49.39% | 43.71%

--- TRANG 12 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 7: Mẫu các Test Case Text-to-SQL từ bộ dữ liệu Spider Yu et al. (2019b) và Ví dụ Dự đoán bởi Mô hình SFT-Mixed và Mô hình MFT-5Tasks.

PROMPT:<|role_start|>system<|role_end|>Bạn là một kỹ sư SQL chuyên nghiệp và bạn đang viết các truy vấn SQL cho các tác vụ truy vấn dữ liệu.
<|role_start|>human<|role_end|>Tên của những người đứng đầu sinh ra ngoài bang California là gì?
schema:head(head_ID|name|born_state|age
<|role_start|>bot<|role_end|>

LABEL:SELECT name FROM head WHERE born_state != 'California'

SFT-MIXED:Bạn có thể sử dụng câu lệnh SQL SELECT để lấy tên của những người đứng đầu sinh ra ngoài bang California. Đây là một truy vấn mẫu:

"'sql
SELECT name
FROM head
WHERE born_state <> 'California';
"'

Truy vấn này chọn cột name từ bảng head nơi cột born_state không bằng 'California'.

MFT-5TASKS:SELECT name
FROM head
WHERE born_state <> 'California';

4.3.6 Tổng quát hóa trên một Nhiệm vụ Chưa thấy

Ngoài việc đánh giá hiệu suất của các mô hình của chúng tôi trên các nhiệm vụ có dữ liệu đào tạo để giải quyết RQ1 và RQ2, chúng tôi cũng có RQ3 để trả lời, đó là liệu các mô hình được đào tạo bằng phương pháp MFT có thể hiện khả năng tổng quát hóa tốt hơn trên các nhiệm vụ chưa thấy so với các mô hình được đào tạo bằng cách kết hợp nhiều bộ dữ liệu thành một phương pháp SFT duy nhất. Để điều tra điều này, chúng tôi chọn nhiệm vụ sinh SQL từ văn bản làm mục tiêu kiểm tra. Dữ liệu cho nhiệm vụ này không được bao gồm trong việc đào tạo bảy mô hình hiện có. Hơn nữa, nhiệm vụ này liên quan đến mã nhưng khác biệt rõ ràng với năm nhiệm vụ phía dưới trước đó.

Chúng tôi đã chọn hai chỉ số đánh giá, điểm BLEU và độ chính xác logic của các câu lệnh SQL. Điểm BLEU đánh giá sự tương tự văn bản giữa các đầu ra được sinh ra và các câu trả lời tham chiếu. Mặt khác, chỉ số độ chính xác logic cho phép chúng tôi giải quyết các biến thể trong cú pháp SQL có thể xảy ra. Cụ thể, Độ chính xác Logic đo tỷ lệ các mẫu kiểm tra trong bộ dữ liệu nơi các câu lệnh SQL được sinh ra vừa đúng cú pháp vừa tương đương về mặt ngữ nghĩa với các câu trả lời tham chiếu.

Chúng tôi chọn năm bộ dữ liệu text-to-SQL đại diện, bao gồm WikiSQL Zhong et al. (2017), Spider Yu et al. (2019b), CSpider Min et al. (2019), CoSQL Yu et al. (2019a), và BirdSQL Li et al. (2023d), và lấy mẫu ngẫu nhiên 200 ví dụ từ mỗi bộ dữ liệu để đánh giá. Các ví dụ test case được hiển thị trong Bảng 7, nơi dòng đầu tiên thể hiện định dạng dữ liệu được tinh chỉnh tương tự như định dạng OpenAI ChatML⁶. Sử dụng mỗi bộ dữ liệu được lấy mẫu, chúng tôi kiểm tra độ chính xác logic và điểm BLEU của các mô hình SFT-MIXED và MFT-5TASKS, như được hiển thị trong Bảng 8.

Theo Bảng 8, MFT-5TASKS vượt trội hơn SFT-MIXED về điểm BLEU trên từng bộ dữ liệu, trung bình cao hơn 2.78 lần. Điều này cho thấy rằng các kết quả được sinh ra của MFT-5TASKS thể hiện sự tương tự cao hơn với các văn bản câu trả lời tham chiếu. Sự tương tự này cũng có thể được quan sát trong Bảng 7, nơi MFT-5TASKS tạo ra kết quả sạch sẽ hơn, trong khi SFT-MIXED cung cấp nhiều giải thích hơn, điều này có thể được ưa thích trong một số tình huống nhất định. Hơn nữa, MFT-5TASKS thể hiện hiệu suất tốt hơn về độ chính xác logic, đạt độ chính xác tổng thể cao hơn 2.18 lần so với mô hình SFT-MIXED, và lên đến 4.67 lần cao hơn trên bộ dữ liệu WikiSQL. Về mặt số lượng, MFT-5TASKS thể hiện hiệu suất vượt trội so với SFT-MIXED, cho thấy khả năng tổng quát hóa mạnh hơn của các mô hình được đào tạo MFT trên nhiệm vụ Text-to-SQL, một nhiệm vụ chưa thấy trong quá trình đào tạo.

⁶https://github.com/openai/openai-python/blob/main/chatml.md

--- TRANG 13 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 8: So sánh khả năng tổng quát hóa giữa MFT-5TASKS và SFT-MIXED trên nhiệm vụ Text-to-SQL. Các chỉ số đánh giá bao gồm độ chính xác logic SQL và điểm BLEU.

Mô hình Được đào tạo | WIKISQL | SPIDER | CSPIDER | COSQL | BiRDSQL | Trung bình

Độ chính xác Logic
SFT-MIXED | 1.5% | 2.0% | 7.0% | 6.5% | 5.5% | 4.5%
MFT-5TASKS | 7.0% (4.67x) | 4.5% (2.25x) | 16.5% (2.36x) | 10.5%(1.62x) | 10.5% (1.91x) | 9.8% (2.18x)

BLEU
SFT-MIXED | 0.032 | 0.047 | 0.025 | 0.081 | 0.026 | 0.042
MFT-5TASKS | 0.138 | 0.138 | 0.116 | 0.119 | 0.074 | 0.117

4.4 Tóm tắt Đánh giá

Chúng tôi chọn năm nhiệm vụ phía dưới liên quan đến mã và đào tạo tổng cộng bảy mô hình, bao gồm các mô hình SFT-S-* được tinh chỉnh riêng lẻ cho từng nhiệm vụ, mô hình SFT-MIXED được tinh chỉnh với hỗn hợp tất cả dữ liệu nhiệm vụ, và mô hình MFT-5TASKS được đào tạo bằng phương pháp MFT. Chúng tôi so sánh và kiểm tra hiệu suất của mỗi mô hình về khả năng mục tiêu của chúng. Thêm vào đó, chúng tôi đánh giá hiệu suất tổng quát hóa của phương pháp MFT và phương pháp SFT hỗn hợp trên các nhiệm vụ chưa thấy. Kết quả có thể được tóm tắt như sau:

i Các mô hình được đào tạo bằng phương pháp MFT vượt trội hơn những mô hình được tinh chỉnh riêng lẻ cho từng nhiệm vụ, cho thấy câu trả lời tích cực cho RQ1.

ii Các mô hình được đào tạo bằng phương pháp MFT vượt trội hơn những mô hình được tinh chỉnh với hỗn hợp nhiều nhiệm vụ, cung cấp câu trả lời tích cực cho RQ2.

iii Các mô hình được đào tạo bằng phương pháp MFT thể hiện khả năng tổng quát hóa mạnh hơn trên các nhiệm vụ mới, chưa thấy so với các mô hình SFT được tinh chỉnh với hỗn hợp dữ liệu nhiều nhiệm vụ.

5 Ứng dụng

Xét hiệu suất xuất sắc của phương pháp đào tạo MFT, chúng tôi đã tận dụng MFTCoder³ của chúng tôi, được phát triển dựa trên phương pháp này, để tinh chỉnh các mô hình LLM mã nguồn mở chính hiện có. ví dụ QWen Bai et al. (2023), Baichuan Baichuan (2023), CodeGeex2 Zheng et al. (2023), Llama Touvron et al. (2023a), LLama2 Touvron et al. (2023b), CodeLLama Rozière et al. (2023), StarCoder Li et al. (2023a).

MFTCoder hỗ trợ Lora và QLora, giảm đáng kể số lượng tham số đào tạo mô hình. Kết hợp với lượng tử hóa kép để nén kích thước mô hình, điều này cuối cùng dẫn đến giảm đáng kể yêu cầu bộ nhớ GPU. Kết quả là, việc tinh chỉnh một mô hình 70B trên một GPU A100 duy nhất trở nên dễ dàng. Khi tinh chỉnh các mô hình này bằng MFTCoder, chúng tôi đặt các tham số có thể đào tạo nằm trong khoảng 0.1% đến 5% tổng số tham số. Thông qua bằng chứng thực nghiệm, chúng tôi phát hiện rằng khi tỷ lệ tham số có thể đào tạo tăng lên, cải thiện hiệu suất có xu hướng ổn định. Thực tế, chúng tôi đã quan sát thấy rằng tỷ lệ tham số có thể đào tạo dưới 5% thường đủ để đạt được mức hiệu suất gần với tinh chỉnh quy mô đầy đủ.

Khi tinh chỉnh các mô hình này, chúng tôi cấu hình chúng cho đa nhiệm vụ với khoảng 3-7 nhiệm vụ. Tùy thuộc vào kích thước mô hình, chúng tôi thường sử dụng chế độ Lora cho các mô hình dưới 20B, và chế độ QLora cho các mô hình lớn hơn 20B. Sau khi tinh chỉnh, chúng tôi đánh giá hiệu suất của chúng trong các nhiệm vụ hoàn thành mã và sinh mã từ văn bản, đo hiệu suất của chúng trên HumanEval Chen et al. (2021) và MBPP Austin et al. (2021), như được hiển thị trong Bảng 9 Cột III và IV. Chúng tôi đã tính toán cải thiện trung bình của tinh chỉnh MFT so với các mô hình cơ sở về HumanEval và MBPP. Như được hiển thị trong cột 5, cải thiện từ 6.26% đến 12.75%, với các cải thiện trên HumanEval nhất quán vượt qua những cải thiện trên MBPP. Thêm vào đó, chúng tôi cũng đã đánh giá hiệu suất hoàn thành mã của các mô hình tinh chỉnh MFTCoder trên benchmark đa ngôn ngữ, HumanEval-X Zheng et al. (2023). Kết quả của đánh giá này được trình bày trong Bảng 10. Đáng chú ý, mô hình CodeFuse-CodeLLama-Python-MFT (34B) được tinh chỉnh đạt được pass@1 trung bình 56.88% trên bốn ngôn ngữ: Java, C++, JavaScript, và Golang.

Bảng 9 cũng trình bày hiệu suất của các mô hình mã nguồn mở được tinh chỉnh (ví dụ OctoPack Muennighoff et al. (2023) và WizardCoder-Python Luo et al. (2023)) và các mô hình mã nguồn đóng đại diện (ví dụ, Claude2 Anthropic (2023), GPT-4 OpenAI (2023)) trên HumanEval và MBPP. Đáng chú ý là mô hình tinh chỉnh của chúng tôi, CodeFuse-CodeLLama-34B⁷, dựa trên CodeLlama-34B-Python đạt được hiệu suất xuất sắc 74.4% trên HumanEval, vượt qua

⁷https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B

--- TRANG 14 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 9: hiệu suất pass@1 trên HumanEval Chen et al. (2021) (Hoàn thành Mã) và MBPP Austin et al. (2021) (Sinh mã từ Văn bản) sau khi tinh chỉnh với MFTCoder trên nhiều mô hình mã nguồn mở chính. Các mô hình CodeFuse-*-MFT được đánh giá bằng cách sử dụng kết hợp chiến lược giải mã tham lam và kiểm tra zero-shot, trong khi các giá trị chỉ số cho các mô hình khác được lấy từ các bài báo, báo cáo kỹ thuật, hoặc trang chủ dự án mã nguồn mở tương ứng của chúng.

Mô hình | Kích thước | HumanEval pass@1 | MBPP pass@1 | Trung bình

Các mô hình cơ sở mã nguồn mở
QWen-base Bai et al. (2023) | 14B | 32.3% | 40.8% | 36.55%
Llama-base Touvron et al. (2023a) | 65B | 23.7% | 37.7% | 30.7%
Llama2-base Touvron et al. (2023b) | 70B | 29.9% | 45.0% | 37.45%
StarCoder-base Li et al. (2023a) | 15B | 33.6% | 52.7% | 43.15%
CodeGeex2-base Zheng et al. (2023) | 6B | 35.9% | 42.4% | 39.15%
CodeLlama-Python-base Rozière et al. (2023) | 13B | 43.3% | 49.0% | 46.15%
CodeLlama-Python-base Rozière et al. (2023) | 34B | 53.7% | 56.2% | 54.95%

Các mô hình tinh chỉnh MFT
CodeFuse-QWen-MFT⁸ | 14B | 48.78% | 43.8% | 46.29% (+9.74%)
CodeFuse-Llama-MFT | 65B | 34.76% | 41.8% | 38.28% (+7.58)
CodeFuse-Llama2-MFT | 70B | 40.85% | 40.8% | 40.83% (+3.38%)
CodeFuse-StarCoder-MFT⁹ | 15B | 54.90% | 49.60% | 52.25% (+9.10%)
CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 46.2% | 45.66% (+6.51%)
CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 56.0% | 58.19% (+12.04%)
CodeFuse-CodeLLama-Python-MFT⁷ | 34B | 74.4% | 61.0% | 67.70% (+12.75%)

Các mô hình tinh chỉnh mã nguồn mở
QWen-chat Bai et al. (2023) | 14B | 43.9% | 46.4% | 45.15%
PHI-1 Gunasekar et al. (2023) | 1.3B | 50.6% | 55.5% | 53.05%
OctoCoder Muennighoff et al. (2023) | 15B | 46.2% | NA | NA
WizardCoder Luo et al. (2023) | 15B | 57.3% | 51.8% | 54.55%
Phind-CodeLlama-v2 Phind (2023) | 34B | 71.95% | NA | NA
WizardCoder-Python Luo et al. (2023) | 34B | 73.2% | 61.2% | 67.2%

Các mô hình mã nguồn đóng
PanGu-Coder2 Shen et al. (2023) | 15B | 61.2% | NA | NA
Unnatural CodeLlama Rozière et al. (2023) | 34B | 62.2% | 61.2% | 61.7%
Claude2 Anthropic (2023) | NA | 71.2% | NA | NA
GPT-3.5 OpenAI (2023) | 175B | 48.1% | 52.2% | 50.15%
GPT-4 (zero-shot) OpenAI (2023) | NA | 67.00% | NA | NA

tất cả các mô hình được liệt kê trong bảng, bao gồm GPT-4 (67.00%, zero-shot) OpenAI (2023). Chúng tôi cũng đánh giá hiệu suất của mô hình trên các benchmark khác, bao gồm đa ngôn ngữ HUMAN EVAL-X Zheng et al. (2023), MBPP Austin et al. (2021), DS-1000 Lai et al. (2022) và CODEFUSE EVAL Di et al. (2023), và so sánh với GPT-3.5 và GPT-4, như được hiển thị trong Hình 5. CodeFuse-CodeLLama-34B vượt trội hơn GPT-4 trên CODEFUSEEVAL-UNITTEST và HUMANEVAL, phù hợp với hiệu suất của nó trong khả năng dịch mã, nhưng tụt hậu trong hoàn thành mã tiếng Trung (CODEFUSEEVAL-CN), hoàn thành đa ngôn ngữ, phân tích khoa học dữ liệu (DS-1000), và khả năng sinh mã từ văn bản (MBPP) so với GPT-4. Tuy nhiên, nó vượt qua hoặc bằng GPT-3.5 trên tất cả các bộ dữ liệu đánh giá. Các ví dụ đầu vào-đầu ra trên mỗi bộ dữ liệu đánh giá có thể được tìm thấy trong Phụ lục C.

Hơn nữa, chúng tôi thực hiện đánh giá để đánh giá tác động của việc tinh chỉnh các mô hình với MFTCoder và dữ liệu liên quan đến mã đối với hiệu suất của chúng trong các nhiệm vụ NLP, như được minh họa trong Hình 6. Lấy CODEFUSE-QWEN-14B làm nghiên cứu trường hợp, chúng tôi so sánh nó với mô hình cơ sở QWEN-14B và mô hình chính thức QWEN-14B-CHAT được tinh chỉnh bởi Alibaba Cloud trên đó. Rõ ràng là CODEFUSE-QWEN-14B duy trì khả năng thành thạo NLP của nó. Thực tế, nó thể hiện sự cải thiện nhẹ trong khả năng ngôn ngữ, lập luận và hiểu biết so với hai mô hình khác. Tuy nhiên, có một sự suy giảm nhỏ trong khả năng thi cử của nó khi so sánh với mô hình cơ sở QWEN-14B và các phát hiện tương tự được quan sát đối với mô hình QWEN-14B-CHAT được tinh chỉnh.

⁸https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B
⁹https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B

--- TRANG 15 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Bảng 10: hiệu suất pass@1 trên HumanEval-X Đa ngôn ngữ (Zheng et al., 2023) sau khi tinh chỉnh với MFTCoder trên nhiều mô hình mã nguồn mở chính. Các giá trị chỉ số được đánh dấu bằng dấu sao (*) được lấy từ các bài báo, báo cáo kỹ thuật, hoặc trang chủ dự án mã nguồn mở tương ứng của các mô hình, trong khi các giá trị chỉ số còn lại được đánh giá bằng cách sử dụng kết hợp chiến lược giải mã tham lam và kiểm tra zero-shot.

Mô hình | Kích thước | Python | Java | C++ | JavaScript | Golang | Trung bình
QWen-base | 14B | 32.3%* | 35.37% | 30.49% | 32.93% | 21.34% | 30.49%
CodeFuse-QWen-MFT | 14B | 48.78% | 41.46% | 38.41% | 46.34% | 26.83% | 40.36%
Llama-base | 65B | 23.7%* | 29.26% | 20.73% | 23.78% | 18.9% | 23.27%
CodeFuse-Llama-MFT | 65B | 34.76% | 37.2% | 29.88% | 32.93% | 23.78% | 31.71%
Llama2-base | 70B | 29.9%* | 39.02% | 31.10% | 35.98% | 23.78% | 31.96%
CodeFuse-Llama2-MFT | 70B | 40.85% | 35.98% | 32.32% | 38.41% | 27.44% | 35.00%
StarCoder-base | 15B | 33.6%* | 34.15% | 25.61% | 22.56% | 22.56% | 29.48%
CodeFuse-StarCoder-MFT | 15B | 54.9% | 47.56 | 46.34% | 48.17% | 37.20% | 46.83%
CodeGeex2-base | 6B | 35.9%* | 30.8%* | 29.3%* | 32.2%* | 22.5%* | 30.14%
CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 45.73% | 37.2% | 37.2% | 28.05% | 38.66%
CodeLlama-Python-base | 13B | 43.3%* | 41.46% | 34.76% | 38.41% | 29.27% | 37.44%
CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 57.32% | 46.34% | 54.27% | 45.12% | 52.68%
CodeLlama-34B-Python-base | 34B | 53.7%* | 45.73% | 42.68% | 45.73% | 31.71% | 43.91%
CodeFuse-CodeLLama-Python-MFT | 34B | 74.4% | 61.6% | 54.3% | 61.0% | 50.6% | 60.38%

[THIS IS FIGURE: Biểu đồ radar cho thấy hiệu suất của mô hình CodeFuse-CodeLlama-34B trên các benchmark HUMAN EVAL, HUMAN EVAL-X, MBPP, DS-1000, và CODEFUSE EVAL so với GPT-3.5 và GPT-4.]

Hình 5: Biểu đồ Radar của Mô hình CodeFuse-CodeLlama-34B trên các benchmark HUMAN EVAL, HUMAN EVAL-X, MBPP, DS-1000, và CODEFUSE EVAL so với GPT-3.5 và GPT-4.

6 Thảo luận

Bất chấp hiệu suất vượt trội của phương pháp đào tạo MFT so với phương pháp đào tạo SFT dựa trên trộn dữ liệu nhiệm vụ trong các thí nghiệm nói trên, cần lưu ý rằng hiệu quả của phương pháp MFT phụ thuộc rất nhiều vào chiến lược chia tách nhiệm vụ. Không phải tất cả các tình huống đều phù hợp để được chia thành nhiều nhiệm vụ. Ví dụ, trong kinh nghiệm thực tế của chúng tôi, chúng tôi phát hiện rằng việc chia tách một nhiệm vụ dựa trên mức độ khó và đào tạo nó bằng phương pháp MFT không mang lại kết quả tốt hơn so với phương pháp đào tạo SFT trộn nhiệm vụ. Hơn nữa, việc đào tạo các nhiệm vụ hoàn thành mã như nhiều nhiệm vụ dựa trên ngôn ngữ lập trình cũng không vượt trội hơn phương pháp SFT trộn. Dựa trên kinh nghiệm thực tế của chúng tôi, chúng tôi đã kết luận rằng các nhiệm vụ có khả năng mong muốn chính riêng biệt phù hợp hơn cho việc chia tách nhiệm vụ

--- TRANG 16 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

[THIS IS FIGURE: Biểu đồ radar so sánh hiệu suất của CODEFUSE-QWEN-14B được tinh chỉnh với MFTCoder và dữ liệu liên quan đến mã, mô hình cơ sở QWEN-14B, và mô hình được tinh chỉnh chính thức QWEN-14B-CHAT trên các bộ dữ liệu đánh giá NLP. Các chỉ số được hiển thị cho Ngôn ngữ, Lập luận, Hiểu biết và Thi cử.]

Hình 6: So sánh hiệu suất của CODEFUSE-QWEN-14B được tinh chỉnh với MFTCoder và dữ liệu liên quan đến mã, mô hình cơ sở QWEN-14B, và mô hình được tinh chỉnh chính thức QWEN-14B-CHAT trên các bộ dữ liệu đánh giá NLP. Dữ liệu chi tiết có thể được tìm thấy trong Phụ lục D.

và đào tạo MFT, trong khi các nhiệm vụ có mục tiêu đào tạo chính tương tự không phù hợp với đào tạo MFT. Chúng tôi dự định điều tra thêm và thiết lập các tiêu chí chính xác hơn cho việc phân định nhiệm vụ trong nghiên cứu tương lai.

Trong các thí nghiệm tổng quát hóa nhiệm vụ của chúng tôi, chúng tôi quan sát thấy các mô hình được đào tạo bằng phương pháp MFT tạo ra kết quả suy luận tương tự hơn với các câu trả lời tham chiếu và có nội dung súc tích hơn. Ngược lại, kết quả suy luận được tạo ra bởi phương pháp đào tạo SFT trộn nhiệm vụ chứa nhiều thông tin Chuỗi Suy nghĩ (CoT) hơn. Trong một số tình huống nhất định, phương pháp trước được ưa thích hơn, như trong các plugin IDE, trong khi phương pháp sau được ưa chuộng trong các tình huống khác, như trợ lý web. Kết quả là, chúng tôi không thể đơn giản tổng quát hóa rằng một phương pháp tốt hơn phương pháp khác. Chúng tôi hiện đang nghiên cứu lý do đằng sau những khác biệt hiệu suất này.

Là một phương pháp học đa nhiệm vụ, MFT cũng đối mặt với một thách thức lớn trong quá trình đào tạo: tốc độ hội tụ không nhất quán giữa các nhiệm vụ khác nhau. Ví dụ, trong các thí nghiệm nói trên, nhiệm vụ hoàn thành mã hội tụ nhanh hơn nhiều so với nhiệm vụ sinh test case đơn vị (chi tiết có thể được tìm thấy trong Phụ lục B). Điều này khiến việc tìm một điểm tối ưu hoạt động tốt trên tất cả các nhiệm vụ trở nên khó khăn. Checkpoint được chọn hoặc hội tụ không đủ trên một số nhiệm vụ hoặc quá khớp trên các nhiệm vụ khác. Để giải quyết vấn đề này, chúng tôi đã thí nghiệm với các giải pháp tối ưu hóa cân bằng học đa nhiệm vụ hiện có như FAMO Liu et al. (2023). Tuy nhiên, FAMO đòi hỏi lan truyền ngược kép trong mỗi lần lặp, dẫn đến thời gian đào tạo tăng khoảng gấp đôi. Hơn nữa, số lượng epoch cần thiết để hội tụ cũng tăng đáng kể, và khả năng điều chỉnh tốc độ hội tụ bị hạn chế. Thật không may, chi phí tăng theo cấp số nhân này không mang lại lợi ích tương đương. Để đáp ứng, chúng tôi hiện đang phát triển một phương pháp cân bằng tối ưu hóa đa nhiệm vụ tối ưu và thích ứng hơn.

Hơn nữa, ngay cả sau khi cân bằng tốc độ hội tụ của nhiều nhiệm vụ, nơi cùng một tập hợp tham số được cập nhật, vẫn thách thức để loại bỏ cơ bản các xung đột vốn có trong cập nhật trọng số trên các nhiệm vụ khác nhau. Để giải quyết vấn đề này, chúng tôi hiện đang khám phá việc sử dụng MoE (Hỗn hợp các Chuyên gia) Chen et al. (2022) để đạt được MFT.

7 Kết luận

Bài báo này giới thiệu MFTCoder, một khung hỗ trợ tinh chỉnh đa nhiệm vụ, giải quyết hiệu quả các thách thức về mất cân bằng dữ liệu, mức độ khó khác nhau, và tốc độ hội tụ không nhất quán thông qua thiết kế các hàm mất mát khác nhau. Kết quả thí nghiệm cho thấy phương pháp này vượt trội hơn việc tinh chỉnh riêng lẻ trên từng nhiệm vụ hoặc tinh chỉnh trên tập hợp hỗn hợp các nhiệm vụ. Thêm vào đó, MFTCoder tạo thuận lợi cho đào tạo hiệu quả, bao gồm sử dụng dữ liệu hiệu quả và đào tạo PEFT. Nó cũng cung cấp giải pháp xây dựng bộ dữ liệu hướng dẫn chất lượng cao. Tận dụng MFTCoder để tinh chỉnh trên cơ sở CodeLLama, mô hình CodeFuse-CodeLLama-34B đạt được điểm pass@1 ấn tượng 74.4% trên bộ dữ liệu HumanEval, vượt qua hiệu suất của GPT-4 (67%, zero-shot).

--- TRANG 17 ---
MFTCoder: Tăng cường Code LLM với Tinh chỉnh Đa nhiệm vụ BẢN THẢO

Tài liệu tham khảo

[Tiếp tục với danh sách tài liệu tham khảo đầy đủ được dịch sang tiếng Việt...]
