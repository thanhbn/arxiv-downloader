# Một Khung Học Liên Tục Thống Nhất với Tùy Chỉnh Tham Số Hiệu Quả Tổng Quát

## Tóm tắt

Mô hình "tiền huấn luyện → thích ứng hạ nguồn" mang lại cả cơ hội và thử thách mới cho Học Liên Tục (CL). Mặc dù trình độ tiên tiến nhất gần đây trong CL đạt được thông qua mô hình thích ứng Tùy Chỉnh Tham Số Hiệu Quả (PET), chỉ có prompt được khám phá, hạn chế ứng dụng chỉ cho Transformers. Trong bài báo này, chúng tôi đặt prompting như một thể hiện cụ thể của PET, và đề xuất một khung CL thống nhất với PET tổng quát, được gọi là Learning-Accumulation-Ensemble (LAE). PET, ví dụ, sử dụng Adapter, LoRA, hoặc Prefix, có thể thích ứng mô hình tiền huấn luyện với các tác vụ hạ nguồn với ít tham số và tài nguyên hơn. Với một phương pháp PET, khung LAE của chúng tôi kết hợp nó cho CL với ba thiết kế mới. 1) Học: mô hình tiền huấn luyện thích ứng với tác vụ mới bằng cách điều chỉnh một mô-đun PET trực tuyến, cùng với việc hiệu chỉnh tốc độ thích ứng của chúng tôi để căn chỉnh các mô-đun PET khác nhau, 2) Tích lũy: kiến thức đặc trưng cho tác vụ đã học bởi mô-đun PET trực tuyến được tích lũy vào một mô-đun PET ngoại tuyến thông qua cập nhật momentum, 3) Tổng hợp: Trong quá trình suy luận, chúng tôi tương ứng xây dựng hai chuyên gia với các mô-đun PET trực tuyến/ngoại tuyến (được ưa chuộng bởi các tác vụ mới/lịch sử) cho tổng hợp dự đoán. Chúng tôi chỉ ra rằng LAE tương thích với một loạt các phương pháp PET và đạt được khả năng CL mạnh mẽ. Ví dụ, LAE với Adaptor PET vượt trội so với trình độ tiên tiến nhất trước đây lần lượt 1.3% và 3.6% trong độ chính xác tăng dần cuối cùng trên các tập dữ liệu CIFAR100 và ImageNet-R. Mã nguồn có sẵn tại https://github.com/gqk/LAE.

## 1. Giới thiệu

Học Liên Tục (CL) kiến thức mới là một khả năng thiết yếu cho các mô hình AI trong thế giới liên tục thay đổi. Tuy nhiên, các mạng nơ-ron thường gặp phải hiện tượng quên thảm khốc [9, 39], trong đó kiến thức đã học trước đây bị quên khi mô hình kết hợp thông tin mới. Mặc dù nhiều công trình đã được dành để giảm thiểu việc quên, như mạng động [41, 50, 26, 18], chính quy hóa [28, 20, 52, 1], và phát lại bộ nhớ [38, 15, 6, 30, 2], hiệu suất của chúng vẫn không đạt đến các yêu cầu thực tế.

Gần đây, các kỹ thuật tiền huấn luyện và thích ứng hạ nguồn đã mở ra các cơ hội và thử thách mới cho CL. Về cơ bản, các kỹ thuật này [4, 36, 12, 11, 48] tiền huấn luyện một mô hình sâu trên dữ liệu quy mô lớn và sau đó thích ứng mô hình tiền huấn luyện với các tác vụ mới. Chúng tôi quan sát thấy rằng thích ứng hạ nguồn và CL quan trọng với nhau. Một mặt, trong các hệ thống AI thực tế, các mô hình tiền huấn luyện đôi khi cần được thích ứng với nhiều tác vụ hạ nguồn tuần tự, tạo ra nhu cầu về CL. Mặt khác, các nỗ lực gần đây [47, 46, 45] cho thấy rằng các kỹ thuật "tiền huấn luyện → thích ứng hạ nguồn" có thể tăng cường hiệu suất CL.

Cụ thể, L2P [47], DualPrompt [46], và ESN [45] đều sử dụng một kỹ thuật thích ứng phổ biến có tên là Tùy Chỉnh Tham Số Hiệu Quả (PET). Nói chung, PET thích ứng các mô hình tiền huấn luyện với các tác vụ hạ nguồn với ít tham số có thể học được hơn nhiều, cũng như ít tài nguyên hơn. Mặc dù các phương pháp này đã tiến bộ trong trình độ tiên tiến nhất trong CL, chúng vẫn có một số hạn chế. 1) Chúng đều bị hạn chế với một phương pháp PET cụ thể, tức là prompt tuning, hạn chế tính linh hoạt của chúng, xét rằng prompt chỉ có thể hợp tác với transformers và không phù hợp với các kiến trúc mạng khác. 2) Hầu hết chúng dựa vào việc lựa chọn các tham số đặc trưng cho tác vụ (đặc biệt là các token prompt) cho từng tác vụ riêng lẻ. Việc lựa chọn có xu hướng nhiễu với số lượng tác vụ tăng lên và các prompt đặc trưng cho tác vụ xuất hiện đồng nhất, theo điều tra của chúng tôi trong phần bổ sung.

Để khắc phục những vấn đề này, bài báo này đề xuất Learning-Accumulation-Ensemble (LAE), một khung CL thống nhất dựa trên Tùy Chỉnh Tham Số Hiệu Quả (PET) tổng quát. LAE không bị hạn chế với Prompt, mà cũng có thể sử dụng các mô-đun PET khác nhau như được hiển thị trong Hình 2 (b). Với một phương pháp PET, LAE của chúng tôi trực tiếp định hình lại nó cho CL với ba bước, tức là học, tích lũy, và tổng hợp.

•1) Học với tốc độ được hiệu chỉnh. Mô hình tiền huấn luyện thích ứng với tác vụ mới bằng cách điều chỉnh một mô-đun PET trực tuyến. Để phù hợp với các phương pháp PET khác nhau, một thử thách chính là các mô-đun PET khác nhau có tốc độ thích ứng khác nhau (cho các tác vụ mới), cũng như tốc độ quên khác nhau (cho các tác vụ lịch sử). Để phản hồi, chúng tôi thiết kế một chiến lược hiệu chỉnh thích ứng, dựa trên phân tích gradient cho các mô-đun PET khác nhau. Chúng tôi chỉ ra thông qua thực nghiệm rằng chiến lược hiệu chỉnh này căn chỉnh các PET khác nhau với nhau và rất quan trọng để LAE trở thành một khung thống nhất.

•2) Tích lũy kiến thức đa tác vụ. Sau khi thích ứng mô hình tiền huấn luyện với một tác vụ mới, các tham số trong mô-đun PET trực tuyến có xu hướng nghiêng về tác vụ mới hiện tại và có thể không phù hợp với các tác vụ lịch sử. Thay vì ghi nhớ nhiều tập hợp mô-đun PET và lựa chọn một số tập con (như trong L2P và DualPrompt) cho từng tác vụ riêng lẻ, LAE tích lũy tất cả kiến thức của các tác vụ đã thấy vào một mô-đun PET ngoại tuyến duy nhất thông qua cập nhật momentum. Việc tích lũy đơn giản này tránh lựa chọn nhiễu và có khả năng giảm thiểu việc quên thảm khốc, đặc biệt khi số lượng tác vụ đã học lớn (Hình 3 và 4 trong Phần 5).

•3) Tổng hợp hai mô hình chuyên gia. Các mô-đun PET trực tuyến và ngoại tuyến tương ứng chứa nhiều kiến thức mới và lịch sử hơn, do đó, hai mô hình chuyên gia được xây dựng với chúng tương ứng tốt hơn trong việc xử lý các tác vụ mới hơn và cũ hơn. Thay vì suy luận chỉ sử dụng mô hình chuyên gia trực tuyến hoặc ngoại tuyến, chúng tôi tích hợp các đầu ra của hai mô hình chuyên gia bằng một chỉ số năng lượng (chi tiết trong Phần 4.2) để có được dự đoán cho một mẫu suy luận từ bất kỳ tác vụ đã học nào. Chiến lược tổng hợp chuyên gia này giúp khung của chúng tôi đạt được hiệu suất mạnh mẽ hơn so với suy luận chỉ sử dụng một trong các mô hình chuyên gia.

Các đóng góp của bài báo này được tóm tắt như sau:
• Chúng tôi điều tra kỹ lưỡng mô hình Học Liên Tục mới liên tục thích ứng một mô hình tiền huấn luyện với các tác vụ mới sử dụng các phương pháp Tùy Chỉnh Tham Số Hiệu Quả (PET) tổng quát, và đề xuất một khung Learning-Accumulation-Ensemble (LAE) thống nhất.

• Khung LAE của chúng tôi định hình lại một phương pháp PET đã cho thành một phương pháp Học Liên Tục Không Bộ Nhớ cạnh tranh với ba thiết kế mới: Học với tốc độ được hiệu chỉnh, Tích lũy kiến thức đa tác vụ, và Tổng hợp hai mô hình chuyên gia được xây dựng với các mô-đun PET trực tuyến và ngoại tuyến.

• Chúng tôi tiến hành các thí nghiệm mở rộng trên các benchmark CIFAR100 và ImageNet-R, trên tất cả chúng, LAE của chúng tôi đạt được hiệu suất tăng dần vượt trội so với các phương pháp tiên tiến nhất trước đây.

## 2. Công trình Liên quan

Tùy Chỉnh Tham Số Hiệu Quả. Như một lựa chọn hiệu quả thay thế cho fine-tuning đầy đủ, Adapter-Tuning [16] được đề xuất đầu tiên để chuyển giao các mô hình Ngôn ngữ tiền huấn luyện lớn sang các tác vụ hạ nguồn. Lấy cảm hứng từ prompting văn bản, Prompt-Tuning [25] và Prefix-Tuning [27] chèn các token có thể học được để thích ứng với tác vụ mới. Các phương pháp tiên tiến hơn [51, 7, 17, 33] đạt được hiệu suất tương đương hoặc vượt trội so với fine-tuning đầy đủ, và giữ nguyên chi phí suy luận bằng cách hợp nhất các tham số có thể học được bổ sung vào mô hình tiền huấn luyện gốc. Theo bước của các Vision Transformers thành công [5, 31], VPT [19] và AdapterFormer [3] đã được đề xuất để giải quyết các vấn đề học chuyển giao thị giác. Prompt-Tuning và Prefix-Tuning phụ thuộc vào kiến trúc transformer vì chúng sửa đổi các token đầu vào hoặc ẩn. Adapter và các biến thể của nó có thể tổng quát hóa kiến trúc mạng vì chúng là các mô-đun mới có thể được thực hiện dưới dạng tương thích với các mô hình tiền huấn luyện. Tất cả các loại mô-đun Tùy Chỉnh Tham Số Hiệu Quả có thể được tích hợp vào khung LAE của chúng tôi miễn là chúng phù hợp với mô hình tiền huấn luyện, nhưng chúng tôi tập trung vào Adapter [16], LoRA [17], và Prefix [27] đại diện trong bài báo này.

Học Liên Tục. Vấn đề trung tâm của Học Liên Tục (CL) là chống lại việc quên thảm khốc [9]. Các phương pháp dựa trên bộ nhớ [38, 15, 6, 30, 2] lưu một tập con của các mẫu đã học vào bộ đệm bộ nhớ và phát lại chúng khi học một tác vụ mới. Các phương pháp Không Bộ Nhớ không dựa vào các mẫu cũ có thể gây ra lo ngại về quyền riêng tư, chúng mở rộng mạng động hoặc cô lập các tham số cho các tác vụ khác nhau [41, 50, 26, 18], chính quy hóa các tham số mạng quan trọng với các tác vụ đã học [28, 20, 52, 1], và phát lại dữ liệu sinh hoặc tổng hợp [42, 49, 43, 8]. Các phương pháp CL thông thường học các tác vụ từ đầu sử dụng một mô hình khởi tạo ngẫu nhiên, trong khi các mô hình tiền huấn luyện nhận được ít sự chú ý từ các nhà nghiên cứu CL cho đến gần đây. Hai công trình tiên phong [47, 46] giới thiệu Prompt-Tuning vào CL và đạt được hiệu suất tăng dần cao hơn nhiều so với các phương pháp trước đây, chứng minh lợi thế của việc sử dụng các mô hình tiền huấn luyện trong CL. Side-Tuning [53] áp dụng một kỹ thuật tương tự như Adapter-Tuning nhưng yêu cầu định danh tác vụ của mẫu suy luận. Trong bài báo này, chúng tôi đề xuất một khung thống nhất cho CL Không Bộ Nhớ có thể kết hợp các loại mô-đun PET khác nhau. Đặc biệt, chúng tôi tập trung vào Học Tăng Dần Lớp thực tế với tiềm năng mở rộng LAE của chúng tôi sang các tình huống CL khác trong công việc tương lai.

## 3. Kiến thức Cơ bản

### 3.1. Công thức Học Liên Tục

Chúng tôi tập trung vào Học Liên Tục với các lớp tăng dần, tức là Học Tăng Dần Lớp (CIL), nơi một mô hình học tuần tự các tác vụ T:={T1,T2,···,Tn}, tác vụ thứ i Ti có |Ti| danh mục, tập huấn luyện của Ti được ký hiệu là Di, và các danh mục không chồng lấn giữa các tác vụ. Mô hình f(·;θ,ϕ) dự đoán nhãn danh mục y∈Y cho một mẫu x∈X của các tác vụ đã học, trong đó Y là tất cả các danh mục đã thấy, θ và ϕ là tham số của bộ trích xuất đặc trưng và đầu phân loại, tương ứng. Trong bài báo này, bộ trích xuất đặc trưng là một mô hình tiền huấn luyện được tham số hóa bởi θpre được gắn với mô-đun Tùy Chỉnh Tham Số Hiệu Quả được tham số hóa bởi θpet, và ϕ=concatenate(ϕold,ϕnew), trong đó ϕold và ϕnew là các bộ phân loại của tất cả các tác vụ đã học T1:i và tác vụ học hiện tại Ti. Vì θpre và ϕold được giữ cố định trong quá trình học một tác vụ mới, chúng tôi có thể bỏ qua chúng để ngắn gọn trong phần còn lại của bài báo này.

### 3.2. Xem lại Tùy Chỉnh Tham Số Hiệu Quả

Tùy Chỉnh Tham Số Hiệu Quả (PET) giữ mô hình tiền huấn luyện đóng băng và điều chỉnh một số lượng nhỏ các tham số có thể học được bổ sung, được gọi là mô-đun PET trong bài báo. Dưới đây chúng tôi xem lại một số mô-đun PET đại diện, trong đó g là mô-đun mà PET được gắn vào, e và h là đầu vào và đầu ra của g gốc và h' là đầu ra của g được gắn với PET.

Adapter [16] là một mô-đun nhỏ có thể được chèn vào bất kỳ lớp nào (tức là g) của mô hình tiền huấn luyện. Như được hiển thị trong Hình 2 (b), adapter thường là một khối dư được cấu thành từ một phép chiếu xuống với tham số Wdown, một hàm kích hoạt phi tuyến σ(·), và một phép chiếu lên với tham số Wup. Hai phép chiếu có thể là phép tích chập [37] cho CNN hoặc các lớp tuyến tính [16] cho các kiến trúc Transformer, tương ứng. Chúng tôi công thức hóa adapter như sau:

h′=h+σ(h∗Wdown)∗Wup, (1)

trong đó ∗ là phép nhân ma trận hoặc phép tích chập, σ là hàm kích hoạt. Ngoài ra, adapter cũng có thể song song với g như một nhánh dư [53, 10]:

h′=h+σ(e∗Wdown)∗Wup. (2)

LoRA [17] giả định rằng sự thay đổi của tham số nằm trong một không gian thứ hạng thấp khi điều chỉnh mô hình tiền huấn luyện trên một tác vụ hạ nguồn. Đối với một lớp tuyến tính có trọng số W∈Rd×d′, các bản cập nhật trọng số ΔW có thể được phân tích thành phép nhân của hai ma trận nhỏ:

ΔW=WdownWup, (3)

trong đó Wdown∈Rd×r và Wup∈Rr×d′. Đối với lớp tích chập, các bản cập nhật có thể được định hình lại thành hình dạng kernel. Cuối cùng, LoRA sửa đổi quá trình chuyển tiếp của lớp được thích ứng thành dạng sau:

h′=h+e∗(WdownWup), (4)

trong đó ∗ là phép nhân ma trận hoặc phép tích chập, bias và phép định hình lại được bỏ qua để ngắn gọn. Vì LoRA thích ứng trọng số của g, các bản cập nhật trọng số có thể được hợp nhất vào g để giảm độ trễ suy luận.

Prefix [27] và Prompt [25] là các token có thể học được được thêm vào đầu vào của một khối transformer hoặc keys và values của mô-đun attention. Với hai tập hợp token prefix Pk, Pv∈Rl×d, mô-đun attention được sửa đổi như:

h′= Attn(xWq,[Pk,eWk],[Pv,eWv]), (5)

trong đó [·,·] là phép nối, và Attn được định nghĩa là:

Attn(Q,K,V) := softmax(QKT/√d)V,

và cơ chế đa đầu được bỏ qua để ngắn gọn.

Trong bài báo này, chúng tôi theo [10] để thêm một tham số scale có thể học được s vào Adapter song song (Eq. 2) và LoRA (Eq. 4) tương ứng để có được:

h′=h+s·σ(e∗Wdown)∗Wup, (6)
h′=h+s·e∗(WdownWup). (7)

Các phương trình (6) và (7) là dạng tổng quát của phương trình (2) và (4), và chúng thoái hóa thành phương trình (2) và (4) khi s là hằng số 1. Chúng tôi sử dụng các PET trong phương trình (6) và (7) thay vì phương trình (1) và (4) trong các thí nghiệm của chúng tôi (Phần 5).

Ngoài ba mô-đun PET được mô tả ở trên, còn có nhiều mô-đun khác, như AdaptBias [7], Compacter [33], và AdapterFormer [3], và sẽ có các phương pháp PET mới và vượt trội hơn trong tương lai. Tất cả chúng có thể được áp dụng vào khung CL của chúng tôi (xem Phần 4.2), miễn là chúng tương thích với mô hình tiền huấn luyện.

## 4. Phương pháp

### 4.1. Baseline Đơn giản

Chúng tôi xây dựng một baseline bằng cách tận dụng các mô hình tiền huấn luyện và kỹ thuật PET, theo fine-tuning tuần tự đơn giản (Seq-FT) thường được coi là giới hạn dưới của CIL. Một cách trực quan, baseline của chúng tôi tạo ra một mô-đun PET và gắn nó vào mô hình tiền huấn luyện, sau đó học tuần tự các tác vụ theo cách tương tự như Seq-FT nhưng giữ mô hình tiền huấn luyện đóng băng. Chúng tôi chọn hàm mất mát cross-entropy cục bộ (CE) thay vì hàm mất mát CE toàn cục làm mục tiêu học cho Seq-FT và baseline của chúng tôi vì CE cục bộ thực nghiệm hoạt động tốt hơn CE toàn cục khi sử dụng mô hình tiền huấn luyện lớn [47, 46, 45]. CE cục bộ là CE tiêu chuẩn được tính trên các danh mục của tác vụ hiện tại:

L=1/|Di| ∑(x,y)∈Di Lce(mask(f(x;θ,ϕ)), y), (8)

trong đó y là nhãn sự thật của đầu vào x trong tập huấn luyện hiện tại Di, mask(·) là một hàm lọc ra các logit của các danh mục cũ. Phương trình (8) quay trở lại CE toàn cục khi mask(·) được loại bỏ. Mặc dù baseline của chúng tôi rất đơn giản, hiệu suất tương đương với DualPrompt tiên tiến nhất khi sử dụng cùng mô-đun Prefix.

### 4.2. Khung Được Đề xuất

LAE xây dựng trên baseline tổng quát hóa kiến trúc mạng của chúng tôi và bổ sung giới thiệu ba thiết kế mới, tạo ra một khung mạnh mẽ có thể dễ dàng định hình lại bất kỳ phương pháp PET nào thành một phương pháp Học Liên Tục cạnh tranh. Trong phần tiếp theo, chúng tôi sẽ đi sâu vào ba khía cạnh chính này của LAE: học, tích lũy, và tổng hợp.

**Học với tốc độ được hiệu chỉnh.** Chúng tôi quan sát thấy rằng các mô-đun PET khác nhau về tốc độ tiếp thu kiến thức mới, dẫn đến sự khác biệt về hiệu suất. Về lý thuyết, việc thích ứng mô-đun PET với một tác vụ mới quá nhanh có thể dẫn đến overfitting và gây ra việc quên thảm khốc tồi tệ hơn, trong khi thích ứng chậm hơn có thể duy trì sự ổn định của mô hình nhưng hạn chế tính linh hoạt của nó. Chúng tôi lập luận rằng việc căn chỉnh tốc độ thích ứng của các mô-đun PET khác nhau là quan trọng để chuyển đổi chúng thành một phương pháp CL hiệu quả và mạnh mẽ theo cách thống nhất. Để giải quyết vấn đề này, chúng tôi đề xuất hiệu chỉnh các mô-đun PET để căn chỉnh tốc độ thích ứng của chúng.

Hơn nữa, vì cùng một mô-đun PET θpet được chia sẻ bởi các tác vụ mới và cũ, các thay đổi được thực hiện đối với θpet cho tác vụ mới có thể gây ra việc quên cho các tác vụ cũ. Do đó, việc làm chậm sự thay đổi trong θpet, ví dụ, bằng cách giảm tỷ lệ học (xem bổ sung), có thể giúp giảm thiểu việc quên thảm khốc. Kumar et al. [22] trình bày cách chiến lược linear probing theo sau bởi fine-tuning cân bằng hiệu quả hiệu suất trên các tác vụ ngoài phân phối và trong phân phối. Chúng tôi sử dụng một kỹ thuật tương tự để hiệu chỉnh tốc độ thích ứng của θpet so với ϕnew. Cụ thể, ở đầu quá trình huấn luyện, chúng tôi chỉ học ϕnew với θpet được đóng băng; sau đó sau khi ϕnew đã học đủ và loss đã giảm đáng kể, chúng tôi cùng học cả ϕnew và θpet.

Theo nghiên cứu của He et al. [10], Prefix có thể được chuyển đổi tương đương thành một dạng tương tự như Adapter:

h′←(1−λ(e))h+λ(e)σ(eW1)W2, (9)

trong đó W1=WqP⊤k, W2=Pv, σ=softmax và

λ(e) = ∑i exp(eWqP⊤k)i / (∑i exp(eWqP⊤k)i + ∑j exp(eWqW⊤kC⊤j)). (10)

Vì Pk thường chứa ít token hơn nhiều so với đầu vào C, λ(e) thường là một số dương nhỏ gần 0, điều này ảnh hưởng đến gradient của các token Prefix Pv:

∂L/∂Pv = (∂h′/∂Pv)⊤ ∂L/∂h′ = λ(e)(σ(eW1))⊤ ∂L/∂h′. (11)

Do đó, gradient của Pv nhỏ hơn đáng kể so với Wup của Adapter tương ứng được tham số hóa bởi Wdown=W1 và Wup=W2, và chúng ta có thể đến một kết luận tương tự về Pk và Wdown. Sau đó, chúng ta có thể dễ dàng quan sát thấy rằng Prefix thích ứng với tác vụ mới chậm hơn nhiều so với Adapter. Đây là một phần lý do tại sao các prompt cho các tác vụ khác nhau trong các phương pháp trước [47, 46] có xu hướng đồng nhất (xem bổ sung). Ở đây chúng tôi căn chỉnh Prefix với Adapter bằng cách bù gradient của nó bằng 1/λ(e) và thêm hai tham số scaling có thể học được sk và sv, hiệu chỉnh Prefix được mô tả bởi phương trình (9) thành dạng sau:

h′←(1−λ(e))h+σ(sk·eW1)(sv·W2). (12)

Tốc độ thích ứng của Prefix được hiệu chỉnh gần như tương đương với Adapter được mô tả trong phương trình (6) và nâng cao hiệu suất của nó để ngang bằng với Adapter. Đối với các mô-đun PET khác, chúng tôi cũng có thể phân tích chúng cụ thể và sau đó hiệu chỉnh tốc độ thích ứng của chúng để căn chỉnh với Adapter.

Bằng cách căn chỉnh tốc độ thích ứng của các mô-đun PET và hiệu chỉnh tốc độ thích ứng của chúng so với các bộ phân loại, khung của chúng tôi đạt được sự cân bằng ổn định-linh hoạt tốt hơn và nhất quán hơn với các mô-đun PET khác nhau.

**Tích lũy kiến thức đa tác vụ.** Mô-đun PET θpet được thiết kế để liên tục thích ứng với các tác vụ mới, làm cho mô hình thành thạo hơn trong việc xử lý các tác vụ mới. Tuy nhiên, quá trình thích ứng này có thể dẫn đến việc mô hình dần dần quên cách xử lý các tác vụ cũ hơn. Để giải quyết vấn đề này, chúng tôi đề xuất tạo ra một chuyên gia bổ sung cho các tác vụ cũ hơn để bổ sung cho chuyên gia về các tác vụ mới hơn, lấy cảm hứng từ Hệ thống Học Bổ sung [34, 23] của não người, bao gồm hồi hải mã nhanh chóng học kiến thức mới và vỏ não tích hợp kiến thức đã học theo cách ngoại tuyến theo thời gian. Chúng tôi đạt được điều này bằng cách sao chép mô-đun PET trực tuyến θonpet (tức là θpet trong baseline) gắn vào mô hình như mô-đun PET ngoại tuyến θoffpet sau khi mô hình đã học tác vụ đầu tiên. θoffpet từ từ tích lũy kiến thức đã học khi mô hình học một tác vụ mới bằng một hàm tích lũy, và chúng tôi thực nghiệm thấy rằng thuật toán Trung bình Động Hàm mũ (EMA) đơn giản hoạt động tốt cho LAE của chúng tôi:

θoffpet ← α·θoffpet + (1−α)·θonpet, (13)

trong đó α∈(0,1) là một weight decay lớn (tức là gần 1). Bằng cách này, mô-đun PET ngoại tuyến giống như vỏ não dần dần tích hợp kiến thức đã học theo cách ngoại tuyến chậm, trong khi mô-đun PET trực tuyến giống như hồi hải mã tiếp tục nhanh chóng học kiến thức mới. Sau đó, chúng ta có thể có được hai chuyên gia cho các tác vụ mới hơn và cũ hơn với θonpet và θoffpet, tương ứng. Tuy nhiên, tác vụ mà một mẫu thuộc về không được biết trong quá trình suy luận, chúng ta cần phải thiết kế một phương pháp để sử dụng hiệu quả cả hai chuyên gia cho suy luận.

**Tổng hợp hai mô hình chuyên gia.** Hai mô hình chuyên gia được xây dựng với θonpet và θoffpet tương ứng thành thạo trong việc xử lý các tác vụ mới hơn và cũ hơn. Thay vì suy luận chỉ sử dụng mô hình chuyên gia trực tuyến hoặc ngoại tuyến, chúng tôi tích hợp các đầu ra của chúng để có được dự đoán cho một mẫu suy luận.

Một bộ phân loại có thể được xem như một mô hình năng lượng khi chúng ta định nghĩa xác suất log âm không chuẩn hóa như hàm năng lượng [24]. Mục tiêu tối ưu hóa của mô hình năng lượng là giảm thiểu năng lượng của mô hình trên phân phối dữ liệu của tác vụ học của nó. Nghiên cứu trước đây [29] đã chỉ ra rằng năng lượng của một mô hình năng lượng được huấn luyện trên một miền dữ liệu thường rất cao trên các miền dữ liệu khác. Do đó, phương trình (8) thực sự liên tục giảm thiểu năng lượng của ϕnew trên tác vụ mới. Ngay cả khi dữ liệu cũ không được sử dụng trong quá trình huấn luyện, năng lượng của dữ liệu cũ trên ϕnew sẽ rất cao, như được chứng minh bởi công trình gần đây ESN [45]. Tương tự, vì θonpet và θoffpet tương ứng chứa kiến thức mới và lịch sử tương đối nhiều hơn, về mặt lý thuyết, năng lượng được tạo ra bởi θonpet cho mẫu của các tác vụ mới hơn sẽ nhỏ hơn so với năng lượng được tạo ra bởi θoffpet, và ngược lại đối với mẫu của các tác vụ cũ hơn. Do đó, việc chọn kết quả dự đoán với năng lượng thấp nhất làm dự đoán cuối cùng của một mẫu suy luận có vẻ như là một giải pháp đơn giản nhưng hiệu quả. Tuy nhiên, trong thực tế, chúng tôi thấy rằng việc chuẩn hóa năng lượng được tạo ra bởi θonpet và θoffpet trước khi tổng hợp mang lại kết quả mạnh mẽ hơn. Do đó, chúng tôi áp dụng thuật toán tổng hợp sau:

fens(oon,ooff) := max(σ(oon), σ(ooff)), (14)

trong đó σ là hàm softmax, oon và ooff là đầu ra của các mô hình chuyên gia trực tuyến và ngoại tuyến (tức là f(·;θonpet,ϕ) và f(·;θoffpet,ϕ)) cho một mẫu suy luận, tương ứng.

Như được minh họa trong Hình 2, trong khung LAE của chúng tôi, mô hình học một tác vụ mới với θonpet và tích lũy kiến thức đã học vào θoffpet, hai chuyên gia được ưa chuộng bởi các tác vụ mới hơn và cũ hơn được tổng hợp để có được dự đoán cuối cùng cho một mẫu suy luận. LAE của chúng tôi có thể được áp dụng cho mô hình tiền huấn luyện trong bất kỳ kiến trúc mạng nào miễn là các mô-đun PET tương thích với mô hình.

## 5. Thí nghiệm

### 5.1. Tập dữ liệu và Giao thức Đánh giá

Các thí nghiệm của chúng tôi sử dụng các mô hình tiền huấn luyện trên tập dữ liệu ImageNet21k [40] mà không được chỉ định, và chúng tôi theo các công trình trước để huấn luyện và đánh giá mô hình trên các benchmark CIFAR100 [21] và ImageNet-R [14].

CIFAR100 là một tập dữ liệu được sử dụng rộng rãi trong các công trình học liên tục (CL) trước đây, chứa 100 lớp, mỗi lớp có 500 ảnh huấn luyện và 100 ảnh test có kích thước 32×32×3.

ImageNet-R được giới thiệu đầu tiên vào CL bởi Wang et al. [46], bao gồm 200 danh mục con của ImageNet [40], nhưng các mẫu của nó có các phong cách khác nhau, như hoạt hình, graffiti, và origami. Cũng có một số ví dụ khó từ ImageNet mà các mô hình tiêu chuẩn, ví dụ ResNet [13], thất bại trong việc phân loại. Tập dữ liệu gốc được chia thành tập huấn luyện với 24000 mẫu và tập test với 6000 mẫu, và số lượng mẫu huấn luyện và test khác nhau giữa các lớp.

Chúng tôi theo các công trình trước để chia tập dữ liệu thành 10 tác vụ, và tất cả các tác vụ có cùng số lượng lớp, tức là 10 cho CIFAR100 và 20 cho ImageNet-R. Chúng tôi đánh giá mô hình bằng các số đo tăng dần được sử dụng rộng rãi: độ chính xác tăng dần cuối cùng AN và độ chính xác tăng dần trung bình ¯AN=1/N ∑N i=1 Ai, trong đó N là tổng số tác vụ (tức là 10), và Ai được định nghĩa chính thức là:

Ai = 1/|Dtest1:i| ∑(x,y)∈Dtest1:i 1(ŷ=y), (15)

trong đó 1(·) là hàm chỉ thị ánh xạ giá trị boolean thành {0,1}, Dtest1:i là tập test của tất cả các tác vụ đã thấy cho đến nay, ŷ và y là nhãn dự đoán và sự thật của đầu vào x. Chúng tôi chạy tất cả các thí nghiệm 3 lần với các thứ tự lớp khác nhau và báo cáo trung bình và độ lệch chuẩn của 3 lần chạy này.

### 5.2. Chi tiết Thực hiện và Huấn luyện

Để thực hiện so sánh công bằng, chúng tôi xem xét các phương pháp tiên tiến nhất [47, 46] dựa trên các mô hình tiền huấn luyện như LAE của chúng tôi và sử dụng mã PyTorch được phát hành bởi Jaeho Lee để tiến hành thí nghiệm. Fine-tuning kết hợp (Joint-FT) và fine-tuning tuần tự đơn giản (Seq-FT) thường được công nhận là giới hạn trên và dưới của CIL được thực hiện trong codebase của chúng tôi, tham khảo mã của Jaeho Lee. Chúng tôi cũng so sánh với công trình gần đây ESN [45], sử dụng mã PyTorch chính thức của nó. Chúng tôi chọn ba loại mô-đun PET đại diện và hai kích thước cho mỗi loại cho baseline và khung LAE của chúng tôi, trong đó kích thước biểu thị chiều của phép chiếu xuống của Adapter, thứ hạng của LoRA, hoặc độ dài của Prefix được mô tả trong Phần 3. Chúng tôi giả định rằng chỉ có một mô-đun PET duy nhất được gắn vào mô hình tiền huấn luyện trong thảo luận trước để thuận tiện, trong thực tế, nhiều mô-đun PET được chèn vào các khối Attention của Transformers hoặc các khối tích chập của ConvNets trong các lớp nông, theo DualPrompt [46].

Chiến lược huấn luyện của baseline và khung LAE của chúng tôi giống như DualPrompt, tức là huấn luyện mô hình với bộ tối ưu hóa Adam trong 5 và 50 epoch, và tỷ lệ học không đổi 0.03 và 0.005 dựa trên kích thước batch 256, cho CIFAR100 và ImageNet-R, tương ứng. Weight decay α của thuật toán EMA được định nghĩa trong phương trình (13) được đặt thực nghiệm là 0.9999 trong tất cả các thí nghiệm. Số epoch đóng băng của các mô-đun PET được đặt là 3 và 30 cho CIFAR100 và ImageNet-R, tương ứng. Việc tăng cường dữ liệu nhất quán với việc sử dụng trong tiền huấn luyện mô hình. Chúng tôi huấn luyện Joint-FT và Seq-FT với chiến lược fine-tuning được khuyến nghị của ViT [5], nhưng số epoch huấn luyện giống như của chúng tôi. Thêm chi tiết có thể được tìm thấy trong tài liệu bổ sung.

### 5.3. Kết quả Benchmark

Kết quả benchmark CIFAR100 được trình bày trong Bảng 1. Tất cả các phương pháp sử dụng cùng mô hình ViT-B/16 [5] tiền huấn luyện trên tập dữ liệu ImageNet21k [40]. Hậu tố số của mô-đun PET biểu thị kích thước của nó (tức là chiều phép chiếu xuống hoặc độ dài). L2P và DualPrompt là các phương pháp tiên tiến nhất áp dụng một pool để lưu trữ Prompt hoặc Prefix. Tuy nhiên, độ chính xác của việc lựa chọn prompt của chúng dần dần giảm với sự tăng lên của số lượng tác vụ học và các prompt cho các tác vụ khác nhau xuất hiện đồng nhất (xem bổ sung). Do đó, baseline của chúng tôi rất đơn giản nhưng đạt được hiệu suất tương đương với L2P và DualPrompt, và khung LAE của chúng tôi với tất cả 6 mô-đun PET vượt trội so với DualPrompt và ESN khoảng 1.5% trong độ chính xác tăng dần cuối cùng A10. Mặc dù các mô-đun PET có hiệu suất khác nhau trong baseline, chúng đạt được hiệu suất tốt hơn và cùng mức trong LAE của chúng tôi, chủ yếu do việc hiệu chỉnh tốc độ thích ứng. Đặc biệt, DualPrompt có gấp 3-10 lần số tham số có thể học được hơn so với LAE của chúng tôi.

Benchmark ImageNet-R khó hơn CIFAR100, nhưng nó có thể chứng minh tốt hơn các lợi thế của khung LAE của chúng tôi. Từ kết quả được hiển thị trong Bảng 2, baseline của chúng tôi chỉ có thể đạt được hiệu suất tương đương với DualPrompt khi sử dụng Prefix. Điều này là do Adapter và LoRA thích ứng với một tác vụ mới nhanh hơn Prefix, điều này được phóng đại trong tập dữ liệu ImageNet-R nhưng được giải quyết thành công bởi việc hiệu chỉnh tốc độ thích ứng của khung LAE của chúng tôi. Do đó, chúng ta có thể thấy rằng LAE của chúng tôi đạt được cải thiện hiệu suất hơn 3.5% so với DualPrompt về độ chính xác tăng dần cuối cùng A10, cũng được tăng cường so với tập dữ liệu CIFAR100 dễ hơn. Chúng ta cũng có thể quan sát thấy rằng kích thước của các mô-đun PET có ít tác động đến hiệu suất trong khung LAE của chúng tôi, và LAE của chúng tôi mạnh mẽ hơn đối với thứ tự lớp, trong khi baseline của chúng tôi có phương sai tương đối lớn giữa các thứ tự lớp khác nhau.

CL thực tế là một quy trình bất tận, và hiệu suất của mỗi giai đoạn học đều quan trọng như nhau đối với hệ thống AI. Vì vậy, chúng tôi cũng vẽ độ chính xác tăng dần theo từng tác vụ trong Hình 3a và 3b. Chúng ta có thể quan sát thấy rằng LAE của chúng tôi với tất cả ba loại mô-đun PET hoạt động tốt hơn L2P và DualPrompt ở hầu hết tất cả các giai đoạn học. LAE của chúng tôi vượt trội so với các phương pháp khác với biên độ rộng hơn trong các thí nghiệm 20-tác vụ được trình bày trong tài liệu bổ sung, làm nổi bật khả năng xử lý các tình huống CL dài hạn. Trong phần bổ sung, chúng tôi so sánh LAE của chúng tôi với phương pháp CODA-Prompt [44] đương thời trên các tập dữ liệu ImageNet-R và DomainNet [35]. So sánh tham số và tính toán cũng có thể được tìm thấy trong phần bổ sung.

### 5.4. Nghiên cứu Loại bỏ

LAE của chúng tôi bao gồm ba thiết kế mới chính, tức là học, tích lũy, và tổng hợp, vì vậy chúng tôi loại bỏ chúng và báo cáo kết quả trong Bảng 3. Hàng đầu và cuối là baseline và khung LAE của chúng tôi, tương ứng. Hiệu suất giảm nhiều nhất khi loại bỏ việc học với tốc độ được hiệu chỉnh của chúng tôi, chứng minh rằng nó đóng góp nhiều nhất cho LAE của chúng tôi. Tích lũy và Tổng hợp cũng quan trọng đối với LAE của chúng tôi, không có chúng độ chính xác tăng dần cuối cùng giảm 2.15%. Hàng thứ sáu biểu thị suy luận LAE của chúng tôi chỉ với mô-đun PET Ngoại tuyến, có A10 thậm chí tốt hơn so với suy luận bằng tổng hợp chuyên gia của chúng tôi. Như được minh họa trong Hình 4, suy luận bằng tổng hợp chuyên gia hoạt động tốt hơn so với suy luận với mô-đun PET Trực tuyến hoặc Ngoại tuyến riêng lẻ trong các giai đoạn học sớm hơn. Tuy nhiên, khi số lượng tác vụ đã học tăng lên, lợi thế của tổng hợp chuyên gia so với mô-đun PET Ngoại tuyến dần dần giảm, một phần do hiệu suất của các tác vụ cũ chiếm ưu thế trong hiệu suất tổng thể. Tuy nhiên, suy luận với tổng hợp chuyên gia mang lại hiệu suất mạnh mẽ hơn trong hầu hết các trường hợp.

Chúng tôi cũng đã tiến hành các thí nghiệm loại bỏ về việc hiệu chỉnh được thực hiện đối với Prefix. Chúng ta có thể thấy từ Bảng 4 rằng cả bù gradient và các tham số scaling có thể học được đều riêng lẻ dẫn đến cải thiện đáng kể về hiệu suất. Hơn nữa, khi được sử dụng cùng nhau, lợi ích hiệu suất xấp xỉ bằng tổng của các lợi ích đạt được bằng cách sử dụng từng cái riêng lẻ, cho thấy rằng các đóng góp của chúng cho hiệu suất độc lập với nhau.

### 5.5. Vị trí Gắn của Mô-đun PET

LAE của chúng tôi chèn các mô-đun PET trực tiếp vào 5 khối Transformer đầu tiên, theo DualPrompt. Như được hiển thị trong Hình 5 (trái), việc chèn các mô-đun PET ở vị trí nông nhất tạo ra kết quả tốt hơn so với việc chèn chúng ở các vị trí sâu hơn, điều này nhất quán với quan sát trong DualPrompt. Ngoài ra, Hình 5 (phải) cho thấy rằng việc chèn các mô-đun PET trong 6 khối Transformer đầu tiên đạt được hiệu suất tốt nhất trong khi việc chèn chúng trong 5 khối Transformer đầu tiên (tức là cài đặt mặc định của LAE của chúng tôi) cũng dẫn đến hiệu suất gần như tương tự.

### 5.6. Kết quả trên Biến thể Transformer và ConvNet

Prefix và Prompt không đủ linh hoạt để được áp dụng cho ConvNets và các biến thể Transformer, trong khi LAE của chúng tôi có thể tổng quát hóa kiến trúc mô hình do khả năng tận dụng các mô-đun PET khác nhau. Chúng tôi chọn Swin Transformer [31] và ConvNeXt [32] để xác thực LAE của chúng tôi.

Swin Transformer là một Vision Transformer dựa trên cửa sổ đại diện, nhưng L2P và DualPrompt không thể được áp dụng trực tiếp vào nó vì các token được chèn có thể làm gián đoạn việc chia cửa sổ phù hợp. Do đó, chúng tôi chỉ so sánh LAE của chúng tôi với baseline của chúng tôi khi sử dụng Swin Transformer, và báo cáo kết quả trong Bảng 5. Chúng ta có thể thấy rằng LAE của chúng tôi đạt được hiệu suất tốt hơn với Swin-B so với ViT-B/16, chủ yếu do hiệu suất vượt trội của Swin-B. Hơn nữa, LAE của chúng tôi cải thiện đáng kể hiệu suất của baseline của chúng tôi trên cả hai tập dữ liệu.

ConvNeXt là một ConvNet hiện đại cho thập niên 2020 vượt trội so với Swin Transformer bằng cách kết hợp một số thiết kế mới vào ResNet [13] tiêu chuẩn. Tương tự, chúng tôi so sánh LAE của chúng tôi với baseline trong Bảng 6. Các phép chiếu xuống và lên của Adapter được thực hiện bằng các lớp tích chập 1×1. Cả baseline và LAE của chúng tôi đều đạt được hiệu suất tốt hơn đáng kể khi sử dụng ConvNeXt-B so với việc sử dụng ViT-B/16 và Swin-B, làm nổi bật điểm mạnh của LAE vượt ra ngoài việc bị hạn chế với Transformers. LAE của chúng tôi cải thiện nhất quán hiệu suất của baseline của chúng tôi trên cả hai tập dữ liệu.

## 6. Kết luận

Bài báo này đã nghiên cứu kỹ lưỡng mô hình Học Liên Tục (CL) mới bắt đầu với một mô hình tiền huấn luyện và liên tục thích ứng mô hình với các tác vụ đến sử dụng các phương pháp Tùy Chỉnh Tham Số Hiệu Quả (PET) tổng quát. Chúng tôi đã xây dựng một baseline đơn giản đạt được hiệu suất tương đương với các phương pháp tiên tiến nhất trước đây. Chúng tôi đã đề xuất khung Learning-Accumulation-Ensemble (LAE) bằng cách giới thiệu ba thiết kế mới vào baseline. LAE của chúng tôi có thể chuyển đổi bất kỳ phương pháp PET nào thành một phương pháp CL hiệu quả mà không cần truy cập bất kỳ dữ liệu cũ nào. Chúng tôi đã tiến hành các thí nghiệm mở rộng để xác thực hiệu quả của LAE của chúng tôi, và các kết quả chứng minh rằng LAE của chúng tôi vượt trội đáng kể so với các phương pháp tiên tiến nhất trước đây.

**Hạn chế.** Vẫn còn một số hạn chế cần được cải thiện trong tương lai, chẳng hạn như cách tích lũy kiến thức hiệu quả hơn và tổng hợp các mô hình chuyên gia tốt hơn. Hơn nữa, do thiếu các tập dữ liệu quy mô lớn không chồng lấn với tập dữ liệu tiền huấn luyện, LAE của chúng tôi chưa được xác minh trong các tình huống CL với số lượng tác vụ lớn hơn.

Nhìn chung, bài báo này cung cấp một giải pháp mới cho CL Không Bộ Nhớ và đưa ra một số tài liệu tham khảo lý thuyết và thực nghiệm cho nghiên cứu tương lai về mô hình CL mới này.
