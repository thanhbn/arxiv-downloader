# 2204.04392.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2204.04392.pdf
# Kích thước tệp: 628796 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Điều chỉnh Minh họa Đối tương cho các Mô hình Ngôn ngữ Tiền huấn luyện
Xiaozhuan Liang1,2, Ningyu Zhang1,2*, Siyuan Cheng1,2, Zhenru Zhang3,
Chuanqi Tan3, Huajun Chen1,2
1Trường Đại học Chiết Giang & Phòng thí nghiệm chung AZFT cho Động cơ Tri thức, Trung Quốc
2Trung tâm Đổi mới Hàng Châu, Trường Đại học Chiết Giang, Trung Quốc
3Tập đoàn Alibaba, Trung Quốc
{liangxiaozhuan,zhangningyu,sycheng,huajunsir}@zju.edu.cn
{zhangzhenru.zzr,chuanqi.tcq}@alibaba-inc.com

Tóm tắt
Các mô hình ngôn ngữ tiền huấn luyện có thể được kích thích hiệu quả bằng các prompt văn bản hoặc minh họa, đặc biệt trong các tình huống ít dữ liệu. Các nghiên cứu gần đây đã tập trung vào việc tự động tìm kiếm các prompt rời rạc hoặc liên tục hoặc các verbalizer được tối ưu hóa, nhưng các nghiên cứu về minh họa vẫn còn hạn chế. Cụ thể, các ví dụ minh họa là rất quan trọng cho hiệu suất cuối cùng xuất sắc của prompt-tuning. Trong bài báo này, chúng tôi đề xuất một phương pháp mới có thể cắm vào, mở rộng và hiệu quả được gọi là điều chỉnh minh họa đối tương, không cần lấy mẫu minh họa. Hơn nữa, phương pháp được đề xuất có thể: (i) Được cắm vào bất kỳ phương pháp prompt-tuning nào trước đây; (ii) Được mở rộng cho các tác vụ phân loại rộng rãi với số lượng lớn các danh mục. Kết quả thực nghiệm trên 16 bộ dữ liệu cho thấy phương pháp của chúng tôi được tích hợp với các phương pháp trước đây LM-BFF và P-tuning có thể mang lại hiệu suất tốt hơn1.

1 Giới thiệu
Các mô hình ngôn ngữ tiền huấn luyện (PLM) đã được áp dụng cho các tác vụ hiểu và tạo sinh ngôn ngữ tự nhiên rộng rãi, được chứng minh là đạt được những cải thiện đáng kể trên các benchmark (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020a; Dong et al., 2019; Bao et al., 2020; Zhang et al., 2022c; Xie et al., 2022a; Zhang et al., 2022a). Một mô hình của PLM là pre-train-fine-tune, đã trở thành tiêu chuẩn de facto cho xử lý ngôn ngữ tự nhiên (NLP), trong đó các mục tiêu cụ thể cho tác vụ và các tham số bổ sung được tận dụng trong quy trình điều chỉnh. Gần đây, mô hình thích ứng của PLM đã thay đổi. Một phương pháp điều chỉnh mới được gọi là prompt-tuning với một prompt ngôn ngữ tự nhiên và một vài minh họa đã tạo ra làn sóng trong cộng đồng NLP bằng cách chứng minh khả năng few-shot đáng kinh ngạc trên vô số tác vụ hiểu ngôn ngữ. Các nghiên cứu tiếp theo cố gắng giảm thiểu việc kỹ thuật prompt tốn nhiều công sức với tìm kiếm prompt rời rạc (Shin et al., 2020) hoặc tối ưu hóa prompt liên tục (Liu et al., 2021d; Li and Liang, 2021; Hambardzumyan et al., 2021a; Zhong et al., 2021). Tuy nhiên, ít nghiên cứu tập trung vào minh họa, đây là một thành phần không thể thiếu trong các phương pháp hướng prompt.

Trong các nghiên cứu trước đây, minh họa là các ví dụ được lấy mẫu trong tập huấn luyện. Mô hình "in-context learning" đơn giản của GPT-3 chọn lên đến 32 phiên bản được lấy mẫu ngẫu nhiên làm minh họa và nối trực tiếp chúng với chuỗi đầu vào (Liu et al., 2021a; Min et al., 2022). Vì minh họa có thông tin là rất quan trọng cho hiệu suất mô hình, Gao et al. (2021a) phát triển một chiến lược tinh tế thông qua việc lấy mẫu các cặp đầu vào với các ví dụ tương tự, do đó cung cấp cho mô hình các so sánh phân biệt hơn. Tuy nhiên, vẫn không được đảm bảo ưu tiên các minh họa có thông tin nhất vì (1) việc lấy mẫu dựa trên độ tương tự có thể thu được các minh họa suy thoái trong các lớp khác nhau nhưng có khoảng cách tương tự với đầu vào; (2) số lượng minh họa có thể sử dụng vẫn bị giới hạn bởi độ dài đầu vào tối đa của mô hình. Ví dụ, như được hiển thị trong Hình 1, các đường màu tím đề cập đến việc lấy mẫu ngẫu nhiên trong khi các đường màu xanh lam chỉ ra việc lấy mẫu dựa trên độ tương tự. Lưu ý rằng việc lấy mẫu dựa trên độ tương tự có thể thu được các ví dụ rất giống với chuỗi đầu vào. Tuy nhiên, những ví dụ được lấy mẫu với các nhãn khác nhau có thể có xu hướng có biểu diễn tương tự và do đó làm rối loạn khả năng phân biệt của mô hình. Hơn nữa, đối với các bộ dữ liệu có nhiều lớp, việc nối tất cả các minh họa được lấy mẫu vẫn không đơn giản. Những thách thức được đề cập ở trên cản trở khả năng áp dụng của minh họa trong prompt-tuning.

Để giải quyết những vấn đề đó, trong bài báo này, chúng tôi đề xuất Điều chỉnh DEMO nstration Đối tương (Demo-tuning) cho các mô hình ngôn ngữ tiền huấn luyện. Cụ thể, chúng tôi tận dụng các embedding liên tục có thể học được (ví dụ: một hoặc hai token có thể học được) làm minh họa ảo để giảm số lượng danh mục tối đa. Chúng tôi nối những minh họa ảo đó với chuỗi đầu vào; do đó, phương pháp của chúng tôi có thể được mở rộng cho nhiều loại tác vụ phân loại với nhiều danh mục. Để tối ưu hóa những embedding liên tục đó, chúng tôi khám phá một khung đối tương đơn giản mà không có cặp âm tính (Grill et al., 2020) vì khó tìm được một cặp âm tính phù hợp trong không gian ngữ nghĩa cho NLP. Trong mỗi batch huấn luyện, chúng tôi lấy mẫu ngẫu nhiên một ví dụ thực và coi các ví dụ ảo và thực là các cặp dương tính. Với học đối tương, chúng tôi có thể thu được các minh họa ảo có thông tin, được tối ưu hóa với các so sánh phân biệt hơn.

Chúng tôi tiến hành các thí nghiệm rộng rãi trên 16 bộ dữ liệu NLP. Điều chỉnh minh họa đối tương của chúng tôi có thể mang lại hiệu suất tốt hơn khi được tích hợp với các phương pháp dựa trên prompt trước đây (ví dụ: LM-BFF (Gao et al., 2021a), P-tuning (Liu et al., 2021d)). Hơn nữa, phương pháp của chúng tôi có thể được áp dụng cho các bộ dữ liệu có nhiều danh mục và vượt trội hơn các baseline. Lưu ý rằng phương pháp của chúng tôi là model-agnostic và có thể được cắm vào nhiều phương pháp dựa trên prompt mà không cần nỗ lực chọn minh họa phù hợp. Các đóng góp chính của nghiên cứu này như sau:

• Chúng tôi đề xuất một phương pháp có thể cắm vào, mở rộng và hiệu quả để điều chỉnh minh họa đối tương cho các mô hình ngôn ngữ tiền huấn luyện. Theo hiểu biết tốt nhất của chúng tôi, tối ưu hóa minh họa cũng là một nhánh nghiên cứu mới chưa được khám phá trong prompting mô hình ngôn ngữ.

• Chúng tôi đề xuất minh họa ảo và tận dụng học đối tương để thu được minh họa có thông tin và cũng giảm số lượng danh mục tối đa trong các tác vụ phân loại.

• Một đánh giá hệ thống của 16 bộ dữ liệu NLP cho thấy phương pháp đơn giản nhưng hiệu quả được đề xuất đóng góp vào cải thiện trên tất cả các tác vụ này.

2 Công trình liên quan

2.1 Prompt-tuning
Với sự phổ biến của GPT-3 (Brown et al., 2020), prompting PLM cho few-shot learning đã trở thành một mô hình học mới, phổ biến trong xử lý ngôn ngữ tự nhiên (Schick and Schütze, 2021; Tam et al., 2021; Liu et al., 2021b) và thu hút các nhà nghiên cứu. Gần đây, prompt-tuning đã được áp dụng cho các tác vụ NLP khác nhau, chẳng hạn như nhận dạng thực thể có tên (Cui et al., 2021; Chen et al., 2021b; Zhou et al., 2021; Ma et al., 2022), phân loại thực thể (Ding et al., 2021), trích xuất quan hệ (Han et al., 2021), trích xuất sự kiện (Hsu et al., 2021; Ye et al., 2021), phân tích cảm xúc (Li et al., 2021), dịch máy (Tan et al., 2021), và hoàn thành đồ thị tri thức (Xie et al., 2022b). Schick and Schütze (2021, 2020) đề xuất PET, chuyển đổi các tác vụ NLP thành câu hỏi kiểu cloze và mang lại hiệu suất thỏa đáng. Tam et al. (2021) tiếp tục đề xuất một đối tượng giám sát dày đặc hơn trong quá trình điều chỉnh để cải thiện PET.

Lưu ý rằng việc tạo thủ công một prompt có hiệu suất tốt nhất giống như tìm kim trong đống cỏ khô, điều này tạo điều kiện cho kỹ thuật prompt tốn nhiều công sức. Do đó, các nghiên cứu gần đây (Qin and Eisner, 2021; Hambardzumyan et al., 2021b; Ye et al., 2022; Chen et al., 2021c) được tiến hành trong lĩnh vực này đã tập trung vào việc tự động tìm kiếm các prompt. Shin et al. (2020) đề xuất AUTOPROMPT, đây là một phương pháp dựa trên gradient để thu được các template và từ nhãn cho prompt-tuning. Wang et al. (2021) đề xuất EFL, chuyển đổi tác vụ NLP thành một tác vụ entailment và biến các LM nhỏ thành những learner few-shot tốt hơn. Ngoài ra, Gao et al. (2020) đề xuất LM-BFF-điều chỉnh few-shot tốt hơn các mô hình ngôn ngữ, sử dụng một mô hình sinh để thu được các template và một chiến lược tinh tế để động và có chọn lọc kết hợp minh họa vào mỗi bối cảnh. Tuy nhiên, nó không tối ưu cho việc tìm kiếm prompt rời rạc do bản chất liên tục của mạng neural.

Để vượt qua những hạn chế này, Liu et al. (2021d,c) đề xuất P-tuning để tự động tìm kiếm prompt trong không gian liên tục. Li and Liang (2021) đề xuất prefix-tuning, tối ưu hóa một chuỗi các vector cụ thể tác vụ liên tục và giữ các tham số mô hình ngôn ngữ đông lạnh. Lester et al. (2021a) tận dụng một cơ chế để học "soft prompts" để điều kiện các mô hình ngôn ngữ đông lạnh. Zhang et al. (2021) đề xuất một phương pháp học prompt có thể vi phân cho few-shot NLP với các template prompt được tối ưu hóa cũng như nhãn. Vu et al. (2021) đề xuất SPoT, học một prompt trên một hoặc nhiều tác vụ nguồn và sau đó sử dụng nó để khởi tạo prompt cho một tác vụ đích để tăng hiệu suất trên nhiều tác vụ. Các công trình liên quan khác bao gồm WARP (Hambardzumyan et al., 2021a) và OPTIPROMPT (Zhong et al., 2021) cũng đề xuất tận dụng các template liên tục, hiệu quả hơn tìm kiếm prompt rời rạc. Để kết luận, hầu hết các công trình hiện có cố gắng thu được các prompt được tối ưu hóa cho các tác vụ NLP rộng rãi; tuy nhiên, ít nghiên cứu tập trung vào minh họa, đây là một thành phần không thể thiếu trong học hướng prompt.

Công trình của chúng tôi trực giao với các phương pháp prompt-tuning trước đây, nhằm tối ưu hóa prompt. Sự khác biệt chính giữa minh họa ảo và prompt liên tục là: 1) chúng có một chiến lược huấn luyện hoàn toàn khác nhau vì prompt liên tục được tối ưu hóa qua backpropagation với một tập huấn luyện, trong khi phương pháp của chúng tôi sử dụng học đối tương. 2) phương pháp của chúng tôi không yêu cầu kiến trúc bên ngoài (ví dụ: LSTM trong P-tuning), do đó, làm cho nó hiệu quả và có thể cắm vào bất kỳ phương pháp prompt-tuning nào. Cho đến nay, Lee et al. (2021) là phương pháp duy nhất nghiên cứu minh họa và trình bày một phương pháp học dựa trên minh họa đơn giản cho nhận dạng thực thể có tên. Ngoài Lee et al. (2021), phương pháp của chúng tôi tập trung vào các tác vụ phân loại NLP tổng quát. Hơn nữa, chúng tôi đề xuất minh họa ảo với các chiến lược học đối tương, có thể thu được minh họa tốt hơn và cũng giảm số lượng danh mục tối đa trong các bộ dữ liệu.

2.2 Học đối tương
Học đối tương từ lâu được coi là hiệu quả trong việc học các biểu diễn có ý nghĩa. Trong giai đoạn đầu, Mikolov et al. (2013) đề xuất học word embedding bằng cách coi các từ gần một từ mục tiêu là một phiên bản dương tính trong khi những từ khác là âm tính. Logeswaran and Lee (2018); Chen et al. (2021a) tiếp tục tổng quát hóa phương pháp này để học biểu diễn câu. Gần đây, Kim et al. (2021) đề xuất một phương pháp học đối tương sử dụng cơ chế tự hướng dẫn. Yan et al. (2021) đề xuất ConSERT, một khung đối tương cho chuyển giao biểu diễn câu tự giám sát. Giorgi et al. (2021) đề xuất DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Gao et al. (2021b) tận dụng dropout làm augmentation dữ liệu tối thiểu và đề xuất SimCSE, một khung học đối tương đơn giản giúp cải thiện đáng kể các sentence embedding hiện đại.

Mặt khác, học đối tương cũng đã thu hút cộng đồng thị giác máy tính (Jaiswal et al., 2020; Liu et al., 2020). Chen et al. (2020) đề xuất SimCLR: một khung đơn giản cho học đối tương của biểu diễn thị giác mà không yêu cầu kiến trúc chuyên biệt hoặc memory bank. Chen and He (2021) quan sát rằng các mạng siamese đơn giản có thể học các biểu diễn có ý nghĩa ngay cả khi sử dụng không có cặp mẫu âm tính, batch lớn và momentum encoder.

Công trình của chúng tôi liên quan đến Grill et al. (2020), một phương pháp học tự giám sát không đối tương, dựa vào hai mạng neural, được gọi là mạng online và target, tương tác và học từ nhau. Tuy nhiên, trái ngược với phương pháp này, chúng tôi sử dụng encoder trong cùng trạng thái trong khi Grill et al. (2020) tận dụng hai mạng trong các trạng thái khác nhau. Hơn nữa, chúng tôi tập trung vào tối ưu hóa minh họa trong prompt-tuning cho NLP, bao gồm học minh họa có thông tin và thu được các temple prompt và token nhãn.

3 Kiến thức cơ bản
Trong công trình này, chúng tôi tập trung vào các tác vụ phân loại trong setting few-shot, bao gồm phân loại văn bản và hiểu ngôn ngữ tự nhiên, trong đó đầu vào xin là một câu xin=x1 hoặc một cặp câu xin= (x1, x2). Ở đây, chúng tôi để Dtrain={(xi, yi)}K×|Y|i biểu thị tập huấn luyện của một tác vụ downstream được tạo thành từ chỉ K ví dụ huấn luyện trên mỗi lớp, trong đó Y là không gian nhãn của tác vụ. Cho một mô hình ngôn ngữ tiền huấn luyện bao gồm hai giai đoạn: một encoder f(·) và một classifier g(·)2, chúng tôi encode đầu vào x thành một chuỗi các vector ẩn {hk∈Rd} và lấy vector ẩn h[CLS] =f(xin) của [CLS]3 qua classifier để thu được phân phối xác suất p(y|x) =g(h[CLS]) trên y trong Y.

Điều chỉnh dựa trên Prompt Điều chỉnh dựa trên prompt (Schick and Schütze, 2021; Gao et al., 2021a) là một công việc hiệu quả bằng cách thiết kế template kiểu cloze T và verbalizer M:Y → V ánh xạ nhãn tác vụ thành các từ riêng lẻ từ từ vựng V của mô hình ngôn ngữ tiền huấn luyện để lấp đầy khoảng cách giữa mục tiêu masked LM của mô hình ngôn ngữ tiền huấn luyện và mục tiêu điều chỉnh downstream.

Template Trong mô hình điều chỉnh dựa trên prompt, template T chủ yếu bao gồm đầu vào xin và một prompt P= [Pi]mi, trong đó prompt có thể là một chuỗi các token rời rạc (Schick and Schütze, 2021) hoặc các pseudo token liên tục (Liu et al., 2021d). Ví dụ, trong tác vụ phân tích cảm xúc (xem Hình 2), một template với prompt thủ công có thể là: T(x) = [CLS] x1, It was [MASK] .[SEP] trong đó " It was ... . " là prompt và [MASK] là target chuyển đổi tác vụ phân loại thành một tác vụ mô hình hóa ngôn ngữ.

Verbalizer Một verbalizer M định nghĩa một ánh xạ của các token nhãn từ không gian nhãn của một tác vụ cụ thể. Trong Hình 2a, verbalizer ánh xạ " negative/positive " thành " terrible/great ". Bằng cách này, chúng ta có thể tái sử dụng trọng số đầu ra Wv∈Rd×|V| được gọi là MLM head được sử dụng trong pre-training và mô hình hóa xác suất dự đoán token M(y) trong V là p(y|x) =g(h[MASK] ) trên vector ẩn h[MASK] .

Minh họa Để Dctrain là tập con của tất cả ví dụ của lớp c. Chúng tôi lấy mẫu minh họa dc= (x(c)in, y(c)) trong Dctrain và chuyển đổi nó thành T(x(c)in, y(c)) trong đó [MASK] được thay thế bằng M(y(c)). Sau đó chúng tôi kết hợp template gốc T với các template trên trong tất cả các lớp để tạo thành T*(xin), sẽ được sử dụng làm template trong quá trình điều chỉnh và suy luận dựa trên prompt (Xem Hình 2).

4 Điều chỉnh Minh họa Đối tương
Trong công trình này, chúng tôi tập trung vào cách học một minh họa ảo compact và có thể vi phân để phục vụ như augmentation prompt thay vì thiết kế các chiến lược lấy mẫu cụ thể cho học dựa trên minh họa. Chúng tôi đề xuất một khung học dựa trên phương pháp học đối tương có thể tương thích với mô hình học dựa trên prompt hiện tại. Phần này giới thiệu các khái niệm về điều chỉnh minh họa đối tương (Demo-tuning) và cung cấp chi tiết về phương pháp này.

Minh họa Ảo Để [D(c)i]ni đề cập đến minh họa ảo của lớp thứ c trong đó n là một siêu tham số để đặt độ dài của minh họa ảo, nhỏ hơn nhiều so với độ dài của minh họa thực. Ví dụ, cho một template của tác vụ phân loại nhị phân (xem Hình 2) như:

T̃(x) =T(x)⊕[D(1)]⊕[D(2)] (1)

trong đó ⊕ biểu thị nối chuỗi đầu vào. [D(1)] và [D(2)] tương ứng biểu thị minh họa ảo của hai lớp. Minh họa ảo có thể rất linh hoạt để có thể được tích hợp vào nhiều loại phương pháp học prompt (Liu et al., 2021d; Lester et al., 2021b).

Tiếp theo, chúng tôi nghiên cứu cách thu được minh họa ảo tối ưu, được khởi tạo như một chuỗi các pseudo token ở đầu điều chỉnh. Để giải quyết vấn đề thách thức này, chúng tôi đề xuất sử dụng học đối tương, nhằm thu được biểu diễn hiệu quả bằng cách kéo các hàng xóm gần nhau về mặt ngữ nghĩa lại gần nhau. Trực quan, chúng tôi tin rằng minh họa ảo tối ưu có thể tương tự như "prototype" (Snell et al., 2017), đại diện cho lớp tương ứng, và chúng tôi sẽ thảo luận trong §6.

Phiên bản Dương tính Một yếu tố chính của học đối tương là cách xây dựng các cặp xin, x+in hợp lý. Ở đây, chúng tôi thiết kế một template mới T̃+(x) dựa trên template T̃(x) bằng cách thay thế ngẫu nhiên một trong các minh họa ảo [D(c)] bằng minh họa thực dc như được hiển thị trong Hình 2b:

T̃+(x) =T(x)⊕ T(x(1)in, y(1))⊕[D(2)] (2)

trong đó [D(1)] được thay thế bằng một minh họa d1 của lớp " terrible ". Sử dụng template này, chúng ta có thể chuyển đổi đầu vào x thành ví dụ dương tính tương ứng x+in, tức là, ⟨T̃(xin),T̃+(xin)⟩ là một phiên bản huấn luyện dương tính. Bằng cách này, căn chỉnh minh họa ảo [D(c)] với dc, sự khác biệt duy nhất giữa xin và x+in, và kéo các biểu diễn (hin,h+in) gần nhau hơn trong không gian ngữ nghĩa có thể giải quyết hiệu quả vấn đề tồn tại minh họa terrible hoặc không liên quan bởi các chiến lược lấy mẫu trước đây.

Tối ưu hóa Tương tự như Chen et al. (2020), chúng ta có thể lấy mẫu ngẫu nhiên một minibatch N ví dụ từ Dtrain để xây dựng các cặp dương tính {(xi, x+i)}Ni=1 và lấy một mục tiêu cross-entropy với các âm tính trong batch cho (xi, x+i):

ℓi=−log exp(sim( hi,h+i)/tau)/∑Nj=1 exp(sim( hi,h+j)/tau) (3)

trong đó tau biểu thị một tham số nhiệt độ và sim(hi,hj) là độ tương tự cosine hTihj/∥hi∥·∥hj∥. Các cặp âm tính được tạo thành từ hai ví dụ khác nhau với cùng minh họa trong một minibatch. Trong công trình này, chúng tôi cũng khám phá một khung đối tương đơn giản mà không có cặp âm tính4 tương tự như học tự giám sát không đối tương gần đây (Grill et al., 2020). Liên quan đến khó khăn trong việc tìm một cặp âm tính phù hợp trong không gian ngữ nghĩa cho NLP, đặc biệt trong setting few-shot, chúng tôi chỉ xây dựng các cặp dương tính và định nghĩa sai số bình phương trung bình sau giữa hi và h+i với chuẩn hóa ℓ2,

ℓi=∥hi−h+i∥22= 2−2·hTih+i/∥hi∥2· ∥h+i∥2 (4)

trong đó hi và h+i được thu được qua encoder f(·) trong cùng trạng thái khác với Grill et al. (2020) encode xi và x+i qua hai mạng trong các trạng thái khác nhau (mạng online và target).

Khi các ví dụ có giám sát Dtrain có sẵn, mô hình ngôn ngữ tiền huấn luyện có thể được điều chỉnh để tối thiểu hóa mục tiêu chung bao gồm cross-entropy và mục tiêu đối tương của Phương trình (4). Bằng cách này, trong quá trình suy luận, chúng ta có thể nối đầu vào xin với minh họa ảo được huấn luyện trong template T̃(x), không cần lấy mẫu minh họa thực. Bên cạnh đó, chúng tôi cung cấp phân tích thực nghiệm về lấy mẫu âm tính trong §5.4.

5 Thí nghiệm

5.1 Bộ dữ liệu
Để đánh giá Demo-tuning, chúng tôi tiến hành thí nghiệm trên 6 tác vụ từ bảng xếp hạng GLUE (Wang et al., 2019) và 10 tác vụ phân loại phổ biến khác, bao gồm suy luận ngôn ngữ tự nhiên (SNLI, MNLI, QNLI, RTE), phân loại cảm xúc (SST-2, SST-5, MR, CR, MPQA), paraphrase và tương tự (MRPC, QQP) và phân loại câu (DBpedia, Subj, TREC, Yahoo! Answers). Thống kê chi tiết trong Phụ lục A.

5.2 Cài đặt
Đánh giá Trong quá trình huấn luyện, chúng tôi tuân theo giao thức đánh giá được áp dụng trong Gao et al. (2021a) và giả định một tập phát triển Ddev để lựa chọn mô hình và điều chỉnh siêu tham số, trong đó kích thước giống với Dtrain, tức là |Ddev|=|Dtrain|. Đối với mỗi thí nghiệm, chúng tôi đo hiệu suất trung bình trên 5 phân chia Dtrain và Ddev được lấy mẫu ngẫu nhiên khác nhau sử dụng một tập seed cố định.

Lựa chọn Siêu tham số Chúng tôi triển khai khung của chúng tôi và tái tạo P-tuning bằng cách sử dụng PyTorch (Paszke et al., 2019) và HuggingFace (Wolf et al., 2020). Kết quả chính của LM-BFF trong Bảng 1 từ Gao et al. (2021a). Chúng tôi sử dụng RoBERTa LARGE (Liu et al., 2019) làm mô hình ngôn ngữ tiền huấn luyện và đặt K= 16 . Đối với độ dài n của minh họa ảo trên mỗi lớp, chúng tôi chọn nó từ tập ứng viên {1,2,3,5}.

5.3 Kết quả chính
Chúng tôi áp dụng phương pháp của chúng tôi cho hai kỹ thuật điều chỉnh dựa trên prompt phổ biến, LM-BFF và P-tuning, và so sánh chúng với một số baseline, cụ thể: (1) điều chỉnh tiêu chuẩn trong setting few-shot; (2) "GPT-3" in-context learning: dự đoán zero-shot, nối prompt (ví dụ: minh họa được lấy mẫu ngẫu nhiên); (3) LM-BFF sử dụng minh họa trong bối cảnh với template thủ công. (4) P-tuning sử dụng minh họa trong bối cảnh với template thủ công, trong đó chúng tôi không tìm kiếm cụ thể độ dài tối ưu của prompt liên tục và cố định độ dài m thành 4 trong tất cả các tác vụ.

Trong Bảng 1, chúng tôi báo cáo hiệu suất của các phương pháp baseline và hai biến thể của chúng tôi. Đầu tiên, in-context learning có thể đạt được hiệu suất tương đương hoặc thậm chí cao hơn phương pháp điều chỉnh tiêu chuẩn và các phương pháp prompt-tuning (LM-BFF và P-tuning); sử dụng minh họa trong bối cảnh mang lại cải thiện nhất quán trong đa số các tác vụ, có nghĩa là minh họa đáng được khai thác.

Thứ hai, phương pháp của chúng tôi dựa trên hai kỹ thuật điều chỉnh dựa trên prompt có thể vượt trội nhất quán so với các phương pháp vanilla. Chi tiết, Demo-tuning dựa trên LM-BFF cải thiện điểm trung bình 0.75, so với LM-BFF với minh họa trong bối cảnh đầu vào. Quan trọng hơn, Demo-tuning linh hoạt và trực giao với hầu hết các phương pháp điều chỉnh. Ở đây, để đánh giá tính tương thích, chúng tôi kết hợp Demo-tuning với P-tuning (Liu et al., 2021d), có thể dẫn đến cải thiện điểm trung bình 1.0 tổng cộng. Trong công trình này, chúng tôi không thiết kế template đặc biệt cho P-tuning5. Mặc dù các template cho P-tuning và độ dài prompt không tối ưu, chúng tôi thấy rằng Demo-tuning với P-tuning dẫn đến cải thiện nhất quán trong đa số các tác vụ.

Thứ ba, một ưu điểm của minh họa ảo được đề xuất của chúng tôi là nó có thể được áp dụng tốt cho các tác vụ phân loại câu đa lớp. Bảng 2 đưa ra kết quả của Demo-tuning so với điều chỉnh tiêu chuẩn và điều chỉnh dựa trên prompt. Do hạn chế độ dài đầu vào của mô hình, in-context learning và LM-BFF với minh họa không thể được áp dụng trong tình huống này. Chúng tôi nhận thấy rằng trong khi hiệu suất của LM-BFF kém hơn điều chỉnh, Demo-tuning dựa trên LM-BFF cải thiện điểm 1.7 trong Yahoo và đạt điểm tốt hơn so với điều chỉnh.

5.4 Phân tích Minh họa Ảo
Việc lựa chọn minh họa rất quan trọng cho học dựa trên minh họa (ví dụ: in-context learning và LM-BFF với minh họa). Tiếp theo, chúng tôi so sánh và thảo luận minh họa ảo được đề xuất của chúng tôi với các phương pháp hiện tại.

Lấy mẫu Minh họa Bảng 3 cung cấp tác động của các chiến lược lấy mẫu minh họa. Trong quá trình suy luận, minh họa ảo được đề xuất của chúng tôi thu được bằng học đối tương trong quá trình huấn luyện có thể là một thay thế cho minh họa thực, có thể được xem như một chiến lược lấy mẫu ngầm. Chúng tôi so sánh phương pháp của chúng tôi với các chiến lược lấy mẫu trước đây dựa trên LM-BFF.

Trong khi hiệu suất của lấy mẫu minh họa đồng nhất từ mỗi lớp tốt hơn LM-BFF vanilla trong TREC và SNLI, chúng tôi nhận thấy rằng trên tác vụ MRPC, phương pháp này gây ra mất mát độ chính xác nghiêm trọng, lên đến 3.6. Chúng tôi nghĩ rằng lấy mẫu ngẫu nhiên dễ tạo ra thông tin không liên quan trong minh họa. Để giải quyết vấn đề trên, Gao et al. (2021a) sử dụng RoBERTa hoặc SBERT (Reimers and Gurevych, 2019) để chọn minh họa liên quan đến ví dụ. Chiến lược lấy mẫu dựa trên bộ lọc có thể đạt được cải thiện nhất quán trong đa số các tác vụ, mang lại cải thiện cao nhất với 3.6 trên tác vụ TREC. Chúng tôi cho rằng phương pháp kiểu KNN này, nối các ví dụ và minh họa gần nhau về mặt ngữ nghĩa với ví dụ, có thể thúc đẩy các mô hình ngôn ngữ giải mã các mẫu có ý nghĩa.

Minh họa ảo, một thay thế cho minh họa thực trong quá trình suy luận, tức là tránh các bước lấy mẫu phức tạp, có thể đạt được cải thiện trong hầu hết các tác vụ. Bên cạnh phương pháp được đề xuất của chúng tôi, chúng tôi thiết kế một chiến lược đơn giản để xây dựng minh họa ảo thông qua việc lấy trung bình các biểu diễn của các phiên bản có cùng nhãn. Chúng tôi nhận thấy rằng xây dựng minh họa ảo với việc lấy trung bình đơn giản các phiên bản gây ra hiệu suất kém trong hầu hết các tác vụ. Tuy nhiên, phương pháp của chúng tôi với học đối tương chiếm ưu thế hơn các phương pháp trước đây. Ngoại lệ duy nhất là SNLI, điểm chỉ tương đương với lấy mẫu ngẫu nhiên. Chúng tôi giả định rằng điều này được gây ra bởi một số vấn đề nhầm lẫn, có thể tồn tại trong chiến lược dựa trên bộ lọc liên quan đến sự gần gũi về mặt ngữ nghĩa giữa các minh họa đối tương.

Tối ưu hóa có/ không có Mẫu Âm tính Hình 3 đưa ra kết quả so sánh giữa tối ưu hóa minh họa ảo với lấy mẫu âm tính và không có lấy mẫu âm tính. Chúng tôi tiến hành thí nghiệm với các chiến lược tối ưu hóa khác nhau trên 3 tác vụ. Chúng tôi thấy rằng tối ưu hóa mục tiêu của Phương trình 3, tức là học đối tương thông thường với mẫu âm tính, gây ra suy giảm hiệu suất đáng kể, trong đó điểm trung bình thậm chí thấp hơn LM-BFF. Chúng tôi nghĩ có hai lý do có thể: (1) Trong các tác vụ NLP, việc tìm một cặp âm tính hợp lý về mặt ngữ nghĩa là khó khăn, đặc biệt trong setting few-shot; (2) Các cặp âm tính có thể trở thành các cặp ví dụ-minh họa mà không có giới hạn cụ thể, điều này sẽ gây ra một sự nhầm lẫn nhất định cho mô hình. Hơn nữa, mục tiêu của chúng tôi là thu được minh họa ảo tối ưu cho các tác vụ downstream. Sử dụng tối ưu hóa đối tương mà không có lấy mẫu âm tính có thể là một giải pháp phù hợp hơn.

Độ dài Minh họa Hình 4 hiển thị nghiên cứu ablation về độ dài n của minh họa ảo trên mỗi lớp. Chúng tôi so sánh Demo-tuning với biến thể của nó không có học đối tương trong các cài đặt khác nhau về độ dài n. Đáng chú ý rằng không có học đối tương, một minh họa ảo sẽ suy thoái thành một prompt liên tục. Chúng tôi thấy rằng một độ dài tương đối ngắn hơn (ví dụ: 2 hoặc 3) có thể đạt được cải thiện hiệu suất ổn định trong QNLI và MR. Ngược lại, một độ dài lớn hơn (ví dụ: 20) có thể giảm hiệu suất. Chúng tôi cho rằng khi độ dài của minh họa ảo tăng, nó sẽ đưa thêm tham số vào mô hình, làm cho việc học từ một lượng nhỏ dữ liệu được chú thích trở nên thách thức. Demo-tuning có thể đạt được cải thiện nhất quán trong các độ dài khác nhau so với biến thể của nó. Do đó, chúng ta có thể kết luận rằng minh họa ảo được tối ưu hóa bởi khung đối tương đơn giản đóng một vai trò khác với prompt liên tục.

6 Thảo luận
Chúng tôi sẽ thảo luận một số tính chất thuận lợi của điều chỉnh minh họa đối tương và trình bày một số vấn đề mở:

Bổ sung có thể cho Điều chỉnh Hiệu quả Tham số. Các nghiên cứu trước đây (Liu et al., 2021d; Li and Liang, 2021) đã chứng minh hiệu quả của prompt-tuning (ví dụ: P-tuning, Prefix-tuning) như một phương pháp điều chỉnh hiệu quả tham số cho PLM khổng lồ. Phương pháp của chúng tôi có thể phục vụ như một bổ sung hoặc điều chỉnh hiệu quả tham số thông qua chỉ điều chỉnh minh họa với PLM cố định. Chúng tôi để lại điều này cho công việc tương lai.

Quan hệ với Học Prototype. Trong §4, chúng tôi lưu ý rằng minh họa ảo tối ưu có thể tương tự với "prototype" (Snell et al., 2017), đại diện cho lớp tương ứng. Phương pháp của chúng tôi có thể có kết nối với học prototype, và phân tích thực nghiệm và lý thuyết tiếp theo nên được tiến hành.

Minh họa như Tri thức Bên ngoài. Nhớ lại rằng những minh họa được nối đó tương tự như các nghiên cứu trước đây như RAG (Lewis et al., 2020b), REALM (Guu et al., 2020) truy xuất và nối các văn bản liên quan làm tri thức bên ngoài (Zhang et al., 2022b). Chúng tôi nghĩ rằng việc điều tra các phương pháp tiêm tri thức mới thông qua minh họa cũng thú vị.

Chúng tôi tiếp tục thảo luận một vài điểm yếu của phương pháp chúng tôi trong hình thức hiện tại và xem xét một số hướng có thể cho công việc tương lai. Một mặt, công trình của chúng tôi vẫn bị ảnh hưởng bởi phân phối nhãn thiên lệch/đuôi dài. Lưu ý rằng chúng tôi thu được minh họa ảo được tối ưu hóa thông qua học đối tương; do đó, những minh họa ảo của các lớp có nhiều mẫu có thể chi phối giai đoạn huấn luyện. Hạn chế này có thể được cải thiện với các chiến lược lấy mẫu có trọng số. Mặt khác, phương pháp của chúng tôi không thể xử lý trực tiếp các tác vụ dự đoán cấu trúc. Tích hợp minh họa với các phương pháp dựa trên prefix-tuning có thể giúp giảm thiểu những hạn chế như vậy.

7 Kết luận và Công việc Tương lai
Trong công trình này, chúng tôi đề xuất điều chỉnh minh họa đối tương, một phương pháp model-agnostic đơn giản cho các mô hình ngôn ngữ tiền huấn luyện, cải thiện hiệu suất prompt-tuning hiện đại mà không cần thiết phải lựa chọn minh họa. Trong tương lai, chúng tôi dự định khám phá các hướng sau: 1) nghiên cứu kết nối giữa minh họa ảo và prototype và phân tích lý thuyết giải pháp tối ưu của minh họa cho prompt-tuning. 2) áp dụng công trình của chúng tôi cho nhiều tác vụ NLP hơn và cố gắng thích ứng với tạo sinh ngôn ngữ tự nhiên.

8 Hạn chế
Điều chỉnh minh họa đối tương của chúng tôi có những hạn chế. Đầu tiên, mô hình của chúng tôi tận dụng mô hình ngôn ngữ tiền huấn luyện; do đó, cần thiết phải chi phí tài nguyên GPU. Bên cạnh đó, trong setting few-shot, cải thiện hiệu suất vẫn còn hạn chế với minh họa ảo được học chỉ trong một vài phiên bản huấn luyện. Đáng để nghiên cứu việc truy xuất bối cảnh liên quan từ internet làm "minh họa" để giúp NLP hiệu quả.

Lời cảm ơn
Chúng tôi muốn bày tỏ lòng biết ơn đến các reviewer ẩn danh vì những bình luận tử tế của họ. Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62206246, 91846204 và U19B2027), Quỹ Khoa học Tự nhiên tỉnh Chiết Giang, Trung Quốc (Số LGG22F030011), Quỹ Khoa học Tự nhiên Ninh Ba (2021J190), và Chương trình Giới thiệu Nhân tài Vĩnh Giang (2021A-156-G). Công trình của chúng tôi được hỗ trợ bởi Trung tâm Công nghệ Thông tin và Phòng thí nghiệm Trọng điểm Nhà nước về CAD&CG, Trường Đại học Chiết Giang.

Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên như bản gốc do chứa các thông tin trích dẫn chuẩn]
