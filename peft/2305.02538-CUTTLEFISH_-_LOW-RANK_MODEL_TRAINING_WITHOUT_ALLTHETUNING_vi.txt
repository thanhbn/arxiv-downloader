# 2305.02538.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.02538.pdf
# Kích thước tệp: 3001830 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CUTTLEFISH: HUẤN LUYỆN MÔ HÌNH RANK THẤP MÀ KHÔNG CẦN TẤT CẢ VIỆC ĐIỀU CHỈNH
Hongyi Wang1 Saurabh Agarwal2 Pongsakorn U-chupala3 Yoshiki Tanaka3
Eric P. Xing4 1 5 Dimitris Papailiopoulos6

TÓM TẮT
Nghiên cứu gần đây đã chỉ ra rằng huấn luyện mạng nơ-ron rank thấp có thể giảm hiệu quả tổng số tham số có thể huấn luyện mà không hy sinh độ chính xác dự đoán, dẫn đến tăng tốc end-to-end. Tuy nhiên, huấn luyện mô hình rank thấp đòi hỏi phải điều chỉnh một số siêu tham số phân tách bổ sung, chẳng hạn như rank của phân tách tại mỗi lớp. Trong bài báo này, chúng tôi giải quyết thách thức này bằng cách giới thiệu CUTTLEFISH, một phương pháp huấn luyện rank thấp tự động loại bỏ nhu cầu điều chỉnh các siêu tham số phân tách. CUTTLEFISH tận dụng quan sát rằng sau một vài epoch huấn luyện full-rank, stable rank (tức là một xấp xỉ của rank thật) của mỗi lớp ổn định ở một giá trị không đổi. CUTTLEFISH chuyển từ huấn luyện full-rank sang huấn luyện rank thấp khi stable rank của tất cả các lớp đã hội tụ, đặt chiều của mỗi phân tách theo stable rank tương ứng của nó. Kết quả của chúng tôi cho thấy CUTTLEFISH tạo ra các mô hình nhỏ hơn đến 5,6 lần so với các mô hình full-rank, và đạt được quá trình huấn luyện end-to-end nhanh hơn đến 1,2 lần trong khi vẫn duy trì độ chính xác tương đương. Hơn nữa, CUTTLEFISH vượt trội so với các phương pháp huấn luyện mô hình rank thấp hiện đại và các baseline nổi bật khác. Mã nguồn cho việc triển khai của chúng tôi có thể được tìm thấy tại: https://github.com/hwang595/Cuttlefish.

1 GIỚI THIỆU
Khi các mô hình dựa trên mạng nơ-ron đã trải qua sự tăng trưởng theo cấp số nhân về số lượng tham số, từ 23 triệu trong ResNet-50 (2015) đến 175 tỷ trong GPT-3 (2020) và OPT-175B (2022) (Devlin et al., 2018; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2022), việc huấn luyện các mô hình này đã trở nên ngày càng thách thức, ngay cả với sự hỗ trợ của các bộ tăng tốc hiện đại như GPU và TPU. Vấn đề này đặc biệt rõ rệt trong các môi trường hạn chế tài nguyên, chẳng hạn như federated learning trên các thiết bị khác nhau (Kairouz et al., 2019; Wang et al., 2020b; 2021b). Để đáp ứng thách thức này, các nhà nghiên cứu đã khám phá việc giảm tham số có thể huấn luyện trong giai đoạn đầu của quá trình huấn luyện (Frankle & Carbin, 2018; Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a) như một chiến lược để tăng tốc quá trình huấn luyện.

--- TRANG 2 ---
CUTTLEFISH: Huấn luyện Mô hình Rank Thấp mà không cần Tất cả Việc Điều chỉnh

ure 1. Tổng số tham số Full-rank training epochs, E, là cần thiết (ví dụ, cả E = 0 và E = 120 đều không mang lại độ chính xác mô hình tối ưu). Hơn nữa, để đạt được độ chính xác thỏa đáng, một số công trình trước đó (Wang et al., 2021a) đã đề xuất loại trừ phân tách khỏi K lớp đầu tiên, tạo ra một "mạng hybrid" cân bằng kích thước mô hình và độ chính xác thông qua lựa chọn K.

[Hình 1 hiển thị các biểu đồ so sánh giữa CUTTLEFISH và kết quả điều chỉnh grid search với các tham số khác nhau]

Trong bài báo này, chúng tôi giới thiệu một phương pháp mới để tự động xác định các siêu tham số liên quan đến huấn luyện rank thấp, đảm bảo rằng mô hình phân tách kết quả đạt được cả kích thước nhỏ gọn và độ chính xác cuối cùng cao.

Thách thức. Chúng tôi muốn nhấn mạnh một số lý do tại sao vấn đề này đặt ra những thách thức đáng kể. Thứ nhất, không gian tìm kiếm S rất lớn. Đối với một mạng nơ-ron fully connected (FC) hai lớp ẩn với 100 nơ-ron trong mỗi lớp (giả sử rank cho mỗi lớp là 100) và huấn luyện với 100 epoch, cardinality của không gian tìm kiếm là |S| = 100^100 × 100^2 = 2 × 10^6. Hơn nữa, mục tiêu tối ưu hóa tự động các siêu tham số phân tách huấn luyện rank thấp trong khi duy trì các lợi thế của tăng tốc huấn luyện end-to-end khiến các phương pháp neural architecture search (NAS) truyền thống trở nên không thực tế. NAS đòi hỏi huấn luyện đồng thời cả kiến trúc mạng và trọng số mạng, dẫn đến yêu cầu tính toán vượt xa những gì cần thiết cho huấn luyện mô hình tiêu chuẩn.

Trong công trình này, chúng tôi trình bày CUTTLEFISH, một phương pháp huấn luyện phân tách rank thấp tự động loại bỏ nhu cầu điều chỉnh các siêu tham số phân tách. Chúng tôi quan sát thấy một mẫu quan trọng trong đó rank ước tính của mỗi lớp thay đổi nhanh chóng trong giai đoạn đầu của huấn luyện rồi ổn định xung quanh một giá trị không đổi (như mô tả trong Hình 2). Chúng tôi khai thác quan sát này để phát triển một heuristic đơn giản cho việc chọn rank lớp R và thời gian huấn luyện full-rank E: (i) chuyển từ huấn luyện mô hình full-rank sang huấn luyện mô hình rank thấp khi tất cả stable rank của lớp đã hội tụ về một hằng số, và (ii) sử dụng những hằng số này làm rank của phân tách.

[Hình 2 hiển thị rank ước tính cho các lớp khác nhau trong ResNet-18]

Ngoài việc xác định rank phân tách và thời gian huấn luyện full-rank, CUTTLEFISH cũng giải quyết vấn đề quyết định lớp nào cần phân tách. Đối với các mạng nơ-ron tích chập (CNN), CUTTLEFISH quan sát thấy rằng việc phân tách các lớp đầu không dẫn đến tăng tốc đáng kể, như được trình bày chi tiết trong Phần 3.5. Quan sát này, cùng với những hiểu biết từ nghiên cứu trước đó (Wang et al., 2021a), ngụ ý rằng việc phân tách các lớp đầu có thể ảnh hưởng tiêu cực đến độ chính xác mô hình cuối cùng mà không mang lại lợi ích hiệu suất đáng kể. Để giải quyết thách thức này, CUTTLEFISH thực hiện profiling nhẹ để xác định các lớp cần phân tách, đảm bảo rằng phân tách chỉ xảy ra ở những lớp có thể nâng cao hiệu quả tốc độ huấn luyện.

--- TRANG 3 ---
CUTTLEFISH: Huấn luyện Mô hình Rank Thấp mà không cần Tất cả Việc Điều chỉnh

Đóng góp của chúng tôi. Chúng tôi quan sát hiệu ứng ổn định trong stable rank của các lớp mạng nơ-ron (NN) trong quá trình huấn luyện, trong đó stable rank của chúng ban đầu thay đổi nhanh chóng rồi hội tụ về một hằng số. Dựa trên quan sát này, chúng tôi thiết kế một heuristic để lựa chọn thích ứng rank của mỗi lớp và thời gian khởi động huấn luyện full-rank. Chúng tôi triển khai kỹ thuật của mình, được gọi là CUTTLEFISH, và đánh giá nó trong môi trường huấn luyện quy mô lớn cho cả tác vụ ngôn ngữ và thị giác máy tính. Kết quả thí nghiệm toàn diện của chúng tôi chứng minh rằng CUTTLEFISH tự động chọn tất cả các siêu tham số phân tách trong quá trình huấn luyện một cách tức thời, loại bỏ nhu cầu nhiều thử nghiệm thực nghiệm để điều chỉnh siêu tham số phân tách. CUTTLEFISH cân bằng giữa kích thước mô hình và độ chính xác dự đoán cuối cùng, xuất sắc ở ít nhất một khía cạnh của việc tạo ra các mô hình nhỏ hơn, chính xác hơn và đạt được tăng tốc huấn luyện đáng kể so với huấn luyện rank thấp hiện đại, pruning có cấu trúc, huấn luyện thưa thớt, huấn luyện lượng tử hóa, và các phương pháp phân tách có thể học (Rastegari et al., 2016; Frankle et al., 2019; Wang et al., 2020a; Yu & Huang, 2019; You et al., 2019; Idelbayev & Carreira-Perpinán, 2020; Khodak et al., 2020; Wang et al., 2021a).

1.1 Công trình liên quan.
Một số phương pháp đã được phát triển trong tài liệu để loại bỏ sự dư thừa trong các tham số của NN hiện đại. Nén mô hình phấn đấu loại bỏ sự dư thừa trong các tham số của NN đã được huấn luyện (Han et al., 2015a). Theo thời gian, nhiều phương pháp đã được thiết kế để loại bỏ các trọng số dư thừa trong NN, bao gồm pruning (Li et al., 2016; Wen et al., 2016; Hu et al., 2016; Zhu & Gupta, 2017; He et al., 2017; Yang et al., 2017; Liu et al., 2018; Yu et al., 2018; 2019; Sreenivasan et al., 2022a), lượng tử hóa (Rastegari et al., 2016; Zhu et al., 2016; Hubara et al., 2016; Wu et al., 2016; Hubara et al., 2017; Zhou et al., 2017), phân tách rank thấp (Xue et al., 2013; Sainath et al., 2013; Jaderberg et al., 2014; Wiesler et al., 2014; Konečný et al., 2016), và chưng cất kiến thức (Hinton et al., 2015; Yu et al., 2019; Sanh et al., 2019; Jiao et al., 2020; Li et al., 2023).

Giả thuyết Vé Số Trúng Thưởng (LTH) gợi ý rằng các mạng con nhỏ hơn, được khởi tạo ngẫu nhiên có thể được huấn luyện để đạt mức độ chính xác tương đương với mạng đầy đủ, mặc dù việc xác định những mạng con này có thể là thách thức về mặt tính toán (Frankle & Carbin, 2018). Iterative Magnitude Pruning (IMP) được thiết kế để ổn định LTH trong khi giảm chi phí tính toán thông qua các bước khởi động (Frankle et al., 2019). Các nỗ lực khác đã tìm cách loại bỏ nhu cầu rewinding trọng số mô hình (Renda et al., 2020) và xác định vé thắng tại thời điểm khởi tạo (Wang et al., 2020a; Sreenivasan et al., 2022b). Hơn nữa, các nhà nghiên cứu đã khám phá việc làm thưa thớt NN trong quá trình huấn luyện (Evci et al., 2020). Tuy nhiên, những phương pháp thưa thớt này tập trung vào tính thưa thớt không có cấu trúc, không mang lại tăng tốc thực tế trên phần cứng hiện tại. Ngược lại, huấn luyện rank thấp có thể dẫn đến tăng tốc hữu hình.

Huấn luyện phân tách rank thấp, cũng như các phương pháp pruning có cấu trúc khác, nhằm đạt được NN với tính thưa thớt có cấu trúc trong quá trình huấn luyện, cho phép thu được tăng tốc hữu hình (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; You et al., 2020; Wang et al., 2021a; Chen et al., 2021b). Huấn luyện phân tách rank thấp cũng đã được sử dụng trong các phương pháp federated learning để cải thiện hiệu quả giao tiếp và nhận thức về tính không đồng nhất của phần cứng (Hyeon-Woo et al., 2022; Yao et al., 2021). Các kỹ thuật phân tách rank thấp đã được chứng minh là có thể kết hợp và sử dụng để huấn luyện mạng rank thấp từ đầu (Ioannou et al., 2015). Tuy nhiên, phương pháp này dẫn đến sự mất mát độ chính xác đáng chú ý, như được chứng minh trong (Wang et al., 2021a). Để giải quyết vấn đề này, (Khodak et al., 2020) giới thiệu khởi tạo phổ và Frobenius decay, trong khi (Vodrahalli et al., 2022) đề xuất phương pháp Nonlinear Kernel Projection như một thay thế cho SVD. Huấn luyện rank thấp cũng đã được nghiên cứu để fine-tuning các mô hình pre-trained quy mô lớn (Hu et al., 2021). Những kỹ thuật này đều đòi hỏi các siêu tham số bổ sung, có thể tẻ nhạt khi fine-tune. Phương pháp nén LC cố gắng giải quyết vấn đề này bằng cách học một cách rõ ràng R trong quá trình huấn luyện mô hình thông qua tối ưu hóa xen kẽ (Idelbayev & Carreira-Perpinán, 2020). Tuy nhiên, cách tiếp cận này đòi hỏi nhiều tính toán. Phương pháp CUTTLEFISH đề xuất của chúng tôi tự động xác định tất cả các siêu tham số phân tách trong quá trình huấn luyện một cách tức thì, loại bỏ chi phí tính toán nặng và nhu cầu nhiều thử nghiệm thực nghiệm để điều chỉnh siêu tham số phân tách.

--- TRANG 4 ---
CUTTLEFISH: Huấn luyện Mô hình Rank Thấp mà không cần Tất cả Việc Điều chỉnh

Các phép biến đổi thay thế cũng đã được điều tra, bao gồm ma trận Butterfly (Chen et al., 2022), sự kết hợp của phân tách rank thấp và thưa thớt hóa (Chen et al., 2021a), và ma trận block-diagonal (Dao et al., 2022). Hơn nữa, các kiến trúc mới đã được phát triển để nâng cao hiệu quả huấn luyện hoặc suy luận, chẳng hạn như SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al., 2016), ShuffleNet (Zhang et al., 2018), EfficientNet (Tan & Le, 2019), MobileNets (Howard et al., 2017), Xception (Chollet, 2017), ALBERT (Lan et al., 2019), và Reformer (Kitaev et al., 2020).

2 SƠ BỘ
Trong phần này, chúng tôi trình bày tổng quan về các khái niệm cốt lõi của phân tách rank thấp cho các lớp NN khác nhau, cùng với một lựa chọn các phương pháp huấn luyện chuyên biệt được thiết kế đặc biệt cho huấn luyện phân tách rank thấp.

2.1 Phân tách rank thấp của các lớp NN
Lớp FC/MLP Mixer. Một mạng nơ-ron fully connected (FC) 2 lớp có thể được biểu diễn là f(x) = σ((xW₁)W₂), trong đó W là các ma trận trọng số, σ() là một hàm kích hoạt tùy ý, và x là điểm dữ liệu đầu vào. Ma trận trọng số W có thể được phân tách thành UV^T. Một cách tiếp cận tương tự có thể được áp dụng cho các lớp ResMLP/MLP mixer, trong đó mỗi trọng số có thể học được có thể được phân tách theo cách tương tự (Touvron et al., 2021a; Tolstikhin et al., 2021).

Lớp tích chập. Đối với một lớp tích chập với các chiều (m,n,k,k), trong đó m và n là số kênh đầu vào và đầu ra và k đại diện cho kích thước của bộ lọc tích chập, một cách tiếp cận phổ biến bao gồm việc phân tách ma trận 2D được trải phẳng. Chúng tôi sẽ thảo luận về một phương pháp phổ biến để phân tách một lớp tích chập. Ban đầu, tensor 4D W được trải phẳng để có được một ma trận 2D có hình dạng (mk²,n), trong đó mỗi cột đại diện cho trọng số của một bộ lọc tích chập được vector hóa. Rank của ma trận được trải phẳng được xác định bởi min{mk²,n}. Việc phân tách ma trận được trải phẳng tạo ra U ∈ ℝ^(mk²×r) và V^T ∈ ℝ^(r×n). Việc định hình lại các ma trận đã phân tách U,V^T trở lại 4D tạo ra U ∈ ℝ^(m×r×k×k) và V^T ∈ ℝ^(r×n). Do đó, việc phân tách một lớp tích chập tạo ra một lớp tích chập mỏng hơn U với r bộ lọc tích chập và một lớp chiếu tuyến tính V^T. Các V^T cũng có thể được biểu diễn bằng một lớp tích chập 1×1, chẳng hạn như V^T ∈ ℝ^(r×n×1×1), phù hợp hơn cho các tác vụ thị giác máy tính vì nó hoạt động trực tiếp trong miền không gian (Lin et al., 2013; Wang et al., 2021a).

Lớp Multi-head attention (MHA). Một lớp attention p-head học các cơ chế attention trên key, value, và query (K,V,Q) của mỗi token đầu vào:
MHA(Q,K,V) = Concat(head₁,...,head_p)W^O.
Mỗi head thực hiện tính toán:
head_i = Attention(QW_Q^(i), KW_K^(i), VW_V^(i))
= softmax((QW_Q^(i)(W_K^(i))^T K^T)/√(d/p)) VW_V^(i).
trong đó d là chiều ẩn. Các trọng số có thể huấn luyện W_Q^(i), W_K^(i), W_V^(i), i ∈ {1,2,...,p} có thể được phân tách bằng cách đơn giản phân rã tất cả các trọng số có thể học W trong một lớp attention và thu được UV^T (Vaswani et al., 2017).

2.2 Phương pháp huấn luyện cho mạng rank thấp
Kiến trúc NN hybrid. Đã được lưu ý rằng việc phân tách các lớp ban đầu có thể ảnh hưởng tiêu cực đến độ chính xác của mô hình (Konečný et al., 2016; Waleffe & Rekatsinas, 2020; Wang et al., 2021a). Một giải thích có thể là các lớp NN có thể được xem như các bộ trích xuất đặc trưng, và các đặc trưng kém được trích xuất bởi các lớp đầu có thể tích lũy và lan truyền khắp NN. Để giải quyết vấn đề này, kiến trúc NN hybrid đã được đề xuất, chỉ phân tách các lớp dưới trong khi giữ các lớp ban đầu ở dạng full-rank (Wang et al., 2021a). Các trọng số của một NN L-lớp full-rank có thể được biểu diễn là W = {W_i | 1 ≤ i ≤ L}. Các trọng số của mô hình hybrid tương ứng có thể được biểu diễn là H = {W₁, W₂, ..., W_K, U_{K+1}, V^T_{K+1}, ..., U_{L-1}, V^T_{L-1}, W_L}, trong đó K là số lớp không được phân tách và được coi là một siêu tham số cần được điều chỉnh thủ công (Wang et al., 2021a). Điều quan trọng cần lưu ý là lớp phân loại cuối cùng, tức là W_L, thường không được phân tách (Khodak et al., 2020; Wang et al., 2021a).

Huấn luyện từ full-rank sang rank thấp. Huấn luyện các mô hình phân tách rank thấp từ đầu thường dẫn đến giảm độ chính xác (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). Để giảm thiểu sự sụt giảm này, thông thường sẽ huấn luyện mô hình full-rank trong E epoch trước khi phân tách nó (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang et al., 2021a). Tuy nhiên, việc xác định số epoch huấn luyện full-rank phù hợp được coi là một siêu tham số và thường được điều chỉnh thủ công trong các thí nghiệm (Wang et al., 2021a). Các quan sát cho thấy rằng việc tìm đúng số epoch huấn luyện full-rank là rất quan trọng để đạt được độ chính xác mô hình cuối cùng tối ưu trong NN phân tách rank thấp.

Khởi tạo và weight decay. Các mạng rank thấp phân tách có thể hưởng lợi từ các phương pháp khởi tạo được thiết kế riêng (Ioannou et al., 2015; Khodak et al., 2020). Một phương pháp như vậy, được gọi là khởi tạo phổ, nhằm xấp xỉ hành vi của các phương pháp khởi tạo hiện có (Khodak et al., 2020). Khởi tạo phổ đại diện cho một trường hợp đặc biệt của việc chuyển từ huấn luyện full-rank sang rank thấp với E = 0. Ngoài ra, các kỹ thuật chính quy hóa cụ thể đã được đề xuất để nâng cao độ chính xác của các mạng rank thấp. Ví dụ, Frobenius decay áp dụng weight decay trên ||UV^T||²_F thay vì ||U||²_F + ||V^T||²_F trong quá trình huấn luyện phân tách rank thấp.

--- TRANG 5 ---
3 CUTTLEFISH: HUẤN LUYỆN PHÂN TÁCH RANK THẤP TỰ ĐỘNG

Trong phần này, chúng tôi phác thảo công thức hóa vấn đề của CUTTLEFISH, trình bày chi tiết về từng siêu tham số phân tách, và mô tả các heuristic của CUTTLEFISH để xác định tất cả những siêu tham số phân tách này.

3.1 Công thức hóa vấn đề.
Không gian tìm kiếm cho điều chỉnh phân tách thích ứng được định nghĩa bởi ba tập hợp siêu tham số, cụ thể là S = (E,K,R) (epoch huấn luyện full-rank, số lớp ban đầu vẫn không được phân tách, và rank phân tách lớp). Mục tiêu của CUTTLEFISH là tìm một ŝ ∈ S tối ưu một cách tức thì, với chi phí tính toán tối thiểu trong quá trình huấn luyện, sao cho các mô hình phân tách rank thấp kết quả vừa nhỏ gọn vừa duy trì độ chính xác cao, có thể so sánh với các đối tác full-rank của chúng.

3.2 Các thành phần trong không gian tìm kiếm và sự đánh đổi giữa các lựa chọn siêu tham số.
Epoch huấn luyện full-rank E. Giá trị của E có thể dao động từ 0 đến T-1. Cả giá trị quá nhỏ (ví dụ, E = 0) và quá lớn (ví dụ, E = 120) của E đều không mang lại độ chính xác tốt nhất (Hình 1), nhấn mạnh sự cần thiết của việc điều chỉnh E.

Số lớp full-rank K. Như đã đề cập trước đó, các trọng số của kiến trúc NN hybrid có thể được biểu diễn bởi H = {W₁,...,W_K,U_{K+1},V^T_{K+1},...,U_{L-1},V^T_{L-1},W_L}, trong đó K là một siêu tham số cần được điều chỉnh. Việc lựa chọn K có thể dao động từ 1 đến L-1, có nghĩa là lớp đầu tiên và cuối cùng luôn không được phân tách. Việc phân tách thêm các lớp dẫn đến mất mát độ chính xác tăng lên nhưng cũng giảm kích thước mô hình và độ phức tạp tính toán. Do đó, một lựa chọn tối ưu cho K nên cân bằng sự đánh đổi giữa mất mát độ chính xác và tỷ lệ nén mô hình.

Lựa chọn rank cho các lớp phân tách R. R chỉ định các rank được sử dụng khi phân tách (L-K-1) lớp trong kiến trúc NN hybrid, tức là R = {r_i | K+1 ≤ i ≤ L-1}. Đối với một trọng số lớp W_i ∈ ℝ^{m×n}, rank đầy đủ của lớp là rank(W_i) = min{m,n}. Do đó, 1 ≤ r_i ≤ rank(W_i), ∀i ∈ {K+1,...,L-1}. Sử dụng r quá nhỏ để phân tách một lớp có thể dẫn đến giảm độ chính xác. Tuy nhiên, việc sử dụng r tương đối lớn để phân tách lớp có thể ảnh hưởng tiêu cực đến tỷ lệ nén mô hình.

Trong bài báo này, chúng tôi phát triển các heuristic tự động xác định các lựa chọn tối ưu cho từng thành phần trong không gian tìm kiếm, tức là ŝ ∈ S, để cân bằng giữa độ chính xác cuối cùng và kích thước mô hình.

Tại sao việc tìm một s ∈ S phù hợp lại khó khăn?
Thứ nhất, cardinality của không gian tìm kiếm, |S|, rất lớn. Hơn nữa, mục tiêu chính của huấn luyện phân tách rank thấp là tăng tốc huấn luyện mô hình. Do đó, việc xác định ŝ mà không đưa vào chi phí tính toán đáng kể là rất quan trọng. Trong khi các phương pháp kiểu Neural Architecture Search (NAS) có thể được sử dụng để tìm kiếm s ∈ S, chúng dẫn đến chi phí tính toán cao. Do đó, việc áp dụng các thuật toán dựa trên NAS không phù hợp cho kịch bản của chúng ta, vì mục tiêu của chúng ta là đạt được huấn luyện nhanh hơn.

3.3 Xác định rank phân tách (R) cho các lớp NN.
Trong công trình trước đây, rank của các lớp phân tách thường được coi là một siêu tham số, với một tỷ lệ rank toàn cục cố định thường được sử dụng, ví dụ, R = {αrank(W_i) | 1 ≤ i ≤ L} (Khodak et al., 2020; Wang et al., 2021a). Tuy nhiên, một câu hỏi quan trọng cần xem xét là tất cả các lớp có hội tụ về cùng một α trong quá trình huấn luyện không?

Metric ước tính rank. Người ta có thể thắc mắc tại sao chúng ta không thể đơn giản sử dụng rank thông thường để ước tính rank lớp của NN. Câu trả lời là rank thông thường luôn đầy đủ đối với trọng số lớp của NN. Tuy nhiên, ma trận trọng số NN "gần như" rank thấp khi chúng thể hiện sự suy giảm phổ nhanh chóng. Do đó, chúng ta cần một metric để ước tính rank lớp. Trong CUTTLEFISH, chúng tôi sử dụng stable rank, đóng vai trò như một proxy có giá trị cho rank thực tế vì nó phần lớn không bị ảnh hưởng bởi các giá trị kỳ dị nhỏ, để ước tính rank của trọng số lớp mô hình W. Định nghĩa của stable rank là stable rank(Σ) = 1^T Σ^2 1 / σ²_max(W), trong đó 1, σ²_max(), và Σ tương ứng đại diện cho vector cột đơn vị, giá trị kỳ dị bình phương lớn nhất, và ma trận đường chéo lưu trữ tất cả các giá trị kỳ dị theo thứ tự giảm dần, tức là σ₁ ≥ σ₂ ≥ ... ≥ σ_{rank(W)}. Một lợi thế khác của việc sử dụng stable rank là việc tính toán của nó không yêu cầu chỉ định bất kỳ siêu tham số bổ sung nào.

Scaled stable rank. Stable rank, bỏ qua các giá trị kỳ dị nhỏ bé, thường dẫn đến ước tính rank rất thấp. Điều này có thể hoạt động tốt cho các tác vụ tương đối nhỏ, chẳng hạn như CIFAR-10. Tuy nhiên, đối với các tác vụ quy mô lớn hơn như ImageNet, việc sử dụng stable rank trực tiếp dẫn đến sụt giảm độ chính xác không tầm thường là 2,3% (chi tiết có thể được tìm thấy trong phụ lục). Để giải quyết vấn đề này, chúng tôi đề xuất sử dụng scaled stable rank. Scaled stable rank giả định rằng rank ước tính của một ma trận được khởi tạo ngẫu nhiên, tức là W⁰ (trọng số mô hình tại epoch thứ 0), nên gần hoặc bằng rank đầy đủ. Tuy nhiên, dựa trên các quan sát thực nghiệm của chúng tôi, ước tính stable rank của các trọng số được khởi tạo ngẫu nhiên có xu hướng không phải là rank đầy đủ. Do đó, chúng tôi lưu trữ tỷ lệ của rank đầy đủ với stable rank ban đầu (ký hiệu là β, ví dụ, nếu rank(W) = 512 và stable rank(Σ⁰) = 200, thì β = 512/200). Chúng tôi mở rộng stable rank của mỗi epoch bằng:

scaled stable rank(Σᵗ,β) = β · stable rank(Σᵗ);
β = rank(W⁰)/stable rank(Σ⁰), ∀t ∈ {1,2,...,T}.

Lựa chọn rank CUTTLEFISH. Chúng tôi quan sát thấy rằng các lớp khác nhau có xu hướng hội tụ về các stable rank khác nhau (một ví dụ được hiển thị trong Hình 3, với các xu hướng tương tự được tìm thấy trong các tác vụ khác, như được nêu chi tiết trong phụ lục). Các lớp giữa thường hội tụ về α lớn hơn, cho thấy sự dư thừa lớn hơn. Do đó, không có khả năng một tỷ lệ rank cố định là tối ưu, vì nó có thể không loại bỏ được tất cả sự dư thừa trong trọng số lớp hoặc quá tích cực trong việc nén trọng số mô hình, do đó làm tổn hại đến độ chính xác cuối cùng. CUTTLEFISH sử dụng scaled stable rank tại epoch E (tức là điểm chuyển đổi từ full-rank sang rank thấp) để phân tách mô hình full-rank và thu được mô hình phân tách rank thấp.

[Hình 3 hiển thị tỷ lệ rank (α) của stable rank cho ResNet-18 được huấn luyện trên CIFAR-10]

3.4 Xác định epoch huấn luyện full-rank E
Như đã thảo luận trong Phần 1, cả giá trị E quá nhỏ và quá lớn đều không mang lại độ chính xác tối ưu. Hơn nữa, các giá trị E lớn hơn cũng dẫn đến thời gian huấn luyện chậm hơn, vì các mô hình full-rank có độ phức tạp tính toán cao hơn. CUTTLEFISH được truyền cảm hứng từ quan sát rằng các rank ước tính cho tất cả các lớp NN, R, thay đổi nhanh chóng trong giai đoạn huấn luyện sớm nhưng ổn định trong các epoch huấn luyện sau. Do đó, một heuristic hợp lý là chuyển từ huấn luyện full-rank sang huấn luyện rank thấp khi các rank ước tính không còn thay đổi đáng kể. Câu hỏi bây giờ là, làm thế nào chúng ta có thể xác định xem các đường cong của các rank ước tính đã ổn định chưa? CUTTLEFISH theo dõi các chuỗi stable rank cho mỗi lớp tại mỗi epoch, tức là ρₗ = {r₀, r₁, ..., rₜ}. CUTTLEFISH đo lường đạo hàm của các chuỗi rank ước tính cho tất cả trọng số lớp (dρₗ/dt) để phát hiện khi chúng ngừng thay đổi đáng kể, sử dụng điều kiện: |dρₗ/dt| < ε, ∀l ∈ {K+1,...,L-1}, trong đó ε là ngưỡng ổn định rank gần bằng không.

3.5 Xác định K cho kiến trúc hybrid
K cân bằng độ chính xác cuối cùng và tỷ lệ nén mô hình. Tuy nhiên, việc nhận biết mối quan hệ giữa K và độ chính xác cuối cùng mà không huấn luyện đầy đủ mô hình đến hội tụ là thách thức và không thực tế để đạt được tốc độ huấn luyện nhanh hơn. Đối với mỗi tác vụ, CUTTLEFISH thực hiện profiling nhẹ để đo thời gian chạy của NN rank thấp khi phân tách mỗi stack lớp, vì các lớp trong cùng một stack có trọng số và kích thước đầu vào giống hệt nhau, và đánh giá xem nó có dẫn đến tăng tốc đáng kể hay không. CUTTLEFISH chỉ thực hiện phân tách (với các ứng cử viên tỷ lệ rank profiling: α) nếu nó dẫn đến tăng tốc có ý nghĩa (được xác định bởi ngưỡng τ). Ví dụ, nếu thời gian full-rank > 1,5 × thời gian phân tách khi τ = 1,5 và α = 1/4, thì CUTTLEFISH tiến hành phân tách. Hình 4 minh họa một ví dụ benchmark, trong đó việc phân tách stack tích chập đầu tiên (tức là lớp 2 đến lớp 5) không mang lại tăng tốc đáng kể. Do đó, CUTTLEFISH không phân tách những lớp này và trả về K̂ = 6.

[Hình 4 hiển thị thời gian forward trên mỗi lần lặp tính bằng millisecond]

Tại sao không phân tách các lớp ban đầu lại không dẫn đến tăng tốc đáng kể? Lý do cho điều này có thể được quy cho khái niệm arithmetic intensity, được định nghĩa là tỷ lệ FLOPS so với byte dữ liệu phải được truy cập cho một tính toán cụ thể (Jeffers et al., 2016). Khi một lớp nhất định có arithmetic intensity thấp, GPU không thể hoạt động ở hiệu suất đỉnh, và do đó, ngay cả khi FLOPS được giảm đáng kể, tăng tốc thực tế sẽ không đáng kể. Đối với một lớp tích chập, arithmetic intensity của nó tỷ lệ thuận với O(Bmnk²HW/(mnk² + BmHW)) trong đó B,H,W đại diện cho kích thước batch, chiều cao và chiều rộng của hình ảnh đầu vào. Trong các mạng tích chập, nói chung được giả định rằng các lớp ban đầu có mn nhỏ nhưng HW lớn, dẫn đến BmHW ≫ mnk² và O(Bmnk²HW/(mnk² + BmHW)) → O(nk²). Đối với các lớp sau, trong đó H,W nhỏ và mnk² lớn, và do đó mnk² ≫ BmHW, O(Bmnk²HW/(mnk² + BmHW)) → O(BHW). Trong ví dụ được hiển thị trong Hình 4, nk² = 64 × 9 = 576 cho stack lớp đầu tiên, trong khi đối với stack lớp cuối cùng, BHW = 1024 × 8 × 8 = 65.536 ≫ 576. Do đó, các lớp dưới thể hiện arithmetic intensity cao hơn nhiều, và kết quả là, phân tách dẫn đến cải thiện tốc độ đáng kể. Đối với Transformer, trong đó mỗi lớp có kích thước trọng số và đầu vào giống hệt nhau (tức là arithmetic intensity giống nhau), chúng tôi nhất quán phân tách tất cả các lớp Transformer ngoại trừ các lớp embedding chuỗi từ/hình ảnh.

3.6 Kết hợp mọi thứ lại với nhau
Thuật toán chính của CUTTLEFISH được nêu trong Thuật toán 1. CUTTLEFISH bắt đầu với profiling để xác định K̂. Sau đó, phương pháp huấn luyện bắt đầu với huấn luyện full-rank cho đến khi stable rank cho các lớp cần được phân tách hội tụ, tức là tại epoch Ê. Tiếp theo, CUTTLEFISH phân tách mạng full-rank được huấn luyện một phần sử dụng các scaled stable rank đã hội tụ R để thu được mô hình rank thấp phân tách. Cuối cùng, mô hình rank thấp được huấn luyện cho đến khi đạt hội tụ đầy đủ. Trong các thí nghiệm của chúng tôi, chúng tôi đặt ε = 0,1 và τ = 1,5.

[Thuật toán 1 và Thuật toán 2 được trình bày với chi tiết]

--- TRANG 7 ---
4 THÍ NGHIỆM
Chúng tôi đã phát triển một triển khai hiệu quả của CUTTLEFISH và tiến hành các thí nghiệm rộng rãi để đánh giá hiệu suất của nó trên nhiều tác vụ thị giác và xử lý ngôn ngữ tự nhiên. Nghiên cứu của chúng tôi tập trung vào các khía cạnh sau: (i) kích thước của các mô hình phân tách mà CUTTLEFISH khám phá và độ chính xác cuối cùng của chúng; (ii) tăng tốc huấn luyện end-to-end mà CUTTLEFISH đạt được so với huấn luyện full-rank và các phương pháp baseline khác; (iii) cách so sánh ŝ được tìm thấy bởi CUTTLEFISH với những cái được điều chỉnh thủ công và học một cách rõ ràng. Kết quả thí nghiệm toàn diện của chúng tôi chứng minh rằng CUTTLEFISH tự động chọn tất cả các siêu tham số phân tách trong quá trình huấn luyện một cách tức thì, loại bỏ nhu cầu nhiều thử nghiệm thực nghiệm để điều chỉnh siêu tham số phân tách. Cụ thể hơn, các kết quả thí nghiệm tiết lộ rằng CUTTLEFISH tạo ra các mô hình nhỏ hơn đến 5,6 lần so với các mô hình full-rank, và đạt được quá trình huấn luyện end-to-end nhanh hơn đến 1,2 lần trong khi vẫn duy trì độ chính xác tương đương. Hơn nữa, CUTTLEFISH vượt trội so với các phương pháp huấn luyện mô hình rank thấp hiện đại và các baseline nổi bật khác.

4.1 Thiết lập thí nghiệm và chi tiết triển khai
Các tác vụ ML pre-training. Chúng tôi đã tiến hành thí nghiệm trên nhiều tác vụ pre-training thị giác máy tính, bao gồm CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), và ImageNet (ILSVRC2012) (Deng et al., 2009). Đối với CIFAR-10, CIFAR-100, và SVHN (Netzer et al., 2011), chúng tôi huấn luyện VGG-19-BN (gọi là VGG-19) (Simonyan & Zisserman, 2014) và ResNet-18 (He et al., 2016). Trong trường hợp tập dữ liệu SVHN, chúng tôi sử dụng các hình ảnh huấn luyện gốc và loại trừ các hình ảnh bổ sung. Đối với ImageNet, các thí nghiệm của chúng tôi bao gồm ResNet-50, WideResNet-50-2 (gọi là WideResNet-50), DeiT-base, và ResMLP-S36 (He et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al., 2021b;a). Chi tiết thêm về các tác vụ machine learning có thể được tìm thấy trong phụ lục.

Các tác vụ ML fine-tuning. Chúng tôi thí nghiệm fine-tuning BERT BASE trên benchmark GLUE (Wang et al., 2018).

Siêu tham số & lịch trình huấn luyện. Đối với huấn luyện VGG-19 và ResNet-18 trên CIFAR-10 và CIFAR-100, chúng tôi huấn luyện NN trong 300 epoch, trong khi trên SVHN, chúng tôi huấn luyện NN trong 200 epoch. Chúng tôi sử dụng kích thước batch 1.024 cho các tác vụ CIFAR và SVHN để đạt được arithmetic intensity cao. Tốc độ học ban đầu được mở rộng tuyến tính từ 0,1 đến 0,8 trong năm epoch và sau đó được giảm tại các mốc 50% và 75% của tổng số epoch huấn luyện (Goyal et al., 2017). Đối với huấn luyện WideResNet-50 và ResNet-50 trên tập dữ liệu ImageNet, chúng tôi tuân theo cài đặt siêu tham số trong (Goyal et al., 2017), trong đó các mô hình được huấn luyện trong 90 epoch với tốc độ học ban đầu 0,1, được giảm bởi hệ số 0,1 tại epoch 30, 60, và 80 sử dụng kích thước batch 256. Ngoài (Goyal et al., 2017), chúng tôi áp dụng kỹ thuật label smoothing như được mô tả trong (Wang et al., 2021a). Đối với DeiT và ResMLP, chúng tôi huấn luyện chúng từ đầu, tuân thủ lịch trình huấn luyện được đề xuất trong (Touvron et al., 2021b). Đối với benchmark fine-tuning GLUE, chúng tôi tuân theo thiết lập siêu tham số mặc định trong (Devlin et al., 2018; Jiao et al., 2020). Vì CUTTLEFISH tổng quát hóa spectral initialization (SI) và tương thích với Frobenius decay (FD), chúng tôi triển khai FD kết hợp với CUTTLEFISH khi nó góp phần vào độ chính xác tốt hơn. Chi tiết thêm về siêu tham số và lịch trình huấn luyện có thể được tìm thấy trong phụ lục.

Môi trường thí nghiệm. Chúng tôi sử dụng NVIDIA NGC Docker container cho các phụ thuộc phần mềm. Thí nghiệm cho các tác vụ CIFAR, SVHN, và GLUE được tiến hành trên instance EC2 p3.2xlarge (có một GPU V100) sử dụng độ chính xác FP32. Đối với fine-tuning BERT và huấn luyện ImageNet của ResNet-50 và WideResNet-50, các thí nghiệm được thực hiện trên instance EC2 g4dn.metal (được trang bị tám GPU T4) sử dụng độ chính xác FP32. Đối với huấn luyện ImageNet của DeiT và ResMLP, các thí nghiệm được thực hiện trên một instance p4d.24xlarge duy nhất (chứa tám GPU A100) với huấn luyện mixed-precision được kích hoạt.

Các phương pháp baseline. Chúng tôi triển khai CUTTLEFISH và tất cả các baseline được xem xét trong PyTorch (Paszke et al., 2019). Các phương pháp baseline được xem xét là: (i) PUFFERFISH, sử dụng s được điều chỉnh thủ công (Wang et al., 2021a). Để so sánh với PUFFERFISH, chúng tôi sử dụng cùng ResNet-18, VGG-19, ResNet-50, và WideResNet-50 phân tách như được báo cáo trong (Wang et al., 2021a). Đối với các mô hình DeiT và ResMLP, không được khám phá trong bài báo PUFFERFISH gốc, chúng tôi áp dụng cùng heuristic của việc sử dụng tỷ lệ rank toàn cục cố định α = 1/4, điều chỉnh K để phù hợp với kích thước mô hình phân tách được tìm thấy bởi CUTTLEFISH, và đặt E = 80 cho toàn bộ epoch huấn luyện T = 300 (Wang et al., 2021a); (ii) phương pháp huấn luyện phân tách rank thấp với SI và FD được đề xuất bởi (Khodak et al., 2020) (gọi là "SI&FD"), với s của SI&FD được điều chỉnh để phù hợp với kích thước của các mô hình phân tách được tìm thấy bởi CUTTLEFISH; (iii) phương pháp pruning có cấu trúc thời gian huấn luyện, hoặc "early bird ticket" (EB Train) (You et al., 2020); (iv) phương pháp IMP trong đó mỗi vòng pruning tuân theo độ dài huấn luyện và loại bỏ 20% trọng số mô hình còn lại ở mỗi cấp độ, rewinding về epoch thứ 6 (Frankle et al., 2019); (v) Gradient Signal Preservation (GraSP) (Wang et al., 2020a); (vi) phương pháp huấn luyện phân tách rank thấp có thể học, hoặc nén mô hình LC, trong đó rank lớp R được tối ưu hóa một cách rõ ràng cùng với trọng số mô hình W thông qua một quá trình tối ưu hóa xen kẽ (Idelbayev & Carreira-Perpinán, 2020). Đối với fine-tuning GLUE, chúng tôi so sánh CUTTLEFISH với DistillBERT và TinyBERT (Sanh et al., 2019; Jiao et al., 2020); (vii) XNOR-Nets cho phương pháp lượng tử hóa thời gian huấn luyện (Rastegari et al., 2016), dựa trên triển khai PyTorch công cộng có sẵn tại ¹.

CUTTLEFISH với FD. Để triển khai FD, tức là ℓ(·) + λ/2||UV^T||²_F (trong đó ℓ(·) đại diện cho hàm mất mát), người ta phải tính gradient trên số hạng chính quy hóa, tức là ∇_U λ/2||UV^T||²_F = λUV^T V, ∇_V λ/2||UV^T||²_F = λU^T UV^T trong đó người ta có thể thấy có một số hạng chung UV^T, không cần phải tính toán lại. Chúng tôi tối ưu hóa việc triển khai để chỉ tính UV^T một lần. Đối với các kiến trúc NN hybrid, weight decay ℓ2 thông thường được thực hiện trên các lớp full-rank khi FD được kích hoạt cho các lớp phân tách rank thấp.

Các lớp BatchNorm bổ sung. Trong các thí nghiệm trong đó FD không được kích hoạt cho CUTTLEFISH, chúng tôi tích hợp một lớp BatchNorm (BN) bổ sung sau lớp U, tức là BN_V(BN_U(xU)V^T), lấy cảm hứng từ thiết kế kiến trúc mạng của MobileNets (Howard et al., 2017). Chúng tôi cũng áp dụng cách tiếp cận này cho PUFFERFISH.

4.2 Kết quả thí nghiệm và phân tích
Cách so sánh s của CUTTLEFISH với những cái được điều chỉnh thủ công/học? Một câu hỏi quan trọng cần xem xét là sự xuất hiện của s được trả về bởi CUTTLEFISH. Chúng tôi hiển thị R được khám phá bởi CUTTLEFISH, PUFFERFISH, và nén LC cho VGG-19 được huấn luyện trên các tập dữ liệu CIFAR-10, CIFAR-100, và SVHN (kết quả được trình bày trong Hình 5). Ở đây, rõ ràng là CUTTLEFISH cung cấp một lựa chọn R gần gũi với các lựa chọn rank được huấn luyện một cách rõ ràng, tức là nén LC, trong đó lựa chọn rank và trọng số mô hình rank thấp được học cùng nhau trong quá trình huấn luyện mô hình. Điều này chứng minh hiệu quả của heuristic lựa chọn rank được sử dụng bởi CUTTLEFISH.

[Hình 5 hiển thị so sánh về rank đã chọn R]

Giảm tham số và độ chính xác mô hình. Chúng tôi điều tra kỹ lưỡng hiệu quả của CUTTLEFISH và tiến hành so sánh rộng rãi với các baseline, với kết quả được hiển thị trong Bảng 1, 2, 3, và 4. Quan sát chính là CUTTLEFISH thành công giảm số lượng tham số trong khi chỉ gây ra mất mát tối thiểu về độ chính xác. Đáng chú ý, đối với VGG-19 được huấn luyện trên CIFAR-10, CUTTLEFISH xác định một mô hình phân tách nhỏ hơn 10,8 lần so với mô hình VGG-19 full-rank (vanilla), trong khi đạt được độ chính xác validation thậm chí còn tốt hơn. So với PUFFERFISH (được hiển thị trong Bảng 1), CUTTLEFISH khám phá một mô hình phân tách rank thấp nhỏ hơn 4,4 lần với độ chính xác mô hình cuối cùng tương tự cho VGG-19 được huấn luyện trên CIFAR-10. Để đạt được một mô hình phân tách có cùng kích thước, SI&FD không phải lúc nào cũng mang lại độ chính xác tương đương với mô hình full-rank. Ví dụ, đối với CIFAR-10 và CIFAR-100 được huấn luyện trên VGG-19, SI&FD dẫn đến sụt giảm độ chính xác không tầm thường là 1,2% và 1,8%, tương ứng, vì K = 1 luôn được sử dụng trong SI&FD, điều này ảnh hưởng tiêu cực đến độ chính xác mô hình cuối cùng. Trên ImageNet, CUTTLEFISH đạt được ResNet-50 phân tách nhỏ hơn (ít hơn 0,5M tham số) và WideResNet-50 (ít hơn 2,7M tham số) với độ chính xác validation top-1 và top-5 cao hơn so với PUFFERFISH. Đối với DeiT và ResMLP, chúng tôi sử dụng tỷ lệ rank cố định α = 1/4 và điều chỉnh K của PUFFERFISH để phù hợp với kích thước mô hình rank thấp phân tách của CUTTLEFISH để so sánh công bằng. DeiT và ResMLP phân tách PUFFERFISH liên tục dẫn đến độ chính xác mô hình kém hơn so với CUTTLEFISH. Điều này xảy ra vì trọng số mô hình của DeiT và ResMLP ít có khả năng là rank thấp, vì vậy việc sử dụng α = 1/4 theo heuristic PUFFERFISH ban đầu dẫn đến ước tính rank quá tích cực. CUTTLEFISH, ngược lại, phát hiện điều này thông qua heuristic ước tính rank phù hợp hơn.

[Các bảng 1-4 hiển thị kết quả chi tiết cho các mô hình và tập dữ liệu khác nhau]

Thời gian chạy end-to-end và độ phức tạp tính toán. Như đã thảo luận trong Phần 3.5, huấn luyện phân tách rank thấp đạt được tăng tốc đáng kể khi arithmetic intensity cao. Một cách để đạt được arithmetic intensity cao là sử dụng kích thước batch lớn cho huấn luyện. Do đó, chúng tôi sử dụng kích thước batch lớn là 1.024 và đo thời gian huấn luyện end-to-end cho các thí nghiệm trên CIFAR. Kết quả, được trình bày trong Bảng 1, chứng minh rằng CUTTLEFISH liên tục dẫn đến thời gian huấn luyện end-to-end nhanh hơn (bao gồm các epoch full-rank và tất cả các tính toán overhead khác, chẳng hạn như profiling và tính toán stable rank) so với huấn luyện full-rank. Ví dụ, CUTTLEFISH đạt được tăng tốc huấn luyện end-to-end 1,2× trên cả ResNet-18 và VGG-19 được huấn luyện trên CIFAR-10. CUTTLEFISH mang lại thời gian chạy tương đương với PUFFERFISH cho ResNet-18 và thời gian chạy nhanh hơn trên VGG-19 vì nó tìm thấy K nhỏ hơn cho VGG-19, tức là K = 4, trong khi PUFFERFISH sử dụng K = 9. SI&FD đạt được thời gian chạy nhanh hơn CUTTLEFISH do việc sử dụng K = 1 (và nói chung là độ phức tạp tính toán cao hơn cho các lớp tích chập ban đầu). Tuy nhiên, việc sử dụng một giá trị tích cực như vậy cho K chắc chắn dẫn đến mất mát độ chính xác, như đã thảo luận trước đó. Cả nén FC và IMP đều yêu cầu tính toán nặng để đạt được các mô hình nhỏ, chậm hơn đáng kể so với huấn luyện full-rank. XNOR-Nets sử dụng trọng số và kích hoạt mô hình nhị phân, dẫn đến giảm độ chính xác cuối cùng cho các tác vụ so với mạng dày đặc. Lý tưởng nhất, việc sử dụng trọng số và kích hoạt nhị phân sẽ tăng tốc đáng kể việc huấn luyện mô hình và giảm đáng kể mức tiêu thụ bộ nhớ trong quá trình này. Tuy nhiên, PyTorch thiếu triển khai hiệu quả của toán tử tích chập nhị phân. Do đó, các thí nghiệm của chúng tôi sử dụng mạng và kích hoạt FP32 để mô phỏng mạng nhị phân, dẫn đến thời gian chạy chậm hơn đáng chú ý so với huấn luyện FP32 thông thường. Điều này là do đầu ra của mỗi lớp đòi hỏi nhị phân hóa, và trọng số mô hình phải được nhị phân hóa lại cho mỗi lần lặp. Đối với các thí nghiệm ImageNet (được trình bày trong Bảng 2), dấu chân bộ nhớ cao, giới hạn chúng tôi ở kích thước batch 256. CUTTLEFISH xác định các mô hình ResNet-50 và WideResNet-50 phân tách đạt được tăng tố end-to-end 1,2× và 1,3× cho huấn luyện ImageNet, tương ứng. Mặc dù các mô hình phân tách được tìm thấy bởi CUTTLEFISH có thể so sánh với PUFFERFISH, nó loại bỏ nhu cầu điều chỉnh siêu tham số rộng rãi cho các tác vụ quy mô lớn như vậy.

4.3 Chi phí tính toán được giới thiệu bởi CUTTLEFISH.
Chi phí tính toán của profiling. Quá trình profiling trong CUTTLEFISH là một hoạt động nhẹ. Ví dụ, với ResNet-18 trên tập dữ liệu CIFAR-10, chúng tôi thực hiện profiling sử dụng ζ = 11 lần lặp và loại trừ thời gian chạy cho lần lặp đầu tiên cho cả mô hình full-rank và rank thấp (tức là chạy tổng cộng 22 lần lặp). Sau đó chúng tôi tính trung bình các số liệu thời gian chạy cho 10 lần lặp còn lại để benchmarking. Trung bình từ ba lần chạy độc lập, toàn bộ giai đoạn profiling mất 3,98 giây, chỉ chiếm 0,16% tổng thời gian chạy của CUTTLEFISH trên ResNet-18 được huấn luyện trên tập dữ liệu CIFAR-10.

Chi phí tính toán của ước tính rank. Điều quan trọng cần nhấn mạnh là CUTTLEFISH cần tính toán các giá trị kỳ dị của toàn bộ trọng số mạng vào cuối mỗi epoch. Cần lưu ý rằng để tính toán stable rank, chỉ cần các giá trị kỳ dị, chứ không phải các vector kỳ dị. Quá trình này có thể được tăng tốc bằng cách tận dụng các API, chẳng hạn như scipy.linalg.svdvals, chỉ tính toán các giá trị kỳ dị của một ma trận cho trước. Lấy ResNet-18 được huấn luyện trên CIFAR-10 làm ví dụ, thời gian trung bình để ước tính rank sử dụng stable rank là 0,49 giây mỗi epoch. Đối với CUTTLEFISH, yêu cầu E = 82,3 epoch (trung bình) cho huấn luyện full-rank, ước tính stable rank mất tổng cộng 39,97 giây, chiếm 1,6% toàn bộ thời gian chạy end-to-end.

4.4 Nghiên cứu Ablation
Độ chính xác và hiệu quả thời gian chạy của các BN bổ sung. Chúng tôi tiến hành các nghiên cứu ablation bổ sung để kiểm tra ảnh hưởng của việc tích hợp các lớp BN bổ sung đối với hiệu suất CUTTLEFISH. Trong các thí nghiệm chính của chúng tôi, chúng tôi sử dụng FD và vô hiệu hóa các lớp BN bổ sung để đảm bảo tính toán gradient FD chính xác khi FD dẫn đến độ chính xác tốt hơn. Các nghiên cứu ablation của chúng tôi bao gồm huấn luyện ResNet-18 và VGG-19 trên CIFAR-10 và CIFAR-100 cũng như ResNet-50 trên ImageNet, và đánh giá kích thước mô hình, độ chính xác validation tốt nhất (top-1 cho ImageNet), thời gian huấn luyện end-to-end, và thời gian trên mỗi lần lặp trên các mô hình rank thấp. Các siêu tham số được sử dụng trong các nghiên cứu ablation nhất quán với những cái được sử dụng cho các kết quả chính trong phần Thí nghiệm. Kết quả nghiên cứu ablation, được hiển thị trong Bảng 5, tiết lộ rằng việc thêm các lớp BN bổ sung nói chung dẫn đến kích thước mô hình lớn hơn một chút và thời gian chạy end-to-end và trên mỗi lần lặp chậm hơn. Ví dụ, khi huấn luyện ResNet-18 trên CIFAR-10 mà không có các BN bổ sung, thời gian huấn luyện end-to-end nhanh hơn 1,4%, và thời gian chạy trên mỗi lần lặp nhanh hơn 2,8%. Tác động của các BN bổ sung đối với độ chính xác validation cuối cùng khác nhau giữa các thí nghiệm: việc kích hoạt các BN bổ sung cải thiện độ chính xác một chút cho ResNet-18 và VGG-19 trên CIFAR-10, nhưng không cho CIFAR-100. Tuy nhiên, đối với thí nghiệm ImageNet, việc thêm các BN bổ sung dẫn đến tăng độ chính xác mô hình không tầm thường trung bình 0,21% qua ba lần chạy độc lập với các seed ngẫu nhiên khác nhau. Cải thiện này có ý nghĩa, xem xét nó liên quan đến độ chính xác top-1 cho một vấn đề phân loại 1000 lớp. Có hai giải thích tiềm năng tại sao các BN bổ sung giúp cải thiện độ chính xác cho các thí nghiệm ImageNet nhiều hơn so với các thí nghiệm CIFAR: 1) Khả năng mô hình của ResNet-18 và VGG-19 có vẻ đủ lớn cho các tập dữ liệu CIFAR, cho phép tỷ lệ nén cao (ví dụ, ∼5-10×). Ngược lại, đối với ResNet-50 trên ImageNet, khả năng mô hình có vẻ không đủ. CUTTLEFISH, trong trường hợp này, không đạt được tỷ lệ nén cực kỳ cao (tức là ít hơn 2×). Do đó, việc bao gồm các BN bổ sung có vẻ cung cấp cho mô hình phân tách rank thấp khả năng bổ sung để nâng cao độ chính xác của nó. 2) Đối với các thí nghiệm CIFAR, chúng tôi sử dụng kích thước batch 1024, bị hạn chế bởi bộ nhớ GPU, trong khi kích thước batch 256 được sử dụng cho các thí nghiệm ImageNet. Có thể các BN bổ sung mang lại lợi ích đáng kể hơn trong các cài đặt batch nhỏ hơn. Lưu ý rằng đối với các thí nghiệm dựa trên mô hình Transformer, chúng tôi không kích hoạt các BN bổ sung vì LayerNorm thường được sử dụng thay vì BN, điều này nằm ngoài phạm vi của nghiên cứu ablation này.

[Hình 6 hiển thị nghiên cứu ablation về chi phí theo lớp]

Hiệu quả của phân tách rank thấp trên các loại lớp khác nhau. Để so sánh hiệu quả của phân tách rank thấp với các lớp tích chập, FC, MLP, và multi-head attention, chúng tôi tiến hành một nghiên cứu ablation sử dụng ResNet-50 và DeiT-small trên tập dữ liệu ImageNet. Thời gian trên mỗi lần lặp của mỗi lớp được đo và kết quả được mô tả trong Hình 6 (đối với ResNet-50, chúng tôi cũng minh họa các tỷ lệ scaled stable rank được chọn bởi CUTTLEFISH). Chúng tôi tập trung vào thời gian tính toán forward cho nghiên cứu này, vì đã biết rằng có một hệ số không đổi giữa thời gian tính toán forward và backward, và cái trước đóng vai trò như một proxy tốt cho thời gian trên mỗi lần lặp. Do hạn chế về không gian, chúng tôi chỉ vẽ kết quả cho lớp tích chập thứ 21 trở đi trong các thí nghiệm ResNet-50, mặc dù tăng tốc có ý nghĩa đã được quan sát cho 20 lớp đầu tiên cũng như vậy. Trong trường hợp các lớp tích chập, các thí nghiệm của chúng tôi tiết lộ tăng tốc trung bình 2,1× qua tất cả 49 lớp khi sử dụng tỷ lệ rank 1/4. Tuy nhiên, chúng tôi quan sát thấy rằng lớp FC cuối cùng thực sự chậm lại khi được phân tách, bất kể tỷ lệ rank được sử dụng. Điều này có thể được quy cho kích thước nhỏ của lớp FC, gây ra chi phí khởi động kernel lớn khi được chia thành hai lớp nhỏ hơn, do đó vô hiệu hóa bất kỳ tiết kiệm chi phí tính toán nào. Các thí nghiệm của chúng tôi với DeiT cho thấy rằng việc phân tách cả lớp multi-head attention và MLP dẫn đến tăng tốc đáng kể cho tất cả 12 khối mã hóa Transformer. Ngoài ra, việc phân tách lớp MLP dẫn đến lợi ích tăng tốc lớn hơn so với việc phân tách lớp multi-head attention. Cụ thể, việc phân tách lớp multi-head attention dẫn đến tăng tốc 1,26× trung bình, trong khi việc phân tách lớp MLP dẫn đến tăng tốc 1,73× trung bình cho tất cả 12 khối ở tỷ lệ rank 1/4.

4.5 Hạn chế của CUTTLEFISH
Một hạn chế của CUTTLEFISH là các siêu tham số s mà nó điều chỉnh bị ảnh hưởng bởi tính ngẫu nhiên của thuật toán huấn luyện và khởi tạo mô hình. Do đó, các lần chạy thử nghiệm khác nhau có thể không tạo ra các mô hình phân tách có kích thước giống hệt nhau (mặc dù phương sai là tối thiểu). Điều này có thể ảnh hưởng đến khả năng tái tạo chính xác.

5 KẾT LUẬN
Chúng tôi trình bày CUTTLEFISH, một phương pháp huấn luyện rank thấp tự động loại bỏ nhu cầu điều chỉnh các siêu tham số phân tách bổ sung, tức là S = (E,K,R). CUTTLEFISH tận dụng hai hiểu biết quan trọng liên quan đến sự xuất hiện của stable rank trong quá trình huấn luyện và lợi ích tăng tốc thực tế đạt được bằng cách phân tách các lớp NN khác nhau. Sử dụng những hiểu biết này, nó thiết kế các heuristic để tự động chọn s ∈ S. Các thí nghiệm rộng rãi của chúng tôi chứng minh rằng CUTTLEFISH xác định các mô hình rank thấp không chỉ nhỏ hơn, mà còn mang lại độ chính xác cuối cùng tốt hơn trong hầu hết các trường hợp khi so sánh với các phương pháp huấn luyện rank thấp hiện đại.

Lời cảm ơn
Chúng tôi cảm ơn người hướng dẫn của chúng tôi, Bilge Acun, và các nhà đánh giá MLSys ẩn danh vì những hiểu biết và khuyến nghị có giá trị của họ, đã nâng cao công trình của chúng tôi. Nghiên cứu này đã được tài trợ một cách hào phóng bởi ONR Grant No. N00014-21-1-2806, ba Sony Faculty Innovation Awards, NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NSF IIS1955532, NSF CNS2008248, NSF IIS2123952, và NSF BCS2040381, và NGA HM04762010002.

--- TRANG 12 ---
[Phần tài liệu tham khảo và phụ lục tiếp tục với danh sách các tài liệu tham khảo và chi tiết bổ sung về artifact, thiết lập thí nghiệm, v.v.]
