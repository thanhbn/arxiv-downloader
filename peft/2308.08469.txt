# 2308.08469.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2308.08469.pdf
# File size: 1324106 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint Notice
This paper has been accepted for publication in ACM Transactions on Intelligent Systems
and Technology (TIST) 2025. The final version will be available at https://doi.org/
10.1145/3719207 .
1arXiv:2308.08469v6  [cs.LG]  20 Feb 2025

--- PAGE 2 ---
LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient
Time-Series Forecasters
Ching Chang1, Wei-Yao Wang1, Wen-Chih Peng1, Tien-Fu Chen1
1National Yang Ming Chiao Tung University, Hsinchu, Taiwan
blacksnail789521.cs10@nycu.edu.tw, sf1638.cs05@nctu.edu.tw,
wcpeng@cs.nycu.edu.tw, tfchen@cs.nycu.edu.tw
Abstract
Multivariate time-series forecasting is vital in various domains, e.g., economic planning
and weather prediction. Deep train-from-scratch models have exhibited effective perfor-
mance yet require large amounts of data, which limits real-world applicability. Recently,
researchers have leveraged the representation learning transferability of pre-trained Large
Language Models (LLMs) to handle limited non-linguistic datasets effectively. However,
incorporating LLMs with time-series data presents challenges of limited adaptation due to
different compositions between time-series and linguistic data, and the inability to process
multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a frame-
work for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage
fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of
time-series data, and the forecasting fine-tuning stage for downstream time-series forecast-
ing tasks. Furthermore, our framework features a novel two-level aggregation method that
integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to
interpret time-specific information. In experiments across 7 time-series forecasting datasets,
LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch
models in full-shot scenarios, and also achieves the highest rank in few-shot scenarios. In ad-
dition, evaluations compared with different unsupervised representation learning approaches
highlight LLM4TS’s effectiveness with representation learning in forecasting tasks. Abla-
tion studies further validate each component’s contribution to LLM4TS and underscore the
essential role of utilizing LLM’s pre-trained weights for optimal performance. The code is
available at https://github.com/blacksnail789521/LLM4TS .
1 Introduction
Forecasting is a vital task in multivariate time-series analysis, not only for its ability to operate
without manual labeling but also for its importance in practical applications such as economic
planning [21, 33, 6] and weather prediction [51, 23, 14]. Recently, numerous deep train-from-
scratch models have been developed for time-series forecasting [46, 28, 40, 52, 51, 44], although
some lean towards unsupervised representation learning [42, 45, 34, 8, 50, 5, 27] and transfer
learning [47, 53, 11, 36, 12]. Generally, these approaches aim to employ adept representation
learners: first extracting rich representations from the time-series data and then using these
representations for forecasting.
Achieving an adept representation learner requires sufficient training data [15, 48, 37], yet
in real-world scenarios, there is often a lack of large-scale time-series datasets. For instance, in
industrial manufacturing, the sensor data for different products cannot be combined for further
analysis, leading to limited data for each product type [43, 7, 1]. Recent research has pivoted
towards pre-trained LLMs in Natural Language Processing (NLP) [29, 3, 35], exploiting their
robust representation learning and few-shot learning capabilities. Moreover, these LLMs can
adapt to non-linguistic datasets (e.g., images [26, 24], audio [9, 30], tabular data [13, 31], and
2

--- PAGE 3 ---
time-series data [53, 17]) by fine-tuning with only a few parameters and limited data. While
LLMs are renowned for their exceptional transfer learning capabilities across various fields, the
domain-specific nuances of time-series data introduce two challenges in leveraging these models
for time-series forecasting.
The first challenge of employing LLMs for time-series forecasting is their limited adaptation
to the unique characteristics of time-series data due to LLMs’ initial pre-training focus on the
linguistic corpus. While LLMs have been both practically and theoretically proven [53] to
be effective in transfer learning across various modalities thanks to their data-independent self-
attention mechanism, their primary focus on general text during pre-training causes a shortfall in
recognizing key time-series patterns and nuances crucial for accurate forecasting. This limitation
is evident in areas such as meteorology and electricity forecasting [51], where failing to account
for weather patterns and energy consumption trends leads to inaccurate predictions.
The second challenge lies in the limited capacity to process multi-scale temporal information.
While LLMs are adept at understanding the sequence and context of words, they struggle to
understand temporal information due to the lack of utilizing multi-scale time-related data such
as time units (e.g., seconds, minutes, hours, etc.) and specific dates (e.g., holidays, significant
events). This temporal information is vital in time-series analysis for identifying and predicting
patterns [40, 39]; for instance, in energy management, it is used to address consumption spikes
during daytime and in summer/winter, in contrast to the lower demand during the night and in
milder seasons [51]. This underscores the importance of models adept at interpreting multi-scale
temporal patterns (hourly to seasonal) for precise energy demand forecasting. However, most
LLMs (e.g., [29, 35]) built on top of the Transformer architecture do not naturally incorporate
multi-scale temporal information, leading to models that fail to capture crucial variations across
different time scales.
To address the above issues, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. Regarding the first challenge, our framework introduces a two-stage
fine-tuning approach: the time-series alignment stage and the forecasting fine-tuning stage. The
first stage focuses on aligning the LLMs with the characteristics of time-series data by utilizing
the autoregressive objective, enabling the fine-tuned LLMs to adapt to time-series representa-
tions. The second stage is incorporated to learn corresponding time-series forecasting tasks. In
this manner, our model supports effective performance in full- and few-shot scenarios. Notably,
throughout both stages, most parameters in the pre-trained LLMs are frozen, thus preserving
the model’s inherent representation learning capability. To overcome the limitation of LLMs in
integrating multi-scale temporal information, we introduce a novel two-level aggregation strat-
egy. This approach embeds multi-scale temporal information into the patched time-series data,
ensuring that each patch not only represents the series values but also encapsulates the critical
time-specific context. Consequently, LLM4TS emerges as a data-efficient time-series forecaster,
demonstrating robust few-shot performance across various datasets (Fig. 1).
In summary, the paper’s main contributions are as follows:
•Aligning LLMs Toward Time-Series Data: To the best of our knowledge, LLM4TS
is the first method that aligns pre-trained Large Language Models with time-series char-
acteristics, effectively utilizing existing representation learning and few-shot learning ca-
pabilities.
•Multi-Scale Temporal Information in LLMs: To adapt to time-specific information,
a two-level aggregation method is proposed to integrate multi-scale temporal data within
pre-trained LLMs.
•Robust Performance in Forecasting: LLM4TS excels in 7 real-world time-series fore-
casting benchmarks, outperforming state-of-the-art methods, including those trained from
scratch. It also demonstrates strong few-shot capabilities, particularly with only 5% of
3

--- PAGE 4 ---
(a) 5% training data (b) 10% training data
Figure 1: Model performance comparison on few-shot forecasting.
data, where it surpasses the best baseline that uses 10% of data. This efficiency makes
LLM4TS highly relevant for practical, real-world forecasting applications.
2 Related Work
2.1 Transfer Learning Across Various Modalities with LLMs
LLMs have demonstrated their effectiveness in transfer learning across a variety of modalities,
such as images [26, 24], audio [9, 30], tabular data [13, 31], and time-series data [53, 17]. A
key motivation for employing LLMs in various modalities is their ability to achieve notable
performance with limited data [53]. To preserve their data-independent representation learn-
ing capability, most parameters in these LLMs are kept fixed. Empirical evidence [26, 53]
indicates that LLMs keeping most parameters unchanged often outperform those trained from
scratch, underscoring the value of maintaining these models’ pre-existing representation learn-
ing strengths (more experiments can be found in Section 5.4.3). Theoretically, it is shown
that the self-attention modules in these pre-trained transformers develop the capacity for data-
independent operations (akin to principal component analysis [53]), enabling them to function
effectively as universal compute engines [26] or general computation calculators [10]. In the
time-series domain, GPT4TS [53] utilizes the pre-trained GPT-2 and demonstrates strong per-
formance in time-series forecasting under few-shot conditions without modifying most param-
eters. Time-LLM [18] reprograms LLMs for time series forecasting by converting time series
into text prototypes and using prompts to guide prediction. It outperforms specialized models,
particularly in few-shot and zero-shot settings. TEMPO [4] adapts GPT-like models for time
series forecasting by decomposing trends and using prompts for better distribution adaptation,
excelling in zero-shot scenarios. TEST [32] aligns time series with LLM embeddings through
tokenization and contrastive learning, enabling effective time series forecasting using pre-trained
LLMs without fine-tuning. With our LLM4TS, we address the challenges of limited adaptation
to time-series characteristics and the difficulty in processing multi-scale temporal information,
thereby enhancing performance in time-series forecasting.
4

--- PAGE 5 ---
Figure 2: Problem formulation for multivariate time-series forecasting .
2.2 Long-term Time-Series Forecasting
Numerous efforts have been dedicated to employing Transformer models for long-term time-
series forecasting [51, 40, 52, 28, 49, 22]. While Transformer-based models have gained traction,
DLinear [46] reveals that a single-layer linear model can surpass many of these sophisticated
Transformer-based approaches. These deep train-from-scratch models exhibit outstanding per-
formance when trained on sufficient datasets, but their efficacy decreases in limited-data sce-
narios. In contrast, LLM4TS sets new benchmarks alongside these state-of-the-art approaches
in both full- and few-shot scenarios.
2.3 Time-Series Representation Learning
In the time-series domain, self-supervised learning emerges as a prominent approach to represen-
tation learning. While Transformers are widely recognized as prime candidates for end-to-end
time-series analysis [41, 25, 28, 38, 2], CNN-based [45] or RNN-based [34] backbones consis-
tently stand out as the preferred architecture in time-series self-supervised learning. However,
the inherent capability of Transformers to model long-range dependencies and capture patterns
aligns perfectly with time-series data, which involve complex sequential relationships. Since
thetime-series alignment stage in LLM4TS can be seen as a self-supervised learning approach,
we evaluate LLM4TS’s representation learning capability and demonstrate the full potential of
Transformers in unsupervised representation learning, surpassing the performance of conven-
tional CNN and RNN-based models.
3 Problem Formulation
Given a complete and evenly-sampled multivariate time-series, we use a sliding data window
to extract sequential samples, as illustrated in Fig. 2. This window moves with a stride
of 1 and has a total length of Tin+Tout— comprising past data xin= (d1, . . . , d Tin) with a
look-back window length Tinand future data xout= (dTin+1, . . . , d Tin+Tout) with a prediction
length Tout. For each time step t,dtrepresents a C-dimensional vector, where Cdenotes the
number of features. Our objective is to use the past data xin∈RTin×Cto predict the future
dataxout∈RTout×C.
4 The Proposed LLM4TS
Fig. 3 illustrates our LLM4TS framework, leveraging the pre-trained GPT-2 [29] as the back-
bone model. We first introduce the time-series alignment stage, which focuses on aligning the
LLMs with the characteristics of time-series data using an autoregressive objective (Section
5

--- PAGE 6 ---
(a) Time-Series Alignment (b) Forecasting Fine-Tuning
Figure 3: LLM4TS framework. The numbers in the patched time-series (e.g., 1, 2, ..., 16
in the first patch) indicate the sequential order of the timestamps. The framework consists
of two stages: (a) Time-series alignment, which uses the autoregressive approach to align the
pre-trained LLM with patched time-series data. (b) Forecasting fine-tuning, which starts with
linear probing (i.e., only the output layer is unfrozen), followed by full fine-tuning (all the layers
and PEFT components in the LLM are unfrozen).
4.1). Subsequently, the forecasting fine-tuning stage is designed to further enhance the model’s
ability to handle time-series forecasting tasks (Section 4.2).
4.1 Time-Series Alignment
Existing LLMs are pre-trained on a general language corpus, which means they fail to learn
contextualized information outside linguistic domains; therefore, the time-series alignment stage
is proposed to align LLMs with the characteristics of time-series data. Given our selection of
GPT-2 [29] as the backbone model, which is a causal language model, we ensure that this stage
adopts the same autoregressive training methodology used during its pre-training phase. Fig.
3(a) illustrates the autoregressive objective in the time-series alignment stage: given an input
sequence of patched time-series data (e.g., 1stpatch, 2ndpatch, 3rdpatch, etc.), the backbone
model generates an output sequence shifted one patch to the right (e.g., 2ndpatch, 3rdpatch,
4thpatch, etc.).
Instance Normalization Data normalization is essential for stable performance when adapt-
ing pre-trained models across various modalities. Alongside the layer normalization used in the
pre-trained LLM, we incorporate instance normalization to improve consistency and reliabil-
ity in handling diverse time-series datasets. In our model, instance normalization is employed
without incorporating a trainable affine transformation. This is crucial because when a batch
of data is gathered and instance normalization is applied with a trainable affine transformation,
the resulting transformed data becomes unsuitable to be the ground truth for the output. Given
that an autoregressive objective is used at this stage, applying a trainable affine transformation
is not feasible.
Given an input time-series sample xin∈RTin×C, we apply instance normalization (IN) to
produce a normalized time-series sample xnormed ∈RTin×Cwith zero mean and unit standard
deviation:
xnormed = IN( xin). (1)
6

--- PAGE 7 ---
Figure 4: Multi-scale temporal encoding for patched time-series data . This process
involves a two-level aggregation. Here, only the first patch is shown for simplicity; in practice,
all patches in a batch are processed simultaneously. Level 1 aggregation calculates the temporal
embedding for each time unit and sums them together. Next, Level 2 aggregation applies a
pooling method to extract the final temporal embedding.
Time-Series Tokenization The context window sizes in pre-trained LLMs (e.g., 1024 in
GPT-2) are sufficient for NLP tasks but are inadequate for long-term time-series forecasting.
In our experiments, a prediction length of 720 combined with a look-back window size of 512
easily exceeds these limits. To address this, we adopt channel-independence along with patch-
ing[28] for time-series tokenization, effectively resolving the context window size constraint
and simultaneously reducing the time and space complexity of the Transformer quadratically.
Channel-independence converts multivariate time-series data into multiple univariate time-series
data, thus transforming the data’s dimension to RTin×1, with the channel dimension Cmerged
into the batch size dimension. The subsequent patching step groups adjacent time steps into a
singular patch-based token, reducing the input sample’s time dimension from TintoTp, where
Tpdenotes the number of patches, and concurrently expanding the feature dimension from 1 to
P, with Prepresenting the patch length.
Given a normalized time-series sample xnormed ∈RTin×C, we first apply channel-independence
(CI), and then patching to produce a series of patches p∈RTp×P:
p= patching(CI( xnormed )). (2)
Three Encodings for Patched Time-Series Data Given our goal to adapt a pre-trained
LLM for time-series data, the original token encoding layer (designed for text) becomes unsuit-
able due to the mismatched modalities. Additionally, we design a new multi-scale temporal
encoding layer to address the inability to process multi-scale temporal information.
Given a series of tokens, applying token encoding is necessary to align their dimensions with
the latent embedding dimension of the pre-trained LLM. In standard NLP practices, this encod-
ing uses a trainable lookup table to map tokens into a high-dimensional space. However, this
method only suits scalar tokens, whereas our patched time-series data are vectors . Therefore,
we drop the original token encoding layer in the LLM, and employ a one-dimensional convo-
lutional layer Conv token as our new token encoding layer. As opposed to employing a linear
layer [53], we choose a convolutional layer due to its superior ability to retain local semantic
information within the time-series data. This results in the generation of the token embedding
etoken∈RTp×D, where Ddenotes the dimension of the embeddings:
etoken = Conv token(p). (3)
7

--- PAGE 8 ---
For the positional encoding layer, we employ a trainable lookup table Eposto map patch
locations. This results in the generation of the positional embedding epos∈RTp×D:
epos=Epos(i), (4)
where i∈RTprepresents the indices of the patch locations.
To address the challenge LLMs face in processing multi-scale temporal information, we
introduce a multi-scale temporal encoding layer. When processing time-related data, we face
two challenges due to the need to aggregate multiple pieces of information into one unified
representation (Fig. 4):
1. Each timestamp includes a range of multi-scale temporal attributes (e.g., seconds, minutes,
hours, holidays, etc.).
2. Each patch encompasses multiple timestamps.
To address the first challenge associated with diverse temporal attributes within a timestamp,
we employ Level 1 aggregation: a trainable lookup table for each temporal attribute (e.g.,
Esec, Emin, ...), mapping it into a high-dimensional space, and then summing them to produce a
singular temporal embedding. In response to the second challenge of multiple timestamps within
a patch, we use Level 2 aggregation: a pooling method to extract the final temporal embedding.
For the pooling method, we opt for the “select first” method, where the initial timestamp is
designated as representative of the entire patch. This is because the first timestamp often
carries the most significant and representative information for the entire duration covered by
the patch, especially in time-series data where earlier events can have a substantial influence on
the subsequent sequence. This process generates the final temporal embedding etemp∈RTp×D:
etemp= Pooling
X
a∈{sec,min,hour,...}Ea(ta)
, (5)
where arepresents different temporal attributes (seconds, minutes, hours, holidays, etc.), Ea
denotes the trainable lookup table for each temporal attribute, ta∈RTp×Pare the series of
patches containing temporal information for that temporal attribute, and Pooling applies the
pooling method to the aggregated embeddings.
Finally, the token, positional, and temporal embeddings are summed to yield the final
embedding e∈RTp×D, which is then fed into the pre-trained Transformer blocks:
e=etoken +epos+etemp. (6)
Pre-Trained LLM To preserve LLMs’ data-independent representation learning capability,
most parameters in these LLMs are kept fixed. Empirical evidence [26, 53] shows that training
these LLMs from scratch often hurts performance, highlighting the importance of fixing most
parameters to retain the LLM’s representation learning capability. To that end, we opt for
freezing most parameters, particularly those associated with the multi-head attention and feed-
forward layers in the Transformer block, as they are the most responsible for representation
learning [53].
For the remaining trainable parameters in the pre-trained LLM, we employ two Parameter-
Efficient Fine-Tuning (PEFT) methods to selectively adjust or introduce a limited set of train-
able parameters. Specifically, we utilize Layer Normalization Tuning [26] to adjust pre-existing
parameters in Transformer blocks, making the affine transformation in layer normalization train-
able. Concurrently, we employ Low-Rank Adaptation (LoRA) [16], which introduces trainable
low-rank matrices that are applied to the query (Q) and key (K) matrices in the self-attention
8

--- PAGE 9 ---
mechanism. With these two PEFT techniques, only 1 .5% of the pre-trained LLM’s total pa-
rameters are used to be trained.
Given the embedding e(which is adjusted to the required embedding dimension Dby three
encoding layers), we pass it into the pre-trained LLM, which comprises a series of pre-trained
Transformer blocks (TBs) (with Lblocks in total). This process yields the final embeddings
z∈RTp×D:
z= TBs( e). (7)
After being processed by the pre-trained LLM, we employ a linear output layer Wtsa∈RP×D
to transform the output embedding back to patched time-series data:
ˆpshifted =zW⊤
tsa, (8)
where ˆpshifted ∈RTp×Prepresents our prediction target, corresponding to the original time-
series patches ( p) shifted one patch to the right, in line with the autoregressive objective of
this stage. To ensure the prediction precisely reconstructs the actual shifted patched data
pshifted ∈RTp×P, we use the Mean Squared Error (MSE) as the loss function:
Ltsa= MSE( pshifted ,ˆpshifted ). (9)
4.2 Forecasting Fine-tuning
After aligning the pre-trained LLM with patched time-series data in the time-series alignment
stage, we transfer the trained weights of the backbone model, including those from the encoding
layers, to the forecasting fine-tuning stage. When fine-tuning the backbone model for the
forecasting task, two primary training strategies are available: full fine-tuning (where all model
parameters are updated) and linear probing (where only the final linear output layer is updated).
Studies have shown that a sequential approach—initial linear probing followed by full fine-tuning
(LP-FT, as illustrated in Fig. 3(b))—consistently surpasses strategies exclusively employing
either method [20]. The superiority of LP-FT is due to its dual-phase approach: first finding
an optimized output layer to minimize later adjustments in fine-tuning (preserving feature
extractor efficacy for out-of-distribution (OOD) scenarios), and then employing full fine-tuning
to adapt the model to the specific task (enhancing in-distribution (ID) accuracy) [20].
For the model architecture in the forecasting fine-tuning stage, we preserve most of the
structure as in the time-series alignment stage, including the three encoding layers and the
pre-trained LLM. However, there are two architectural differences in this stage: instance nor-
malization and the output layer.
The first architectural difference is in the instance normalization, where we adopt Reversible
Instance Normalization (RevIN) [19] to enhance forecasting accuracy. RevIN involves batch-
specific instance normalization and subsequent denormalization, both sharing the same trainable
affine transformation. The additional denormalization step addresses distribution shifts between
training and testing data, which is a common challenge in the time-series domain (e.g., seasonal
changes). Therefore, during the time-series tokenization step, we apply RevIN’s normalization,
succeeded by channel-independence and patching:
p= patching(CI(RevIN norm(xin))). (10)
Notably, the denormalization step is applicable only to unpatched time-series data; hence, in
the time-series alignment stage, standard instance normalization is employed.
The second architectural difference lies in the output layer, whose function is to transform
the final embedding zinto the predicted future data, presented in the general (unpatched) time-
series format. This involves flattening the data and passing it through the linear output layer
9

--- PAGE 10 ---
Table 1: Statistical overview of the 7 datasets for long-term time-series forecasting.
Datasets Features Timesteps Granularity
Weather 21 52,696 10 min
Traffic 862 17,544 1 hour
Electricity 321 26,304 1 hour
ETTh1 & ETTh2 7 17,420 1 hour
ETTm1 & ETTm2 7 69,680 5 min
Wfft∈RTout×Tp·D, followed by rearrangement, and then applying RevIN’s denormalization to
obtain the final prediction ˆxout∈RTout×C:
ˆxout= RevIN denorm (Rearrange((Flatten( z))W⊤
fft). (11)
To ensure that this prediction accurately reconstructs the future data xout∈RTout×C, we use
MSE as the loss function:
Lfft= MSE( xout,ˆxout). (12)
5 Experiments
Datasets In our long-term forecasting analysis, we experiment on 7 real-world, publicly acces-
sible benchmark datasets. We present detailed statistics for these datasets in Table 1, including
the number of features, the total length of the datasets, and their sampling frequency.
Weather1comprises local climatological data spanning 4 years for approximately 1,600
locations in the U.S. It includes 11 weather variables in each record, in addition to the target
variable ’wet bulb.’ Traffic2consists of hourly observations from the California Department of
Transportation, detailing road occupancy rates captured by various sensors located on freeways
in the San Francisco Bay area. Electricity3comprises hourly power usage data for 321 cus-
tomers, spanning from 2012 to 2014, with ’MT 320’ set as the target variable. ETT [51] focuses
on long-duration electric power deployment data. The collection includes two datasets sampled
hourly (ETTh1, ETTh2) and two datasets sampled every 15 minutes (ETTm1, ETTm2), cov-
ering a period of over two years from various provinces in China. Each dataset in the ETT
series features one oil temperature variable alongside six power load variables.
Evaluation Metrics In time-series forecasting, Mean Squared Error (MSE) and Mean Ab-
solute Error (MAE) are commonly utilized metrics for evaluating performance. The Mean
Squared Error (MSE) can be expressed as:
MSE =1
NNX
n=1(xout−ˆxout)2. (13)
Here, xoutindicates the actual future data that corresponds to the past data xin, while ˆ xout
represents the predicted future data based on the input past data. Nis the total number of
samples. The Mean Absolute Error (MAE) is given by:
MAE =1
NNX
n=1|xout−ˆxout|. (14)
1https://www.ncei.noaa.gov/data/local-climatological-data/
2http://pems.dot.ca.gov/
3https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014
10

--- PAGE 11 ---
Baselines For long-term time-series forecasting, we focus on a range of state-of-the-art mod-
els. The same set of models is used for few-shot learning and ablation studies. GPT4TS
[53] employs patching and channel independence to initially transform time-series data into to-
kens. It then utilizes a pre-trained Large Language Model (GPT-2), maintaining the pre-trained
weights in the self-attention and feedforward layers of the residual blocks from the pre-trained
LLM. DLinear [46] challenges the prevailing use of Transformer-based models for long-term
time-series forecasting. It introduces a simple one-layer linear model that surprisingly outper-
forms complex Transformer-based models on several real datasets. PatchTST [28] introduces
a Transformer-based approach for time-series forecasting, focusing on efficiency by employing
patching and channel-independence to transform time-series data into patches. FEDformer
[52] combines Transformers with seasonal-trend decomposition and frequency enhancement for
efficient and effective long-term series forecasting, overcoming traditional Transformer limita-
tions by capturing both global trends and detailed structures with linear complexity. Time-
LLM [18] reprograms LLMs for time series forecasting by converting time series into text
prototypes and using prompts to guide prediction. It outperforms specialized models, par-
ticularly in few-shot and zero-shot settings. TEMPO [4] adapts GPT-like models for time
series forecasting by decomposing trends and using prompts for better distribution adaptation,
excelling in zero-shot scenarios. TEST [32] aligns time series with LLM embeddings through
tokenization and contrastive learning, enabling effective time series forecasting using pre-trained
LLMs without fine-tuning.
For unsupervised representation learning in time-series analysis, we explore advanced models
that excel in extracting meaningful representations without relying on labeled data. PatchTST
[28] in this context is a variant distinct from the one used in forecasting experiments, with an
emphasis on representation learning. It adopts an MLM (Masked Language Model) strategy,
similar to BERT, to learn representations. BTSF [42] introduces a Bilinear Temporal-Spectral
Fusion framework to improve time-series representation learning by integrating temporal and
spectral information. This approach minimizes sampling bias and optimizes feature represen-
tation through instance-level augmentation and fusion techniques. TS2Vec [45] is the first
universal framework dedicated to learning representations of time-series data. It emphasizes
distinguishing multi-scale contextual information on both the instance and timestamp levels,
demonstrating effectiveness in various time-series tasks. TNC [34] utilizes the Augmented
Dickey-Fuller test to detect temporal neighborhoods and implements Positive-Unlabeled learn-
ing to address sampling bias. TS-TCC [8] produces two different views via strong and weak
augmentations and enhances representations through contrastive learning, focusing on the tem-
poral and contextual differences between these views.
Implementation Details For our experiments in long-term time-series forecasting, few-shot
learning, and ablation studies, we utilize the settings from PatchTST [28] for a consistent
comparison. We first explore the model’s performance in few-shot scenarios, followed by a
comprehensive evaluation under full-shot settings to ensure a thorough and balanced analysis.
We set our look-back window length Tinto either 336 or 512 (reporting the best results), and
configure the patch length Pas 16 with a stride Sof 8. For unsupervised representation learning,
the settings are slightly adjusted to Tin= 512, P= 12, and S= 12. Aligned with the GPT4TS
configuration [53], we utilize only the first 6 layers of the 12-layer GPT-2 base [29].
5.1 Few-Shot Learning in Long-Term Time-Series Forecasting
Table 2 shows the results of long-term time-series forecasting using only 5% of the training data,
while Table 3 presents similar results but with merely 10% of the training data utilized. In our
experiments, consistent splits for training, validation, and test sets are maintained across both
full and few-shot learning scenarios. We deliberately limited the training data percentage to 5%
and 10% to evaluate model performance in few-shot scenarios. For each dataset, we train a single
11

--- PAGE 12 ---
Table 2: Few-shot long-term forecasting using 5%of the training data. For most
datasets, results are reported over prediction lengths Tout∈ {96,192,336,720}. However, for
datasets marked with * (ETTh1, ETTh2, and Traffic), only Tout∈ {96,192,336}are used
because there are insufficient data to constitute a training set when Tout= 720. The best
results are in bold , while the second-best results are in underlined. Note that TEMPO* is
evaluated under a zero-shot setting as it is a prompt-based method.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.173 0.227 0.175 0.230 0.184 0.242 0.171 0.224 0.172 0.263 0.211 0.254 0.182 0.276
1920.218 0.265 0.227 0.276 0.228 0.283 0.230 0.277 0.224 0.271 0.254 0.298 0.273 0.283
3360.276 0.310 0.286 0.322 0.279 0.322 0.294 0.326 0.282 0.321 0.292 0.332 0.294 0.325Weather
7200.355 0.366 0.366 0.379 0.364 0.388 0.384 0.387 0.366 0.381 0.370 0.379 0.383 0.388
960.509 0.484 0.543 0.506 0.547 0.503 0.557 0.519 0.483 0.464 0.400 0.406 0.531 0.447
1920.717 0.581 0.748 0.580 0.720 0.604 0.711 0.570 0.629 0.540 0.426 0.421 0.750 0.533
3360.728 0.589 0.754 0.595 0.984 0.727 0.816 0.619 0.768 0.626 0.441 0.430 0.741 0.636ETTh1
720 - - - - - - - - - - - - - -
960.314 0.375 0.376 0.421 0.442 0.456 0.401 0.421 0.336 0.397 0.301 0.353 0.368 0.457
1920.365 0.408 0.418 0.441 0.617 0.542 0.452 0.455 0.406 0.425 0.355 0.389 0.407 0.486
3360.398 0.432 0.408 0.439 1.424 0.849 0.464 0.469 0.405 0.432 0.379 0.408 0.402 0.428ETTh2
720 - - - - - - - - - - - - - -
960.349 0.379 0.386 0.405 0.332 0.374 0.399 0.414 0.316 0.377 0.438 0.424 0.340 0.381
1920.374 0.394 0.440 0.438 0.358 0.390 0.441 0.436 0.450 0.464 0.461 0.432 0.473 0.451
3360.411 0.417 0.485 0.459 0.402 0.416 0.499 0.467 0.450 0.424 0.515 0.467 0.519 0.464ETTm1
7200.516 0.479 0.577 0.499 0.511 0.489 0.767 0.587 0.483 0.471 0.591 0.509 0.604 0.499
960.192 0.273 0.199 0.280 0.236 0.326 0.206 0.288 0.174 0.261 0.185 0.267 0.254 0.275
1920.249 0.309 0.256 0.316 0.306 0.373 0.264 0.324 0.215 0.287 0.243 0.304 0.265 0.286
3360.301 0.342 0.318 0.353 0.380 0.423 0.334 0.367 0.273 0.330 0.309 0.345 0.360 0.373ETTm2
7200.402 0.405 0.460 0.436 0.674 0.583 0.454 0.432 0.433 0.412 0.386 0.395 0.511 0.439
960.139 0.235 0.143 0.241 0.150 0.251 0.145 0.244 0.147 0.242 0.178 0.276 0.144 0.246
1920.155 0.249 0.159 0.255 0.163 0.263 0.163 0.260 0.158 0.241 0.198 0.293 0.180 0.248
3360.174 0.269 0.179 0.274 0.175 0.278 0.183 0.281 0.178 0.277 0.209 0.309 0.194 0.304ECL
7200.222 0.310 0.233 0.323 0.219 0.311 0.233 0.323 0.224 0.312 0.279 0.355 0.205 0.277
960.401 0.285 0.419 0.298 0.427 0.304 0.404 0.286 0.414 0.291 0.476 0.343 0.443 0.317
1920.418 0.293 0.434 0.305 0.447 0.315 0.412 0.294 0.419 0.291 0.496 0.355 0.407 0.320
3360.436 0.308 0.449 0.313 0.478 0.333 0.439 0.310 0.437 0.314 0.503 0.356 0.440 0.323Traffic
720 - - - - - - - - - - - - - -
Avg. Rank 2.120 2.200 4.200 4.000 4.680 5.080 4.760 4.640 2.720 2.920 4.400 4.280 5.000 4.640
model in the time-series alignment stage, which is then applied consistently across all prediction
lengths. In contrast, in the forecasting fine-tuning stage, we fine-tune a distinct model for each
prediction length, while ensuring that all these models share the same hyperparameters.
Both LLM4TS and GPT4TS [53] consistently surpass most train-from-scratch models in
limited-data scenarios across various datasets, thanks to the pre-existing representation learn-
ing capability encapsulated in GPT-2. With the additional time-series alignment and multi-scale
temporal information integration, LLM4TS emerges as a better data-efficient time-series fore-
caster against GPT4TS, achieving better performance across all datasets. Notably, LLM4TS
with only 5% of data outperforms the best baseline that uses 10% of data. For the largest
dataset (Traffic), PatchTST emerges as the leading model in the full-shot scenario, though this
trend does not extend to few-shot scenarios. With only 10% training data, LLM4TS outper-
forms PatchTST in 5 out of 8 evaluations, and with just 5% training data, it leads in 5 out
of 6 evaluations. This suggests that in few-shot scenarios, traditional deep train-from-scratch
models generally still underperform compared to those leveraging pre-trained LLMs.
12

--- PAGE 13 ---
Table 3: Few-shot long-term forecasting using 10%of the training data. We use
prediction lengths T∈ {96,192,336,720}for all datasets. The best results are in bold , while
the second-best results are underlined. Note that TEMPO* is evaluated under a zero-shot
setting as it is a prompt-based method.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.158 0.207 0.163 0.215 0.171 0.224 0.165 0.215 0.161 0.210 0.211 0.254 0.163 0.213
1920.204 0.249 0.210 0.254 0.215 0.263 0.210 0.257 0.204 0.248 0.254 0.298 0.230 0.263
3360.254 0.288 0.256 0.292 0.258 0.299 0.259 0.297 0.261 0.302 0.292 0.332 0.258 0.282Weather
7200.322 0.336 0.321 0.339 0.320 0.346 0.332 0.346 0.309 0.332 0.370 0.379 0.301 0.328
960.417 0.432 0.458 0.456 0.492 0.495 0.516 0.485 0.448 0.460 0.400 0.406 0.455 0.457
1920.469 0.468 0.570 0.516 0.565 0.538 0.598 0.524 0.484 0.483 0.426 0.421 0.572 0.519
3360.505 0.499 0.608 0.535 0.721 0.622 0.657 0.550 0.589 0.540 0.441 0.430 0.578 0.531ETTh1
7200.708 0.572 0.725 0.591 0.986 0.743 0.762 0.610 0.700 0.604 0.443 0.451 0.723 0.594
960.282 0.351 0.331 0.374 0.357 0.411 0.353 0.389 0.275 0.326 0.301 0.353 0.332 0.374
1920.364 0.400 0.402 0.411 0.569 0.519 0.403 0.414 0.374 0.373 0.355 0.389 0.401 0.433
3360.374 0.416 0.406 0.433 0.671 0.572 0.426 0.441 0.406 0.429 0.379 0.408 0.408 0.440ETTh2
7200.445 0.461 0.449 0.464 0.824 0.648 0.477 0.480 0.427 0.449 0.409 0.440 0.459 0.480
960.360 0.388 0.390 0.404 0.352 0.392 0.410 0.419 0.346 0.388 0.438 0.424 0.392 0.401
1920.386 0.401 0.429 0.423 0.382 0.412 0.437 0.434 0.373 0.416 0.461 0.432 0.423 0.426
3360.415 0.417 0.469 0.439 0.419 0.434 0.476 0.454 0.413 0.426 0.515 0.467 0.471 0.444ETTm1
7200.470 0.445 0.569 0.498 0.490 0.477 0.681 0.556 0.485 0.476 0.591 0.509 0.552 0.501
960.184 0.265 0.188 0.269 0.213 0.303 0.191 0.274 0.177 0.261 0.185 0.267 0.233 0.262
1920.240 0.301 0.251 0.309 0.278 0.345 0.252 0.317 0.241 0.314 0.243 0.304 0.303 0.302
3360.294 0.337 0.307 0.346 0.338 0.385 0.306 0.353 0.274 0.327 0.309 0.345 0.359 0.341ETTm2
7200.386 0.393 0.426 0.417 0.436 0.440 0.433 0.427 0.417 0.390 0.386 0.395 0.452 0.419
960.135 0.231 0.139 0.237 0.150 0.253 0.140 0.238 0.139 0.241 0.178 0.276 0.138 0.235
1920.152 0.246 0.156 0.252 0.164 0.264 0.160 0.255 0.151 0.248 0.198 0.293 0.158 0.255
3360.173 0.267 0.175 0.270 0.181 0.282 0.180 0.276 0.169 0.270 0.209 0.309 0.176 0.275ECL
7200.229 0.312 0.233 0.317 0.223 0.321 0.241 0.323 0.240 0.322 0.279 0.355 0.230 0.311
960.402 0.288 0.414 0.297 0.419 0.298 0.403 0.289 0.418 0.291 0.476 0.343 0.415 0.317
1920.416 0.294 0.426 0.301 0.434 0.305 0.415 0.296 0.414 0.296 0.496 0.355 0.425 0.300
3360.429 0.302 0.434 0.303 0.449 0.313 0.426 0.304 0.421 0.311 0.503 0.356 0.436 0.310Traffic
7200.480 0.326 0.487 0.337 0.484 0.336 0.474 0.331 0.462 0.327 0.538 0.376 0.489 0.338
Avg. Rank 2.036 1.679 4.000 3.786 5.143 5.679 5.036 5.071 2.214 2.786 4.786 4.821 4.536 3.857
We also compare the latest LLM-based methods in the domain of time-series forecasting:
Time-LLM, TEMPO, and TEST. It’s important to note that TEMPO is evaluated under a
zero-shot setting, as it is a prompt-based method. Consequently, in both few-shot and full-shot
scenarios, TEMPO consistently reports zero-shot results. For larger datasets (like Weather,
Electricity, and Traffic), LLM4TS consistently achieves the best performance, even with only
5% or 10% of the training data. TEMPO performs best on the ETTh1 and ETTh2 datasets,
while Time-LLM leads on the ETTm1 and ETTm2 datasets. Overall, LLM4TS demonstrates
superior performance across all these LLM-based methods in both the 5% and 10% training
data scenarios.
5.2 Full-Shot Learning in Long-Term Time-Series Forecasting
Table 4 presents the results of long-term time-series forecasting averaged over a consistent pre-
diction length set Tout∈ {96,192,336,720}for all datasets. While the primary focus of using
pre-trained LLMs is on few-shot learning, LLM4TS not only excels in this area but also outper-
13

--- PAGE 14 ---
Table 4: Long-term forecasting for multivariate time-series data. We use prediction
lengths T∈ {96,192,336,720}for all datasets. The best results are in bold , while the second-
best results are underlined. Note that TEMPO* is evaluated under a zero-shot setting as it is
a prompt-based method.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.147 0.196 0.162 0.212 0.176 0.237 0.149 0.198 0.147 0.201 0.211 0.254 0.150 0.202
1920.191 0.238 0.204 0.248 0.220 0.282 0.194 0.241 0.189 0.234 0.254 0.298 0.198 0.246
3360.241 0.277 0.254 0.286 0.265 0.319 0.245 0.282 0.262 0.279 0.292 0.332 0.245 0.286Weather
7200.313 0.329 0.326 0.337 0.333 0.362 0.314 0.334 0.304 0.316 0.370 0.379 0.324 0.342
960.371 0.394 0.376 0.397 0.375 0.399 0.370 0.399 0.362 0.392 0.400 0.406 0.372 0.400
1920.403 0.412 0.416 0.418 0.405 0.416 0.413 0.421 0.398 0.418 0.426 0.421 0.414 0.422
3360.420 0.422 0.442 0.433 0.439 0.443 0.422 0.436 0.430 0.427 0.441 0.430 0.422 0.437ETTh1
7200.422 0.444 0.477 0.456 0.472 0.490 0.447 0.466 0.442 0.457 0.443 0.451 0.447 0.467
960.269 0.332 0.285 0.342 0.289 0.353 0.274 0.336 0.268 0.328 0.301 0.353 0.275 0.338
1920.328 0.377 0.354 0.389 0.383 0.418 0.339 0.379 0.329 0.375 0.355 0.389 0.340 0.379
3360.353 0.396 0.373 0.407 0.448 0.465 0.329 0.380 0.368 0.409 0.379 0.408 0.329 0.381ETTh2
7200.383 0.425 0.406 0.441 0.605 0.551 0.379 0.422 0.372 0.420 0.409 0.440 0.381 0.423
960.285 0.343 0.292 0.346 0.299 0.343 0.290 0.342 0.272 0.334 0.438 0.424 0.293 0.346
1920.324 0.366 0.332 0.372 0.335 0.365 0.332 0.369 0.310 0.358 0.461 0.432 0.332 0.369
3360.353 0.385 0.366 0.394 0.369 0.386 0.366 0.392 0.352 0.384 0.515 0.467 0.368 0.392ETTm1
7200.408 0.419 0.417 0.421 0.425 0.421 0.416 0.420 0.383 0.411 0.591 0.509 0.418 0.420
960.165 0.254 0.173 0.262 0.167 0.269 0.165 0.255 0.161 0.253 0.185 0.267 0.193 0.237
1920.220 0.292 0.229 0.301 0.224 0.303 0.220 0.292 0.219 0.293 0.243 0.304 0.257 0.264
3360.268 0.326 0.286 0.341 0.281 0.342 0.274 0.329 0.271 0.329 0.309 0.345 0.289 0.295ETTm2
7200.350 0.380 0.378 0.401 0.397 0.421 0.362 0.385 0.352 0.379 0.386 0.395 0.375 0.369
960.128 0.223 0.139 0.238 0.140 0.237 0.129 0.222 0.131 0.224 0.178 0.276 0.132 0.223
1920.146 0.240 0.153 0.251 0.153 0.249 0.157 0.240 0.152 0.241 0.198 0.293 0.158 0.241
3360.163 0.258 0.169 0.266 0.169 0.267 0.163 0.259 0.160 0.248 0.209 0.309 0.163 0.260ECL
7200.200 0.292 0.206 0.297 0.203 0.301 0.197 0.290 0.192 0.298 0.279 0.355 0.199 0.291
960.372 0.259 0.388 0.282 0.410 0.282 0.360 0.249 0.362 0.248 0.476 0.343 0.407 0.282
1920.391 0.265 0.407 0.290 0.423 0.287 0.379 0.256 0.374 0.247 0.496 0.355 0.423 0.287
3360.405 0.275 0.412 0.294 0.436 0.296 0.392 0.264 0.385 0.271 0.503 0.356 0.430 0.296Traffic
7200.437 0.292 0.450 0.312 0.466 0.315 0.432 0.286 0.430 0.288 0.538 0.376 0.463 0.315
Avg. Rank 2.036 2.214 4.786 4.750 5.536 5.357 2.571 2.750 1.643 2.143 6.607 6.250 4.214 3.679
forms all deep train-from-scratch methods, even with full dataset access, thanks to its two-stage
fine-tuning and integration of multi-scale temporal information. In contrast, GPT4TS, despite
utilizing the pre-trained GPT-2’s representation learning capabilities, does not achieve superior
performance over traditional train-from-scratch baselines in full-shot scenarios. This is partic-
ularly evident when handling extensive volumes of training data. This limitation mainly stems
from its absence of time-series alignment and the inclusion of multi-scale temporal information,
which is essential for enhancing performance in time-series forecasting. Interestingly, for the
largest dataset (Traffic), PatchTST can outcompete both LLM4TS and GPT4TS. This suggests
that with complete dataset access and sufficient data volume, traditional deep train-from-scratch
models may sometimes outshine those leveraging pre-trained LLMs.
We also compare the latest LLM-based methods in the domain of time-series forecasting:
Time-LLM, TEMPO, and TEST. It’s important to note that TEMPO is evaluated under a zero-
shot setting, as it is a prompt-based method, and therefore consistently reports zero-shot results
in both few-shot and full-shot scenarios. Time-LLM emerges as the leading model, thanks to
its innovative approach of reprogramming LLMs for time series forecasting by converting time
14

--- PAGE 15 ---
Table 5: Unsupervised representation learning evaluation in forecasting with linear
probing. We use prediction lengths Tout∈ {24,48,168,336,720}for the ETTh1 dataset. The
best average results are in bold , while the second-best results are in underlined.
Methods LLM4TS PatchTST BTSF TS2Vec TNC TS-TCC
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
24 0.315 0.365 0.322 0.369 0.541 0.519 0.599 0.534 0.632 0.596 0.653 0.610
48 0.342 0.384 0.354 0.385 0.613 0.524 0.629 0.555 0.705 0.688 0.720 0.693
168 0.401 0.415 0.419 0.424 0.640 0.532 0.755 0.636 1.097 0.993 1.129 1.044
336 0.421 0.427 0.445 0.446 0.864 0.689 0.907 0.717 1.454 0.919 1.492 1.076
720 0.426 0.447 0.487 0.478 0.993 0.712 1.048 0.790 1.604 1.118 1.603 1.206ETTh1
Avg. 0.381 0.408 0.405 0.420 0.730 0.595 0.788 0.646 1.098 0.863 1.119 0.926
series into text prototypes and using prompts to guide predictions. However, LLM4TS remains
very close in performance to Time-LLM. Notably, LLM4TS still outperforms Time-LLM in
few-shot scenarios, demonstrating its exceptional data efficiency and robustness in limited-data
settings.
5.3 Unsupervised Representation Learning
Given that the autoregressive objective used in the time-series alignment stage can be seen as a
pretext task in unsupervised representation learning, we aim to assess LLM4TS’s representation
learning capability. To evaluate the effectiveness of unsupervised representation learning, we
conduct a linear evaluation on time-series forecasting. This involves pre-training the backbone
model using the pretext task, freezing its weights, and then training an attached linear layer
on the downstream forecasting task. With the backbone model’s parameters fixed, strong per-
formance in forecasting depends on the expressiveness of the learned representations. Table 5
shows LLM4TS’s superior performance over competitors on the ETTh1 dataset, highlighting
the effectiveness of adapting the LLM to time-series characteristics in the time-series alignment
stage. This comparison exclusively includes self-supervised learning methods, thereby exclud-
ing deep train-from-scratch models designed explicitly for time-series forecasting. Similarly,
GPT4TS is not part of this experiment as it lacks a distinct stage of representation learning.
The variant of PatchTST used here differs from that in the forecasting experiments; this variant
focuses on representation learning. PatchTST incorporates an MLM (Masked Language Model)
approach, akin to BERT, for learning representations. Despite this, LLM4TS still emerges as
the top method in representation learning capability among all evaluated methods, achieving
an average improvement of 6 .02% in MSE. Within the domain of unsupervised representation
learning for time-series data, models based on CNN (such as BTSF, TS2Vec, TS-TCC) and
RNN (like TNC) are typically favored over Transformers. However, our experiments reveal
that Transformer-based models can surpass these traditional choices in performance with well-
designed pretext tasks.
5.4 Ablation Study
5.4.1 Key Components in LLM4TS
Fig. 5 explores the effects of time-series alignment, multi-scale temporal encoding, and PEFT
in LLM4TS, assessing both full- and few-shot scenarios on the ETTh1 dataset. A compara-
tive analysis—with and without these components—highlights their individual importance in
enhancing forecasting accuracy in both scenarios. Notably, LLM4TS delivers exceptional per-
15

--- PAGE 16 ---
Figure 5: Ablation study on key components in LLM4TS. Each ablation is conducted
under both full- and few-shot learning with 10% training data. We report results averaged over
prediction lengths Tout∈ {96,192,336,720}for the ETTh1 dataset. The best results are in
bold .
formance in few-shot learning, averaging a 6 .2% reduction in MSE with each incorporation of
these components.
In the experimental results, we observe three key insights. First, there is a notable trend
where the MSE improvement increases as the prediction length extends. This indicates that
the core elements of LLM4TS become increasingly beneficial in situations where a higher level
of predictive capability is required, particularly evident with longer prediction lengths. Second,
few-shot scenarios exhibit more substantial gains than full-shot scenarios upon integrating these
main components into LLM4TS. It emphasizes LLM4TS’s strength as a data-efficient time-series
forecaster, a quality primarily attributed to its intrinsic components. Third, among the two
PEFT methods, LoRA proves to be more beneficial than Layer Normalization. This advantage is
consistently observed in both full-shot and few-shot scenarios, highlighting LoRA’s effectiveness
in enhancing the model’s performance.
5.4.2 Training Strategies in Forecasting Fine-Tuning
As discussed in Section 4.2, while linear probing (LP) shows superior performance in out-of-
distribution (OOD) scenarios and full fine-tuning (FT) excels in in-distribution (ID) scenarios,
LP-FT can surpass FT and LP in both OOD and ID scenarios. Fig. 6 shows that LP-FT
enhances performance in both full- and few-shot learning on the ETTh1 dataset, achieving an
average improvement of 0 .7% in MSE for full-shot learning and 2 .51% for few-shot learning.
The subtle improvements in both scenarios can be attributed to the limited number of trainable
parameters in the LLM4TS’s backbone model even when using FT, which narrows the distinc-
tion between LP and FT. The results further reveal that few-shot learning derives a greater
advantage from LP-FT, primarily due to its higher vulnerability to severe OOD issues. Addi-
tionally, consistent with observations in the ablation study of LLM4TS’s main components, we
note a similar trend where longer prediction lengths yield more significant benefits in few-shot
scenarios.
16

--- PAGE 17 ---
Figure 6: Ablation study on training strategies in forecasting fine-tuning. Each ab-
lation is conducted under both full- and few-shot learning with 10% training data. We report
results averaged over prediction lengths Tout∈ {96,192,336,720}for the ETTh1 dataset. The
best results are in bold .
Table 6: Ablation study on the effectiveness of LLM’s pre-trained weights . Each
ablation is conducted under few-shot learning with 10% and 5% training data. ’No Freeze’
refers to the model utilizing LLM’s pre-trained weights without freezing any layers during
training, whereas ’No Pretrain’ denotes the model not utilizing LLM’s pre-trained weights,
implying the model is trained from scratch. We report results averaged over prediction lengths
Tout∈ {96,192,336,720}for the Weather, ETTm1, and ETTm2 datasets. The best average
results are in bold .
Methods LLM4TS GPT4TS No Freeze No Pretrain
Metric MSE MAE MSE MAE MSE MAE MSE MAE
Weather 0.235 0.270 0.238 0.275 0.273 0.302 0.278 0.305
ETTm1 0.408 0.413 0.464 0.441 0.546 0.484 0.473 0.446 10%
ETTm2 0.276 0.324 0.293 0.335 0.340 0.367 0.361 0.385
Weather 0.256 0.292 0.264 0.302 0.284 0.312 0.298 0.324
ETTm1 0.413 0.417 0.467 0.450 0.562 0.496 0.470 0.452 5%
ETTm2 0.286 0.332 0.308 0.347 0.327 0.362 0.413 0.411
5.4.3 Effectiveness of LLM’s Pre-trained Weights
As discussed in Section 2.1, most parameters in these LLMs are kept fixed to preserve their
data-independent representation learning capability. Table 6 shows that LLM4TS performs the
best when most parameters remain unchanged on the Weather, ETTm1, and ETTm2 datasets.
Specifically, LLM4TS demonstrates a notable average improvement of 17 .78% in MSE com-
pared to the ’No Freeze’ approach, where pre-trained weights are utilized without freezing any
layers during training. Furthermore, when compared to the ’No Pretrain’ approach, where the
model is trained from scratch without leveraging pre-trained weights, LLM4TS showcases an
even more significant average improvement of 18 .28% in MSE. This emphasizes the importance
of retaining the pre-existing strengths in representation learning inherent to these models, at-
tributable primarily to the self-attention mechanisms within pre-trained transformers, which
facilitate the development of data-independent operations.
5.5 Training and Inference Cost
Evaluating the computational costs of LLM-based models is essential for determining their
practicality in real-world scenarios. In this context, we compare LLM4TS with two other
17

--- PAGE 18 ---
Table 7: Training parameters .
Model Trainable Parameters Total Parameters Trainable Parameters Percentage
LLM4TS 3.4M 85M 4%
PatchTST 20M 20M 100%
FEDformer 33M 33M 100%
Figure 7: Comparison of training and inference time (in seconds) for one batch. We
use the prediction length Tout= 96 for the ETTh2 dataset. The best results are in bold .
leading Transformer-based baselines, PatchTST and FEDformer. Details regarding the number
of trainable parameters and the total parameters can be found in Table 7. LLM4TS distinguishes
itself by keeping most pre-trained parameters fixed and employing two Parameter-Efficient Fine-
Tuning (PEFT) methods: Layer Normalization Tuning and LoRA. Consequently, only 4% of its
parameters are trainable, rendering the number of trainable parameters in LLM4TS significantly
lower than those in its train-from-scratch counterparts.
The execution time for both training and inference of LLM4TS, in comparison with PatchTST
and FEDformer, is evaluated using an NVIDIA Tesla V100 GPU, and the results are illustrated
in Fig. 7. To ensure a fair comparison across all methods, we standardized the batch size at
128 and set the hidden dimensions to 768, aligning with the specifications of GPT-2. The
evaluation was conducted on a single batch, and for LLM4TS, we provided the training time for
both stages, as it involved a two-stage training process. LLM4TS outperformed the baselines in
execution time during both the training and inference stages. This enhanced efficiency is cred-
ited to its architecture, which leverages a majority of non-trainable parameters, thus markedly
diminishing the computational burden throughout both the training and inference phases.
6 Conclusion
In this paper, we present LLM4TS, a framework for time-series forecasting utilizing pre-trained
LLMs. LLM4TS employs a two-stage fine-tuning strategy, beginning with the time-series align-
ment stage to adapt LLMs to the characteristics of time-series data, followed by the forecasting
fine-tuning stage designed for time-series forecasting tasks. Our framework also introduces a
novel two-level aggregation method, integrating multi-scale temporal data within pre-trained
18

--- PAGE 19 ---
LLMs to improve their interpretation of time-related information. Through experiments on
7 time-series forecasting datasets, LLM4TS demonstrates superior performance over existing
state-of-the-art methods, including those trained from scratch, in both full and few-shot sce-
narios.
In future work, we plan to extend our research in two directions. First, while we chose GPT-
2 as our primary LLM in this paper for a fair comparison over GPT4TS, we plan to evaluate
more recent LLMs like GPT-3.5 and LLaMA-2 to assess their advancements. Second, we aim
to explore other tasks, such as classification and anomaly detection. Although forecasting is
highly relevant to real-world applications without the need for manual labeling, extending it to
other tasks enables the broader applicability of our LLM4TS framework.
References
[1] Akbar Abbaspour Ghadim Bonab. A comparative study of demand forecasting based
on machine learning methods with time series approach. Journal of applied research on
industrial engineering , 9(3):331–353, 2022.
[2] Sabeen Ahmed, Ian E. Nielsen, Aakash Tripathi, Shamoon Siddiqui, Ravi Prakash Ra-
machandran, and Ghulam Rasool. Transformers in time-series analysis: A tutorial. Circuits
Syst. Signal Process. , 42(12):7433–7466, 2023.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In NeurIPS , 2020.
[4] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan
Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting.
InThe Twelfth International Conference on Learning Representations , 2024.
[5] Ching Chang, Chiao-Tung Chan, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen.
Timedrl: Disentangled representation learning for multivariate time-series. CoRR ,
abs/2312.04142, 2023.
[6] Ananda Chatterjee, Hrisav Bhowmick, and Jaydip Sen. Stock price prediction using time
series, econometric, machine learning, and deep learning models. CoRR , abs/2111.01137,
2021.
[7] Hao-Yi Chih, Yao-Chung Fan, Wen-Chih Peng, and Hai-Yuan Kuo. Product quality pre-
diction with convolutional encoder-decoder architecture and transfer learning. In CIKM ,
pages 195–204. ACM, 2020.
[8] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli
Li, and Cuntai Guan. Time-series representation learning via temporal and contextual
contrasting. In IJCAI , pages 2352–2359. ijcai.org, 2021.
[9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-
audio generation using instruction guided latent diffusion model. In ACM Multimedia ,
pages 3590–3598. ACM, 2023.
19

--- PAGE 20 ---
[10] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and
Dimitris Papailiopoulos. Looped transformers as programmable computers. In ICML ,
volume 202 of Proceedings of Machine Learning Research , pages 11398–11442. PMLR,
2023.
[11] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Transfer learning for financial
time series forecasting. In PRICAI (2) , volume 11671 of Lecture Notes in Computer Science ,
pages 24–36. Springer, 2019.
[12] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Multi-source transfer learning
with ensemble for financial time series forecasting. In WI/IAT , pages 227–233. IEEE, 2020.
[13] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and
David A. Sontag. Tabllm: Few-shot classification of tabular data with large language
models. In AISTATS , volume 206 of Proceedings of Machine Learning Research , pages
5549–5581. PMLR, 2023.
[14] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani,
Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (TCN) network for
an effective weather forecasting using time-series data from the local weather station. Soft
Comput. , 24(21):16453–16482, 2020.
[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol
Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR ,
abs/2203.15556, 2022.
[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In
ICLR . OpenReview.net, 2022.
[17] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu
Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series
forecasting by reprogramming large language models. CoRR , abs/2310.01728, 2023.
[18] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu
Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time
series forecasting by reprogramming large language models. In The Twelfth International
Conference on Learning Representations , 2024.
[19] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
Reversible instance normalization for accurate time-series forecasting against distribution
shift. In ICLR . OpenReview.net, 2022.
[20] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang.
Fine-tuning can distort pretrained features and underperform out-of-distribution. In ICLR .
OpenReview.net, 2022.
[21] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and
short-term temporal patterns with deep neural networks. In SIGIR , pages 95–104. ACM,
2018.
[22] Sangwon Lee, Junho Hong, Ling Liu, and Wonik Choi. Ts-fastformer: Fast transformer
for time-series forecasting. ACM Trans. Intell. Syst. Technol. , 15(2), feb 2024.
20

--- PAGE 21 ---
[23] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philo-
sophical Transactions of the Royal Society A , 379(2194):20200209, 2021.
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
InNeurIPS , 2023.
[25] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang, and
Wei Song. Gated transformer networks for multivariate time series classification. CoRR ,
abs/2103.14438, 2021.
[26] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transform-
ers as universal computation engines. In AAAI , pages 7628–7636. AAAI Press, 2022.
[27] Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsu-
pervised representation learning for time series: A review. CoRR , abs/2308.01578, 2023.
[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series
is worth 64 words: Long-term forecasting with transformers. In ICLR . OpenReview.net,
2023.
[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1:9, 2019.
[30] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual LLM for video under-
standing. CoRR , abs/2312.06720, 2023.
[31] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm:
Can large language models understand structured table data? a benchmark and empirical
study. In Proceedings of the 17th ACM International Conference on Web Search and Data
Mining , WSDM ’24, page 645–654, New York, NY, USA, 2024. Association for Computing
Machinery.
[32] Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. TEST: Text prototype aligned em-
bedding to activate LLM’s ability for time series. In The Twelfth International Conference
on Learning Representations , 2024.
[33] Hiteshi Tandon, Prabhat Ranjan, Tanmoy Chakraborty, and Vandana Suhag. Coronavirus
(covid-19): Arima-based time-series analysis to forecast near future and the effect of school
reopening in india. Journal of Health Management , 24(3):373–388, 2022.
[34] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learn-
ing for time series with temporal neighborhood coding. In ICLR . OpenReview.net, 2021.
[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´ elien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and
efficient foundation language models. CoRR , abs/2302.13971, 2023.
[36] Alexandros-Menelaos Tzortzis, Sotiris Pelekis, Evangelos Spiliotis, Spiros Mouzakitis,
John E. Psarras, and Dimitris Askounis. Transfer learning for day-ahead load forecasting:
a case study on european national electricity demand time series. CoRR , abs/2310.15555,
2023.
[37] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori
Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities
of large language models. Trans. Mach. Learn. Res. , 2022, 2022.
21

--- PAGE 22 ---
[38] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang
Sun. Transformers in time series: A survey. In IJCAI , pages 6778–6786. ijcai.org, 2023.
[39] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times-
net: Temporal 2d-variation modeling for general time series analysis. In ICLR . OpenRe-
view.net, 2023.
[40] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition
transformers with auto-correlation for long-term series forecasting. Advances in Neural
Information Processing Systems , 34:22419–22430, 2021.
[41] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time
series anomaly detection with association discrepancy. In ICLR . OpenReview.net, 2022.
[42] Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iter-
ative bilinear temporal-spectral fusion. In ICML , volume 162 of Proceedings of Machine
Learning Research , pages 25038–25054. PMLR, 2022.
[43] Cheng-Han Yeh, Yao-Chung Fan, and Wen-Chih Peng. Interpretable multi-task learning
for product quality prediction with attention mechanism. In ICDE , pages 1910–1921. IEEE,
2019.
[44] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian,
Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in
time series forecasting. Advances in Neural Information Processing Systems , 36, 2024.
[45] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong,
and Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI , pages
8980–8987. AAAI Press, 2022.
[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time
series forecasting? In AAAI , pages 11121–11128. AAAI Press, 2023.
[47] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised
contrastive pre-training for time series via time-frequency consistency. Advances in Neural
Information Processing Systems , 35:3988–4003, 2022.
[48] Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need
billions of words of pretraining data? In ACL/IJCNLP (1) , pages 1112–1125. Association
for Computational Linguistics, 2021.
[49] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension de-
pendency for multivariate time series forecasting. In ICLR . OpenReview.net, 2023.
[50] Xiaochen Zheng, Xingyu Chen, Manuel Sch¨ urch, Amina Mollaysa, Ahmed Allam, and
Michael Krauthammer. Simts: Rethinking contrastive representation learning for time
series forecasting. arXiv preprint arXiv:2303.18205 , 2023.
[51] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wan-
cai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting.
InAAAI , pages 11106–11115. AAAI Press, 2021.
[52] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:
Frequency enhanced decomposed transformer for long-term series forecasting. In ICML ,
volume 162 of Proceedings of Machine Learning Research , pages 27268–27286. PMLR, 2022.
[53] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general
time series analysis by pretrained LM. In NeurIPS , 2023.
22
