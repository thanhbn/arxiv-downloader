# 2401.01335.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2401.01335.pdf
# Kích thước tệp: 1432128 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tinh Chỉnh Tự Đấu Chuyển Đổi Mô Hình Ngôn Ngữ Yếu Thành Mô Hình Ngôn Ngữ Mạnh

Zixiang Chen* 1Yihe Deng* 1Huizhuo Yuan* 1Kaixuan Ji1Quanquan Gu1

Tóm tắt
Khai thác sức mạnh của dữ liệu được chú thích bởi con người thông qua Tinh Chỉnh Có Giám Sát (SFT) là then chốt để thúc đẩy các Mô Hình Ngôn Ngữ Lớn (LLM). Trong bài báo này, chúng tôi đi sâu vào triển vọng phát triển một LLM mạnh từ một LLM yếu mà không cần thu thập thêm dữ liệu được chú thích bởi con người. Chúng tôi đề xuất một phương pháp tinh chỉnh mới được gọi là Self-Play fIne-tuNing (SPIN), bắt đầu từ một mô hình đã được tinh chỉnh có giám sát. Trọng tâm của SPIN là một cơ chế tự đấu, trong đó LLM tinh chỉnh khả năng của mình bằng cách đấu với các phiên bản của chính nó. Cụ thể hơn, LLM tạo ra dữ liệu huấn luyện của riêng mình từ các lần lặp trước đó, tinh chỉnh chính sách của mình bằng cách phân biệt những phản hồi được tự tạo ra này với những phản hồi thu được từ dữ liệu được chú thích bởi con người. Phương pháp của chúng tôi dần dần nâng cao LLM từ một mô hình sơ khai đến một mô hình đáng gờm, mở khóa toàn bộ tiềm năng của dữ liệu minh họa được chú thích bởi con người cho SFT. Về mặt lý thuyết, chúng tôi chứng minh rằng tối ưu toàn cục cho hàm mục tiêu huấn luyện của phương pháp chúng tôi chỉ đạt được khi chính sách LLM phù hợp với phân phối dữ liệu đích. Về mặt thực nghiệm, chúng tôi đánh giá phương pháp của mình trên một số bộ dữ liệu chuẩn bao gồm HuggingFace Open LLM Leaderboard, MT-Bench, và các bộ dữ liệu từ Big-Bench. Kết quả của chúng tôi cho thấy SPIN có thể cải thiện đáng kể hiệu suất của LLM trên nhiều bộ chuẩn khác nhau và thậm chí vượt trội hơn các mô hình được huấn luyện thông qua tối ưu hóa sở thích trực tiếp (DPO) được bổ sung với dữ liệu sở thích GPT-4 bổ sung. Điều này làm sáng tỏ lời hứa của tự đấu, cho phép đạt được hiệu suất ở mức con người trong LLM mà không cần đến các đối thủ chuyên gia. Mã nguồn có sẵn tại https://github.com/uclaml/SPIN.

*Đóng góp bình đẳng1Khoa Khoa học Máy tính, Đại học California, Los Angeles, CA 90095, USA. Liên hệ: Quanquan Gu <qgu@cs.ucla.edu>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học Máy, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

1 Giới thiệu
Các Mô Hình Ngôn Ngữ Lớn (LLM) đã bắt đầu một kỷ nguyên đột phá trong trí tuệ nhân tạo tổng quát (AGI), thể hiện khả năng phi thường trên một loạt rộng các lĩnh vực đòi hỏi lý luận phức tạp và kiến thức chuyên môn. Những mô hình này xuất sắc trong các lĩnh vực như lý luận/giải quyết vấn đề toán học (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022), tạo mã/lập trình (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), tạo văn bản (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023), tóm tắt và viết sáng tạo, cùng nhiều lĩnh vực khác. Một tiến bộ đáng kể trong LLM là việc căn chỉnh sau tiền huấn luyện với các hành vi mong muốn hơn (Mishra et al., 2021; Victor et al., 2022; Chung et al., 2022; Thoppilan et al., 2022), một quá trình thường dựa vào dữ liệu được chú thích bởi con người tốn kém. Các phương pháp căn chỉnh điển hình bao gồm Tinh Chỉnh Có Giám Sát (SFT) (Ouyang et al., 2022; Tunstall et al., 2023a) dựa trên các minh họa của con người, và Học Tăng Cường từ Phản Hồi của Con Người (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Bai et al., 2022a) dựa trên sở thích của con người.

Tất cả các phương pháp căn chỉnh đã nêu trên đều yêu cầu một khối lượng lớn dữ liệu được chú thích bởi con người. Do đó, có sự quan tâm ngày càng tăng trong việc phát triển các phương pháp tinh chỉnh có thể sử dụng hiệu quả dữ liệu của con người, từ đó hợp lý hóa quá trình căn chỉnh. Điều này thúc đẩy chúng tôi nghiên cứu tinh chỉnh LLM mà không cần dữ liệu được chú thích bởi con người bổ sung ngoài bộ dữ liệu tinh chỉnh. Nghiên cứu của chúng tôi cũng liên quan đến mục tiêu rộng lớn hơn là chuyển đổi mô hình yếu thành mô hình mạnh mà không yêu cầu dữ liệu huấn luyện bổ sung, đây là mối quan tâm trung tâm trong học máy có thể truy nguyên trở lại các thuật toán boosting (Kearns & Valiant, 1994; Schapire, 1990; Freund, 1995; Freund & Schapire, 1997). Thuật toán tự huấn luyện (Vapnik, 1999; Grandvalet & Bengio, 2004; Lee, 2013) cũng đã được chứng minh có thể chuyển đổi người học yếu thành người học mạnh trong các mô hình hỗn hợp mà không cần dữ liệu được gắn nhãn bổ sung (Frei et al., 2022; Kou et al., 2022). Tuy nhiên, việc theo đuổi tự động nâng cao một LLM yếu mà không có hướng dẫn bên ngoài vừa hấp dẫn vừa chưa được nghiên cứu đầy đủ. Điều này đặt ra câu hỏi sau:

1arXiv:2401.01335v3 [cs.LG] 14 Jun 2024

--- TRANG 2 ---
Tinh Chỉnh Tự Đấu Chuyển Đổi Mô Hình Ngôn Ngữ Yếu Thành Mô Hình Ngôn Ngữ Mạnh

Liệu chúng ta có thể trao quyền cho một LLM yếu để cải thiện bản thân mà không cần thu thập thêm dữ liệu được chú thích bởi con người?

Trong bài báo này, chúng tôi trả lời câu hỏi này một cách khẳng định. Lấy cảm hứng từ sự thành công của các cơ chế tự đấu (Samuel, 2000) trong trò chơi, được minh họa bởi AlphaGo Zero (Silver et al., 2017b), AlphaZero (Silver et al., 2017a), với nguồn gốc lịch sử truy nguyên từ TD-Gammon (Tesauro et al., 1995), chúng tôi đề xuất chuyển đổi một LLM yếu thành một LLM mạnh thông qua lăng kính tự đấu, trong đó mô hình được nâng cao bằng cách đấu với chính nó mà không yêu cầu bất kỳ giám sát trực tiếp nào. Cụ thể, chúng tôi đề xuất một phương pháp tinh chỉnh mới được gọi là Self-Play fIne-tuNing (SPIN), bắt đầu từ một mô hình được tinh chỉnh có giám sát. SPIN cho phép LLM tham gia vào tự đấu, loại bỏ nhu cầu về một người chú thích chuyên gia như con người hoặc các LLM tiên tiến hơn như GPT-4. Cụ thể, với LLM từ lần lặp trước đó t được ký hiệu là pθt, chúng tôi sử dụng nó để tạo ra các phản hồi y′ cho các prompt x trong bộ dữ liệu SFT được chú thích bởi con người. Mục tiêu tiếp theo là tìm một LLM mới pθt+1, có khả năng phân biệt các phản hồi y′ được tạo ra bởi pθt với các phản hồi y được tạo ra bởi con người. Quá trình này có thể được xem như một trò chơi hai người: người chơi chính, hoặc LLM mới pθt+1, tìm cách phân biệt giữa các phản hồi của người chơi đối thủ pθt và các phản hồi được tạo ra bởi con người, trong khi đối thủ, hoặc LLM cũ pθt, tạo ra các phản hồi giống với những phản hồi trong bộ dữ liệu SFT được chú thích bởi con người càng nhiều càng tốt. LLM mới pθt+1 được thu được bằng cách tinh chỉnh LLM cũ pθt để ưu tiên các phản hồi từ pdata hơn pθt, dẫn đến một phân phối pθt+1 phù hợp hơn với pdata. Trong lần lặp tiếp theo, LLM mới thu được pθt+1 trở thành đối thủ để tạo phản hồi, với quá trình tự đấu nhằm mục đích cuối cùng là LLM hội tụ về pθ∗=pdata, để LLM mạnh nhất có thể không còn có thể phân biệt các phản hồi được tạo ra bởi phiên bản trước đó của nó và những phản hồi được tạo ra bởi con người.

Thú vị là, phương pháp của chúng tôi thể hiện sự tương đồng với phương pháp tối ưu hóa sở thích trực tiếp (DPO) được giới thiệu gần đây (Rafailov et al., 2023), với sự khác biệt đáng chú ý là bản chất tự đấu của phương pháp chúng tôi. Do đó, cách tiếp cận của chúng tôi nổi bật bằng cách loại bỏ nhu cầu về dữ liệu sở thích con người bổ sung, một yêu cầu có trong phương pháp DPO. Ngoài ra, cơ chế tự đấu trong phương pháp của chúng tôi giống với ý tưởng của mạng đối nghịch tạo sinh (GAN) (Goodfellow et al., 2014; Arjovsky et al., 2017), mặc dù cả bộ phân biệt (người chơi chính) và bộ tạo sinh (đối thủ) trong phương pháp của chúng tôi đều là các thể hiện của cùng một LLM từ các lần lặp khác nhau. Về mặt lý thuyết, chúng tôi chứng minh rằng phương pháp của chúng tôi hội tụ khi phân phối của LLM trùng khớp với phân phối dữ liệu đích, tức là pθt=pdata. Kết quả thực nghiệm của chúng tôi trên zephyr-7b-sft-full (Tunstall et al., 2023a), một LLM được tinh chỉnh dựa trên Mistral-7B (Jiang et al., 2023), cho thấy rằng trong khi việc tiếp tục huấn luyện sử dụng SFT trên bộ dữ liệu SFT của chính nó Ultrachat200k (Ding et al., 2023) đạt đến một trạng thái ổn định về hiệu suất hoặc thậm chí giảm điểm đánh giá, phương pháp của chúng tôi liên tục cải thiện zephyr-7b-sft-full qua các lần lặp liên tiếp trong khi chỉ tận dụng một tập con 50k của bộ dữ liệu Ultrachat200k. Cuối cùng, SPIN cải thiện hiệu quả điểm trung bình của mô hình cơ sở từ 58.14 lên 63.16 trên HuggingFace Open LLM Leaderboard (Beeching et al., 2023) với sự cải thiện đáng kể hơn 10% trong điểm số trên GSM8k và TruthfulQA, và từ 5.94 lên 6.78 trên MT-Bench (Zheng et al., 2023). Đáng chú ý, SPIN đạt được kết quả thậm chí có thể so sánh với các mô hình được huấn luyện trên 62k bộ dữ liệu sở thích bổ sung (Tunstall et al., 2023a) trên Open LLM leaderboard và MT-Bench.

Đồng thời với công trình của chúng tôi, Singh et al. (2023) đề xuất sử dụng dữ liệu tổng hợp với phản hồi nhị phân trong tự huấn luyện, giảm sự phụ thuộc vào dữ liệu con người. Ngược lại, cách tiếp cận của chúng tôi loại bỏ nhu cầu về phản hồi nhị phân bổ sung từ con người hoặc một mô hình phần thưởng bổ sung nhờ vào cơ chế tự đấu. Ngoài ra, Burns et al. (2023) sử dụng một mô hình LLM yếu như hướng dẫn để huấn luyện các LLM mạnh hơn theo cách thức tạo sinh từ yếu đến mạnh. Không giống như Burns et al. (2023), cần cả một giám sát viên yếu và một mô hình mạnh, SPIN của chúng tôi hoạt động hiệu quả với một LLM duy nhất.

Ký hiệu. Chúng tôi sử dụng chữ thường và chữ thường in đậm để biểu thị các vô hướng và vectơ, tương ứng. Chúng tôi sử dụng [N] để biểu thị tập chỉ số {1, . . . , N}. Trong không gian hàm, gọi F là lớp hàm. Ký hiệu qdata chỉ định phân phối dữ liệu đích, trong khi prep biểu thị xác suất có điều kiện của phản hồi LLM (tức là chính sách LLM).

2 Công trình liên quan
Tự Đấu. Tự đấu (Samuel, 1959; Tesauro et al., 1995), trong đó thuật toán học bằng cách đấu với chính nó, đã nhận được sự chú ý đáng kể do hiệu quả của nó trong học tăng cường đa tác nhân (MARL). Phương pháp này bao gồm các tác nhân tương tác với các bản sao của chính chúng, cho phép một mức độ thách thức và phức tạp ngày càng tăng trong môi trường học tập. Một công trình cơ bản trong lĩnh vực tự đấu là AlphaGo Zero (Silver et al., 2017b), đã thể hiện hiệu suất vượt trội so với người chơi con người bằng cách sử dụng một sơ đồ học tập tự đấu. Nghiên cứu tiếp theo đã mở rộng khái niệm tự đấu, khám phá các thích ứng và triển khai khác nhau (Anthony et al., 2017; Lanctot et al., 2017; Bansal et al., 2018; Hernandez-Leal et al., 2018; Muller et al., 2019; Vinyals et al., 2019). Phương pháp của chúng tôi áp dụng cách tiếp cận tự đấu tương tự như AlphaGo Zero, có thể chuyển đổi một mô hình yếu thành một mô hình mạnh mà không cần dữ liệu được chú thích bởi con người bổ sung. Mặc dù hiệu quả của tự đấu trong MARL được thiết lập tốt, theo hiểu biết của chúng tôi, công trình của chúng tôi là công trình đầu tiên áp dụng cách tiếp cận này để nâng cao LLM.

--- TRANG 3 ---
Tinh Chỉnh Tự Đấu Chuyển Đổi Mô Hình Ngôn Ngữ Yếu Thành Mô Hình Ngôn Ngữ Mạnh

Dữ liệu Tổng hợp cho LLM. Trong bối cảnh tinh chỉnh có giám sát (SFT) của LLM, dữ liệu được tạo ra bởi con người đã được chứng minh là một nguồn cực kỳ hiệu quả giúp nâng cao hiệu suất của LLM trong các tác vụ như tạo mã (Roziere et al., 2023; Yang et al., 2023) và lý luận toán học (Yuan et al., 2023; Luo et al., 2023). Mặc dù dữ liệu con người thường thể hiện chất lượng cao, việc thu thập đủ lượng dữ liệu như vậy đặt ra thách thức về chi phí. Dưới ánh sáng của cân nhắc này, việc sử dụng dữ liệu tổng hợp đã trở nên ngày càng phổ biến và được coi là một proxy cho dữ liệu con người. Cách tiếp cận này chủ yếu tận dụng các LLM tiên tiến như dòng GPT (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) làm hướng dẫn để tạo ra dữ liệu chất lượng cao (Josifoski et al., 2023; Taori et al., 2023; Chiang et al., 2023; Li et al., 2023). Nghiên cứu gần đây cũng đã nhấn mạnh khả năng diễn đạt lại của LLM trong việc nhắc nhở để có phản hồi LLM tốt hơn (Deng et al., 2023; Prasad et al., 2023) cũng như việc tăng cường dữ liệu tổng hợp để SFT hiệu quả hơn (Yu et al., 2023; Liu et al., 2023). Trái ngược với các nghiên cứu trước đó sử dụng các mô hình tiên tiến hơn để tạo dữ liệu tổng hợp khi tiền huấn luyện hoặc tinh chỉnh một mô hình đích, cách tiếp cận của chúng tôi trực tiếp tạo ra dữ liệu tổng hợp từ chính mô hình đích.

3 Thiết lập vấn đề và kiến thức cơ bản
Chúng tôi xem xét một Mô hình Ngôn Ngữ Lớn (LLM) được tham số hóa bởi θ và ký hiệu là pθ. Mô hình nhận đầu vào là một chuỗi x = [x1, . . . , xn], thường được gọi là prompt, để tạo ra phản hồi tương ứng y = [y1, . . . , ym]. Do đó, phản hồi y được coi là một mẫu từ phân phối xác suất có điều kiện pθ(·|x). Trong LLM, xi và yj đại diện cho các token riêng lẻ từ một từ vựng được xác định trước trong các chuỗi x và y, tương ứng. Mô hình tự hồi quy pθ tạo ra các token tuần tự cho một vị trí nhất định, chỉ tận dụng chuỗi các token được tạo ra trước đó. Do đó, mô hình này tạo thành một quá trình Markov, trong đó phân phối xác suất có điều kiện pθ(y|x) có thể được biểu diễn thông qua phân tích như sau:

pθ(y|x) = ∏(j=1 to m) pθ(yj|x,y<j),

trong đó y<1 là null và y<j = [y1, . . . , yj−1] cho j = 2, . . . , m. Sau đây, chúng tôi xem xét hai phương pháp tinh chỉnh chính cho LLM: tinh chỉnh có giám sát và tinh chỉnh học tăng cường (RL).

3.1 Tinh Chỉnh Có Giám Sát
Tinh chỉnh có giám sát (SFT) được sử dụng để điều chỉnh một LLM được tiền huấn luyện cho các tác vụ downstream cụ thể, tận dụng bộ dữ liệu tương đối nhỏ hơn của các ví dụ có nhãn so với dữ liệu tiền huấn luyện quy mô lớn (Ouyang et al., 2022; Yu et al., 2023). Trong bối cảnh này, chúng tôi xem xét một tác vụ cụ thể trong đó các prompt, ký hiệu là x, được lấy từ một phân phối xác định q(·). Ký hiệu pdata(·|x) sau đó biểu thị phân phối xác suất của các phản hồi chất lượng cao y liên quan từ dữ liệu huấn luyện. Do đó, SFT bao gồm việc huấn luyện LLM để tối thiểu hóa mất mát log-likelihood âm sau đây liên quan đến các phân phối này,

LSFT(θ) = -Ex∼q(·),y∼pdata(·|x)[log pθ(y|x)].                    (3.1)

Cần lưu ý rằng việc loại trừ x∼q(·) khỏi số hạng kỳ vọng sẽ cho ra mất mát cross-entropy điển hình, được biểu diễn là -Ey∼pdata(·|x)[log pθ(y|x)]. LSFT(θ) đạt minimum khi phân phối dự đoán của mô hình pθ(y|x) phù hợp hoàn hảo với phân phối của các phản hồi chất lượng cao có nhãn pdata(y|x).

Do đó, LLM sau SFT được dự kiến sẽ tạo ra các phản hồi gần giống với những phản hồi từ pdata(y|x). Quy trình này do đó được mong đợi sẽ nâng cao đáng kể hiệu suất của mô hình trong việc tạo ra các phản hồi phù hợp cho một tác vụ cụ thể.

3.2 Tinh Chỉnh RL
Tinh chỉnh RL (Christiano et al., 2017; Bai et al., 2022a; Gao et al., 2023a) cung cấp một phương pháp khác để nâng cao khả năng cụ thể của các mô hình được tiền huấn luyện mục đích tổng quát. Thông thường, tinh chỉnh RL được sử dụng sau SFT để đạt được sự căn chỉnh cải thiện cho LLM (Tunstall et al., 2023a).

Đối với một cặp chuỗi nhất định (x,y), tinh chỉnh RL đòi hỏi một hàm phần thưởng xác định r(x,y). Phần thưởng r(x,y) càng cao, phản hồi y càng tốt đối với prompt x nhất định. Mục tiêu của quá trình tinh chỉnh RL sau đó là tối đa hóa hàm mục tiêu sau:

LRL(θ) = Ex∼q(·),y∼pθ(·|x)[r(x,y)] - λEx∼q(·)[KL(pθ(·|x)||pref(·|x))],

trong đó chính quy hóa Kullback-Leibler (KL) buộc mô hình mới pθ phải gần với mô hình tham chiếu pref, và λ > 0 là tham số chính quy hóa để kiểm soát độ lệch của mô hình mới pθ so với mô hình tham chiếu pref. Trong thực tế, mô hình tham chiếu pref thường được khởi tạo như mô hình được tinh chỉnh có giám sát. Việc bao gồm chính quy hóa KL là quan trọng để ngăn chặn độ lệch quá mức so với mô hình tham chiếu, từ đó giảm nguy cơ sụp đổ mode.

Trong khi đó, thách thức chính trong tinh chỉnh RL nằm ở việc tìm một hàm phần thưởng tốt. Thông thường, hàm này đòi hỏi huấn luyện trên một bộ dữ liệu sở thích. Việc biên soạn bộ dữ liệu như vậy đòi hỏi tài nguyên đáng kể, thường bao gồm các đánh giá toàn diện bởi những người chú thích con người, tức là học tăng cường từ phản hồi con người (RLHF) (Christiano et al., 2017; Bai et al., 2022a) hoặc các tác nhân AI mạnh, tức là học tăng cường từ phản hồi AI (RLAIF) (Bai et al., 2022b).

4 Phương pháp
Trong phần này, chúng tôi giới thiệu một phương pháp tinh chỉnh mới để nâng cao hiệu suất của LLM mà không dựa vào phản hồi con người hoặc AI bổ sung. Xem xét một bộ dữ liệu tinh chỉnh có giám sát (SFT) chất lượng cao SSFT={(x,y)}ni=1, được lấy mẫu từ phân phối biên q(x) và pdata(y|x). Cho một LLM được tinh chỉnh có giám sát pθ0, việc áp dụng tiếp cách tiếp cận SFT trong (3.1) với SSFT sẽ không hiệu quả và có thể dẫn đến hiệu suất tệ hơn. Ngoài ra, không có phản hồi con người và/hoặc AI, việc thu thập một bộ dữ liệu sở thích cho tinh chỉnh RL (ví dụ, RLHF và RLAIF) trở nên không khả thi. Điều này cản trở việc áp dụng các kỹ thuật tinh chỉnh RL.

Chúng tôi đánh giá pθ0 so với SSFT, trong đó pθ0 là LLM đạt được bằng SFT sử dụng (3.1). Chúng tôi nhận thấy một khoảng cách chất lượng liên tục giữa phản hồi ground truth y từ SSFT và phản hồi được tạo ra bởi LLM y′∼pθ(·|x) (tham khảo Hình 1). Sự khác biệt này cho thấy vẫn còn chỗ để cải thiện so với pθ0. Do đó, chúng tôi đề xuất sử dụng dữ liệu tổng hợp được tạo ra bởi LLM để nâng cao hiệu suất của LLM bắt đầu từ pθ0 một cách lặp lại.

4.1 Tinh Chỉnh Tự Đấu (SPIN)
Hãy xem xét một trò chơi hai người, trong đó mục tiêu của người chơi chính là phân biệt các phản hồi được tạo ra bởi LLM và những phản hồi được tạo ra bởi con người. Trong khi đó, vai trò của đối thủ là tạo ra các phản hồi không thể phân biệt với các phản hồi của con người. Trọng tâm của phương pháp chúng tôi là cơ chế tự đấu, trong đó cả người chơi chính và đối thủ đều là cùng một LLM, nhưng từ các lần lặp khác nhau. Cụ thể hơn, đối thủ là LLM cũ từ lần lặp trước, và người chơi chính là LLM mới được học trong lần lặp hiện tại.

Trong lần lặp t+1, đối thủ là LLM từ lần lặp trước, ký hiệu là pθt, tạo ra các phản hồi y′ cho những prompt x trong bộ dữ liệu SFT theo pθt(·|x). Do đó, phương pháp của chúng tôi bao gồm hai bước sau tại lần lặp t+1: (1) huấn luyện người chơi chính, và (2) cập nhật người chơi đối thủ.

Huấn luyện Người Chơi Chính. Chúng tôi bắt đầu bằng cách minh họa cách chúng tôi mong đợi một người chơi chính được huấn luyện để phân biệt các phản hồi LLM với các phản hồi con người. Được thúc đẩy bởi integral probability metric (IPM) (Müller, 1997), chúng tôi xây dựng hàm mục tiêu sao cho người chơi chính ft+1 tối đa hóa khoảng cách giá trị kỳ vọng giữa phân phối dữ liệu đích pdata và phân phối của người chơi đối thủ pθt:

ft+1 = arg max f∈Ft E[f(x,y) - f(x,y′)],                    (4.1)

trong đó kỳ vọng được tính trên các phân phối x∼q(·), y∼pdata(·|x), y′∼pθt(·|x), và Ft là một chuỗi các lớp hàm có tính biểu diễn cao mà chúng tôi sẽ xác định trong suy luận sau. Chỉ số dưới t trong Ft là do lớp hàm phụ thuộc vào pθt. Cho một ft+1 như vậy và một chuỗi phản hồi y cho prompt x, giá trị của ft+1(x,y) phản ánh mức độ tin tưởng của người chơi chính rằng y có nguồn gốc từ pdata hơn là pθt. Lý tưởng nhất, người chơi chính ft+1 nên cho ra giá trị cao khi y∼pdata(·|x) và giá trị thấp khi y′∼pθt(·|x), trong đó pθt là phân phối của đối thủ. Thay vì giải quyết (4.1), chúng ta cũng có thể giải quyết bài toán tối ưu tổng quát hơn sau đây,

ft+1 = arg min f∈Ft E[ℓ(f(x,y) - f(x,y′))],                    (4.2)

trong đó kỳ vọng được tính trên phân phối x∼q(·), y∼pdata(·|x), y′∼pθt(·|x), và ℓ(·) là một hàm mất mát vừa giảm đơn điệu vừa lồi. Ví dụ, một hàm mất mát tuyến tính ℓ(t) = -t giảm (4.2) xuống phiên bản tối thiểu của (4.1). Tuy nhiên, việc sử dụng một hàm mất mát tuyến tính dẫn đến giá trị mục tiêu không bị chặn, trong quá trình huấn luyện liên tục, dẫn đến giá trị âm vô hạn của f(x,y′) trên các phản hồi của người chơi đối thủ. Do đó, trong công trình của chúng tôi, chúng tôi chọn hàm mất mát logistic ℓ(t) := log(1+exp(-t)) vì tính không âm, mượt mà, và đuôi suy giảm theo cấp số nhân khi t→∞. Việc chọn hàm mất mát như vậy giúp ngăn chặn sự tăng trưởng quá mức về giá trị tuyệt đối của f.

Cập nhật Người Chơi Đối Thủ. Trước đây chúng tôi đã thảo luận về việc huấn luyện ft+1 cho phân phối pθt của người chơi đối thủ. Bây giờ giả sử chúng tôi đã tối ưu hóa người chơi chính ft+1 có thể phân biệt pdata từ pθt, trong một lớp hàm Ft nhất định, chúng tôi trình bày chi tiết cách chúng tôi có được tham số θt+1 của người chơi đối thủ. Cụ thể, khi được trình bày với hai phản hồi y và y′ cho cùng một prompt x, ft+1 đánh giá các giá trị ft+1(x,y) và ft+1(x,y′). Sau đó nó suy ra rằng phản hồi có giá trị cao hơn là từ phân phối dữ liệu thực pdata và phản hồi có giá trị thấp hơn được quy cho LLM pθt. Tiếp theo, mục tiêu của người chơi đối thủ là tìm một LLM tốt hơn tạo ra các phản hồi không thể phân biệt từ pdata đối với người chơi chính. Điều này được thực hiện bằng cách tối đa hóa giá trị kỳ vọng Ex∼q(·),y∼p(·|x)[ft+1(x,y)]. Ngoài ra, để ngăn chặn độ lệch quá mức của pθt+1 so với pθt và ổn định tự đấu, chúng tôi kết hợp một số hạng chính quy hóa Kullback-Leibler (KL). Kết hợp tất cả những điều này tạo ra bài toán tối ưu sau:

arg max p Ex∼q(·),y∼p(·|x)[ft+1(x,y)] - λEx∼q(·)[KL(p(·|x)||pθt(·|x))],        (4.3)

trong đó λ > 0 là tham số chính quy hóa. Đáng chú ý, (4.3) có một nghiệm dạng đóng p̂(·|x):

p̂(y|x) ∝ pθt(y|x) exp(λ^(-1)ft+1(x,y)).                    (4.4)

Đáng chú ý rằng p̂(·|x) không được đảm bảo thuộc về không gian LLM {pθ(·|x)|θ∈Θ}. Vì chúng tôi hy vọng rằng nghiệm dạng đóng p̂ trong không gian xác suất có thể được thực hiện bởi một LLM với tham số θ, tức là pθ(y|x) = p̂(y|x), giải quyết cho pθ(y|x) ∝ pθt(y|x) exp(λ^(-1)ft+1(x,y)) cho ta ft+1(x,y) = λ·log(pθ(·|x)/pθt(·|x)). Điều này gợi ý lớp hàm Ft sau đây cho ft+1:

Ft = {λ·log(pθ(y|x)/pθt(y|x)) | θ∈Θ},                    (4.5)

trong đó Θ là không gian tham số của LLM được xem xét. Cho việc chọn Ft trong (4.5), tối ưu hóa (4.2) cho ft+1 được tham số hóa bởi θt+1 dưới dạng sau:

ft+1(x,y) = λ·log(pθt+1(y|x)/pθt(y|x)).                    (4.6)

Thay thế (4.6) vào (4.4) cho ta p̂(y|x) = pθt+1(y|x). Nói cách khác, θt+1 được học từ (4.2) chính xác là tham số LLM cho việc chọn đối thủ lý tưởng của chúng tôi.

Mục tiêu Huấn luyện End-to-end. Chúng tôi tích hợp hai bước đã thảo luận trước đây thành một mục tiêu huấn luyện end-to-end duy nhất với quy tắc cập nhật của θt+1. Cụ thể, việc thay thế (4.5) vào (4.2) đạt được quy tắc cập nhật θt+1 = arg min θ∈Θ LSPIN(θ,θt), trong đó LSPIN là mục tiêu huấn luyện được định nghĩa như sau:

LSPIN = E[ℓ(λlog(pθ(y|x)/pθt(y|x)) - λlog(pθ(y′|x)/pθt(y′|x)))],        (4.7)

trong đó kỳ vọng được tính trên phân phối x∼q(·), y∼pdata(·|x), y′∼pθt(·|x). Chúng tôi tóm tắt quá trình tự đấu lặp lại của phương pháp SPIN như sau:

... → pθt(·|x) (Người chơi đối thủ tại t) → λ·log(pθt+1(·|x)/pθt(·|x)) (Người chơi chính tại t+1) → pθt+1(·|x) (Người chơi đối thủ tại t+1) → ...

Cụ thể, người chơi đối thủ được chọn từ lần lặp trước t được sử dụng để huấn luyện người chơi chính tại lần lặp t+1, dẫn đến LLM được tham số hóa bởi θt+1. Sau đó chúng tôi xác định người chơi đối thủ tiếp theo tại lần lặp t+1 bằng cách trực tiếp sao chép tham số LLM θt+1, sau đó được sử dụng trong việc huấn luyện người chơi chính tại lần lặp t+2. Thuật toán chi tiết được trình bày trong Thuật toán 1.

4.2 So sánh giữa SPIN và DPO
Trong Phần 4.1, chúng tôi đề xuất Self-Play Fine-Tuning (SPIN) với mục tiêu huấn luyện end-to-end (4.7) cho mỗi lần lặp. (4.7) có sự tương đồng với tối ưu hóa sở thích trực tiếp (DPO) (Rafailov et al., 2023) cho tinh chỉnh RL. Tuy nhiên, SPIN và DPO khác nhau về cơ bản.

DPO dựa trên mô hình Bradley-Terry (BT): p(y1≻y2|x) = exp(r*(x,y1))/(exp(r*(x,y1))+exp(r*(x,y2))), và tối đa hóa log-likelihood của p(y1≻y2|x) bằng tối ưu hóa chính sách trực tiếp mà không cần ước lượng phần thưởng rõ ràng. Ngược lại, SPIN dựa vào việc tối đa hóa IPM để cạnh tranh với một phiên bản ngày càng mạnh hơn của chính nó. Các so sánh chi tiết hơn được nêu bật như sau:

1. DPO không vốn dĩ dẫn đến huấn luyện lặp lại. Cụ thể hơn, DPO nhằm mục đích khớp xác suất sở thích p(y1≻y2|x) được tạo ra từ mô hình phần thưởng của nó với phân phối dữ liệu pdata(y1≻y2|x) trong một lần lặp duy nhất. Ngược lại, cơ chế tự đấu của SPIN tự nhiên dẫn đến một quy trình huấn luyện lặp lại. SPIN lặp lại tinh chỉnh phân phối tạo sinh pθ(y|x) để khớp với phân phối đích pdata(y|x) qua các lần lặp.

2. SPIN chỉ yêu cầu bộ dữ liệu SFT, được biểu diễn bởi các cặp (x,y). Ngược lại, DPO đòi hỏi một bộ dữ liệu sở thích, được biểu diễn bởi (x,yw,yl), trong đó yw và yl biểu thị các phản hồi thắng (được chọn) và thua (bị từ chối), tương ứng. Hơn nữa, SPIN có thể được áp dụng giữa SFT và tinh chỉnh RL.

3. Trong SPIN, chúng tôi có thể chọn các hàm mất mát ℓ khác nhau chỉ cần lồi và giảm (chi tiết sau trong Định lý 5.2), bao gồm mất mát tương quan, mất mát hinge, và mất mát logistic. Chỉ khi ℓ được chọn là mất mát logistic thì mục tiêu huấn luyện của SPIN mới trở nên tương tự như DPO.

Gần đây, Xu et al. (2023) đề xuất sử dụng tối ưu hóa sở thích lặp lại với Pairwise Cringe Loss (PCO), và tổng quát hóa DPO thành DPO lặp lại. Đồng thời với công trình của chúng tôi, Yuan et al. (2024) tiếp tục đề xuất một framework có tên "self-rewarding language models", tận dụng chính LLM làm mô hình phần thưởng để cung cấp phản hồi sở thích, và sử dụng DPO lặp lại để huấn luyện LLM. So với Xu et al. (2023); Yuan et al. (2024), việc tự đánh giá của SPIN là ngầm, vì không cần phần thưởng trung gian hoặc phản hồi sở thích.

5 Phân tích lý thuyết
Trong phần này, chúng tôi cung cấp một phân tích lý thuyết cho Thuật toán 1 trong Phần 4. Dưới giả định tính đơn điệu và tính lồi của hàm mục tiêu ℓ, chúng tôi chỉ ra rằng tối ưu toàn cục được thu được khi và chỉ khi tham số θt tạo ra phân phối dữ liệu. Chúng tôi tóm tắt các giả định của mình như sau:

Giả định 5.1. Hàm mất mát ℓ(t) : R → R giảm đơn điệu, tức là ∀t, ℓ′(t) ≤ 0 và thỏa mãn ℓ′(0) < 0. Ngoài ra, ℓ(t) là một hàm lồi.

Giả định 5.1 đúng cho một loạt rộng các hàm mất mát thường được sử dụng trong học máy, bao gồm mất mát tương quan ℓ(t) = 1 - t, mất mát hinge ℓ(t) = max(0, 1 - t), mất mát mũ ℓ(t) = exp(-t) và mất mát logistic ℓ(t) = log(1 + exp(-t)). Dưới Giả định 5.1, chúng tôi trình bày định lý sau, quan trọng trong việc hiểu động lực tối ưu của phương pháp chúng tôi.

Định lý 5.2. Dưới Giả định 5.1, giả sử tồn tại pθ(·|x) = pdata(·|x), thì chúng ta có:
• (Tính đủ) Nếu pθt(·|x) = pdata(·|x), thì θt là minimum toàn cục của (4.7) cho bất kỳ λ ≥ 0.
• (Tính cần thiết) Nếu pθt(·|x) ≠ pdata(·|x), tồn tại một λ được chọn phù hợp, sao cho θt không phải là minimum toàn cục của (4.7).

Nhận xét 5.3. Định lý 5.2 gợi ý rằng dưới những điều kiện nhất định, quá trình tối ưu của phương pháp chúng tôi tự nhiên dừng lại tại điểm pθ(·|x) = pdata(·|x), ngụ ý hiệu quả của cách tiếp cận chúng tôi trong việc căn chỉnh phân phối của LLM với phân phối dữ liệu đích. Hơn nữa, Định lý 5.2 cũng chỉ ra rằng quá trình tối ưu chỉ dừng lại khi tính tối ưu toàn cục được đạt được, tức là phân phối của LLM căn chỉnh với phân phối dữ liệu đích.

Đối với hàm mất mát logistic ℓ(t) = log(1 + exp(-t)), định lý sau đây đưa ra một đặc tính chính xác hơn về người chơi đối thủ, cho phép hiểu rõ hơn về SPIN.

Định lý 5.4. Xem xét việc chọn mất mát logistic ℓ(t) = log(1 + exp(-t)) trong SPIN. Giả sử rằng pθt(y|x)(pdata(y|x)/pθt(y|x))^(1/λ) nằm trong không gian LLM {pθ(y|x)|θ∈Θ} và θt+1 là minimum toàn cục của LSPIN(θ,θt), thì người chơi đối thủ tại lần lặp t+1 thỏa mãn:

pθt+1(y|x) ∝ pθt(y|x)(pdata(y|x)/pθt(y|x))^(1/λ).

Nhận xét 5.5. Theo Định lý 5.4, việc cập nhật mô hình từ pθt(y|x) sang pθt+1(y|x) có xu hướng tăng xác suất pθt+1(y|x) khi pθt(y|x) nhỏ hơn pdata(y|x), và giảm nó khi pθt(y|x) lớn hơn pdata(y|x). Do đó, Định lý 5.4 tiếp tục xác nhận rằng quá trình tối ưu của phương pháp chúng tôi tự nhiên hội tụ về điểm mà pθ(·|x) bằng pdata(·|x). Việc cập nhật người chơi đối thủ được kiểm soát bởi (pdata(y|x)/pθt(y|x))^(1/λ), được điều chỉnh bởi hệ số 1/λ. Một λ nhỏ hơn dẫn đến một thay đổi lớn hơn của người chơi đối thủ, trong khi một λ lớn hơn dẫn đến một thay đổi nhỏ hơn. Do đó, khi pθ(·|x) tiến gần pdata(·|x), việc tăng λ tăng cường sự ổn định của việc huấn luyện LLM. Quan sát này phù hợp với (4.3), trong đó λ là tham số chính quy hóa của chính quy hóa KL được sử dụng để kiểm soát độ lệch của người chơi đối thủ.

6 Thực nghiệm
Phần này cung cấp một phân tích thực nghiệm chi tiết về SPIN. Các phát hiện của chúng tôi nêu bật một số điểm chính: (1) SPIN nâng cao đáng kể hiệu suất mô hình trên một loạt rộng các bộ chuẩn đánh giá bằng cách phá vỡ giới hạn của SFT; (2) ngay cả khi không đưa vào dữ liệu được chú thích mới bởi con người, SPIN tại lần lặp 0 đạt được hiệu suất ngang bằng với huấn luyện DPO sử dụng thậm chí nhiều dữ liệu hơn; (3) huấn luyện lặp lại là một thành phần cần thiết trong SPIN vì nó phá vỡ giới hạn của huấn luyện đa epoch.

6.1 Thiết lập thực nghiệm
Mô hình và Bộ dữ liệu. Trong nghiên cứu này, chúng tôi áp dụng zephyr-7b-sft-full làm mô hình cơ sở. Mô hình này bắt nguồn từ Mistral-7B được tiền huấn luyện (Jiang et al., 2023) và đã được tinh chỉnh thêm trên bộ dữ liệu SFT Ultrachat200k¹ bởi HuggingFace. Ultrachat200k đại diện cho một tập con chất lượng cao 200k của corpus UltraChat lớn hơn (Ding et al., 2023), bao gồm khoảng 1.4M cuộc đối thoại được tạo ra bằng các API Turbo của OpenAI. Từ UltraChat200k, chúng tôi lấy mẫu ngẫu nhiên 50k prompt và sử dụng mô hình cơ sở để tạo ra các phản hồi tổng hợp. Sau đó chúng tôi tuân theo phương pháp tối ưu được mô tả trong Phần 4.1 để huấn luyện thêm. Trong nhiều lần lặp, chúng tôi tận dụng dữ liệu tổng hợp từ lần lặp gần nhất và thêm vào dữ liệu tổng hợp mới được tạo ra, do đó dẫn đến kích thước bộ dữ liệu tổng hợp là 50k tại lần lặp 0 và 100k tại lần lặp 1, 2 và 3. Tại mỗi lần lặp, chúng tôi huấn luyện mô hình của mình trong 2 epoch.

Đánh giá. Chúng tôi sử dụng Huggingface Open LLM Leaderboard được sử dụng rộng rãi (Beeching et al., 2023) làm bộ chuẩn đánh giá, sử dụng cùng thư viện Language Model Evaluation Harness (Gao et al., 2023b). Bảng xếp hạng này bao gồm 6 bộ dữ liệu khác nhau, mỗi bộ tập trung vào một khả năng cụ thể của LLM. Tổng hợp lại, các bộ dữ liệu này cung cấp một framework đánh giá toàn diện, đánh giá LLM về lý luận thông thường (Arc (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021)), hiểu biết ngôn ngữ đa nhiệm vụ (MMLU (Hendrycks et al., 2020)), bắt chước giả mạo của con người (TruthfulQA (Lin et al., 2021)) và giải quyết vấn đề toán học (GSM8k (Cobbe et al., 2021)). Chúng tôi để lại các chi tiết triển khai tiếp theo cho Phụ lục B với thiết lập đánh giá chi tiết được áp dụng bởi cả bảng xếp hạng và các thực nghiệm của chúng tôi.

6.2 SPIN Cải thiện Hiệu suất Chuẩn một cách Hiệu quả

[Hình 2 và 3 được mô tả trong văn bản]

Trong Hình 2, chúng tôi chứng minh hiệu quả của SPIN sử dụng HuggingFace Open LLM Leaderboard làm đánh giá. Trong Hình 3, chúng tôi so sánh hiệu suất của mô hình được tinh chỉnh bởi SPIN sau các lần lặp 0 đến 3 với mô hình cơ sở zephyr-7b-sft-full trên mỗi tác vụ được bao gồm trong bảng xếp hạng. Hiệu suất chi tiết được trình bày trong Bảng 4 trong Phụ lục B. Chúng tôi có thể quan sát rằng SPIN thể hiện hiệu quả đáng kể trong việc cải thiện hiệu suất của mô hình bằng cách tận dụng thêm bộ dữ liệu SFT, mà mô hình cơ sở đã được tinh chỉnh đầy đủ trước đó. Tại lần lặp 0, nơi các phản hồi mô hình được tạo ra từ zephyr-7b-sft-full, chúng tôi quan sát một cải thiện tổng thể 2.66% về điểm trung bình. Sự cải thiện đặc biệt đáng kể trên các bộ chuẩn TruthfulQA và GSM8k, với cải thiện vượt quá 5% và 10% tương ứng. Tại lần lặp 1, chúng tôi sử dụng mô hình LLM từ lần lặp 0 để tạo ra các phản hồi mới cho SPIN, tuân theo quy trình được nêu trong Thuật toán 1. Lần lặp này mang lại sự nâng cao tiếp theo 1.32% trung bình, và đặc biệt đáng kể trên các bộ chuẩn Arc Challenge và TruthfulQA. Các lần lặp tiếp theo tiếp tục xu hướng cải thiện dần dần này trên các tác vụ khác nhau. Trong khi đó, sự cải thiện tại lần lặp t+1 tự nhiên nhỏ hơn so với lần lặp t. Khi quá trình huấn luyện lặp lại tiến triển, mức độ cải thiện dần dần tiến gần về không, gợi ý rằng mô hình đã đạt đến một điểm giới hạn trong lần lặp cuối cùng.

So sánh với DPO. zephyr-7b-beta là một mô hình được tạo ra từ zephyr-7b-sft-full, được huấn luyện với DPO trên khoảng 62k dữ liệu sở thích. Dữ liệu này, bộ dữ liệu UltraFeedback Binarized (Cui et al., 2023)², bao gồm cả các completion được chọn và bị từ chối được đánh giá bởi GPT-4. Chúng tôi lưu ý rằng, DPO yêu cầu đầu vào con người hoặc phản hồi mô hình ngôn ngữ tiên tiến để xác định sở thích, làm cho việc tạo dữ liệu trở thành một quy trình khá đắt đỏ. Ngược lại, SPIN của chúng tôi chỉ yêu cầu chính mô hình ban đầu. Hơn nữa, không giống như DPO yêu cầu nguồn dữ liệu mới, phương pháp của chúng tôi độc quyền tận dụng bộ dữ liệu SFT hiện có. Trong Hình 3, chúng tôi hiển thị so sánh hiệu suất của SPIN tại các lần lặp 0 và 1 (sử dụng 50k dữ liệu SFT) với huấn luyện DPO, từ cùng một checkpoint SFT. Chúng tôi có thể quan sát rằng, trong khi DPO tận dụng nhiều dữ liệu hơn từ các nguồn mới, SPIN dựa trên dữ liệu SFT hiện có đã có thể đạt được hiệu suất trung bình tương đương với huấn luyện DPO tại lần lặp 0. Từ lần lặp 1, SPIN thậm chí vượt trội hơn hiệu suất của DPO trên bộ chuẩn bảng xếp hạng.

6.3 Nghiên cứu phân tích
Trong phần này, chúng tôi kiểm tra tác động của kích thước bộ dữ liệu tổng hợp và số epoch huấn luyện trong một lần lặp. Phân tích của chúng tôi chứng minh hiệu quả của dữ liệu tổng hợp được sử dụng bởi SPIN so với dữ liệu SFT, cũng như sự cần thiết của huấn luyện lặp lại trong SPIN. Trong Phụ lục B, chúng tôi trình bày đánh giá của SPIN trên các tác vụ chuẩn bổ sung.

Kích thước Huấn luyện. Chúng tôi điều tra tác động của việc thay đổi kích thước dữ liệu huấn luyện đối với hiệu suất của SPIN. Trong Hình 5, chúng tôi chứng minh tác động của kích thước huấn luyện cho SPIN trong lần lặp 0 và bổ sung so sánh với SFT với bộ dữ liệu gốc đầy đủ. Cụ thể, đối với đường cơ sở SFT, chúng tôi tinh chỉnh đầy đủ Mistral-7B trên Ultrachat200k trong ba epoch và báo cáo hiệu suất epoch đầu tiên như điểm khởi đầu (với trục x 0) trong hình cho SFT. Đối với SPIN, chúng tôi báo cáo checkpoint zephyr-7b-sft-full như điểm khởi đầu, cũng đã được tinh chỉnh trên Ultrachat200k trong một epoch. Chúng tôi chọn kích thước huấn luyện của SPIN tại lần lặp 0 là 14k, 26k, và 50k và tạo ra dữ liệu tương ứng, đảm bảo rằng bộ dữ liệu lớn hơn bao gồm bộ dữ liệu nhỏ hơn. Hiệu suất của SPIN sau đó được đánh giá sau 1 epoch tinh chỉnh tự đấu cho mỗi kích thước huấn luyện. Chúng tôi có thể quan sát rằng, trong khi SPIN dẫn đến cải thiện đáng chú ý với kích thước huấn luyện tăng, SFT trên các epoch tiếp theo 2 và 3 không thể mang lại cải thiện hơn 1%. Kết quả bổ sung được trì hoãn đến Phụ lục B.

Huấn luyện Lặp lại so với Huấn luyện trong Nhiều Epoch. Chúng tôi tiếp tục nghiên cứu việc huấn luyện trong lần lặp 0 và so sánh với hiệu suất đạt được trong lần lặp 1, đặc biệt đối chiếu hiệu suất kiểm tra thu được từ thời gian huấn luyện kéo dài với hiệu suất từ lần lặp tiếp theo. Hình 4 mô tả quỹ đạo hiệu suất của mô hình được huấn luyện bằng SPIN qua nhiều epoch tại lần lặp 0. Rõ ràng là cải thiện đáng kể nhất xảy ra trong hai epoch đầu tiên, tiếp theo là chỉ những cải thiện khiêm tốn trong các epoch tiếp theo. Đáng chú ý, SPIN thể hiện sự mạnh mẽ và ổn định; việc mở rộng thời gian huấn luyện không làm giảm hiệu suất mà thay vào đó duy trì một mức độ khá nhất quán. Tuy nhiên, quan sát cho thấy một giới hạn vốn có đối với hiệu suất có thể đạt được trong một lần lặp duy nhất, từ đó nhấn mạnh sự cần thiết cho việc huấn luyện lặp lại. Như được hiển thị bởi hiệu suất kiểm tra đạt được tại lần lặp 1 trong các hình, việc mở rộng huấn luyện trong lần lặp 0 không thể đạt được hiệu suất tương đương với lần lặp 1.

7 Kết luận và Thảo luận
Bài báo này giới thiệu một phương pháp tinh chỉnh mới SPIN, để chuyển đổi một LLM yếu thành một LLM mạnh bằng cách phát huy toàn bộ sức mạnh của dữ liệu được chú thích bởi con người. Trung tâm của phương pháp này là một cơ chế tự đấu, trong đó một người chơi chính (LLM) được tinh chỉnh để phân biệt các phản hồi của người chơi đối thủ (LLM từ lần lặp trước) với phân phối dữ liệu đích, và LLM được căn chỉnh lặp lại với phân phối dữ liệu đích. Do đó, SPIN tạo điều kiện cho việc tự đánh giá và nâng cao lặp lại của LLM thông qua tự đấu. So với các phương pháp tinh chỉnh có giám sát và tinh chỉnh RL, SPIN cho phép LLM tự cải thiện mà không cần dữ liệu con người bổ sung hoặc phản hồi từ các LLM mạnh hơn. Kết quả thực nghiệm chứng minh rằng SPIN nâng cao đáng kể hiệu suất LLM trên các bộ chuẩn đa dạng, thậm chí vượt trội hơn các mô hình được huấn luyện với dữ liệu con người bổ sung hoặc phản hồi AI.

Hạn chế và Công việc Tương lai. Kết quả lý thuyết của chúng tôi chứng minh rằng quá trình tối ưu của SPIN hội tụ khi và chỉ khi phân phối của LLM căn chỉnh với pdata. Do đó, nghiên cứu của chúng tôi tập trung vào một phân phối dữ liệu đích cố định được tạo ra bởi con người, vốn dĩ áp đặt một trần về hiệu suất của LLM được tinh chỉnh. Khám phá phân phối dữ liệu đích thay đổi động là một hướng quan trọng để vượt qua giới hạn này và nâng cao hiệu suất của LLM vượt ra ngoài trần này hoặc thậm chí đến mức siêu con người. Hơn nữa, xem xét yêu cầu tài nguyên của việc tạo dữ liệu tổng hợp, một con đường hứa hẹn khác để khám phá thêm là giảm khối lượng dữ liệu tổng hợp cần thiết.

Lời cảm ơn
Chúng tôi cảm ơn các nhà phản biện ẩn danh và chủ tịch khu vực vì những nhận xét hữu ích của họ. ZC, YD, HY, KJ, và QG được hỗ trợ một phần bởi Giải thưởng CAREER của Quỹ Khoa học Quốc gia 1906169, IIS-2008981, CHE-2247426 và Học bổng Nghiên cứu Sloan. Các quan điểm và kết luận được chứa trong bài báo này thuộc về các tác giả và không nên được diễn giải là đại diện cho bất kỳ cơ quan tài trợ nào.

[Phần còn lại của tài liệu bao gồm các bảng biểu, hình ảnh, tài liệu tham khảo và phụ lục sẽ được tiếp tục dịch theo cùng định dạng...]
