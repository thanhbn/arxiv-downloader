# 2309.08513.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/peft/2309.08513.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1301605 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
Báº£n tháº£o khÃ´ng tÃªn sá»‘
(sáº½ Ä‘Æ°á»£c chÃ¨n bá»Ÿi biÃªn táº­p viÃªn)
SCT: Má»™t PhÆ°Æ¡ng PhÃ¡p ÄÆ¡n Giáº£n cho Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘
thÃ´ng qua CÃ¡c KÃªnh Ná»•i Báº­t
Henry Hengyuan Zhao Â·Pichao Wang Â·Yuyang Zhao Â·Hao Luo Â·Fan
Wang Â·Mike Zheng Shou
Nháº­n: ngÃ y / Cháº¥p nháº­n: ngÃ y
TÃ³m táº¯t CÃ¡c vision transformer Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ lá»£i Ã­ch biá»ƒu diá»…n máº¡nh máº½ cho nhiá»u tÃ¡c vá»¥ downstream khÃ¡c nhau.
Gáº§n Ä‘Ã¢y, nhiá»u phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT) Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t, vÃ  cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng chá»©ng minh ráº±ng viá»‡c chá»‰nh chá»‰ 1% tham sá»‘ bá»• sung cÃ³ thá»ƒ vÆ°á»£t qua tinh chá»‰nh Ä‘áº§y Ä‘á»§ trong cÃ¡c tÃ¬nh huá»‘ng tÃ i nguyÃªn dá»¯ liá»‡u tháº¥p. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y bá» qua thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ khi tinh chá»‰nh cÃ¡c tÃ¡c vá»¥ downstream Ä‘a dáº¡ng. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ Ä‘Æ°á»£c gá»i lÃ  "Tinh Chá»‰nh KÃªnh Ná»•i Báº­t" (SCT) Ä‘á»ƒ táº­n dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ báº±ng cÃ¡ch chuyá»ƒn tiáº¿p mÃ´ hÃ¬nh vá»›i cÃ¡c hÃ¬nh áº£nh tÃ¡c vá»¥ Ä‘á»ƒ chá»n cÃ¡c kÃªnh má»™t pháº§n trong báº£n Ä‘á»“ Ä‘áº·c trÆ°ng cho phÃ©p chÃºng tÃ´i chá»‰ chá»‰nh chá»‰ 1/8 kÃªnh dáº«n Ä‘áº¿n chi phÃ­ tham sá»‘ tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ. CÃ¡c thÃ­ nghiá»‡m trÃªn 19 tÃ¡c vá»¥ downstream há»c chuyá»ƒn giao thá»‹ giÃ¡c chá»©ng minh ráº±ng SCT cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ trÃªn 18 trong sá»‘ 19 tÃ¡c vá»¥ báº±ng cÃ¡ch chá»‰ thÃªm 0.11M tham sá»‘ cá»§a ViT-B, Ã­t hÆ¡n 780 Ã— so vá»›i
Henry Hengyuan Zhao
Show Lab, Äáº¡i há»c Quá»‘c gia Singapore, Singapore
E-mail: hengyuan.z@u.nus.edu
Pichao Wang
Alibaba Group, Hoa Ká»³
E-mail: pichaowang@gmail.com
Yuyang Zhao
Äáº¡i há»c Quá»‘c gia Singapore, Singapore
E-mail: yuyang.zhao@u.nus.edu
Hao Luo
Alibaba Group, Trung Quá»‘c
E-mail: michuan.lh@alibaba-inc.com
Fan Wang
Alibaba Group, Hoa Ká»³
E-mail: fan.w@alibaba-inc.com
Mike Zheng Shou (TÃ¡c giáº£ tÆ°Æ¡ng á»©ng)
Show Lab, Äáº¡i há»c Quá»‘c gia Singapore, Singapore
E-mail: mike.zheng.shou@gmail.com
SCT Adapter SSF LoRA NOAH VPT0.10.20.30.40.50.6# Params(M)
0.110.160.240.290.430.53
SCT Adapter SSF LoRA NOAH VPT697071727374Äá»™ ChÃ­nh XÃ¡c Top-173.6
71.372.6
72.373.3
69.4HÃ¬nh 1: So sÃ¡nh tham sá»‘ vÃ  Ä‘á»™ chÃ­nh xÃ¡c top-1 trÃªn benchmark VTAB-1K vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p baseline khÃ¡c nhau.
ChÃºng tÃ´i chá»‰ chá»‰nh chá»‰ 96 kÃªnh trong 768 kÃªnh cá»§a ViT-B/16, Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c.
Ä‘á»‘i tÃ¡c tinh chá»‰nh Ä‘áº§y Ä‘á»§ cá»§a nÃ³. HÆ¡n ná»¯a, cÃ¡c thÃ­ nghiá»‡m vá» khÃ¡i quÃ¡t hÃ³a miá»n vÃ  phÃ¢n loáº¡i few-shot tiáº¿p tá»¥c chá»©ng minh tÃ­nh hiá»‡u quáº£ vÃ  tá»•ng quÃ¡t cá»§a phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i. MÃ£ nguá»“n cÃ³ sáºµn táº¡i
https://github.com/showlab/SCT

1 Giá»›i thiá»‡u
CÃ¡c vision transformer lá»›n (ViT) Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ trong cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c mÃ¡y tÃ­nh (Dosovitskiy
et al., 2020; Liu et al., 2021; Yuan et al., 2022; Zhou
et al., 2021a; Carion et al., 2020; Li et al., 2021; Strudel
et al., 2021) báº±ng cÃ¡ch sá»­ dá»¥ng dá»¯ liá»‡u huáº¥n luyá»‡n quy mÃ´ lá»›n, nhÆ° ImageNet21K vÃ  JFT-300M, Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n máº¡nh máº½. Tuy nhiÃªn, cÃ¡c tÃ¡c vá»¥ nháº­n dáº¡ng downstream thÆ°á»ng thiáº¿u Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n tá»« Ä‘áº§u. Do Ä‘Ã³, viá»‡c chuyá»ƒn giao kiáº¿n thá»©c tá»« cÃ¡c mÃ´ hÃ¬nh ViT Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ khÃ³ trong huáº¥n luyá»‡n vÃ  táº¡o ra káº¿t quáº£ Ä‘áº§y há»©a háº¹n. Tinh chá»‰nh Ä‘áº§y Ä‘á»§ end-to-end lÃ  má»™t cÃ¡ch Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ káº¿ thá»«a nhá»¯ng biá»ƒu diá»…n máº¡nh máº½ nÃ y, nhÆ°ng nÃ³ pháº£i Ä‘á»‘i máº·t vá»›i hai thÃ¡ch thá»©c. Thá»© nháº¥t, cÃ¡c mÃ´ hÃ¬nh lá»›n dá»… bá»‹ overfitting khi chá»‰nh chá»‰ khá»‘i lÆ°á»£ng trá»ng sá»‘ khá»•ng lá»“ cá»§a chÃºng trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n downstream nhá». Thá»© hai, cÃ¡c mÃ´ hÃ¬nh ViT quÃ¡ lá»›n Ä‘á»ƒ lÆ°u trá»¯ táº¥t cáº£ trá»ng sá»‘ cho má»—i tÃ¡c vá»¥ downstream, khiáº¿n viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ tinh chá»‰nh trÃªn cÃ¡c thiáº¿t bá»‹ cÃ³ tÃ i nguyÃªn háº¡n cháº¿ trá»Ÿ nÃªn khÃ´ng kháº£ thi.arXiv:2309.08513v5  [cs.CV]  29 Apr 2024

--- TRANG 2 ---
2 Henry Hengyuan Zhao et al.
Phi tuyáº¿n
Giáº£m máº«uTÄƒng máº«u
1 Ã—ğ·â€²
Äáº·c trÆ°ng Ä‘áº§u vÃ o: 1 Ã—ğ·Äáº·c trÆ°ng Ä‘áº§u ra: 1 Ã—ğ·
Lá»±a Chá»n 
KÃªnhTuyáº¿n tÃ­nh
1 Ã—ğ¾
Äáº·c trÆ°ng Ä‘áº§u vÃ o: 1 Ã—ğ·Äáº·c trÆ°ng Ä‘áº§u ra: 1 Ã—ğ·
(a) Adapter (b) Cá»§a chÃºng tÃ´i
HÃ¬nh 2: So sÃ¡nh kiáº¿n trÃºc giá»¯a Adapter vÃ  SCT cá»§a chÃºng tÃ´i. "Giáº£m máº«u" vÃ  "TÄƒng máº«u" Ä‘áº¡i diá»‡n cho cÃ¡c phÃ©p toÃ¡n giáº£m máº«u vÃ  tÄƒng máº«u kÃªnh. D Ä‘áº¡i diá»‡n cho sá»‘ chiá»u kÃªnh.

Äá»ƒ giáº£m thiá»ƒu hai thÃ¡ch thá»©c trÃªn, má»™t sá»‘ nghiÃªn cá»©u Ä‘á» xuáº¥t chá»‰nh chá»‰ má»™t táº­p con tham sá»‘ (?) hoáº·c Ã¡p dá»¥ng má»™t mÃ´-Ä‘un cÃ³ thá»ƒ huáº¥n luyá»‡n bÃªn ngoÃ i (Houlsby et al., 2019; Hu et al., 2021) Ä‘á»ƒ báº£o tá»“n kiáº¿n thá»©c há»c Ä‘Æ°á»£c tá»« cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. Äá»‘i vá»›i viá»‡c chá»‰nh chá»‰ má»™t táº­p con tham sá»‘, cÃ³ hai phÆ°Æ¡ng phÃ¡p Ä‘áº¡i diá»‡n: chá»‰nh chá»‰ Ä‘áº§u phÃ¢n loáº¡i (Mahajan et al., 2018; Jia et al., 2021; Chen et al., 2021b) vÃ  chá»‰nh chá»‰nh sá»‘ háº¡ng bias (Cai et al., 2020). Chá»‰nh chá»‰ Ä‘áº§u phÃ¢n loáº¡i bao gá»“m viá»‡c Ä‘Ã³ng bÄƒng trá»ng sá»‘ cá»§a máº¡ng backbone vÃ  chá»‰ cáº­p nháº­t Ä‘áº§u tuyáº¿n tÃ­nh, trong khi chá»‰nh chá»‰ sá»‘ háº¡ng bias bao gá»“m viá»‡c má»Ÿ Ä‘Ã³ng bÄƒng sá»‘ háº¡ng bias trong máº¡ng backbone. Cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m hÆ¡n.

Gáº§n Ä‘Ã¢y, má»™t sá»‘ nghiÃªn cá»©u Ä‘á» xuáº¥t giáº£i quyáº¿t má»™t tÃ¡c vá»¥ má»›i Ä‘Æ°á»£c gá»i lÃ  Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ (PEFT) báº±ng cÃ¡ch táº­n dá»¥ng má»™t mÃ´-Ä‘un cÃ³ thá»ƒ huáº¥n luyá»‡n bÃªn ngoÃ i cho viá»‡c thÃ­ch á»©ng mÃ´ hÃ¬nh. Má»™t loáº¡i phÆ°Æ¡ng phÃ¡p bao gá»“m viá»‡c tÃ­ch há»£p cÃ¡c prompt cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ o Ä‘áº§u vÃ o mÃ´ hÃ¬nh cho má»—i tÃ¡c vá»¥ downstream, Ä‘Æ°á»£c minh há»a báº±ng cÃ´ng trÃ¬nh tiÃªn phong, VPT(Jia et al., 2022). Tuy nhiÃªn, VPT gáº·p pháº£i má»™t sá»‘ thÃ¡ch thá»©c. Thá»© nháº¥t, cÃ¡c prompt Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ cá»§a nÃ³ Ä‘Æ°á»£c suy ra thÃ´ng qua há»c cÃ³ giÃ¡m sÃ¡t, khÃ¡c vá»›i cÃ¡c prompt vÄƒn báº£n do ngÆ°á»i dÃ¹ng cung cáº¥p thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong lÄ©nh vá»±c NLP. Thá»© hai, VPT yÃªu cáº§u tÃ¬m kiáº¿m Ä‘á»™ dÃ i prompt tá»‘t nháº¥t cho má»—i tÃ¡c vá»¥, Ä‘iá»u nÃ y thiáº¿u tÃ­nh linh hoáº¡t vÃ  khÃ´ng thá»±c táº¿ khi gáº·p cÃ¡c tÃ¡c vá»¥ downstream má»›i.

Má»™t loáº¡i nghiÃªn cá»©u khÃ¡c lÃ  thÃªm má»™t adapter (Chen et al., 2022b; Houlsby et al., 2019; Jie and Deng, 2022) cÃ¹ng vá»›i khá»‘i multi-head self-attention (MHSA) hoáº·c MLP vÃ  xá»­ lÃ½ toÃ n bá»™ Ä‘áº·c trÆ°ng má»™t cÃ¡ch Ä‘á»“ng Ä‘á»u mÃ  khÃ´ng xem xÃ©t thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ vÃ o thiáº¿t káº¿ mÃ´-Ä‘un hiá»‡u quáº£. HÆ¡n ná»¯a, sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n khÃ´ng pháº£i lÃ  khÃ¡ nhá».

Báº±ng cÃ¡ch Ä‘iá»u tra cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y, viá»‡c sá»­ dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ trong thiáº¿t káº¿ mÃ´-Ä‘un hiá»‡u quáº£ váº«n chÆ°a Ä‘Æ°á»£c khÃ¡m phÃ¡ Ä‘áº§y Ä‘á»§ nhÆ° Ä‘Æ°á»£c nháº¥n máº¡nh trong má»™t nghiÃªn cá»©u gáº§n Ä‘Ã¢y (Luo et al., 2022) ráº±ng bias kÃªnh tá»“n táº¡i trong cÃ¡c tÃ¡c vá»¥ downstream Ä‘a dáº¡ng. Trong bÃ i bÃ¡o nÃ y, Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» PEFT, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline Ä‘Æ¡n giáº£n Ä‘á»ƒ liÃªn quan Ä‘áº¿n thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ báº±ng cÃ¡ch chuyá»ƒn tiáº¿p mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i cÃ¡c hÃ¬nh áº£nh downstream. HÆ¡n ná»¯a, chÃºng tÃ´i tiáº¿n hÃ nh chá»n má»™t pháº§n nhá» cÃ¡c kÃªnh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c tinh chá»‰nh hiá»‡u quáº£, giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ tham sá»‘. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trong Má»¥c 4.2 xÃ¡c nháº­n láº¡i ráº±ng viá»‡c chá»‰nh chá»‰ chá»‰ má»™t pháº§n nhá» cÃ¡c kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ lÃ  Ä‘á»§ cho viá»‡c thÃ­ch á»©ng tÃ¡c vá»¥ downstream trong cháº¿ Ä‘á»™ dá»¯ liá»‡u tháº¥p. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu cÃ³ Ä‘Æ°á»£c nhá»¯ng kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t Äiá»ƒm Quan Trá»ng Nháº­n Thá»©c Lá»›p (CAIS) báº±ng cÃ¡ch Ã¡p dá»¥ng chuáº©n L2 lÃ m tiÃªu chÃ­ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a kÃªnh Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« phÆ°Æ¡ng phÃ¡p pruning cá»• Ä‘iá»ƒn (Han et al., 2015). ChÃºng tÃ´i gá»i cÃ¡c kÃªnh Ä‘Æ°á»£c chá»n lÃ  "kÃªnh ná»•i báº­t" (SC) vÃ¬ chÃºng cÃ³ giÃ¡ trá»‹ kÃ­ch hoáº¡t cao hÆ¡n vÃ  ráº¥t quan trá»ng cho hiá»‡u suáº¥t tÃ¡c vá»¥. Cáº§n lÆ°u Ã½ ráº±ng lá»±a chá»n kÃªnh cÃ³ thá»ƒ trÃ¡nh chi phÃ­ huáº¥n luyá»‡n báº±ng cÃ¡ch chá»n cáº¥u trÃºc tá»‘t nháº¥t nhÆ° trong NOAH(Zhang et al., 2022) hoáº·c Ä‘á»™ dÃ i prompt tá»‘t nháº¥t nhÆ° trong VPT(Jia et al., 2022). HÆ¡n ná»¯a, so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn adapter (Houlsby et al., 2019; Chen et al., 2022b), viá»‡c chá»n cÃ¡c kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ má»™t pháº§n cho phÃ©p chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c chi phÃ­ tham sá»‘ tháº¥p hÆ¡n báº±ng cÃ¡ch loáº¡i bá» cÃ¡c phÃ©p toÃ¡n "giáº£m máº«u" vÃ  "phi tuyáº¿n" nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2.

TÃ³m láº¡i, cÃ¡c Ä‘Ã³ng gÃ³p Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:
ÄÃ³ng gÃ³p
â€“ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline Ä‘Æ¡n giáº£n vá»›i gÃ³c nhÃ¬n má»›i trong viá»‡c chá»‰nh chá»‰ kÃªnh má»™t pháº§n (tá»©c lÃ , chá»‰nh chá»‰ kÃªnh ná»•i báº­t) cho viá»‡c giáº£i quyáº¿t tÃ¡c vá»¥ PEFT Ä‘á»ƒ táº­n dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t Ä‘áº§y há»©a háº¹n trÃªn 19 tÃ¡c vá»¥ há»c chuyá»ƒn giao thá»‹ giÃ¡c, 4 tÃ¡c vá»¥ khÃ¡i quÃ¡t hÃ³a miá»n, vÃ  5 tÃ¡c vá»¥ há»c few-shot. CÃ¡c thÃ­ nghiá»‡m chá»©ng minh ráº±ng viá»‡c chá»‰nh chá»‰ má»™t táº­p con kÃªnh trong báº£n Ä‘á»“ Ä‘áº·c trÆ°ng lÃ  hiá»‡u quáº£ vÃ  tá»•ng quÃ¡t.
â€“Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn giáº£m chi phÃ­ tham sá»‘ xuá»‘ng má»©c 0.11M (Ã­t hÆ¡n 780 Ã— tham sá»‘ so vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§) cá»§a ViT-B trong khi Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t trung bÃ¬nh tá»‘t nháº¥t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c.

--- TRANG 3 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 3
[Hiá»ƒn thá»‹ hÃ¬nh áº£nh hÃ³a báº£n Ä‘á»“ Ä‘áº·c trÆ°ng Ä‘Æ°á»£c trÃ­ch xuáº¥t trÃªn táº­p dá»¯ liá»‡u Caltech101 á»Ÿ má»—i lá»›p transformer, vá»›i trá»¥c Y Ä‘áº¡i diá»‡n cho chá»‰ sá»‘ lá»›p vÃ  trá»¥c X Ä‘áº¡i diá»‡n cho chá»‰ sá»‘ kÃªnh (tá»•ng cá»™ng 768 kÃªnh), cho tháº¥y má»™t sá»‘ kÃªnh cÃ³ giÃ¡ trá»‹ kÃ­ch hoáº¡t cao hÆ¡n cÃ¡c kÃªnh khÃ¡c]

HÃ¬nh 3: HÃ¬nh áº£nh hÃ³a báº£n Ä‘á»“ Ä‘áº·c trÆ°ng vÃ  cÃ¡c chá»‰ sá»‘ kÃªnh ná»•i báº­t Ä‘Æ°á»£c chá»n. Táº¥t cáº£ káº¿t quáº£ Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng ViT-B/16 Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn ImageNet21K.

2 NghiÃªn cá»©u liÃªn quan
2.1 Vision Transformers
Transformer (Vaswani et al., 2017) Ä‘Ã£ chá»©ng minh káº¿t quáº£ xuáº¥t sáº¯c trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c mÃ¡y tÃ­nh. Ráº¥t nhiá»u vision transformer (Chen et al., 2021a; d'Ascoli et al., 2021; Dong et al., 2022; Ali et al., 2021; Fan et al., 2021; Han et al., 2021; Rao et al., 2021; Yuan et al., 2021; Touvron et al., 2021; Liu et al., 2021; Wang et al., 2021; Zhou et al., 2021a) Ä‘Æ°á»£c Ä‘á» xuáº¥t sau cÃ´ng trÃ¬nh tiÃªn phong ViT (Dosovit-

--- TRANG 4 ---
4 Henry Hengyuan Zhao et al.
skiy et al., 2020). Nhiá»u trong sá»‘ chÃºng tÄƒng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh dáº§n dáº§n Ä‘á»ƒ Ä‘áº¡t káº¿t quáº£ tá»‘i tÃ¢n vÃ  há»c cÃ¡c biá»ƒu diá»…n phong phÃº thÃ´ng qua cÃ¡c thiáº¿t káº¿ kiáº¿n trÃºc khÃ¡c nhau. Cáº§n lÆ°u Ã½ ráº±ng háº§u háº¿t chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u tá»± nhiÃªn vÃ  cÃ³ tiá»m nÄƒng máº¡nh máº½ Ä‘á»ƒ Ä‘Æ°á»£c chuyá»ƒn giao sang cÃ¡c miá»n/tÃ¡c vá»¥ khÃ¡c. HÆ¡n ná»¯a, viá»‡c Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh nÃ y cho cÃ¡c tÃ¡c vá»¥ downstream cÃ³ thá»ƒ giáº£m bá»›t khÃ³ khÄƒn trong huáº¥n luyá»‡n vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ Ä‘áº§y há»©a háº¹n.

2.2 PhÆ°Æ¡ng phÃ¡p Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘
PEFT táº­p trung vÃ o viá»‡c Ã¡p dá»¥ng má»™t mÃ´-Ä‘un cÃ³ thá»ƒ huáº¥n luyá»‡n vá»›i má»™t sá»‘ Ã­t tham sá»‘ cho tinh chá»‰nh. Hai hÆ°á»›ng PEFT Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t gáº§n Ä‘Ã¢y. Má»™t máº·t, viá»‡c Ã¡p dá»¥ng prompts (Jia et al., 2022; Liu et al., 2022; Xing et al., 2022; Zheng et al., 2022; Nie et al., 2022; Wang et al., 2022; Zhou et al., 2022a,b; Liao et al., 2023; Manli et al., 2022; Zhang et al., 2023b; Zang et al., 2022; Bar et al., 2022) vÃ o cÃ¡c máº¡ng backbone cho tháº¥y thÃ nh cÃ´ng trÃªn má»™t sá»‘ tÃ¡c vá»¥ thá»‹ giÃ¡c. VPT (Jia et al., 2022) láº§n Ä‘áº§u tiÃªn Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn prompt trong lÄ©nh vá»±c thá»‹ giÃ¡c mÃ¡y tÃ­nh báº±ng cÃ¡ch chÃ¨n prompts vÃ o má»—i lá»›p transformer. Tuy nhiÃªn, má»™t háº¡n cháº¿ chÃ­nh cá»§a VPT lÃ  nÃ³ dá»±a vÃ o viá»‡c lá»±a chá»n thá»§ cÃ´ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ dÃ i prompt tá»‘i Æ°u cho má»—i tÃ¡c vá»¥. Äiá»u nÃ y khÃ´ng linh hoáº¡t khi Ã¡p dá»¥ng cho má»™t tÃ¡c vá»¥ downstream má»›i. Má»™t háº¡n cháº¿ khÃ¡c lÃ  cÃ¡c prompt Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ cá»§a VPT Ä‘Æ°á»£c thu Ä‘Æ°á»£c thÃ´ng qua quy trÃ¬nh huáº¥n luyá»‡n vÃ  khÃ´ng thá»ƒ Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng cung cáº¥p nhÆ° trong lÄ©nh vá»±c NLP. Trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i, Ä‘á»ƒ giáº£m rÃµ rÃ ng cÃ¡c tÃ­nh toÃ¡n tÃ¬m kiáº¿m Ä‘á»™ dÃ i prompt Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥, chÃºng tÃ´i truyá»n cÃ¡c hÃ¬nh áº£nh huáº¥n luyá»‡n vÃ o máº¡ng backbone vÃ  táº­n dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh ná»•i báº­t. Quy trÃ¬nh nÃ y chá»‰ cáº§n má»™t láº§n lan truyá»n tiáº¿n mÃ  khÃ´ng cÃ³ chi phÃ­ huáº¥n luyá»‡n.

Máº·t khÃ¡c, viá»‡c thÃªm má»™t mÃ´-Ä‘un dÆ° (Houlsby et al., 2019; Chen et al., 2022b; Jie and Deng, 2022; Chen et al., 2022a) vÃ o cÃ¡c máº¡ng backbone cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ Ä‘áº§y há»©a háº¹n vá» hiá»‡u suáº¥t vÃ  hiá»‡u quáº£. Adapter (Houlsby et al., 2019), má»™t baseline Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u tÃ¡c vá»¥ (Sung et al., 2022; Pan et al., 2022; Zhang et al., 2023a), Ä‘á» xuáº¥t má»™t mÃ´-Ä‘un giá»‘ng MLP, má»™t thiáº¿t káº¿ thÃ nh cÃ´ng mÃ  Ä‘áº§u tiÃªn chiáº¿u cÃ¡c Ä‘áº·c trÆ°ng cÃ³ chiá»u gá»‘c thÃ nh má»™t chiá»u nhá» hÆ¡n vá»›i má»™t lá»›p phi tuyáº¿n vÃ  chiáº¿u ngÆ°á»£c láº¡i vá» chiá»u gá»‘c. NÃ³ giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘. ÄÆ°á»£c láº¥y cáº£m há»©ng tá»« cÃ¡c chiá»u trung gian nhá» nháº¥t cá»§a nÃ³, viá»‡c tÃ¬m má»™t sá»‘ lÆ°á»£ng nhá» cÃ¡c kÃªnh ná»•i báº­t trong báº£n Ä‘á»“ Ä‘áº·c trÆ°ng cÃ³ thá»ƒ Ä‘á»§ cho viá»‡c thÃ­ch á»©ng. KhÃ´ng giá»‘ng nhÆ° viá»‡c chÃ¨n cÃ¡c mÃ´-Ä‘un cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ o cÃ¡c khá»‘i transformer, LoRA (Hu et al., 2021) tá»‘i Æ°u hÃ³a má»™t ma tráº­n phÃ¢n rÃ£ thá»© háº¡ng tháº¥p vá»›i má»™t chiá»u ná»™i táº¡i tháº¥p Ä‘á»ƒ chiáº¿u cÃ¡c Ä‘áº·c trÆ°ng query, key. Äá»‘i vá»›i NOAH (Zhang et al., 2022), má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng tháº§n kinh káº¿t há»£p Adapter, LoRA, vÃ  VPT vÃ o khÃ´ng gian tÃ¬m kiáº¿m máº¡ng cá»§a nÃ³. NOAH cung cáº¥p má»™t baseline máº¡nh máº½ Ä‘á»ƒ thá»±c hiá»‡n tá»‘t má»™t cÃ¡ch nháº¥t quÃ¡n trÃªn cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau. SSF (Lian et al., 2022), má»™t baseline máº¡nh máº½ Ä‘Æ°á»£c Ä‘á» xuáº¥t gáº§n Ä‘Ã¢y chá»‰ báº±ng cÃ¡ch chia tá»· lá»‡ vÃ  dá»‹ch chuyá»ƒn cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»ƒ thá»±c hiá»‡n tinh chá»‰nh mÃ´ hÃ¬nh hiá»‡u quáº£. Child-Tuning Xu et al. (2021) cáº­p nháº­t má»™t táº­p con tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thÃ´ng qua viá»‡c che máº·t náº¡ chiáº¿n lÆ°á»£c gradient cá»§a máº¡ng khÃ´ng pháº£i child trong quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c. AdaptFormer Chen et al. (2022b) lÃ  má»™t phÆ°Æ¡ng phÃ¡p giá»‘ng adapter. NÃ³ khÃ¡m phÃ¡ hiá»‡u quáº£ cá»§a kiáº¿n trÃºc giá»‘ng adapter cá»§a tinh chá»‰nh vision transformer.

KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p trÃªn, SCT cá»§a chÃºng tÃ´i xá»­ lÃ½ PEFT báº±ng cÃ¡ch táº­n dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ vÃ o thiáº¿t káº¿ phÆ°Æ¡ng phÃ¡p vÃ  sau Ä‘Ã³ Ä‘á» xuáº¥t Tinh Chá»‰nh KÃªnh Ná»•i Báº­t, Ä‘áº¡t Ä‘Æ°á»£c chi phÃ­ tham sá»‘ tháº¥p hÆ¡n vÃ  hiá»‡u suáº¥t cao hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã³.

3 PhÆ°Æ¡ng phÃ¡p
3.1 KhÃ´ng pháº£i táº¥t cáº£ cÃ¡c kÃªnh Ä‘á»u bÃ¬nh Ä‘áº³ng
Äá»ƒ táº­n dá»¥ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥, bÃ i bÃ¡o nÃ y nháº±m tÃ¬m cÃ¡c kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ Ä‘á»ƒ tinh chá»‰nh. á» Ä‘Ã¢y chÃºng tÃ´i cung cáº¥p má»™t quan sÃ¡t Ä‘Æ¡n giáº£n nháº¥n máº¡nh "KÃªnh Ná»•i Báº­t" tá»“n táº¡i trong cÃ¡c tÃ¡c vá»¥ downstream.

CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y (Luo et al., 2017; Li et al., 2016; He et al., 2018; Liu et al., 2018; Han et al., 2015; Li et al., 2017) chá»©ng minh ráº±ng viá»‡c cáº¯t tá»‰a má»™t sá»‘ kÃªnh cá»§a máº¡ng tháº§n kinh sÃ¢u cÃ³ áº£nh hÆ°á»Ÿng nhá» Ä‘áº¿n hiá»‡u suáº¥t mÃ´ hÃ¬nh nhÆ°ng cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ vÃ  chi phÃ­ tÃ­nh toÃ¡n. Nhá»¯ng káº¿t quáº£ nhÆ° váº­y pháº£n Ã¡nh ráº±ng táº§m quan trá»ng cá»§a cÃ¡c kÃªnh khÃ¡c nhau khÃ´ng giá»‘ng nhau, tá»©c lÃ , "KhÃ´ng pháº£i táº¥t cáº£ cÃ¡c kÃªnh Ä‘á»u bÃ¬nh Ä‘áº³ng". Má»™t cÃ¡ch trá»±c quan, táº§m quan trá»ng cá»§a kÃªnh khÃ¡c nhau vá» máº·t tÃ¡c vá»¥, Ä‘iá»u nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a viá»‡c lá»±a chá»n kÃªnh trong tinh chá»‰nh mÃ´ hÃ¬nh.

Äá»ƒ tÃ¬m cÃ¡c kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥, má»™t cÃ¡ch trá»±c quan lÃ  tÃ¬m má»™t sá»‘ kÃªnh chung trÃªn cÃ¡c danh má»¥c khÃ¡c nhau. Do Ä‘Ã³, chÃºng tÃ´i Ä‘áº§u tiÃªn minh há»a má»™t quan sÃ¡t giá»¯a mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  tÃ¡c vá»¥ downstream. ChÃºng tÃ´i chá»n Caltech101 (Fei-Fei et al., 2004) (má»™t trong cÃ¡c tÃ¡c vá»¥ downstream tá»« benchmark VTAB-1K) lÃ m vÃ­ dá»¥ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c Ä‘áº·c trÆ°ng trung gian cá»§a má»—i lá»›p transformer cá»§a ViT. ViT-B Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn ImageNet21K lÃ  backbone, vÃ  chÃºng tÃ´i Ä‘Æ°a toÃ n bá»™ táº­p dá»¯ liá»‡u Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng giá»¯a cÃ¡c khá»‘i multi-head self-attention ("Attn") vÃ  "MLP" trong táº¥t cáº£ 12 lá»›p transformer. HÃ¬nh 3 (a) cho tháº¥y má»™t sá»‘ Ä‘Æ°á»ng tháº³ng Ä‘á»©ng trong má»—i hÃ¬nh con cá»§a cÃ¡c lá»›p transformer. NÃ³ chá»‰ ra ráº±ng khi sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng trÃªn táº­p dá»¯ liá»‡u má»¥c tiÃªu,

--- TRANG 5 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 5
Äiá»ƒm 
Quan trá»ngTuyáº¿n tÃ­nh
BNDTop K
BN
Äáº·c trÆ°ng Ä‘áº§u vÃ oÄáº·c trÆ°ng Ä‘Æ°á»£c biáº¿n Ä‘á»•iÄáº·c trÆ°ng ná»•i báº­tThay tháº¿ Tá»· lá»‡
Äáº·c trÆ°ng Ä‘áº§u vÃ o Äáº·c trÆ°ng Ä‘áº§u raÄáº·c trÆ°ng trung gianSCTMTrong quÃ¡ trÃ¬nh Tinh chá»‰nh
HÃ¬nh 4: Tá»•ng quan vá» mÃ´-Ä‘un tinh chá»‰nh kÃªnh ná»•i báº­t Ä‘Æ°á»£c Ä‘á» xuáº¥t.

má»™t sá»‘ kÃªnh cÃ³ giÃ¡ trá»‹ kÃ­ch hoáº¡t cao hÆ¡n cÃ¡c kÃªnh khÃ¡c, báº¥t ká»ƒ danh má»¥c. ChÃºng tÃ´i gá»i nhá»¯ng kÃªnh nÃ y lÃ  "kÃªnh ná»•i báº­t". Äá»ƒ Ä‘iá»u tra vá»‹ trÃ­ cá»§a cÃ¡c kÃªnh ná»•i báº­t, chÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o chá»‰ sá»‘ cá»§a 36 kÃªnh ná»•i báº­t hÃ ng Ä‘áº§u Ä‘Æ°á»£c chá»n sau khi tÃ­nh Ä‘iá»ƒm quan trá»ng cá»§a má»—i lá»›p trong HÃ¬nh 3 (b). Sá»‘ Ä‘áº¡i diá»‡n cho chá»‰ sá»‘ kÃªnh trong tá»•ng sá»‘ 768 kÃªnh. MÃ u tá»‘i hÆ¡n Ä‘áº¡i diá»‡n cho giÃ¡ trá»‹ kÃ­ch hoáº¡t cao hÆ¡n. ChÃºng ta cÃ³ thá»ƒ tháº¥y cÃ¡c lá»›p khÃ¡c nhau cÃ³ táº­p há»£p kÃªnh ná»•i báº­t khÃ¡c nhau. Khi lá»›p Ä‘i sÃ¢u hÆ¡n, cÃ¡c kÃªnh ná»•i báº­t xuáº¥t hiá»‡n táº­p trung hÆ¡n. ChÃºng tÃ´i giáº£ thuyáº¿t ráº±ng cÃ¡c lá»›p sÃ¢u hÆ¡n chá»©a thÃ´ng tin trá»«u tÆ°á»£ng hÆ¡n vÃ  thÃ´ng tin Ä‘Æ°á»£c táº­p há»£p trong má»™t vÃ i kÃªnh. NgÆ°á»£c láº¡i, cÃ¡c lá»›p nÃ´ng trÃ­ch xuáº¥t biá»ƒu diá»…n cáº¥p tháº¥p hÆ¡n, lÃ m cho thÃ´ng tin dÃ y Ä‘áº·c.

Äiá»u nÃ y tá»± nhiÃªn Ä‘áº·t ra má»™t cÃ¢u há»i: Liá»‡u chÃºng ta cÃ³ thá»ƒ tÃ¬m cÃ¡c kÃªnh ná»•i báº­t trong báº£n Ä‘á»“ Ä‘áº·c trÆ°ng vÃ  sau Ä‘Ã³ chá»‰ chá»‰nh chá»‰ nhá»¯ng kÃªnh ná»•i báº­t nÃ y Ä‘á»ƒ tinh chá»‰nh hiá»‡u quáº£?
Äá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y, chÃºng tÃ´i thiáº¿t káº¿ má»™t Ä‘iá»ƒm quan trá»ng nháº­n thá»©c lá»›p Ä‘Æ¡n giáº£n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh ná»•i báº­t nÃ y vÃ  táº­n dá»¥ng má»™t mÃ´-Ä‘un tinh chá»‰nh kÃªnh ná»•i báº­t cho tinh chá»‰nh hiá»‡u quáº£.

3.2 Äiá»ƒm Quan Trá»ng Nháº­n Thá»©c Lá»›p
Viá»‡c lá»±a chá»n cÃ¡c kÃªnh ná»•i báº­t lÃ  quan trá»ng cho viá»‡c thÃ­ch á»©ng vá»›i cÃ¡c tÃ¡c vá»¥ downstream, Ä‘iá»u nÃ y nÃªn chá»©a thÃ´ng tin chung trong táº¥t cáº£ cÃ¡c lá»›p. Má»™t cÃ¡ch trá»±c quan, chÃºng ta cÃ³ thá»ƒ trá»±c tiáº¿p sá»­ dá»¥ng Ä‘áº·c trÆ°ng trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c máº«u dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº¡i diá»‡n cho phÃ¢n phá»‘i tÃ¡c vá»¥ downstream Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh ná»•i báº­t. Tuy nhiÃªn, xem xÃ©t cÃ¡c táº­p dá»¯ liá»‡u downstream cÃ³ thá»ƒ khÃ´ng luÃ´n cÃ¢n báº±ng, viá»‡c coi toÃ n bá»™ táº­p dá»¯ liá»‡u nhÆ° má»™t Ä‘Æ¡n vá»‹ cÃ³ thá»ƒ thiÃªn vá»‹ Ä‘á»‘i vá»›i cÃ¡c lá»›p Ä‘áº§u, dáº«n Ä‘áº¿n cÃ¡c kÃªnh Ä‘Æ°á»£c chá»n khÃ´ng Ä‘áº¡i diá»‡n cho thÃ´ng tin chung trÃªn táº¥t cáº£ cÃ¡c lá»›p. Do Ä‘Ã³, chÃºng tÃ´i Ä‘á» xuáº¥t Äiá»ƒm Quan Trá»ng Nháº­n Thá»©c Lá»›p (CAIS), trong Ä‘Ã³ viá»‡c tÃ­nh toÃ¡n Ä‘iá»ƒm quan trá»ng Ä‘Æ°á»£c thá»±c hiá»‡n á»Ÿ má»—i lá»›p vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c lá»›p. Giáº£ sá»­ cÃ¡c báº£n Ä‘á»“ Ä‘áº·c trÆ°ng trung gian lÃ  {fl_m âˆˆ R^(BmÃ—NÃ—D)|l, mâˆˆN,1â‰¤lâ‰¤L vÃ  1â‰¤mâ‰¤M}, trong Ä‘Ã³ N vÃ  D Ä‘áº¡i diá»‡n cho sá»‘ lÆ°á»£ng token vÃ  sá»‘ chiá»u kÃªnh, tÆ°Æ¡ng á»©ng. L Ä‘áº¡i diá»‡n cho tá»•ng sá»‘ lá»›p cá»§a backbone ViT. Bm lÃ  khá»‘i lÆ°á»£ng cá»§a má»—i lá»›p, vÃ  M Ä‘áº¡i diá»‡n cho sá»‘ danh má»¥c cá»§a tÃ¡c vá»¥ nháº­n dáº¡ng downstream. ChÃºng tÃ´i Ä‘áº§u tiÃªn Ã¡p dá»¥ng chuáº©n hÃ³a chuáº©n L2 cá»§a má»—i chiá»u kÃªnh cá»§a Ä‘áº·c trÆ°ng fl_m:

Ëœfl_m={âˆ¥fl_m,1âˆ¥2,âˆ¥fl_m,iâˆ¥2, ...,âˆ¥fl_m,Dâˆ¥2},
âˆ¥fl_m,iâˆ¥2âˆˆR, iâˆˆN,1â‰¤iâ‰¤D,(1)

trong Ä‘Ã³ Ëœfl_mâˆˆR^(1Ã—D) lÃ  vector Ä‘iá»ƒm quan trá»ng á»Ÿ lá»›p thá»© l vÃ  lá»›p thá»© m. Äá»ƒ Ä‘iá»u tra cÃ¡c kÃªnh ná»•i báº­t, chÃºng tÃ´i tÃ­nh trung bÃ¬nh Ëœfl_mâˆˆR^(1Ã—D) trÃªn táº¥t cáº£ cÃ¡c lá»›p Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ä‘iá»ƒm quan trá»ng á»Ÿ lá»›p thá»© l:

Zl=1/M âˆ‘(m=1 to M) Ëœfl_m, Ëœfl_mâˆˆR^(1Ã—D), (2)

Sau khi cÃ³ Ä‘Æ°á»£c vector Ä‘iá»ƒm quan trá»ng {ZlâˆˆR^(1Ã—D)|lâˆˆN,1â‰¤lâ‰¤L}, chÃºng ta cÃ³ thá»ƒ chá»n K giÃ¡ trá»‹ lá»›n nháº¥t cá»§a Zl vÃ  sau Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ suy ra cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c chá»n lÃ  Il=topK(Zl), IlâˆˆN^(1Ã—K) á»Ÿ lá»›p thá»© l. Sau Ä‘Ã³, cÃ¡c kÃªnh ná»•i báº­t á»Ÿ má»—i lá»›p khÃ¡c nhau trÃªn cÃ¡c tÃ¡c vá»¥ downstream khÃ¡c nhau, vÃ  nÃ³ cÃ³ thá»ƒ liÃªn quan rÃµ rÃ ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ trong tinh chá»‰nh mÃ´ hÃ¬nh. CÃ¡c quy trÃ¬nh Ä‘á»ƒ chá»n cÃ¡c kÃªnh ná»•i báº­t Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Thuáº­t toÃ¡n 1.

3.3 ThÃ­ch á»©ng ViT thÃ´ng qua Tinh Chá»‰nh KÃªnh Ná»•i Báº­t
Vá»›i Ä‘iá»ƒm quan trá»ng cá»§a viá»‡c chá»n K kÃªnh ná»•i báº­t hÃ ng Ä‘áº§u, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t MÃ´-Ä‘un Tinh Chá»‰nh KÃªnh Ná»•i Báº­t (SCTM) trong bÃ i bÃ¡o nÃ y. Tá»•ng quan vá» SCTM Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 4. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c, SCTM cá»§a chÃºng tÃ´i chá»‰ chá»©a má»™t lá»›p tuyáº¿n tÃ­nh thay vÃ¬ má»™t adapter giá»‘ng MLP (Houlsby et al., 2019) bao gá»“m hai lá»›p cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ  má»™t lá»›p kÃ­ch hoáº¡t Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2.

--- TRANG 6 ---
6 Henry Hengyuan Zhao et al.
Cáº¥u trÃºc Ä‘Æ¡n giáº£n nhÆ° váº­y dá»… tÃ¡i táº¡o vÃ  duy trÃ¬ chi phÃ­ tham sá»‘ tÆ°Æ¡ng Ä‘á»‘i tháº¥p. Äá»ƒ káº¿ thá»«a cÃ¡c biá»ƒu diá»…n máº¡nh máº½ ban Ä‘áº§u, chÃºng tÃ´i sá»­ dá»¥ng má»™t shortcut dÆ° Ä‘á»ƒ káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng ná»•i báº­t vá»›i cÃ¡c Ä‘áº·c trÆ°ng trung gian vÃ  sá»­ dá»¥ng Scale âˆˆR (nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4) Ä‘á»ƒ Ä‘iá»u chá»‰nh trá»ng sá»‘ giá»¯a cáº£ hai Ä‘áº·c trÆ°ng, Ä‘Ã¢y lÃ  má»™t tham sá»‘ háº±ng sá»‘. Sau khi cÃ³ Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biáº¿n Ä‘á»•i, chÃºng tÃ´i chÃ¨n cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biáº¿n Ä‘á»•i trá»Ÿ láº¡i vÃ o cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o á»Ÿ cÃ¹ng vá»‹ trÃ­. CÃ¡c kÃªnh khÃ´ng Ä‘Æ°á»£c chá»n chÃºng tÃ´i giá»¯ Ä‘Ã³ng bÄƒng trong quÃ¡ trÃ¬nh tinh chá»‰nh. Trong HÃ¬nh 6, chÃºng tÃ´i trÃ¬nh bÃ y hai hÃ¬nh thá»©c chÃ¨n SCTM vÃ o ViT. "Ours-MLP" Ä‘áº¡i diá»‡n cho viá»‡c chÃºng tÃ´i chÃ¨n SCTM sau khá»‘i MLP, vÃ  "Our-Attn" chá»‰ ra ráº±ng chÃºng tÃ´i Ä‘áº·t SCTM sau khá»‘i MHSA trong khi trÆ°á»›c khá»‘i MLP. Trong cÃ¡c thÃ­ nghiá»‡m sau, "Ours-Attn" lÃ  vá»‹ trÃ­ tiÃªm máº·c Ä‘á»‹nh so vá»›i cÃ¡c baseline khÃ¡c.

Tá»•ng quan. SCT cá»§a chÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘Æ°a táº­p huáº¥n luyá»‡n vÃ o máº¡ng backbone Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng trung gian trong cÃ¡c lá»›p khÃ¡c nhau. Sau Ä‘Ã³, sá»­ dá»¥ng CAIS Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh ná»•i báº­t vÃ  lÆ°u cÃ¡c chá»‰ sá»‘ kÃªnh. Trong quÃ¡ trÃ¬nh tinh chá»‰nh, chÃºng tÃ´i thÃªm SCTM cá»§a chÃºng tÃ´i vÃ o má»—i lá»›p transformer vÃ  chá»‰nh chá»‰ cÃ¡c kÃªnh Ä‘Æ°á»£c chá»n vá»›i má»™t lá»›p tuyáº¿n tÃ­nh trong khi Ä‘Ã³ng bÄƒng cÃ¡c kÃªnh khÃ´ng Ä‘Æ°á»£c chá»n khÃ¡c.

Tháº£o luáº­n. Thá»© nháº¥t, cÃ¡c tÃ¡c vá»¥ downstream khÃ¡c nhau cÃ³ Ä‘áº·c thÃ¹ riÃªng cá»§a chÃºng, tá»©c lÃ , "KhÃ´ng pháº£i táº¥t cáº£ cÃ¡c kÃªnh Ä‘á»u bÃ¬nh Ä‘áº³ng". CÃ¡c phÆ°Æ¡ng phÃ¡p PEFT trÆ°á»›c Ä‘Ã¢y (tá»©c lÃ , Adapter) thiáº¿u viá»‡c xem xÃ©t sá»­ dá»¥ng rÃµ rÃ ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i Ä‘á» xuáº¥t SCT táº­n dá»¥ng rÃµ rÃ ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ cho viá»‡c lá»±a chá»n kÃªnh, Ä‘iá»u nÃ y cÃ³ thá»ƒ cho phÃ©p chÃºng tÃ´i tiáº¿t kiá»‡m cÃ¡c tÃ­nh toÃ¡n vÃ  tham sá»‘ bá»• sung cá»§a cÃ¡c phÃ©p toÃ¡n "Giáº£m máº«u" vÃ  "Phi tuyáº¿n" so vá»›i Adapter (Houlsby et al., 2019). Thá»© hai, viá»‡c tÃ­nh toÃ¡n cÃ¡c kÃªnh ná»•i báº­t offline cÃ³ thá»ƒ tiáº¿t kiá»‡m cÃ¡c tÃ­nh toÃ¡n tÃ¬m kiáº¿m Ä‘á»™ dÃ i prompt tá»‘t nháº¥t (Jia et al., 2022) hoáº·c cáº¥u trÃºc máº¡ng tá»‘t nháº¥t (Zhang et al., 2022). Thá»© ba, káº¿t quáº£ cá»§a chÃºng tÃ´i trong HÃ¬nh 1 chá»©ng minh ráº±ng chá»‰ thÃ­ch á»©ng 12.5% kÃªnh cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ cáº¡nh tranh so vá»›i cÃ¡c baseline khÃ¡c. Äá»“ng thá»i, viá»‡c biáº¿n Ä‘á»•i má»™t pháº§n nhá» Ä‘áº·c trÆ°ng cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c (vÃ­ dá»¥, 12 Ã— 96 Ã— 96 tÆ°Æ¡ng Ä‘á»‘i nhá» hÆ¡n 12 Ã— 768 Ã— 768). Thá»© tÆ°, má»™t lÆ°á»£ng tham sá»‘ nhá» nhÆ° váº­y giáº£m Ä‘Ã¡ng ká»ƒ khÃ³ khÄƒn trong viá»‡c lÆ°u trá»¯ kiáº¿n thá»©c Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ khi chÃºng ta cÃ³ nhiá»u tÃ¡c vá»¥ downstream. Äá»ƒ Ä‘Ã¡nh giÃ¡ baseline Ä‘Æ¡n giáº£n nÃ y, chÃºng tÃ´i cÅ©ng kiá»ƒm tra kháº£ nÄƒng few-shot vÃ  kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a miá»n cá»§a nÃ³ Ä‘á»ƒ Ä‘Ã¡p á»©ng cÃ¡c yÃªu cáº§u cá»§a cÃ¡c á»©ng dá»¥ng thá»±c táº¿.

Thuáº­t toÃ¡n 1 CÃ¡c quy trÃ¬nh lá»±a chá»n kÃªnh thÃ´ng qua Ä‘iá»ƒm quan trá»ng nháº­n thá»©c lá»›p.
Äáº§u vÃ o:
MÃ´ hÃ¬nh ViT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c F vÃ  sá»‘ lá»›p L;
Sá»‘ kÃªnh Ä‘Æ°á»£c chá»n K vÃ  tá»•ng sá»‘ lá»›p M;
1: foreach lâˆˆ[1, L] do
2: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng fl_m á»Ÿ má»—i lá»›p m;
3: Ãp dá»¥ng chuáº©n hÃ³a chuáº©n L2 trÃªn má»—i kÃªnh Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ëœfl_m thÃ´ng qua Eq. 1;
4: TÃ­nh Ä‘iá»ƒm quan trá»ng Zl cá»§a lá»›p thá»© l thÃ´ng qua Eq. 2;
5: Chá»n K kÃªnh lá»›n nháº¥t Il= topK( Zl);
6: end for
Äáº§u ra: CÃ¡c chá»‰ sá»‘ kÃªnh Ä‘Æ°á»£c chá»n cá»§a má»—i lá»›p transformer.

3.4 Kiá»ƒm tra XÃ³a KÃªnh
Äá»ƒ Ä‘Ã¡nh giÃ¡ cáº¥u hÃ¬nh lá»±a chá»n kÃªnh, chÃºng tÃ´i Ä‘Æ°a ra hai kiá»ƒm tra Ä‘Æ¡n giáº£n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c kÃªnh ná»•i báº­t nhÆ° trong HÃ¬nh 5. Kiá»ƒm tra Ä‘áº§u tiÃªn trong HÃ¬nh 5 (a) chá»©ng minh hiá»‡u quáº£ cá»§a cÃ¡c kÃªnh ná»•i báº­t so vá»›i viá»‡c lá»±a chá»n kÃªnh ngáº«u nhiÃªn. Kiá»ƒm tra thá»© hai trong HÃ¬nh 5 (b) Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a má»—i lá»›p. ChÃºng tÃ´i xÃ³a cÃ¡c kÃªnh ná»•i báº­t tá»«ng lá»›p má»™t. CÃ¡c Ä‘Æ°á»ng cong cho tháº¥y ráº±ng Ä‘á»‘i vá»›i tÃ¡c vá»¥ loáº¡i Tá»± nhiÃªn Caltech101, bá»‘n lá»›p Ä‘áº§u tiÃªn quan trá»ng hÆ¡n bá»‘n lá»›p cuá»‘i cÃ¹ng vÃ¬ Ä‘á»™ chÃ­nh xÃ¡c giáº£m Ä‘Ã¡ng ká»ƒ khi xÃ³a bá»‘n lá»›p Ä‘áº§u tiÃªn. Má»™t xu hÆ°á»›ng tÆ°Æ¡ng tá»± rÃµ rÃ ng trong tÃ¡c vá»¥ Resisc45. Äá»‘i vá»›i tÃ¡c vá»¥ KITTI, bá»‘n lá»›p cuá»‘i cÃ¹ng cÅ©ng quan trá»ng nhÆ° bá»‘n lá»›p Ä‘áº§u.

Dá»±a trÃªn cÃ¡c kiá»ƒm tra nÃªu trÃªn, chÃºng tÃ´i thao tÃ¡c sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c tinh chá»‰nh trong bá»‘n lá»›p Ä‘áº§u tiÃªn vÃ  tÃ¡m lá»›p cuá»‘i cÃ¹ng, nhÆ° Ä‘Æ°á»£c nÃªu trong Báº£ng 1. Viá»‡c tÄƒng sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c tinh chá»‰nh trong bá»‘n lá»›p Ä‘áº§u tiÃªn (hÃ ng thá»© nháº¥t vÃ  thá»© ba) mang láº¡i cáº£i thiá»‡n 0.1% cho Caltech101 vÃ  0.5% cho KITTI, máº·c dÃ¹ cÃ³ sá»± giáº£m 0.1% trong hiá»‡u suáº¥t Resisc45. ÄÃ¡ng chÃº Ã½, áº£nh hÆ°á»Ÿng cá»§a bá»‘n lá»›p cuá»‘i cÃ¹ng Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  nhá». Do Ä‘Ã³, chÃºng tÃ´i khÃ¡m phÃ¡ tÃ¡c Ä‘á»™ng cá»§a viá»‡c giáº£m sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c chá»‰nh trong tÃ¡m lá»›p cuá»‘i cÃ¹ng. CÃ¡c káº¿t quáº£ (hÃ ng thá»© tÆ° vÃ  thá»© nÄƒm) Ä‘Æ°á»£c nÃªu chi tiáº¿t trong Báº£ng 1 chá»‰ ra ráº±ng viá»‡c giáº£m K tá»« 96 xuá»‘ng 32 dáº«n Ä‘áº¿n sá»± suy giáº£m Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t. Tuy nhiÃªn, má»™t giÃ¡ trá»‹ hÆ¡i cao hÆ¡n so vá»›i viá»‡c Ä‘áº·t K= 32 trÃªn táº¥t cáº£ 12 lá»›p.

4 ThÃ­ nghiá»‡m
Pháº§n nÃ y so sÃ¡nh SCT cá»§a chÃºng tÃ´i vá»›i cÃ¡c baseline PEFT tá»‘i tÃ¢n khÃ¡c trÃªn benchmark VTAB-1K, sá»­ dá»¥ng cÃ¡c backbone ViT vÃ  Swin Transformer. NgoÃ i ra, chÃºng tÃ´i phÃ¢n tÃ­ch chiáº¿n lÆ°á»£c lá»±a chá»n kÃªnh, Ä‘iá»ƒm quan trá»ng nháº­n thá»©c lá»›p, vá»‹ trÃ­ chÃ¨n, Ä‘á»™ dÃ i chÃ¨n, vÃ 

--- TRANG 7 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 7
[Biá»ƒu Ä‘á»“ cho tháº¥y káº¿t quáº£ kiá»ƒm tra xÃ³a kÃªnh trÃªn ba táº­p dá»¯ liá»‡u, so sÃ¡nh viá»‡c xÃ³a kÃªnh ngáº«u nhiÃªn vÃ  kÃªnh ná»•i báº­t]

HÃ¬nh 5: Kiá»ƒm tra xÃ³a kÃªnh.

Cáº¥u hÃ¬nh kÃªnh Caltech101 Resisc45 KITTI Params
[96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96] 91.6 86.3 80.2 0.11M
[192, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192] 91.5 87.0 81.0 0.44M
[192, 192, 192, 192, 96, 96, 96, 96, 96, 96, 96, 96] 91.7 86.2 80.7 0.22M
[96, 96, 96, 96, 32, 32, 32, 32, 32, 32, 32, 32] 91.1 85.0 76.2 0.045M
[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32] 90.3 84.8 74.0 0.01M
Báº£ng 1: Sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c tinh chá»‰nh khÃ¡c nhau trÃªn ba tÃ¡c vá»¥.

sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c chá»n Ä‘á»ƒ xÃ¡c minh hiá»‡u quáº£ cá»§a SCT hÆ¡n ná»¯a. NgoÃ i viá»‡c Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t trÃªn cÃ¡c tÃ¡c vá»¥ há»c chuyá»ƒn giao thá»‹ giÃ¡c, chÃºng tÃ´i cÅ©ng xÃ¡c thá»±c kháº£ nÄƒng cá»§a bá»‘n táº­p dá»¯ liá»‡u khÃ¡i quÃ¡t hÃ³a miá»n vÃ  kiá»ƒm tra hiá»‡u suáº¥t cá»§a SCT trong cÃ¡c tÃ¬nh huá»‘ng dá»¯ liá»‡u tháº¥p trÃªn nÄƒm táº­p dá»¯ liá»‡u trong thiáº¿t láº­p few-shot.

4.1 Chi tiáº¿t huáº¥n luyá»‡n
Theo VPT (Jia et al., 2022), chÃºng tÃ´i sá»­ dá»¥ng tÃ¬m kiáº¿m lÆ°á»›i Ä‘á»ƒ tÃ¬m cÃ¡c siÃªu tham sá»‘ Ä‘áº·c thÃ¹ cho tinh chá»‰nh, bao gá»“m tá»‘c Ä‘á»™ há»c vÃ  weight decay, dá»±a trÃªn táº­p val cá»§a má»—i tÃ¡c vá»¥ nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Tab.2. Äá»‘i vá»›i siÃªu tham sá»‘ Scale, Ä‘Ã¢y lÃ  má»™t háº±ng sá»‘ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a thá»§ cÃ´ng Ä‘á»ƒ cÃ¢n báº±ng trá»ng sá»‘ cá»§a Ä‘áº·c trÆ°ng Ä‘Æ°á»£c biáº¿n Ä‘á»•i vÃ  Ä‘áº·c trÆ°ng ná»•i báº­t, chÃºng tÃ´i tÃ¬m kiáº¿m nÃ³ trong pháº¡m vi {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i sá»­ dá»¥ng chiáº¿n lÆ°á»£c tÄƒng cÆ°á»ng hÃ¬nh áº£nh tiÃªu chuáº©n: chuáº©n hÃ³a vá»›i mean vÃ  standard deviation cá»§a ImageNet, resize crop ngáº«u nhiÃªn thÃ nh 224Ã—224, vÃ  láº­t ngang ngáº«u nhiÃªn nhÆ° trong VPT.

4.2 ThÃ­ nghiá»‡m trÃªn Benchmark VTAB-1K
Táº­p dá»¯ liá»‡u. VTAB-1K (Zhai et al., 2019) chá»©a 19 tÃ¡c vá»¥ phÃ¢n loáº¡i thá»‹ giÃ¡c bao gá»“m má»™t phá»• rá»™ng cÃ¡c miá»n vÃ  ngá»¯ nghÄ©a trong ba nhÃ³m, tá»©c lÃ , Tá»± nhiÃªn, ChuyÃªn biá»‡t, vÃ  CÃ³ cáº¥u trÃºc. NhÃ³m Tá»± nhiÃªn chá»©a 7 táº­p dá»¯ liá»‡u phÃ¢n loáº¡i cá»• Ä‘iá»ƒn cá»§a hÃ¬nh áº£nh tá»± nhiÃªn. NhÃ³m ChuyÃªn biá»‡t bao gá»“m 4 táº­p dá»¯ liá»‡u cá»§a hai tÃ¬nh huá»‘ng Ä‘áº·c biá»‡t: y táº¿ vÃ  viá»…n thÃ¡m. NhÃ³m CÃ³ cáº¥u trÃºc cÃ³ 8 táº­p dá»¯ liá»‡u, chá»§ yáº¿u táº­p trung vÃ o hiá»ƒu cáº¥u trÃºc cá»§a má»™t cáº£nh, nhÆ° Ä‘áº¿m Ä‘á»‘i tÆ°á»£ng vÃ  dá»± Ä‘oÃ¡n Ä‘á»™ sÃ¢u. Má»—i tÃ¡c vá»¥ cá»§a VTAB-1K chá»©a 1000 hÃ¬nh áº£nh huáº¥n luyá»‡n. Chi tiáº¿t hÆ¡n cÃ³ sáºµn trong Tab.3.

--- TRANG 8 ---
8 Henry Hengyuan Zhao et al.
VTAB-1K, KhÃ¡i quÃ¡t hÃ³a Miá»n, vÃ  Há»c Few-shot
Optimizer AdamW (Loshchilov and Hutter, 2017)
baselr range {0.1, 0.5, 0.01, 0.05 0.001, 0.005, 0.0001, 0.0005}
Weight decay range {0.1 0.01, 0.05, 0.001, 0.005, 0.0005, 0.0001}
Learning rate schedule cosine decay
Warm up epochs 10
Total epochs 100
Batch size 64
Báº£ng 2: Chi tiáº¿t huáº¥n luyá»‡n trÃªn má»—i tÃ¡c vá»¥ thá»‹ giÃ¡c vá»›i ViT-B/16.

[Báº£ng 3 chá»©a thÃ´ng sá»‘ ká»¹ thuáº­t cá»§a cÃ¡c táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn benchmark VTAB-1K, KhÃ¡i quÃ¡t hÃ³a Miá»n, vÃ  Há»c Few-shot]

Theo Zhang et al. (2022); Jia et al. (2022), chÃºng tÃ´i sá»­ dá»¥ng phÃ¢n chia train-val 800-200 Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c siÃªu tham sá»‘ vÃ  toÃ n bá»™ 1000 dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c top-1 trung bÃ¬nh trÃªn táº­p test.

Baselines. ChÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i ba baseline Full fine-tuning, Linear, vÃ  Bias khÃ´ng cÃ³ tham sá»‘ bÃªn ngoÃ i vÃ  bá»‘n baseline Adapter (Houlsby et al., 2019), LoRA (Hu et al., 2021), VPT (Jia et al., 2022), vÃ  NOAH (Zhang et al., 2022) cÃ³ tham sá»‘ bÃªn ngoÃ i. PhÆ°Æ¡ng phÃ¡p Bias chá»‰ cáº­p nháº­t táº¥t cáº£ cÃ¡c sá»‘ háº¡ng bias trong backbone Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. Adapter chÃ¨n má»™t mÃ´-Ä‘un MLP bá»• sung vÃ o má»—i lá»›p transformer. LoRA Ã¡p dá»¥ng má»™t ma tráº­n thá»© háº¡ng tháº¥p Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho mÃ´-Ä‘un MHSA trong cÃ¡c lá»›p transformer. VPT lÃ  má»™t thuáº­t toÃ¡n prompt thá»‹ giÃ¡c Ä‘á»ƒ káº¿t há»£p cÃ¡c prompt vá»›i token vÃ o backbone. NOAH lÃ  má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng tháº§n kinh káº¿t há»£p Adapter, LoRA, vÃ  VPT vÃ o tÃ¬m kiáº¿m máº¡ng. SSF Ä‘á» xuáº¥t hai yáº¿u tá»‘ cÃ³ thá»ƒ há»c Ä‘á»ƒ chia tá»· lá»‡ vÃ  dá»‹ch chuyá»ƒn cÃ¡c Ä‘áº·c trÆ°ng khi tinh chá»‰nh cÃ¡c tÃ¡c vá»¥ downstream. AdaptFormer Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc giá»‘ng adapter cho cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c. ChÃºng tÃ´i trá»±c tiáº¿p sá»­ dá»¥ng káº¿t quáº£ Ä‘Ã£ phÃ¡t hÃ nh cá»§a há» hoáº·c cháº¡y mÃ£ cá»§a há» Ä‘á»ƒ táº¡o ra chÃºng nháº±m cung cáº¥p má»™t so sÃ¡nh cÃ´ng báº±ng.

--- TRANG 9 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 9
LayerNormMHSALayerNormMLPSCTM
(a) Ours-MLPLayerNormMHSALayerNormMLP
(b) Ours-AttnSCTM
HÃ¬nh 6: Hai loáº¡i cáº¥u trÃºc khi chÃ¨n SCTM vÃ o backbone. Chá»‰ SCTM cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘Æ°á»£c, vÃ  cÃ¡c mÃ´-Ä‘un khÃ¡c Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng.

Hiá»‡u suáº¥t vá»›i backbone ViT. ChÃºng tÃ´i so sÃ¡nh SCT cá»§a chÃºng tÃ´i vá»›i 7 baseline trÃªn trong Tab. 4 vÃ  HÃ¬nh 7. ChÃºng tÃ´i sá»­ dá»¥ng ViT-B/16 lÃ m backbone vÃ  chÃ¨n SCTM vÃ o má»—i lá»›p transformer. K máº·c Ä‘á»‹nh Ä‘Æ°á»£c Ä‘áº·t lÃ  96, 1/8 tá»•ng sá»‘ kÃªnh, dáº«n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n chá»‰ lÃ  0.11M. Thá»© nháº¥t, SCT cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh 73.6% trÃªn 19 táº­p dá»¯ liá»‡u, vÆ°á»£t trá»™i so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ (85.8M) trÃªn 18 trong sá»‘ 19 táº­p dá»¯ liá»‡u vÃ  Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n 6.3%, 2.5%, vÃ  12.3% trong ba nhÃ³m, tÆ°Æ¡ng á»©ng, vá»›i chá»‰ 0.13% tham sá»‘ backbone bá»• sung. Nhá»¯ng káº¿t quáº£ nhÆ° váº­y pháº£n Ã¡nh ráº±ng SCT cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ khÃ´ng gian lÆ°u trá»¯ vÃ  giáº£m bá»›t váº¥n Ä‘á» overfitting phá»• biáº¿n trong tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh lá»›n. Thá»© hai, so vá»›i Adapter (0.16M) (Houlsby et al., 2019) xá»­ lÃ½ táº¥t cáº£ cÃ¡c kÃªnh má»™t cÃ¡ch Ä‘á»“ng Ä‘á»u, viá»‡c chá»n má»™t pháº§n kÃªnh ná»•i báº­t cho má»—i tÃ¡c vá»¥ downstream hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t hÆ¡n, vÆ°á»£t trá»™i hÆ¡n 2.3% vá» Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh. Thá»© ba, so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c, SCT cá»§a chÃºng tÃ´i vÆ°á»£t qua NOAH (0.43M) (Zhang et al., 2022) 0.3% vá»›i chá»‰ má»™t pháº§n tÆ° tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cá»§a nÃ³. Khi tÄƒng K lÃªn 192 vá»›i 0.44M tham sá»‘ bá»• sung, SCT cá»§a chÃºng tÃ´i vÆ°á»£t qua NOAH (0.43) 0.7% Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh. HÆ¡n ná»¯a, SCT cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i VPT (Jia et al., 2022) 3.8%, 3.5%, vÃ  4.9% trong ba nhÃ³m, tÆ°Æ¡ng á»©ng. Nhá»¯ng káº¿t quáº£ nÃ y chá»©ng minh ráº±ng thay vÃ¬ cÃ¡c cáº¥u trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho má»—i táº­p dá»¯ liá»‡u downstream, viá»‡c táº­n dá»¥ng rÃµ rÃ ng thÃ´ng tin Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥ cÃ³ thá»ƒ giáº£m chi phÃ­ tÃ­nh toÃ¡n trong tÃ¬m kiáº¿m mÃ´ hÃ¬nh vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t. HÆ¡n ná»¯a, so vá»›i baseline SSF (0.20M) (Lian et al., 2022), má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cÃ³ mÃ´-Ä‘un cÃ³ thá»ƒ huáº¥n luyá»‡n bá»• sung, SCT cá»§a chÃºng tÃ´i chá»‰ cáº§n gáº§n má»™t ná»­a tham sá»‘ cá»§a nÃ³ vÃ  Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n trung bÃ¬nh 1.0% trÃªn 19 tÃ¡c vá»¥ downstream.

Hiá»‡u suáº¥t vá»›i Backbone Swin Transformer. Äá»ƒ xÃ¡c minh hiá»‡u quáº£ cá»§a SCT vá»›i cÃ¡c backbone khÃ¡c nhau, chÃºng tÃ´i Ã¡p dá»¥ng SCT trÃªn cÃ¡c transformer phÃ¢n cáº¥p, tá»©c lÃ , Swin-B. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng thiáº¿t láº­p chÃ¨n SCTM nhÆ° trong backbone ViT: chÃ¨n SCTM sau khá»‘i "Attn" trong lá»›p transformer. Xem xÃ©t cÃ¡c lá»›p sÃ¢u chá»©a thÃ´ng tin ngá»¯ nghÄ©a hÆ¡n trong cáº¥u trÃºc phÃ¢n cáº¥p, thay vÃ¬ Ã¡p dá»¥ng SCTM trÃªn táº¥t cáº£ cÃ¡c lá»›p transformer, chÃºng tÃ´i chÃ¨n nÃ³ vÃ o ná»­a cuá»‘i cá»§a cÃ¡c lá»›p trong stage 3 vÃ  táº¥t cáº£ cÃ¡c lá»›p cá»§a stage 4 cá»§a Swin-B Ä‘á»ƒ giá»¯ má»©c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n tÆ°Æ¡ng tá»±. Káº¿t quáº£ cá»§a Tab. 5 cho tháº¥y SCT vÆ°á»£t trá»™i so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ trong táº¥t cáº£ 19 tÃ¡c vá»¥ downstream vá»›i chá»‰ 0.12% tham sá»‘ trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c khÃ´ng thá»ƒ. NgoÃ i ra, so vá»›i phÆ°Æ¡ng phÃ¡p PEFT, SCT vÆ°á»£t trá»™i so vá»›i VPT (Jia et al., 2022) 5.9%, 3.0%, vÃ  7.2% trong ba nhÃ³m, tÆ°Æ¡ng á»©ng. Táº¥t cáº£ cÃ¡c káº¿t quáº£ trÃªn cho tháº¥y SCT cá»§a chÃºng tÃ´i cÅ©ng Ã¡p dá»¥ng cho cÃ¡c transformer phÃ¢n cáº¥p vÃ  cÃ³ thá»ƒ mang láº¡i cáº£i thiá»‡n nhiá»u hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c.

4.3 ÄÃ¡nh giÃ¡
Hiá»‡u quáº£ cá»§a Tinh Chá»‰nh KÃªnh Ná»•i Báº­t. ChÃºng tÃ´i so sÃ¡nh ba chiáº¿n lÆ°á»£c lá»±a chá»n kÃªnh Ä‘á»ƒ xÃ¡c minh hiá»‡u quáº£ cá»§a viá»‡c chá»n cÃ¡c kÃªnh ná»•i báº­t trong Tab. 6. CÃ¡c chiáº¿n lÆ°á»£c lá»±a chá»n bao gá»“m Lá»±a chá»n KÃªnh Ná»•i Báº­t (SC), Lá»±a chá»n KÃªnh KhÃ´ng Ná»•i Báº­t (IC), vÃ  Lá»±a chá»n KÃªnh Ngáº«u NhiÃªn (RC). IC chá»n cÃ¡c kÃªnh vá»›i K giÃ¡ trá»‹ tháº¥p nháº¥t cá»§a chuáº©n L2, vÃ  RC biá»ƒu thá»‹ viá»‡c chá»n K kÃªnh ngáº«u nhiÃªn. ChÃºng tÃ´i chá»n ngáº«u nhiÃªn ba táº­p kÃªnh (RC-1/2/3) cho viá»‡c lá»±a chá»n kÃªnh ngáº«u nhiÃªn Ä‘á»ƒ giáº£m bá»›t cÃ¡c ngoáº¡i lá»‡. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Tab. 6, SC Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t vÃ  vÆ°á»£t trá»™i so vá»›i IC 2.4% vá» Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh. Lá»±a chá»n kÃªnh ngáº«u nhiÃªn cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n tinh chá»‰nh Ä‘áº§y Ä‘á»§, nhÆ°ng táº¥t cáº£ chÃºng Ä‘á»u hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n SC. ThÃº vá»‹ lÃ , ngay cáº£ IC cÅ©ng cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n tinh chá»‰nh Ä‘áº§y Ä‘á»§, chá»©ng minh ráº±ng viá»‡c chá»n má»™t táº­p con nhá» cÃ¡c kÃªnh cÃ³ thá»ƒ ngÄƒn mÃ´ hÃ¬nh lá»›n khá»i overfitting vÃ o táº­p huáº¥n luyá»‡n nhá».

Hiá»‡u quáº£ cá»§a TÃ­nh toÃ¡n Nháº­n thá»©c Lá»›p cho Ä‘iá»ƒm quan trá»ng. Trong Má»¥c 3.2, chÃºng tÃ´i giá»›i thiá»‡u phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i, xem xÃ©t tÃ¡c Ä‘á»™ng cá»§a cÃ¡c lá»›p riÃªng láº» thay vÃ¬ coi toÃ n bá»™ táº­p dá»¯ liá»‡u nhÆ° má»™t tá»•ng thá»ƒ khi Æ°á»›c tÃ­nh Ä‘iá»ƒm quan trá»ng (IS). CÃ¡ch tiáº¿p cáº­n nÃ y

--- TRANG 10 ---
10 Henry Hengyuan Zhao et al.
[Báº£ng 4: So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i tÃ¢n trÃªn benchmark VTAB-1K vá»›i ViT-B/16, hiá»ƒn thá»‹ káº¿t quáº£ chi tiáº¿t cho tá»«ng tÃ¡c vá»¥ vÃ  cÃ¡c chá»‰ sá»‘ hiá»‡u suáº¥t]

[HÃ¬nh 7: Káº¿t quáº£ trung bÃ¬nh theo nhÃ³m trÃªn benchmark VTAB-1K]

nháº±m xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh quan trá»ng nháº¥t dá»±a trÃªn thÃ´ng tin chung giá»¯a cÃ¡c danh má»¥c vÃ  giáº£m tÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng. Káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c trÃ¬nh bÃ y trong Tab. 7, chá»©ng minh ráº±ng viá»‡c Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n nháº­n thá»©c lá»›p cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn táº¥t cáº£ ba nhÃ³m.

Vá»‹ trÃ­ ChÃ¨n. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6, SCTM cÃ³ thá»ƒ Ä‘Æ°á»£c chÃ¨n sau khá»‘i MLP (SCT MLP) hoáº·c giá»¯a khá»‘i MHSA vÃ  khá»‘i MLP (SCT Attn). Äá»ƒ Ä‘iá»u tra áº£nh hÆ°á»Ÿng cá»§a vá»‹ trÃ­ chÃ¨n, chÃºng tÃ´i so sÃ¡nh hai hÃ¬nh thá»©c trÃªn benchmark VTAB-1K trong Tab. 8. Cáº£ hai Ä‘á»u Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t Ä‘áº§y há»©a háº¹n, vÃ  SCT Attn vÆ°á»£t trá»™i so vá»›i SCT MLP trÃªn hai trong ba nhÃ³m. ChÃºng tÃ´i Ä‘oÃ¡n ráº±ng sau khi thu tháº­p cÃ¡c phá»¥ thuá»™c táº§m xa vá»›i MHSA, cÃ¡c kÃªnh ná»•i báº­t cá»§a Ä‘áº·c trÆ°ng cÃ³ tÃ­nh Ä‘áº¡i diá»‡n hÆ¡n, cÃ³ thá»ƒ Ä‘Æ°á»£c thÃ­ch á»©ng tá»‘t hÆ¡n vá»›i cÃ¡c tÃ¡c vá»¥ downstream. ChÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a viá»‡c chÃ¨n SCTM sau cáº£ khá»‘i MHSA vÃ  MLP, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Tab. 10. Viá»‡c chÃ¨n chung SCT Attn vÃ  SCT MLP chá»‰ mang láº¡i cáº£i thiá»‡n nhá» trong khi sá»‘ lÆ°á»£ng tham sá»‘ tÄƒng gáº¥p Ä‘Ã´i.

Sá»‘ Ä‘á»™ sÃ¢u chÃ¨n. Äá»™ sÃ¢u chÃ¨n lÃ  má»™t yáº¿u tá»‘ quan trá»ng cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. Äá»ƒ Ä‘iá»u tra Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m báº±ng cÃ¡ch chÃ¨n SCTM vÃ o l lá»›p cuá»‘i cÃ¹ng cá»§a ViT-B vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t trÃªn ba tÃ¡c vá»¥ Ä‘áº¡i diá»‡n tá»« má»—i nhÃ³m cá»§a VTAB-1K. PhÃ¡t hiá»‡n cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c trÃ¬nh bÃ y trong HÃ¬nh 9, cho tháº¥y viá»‡c tÄƒng Ä‘á»™ sÃ¢u chÃ¨n dáº«n Ä‘áº¿n cáº£i thiá»‡n dáº§n dáº§n trong hiá»‡u suáº¥t. Äá»‘i vá»›i cÃ¡c tÃ¡c vá»¥ Resisc45 vÃ  DMLab, chÃºng tÃ´i quan sÃ¡t tháº¥y hiá»‡u suáº¥t cáº¡nh tranh Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c khi SCTM Ä‘Æ°á»£c chÃ¨n vÃ o sÃ¡u lá»›p cuá»‘i cÃ¹ng. Viá»‡c tÄƒng thÃªm Ä‘á»™ sÃ¢u chÃ¨n dáº«n Ä‘áº¿n sá»± á»•n Ä‘á»‹nh trong

--- TRANG 11 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 11
[Báº£ng 5: Káº¿t quáº£ tá»«ng tÃ¡c vá»¥ trÃªn benchmark VTAB-1K vá»›i Swin-B Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c]

[Báº£ng 6-9: CÃ¡c báº£ng so sÃ¡nh hiá»‡u suáº¥t vá»›i cÃ¡c cáº¥u hÃ¬nh vÃ  phÆ°Æ¡ng phÃ¡p khÃ¡c nhau]

hiá»‡u suáº¥t, chá»‰ ra ráº±ng cÃ¡c lá»›p bá»• sung khÃ´ng Ä‘Ã³ng gÃ³p Ä‘Ã¡ng ká»ƒ vÃ o hiá»‡u suáº¥t cuá»‘i cÃ¹ng.

Sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c chá»n K. SiÃªu tham sá»‘ quan trá»ng nháº¥t cá»§a SCTM lÃ  sá»‘ lÆ°á»£ng kÃªnh Ä‘Æ°á»£c chá»n K, áº£nh hÆ°á»Ÿng Ä‘áº¿n kiáº¿n trÃºc mÃ´ hÃ¬nh vÃ  sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n. LÆ°u Ã½ ráº±ng khÃ¡c vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y (Jia et al., 2022; Zhang et al., 2022) chá»n siÃªu tham sá»‘ cho má»—i táº­p dá»¯ liá»‡u, chÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng K cho táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Tab.9, SCT K=32 Ä‘Ã¡nh báº¡i tinh chá»‰nh Ä‘áº§y Ä‘á»§ vÃ  tinh chá»‰nh bias láº§n lÆ°á»£t 3.5% vÃ  7.0%. HÆ¡n ná»¯a, SCT K=32 chá»‰ Ã¡p dá»¥ng 0.01M tham sá»‘ trong khi tinh chá»‰nh sá»‘ háº¡ng bias Ã¡p dá»¥ng 0.10M. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8, hiá»‡u suáº¥t thÆ°á»ng cáº£i thiá»‡n cÃ¹ng vá»›i sá»± gia tÄƒng cá»§a K. Tuy nhiÃªn, cáº£i thiá»‡n cá»§a K= 192 so vá»›i K= 96 lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ, trong khi sá»‘ lÆ°á»£ng tham sá»‘ lá»›n gáº¥p bá»‘n láº§n. Xem xÃ©t cáº£ hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t, chÃºng tÃ´i Ä‘áº·t K thÃ nh 96 theo máº·c Ä‘á»‹nh.

--- TRANG 12 ---
12 Henry Hengyuan Zhao et al.
[Báº£ng cÃ¡c so sÃ¡nh hiá»‡u suáº¥t vÃ  phÃ¢n tÃ­ch tÃ­nh toÃ¡n]

4.4 PhÃ¢n tÃ­ch TÃ­nh toÃ¡n
PhÃ¢n tÃ­ch cá»§a chÃºng tÃ´i xem xÃ©t má»™t backbone ViT-B vá»›i L lá»›p vÃ  D chiá»u vÃ  N token cho má»™t hÃ¬nh áº£nh duy nháº¥t. ChÃºng tÃ´i cÅ©ng giáº£ sá»­ ráº±ng chiá»u trung gian cá»§a Adapter (Houlsby et al., 2019) lÃ  Dâ€², Ä‘á»™ dÃ i prompt cá»§a VPT (Jia et al., 2022) lÃ  n, vÃ  tá»•ng sá»‘ láº§n chÃ¨n cá»§a SSF (Lian et al., 2022) lÃ  m. Cuá»‘i cÃ¹ng, chÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p SCT Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i vá»›i Adapter, VPT, vÃ  SSF vá» tham sá»‘ vÃ  FLOPs, nhÆ° Ä‘Æ°á»£c tÃ³m táº¯t trong Tab. 12. ÄÃ¡ng chÃº Ã½, viá»‡c chá»n K cá»§a chÃºng tÃ´i lÃ  1/8 D khÃ¡ nhá» so vá»›i D. Khi tá»· lá»‡ giáº£m cá»§a Adapter cÅ©ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  8, cÃ¡c tham sá»‘ cá»§a Adapter lÃ  1/4 LDÂ² trong khi tham sá»‘ cá»§a chÃºng tÃ´i lÃ  1/64 LDÂ². Khi chÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i SSF, chÃ¨n mÃ´-Ä‘un cá»§a nÃ³ m láº§n trong backbone ViT-B, sá»‘ lÆ°á»£ng tham sá»‘ cho SSF lÃ  mLD. Kiá»ƒm tra backbone ViT-B, chÃºng tÃ´i tháº¥y ráº±ng m= 74 vÃ  1/64 D= 12, tham sá»‘ cá»§a chÃºng tÃ´i (12 LD < 74LD) vÃ  FLOPs (12NLD < 74NLD) nhá» hÆ¡n SSF. NhÃ¬n chung, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i cho tháº¥y SCT cÃ³ thá»ƒ cung cáº¥p cho PEFT má»™t baseline hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t hÆ¡n.

Äá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ tÃ­nh toÃ¡n cá»§a phÆ°Æ¡ng phÃ¡p SCT cá»§a chÃºng tÃ´i, chÃºng tÃ´i chá»n so sÃ¡nh nÃ³ vá»›i Adapter (0.16M) vÃ  SSF (0.20M) vÃ¬ chÃºng cÃ³ má»©c tham sá»‘ tÆ°Æ¡ng tá»± vÃ  táº¥t cáº£ chÃºng Ä‘á»u Ä‘Æ°á»£c Ã¡p dá»¥ng trong máº¡ng backbone. ChÃºng tÃ´i Ä‘o thá»i gian cháº¡y vÃ  sá»­ dá»¥ng bá»™ nhá»› cho cáº£ giai Ä‘oáº¡n huáº¥n luyá»‡n vÃ  kiá»ƒm tra sá»­ dá»¥ng kÃ­ch thÆ°á»›c batch lÃ  64 trÃªn táº­p dá»¯ liá»‡u Cifar100 (VTAB-1K) vá»›i backbone ViT-B. ChÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m huáº¥n luyá»‡n trÃªn má»™t GPU NVIDIA V100-32GB duy nháº¥t vÃ  cÃ¡c thÃ­ nghiá»‡m kiá»ƒm tra trÃªn má»™t GPU NVIDIA A100-40GB duy nháº¥t. VÃ¬ viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU trong giai Ä‘oáº¡n huáº¥n luyá»‡n vÃ  kiá»ƒm tra lÃ  khÃ¡c nhau. Trong giai Ä‘oáº¡n huáº¥n luyá»‡n, cÃ¡c kÃ­ch hoáº¡t tensor, tham sá»‘ mÃ´ hÃ¬nh, gradient, vÃ  tráº¡ng thÃ¡i optimizer (mÃ´ hÃ¬nh, gradient, momentum) lÃ  cÃ¡c nguá»“n chÃ­nh. Trong giai Ä‘oáº¡n kiá»ƒm tra, cÃ¡c kÃ­ch hoáº¡t tensor vÃ  tham sá»‘ mÃ´ hÃ¬nh lÃ  cÃ¡c nguá»“n chÃ­nh. Káº¿t quáº£ cá»§a chÃºng tÃ´i, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 10, chá»‰ ra ráº±ng phÆ°Æ¡ng phÃ¡p SCT cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i Adapter vÃ  SSF vá» thá»i gian huáº¥n luyá»‡n vÃ  sá»­ dá»¥ng bá»™ nhá»›. VÃ¬ cÃ¡c tham sá»‘ cÃ³ thá»ƒ chá»‰nh cá»§a chÃºng tÃ´i nhá» hÆ¡n Adapter vÃ  SSF, chi phÃ­ gradient vÃ  tráº¡ng thÃ¡i optimizer nhá» giÃºp chÃºng tÃ´i tiáº¿t kiá»‡m bá»™ nhá»› GPU trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Cá»¥ thá»ƒ

--- TRANG 13 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 13
[Biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ viá»‡c chá»n cÃ¡c giÃ¡ trá»‹ K khÃ¡c nhau trÃªn 19 tÃ¡c vá»¥ downstream vÃ  káº¿t quáº£ chÃ¨n SCTM vÃ o cÃ¡c lá»›p khÃ¡c nhau]

thá»ƒ, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i chá»‰ yÃªu cáº§u 12 tÃ­nh toÃ¡n SCT bá»• sung, trong khi SSF yÃªu cáº§u 74 phÃ©p toÃ¡n chia tá»· lá»‡ vÃ  dá»‹ch chuyá»ƒn. Viá»‡c huáº¥n luyá»‡n SSF khÃ´ng khÃ¡ hiá»‡u quáº£ so vá»›i SCT cá»§a chÃºng tÃ´i. Tuy nhiÃªn, trong quÃ¡ trÃ¬nh kiá»ƒm tra, thá»i gian kiá»ƒm tra cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i hÆ¡i cao hÆ¡n Adapter vÃ  SSF. LÆ°u Ã½, SSF cÃ³ thá»ƒ há»£p nháº¥t cÃ¡c tham sá»‘ bá»• sung cá»§a nÃ³ vÃ o backbone gá»‘c mÃ  khÃ´ng cÃ³ chi phÃ­ suy luáº­n bá»• sung trong khi phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i khÃ´ng táº­p trung vÃ o viá»‡c giáº£m chi phÃ­ tÃ­nh toÃ¡n trong quÃ¡ trÃ¬nh suy luáº­n. LÃ½ do cho chi phÃ­ kiá»ƒm tra cao hÆ¡n so vá»›i Adapter cÃ³ thá»ƒ lÃ  cÃ¡c phÃ©p toÃ¡n bá»• sung cá»§a viá»‡c truy váº¥n cÃ¡c kÃªnh ná»•i báº­t tá»« má»™t Ä‘áº·c trÆ°ng Ä‘áº§y Ä‘á»§ vÃ  sau Ä‘Ã³ Ä‘Æ°a cÃ¡c kÃªnh ná»•i báº­t Ä‘Æ°á»£c chiáº¿u trá»Ÿ láº¡i. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘Æ°a ra káº¿t quáº£ chi phÃ­ thá»i gian kiá»ƒm tra chÃ­nh xÃ¡c trong Tab. 11. á» Ä‘Ã¢y LoRA vÃ  SSF cÃ³ thá»ƒ há»£p nháº¥t tham sá»‘ cá»§a nÃ³ vÃ o máº¡ng backbone do Ä‘Ã³ chÃºng tÃ´i chá»n Fine-tuning Ä‘áº§y Ä‘á»§ lÃ m Ä‘áº¡i diá»‡n.

4.5 ThÃ­ nghiá»‡m vá» KhÃ¡i quÃ¡t hÃ³a Miá»n
Táº­p dá»¯ liá»‡u. NgoÃ i viá»‡c Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u kiá»ƒm tra cá»§a cÃ¹ng phÃ¢n phá»‘i, cÃ¡c máº¡ng tháº§n kinh sÃ¢u hiá»‡n Ä‘áº¡i thÆ°á»ng gáº·p pháº£i sá»± suy giáº£m hiá»‡u suáº¥t khi phÃ¢n phá»‘i kiá»ƒm tra khÃ¡c vá»›i phÃ¢n phá»‘i

--- TRANG 14 ---
14 Henry Hengyuan Zhao et al.
PhÆ°Æ¡ng phÃ¡p Thá»i gian kiá»ƒm tra Bá»™ nhá»› Tham sá»‘
Tinh chá»‰nh Ä‘áº§y Ä‘á»§ 5.78 ms 336 MB 85.8 M
Adapter-8 6.87 ms 337 MB 85.8+0.16 M
AdaptFormer-8 7.88 ms 337 MB 85.8+0.18 M
SCT (Cá»§a chÃºng tÃ´i) 8.43 ms 337 MB 85.8+0.11 M
Báº£ng 11: ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ thá»i gian kiá»ƒm tra cho 1 hÃ¬nh áº£nh vá»›i Ä‘á»™ phÃ¢n giáº£i 224. ChÃºng tÃ´i cháº¡y 200 láº§n vÃ  táº¡o ra káº¿t quáº£ trung bÃ¬nh.

Adapter VPT-Deep SSF TTC-Module
# Tham sá»‘ Bá»• sung 2LDDâ€² nLD mLD LKK
# FLOPs Bá»• sung 2NLDDâ€² 2n(2N+n)LD mNLD NLKK
Báº£ng 12: PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p cá»§a Adapter (Houlsby et al., 2019), VPT (Jia et al., 2022), SSF (Lian et al., 2022), vÃ  TTC-Module Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i.

cá»§a táº­p huáº¥n luyá»‡n, tá»©c lÃ , dá»‹ch chuyá»ƒn miá»n, Ä‘iá»u nÃ y khÃ´ng thá»ƒ trÃ¡nh khá»i trong á»©ng dá»¥ng thá»±c táº¿. Äá»ƒ giáº£m bá»›t váº¥n Ä‘á» nÃ y, khÃ¡i quÃ¡t hÃ³a miá»n (Zhou et al., 2021b; Zhao et al., 2022; Zhou et al., 2022c; Yang et al., 2022a,b, 2021) Ä‘Æ°á»£c Ä‘iá»u tra trong cá»™ng Ä‘á»“ng, nháº±m huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh vá»›i má»™t hoáº·c nhiá»u miá»n nguá»“n cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t trÃªn cÃ¡c miá»n má»¥c tiÃªu chÆ°a tháº¥y khÃ¡c. Äá»ƒ xÃ¡c minh kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a cá»§a SCT cá»§a chÃºng tÃ´i, chÃºng tÃ´i theo Zhang et al. (2022) Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m trÃªn ImageNet vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³. Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng ImageNet-1K (Deng et al., 2009) lÃ m miá»n nguá»“n vá»›i 16-shot trÃªn má»—i danh má»¥c vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trÃªn ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), vÃ  ImageNet-R (Hendrycks et al., 2021a). ImageNetV2 (Recht et al., 2019) Ä‘Æ°á»£c thu tháº­p tá»« cÃ¡c nguá»“n khÃ¡c nhau tá»« ImageNet-1K vá»›i cÃ¹ng giao thá»©c, vÃ  ImageNet-Sketch (Wang et al., 2019) chá»©a cÃ¡c hÃ¬nh áº£nh phÃ¡c tháº£o cá»§a cÃ¡c lá»›p ImageNet. Cáº£ hai Ä‘á»u sá»­ dá»¥ng cÃ¡c lá»›p giá»‘ng nhÆ° ImageNet-1K. ImageNet-A (Hendrycks et al., 2021b) vÃ  ImageNet-R (Hendrycks et al., 2021a) chá»©a cÃ¡c hÃ¬nh áº£nh Ä‘Æ°á»£c lá»c Ä‘á»‘i nghá»‹ch vÃ  cÃ¡c thá»ƒ hiá»‡n cá»§a dá»¯ liá»‡u ImageNet cá»§a má»™t táº­p con 200 lá»›p, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i sá»­ dá»¥ng phiÃªn báº£n lá»›n cá»§a SCT, tá»©c lÃ , SCT-B, chá»©a tham sá»‘ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i NOAH (0.44M so vá»›i 0.43M).

Káº¿t quáº£. Trong Tab. 13, chÃºng tÃ´i so sÃ¡nh SCT-B cá»§a chÃºng tÃ´i vá»›i Adapter (Houlsby et al., 2019), VPT (Jia et al., 2022), LoRA (Hu et al., 2021), vÃ  NOAH (Zhang et al., 2022) trÃªn cÃ¡c táº­p dá»¯ liá»‡u trÃªn Ä‘á»ƒ xÃ¡c minh kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a. ChÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n hai quan sÃ¡t. Thá»© nháº¥t, SCT-B vÆ°á»£t trá»™i so vá»›i phÆ°Æ¡ng phÃ¡p tá»‘t nháº¥t trÆ°á»›c Ä‘Ã¢y (NOAH) trÃªn ba trong sá»‘ bá»‘n táº­p dá»¯ liá»‡u má»¥c tiÃªu vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn ImageNetV2. Cá»¥ thá»ƒ, SCT-B mang láº¡i cáº£i thiá»‡n 2.5% trÃªn ImageNet-R so vá»›i NOAH. Thá»© hai, SCT-B cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c 77.1% trÃªn miá»n nguá»“n, vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y 6%. VÃ¬ mÃ´ hÃ¬nh backbone Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn ImageNet-21K, káº¿t quáº£ trÃªn ImageNet-1K cho tháº¥y SCT cÃ³ thá»ƒ tÄƒng cÆ°á»ng tá»‘t hÆ¡n viá»‡c chuyá»ƒn giao kiáº¿n thá»©c tá»« táº­p con Ä‘áº¿n táº­p cha. Hai quan sÃ¡t chá»©ng minh sá»± Æ°u viá»‡t cá»§a SCT cá»§a chÃºng tÃ´i so vá»›i cÃ¡c ká»¹ thuáº­t tinh chá»‰nh trÆ°á»›c Ä‘Ã¢y vá» kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a máº¡nh máº½.

4.6 ThÃ­ nghiá»‡m vá» Há»c Few-shot
Táº­p dá»¯ liá»‡u. Theo Zhang et al. (2022), chÃºng tÃ´i chá»n nÄƒm táº­p dá»¯ liá»‡u nháº­n dáº¡ng thá»‹ giÃ¡c tinh vi, bao gá»“m Food101 (Bossard et al., 2014), Flowers102 (Nilsback and Zisserman, 2006), StandfordCars (Krause et al., 2013), OxfordPets (Parkhi et al., 2012), vÃ  FGVCAircraft (Maji et al., 2013), Ä‘á»ƒ Ä‘iá»u tra hiá»‡u quáº£ cá»§a SCT trong há»c few-shot. CÃ¡c danh má»¥c trong cÃ¡c táº­p dá»¯ liá»‡u nÃ y bao gá»“m cÃ¡c khÃ¡i niá»‡m thá»‹ giÃ¡c khÃ¡c nhau liÃªn quan cháº·t cháº½ Ä‘áº¿n cuá»™c sá»‘ng hÃ ng ngÃ y cá»§a chÃºng ta: thá»±c pháº©m, thá»±c váº­t, phÆ°Æ¡ng tiá»‡n, vÃ  Ä‘á»™ng váº­t. Tiáº¿p theo, chÃºng tÃ´i theo cÃ¡c nghiÃªn cá»©u hiá»‡n cÃ³ (Zhang et al., 2022; Jie and Deng, 2022) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i dÆ°á»›i cÃ¡c thiáº¿t láº­p 1, 2, 4, 8, vÃ  16 shot. Thiáº¿t láº­p thÃ­ nghiá»‡m giá»‘ng nhÆ° trong VTAB-1K.

Káº¿t quáº£. Theo HÃ¬nh 11, SCT cá»§a chÃºng tÃ´i vÆ°á»£t qua cÃ¡c baseline khÃ¡c trong táº¥t cáº£ cÃ¡c thiáº¿t láº­p vá» hiá»‡u suáº¥t trung bÃ¬nh. NgoÃ i ra, SCT chá»§ yáº¿u Ä‘áº¡t Ä‘Æ°á»£c lá»£i tháº¿ táº¡i cÃ¡c táº­p dá»¯ liá»‡u FGVCAircraft vÃ  OxfordPets. Äá»‘i vá»›i cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c, SCT Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh so vá»›i cÃ¡c baseline khÃ¡c. Nhá»¯ng káº¿t quáº£ nhÆ° váº­y chá»©ng minh ráº±ng SCT cÃ³ kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a máº¡nh máº½ cÃ³ thá»ƒ Ä‘Æ°á»£c chuyá»ƒn giao dá»… dÃ ng Ä‘áº¿n cÃ¡c tÃ¡c vá»¥ downstream chá»‰ vá»›i má»™t vÃ i máº«u.

5 Káº¿t luáº­n
BÃ i bÃ¡o nÃ y giá»›i thiá»‡u má»™t baseline Ä‘Æ¡n giáº£n cho PEFT, vá»«a Ä‘Æ¡n giáº£n vá»«a hiá»‡u quáº£. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y, chÃºng tÃ´i Ã¡p dá»¥ng gÃ³c nhÃ¬n lá»±a chá»n kÃªnh vÃ  Ä‘á» xuáº¥t má»™t Ä‘iá»ƒm quan trá»ng Ä‘Æ¡n giáº£n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c kÃªnh Ä‘áº·c thÃ¹ cho tÃ¡c vá»¥. Báº±ng cÃ¡ch loáº¡i bá» cÃ¡c phÃ©p toÃ¡n giáº£m máº«u vÃ  phi tuyáº¿n, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i so vá»›i phÆ°Æ¡ng phÃ¡p Adapter tÆ°Æ¡ng tá»±, yÃªu cáº§u Ã­t tham sá»‘ hÆ¡n. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ thuáº­t toÃ¡n cá»§a chÃºng tÃ´i trÃªn 19 tÃ¡c vá»¥ downstream vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t vá»›i chá»‰ Ã­t hÆ¡n 780 Ã— tham sá»‘ so vá»›i backbone gá»‘c. ChÃºng tÃ´i cÅ©ng kiá»ƒm tra phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i trÃªn bá»‘n táº­p dá»¯ liá»‡u vá»›i dá»‹ch chuyá»ƒn phÃ¢n phá»‘i tá»± nhiÃªn Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a miá»n vÃ  trÃªn nÄƒm táº­p dá»¯ liá»‡u trong tÃ¬nh huá»‘ng few-shot Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t trong cháº¿ Ä‘á»™ dá»¯ liá»‡u tháº¥p. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n trong cáº£ quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  kiá»ƒm tra. HÆ¡n ná»¯a, nÃ³ Ä‘á»™c láº­p vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn prompt vÃ  giá»‘ng LoRa, cho phÃ©p khÃ¡m phÃ¡ thÃªm trong nghiÃªn cá»©u tÆ°Æ¡ng lai. Máº·c dÃ¹ ká»¹ thuáº­t lá»±a chá»n kÃªnh cá»§a chÃºng tÃ´i Ä‘Æ¡n giáº£n, váº«n cÃ³ tiá»m nÄƒng Ä‘Ã¡ng ká»ƒ

--- TRANG 15 ---
Tinh Chá»‰nh KÃªnh Ná»•i Báº­t 15
Nguá»“n Má»¥c tiÃªu
ImageNet -V2 -Sketch -A -R
Adapter (Houlsby et al., 2019) 70.5 59.1 16.4 5.5 22.1
VPT (Jia et al., 2022) 70.5 58.0 18.3 4.6 23.2
LoRA (Hu et al., 2021) 70.8 59.3 20.0 6.9 23.3
NOAH (Zhang et al., 2022) 71.5 66.1 24.8 11.9 28.5
SCT-B (cá»§a chÃºng tÃ´i) 77.1 65.8 28.5 12.1 31.0
Báº£ng 13: So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y vá» khÃ¡i quÃ¡t hÃ³a miá»n. Máº¡ng backbone lÃ  ViT-B/16. Sá»‘ lÆ°á»£ng kÃªnh ná»•i báº­t lÃ  192. VÄƒn báº£n in Ä‘áº­m Ä‘áº¡i diá»‡n cho hiá»‡u suáº¥t tá»‘t nháº¥t.

[HÃ¬nh 10: Chi phÃ­ tÃ­nh toÃ¡n cá»§a cÃ¡c giai Ä‘oáº¡n huáº¥n luyá»‡n vÃ  kiá»ƒm tra]

[HÃ¬nh 11: Káº¿t quáº£ há»c few-shot trÃªn nÄƒm táº­p dá»¯ liá»‡u nháº­n dáº¡ng thá»‹ giÃ¡c tinh vi]

Ä‘á»ƒ nÃ¢ng cao hiá»‡u suáº¥t cá»§a nÃ³. Baseline cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c tÃ¬nh huá»‘ng háº¡n cháº¿ tÃ i nguyÃªn vÃ¬ nÃ³ yÃªu cáº§u Ã­t tham sá»‘ bá»• sung hÆ¡n vÃ  cho phÃ©p thÃ­ch á»©ng nhanh chÃ³ng vá»›i cÃ¡c tÃ¡c vá»¥ má»›i chá»‰ vá»›i má»™t vÃ i máº«u, lÃ m cho cÃ¡c mÃ´-Ä‘un Ä‘Æ¡n giáº£n vÃ  nhá» trá»Ÿ thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ lÆ°u trá»¯ kiáº¿n thá»©c hiá»‡u quáº£.

--- TRANG 16 ---
16 Henry Hengyuan Zhao et al.
Lá»i cáº£m Æ¡n
NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Quá»¹ NghiÃªn cá»©u Quá»‘c gia, Singapore vÃ  A*STAR, dÆ°á»›i chÆ°Æ¡ng trÃ¬nh RIE2020 Industry Alignment Fund â€“ Industry Collaboration Projects (IAF-ICP) (Sá»‘ hiá»‡u Grant I2001E0059) â€“ SIA-NUS Digital Aviation Corp Lab. Mike Zheng Shou Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Quá»¹ NghiÃªn cá»©u Quá»‘c gia, Singapore, dÆ°á»›i Giáº£i thÆ°á»Ÿng NRFF NRF-NRFF13-2021-0008, vÃ  Start-Up Grant cá»§a Mike Zheng Shou tá»« NUS.

TÃ i liá»‡u tham kháº£o
[Danh sÃ¡ch tÃ i liá»‡u tham kháº£o Ä‘áº§y Ä‘á»§ tá»« trang 16-19 vá»›i cÃ¡c trÃ­ch dáº«n há»c thuáº­t vá» vision transformers, parameter-efficient fine-tuning, vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p liÃªn quan]
