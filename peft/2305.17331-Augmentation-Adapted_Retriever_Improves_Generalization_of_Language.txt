# 2305.17331.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.17331.pdf
# File size: 1026060 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Augmentation-Adapted Retriever Improves Generalization of Language
Models as Generic Plug-In
Zichun Yu1Chenyan Xiong2Shi Yu1Zhiyuan Liu13
1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China
2Microsoft Research, Redmond, USA
3Beijing National Research Center for Information Science and Technology, Beijing, China
{yuzc19, yus21}@mails.tsinghua.edu.cn ;chenyan.xiong@microsoft.com
liuzy@tsinghua.edu.cn
Abstract
Retrieval augmentation can aid language
models (LMs) in knowledge-intensive tasks
by supplying them with external information.
Prior works on retrieval augmentation usually
jointly fine-tune the retriever and the LM,
making them closely coupled. In this paper, we
explore the scheme of generic retrieval plug-in:
the retriever is to assist target LMs that may
not be known beforehand or are unable to
be fine-tuned together. To retrieve useful
documents for unseen target LMs, we propose
augmentation-adapted retriever (AAR), which
learns LM’s preferences obtained from a
known source LM. Experiments on the MMLU
and PopQA datasets demonstrate that our AAR
trained with a small source LM is able to signif-
icantly improve the zero-shot generalization of
larger target LMs ranging from 250M Flan-T5
to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap,
enabling AAR trained with a single source
LM to serve as a generic plug-in for various
target LMs. Our code is open-sourced at
https://github.com/OpenMatch/Augmentation-
Adapted-Retriever.
1 Introduction
Large language models (LMs) that possess bil-
lions of parameters are able to capture a signif-
icant amount of human knowledge, leading to
consistent improvements on various downstream
tasks (Brown et al., 2020; Kaplan et al., 2020;
Roberts et al., 2020). However, the undeniable
drawback of large LMs lies in their high compu-
tational cost, which negatively impacts their effi-
ciency (Strubell et al., 2019; Bender et al., 2021).
Furthermore, the knowledge memorized from pre-
training and the implicit reasoning process of LMs
can be inaccurate and intractable sometimes, hin-
dering their applications on knowledge-intensive
tasks (Guu et al., 2020; Lewis et al., 2020; Mallen
et al., 2022; Wei et al., 2022).
Flan-T5Base
(250M)Flan-T5Large
(780M)Flan-T5XL
(3B)InstructGPT
(175B)
# Parameters35404550556065MMLU AccuracyStandalone LM
LM w/ Few-Shot Prompting
LM w/ Adaptive Retrieval
LM w/ AAR (Ours)Figure 1: Performance of LM w/ AAR (Ours).
Instead of leveraging the knowledge and rea-
soning abilities embedded within the parameters
of the LMs, retrieval augmentation (Guu et al.,
2020; Lewis et al., 2020; Borgeaud et al., 2022)
enhances the LM with a retriever that can retrieve
knowledge from an external corpus. On the other
hand, prior retrieval augmentation methods (Izac-
ard and Grave, 2021a; Izacard et al., 2022) necessi-
tate fine-tuning the backbone LM to adjust to the
retriever and tackle specific downstream tasks. This
kind of fine-tuning can be expensive when more
and more unique demands emerge (Maronikolakis
and Schütze, 2021). More importantly, many top-
tier LMs can only be accessed through black-box
APIs (Ouyang et al., 2022; OpenAI, 2023). These
APIs allow users to submit queries and receive re-
sponses but typically do not support fine-tuning.
In this paper, we introduce Augmentation-
Adapted Retriever (AAR) to assist black-box LMs
with downstream tasks as generic plug-in . To re-
trieve valuable documents for many unseen LMs,
we propose to leverage a small source LM to pro-
vide LM-preferred signals for retriever’s training.
The retriever after training (i.e., AAR) can be di-
rectly utilized to assist a large target LM by plug-
ging in the retrieved documents.
Specifically, we choose a small encoder-decoder
LM as the source LM and utilize its fusion-arXiv:2305.17331v1  [cs.CL]  27 May 2023

--- PAGE 2 ---
in-decoder attention scores (Izacard and Grave,
2021a) to annotate LM-preferred documents. The
LM-preferred documents are then combined with
human-preferred documents to form the positive
document set. Negative documents are mined by
the retriever itself using the ANCE (Xiong et al.,
2021) technique. After fine-tuning the retriever
with LM’s preferences, it can directly assist unseen
target LMs in the zero-shot task generalization.
We evaluate AAR on a multi-task language
understanding dataset MMLU (Hendrycks et al.,
2021) and an entity-centric question answering
dataset PopQA (Mallen et al., 2022). For the tar-
get LMs, we choose Flan-T5 (Chung et al., 2022)
series as our backbone for encoder-decoder LMs
and InstructGPT (Ouyang et al., 2022) as our back-
bone for decoder-only LMs. Figure 1 shows that
assisted with a generic AAR, LMs of different sizes
and architectures can consistently outperform the
standalone LMs; the performance of smaller LMs
can sometimes surpass the standalone counterparts
of significantly larger sizes (e.g., Flan-T5 Large w/
AAR outperforms standalone Flan-T5 XLby 0.6%).
AAR also demonstrates advantages over other aug-
mentation approaches such as few-shot prompting
and adaptive retrieval (Mallen et al., 2022).
Further analysis reveals that the preferences ob-
tained from different-sized source LMs are similar,
and LMs with near capacities tend to yield closer
preferred document sets. As a result, our AAR
model trained from a small source LM can be con-
sidered as a generic plug-in to enhance the zero-
shot generalization of a significantly larger target
LM. We also discover that the documents preferred
by LMs can provide assistance to the model from
alternative perspectives, rather than relying solely
on the full information favored by search users.
2 Related Work
Retrieval Augmentation. Augmenting LMs with
retrieved information from external memories has
shown effective on diverse knowledge-intensive
tasks (Guu et al., 2020). Prior works explore
novel ways to train the whole retriever-LM sys-
tem in an end-to-end fashion, using retrieval-
augmented sequence log-likelihood (Lewis et al.,
2020; Borgeaud et al., 2022), fusion-in-decoder
attention distillation (Izacard and Grave, 2021a;
Izacard et al., 2022), or knowledge graph (Ju et al.,
2022). To decouple the retriever from LM, Rubin
et al. (2022) train an independent prompt retrieverfor in-context learning, and Lin et al. (2022) only
fine-tune the LM via the retrieved data that is simi-
lar to few-shot unsupervised samples.
Recent researches adopt zero-shot retrieval aug-
mentation that does not fine-tune the LM on In-
structGPT (Ouyang et al., 2022). It can benefit
entity-centric question answering (Mallen et al.,
2022), chain-of-thought reasoning (He et al., 2022),
and multi-hop question answering (Khattab et al.,
2022). Parallel work (Shi et al., 2023) uses LM
likelihood to train the retriever for satisfying black-
box LM’s preferences, and they adopt GPT-3
Curie (Brown et al., 2020) to provide the super-
vision signals. In this work, we devise the retriever
that can be used as a generic plug-in to assist a
variety of unseen LMs.
Zero-shot Learning and Reasoning. Large-
scale unsupervised pre-trained LMs like GPT-
3 (Brown et al., 2020), GPT-4 (OpenAI, 2023),
and PaLM (Chowdhery et al., 2022) are able to
perform zero-shot learning on many downstream
tasks with a task description provided at inference
time. Instruction-finetuned LMs (Sanh et al., 2022;
Chung et al., 2022; Ouyang et al., 2022), which
are pre-trained on multiple supervised tasks using
human instructions, also also exhibit robust zero-
shot learning capabilities. Yu et al. (2023) pro-
pose a new scheme of zero-shot reasoning, which
first prompts large LMs to generate relevant docu-
ments and then perform reading comprehension
on the generated contents. Recently, there has
been a growing trend of utilizing plug-and-play
knowledge injection to enhance the zero-shot per-
formance of LMs, which is achieved through map-
ping network (Zhang et al., 2023) or document
encoding (Xiao et al., 2023). Our work improves
the zero-shot generalization of LMs by utilizing the
retrieved information. We demonstrate that identi-
fying LMs’ preferences to train the retriever can in
turn bring additional evidence texts for LMs.
3 Method
In this section, we first introduce the preliminaries
of the dense retrieval and the retrieval-augmented
LM (§ 3.1), then propose our augmentation-
adapted retriever (§ 3.2).
3.1 Preliminaries
Retrieval-augmented LM (Guu et al., 2020; Lewis
et al., 2020) is a type of LM that leverages external
information to improve its performance. It retrieves

--- PAGE 3 ---
relevant documents from a corpus using a retriever,
and then utilizes the documents to enhance its lan-
guage generation capabilities.
The objective of the retriever is to find an aug-
mentation document set Dafrom a corpus Cthat
helps the LM handle a given query q. Previous
researches (Karpukhin et al., 2020; Xiong et al.,
2021) concentrate primarily on the dense retrieval
system that searches in the dense vector space since
dense retrieval usually performs more accurately
and efficiently than sparse one.
A dense retrieval model first represents qand
the document dinto an embedding space using a
pre-trained encoder g,
q=g(q);d=g(d), d∈C, (1)
and match their embeddings by dot product func-
tionf, which supports fast approximate nearest
neighbor search (ANN) (André et al., 2016; John-
son et al., 2021). We then define Dathat contains
top-Nretrieved documents as:
Da={da
1. . . da
N}=ANNN
f(q,◦). (2)
For the LM backbones, the decoder-only and
the encoder-decoder models are the two primary
choices of the retrieval-augmented LMs (Izacard
and Grave, 2021b; Yu et al., 2023).
Given a decoder-only LM like GPT-3 (Brown
et al., 2020), the LM input can be a simple concate-
nation of the query and all the augmentation docu-
ments{da
1. . . da
N}. Then, the LM will generate the
answer based on the inputs auto-regressively.
For an encoder-decoder LM like T5 (Raffel et al.,
2020), taking simple concatenation as the encoder
input may still be effective. However, this method
may not scale to a large volume of documents due
to the quadratic self-attention computation associ-
ated with the number of documents. To aggregate
multiple documents more efficiently, Izacard and
Grave (2021b) propose the fusion-in-decoder (FiD)
mechanism, which soon becomes the mainstream
in the development of encoder-decoder retrieval-
augmented LMs. It first encodes each concatena-
tion of the ( da
i,q) pair separately and then lets the
decoder attend to all parts:
FiD(q) =Dec(Enc(da
1⊕q). . .Enc(da
N⊕q)).(3)
In this way, the encoder computes self-attention
over one document at a time so that the compu-
tational cost can grow linearly with the number
of documents. Furthermore, FiD cross-attention
is found effective in estimating the relative im-
portance of the augmentation documents from
Negatives  
ANCE Sampling  Positives  
Ground T ruth  
Top-K FiDAtt  
  
  
 Pre-T rained R etrieverQ + D1 ...Enc EncDec
Enc...Source LM
Fusion-in-Decoder
Retrieve  
N Docs  Source T ask Q + D2 Q + DN
Augmentation-Adapted R etriever
Target LMs Target T asksGeneric  
Plug-In  
Figure 2: Illustration of augmentation-adapted retriever.
the LM’s perspective (Izacard and Grave, 2021a).
Therefore, soft FiD distillation (Izacard and Grave,
2021a; Izacard et al., 2022; Shi et al., 2023), which
minimizes the KL-divergence between retrieval
likelihood and LM likelihood, is often used to train
the retriever and the LM end-to-end.
3.2 Augmentation-adapted Retriever
Due to the emerging real-world demands and
the limitations of black-box APIs, fine-tuning
retrieval-augmented LM for each possible down-
stream task can be infeasible. Hence, we intro-
duce Augmentation- Adapted Retriever (AAR) as a
generic plug-in for black-box LMs. As illustrated
in Figure 2, AAR can learn the preferences of LMs
without the need for fine-tuning them.
Specifically, we utilize an encoder-decoder LM
as source LM ( Ls) to provide LM-preferred signals
on a source task ( Ts) for fine-tuning a pre-trained
retriever. Then, we plug the fine-tuned retriever
into unseen target LM ( Lt) on a set of target tasks
(Tt) non-intersecting with Ts.
Our training method starts from a source task Ts,
where we aggregate the source LM Ls’s average
FiD cross-attention (FiDAtt) scores Sa
icorrespond-
ing to document da
ifrom the first decoder token
over all the layers, all the heads and all the input
tokens tofda
i⊕q:
Sa
i=1
ln∗hn∗tnX
layersX
headsX
t∈da
i⊕qFiDAtt (FiD(q)).(4)
where ln, hn, tn are the numbers of the layers, the
heads and the input tokens.
To make the training process more robust, we uti-
lize the FiDAtt scores to annotate the LM-preferred
positive documents in a discrete way:
Da+=Dh+∪Top-KSa
i,Da, (5)

--- PAGE 4 ---
where Dh+is the human-preferred positive doc-
ument set (i.e., ground truth) on Ts. Top- KSa
i,Da
means the documents with the top-k average Fi-
DAtt scores Sa
iin the retrieved document set Da.
Then, we sample hard negatives following
ANCE (Xiong et al., 2021) and formulate the train-
ing loss Lof the retriever as:
D−=ANNM
f(q,◦)\Da+, (6)
L=X
qX
d+∈Da+X
d−∈D−l(f(q,d+), f(q,d−)),(7)
where Mis the hyperparameter of the negative
sampling depth and lis the standard cross entropy
loss. After fine-tuning the retriever, we directly use
it to augment unseen target LM Lton each task
from target task set Tt.
4 Experimental Methodologies
In this section, we discuss our main experimental
setup. More details can be found in Appendix A.
4.1 Target Tasks
Following prior works (Chung et al., 2022; Mallen
et al., 2022), we choose MMLU (Hendrycks et al.,
2021) and PopQA (Mallen et al., 2022) as target
tasksTt.
MMLU is a multitask language understanding
dataset, which includes 57 multi-choice question
answering subtasks. These subtasks can be gen-
erally classified into four categories: humanities,
social sciences, STEM, and other. We average the
accuracy of the subtasks in each category to ob-
tain the final score. We report the accuracy of the
evaluation set in our main experiments.
PopQA is an entity-centric question answering
dataset that mainly concentrates on long-tail ques-
tions. We report the accuracy of the test set in our
main experiments.
4.2 Our Method
Retrievers. We adopt two widely used retriev-
ers to initialize AAR: ANCE initialized from
T5Base(Raffel et al., 2020; Ge et al., 2023) and
Contriever (Izacard et al., 2021) initialized from
BERT Base(Devlin et al., 2019). Both of them have
been fine-tuned on MS MARCO (Bajaj et al., 2016)
previously. For the retrieval corpus, we choose the
MS MARCO (Bajaj et al., 2016) for MMLU and
the KILT-Wikipedia (Petroni et al.) for PopQA.Language Models. We adopt Flan-T5 (Chung
et al., 2022) series as our backbone for encoder-
decoder LMs and InstructGPT1(Ouyang et al.,
2022) as our backbone for decoder-only LMs.
These models have been multi-task instruction-
finetuned and are widely utilized for assessing zero-
shot generalization (Zhou et al., 2023).
Implementation Details. We utilize the MS
MARCO (Bajaj et al., 2016) as our source task
Tssince it is the common choice to train the re-
triever (Xin et al., 2022). This dataset consists
of high-quality questions that require real-world
knowledge to answer, which aligns strongly with
our target tasks Ttand possesses no overlap with
them. Considering the implementation efficiency,
we take the Flan-T5 Baseas the source LM Lsand
treat the larger model as the target LM Lt. We di-
rectly set the total document number N= 10 , LM-
preferred document number K= 2, and the nega-
tive mining depth M= 100 in the augmentation-
adapted training. We run all experiments on a sin-
gle A100 GPU (40G).
4.3 Baselines
Zero-shot Setting. We compare our method with
the state-of-the-art zero-shot baselines. Standalone
LMs, including Flan-T5 (Chung et al., 2022), In-
structGPT (Ouyang et al., 2022), GAL (Taylor
et al., 2022) and OPT-IML-Max (Iyer et al., 2022),
are prompted by a natural language instruction that
describes the desired task and question. Adaptive
retrieval (Mallen et al., 2022) selectively utilizes
non-parametric memory (retrieval augmentation)
and parametric memory (the knowledge obtained
from pre-training) based on questions’ popularity.
In our main experiment, we select the optimal com-
bination in their paper, which consists of Contriever
as the non-parametric memory and GenRead (Yu
et al., 2023) as the parametric memory.
Few-shot Setting. We also include the results of
previous few-shot models for reference. Flan-T5,
InstructGPT, Chinchilla (Hoffmann et al., 2022)
and OPT-IML-Max adopt few-shot demonstrations,
which provide the LMs with a limited number of
task examples. This enables the models to gener-
alize from these examples and generate accurate
responses (Gao et al., 2021). Atlas (Izacard et al.,
2022) is a state-of-the-art retrieval-augmented LM,
which jointly pre-trains the retriever with the LM
1We use the GPT-3 text-davinci-002 December 2022 version.

--- PAGE 5 ---
Settings Methods # ParametersMMLU PopQA
All Hum. Soc. Sci. STEM Other All
Base Setting: T5 Base Size
Few-shot Flan-T5 Base(Chung et al., 2022) 250M 35.8 39.6 39.8 26.3 41.2 8.0
Zero-shotFlan-T5 Base 250M 36.1 40.4 39.8 27.0 40.6 8.8
Flan-T5 Basew/ AR (Mallen et al., 2022) 250M 42.8 43.5 44.0 35.8 50.0 29.4
Flan-T5 Basew/ AAR Contriever (Ours) 250M 44.4 44.7 47.7 35.8 52.2 31.9
Flan-T5 Basew/ AAR ANCE (Ours) 250M 44.8 42.2 46.4 39.0 53.2 37.7
Large Setting: T5 Large Size
Few-shotAtlas Large FT (Izacard et al., 2022) 770M 38.9 37.3 41.7 32.3 44.9 n.a.
Flan-T5 Large 780M 45.1 47.7 53.5 34.4 49.2 9.3
Zero-shotFlan-T5 Large 780M 44.8 46.3 51.4 34.8 50.6 7.2
Flan-T5 Large w/ AR 780M 49.8 50.0 55.6 38.4 59.5 29.6
Flan-T5 Large w/ AAR Contriever (Ours) 780M 51.8 50.8 59.7 39.4 61.8 33.4
Flan-T5 Large w/ AAR ANCE (Ours) 780M 50.4 48.0 58.1 39.3 60.2 39.3
XL Setting: T5 XL Size
Few-shotAtlas XLFT 3B 42.3 40.0 46.8 35.0 48.1 n.a.
Flan-T5 XL 3B 51.6 55.0 61.1 36.8 59.5 11.1
Zero-shotFlan-T5 XL 3B 51.2 55.5 57.4 38.1 58.7 11.3
Flan-T5 XLw/ AR 3B 55.5 56.7 64.5 43.0 62.6 33.7
Flan-T5 XLw/ AAR Contriever (Ours) 3B 56.7 57.7 65.4 43.6 65.1 31.5
Flan-T5 XLw/ AAR ANCE (Ours) 3B 56.2 59.4 64.8 41.5 64.9 38.0
Giant Setting: Over 70B Size
Few-shotChinchilla (Hoffmann et al., 2022) 70B 67.5 63.6 79.3 55.0 73.9 n.a.
OPT-IML-Max (Iyer et al., 2022) 175B 47.1 n.a. n.a. n.a. n.a. n.a.
InstructGPT (Ouyang et al., 2022) 175B 60.5 62.0 71.8 44.3 70.1 35.2
Zero-shotGAL (Taylor et al., 2022) 120B 52.6 n.a. n.a. n.a. n.a. n.a.
OPT-IML-Max 175B 49.1 n.a. n.a. n.a. n.a. n.a.
InstructGPT 175B 60.2 65.7 68.0 46.1 66.5 34.7
InstructGPT w/ AR 175B 60.5 62.2 71.3 44.7 69.7 43.3
InstructGPT w/ AAR Contriever (Ours) 175B 61.5 64.5 73.1 45.0 69.9 43.9
InstructGPT w/ AAR ANCE (Ours) 175B 62.2 62.0 72.0 49.2 70.7 52.0
Table 1: Our main results on MMLU and PopQA dataset. We group the methods mainly by the parameters. Our
Lsis Flan-T5 Base. AAR Contriever : AAR initialized from Contriever; AAR ANCE : AAR initialized from ANCE; FT:
fine-tuning; AR: adaptive retrieval. Unspecified methods represent direct prompting. The score marked as bold
means the best performance among the models in the zero-shot setting.
2.5 5.0 7.5
Training FLOPs 1e2130354045505560MMLU AccuracyAARANCE
Ls=Flan-T5Base, Lt=Flan-T5Base
AARANCE
Ls=Flan-T5Base, Lt=Flan-T5Large
AARANCE
Ls=Flan-T5Base, Lt=Flan-T5XL
AARANCE
Ls=Lt=Flan-T5Large
AARANCE
Ls=Lt=Flan-T5XL
AtlasLarge
AtlasXL
Figure 3: Training FLOPs of retrieval augmentation
methods.
using unsupervised data and fine-tunes the retriever
via the attention distillation on few-shot data.
5 Evaluation Results
In this section, we discuss our main results on
MMLU and PopQA datasets (§ 5.1) and conductcomprehensive studies about how (§ 5.2, § 5.3,
§ 5.4) and when (§ 5.5, § 5.6) AAR helps.
5.1 Overall Performance
Table 1 demonstrates that, with the assistance of a
generic AAR, target LMs of different sizes and
architectures can significantly outperform their
standalone baselines in the zero-shot setting. No-
tably, AAR even improves powerful InstructGPT
by 2% on MMLU and by nearly 20% on PopQA.
We hypothesize that the PopQA dataset mainly
comprises long-tail questions and thus necessitates
more augmentation information to attain high accu-
racy. AAR outperforms other augmentation meth-
ods like few-shot prompting and adaptive retrieval,
as they may not offer as extensive evidence text as
AAR does.
Meanwhile, AAR is a highly efficient augmenta-
tion approach since it only relies on a small source

--- PAGE 6 ---
250M 780M 3B 175B
# Parameters4045505560657075MMLU AccuracyANCE
AARANCE
Contriever
AARContriever(a) Pre-trained Retrievers.
250M 780M 3B 175B
# Parameters4045505560657075MMLU AccuracyHuman
Ls=Flan-T5Base LM
Ls=Flan-T5Base
Ls=Flan-T5Large (b) Positive docs selection.
Figure 4: AAR’s performance when (a) using differ-
ent pre-trained retrievers and (b) trained with different
positive documents, using Flan-T5 Base(250M), Flan-
T5Large (780M), Flan-T5 XL(3B), InstructGPT (175B)
asLt. The retriever in (b) is initialized from ANCE.
LM Flan-T5 Base(250M) to provide training signals
and can generalize well to target LMs of larger ca-
pacities. Figure 3 illustrates that solely setting the
source LM as the target LM (represented by the in-
verted triangles) does not significantly enhance the
MMLU accuracy. However, it may triple the train-
ing budget required. Only using a small source LM
is able to outperform the powerful Atlas by large
margins with fewer training FLOPs.
5.2 Ablation Study
In this experiment, we conduct the ablation study of
augmentation-adapted training and analyze model
behaviors during the training process.
Figure 4a illustrates that augmentation-adapted
training can bring additional improvements com-
pared to the pre-trained retrievers. In general,
ANCE benefits more from augmentation-adapted
training than Contriever. This may be due to the
fact that Contriever has been already intensively
pre-trained on massive data augmentations as well
as MS MARCO whereas ANCE is trained only on
MS MARCO. We provide exact numbers in Table 7
and PopQA results in Figure 8, which yield similar
observations as MMLU.
In Figure 4b, we compare retrievers trained with
different positive documents, including human-
preferred documents annotated by search users (the
blue bar), LM-preferred documents obtained by
the source LM (the orange bar), and their combi-
nations (the green bar and the red bar). Since the
retriever has been pre-trained on user-annotated
MS MARCO, simply using human-preferred docu-
ments to train it may be meaningless and therefore
performs the worst among all approaches. Only
using LM-preferred documents demonstrates no-
table gains over only using human-preferred doc-
010K 30K 50K 70K
Training step1.01.11.21.31.4Loss
2830323436
MS MARCO MRR@10
Train Loss
Eval Loss
MS MARCO(a) Retriever’s performance.
010K 30K 50K 70K
Training step424344454647MMLU Accuracy
MMLU
28.028.428.829.229.630.0
MSMARCO QA Rouge-L
MSMARCO QA (b)Lt’s performance.
Figure 5: AAR’s training process. (a) exhibits the re-
triever’s (ANCE) performance on MS MARCO. (b)
presents the Lt’s (Flan-T5 Base) performance on MS-
MARCO QA and MMLU.
uments, and merging both human-preferred and
LM-preferred documents (our main setup) further
enhances the retriever’s performance. Finally, us-
ing Flan-T5 Baseas source LM yields better results
compared to using Flan-T5 Large when the target
LMs are relatively small. However, as the target
LM’s size increases, both approaches achieve com-
parable performance. Hence, our choice to utilize
a small source LM in the augmentation-adapted
training is reasonable and effective.
Figure 5a and Figure 5b plot the retriever’s and
LM’s performance during augmentation-adapted
training, respectively. At the beginning of the train-
ing, the retriever’s MRR@10 on the MS MARCO
drops dramatically, indicating a large distribution
gap between human-preferred and LM-preferred
documents. As the retriever’s train and dev loss
continually decline, the retrieval-augmented LM
gradually performs better on MSMARCO QA and
eventually, on MMLU. This result implies that LMs
on different task may share common preferences,
making AAR generalize well from single source
task to heterogeneous target tasks.
5.3 Analysis of LM-preferred Documents
We highlight the necessity of adapting existing re-
trievers to LMs by comparing the preferred docu-
ments between search users and LMs. In general,
we discover that LM-preferred documents can as-
sist LM from alternative perspectives rather than
the full information favored by search users.
First, we define the set overlap Obetween two
positive documents set D+
1andD+
2as:
O=D+
1∩D+
2
D+
1∪D+
2. (8)
As illustrated in Figure 6a, the set overlaps of the
positive document sets annotated by human users

--- PAGE 7 ---
Question Human-preferred Document LM-preferred Document
what happens if you miss
your cruise shipIf you do miss the ship, go into the
cruise terminal and talk with the port
agents, who are in contact with both
shipboard and shoreside personnel.
They can help you decide the best way
to meet your ...The cruise line is not financially respon-
sible for getting passengers to the next
port if they miss the ship. Your travel
to the subsequent port, or home, is on
your dime, as are any necessary hotel
stays and meals...
what is annexation? Annexation is an activity in which two
things are joined together, usually with
a subordinate or lesser thing being at-
tached to a larger thing. In strict legal
terms, annexation simply involves...Annexation (Latin ad, to, and nexus,
joining) is the administrative action and
concept in international law relating to
theforcible transition of one state’s ter-
ritory by another state . It is generally
held to be an illegal act...
Table 2: Cases study on MSMARCO QA dataset. We show Top-1 document annotated by human users and FiDAtt
scores. Red texts are the gold answer spans.
Base Large XLHuman
Base
Large
XL
Human100.0%
60.6%
55.2%
13.2%60.6%
100.0%
69.2%
13.3%55.2%
69.2%
100.0%
13.1%13.2%
13.3%
13.1%
100.0%
0.20.40.60.81.0
Set Overlap
(a) Positive docs overlap.
0102030405060 MSMARCO QA Rouge-LStandalone LM
Human
Human (Ans-Deletion)
LM
LM (Ans-Deletion) (b) Answer-deletion test.
Figure 6: Analysis of LM-preferred documents. (a)
shows the overlaps of positive document sets, where
used LMs are Flan-T5 series. (b) presents the answer-
deletion experiments on the MSMARCO QA dataset.
The retriever is initialized from ANCE.
(Dh+) and LMs ( Top-KSa
i,Da) are quite low (near
13%), demonstrating their distinct tendencies in
selecting valuable documents. On the contrary, the
overlaps between different LMs are relatively high
(over 55%). This evidence provides a strong ratio-
nale for the generalization ability of AAR since
LMs with different sizes tend to annotate simi-
lar positive documents. Furthermore, LMs whose
sizes are closer generally possess higher overlaps.
This implies a better generalization ability of the
AAR to the LMs whose capacity is near the source
LM. The findings further validate the results illus-
trated in Figure 4b.
To give an in-depth analysis of how human-
preferred and LM-preferred documents differ, we
show two representative cases sampled from the
MSMARCO QA in Table 2. We observe that the
human-preferred document can always present the
gold answer at the beginning of the text, while the
LM-preferred document may not contain the ex-
act answer. However, an LM-preferred document
250M 780M 3B 175B
# Parameters4045505560657075MMLU AccuracyTART
AARContriever (MSMARCO QA)
AARContriever (KILT)
AARANCE (MSMARCO QA)
AARANCE (KILT)Figure 7: Comparison between single-task (MS-
MARCO QA) and multi-task (KILT) trained AAR.
TART (Asai et al., 2022) is a multi-task instruction-
finetuned retriever that has not been finetuned with LM-
preferred signals.
can (1) deliver a new perspective to answer the
given question, e.g., “the cruise line’s responsibil-
ity if you miss your cruise ship” and (2) give a
specific explanation instead of an abstract defini-
tion, e.g., “forcible transition of one state’s territory
by another state”, These characteristics differ from
search users who want the full information and can
further assist LMs in knowledge-based reasoning.
We further examine the unique characteristics
of LM-preferred documents through the answer-
deletion test (i.e., deleting the exact answer span
from the retrieved documents). As shown in
Figure 6b, the retriever trained by either human-
preferred (i.e., human-preferred retriever) or LM-
preferred documents (i.e., LM-preferred retriever)
can help LM answer the given question. Never-
theless, after the answer-deletion, the performance
of LM with the human-preferred retriever declines
more significantly than with the LM-preferred re-
triever. Despite having fewer exact match answers
(0.6% for LM-preferred documents vs. 13.0% for

--- PAGE 8 ---
CorporaMMLU PopQA
All Hum. Soc. Sci. STEM Other All
MS MARCO 44.8 42.2 46.4 39.0 53.2 13.6
KILT-Wikipedia 42.6 42.5 45.9 34.3 50.5 37.7
Standalone LM 36.1 40.4 39.8 27.0 40.6 8.8
Table 3: Ablation of the retrieval corpus, with Flan-
T5Baseas LM and AAR ANCE as retriever.
human-preferred documents), LM-preferred docu-
ments provide helpful information from alternative
perspectives. Therefore, adapting retrievers with
LM-preferred documents can in turn make retrieval-
augmented LM perform better.
5.4 Multi-task Training of AAR
In this section, we explore if the multi-task training
of AAR can endow the retriever with better gener-
alization to the target task. Specifically, we choose
KILT (Petroni et al.) as our multi-task data source,
which consists of 5 categories (Fact Checking, En-
tity Linking, Slot Filling, Open Domain QA, and
Dialogue). We take one representative subtask per
category to form a mixture of multiple source tasks.
Figure 7 illustrates that ANCE trained with
multi-task KILT can consistently outperform the
single-task MSMARCO QA, proving the bet-
ter generalization ability brought by multi-task
augmentation-adapted training. It is possible that
LMs may vary slightly in preferred documents for
different tasks and AAR can switch more smoothly
to the target task with the help of multi-task train-
ing. Contriever does not benefit greatly from multi-
task training. We conjecture that this is because
Contriever has been pre-trained with multiple for-
mats of data augmentations and thus generalizes
better to new data distribution than ANCE. Inter-
estingly, multi-task instruction-finetuned retriever
TART (Asai et al., 2022) has an overall worse per-
formance compared to AAR, highlighting the ben-
efits of having LM-preferred documents during the
multi-task training. A more detailed analysis about
the selection of source tasks is in Appendix B.
5.5 Effect of Retrieval Corpus
Table 3 demonstrates that regardless of the retrieval
corpus, AAR results in consistent and substantial
performance gains over the standalone LM.
On MMLU, using MS MARCO as the retrieval
corpus improves the LM more compared to KILT-
Wikipedia. We hypothesize that the retriever has
been trained with MS MARCO corpus and thus
holds better retrieval performance on it.Settings MethodsMMLU PopQA
All All
Few-shotOPT (Zhang et al., 2022) 26.0 12.3
GPT-neo (Black et al., 2021) 28.7 11.3
Zero-shotOPT 22.7 12.0
GPT-neo 25.3 9.9
OPT GenRead 22.3 12.2
GPT-neo GenRead 24.4 11.9
OPT w/ AAR Contriever (Ours) 23.2 29.1
GPT-neo w/ AAR Contriever (Ours) 25.2 27.8
OPT w/ AAR ANCE (Ours) 23.7 32.9
GPT-neo w/ AAR ANCE (Ours) 26.6 30.1
Table 4: Results of OPT and GPT-neo. We use their
1.3B version. The score marked as bold means the best
performance in the zero-shot setting.
On PopQA, model performance will drop by
large margins if we use MS MARCO as the re-
trieval corpus instead of KILT-Wikipedia. The pri-
mary reason is that the PopQA dataset is sampled
from Wikidata and designed for long-tail questions.
Partial long-tail knowledge can be only found in
KILT-Wikipedia (Mallen et al., 2022) while MS
MARCO lacks the indispensable evidence that
should be utilized for answer prediction. For in-
stance, given the question “Who is the mother
of Melissa Benn?”, there is no document in MS
MARCO containing the answer “Caroline Benn”.
Under such circumstances, aligning the retrieval
corpus with the data source can be necessary to
leverage AAR’s ability.
5.6 Application Scenarios of AAR
To examine if AAR works for unseen LMs that
lack zero-shot generalization ability, we also report
the results of OPT (Zhang et al., 2022) and GPT-
neo (Black et al., 2021). These models may have
poor zero-shot performance due to the lack of multi-
task instruction tuning.
From Table 4, we find that our AAR improves
both LMs marginally on MMLU while achieving
significant gains on PopQA. We conjecture that
LMs can benefit more easily from retrieval augmen-
tation on the knowledge-probing task like PopQA,
where the answer span can be directly acquired
from the retrieved documents. MMLU requires the
LM to not only comprehend the retrieved pieces of
evidence but also perform knowledge-based reason-
ing over them. OPT and GPT-neo may not possess
such abilities in zero-shot scenarios.
In summary, although AAR perfectly fits the
multi-task instruction-finetuned LMs such as the
Flan-T5 series and InstructGPT, it may not bring
significant gains for LMs whose zero-shot perfor-

--- PAGE 9 ---
mance is sometimes poor, especially on knowledge-
based reasoning. However, we believe that multi-
task instruction-finetuned models will be the foun-
dation of future work due to their outstanding zero-
shot generalization capabilities, ensuring the wide-
ranging application scenarios of AAR.
6 Discussions
LM-preferred Documents. Acquiring discrete
feedback signals from LMs is challenging as it re-
quires superior labeling ability, which is not the de-
signed purpose of LMs. Inspired by ADist (Izacard
and Grave, 2021a) and Atlas (Izacard et al., 2022),
we utilize the FiDAtt scores to select LM-preferred
documents for the augmentation-adapted training.
However, FiDAtt scores may not reflect the actual
contribution of each document faithfully since LM
may prefer attending to readable rather than in-
formative documents. Furthermore, the quality of
LM-preferred documents depends heavily on the
initial performance of the retrieval-augmented LM.
Parallel work (Shi et al., 2023) computes the KL
divergence between retrieval likelihood and LM
likelihood to train the retriever. Nevertheless, they
require a larger source LM, Curie (6.7B), to pro-
vide accurate LM likelihood signals. In the future,
reinforcement learning could serve as an alterna-
tive method to train the retriever, as it optimizes
the retriever by directly leveraging LM’s signals
without relying on the devised rule.
Generic Retrieval Plug-in. Chatgpt-retrieval-
plugin2has recently gained attention in the NLP
community as a generic retrieval plug-in. It re-
trieves the most relevant document from users’ data
sources and tailor ChatGPT’s response to meet their
specific needs. We believe that techniques such as
AAR will enhance the ability of black-box Chat-
GPT to generate more reasonable responses based
on the retrieved information, thereby promoting the
development of human-centered LM design.
7 Conclusion and Future Work
This paper introduces generic retrieval plug-in that
utilizes a generic retriever to enhance target LMs
that may be unknown in advance or are unable
to be fine-tuned jointly. Our proposed retriever,
AAR, can directly support black-box LMs without
requiring any fine-tuning of the LMs. This is ac-
complished by building the AAR’s training data
2https://github.com/openai/chatgpt-retrieval-pluginwith preferred documents from a small source LM
together with the ground truth.
Empirical results on MMLU and PopQA demon-
strate that AAR-assisted LMs greatly outperform
the standalone ones in zero-shot scenarios, and
AAR generalizes well to LMs of different sizes
and structures. Analytical results reveal that LM-
preferred and human-preferred documents comple-
ment each other; LM-preferred documents from
different LMs overlap significantly, and LMs with
similar sizes tend to yield closer document sets.
We leave a more detailed explanation of how dif-
ferent LMs interact with augmentation documents
and a more reasonable selection of LM-preferred
documents for future work. We hope our work
shed light on a path to a generic way of treating
large LMs as black boxes and adapting retrievers
to augment them.
Limitations
Due to the limitation of computational resources,
we have not evaluated the Flan-T5 XXLwhose num-
ber of parameters is 11B, and the OPT whose num-
ber of parameters is greater than 1.3B.
Since OPT and GPT-neo perform poorly in the
zero-shot setting and separating attention scores of
each document in the input is tedious for decoder-
only models, we choose not to use them as source
LMs. However, we prove that taking the encoder-
decoder model Flan-T5 Baseas our source LM is
also robust to augment decoder-only models. We
will explore new methods to annotate LM-preferred
documents of decoder-only models based on their
inherent signals.
Acknowledgement
Zichun Yu, Shi Yu, and Zhiyuan Liu are supported
by Institute Guo Qiang at Tsinghua University, Bei-
jing Academy of Artificial Intelligence (BAAI).
All authors proposed the original idea together.
Zichun Yu conducted the experiments. Zichun Yu,
Chenyan Xiong, Shi Yu, and Zhiyuan Liu wrote
the paper. Chenyan Xiong and Zhiyuan Liu pro-
vided valuable suggestions for the research. We
thank Suyu Ge for sharing the ANCE checkpoint
initialized from T5 Base.

--- PAGE 10 ---
References
Fabien André, Anne-Marie Kermarrec, and Nicolas
Le Scouarnec. 2016. Cache locality is not enough:
High-performance nearest neighbor search with prod-
uct quantization fast scan. In VLDB , page 12.
Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen,
Gautier Izacard, Sebastian Riedel, Hannaneh Ha-
jishirzi, and Wen-tau Yih. 2022. Task-aware retrieval
with instructions. arXiv preprint arXiv:2211.09260 .
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-
drew McNamara, Bhaskar Mitra, Tri Nguyen, et al.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. In CoCo@NeurIPS .
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of ACM FAccT , pages
610–623.
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and
Stella Biderman. 2021. Gpt-neo: Large scale autore-
gressive language modeling with mesh-tensorflow.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
De Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Mag-
giore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals,
Simon Osindero, Karen Simonyan, Jack Rae, Erich
Elsen, and Laurent Sifre. 2022. Improving language
models by retrieving from trillions of tokens. In
ICML , pages 2206–2240.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In NeurIPS , pages 1877–1901.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, Parker Schuh, and et al. 2022. Palm:
Scaling language modeling with pathways. arXiv
preprint arXiv:2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Web-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
gun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,
Yanping Huang, Andrew Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of NAACL , pages 4171–
4186.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of ACL , pages 3816–3830.
Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Over-
wijk, Jiawei Han, and Paul Bennett. 2023. Augment-
ing zero-shot dense retrievers with plug-in mixture-
of-memories. arXiv preprint arXiv:2302.03754 .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
augmented language model pre-training. In ICML ,
pages 3929–3938.
Hangfeng He, Hongming Zhang, and Dan Roth. 2022.
Rethinking with retrieval: Faithful large language
model inference. arXiv preprint arXiv:2301.00303 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In ICLR .
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks, Jo-
hannes Welbl, Aidan Clark, Thomas Hennigan, Eric
Noland, Katherine Millican, George van den Driess-
che, Bogdan Damoc, Aurelia Guy, Simon Osindero,
Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack
Rae, and Laurent Sifre. 2022. An empirical analysis
of compute-optimal large language model training.
InNeurIPS , pages 30016–30030.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,
Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,
Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-
pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,
and Ves Stoyanov. 2022. Opt-iml: Scaling language
model instruction meta learning through the lens of
generalization. arXiv preprint arXiv:2212.12017 .
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense infor-
mation retrieval with contrastive learning. TMLR .
Gautier Izacard and Edouard Grave. 2021a. Distilling
knowledge from reader to retriever for question an-
swering. In ICLR .
Gautier Izacard and Edouard Grave. 2021b. Leveraging
passage retrieval with generative models for open
domain question answering. In Proceedings of EACL ,
pages 874–880.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and

--- PAGE 11 ---
Edouard Grave. 2022. Few-shot Learning with Re-
trieval Augmented Language Models. arXiv preprint
arXiv:2208.03299 .
Jeff Johnson, Matthijs Douze, and Herve Jegou. 2021.
Billion-scale similarity search with gpus. IEEE TBD ,
7(3):535–547.
Mingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,
and Yanfang Ye. 2022. Grape: Knowledge graph
enhanced passage reader for open-domain question
answering. In Findings of EMNLP .
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings
of EMNLP , pages 6769–6781.
Omar Khattab, Keshav Santhanam, Xiang Lisa
Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia. 2022. Demonstrate-search-
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp. arXiv preprint
arXiv:2212.14024 .
Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
intensive NLP tasks. In NeurIPS , pages 9459–9474.
Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen
Tian, and Xiang Ren. 2022. Unsupervised cross-
task generalization via retrieval augmentation. In
NeurIPS , pages 22003–22017.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi
Das, Hannaneh Hajishirzi, and Daniel Khashabi.
2022. When not to trust language models: Inves-
tigating effectiveness and limitations of paramet-
ric and non-parametric memories. arXiv preprint
arXiv:2212.10511 .
Antonis Maronikolakis and Hinrich Schütze. 2021. Mul-
tidomain pretrained language models for green NLP.
InProceedings of AdaptNLP , pages 1–8.
OpenAI. 2023. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In NeurIPS , pages 27730–27744.Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
Riedel. KILT: a benchmark for knowledge intensive
language tasks. In Proceedings of NAACL , pages
2523–2544.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. JMLR , 21:140:1–140:67.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the parame-
ters of a language model? In Proceedings of EMNLP ,
pages 5418–5426.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of NAACL , pages 2655–
2671.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.
Multitask prompted training enables zero-shot task
generalization. In ICLR .
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen tau Yih. 2023. Replug: Retrieval-
augmented black-box language models. arXiv
preprint arXiv:2301.12652 .
Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for
deep learning in NLP. In Proceedings of ACL , pages
3645–3650.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Galactica: A large language model for science. arXiv
preprint arXiv:2211.09085 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,
and Denny Zhou. 2022. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
NeurIPS , pages 24824–24837.
Chaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min
Chan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,
Zhonghua Li, Zhao Cao, and Maosong Sun. 2023.
Plug-and-play document modules for pre-trained
models. In Proceedings of ACL .
Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita
Sharma, Damien Jose, and Paul Bennett. 2022. Zero-
shot dense retrieval with momentum adversarial do-
main invariant representations. In Findings of ACL ,
pages 4008–4020.

--- PAGE 12 ---
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In ICLR .
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In ICLR .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models. arXiv preprint
arXiv:2205.01068 .
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong
Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan
Liu, Peng Li, Maosong Sun, and Jie Zhou. 2023.
Plug-and-play knowledge injection for pre-trained
language models. In Proceedings of ACL .
Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,
Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,
Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,
Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,
and Lichao Sun. 2023. A comprehensive survey on
pretrained foundation models: A history from bert to
chatgpt. arXiv preprint arXiv:2302.09419 .

--- PAGE 13 ---
A Experimental Settings
A.1 Training Hyperparameters
We take the ANCE initialized from T5 Base3(Xiong
et al., 2021; Ge et al., 2023) and Contriever4(Izac-
ard et al., 2021)’s hyperparameters in the
augmentation-adapted training. Specifically, we fix
batch size as 8, learning rate as 5e-6, and epochs as
6 for ANCE while taking batch size as 8, learning
rate as 1e-5, and epochs as 3 for Contriever. We
choose their best checkpoints based on the perfor-
mance of the development set. The information
about our source tasks and target tasks are listed in
Table 6.
A.2 Number of Augmentation Documents
LMs of different sizes, facing various target tasks,
may require indefinite numbers of augmentation
documents to achieve their best performance.
For MMLU, we analyze how the number of aug-
mentation documents affects LMs’ performance.
As illustrated in Figure 9, we discover that LMs of
larger capacity generally benefit more from more
augmentation documents. A possible explanation
is that larger LMs are more capable of integrating
information from multiple documents and perform-
ing complicated reasoning based on them.
For PopQA, using 3 augmentation documents
achieves the best performance across all LMs.
A.3 Prompt Templates
The prompt template for MMLU is:
Here’s a problem to solve: {question}
Among the 4 following options, which is
the correct answer?
- A: {choice_A}
- B: {choice_B}
- C: {choice_C}
- D: {choice_D}
The prompt template for PopQA is:
Q: {question} A:
B Selection of Source Task
We provide a detailed selection of the source tasks
here, using a variety of source and target tasks to an-
alyze. MSMARCO QA, KILT-TriviaQA, and NQ
belong to Open Domain QA, while KILT-T-REx
and zsRE belong to Slot Filling. MMLU belongs
to Multi-task Language Understanding, which is
3https://huggingface.co/OpenMatch/t5-ance
4https://huggingface.co/facebook/contriever-msmarcoTsTtMMLU NQ zsRE
MSMARCO QA 44.8 46.7 75.1
KILT-TriviaQA 43.6 46.4 74.9
KILT-T-REx 44.1 45.9 77.2
Table 5: Relationship between the selection of source
taskTsand the performance of target task Tt. The
model is Flan-T5 Basew/ AAR ANCE . As NQ and zsRE
are included in the Flan-T5 training data, we only report
their F1 results here for reference.
closer to the Open Domain QA in terms of the task
objective. As shown in Table 5, when we align the
category of the source task with the target task, the
LM w/ AAR can generally achieve the best results.
We suppose that this is because LM may share sim-
ilar document preferences on the tasks from the
same dataset category, making AAR easier to gen-
eralize. Furthermore, taking MSMARCO QA as
the source task performs the best on MMLU. This
validates the rationality to set Tsas MSMARCO
QA in our main experimental settings.
C AAR’s Improvements on PopQA
250M 780M 3B 175B
# Parameters25303540455055PopQA AccuracyANCE
AARANCE
Contriever
AARContriever
Figure 8: AAR’s improvements on PopQA, using Flan-
T5Base(250M), Flan-T5 Large (780M), Flan-T5 XL(3B),
InstructGPT (175B) as target LMs.
D Fine-tuning Results
We also report the fine-tuning results of Flan-
T5Baseand Flan-T5 Large on MMLU auxiliary train-
ing data (Hendrycks et al., 2021) in Table 7. Due to
the limitation of the computational resources, we
do not include the fine-tuning result of Flan-T5 XL.
We take batch size as 32, learning rate as 5e-5, and
epochs as 3 in fine-tuning. In general, the LM that
has already been massively multi-task instruction-
finetuned, such as Flan-T5, improves little from
fine-tuning on extra tasks but benefits greatly from
our AAR. The results further validate the power of
zero-shot retrieval augmentation.

--- PAGE 14 ---
12345678910
Number of documents3638404244MMLU Accuracy
Standalone LM
Lt=Flan-T5Base(a) Flan-T5 Basew/ AAR ANCE.
12345678910
Number of documents4446485052MMLU Accuracy
Standalone LM
Lt=Flan-T5Large (b) Flan-T5 Largew/ AAR ANCE.
12345678910
Number of documents5052545658MMLU Accuracy
Standalone LM
Lt=Flan-T5XL (c) Flan-T5 XLw/ AAR ANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
TsMSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
TtMMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
MethodsMMLU
All Hum. Soc. Sci. STEM Other
Flan-T5 Base 36.1 40.4 39.8 27.0 40.6
Flan-T5 BaseFine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5 Basew/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5 Basew/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5 Basew/ AAR Contriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5 Basew/ AAR ANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5 Large 45.1 47.7 53.5 34.4 49.2
Flan-T5 Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5 Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5 Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5 Large w/ AAR Contriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5 Large w/ AAR ANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5 XL 51.2 55.5 57.4 38.1 58.7
Flan-T5 XLw/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5 XLw/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5 XLw/ AAR Contriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5 XLw/ AAR ANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AAR Contriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AAR ANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.
