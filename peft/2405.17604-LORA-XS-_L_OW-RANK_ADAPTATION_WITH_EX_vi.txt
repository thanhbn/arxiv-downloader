# 2405.17604.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2405.17604.pdf
# Kích thước file: 639953 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
LORA-XS: THÍCH ỨNG THỨ HẠNG THẤP VỚI SỐ LƯỢNG THAM SỐ CỰC KỲ NHỎ

Klaudia Bałazy∗1Mohammadreza Banaei∗2Karl Aberer2Jacek Tabor1
1Đại học Jagiellonian,2EPFL

TÓM TẮT
Sự mở rộng nhanh chóng của các mô hình ngôn ngữ lớn (LLM) đã nhấn mạnh nhu cầu về các phương pháp tinh chỉnh hiệu quả tham số, với LoRA (Thích ứng Thứ hạng Thấp) nổi lên như một giải pháp phổ biến. Mặc dù LoRA giảm số lượng tham số có thể huấn luyện, việc phục vụ nhiều mô-đun LoRA (theo nhiệm vụ hoặc cụ thể người dùng) trên đầu một mô hình cơ sở vẫn tạo ra những thách thức lưu trữ đáng kể. Để giải quyết vấn đề này, sử dụng dẫn xuất lý thuyết, chúng tôi giới thiệu LoRA-XS (Thích ứng Thứ hạng Thấp với số lượng tham số cực kỳ Nhỏ), một phương pháp thích ứng thứ hạng thấp mới giảm đáng kể các tham số có thể huấn luyện trong khi cho thấy hiệu suất vượt trội hoặc cạnh tranh. LoRA-XS đạt được điều này bằng cách chèn một ma trận trọng số r×r nhỏ có thể huấn luyện giữa các ma trận thứ hạng thấp bị đóng băng, được xây dựng bởi Phân tích Giá trị Đơn (SVD) của ma trận trọng số gốc. Ma trận nhẹ này cho phép tinh chỉnh với yêu cầu lưu trữ giảm mạnh, làm cho việc triển khai hàng triệu mô hình cá nhân hóa trở nên khả thi trong khi giảm thiểu chi phí bộ nhớ. Ví dụ, LoRA-XS đạt được sự giảm đáng kể các tham số có thể huấn luyện hơn 100 lần trong các mô hình 7B so với LoRA. Các đánh giá của chúng tôi trên nhiều điểm chuẩn khác nhau (bao gồm GLUE, GSM8K, MATH, và tám bộ dữ liệu lý luận thông thường) cho thấy LoRA-XS hoạt động cạnh tranh hoặc tốt hơn LoRA và các phương pháp gần đây khác như VeRA trong khi hiệu quả tham số hơn đáng kể. Chúng tôi cũng cung cấp một nghiên cứu loại bỏ chi tiết về tầm quan trọng của các vectơ đơn trong trọng số transformer, làm sáng tỏ các cơ chế cơ bản thúc đẩy hiệu quả nâng cao của LoRA-XS. Những phát hiện này cho thấy LoRA-XS không chỉ là một thay thế hiệu quả lưu trữ, mà còn là một công cụ mạnh mẽ để mở rộng quy mô và cá nhân hóa LLM ở quy mô chưa từng có.1

1 GIỚI THIỆU
Trong những năm gần đây, sự phát triển của các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa lĩnh vực xử lý ngôn ngữ tự nhiên (NLP), cho phép hiệu suất chưa từng có trên nhiều nhiệm vụ khác nhau. Tuy nhiên, những mô hình tiên tiến này thường đi kèm với một số lượng tham số khổng lồ, đặt ra những thách thức đáng kể cho việc tinh chỉnh và thích ứng với các nhiệm vụ hạ nguồn cụ thể. Việc sửa đổi và lưu trữ những mô hình khổng lồ này gây ra những thách thức về tính toán và lưu trữ.

Trước những thách thức này, các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT), trong đó chỉ một số lượng tương đối nhỏ tham số được tinh chỉnh, đã nổi lên như một giải pháp tiềm năng để bù đắp cho chi phí tính toán/lưu trữ khổng lồ của việc tinh chỉnh tham số đầy đủ (Houlsby et al., 2019; Hu et al., 2021; Lester et al., 2021; Li & Liang, 2021; Zaken et al., 2021). Trong số các phương pháp PEFT, LoRA (Hu et al., 2021) được sử dụng rộng rãi trong tài liệu gần đây do khả năng tổng quát hóa tốt và thực tế là nó không giới thiệu các mô-đun bổ sung trong giai đoạn suy luận. Tuy nhiên, ngay cả những kỹ thuật này cũng có thể yêu cầu tài nguyên lưu trữ và tính toán đáng kể, đặc biệt khi mục tiêu là cho phép thích ứng cá nhân hóa hoặc theo nhiệm vụ quy mô lớn. Ví dụ, áp dụng LoRA trên mô hình GPT-3 (Brown et al., 2020) với thứ hạng 16 và chỉ các ma trận truy vấn và giá trị được thích ứng, sẽ dẫn đến 144MB bộ nhớ cho mỗi điểm kiểm tra, điều này sẽ tương đương với 144TB bộ nhớ khi phục vụ 1 triệu mô hình cá nhân hóa.

*Đóng góp ngang nhau. Liên hệ: klaudia.balazy@doctoral.uj.edu.pl ; mohammadreza.banaei@epfl.ch.
1Mã của chúng tôi có sẵn tại https://github.com/MohammadrezaBanaei/LoRA-XS.
1arXiv:2405.17604v2 [cs.LG] 8 Oct 2024

--- TRANG 2 ---
Theo sau LoRA, nhiều phương pháp kế thừa đã được đề xuất để tiếp tục giảm số lượng tham số và cải thiện hiệu quả (Kopiczko et al., 2023; Liu et al., 2024; Zhang et al., 2023). Một phương pháp tiên tiến gần đây như vậy là VeRA (Kopiczko et al., 2023), giảm số lượng tham số có thể huấn luyện bằng cách đóng băng các ma trận LoRA và sử dụng một cặp ma trận thứ hạng thấp duy nhất được chia sẻ trên tất cả các lớp trong khi học các vectơ tỷ lệ nhỏ thay thế. Mặc dù VeRA cải thiện hiệu quả tham số, số lượng tham số của nó vẫn phụ thuộc vào các chiều ẩn của mô hình, điều này trở nên ngày càng đáng kể đối với các mô hình ngôn ngữ lớn hơn.2 Sự phụ thuộc này có thể dẫn đến yêu cầu lưu trữ và tính toán đáng kể khi kích thước mô hình tiếp tục tăng.

Số lượng tham số có thể huấn luyện tính bằng triệu Hiệu suất trung bình 83858789
0,0010,010,1110100FTLoRAVeRALoRA-XS(CỦA CHÚNG TÔI)
(thang logarit) Các mô-đun nhỏ hơn, không mất chất lượng

Hình 1: Hiệu suất trung bình của RoBERTa-large trên một tập con các nhiệm vụ GLUE (xem Bảng 1) như một hàm của số lượng tham số có thể huấn luyện (tính bằng triệu) cho các phương pháp thích ứng khác nhau: Tinh chỉnh Đầy đủ (FT), LoRA, VERA, và LoRA-XS. Các điểm LoRA-XS tương ứng với các thứ hạng khác nhau từ 4 đến 25. LoRA-XS luôn vượt trội hơn các phương pháp khác về cả hiệu suất trung bình và hiệu quả tham số. Không giống như các cách tiếp cận khác, LoRA-XS cung cấp tính linh hoạt lớn hơn trong việc giảm số lượng tham số có thể huấn luyện, vì nó không bị ràng buộc bởi chiều mô hình, cho phép thích ứng hiệu quả hơn mà không có giới hạn dưới.

Trong bài báo này, bằng cách áp dụng biện minh lý thuyết, chúng tôi đề xuất LoRA-XS, một phương pháp dựa trên LoRA cực kỳ hiệu quả tham số. LoRA-XS được thiết kế để đạt được hiệu suất thích ứng tương tự hoặc vượt trội với ít tham số hơn đáng kể - đặc biệt, số lượng tham số có thể huấn luyện của LoRA-XS độc lập với các chiều ẩn của mô hình. Do đó, LoRA-XS đánh dấu một mô hình mới trong tinh chỉnh hiệu quả tham số (PEFT), vượt qua các hạn chế của các cách tiếp cận hiện có và cung cấp một con đường hiệu quả hơn để cá nhân hóa mô hình và tối ưu hóa theo nhiệm vụ cụ thể. Sử dụng ví dụ trước về thích ứng thứ hạng thấp của GPT-3 với thứ hạng 16, phục vụ 1 triệu mô hình cá nhân hóa với LoRA-XS sẽ chỉ yêu cầu 96GB lưu trữ, so với 144TB của LoRA, dẫn đến sự giảm hơn 1500 lần trong yêu cầu lưu trữ.

Hơn nữa, với LoRA-XS, chúng ta có thể kiểm soát chính xác số lượng tham số bổ sung, cho phép sử dụng bộ nhớ linh hoạt (xem Hình 1). Tính linh hoạt này đặc biệt có lợi cho các mô hình ngày càng lớn hơn, nơi các phương pháp truyền thống áp đặt một số lượng tối thiểu nhất định các tham số bổ sung. Hơn nữa, LoRA-XS giữ lại các lợi thế cốt lõi của LoRA, chẳng hạn như không yêu cầu bất kỳ sửa đổi nào đối với kiến trúc mô hình và không giới thiệu độ trễ bổ sung trong quá trình suy luận, làm cho nó trở thành một giải pháp hiệu quả và liền mạch cho triển khai thực tế.

LoRA-XS đạt được hiệu quả tham số cực cao này bằng cách thiết lập các ma trận chiếu của LoRA sử dụng Phân tích Giá trị Đơn (SVD) của trọng số mô-đun được huấn luyện trước và giữ chúng bị đóng băng trong quá trình huấn luyện (xem Hình 2). Tham số có thể huấn luyện duy nhất của LoRA-XS là một ma trận r×r (tức là R) giữa các ma trận chiếu LoRA bị đóng băng, trong đó r biểu thị thứ hạng của LoRA.

Việc cố định những ma trận này trong quá trình huấn luyện chuyển đổi phương pháp của chúng tôi thành một cách tiếp cận chỉnh sửa tiềm ẩn, với ma trận R chỉ sử dụng r2 tham số. Mặc dù LoRA-XS huấn luyện mô hình trong một không gian tham số bị ràng buộc, như chúng tôi sẽ chỉ ra trong các phần sau, hiệu suất của nó vẫn cạnh tranh hoặc tốt hơn so với đường cơ sở LoRA và các phương pháp gần đây hơn như VeRA trên nhiều điểm chuẩn và quy mô mô hình khác nhau. Chúng tôi chứng minh hiệu suất của LoRA-XS trên một loạt các điểm chuẩn, bao gồm GLUE (Wang et al., 2018) cho hiểu biết ngôn ngữ tự nhiên, GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021) cho lý luận toán học, và tám bộ dữ liệu lý luận thông thường (xem Phần 4).

2Ví dụ, trong khi các mô hình transformer sớm như BERT có chiều ẩn là 768, mô hình GPT-3 gần đây có chiều ẩn là 12288, điều này ảnh hưởng trực tiếp đến số lượng tham số có thể huấn luyện cho các phương pháp LoRA và VeRA.

Chúng tôi cũng tiến hành một nghiên cứu loại bỏ chi tiết, tiết lộ vai trò thiết yếu của các vectơ đơn trong trọng số transformer, làm nổi bật cơ chế cốt lõi đằng sau hiệu quả của LoRA-XS (xem Phần 5). Những phát hiện của chúng tôi cho thấy LoRA-XS không chỉ cực kỳ hiệu quả tham số mà còn là một yếu tố hỗ trợ mạnh mẽ để mở rộng quy mô và cá nhân hóa các mô hình ngôn ngữ lớn ở quy mô chưa từng có.

Tóm lại, những đóng góp của chúng tôi như sau:
• Bằng cách áp dụng dẫn xuất lý thuyết, chúng tôi giới thiệu LoRA-XS, một phương pháp tinh chỉnh cực kỳ hiệu quả tham số giảm số lượng tham số có thể huấn luyện hơn 100 lần trong các mô hình quy mô lớn mà không ảnh hưởng đến hiệu suất.
• LoRA-XS vượt trội hơn LoRA và các cách tiếp cận gần đây khác như VeRA trên nhiều kích thước mô hình khác nhau (bao gồm LLM 7B và 8B) và một loạt các nhiệm vụ, bao gồm GLUE, GSM8k, MATH, và tám điểm chuẩn lý luận thông thường.
• Không giống như các biến thể LoRA hiện có, LoRA-XS cung cấp tính linh hoạt chưa từng có, với số lượng tham số của nó độc lập với các chiều mô hình, làm cho nó thân thiện hơn với lưu trữ và có thể thích ứng (xem Hình 1).

2 CÔNG TRÌNH LIÊN QUAN
Thích ứng Hiệu quả Gần đây, đã có nhiều biến thể của các phương pháp tinh chỉnh dựa trên adapter được đề xuất trong đó một tập hợp các mô-đun adapter được giới thiệu vào mô hình transformer (Vaswani et al., 2017). Những mô-đun này có thể được giới thiệu như các lớp adapter bổ sung vào khối transformer (Houlsby et al., 2019; Liu et al., 2022; Pfeiffer et al., 2020), hoặc như một tập hợp tham số bổ sung sửa đổi các kích hoạt lớp đầu vào (Asai et al., 2022; Lester et al., 2021; Li & Liang, 2021; Liu et al., 2023). Mặc dù những cách tiếp cận này giới thiệu một số lượng tương đối nhỏ tham số, chúng làm xấu đi độ trễ của mô hình trong suy luận trực tuyến, đặc biệt trong các tình huống sản xuất quy mô lớn.

Thích ứng Thứ hạng Thấp Thích ứng thứ hạng thấp của các mô hình transformer, được đề xuất bởi LoRA (Hu et al., 2021), cung cấp một sự thay thế mạnh mẽ cho các phương pháp PEFT trước đó, trong đó hiệu suất tổng quát hóa cạnh tranh với tinh chỉnh đầy đủ trong khi không giới thiệu thêm độ trễ nào trong giai đoạn suy luận. Dựa trên phương pháp LoRA, đã có nhiều nỗ lực gần đây để cải thiện đường cong học của nó (Liu et al., 2024; Hayou et al., 2024; Meng et al., 2024), giảm các tham số có thể huấn luyện (Zhang et al., 2023; Kopiczko et al., 2023; Renduchintala et al., 2023), hoặc thậm chí huấn luyện nó với trọng số được huấn luyện trước được lượng tử hóa để cải thiện dấu chân bộ nhớ trong quá trình huấn luyện (Dettmers et al., 2024; Li et al., 2023). Phương pháp của chúng tôi, LoRA-XS, thuộc về danh mục thứ hai, trong đó chúng tôi nhằm mục đích giảm đáng kể các tham số có thể huấn luyện trong khi hoạt động cạnh tranh với LoRA trên nhiều điểm chuẩn và quy mô mô hình khác nhau.

Các biến thể LoRA bị ràng buộc tham số Một số công trình gần đây đề xuất các biến thể của LoRA giảm số lượng tham số có thể huấn luyện trong khi duy trì hiệu suất cạnh tranh. AdaLoRA (Zhang et al., 2023) nghiên cứu một điều chỉnh thứ hạng động cho các ma trận thứ hạng thấp của các mô-đun khác nhau trái ngược với phân bổ tham số đồng nhất trong LoRA. Tied-LoRA (Renduchintala et al., 2023) cải thiện hiệu quả tham số bằng cách ràng buộc các ma trận LoRA trên tất cả các lớp của mô hình transformer. VeRA (Kopiczko et al., 2023), có liên quan chặt chẽ đến công trình của chúng tôi, chia sẻ các ma trận LoRA bị đóng băng được khởi tạo ngẫu nhiên trên các lớp và thêm các vectơ tỷ lệ có thể huấn luyện. Tuy nhiên, không giống như VeRA, LoRA-XS khởi tạo các ma trận thứ hạng thấp của mình sử dụng SVD của trọng số mô hình được huấn luyện trước, cung cấp cả biện minh lý thuyết (xem Phần 3.1) và hiệu suất thực nghiệm mạnh mẽ (xem Phần 5). Ngoài ra, số lượng tham số có thể huấn luyện của LoRA-XS độc lập với các chiều ẩn của mô hình, cho phép giảm đáng kể các tham số, đặc biệt trong các mô hình quy mô lớn.

--- TRANG 4 ---
Trọng số Được huấn luyện trước
𝑊 ∈ ℝm × n
= UΣVT

A = U rΣrRB = VrT
xh
R = 𝑁 (0, 𝜎2) rBị đóng băng
Có thể huấn luyện

Trọng số Được huấn luyện trước
𝑊 ∈ ℝm × n
A = 𝑁 (0, 𝜎2)B = 0
r
xhLoRA LoRA-XS (Của chúng tôi)

Hình 2: So sánh trực quan các kỹ thuật LoRA và LoRA-XS. Sự đổi mới chính của LoRA-XS là việc sử dụng một ma trận R nhỏ có thể huấn luyện được đặt giữa hai ma trận thứ hạng thấp bị đóng băng, A và B, được dẫn xuất từ SVD cắt ngắn của trọng số được huấn luyện trước, giữ lại các thành phần đơn r hàng đầu. LoRA-XS cho phép hiệu quả tham số cực cao trong khi duy trì hiệu suất.

3 PHƯƠNG PHÁP
Phần này giới thiệu LoRA-XS (Thích ứng Thứ hạng Thấp với số lượng tham số cực kỳ Nhỏ), một phương pháp mới được thiết kế để cải thiện hiệu quả tham số của việc tinh chỉnh các mô hình ngôn ngữ lớn bằng cách tận dụng những hiểu biết từ thích ứng thứ hạng thấp. Dựa trên những ý tưởng cốt lõi của LoRA, cách tiếp cận của chúng tôi giải quyết các vấn đề về khả năng mở rộng và lưu trữ trong khi duy trì hiệu suất có tính cạnh tranh cao.

Trong những năm gần đây, LoRA (Hu et al., 2021) đã có vai trò then chốt trong việc điều chỉnh hiệu quả tham số bằng cách giới thiệu các ma trận thứ hạng thấp để thích ứng, giảm đáng kể số lượng tham số có thể huấn luyện. Tuy nhiên, việc triển khai LoRA ở quy mô lớn, đặc biệt trên các mô hình theo nhiệm vụ cụ thể hoặc người dùng cụ thể quy mô lớn, có thể tăng đáng kể nhu cầu lưu trữ cần thiết. Khi các mô hình hiện đại tăng về kích thước và độ phức tạp, nhu cầu về các chiến lược điều chỉnh hiệu quả tham số hơn nữa trở thành một vấn đề quan trọng.

LoRA đã chứng minh rằng các bản cập nhật trọng số thứ hạng thấp của nó (tức là ∆W) căn chỉnh với một số hướng nhất định đã có trong trọng số của mô hình. Dựa trên hiểu biết này và dẫn xuất lý thuyết của chúng tôi (xem Phần 3.1), chúng tôi đề xuất khởi tạo các ma trận thích ứng LoRA (A và B trong Hình 2) sử dụng các vectơ đơn hàng đầu từ SVD của ma trận trọng số được huấn luyện trước W. Những ma trận này được giữ cố định trong quá trình huấn luyện. Để giới thiệu tính linh hoạt, chúng tôi thêm một ma trận r×r có thể huấn luyện R giữa A và B, làm cho R trở thành thành phần có thể học duy nhất. Điều này giảm mạnh số lượng tham số có thể huấn luyện, với số lượng tham số độc lập với các chiều mô hình. Hình 2 cung cấp một cái nhìn tổng quan về phương pháp LoRA-XS, làm nổi bật sự khác biệt của nó so với khung LoRA gốc.

3.1 DẪN XUẤT LÝ THUYẾT CỦA LORA-XS
Trong phần này, chúng tôi dẫn xuất các nền tảng lý thuyết của cách tiếp cận của chúng tôi, với chi tiết đầy đủ được cung cấp trong Phụ lục A. Độc giả chủ yếu quan tâm đến các khía cạnh thực tế của LoRA-XS có thể tiếp tục đến các phần tiếp theo.

Chúng tôi bắt đầu bằng cách xem xét một mạng neural với kiến trúc transformer. Gọi W∈Rn×n là một ma trận trọng số vuông cho một lớp tuyến tính tùy ý trong mạng này. Mục tiêu của chúng tôi là thích ứng các trọng số với các nhiệm vụ mới bằng cách áp dụng một hiệu chỉnh ∆W, mà chúng tôi muốn ràng buộc vào một không gian con chiều thấp hơn của Rn×n. Chúng tôi nhằm mục đích chỉ ra cách chọn một không gian con như vậy để cho phép tính linh hoạt đáng kể và hoạt động hiệu quả cho các thích ứng gradient tương lai.

Trong phương pháp LoRA tiêu chuẩn, không gian con được sử dụng để thích ứng được đặc trưng như tập hợp của tất cả các ma trận có dạng AB, trong đó A∈Rr×n và B∈Rn×r, dẫn đến một không gian có chiều bằng 2nr. Tuy nhiên, trong khung LoRA-XS được đề xuất của chúng tôi, chúng tôi giới thiệu một tham số hóa tổng quát hơn của các không gian con, cho phép một chiều r2, trong đó r có thể dao động từ 1 đến n. So với LoRA, có ít nhất chiều 2n, cách tiếp cận của chúng tôi có thể sử dụng một lượng bộ nhớ nhỏ tùy ý (xem Hình 1).

Chính thức, cho các ma trận trực giao cố định A∈Rr×n và B∈Rn×r, chúng tôi định nghĩa không gian con Sr A,B là
Sr A,B={AXBT:X∈Rr×r}.
Không gian con này có chiều r2 và cho phép lựa chọn linh hoạt r để điều chỉnh chiều. Hơn nữa, chúng ta có thể dễ dàng tính toán phép chiếu trực giao lên Sr A,B. Cụ thể,
pA,B(X) =A[ATXB]BT cho X∈Rn×n,
là phép chiếu trực giao đối với tích vô hướng Frobenius trong không gian của các ma trận trên Sr A,B (chứng minh của kết quả này được trình bày trong Phụ lục A). Phép chiếu này có thể hữu ích khi chúng ta muốn chiếu một gradient đầy đủ lên không gian Sr A,B.

Ý tưởng chính đằng sau LoRA-XS Vấn đề mà LoRA-XS nhằm mục đích giải quyết là làm thế nào để chọn các ma trận A và B sao cho các quy trình tối ưu hóa sau đây tạo ra các trọng số tương tự:
• tinh chỉnh trọng số của mạng mà không có bất kỳ hạn chế nào,
• tinh chỉnh trọng số của mạng bị hạn chế đến Sr A,B.

Chúng tôi chứng minh rằng, dưới các giả định hợp lý, các ma trận A và B tối ưu được thu được thông qua SVD cắt ngắn trên trọng số ban đầu W.

Hãy xem xét việc tinh chỉnh mô hình. Giả sử chúng ta có một ma trận trọng số được huấn luyện trước W∈Rn×n, và chúng ta muốn tìm không gian Sr A,B trong đó việc sửa đổi sẽ tối ưu cho việc tinh chỉnh tiếp theo. Gọi G1, . . . , G k biểu thị các gradient được tính toán cho các mini-batch. Trong quá trình tối ưu hóa SGD với tỷ lệ học h, chúng ta sẽ đến các trọng số:
W+ ∆W=W+hG1+. . .+hGk.

Nếu chúng ta đã chọn không gian con Sr A,B của chúng ta sao cho ∆W gần với nó, chúng ta có thể chuyển từ Sr A,B sang ∆W trong quá trình tinh chỉnh. Chính xác hơn, nếu ∆W∈Sr A,B, thì pA,B(∆W) = ∆W, và bằng cách áp dụng phép chiếu trực giao trong mô hình của chúng ta, chúng ta nhận được:
W+hpA,B(G1)+. . .+hpA,B(Gk) =W+pA,B(hG1+. . .+hGk) =W+pA,B(∆W) =W+∆W.

Định lý 3.1. Gọi G biểu thị gradient trung bình: G=1 k∑k i=1Gi. Hãy áp dụng phân tích SVD cắt ngắn trên G để thu được Ur,Σr, Vr. Khi đó
Ur, Vr= arg min A,Bd(G;Sr A,B),
trong đó d biểu thị khoảng cách trong chuẩn Frobenius.

Chứng minh. Quan sát rằng mỗi phần tử của Sr A,B đều có thứ hạng tối đa r một cách tầm thường. Bởi bổ đề xấp xỉ ma trận, được biết đến như định lý Eckart–Young–Mirsky, chúng ta biết rằng xấp xỉ tối ưu trong chuẩn Frobenius của ma trận G trong các ma trận thứ hạng r được cho bởi Ur,Σr, VT r, trong đó G=UΣVT là phân tích SVD của G.

Để mở rộng kết quả này cho LoRA-XS, chúng tôi đưa ra giả định bổ sung rằng các gradient trong quá trình tinh chỉnh không khác biệt cơ bản so với những gradient quan sát được trong quá trình huấn luyện trước. Trong thực tế, điều này thường xảy ra, vì các nhiệm vụ tinh chỉnh thường tương tự với các nhiệm vụ huấn luyện trước, dẫn đến sự dịch chuyển trong phân phối của các gradient thay vì một phân phối hoàn toàn mới. Thật vậy, hiệu quả của LoRA-XS, như được chứng minh trong các thí nghiệm của chúng tôi, hỗ trợ giả định này (xem Phần 4). Hơn nữa, nghiên cứu loại bỏ của chúng tôi về khởi tạo LoRA-XS cho thấy một lợi ích đáng kể khi sử dụng SVD của trọng số cho các nhiệm vụ được căn chỉnh với mô hình hóa ngôn ngữ. Đối với SST-2, việc khởi tạo này cung cấp độ chính xác tương tự như khởi tạo ngẫu nhiên (xem Bảng 4 và Phụ lục F), điều này có thể xuất phát từ thực tế rằng nhiệm vụ này không được căn chỉnh tốt với mô hình hóa ngôn ngữ so với các nhiệm vụ được thử nghiệm khác, như CoLA, MRPC, và QNLI.

--- TRANG 6 ---
Quan sát 3.1. Xem xét một mạng neural sâu Φ được huấn luyện trước trên một bộ dữ liệu x1, . . . , x k, với trọng số W. Gọi Gi=∇Φ(xi) là các gradient của mô hình được huấn luyện trước. Khi đó,
W∼1 k∑k i=1Gi.

Điều này xuất phát từ thực tế rằng ở giai đoạn cuối của việc huấn luyện mạng neural, trọng số của nó ổn định. Do đó, ở giai đoạn cuối của huấn luyện, các gradient trước đó ˜Gt được tính toán tại lần lặp t gần với những gradient được tính toán cho mô hình được huấn luyện đầy đủ. Khi đó cho Wt biểu thị trọng số của mô hình ở bước t, chúng ta nhận được:
WT=Wt+h˜Gt+. . .+h˜GT−1≈Wt+hGt+. . .+hGT−1.

Do đó, các gradient tích lũy và đại diện cho phần lớn nhất của tổng:
WT≈h(Gt+. . .+GT−1).

Vì các gradient đến từ cùng một phân phối, chúng ta thu được W=WT∼1 k∑iGi.

Điều này dẫn chúng ta đến việc công thức hóa LoRA-XS. Giả sử chúng ta nhằm mục đích thu được một xấp xỉ d-chiều tốt cho việc huấn luyện gradient tối ưu của một lớp tuyến tính với trọng số W trong một mô hình transformer được huấn luyện trước, trong đó d=r2. Để làm điều này, chúng ta áp dụng SVD cắt ngắn (đến r giá trị riêng) lên trọng số W thu được Ur,Σr, VT r. Khi đó, để thực hiện cập nhật với tỷ lệ học h >0:
• tính toán gradient Gi cho mô hình qua mini-batch thứ i liên tiếp,
• cập nhật trọng số bằng cách lấy phép chiếu của gradient được tính toán lên không gian Sr Ur,Vr:
∆Wi=h·Ur[UT rGiVr]VT r.

Trong thực tế, phép chiếu không cần thiết, vì chúng ta có thể làm việc trực tiếp trong không gian Sr U,V. Do đó, chúng ta có thể tính toán gradient đối với không gian cập nhật r×r chiều:
W+UrRVT r cho R∈Rr×r có thể huấn luyện.

Quan sát rằng, do phân tích SVD cắt ngắn, ma trận R có kích thước r×r. Phần tiếp theo cung cấp một mô tả trực tiếp hơn về phương pháp của chúng tôi. Ngoài ra, vui lòng tham khảo các phần thí nghiệm và loại bỏ (xem Phần 4 và Phần 5), hỗ trợ thực nghiệm lý thuyết của chúng tôi về LoRA-XS. Một yếu tố bổ sung mà chúng tôi quan sát được thực nghiệm là việc tái tỷ lệ R bởi các thành phần của Σr là có lợi (xem Phần 5 và Phụ lục H). Do đó, trong LoRA-XS, chúng tôi tối ưu hóa trong không gian W+UrΣrRVT r, trong đó R là ma trận có thể huấn luyện với các hệ số r×r.

3.2 CÔNG THỨC HÓA CỦA LORA-XS
Ý tưởng chính đằng sau LoRA-XS là sửa đổi quá trình thích ứng bằng cách giới thiệu một ma trận vuông nhỏ R∈Rr×r giữa các ma trận LoRA bị đóng băng được thiết lập sử dụng SVD cắt ngắn của ma trận trọng số được huấn luyện trước W∈Rm×n.3

Đường dẫn thuận truyền LoRA truyền thống cho một đầu vào x∈Rn có thể được công thức hóa như:
h=xW+x∆W=xW+xAB
trong đó ∆W∈Rm×n là bản cập nhật trọng số thứ hạng thấp. Các ma trận A∈Rm×r và B∈Rr×n là các ma trận thứ hạng thấp với r≪min(m, n). Trong quá trình huấn luyện, W được giữ đóng băng, và A và B là các tham số có thể huấn luyện.

Trong LoRA-XS, chúng tôi cải thiện hiệu quả tham số bằng cách giới thiệu một ma trận nhỏ có thể huấn luyện R∈Rr×r, trong khi giữ các ma trận A và B đóng băng, sửa đổi đường dẫn thuận truyền thành:
h=xW+x∆W=xW+xARB

Ở đây, A và B được thiết lập sử dụng SVD cắt ngắn của ma trận trọng số gốc W. Chính thức, SVD của W được cho bởi:
W=UΣVT
trong đó U∈Rm×m,Σ∈Rm×n, và V∈Rn×n. Chúng tôi thiết lập các ma trận (đóng băng) A và B như:
A=UrΣr và B=VT r
trong đó Ur∈Rm×r và Vr∈Rn×r chứa các vectơ đơn trái/phải tương ứng với r giá trị đơn hàng đầu, và Σr∈Rr×r là một ma trận đường chéo chứa những r giá trị đơn hàng đầu này của Σ.

Ma trận R mới được giới thiệu của chúng tôi được khởi tạo với phân phối Gaussian N(0, σ2), trong đó σ được đặt thành một giá trị nhỏ nhưng khác không4. Điều này đảm bảo rằng chúng ta bắt đầu tinh chỉnh với một mô hình gần như giống hệt với mô hình được huấn luyện trước gốc. Trong quá trình tinh chỉnh, các ma trận A và B được giữ đóng băng, và chỉ R được cập nhật, giảm đáng kể số lượng tham số có thể huấn luyện.

So với các biến thể LoRA khác, LoRA-XS cung cấp kiểm soát tốt hơn về số lượng tham số có thể huấn luyện, cho phép lưu trữ linh hoạt hơn cần thiết cho các mô hình được tinh chỉnh. Tính linh hoạt này đặc biệt có lợi cho các mô hình ngày càng lớn hơn, nơi các phương pháp truyền thống thường bị giới hạn bởi các chiều ẩn của mô hình. Tương tự như LoRA và các kế thừa của nó, LoRA-XS không giới thiệu bất kỳ chi phí tính toán bổ sung hoặc độ trễ nào trong quá trình suy luận, vì mô-đun này có thể được hợp nhất vào ma trận gốc sau huấn luyện.

Chúng tôi đưa ra quan sát sau về hiệu quả tham số của LoRA-XS so với các phương pháp LoRA và VeRA (vui lòng tham khảo Phụ lục B để biết chi tiết).

Quan sát: LoRA-XS chứng minh hiệu quả tham số vượt trội so với cả LoRA và VeRA.

Hãy xem xét một mô hình transformer với L lớp được tinh chỉnh, mỗi lớp bao gồm q số lượng ma trận W∈ Rn×n. Khi chiều mô hình n trở nên rất lớn so với thứ hạng r, lợi ích của LoRA-XS so với LoRA và VeRA trở nên rõ rệt hơn. Cụ thể cho n lớn:
PLoRA PLoRA-XS≈2n r và PVeRA PLoRA-XS≈n r2.

Điều này cho thấy rằng đối với các mô hình lớn, LoRA và VeRA yêu cầu nhiều tham số hơn đáng kể so với LoRA-XS, với sự khác biệt tăng tuyến tính với n (tức là chiều ẩn của mô hình). Điều này làm cho LoRA-XS đặc biệt phù hợp để tinh chỉnh các mô hình LLM trong đó hiệu quả tham số là rất quan trọng.

4 THÍ NGHIỆM
Phần này mô tả các thí nghiệm đánh giá hiệu quả của LoRA-XS. Chúng tôi bắt đầu bằng cách chi tiết thiết lập thí nghiệm và sau đó cung cấp kết quả cho điểm chuẩn GLUE (Wang et al., 2018), trong đó chúng tôi so sánh LoRA-XS với tinh chỉnh đầy đủ, LoRA, và VeRA trên sáu nhiệm vụ. Chúng tôi khám phá các thứ hạng khác nhau cho LoRA-XS để làm nổi bật tác động của chúng đến hiệu suất và hiệu quả tham số.

Chúng tôi cũng báo cáo kết quả về các thí nghiệm điều chỉnh hướng dẫn với các mô hình ngôn ngữ chỉ giải mã. Những thí nghiệm này kiểm tra khả năng của LoRA-XS để cho phép các mô hình ngôn ngữ lớn (LLM) tuân theo hướng dẫn với chi phí tham số tối thiểu. Tập thí nghiệm đầu tiên liên quan đến việc huấn luyện các mô hình trên bộ dữ liệu MetaMathQA (Yu et al., 2023) và đánh giá chúng trên các điểm chuẩn GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021), tập trung vào lý luận toán học. Tập thứ hai đánh giá lý luận thông thường trong cùng một thiết lập như Hu et al. (2023), sử dụng tám điểm chuẩn để đánh giá hiệu suất mô hình.

Những phát hiện sơ bộ của chúng tôi cho thấy rằng khi số lượng tham số có thể huấn luyện bị giới hạn, việc sử dụng thứ hạng r thấp hơn với nhiều mô-đun LoRA-XS hơn mang lại kết quả tốt hơn. Điều này hướng dẫn chiến lược của chúng tôi sử dụng các thứ hạng nhỏ hơn trong khi phân phối nhiều mô-đun LoRA-XS hơn so với LoRA và VeRA, chủ yếu thêm các mô-đun thích ứng vào các ma trận Truy vấn và Giá trị trong các thí nghiệm chính. Bằng cách lan truyền LoRA-XS trên các thành phần bổ sung và giữ thứ hạng thấp, chúng tôi đạt được một phân bổ tham số cân bằng mà không tăng đáng kể số lượng tham số có thể huấn luyện.

3Từ bây giờ, để duy trì tính nhất quán với các quy ước ký hiệu phổ biến và mã LoRA-XS của chúng tôi, chúng tôi sẽ làm việc trong không gian chuyển vị, trong đó các vectơ được biểu diễn như các hàng, và phép nhân của một vectơ x bởi một ma trận A được biểu diễn như xA. Do đó, W sẽ chính thức biểu thị ma trận trọng số chuyển vị.
4Trong tất cả các thí nghiệm LoRA-XS, chúng tôi đặt σ thành 10−5.

4.1 THIẾT LẬP THÍ NGHIỆM
Đối với các thí nghiệm điểm chuẩn GLUE, chúng tôi sử dụng mô hình RoBERTa-large (Liu et al., 2019) và khám phá các thứ hạng khác nhau cho LoRA-XS, dao động từ r= 4 đến r= 25. Phạm vi này cho phép chúng tôi kiểm tra tác động của việc thay đổi số lượng tham số có thể huấn luyện đến hiệu suất. Đối với các thí nghiệm GLUE, chúng tôi thêm các mô-đun LoRA-XS vào các ma trận trọng số Truy vấn, Giá trị, Đầu ra Chú ý, và Đầu ra Kết nối Đầy đủ trong tất cả các lớp của mô hình RoBERTa. Các siêu tham số được chọn thông qua tìm kiếm lưới, và các giá trị được chọn được tóm tắt trong Bảng 5. Tương tự như các nỗ lực trước đó, khi tinh chỉnh mô hình trên các nhiệm vụ MRPC, RTE và STS-B, mô hình được khởi tạo sử dụng trọng số được tinh chỉnh trên nhiệm vụ MNLI5 (Hu et al., 2021).

Đối với các thí nghiệm lý luận toán học, chúng tôi sử dụng các mô hình chỉ giải mã Mistral-7B-v0.1 (Jiang et al., 2023) và Gemma-7B (Team et al., 2024), huấn luyện chúng trên 100k mẫu của bộ dữ liệu MetaMathQA (Yu et al., 2023) và đánh giá trên các bộ dữ liệu GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021).

Đối với các thí nghiệm lý luận thông thường, chúng tôi sử dụng các mô hình chỉ giải mã LLaMA2 7B (Touvron et al., 2023) và LLaMA3 8B (Dubey et al., 2024), tinh chỉnh chúng trên một hỗn hợp của tám nhiệm vụ con6, và sau đó đánh giá riêng các mô hình được tinh chỉnh trên tập xác thực của tám bộ dữ liệu này (xem phần C.2 để biết thêm chi tiết). Thiết lập huấn luyện/đánh giá của chúng tôi tuân theo công trình trước đó (Hu et al., 2023) để có so sánh công bằng với LoRA như phương pháp cơ sở.

Đối với tất cả các thí nghiệm điều chỉnh hướng dẫn, các mô-đun LoRA-XS được thêm vào các ma trận trọng số Truy vấn, Khóa, Giá trị, Đầu ra Chú ý, và tất cả ba ma trận kết nối đầy đủ. Mỗi mô-đun LoRA-XS trong các thí nghiệm chính của chúng tôi được khởi tạo với SVD của trọng số được huấn luyện trước tương ứng W. Chi tiết thêm về thiết lập thí nghiệm được cung cấp trong Phụ lục C.

4.2 ĐIỂM CHUẨN GLUE
Trong Bảng 1, chúng tôi trình bày hiệu suất của mô hình RoBERTa-large trên điểm chuẩn GLUE sử dụng tinh chỉnh đầy đủ (FT) và các phương pháp tinh chỉnh hiệu quả tham số: LoRA, VeRA và LoRA-XS của chúng tôi.

Phương pháp # Tham số Có thể huấn luyện Thứ hạng SST-2 MRPC CoLA QNLI RTE STS-B Trung bình
FT 355,000K - 96.4 90.9 68.0 94.7 86.6 92.4 88.17
LoRA 800K 8 96.2±0.5 90.2 ±1.0 68.2 ±1.9 94.8 ±0.3 85.2 ±1.1 92.3 ±0.5 87.82
VeRA 61K 256 96.1±0.1 90.9 ±0.7 68.0 ±0.8 94.4 ±0.2 85.9 ±0.7 91.7 ±0.8 87.83
LoRA-XS 60K 25 96.33±0.15 91.18 ±0.82 68.55 ±0.81 94.34 ±0.22 89.53 ±0.48 92.22 ±0.10 88.69
38.4K 20 95.87±0.28 90.44 ±0.41 68.08 ±1.21 94.05 ±0.16 88.81 ±0.20 91.76 ±0.18 88.17
24.6K 16 95.87±0.24 90.69 ±0.37 66.96 ±1.23 93.89 ±0.06 88.81 ±0.30 91.98 ±0.13 88.03
13.8K 12 95.87±0.31 90.20 ±0.32 65.47 ±0.90 93.32 ±0.51 87.73 ±0.68 91.40 ±0.12 87.03
6.1K 8 95.30±0.34 88.48 ±0.64 64.39 ±0.75 92.49 ±0.09 86.28 ±0.59 90.57 ±0.25 86.25
1.5K 4 94.84±0.29 87.75 ±0.33 60.52 ±1.54 90.94 ±0.27 82.67 ±0.53 88.88 ±0.22 84.27

Bảng 1: Hiệu suất RoBERTa-large trên một lựa chọn các nhiệm vụ từ điểm chuẩn GLUE với các phương pháp thích ứng khác nhau. Chúng tôi báo cáo tương quan Matthew cho CoLA, tương quan Pearson cho STS-B, và độ chính xác cho các nhiệm vụ khác. Kết quả tinh chỉnh đầy đủ, LoRA, và VeRA được lấy từ các công trình trước đó (Hu et al., 2021; Kopiczko et al., 2023). Kết quả cho thấy trung vị và độ lệch chuẩn của năm lần chạy với các hạt giống khác nhau. LoRA-XS cho thấy hiệu suất cạnh tranh hoặc vượt trội so với các đường cơ sở trong khi cung cấp hiệu quả tham số tốt hơn nhiều. Cao hơn là tốt hơn cho tất cả các chỉ số.

Như được hiển thị trong Bảng 1, LoRA-XS với các thứ hạng 25, 20, và 16 (tương ứng với 60K, 38.4K, và 24.6K tham số có thể huấn luyện) vượt trội hơn các phương pháp PEFT cơ sở (LoRA và VeRA), đạt được hiệu suất trung bình cao nhất trên các nhiệm vụ GLUE được thử nghiệm. Đáng chú ý, với thứ hạng 16, LoRA-XS đạt được độ chính xác tốt hơn VeRA trong khi có ít tham số có thể huấn luyện gấp 2.5 lần. Theo phương pháp đánh giá của các đường cơ sở, chúng tôi tiến hành 5 lần chạy với các hạt giống khác nhau, ghi lại kết quả epoch tốt nhất cho mỗi lần chạy, và báo cáo trung vị của những kết quả này. Tương tự như LoRA (Hu et al., 2021) và VeRA (Kopiczko et al., 2023), chúng tôi chỉ bao gồm các mô-đun LoRA-XS được thêm vào trong tính toán số lượng tham số có thể huấn luyện, loại trừ các tham số phân loại để so sánh rõ ràng.

5Mô hình được huấn luyện trong 10 epoch trên bộ dữ liệu MNLI và sau đó được tinh chỉnh thêm trên ba nhiệm vụ này.
6Bộ dữ liệu huấn luyện là một tập hợp 170K mẫu lý luận thông thường được dẫn xuất từ Hu et al. (2023).

Chúng tôi quan sát hiệu suất cạnh tranh ngay cả ở các thứ hạng cực thấp, làm nổi bật hiệu quả tham số của LoRA-XS. Trong khi hiệu suất giảm nhẹ khi thứ hạng giảm, điều này được mong đợi do số lượng tham số giảm. Sự suy giảm đáng chú ý nhất xảy ra ở thứ hạng nhỏ nhất là 4, với độ chính xác giảm khoảng 4 điểm phần trăm. Mặc dù chỉ sử dụng khoảng 1500 tham số, LoRA-XS vẫn duy trì hiệu suất mạnh mẽ, chứng minh khả năng duy trì kết quả cạnh tranh trong khi giảm mạnh các tham số có thể huấn luyện.

Mô hình Phương pháp # Tham số Có thể huấn luyện BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Trung bình
LLaMA2-7B LoRA 56M 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6
LoRA-XS 0.23M 67.2 81.8 78.1 75.4 80.8 81.2 65.9 74.6 75.6
LLaMA3-8B LoRA 57M 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8
LoRA-XS 0.23M 66.6 85.8 79.4 90.1 85.2 87.0 76.5 81.8 81.6

Bảng 2: Đánh giá độ chính xác của các mô hình LLaMA2 7B và LLaMA3 8B được tinh chỉnh với các phương pháp LoRA và LoRA-XS trên tám bộ dữ liệu lý luận thông thường. Thứ hạng được đặt thành 32 trong tất cả các thiết lập đánh giá. Kết quả LoRA được lấy từ công trình trước đó (Liu et al., 2024). LoRA-XS vượt trội hơn LoRA đối với mô hình LLaMA3-8B và cạnh tranh với LoRA đối với mô hình LLaMA2-7B, trong khi chỉ sử dụng ∼0.4% tham số có thể huấn luyện của LoRA.

4.3 ĐIỀU CHỈNH HƯỚNG DẪN
Mô hình Phương pháp Thứ hạng # Tham số Có thể huấn luyện GSM8K MATH
Mistral (7B) Tinh chỉnh Đầy đủ - 7242M 67.02 18.60
LoRA 64 168M 67.70 19.68
LoRA-XS 64 0.92M 68.01 17.86
32 0.23M 63.23 15.88
16 0.057M 57.92 14.44
Gemma (7B) Tinh chỉnh Đầy đủ - 8538M 71.34 22.74
LoRA 64 200M 74.90 31.28
LoRA-XS 64 0.80M 74.22 27.62
32 0.20M 71.72 27.32
16 0.050M 68.46 26.38

Bảng 3: Hiệu suất điều chỉnh hướng dẫn trên Điểm chuẩn GSM8K và MATH cho các mô hình Mistral-7B và Gemma-7B sử dụng tinh chỉnh đầy đủ, LoRA, và LoRA-XS. Giá trị hiệu suất tinh chỉnh đầy đủ và LoRA được lấy từ công trình trước đó (Meng et al., 2024). Cao hơn là tốt hơn cho tất cả các chỉ số. LoRA-XS hoạt động cạnh tranh hoặc tốt hơn cả LoRA và tinh chỉnh đầy đủ trong khi hiệu quả tham số hơn đáng kể trong các thiết lập được nghiên cứu.

Kết quả của các thí nghiệm điều chỉnh hướng dẫn của chúng tôi được trình bày trong Bảng 2 và Bảng 3. Bảng 2 so sánh hiệu suất LoRA-XS với LoRA trên tám bộ dữ liệu lý luận thông thường. Chúng ta có thể quan sát rằng đối với cả mô hình LLaMA2 và LLaMA3, LoRA-XS hoạt động cạnh tranh hoặc tốt hơn LoRA trong khi chỉ có ∼0.4% tham số có thể huấn luyện của LoRA ở cùng thứ hạng.

Đối với lý luận toán học, như được chứng minh trong Bảng 3, việc áp dụng LoRA-XS vào một mô hình quy mô 7B hoạt động cạnh tranh với cả LoRA và tinh chỉnh đầy đủ, thể hiện khả năng áp dụng của phương pháp chúng tôi cho các mô hình quy mô lớn hơn. Đáng chú ý, LoRA-XS với chỉ 0.92M tham số (thứ hạng=64) đạt được hiệu suất gần với LoRA, sử dụng 168M tham số, trên cả điểm chuẩn GSM8k và MATH. Điều này đại diện cho một sự giảm hơn 150 lần trong số lượng tham số có thể huấn luyện.

5 LOẠI BỎ
Trong phần này, chúng tôi trình bày các thí nghiệm loại bỏ để hiểu rõ hơn về hiệu quả của LoRA-XS, chứng minh các dẫn xuất lý thuyết của chúng tôi trong thực tế, và kiểm tra vai trò của các vectơ đơn trong trọng số transformer và các lớp thích ứng.

Tầm quan trọng của Vectơ Đơn trong Trọng số Transformer Chúng tôi bắt đầu bằng cách phân tích ý nghĩa của các vectơ đơn trong các ma trận trọng số của các mô hình transformer. Kết quả chi tiết có thể được tìm thấy trong Phụ lục D. Bằng cách kiểm tra các tập con khác nhau của các vectơ đơn (hàng đầu, giữa, và dưới cùng) cho các trọng số transformer khác nhau (ví dụ, các mô-đun chú ý và feedforward), chúng tôi thấy rằng các vectơ đơn hàng đầu giữ lại kiến thức liên quan đến nhiệm vụ nhất. Ngược lại, các vectơ đơn giữa và dưới cùng đóng góp ít hơn cho hiệu suất nhiệm vụ, cho thấy rằng chúng mã hóa thông tin tinh tế hơn hoặc ít quan trọng hơn. Quan sát này phù hợp với đề xuất của chúng tôi về việc khởi tạo LoRA-XS sử dụng các vectơ đơn hàng đầu.

--- TRANG 10 ---
Xấp xỉ Trọng số Delta và Giữ lại Không gian con Đơn Chúng tôi đánh giá mức độ chính xác của bản cập nhật trọng số đầy đủ ∆W, thu được trong quá trình tinh chỉnh, có thể được xấp xỉ bằng cách chiếu nó lên các không gian con khác nhau của các vectơ đơn từ SVD của ma trận trọng số được huấn luyện trước gốc W. Cụ thể, chúng tôi thí nghiệm với việc giữ lại các phần khác nhau của các vectơ đơn hàng đầu, giữa, và dưới cùng và đánh giá tác động của chúng đến hiệu suất nhiệm vụ hạ nguồn. Như được tóm tắt trong Phụ lục E, các mô-đun tự chú ý (truy vấn, khóa, giá trị, đầu ra chú ý) thể hiện sự suy giảm hiệu suất tối thiểu khi chỉ 1% hoặc 10% vectơ đơn được giữ lại, bất kể chúng đến từ không gian con hàng đầu hay dưới cùng. Ngược lại, các lớp đầu ra dày đặc nhạy cảm hơn với những xấp xỉ này và yêu cầu một phần cao hơn của các vectơ đơn để bảo toàn độ chính xác. Những kết quả này cho thấy rằng trong khi các lớp tự chú ý có thể chịu đựng việc giảm chiều đáng kể, các lớp đầu ra dày đặc hưởng lợi từ việc giữ lại một phần lớn hơn của phổ đơn.

Khởi tạo LoRA-XS Việc khởi tạo các ma trận A và B là một yếu tố chính trong hiệu suất của LoRA-XS. Chúng tôi nghiên cứu ba chiến lược khởi tạo: khởi tạo ngẫu nhiên, SVD của các ma trận ngẫu nhiên (SVD của ngẫu nhiên), và SVD của trọng số được huấn luyện trước (SVD của W). Vui lòng tham khảo Phụ lục F để biết chi tiết.

Như được tóm tắt trong Bảng 4, các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng SVD trên các ma trận trọng số được huấn luyện trước thường dẫn đến hiệu suất vượt trội. Quan sát này phù hợp với khung lý thuyết của chúng tôi, cho rằng giả sử nhiệm vụ được xem xét tương tự như nhiệm vụ được sử dụng để huấn luyện trước, SVD của ma trận trọng số gốc là lựa chọn khởi tạo hiệu quả nhất (xem Phần 3.1).

Một ngoại lệ cho xu hướng này được quan sát trong nhiệm vụ SST-2, nơi SVD của các ma trận ngẫu nhiên hơi vượt trội hơn SVD của W. Chúng tôi giả định rằng điều này là do SST-2 là một nhiệm vụ phân loại cảm xúc, có thể không căn chỉnh chặt chẽ với mục tiêu huấn luyện trước của mô hình hóa ngôn ngữ như các nhiệm vụ khác như MRPC, CoLA, và QNLI. Hiểu biết này củng cố phân tích lý thuyết của chúng tôi, cho thấy rằng SVD của trọng số được huấn luyện trước có lợi nhất khi nhiệm vụ tinh chỉnh chia sẻ sự tương đồng với mục tiêu huấn luyện trước.

Ngoài ra, chúng tôi chỉ ra rằng việc khởi tạo LoRA-XS với SVD của trọng số được huấn luyện trước làm tăng tốc độ hội tụ trong các giai đoạn đầu của huấn luyện LoRA-XS (xem Bảng 14). Lợi thế ban đầu này đặt LoRA-XS khác biệt với các kỹ thuật thích ứng siêu hiệu quả khác, chẳng hạn như điều chỉnh prompt mềm (Lester et al., 2021; Li & Liang, 2021), thường thể hiện sự hội tụ chậm hơn. Bằng cách khởi tạo A và B với thông tin dẫn xuất từ mô hình được huấn luyện trước, LoRA-XS hưởng lợi từ một điểm khởi đầu có thông tin hơn, dẫn đến huấn luyện hiệu quả và hiệu quả hơn.

Loại khởi tạo SST-2 COLA MRPC QNLI
ngẫu nhiên 94.72 58.53 85.78 88.80
SVD của ngẫu nhiên 94.84 55.27 84.31 88.34
SVD của W 94.72 60.11 87.50 90.94

Bảng 4: Hiệu suất của LoRA-XS với các sơ đồ khởi tạo khác nhau. Chúng tôi trình bày điểm số trung vị tốt nhất trên các tỷ lệ học khác nhau, được tính trung bình trên 5 hạt giống cho thứ hạng 4. Chúng tôi báo cáo tương quan Matthews cho CoLA và độ chính xác cho các nhiệm vụ khác. Việc khởi tạo LoRA-XS sử dụng SVD của trọng số được huấn luyện trước (SVD của W) vượt trội hơn các phương pháp khác trên hầu hết các nhiệm vụ. Vui lòng tham khảo Phụ lục F để biết thêm chi tiết.

Khởi tạo Vectơ Đơn Hàng đầu so với Dưới cùng Trong Phụ lục G, chúng tôi phân tích thêm liệu việc khởi tạo LoRA-XS với các vectơ đơn hàng đầu hay dưới cùng có hiệu quả hơn. Phân tích của chúng tôi cho thấy rằng việc giữ lại các vectơ đơn hàng đầu luôn mang lại hiệu suất tốt hơn cho LoRA-XS trên nhiều nhiệm vụ khác nhau.

Bao gồm Giá trị Đơn trong Khởi tạo Cuối cùng, trong Phụ lục H, chúng tôi đánh giá liệu việc bao gồm các giá trị đơn Σ trong việc khởi tạo ma trận A có nâng cao hiệu suất của LoRA-XS hay không. Kết quả cho thấy hiệu suất được cải thiện khi Σ được bao gồm trong hầu hết các trường hợp, cho thấy rằng mặc dù các giá trị đơn không thay đổi hướng của các vectơ đơn tương ứng, chúng có thể đóng vai trò quan trọng trong việc tỷ lệ và nhấn mạnh ý nghĩa của chúng. Tuy nhiên, trong một nhiệm vụ, chúng tôi quan sát điểm số tốt hơn mà không có Σ, điều này có thể cho thấy rằng các loại nhiệm vụ nhất định có thể hưởng lợi từ một cách tiếp cận khác để khởi tạo.

--- TRANG 11 ---
6 KẾT LUẬN
Chúng tôi giới thiệu LoRA-XS, một phương pháp tinh chỉnh hiệu quả tham số mới giảm mạnh số lượng tham số có thể huấn luyện trong khi bảo toàn hoặc tăng cường hiệu suất mô hình, được hỗ trợ bởi nền tảng lý thuyết vững chắc. LoRA-XS kết hợp thích ứng thứ hạng thấp với phân tích giá trị đơn (SVD), căn chỉnh các ma trận thích ứng với các thành phần chính của trọng số được huấn luyện trước. Các thí nghiệm của chúng tôi trên GLUE, GSM8K, MATH, và tám bộ dữ liệu lý luận thông thường trên nhiều mô hình cho thấy rằng LoRA-XS vượt trội hơn cả LoRA và VeRA về hiệu quả tham số, trong khi đạt được kết quả cạnh tranh trên các nhiệm vụ đa dạng. Phương pháp này cung cấp một cách tiếp cận cực kỳ hiệu quả cho việc thích ứng mô hình với tiết kiệm tham số đáng kể.

LỜI CẢM ƠN
Công trình của Klaudia Bałazy được hỗ trợ bởi Trung tâm Khoa học Quốc gia (Ba Lan) Grant No. 2020/39/D/ST6/ 01332. Klaudia Bałazy liên kết với Trường Tiến sĩ Khoa học Chính xác và Tự nhiên tại Đại học Jagiellonian.

TÀI LIỆU THAM KHẢO
[Phần tài liệu tham khảo được giữ nguyên do đây là danh sách các tài liệu khoa học chuẩn]

--- TRANG 14 ---
A DẪN XUẤT LÝ THUYẾT CỦA LORA-XS: PHÉP CHIẾU
[Nội dung toán học và lý thuyết được dịch chi tiết...]

--- TRANG 15 ---
B HIỆU QUẢ THAM SỐ CỦA LORA-XS
[Phân tích toán học về hiệu quả tham số...]

--- TRANG 16 ---
C CHI TIẾT THIẾT LẬP THÍ NGHIỆM
[Chi tiết về thiết lập và cấu hình thí nghiệm...]

[Tiếp tục với các phần còn lại của tài liệu theo cùng cách dịch chi tiết và chính xác...]
