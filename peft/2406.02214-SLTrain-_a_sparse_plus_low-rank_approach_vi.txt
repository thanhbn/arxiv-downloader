# SLTrain: một phương pháp thưa thớt cộng rank thấp cho việc huấn luyện trước hiệu quả về tham số và bộ nhớ

Andi Han1Jiaxiang Li2Wei Huang1Mingyi Hong2Akiko Takeda1,3
Pratik Jawanpuria4Bamdev Mishra4

1RIKEN AIP (andi.han@riken.jp, wei.huang.vr@riken.jp)
2University of Minnesota, Twin Cities (li003755@umn.edu, mhong@umn.edu)
3University of Tokyo (takeda@mist.i.u-tokyo.ac.jp)
4Microsoft, India (pratik.jawanpuria@microsoft.com, bamdevm@microsoft.com)

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng ấn tượng trên nhiều tác vụ khác nhau. Tuy nhiên, việc huấn luyện LLM từ đầu đòi hỏi sức mạnh tính toán đáng kể và dung lượng bộ nhớ lớn. Các nghiên cứu gần đây đã khám phá cấu trúc rank thấp trên trọng số để tinh chỉnh hiệu quả về mặt tham số và bộ nhớ, thông qua việc thích ứng rank thấp hoặc phân tích nhân tử. Mặc dù hiệu quả cho việc tinh chỉnh, cấu trúc rank thấp thường ít phù hợp cho việc huấn luyện trước vì chúng hạn chế tham số vào một không gian con chiều thấp. Trong nghiên cứu này, chúng tôi đề xuất tham số hoá trọng số như một tổng của ma trận rank thấp và thưa thớt cho việc huấn luyện trước, mà chúng tôi gọi là SLTrain. Thành phần rank thấp được học thông qua phân tích nhân tử ma trận, trong khi đối với thành phần thưa thớt, chúng tôi sử dụng một chiến lược đơn giản là chọn đồng nhất support thưa thớt một cách ngẫu nhiên và chỉ học các entry khác không với support cố định. Mặc dù đơn giản, chiến lược học thưa thớt support cố định ngẫu nhiên cải thiện đáng kể việc huấn luyện trước khi kết hợp với học rank thấp. Kết quả của chúng tôi cho thấy SLTrain thêm chi phí tham số và bộ nhớ tối thiểu so với huấn luyện trước với tham số hoá rank thấp, nhưng đạt hiệu suất tốt hơn đáng kể, có thể so sánh với huấn luyện full-rank. Đáng chú ý, khi kết hợp với lượng tử hoá và cập nhật theo từng lớp, SLTrain có thể giảm yêu cầu bộ nhớ lên đến 73% khi huấn luyện trước mô hình LLaMA 7B.

## 1 Giới thiệu
Các mô hình nền tảng lớn đã đạt được thành công to lớn trong nhiều lĩnh vực khác nhau, bao gồm ngôn ngữ học, thị giác máy tính và sinh học. Đặc biệt, các mô hình ngôn ngữ lớn (LLM), như series GPT [39,5] và họ LLaMA [51,52] đã thay đổi nhận thức về cách máy hiểu ngôn ngữ con người. Thành công chủ yếu của những mô hình này chủ yếu do kích thước mô hình, thường mở rộng đến hàng trăm tỷ tham số. Các định luật mở rộng dường như gợi ý rằng khả năng của LLM tăng theo kích thước mô hình [25], nhưng tuy nhiên đòi hỏi lượng tài nguyên khổng lồ cho việc huấn luyện trước, lưu trữ và tinh chỉnh. Đặc biệt, yêu cầu bộ nhớ cho việc huấn luyện LLM tạo ra rào cản cứng cho việc triển khai mô hình trên GPU thương mại. Ví dụ, mô hình LLaMA 7B yêu cầu chi phí bộ nhớ tối thiểu khoảng 42G dưới định dạng điểm nổi 16-bit, bao gồm 14G trạng thái tham số và 28G trạng thái optimizer cho các optimizer dựa trên momentum, như Adam [59, 28].

Xây dựng một LLM (từ đầu) cho các tác vụ downstream thường bao gồm hai giai đoạn, tức là huấn luyện trước và tinh chỉnh. Mục tiêu của huấn luyện trước là nắm bắt các mẫu và ngữ nghĩa ngôn ngữ tổng quát, cho phép mô hình thu được các biểu diễn hữu ích của từ và câu. Các mục tiêu huấn luyện trước phổ biến bao gồm mô hình hoá ngôn ngữ có mặt nạ [26], dự đoán token tiếp theo [39,40], v.v.

Tinh chỉnh sau đó điều chỉnh các biểu diễn mô hình đã học từ huấn luyện trước cho các tác vụ downstream, điều chỉnh trọng số của nó để nâng cao hiệu suất trên các mục tiêu cụ thể. Được khởi xướng bởi LoRA [21], các nghiên cứu gần đây đã phổ biến việc tinh chỉnh rank thấp của một mô hình đã được huấn luyện trước (W0), trong đó W0 thường là full-rank (tức là được huấn luyện trước mà không có bất kỳ ràng buộc nào). Tiền đề là LLM thường thích ứng với các tác vụ downstream trong một không gian con chiều thấp, điều này cho phép tham số hoá việc cập nhật bằng các nhân tử rank thấp. Tinh chỉnh rank thấp yêu cầu tham số có thể huấn luyện tối thiểu và giảm đáng kể tài nguyên bộ nhớ và tính toán [10,12]. Một số nghiên cứu [14,53,18,29,30,34,3] đã xuất hiện để cải thiện thêm hiệu quả và khả năng thích ứng của LoRA.

Trong khi hầu hết các nghiên cứu đã tập trung vào khai thác cấu trúc rank thấp cho việc tinh chỉnh, chỉ một số ít [27,24,43,47] đã xem xét huấn luyện trước với trọng số rank thấp. Người ta quan sát thấy rằng hiệu suất của huấn luyện rank thấp thường tụt hậu so với huấn luyện full-rank mặc dù có tiềm năng lớn để cải thiện hiệu quả huấn luyện và bộ nhớ [47,59]. Điều này là do mạng neural thường thể hiện cấu trúc full-rank trong trọng số và việc áp đặt các hạn chế rank thấp có thể hạn chế đáng kể sức mạnh biểu diễn của chúng. Do đó, các nghiên cứu gần đây đã khám phá huấn luyện full-rank với các cập nhật rank thấp. Ví dụ, ReLoRA [32] định kỳ khởi động lại LoRA, trong đó các cập nhật rank thấp được hợp nhất với trọng số từ giai đoạn cuối. Tuy nhiên, ReLoRA cũng yêu cầu huấn luyện full-rank khởi động ấm để đạt hiệu suất cạnh tranh [32]. GaLore [59] đi theo một con đường khác bằng cách thực thi cấu trúc rank thấp không phải trên trọng số mà trên gradient. Điều này cho phép các trạng thái optimizer Adam được lưu trữ trong không gian chiều thấp. Mặc dù hiệu quả về bộ nhớ, GaLore không hiệu quả về tham số vì nó vẫn thực hiện cập nhật tham số đầy đủ với gradient rank thấp "được chiếu ngược".

Hiệu quả tham số là một thuộc tính mong muốn sau huấn luyện trước cho việc triển khai mô hình, tinh chỉnh và lưu trữ mô hình. Mặt khác, hiệu quả bộ nhớ là cần thiết để huấn luyện mô hình với yêu cầu phần cứng thấp hơn. Mặc dù tầm quan trọng của cả hiệu quả tham số và bộ nhớ, hai mục tiêu này thường được theo đuổi độc lập. Trong khi các mô hình rank thấp đạt được cả hiệu quả tham số và bộ nhớ, như đã thảo luận trước đó, chúng không hoạt động tốt nói chung [47,59]. Do đó, một câu hỏi tự nhiên là:

làm thế nào chúng ta có thể điều chỉnh huấn luyện rank thấp để đạt được hiệu suất tương đương với huấn luyện full-rank trong khi duy trì cả hiệu quả tham số và bộ nhớ?

**Đóng góp.** Trong nghiên cứu này, chúng tôi trả lời câu hỏi trên bằng cách tham số hoá trực tiếp trọng số như các nhân tử rank thấp cộng thưa thớt cho huấn luyện trước. Cần lưu ý rằng cả nhân tử rank thấp và thưa thớt đều tạo thuận lợi cho hiệu quả tham số một cách riêng lẻ. Hơn nữa, sự kết hợp của chúng (thường) đảm bảo rằng mô hình được huấn luyện trước cuối cùng có rank cao. Các chiến lược hiện có cho học thưa thớt thường bao gồm prune-and-growth [13,2,58,49] mà lặp đi lặp lại huấn luyện, cắt tỉa và phát triển neuron. Chiến lược như vậy thường không hiệu quả về bộ nhớ do nhu cầu lưu trữ (và học) một support và một ma trận trọng số dày đặc. Ngược lại, chúng tôi thúc đẩy và áp dụng một chiến lược đơn giản hơn là cố định một support ngẫu nhiên đồng nhất (cho nhân tử thưa thớt). Điều này cho phép chỉ lưu trữ chỉ số và giá trị để huấn luyện hiệu quả bộ nhớ, mà mở rộng theo số lượng entry khác không. Chúng tôi cho thấy phương pháp đơn giản như vậy cho phép giảm thêm mức tiêu thụ bộ nhớ trong quá trình huấn luyện trước so với ReLoRA [32] và GaLore [59] mà không hy sinh hiệu suất. Chúng tôi cho thấy điều này thông qua một tập hợp thí nghiệm mở rộng trên các mô hình ngôn ngữ LLaMA với kích thước mô hình khác nhau từ 60M lên đến 7B tham số. Chúng tôi gọi thuật toán huấn luyện trước thưa thớt cộng rank thấp được đề xuất của chúng tôi là SLTrain. Trong Hình 1, chúng tôi quan sát rằng SLTrain có được điểm perplexity tương đương với mô hình full-rank với hiệu quả bộ nhớ và tham số đáng kể.

Chúng tôi kết thúc phần này bằng cách lưu ý rằng ý tưởng kết hợp các nhân tử rank thấp và thưa thớt đã được khám phá cho khôi phục ma trận mạnh mẽ [6,57,4], xấp xỉ ma trận attention [7], và nén mạng neural [31]. Tuy nhiên, nó được giới thiệu lần đầu tiên cho huấn luyện trước LLM trong nghiên cứu của chúng tôi.

## 2 Nền tảng về huấn luyện trước rank thấp

Các nghiên cứu huấn luyện trước hiện có [24,43] đã khám phá tham số hoá rank thấp của trọng số lớp trực tiếp như W=BA. Tuy nhiên, người ta quan sát thực nghiệm rằng tham số hoá rank thấp vanilla chịu sự suy giảm hiệu suất lớn do khả năng biểu diễn hạn chế [47,32,59]. Do đó, được thúc đẩy từ thích ứng rank thấp (LoRA) [21] cho tinh chỉnh, cho huấn luyện trước, ReLoRA [32] đề xuất tham số hoá trọng số lớp như

W=W0+∑(s=1 to m) BsAs, (1)

trong đó m biểu thị số lượng nhân tử rank thấp. Tham số hoá này dẫn đến một cập nhật rank cao tổng thể so với LoRA vì tổng của các ma trận rank thấp thường là một ma trận rank cao hơn. Tối ưu hoá được thực hiện bằng cách huấn luyện Bs, As lặp đi lặp lại, hợp nhất Ws←Ws−1+BsAs, và sau đó khởi động lại tối ưu hoá cho Bs+1, As+1. Một nhược điểm chính của ReLoRA là nó lưu trữ ma trận full-rank Ws trong suốt giai đoạn huấn luyện và suy luận. Do đó, nó tốn bộ nhớ và không hiệu quả về tham số. Trong khi ReLoRA thực hiện các cập nhật rank thấp tuần tự trong (1), một nghiên cứu gần đây [22] đã khám phá các cập nhật rank thấp song song và hợp nhất chúng cho huấn luyện trước.

Một nghiên cứu gần đây hơn, GaLore [59], áp đặt cấu trúc rank thấp trên gradient. Cụ thể, GaLore vẫn tối ưu hoá trọng số full-rank và tính toán gradient full-rank Gt tại lần lặp t, nhưng cập nhật các moment Adam Mt, Vt trong không gian chiều thấp, tức là,

Mt←β1Mt−1+ (1−β1)P⊤tGt, Vt←β2Vt−1+ (1−β2)(P⊤tGt)²
Wt+1←Wt−η PtMt/(√Vt+ε),

trong đó Pt là ma trận chiếu được xây dựng bằng cách lấy các vector đơn vị trái lớn nhất của Gt. Để giảm chi phí tính toán, Pt được tính toán mỗi vài lần lặp và được lưu trữ ở giữa. Mặc dù hiệu quả về bộ nhớ (vì Mt và Vt được tính toán trong chiều nhỏ hơn), GaLore không hiệu quả về tham số do tính toán PtMt để cập nhật Wt.

## 3 SLTrain: huấn luyện trước thưa thớt cộng rank thấp được đề xuất

Để đạt được cả hiệu quả tham số và bộ nhớ, chúng tôi đề xuất điều chỉnh tham số hoá rank thấp bằng cách giới thiệu một nhân tử thưa thớt. Chúng tôi mô hình hoá các ma trận trọng số như một tổng của các ma trận thưa thớt và rank thấp. Mô hình hoá được đề xuất của chúng tôi được gọi là SLTrain. Dưới đây, chúng tôi thảo luận động lực, chi tiết mô hình hoá và các cân nhắc thực tế để thực hiện SLTrain.

### 3.1 Động lực cho tham số hoá thưa thớt cộng rank thấp

Cả rank thấp và độ thưa thớt đều là các chiến lược mô hình hoá tiết kiệm để khám phá các ma trận trọng số chiều thấp. Thành phần rank thấp nhằm học các cơ sở chiều thấp hoặc eigenspace của trọng số. Mặt khác, thành phần thưa thớt xác định các tương tác neuron hiệu quả và bỏ qua những tương tác không biểu cảm. Theo thuật ngữ đại số tuyến tính, thành phần rank thấp thực thi độ thưa thớt của các giá trị đơn lẻ, trong khi thành phần thưa thớt thực thi độ thưa thớt của các entry riêng lẻ. Nói chung, các ma trận rank thấp không thưa thớt, và các ma trận thưa thớt không nhất thiết phải rank thấp [6]. Những khái niệm này cung cấp thông tin bổ sung mà nên được khám phá đồng thời.

Mặc dù việc mô hình hoá rank thấp đơn thuần có thể có khả năng biểu đạt hạn chế do cấu trúc rank thấp mà nó áp đặt, chúng tôi cho thấy trong mệnh đề dưới đây rằng rank thấp cộng một ma trận thưa thớt đồng nhất với chỉ Ω(log n/n) số lượng entry là full-rank với xác suất cao.

**Mệnh đề 1.** Xem xét một ma trận S∈Rn×n với support S được lấy mẫu đồng nhất ngẫu nhiên với xác suất δ∈(0,1), tức là, P[(i, j)∈ S] =δ, cho tất cả i, j∈[n]. Giả sử δ= Ω(log n/n), thì với xác suất ít nhất 1−O(1/n), BA+S là full rank cho B, A được tạo ngẫu nhiên tùy ý.

Để thúc đẩy thêm việc mô hình hoá thưa thớt cộng rank thấp, trong Hình 2, chúng tôi minh hoạ các thống kê khác nhau từ các ma trận trọng số của một mô hình LLaMA 60M được huấn luyện trước trên tập dữ liệu C4 (được giới thiệu sau trong Phần 5). Trong Hình 2(a), chúng tôi vẽ các giá trị đơn lẻ của các ma trận trọng số của các lớp khác nhau. Biểu đồ thể hiện sự suy giảm nhanh chóng của các giá trị đơn lẻ tiếp theo là sự suy giảm ổn định hơn của các giá trị đơn lẻ (nhỏ hơn). Điều này gợi ý rằng các không gian con hàng đầu có thể hiệu quả trong nén mô hình, và do đó, xây dựng một trường hợp cho mô hình hoá rank thấp. Tuy nhiên, phân phối giá trị đơn lẻ đuôi cho thấy rằng việc mô hình hoá rank thấp thuần túy có thể không đủ. Để hiểu rõ hơn phần đuôi, trong Hình 2(b), chúng tôi trực quan hoá độ lớn của ma trận trọng số đầu ra attention trước và sau khi chúng tôi trích xuất các không gian con r-chiều hàng đầu (r= 128) cho lớp attention cuối cùng. Rõ ràng là, sau khi loại bỏ các không gian con hàng đầu, cả độ lớn và biến thiên của các entry có mặt trong ma trận dư trở nên nhỏ hơn. Vẽ biểu đồ độ lớn của các entry trong Hình 2(c) chúng ta thấy rằng 97% các entry có độ lớn dưới 0.04. Trong Phụ lục B và C, chúng tôi cung cấp các trực quan hoá như vậy cho các lớp khác của LLaMA 60M và Pythia 70M để hỗ trợ thêm các phát hiện. Nhìn chung, các hình cho thấy rằng một ma trận thưa thớt với support ngẫu nhiên có thể xấp xỉ tốt phần dư cho rằng các độ lớn không thay đổi quá nhiều trên các entry.

Trong Bảng 1, chúng tôi thực hiện một nghiên cứu ablation xác minh tính khả thi của việc sử dụng support thưa thớt ngẫu nhiên để xấp xỉ ma trận dư. Cụ thể, chúng tôi lấy L0 là xấp xỉ rank-r tốt nhất (r= 128) cho ma trận trọng số được huấn luyện trước W0 và đánh giá điểm perplexity (PPL) trên tập validation. Chúng ta thấy rằng so với mô hình được huấn luyện trước full-rank, xấp xỉ rank thấp L0 chịu sự sụt giảm hiệu suất mạnh. Chúng tôi cũng tăng cường xấp xỉ rank thấp L0 với top 3% hoặc random 3% entry của ma trận dư, mà chúng tôi gắn nhãn là top sparse hoặc random sparse pruning, tương ứng. Chúng ta quan sát rằng L0 cộng top sparse pruning hoạt động tốt hơn so với L0 cộng random sparse pruning. Tuy nhiên, cả hai hiệu suất đều kém. Chúng tôi đánh giá thêm việc cố định xấp xỉ rank thấp (thành L0) và chỉ tối ưu hoá các thành phần thưa thớt với top support hoặc random support (cả hai chạy năm lần). PPL trung bình (qua năm lần chạy) cho cả hai phương pháp đều cải thiện và có thể so sánh được. Điều này cho thấy rằng việc cố định một support ngẫu nhiên cho nhân tử thưa thớt là một chiến lược đầy hứa hẹn từ cả quan điểm hiệu quả và hiệu suất. Chúng tôi khám phá việc học cả nhân tử thưa thớt và rank thấp trong phần tiếp theo.

### 3.2 Mô hình hoá được đề xuất của chúng tôi

Dựa trên Phần 3.1, chúng tôi đề xuất tham số hoá các ma trận trọng số W∈Rd×p như

W=BA+S,

trong đó B∈Rd×r, A∈Rr×p là các nhân tử rank thấp với r <min{d, p} là tham số rank và S∈Rm×n là một ma trận thưa thớt. Số lượng entry khác không (nnz) trong S được xác định bởi tham số mức độ thưa thớt δ∈(0,1), tức là, nnz(S) =δdp. Vậy, tổng số tham số cho tham số hoá được đề xuất là (d+p)r+δdp, nhỏ hơn nhiều so với các tham số lớp full-rank dp khi chúng ta chọn δ≪1. Ngoài việc hiệu quả về tham số, các trạng thái tối ưu hoá cũng tốn ít bộ nhớ hơn và mở rộng theo số lượng tham số có thể huấn luyện. Cuối cùng, chúng tôi lưu ý rằng rank tổng thể của W thường sẽ cao do sự hiện diện của nhân tử thưa thớt S, dựa trên Mệnh đề 1.

Hiệu suất của tham số hoá như vậy phụ thuộc rất nhiều vào việc liệu có tồn tại một implementation vừa hiệu quả về tính toán vừa hiệu quả về bộ nhớ hay không. Tuy nhiên, phần cứng GPU hiện đại không phù hợp cho phép nhân ma trận thưa thớt Sx cho input x đã cho, cũng như gradient của nó, đặc biệt khi S trình bày một mẫu thưa thớt không có cấu trúc [7]. Điều này gây ra nút thắt cổ chai tính toán tăng lên mặc dù cho thấy lợi thế bộ nhớ. Do đó, các nghiên cứu hiện có về mạng thưa thớt và huấn luyện chủ yếu dựa vào việc học và lưu trữ một parameter mask (tức là, support) [48,15,33] bằng cách để S=M⊙U, trong đó M∈ {0,1}d×p là một binary mask và U∈Rd×p là một tham số dày đặc. Điều này cho phép khai thác GPU accelerator để tính toán ma trận dày đặc. Tuy nhiên, masking yêu cầu lưu trữ cả support và một tham số dày đặc để huấn luyện, điều này làm tăng đáng kể chi phí bộ nhớ.

Trong nghiên cứu này, chúng tôi đạt được hiệu quả bộ nhớ bằng cách biểu diễn S theo các chỉ số và giá trị của nó, tức là, (I,V)∈R2nnz(S). Điều này có thể vì chúng ta cố định support ngẫu nhiên (và đồng nhất) a priori. Động lực cho việc sử dụng support ngẫu nhiên (nhưng cố định) đến từ tính hữu ích của support ngẫu nhiên trong Bảng 1. Điều này đảm bảo bộ nhớ mở rộng chỉ với độ thưa thớt trong S (tức là, kích thước support) thay vì kích thước đầy đủ của S. Hơn nữa, lượt truyền thuận bao gồm tính toán

BAx +Sx= (BA⊕IV)x,

trong đó chúng ta ký hiệu W⊕IV như scatter-adding V vào W tại các chỉ số được chỉ định trong I. Bởi vì phép toán này dẫn đến một ma trận dày đặc, phép nhân ma trận thưa thớt được tránh. Do đó, điều này thân thiện với GPU mà không yêu cầu lưu trữ binary mask.

Chúng tôi nhận xét rằng mặc dù chúng ta yêu cầu tính toán một ma trận dày đặc, có cùng kích thước với ma trận full-rank, chúng ta không bao giờ lưu trữ nó cho backpropagation. Đặc biệt, chúng ta có thể tính toán gradient đối với B, A, V, và input x như

∇BL=∇zL x⊤A⊤,∇AL=B⊤∇zL x⊤,∇VL= (∇zL x⊤)I,∇xL= (BA⊕IV)⊤∇zL, (2)

trong đó chúng ta để z= (BA⊕IV)x và L ký hiệu hàm loss. Chúng ta cũng ký hiệu WI như thu thập các giá trị của W tại chỉ số I. Nói cách khác, chúng ta chỉ cần lưu trữ B, A,I,V cho backpropagation. Điều này được minh hoạ trong Thuật toán 1 nơi chúng ta định nghĩa một lớp tuyến tính tùy chỉnh trong SLTrain. Chúng tôi nhấn mạnh rằng tham số hoá như vậy là agnostic đối với các optimizer được chọn và có thể dễ dàng tích hợp với bất kỳ optimizer nào bao gồm Adam.

So với các nghiên cứu huấn luyện trước gần đây dựa trên các nhân tử/gradient rank thấp, SLTrain hiệu quả hơn về tham số và bộ nhớ so với ReLoRA [32] và GaLore [59] vì nó chỉ tối ưu hoá các nhân tử rank thấp và thưa thớt mà không cần lưu trữ các ma trận full-rank.

### 3.3 Các cân nhắc thực tế

**Khởi tạo và scaling.** Chúng tôi xem xét kiểu khởi tạo LoRA cho các nhân tử rank thấp, tức là, khởi tạo Kaiming [19] cho nhân tử A và khởi tạo zero cho nhân tử B. Đối với nhân tử thưa thớt, chúng tôi áp dụng khởi tạo đồng nhất cho các giá trị V trong khoảng [−1/√din,1/√din], trong đó din ký hiệu chiều đặc trưng đầu vào. Chúng tôi chọn support thưa thớt I đồng nhất ngẫu nhiên lên đến mức độ thưa thớt mong muốn δ. Hơn nữa, để cân bằng đóng góp của nhân tử rank thấp và nhân tử thưa thớt, chúng tôi theo LoRA [21] để scale các nhân tử rank thấp bằng α/r trong đó tham số cân bằng α là một siêu tham số. Siêu tham số này, cùng với stepsize có ảnh hưởng chung đến tốc độ huấn luyện của các nhân tử rank thấp so với nhân tử thưa thớt.

**Regularization và preconditioning.** Dự kiến rằng việc tối ưu hoá các nhân tử rank thấp có thể gây ra bất ổn khi sử dụng stepsize lớn hơn hoặc tham số cân bằng α lớn hơn, một vấn đề đã có mặt trong huấn luyện rank thấp [43]. Điều này chủ yếu do các cập nhật nhân tính của B, A đồng thời. Các giải pháp hiện có, như ràng buộc trực giao hoặc regularization [43], preconditioning [50,23,56], có thể dễ dàng kết hợp với mô hình hoá được đề xuất để hội tụ ổn định hơn.

**Tích hợp với các kỹ thuật khác.** Vì phương pháp thưa thớt cộng rank thấp được đề xuất theo đuổi tiết kiệm bộ nhớ từ góc độ reparameterization, SLTrain có thể dễ dàng tích hợp với các kỹ thuật dựa trên optimizer để cải thiện thêm hiệu quả bộ nhớ, bao gồm quantization [9] (sử dụng bit thấp hơn để lưu trữ moment states mà không hy sinh hiệu suất), cập nhật trọng số theo từng lớp [36] (cập nhật các tham số cùng với backpropagation), và activation checkpointing (tính toán lại activation states thay vì lưu trữ chúng). Ngoài ra, SLTrain thậm chí có thể được kết hợp với gradient rank thấp trong GaLore [59] cho các nhân tử rank thấp. Điều này có thể giảm thêm dấu chân bộ nhớ, đặc biệt cho các mô hình lớn hơn nơi rank r được đặt cao. Mặt khác, vì chúng ta sử dụng một chiến lược đơn giản của học thưa thớt support cố định, có thể có lợi khi kết hợp với các kỹ thuật khác nhau cho học support động [2, 20].

## 4 Các nghiên cứu liên quan

**Tinh chỉnh và huấn luyện rank thấp.** Dựa trên ý tưởng của LoRA [21] mà tham số hoá cập nhật như các nhân tử rank thấp, tức là, ∆W=BA, ROSA [14] điều chỉnh động các không gian con để huấn luyện, trong đó các không gian con được chọn bằng cách lấy SVD của các ma trận trọng số hiện tại. Chain of Lora [53] phân tích cập nhật rank thấp thành một chuỗi tích ma trận kích thước nhỏ, ∆W=∑(j=1 to k)BjAj. NOLA [29] tham số hoá hai ma trận nhỏ như tổ hợp tuyến tính của hai tập basis ngẫu nhiên tương ứng, B=∑(i=1 to m)αiBi, A=∑(j=1 to n)βjAj trong đó Ai, Bj là các ma trận ngẫu nhiên cố định. NOLA tối ưu hoá các hệ số, do đó cải thiện thêm hiệu quả tham số. VeRA [30] xem xét một tham số hoá tương tự như NOLA trong đó B= diag(b)B̃, A = diag(a)Ã cho các ma trận ngẫu nhiên cố định B̃,Ã. DoRA [34] phân tích trọng số được huấn luyện trước thành các thành phần độ lớn và hướng và tinh chỉnh riêng biệt với LoRA được áp dụng cho cập nhật hướng. SoRA [11] giới thiệu một chiến lược thích ứng rank động để điều chỉnh rank LoRA. ResLoRA [46] thêm một đường dẫn dư cho các adaptor LoRA.

Đối với **huấn luyện trước**, ngoài ReLoRA [32] và GaLore [59], Flora [17] chứng minh các cập nhật LoRA xấp xỉ chiếu ngẫu nhiên của gradient, và bằng cách lấy mẫu lại chiếu ngẫu nhiên, huấn luyện rank cao có thể đạt được. LTE [22] áp dụng ý tưởng tương tự của việc tham số hoá một ma trận rank cao thông qua tổng của các ma trận rank thấp và áp dụng chiến lược train-and-merge song song như đối lập với tuần tự trong ReLoRA [32].

**Tinh chỉnh thưa thớt, huấn luyện và mạng thưa thớt.** Tinh chỉnh/huấn luyện thưa thớt nhằm mục đích cập nhật có chọn lọc các trọng số với những cái khác được cố định [48,1,2,49,15,33]. Điều này thường đòi hỏi việc chọn một tập con thích hợp của các tham số một cách ngẫu nhiên [49], hoặc dựa trên thông tin Fisher xấp xỉ [1], độ lớn của thay đổi [48], gradient và momenta [2], cũng như bằng cách học một parameter mask (để lưu trữ support) với regularization thưa thớt [15]. Mặt khác, mạng thưa thớt, còn được gọi là model pruning, tìm kiếm trực tiếp một kiến trúc tối thiểu [16,35] bằng cách loại bỏ các trọng số dư thừa. Chúng tôi tham khảo khảo sát [20] để thảo luận đầy đủ về pruning mạng thưa thớt.

**Thưa thớt cộng rank thấp.** Phân tích một ma trận thành tổng của ma trận rank thấp và thưa thớt là một bài toán tối ưu hoá cổ điển cho khôi phục ma trận [6,54,4]. Gần đây, một số nghiên cứu cũng xem xét khai thác cả cấu trúc rank thấp và độ thưa thớt cho nén mạng neural. Scatterbrain [7] xem xét xấp xỉ ma trận attention để suy luận nhanh hơn với các nhân tử thưa thớt cộng rank thấp. Cụ thể hơn, cho Q, K, V ∈Rn×d Mục tiêu chính là xấp xỉ hiệu quả exp(QK⊤)V, mà chịu độ phức tạp bậc hai trong độ dài chuỗi n. Do đó, [7] đề xuất tận dụng một bản đồ đặc trưng ngẫu nhiên ϕ:Rd→Rm, được định nghĩa như ϕ(x) =1/√m exp(Wx− ∥x∥²/2) với các entry của W được lấy mẫu từ phân phối Gaussian N(0,1), mà định nghĩa một xấp xỉ rank thấp ϕ(Q)ϕ(K)⊤≈exp(QK⊤). Sau đó một ma trận thưa thớt được xây dựng dựa trên locality sensitivity hashing với các entry khác không Si,j= exp(QK⊤)i,j−ϕ(Q)⊤iϕ(K)j. Tuy nhiên, mục tiêu của [7] là xấp xỉ ma trận attention để giảm chi phí tính toán trong khi chúng tôi nhằm đạt hiệu quả bộ nhớ bằng cách tham số hoá trực tiếp ma trận trọng số. Cụ thể hơn, trong bối cảnh self-attention nơi Q=XWQ, K=XWK, V=XWV, chúng tôi tham số hoá trực tiếp mỗi ma trận chiếu WQ, WK, WV như các nhân tử rank thấp cộng thưa thớt, ví dụ, WQ=BA+S. Ngoài ra, LoSparse [31] đề xuất phân tích các trọng số được huấn luyện trước thành các nhân tử rank thấp cộng thưa thớt để nén có cấu trúc. Tuy nhiên, họ xem xét tối ưu hoá ma trận thưa thớt qua thresholding lặp, điều này yêu cầu lưu trữ ma trận thưa thớt kích thước đầy đủ. Thay vào đó, chúng tôi xem xét tối ưu hoá trực tiếp ma trận thưa thớt trên các entry khác không của nó để huấn luyện trước hiệu quả bộ nhớ.

**Huấn luyện hiệu quả bộ nhớ.** Để vượt qua giới hạn bộ nhớ của LLM, nhiều kỹ thuật đã được đề xuất, như reduced-precision, quantization [9,10], gradient checkpointing [42] và gradient accumulation [8], và row-sum/column-sum của thống kê bậc hai trong Adafactor [45], trong số nhiều kỹ thuật khác. Như đã thảo luận, tham số hoá thưa thớt cộng rank thấp được đề xuất trực giao với những phát triển này nơi các kỹ thuật có thể dễ dàng tích hợp để giảm bộ nhớ thêm nữa.

## 5 Thí nghiệm

Phần này xác nhận hiệu quả của cấu trúc rank thấp cộng thưa thớt cho huấn luyện trước và tinh chỉnh các mô hình ngôn ngữ lớn. Tất cả các thí nghiệm được chạy trên GPU NVIDIA A100. Code có sẵn tại https://github.com/andyjm3/SLTrain.

### 5.1 Huấn luyện trước LLM

Theo [32,59], chúng tôi xem xét huấn luyện trước các mô hình ngôn ngữ LLaMA [51] với pre-normalization, RMSnorm [55], và SwiGLU activation [44]. Chúng tôi huấn luyện LLaMA LLM trên tập dữ liệu C4 (Colossal Clean Crawled Corpus) [41], được thiết kế đặc biệt cho huấn luyện trước. Việc huấn luyện được thực hiện mà không lặp lại dữ liệu và chúng tôi xem xét LLaMA với kích thước mô hình khác nhau từ 60M lên đến 7B tham số.

**Baseline.** Chúng tôi so sánh SLTrain của chúng tôi với các baseline khai thác cấu trúc rank thấp.
• **Full-Rank**: Đây là baseline vanilla huấn luyện trước với trọng số full-rank sử dụng optimizer Adam.
• **Low-Rank [24]**: Đây là tham số hoá rank thấp của trọng số bằng cách phân tích nhân tử W=BA trong đó tối ưu hoá trên B, A.
• **ReLoRA [32]**: ReLoRA định kỳ khởi động lại LoRA [21] bằng cách hợp nhất các adaptor rank thấp đã học với trọng số lớp và khởi tạo lại trạng thái optimizer và learning rate.
• **GaLore [59]**: GaLore khám phá cấu trúc rank thấp cho gradient hơn là cho các tham số.

Chúng tôi thực hiện SLTrain với Adam bằng cách reparameterizing trọng số từ tất cả các lớp tuyến tính, bao gồm các lớp fully-connected cũng như các lớp chiếu query, key, value. Các tham số còn lại được cập nhật với tham số hoá full-rank. Điều này phù hợp với setup được sử dụng trong [21, 32, 59].

**Siêu tham số.** Đối với SLTrain, chúng tôi cố định rank r giống như các baseline và cố định tỷ lệ thưa thớt δ= 0.03 trên tất cả kích thước mô hình trừ LLaMA 7B nơi chúng tôi chọn δ= 0.05, điều này đạt được sự cân bằng tốt giữa hiệu quả và hiệu suất. Chúng tôi điều chỉnh và cố định stepsize là 0.003 và điều chỉnh α trong khoảng [8,16,32] cho các mô hình LLaMA 60M, 130M, 250M, 1B. Chúng tôi cố định các tham số khác theo cài đặt mặc định của chúng. Đặc biệt, chúng tôi chọn α= 32 cho mô hình LLaMA 60M và α= 16 cho các mô hình 130M và 350M và α= 8 cho 1B. Đối với mô hình LLaMA 7B, chúng tôi chọn stepsize là 0.0005 và α= 8. Trừ mô hình 7B, chúng tôi kế thừa trực tiếp kết quả perplexity từ [59] và do đó không cần điều chỉnh siêu tham số từ các baseline. Chúng tôi đảm bảo so sánh công bằng dựa trên số lượng token huấn luyện.

**Ước tính chi phí bộ nhớ.** Chúng tôi so sánh SLTrain được đề xuất với các mô hình baseline rank thấp về mức tiêu thụ bộ nhớ ước tính. Theo [59], chúng tôi tính toán ước tính bộ nhớ với định dạng bfloat16, trong đó mỗi số điểm nổi chiếm 2 byte. Chúng tôi nhận xét rằng SLTrain lưu trữ các chỉ số với định dạng int64, chiếm 8 byte mỗi chữ số. Chi phí bộ nhớ cho một thuật toán huấn luyện bao gồm bộ nhớ tham số và bộ nhớ trạng thái optimizer. Bộ nhớ tham số đề cập đến bộ nhớ được chiếm dụng bởi việc lưu trữ tham số, và bộ nhớ trạng thái optimizer đề cập đến bộ nhớ cần thiết để lưu trữ thống kê moment bậc nhất và bậc hai, ví dụ, trong Adam. Bảng 2 báo cáo tổng chi phí bộ nhớ ước tính cho mỗi phương pháp. Sự phân tích chi tiết của ước tính bộ nhớ có thể được tìm thấy trong Phụ lục F.

**Perplexity vs hiệu quả.** Trong Bảng 2, chúng tôi so sánh hiệu suất của các phương pháp khác nhau trong ba khía cạnh: điểm perplexity, kích thước tham số và chi phí bộ nhớ. Chúng ta quan sát rằng SLTrain hoạt động tương đương với huấn luyện full-rank và GaLore [59] trong khi đạt được giảm thêm về kích thước tham số và chi phí bộ nhớ. Ngoài ra, SLTrain chỉ thêm một overhead tham số và bộ nhớ nhỏ cho tham số hoá rank thấp, nhưng cải thiện đáng kể điểm perplexity. Do đó, việc học nhân tử thưa thớt bổ sung thực sự giúp tăng cường khả năng biểu diễn của SLTrain. Trực giác này cũng được xác nhận trong Hình 10 (Phụ lục D), nơi chúng tôi vẽ phân phối giá trị đơn lẻ cho các ma trận trọng số khác nhau. Do sự hiện diện của nhân tử thưa thớt, chúng ta quan sát rằng phổ của các ma trận trọng số SLTrain được tăng cường vượt ra ngoài r= 128.

**Đo dấu chân bộ nhớ thực tế.** Trong Hình 3, chúng tôi ghi lại dấu chân bộ nhớ thực tế của các phương pháp khác nhau trên nhiều kích thước mô hình khác nhau, trên một GPU A100 80G duy nhất. Chúng tôi đo bộ nhớ của SLTrain 8-bit với cập nhật trọng số theo từng lớp sử dụng batch size đơn và kiểu dữ liệu bfloat16. Gradient checkpointing không được sử dụng cho bất kỳ phương pháp nào. Các baseline bao gồm Adam được huấn luyện trên trọng số full-rank, Adam 8-bit, và GaLore 8-bit với trọng số theo từng lớp. Từ hình, chúng ta thấy rằng SLTrain đạt được giảm bộ nhớ 51%, 58%, 73% cho các mô hình 350M, 1B, 7B, tương ứng. Đáng chú ý, so với phương pháp hiệu quả bộ nhớ hiện đại GaLore [59], SLTrain giảm yêu cầu bộ nhớ 29%, 34%, 17% khi huấn luyện trước các mô hình 350M, 1B, 7B, tương ứng.

**Đo throughput.** Chúng tôi đo throughput của SLTrain cho huấn luyện trước trên các mô hình LLaMA 350M và LLaMA 1B với token batch size 256 trên GPU 1 ×80G A100 và 4 ×80G A100, tương ứng. Throughput được tính trung bình qua 5000 bước huấn luyện. Chúng ta quan sát trong Bảng 3 rằng throughput token của SLTrain thấp hơn một chút so với các baseline full-rank và GaLore. Điều này chủ yếu do việc truy xuất và thiết lập cho các entry thưa thớt. Chúng tôi tin rằng implementation hiệu quả hơn có thể được phát triển về vấn đề này.

**Scaling lên mô hình LLaMA 7B.** Để huấn luyện trước mô hình LlaMA 7B, do hạn chế tài nguyên, chúng tôi chỉ so sánh SLTrain với GaLore, được thực hiện với Adam 8-bit [9] trên 4×80G A100 GPU, mà không có cập nhật trọng số theo từng lớp cũng như gradient checkpointing. Chúng tôi sử dụng trực tiếp các script huấn luyện của GaLore. Trong Bảng 4, chúng ta quan sát rằng SLTrain hoạt động tương đương với GaLore về perplexity và throughput trong khi đạt được giảm bộ nhớ đáng kể 26% trên mỗi thiết bị GPU.

**Bộ nhớ và throughput suy luận.** Trong Bảng 5, chúng tôi so sánh việc sử dụng bộ nhớ suy luận và throughput giữa SLTrain và mô hình full-rank trên nhiều kích thước mô hình khác nhau, từ LLaMA 130M đến 7B. Một sự đánh đổi rõ ràng giữa bộ nhớ và chi phí tính toán được quan sát. Cụ thể, khi kích thước mô hình tăng, tỷ lệ phần trăm tiết kiệm bộ nhớ trở nên rõ rệt hơn, trong khi sự gia tăng tương ứng trong chi phí tính toán ít đáng kể hơn. Điều này nhấn mạnh lợi thế ngày càng tăng của SLTrain khi sử dụng các mô hình lớn hơn.

**Thay đổi support thưa thớt ngẫu nhiên.** Trái ngược với các chiến lược pruning phổ biến yêu cầu học cẩn thận support thưa thớt, chúng tôi cho thấy rằng SLTrain hoạt động với support được chọn ngẫu nhiên. Về vấn đề này, chúng tôi thực hiện huấn luyện trước trên Llama 60M và 130M với năm support được chọn ngẫu nhiên khác nhau cho nhân tử thưa thớt S. Các biểu đồ hội tụ được hiển thị trong Hình 4, nơi chúng ta thấy rằng việc thay đổi support ngẫu nhiên cho nhân tử thưa thớt không ảnh hưởng đáng kể đến hiệu suất.

**Rank r và sparsity δ ảnh hưởng đến hiệu suất như thế nào?** Trong Bảng 6, chúng tôi xác nhận hiệu suất của việc huấn luyện các mô hình Llama 60M và 130M bằng cách thay đổi các siêu tham số r và δ. Chúng ta nhận thấy rằng, nói chung, khi thêm nhiều tham số hơn (tương ứng với r và δ cao hơn), hiệu suất tốt hơn. Tuy nhiên, việc cải thiện hiệu suất này cũng đi kèm với sự gia tăng dấu chân bộ nhớ. Trong các thí nghiệm ban đầu của chúng tôi, chúng tôi cũng thử cài đặt cực đoan với r= 0, tức là, chỉ thành phần S được học nhưng với giá trị δ cao hơn. Cài đặt này thể hiện hiệu suất khá tốt. Các điều tra sâu hơn theo hướng này được để lại cho nghiên cứu tương lai.

**So sánh thêm với hiệu suất full-rank.** Trong phần này, chúng tôi đánh giá tiềm năng của SLTrain để đạt được hiệu suất tương đương với các mô hình full-rank bằng cách điều chỉnh tỷ lệ thưa thớt, δ. Như được hiển thị trong Bảng 7, chúng tôi tăng δ từ 0.03 lên 0.1 cho các mô hình LLaMA 350M và 1B. Kết quả của chúng tôi chỉ ra rằng việc đặt δ= 0.1 cho phép SLTrain đạt được điểm perplexity tương tự như các mô hình full-rank, trong khi duy trì hiệu quả bộ nhớ và tham số. Đáng chú ý, tại δ= 0.1, SLTrain giảm kích thước tham số 42% cho mô hình 350M và 45% cho mô hình 1B. Những kết quả này nêu bật hiệu quả của SLTrain trong việc giảm đáng kể kích thước mô hình mà không hy sinh hiệu suất.

## 6 Nhận xét kết luận

Trong bài báo này, chúng tôi đề xuất SLTrain để đạt được huấn luyện trước LLM hiệu quả cả về bộ nhớ và tham số. SLTrain kết hợp hai cấu trúc tiết kiệm bổ sung, rank thấp và thưa thớt, để học các mô hình với khả năng biểu diễn cao. Trong khi rank thấp được mô hình hoá qua tích BA của các ma trận r-chiều, support của nhân tử thưa thớt S được thu được bằng lấy mẫu ngẫu nhiên đồng nhất trên các chỉ số. Các ma trận B, A, S được học cho các lớp khác nhau qua backpropagation. Thực nghiệm, chúng tôi đạt được giảm bộ nhớ hiện đại trong quá trình huấn luyện trước trong khi duy trì hiệu suất cạnh tranh. Mặc dù chúng tôi cho thấy kết quả trên các mô hình ngôn ngữ LLaMA (với kích thước khác nhau từ 60M đến 7B tham số), chúng tôi tin rằng SLTrain cũng có thể giúp cải thiện hiệu quả bộ nhớ cho các mô hình nền tảng thị giác và mô hình nền tảng đa phương thức, như mô hình diffusion [37] và CLIP [38]. Những cải tiến đáng kể được thể hiện bởi SLTrain cũng thúc đẩy các nghiên cứu tương lai để hiểu các đảm bảo lý thuyết của việc huấn luyện với cả nhân tử rank thấp và thưa thớt, như hội tụ và landscape loss. Chúng tôi hy vọng nghiên cứu này khởi xướng khám phá sự kết hợp của các cấu trúc tiết kiệm khác cho huấn luyện trước như tích Kronecker hoặc thưa thớt có cấu trúc (ví dụ, block-diagonal, group-lasso).
