# 2309.16119.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.16119.pdf
# File size: 277338 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.16119v2  [cs.LG]  10 Mar 2024ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by
Integrating with Modular Quantizers
Junjie Yin jyin27@jhu.edu
Department of Computer Science
Johns Hopkins University
Jiahao Dong jd787@cornell.edu
Department of Computer Science
Cornell University and Cornell Tech
Yingheng Wang yw2349@cornell.edu
Department of Computer Science
Cornell University
Christopher De Sa cdesa@cs.cornell.edu
Department of Computer Science
Cornell University
Volodymyr Kuleshov kuleshov@cornell.edu
Department of Computer Science
Cornell University and Cornell Tech
Abstract
We propose a memory-eﬃcient ﬁnetuning algorithm for large l anguage models (LLMs) that
supports ﬁnetuning LLMs with 65B parameters in 2/3/4-bit pr ecision on as little as one
24GB GPU. Our method, modular low-rank adaptation ( ModuLoRA ), integrates any
user-speciﬁed weight quantizer with ﬁnetuning via low-ran k adapters (LoRAs). Our ap-
proach relies on a simple quantization-agnostic backward p ass that adaptively materializes
low-precision LLM weights from a custom black-box quantiza tion module. This approach
enables ﬁnetuning 2-bit and 3-bit LLMs for the ﬁrst time—lev eraging state-of-the-art 2-bit
QuIP# quantization and 3-bit OPTQ quantization—outperfor ming ﬁnetuning that relies
on less sophisticated 4-bit and 8-bit methods. In our experi ments, ModuLoRA attains
competitive performance on text classiﬁcation, natural la nguage inference, and instruction
following tasks using signiﬁcantly less memory than existi ng approaches, and we also surpass
the state-of-the-art ROUGE score on a popular summarizatio n task. We release Modu-
LoRA together with a series of low-precision models as part of LLMTools , a user-friendly
library for quantizing, running, and ﬁnetuning LLMs on cons umer GPUs.
1 Introduction
Large language models (LLMs) excel across diverse tasks suc h as code generation, instruction following,
and reasoning (Brown et al., 2020; Scao et al., 2023; Zhang et al., 2022). However, the massive size of
these models—often reaching into hundreds of billions of pa rameters—makes them challenging to deploy on
downstream tasks and motivates research into eﬃcient ﬁnetu ning algorithms (Li & Liang, 2021; Hu et al.,
2022).
Here, we propose modular low-rank adaptation ( ModuLoRA ), a memory-eﬃcient ﬁnetuning algorithm for
large language models (LLMs) that runs on consumer-grade ha rdware. For example, in 3-bit precision,
ModuLoRA ﬁnetunes a LLaMA-30B model (Touvron et al., 2023) on one Nvid ia RTX 3090 24GB GPU
1

--- PAGE 2 ---
and a LLaMA-65B on one RTX A6000 48GB GPU. In 2-bit precision, ModuLoRA ﬁnetunes a LLaMA-30B
or LLaMA-65B on one Nvidia RTX 3090 24GB GPU.
Our approach adds high-precision low-rank adapters to the l ow-precision 3-bit or 4-bit weights of a frozen
base LLM obtained via modern quantization algorithms (Huba ra et al., 2021; Yao et al., 2021; Frantar et al.,
2023). Crucially, ModuLoRA does not specify its own quantization procedure—rather, it integrates with
user-deﬁned quantizers via a simple quantization-agnosti c backward pass. This backward pass adaptively
materializes low-precision LLM weights obtained from a bla ck-box quantizer and integrates them with high-
precision low-rank adapters.
We release ModuLoRA as part of LLMTools , a user-friendly library that enables ﬁnetuning LLMs on
consumer GPUs. When paired with the modern OPTQ quantizer (F rantar et al., 2023), ModuLoRA enables
ﬁnetuning 3-bit LLMs for the ﬁrst time, often outperforming methods based on less sophisticated 4-bit and
8-bit quantization. When paired with the state-of-the-art QuIP# quantizer Chee et al. (2023); Tseng et al.
(2023), ModuLoRA enables ﬁnetuning 2-bit LLMs for the ﬁrst time, matching met hods’ performance on less
sophisticated 4-bit and 8-bit quantization method. Across tasks in classiﬁcation, natural language inference,
and instruction following, our low-precision models achie ve competitive performance using signiﬁcantly less
memory than existing approaches. On a popular summarizatio n benchmark, we attain a new state-of-the-
art ROUGE score using a quantized LLaMA-65B model. We open-s ource all our low-precision models,
including the ﬁrst 3-bit family of Alpaca models that featur e strong instruction-following performance at
multiple model sizes. Our ﬁndings reveal that high performa nce can be achieved using smaller quantized
LLMs than previously thought.
Contributions. In summary, this paper makes the following contributions: ( 1) we propose ModuLoRA ,
a memory-eﬃcient ﬁnetuning method that operates over low-p recision weights obtained via a user-speciﬁed
black-box quantization module; (2) we release LLMTools , a user-friendly Python library that features an
implementation of ModuLoRA and that enables users to easily ﬁnetune the largest LLMs on c onsumer
GPUs; (3) we provide empirical evidence that high performan ce on downstream tasks can be achieved with
a smaller LLM than previously thought.
2 Background and Related Work
We are interested in ﬁnetuning a pre-trained LLM for downstr eam tasks (Li & Liang, 2021; Lester et al.,
2021; Houlsby et al., 2019; Rebuﬃ et al., 2017). LLMs use a tra nsformer architecture where almost all of
the learnable weights—and almost all of the memory used to st ore these weights—appear in linear layers.1
We let the weights and biases of these nlinear layers be denoted W(i)andb(i)fori∈ {1,2, ..., n }. Given a
pretrained network, our goal is to ﬁnetune it for downstream tasks using much less working memory than
would be needed to store all of the Win full precision.
2.1 Large Language Model Finetuning
Because of the high memory requirements needed to ﬁne-tune a nd store all the weights of a LLM, practitioners
have developed a variety of parameter-eﬃcient ﬁne tuning methods that learn in a lower dimensional space.
These methods include tuning only the output layer (Devlin e t al., 2018) and tuning the prompt or preﬁx
passed as input to an LLM (Lester et al., 2021; Li & Liang, 2021 ; Liu et al., 2023a;b), as well as LoRA,
which is the focus of this work.
Low-Rank Adaptation (LoRA) The LoRA algorithm (Hu et al., 2022) decomposes the weights Winto
a sum of frozen base model weights W0∈Rd×dand a small additive low-rank adapter AB⊤consisting of
the product of two rectangular matrices A,B∈Rd×r, where r >0 indicates the rank2:
W=W0+AB⊤. (1)
1These layers include the K,V,Q, and Oprojection matrices of attention blocks and the linear laye rs of MLP blocks.
2For simplicity here we consider square weight matrices W; the rectangular case is a straightforward generalization .
2

--- PAGE 3 ---
LoRA reduces the number of trained parameters by a factor of 2 r/d, lowering the storage, transmission, and
task-switching overhead of inference on a system that alrea dy maintains the base model. However, LoRA
must hold the base weights W0in memory, which requires multiple high-end GPUs and preclu des tuning
large LLMs on commodity hardware.
2.2 Low-Precision Machine Learning
The computational requirements of modern machine learning models motivate a wide range of eﬃcient
machine learning algorithms (Li & Liang, 2021; Hu et al., 202 2; Frantar et al., 2023).
Quantization Quantization methods for neural networks reduce the number of bits required to store model
weights (Dong et al., 2019; 2020; Yao et al., 2022; Park et al. , 2023). A b-bit quantization method has the
form
(ˆWq,z,s) =Q(W) ˆW=D(ˆWq,z,s). (2)
Here, the quantization algorithm Qtakes a weight matrix W∈Rd×d(or its subset) and outputs a quantized
version ˆWq∈ {0,1, . . . , 2b−1}d×d(using bbits to represent each entry of W), as well as zero and scale
parameters z,s∈Rd(in full precision). The dequantization algorithm D(ˆWq,z,s) recovers an approximation
ˆW∈Rd×dby rescaling the quantized weights as ˆW=s⊙ˆWq+z, where ⊙denotes the Hadamard product,
and⊙,+ are extended with numpy-style broadcasting.
Recently, Frantar et al. (2023) proposed OPTQ, a quantizati on algorithm that scales to modern LLMs. The
method iteratively runs two steps over the weight columns: ( 1) quantize with nearest rounding and compute
the error, (2) update the remaining weights with a scaled err or. Many of our experiments ﬁnetune LLMs
quantized with OPTQ.
Following OPTQ, Chee et al. (2023) proposed QuIP, a quantiza tion algorithm that makes two-bit LLM
compression viable for the ﬁrst time. The method follows a 2- step procedure: (1) an adaptive rounding
procedure that minimizes a quadratic proxy objective„ (2) a n eﬃcient pre- and post-processing procedure
ensuring weight and Hessian incoherence through multiplic ation by random orthogonal matrices. Further,
Tseng et al. (2023) proposed QuIP#, combining lattice codeb ooks with incoherence processing from QuIP
to create state-of-the-art 2 bit quantized models. We show t he performance of QuIP# (with D4codebooks)
quantized LLMs on the SAMSum summarization experiment.
In concurrent work, Dettmers et al. (2023) proposed QLoRA, a n approach for tuning quantized LLMs based
on LoRA. While our work seeks to integrate with any user-deﬁn ed quantization module (such as OPTQ),
QLoRA deﬁnes its own quantization scheme, which is simpler t han, say, OPTQ or QuIP. One advantage of
our approach is support for 2-bit and 3-bit ﬁnetuning; QLoRA only supports 4-bit ﬁnetuning. We will also
identify settings where using advanced quantizers yields p erformance gains over QLoRA. See Section 5.1 for
details.
3 Low-Precision Low-Rank Adaptation with a Modular Quantiz er
In this section, we describe modular low-rank adaptation ( ModuLoRA ), a memory-eﬃcient ﬁnetuning
algorithm for large language models (LLMs) that leverages c ustom quantization algorithms and runs on
consumer GPU hardware.
3

--- PAGE 4 ---
class ModuLoRALinear (Module):
"""Linear ModuLoRA Layer"""
def__init__ (self,...):
self.hatWq_z_s =quantize(pretrained_W)
(self.A,self.B)=lora_init( ...)
defforward (self, x):
(hatWq, z, s) =self.hatWq_z_s
return LPLinear .apply(x, hatWq, z, s) \
+(x@self.B)@self.A.t()+self.biasclass LPLinear (Function):
"""Low-Precision Linear Map"""
@staticmethod
defforward (ctx, input , hatWq, z, s):
ctx.save_for_backward(hatWq, z, s)
hatW =dequantize(hatWq, z, s)
output =input @hatW .t()
return output # hatW is deallocated
@staticmethod
defbackward (ctx, grad_output):
hatWq, z, s =ctx.saved_tensors
# we recompute hatW
hatW =dequantize(hatWq, z, s)
grad_input =grad_output @hatW
# here hatW can be deallocated
return grad_input, None ,None ,None
Figure 1: PyTorch pseudocode for ModuLoRA .
3.1 Low-Rank Adaptation of Low-Precision Models
The ﬁrst step of our approach is quantization : we apply a black-box quantization algorithm Qto a set of pre-
trained weight matrices W(i). This yields quantized weights, zeros, and scales ( ˆW(i)
q,z(i),s(i)) =Q(W(i)).
We use ˆW(i)
qto denote the quantized weighs stored in low precision, whil eˆW(i)denotes the same weights
materialized in high precision (both approximate the origi nal weights W(i)). Crucially, we do not specify a
quantization procedure Qas part of ModuLoRA —rather, we seek to support user-deﬁned quantizers that
are treated by our method is a black-box.
The core of our eﬀorts focuses on ﬁnetuning the base quantized model. Our method ﬁrst modiﬁes the
network by replacing each linear layer—originally deﬁned b y the aﬃne map x/mapsto→x(W(i))⊤+b(i)—with the
reparameterized low precision ModuLoRALinear layer in Figure 1, given by
x/mapsto→x(ˆW(i))⊤+xB(i)(A(i))⊤+b(i). (3)
Here A(i),B(i)∈Rd×rare learnable parameters initialized as in Hu et al. (2022), and ˆW(i)=
D(ˆW(i)
q,z(i),s(i)) is the ﬁxed dequantized weight matrix. Note that this is alg ebraically (but not com-
putationally) equivalent to transforming the quantized ma trix as given in (1). Lastly, ModuLoRA ﬁts the
A(i)andB(i)using backprop and gradient-based learning.
A key challenge in this procedure is to eﬃciently perform com putations with high-precision and low-precision
tensors. Clearly, the forward pass requires multiplying by weights stored in quantized ˆW(i)
q’s. Below, we
derive the backward pass for A(i),B(i)and show that it also requires multiplying by the transpose o f the
ˆW(i)
q’s.
3.1.1 The Structure of a Quantized Backward Pass
We illustrate the technical challenges that arise in the des ign of a quantized backward pass in the context
of a network of nModuLoRALinear layers. Each ModuLoRALinear is eﬀectively a fully connected layer with
reparameterized dense weights deﬁned as
W(i)
l=ˆW(i)+A(i)(B(i))⊤, (4)
biases b(i), and outputs yifori= 1,2, ..., n . We use ¯yi=W(i)
lx+b(i)to denote the pre-activation output
of the i-th step and we use Lto denote the loss. The backward pass seeks to compute gradie ntsdL/dA(i)
4

--- PAGE 5 ---
anddL/dB(i), where we overload the Leibniz notation for derivatives to a lso denote gradients. By the chain
rule,
dL
dA(i)=dL
d¯yi·d¯yi
dA(i). (5)
Because of the additive structure of the weights W(i)
lin (4), dyi/dA(i)is straightforward to handle as it
is not a function of the quantized weights ˆW(i)
q. The second term can be computed via the chain rule of
calculus asdL
d¯yi=dL
d¯yi+1·d¯yi+1
dyi·dyi
d¯yi, (6)
where dyi/d¯yiis the derivative of the activation function, and d¯yi+1/dyi= (W(i)
l)⊤= ( ˆW(i))⊤+
B(i)(A(i))⊤.
The above derivations indicate that computing the gradient dL/dA(i)(the argument for dL/dB(i)is identical)
requires performing a matrix-vector multiplydL
dyi+1·(ˆW(i))⊤between a high-precision vectordL
dyi+1with a
quantized matrix (ˆW(i))⊤. Performing this multiplication in a stable and eﬃcient way is a challenge that
we must address.
3.1.2 Eﬃcient Mixed-Precision Computation of Forward and B ackward Passes
If we could precompute all dequantized weight matrices (ˆW(i))⊤in a high-precision format, our challenge
would be solved: the matrix-vetor multiplicationdL
dyi+1·(ˆW(i))⊤in the backward pass would operate over
two high-precision arrays, and would not introduce questio ns of eﬃciency and stability.
Unfortunately, precomputing all dequantized weight matri ces(ˆW(i))⊤requires the same amount of GPU
memory as it would take to store the original high-precision LLM. For this computation to ﬁt on consumer
GPU hardware, we need to avoid manifesting all the ˆW(i)in memory at once. Using (3) naively, backprop
would store all the ˆW(i)from the forward pass to use them in the backward pass.
Eﬃcient Mixed Precision Computation. Our strategy is to recompute the high-precision materializa-
tion ˆW(i)of the quantized ˆW(i)
qin the backward pass rather than save it (Figure 1). In the LPLinear function,
theforward method dequantizes ˆW(i)and performs multiplication. Similarly, backward re-dequantizes ˆW(i)
and computes the gradient via dynamic programming. The hatW goes out of scope and can be freed at the
end of each method, so only one ˆW(i)is ever stored in memory at any given time.
The amount of memory used in the forward pass of the LPLoRA module is small: all the intermediates are
either the same size as the input x, or even smaller (e.g. if x∈Rm×dthenx@self.Bis of size Rm×r
forr≪d). The amount of additional computation involved is also sma ll: the dequantization procedure
ˆW=s⊙ˆWq+zonly requires multiplying and adding a scalar to each row of ˆWq.
Increasing Eﬃciency Further. Figure 1 depicts a weight materialization strategy in which ˆW(i)is fully
materialized at each layer in both forward and backward pass es. To further reduce memory, we can materi-
alize elements of ˆW(i)only as needed. For many quantization algorithms (Nagel et a l., 2020; Frantar et al.,
2023), we can perform row materialization : dequantize ˆW(i)one row at a time and immediately multiply it
with an input x.ModuLoRA also naturally generalizes to any direct vector-by-quanti zed-matrix product
subroutine provided by the quantizer Q, in which case materializing any part of ˆW(i)may be unnecessary.
3.2 LLMTools: A Library for Eﬃcient LLM Finetuning Using Mod uLoRA.
We implement ModuLoRA as part of LLMTools , a user friendly library that enables users to in-
teract with the largest LLMs on consumer hardware. The LLMTools library enables ﬁnetuning
LLMs in 2-bit, 3-bit, and 4-bit precision using the ModuLoRA algorithm. It also provides an easy-
to-use Python API for quantization, inference, and ﬁnetuni ng, as well as modular support for multi-
ple quantizers, LLMs (including LLaMA1, LLaMA2, BLOOM, and OPT), and optimization algorithms
(including all that are compatible with the Hugging Face Tra iner class). Lastly, LLMTools sup-
ports easily loading datasets and sharing models via the Hug gingFace Hub. Our code is available at:
5

--- PAGE 6 ---
https://github.com/kuleshov-group/llmtools ; our evaluation code to reproduce our results is available
at:https://github.com/kuleshov-group/MODULoRA-Experime nt.
A key quantization algorithm implemented in LLMTools is OPTQ (Frantar et al., 2023). In order to
integrate OPTQ with LoRA-based ﬁnetuning, LLMTools provides eﬃcient CUDA implementations of
mixed-precision matrix-vector multiplication, includin g row and weight materialization. We provide CUDA
kernels for both row and weight materialization in both the f orward and backward passes. For maximum
eﬃciency, we materialize elements of ˆW(i)
qin ﬂoat16. The base quantized LLM models are represented via
weights ˆW(i)
qstored in 3or4bits, with scales and zeros s(i),z(i)as well as biases b(i)all stored as ﬂoat16.
Similarly, to integrate QuIP# with LoRA, LLMTools provides CUDA kernels for weight re-materialization
and orthogonal matrices multiplication in the forward and b ackward passses. The base quantized LLM
models are represented via weights ˆW(i)
qstored in 2bits.
4 Experiments
4.1 Setup
Models. We evaluate ModuLoRA andLLMTools on the recent LLaMA (Touvron et al., 2023) family
of models, as well as open-source BLOOM (Scao et al., 2023) an d OPT models (Zhang et al., 2022). We
quantize the models to 3 bits and 4 bits using OPTQ as in Franta r et al. (2023) with calibration 128 samples
from C4 (Raﬀel et al., 2020). We quantize the models to 2 bits u sing QuIP# as in Chee et al. (2023);
Tseng et al. (2023) with E8lattice codebooks.
Baseline. We use LoRA (as implemented in the PEFT library (Mangrulkar e t al., 2022)) to ﬁnetune
models quantized in 8 bits using the BitsAndBytes library (D ettmers et al., 2022); we also compare to
full-precision results from the literature. In concurrent work, Dettmers et al. (2023) proposed QLoRA, a
related 4-bit ﬁnetuning algorithm implemented in the BitsA ndBytes library. Accordingly, we present an
experimental comparison of QLoRA with our approach, along w ith an in-depth discussion.
Training. We ﬁnetune all models on NVIDIA TITAN, 3090, and A6000 GPUs (d epending on the model)
with a LoRA rank of r= 8and alpha of a= 32 , and report results from 3 random seeds. We set up the
training procedure following Hu et al. (2022), with slight v ariation to accommodate our particular language
models. For a fair comparison with the concurrent work by Det tmers et al. (2023), we use the exact same
hyperparameter set up. Please see Appendix C for details on t he hyperparameters used for each of our
experiment.
4.2 Text Classiﬁcation
Data & Metrics. We start with a simple text classiﬁcation task where we seek t o classify a short text
snippet (up to 50 words) into its genre (e.g., ﬁction, teleph one chat, etc.). We ﬁnetune 13B to 65B LLAMA
models on 392,702 snippets from ﬁve genres and evaluate on 9, 815 held out instances (Williams et al., 2018),
reporting accuracy. This yields a challenging classiﬁcati on task for LLMs of all sizes.
LLAMA Tuning 13B 30B 65B
LLMTools (3-bit) 93.5 ±0.7 97.0 ±0.9 97.2 ±0.8
LLMTools (4-bit) 92.9 ±0.7 96.3 ±1.0 98.0 ±0.9
Bits&Bytes 8-bit (LLM.int8()) 93.0 ±0.7 93.7 ±1.0 98.6 ±1.0
Table 1: Text classiﬁcation accuracy (%) for LLAMAs ﬁnetune d with LoRA & ModuLoRA in 3, 4, 8 bits.
Results. We observe that classiﬁcation accuracy consistently impro ves as we increase the number of
parameters of the LLM. ModuLoRA combined with a 3-bit or a 4-bit LLM oﬀers comparable perform ance
to 8-bit ﬁnetuning in Bits&Bytes while using signiﬁcantly l ess memory (Table 1).
6

--- PAGE 7 ---
4.3 Natural Language Inference
Data & Metrics. Next, we ﬁnetune LLMs on natural language inference tasks. T he model is asked to
predict a label from a small set (entailment, contradiction , or neutral) after being presented with a sentence
pairing (a hypothesis and premise sentence pair). We ﬁnetun e 7B to 65B LLaMA models on the Multi-Genre
Natural Language Inference Corpus (MNLI) (Williams et al., 2018) and evaluate on the matched test sets
(in-domain examples), reporting accuracy. Baselines from GPT-3 and T5 are included, as presented in Hu
et al. (2022) and Chung et al. (2022).
Results. Our 2-bit and 3-bit 65B LLaMA model matches the performance o f a full-precision GPT-3+LoRA
baseline. Notably, 2-bit 65B models ﬁnetuned with QuIP# out performs the rest of 65B models with higher
precisions. We also ﬁnd that 3-bit and 4-bit models from LLMTools outperform 8-bit model s from
the Bits&Bytes library for the entire model size range . 2-bit, 3-bit and 4-bit ModuLoRA models
either match or outperform their 4-bit QLoRA counterparts, often using less memory because of lower
precision models.
Baselines
Models Finetuning Adaptation Model Size # Trainable Parame ters MNLI-m ( accuracy )
GPT-3 Full Finetuning 175B 175,255.8M 89.5 ±0.1
GPT-3 Adapter 175B 40.1M 91.5 ±0.1
GPT-3 LoRA 175B 4.7M 91.7 ±0.1
T5 Full Finetuning 11B 11,307.4M 92.2 ±0.1
LLaMA Finetuning Quantizer 7B 13B 30B 65B
LLMTools (2-bit) QuIP#( E8) 88.50 ±0.3 89.72 ±0.3 91.30 ±0.3 91.85 ±0.3
LLMTools (3-bit) OPTQ 88.98 ±0.2 90.20 ±0.2 91.09 ±0.2 91.42 ±0.1
LLMTools (4-bit) OPTQ 89.31 ±0.2 90.41 ±0.2 91.31 ±0.1 91.59 ±0.2
Bits&Bytes (4-bit) QLoRA 89.28 ±0.2 89.67 ±0.2 91.22 ±0.1 91.36 ±0.2
Bits&Bytes (8-bit) LLM.int8() 88.95 ±0.1 90.08 ±0.1 91.15 ±0.1 91.55 ±0.1
Table 2: Natural language inference on the MNLI-m dataset ev aluated using classiﬁcation accuracy (%).
Our LLaMA-65B-3bit model approaches state-of-the-art sco res using signiﬁcantly less memory.
4.4 Abstractive Summarization
Data & Metrics. We ﬁnetune 7B-65B LLaMA and 7B-13B OPT models on the SAMSum
dataset (Gliwa et al., 2019), consisting of 14,732 (text, su mmary) training pairs and 819 test pairs. Our
methodology fully mirrors the evaluation of GPT-style mode ls ﬁnetuned using LoRA (Hu et al., 2022). We
evaluate summarization quality using ROUGE-1/2/L; we incl ude GPT-3 baselines from Hu et al. (2022).
Results. Our 4-bit 65B LLaMA models ﬁnetuned with ModuLoRA outperform the GPT-3 baseline
and even reach new state-of-the-art performance on this dataset (Table 3). Importantly, Modu-
LoRA demonstrates performance improvements over the 4-bit QLoR A and the 8-bit BitsAndBytes meth-
ods. In the 7B to 65B model size range, ModuLoRA models (3-bit or 4-bit) outperform 8-bit LoRAs in
BitsAndBytes and LLM.int8() and 4-bit LoRAs in BitsAndByte s and QLoRA. ModuLoRA models (2-bit)
match the performance of 8-bit LoRAs in BitsAndBytes and LLM .int8() and 4-bit LoRAs in BitsAndBytes
and QLoRA. We argue that a data-driven lower precision quant ization scheme can improve over a higher
precision zero-shot quantizer like LLM.int8(). Switching from 4-bit to 3-bit, and then from 3-bit to 2-bit,
precision within ModuLoRA reduces ROUGE by only about 1%.
Round-to-Nearest Quantization We also perform an ablation where we replace the OPTQ quantiz er
with a rount-to-nearest (RTN) approach (Table 4); OPTQ perf orms better than RTN, highlighting the
importance of advanced quantizers.
7

--- PAGE 8 ---
Baselines
Models Finetuning Adaptation # Trainable Parameters SAMSu m (Rouge 1/2/L )
GPT-3 Full Finetuning 175,255.8M 52.0 / 28.0 / 44.5
GPT-3 Adapter 40.1M 53.2 / 29.0 / 45.1
GPT-3 LoRA 4.7M 53.8 / 29.8 / 45.9
Pegasus SliC 2B 54.4 / 29.9 / 45.9
LLAMA Finetuning Quantizer 7B 13B 30B 65B
LLMTools (2-bit) QuIP# ( E8) 51.3 / 27.3 / 43.7 52.3 / 29.0 / 45.0 53.3 / 30.2 / 46.0 54.0/ 30. 6 / 46.2
LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 52.4 / 29.6 / 45.1 53.6 / 30.8 / 46 .3 54.1 / 30.9 / 46.5
LLMTools (4-bit) OPTQ 51.7 / 28.3 / 44.4 53.2 / 30.2 / 46.1 53.9 / 31.2 / 46 .954.8 / 31.3 / 47.2
Bits&Bytes (4-bit) QLoRA 51.6 / 28.3 / 44.5 51.3 / 28.1 / 44.1 5 3.0 / 30.2 / 45.7 53.8 / 30.5 / 45.9
Bits&Bytes (8-bit) LLM.int8() 51.9 / 28.1 / 44.5 51.3 / 28.2 / 43.6 50.8 / 28.4 / 44.1 53.9 / 30.4 / 46.3
Table 3: Abstractive summarization on the SAMSum dataset ev aluated using ROUGE 1/2/L. Our LLAMA-
65B-4bit model obtains state-of-the-art ROUGE scores. All metrics have ±0.5conﬁdence intervals.
SAMSum Performance Quantizer 7B 13B
LLMTools (3-bit)OPTQ 51.2 / 28.2 / 44.0 / 44.2 52.4 / 29.6 / 45.1 / 45.1
RTN 50.7 / 27.2 / 43.6 / 43.6 51.1 / 28.7 / 44.3 / 44.5
LLMTools (4-bit)OPTQ 51.7 / 28.3 / 44.4 / 44.4 53.2 / 30.2 / 46.1 / 46.1
RTN 51.2 / 28.5 / 44.2 / 44.2 52.5 / 29.9 / 45.5 / 45.5
Table 4: OPTQ and RTN quantization with diﬀerent LLaMA model sizes on the SAMSum dataset. The
evaluation was done on ROUGE 1/2/L/L-Sum.
Other Model Families We also apply LLMTools to the OPT (Zhang et al., 2022) families of models
(Table 5). Although these models perform worse than LLaMA, ModuLoRA matches or outperforms more
memory-intensive 4-bit and 8-bit ﬁnetuning, which is consi stent with our results on LLaMA.
OPT Finetuning Quantizer 13B 30B
LLMTools (3-bit) OPTQ 48.8 / 26.7 / 41.9 49.9 / 27.1 / 42.5
LLMTools (4-bit) OPTQ 49.3 / 26.8 / 42.0 49.6 / 27.1 / 42.4
Bits&Bytes (4-bit) QLoRA 49.2 / 27.0 / 42.1 49.9 / 27.0 / 42.5
Bits&Bytes (8-bit) LLM.int8() 48.8 / 26.5 / 41.7 49.3 / 27.1 / 42.3
Table 5: Abstractive summarization with OPT models on the SA MSum dataset. ModuLoRA in 3-bit and
4-bit precision matches ROUGE 1/2/L scores of 4-bit and 8-bi t baselines. All metrics have ±0.5conﬁdence
intervals.
4.5 Instruction Following
Data & Metrics. We ﬁnetune 7B-65B LLaMA models on the Alpaca dataset (Taori e t al., 2023), con-
sisting 52,000 instructions, as well on the CodaAlpaca data set (Chaudhary, 2023), consisting of 20K code
generation instructions (ses Table 9). We evaluate our Alpa ca instruction-tuned models on the BigBench-
Hard (BBH) benchmark (Suzgun et al., 2022), consisting of 23 challenging tasks on which LLMs do not
exceed human performance. We evaluate 3-shot performance v ia "answer-only" prompting and use exact
match accuracy as our measurement standard, testing on 6,51 1 samples ( ∼1.5k tokens each). We include
Flan and LLaMA baselines from Chia et al. (2023).
Results. We ﬁnd that 2-bit, 3-bit, and 4-bit performance drops only sl ightly relative to 8-bit models.
Crucially, 2-bit models, despite their aggressive compres sion, match the performance of 4-bit
QLoRA in all model sizes. 4-bit and 3-bit 65B models outperform 8-bit 30B models, desp ite using
fewer total bits. Furthermore, 4-bit ModuLoRA compares well to 4-bit QLoRA, and provides consistent
8

--- PAGE 9 ---
performance improvements, especially at smaller model siz es, where sophisticated quantization ought to
provide greater beneﬁts. This further highlights the beneﬁ ts of one-shot quantization methods. Appendix B
also reports experiments on the CodeAlpaca dataset.
Baselines
Model Method Quantizer BASE (250M) L (780M) XL (3B) XXL (11B)
FLAN-T5 No Finetuning None 30.8 30.3 39.9 47.4
Model Methods Quantizer 7B 13B 30B 65B
LLaMALLMTools (2-bit) QuIP# ( E8) 30.8 ±0.5 33.8 ±0.5 38.3 ±0.6 43.5 ±0.5
LLMTools (3-bit) OPTQ 31.1 ±0.4 35.3 ±0.2 37.2 ±0.6 43.3 ±0.4
LLMTools (4-bit) OPTQ 33.1 ±0.2 36.2 ±0.4 40.4 ±0.2 43.7 ±0.4
Bits&Bytes (4-bit) QLoRA 31.9 ±0.1 35.4 ±0.2 39.0 ±0.4 43.5 ±0.5
Bits&Bytes (8-bit) LLM.int8() 33.3 ±0.3 36.8 ±0.2 39.1 ±0.5 44.7 ±0.4
No Finetuning None 30.9 37.1 39.3 42.6
Table 6: Instruction-tuned models evaluated on BigBench Ha rd (BBH). We ﬁnetune LLaMA models on the
Alpaca dataset in 2 to 8 bits. We provide exact standard devia tion here.
4.6 Memory Requirements
We show the memory required to perform ﬁnetuning on MNLI-M fo r diﬀerent LLaMA model sizes in table
7.ModuLoRA signiﬁcantly minimizes the memory requirements for ﬁnetun ing on these models. We
plot the memory requirements in ﬁgure 2 for better visualiza tion. As the model size increases to 65B,
ModuLoRA uses only about 6% of the memory to run memory-eﬃcient ﬁnetun ing method LoRA. As the
table and ﬁgure illustrates, with ModuLoRA it’s possible to not only run inference but also ﬁnetune 65B
model on a single 24GB GPU. To produce this table, we run our qu antizer-agnostic forward/backward passes
for the entire LLaMA model size range with batch size 1 and max imum sequence length 128 on MNLI-m.
LLaMA Finetuning 7B 13B 30B 65B
LLMTools (2-bit) 3.2 GB 5.4 GB 11.4 GB 21.8 GB
QLoRA (4-bit) 5.2 GB 8.6 GB 19.5 GB 36.7 GB
Full Precision (LoRA) 38.4 GB 73.9 GB 183.3 GB 360.4 GB
Table 7: Memory requirements to ﬁnetune LLaMA models on MNLI -M with batch size 1 and maximum
sequence length 128. For comparison, we include the memory r equirements to ﬁnetune on LoRA and QLoRA.
5 Discussion
5.1 Comparison to Related Work
7B 13B 33B 65B0GB10GB100GB400GB
351122
5920373874183360
Model Parameter Size (Billion)Required Memory (Gigabyte)LLMTools (2-bit)
QLoRA (4-bit)
Full precision
Figure 2: Visualization of memory require-
ments with diﬀerent methods.Comparison to QLoRA In concurrent work,
Dettmers et al. (2023) proposed QLoRA, a related ap-
proach for ﬁnetuning a quantized LLM. We highlight
methodological and experimental diﬀerences below. From
a methods perspective, ModuLoRA integrates with a
user-speciﬁed black-box quantization module. In our
experiments, we ﬁnd that using a sophisticated data-driven
quantizer like OPTQ improves performance over simpler
zero-shot strategies, e.g., a round-to-nearest baseline. Un-
likeModuLoRA , QLoRA deﬁnes a quantization approach
9

--- PAGE 10 ---
similar to RTN, but also introduces a specialized packing
routine, quantization of zeros and scales, and other innova tions.
From an experiments and capabilities perspective, integra ting with OPTQ enables ModuLoRA to ﬁntune
models quantized in 2-bits and 3-bits, which QLoRA cannot do . Lastly, we identify settings where Modu-
LoRA yields LLMs with better performance than LLMs from QLoRA; th is gap is likely due to the use of
improved quantizers.
Comparison to Other Parameter-Eﬃcient Finetuning Methods Recent Parameter-Eﬃcient Fine-
tuning (PEFT) methods have encompassed a range of technique s such as prompt tuning (Lester et al.,
2021; Li & Liang, 2021; Qin & Eisner, 2021; Liu et al., 2022b), modiﬁcation of the embedding layer in-
puts (An et al., 2022) or hidden states (Liu et al., 2022a), in clusion of full layers (Houlsby et al., 2019),
only tuning biases (Zaken et al., 2021), and others (Sung et a l., 2021; Karimi Mahabadi et al., 2021). An
important shortcoming of these methods is the need to store i n memory a signiﬁcant amount of frozen base
model parameters. This limits their ability to ﬁnetune the l argest LLMs on consumer GPU, a limitation
that we address.
5.2 Running LLMs on Consumer GPUs
Eﬃcient LLM Algorithms The computational requirements of modern deep neural netwo rks motivate a
wide range of eﬃcient machine learning algorithms. Quantiz ation methods reduce the number of bits required
to store weights (Dong et al., 2019; 2020; Hubara et al., 2021 ; Li et al., 2021; Yao et al., 2021), including via
adaptive methods (Nagel et al., 2020). SmoothQuant (Xiao et al., 2023) rescales between activations and
weights to remove outliers from the activations and make qua ntization overall easier. ZeroQuant (Yao et al.,
2022) proposes a per-layer knowledge distillation method. LLM.int8() (Dettmers et al., 2022) decompose
matrix multiplications into a majority of 8 bit and a minorit y of 16 bit operations. LUT-GEMM (Park et al.,
2023) designs kernels to accelerate quantized matrix multi plications. RPTQ (Yuan et al., 2023) reorders
activations and quantizes them in groups, reducing the impa ct of range diﬀerences between channels.
Running LLMs on Consumer GPUs Our methods for 3-bit and 4-bit precision enable the ﬁnetuni ng
of a 65B LLM on a 48GB GPU, and a 30B LLM on a 24GB GPU. Additional ly, our 2-bit approach allows
for the ﬁnetuning of a 65B LLM on a 24GB GPU, making the ﬁnetuni ng of LLMs accessible on consumer
hardware. Moreover, ﬁtting an entire LLM on GPU unlocks data parallelism, which is more eﬃcient than
model parallelism. Previous 8-bit quantization methods re quired a 96GB GPU to fully ﬁt a 65B model.
Finetuning GPUs on consumer hardware holds promise to accel erate model iteration and apply LLMs to a
wider range of domains by a larger number of practitioners.
5.3 What is a Good Base LLM for Finetuning?Models Quantization BBH PPL
LLAMA (13B)3-bit 35.3 6.63
4-bit 36.2 5.36
LLAMA (65B)3-bit 43.3 5.04
4-bit 43.7 3.84
Table 8: BBH vs. PPLThe traditional measure of a base LLM is perplexity. In
the adjacent table, we report LLaMA perplexity (PPL)
on Wiki2 as well as ﬁnetuning performance on BBH. In-
terestingly, the correlation is not perfect: large gaps in
PPL admit small gaps in BBH. This questions LLM eval-
uation when the goal is ﬁnetuning, and suggests exploring
new training strategies.
More generally, our results provide empirical evidence tha t high performance on downstream tasks can
be achieved with a smaller quantized LLM than previously tho ught. While existing methods (e.g.,
LLM.int8()+LoRA; Dettmers et al. (2022)) operate in 8 bits, we ﬁnd that 2-bit, 3-bit, or 4-bit ﬁnetun-
ing yields the best results for a ﬁxed bit budget. For example , we ﬁnd that 4-bit and 3-bit 65B models
outperform 8-bit and 16-bit 30B models on instruction follo wing tasks. On the SAMSum summarization
task, we ﬁnd that 3-bit models are able to attain a new state-o f-the-art ROUGE score, and 2-bit models
match the performance of 8-bit models quantized with LLM.in t8(). The high performance of these low-
10

--- PAGE 11 ---
precision models suggests that competitive ﬁnetuning perf ormance can be achieved on any base quantized
LLM with x-bit precision, provided that the LLM exhibits rea sonably good performance from the beginning.
5.4 Limitations
An advantage of LoRA is that it has low inference overhead, si nce the low-rank adaptor can be added in
to the full-precision weight matrix when deploying. One lim itation of ModuLoRA is that it does not
share this advantage relative to the black-box quantized mo del: the low-rank adaptor cannot be trivially
added to the weight matrix because the weight matrix is quant ized while the adaptor is not. So, the weight
matrix and adaptor cannot be fused readily, and an implement ation as in Figure 1 is required at inference
time. A second limitation of ModuLoRA is that making ﬁnetuning possible on widely available commo dity
hardware may make ﬁnetuning too easy, presenting potential problems related to LLM safety. Another
limitation of ModuLoRA is that the largest models in use today (e.g. GPT-4) can have u p to 1 trillion
parameters, and even at the minimum of 1 bit per parameter thi s still would take up 125 GB, which exceeds
memory on commodity GPUs: thus a straightforward applicati on of ModuLoRA will be unable to make
these largest-scale models ﬁnetunable on commodity hardwa re.
6 Conclusion
Finetuning large language models typically requires subst antial hardware and storage resources. Our method,
ModuLoRA , enables 2-bit ﬁnetuning of 65B models on a single 24GB consu mer GPU and also supports
3-bit and 4-bit ﬁnetuning of the same models using a single 48 GB GPU. At the core of our approach
is a simple, quantization-agnostic backward pass that enab les integrating low-rank adapters with frozen
LLM weights obtained from a user-deﬁned quantization modul e. By integrating with modern quantizers,
ModuLoRA achieves state-of-the-art performance compared to both pa rameter-eﬃcient and full ﬁne-tuning
techniques.
ModuLoRA ’s ﬂexibility and competitive performance make ﬁnetuning m ore accessible and cost-eﬀective in
a resource-constrained setting. This assists open-source model development and facilitates scientiﬁc research.
More broadly, we believe that ModuLoRA will help democratize access to large language models and ma ke
them available to a broader audience.
11

--- PAGE 12 ---
References
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and
Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint
arXiv:2203.03131 , 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jare d D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, a nd et. al. Language models are few-shot
learners. In Conference on Neural Information Processing Systems , 2020.
Sahil Chaudhary. Code alpaca: An instruction-following ll ama model for code generation.
https://github.com/sahil280114/codealpaca , 2023.
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christophe r De Sa. Quip: 2-bit quantization of large
language models with guarantees, 2023.
Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation
of instruction-tuned large language models. arXiv preprint arXiv:2306.04757 , 2023.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instru ction-ﬁnetuned language models. arXiv
preprint arXiv:2210.11416 , 2022.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemo yer. Llm.int8(): 8-bit matrix multiplication
for transformers at scale. In Conference on Neural Information Processing Systems , 2022.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zett lemoyer. Qlora: Eﬃcient ﬁnetuning of
quantized llms. arXiv preprint arXiv:2305.14314 , 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Tout anova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Hawq: Hessian aware
quantization of neural networks with mixed-precision. In International Conference on Computer Vision ,
2019.
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michae l W. Mahoney, and Kurt Keutzer. Hawq-
v2: Hessian aware trace-weighted quantization of neural ne tworks. In Conference on Neural Information
Processing Systems , 2020.
Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alis tarh. Optq: Accurate quantization for genera-
tive pre-trained transformers. In International Conference on Learning Representations , 2023.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander W awer. SAMSum corpus: A human-annotated
dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in
Summarization , pp. 70–79, Hong Kong, China, November 2019. Association fo r Computational Linguistics.
doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Brun a Morrone, Quentin De Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. Parameter-eﬃcie nt transfer learning for nlp. In International
Conference on Machine Learning , pp. 2790–2799. PMLR, 2019.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, S hean Wang, Lu Wang, Weizhu Chen, et al.
Lora: Low-rank adaptation of large language models. In International Conference on Learning Represen-
tations , 2022.
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Dani el Soudry. Accurate post training quanti-
zation with small calibration sets. In International Conference on Machine Learning . PMLR, 2021.
Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Rud er. Compacter: Eﬃcient low-rank hyper-
complex adapter layers. Advances in Neural Information Processing Systems , 34:1022–1035, 2021.
12

--- PAGE 13 ---
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of s cale for parameter-eﬃcient prompt tuning. In
Proceedings of the 2021 Conference on Empirical Methods in N atural Language Processing , pp. 3045–3059,
Online and Punta Cana, Dominican Republic, November 2021. A ssociation for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243 .
Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing con tinuous prompts for generation. In Pro-
ceedings of the 59th Annual Meeting of the Association for Co mputational Linguistics and the 11th In-
ternational Joint Conference on Natural Language Processi ng (Volume 1: Long Papers) , pp. 4582–4597,
Online, August 2021. Association for Computational Lingui stics. doi: 10.18653/v1/2021.acl-long.353. URL
https://aclanthology.org/2021.acl-long.353 .
Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, F engwei Yu, Wei Wang, and Shi
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International
Conference on Learning Representations , 2021.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A
Raﬀel. Few-shot parameter-eﬃcient ﬁne-tuning is better an d cheaper than in-context learning. Advances
in Neural Information Processing Systems , 35:1950–1965, 2022a.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroak i Hayashi, and Graham Neubig. Pre-train,
prompt, and predict: A systematic survey of prompting metho ds in natural language processing. ACM
Computing Surveys , 55(9):1–35, 2023a.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhi lin Yang, and Jie Tang. P-tuning: Prompt
tuning can be comparable to ﬁne-tuning across scales and tas ks. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 2 : Short Papers) , pp. 61–68, 2022b.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Z hilin Yang, and Jie Tang. Gpt understands,
too. AI Open , 2023b.
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-of-the-
art parameter-eﬃcient ﬁne-tuning methods. https://github.com/huggingface/peft , 2022.
Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Loui zos, and Tijmen Blankevoort. Up or down?
adaptive rounding for post-training quantization. In International Conference on Machine Learning , pp.
7197–7206. PMLR, 2020.
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeongho on Kim, Beomseok Kwon, Se Jung Kwon,
Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Qu antized matrix multiplication based
on luts for eﬃcient inference in large-scale generative lan guage models. arXiv preprint arXiv:2206.09557 ,
2023.
Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv
preprint arXiv:2104.06599 , 2021.
Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha ran Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer lea rning with a uniﬁed text-to-text transformer,
2020.
Sylvestre-Alvise Rebuﬃ, Alexander Kolesnikov, Georg Sper l, and Christoph H Lampert. icarl: Incremental
classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pp. 2001–2010, 2017.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick , Suzana Ilić, Daniel Hesslow, Roman Castagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, e t al. Bloom: A 176b-parameter open-access
multilingual language model. arXiv preprint arXiv:2211.05100 , 2023.
Yi-Lin Sung, Varun Nair, and Colin A Raﬀel. Training neural n etworks with ﬁxed sparse masks. Advances
in Neural Information Processing Systems , 34:24193–24205, 2021.
13

--- PAGE 14 ---
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Ch allenging big-bench tasks and
whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, X uechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An inst ruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca , 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mart inet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, e t al. Llama: Open and eﬃcient foundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Chris De sa. Quip#: Quip with lattice
codebooks. https://cornell-relaxml.github.io/quip-sharp/ , 2023.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-c overage challenge corpus for sen-
tence understanding through inference. In Proceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguist ics: Human Language Technologies, Vol-
ume 1 (Long Papers) , pp. 1112–1122. Association for Computational Linguistic s, 2018. URL
http://aclweb.org/anthology/N18-1101 .
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demou th, and Song Han. Smoothquant: Accurate
and eﬃcient post-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2023.
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jial i Yu, Eric Tan, Leyuan Wang, Qijing Huang,
Yida Wang, Michael W. Mahoney, and Kurt Keutzer. Hawq-v3: Dy adic neural network quantization. In
International Conference on Machine Learning . PMLR, 2021.
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia W u, Conglong Li, and Yuxiong He. Zeroquant:
Eﬃcient and aﬀordable post-training quantization for larg e-scale transformers. In Conference on Neural
Information Processing Systems , 2022.
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu,
Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-trai ning quantization for large language models.
arXiv preprint arXiv:2304.01089 , 2023.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: S imple parameter-eﬃcient ﬁne-tuning for
transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Mo ya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott , Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and L uke Zettlemoyer. Opt: Open pre-trained
transformer language models, 2022.
14

--- PAGE 15 ---
A Additional Implementation Details
A.1 Conﬁgurations for BBH Evaluation
We evaluate the BBH dataset using LoRA adapter weights from h uggingface hub with diﬀerent conﬁgurations.
For the Bits&Bytes 8-bit (LLM.int8()) LoRA adapter weights , we utilized two sources: the Alpaca-7B
one is obtained from the ’tloen/alpaca-lora-7b’ repositor y, while the weights for Alpaca-13b and 30b were
sourced from ’chansung/alpaca-lora-xxb’ . In the case of Bi ts&Bytes 4-bit (QLoRA) adapter weights, all
conﬁgurations (Alpaca-7B, 13B, and 30B)—were uniformly ac cessed from ’timdettmers/qlora-alpaca-xxb’ .
Note that for the Bits&Bytes 4-bit (QLoRA) and Bits&Bytes 8- bit (LLM.int8()) adapter wights of the 65B
model, we obtain them by ﬁnetuning the base 65B LLaMa model on Alpaca dataset using the same set of
hyperparameters as ours.
B Additional Empirical Experiments
B.1 Additional Experiments on Code-Alpaca with LLaMA
We conducted additional experiment on Code-Alpaca ((Chaud hary, 2023)). The result is shown in Table 9.
Consistent with our hypothesis, ModuLoRA performs better than or at least on par with the higher precis ion
8-bit models given the same number of trainable parameters a nd set up.
Code Alpaca Per-
formance7B 13B 30B 65B
LLMTools (3-bit) 53.6 / 36.3 / 50.7 57.0 / 40.0 / 53.3 58.1 / 40.7 / 54.3 60 .0 / 44.1 / 58.8
LLMTools (4-bit) 54.6 / 37.2 / 51.4 57.4 / 40.6 / 54.3 59.0 / 41.4 / 57.5 60 .2 / 43.5 / 56.8
Bits&Bytes 8-bit
(LLM.int8())54.0 / 36.3 / 50.9 57.7 / 41.3 / 54.9 60.6 / 43.5 / 57.5 61.1 / 44.1 / 58.0
Table 9: Instruction-tuned models evaluated using ROUGE 1/ 2/LSum on Code Alpaca in 3, 4, and 8 bits.
B.2 Finetuning & Inference Latency
We conducted experiment to test the ﬁnetuning and inference latency of ModuLoRA .
Finetuning . During ﬁnetuning, ModuLoRA signiﬁcantly outperforms full-precision LoRA as show in
table 10, reducing the training time by approximately 59.3% and memory usage by 91.5%. This eﬃciency
in ﬁnetuning speed is primarily attributed to reduced data m ovement within the GPU memory.
Inference . During inference, ModuLoRA has a slightly lower speed compared to LoRA and QLoRA as
shown in table 11. We attribute this to the use of CUDA kernels that are currently not as optimized as
those of QLoRA. Note that
Precision LLMTools QLoRA LoRA
(2-bit) (4-bit) (Full Precision)
Seconds/Iteration 0.61 s/it 0.80 s/it 1.50 s/it
Table 10: Finetuning speed for LLAMA 7B on MNLI-
m benchmark with batch size 1. We report the aver-
age time to complete one step for one training data
entry. To ensure fair comparison, we use a single
A6000 to run on all three methods.Precision LLMTools QLoRA LoRA
(2-bit) (4-bit) (Full Precision)
Seconds/Iteration 0.68 s/it 0.52 s/it 0.52 s/it
Table 11: Inference speed for LLAMA 7B on MNLI-m
benchmark. We report the average time to complete
inference for one evaluation data entry. To ensure fair
comparison, we use a single A6000 to run on all three
methods.
15

--- PAGE 16 ---
C Hyperparamters Used in Experiments
C.1 LLaMA / OPT on SAMSum
We set up the training procedure following Hu et al. (2022), w ith particular accommodation to our particular
language models. For a fair comparison with the concurrent w ork QLoRA, we use the exact same hyperpa-
rameter set up as shown in Table 12 . We train using AdamW for 35 0 steps with a batch size of 128 samples.
We report the results over 3 random seeds; the result for each run is taken from the training steps with the
lowest validation loss.
Dataset Model LLaMA 7B / 13B / 30B / 65B OPT 7B/ 13B / 30B
SAMSumOptimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 16
Evaluation Steps 50
Total # Training Steps 350
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRAConﬁg rq=rv= 8
LoRA α 32
Max Seq. Len 250
Table 12: Hyperparamters conﬁguration for ModuLoRA, Q-LoR A on SAMSum
C.2 LLaMA on Code-Alpaca & Text-Classiﬁcation
We again train using AdamW optimizer with a warmup ratio of 0. 06. We tune learning rate, batch size,
training steps for each task. We report the results over 3 ran dom seeds. The result for each run is taken
from the training steps that yield the lowest validation los s.
Dataset LLaMA Model 13/30/65 B
Text-
ClassiﬁcationOptimizer AdamW
Warmup Ratio 0.06
Batch size 256
Evaluation Batch size 32
Evaluation Steps 100
Total # Training Steps 1000
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRAConﬁg rq=rv= 8
LoRA α 32
Max Seq. Len 128
Table 13: Hyperparamters conﬁguration for Mod-
uLoRA, Q-LoRA on Text-ClassiﬁcationDataset LLaMA Model 7/13/30/65 B
Code-
AlpacaOptimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 4
Evaluation Steps 40
Total # Training Steps 120
Learning Rate Schedule Linear
Learning Rate 1e-3
WeightDecay 0.0
LoRAConﬁg rq=rv= 8
LoRA α 32
Max Seq. Len 165
Table 14: Hyperparamters conﬁguration for Modu-
LoRA, Q-LoRA on Alpaca-Code
16

--- PAGE 17 ---
C.3 LLaMA on MNLI-M
Training is conducted using the AdamW optimizer, with a warm up ratio set at 0.06. We tune the learning
rate, batch size, and training steps. Results are reported o ver three random seeds, and for each run, the
performance metric is derived from the training step with th e lowest validation loss. See Table 15 for more
details on the hyperparameters used.
Dataset Model LLaMA 7B / 13B / 30B / 65B
MNLI-MOptimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 64
Evaluation Steps 64
Total # Training Epoch 1.0
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRAConﬁg rq=rv= 8
LoRA α 32
Max Seq. Len 128
Table 15: Hyperparamters conﬁguration for ModuLoRA, Q-LoR A on MNLI-M
C.4 LLaMA on Alpaca for BBH Evaluation
Training is conducted using the AdamW optimizer, with a warm up ratio set at 0.06. We tune the learning
rate, batch size, and training steps. Results are reported o ver three random seeds. See Table 16 for more
details on the hyperparameters used.
Dataset Model LLaMA 7B / 13B / 30B / 65B
AlpacaOptimizer AdamW
Warmup Ratio 0.06
Batch size 128
Total # Training Epochs 3
Learning Rate Schedule Linear
Learning Rate 1e-3
WeightDecay 0.0
LoRAConﬁg rq=rv= 8
LoRA α 16
Max Seq. Len 256
Table 16: Hyperparamters conﬁguration for ModuLoRA on Alpa ca
17
