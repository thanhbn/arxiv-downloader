# 2309.16119.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.16119.pdf
# Kích thước tệp: 277338 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2309.16119v2 [cs.LG] 10 Tháng 3 2024ModuLoRA: Tinh chỉnh LLMs 2-Bit trên GPU tiêu dùng bằng cách
Tích hợp với Bộ lượng tử hóa mô-đun
Junjie Yin jyin27@jhu.edu
Khoa Khoa học máy tính
Đại học Johns Hopkins
Jiahao Dong jd787@cornell.edu
Khoa Khoa học máy tính
Đại học Cornell và Cornell Tech
Yingheng Wang yw2349@cornell.edu
Khoa Khoa học máy tính
Đại học Cornell
Christopher De Sa cdesa@cs.cornell.edu
Khoa Khoa học máy tính
Đại học Cornell
Volodymyr Kuleshov kuleshov@cornell.edu
Khoa Khoa học máy tính
Đại học Cornell và Cornell Tech
Tóm tắt
Chúng tôi đề xuất một thuật toán tinh chỉnh tiết kiệm bộ nhớ cho các mô hình ngôn ngữ lớn (LLMs) hỗ trợ
tinh chỉnh LLMs với 65B tham số ở độ chính xác 2/3/4-bit trên chỉ một GPU 24GB. Phương pháp của chúng tôi, 
thích ứng thứ hạng thấp mô-đun (ModuLoRA), tích hợp bất kỳ bộ lượng tử hóa trọng số do người dùng chỉ định nào
với tinh chỉnh thông qua các bộ chuyển đổi thứ hạng thấp (LoRAs). Cách tiếp cận của chúng tôi dựa trên một lượt 
truyền ngược bất khả tri lượng tử hóa đơn giản tự động tạo ra các trọng số LLM độ chính xác thấp từ một mô-đun 
lượng tử hóa hộp đen tùy chỉnh. Cách tiếp cận này cho phép tinh chỉnh LLMs 2-bit và 3-bit lần đầu tiên—tận dụng
lượng tử hóa QuIP# 2-bit và lượng tử hóa OPTQ 3-bit hiện đại—vượt trội hơn tinh chỉnh dựa trên các phương pháp 
4-bit và 8-bit kém tinh vi hơn. Trong các thí nghiệm của chúng tôi, ModuLoRA đạt được hiệu suất cạnh tranh 
trên các tác vụ phân loại văn bản, suy luận ngôn ngữ tự nhiên và làm theo hướng dẫn sử dụng ít bộ nhớ hơn 
đáng kể so với các phương pháp hiện tại, và chúng tôi cũng vượt qua điểm ROUGE hiện đại nhất trên một tác vụ 
tóm tắt phổ biến. Chúng tôi phát hành ModuLoRA cùng với một loạt các mô hình độ chính xác thấp như một phần 
của LLMTools, một thư viện thân thiện với người dùng để lượng tử hóa, chạy và tinh chỉnh LLMs trên GPU tiêu dùng.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) xuất sắc trên nhiều tác vụ đa dạng như tạo mã, làm theo hướng dẫn và
suy luận (Brown et al., 2020; Scao et al., 2023; Zhang et al., 2022). Tuy nhiên, kích thước khổng lồ của các 
mô hình này—thường đạt đến hàng trăm tỷ tham số—khiến chúng trở nên khó triển khai trên các tác vụ hạ nguồn 
và thúc đẩy nghiên cứu về các thuật toán tinh chỉnh hiệu quả (Li & Liang, 2021; Hu et al., 2022).

Ở đây, chúng tôi đề xuất thích ứng thứ hạng thấp mô-đun (ModuLoRA), một thuật toán tinh chỉnh tiết kiệm bộ nhớ 
cho các mô hình ngôn ngữ lớn (LLMs) chạy trên phần cứng cấp tiêu dùng. Ví dụ, ở độ chính xác 3-bit, 
ModuLoRA tinh chỉnh một mô hình LLaMA-30B (Touvron et al., 2023) trên một GPU Nvidia RTX 3090 24GB
1

--- TRANG 2 ---
và một LLaMA-65B trên một GPU RTX A6000 48GB. Ở độ chính xác 2-bit, ModuLoRA tinh chỉnh một LLaMA-30B
hoặc LLaMA-65B trên một GPU Nvidia RTX 3090 24GB.

Cách tiếp cận của chúng tôi thêm các bộ chuyển đổi thứ hạng thấp độ chính xác cao vào các trọng số 3-bit hoặc 4-bit
độ chính xác thấp của một LLM cơ sở đông lạnh thu được thông qua các thuật toán lượng tử hóa hiện đại (Hubara et al., 2021; 
Yao et al., 2021; Frantar et al., 2023). Quan trọng, ModuLoRA không chỉ định thủ tục lượng tử hóa riêng của mình—
thay vào đó, nó tích hợp với các bộ lượng tử hóa do người dùng định nghĩa thông qua một lượt truyền ngược 
bất khả tri lượng tử hóa đơn giản. Lượt truyền ngược này tự động tạo ra các trọng số LLM độ chính xác thấp 
thu được từ một bộ lượng tử hóa hộp đen và tích hợp chúng với các bộ chuyển đổi thứ hạng thấp độ chính xác cao.

Chúng tôi phát hành ModuLoRA như một phần của LLMTools, một thư viện thân thiện với người dùng cho phép 
tinh chỉnh LLMs trên GPU tiêu dùng. Khi kết hợp với bộ lượng tử hóa OPTQ hiện đại (Frantar et al., 2023), 
ModuLoRA cho phép tinh chỉnh LLMs 3-bit lần đầu tiên, thường vượt trội hơn các phương pháp dựa trên 
lượng tử hóa 4-bit và 8-bit kém tinh vi hơn. Khi kết hợp với bộ lượng tử hóa QuIP# hiện đại nhất 
Chee et al. (2023); Tseng et al. (2023), ModuLoRA cho phép tinh chỉnh LLMs 2-bit lần đầu tiên, 
đạt được hiệu suất tương đương với các phương pháp lượng tử hóa 4-bit và 8-bit kém tinh vi hơn. 
Trên các tác vụ phân loại, suy luận ngôn ngữ tự nhiên và làm theo hướng dẫn, các mô hình độ chính xác thấp 
của chúng tôi đạt được hiệu suất cạnh tranh sử dụng ít bộ nhớ hơn đáng kể so với các phương pháp hiện tại. 
Trên một điểm chuẩn tóm tắt phổ biến, chúng tôi đạt được điểm ROUGE hiện đại nhất mới sử dụng một mô hình 
LLaMA-65B được lượng tử hóa. Chúng tôi mở nguồn tất cả các mô hình độ chính xác thấp của mình, 
bao gồm họ mô hình Alpaca 3-bit đầu tiên có hiệu suất làm theo hướng dẫn mạnh mẽ ở nhiều kích thước mô hình khác nhau. 
Các phát hiện của chúng tôi tiết lộ rằng hiệu suất cao có thể đạt được sử dụng các LLMs được lượng tử hóa 
nhỏ hơn so với những gì nghĩ trước đây.

Đóng góp. Tóm lại, bài báo này đưa ra các đóng góp sau: (1) chúng tôi đề xuất ModuLoRA, một phương pháp 
tinh chỉnh tiết kiệm bộ nhớ hoạt động trên các trọng số độ chính xác thấp thu được thông qua một mô-đun 
lượng tử hóa hộp đen do người dùng chỉ định; (2) chúng tôi phát hành LLMTools, một thư viện Python 
thân thiện với người dùng có triển khai ModuLoRA và cho phép người dùng dễ dàng tinh chỉnh các LLMs lớn nhất 
trên GPU tiêu dùng; (3) chúng tôi cung cấp bằng chứng thực nghiệm rằng hiệu suất cao trên các tác vụ hạ nguồn 
có thể đạt được với một LLM nhỏ hơn so với những gì nghĩ trước đây.

2 Bối cảnh và Công trình liên quan
Chúng tôi quan tâm đến việc tinh chỉnh một LLM đã được tiền huấn luyện cho các tác vụ hạ nguồn (Li & Liang, 2021; 
Lester et al., 2021; Houlsby et al., 2019; Rebuffi et al., 2017). LLMs sử dụng kiến trúc transformer 
trong đó hầu hết các trọng số có thể học—và hầu hết bộ nhớ được sử dụng để lưu trữ các trọng số này—
xuất hiện trong các lớp tuyến tính.1 Chúng tôi đặt các trọng số và bias của n lớp tuyến tính này 
được ký hiệu là W(i) và b(i) cho i ∈ {1,2, ..., n}. Với một mạng đã được tiền huấn luyện, 
mục tiêu của chúng tôi là tinh chỉnh nó cho các tác vụ hạ nguồn sử dụng ít bộ nhớ làm việc hơn nhiều 
so với việc cần thiết để lưu trữ tất cả các W ở độ chính xác đầy đủ.

2.1 Tinh chỉnh Mô hình Ngôn ngữ Lớn
Do các yêu cầu bộ nhớ cao cần thiết để tinh chỉnh và lưu trữ tất cả các trọng số của một LLM, 
các chuyên gia đã phát triển nhiều phương pháp tinh chỉnh hiệu quả tham số học trong một không gian 
chiều thấp hơn. Các phương pháp này bao gồm chỉ tinh chỉnh lớp đầu ra (Devlin et al., 2018) 
và tinh chỉnh prompt hoặc tiền tố được truyền như đầu vào cho một LLM (Lester et al., 2021; 
Li & Liang, 2021; Liu et al., 2023a;b), cũng như LoRA, là trọng tâm của công trình này.

Thích ứng Thứ hạng Thấp (LoRA) Thuật toán LoRA (Hu et al., 2022) phân tích các trọng số W thành 
tổng của các trọng số mô hình cơ sở đông lạnh W0∈Rd×d và một bộ chuyển đổi thứ hạng thấp cộng thêm nhỏ AB⊤ 
bao gồm tích của hai ma trận hình chữ nhật A,B∈Rd×r, trong đó r >0 chỉ ra thứ hạng2:

W=W0+AB⊤. (1)

1Các lớp này bao gồm các ma trận chiếu K,V,Q, và O của các khối attention và các lớp tuyến tính của các khối MLP.
2Để đơn giản ở đây chúng tôi xem xét các ma trận trọng số vuông W; trường hợp hình chữ nhật là một tổng quát hóa đơn giản.
2

--- TRANG 3 ---
LoRA giảm số lượng tham số được huấn luyện bằng một hệ số 2r/d, giảm chi phí lưu trữ, truyền và 
chuyển đổi tác vụ của suy luận trên một hệ thống đã duy trì mô hình cơ sở. Tuy nhiên, LoRA 
phải giữ các trọng số cơ sở W0 trong bộ nhớ, điều này đòi hỏi nhiều GPU cao cấp và ngăn cản 
việc tinh chỉnh các LLMs lớn trên phần cứng hàng hóa.

2.2 Học máy Độ chính xác Thấp
Các yêu cầu tính toán của các mô hình học máy hiện đại thúc đẩy một loạt các thuật toán học máy hiệu quả 
(Li & Liang, 2021; Hu et al., 2022; Frantar et al., 2023).

Lượng tử hóa Các phương pháp lượng tử hóa cho mạng nơ-ron giảm số bit cần thiết để lưu trữ trọng số mô hình 
(Dong et al., 2019; 2020; Yao et al., 2022; Park et al., 2023). Một phương pháp lượng tử hóa b-bit có dạng

(ŴWq,z,s) =Q(W) ŴW=D(ŴWq,z,s). (2)

Ở đây, thuật toán lượng tử hóa Q lấy một ma trận trọng số W∈Rd×d (hoặc tập con của nó) và đưa ra một 
phiên bản lượng tử hóa ŴWq∈ {0,1, . . . , 2b−1}d×d (sử dụng b bit để biểu diễn mỗi phần tử của W), 
cũng như các tham số không và tỷ lệ z,s∈Rd (ở độ chính xác đầy đủ). Thuật toán dequantization D(ŴWq,z,s) 
khôi phục một xấp xỉ ŴW∈Rd×d bằng cách tái tỷ lệ các trọng số lượng tử hóa như ŴW=s⊙ŴWq+z, 
trong đó ⊙ biểu thị tích Hadamard, và⊙,+ được mở rộng với phát sóng kiểu numpy.

Gần đây, Frantar et al. (2023) đã đề xuất OPTQ, một thuật toán lượng tử hóa mở rộng cho các LLMs hiện đại. 
Phương pháp này lặp lại hai bước trên các cột trọng số: (1) lượng tử hóa với làm tròn gần nhất và tính toán lỗi, 
(2) cập nhật các trọng số còn lại với một lỗi được tỷ lệ. Nhiều thí nghiệm của chúng tôi tinh chỉnh 
các LLMs được lượng tử hóa với OPTQ.

Theo sau OPTQ, Chee et al. (2023) đã đề xuất QuIP, một thuật toán lượng tử hóa làm cho nén LLM hai bit 
khả thi lần đầu tiên. Phương pháp này theo một quy trình 2 bước: (1) một thủ tục làm tròn thích ứng 
tối thiểu hóa một mục tiêu proxy bậc hai, (2) một thủ tục tiền và hậu xử lý hiệu quả đảm bảo tính không 
nhất quán của trọng số và Hessian thông qua nhân với các ma trận trực giao ngẫu nhiên. Hơn nữa, 
Tseng et al. (2023) đã đề xuất QuIP#, kết hợp các codebook lattice với xử lý không nhất quán từ QuIP 
để tạo ra các mô hình lượng tử hóa 2 bit hiện đại nhất. Chúng tôi cho thấy hiệu suất của các LLMs 
lượng tử hóa QuIP# (với codebook D4) trên thí nghiệm tóm tắt SAMSum.

Trong công trình đồng thời, Dettmers et al. (2023) đã đề xuất QLoRA, một phương pháp tinh chỉnh LLMs 
lượng tử hóa dựa trên LoRA. Trong khi công trình của chúng tôi tìm cách tích hợp với bất kỳ mô-đun 
lượng tử hóa do người dùng định nghĩa nào (như OPTQ), QLoRA định nghĩa lược đồ lượng tử hóa riêng của mình, 
đơn giản hơn so với, chẳng hạn, OPTQ hoặc QuIP. Một lợi thế của phương pháp chúng tôi là hỗ trợ 
tinh chỉnh 2-bit và 3-bit; QLoRA chỉ hỗ trợ tinh chỉnh 4-bit. Chúng tôi cũng sẽ xác định các cài đặt 
trong đó sử dụng các bộ lượng tử hóa tiên tiến mang lại lợi ích hiệu suất so với QLoRA. Xem Phần 5.1 để biết chi tiết.

3 Thích ứng Thứ hạng Thấp Độ chính xác Thấp với Bộ lượng tử hóa Mô-đun
Trong phần này, chúng tôi mô tả thích ứng thứ hạng thấp mô-đun (ModuLoRA), một thuật toán tinh chỉnh 
tiết kiệm bộ nhớ cho các mô hình ngôn ngữ lớn (LLMs) tận dụng các thuật toán lượng tử hóa tùy chỉnh 
và chạy trên phần cứng GPU tiêu dùng.

3

--- TRANG 4 ---
class ModuLoRALinear (Module):
"""Lớp ModuLoRA Tuyến tính"""
def__init__ (self,...):
self.hatWq_z_s =quantize(pretrained_W)
(self.A,self.B)=lora_init( ...)
defforward (self, x):
(hatWq, z, s) =self.hatWq_z_s
return LPLinear .apply(x, hatWq, z, s) \
+(x@self.B)@self.A.t()+self.biasclass LPLinear (Function):
"""Ánh xạ Tuyến tính Độ chính xác Thấp"""
@staticmethod
defforward (ctx, input , hatWq, z, s):
ctx.save_for_backward(hatWq, z, s)
hatW =dequantize(hatWq, z, s)
output =input @hatW .t()
return output # hatW được giải phóng
@staticmethod
defbackward (ctx, grad_output):
hatWq, z, s =ctx.saved_tensors
# chúng tôi tính toán lại hatW
hatW =dequantize(hatWq, z, s)
grad_input =grad_output @hatW
# ở đây hatW có thể được giải phóng
return grad_input, None ,None ,None

Hình 1: Pseudocode PyTorch cho ModuLoRA.

3.1 Thích ứng Thứ hạng Thấp của Mô hình Độ chính xác Thấp
Bước đầu tiên của phương pháp chúng tôi là lượng tử hóa: chúng tôi áp dụng một thuật toán lượng tử hóa 
hộp đen Q cho một tập hợp các ma trận trọng số tiền huấn luyện W(i). Điều này mang lại các trọng số 
lượng tử hóa, số không và tỷ lệ (ŴW(i)q,z(i),s(i)) =Q(W(i)). Chúng tôi sử dụng ŴW(i)q để biểu thị 
các trọng số lượng tử hóa được lưu trữ ở độ chính xác thấp, trong khi ŴW(i) biểu thị các trọng số 
tương tự được hiện thực hóa ở độ chính xác cao (cả hai đều xấp xỉ các trọng số ban đầu W(i)). 
Quan trọng, chúng tôi không chỉ định một thủ tục lượng tử hóa Q như một phần của ModuLoRA—
thay vào đó, chúng tôi tìm cách hỗ trợ các bộ lượng tử hóa do người dùng định nghĩa được phương pháp 
của chúng tôi coi như một hộp đen.

Cốt lõi của các nỗ lực chúng tôi tập trung vào tinh chỉnh mô hình cơ sở lượng tử hóa. Phương pháp của chúng tôi 
đầu tiên sửa đổi mạng bằng cách thay thế mỗi lớp tuyến tính—ban đầu được định nghĩa bởi ánh xạ affine 
x↦x(W(i))⊤+b(i)—với lớp ModuLoRALinear độ chính xác thấp được tham số hóa lại trong Hình 1, được cho bởi

x↦x(ŴW(i))⊤+xB(i)(A(i))⊤+b(i). (3)

Ở đây A(i),B(i)∈Rd×r là các tham số có thể học được khởi tạo như trong Hu et al. (2022), và 
ŴW(i)=D(ŴW(i)q,z(i),s(i)) là ma trận trọng số dequantized cố định. Lưu ý rằng điều này tương đương 
về mặt đại số (nhưng không phải về mặt tính toán) với việc biến đổi ma trận lượng tử hóa như đã cho trong (1). 
Cuối cùng, ModuLoRA khớp các A(i) và B(i) sử dụng backprop và học dựa trên gradient.

Một thách thức chính trong thủ tục này là thực hiện tính toán hiệu quả với các tensor độ chính xác cao 
và độ chính xác thấp. Rõ ràng, lượt truyền xuôi yêu cầu nhân với các trọng số được lưu trữ trong 
các ŴW(i)q lượng tử hóa. Dưới đây, chúng tôi suy ra lượt truyền ngược cho A(i),B(i) và cho thấy 
rằng nó cũng yêu cầu nhân với chuyển vị của các ŴW(i)q.

3.1.1 Cấu trúc của một Lượt truyền ngược Lượng tử hóa
Chúng tôi minh họa các thách thức kỹ thuật phát sinh trong thiết kế của một lượt truyền ngược lượng tử hóa 
trong bối cảnh của một mạng gồm n lớp ModuLoRALinear. Mỗi ModuLoRALinear thực chất là một lớp 
kết nối đầy đủ với các trọng số dày đặc được tham số hóa lại được định nghĩa là

W(i)l=ŴW(i)+A(i)(B(i))⊤, (4)

bias b(i), và đầu ra yi cho i= 1,2, ..., n. Chúng tôi sử dụng ȳi=W(i)lx+b(i) để biểu thị đầu ra 
tiền kích hoạt của bước thứ i và chúng tôi sử dụng L để biểu thị hàm mất mát. Lượt truyền ngược 
tìm cách tính toán các gradient dL/dA(i)
4

--- TRANG 5 ---
và dL/dB(i), trong đó chúng tôi nạp chồng ký hiệu Leibniz cho đạo hàm để cũng biểu thị các gradient. 
Theo quy tắc chuỗi,

dL/dA(i)=dL/dȳi·dȳi/dA(i). (5)

Do cấu trúc cộng của các trọng số W(i)l trong (4), dyi/dA(i) đơn giản để xử lý vì nó không phải 
là một hàm của các trọng số lượng tử hóa ŴW(i)q. Số hạng thứ hai có thể được tính toán thông qua 
quy tắc chuỗi của tích phân như dL/dȳi=dL/dȳi+1·dȳi+1/dyi·dyi/dȳi, (6)

trong đó dyi/dȳi là đạo hàm của hàm kích hoạt, và dȳi+1/dyi= (W(i)l)⊤= (ŴW(i))⊤+B(i)(A(i))⊤.

Các suy dẫn trên chỉ ra rằng tính toán gradient dL/dA(i) (lập luận cho dL/dB(i) là giống hệt) 
yêu cầu thực hiện một phép nhân ma trận-vector dL/dyi+1·(ŴW(i))⊤ giữa một vector độ chính xác cao 
dL/dyi+1 với một ma trận lượng tử hóa (ŴW(i))⊤. Thực hiện phép nhân này một cách ổn định và hiệu quả 
là một thách thức mà chúng tôi phải giải quyết.

3.1.2 Tính toán Hiệu quả Hỗn hợp Độ chính xác của Lượt truyền Xuôi và Ngược
Nếu chúng tôi có thể tính toán trước tất cả các ma trận trọng số dequantized (ŴW(i))⊤ trong định dạng 
độ chính xác cao, thách thức của chúng tôi sẽ được giải quyết: phép nhân ma trận-vector dL/dyi+1·(ŴW(i))⊤ 
trong lượt truyền ngược sẽ hoạt động trên hai mảng độ chính xác cao, và sẽ không đưa ra các câu hỏi 
về hiệu quả và tính ổn định.

Thật không may, tính toán trước tất cả các ma trận trọng số dequantized (ŴW(i))⊤ đòi hỏi cùng một 
lượng bộ nhớ GPU như việc lưu trữ LLM độ chính xác cao ban đầu. Để tính toán này vừa với phần cứng 
GPU tiêu dùng, chúng ta cần tránh hiện thực hóa tất cả các ŴW(i) trong bộ nhớ cùng lúc. Sử dụng (3) 
một cách ngây thơ, backprop sẽ lưu trữ tất cả các ŴW(i) từ lượt truyền xuôi để sử dụng chúng 
trong lượt truyền ngược.

Tính toán Hỗn hợp Độ chính xác Hiệu quả. Chiến lược của chúng tôi là tính toán lại việc hiện thực hóa 
độ chính xác cao ŴW(i) của ŴW(i)q lượng tử hóa trong lượt truyền ngược thay vì lưu nó (Hình 1). 
Trong hàm LPLinear, phương thức forward dequantize ŴW(i) và thực hiện phép nhân. Tương tự, 
backward tái dequantize ŴW(i) và tính toán gradient thông qua quy hoạch động. hatW ra khỏi phạm vi 
và có thể được giải phóng ở cuối mỗi phương thức, vì vậy chỉ một ŴW(i) được lưu trữ trong bộ nhớ 
tại bất kỳ thời điểm nào.

Lượng bộ nhớ được sử dụng trong lượt truyền xuôi của mô-đun LPLoRA là nhỏ: tất cả các trung gian 
đều có cùng kích thước với đầu vào x, hoặc thậm chí nhỏ hơn (ví dụ nếu x∈Rm×d thì x@self.B 
có kích thước Rm×r cho r≪d). Lượng tính toán bổ sung cũng nhỏ: thủ tục dequantization 
ŴW=s⊙ŴWq+z chỉ yêu cầu nhân và cộng một vô hướng với mỗi hàng của ŴWq.

Tăng hiệu quả hơn nữa. Hình 1 mô tả một chiến lược hiện thực hóa trọng số trong đó ŴW(i) được 
hiện thực hóa đầy đủ tại mỗi lớp trong cả lượt truyền xuôi và ngược. Để giảm bộ nhớ hơn nữa, 
chúng ta có thể hiện thực hóa các phần tử của ŴW(i) chỉ khi cần thiết. Đối với nhiều thuật toán 
lượng tử hóa (Nagel et al., 2020; Frantar et al., 2023), chúng ta có thể thực hiện hiện thực hóa hàng: 
dequantize ŴW(i) từng hàng một và ngay lập tức nhân nó với một đầu vào x. ModuLoRA cũng tự nhiên 
tổng quát hóa cho bất kỳ chương trình con tích vector-by-quantized-matrix trực tiếp nào được cung cấp 
bởi bộ lượng tử hóa Q, trong trường hợp đó việc hiện thực hóa bất kỳ phần nào của ŴW(i) có thể 
không cần thiết.

3.2 LLMTools: Một Thư viện cho Tinh chỉnh LLM Hiệu quả sử dụng ModuLoRA.
Chúng tôi triển khai ModuLoRA như một phần của LLMTools, một thư viện thân thiện với người dùng 
cho phép người dùng tương tác với các LLMs lớn nhất trên phần cứng tiêu dùng. Thư viện LLMTools 
cho phép tinh chỉnh LLMs ở độ chính xác 2-bit, 3-bit và 4-bit sử dụng thuật toán ModuLoRA. 
Nó cũng cung cấp một API Python dễ sử dụng cho lượng tử hóa, suy luận và tinh chỉnh, 
cũng như hỗ trợ mô-đun cho nhiều bộ lượng tử hóa, LLMs (bao gồm LLaMA1, LLaMA2, BLOOM và OPT), 
và các thuật toán tối ưu hóa (bao gồm tất cả những gì tương thích với lớp Hugging Face Trainer). 
Cuối cùng, LLMTools hỗ trợ dễ dàng tải các tập dữ liệu và chia sẻ mô hình thông qua Hugging Face Hub. 
Mã của chúng tôi có sẵn tại:
5

--- TRANG 6 ---
https://github.com/kuleshov-group/llmtools ; mã đánh giá của chúng tôi để tái tạo kết quả có sẵn tại:
https://github.com/kuleshov-group/MODULoRA-Experiment.

Một thuật toán lượng tử hóa chính được triển khai trong LLMTools là OPTQ (Frantar et al., 2023). 
Để tích hợp OPTQ với tinh chỉnh dựa trên LoRA, LLMTools cung cấp các triển khai CUDA hiệu quả 
của phép nhân ma trận-vector hỗn hợp độ chính xác, bao gồm hiện thực hóa hàng và trọng số. 
Chúng tôi cung cấp các kernel CUDA cho cả hiện thực hóa hàng và trọng số trong cả lượt truyền xuôi 
và ngược. Để đạt hiệu quả tối đa, chúng tôi hiện thực hóa các phần tử của ŴW(i)q trong float16. 
Các mô hình LLM cơ sở lượng tử hóa được biểu diễn thông qua các trọng số ŴW(i)q được lưu trữ 
trong 3 hoặc 4 bit, với các tỷ lệ và số không s(i),z(i) cũng như bias b(i) đều được lưu trữ 
như float16.

Tương tự, để tích hợp QuIP# với LoRA, LLMTools cung cấp các kernel CUDA cho tái hiện thực hóa 
trọng số và nhân ma trận trực giao trong lượt truyền xuôi và ngược. Các mô hình LLM cơ sở lượng tử hóa 
được biểu diễn thông qua các trọng số ŴW(i)q được lưu trữ trong 2 bit.

4 Thí nghiệm
4.1 Thiết lập
Mô hình. Chúng tôi đánh giá ModuLoRA và LLMTools trên họ mô hình LLaMA gần đây (Touvron et al., 2023), 
cũng như các mô hình BLOOM (Scao et al., 2023) và OPT mã nguồn mở (Zhang et al., 2022). 
Chúng tôi lượng tử hóa các mô hình xuống 3 bit và 4 bit sử dụng OPTQ như trong Frantar et al. (2023) 
với 128 mẫu hiệu chỉnh từ C4 (Raffel et al., 2020). Chúng tôi lượng tử hóa các mô hình xuống 2 bit 
sử dụng QuIP# như trong Chee et al. (2023); Tseng et al. (2023) với codebook lattice E8.

Baseline. Chúng tôi sử dụng LoRA (như được triển khai trong thư viện PEFT (Mangrulkar et al., 2022)) 
để tinh chỉnh các mô hình lượng tử hóa 8 bit sử dụng thư viện BitsAndBytes (Dettmers et al., 2022); 
chúng tôi cũng so sánh với kết quả độ chính xác đầy đủ từ tài liệu. Trong công trình đồng thời, 
Dettmers et al. (2023) đã đề xuất QLoRA, một thuật toán tinh chỉnh 4-bit liên quan được triển khai 
trong thư viện BitsAndBytes. Theo đó, chúng tôi trình bày một so sánh thực nghiệm của QLoRA 
với phương pháp của chúng tôi, cùng với một thảo luận sâu.

Huấn luyện. Chúng tôi tinh chỉnh tất cả các mô hình trên GPU NVIDIA TITAN, 3090 và A6000 
(tùy thuộc vào mô hình) với thứ hạng LoRA r= 8 và alpha a= 32, và báo cáo kết quả từ 3 seed ngẫu nhiên. 
Chúng tôi thiết lập quy trình huấn luyện theo Hu et al. (2022), với một số biến đổi nhỏ để phù hợp 
với các mô hình ngôn ngữ cụ thể của chúng tôi. Để so sánh công bằng với công trình đồng thời 
của Dettmers et al. (2023), chúng tôi sử dụng cùng một bộ siêu tham số. Vui lòng xem Phụ lục C 
để biết chi tiết về các siêu tham số được sử dụng cho mỗi thí nghiệm của chúng tôi.

4.2 Phân loại Văn bản
Dữ liệu & Chỉ số. Chúng tôi bắt đầu với một tác vụ phân loại văn bản đơn giản trong đó chúng tôi 
tìm cách phân loại một đoạn văn bản ngắn (lên đến 50 từ) thành thể loại của nó (ví dụ: tiểu thuyết, 
trò chuyện điện thoại, v.v.). Chúng tôi tinh chỉnh các mô hình LLAMA 13B đến 65B trên 392,702 
đoạn từ năm thể loại và đánh giá trên 9,815 trường hợp giữ lại (Williams et al., 2018), 
báo cáo độ chính xác. Điều này mang lại một tác vụ phân loại thách thức cho LLMs ở tất cả các kích thước.

LLAMA Tuning 13B 30B 65B
LLMTools (3-bit) 93.5 ±0.7 97.0 ±0.9 97.2 ±0.8
LLMTools (4-bit) 92.9 ±0.7 96.3 ±1.0 98.0 ±0.9
Bits&Bytes 8-bit (LLM.int8()) 93.0 ±0.7 93.7 ±1.0 98.6 ±1.0

Bảng 1: Độ chính xác phân loại văn bản (%) cho LLAMAs được tinh chỉnh với LoRA & ModuLoRA ở 3, 4, 8 bit.

Kết quả. Chúng tôi quan sát thấy rằng độ chính xác phân loại cải thiện đáng kể khi chúng tôi tăng 
số lượng tham số của LLM. ModuLoRA kết hợp với LLM 3-bit hoặc 4-bit mang lại hiệu suất tương đương 
với tinh chỉnh 8-bit trong Bits&Bytes trong khi sử dụng ít bộ nhớ hơn đáng kể (Bảng 1).
6

--- TRANG 7 ---
4.3 Suy luận Ngôn ngữ Tự nhiên
Dữ liệu & Chỉ số. Tiếp theo, chúng tôi tinh chỉnh LLMs trên các tác vụ suy luận ngôn ngữ tự nhiên. 
Mô hình được yêu cầu dự đoán một nhãn từ một tập nhỏ (entailment, contradiction, hoặc neutral) 
sau khi được trình bày một cặp câu (một cặp câu giả thuyết và tiền đề). Chúng tôi tinh chỉnh 
các mô hình LLaMA 7B đến 65B trên Multi-Genre Natural Language Inference Corpus (MNLI) 
(Williams et al., 2018) và đánh giá trên các tập kiểm tra matched (các ví dụ trong miền), 
báo cáo độ chính xác. Các baseline từ GPT-3 và T5 được bao gồm, như được trình bày 
trong Hu et al. (2022) và Chung et al. (2022).

Kết quả. Mô hình LLaMA 65B 2-bit và 3-bit của chúng tôi phù hợp với hiệu suất của baseline 
GPT-3+LoRA độ chính xác đầy đủ. Đáng chú ý, các mô hình 65B 2-bit được tinh chỉnh với QuIP# 
vượt trội hơn các mô hình 65B còn lại với độ chính xác cao hơn. Chúng tôi cũng thấy rằng 
các mô hình 3-bit và 4-bit từ LLMTools vượt trội hơn các mô hình 8-bit từ thư viện Bits&Bytes 
cho toàn bộ dải kích thước mô hình. Các mô hình ModuLoRA 2-bit, 3-bit và 4-bit đều phù hợp 
hoặc vượt trội hơn các đối tác QLoRA 4-bit của chúng, thường sử dụng ít bộ nhớ hơn do các mô hình 
độ chính xác thấp hơn.

Baselines
Models Finetuning Adaptation Model Size # Trainable Parameters MNLI-m (accuracy)
GPT-3 Full Finetuning 175B 175,255.8M 89.5 ±0.1
GPT-3 Adapter 175B 40.1M 91.5 ±0.1
GPT-3 LoRA 175B 4.7M 91.7 ±0.1
T5 Full Finetuning 11B 11,307.4M 92.2 ±0.1
LLaMA Finetuning Quantizer 7B 13B 30B 65B
LLMTools (2-bit) QuIP#(E8) 88.50 ±0.3 89.72 ±0.3 91.30 ±0.3 91.85 ±0.3
LLMTools (3-bit) OPTQ 88.98 ±0.2 90.20 ±0.2 91.09 ±0.2 91.42 ±0.1
LLMTools (4-bit) OPTQ 89.31 ±0.2 90.41 ±0.2 91.31 ±0.1 91.59 ±0.2
Bits&Bytes (4-bit) QLoRA 89.28 ±0.2 89.67 ±0.2 91.22 ±0.1 91.36 ±0.2
Bits&Bytes (8-bit) LLM.int8() 88.95 ±0.1 90.08 ±0.1 91.15 ±0.1 91.55 ±0.1

Bảng 2: Suy luận ngôn ngữ tự nhiên trên tập dữ liệu MNLI-m được đánh giá bằng độ chính xác phân loại (%). 
Mô hình LLaMA-65B-3bit của chúng tôi tiến gần đến điểm số hiện đại nhất sử dụng ít bộ nhớ hơn đáng kể.

4.4 Tóm tắt Trừu tượng
Dữ liệu & Chỉ số. Chúng tôi tinh chỉnh các mô hình LLaMA 7B-65B và OPT 7B-13B trên tập dữ liệu 
SAMSum (Gliwa et al., 2019), bao gồm 14,732 cặp (văn bản, tóm tắt) huấn luyện và 819 cặp kiểm tra. 
Phương pháp của chúng tôi hoàn toàn phản ánh việc đánh giá các mô hình kiểu GPT được tinh chỉnh 
sử dụng LoRA (Hu et al., 2022). Chúng tôi đánh giá chất lượng tóm tắt sử dụng ROUGE-1/2/L; 
chúng tôi bao gồm các baseline GPT-3 từ Hu et al. (2022).

Kết quả. Các mô hình LLaMA 65B 4-bit của chúng tôi được tinh chỉnh với ModuLoRA vượt trội hơn 
baseline GPT-3 và thậm chí đạt hiệu suất hiện đại nhất mới trên tập dữ liệu này (Bảng 3). 
Quan trọng, ModuLoRA thể hiện cải thiện hiệu suất so với các phương pháp QLoRA 4-bit và 
BitsAndBytes 8-bit. Trong dải kích thước mô hình 7B đến 65B, các mô hình ModuLoRA (3-bit hoặc 4-bit) 
vượt trội hơn các LoRA 8-bit trong BitsAndBytes và LLM.int8() và các LoRA 4-bit trong BitsAndBytes 
và QLoRA. Các mô hình ModuLoRA (2-bit) phù hợp với hiệu suất của các LoRA 8-bit trong BitsAndBytes 
và LLM.int8() và các LoRA 4-bit trong BitsAndBytes và QLoRA. Chúng tôi lập luận rằng một lược đồ 
lượng tử hóa độ chính xác thấp hướng dữ liệu có thể cải thiện so với một bộ lượng tử hóa zero-shot 
độ chính xác cao hơn như LLM.int8(). Chuyển từ 4-bit sang 3-bit, và sau đó từ 3-bit sang 2-bit, 
độ chính xác trong ModuLoRA chỉ giảm ROUGE khoảng 1%.

Lượng tử hóa Round-to-Nearest Chúng tôi cũng thực hiện một ablation trong đó chúng tôi thay thế 
bộ lượng tử hóa OPTQ bằng phương pháp round-to-nearest (RTN) (Bảng 4); OPTQ hoạt động tốt hơn RTN, 
làm nổi bật tầm quan trọng của các bộ lượng tử hóa tiên tiến.
7

--- TRANG 8 ---
Baselines
Models Finetuning Adaptation # Trainable Parameters SAMSum (Rouge 1/2/L)
GPT-3 Full Finetuning 175,255.8M 52.0 / 28.0 / 44.5
GPT-3 Adapter 40.1M 53.2 / 29.0 / 45.1
GPT-3 LoRA 4.7M 53.8 / 29.8 / 45.9
Pegasus SliC 2B 54.4 / 29.9 / 45.9
LLAMA Finetuning Quantizer 7B 13B 30B 65B
LLMTools (2-bit) QuIP# (E8) 51.3 / 27.3 / 43.7 52.3 / 29.0 / 45.0 53.3 / 30.2 / 46.0 54.0/ 30.6 / 46.2
LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 52.4 / 29.6 / 45.1 53.6 / 30.8 / 46.3 54.1 / 30.9 / 46.5
LLMTools (4-bit) OPTQ 51.7 / 28.3 / 44.4 53.2 / 30.2 / 46.1 53.9 / 31.2 / 46.9 54.8 / 31.3 / 47.2
Bits&Bytes (4-bit) QLoRA 51.6 / 28.3 / 44.5 51.3 / 28.1 / 44.1 53.0 / 30.2 / 45.7 53.8 / 30.5 / 45.9
Bits&Bytes (8-bit) LLM.int8() 51.9 / 28.1 / 44.5 51.3 / 28.2 / 43.6 50.8 / 28.4 / 44.1 53.9 / 30.4 / 46.3

Bảng 3: Tóm tắt trừu tượng trên tập dữ liệu SAMSum được đánh giá bằng ROUGE 1/2/L. Mô hình LLAMA-65B-4bit 
của chúng tôi đạt được điểm ROUGE hiện đại nhất. Tất cả các chỉ số có khoảng tin cậy ±0.5.

SAMSum Performance Quantizer 7B 13B
LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 / 44.2 52.4 / 29.6 / 45.1 / 45.1
RTN 50.7 / 27.2 / 43.6 / 43.6 51.1 / 28.7 / 44.3 / 44.5
LLMTools (4-bit) OPTQ 51.7 / 28.3 / 44.4 / 44.4 53.2 / 30.2 / 46.1 / 46.1
RTN 51.2 / 28.5 / 44.2 / 44.2 52.5 / 29.9 / 45.5 / 45.5

Bảng 4: Lượng tử hóa OPTQ và RTN với các kích thước mô hình LLaMA khác nhau trên tập dữ liệu SAMSum. 
Việc đánh giá được thực hiện trên ROUGE 1/2/L/L-Sum.

Các họ Mô hình Khác Chúng tôi cũng áp dụng LLMTools cho họ mô hình OPT (Zhang et al., 2022) (Bảng 5). 
Mặc dù các mô hình này hoạt động kém hơn LLaMA, ModuLoRA phù hợp hoặc vượt trội hơn tinh chỉnh 
4-bit và 8-bit tốn bộ nhớ hơn, điều này nhất quán với kết quả của chúng tôi trên LLaMA.

OPT Finetuning Quantizer 13B 30B
LLMTools (3-bit) OPTQ 48.8 / 26.7 / 41.9 49.9 / 27.1 / 42.5
LLMTools (4-bit) OPTQ 49.3 / 26.8 / 42.0 49.6 / 27.1 / 42.4
Bits&Bytes (4-bit) QLoRA 49.2 / 27.0 / 42.1 49.9 / 27.0 / 42.5
Bits&Bytes (8-bit) LLM.int8() 48.8 / 26.5 / 41.7 49.3 / 27.1 / 42.3

Bảng 5: Tóm tắt trừu tượng với các mô hình OPT trên tập dữ liệu SAMSum. ModuLoRA ở độ chính xác 3-bit 
và 4-bit phù hợp với điểm ROUGE 1/2/L của các baseline 4-bit và 8-bit. Tất cả các chỉ số có khoảng tin cậy ±0.5.

4.5 Làm theo Hướng dẫn
Dữ liệu & Chỉ số. Chúng tôi tinh chỉnh các mô hình LLaMA 7B-65B trên tập dữ liệu Alpaca (Taori et al., 2023), 
bao gồm 52,000 hướng dẫn, cũng như trên tập dữ liệu CodaAlpaca (Chaudhary, 2023), bao gồm 20K hướng dẫn 
tạo mã (xem Bảng 9). Chúng tôi đánh giá các mô hình Alpaca được tinh chỉnh hướng dẫn của chúng tôi 
trên điểm chuẩn BigBench-Hard (BBH) (Suzgun et al., 2022), bao gồm 23 tác vụ thách thức mà LLMs 
không vượt qua hiệu suất con người. Chúng tôi đánh giá hiệu suất 3-shot thông qua prompting "answer-only" 
và sử dụng độ chính xác khớp chính xác làm tiêu chuẩn đo lường, kiểm tra trên 6,511 mẫu (~1.5k token mỗi mẫu). 
Chúng tôi bao gồm các baseline Flan và LLaMA từ Chia et al. (2023).

Kết quả. Chúng tôi thấy rằng hiệu suất 2-bit, 3-bit và 4-bit chỉ giảm nhẹ so với các mô hình 8-bit. 
Quan trọng, các mô hình 2-bit, mặc dù nén tích cực, phù hợp với hiệu suất của QLoRA 4-bit 
ở tất cả các kích thước mô hình. Các mô hình 65B 4-bit và 3-bit vượt trội hơn các mô hình 30B 8-bit, 
mặc dù sử dụng ít tổng bit hơn. Hơn nữa, ModuLoRA 4-bit so sánh tốt với QLoRA 4-bit, 
và cung cấp cải thiện hiệu suất nhất quán, đặc biệt là ở các kích thước mô hình nhỏ hơn, 
nơi lượng tử hóa tinh vi nên mang lại lợi ích lớn hơn. Điều này càng làm nổi bật lợi ích 
của các phương pháp lượng tử hóa một lần. Phụ lục B cũng báo cáo các thí nghiệm trên tập dữ liệu CodeAlpaca.
8

--- TRANG 9 ---
Baselines
Model Method Quantizer BASE (250M) L (780M) XL (3B) XXL (11B)
FLAN-T5 No Finetuning None 30.8 30.3 39.9 47.4
Model Methods Quantizer 7B 13B 30B 65B
LLaMA LLMTools (2-bit) QuIP# (E8) 30.8 ±0.5 33.8 ±0.5 38.3 ±0.6 43.5 ±0.5
LLMTools (3-bit) OPTQ 31.1 ±0.4 35.3 ±0.2 37.2 ±0.6 43.3 ±0.4
LLMTools (4-bit) OPTQ 33.1 ±0.2 36.2 ±0.4 40.4 ±0.2 43.7 ±0.4
Bits&Bytes (4-bit) QLoRA 31.9 ±0.1 35.4 ±0.2 39.0 ±0.4 43.5 ±0.5
Bits&Bytes (8-bit) LLM.int8() 33.3 ±0.3 36.8 ±0.2 39.1 ±0.5 44.7 ±0.4
No Finetuning None 30.9 37.1 39.3 42.6

Bảng 6: Các mô hình được tinh chỉnh hướng dẫn được đánh giá trên BigBench Hard (BBH). Chúng tôi tinh chỉnh 
các mô hình LLaMA trên tập dữ liệu Alpaca từ 2 đến 8 bit. Chúng tôi cung cấp độ lệch chuẩn chính xác ở đây.

4.6 Yêu cầu Bộ nhớ
Chúng tôi hiển thị bộ nhớ cần thiết để thực hiện tinh chỉnh trên MNLI-M cho các kích thước mô hình LLaMA 
khác nhau trong bảng 7. ModuLoRA giảm thiểu đáng kể các yêu cầu bộ nhớ để tinh chỉnh các mô hình này. 
Chúng tôi vẽ biểu đồ các yêu cầu bộ nhớ trong hình 2 để trực quan hóa tốt hơn. Khi kích thước mô hình 
tăng lên 65B, ModuLoRA chỉ sử dụng khoảng 6% bộ nhớ để chạy phương pháp tinh chỉnh tiết kiệm bộ nhớ LoRA. 
Như bảng và hình minh họa, với ModuLoRA có thể không chỉ chạy suy luận mà còn tinh chỉnh mô hình 65B 
trên một GPU 24GB duy nhất. Để tạo ra bảng này, chúng tôi chạy các lượt truyền xuôi/ngược bất khả tri 
bộ lượng tử hóa của chúng tôi cho toàn bộ dải kích thước mô hình LLaMA với kích thước batch 1 và 
độ dài chuỗi tối đa 128 trên MNLI-m.

LLaMA Finetuning 7B 13B 30B 65B
LLMTools (2-bit) 3.2 GB 5.4 GB 11.4 GB 21.8 GB
QLoRA (4-bit) 5.2 GB 8.6 GB 19.5 GB 36.7 GB
Full Precision (LoRA) 38.4 GB 73.9 GB 183.3 GB 360.4 GB

Bảng 7: Yêu cầu bộ nhớ để tinh chỉnh các mô hình LLaMA trên MNLI-M với kích thước batch 1 và độ dài chuỗi tối đa 128. 
Để so sánh, chúng tôi bao gồm các yêu cầu bộ nhớ để tinh chỉnh trên LoRA và QLoRA.

5 Thảo luận
5.1 So sánh với Công trình liên quan

7B 13B 33B 65B0GB10GB100GB400GB
351122
5920373874183360
Model Parameter Size (Billion)Required Memory (Gigabyte)LLMTools (2-bit)
QLoRA (4-bit)
Full precision

Hình 2: Trực quan hóa các yêu cầu bộ nhớ với các phương pháp khác nhau.

So sánh với QLoRA Trong công trình đồng thời,
Dettmers et al. (2023) đã đề xuất QLoRA, một phương pháp 
liên quan để tinh chỉnh một LLM lượng tử hóa. Chúng tôi làm nổi bật 
các khác biệt về phương pháp và thực nghiệm dưới đây. Từ góc độ 
phương pháp, ModuLoRA tích hợp với một mô-đun lượng tử hóa 
hộp đen do người dùng chỉ định. Trong các thí nghiệm của chúng tôi, 
chúng tôi thấy rằng sử dụng một bộ lượng tử hóa hướng dữ liệu 
tinh vi như OPTQ cải thiện hiệu suất so với các chiến lược 
zero-shot đơn giản hơn, ví dụ như baseline round-to-nearest. 
Khác với ModuLoRA, QLoRA định nghĩa một phương pháp 
lượng tử hóa tương tự RTN, nhưng cũng giới thiệu một 
quy trình đóng gói chuyên biệt, lượng tử hóa số không 
và tỷ lệ, và các đổi mới khác.
9

--- TRANG 10 ---
Từ góc độ thí nghiệm và khả năng, việc tích hợp với OPTQ cho phép ModuLoRA tinh chỉnh các mô hình 
lượng tử hóa 2-bit và 3-bit, điều mà QLoRA không thể làm. Cuối cùng, chúng tôi xác định các cài đặt 
trong đó ModuLoRA mang lại LLMs có hiệu suất tốt hơn so với LLMs từ QLoRA; khoảng cách này 
có thể do việc sử dụng các bộ lượng tử hóa cải tiến.

So sánh với Các phương pháp Tinh chỉnh Hiệu quả Tham số Khác Các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) 
gần đây đã bao gồm một loạt các kỹ thuật như prompt tuning (Lester et al., 2021; Li & Liang, 2021; 
Qin & Eisner, 2021; Liu et al., 2022b), sửa đổi các đầu vào lớp embedding (An et al., 2022) 
hoặc các trạng thái ẩn (Liu et al., 2022a), bao gồm các lớp đầy đủ (Houlsby et al., 2019), 
chỉ tinh chỉnh bias (Zaken et al., 2021), và các phương pháp khác (Sung et al., 2021; 
Karimi Mahabadi et al., 2021). Một nhược điểm quan trọng của các phương pháp này là cần lưu trữ 
trong bộ nhớ một lượng đáng kể các tham số mô hình cơ sở đông lạnh. Điều này giới hạn khả năng 
tinh chỉnh các LLMs lớn nhất trên GPU tiêu dùng, một giới hạn mà chúng tôi giải quyết.

5.2 Chạy LLMs trên GPU Tiêu dùng
Các thuật toán LLM Hiệu quả Các yêu cầu tính toán của các mạng nơ-ron sâu hiện đại thúc đẩy 
một loạt các thuật toán học máy hiệu quả. Các phương pháp lượng tử hóa giảm số bit cần thiết 
để lưu trữ trọng số (Dong et al., 2019; 2020; Hubara et al., 2021; Li et al., 2021; Yao et al., 2021), 
bao gồm thông qua các phương pháp thích ứng (Nagel et al., 2020). SmoothQuant (Xiao et al., 2023) 
tái tỷ lệ giữa kích hoạt và trọng số để loại bỏ các ngoại lệ từ kích hoạt và làm cho lượng tử hóa 
tổng thể dễ dàng hơn. ZeroQuant (Yao et al., 2022) đề xuất một phương pháp chưng cất kiến thức 
theo từng lớp. LLM.int8() (Dettmers et al., 2022) phân tích các phép nhân ma trận thành đa số 
các phép toán 8 bit và thiểu số các phép toán 16 bit. LUT-GEMM (Park et al., 2023) thiết kế 
các kernel để tăng tốc các phép nhân ma trận lượng tử hóa. RPTQ (Yuan et al., 2023) sắp xếp lại 
các kích hoạt và lượng tử hóa chúng theo nhóm, giảm tác động của sự khác biệt phạm vi giữa các kênh.

Chạy LLMs trên GPU Tiêu dùng Các phương pháp của chúng tôi cho độ chính xác 3-bit và 4-bit 
cho phép tinh chỉnh LLM 65B trên GPU 48GB, và LLM 30B trên GPU 24GB. Ngoài ra, phương pháp 
2-bit của chúng tôi cho phép tinh chỉnh LLM 65B trên GPU 24GB, làm cho việc tinh chỉnh LLMs 
trở nên có thể trên phần cứng tiêu dùng. Hơn nữa, việc khớp toàn bộ LLM trên GPU mở khóa 
tính song song dữ liệu, hiệu quả hơn tính song song mô hình. Các phương pháp lượng tử hóa 
8-bit trước đó yêu cầu GPU 96GB để hoàn toàn khớp mô hình 65B. Tinh chỉnh GPUs trên phần cứng 
tiêu dùng hứa hẹn tăng tốc quá trình lặp mô hình và áp dụng LLMs cho phạm vi miền rộng hơn 
bởi số lượng lớn hơn các chuyên gia.

5.3 LLM Cơ sở Tốt cho Tinh chỉnh là gì?

Models Quantization BBH PPL
LLAMA (13B) 3-bit 35.3 6.63
4-bit 36.2 5.36
LLAMA (65B) 3-bit 43.3 5.04
4-bit 43.7 3.84

Bảng 8: BBH vs. PPL

Thước đo truyền thống của một LLM cơ sở là perplexity. 
Trong bảng liền kề, chúng tôi báo cáo perplexity (PPL) 
LLaMA trên Wiki2 cũng như hiệu suất tinh chỉnh trên BBH. 
Thú vị, tương quan không hoàn hảo: khoảng cách lớn trong PPL 
thừa nhận khoảng cách nhỏ trong BBH. Điều này đặt câu hỏi 
về đánh giá LLM khi mục tiêu là tinh chỉnh, và gợi ý 
khám phá các chiến lược huấn luyện mới.

Tổng quát hơn, kết quả của chúng tôi cung cấp bằng chứng thực nghiệm rằng hiệu suất cao trên các tác vụ 
hạ nguồn có thể đạt được với một LLM lượng tử hóa nhỏ hơn so với những gì nghĩ trước đây. 
Trong khi các phương pháp hiện tại (ví dụ, LLM.int8()+LoRA; Dettmers et al. (2022)) hoạt động 
ở 8 bit, chúng tôi thấy rằng tinh chỉnh 2-bit, 3-bit hoặc 4-bit mang lại kết quả tốt nhất 
cho một ngân sách bit cố định. Ví dụ, chúng tôi thấy rằng các mô hình 65B 4-bit và 3-bit 
vượt trội hơn các mô hình 30B 8-bit và 16-bit trên các tác vụ làm theo hướng dẫn. Trên tác vụ 
tóm tắt SAMSum, chúng tôi thấy rằng các mô hình 3-bit có thể đạt được điểm ROUGE hiện đại nhất mới, 
và các mô hình 2-bit phù hợp với hiệu suất của các mô hình 8-bit lượng tử hóa với LLM.int8(). 
Hiệu suất cao của các mô hình độ chính xác thấp này gợi ý rằng hiệu suất tinh chỉnh cạnh tranh 
có thể đạt được trên bất kỳ LLM cơ sở lượng tử hóa nào với độ chính xác x-bit, miễn là LLM 
thể hiện hiệu suất tốt đáng kể từ đầu.
10

--- TRANG 11 ---
5.4 Hạn chế
Một lợi thế của LoRA là nó có chi phí suy luận thấp, vì bộ chuyển đổi thứ hạng thấp có thể được 
thêm vào ma trận trọng số độ chính xác đầy đủ khi triển khai. Một hạn chế của ModuLoRA là 
nó không chia sẻ lợi thế này so với mô hình lượng tử hóa hộp đen: bộ chuyển đổi thứ hạng thấp 
không thể được thêm vào ma trận trọng số một cách tầm thường vì ma trận trọng số được lượng tử hóa 
trong khi bộ chuyển đổi thì không. Vì vậy, ma trận trọng số và bộ chuyển đổi không thể được 
hợp nhất dễ dàng, và một triển khai như trong Hình 1 được yêu cầu tại thời điểm suy luận. 
Một hạn chế thứ hai của ModuLoRA là việc làm cho tinh chỉnh trở nên khả thi trên phần cứng 
hàng hóa có sẵn rộng rãi có thể làm cho tinh chỉnh trở nên quá dễ dàng, đặt ra các vấn đề 
tiềm ẩn liên quan đến an toàn LLM. Một hạn chế khác của ModuLoRA là các mô hình lớn nhất 
được sử dụng ngày nay (ví dụ GPT-4) có thể có đến 1 nghìn tỷ tham số, và thậm chí ở mức tối thiểu 
1 bit trên mỗi tham số điều này vẫn sẽ chiếm 125 GB, vượt quá bộ nhớ trên GPU hàng hóa: 
do đó một ứng dụng đơn giản của ModuLoRA sẽ không thể làm cho các mô hình quy mô lớn nhất 
này có thể tinh chỉnh được trên phần cứng hàng hóa.

6 Kết luận
Tinh chỉnh các mô hình ngôn ngữ lớn thường yêu cầu tài nguyên phần cứng và lưu trữ đáng kể. 
Phương pháp của chúng tôi, ModuLoRA, cho phép tinh chỉnh 2-bit các mô hình 65B trên một GPU 
tiêu dùng 24GB duy nhất và cũng hỗ trợ tinh chỉnh 3-bit và 4-bit của các mô hình tương tự 
sử dụng một GPU 48GB duy nhất. Cốt lõi của phương pháp chúng tôi là một lượt truyền ngược 
bất khả tri lượng tử hóa đơn giản cho phép tích hợp các bộ chuyển đổi thứ hạng thấp với 
các trọng số LLM đông lạnh thu được từ một mô-đun lượng tử hóa do người dùng định nghĩa. 
Bằng cách tích hợp với các bộ lượng tử hóa hiện đại, ModuLoRA đạt được hiệu suất hiện đại 
nhất so với cả các kỹ thuật tinh chỉnh hiệu quả tham số và tinh chỉnh đầy đủ.

Tính linh hoạt và hiệu suất cạnh tranh của ModuLoRA làm cho tinh chỉnh trở nên dễ tiếp cận 
và hiệu quả chi phí hơn trong môi trường hạn chế tài nguyên. Điều này hỗ trợ phát triển 
mô hình mã nguồn mở và tạo điều kiện thuận lợi cho nghiên cứu khoa học. Tổng quát hơn, 
chúng tôi tin rằng ModuLoRA sẽ giúp dân chủ hóa quyền truy cập vào các mô hình ngôn ngữ lớn 
và làm cho chúng có sẵn cho đối tượng rộng hơn.
11

--- TRANG 12 ---
Tài liệu tham khảo
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, và
Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint
arXiv:2203.03131, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và et. al. Language models are few-shot
learners. Trong Conference on Neural Information Processing Systems, 2020.

Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation.
https://github.com/sahil280114/codealpaca, 2023.

Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, và Christopher De Sa. Quip: 2-bit quantization of large
language models with guarantees, 2023.

Yew Ken Chia, Pengfei Hong, Lidong Bing, và Soujanya Poria. Instructeval: Towards holistic evaluation
of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-ﬁnetuned language models. arXiv
preprint arXiv:2210.11416, 2022.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication
for transformers at scale. Trong Conference on Neural Information Processing Systems, 2022.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Eﬃcient ﬁnetuning of
quantized llms. arXiv preprint arXiv:2305.14314, 2023.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, và Kurt Keutzer. Hawq: Hessian aware
quantization of neural networks with mixed-precision. Trong International Conference on Computer Vision,
2019.

Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, và Kurt Keutzer. Hawq-
v2: Hessian aware trace-weighted quantization of neural networks. Trong Conference on Neural Information
Processing Systems, 2020.

Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, và Dan Alistarh. Optq: Accurate quantization for genera-
tive pre-trained transformers. Trong International Conference on Learning Representations, 2023.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, và Aleksander Wawer. SAMSum corpus: A human-annotated
dialogue dataset for abstractive summarization. Trong Proceedings of the 2nd Workshop on New Frontiers in
Summarization, pp. 70–79, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, và Sylvain Gelly. Parameter-eﬃcient transfer learning for nlp. Trong International
Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.

Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.
Lora: Low-rank adaptation of large language models. Trong International Conference on Learning Represen-
tations, 2022.

Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, và Daniel Soudry. Accurate post training quanti-
zation with small calibration sets. Trong International Conference on Machine Learning. PMLR, 2021.

Rabeeh Karimi Mahabadi, James Henderson, và Sebastian Ruder. Compacter: Eﬃcient low-rank hyper-
complex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021.
12

--- TRANG 13 ---
Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-eﬃcient prompt tuning. Trong
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059,
Online và Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.

Xiang Lisa Li và Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. Trong Pro-
ceedings of the 59th Annual Meeting of the Association for Computational Linguistics và the 11th In-
ternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597,
Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL
https://aclanthology.org/2021.acl-long.353.

Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, và Shi
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. Trong International
Conference on Learning Representations, 2021.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin A
Raﬀel. Few-shot parameter-eﬃcient ﬁne-tuning is better và cheaper than in-context learning. Advances
in Neural Information Processing Systems, 35:1950–1965, 2022a.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, và Graham Neubig. Pre-train,
prompt, và predict: A systematic survey of prompting methods in natural language processing. ACM
Computing Surveys, 55(9):1–35, 2023a.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, và Jie Tang. P-tuning: Prompt
tuning can be comparable to ﬁne-tuning across scales và tasks. Trong Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61–68, 2022b.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, và Jie Tang. Gpt understands,
too. AI Open, 2023b.

Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, và Sayak Paul. Peft: State-of-the-
art parameter-eﬃcient ﬁne-tuning methods. https://github.com/huggingface/peft, 2022.

Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, và Tijmen Blankevoort. Up or down?
adaptive rounding for post-training quantization. Trong International Conference on Machine Learning, pp.
7197–7206. PMLR, 2020.

Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon,
Byeongwook Kim, Youngjoo Lee, và Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based
on luts for eﬃcient inference in large-scale generative language models. arXiv preprint arXiv:2206.09557,
2023.

Guanghui Qin và Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv
preprint arXiv:2104.06599, 2021.

Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,
2020.

Sylvestre-Alvise Rebuﬃ, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental
classiﬁer và representation learning. Trong Proceedings of the IEEE conference on Computer Vision và
Pattern Recognition, pp. 2001–2010, 2017.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access
multilingual language model. arXiv preprint arXiv:2211.05100, 2023.

Yi-Lin Sung, Varun Nair, và Colin A Raﬀel. Training neural networks with ﬁxed sparse masks. Advances
in Neural Information Processing Systems, 34:24193–24205, 2021.
13

--- TRANG 14 ---
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks và
whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open và eﬃcient foundation
language models. arXiv preprint arXiv:2302.13971, 2023.

Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, và Chris De sa. Quip#: Quip with lattice
codebooks. https://cornell-relaxml.github.io/quip-sharp/, 2023.

Adina Williams, Nikita Nangia, và Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. Trong Proceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-
ume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018. URL
http://aclweb.org/anthology/N18-1101.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, và Song Han. Smoothquant: Accurate
và eﬃcient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2023.

Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang,
Yida Wang, Michael W. Mahoney, và Kurt Keutzer. Hawq-v3: Dyadic neural network quantization. Trong
International Conference on Machine Learning. PMLR, 2021.

Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, và Yuxiong He. Zeroquant:
Eﬃcient và aﬀordable post-training quantization for large-scale transformers. Trong Conference on Neural
Information Processing Systems, 2022.

Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu,
Jiaxiang Wu, và Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models.
arXiv preprint arXiv:2304.01089, 2023.

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitﬁt: Simple parameter-eﬃcient ﬁne-tuning for
transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. Opt: Open pre-trained
transformer language models, 2022.
14

--- TRANG 15 ---
A Chi tiết Triển khai Bổ sung
A.1 Cấu hình cho Đánh giá BBH
Chúng tôi đánh giá tập dữ liệu BBH sử dụng các trọng số bộ chuyển đổi LoRA từ huggingface hub 
với các cấu hình khác nhau. Đối với các trọng số bộ chuyển đổi LoRA Bits&Bytes 8-bit (LLM.int8()), 
chúng tôi sử dụng hai nguồn: Alpaca-7B được lấy từ repository 'tloen/alpaca-lora-7b', 
trong khi các trọng số cho Alpaca-13b và 30b được lấy từ 'chansung/alpaca-lora-xxb'. 
Trong trường hợp các trọng số bộ chuyển đổi Bits&Bytes 4-bit (QLoRA), tất cả các cấu hình 
(Alpaca-7B, 13B và 30B)—đều được truy cập đồng nhất từ 'timdettmers/qlora-alpaca-xxb'. 
Lưu ý rằng đối với các trọng số bộ chuyển đổi Bits&Bytes 4-bit (QLoRA) và Bits&Bytes 8-bit 
(LLM.int8()) của mô hình 65B, chúng tôi thu được chúng bằng cách tinh chỉnh mô hình LLaMa 
65B cơ sở trên tập dữ liệu Alpaca sử dụng cùng một bộ siêu tham số như của chúng tôi.

B Thí nghiệm Thực nghiệm Bổ sung
B.1 Thí nghiệm Bổ sung trên Code-Alpaca với LLaMA
Chúng tôi đã thực hiện thí nghiệm bổ sung trên Code-Alpaca (Chaudhary, 2023). Kết quả được hiển thị 
trong Bảng 9. Nhất quán với giả thuyết của chúng tôi, ModuLoRA hoạt động tốt hơn hoặc ít nhất 
là ngang bằng với các mô hình 8-bit độ chính xác cao hơn với cùng số lượng tham số có thể huấn luyện 
và thiết lập.

Code Alpaca Per-
formance 7B 13B 30B 65B
LLMTools (3-bit) 53.6 / 36.3 / 50.7 57.0 / 40.0 / 53.3 58.1 / 40.7 / 54.3 60.0 / 44.1 / 58.8
LLMTools (4-bit) 54.6 / 37.2 / 51.4 57.4 / 40.6 / 54.3 59.0 / 41.4 / 57.5 60.2 / 43.5 / 56.8
Bits&Bytes 8-bit
(LLM.int8()) 54.0 / 36.3 / 50.9 57.7 / 41.3 / 54.9 60.6 / 43.5 / 57.5 61.1 / 44.1 / 58.0

Bảng 9: Các mô hình được tinh chỉnh hướng dẫn được đánh giá bằng ROUGE 1/2/LSum trên Code Alpaca 
ở 3, 4 và 8 bit.

B.2 Độ trễ Tinh chỉnh & Suy luận
Chúng tôi đã thực hiện thí nghiệm để kiểm tra độ trễ tinh chỉnh và suy luận của ModuLoRA.

Tinh chỉnh. Trong quá trình tinh chỉnh, ModuLoRA vượt trội đáng kể so với LoRA độ chính xác đầy đủ 
như hiển thị trong bảng 10, giảm thời gian huấn luyện khoảng 59.3% và sử dụng bộ nhớ 91.5%. 
Hiệu quả trong tốc độ tinh chỉnh này chủ yếu do việc giảm di chuyển dữ liệu trong bộ nhớ GPU.

Suy luận. Trong quá trình suy luận, ModuLoRA có tốc độ hơi thấp hơn so với LoRA và QLoRA 
như hiển thị trong bảng 11. Chúng tôi cho rằng điều này do việc sử dụng các kernel CUDA hiện tại 
chưa được tối ưu hóa như của QLoRA. Lưu ý rằng

Precision LLMTools QLoRA LoRA
(2-bit) (4-bit) (Full Precision)
Seconds/Iteration 0.61 s/it 0.80 s/it 1.50 s/it

Bảng 10: Tốc độ tinh chỉnh cho LLAMA 7B trên 
điểm chuẩn MNLI-m với kích thước batch 1. Chúng tôi 
báo cáo thời gian trung bình để hoàn thành một bước 
cho một mục dữ liệu huấn luyện. Để đảm bảo so sánh 
công bằng, chúng tôi sử dụng một A6000 duy nhất 
để chạy trên cả ba phương pháp.

Precision LLMTools QLoRA LoRA
(2-bit) (4-bit) (Full Precision)
Seconds/Iteration 0.68 s/it 0.52 s/it 0.52 s/it

Bảng 11: Tốc độ suy luận cho LLAMA 7B trên điểm 
chuẩn MNLI-m. Chúng tôi báo cáo thời gian trung bình 
để hoàn thành suy luận cho một mục dữ liệu đánh giá. 
Để đảm bảo so sánh công bằng, chúng tôi sử dụng 
một A6000 duy nhất để chạy trên cả ba phương pháp.
15

--- TRANG 16 ---
C Siêu tham số Sử dụng trong Thí nghiệm
C.1 LLaMA / OPT trên SAMSum
Chúng tôi thiết lập quy trình huấn luyện theo Hu et al. (2022), với sự điều chỉnh cụ thể cho các mô hình ngôn ngữ 
cụ thể của chúng tôi. Để so sánh công bằng với công trình đồng thời QLoRA, chúng tôi sử dụng cùng một 
thiết lập siêu tham số như hiển thị trong Bảng 12. Chúng tôi huấn luyện sử dụng AdamW trong 350 bước 
với kích thước batch 128 mẫu. Chúng tôi báo cáo kết quả trên 3 seed ngẫu nhiên; kết quả cho mỗi lần chạy 
được lấy từ các bước huấn luyện với validation loss thấp nhất.

Dataset Model LLaMA 7B / 13B / 30B / 65B OPT 7B/ 13B / 30B
SAMSum Optimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 16
Evaluation Steps 50
Total # Training Steps 350
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRA Config rq=rv= 8
LoRA α 32
Max Seq. Len 250

Bảng 12: Cấu hình siêu tham số cho ModuLoRA, Q-LoRA trên SAMSum

C.2 LLaMA trên Code-Alpaca & Text-Classification
Chúng tôi lại huấn luyện sử dụng tối ưu hóa AdamW với tỷ lệ warmup 0.06. Chúng tôi tinh chỉnh 
learning rate, kích thước batch, các bước huấn luyện cho mỗi tác vụ. Chúng tôi báo cáo kết quả 
trên 3 seed ngẫu nhiên. Kết quả cho mỗi lần chạy được lấy từ các bước huấn luyện cho validation loss thấp nhất.

Dataset LLaMA Model 13/30/65 B
Text-
Classification Optimizer AdamW
Warmup Ratio 0.06
Batch size 256
Evaluation Batch size 32
Evaluation Steps 100
Total # Training Steps 1000
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRA Config rq=rv= 8
LoRA α 32
Max Seq. Len 128

Bảng 13: Cấu hình siêu tham số cho Mod-
uLoRA, Q-LoRA trên Text-Classification

Dataset LLaMA Model 7/13/30/65 B
Code-
Alpaca Optimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 4
Evaluation Steps 40
Total # Training Steps 120
Learning Rate Schedule Linear
Learning Rate 1e-3
WeightDecay 0.0
LoRA Config rq=rv= 8
LoRA α 32
Max Seq. Len 165

Bảng 14: Cấu hình siêu tham số cho Modu-
LoRA, Q-LoRA trên Alpaca-Code
16

--- TRANG 17 ---
C.3 LLaMA trên MNLI-M
Huấn luyện được thực hiện sử dụng tối ưu hóa AdamW, với tỷ lệ warm up được đặt ở 0.06. 
Chúng tôi tinh chỉnh learning rate, kích thước batch và các bước huấn luyện. Kết quả được báo cáo 
trên ba seed ngẫu nhiên, và cho mỗi lần chạy, chỉ số hiệu suất được lấy từ bước huấn luyện 
với validation loss thấp nhất. Xem Bảng 15 để biết thêm chi tiết về các siêu tham số được sử dụng.

Dataset Model LLaMA 7B / 13B / 30B / 65B
MNLI-M Optimizer AdamW
Warmup Ratio 0.06
Batch size 128
Evaluation Batch size 64
Evaluation Steps 64
Total # Training Epoch 1.0
Learning Rate Schedule Cosine
Learning Rate 1e-3
WeightDecay 0.0
LoRA Config rq=rv= 8
LoRA α 32
Max Seq. Len 128

Bảng 15: Cấu hình siêu tham số cho ModuLoRA, Q-LoRA trên MNLI-M

C.4 LLaMA trên Alpaca cho Đánh giá BBH
Huấn luyện được thực hiện sử dụng tối ưu hóa AdamW, với tỷ lệ warm up được đặt ở 0.06. 
Chúng tôi tinh chỉnh learning rate, kích thước batch và các bước huấn luyện. Kết quả được báo cáo 
trên ba seed ngẫu nhiên. Xem Bảng 16 để biết thêm chi tiết về các siêu tham số được sử dụng.

Dataset Model LLaMA 7B / 13B / 30B / 65B
Alpaca Optimizer AdamW
Warmup Ratio 0.06
Batch size 128
Total # Training Epochs 3
Learning Rate Schedule Linear
Learning Rate 1e-3
WeightDecay 0.0
LoRA Config rq=rv= 8
LoRA α 16
Max Seq. Len 256

Bảng 16: Cấu hình siêu tham số cho ModuLoRA trên Alpaca
17
