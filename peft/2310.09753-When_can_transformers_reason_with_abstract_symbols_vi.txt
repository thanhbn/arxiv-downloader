# 2310.09753.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2310.09753.pdf
# Kích thước tệp: 2820897 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khi nào transformers có thể lý luận với các ký hiệu trừu tượng?
Enric Boix-Adserà*1,2Omid Saremi1Emmanuel Abbe1,3
Samy Bengio1Etai Littwin1Joshua Susskind1
1Apple2MIT3EPFL
eboix@mit.edu,emmanuel.abbe@epfl.ch
{osaremi,bengio,elittwin,jsusskind}@apple.com
17 tháng 4, 2024
Tóm tắt
Chúng tôi nghiên cứu khả năng của các mô hình transformer trong các tác vụ lý luận quan hệ. Trong những tác vụ này, các mô hình được huấn luyện trên một tập hợp các chuỗi mã hóa các quan hệ trừu tượng, và sau đó được kiểm tra ngoài phân phối trên dữ liệu chứa các ký hiệu không xuất hiện trong tập dữ liệu huấn luyện. Chúng tôi chứng minh rằng đối với bất kỳ tác vụ lý luận quan hệ nào trong một họ lớn các tác vụ, transformers học được các quan hệ trừu tượng và tổng quát hóa cho tập kiểm tra khi được huấn luyện bằng gradient descent trên lượng dữ liệu huấn luyện đủ lớn. Điều này trái ngược với các mạng fully-connected cổ điển, mà chúng tôi chứng minh không thể học được cách lý luận. Kết quả của chúng tôi truyền cảm hứng cho việc điều chỉnh kiến trúc transformer chỉ thêm hai tham số có thể huấn luyện cho mỗi head, và chúng tôi chứng minh thực nghiệm cải thiện hiệu quả dữ liệu cho việc học lý luận.

1 Giới thiệu
Khi các mô hình ngôn ngữ lớn (LLMs) được huấn luyện với lượng dữ liệu ngày càng tăng, chúng bắt đầu thể hiện khả năng lý luận toán học [Kap+20; Yua+23]. Tại sao nhiều dữ liệu hơn giúp một LLM học được cách lý luận? Và liệu chúng ta có thể làm cho LLMs hiệu quả hơn về mặt dữ liệu trong việc học lý luận?

Trong bài báo này, chúng tôi nghiên cứu lý luận quan hệ với các ký hiệu trừu tượng, đây là một khả năng cơ bản được giả định là nền tảng cho các khả năng phức tạp hơn trong nhận thức con người [Fod75; New80; SKM84; Mar98; Hol12; Kri+13; WSC20]. Một ví dụ là trong toán học hoặc khoa học máy tính, nơi lý luận quan hệ là cần thiết để phân tích một chứng minh hoặc một chương trình: tên biến là các ký hiệu trừu tượng và chức năng của chứng minh hoặc chương trình chỉ phụ thuộc vào cách chúng liên quan đến nhau chứ không phải vào chính tên biến.

Các đóng góp của chúng tôi có ba phần: (i) chúng tôi hình thức hóa lý luận quan hệ thông qua "template tasks"; (ii) chúng tôi tiến hành phân tích về khi nào transformers có thể học template tasks khi được huấn luyện bằng gradient descent và cho thấy sự khác biệt với các kiến trúc mạng neural fully-connected cổ điển; (iii) chúng tôi đề xuất các điều chỉnh cho transformers nhằm cải thiện hiệu quả dữ liệu cho việc học lý luận.

1.1 Nắm bắt lý luận quan hệ với template tasks
Dựa trên một dòng công việc trong khoa học thần kinh [Mar98; MK16; KRS18; WSC20; Ker+22; Alt+23; WHL23; Gei+23], chúng tôi hình thức hóa một khuôn khổ các tác vụ lý luận được gọi là template tasks.

--- TRANG 2 ---
(a)
123421 (b)
123421
1234? (c)
123421
1234?Hình 1: Các tác vụ từ [Rav38; WSC20] nằm
trong lý thuyết của chúng tôi. Các mạng được huấn luyện với một bảng chữ cái ký hiệu và sau đó được kiểm tra trên các ký hiệu giữ lại. Chi tiết trong Phụ lục A.

Thiết lập hồi quy Trong thiết lập hồi quy, một template task được chỉ định bởi một tập hợp các chuỗi "template" được gán nhãn bằng các số thực, được sử dụng để tạo dữ liệu train và test. Cách đơn giản nhất để mô tả chúng là thông qua một ví dụ. Ví dụ, hãy xem xét các templates
"α=1;β=-1;print( α)"→label=+1 và " α=1;β=-1;print( β)"→label=-1 .(1)

Chúng được sử dụng để tạo ra các tập dữ liệu trong Hình 2, nơi mỗi mẫu (xi, yi)∈ Xk×Y được hình thành bằng cách chọn một template và thay thế các placeholder α, β (mà chúng tôi gọi là "wildcards") bằng tên biến. Việc ghi nhớ dữ liệu huấn luyện là dễ dàng [Zha+21b], nhưng chúng tôi muốn đo lường khả năng lý luận: liệu mô hình có học được cách xử lý tên biến như các ký hiệu trừu tượng, cho phép tổng quát hóa ngoài phân phối huấn luyện? Để đánh giá điều này, chúng tôi áp dụng một thiết lập ngoài phân phối, nơi phân phối dữ liệu train và test khác nhau [Mar98; Abb+23]. Tập dữ liệu test bao gồm các chương trình tương tự, nhưng với tên biến mới chưa từng thấy trong quá trình huấn luyện. Bằng cách kiểm tra trên các ký hiệu chưa thấy trong tập train, chúng tôi đo lường khả năng của một LLM học các quy tắc logic về các quan hệ giữa các ký hiệu. Để thành công, LLM phải suy ra hiệu quả các templates từ dữ liệu huấn luyện, và tại thời điểm test khớp các mẫu với các templates tương ứng để tạo ra nhãn của chúng.

(a) Dữ liệu Train (b) Dữ liệu Test (c) Hiệu suất Transformer
xi yi
a=1;b=-1;print(a) +1
c=1;a=-1;print(a) -1
f=1;c=-1;print(f) +1
h=1;q=-1;print(q) -1
. . . . . .xtest
i ytest
i
R=1;A=-1;print(R) +1
Q=1;V=-1;print(V) -1
. . . . . .

101
102
103
104
Số mẫu huấn luyện0.00.51.01.52.0Mất mát TestTransformer
Transformer + KQ identity

Hình 2: (a,b) Tên biến trong dữ liệu test không bao giờ xuất hiện trong dữ liệu train (được chỉ ra bằng tên chữ thường/chữ hoa). (c) Đáng chú ý, khi kích thước tập huấn luyện tăng, khả năng lý luận của LLM ngoài dữ liệu huấn luyện được cải thiện, khi nó học được sử dụng các quan hệ giữa tên biến để phân loại, thay vì chỉ đơn giản ghi nhớ dữ liệu huấn luyện. Lý thuyết của chúng tôi thúc đẩy một kiến trúc transformer được điều chỉnh (xem Quan sát 1.2), giải quyết tác vụ lý luận với ít dữ liệu huấn luyện hơn. Chi tiết trong Phụ lục A.

Ngoài các tác vụ lập trình như trong Hình 2, khuôn khổ này nắm bắt một số vấn đề tự nhiên:
•Tác vụ giống/khác nhau. Tác vụ lý luận quan hệ đơn giản nhất là khi các templates là " αα" và " αβ" được gán nhãn bằng +1 và −1. Điều này mã hóa việc học phân loại hai ký hiệu là bằng nhau (ví dụ: AA,BB) hoặc khác nhau (ví dụ: AB,BC), ngay cả khi các ký hiệu chưa được thấy trong dữ liệu huấn luyện. Tác vụ này đã được nghiên cứu thực nghiệm trong hành vi động vật [MK16] và trong mạng neural [KRS18; WSC20].

•Bài toán lời. Bài toán lời thường có các khối xây dựng tuân theo các templates đơn giản. Ví dụ, template "If αgives β5γ, how many γdoes βhave?" được gán nhãn bằng +5, có thể tạo ra dữ liệu "If Alice gives Bob 5 oranges, how many oranges does Bob have?" hoặc dữ liệu "If Rob gives Ada 5 apples, how many apples does Ada have?"

--- TRANG 3 ---
•Các bài kiểm tra tâm lý. Các bài kiểm tra tâm lý về lý luận quan hệ, được sử dụng gần đây để thăm dò LLMs [Rav38; WSC20; Alt+23; Ker+22; WHL23; Web+23], thường là các template tasks. Hình 1 minh họa một số ví dụ.

Thiết lập dự đoán token tiếp theo Trong thiết lập dự đoán token tiếp theo, có một lớp phức tạp bổ sung: mỗi mẫu được gán nhãn bằng một ký hiệu. Để LLM tổng quát hóa cho các ký hiệu chưa thấy tại thời điểm train, nó không chỉ phải học theo dõi giá trị được lưu trữ trong một biến, mà còn phải học dự đoán nhãn tại thời điểm test có thể không xuất hiện trong dữ liệu huấn luyện của nó. Ví dụ, các tập dữ liệu train và test trong Hình 3 được tạo ra bởi:
"α="γ";β="δ";print( α)"→label= γvà " α="γ";β="δ";print( β)"→label= δ ,(2)
trong đó α, β, γ, δ là wildcards. Các vấn đề khác được bao phủ bởi những tác vụ này bao gồm:

•Lập trình. Template " print(" α")" được gán nhãn bằng α tạo ra (print("A") ,A) hoặc (print("dog") ,dog), và do đó một LLM học trên tác vụ tương ứng có thể đánh giá mạnh mẽ các câu lệnh print trên các ký hiệu không thấy trong dữ liệu huấn luyện.

•Hàm toán học. Ví dụ, tập hợp các templates {ααα, αβα, ααβ, βαα} được gán nhãn bằng α mã hóa tác vụ xuất ra token đa số trong một chuỗi độ dài 3 với từ vựng gồm hai ký hiệu. Tương tự, đối với chuỗi độ dài k, tác vụ xuất ra phần tử đa số có thể được mã hóa với 2k−1 templates.

(a) Dữ liệu Train (b) Dữ liệu Test (c) Hiệu suất Transformer
xi yi
a="d";b="q";print(a) d
c="r";a="w";print(a) w
f="y";c="u";print(f) y
h="o";q="s";print(q) s
. . . . . .xtest
i ytest
i
R="F";A="Z";print(R) F
Q="B";V="A";print(V) A
. . . . . .

101
102
103
104
105
Số mẫu huấn luyện0.00.51.01.52.0Lỗi TestTransformer
Transformer + KQ,VO identity

Hình 3: (a,b) Các nhãn là ký hiệu. (c) Chúng tôi đề xuất một transformer được điều chỉnh học tác vụ lý luận với ít dữ liệu hơn (xem Quan sát 1.2 và Định lý 1.4). Chi tiết trong Phụ lục A.

1.2 Kết quả chính
Hiện tượng từ Hình 2 và 3 mà chúng tôi tìm cách hiểu là: tại sao hiệu suất ngoài phân phối của kiến trúc transformer được cải thiện khi số lượng mẫu huấn luyện tăng? Chúng tôi phân tích các thiết lập hồi quy và dự đoán token tiếp theo một cách riêng biệt.

(1) MLPs thất bại trong việc tổng quát hóa cho các ký hiệu chưa thấy Một lời phê bình cổ điển về chủ nghĩa kết nối của [Mar98] là các mạng neural không học được lý luận quan hệ khi được huấn luyện. Chúng tôi hỗ trợ lời phê bình này trong Phụ lục I bằng cách chứng minh rằng các kiến trúc MLP cổ điển (hay còn gọi là mạng fully-connected) được huấn luyện bằng SGD hoặc Adam sẽ không tổng quát hóa trong template tasks trên các ký hiệu chưa thấy trong quá trình huấn luyện, ngay cả trong thiết lập hồi quy. Sự thất bại trong lý luận quan hệ này xảy ra bất kể kích thước dữ liệu huấn luyện. Chứng minh sử dụng tính chất bất biến hoán vị của huấn luyện MLP [Ng04; Sha18; LZA20; Abb+22; AB22].

--- TRANG 4 ---
(2) Transformers tổng quát hóa cho các ký hiệu chưa thấy, nhưng cần sự đa dạng dữ liệu lớn Tuy nhiên, chúng tôi chứng minh rằng lời phê bình của [Mar98] không hợp lệ đối với các kiến trúc transformer hiện đại [Vas+17]. Chúng tôi phân tích động lực huấn luyện của một mô hình transformer và thiết lập rằng nó có thể học lý luận quan hệ:

Định lý 1.1 (Định lý 3.4 không chính thức). Đối với bất kỳ template task hồi quy nào, một kiến trúc transformer đủ rộng được huấn luyện bằng gradient flow trên đủ nhiều mẫu sẽ tổng quát hóa trên các ký hiệu chưa thấy.

Ở đây các điểm chính là: (a) Tính toàn cục. Kiến trúc transformer tổng quát hóa trên các ký hiệu chưa thấy trong dữ liệu train bất kể templates nào và bao nhiêu templates được sử dụng để định nghĩa tác vụ lý luận. (b) Số lượng mẫu đủ lớn. Các đảm bảo lý thuyết của chúng tôi yêu cầu kích thước tập dữ liệu huấn luyện phải lớn, và ngay cả đối với các tác vụ rất cơ bản như tác vụ hai template trong Hình 2, việc tổng quát hóa tốt chỉ bắt đầu xảy ra ở một số lượng mẫu huấn luyện rất lớn xét độ đơn giản của tác vụ. Điều này đặt ra câu hỏi về cách có thể cải thiện thiên hướng quy nạp của transformer.

Chứng minh của Định lý 1.1 truyền cảm hứng cho một điều chỉnh tham số hóa giảm thực nghiệm lượng dữ liệu cần thiết xuống một bậc độ lớn. Một attention head transformer tiêu chuẩn nhận đầu vào X∈Rk×demb được cho bởi

smax( XW KWT
QXT)XW VWT
O, (3)

trong đó WK,WQ,WV,WO là các tham số có thể huấn luyện. Điều chỉnh của chúng tôi làm cho transformer dễ dàng hơn trong việc truy cập ma trận incidence XXT∈Rk×k của đầu vào, bất biến với các hoán vị của bảng chữ cái ký hiệu và có thể được sử dụng để giải quyết tác vụ lý luận quan hệ:

Quan sát 1.2. Thêm một tham số có thể huấn luyện a vào mỗi attention head sao cho WKWT
Q được thay thế bằng WKWT
Q+aI cải thiện hiệu quả dữ liệu của transformers trên template tasks.

(3) Transformers thất bại trong việc sao chép các ký hiệu chưa thấy Câu chuyện hơi khác một chút đối với các tác vụ dự đoán token tiếp theo, do nút thắt cổ chai của việc học xuất ra một ký hiệu chưa từng thấy trong tập dữ liệu huấn luyện. Hiệu suất của Transformers giảm khi mô hình lớn lên (một quy luật "inverse scaling" [McK+23]). Các transformer lớn thất bại ngay cả đối với tác vụ sao chép đầu vào.

Định lý 1.3 (Định lý 4.1 không chính thức). Transformers với chiều embedding lớn thất bại trong việc tổng quát hóa trên các ký hiệu chưa thấy cho tác vụ sao chép xuất ra nhãn " α" trên template " α".

Tuy nhiên, chúng tôi đề xuất thêm một attention-modulated skip connection, điều này sửa chữa sự thất bại này, làm cho transformer dễ dàng học sao chép dữ liệu giữa các residual streams của nó:

Định lý 1.4 (Định lý 4.2 không chính thức). Thêm một tham số có thể huấn luyện b vào mỗi head sao cho WVWT
O được thay thế bằng WVWT
O+bI làm cho transformers tổng quát hóa trên tác vụ của Định lý 1.3.

(4) Thí nghiệm Chúng tôi kết thúc với việc xác thực thực nghiệm các điều chỉnh kiến trúc của chúng tôi, và thấy rằng chúng cải thiện hiệu quả dữ liệu trên các tác vụ lý luận quan hệ lên một bậc độ lớn, và cải thiện hiệu suất mô hình hóa ngôn ngữ khi huấn luyện kiến trúc GPT-2 trên Wikitext.

1.3 Tài liệu liên quan
Một loạt công việc gần đây nghiên cứu liệu và làm thế nào LLMs thực hiện các tác vụ lý luận khác nhau, mỗi tác vụ tập trung vào một thành phần của lý luận: chúng bao gồm nhận biết ngữ pháp phi ngữ cảnh [Zha+23; AL23], học các hàm thưa [Ede+22], học theo cách tổng hợp [Hup+20], tổng quát hóa ngoài phân phối khi học các hàm Boolean [Abb+23], thực hiện số học [Nan+23], học trong ngữ cảnh [Gar+22; Ahn+23; ZFB23], và đánh giá lập chỉ mục [Zha+21a]. Thiết lập của chúng tôi gần nhất với công việc thực nghiệm nghiên cứu mạng neural trên các tác vụ lý luận quan hệ [Gei+23; Web+23].

--- TRANG 5 ---
Ví dụ, bốn tác vụ trong [WSC20], tác vụ matrix digits trong [WHL23], tác vụ trò chơi SET trong [Alt+23], và hầu hết các tác vụ trong [Ker+22] (ngoại trừ các tác vụ trò chơi quan hệ), là các ví dụ về regression template tasks nằm dưới lý thuyết của chúng tôi. Hơn nữa, [KRS18] cho thấy thực nghiệm rằng MLPs thất bại trên tác vụ template giống/khác nhau, và chúng tôi cung cấp một chứng minh cho điều này trong Phụ lục I. Cũng có một tài liệu về việc điều chỉnh huấn luyện để cải thiện lý luận quan hệ: [Web+20] đề xuất áp dụng Temporal Context Normalization trong quá trình huấn luyện, và [San+17; San+18; PPW18; Sha+20; WSC20; Ker+22; Alt+23] đề xuất các kiến trúc mới. Cuối cùng, một số công việc gần đây trong khả năng diễn giải cơ học tìm kiếm các mạng con trong các mạng được huấn luyện chịu trách nhiệm cho các tác vụ như ràng buộc biến [Ols+22; Dav+23]. Ngược lại, trọng tâm của chúng tôi là chứng minh khi nào kiến trúc transformer học được hoặc thất bại trong việc học, và áp dụng hiểu biết lý thuyết này để cải thiện hiệu quả dữ liệu của nó cho lý luận quan hệ.

2 Định nghĩa chính thức của template tasks
Chúng tôi định nghĩa chính thức regression template tasks. Đối với dự đoán token tiếp theo, xem Phụ lục J.

Định nghĩa 2.1. Một template là một chuỗi z∈(X ∪ W )k, trong đó X là một bảng chữ cái các token, và W là một bảng chữ cái các "wildcards". Một substitution map là một hàm đơn ánh s:W → X. Chúng tôi viết sub(z, s)∈ Xk cho chuỗi mà mỗi wildcard được thay thế bằng token tương ứng: sub(z, s)i=zi nếu zi∈ X, và sub(z, s)i=s(zi) nếu zi∈ W. Chuỗi x∈ Xk khớp với template z nếu x= sub( z, s) cho một số substitution map s và cũng s(W)∩ {zi}i∈[k]=∅: tức là, các token được thay thế không xuất hiện sẵn trong template z.

Ví dụ Sử dụng các chữ cái Hy Lạp để biểu thị wildcards và các chữ cái Latin để biểu thị các token thông thường, template " ααβST" khớp với chuỗi "QQRST", nhưng không phải "QQQST" (vì substitution map không đơn ánh) và không phải "QQSST" (vì β được thay thế bằng S đã có trong template).

Dữ liệu huấn luyện của một template task được tạo ra bằng cách chọn một template ngẫu nhiên từ một phân phối, và thay thế các wildcards của nó bằng một substitution map ngẫu nhiên.

Định nghĩa 2.2. Một template data distribution D=D(µtmplt,{µsub,z}z, f∗, σ) được cho bởi
•một template distribution µtmplt được hỗ trợ trên các templates trong (X ∪ W )k,
•đối với mỗi z∈supp( µtmplt), một phân phối µsub,z trên các substitution maps s:W → X,
•template labelling function f∗: supp( µtmplt)→R, và một tham số nhiễu nhãn σ≥0.

Chúng tôi rút một mẫu (x, y) = (sub( z, s), f∗(z) +ξ)∼ D, bằng cách rút một template z∼µtmplt, một substitution map s∼µsub,z, và nhiễu nhãn ξ∼ N(0, σ2).

Cuối cùng, chúng tôi định nghĩa ý nghĩa của việc một mô hình giải quyết template task và tổng quát hóa trên các ký hiệu chưa thấy; cụ thể, mô hình nên xuất ra nhãn đúng cho bất kỳ chuỗi x∈ Xk nào khớp với một template, bất kể chuỗi đó có nằm trong support của phân phối huấn luyện hay không.

Định nghĩa 2.3. Một estimator (ngẫu nhiên) ˆf:Xk→R tổng quát hóa trên các ký hiệu chưa thấy với lỗi (ϵ, δ) nếu điều sau đây đúng. Đối với bất kỳ x∈ Xk nào khớp với một template z∈supp( µtmplt), chúng ta có
(ˆf(x)−f∗(z))2≤ϵ ,
với xác suất ít nhất 1−δ trên tính ngẫu nhiên của estimator ˆf.

--- TRANG 6 ---
Ví dụ Nếu dữ liệu huấn luyện được tạo từ một phân phối đều trên các templates " αα" với nhãn 1 và " αβ" với nhãn -1, thì nó có thể bao gồm các mẫu dữ liệu {(AA,1),(BB,1),(AB,−1),(BA,−1)}. Một estimator tổng quát hóa cho các ký hiệu chưa thấy phải gán nhãn đúng cho chuỗi CC với +1 và chuỗi CD với −1, mặc dù những chuỗi này bao gồm các ký hiệu không xuất hiện trong tập huấn luyện. Đây là một tác vụ lý luận không tầm thường vì nó yêu cầu học sử dụng các quan hệ giữa các ký hiệu để phân loại thay vì danh tính của các ký hiệu.

3 Phân tích cho template tasks trong thiết lập hồi quy
Chúng tôi thiết lập rằng các transformer một lớp có độ rộng đủ lớn tổng quát hóa cho các ký hiệu chưa thấy, khi được huấn luyện với đủ dữ liệu trên regression template tasks. Điều quan trọng cần lưu ý là điều này không đúng cho tất cả các kiến trúc, vì chúng tôi chứng minh trong Phụ lục I rằng các MLP được huấn luyện bằng SGD hoặc Adam sẽ không thành công.

3.1 Transformer random features kernel
Kiến trúc transformer một lớp mà chúng tôi phân tích bao gồm một lớp embedding, một cơ chế multihead attention, một lớp MLP, và một lớp unembedding wU. Điều này được viết toán học trong Phụ lục H. Chúng tôi phân tích việc chỉ huấn luyện lớp cuối wU của transformer, giữ các trọng số khác cố định tại khởi tạo Gaussian ngẫu nhiên của chúng. Đáng ngạc nhiên, mặc dù chúng tôi chỉ huấn luyện lớp cuối của transformer, điều này đủ để đảm bảo tổng quát hóa trên các ký hiệu chưa thấy.

Lấy độ rộng và chiều embedding và head về vô cùng, và kích thước bước về 0, thuật toán huấn luyện SGD với weight decay hội tụ về kernel gradient flow với kernel sau Ktrans trong giới hạn vô cùng rộng, vô cùng nhỏ kích thước bước. Ở đây và trong phần còn lại của bài báo, chúng tôi có thể biểu thị một đầu vào bằng một chuỗi x∈ Xk hoặc một ma trận X∈Rk×m được xây dựng bằng cách xếp chồng các vector one-hot X= [ex1, . . . ,exk]T của các token của chuỗi. ϕ:R→R là lớp kích hoạt MLP, β, γ∈R là các siêu tham số kiểm soát nhiệt độ và độ lớn của các kích hoạt vị trí.

Ktrans(X,Y) =Eu,v[ϕ(u)ϕ(v)]cho u, v∼N(0,Kattn(X,X)Kattn(X,Y)
Kattn(Y,X)Kattn(Y,Y)
)(4)

trong đó Kattn(X,Y) =Em(X),m(Y)[smax( βm(X))T(XYT+γ2I)smax( βm(Y))]
[m(X),m(Y)]∼N(0,XXT+γ2I XYT+γ2I
Y XT+γ2I Y YT+γ2I
).

Hàm được xuất ra bởi kernel gradient flow được biết là có một giải pháp dạng đóng theo các mẫu, kernel, và tham số weight-decay λ, mà chúng tôi nhớ lại trong Mệnh đề 3.1.

Mệnh đề 3.1 (Cách kernel gradient flow tổng quát hóa; xem ví dụ, [Wel13]). Cho các mẫu huấn luyện (X1, y1), . . . , (Xn, yn). Với loss bình phương và ridge-regularization có độ lớn λ, kernel gradient flow với kernel K hội tụ về giải pháp sau
ˆf(X) =yT(ˆK+λI)−1k(X), (5)
trong đó y= [y1, . . . , y n]∈Rn là các nhãn train, ˆK∈Rn×n là ma trận kernel thực nghiệm và có các entries ˆKij=K(Xi,Xj), và k(X)∈Rn có các entries ki(X) =K(Xi,X).

--- TRANG 7 ---
3.2 Transformers tổng quát hóa trên các ký hiệu chưa thấy
Chúng tôi chứng minh rằng transformers sẽ tổng quát hóa ngoài phân phối trên các ký hiệu chưa thấy khi được huấn luyện trên template tasks. Chúng tôi yêu cầu các templates trong phân phối µtmplt phải "disjoint", vì nếu không nhãn đúng cho một chuỗi x không được định nghĩa duy nhất, vì x có thể khớp với nhiều hơn một template:

Định nghĩa 3.2. Hai templates z,z′∈(X ∪ W )k là disjoint nếu không có x∈ Xk nào khớp với cả z và z′.

Hơn nữa, để đảm bảo rằng các mẫu không phải tất cả đều là bản sao của nhau (điều này sẽ không giúp tổng quát hóa), chúng ta phải áp đặt một điều kiện đa dạng trên dữ liệu.

Định nghĩa 3.3. Sự đa dạng dữ liệu được đo bằng ρ= min z∈supp( µtmplt)mint∈X1
Ps∼µsub,z[t∈s(W)].

Khi sự đa dạng dữ liệu ρ lớn, thì không có token nào có khả năng được thay thế cao hơn nhiều so với các token khác. Nếu ρ có thứ tự như số lượng mẫu n, thì hầu hết các cặp mẫu dữ liệu sẽ không bằng nhau.

Định lý 3.4 (Transformers tổng quát hóa trên các ký hiệu chưa thấy). Cho µtmplt được hỗ trợ trên một tập hữu hạn các templates đôi một disjoint kết thúc bằng các token [CLS]. Khi đó, đối với hầu hết bất kỳ tham số β, γ, b1, b2 (ngoại trừ một tập có độ đo Lebesgue bằng không), transformer random features với ϕ(t) = cos( b1t+b2) tổng quát hóa trên các ký hiệu chưa thấy.¹ Chính thức, có các hằng số c, C > 0 và tham số ridge regularization λ >0 chỉ phụ thuộc vào β, γ, b1, b2, µtmplt, f∗, σ, sao cho đối với bất kỳ x nào khớp với một template z∈supp( µtmplt) thì kernel ridge regression estimator ˆf trong (5) với kernel Ktrans thỏa mãn

|ˆf(x)−f∗(z)| ≤Cp
log(1/δ)/n+Cp
1/ρ ,

với xác suất ít nhất 1−δ−exp(−cn) trên các mẫu ngẫu nhiên.

Số hạng đầu tiên là do nhiễu có thể có trong các nhãn. Số hạng thứ hai định lượng lượng đa dạng mẫu trong dữ liệu. Cả sự đa dạng mẫu và số lượng mẫu đều phải tiến về vô cùng để có đảm bảo lỗi nhỏ tùy ý.

Phác thảo chứng minh (1) Trong Lemma 3.5 chúng tôi thiết lập một điều kiện đủ để kernel ridge regression tổng quát hóa trên các ký hiệu chưa thấy. (2) Chúng tôi chứng minh rằng Ktrans thỏa mãn nó.

(1) Điều kiện đủ. Cho µtmplt được hỗ trợ trên các templates z1, . . . ,zr. Cho R=∪i∈[k],j∈[r]{zj,i} là các token xuất hiện trong các templates. Cho [n] =I1⊔ I2⊔ ··· ⊔ In là phân hoạch của các mẫu sao cho nếu a∈ Ij thì mẫu (xa, ya) được rút bằng cách thay thế các wildcards của template zj. Hai mẫu xa,xb được rút từ cùng một template zj có thể cách xa nhau khi được đo bằng kernel: tức là, tích vô hướng kernel K(xa,xb) có thể nhỏ. Tuy nhiên, những mẫu này sẽ có mối quan hệ tương tự với hầu hết các mẫu khác:

K(xa,xi) =K(xb,xi) cho hầu hết i∈[n]. (6)

Cụ thể, nếu các wildcards của xa,xb và xi được thay thế bằng các tập token disjoint không xuất hiện trong các templates, thì (6) đúng. Do đó, khi sự đa dạng mẫu ρ tăng, ma trận kernel thực nghiệm ˆK trở nên gần như có cấu trúc khối với các khối Ij× Ij′. Đối với hầu hết các mẫu xa,xb tương ứng với template zj, và hầu hết xa′,xb′ tương ứng với template zj′ chúng ta có

K(xa,xa′) =K(xb,xb′) =K(sub(zj, s),sub(zj′, s′)) := Nj,j′, (7)

¹Chúng tôi phân tích hàm kích hoạt cosine đã dịch chuyển và tỷ lệ ϕ(t) = cos( b1t+b2) vì lý do kỹ thuật thuận tiện, nhưng phỏng đoán rằng hầu hết các hàm kích hoạt không đa thức nên thành công.

--- TRANG 8 ---
ˆK=I1I2
N=[K(aa,bb)K(aa,bc)K(aa,bc)K(ab,cd)]
I1
I2∈Rn×n, N=K(AA, BB )K(AA, BC )
K(BC, AA )K(AB, CD )
=
N=[K(aa,bb)K(aa,bc)K(aa,bc)K(ab,cd)] ∈R2×2

Hình 4: Minh họa cấu trúc của ˆK và N cho tác vụ giống/khác nhau, có r= 2 templates z1=αα và z2=αβ. Khi sự đa dạng mẫu ρ tăng và số lượng mẫu n tăng, ma trận kernel thực nghiệm ˆK∈Rn×n trở nên gần như có cấu trúc khối (r×r), và trong mỗi khối hầu hết các entries được cho bởi N∈Rr×r; các ngoại lệ khi điều này không đúng, bao gồm các đường chéo, được vẽ bằng màu đen. Hơn nữa, phổ của ˆK ngày càng được xác định bởi phổ của N, và nếu N không suy biến thì không gian eigen trên ngày càng thẳng hàng với span của các vector chỉ thị trên I1, . . . ,Ir.

trong đó s, s′:W → X là các substitution maps thỏa mãn
s(W)∩s′(W) = 0 và s(W)∩ R=s′(W)∩ R=∅. (8)

Người ta có thể kiểm tra rằng (7) và (8) định nghĩa duy nhất một ma trận N∈Rr×r cho các entries trong các khối của ˆK, với một khối cho mỗi cặp templates.² Xem Hình 4.

Nếu ma trận N không suy biến và số lượng mẫu lớn, thì span của r eigenvectors trên của ˆK sẽ thẳng hàng với span của các vector chỉ thị trên các tập I1, . . . ,Ir. Hơn nữa, khi kiểm tra một chuỗi xtest khớp với template zj, nhưng có thể không xuất hiện trong tập huấn luyện, ta có đối với hầu hết a∈ Ij, chúng ta có

k(xtest) = [K(xtest,x1), . . . , K (xtest,xn)]≈[K(xa,x1), . . . , K (xa,xn)] = ˆKa,:.

Nói cách khác, mối quan hệ tương tự của xtest với các mẫu huấn luyện gần như giống với mối quan hệ tương tự của xa với các mẫu huấn luyện. Vì vậy giải pháp kernel ridge regression (5) gần như bằng trung bình của các nhãn của các mẫu tương ứng với template zj, mà lần lượt gần như bằng nhãn template bằng một bound Chernoff,

yT(ˆK+λI)−1k(xtest)≈1
|Ij|X
a∈Ijyi≈f∗(zj). (9)

Do đó, kernel ridge regression tổng quát hóa trên xtest. Điều quan trọng cần lưu ý là số lượng mẫu cần thiết cho đến khi (9) là một xấp xỉ tốt phụ thuộc vào tính không suy biến của N. Điều này mang lại điều kiện đủ để kernel ridge regression thành công (chứng minh trong Phụ lục C).

Lemma 3.5 (Lemma C.3 không chính thức). Nếu N không suy biến, thì (5) tổng quát hóa cho các ký hiệu chưa thấy.

(2) Ktrans thỏa mãn điều kiện đủ. Bây giờ chúng tôi cho thấy rằng đối với bất kỳ tập hợp các templates disjoint z1, . . . ,zr nào, ma trận Ntrans:=N∈Rr×r được định nghĩa với kernel K=Ktrans là không suy biến. Thách thức là Ktrans không có giải pháp dạng đóng vì kỳ vọng trên các số hạng softmax trong định nghĩa (4) của nó. Do đó, phân tích của chúng tôi về transformer random feature kernel là, theo hiểu biết tốt nhất của chúng tôi, phân tích lý thuyết đầu tiên cho thấy rằng transformer random features học được một lớp hàm không tầm thường của các chuỗi. Chúng tôi tiến hành bằng cách phân tích lớp MLP và lớp attention riêng biệt, quan sát rằng một điều kiện "yếu" trên Kattn có thể được nâng lên thành kết quả "mạnh" rằng Ntrans không suy biến. Trực giác là miễn là Kattn không phải là một kernel rất suy biến, không có khả năng lớp MLP có các triệt tiêu để làm cho Ntrans không suy biến.

²Điều này giả định một tính chất "token-symmetry" của K được thỏa mãn bởi transformers; chi tiết trong chứng minh đầy đủ.

--- TRANG 9 ---
Lemma 3.6 (Tính không suy biến của Ntrans). Giả sử đối với mọi hoán vị không đồng nhất τ∈Sr\ {id},
X
i∈[r]Kattn(sub(zi, s),sub(zi, s′))̸=X
i∈[r]Kattn(sub(zi, s),sub(zτ(i), s′)), (10)
trong đó s, s′ là các substitution maps trong định nghĩa của Ntrans trong (8). Cho hàm kích hoạt của lớp MLP là ϕ(t) = cos( b1t+b2). Khi đó đối với hầu hết bất kỳ lựa chọn b1, b2 nào (ngoại trừ một tập có độ đo Lebesgue bằng không), ma trận Ntrans không suy biến.

Điều này được chứng minh trong Phụ lục E, bằng cách đánh giá một tích phân Gaussian và cho thấy Ntrans có cấu trúc Vandermonde. Mặc dù chúng tôi sử dụng hàm kích hoạt cosine, chúng tôi phỏng đoán rằng kết quả này đúng cho hầu hết các hàm kích hoạt không đa thức. Tiếp theo, chúng tôi chứng minh điều kiện trên Nattn.

Lemma 3.7 (Tính không suy biến của Kattn). Điều kiện (10) đúng cho Lebesgue-hầu hết bất kỳ β, γ.

Chứng minh ở Phụ lục F. Đầu tiên, chúng tôi chứng minh tính giải tích của kernel Kattn theo các siêu tham số β và γ. Do định lý đồng nhất cho các hàm giải tích, đủ để chỉ ra ít nhất một lựa chọn siêu tham số β và γ thỏa mãn (10) cho tất cả các hoán vị không đồng nhất τ. Vì Kattn không có giải pháp dạng đóng, chúng tôi tìm lựa chọn β và γ như vậy bằng cách phân tích khai triển chuỗi Taylor của Kattn xung quanh β= 0 và γ= 0 đến các đạo hàm bậc 10.

3.3 Cải thiện hiệu quả dữ liệu của transformer với tham số hóa WKWT
Q+aI

Liệu chúng ta có thể sử dụng những hiểu biết này để cải thiện hiệu quả dữ liệu của transformers trong template tasks? Trong chứng minh, tính không suy biến của N trong Lemma 3.5 thúc đẩy khả năng tổng quát hóa của mô hình trên các ký hiệu chưa thấy. Điều này gợi ý rằng một cách tiếp cận để cải thiện hiệu quả dữ liệu là làm cho N có điều kiện tốt hơn bằng cách điều chỉnh tham số hóa transformer. Chúng tôi xem xét ở đây tác vụ đơn giản nhất, với các templates " αα" và " αβ" được gán nhãn +1 và −1, tương ứng. Đối với các tokens A, B, C, D ∈ X, ma trận N là

N=K(AA, BB )K(AA, BC )
K(BC, AA )K(AB, CD )

Nếu K là một inner-product kernel, K(x,x′) =κ(P
i∈[k]1(xi=x′
i)), như từ một MLP, thì K(AA, BB ) = K(AA, BC ) = K(BC, AA ) = K(AB, CD ) = κ(0), vì vậy N suy biến và việc tổng quát hóa không đạt được. Trực giác, mỗi mẫu xi có gần như cùng "hồ sơ tương tự với dữ liệu khác" ˆKi,:= [K(xi,x1), . . . , K (xi,xn)], vì vậy phương pháp kernel không thể xác định các mẫu đến từ cùng template với xtest. Ngược lại, transformer kernel (4) thành công bằng cách sử dụng thông tin về ma trận incidence XXT, khác nhau giữa các templates, và không phụ thuộc vào việc thay thế ký hiệu. Do đó chúng tôi đề xuất nhấn mạnh ma trận incidence XXT bằng cách tham số hóa lại mỗi head thành WKWT
Q+aI, trong đó a là một tham số có thể huấn luyện. Điều này thêm một scaling của XXT trong attention, và có thể cải thiện hiệu quả dữ liệu thực nghiệm lên một bậc độ lớn trên một số template tasks (xem Hình 2 và 3, cũng như các thí nghiệm bổ sung trong Phụ lục B).

4 Phân tích cho template tasks trong thiết lập dự đoán token tiếp theo
Chúng tôi chuyển sang thiết lập dự đoán token tiếp theo với cross-entropy loss, trong đó nhãn đầu ra có thể là một token như trong ví dụ của Hình 3; định nghĩa chính thức ở Phụ lục J. Tác vụ đơn giản nhất bao gồm template " α" được gán nhãn bằng " α". Một ví dụ tập train là {(A, A),(B, B),(C, C)}, trong đó A, B, C ∈ X là các tokens, và sau đó chúng ta kiểm tra với (xtest, ytest) = (D, D ) không có trong tập train. Tác vụ này nắm bắt khả năng của một mô hình học cách sao chép một ký hiệu, điều này quan trọng cho

--- TRANG 10 ---
(a) Vanilla transformer (b) Transformer với WVWT
O+bI
101
102
103
Số mẫu huấn luyện0.00.51.01.5Lỗi Testdim=64
dim=128
dim=256
dim=512
dim=1024
dim=2048
dim=4096
101
102
103
Số mẫu huấn luyện0.00.51.01.5Lỗi Testdim=64
dim=128
dim=256
dim=512
dim=1024
dim=2048
dim=4096
101
102
103
Số mẫu huấn luyện0.00.51.01.5Lỗi Test

Hình 5: (a) Transformers thất bại trong tác vụ sao chép khi chiều embedding demb tăng (Định lý 4.1); (b) Thành công khi tham số hóa lại WVWT
O thành WVWT
O+bI (Định lý 4.2). Chi tiết trong Phụ lục A.

các LLMs giải quyết các vấn đề với các tính toán trung gian nhiều giai đoạn và phải sao chép chúng đến các phần sau của một giải pháp [CIS21]. Từ bây giờ, chúng tôi chỉ xem xét tác vụ "sao chép" này.

Chúng tôi xem xét một kiến trúc fattn(x;θ) chỉ với một lớp multi-head attention, và chúng tôi tie trọng số embedding và unembedding như trong thực tế [Bro+20]. Định nghĩa train loss và test loss như sau, trong đó ℓ là cross-entropy loss và xtest là một token chưa thấy trong dữ liệu huấn luyện:

Ltrain(θ) =1
nPn
i=1ℓ(fattn(xi;θ), yi) và Ltest(θ) =ℓ(fattn(xtest), ytest). Chúng tôi chứng minh mạng này không tổng quát hóa trên các ký hiệu chưa thấy khi được huấn luyện, khi chúng ta lấy chiều embedding lớn. Bằng chứng của chúng tôi từ việc phân tích thời gian đầu của huấn luyện, và cho thấy rằng test loss trên các ký hiệu chưa thấy không giảm.

Định lý 4.1 (Thất bại của transformers trong việc sao chép). Đối với bất kỳ learning rates nào sao cho −∂Ltrain
∂t|t=0= O(1), chúng ta phải có ∂Ltest
∂t|t=0→0 khi demb→ ∞.

Ý tưởng chứng minh là vì chuỗi đầu vào có độ dài k= 1, kiến trúc đơn giản hóa: tất cả softmaxes trong các attention heads xuất ra 1, và mạng là một tổng của các attention heads dạng XW EWVWT
OWT
E. Ở thời gian đầu sự tiến hóa của các trọng số WVWT
O sẽ gần như nằm trong span của {WT
EexieT
xiWE}i∈[n], mà khi chiều embedding trở nên lớn sẽ gần như trực giao với hướng WT
EextesteT
xtestWE sẽ làm giảm test loss. Điều này gợi ý sự điều chỉnh sau đây cho transformers cho phép chúng sao chép các ký hiệu chưa từng thấy trong huấn luyện:

Định lý 4.2 (Thêm một tham số cho phép sao chép). Sau khi tham số hóa lại attention (3) sao cho trong mỗi head WVWT
O được thay thế bằng WVWT
O+bI trong đó b là một tham số có thể huấn luyện, có các learning rates sao cho −∂Ltrain
∂t|t=0=O(1) và −∂Ltest
∂t|t=0= Ω(1) khi demb→ ∞.

Hình 3 và 5 minh họa lợi ích của tham số bổ sung cho mỗi head này trên tác vụ sao chép. Nó không tương đương với việc thêm một trainable skip connection như trong ResNet [He+16]. Thay vào đó, việc thêm bhI mã hóa một attention-modulated skip connection cho phép sao chép tokens giữa các streams của transformer. Một điều chỉnh liên quan của việc thêm một head với XXT được hardcode như ma trận attention của nó đã được đề xuất trong [Zha+22].

5 Thí nghiệm
Hình 2 và 3 (và các thí nghiệm bổ sung trong Phụ lục B) cho thấy rằng các tham số hóa lại của chúng tôi có thể mang lại lợi ích hiệu quả dữ liệu đáng kể trên template tasks. Hình 6 cho thấy chúng cũng có thể mang lại cải thiện trên dữ liệu thực. Trong Hình 7, chúng ta thấy rằng pretraining vượt trội so với khởi tạo ngẫu nhiên trên một template task. Điều này có thể được giải thích bằng một số heads của mô hình pretrained có đường chéo mạnh hơn so với các trọng số khác (ban đầu được quan sát trong [TK23]). Những đường chéo đã học này giống với các điều chỉnh transformer mà chúng tôi đề xuất và vì vậy có thể đang thúc đẩy hiệu quả dữ liệu của việc fine-tuning một

--- TRANG 11 ---
Dataset GPT-2 GPT-2 + trainable identity scalings (ours)
Wikitext2 64.00 60.46
Wikitext103 16.83 16.40

Hình 6: Perplexity của GPT-2 được huấn luyện từ khởi tạo ngẫu nhiên với Adam learning rate 3e-4 trong 20 epochs trên Wikitext (perplexity nhỏ hơn là tốt hơn). GPT-2 có 117M tham số, và chúng tôi thêm 288 tham số bổ sung (2 cho mỗi head). Thú vị là, mặc dù tác vụ là mô hình hóa Wikipedia, và do đó không phải là một tác vụ lý luận thuần túy, các điều chỉnh transformer vẫn mang lại cải thiện.

Ảnh hưởng của pretraining WKWT
QHead 12, Layer 5 WVWT
OHead 12, Layer 11
101
102
103
104
Số mẫu huấn luyện0.00.51.01.52.0Test lossGPT-2 from scratch
GPT-2 pretrained
0 25 50 750
20
40
60
80
0 25 50 750
20
40
60
80

Hình 7: Trái: Test loss của GPT-2 pretrained so với khởi tạo ngẫu nhiên khi được fine-tuned trên tác vụ template αβα vs. αββ. Phải: một số heads GPT-2 pretrained có đường chéo mạnh (được zoom đến góc trên-trái 100x100).

mô hình pretrained. Phụ lục B cung cấp các thí nghiệm rộng rãi về ảnh hưởng của các siêu tham số, thiên hướng quy nạp của các mô hình khác nhau, và các mức độ khó khăn tác vụ khác nhau.

6 Thảo luận
Chúng tôi cho thấy rằng transformers là một kiến trúc universal cho template tasks trong thiết lập hồi quy: khi được huấn luyện với gradient descent với đủ dữ liệu huấn luyện chúng học được lý luận quan hệ. Tuy nhiên, transformers không optimal – thực nghiệm chúng yêu cầu lượng dữ liệu lớn để học các tác vụ cơ bản, và trong thiết lập dự đoán token tiếp theo chúng thất bại trong việc sao chép các ký hiệu chưa thấy. Do đó, chúng tôi đã đề xuất các điều chỉnh kiến trúc để cải thiện thiên hướng quy nạp của chúng đối với lý luận logic. Có vẻ hứa hẹn để khám phá các tác vụ lý luận khác (ví dụ, lý luận với syllogisms, lý luận bằng đối xứng, và lý luận tổng hợp). Cũng có thể có lợi khi nghiên cứu các phương pháp data augmentation (ví dụ, nối tensorization XXT vào đầu vào, để khuyến khích sử dụng thông tin quan hệ). Ngoài ra, các bounds định lượng chặt chẽ về dữ liệu và độ rộng của kiến trúc cần thiết, tùy thuộc vào template task, là một hướng mở thú vị.

Tài liệu tham khảo
[AB22] Emmanuel Abbe and Enric Boix-Adsera. "On the non-universality of deep learning: quantifying the cost of symmetry". In: Advances in Neural Information Processing Systems35 (2022), pp. 17188–17201.

[Abb+22] Emmanuel Abbe et al. "An initial alignment between neural network and target is needed for gradient descent to learn". In: International Conference on Machine Learning. PMLR. 2022, pp. 33–52.

--- TRANG 12 ---
[Abb+23] Emmanuel Abbe et al. "Generalization on the unseen, logic reasoning and degree curriculum". In: arXiv preprint arXiv:2301.13105 (2023).

[Ahn+23] Kwangjun Ahn et al. "Transformers learn to implement preconditioned gradient descent for in-context learning". In: arXiv preprint arXiv:2306.00297 (2023).

[AL23] Zeyuan Allen-Zhu and Yuanzhi Li. "Physics of Language Models: Part 1, Context-Free Grammar". In: arXiv preprint arXiv:2305.13673 (2023).

[Alt+23] Awni Altabaa et al. "Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning". In: arXiv preprint arXiv:2304.00195 (2023).

[Bro+20] Tom Brown et al. "Language models are few-shot learners". In: Advances in neural information processing systems 33 (2020), pp. 1877–1901.

[CB18] Lenaic Chizat and Francis Bach. "On the global convergence of gradient descent for over-parameterized models using optimal transport". In: Advances in neural information processing systems 31 (2018).

[CIS21] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. "The neural data router: Adaptive control flow in transformers improves systematic generalization". In: arXiv preprint arXiv:2110.07732 (2021).

[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. "On lazy training in differentiable programming". In: Advances in neural information processing systems 32 (2019).

[Cyb89] George Cybenko. "Approximation by superpositions of a sigmoidal function". In: Mathematics of control, signals and systems 2.4 (1989), pp. 303–314.

[Dav+23] Xander Davies et al. "Discovering variable binding circuitry with desiderata". In: arXiv preprint arXiv:2307.03637 (2023).

[Ede+22] Benjamin L Edelman et al. "Inductive biases and variable creation in self-attention mechanisms". In: International Conference on Machine Learning. PMLR. 2022, pp. 5793–5831.

[Fod75] Jerry A Fodor. The language of thought. Vol. 5. Harvard university press, 1975.

[Gar+22] Shivam Garg et al. "What can transformers learn in-context? a case study of simple function classes". In: Advances in Neural Information Processing Systems 35 (2022), pp. 30583–30598.

[Gei+23] Atticus Geiger et al. "Relational reasoning and generalization using nonsymbolic neural networks." In: Psychological Review 130.2 (2023), p. 308.

[He+16] Kaiming He et al. "Deep residual learning for image recognition". In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770–778.

[Hol12] Keith J Holyoak. "Analogy and relational reasoning". In: The Oxford handbook of thinking and reasoning (2012), pp. 234–259.

[Hup+20] Dieuwke Hupkes et al. "Compositionality decomposed: How do neural networks generalise?" In: Journal of Artificial Intelligence Research 67 (2020), pp. 757–795.

[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler. "Neural tangent kernel: Convergence and generalization in neural networks". In: Advances in neural information processing systems 31 (2018).

[Kap+20] Jared Kaplan et al. "Scaling laws for neural language models". In: arXiv preprint arXiv:2001.08361 (2020).

--- TRANG 13 ---
[Ker+22] Giancarlo Kerg et al. "On neural architecture inductive biases for relational tasks". In: arXiv preprint arXiv:2206.05056 (2022).

[KP02] Steven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science & Business Media, 2002.

[Kri+13] Trenton Kriete et al. "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia". In: Proceedings of the National Academy of Sciences 110.41 (2013), pp. 16390–16395.

[KRS18] Junkyung Kim, Matthew Ricci, and Thomas Serre. "Not-So-CLEVR: learning same–different relations strains feedforward neural networks". In: Interface focus 8.4 (2018), p. 20180011.

[LZA20] Zhiyuan Li, Yi Zhang, and Sanjeev Arora. "Why are convolutional nets more sample-efficient than fully-connected nets?" In: arXiv preprint arXiv:2010.08515 (2020).

[Mar98] Gary F Marcus. "Rethinking eliminative connectionism". In: Cognitive psychology 37.3 (1998), pp. 243–282.

[McK+23] Ian R McKenzie et al. "Inverse Scaling: When Bigger Isn't Better". In: arXiv preprint arXiv:2306.09479 (2023).

[Mit20] Boris Samuilovich Mityagin. "The zero set of a real analytic function". In: Mathematical Notes107.3-4 (2020), pp. 529–530.

[MK16] Antone Martinho III and Alex Kacelnik. "Ducklings imprint on the relational concept of "same or different"". In: Science353.6296 (2016), pp. 286–288.

[MMM19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit". In: Conference on Learning Theory. PMLR. 2019, pp. 2388–2464.

[MMN18] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. "A mean field view of the landscape of two-layer neural networks". In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665–E7671.

[Nan+23] Neel Nanda et al. "Progress measures for grokking via mechanistic interpretability". In: arXiv preprint arXiv:2301.05217 (2023).

[New80] Allen Newell. "Physical symbol systems". In: Cognitive science 4.2 (1980), pp. 135–183.

[Ng04] Andrew Y Ng. "Feature selection, L1 vs. L2 regularization, and rotational invariance". In: Proceedings of the twenty-first international conference on Machine learning. 2004, p. 78.

[Ols+22] Catherine Olsson et al. "In-context learning and induction heads". In: arXiv preprint arXiv:2209.11895 (2022).

[PPW18] Rasmus Palm, Ulrich Paquet, and Ole Winther. "Recurrent relational networks". In: Advances in neural information processing systems 31 (2018).

[Rav38] John C Raven. "Progressive matrices: A perceptual test of intelligence". In: London: HK Lewis 19 (1938), p. 20.

[RV18] Grant Rotskoff and Eric Vanden-Eijnden. "Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks". In: Advances in neural information processing systems 31 (2018).

--- TRANG 14 ---
[San+17] Adam Santoro et al. "A simple neural network module for relational reasoning". In: Advances in neural information processing systems 30 (2017).

[San+18] Adam Santoro et al. "Relational recurrent neural networks". In: Advances in neural information processing systems 31 (2018).

[Sha+20] Murray Shanahan et al. "An explicitly relational neural network architecture". In: International Conference on Machine Learning. PMLR. 2020, pp. 8593–8603.

[Sha18] Ohad Shamir. "Distribution-specific hardness of learning neural networks". In: The Journal of Machine Learning Research 19.1 (2018), pp. 1135–1163.

[SKM84] Richard E Snow, Patrick C Kyllonen, and Brachia Marshalek. "The topography of ability and learning correlations". In: Advances in the psychology of human intelligence (1984), pp. 47–103.

[SS22] Justin Sirignano and Konstantinos Spiliopoulos. "Mean field analysis of deep neural networks". In: Mathematics of Operations Research 47.1 (2022), pp. 120–152.

[TK23] Asher Trockman and J Zico Kolter. "Mimetic Initialization of Self-Attention Layers". In: arXiv preprint arXiv:2305.09828 (2023).

[Vas+17] Ashish Vaswani et al. "Attention is all you need". In: Advances in neural information processing systems 30 (2017).

[Web+20] Taylor Webb et al. "Learning representations that support extrapolation". In: International conference on machine learning. PMLR. 2020, pp. 10136–10146.

[Web+23] Taylor W Webb et al. "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction". In: arXiv preprint arXiv:2309.06629 (2023).

[Wel13] Max Welling. "Kernel ridge regression". In: Max Welling's classnotes in machine learning(2013), pp. 1–3.

[WHL23] Taylor Webb, Keith J Holyoak, and Hongjing Lu. "Emergent analogical reasoning in large language models". In: Nature Human Behaviour (2023), pp. 1–16.

[WSC20] Taylor W Webb, Ishan Sinha, and Jonathan D Cohen. "Emergent symbols through binding in external memory". In: arXiv preprint arXiv:2012.14601 (2020).

[YH21] Greg Yang and Edward J Hu. "Tensor programs iv: Feature learning in infinite-width neural networks". In: International Conference on Machine Learning. PMLR. 2021, pp. 11727–11737.

[Yua+23] Zheng Yuan et al. "Scaling relationship on learning mathematical reasoning with large language models". In: arXiv preprint arXiv:2308.01825 (2023).

[ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. "Trained Transformers Learn Linear Models In-Context". In: arXiv preprint arXiv:2306.09927 (2023).

[Zha+21a] Chiyuan Zhang et al. "Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization". In: arXiv preprint arXiv:2107.12580 (2021).

[Zha+21b] Chiyuan Zhang et al. "Understanding deep learning (still) requires rethinking generalization". In: Communications of the ACM 64.3 (2021), pp. 107–115.

[Zha+22] Yi Zhang et al. "Unveiling transformers with lego: a synthetic reasoning task". In: arXiv preprint arXiv:2206.04301 (2022).

[Zha+23] Haoyu Zhao et al. "Do Transformers Parse while Predicting the Masked Word?" In: arXiv preprint arXiv:2303.08117 (2023).

--- TRANG 15 ---
Mục lục
1 Giới thiệu 1
1.1 Nắm bắt lý luận quan hệ với template tasks . . . . . . . . . . . . . . . . . . . 1
1.2 Kết quả chính . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Tài liệu liên quan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Định nghĩa chính thức của template tasks 5
3 Phân tích cho template tasks trong thiết lập hồi quy 6
3.1 Transformer random features kernel . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Transformers tổng quát hóa trên các ký hiệu chưa thấy . . . . . . . . . . . . . . . 7
3.3 Cải thiện hiệu quả dữ liệu của transformer với tham số hóa WKWT
Q+aI . . . . . 9
4 Phân tích cho template tasks trong thiết lập dự đoán token tiếp theo 9
5 Thí nghiệm 10
6 Thảo luận 11
A Chi tiết cho các hình trong văn bản chính 16
B Thí nghiệm bổ sung 17
B.1 Ảnh hưởng của các siêu tham số transformer . . . . . . . . . . . . . . . . . . . . 17
B.2 Ảnh hưởng của độ phức tạp của tác vụ . . . . . . . . . . . . . . . . . . . . . . . 18
B.3 Ảnh hưởng của thiên hướng quy nạp của mô hình . . . . . . . . . . . . . . . . . 18
C Chứng minh của Định lý 3.4 27
C.1 Phần 1. Điều kiện đủ tổng quát cho test loss tốt . . . . . . . . . . . . . . . . . . 27
C.2 Phần 2. Phân tích transformer random features kernel . . . . . . . . . . . . . . 28
C.3 Kết luận chứng minh của Định lý 3.4 . . . . . . . . . . . . . . . . . . . . . . . . 29
D Điều kiện đủ để phương pháp kernel tổng quát hóa trên các ký hiệu chưa thấy (Chứng minh của Lemma C.3) 29
D.1 Chứng minh hoãn lại của các claims . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.2 Ghi chú: sự phụ thuộc rõ ràng vào ∥N−1∥. . . . . . . . . . . . . . . . . . . . . 34
E Tính không suy biến của random features sau lớp MLP (Chứng minh của Lemma 3.6) 34
F Phân tích các features của lớp attention (Chứng minh của Lemma 3.7) 36
F.1 Các đạo hàm bậc thấp của attention kernel . . . . . . . . . . . . . . . . . . . . . 36
F.2 Đơn giản hóa các số hạng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2.1 Giả định [1TX]R= [1TY]R. . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.2 Giả định [X][k]×R= [Y][k]×R. . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.3 Giả định 1TXXT1 = 1TY YT1. . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.4 Giả định 1TXXT= 1TY YT. . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.3 Chứng minh của Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

--- TRANG 16 ---
G Tính giải tích của attention kernel (kết quả kỹ thuật) 41
G.1 Các lemmas kỹ thuật để định lượng sự hội tụ power series . . . . . . . . . . . . 42
G.2 Ứng dụng các lemmas kỹ thuật vào attention kernel . . . . . . . . . . . . . . . . 43
H Dẫn xuất transformer kernel 45
H.1 Kiến trúc Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
H.2 Random features kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
H.3 Dẫn xuất không chính thức . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
I MLPs thất bại trong việc tổng quát hóa trên các ký hiệu chưa thấy 48
J Chi tiết hoãn lại cho next-token-prediction template tasks 51
J.1 Định nghĩa của next-token-prediction template tasks . . . . . . . . . . . . . . . . 51
J.2 Thất bại của transformers trong việc sao chép và điều chỉnh thành công . . . . . . 51

A Chi tiết cho các hình trong văn bản chính
Code có sẵn tại https://github.com/eboix/relational-reasoning/ .

Các tác vụ tâm lý Chúng tôi mô tả cách các tác vụ trong Hình 1 nằm dưới khuôn khổ template.
•(a) Phân phối của 3. Tác vụ là hoàn thành hàng dưới sao cho tập hợp các phần tử giống như trong hàng trên (đáp án: 2). Để nhập tác vụ này vào một mô hình ngôn ngữ, một token được sử dụng để biểu diễn mỗi ký hiệu. Ví dụ trong hình khớp với template " αβγ γα □ϵαβγ", với nhãn +2. Có các templates khác cho tác vụ này, tương ứng với các sắp xếp khác nhau của các đối tượng, như " αβγ βγ □αγϵβ" với nhãn +1, và " αβγ γβ □ϵβαγ" với nhãn +3. Tổng cộng có 144 templates, vì 3 phần tử đầu tiên của template luôn là αβγ, và sau đó có 6 lựa chọn cho hoán vị trong hàng tiếp theo, và cuối cùng 24 lựa chọn cho hoán vị trong hàng cuối.

•(b) Relational match-to-sample. Tác vụ là khớp hàng đầu tiên với một trong hai mẫu thay thế (đáp án: 1). Một lần nữa, một token được sử dụng để biểu diễn mỗi ký hiệu. Ví dụ trong hình khớp với " αββ γδδ ϵϵτ " với nhãn +1. Một tính toán tổ hợp đơn giản cho tổng cộng 40 templates (5 mẫu có thể trong hàng đầu tiên, nhân với 2 lựa chọn cho việc tùy chọn đầu tiên hay tùy chọn thứ hai đúng, nhân với 4 lựa chọn cho mẫu của tùy chọn thay thế).

•(c) Raven's progressive matrices. Một tác vụ Raven's progressive matrices tiêu chuẩn [Rav38] (đáp án: ba vòng tròn tối). Đối với mỗi chiều của hình dạng, số lượng, và màu sắc, chúng ta có một tác vụ "phân phối của 3" với một nhãn ký hiệu. Ví dụ, đối với các hình dạng trong hình, tác vụ là " αβγ βγα γβ ?" với nhãn α. Vì một khả năng khác là mỗi hàng là hằng số (như trong, ví dụ, trường hợp của các số), một template có thể khác là " ααα βββ γγ ?" với nhãn γ, và vì vậy có tổng cộng 36+1 = 37 templates có thể cho mỗi chiều. Thảo luận này giả định rằng các mẫu duy nhất trong progressive matrices là phân phối của 3, và hằng số. Nếu các tiến trình cũng được cho phép như trong [WHL23], chúng có thể được tích hợp bằng cách thêm các templates tương ứng.

--- TRANG 17 ---
Hiệu suất Transformer Trong tất cả các thí nghiệm, các kiến trúc transformer tiêu chuẩn được sử dụng. Trong Hình 2, Kiến trúc là một transformer 2 lớp với 16 heads mỗi lớp, chiều embedding 128, chiều head 64, chiều MLP 256, được huấn luyện với Adam với learning rate 1e-3 và batch-size 1024. n mẫu huấn luyện được chọn bằng cách chọn tên biến ngẫu nhiên từ một bảng chữ cái có ntokens. Tập test là cùng hai chương trình nhưng với tên biến disjoint. Các thanh lỗi được báo cáo là trung bình trên 5 lần thử. Learning rate cho mỗi đường cong được chọn là cái đạt được tổng quát hóa tốt nhất trong {10−5,10−4,10−3,10−2}. Trong Hình 3, thiết lập giống nhau ngoại trừ transformer là 4-lớp transformer và có chiều embedding 512. Trong Hình 5 các siêu tham số giống như trong Hình 2 được sử dụng. Để đo hiệu suất tổng quát hóa của mô hình đã học trên các ký hiệu chưa thấy, chúng tôi đánh giá nó trên một tập test và một tập validation mỗi tập bao gồm 100 mẫu được rút theo cách tương tự như tập dữ liệu huấn luyện, nhưng mỗi tập sử dụng một bảng chữ cái disjoint có kích thước 100. Do đó, không có sự chồng chéo trong support của các phân phối train, test, và validation. Chúng tôi sử dụng validation loss để chọn epoch tốt nhất của huấn luyện trong 1000 epochs. Chúng tôi báo cáo test loss trên mô hình đã lưu này.

B Thí nghiệm bổ sung
Chúng tôi báo cáo các thí nghiệm bổ sung rộng rãi thăm dò khuôn khổ template task. Trong mỗi thí nghiệm này, tập dữ liệu huấn luyện bao gồm n mẫu huấn luyện ngẫu nhiên. Mỗi mẫu được rút theo một phân phối template. Sau đây là các template tasks mà chúng tôi kiểm tra.

•Tác vụ αβα vs. αββ. Đều trên hai templates αβα và αββ với nhãn 1, -1 tương ứng và α và β là wildcards.

•Tác vụ αβαβ vs. ααββ. Giống như trên, ngoại trừ với templates αβαβ và ααββ.

•Tác vụ đa số độ dài k. Đều trên 2k−1 templates α×{α, β}k−1 trong đó α và β là wildcards. Một template z có nhãn 1 nếu token đầu tiên của nó xuất hiện trong đa số phần còn lại của chuỗi, và -1 nếu ngược lại. Tức là, f∗(z) =(
1,|{i:z1=zi}|>(k+ 1)/2
−1,ngược lại.

•Random template task. Một số r templates được rút đều từ (W ∪X )k, điều kiện là đôi một khác nhau. Tác vụ là phân phối đều trên r templates này, với nhãn Gaussian ngẫu nhiên được centered và scaled sao cho trivial MSE là 1.

Đối với bất kỳ tác vụ nào trong số này, chúng tôi tạo n mẫu huấn luyện như sau. Chúng tôi thay thế các wildcards cho các token thông thường bằng cách sử dụng một hàm đơn ánh được chọn ngẫu nhiên s:W → X trong đó X là một bảng chữ cái có kích thước n (cùng kích thước với số lượng mẫu). Ví dụ, nếu một mẫu cho trước được tạo từ template αβα với substitution map s ánh xạ s(A) = 12,s(B) = 5, thì mẫu sẽ là [12,5,12]. Các thanh lỗi là trên 5 lần thử, trừ khi có ghi chú khác.

B.1 Ảnh hưởng của các siêu tham số transformer
Chúng tôi kiểm tra một kiến trúc transformer tiêu chuẩn trên tác vụ αβα vs. αββ, thay đổi một số siêu tham số của transformer để cô lập ảnh hưởng của chúng trong khi giữ tất cả các siêu tham số khác cố định. Các siêu tham số cơ sở là độ sâu 2, chiều embedding 128, chiều head 64, số heads mỗi lớp 16, được huấn luyện với Adam với minibatch size 1024 trong 1000 epochs. Các thí nghiệm của chúng tôi như sau:

•Learning rate và n. Trong Hình 8 chúng tôi thay đổi learning rate và n.

--- TRANG 18 ---
•Learning rate và độ sâu. Trong Hình 9 và Hình 10, chúng tôi thay đổi learning rate và độ sâu, cho n= 512 và n= 1024, tương ứng.

•Learning rate và số heads. Trong Hình 11 và 12, chúng tôi thay đổi learning rate và số heads, cho n= 512 và n= 1024, tương ứng.

•Learning rate và chiều embedding. Trong Hình 13 chúng tôi thay đổi learning rate và chiều embedding cho n= 1024.

•Learning rate và batch size. Trong Hình 14, chúng tôi thay đổi learning rate và batch-size cho n= 512. Trong Hình 16 chúng tôi thay đổi batch-size và n cho learning rate 0.001.

•Chỉ huấn luyện lớp cuối. Trong Hình 15, chúng tôi chỉ huấn luyện lớp cuối, và thấy rằng mạng thực sự học được tổng quát hóa ngoài phân phối, như được dự đoán bởi lý thuyết của chúng tôi. Tuy nhiên, số lượng mẫu và số epochs cần thiết lớn hơn so với khi tất cả các tham số được huấn luyện. Chúng tôi huấn luyện trong 10000 epochs và có 64 heads mỗi lớp trong thí nghiệm này.

B.2 Ảnh hưởng của độ phức tạp của tác vụ
Chúng tôi kiểm tra một kiến trúc transformer out-of-the-box với độ sâu 2, chiều embedding 128, chiều head 64, số heads 16, được huấn luyện với Adam với batch-size 1024 trong 1000 epochs, trên các template tasks khác nhau.

•So sánh độ khó của các tác vụ khác nhau. Hình 17 chúng tôi vẽ hiệu suất trên các tác vụ đơn giản khác nhau.

•Random tasks. Trong Hình 18, 19, 20, và 21, chúng tôi kiểm tra trên các random template tasks, và điều tra ảnh hưởng của độ dài template, kích thước bảng chữ cái wildcard, kích thước bảng chữ cái token thông thường, số lượng templates.

B.3 Ảnh hưởng của thiên hướng quy nạp của mô hình
Chúng tôi cung cấp các thí nghiệm thăm dò ảnh hưởng của thiên hướng quy nạp của mô hình:

•Các kiến trúc khác nhau. Trong Hình 22, chúng tôi vẽ test loss cho các kiến trúc khác nhau trên tác vụ template αβα vs. αββ, bao gồm transformers với các nhiễu động đồng nhất có thể huấn luyện đối với WQWT
K, đối với WVWT
O, đối với cả WQWT
K và WVWT
O, hoặc không có cái nào. Hình 23 minh họa ảnh hưởng có lợi của điều chỉnh transformer cho tác vụ đa số với các độ dài khác nhau, giảm lượng dữ liệu cần thiết xuống một bậc độ lớn.

•Kích thước của mô hình. Trong Hình 24 chúng tôi so sánh test loss của việc fine-tuning các mạng GPT-2 pretrained nhỏ, trung bình và lớn trên tác vụ template αβα vs. αββ.

•MLP với data augmentation XXT so với transformer. Trong Hình 25, chúng tôi so sánh test loss của một transformer với test loss của một MLP nơi dữ liệu đầu vào đã được augmented bằng cách nối vec(XXT), đây là một data augmentation cải thiện hiệu suất dưới tiêu chí NTK tương tự như thảo luận trong Mục 3.3 và phần thảo luận.

--- TRANG 19 ---
[Các hình và biểu đồ từ trang 19-26 sẽ được dịch tương tự, nhưng tôi sẽ bỏ qua vì chúng chủ yếu là các biểu đồ với chú thích tiếng Anh. Nếu cần, tôi có thể dịch từng hình cụ thể.]

--- TRANG 27 ---
C Chứng minh của Định lý 3.4
Có hai phần chính trong chứng minh. Đầu tiên, trong Mục C.1 chúng tôi thiết lập một lemma với điều kiện đủ để một phương pháp kernel có test loss tốt. Thứ hai, trong Mục C.2 chúng tôi chứng minh rằng transformer random features kernel Ktrans thỏa mãn điều kiện này đối với hầu hết bất kỳ tham số β, γ, b1, b2. Chúng tôi kết luận trong Mục C.3.

Ghi chú C.1. Lý do chúng tôi phát biểu kết quả của mình với mean-squared error loss là chúng tôi có giải pháp dạng đóng (5) cho hàm mà phương pháp kernel học được theo kernel và dữ liệu của nó. Biểu thức như vậy không được biết đến cho cross-entropy loss.

C.1 Phần 1. Điều kiện đủ tổng quát cho test loss tốt
Chúng tôi giới hạn bản thân với các token-symmetric kernels, đây là các kernels có giá trị không thay đổi nếu các tokens được relabel bằng một hoán vị.

Định nghĩa C.2 (Token-symmetric kernel). K là token-symmetric nếu đối với bất kỳ hoán vị π:X → X nào chúng ta có K(x,y) =K([π(x1), . . . , π (xk)],[π(y1), . . . , π (yk)]).

Token-symmetry là một điều kiện nhẹ, vì hầu hết các kiến trúc mạng được sử dụng trong thực tế (bao gồm transformers) có neural tangent kernels token-symmetric tại khởi tạo. Chúng tôi nhấn mạnh rằng token-symmetry không đủ cho test loss tốt vì MLPs là một phản ví dụ (xem Phụ lục I.)

Để phát biểu điều kiện đủ cho test loss tốt, cho {z1, . . . ,zr}= supp( µtmplt) là support của phân phối template. Định nghĩa cũng tập R=∪i∈[k],j∈[r]{zj,i} của các tokens xuất hiện trong các templates. Cuối cùng, định nghĩa N∈Rr×r bằng

Nij=K(sub(zi, s),sub(zj, s′)), (11)

trong đó s, s′:W → X là các substitution maps thỏa mãn
s(W)∩s′(W) = 0 và s(W)∩ R=s′(W)∩ R=∅. (12)

Người ta có thể kiểm tra rằng do token-symmetry của kernel K, ma trận N được định nghĩa duy nhất bất kể các substitution maps s, s′ được chọn, miễn là chúng thỏa mãn (12).

Lemma C.3 (Đủ để N không suy biến). Nếu K là một token-symmetric kernel, và N không suy biến, thì kernel ridge regression đạt được test loss triệt tiêu.

Chính thức, có các hằng số c, C > 0 và tham số ridge regularization λ >0 chỉ phụ thuộc vào µtmplt,σ,|W|,∥N−1∥ và ∥K∥∞= max xK(x,x), sao cho đối với bất kỳ x nào khớp với một template z∈supp( µtmplt) thì kernel ridge regression estimator ˆf trong (5) với kernel K thỏa mãn

|ˆf(x)−f∗(z)| ≤Cr
log(1/δ)
n+Cr1
ρ,

với xác suất ít nhất 1−δ−exp(−cn) trên các mẫu ngẫu nhiên.

Chứng minh ở Phụ lục D, nhưng chúng tôi phát triển một trực giác ở đây về tại sao tính không suy biến của ma trận N quan trọng. Cho [n] =I1⊔ I2⊔ ··· ⊔ In là phân hoạch của các mẫu sao cho nếu i∈ Ij thì mẫu (xi, yi) được rút bằng cách thay thế các wildcards của template zj với substitution map si:W → X. Chúng tôi cho thấy rằng đối với bất kỳ chuỗi x nào khớp với template zj, giải pháp kernel ridge regression (5) gần như bằng trung bình của các nhãn của các mẫu tương ứng với template j,

yT(ˆK+λI)−1k(x)≈1
|Ij|X
i∈Ijyi≈f∗(zj). (13)

Để thấy tại sao điều này đúng, hãy xem xét chế độ trong đó sự đa dạng mẫu rất cao, tức là ρ≫1. Vì ρ lớn, bất kỳ token cụ thể nào rất không có khả năng được thay thế. Điều này có những hàm ý sau:

•Đối với hầu hết các cặp mẫu i̸=i′∈[n], các maps si và si′ có phạm vi disjoint: si(W)∩s′
i(W).

•Đối với hầu hết các mẫu i∈[n], các tokens được thay thế không có trong các templates: si(W)∩ R=∅.

Đây là những điều kiện tương tự như trong (8). Vì vậy do token-symmetry của kernel, đối với hầu hết các cặp mẫu ma trận kernel thực nghiệm được cho bởi N:

ˆKi,i′:=K(xi,xi′) =Nj,j′ cho hầu hết i∈ Ij, i′∈ Ij′.

Vì vậy nếu N không suy biến, thì ˆK có r eigenvalues lớn, và n−r eigenvalues nhỏ hơn nhiều. Điều này hóa ra đủ để (9) đúng. Chúng tôi tham khảo người đọc đến Phụ lục D để biết thêm chi tiết.

C.2 Phần 2. Phân tích transformer random features kernel
Chúng tôi cho thấy rằng transformer random features kernel Ktrans thỏa mãn điều kiện đủ của Lemma C.3 cho test loss triệt tiêu. Rõ ràng là kernel này token-symmetric vì định nghĩa bất biến với các relabeling hoán vị của các tokens. Phần khó khăn là cho thấy rằng ma trận Ntrans:=N được định nghĩa với kernel K=Ktrans trong (11) không suy biến. Thách thức chính là transformer kernel không có giải pháp dạng đóng đã biết vì các số hạng softmax trong định nghĩa (4) của nó. Hơn nữa, kết quả đặc biệt thách thức để chứng minh vì nó phải đúng cho bất kỳ tập hợp các templates disjoint z1, . . . ,zr nào.

Chúng tôi phân tích lớp MLP và lớp attention của transformer riêng biệt. Chúng tôi quan sát rằng một điều kiện "yếu" trên Kattn có thể được nâng lên thành kết quả "mạnh" rằng Ntrans không suy biến. Trực giác, miễn là Kattn không phải là một kernel rất suy biến, rất không có khả năng lớp MLP có các triệt tiêu cần thiết để làm cho Ntrans không suy biến.

Lemma C.4 (Tính không suy biến của Ntrans, tái phát biểu của Lemma 3.6). Giả sử đối với mọi hoán vị không đồng nhất τ∈Sr\ {id},
X
i∈[r]Kattn(sub(zi, s),sub(zi, s′))̸=X
i∈[r]Kattn(sub(zi, s),sub(zτ(i), s′)), (14)
trong đó s, s′ là các substitution maps trong định nghĩa của Ntrans trong (12). Cho hàm kích hoạt của lớp MLP là ϕ(t) = cos( b1t+b2). Khi đó đối với hầu hết bất kỳ lựa chọn b1, b2 nào (ngoại trừ một tập có độ đo Lebesgue bằng không), ma trận Ntrans không suy biến.

Lemma này được chứng minh trong Phụ lục E, bằng cách đánh giá rõ ràng tích phân Gaussian, điều này có thể thực hiện được vì hàm kích hoạt là hàm cosine. Mặc dù trong chứng minh của chúng tôi chúng tôi sử dụng hàm kích hoạt cosine, chúng tôi phỏng đoán rằng kết quả này về mặt đạo đức nên đúng cho các hàm kích hoạt không đa thức đủ tổng quát. Tiếp theo, chúng tôi chứng minh điều kiện trên Nattn.

Lemma C.5 (Tính không suy biến của Kattn, tái phát biểu của Lemma 3.7). Điều kiện (14) đúng cho Lebesgue-hầu hết bất kỳ β, γ.

Chứng minh ở Phụ lục F. Đầu tiên, chúng tôi chứng minh tính giải tích của kernel Kattn theo các siêu tham số β và γ kiểm soát softmax inverse temperature và positional embeddings. Do định lý đồng nhất cho các hàm giải tích, đủ để chỉ ra ít nhất một lựa chọn siêu tham số β và γ thỏa mãn (14) cho tất cả các hoán vị không đồng nhất τ. Vì Kattn không có giải pháp dạng đóng, chúng tôi tìm lựa chọn β và γ như vậy bằng cách phân tích khai triển chuỗi Taylor của Kattn xung quanh β= 0 và γ= 0 đến các đạo hàm bậc 10, điều này tình cờ đủ.

C.3 Kết luận chứng minh của Định lý 3.4
Bởi Lemma C.3, đủ để chứng minh tính không suy biến của ma trận Ntrans được định nghĩa trong (11) với kernel K=Ktrans. Lemma 3.6 đưa ra một điều kiện cho tính không suy biến đúng cho hầu hết bất kỳ b1, b2. Lemma 3.7 chứng minh điều kiện này cho hầu hết bất kỳ β, γ. Do đó, Định lý 3.4 theo sau.

[Phần còn lại của tài liệu tiếp tục với các chứng minh kỹ thuật và chi tiết bổ sung...]
