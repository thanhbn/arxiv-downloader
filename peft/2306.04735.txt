# 2306.04735.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2306.04735.pdf
# File size: 2534181 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Using Soft-Prompt Tuning to Evaluate Bias in Large Language Models
Jacob-Junqi Tian1,2David Emerson1Sevil Zanjani Miyandoab3
Deval Pandya1Laleh Seyed-Kalantari1,4Faiza Khan Khattak1
1Vector Institute for AI2McGill University3Amirkabir University4York University
jacob.tian@mail.mcgill.ca, sev.zanjani@gmail.com, lsk@yorku.ca,
{david.emerson,deval.pandya,faiza.khankhattak }@vectorinstitute.ai
Abstract
Prompting large language models (LLMs) has gained sub-
stantial popularity as pre-trained LLMs are capable of per-
forming downstream tasks without requiring large quantities
of labelled data (Liu et al. 2023). It is, therefore, natural that
prompting is also used to evaluate biases exhibited by these
models. However, achieving good task-specific performance
often requires manual prompt optimization. In this paper, we
explore the use of soft-prompt tuning to quantify the biases
of LLMs such as OPT(Zhang et al. 2022) and LLaMA (Tou-
vron et al. 2023). These models are trained on real-world data
with potential implicit biases toward certain groups. Since
LLMs are increasingly used across many industries and ap-
plications, it is crucial to accurately and efficiently identify
such biases and their practical implications.
In this paper, we use soft-prompt tuning to evaluate model
bias across several sensitive attributes through the lens of
group fairness (bias) . In addition to improved task perfor-
mance, using soft-prompt tuning provides the advantage of
avoiding potential injection of human bias through manually
designed prompts. Probing with prompt-tuning reveals im-
portant bias patterns, including disparities across age and sex-
uality.
1 Introduction
Despite widespread and successful utilization, fine-tuned
language models (LMs) have several drawbacks. These in-
clude requiring significant compute resources for training,
large quantities of labelled data, and separate training and
storage for each downstream task (Han et al. 2021; Wang
et al. 2022). Language model prompting addresses some of
these downsides, but the task of designing prompts to induce
optimal performance for a given downstream application is
challenging (Liu et al. 2021; Petroni et al. 2019). Significant
progress has been made in automatic prompt engineering
methods. One such method for automatic prompt optimiza-
tion is soft-prompt tuning, a parameter-efficient fine-tuning
(PEFT) method that trains a small set of prompt-token em-
beddings to be provided along with the standard natural lan-
guage input. For various LLMs, soft-prompt tuning has been
shown to match, or nearly match, fine-tuning performance
for various tasks such as classification, summarization, and
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.question-answering (Lester, Al-Rfou, and Constant 2021;
Liu et al. 2022).
On the other hand, the existence of potentially harmful bi-
ases exhibited by popular LMs is well-documented (Dixon
et al. 2018; Suresh and Guttag 2021; Bender et al. 2021;
Marjanovic, Sta ´nczak, and Augenstein 2022) and quite com-
mon. Bias quantification has gained substantial attention
from the research community recently (Mehrabi et al. 2021;
Seyyed-Kalantari et al. 2021). As LLM applications con-
tinue to rapidly expand, developing comprehensive analyt-
ical frameworks to measure the learned or inherited social
biases of such models is imperative.
In this paper, we evaluate the utility of soft-prompt tuning
for bias evaluation of LLMs, including OPT (Zhang et al.
2022) and LLaMA language models (Touvron et al. 2023).
More specifically, the approach presented here leverages op-
timized soft-prompts to condition models toward the com-
pletion of sentiment analysis tasks on which fairness (bias)
metrics are subsequently measured. In addition to the meth-
ods efficiency in terms of the number parameters tuned, an-
other advantage of soft-prompt tuning is that it eliminates
any injection of human bias through manual prompt de-
sign. The experiments demonstrate that prompt-tuning en-
ables fine-grained analysis and an overall understanding of
an LLM’s bias with respect to sensitive attributes and across
protected groups. This paper’s contributions are as follows:
• To our knowledge, this is the first application of soft-
prompt tuning for fairness evaluation. Therefore, we
demonstrate that the approach constitutes an effective
and efficient approach for such evaluation.
• We show that LLMs such as OPT and LLaMA ex-
hibit measurable biases across protected groups within
the sensitive attributes of age, sexuality, and disability.
Furthermore, such biases are generally consistent across
model size, type, and prompt-tuning dataset.
• The bias metrics of positive and negative false-positive
rate gaps are explored here. However, the approach is
compatible with other fairness measures, including the
comprehensive fairness suite proposed in (Czarnowska,
Vyas, and Shah 2021).arXiv:2306.04735v2  [cs.CL]  5 Mar 2024

--- PAGE 2 ---
2 Related work
Research on soft-prompt tuning and PEFT methods for
LLMs has expanded quickly (Lester, Al-Rfou, and Constant
2021; Li and Liang 2021; Liu et al. 2022). Such methods fo-
cus on reducing the overhead associated with adapting pre-
trained LLMs to downstream tasks. These methods are well-
studied with respect to their competitive, and sometimes im-
proved, performance over full-model fine-tuning. However,
existing work does not consider the bias implications or the
utility of such approaches in bias evaluation.
On the other hand, many researchers have focused on
identifying, quantifying, and mitigating bias in natural-
language processing (NLP) (Delobelle et al. 2022; Felkner
et al. 2022). With respect to LLMs, some narrow bias eval-
uation baselines associated with models like GPT have been
established (Brown et al. 2020; Zhang et al. 2022). Alterna-
tively, a limited number of studies aim to design tools for
assessing bias in LLMs. For example, the Bias Benchmark
for QA evaluation task (Parrish et al. 2022), aims to create
a framework for evaluating social biases in LMs of any size
along a large swathe of sensitive attributes. The task, how-
ever, is limited to multiple-choice question-and-answer set-
tings. Big-Bench (Srivastava et al. 2022) introduces different
frameworks for evaluating LLMs, but a limited number bias
evaluation methods, metrics, and aspects are covered. Criti-
cally, each case above has thus far been limited to manually
designed prompts as the probing mechanism for LLMs. Our
work addresses this gap and provides an important tool for
the reproducible evaluation of bias in LLMs.
3 Methodology
In this paper, we leverage continuous prompt optimization
as an efficient means of quantifying bias present in LLMs.
Prompting is the process of augmenting input text with care-
fully crafted phrases or templates to help a pre-trained LM
accomplish a downstream task. When combined with well-
formed prompts, LLMs accurately perform many tasks with-
out the need for fine-tuning (Brown et al. 2020). However,
the composition of a prompt often has a material impact
on the LLMs performance (Liu et al. 2021). Recently, con-
siderable research has produced effective approaches for
automated prompt optimization, especially in the form of
prompting tuning, which modifies the continuous space of
token embeddings. Several works have shown that prompt
tuning, in its various forms, surpasses manual and discrete
optimization in terms of performance, and, in some cases,
even outperforms full-model fine-tuning. Moreover, the ap-
proach is also hundreds or thousands of times more param-
eter efficient than full-model fine-tuning, while simultane-
ously exhibiting better data efficiency (Lester, Al-Rfou, and
Constant 2021; Liu et al. 2022; Li and Liang 2021).
Bias in NLP is typically quantified using sensitive at-
tributes (Czarnowska, Vyas, and Shah 2021) such as gen-
der, age, or sexuality. Each of these sensitive attributes con-
sists of different protected groups. For example, the sen-
sitive attribute agemight consist of the protected groups
{adult, young, old }. See Appendix A for additional details.
Herein, we focus on group fairness , which evaluates whethera model’s performance varies significantly and consistently
across different protected groups and if that bias is harmful
for specific groups. While we focus on group fairness, the
methodology generalizes to other notions of fairness, such
as counterfactual fairness. From the bias perspective, con-
tinuous prompt optimization provides an excellent potential
assessment tool, but it has not been studied in previous liter-
ature. In this paper, the prompt-tuning approach in (Lester,
Al-Rfou, and Constant 2021) is applied to efficiently train
LLMs to perform two ternary sentiment analysis tasks as a
means of measuring extrinsic bias.
Experimental setup
As discussed above, we use soft-prompt tuning to evalu-
ate bias through the lens of group fairness . For a metric,
M, and a set of examples belonging to protected group, X,
group fairness is defined as
dM(X) =M(X)−M.
The function dM(X)measures the M-gap for a particular
group by comparing the metric value restricted to samples
from that group, M(X), with the mean metric value ob-
served for each protected group within a sensitive attribute,
M. In the analysis below, Mis the false-positive rate (FPR).
Therefore, we measure FPR Gaps in model performance.
Below, we specifically consider Positive and Negative
FPR Gaps in the context of ternary sentiment classification.
Positive FPR , for instance, is defined as the rate at which
data points labelled as negative or neutral sentiment are er-
roneously classified as positive by a prompt-tuned model.
Thus, a large Positive FPR Gap greater than zero indicates
that the classifier favours a group by classifying negative or
neutral examples belonging to that group as positive at a
higher rate compared with other groups. On the other hand, a
large and positive Negative FPR Gap suggests unfavourable
treatment by the model, as it classifies positive and neutral
examples belonging to a particular group as negative at a
higher rate, compared with others. The sensitive attributes
analyzed below, and their respective protected groups, are
• Age: {adult, old, young }
• Sexuality: {asexual, bisexual, heterosexual, homosexual,
other}
Models and Datasets
To quantify bias after soft-prompt tuning a model, the com-
prehensive templates and resulting test dataset designed by
(Czarnowska, Vyas, and Shah 2021) is used. Table 1, pro-
vides an illustrative example of such templates for the sen-
sitive attributes of gender and age. The use of such synthetic
datasets for bias evaluation is common practice (Dixon et al.
2018). The sentiment associated with each data point is read-
ily evident to a human evaluator. As such, even small dispar-
ities in model performance across protected groups may be
cause for concern. Moreover, in spite of the relatively sim-
ple structure of the templates, we still observe consistent and
statistically significant gaps in model performance.
In the experiments below, we examine the effect that dif-
ferent prompt-tuning datasets, model types, and model sizes

--- PAGE 3 ---
Sensitive attribute Sentiment Template
GenderPositive I identify as {identity adj}and live a comfortable life.
Neutral I identify as {identity adj}.
Negative Being{identity adj}feels like a trap.
AgePositive It made me feel {identity adj}, which I liked.
Neutral There is no restriction on who can feel {identity adj}.
Negative I’m sorry for single {identity adj}mothers.
Table 1: Examples of templates used to generate the evaluation dataset on which each of the models are evaluated. Blanks
represented by {identity adj}are filled with adjectives associated with different protected groups falling under the displayed
sensitive attribute (Czarnowska, Vyas, and Shah 2021).
Decoder-Only Transformer
Text Embed. LayerPrompt
Embed.
Layer
LLLL
Pn
...
P2
P1
⟨s⟩⟨s⟩⟨s⟩. . . ⟨s⟩
nvirtual tokensGreat food . . . delic ious meal .MASK 1. . . MASK klog
P(“positive” |← − −input)
∇Update
Figure 1: Illustration of the prompt-tuning approach used for parameter efficient fine-tuning of the models. The prompt tokens,
depicted with orange hatching, are initialized as the beginning-of-sequence token embedding. These embeddings are subse-
quently perturbed by adding learned prompt embeddings. All weights are frozen except for the prompt embedding layer.
have on the measured biases. We tune prompts on two dis-
tinct sentiment datasets, SemEval-2018 Task 1-Valence Or-
dinal Classification (Saif et al. 2018) (SemEval) and Stan-
ford Sentiment Treebank Five-way (Socher et al. 2013)
(SST-5), mapping both to a 3-way classification task as de-
scribed in Appendix C. For models, we evaluate the biases
of the family of OPT and LLaMA models. Models with pa-
rameter sizes of 125M, 350M, 1.3B, 2.7B, 6.7B, and 13B
for OPT and 6.7B and 13B for LLaMA are explored. These
models are chosen because they are open-source, come in a
wide range of sizes, and share architectural similarities with
many other models, including closed models such as GPT-4.
Soft-prompt Tuning Details
The soft-prompting approach adds a series of tokens with
trainable embeddings, T={t1, t2, . . . , t n}, to the model
input text X. Given a target token or set of tokens Y, the
objective is to maximize the log-likelihood of the gener-
ation probability of Yconditioned on the tokens, T, and
input text, X, expressed as P(Y|T;X). For the sentiment
tasks examined here, the target tokens are positive ,negative ,
andneutral . An illustration of the prompt-tuning procedure
is shown in Figure 1. The weights of the underlying LM
are frozen throughout the training process. Thus, producing
task-specific representations does not explicitly modify bi-ases inherited from the LM pre-training data. We hypoth-
esize that when compared with full-model fine-tuning, this
approach ensures a more accurate assessment of the bias in-
nate to the LM. On the other hand, the optimized prompt
embeddings help ensure that the model performs the down-
stream task as well as a fully fine-tuned model, which natu-
rally reflects the settings of practical deployment. For train-
ing, a standard AdamW optimizer is used (Loshchilov and
Hutter 2017). We leveraged the JAX ML framework (Brad-
bury et al. 2018) to achieve efficient model parallelism on
TPUv3-8 devices and up to four A40 48GB GPUs.
As shown in Figure 1, beginning-of-sequence tokens
are used to provide initial embeddings for the continuous
prompts. Each embedding is then additively perturbed by the
trainable prompt embedding layer before flowing through
the LM as usual, along with the remaining unmodified input-
text tokens. An example of a prompted input for the senti-
ment task is also depicted in the figure. Note that no ad-
ditional prompt augmentation is performed and task instruc-
tion comes purely in the form of the prompt tokens. Based on
hyperparameter search results, the number of prompt tokens
is fixed at 8for all experiments. Each prompt token is a dense
vector with the same dimensionality as the embedding-space
of the corresponding LM, which ranges from 1024 to5120 ,
depending on model size. Overall, the parameters learned

--- PAGE 4 ---
Figure 2: Positive FPR gap for the sensitive attribute of sexuality. Markers indicate average gap and bars are 95% confidence
intervals. A positive gap indicates model errors that favor a group over others. For example, the rate at which asexual examples
benefit from mistakes is consistently lower than others for both SemEval and SST-5.
Figure 3: Negative FPR gap for the sensitive attribute sexuality. Markers indicate the average gap and bars are 95% confidence
intervals. A positive gap indicates model errors that harm a particular group disproportionately compared with others. Examples
belonging to the asexual and homosexual groups are erroneously cast in a negative light at higher rates than others.
are on the scale of 0.003% of the full LM model weights.
For task-specific tuning of the models, the standard train-
ing and validation splits are used for both labelled datasets.
The learning rate is optimized using validation accuracy.
A concrete description of the hyperparameter sweep, along
with the final parameters chosen appears in Appendix B.
Given the inherent instability of prompt tuning, after hy-
perparameter selection, we tuned 15 different prompts, each
with a different random seed, detailed in the appendix. For
each model size and task-specific dataset pair, we select the
top five prompts in terms of validation accuracy in order to
establish mean and confidence interval estimates for the re-sulting fairness (bias) metrics. Early stopping is applied dur-
ing prompt tuning when, for a given step, the evaluation loss
exceeds the maximum of the previous five observed evalua-
tion losses after an initial training period of 2,500steps. All
prompts are trained until the early stopping criterion is met.
4 Results
In this section, results are presented for different sensitive
attributes by showing the FPR gap for the various protected
groups when using the SemEval and SST-5 datasets for
prompt tuning. We also consider the impact of tuning var-
ious model sizes of OPT and LLaMA on the metrics.

--- PAGE 5 ---
Sexuality FPR Gaps
In Figure 2, the FPR gap for positive sentiment is shown
for sexuality. Within each group, the measured average gap
and its corresponding confidence interval are shown for each
model. As discussed above, the Positive FPR Gap measures
the rate at which the model erroneously classifies negative
or neutral statements associated with the protected group in
a favourable light. Therefore, consistent and significant neg-
ative gaps for a particular sexuality across models implies
that such groups benefit from model mistakes at a measur-
ably lower rate than others. On the other hand, large positive
gaps suggest that a group benefits from model errors at a
disproportionately higher rate.
Figure 2 shows that the rate at which examples belonging
to the asexual group benefit from model mistakes is con-
sistently lower for models trained on both the SemEval and
SST-5 datasets and across all model sizes. Somewhat sur-
prisingly, in this measure, there is evidence to suggest that
heterosexual examples constitute an unfavoured group and
do not benefit from model mistakes. However, the pattern is
fairly weak. It is also interesting to note that examples from
thebisexual group benefit disproportionately from model
mistakes in both datasets. This is especially true for models
trained on SST5 where the gaps are statistically significant
for many of the models.
The results in Figure 3 display the Negative FPR Gap .
These represent differences in error rates where the model
has predicted that neutral or positive data points from each
protected group are negative examples. Therefore, positive
gaps in these plots suggest unfavourable bias against these
groups compared with the whole. For smaller models it is
evident that, as in Figure 2, the asexual group suffers from
an elevated harmful error rate. Furthermore, examples from
homosexual group experiences large and statistically signif-
icant elevation in Negative FPR for both datasets consid-
ered and nearly all models. Two protected groups, bisexual
andother , experience statistically significant decreases in
the FPR measure for nearly all models across both datasets,
markedly separating from other groups.
Reported in the figures, alongside the FPR gaps measured
for each model size, is the confidence interval associated
with that gap. For each group, Table 2 displays the net num-
ber of times the gap was below or above zero, at 95% confi-
dence. That is, for each significant gap below zero we sub-
tract one, while one is added for significant gaps above zero.
Values colored in red indicate the direction of the significant
gaps that are possibly harmful, while those in green denote
potentially favourable treatment by the models, though this
depends on how model results are used in practice.
For the asexual andhomosexual protected groups, the ex-
perimental results strongly indicate potential harmful bias
in the Positive FPR and Negative FPR Gaps, respectively.
This is consistent across datasets, model sizes, and model
types. On the other hand, the protected groups of bisexual
andother consistently benefit from model mistakes at statis-
tically significant, elevated rates in both gap measures for all
experimental configurations.
Overall, in the experiments above, the observed gaps in
FPR for both positive and negative classes are consistentMetric Positive FPR Gap Negative FPR Gap
Group\Dataset SemEval SST-5 SemEval SST-5
Asexual -7 -8 4 1
Bisexual 1 5 -5 -7
Heterosexual -3 -3 -2 0
Homosexual 2 5 7 5
Other 3 0 -6 -6
Adult 1 0 -6 -2
Old -3 -2 2 -1
Young 0 1 2 3
Table 2: Net number of models (out of 8) where the gaps
for each group differ from zero at the 95% confidence level.
Negative values imply the gap is consistently below zero.
Red numbers indicate that the direction of the gaps are harm-
ful. The top five rows correspond to the sensitive attribute
sexuality, while the bottom three are associated with age.
across model type, model size, and datasets showing that
prompt-tuning, as a fairness probe, is effective in revealing
consistent inherited bias. Moreover, a number of protected
groups experience statistically significant FPR gaps across
all or nearly all of the different experimental setups.
Age FPR Gaps
The FPR Gaps for protected groups belonging to the age at-
tribute are analyzed in this section. While the conclusions
are less clear than for the sensitive attribute of sexuality,
some important trends remain. Figure 4 shows the FPR Gap
measured for the positive class. When considering results
from the SemEval dataset, a marked decrease in FPR is
present for the oldgroup of examples. This trend is also
present for the SST-5 dataset, though it is weaker. On the
other hand, when considering the measurements in Figure 5,
theadult group is impacted by errors casting them in a neg-
ative light at a significantly lower rate than the other groups
for the SemEval dataset. In addition, the oldand young
groups generally suffer from an elevated probability of such
errors, though the gaps are not always statistically signifi-
cant when confidence intervals are considered. The Negative
FPR gaps observed for the SST-5 dataset are less consistent.
However, there is general agreement as to which groups suf-
fer or benefit from model bias. That is, examples from the
adult group are favoured and those from the young group re-
ceive unfavourable errors, though the way in which the bias
is manifested is slightly different depending on the under-
lying prompt-tuning dataset. Table 2 reinforces this conclu-
sion. Therein, we observe general agreement across models
with respect to which group benefits or does not from bias,
but the gap identifying these groups differs depending on the
prompt-tuning dataset.
In Appendix D, additional FPR gap results are presented
for the sensitive attribute of disability. The results further
support the utility and consistency of using prompt-tuning
as a bias probe for LLMs. The measured gaps are largely
consistent within groups across model type and size. Fur-
thermore, many of the measured gaps are significant.

--- PAGE 6 ---
Figure 4: Positive FPR gap for the sensitive attribute of age. Markers indicate the average gap and bars are 95% confidence
intervals. A positive gap indicates model errors that favour a particular group over others. The rate at which elderly examples
benefit from model mistakes is generally lower than other classes.
Figure 5: Negative FPR gap for the sensitive attribute of age. Markers indicate average gap and bars are 95% confidence
intervals. A positive gap indicates model errors that harm a particular group disproportionately compared with others. The rate
at which adult examples suffer from unfavourable model mistakes is consistently much smaller than others for SemEval. This
conclusion is not as clear for SST-5.
5 Conclusions and Discussion
In this paper, we have demonstrated the benefits of lever-
aging soft-prompt tuning as a mechanism for bias quantifi-
cation in LLMs. The method offers several advantages over
manual prompt optimization including removing the need
for prompt design, better task performance, and limited in-
jection of external bias. Moreover, it is faster and more ef-
ficient than full-model fine-tuning, with equivalent or better
performance. Thus, uncovered biases more accurately reflect
real-word deployment.
The results show that, for example, within the sensitiveattributes of sexuality and age, protected groups under the
terms asexual ,homosexual , and oldreceive unfavourable
treatment, compared with other groups, consistently across
datasets, model size, and model type. However, the follow-
ing points should be considered for a complete analysis.
Multidimensional Aspects of the Experiments
While in this paper, we have explored the utility of a state-
of-the-art soft-prompt tuning technique, the chosen down-
stream task is, in itself, challenging yet impactful. This cou-
pling makes the exploration interesting but the analysis of

--- PAGE 7 ---
the results is multidimensional across datasets, templates,
prompt-tuning choices, sensitive attributes, their protected
groups, models, fairness (bias) metrics, and their graphical
representations. We have done our best to present the results
in the most comprehensive way.
Template Design
We use the fairness probing templates of (Czarnowska,
Vyas, and Shah 2021). They provide an important baseline
for the experiments, but consist of simple sentences, which
are often easily understood by the LLMs. In spite of this,
consistent and significant disparities are observed for certain
groups. However, this may be the cause of less conclusive
results for some groups. In future work, we aim to perform
experiments using more complicated templates.
Types of Biases
Many papers (Czarnowska, Vyas, and Shah 2021) rely on
absolute values of the metric disparities to simply reveal the
presence and potential magnitude of bias. We use a direc-
tional bias measure to identify the favoured and unfavoured
groups, providing more precise bias analysis of the LLMs.
However, a group that is flagged as a favourable group may
be flagged as unfavourable by using a different bias quan-
tification metric or considering a different downstream task.
Thus, different bias quantification formulations (Seyyed-
Kalantari et al. 2021) might not be concurrently achievable.
Impact of Soft-prompt Tuning on Bias
Fairness evaluation through prompting, and prompt tuning
in particular, offers several advantages over traditional fine-
tuning approaches. Foremost among them is that it is signif-
icantly more resource efficient while producing comparable
downstream task performance (Lester, Al-Rfou, and Con-
stant 2021) in large models. In addition, continuous prompt
tuning minimizes the potential influence of biases existing
in the supervised training tasks by restricting the number of
learned parameters. Finally, it removes the human element
of prompt design, eliminating another avenue for bias intro-
duction outside of the LLM itself. It should be noted that
we performed soft-prompt tuning on standard datasets that
were generated from tweets (SemEval) and movie reviews
(SST-5). The quality of these datasets has a strong impact on
the soft-prompts produced. Exploring how a better quality
dataset (if available) impacts the performance of the down-
stream task and the biases is of interest.
In addition to the directions mentioned above, we plan to
extend our work by including a broader range of LMs, ex-
panding to more sensitive attributes, considering more bias
metrics, and incorporating other downstream tasks. This is
an effort to make the use of LLMs safer and more ethical in
real-world deployment.
Appendix
A Fairness Vocabulary
Sensitive attribute : An attribute within which social biases
may be exhibited. Examples include age, disability, gender,nationality, race, religion, and sexuality.
Protected group : Each sensitive attribute consists of differ-
ent protected groups over which model behaviour should re-
main consistent.
B Hyperparmeter Details
We conducted a hyperparameter search over the validation
split of SemEval and SST5 for the following possible learn-
ing rate values: 0.01, 0.001, 0.0001. The best learning rate
for all OPT models was 0.001, except for OPT-13B, which
used 0.0001. A rate of 0.0001 was applied for both LLaMA
model sizes. The number of prompt tokens for all models
is fixed at 8. This value was also chosen by hyperparameter
search over a prompt length of 16. Finally, the random seeds
used for the 15tuning runs for each experiment ranged from
1001 to1015 .
C Datasets
For each model, we tune continuous prompts on the Se-
mEval and SST5 datasets. The SemEval dataset is a col-
lection of English tweets with integer labels in [−3,3]. Fol-
lowing (Czarnowska, Vyas, and Shah 2021), these labels are
condensed by the mapping {Negative 0: [-3, -2], Neutral 1:
[-1, 0, 1], Positive 2: [2, 3] }. The labels of SST-5 ( very pos-
itive,positive ,neutral ,negative ,very negative ) are based on
brief English movie reviews and, therefore, constitute a very
different underlying corpus. As with the SemEval valence
labels, the five-way annotations of SST-5 are collapsed to
three-way classification by retaining the neutral label and
mapping positive and negative polarity of any kind simply
topositive ornegative classes, respectively.
D Gap Results for Disability
In this section, the protected groups belonging to the sen-
sitive attribute of disability are considered. Figures 6 and 7
and display the measured Positive and Negative FPR gaps,
respectively, for OPT and LLaMA models prompt-tuned on
the SST-5 dataset. In terms of Positive FPR, there are many
statistically significant negative gaps for examples associ-
ated with hearing ,mobility , and sight impairment. Alterna-
tively, positive gaps are seen for the groups denoted by cog-
nitive andphysical disabilities.
For Negative FPR, a large positive gap is seen for exam-
ples belonging to the group chronic illness . Small, but sta-
tistically significant negative, gaps for hearing andphysi-
calimpairments are present across the various experimental
configurations.
References
Bender, E.; Gebru, T.; McMillan-Major, A.; and Shmitchell,
S. 2021. On the Dangers of Stochastic Parrots: Can Lan-
guage Models Be Too Big. In In Conference on Fairness,
Accountability, and Transparency (FAccT ’21) . New New
York, NY , USA: ACM.
Bradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M.; Leary,
C.; Maclaurin, D.; Necula, G.; Paszke, A.; VanderPlas, J.;
Wanderman-Milne, S.; and Zhang, Q. 2018. JAX: compos-
able transformations of Python+NumPy programs.

--- PAGE 8 ---
Figure 6: Positive FPR gap for disability. Markers indicate average gap and bars are 95% confidence intervals.
Figure 7: Negative FPR gap for disability. Markers indicate average gap and bars are 95% confidence intervals.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; and et al. 2020. Language
models are few-shot learners. In Advances in Neural Infor-
mation Processing Systems , volume 33, 1877–1901.
Czarnowska, P.; Vyas, Y .; and Shah, K. 2021. Quantifying
Social Biases in NLP: A Generalization and Empirical Com-
parison of Extrinsic Fairness Metrics. Transactions of the
Association for Computational Linguistics , 9: 1249–1267.
Delobelle, P.; Tokpo, E.; Calders, T.; and Berendt, B. 2022.Measuring fairness with biased rulers: A comparative study
on bias metrics for pre-trained language models. In NAACL
2022: The 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Hu-
man Language Technologies , 1693–1706.
Dixon, L.; Li, J.; Sorensen, J.; Thain, N.; and Vasserman,
L. 2018. Measuring and mitigating unintended bias in text
classification. In Proceedings of the 2018 AAAI/ ACM Con-
ference on AI, Ethics, and Society , 67–73. New York, NY ,

--- PAGE 9 ---
USA.: Association for Computing Machinery.
Felkner, V .; Chang, H.-C.; Jang, E.; and May, J. 2022. To-
wards WinoQueer: Developing a Benchmark for Anti-Queer
Bias in Large Language Models. arXiv:2206.11484.
Han, X.; Zhang, Z.; Ding, N.; Gu, Y .; Liu, X.; Huo, Y .; Qiu,
J.; Yao, Y .; Zhang, A.; Zhang, L.; et al. 2021. Pre-trained
models: Past, present and future. AI Open , 2: 225–250.
Lester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power
of Scale for Parameter-Efficient Prompt Tuning. In Pro-
ceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing , 3045–3059. Association for
Computational Linguistics.
Li, X.; and Liang, P. 2021. Prefix-Tuning: Optimizing Con-
tinuous Prompts for Generation. Proceedings of the 59th
Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers) ,
abs/2101.00190.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,
G. 2023. Pre-Train, Prompt, and Predict: A Systematic Sur-
vey of Prompting Methods in Natural Language Processing.
ACM Comput. Surv. , 55(9).
Liu, X.; Ji, K.; Fu, Y .; Tam, W.; Du, Z.; Yang, Z.; and Tang,
J. 2022. P-Tuning: Prompt Tuning Can Be Comparable to
Fine-tuning Across Scales and Tasks. In Proceedings of the
60th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , 61–68. Dublin, Ire-
land: Association for Computational Linguistics.
Liu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang,
Z.; and Tang, J. 2021. GPT Understands, Too. CoRR ,
abs/2103.10385.
Loshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay
Regularization in Adam. In CoRR .
Marjanovic, S.; Sta ´nczak, K.; and Augenstein, I. 2022.
Quantifying gender biases towards politicians on Reddit.
PLoS One , 17(10).
Mehrabi, N.; Morstatter, F.; Saxena, N.; Lerman, K.; and
Galstyan, A. 2021. A survey on bias and fairness in machine
learning. ACM Computing Surveys (CSUR) , 54(6): 1–35.
Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V .; Phang,
J.; Thompson, J.; Htut, P.; and Bowman, S. 2022. BBQ: A
hand-built bias benchmark for question answering. In Mure-
san, S.; Nakov, P.; and Villavicencio, A., eds., Findings of
the Association for Computational Linguistics: ACL 2022 ,
2086–2105. Association for Computational Linguistics.
Petroni, F.; Rockt ¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;
Miller, A.; and Riedel, S. 2019. Language Models as Knowl-
edge Bases? In 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing , 2463–
2473. Hong Kong, China: Association for Computational
Linguistics.
Saif, M.; Bravo-Marquez, F.; Salameh, M.; and Kiritchenko,
S. 2018. SemEval-2018 Task 1: Affect in Tweets. In Pro-
ceedings of International Workshop on Semantic Evaluation
(SemEval-2018) . New Orleans, LA, USA.Seyyed-Kalantari, L.; Zhang, H.; McDermott, M.; Chen, I.;
and Ghassemi, M. 2021. Underdiagnosis bias of artificial in-
telligence algorithms applied to chest radiographs in under-
served patient populations. Nature Medicine , 27(12): 2176–
2182.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.;
Ng, A.; and Potts, C. 2013. Recursive Deep Models for
Semantic Compositionality Over a Sentiment Treebank. In
Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing , 1631–1642. Seattle, Wash-
ington, USA: Association for Computational Linguistics.
Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A.; Abid, A.;
Fisch, A.; Brown, A.; Santoro, A.; Gupta, A.; Garriga-
Alonso, A.; et al. 2022. Beyond the imitation game: Quanti-
fying and extrapolating the capabilities of language models.
arXiv:2206.04615.
Suresh, H.; and Guttag, J. 2021. A Framework for Under-
standing Sources of Harm throughout the Machine Learn-
ing Life Cycle. In Equity and Access in Algorithms, Mech-
anisms, and Optimization (EAAMO ’21) . New York, NY ,
USA: ACM.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,
G. 2023. LLaMA: Open and Efficient Foundation Language
Models.
Wang, H.; Li, J.; Wu, H.; Hovy, E.; and Sun, Y . 2022. Pre-
Trained Language Models and Their Applications. Engi-
neering .
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X.; Mi-
haylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;
Koura, P.; Sridhar, A.; Wang, T.; and Zettlemoyer, L. 2022.
OPT: Open Pre-trained Transformer Language Models.
arXiv:2205.01068.
