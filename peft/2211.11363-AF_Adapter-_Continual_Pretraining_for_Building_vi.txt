# AF Adapter: Tiền huấn luyện liên tục để xây dựng
Mô hình ngôn ngữ y sinh học Trung Quốc

Yongyu Yan
Trường Khoa học và Kỹ thuật Thông tin
Đại học Khoa học và Công nghệ Đông Trung Quốc
Thượng Hải, Trung Quốc
y30221069@mail.ecust.edu.cn

Kui Xue
Phòng thí nghiệm AI Thượng Hải
Thượng Hải, Trung Quốc
xuekui@pjlab.org.cn

Xiaoming Shi
Phòng thí nghiệm AI Thượng Hải
Thượng Hải, Trung Quốc
shixiaoming@pjlab.org.cn

Qi Ye
Trường Khoa học và Kỹ thuật Thông tin
Đại học Khoa học và Công nghệ Đông Trung Quốc
Thượng Hải, Trung Quốc
yeh qi1125@ecust.edu.cn

Jingping Liu
Trường Khoa học và Kỹ thuật Thông tin
Đại học Khoa học và Công nghệ Đông Trung Quốc
Thượng Hải, Trung Quốc
jingpingliu@ecust.edu.cn

Tong Ruan
Trường Khoa học và Kỹ thuật Thông tin
Đại học Khoa học và Công nghệ Đông Trung Quốc
Thượng Hải, Trung Quốc
ruantong@ecust.edu.cn

Tóm tắt—Tiền huấn luyện liên tục là một cách phổ biến để xây dựng mô hình ngôn ngữ tiền huấn luyện chuyên biệt cho miền từ một mô hình ngôn ngữ miền tổng quát. Mặc dù có hiệu quả cao, tiền huấn luyện liên tục gặp phải vấn đề quên thảm khốc, có thể gây hại đến hiệu suất của mô hình trong các nhiệm vụ xuôi dòng. Để giảm thiểu vấn đề này, trong bài báo này, chúng tôi đề xuất một phương pháp tiền huấn luyện liên tục cho mô hình dựa trên BERT, được gọi là Attention-FFN Adapter. Ý tưởng chính của nó là giới thiệu một số lượng nhỏ các đầu chú ý và đơn vị ẩn bên trong mỗi lớp tự chú ý và mạng truyền thẳng. Hơn nữa, chúng tôi huấn luyện một mô hình ngôn ngữ chuyên biệt cho miền được gọi là AF Adapter dựa trên RoBERTa cho miền y sinh học Trung Quốc. Trong các thí nghiệm, các mô hình được áp dụng cho các nhiệm vụ xuôi dòng để đánh giá. Kết quả chứng minh rằng chỉ với khoảng 17% tham số mô hình được huấn luyện, AF Adapter đạt được cải thiện hiệu suất trung bình 0.6%, 2% so với các baseline mạnh. Kết quả thí nghiệm bổ sung cho thấy phương pháp của chúng tôi giảm thiểu vấn đề quên thảm khốc 11% so với phương pháp tinh chỉnh. Mã nguồn có sẵn tại https://github.com/yanyongyu/AF-Adapter.

Từ khóa chỉ mục—Tiền huấn luyện liên tục, xử lý ngôn ngữ tự nhiên y sinh học Trung Quốc, điều chỉnh Adapter

I. GIỚI THIỆU

Hiện tại, một khối lượng lớn tài liệu y sinh học tiếng Trung xuất hiện, bao gồm các cuộc đối thoại chẩn đoán y tế trên các cộng đồng y tế Trung Quốc và kiến thức y tế trong các bách khoa toàn thư tiếng Trung. Ví dụ, DXY1 (một cộng đồng y tế Trung Quốc) chứa hàng triệu cuộc đối thoại y tế giữa 5.5 triệu người dùng bệnh nhân và 2 triệu người dùng bác sĩ, trong khi Baidu Encyclopedia2 (một bách khoa toàn thư tiếng Trung) bao gồm hơn 30 triệu bài viết, bao phủ một loạt các chủ đề liên quan đến sức khỏe, bệnh tật và điều trị y tế. Kết quả là, có nhu cầu ngày càng tăng về các công cụ khai thác văn bản y sinh học tiếng Trung chính xác để trích xuất thông tin có giá trị từ tài liệu y tế tiếng Trung một cách hiệu quả.

Các phương pháp xử lý ngôn ngữ tự nhiên (NLP) cải thiện đáng kể việc khai thác văn bản tự động từ tài liệu tiếng Trung một cách chính xác. Các phương pháp này đã trải qua nhiều giai đoạn, bao gồm các thuật toán dựa trên kiến thức chuyên gia quy mô nhỏ [1], các thuật toán học máy nông [2], và các thuật toán học sâu [3] [4]. Trong số các thuật toán học sâu này, gần đây, các mô hình ngôn ngữ tiền huấn luyện (PLM) [5] [6] đã trở nên ngày càng phổ biến, vì chúng thể hiện hiệu suất ấn tượng. Cụ thể, các PLM được huấn luyện trên một lượng lớn dữ liệu văn bản để học các biểu diễn tốt hơn của ngôn ngữ tự nhiên, cung cấp nền tảng vững chắc cho các nhiệm vụ NLP xuôi dòng.

Tuy nhiên, việc áp dụng trực tiếp các PLM trên các miền tổng quát cho khai thác văn bản y sinh học gặp phải vấn đề thiên lệch phân bố từ giữa văn bản tổng quát và y sinh học, điều này làm tổn hại hiệu suất. Để giảm thiểu vấn đề này, các PLM trong miền y sinh học được giới thiệu. Các phương pháp PLM trên các miền cụ thể được chia thành hai cách: tiền huấn luyện từ đầu [7] và tiền huấn luyện liên tục [8] dựa trên các mô hình ngôn ngữ miền tổng quát. Tiền huấn luyện từ đầu có nghĩa là huấn luyện trực tiếp trên một kho ngữ liệu chuyên biệt với một từ vựng chuyên biệt. Đồng thời, tiền huấn luyện liên tục đầu tiên khởi tạo với các PLM trên miền tổng quát và sau đó tiếp tục tiền huấn luyện trên kho ngữ liệu chuyên biệt cho miền. So với tiền huấn luyện từ đầu, tiền huấn luyện liên tục có lợi từ hiệu quả huấn luyện cao hơn. Trong công việc này, cách tiền huấn luyện liên tục được áp dụng.

Mặc dù có hiệu quả huấn luyện cao, tiền huấn luyện liên tục gặp phải vấn đề quên thảm khốc [9] [10] [11]. Quên thảm khốc có nghĩa là mô hình được huấn luyện trên kho ngữ liệu mới có xu hướng quên đi kiến thức của dữ liệu trước đó. Ví dụ, trong Hình 1, RoBERTa được tinh chỉnh gặp phải quên thảm khốc, mắc lỗi khi dự đoán các từ bị che. Quên thảm khốc kiến thức thông thường làm tổn hại hiệu suất.

Để giảm thiểu vấn đề này, có ba cách tiếp cận chính. Cách thứ nhất là các phương pháp dựa trên huấn luyện, bao gồm tinh chỉnh phân biệt, tỷ lệ học tập tam giác xiên và giải phong dần dần [12]. Cách thứ hai là dự trữ tham số [13], giữ lại các tham số tiền huấn luyện được chọn ngẫu nhiên. Cách thứ ba là các phương pháp điều chỉnh dựa trên adapter [14] [15], chèn các lớp bổ sung sau các lớp cụ thể của các mô hình tiền huấn luyện. Các phương pháp điều chỉnh dựa trên adapter giảm thiểu vấn đề quên tốt hơn so với các phương pháp khác, do đó trở nên phổ biến gần đây.

Mặc dù có hiệu suất tốt, cách thêm lớp của các phương pháp điều chỉnh dựa trên adapter hiện tại thêm độ sâu vào mạng. Đầu vào cần được truyền thẳng đến nhiều lớp hơn và do đó có nhiều khả năng bị quên hơn. Để giảm thiểu vấn đề này, chúng tôi đề xuất một phương pháp tiền huấn luyện liên tục mở rộng lớp, được gọi là Attention-FFN Adapter (AF Adapter), mở rộng các ma trận chú ý và mạng truyền thẳng (FFN). Cụ thể, ý tưởng cơ bản của nó là giới thiệu một số lượng nhỏ các đầu và đơn vị ẩn bên trong mỗi lớp tự chú ý và FFN của BERT. Trong phương pháp này, chỉ các tham số được thêm vào là có thể huấn luyện được, và các tham số tiền huấn luyện ban đầu được đóng băng. Phương pháp mở rộng lớp này không tăng độ sâu lớp mạng, do đó giảm thiểu vấn đề quên thông tin đầu vào. Sau đó, AF Adapter dựa trên RoBERTa được thu được bằng cách tinh chỉnh một mô hình ngôn ngữ tiền huấn luyện trên miền y tế với AF Adapter. Để ước tính tính khả thi của AF Adapter, AF Adapter dựa trên RoBERTa được so sánh với các mô hình khác trong các nhiệm vụ NLP xuôi dòng. Kết quả thí nghiệm chứng minh rằng chỉ với 17% tham số mô hình được huấn luyện, AF Adapter dựa trên RoBERTa đạt được cải thiện hiệu suất trung bình 0.6%, 2% so với các mô hình tiên tiến.

Đóng góp. Các đóng góp trong bài báo này được tóm tắt như sau:
• Chúng tôi đề xuất một phương pháp tiền huấn luyện liên tục mở rộng lớp được gọi là AF Adapter (Attention-FFN Adapter), nhằm mở rộng các đầu chú ý và đơn vị ẩn cho mô hình dựa trên BERT. Phương pháp này tiếp tục giảm thiểu quên thảm khốc so với các phương pháp thêm lớp.
• Chúng tôi đề xuất một mô hình ngôn ngữ tiền huấn luyện miền y sinh học Trung Quốc được gọi là AF Adapter dựa trên RoBERTa, được huấn luyện sử dụng AF Adapter và kho ngữ liệu y sinh học. AF Adapter dựa trên RoBERTa đóng góp lớn cho các nhiệm vụ y tế xuôi dòng.
• Phương pháp của chúng tôi đạt được cải thiện hiệu suất trung bình 0.6%, 2% so với các baseline mạnh trên benchmark CBLUE. Ngoài ra, AF Adapter giảm thiểu vấn đề quên thảm khốc 11% so với phương pháp tinh chỉnh.

II. CÔNG VIỆC LIÊN QUAN

Trong phần này, chúng tôi cung cấp tổng quan ngắn gọn về các cách tiếp cận hiện có trong các lĩnh vực mô hình hóa ngôn ngữ và tiền huấn luyện chuyên biệt cho miền. Chúng tôi trình bày tóm tắt các phương pháp và kỹ thuật khác nhau đã được sử dụng trong các lĩnh vực này.

A. Mô hình hóa ngôn ngữ

Trong xử lý ngôn ngữ tự nhiên, tiền huấn luyện các mô hình ngôn ngữ với lượng lớn dữ liệu không được chú thích đã được chứng minh là một chiến lược thành công cho học chuyển giao. Việc học các biểu diễn ngôn ngữ tổng quát từ các mô hình ngôn ngữ và chuyển giao mô hình cho các nhiệm vụ xuôi dòng cụ thể bằng cách tinh chỉnh là hiệu quả.

Các mô hình ngôn ngữ thường được huấn luyện trên các nhiệm vụ tiền huấn luyện cụ thể sử dụng kho ngữ liệu bách khoa toàn thư và tin tức. Các nhiệm vụ tiền huấn luyện rất quan trọng để học các biểu diễn ngôn ngữ tổng quát. Đối với tiền huấn luyện tự giám sát, các nhiệm vụ được sử dụng rộng rãi như sau:

Mô hình hóa ngôn ngữ (LM) là một vấn đề ước tính mật độ xác suất cổ điển, được sử dụng trong ví dụ GPT-2 [16], ULMFiT [12] và SiATL [17]. Token đầu ra tiếp theo phụ thuộc vào xác suất kết hợp của các token trước đó. Nhược điểm của mô hình hóa ngôn ngữ một chiều là biểu diễn của mỗi token chỉ mã hóa các token ngữ cảnh bên trái. Tuy nhiên, các biểu diễn ngữ cảnh tốt hơn của văn bản nên mã hóa thông tin ngữ cảnh từ cả hai hướng.

Mô hình hóa ngôn ngữ có mặt nạ (MLM) là một nhiệm vụ tiền huấn luyện để khắc phục nhược điểm của mô hình hóa ngôn ngữ một chiều. MLM đầu tiên che một số token trong văn bản đầu vào và sau đó huấn luyện mô hình để dự đoán các token bị che. MLM được sử dụng trong BERT [5], MASS [18], T5 [19], RoBERTa [6], v.v.

Mô hình hóa ngôn ngữ hoán vị (PLM) là một nhiệm vụ mô hình hóa ngôn ngữ trên một hoán vị ngẫu nhiên của văn bản đầu vào, được sử dụng bởi XLNet [20], BART [21]. Họ quan sát thấy rằng một số token đặc biệt, chẳng hạn như [MASK], không có mặt trong các nhiệm vụ xuôi dòng. Để thu hẹp khoảng cách giữa tiền huấn luyện và tinh chỉnh, mô hình được huấn luyện để dự đoán một số token trong chuỗi token hoán vị, phụ thuộc vào phần còn lại của các token.

Học tương phản (CTL) giả định một số cặp văn bản quan sát được có ngữ nghĩa tương tự hơn so với văn bản được lấy mẫu ngẫu nhiên. Các nhiệm vụ CTL phổ biến bao gồm Dự đoán câu tiếp theo (NSP) [5] [22], Dự đoán thứ tự câu (SOP) [23] [24].

B. Tiền huấn luyện chuyên biệt cho miền

Các miền chuyên biệt như y sinh học đặt ra thách thức cho các mô hình ngôn ngữ tiền huấn luyện miền tổng quát do các lý do sau:
• Các mô hình ngôn ngữ tiền huấn luyện miền tổng quát được huấn luyện trên các tập dữ liệu chủ yếu có nguồn gốc từ bách khoa toàn thư và tin tức, khiến việc ước tính hiệu suất của chúng trên văn bản miền y sinh học trở nên khó khăn.
• Phân bố từ khác nhau đáng kể giữa văn bản miền tổng quát và văn bản miền y sinh học, bao gồm sự hiện diện của các thuật ngữ y sinh học.
• Văn bản y sinh học trong y sinh học Trung Quốc thể hiện các kết hợp và cấu trúc cụm từ phức tạp.

Để giải quyết những thách thức này, các nhà nghiên cứu đã đề xuất các nghiên cứu tập trung vào tiền huấn luyện chuyên biệt cho miền y sinh học, có thể được phân loại thành hai loại chính.

Tiền huấn luyện chuyên biệt cho miền từ đầu dựa trên các kho ngữ liệu y sinh học quy mô lớn. Ví dụ, PubMedBERT [7] và BioGPT [25] được huấn luyện trên các kho ngữ liệu dựa trên PubMed, chứa 3.1 tỷ từ và 15 triệu mục tương ứng. Một lợi thế đáng chú ý của tiền huấn luyện chuyên biệt cho miền từ đầu là khả năng tùy chỉnh từ vựng của mô hình cho miền. Điều này cho phép tokenization phù hợp hơn của các thuật ngữ y tế, tránh các biểu diễn subword bị phân mảnh.

Tiền huấn luyện liên tục của một mô hình tiền huấn luyện miền tổng quát là một cách tiếp cận phổ biến để tiền huấn luyện một mô hình y sinh học. Cách tiếp cận này bao gồm khởi tạo với một mô hình tiêu chuẩn được tiền huấn luyện trên các kho ngữ liệu bách khoa toàn thư và tin tức và sau đó tiếp tục quá trình tiền huấn luyện sử dụng các kho ngữ liệu y sinh học. Tiền huấn luyện liên tục có lợi từ kiến thức thu được trong quá trình tiền huấn luyện miền tổng quát. Các nghiên cứu gần đây đã chứng minh rằng việc tiêm thêm thông tin kiến thức có thể nâng cao mô hình [26], chẳng hạn như thu nhận kiến thức [27]. Tuy nhiên, huấn luyện nhiệm vụ tuần tự có thể dẫn đến quên thảm khốc, nơi mô hình quên kiến thức miền tổng quát [9] [10] [11]. BioBERT là một ví dụ của cách tiếp cận này, trong đó tiền huấn luyện liên tục được khởi tạo với các trọng số từ BERT và được tiến hành sử dụng kho ngữ liệu PubMed [8]. Trong bối cảnh Trung Quốc, PCL-MedBERT được huấn luyện dựa trên BERT sử dụng văn bản y sinh học và kho ngữ liệu QA y tế.

III. KIẾN THỨC CHUẨN BỊ

Trong phần này, chúng tôi cung cấp đánh giá ngắn gọn về hai thành phần chính trong kiến trúc BERT: cơ chế tự chú ý đa đầu và mạng truyền thẳng. Chúng tôi giải thích ngắn gọn vai trò và chức năng của chúng trong mô hình BERT.

A. BERT

Kiến trúc mô hình BERT được xây dựng dựa trên cấu trúc mã hóa Transformer hai chiều đa lớp [28]. Mỗi bộ mã hóa bao gồm một chồng các khối giống hệt nhau kết hợp tự chú ý đa đầu và mạng truyền thẳng. Lớp tự chú ý đa đầu có thể được công thức hóa như sau:

Q(x) = xW_Q + b_Q,
K(x) = xW_K + b_K,
V(x) = xW_V + b_V,  (1)

trong đó W_Q ∈ R^{d_{model} × hd_k}, W_K ∈ R^{d_{model} × hd_k}, W_V ∈ R^{d_{model} × hd_v}, b_Q ∈ R^{d_k}, b_K ∈ R^{d_k} và b_V ∈ R^{d_v} biểu diễn các ma trận trọng số và bias. d_k = d_v = d_{model}/h, d_{model} là chiều mô hình, và h là số lượng đầu chú ý.

Q, K, và V sau đó được chia thành h phần để tính toán chú ý đa đầu. Điều này có thể được biểu diễn như:

head_j = Attention(Q_j, K_j, V_j),
MultiHead(x) = [head_1, ..., head_h]W_O,  (2)

trong đó Q = [Q_1 : ... : Q_h], K = [K_1 : ... : K_h], V = [V_1 : ... : V_h] và W_O ∈ R^{hd_v × d_{model}}. Phép toán ":" đại diện cho phép nối theo cột.

Sau chú ý đa đầu, mạng truyền thẳng được sử dụng, bao gồm hai phép biến đổi tuyến tính với một hàm kích hoạt GeLU ở giữa:

FFN(x) = GeLU(xW_1 + b_1)W_2 + b_2,  (3)

trong đó W_1 ∈ R^{d_{model} × d_{ff}}, W_2 ∈ R^{d_{ff} × d_{model}}, b_1 ∈ R^{d_{ff}} và b_2 ∈ R^{d_{model}} biểu diễn các ma trận trọng số và bias được áp dụng cho đầu vào x trong mạng truyền thẳng.

IV. PHƯƠNG PHÁP LUẬN

Trong phần này, AF Adapter được giới thiệu để huấn luyện các PLM chuyên biệt cho miền dựa trên các PLM miền tổng quát. Kiến trúc tổng thể của AF Adapter được minh họa trong Hình 2.

A. Kiến trúc mô hình

Theo điều chỉnh adapter [14] [29], để giảm thiểu quên thảm khốc trong giai đoạn tinh chỉnh, các tham số chuyên biệt cho miền có thể huấn luyện bổ sung được chèn vào phần mở rộng của lớp chú ý và FFN, trong khi các tham số trong mô hình ban đầu được cố định để bảo tồn kiến thức của miền tổng quát.

Lớp chú ý mở rộng. Một thành phần chính của BERT là cơ chế tự chú ý tích vô hướng có tỷ lệ đa đầu. Trong công việc này, lớp tự chú ý được mở rộng để giảm thiểu quên thảm khốc. Cụ thể, như được hiển thị trong Hình 3, một số "đầu chuyên biệt cho miền" trong mỗi lớp chú ý được thêm vào. Lưu ý rằng các đầu chuyên biệt cho miền được thêm vào trong các lớp khác nhau độc lập với nhau. Để mở rộng cơ chế tự chú ý, i đầu chú ý bổ sung được thêm vào. Chính thức, đầu vào vào lớp tự chú ý đa đầu được ký hiệu là x. x đầu tiên được đưa vào các lớp tuyến tính, và được ánh xạ vào Q', K', V'. Các ma trận trọng số mở rộng được ký hiệu là [W_Q : W'_Q], [W_K : W'_K], và [W_V : W'_V]. W_Q, W_K, và W_V là các ma trận từ mô hình tiền huấn luyện ban đầu, như được mô tả trong Phương trình 1. Sau đó, Q'(x) ∈ R^{d_{model} × (h+i)d_k}, K'(x) ∈ R^{d_{model} × (h+i)d_k}, V'(x) ∈ R^{d_{model} × (h+i)d_v} được thu được,

Q'(x) = x[W_Q : W'_Q] + [b_Q : b'_Q],
K'(x) = x[W_K : W'_K] + [b_K : b'_K],
V'(x) = x[W_V : W'_V] + [b_V : b'_V],  (4)

trong đó các phép chiếu là các ma trận tham số W'_Q ∈ R^{d_{model} × id_k}, W'_K ∈ R^{d_{model} × id_k}, W'_V ∈ R^{d_{model} × id_v}, b'_Q ∈ R^{id_k}, b'_K ∈ R^{id_k} và b'_V ∈ R^{id_v}. d_k, d_v, d_{model}, h là các siêu tham số được mô tả trong Phương trình 1.

Sau đó, Q', K' và V' được chia thành h+i phần để tính toán chú ý đa đầu,

head'_j = Attention(Q'_j, K'_j, V'_j),
trong đó Q' = [Q_1 : ... : Q_h : Q'_1 : ... : Q'_i],
K' = [K_1 : ... : K_h : K'_1 : ... : K'_i],
V' = [V_1 : ... : V_h : V'_1 : ... : V'_i].  (5)

Cuối cùng, các đầu ra của các đầu này được nối với nhau và sau đó được đưa vào các phép biến đổi tuyến tính,

MultiHead(x) = [head_1, ..., head_h, head'_1, ..., head'_i][W_O ⊥ W'_O],  (6)

trong đó W'_O ∈ R^{(d_k × i) × d_{model}}, và head_j là đầu thứ j được mô tả trong Phương trình 2. Phép toán "⊥" biểu diễn phép nối theo hàng.

Mạng truyền thẳng mở rộng. Đối với FFN trong mô hình, một perceptron đa lớp với một lớp ẩn, chúng tôi thêm một số đơn vị ẩn trong mỗi lớp FFN với kích hoạt GeLU giữa hai phép biến đổi tuyến tính, như được hiển thị trong Hình 4. Chính thức, chúng tôi thêm "đơn vị ẩn chuyên biệt cho miền" có kích thước a bằng cách mở rộng W_1 và W_2 thành [W_1 : W'_1] và [W_2 ⊥ W'_2],

FFN(x) = GeLU(x[W_1 : W'_1] + [b_1 : b'_1])[W_2 ⊥ W'_2] + b_2 + b'_2,  (7)

trong đó W'_1 ∈ R^{d_{model} × a}, b'_1 ∈ R^a, W'_2 ∈ R^{a × d_{model}}, và b'_2 ∈ R^{d_{model}}. W_1, W_2, b_1 và b_2 là các ma trận được mô tả trong Phương trình 3.

B. Chi tiết tiền huấn luyện

Nhiệm vụ tiền huấn luyện. Để liên tục tiền huấn luyện mô hình dựa trên BERT, mô hình hóa ngôn ngữ có mặt nạ (MLM) được sử dụng để tiền huấn luyện mô hình. Trong MLM, một tập con của các token đầu vào được thay thế ngẫu nhiên bằng một token đặc biệt (ví dụ, [MASK]), và MLM được thiết kế để dự đoán các token này. Mục tiêu huấn luyện là mất mát entropy chéo giữa các token ban đầu và các token được dự đoán. Giống như BERT và RoBERTa, 15% các token đầu vào được chọn, trong số đó 80% ngẫu nhiên được thay thế bằng [MASK], 10% được để nguyên, và 10% được thay thế ngẫu nhiên bằng một token từ từ vựng.

Che toàn bộ từ. Trong BERT ban đầu, văn bản được chia thành các từ phụ và được tokenize thành các token. Việc che toàn bộ từ giảm thiểu nhược điểm của việc chỉ che một phần của toàn bộ từ. Đối với việc che toàn bộ từ trong tiếng Trung, công cụ phân đoạn từ tiếng Trung truyền thống "Jieba" được sử dụng để chia câu thành nhiều từ và cung cấp thông tin từ bổ sung cho bộ thu thập dữ liệu trước khi che.

Chiến lược huấn luyện. Tinh chỉnh với các kho ngữ liệu chuyên biệt cho miền là một phương pháp tiêu chuẩn để liên tục tiền huấn luyện các mô hình miền tổng quát. Tuy nhiên, phương pháp này gặp phải vấn đề quên thảm khốc [9].

Để giảm thiểu vấn đề này, chúng tôi đầu tiên đạt được tiến bộ trong kiến trúc mô hình và không huấn luyện tất cả các tham số trong mô hình. Chỉ các tham số chuyên biệt cho miền được thêm vào trong tự chú ý và FFN (W'_Q, W'_K, W'_V, W'_O, W'_1, W'_2) được huấn luyện và tất cả các tham số kế thừa từ mô hình miền tổng quát được đóng băng.

Các kho ngữ liệu được tiền xử lý thành tập dữ liệu trước khi huấn luyện. Các đoạn văn được chia thành câu bằng các dấu phân cách, và các câu được nhóm theo độ dài chuỗi tối đa của mô hình.

V. THỰC NGHIỆM

Trong phần này, chúng tôi cung cấp chi tiết về thiết lập thí nghiệm của chúng tôi và trình bày kết quả thu được từ đánh giá. Chúng tôi tiền huấn luyện các mô hình chuyên biệt cho miền sử dụng các tập dữ liệu được thu thập và sau đó đánh giá chúng trên các nhiệm vụ xuôi dòng từ benchmark CBLUE [30]. Hơn nữa, chúng tôi tiến hành nghiên cứu loại bỏ về các kỹ thuật tiền huấn luyện khác nhau và điều tra hội tụ và ổn định. Ngoài ra, chúng tôi thực hiện phân tích quên thảm khốc để đánh giá khả năng giữ lại kiến thức đã học trước đó của các mô hình khi được huấn luyện trên các nhiệm vụ mới.

A. Dữ liệu và cài đặt tiền huấn luyện

Tập dữ liệu. Để đánh giá hiệu suất của phương pháp chúng tôi, chúng tôi thu thập nhiều kho ngữ liệu y sinh học tiếng Trung. Các kho ngữ liệu này bao gồm hỏi đáp y sinh học tiếng Trung, bách khoa toàn thư y tế tiếng Trung từ Baidu Encyclopedia và Wikipedia, và Hồ sơ y tế điện tử từ Trung tâm ung thư Thượng Hải Đại học Fudan. Chi tiết về các kho ngữ liệu tiền huấn luyện được sử dụng cho các thí nghiệm của chúng tôi được trình bày trong Bảng I. Chúng tôi sử dụng các kho ngữ liệu này để tiền huấn luyện mô hình của chúng tôi trước khi tinh chỉnh trên các nhiệm vụ đánh giá. Bằng cách tận dụng các tập dữ liệu đa dạng và toàn diện này, chúng tôi nhằm cải thiện khả năng hiểu văn bản ngôn ngữ tự nhiên trong miền y sinh học của mô hình.

Backbone. Chúng tôi thực hiện các thí nghiệm dựa trên mô hình Chinese RoBERTa-wwm-ext-base [31], được lấy từ HuggingFace Hub. Cụ thể, mô hình ban đầu bao gồm 12 lớp mã hóa transformer, mỗi lớp có kích thước ẩn 768. Mỗi lớp cũng bao gồm 12 đầu tự chú ý và một mạng truyền thẳng với kích thước trung gian 3,072. Chúng tôi thêm một đầu chú ý bổ sung vào mỗi lớp và tăng kích thước trung gian của mạng truyền thẳng thêm 1,024. Chúng tôi chuyển tất cả các tham số mô hình từ mô hình Chinese RoBERTa-wwm-ext-base sang kiến trúc đã sửa đổi của chúng tôi.

Cài đặt tiền huấn luyện. Chúng tôi tiền huấn luyện mô hình của chúng tôi sử dụng từ vựng ban đầu được cung cấp với mô hình Chinese RoBERTa. Chúng tôi sử dụng bộ tối ưu hóa AdamW với warm-up và weight decay. Cụ thể, chúng tôi bắt đầu với tỷ lệ học tập bằng không, tăng tuyến tính lên tỷ lệ đỉnh 4×10^-4 trong 1000 bước đầu tiên của huấn luyện. Tỷ lệ học tập sau đó giảm tuyến tính về không trong các bước còn lại. Chúng tôi huấn luyện mô hình trong 100,000 bước sử dụng tổng kích thước batch 512, trải rộng trên hai GPU NVIDIA A100 (80G) với kích thước batch 64 và các bước tích lũy gradient 4. Chúng tôi sử dụng kỹ thuật che toàn bộ từ tiếng Trung (WWM) trong quá trình tiền huấn luyện, với tỷ lệ che 15% và độ dài chuỗi tối đa 512 token.

Baseline. Các mô hình baseline được sử dụng trong thí nghiệm của chúng tôi có kích thước tương tự và đã được sử dụng rộng rãi. Để so sánh các PLM, các mô hình bao gồm BERT-base [5], BERT-wwm-ext-base [31], RoBERTa-wwm-ext-base [31], PCL-MedBERT, và MacBERT-base [32]. Tất cả các mô hình này đã trải qua tiền huấn luyện trên các kho ngữ liệu quy mô lớn. Để so sánh các kỹ thuật tiền huấn luyện, các baseline bao gồm Fine-Tuning, FL-Tuning [29], và LoRA [33]. Chúng tôi so sánh hiệu suất của mô hình chúng tôi với các baseline này để đánh giá hiệu quả của nó.

B. Nhiệm vụ đánh giá

Chúng tôi so sánh các mô hình bằng cách áp dụng chúng cho các nhiệm vụ NLP xuôi dòng, cụ thể là benchmark Chinese Biomedical Language Understanding Evaluation (CBLUE [30]). Như được hiển thị trong Bảng II, benchmark chia tám nhiệm vụ thành sáu loại: nhận diện thực thể có tên y tế (NER), trích xuất mối quan hệ y tế (RE), chuẩn hóa chẩn đoán y tế (NORM), tương tự văn bản y tế (TS), phân loại văn bản (TC), và suy luận ngôn ngữ tự nhiên (NLI).

• Nhận diện thực thể có tên (NER) là nhiệm vụ gắn thẻ các thực thể trong văn bản với loại tương ứng của chúng. Nhiệm vụ CMeEE cung cấp một lược đồ được định nghĩa trước bao gồm chín loại thực thể.

• Trích xuất mối quan hệ (RE) nhằm trích xuất các mối quan hệ ngữ nghĩa giữa hai hoặc nhiều thực thể của một loại nhất định từ văn bản không có cấu trúc thành một số loại ngữ nghĩa. Có 53 quan hệ được định nghĩa trong nhiệm vụ CMeIE, bao gồm 10 mối quan hệ phụ đồng nghĩa và 43 mối quan hệ phụ khác.

• Chuẩn hóa từ vựng (NORM) là nhiệm vụ biến đổi một văn bản không chuẩn thành một thanh ghi chuẩn. Về mặt lâm sàng, có thể có đến hàng trăm từ đồng nghĩa khác nhau cho cùng một chẩn đoán, triệu chứng, hoặc thủ tục. Nhiệm vụ này nhằm tìm các cụm từ chuẩn cho thuật ngữ lâm sàng đã cho.

• Tương tự văn bản (TS) xử lý việc xác định mức độ tương tự ngữ nghĩa giữa hai đoạn văn bản. Nhiệm vụ CHIP-STS nhằm đánh giá khả năng tổng quát hóa giữa các loại bệnh trên dữ liệu câu hỏi và câu trả lời bệnh tiếng Trung.

• Phân loại văn bản (TC) bao gồm phân loại văn bản ngắn đơn giản và phân loại ý định câu. Đối với CHIP-CTC, nhiệm vụ là phân loại tiêu chí đủ điều kiện của các thử nghiệm lâm sàng. Đối với KUAKE-QIC, nhiệm vụ là phân loại mỗi câu thành một trong 11 loại ý định y tế được định nghĩa trước.

• Suy luận ngôn ngữ tự nhiên (NLI) là nhiệm vụ xác định xem một giả thuyết là đúng (kéo theo) hay sai (mâu thuẫn) hay không xác định (trung tính) dựa trên một tiền đề. Đối với các nhiệm vụ KUAKE-QTR và KUAKE-QQR, tập dữ liệu được sử dụng để ước tính sự liên quan giữa truy vấn và tiêu đề hoặc truy vấn khác.

Đối với nhiệm vụ nhận diện thực thể có tên, chúng tôi sử dụng ký hiệu BIO [34], phân biệt phần đầu (B), phần bên trong (I) của các thực thể, và phần bên ngoài (O) của các thực thể. Đối với nhiệm vụ trích xuất mối quan hệ, chúng tôi chia nó thành hai bước: đầu tiên, chúng tôi nhận diện các thực thể chủ thể và đối tượng và sau đó trích xuất các mối quan hệ ngữ nghĩa giữa chúng. Đối với nhiệm vụ chuẩn hóa từ vựng, chúng tôi sử dụng phương pháp recall và ranking. Đối với các nhiệm vụ khác, chúng tôi sử dụng kiến trúc phân loại đơn giản để dự đoán kết quả.

Quá trình đánh giá tương tự như bộ công cụ benchmark CBLUE [30], và chúng tôi tận dụng các siêu tham số được cung cấp bởi baseline RoBERTa của CBLUE.

C. Kết quả chính

Trong phần này, chúng tôi trình bày phân tích so sánh của các PLM và kỹ thuật tiền huấn luyện khác nhau. Chúng tôi tóm tắt các phát hiện chính và làm nổi bật sự khác biệt hiệu suất được quan sát giữa các mô hình và kỹ thuật.

1) So sánh các PLM: Để so sánh, chúng tôi sử dụng kết quả công khai của CBLUE về baseline BERT-base, BERT-wwm-ext-base, RoBERTa-wwm-ext-base, PCL-MedBERT, và MacBERT-base. Bảng III hiển thị hiệu suất của các nhiệm vụ NLP tiếng Trung của benchmark CBLUE.

Đối với nhiệm vụ NER, AF Adapter dựa trên RoBERTa của chúng tôi đạt điểm số 62.984%, vượt trội so với các mô hình khác. Điều này chứng minh khả năng trích xuất chính xác các thực thể y sinh học từ văn bản không có cấu trúc của nó.

Trong nhiệm vụ RE, chúng tôi quan sát thấy AF Adapter dựa trên RoBERTa vượt trội so với mô hình backbone khoảng 1.2%, cũng vượt trội so với các mô hình khác. Điều này làm nổi bật hiệu quả của nó trong việc trích xuất các mối quan hệ ngữ nghĩa giữa các thực thể của một loại nhất định, chứng minh hiệu suất vượt trội của nó trong trích xuất thông tin y sinh học.

Về các nhiệm vụ NORM, TS, và TC, AF Adapter dựa trên RoBERTa đạt hiệu suất tương tự so với mô hình backbone. Đáng chú ý, mô hình của chúng tôi vượt trội so với các mô hình khác trong nhiệm vụ QIC, thể hiện lợi thế của nó trong việc phân loại chính xác các ý định y tế.

Trong các nhiệm vụ NLI, mô hình của chúng tôi đạt hiệu suất tốt nhất, vượt trội so với mô hình backbone khoảng 1.9%. Điều này biểu thị khả năng xác định các mối quan hệ logic giữa truy vấn và tiêu đề của nó, làm nổi bật sự vượt trội của nó trong việc giải quyết các thách thức cho các công cụ tìm kiếm.

Nhìn chung, AF Adapter dựa trên RoBERTa của chúng tôi thể hiện hiệu suất mạnh mẽ trên nhiều loại nhiệm vụ và hoạt động tốt nhất trong số các mô hình trung bình. Nó vượt trội so với mô hình backbone RoBERTa-wwm-ext-base và mô hình chuyên biệt cho miền y tế PCL-MedBERT lần lượt là 0.6% và 2%. Trong khi đạt hiệu suất tương tự trong các nhiệm vụ NORM, TS, và TC, mô hình của chúng tôi nổi bật trong các nhiệm vụ NER, RE, và NLI. Những kết quả này làm nổi bật tính linh hoạt và hiệu quả của AF Adapter dựa trên RoBERTa của chúng tôi trong việc giải quyết một loạt các thách thức hiểu ngôn ngữ y sinh học.

2) So sánh các kỹ thuật tiền huấn luyện: Để điều tra tác động của các kỹ thuật tiền huấn luyện đối với các mô hình chuyên biệt cho miền, chúng tôi tiến hành một số thí nghiệm sử dụng Chinese RoBERTa-wwm-ext-base làm mô hình backbone. Với từ vựng ban đầu và cùng kho ngữ liệu như AF Adapter dựa trên RoBERTa, chúng tôi liên tục tiền huấn luyện mô hình sử dụng Fine-Tuning và các phương pháp khác. Để đảm bảo so sánh công bằng, chúng tôi điều chỉnh các siêu tham số của các kỹ thuật, loại trừ Fine-Tuning, để đạt được kích thước tương tự của các tham số có thể huấn luyện. Kết quả đánh giá của các kỹ thuật tiền huấn luyện được trình bày trong Bảng IV.

Kết quả thí nghiệm làm nổi bật hiệu suất vượt trội của mô hình tiền huấn luyện sử dụng AF Adapter. So với cách tiếp cận Fine-Tuning, AF Adapter đạt được kết quả đáng kể trong khi chỉ huấn luyện khoảng 17% tham số mô hình. Bằng cách thêm các tham số bổ sung, AF Adapter liên tục vượt trội so với LoRA với biên độ trung bình 5.5%. Đáng chú ý, các thí nghiệm của chúng tôi cho thấy phương pháp Fine-Tuning dẫn đến hiệu suất dưới tối ưu. Những phát hiện này cho thấy rằng tiếp tục tiền huấn luyện mà không xem xét kiến thức và biểu diễn chuyên biệt cho miền có thể không cân bằng hiệu quả thông tin chuyên biệt cho miền với các biểu diễn miền tổng quát.

D. Phân tích chi tiết

Trong phần này, chúng tôi đi sâu vào phân tích chi tiết về hội tụ và ổn định của các kỹ thuật tiền huấn luyện trong nghiên cứu của chúng tôi. Ngoài ra, chúng tôi tiến hành phân tích quên thảm khốc, đánh giá mức độ mà mỗi phương pháp giữ lại kiến thức đã học trước đó khi thích nghi với một miền mới.

Phân tích hội tụ và ổn định. Chúng tôi kiểm tra hội tụ và ổn định của các kỹ thuật tiền huấn luyện trong suốt quá trình tiền huấn luyện và trình bày kết quả trong Hình 5.

Từ kết quả, chúng tôi quan sát thấy AF Adapter của chúng tôi thể hiện sự giảm đáng kể về mất mát so với LoRA. Sự hội tụ chậm hơn này cho thấy rằng mô hình cần nhiều lần lặp hơn để đạt được giải pháp tối ưu. Một lời giải thích có thể cho quan sát này là AF Adapter kết hợp cấu trúc "song song" và các tham số bổ sung cho phép mô hình học tốt hơn và tập trung vào thông tin chuyên biệt cho miền.

Cùng với hội tụ, AF Adapter của chúng tôi thể hiện hiệu suất nhất quán và ổn định, duy trì sự giảm mất mát mượt mà và dần dần. Ngược lại, FL-Tuning và Fine-Tuning thể hiện nhiều biến động và thay đổi trong các giá trị mất mát hơn. Phân tích ổn định này tiếp tục hỗ trợ tính mạnh mẽ của cách tiếp cận AF Adapter của chúng tôi trong việc học thông tin chuyên biệt cho miền.

Phân tích quên thảm khốc. Để đánh giá vấn đề quên do huấn luyện nhiệm vụ tuần tự gây ra, chúng tôi chọn ngẫu nhiên 10k mẫu miền tổng quát từ kho ngữ liệu "WuDao" [35]. Các mẫu này được tiền xử lý sử dụng tỷ lệ che 15%, đảm bảo đầu vào nhất quán trên tất cả các mô hình được so sánh. Chúng tôi đánh giá độ chính xác giữa Chinese RoBERTa-wwm-ext-base, Fine-Tuning dựa trên, FL-Tuning dựa trên, LoRA dựa trên, và AF Adapter dựa trên RoBERTa được huấn luyện trong Phần V-C2. Kết quả thí nghiệm, được trình bày trong Bảng V, chứng minh hiệu suất của các mô hình về mặt quên thảm khốc. Mô hình của chúng tôi chỉ đạt được giảm độ chính xác -5.324% so với RoBERTa ban đầu. Ngoài ra, rõ ràng là AF Adapter dựa trên RoBERTa vượt trội so với Fine-Tuning dựa trên RoBERTa với biên độ đáng kể, đạt độ chính xác 82.428 so với 70.964. Điều này cho thấy mô hình của chúng tôi giữ lại kiến thức đã học trước đó tốt hơn khi được huấn luyện trên các nhiệm vụ mới. Mô hình được huấn luyện sử dụng tinh chỉnh thể hiện sự sụt giảm đáng kể về độ chính xác, cho thấy việc quên đáng kể kiến thức tổng quát. Điều này phù hợp với hiệu suất tổng thể kém của nó trong CBLUE, như được hiển thị trong Bảng IV.

VI. KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất một cách tiếp cận tiền huấn luyện liên tục, được gọi là AF Adapter, để xây dựng một mô hình ngôn ngữ tiền huấn luyện chuyên biệt cho miền. Bằng cách kết hợp các đầu chú ý và đơn vị ẩn bổ sung trong mô hình dựa trên BERT, AF Adapter cho phép học hiệu quả kiến thức chuyên biệt cho miền trong khi tận dụng các biểu diễn ngôn ngữ tổng quát. Chúng tôi cũng trình bày một mô hình ngôn ngữ tiền huấn luyện miền y sinh học Trung Quốc được gọi là AF Adapter dựa trên RoBERTa, được huấn luyện sử dụng AF Adapter và kho ngữ liệu y sinh học. Đánh giá AF Adapter dựa trên RoBERTa trên benchmark CBLUE thể hiện hiệu suất vượt trội của nó, vượt trội so với các mô hình "kích thước tương tự" khác trung bình 0.98%. Hơn nữa, phân tích của chúng tôi cho thấy AF Adapter thể hiện hội tụ và ổn định mạnh mẽ so với các kỹ thuật tiền huấn luyện khác. Những phát hiện này làm nổi bật hiệu quả và tiềm năng của AF Adapter trong tiền huấn luyện chuyên biệt cho miền cho các mô hình ngôn ngữ.

LỜI CẢM ƠN

Các tác giả cảm ơn các nhà đánh giá ẩn danh vì những gợi ý có giá trị của họ. Công việc này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc (2021YFC2701800, 2021YFC2701801).
