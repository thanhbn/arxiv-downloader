# 2309.09276.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.09276.pdf
# Kích thước file: 4145048 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
NỘP CHO IEEE TRANSACTIONS 1
MVP: Meta Visual Prompt Tuning cho Phân loại Cảnh
Ảnh Viễn thám Ít mẫu
Junjie Zhu, Yiying Li, Chunping Qiu, Ke Yang*, Naiyang Guan*, và Xiaodong Yi
Tóm tắt —Các mô hình Vision Transformer (ViT) gần đây
đã nổi lên như những mô hình mạnh mẽ và linh hoạt cho nhiều
tác vụ thị giác khác nhau. Gần đây, một công trình được gọi là
PMF [1] đã đạt được kết quả hứa hẹn trong phân loại ảnh ít mẫu
bằng cách sử dụng các mô hình vision transformer được tiền
huấn luyện. Tuy nhiên, PMF sử dụng fine-tuning toàn bộ để
học các tác vụ downstream, dẫn đến tình trạng overfitting nghiêm
trọng và các vấn đề về lưu trữ, đặc biệt trong lĩnh vực viễn thám.
Để giải quyết những vấn đề này, chúng tôi chuyển sang các
phương pháp điều chỉnh hiệu quả tham số được đề xuất gần đây,
như VPT [2], chỉ cập nhật các tham số prompt mới được thêm
vào trong khi giữ nguyên backbone đã được tiền huấn luyện.
Lấy cảm hứng từ VPT, chúng tôi đề xuất phương pháp Meta
Visual Prompt Tuning (MVP). Cụ thể, chúng tôi tích hợp
phương pháp VPT vào framework meta-learning và điều chỉnh
nó cho lĩnh vực viễn thám, tạo ra một framework hiệu quả cho
Phân loại Cảnh Viễn thám Ít mẫu (FS-RSSC). Hơn nữa, chúng
tôi giới thiệu một chiến lược tăng cường dữ liệu mới dựa trên
việc tái kết hợp patch embedding để nâng cao khả năng biểu
diễn và đa dạng của các cảnh cho mục đích phân loại. Kết quả
thí nghiệm trên benchmark FS-RSSC chứng minh hiệu suất
vượt trội của MVP được đề xuất so với các phương pháp hiện
có trong nhiều cài đặt khác nhau, như various-way-various-shot,
various-way-one-shot, và thích ứng cross-domain.
Từ khóa chỉ mục —Học ít mẫu, Viễn thám, Prompt Tuning,
Meta-learning, Fine-tuning hiệu quả tham số

I. GIỚI THIỆU
Phân loại cảnh viễn thám ít mẫu (FS-RSSC) [3], [4] nhằm
phân loại ảnh viễn thám thành các danh mục khác nhau chỉ
sử dụng một vài ví dụ có nhãn cho mỗi danh mục. Đây là
một tác vụ machine learning đầy thử thách nhưng quan trọng
cho các ứng dụng thực tế như phân loại sử dụng đất và giám
sát môi trường, nơi việc thu thập các bộ dữ liệu có nhãn quy
mô lớn và chất lượng cao rất tốn kém và mất thời gian.
Transfer learning [5], meta-learning [6], và metric learning [3]
đã được sử dụng cho các tác vụ FS-RSSC. Những phương
pháp này chủ yếu sử dụng mạng nơ-ron tích chập (CNN) thường
bị hạn chế ở các mô hình nhỏ hơn với ít tham số hơn, như
Conv4 [7], ResNet12 [8], và ResNet18 [8].

Gần đây, Vision Transformers (ViT) đã đạt được những
thành tựu đáng chú ý trong nhiều tác vụ thị giác khác nhau.
Viễn cảnh áp dụng chúng vào lĩnh vực học ít mẫu rất hấp
dẫn và có sức hút đáng kể. Ví dụ, PMF [1], một phương pháp
dựa trên ViT bao gồm pipeline học ba giai đoạn, đã đạt được
tiến bộ đáng kể trong các tác vụ phân loại ít mẫu. PMF tiền
huấn luyện mô hình ViT trên dữ liệu bên ngoài không giám sát, sau đó

Junjie Zhu, Ke Yang, Yiying Li, Chunping Qiu, Naiyang Guan, và
Xiaodong Yi đều thuộc Viện Đổi mới Quốc gia về Công nghệ Quốc phòng,
Bắc Kinh, Trung Quốc (*Tác giả liên hệ: Ke Yang, Naiyang Guan. E-mail:
yangke13@nudt.edu.cn, nyguan@sina.com).

Hình 1: MVP so với các thuật toán FS-RSSC SOTA (tức là
SimCNAPs [9], DC-DML [10], PMF [1]): Đánh giá ngắn gọn
về hiệu suất độ chính xác của chúng trên bộ dữ liệu AIFS [10].

meta-trained mô hình trên các danh mục cơ sở, và cuối cùng
fine-tuned mô hình trên tác vụ mới. Họ đã chỉ ra rằng pipeline
dựa trên transformer đơn giản này mang lại hiệu suất tốt
bất ngờ trên các benchmark tiêu chuẩn như Mini-ImageNet [7],
CIFAR-FS [11], CDFSL [12], và Meta-Dataset [13].

Trong ba giai đoạn học, PMF sử dụng full fine-tuning để cập
nhật trọng số mạng. Tuy nhiên, điều này gây ra hai vấn đề
đáng kể, đặc biệt trong các ứng dụng viễn thám. Thứ nhất,
so với tác vụ phân loại ít mẫu trong ảnh tự nhiên, lĩnh vực
viễn thám đối mặt với vấn đề khan hiếm mẫu nghiêm trọng
hơn, và do đó việc fine-tuning hoàn toàn ViT với số lượng
lớn trọng số có thể dẫn đến các vấn đề overfitting nghiêm
trọng. Thứ hai, việc huấn luyện một mô hình ViT cho mỗi
tác vụ viễn thám là không khả thi do hạn chế về lưu trữ trên
các nền tảng vệ tinh hoặc máy bay không người lái nơi thuật
toán được triển khai. Kết quả thí nghiệm của chúng tôi cũng
chỉ ra rằng các mô hình ViT dựa trên chiến lược full fine-tuning
thể hiện hiệu quả thấp hơn trong việc giải quyết các tác vụ
FS-RSSC.

Để giải quyết những vấn đề này, chúng tôi chuyển sang khám
phá một phương pháp fine-tuning cho các mô hình ViT phù
hợp với các tác vụ FS-RSSC. Một giải pháp tiềm năng là các
phương pháp Parameter-Efficient Fine-Tuning (PEFT) [14],
đã nhận được sự chú ý đáng kể trong nhận dạng ảnh tự nhiên
gần đây. Trong paradigm PEFT, chỉ một số lượng nhỏ tham số
mới được thêm vào được cập nhật trong quá trình huấn luyện,
trong khi backbone được tiền huấn luyện được giữ nguyên.
Ví dụ, Visual Prompt Tuning

--- TRANG 2 ---
NỘP CHO IEEE TRANSACTIONS 2
Hình 2: Framework Meta Prompt Tuning cho Phân loại Cảnh Viễn thám. Query trong hộp nét đứt: ảnh thực tế trong
meta-training và pseudo-query được tạo ra thông qua phương pháp tăng cường dữ liệu mới trong meta fine-tuning. 'E' đại
diện cho token embedding backbone gốc; 'P' đại diện cho token prompt mới được thêm vào.

(VPT) [2] là một phương pháp PEFT thị giác được đề xuất gần
đây, thêm token prompt vào không gian đầu vào và chỉ cập
nhật các tham số mới được thêm vào. Các tham số được điều
chỉnh cho mỗi tác vụ downstream ít hơn 1% tham số mô hình,
do đó nó có thể giảm nhu cầu lưu trữ và giảm thiểu hiệu quả
các vấn đề overfitting mô hình của các ứng dụng viễn thám.

Lấy cảm hứng từ VPT, chúng tôi đề xuất phương pháp Meta
Visual Prompt Tuning (MVP) như một cách tiếp cận hiệu quả
để giải quyết các tác vụ FS-RSSC. Trong framework meta-learning,
MVP tận dụng prompt tuning để thích ứng mô hình ViT được
tiền huấn luyện với các tác vụ mới với dữ liệu và tài nguyên
tính toán hạn chế. Không giống như các phương pháp trước đó
fine-tune toàn bộ mô hình ViT, MVP chỉ cập nhật các tham số
prompt mới được thêm vào trong khi giữ nguyên mạng backbone
ViT được tiền huấn luyện. Cụ thể, MVP nhúng các tham số
prompt vào một framework meta-learning hiệu quả tham số mới.
Trong giai đoạn meta-training, MVP học cách học một khởi
tạo tốt cho các tham số prompt mới được thêm vào trên nhiều
tập các tác vụ nguồn FS-RSSC. Chúng tôi ký hiệu các tham
số prompt khởi tạo tối ưu bằng θ. Trong giai đoạn meta fine-tuning,
MVP fine-tune θ với một vài bước gradient trên tác vụ mục tiêu
và sau đó đưa ra dự đoán cho các danh mục cảnh viễn thám.

Ngoài ra, chúng tôi thiết kế một phương pháp tăng cường dữ
liệu mới cho meta fine-tuning. Phương pháp của chúng tôi
được thúc đẩy bởi quan sát rằng ảnh cảnh viễn thám của cùng
một danh mục có xu hướng có tính nhất quán cao, điều này
có thể dẫn đến overfitting mô hình và khái quát hóa kém đối
với các biến thể trong điều kiện chụp ảnh. Để giải quyết vấn
đề này, chúng tôi đề xuất nâng cao tính đa dạng của các cảnh
viễn thám bằng cách nhúng các patch ảnh của các danh mục
khác vào ảnh hiện tại, được truyền cảm hứng từ nghiên cứu
tái kết hợp patch ảnh [15]. Cụ thể, một phương pháp tái kết
hợp patch ảnh mới dựa trên mạng ViT được thiết kế, hoạt
động trên embeddings patch ảnh sau phép chiếu tuyến tính
của ViT. Hơn nữa, chúng tôi ngẫu nhiên chọn và hoán đổi
một số patch embeddings của một ảnh đầu vào với những
patch từ các ảnh khác trong cùng một batch. Thí nghiệm
cho thấy rằng phương pháp tăng cường dữ liệu của chúng tôi
có thể nâng cao hiệu quả khả năng khái quát hóa cho các
danh mục mới trong giai đoạn meta fine-tuning.

Các tác vụ FS-RSSC trong thế giới thực thể hiện hai đặc điểm
chính: số lượng danh mục và mẫu trong các tác vụ mới khác
nhau, và phân phối dữ liệu không thể dự đoán được, do đó
cần thiết thích ứng cross-domain [10]. Bộ dữ liệu AIFS [10]
đáp ứng cả hai tiêu chí như benchmark để đánh giá. Chúng
tôi đánh giá kỹ lưỡng mô hình MVP được đề xuất trên bộ dữ
liệu AIFS thông qua các thí nghiệm toàn diện, dưới các cài
đặt various-way-various-shot, various-way-one-shot, và thích
ứng cross-domain. Hình 1 cho thấy tổng quan về so sánh kết
quả và Hình 2 mô tả pipeline của mô hình MVP. MVP của
chúng tôi đã thể hiện hiệu suất vượt trội trên nhiều benchmark
in-domain và cross-domain đầy thử thách, vượt trội đáng kể
so với các phương pháp hiện có. Đóng góp của chúng tôi có
thể được tóm tắt như sau:

• Theo hiểu biết của chúng tôi, MVP được đề xuất là nghiên
cứu đầu tiên khám phá Parameter-Efficient Tuning (PETuning)
trên các ứng dụng viễn thám.

• Chúng tôi tích hợp PETuning vào framework meta-learning
bằng cách sử dụng paradigm meta-learning để khởi tạo các
tham số prompt mới được thêm vào, tạo điều kiện thích ứng
nhanh chóng với các tác vụ FS-RSSC mới.

• MVP được đề xuất của chúng tôi trao quyền cho các mô hình
transformer lớn hoạt động tốt trong các tình huống chỉ có
dữ liệu hạn chế, giảm thiểu đáng kể vấn đề overfitting.

• Chúng tôi đề xuất một phương pháp tăng cường dữ liệu được
thiết kế riêng cho các mô hình FS-RSSC dựa trên ViT để
nâng cao khả năng thích ứng của chúng với các cảnh viễn thám.

• MVP của chúng tôi thể hiện hiệu suất đặc biệt trên bộ dữ
liệu FS-RSSC đầy thử thách.

II. CÔNG TRÌNH LIÊN QUAN

A. Phân loại Cảnh với Học Ít mẫu
Phân loại cảnh viễn thám ít mẫu (FS-RSSC) là một tác vụ
đầy thử thách do tính khả dụng hạn chế của dữ liệu được chú
thích để huấn luyện mô hình. Các phương pháp cổ điển có thể
được chia thành hai danh mục: dựa trên metric và dựa trên
meta-learning [16]. Các phương pháp dựa trên metric học một
không gian đặc trưng hoặc hàm khoảng cách để đo sự tương
đồng giữa các lớp [3], [17], [18]. Ví dụ, RS-MetaNet [17] giới
thiệu một balance loss mới để cung cấp các mặt phẳng phân
đoạn tuyến tính tốt hơn cho các cảnh trong các danh mục khác
nhau. Một ví dụ khác là DLA-MatchNet [18], đề xuất một
cách tiếp cận để tự động khám phá các vùng phân biệt. MCMNet [19]
tiến thêm một bước bằng cách đề xuất một mạng hiệp phương
sai đa quy mô để tối ưu hóa không gian đa tạp.

Mặt khác, các phương pháp dựa trên meta-learning [20], [21]
nhằm học một meta-model có thể nhanh chóng thích ứng với
các tác vụ mới với vài bước cập nhật gradient. Ví dụ, MetaRS [21]
khám phá việc sử dụng meta-learning để cải thiện khả năng
khái quát hóa của mạng nơ-ron sâu (DNN) trên phân loại
cảnh viễn thám với dữ liệu huấn luyện hạn chế. Một ví dụ
khác là PTMeta [22], áp dụng transfer tham số để cố định
các tham số trong DNN để giảm bớt vấn đề huấn luyện một
số lượng lớn tham số trong framework meta-learning.

Một cuộc đánh giá các phương pháp này cho thấy hầu hết
sử dụng CNN nông như mạng backbone của chúng. Mặc dù
cách tiếp cận này có thể giảm thiểu overfitting khi dữ liệu
huấn luyện hạn chế, nó cũng hạn chế các cải tiến thêm trong
hiệu suất phân loại. Gần đây, Vision Transformers (ViT) đã
thể hiện kết quả hứa hẹn trong các tác vụ thị giác [23], [1],
và đã có những nỗ lực để áp dụng các cách tiếp cận dựa trên
ViT cho phân loại viễn thám quy mô lớn [24], [25]. Tuy nhiên,
nghiên cứu về việc ứng dụng các phương pháp dựa trên ViT
cho các tác vụ FS-RSSC vẫn còn hạn chế.

B. Điều chỉnh Hiệu quả cho Visual Transformer
Điều chỉnh hiệu quả của các mô hình ngôn ngữ được tiền
huấn luyện (PLMs) đã thu hút nhiều sự chú ý gần đây [26], [27],
vì nó nhằm giảm kích thước tham số và chi phí tính toán của
việc fine-tuning PLMs trên các tác vụ downstream. Nhiều
phương pháp khác nhau đã được đề xuất để đạt được điều
chỉnh hiệu quả, chẳng hạn như thêm các mô-đun adapter nhẹ [28], [29],
chèn các token prefix cụ thể cho tác vụ [30], hoặc thêm các
token prompt [31] vào PLMs và chỉ cập nhật những tham số
bổ sung này trong khi giữ nguyên PLMs. Prompt tuning gần
đây đã được mở rộng cho các tác vụ thị giác. Ví dụ, Visual
Prompt Tuning (VPT) [2] đạt được độ chính xác cao hơn so
với full fine-tuning bằng cách chỉ cập nhật 1% tham số của
mô hình. Tuy nhiên, prompt tuning cũng đối mặt với một số
thách thức trong các cài đặt học ít mẫu, nơi mỗi tác vụ chỉ
có một lượng nhỏ dữ liệu có nhãn sẵn có. Vẫn còn thiếu nghiên
cứu đầy đủ về cách tận dụng prompt tuning cho các tình huống
học ít mẫu. Trong bài báo này, chúng tôi kế thừa kỹ thuật
VPT và đề xuất một phương pháp mới thúc đẩy hiệu suất
state-of-the-art trên các tác vụ FS-RSSC.

C. Tăng cường Dữ liệu
Trong cả phân loại ảnh chung và ít mẫu, tăng cường dữ liệu
mở rộng số lượng ảnh có sẵn cho mỗi lớp và tạo ra các lớp
và tác vụ mới [32]. Các kỹ thuật từ các phép quay đơn giản [33]
và cắt [34] đến các chiến lược tinh tế hơn như cutmix [35]
và mixup [36]. Một số phương pháp sử dụng mạng GAN để
mô phỏng phân phối dữ liệu mục tiêu [37]. Nghiên cứu gần
đây đã chỉ ra rằng tăng cường dữ liệu có các tác động khác
nhau đối với giai đoạn meta-training và meta-testing của
pipeline meta-learning [38]. Ví dụ, việc tăng số lượng mẫu
query và tác vụ trong meta-testing cải thiện hiệu suất của
meta-learners nhiều hơn việc tăng số lượng mẫu support trong
meta-training. So với các bộ dữ liệu ít mẫu chung, các bộ
dữ liệu FS-RSSC có khối lượng nhỏ hơn [3]. Để khắc phục
vấn đề khan hiếm dữ liệu này, một số phương pháp đã được
đề xuất để tăng cường dữ liệu huấn luyện cho FS-RSSC theo
các cách khác nhau. Ví dụ, phương pháp quad-patch [15] tạo
ra các mẫu tổng hợp bằng cách cắt và tái lắp ráp các patch
từ các ảnh hiện có, trong khi phương pháp spatial vector
enhancement [39] mô phỏng phân phối của các lớp lân cận
để làm phong phú không gian đặc trưng. Tuy nhiên, những
phương pháp này không xem xét các tính chất cụ thể của ViT
như mạng backbone. Trong bài báo này, chúng tôi trình bày
một phương pháp tăng cường dữ liệu mới được tùy chỉnh cho
các đặc trưng cấu trúc của ViT và các thuộc tính của ảnh viễn
thám và khai thác đầy đủ tiềm năng của kiến trúc ViT.

III. PHƯƠNG PHÁP LUẬN

A. Tổng quan
Định nghĩa Vấn đề. Phân loại cảnh viễn thám ít mẫu (FS-RSSC)
là một tác vụ yêu cầu mô hình phân loại nhanh chóng và chính
xác các ảnh cảnh chưa thấy chỉ với một vài mẫu được chú thích [3].
Tác vụ này được thúc đẩy bởi thách thức thích ứng domain
trong ảnh viễn thám, thường được thu thập từ các cảm biến,
vùng và mùa khác nhau, dẫn đến một khoảng cách domain
lớn giữa domain nguồn và mục tiêu. Hơn nữa, số lượng mẫu
được chú thích khác nhau đáng kể giữa các tác vụ khác nhau,
làm cho việc huấn luyện các mô hình có thể đối phó với số
lượng mẫu được chú thích khác nhau trở nên cần thiết. Chính
thức, bộ dữ liệu được chú thích, được gọi là tập support S,
bao gồm C danh mục (way) với K mẫu (shot) cho mỗi danh
mục, trong đó C∈[5,MAXWAY] và K∈[1,MAXSHOT]. Mô hình
được huấn luyện trên tập support và sau đó được sử dụng
để dự đoán các danh mục của tập query Q, chứa các ảnh
không có nhãn từ cùng các danh mục như tập support.

Quy trình Meta-learning. Các phương pháp meta-learning đã
cho thấy triển vọng cho tác vụ FS-RSSC. Quy trình meta-learning
cơ bản bao gồm hai giai đoạn: meta-training và meta fine-tuning [6], [21].
Với sự phát triển của ViT, phương pháp PMF [1] nâng cao
quy trình meta-learning bằng cách thêm giai đoạn tiền huấn
luyện. PMF đề xuất một pipeline ba giai đoạn mới: tiền huấn
luyện, meta-training, và meta fine-tuning. Mạng backbone
đầu tiên được tiền huấn luyện trên một bộ dữ liệu bên ngoài
quy mô lớn như ImageNet [40], sau đó được meta-trained
trên nhiều bộ dữ liệu nguồn, và cuối cùng, được meta fine-tuned
trên bộ dữ liệu mục tiêu với các mẫu support được chú thích
hạn chế. Lưu ý rằng các bộ dữ liệu nguồn và mục tiêu có
các domain không chồng chéo.

Meta-learning dựa trên Prompt. Trong ba giai đoạn học,
PMF sử dụng full fine-tuning để cập nhật mạng backbone.
Tuy nhiên, điều này gây ra hai thách thức đáng kể, đặc biệt
trong các ứng dụng viễn thám. Một thách thức là làm thế nào
để tránh overfitting khi fine-tuning toàn bộ

--- TRANG 3 ---
NỘP CHO IEEE TRANSACTIONS 3

mô hình ViT trên các mẫu support hạn chế. Thách thức khác
là làm thế nào để giảm không gian lưu trữ cần thiết để lưu
trữ các mô hình ViT khác nhau cho các tác vụ khác nhau. Để
giải quyết những vấn đề này, chúng tôi đề xuất một framework
meta-visual prompt tuning (MVP). Nghiên cứu của chúng tôi
nhằm fine-tune hiệu quả các mô hình backbone ViT được tiền
huấn luyện cho tác vụ FS-RSSC.

Với một mạng backbone ViT được tiền huấn luyện, MVP giới
thiệu một mô-đun prompt mới vào không gian đầu vào của
mô hình ViT được tiền huấn luyện (như được hiển thị trong
Hình 3). Các tham số prompt không cần trải qua giai đoạn
tiền huấn luyện như trong PMF. Trong các giai đoạn meta-training
và meta fine-tuning, MVP chỉ cập nhật các tham số prompt
để phù hợp với các đặc trưng tác vụ FS-RSSC, trong khi toàn
bộ mạng ViT được giữ nguyên.

Trong giai đoạn meta-training của MVP, chúng tôi tối ưu hóa
các tham số prompt trên nhiều bộ dữ liệu nguồn có domain
riêng biệt với bộ dữ liệu mục tiêu. Chúng tôi sử dụng một
thuật toán meta-learning mô phỏng tác vụ FS-RSSC bằng cách
lấy mẫu các episode từ các bộ dữ liệu nguồn. Một episode
là một tác vụ học ít mẫu chứa một tập support và một tập
query với cùng các danh mục nhưng các ảnh khác nhau. Chúng
tôi sử dụng hàm loss prototypical [41] để đánh giá hiệu suất
phân loại của mô hình MVP trên tập query và cập nhật các
tham số prompt sử dụng gradient descent.

Sau khi khởi tạo các tham số prompt trong meta-training,
mô hình MVP có thể thích ứng với bộ dữ liệu mục tiêu của
các cảnh viễn thám trong giai đoạn meta fine-tuning. Những
cảnh mục tiêu này hoàn toàn mới và khác biệt so với bộ dữ
liệu nguồn. Mô hình MVP thực hiện meta fine-tuning trên
các tác vụ phụ trợ dựa trên dữ liệu support, sử dụng một
phương pháp tăng cường dữ liệu mới. Sau meta fine-tuning,
mô hình MVP có thể phân loại tất cả dữ liệu query không có
nhãn còn lại.

B. Kiến trúc Mô hình
Mạng Backbone ViT. ViT tiêu chuẩn [23] được sử dụng
làm mạng backbone của chúng tôi để giải quyết tác vụ FS-RSSC.
Là đầu vào cho mạng backbone ViT, ảnh x∈R³×H×W
ban đầu được chia thành m patch có kích thước cố định
{I_j∈R³×h×w|j∈N,1≤j≤m}. Tiếp theo, mỗi patch được
chiếu thành embedding đặc trưng d-chiều với mã hóa vị trí [23]:

e_j^0=Embed(I_j), (1)

trong đó e_j^0∈R^d. Sau đó, tập hợp các embedding patch ảnh
E_i={e_j^i∈R^d|i∈N,1≤i≤N} được sử dụng làm đầu vào
cho lớp Transformer thứ (i+1) L_{i+1}. Chính thức, toàn bộ
mạng backbone ViT có thể được phát biểu như:

[CLS_i,E_i] = L_i([CLS_{i-1},E_{i-1}]), (2)
f_θ(x) = CLS_N, (3)

trong đó CLS_{i-1}∈R^d đề cập đến class token trong chuỗi
đầu vào của L_i. Hơn nữa, CLS_N trong đầu ra của lớp cuối
cùng được sử dụng làm biểu diễn đặc trưng f(x) của ảnh
đầu vào x. Hơn nữa, θ đại diện cho các tham số mô hình của ViT.

Mạng ViT dựa trên Prompt. Phù hợp với mô hình VPT [2]
và với một mạng backbone ViT được tiền huấn luyện, một
tập các token prompt P_i={p_t^i∈R^d|t∈N,1≤t≤p}
được nối vào không gian đầu vào của lớp transformer.
Chính thức, kiến trúc ViT trong Phương trình 2 có thể được
thay thế bằng:

[CLS_i,P_i,E_i] = L_i([CLS_{i-1},P_{i-1},E_{i-1}]), (4)
f_{θ'}(x) = CLS_N, (5)

trong đó [x_{i-1},P_{i-1},E_{i-1}]∈R^{(1+p+m)×d}. θ' ký hiệu
các tham số mô hình của ViT, cũng như các tham số prompt
bổ sung θ_P. Cấu trúc mạng của mô hình được đề xuất,
Meta Visual Prompt Tuning (MVP), được minh họa trong Hình 3.

Trong suốt giai đoạn meta-training và meta fine-tuning,
chỉ các tham số prompt θ_P mới được thêm vào được cập nhật,
trong khi tất cả các tham số khác của mạng backbone ViT
vẫn không thay đổi. Do đó, một tác vụ phân loại liên quan
đến việc dự đoán nhãn y có thể được biểu diễn như sau:

arg max_{θ_P} ∑_x log p(y|f_{θ'}(x);θ_P). (6)

C. Quy trình Meta Fine-Tuning
Meta fine-tuning cho các tác vụ ít mẫu mục tiêu yêu cầu
sử dụng hiệu quả một lượng nhỏ dữ liệu support có nhãn
và đạt được meta fine-tuning trong một vài bước. Một giải
pháp phổ biến là sử dụng tăng cường dữ liệu để mở rộng
tập support [1]. Đối với một tác vụ ít mẫu T={S, Q}, trong
đó S là một tập các ảnh support có nhãn, và Q là các ảnh
query không có nhãn khác, một phương pháp là tạo ra một
tác vụ phụ trợ T'={S, Q'}, trong đó pseudo-query Q'=augment(S)
được cấu thành từ các ảnh support được tăng cường. Quy
trình meta fine-tuning dựa trên tác vụ phụ trợ này nâng cao
khả năng thích ứng của mô hình với các tác vụ mới bằng
cách tận dụng dữ liệu được tăng cường.

Trong công trình này, chúng tôi đề xuất một phương pháp
tăng cường dữ liệu mới được gọi là Random Patch Recombination (RPR),
được thiết kế riêng cho các mô hình dựa trên ViT. Như được
hiển thị trong Hình 4, phương pháp RPR được áp dụng sau
phép chiếu tuyến tính của dữ liệu. Không giống như các
phương pháp truyền thống [1], [38] tăng cường dữ liệu trước
khi đưa vào mạng backbone, phương pháp của chúng tôi có
thể tận dụng đầy đủ

--- TRANG 4 ---
NỘP CHO IEEE TRANSACTIONS 4

Hình 3: Cấu trúc mạng của mô hình Meta Visual Prompt
Tuning (MVP) được đề xuất.

cấu trúc của ViT. Cụ thể, đối với embeddings patch
E={e_{pos}∈R^d|pos∈N,1≤pos≤m} của một ảnh
cho trước trong tập support S, chúng tôi chọn một tập con
của E và ký hiệu các vị trí tương ứng của chúng là
{pos∈R^{m'},1≤m'≤m}, với tỷ lệ tái kết hợp là α. Sau
đó, chúng tôi thay thế các patch đã chọn bằng các patch
từ các ảnh khác trong tập support S có cùng vị trí pos.
Thuật toán 1 tóm tắt quy trình một cách chi tiết.

D. Hàm Loss
Với mạng backbone ViT dựa trên prompt, chúng tôi thảo luận
về cách định nghĩa mục tiêu huấn luyện. Chúng tôi ký hiệu
biểu diễn đặc trưng của ảnh support và query là f_{θ'}(x_s)
và f_{θ'}(x_q) và viết chúng là f^s_{θ'} và f^q_{θ'} để đơn giản.
Theo mạng prototypical [41], các vector prototype của dữ
liệu support được ký hiệu là Ω={μ_c∈R^d|c∈N,1≤c≤C}.
Trong đó μ_c=1/|S_c| ∑_{i:y^s_i=c} f^{si}_{θ'} là prototype
của lớp c và |S_c|=∑_{i:y^s_i=c} 1. Sau đó xác suất của
một ảnh query x_q được định nghĩa là một hàm của sự tương
đồng của nó với các prototype của dữ liệu support:

p(y_q=c|x_q) = exp(-d(f^q_{θ'},μ_c)) / ∑_{c'} exp(-d(f^q_{θ'},μ_{c'})), (7)

trong đó d là khoảng cách cosine. Cuối cùng, mục tiêu huấn
luyện L của MVP được đề xuất là tối thiểu hóa negative log-likelihood
và L=-log p(y_q=c|x_q), có thể được viết thêm là:

L = 1/|S_c| [d(f^q_{θ'},μ_c) + log ∑_{c'} exp(-d(f^q_{θ'},μ_{c'}))]. (8)

IV. THÍ NGHIỆM
Trong phần này, chúng tôi trước tiên giới thiệu bộ dữ liệu
đánh giá và các chi tiết triển khai một cách ngắn gọn nhưng
toàn diện. Sau đó, chúng tôi trình bày các thí nghiệm ablation
chứng minh hiệu quả đáng kể của thiết kế MVP. Cuối cùng,
chúng tôi so sánh MVP được đề xuất với các đối thủ cạnh
tranh state-of-the-art (SoTA).

Thuật toán 1 Mã giả PyTorch cho tăng cường dữ liệu
# Đầu vào: Patch embeddings của ảnh trong tập support.
# Đầu ra: Tái kết hợp của embeddings patch đầu vào.
def random_recombine(emb_x, rate):
    # x: [bs, pos, dim]
    # rate: tỷ lệ lựa chọn của \alpha
    pos = emb_x.size(1)
    rate = int(pos * rate)
    # lặp qua tất cả embeddings patch ảnh
    for emb_i, _ in enumerate(emb_x):
        select_pos = random.sample(range(0, pos), rate)
        emb_x = replace(emb_x, emb_i, select_pos)

def replace(emb_x, emb_i, select_pos):
    # lặp qua tất cả vị trí đã chọn
    bs = emb_x.size(0)
    for pos_i in select_pos:
        # lưu ý: xác suất p(i=j) xấp xỉ 0
        emb_j = random.randint(0, bs-1)
        emb_x[emb_i, pos_i, :] = emb_x[emb_j, pos_i, :]
    return emb_x

A. Thiết lập Thí nghiệm
Bộ dữ liệu. Chúng tôi đánh giá các phương pháp và thuật toán
SoTA của chúng tôi sử dụng một benchmark FS-RSSC đầy
thử thách có tên AIFS-DATASET [10]. Benchmark này là
một biến thể của META-DATASET [13] bao gồm một tập
hợp các bộ dữ liệu viễn thám. AIFS-DATASET được cấu
thành từ hai tập con: tập in-domain và out-of-domain. Tập
in-domain bao gồm sáu bộ dữ liệu nguồn mở không có
các tình huống viễn thám: CUB-200-2011, Describable Textures,
Fungi, VGG Flower, Traffic Signs, và CIFAR100. Sáu bộ
dữ liệu này được phân vùng sao cho khoảng 70%, 15%, và
15% dữ liệu được gán cho tập huấn luyện, validation, và
kiểm tra tương ứng. Tập out-of-domain bao gồm bốn bộ
dữ liệu viễn thám, cụ thể là NWPU-RESISC45, UC-Merced,
WHU-RS19, và AID, tất cả đều được sử dụng độc quyền cho
mục đích kiểm tra. Bảng I hiển thị việc phân vùng bộ dữ
liệu cụ thể.

BẢNG I: THÀNH PHẦN DỮ LIỆU VÀ PHÂN CHIA CỦA
AIFS-DATASET [10].
Bộ dữ liệu Domain Tổng Train Val Test
CUB-200-2011 In 200 140 30 30
Describable Textures 47 33 7 7
Fungi 1394 994 200 200
VGG Flower 102 71 15 16
Traffic Signs 43 30 6 7
CIFAR100 100 72 13 15
UCMerced Out 21 0 0 21
WHU-RS19 19 0 0 19
NWPU-RESISC45 45 0 0 45
AID 30 0 0 30

Phương pháp Benchmark. Trong bài báo này, chúng tôi so
sánh phương pháp của chúng tôi với một số phương pháp
SoTA cho FS-RSSC, như k-NN [42], Finetune [43], MatchingNet [7],
ProtoNet [41], fo-MAML [20], RelationNet [44], fo-Proto-MAML [20],
CNAPs [42], SimpleCNAPs [9], và DC-DML [10]. Những
phương pháp này đã được đánh giá trên AIFS-DATASET [10],
một benchmark quy mô lớn cho FS-RSSC. Ngoài ra, chúng
tôi sử dụng PMF [1] như một mô hình baseline khác, vì nó
đã đạt được hiệu suất SoTA trên nhiều benchmark FSL khác
nhau. Chúng tôi theo mã và cài đặt chính thức của PMF để
tái tạo kết quả của nó trên bộ dữ liệu AIFS. Đối với cài đặt
huấn luyện, tất cả các phương pháp trên sử dụng phương
pháp full fine-tuning. Ngược lại, phương pháp MVP được
đề xuất của chúng tôi sử dụng phương pháp prompt fine-tuning.

Chi tiết Triển khai. Về tổ chức dữ liệu, chúng tôi theo cùng
một thiết lập như AIFS-DATASET [10]. Cụ thể, số lượng
lớp được bao gồm trong mỗi tác vụ được chọn ngẫu nhiên
từ phạm vi [5, MAXWAY], trong khi số lượng mẫu support
cho mỗi lớp được chọn ngẫu nhiên từ phạm vi [1, MAXSHOT].
Thuật toán lấy mẫu được sử dụng trong quy trình này dựa
trên lấy mẫu đều. Trong văn bản tiếp theo, chúng tôi sử dụng
MW và MS để đại diện cho MAXWAY và MAXSHOT tương ứng.
Để đánh giá toàn diện hiệu suất mô hình trong thí nghiệm
của chúng tôi, chúng tôi đặt MW thành 5/10/20 và MS thành
1/5/10/20. Đối với giai đoạn tiền huấn luyện, nhất quán với
PMF [1], chúng tôi cũng sử dụng ViT làm mạng backbone.
ViT được tiền huấn luyện trên bộ dữ liệu ImageNet1K sử
dụng phương pháp tự giám sát cổ điển DINO [45]. Trong
thí nghiệm của chúng tôi, chúng tôi trình bày kết quả dựa
trên ViT-tiny và ViT-small. Trong các giai đoạn meta-training
và meta fine-tuning, các tham số của ViT được cố định,
và chỉ các tham số prompt mới được thêm vào được cập nhật.

B. Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành một nghiên cứu ablation
để phân tích hiệu quả của phương pháp MVP được đề xuất.
Đầu tiên, chúng tôi so sánh hiệu suất của meta-visual prompt-tuning
và fully fine-tuning và trình bày kết quả trong Bảng II.
Bảng III hiển thị số lượng tham số có thể học được cần
được cập nhật sử dụng các phương pháp fine-tuning khác nhau.
Thứ hai, chúng tôi đánh giá hiệu quả và hiệu suất tính toán
của phương pháp tăng cường dữ liệu RPR được đề xuất trong
bài báo này. Cuối cùng, chúng tôi điều tra cách số lượng
token prompt ảnh hưởng đến hiệu suất phân loại và hiệu
suất tính toán.

PMF có Hiệu quả trong Lĩnh vực Viễn thám không? Để
điều tra hiệu suất của phương pháp full fine-tuning trong
tác vụ FS-RSSC, chúng tôi đã tiến hành một nghiên cứu
ablation trên AIFS-DATASET sử dụng framework PMF. Chúng
tôi so sánh bốn cài đặt: (1) M1, là mô hình được tiền huấn
luyện mà không có bất kỳ quy trình meta-learning nào; (2)
M2, là mô hình sau chỉ quy trình meta-training; (3) M3, là
mô hình sau chỉ quy trình meta-fine-tuning; (4) M4, là mô
hình PMF [1] hoàn chỉnh. Từ kết quả trong Bảng II, chúng
tôi có thể rút ra kết luận rằng PMF (full fine-tuning) cải
thiện hiệu suất so với mô hình được tiền huấn luyện mà
không có bất kỳ quy trình meta-learning nào (so sánh các
mô hình M4, M3, M2 với M1). Những điều này cho thấy
rằng full fine-tuning có thể không phải là một giải pháp
tốt cho các tác vụ FS-RSSC.

Hiệu quả của MVP cho FS-RSSC và Áp dụng MVP ở Giai
đoạn Nào? Để đánh giá hiệu quả của phương pháp meta
visual prompt-tuning (MVP) cho FS-RSSC, chúng tôi đã thực
hiện các nghiên cứu ablation dưới ba cài đặt: (1) M5, chỉ
áp dụng MVP cho meta-training; (2) M6, chỉ áp dụng MVP
cho meta fine-tuning; (3) M9, áp dụng MVP cho cả meta-training
và meta fine-tuning. Kết quả trong Bảng II cho thấy rằng:
(1) mô hình với MVP hoàn thành quy trình meta-training
hoặc meta fine-tuning có hiệu suất tốt hơn so với mô hình
PMF với cùng quy trình (M5

BẢNG II: TÁC ĐỘNG CỦA VIỆC CẬP NHẬT BACKBONE
HOẶC THAM SỐ PROMPT QUA CÁC GIAI ĐOẠN META-LEARNING
KHÁC NHAU ĐỐI VỚI ĐỘ CHÍNH XÁC PHÂN LOẠI TRUNG
BÌNH (%) CỦA AIFS-DATASET DƯỚI CÁC TÌNH HUỐNG
MW5 MS5 VÀ MW10 MS10. "-" CHỈ RA RẰNG GIAI ĐOẠN
NÀY SẼ KHÔNG ĐƯỢC THỰC HIỆN.

Cấu hình Huấn luyện Kết quả Benchmark
Mô hình Meta Train Meta Finetune MW5 MS5 MW10 MS10
M1 − − 75.5 ± 0.1 76.4 ± 0.3
M2 Backbone − 75.6 ± 0.2 76.6 ± 0.2
M3 − Backbone 74.9 ± 0.1 77.2 ± 0.3
M4 Backbone Backbone 76.6 ± 0.2 78.1 ± 0.1
M5 Prompt − 79.8 ± 0.3 80.4 ± 0.2
M6 − Prompt 76.3 ± 0.2 78.6 ± 0.2
M7 Backbone Prompt 75.6 ± 0.1 78.9 ± 0.2
M8 Prompt Backbone+Prompt 78.3 ± 0.2 80.1 ± 0.4
M9 Prompt Prompt 79.6 ± 0.3 81.2 ± 0.2

so với M2, và M6 so với M3); (2) Hơn nữa, mô hình với MVP
hoàn thành quy trình meta-training hoặc meta fine-tuning
thậm chí còn vượt trội hơn mô hình PMF hoàn thành cả hai
quy trình (M5 so với M4 và M6 so với M4); (3) Mô hình
MVP hoàn chỉnh vượt trội đáng kể so với mô hình PMF hoàn
chỉnh (M9 so với M4). Những phát hiện này chỉ ra rằng MVP
là một phương pháp hiệu quả và vượt trội cho FS-RSSC,
vì nó có thể học từ một vài ví dụ hiệu quả và chính xác hơn
so với PMF.

BẢNG III: SỐ LƯỢNG THAM SỐ CÓ THỂ HUẤN LUYỆN
CHO RN18, VIT VÀ PROMPT VIT VỚI 200 TOKEN PROMPT.
Backbone Kích thước ảnh Tham số huấn luyện (M)
RN18 224×224 11.28
ViT-tiny 224×224 5.52
ViT-small 224×224 21.66
ViT-base 224×224 85.79
Prompt ViT-tiny 224×224 0.46
Prompt ViT-small 224×224 0.92
Prompt ViT-base 224×224 1.84

Kết hợp MVP và Full Fine-Tuning. Để xác minh liệu MVP
và full fine-tuning có thể làm việc cùng nhau để đạt được
kết quả tốt hơn không, chúng tôi đã thực hiện thí nghiệm
dưới ba cài đặt: (1) M7 áp dụng full fine-tuning trong giai
đoạn meta-training và MVP tuning trong giai đoạn meta
fine-tuning; (2) M8 áp dụng MVP tuning trong giai đoạn
meta-training và full fine-tuning cho tất cả tham số trong
giai đoạn meta fine-tuning; (3) M9 áp dụng MVP tuning
cho cả meta-training và meta fine-tuning. Kết quả trong
Bảng II cho thấy rằng: (1) M7 không cho thấy cải thiện đáng
kể so với M4, chỉ ra rằng việc thực hiện MVP chỉ trong giai
đoạn meta fine-tuning là hiệu quả nhưng không nổi bật; (2)
M8 đạt được cải thiện đáng kể so với M4, chỉ ra rằng MVP
có thể giúp mô hình có được một điểm khởi tạo tốt hơn dẫn
đến khái quát hóa tốt hơn cho các tác vụ mới; (3) M9 đạt
được kết quả SoTA, chứng minh rằng việc sử dụng MVP
trong cả giai đoạn meta-training và meta fine-tuning có thể
đạt được lợi ích cải thiện tối đa.

Hiệu quả của Tăng cường Dữ liệu RPR. Nghiên cứu này
đề xuất một phương pháp tăng cường dữ liệu mới được gọi
là Random Patch

--- TRANG 5 ---
NỘP CHO IEEE TRANSACTIONS 5

Hình 4: Random Patch Recombination: một phương pháp
tăng cường dữ liệu mới được đề xuất trong bài báo này.

Recombination (RPR-aug), được chi tiết trong Mục III-C.
Phần này chủ yếu xác minh hiệu suất của phương pháp RPR
so với các phương pháp tăng cường dữ liệu khác. Chúng tôi
sử dụng phương pháp tăng cường dữ liệu PMF [1] được xác
minh (PMF-aug) làm phương pháp so sánh chính. Phương
pháp tăng cường dữ liệu PMF bao gồm các kỹ thuật phổ biến
như mixup, cutmix, color-jitter, translation, và cutout, và
kích hoạt một hoặc nhiều phương pháp này dựa trên xác suất.
Tương tự như PMF, chúng tôi áp dụng RPR được đề xuất để
xây dựng pseudo queries dựa trên ảnh support, được sử dụng
làm tác vụ phụ trợ trong giai đoạn meta fine-tuning. Tuy
nhiên, không giống như PMF, những tác vụ phụ trợ này chỉ
được sử dụng để cập nhật tham số prompt trong khi giữ nguyên
mạng backbone. Hơn nữa, chúng tôi cũng tự động chọn tỷ
lệ học lr và tỷ lệ tái kết hợp α cho mỗi tác vụ. Chúng tôi
sử dụng MVP để chọn lr và α tối ưu từ các phạm vi lr∈[1e−4,1e−3,1e−2,0.1,0]
và α∈[0.05,0.1,0.2,0.25], và sau đó thực hiện meta fine-tuning
với chúng.

Để đánh giá hiệu quả của RPR, chúng tôi đã tiến hành một
loạt thí nghiệm trên bốn bộ dữ liệu của AIFS (UCM, WHU,
NWPU, và AID) cho cả tình huống various-way-various-shot
và various-way-one-shot và so sánh kết quả. Kết quả của
những thí nghiệm này được trình bày trong Hình 5. Những
phát hiện của chúng tôi cho thấy rằng nhìn chung, RPR vượt
trội hơn PMF đáng kể. Cụ thể, khi xem xét kết quả trên tất
cả bốn tác vụ (MW5 S1, MW5 MS5, MW10 S1, MW10 MS10),
RPR mang lại cải thiện 0.62%, 0.7%, 0.98%, và 0.68% so
với PMF trên UCM, WHU, NWPU, và AID tương ứng. Đặc
biệt, trong các tác vụ học 1-shot, RPR đạt được những cải
thiện đặc biệt đáng kể so với PMF, với tác vụ MW10 S1 cho
thấy cải thiện 1.36%, 0.97%, 1.71%, và 0.74% trên UCM,
WHU, NWPU, và AID tương ứng. Tóm lại, kỹ thuật tăng
cường dữ liệu RPR được đề xuất vượt trội hơn phương pháp
PMF đáng kể trong các tác vụ phân loại ảnh ít mẫu, đặc biệt
trong các tình huống học 1-shot.

BẢNG IV: PHÂN TÍCH THỜI GIAN CHẠY TRÊN CÁC TÁC VỤ
FS-RSSC VỚI MẠNG BACKBONE VIT.
Backbone Phương pháp Thời gian trung bình (s)
ViT-tiny PMF-Aug 3.16
RPR-Aug 0.67
ViT-small PMF-Aug 3.08
RPR-Aug 0.97
ViT-base PMF-Aug 3.03
RPR-Aug 1.61

Hiệu suất của Tăng cường Dữ liệu RPR. Để so sánh hiệu
suất của phương pháp RPR-aug và phương pháp PMF-aug,
chúng tôi tiến hành thêm thí nghiệm trên AIFS sử dụng các
mạng backbone khác nhau của ViT-tiny và ViT-small. Chúng
tôi kiểm tra sáu tác vụ 10-way k-shot, trong đó k∈[1,2,4,6,8,10],
và lặp lại 1000 thí nghiệm tăng cường dữ liệu cho mỗi tác vụ.
Để đảm bảo tính công bằng của việc so sánh, chúng tôi đặt
tỷ lệ tái kết hợp α của RPR-aug ở giá trị tối đa 0.25 và sử
dụng cùng mô hình và cùng batch. Bảng IV hiển thị tổng
thời gian chạy trung bình của 1000 thí nghiệm cho mỗi tác vụ.
Từ Bảng IV, chúng ta có thể thấy rằng RPR-aug nhanh hơn
PMF-aug trung bình 3 lần, chỉ ra rằng phương pháp RPR-aug
của chúng tôi hiệu quả hơn và phù hợp hơn cho các tác vụ
FS-RSSC dựa trên ViT.

Số lượng Token Prompt. Đây là một siêu tham số quan trọng
cần điều chỉnh cho MVP, chúng tôi thực hiện thí nghiệm để
kiểm tra tác động của số lượng token prompt đối với hiệu
suất và hiệu quả của mô hình. Để tiến hành thí nghiệm so
sánh, chúng tôi thiết kế thiết lập thí nghiệm theo nguyên tắc
kiểm soát biến số. Chúng tôi sử dụng bộ dữ liệu NWPU của
AIFS làm benchmark và đánh giá hiệu suất của mô hình MVP
trên bốn tác vụ RS-FSSC, bao gồm 5-way 1-shot, 5-way 5-shot,
10-way 1-shot, và 10-way 10-shot. Bảng V hiển thị so sánh
hiệu suất của mô hình MVP với số lượng token prompt khác
nhau được tải trong bốn tác vụ này. Như có thể thấy từ các
con số, khi số lượng token được đặt thành 10, mô hình MVP
đạt được độ chính xác phân loại cao nhất trong hầu hết các
tác vụ. Hơn nữa, chúng tôi quan sát thấy rằng khi số lượng
token tăng, thời gian tiêu thụ của mô hình cũng tăng theo.
Tóm lại, xem xét sự cân bằng giữa độ chính xác hiệu suất
và hiệu quả tính toán, chúng tôi cố định số lượng token prompt
cho MVP

BẢNG V: SO SÁNH THAM SỐ, ĐỘ CHÍNH XÁC TRUNG BÌNH
VÀ THỜI GIAN CHẠY CHO SỐ LƯỢNG TOKEN KHÁC NHAU.
Token Tham số Độ chính xác TB (%) Thời gian/Lần lặp (s)
10 1,150 81.9 ± 0.1 2.70
20 1,210 80.3 ± 0.3 5.87
50 1,385 81.8 ± 0.2 6.20
200 2,542 76.5 ± 0.4 7.53

--- TRANG 6 ---
NỘP CHO IEEE TRANSACTIONS 6

(a) (b) (c) (d)
Hình 5: So sánh các phương pháp tăng cường dữ liệu khác nhau trên học various-way-various-shot cho bốn bộ dữ liệu
out-of-domain. (a) Kết quả trên bộ dữ liệu UCM. (b) Kết quả trên bộ dữ liệu WHU. (c) Kết quả trên bộ dữ liệu NWPU.
(d) Kết quả trên bộ dữ liệu AID. None nghĩa là không có tăng cường dữ liệu, RPR-aug nghĩa là Random Patch Recombination,
và PMF-aug nghĩa là phương pháp tăng cường dữ liệu PMF.

mô hình ở 10 trong bài báo này và áp dụng cài đặt này một
cách nhất quán trên tất cả các tác vụ.

C. So sánh với Phương pháp SoTA
Kết quả Chính và So sánh. Chúng tôi trình bày kết quả thí
nghiệm thu được dưới ba benchmark đánh giá riêng biệt,
trong đó MW và MS được đặt thành 5, 10, và 20 tương ứng,
trong Bảng VI-VIII. So sánh những kết quả này, chúng ta có
thể suy ra rằng MVP vượt trội hơn các phương pháp SoTA
về độ chính xác trung bình. Cụ thể, phương pháp MVP của
chúng tôi đạt được cải thiện đáng kể so với phương pháp
baseline mạnh PMF, với mức tăng trung bình 3.0%, 3.1%,
và 3.8% ở MW và MS 5/10/20. Những phát hiện này hỗ trợ
quan điểm của chúng tôi rằng các kỹ thuật prompt-tuning
phù hợp hơn cho các tác vụ phân loại ít mẫu so với các kỹ
thuật full-tuning.

Trong bối cảnh các tác vụ in-domain, MVP thể hiện lợi thế
đáng kể so với các phương pháp SoTA khác, đặc biệt trong
các benchmark đánh giá chi tiết như bộ dữ liệu CUB, Fungi,
và Flower. Trong những bộ dữ liệu này, MVP vượt trội hơn
tất cả các phương pháp được liệt kê khác về hiệu suất. Quan
sát này nhấn mạnh thêm khả năng vượt trội của công nghệ
MVP trong các tác vụ phân loại ít mẫu chi tiết. Đáng chú ý,
khi so sánh với phương pháp PMF, cách tiếp cận MVP thể
hiện lợi thế nổi bật nhất trong bộ dữ liệu Fungi ở MW và
MS 5/10/20, với cải thiện 5.1%, 7%, và 11.4% tương ứng.

Liên quan đến các tác vụ out-of-domain, phân tích của chúng
tôi chỉ ra rằng MVP thường vượt trội hơn phương pháp PMF
đứng thứ hai tốt nhất. Xu hướng này đặc biệt nổi bật trong
bộ dữ liệu NWPU, nơi MVP cải thiện 0.7%, 1.4%, và 4%
ở MW và MS 5/10/20. Hơn nữa, những kết quả này cho thấy
rằng MVP có khả năng khái quát hóa tốt hơn khi đối phó với
các tác vụ FS-RSSC mới chưa thấy.

Phân tích Tham số Mô hình và Hiệu suất. Bằng cách sử
dụng Bảng VI-VIII và Bảng III, chúng ta có thể thực hiện
phân tích toàn diện về cách các tham số mô hình tác động
đến hiệu suất và độ chính xác của mô hình. Nghiên cứu của
chúng tôi chứng minh rằng, với việc sử dụng framework
prompt-tuning dựa trên meta, ViT-tiny đạt được cải thiện
hiệu suất đáng kể so với RN18 mặc dù có số lượng tham số
tương tự. Cụ thể, trong tác vụ FS-RSSC out-of-domain,
phương pháp MVP-tiny của chúng tôi cho thấy mức tăng
độ chính xác trung bình 19.2% so với phương pháp DC-DML [10]
dựa trên RN18 trên tất cả ba benchmark đánh giá. Hơn nữa,
trong khi ViT-small sở hữu gấp bốn lần nhiều tham số hơn
ViT-tiny, nó không thể hiện sự gia tăng độ chính xác đáng
kể cho tác vụ FS-RSSC. Tuy nhiên, phương pháp MVP-small
chỉ cho thấy mức tăng độ chính xác trung bình 3.9% so với
MVP-tiny trên tất cả ba benchmark đánh giá. Do đó, kết quả
của chúng tôi cho thấy rằng việc triển khai các ứng dụng
dựa trên MVP-tiny có thể cung cấp sự cân bằng tốt hơn giữa
hiệu quả và hiệu suất.

Tác động của Domain Shift đối với Độ chính xác Phân loại.
Kết quả cross-domain được hiển thị trong Bảng VII và Bảng VIII
chỉ ra rằng tất cả các thuật toán đều gặp phải mức độ suy
giảm độ chính xác khác nhau khi xử lý tác vụ MW20 MS20
so với tác vụ MW10 MS10. Hiện tượng này không có trong
các tình huống in-domain. Điều này cho thấy rằng domain
shift, đặc biệt khi số lượng danh mục tăng lên, có tác động
đáng kể đến độ chính xác phân loại. Tuy nhiên, kết quả cũng
cho thấy rằng mô hình MVP có khả năng chống chịu tốt hơn
với domain shift. Cụ thể, MVP đạt được độ chính xác cao
trên các bộ dữ liệu out-of-domain trong khi giảm thiểu hiệu
quả tác động của domain shift. Trên các bộ dữ liệu UCM,
WHU, NWPU, và AID, MVP-small và MVP-tiny cho thấy
mức giảm độ chính xác trung bình 1.34% và 1.5% tương ứng,
trong khi PMF-small và PMF-tiny cho thấy mức giảm trung
bình 1.95% và 5.65%, và DC-DML cho thấy mức giảm trung
bình 7.6%. Do đó, mô hình MVP vượt trội hơn các mô hình
khác trong các tác vụ FS-RSSC cross-domain, đặc biệt khi
số lượng danh mục lớn.

Kết quả cho Học One-Shot. Hình 6 trình bày kết quả của
mô hình chúng tôi trên tác vụ học one-shot với một ảnh
support cho mỗi danh mục. Phương pháp MVP của chúng
tôi vượt trội hơn phương pháp baseline mạnh PMF trong
tất cả các tác vụ học one-shot. Cụ thể, khi sử dụng ViT-tiny
làm mạng backbone, MVP-tiny đạt được kết quả tốt hơn
PMF-tiny trong tất cả mười tác vụ đánh giá. Trung bình
tất cả các tác vụ đánh giá, MVP-tiny cải thiện 6.64%, 6.84%,
và 3.80% so với PMF-tiny khi MW và MS là 5/10/20 tương ứng;
MVP-small cải thiện

(a) (b)
Hình 6: Kết quả của mô hình PMF [1] và MVP của chúng tôi
trên học various-way-one-shot. (a) Kết quả với mạng backbone
ViT-tiny. (b) Kết quả với mạng backbone ViT-small.

3.46%, 3.94%, và 3.49% so với PMF-small khi MW và MS
là 5/10/20 tương ứng. Trung bình tất cả các tác vụ out-of-domain,
MVP-tiny cải thiện 2.44%, 3.07%, và 2.73% so với PMF-tiny
trong ba tác vụ này tương ứng; MVP-small cải thiện 0.82%,
0.62%, và 0.12% so với PMF-small trong ba tác vụ này tương ứng.
Những kết quả này chứng minh rằng phương pháp MVP của
chúng tôi có khả năng khái quát hóa tốt hơn trong tác vụ học
1-shot đầy thử thách.

Phân tích Định tính về Phân loại Ít mẫu. Mục đích của
phần này là kiểm tra sự khác biệt giữa các đầu ra được tạo
ra bởi các mô hình MVP và PMF. Dựa trên kết quả in-domain
được trình bày trong Bảng VI đến VIII, được quan sát thấy
rằng mô hình MVP thể hiện lợi thế đáng chú ý trong việc
xử lý các tác vụ phân loại chi tiết. Điều này đã dẫn chúng
tôi đến giả thuyết rằng việc chuyển giao khả năng phân loại
chi tiết có thể góp phần vào hiệu suất vượt trội của mô hình
MVP trong các tác vụ downstream. Để đánh giá giả thuyết
này, một thí nghiệm được tiến hành trong đó một tác vụ
10-way 5-shot được trích xuất từ bộ dữ liệu NWPU, khác
biệt với bộ dữ liệu AIFS-in-domain được sử dụng để huấn
luyện các mô hình. Các đầu ra được tạo ra bởi mỗi mô hình
sau đó được so sánh cho mỗi ảnh query. Một cuộc kiểm tra
tỉ mỉ được thực hiện trên các danh mục được dự đoán chính
xác bởi mô hình MVP nhưng dự đoán sai bởi mô hình PMF.
Đáng chú ý, khi so sánh hai danh mục tương tự: "dense residential"
và "medium residential", được xác định rằng mô hình MVP
đạt được độ chính xác trung bình cao hơn đáng kể so với
mô hình PMF. Để kiểm tra thêm hiện tượng này, một thí
nghiệm 2-way 1-shot được thiết kế. Như được mô tả trong
Hình 7, rõ ràng là mô hình MVP được trang bị tốt hơn để
xử lý các vấn đề phân loại chi tiết. Tóm lại, những quan sát
này chỉ ra

--- TRANG 7 ---
NỘP CHO IEEE TRANSACTIONS 7

BẢNG VI: ĐỘ CHÍNH XÁC IN-DOMAIN VÀ OUT-OF-DOMAIN CỦA CÁC MÔ HÌNH KHÁC NHAU VỚI MAXWAY = 5 VÀ
MAXSHOT = 5. '†' BIỂU THỊ CÁC KẾT QUẢ MÀ CHÚNG TÔI TRIỂN KHAI LẠI.

Mô hình Backbone Điều chỉnh Độ chính xác In-Domain (%) Độ chính xác Out-of-Domain (%) TB
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID

k-NN [42] RN18 Full 64.1±0.6 36.8±0.4 45.9±0.6 73.8±0.5 45.9±0.5 50.1±0.5 51.8±0.6 54.6±0.7 42.1±0.5 45.9±0.5 51.1
Finetune [43] RN18 Full 57.0±1.7 38.3±1.2 45.8±1.8 73.9±1.2 50.1±1.3 54.5±1.4 63.6±1.7 76.1±1.5 55.5±1.6 60.2±1.7 57.5
MatchingNet [7] RN18 Full 49.7±0.5 36.7±0.4 38.2±0.5 66.1±0.4 52.6±0.4 46.9±0.4 54.2±0.5 65.0±0.5 46.9±0.5 51.3±0.5 50.8
ProtoNet [41] RN18 Full 44.9±0.5 33.5±0.3 35.0±0.4 56.2±0.5 35.7±0.4 42.8±0.5 51.1±0.5 54.2±0.5 42.2±0.4 44.6±0.4 44.0
fo-MAML [20] RN18 Full 59.5±0.6 38.5±0.4 44.0±0.5 70.3±0.5 49.0±0.4 49.7±0.5 52.1±0.5 60.8±0.5 44.5±0.5 50.0±0.5 51.8
RelationNet [44] RN18 Full 60.5±1.9 38.8±1.3 46.8±1.9 72.8±1.4 79.9±1.2 50.9±1.6 56.6±1.8 65.4±1.6 49.3±1.5 52.8±1.6 57.4
Proto-MAML [13] RN18 Full 47.0±1.5 33.3±1.1 35.5±1.4 60.1±1.4 36.6±1.2 41.9±1.3 50.9±1.4 52.6±1.4 41.6±1.4 44.2±1.5 44.4
CNAPs [42] RN18 Full 65.6±0.6 41.5±0.5 46.5±0.5 69.7±0.9 43.2±0.7 55.7±0.5 66.9±0.6 61.8±0.5 49.1±0.5 54.6±0.5 55.5
SimpleCNAPs [9] RN18 Full 64.0±0.5 44.9±0.9 49.8±0.5 73.1±0.4 48.5±0.4 64.8±0.5 74.6±0.5 74.5±0.5 57.0±0.5 65.2±0.5 61.6
DC-DML [10] RN18 Full 62.2±0.5 49.5±0.5 50.6±0.6 82.2±0.4 48.7±0.4 59.7±0.5 78.9±0.5 80.9±0.5 60.6±0.5 68.6±0.6 64.2
PMF-tiny † [1] ViT-t Full 83.2±0.3 61.5±0.2 55.6±0.5 83.8±0.2 43.5±0.2 54.6±0.4 83.5±0.2 88.2±0.1 75.3±0.3 77.6±0.2 70.7
PMF-small † [1] ViT-s Full 84.3±0.2 72.3±0.1 61.8±0.5 86.3±0.1 53.7±0.5 61.3±0.3 89.1±0.1 93.0±0.4 80.8±0.2 83.8±0.1 76.6
MVP-tiny (ours) ViT-t Prompt 87.9±0.3 66.1±0.2 65.9±0.4 89.1±0.4 55.9±0.3 63.5±0.2 84.1±0.3 89.6±0.4 77.2±0.2 79.6±0.2 75.9
MVP-small (ours) ViT-s Prompt 87.2±0.2 73.4±0.1 66.9±0.4 91.7±0.3 60.3±0.4 67.3±0.2 89.3±0.3 93.4±0.1 81.5±0.2 84.7±0.2 79.6

BẢNG VII: ĐỘ CHÍNH XÁC IN-DOMAIN VÀ OUT-OF-DOMAIN CỦA CÁC MÔ HÌNH KHÁC NHAU VỚI MAXWAY = 10 VÀ
MAXSHOT = 10. '†' BIỂU THỊ CÁC KẾT QUẢ MÀ CHÚNG TÔI TRIỂN KHAI LẠI.

Mô hình Backbone Điều chỉnh Độ chính xác In-Domain (%) Độ chính xác Out-of-Domain (%) TB
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID

k-NN [42] RN18 Full 59.2±1.6 36.5±1.2 39.3±1.6 68.1±1.3 42.4±1.3 45.0±1.3 49.5±1.5 52.7±1.5 38.2±1.4 41.7±1.3 47.3
Finetune [43] RN18 Full 37.4±1.1 31.9±1.2 33.7±1.3 56.1±1.3 46.8±1.1 37.6±1.2 48.5±1.5 61.1±1.3 41.8±1.3 46.8±1.4 44.2
MatchingNet [7] RN18 Full 42.3±1.5 33.5±1.1 31.5±1.3 58.1±1.4 52.9±1.3 39.7±1.3 48.9±1.5 58.3±1.3 40.0±1.4 46.0±1.3 45.1
ProtoNet [41] RN18 Full 35.4±1.2 29.1±1.1 24.2±1.1 42.1±1.3 28.4±1.0 34.8±1.2 42.3±1.3 45.9±1.3 33.9±1.3 35.1±1.2 35.1
fo-MAML [20] RN18 Full 51.3±1.5 35.6±1.3 37.5±1.4 61.5±1.3 43.2±1.2 40.3±1.3 50.1±1.4 58.2±1.4 39.2±1.4 44.8±1.3 46.2
RelationNet [44] RN18 Full 44.9±1.5 33.1±1.2 35.7±1.7 58.3±1.5 73.4±1.3 36.8±1.4 49.1±1.5 59.8±1.3 40.2±1.4 46.0±1.5 47.7
Proto-MAML [13] RN18 Full 37.5±1.4 28.3±1.0 27.0±1.0 48.3±1.4 28.4±1.0 33.0±1.2 39.3±1.4 41.6±1.5 32.2±1.1 33.6±1.3 34.9
CNAPs [42] RN18 Full 59.5±0.5 44.4±0.4 46.6±0.5 70.1±0.4 55.8±0.5 52.8±0.5 61.7±0.5 60.6±0.5 44.3±0.5 52.0±0.5 54.8
SimpleCNAPs [9] RN18 Full 64.6±0.5 40.4±0.4 47.8±0.6 68.6±0.4 54.3±0.5 51.6±0.5 62.1±0.5 61.5±0.5 43.6±0.5 52.7±0.5 54.7
DC-DML [10] RN18 Full 56.3±0.5 45.7±0.5 43.1±0.6 71.9±0.4 47.5±0.4 53.7±0.5 69.4±0.6 72.2±0.5 49,7±0.6 56.8±0.6 57.4
PMF-tiny † [1] ViT-t Full 85.9±0.1 67.6±0.4 54.4±0.4 89.1±0.1 51.5±0.4 60.2±0.6 86.2±0.2 90.4±0.5 78.2±0.3 80.5±0.3 74.4
PMF-small † [1] ViT-s Full 86.1±0.1 75.5±0.1 60.4±0.4 87.2±0.1 58.3±0.3 62.7±0.4 89.8±0.3 93.8±0.2 81.8±0.1 84.8±0.1 78.1
MVP-tiny (ours) ViT-t Prompt 89.1±0.3 68.9±0.1 66.0±0.3 89.0±0.2 56.9±0.3 64.4±0.4 86.8±0.1 90.0±0.4 78.6±0.3 82.1±0.2 77.3
MVP-small (ours) ViT-s Prompt 88.8±0.2 76.2±0.2 67.4±0.1 92.4±0.3 64.9±0.4 69.3±0.3 89.9±0.2 94.4±0.3 83.2±0.4 85.1±0.1 81.2

BẢNG VIII: ĐỘ CHÍNH XÁC IN-DOMAIN VÀ OUT-OF-DOMAIN CỦA CÁC MÔ HÌNH KHÁC NHAU VỚI MAXWAY = 20 VÀ
MAXSHOT = 20. '†' BIỂU THỊ CÁC KẾT QUẢ MÀ CHÚNG TÔI TRIỂN KHAI LẠI.

Mô hình Backbone Điều chỉnh Độ chính xác In-Domain (%) Độ chính xác Out-of-Domain (%) TB
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID

k-NN [42] RN18 Full 49.7±0.6 45.4±0.4 32.7±0.5 72.5±0.4 48.5±0.5 44.9±0.5 49.2±0.5 53.8±0.5 35.7±0.5 40.5±0.5 47.3
Finetune [43] RN18 Full 33.3±1.0 40.4±0.7 27.5±0.7 54.6±1.0 47.8±1.0 37.0±1.0 46.9±1.0 56.1±1.0 37.1±1.0 41.4±1.0 42.2
MatchingNet [7] RN18 Full 38.5±0.6 41.8±0.4 27.4±0.4 63.9±0.5 58.8±0.5 40.5±0.5 44.7±0.6 54.6±0.5 35.9±0.5 40.6±0.5 44.7
ProtoNet [41] RN18 Full 30.8±0.6 40.0±0.4 20.1±0.4 45.2±0.6 31.5±0.4 32.8±0.6 38.2±0.6 41.7±0.7 29.4±0.6 30.5±0.6 34.1
fo-MAML [20] RN18 Full 45.2±0.6 42.9±0.4 31.4±0.5 63.7±0.5 48.1±0.5 39.5±0.6 42.4±0.6 48.8±0.6 32.4±0.5 36.3±0.5 43.1
RelationNet [44] RN18 Full 35.3±0.6 38.0±0.4 27.6±0.5 65.5±0.5 85.7±0.3 34.3±0.5 40.9±0.6 50.7±0.6 31.9±0.5 37.5±0.6 44.7
Proto-MAML [13] RN18 Full 35.4±1.1 39.4±0.7 22.7±0.7 49.8±1.1 33.8±1.0 34.0±1.0 36.4±1.1 40.5±1.1 28.4±1.0 30.7±1.0 35.1
CNAPs [42] RN18 Full 54.2±0.6 48.9±0.4 41.5±0.6 77.0±0.4 62.7±0.5 51.2±0.6 57.8±0.5 52.5±0.6 38.8±0.6 44.2±0.6 52.9
SimpleCNAPs [9] RN18 Full 54.9±0.6 47.8±0.4 41.7±0.6 74.2±0.4 71.1±0.4 48.0±0.6 55.6±0.6 50.3±0.7 40.0±0.6 45.5±0.6 52.9
DC-DML [10] RN18 Full 48.6±0.7 54.5±0.4 36.0±0.6 76.9±0.4 51.3±0.4 53.0±0.6 62.8±0.6 63.0±0.6 43.3±0.7 48.6±0.7 53.8
PMF-tiny † [1] ViT-t Full 84.4±0.2 69.1±0.3 49.5±0.6 84.5±0.6 40.8±0.4 52.7±0.5 81.3±0.2 86.2±0.1 70.3±0.4 74.9±0.4 69.4
PMF-small † [1] ViT-s Full 84.1±0.3 80.3±0.4 55.7±0.2 85.6±0.2 74.7±0.3 60.4±1.5 88.2±0.3 93.5±0.1 77.8±0.2 83.0±1.1 78.3
MVP-tiny (ours) ViT-t Prompt 87.2±0.3 72.2±0.3 61.3±0.4 89.9±0.4 64.4±0.5 63.4±0.5 85.6±0.2 90.1±0.1 76.9±0.3 79.9±0.3 77.1
MVP-small (ours) ViT-s Prompt 88.9±0.2 80.9±1.4 67.1±0.3 94.3±0.6 78.7±0.5 67.1±0.4 89.7±0.1 93.7±0.2 81.8±0.4 84.3±0.2 82.1

rằng mô hình MVP sở hữu khả năng học các đặc trưng phân
biệt và có thể chuyển giao hơn, từ đó tạo điều kiện thuận
lợi cho việc chuyển giao hiệu quả các khả năng phân loại
chi tiết cho các tác vụ downstream.

V. KẾT LUẬN
Trong bài báo này, chúng tôi tập trung vào tình huống thử
thách và thực tế của phân loại cảnh viễn thám ít mẫu (FS-RSSC),
nơi mục tiêu là phân loại ảnh viễn thám thành các danh mục
khác nhau chỉ sử dụng một vài ví dụ có nhãn cho mỗi danh
mục. Để giải quyết vấn đề này, chúng tôi đã đề xuất một
phương pháp mới và hiệu quả được gọi là Meta Visual Prompt
Tuning (MVP) tận dụng prompt tuning và meta-learning để
thích ứng mô hình visual transformer (ViT) được tiền huấn
luyện với các tác vụ mới với dữ liệu và tài nguyên tối thiểu.
Cụ thể, MVP có ba thành phần chính: (1) prompt tuning,
một kỹ thuật fine-tuning hiệu quả tham số chỉ cập nhật các
tham số prompt mới được thêm vào và giữ nguyên phần còn
lại của mô hình, điều này giảm nhu cầu lưu trữ và giảm thiểu
rủi ro overfitting; (2) meta-learning, một kỹ thuật thích ứng
nhanh học cách khởi tạo các tham số prompt từ nhiều tác vụ
nguồn và nhanh chóng thích ứng chúng với các tác vụ mới
chỉ với một vài bước gradient, điều này tạo điều kiện thuận
lợi cho thích ứng cross-domain; và (3) tăng cường dữ liệu,
một kỹ thuật mới hoạt động trên các patch embeddings của
các token đầu vào của các khối transformer để nâng cao
biểu diễn cảnh và tính đa dạng. Chúng tôi đã đánh giá MVP
trên một bộ dữ liệu benchmark FS-RSSC cross-domain thực
tế và chứng minh hiệu suất vượt trội của nó so với các phương
pháp hiện có. MVP đạt được kết quả đặc biệt đáng chú ý
trên các tác vụ vary-way, vary shot, và one-shot, những tác
vụ này thử thách hơn và phù hợp hơn cho các ứng dụng thực
tế. Công trình của chúng tôi đã mở đường cho việc áp dụng
các mô hình ViT vào các tác vụ FS-RSSC và cung cấp một
giải pháp phù hợp cho nền tảng triển khai các thuật toán
FS-RSSC. Trong tương lai, chúng tôi kỳ vọng mở rộng domain
dữ liệu huấn luyện để bao gồm nhiều phân phối dữ liệu hơn,
điều này có thể cải thiện thêm độ chính xác và tính mạnh
mẽ của các phương pháp dựa trên prompt tuning cho các
tác vụ FS-RSSC, đặc biệt là để nâng cao hiệu suất cross-domain.

TÀI LIỆU THAM KHẢO
[1] Hu Shell Xu, Li Da, Stühmer Jan, Kim Minyoung, Hospedales Timothy
M, Pushing the limits of simple pipelines for few-shot learning: External
data and fine-tuning make a difference, trong: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
9068–9077.
[2] Jia Menglin, Tang Luming, Chen Bor-Chun, Cardie Claire, Belongie
Serge, Hariharan Bharath, Lim Ser-Nam, Visual prompt tuning, trong:
Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXXIII, Springer, 2022,
pp. 709–727.
[3] Zhenqi Cui, Wang Yang, Li Chen, Haifeng Li, Mkn: Metakernel networks for few shot remote sensing scene classification, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 60.
[4] Zhu Xiao Xiang, Tuia Devis, Mou Lichao, Xia Gui-Song, Zhang
Liangpei, Xu Feng, Fraundorfer Friedrich, Deep learning in remote
sensing: A comprehensive review and list of resources, IEEE geoscience
and remote sensing magazine 5 (4) (2017) 8–36.
[5] Pires de Lima Rafael, Marfurt Kurt, Convolutional neural network for
remote-sensing scene classification: Transfer learning analysis, Remote
Sensing 12 (1) (2019) 86.
[6] Li Yong, Shao Zhenfeng, Huang Xiao, Cai Bowen, Peng Song, Metafseo: A meta-learning fast adaptation with self-supervised embedding
optimization for few-shot remote sensing scene classification, Remote
Sensing 13 (14) (2021) 2776.
[7] Vinyals Oriol, Blundell Charles, Lillicrap Tim, Wierstra Daan, và cộng sự,
Matching networks for one shot learning, trong: nips, 2016, pp. 3630–3638.
[8] He Kaiming, Zhang Xiangyu, Ren Shaoqing, Sun Jian, Deep residual
learning for image recognition, trong: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 770–778.
[9] Bateni Peyman, Goyal Raghav, Masrani Vaden, Wood Frank, Sigal
Leonid, Improved few-shot visual classification, trong: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 14493–14502.
[10] Li Lingjun, Yao Xiwen, Cheng Gong, Han Junwei, Aifs-dataset for few-shot aerial image scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2022) 1–11.
[11] Bertinetto Luca, Henriques Joao F, Torr Philip HS, Vedaldi Andrea,
Meta-learning with differentiable closed-form solvers, arXiv preprint
arXiv:1805.08136.
[12] Guo Yunhui, Codella Noel C, Karlinsky Leonid, Codella James V, Smith
John R, Saenko Kate, Rosing Tajana, Feris Rogerio, A broader study of
cross-domain few-shot learning, trong: European Conference on Computer
Vision, Springer, 2020, p. 124–141.
[13] Triantafillou Eleni, Zhu Tyler, Dumoulin Vincent, Lamblin Pascal,
Evci Utku, Xu Kelvin, Goroshin Ross, Gelada Carles, Swersky Kevin,
Manzagol Pierre-Antoine, và cộng sự, Meta-dataset: A dataset of datasets for
learning to learn from few examples, arXiv preprint arXiv:1903.03096.
[14] Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan
Jared D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry
Girish, Askell Amanda, và cộng sự, Language models are few-shot learners,
Advances in neural information processing systems 33 (2020) 1877–
1901.
[15] Gong Maoguo, Li Jianzhao, Zhang Yourun, Wu Yue, Zhang Mingyang,
Two-path aggregation attention network with quad-patch data augmentation for few-shot scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2022) 1–16.
[16] Cheng Gong, Cai Liming, Lang Chunbo, Yao Xiwen, Chen Jinyong, Guo
Lei, Han Junwei, Spnet: Siamese-prototype network for few-shot remote
sensing image scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2021) 1–11.
[17] Li Haifeng, Cui Zhenqi, Zhu Zhiqing, Chen Li, Zhu Jiawei, Huang
Haozhe, Tao Chao, Rs-metanet: Deep meta metric learning for few-shot
remote sensing scene classification, arXiv preprint arXiv:2009.13364.

--- TRANG 8 ---
NỘP CHO IEEE TRANSACTIONS 8

(a) (b)

(c) (d)

Hình 7: Kết quả định tính của phân loại ít mẫu cho PMF
và MVP của chúng tôi.

[18] Li Lingjun, Han Junwei, Yao Xiwen, Cheng Gong, Guo Lei, Dla-matchnet for few-shot remote sensing image scene classification, IEEE
Transactions on Geoscience and Remote Sensing 59 (9) (2020) 7844–
7853.
[19] Zeng Qingjie, Geng Jie, Task-specific contrastive learning for few-shot
remote sensing image scene classification, ISPRS Journal of Photogrammetry and Remote Sensing 191 (2022) 143–154.
[20] Finn Chelsea, Abbeel Pieter, Levine Sergey, Model-agnostic meta-learning for fast adaptation of deep networks, trong: Proc. Int. Conf.
Machine Learning, 2017, pp. 1126–1135.
[21] Zhang Pei, Bai Yunpeng, Wang Dong, Bai Bendu, Li Ying, A meta-learning framework for few-shot classification of remote sensing scene,
trong: ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 4590–4594.
[22] Ma Chenhui, Mu Xiaodong, Zhao Peng, Yan Xin, Meta-learning based
on parameter transfer for few-shot classification of remote sensing
scenes, Remote Sensing Letters 12 (6) (2021) 531–541.
[23] Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn
Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer
Matthias, Heigold Georg, Gelly Sylvain, và cộng sự, An image is worth
16x16 words: Transformers for image recognition at scale, arXiv preprint
arXiv:2010.11929.
[24] Roy Swalpa Kumar, Deria Ankur, Hong Danfeng, Rasti Behnood, Plaza
Antonio, Chanussot Jocelyn, Multimodal fusion transformer for remote
sensing image classification, arXiv preprint arXiv:2203.16952.
[25] Bi Meiqiao, Wang Minghua, Li Zhi, Hong Danfeng, Vision transformer
with contrastive learning for remote sensing image scene classification,
IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 16 (2022) 738–749.
[26] Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina, Bert:
Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805.
[27] Logan IV Robert L, Balažević Ivana, Wallace Eric, Petroni Fabio,
Singh Sameer, Riedel Sebastian, Cutting down on prompts and parameters: Simple few-shot learning with language models, arXiv preprint
arXiv:2106.13353.
[28] Rebuffi Sylvestre-Alvise, Bilen Hakan, Vedaldi Andrea, Efficient
parametrization of multi-domain deep neural networks, trong: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 8119–8127.
[29] Zhang Jeffrey O, Sax Alexander, Zamir Amir, Guibas Leonidas, Malik
Jitendra, Side-tuning: a baseline for network adaptation via additive side
networks, trong: Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, Springer,
2020, pp. 698–714.
[30] Li Xiang Lisa, Liang Percy, Prefix-tuning: Optimizing continuous
prompts for generation, arXiv preprint arXiv:2101.00190.
[31] Lester Brian, Al-Rfou Rami, Constant Noah, The power of scale for
parameter-efficient prompt tuning, arXiv preprint arXiv:2104.08691.
[32] Shorten Connor, Khoshgoftaar Taghi M, A survey on image data
augmentation for deep learning, Journal of big data 6 (1) (2019) 1–48.
[33] Gidaris Spyros, Singh Praveer, Komodakis Nikos, Unsupervised representation learning by predicting image rotations, ICLR.
[34] Zhang Chi, Cai Yujun, Lin Guosheng, Shen Chunhua, Deepemd: Few-shot image classification with differentiable earth mover's distance and
structured classifiers, trong: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2020, pp. 12203–12213.
[35] Yun Sangdoo, Han Dongyoon, Oh Seong Joon, Chun Sanghyuk, Choe
Junsuk, Yoo Youngjoon, Cutmix: Regularization strategy to train strong
classifiers with localizable features, trong: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 6023–6032.
[36] Kim Jang-Hyun, Choo Wonho, Song Hyun Oh, Puzzle mix: Exploiting
saliency and local statistics for optimal mixup, trong: International Conference on Machine Learning, PMLR, 2020, pp. 5275–5285.
[37] Li Kai, Zhang Yulun, Li Kunpeng, Fu Yun, Adversarial feature hallucination networks for few-shot learning, trong: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
13470–13479.
[38] Ni Renkun, Goldblum Micah, Sharaf Amr, Kong Kezhi, Goldstein Tom,
Data augmentation for meta-learning, trong: International Conference on
Machine Learning, PMLR, 2021, pp. 8152–8161.
[39] Yang Shuo, Liu Lu, Xu Min, Free lunch for few-shot learning: Distribution calibration, arXiv preprint arXiv:2101.06395.
[40] Deng Jia, Dong Wei, Socher Richard, Li Li-Jia, Li Kai, Fei-Fei Li,
Imagenet: A large-scale hierarchical image database, trong: 2009 IEEE
conference on computer vision and pattern recognition, Ieee, 2009, pp.
248–255.
[41] Snell Jake, Swersky Kevin, Zemel Richard, Prototypical networks for
few-shot learning, trong: nips, 2017, pp. 4077–4087.
[42] Requeima James, Gordon Jonathan, Bronskill John, Nowozin Sebastian,
Turner Richard E, Fast and flexible multi-task classification using
conditional neural adaptive processes, Advances in Neural Information
Processing Systems 32.
[43] Yosinski Jason, Clune Jeff, Bengio Yoshua, Lipson Hod, How transferable are features in deep neural networks?, Advances in neural
information processing systems 27.
[44] Yang Flood Sung Yongxin, Zhang Li, Xiang Tao, Torr Philip HS,
Hospedales Timothy M, Learning to compare: Relation network for few-shot learning, trong: cvpr, 2018, pp. 1199–1208.
[45] Caron Mathilde, Touvron Hugo, Misra Ishan, Jégou Hervé, Mairal
Julien, Bojanowski Piotr, Joulin Armand, Emerging properties in self-supervised vision transformers, trong: Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 9650–9660.
