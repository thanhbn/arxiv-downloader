Điều chỉnh tham số hiệu quả với cắt tỉa lớp trong mô hình hóa chuỗi-đến-chuỗi văn bản tự do

Yunqi Zhu1,2 Xuebing Yang2 Yuanyuan Wu1 Wensheng Zhang1,2,3
1Trường Kỹ thuật Thông tin và Truyền thông, Đại học Hainan
2Phòng thí nghiệm Trọng điểm Nhà nước về Hệ thống Trí tuệ Nhân tạo Đa phương thức, Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc
3Đại học Quảng Châu

Tóm tắt
Kích thước ngày càng tăng của các mô hình ngôn ngữ làm nảy sinh sự quan tâm nghiên cứu lớn về điều chỉnh tham số hiệu quả như LoRA, phương pháp đóng băng mô hình đã được tiền huấn luyện và chèn các tham số có thể huấn luyện quy mô nhỏ cho nhiều tác vụ hạ lưu (ví dụ: tóm tắt, hỏi đáp và dịch thuật). Để tiếp tục nâng cao hiệu quả của việc điều chỉnh, chúng tôi đề xuất một khung kết hợp LoRA và cắt tỉa lớp có cấu trúc. Khung tích hợp này được xác thực trên hai bộ dữ liệu tóm tắt báo cáo y tế đã được khử định danh được tạo ra dựa trên MIMIC-IV-Note và hai bộ dữ liệu đối thoại y tế công khai. Bằng cách điều chỉnh 0,6% tham số của mô hình gốc và cắt tỉa hơn 30% các lớp Transformer, khung của chúng tôi có thể giảm 50% việc sử dụng bộ nhớ GPU và tăng tốc 100% giai đoạn huấn luyện, trong khi vẫn duy trì hơn 92% chất lượng sinh ra trên các tác vụ chuỗi-đến-chuỗi văn bản tự do.

1 Giới thiệu
Mô hình ngôn ngữ tiền huấn luyện (PLM) đã thống trị lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) do hiệu suất vượt trội bằng cách điều chỉnh mô hình cho các tác vụ NLP hạ lưu khác nhau (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020). Với kích thước mô hình ngày càng tăng của PLM (ví dụ: 11B cho T5 (Raffel et al., 2020), 175B cho GPT-3 (Brown et al., 2020) và 540B cho PaLM (Chowdhery et al., 2022)), việc điều chỉnh toàn bộ PLM đòi hỏi lưu trữ tất cả các tham số mô hình được cập nhật cho từng tác vụ, điều này sẽ tốn kém và mất thời gian.

Để giảm bớt vấn đề trên, các phương pháp điều chỉnh tham số hiệu quả đang xuất hiện. Ví dụ, việc điều chỉnh dựa trên Adapter (Houlsby et al., 2019; Pfeiffer et al., 2020; Wang et al., 2021; Pfeiffer et al., 2021; Karimi Mahabadi et al., 2021) chèn các mạng nơ-ron có thể huấn luyện quy mô nhỏ và đóng băng các tham số khác của PLM trong quá trình huấn luyện. Dựa trên kiến thức tiên nghiệm của các tác vụ hạ lưu, việc điều chỉnh dựa trên prompt (Li và Liang, 2021; Liu et al., 2019; Lester et al., 2021; Liu et al., 2022) bao gồm các token ngữ cảnh có thể huấn luyện cụ thể cho tác vụ xung quanh chuỗi đầu vào và đóng băng đầu vào gốc trong quá trình huấn luyện. Gần đây, thích ứng thứ hạng thấp (LoRA) (Hu et al., 2022), một biến thể dựa trên việc điều chỉnh Adapter nhưng loại bỏ kích hoạt phi tuyến, đã trở nên ngày càng phổ biến cho việc điều chỉnh tham số quy mô nhỏ. Bên cạnh đó, việc cắt tỉa PLM phục vụ để loại bỏ một số tham số của mô hình trong khi vẫn phần lớn bảo tồn khả năng cho các tác vụ hạ lưu (Jiao et al., 2020; Zhang và He, 2020; Sanh et al., 2020; Fan et al., 2020; Sajjad et al., 2023; Lagunas et al., 2021; Xia et al., 2022), thường được xử lý theo cách thích ứng hoặc có cấu trúc.

Vì cả việc điều chỉnh dựa trên Adapter và cắt tỉa đều đưa ra giả thuyết rằng PLM bị dư thừa tham số, chúng tôi tích hợp LoRA (Hu et al., 2022) và cắt tỉa lớp có cấu trúc (L-Prun) (Fan et al., 2020; Sajjad et al., 2023) trên PLM, và Hình 1 minh họa khung tổng thể của việc tích hợp LoRA và L-Prun trên PLM dựa trên Transformer (Vaswani et al., 2017). Đề xuất của chúng tôi là cung cấp một khung sơ bộ cho phép triển khai linh hoạt và nhẹ các tham số học plug-in quy mô nhỏ với LoRA, trong khi thu nhỏ PLM với L-Prun.

Khung được đề xuất được xác thực trên hai tác vụ hạ lưu (tức là Tóm tắt Báo cáo Y tế và Sinh ra Đối thoại Y tế). Dựa trên MIMIC-IV-Note (Johnson et al., 2023a,c,b), chúng tôi đầu tiên tạo ra hai bộ dữ liệu tóm tắt báo cáo y tế văn bản tự do đã được khử định danh, có tên MIMIC-IV-discharge và MIMIC-IV-radiology. Tiếp theo, chúng tôi thực hiện các thí nghiệm mở rộng với mô hình hóa chuỗi-đến-chuỗi (Seq2Seq), bao gồm BART-large (Lewis et al., 2020) và T5-large (Raffel et al., 2020), trên cả hai bộ dữ liệu này và hai bộ dữ liệu đối thoại y tế công khai bổ sung (HealthCareMagic và iCliniq) (Zeng et al., 2020; Li et al., 2023). Hình 2 cho thấy hiệu suất của việc điều chỉnh toàn bộ, LoRA và L-Prun với BART-large tiền huấn luyện, trong đó chất lượng sinh ra của khung chúng tôi là đầy hứa hẹn.

Đóng góp của chúng tôi có thể được tóm tắt như sau: (1) Chúng tôi đề xuất một khung tích hợp mô-đun điều chỉnh tham số quy mô nhỏ LoRA và L-Prun có cấu trúc. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên kết hợp LoRA với L-Prun trên PLM. (2) Dựa trên MIMIC-IV-Note, chúng tôi tạo ra hai bộ dữ liệu tóm tắt báo cáo y tế. Chúng tôi thực hiện các thí nghiệm toàn diện trên các bộ dữ liệu được đề xuất và hai bộ dữ liệu sinh ra đối thoại y tế khác. (3) Bằng cách cập nhật 0,6% tham số so với PLM gốc và cắt tỉa hơn 30% các lớp Transformer, phương pháp được đề xuất có thể tăng gấp đôi tốc độ huấn luyện và tiết kiệm 50% việc sử dụng bộ nhớ GPU, trong khi vẫn bảo tồn hơn 92% chất lượng sinh ra trên các tác vụ Seq2Seq y tế.

2 Công việc liên quan
Transformer (Vaswani et al., 2017) đã đề xuất mô hình Transformer, chứa một bộ mã hóa và một bộ giải mã cho việc học biểu diễn của các chuỗi đầu vào và đầu ra tương ứng. Lớp bộ mã hóa Transformer bao gồm một khối attention (ATTN), một mạng feed-forward (FFN), các kết nối dư và chuẩn hóa lớp. Lớp bộ giải mã Transformer chứa một khối cross-attention bổ sung ngay sau ATTN gốc, và cross-attention có cùng thiết kế với self-attention. Được thiết kế với up- và down-projection, FFN là hai lớp kết nối đầy đủ (tức là perceptron đa lớp (MLP)).

Điều chỉnh tham số hiệu quả Việc tiền huấn luyện một mô hình ngôn ngữ lớn với mô hình hóa ngôn ngữ tự giám sát và điều chỉnh nó cho nhiều miền hạ lưu là một mô hình được áp dụng rộng rãi (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020). (Ding et al., 2023) phân loại các phương pháp điều chỉnh tham số hiệu quả phổ biến thành ba loại: Bổ sung, Xác định và Tham số hóa lại.

Bổ sung: Việc điều chỉnh dựa trên Adapter (Houlsby et al., 2019; Pfeiffer et al., 2020; Wang et al., 2021; Pfeiffer et al., 2021; Karimi Mahabadi et al., 2021) chèn các MLP down- và up-projection nhẹ và một hàm kích hoạt phi tuyến, và việc điều chỉnh dựa trên prompt (Li và Liang, 2021; Liu et al., 2019; Lester et al., 2021; Liu et al., 2022; Gu et al., 2022) đóng băng đầu vào và giới thiệu các ngữ cảnh có thể huấn luyện bổ sung (dựa trên kiến thức tiên nghiệm của các tác vụ NLP) làm tiền tố, hậu tố hoặc bán-template cho đầu vào gốc.

Xác định: điều chỉnh chỉ một số lớp, mô-đun, trọng số và bias được chỉ định hoặc thích ứng có số lượng hạn chế của mô hình (Lee et al., 2019; Zhao et al., 2020; Guo et al., 2021; He et al., 2022; Ben Zaken et al., 2022), loại phương pháp này có thể hiệu quả nhất về bộ nhớ cho việc huấn luyện vì không có tham số bổ sung nào được liên quan.

Tham số hóa lại: dựa trên giả thuyết rằng các thích ứng của PLM cho các tác vụ NLP hạ lưu có thể được tham số hóa lại thành một tối ưu hóa thứ hạng nội tại thấp (Rebuffi et al., 2017; Li et al., 2018; Aghajanyan et al., 2021), LoRA (Hu et al., 2022) kế thừa thiết kế của Adapter (Houlsby et al., 2019) nhưng loại bỏ kích hoạt phi tuyến. Do đó, các trọng số nhân của các MLP down- và up-projection có thể được thêm trực tiếp vào trọng số của mục tiêu chèn trong giai đoạn suy luận.

Ngoài ra, có các phương pháp thay thế để khai thác hiệu quả mô hình ngôn ngữ tiền huấn luyện, bao gồm Chưng cất Kiến thức (Hinton et al., 2019; Kim và Rush, 2016; Sanh et al., 2019; Jiao et al., 2020) tối đa hóa sự tương tự giữa các dự đoán của giáo viên (mô hình lớn) và học sinh (mô hình được chưng cất), Lượng tử hóa (Jacob et al., 2018; Wang et al., 2018; Dettmers et al., 2022) chuyển đổi mô hình 16/32-bit thành 8-bit hoặc thậm chí thấp hơn, điều này vừa hiệu quả cho việc huấn luyện vừa suy luận.

Cắt tỉa tham số Một mô hình tiền huấn luyện lớn có thể bị dư thừa tham số cho các tác vụ NLP hạ lưu, (Jiao et al., 2020; Zhang và He, 2020; Sanh et al., 2020; Fan et al., 2020; Sajjad et al., 2023; Lagunas et al., 2021; Xia et al., 2022) đã chỉ ra rằng việc cắt tỉa có cấu trúc và thích ứng của PLM có thể đủ khả năng cho nhiều kịch bản phân loại hoặc Seq2Seq. Cắt tỉa có cấu trúc có thể dựa trên lựa chọn thực nghiệm hoặc heuristic của mô hình, trong khi cắt tỉa không có cấu trúc có thể dựa vào một vài ngưỡng được định nghĩa trước hoặc các tham số có thể huấn luyện để quyết định thích ứng MLP, khối attention hoặc toàn bộ lớp nào nên được loại bỏ.

Ngoài ra, các phương pháp cắt tỉa tham số thay thế như loại bỏ hoặc hợp nhất trạng thái ẩn của các token không quan trọng của chuỗi (Goyal et al., 2020; Guo et al., 2022; Kim et al., 2022; Hou et al., 2022), và thực hiện attention thưa thớt (Child et al., 2019; Beltagy et al., 2020; Kitaev et al., 2020) góp phần giảm việc sử dụng bộ nhớ nhưng không thể giảm kích thước mô hình.

3 Phương pháp
Chúng tôi đề xuất một khung lai của LoRA và cắt tỉa lớp có cấu trúc có thể giảm đáng kể các tham số được huấn luyện, giảm việc sử dụng bộ nhớ, tăng tốc độ huấn luyện và nén kích thước mô hình cho các tác vụ hạ lưu. Động lực cho việc tích hợp này là cung cấp một khung sơ bộ có thể thu nhỏ PLM trên các tác vụ NLP hạ lưu thông qua L-Prun, và cho phép triển khai linh hoạt và nhẹ các tham số plug-in được huấn luyện quy mô nhỏ thông qua LoRA. Chúng tôi đầu tiên xóa các lớp Transformer theo cách xen kẽ và sau đó chèn LoRA cho mô hình, do đó phương pháp này không yêu cầu một giai đoạn cắt tỉa thích ứng hoàn toàn sau khi mô hình được điều chỉnh.

Hình ảnh hóa của khung được hiển thị trong Hình 1. Chính thức, cho một PLM với Pm tham số, LoRA cho phép Pm:lora tham số có thể huấn luyện, trong đó Pm:lora ≪ Pm, và L-Prun làm cho các tham số của PLM thu nhỏ thành Pprun, trong đó Pprun < Pm. Đối với N tác vụ hạ lưu, lưu trữ tổng thể là N × Pm cho việc điều chỉnh toàn bộ, N × Pm:lora + Pm nếu LoRA được kích hoạt, và N × Pprun:lora + Pprun nếu cả LoRA và L-Prun đều được kích hoạt. Lưu ý rằng Pprun:lora < Pm:lora do LoRA được chèn vào tất cả các lớp Transformer. Trong phần sau, chúng tôi giới thiệu ngắn gọn LoRA và Cắt tỉa Lớp, cũng như làm rõ việc triển khai của chúng tôi.

LoRA (Hu et al., 2022) đã đề xuất một phương pháp thích ứng thứ hạng thấp đóng băng các trọng số của PLM dựa trên Transformer và chèn hai lớp dày đặc có thể huấn luyện (Wdown ∈ R^(r×d) và Wup ∈ R^(d×r)) trong một lớp dày đặc W0 ∈ R^(d×d), trong đó thứ hạng r ≪ d. Do đó đầu ra ex của LoRA là:

ex = W0x + α/r WupWdownx                    (1)

trong đó α là một hệ số tỷ lệ hằng số. LoRA có thể được xem như một biến thể của Adapter (Houlsby et al., 2019) chứa các MLP down- và up-projection. Tuy nhiên, không giống như các phương pháp Adapter thông thường áp dụng kích hoạt phi tuyến giữa hai lớp dày đặc có thể huấn luyện, LoRA có thể duy trì độ trễ suy luận thấp sau giai đoạn huấn luyện bằng cách hợp nhất các trọng số đông lạnh gốc với việc chèn của LoRA:

W0 = W0 + α/r WupWdown                      (2)

Với down- và up-projection, LoRA có ít tham số hơn nhiều so với mục tiêu chèn.

Ban đầu, LoRA (LoRA QV) được áp dụng trên lớp dày đặc Query và lớp dày đặc Value trong mỗi mô-đun attention. Các nghiên cứu tiếp theo (Zhang et al., 2023; Lei et al., 2023) đã chỉ ra rằng việc chèn LoRA (LoRA FF) trong FFN có thể là một lựa chọn vượt trội. Gần đây, các nghiên cứu cắt tỉa tham số dựa trên Adapter (Rücklé et al., 2021; Hedegaard et al., 2022; Zhang et al., 2023; Wang et al., 2023) tập trung vào việc loại bỏ thích ứng các tham số có thể huấn luyện dư thừa trong đơn vị LoRA, và bỏ qua thích ứng các trạng thái ẩn của token không quan trọng (Lei et al., 2023), và khám phá cấu hình tối ưu của các phương pháp học tham số hiệu quả (Wang et al., 2022; Yu et al., 2023).

Cắt tỉa Lớp (Fan et al., 2020; Sajjad et al., 2023) đã đề xuất các chiến lược cắt tỉa có cấu trúc của các lớp Transformer bên trong PLM dựa trên Transformer để điều chỉnh các tác vụ hạ lưu. Các thí nghiệm thực nghiệm cho thấy rằng việc xóa các lớp Transformer theo cách xen kẽ và bảo tồn một vài lớp trên cùng và lớp dưới cùng có thể duy trì khả năng của các mô hình ngôn ngữ. Trong khi đó, do việc giảm kích thước mô hình, giai đoạn huấn luyện và giai đoạn suy luận có thể được tăng tốc đáng kể. Chúng tôi đối xứng loại bỏ các lớp Transformer trong bộ mã hóa và bộ giải mã, đối với PLM với n lớp của bộ mã hóa và n lớp của bộ giải mã: {l1, ..., ln}. Ký hiệu các lớp bị loại bỏ là li, trong đó i = 2k, k ∈ N, i ∈ [4, n-2]. Do đó, tổng cộng 2⌊(n-4)/2⌋ lớp bị loại bỏ.

4 Thí nghiệm

4.1 Bộ dữ liệu
Dựa trên MIMIC-IV-Note (Johnson et al., 2023a,c,b), một ghi chú lâm sàng dạng tự do đã được khử định danh có sẵn công khai với 331K báo cáo xuất viện và 2.3M báo cáo chụp X-quang tại Trung tâm Y tế Beth Israel Deaconess ở Boston, MA, Hoa Kỳ, chúng tôi tạo ra hai bộ dữ liệu tóm tắt báo cáo y tế với việc khớp mẫu chuỗi thông thường trên các tiêu đề phần phụ của báo cáo, có tên MIMIC-IV-discharge và MIMIC-IV-radiology.

Chúng tôi tiến hành các thí nghiệm mở rộng trên bốn bộ dữ liệu y tế, bao gồm MIMIC-IV-discharge, MIMIC-IV-radiology, HealthCareMagic (Zeng et al., 2020; Li et al., 2023) và iCliniq (Zeng et al., 2020; Li et al., 2023). Thống kê của bốn bộ dữ liệu được hiển thị trong Bảng 1.

Tóm tắt Chúng tôi sử dụng MIMIC-IV-discharge và MIMIC-IV-radiology cho việc tóm tắt báo cáo y tế. (i) MIMIC-IV-discharge: từ MIMIC-IV-Note, một báo cáo xuất viện sẽ bao gồm các ghi chú về nhập viện, tiền sử bệnh, chăm sóc sức khỏe, thủ tục, v.v. Bản tóm tắt xuất viện tương ứng chứa chẩn đoán tổng thể của bệnh nhân. (ii) MIMIC-IV-radiology: từ MIMIC-IV-Note, được viết với các template bán cấu trúc, một báo cáo chụp X-quang chứa mô tả đầy đủ về kết quả hình ảnh y tế, trong khi bản tóm tắt tương ứng bao gồm các điểm nổi bật dạng văn bản tự do của báo cáo.

Đối thoại Chúng tôi sử dụng HealthCareMagic và iCliniq cho việc sinh ra đối thoại y tế. (i) HealthCareMagic: một bộ dữ liệu đối thoại bệnh nhân-bác sĩ được thu thập từ một nền tảng tư vấn y tế trực tuyến. Chúng tôi đã sử dụng phiên bản "HealthCareMagic-200K" từ (Li et al., 2023). (ii) iCliniq: một bộ dữ liệu đối thoại bệnh nhân-bác sĩ được thu thập từ một nền tảng tư vấn y tế trực tuyến. Chúng tôi đã sử dụng phiên bản "iCliniq-15K" (Li et al., 2023).

4.2 Chi tiết triển khai
Chúng tôi thực hiện các thí nghiệm với BART-large và T5-large được tiền huấn luyện. Chúng tôi đặt siêu tham số rank r là 16 và factor α là 32 cho LoRA trong công việc này. Đối với r, các thí nghiệm bổ sung (Phụ lục A.1 Bảng 6) cho thấy rằng 16 sẽ là một lựa chọn cân bằng và có thẩm quyền. Trong Bảng 2, chúng tôi hiển thị các chỉ số bảo tồn cụ thể của các lớp bộ mã hóa và bộ giải mã cho BART-large và T5-large. Do đó, 33% và 41% các lớp Transformer bị loại bỏ đối với BART-large và T5-large. Tiếp theo, dựa trên các cài đặt siêu tham số thực nghiệm, chúng tôi sử dụng framework PyTorch với mixed-precision, optimizer AdamW (Loshchilov và Hutter, 2019) (β1 = 0.1, β2 = 0.999, ε = 10^-8) và các bước warm-up là 1000. Chúng tôi điều chỉnh mô hình gốc và mô hình L-Prun với tốc độ học 5×10^-5, và điều chỉnh mô hình với LoRA với tốc độ học 1×10^-4. Hơn nữa, độ dài nguồn tối đa là 1024 và 512 được đặt cho các bộ dữ liệu tóm tắt và đối thoại tương ứng. Tất cả độ dài mục tiêu tối đa đều là 128. Kích thước batch là 8 được áp dụng cho tất cả các thí nghiệm. Tuy nhiên, chúng tôi sử dụng cài đặt: (kích thước batch: 4; tích lũy gradient: 2) chỉ khi điều chỉnh T5-large trên MIMIC-IV-discharge do bộ nhớ GPU hạn chế, trong khi tích lũy gradient được vô hiệu hóa trong các trường hợp khác. Chúng tôi điều chỉnh các bộ dữ liệu với 10 epoch. Cứ mỗi 0.3 epoch, mô hình được đánh giá, và checkpoint với điểm ROUGE-1 cao nhất được tải cho tập kiểm tra. Chúng tôi sử dụng beam search trong quá trình giải mã tự hồi quy với độ rộng beam là 6. Tất cả các thí nghiệm được thực hiện trên một GPU NVIDIA A40 48GB duy nhất.

4.3 Chỉ số đánh giá
Các đầu ra được sinh ra bởi máy được đánh giá với các chỉ số sau. Lưu ý rằng các bản tóm tắt được sinh ra bởi máy được đánh giá với ROUGE, BERTScore và SummaC, trong khi các đối thoại được sinh ra bởi máy được đánh giá với ROUGE, BERTScore và BLEU.

ROUGE (Lin, 2004) là một chỉ số định hướng recall dựa trên N-gram phổ biến cho việc đánh giá tóm tắt tự động. R-1, R-2 và R-L đại diện cho mức độ chồng chéo của unigram, bigram và chuỗi con chung dài nhất giữa ứng viên và tham chiếu tương ứng.

BERTScore (Zhang et al., 2020) là một chỉ số đánh giá ngữ nghĩa ngữ cảnh dựa trên BERT được tiền huấn luyện, so sánh độ tương tự cosine giữa biểu diễn tiềm ẩn của token [CLS] của ứng viên với token của tham chiếu.

SummaC (Laban et al., 2022) là một chỉ số đánh giá tính nhất quán thực tế không tham chiếu nhẹ cho tóm tắt tự động. Sử dụng mô hình suy luận ngôn ngữ tự nhiên (NLI) được điều chỉnh, khung phát hiện sự không nhất quán ở cấp độ câu giữa tài liệu nguồn và bản tóm tắt được sinh ra bởi máy.

BLEU (Papineni et al., 2002) là một chỉ số đánh giá định hướng precision dựa trên N-gram phổ biến so sánh mức độ chồng chéo trong token giữa văn bản được sinh ra bởi máy và văn bản tham chiếu. BLEU-N biểu thị trung bình có trọng số của đánh giá khớp N-gram, N = {1, ..., N}, và chúng tôi báo cáo BLEU-1 và BLEU-4 cho các bộ dữ liệu đối thoại.

4.4 Baseline và tham số
Chúng tôi thực hiện phương pháp được đề xuất với BART-large và T5-large:

BART-large (Lewis et al., 2020): một mô hình bộ mã hóa-bộ giải mã dựa trên Transformer được tiền huấn luyện trên sự kết hợp của sách tiếng Anh, tin tức, truyện và đoạn văn Wikipedia với mô hình hóa ngôn ngữ có mặt nạ. BART-large (406M tham số) có 12 lớp bộ mã hóa, 12 lớp bộ giải mã, kích thước trạng thái ẩn là 1024 và kích thước từ vựng là 50K.

T5-large (Raffel et al., 2020): một mô hình bộ mã hóa-bộ giải mã dựa trên Transformer được tiền huấn luyện trên một corpus sạch lớn (C4) bằng tiếng Anh với mô hình hóa ngôn ngữ có mặt nạ, mô hình được tiền huấn luyện thực hiện nhiều tác vụ huấn luyện NLP thành một mô hình văn bản-sang-văn bản. T5-large (770M tham số) chứa 24 lớp bộ mã hóa, 24 lớp bộ giải mã, kích thước trạng thái ẩn là 1024 và kích thước từ vựng là 32K.

4.5 Kết quả thí nghiệm
LoRA QV hoặc LoRA FF Trong Bảng 3, chúng tôi hiển thị hiệu suất của việc chèn LoRA trong Query và Key so với FFN (tức là LoRA QV và LoRA FF). "Speed" đề cập đến tốc độ tương đối của giai đoạn huấn luyện, và việc điều chỉnh toàn bộ gốc được coi là baseline tốc độ. Vì một lớp bộ giải mã Transformer có hai khối attention và một FFN, LoRA QV chậm hơn khá nhiều so với LoRA FF trong giai đoạn huấn luyện. BART-large +LoRA FF+L-Prun là giải pháp thực tế tổng thể cho việc điều chỉnh nhanh, sử dụng bộ nhớ GPU thấp và chất lượng tóm tắt tốt. Do đó, các kết quả thí nghiệm sau đây tương ứng với LoRA FF với L-Prun. Lưu ý rằng các chỉ số Speed, R1, R2, R-L, BERTScore, Summac càng cao càng tốt.

Tiếp theo, chúng tôi trình bày kết quả thí nghiệm trong Bảng 4a (MIMIC-IV-discharge), Bảng 4b (MIMIC-IV-radiology), Bảng 5a (HealthCareMagic) và Bảng 5b (iCliniq).

Tóm tắt báo cáo y tế Trong Bảng 4a và Bảng 4b, các đánh giá giữa ROUGE và BERTScore cho thấy rằng việc áp dụng L-Prun đơn lẻ không làm giảm đáng kể hiệu suất của BART-large hoặc T5-large. Thứ hai, việc sử dụng LoRA và L-Prun chỉ tiêu thụ 68% và 75% bộ nhớ GPU so với việc điều chỉnh toàn bộ gốc tương ứng. Hơn nữa, kết quả của SummaC trở nên tốt hơn khi ROUGE và BERTScore tổng thể đang giảm, tuy nhiên điều này có thể là do chỉ số tính nhất quán thực tế được huấn luyện trên corpus chung không thể đáp ứng yêu cầu của lĩnh vực y tế. Thêm vào đó, việc tích hợp LoRA và L-Prun cho phép các mô hình huấn luyện 0,6% tham số so với mô hình gốc và giảm 33% hoặc 41% các lớp Transformer của BART-large hoặc T5-large, trong khi chỉ quan sát thấy khoảng 8% sự sụt giảm hiệu suất với điểm ROUGE và BERTScore so với việc điều chỉnh toàn bộ gốc.

Sinh ra đối thoại y tế Như được hiển thị trong Bảng 5a và Bảng 5b, các bộ dữ liệu sinh ra đối thoại thách thức hơn so với tóm tắt vì % novel N-gram của đầu ra tham chiếu cho các bộ dữ liệu đối thoại gần như 100% (Bảng 1), nhưng phương pháp được đề xuất chỉ mang lại sự suy giảm hiệu suất nhỏ (< 5%) trên R-1 và BLEU-1. Ngoài ra, tốc độ tổng thể và tiết kiệm bộ nhớ GPU nhất quán với tóm tắt, và tốc độ huấn luyện nhanh hơn đến +122% được hoàn thành trên HealthCareMagic.

Nhìn chung, các kết quả chính cho thấy rằng phương pháp được đề xuất là một cách tiếp cận tiết kiệm năng lượng đầy hứa hẹn. Lấy BART-large làm ví dụ, tổng số tham số được tiết kiệm cho bốn tác vụ Seq2Seq với các phương pháp khác nhau được hiển thị như sau:

• Điều chỉnh toàn bộ: 4 × 406.2M
• +LoRA FF: 4 × 4.0M + 406.2M
• +L-Prun: 4 × 288.7M
• +LoRA FF+L-Prun: 4 × 2.7M + 288.7M

5 Kết luận
Chúng tôi đề xuất một khung điều chỉnh tham số hiệu quả tích hợp LoRA và L-Prun. Dựa trên giả định PLM bị dư thừa tham số cho nhiều tác vụ NLP hạ lưu, phương pháp được đề xuất sử dụng các tham số có thể huấn luyện quy mô nhỏ để khai thác PLM, và giảm kích thước mô hình tổng thể theo cách cắt tỉa lớp có cấu trúc. Hơn nữa, chúng tôi đã tạo ra hai bộ dữ liệu tóm tắt báo cáo y tế từ MIMIC-IV-Note và xác thực phương pháp được đề xuất trên tóm tắt báo cáo y tế và sinh ra đối thoại y tế. Bằng cách điều chỉnh 0,6% tham số của PLM, loại bỏ hơn 30% các lớp Transformer trong PLM, PLM có thể duy trì hơn 92% chất lượng sinh ra trên các tác vụ Seq2Seq y tế và tăng gấp đôi tốc độ huấn luyện.

Hạn chế
(i) Nghiên cứu này trình bày việc tích hợp LoRA và L-Prun có cấu trúc, một khung L-Prun có cấu trúc tự thích ứng tự động hoặc cắt tỉa khối với việc điều chỉnh tham số hiệu quả dựa trên Adapter/LoRA sẽ xứng đáng cho nghiên cứu sâu hơn. (ii) Khung được đề xuất có thể trực giao với các cách tiếp cận điều chỉnh hiệu quả khác, chẳng hạn như Chưng cất Kiến thức và Lượng tử hóa. Khám phá một mô hình tích hợp toàn diện cho việc huấn luyện tiết kiệm năng lượng tốt hơn là quan trọng cho các nghiên cứu tương lai. (iii) Nghiên cứu này thực hiện khung được đề xuất trên các bộ dữ liệu văn bản-sang-văn bản y tế, nghiên cứu sâu hơn có thể tập trung vào việc điều chỉnh tham số hiệu quả của các tác vụ sinh ra đa phương thức.

Tuyên bố đạo đức
Đáng chú ý rằng mô hình ngôn ngữ không thể được dựa vào để sinh ra các đầu ra có tính chính xác thực tế, đáng tin cậy và dựa trên kiến thức cho lĩnh vực y tế. Việc truyền thông tin dựa trên kiến thức và thực tế thông qua kỹ thuật prompt và hiệu chuẩn có thể giảm bớt vấn đề này.
