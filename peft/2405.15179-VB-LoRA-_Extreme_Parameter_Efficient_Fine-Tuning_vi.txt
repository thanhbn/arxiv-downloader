# 2405.15179.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2405.15179.pdf
# Kích thước file: 885305 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
VB-LoRA: Tinh chỉnh Cực kỳ Hiệu quả Tham số
với Ngân hàng Vector
Yang Li
Khoa Khoa học Máy tính
Đại học Bang Georgia
Atlanta, GA 30303
yli93@student.gsu.eduShaobo Han
Mạng Quang học và Cảm biến
NEC Laboratories America
Princeton, NJ 08540
shaobo@nec-labs.comShihao Ji∗
Trường Điện toán
Đại học Connecticut
Storrs, CT 06269
shihao.ji@uconn.edu
Tóm tắt
Khi việc áp dụng các mô hình ngôn ngữ lớn tăng lên và nhu cầu tùy chỉnh mô hình theo người dùng hoặc tác vụ phát triển, các phương pháp tinh chỉnh hiệu quả tham số (PEFT), chẳng hạn như thích ứng thứ hạng thấp (LoRA) và các biến thể của nó, gây ra chi phí lưu trữ và truyền tải đáng kể. Để giảm thêm các tham số được lưu trữ, chúng tôi giới thiệu một mô hình "chia sẻ và phân chia" phá vỡ các rào cản của phân tích thứ hạng thấp qua các chiều ma trận, mô-đun và lớp bằng cách chia sẻ tham số toàn cục thông qua một ngân hàng vector. Như một sự thể hiện của mô hình này đối với LoRA, VB-LoRA được đề xuất của chúng tôi tạo thành tất cả các ma trận thứ hạng thấp của LoRA từ một ngân hàng vector được chia sẻ với một mô-đun hỗn hợp top-k có thể phân biệt. VB-LoRA đạt được hiệu quả tham số cực kỳ cao trong khi duy trì hiệu suất tương đương hoặc tốt hơn so với các phương pháp PEFT tiên tiến. Các thí nghiệm rộng rãi chứng minh hiệu quả của VB-LoRA trên các tác vụ hiểu ngôn ngữ tự nhiên, tạo ngôn ngữ tự nhiên, điều chỉnh hướng dẫn và lý luận toán học. Khi tinh chỉnh mô hình Llama2-13B, VB-LoRA chỉ sử dụng 0,4% tham số được lưu trữ của LoRA, nhưng đạt được kết quả vượt trội. Mã nguồn của chúng tôi có sẵn tại https://github.com/leo-yangli/VB-LoRA. Phương pháp này đã được hợp nhất vào gói PEFT của Hugging Face2.

1 Giới thiệu
104105106
# tham số được lưu trữ6566676869Tương quan Matthews
VB-LoRA (Của chúng tôi)
VeRA
Tied-LoRA
LoRA
Hình 1: So sánh các phương pháp PEFT trên RoBERTa-Large. VB-LoRA của chúng tôi đạt điểm số cao hơn với số lượng tham số được lưu trữ nhỏ hơn đáng kể.

Tinh chỉnh hiệu quả tham số (PEFT) tạo ra một mô hình mới tận dụng kiến thức tiên nghiệm mạnh mẽ được xây dựng trong các mô hình nền tảng và thích ứng chúng với một loạt các tác vụ hạ lưu bằng cách cập nhật một lượng nhỏ tham số có thể huấn luyện [He et al., 2021]. So với tinh chỉnh tiền tố/gợi ý [Li and Liang, 2021, Lester et al., 2021] hoặc học theo ngữ cảnh [Brown et al., 2020], tinh chỉnh một mô hình được tiền huấn luyện quy mô lớn mang lại chuyên môn hóa miền tốt hơn được quyết định bởi các bộ dữ liệu chất lượng cao [Brown et al., 2020, Liu et al., 2022, Zhao et al., 2023]. Quá trình này có thể được lặp lại để phù hợp với nhu cầu của các kịch bản triển khai luôn thay đổi và cá nhân hóa. Tuy nhiên, khối lượng tham số khổng lồ qua nhiều thể hiện [Sheng et al., 2023] đặt ra thách thức cho lưu trữ, truyền tải và tính toán, đặc biệt đối với phần cứng tài nguyên thấp và mạng lưới tiêu dùng [Borzunov et al., 2024].

Để giảm thiểu những thách thức này, nhiều phương pháp PEFT khác nhau đã được đề xuất bằng cách thêm hoặc thích ứng một lượng nhỏ tham số có thể huấn luyện cho mỗi tác vụ mà không hy sinh hiệu suất [Houlsby et al., 2019,

∗Một phần công việc được thực hiện khi tác giả liên kết với Đại học Bang Georgia.
2https://huggingface.co/docs/peft/en/package_reference/vblora

Hội nghị lần thứ 38 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2024).arXiv:2405.15179v3 [cs.CL] 29 Oct 2024

--- TRANG 2 ---
vector con #2
LogitsTopk-softmaxĐa Đầu ChăngQKVTruyền Tiến
OThêm & Chuẩn hóa
Thêm & Chuẩn hóaWdownWup
Ngân hàng VectorLogitscho vector con #1Top-K Softmax + WpretrainedLoRAAB
⨂⨂vector con #1⊕ChọnLát
🔥
🔥
❄...
💾
💾vector con #8
💾Chọn và gộp
🔥Tham số có thể huấn luyện
❄Tham số đông lạnh
💾Tham số được lưu trữMô-đunQKVOWdownWupNgân hàng VectorCácLớpLớp 1Lớp 2...GộpVector ConAB

Hình 2: Trái: Các tham số mô hình có thể được biểu diễn như một tổ hợp các vector từ một ngân hàng vector, được chia sẻ qua các vector con, mô-đun và lớp. Phải: Kiến trúc của VB-LoRA. Chúng tôi sử dụng một hàm softmax top-k để chọn k vector từ ngân hàng vector. Các vector được chọn sau đó được gộp thành một vector con, được sắp xếp tại vị trí mong muốn, tạo thành các tham số của LoRA.

Karimi Mahabadi et al., 2021, Ding et al., 2023]. Các phương pháp này khai thác sự phụ thuộc giữa các tham số mô hình để giảm sự dư thừa. Ví dụ, Hu et al. [2021] đề xuất thích ứng thứ hạng thấp (LoRA) để xấp xỉ cập nhật gradient tích lũy cho các mô-đun tự chú ý, và tạo ra sự liên kết tham số nội ma trận. Renduchintala et al. [2024] tiếp tục nghiên cứu các tùy chọn cho phép chia sẻ tham số liên ma trận thông qua liên kết trọng số qua tất cả các lớp. Trong cả hai trường hợp, số lượng tham số có thể huấn luyện được giảm đáng kể. Hai phương pháp này đứng ở hai cực của phổ trong việc quyết định phạm vi tái sử dụng thành phần mô hình (cục bộ hoặc qua các lớp) và chỉ định ma trận thứ hạng thấp nào cần được chia sẻ và cập nhật. Tuy nhiên, khi kích thước mô hình tăng và nhu cầu về các mô hình tùy chỉnh người dùng qua các dịch vụ khác nhau tăng lên, chi phí lưu trữ và truyền tải các tùy chỉnh cho mỗi tổ hợp tăng vọt và xuất hiện như một vấn đề quan trọng. Do đó, việc nghiên cứu các phương pháp PEFT với số lượng tham số có thể huấn luyện nhỏ hơn đáng kể đã thu hút rất nhiều quan tâm nghiên cứu [Kopiczko et al., 2024, Renduchintala et al., 2024].

Bài báo này giới thiệu VB-LoRA, tinh chỉnh cực kỳ hiệu quả tham số với ngân hàng vector dựa trên một mô hình "chia sẻ và phân chia" đơn giản nhưng hiệu quả. Chúng tôi đẩy giới hạn hiệu quả tham số LoRA bằng cách phá vỡ hai rào cản của phân tích thứ hạng thấp: (1) cục bộ trong mỗi mô-đun và mỗi lớp, và (2) chỉ qua hai chiều ma trận gốc (không chia tách; xem Phần 3.2 để biết chi tiết). Chúng tôi lập luận rằng các tham số qua các mô-đun và lớp khác nhau có thể được chia sẻ, và do đó sự dư thừa trong tham số có thể được giảm thêm. Ngoài ra, bằng cách phân chia các vector thành phần hạng một thành các vector con, chúng tôi giới thiệu các chiều "ảo" sao cho cấu trúc sâu trong không gian tham số có thể được biểu diễn bằng một phân tích ma trận nén cao.

VB-LoRA lấy cảm hứng từ dòng công việc trước đây về mạng tensor lượng tử hóa [Oseledets, 2010, Cichocki, 2014] trong việc phá vỡ ràng buộc của chiều vật lý để nén tham số cực kỳ. Cụ thể, VB-LoRA tái tham số hóa thích ứng thứ hạng thấp của LoRA bằng một phân tích hạng một và sau đó chia các vector kết quả thành các vector con cùng kích thước. Một cơ chế chia sẻ toàn cục sau đó được học dựa trên một mô-đun hỗn hợp top-k thưa thớt. Các vector con cùng kích thước cho phép tham số được chia sẻ qua các mô-đun và lớp ở mức vector con. Hơn nữa, so với các phương pháp nén ma trận hậu hoc [Oseledets, 2010, Khoromskij, 2011], VB-LoRA có thể phân biệt đầu cuối, và do đó quá trình tinh chỉnh nhận thức được dạng nén, cho phép nén hướng tác vụ. Hình 1 minh họa hiệu quả tham số của VB-LoRA so với các phương pháp PEFT tiên tiến. Đóng góp của chúng tôi được tóm tắt như sau:

1. Chúng tôi giới thiệu một mô hình "chia sẻ và phân chia" phá vỡ các rào cản của phân tích thứ hạng thấp qua các chiều ma trận, mô-đun và lớp bằng cách chia sẻ tham số toàn cục thông qua một ngân hàng vector.

2. Chúng tôi tái tham số hóa phân tích thứ hạng thấp của LoRA bằng một phân tích hạng một, và chia các vector kết quả thêm thành các vector con cùng kích thước, cho phép hiệu quả tham số cực kỳ ở mức vector con.

--- TRANG 3 ---
3. Chúng tôi đề xuất một mô-đun top-k thưa thớt dựa trên mô hình hỗn hợp để học một cơ chế chia sẻ toàn cục, làm cho khung của chúng tôi có thể phân biệt đầu cuối và nhận thức nén.

4. Phương pháp của chúng tôi đạt được hiệu quả tham số cực kỳ trong khi duy trì hiệu suất thực nghiệm tương đương hoặc tốt hơn so với các phương pháp PEFT tiên tiến trên các tác vụ hiểu ngôn ngữ tự nhiên, tạo ngôn ngữ tự nhiên, điều chỉnh hướng dẫn và lý luận toán học.

2 Công việc Liên quan

Khai thác Sự dư thừa Toàn cục để Tăng cường Hiệu quả Tham số Các tham số của mạng nơ-ron sâu (DNN) có thể được chia tự nhiên theo lớp, đầu hoặc loại (MHA hoặc FFN). Trong khi LoRA [Hu et al., 2021] chỉ khai thác sự phụ thuộc nội ma trận, Tied-LoRA [Renduchintala et al., 2024] sử dụng một sơ đồ liên kết trọng số đơn giản trên các ma trận thứ hạng thấp A và B qua các lớp để giảm sự dư thừa liên ma trận. Khi A và B được khởi tạo ngẫu nhiên, đông lạnh và chia sẻ qua tất cả các lớp, Tied-LoRA thoái hóa thành VeRA [Kopiczko et al., 2024], chỉ yêu cầu hai vector tỷ lệ được cập nhật, dẫn đến hiệu quả tham số ấn tượng. Một công việc đồng thời, LoRA-XS [Bałazy et al., 2024], tiếp tục cải thiện hiệu quả tham số của LoRA bằng cách giới thiệu các ma trận nhỏ có thể huấn luyện giữa các ma trận chiếu LoRA đông lạnh, được khởi tạo bằng Phân tích Giá trị Đơn lẻ (SVD) của trọng số mô-đun được tiền huấn luyện. VB-LoRA của chúng tôi đẩy giới hạn hiệu quả tham số LoRA bằng cách chia sẻ tham số toàn cục qua các mô-đun và lớp ở mức vector con.

Về tái tham số hóa chiều thấp, Aghajanyan et al. [2020] chứng minh thực nghiệm rằng tồn tại một tái tham số hóa chiều thấp hiệu quả như không gian tham số đầy đủ để tinh chỉnh. Việc hiện thực hóa phép chiếu ngẫu nhiên được đạt thông qua biến đổi Fastfood [Le et al., 2013] cho các mô hình ngôn ngữ được tiền huấn luyện quy mô lớn. Để làm cho nó nhận thức cấu trúc, một tập các tham số tỷ lệ theo lớp được bao gồm như một phần của các tham số huấn luyện. Theo trực giác này, chúng tôi nghiên cứu tinh chỉnh nhẹ trong LoRA dựa trên tái tham số hóa tùy chỉnh phát sinh từ phân tích ma trận hạng một.

Hơn nữa, phân tích tensor đã được tận dụng cho PEFT trong các mô hình ViT [Jie and Deng, 2023] dựa trên các định dạng cổ điển, chẳng hạn như tensor-train hoặc Tucker [Kolda and Bader, 2009]. Chúng tôi thấy rằng việc ép buộc phân tích đa tuyến tính qua nhiều chế độ dẫn đến số hạng cao hơn, có hại cho mục tiêu nén tham số. Một so sánh gián tiếp của VB-LoRA với Jie and Deng [2023] có thể được thực hiện bằng cách tham chiếu tỷ lệ nén với LoRA. Từ góc độ này, VB-LoRA của chúng tôi có thể được xem như một định dạng tensor tùy chỉnh được trang bị cấu trúc hình học lồi, được kích hoạt bởi mô hình hỗn hợp top-k thưa thớt mà chúng tôi đề xuất.

So với phương pháp fusion sâu [Mazzawi et al., 2024] nơi các tham số LLM được phân tách và khởi tạo bằng các mạng nhỏ hơn được tiền huấn luyện dưới một cơ chế tăng trưởng mạng được thiết kế, việc phân chia tham số của chúng tôi hoạt động trên các vector thành phần hạng một. Phân chia vector con cho phép các mở rộng tương tự để tận dụng khởi tạo ngân hàng vector được tiền huấn luyện từ các mô hình nhỏ hơn và huấn luyện phân tán sử dụng song song mô hình.

Mô hình Tham số dựa trên Mô hình Hỗn hợp Thưa thớt Các mô hình hỗn hợp đã được sử dụng rộng rãi trong di truyền học quần thể [Pritchard et al., 2000], mô hình chủ đề [Reisinger et al., 2010, Inouye et al., 2014], và tách phổ siêu [Li and Bioucas-Dias, 2008, Fu et al., 2015] để trích xuất các thành phần nguyên mẫu (hoặc thành viên cuối) từ dữ liệu quan sát. Các thành phần nguyên mẫu có thể được nới lỏng để có dấu hỗn hợp [Ding et al., 2008] với đảm bảo nhận dạng [Lin et al., 2015]. Theo cách thông thường, ước lượng tham số được thực hiện dựa trên lập trình tuyến tính [Chan et al., 2009] hoặc thuật toán tổ hợp [Arora et al., 2013]. Tuy nhiên, một bài toán lập trình nguyên phức tạp phát sinh khi kết hợp một ràng buộc top-k bổ sung vào trọng số hỗn hợp đặc biệt thách thức đối với các mô hình ngôn ngữ quy mô lớn. Trong công việc này, chúng tôi đề xuất học các ngân hàng vector nguyên mẫu không từ dữ liệu quan sát mà từ các tham số mô hình của LLM. Bằng cách sửa đổi mô-đun top-k thưa thớt [Shazeer et al., 2016] thường được sử dụng trong các mô hình Mixture-of-Expert [Jiang et al., 2024], trọng số hỗn hợp và ngân hàng vector được tối ưu hóa bằng lan truyền ngược dưới mục tiêu của các tác vụ tinh chỉnh hạ lưu. Mô hình hỗn hợp top-k được đề xuất là bất khả tri mô hình theo nghĩa nó có thể được tích hợp sẵn vào bất kỳ tham số mạng nơ-ron hoặc cập nhật gradient tích lũy nào.

--- TRANG 4 ---
3 Phương pháp Đề xuất

3.1 Kiến thức Cơ bản: Kiến trúc Transformer và Bộ Chuyển đổi LoRA

Kiến trúc transformer [Vaswani et al., 2017] bao gồm L lớp, mỗi lớp chứa hai loại khối: Multi-Head Attention (MHA) và Feed-Forward Network (FFN). Chúng tôi ký hiệu các ma trận query, key, value và output của MHA tại lớp ℓ là Wℓt={Wit}Nhi=1,t∈{q,k,v,o}, trong đó Wit∈Rd×d, và Nh là số đầu. Cho FFN(x)=WdownReLU(Wupx) với x∈Rd, xem FFN như một hoạt động đa đầu, chúng tôi tiếp tục chia Wup∈Rcd×d và Wdown∈Rd×cd thành c ma trận có kích thước d×d, ký hiệu bởi Wℓup={Wℓ,iup}ci=1 và Wℓdown={Wℓ,idown}ci=1. c=4.

Cho một ma trận được tiền huấn luyện W0∈Rm×n, LoRA [Hu et al., 2021] ràng buộc các gia tăng trọng số ΔW như một phân tích thứ hạng thấp ΔW=BA, trong đó B∈Rm×r, A∈Rr×n là các tham số có thể huấn luyện, với r≪min(m,n). VeRA [Kopiczko et al., 2024] tiếp tục giới hạn các tham số có thể huấn luyện thành hai vector tỷ lệ b và d, tạo thành các phần tử đường chéo của hai ma trận đường chéo Λb và Λd. Do đó, VeRA có thể được biểu diễn như ΔW=ΛbBΛdA, trong đó B và A được khởi tạo ngẫu nhiên, đông lạnh và chia sẻ qua các lớp.

Tổng thể, chúng tôi ký hiệu các tham số mô hình của transformer như Ω={{Wℓq,Wℓk,Wℓv,Wℓo}∪{Wℓup,Wℓdown}}Lℓ=1∈R12L×d×d. Trong phần tiếp theo, chúng tôi đề xuất một tái tham số hóa toàn cục trên các gia tăng trọng số của W∈Ω dựa trên phân tích LoRA ΔW=BA. chúng tôi sẽ chỉ ra cách hiệu quả tham số cực kỳ có thể đạt được bằng (1) chia sẻ tham số qua các chiều ma trận của A và B dựa trên phân tích hạng một và phân vùng vector con (Phần 3.2), và (2) qua các mô-đun và lớp bất kể chỉ số hoặc loại ma trận (Phần 3.3).

3.2 Chia sẻ và Phân chia: một Mô hình Mới cho Chia sẻ Tham số

Phân tích thứ hạng thấp của LoRA có thể được biểu diễn tương đương dưới dạng hạng một như sau:

ΔW=BA=∑rk=1bk⊗ak=∑rk=1⊗2i=1v(i)k,v(1)k=bk,v(2)k=ak, (3.1)

trong đó ⊗ ký hiệu toán tử tích ngoài và v(i)k là một vector có kích thước di.

Phân chia Dựa trên phân tích hạng một ở trên, chúng tôi tiếp tục biểu diễn mỗi vector thành phần v(i)k như một nối của một tập các vector con,

v(i)k=concat(u(i)k,1,u(i)k,2,...,u(i)k,d′i),u(i)k,j∈Rb, j∈{1,...,d′i}, (3.2)

trong đó {di}i=1,2 biểu diễn kích thước của chiều ma trận của ΔW. Nói chung, {di}i=1,2 không bằng nhau qua A và B, và chúng tôi chọn b như một thừa số chung của di sao cho d′i=di/b và d′i∈Z.

Chia sẻ Để tạo điều kiện chia sẻ tham số qua các chiều mô hình, chúng tôi giả định mỗi vector con u(i)k,j như một hỗn hợp top-k của các phần tử cơ bản từ ngân hàng vector B={α1,...,αh}, trong đó αi∈Rb với i∈{1,...,h}, và được định nghĩa như sau (với các chỉ số phụ được bỏ qua để rõ ràng):

u=∑hs=1ws(σ)αs,w(σ)=Softmax(TopK(σ,k)), (3.3)

trong đó TopK(σ,k)i=σi nếu σi nằm trong top-k của σ và TopK(σ,k)i=−∞ ngược lại. Đối với mỗi vector con u, chúng tôi giới thiệu logits σ∈Rh như các tham số có thể học của nó. Chúng tôi gọi mô hình được biểu diễn trong Eq. 3.3 là mô-đun hỗn hợp top-k (TKAM), có thể phân biệt. Thiết kế này cho phép học chung ngân hàng vector B và logits σ theo cách đầu cuối, phù hợp với tinh chỉnh mô hình cho các tác vụ hạ lưu.

Mô-đun TKAM thúc đẩy tính thưa thớt bằng cách chọn k vector của các logits lớn nhất từ ngân hàng vector. Bằng cách đặt k≪h, chúng tôi hạn chế vector con u phải thưa thớt. Nghĩa là, trong mỗi lần lặp, các cập nhật cho ngân hàng vector vẫn bị chi phối cục bộ – với nhiều nhất k vector cơ sở α∈B bị ảnh hưởng bởi lan truyền ngược qua u– với hy vọng rằng các vector đã học có thể chuyên biệt hơn và kiến thức được đóng gói trong ngân hàng vector có thể được kích hoạt và cập nhật một cách thưa thớt.

--- TRANG 5 ---
Mô-đun Top-k Không Nhiễu Mô-đun Noisy Top-k Gating [Shazeer et al., 2016] đã được sử dụng rộng rãi để thay thế các lớp kết nối đầy đủ bằng các lớp Mixture of Experts (MoE) trong các mô hình ngôn ngữ lớn [Jiang et al., 2024]. Ngược lại, chúng tôi sử dụng Eq. 3.3 để học sơ đồ chia sẻ chọn lọc qua các vector thành phần hạng một mà không thay đổi mô hình gốc. Do phân tích, chúng tôi thấy rằng các cập nhật tham số gradient tích lũy nhạy cảm hơn so với các tham số mô hình gốc trong quá trình huấn luyện. Điều này có thể liên quan đến các vấn đề bất ổn định huấn luyện được quan sát trong hypernetworks [Ortiz et al., 2024], nơi các tham số được tạo ra bởi một mô hình tham số hóa khác. Do đó, giữ nhiễu bằng không trong hàm gating có thể giúp việc học hiệu quả và ổn định hơn. Một nghiên cứu loại bỏ về các phương pháp chọn vector khác nhau, bao gồm Gumbel-softmax, được cung cấp trong Phần 4.5.

3.3 Phá vỡ Ranh giới của LoRA cho Chia sẻ Tham số Toàn cục

Trong khi LoRA chỉ áp dụng phân tích thứ hạng thấp cho mỗi gia tăng trọng số riêng lẻ, ranh giới có thể được phá vỡ bởi sơ đồ chia sẻ và phân chia mà chúng tôi đề xuất trong Phần 3.2. Phương pháp chia sẻ và phân chia của chúng tôi có thể được diễn giải như phân tích tensor phân cấp và có ràng buộc, tạo điều kiện chia sẻ tham số toàn cục hiệu quả vượt ra ngoài biểu diễn thứ hạng thấp của ma trận LoRA.

Toán tử chia lần đầu được giới thiệu trong Quantized Tensor Train (QTT) để nén siêu của các ma trận quy mô lớn [Oseledets, 2010, Cichocki, 2014]. Ví dụ, phân chia dyadic định hình lại một vector có độ dài L=2p thành một mảng p chiều tạo điều kiện cho phân tích Tensor Train hiệu quả được sử dụng. Toán tử chia của chúng tôi thay vào đó áp dụng cho các vector thành phần hạng một v(i)k, và biểu diễn tensorial phân cấp kết quả của ΔW có thể được xem như Canonical Polyadic Decomposition (CPD) [Kolda and Bader, 2009] với các vector thành phần v(i)k được gấp thành các mảng 2 chiều với các vector con u(i)k,j như các cột. Mỗi vector con ui được tạo thành từ một ngân hàng vector toàn cục B chia sẻ thông qua TKAM, trong đó i=[j,v] là một chỉ số đa bao gồm các chỉ số vật lý j, chẳng hạn như mô-đun, lớp, đầu và ma trận phân tích trái/phải, và các chỉ số ảo v (được tạo từ phân vùng vector).

Toán tử chia sẻ (mô-đun TKAM) có thể được xem như một mô hình nhân tố với các ràng buộc simplex trên trọng số hỗn hợp (ví dụ, k=2, vector con u nằm trên các cạnh của simplex) và các nhân tố chung được lưu trữ trong B. Gọi u∈Rb và u=∑hs=1αsws, trong đó αs là nhân tố thứ s, và w là điểm nhân tố cho vector con u. Chúng tôi xem xét các tùy chọn sau cho w: (1) Admixture (tổ hợp lồi): w∈[0,1]h và ∑hs=1ws=1, thường được sử dụng trong nhiều cộng đồng khác nhau. (2) Sparse Admixture (TKAM): w∈[0,1]h và ∑hs=1ws=1 với chỉ k≪h phần tử khác không được phép. Đáng chú ý rằng việc thêm thông tin chỉ số đa vào cơ chế chọn vector có thể làm cho mô hình TKAM nhận thức cấu trúc, có khả năng mang lại lợi ích bổ sung. Một khả năng là làm cho logits của chọn vector có điều kiện trên các embedding của lớp, mô-đun và loại ma trận, có thể được thực hiện thông qua một hypernetwork [Mahabadi et al., 2021]. Tuy nhiên, chúng tôi để lại điều này cho công việc tương lai.

Tóm lại, LoRA cung cấp một phân tích thứ hạng thấp cục bộ cho mỗi ma trận d1×d2 ΔW độc lập. Ngược lại, VB-LoRA của chúng tôi giới thiệu một phân tích thứ hạng thấp toàn cục trên một ma trận b×|{i}| được tạo thành từ các vector hạng một được phân vùng, trong đó |{i}| ký hiệu số lượng của tập chỉ số bao gồm cả chỉ số vật lý và ảo. Như chúng ta sẽ thấy bên dưới, sự khác biệt này có thể tận dụng tốt hơn sự dư thừa trong các gradient tích lũy, dẫn đến hiệu quả tham số cực kỳ.

Hình 2 tổng quan phương pháp của chúng tôi. Phần bên trái chứng minh ý tưởng cấp cao của VB-LoRA: ngân hàng vector được chia sẻ qua các vector con, mô-đun và lớp. Phần bên phải chi tiết kiến trúc của nó. Để tạo thành mỗi vector con, chúng tôi sử dụng một hàm softmax top-k để chọn k vector từ ngân hàng vector, sau đó được gộp thành một vector con. Các vector con này được sắp xếp ở các vị trí mong muốn, tạo thành các tham số cho LoRA với chi phí tính toán không đáng kể. Thuật toán 1 cung cấp mã giả giống PyTorch cho VB-LoRA, có thể được tích hợp liền mạch vào khung PyTorch.

3.4 Đếm Tham số

Trong tinh chỉnh đầy đủ, số lượng tham số có thể huấn luyện bằng kích thước mô hình, tức là LMd2, trong đó L là số lớp, M là số mô-đun được tinh chỉnh, và d là chiều ẩn.

--- TRANG 6 ---
Thuật toán 1 Mã giả của VB-LoRA theo phong cách giống PyTorch
# d: chiều ẩn; b: độ dài của vector con; r: hạng; h: kích thước ngân hàng vector
# k: số vector được chọn sử dụng trong mô-đun hỗn hợp top-k  
# logits: Mỗi lớp tuyến tính có hai tham số có thể huấn luyện: logits_A và logits_B.
# Cả hai tham số đều có hình dạng (d/b)*r*h.
# vector_bank: Ngân hàng vector được chia sẻ với hình dạng h*b.
# x và W: đầu vào và trọng số gốc.

def get_low_rank_matrix(logits, vector_bank, k):
    topk_logits, topk_indices = logits.topk(k, dim=-1)
    topk_weights = torch.softmax(topk_logits, dim=-1)
    matrix = (topk_weights * vector_bank[topk_indices]).sum(-2)
    return matrix

def VBLoRA_forward(x, vector_bank, logits_A, logits_B, k):
    r = logits_A.shape[1]
    A = get_low_rank_matrix(logits_A, vector_bank, k).transpose(0, 1).reshape(r, -1)
    B = get_low_rank_matrix(logits_B, vector_bank, k).transpose(1, 2).reshape(-1, r)
    # Để tiết kiệm bộ nhớ, chúng tôi tránh tính toán rõ ràng \delta W = B @ A.
    return x @ W + (x @ B) @ A

LoRA giảm số này xuống 2LMdr, trong khi VeRA tiếp tục giảm xuống LM(d+r). Các tham số có thể huấn luyện của LoRA và VeRA giống với các tham số chúng cần lưu trữ.

Trong VB-LoRA, các tham số có thể huấn luyện bao gồm hai phần: các tham số của ngân hàng vector B và các tham số của logits σ. Tuy nhiên, khi kết thúc huấn luyện, các tham số logit có thể được bỏ qua và chỉ cần lưu trữ k chỉ số được chọn và trọng số hỗn hợp top-k. Do đó, các tham số được lưu trữ có thể được biểu diễn bởi một bộ ba Θ={B,I,V}, trong đó B∈Rh×b là một ngân hàng vector chứa h vector b chiều, I∈R2×L×M×r×(d/b)×k là các chỉ số top-k của các vector trong B cho tất cả vector con, và V∈R2×L×M×r×(d/b)×(k−1) là trọng số hỗn hợp top-k được sử dụng để tạo thành các vector con từ ngân hàng. Đáng chú ý rằng trọng số hỗn hợp top-k chỉ có k−1 bậc tự do vì chúng phải có tổng bằng 1. Ngoài ra, tùy thuộc vào kích thước ngân hàng vector h, các chỉ số I có thể được lưu trữ hiệu quả như số nguyên không dấu (ví dụ, uint8 khi h≤256), và do đó, chúng tôi đếm số tham số như kích thước tương đương float32 để so sánh công bằng. Khi chúng tôi sử dụng k=2 và uint8 cho chỉ số, số tham số được lưu trữ của VB-LoRA là hb+3LMr(d/b). Không như LoRA và VeRA, số tham số trong VB-LoRA không tăng tuyến tính với kích thước mô hình (được quyết định bởi L và d) hoặc số mô-đun được tinh chỉnh, tức là M. Trong khi số hạng thứ hai của tham số VB-LoRA là một hàm tuyến tính của LMd, hệ số là 3r/b, thường rất nhỏ. Ví dụ, trong các thí nghiệm của chúng tôi, các giá trị điển hình là r=4 và b=256, dẫn đến hệ số 0.04, trong khi hệ số là 2r cho LoRA và 1 cho VeRA. Hầu hết các tham số trong VB-LoRA nằm trong ngân hàng vector được chia sẻ, có kích thước không tăng tuyến tính với kích thước mô hình hoặc số mô-đun được tinh chỉnh.

4 Thí nghiệm

Trong phần này, chúng tôi tiến hành đánh giá toàn diện phương pháp của chúng tôi thông qua một loạt thí nghiệm. Chúng tôi bắt đầu bằng việc so sánh VB-LoRA với các phương pháp PEFT tiên tiến: LoRA, VeRA và Tied-LoRA trên benchmark GLUE. Tiếp theo, chúng tôi mở rộng phân tích của chúng tôi sang các tác vụ tạo ngôn ngữ tự nhiên sử dụng GPT-2, các tác vụ điều chỉnh hướng dẫn trên Llama2, cũng như các tác vụ lý luận toán học trên các mô hình Mistral và Gemma. Tất cả các thí nghiệm của chúng tôi được thực hiện trên một máy chủ được trang bị 8 GPU NVIDIA A100. Để tái tạo, chúng tôi cung cấp các siêu tham số chi tiết và thông số kỹ thuật của tài nguyên tính toán cho mỗi thí nghiệm trong phụ lục. Mã nguồn có sẵn tại https://github.com/leo-yangli/VB-LoRA.

4.1 Hiểu Ngôn ngữ Tự nhiên

Chúng tôi áp dụng benchmark General Language Understanding Evaluation (GLUE)3 [Wang et al., 2018] để đánh giá hiệu suất của VB-LoRA qua nhiều tác vụ hiểu ngôn ngữ tự nhiên, bao gồm

3https://gluebenchmark.com/

--- TRANG 7 ---
Bảng 1: Kết quả với RoBERTa base và RoBERTa large trên benchmark GLUE. Kết quả tốt nhất trong mỗi nhóm được hiển thị bằng chữ đậm. Chúng tôi báo cáo tương quan Matthew cho CoLA, tương quan Pearson cho STS-B, và độ chính xác cho tất cả các tập dữ liệu khác. Kết quả cho LoRA qv và VeRA qv được lấy từ các bài báo gốc tương ứng, trong khi các kết quả khác dựa trên việc thực hiện của chúng tôi. Chúng tôi báo cáo hiệu suất trung vị từ 5 lần chạy sử dụng các hạt giống ngẫu nhiên khác nhau.

[Bảng chi tiết với các số liệu hiệu suất cho các phương pháp khác nhau trên các tác vụ GLUE]

các tác vụ tương tự, paraphrase và suy luận. Theo Kopiczko et al. [2024], chúng tôi tập trung vào sáu tác vụ từ GLUE: CoLA [Warstadt et al., 2019] (khả năng chấp nhận ngôn ngữ), SST-2 [Socher et al., 2013] (phân tích cảm xúc), MRPC [Dolan and Brockett, 2005] (phát hiện paraphrase), STS-B [Cer et al., 2017] (tương tự văn bản ngữ nghĩa), QNLI [Rajpurkar et al., 2018] (suy luận), và RTE (suy luận). Các thí nghiệm của chúng tôi được thực hiện với RoBERTa base và RoBERTa large [Liu et al., 2019]. Trong khi LoRA và VeRA chỉ tinh chỉnh các mô-đun query và value, chúng tôi khám phá hai chiến lược tinh chỉnh: chỉ query và value (VB-LoRA qv), và tất cả các mô-đun tuyến tính (VB-LoRA all), bao gồm Wq, Wk, Wv, Wo, Wup, và Wdown. Chúng tôi tạo một ngân hàng vector của 90 vector có độ dài 256, được khởi tạo với phân phối đồng đều U(−0.02,0.02). Các logits được khởi tạo với phân phối chuẩn N(0,0.01). Tỷ lệ học cho các tham số ngân hàng vector và logit được đặt lần lượt là 0.001 và 0.01. Chúng tôi đặt hạng là 4 và k=2 cho tất cả các thí nghiệm.

Bảng 1 tiết lộ rằng VB-LoRA đạt được hiệu suất cạnh tranh hoặc vượt trội so với VeRA và Tied-LoRA, trong khi hiệu quả tham số hơn. Ví dụ, khi tinh chỉnh các mô-đun query và value trên mô hình RoBERTa large, phương pháp của chúng tôi giảm các tham số được lưu trữ xuống dưới 40% so với VeRA hoặc Tied-LoRA, trong khi vượt trội chúng qua tất cả các tác vụ. Những kết quả này cho thấy rằng hiệu suất mô hình không chỉ phụ thuộc vào số lượng tham số có thể huấn luyện mà còn vào cách chúng được tạo thành.

Hơn nữa, các kết quả nhất quán chỉ ra rằng tinh chỉnh tất cả các mô-đun, ngoài chỉ các mô-đun query và value, tăng cường hiệu suất cho tất cả các phương pháp. Tuy nhiên, LoRA, VeRA và Tied-LoRA yêu cầu 2-4 lần tham số trong trường hợp này vì số tham số của chúng tăng tuyến tính với số mô-đun được tinh chỉnh. Ngược lại, phương pháp của chúng tôi chỉ sử dụng thêm 37.5% tham số vì chúng tôi duy trì cùng kích thước ngân hàng vector nhưng thêm các tham số bổ sung cho chỉ số và trọng số top-k. Do đó, với chỉ 12.8% tham số so với VeRA all (4% so với LoRA qv), phương pháp của chúng tôi đạt được hiệu suất trung bình tốt nhất.

4.2 Tạo Ngôn ngữ Tự nhiên

Cho các thí nghiệm tạo ngôn ngữ tự nhiên, chúng tôi tinh chỉnh các mô hình GPT-2 Medium và Large [Radford et al., 2019] trên tập dữ liệu E2E4 [Novikova et al., 2017], chứa khoảng 42,000 ví dụ huấn luyện, 4,600 ví dụ xác thực và 4,600 ví dụ kiểm tra từ lĩnh vực nhà hàng. Chúng tôi sử dụng một ngân hàng vector có kích thước 256 cho GPT-2 Medium và 350 cho GPT-2 Large. Độ dài vector được đặt là 256 và hạng được đặt là 4 cho cả hai mô hình. Để đạt được hiệu suất tốt nhất, chúng tôi tinh chỉnh tất cả các lớp attention và lớp FFN. Như thể hiện trong Bảng 2, phương pháp của chúng tôi đạt được hiệu suất cạnh tranh so với VeRA, trong khi yêu cầu khoảng 20% ít tham số được lưu trữ hơn cho cả hai mô hình.

4Licensed under CC BY-SA 4.0. URL: https://github.com/tuetschek/e2e-dataset

--- TRANG 8 ---
Bảng 2: Kết quả với GPT-2 Medium và GPT-2 Large trên benchmark E2E. Kết quả cho FT và LoRA được lấy từ Hu et al. [2021], và kết quả cho VeRA được lấy từ Kopiczko et al. [2024]. Chúng tôi báo cáo trung bình của 3 lần chạy sử dụng các hạt giống ngẫu nhiên khác nhau.

[Bảng với kết quả hiệu suất cho các phương pháp khác nhau]

4.3 Điều chỉnh Hướng dẫn

Điều chỉnh hướng dẫn là một quá trình tinh chỉnh mô hình với một tập các hướng dẫn hoặc gợi ý để tăng cường hiệu suất của nó trên các hướng dẫn cụ thể [Ouyang et al., 2022]. Đầu tiên chúng tôi thí nghiệm trên một tập dữ liệu điều chỉnh hướng dẫn tổng quát. Chúng tôi sử dụng Cleaned Alpaca Dataset5, cải thiện chất lượng dữ liệu của tập dữ liệu Alpaca gốc [Taori et al., 2023]. Chúng tôi đánh giá các mô hình được tinh chỉnh trên MT-Bench6 [Zheng et al., 2024], chứa 80 câu hỏi đa lượt.

Theo Kopiczko et al. [2024], chúng tôi tinh chỉnh mô hình Llama2 [Touvron et al., 2023] trong khung QLoRA7 [Dettmers et al., 2023], nhằm giảm sử dụng bộ nhớ khi tinh chỉnh các mô hình ngôn ngữ lớn trên một GPU duy nhất. Chúng tôi sử dụng chiến lược lượng tử hóa được cung cấp bởi QLoRA, bao gồm 4-bit NormalFloat cho dữ liệu lưu trữ, BFloat16 cho tham số tính toán, lượng tử hóa kép và bộ tối ưu hóa phân trang để huấn luyện nó trên một GPU duy nhất. Các mô hình được tinh chỉnh của chúng tôi tạo ra các phản hồi cho những câu hỏi này, và sau đó, GPT-4 được sử dụng để xem xét và đánh giá các câu trả lời được tạo, gán một điểm số định lượng trên thang điểm 10. Lưu ý rằng phù hợp với VeRA, chúng tôi báo cáo điểm số của lượt đầu tiên của cuộc trò chuyện. Theo Kopiczko et al. [2024], chúng tôi áp dụng VB-LoRA cho tất cả các lớp tuyến tính ngoại trừ lớp trên cùng. Cho Llama2 7B, chúng tôi sử dụng một ngân hàng vector của 2,048 vector, mỗi vector có độ dài 256, và hạng được đặt là 4, dẫn đến tổng cộng 0.8M tham số được lưu trữ. Cho Llama2 13B, chúng tôi sử dụng ngân hàng vector cùng kích thước nhưng tăng hạng lên 6, dẫn đến 1.1M tham số được lưu trữ. Cho tất cả các thí nghiệm, chúng tôi huấn luyện trong một epoch.

Kết quả được báo cáo trong Bảng 3. Đáng chú ý, chúng tôi báo cáo hai tập kết quả LoRA cho mỗi thí nghiệm: một từ việc thực hiện của chúng tôi và một từ Kopiczko et al. [2024], do có sự khác biệt đáng chú ý giữa các điểm số. Vì chúng tôi tuân thủ chặt chẽ các thiết lập thí nghiệm của Kopiczko et al. [2024], chúng tôi suy đoán rằng sự khác biệt là do thay đổi trong mô hình GPT-4 theo thời gian. Tuy nhiên, việc so sánh các cải thiện tương đối của VeRA và VB-LoRA với việc thực hiện LoRA tương ứng của chúng vẫn công bằng. VB-LoRA đạt được điểm số cao hơn LoRA trong khi chỉ sử dụng 0.5% (Llama2 7B) và 0.4% (Llama2 13B) tham số được lưu trữ. Trong khi VeRA có thể đạt điểm số tương tự với việc thực hiện LoRA của chúng, nó yêu cầu hơn gấp đôi tham số so với VB-LoRA.

4.4 Lý luận Toán học

Để đánh giá khả năng lý luận toán học, chúng tôi tinh chỉnh các mô hình Mistral-7B-v0.1 và Gemma-7B trên tập dữ liệu MetaMathQA8 [Yu et al., 2023] và kiểm tra chúng trên các tập dữ liệu GSM8K9 [Cobbe et al., 2021] và MATH10 [Hendrycks et al., 2021]. Chúng tôi so sánh kết quả của chúng tôi với công việc đồng thời LoRA-XS [Bałazy et al., 2024], theo cấu hình thí nghiệm của nó. Kết quả được hiển thị trong Bảng 4. Phương pháp của chúng tôi vượt trội tất cả baseline trên GSM8K, với Mistral-7B chỉ sử dụng 0.4% tham số so với LoRA, và Gemma-7B chỉ sử dụng 0.3%. So với LoRA-XS, phương pháp của chúng tôi vượt trội trên cả hai tập dữ liệu đánh giá trong khi sử dụng 70% (Mistral-7B) và 83% (Gemma-7B) tham số của LoRA-XS.

5Licensed under CC BY-NC 4.0. URLs: https://huggingface.co/datasets/tatsu-lab/alpaca, https://huggingface.co/datasets/yahma/alpaca-cleaned
6Licensed under CC BY 4.0. URL: https://huggingface.co/datasets/lmsys/mt_bench_human_judgments
7https://github.com/artidoro/qlora
8Licensed under MIT. URL: https://huggingface.co/datasets/meta-math/MetaMathQA
9Licensed under MIT. URL: https://huggingface.co/datasets/openai/gsm8k
10Licensed under MIT. URL: https://github.com/hendrycks/math/

--- TRANG 9 ---
Bảng 3: Kết quả với Llama2 trên MT-Bench, được chấm điểm bởi GPT-4 trên thang 10. LoRA† và VeRA được lấy từ Kopiczko et al. [2024]. LoRA‡ và VB-LoRA từ việc thực hiện của chúng tôi. Sự khác biệt giữa LoRA† và LoRA‡ có thể do thay đổi trong mô hình GPT-4 theo thời gian.

Bảng 4: Kết quả với các mô hình Mistral-7B và Gemma-7B trên Benchmark GSM8K và MATH. Cụ thể, trong VB-LoRA, chúng tôi sử dụng kích thước ngân hàng vector 2,048 với b=256, đặt hạng là 4, và huấn luyện với kích thước batch 128 trong 2 epoch. Tỷ lệ warm-up là 0.02, và huấn luyện sử dụng bộ lập lịch tỷ lệ học cosine, với tỷ lệ học ban đầu 0.001 cho ngân hàng vector và 0.01 cho logits. Kết quả baseline được lấy từ Bałazy et al. [2024].

[Các bảng với kết quả chi tiết]

4.5 Nghiên cứu Loại bỏ

Chúng tôi tiến hành một nghiên cứu loại bỏ để xem xét tác động của từng thành phần riêng lẻ của VB-LoRA. Các thí nghiệm được thực hiện trên RoBERTa-large, chỉ tinh chỉnh các mô-đun query và value.

Phương pháp Chọn Vector Ngoài mô-đun hỗn hợp top-k (viết tắt là Top-k bên dưới), tồn tại một số phương pháp tối ưu hóa rời rạc thường được sử dụng để chọn vector, bao gồm Noisy Top-k [Shazeer et al., 2016], Gumbel-Softmax (GS), và Straight-Through Gumbel-Softmax [Jang et al., 2017, Maddison et al., 2016]. Cho Top-k và Noisy Top-k, chúng tôi đánh giá tác động của k khác nhau đến hiệu suất trên tập dữ liệu CoLA. Cho GS và Straight-Through GS, chúng tôi đặt nhiệt độ τ=1/3 trong huấn luyện và sử dụng Top-1 và Top-2 Softmax cho suy luận. Ngoài ra, chúng tôi khám phá "Select All", một trường hợp đặc biệt của Top-k với k bằng kích thước ngân hàng vector h. Như thể hiện trong Bảng 5, Noisy Top-k, GS, và Straight-Through GS kém hiệu suất đáng kể so với Top-k và "Select All". Chúng tôi giả thuyết rằng nhiễu ngẫu nhiên được tiêm bởi các phương pháp này có thể làm gián đoạn các tham số của ngân hàng vector, dẫn đến bất ổn định trong quá trình học.

Chúng tôi tiếp tục điều tra tác động của k đến động lực huấn luyện và hiệu suất của VB-LoRA. Như thảo luận trong Phần 3.4, việc chọn k không chỉ ảnh hưởng đến hiệu suất của mô hình mà còn đến số lượng tham số được lưu trữ. Do đó, một k nhỏ hơn thường được ưa thích để cải thiện hiệu quả tham số. Bảng 5 cho thấy k=2 mang lại kết quả tốt nhất trên CoLA, trong khi k=1 hoạt động kém hơn đáng kể. Để giải thích điều này, chúng tôi đi sâu vào động lực huấn luyện của VB-LoRA. Như thể hiện trong Hình 3 (a), khi k=1, các vector được chọn vẫn phần lớn không thay đổi trong quá trình huấn luyện. Ngược lại, khi k>1, mô hình tích cực khám phá ngân hàng vector như minh họa trong Hình 3 (b) và (c), tức là các vector khác nhau được chọn và cập nhật tích cực trong quá trình huấn luyện. Ngoài ra, chúng tôi quan sát thấy rằng việc khám phá vector này chủ yếu xảy ra trong các giai đoạn đầu của huấn luyện, với các cập nhật trở nên thưa thớt dần trong các giai đoạn sau, như thể hiện trong Hình 5 trong phụ lục. Điều này cho thấy rằng các vector trở nên ngày càng chuyên biệt cho các vector con cụ thể khi huấn luyện tiến triển.

Độ dài Vector con b VB-LoRA giới thiệu một chiều ảo mới chia các chiều gốc của ma trận LoRA thành các vector con có độ dài b. Lưu ý rằng b phải là một thừa số chung của tất cả các chiều ẩn để đảm bảo tương thích qua toàn bộ mô hình. Tuy nhiên, giá trị tối ưu của b phụ thuộc vào tác vụ cụ thể và yêu cầu điều chỉnh như một siêu tham số. Về mặt lý thuyết, với một ngân sách ngân hàng vector cố định, một b lớn hơn giảm số lượng vector trong ngân hàng vector, có khả năng làm cho mỗi vector ít chuyên biệt hơn. Mặt khác, một b nhỏ hơn tăng số lượng tham số có thể huấn luyện và làm phức tạp quá trình chọn vector. Như thể hiện trong Bảng 6, một b=256 vừa phải mang lại hiệu suất tốt nhất trên tác vụ CoLA.

--- TRANG 10 ---
Bảng 5: Nghiên cứu loại bỏ các phương pháp chọn vector khác nhau. S.: Softmax, GS: Gumbel-Softmax, ST-GS: Straight Through Gumbel-Softmax.

Bảng 6: Nghiên cứu loại bỏ độ dài vector con.

[Các hình ảnh biểu đồ hiển thị dấu chân chọn vector của VB-LoRA trong quá trình huấn luyện]

5 Kết luận

Bài báo này giới thiệu một mô hình "chia sẻ và phân chia" và một mô-đun hỗn hợp top-k có thể phân biệt cho tinh chỉnh cực kỳ hiệu quả tham số với ngân hàng vector. VB-LoRA được đề xuất của chúng tôi đạt được độ chính xác cạnh tranh hoặc cao hơn trong khi sử dụng số lượng tham số được lưu trữ nhỏ hơn đáng kể so với các phương pháp PEFT tiên tiến, bao gồm LoRA, VeRA và Tied-LoRA. Ngoài ra, VB-LoRA là bất khả tri mô hình và có thể áp dụng cho các phương pháp PEFT khác [Ding et al., 2023], bao gồm bộ chuyển đổi được chèn [Karimi Mahabadi et al., 2021], điều chỉnh gợi ý [Qin et al., 2021], và BitFit [Ben Zaken et al., 2022]. Mặc dù VB-LoRA tập trung vào giảm chi phí lưu trữ và truyền tải cho tinh chỉnh LLM, chúng tôi tin rằng sơ đồ được đề xuất có thể được mở rộng để tinh chỉnh tiết kiệm bộ nhớ và tiền huấn luyện hiệu quả tham số. Chúng tôi để lại những điều này cho khám phá trong tương lai.

Tinh chỉnh một mô hình được tiền huấn luyện yêu cầu đưa ra các lựa chọn thiết kế về lớp nào của mô hình nên được đông lạnh hoặc cập nhật. Tinh chỉnh đa tác vụ thêm độ phức tạp bổ sung về tham số nào nên được chia sẻ hoặc cụ thể theo tác vụ. Theo dòng công việc này, Polytropon [Ponti et al., 2022] học chung một kho nhỏ các bộ chuyển đổi LoRA và một hàm định tuyến chọn một tập con có kích thước thay đổi của các bộ chuyển đổi để thích ứng few-shot. Caccia et al. [2023] nhấn mạnh tầm quan trọng của độ chi tiết định tuyến và tiếp tục đề xuất một sự pha trộn mịn hơn qua nhiều đầu. Theo các công việc này, sẽ thú vị khi khám phá một sự chuyển giao tham số mịn hơn qua các tác vụ, đầu, loại và lớp ở mức vector con cho tinh chỉnh đa tác vụ.

Hạn chế và tác động rộng hơn Các thí nghiệm của chúng tôi bị giới hạn ở các thiết lập đơn phương thức (dựa trên văn bản), đơn ngôn ngữ (tiếng Anh) và chỉ LoRA. Ngoài ra, việc khám phá ngân hàng vector của chúng tôi hơi hạn chế, vì chúng tôi chỉ xem xét một phạm vi nhỏ cấu hình cho kích thước ngân hàng và độ dài vector. Về tác động rộng hơn, VB-LoRA giảm chi phí lưu trữ và truyền tải của các bộ chuyển đổi LLM và chứng minh cải thiện hiệu quả bộ nhớ, làm cho các LLM tùy chỉnh dễ tiếp cận hơn. Chúng tôi không thấy trước bất kỳ tác động xã hội tiêu cực nào ngoài những tác động thường liên quan đến LLM.

Lời cảm ơn

Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh về các bình luận và đề xuất của họ, đã giúp cải thiện chất lượng của bài báo này.

--- TRANG 11 ---
[Phần tài liệu tham khảo với danh sách đầy đủ các nguồn trích dẫn]

--- TRANG 12-22 ---
[Phần phụ lục với các siêu tham số chi tiết, tài nguyên tính toán, hình ảnh trực quan hóa và các ví dụ bổ sung]
