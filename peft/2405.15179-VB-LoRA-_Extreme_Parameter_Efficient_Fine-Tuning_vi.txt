# 2405.15179.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/peft/2405.15179.pdf
# KÃ­ch thÆ°á»›c file: 885305 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
VB-LoRA: Tinh chá»‰nh Cá»±c ká»³ Hiá»‡u quáº£ Tham sá»‘
vá»›i NgÃ¢n hÃ ng Vector
Yang Li
Khoa Khoa há»c MÃ¡y tÃ­nh
Äáº¡i há»c Bang Georgia
Atlanta, GA 30303
yli93@student.gsu.eduShaobo Han
Máº¡ng Quang há»c vÃ  Cáº£m biáº¿n
NEC Laboratories America
Princeton, NJ 08540
shaobo@nec-labs.comShihao Jiâˆ—
TrÆ°á»ng Äiá»‡n toÃ¡n
Äáº¡i há»c Connecticut
Storrs, CT 06269
shihao.ji@uconn.edu
TÃ³m táº¯t
Khi viá»‡c Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n tÄƒng lÃªn vÃ  nhu cáº§u tÃ¹y chá»‰nh mÃ´ hÃ¬nh theo ngÆ°á»i dÃ¹ng hoáº·c tÃ¡c vá»¥ phÃ¡t triá»ƒn, cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT), cháº³ng háº¡n nhÆ° thÃ­ch á»©ng thá»© háº¡ng tháº¥p (LoRA) vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³, gÃ¢y ra chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n táº£i Ä‘Ã¡ng ká»ƒ. Äá»ƒ giáº£m thÃªm cÃ¡c tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯, chÃºng tÃ´i giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh "chia sáº» vÃ  phÃ¢n chia" phÃ¡ vá»¡ cÃ¡c rÃ o cáº£n cá»§a phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p qua cÃ¡c chiá»u ma tráº­n, mÃ´-Ä‘un vÃ  lá»›p báº±ng cÃ¡ch chia sáº» tham sá»‘ toÃ n cá»¥c thÃ´ng qua má»™t ngÃ¢n hÃ ng vector. NhÆ° má»™t sá»± thá»ƒ hiá»‡n cá»§a mÃ´ hÃ¬nh nÃ y Ä‘á»‘i vá»›i LoRA, VB-LoRA Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i táº¡o thÃ nh táº¥t cáº£ cÃ¡c ma tráº­n thá»© háº¡ng tháº¥p cá»§a LoRA tá»« má»™t ngÃ¢n hÃ ng vector Ä‘Æ°á»£c chia sáº» vá»›i má»™t mÃ´-Ä‘un há»—n há»£p top-k cÃ³ thá»ƒ phÃ¢n biá»‡t. VB-LoRA Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tham sá»‘ cá»±c ká»³ cao trong khi duy trÃ¬ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tá»‘t hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªn tiáº¿n. CÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i chá»©ng minh hiá»‡u quáº£ cá»§a VB-LoRA trÃªn cÃ¡c tÃ¡c vá»¥ hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn, táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘iá»u chá»‰nh hÆ°á»›ng dáº«n vÃ  lÃ½ luáº­n toÃ¡n há»c. Khi tinh chá»‰nh mÃ´ hÃ¬nh Llama2-13B, VB-LoRA chá»‰ sá»­ dá»¥ng 0,4% tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ cá»§a LoRA, nhÆ°ng Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ vÆ°á»£t trá»™i. MÃ£ nguá»“n cá»§a chÃºng tÃ´i cÃ³ sáºµn táº¡i https://github.com/leo-yangli/VB-LoRA. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã£ Ä‘Æ°á»£c há»£p nháº¥t vÃ o gÃ³i PEFT cá»§a Hugging Face2.

1 Giá»›i thiá»‡u
104105106
# tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯6566676869TÆ°Æ¡ng quan Matthews
VB-LoRA (Cá»§a chÃºng tÃ´i)
VeRA
Tied-LoRA
LoRA
HÃ¬nh 1: So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT trÃªn RoBERTa-Large. VB-LoRA cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘iá»ƒm sá»‘ cao hÆ¡n vá»›i sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ.

Tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT) táº¡o ra má»™t mÃ´ hÃ¬nh má»›i táº­n dá»¥ng kiáº¿n thá»©c tiÃªn nghiá»‡m máº¡nh máº½ Ä‘Æ°á»£c xÃ¢y dá»±ng trong cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng vÃ  thÃ­ch á»©ng chÃºng vá»›i má»™t loáº¡t cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u báº±ng cÃ¡ch cáº­p nháº­t má»™t lÆ°á»£ng nhá» tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n [He et al., 2021]. So vá»›i tinh chá»‰nh tiá»n tá»‘/gá»£i Ã½ [Li and Liang, 2021, Lester et al., 2021] hoáº·c há»c theo ngá»¯ cáº£nh [Brown et al., 2020], tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n quy mÃ´ lá»›n mang láº¡i chuyÃªn mÃ´n hÃ³a miá»n tá»‘t hÆ¡n Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi cÃ¡c bá»™ dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao [Brown et al., 2020, Liu et al., 2022, Zhao et al., 2023]. QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c láº·p láº¡i Ä‘á»ƒ phÃ¹ há»£p vá»›i nhu cáº§u cá»§a cÃ¡c ká»‹ch báº£n triá»ƒn khai luÃ´n thay Ä‘á»•i vÃ  cÃ¡ nhÃ¢n hÃ³a. Tuy nhiÃªn, khá»‘i lÆ°á»£ng tham sá»‘ khá»•ng lá»“ qua nhiá»u thá»ƒ hiá»‡n [Sheng et al., 2023] Ä‘áº·t ra thÃ¡ch thá»©c cho lÆ°u trá»¯, truyá»n táº£i vÃ  tÃ­nh toÃ¡n, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i pháº§n cá»©ng tÃ i nguyÃªn tháº¥p vÃ  máº¡ng lÆ°á»›i tiÃªu dÃ¹ng [Borzunov et al., 2024].

Äá»ƒ giáº£m thiá»ƒu nhá»¯ng thÃ¡ch thá»©c nÃ y, nhiá»u phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t báº±ng cÃ¡ch thÃªm hoáº·c thÃ­ch á»©ng má»™t lÆ°á»£ng nhá» tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cho má»—i tÃ¡c vá»¥ mÃ  khÃ´ng hy sinh hiá»‡u suáº¥t [Houlsby et al., 2019,

âˆ—Má»™t pháº§n cÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n khi tÃ¡c giáº£ liÃªn káº¿t vá»›i Äáº¡i há»c Bang Georgia.
2https://huggingface.co/docs/peft/en/package_reference/vblora

Há»™i nghá»‹ láº§n thá»© 38 vá» Há»‡ thá»‘ng Xá»­ lÃ½ ThÃ´ng tin Tháº§n kinh (NeurIPS 2024).arXiv:2405.15179v3 [cs.CL] 29 Oct 2024

--- TRANG 2 ---
vector con #2
LogitsTopk-softmaxÄa Äáº§u ChÄƒngQKVTruyá»n Tiáº¿n
OThÃªm & Chuáº©n hÃ³a
ThÃªm & Chuáº©n hÃ³aWdownWup
NgÃ¢n hÃ ng VectorLogitscho vector con #1Top-K Softmax + WpretrainedLoRAAB
â¨‚â¨‚vector con #1âŠ•Chá»nLÃ¡t
ğŸ”¥
ğŸ”¥
â„...
ğŸ’¾
ğŸ’¾vector con #8
ğŸ’¾Chá»n vÃ  gá»™p
ğŸ”¥Tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n
â„Tham sá»‘ Ä‘Ã´ng láº¡nh
ğŸ’¾Tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯MÃ´-Ä‘unQKVOWdownWupNgÃ¢n hÃ ng VectorCÃ¡cLá»›pLá»›p 1Lá»›p 2...Gá»™pVector ConAB

HÃ¬nh 2: TrÃ¡i: CÃ¡c tham sá»‘ mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t tá»• há»£p cÃ¡c vector tá»« má»™t ngÃ¢n hÃ ng vector, Ä‘Æ°á»£c chia sáº» qua cÃ¡c vector con, mÃ´-Ä‘un vÃ  lá»›p. Pháº£i: Kiáº¿n trÃºc cá»§a VB-LoRA. ChÃºng tÃ´i sá»­ dá»¥ng má»™t hÃ m softmax top-k Ä‘á»ƒ chá»n k vector tá»« ngÃ¢n hÃ ng vector. CÃ¡c vector Ä‘Æ°á»£c chá»n sau Ä‘Ã³ Ä‘Æ°á»£c gá»™p thÃ nh má»™t vector con, Ä‘Æ°á»£c sáº¯p xáº¿p táº¡i vá»‹ trÃ­ mong muá»‘n, táº¡o thÃ nh cÃ¡c tham sá»‘ cá»§a LoRA.

Karimi Mahabadi et al., 2021, Ding et al., 2023]. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y khai thÃ¡c sá»± phá»¥ thuá»™c giá»¯a cÃ¡c tham sá»‘ mÃ´ hÃ¬nh Ä‘á»ƒ giáº£m sá»± dÆ° thá»«a. VÃ­ dá»¥, Hu et al. [2021] Ä‘á» xuáº¥t thÃ­ch á»©ng thá»© háº¡ng tháº¥p (LoRA) Ä‘á»ƒ xáº¥p xá»‰ cáº­p nháº­t gradient tÃ­ch lÅ©y cho cÃ¡c mÃ´-Ä‘un tá»± chÃº Ã½, vÃ  táº¡o ra sá»± liÃªn káº¿t tham sá»‘ ná»™i ma tráº­n. Renduchintala et al. [2024] tiáº¿p tá»¥c nghiÃªn cá»©u cÃ¡c tÃ¹y chá»n cho phÃ©p chia sáº» tham sá»‘ liÃªn ma tráº­n thÃ´ng qua liÃªn káº¿t trá»ng sá»‘ qua táº¥t cáº£ cÃ¡c lá»›p. Trong cáº£ hai trÆ°á»ng há»£p, sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ. Hai phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»©ng á»Ÿ hai cá»±c cá»§a phá»• trong viá»‡c quyáº¿t Ä‘á»‹nh pháº¡m vi tÃ¡i sá»­ dá»¥ng thÃ nh pháº§n mÃ´ hÃ¬nh (cá»¥c bá»™ hoáº·c qua cÃ¡c lá»›p) vÃ  chá»‰ Ä‘á»‹nh ma tráº­n thá»© háº¡ng tháº¥p nÃ o cáº§n Ä‘Æ°á»£c chia sáº» vÃ  cáº­p nháº­t. Tuy nhiÃªn, khi kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÄƒng vÃ  nhu cáº§u vá» cÃ¡c mÃ´ hÃ¬nh tÃ¹y chá»‰nh ngÆ°á»i dÃ¹ng qua cÃ¡c dá»‹ch vá»¥ khÃ¡c nhau tÄƒng lÃªn, chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n táº£i cÃ¡c tÃ¹y chá»‰nh cho má»—i tá»• há»£p tÄƒng vá»t vÃ  xuáº¥t hiá»‡n nhÆ° má»™t váº¥n Ä‘á» quan trá»ng. Do Ä‘Ã³, viá»‡c nghiÃªn cá»©u cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT vá»›i sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ Ä‘Ã£ thu hÃºt ráº¥t nhiá»u quan tÃ¢m nghiÃªn cá»©u [Kopiczko et al., 2024, Renduchintala et al., 2024].

BÃ i bÃ¡o nÃ y giá»›i thiá»‡u VB-LoRA, tinh chá»‰nh cá»±c ká»³ hiá»‡u quáº£ tham sá»‘ vá»›i ngÃ¢n hÃ ng vector dá»±a trÃªn má»™t mÃ´ hÃ¬nh "chia sáº» vÃ  phÃ¢n chia" Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£. ChÃºng tÃ´i Ä‘áº©y giá»›i háº¡n hiá»‡u quáº£ tham sá»‘ LoRA báº±ng cÃ¡ch phÃ¡ vá»¡ hai rÃ o cáº£n cá»§a phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p: (1) cá»¥c bá»™ trong má»—i mÃ´-Ä‘un vÃ  má»—i lá»›p, vÃ  (2) chá»‰ qua hai chiá»u ma tráº­n gá»‘c (khÃ´ng chia tÃ¡ch; xem Pháº§n 3.2 Ä‘á»ƒ biáº¿t chi tiáº¿t). ChÃºng tÃ´i láº­p luáº­n ráº±ng cÃ¡c tham sá»‘ qua cÃ¡c mÃ´-Ä‘un vÃ  lá»›p khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº», vÃ  do Ä‘Ã³ sá»± dÆ° thá»«a trong tham sá»‘ cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m thÃªm. NgoÃ i ra, báº±ng cÃ¡ch phÃ¢n chia cÃ¡c vector thÃ nh pháº§n háº¡ng má»™t thÃ nh cÃ¡c vector con, chÃºng tÃ´i giá»›i thiá»‡u cÃ¡c chiá»u "áº£o" sao cho cáº¥u trÃºc sÃ¢u trong khÃ´ng gian tham sá»‘ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t phÃ¢n tÃ­ch ma tráº­n nÃ©n cao.

VB-LoRA láº¥y cáº£m há»©ng tá»« dÃ²ng cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y vá» máº¡ng tensor lÆ°á»£ng tá»­ hÃ³a [Oseledets, 2010, Cichocki, 2014] trong viá»‡c phÃ¡ vá»¡ rÃ ng buá»™c cá»§a chiá»u váº­t lÃ½ Ä‘á»ƒ nÃ©n tham sá»‘ cá»±c ká»³. Cá»¥ thá»ƒ, VB-LoRA tÃ¡i tham sá»‘ hÃ³a thÃ­ch á»©ng thá»© háº¡ng tháº¥p cá»§a LoRA báº±ng má»™t phÃ¢n tÃ­ch háº¡ng má»™t vÃ  sau Ä‘Ã³ chia cÃ¡c vector káº¿t quáº£ thÃ nh cÃ¡c vector con cÃ¹ng kÃ­ch thÆ°á»›c. Má»™t cÆ¡ cháº¿ chia sáº» toÃ n cá»¥c sau Ä‘Ã³ Ä‘Æ°á»£c há»c dá»±a trÃªn má»™t mÃ´-Ä‘un há»—n há»£p top-k thÆ°a thá»›t. CÃ¡c vector con cÃ¹ng kÃ­ch thÆ°á»›c cho phÃ©p tham sá»‘ Ä‘Æ°á»£c chia sáº» qua cÃ¡c mÃ´-Ä‘un vÃ  lá»›p á»Ÿ má»©c vector con. HÆ¡n ná»¯a, so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ©n ma tráº­n háº­u hoc [Oseledets, 2010, Khoromskij, 2011], VB-LoRA cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘áº§u cuá»‘i, vÃ  do Ä‘Ã³ quÃ¡ trÃ¬nh tinh chá»‰nh nháº­n thá»©c Ä‘Æ°á»£c dáº¡ng nÃ©n, cho phÃ©p nÃ©n hÆ°á»›ng tÃ¡c vá»¥. HÃ¬nh 1 minh há»a hiá»‡u quáº£ tham sá»‘ cá»§a VB-LoRA so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªn tiáº¿n. ÄÃ³ng gÃ³p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:

1. ChÃºng tÃ´i giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh "chia sáº» vÃ  phÃ¢n chia" phÃ¡ vá»¡ cÃ¡c rÃ o cáº£n cá»§a phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p qua cÃ¡c chiá»u ma tráº­n, mÃ´-Ä‘un vÃ  lá»›p báº±ng cÃ¡ch chia sáº» tham sá»‘ toÃ n cá»¥c thÃ´ng qua má»™t ngÃ¢n hÃ ng vector.

2. ChÃºng tÃ´i tÃ¡i tham sá»‘ hÃ³a phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p cá»§a LoRA báº±ng má»™t phÃ¢n tÃ­ch háº¡ng má»™t, vÃ  chia cÃ¡c vector káº¿t quáº£ thÃªm thÃ nh cÃ¡c vector con cÃ¹ng kÃ­ch thÆ°á»›c, cho phÃ©p hiá»‡u quáº£ tham sá»‘ cá»±c ká»³ á»Ÿ má»©c vector con.

--- TRANG 3 ---
3. ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t mÃ´-Ä‘un top-k thÆ°a thá»›t dá»±a trÃªn mÃ´ hÃ¬nh há»—n há»£p Ä‘á»ƒ há»c má»™t cÆ¡ cháº¿ chia sáº» toÃ n cá»¥c, lÃ m cho khung cá»§a chÃºng tÃ´i cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘áº§u cuá»‘i vÃ  nháº­n thá»©c nÃ©n.

4. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tham sá»‘ cá»±c ká»³ trong khi duy trÃ¬ hiá»‡u suáº¥t thá»±c nghiá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tá»‘t hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªn tiáº¿n trÃªn cÃ¡c tÃ¡c vá»¥ hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn, táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘iá»u chá»‰nh hÆ°á»›ng dáº«n vÃ  lÃ½ luáº­n toÃ¡n há»c.

2 CÃ´ng viá»‡c LiÃªn quan

Khai thÃ¡c Sá»± dÆ° thá»«a ToÃ n cá»¥c Ä‘á»ƒ TÄƒng cÆ°á»ng Hiá»‡u quáº£ Tham sá»‘ CÃ¡c tham sá»‘ cá»§a máº¡ng nÆ¡-ron sÃ¢u (DNN) cÃ³ thá»ƒ Ä‘Æ°á»£c chia tá»± nhiÃªn theo lá»›p, Ä‘áº§u hoáº·c loáº¡i (MHA hoáº·c FFN). Trong khi LoRA [Hu et al., 2021] chá»‰ khai thÃ¡c sá»± phá»¥ thuá»™c ná»™i ma tráº­n, Tied-LoRA [Renduchintala et al., 2024] sá»­ dá»¥ng má»™t sÆ¡ Ä‘á»“ liÃªn káº¿t trá»ng sá»‘ Ä‘Æ¡n giáº£n trÃªn cÃ¡c ma tráº­n thá»© háº¡ng tháº¥p A vÃ  B qua cÃ¡c lá»›p Ä‘á»ƒ giáº£m sá»± dÆ° thá»«a liÃªn ma tráº­n. Khi A vÃ  B Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, Ä‘Ã´ng láº¡nh vÃ  chia sáº» qua táº¥t cáº£ cÃ¡c lá»›p, Tied-LoRA thoÃ¡i hÃ³a thÃ nh VeRA [Kopiczko et al., 2024], chá»‰ yÃªu cáº§u hai vector tá»· lá»‡ Ä‘Æ°á»£c cáº­p nháº­t, dáº«n Ä‘áº¿n hiá»‡u quáº£ tham sá»‘ áº¥n tÆ°á»£ng. Má»™t cÃ´ng viá»‡c Ä‘á»“ng thá»i, LoRA-XS [BaÅ‚azy et al., 2024], tiáº¿p tá»¥c cáº£i thiá»‡n hiá»‡u quáº£ tham sá»‘ cá»§a LoRA báº±ng cÃ¡ch giá»›i thiá»‡u cÃ¡c ma tráº­n nhá» cÃ³ thá»ƒ huáº¥n luyá»‡n giá»¯a cÃ¡c ma tráº­n chiáº¿u LoRA Ä‘Ã´ng láº¡nh, Ä‘Æ°á»£c khá»Ÿi táº¡o báº±ng PhÃ¢n tÃ­ch GiÃ¡ trá»‹ ÄÆ¡n láº» (SVD) cá»§a trá»ng sá»‘ mÃ´-Ä‘un Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n. VB-LoRA cá»§a chÃºng tÃ´i Ä‘áº©y giá»›i háº¡n hiá»‡u quáº£ tham sá»‘ LoRA báº±ng cÃ¡ch chia sáº» tham sá»‘ toÃ n cá»¥c qua cÃ¡c mÃ´-Ä‘un vÃ  lá»›p á»Ÿ má»©c vector con.

Vá» tÃ¡i tham sá»‘ hÃ³a chiá»u tháº¥p, Aghajanyan et al. [2020] chá»©ng minh thá»±c nghiá»‡m ráº±ng tá»“n táº¡i má»™t tÃ¡i tham sá»‘ hÃ³a chiá»u tháº¥p hiá»‡u quáº£ nhÆ° khÃ´ng gian tham sá»‘ Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tinh chá»‰nh. Viá»‡c hiá»‡n thá»±c hÃ³a phÃ©p chiáº¿u ngáº«u nhiÃªn Ä‘Æ°á»£c Ä‘áº¡t thÃ´ng qua biáº¿n Ä‘á»•i Fastfood [Le et al., 2013] cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n quy mÃ´ lá»›n. Äá»ƒ lÃ m cho nÃ³ nháº­n thá»©c cáº¥u trÃºc, má»™t táº­p cÃ¡c tham sá»‘ tá»· lá»‡ theo lá»›p Ä‘Æ°á»£c bao gá»“m nhÆ° má»™t pháº§n cá»§a cÃ¡c tham sá»‘ huáº¥n luyá»‡n. Theo trá»±c giÃ¡c nÃ y, chÃºng tÃ´i nghiÃªn cá»©u tinh chá»‰nh nháº¹ trong LoRA dá»±a trÃªn tÃ¡i tham sá»‘ hÃ³a tÃ¹y chá»‰nh phÃ¡t sinh tá»« phÃ¢n tÃ­ch ma tráº­n háº¡ng má»™t.

HÆ¡n ná»¯a, phÃ¢n tÃ­ch tensor Ä‘Ã£ Ä‘Æ°á»£c táº­n dá»¥ng cho PEFT trong cÃ¡c mÃ´ hÃ¬nh ViT [Jie and Deng, 2023] dá»±a trÃªn cÃ¡c Ä‘á»‹nh dáº¡ng cá»• Ä‘iá»ƒn, cháº³ng háº¡n nhÆ° tensor-train hoáº·c Tucker [Kolda and Bader, 2009]. ChÃºng tÃ´i tháº¥y ráº±ng viá»‡c Ã©p buá»™c phÃ¢n tÃ­ch Ä‘a tuyáº¿n tÃ­nh qua nhiá»u cháº¿ Ä‘á»™ dáº«n Ä‘áº¿n sá»‘ háº¡ng cao hÆ¡n, cÃ³ háº¡i cho má»¥c tiÃªu nÃ©n tham sá»‘. Má»™t so sÃ¡nh giÃ¡n tiáº¿p cá»§a VB-LoRA vá»›i Jie and Deng [2023] cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch tham chiáº¿u tá»· lá»‡ nÃ©n vá»›i LoRA. Tá»« gÃ³c Ä‘á»™ nÃ y, VB-LoRA cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t Ä‘á»‹nh dáº¡ng tensor tÃ¹y chá»‰nh Ä‘Æ°á»£c trang bá»‹ cáº¥u trÃºc hÃ¬nh há»c lá»“i, Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi mÃ´ hÃ¬nh há»—n há»£p top-k thÆ°a thá»›t mÃ  chÃºng tÃ´i Ä‘á» xuáº¥t.

So vá»›i phÆ°Æ¡ng phÃ¡p fusion sÃ¢u [Mazzawi et al., 2024] nÆ¡i cÃ¡c tham sá»‘ LLM Ä‘Æ°á»£c phÃ¢n tÃ¡ch vÃ  khá»Ÿi táº¡o báº±ng cÃ¡c máº¡ng nhá» hÆ¡n Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n dÆ°á»›i má»™t cÆ¡ cháº¿ tÄƒng trÆ°á»Ÿng máº¡ng Ä‘Æ°á»£c thiáº¿t káº¿, viá»‡c phÃ¢n chia tham sá»‘ cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng trÃªn cÃ¡c vector thÃ nh pháº§n háº¡ng má»™t. PhÃ¢n chia vector con cho phÃ©p cÃ¡c má»Ÿ rá»™ng tÆ°Æ¡ng tá»± Ä‘á»ƒ táº­n dá»¥ng khá»Ÿi táº¡o ngÃ¢n hÃ ng vector Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n tá»« cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  huáº¥n luyá»‡n phÃ¢n tÃ¡n sá»­ dá»¥ng song song mÃ´ hÃ¬nh.

MÃ´ hÃ¬nh Tham sá»‘ dá»±a trÃªn MÃ´ hÃ¬nh Há»—n há»£p ThÆ°a thá»›t CÃ¡c mÃ´ hÃ¬nh há»—n há»£p Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong di truyá»n há»c quáº§n thá»ƒ [Pritchard et al., 2000], mÃ´ hÃ¬nh chá»§ Ä‘á» [Reisinger et al., 2010, Inouye et al., 2014], vÃ  tÃ¡ch phá»• siÃªu [Li and Bioucas-Dias, 2008, Fu et al., 2015] Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c thÃ nh pháº§n nguyÃªn máº«u (hoáº·c thÃ nh viÃªn cuá»‘i) tá»« dá»¯ liá»‡u quan sÃ¡t. CÃ¡c thÃ nh pháº§n nguyÃªn máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c ná»›i lá»ng Ä‘á»ƒ cÃ³ dáº¥u há»—n há»£p [Ding et al., 2008] vá»›i Ä‘áº£m báº£o nháº­n dáº¡ng [Lin et al., 2015]. Theo cÃ¡ch thÃ´ng thÆ°á»ng, Æ°á»›c lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c thá»±c hiá»‡n dá»±a trÃªn láº­p trÃ¬nh tuyáº¿n tÃ­nh [Chan et al., 2009] hoáº·c thuáº­t toÃ¡n tá»• há»£p [Arora et al., 2013]. Tuy nhiÃªn, má»™t bÃ i toÃ¡n láº­p trÃ¬nh nguyÃªn phá»©c táº¡p phÃ¡t sinh khi káº¿t há»£p má»™t rÃ ng buá»™c top-k bá»• sung vÃ o trá»ng sá»‘ há»—n há»£p Ä‘áº·c biá»‡t thÃ¡ch thá»©c Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ quy mÃ´ lá»›n. Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t há»c cÃ¡c ngÃ¢n hÃ ng vector nguyÃªn máº«u khÃ´ng tá»« dá»¯ liá»‡u quan sÃ¡t mÃ  tá»« cÃ¡c tham sá»‘ mÃ´ hÃ¬nh cá»§a LLM. Báº±ng cÃ¡ch sá»­a Ä‘á»•i mÃ´-Ä‘un top-k thÆ°a thá»›t [Shazeer et al., 2016] thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh Mixture-of-Expert [Jiang et al., 2024], trá»ng sá»‘ há»—n há»£p vÃ  ngÃ¢n hÃ ng vector Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng lan truyá»n ngÆ°á»£c dÆ°á»›i má»¥c tiÃªu cá»§a cÃ¡c tÃ¡c vá»¥ tinh chá»‰nh háº¡ lÆ°u. MÃ´ hÃ¬nh há»—n há»£p top-k Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  báº¥t kháº£ tri mÃ´ hÃ¬nh theo nghÄ©a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p sáºµn vÃ o báº¥t ká»³ tham sá»‘ máº¡ng nÆ¡-ron hoáº·c cáº­p nháº­t gradient tÃ­ch lÅ©y nÃ o.

--- TRANG 4 ---
3 PhÆ°Æ¡ng phÃ¡p Äá» xuáº¥t

3.1 Kiáº¿n thá»©c CÆ¡ báº£n: Kiáº¿n trÃºc Transformer vÃ  Bá»™ Chuyá»ƒn Ä‘á»•i LoRA

Kiáº¿n trÃºc transformer [Vaswani et al., 2017] bao gá»“m L lá»›p, má»—i lá»›p chá»©a hai loáº¡i khá»‘i: Multi-Head Attention (MHA) vÃ  Feed-Forward Network (FFN). ChÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c ma tráº­n query, key, value vÃ  output cá»§a MHA táº¡i lá»›p â„“ lÃ  Wâ„“t={Wit}Nhi=1,tâˆˆ{q,k,v,o}, trong Ä‘Ã³ WitâˆˆRdÃ—d, vÃ  Nh lÃ  sá»‘ Ä‘áº§u. Cho FFN(x)=WdownReLU(Wupx) vá»›i xâˆˆRd, xem FFN nhÆ° má»™t hoáº¡t Ä‘á»™ng Ä‘a Ä‘áº§u, chÃºng tÃ´i tiáº¿p tá»¥c chia WupâˆˆRcdÃ—d vÃ  WdownâˆˆRdÃ—cd thÃ nh c ma tráº­n cÃ³ kÃ­ch thÆ°á»›c dÃ—d, kÃ½ hiá»‡u bá»Ÿi Wâ„“up={Wâ„“,iup}ci=1 vÃ  Wâ„“down={Wâ„“,idown}ci=1. c=4.

Cho má»™t ma tráº­n Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n W0âˆˆRmÃ—n, LoRA [Hu et al., 2021] rÃ ng buá»™c cÃ¡c gia tÄƒng trá»ng sá»‘ Î”W nhÆ° má»™t phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p Î”W=BA, trong Ä‘Ã³ BâˆˆRmÃ—r, AâˆˆRrÃ—n lÃ  cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n, vá»›i râ‰ªmin(m,n). VeRA [Kopiczko et al., 2024] tiáº¿p tá»¥c giá»›i háº¡n cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n thÃ nh hai vector tá»· lá»‡ b vÃ  d, táº¡o thÃ nh cÃ¡c pháº§n tá»­ Ä‘Æ°á»ng chÃ©o cá»§a hai ma tráº­n Ä‘Æ°á»ng chÃ©o Î›b vÃ  Î›d. Do Ä‘Ã³, VeRA cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° Î”W=Î›bBÎ›dA, trong Ä‘Ã³ B vÃ  A Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, Ä‘Ã´ng láº¡nh vÃ  chia sáº» qua cÃ¡c lá»›p.

Tá»•ng thá»ƒ, chÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c tham sá»‘ mÃ´ hÃ¬nh cá»§a transformer nhÆ° Î©={{Wâ„“q,Wâ„“k,Wâ„“v,Wâ„“o}âˆª{Wâ„“up,Wâ„“down}}Lâ„“=1âˆˆR12LÃ—dÃ—d. Trong pháº§n tiáº¿p theo, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t tÃ¡i tham sá»‘ hÃ³a toÃ n cá»¥c trÃªn cÃ¡c gia tÄƒng trá»ng sá»‘ cá»§a WâˆˆÎ© dá»±a trÃªn phÃ¢n tÃ­ch LoRA Î”W=BA. chÃºng tÃ´i sáº½ chá»‰ ra cÃ¡ch hiá»‡u quáº£ tham sá»‘ cá»±c ká»³ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c báº±ng (1) chia sáº» tham sá»‘ qua cÃ¡c chiá»u ma tráº­n cá»§a A vÃ  B dá»±a trÃªn phÃ¢n tÃ­ch háº¡ng má»™t vÃ  phÃ¢n vÃ¹ng vector con (Pháº§n 3.2), vÃ  (2) qua cÃ¡c mÃ´-Ä‘un vÃ  lá»›p báº¥t ká»ƒ chá»‰ sá»‘ hoáº·c loáº¡i ma tráº­n (Pháº§n 3.3).

3.2 Chia sáº» vÃ  PhÃ¢n chia: má»™t MÃ´ hÃ¬nh Má»›i cho Chia sáº» Tham sá»‘

PhÃ¢n tÃ­ch thá»© háº¡ng tháº¥p cá»§a LoRA cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n tÆ°Æ¡ng Ä‘Æ°Æ¡ng dÆ°á»›i dáº¡ng háº¡ng má»™t nhÆ° sau:

Î”W=BA=âˆ‘rk=1bkâŠ—ak=âˆ‘rk=1âŠ—2i=1v(i)k,v(1)k=bk,v(2)k=ak, (3.1)

trong Ä‘Ã³ âŠ— kÃ½ hiá»‡u toÃ¡n tá»­ tÃ­ch ngoÃ i vÃ  v(i)k lÃ  má»™t vector cÃ³ kÃ­ch thÆ°á»›c di.

PhÃ¢n chia Dá»±a trÃªn phÃ¢n tÃ­ch háº¡ng má»™t á»Ÿ trÃªn, chÃºng tÃ´i tiáº¿p tá»¥c biá»ƒu diá»…n má»—i vector thÃ nh pháº§n v(i)k nhÆ° má»™t ná»‘i cá»§a má»™t táº­p cÃ¡c vector con,

v(i)k=concat(u(i)k,1,u(i)k,2,...,u(i)k,dâ€²i),u(i)k,jâˆˆRb, jâˆˆ{1,...,dâ€²i}, (3.2)

trong Ä‘Ã³ {di}i=1,2 biá»ƒu diá»…n kÃ­ch thÆ°á»›c cá»§a chiá»u ma tráº­n cá»§a Î”W. NÃ³i chung, {di}i=1,2 khÃ´ng báº±ng nhau qua A vÃ  B, vÃ  chÃºng tÃ´i chá»n b nhÆ° má»™t thá»«a sá»‘ chung cá»§a di sao cho dâ€²i=di/b vÃ  dâ€²iâˆˆZ.

Chia sáº» Äá»ƒ táº¡o Ä‘iá»u kiá»‡n chia sáº» tham sá»‘ qua cÃ¡c chiá»u mÃ´ hÃ¬nh, chÃºng tÃ´i giáº£ Ä‘á»‹nh má»—i vector con u(i)k,j nhÆ° má»™t há»—n há»£p top-k cá»§a cÃ¡c pháº§n tá»­ cÆ¡ báº£n tá»« ngÃ¢n hÃ ng vector B={Î±1,...,Î±h}, trong Ä‘Ã³ Î±iâˆˆRb vá»›i iâˆˆ{1,...,h}, vÃ  Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau (vá»›i cÃ¡c chá»‰ sá»‘ phá»¥ Ä‘Æ°á»£c bá» qua Ä‘á»ƒ rÃµ rÃ ng):

u=âˆ‘hs=1ws(Ïƒ)Î±s,w(Ïƒ)=Softmax(TopK(Ïƒ,k)), (3.3)

trong Ä‘Ã³ TopK(Ïƒ,k)i=Ïƒi náº¿u Ïƒi náº±m trong top-k cá»§a Ïƒ vÃ  TopK(Ïƒ,k)i=âˆ’âˆ ngÆ°á»£c láº¡i. Äá»‘i vá»›i má»—i vector con u, chÃºng tÃ´i giá»›i thiá»‡u logits ÏƒâˆˆRh nhÆ° cÃ¡c tham sá»‘ cÃ³ thá»ƒ há»c cá»§a nÃ³. ChÃºng tÃ´i gá»i mÃ´ hÃ¬nh Ä‘Æ°á»£c biá»ƒu diá»…n trong Eq. 3.3 lÃ  mÃ´-Ä‘un há»—n há»£p top-k (TKAM), cÃ³ thá»ƒ phÃ¢n biá»‡t. Thiáº¿t káº¿ nÃ y cho phÃ©p há»c chung ngÃ¢n hÃ ng vector B vÃ  logits Ïƒ theo cÃ¡ch Ä‘áº§u cuá»‘i, phÃ¹ há»£p vá»›i tinh chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u.

MÃ´-Ä‘un TKAM thÃºc Ä‘áº©y tÃ­nh thÆ°a thá»›t báº±ng cÃ¡ch chá»n k vector cá»§a cÃ¡c logits lá»›n nháº¥t tá»« ngÃ¢n hÃ ng vector. Báº±ng cÃ¡ch Ä‘áº·t kâ‰ªh, chÃºng tÃ´i háº¡n cháº¿ vector con u pháº£i thÆ°a thá»›t. NghÄ©a lÃ , trong má»—i láº§n láº·p, cÃ¡c cáº­p nháº­t cho ngÃ¢n hÃ ng vector váº«n bá»‹ chi phá»‘i cá»¥c bá»™ â€“ vá»›i nhiá»u nháº¥t k vector cÆ¡ sá»Ÿ Î±âˆˆB bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi lan truyá»n ngÆ°á»£c qua uâ€“ vá»›i hy vá»ng ráº±ng cÃ¡c vector Ä‘Ã£ há»c cÃ³ thá»ƒ chuyÃªn biá»‡t hÆ¡n vÃ  kiáº¿n thá»©c Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i trong ngÃ¢n hÃ ng vector cÃ³ thá»ƒ Ä‘Æ°á»£c kÃ­ch hoáº¡t vÃ  cáº­p nháº­t má»™t cÃ¡ch thÆ°a thá»›t.

--- TRANG 5 ---
MÃ´-Ä‘un Top-k KhÃ´ng Nhiá»…u MÃ´-Ä‘un Noisy Top-k Gating [Shazeer et al., 2016] Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ thay tháº¿ cÃ¡c lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ báº±ng cÃ¡c lá»›p Mixture of Experts (MoE) trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n [Jiang et al., 2024]. NgÆ°á»£c láº¡i, chÃºng tÃ´i sá»­ dá»¥ng Eq. 3.3 Ä‘á»ƒ há»c sÆ¡ Ä‘á»“ chia sáº» chá»n lá»c qua cÃ¡c vector thÃ nh pháº§n háº¡ng má»™t mÃ  khÃ´ng thay Ä‘á»•i mÃ´ hÃ¬nh gá»‘c. Do phÃ¢n tÃ­ch, chÃºng tÃ´i tháº¥y ráº±ng cÃ¡c cáº­p nháº­t tham sá»‘ gradient tÃ­ch lÅ©y nháº¡y cáº£m hÆ¡n so vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh gá»‘c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c váº¥n Ä‘á» báº¥t á»•n Ä‘á»‹nh huáº¥n luyá»‡n Ä‘Æ°á»£c quan sÃ¡t trong hypernetworks [Ortiz et al., 2024], nÆ¡i cÃ¡c tham sá»‘ Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t mÃ´ hÃ¬nh tham sá»‘ hÃ³a khÃ¡c. Do Ä‘Ã³, giá»¯ nhiá»…u báº±ng khÃ´ng trong hÃ m gating cÃ³ thá»ƒ giÃºp viá»‡c há»c hiá»‡u quáº£ vÃ  á»•n Ä‘á»‹nh hÆ¡n. Má»™t nghiÃªn cá»©u loáº¡i bá» vá» cÃ¡c phÆ°Æ¡ng phÃ¡p chá»n vector khÃ¡c nhau, bao gá»“m Gumbel-softmax, Ä‘Æ°á»£c cung cáº¥p trong Pháº§n 4.5.

3.3 PhÃ¡ vá»¡ Ranh giá»›i cá»§a LoRA cho Chia sáº» Tham sá»‘ ToÃ n cá»¥c

Trong khi LoRA chá»‰ Ã¡p dá»¥ng phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p cho má»—i gia tÄƒng trá»ng sá»‘ riÃªng láº», ranh giá»›i cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¡ vá»¡ bá»Ÿi sÆ¡ Ä‘á»“ chia sáº» vÃ  phÃ¢n chia mÃ  chÃºng tÃ´i Ä‘á» xuáº¥t trong Pháº§n 3.2. PhÆ°Æ¡ng phÃ¡p chia sáº» vÃ  phÃ¢n chia cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c diá»…n giáº£i nhÆ° phÃ¢n tÃ­ch tensor phÃ¢n cáº¥p vÃ  cÃ³ rÃ ng buá»™c, táº¡o Ä‘iá»u kiá»‡n chia sáº» tham sá»‘ toÃ n cá»¥c hiá»‡u quáº£ vÆ°á»£t ra ngoÃ i biá»ƒu diá»…n thá»© háº¡ng tháº¥p cá»§a ma tráº­n LoRA.

ToÃ¡n tá»­ chia láº§n Ä‘áº§u Ä‘Æ°á»£c giá»›i thiá»‡u trong Quantized Tensor Train (QTT) Ä‘á»ƒ nÃ©n siÃªu cá»§a cÃ¡c ma tráº­n quy mÃ´ lá»›n [Oseledets, 2010, Cichocki, 2014]. VÃ­ dá»¥, phÃ¢n chia dyadic Ä‘á»‹nh hÃ¬nh láº¡i má»™t vector cÃ³ Ä‘á»™ dÃ i L=2p thÃ nh má»™t máº£ng p chiá»u táº¡o Ä‘iá»u kiá»‡n cho phÃ¢n tÃ­ch Tensor Train hiá»‡u quáº£ Ä‘Æ°á»£c sá»­ dá»¥ng. ToÃ¡n tá»­ chia cá»§a chÃºng tÃ´i thay vÃ o Ä‘Ã³ Ã¡p dá»¥ng cho cÃ¡c vector thÃ nh pháº§n háº¡ng má»™t v(i)k, vÃ  biá»ƒu diá»…n tensorial phÃ¢n cáº¥p káº¿t quáº£ cá»§a Î”W cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° Canonical Polyadic Decomposition (CPD) [Kolda and Bader, 2009] vá»›i cÃ¡c vector thÃ nh pháº§n v(i)k Ä‘Æ°á»£c gáº¥p thÃ nh cÃ¡c máº£ng 2 chiá»u vá»›i cÃ¡c vector con u(i)k,j nhÆ° cÃ¡c cá»™t. Má»—i vector con ui Ä‘Æ°á»£c táº¡o thÃ nh tá»« má»™t ngÃ¢n hÃ ng vector toÃ n cá»¥c B chia sáº» thÃ´ng qua TKAM, trong Ä‘Ã³ i=[j,v] lÃ  má»™t chá»‰ sá»‘ Ä‘a bao gá»“m cÃ¡c chá»‰ sá»‘ váº­t lÃ½ j, cháº³ng háº¡n nhÆ° mÃ´-Ä‘un, lá»›p, Ä‘áº§u vÃ  ma tráº­n phÃ¢n tÃ­ch trÃ¡i/pháº£i, vÃ  cÃ¡c chá»‰ sá»‘ áº£o v (Ä‘Æ°á»£c táº¡o tá»« phÃ¢n vÃ¹ng vector).

ToÃ¡n tá»­ chia sáº» (mÃ´-Ä‘un TKAM) cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t mÃ´ hÃ¬nh nhÃ¢n tá»‘ vá»›i cÃ¡c rÃ ng buá»™c simplex trÃªn trá»ng sá»‘ há»—n há»£p (vÃ­ dá»¥, k=2, vector con u náº±m trÃªn cÃ¡c cáº¡nh cá»§a simplex) vÃ  cÃ¡c nhÃ¢n tá»‘ chung Ä‘Æ°á»£c lÆ°u trá»¯ trong B. Gá»i uâˆˆRb vÃ  u=âˆ‘hs=1Î±sws, trong Ä‘Ã³ Î±s lÃ  nhÃ¢n tá»‘ thá»© s, vÃ  w lÃ  Ä‘iá»ƒm nhÃ¢n tá»‘ cho vector con u. ChÃºng tÃ´i xem xÃ©t cÃ¡c tÃ¹y chá»n sau cho w: (1) Admixture (tá»• há»£p lá»“i): wâˆˆ[0,1]h vÃ  âˆ‘hs=1ws=1, thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong nhiá»u cá»™ng Ä‘á»“ng khÃ¡c nhau. (2) Sparse Admixture (TKAM): wâˆˆ[0,1]h vÃ  âˆ‘hs=1ws=1 vá»›i chá»‰ kâ‰ªh pháº§n tá»­ khÃ¡c khÃ´ng Ä‘Æ°á»£c phÃ©p. ÄÃ¡ng chÃº Ã½ ráº±ng viá»‡c thÃªm thÃ´ng tin chá»‰ sá»‘ Ä‘a vÃ o cÆ¡ cháº¿ chá»n vector cÃ³ thá»ƒ lÃ m cho mÃ´ hÃ¬nh TKAM nháº­n thá»©c cáº¥u trÃºc, cÃ³ kháº£ nÄƒng mang láº¡i lá»£i Ã­ch bá»• sung. Má»™t kháº£ nÄƒng lÃ  lÃ m cho logits cá»§a chá»n vector cÃ³ Ä‘iá»u kiá»‡n trÃªn cÃ¡c embedding cá»§a lá»›p, mÃ´-Ä‘un vÃ  loáº¡i ma tráº­n, cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t hypernetwork [Mahabadi et al., 2021]. Tuy nhiÃªn, chÃºng tÃ´i Ä‘á»ƒ láº¡i Ä‘iá»u nÃ y cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

TÃ³m láº¡i, LoRA cung cáº¥p má»™t phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p cá»¥c bá»™ cho má»—i ma tráº­n d1Ã—d2 Î”W Ä‘á»™c láº­p. NgÆ°á»£c láº¡i, VB-LoRA cá»§a chÃºng tÃ´i giá»›i thiá»‡u má»™t phÃ¢n tÃ­ch thá»© háº¡ng tháº¥p toÃ n cá»¥c trÃªn má»™t ma tráº­n bÃ—|{i}| Ä‘Æ°á»£c táº¡o thÃ nh tá»« cÃ¡c vector háº¡ng má»™t Ä‘Æ°á»£c phÃ¢n vÃ¹ng, trong Ä‘Ã³ |{i}| kÃ½ hiá»‡u sá»‘ lÆ°á»£ng cá»§a táº­p chá»‰ sá»‘ bao gá»“m cáº£ chá»‰ sá»‘ váº­t lÃ½ vÃ  áº£o. NhÆ° chÃºng ta sáº½ tháº¥y bÃªn dÆ°á»›i, sá»± khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ táº­n dá»¥ng tá»‘t hÆ¡n sá»± dÆ° thá»«a trong cÃ¡c gradient tÃ­ch lÅ©y, dáº«n Ä‘áº¿n hiá»‡u quáº£ tham sá»‘ cá»±c ká»³.

HÃ¬nh 2 tá»•ng quan phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i. Pháº§n bÃªn trÃ¡i chá»©ng minh Ã½ tÆ°á»Ÿng cáº¥p cao cá»§a VB-LoRA: ngÃ¢n hÃ ng vector Ä‘Æ°á»£c chia sáº» qua cÃ¡c vector con, mÃ´-Ä‘un vÃ  lá»›p. Pháº§n bÃªn pháº£i chi tiáº¿t kiáº¿n trÃºc cá»§a nÃ³. Äá»ƒ táº¡o thÃ nh má»—i vector con, chÃºng tÃ´i sá»­ dá»¥ng má»™t hÃ m softmax top-k Ä‘á»ƒ chá»n k vector tá»« ngÃ¢n hÃ ng vector, sau Ä‘Ã³ Ä‘Æ°á»£c gá»™p thÃ nh má»™t vector con. CÃ¡c vector con nÃ y Ä‘Æ°á»£c sáº¯p xáº¿p á»Ÿ cÃ¡c vá»‹ trÃ­ mong muá»‘n, táº¡o thÃ nh cÃ¡c tham sá»‘ cho LoRA vá»›i chi phÃ­ tÃ­nh toÃ¡n khÃ´ng Ä‘Ã¡ng ká»ƒ. Thuáº­t toÃ¡n 1 cung cáº¥p mÃ£ giáº£ giá»‘ng PyTorch cho VB-LoRA, cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p liá»n máº¡ch vÃ o khung PyTorch.

3.4 Äáº¿m Tham sá»‘

Trong tinh chá»‰nh Ä‘áº§y Ä‘á»§, sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n báº±ng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, tá»©c lÃ  LMd2, trong Ä‘Ã³ L lÃ  sá»‘ lá»›p, M lÃ  sá»‘ mÃ´-Ä‘un Ä‘Æ°á»£c tinh chá»‰nh, vÃ  d lÃ  chiá»u áº©n.

--- TRANG 6 ---
Thuáº­t toÃ¡n 1 MÃ£ giáº£ cá»§a VB-LoRA theo phong cÃ¡ch giá»‘ng PyTorch
# d: chiá»u áº©n; b: Ä‘á»™ dÃ i cá»§a vector con; r: háº¡ng; h: kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vector
# k: sá»‘ vector Ä‘Æ°á»£c chá»n sá»­ dá»¥ng trong mÃ´-Ä‘un há»—n há»£p top-k  
# logits: Má»—i lá»›p tuyáº¿n tÃ­nh cÃ³ hai tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n: logits_A vÃ  logits_B.
# Cáº£ hai tham sá»‘ Ä‘á»u cÃ³ hÃ¬nh dáº¡ng (d/b)*r*h.
# vector_bank: NgÃ¢n hÃ ng vector Ä‘Æ°á»£c chia sáº» vá»›i hÃ¬nh dáº¡ng h*b.
# x vÃ  W: Ä‘áº§u vÃ o vÃ  trá»ng sá»‘ gá»‘c.

def get_low_rank_matrix(logits, vector_bank, k):
    topk_logits, topk_indices = logits.topk(k, dim=-1)
    topk_weights = torch.softmax(topk_logits, dim=-1)
    matrix = (topk_weights * vector_bank[topk_indices]).sum(-2)
    return matrix

def VBLoRA_forward(x, vector_bank, logits_A, logits_B, k):
    r = logits_A.shape[1]
    A = get_low_rank_matrix(logits_A, vector_bank, k).transpose(0, 1).reshape(r, -1)
    B = get_low_rank_matrix(logits_B, vector_bank, k).transpose(1, 2).reshape(-1, r)
    # Äá»ƒ tiáº¿t kiá»‡m bá»™ nhá»›, chÃºng tÃ´i trÃ¡nh tÃ­nh toÃ¡n rÃµ rÃ ng \delta W = B @ A.
    return x @ W + (x @ B) @ A

LoRA giáº£m sá»‘ nÃ y xuá»‘ng 2LMdr, trong khi VeRA tiáº¿p tá»¥c giáº£m xuá»‘ng LM(d+r). CÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cá»§a LoRA vÃ  VeRA giá»‘ng vá»›i cÃ¡c tham sá»‘ chÃºng cáº§n lÆ°u trá»¯.

Trong VB-LoRA, cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n bao gá»“m hai pháº§n: cÃ¡c tham sá»‘ cá»§a ngÃ¢n hÃ ng vector B vÃ  cÃ¡c tham sá»‘ cá»§a logits Ïƒ. Tuy nhiÃªn, khi káº¿t thÃºc huáº¥n luyá»‡n, cÃ¡c tham sá»‘ logit cÃ³ thá»ƒ Ä‘Æ°á»£c bá» qua vÃ  chá»‰ cáº§n lÆ°u trá»¯ k chá»‰ sá»‘ Ä‘Æ°á»£c chá»n vÃ  trá»ng sá»‘ há»—n há»£p top-k. Do Ä‘Ã³, cÃ¡c tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t bá»™ ba Î˜={B,I,V}, trong Ä‘Ã³ BâˆˆRhÃ—b lÃ  má»™t ngÃ¢n hÃ ng vector chá»©a h vector b chiá»u, IâˆˆR2Ã—LÃ—MÃ—rÃ—(d/b)Ã—k lÃ  cÃ¡c chá»‰ sá»‘ top-k cá»§a cÃ¡c vector trong B cho táº¥t cáº£ vector con, vÃ  VâˆˆR2Ã—LÃ—MÃ—rÃ—(d/b)Ã—(kâˆ’1) lÃ  trá»ng sá»‘ há»—n há»£p top-k Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o thÃ nh cÃ¡c vector con tá»« ngÃ¢n hÃ ng. ÄÃ¡ng chÃº Ã½ ráº±ng trá»ng sá»‘ há»—n há»£p top-k chá»‰ cÃ³ kâˆ’1 báº­c tá»± do vÃ¬ chÃºng pháº£i cÃ³ tá»•ng báº±ng 1. NgoÃ i ra, tÃ¹y thuá»™c vÃ o kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vector h, cÃ¡c chá»‰ sá»‘ I cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ hiá»‡u quáº£ nhÆ° sá»‘ nguyÃªn khÃ´ng dáº¥u (vÃ­ dá»¥, uint8 khi hâ‰¤256), vÃ  do Ä‘Ã³, chÃºng tÃ´i Ä‘áº¿m sá»‘ tham sá»‘ nhÆ° kÃ­ch thÆ°á»›c tÆ°Æ¡ng Ä‘Æ°Æ¡ng float32 Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng. Khi chÃºng tÃ´i sá»­ dá»¥ng k=2 vÃ  uint8 cho chá»‰ sá»‘, sá»‘ tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ cá»§a VB-LoRA lÃ  hb+3LMr(d/b). KhÃ´ng nhÆ° LoRA vÃ  VeRA, sá»‘ tham sá»‘ trong VB-LoRA khÃ´ng tÄƒng tuyáº¿n tÃ­nh vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh (Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi L vÃ  d) hoáº·c sá»‘ mÃ´-Ä‘un Ä‘Æ°á»£c tinh chá»‰nh, tá»©c lÃ  M. Trong khi sá»‘ háº¡ng thá»© hai cá»§a tham sá»‘ VB-LoRA lÃ  má»™t hÃ m tuyáº¿n tÃ­nh cá»§a LMd, há»‡ sá»‘ lÃ  3r/b, thÆ°á»ng ráº¥t nhá». VÃ­ dá»¥, trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, cÃ¡c giÃ¡ trá»‹ Ä‘iá»ƒn hÃ¬nh lÃ  r=4 vÃ  b=256, dáº«n Ä‘áº¿n há»‡ sá»‘ 0.04, trong khi há»‡ sá»‘ lÃ  2r cho LoRA vÃ  1 cho VeRA. Háº§u háº¿t cÃ¡c tham sá»‘ trong VB-LoRA náº±m trong ngÃ¢n hÃ ng vector Ä‘Æ°á»£c chia sáº», cÃ³ kÃ­ch thÆ°á»›c khÃ´ng tÄƒng tuyáº¿n tÃ­nh vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh hoáº·c sá»‘ mÃ´-Ä‘un Ä‘Æ°á»£c tinh chá»‰nh.

4 ThÃ­ nghiá»‡m

Trong pháº§n nÃ y, chÃºng tÃ´i tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i thÃ´ng qua má»™t loáº¡t thÃ­ nghiá»‡m. ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng viá»‡c so sÃ¡nh VB-LoRA vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªn tiáº¿n: LoRA, VeRA vÃ  Tied-LoRA trÃªn benchmark GLUE. Tiáº¿p theo, chÃºng tÃ´i má»Ÿ rá»™ng phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i sang cÃ¡c tÃ¡c vá»¥ táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn sá»­ dá»¥ng GPT-2, cÃ¡c tÃ¡c vá»¥ Ä‘iá»u chá»‰nh hÆ°á»›ng dáº«n trÃªn Llama2, cÅ©ng nhÆ° cÃ¡c tÃ¡c vá»¥ lÃ½ luáº­n toÃ¡n há»c trÃªn cÃ¡c mÃ´ hÃ¬nh Mistral vÃ  Gemma. Táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn má»™t mÃ¡y chá»§ Ä‘Æ°á»£c trang bá»‹ 8 GPU NVIDIA A100. Äá»ƒ tÃ¡i táº¡o, chÃºng tÃ´i cung cáº¥p cÃ¡c siÃªu tham sá»‘ chi tiáº¿t vÃ  thÃ´ng sá»‘ ká»¹ thuáº­t cá»§a tÃ i nguyÃªn tÃ­nh toÃ¡n cho má»—i thÃ­ nghiá»‡m trong phá»¥ lá»¥c. MÃ£ nguá»“n cÃ³ sáºµn táº¡i https://github.com/leo-yangli/VB-LoRA.

4.1 Hiá»ƒu NgÃ´n ngá»¯ Tá»± nhiÃªn

ChÃºng tÃ´i Ã¡p dá»¥ng benchmark General Language Understanding Evaluation (GLUE)3 [Wang et al., 2018] Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a VB-LoRA qua nhiá»u tÃ¡c vá»¥ hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn, bao gá»“m

3https://gluebenchmark.com/

--- TRANG 7 ---
Báº£ng 1: Káº¿t quáº£ vá»›i RoBERTa base vÃ  RoBERTa large trÃªn benchmark GLUE. Káº¿t quáº£ tá»‘t nháº¥t trong má»—i nhÃ³m Ä‘Æ°á»£c hiá»ƒn thá»‹ báº±ng chá»¯ Ä‘áº­m. ChÃºng tÃ´i bÃ¡o cÃ¡o tÆ°Æ¡ng quan Matthew cho CoLA, tÆ°Æ¡ng quan Pearson cho STS-B, vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c. Káº¿t quáº£ cho LoRA qv vÃ  VeRA qv Ä‘Æ°á»£c láº¥y tá»« cÃ¡c bÃ i bÃ¡o gá»‘c tÆ°Æ¡ng á»©ng, trong khi cÃ¡c káº¿t quáº£ khÃ¡c dá»±a trÃªn viá»‡c thá»±c hiá»‡n cá»§a chÃºng tÃ´i. ChÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t trung vá»‹ tá»« 5 láº§n cháº¡y sá»­ dá»¥ng cÃ¡c háº¡t giá»‘ng ngáº«u nhiÃªn khÃ¡c nhau.

[Báº£ng chi tiáº¿t vá»›i cÃ¡c sá»‘ liá»‡u hiá»‡u suáº¥t cho cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau trÃªn cÃ¡c tÃ¡c vá»¥ GLUE]

cÃ¡c tÃ¡c vá»¥ tÆ°Æ¡ng tá»±, paraphrase vÃ  suy luáº­n. Theo Kopiczko et al. [2024], chÃºng tÃ´i táº­p trung vÃ o sÃ¡u tÃ¡c vá»¥ tá»« GLUE: CoLA [Warstadt et al., 2019] (kháº£ nÄƒng cháº¥p nháº­n ngÃ´n ngá»¯), SST-2 [Socher et al., 2013] (phÃ¢n tÃ­ch cáº£m xÃºc), MRPC [Dolan and Brockett, 2005] (phÃ¡t hiá»‡n paraphrase), STS-B [Cer et al., 2017] (tÆ°Æ¡ng tá»± vÄƒn báº£n ngá»¯ nghÄ©a), QNLI [Rajpurkar et al., 2018] (suy luáº­n), vÃ  RTE (suy luáº­n). CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i RoBERTa base vÃ  RoBERTa large [Liu et al., 2019]. Trong khi LoRA vÃ  VeRA chá»‰ tinh chá»‰nh cÃ¡c mÃ´-Ä‘un query vÃ  value, chÃºng tÃ´i khÃ¡m phÃ¡ hai chiáº¿n lÆ°á»£c tinh chá»‰nh: chá»‰ query vÃ  value (VB-LoRA qv), vÃ  táº¥t cáº£ cÃ¡c mÃ´-Ä‘un tuyáº¿n tÃ­nh (VB-LoRA all), bao gá»“m Wq, Wk, Wv, Wo, Wup, vÃ  Wdown. ChÃºng tÃ´i táº¡o má»™t ngÃ¢n hÃ ng vector cá»§a 90 vector cÃ³ Ä‘á»™ dÃ i 256, Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i phÃ¢n phá»‘i Ä‘á»“ng Ä‘á»u U(âˆ’0.02,0.02). CÃ¡c logits Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i phÃ¢n phá»‘i chuáº©n N(0,0.01). Tá»· lá»‡ há»c cho cÃ¡c tham sá»‘ ngÃ¢n hÃ ng vector vÃ  logit Ä‘Æ°á»£c Ä‘áº·t láº§n lÆ°á»£t lÃ  0.001 vÃ  0.01. ChÃºng tÃ´i Ä‘áº·t háº¡ng lÃ  4 vÃ  k=2 cho táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m.

Báº£ng 1 tiáº¿t lá»™ ráº±ng VB-LoRA Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh hoáº·c vÆ°á»£t trá»™i so vá»›i VeRA vÃ  Tied-LoRA, trong khi hiá»‡u quáº£ tham sá»‘ hÆ¡n. VÃ­ dá»¥, khi tinh chá»‰nh cÃ¡c mÃ´-Ä‘un query vÃ  value trÃªn mÃ´ hÃ¬nh RoBERTa large, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i giáº£m cÃ¡c tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ xuá»‘ng dÆ°á»›i 40% so vá»›i VeRA hoáº·c Tied-LoRA, trong khi vÆ°á»£t trá»™i chÃºng qua táº¥t cáº£ cÃ¡c tÃ¡c vá»¥. Nhá»¯ng káº¿t quáº£ nÃ y cho tháº¥y ráº±ng hiá»‡u suáº¥t mÃ´ hÃ¬nh khÃ´ng chá»‰ phá»¥ thuá»™c vÃ o sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n mÃ  cÃ²n vÃ o cÃ¡ch chÃºng Ä‘Æ°á»£c táº¡o thÃ nh.

HÆ¡n ná»¯a, cÃ¡c káº¿t quáº£ nháº¥t quÃ¡n chá»‰ ra ráº±ng tinh chá»‰nh táº¥t cáº£ cÃ¡c mÃ´-Ä‘un, ngoÃ i chá»‰ cÃ¡c mÃ´-Ä‘un query vÃ  value, tÄƒng cÆ°á»ng hiá»‡u suáº¥t cho táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p. Tuy nhiÃªn, LoRA, VeRA vÃ  Tied-LoRA yÃªu cáº§u 2-4 láº§n tham sá»‘ trong trÆ°á»ng há»£p nÃ y vÃ¬ sá»‘ tham sá»‘ cá»§a chÃºng tÄƒng tuyáº¿n tÃ­nh vá»›i sá»‘ mÃ´-Ä‘un Ä‘Æ°á»£c tinh chá»‰nh. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i chá»‰ sá»­ dá»¥ng thÃªm 37.5% tham sá»‘ vÃ¬ chÃºng tÃ´i duy trÃ¬ cÃ¹ng kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vector nhÆ°ng thÃªm cÃ¡c tham sá»‘ bá»• sung cho chá»‰ sá»‘ vÃ  trá»ng sá»‘ top-k. Do Ä‘Ã³, vá»›i chá»‰ 12.8% tham sá»‘ so vá»›i VeRA all (4% so vá»›i LoRA qv), phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t trung bÃ¬nh tá»‘t nháº¥t.

4.2 Táº¡o NgÃ´n ngá»¯ Tá»± nhiÃªn

Cho cÃ¡c thÃ­ nghiá»‡m táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn, chÃºng tÃ´i tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh GPT-2 Medium vÃ  Large [Radford et al., 2019] trÃªn táº­p dá»¯ liá»‡u E2E4 [Novikova et al., 2017], chá»©a khoáº£ng 42,000 vÃ­ dá»¥ huáº¥n luyá»‡n, 4,600 vÃ­ dá»¥ xÃ¡c thá»±c vÃ  4,600 vÃ­ dá»¥ kiá»ƒm tra tá»« lÄ©nh vá»±c nhÃ  hÃ ng. ChÃºng tÃ´i sá»­ dá»¥ng má»™t ngÃ¢n hÃ ng vector cÃ³ kÃ­ch thÆ°á»›c 256 cho GPT-2 Medium vÃ  350 cho GPT-2 Large. Äá»™ dÃ i vector Ä‘Æ°á»£c Ä‘áº·t lÃ  256 vÃ  háº¡ng Ä‘Æ°á»£c Ä‘áº·t lÃ  4 cho cáº£ hai mÃ´ hÃ¬nh. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t, chÃºng tÃ´i tinh chá»‰nh táº¥t cáº£ cÃ¡c lá»›p attention vÃ  lá»›p FFN. NhÆ° thá»ƒ hiá»‡n trong Báº£ng 2, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh so vá»›i VeRA, trong khi yÃªu cáº§u khoáº£ng 20% Ã­t tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ hÆ¡n cho cáº£ hai mÃ´ hÃ¬nh.

4Licensed under CC BY-SA 4.0. URL: https://github.com/tuetschek/e2e-dataset

--- TRANG 8 ---
Báº£ng 2: Káº¿t quáº£ vá»›i GPT-2 Medium vÃ  GPT-2 Large trÃªn benchmark E2E. Káº¿t quáº£ cho FT vÃ  LoRA Ä‘Æ°á»£c láº¥y tá»« Hu et al. [2021], vÃ  káº¿t quáº£ cho VeRA Ä‘Æ°á»£c láº¥y tá»« Kopiczko et al. [2024]. ChÃºng tÃ´i bÃ¡o cÃ¡o trung bÃ¬nh cá»§a 3 láº§n cháº¡y sá»­ dá»¥ng cÃ¡c háº¡t giá»‘ng ngáº«u nhiÃªn khÃ¡c nhau.

[Báº£ng vá»›i káº¿t quáº£ hiá»‡u suáº¥t cho cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau]

4.3 Äiá»u chá»‰nh HÆ°á»›ng dáº«n

Äiá»u chá»‰nh hÆ°á»›ng dáº«n lÃ  má»™t quÃ¡ trÃ¬nh tinh chá»‰nh mÃ´ hÃ¬nh vá»›i má»™t táº­p cÃ¡c hÆ°á»›ng dáº«n hoáº·c gá»£i Ã½ Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t cá»§a nÃ³ trÃªn cÃ¡c hÆ°á»›ng dáº«n cá»¥ thá»ƒ [Ouyang et al., 2022]. Äáº§u tiÃªn chÃºng tÃ´i thÃ­ nghiá»‡m trÃªn má»™t táº­p dá»¯ liá»‡u Ä‘iá»u chá»‰nh hÆ°á»›ng dáº«n tá»•ng quÃ¡t. ChÃºng tÃ´i sá»­ dá»¥ng Cleaned Alpaca Dataset5, cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»¯ liá»‡u cá»§a táº­p dá»¯ liá»‡u Alpaca gá»‘c [Taori et al., 2023]. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh trÃªn MT-Bench6 [Zheng et al., 2024], chá»©a 80 cÃ¢u há»i Ä‘a lÆ°á»£t.

Theo Kopiczko et al. [2024], chÃºng tÃ´i tinh chá»‰nh mÃ´ hÃ¬nh Llama2 [Touvron et al., 2023] trong khung QLoRA7 [Dettmers et al., 2023], nháº±m giáº£m sá»­ dá»¥ng bá»™ nhá»› khi tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n trÃªn má»™t GPU duy nháº¥t. ChÃºng tÃ´i sá»­ dá»¥ng chiáº¿n lÆ°á»£c lÆ°á»£ng tá»­ hÃ³a Ä‘Æ°á»£c cung cáº¥p bá»Ÿi QLoRA, bao gá»“m 4-bit NormalFloat cho dá»¯ liá»‡u lÆ°u trá»¯, BFloat16 cho tham sá»‘ tÃ­nh toÃ¡n, lÆ°á»£ng tá»­ hÃ³a kÃ©p vÃ  bá»™ tá»‘i Æ°u hÃ³a phÃ¢n trang Ä‘á»ƒ huáº¥n luyá»‡n nÃ³ trÃªn má»™t GPU duy nháº¥t. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh cá»§a chÃºng tÃ´i táº¡o ra cÃ¡c pháº£n há»“i cho nhá»¯ng cÃ¢u há»i nÃ y, vÃ  sau Ä‘Ã³, GPT-4 Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xem xÃ©t vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o, gÃ¡n má»™t Ä‘iá»ƒm sá»‘ Ä‘á»‹nh lÆ°á»£ng trÃªn thang Ä‘iá»ƒm 10. LÆ°u Ã½ ráº±ng phÃ¹ há»£p vá»›i VeRA, chÃºng tÃ´i bÃ¡o cÃ¡o Ä‘iá»ƒm sá»‘ cá»§a lÆ°á»£t Ä‘áº§u tiÃªn cá»§a cuá»™c trÃ² chuyá»‡n. Theo Kopiczko et al. [2024], chÃºng tÃ´i Ã¡p dá»¥ng VB-LoRA cho táº¥t cáº£ cÃ¡c lá»›p tuyáº¿n tÃ­nh ngoáº¡i trá»« lá»›p trÃªn cÃ¹ng. Cho Llama2 7B, chÃºng tÃ´i sá»­ dá»¥ng má»™t ngÃ¢n hÃ ng vector cá»§a 2,048 vector, má»—i vector cÃ³ Ä‘á»™ dÃ i 256, vÃ  háº¡ng Ä‘Æ°á»£c Ä‘áº·t lÃ  4, dáº«n Ä‘áº¿n tá»•ng cá»™ng 0.8M tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯. Cho Llama2 13B, chÃºng tÃ´i sá»­ dá»¥ng ngÃ¢n hÃ ng vector cÃ¹ng kÃ­ch thÆ°á»›c nhÆ°ng tÄƒng háº¡ng lÃªn 6, dáº«n Ä‘áº¿n 1.1M tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯. Cho táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m, chÃºng tÃ´i huáº¥n luyá»‡n trong má»™t epoch.

Káº¿t quáº£ Ä‘Æ°á»£c bÃ¡o cÃ¡o trong Báº£ng 3. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i bÃ¡o cÃ¡o hai táº­p káº¿t quáº£ LoRA cho má»—i thÃ­ nghiá»‡m: má»™t tá»« viá»‡c thá»±c hiá»‡n cá»§a chÃºng tÃ´i vÃ  má»™t tá»« Kopiczko et al. [2024], do cÃ³ sá»± khÃ¡c biá»‡t Ä‘Ã¡ng chÃº Ã½ giá»¯a cÃ¡c Ä‘iá»ƒm sá»‘. VÃ¬ chÃºng tÃ´i tuÃ¢n thá»§ cháº·t cháº½ cÃ¡c thiáº¿t láº­p thÃ­ nghiá»‡m cá»§a Kopiczko et al. [2024], chÃºng tÃ´i suy Ä‘oÃ¡n ráº±ng sá»± khÃ¡c biá»‡t lÃ  do thay Ä‘á»•i trong mÃ´ hÃ¬nh GPT-4 theo thá»i gian. Tuy nhiÃªn, viá»‡c so sÃ¡nh cÃ¡c cáº£i thiá»‡n tÆ°Æ¡ng Ä‘á»‘i cá»§a VeRA vÃ  VB-LoRA vá»›i viá»‡c thá»±c hiá»‡n LoRA tÆ°Æ¡ng á»©ng cá»§a chÃºng váº«n cÃ´ng báº±ng. VB-LoRA Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm sá»‘ cao hÆ¡n LoRA trong khi chá»‰ sá»­ dá»¥ng 0.5% (Llama2 7B) vÃ  0.4% (Llama2 13B) tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯. Trong khi VeRA cÃ³ thá»ƒ Ä‘áº¡t Ä‘iá»ƒm sá»‘ tÆ°Æ¡ng tá»± vá»›i viá»‡c thá»±c hiá»‡n LoRA cá»§a chÃºng, nÃ³ yÃªu cáº§u hÆ¡n gáº¥p Ä‘Ã´i tham sá»‘ so vá»›i VB-LoRA.

4.4 LÃ½ luáº­n ToÃ¡n há»c

Äá»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng lÃ½ luáº­n toÃ¡n há»c, chÃºng tÃ´i tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh Mistral-7B-v0.1 vÃ  Gemma-7B trÃªn táº­p dá»¯ liá»‡u MetaMathQA8 [Yu et al., 2023] vÃ  kiá»ƒm tra chÃºng trÃªn cÃ¡c táº­p dá»¯ liá»‡u GSM8K9 [Cobbe et al., 2021] vÃ  MATH10 [Hendrycks et al., 2021]. ChÃºng tÃ´i so sÃ¡nh káº¿t quáº£ cá»§a chÃºng tÃ´i vá»›i cÃ´ng viá»‡c Ä‘á»“ng thá»i LoRA-XS [BaÅ‚azy et al., 2024], theo cáº¥u hÃ¬nh thÃ­ nghiá»‡m cá»§a nÃ³. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 4. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i táº¥t cáº£ baseline trÃªn GSM8K, vá»›i Mistral-7B chá»‰ sá»­ dá»¥ng 0.4% tham sá»‘ so vá»›i LoRA, vÃ  Gemma-7B chá»‰ sá»­ dá»¥ng 0.3%. So vá»›i LoRA-XS, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i trÃªn cáº£ hai táº­p dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ trong khi sá»­ dá»¥ng 70% (Mistral-7B) vÃ  83% (Gemma-7B) tham sá»‘ cá»§a LoRA-XS.

5Licensed under CC BY-NC 4.0. URLs: https://huggingface.co/datasets/tatsu-lab/alpaca, https://huggingface.co/datasets/yahma/alpaca-cleaned
6Licensed under CC BY 4.0. URL: https://huggingface.co/datasets/lmsys/mt_bench_human_judgments
7https://github.com/artidoro/qlora
8Licensed under MIT. URL: https://huggingface.co/datasets/meta-math/MetaMathQA
9Licensed under MIT. URL: https://huggingface.co/datasets/openai/gsm8k
10Licensed under MIT. URL: https://github.com/hendrycks/math/

--- TRANG 9 ---
Báº£ng 3: Káº¿t quáº£ vá»›i Llama2 trÃªn MT-Bench, Ä‘Æ°á»£c cháº¥m Ä‘iá»ƒm bá»Ÿi GPT-4 trÃªn thang 10. LoRAâ€  vÃ  VeRA Ä‘Æ°á»£c láº¥y tá»« Kopiczko et al. [2024]. LoRAâ€¡ vÃ  VB-LoRA tá»« viá»‡c thá»±c hiá»‡n cá»§a chÃºng tÃ´i. Sá»± khÃ¡c biá»‡t giá»¯a LoRAâ€  vÃ  LoRAâ€¡ cÃ³ thá»ƒ do thay Ä‘á»•i trong mÃ´ hÃ¬nh GPT-4 theo thá»i gian.

Báº£ng 4: Káº¿t quáº£ vá»›i cÃ¡c mÃ´ hÃ¬nh Mistral-7B vÃ  Gemma-7B trÃªn Benchmark GSM8K vÃ  MATH. Cá»¥ thá»ƒ, trong VB-LoRA, chÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vector 2,048 vá»›i b=256, Ä‘áº·t háº¡ng lÃ  4, vÃ  huáº¥n luyá»‡n vá»›i kÃ­ch thÆ°á»›c batch 128 trong 2 epoch. Tá»· lá»‡ warm-up lÃ  0.02, vÃ  huáº¥n luyá»‡n sá»­ dá»¥ng bá»™ láº­p lá»‹ch tá»· lá»‡ há»c cosine, vá»›i tá»· lá»‡ há»c ban Ä‘áº§u 0.001 cho ngÃ¢n hÃ ng vector vÃ  0.01 cho logits. Káº¿t quáº£ baseline Ä‘Æ°á»£c láº¥y tá»« BaÅ‚azy et al. [2024].

[CÃ¡c báº£ng vá»›i káº¿t quáº£ chi tiáº¿t]

4.5 NghiÃªn cá»©u Loáº¡i bá»

ChÃºng tÃ´i tiáº¿n hÃ nh má»™t nghiÃªn cá»©u loáº¡i bá» Ä‘á»ƒ xem xÃ©t tÃ¡c Ä‘á»™ng cá»§a tá»«ng thÃ nh pháº§n riÃªng láº» cá»§a VB-LoRA. CÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn RoBERTa-large, chá»‰ tinh chá»‰nh cÃ¡c mÃ´-Ä‘un query vÃ  value.

PhÆ°Æ¡ng phÃ¡p Chá»n Vector NgoÃ i mÃ´-Ä‘un há»—n há»£p top-k (viáº¿t táº¯t lÃ  Top-k bÃªn dÆ°á»›i), tá»“n táº¡i má»™t sá»‘ phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a rá»i ráº¡c thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chá»n vector, bao gá»“m Noisy Top-k [Shazeer et al., 2016], Gumbel-Softmax (GS), vÃ  Straight-Through Gumbel-Softmax [Jang et al., 2017, Maddison et al., 2016]. Cho Top-k vÃ  Noisy Top-k, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a k khÃ¡c nhau Ä‘áº¿n hiá»‡u suáº¥t trÃªn táº­p dá»¯ liá»‡u CoLA. Cho GS vÃ  Straight-Through GS, chÃºng tÃ´i Ä‘áº·t nhiá»‡t Ä‘á»™ Ï„=1/3 trong huáº¥n luyá»‡n vÃ  sá»­ dá»¥ng Top-1 vÃ  Top-2 Softmax cho suy luáº­n. NgoÃ i ra, chÃºng tÃ´i khÃ¡m phÃ¡ "Select All", má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a Top-k vá»›i k báº±ng kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vector h. NhÆ° thá»ƒ hiá»‡n trong Báº£ng 5, Noisy Top-k, GS, vÃ  Straight-Through GS kÃ©m hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so vá»›i Top-k vÃ  "Select All". ChÃºng tÃ´i giáº£ thuyáº¿t ráº±ng nhiá»…u ngáº«u nhiÃªn Ä‘Æ°á»£c tiÃªm bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ lÃ m giÃ¡n Ä‘oáº¡n cÃ¡c tham sá»‘ cá»§a ngÃ¢n hÃ ng vector, dáº«n Ä‘áº¿n báº¥t á»•n Ä‘á»‹nh trong quÃ¡ trÃ¬nh há»c.

ChÃºng tÃ´i tiáº¿p tá»¥c Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a k Ä‘áº¿n Ä‘á»™ng lá»±c huáº¥n luyá»‡n vÃ  hiá»‡u suáº¥t cá»§a VB-LoRA. NhÆ° tháº£o luáº­n trong Pháº§n 3.4, viá»‡c chá»n k khÃ´ng chá»‰ áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh mÃ  cÃ²n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯. Do Ä‘Ã³, má»™t k nhá» hÆ¡n thÆ°á»ng Ä‘Æ°á»£c Æ°a thÃ­ch Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ tham sá»‘. Báº£ng 5 cho tháº¥y k=2 mang láº¡i káº¿t quáº£ tá»‘t nháº¥t trÃªn CoLA, trong khi k=1 hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ. Äá»ƒ giáº£i thÃ­ch Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘i sÃ¢u vÃ o Ä‘á»™ng lá»±c huáº¥n luyá»‡n cá»§a VB-LoRA. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 3 (a), khi k=1, cÃ¡c vector Ä‘Æ°á»£c chá»n váº«n pháº§n lá»›n khÃ´ng thay Ä‘á»•i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. NgÆ°á»£c láº¡i, khi k>1, mÃ´ hÃ¬nh tÃ­ch cá»±c khÃ¡m phÃ¡ ngÃ¢n hÃ ng vector nhÆ° minh há»a trong HÃ¬nh 3 (b) vÃ  (c), tá»©c lÃ  cÃ¡c vector khÃ¡c nhau Ä‘Æ°á»£c chá»n vÃ  cáº­p nháº­t tÃ­ch cá»±c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. NgoÃ i ra, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng viá»‡c khÃ¡m phÃ¡ vector nÃ y chá»§ yáº¿u xáº£y ra trong cÃ¡c giai Ä‘oáº¡n Ä‘áº§u cá»§a huáº¥n luyá»‡n, vá»›i cÃ¡c cáº­p nháº­t trá»Ÿ nÃªn thÆ°a thá»›t dáº§n trong cÃ¡c giai Ä‘oáº¡n sau, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 5 trong phá»¥ lá»¥c. Äiá»u nÃ y cho tháº¥y ráº±ng cÃ¡c vector trá»Ÿ nÃªn ngÃ y cÃ ng chuyÃªn biá»‡t cho cÃ¡c vector con cá»¥ thá»ƒ khi huáº¥n luyá»‡n tiáº¿n triá»ƒn.

Äá»™ dÃ i Vector con b VB-LoRA giá»›i thiá»‡u má»™t chiá»u áº£o má»›i chia cÃ¡c chiá»u gá»‘c cá»§a ma tráº­n LoRA thÃ nh cÃ¡c vector con cÃ³ Ä‘á»™ dÃ i b. LÆ°u Ã½ ráº±ng b pháº£i lÃ  má»™t thá»«a sá»‘ chung cá»§a táº¥t cáº£ cÃ¡c chiá»u áº©n Ä‘á»ƒ Ä‘áº£m báº£o tÆ°Æ¡ng thÃ­ch qua toÃ n bá»™ mÃ´ hÃ¬nh. Tuy nhiÃªn, giÃ¡ trá»‹ tá»‘i Æ°u cá»§a b phá»¥ thuá»™c vÃ o tÃ¡c vá»¥ cá»¥ thá»ƒ vÃ  yÃªu cáº§u Ä‘iá»u chá»‰nh nhÆ° má»™t siÃªu tham sá»‘. Vá» máº·t lÃ½ thuyáº¿t, vá»›i má»™t ngÃ¢n sÃ¡ch ngÃ¢n hÃ ng vector cá»‘ Ä‘á»‹nh, má»™t b lá»›n hÆ¡n giáº£m sá»‘ lÆ°á»£ng vector trong ngÃ¢n hÃ ng vector, cÃ³ kháº£ nÄƒng lÃ m cho má»—i vector Ã­t chuyÃªn biá»‡t hÆ¡n. Máº·t khÃ¡c, má»™t b nhá» hÆ¡n tÄƒng sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ  lÃ m phá»©c táº¡p quÃ¡ trÃ¬nh chá»n vector. NhÆ° thá»ƒ hiá»‡n trong Báº£ng 6, má»™t b=256 vá»«a pháº£i mang láº¡i hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn tÃ¡c vá»¥ CoLA.

--- TRANG 10 ---
Báº£ng 5: NghiÃªn cá»©u loáº¡i bá» cÃ¡c phÆ°Æ¡ng phÃ¡p chá»n vector khÃ¡c nhau. S.: Softmax, GS: Gumbel-Softmax, ST-GS: Straight Through Gumbel-Softmax.

Báº£ng 6: NghiÃªn cá»©u loáº¡i bá» Ä‘á»™ dÃ i vector con.

[CÃ¡c hÃ¬nh áº£nh biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ dáº¥u chÃ¢n chá»n vector cá»§a VB-LoRA trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n]

5 Káº¿t luáº­n

BÃ i bÃ¡o nÃ y giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh "chia sáº» vÃ  phÃ¢n chia" vÃ  má»™t mÃ´-Ä‘un há»—n há»£p top-k cÃ³ thá»ƒ phÃ¢n biá»‡t cho tinh chá»‰nh cá»±c ká»³ hiá»‡u quáº£ tham sá»‘ vá»›i ngÃ¢n hÃ ng vector. VB-LoRA Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cáº¡nh tranh hoáº·c cao hÆ¡n trong khi sá»­ dá»¥ng sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c lÆ°u trá»¯ nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªn tiáº¿n, bao gá»“m LoRA, VeRA vÃ  Tied-LoRA. NgoÃ i ra, VB-LoRA lÃ  báº¥t kháº£ tri mÃ´ hÃ¬nh vÃ  cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c [Ding et al., 2023], bao gá»“m bá»™ chuyá»ƒn Ä‘á»•i Ä‘Æ°á»£c chÃ¨n [Karimi Mahabadi et al., 2021], Ä‘iá»u chá»‰nh gá»£i Ã½ [Qin et al., 2021], vÃ  BitFit [Ben Zaken et al., 2022]. Máº·c dÃ¹ VB-LoRA táº­p trung vÃ o giáº£m chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n táº£i cho tinh chá»‰nh LLM, chÃºng tÃ´i tin ráº±ng sÆ¡ Ä‘á»“ Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ tinh chá»‰nh tiáº¿t kiá»‡m bá»™ nhá»› vÃ  tiá»n huáº¥n luyá»‡n hiá»‡u quáº£ tham sá»‘. ChÃºng tÃ´i Ä‘á»ƒ láº¡i nhá»¯ng Ä‘iá»u nÃ y cho khÃ¡m phÃ¡ trong tÆ°Æ¡ng lai.

Tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n yÃªu cáº§u Ä‘Æ°a ra cÃ¡c lá»±a chá»n thiáº¿t káº¿ vá» lá»›p nÃ o cá»§a mÃ´ hÃ¬nh nÃªn Ä‘Æ°á»£c Ä‘Ã´ng láº¡nh hoáº·c cáº­p nháº­t. Tinh chá»‰nh Ä‘a tÃ¡c vá»¥ thÃªm Ä‘á»™ phá»©c táº¡p bá»• sung vá» tham sá»‘ nÃ o nÃªn Ä‘Æ°á»£c chia sáº» hoáº·c cá»¥ thá»ƒ theo tÃ¡c vá»¥. Theo dÃ²ng cÃ´ng viá»‡c nÃ y, Polytropon [Ponti et al., 2022] há»c chung má»™t kho nhá» cÃ¡c bá»™ chuyá»ƒn Ä‘á»•i LoRA vÃ  má»™t hÃ m Ä‘á»‹nh tuyáº¿n chá»n má»™t táº­p con cÃ³ kÃ­ch thÆ°á»›c thay Ä‘á»•i cá»§a cÃ¡c bá»™ chuyá»ƒn Ä‘á»•i Ä‘á»ƒ thÃ­ch á»©ng few-shot. Caccia et al. [2023] nháº¥n máº¡nh táº§m quan trá»ng cá»§a Ä‘á»™ chi tiáº¿t Ä‘á»‹nh tuyáº¿n vÃ  tiáº¿p tá»¥c Ä‘á» xuáº¥t má»™t sá»± pha trá»™n má»‹n hÆ¡n qua nhiá»u Ä‘áº§u. Theo cÃ¡c cÃ´ng viá»‡c nÃ y, sáº½ thÃº vá»‹ khi khÃ¡m phÃ¡ má»™t sá»± chuyá»ƒn giao tham sá»‘ má»‹n hÆ¡n qua cÃ¡c tÃ¡c vá»¥, Ä‘áº§u, loáº¡i vÃ  lá»›p á»Ÿ má»©c vector con cho tinh chá»‰nh Ä‘a tÃ¡c vá»¥.

Háº¡n cháº¿ vÃ  tÃ¡c Ä‘á»™ng rá»™ng hÆ¡n CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i bá»‹ giá»›i háº¡n á»Ÿ cÃ¡c thiáº¿t láº­p Ä‘Æ¡n phÆ°Æ¡ng thá»©c (dá»±a trÃªn vÄƒn báº£n), Ä‘Æ¡n ngÃ´n ngá»¯ (tiáº¿ng Anh) vÃ  chá»‰ LoRA. NgoÃ i ra, viá»‡c khÃ¡m phÃ¡ ngÃ¢n hÃ ng vector cá»§a chÃºng tÃ´i hÆ¡i háº¡n cháº¿, vÃ¬ chÃºng tÃ´i chá»‰ xem xÃ©t má»™t pháº¡m vi nhá» cáº¥u hÃ¬nh cho kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng vÃ  Ä‘á»™ dÃ i vector. Vá» tÃ¡c Ä‘á»™ng rá»™ng hÆ¡n, VB-LoRA giáº£m chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n táº£i cá»§a cÃ¡c bá»™ chuyá»ƒn Ä‘á»•i LLM vÃ  chá»©ng minh cáº£i thiá»‡n hiá»‡u quáº£ bá»™ nhá»›, lÃ m cho cÃ¡c LLM tÃ¹y chá»‰nh dá»… tiáº¿p cáº­n hÆ¡n. ChÃºng tÃ´i khÃ´ng tháº¥y trÆ°á»›c báº¥t ká»³ tÃ¡c Ä‘á»™ng xÃ£ há»™i tiÃªu cá»±c nÃ o ngoÃ i nhá»¯ng tÃ¡c Ä‘á»™ng thÆ°á»ng liÃªn quan Ä‘áº¿n LLM.

Lá»i cáº£m Æ¡n

ChÃºng tÃ´i muá»‘n cáº£m Æ¡n cÃ¡c nhÃ  Ä‘Ã¡nh giÃ¡ áº©n danh vá» cÃ¡c bÃ¬nh luáº­n vÃ  Ä‘á» xuáº¥t cá»§a há», Ä‘Ã£ giÃºp cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a bÃ i bÃ¡o nÃ y.

--- TRANG 11 ---
[Pháº§n tÃ i liá»‡u tham kháº£o vá»›i danh sÃ¡ch Ä‘áº§y Ä‘á»§ cÃ¡c nguá»“n trÃ­ch dáº«n]

--- TRANG 12-22 ---
[Pháº§n phá»¥ lá»¥c vá»›i cÃ¡c siÃªu tham sá»‘ chi tiáº¿t, tÃ i nguyÃªn tÃ­nh toÃ¡n, hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ  cÃ¡c vÃ­ dá»¥ bá»• sung]
