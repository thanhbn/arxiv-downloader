# 2309.05173.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.05173.pdf
# Kích thước tệp: 835631 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
DEPT: ĐIỀU CHỈNH PROMPT PHÂN TÍCH CHO
TINH CHỈNH HIỆU QUẢ THAM SỐ
Zhengxiang Shi, Aldo Lipani
Đại học College London, Vương quốc Anh
{zhengxiang.shi.19,aldo.lipani }@ucl.ac.uk
https://github.com/ZhengxiangShi/DePT
TÓM TẮT
Điều chỉnh prompt (PT), trong đó một lượng nhỏ các vector prompt mềm (liên tục) có thể huấn luyện được gắn vào đầu vào mô hình, đã cho thấy kết quả hứa hẹn trên các nhiệm vụ và kiến trúc mô hình khác nhau cho tinh chỉnh hiệu quả tham số (PEFT). PT nổi bật so với các phương pháp PEFT khác vì nó duy trì hiệu suất cạnh tranh với ít tham số có thể huấn luyện hơn và không tăng mạnh các tham số khi kích thước mô hình mở rộng. Tuy nhiên, PT đưa thêm các token prompt mềm, dẫn đến chuỗi đầu vào dài hơn, điều này ảnh hưởng đáng kể đến thời gian huấn luyện/suy luận và việc sử dụng bộ nhớ do độ phức tạp bậc hai của Transformer. Điều này đặc biệt quan ngại đối với Mô hình Ngôn ngữ Lớn (LLM) phải đối mặt với việc truy vấn hàng ngày nặng nề. Để giải quyết vấn đề này, chúng tôi đề xuất Điều chỉnh Prompt Phân tích (DEPT), phân tích prompt mềm thành một prompt mềm ngắn hơn và một cặp ma trận thấp hạng sau đó được tối ưu hóa với hai tốc độ học khác nhau. Điều này cho phép DEPT đạt được hiệu suất tốt hơn trong khi tiết kiệm đáng kể chi phí bộ nhớ và thời gian so với PT vanilla và các biến thể của nó, mà không thay đổi kích thước tham số có thể huấn luyện. Thông qua các thí nghiệm rộng rãi trên 23 nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác-ngôn ngữ (VL), chúng tôi chứng minh rằng DEPT vượt trội so với các phương pháp PEFT tiên tiến nhất, bao gồm cả baseline tinh chỉnh đầy đủ, trong một số kịch bản. Ngoài ra, chúng tôi chứng minh thực nghiệm rằng DEPT trở nên hiệu quả hơn khi kích thước mô hình tăng. Nghiên cứu thêm của chúng tôi tiết lộ rằng DEPT tích hợp liền mạch với học chuyển giao hiệu quả tham số trong setting học few-shot và nhấn mạnh khả năng thích ứng của nó với các kiến trúc và kích thước mô hình khác nhau.

1 GIỚI THIỆU
Mô hình Được Huấn luyện Trước Có thể Huấn luyện
!
!
!
!
!
!Nhúng Văn bản Đầu vào Mô hình Được Huấn luyện Trước Đóng băng Văn bản Đầu vào
❄
❄
❄
❄
❄
❄Prompt Văn bản
❄
❄
❄(a) Tinh chỉnh (c) Kỹ thuật Prompt Mô hình Được Huấn luyện Trước Đóng băng
!
!Văn bản Đầu vào
❄
❄
❄
❄
❄
❄
!Prompt Mềm (b) Điều chỉnh Prompt

Hình 1: Tổng quan về Tinh chỉnh (FT), Điều chỉnh Prompt (PT), và Kỹ thuật Prompt. PT tăng độ dài của chuỗi đầu vào, dẫn đến nhu cầu tính toán lớn hơn nhiều trong các giai đoạn huấn luyện và suy luận. Tinh chỉnh (FT) mô hình ngôn ngữ (LM) (Raffel et al., 2020; Touvron et al., 2023) trên các nhiệm vụ downstream mang lại cải thiện hiệu suất lớn trên các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau, nhưng nó đòi hỏi cập nhật và lưu trữ toàn bộ tham số của LM (xem Hình 1a), điều này đặc biệt tốn kém khi LM chứa hàng trăm triệu hoặc thậm chí hàng tỷ tham số. Kỹ thuật prompt (Brown et al., 2020) không cập nhật bất kỳ tham số nào trong khi thường khó thiết kế và có phương sai hiệu suất cao (Wang et al., 2023a) (xem Hình 1c). Do đó, các phương pháp tinh chỉnh hiệu quả tham số (PEFT) (Liu et al., 2022) đã thu hút sự quan tâm ngày càng tăng, nhằm học chỉ một số lượng nhỏ tham số cho mỗi nhiệm vụ trong khi duy trì mức hiệu suất tương đương với tinh chỉnh đầy đủ.

Điều chỉnh Prompt (PT) (Lester et al., 2021) đã nổi lên như một phương pháp PEFT hứa hẹn, thêm các vector prompt liên tục có thể huấn luyện vào đầu vào (xem Hình 1b). PT nổi bật so với các phương pháp PEFT khác vì nó duy trì hiệu suất cạnh tranh với ít tham số có thể huấn luyện hơn và không tăng mạnh các tham số có thể huấn luyện khi kích thước mô hình mở rộng. Các nghiên cứu gần đây cho thấy rằng phần lớn kiến thức của LM được thu thập trong giai đoạn huấn luyện trước (Zhou et al., 2023), và

--- TRANG 2 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
rằng học trong ngữ cảnh (ICL) chỉ với một vài ví dụ phong cách được thiết kế cẩn thận và một prompt hệ thống được thiết kế cẩn thận có thể đạt được kết quả căn chỉnh ấn tượng (Lin et al., 2023). Xét các tình huống mà các nhiệm vụ đã được LM hiểu phần nào và thách thức chính chỉ là tạo prompt LM một cách phù hợp, PT nổi lên như một lựa chọn có tiềm năng tốt hơn so với các phương pháp PEFT khác.

Mặc dù PT đã cho thấy kết quả hứa hẹn trên các nhiệm vụ và mô hình khác nhau, nó có hai hạn chế chính: (1) PT thường gặp phải tình trạng hội tụ chậm và nhạy cảm với việc khởi tạo (Lester et al., 2021; Vu et al., 2022; Wang et al., 2023b); và (2) PT mở rộng tổng chiều dài của chuỗi đầu vào, do đó làm trầm trọng thêm nhu cầu tính toán (tức là thời gian huấn luyện/suy luận và chi phí bộ nhớ), do độ phức tạp bậc hai của Transformer (Vaswani et al., 2017). Điều này được nhấn mạnh thêm do vấn đề hội tụ chậm. Các nghiên cứu gần đây (Su et al., 2022; Vu et al., 2022; Li et al., 2022) đã đề xuất các biến thể của PT vanilla để giải quyết vấn đề đầu tiên bằng cách ban đầu huấn luyện trước các prompt mềm trên nhiều nhiệm vụ nguồn khác nhau, được gọi là Học Chuyển giao Hiệu quả Tham số (PETL), như mô tả trong Hình 2a. Một số nghiên cứu (Asai et al., 2022; Wang et al., 2023b) cũng cải thiện hiệu suất của PT bằng cách huấn luyện chung các prompt đã học từ những nhiệm vụ nguồn này trên nhiều nhiệm vụ đích (được gọi là Học Đa nhiệm vụ). Tuy nhiên, vấn đề tăng tải tính toán do việc mở rộng độ dài chuỗi vẫn chưa được giải quyết. Mặc dù các phương pháp PETL có thể giảm các bước huấn luyện để mô hình hội tụ, mỗi bước tối ưu hóa vẫn tốn kém về mặt tính toán về thời gian và bộ nhớ. Quan trọng nhất, nó không tăng cường hiệu quả trong giai đoạn suy luận, điều này đặc biệt quan trọng trong kỷ nguyên của Mô hình Ngôn ngữ Lớn (LLM), xét rằng các mô hình đã huấn luyện có thể được truy vấn hàng triệu lần mỗi ngày.

(a) Khung Học Chuyển giao Hiệu quả Tham số
Mô hình Được Huấn luyện Trước
ŷNhiệm vụ Đích XXXXLMĐóng băng
ŷNhiệm vụ Nguồn A
PA
PA
PAXXXXLM Đóng băng
ŷNhiệm vụ Nguồn B
PB
PB
PB
Mô hình Được Huấn luyện Trước
ŷNhiệm vụ Nguồn N…Học Chuyển giao XXXXLMĐóng băng
ŷNhiệm vụ Đích 1
PA
PA
PAXXXXLM Đóng băng
ŷNhiệm vụ Đích 2
PB
PB
PB
Mô hình Được Huấn luyện Trước
ŷNhiệm vụ Đích M Học Đa nhiệm vụ Ngân hàng Prompt Nguồn Khởi tạo
@+(b) Điều chỉnh Prompt Phân tích (DePT) Phân tích Prompt Mềm Tương đương Cập nhật Kích thước Nhúng Từ Đóng băng
!
!
!
!
!
!
!
!
!
!
!
!
❄
❄
❄
❄
❄
❄
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
❄
❄
❄
❄
❄
❄
❄
❄
❄
❄
❄
❄
Nhúng Từ Đầu vào Đóng băng Prompt Mềm Có thể Huấn luyện Prompt Mềm
Ma trận Thấp hạng

Hình 2: Tổng quan về khung PETL (Trên) và phương pháp của chúng tôi DEPT (Dưới). DEPT phân tích một prompt mềm có thể huấn luyện của PT vanilla thành một prompt mềm ngắn hơn và một cặp ma trận thấp hạng, trong đó phép nhân của các ma trận thấp hạng phục vụ để cập nhật nhúng từ đóng băng.

Trong nghiên cứu này, chúng tôi đề xuất Điều chỉnh Prompt Phân tích (DEPT), phân tích một prompt mềm có thể huấn luyện thành một prompt mềm ngắn hơn và một cặp ma trận thấp hạng, trong đó phép nhân của các ma trận thấp hạng sau đó được thêm theo từng phần tử vào nhúng từ đóng băng, như được hiển thị trong Hình 2b (§2.2). Prompt mềm ngắn hơn này và ma trận nhúng từ được cập nhật sau đó được tối ưu hóa bằng hai tốc độ học khác nhau - một bước quan trọng cho việc hội tụ mô hình (§3.4). Trực giác của thiết kế này là cho phép cập nhật biểu diễn trong nhúng từ đóng băng, từ đó tăng khả năng thích ứng của các biểu diễn đầu vào mà trước đây không có sẵn. Kết quả thực nghiệm trên 23 nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác-ngôn ngữ (VL) chứng minh DEPT vượt trội so với các phương pháp PEFT tiên tiến nhất, bao gồm cả baseline tinh chỉnh đầy đủ trong một số kịch bản nhất định (§3.2). Nghiên cứu của chúng tôi chứng minh thực nghiệm rằng DEPT cải thiện đáng kể hiệu quả huấn luyện trên các kiến trúc và kích thước mô hình khác nhau, tiết kiệm hơn 20% (sử dụng T5-BASE) trong cả thời gian huấn luyện và chi phí bộ nhớ so với PT vanilla. Quan trọng là, DEPT trở nên hiệu quả hơn khi kích thước mô hình tăng, làm cho nó đặc biệt có lợi và phù hợp với LLM (§3.3). Hơn nữa, phân tích bổ sung của chúng tôi trong setting học few-shot tiết lộ khả năng tương thích của DEPT với các phương pháp PETL (§3.4).

Tóm lại, các đóng góp chính của bài báo này như sau:
• Chúng tôi đề xuất phương pháp DEPT, giải quyết một hạn chế hiệu quả chính của Điều chỉnh Prompt bằng cách phân tích prompt mềm của nó để giảm độ dài chuỗi đầu vào. DEPT cải thiện đáng kể hiệu quả huấn luyện và suy luận, về cả chi phí thời gian và bộ nhớ;
• Đánh giá toàn diện của chúng tôi trên 23 nhiệm vụ NLP và VL chứng minh rằng DEPT vượt trội so với các phương pháp PEFT tiên tiến nhất, bao gồm cả tinh chỉnh đầy đủ trong một số kịch bản.

--- TRANG 3 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
Ngoài ra, các thí nghiệm của chúng tôi cho thấy rằng DEPT tích hợp trơn tru với các phương pháp PETL và lợi thế của DEPT vẫn tồn tại trong setting học few-shot;
• Chúng tôi chứng minh thực nghiệm rằng DEPT trở nên hiệu quả hơn khi kích thước mô hình tăng, làm cho nó đặc biệt phù hợp với LLM. Hơn nữa, DEPT trực giao với các phương pháp PEFT khác nhau (tức là Adapter, LoRA) và có thể dễ dàng kết hợp với nhau.

2 PHƯƠNG PHÁP
Trong phần này, chúng tôi trước tiên xem xét lại nền tảng của Điều chỉnh Prompt (PT) trong §2.1 và sau đó giới thiệu phương pháp đề xuất của chúng tôi, Điều chỉnh Prompt Phân tích (DEPT) trong §2.2.

2.1 NỀN TẢNG: ĐIỀU CHỈNH PROMPT (PT)
Gọi L≜{xi,yi}Ni=1 là N dữ liệu huấn luyện có nhãn cho nhiệm vụ đích T. Cho một mô hình backbone được tham số hóa bởi Θ, mỗi văn bản đầu vào xi được ánh xạ thành một chuỗi nhúng từ Wi∈Rs×d, trong đó s và d đại diện cho độ dài chuỗi tối đa và chiều của nhúng từ. PT thêm một ma trận prompt có thể huấn luyện P∈Rl×d vào ma trận nhúng từ đóng băng Wi, trong đó l là một siêu tham số cho số lượng token ảo. Prompt mềm P có thể được khởi tạo ngẫu nhiên hoặc bằng cách lấy mẫu nhúng từ từ từ vựng. Do đó, đầu vào của mô hình trở thành ma trận kết hợp [P;Wi]∈R(l+s)×d. Hàm mất mát mục tiêu được công thức hóa như sau:

LPT=−∑i log P(yi|[P,Wi] ; Θ), (1)

trong đó hàm mất mát chỉ được tối ưu hóa đối với ma trận prompt mềm P.

2.2 PHƯƠNG PHÁP CỦA CHÚNG TÔI: ĐIỀU CHỈNH PROMPT PHÂN TÍCH (DEPT)
Việc phân tích prompt mềm. DEPT khác với phương pháp PT vanilla ở khía cạnh đầu vào. Như được hiển thị trong Hình 2b, chúng tôi phân tích ma trận prompt có thể huấn luyện P∈Rl×d từ PT vanilla thành hai thành phần: (1) một ma trận prompt có thể huấn luyện ngắn hơn Ps∈Rm×d; và (2) một cặp ma trận thấp hạng, A∈Rs×r và B∈Rr×d, trong đó thông thường hạng của các ma trận r≪min(s, d).

Thành phần đầu tiên, ma trận prompt có thể huấn luyện nhỏ hơn, được thêm vào ma trận nhúng từ theo cách tương tự như trong PT vanilla. Thành phần thứ hai sử dụng phép nhân của hai ma trận thấp hạng để biểu diễn việc cập nhật nhúng từ thông qua tổng theo tọa độ:

W'i=Wi+ ∆Wi=Wi+BA∈Rs×d, (2)

trong đó Wi được đóng băng và không nhận cập nhật gradient trong quá trình huấn luyện, trong khi A và B có thể huấn luyện. Theo Hu et al. (2021), chúng tôi sử dụng khởi tạo Gaussian ngẫu nhiên cho A và zero cho B, do đó ∆W=BA bằng zero khi bắt đầu huấn luyện. Hàm mất mát sau đó được tối ưu hóa như sau:

LDEPT=−∑i log P(yi|[Ps,W'i] ; Θ) (3)

Trong thí nghiệm của chúng tôi, chúng tôi chọn các giá trị của m và r để thỏa mãn phương trình l×d=m×d+(s+d)×r để duy trì kích thước chính xác của các tham số có thể huấn luyện như trong PT vanilla. Do đó, m luôn nhỏ hơn l khi r >0. Thiết kế này cải thiện hiệu quả bộ nhớ và giảm chi phí tính toán so với PT vanilla, vì độ dài chuỗi đầu vào ngắn hơn (tức là m+s < l +s) giảm đáng kể tính toán do độ phức tạp bậc hai của Transformer (Vaswani et al., 2017).

Hai tốc độ học. DEPT cũng khác với PT vanilla trong huấn luyện. Chúng tôi huấn luyện ma trận prompt có thể huấn luyện ngắn hơn, Ps, với tốc độ học α1 và cặp ma trận thấp hạng, A và B, với tốc độ học α2, thay vì sử dụng một tốc độ học duy nhất như trong PT vanilla. α1 thường lớn hơn nhiều so với α2. Chúng tôi sẽ xác thực thực nghiệm tầm quan trọng của lựa chọn này trong §3.4.

Tuy nhiên, DEPT có thể đưa ra chi phí huấn luyện bổ sung cho tối ưu hóa siêu tham số (xem §5).

3 THÍ NGHIỆM VÀ KẾT QUẢ
Trong phần này, chúng tôi giới thiệu thiết lập thí nghiệm của chúng tôi (xem §3.1), đánh giá hiệu suất của DEPT trên 23 nhiệm vụ NLP và VL khác nhau (xem §3.2), và đánh giá thời gian huấn luyện/suy luận tương đối và chi phí bộ nhớ của DEPT (xem §3.3), và khám phá hiệu quả của DEPT trong setting học few-shot và tầm quan trọng của hai tốc độ học khác nhau để huấn luyện DEPT (xem §3.4).

--- TRANG 4 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
3.1 THIẾT LẬP THÍ NGHIỆM
Bộ dữ liệu và nhiệm vụ. Chúng tôi đánh giá phương pháp đề xuất DEPT trên 21 nhiệm vụ NLP và 2 nhiệm vụ thị giác-ngôn ngữ. Đối với các nhiệm vụ NLP, chúng tôi theo các nghiên cứu trước (Vu et al., 2022; Sung et al., 2022b; Asai et al., 2022; Wang et al., 2023b) và sử dụng các bộ dữ liệu khác nhau có nguồn từ: (1) benchmark GLUE (Wang et al., 2018), bao gồm MNLI (Williams et al., 2018), QQP1, QNLI (Rajpurkar et al., 2016), SST-2 (Socher et al., 2013), STS-B (Cer et al., 2017), MRPC (Dolan & Brockett, 2005), RTE (Giampiccolo et al., 2007) và CoLA (Warstadt et al., 2019); (2) benchmark SuperGLUE (Wang et al., 2019), bao gồm MultiRC (Khashabi et al., 2018), BoolQ (Clark et al., 2019), WiC (Pilehvar & Camacho-Collados, 2019), WSC (Levesque et al., 2012), và CB (De Marneffe et al., 2019); (3) MRQA 2019 Shared Task (Fisch et al., 2019), bao gồm Natural Questions (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), SearchQA (Dunn et al., 2017) và NewsQA (Trischler et al., 2017); (4) các bộ dữ liệu khác, bao gồm WinoGrande (Sakaguchi et al., 2021), Yelp-2 (Zhang et al., 2015), SciTail (Khot et al., 2018) và PAWS-Wiki (Zhang et al., 2019). Đối với các nhiệm vụ thị giác-ngôn ngữ, chúng tôi theo các nghiên cứu trước (Sung et al., 2022a;b) để thí nghiệm với nhiệm vụ trả lời câu hỏi thị giác, VQA (Goyal et al., 2017), và nhiệm vụ tạo chú thích hình ảnh, MSCOCO (Chen et al., 2015).

Baseline. Chúng tôi so sánh DEPT với nhiều baseline khác nhau: (1) tinh chỉnh (FT), trong đó tất cả các tham số mô hình được điều chỉnh trong quá trình thích ứng trên từng nhiệm vụ downstream; (2) PT vanilla (Lester et al., 2021), trong đó các vector prompt đích được khởi tạo bằng các từ vựng hàng đầu được lấy mẫu ngẫu nhiên, và các biến thể của nó sử dụng học chuyển giao và đa nhiệm vụ bổ sung, bao gồm SPoT (Vu et al., 2022), ATTEMPT (Asai et al., 2022), và MPT (Wang et al., 2023b); (3) các phương pháp PEFT tiên tiến bao gồm Adapters (Houlsby et al., 2019), AdapterDrop (Rückle et al., 2021), BitFit (Ben Zaken et al., 2022), HyperFomer (Karimi Mahabadi et al., 2021), HyperDecoder (Ivison & Peters, 2022), P-tuning (Liu et al., 2021), LoRA (Hu et al., 2021), LST (Sung et al., 2022b), và các biến thể học đa nhiệm vụ của chúng. Để so sánh công bằng, chúng tôi trực tiếp trích dẫn các số liệu hiệu suất từ các bài báo đã xuất bản (Mahabadi et al., 2021; Karimi Mahabadi et al., 2021; Asai et al., 2022; Wang et al., 2023b; Sung et al., 2022b) để so sánh công bằng, trong đó tất cả các baseline này sử dụng T5-BASE làm backbone và tuân thủ các split huấn luyện, xác thực và kiểm tra được sử dụng bởi Karimi Mahabadi et al. (2021); Mahabadi et al. (2021) cho các nhiệm vụ NLP và bởi Sung et al. (2022b) cho các nhiệm vụ thị giác-ngôn ngữ.

Chi tiết triển khai. Trong nghiên cứu của chúng tôi, chúng tôi chủ yếu thí nghiệm sử dụng mô hình T5-BASE với 220M tham số (Raffel et al., 2020). Chúng tôi nhất quán đặt số token ảo l là 100 trên tất cả các nhiệm vụ cho PT vanilla và điều chỉnh các siêu tham số của DEPT tương ứng để duy trì số lượng tham số có thể huấn luyện tương đương. Ví dụ, PT vanilla chứa l×d tham số có thể huấn luyện trong đó kích thước ẩn d là 768 cho T5-BASE, và DEPT có thể cấu hình số token ảo m là 40 và hạng của các ma trận thấp r là 45, dẫn đến m×d+(s+d)×r tham số có thể huấn luyện. Điều này mang lại tổng cộng 76,800 tham số có thể huấn luyện, phù hợp với PT vanilla. Đối với các nhiệm vụ VL, chúng tôi sử dụng kiến trúc CLIP-T5 kết hợp CLIP (Radford et al., 2021) và T5-BASE (Raffel et al., 2020), với CLIP được đóng băng. Chúng tôi theo nghiên cứu trước (Sung et al., 2022b) để kết nối biểu diễn thị giác từ CLIP với nhúng văn bản từ T5-BASE, trong đó một lớp chiếu thị giác có thể huấn luyện được sử dụng giữa CLIP và T5 để căn chỉnh biểu diễn thị giác với cùng chiều như nhúng văn bản.

Chúng tôi cũng mở rộng đánh giá của mình để bao gồm các mô hình T5-SMALL (60M), T5-LARGE (770M), GPT2-SMALL (110M), GPT2-MEDIUM (345M), và GPT2-LARGE (774M). Trong các thí nghiệm few-shot, chúng tôi chọn ngẫu nhiên k ví dụ ba lần từ tập huấn luyện và báo cáo giá trị trung bình và độ lệch chuẩn cho mỗi thí nghiệm k-shot. Theo các nghiên cứu trước trong PETL cho PT (Vu et al., 2022; Su et al., 2022; Asai et al., 2022), chúng tôi sử dụng MNLI, QQP, SST-2, SQUAD (Rajpurkar et al., 2016), và ReCoRD (Zhang et al., 2018) như năm nhiệm vụ nguồn. Prompt mềm và các cặp ma trận thấp hạng của chúng tôi được khởi tạo từ các prompt mềm được suy ra từ một trong những nhiệm vụ nguồn được chọn này. Vui lòng xem thêm chi tiết siêu tham số và triển khai trong Phụ lục §D.

3.2 KẾT QUẢ CHÍNH
Phần này hiển thị bằng chứng thực nghiệm hỗ trợ hiệu quả của phương pháp đề xuất DEPT trên 23 nhiệm vụ NLP và VL. Bảng 1, 2, và 3 trình bày kết quả thí nghiệm của chúng tôi trên các benchmark GLUE và SuperGLUE, MRQA 2019 Shared Task và bốn bộ dữ liệu NLP khác, cũng như hai nhiệm vụ VL. Ngoài ra, chúng tôi trực quan hóa hiệu suất mô hình so với số lượng tham số có thể huấn luyện.

--- TRANG 5 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
Bảng 1: Kết quả kiểm tra trên các benchmark GLUE và SuperGLUE, với kích thước tương ứng của các tham số có thể huấn luyện. Tất cả kết quả đều dựa trên các mô hình T5-BASE. Chúng tôi sử dụng tương quan Pearson cho STS-B, F1 cho MultiRC (Multi), và độ chính xác cho các nhiệm vụ khác làm số liệu đánh giá.

Phương pháp #Para GLUE SuperGLUE
MNLI QQP QNLI SST-2 STS-B MRPC RTE CoLA Mean Multi Bool WiC WSC CB Mean
Học Một nhiệm vụ
Tinh chỉnh 220M 86.8 91.6 93.0 94.6 89.7 90.2 71.9 61.8 84.9 72.8 81.1 70.2 59.6 85.7 73.9
Adapter 11.9M 86.5 90.2 93.2 93.8 90.7 85.3 71.9 64.0 84.5 75.9 82.5 67.1 67.3 85.7 75.7
AdapterDrop 11.1M 86.3 90.2 93.2 93.6 91.4 86.3 71.2 62.7 84.4 72.9 82.3 68.3 67.3 85.7 75.3
BitFit 280k 85.3 90.1 93.0 94.2 90.9 86.8 67.6 58.2 83.3 74.5 79.6 70.0 59.6 78.6 72.5
LoRA 3.8M 86.3 89.0 93.2 94.3 90.9 90.1 75.5 63.3 85.3 72.6 81.3 68.3 67.3 92.9 76.5
LST 3.8M 85.6 88.8 93.3 94.1 90.7 90.4 71.9 58.1 84.1 – – – – – –
PT 76.8k 83.4 90.2 93.1 91.9 90.2 90.1 78.8 60.7 84.8 65.7 63.7 50.8 51.9 67.9 60.0
DEPT (chúng tôi) 76.8k 85.0 90.4 93.2 94.2 90.8 90.7 79.1 63.8 85.9 74.3 79.3 68.7 67.3 92.9 76.5

Học Đa nhiệm vụ
Tinh chỉnh (m) 28M 85.7 91.1 92.0 92.5 88.8 90.2 75.4 54.9 83.8 74.4 81.1 70.0 71.2 85.7 76.1
Adapter (m) 11.8M 86.3 90.5 93.2 93.0 89.9 90.2 70.3 61.5 84.4 72.6 82.3 66.5 67.3 89.3 75.6
HyperFormer (m) 638k 85.7 90.0 93.0 94.0 89.7 87.2 75.4 63.7 84.8 72.9 82.5 69.0 67.3 85.7 75.4
HyperDecoder (m) 11.8M 86.0 90.5 93.4 94.0 90.5 87.7 71.7 55.9 83.7 70.4 78.8 67.1 61.5 82.1 72.0

Huấn luyện Một nhiệm vụ + Học Chuyển giao
SPoT 76.8k 85.4 90.1 93.0 93.4 90.0 79.7 69.8 57.1 82.3 74.0 77.2 67.0 50.0 46.4 62.9
ATTEMPT 232k 84.3 90.3 93.0 93.2 89.7 85.7 73.4 57.4 83.4 74.4 78.8 66.8 53.8 78.6 70.5
MPT 77.6k 85.9 90.3 93.1 93.8 90.4 89.1 79.4 62.4 85.6 74.8 79.6 69.0 67.3 79.8 74.1

Học Đa nhiệm vụ + Học Chuyển giao
ATTEMPT (m) 396k∗ 83.8 90.0 93.1 93.7 90.8 86.1 79.9 64.3 85.2 74.4 78.5 66.5 69.2 82.1 74.1
MPT(m) 310.5k∗ 84.3 90.0 93.0 93.3 90.4 89.2 82.7 63.5 85.8 74.8 79.2 70.2 67.3 89.3 76.1

cho GLUE và SuperGLUE trong Hình 6 của Phụ lục §A. Hơn nữa, chúng tôi đánh giá hiệu suất của DEPT bằng LLAMA-2 (Touvron et al., 2023) trong Phụ lục §B. Kết quả thí nghiệm tiết lộ ba phát hiện chính: (1) DEPT nhất quán vượt trội so với PT vanilla và các biến thể PETL của nó; (2) DEPT đạt được hiệu suất cạnh tranh hoặc thậm chí tốt hơn so với các phương pháp PEFT tiên tiến nhất trong khi sử dụng ít tham số có thể huấn luyện hơn; và (3) DEPT thiếu sót trong một số nhiệm vụ nhất định. Dưới đây chúng tôi đi sâu hơn về các nhiệm vụ khác nhau.

#1. Hiệu suất trên các benchmark GLUE và SuperGLUE. Như được hiển thị trong Bảng 1, kết quả thí nghiệm của chúng tôi cho thấy rằng DEPT vượt trội so với các phương pháp PEFT tiên tiến nhất, như Adapter, LoRA và LST trên các benchmark GLUE và SuperGLUE, trong khi sử dụng ít tham số có thể huấn luyện hơn. Đáng chú ý, DEPT cũng vượt trội so với baseline tinh chỉnh đầy đủ trên cả hai benchmark. Ngoài ra, DEPT vượt trội so với PT vanilla và tất cả các biến thể của PT có đưa thêm học chuyển giao và học đa nhiệm vụ. Ví dụ, ATTEMPT, đòi hỏi huấn luyện bổ sung cho prompt mềm trên các nhiệm vụ nguồn, đạt được điểm trung bình 83.4 trên benchmark GLUE và 70.5 trên benchmark SuperGLUE. Trong khi đó, DEPT vượt trội so với ATTEMPT với điểm số 85.9 và 76.5 trên GLUE và SuperGLUE, mặc dù huấn luyện ít tham số hơn. Tương tự, DEPT vượt trội so với MPT với 0.1% trên benchmark GLUE và 0.4% trên benchmark SuperGLUE, mà không sử dụng học chuyển giao hoặc học đa nhiệm vụ bổ sung. Những kết quả này được đạt với thời gian suy luận ít hơn và tài nguyên bộ nhớ giảm (tham khảo §3.3 để biết chi tiết), điều này xác nhận hiệu quả của DEPT. Vì PT thường hoạt động kém trong các tình huống có dữ liệu có nhãn hạn chế (Gu et al., 2022), chúng tôi điều tra khả năng tương thích của DEPT và PETL sau này trong setting học few-shot (§3.4).

#2. Hiệu suất trên MRQA 2019 Shared Task và các bộ dữ liệu NLP khác. Bảng 2 trình bày hiệu suất của các phương pháp PEFT khác nhau, bao gồm DEPT, trên MRQA 2019 Shared Task và bốn bộ dữ liệu khác. Chúng tôi quan sát thấy rằng DEPT cải thiện hiệu suất trung bình của PT vanilla với một mức tăng đáng kể là +3.6% trên MRQA và +14.2% trên các bộ dữ liệu khác. DEPT vượt trội so với hiệu suất của các biến thể PT tận dụng học chuyển giao và học đa nhiệm vụ bổ sung, mà không đưa thêm tham số có thể huấn luyện vào PT vanilla hoặc dựa vào bất kỳ phương pháp PETL nào. Mặc dù

--- TRANG 6 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
Bảng 2: Kết quả kiểm tra trên MRQA 2019 Shared Task và các bộ dữ liệu khác sử dụng mô hình T5-BASE. Chúng tôi báo cáo F1 cho các nhiệm vụ MRQA và độ chính xác cho các bộ dữ liệu khác trên ba seed, với độ lệch chuẩn ở dưới. Tất cả kết quả baseline được trích dẫn trực tiếp từ Wang et al. (2023b).

Phương pháp #Para MRQA Khác
NQ HP SQA News Mean WG Yelp SciTail PAWS Mean
Tinh chỉnh 220M 75.1 77.5 81.1 65.2 74.7 61.9 96.7 95.8 94.1 87.1
Adapters 1.9M 74.2 77.6 81.4 65.6 74.7 59.2 96.9 94.5 94.3 86.2
BitFit 280K 70.7 75.5 77.7 64.1 72.0 57.2 94.7 94.7 92.0 84.7
LoRA 3.8M 72.4 62.3 72.5 56.9 66.0 58.2 97.1 94.7 94.0 86.0
PT 76.8K 67.9 72.9 75.7 61.1 69.4 49.6 95.1 87.9 55.8 72.1
SPoT 76.8K 68.2 74.8 75.3 58.2 69.1 50.4 95.4 91.2 91.1 82.0
ATTEMPT 232K 70.4 75.2 77.3 62.8 71.4 57.6 96.7 93.1 92.1 84.9
MPT 77.6K 72.0₀.₁75.8₀.₁77.2₀.₁63.7₀.₁72.2 56.5₀.₉96.4₀.₀95.5₀.₁93.5₀.₁85.5
DEPT (chúng tôi) 76.8K 73.2₀.₁76.8₀.₃77.6₀.₂64.4₀.₁ 73.0 59.0₀.₂96.8₀.₁95.6₀.₂93.7₀.₁ 86.3

DEPT cải thiện so với PT vanilla và các biến thể của nó là hứa hẹn, vẫn còn sự chênh lệch về hiệu suất khi so sánh với baseline tinh chỉnh đầy đủ. Việc nghiên cứu các cách để kết hợp DEPT với các phương pháp PEFT khác, như LoRA và Adapter, có thể cung cấp một hướng nghiên cứu có giá trị cho việc thu hẹp khoảng cách hiệu suất này.

Bảng 3: Kết quả kiểm tra trên bộ dữ liệu VQA và MSCOCO sử dụng mô hình T5-BASE. Chúng tôi báo cáo kết quả trung bình trên ba seed, với độ lệch chuẩn ở dưới. Tất cả kết quả baseline được trích dẫn trực tiếp từ Sung et al. (2022b). Hiệu suất tốt nhất cho mỗi cột được làm nổi bật bằng màu xanh.

Phương pháp Tham số Được cập nhật VQA MSCOCO
(%) Karpathy test Karpathy test
Acc. (%) CIDEr
FT 100 67.1₀.₁ 112.2₀.₃
Adapters 7.98 67.1₀.₁ 111.8₀.₁
LoRA 7.54 63.7₀.₂ 110.3₀.₄
BitFit 0.83 55.1₀.₂ 101.2₀.₂
P-Tuning 1.26 47.4₀.₇ 96.1₀.₉
LST 7.46 66.5₀.₁ 113.5₀.₃
DEPT (chúng tôi) 0.74 59.8₀.₄ 113.7₀.₃

#3. Hiệu suất trên các nhiệm vụ Thị giác-Ngôn ngữ. Bảng 3 cung cấp tổng quan về hiệu suất của các phương pháp PEFT khác nhau trên hai nhiệm vụ VL, cụ thể là VQA và MS COCO Caption Generation. Kết quả cho thấy rằng DEPT, trong khi cập nhật ít tham số hơn nhiều, đạt được điểm CIDEr 113.7 trên nhiệm vụ MS COCO Caption Generation, vượt trội so với các phương pháp PEFT tiên tiến nhất. Điều này cho thấy hiệu quả của phương pháp đề xuất của chúng tôi. Tuy nhiên, mặc dù DEPT vượt trội so với các phương pháp như P-tuning và BitFit trên bộ dữ liệu VQA, nó vẫn chưa đạt được hiệu suất tinh chỉnh đầy đủ. Điều này cho thấy rằng trong một số nhiệm vụ nhất định, việc sử dụng số lượng tham số có thể huấn luyện lớn hơn có thể có lợi.

3.3 HIỆU QUẢ THỜI GIAN VÀ BỘ NHỚ
Phần này hiển thị bằng chứng thực nghiệm hỗ trợ hiệu quả của DEPT, trải rộng trên các kiến trúc mô hình đa dạng với quy mô khác nhau trên benchmark GLUE. Để đảm bảo so sánh công bằng, chúng tôi nhất quán giữ số lượng tham số có thể huấn luyện trong DEPT giống như trong PT vanilla (l= 100). Do đó, một khi chúng tôi chọn độ dài của prompt mềm m trong DEPT, hạng của các ma trận thấp hạng r trở nên được xác định. Trong các thí nghiệm của chúng tôi, chúng tôi chủ yếu so sánh DEPT với PT vanilla sử dụng 5 độ dài prompt mềm khác nhau m (tức là 0, 20, 40, 60, 80). Hình 3 và 4 mô tả hiệu suất GLUE trung bình của DEPT, cùng với thời gian huấn luyện/suy luận và chi phí bộ nhớ liên quan so với PT vanilla. Dưới đây chúng tôi thảo luận về hai phát hiện chính.

# 1. DEPT cải thiện hiệu quả thời gian và bộ nhớ đáng kể. Hình 3 trình bày hiệu suất trung bình của DEPT, liên quan đến thời gian huấn luyện và bộ nhớ trung bình, trên các benchmark GLUE, so với các độ dài prompt mềm m khác nhau. Thời gian huấn luyện trung bình và chi phí bộ nhớ được tính toán trên 8 nhiệm vụ trên benchmark GLUE và ba kích thước mô hình khác nhau. Cả mô hình encoder-decoder (T5) và decoder-only (GPT-2) đều được đánh giá trên ba kích thước mô hình khác nhau. Nghiên cứu tiết lộ rằng việc phân tích prompt mềm (l= 100) thành một prompt mềm nhỏ và các ma trận thấp hạng mang lại hiệu suất tương đương hoặc thậm chí tốt hơn trong khi tăng cường đáng kể hiệu quả huấn luyện và giảm việc sử dụng bộ nhớ. Cụ thể, sử dụng độ dài prompt mềm lớn hơn 20 trong DEPT với mô hình T5 dẫn đến hiệu suất trung bình tốt hơn trên benchmark GLUE so với PT vanilla, trong khi cải thiện hiệu quả huấn luyện và giảm việc sử dụng bộ nhớ khoảng 25%. Sự cải thiện này rõ rệt hơn (37% trên bộ dữ liệu SST-2) khi chúng tôi kiểm tra DEPT (với m= 60) sử dụng mô hình T5-3B (xem §B để biết chi tiết). Những quan sát tương tự cũng được tìm thấy khi sử dụng mô hình GPT, cho thấy khả năng thích ứng của DEPT cho các kiến trúc mô hình khác nhau. Đáng chú ý là DEPT có thể có sự sụt giảm hiệu suất đáng kể bất kể

--- TRANG 7 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
việc sử dụng T5 hay GPT-2, khi prompt mềm bị loại bỏ (m= 0) và mô hình chỉ phụ thuộc vào cặp ma trận thấp hạng.

# 2. DEPT trở nên hiệu quả hơn khi kích thước mô hình tăng. Hình 4 biểu diễn tốc độ suy luận, được đo bằng số lượng mẫu trung bình được đánh giá mỗi giây trên benchmark GLUE sử dụng một GPU RTX 3090 đơn. Thời gian suy luận được tính toán bằng Huggingface Trainer Class. Chúng tôi quan sát thấy rằng sự cải thiện tương đối về số lượng mẫu suy luận mỗi giây so với PT vanilla tăng lên khi kích thước mô hình tăng. Ví dụ, khi sử dụng mô hình T5-SMALL, PT vanilla đánh giá 167.3 mẫu mỗi giây, trong khi DEPT (m= 20) đánh giá 178.3 mẫu mỗi giây, dẫn đến tăng 6.5% tốc độ suy luận. Ngược lại, khi sử dụng T5-LARGE, PT vanilla đánh giá 21.0 mẫu mỗi giây và DEPT (m= 20) đánh giá 24.8 mẫu mỗi giây, dẫn đến tăng 18.1% tốc độ suy luận, tăng đáng kể từ 6.5% trước đó. Điều này cho thấy rằng DEPT đặc biệt có lợi và có thể áp dụng hơn trong bối cảnh LLM. Vui lòng tham khảo Phụ lục §B để biết tốc độ suy luận của DEPT và PT sử dụng T5-3B và LLAMA-2.

3.4 PHÂN TÍCH THÊM
Học Few-shot. PT vanilla thường hoạt động kém trong các nhiệm vụ học few-shot (Gu et al., 2022) do hạn chế đầu tiên được thảo luận trong §1. Để đánh giá hiệu suất của DEPT trong setting few-shot, chúng tôi sử dụng phương pháp học chuyển giao được lấy cảm hứng từ các nghiên cứu PETL gần đây, như được minh họa trong Hình 2a. Cụ thể, chúng tôi huấn luyện trước cả prompt mềm và cặp thấp hạng trên các nhiệm vụ nguồn và chọn checkpoint tốt nhất trước khi tiến hành nhiệm vụ đích. Theo các nghiên cứu trước (Karimi Mahabadi et al., 2021; Asai et al., 2022; Wang et al., 2023b), chúng tôi đánh giá hiệu quả của DEPT trên 14 nhiệm vụ NLP, với k ví dụ huấn luyện trong đó k = 4, 16, 32. Các phát hiện thí nghiệm của chúng tôi tiết lộ hai quan sát chính như sau: (1) DEPT tích hợp liền mạch với các phương pháp PETL; và (2) DEPT đạt được hiệu suất cạnh tranh hoặc thậm chí tốt hơn so với các phương pháp PEFT tiên tiến nhất trong setting học few-shot.

Bảng 4 so sánh hiệu quả của phương pháp đề xuất DEPT với các phương pháp PEFT khác nhau trong các thí nghiệm few-shot, bao gồm tinh chỉnh đầy đủ (FT), Adapters (AD), PT vanilla (PT), SPoT (ST),

--- TRANG 8 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
Bảng 4: Kết quả học few-shot với k={4, 16, 32} trên các bộ dữ liệu SuperGLUE BooQ, SuperGLUE CB và SciTail. Chúng tôi báo cáo kết quả trung bình trên ba seed, với độ lệch chuẩn ở dưới. Kết quả baseline được trích dẫn trực tiếp từ Wang et al. (2023b). Hiệu suất tốt nhất cho mỗi hàng được làm nổi bật bằng màu xanh.

Nhiệm vụ k-shot FT AD PT ST HF (IA)³ ATP MPT DEPT
#Para 220M 1.9M 76.8K 76.8K 638K 55.3K 232K 77.6K 76.8K
BoolQ 4 50.5 53.4 61.6 50.5 48.0 56.7 61.8 62.2 62.7₅.₄
16 56.5 51.4 61.9 50.6 50.2 62.0 60.0 63.3 66.9₄.₄
32 58.4 54.5 61.7 61.2 58.3 67.2 65.3 68.9 67.2₃.₄
CB 4 57.7 51.1 53.5 71.4 60.7 65.5 67.9 73.6 75.0₅.₁
16 77.0 74.8 63.5 64.3 76.3 71.4 71.4 78.6 78.6₄.₃
32 80.0 74.8 67.8 64.3 81.4 75.0 78.5 82.1 82.1₂.₃
SciTail 4 79.6 79.5 57.7 69.6 82.0 65.4 80.2 80.2 78.1₂.₅
16 80.0 83.2 60.8 71.9 86.5 74.4 79.5 87.3 78.5₁.₄
32 81.9 85.0 60.2 71.9 85.8 80.4 80.2 86.3 85.4₃.₁

Bảng 5: Kết quả học few-shot với k={4, 16, 32} trên các benchmark GLUE và SuperGLUE. Chúng tôi báo cáo kết quả trung bình trên ba seed, với độ lệch chuẩn ở dưới. Kết quả baseline được trích dẫn trực tiếp từ Wang et al. (2023b).

k-shot Phương pháp GLUE SuperGLUE
MNLI QQP QNLI SST-2 STS-B MRPC RTE CoLA Avg. Multi BoolQ WiC WSC CB Avg.
4 PT 40.1 63.2 40.4 53.0 88.8 68.1 56.3 27.4 54.7 61.8 61.6 51.2 60.4 53.5 57.7
MPT 59.4 82.0 86.2 56.5 89.1 68.1 62.6 34.8 67.3 62.2 62.2 52.9 67.3 73.6 63.6
DEPT 44.0₁.₁77.4₆.₇85.8₄.₄59.3₃.₁84.1₂.₇73.5₂.₈63.5₂.₈29.3₂.₃64.6 62.3₁.₃62.7₅.₄57.5₁.₁67.9₀.₉75.0₅.₁65.1

16 PT 41.5 62.3 59.9 50.9 87.8 68.1 54.7 28.5 56.7 60.3 61.9 48.9 44.2 63.5 55.8
MPT 61.6 84.7 90.6 63.2 89.1 70.1 64.8 32.1 69.5 64.5 63.3 49.8 67.3 78.6 64.7
DEPT 61.8₂.₅80.3₁.₃91.2₀.₅77.6₆.₃87.1₁.₇78.1₂.₃71.9₁.₀27.1₁.₇71.9 60.6₂.₈66.9₄.₄59.6₀.₇57.7₂.₇78.6₄.₃64.7

32 PT 37.0 62.3 56.7 50.9 87.5 68.1 54.7 23.2 55.1 59.2 61.7 52.6 67.3 67.8 61.7
MPT 63.6 88.5 91.0 75.9 89.7 74.5 59.7 30.8 71.7 63.3 68.9 53.9 67.3 82.1 67.1
DEPT 63.3₃.₅80.1₀.₇91.3₀.₅80.4₈.₇89.2₀.₁81.4₃.₃72.7₂.₉28.6₂.₁73.4 60.1₂.₇67.2₃.₄58.0₀.₇63.1₃.₆82.1₂.₃66.4

HyperFormer (HF), (IA)³, ATTEMPT (ATP), và MPT trên các bộ dữ liệu BoolQ, CB, và SciTail. Bảng 5 trình bày hiệu suất của DEPT so với PT vanilla và MPT trên benchmark GLUE và SuperGLUE. Kết quả thí nghiệm cho thấy rằng PT vanilla gặp khó khăn với các nhiệm vụ few-shot, cho thấy tầm quan trọng của PETL cho PT trong các nhiệm vụ học few-shot như được gợi ý trong các nghiên cứu trước (Vu et al., 2022; Su et al., 2022). Tuy nhiên, hiệu suất của DEPT hưởng lợi lớn từ khung PETL (xem Hình 2a). Ví dụ, trong khi PT vanilla đạt được độ chính xác 53.5% trên bộ dữ liệu SuperGLUE CB và 57.7% trên bộ dữ liệu SciTail khi k=4, DEPT với PETL đạt được độ chính xác 75.0% trên bộ dữ liệu SuperGLUE CB và 78.1% trên bộ dữ liệu SciTail, cho cùng giá trị k. Kết quả này hỗ trợ quan sát đầu tiên của chúng tôi về khả năng tương thích của DEPT và các phương pháp PETL. Hơn nữa, DEPT với học chuyển giao đạt được hiệu suất tương đương với biến thể của PT, MPT trên 14 nhiệm vụ NLP. Đáng chú ý, DEPT vượt trội so với hiệu suất của tất cả các biến thể khác của PT (tức là SPoT, ATTEMPT) và các phương pháp PEFT khác, chứng minh hiệu quả của phương pháp và ủng hộ quan sát thứ hai của chúng tôi.

Tầm quan trọng của các tốc độ học khác nhau. Hình 5 trình bày kết quả thí nghiệm từ 3 cài đặt tốc độ học khác nhau để huấn luyện prompt mềm và cặp ma trận thấp hạng như sau: (1) sử dụng tốc độ học duy nhất 3e-1; (2) sử dụng tốc độ học duy nhất 5e-4; (3) áp dụng tốc độ học hỗn hợp (với tìm kiếm lưới), trong đó prompt mềm được huấn luyện với tốc độ lớn hơn và cặp ma trận thấp hạng được huấn luyện với tốc độ thấp hơn. Trong các thí nghiệm của chúng tôi, lựa chọn đầu tiên đạt được hiệu suất trung bình 40.8 trên benchmark GLUE. Lựa chọn thứ hai thể hiện hiệu suất trung bình 54.7, trong khi lựa chọn thứ ba chứng minh hiệu suất trung bình được cải thiện lớn là 85.7 trên benchmark GLUE. Điều này cho thấy tầm quan trọng của việc huấn luyện DEPT với hai tốc độ học khác nhau.

Hình 5: Kết quả kiểm tra trên benchmark GLUE sử dụng T5-BASE, cho thấy tầm quan trọng của việc huấn luyện DEPT với các tốc độ học khác nhau.

4 NGHIÊN CỨU LIÊN QUAN
Tinh chỉnh hiệu quả tham số. Trái ngược với tinh chỉnh tiêu chuẩn và tinh chỉnh dựa trên prompt (Devlin et al., 2019; Schick & Schütze, 2021; Shi & Lipani, 2023) trong đó các tham số đầy đủ được cập nhật, các phương pháp tinh chỉnh hiệu quả tham số (PEFT) đã chứng minh hiệu suất đáng kể trên nhiều nhiệm vụ (Wang et al., 2018; Shi et al., 2022; Wu et al., 2023a; Hendriksen et al., 2022; Wu et al., 2023b; Yang et al., 2023) trong khi chỉ cập nhật một số lượng hạn chế tham số. Adapters (Houlsby et al., 2019), cùng với các biến thể của nó, HyperFormer (Karimi Mahabadi et al., 2021) và Compacter (Mahabadi et al., 2021), thêm các mô-đun có thể huấn luyện mới (adapter) vào mỗi khối transformer của mô hình T5 (Raffel et al., 2020). BitFit (Ben Zaken et al., 2022) giới hạn cập nhật chỉ cho các tham số bias, trong khi phương pháp này có xu hướng hoạt động kém trên các mạng lớn hơn (Lialin et al., 2023). Prefix-tuning (Li & Liang, 2021) thêm một prompt mềm, được tham số hóa bởi một mạng feed-forward, vào đầu vào mô hình. Diff pruning (Guo et al., 2021) học một cập nhật thưa của trọng số mạng nơ-ron với chi phí sử dụng bộ nhớ nhiều hơn. FishMask (Sung et al., 2021) cũng thực hiện cập nhật thưa, nhưng nó tốn kém về mặt tính toán và không hiệu quả trên phần cứng học sâu hiện đại (Lialin et al., 2023). LoRA (Hu et al., 2021; Yang et al., 2024) sử dụng phân tích ma trận thấp hạng đơn giản để tham số hóa việc cập nhật trọng số. (IA)³ (Liu et al., 2022) chia tỷ lệ các kích hoạt bằng các vector đã học cho học few-shot. LST (Sung et al., 2022b) vận hành một mạng transformer nhỏ bên cạnh mạng được huấn luyện trước, nhằm giảm bộ nhớ huấn luyện. Điều chỉnh Prompt (PT) (Lester et al., 2021) thêm một prompt mềm có thể huấn luyện vào các nhúng đầu vào mô hình. So với các phương pháp PEFT được đề cập ở trên, PT sử dụng ít tham số có thể huấn luyện hơn, không tăng lên khi kích thước mô hình mở rộng. Mao et al. (2022) giới thiệu một phương pháp kết hợp Prefix-tuning, Adapters, và LoRA thông qua cơ chế gating. DEPT cũng có thể áp dụng cho phương pháp này và có thể dễ dàng tích hợp với các phương pháp PEFT khác.

Học chuyển giao cho PT. Các nghiên cứu gần đây nhằm tăng cường hiệu suất của PT thông qua PETL. PPT (Gu et al., 2022) cố gắng cải thiện hiệu suất của PT (Lester et al., 2021) bằng cách huấn luyện trước thêm (Gururangan et al., 2020; Shi et al., 2023), đòi hỏi một tập hợp thiết kế cụ thể cho nhiệm vụ được làm thủ công và chi phí tính toán đáng kể. Su et al. (2022) cải thiện PT thông qua chuyển giao prompt qua các nhiệm vụ và mô hình khác nhau. SPoT (Vu et al., 2022) áp dụng một prompt duy nhất, được chọn dựa trên một thước đo độ tương tự với chi phí tìm kiếm lớn. ATTEMPT (Asai et al., 2022) sử dụng cơ chế attention trên các prompt nguồn để khởi tạo prompt cho các nhiệm vụ đích với chi phí tham số bổ sung. MPT (Wang et al., 2023b) áp dụng một prompt mềm được chia sẻ qua các nhiệm vụ khác nhau, trong khi hiệu quả của nó cho một phạm vi rộng các nhiệm vụ nguồn vẫn chưa được kiểm tra. Chúng tôi thấy rằng PETL cho PT (Asai et al., 2022; Wang et al., 2023b) có thể tăng tốc hiệu quả việc hội tụ huấn luyện, và rằng PETL cho PT hữu ích hơn để cải thiện hiệu suất mô hình trong setting học few-shot cho PT (Gu et al., 2022; Wu et al., 2022). Tuy nhiên, khi có sẵn các bộ dữ liệu có nhãn rộng rãi, việc huấn luyện PT hoặc DEPT cho các bước bổ sung thường dẫn đến cải thiện hiệu suất.

5 KẾT LUẬN
Kết luận. Trong nghiên cứu này, chúng tôi đề xuất Điều chỉnh Prompt Phân tích (DEPT), cải thiện đáng kể hiệu quả của PT vanilla về thời gian và bộ nhớ trong khi mang lại hiệu suất cạnh tranh hoặc thậm chí vượt trội so với các phương pháp PEFT tiên tiến nhất. Đáng chú ý, hiệu quả của DEPT tăng lên với kích thước mô hình tăng, làm cho nó đặc biệt phù hợp với LLM. Phân tích thêm của chúng tôi cho thấy khả năng tương thích của DEPT với các phương pháp PETL và làm nổi bật tính linh hoạt của nó trên các kiến trúc và quy mô mô hình đa dạng.

Hạn chế và Nghiên cứu tương lai. Chúng tôi nêu ra một số hạn chế trong công việc của chúng tôi: (1) hạn chế chính của DEPT là việc đưa ra các siêu tham số bổ sung để điều chỉnh, ví dụ như tốc độ học của các ma trận thấp hạng. Điều này có thể đưa ra một số chi phí tính toán bổ sung trong giai đoạn tối ưu hóa siêu tham số của việc huấn luyện mô hình. Trong nghiên cứu của chúng tôi, chúng tôi huấn luyện DEPT lên đến 300k bước (trong setting dữ liệu phong phú) theo (Vu et al., 2022) với việc tìm kiếm cẩn thận cho tốc độ học tối ưu, có thể tăng chi phí huấn luyện. Tuy nhiên, số lượng bước huấn luyện có thể được giảm hiệu quả bằng PETL, điều mà chúng tôi dự định nghiên cứu trong công việc tương lai. Ngoài ra, quan trọng cần lưu ý rằng quá trình huấn luyện mô hình là sự kiện một lần, trong khi suy luận mô hình thì không. Trong bối cảnh này, lợi ích hiệu quả của DEPT trở nên đặc biệt có giá trị; (2) số lượng tham số có thể huấn luyện trong DEPT phụ thuộc vào độ dài chuỗi tối đa s. Trong nghiên cứu này, chúng tôi đã giới hạn đánh giá của mình cho các nhiệm vụ có hàng trăm token đầu vào. Công việc tương lai có thể khám phá hiệu suất của DEPT khi s cực lớn; và (3) nghiên cứu của chúng tôi tập trung vào việc tận dụng các kỹ thuật prompting cho LM, trong đó các nghiên cứu trước (Bender & Koller, 2020; Brown et al., 2020; Bender et al., 2021) đã giải quyết các mối quan ngại và nguy hiểm tiềm ẩn liên quan đến LM.

--- TRANG 10 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
LỜI CẢM ŠN
Các tác giả bày tỏ lòng biết ơn với các nhà phản biện và chủ tịch khu vực ICLR vì những thảo luận sâu sắc của họ. Zhengxiang được tài trợ bởi Học bổng Nghiên cứu từ Đại học College London (UCL).

TÀI LIỆU THAM KHẢO
[Các tài liệu tham khảo được liệt kê đầy đủ từ trang 10-16]

--- TRANG 17 ---
Được xuất bản dưới dạng bài báo hội nghị tại ICLR 2024
TỔNG QUAN PHỤ LỤC
Phụ lục được cấu trúc như sau:
Phụ lục §A cung cấp trực quan hóa hiệu suất mô hình so với số lượng tham số có thể huấn luyện trên các benchmark GLUE và SuperGLUE.
Phụ lục §B trình bày các kết quả thí nghiệm bổ sung, bao gồm sử dụng kích thước lớn hơn của các mô hình ngôn ngữ (LLAMA-2 và TB-3B) và kiểm tra tác động của các độ dài prompt mềm khác nhau.
Phụ lục §C cung cấp mô tả ngắn gọn về tất cả các bộ dữ liệu được sử dụng trong nghiên cứu này.
Phụ lục §D cung cấp chi tiết triển khai và siêu tham số cho tất cả các phương pháp so sánh được sử dụng trong các thí nghiệm của chúng tôi.
Phụ lục §E cung cấp thảo luận thêm về trực giác và các nghiên cứu liên quan.

A HIỆU SUẤT MÔ HÌNH SO VỚI HIỆU QUẢ THAM SỐ
Chúng tôi trực quan hóa kết quả thí nghiệm trong Bảng 1, như được hiển thị trong Hình 6. Việc trực quan hóa cho thấy rằng phương pháp đề xuất DEPT của chúng tôi vượt trội so với các phương pháp PEFT khác và các baseline tinh chỉnh đầy đủ trên benchmark GLUE và SuperGLUE (trục y) trong khi chỉ cập nhật một số lượng nhỏ tham số có thể huấn luyện (trục x).

Hình 6: Hiệu suất trung bình so với số lượng tham số có thể huấn luyện trên benchmark GLUE và SuperGLUE sử dụng mô hình T5-BASE.

B THÍ NGHIỆM BỔ SUNG
LLAMA-2. Chúng tôi đánh giá hiệu suất và tốc độ suy luận của phương pháp đề xuất DEPT sử dụng LLAMA-2-7B và LLAMA-2-13B (Touvron et al., 2023) trên bộ dữ liệu SST-2. Trong thí nghiệm của chúng tôi, độ dài prompt mềm cho PT vanilla được đặt là l= 100. Đối với DEPT, chúng tôi đặt độ dài prompt mềm là m= 60 và chọn hạng r= 40 cho các ma trận thấp hạng. Như được hiển thị trong Bảng 6, kết quả thí nghiệm của chúng tôi cho thấy rằng DEPT không chỉ vượt trội so với PT vanilla về độ chính xác kiểm tra mà còn cải thiện tốc độ suy luận. Chúng tôi chỉ giới hạn đánh giá DEPT trên bộ dữ liệu SST-2 do chi phí tính toán cao. Chúng tôi sẽ cố gắng hết sức để có được các tài nguyên cần thiết để khám phá thêm hiệu suất của DEPT, nhằm cung cấp đánh giá toàn diện hơn trong công việc tương lai.

Bảng 6: Kết quả kiểm tra sử dụng LLAMA-2-7B và LLAMA-2-13B trên bộ dữ liệu SST-2.

Điều chỉnh Prompt DEPT (chúng tôi)
Phương pháp Acc Kiểm tra Mẫu suy luận mỗi giây Acc Kiểm tra Mẫu suy luận mỗi giây
LLAMA-2-7B 94.48 3.895 94.95 4.857
LLAMA-2-13B 95.99 2.083 96.01 2.835

T5-3B. Chúng tôi đánh giá hiệu suất và tốc độ suy luận của phương pháp đề xuất DEPT sử dụng mô hình T5-3B. Chúng tôi báo cáo hiệu suất trung bình trên bộ dữ liệu Glue cũng như tốc độ suy luận, được đo bằng mẫu suy luận mỗi giây. Như được hiển thị trong Bảng 7, các phát hiện của chúng tôi cho thấy rằng DePT (m=60, r=30) vượt trội so với PT về tốc độ suy luận 37%. Điều này cho thấy lợi thế của DePT tăng lên khi kích thước mô hình tăng.

Bảng 7: Kết quả kiểm tra sử dụng T5-3B trên Benchmark Glue.

Phương pháp Hiệu suất Glue Trung bình Mẫu suy luận mỗi giây
DEPT (m=60, r=30) 86.4 8.9
PT (m=100) 85.6 6.5

Độ dài prompt khác nhau. Chúng tôi đã thực hiện các thí nghiệm bổ sung về độ dài prompt khác nhau, như được hiển thị trong Bảng dưới đây. Cụ thể, chúng tôi đã tăng kích thước tham số có thể huấn luyện trong cả DEPT và PT lên gấp đôi. Chúng tôi sử dụng T5-BASE làm backbone. Như được hiển thị trong Bảng 8, chúng tôi báo cáo hiệu suất trung bình trên bộ dữ liệu Glue cũng như tốc độ suy luận, được đo bằng mẫu suy luận mỗi giây. Các phát hiện của chúng tôi cho thấy rằng DEPT (m=120, r=60) vượt trội so với PT về tốc độ suy luận 34%. Chúng tôi tin rằng lợi thế hiệu suất này có thể được tăng cường thêm bằng cách giảm giá trị của m, đại diện cho độ dài của prompt mềm. Để cung cấp một ví dụ cụ thể, trên bộ dữ liệu SST-2, DEPT có thể đạt được tốc độ suy luận 77.2 mẫu mỗi giây, trong khi PT chỉ có thể suy luận 57.4 mẫu mỗi giây. Điều này cho thấy lợi thế của DEPT so với PT tăng lên khi kích thước mô hình tăng.

Bảng 8: Tác động của việc sử dụng độ dài prompt mềm dài hơn. Kết quả kiểm tra sử dụng T5-BASE trên Benchmark Glue.

Phương pháp Hiệu suất Glue Trung bình Mẫu suy luận mỗi giây
DEPT (m=120, r=60) 86.0 54.8
PT (m=200) 85.2 40.8

C BỘ DỮ LIỆU
Trong nghiên cứu này, chúng tôi sử dụng 23 bộ dữ liệu phổ biến từ nghiên cứu học few-shot và PEFT trước đây. Chúng tôi giới hạn số lượng dữ liệu huấn luyện tối đa của Yelp-2 là 100k mẫu. Chúng tôi huấn luyện MNLI với nhiều bước hơn, tổng cộng 200k bước. Đối với bộ dữ liệu GLUE, chúng tôi sử dụng bộ dữ liệu HuggingFace. Đối với bộ dữ liệu Super GLUE, chúng tôi sử dụng bộ dữ liệu HuggingFace. Đối với MRQA 2019 Shared Task và các bộ dữ liệu khác, chúng tôi sử dụng bộ dữ liệu HuggingFace.

[Bảng 9: Các bộ dữ liệu được đánh giá trong nghiên cứu này - bảng chi tiết với thông tin về từng bộ dữ liệu]

D CHI TIẾT TRIỂN KHAI
Mã của chúng tôi được triển khai sử dụng Pytorch, Huggingface Transformers, và Huggingface PEFT. Dưới đây, chúng tôi cung cấp danh sách toàn diện các siêu tham số được sử dụng trong mã của chúng tôi. Trong nghiên cứu của chúng tôi, chúng tôi chủ yếu trích dẫn kết quả thí nghiệm từ các nghiên cứu trước... [chi tiết triển khai tiếp theo]

E THẢO LUẬN THÊM
Trực giác. Trực giác của DEPT là (1) với cùng số lượng tham số có thể huấn luyện, việc cho phép một số cập nhật cho nhúng từ sẽ cải thiện hiệu suất; và (2) một prompt mềm ngắn hơn sẽ cải thiện hiệu quả... [thảo luận tiếp theo về trực giác và các nghiên cứu liên quan]
