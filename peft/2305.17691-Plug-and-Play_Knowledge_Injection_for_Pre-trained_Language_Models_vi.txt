# 2305.17691.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.17691.pdf
# Kích thước tệp: 1946853 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chèn Kiến thức Cắm và Chạy cho Mô hình Ngôn ngữ Được Tiền huấn luyện
Zhengyan Zhang1∗, Zhiyuan Zeng1∗, Yankai Lin2,3, Huadong Wang1, Deming Ye1
Chaojun Xiao1, Xu Han1†, Zhiyuan Liu1,4,5†, Peng Li6, Maosong Sun1,4, Jie Zhou7
1Nhóm NLP, DCST, IAI, BNRIST, Đại học Thanh Hoa, Bắc Kinh
2Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc, Bắc Kinh
3Phòng thí nghiệm Trọng điểm Bắc Kinh về Phương pháp Quản lý và Phân tích Dữ liệu Lớn
4Trung tâm Đổi mới Quốc tế Đại học Thanh Hoa, Thượng Hải 5Phòng thí nghiệm Quan Cheng
6Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa, Trung Quốc
7Trung tâm Nhận dạng Mẫu, AI WeChat, Tencent Inc
{zy-z19,zengzy20}@mails.tsinghua.edu.cn {hanxu2022,liuzy}@tsinghua.edu.cn

Tóm tắt
Chèn kiến thức bên ngoài có thể cải thiện hiệu suất của mô hình ngôn ngữ được tiền huấn luyện (PLM) trên các tác vụ NLP downstream khác nhau. Tuy nhiên, việc huấn luyện lại quy mô lớn là cần thiết để triển khai các phương pháp chèn kiến thức mới hoặc cơ sở kiến thức cho các tác vụ downstream. Trong nghiên cứu này, chúng tôi là những người đầu tiên nghiên cứu cách cải thiện tính linh hoạt và hiệu quả của việc chèn kiến thức bằng cách tái sử dụng các mô hình downstream hiện có. Để đạt được điều này, chúng tôi khám phá một mô hình mới chèn kiến thức cắm và chạy, nơi các cơ sở kiến thức được chèn vào các mô hình downstream hiện có đã đóng băng bằng một plugin kiến thức. Tương ứng, chúng tôi đề xuất một phương pháp chèn cắm và chạy map-tuning, huấn luyện một ánh xạ của các embedding kiến thức để làm phong phú đầu vào mô hình với các embedding đã ánh xạ trong khi giữ các tham số mô hình đóng băng. Kết quả thực nghiệm trên ba tác vụ NLP dựa trên kiến thức cho thấy các phương pháp chèn hiện có không phù hợp với mô hình mới, trong khi map-tuning hiệu quả cải thiện hiệu suất của các mô hình downstream. Hơn nữa, chúng tôi chỉ ra rằng một mô hình downstream đóng băng có thể được thích ứng tốt với các miền khác nhau với các mạng ánh xạ khác nhau của kiến thức miền. Mã nguồn và mô hình của chúng tôi có tại https://github.com/THUNLP/Knowledge-Plugin.

1 Giới thiệu
Những năm gần đây đã chứng kiến sự phát triển nhanh chóng trong việc nâng cao mô hình ngôn ngữ được tiền huấn luyện (PLM) với các cơ sở kiến thức bên ngoài khác nhau, tức là, chèn kiến thức cho PLM (Levine et al., 2020; Zhou et al., 2020; Zhang et al., 2019; Peters et al., 2019; Bosselut et al., 2019; Guan et al., 2020). Chèn kiến thức cải thiện hiệu suất của PLM trên một loạt các tác vụ như trích xuất thông tin (Liu et al., 2020a; Wang et al., 2021b), trả lời câu hỏi (Xiong et al., 2020; Wang et al., 2021a), và tạo văn bản (Chen et al., 2020).

Các phương pháp chèn hiện có thường chèn kiến thức bằng tiền huấn luyện hoặc tinh chỉnh có nhận thức kiến thức (Peters et al., 2019; Yamada et al., 2020; Liu et al., 2020a; Wang et al., 2021a). Tuy nhiên, ít được nghiên cứu là cách chèn kiến thức vào một mô hình downstream đã được thích ứng với một tác vụ cụ thể. Nếu chúng ta muốn áp dụng một phương pháp chèn kiến thức mới để nâng cao mô hình trên một tác vụ cụ thể, chúng ta phải loại bỏ các mô hình downstream cụ thể theo tác vụ và huấn luyện lại chúng. Ngoài ra, một mô hình downstream làm việc với nhiều cơ sở kiến thức đòi hỏi tự huấn luyện lại để chèn từng cơ sở kiến thức. Huấn luyện lại mô hình tốn thời gian và tài nguyên, dẫn đến nhu cầu về một mô hình chèn linh hoạt và hiệu quả.

Hướng tới việc chèn linh hoạt và hiệu quả, chúng tôi khám phá một mô hình mới chèn kiến thức cắm và chạy, nơi các cơ sở kiến thức được chèn vào các mô hình downstream hiện có đã đóng băng bằng các mô-đun kiến thức. Mô-đun kiến thức kết nối cơ sở kiến thức và mô hình, và chúng tôi gọi nó một cách sinh động là plugin. Dưới mô hình này, một mô hình downstream sẽ có nhiều plugin, mỗi plugin tương ứng với một sự kết hợp của một phương pháp chèn và một cơ sở kiến thức, điều này đảm bảo tính linh hoạt. Hơn nữa, các plugin kiến thức nên đủ nhỏ để đảm bảo hiệu quả. Một cách trực quan, như được thể hiện trong Hình 1, chúng tôi coi các mô hình và cơ sở kiến thức như máy tính và đĩa flash tương ứng.

Trong nghiên cứu này, chúng tôi nghiên cứu hai cài đặt cho mô hình chèn kiến thức cắm và chạy. Cài đặt đầu tiên là chèn kiến thức cắm và chạy chung, nhằm chèn kiến thức vào tất cả các mô hình downstream (được huấn luyện từ một PLM cụ thể) bằng một plugin chung mà không cần bất kỳ huấn luyện cụ thể theo tác vụ nào. Trong cài đặt này, tất cả các mô hình downstream chia sẻ chính xác một plugin cho một sự kết hợp của một phương pháp chèn và một cơ sở kiến thức. Cài đặt thứ hai là chèn kiến thức cắm và chạy cụ thể theo tác vụ, nơi các plugin kiến thức được huấn luyện để thích ứng tốt hơn với các tác vụ downstream trong khi giữ các mô hình downstream đóng băng.

Qua nghiên cứu thí điểm của chúng tôi, chúng tôi thấy rằng các phương pháp hiện có (Poerner et al., 2020; Ye et al., 2022; Wang et al., 2021a; Lewis et al., 2020) có thể được sử dụng trực tiếp không thể được áp dụng tốt cho mô hình chèn cắm và chạy. Để đạt được điều này, chúng tôi đề xuất map-tuning, một khám phá sơ bộ về việc học các plugin kiến thức. Cụ thể, chúng tôi huấn luyện một mạng ánh xạ nhẹ để tăng cường đầu vào mô hình với các biểu diễn kiến thức đã ánh xạ, ví dụ như TransE (Bordes et al., 2013). Để đáp ứng các yêu cầu chèn chung và cụ thể theo tác vụ, chúng tôi thiết kế map-tuning chung và map-tuning cụ thể theo tác vụ tương ứng. Map-tuning chung áp dụng mô hình hóa ngôn ngữ như mục tiêu để học các plugin kiến thức và tìm kiếm khả năng tổng quát hóa tốt hơn. Map-tuning cụ thể theo tác vụ áp dụng các mục tiêu tác vụ cho việc học plugin và tìm kiếm thích ứng tác vụ tốt hơn.

Chúng tôi sử dụng ba tác vụ NLP dựa trên kiến thức điển hình để đánh giá việc chèn kiến thức cắm và chạy của chúng tôi, bao gồm phân loại quan hệ (Han et al., 2018), phân loại thực thể (Xin et al., 2018), và trả lời câu hỏi (Sciavolino et al., 2021). Kết quả thực nghiệm cho thấy: (1) sau khi thích ứng PLM với các tác vụ downstream thông qua tinh chỉnh toàn bộ tham số hoặc tinh chỉnh hiệu quả tham số, còn được gọi là delta tuning (Liu et al., 2021; Ding et al., 2022), việc chèn kiến thức vào các mô hình downstream này bằng map-tuning chung dẫn đến cải thiện hiệu suất trong hầu hết tất cả các trường hợp; (2) sử dụng map-tuning cụ thể theo tác vụ để chèn kiến thức miền tiếp tục cho phép một mô hình downstream đóng băng hoạt động tốt trong các miền khác nhau. Chúng tôi hy vọng đóng góp của chúng tôi có thể thu hút nhiều sự chú ý hơn đến mô hình chèn kiến thức cắm và chạy và truyền cảm hứng cho nhiều nghiên cứu tương lai hơn.

--- TRANG 2 ---
tương ứng với một sự kết hợp của một phương pháp chèn và một cơ sở kiến thức, điều này đảm bảo tính linh hoạt. Hơn nữa, các plugin kiến thức nên đủ nhỏ để đảm bảo hiệu quả. Một cách trực quan, như được thể hiện trong Hình 1, chúng tôi coi các mô hình và cơ sở kiến thức như máy tính và đĩa flash tương ứng.

Trong nghiên cứu này, chúng tôi nghiên cứu hai cài đặt cho mô hình chèn kiến thức cắm và chạy. Cài đặt đầu tiên là chèn kiến thức cắm và chạy chung, nhằm chèn kiến thức vào tất cả các mô hình downstream (được huấn luyện từ một PLM cụ thể) bằng một plugin chung mà không cần bất kỳ huấn luyện cụ thể theo tác vụ nào. Trong cài đặt này, tất cả các mô hình downstream chia sẻ chính xác một plugin cho một sự kết hợp của một phương pháp chèn và một cơ sở kiến thức. Cài đặt thứ hai là chèn kiến thức cắm và chạy cụ thể theo tác vụ, nơi các plugin kiến thức được huấn luyện để thích ứng tốt hơn với các tác vụ downstream trong khi giữ các mô hình downstream đóng băng.

Qua nghiên cứu thí điểm của chúng tôi, chúng tôi thấy rằng các phương pháp hiện có (Poerner et al., 2020; Ye et al., 2022; Wang et al., 2021a; Lewis et al., 2020) có thể được sử dụng trực tiếp không thể được áp dụng tốt cho mô hình chèn cắm và chạy. Để đạt được điều này, chúng tôi đề xuất map-tuning, một khám phá sơ bộ về việc học các plugin kiến thức. Cụ thể, chúng tôi huấn luyện một mạng ánh xạ nhẹ để tăng cường đầu vào mô hình với các biểu diễn kiến thức đã ánh xạ, ví dụ như TransE (Bordes et al., 2013). Để đáp ứng các yêu cầu chèn chung và cụ thể theo tác vụ, chúng tôi thiết kế map-tuning chung và map-tuning cụ thể theo tác vụ tương ứng. Map-tuning chung áp dụng mô hình hóa ngôn ngữ như mục tiêu để học các plugin kiến thức và tìm kiếm khả năng tổng quát hóa tốt hơn. Map-tuning cụ thể theo tác vụ áp dụng các mục tiêu tác vụ cho việc học plugin và tìm kiếm thích ứng tác vụ tốt hơn.

Chúng tôi sử dụng ba tác vụ NLP dựa trên kiến thức điển hình để đánh giá việc chèn kiến thức cắm và chạy của chúng tôi, bao gồm phân loại quan hệ (Han et al., 2018), phân loại thực thể (Xin et al., 2018), và trả lời câu hỏi (Sciavolino et al., 2021). Kết quả thực nghiệm cho thấy: (1) sau khi thích ứng PLM với các tác vụ downstream thông qua tinh chỉnh toàn bộ tham số hoặc tinh chỉnh hiệu quả tham số, còn được gọi là delta tuning (Liu et al., 2021; Ding et al., 2022), việc chèn kiến thức vào các mô hình downstream này bằng map-tuning chung dẫn đến cải thiện hiệu suất trong hầu hết tất cả các trường hợp; (2) sử dụng map-tuning cụ thể theo tác vụ để chèn kiến thức miền tiếp tục cho phép một mô hình downstream đóng băng hoạt động tốt trong các miền khác nhau. Chúng tôi hy vọng đóng góp của chúng tôi có thể thu hút nhiều sự chú ý hơn đến mô hình chèn kiến thức cắm và chạy và truyền cảm hứng cho nhiều nghiên cứu tương lai hơn.

2 Mô hình Chèn Kiến thức Cắm và Chạy

Mô tả Mô hình. Cho một mô hình downstream D được huấn luyện trên một tác vụ downstream với PLM P làm xương sống, chúng tôi có ý định cải thiện hiệu suất của nó trên tác vụ này bằng cách kết hợp một cơ sở kiến thức bổ sung B và đóng băng các tham số của D, để đó chúng tôi cần huấn luyện một plugin kiến thức M. Lưu ý rằng không tiền huấn luyện hay tinh chỉnh nào huấn luyện mô hình D để hợp tác với B hoặc M.

Hai Cài đặt Chèn. Như được thể hiện trong Hình 2(a), chèn kiến thức cắm và chạy tách rời việc chèn kiến thức khỏi huấn luyện mô hình, điều này khác với các mô hình hiện có. Đối với chèn kiến thức cắm và chạy chung, M được thu được chỉ dựa trên P và B, và sau đó nó được cắm trực tiếp vào tất cả các mô hình downstream, D1, D2, ..., mà không có bất kỳ huấn luyện bổ sung nào. Đối với chèn kiến thức cắm và chạy cụ thể theo tác vụ, nó được phép huấn luyện M1, M2, ... cho D1, D2, ... tương ứng trong khi giữ D1, D2, ... đóng băng.

Thách thức. Chèn kiến thức cắm và chạy chung đặt ra những thách thức nghiêm trọng cho các phương pháp được thiết kế cho nó. M được kỳ vọng cải thiện hiệu suất của D, nhưng D chưa bao giờ thấy M hoặc được thấy bởi M trong quá trình huấn luyện. Điều kiện tiên quyết duy nhất là P và B hiển thị trong quá trình huấn luyện M. Do đó, các phương pháp được thiết kế cho chèn chung cần phú cho M đủ khả năng tổng quát hóa sao cho M có thể thích ứng tốt với D1, D2, ... không xác định. Mặc dù cơ sở kiến thức B có thể có kiến thức phong phú, mà không có sự thích ứng tốt của M, thông tin hữu ích mang đến cho D sẽ ít hơn tiếng ồn gây rối.

Chèn kiến thức cắm và chạy cụ thể theo tác vụ nới lỏng các hạn chế, nơi Mi được phép huấn luyện với Di đóng băng. So với việc chèn trong quá trình tinh chỉnh, việc huấn luyện Mi nên nhanh và số lượng tham số của Mi nên nhỏ so với Di. Nếu không, các phương pháp sẽ vô nghĩa. Do đó, nó đòi hỏi thiết kế kiến trúc đơn giản và hiệu quả và các mục tiêu huấn luyện thông tin cho Mi.

Tiềm năng Sử dụng Các Phương pháp Hiện có. Ít phương pháp chèn kiến thức hiện có có thể được sử dụng trực tiếp cho chèn kiến thức cắm và chạy chung. Chúng tôi tóm tắt các phương pháp chèn kiến thức hiện có

--- TRANG 3 ---
[CLS] yêu đậu phộng bơ. [SEP] / [MASK] / Map Map Đầu vào Bổ sung [CLS] yêu đậu phộng bơ. [SEP] / / Map Map đậu phộng bơ MLM Mất mát Đóng băng PLM Tác vụ Mất mát Đóng băng DM PLM Kiến thức

(1) Tinh chỉnh
(2) Chèn

Chèn Kiến thức Cắm và Chạy

Chuột

(a) (b)

Hình 2: Trái: So sánh giữa các mô hình trước đây và mô hình cắm và chạy được đề xuất của chúng tôi. Phải: Hai cách cho map-tuning. Văn bản đầu vào là "Chuột yêu đậu phộng bơ.". "DM" đề cập đến "mô hình downstream".

các phương pháp chèn¹ có khả năng được sử dụng cho chèn kiến thức cắm và chạy chung như sau. (1) Các phương pháp dựa trên embedding: E-BERT (Poerner et al., 2020) và PELT (Ye et al., 2022) xây dựng một bảng tra cứu embedding thực thể trong không gian biểu diễn của embedding token và kết hợp embedding thực thể với embedding token để xây dựng embedding đầu vào. (2) Các phương pháp dựa trên truy vấn: RAG (Lewis et al., 2020) truy vấn văn bản thuần túy từ cơ sở kiến thức và tăng cường văn bản đầu vào gốc với văn bản thuần túy như kiến thức được chèn. (3) Các phương pháp dựa trên adapter: K-Adapter (Wang et al., 2021a) tính toán các biểu diễn có kiến thức dựa trên đầu ra của các mô hình downstream đi kèm với các adapter có kiến thức, được huấn luyện với PLM đóng băng và được cắm vào tất cả các mô hình downstream.

Mặc dù các phương pháp này có thể mang lại kiến thức mà không huấn luyện PLM, không rõ liệu chúng có hoạt động tốt trong mô hình chèn kiến thức cắm và chạy hay không, tức là, liệu kiến thức được mang lại bởi chúng có thể sử dụng được cho các mô hình downstream chưa bao giờ học cách sử dụng các phương pháp này hay không.

3 Map-Tuning

Trong phần này, trước tiên chúng tôi trình bày khung tổng thể của map-tuning, được thiết kế cho chèn kiến thức cắm và chạy. Sau đó, chúng tôi chỉ ra cách sử dụng nó cho chèn chung và chèn cụ thể theo tác vụ, nơi các phương pháp được gọi là map-tuning chung và map-tuning cụ thể theo tác vụ tương ứng.

3.1 Khung Tổng thể

Chúng tôi nhắm mục tiêu các cơ sở kiến thức bao gồm một tập hợp các thực thể và kiến thức có cấu trúc hoặc không có cấu trúc về các thực thể này. Để sử dụng một cơ sở kiến thức B như vậy, chúng tôi giả định một mô hình biểu diễn kiến thức K để gán mỗi thực thể e một embedding thực thể e ∈ R^dKE, nơi dKE là chiều của embedding thực thể. Map-tuning chèn kiến thức bằng cách ánh xạ các biểu diễn kiến thức vào không gian của embedding token và sử dụng các biểu diễn đã ánh xạ như đầu vào bổ sung, điều này cũng được áp dụng bởi Poerner et al. (2020); Ye et al. (2022).

Cụ thể, cho một văn bản đầu vào, trước tiên chúng tôi khớp các đề cập thực thể trong văn bản với các thực thể trong B. Văn bản đầu vào được ký hiệu bởi {w1, w2, ..., wn}, nơi wi là token thứ i và n là số lượng token trong văn bản đầu vào. Chúng tôi sử dụng một bộ ba (e, l, r) để biểu diễn một khoảng đề cập, nơi e là thực thể được khớp, l và r là các chỉ số token trái và phải của khoảng đề cập. Khoảng đề cập tương ứng là {wl, wl+1, ..., wr}.

Giả định có m thực thể trong văn bản, (e1, l1, r1), (e2, l2, r2), ..., (em, lm, rm), nơi 1 ≤ l1 ≤ r1 < l2 ≤ r2 < ... < lm ≤ rm ≤ n.

Chuỗi gốc của embedding đầu vào là {w1, w2, ..., wn}, nơi wi ∈ R^dPLM là embedding token thứ i và dPLM là chiều của embedding token. Sau đó, chúng tôi ánh xạ mỗi embedding thực thể ei thành M(ei) ∈ R^dPLM bằng một mạng ánh xạ M. Cuối cùng, chúng tôi thay thế {wli, wli+1, ..., wri} bằng {M(ei), /, wli, ..., wri} cho mọi (ei, li, ri) để xây dựng một chuỗi đầu vào mới. Lưu ý rằng / là embedding token của "/".

3.2 Map-tuning Chung

Map-tuning chung nhằm huấn luyện một mạng ánh xạ M dựa trên P và K. Nó đòi hỏi M phải có đủ khả năng tổng quát hóa để xử lý các tác vụ downstream khác nhau vì M sẽ được cắm vào tất cả các mô hình downstream. Do đó, chúng tôi huấn luyện M với một

¹Các phương pháp này ban đầu được thiết kế để chèn kiến thức trong quá trình tiền huấn luyện hoặc tinh chỉnh.

tác vụ tiền huấn luyện về kiến thức cho những thực thể này. Để sử dụng một cơ sở kiến thức B như vậy, chúng tôi giả định một mô hình biểu diễn kiến thức K để gán mỗi thực thể e một embedding thực thể e ∈ R^dKE, nơi dKE là chiều của embedding thực thể. Map-tuning chèn kiến thức bằng cách ánh xạ các biểu diễn kiến thức vào không gian của embedding token và sử dụng các biểu diễn đã ánh xạ như đầu vào bổ sung, điều này cũng được áp dụng bởi Poerner et al. (2020); Ye et al. (2022).

--- TRANG 4 ---
tác vụ tiền huấn luyện chung trong khi cắm nó vào P, như mô hình hóa ngôn ngữ, đã được chứng minh là học đa tác vụ không giám sát (Radford et al., 2019). Chúng tôi đóng băng các tham số của P và chỉ huấn luyện mạng ánh xạ M để đáp ứng yêu cầu của chèn kiến thức cắm và chạy.

Chúng tôi áp dụng một biến thể của Mô hình Ngôn ngữ Có Mặt nạ (MLM) (Devlin et al., 2019), được đặt tên là Mô hình Ngôn ngữ Có Mặt nạ Đề cập (MMLM), như tác vụ để huấn luyện M. Theo quan sát của chúng tôi trong các thí nghiệm sơ bộ, việc dự đoán hầu hết các token chỉ đòi hỏi khả năng ngôn ngữ thay vì kiến thức bên ngoài, như của một số từ dừng, trong khi việc dự đoán các đề cập thực thể dựa vào kiến thức bên ngoài thường xuyên hơn. Do đó, như được thể hiện trong Hình 2(b), chúng tôi ngẫu nhiên che mặt nạ chỉ các đề cập thực thể² trong văn bản đầu vào để đảm bảo rằng mạng ánh xạ được huấn luyện đầy đủ và các embedding đã ánh xạ được sử dụng tốt trong PLM. Theo cách này, khả năng của PLM để dự đoán các đề cập thực thể bị che mặt nạ được tăng cường bởi các embedding đã ánh xạ của cả thực thể bị che mặt nạ và các thực thể khác trong ngữ cảnh. Chúng tôi che mặt nạ tất cả các token của một đề cập thực thể bị che mặt nạ, và mất mát MMLM giống với mất mát MLM gốc (Devlin et al., 2019).

Sau map-tuning chung, M có thể được sử dụng cho chèn cắm và chạy chung. Mặc dù mạng ánh xạ M không được huấn luyện với bất kỳ mô hình downstream D nào trước đó, chúng ta có thể cắm M trực tiếp vào mỗi D.

3.3 Map-tuning Cụ thể theo Tác vụ

Map-tuning cụ thể theo tác vụ nhằm thích ứng một mạng ánh xạ M cho một mô hình downstream D cho trước. Chúng tôi đóng băng các tham số của D và huấn luyện mạng ánh xạ M trên tác vụ downstream, quy trình này được thể hiện trong Hình 2(b). Mục tiêu huấn luyện giống hệt với mục tiêu gốc của tác vụ này. Nếu các biểu diễn kiến thức cung cấp thông tin hữu ích cho tác vụ này, mạng ánh xạ sẽ học cách trích xuất thông tin này và làm cho nó có thể nhận biết được đối với mô hình downstream D. Lưu ý rằng mạng ánh xạ không chỉ có thể được huấn luyện từ đầu, mà còn có thể được khởi tạo với một mạng ánh xạ được học với map-tuning chung, điều này có thể cung cấp một điểm khởi đầu tốt.

²Nếu số lượng đề cập thực thể nhỏ, chúng ta có thể chọn bao phủ tất cả các tổ hợp che mặt nạ.

4 Thí nghiệm

4.1 Thiết lập Thí nghiệm

Phương pháp Huấn luyện Mô hình Downstream. Chúng tôi áp dụng BERT base (Devlin et al., 2019) làm PLM xương sống trong các thí nghiệm và xem xét bốn phương pháp huấn luyện để thích ứng nó với các tác vụ downstream. Bên cạnh tinh chỉnh toàn mô hình vanilla, chúng tôi cũng xem xét ba phương pháp tinh chỉnh hiệu quả tham số (PET), đã trở nên ngày càng quan trọng trong kỷ nguyên của PLM quy mô lớn (Liu et al., 2021). Vì tiết kiệm tài nguyên là cả chèn kiến thức cắm và chạy và PET, việc áp dụng mô hình này cho các mô hình downstream được huấn luyện bằng các phương pháp PET trong tình huống hạn chế tài nguyên là có ý nghĩa. (1) Tinh chỉnh tối ưu hóa tất cả các tham số của PLM với mục tiêu tác vụ theo BERT gốc. (2) LoRA (Hu et al., 2021) đóng băng hầu hết các tham số PLM và biểu diễn cập nhật trọng số trong quá trình huấn luyện mô hình với phân tách rank thấp. (3) Adapter (Houlsby et al., 2019) chèn các mạng adapter bổ sung với các tham số PLM đóng băng. (4) BitFit (Zaken et al., 2021) chỉ tối ưu hóa các tham số của vector bias và đóng băng các tham số còn lại. Các siêu tham số được báo cáo trong Phụ lục A.

Tác vụ Downstream. Chúng tôi đánh giá các phương pháp dưới mô hình chèn kiến thức cắm và chạy trên ba loại tác vụ NLP dựa trên kiến thức bao gồm phân loại quan hệ, phân loại thực thể, và trả lời câu hỏi. Đối với phân loại quan hệ, đòi hỏi các mô hình phân loại quan hệ giữa hai thực thể cho một ngữ cảnh, chúng tôi thí nghiệm trên cả cài đặt few-shot và dữ liệu đầy đủ. Trong cài đặt few-shot, chúng tôi nhằm đánh giá hiệu suất mô hình trên các quan hệ long-tail có các instance huấn luyện không đủ. Cụ thể, chúng tôi sử dụng FewRel 1.0 (Han et al., 2018) và FewRel 2.0 (Gao et al., 2019).³ Trong cài đặt dữ liệu đầy đủ, chúng tôi đánh giá các mô hình trên Wiki80 (Han et al., 2019), chứa 80 loại quan hệ từ Wikidata, và tuân theo phân chia dữ liệu của Zhang et al. (2019). Đối với phân loại thực thể, đòi hỏi các mô hình phân loại loại của một thực thể cho một ngữ cảnh, chúng tôi đánh giá các mô hình trên Wiki-ET (Xin et al., 2018) chứa 68 loại thực thể từ Freebase. Đối với trả lời câu hỏi, chúng tôi đánh giá các mô hình trên EntityQuestions (Sciavolino

³Chúng tôi ngẫu nhiên lấy mẫu 5000 instance từ dữ liệu test của FewRel 1.0 và FewRel 2.0 tương ứng để đánh giá nhanh. Lưu ý rằng dữ liệu test không được công bố công khai và chúng tôi nhận dữ liệu từ các tác giả. Chúng tôi thí nghiệm với dữ liệu test đầy đủ của FewRel 1.0 trên bảng xếp hạng chính thức trong Phần 4.4.

--- TRANG 5 ---
et al., 2021), một tập dữ liệu QA miền mở bao gồm các câu hỏi tập trung vào thực thể. Chúng tôi sử dụng các mô hình được tăng cường kiến thức để trả lời trực tiếp các câu hỏi mà không truy vấn các tài liệu liên quan. Chúng tôi báo cáo độ chính xác trên phân loại quan hệ và trả lời câu hỏi, và điểm F1 trên phân loại thực thể.

Cơ sở Kiến thức. Chúng tôi sử dụng Wikidata và UMLS⁴ làm cơ sở kiến thức bên ngoài của chúng tôi cho miền Wikipedia và miền PubMed⁵ tương ứng. Cụ thể, chúng tôi sử dụng các bộ ba Wikidata được cung cấp bởi Zhang et al. (2019) và các mô tả thực thể Wikidata được cung cấp bởi Wang et al. (2021b). Để tránh rò rỉ thông tin trong tác vụ phân loại quan hệ, chúng tôi loại bỏ các bộ ba xuất hiện trong các tập dữ liệu từ các cơ sở kiến thức này. Chúng tôi áp dụng TransE (Bordes et al., 2013) làm mô hình biểu diễn kiến thức của chúng tôi và chiều của embedding kiến thức được đặt thành 100.

Các Phương pháp Hiện có Được Đánh giá. Chúng tôi đánh giá các phương pháp hiện có có thể được áp dụng cho chèn kiến thức cắm và chạy chung. (1) E-BERT (Poerner et al., 2020) cũng thu được một mạng ánh xạ để biến đổi embedding kiến thức. Khác với map-tuning, E-BERT xây dựng kết nối giữa từ vựng và thực thể bằng khớp chuỗi, và sau đó làm cho các embedding kiến thức đã ánh xạ gần với embedding token tương ứng của chúng. Trong nghiên cứu này, E-BERT sử dụng cùng embedding TransE như map-tuning thay vì wikipedia2vec để so sánh công bằng. (2) PELT (Ye et al., 2022) tổng hợp các biểu diễn đầu ra của một thực thể cụ thể trong nhiều ngữ cảnh để xây dựng biểu diễn thực thể. Sau đó, biểu diễn thực thể có thể được nối vào đầu vào mô hình mà không cần bất kỳ ánh xạ nào vì không gian đầu vào và không gian đầu ra giống nhau đối với hầu hết PLM. Ngữ cảnh liên quan đến thực thể có thể được coi như một cơ sở kiến thức văn bản bên ngoài. (3) Tăng cường Truy vấn (RA) là để tăng cường văn bản đầu vào với kiến thức không có cấu trúc được truy vấn bổ sung, như RAG (Lewis et al., 2020) và REALM (Guu et al., 2020). Trong nghiên cứu này, chúng tôi truy vấn các mô tả thực thể từ Wikidata5M và nối chúng vào văn bản đầu vào. (4) K-Adapter (Wang et al., 2021a) lưu trữ ngầm kiến thức trong các tham số của mạng adapter. Chúng tôi tuân theo quy trình gốc của K-Adapter trong khi giữ các tham số của PLM và adapter đóng băng.⁶

⁴UMLS đại diện cho Hệ thống Ngôn ngữ Y tế Thống nhất, là nhãn hiệu của Thư viện Y học Quốc gia Hoa Kỳ.
⁵https://pubmed.ncbi.nlm.nih.gov/
⁶Quy trình này vẫn đòi hỏi huấn luyện lớp kết nối đầy đủ cuối cùng

Chi tiết của Map-tuning. Kiến trúc của mạng ánh xạ đơn giản là một phép biến đổi affine We + b, nơi W ∈ R^(dPLM×dKE) và b ∈ R^dPLM. Trong nghiên cứu này, lượng tham số của mạng ánh xạ là 768×128 + 768 < 0.1M. Đối với Mô hình Ngôn ngữ Có Mặt nạ Đề cập, chúng tôi sử dụng văn bản thô của Wiki20M (Gao et al., 2021), được lấy mẫu từ corpus Wikipedia và cung cấp các chú thích của liên kết thực thể. Tổng kích thước khoảng 300MB, nhỏ hơn nhiều so với corpus tiền huấn luyện thông thường. Vì map-tuning chỉ nhằm thích ứng mạng ánh xạ cho một PLM, nó không đòi hỏi nhiều dữ liệu huấn luyện. Chúng tôi huấn luyện mạng ánh xạ trong 5 epoch, chỉ mất 12 giờ trên NVIDIA Tesla V100. Map-tuning chung về cơ bản xây dựng một bảng tra cứu embedding thực thể. Để đánh giá chất lượng của nó, chúng tôi đánh giá nó trong mô hình chèn trong quá trình tinh chỉnh truyền thống như một thí nghiệm sơ bộ. Để cụ thể hơn, chúng tôi tinh chỉnh PLM trên các tác vụ downstream, trong đó mạng ánh xạ được cắm vào chúng. Chi tiết ở Phụ lục E. Chúng tôi thấy rằng map-tuning liên tục vượt trội hơn E-BERT và PELT trong mô hình truyền thống, điều này cũng xây dựng bảng tra cứu embedding thực thể.

4.2 Chèn Cắm và Chạy Chung

Trong phần này, chúng tôi đánh giá các phương pháp chèn kiến thức trong cài đặt chèn kiến thức cắm và chạy chung, nơi chúng tôi cắm trực tiếp các mô-đun kiến thức vào các mô hình downstream mà không có bất kỳ huấn luyện nào. Kết quả được báo cáo trong Bảng 1.

Từ bảng này, chúng tôi có bốn quan sát: (1) Tất cả bốn phương pháp hiện có không thể cải thiện hiệu suất của các mô hình downstream một cách nhất quán. Trong hầu hết các trường hợp, việc chèn các mô-đun kiến thức này làm giảm hiệu suất mô hình, thường ở mức độ lớn. Điều này chứng minh thực nghiệm rằng cài đặt chèn cắm và chạy chung là thách thức và bốn phương pháp này không phù hợp trong cài đặt này. Kiến thức được cung cấp bởi các phương pháp này không thể được sử dụng trực tiếp, vì vậy chúng về cơ bản là tiếng ồn gây rối đối với các mô hình downstream. (2) Map-tuning chung được đề xuất của chúng tôi đạt được cải thiện nhất quán trên hầu hết tất cả các mô hình downstream, gợi ý rằng mạng ánh xạ hiệu quả biến đổi embedding kiến thức thành không gian của embedding token và các embedding đã ánh xạ có thể được sử dụng trực tiếp bởi các mô hình downstream

, điều này không nghiêm ngặt đáp ứng cài đặt Chèn cắm và chạy chung.

--- TRANG 6 ---
[Bảng 1: Kết quả chèn cắm và chạy chung được hiển thị với các phương pháp khác nhau và cài đặt thí nghiệm]

mô hình. Chúng tôi nhấn mạnh tầm quan trọng của Mô hình Ngôn ngữ Có Mặt nạ Đề cập, cung cấp đủ instance huấn luyện cho map-tuning chung, trong khi các cặp thực thể-token được khớp cho E-BERT là không đủ để huấn luyện mạng ánh xạ. (3) Một cách trực quan, map-tuning chung có thể hoạt động tốt hơn với các phương pháp PET hơn với tinh chỉnh toàn mô hình vì các phương pháp PET thay đổi ít tham số hơn nhiều từ PLM và map-tuning chung được huấn luyện dựa trên PLM. Trên thực tế, cải thiện hiệu suất mang lại cho các mô hình được huấn luyện bằng tinh chỉnh toàn mô hình có thể so sánh với các phương pháp PET. Điều này chứng minh rằng map-tuning là một phương pháp đầy hứa hẹn bất kể các phương pháp huấn luyện của các mô hình downstream. (4) Đáng chú ý cao là cải thiện hiệu suất được mang lại bởi RA cho BERT được tinh chỉnh trên EntityQuestions. Chúng tôi quan sát thấy rằng mô tả thực thể được truy vấn chứa câu trả lời chính xác như một chuỗi con cho 62.19% instance trong tập test, và chúng tôi loại bỏ các instance này và báo cáo kết quả trong Bảng 17. Chúng tôi thấy rằng RA vẫn đạt hiệu suất cao hơn một chút so với map-tuning đối với BERT được tinh chỉnh, nhưng mang lại sự sụt giảm hiệu suất đáng kể cho các mô hình downstream khác, trong khi map-tuning mang lại cải thiện hiệu suất nhất quán cho tất cả các mô hình downstream. Điều này gợi ý rằng BERT được tinh chỉnh có khả năng tổng quát hóa đáng ngạc nhiên để trích xuất một chuỗi con trong

[Bảng 2: Kết quả map-tuning cụ thể theo tác vụ với các cài đặt khác nhau]

ngữ cảnh bổ sung như câu trả lời, và thậm chí để tiết lộ câu trả lời ẩn trong ngữ cảnh bổ sung mà không khớp chuỗi. Ngược lại, các mô hình downstream khác không thể tiết lộ câu trả lời ẩn. Do đó, việc điều tra RA với các mô-đun kiến thức có thể cắm để cung cấp thông tin ổn định cho các mô hình downstream khác nhau là đáng giá, thay vì nối trực tiếp văn bản không có cấu trúc vào đầu vào mô hình.

4.3 Chèn Cắm và Chạy Cụ thể theo Tác vụ

Vì map-tuning đạt hiệu suất tốt nhất trong cài đặt chèn cắm và chạy chung, chúng tôi tiếp tục đánh giá nó trong cài đặt chèn cắm và chạy cụ thể theo tác vụ, nơi chúng tôi huấn luyện các mạng ánh xạ dựa trên các mô hình downstream với các mục tiêu tác vụ. Nếu chúng ta đã tiến hành map-tuning chung trên một PLM, chúng ta có thể khởi tạo mạng với mạng ánh xạ chung. Nếu không, chúng ta phải huấn luyện mạng từ đầu.

--- TRANG 7 ---
Trước tiên chúng tôi đánh giá map-tuning cụ thể theo tác vụ trên Wiki80 và Wiki-ET. Kết quả được báo cáo trong Bảng 2. Từ bảng, chúng tôi có hai quan sát: (1) Map-tuning cụ thể theo tác vụ đạt hiệu suất tốt hơn trên hai tập dữ liệu này so với map-tuning chung. Điều này chỉ ra rằng mạng ánh xạ trích xuất kiến thức thông tin hơn cho tác vụ cụ thể bằng huấn luyện cụ thể theo tác vụ hơn là bằng cách chung. (2) Nếu mạng ánh xạ chung có sẵn, được khuyến nghị sử dụng nó để khởi tạo mạng ánh xạ, điều này tiếp tục cải thiện hiệu suất mô hình.

Sau đó, chúng tôi đánh giá map-tuning cụ thể theo tác vụ trong thích ứng miền, đây là một cài đặt thách thức hơn. Trong cài đặt này, chúng tôi nhằm cắm nhiều cơ sở kiến thức vào một mô hình downstream duy nhất. Cụ thể, một mô hình downstream được huấn luyện trên miền nguồn, và sau đó chúng tôi cắm các mô-đun kiến thức của miền đích vào nó để thích ứng miền. Ở đây, chúng tôi sử dụng các tập dữ liệu phân loại quan hệ trên miền Wikipedia (FewRel 1.0) và miền PubMed (FewRel 2.0). FewRel 1.0 là miền nguồn. FewRel 2.0 là miền đích. Cơ sở kiến thức cho FewRel 2.0 là UMLS. Vì FewRel 2.0 gốc không cung cấp instance huấn luyện, chúng tôi sắp xếp lại FewRel 2.0 và có phân chia dữ liệu sau. Vì FewRel 2.0 có 25 quan hệ, chúng tôi tách 15 quan hệ cho huấn luyện và phát triển và 10 quan hệ còn lại được sử dụng để kiểm tra.

Từ Bảng 3, chúng tôi có hai quan sát: (1) Đối với thích ứng miền từ Wikipedia sang PubMed, map-tuning cải thiện đáng kể hiệu suất mô hình (ví dụ, từ 76.7 lên 81.2 trong 5-1) và đạt hiệu suất tốt hơn mô hình được tinh chỉnh trên miền PubMed (ví dụ, từ 78.6 lên 81.2 trong 5-1). Điều này gợi ý rằng việc sử dụng map-tuning để giới thiệu kiến thức bên ngoài cho thích ứng miền là đầy hứa hẹn. (2) Huấn luyện đa miền làm giảm hiệu suất mô hình trên miền Wikipedia và duy trì hiệu suất của nó trên miền PubMed trong khi map-tuning không làm giảm hiệu suất trên mỗi miền. Điều này chỉ ra rằng các mạng ánh xạ có thể cắm phù hợp cho thích ứng miền liên tục.

4.4 Hiệu quả Tính toán

Chúng tôi so sánh mô hình chèn kiến thức cắm và chạy được đề xuất của chúng tôi với các mô hình chèn kiến thức trước đây về chi phí thời gian. Chúng tôi đánh giá thời gian huấn luyện trên NVIDIA Tesla V100 và so sánh hiệu suất mô hình trên cài đặt 10-way 1-shot của FewRel 1.0. ERNIE (Zhang et al., 2019), KEPLER (Wang et al., 2021b), và LUKE (Yamada et al., 2020) chèn kiến thức trong quá trình tiền huấn luyện. PELT (Ye et al., 2022) chèn kiến thức trong quá trình tinh chỉnh. Map-tuning chèn kiến thức sau tinh chỉnh.

Kết quả được thể hiện trong Hình 3. Từ hình này, chúng tôi quan sát thấy rằng thời gian huấn luyện của map-tuning ngắn hơn nhiều so với các phương pháp dưới mô hình chèn trong quá trình tiền huấn luyện, và nó có thể so sánh với PELT. Bên cạnh đó, hiệu suất của map-tuning cũng cạnh tranh so với các phương pháp chèn kiến thức trước đây. Hơn nữa, map-tuning chỉ tối ưu hóa 0.1% tham số bổ sung và chúng tôi báo cáo số lượng tham số được tối ưu hóa cho các phương pháp chèn kiến thức khác nhau trong Phụ lục G. Chèn kiến thức cắm và chạy có tiềm năng lớn để có thể so sánh với các mô hình trước đây về hiệu suất tác vụ, trong khi duy trì tính linh hoạt và hiệu quả vốn có của nó.

4.5 Nghiên cứu Trường hợp

Chúng tôi trình bày một phân tích định tính của map-tuning trong Bảng 4. Trong trường hợp đầu tiên, mô hình downstream gốc không hiểu rằng "flying officer" là một cấp bậc quân sự và dự đoán sai quan hệ là "occupation". Với mạng ánh xạ chung, làm phong phú ý nghĩa của "flying officer", mô hình dự đoán đúng quan hệ.

Tuy nhiên, mạng ánh xạ chung có thể gây hiểu lầm trong một số trường hợp. Trong trường hợp thứ hai, mô hình downstream gốc dễ dàng nhận ra "Wasp" là thành viên của "Avengers" mà không cần bất kỳ kiến thức bên ngoài nào vì sự thật này có thể được suy ra bởi từ "other". So với kiến thức bên ngoài được cung cấp bởi mạng ánh xạ cụ thể theo tác vụ, thô sơ là kiến thức được cung cấp bởi mạng ánh xạ chung, vì không có

[Bảng 3: Kết quả thích ứng miền với các cài đặt khác nhau]

--- TRANG 8 ---
[Bảng 4: Nghiên cứu trường hợp cho map-tuning trên Wiki80 với các ví dụ cụ thể]

huấn luyện bổ sung trước khi suy luận. Kết quả là, mô hình sai lầm nhận ra "Avengers" như truyện tranh thay vì đội siêu anh hùng hư cấu, và do đó thay đổi dự đoán mô hình đúng. Map-tuning cụ thể theo tác vụ, được thích ứng thêm với tác vụ, sửa chữa dự đoán.

5 Nghiên cứu Liên quan

Để nâng cao PLM với kiến thức bên ngoài, có hai mô hình chính: chèn trong quá trình tiền huấn luyện và chèn trong quá trình tinh chỉnh (Yin et al., 2022). Đối với chèn trong quá trình tiền huấn luyện, các nhà nghiên cứu thường xây dựng các mục tiêu có nhận thức kiến thức mới, như dự đoán thực thể (Xu et al., 2021), phân biệt thực thể (Xiong et al., 2020), phân biệt thực thể và quan hệ (Qin et al., 2021), và dự đoán liên kết (Wang et al., 2021b). Theo cách này, kiến thức sẽ được lưu trữ ngầm trong các tham số của PLM. Chèn kiến thức trong quá trình tiền huấn luyện có thể đồng thời cải thiện hiệu suất trên một loạt các tác vụ dựa trên kiến thức downstream. Tuy nhiên, chi phí huấn luyện của mô hình này là đắt đỏ. Lấy các PLM tăng cường kiến thức điển hình LUKE (Yamada et al., 2020) và KEPLER (Wang et al., 2021b) làm ví dụ, mất hơn 3.000 giờ GPU để huấn luyện chúng.

Chèn kiến thức trong quá trình tinh chỉnh là một mô hình tương đối nhẹ, nơi kiến thức bên ngoài thường được sử dụng để tăng cường đầu vào mô hình cho các tác vụ cụ thể (Zhou et al., 2019; Lin et al., 2019; Liu et al., 2020b; Cheng et al., 2021; Kang et al., 2022). Khi chèn kiến thức văn bản không có cấu trúc, một số phương pháp truy vấn thông tin liên quan đến tác vụ từ corpus bên ngoài để tăng cường văn bản đầu vào gốc (Karpukhin et al., 2020; Liu et al., 2020a). Khi sử dụng kiến thức có cấu trúc, như đồ thị kiến thức, các phương pháp hiện có thường áp dụng các phương pháp học biểu diễn kiến thức (Bordes et al., 2013; Lin et al., 2015a) để mã hóa kiến thức có cấu trúc thành embedding, và sau đó hợp nhất các embedding kiến thức này với embedding token đầu vào bằng cách sử dụng các phương pháp chèn kiến thức (Sun et al., 2020; Su et al., 2021; Yasunaga et al., 2021).

Nói chung, các phương pháp chèn kiến thức hiện có chủ yếu nhắm mục tiêu PLM và áp dụng các mô hình nơi kiến thức và mô hình được kết nối chặt chẽ. Hướng tới việc chèn linh hoạt và hiệu quả, chúng tôi nghiên cứu một mô hình mới, chèn kiến thức cắm và chạy, nơi chúng tôi tách rời mô hình và nguồn kiến thức, và sau đó chèn kiến thức vào các mô hình downstream mà không huấn luyện lại mô hình. Nghiên cứu này cũng liên quan đến tinh chỉnh hiệu quả tham số (Liu et al., 2021; Ding et al., 2022) và plugin cho mô hình ngôn ngữ lớn (Xiao et al., 2023; Dathathri et al., 2020; Lauscher et al., 2021; Chronopoulou et al., 2022; Yu et al., 2023; Xu et al., 2023; Alayrac et al., 2022)

--- TRANG 9 ---
trong khi chúng tôi là những người đầu tiên nghiên cứu chèn kiến thức theo cách hiệu quả tham số và cắm và chạy.

6 Kết luận

Trong nghiên cứu này, chúng tôi đề xuất một mô hình mới của chèn hướng tới việc chèn kiến thức linh hoạt và hiệu quả. Trong mô hình này, các mô hình downstream có thể được tăng cường với chi phí tính toán ít, điều này có lợi cho số lượng lớn các mô hình. Chúng tôi đầu tiên đánh giá hệ thống các phương pháp chèn kiến thức hiện có và thấy rằng chúng không phù hợp cho chèn cắm và chạy. Sau đó, chúng tôi đề xuất map-tuning cho mô hình này, hiệu quả chèn kiến thức vào các mô hình downstream để tăng cường chúng.

Có bốn hướng đầy hứa hẹn cho nghiên cứu tương lai về chèn kiến thức cắm và chạy. (1) Làm thế nào chúng ta có thể giảm khoảng cách hiệu suất giữa các phương pháp cho mô hình mới này và những phương pháp cho các mô hình chèn trước đây, trong khi duy trì tính linh hoạt và hiệu quả vượt trội? (2) Bên cạnh kiến thức thực tế, làm thế nào chúng ta có thể hiệu quả cắm các cơ sở kiến thức đa dạng, như corpus văn bản, giọng nói, hình ảnh, và thậm chí các PLM khác? (3) Sau khi chèn kiến thức theo cách cắm và chạy, làm thế nào PLM có thể làm các loại lý luận phức tạp khác nhau dựa trên kiến thức được chèn (Onoe et al., 2023)? (4) Các phương pháp chèn kiến thức cắm và chạy cho các nguồn này có thể được thống nhất, vì vậy chúng ta có thể cắm một sự kết hợp của nhiều nguồn không? Chúng tôi hy vọng nghiên cứu này có thể thu hút sự chú ý và truyền cảm hứng cho nghiên cứu về những vấn đề này.

Giới hạn

Trong bài báo này, chúng tôi trình bày một mô hình chèn kiến thức mới chèn kiến thức cắm và chạy cho PLM. Chúng tôi chỉ ra các phương pháp hiện có không thể được áp dụng tốt cho mô hình mới và đề xuất map-tuning như một khám phá sơ bộ của các phương pháp.

Mô hình chèn kiến thức cắm và chạy có một giới hạn về giả định của nó. Nó giả định rằng một PLM nên được tinh chỉnh cho các tác vụ downstream. Tuy nhiên, PLM quy mô rất lớn có thể thực hiện học zero-shot hoặc học trong ngữ cảnh trên các tác vụ downstream mà không được tinh chỉnh. Nghiên cứu tương lai có thể mở rộng định nghĩa của mô hình được đề xuất để làm cho nó có ý nghĩa trong những tình huống này.

Phương pháp map-tuning có ba giới hạn về khả năng áp dụng của nó. Thứ nhất, chúng tôi không đánh giá map-tuning cho PLM được tiền huấn luyện bởi các mục tiêu mô hình hóa ngôn ngữ khác (ví dụ, mô hình hóa ngôn ngữ nhân quả) bên cạnh MLM. Vì tinh thần của nó có thể dễ dàng được tổng quát hóa cho các mục tiêu mô hình hóa ngôn ngữ khác nhau, chúng tôi để đánh giá này cho nghiên cứu tương lai. Thứ hai, chúng tôi không đánh giá liệu PLM có thể làm lý luận phức tạp (ví dụ, lý luận đa bước) dựa trên kiến thức được chèn bởi map-tuning. Thứ ba, map-tuning được thiết kế để cắm kiến thức sự thật có cấu trúc. Cũng có ý nghĩa để cắm các cơ sở kiến thức đa dạng khác, bao gồm corpus văn bản, giọng nói, hình ảnh, và thậm chí các PLM khác, không được bao phủ bởi nghiên cứu của chúng tôi.

Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia Trung Quốc (Số 2022ZD0116312), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62236004).

Đóng góp Tác giả Zhengyan Zhang, Zhiyuan Zeng, Huadong Wang, và Deming Ye đã viết mã và tiến hành các thí nghiệm. Zhengyan Zhang xây dựng khung thí nghiệm cơ bản bao gồm mã và tập dữ liệu. Zhiyuan Zeng phụ trách các thí nghiệm cắm và chạy và tinh chỉnh. Huadong Wang và Deming Ye cung cấp embedding TransE và PELT tương ứng. Zhengyan Zhang và Zhiyuan Zeng đóng góp vào các thí nghiệm phân tích. Zhengyan Zhang và Zhiyuan Zeng viết bản thảo ban đầu. Yankai Lin, Huadong Wang, Chaojun Xiao, Xu Han, và Zhiyuan Liu đã chỉnh sửa và cải thiện đáng kể bài báo. Peng Li, Maosong Sun, và Jie Zhou cung cấp lời khuyên có giá trị cho nghiên cứu.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo với các nghiên cứu và bài báo được trích dẫn trong toàn bộ nghiên cứu]

--- TRANG 10-16 ---
[Phần còn lại của tài liệu chứa các tài liệu tham khảo, phụ lục với các bảng siêu tham số, kết quả bổ sung, và phân tích chi tiết các thí nghiệm]
