# 2406.00605.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2406.00605.pdf
# File size: 472828 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LongSkywork: A Training Recipe for Efficiently Extending Context Length
in Large Language Models
Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang
Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo Zhu, Yimeng Gan
Rui Hu, Shuicheng Yan, Han Fang, Yahui Zhou∗
Skywork Team, Kunlun Inc.
Abstract
We introduce LongSkywork, a long-context
Large Language Model (LLM) capable of pro-
cessing up to 200,000 tokens. We provide a
training recipe for efficiently extending con-
text length of LLMs. We identify that the criti-
cal element in enhancing long-context process-
ing capability is to incorporate a long-context
SFT stage following the standard SFT stage.
A mere 200 iterations can convert the stan-
dard SFT model into a long-context model. To
reduce the effort in collecting and annotating
data for long-context language modeling, we
develop two novel methods for creating syn-
thetic data. These methods are applied during
the continual pretraining phase as well as the
Supervised Fine-Tuning (SFT) phase, greatly
enhancing the training efficiency of our long-
context LLMs. Our findings suggest that syn-
thetic long-context SFT data can surpass the
performance of data curated by humans to some
extent. LongSkywork achieves outstanding per-
formance on a variety of long-context bench-
marks. In the Needle test, a benchmark for
long-context information retrieval, our mod-
els achieved perfect accuracy across multiple
context spans. Moreover, in realistic appli-
cation scenarios, LongSkywork-13B demon-
strates performance on par with Claude2.1, the
leading long-context model, underscoring the
effectiveness of our proposed methods.
1 Introduction
The advancement of large language models (Tou-
vron et al., 2023a; Anil et al., 2023; Bai et al., 2023;
Du et al., 2021; Wei et al., 2023b), exemplified
by ChatGPT (OpenAI, 2023), is greatly impacting
the world. These models are being used in vari-
ous fields, such as chitchat (Ouyang et al., 2022),
composition (Fyfe, 2023), and dynamic agents (Xi
et al., 2023; Bozkurt, 2023). A key factor driving
this progress is the ability of these models to handle
∗Email: {forename}.{surname}@kunlun-inc.com
Figure 1: Needle Test (Kamradt, 2023) result for
LongSkywork-13B. The Needle Test evaluates the key
information retrieval capability of an LLM.
inputs with extensive context. However, a notable
limitation remains: because of limited resources,
many existing language models are mainly trained
on shorter texts. For example, Llama1 (Touvron
et al., 2023a) is trained on 2K context windows,
while Llama2 (Touvron et al., 2023b) is trained on
4K context windows. Consequently, their effec-
tiveness might be reduced when faced with longer-
form prompts that are frequently encountered in
real-world applications.
The demand for LLMs that can understand and
generate lengthy sequences efficiently has grown
significantly, given their widespread use in appli-
cations that involve complex scenarios (Liu et al.,
2023a; Beltagy et al., 2020; Milne-Ives et al., 2020).
Consequently, researchers have dedicated substan-
tial efforts to enhance the Transformer architecture
with long-context processing capability (Roziere
et al., 2023). This includes techniques such as
continual pretraining with long data (Xiong et al.,
2023), positional embedding interpolation (Chen
et al., 2023a; Peng et al., 2023), memory-efficient
attention such as Flash attention (Dao et al., 2022).
However, the most powerful long-context models
currently available are proprietary systems, such as
GPT4 (OpenAI, 2023), Claude1and Moonshot2.
1https://www.anthropic.com/news/claude-2-1
2https://kimi.moonshot.cn/arXiv:2406.00605v1  [cs.CL]  2 Jun 2024

--- PAGE 2 ---
There remains a need for more information on how
to develop high-performing, long-context language
models for public use. Most open-sourced mod-
els that can process over 100k tokens are either
base models unsuitable for assisting real-world
tasks, or fine-tuned only on long dialogues (Zeng
et al., 2022), exhibiting poor comprehension of
lengthy contexts. Moreover, high-quality, natural
long-form datasets are scarce for pretraining and
supervised fine-tuning(SFT), presenting challenges
in training powerful long-context LLMs.
To bridge this gap, we introduce LongSkywork,
a long-context LLM capable of processing up to
200,000-token context windows, which is specif-
ically tailored to facilitate in-depth comprehen-
sion and problem-solving across diverse scenar-
ios. Furthermore, we offer critical insights into
the developmental framework of LongSkywork.
Our findings suggest that the model’s ability to
handle extended contexts can be systematically
augmented through a four-stage process. Beyond
the conventional stages of pretraining and super-
vised fine-tuning, our approach incorporates spe-
cialized stages for long-context pretraining and
long-context supervised fine-tuning. These addi-
tional stages are meticulously designed to bolster
the model’s proficiency in managing extended tex-
tual segments. Significant performance enhance-
ments during both long-context training phases
were observed upon the integration of synthetically
generated data. For the long-context pretraining
phase, we propose a novel method where docu-
ments are splited into segments, which are then
arranged in an interleaved fashion to form pseudo
long-context pretraining samples, a technique we
term “Chunk Interleaved Pretraining (CIP)”. This
technique obliges the model to process and inte-
grate information over considerable textual dis-
tances.
During the long-context supervised fine-tuning
phase, we present a method for generating queries
and answers from tables. These tables are automati-
cally synthesized by the program and referred to as
“Synthetic Lengthy Tables” (SynL). This method
mandates that the model not only extract relevant
information from extensive tabular data but also
engage in complex reasoning tasks under multi-
ple constraints. Additionally, we engineer specific
tasks to enhance the model’s global reasoning ca-
pabilities, such as the transformation and sorting
of tables. These exercises are intricately designedto compel the model to develop a more comprehen-
sive and nuanced understanding of intricate data
structures and their inherent relationships.
Our extensive experiments indicate that the
long-context SFT stage is crucial in enhancing
the model’s long context ability. Our approach
is efficient, necessitating only hundreds of long-
context pretraining and SFT iterations to furnish
a short-text LLM with long context capability.
LongSkywork performs well on the Needle in a
Haystack test, as shown in the Figure 1. Further-
more, we validate our model using both the long-
context benchmark InfiniteBench (Zhang et al.,
2023) and a real-world evaluation collected from
online long-context question-answering. The re-
sults show that LongSkywork-13B has strong long-
context retrieval capabilities, outperforming GPT4-
128K and Claude2.1-200K in three retrieval-based
tasks and achieving average results on par with
Moonshot in InfiniteBench. In real-world evalua-
tions, LongSkywork-13B performs comparably to
Claude2.1, but with significantly fewer parameters.
The contributions of this paper are summarized
as follows:
•We introduce LongSkywork, a long-context
LLM with up to 200K context windows. We
provide a training recipe for efficiently extend-
ing context length in LLMs.
•We propose methods to construct synthetic
data to accelerate the learning of long con-
text for solving the data scarcity problem on
training long-context LLMs.
•Our models show powerful capability in tasks
that require long context retrieval and com-
prehensive ability. Our model is on par with
GPT-4-128K and Claude2.1 in InfiniteBench.
2 Related work
2.1 Long-context LLMs
Adapting Transformers ( ??) and LLMs (Touvron
et al., 2023a) to handle long contexts is an im-
portant area of research. There are two lines of
approaches related to our methods. The first one is
the extended Rotary Position Embedding(RoPE).
RoPE, proposed by Su et al. (2024), is widely
used in popular LLMs, such as Llama (Touvron
et al., 2023a,b) and PaLM (Anil et al., 2023). It
uses absolute positional embedding to represent

--- PAGE 3 ---
Pretraining
StageLong-
context
Pretraining
StageSFT StageLong-
context
SFT Stage Adapt to new positional
embedding Align to human
preferenceEnhance long-context
information processing
capabilityPretrainig Corpus
Directly concatenate to form the pretraining sample
(megatron-lm default)Previous method : Concatenate
multiple short documents to
directly create a long-context
training sample.
Our method : Chunk documents of
varying lengths and interleave
them to create a long-context
training sample.
2. Form the pretraining sample in an interleaved manner .
Compel the model to attend to very long ranges of information.1: Chunking documents into
text with dif ferent lengthInformation retrieval
Prompt1 : Select the youngest
people in this table, give me the
email of her/him. 
Response1 : The youngest
people I find in this table is
adam, his email is adamxxx
CoT Reasoning
Prompt2 : Calculate the age
difference between the oldest male
and the youngest female.
Response2 : The oldest male is
James, his age is 65. The youngest
female is Emmy , her age is 22. So
the age dif ference is 65 - 22 = 43. Global Comprehension
Prompt3: Sort this table by
age.
Response3: 
| name | age | gender | email |
| Emmy | 22 | female | emmyzz |
| Cora   | 27  | female |cora123 |Synthetic T able 
Figure 2: An illustration of our proposed methods. Except regular pretaining stage and SFT stage, we add two
long-context related stages to empower LLMs with capability of long-context information processing.
relative position and provides a clear theoretical
explanation in a complex space. Recently, many
researchers found that RoPE positional embedding
can effectively extend the inference context length
with minimal or no finetuning (Chen et al., 2023a;
Peng et al., 2023). Chen et al. (2023a) proposed
Positional Interpolation (PI), which applies linear
scaling on each positional index from n to n/k,
densifying the representation space to extend the
farthest length by k times. However, simple lin-
ear scaling may result in limited space, which can
impair the model’s understanding of fine-grained
distances. Based on Neural Tangent Kernel the-
ory (Jacot et al., 2018), bloc97 (2023) proposed
to extend the extrapolated length by adjusting the
base in the position function. These methods are re-
ferred to as NTK-aware Scaling RoPE (NTK-aware
RoPE), which combines high-frequency extrapo-
lation and low-frequency interpolation. Due to its
simplicity and efficiency, many long-context LLMs
exploit this approach for extrapolation (Roziere
et al., 2023; Xiong et al., 2023). Yarn (Peng et al.,
2023) combines NTK-aware RoPE and positional
interpolation. This method suggests not interpolat-
ing the higher frequency dimensions, while always
interpolating the lower ones.
The other line of related research involves studythe training data in extending context windows
of LLMs. PI and YARN models are finetuned
on the PG19 dataset (Rae et al., 2019), which is
a book dataset with significantly longer context.
However, training on a different distribution than
the pretraining stage may have a negative impact
on the model’s performance. Xiong et al. (2023)
suggested combining multiple short contexts to
achieve the desired length, which also makes train-
ing more efficient. They also found that increasing
the proportion of long text does not necessarily im-
prove the model’s performance on long context. In
this paper, we show that by merely dividing the
text into chunks and interleaving them to form a
long-context pretraining sample, we can improve
the model’s ability to handle long context without
altering the training distribution of the pretrain-
ing phase. Furthermore, previous works mainly
focus on the long-context pretraining stage and pro-
vides limited insights into how the long-context
SFT stage impacts the model’s performance.
2.2 Synthetic data used in LLMs
Training LLMs on human-collected data remains
the predominant approach. However, human-
generated data is limited and hard to collect, es-
pecially in long-context language modeling scenar-

--- PAGE 4 ---
ios (Xiong et al., 2023). Besides, human-generated
data will be exhausted in the next few years ( ?).
Therefore, exploring the use of synthetic data to
assist in LLM training and alignment is a promis-
ing research area. Cao et al. (2023) found that
GPT4 can almost perfectly reconstruct the origi-
nal sentences from scrambled ones. As a result,
researchers in the community speculate that GPT4
may utilize synthetic data for learning. Addition-
ally, there are ample studies of using synthetic data
during the SFT stage. Wei et al. (2023a) discovered
that incorporating synthetic math data during SFT
significantly outperforms finetuning using only hu-
man data. Shao et al. (2023) suggested using a few
handcrafted examples to prompt the model to gen-
erate more examples on its own and select effective
demonstrations to elicit better reasoning. Sun et al.
(2023) proposed to prompting LLMs to generate
alignment SFT data based on several seed exam-
ples. However, those methods only focuses on the
adoption of LLMs to generate synthetic data. The
use of programming to generate synthetic data to
assist in the SFT stage has not been extensively
researched. In this paper, we use synthetic data in
both pretraining and SFT stages, and shows that
it can significantly boost the power of LLMs in
long-context retrieval and understanding.
3 Methodology
3.1 Preliminaries
Traditionally, training an LLM can be split into two
stages. The first stage is referred to as the pretrain-
ing stage. It uses an auto-regressive language objec-
tive to generate a sequence y={y1, y2, y3, . . . y n}
based on the previously generated tokens. Assum-
ing that the language model is parameterized by
θ, the auto-regressive process can be described as
follows:
pθ(y) =pθ(yt, y<t).
The pretraining stage is seen as injecting most
of the knowledge into LLMs (Zhou et al., 2023).
The second stage is referred to as the supervised
finetuning stage, which can be seen as teaching the
model to align format. An SFT training sample
consists of a query, denoted as x, which is used as
input, and an output ythat the model wants to gen-
erate conditioned on that prompt. Therefore, the
training process in the SFT stage can be formalized
as:
pθ(y|x) =pθ(yt, y<t|x),
Figure 3: Loss curves during long-context pretraining
stage. The training loss converges in just a few hundred
iterations. NTK-aware RoPE converges faster than di-
rectly training.
where xis masked and only yproduces gradient
to update the parameters of the model.
Previous work (Xiong et al., 2023) adds a long-
context pretraining stage after the traditional pre-
training stage. The long-context pretraining stage
is designed to help the model adjust to the new
context, mainly the new parameter of RoPE po-
sitional embedding. As shown in Figure 3, only
hundreds of long-context training iterations make
the training loss converge. PI and YARN use book
data to conduct long-context training. We empiri-
cally find that it makes the long-context pretraining
stage learn a different distribution than that in pre-
training stage. Xiong et al. (2023) demonstrated
that natural long context, such as a book or code
repository, isn’t necessary during the long-context
training stage. Instead, we can concatenate short
contexts to form a longer one. Surprisingly, this
method can outperform a dataset with a higher long
text ratio. Following Xiong et al. (2023), we use
the training data distribution from the pretraining
stage and concatenate it to the long context to form
the training data for the long context pretraining
stage.
After conducting a standard SFT stage, we
found that implementing a long-context SFT stage
significantly improves retrieval and comprehen-
sion of long-context information. Xiong et al.
(2023) suggested merging normal SFT samples
with long-context SFT samples and training them
together.However, we noticed that when blending
normal SFT with long-context SFT data, the gra-
dient of the short context tends to overshadow the

--- PAGE 5 ---
Table 1: Statistics of average token counts for samples
and responses in the normal SFT and long-context SFT
corpora after packing. To create the training data for
the long-context SFT stage, we concatenate multiple
SFT samples together to achieve a length of approxi-
mately 100K tokens. Regular SFT data refers to SFT
data commonly used for alignment. Long-context SFT
data refers to SFT data that focuses on long-context
modeling, such as book QA and book summarization.
Data #Number #Tokens #Resp. Tokens
Long-context SFT 21398 88266 1959
Regular SFT 8484 98994 85582
long-context SFT data and the model performs
poorly in long-context tasks. This happens because
the SFT stage only calculates the gradient for the
response. As seen in Table 1, long-context SFT
samples usually have a shorter response relative
to their prompts. These prompts often include a
context from Wikipedia or a book, followed by a
related question. The answer to the question, typ-
ically brief, is often the key information in this
context. Conversely, normal SFT data often in-
volves creatively written queries with relatively
longer responses to their prompts. Moreover, train-
ing a model with 13B parameters in a 4K context
window yields a throughput of approximately 1775
tokens per second per GPU. However, when train-
ing with a 100K context window, the speed drops
to about 180 tokens per second per GPU. If we
combine short SFT with long-context SFT together
and only perform one long-context SFT stage, a
considerable amount of training resources would
be wasted. Thus, we suggest using a short-context
window, like 4K, for alignment. Then, we use
a long context windows, such as 100K, to train
a mix of long-context SFT data and normal SFT
data. We incorporate normal SFT samples in the
long-context SFT stage to preserve the model’s
alignment capability.
3.2 Synthetic data in the long-context
continued pretraining stage
Xiong et al. (2023) suggested joining short texts
into longer ones to maintain a similar data distribu-
tion as the pretraining stage. However, in our study,
we found that only joining short texts during pre-
training causes the model to learn short patterns and
struggle with understanding long context dependen-
cies. We suggest a simple yet effective method to
speed up this process. This method uses short textsto learn long dependencies. Specifically, we split
several relatively short documents into segments
and then rearrange these segments in an interleaved
way to form a longer training sample. Assume
D={d1, d2, ..., d m}is a pretraining dataset con-
taining mdocuments. To improve the narrative
flow, we assume that the token count of all doc-
uments is n. Therefore, di={x1
i, x2
i, . . . .xn
i},
where x represents words in the document. In
what follows, we use the example of n=3 to il-
lustrate our method. Previous methods for com-
bining multiple short documents to form a longer
document involve directly connecting dlong =
{x1
1, x1
2, x1
n, x2
1, x2
3, x2
n, x3
1, x3
2, x3
n}. Our proposed
method is to firstly split the document into multiple
chunks, di={ci
1, ci
2, ci
3},dj={cj
1, cj
2, cj
3}where
c denotes the document chunk, which consists of
tokens. Then we interleaved organize the chunks
from multiple documents into a single one like
dlong ={ci
1, cj
1, ci
2, cj
2, ci
3, cj
3}. This process com-
pels the model to focus on relevant long-distance
information, thereby minimizing loss during the
generation of the current token in the pre-training
phase. Note that the word order in the document is
preserved, which allows the attention mechanism to
attend to the necessary information for generating
current tokens. If the length exceeds the maximum
length set, it is truncated to fit within the limit. If
the length is shorter than the maximum length, it is
padded to reach the maximum.
3.3 Synthetic data in long-context SFT stage
Creating lengthy and meaningful supervised data
is a costly and labor-intensive task. This raises
the question: can we utilize synthetic data to en-
hance the model’s capacity to process extensive
contextual information?
In this paper, we propose using program-
generated synthetic data to improve the ability of
Language Models to retrieve and understand long-
context information. We define synthetic long-
context SFT data as a task for processing tables.
This is because it’s simple to generate a table pro-
grammatically and control its attributes such as
length and difficulty. The input table could be an
IP address table, an information registration form,
or even an HTML or markdown file with multiple
fields and lines. To accommodate information of
varying lengths, we generate tables ranging from
2k to 100K tokens. The fields and data in the table

--- PAGE 6 ---
are generated using Faker3. Detailed descriptions
of the three main types of tasks we constructed will
be provided.
•Information retrieval task : We prompt the
language model to retrieve information from
a table based on certain constraints, such as
"Select the youngest people in this table and
give me their email."
•CoT reasoning task : We prompt the
language model to generate a Chain-of-
thought (Wei et al., 2022) reasoning step to
answer the question. As an example, when
asked with "Calculating the age difference
between the oldest male and the youngest fe-
male", the model first identifies the ages of the
oldest male and youngest female. The final
result is then computed by subtracting these
two values.
•Global comprehension task : This task ne-
cessitates the model to possess a thorough un-
derstanding of the structure and information
presented in the table to offer precise results.
For instance, when prompted to sort the table
by age, the language model needs to have a
comprehensive understanding of the table and
then give the correctly sorted table.
4 Experiments
4.1 Chunks interleaved long-Context
pretraining (CIP)
In line with previous work (Roziere et al., 2023;
Xiong et al., 2023), we maintain the core Skywork
architecture mostly unchanged for long context pre-
training. We only make a required adjustment to
the positional encoding, enabling the model to fo-
cus on longer context. Specifically, we lower the
rotation angle (controlled by the hyperparameter
"base frequency b"), which reduces the diminish-
ing effect of RoPE for distant tokens. We changed
the base from 10000 to 2600000, as advised by Liu
et al. (2023b), to scale a pretrained model from a
4K context window to extend it to a 200K context
window. For additional information, individuals
who are interested can refer to their paper.
Training large language model on long context
windows can be very costly. In our case, we uti-
lized a Skywork-3B model that was trained on 64K
context windows of Redpajama (Computer, 2023)
3https://github.com/joke2k/fakerweb text to evaluate the efficacy of our proposed
methods. To create our final document, we com-
bine multiple documents until they reach a length
of approximately 64K. We then pad the document
to 64K. If the final document exceeds 64K, we sim-
ply remove any excess tokens. It is interesting to
discover the impact of the number of chunks per
document on the model’s performance. We heuris-
tically divide the text input into 2 chunks, 4 chunks,
and 8 chunks. The models trained with these meth-
ods are referred to as 3B-CIP-2 ,3B-CIP-4 , and
3B-CIP-8 . The baseline model is trained on a cor-
pus where documents are directly concatenated to
long ones, which we denotes as 3B-DC. The train-
ing corpus used for both the baseline model and our
model is identical. Therefore, the baseline model
can also be considered as a model with Chunk1.
We use the AdamW optimizer (Loshchilov and
Hutter, 2017) and set the maximum learning rate
to 1e-5, with a learning rate decay to 1e-6. We
train each model for 500 iterations. The perfor-
mance evaluation of the models after long-context
continued pretraining stage focus on its ability to
retrieve key information. Specifically, we employ
a five-shot manner to evaluate the pretrained mod-
els, as introduced by Team (2023). As shown in
Table 2, our methods can significantly improve the
few-shot information retrieval capability of a pre-
trained models. 3B-CIP-2 achieve a retrieval score
of 0.514 which increase baseline model accuracy
of 0.338 by about 52.07% . In addition, we evaluate
each model using Chinese and English language
modeling tasks, as well as language understanding
tasks. As seen in Table 3, compared to the baseline
method, our proposed method slightly increases the
validation loss for English and Chinese language
modeling. Specifically, the English web text valida-
tion loss of the 3B-CIP-2 model is 2.101, which is
0.33% lower than the baseline model. The Chinese
web text validation loss of the 3B-CIP-2 model
is 2.152, which is 0.13% lower than the baseline
model. When measuring the capabilities of the En-
glish and Chinese language understanding by two
popular benchmarks, C-EV AL (Huang et al., 2023)
and MMLU (Hendrycks et al., 2020), we find that
our methods performed on par with the baseline
model in both tasks. Due to the objective of im-
proving the model’s capacity to handle extensive
contextual information, a slight decrease in normal
language modeling task is anticipated. As we find
that 3B-CIP-2 performs the best overall, we follow

--- PAGE 7 ---
Models 1K 2K 4K 16K 64K A VG.
3B-DC 0.68 0.45 0.40 0.10 0.06 0.338
3B-CIP-2 0.93 0.79 0.58 0.14 0.13 0.514
3B-CIP-4 0.92 0.73 0.53 0.12 0.17 0.494
3B-CIP-8 0.90 0.68 0.60 0.19 0.09 0.492
Table 2: Results of the line retrieval task. We evaluate
different models using various context windows ranging
from 1K to 64K to measure its information retrieval
capability.
Models En LM ↓Zh LM ↓C-EV AL ↑MMLU ↑
3B-DC 2.094 2.149 28.47 28.09
3B-CIP-2 2.101 2.152 28.57 28.10
3B-CIP-4 2.100 2.156 28.29 28.12
3B-CIP-8 2.105 2.157 28.20 27.95
Table 3: Results of the language model and language
understanding tasks. "EN LM" denotes English web
text validation loss, "ZH LM" denotes Chinese web text
validation loss.
this setting to train our larger models.
4.2 Synthetic Long-context SFT (SynL)
Following an long-context pretraining stage, we
performs an SFT stage to align the model, with the
context window set to 4k. Please note that during
the long-context continued pretraining stage, the
SFT stage, and the long-context SFT stage, the
model structure and base in RoPE remain constant,
set at 260000.
In the long-context SFT stage, the data source
can be divided into three categories. The first cate-
gory includes normal SFT data, maintained to pre-
serve alignment capability. The second category
involves naturally collected long-context SFT data,
such as book QA and book summarization. We re-
fer to this data as true long-context SFT data. The
third category pertains to our synthetic long-context
SFT data. Based on our preliminary experiments,
we found that maintaining the ratio of normal SFT
data at 20% yielded the best performance. There-
fore, in the following tests, we will keep the ratio
of normal SFT data unchanged.
We evaluate the long-context capability of each
model on InfiniteBench (Zhang et al., 2023), which
is designed to evaluate the capabilities of language
models to process, understand, and reason over
super long contexts (100k+ tokens). This bench-
mark involves tasks related to long-context retrieval
and understanding in English, Chinese, Code, and
Math.4.2.1 Baselines
We compared our methods with several proprietary
model APIs, such as GPT4, Claude, Moonshot, and
several open-sourced baseline models as follows:
•Yarn-Mistral-7B-128K (Peng et al., 2023):
This model is a long-context model presented
by NousResearch4. The model is based
on Mistral (Jiang et al., 2023) and using
Yarn (Peng et al., 2023) to extend the context
windows to 128K.
•Yarn-Llama2-13B-128K (Peng et al., 2023):
This model is based on Llama2-13B (Touvron
et al., 2023b) and using Yarn to extend the
context windows to 128K.
•LongSkywork-13B-T : This model is a vari-
ant of LongSkywork-13B which use 20% nor-
mal SFT data and 80% true long-context SFT
data collected from LongAlpaca (Chen et al.,
2023b) and LongCollection5.
•LongSkywork-13B-S : This model is a variant
of LongSkywork-13B, which uses 20% nor-
mal SFT data and 80% long-context synthetic
SFT data.
•LongSkywork-13B-S&T : This model is a
variant of LongSkywork-13B, which uses
20% normal SFT data, 40% true long-context
SFT data, and 40% synthetic long-context
SFT data.
The results are outlined in Table 4. Our proposed
synthetic data is highly efficient. LongSkywork-
13B-S achieves an average score of 37.8%, out-
performing LongSkywork-13B-T by a large mar-
gin, which scores 24.7%. We find synthetic data
can significantly enhance the long-context retrieval
capability of an LLM, raising it from 49.3% to
94.0% on average across three retrieval-based tasks.
In addition to the retrieval-based tasks, synthetic
data can also improve the Chinese QA task from
11.6% to 15.1%, demonstrating that our methods
can enhance the long-context understanding of an
LLM. However, we observed a decrease in the
EN.sum task from 16.4% to 8.5% because there
is no summary-related data in the synthetic data.
This leaves the construction of summary-related
data as future work. Upon merging synthetic data
4https://nousresearch.com/
5https://huggingface.co/datasets/togethercomputer/Long-
Data-Collections

--- PAGE 8 ---
Models Re.PKey Re.Num Re.KV En.Dia En.Sum En.MC En.QA Zh.QA Math.Find C.Debug Re.Avg Avg
GPT-4* 100.0% 100.0% 89.0% 22.2% 14.7% 67.3% 8.5% 24.3% 60.0% 39.6% 96.3% 52.6%
Claude2.1* 97.8% 98.1% 65.4% 46.5% 14.5% 62.9% 12.0% 10.5% 32.3% <5% 87.1% 44.5%
Moonshot* 98.1% 95.4% 34.2% 16.5% 17.9% 72.5% 11.5% 17.3% 12.6% 18.0% 87.1% 39.4%
Y-Mistral-7B* 92.7% 56.6% <5% 9.6% 9.1% 28.0% 7.5% 14.4% 17.1% <5% 51.4% 24.5%
Y-Llama2-13B 44.0% <5% <5% 5.0% 12.0% 32.0% 8.7% 6.4% 11.0% <5% 18.0% 13.4%
LongS-13B-T 45.0% 98.0% <5% 7.0% 16.4% 37.0% 6.2% 11.6% <5% 17.0% 49.3% 24.7%
LongS-13B-S 100.0% 95.0% 87.0% 6.0% 8.5% 33.0% 5.6% 15.1% <5% 26.0% 94.0% 37.8%
LongS-13B-S&T 86.0% 85.0% 69.0% 12.0% 14.9% 40.0% 6.8% 18.1% 30.0% 28.0% 80.0% 39.0%
LongS-13B 100.0% 99.0% 98.0% 10.0% 14.0% 46.0% 31.2% 28.0% 29.0% 20.0% 99.0% 47.5%
Table 4: Results on InfiniteBench: we removed the tasks of MATH.Calc and Code.Run, as most models showed less
than 5% accuracy in these tasks. Performing long context inference requires a significant amount of resources and is
time-consuming. Therefore, when evaluating tasks with more than 100 numbers, we only consider the performance
of the first 100 samples. * denotes we use the result test by official repertory of InfiniteBench.
with actual long-SFT data, we observe an uplift
in the average score, albeit with a minor decrease
in retrieval-based tasks. We believe this is due
to an imprecise data mixing ratio. Our refined
model, LongSkywork-13B, enhances this data mix-
ing and incorporates selected annotator data. The
end result is a performance of 47.5%, outperform-
ing both Moonshot and Claude2.1. Note that com-
pared to other proprietary models, our model may
have fewer parameters, specifically just 13 billion
parameters.
To evaluate the effectiveness of LongSkywork
in realistic-usage scenarios, we create a benchmark
based on actual use cases from the Tiangong AI
Reading website6. We collect 856 questions of var-
ious types, including questions about details in a
document, document summarization, and questions
that cannot be answered based on the document.
We then ask the annotators to provide a reference
answer for each question. We used GPT-4 for eval-
uation, comparing the reference answers to the an-
swers provided by the models. The results are dis-
played in Table 5. Our LongSkywork-13B model
attains an overall score of 69.24, which is slightly
lower than the state-of-the-art long-context model
Claude2.1, scoring 72.27. This indicates that our
method demonstrates effective performance when
it comes to real-world scenario questions.
4.2.2 Limitations and future work
In this section, we discuss some limitations of our
proposed methods for future research. We iden-
tify two issues with LongSkywork as shown in
Table 6. First, it struggles to effectively handle
non-answerable questions, likely due to a limited
number of non-answerable training samples. Sec-
ond, it has difficulty with long-context complex rea-
6https://work.tiangong.cn/chatdoc/d/doc/indexModels Wrong Part.Right Right Overall
Claude2.1 4.67% 34.58% 60.75% 72.27
LongS-3B 10.51% 43.22% 46.03% 60.44
LongS-13B 7.13% 35.28% 57.48% 69.24
Table 5: Results on a realistic application scenario
benchmark. Part.Right denotes partially correct. The
score are judged by GPT-4. Overall score are calculated
by (3 * Right + Part.Right ) / 3. LongS stands for our
LongSkywork model.
Input 1 :In which countries and regions is the market size
larger for games independently developed in our country in
2022?
Reference Answer :The report did not specify any particular
country or region, only mentioning that developed markets like
the United States and emerging markets such as the Middle
East, Africa, and Latin America have great potential.
LongSkywork-13B :In 2022, the games independently de-
veloped by our country have the largest market share in the
United States, accounting for 32.31%; followed by Japan, ac-
counting for 17.12%; South Korea accounts for 6.97%.
Input 2 :How many people died in the 9/11 events, excluding
those in the New York World Trade Center and the Pentagon
in Washington?
Reference Answer :The "9 ·11" incident resulted in a total
of 3126 deaths. Of these, 2550 people died in the World
Trade Center in New York; 343 people died on American
Airlines Flight AA11 and United Airlines Flight UA175; 125
people died in the Pentagon in Washington; 64 people died
on American Airlines Flight AA77; 44 people died on United
Airlines Flight UA93 in Pennsylvania. Therefore, excluding
the World Trade Center in New York and the Pentagon in
Washington, a total of 3126-2250-125=751 people died.
LongSkywork-13B :About 343 people died in the "9 ·11"
event, excluding the New York World Trade Center and the
Pentagon in Washington.
Table 6: Case studies on two limitations of
LongSkywork-13B.
soning and calculations, possibly due to the chal-
lenge of deriving correct reasoning steps from a
13B parameter base model. In the future, we will
strive to enhance the two aforementioned aspects
of LongSkywork.

--- PAGE 9 ---
5 Conclusion
In this paper, we present the training recipe of
LongSkywork and propose two methods for con-
structing synthetic data to enhance the long context
retrieval and understanding capabilities of LLMs.
The first method, referred to as CIP, reorganizes
chunks of training documents to create a long train-
ing sample. This approach helps LLMs learn long-
term dependencies more effectively. The second
method, referred to as SynL, generates a variety
of long-context question pairs, which can enhance
reasoning and tackle the issue of data scarcity com-
mon in long-context SFT. We conduct extensive
experiments to evaluate the effectiveness of the pro-
posed methods. In InfiniteBench, LongSkywork-
13B achieves an average score of 47.5, matching
or exceeding the performance of top-tier propri-
etary LLM APIs. In a real-world usage evaluation,
our LongSkywork-13B model performance only
slightly decreases compared to Claud2.1. We also
pinpoint two limitations of LongSkywork: han-
dling non-answerable questions and dealing with
complex, long reasoning problems. We will priori-
tize these topics in our future work.
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
bloc97. 2023. Ntk-aware scaled rope.
Aras Bozkurt. 2023. Generative artificial intelligence
(ai) powered conversational educational agents: The
inevitable paradigm shift. Asian Journal of Distance
Education , 18(1).
Qi Cao, Takeshi Kojima, Yutaka Matsuo, and Yusuke
Iwasawa. 2023. Unnatural error correction: Gpt-4
can almost perfectly handle unnatural scrambled text.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
8898–8913.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context windowof large language models via positional interpolation.
arXiv preprint arXiv:2306.15595 .
Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang,
Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b.
Long alpaca: Long-context instruction-following
models. https://github.com/dvlab-research/
LongLoRA .
Together Computer. 2023. Redpajama: an open dataset
for training large language models.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems ,
35:16344–16359.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021.
Glm: General language model pretraining with
autoregressive blank infilling. arXiv preprint
arXiv:2103.10360 .
Paul Fyfe. 2023. How to cheat on your final paper:
Assigning ai for student writing. AI & SOCIETY ,
38(4):1395–1405.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing. arXiv preprint arXiv:2009.03300 .
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei
Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,
Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu,
Maosong Sun, and Junxian He. 2023. C-eval: A
multi-level multi-discipline chinese evaluation suite
for foundation models. In Advances in Neural Infor-
mation Processing Systems .
Arthur Jacot, Franck Gabriel, and Clément Hongler.
2018. Neural tangent kernel: Convergence and gen-
eralization in neural networks. Advances in neural
information processing systems , 31.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Gregory Kamradt. 2023. Needle in a haystack - pressure
testing llms.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023a. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172 .
Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,
Xipeng Qiu, and Dahua Lin. 2023b. Scaling
laws of rope-based extrapolation. arXiv preprint
arXiv:2310.05209 .

--- PAGE 10 ---
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Madison Milne-Ives, Caroline de Cock, Ernest Lim,
Melissa Harper Shehadeh, Nick de Pennington, Guy
Mole, Eduardo Normando, and Edward Meinert.
2020. The effectiveness of artificial intelligence con-
versational agents in health care: systematic review.
Journal of medical Internet research , 22(10):e20346.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models. arXiv preprint
arXiv:2309.00071 .
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
and Timothy P Lillicrap. 2019. Compressive trans-
formers for long-range sequence modelling. arXiv
preprint arXiv:1911.05507 .
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Zhihong Shao, Yeyun Gong, Yelong Shen, Min-
lie Huang, Nan Duan, and Weizhu Chen. 2023.
Synthetic prompting: Generating chain-of-thought
demonstrations for large language models. arXiv
preprint arXiv:2302.00618 .
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing , 568:127063.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
alignment of language models from scratch with
minimal human supervision. arXiv preprint
arXiv:2305.03047 .
LongChat Team. 2023. How long can open-source llms
truly promise on context length?
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and
Quoc V Le. 2023a. Simple synthetic data reduces
sycophancy in large language models. arXiv preprint
arXiv:2308.03958 .
Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu,
Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng,
Weiwei Lü, Rui Hu, et al. 2023b. Skywork: A more
open bilingual foundation model. arXiv preprint
arXiv:2310.19341 .
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen
Ding, Boyang Hong, Ming Zhang, Junzhe Wang,
Senjie Jin, Enyu Zhou, et al. 2023. The rise and
potential of large language model based agents: A
survey. arXiv preprint arXiv:2309.07864 .
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
et al. 2023. Effective long-context scaling of founda-
tion models. arXiv preprint arXiv:2309.16039 .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Xinrong Zhang, Yingfa Chen, Shengding Hu, Qihao Wu,
Junhao Chen, Zihang Xu, Zhenning Dai, Xu Han,
Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2023.
Infinitebench: 128k long-context benchmark for lan-
guage models.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .
