# 2307.08303.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2307.08303.pdf
# File size: 856809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Soft Prompt Tuning for Augmenting Dense Retrieval with Large
Language Models
Zhiyuan Pengâˆ—
Santa Clara University
Santa Clara, USA
zpeng@scu.eduXuyang Wuâˆ—
Santa Clara University
Santa Clara, USA
xwu5@scu.edu
Qifan Wang
Meta AI
Menlo Park, USA
wqfcr@meta.comYi Fangâ€ 
Santa Clara University
Santa Clara, USA
yfang@scu.edu
ABSTRACT
Dense retrieval (DR) converts queries and documents into dense
embeddings and measures the similarity between queries and docu-
ments in vector space. One of the major challenges in DR is the lack
of domain-specific training data. While DR models can learn from
large-scale public datasets like MS MARCO through transfer learn-
ing, evidence shows that not all DR models and domains can benefit
from transfer learning. Recently, researchers have resorted to large
language models (LLMs) to improve the zero-shot and few-shot DR
models. However, the hard prompts or human-written prompts uti-
lized in these works are suboptimal and the generated weak queries
are often sensitive to the prompts. To tackle this, we propose soft
prompt tuning for augmenting DR (SPTAR1): for each task, we
leverage soft prompt-tuning to optimize a task-specific soft prompt
on limited ground truth data and then prompt the LLMs to tag
unlabeled documents with weak queries, yielding weak document-
query pairs to train task-specific dense retrievers. We design a filter
to select high-quality example document-query pairs in the prompt
to further improve the quality of weak tagged queries. To the best
of our knowledge, there is no prior work utilizing soft prompt
tuning to augment DR models. Moreover, unlike much of the ex-
isting work, ours is based on popular open-source LLMs to ensure
reproducible and deterministic results. Our experimental results
demonstrate that SPTAR outperforms both unsupervised baselines
and the recently proposed LLMs-based augmentation method for
DR.
CCS CONCEPTS
â€¢Information systems â†’Information retrieval ;â€¢Computing
methodologiesâ†’Natural language generation .
KEYWORDS
Large Language Models, Dense Retrieval, Prompt Tuning, Data
Augmentation
1 INTRODUCTION
Information retrieval (IR) plays a pivotal role in a wide array of
applications, ranging from prominent web search engines such as
âˆ—Both authors contributed equally to this research.
â€ Yi Fang is the corresponding author.
1https://github.com/zhiyuanpeng/SPTAR.gitGoogle and Bing to personalized recommendation systems like
Walmartâ€™s product recommendations and Apple Musicâ€™s song sug-
gestions. Traditional IR methods, like TF-IDF and BM25 [ 39], are
built on token-level similarity matching, which can sometimes fall
short due to a lexical gap [ 1]. This gap occurs when semantically
similar terms, such as synonyms, are overlooked because of their
lexical differences. This oversight can potentially impact the quality
of search results and the user experience.
Given these constraints, researchers have turned to advance-
ments in deep learning to tackle the lexical gap in conventional
IR. One notable approach is Dense Retrieval (DR), which aims to
capture the overarching semantic essence of content rather than fix-
ating on individual tokens. DR models like dense passage retrieval
(DPR) [ 17] and ColBERT [ 18,41] encode each query or document
into a dense vector, with the dimensionality determined by the
neural networks. In practice, dense retrievers pre-compute docu-
ment embeddings and construct an approximate nearest neighbor
(ANN) index for rapid search. When a new query is introduced,
only its embedding is computed and subsequently processed by the
ANN search system. Unlike TF-IDF and BM25, DR places greater
emphasis on assessing the similarity of the overall semantic context.
While DR methods have made strides in bridging the lexical
gap, they are still constrained by the limited availability of domain-
specific training data, hindering their performance in specialized
domains. Although some researchers have proposed to leverage
transfer learning to mitigate this challenge, studies [ 8,48] indi-
cate that not all DR models and domains can benefit from transfer
learning equally. Recently, LLMs like CPT-3 [ 4], LLaMA [ 49], and Vi-
cuna [ 5] have demonstrated potent zero-shot and few-shot learning.
Rather than fine-tuning the LLMs on task-specific data, prompting
integrates task instructions (e.g., TL;DR translate to English) and a
few relevant examples as input and extracts the answers from the
output of large language model (LLM). The terms â€œhard prompâ€ and
â€œsoft promptâ€ refer to different approaches to guiding the LLMâ€™s
behavior during text generation or other tasks. A hard prompt [ 55]
involves using explicitly defined and unchangeable text inputs to
instruct the model. The prompt does not involve additional training
or fine-tuning of the model. On the other hand, a soft prompt in-
volves using trainable vectors or learnable embeddings to guide the
modelâ€™s behavior. Unlike hard prompts, soft prompts are not explicit
text instructions but rather embeddings that influence the modelâ€™s
output. These embeddings are typically learned through a process
1arXiv:2307.08303v5  [cs.IR]  17 Jun 2024

--- PAGE 2 ---
known as prompt tuning [ 20,21,26,57]. The existing work [ 43,44]
suggested that prompts provide a method for injecting task-specific
guidance, which is beneficial in low-data regimes. Recent research
[42] further quantified this benefit through comprehensive test-
ing of prompts. The results showed that well-crafted prompts can
significantly reduce the dependency on large volumes of training
data across downstream tasks. Both InPars [ 2] and PROMPTAGA-
TOR [ 8] employ hard prompts to guide LLMs in tagging unlabeled
documents with weak queries, subsequently training task-specific
retrievers. Nonetheless, hard prompts come with limitations: a)
Crafting effective hard prompts is challenging and often requires it-
erative human effort, intuition, and sometimes a bit of luck; b) Even
with hand-crafted prompts, the downstream tasks still underper-
form tuned models. For instance, compared with the performance
of fine-tuned T5-XXL [ 37] on SuperGLUE [ 51], GPT-3 175B few-
shot gets a 17.5 points smaller score despite using 16 times more
parameters [ 20]. These limitations of hard prompts underscore
their effectiveness and addressing these challenges draws academic
interest as well as generating industrial value.
Given the limitations of hard prompts, we investigate an alterna-
tive. Rather than utilizing humanly-readable words as hard prompts
[33], the soft prompt [ 20,21,26,57] comprises a set of embeddings
which are unrecognizable to humans and are prepended at the be-
ginning of the neural network input. During the soft prompt tuning,
the parameters of the LLM are frozen, and only the parameters as-
sociated with the soft prompt are updated. While both [ 20] and [ 21]
demonstrate that soft prompts surpass the hard prompts, there is
no work utilizing soft prompt tuning to augment DR. In this paper,
we propose soft prompt tuning for augmenting DR (SPTAR). Specif-
ically, for each task, we leverage soft prompt tuning to optimize
the parameters associated with the soft prompt on limited ground
truth data and then prompt the LLMs to tag unlabeled documents
with weak queries, yielding enough weak document-query pairs
to train task-specific retrievers. Moreover, we find that even with
the optimized soft prompt, the quality of generated weak queries is
sometimes sensitive to the example document-query pairs in the
prompt. Thus, we designed a filter to select high-quality example
document-query pairs in the prompt to further improve the qual-
ity of weakly tagged queries as well as the DR tasks. In addition,
most of the existing work has been built on proprietary models
hidden behind opaque API endpoints, which may produce non-
reproducible or non-deterministic experimental results. Instead,
our work is based on widely used open source LLMs [ 34]. Our main
contributions can be summarized as follows:
â€¢To the best of our knowledge, our work stands as one of
the early attempts of LLMs in combination with soft prompt
tuning for enhancing DR tasks.
â€¢We introduce a soft prompt filter designed to curate document-
query pairs within the prompt, thus enhancing the overall
quality of the generated weak data. Additionally, we design
a BM25 filter to reduce noise in the generated data, further
improving performance.
â€¢We conduct a comprehensive set of experiments involving
four datasets and seven retrievers and re-rankers, demon-
strating the generality and superior performance of our ap-
proach over several state-of-the-art baselines.â€¢Experiments are based on the recent open-source LLMs to
ensure reproducible and deterministic experimental results.
All code and data are publicly available2.
2 RELATED WORK
2.1 Dense Retrieval
DR converts the queries and documents into dense vectors on which
the ANN index can be built for fast search. DPR [ 17] employs a
two-tower structure: one BERT model for queries and another for
documents. For each query with one positive document and several
negative documents, DPR measures the similarity between query
embedding and document embeddings and then maximizes the
log-likelihood of the positive passage. A variant of DPR is to utilize
one BERT by concatenating query and document as input and ex-
tracting the query embedding and document embedding after the
encoding. The query encoder and document encoder of ColBERT
[18] [41] share the same BERT but utilize a different special token
following the â€œ[CLS]â€ to distinguish query and document. Unlike
DPR directly measures the similarity between query embedding
and document embeddings, ColBERT introduces a late interaction
mechanism. Specifically, for each token in the query, ColBERT com-
putes its similarity with all the tokens in the document and applies
a maximum pooling on these similarity scores. The similarity score
of a pair of query and document is the summarization of all the
scores after the maximum pooling. Given a query with one positive
document and one negative document, ColBERT is optimized by
the pairwise softmax cross-entropy loss over the computed scores
of the positive and negative documents. ANCE [ 56] is a bi-encoder
trained on (query, positive document, negative document) tuples
where the negative document is retrieved from an ANN built on
the checkpoint of the last step. TAS-B [ 11] groups queries by their
embedding similarities and employs a training data sampling tech-
nique coupled with dual-teacher supervision distillation. Contriever
[13] trains a bi-encoder model through contrastive learning. Instead
of training the model on the labeled dataset, Contriever generates
positive query-document pairs from unlabeled corpus by two strate-
gies â€œinverse cloze tasksâ€ and â€œindependent croppingâ€. ReContriever
[19] adopts the same method as Contriever to generate the weak
query-document pairs, but ReContriever scores the weak query-
document pairs by itself during the training and the loss is weighted
by the weights. BM25CE [ 52] is a re-ranking-based DR. BM25CE
first applies BM25 to retrieve documents and then employs the
trained crossed-encoder to re-rank the retrieved documents. Our
contribution is not to propose new dense retrievers but to propose
a novel method to augment the existing dense retrievers.
2.2 Data Augmentation for Dense Retrieval
For DR datasets, usually, only a fraction of documents are labeled
with queries, for instance, MS MARCO [ 31], a widely used dataset
in DR, has a corpus of 8841823 documents but only has 532761 train-
ing document-query pairs. Given DR demands substantial train-
ing data to achieve quality dense embeddings, some researchers
have turned to data augmentation to generate more document-
query pairs to train better dense embeddings. InPars [ 2] feeds
2https://github.com/zhiyuanpeng/SPTAR.git
2

--- PAGE 3 ---
a task-specific human-written prompt and 3 example document-
query pairs to a 6B GPT-3 [ 4] model Curie to generate 100K weak
document-query pairs and selects the top 10K queries with respect
to the probability of query ğ‘to augment the training data. InPars
[2] employs the same dense retrieval model proposed in [ 32], which
treats the retrieval as a sequence-to-sequence task by concatenating
a query and a document as input to T5 mode and outputs the rele-
vance score. Improved variations of InPars [ 2], such as InPars-v2
[15] and InPars-Light [ 3], have been introduced to enhance the
original methodology. Like InPars [ 2], PROMPTAGATOR [ 8] also
feeds a task-specific human-written prompt and at most 8 example
document-query pairs to LLM to generate weak data. Instead of
selecting the top weak queries by their probabilities, PROMPTA-
GATOR first trains a filter on uncleaned document-query pairs to
filter the weak queries by dropping the weak queries that cannot
retrieve their paired documents in the Top- ğ‘˜retrieved documents.
By repeating this process multiple times, the filter significantly
improves the performance of a dual-encoder DPR retriever. Besides,
PROMPTAGATOR [ 8] utilizes a much bigger LLM: a 175B model
Flan [ 54] which cannot be accessed by most researchers. DAR [ 14]
argues that the method that generates queries from unlabeled docu-
ments is costly as well as does not add variations to the documents.
To do data augmentation efficiently, DAR [ 14] not only interpo-
lates two different document representations associated with the
labeled query but also stochastically perturbs the representations
of labeled documents in embedding space. RocketQA [ 36] applies a
pre-trained cross-encoder retriever to retrieve positive and negative
documents for a new collection of queries with high confidence
scores. RocketQAv2 [ 38] augments the DR by jointly optimizing
the bi-encoder structure DR and cross-encoder structure reranking
model to have similar output distributions. DRAGON [ 25] fuses
multiple teacher models by progressively training the base DR
model.
2.3 LLMs in Dense Retrieval
Most of the current literature in this domain explores the poten-
tial of LLMs to improve DR tasks through various data generation
techniques, including query generation [ 2,3,8,9,15,40], relevance
generation [ 22], and permutation generation [ 27,35,45]. PROMP-
TAGATOR [ 8] and InPars [ 2] with its variations InPars-v2 [ 15] and
InPars-Light [ 3] are illustrated in section 2.2. UPR [ 40] utilizes LLM
as a zero-shot reranker to re-rank the passages retrieved by retriev-
ers like BM25 and DPR. Given a query, for each retrieved passage,
UPR utilizes a prompt â€œ Please write a question based on this passage â€
to prompt a LLM and computes the average log-likelihood of the
question tokens conditioned on the input document as the relevance
score. Due to the intensive computational resources required to
train LLMs, all these works utilize LLMs as query generators instead
of fine-tuning them. HyDE [ 9] leverages LLMs to augment queries
by generating hypothetical documents, effectively capturing rele-
vance patterns for unsupervised retrieval. LRL [ 27] trains a listwise
zero-shot re-ranker that leverages LLMs without task-specific su-
pervised training. Unlike pointwise re-rankers, LRL considers all
candidate documents to determine their relative ranking positions.
Another approach involves instructional permutation generation
[45], where the focus is on instructing LLMs to directly outputpermutations of passages. Permutation distillation techniques are
employed to transfer the passage ranking capabilities of ChatGPT
into a smaller, specialized ranking model. While these works uti-
lize LLMs as query generators without fine-tuning, our SPTAR
approach takes a different approach. We first perform soft prompt
tuning to optimize task-specific soft prompts and then employ data
filtering to enhance the quality of the generated weak data.
2.4 Prompt Tuning
Prompt tuning offers a promising avenue for adapting pre-trained
LLMs to specific tasks by focusing on tuning the prompt module
instead of fine-tuning the entire model [ 46]. Prefix-Tuning [ 21] in-
troduces a prompt module with learnable parameters ğœƒoutputting
embeddings which are prepended to the embeddings of other in-
putted tokens. This approach preserves the original training ob-
jective intact while updating only the prefix parameters ğœƒthrough
gradient descent for each task. Another similar technique, referred
to as â€œ gisting â€ [30], compresses arbitrary prompts into a condensed
set of virtual â€œ gistâ€ tokens using a meta-learning approach. Building
upon T5 [37], Lester et al. [20] propose a method where the learn-
able embeddings of a task-specific prompt are prepended to the
encoderâ€™s output. The concatenated embeddings are then passed
through the decoder to compute the training objective. This ap-
proach enables the model to incorporate task-specific information
into the decoding process. Zhou et al . [59] introduce Dual Context-
guided Continuous Prompt (DCCP), which employs soft prompt
tuning using dual inputs: context-aware prompt and label-aware
context representations. This approach leverages both prompt in-
formation and contextual understanding to enhance the modelâ€™s
performance. Prompt tuning can benefit from multi-task learning.
For instance, ATTEMPT proposed by Wang et al . [53] introduces a
multi-task tuning method that transfers knowledge across different
tasks through a mixture of soft prompts. In the context of Multilin-
gual Information Retrieval, Huang et al . [12] explore a soft prompt
decoding approach that treats retrieval in each language as a sepa-
rate task while jointly modeling them to capture shared underlying
structures. They use decomposable prompts in KD-SPD to model
languages, highlighting that languages share common features
and concepts despite their unique properties. Regarding IR tasks,
DPTDR by Tang et al. [47] employs a dual-encoder, two RoBERTa
models, for retrieval. It initializes the dual-encoder through con-
trastive learning and appends learnable soft prompts for query and
document. Both the dual-encoder and the learnable prompts are
updated during the training process.
In contrast, unlike the current DR data augmentation works that
only prompt LLMs to generate weak queries for unlabeled docu-
ments with a few labeled document-query pairs as examples, we
propose to learn task-specific soft prompts on a small proportion of
the labeled data and a novel soft prompt filter method to select high-
quality example document-query pairs in the prompt to improve
the DR tasks further. The whole augmentation pipeline makes our
approach different from the current works.
3

--- PAGE 4 ---
Notation Definition
ğ‘,ğ‘‘ query, document
ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› collection of query-document pairs for training
ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ collection of query-document pairs for testing
ğ·ğ‘’ğ‘£ğ‘ğ‘™ collection of query-document pairs for evaluation
ğ¶ collection of all the documents
ğ¶ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ unlabeled documents in ğ¶
Î¦ freezed original parameters of large language model
ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›collection of ğ‘‹sampled queries associated with corresponding documents from ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘“ğœƒ(Â·) promptâ€™s embedding layer parametrized by ğœƒ
ğ‘“ğœƒ(ğ‘ ) soft prompt initialized based on a hard prompt ğ‘ with a length of ğ‘™ğ‘ 
(ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1collection of ğ‘€sampled document-query pairs from ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, as examples in prompt
ğ‘ğ‘— concatenation of(ğ‘‘ğ‘—,ğ‘ğ‘—)âˆˆ(ğ‘‘ğ‘—,ğ‘ğ‘—)ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)âˆ’ğ‘€
ğ‘—=1and all the example pairs (ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1
(ğ‘‘ğ‘—,ğ‘ğ‘—)ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)âˆ’ğ‘€
ğ‘—=1for each epoch, loss is computed on ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)âˆ’ğ‘€document-query pairs from ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘ğœƒ,Î¦(ğ‘ğ‘—|ğ‘¡ğ‘—) the probability of ğ‘ğ‘—conditioned on ğ‘¡ğ‘—given parameters ğœƒandÎ¦
â„ğ‘—,ğ‘– output hidden vector of ğ‘–ğ‘¡â„time step for ğ‘—ğ‘¡â„instance
ğ‘§ğ‘—,ğ‘– ID ofğ‘–ğ‘¡â„token ofğ‘—ğ‘¡â„instance
ğ¿ loss function
ğ‘†ğ‘Œ
ğ‘’ğ‘£ğ‘ğ‘™collection of ğ‘Œqueries associated with corresponding documents from ğ·ğ‘’ğ‘£ğ‘ğ‘™
ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ collection of 100K sampled documents from ğ¶ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ and each document has a generated weak
query
ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ collection of 5000 sampled documents ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’
ğ¹ğ‘˜(Â·) topğ‘˜weak data filter function
ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’) ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ filtered by weak data filter ğ¹ğ‘˜
Table 1: Summary of notation.
Data
PreparationSoft Prompt
TuningSoft Prompt
Filter
Soft Prompt
AugmentorWeak Data
FilterDense
Retrieval
Figure 1: The pipeline of the proposed Soft Prompt Tuning
for Augmenting dense Retrieval (SPTAR).
3SOFT PROMPT TUNING FOR AUGMENTING
DENSE RETRIEVAL
As shown in Figure 1, SPTAR comprises six modules: a) data prepa-
ration; b) soft prompt tuning; c) soft prompt filter; d) soft prompt
augmentor; e) weak data filter; f) DR. In Section 3.1, we elaborate on
how to generate the training and evaluation datasets of soft prompt
tuning. With the training and evaluation datasets, we conduct soft
prompt tuning (Section 3.2) to learn a task-specific soft prompt. To
further improve the quality of the weak generated queries, we in-
troduce the soft prompt filter (Section 3.3) which identifies optimal
example document-query pairs to optimize the task-specific prompt.
We then prompt LLMs to generate weak queries for unlabeled doc-
uments (Section 3.5), yielding enough training data to train DR.
Finally, we train the DR (Section 3.6) models on filtered weak data
(Section 3.4). The notations used in this paper are provided in Table
1.3.1 Data Preparation
We study the augmentation of DR using limited data. The initial
step involves sampling a small dataset on which we fine-tune a
task-specific soft prompt. We define dataset ğ·asğ·={(ğ‘ğ‘›,ğ‘‘ğ‘›)}ğ‘
ğ‘›=1
where for each query ğ‘ğ‘›, there is a relevant document ğ‘‘ğ‘›. There may
exist duplicated queries as one query may have multiple relevant
documents. This domain-specific dataset ğ·is categorized into train,
test, and evaluation subsets, denoted as ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ,ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡, andğ·ğ‘’ğ‘£ğ‘ğ‘™,
respectively. Apart from dataset ğ·, there is a much bigger document
collectionğ¶which contains all the documents in ğ·but has more
unlabeled documents denoted as ğ¶ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ . After training, DR
encodes all the documents in ğ¶into vectors. When a new query
comes in, DR encodes the query into a vector and searches the
top-ğ‘˜similar documents in vector space.
We randomly sample document-query pairs from the original
training dataset ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› to construct the training and evaluation
datasets for the soft prompt module, namely ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘†ğ‘Œ
ğ‘’ğ‘£ğ‘ğ‘™where
indices X and Y signify the number of distinct queries within
the training and evaluation datasets respectively. The function
ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘¥)designates the quantity of document-query pairs given
ğ‘¥distinct queries in the dataset. Since each query may have more
than one positive document, ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘¥)may be bigger than |ğ‘¥|.
Hence,ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›containsğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)document-query pairs, simi-
larly,ğ‘†ğ‘Œ
ğ‘’ğ‘£ğ‘ğ‘™comprisesğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘Œ)document-query pairs. For il-
lustration, in our experiment, we draw 50 unique queries and their
corresponding documents from the training dataset ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› to form
4

--- PAGE 5 ---
Tokenizer
initialized Prompt  with length D-Q PairsPrompt's Embedding Layer LLM's Embedding LayerLearnable Parameters 
Freeze Parameters 
...Document ()Â : The function of
a moto r neuron is to carry an
electrical signal to a muscle,
triggering it to either contract or
relax.
Relevant Query () :Â what is the
structure and function of a motor
neuron?
Document ():Â China likely
biggest inves tor in Indonesia.
Investment Coordinating Board
(BKPM) has recorded Singapore
as the largest contributor of
investment in Indonesia with
US$5.9 billion, a 20 percent share
of total foreign investment in 2015.Weak Relevant QueryÂ  ()Â :Â what
country invests the most in
indonesia
Weak Query GenerationÂ 
Doc to be LabeledTrue QueryÂ  ()Â : which country invests the most in indonesia
Initialized Prompt
D-Q Pairs...
Doc to be Labeled
Initialized Prompt
Filtered D-Q Pairs...
Doc to be Labeled(c) Soft Prompt Filter (d) Soft Prompt Augmentor(b) Soft Prompt Tuning (a) Shared LLMData (Section 3.1)
LLM
please generate query for this documentFigure 2: The main architecture of the proposed SPTAR: a) The same LLM is shared by soft prompt tuning module, soft prompt
filter module and soft prompt augmentor module; b) soft prompt tuning module fixs the LLMâ€™s original parameters Î¦and
only fine-tune the parameters of soft promptâ€™s embedding layer ğœƒon the sampled small dataset (Section 3.1); c) soft prompt
filter module fixs the learned parameters ğœƒâˆ—, and for each group of sampled example document-query pairs, computes the loss
on evaluation dataset. The group of example document-query pairs with the smallest loss will be utilized in the soft prompt
augmentor module; d) with the learned parameters ğœƒâˆ—and a group of filtered example document-query pairs, the soft prompt
augmentor module iterates over the unlabeled document dataset ğ·ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ to generate weak queries.
ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ‘‹=50). From the remaining data in ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› , we randomly se-
lect 100 unique queries and their associated documents to compose
ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™(ğ‘Œ=100).ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›serves for optimizing the soft prompt, while
ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™is employed to assess the modelâ€™s convergence, enabling us to
terminate the training process in advance and mitigate overfitting
risks. We also tried other values of ğ‘‹, and the influence of ğ‘‹is
studied in Section 5.2.5.
3.2 Soft Prompt Tuning
Soft prompts [ 10,58] introduce a novel technique to steer a modelâ€™s
behavior without the need for extensive fine-tuning. Unlike hard
prompts, which are human-readable instructions, soft prompts com-
prise trained embeddings optimized for specific tasks. The soft
prompt tuning module learns a task-specific soft prompt on a small
proportion of labeled data. Figure 2 (b) illustrates the structure of
the soft prompt tuning module, where the red boxes represent the
parametersğœƒto be optimized during model training and the green
boxes represent LLMâ€™s original parameters Î¦that are retained dur-
ing the training. ğ‘ represents the initialized hard prompt with size
ğ‘™ğ‘ , like repeating "please generate query for document" until the
length ofğ‘ equalsğ‘™ğ‘ . Letğ‘“ğœƒ(Â·)denote the promptâ€™s embedding layer
implemented by an embedding matrix initialized as the embeddings
ofğ‘ encoded by LLMâ€™s original embedding layer. ğ‘“ğœƒ(ğ‘ )represents
the soft prompt.
For each training epoch, we first randomly sample ğ‘€document-
query pairs from training dataset ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›as example document-
query pairs(ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1, then iterate over the left document-query
pairs(ğ‘‘ğ‘—,ğ‘ğ‘—)ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)âˆ’ğ‘€
ğ‘—=1to compute loss. Example pairs (ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1
are concatenated with each pair (ğ‘‘ğ‘—,ğ‘ğ‘—)by keywords like â€œ docu-
ment â€ and â€œ query â€ asğ‘ğ‘—. Finally, we concatenate ğ‘ withğ‘ğ‘—as onetraining instance ğ‘¡ğ‘—=[ğ‘ ;ğ‘ğ‘—]and there are ğ‘ğ‘¢ğ‘šğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘‹)âˆ’ğ‘€in-
stances in each epoch. When ğ‘¡ğ‘—is inputted into the soft prompt
tuning module, it is first tokenized into a list of IDs ğ‘§ğ‘—indexed byğ‘–
then the embeddings of IDs are extracted and fed into the following
layers to compute the hidden vectors. ğ‘“ğœƒ(Â·)takes the IDs of ğ‘ as
inputs and outputs its embeddings while the embeddings of ğ‘ğ‘—are
generated by LLMâ€™s original embedding layer. For simplicity, we
postulate that each token in ğ‘¡ğ‘—has one corresponding ID in ğ‘§ğ‘—. For
training instance ğ‘¡ğ‘—, the hidden vector of ğ‘–ğ‘¡â„time step is defined
asâ„ğ‘—,ğ‘–âˆˆRğ‘‘whereâ„ğ‘—,ğ‘–=h
â„(1)
ğ‘—,ğ‘–;Â·Â·Â·;â„(ğ‘˜)
ğ‘—,ğ‘–i
andğ‘˜is the number of
layers in LLM. The objective function for training is given by:
max
ğœƒlogğ‘ğœƒ,ğœ™(ğ‘ğ‘—|ğ‘¡ğ‘—)=max
ğœƒâˆ‘ï¸
ğ‘–âˆˆğ‘–ğ‘‘ğ‘¥ğ‘ğ‘—logğ‘ğœƒ,ğœ™ ğ‘§ğ‘—,ğ‘–|â„ğ‘—,<ğ‘–(1)
whereğ‘–ğ‘‘ğ‘¥ğ‘ğ‘—denotes the indexes corresponding to the IDs of ğ‘‘ğ‘—.
Additionally, ğ‘ğœƒ,ğœ™ ğ‘§ğ‘—,ğ‘–|â„ğ‘—,<ğ‘–signifies the probability of the sub-
sequent token with ID ğ‘§ğ‘—,ğ‘–. For loss function ğ¿, we employ the
negative log-likelihood defined as:
ğ¿=âˆ’logğ‘ğœƒ,ğœ™(ğ‘ğ‘—|ğ‘¡ğ‘—) (2)
We implemented our soft prompt tuning module based on a public
prompt tuning package PEFT [29].
3.3 Soft Prompt Filter
During the development of the soft prompt module, we observe
that the choice of example document-query pairs (ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1pro-
foundly affects the quality of text generation. Therefore, upon com-
pleting the soft prompt training, with the learned parameters ğœƒâˆ—,
we try to select the best group of document-query pairs from ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
as example document-query pairs in soft prompt augmentor. For
ğ‘€=2, there are 1225 ( 50âˆ—49/2) groups of example pairs, which
5

--- PAGE 6 ---
makes it impractical to evaluate all. To reduce the computation
complexity, we randomly sample ğ‘‹groups of example pairs from
ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›to evaluate them on the evaluation dataset ğ‘†ğ‘Œ
ğ‘’ğ‘£ğ‘ğ‘™and the
group of example pairs with the best evaluation metric will be cho-
sen as the example pairs in soft prompt augmentor. As shown in
Figure 2 (c), the only difference between soft prompt tuning and
soft prompt filter is the dataset where the ğ‘‘ğ‘—comes from. Suppose
we sampled ğ‘‹groups of document-query pairs each of which has
ğ‘€document-query pairs (ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1. Evaluation dataset ğ‘†ğ‘Œ
ğ‘’ğ‘£ğ‘ğ‘™has
ğ‘ğ‘¢ğ‘š(ğ‘Œ)document-query pairs and example pairs (ğ‘‘ğ‘š,ğ‘ğ‘š)ğ‘€
ğ‘š=1are
concatenated with each pair (ğ‘‘ğ‘—,ğ‘ğ‘—)by keywords like â€œ document â€
and â€œ query â€ asğ‘ğ‘—. Then,ğ‘ğ‘—is concatenated with the initialized
promptğ‘ asğ‘¡ğ‘—=[ğ‘ ,ğ‘ğ‘—]. The evaluation metric is the same as the
loss function ğ¿(Equation 2). We study the effectiveness of soft
prompt filter in Section 5.2.3 and the filtered example document-
query pairs are documented in Appendix A and B.
3.4 Weak Data Filter
Both InPars [ 2] and PROPAGATE [ 8] emphasize the importance of
filtering weak document-query pairs as the generated weak queries
are not guaranteed to be always relevant to the input documents.
Employing the methodology from InPars [ 2], we clean the weak
data. Upon acquiring these generated weak pairs (Section 3.5), we
apply a BM25-based filtering: For each weak query, BM25 retrieves
the topğ‘˜documents from the corpus ğ¶. If the document linked
to a weak query isnâ€™t among the top ğ‘˜results, the pair gets dis-
carded. This filtering approach is denoted as ğ¹ğ‘˜(Â·). For datasets
MS MARCO and FiQA-2018, we experimented with different top
ğ‘˜values from the set {10,30,50,70}and reported the best results.
The effectiveness of this weak data filter module is discussed in
Section 5.2.4.
3.5 Soft Prompt Augmentor
Generating high-quality queries for unlabeled documents remains
a formidable challenge. Our soft prompt augmentor module har-
nesses both the potency of the learned soft prompts and the context
offered by the best example document-query pairs, providing a
synergistic effect that ensures not just relevance, but also the su-
perior quality of the generated queries. As shown in Figure 2 (d),
with the learned parameters ğœƒâˆ—and the filtered group of example
document-query pairs, soft prompt augmentor generates a weak
query for an unlabeled document ğ‘‘ğ‘—sampled from ğ·ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ . In
this paper, for each dataset, we first created two weak datasets: a)
ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ .100ğ¾unlabled documents are sampled from ğ·ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ to
generate their weak queries. If the number of unlabeled documents
inğ·ğ‘¢ğ‘›ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ is smaller than 100ğ¾, all the unlabeled documents are
utilized to generate weak queries; b) ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ . 5000 document-query
pairs are sampled from ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ . Then, we filtered ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ andğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™
by weak data filter, described in Section 3.4, to get ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)and
ğ¹ğ‘˜(ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™). During the weak query generation process, LLM not
only utilizes the soft prompt embeddings to capture domain-specific
information but also benefits from the supplementary context pro-
vided by the best example document-query pairs.3.6 Dense Retrieval
DR serves as the concluding step in our methodology, where we
harness the capabilities of neural networks to retrieve relevant
documents. We conducted the experiments on five popular dense
retrievers: DPR, ColBERT, TAS-B, Contriever, and ReContriever.
The descriptions of the models can be found in Section 2.1. For TAS-
B, we used pre-trained bi-encoder model3for clustering queries
and cross-encoder model4and ColBERTv25for teacher mod-
els. For Contriever, we refer to the officially released checkpoint
6asğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ and used it for initial evaluations. Addition-
ally, we fine-tuned ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ as a bi-encoder dense retrieval
model, denoted ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’. Similary, for ReContriever, we have
ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’7andğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’. Further, we incorporated
the cross-encoder model BM25CE, as established in previous litera-
ture [ 52], which re-ranks the top 1000 items retrieved by BM25. Like
BM25CE, we employed the DPR model as a bi-encoder to re-rank
the top 1000 items retrieved by BM25, referring to this method as
BM25BE.
4 EXPERIMENTAL SETUP
4.1 Datasets
Experiments were performed on four datasets MS MARCO [ 31] and
FiQA-2018 [ 28], sourced from BEIR [ 48] and DL2019 [ 7], DL2020
[6]. The description of the four datasets can be found in Table 2.
We follow BEIR [ 48] to report the metrics on the evaluation dataset
instead of test data for MS MARCO, so, for MS MARCO, ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡is
the same as ğ·ğ‘’ğ‘£ğ‘ğ‘™.
As shown in Table 3: a) BM25, ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ andğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’
are evaluated on the original testing split ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡; b) W/O Aug mod-
els are trained on datasets ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™utilized to fine-tune
the soft prompt; c) InPars [ 2] models are trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and
ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™plusğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ filtered byğ¹ğ‘˜, Section 3.4) generated
by human-written prompts. d) SPTARâ€™s soft prompt tuning mod-
ule (SPTAR-Tuning) is trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and evaluated on ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™;
SPTARâ€™s DR models (SPTAR-DR) are trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™
plusğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ filtered byğ¹ğ‘˜, Section 3.4) generated by
soft prompt augmentor (Section 3.5); e) W/O Aug, InPars [ 2] and
SPTAR are all evaluated and tested on the same splits for a fair
comparison; f) For ğ¹ğ‘˜(Â·), we triedğ‘˜âˆˆ(10,30,50,70)and selected
theğ‘˜with the best NDCG@10 score on evaluation dataset.
4.2 Training Details
To train the soft prompt module, we performed fine-tuning using
two open-source LLMs: LLaMA-7B and Vicuna-7B. The specific
training hyper-parameters are documented in Table 4.
3https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5
4https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2
5https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz
6https://huggingface.co/facebook/contriever
7https://huggingface.co/Yibin-Lei/ReContriever
6

--- PAGE 7 ---
Task Domain DatasetTrain Eval Test Avg. Word Lengths
#Pairs #Query #Query #Corpus Avg. D/Q Query Document
Passage Retrieval Misc. MS MARCO [31] 532,761 N/A 6,980 8,841,823 1.1 5.96 55.98
Passage Retrieval Misc. DL2019 [7] 532,761 N/A 43 8,841,823 215.3 5.96 55.98
Passage Retrieval Misc. DL2020 [6] 532,761 N/A 54 8,841,823 210.9 5.96 55.98
Question Answering Finance FiQA-2018 [28] 14,166 500 648 57,638 2.6 10.77 132.32
Table 2: Statistics of datasets in BEIR benchmark. Avg. D/Q indicates the average number of relevant documents per query.
Model Train Eval Test
BM25 N/A N/A ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ N/A N/A ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ N/A N/A ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
W/O Aug ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™ğ·ğ‘’ğ‘£ğ‘ğ‘™ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
InPars ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)ğ·ğ‘’ğ‘£ğ‘ğ‘™ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
SPTAR-Tuning ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™N/A
SPTAR-DR ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)ğ·ğ‘’ğ‘£ğ‘ğ‘™ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡
Table 3: Dataset partition for different methods.
Hyperparameters LLaMA-7B Vicuna-7B
Batch Size 4 2
Max Length 1024 1024
Learning Rate 3ğ‘’âˆ’2 3 ğ‘’âˆ’2
Optimizer AdamW AdamW
Early Stop 5 5
Max epochs 100 100
GPU 1 A100 (80G) 1 A100 (80G)
Table 4: Hyperparameters of soft prompt tuning
The training hyper-parameters of dense retrievers are in Table
5. For ColBERT, there is no early stop in the official code and we
saved a checkpoint after each epoch. After training, we manually
evaluated some checkpoints (3, 5, 10, 15, 18, 20) and reported the
testing results of the checkpoint with the highest NDCG@10 score.
4.3 Evaluation Metrics
In the context of text generation models, Perplexity is a commonly
employed metric that quantifies the level of uncertainty exhibited
by a language model when generating new tokens. This metric is
defined as the exponentiated average negative log-likelihood of a
sequence, and a lower perplexity value indicates a higher-quality
language model. Perplexity is used to evaluate the soft prompt
tuning and soft prompt filter modules.
In our evaluation of DR models, we adhere to the metrics estab-
lished in previous studies [ 16,24]. For the MS MARCO and FiQA-
2018 datasets, we utilize Mean Reciprocal Rank at 10 (MRR@10)
and Recall@100. For the DL2019 and DL2020 datasets, we employ
Mean Average Precision (MAP), Normalized Discounted Cumula-
tive Gain at 10 (nDCG@10), and Recall@100. For the BM25CE andBM25BE models, which both re-rank the top 1000 items retrieved by
BM25, Recall@100 is used to specifically evaluate their re-ranking
effectiveness. These metrics together provide a comprehensive as-
sessment of how augmented queries influence the performance of
DR models.
4.4 Baseline Methods
Our study incorporates five baseline methods: BM25, ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’,
ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’, Without Augmentation (W/O Aug), and InPars
[2] (Section 2.3). The training, evaluation, and testing datasets are
documented in Section 4.1. For BM25 [ 39], we use Anserini [ 23]
with the default Lucene parameters ( ğ‘˜=0.9andğ‘=0.4). TAS-
B,ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ , andğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ are described in Section 3.6. The
differences between InPars [ 2] and SPTAR are twofold: a) InPars
[2] utilizes the human-written prompt while SPTAR utilizes an
optimized soft prompt; b) SPTAR has a soft prompt filter module to
select example document-query pairs. To make it a fair comparison
with InPars [ 2], we choose the same example document-query pairs
in the prompt of SPTAR for InPars [ 2] and utilize InParsâ€™ original
human-written prompt to prompt the LLaMA and Vicuna to obtain
weak document-query pairs. We find for InParsâ€™ human-written
prompt, the quality of generated weak document-query pairs of
Vicuna is much better than that of LLaMA, so, for InPars [ 2], we
choose Vicuna as the weak data generator. For SPTAR, LLaMA is
better than Vicuna and we choose LLaMA for SPTAR.
4.5 Research Questions
An extensive set of experiments was designed to address the fol-
lowing research questions:
RQ1 : Can the proposed SPTAR framework achieve improved
performance on DR tasks over the baseline models? (Section 5.1)
RQ2 : During the soft prompt tuning process, does the soft
prompt tuning module indeed distill the knowledge from the dataset
to the learned soft prompt? What factors contribute to the learned
soft prompts? (Section 5.2.1)
RQ3 : What are the costs of the soft prompt tuning module? Does
the soft prompt tuning module greatly increase the training time
and computational resources? (Section 5.2.2)
RQ4 : What specific role does the soft prompt filter play in SP-
TAR? (Section 5.2.3)
RQ5 : Can the weak data filter further improve the performances
of DR models? (Section 5.2.4)
RQ6 : For SPTARâ€™s soft prompt tuning module, what is the in-
fluence of the size of training data ğ‘‹? Is a larger ğ‘‹better than a
smaller one? (Section 5.2.5)
7

--- PAGE 8 ---
Hyperparameters DPR ColBERT ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’ BM25CE TAS-B
Batch Size 32 32 32 32 96 40
Max Length 350 350 350 350 512 300
Learning Rate 2ğ‘’âˆ’5 2 ğ‘’âˆ’5 2 ğ‘’âˆ’5 2 ğ‘’âˆ’5 2 ğ‘’âˆ’5 2 ğ‘’âˆ’5
DDP No Yes No No No No
Optimizer AdamW AdamW AdamW AdamW AdamW AdamW
Early Stop 10 None 10 10 10 10
Max epochs 20 20 20 20 20 20
GPU 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G)
Table 5: Hyperparameters of DR Models
RQ7 : For SPTARâ€™s soft prompt augmentor module, what is the
influence of the number of example document-query pairs ğ‘€? Is a
largerğ‘€better than a smaller one? (Section 5.2.6)
5 EXPERIMENTAL RESULTS
5.1 SPTAR vs Baseline Models (RQ1)
We conducted evaluations of our SPTAR method on MS MARCO
and its related datasets, including DL2019, DL2020, and FiQA 2018.
As presented in Table 6, the SPTAR data augmentation method
consistently outperforms established baselines, including W/O Aug
and InPars, across a spectrum of widely used datasets and key
retrieval metrics only except the R@100 of SPTARâ€™s ColBERT which
is slightly outdone by InPars. TAS-B method gets the most best
results (4 out of 10, there are 10 metrics in each row of Table 6) as it
is distilled from two teacher dense retrieval models (cross-encoder
and ColBERT) that are both trained on the full MS MARCO dataset.
For all 7 retrievers, 55 out of 70 improvements are statistically
significant, with p-values < 0.05. ColBERT takes 8 out of 15 un-
significant improvements.
The token-level matching complexity of ColBERT between queries
and documents could explain why SPTARâ€™s enhancements on Col-
BERT did not significantly surpass the InPars baseline on MS MARCO
related datasets. DPRâ€™s simple architecture makes it have better
generalization ability than ColBERT, which is evidenced by the
fact that on MS MARCO related datasets, DPRâ€™ W/O Aug is better
than ColBERTâ€™s W/O Aug. Also, there exists noise in the gener-
ated queries, especially when the soft prompt is learned from a
small dataset (same as the dataset of W/O Aug), and ColBERTâ€™s
token-level interaction mechanism is more sensitive than DPR.
Regarding the re-ranking models, BM25CE and BM25BE, both
re-rank the top 1000 items initially retrieved by BM25. BM25CE
employs a cross-encoder for re-ranking, whereas BM25BE inte-
grates a DPR model. Without data augmentation, BM25BE beats
BM25CE on 7 out of 10 metrics when trained on a small labeled
dataset (W/O Aug). The cross-encoder model of BM25CE, requir-
ing more extensive data to effectively learn complex interactions
and mitigate overfitting risks, ultimately demonstrated superior
performance under SPTAR over BM25BE on 6 out of 10 metrics.
The officially released checkpoint of ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ is loaded
to be evaluated, and we further improved its performance by fine-
tuning it as a bi-encoder model denoted as ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’in Table 6.We found performance improvement even fine-tuning ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’
on a small dataset as Contrieverâ€™s W/O Aug is better than ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’
on all four datasets. Our method, ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’â€™s SPTAR, can further
improveğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ over the second-best value significantly.
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’â€™s InPars only beats W/O Aug on 4 out of 10 metrics.
Similarly, we evaluated the Re ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ andğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’
and the same patterns as Contriever are observed as well.
By harnessing the benefits of soft prompt tuning and LLMs, our
model generates high-quality weak queries that greatly enhance DR
tasks. Moreover, the consistent improvements observed across DPR,
TAS-B, BM25BE, BM25CE, Contriever, and ReContriever substan-
tiate the general applicability of our approach, extending beyond
specific dense retrievers. It is worth noting that in the absence of
augmentation data, all dense retrievers except for the Contriever
and ReContriever perform worse than the unsupervised model
BM25. This underscores the significant reliance of DR on domain-
specific labeled data and highlights the limitations of directly train-
ing dense retrievers in scenarios with limited ground-truth data,
where the expected performance may not be attainable.
5.2 Ablation Study
In this section, our primary objective is to evaluate the distinct
contributions of each module to the overall efficacy of the SPTAR
framework. Our experiments focus on evaluating the perplexity
and NDCG@10 metrics. The perplexity metric, derived from the
ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™dataset, provided insights into the modelâ€™s text generation
quality. The default NDCG@10 scores in this section are obtained
by evaluating the SPTAR-DPR model trained, evaluated, and tested
onğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘™+ğ‘†100ğ‘’ğ‘£ğ‘+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ ,ğ·ğ‘’ğ‘£ğ‘ğ‘™andğ·ğ‘¡ğ‘’ğ‘ ğ‘¡respectively. We didnâ€™t
filterğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ so that the NDCG@10 score can genuinely represent
the quality of the weak data.
5.2.1 The Impact of Soft Prompt Tuning Module (RQ2). To gain
deeper insights into the optimized parameters ğœƒâˆ—, we employed the
t-SNE algorithm [ 50] to visualize the virtual token vectors of the
learned soft prompt ğ‘“ğœƒâˆ—(ğ‘ )whenğœƒâˆ—are converged with different
datasets and LLMs.
Figure 3a illustrates the distribution of virtual token vectors in a
two-dimensional space. We utilized the LLaMA-7B language model
with a virtual token length ğ‘™ğ‘ =50for this experiment. The red
and blue points indicate the MS MARCO and FiQA datasets, re-
spectively. The visual analysis clearly reveals that the virtual token
8

--- PAGE 9 ---
Retriever MS MARCO FiQA-2018 DL2019 DL2020
MRR@10 Recall@100 MRR@10 Recall@100 MAP nDCG@10 Recall@100 MAP nDCG@10 Recall@100
BM25 0.1840 0.6578 0.2956 0.5395 0.3013 0.5058 0.4910 0.2856 0.4796 0.5599
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ 0.1501 0.6412 0.1726 0.3634 0.2187 0.4243 0.4330 0.2324 0.4028 0.4957
ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’ 0.1606 0.6729 0.2199 0.4871 0.2489 0.4559 0.4806 0.2449 0.4159 0.5177
DPRW/O Aug 0.1303 0.5141 0.1433 0.3738 0.1796 0.3484 0.3289 0.2196 0.3806 0.4218
InPars 0.1519 0.6092 0.2720 0.5289 0.2474 0.4460 0.3919 0.2407 0.3913 0.4646
SPTARâ€ 0.2114â€ 0.7118 0.2885â€ 0.5747â€ 0.3091â€ 0.5253â€ 0.4928â€ 0.3042â€ 0.5219â€ 0.5585
ColBERTW/O Aug 0.0665 0.3157 0.1498 0.3811 0.0679 0.2257 0.1998 0.0654 0.1612 0.2408
InPars 0.1965 0.5232 0.3031 0.4715 0.1908 0.4618 0.3402 0.2278 0.4645 0.4209
SPTAR 0.2000 0.5312â€ 0.3472â€ 0.5107 0.1923 0.4703 0.3332 0.2338 0.4752 0.4259
BM25BEW/O Aug 0.1456 0.6225 0.1659 0.4779 0.2330 0.4211 0.4155 0.2623 0.4236 0.5239
InPars 0.1660 0.6799 0.2891 0.5700 0.2939 0.4905 0.4720 0.2765 0.4286 0.5552
SPTARâ€ 0.2159â€ 0.7222 0.3000â€ 0.6046â€ 0.3379â€ 0.5596â€ 0.4999â€ 0.3194â€ 0.5395â€ 0.5823
BM25CEW/O Aug 0.0876 0.5978 0.2319 0.6013 0.2137 0.3108 0.4337 0.1510 0.2263 0.5055
InPars 0.1889 0.6230 0.3646 0.6155 0.1549 0.2188 0.3732 0.1425 0.1700 0.4780
SPTARâ€ 0.2009â€ 0.7484 0.3705 0.6193â€ 0.3440â˜…â€ 0.5488â€ 0.5459â˜…â€ 0.2921â€ 0.4405â€ 0.6395â˜…
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’W/O Aug 0.1872 0.7228 0.2963 0.5920 0.2876 0.4735 0.4959 0.2944 0.4618 0.5851
InPars 0.1752 0.7253 0.3541 0.6196 0.2704 0.4782 0.4910 0.2748 0.4443 0.5736
SPTARâ€ 0.2148â€ 0.7717â˜…â€ 0.3836â˜…â€ 0.6567â€ 0.3284â€ 0.5272â€ 0.5402â€ 0.3325â€ 0.5241â€ 0.6267
ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘’W/O Aug 0.1938 0.7393 0.3402 0.6212 0.2979 0.4801 0.4913 0.2965 0.4661 0.5902
InPars 0.1743 0.7367 0.3535 0.6500 0.2914 0.4554 0.5323 0.2849 0.4270 0.5927
SPTARâ€ 0.2121â€ 0.7676â€ 0.3757â€ 0.6715â˜…â€ 0.3343â€ 0.5207 0.5451â€ 0.3258â€ 0.5206â€ 0.6231
ğ‘‡ğ´ğ‘†âˆ’ğµW/O Aug 0.0297 0.2046 0.1625 0.3903 0.0525 0.1306 0.1312 0.0388 0.0902 0.1115
InPars 0.2089 0.6911 0.2582 0.5221 0.2828 0.5257 0.4470 0.2982 0.4812 0.5065
SPTARâ€ 0.2541â˜…â€ 0.7353â€ 0.2943â€ 0.5625 0.3049â€ 0.5972â˜…0.4737â€ 0.3488â˜…â€ 0.5612â˜…â€ 0.5690
Table 6: SPTAR vs baseline models: a) ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’is the original Contriever model pre-trained on CC-net and English
Wikipedia; b) ğ‘…ğ‘’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’is the original ReContriever model pre-trained on the same datasets as Contriever; c) W/O Aug
doesnâ€™t use any augmented data and only applies the training data specified in Table 3; d) InPars [ 2] utilizes human-written
prompts and it has no soft prompt filter mechanism; e) Within each method, the best results are underscored and the symbols
â€ denote statistically significant enhancements over the second best result, with p-values < 0.05, as determined by a t-test; f)
The best results cross different methods are denoted with symbol â˜…. g) Table 3 documents the data splits for each method, and
the filtered example document-query pairs of SPTAR are documented in Appendix A and B.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2MSMARCO
FiQA2018
(a) Different datasets.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2GPT2
LLaMA7B
Vicuna7B
 (b) Different LLMs.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2Softpromptlength=40
Softpromptlength=50
Softpromptlength=80
 (c) Different lengths.
Figure 3: T-SNE embedding visualization of soft promptâ€™s virtual tokens: a) soft promptâ€™s virtual tokens with different datasets;
b) soft promptâ€™s virtual tokens with different LLMs; c) virtual tokens of soft prompt with different lengths.
vectors from the two datasets exhibit distinct distributions in the
two-dimensional space, with minimal overlap. Notably, at the model
initialization phase, both datasets share the same prompt ğ‘ , mak-
ing the observed changes in vector distribution after convergenceparticularly significant. These findings highlight the remarkable ca-
pability of prompt tuning to distill domain-specific knowledge from
datasets to the learned prompt token vectors. This accomplishment
is particularly noteworthy in the scenario where ground-truth data
9

--- PAGE 10 ---
are too limited that human-written prompts struggle to capture
domain-specific information and incorporate it effectively into the
prompt design.
In Figure 3b, various colors distinguish distinct LLMs: GPT-2,
LLaMA-7B, and Vicuna-7B. We kept all the hyperparameters the
same except for the language model to evaluate the influence of
different language models on the parameters ğœƒ. The dispersion
of points with the same color indicates the extent of parameter
updated during training. Figure 3b clearly illustrates that the red
point cloud representing the GPT-2 model has less dispersion, with
points tightly clustered together. In contrast, the blue point cloud
representing LLaMA-7B and the green point cloud representing
Vicuna-7B exhibit greater dispersion of virtual token vectors. This
observation suggests that, when trained on the same dataset, the
LLaMA-7B and Vicuna-7B models enable the soft prompt module
to absorb more domain-specific knowledge, leading to an enhance-
ment in the generation of synthesized queries. Moreover, similar
findings were obtained when decoding the virtual tokens into cor-
responding words. For instance, after training the GPT-2 model,
we observed that the resulting soft prompt merely replicates the
prompt tokens used during initialization, essentially duplicating the
manual prompt without additional learning. In contrast, when de-
coding the virtual token vectors into words utilizing the LLaMA-7B
and Vicuna-7B, we discovered that these models not only retain the
initial prompt tokens but also acquire additional symbols and repre-
sentations associated with relevant text, such as â€œ query â€, â€œrewrite â€,
â€œargument â€, â€œenhance â€ and â€œ adding â€, indicating parameters ğœƒdoes
learn task-specific knowledge.
In Figure 3c, we aim to understand the effects of different soft
prompt lengths on the tuning module by examining the virtual to-
ken vector distribution of the learned soft prompt. This experiment
was conducted on LLaMA-7B and dataset MS MARCO and all the
hyperparameters are the same except for the soft prompt length.
The three lengths 40, 50, and 80 are represented by the colors red,
blue, and green, respectively. From the point distribution in Figure
3c, we observe partial overlap between the red and blue points, as
well as some distinct points. As the virtual token length increases,
the embedding distribution area of the longer soft prompt encom-
passes the regions corresponding to the shorter ones: 40 and 50.
This result aligns with our expectation: with different lengths of
soft prompts, the embedding distributions of soft promptsâ€™ virtual
tokens are different. Nevertheless, regardless of their lengths, the
distributions of these prompts tend to have significant overlap and
shared regions.
For RQ2, we have conclusions: a) datasets can be distinguished
from the learned soft prompts, demonstrating that soft prompt
tuning does learn task-specific soft prompts; b) both the LLMs and
the length of soft prompts influence the learned soft prompts.
5.2.2 The Efficiency of Soft-Prompt Tuning (RQ3). Table 7 presents
the number of learnable parameters and convergence efficiency of
soft prompt tuning for different LLMs on the MS MARCO dataset.
For the soft prompt tuning module in our proposed SPTAR, despite
the vast number of LLMâ€™s original parameters Î¦,Î¦remains frozen
and does not require fine-tuning. The trainable parameters ğœƒas-
sociated with the fine-tuning of the soft prompt are substantially
fewer. The percentages in the second column highlight that the softLLMğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğœƒ)/ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(Î¦)Best Epoch #
GPT-2 0.0308% 17
LLaMA-7B 0.0030% 5
Vicuna-7B 0.0030% 4
Table 7: Efficiency evaluation of SPTARâ€™s soft prompt tuning
module on MS MARCO ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™(Section 3.1).
prompt moduleâ€™s fine-tuning involves an exceptionally small set of
parametersğœƒ, roughly equating to 0.003% of the size of Î¦. Notably,
the size ofğœƒstays constant, irrespective of the growth of Î¦. This
characteristic significantly enhances the practicality and training
efficiency of SPTAR, as we can fine-tune task-specific soft prompts
with a minimal fraction of parameters for optimization.
Furthermore, for a new task or dataset, SPTAR can efficiently
complete the fine-tuning process of the soft prompt tuning module
within a few epochs. As highlighted in the third column of the ta-
ble, we examined the convergence speed of the soft prompt tuning
model on the evaluation dataset ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™(Section 3.1) by the best epoch
number and the lower this number is, the faster it converges. It
becomes apparent that employing a more advanced language model
expedites the convergence of the soft prompt tuning module, requir-
ing a mere four or five epochs for convergence. Considering both
the count of ğœƒand the convergence speed, we can confidently con-
clude that the soft prompt tuning module leverages the advantages
offered by LLMs while effectively mitigating the computational
resource consumption associated with fine-tuning the whole LLMs.
In conclusion, the soft prompt tuning model only fine-tunes a
small part of the parameters ğœƒ, and the training converges quickly
on LLMs.
Dataset Filter Perplexity (Dec%) NDCG@10 (Imp%)
MS MARCOWorst 4.1934 0.2132
Best 3.6649 (+12.60%) 0.2376 (+11.44%)
FiQA-2018Worst 410.9207 0.1855
Best 5.7898 (+98.59%) 0.1923 (+3.67%)
Table 8: Evaluation of SPTAR-DPR with the best and worst
example document-query pairs in soft prompt augmentor
module. SPTAR-DPR is trained on ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ and
tested onğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. Results are obtained on LLaMA-7B. For MS
MARCO and FiQA-2018, ğ‘€=2andğ‘€=1respectively.
5.2.3 The Impact of Soft Prompt Filter Module (RQ4). With the
learned parameters ğœƒâˆ—in SPTARâ€™s soft prompt tuning module, we
observe the example document-query pairs in SPTARâ€™s soft prompt
augmentor module do influence the quality of the generated weak
data, so it is necessary to select certain ğ‘€document-query pairs
fromğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. In this section, we study the impact of SPTARâ€™s soft
prompt filter module. In Table 8, we report the best results of SPTAR-
DPR (Section 5.2.6): a) for MS MARCO, we report the results of
SPTAR-DPR with LLaMA-7B and ğ‘€=2; b) for FiQA-2018, we
report the results of SPTAR-DPR with LLaMA-7B and ğ‘€=1. The
SPTAR-DPR is trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ and tested on
10

--- PAGE 11 ---
W/O 10 30 50 700.220.240.26
Top-ğ‘˜of Weak Data FilterNDCG@10MSMARCO
FiQA-2018
Figure 4: SPTAR-DPR NDCG@10 scores with different top- ğ‘˜
of weak data filter. SPTAR-DPR is trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+
ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’)(Section 4.1). Results are obtained on LLaMA-7B.
For MS MARCO and FiQA-2018, ğ‘€=2andğ‘€=1respectively.
ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. The best and worst ğ‘€example pairs in Table 8 are filtered
by the method proposed in Section 3.3.
As shown in Table 8, the results unequivocally demonstrate that
the soft prompt filter significantly enhances performance across
all comparisons. Specifically, we observe a noteworthy 12.60% to
98.59% decrease in perplexity and a substantial 3.67% to11.44% im-
provement on NDCG@10 in the downstream DPR model. Further-
more, our experimental findings indicate that while the utilization
of in-context learning theory, complemented by limited examples,
greatly enhances the quality of generated weak queries, the choice
of example document-query pairs also exerts a considerable influ-
ence on text generation quality.
5.2.4 The Impact of Weak Data Filter Module (RQ5). To assess the
enhancements achieved by filtering weak data, we applied vari-
ous top-ğ‘˜values to filter the generated weak data ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ , yielding
ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’). We then assessed the performance of the SPTAR-DPR
model, trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ¹ğ‘˜(ğ‘Šğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’), onğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. This com-
parison served to quantify the gains over the approach devoid of
a weak data filter. Optimal parameters, namely LLM and ğ‘€, were
identified and held constant in this section to exclusively assess the
influence of top- ğ‘˜.
As shown in Figure 4, on MS MARCO, the SPTAR-DPR model
without the data filter gets an NDCG@10 score of 0.2319 while
the score rises to 0.2580 with data filter top- ğ‘˜=30. On FiQA-2018,
SPTAR-DPR with filter top- ğ‘˜=70 gets the highest NDCG@10 score
of0.2404 , while it gets an NDCG@10 score of 0.2242 without a data
filter. These consistent gains across different datasets underscore
the effectiveness of the weak data filter module (Section 3.4). We
did not discern any correlation between top- ğ‘˜and the NDCG@10
metric; thus, in real-world scenarios, top- ğ‘˜acts as a hyperparameter
requiring tuning per dataset.
5.2.5 The Impact of Training Size ğ‘‹(RQ6). In this section, we ana-
lyze the impact of different training sizes ğ‘‹in SPTARâ€™s soft prompt
tuning module. To evaluate the impact of ğ‘‹, we first conducted
soft prompt tuning on ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and evaluated the perplexity on ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™.
Notably, perplexity serves as an intrinsic metric to measure the im-
pact ofğ‘‹on the quality of generated weak queries. Subsequently,
we generated ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ and tested the SPTAR-DPR model trained
onğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ onğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. NDCG@10 score is applied10 30 5099.699.7
ğ‘‹PercentagePerplexity(Dec%)
10 30 5010203040
ğ‘‹PercentageNDCG@10(Imp%)
Figure 5: Evaluation of SPTAR-DPR with different ğ‘‹com-
pared with W/O (Section 4.1). SPTAR-DPR is trained on
ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ and tested on ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. Results are obtained
on LLaMA-7B and MS MARCO.
to measure the impact of ğ‘‹on downstream DR models, like DPR.
As shown in Figure 5, the findings conclusively demonstrate sub-
stantial improvements when employing soft prompt tuning with
varying training sizes ğ‘‹compared with the results obtained without
soft prompt tuning (W/O in Section 4.1). Specifically, when ğ‘‹=50,
perplexity is decreased by 99.78% , and an impressive 37.66% en-
hancement is observed. Interestingly, itâ€™s apparent that enhancing
perplexity is more straightforward than improving NDCG@10,
suggesting a disparity between these metrics.
Different from InPars [ 2] and Promptagator [ 8], which only
utilizes several example document-query pairs in human-written
prompts, our findings underscore the benefits of a marginally larger
training size ğ‘‹in soft prompt tuning, leading to better performance.
This superiority is manifest in the reduced perplexity and the en-
hanced NDCG@10 scores in downstream tasks with the increment
of training size ğ‘‹.
5.2.6 The Impact of Number of Example Pairs ğ‘€(RQ7). In SPTARâ€™s
soft prompt agumentor module, when tagging the unlabeled docu-
ments with weak queries, ğ‘€filtered example document-query pairs
are utilized to instruct the LLM. In this section, we explore the im-
pact of different ğ‘€. Initially, we selected LLaMA-7B as the LLM and
did soft prompt tuning on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, computing perplexity on ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™.
Subsequently, with the filtered ğ‘€example document-query pairs
from SPTARâ€™s soft prompt filter module (Section 3.3), we generated
ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ . Ultimately, SPTAR-DPR trained on ğ‘†50
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™
is tested on ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡to compute NDCG@10. We also did the same ex-
periments on Vicuna, and we found LLaMA-7B model consistently
delivers better results than the Vicuna-7B model, no matter whether
ğ‘€=1orğ‘€=2. Thus, we only report the results on LLaMA-7B in
Figure 6.
As depicted in Figure 6, for dataset MS MARCO, ğ‘€=2achieves
the best performance in terms of perplexity and NDCG@10. In
contrast, for dataset FiQA-2008, ğ‘€=1demonstrates superior per-
formance. These results contradict our initial assumption that the
biggerğ‘€is the better the perplexity and NDCG@10 are. We at-
tribute this inconsistency to varying dataset distributions. Con-
sidering that many QA datasets contain documents with multiple
relevant queries, where each query is constructed from a subset
of the document, it implies a heightened level of uncertainty and
11

--- PAGE 12 ---
MS MARCO FiQA-2018456Perplexityğ‘€=1
ğ‘€=2
MS MARCO FiQA-20180.160.180.20.220.24NDCG@10ğ‘€=1
ğ‘€=2
Figure 6: Evaluation of SPTAR-DPR with different numbers
of example pairs ğ‘€. SPTAR-DPR is trained on ğ‘†ğ‘‹
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›+ğ‘†100
ğ‘’ğ‘£ğ‘ğ‘™+
ğ‘Šğ‘ ğ‘šğ‘ğ‘™ğ‘™ and tested on ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡. Results are obtained on LLaMA-7B
and MS MARCO.
complexity for the learning model. These intricacies, in turn, pro-
duce varying performances across different datasets. Therefore, we
acknowledge the importance of delving deeper into this topic in
subsequent research.
6 CONCLUSION AND FUTURE WORK
In this paper, we introduce the soft prompt tuning for augmenting
DR (SPTAR) framework to tackle the challenge of limited domain-
specific training data in DR tasks. Our approach harnesses soft
prompt tuning to optimize soft prompts on limited ground truth
data. By prompting LLMs with these optimized soft prompts as
well as example document-query pairs, we generate weak queries
for unlabeled documents, resulting in an abundant collection of
weak document-query pairs for training domain-specific dense
retrievers. To further enhance the quality of the generated weak
tagged queries, we incorporate a soft prompt filter that selects
high-quality example document-query pairs in the prompt as well
as a weak data filter module to clean the generated weak data.
The effectiveness of our proposed approach is validated through
comprehensive experiments. This work represents an initial step
toward a promising research direction. In future work, we aim to
scrutinize SPTARâ€™s broad applicability by testing it across diverse
datasets. Itâ€™s noteworthy that the loss function employed herein
is a pointwise loss, implying a suboptimal utilization of negative
instances. Future studies might benefit from delving into pairwise
and listwise losses. Moreover, there lies potential in probing multi-
task soft prompt tuning methods to bolster both efficiency and
outcome.
A FILTERED EXAMPLE DOCUMENT-QUERY
PAIRS FOR MS MARCO AND LLAMA
For MS MARCO, ğ‘€=2is better than ğ‘€=1when LLaMA is
employed.
A.1ğ‘€=1
Document : According to price comparison website Gocompare.com,
the cost of becoming a new driver has soared by almost a fifth in
the past five years. A survey of 2,000 parents found the average
cost of a young driverâ€™s first car is Ã‚Â£3,825, with insurance pricedat Ã‚Â£2,232.Scroll down for video. Expensive: A survey of 2,000 par-
ents found the average cost of a young driverâ€™s first car is Ã‚Â£3,825.
The typical learner also needs Ã‚Â£480 of driving lessons. survey of
2,000 parents found the average cost of a young driverâ€™s first car is
Ã‚Â£3,825, with insurance priced at Ã‚Â£2,232. Scroll down for video.
Expensive: A survey of 2,000 parents found the average cost of a
young driverâ€™s first car is Ã‚Â£3,825.
Query : average insurance cost for new drivers
A.2ğ‘€=2
Document : Oakland weather forecast from AccuWeather.com. Ex-
tended forecast in Oakland, MD 21550 for up to 25 days includes
high temperature, RealFeel and chance of precipitation Oakland
weather forecast from AccuWeather.com. Extended forecast in Oak-
land, MD 21550 for up to 25 days includes high temperature, Re-
alFeel and chance of precipitation my recent locations Ã‚ Â°f Oakland,
MD 41Ã‚ Â°
Query : weather in oakland md
Document : As their name suggests, the triglycerides are composed
of one molecule of glycerol and joined via ester bonds with three
molecules of fatty acids. As is shown in Figure 12, fatty acids are
long chains of carbon and hydrogen usually between 14-24 carbons
long (and they always have an even number of carbons).. Phos-
pholipids: This class of lipids are really derivatives of triglycerides.
The are composed of a glycerol molecule with two fatty acids (a
diglyceride). The third carbon contains a phosphate group and usu-
ally some added polar molecule (such as ethanolamine, serine or
choline).
Query : what are triglycerides composed of
B FILTERED EXAMPLE DOCUMENT-QUERY
PAIRS FOR FIQA-2018 AND LLAMA
For FiQA-2018, ğ‘€=1is better than ğ‘€=2when LLaMA is em-
ployed.
B.1ğ‘€=1
Document : As your is a very specific case, please get an advice
of CA. It should not cost you much and make it easier. The sale of
agriculture land is taxable in certain conditions and exempt from
tax in other cases. Sale of agricultural land is subject to capital gains
tax. But there are certain exemptions under Section 54B, subject
to conditions, which are as follows: If deemed taxable, you can
avail indexation, ie the price at which you grandfather got [the
date when he inherited it as per indexation] and pay 10% on the
difference. If the price is not known, you can take the govt pre-
scribed rate. As there is a large deposit in your fathers account,
there can be tax queries and need to be answered. Technically there
is no tax liable even if your grandfather gifts the money to your
father. More details at http://www.telegraphindia.com/1130401/jsp/
business/story_16733007.jsp and http://www.incometaxindia.gov.
in/publications/4_compute_your_capital_gains/chapter2.asp
Query : Is the amount taxable if my grandfather sells agricultural
12

--- PAGE 13 ---
land
B.2ğ‘€=2
Document : Others have already commented on the impact of any-
thing which dissuades merchants from raising possible breaches, so
I won Â´t dwell on that. Maybe we need stronger legislation, maybe we
donÂ´t, but it doesn Â´t change todayÅ› answer. Often it works the other
way around to what you might expect - rather than the merchant
noticing and notifying Visa/MC/others, Visa/MC/others spot pat-
terns of suspicious activity (example 1). I don Â´t have any data on the
relative numbers of who is being notified/notifying between mer-
chants and payment processors, but at the point when your card is
identified as compromised thereÅ› no reason to suppose that an indi-
vidual merchant in the traditional sense has been compromised, let
alone identified. In fact because thereÅ› a fast moving investigation
it could even be a false alarm that led to your card getting cancelled.
Conversely it could be a hugely complex multinational investiga-
tion which would be jeopardised. ItÅ› simply not safe to assume that
simply ""brand X"" has been compromised, therefore everything
""brand X"" knows about you is also compromised: Furthermore
thereÅ› no reason to assume the merchant has even admitted to, or
discovered the root cause. MC/Visa/Banks, at the point at which
theyÅ•e cancelling cards simply can Â´t say (at least not in a way that
might expensively backfire involving lots of lawyers) because the
standard of proof needed to go on record blaming someone is sim-
ply not yet met. So: yes itÅ› common that you aren Â´t told anything
for all of the above reasons. And of course if you really want to
find out more you may have some success with your local data
protection legislation and formally make a subject access request
(or local equivalent) to see what that brings back. Be sure to do it in
writing, to the official address of both mastercard and your bank.
Query : MasterCard wonâ€™t disclose who leaked my credit card de-
tails
Document : Surprised nobody has mentioned Freshbooks yet. Itâ€™s
lightweight, easy to use, and free for low-end use (scaling up price-
wise as you scale up).
Query : Whatâ€™s the best application, software or tool that can be
used to track time?
REFERENCES
[1]Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mittal.
2000. Bridging the lexical chasm: statistical approaches to answer-finding. In
SIGIR . ACM, 192â€“199.
[2]Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Fras-
setto Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information
Retrieval. In SIGIR . ACM, 2387â€“2392.
[3]Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,
Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-
pervised Training of Efficient Rankers. CoRR abs/2301.02998 (2023).
[4]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
(2020).
[5]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with
90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/
[6]Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview
of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text
REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA],
November 16-20, 2020 (NIST Special Publication, Vol. 1266) , Ellen M. Voorhees
and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST).
https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf
[7]Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.
Voorhees. 2020. Overview of the TREC 2019 deep learning track. CoRR
abs/2003.07820 (2020). arXiv:2003.07820 https://arxiv.org/abs/2003.07820
[8]Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot
dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[9]Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot
Dense Retrieval without Relevance Labels. CoRR abs/2212.10496 (2022).
[10] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi,
and Dongfang Liu. 2023. EË†2VPT: An Effective and Efficient Approach for Visual
Prompt Tuning. CoRR abs/2307.13770 (2023). https://doi.org/10.48550/arXiv.
2307.13770 arXiv:2307.13770
[11] Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan
Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced
Topic Aware Sampling. In SIGIR â€™21: The 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval, Virtual Event, Canada, July
11-15, 2021 , Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones,
and Tetsuya Sakai (Eds.). ACM, 113â€“122. https://doi.org/10.1145/3404835.3462891
[12] Zhiqi Huang, Hansi Zeng, Hamed Zamani, and James Allan. 2023. Soft Prompt
Decoding for Multilingual Dense Retrieval. CoRR abs/2305.09025 (2023).
[13] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-
mation Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022 (2022).
https://openreview.net/forum?id=jKN1pXi7b0
[14] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park.
2022. Augmenting Document Representations for Dense Retrieval with In-
terpolation and Perturbation. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022 , Smaranda Muresan, Preslav Nakov, and
Aline Villavicencio (Eds.). Association for Computational Linguistics, 442â€“452.
https://doi.org/10.18653/v1/2022.acl-short.48
[15] Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee,
Roberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023.
InPars-v2: Large Language Models as Efficient Dataset Generators for Informa-
tion Retrieval. CoRR abs/2301.01820 (2023).
[16] Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, Jheng-Hong
Yang, and Jimmy Lin. 2024. Resources for Brewing BEIR: Reproducible Reference
Models and Statistical Analyses. (2024).
[17] Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. arXiv preprint arXiv:2004.04906 (2020).
[18] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In SIGIR . ACM, 39â€“48.
[19] Yibin Lei, Liang Ding, Yu Cao, Changtong Zan, Andrew Yates, and Dacheng
Tao. 2023. Unsupervised Dense Retrieval with Relevance-Aware Contrastive
Pre-Training. In Findings of the Association for Computational Linguistics: ACL
2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 10932â€“10940.
https://doi.org/10.18653/V1/2023.FINDINGS-ACL.695
[20] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for
Parameter-Efficient Prompt Tuning. In EMNLP . Association for Computational
Linguistics, 3045â€“3059.
[21] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous
Prompts for Generation. In ACL/IJCNLP . Association for Computational Linguis-
tics, 4582â€“4597.
[22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-
hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
Christopher D. Manning, Christopher RÃ©, Diana Acosta-Navas, Drew A. Hudson,
Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu
Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert YÃ¼ksekgÃ¶nÃ¼l,
Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya
Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,
William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022.
Holistic Evaluation of Language Models. CoRR abs/2211.09110 (2022).
[23] Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya,
John Foley, Grant Ingersoll, Craig MacDonald, and Sebastiano Vigna. 2016. To-
ward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In
ECIR (Lecture Notes in Computer Science, Vol. 9626) . Springer, 408â€“420.
13

--- PAGE 14 ---
[24] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,
and Rodrigo Frassetto Nogueira. 2021. Pyserini: A Python Toolkit for Repro-
ducible Information Retrieval Research with Sparse and Dense Representations.
InSIGIR â€™21: The 44th International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 , Fernando
Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai
(Eds.). ACM, 2356â€“2362. https://doi.org/10.1145/3404835.3463238
[25] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar
Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your
DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval.
CoRR abs/2302.07452 (2023). https://doi.org/10.48550/arXiv.2302.07452
arXiv:2302.07452
[26] Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun
Quan, and Dawei Song. 2022. XPrompt: Exploring the Extreme of Prompt Tuning.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates . Association for Com-
putational Linguistics, 11033â€“11047. https://preview.aclanthology.org/emnlp-
22-ingestion/2022.emnlp-main.758.pdf
[27] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot
Listwise Document Reranking with a Large Language Model. arXiv preprint
arXiv:2305.02156 (2023).
[28] Macedo Maia, Siegfried Handschuh, AndrÃ© Freitas, Brian Davis, Ross McDer-
mott, Manel Zarrouk, and Alexandra Balahur. 2018. WWWâ€™18 Open Challenge:
Financial Opinion Mining and Question Answering. In WWW . ACM, 1941â€“1942.
[29] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
Paul. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods.
https://github.com/huggingface/peft.
[30] Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. 2023. Learning to Compress
Prompts with Gist Tokens. CoRR abs/2304.08467 (2023).
[31] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading
comprehension dataset. choice 2640 (2016), 660.
[32] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a
pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).
[33] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-Shot Learning
with Language Models. In NeurIPS . 11054â€“11070.
[34] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna:
Zero-Shot Listwise Document Reranking with Open-Source Large Language
Models. CoRR abs/2309.15088 (2023). https://doi.org/10.48550/arXiv.2309.15088
arXiv:2309.15088
[35] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen,
Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky.
2023. Large Language Models are Effective Text Rankers with Pairwise Ranking
Prompting. CoRR abs/2306.17563 (2023).
[36] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxi-
ang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training
Approach to Dense Passage Retrieval for Open-Domain Question Answering. In
Proceedings of the 2021 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2021, Online, June 6-11, 2021 , Kristina Toutanova, Anna Rumshisky, Luke Zettle-
moyer, Dilek Hakkani-TÃ¼r, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy
Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics,
5835â€“5847. https://doi.org/10.18653/v1/2021.naacl-main.466
[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485â€“5551.
[38] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu,
Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A Joint Training Method
for Dense Passage Retrieval and Passage Re-ranking. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , Marie-
Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.).
Association for Computational Linguistics, 2825â€“2835. https://doi.org/10.18653/
v1/2021.emnlp-main.224
[39] Stephen Robertson, Hugo Zaragoza, et al .2009. The probabilistic relevance
framework: BM25 and beyond. Foundations and Trends Â®in Information Retrieval
3, 4 (2009), 333â€“389.
[40] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau
Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with
zero-shot question generation. arXiv preprint arXiv:2204.07496 (2022).
[41] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight lateinteraction. arXiv preprint arXiv:2112.01488 (2021).
[42] Teven Le Scao and Alexander M Rush. 2021. How many data points is a prompt
worth? arXiv preprint arXiv:2103.08493 (2021).
[43] Timo Schick and Hinrich SchÃ¼tze. 2021. Exploiting Cloze-Questions for Few-
Shot Text Classification and Natural Language Inference. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , Paola Merlo, JÃ¶rg
Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics,
255â€“269. https://doi.org/10.18653/V1/2021.EACL-MAIN.20
[44] Timo Schick and Hinrich SchÃ¼tze. 2021. Itâ€™s Not Just Size That Matters:
Small Language Models Are Also Few-Shot Learners. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021 , Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-TÃ¼r, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (Eds.). Association for Computational Linguistics, 2339â€“2352.
https://doi.org/10.18653/V1/2021.NAACL-MAIN.185
[45] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun
Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as
Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).
[46] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Ji-
ahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes
generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087
(2022).
[47] Zhengyang Tang, Benyou Wang, and Ting Yao. 2022. DPTDR: Deep Prompt
Tuning for Dense Passage Retrieval. In COLING . International Committee on
Computational Linguistics, 1193â€“1202.
[48] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna
Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of
information retrieval models. arXiv preprint arXiv:2104.08663 (2021).
[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[50] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[51] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier
Benchmark for General-Purpose Language Understanding Systems. (2019), 3261â€“
3275.
[52] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
of Pre-Trained Transformers. (2020).
[53] Zhen Wang, Rameswar Panda, Leonid Karlinsky, RogÃ©rio Feris, Huan Sun, and
Yoon Kim. 2023. Multitask Prompt Tuning Enables Parameter-Efficient Transfer
Learning. In ICLR . OpenReview.net.
[54] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models
are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[55] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping,
and Tom Goldstein. 2023. Hard Prompts Made Easy: Gradient-Based Discrete
Optimization for Prompt Tuning and Discovery. In Advances in Neural Infor-
mation Processing Systems 36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/
a00548031e4647b13042c97c922fadf1-Abstract-Conference.html
[56] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-
tive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808
(2020).
[57] Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. 2023. Prompt
Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration
For Video Captioning. In Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao,
SAR, China . ijcai.org, 1622â€“1630. https://doi.org/10.24963/ijcai.2023/180
[58] Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian
Khabsa, Sinong Wang, Zenglin Xu, and Dongfang Liu. 2023. MixPAVE: Mix-
Prompt Tuning for Few-shot Product Attribute Value Extraction. In Proceedings
of the 61th Annual Meeting of the Association for Computational Linguistics, ACL
2023. Association for Computational Linguistics.
[59] Jie Zhou, Le Tian, Houjin Yu, Zhou Xiao, Hui Su, and Jie Zhou. 2022. Dual
Context-Guided Continuous Prompt Tuning for Few-Shot Learning. In ACL.
Association for Computational Linguistics, 79â€“84.
14
