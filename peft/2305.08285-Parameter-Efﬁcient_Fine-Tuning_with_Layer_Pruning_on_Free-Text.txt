# 2305.08285.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.08285.pdf
# File size: 458530 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Parameter-Efﬁcient Fine-Tuning with Layer Pruning on Free-Text
Sequence-to-Sequence Modeling
Yunqi Zhu1;2Xuebing Yang2;Yuanyuan Wu1Wensheng Zhang1;2;3;
1School of Information and Communication Engineering, Hainan University
2State Key Laboratory of Multimodal Artiﬁcial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences
3Guangzhou University
zhuyunqi96@163.com yangxuebing2013@ia.ac.cn wyuanyuan82@163.com zhangwenshengia@hotmail.com
Abstract
The increasing size of language models raises
great research interests in parameter-efﬁcient
ﬁne-tuning such as LoRA that freezes the pre-
trained model, and injects small-scale train-
able parameters for multiple downstream tasks
(e.g., summarization, question answering and
translation). To further enhance the efﬁciency
of ﬁne-tuning, we propose a framework that
integrates LoRA and structured layer pruning.
The integrated framework is validated on two
created deidentiﬁed medical report summariza-
tion datasets based on MIMIC-IV-Note and
two public medical dialogue datasets. By tun-
ing 0.6% parameters of the original model
and pruning over 30% Transformer-layers, our
framework can reduce 50% of GPU memory
usage and speed up 100% of the training phase,
while preserving over 92% generation quali-
ties on free-text sequence-to-sequence tasks1.
1 Introduction
Pre-trained language model (PLM) has dominated
the natural language processing (NLP) domain be-
cause of its superior performance by ﬁne-tuning the
model to different downstream NLP tasks (Devlin
et al., 2019; Radford et al., 2018; Lewis et al., 2020;
Raffel et al., 2020). With the increasing model size
of PLMs (e.g., 11B for T5 (Raffel et al., 2020),
175B for GPT-3 (Brown et al., 2020) and 540B for
PaLM (Chowdhery et al., 2022)), full ﬁne-tuning
of a PLM requires storing all the updated model pa-
rameters for each task, which would be expensive
and time-consuming.
To alleviate the above issue, parameter-efﬁcient
ﬁne-tuning approaches are emerging. For instance,
Adapter-based tuning (Houlsby et al., 2019; Pfeif-
fer et al., 2020; Wang et al., 2021; Pfeiffer et al.,
2021; Karimi Mahabadi et al., 2021) injects small-
scale trainable neural networks and freezes other
*Corresponding authors.
1The source code and the dataset creation scripts are avail-
able at: https://github.com/zhuyunqi96/LoraLPrun
LinearLinear
Add & Norm
Transformer Layer
LoRAWdown Wup╳
Attention
LoRAWdown Wup╳
Transformer LayerTransformer LayerTransformer LayerTransformer LayerTransformer Layer
…
…
Input ReferenceLinear & SoftmaxOutput Probabilities
Transformer Layer
Transformer LayerTransformer LayerTransformer LayerTransformer LayerTransformer LayerTransformer Layer
…
…Transformer Layer
Frozen Trainable
Pruned
Add & NormFigure 1: Overview of the proposed framework. We
freeze the pre-trained language model and remove
the Transformer-layers in an alternating fashion, then
LoRA is injected on the feed-forward networks.
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000015
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000014/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024QV
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024QV/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024FF
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024FF/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000015
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000015/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024QV
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024QV/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024FF
/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000052/uni00000035/uni00000024FF/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000010/uni00000033/uni00000055/uni00000058/uni00000051
Figure 2: ROUGE-1 and ROUGE-2 evaluations of full
ﬁne-tuning, LoRA and L-Prun with BART-large on
MIMIC-IV-discharge.
parameters of a PLM during training. Based on
the prior knowledge of downstream tasks, prompt-
based tuning (Li and Liang, 2021; Liu et al., 2019;
Lester et al., 2021; Liu et al., 2022) includes task-
speciﬁc trainable contextual tokens around the in-
put sequence and freezes the original input during
training. Recently, low-rank adaptation (LoRA)
(Hu et al., 2022), a variant based on Adapter-
based tuning but removes nonlinear activation,
has become increasingly popular for small-scale
parameter-tuning. Besides, pruning of the PLM
serves to remove some parameters of the model
while largely perserving the capacity for down-
stream tasks (Jiao et al., 2020; Zhang and He, 2020;
Sanh et al., 2020; Fan et al., 2020; Sajjad et al.,arXiv:2305.08285v3  [cs.CL]  19 May 2023

--- PAGE 2 ---
2023; Lagunas et al., 2021; Xia et al., 2022), which
is usually processed in an adaptive or structured
fashion.
Since both Adapter-based tuning and pruning
hypothesize PLMs are over-parameterized, we in-
tegrate LoRA (Hu et al., 2022) and structured layer
pruning (L-Prun) (Fan et al., 2020; Sajjad et al.,
2023) on PLMs, and Figure 1 illustrates the over-
all framework of integrating LoRA and L-Prun on
a Transformer-based (Vaswani et al., 2017) PLM.
Our proposal is to offer a preliminary framework
that enables ﬂexible and lightweight deployment of
small-scale plug-in learned parameters with LoRA,
while scaling down PLMs with L-Prun.
The proposed framework is validated on two
downstream tasks (i.e., Medical Report Summa-
rization and Medical Dialogue Generation). Based
on MIMIC-IV-Note (Johnson et al., 2023a,c,b), we
ﬁrstly create two free-text medical deidentiﬁed re-
port summarization datasets, named MIMIC-IV-
discharge and MIMIC-IV-radiology. Further, we
implement extensive experiments with sequence-
to-sequence (Seq2Seq) modeling, including BART-
large (Lewis et al., 2020) and T5-large (Raffel et al.,
2020), on both two datasets and two extra pub-
licly available medical dialogue datasets (Health-
CareMagic and iCliniq) (Zeng et al., 2020; Li et al.,
2023). Figure 2 shows the performances of full
ﬁne-tuning, LoRA and L-Prun with pre-trained
BART-large, where the generation quality of our
framework is promising.
Our contributions can be summarized as fol-
lows: (1) We propose a framework that integrates
small-scale parameter-tuning module LoRA and
structured L-Prun. To the best of our knowledge,
we are the ﬁrst to combine LoRA with L-Prun on
PLMs. (2) Based on MIMIC-IV-Note, we create
two medical report summarization datasets. We
implement comprehensive experiments on the pro-
posed datasets and the other two medical dialogue
generation datasets. (3) By updating 0.6% parame-
ters over the original PLM and pruning more than
30% Transformer-layers, the proposed method can
double the training speed and save 50% of GPU
memory usage, while perserving over 92% genera-
tion qualities on medical Seq2Seq tasks.
2 Related Work
Transformer (Vaswani et al., 2017) proposed
the Transformer model, containing an encoder and
a decoder for the representation learning of inputand output sequences, respectively. The Trans-
former encoder layer consists of an attention block
(ATTN), a feed-forward network (FFN), residual
connections and layer normalizations. The Trans-
former decoder layer contains an additional cross-
attention block right after the original ATTN, and
the cross-attention has the same design as the self-
attention. Designed with up- and down-projection,
the FFN is two fully connected layers (i.e., multi-
layer perceptrons (MLPs)).
Parameter-Efﬁcient Fine-Tuning Pre-training
a large language model with self-supervised lan-
guage modeling and ﬁne-tuning it to multiple down-
stream domains is a widespread paradigm (Devlin
et al., 2019; Radford et al., 2018; Lewis et al., 2020;
Raffel et al., 2020). (Ding et al., 2023) catego-
rize popular parameter-efﬁcient ﬁne-tuning meth-
ods into three types: Addition ,Speciﬁcation and
Reparameterization .
Addition : Adapter-based tuning (Houlsby et al.,
2019; Pfeiffer et al., 2020; Wang et al., 2021; Pfeif-
fer et al., 2021; Karimi Mahabadi et al., 2021) in-
jects lightweight down- and up-projection MLPs
and a nonlinear activation function, and prompt-
based tuning (Li and Liang, 2021; Liu et al., 2019;
Lester et al., 2021; Liu et al., 2022; Gu et al., 2022)
freezes the input and introduces additional train-
able contexts (based on the prior knowledge of
NLP tasks) as the preﬁx, sufﬁx or semi-template
for the original input.
Speciﬁcation : ﬁne-tuning only speciﬁed or adap-
tive limited number of layers, modules, weights and
biases of the model (Lee et al., 2019; Zhao et al.,
2020; Guo et al., 2021; He et al., 2022; Ben Zaken
et al., 2022), this type of methods can be the most
memory-efﬁcient for training because no additional
parameter is involved.
Reparameterization : based on the hypothesis
that adaptations of the PLM to downstream NLP
tasks can be reparameterized into a low intrinsic
rank optimization (Rebufﬁ et al., 2017; Li et al.,
2018; Aghajanyan et al., 2021), LoRA (Hu et al.,
2022) inherits the design of Adapter (Houlsby
et al., 2019) but removes the nonlinear activation.
Therefore, the multiplied weights of down- and
up-projection MLPs can be directly added to the
weight of the injection target in the inference phase.
In addition, there are alternative methods to ef-
ﬁciently leverage the pre-training language model,
including Knowledge Distillation (Hinton et al.,
2019; Kim and Rush, 2016; Sanh et al., 2019; Jiao

--- PAGE 3 ---
et al., 2020) that maximizes the similarity between
the teacher (large model) and the student’s (dis-
tilled model) predictions, Quantization (Jacob et al.,
2018; Wang et al., 2018; Dettmers et al., 2022) that
converts a 16/32-bit model to an 8-bit or even lower
one, which is both efﬁcient for training and infer-
encing.
Parameter Pruning A large pre-trained model
can be over-parameterized for downstream NLP
tasks, (Jiao et al., 2020; Zhang and He, 2020; Sanh
et al., 2020; Fan et al., 2020; Sajjad et al., 2023;
Lagunas et al., 2021; Xia et al., 2022) showed that
structured and adaptive pruning of the PLM can
be competent enough for many classiﬁcation or
Seq2Seq scenarios. Structured pruning can be
based on an empirical or heuristic choice of the
model, whilst unstructured pruning may rely on a
few pre-deﬁned thresholds or trainable parameters
to adaptively decide which MLP, attention block or
the entire layer should be dropped.
Additionally, alternative parameter pruning
methods like dropping or merging the hidden state
of unimportant tokens of the sequence (Goyal et al.,
2020; Guo et al., 2022; Kim et al., 2022; Hou et al.,
2022), and implementing sparse attention (Child
et al., 2019; Beltagy et al., 2020; Kitaev et al., 2020)
contribute to decrease memory usage but could not
decrease the model size.
3 Methodology
We propose a hybrid framework of LoRA and
structured layer pruning that can considerably de-
crease the trained parameters, lower the memory
usage, increase the training speed, and compress
the model size for downstream tasks. The moti-
vation for this integration is to provide a prelimi-
nary framework that can scale down the PLM on
downstream NLP tasks through L-Prun, and en-
able ﬂexible and lightweight deployment of small-
scale plug-in trained parameters through LoRA.
We ﬁrst delete the Transformer-layers in an alter-
nating fashion and then inject LoRA for the model,
hence the method does not require a whole adap-
tive pruning stage after the model is ﬁne-tuned.
Visualization of the framework is shown in Fig-
ure 1. Formally, given a PLM with Pmparameters,
LoRA enables Pm:loratrainable parameters, where
Pm:loraPm, and L-Prun makes the parame-
ters of PLM shrink to Pprun, wherePprun<Pm.
ForNdownstream tasks, the overall storage is
NPmfor full ﬁne-tuning, NPm:lora+Pmif LoRA is enabled, and NPprun :lora+Pprun
if both LoRA and L-Prun are enabled. Note that
Pprun :lora<Pm:loradue to LoRA is injected into
all Transformer-layers. In the following, we brieﬂy
introduce LoRA and Layer Pruning, as well as clar-
ify our implementation.
LoRA (Hu et al., 2022) proposed a low-rank
adaptation method that freezes the weights of a
Transformer-based PLM, and injects two trainable
dense layers ( Wdown2RrdandWup2Rdr) in
a dense layer W02Rdd, where the rank rd.
Therefore the output exof LoRA is:
ex=W0x+
rWupWdownx (1)
whereis a constant scaling factor. LoRA can
be viewed as a variant of Adapter (Houlsby et al.,
2019) containing down- and up-projection MLPs.
However, unlike conventional Adapter methods ap-
plying the nonlinear activation between two train-
able dense layers, LoRA can retain a low inference
latency after the training phase by merging the orig-
inal frozen weights with LoRA’s injection:
fW0=W0+
rWupWdown (2)
With the down- and up-projection, LoRA has much
less parameters than the injection target.
Originally, LoRA (LoRA QV) was applied on the
Query dense layer and the Value dense layer in
each of the attention module. Further researches
(Zhang et al., 2023; Lei et al., 2023) showed that
injecting LoRA (LoRA FF) in FFN could be a su-
perior choice. Recently, Adapter-based parameter
pruning researches (Rücklé et al., 2021; Hedegaard
et al., 2022; Zhang et al., 2023; Wang et al., 2023)
focus on adaptively removing redundant trainable
parameters within the LoRA unit, and adaptively
skipping unimportant tokens’ hidden states (Lei
et al., 2023), and exploring the optimal conﬁgura-
tion of parameter-efﬁcient learning methods (Wang
et al., 2022; Yu et al., 2023).
Layer Pruning (Fan et al., 2020; Sajjad et al.,
2023) proposed strategies of structured pruning
of Transformer-layers inside a Transformer-based
PLM for ﬁne-tuning downstream tasks. The empiri-
cal experiments show that deleting the Transformer-
layers in an alternating fashion and preserving a
few top layers and bottom layers can sustain the
capacity of language models. Meanwhile, due to
the reduction of the model size, the training phase

--- PAGE 4 ---
and the inference phase can be greatly accelerated.
We symmetrically drop the Transformer-layers in
the encoder and the decoder, for a PLM with n
layers of the encoder and nlayers of the decoder:
fl1;:::;l ng. Denotes the dropped layers as li, where
i= 2k;k2N;i2[4;n 2]. Therefore, a total of
2(n 4)layers are dropped.
4 Experiments
4.1 Datasets
Based on MIMIC-IV-Note (Johnson et al.,
2023a,c,b), a publicly available deidentiﬁed free-
form clinical notes with 331K discharge reports and
2.3M radiography reports at the Beth Israel Dea-
coness Medical Center in Boston, MA, USA, we
create two medical report summarization datasets
with regular string pattern matching over the report
subsection titles, named MIMIC-IV-discharge and
MIMIC-IV-radiology.
We conduct extensive experiments on four
medical datasets, including MIMIC-IV-discharge,
MIMIC-IV-radiology, HealthCareMagic (Zeng
et al., 2020; Li et al., 2023) and iCliniq (Zeng et al.,
2020; Li et al., 2023). Statistics of the four datasets
are shown in Table 1.
Summarization We use MIMIC-IV-discharge
and MIMIC-IV-radiology for medical report sum-
marization. (i)MIMIC-IV-discharge: from
MIMIC-IV-Note, a discharge report would involve
the notes of addmision, medical history, health care,
procedure, etc. The corresponding discharge sum-
mary contains the overall diagnosis of the patient.
(ii)MIMIC-IV-radiology: from MIMIC-IV-Note,
written with semi-structured templates, a radiog-
raphy report contains the full description of the
medical imaging results, while the corresponding
summary includes free-text highlights of the report.
Dialogue We use HealthCareMagic and iCliniq
for medical dialogue generation. (i)Health-
CareMagic: a patient-doctor conversations dataset
crawled from a online medical consultation plat-
form2. We used a “HealthCareMagic-200K” ver-
sion from (Li et al., 2023). (ii)iCliniq: a patient-
doctor conversations dataset crawled from an on-
line medical consultation platform3. We used a
“iCliniq-15K” version (Li et al., 2023).
2healthcaremagic.com
3iclinic.com4.2 Implementation Details
We implement the experiments with the pre-trained
BART-large and T5-large. We set hyperparameter
rankras 16 and factor as 32 for LoRA in this
work. Forr, additional experiments (Appendix A.1
Table 6) show that 16 will be a balanced and com-
petent choice. In Table 2, we show the speciﬁc
perserving indexes of encoder and decoder layers
for BART-large and T5-large. Hence, 33% and 41%
of the Transformer-layers are dropped for BART-
large and T5-large. Next, based on the empirical
hyperparameter settings, we use PyTorch frame-
work with mixed-precision, AdamW (Loshchilov
and Hutter, 2019) optimizer ( 1= 0.1,2= 0.999,
=10 8) and warm-up steps of 1000. We ﬁne-tune
the original model and L-Prun model at a learn-
ing rate of 510 5, and ﬁne-tune the model with
LoRA at a learning rate of 110 4. Furthermore,
the maximum source lengths of 1024 and 512 are
set for summarization and dialogue datasets, re-
spectively. All of the maximum target lengths are
128. A batch size of 8 is applied for all the experi-
ments. However, we use a setting: (batch size: 4;
gradient accumulation: 2) only when ﬁne-tuning
T5-large on MIMIC-IV-discharge because of lim-
ited GPU memory, while the gradient accumulation
is disabled in other cases. We ﬁne-tune the datasets
with 10 epochs. For every 0.3 epoch, the model
is evaluated, and the checkpoint with the highest
ROUGE-1 score is loaded for the test set. We use
beam search during the autoregressive decoding
with a beam width of 6. All the experiments are
conducted on a single NVIDIA A40 48GB GPU.
4.3 Evaluation Metrics
The machine-generated outputs are evaluated with
the following metrics. Note that the machine-
generated summaries are evaluatd with ROUGE,
BERTScore and SummaC, while the machine-
generated dialogues are evaluatd with ROUGE,
BERTScore and BLEU.
ROUGE (Lin, 2004) is an N-gram-based recall-
oriented metric popular for evaluating automatic
summarization. R-1, R-2 and R-L represent the
overlapping degree of unigram, bigram and longest
common subsequence between the candidate and
the reference, respectively.
BERTScore (Zhang et al., 2020) is a contextual
semantic evaluation metric based on a pre-trained
BERT, which compares the cosine similarity be-

--- PAGE 5 ---
Dataset Train/Eval/TestAvg Input Avg Output % novel
Sents Words Sents Words unigram bigram
MIMIC-IV-discharge 254K / 31K / 31K 88.5 1879.9 1.1 13.8 18 58
MIMIC-IV-radiology 1.2M / 0.15M / 0.15M 10.0 132.1 2.9 38.4 40 73
HealthCareMagic 165K / 20K / 20K 4.9 89.9 6.6 90.7 80 98
iCliniq 12k / 1.5K / 1.5K 7.2 111.3 9.4 126.8 77 97
Table 1: Statistics of the datasets. % novel N-gram represents the ratio of unseen N-grams from the reference
output against the source input.
BART-largeEncoder 123 4 5 6 7 8 9101112
Decoder 123 4 5 6 7 8 9101112
T5-largeEncoder 123 4 5 6 7 8 91011 12 13 14 15 16 17 18 19 20 21 22 2324
Decoder 123 4 5 6 7 8 91011 12 13 14 15 16 17 18 19 20 21 22 2324
Table 2: Remaining Transformer-layers of BART-large
+L-Prun and T5-large +L-Prun are highlighted in
grey.
tween the latent representation of the [CLS] token
of a candidate with a reference’s.
SummaC (Laban et al., 2022) is a reference-
free lightweight factual consistency evaluation
metric for automatic summarization. Using ﬁne-
tuned natural language inference (NLI) model, the
framework detects the sentence-level inconsistency
between the source document and the machine-
generated summary.
BLEU (Papineni et al., 2002) is a popular N-
gram-based precision-oriented evaluation metric
that compares the overlapping degree in tokens be-
tween the machine-generated text and reference
text. BLEU- Nindicates the weighted average of
the N-grams matching evaluation, N=f1;:::;Ng,
and we report BLEU-1 and BLEU-4 for the dia-
logue datasets.
4.4 Baselines and Parameters
We implement the proposed method with BART-
large and T5-large:
BART-large (Lewis et al., 2020): a Transformer-
based encoder-decoder model pre-trained on a
combination of English books, news, stories and
Wikipedia paragraphs with masked language mod-
eling. BART-large (406M parameters) has 12 lay-
ers of encoder, 12 layers of decoder, a hidden state
size of 1024 and a vocabulary size of 50K.
T5-large (Raffel et al., 2020): a Transformer-
based encoder-decoder model pre-trained on a large
clean corpus (C4) in English with masked language
modeling, the pre-trained model implements multi-
ple NLP training tasks info a text-to-text paradigm.
T5-large (770M parameters) contains 24 layers ofModel Speed Trained Params Mem Used R-1 R-2
BART-large 100 % 406.2 M 34.56 GB 43.46 25.05
+L-Prun 145 % 288.7 M 23.65 GB 42.93 24.72
+LoRA QV 115 % 2.4 M 28.15 GB 40.32 22.66
+LoRA QV+L-Prun 170 % 1.2 M 19.13 GB 37.84 20.94
+LoRA FF 125 % 4.0 M 27.25 GB 41.60 23.77
+LoRA FF+L-Prun 179 % 2.7 M 17.99 GB 39.96 22.79
Table 3: Comparison of different LoRA insertion meth-
ods and L-Prun methods in BART-large on MIMIC-IV-
discharge.
encoder, 24 layers of decoder, a hidden state size
of 1024 and a vocabulary size of 32K.
4.5 Experimental Results
LoRA QVor LoRA FF In Table 3, we show the
performances of inserting LoRA in Query and Key
verses FFN (i.e., LoRA QVand LoRA FF). “Speed”
refers to the relative speed of the training phase,
and the original full ﬁne-tuning is considered as the
speed baseline. Since a Transformer decoder layer
has two attention blocks and one FFN, LoRA QVis
rather slower than LoRA FFin the training phase.
BART-large +LoRA FF+L-Prun is the overall
practical solution for fast ﬁne-tuning, low GPU
memory usage, and good summarization quality.
Therefore, the following experimental results cor-
respond to LoRA FFwith L-Prun. Note that the
metrics Speed, R1, R2, R-L, BERTScore, Summac
are the higher the better.
Next, we present the experimental result in Ta-
ble 4a (MIMIC-IV-discharge), Table 4b (MIMIC-
IV-radiology), Table 5a (HealthCareMagic) and
Table 5b (iCliniq).
Medical Report Summarization In Table 4a
and Table 4b, the evaluations among ROUGE and
BERTScore show that applying L-Prun alone does
not signiﬁcantly drop the performance of either
BART-large or T5-large. Second, employing LoRA
and L-Prun merely consume 68% and 75% as much
GPU memory as the original full ﬁne-tuning, re-
spectively. Further, the result of SummaC is getting
better when the overall ROUGE and BERTScore

--- PAGE 6 ---
Model Speed Trained Params Mem Used R-1 R-2 R-L BERTScore SummaC
BART-large 100 % 406.2 M 34.56 GB 43.46 25.05 43.03 87.13 56.45
+L-Prun 145 % 288.7 M 23.65 GB 42.93 24.72 42.48 87.09 56.70
+LoRA FF 125 % 4.0 M 27.25 GB 41.60 23.77 41.20 86.85 56.25
+LoRA FF+L-Prun 179 % 2.7 M 17.99 GB 39.96 22.79 39.54 86.44 56.65
T5-large 100 % 737.6 M 42.49 GB 41.22 24.09 38.82 85.80 55.24
+L-Prun 169 % 443.9 M 25.12 GB 40.70 23.94 38.46 85.71 55.86
+LoRA FF 115 % 8.1 M 31.15 GB 38.75 22.34 36.79 85.26 57.97
+LoRA FF+L-Prun 197 % 4.7 M 17.97 GB 37.86 21.79 35.99 85.03 56.28
(a) MIMIC-IV-discharge
Model Speed Trained Params Mem Used R-1 R-2 R-L BERTScore SummaC
BART-large 100 % 406.2 M 27.01 GB 58.35 41.08 57.06 91.62 38.83
+L-Prun 142 % 288.7 M 18.55 GB 58.34 41.09 57.05 91.63 39.04
+LoRA FF 136 % 4.0 M 20.24 GB 55.20 37.40 53.91 91.06 40.75
+LoRA FF+L-Prun 202 % 2.7 M 13.43 GB 54.02 36.36 52.79 90.89 40.88
T5-large 100 % 737.6 M 30.98 GB 57.19 40.05 55.00 89.56 42.88
+L-Prun 162 % 443.9 M 18.46 GB 56.53 39.34 54.36 89.48 42.73
+LoRA FF 153 % 8.1 M 19.87 GB 53.37 35.39 51.12 88.99 44.29
+LoRA FF+L-Prun 206 % 4.7 M 11.66 GB 52.22 34.36 50.03 88.90 44.33
(b) MIMIC-IV-radiology
Table 4: Experiments on medical report summarization datasets.
Model Speed Trained Params Mem Used R-1 R-2 R-L BLEU-1 BLEU-4
BART-large 100 % 406.2 M 15.09 GB 27.43 7.16 25.30 27.57 3.03
+L-Prun 127 % 288.7 M 10.59 GB 27.38 7.16 25.28 27.78 3.03
+LoRA FF 162 % 4.0 M 9.47 GB 26.75 6.48 24.75 27.38 2.42
+LoRA FF+L-Prun 211 % 2.7 M 6.51 GB 26.70 6.56 24.79 27.80 2.43
T5-large 100 % 737.6 M 31.01 GB 26.97 6.83 24.84 26.92 2.67
+L-Prun 158 % 443.9 M 18.48 GB 26.98 6.82 24.93 26.82 2.65
+LoRA FF 132 % 8.1 M 19.87 GB 27.68 6.40 25.74 27.71 1.56
+LoRA FF+L-Prun 222 % 4.7 M 11.66 GB 27.27 6.41 25.56 29.04 1.81
(a) HealthCareMagic
Model Speed Trained Params Mem Used R-1 R-2 R-L BLEU-1 BLEU-4
BART-large 100 % 406.2 M 17.41 GB 28.19 7.66 26.35 34.71 2.91
+L-Prun 138 % 288.7 M 12.21 GB 28.13 7.32 26.32 33.57 2.79
+LoRA FF 158 % 4.0 M 11.65 GB 26.07 6.49 24.56 33.18 2.73
+LoRA FF+L-Prun 230 % 2.7 M 7.98 GB 27.35 6.54 25.81 32.25 2.25
T5-large 100 % 737.6 M 35.19 GB 27.63 7.39 25.96 34.98 3.23
+L-Prun 164 % 443.9 M 20.92 GB 28.58 7.47 26.86 33.87 2.96
+LoRA FF 128 % 8.1 M 23.96 GB 28.25 6.77 26.43 32.94 2.25
+LoRA FF+L-Prun 217 % 4.7 M 13.98 GB 27.65 6.82 25.93 34.50 2.26
(b) iCliniq
Table 5: Experiments on medical dialogue datasets.

--- PAGE 7 ---
are decreasing, however it could be because the
factual consistency metric trained on general cor-
pus cannot meet the requirement of the medical
domain. Furthermore, the integration of LoRA and
L-Prun enables the models to train 0.6% parame-
ters over the original model and to decrease 33%
or 41% Transformer-layers of BART-large or T5-
large, while only a roughly 8% performance drop
with ROUGE scores and BERTScore is observed
compared to the original full ﬁne-tuning.
Medical Dialogue Generation As shown in Ta-
ble 5a and Table 5b, the dialogue generation
datasets is more challenging than the summariza-
tion’s because the % novel N-gram of the reference
output for the dialogue datasets is almost 100%
(Table 1), but the proposed method only brings
marginal performance degradation (< 5%) on R-
1 and BLEU-1. In addition, the overall speedup
and GPU memory saving are consistent with the
summarization’s, and up to +122% faster training
speed is accomplished on HealthCareMagic.
Overall, the key results show that the proposed
method is a promising energy-saving approach.
Take BART-large as an example, the total saved
parameters for four Seq2Seq tasks with different
methods are shown as follows:
• Full ﬁne-tuning: 4 406.2M
•+LoRA FF: 44.0M +406.2M
•+L-Prun: 4 288.7M
•+LoRA FF+L-Prun: 42.7M +288.7M
5 Conclusion
We propose a parameter-efﬁcient ﬁne-tuning frame-
work that integrates LoRA and L-Prun. Based
on the assumption PLMs are over-parameterized
for many downstream NLP tasks, the proposed
method uses small-scale trainable parameters to
leverage PLMs, and reduce the overall model size
in a structured layer pruning way. Further, we cre-
ated two medical report summarization datasets
from MIMIC-IV-Note, and validated the proposed
method on medical report summarization and med-
ical dialogue generation. By ﬁne-tuning 0.6% pa-
rameters of the PLM , dropping more than 30%
Transformer-layers in the PLM, PLMs can sustain
over 92% generation qualities on medical Seq2Seq
tasks and double the speed of training.
Limitations
(i)This study presents the integration of LoRA
and structured L-Prun, an automated self-adaptivestructured L-Prun or block-pruning framework
with Adapter/LoRA-based parameter-efﬁcient ﬁne-
tuning would be worthy of further research. (ii)The
proposed framework can be orthogonal to the other
efﬁcient ﬁne-tuning approaches, such as Knowl-
edge Distillation and Quantization. Exploring an
holistic integration paradigm for better energy-
saving training is critical for future studies. (iii)
This study implements the proposed framework
on medical text-to-text datasets, further research
could focus on the parameter-efﬁcient ﬁne-tuning
of multi-modal generation tasks.
Ethics Statement
It is worth noting that the language model cannot
be relied upon to generate outputs that are factu-
ally accurate, reliable, and knowledge-based for
medical domain. Infusing knowledge- and fact-
based information through prompt engineering and
calibration may alleviate the issue.
References
Armen Aghajanyan, Sonal Gupta, and Luke Zettle-
moyer. 2021. Intrinsic dimensionality explains the
effectiveness of language model ﬁne-tuning. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 7319–
7328, Online. Association for Computational Lin-
guistics.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv preprint arXiv: 2004.05150 .
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. BitFit: Simple parameter-efﬁcient ﬁne-tuning
for transformer-based masked language-models. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1–9, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems
(NeurIPS) , volume 33, pages 1877–1901. Curran As-
sociates, Inc.

--- PAGE 8 ---
Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences
with sparse transformers. arXiv preprint arXiv:
1904.10509 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fe-
dus, Denny Zhou, Daphne Ippolito, David Luan,
Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,
Ryan Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
Palm: Scaling language modeling with pathways.
arXiv preprint arXiv: 2204.02311 .
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Gpt3.int8(): 8-bit matrix multi-
plication for transformers at scale. In Advances in
Neural Information Processing Systems (NeurIPS) ,
volume 35, pages 30318–30332. Curran Associates,
Inc.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL) , pages 4171–4186, Minneapolis,
Minnesota.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,
Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei
Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong
Sun. 2023. Parameter-efﬁcient ﬁne-tuning of large-
scale pre-trained language models. Nature Machine
Intelligence , 5:220–235.
Angela Fan, Edouard Grave, and Armand Joulin. 2020.
Reducing transformer depth on demand with struc-
tured dropout. In International Conference on
Learning Representations (ICLR) .
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
Raje, Venkatesan Chakaravarthy, Yogish Sabharwal,
and Ashish Verma. 2020. PoWER-BERT: Acceler-
ating BERT inference via progressive word-vector
elimination. In Proceedings of the InternationalConference on Machine Learning (ICML) , volume
119 of Proceedings of Machine Learning Research ,
pages 3690–3699. PMLR.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2022. PPT: Pre-trained prompt tuning for few-shot
learning. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 8410–8423, Dublin,
Ireland. Association for Computational Linguistics.
Demi Guo, Alexander Rush, and Yoon Kim. 2021.
Parameter-efﬁcient transfer learning with diff prun-
ing. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 4884–4896, Online. Association for Computa-
tional Linguistics.
Mandy Guo, Joshua Ainslie, David Uthus, Santiago
Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei
Yang. 2022. LongT5: Efﬁcient text-to-text trans-
former for long sequences. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022 ,
pages 724–736, Seattle, United States. Association
for Computational Linguistics.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,
Piotr Dollár, and Ross Girshick. 2022. Masked au-
toencoders are scalable vision learners. In Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 16000–
16009.
Lukas Hedegaard, Aman Alok, Juby Jose, and Alexan-
dros Iosiﬁdis. 2022. Structured pruning adapters.
arXiv preprint arXiv: 2211.10155 .
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2019.
Distilling the knowledge in a neural network. arXiv
preprint arXiv: 1503.02531 .
Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin
Wu, Xinying Song, Xiaodan Song, and Denny Zhou.
2022. Token dropping for efﬁcient BERT pretrain-
ing. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 3774–3784, Dublin, Ire-
land. Association for Computational Linguistics.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for nlp.
InProceedings of the International Conference on
Machine Learning (ICML) .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-rank adaptation of
large language models. In International Conference
on Learning Representations (ICLR) .

--- PAGE 9 ---
Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng-
long Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. 2018. Quanti-
zation and training of neural networks for efﬁcient
integer-arithmetic-only inference. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR) .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2020. TinyBERT: Distilling BERT for natural lan-
guage understanding. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
4163–4174, Online. Association for Computational
Linguistics.
Alistair Johnson, Lucas Bulgarelli, Tom Pollard,
Steven Horng, Leo Anthony Celi, and Roger Mark.
2023a. Mimic-iv. PhysioNet .
Alistair Johnson, Lucas Bulgarelli, Lu Shen, Alvin
Gayles, Ayad Shammout, Steven Horng, Tom J. Pol-
lard, Sicheng Hao, Benjamin Moody, Brian Gow,
Li wei H. Lehman, Leo A. Celi, and Roger G. Mark.
2023b. Mimic-iv, a freely accessible electronic
health record dataset. Scientiﬁc Data , 10.
Alistair Johnson, Tom Pollard, Steven Horng, Leo An-
thony Celi, and Roger Mark. 2023c. Mimic-iv-note:
Deidentiﬁed free-text clinical notes. PhysioNet .
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efﬁcient low-rank
hypercomplex adapter layers. In Advances in Neural
Information Processing Systems (NeurIPS) , pages
1022–1035.
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gho-
lami, Woosuk Kwon, Joseph Hassoun, and Kurt
Keutzer. 2022. Learned token pruning for transform-
ers. In Proceedings of the 28th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining ,
KDD ’22, page 784–794, New York, NY , USA. As-
sociation for Computing Machinery.
Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1317–1327,
Austin, Texas. Association for Computational Lin-
guistics.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efﬁcient transformer. In Inter-
national Conference on Learning Representations
(ICLR) .
Philippe Laban, Tobias Schnabel, Paul N. Bennett, and
Marti A. Hearst. 2022. Summac: Re-visiting nli-
based models for inconsistency detection in summa-
rization. Transactions of the Association for Compu-
tational Linguistics , 10:163–177.
François Lagunas, Ella Charlaix, Victor Sanh, and
Alexander Rush. 2021. Block pruning for fastertransformers. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 10619–10629, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What
would elsa do? freezing layers during transformer
ﬁne-tuning. arXiv preprint arXiv: 1911.03090 .
Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua
Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vin-
cent Y . Zhao, Yuexin Wu, Bo Li, Yu Zhang, and
Ming-Wei Chang. 2023. Conditional adapters:
Parameter-efﬁcient transfer learning with fast infer-
ence. arXiv preprint arXiv: 2304.04947 .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 3045–3059, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics (ACL) , pages 7871–7880, Online.
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Ja-
son Yosinski. 2018. Measuring the intrinsic dimen-
sion of objective landscapes. In International Con-
ference on Learning Representations (ICLR) .
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
4582–4597, Online. Association for Computational
Linguistics.
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and
You Zhang. 2023. Chatdoctor: A medical chat
model ﬁne-tuned on llama model using medical do-
main knowledge.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL) , pages 74–81.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-
iao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:
Prompt tuning can be comparable to ﬁne-tuning
across scales and tasks. In Proceedings of the 60th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers) , pages
61–68, Dublin, Ireland. Association for Computa-
tional Linguistics.

--- PAGE 10 ---
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2019. Gpt un-
derstands, too. arXiv preprint arXiv: 2103.10385 .
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Con-
ference on Learning Representations (ICLR) .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL) , pages 311–318.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 487–503, Online. Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7654–7673, Online. Association for Computa-
tional Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea
Vedaldi. 2017. Learning multiple visual domains
with residual adapters. In Advances in Neural In-
formation Processing Systems (NeurIPS) .
Andreas Rücklé, Gregor Geigle, Max Glockner,
Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2021. AdapterDrop: On the efﬁciency
of adapters in transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 7930–7946,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and
Preslav Nakov. 2023. On the effect of dropping lay-
ers of pre-trained transformer models. Computer
Speech & Language , 77:101429.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv: 1910.01108 .Victor Sanh, Thomas Wolf, and Alexander Rush.
2020. Movement pruning: Adaptive sparsity by ﬁne-
tuning. In Advances in Neural Information Process-
ing Systems (NeurIPS) , volume 33, pages 20378–
20389. Curran Associates, Inc.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) .
Guorun Wang, Jun Yang, and Yaoru Sun. 2023. Task-
oriented memory-efﬁcient pruning-adapter. arXiv
preprint arXiv: 2303.14704 .
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-
Yu Chen, and Kailash Gopalakrishnan. 2018. Train-
ing deep neural networks with 8-bit ﬂoating point
numbers. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) , volume 31. Curran As-
sociates, Inc.
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-
anjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,
and Ming Zhou. 2021. K-Adapter: Infusing Knowl-
edge into Pre-Trained Models with Adapters. In
Findings of the Association for Computational Lin-
guistics: ACL-IJCNLP 2021 , pages 1405–1418, On-
line. Association for Computational Linguistics.
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee,
Xiaodong Liu, Jing Gao, Ahmed Hassan Awadal-
lah, and Jianfeng Gao. 2022. AdaMix: Mixture-of-
adaptations for parameter-efﬁcient model tuning. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 5744–5760, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.
Structured pruning learns compact and accurate
models. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1513–1528, Dublin,
Ireland. Association for Computational Linguistics.
Bruce Yu, Jianlong Chang, Lingbo Liu, Qi Tian, and
Chang Wen Chen. 2023. Towards a uniﬁed view
on visual parameter-efﬁcient transfer learning. In
International Conference on Learning Representa-
tions (ICLR) .
Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,
Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi
Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao
Fang, Penghui Zhu, Shu Chen, and Pengtao Xie.
2020. MedDialog: Large-scale medical dialogue
datasets. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 9241–9250.
Minjia Zhang and Yuxiong He. 2020. Accelerating
training of transformer-based language models with
progressive layer dropping. In Advances in Neural

--- PAGE 11 ---
Information Processing Systems , volume 33, pages
14011–14023. Curran Associates, Inc.
Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and
Tuo Zhao. 2023. Adaptive budget allocation for
parameter-efﬁcient ﬁne-tuning. In International
Conference on Learning Representations (ICLR) .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with BERT. In International
Conference on Learning Representations (ICLR) .
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-
rich Schütze. 2020. Masking as an efﬁcient alter-
native to ﬁnetuning for pretrained language models.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 2226–2241, Online. Association for Computa-
tional Linguistics.A Appendix
A.1 Hyperparameter Analysis
We show the experimental results of different hy-
perparameter settings of rfor LoRA in Table 6.
Model Speed Trained Params Mem Used R-1 R-2
BART-large 100 % 406.2 M 34.56 GB 43.46 25.05
+LoRA QV(r= 4) 115 % 0.6 M 28.11 GB 38.07 21.06
+LoRA QV(r= 8) 115 % 1.2 M 28.12 GB 39.64 22.23
+LoRA QV(r= 16 )115 % 2.4 M 28.15 GB 40.32 22.66
+LoRA QV(r= 32 ) 114 % 4.7 M 28.20 GB 40.88 23.10
+LoRA FF(r= 4) 127 % 1.1 M 27.19 GB 39.79 22.50
+LoRA FF(r= 8) 127 % 2.0 M 27.21 GB 40.73 23.19
+LoRA FF(r= 16 )125 % 4.0 M 27.25 GB 41.60 23.77
+LoRA FF(r= 32 ) 119 % 7.9 M 27.32 GB 42.24 24.19
Table 6: Comparison of different rank rfor LoRA in
BART-large on MIMIC-IV-discharge.
A.2 Machine-Generated Examples
The following Tables 7, 8, 9 and 10 show a
few machine-generated examples of MIMIC-IV-
discharge, MIMIC-IV-radiology, HealthCareMagic
and iCliniq, respectively. The index of a example
in the test set is indicated by the number after the
symbol #. Quantitative evaluations of the outputs
are shown at the end of each row in blue.

--- PAGE 12 ---
Source: service: medicine. chief complaint: s/p cardiac arrest [...] history of present illness: ___ y/o f with h/o severe
aortic stenosis, esrd on hd, paroxysmal atrial ﬁbrillation, and ppm for lbbb/syncope presents following acute loc while
at hd with pulselessness. patient underwent ___ minutes of cpr (no epi or deﬁbrillation per ed report) before rosc was
achieved. [...] patient was awake and talking and denied cp or sob, though she endorsed signiﬁcant ongoing back pain,
which has been a chronic issue. bp subsequently improved with initiation of levophed. cardiology was consulted due to
concern for posterior stemi given ekg ﬁndings; [...] in the ed initial vitals were: [...] moderate to marked enlargement
of the cardiac silhouette. multiple bilateral rib fractures. [...] past medical history: 1. cardiac risk factors - esrd on the
basis of hypertension, on hemodialysis with r av ﬁstula [...] -nonobstructive coronary artery disease. -hypothyroidism.
[...] echo: the left atrium is moderately dilated. there is moderate symmetric left ventricular hypertrophy. overall left
ventricular systolic function is severely depressed (lvef = 25%) secondary to akinesis of the inferior wall, hypokinesis of
the posterior wall, and pacing-induced dyssynchrony. tissue doppler imaging suggests an increased left ventricular ﬁlling
pressure (pcwp>18mmhg). doppler parameters are most consistent with grade iii/iv (severe) left ventricular diastolic
dysfunction. the right ventricular free wall thickness is normal. right ventricular chamber size is normal with borderline
normal free wall function. the aortic valve is not well seen. [...] there is moderate pulmonary artery systolic hypertension.
there is no pericardial effusion. [...] hospital course: ___ y/o f with h/o severe aortic stenosis, esrd on hd, paroxysmal atrial
ﬁbrillation, and ppm for lbbb/syncope who presents following cardiac arrest, transferred to ___ for further management of
hypotension associated with hd sessions and for post cardiac arrest care. [...] chronic issues: #copd on home o2 (2l): not
on any inhalers per patient. continue 2l o2. continued on nebulizer treatment. #chronic back pain: continued gabapentin
and methadone [...]
Reference: primary diagnosis: aortic stenosis. congestive heart failure. s/p cardiac arrest. hypotension. atrial ﬁbrillation
with rapid ventricular response. transiminitis. end stage renal disease on hemodialysis. rib fractures. seconadry diagnosis:
chronic obstructive pulmonary disease. gastroesophageal reﬂux disease. chronic back pain. hypothyroidism.
BART-large: primary diagnosis: cardiac arrest. pulmonary edema. severe aortic stenosis. end stage renal disease on
hemodialysis.
(R-1: 47.27; R-2: 30.18; BertScore: 85.08; SummaC: 41.49)
BART-large +L-Prun: primary diagnosis: cardiac arrest. severe aortic stenosis. secondary diagnoses: atrial ﬁbrillation.
end stage renal disease on hemodialysis.
(R-1: 49.12; R-2: 32.72; BertScore: 85.72; SummaC: 46.19)
BART-large +LoRA FF:primary diagnosis: cardiac arrest. severe aortic stenosis. secondary diagnoses: end stage renal
disease. paroxysmal atrial ﬁbrillation.
(R-1: 42.85; R-2: 25.92; BertScore: 83.67; SummaC: 43.17)
BART-large +LoRA FF+L-Prun: primary diagnosis: non st elevation myocardial infarction. acute on chronic
systolic heart failure. atrial ﬁbrillation with rapid ventricular response. secondary diagnosis: end stage renal disease on
hemodialysis. hypertension. hyperlipidemia.
(R-1: 52.17; R-2: 35.82; BertScore: 85.92; SummaC: 36.88)
T5-large: primary diagnosis: cardiac arrest severe aortic stenosis. secondary diagnosis: esrd on hd.
(R-1: 30.76; R-2: 12.00; BertScore: 79.84; SummaC: 55.46)
T5-large +L-Prun: primary diagnosis: s/p cardiac arrest severe aortic stenosis. secondary diagnosis: esrd on hd
paroxysmal atrial ﬁbrillation.
(R-1: 42.10; R-2: 21.81; BertScore: 81.08; SummaC: 42.15)
T5-large +LoRA FF:s/p cardiac arrest severe aortic stenosis end stage renal disease on hemodialysis paroxysmal atrial
ﬁbrillation.
(R-1: 50.00; R-2: 37.03; BertScore: 83.95; SummaC: 45.61)
T5-large +LoRA FF+L-Prun: primary diagnosis: severe aortic stenosis. secondary diagnosis: end stage renal disease
on hemodialysis paroxysmal atrial ﬁbrillation.
(R-1: 46.42; R-2: 29.62; BertScore: 83.03; SummaC: 45.67)
Table 7: Generated examples of MIMIC-IV-discharge. # 32.

--- PAGE 13 ---
Source: ﬁndings: neck, thoracic inlet, axillae, chest wall: the thyroid is notable for a large partially peripherally calciﬁed
nodule in the right lobe measuring up to 2.5 x 1.9 cm (03:23) there is no axillary lymphadenopathy. tiny supraclavicular
lymph nodes are unchanged since the prior examinations (for example 03:20). the subcutaneous tissues of the chest wall
are unremarkable. upper abdomen: please see report from dedicated ct of the abdomen and pelvis for subdiaphragmatic
ﬁndings. mediastinum: there is no mediastinal lymphadenopathy. the lower esophagus is mildly dilated. hila: there is
no hilar lymphadenopathy. heart and pericardium: the heart is normal in size. there is no pericardial effusion. pleura:
no pleural effusion or pneumothorax lung: 1. parenchyma: re-demonstrated are several millimetric pulmonary nodules,
unchanged since the most recent study, and decreased in size since the examination from ___. a right lower lobe nodule
(3:180) measures up to 4 mm. a previously seen middle lobe nodule is not demonstrated on the current study. a right lower
lobe nodule (3:207) now measures 5 x 4 mm, previously 7 x 3 mm. no new or enlarging nodules are identiﬁed. 2. airways:
the airways are patent to the subsegmental level. 3. vessels: the thoracic aorta and main pulmonary artery are normal in
caliber. chest cage: no suspicious osseous lesions or acute fractures.
Reference: impression: 1. no evidence of thoracic metastases. 2. please see report from dedicated ct of abdomen and
pelvis for subdiaphragmatic ﬁndings.
BART-large: 1. millimetric pulmonary nodules are unchanged since the most recent study, and decreased in size since
___. no new or enlarging pulmonary nodules. 2. please see report from dedicated ct of the abdomen and pelvis for
subdiaphragmatic ﬁndings.
(R-1: 57.62; R-2: 45.61; BertScore: 93.27; SummaC: 41.21)
BART-large +L-Prun: 1. millimetric pulmonary nodules are unchanged since the most recent study, and decreased in
size since ___. no new or enlarging pulmonary nodules. 2. please see report from dedicated ct of the abdomen and pelvis
for subdiaphragmatic ﬁndings.
(R-1: 57.62; R-2: 45.61; BertScore: 93.27; SummaC: 41.21)
BART-large +LoRA FF:1. interval decrease in size of several millimetric pulmonary nodules. no new or enlarging
nodules are identiﬁed. 2. please see report from dedicated ct of the abdomen and pelvis for subdiaphragmatic ﬁndings.
(R-1: 66.66; R-2: 50.00; BertScore: 92.43; SummaC: 61.83)
BART-large +LoRA FF+L-Prun: 1. interval decrease in size of several millimetric pulmonary nodules. no new or
enlarging nodules identiﬁed. 2. please see report from dedicated ct of the abdomen and pelvis for subdiaphragmatic
ﬁndings.
(R-1: 67.92; R-2: 50.98; BertScore: 93.62; SummaC: 66.12)
T5-large: 1. several millimetric pulmonary nodules are unchanged since the most recent study, and decreased in size
since the examination from ___. no new or enlarging nodules are identiﬁed. 2. please see report from dedicated ct
of the abdomen and pelvis for subdiaphragmatic ﬁndings. recommendation(s): thyroid nodule. ultrasound follow up
recommended. ___ college of radiology guidelines recommend further evaluation for incidental thyroid nodules of 1.0 cm
or larger in patients under age ___ or 1.5 cm in patients age.
(R-1: 36.00; R-2: 26.53; BertScore: 88.16; SummaC: 46.79)
T5-large +L-Prun: 1. several millimetric pulmonary nodules are unchanged since the most recent examination, and
decreased in size since the examination from ___. no new or enlarging nodules are identiﬁed. 2. please see report from
dedicated ct of the abdomen and pelvis for subdiaphragmatic ﬁndings. recommendation(s): thyroid nodule. no follow up
recommended. absent suspicious imaging features, unless there is additional clinical concern, ___ college of radiology
guidelines do not recommend further evaluation for incidental thyroid nodules less than 1.0 cm in
(R-1: 35.64; R-2: 26.26; BertScore: 88.33; SummaC: 49.39)
T5-large +LoRA FF:1. re-demonstrated are several millimetric pulmonary nodules, unchanged since the most recent
study, and decreased in size since the examination from ___. no new or enlarging nodules are identiﬁed. 2. please see
report from dedicated ct of the abdomen and pelvis for subdiaphragmatic ﬁndings.
(R-1: 51.51; R-2: 40.62; BertScore: 90.59; SummaC: 65.19)
T5-large +LoRA FF+L-Prun: 1. re-demonstrated are several millimetric pulmonary nodules, unchanged since the
most recent examination, and decreased in size since the examination from ___. no new or enlarging nodules are identiﬁed.
2. please see report from dedicated ct of the abdomen and pelvis for subdiaphragmatic ﬁndings. 3. please see report from
dedicated ct of the abdomen and pelvis for subdiaphragmatic ﬁndings.
(R-1: 44.44; R-2: 32.91; BertScore: 89.33; SummaC: 65.74)
Table 8: Generated examples of MIMIC-IV-radiology. # 31.

--- PAGE 14 ---
Source: Hello and thank you in advance. Woke up to a horrible itch on my arm - looked at it - it is swollen, red about
three inches around, very warm to the touch, and now bumpy. Looks like a spider bite... two tiny red marks in the center. I
cleaned it and sprayed bactine over area. Is there anything else I can do or be concerned about?
Reference: Hi, Welcome to Chat Doctor, It seems that you might have some poisonous insect bite giving allergic rash
with severe itching. Due to bad itching nail scratches might produce some infection on the part. Go for short course of
antibiotic to clear infection. Take anti-histamine like Cetirizine or Levo-cetrizine as and when required. Apply some local
triple acting cream. Keep that part Chat Doctor. Ok and bye.
BART-large: Hello, Welcome to Chat Doctor, The history and symptoms are suggesting me that you may be having
a fungal infection. The fungus is the normal comedians of this area which will be kept under control by the helpful
organisms whenever there is an imbalance in the environment this fungus will activate and produces its symptoms. I would
suggest you to follow. 1. Keep the area Chat Doctor. 2. Topical antifungal cream should be applied. 3. Tab Riﬂeman, 150
mg single dose will help to control the symptoms. Thank you.
(R1: 25.47; R-2: 6.45; BLEU-1: 26.00; BLEU-4: 3.09)
BART-large +L-Prun: Hello, Welcome to Chat Doctor, The history and symptoms are suggesting me that you may be
having a fungal infection of the area. The area is very moist and is prone to develop these infections. I would suggest you
to follow. 1. Keep the area clean and Chat Doctor. 2. Topical antifungal cream should be applied.3. Tab Riﬂeman, 150 mg
single dose will help to control the symptoms. Thank you.
(R1: 28.77; R-2: 7.29; BLEU-1: 31.70; BLEU-4: 3.79)
BART-large +LoRA FF:Hi, Welcome to Chat Doctor. I have gone through your query and can understand your
concerns. As per your complaint it seems that you might be having allergic reaction to spider bite. I would suggest you to
take antihistamine like cetirizine for symptomatic relief. You can also apply calamine lotion on the affected area. I hope
this information has been both informative and helpful for you. Wish you good health.
(R1: 35.97; R-2: 14.59; BLEU-1: 35.89; BLEU-4: 5.33)
BART-large +LoRA FF+L-Prun: Hello, Welcome to Chat Doctor, I have studied your case. As per your symptoms it
seems that you might be having fungal infection. I would suggest you to consult dermatologist and get examined. You
may require oral antifungal Chat Doctor. You may need oral antihistamines like cetirizine or ﬂuconazole for symptomatic
relief. I will advise checking your it B12 and it D3 level. Hope this answers your query. If you have additional questions
or follow-up queries then please do not hesitate in writing to us. Wishing you good health.
(R1: 28.93; R-2: 12.73; BLEU-1: 28.00; BLEU-4: 4.12)
T5-large: Hello, Thank you for posting on Chat Doctor. It seems you are suffering from Tina corporal, a kind of fungal
infection. I would suggest you to consult your dermatologist for proper management of the condition. I usually recommend
proper course of oral antifungal Chat Doctor. You can additionally use antifungal dusting powder containing ketoconazole
during daytime and a soap containing ketoconazole for rinsing of affected areas. Take oral antihistamines for itching as
required. Maintain hygiene over those areas and avoid wearing tight undergarments. Hope your queries are resolved.
(R1: 25.47; R-2: 6.45; BLEU-1: 29.89; BLEU-4: 0.00)
T5-large +L-Prun: Hello, Thank you for posting on Chat Doctor. It seems you are suffering from Tina courts and
corporal, a kind of fungal infection. I would suggest you to consult your dermatologist for proper management of the
condition. I usually recommend proper course of oral antifungal Chat Doctor. You can additionally use antifungal dusting
powder containing ketoconazole during daytime and a soap containing ketoconazole for rinsing of affected areas. Take
oral antihistamines for itching as required. Maintain hygiene over those areas and avoid wearing tight undergarments.
Hope your queries are resolved.
(R1: 25.47; R-2: 6.45; BLEU-1: 29.89; BLEU-4: 0.00)
T5-large +LoRA FF:Thanks for your question on Chat Doctor. I can understand your concern. In my opinion, you
should consult dermatologist and get done clinical examination of affected area. I would suggest you to take antihistamines
like ibuprofen, ibuprofen ointment.
(R1: 17.09; R-2: 1.73; BLEU-1: 23.33; BLEU-4: 0.00)
T5-large +LoRA FF+L-Prun: Thanks for your question on Chat Doctor. I can understand your situation and problem.
You may need antibiotics and antihistamines. You should consult dermatologist and get done clinical examination of
affected area. You should take treatment accordingly. Hope I have answered your question, if you have doubt then I will
be happy to answer. Thanks for using Chat Doctor. Wish you a very good health.
(R1: 20.89; R-2: 3.03; BLEU-1: 27.02; BLEU-4: 0.00)
Table 9: Generated examples of HealthCareMagic. # 46.

--- PAGE 15 ---
Source: Hello doctor,I had been diagnosed with hypothyroid nearly 10 months ago. My TSH was over 100 during that
time. Also, I had uric acid which was high. Doctor prescribed me to take Thyroxine tablet 50 mg daily and to increase it
to 100 mg after one month. Also, uric acid medicine was given. I took that for a couple of months but slowly stopped both
medicines. Instead, I am now taking 750 mg Ashwagandha and 400 mg Shilajeet daily.I got my TSH and uric acid tested
recently and TSH comes to 12.0 and uric acid is 447 (both results are still higher than normal). Can you guide me for a
suitable Ayurveda medicine to bring thyroid and uric acid to a normal level?
Reference: Hi. I have seen your attachments and read the complaints carefully. It is not a big issue and can be managed
through Ayurvedic medicines and yoga practices. Hormonal imbalance, metabolic disorders. TSH (thyroid-stimulating
hormone), uric acid, blood pressure, and blood sugar. You should take the following medicines regularly for one month
and after one month you may revert back with investigation reports. Rx, 1. Kanchanar Guggulu one tablet, Gandamala
kandan rasa one tablet, Giloy ghanvati one tablet thrice a day after meals with lukewarm water. 2. Punarnavasava 15 ml
with equal amount of water after meal twice a day. 3. Amla alovera juice 10 ml daily in the morning. Avoid spicy and
fried junk foods. ChatDoctor. Avoid milk and milk products, egg and non-vegetarian foods, beans and protien rich food.
Do regular Kapalbhati and Pranayam like anulom vilom nadi shodhana bhramari and specially ujjai. Do these things and
provide your valuable feedback after one month.
BART-large: Hello. I just read your query and from what you have mentioned it appears that you have hypothyroidism
and raised uric acid levels. I suggest that you take the following medicines. Consult your specialist doctor, discuss with
him or her and with their consent take the medicines.1. Ashwagandha 750 gm twice a day with warm milk after meals.2.
Shilajeet 400 mg two times a day.3. Avipattikar Guggulu two tablets two times in the morning and two tablets in the
evening after food. Avoid spicy and fried junk foods.
(R1: 37.79; R-2: 12.69; BLEU-1: 54.36; BLEU-4: 8.00)
BART-large +L-Prun: Hello. I just read your query and from what you have mentioned it appears that you have
hypothyroidism. I would recommend that you repeat your thyroid function test after three months and then we will review
again. I hope this helps you. Feel free to write back to me if you have more questions. Thanks and regards. For more
information consult an ayurveda specialist online
(R1: 14.91; R-2: 0.88; BLEU-1: 25.35; BLEU-4: 0.00)
BART-large +LoRA FF:Hello. I have gone through your reports. Your TSH (thyroid-stimulating hormone) level is
high and uric acid level is also high. So, you need to take some ayurvedic medicines to bring your thyroid levels back
to normal. You can take Ashwagandha and Shilajeet. It is a good medicine for hypothyroidism. But, it is not the best
medicine for your problem. For more information consult a hematologist online.
(R1: 29.53; R-2: 11.06; BLEU-1: 52.32; BLEU-4: 8.43)
BART-large +LoRA FF+L-Prun: Hello. I have gone through your query and understand your concern. Your TSH
(thyroid-stimulating hormone) level is high and you are taking Ayurveda medicines for hypothyroidism. I would like to
know a few more details about your thyroid function test. If your TSH and uric acid levels are high, then you need to get a
thyroid proﬁle done to rule out thyroid disease. I suggest you to consult a thyroid specialist for thyroid function tests and
get your thyroid proﬁle checked. For more information consult a medicine physician online
(R1: 21.25; R-2: 3.96; BLEU-1: 32.32; BLEU-4: 2.08)
T5-large: Hello. I just read your query and from what you have mentioned it appears that you have hypothyroidism and
high uric acid. I would like to know a few more details so that I can guide you better. Revert back with the answers to the
above questions to an ayurveda specialist online.
(R1: 18.60; R-2: 2.81; BLEU-1: 36.36; BLEU-4: 0.00)
T5-large +L-Prun: Hello. I have gone through your query and reports (attachment removed to protect patient identity).
Your TSH (thyroid-stimulating hormone) and uric acid levels are high. So, you need to take Ashwagandha and Shilajeet
along with Thyroxine.
(R1: 27.94; R-2: 8.81; BLEU-1: 53.94; BLEU-4: 9.58)
T5-large +LoRA FF:Hello. Your TSH (thyroid stimulating hormone) and uric acid levels are high (attachment removed
to protect patient identity). Thyroxine (Tyrosine sulfate) is an anti-thyroid ChatDoctor. It is a steroid hormone. It can be
taken for a short period of time. You can take Ashwagandha and Shilajeet (Ayurveda medicine) for two months and then
increase it to 100 mg after one month. It will help you to bring your TSH to normal levels. For more information consult a
gynaecologist online
(R1: 29.17; R-2: 11.76; BLEU-1: 43.95; BLEU-4: 6.81)
T5-large +LoRA FF+L-Prun: Hello. I have gone through your query and can understand your concern. You are
suffering from hypothyroidism. I would like to know if you have any uric acid problem. If you are having hyperthyroiditis,
then you need to take Ashwagandha and Shilajeet daily for a few months. I suggest you take a tablet of Thyroxine 50 mg
daily for one month. For more information consult a gynaecologist online.
(R1: 21.65; R-2: 4.36; BLEU-1: 41.33; BLEU-4: 0.00)
Table 10: Generated examples of iCliniq. # 184.
