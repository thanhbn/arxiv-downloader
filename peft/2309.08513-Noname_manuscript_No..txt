# 2309.08513.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.08513.pdf
# File size: 1301605 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Noname manuscript No.
(will be inserted by the editor)
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning
via Salient Channels
Henry Hengyuan Zhao ¬∑Pichao Wang ¬∑Yuyang Zhao ¬∑Hao Luo ¬∑Fan
Wang ¬∑Mike Zheng Shou
Received: date / Accepted: date
Abstract Pre-trained vision transformers have strong
representation benefits to various downstream tasks.
Recently, many parameter-efficient fine-tuning (PEFT)
methods have been proposed, and their experiments
demonstrate that tuning only 1% extra parameters
could surpass full fine-tuning in low-data resource sce-
narios. However, these methods overlook the task-
specific information when fine-tuning diverse down-
stream tasks. In this paper, we propose a simple yet ef-
fective method called ‚ÄúSalient Channel Tuning‚Äù (SCT)
to leverage the task-specific information by forward-
ing the model with the task images to select partial
channels in a feature map that enables us to tune
only 1/8 channels leading to significantly lower param-
eter costs. Experiments on 19 visual transfer learning
downstream tasks demonstrate that our SCT outper-
forms full fine-tuning on 18 out of 19 tasks by adding
only 0.11M parameters of the ViT-B, which is 780 √ó
Henry Hengyuan Zhao
Show Lab, National University of Singapore, Singapore
E-mail: hengyuan.z@u.nus.edu
Pichao Wang
Alibaba Group, USA
E-mail: pichaowang@gmail.com
Yuyang Zhao
National University of Singapore, Singapore
E-mail: yuyang.zhao@u.nus.edu
Hao Luo
Alibaba Group, China
E-mail: michuan.lh@alibaba-inc.com
Fan Wang
Alibaba Group, USA
E-mail: fan.w@alibaba-inc.com
Mike Zheng Shou (Corresponding author)
Show Lab, National University of Singapore, Singapore
E-mail: mike.zheng.shou@gmail.com
SCT Adapter SSF LoRA NOAH VPT0.10.20.30.40.50.6# Params(M)
0.110.160.240.290.430.53
SCT Adapter SSF LoRA NOAH VPT697071727374Top-1 Accuracy73.6
71.372.6
72.373.3
69.4Fig. 1: The comparison of parameters and top-1 accu-
racy on VTAB-1K benchmark with different baselines.
We only tune 96 channels in 768 channels of ViT-B/16,
obtaining the best results compared with other meth-
ods.
fewer than its full fine-tuning counterpart. Further-
more, experiments on domain generalization and few-
shot classification further demonstrate the effectiveness
and generic of our approach. The code is available at
https://github.com/showlab/SCT
1 Introduction
Large vision transformers (ViT) have achieved remark-
able success in computer vision tasks (Dosovitskiy
et al., 2020; Liu et al., 2021; Yuan et al., 2022; Zhou
et al., 2021a; Carion et al., 2020; Li et al., 2021; Strudel
et al., 2021) by using large-scale training data, such as
ImageNet21K and JFT-300M, to create strong repre-
sentations. However, downstream recognition tasks of-
ten lack sufficient data to train from scratch. There-
fore, transferring knowledge from pre-trained ViT mod-
els can significantly reduce training difficulty and pro-
duce promising results. End-to-end full fine-tuning is
a widely used way to inherit these robust representa-
tions, but it faces two challenges. First, large models arearXiv:2309.08513v5  [cs.CV]  29 Apr 2024

--- PAGE 2 ---
2 Henry Hengyuan Zhao et al.
Nonlinearity
DownsampleUpsample
1 √óùê∑‚Ä≤
Input¬†feature:¬†1 √óùê∑Output¬†feature:¬†1 √óùê∑
Channel¬†
SelectionLinear
1 √óùêæ
Input¬†feature:¬†1 √óùê∑Output¬†feature:¬†1 √óùê∑
(a)¬†Adapter (b)¬†Ours
Fig. 2: The architecture comparison between the
Adapter and our SCT. ‚ÄúDownsample‚Äù and ‚ÄúUpsam-
ple‚Äù represent the channel downsampling and upsam-
pling operations. Drepresents the number of channel
dimensions.
prone to overfitting when tuning their massive weights
on small downstream training data. Second, ViT mod-
els are too large to store all weights for each downstream
task, making it infeasible to deploy fine-tuned models
on resource-limited devices.
To mitigate the above two challenges, some works
propose to tune a subset of parameters ( ?) or adopt
an external trainable module (Houlsby et al., 2019; Hu
et al., 2021) to preserve the knowledge learned from
pre-trained models. As for tuning a subset of param-
eters, there are two representative approaches: tuning
the classification head (Mahajan et al., 2018; Jia et al.,
2021; Chen et al., 2021b) and tuning bias term (Cai
et al., 2020). Tuning the classification head involves
freezing the weights of the backbone network and up-
dating only the linear head, whereas tuning the bias
term entails unfreezing the bias term within the back-
bone network. Both approaches result in inferior per-
formance.
Recently, some studies proposed to address a new
task named Parameter-Efficient Fine-Tuning (PEFT)
by leveraging an external trainable module for model
adaptation. One type of method includes integrat-
ing learnable prompts into the model input for each
downstream task, exemplified by the pioneering work,
VPT(Jia et al., 2022). However, VPT encounters sev-
eral challenges. First, its task-specific prompts are de-
rived through supervised learning, which differs from
the user-provided text prompts commonly used in the
NLP field. Second, VPT requires searching for the best
prompt length for each task which lacks flexibility and is
impractical when encountering new downstream tasks.Another type of work is to add an adapter (Chen et al.,
2022b; Houlsby et al., 2019; Jie and Deng, 2022) along-
side the multi-head self-attention (MHSA) or MLP
block and address the whole features equally without
considering the task-specific information into the effi-
cient module design. Moreover, the number of trainable
parameters is not quite small.
By investigating these approaches, utilizing the
task-specific information in the efficient module design
is still underexplored as it is highlighted in a recent
study (Luo et al., 2022) that channel bias exists in di-
verse downstream tasks. In this paper, to address the
PEFT problem, we propose a simple baseline to involve
task-specific information by forwarding the pre-trained
model with downstream images. Furthermore, we pro-
ceed to select a small portion of channels to achieve
efficient fine-tuning, significantly reducing the param-
eter costs. Our experiments in Sec.4.2 reconfirm that
tuning only a small portion of task-specific channels is
sufficient for downstream task adaptation in the low-
data regime. To achieve the goal of obtaining these
task-specific channels, we propose a Class-Aware Im-
portance Score (CAIS) by adopting L2norm as the cri-
teria to evaluate the channel importance inspired by
the classic pruning method (Han et al., 2015). We re-
fer to the selected channels as ‚Äúsalient channels‚Äù (SC)
since they have higher activation values and are crucial
for task performance. It is noted that channel selec-
tion can avoid the training costs by selecting the best
structure as in NOAH(Zhang et al., 2022) or the best
prompt length as in VPT(Jia et al., 2022). Moreover,
compared with adapter-based methods (Houlsby et al.,
2019; Chen et al., 2022b), selecting partial task-specific
channels enables us to achieve lower parameter costs by
removing the operations of ‚Äúdownsample‚Äù and ‚Äúnonlin-
earity‚Äù as shown in Fig. 2.
In summary, the contributions are summarized as
follows:
Contributions
‚ÄìWe propose a simple baseline with a new perspec-
tive in partial channel tuning ( i.e., salient channel
tuning) for addressing the PEFT task to leverage
task-specific information, achieving promising per-
formance on 19 visual transfer learning tasks, 4 do-
main generalization tasks, and 5 few-shot learning
tasks. The experiments demonstrate that tuning a
subset of channels in a feature map is efficient and
generic.
‚ÄìTo the best of our knowledge, it is the first time to
reduce the parameter costs to 0.11M (780 √ófewer
parameters than the full model size) level of ViT-B
while achieving the best average performance com-
pared with other methods.

--- PAGE 3 ---
Salient Channel Tuning 3
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-1
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-2
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-3
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-4
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-5
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-6
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-7
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-8
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-9
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-10
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-11
0 100 200 300 400 500 600 700
Channel Indices0
100Class IndicesLayer-12
(a) Visualizations of the extracted feature maps on the Caltech101 dataset at each transformer layer. Y-axis represents the
class indices, and X-axis represents channel indices, i.e., 768 in total. We categorize the total image features with the class
label and then calculate each channel‚Äôs L2values. Thus, we obtain the 768 dimension vector for each class and find that some
channels have higher activation values than others in all classes, as the vertical lines are shown in this figure.
22 35 38 55 67 79
90 91 97 122 125 148
150 158 166 180 187 188
194 196 199 206 216 218
222 225 238 243 249 256
260 279 291 302 308 309Layer-1
0.930.940.950.960.970.980.991.00
12 23 35 67 71 74
82 90 91 97 125 144
148 153 180 187 188 193
196 206 247 259 260 277
279 281 288 301 308 321
327 340 353 364 369 373Layer-2
0.880.900.920.940.960.981.00
1 12 35 51 52 67
71 82 91 97 101 125
148 180 187 193 206 226
245 247 259 260 272 277
281 301 308 316 321 327
340 353 354 357 364 370Layer-3
0.900.920.940.960.981.00
1 9 35 51 52 63
67 82 91 97 125 126
148 175 176 177 180 187
206 223 226 245 247 267
272 273 281 301 302 309
316 320 321 329 353 357Layer-4
0.880.900.920.940.960.981.00
1 8 9 35 51 59
67 82 86 91 97 123
125 148 177 180 187 202
206 223 226 245 247 248
254 265 272 281 301 309
313 316 325 329 330 353Layer-5
0.880.900.920.940.960.981.00
1 9 12 28 35 36
51 59 67 82 86 91
97 108 112 128 143 177
180 183 184 187 191 202
206 223 226 232 247 265
272 277 281 301 313 316Layer-6
0.750.800.850.900.951.00
1 10 14 35 36 51
59 64 67 72 86 97
112 128 132 140 143 166
177 183 184 187 191 202
206 216 223 226 238 244
265 272 277 281 282 295Layer-7
0.60.70.80.91.0
1 10 14 35 36 51
59 64 67 72 79 86
89 97 112 128 140 143
166 177 183 184 187 191
202 206 216 223 226 229
238 243 244 245 272 277Layer-8
0.60.70.80.91.0
10 14 35 36 51 64
67 72 86 89 97 111
112 114 128 140 143 166
177 183 184 187 191 202
206 216 223 226 238 243
244 245 272 277 281 282Layer-9
0.60.70.80.91.0
10 14 36 51 64 67
72 79 86 89 97 112
114 128 140 143 166 177
183 184 187 191 202 216
223 226 234 243 244 245
263 269 272 277 281 282Layer-10
0.60.70.80.91.0
10 14 18 51 64 86
89 97 112 128 143 177
183 184 187 191 216 222
223 234 243 244 245 259
263 269 276 277 280 281
282 288 295 316 317 323Layer-11
0.60.70.80.91.0
18 60 64 67 69 86
90 97 101 105 112 115
133 143 144 160 163 177
183 184 187 206 210 222
223 234 240 243 263 269
278 280 281 282 288 295Layer-12
0.650.700.750.800.850.900.951.00
(b) The top-36 selected salient channel indices on the Caltech101 dataset at each transformer layer i.e., Layer-1, Layer-2. Each
layer selects different salient channels. As the layer goes deeper, the salient channels appear more concentrated. Darker color
represents a larger activation value.
18 60 64 67 69 86
90 97 101 105 112 115
133 143 144 160 163 177
183 184 187 206 210 222
223 234 240 243 263 269
278 280 281 282 288 295Caltech101
0.650.700.750.800.850.900.951.00
8 40 46 64 67 69
85 86 97 109 112 129
179 183 187 203 214 216
222 232 234 235 240 243
247 266 273 281 283 284
291 292 297 316 327 340DMLab
0.700.750.800.850.900.951.00
22 25 49 62 64 66
69 86 89 97 104 105
112 130 140 145 158 159
177 183 184 185 187 204
211 228 234 235 266 276
281 282 290 295 305 309Resisc45
0.650.700.750.800.850.900.951.00
12 14 19 33 45 51
62 64 66 67 84 86
89 97 112 143 159 173
183 184 187 194 202 211
234 245 259 266 276 278
281 289 295 316 317 323EuroSAT
0.650.700.750.800.850.900.951.00
(c) We select the salient channels from the same layer on four downstream tasks. The results suggest that channel bias exists
in various tasks.
Fig. 3: The visualizations of feature maps and the selected salient channel indices. All the results are obtained by
ViT-B/16 pre-trained on ImageNet21K.
2 Related Work
2.1 Vision Transformers
Transformer (Vaswani et al., 2017) has demonstrated
outstanding results on natural language processingand computer vision tasks. Lots of vision transformers
(Chen et al., 2021a; d‚ÄôAscoli et al., 2021; Dong et al.,
2022; Ali et al., 2021; Fan et al., 2021; Han et al., 2021;
Rao et al., 2021; Yuan et al., 2021; Touvron et al., 2021;
Liu et al., 2021; Wang et al., 2021; Zhou et al., 2021a)
are proposed after the pioneering work ViT (Dosovit-

--- PAGE 4 ---
4 Henry Hengyuan Zhao et al.
skiy et al., 2020). Many of them increase the model
size gradually for state-of-the-art results and learn the
rich representations by various architectural designs. It
is noted that most of them are trained on the natural
dataset and have the strong potential to be transferred
to other domains/tasks. Moreover, adopting these mod-
els to the downstream tasks is able to alleviate the train-
ing difficulty and achieve promising results.
2.2 Parameter-Efficient Fine-Tuning Methods
PEFT focuses on adopting a trainable module with
a few parameters for fine-tuning. Two lines of PEFT
have been proposed recently. On the one hand, applying
prompts (Jia et al., 2022; Liu et al., 2022; Xing et al.,
2022; Zheng et al., 2022; Nie et al., 2022; Wang et al.,
2022; Zhou et al., 2022a,b; Liao et al., 2023; Manli et al.,
2022; Zhang et al., 2023b; Zang et al., 2022; Bar et al.,
2022) to the backbone networks shows success on sev-
eral vision tasks. VPT (Jia et al., 2022) first proposes
the prompt-based method in computer vision fields by
injecting the prompts into each transformer layer. How-
ever, one main limitation of VPT is that it relies on
hand-crafted selection to determine the optimal prompt
length for each task. This is inflexible when applying
to a new downstream task. Another limitation is that
the task-specific prompts of VPT are obtained through
the training procedure and cannot be given by the user
as in the NLP field. In our work, to explicitly reduce
the computations of searching the task-specific prompt
length, we pass the training images into the backbone
network and leverage the task-specific information by
determining the salient channels. This procedure only
needs one forward propagation without training costs.
On the other hand, adding a residual module
(Houlsby et al., 2019; Chen et al., 2022b; Jie and Deng,
2022; Chen et al., 2022a) in the backbone networks
also acquires promising results for performance and ef-
ficiency. Adapter (Houlsby et al., 2019), a widely used
baseline in many tasks (Sung et al., 2022; Pan et al.,
2022; Zhang et al., 2023a), proposes an MLP-like mod-
ule, a successful design that first projects the original
dimensional features into a smaller dimension with one
nonlinear layer and projects it back to the original di-
mensions. It vastly reduces the number of parameters.
Inspired by its smallest intermediate dimensions, find-
ing a small number of salient channels in a feature
map might be enough for the adaptation. Unlike in-
jecting trainable modules into the transformer blocks,
LoRA (Hu et al., 2021) optimizes a low-rank decompo-
sition matrix with a low intrinsic dimension to project
the query, key features. As for NOAH (Zhang et al.,2022), a neural architecture search algorithm incorpo-
rates Adapter, LoRA, and VPT into its network search
space. NOAH provides a strong baseline for performing
consistently well on different datasets. SSF (Lian et al.,
2022), a recently proposed strong baseline by only scal-
ing and shifting the features to implement the efficient
model tuning. Child-Tuning Xu et al. (2021) updates a
subset of parameters of large pre-trained language mod-
els via strategically masking out the gradients of the
non-child network during the backward process. Adapt-
Former Chen et al. (2022b) is an adapter-like method.
It explores the efficiency of the adapter-like architecture
of vision transformers‚Äô fine-tuning.
Unlike the above methods, our SCT handles the
PEFT by leveraging the task-specific information into
the method design and then proposes the Salient Chan-
nel Tuning, achieving lower parameter costs and higher
performance than previous methods.
3 Method
3.1 Not all channels are equal
To leverage task-specific information, this paper aims
to find task-specific channels for fine-tuning. Here we
provide a simple observation highlighting the ‚ÄúSalient
Channels‚Äù that exist in the downstream tasks.
Previous works (Luo et al., 2017; Li et al., 2016;
He et al., 2018; Liu et al., 2018; Han et al., 2015; Li
et al., 2017) demonstrate that pruning some channels
of deep neural networks has a marginal influence on the
model performance but can significantly reduce the pa-
rameter number and computational cost. Such results
reflect that the importance of different channels is not
the same, i.e., ‚ÄúNot all channels are equal‚Äù. Intuitively,
the channel importance is different in terms of the tasks,
which motivates us to investigate the impact of channel
selection in model tuning.
To find task-specific channels, an intuitive way is to
find some mutual channels across different categories.
Thus, We first illustrate an observation between the
pre-trained model and the downstream task. We choose
Caltech101 (Fei-Fei et al., 2004) (one of the downstream
tasks from the VTAB-1K benchmark) as an example to
evaluate the intermediate features of each transformer
layer of ViT. ViT-B pre-trained on ImageNet21K is the
backbone, and we feed the whole dataset to extract
the features between multi-head self-attention (‚ÄúAttn‚Äù)
and ‚ÄúMLP‚Äù blocks in all 12 transformer layers. Fig. 3
(a) shows some vertical lines in each subfigure of trans-
former layers. It indicates that when using a pre-trained
model to extract the features on the target dataset,

--- PAGE 5 ---
Salient Channel Tuning 5
Importance 
scoreLinear
BNDTop K
BN
Input featuresTransformed featuresSalient featuresScale Replace
Input features Output featuresIntermediate featuresSCTMDuring Tuning
Fig. 4: The overview of proposed salient channel tuning module.
some channels have higher activation values than oth-
ers, regardless of categories. We so-call these channels
‚Äúsalient channels‚Äù. To investigate the position of salient
channels, we also report the index of selected top-36
salient channels after calculating the importance score
of each layer in Fig. 3 (b). The number represents the
channel index of the total of 768 channels. The darker
color represents the higher activation values. We could
find different layers have different salient channel sets.
As the layer goes deeper, the salient channels appear
more concentrated. We hypothesize that deeper layers
contain more abstract information and information ag-
gregated in a few channels. In contrast, the shallow lay-
ers extract more low-level representation, making the
information dense.
It naturally raises a question: Could we find
salient channels in a feature map and then only
tune these salient channels for efficient tuning?
To answer this question, we design a simple class-aware
importance score to identify these salient channels and
leverage a salient channel tuning module for efficient
fine-tuning.
3.2 Class-Aware Importance Score
The selection of salient channels is crucial for the adap-
tation to downstream tasks, which should contain mu-
tual information in all classes. Intuitively, we can di-
rectly use the mean feature of all the data samples to
represent the downstream task distribution to deter-
mine the salient channels. However, considering down-
stream datasets may not always be balanced, treating
the whole dataset as a unity may be biased to the head
classes, leading to the selected channels not represent-
ing the mutual information over all the classes. Conse-
quently, we propose the Class-Aware Importance Score
(CAIS), where the importance score calculation is con-
ducted at each class and then averaged across all the
classes. Assume the intermediate feature maps as {fl
m‚àà
RBm√óN√óD|l, m‚ààN,1‚â§l‚â§L and 1‚â§m‚â§M},where the NandDrepresent the number of tokens
and amount of channel dimension, respectively. Lrep-
resents the total layers of the ViT backbone. The Bmis
the volume of each class, and Mrepresents the number
of categories of the downstream recognition task. We
first apply the L2norm regularization of each channel
dimension of feature fl
m:
Àúfl
m={‚à•fl
m,1‚à•2,‚à•fl
m,i‚à•2, ...,‚à•fl
m,D‚à•2},
‚à•fl
m,i‚à•2‚ààR, i‚ààN,1‚â§i‚â§D,(1)
where Àúfl
m‚ààR1√óDis the importance score vector at l-th
layer and m-th class. To investigate the salient channels,
we average the Àúfl
m‚ààR1√óDover all classes to get the
importance score at l-th layer:
Zl=1
MMX
m=1Àúfl
m,Àúfl
m‚ààR1√óD, (2)
After getting the importance score vector {Zl‚àà
R1√óD|l‚ààN,1‚â§l‚â§L}, we can choose the largest
Kvalues of Zland then we can derive selected indices
asIl=topK (Zl), Il‚ààN1√óKatl-th layer. Later, the
salient channels at each layer are different across dif-
ferent downstream tasks, and it could explicitly involve
task-specific information in the model fine-tuning. The
procedures for selecting salient channels are shown in
Algorithm 1.
3.3 Adapting ViT via Salient Channel Tuning
Given the importance score of selecting top- Ksalient
channels, we propose a Salient Channel Tuning Mod-
ule (SCTM) in this paper. The overview of the SCTM
is depicted in Fig. 4. Unlike other PEFT methods, our
SCTM only contains a linear layer rather than an MLP-
like adapter (Houlsby et al., 2019) involving two learn-
able layers and one activation layer illustrated in Fig.2.

--- PAGE 6 ---
6 Henry Hengyuan Zhao et al.
Such a simple structure is easy to reproduce and main-
tains relatively low parameter costs. To inherit the orig-
inal robust representations, we utilize a residual short-
cut to fuse the salient features with intermediate fea-
tures and use the Scale ‚ààR(as shown in Fig. 4) to
adjust the weights between both features, which is a
constant parameter. After obtaining the transformed
features, we insert the transformed features back into
the input features at the same position. The unselected
channels we keep frozen during fine-tuning. In Fig. 6, we
present two forms of inserting the SCTM into the ViT.
The ‚ÄúOurs-MLP‚Äù represents that we insert the SCTM
after the MLP block, and the ‚ÄúOur-Attn‚Äù indicates that
we put the SCTM after the MHSA block while before
the MLP block. In the following experiments, ‚ÄúOurs-
Attn‚Äù is the default injection position compared with
other baselines.
Overall. Our SCT first feeds the training set to a back-
bone network to extract the intermediate features in
different layers. Then, using the proposed CAIS to de-
termine the salient channels and save the channel in-
dices. During fine-tuning, we add our SCTM in each
transformer layer and tune the selected channels with
a linear layer while freezing other unselected channels.
Discussion. Firstly, different downstream tasks have
their peculiarities, i.e., ‚ÄúNot all channels are equal‚Äù.
Previous PEFT methods ( i.e., Adapter) lack the con-
sideration of explicitly using task-specific information.
Instead, we propose the SCT explicitly leverage the
task-specific information for channel selection, which
could enable us to save the computations and extra pa-
rameters of the ‚ÄúDownsample‚Äù and ‚ÄúNonlinearity‚Äù op-
erations compared with Adapter (Houlsby et al., 2019).
Secondly, calculating the salient channels offline could
save the computations of searching the best prompt
length (Jia et al., 2022) or the best network struc-
ture (Zhang et al., 2022). Thirdly, our results in Fig. 1
demonstrate that only adapting 12.5% channels could
obtain competitive results compared to other baselines.
Simultaneously, transforming a small portion of the fea-
tures could significantly reduce the number of learnable
parameters (e.g., 12 √ó96√ó96 is relatively more mi-
nor than the 12 √ó768√ó768). Fourthly, such a small
amount of parameters largely reduces the difficulty of
storing task-specific knowledge when we have many
downstream tasks. To evaluate this simple baseline, we
also test its few-shot capacity and domain generaliza-
tion ability to meet the requirements of real-world ap-
plications.Algorithm 1 The procedures of channel selection via
class-aware importance score.
Inputs:
The pre-trained ViT model Fand the number of layers L;
The number of selected channels Kand the total number of
classes M;
1:foreachl‚àà[1, L]do
2: Extract the feature fl
mat each class m;
3: Apply the L2norm regularization on each channel to
obtain Àúfl
mvia Eq. 1;
4: Calculate the importance score Zlofl-th layer via
Eq. 2;
5: Select top- Klargest channels Il= topK( Zl);
6:end for
Outputs: Selected channel indices of each transformer layer.
3.4 Channel Erasing Test
To assess the channel selection configuration, we give
two toy tests to evaluate the effectiveness of salient
channels as in Figure 5. The first test in Figure 5 (a)
demonstrates the effectiveness of salient channels com-
pared with random channel selection. The second test
in Figure 5 (b) evaluates the importance of each layer.
We erase the salient channels layer by layer. The curves
suggest that for Natural type task Caltech101, the first
four layers are more important than the last four layers
as the accuracy drops significantly in the erasing first
four layers. A similar trend is evident in the Resisc45
task. As for the KITTI task, the last four layers are as
important as the first four.
Based on the aforementioned tests, we manipulate
the number of fine-tuned channels within the first four
layers and the last eight layers, as outlined in Table 1.
Increasing the number of fine-tuned channels within the
first four layers (first and third rows) yielded enhance-
ments of 0.1% for Caltech101 and 0.5% for KITTI, al-
beit with a 0.1% decline in Resisc45 performance. No-
tably, the influence of the last four layers on the final
results is slight. Consequently, we explored the effect
of decreasing the number of tuned channels within the
last eight layers. The results (fourth and fifth rows) de-
tailed in Table 1 indicate that reducing the Kfrom 96
to 32 leads to a significant deterioration in performance.
However, a slightly higher value compared with setting
K= 32 across all 12 layers.
4 Experiments
This section compares our SCT with other state-of-the-
art PEFT baselines on the VTAB-1K benchmark, using
ViT and Swin Transformer backbones. In addition, we
analyze the channel selection strategy, class-aware im-
portance score, insert position, insert length, and the

--- PAGE 7 ---
Salient Channel Tuning 7
1 2 3 4 5 6 7 8 910
Random seeds102030405060708090Top-1 Accuracy
Caltech101
1 2 3 4 5 6 7 8 910
Random seeds10203040506070Top-1 Accuracy
Resisc45
1 2 3 4 5 6 7 8 910
Random seeds3035404550556065Top-1 Accuracy
KITTI
No Erasing
Erasing Random channels
Erasing Salient channels
(a) Evaluate the importance of salient channels on three datasets. We erase all layers with randomly selected channel indices
and salient channel indices. To avoid the noise, we repeat the random selection process 10 times. The results demonstrate the
effectiveness of our salient channel selection.
123456789101112
Erasing channel index606570758085Top-1 Accuracy
Caltech101
123456789101112
Erasing channel index40455055606570Top-1 Accuracy
Resisc45
123456789101112
Erasing layer index45.047.550.052.555.057.560.062.565.0Top-1 Accuracy
KITTI
No Erasing
Erasing single layer
(b) We systematically remove one layer at a time, identified by its index. The resulting curve highlights the relative significance
of the first four layers compared to the last four in the context of the Caltech101 and Resisc45 tasks. When considering the
KITTI task, all layers are important for the final output.
Fig. 5: Channel erasing test.
Channel configurations Caltech101 Resisc45 KITTI Params
[96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96] 91.6 86.3 80.2 0.11M
[192, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192] 91.5 87.0 81.0 0.44M
[192, 192, 192, 192, 96, 96, 96, 96, 96, 96, 96, 96] 91.7 86.2 80.7 0.22M
[96, 96, 96, 96, 32, 32, 32, 32, 32, 32, 32, 32] 91.1 85.0 76.2 0.045M
[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32] 90.3 84.8 74.0 0.01M
Table 1: Different number of fine-tuned channels on three tasks.
number of selected channels to verify SCT‚Äôs effective-
ness further. Beyond evaluating performance on visual
transfer learning tasks, we also validate the capabili-
ties of four domain generalization datasets and test the
performance of SCT in the low-data scenarios on five
datasets in the few-shot setting.
4.1 Training details
Following VPT (Jia et al., 2022), we utilize grid search
to find the tuning-specific hyper-parameters, including
learning rate and weight decay, based on the val set of
each task as shown in Tab.2. For the hyper-parameter
Scale , which is a manually defined constant to balance
the weight of transformed features and salient features,
we search it at the range {0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
0.7, 0.8, 0.9, 1.0 }. During training, we use a standardimage augmentation strategy: normalize with ImageNet
means and standard deviation, randomly resize crop to
224√ó224, and random horizontal flip as in VPT.
4.2 Experiments on VTAB-1K Benchmark
Dataset. VTAB-1K (Zhai et al., 2019) contains 19 vi-
sual classification tasks which cover a broad spectrum
of domains and semantics in three groups, i.e.,Natu-
ral,Specialized , and Structured . The Natural group con-
tains 7 classic classification datasets of natural images.
TheSpecialized group involves 4 datasets of two special
scenarios: medical and remote-sensing. The Structured
group has 8 datasets, mainly focusing on understand-
ing the structure of a scene, such as object counting and
depth prediction. Each task of VTAB-1K contains 1000
training images. More details are available in Tab.3.

--- PAGE 8 ---
8 Henry Hengyuan Zhao et al.
VTAB-1K, Domain Generalization, and Few-shot Learning
Optimizer AdamW (Loshchilov and Hutter, 2017)
baselrrange {0.1, 0.5, 0.01, 0.05 0.001, 0.005, 0.0001, 0.0005 }
Weight decay range {0.1 0.01, 0.05, 0.001, 0.005, 0.0005, 0.0001 }
Learning rate schedule cosine decay
Warm up epochs 10
Total epochs 100
Batch size 64
Table 2: Training details on each visual task with ViT-B/16.
Dataset # Classes Train Val Test
VTAB-1K (Zhai et al., 2019)
NaturalCIFAR100 (Krizhevsky et al., 2009) 100
800/1000 20010,000
Caltech101 (Fei-Fei et al., 2004) 102 6,048
DTD (Cimpoi et al., 2014) 47 1,880
Oxford-Flowers102 (Nilsback and Zisserman, 2006) 192 6,149
Oxford-PetS (Parkhi et al., 2012) 37 3,669
SVHN (Netzer et al., 2011) 10 26,032
Sun397 (Xiao et al., 2010) 397 21,750
SpecializedPatch Camelyon (Veeling et al., 2018) 2
800/1000 20032,768
EuroSAT (Helber et al., 2019) 10 5,400
Resisc45 (Cheng et al., 2017) 45 1,880
Retinopathy (Kaggle and EyePacs, 2015) 5 42,670
StructuredClevr/count (Johnson et al., 2017) 8
800/1000 20015,000
Clevr/distance (Johnson et al., 2017) 6 15,000
DMLab (Beattie et al., 2016) 6 22,735
KITTI-Dist (Geiger et al., 2013) 4 711
dSprites/location (Matthey et al., 2017) 16 73,728
dSprites/orientation (Matthey et al., 2017) 16 73,728
SmallNORB/azimuth (LeCun et al., 2004) 18 12,150
SmallNORB/elevation (LeCun et al., 2004) 18 12,150
Domain generalization
ImageNet-1K (Deng et al., 2009) 1,000 16 per class 50,000 N/A
ImageNet-V2 (Recht et al., 2019) 1,000 N/A N/A 10,000
ImageNet-Sketch (Wang et al., 2019) 1,000 N/A N/A 50,889
ImageNet-A (Hendrycks et al., 2021b) 200 N/A N/A 7,500
ImageNet-R (Hendrycks et al., 2021a) 200 N/A N/A 30,000
Few-shot Learning
Food101 (Bossard et al., 2014) 101
1/2/4/8/16 per class20,200 30,300
Stanford Cars (Krause et al., 2013) 196 1,635 8,041
Oxford-Flowers102 (Nilsback and Zisserman, 2006) 102 1,633 2,463
FGVC-Aircraft (Maji et al., 2013) 100 3,333 3,333
Oxford-Pets (Parkhi et al., 2012) 37 736 3,669
Table 3: Specifications of used datasets on VTAB-1K benchmark, Domain Generalization, and Few-shot Learning.
Following Zhang et al. (2022); Jia et al. (2022), we use
the 800-200 train-val split to determine the hyperpa-
rameters and the entire 1000 training data to train the
final model. We report the average top-1 accuracy on
thetest set.
Baselines. We compare our method with three
baselines Full fine-tuning ,Linear , and Bias
without external parameters and four baselines
Adapter (Houlsby et al., 2019), LoRA (Hu et al.,
2021), VPT (Jia et al., 2022), and NOAH (Zhang
et al., 2022) with external parameters. Bias method
only updates all the bias terms in the pre-trained back-bone. Adapter injects an additional MLP module into
each transformer layer. LoRA adopts an optimized
low-rank matrix to the MHSA module in the trans-
former layers. VPT is a visual prompt algorithm to in-
corporate the prompts with tokens into the backbone.
NOAH is a neural architecture search algorithm in-
corporating the Adapter, LoRA, and VPT into the net-
work search. SSF propose two learnable factors to scale
and shift the features when fine-tuning the downstream
tasks. AdaptFormer proposes an adapter-like archi-
tecture for vision tasks. We directly use their released

--- PAGE 9 ---
Salient Channel Tuning 9
LayerNormMHSALayerNormMLPSCTM
(a) Ours-MLPLayerNormMHSALayerNormMLP
(b) Ours-AttnSCTM
Fig. 6: Two types of structures when inserting SCTM
into the backbone. Only SCTM is trainable, and other
modules are frozen.
results or run their code to generate them to provide a
fair comparison.
Performance with ViT backbone. We compare our
SCT with the above 7 baselines in Tab. 4 and Fig. 7.
We use ViT-B/16 as the backbone and insert SCTM in
each transformer layer. The default Kis set to 96, 1/8
of the total channels, leading to the trainable parame-
ter number being only 0.11M. First , our SCT achieves
the average accuracy of 73.6% on the 19 datasets, out-
performing the full fine-tuning (85.8M) on 18 out of 19
datasets and gains the improvement of 6.3%, 2.5%, and
12.3% in the three groups, respectively, with only addi-
tional 0.13% of the backbone parameters. Such results
reflect that SCT can greatly reduce the storage space
and alleviate the overfitting problem common in fine-
tuning large models. Second , compared with Adapter
(0.16M) (Houlsby et al., 2019) that treats all the chan-
nels equally, selecting a part of salient channels for each
downstream task is more effective and efficient, outper-
forming it by 2.3% in average accuracy. Third , com-
pared with other PEFT methods, our SCT surpasses
NOAH (0.43M) (Zhang et al., 2022) by 0.3% with only
a quarter of its trainable parameters. When increas-
ing the Kto 192 with 0.44M extra parameters, our
SCT surpasses NOAH (0.43) by 0.7 % average accu-
racy. Moreover, our SCT outperforms VPT (Jia et al.,
2022) by 3.8%, 3.5%, and 4.9% in the three groups,
respectively. These results demonstrate that instead
of specially designed structures for each downstreamdataset, explicitly leveraging the task-specific informa-
tion can reduce the computational cost in the model
search and improve the performance. Moreover, com-
pared with SSF (0.20M) (Lian et al., 2022) baseline,
a method without an additional trainable module, our
SCT only needs almost its half parameters and achieves
an average improvement of 1.0% across 19 downstream
tasks.
Performance with Swin Transformer Backbone.
To verify the effectiveness of SCT with different back-
bones, we apply SCT on hierarchical transformers, i.e.,
Swin-B. We use the same setting of inserting SCTM
as in the ViT backbone: inserting the SCTM after the
‚ÄúAttn‚Äù block in the transformer layer. Considering deep
layers contain more semantic information in the hierar-
chical structure, instead of applying SCTM on all the
transformer layers, we insert it to the last half of the
layers in the stage 3 and all layers of stage 4 of the Swin-
B to keep a similar level of trainable parameters. The
results of Tab. 5 show that SCT outperforms full fine-
tuning in all 19 downstream tasks with only 0.12%
parameters while other methods cannot. In addition,
compared with the PEFT method, SCT outperforms
VPT (Jia et al., 2022) by 5.9%, 3.0%, and 7.2% in the
three groups, respectively. All the results above suggest
that our SCT also applies to hierarchical transform-
ers and can yield much more improvement than other
PEFT methods.
4.3 Evaluation
Effectiveness of Salient Channel Tuning. We com-
pare three channel selection strategies to verify the ef-
fectiveness of selecting salient channels in Tab. 6. The
selection strategies include Salient Channel Selection
(SC), Inconspicuous Channel Selection (IC), and Ran-
dom Channel Selection (RC). IC selects channels with
the lowest Kvalues of L2norm, and RC denotes select-
ingKchannels randomly. We randomly select three sets
of channels (RC-1/2/3) for random channel selection to
alleviate the outliers. As shown in Tab. 6, SC achieves
the best results and outperforms IC by 2.4% in the av-
erage accuracy. Random channel selection could obtain
better results than full fine-tuning, but all of them per-
form worse than SC. Interestingly, even IC can perform
better than full fine-tuning, demonstrating that select-
ing a small subset of the channels can prevent the large
model from overfitting to the small training set.
Effectiveness of Class-Aware Calculation for
importance score. In Sec. 3.2, we introduced our
method, which considers the impact of individual
classes rather than treating the entire dataset as a
whole when estimating the importance score (IS). This

--- PAGE 10 ---
10 Henry Hengyuan Zhao et al.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Commonly used methods
Full 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear 0 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.0
Bias 0.10 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.1
PEFT methods
VPT-Deep 0.53 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 69.4
NOAH 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 73.3
LoRA 0.29 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 72.3
Adapter-8‚Ä†0.16 72.8 90.7 70.3 93.8 91.1 87.5 54.8 83.0 84.9 85.7 75.5 82.1 63.4 50.9 79.2 74.8 47.3 28.6 39.1 71.3
AdaptFormer-8‚Ä†0.18 73.5 91.5 70.8 98.9 91.1 87.8 54.3 82.3 94.9 86.9 76.3 82.3 66.3 51.0 78.8 76.5 46.2 32.3 40.1 72.7
SCT 0.11 75.3 91.6 72.2 99.2 91.1 91.2 55.0 85.0 96.1 86.3 76.2 81.5 65.1 51.7 80.2 75.4 46.2 33.2 45.7 73.6
Table 4: Comparisons with state-of-the-art methods on the VTAB-1K benchmark with ViT-B/16. Average re-
sults are calculated across 19 datasets. ‚Äú# Params‚Äù denotes the backbone‚Äôs average number of extra trainable
parameters. The best performance and smallest parameter number are bolded in each column. Underline notes
the second-best result. ‚Ä†means the model we implemented and trained on the same training setting, and the
hyperparameters are selected by standard grid search since Adapter Houlsby et al. (2019) is a strong baseline for
NLP tasks and Adaptformer Chen et al. (2022b) is not evaluated on VTAB-1K. The results of LoRA Hu et al.
(2021) are from NOAH Zhang et al. (2022). The hidden dimensions of the Adapter and AdaptFormer are 8 and
96. The reduction dimension of LoRA we set is 8.
VPT Adapter LoRA NOAH SCT7879808182Top-1 Accuracy
78.480.1
79.580.382.2Natural
VPT Adapter LoRA NOAH SCT82.082.583.083.584.084.585.085.586.0Top-1 Accuracy
82.4 82.384.684.985.9Specialized
VPT Adapter LoRA NOAH SCT5556575859606162Top-1 Accuracy
5558.259.861.3
59.9Structured
Fig. 7: Group-wise average results on VTAB-1K benchmark.
approach aims to identify the most significant channels
based on the mutual information between categories
and reduce the effects of imbalanced data. Our exper-
imental results, presented in Tab. 7, demonstrate that
adopting a class-aware calculation approach improves
performance across all three groups.
Insert Position. As shown in Fig. 6, SCTM can be in-
serted after the MLP block (SCT MLP) or between the
MHSA block and MLP block (SCT Attn). To investi-
gate the influence of insert location, we compare two
forms on the VTAB-1K benchmark in Tab. 8. Both
achieve promising performances, and SCT Attnoutper-
forms SCT MLP on two of the three groups. We conjec-
ture that after gathering long-range dependencies with
MHSA, the salient channels of features are more repre-
sentative, which can be better adapted to downstream
tasks. We also evaluate the performance of insertingSCTM after both MHSA and MLP blocks, as shown in
Tab. 10. Jointly inserting SCT Attn and SCT MLP only
brings small improvements while the number of param-
eters doubles.
Number of insert depth. The insert depth is an im-
portant factor that can significantly influence the per-
formance of a model. To investigate this, we conducted
experiments by inserting SCTM into the last llayers
of ViT-B and evaluated the performance on three rep-
resentative tasks from each group of VTAB-1K. Our
findings, presented in Fig. 9, reveal that increasing the
insert depth results in a gradual improvement in perfor-
mance. For the Resisc45 and DMLab tasks, we observed
that competitive performance was achieved when the
SCTM was inserted into the last six layers. Further
increases in the insert depth resulted in a plateau in

--- PAGE 11 ---
Salient Channel Tuning 11
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Commonly used methods
Full 86.7 72.2 88.0 71.4 98.3 89.5 90.1 45.0 86.6 96.9 87.7 79.4 75.7 59.8 54.6 78.6 79.4 53.6 34.6 40.9 74.2
Linear 0 61.4 90.2 74.8 99.5 90.2 42.7 55.8 81.5 90.1 82.1 69.4 39.1 35.9 40.1 65.0 20.3 26.0 14.3 27.8 56.4
Bias 0.24 73.0 86.8 65.6 97.7 87.5 56.4 52.3 80.4 91.6 76.1 72.5 47.3 48.5 34.7 66.2 57.6 36.2 34.7 66.2 62.1
PEFT methods
VPT-Deep 0.19 79.6 90.8 78.0 99.5 91.4 42.3 51.7 84.9 96.2 85.0 72.0 67.6 59.4 50.1 61.3 74.4 50.6 25.7 25.7 68.6
Adapter-8‚Ä†0.11 74.2 90.7 75.9 99.5 92.5 38.0 55.6 88.2 95.0 88.9 76.9 71.4 56.7 54.1 84.5 79.6 35.8 21.9 35.3 69.2
LoRA‚Ä†0.38 72.0 91.1 76.5 99.6 92.3 81.9 55.5 86.8 96.3 82.0 77.5 71.0 60.6 57.0 78.6 87.0 52.1 30.6 39.5 73.1
AdaptFormer-8‚Ä†0.13 74.6 91.0 76.2 99.6 92.4 83.5 55.4 87.0 95.2 88.9 76.8 73.8 57.9 54.8 82.7 82.2 49.7 21.6 36.9 72.6
SSF‚Ä†0.27 75.4 90.0 75.1 91.0 91.2 92.3 55.0 88.8 84.2 89.1 76.8 71.0 53.5 56.0 83.4 84.2 52.8 30.5 33.2 72.3
SCT (ours) 0.10 75.7 92.6 76.5 99.7 91.7 87.2 55.5 87.6 96.5 89.4 76.6 82.5 63.1 53.7 85.9 86.7 46.1 26.8 40.0 74.4
Table 5: Per-task results on VTAB-1K benchmark with a pre-trained Swin-B. The average result is calculated on
all 19 datasets in the VTAB-1K benchmark. The Bold text represents the best performance.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear 0 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.0
IC 0.11 68.8 91.5 71.5 99.0 91.1 89.0 54.3 85.3 95.8 86.6 75.2 78.1 61.2 50.3 79.0 73.1 40.0 27.4 36.7 71.1
RC-1 0.11 72.1 90.4 71.7 99.1 91.2 90.2 54.0 84.2 95.5 85.7 75.9 78.8 62.5 49.5 77.6 72.2 43.3 27.3 40.9 71.7
RC-2 0.11 74.1 91.4 72.1 99.2 90.9 90.2 55.1 83.0 95.7 86.3 75.0 79.3 62.5 50.4 78.2 72.5 43.2 28.1 39.8 72.0
RC-3 0.11 71.3 91.1 70.9 99.0 90.9 89.6 54.4 83.9 95.7 85.9 74.8 78.6 62.2 48.9 80.2 71.2 42.1 27.8 38.5 71.4
SCT 0.11 75.3 91.6 72.2 99.2 91.1 91.2 55.0 85.0 96.1 86.3 76.2 81.5 65.1 51.7 80.2 75.4 46.2 33.2 45.7 73.6
Table 6: Per-task results on VTAB-1K benchmark with a pre-trained ViT-B/16. The average result is calculated
on all 19 datasets in the VTAB-1K benchmark.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear 0 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.0
w/o CA 0.11 75.1 91.8 71.6 99.1 90.9 91.2 55.1 84.4 95.6 85.6 75.1 81.1 64.7 51.2 78.3 75.0 44.8 31.3 40.4 72.8
w/ CA 0.11 75.3 91.6 72.2 99.2 91.1 91.2 55.0 85.0 96.1 86.3 76.2 81.5 65.1 51.7 80.2 75.4 46.2 33.2 45.7 73.6
Table 7: Per-task results of evaluating the effectiveness of adopting the Class-Aware Importance Score. The average
result is calculated on all 19 datasets in the VTAB-1K benchmark.
performance, indicating that additional layers did not
contribute significantly to the final performance.
Number of selected channels K.The most impor-
tant hyperparameter of SCTM is the number of se-
lected channels K, which influences the model archi-
tecture and the number of trainable parameters. Note
that different from previous works (Jia et al., 2022;
Zhang et al., 2022) that select hyperparameters for each
dataset, we use the same Kfor all the datasets. Asshown in Tab.9, SCT K=32 beats the full fine-tuning and
bias tuning by 3.5% and 7.0%, respectively. Further-
more, SCT K=32 only adopts 0.01M parameters while
the tuning bias term adopts 0.10M. As shown in Fig. 8,
the performance generally improves along with the in-
crease of K. However, the improvement of K= 192 over
K= 96 is marginal, while the number of parameters
is four times larger. Considering both the effectiveness
and efficiency, we set Kto 96 by default.

--- PAGE 12 ---
12 Henry Hengyuan Zhao et al.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear 0 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.0
SCT MLP 0.11 72.9 92.0 71.6 99.1 90.6 90.5 54.9 86.2 95.9 85.9 76.0 81.3 64.7 50.7 78.9 77.2 43.3 29.3 43.6 72.9
SCT Attn 0.11 75.3 91.6 72.2 99.2 91.1 91.2 55.0 85.0 96.1 86.3 76.2 81.5 65.1 51.7 80.2 75.4 46.2 33.2 45.7 73.6
Table 8: Per-task results of evaluating the effectiveness of the insert positions. The average result is calculated on
all 19 datasets in the VTAB-1K benchmark.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear 0 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.0
Bias 0.10 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.1
SCT K=32 0.01 71.7 90.3 70.2 99.0 90.1 85.8 53.1 82.2 95.6 84.8 75.7 71.6 62.1 48.3 74.0 59.1 42.0 24.5 33.6 69.1
SCT K=96 0.11 75.3 91.6 72.2 99.2 91.1 91.2 55.0 85.0 96.1 86.3 76.2 81.5 65.1 51.7 80.2 75.4 46.2 33.2 45.7 73.6
SCT K=192 0.44 76.0 91.5 72.3 99.1 91.2 92.2 55.2 86.2 95.7 87.0 76.1 82.5 64.0 52.1 81.0 75.5 47.5 34.7 45.6 74.0
Table 9: Per-task results of evaluating different Kvalues on VTAB-1K benchmark.
Cifar100 Resisc45 DMLab Params
SCT Attn 75.3 86.3 51.7 0.11M
SCT MLP 72.9 85.9 50.9 0.11M
SCT Attn + SCT MLP 75.7 86.5 51.0 0.22M
Table 10: Comparison of different insert positions. The
Bold text represents the best performance.
4.4 Computation Analysis
Our analysis considers a ViT-B backbone with Llay-
ers and Ddimensions and Ntokens for a single im-
age. We also assume that the intermediate dimension
of Adapter (Houlsby et al., 2019) is D‚Ä≤, that the prompt
length of VPT (Jia et al., 2022) is n, and that the total
insert times of SSF (Lian et al., 2022) is m. Finally,
we compare our proposed SCT approach to Adapter,
VPT, and SSF in terms of parameters and FLOPs, as
summarized in Tab. 12. Notably, our selection of Kas
1
8Dis fairly small compared to D. When the reduction
ratio of the Adapter is also defined as 8, the param-
eters of the Adapter are1
4LD2while our parameters
are1
64LD2. When we compare our approach to SSF,
which inserts its module mtimes in the ViT-B back-
bone, the number of parameters for SSF is mLD . Ex-
amining the ViT-B backbone, we find that m= 74 and1
64D= 12, our parameters (12 LD < 74LD) and FLOPs
(12NLD < 74NLD ) are smaller than SSF. Overall, our
analysis suggests that SCT may offer PEFT a more ef-
ficient and effective baseline.
To assess the computational efficiency of our SCT
approach, we choose to compare it with Adapter
(0.16M) and SSF (0.20M) since they have a similar
level of parameters and all of them are applied in the
backbone network. We measured the running time and
memory usage for both the training and testing phases
using a batch size of 64 on the Cifar100 (VTAB-1K)
dataset with the ViT-B backbone. We conducted train-
ing experiments on a single NVIDIA V100-32GB GPU
and test experiments on a single NVIDIA A100-40GB
GPU. As the usage of GPU memory during the train-
ing and testing phases is different. During the train-
ing phase, tensor activations, model parameters, gra-
dients, and optimizer states (model, gradient, momen-
tum) are the main sources. During the testing phase,
tensor activations and model parameters are the main
sources. Our results, as shown in Fig. 10, indicate that
our SCT approach outperforms Adapter and SSF re-
garding training time and memory usage. As our tun-
able parameters are smaller than Adapter and SSF, the
cost of gradients and optimizer states are small which
helps us save the GPU memory during training. Specifi-

--- PAGE 13 ---
Salient Channel Tuning 13
K=32 K=96 K=1925060708090100
71.775.3 76.090.391.6 91.5
70.272.2 72.399.0 99.2 99.1
90.191.1 91.2
85.891.292.2
53.155.0 55.2Natural
Cifar
Caltech
DTD
Flower102
Pets
SVHN
Sun397
K=32 K=96 K=1927580859095100
82.285.086.295.696.1 95.7
84.886.387.0
75.776.2 76.1Specialized
Camelyon
EuroSAT
Resisc45
Retinopathy
K=32 K=96 K=192304050607080
76.176.1 76.1
62.165.164.0
48.351.7 52.174.080.2 81.0
59.175.4 75.5
42.046.247.5
24.533.234.7
33.645.7 45.6Structured
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Fig. 8: Choosing different Kvalues on 19 downstream tasks.
1 2 3 4 5 6 7 8 910 11 12
Layer depth4045505560657075Top-1 Accuracy
42.047.048.850.452.355.759.563.967.468.771.675.3Cifar100
1 2 3 4 5 6 7 8 910 11 12
Layer depth767880828486Top-1 Accuracy
77.480.782.2 82.183.383.784.3 84.4 84.584.885.186.3Resisc45
1 2 3 4 5 6 7 8 910 11 12
Layer depth363840424446485052Top-1 Accuracy
36.142.944.245.446.247.548.649.7 49.950.9 51.051.7DMLab
Fig. 9: Results of inserting SCTM into different layers. Zoom in for the best view.
cally, our method requires only 12 additional SCT com-
putations, while SSF requires 74 scaling and shifting
operations. The training of SSF is not quite efficient
compared with our SCT. However, during testing, the
test time of our method is slightly higher than Adapter
and SSF. Noted, SSF could merge its extra parameters
to the original backbone with no extra inference costs
while our method does not focus on reducing the com-
putation costs during inference. The reason for higher
test costs compared with Adapter may be the extra
operations of querying the salient channels from a full
feature and then putting the projected salient channelsback. Furthermore, we give exact test time cost results
in Tab. 11. Here LoRA and SSF could merge its pa-
rameter into the backbone network thus we choose Full
fine-tuning as the representative.
4.5 Experiments on Domain Generalization
Dataset. In addition to evaluating the model on test
data of the same distribution, modern deep neural net-
works commonly suffer from performance degradation
when the testing distribution is different from that

--- PAGE 14 ---
14 Henry Hengyuan Zhao et al.
Methods Test time Memory Params
Full fine-tuning 5.78 ms 336 MB 85.8 M
Adapter-8 6.87 ms 337 MB 85.8+0.16 M
AdaptFormer-8 7.88 ms 337 MB 85.8+0.18 M
SCT (Ours) 8.43 ms 337 MB 85.8+0.11 M
Table 11: We evaluate the test time for 1 image with
resolution 224. We run 200 times and yield the average
result.
Adapter VPT-Deep SSF TTC-Module
# Extra Parameters 2LDD‚Ä≤nLD mLD LKK
# Extra FLOPs 2NLDD‚Ä≤2n(2N+n)LD mNLD NLKK
Table 12: A complexity analysis of Adapter (Houlsby
et al., 2019), VPT (Jia et al., 2022), SSF (Lian et al.,
2022), and our proposed TTC-Module.
of the training set, i.e., domain shift, which is in-
evitable in a real-world application. To alleviate this
problem, domain generalization (Zhou et al., 2021b;
Zhao et al., 2022; Zhou et al., 2022c; Yang et al.,
2022a,b, 2021) is investigated in the community, aim-
ing to train a model with one or multiple source do-
mains that can perform well on other unseen target
domains. To verify the generalization ability of our
SCT, we follow Zhang et al. (2022) to conduct exper-
iments on ImageNet and its variants. Specifically, we
use the ImageNet-1K (Deng et al., 2009) as the source
domain with 16-shot per category and evaluate our
model on ImageNetV2 (Recht et al., 2019), ImageNet-
Sketch (Wang et al., 2019), ImageNet-A (Hendrycks
et al., 2021b), and ImageNet-R (Hendrycks et al.,
2021a). ImageNetV2 (Recht et al., 2019) is collected
from different sources from ImageNet-1K with the
same protocol, and ImageNet-Sketch (Wang et al.,
2019) contains the sketch images of ImageNet classes.
Both of them use the same classes as ImageNet-1K.
ImageNet-A (Hendrycks et al., 2021b) and ImageNet-
R (Hendrycks et al., 2021a) contain the adversarially-
filtered images and renditions of ImageNet data of a
200-class subset, respectively. We use a large version of
SCT, i.e., SCT-B, containing comparable parameters
with NOAH (0.44M vs. 0.43M).
Results. In Tab. 13, we compare our SCT-B with
Adapter (Houlsby et al., 2019), VPT (Jia et al., 2022),
LoRA (Hu et al., 2021), and NOAH (Zhang et al., 2022)
on the above datasets to verify the generalization abil-
ity. We can make two observations. First , SCT-B out-
performs the previous best method (NOAH) on three of
the four target datasets and achieves comparable per-
formance on ImageNetV2. Specifically, SCT-B yields an
improvement of 2.5% on ImageNet-R over NOAH. Sec-
ond, our SCT-B achieves an accuracy of 77.1% on the
source domain, greatly outperforming previous meth-
ods by 6%. Since the backbone model is pre-trained onImageNet-21K, the results on ImageNet-1K show that
SCT can better enhance the knowledge transfer from
superset to subset. The two observations demonstrate
the superiority of our SCT over previous fine-tuning
techniques on strong generalization ability.
4.6 Experiments on Few-shot Learning
Datasets. Following Zhang et al. (2022), we choose
five fine-grained visual recognition datasets, including
Food101 (Bossard et al., 2014), Flowers102 (Nilsback
and Zisserman, 2006), StandfordCars (Krause et al.,
2013), OxfordPets (Parkhi et al., 2012), and FGVCAir-
craft (Maji et al., 2013), to investigate the effectiveness
of SCT in few-shot learning. The categories in these
datasets cover various visual concepts closely related to
our daily life: food, plant, vehicle, and animal. Next,
we follow existing studies (Zhang et al., 2022; Jie and
Deng, 2022) to evaluate our model under 1, 2, 4, 8, and
16 shots settings. The experimental setup is the same
as in VTAB-1K.
Results. According to Fig. 11, our SCT surpasses other
baselines in all settings in terms of average perfor-
mance. In addition, SCT largely obtains the advantages
at FGVCAircraft and OxfordPets datasets. For other
datasets, SCT achieves competitive performance com-
pared with other baselines. Such results demonstrate
that SCT has a strong generalization ability that can
be readily transferred to downstream tasks only with a
few samples.
5 Conclusion
This paper introduces a simple baseline for PEFT,
which is both simple and effective. Unlike previous
methods, we adopt a channel selection perspective and
propose a straightforward importance score to iden-
tify task-specific channels. By eliminating downsam-
pling and nonlinearity operations, our method outper-
forms the similar Adapter method, requiring fewer pa-
rameters. We evaluate our algorithm on 19 downstream
tasks and achieve the best results with only 780 √ófewer
parameters than the original backbone. We also test
our approach on four datasets with natural distribu-
tion shifts to assess domain generalization capabilities
and on five datasets in the few-shot scenario to evalu-
ate performance in low-data regimes. Our approach is
computationally efficient during both training and test-
ing. Moreover, it is independent to prompt-based and
LoRa-like methods, allowing for further exploration in
future research. Although our channel selection tech-
nique is straightforward, there is considerable potential

--- PAGE 15 ---
Salient Channel Tuning 15
Source Target
ImageNet -V2 -Sketch -A -R
Adapter (Houlsby et al., 2019) 70.5 59.1 16.4 5.5 22.1
VPT (Jia et al., 2022) 70.5 58.0 18.3 4.6 23.2
LoRA (Hu et al., 2021) 70.8 59.3 20.0 6.9 23.3
NOAH (Zhang et al., 2022) 71.5 66.1 24.8 11.9 28.5
SCT-B (ours) 77.1 65.8 28.5 12.1 31.0
Table 13: Comparison with previous methods on domain generalization. The backbone network is ViT-B/16. The
number of salient channels is 192. The Bold text represents the best performance.
Full Linear Adapter SSF SCT
Methods0100200300400500600700Training Time(s)
Full Linear Adapter SSF SCT
Methods0246810Training Memory(G)
Full Linear Adapter SSF SCT
Methods012345678Test Time(ms)
Full Linear Adapter SSF SCT
Methods0.000.050.100.150.200.250.30Test Memory(G)
Fig. 10: The computational costs of training and test phases.
124 8 16
Number of labeled images per class304050607080Score (%)
Average
Adapter
Lora
VPT
NOAH
SCT
124 8 16
Number of labeled images per class1020304050Score (%)
FGVCAircraft
Adapter
Lora
VPT
NOAH
SCT
124 8 16
Number of labeled images per class60657075808590Score (%)
OxfordPets
Adapter
Lora
VPT
NOAH
SCT
124 8 16
Number of labeled images per class3040506070Score (%)
Food101
Adapter
Lora
VPT
NOAH
SCT
124 8 16
Number of labeled images per class10203040506070Score (%)
StanfordCars
Adapter
Lora
VPT
NOAH
SCT
124 8 16
Number of labeled images per class6065707580859095100Score (%)
Flowers102
Adapter
Lora
VPT
NOAH
SCT
Fig. 11: Results of few-shot learning on five fine-grained visual recognition datasets.
to enhance its performance. Our baseline can be partic-
ularly useful in resource-limited situations as it requires
fewer extra parameters and enables rapid adaptation tonew tasks with just a few samples, making the simple
and small modules effective knowledge storage units.

--- PAGE 16 ---
16 Henry Hengyuan Zhao et al.
Acknowledgement
This research is supported by National Research Foun-
dation, Singapore and A*STAR, under its RIE2020
Industry Alignment Fund ‚Äì Industry Collaboration
Projects (IAF-ICP) grant call (Grant No. I2001E0059)
‚Äì SIA-NUS Digital Aviation Corp Lab. Mike Zheng
Shou is supported by the National Research Founda-
tion, Singapore, under its NRFF Award NRF-NRFF13-
2021-0008, and Mike Zheng Shou‚Äôs Start-Up Grant
from NUS.
References
Ali A, Touvron H, Caron M, Bojanowski P, Douze M,
Joulin A, Laptev I, Neverova N, Synnaeve G, Verbeek
J, et al. (2021) Xcit: Cross-covariance image trans-
formers. In: NeurIPS
Bar A, Gandelsman Y, Darrell T, Globerson A, Efros
AA (2022) Visual prompting via image inpainting.
arXiv preprint arXiv:220900647
Beattie C, Leibo JZ, Teplyashin D, Ward T, Wain-
wright M, K¬® uttler H, Lefrancq A, Green S, Vald¬¥ es V,
Sadik A, et al. (2016) Deepmind lab. arXiv preprint
arXiv:161203801
Bossard L, Guillaumin M, Gool LV (2014) Food-
101‚Äìmining discriminative components with random
forests. In: European conference on computer vision
(ECCV), Springer, pp 446‚Äì461
Cai H, Gan C, Zhu L, Han S (2020) Tinytl: Reduce
memory, not parameters for efficient on-device learn-
ing. In: NeurIPS
Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A,
Zagoruyko S (2020) End-to-end object detection with
transformers. In: European conference on computer
vision, Springer, pp 213‚Äì229
Chen CFR, Fan Q, Panda R (2021a) Crossvit: Cross-
attention multi-scale vision transformer for image
classification. In: Proceedings of the IEEE/CVF in-
ternational conference on computer vision, pp 357‚Äì
366
Chen H, Tao R, Zhang H, Wang Y, Ye W, Wang J, Hu
G, Savvides M (2022a) Conv-adapter: Exploring pa-
rameter efficient transfer learning for convnets. arXiv
preprint arXiv:220807463
Chen S, Ge C, Tong Z, Wang J, Song Y, Wang J,
Luo P (2022b) Adaptformer: Adapting vision trans-
formers for scalable visual recognition. arXiv preprint
arXiv:220513535
Chen X, Xie S, He K (2021b) An empirical study of
training self-supervised vision transformers. In: Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision, pp 9640‚Äì9649Cheng G, Han J, Lu X (2017) Remote sensing image
scene classification: Benchmark and state of the art.
Proceedings of the IEEE 105(10):1865‚Äì1883
Cimpoi M, Maji S, Kokkinos I, Mohamed S, , Vedaldi
A (2014) Describing textures in the wild. In: Pro-
ceedings of the IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR)
Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)
Imagenet: A large-scale hierarchical image database.
In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR),
Ieee, pp 248‚Äì255
Dong X, Bao J, Chen D, Zhang W, Yu N, Yuan L,
Chen D, Guo B (2022) Cswin transformer: A gen-
eral vision transformer backbone with cross-shaped
windows. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,
pp 12124‚Äì12134
Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D,
Zhai X, Unterthiner T, Dehghani M, Minderer M,
Heigold G, Gelly S, et al. (2020) An image is worth
16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:201011929
d‚ÄôAscoli S, Touvron H, Leavitt ML, Morcos AS, Biroli
G, Sagun L (2021) Convit: Improving vision trans-
formers with soft convolutional inductive biases.
In: International Conference on Machine Learning,
PMLR, pp 2286‚Äì2296
Fan H, Xiong B, Mangalam K, Li Y, Yan Z, Malik J,
Feichtenhofer C (2021) Multiscale vision transform-
ers. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp 6824‚Äì6835
Fei-Fei L, Fergus R, Perona P (2004) Learning gen-
erative visual models from few training examples:
An incremental bayesian approach tested on 101 ob-
ject categories. In: conference on computer vision and
pattern recognition workshop, IEEE, pp 178‚Äì178
Geiger A, Lenz P, Stiller C, Urtasun R (2013) Vision
meets robotics: The kitti dataset. The International
Journal of Robotics Research 32(11):1231‚Äì1237
Han K, Xiao A, Wu E, Guo J, Xu C, Wang Y (2021)
Transformer in transformer. In: NeurIPS
Han S, Pool J, Tran J, Dally W (2015) Learning both
weights and connections for efficient neural network.
Advances in neural information processing systems
28
He Y, Kang G, Dong X, Fu Y, Yang Y (2018) Soft fil-
ter pruning for accelerating deep convolutional neural
networks. In: IJCAI International Joint Conference
on Artificial Intelligence
Helber P, Bischke B, Dengel A, Borth D (2019) Eu-
rosat: A novel dataset and deep learning benchmark
for land use and land cover classification. IEEE Jour-

--- PAGE 17 ---
Salient Channel Tuning 17
nal of Selected Topics in Applied Earth Observations
and Remote Sensing 12(7):2217‚Äì2226
Hendrycks D, Basart S, Mu N, Kadavath S, Wang F,
Dorundo E, Desai R, Zhu T, Parajuli S, Guo M,
et al. (2021a) The many faces of robustness: A criti-
cal analysis of out-of-distribution generalization. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV), pp 8340‚Äì8349
Hendrycks D, Zhao K, Basart S, Steinhardt J, Song D
(2021b) Natural adversarial examples. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pp 15262‚Äì
15271
Houlsby N, Giurgiu A, Jastrzebski S, Morrone B,
De Laroussilhe Q, Gesmundo A, Attariyan M, Gelly
S (2019) Parameter-efficient transfer learning for nlp.
In: International Conference on Machine Learning
(ICML), PMLR, pp 2790‚Äì2799
Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang
S, Wang L, Chen W (2021) Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:210609685
Jia M, Wu Z, Reiter A, Cardie C, Belongie S, Lim SN
(2021) Exploring visual engagement signals for repre-
sentation learning. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp
4206‚Äì4217
Jia M, Tang L, Chen BC, Cardie C, Belongie S, Har-
iharan B, Lim SN (2022) Visual prompt tuning. In:
ECCV
Jie S, Deng ZH (2022) Convolutional bypasses are
better vision transformer adapters. arXiv preprint
arXiv:220707039
Johnson J, Hariharan B, Van Der Maaten L, Fei-Fei
L, Lawrence Zitnick C, Girshick R (2017) Clevr: A
diagnostic dataset for compositional language and
elementary visual reasoning. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp 2901‚Äì2910
Kaggle, EyePacs (2015) Kaggle diabetic retinopa-
thy detection URL https://www.kaggle.com/c/
diabetic-retinopathy-detection/data
Krause J, Stark M, Deng J, Fei-Fei L (2013) 3d ob-
ject representations for fine-grained categorization.
In: Proceedings of the IEEE international conference
on computer vision workshops, pp 554‚Äì561
Krizhevsky A, Hinton G, et al. (2009) Learning multiple
layers of features from tiny images
LeCun Y, Huang FJ, Bottou L (2004) Learning meth-
ods for generic object recognition with invariance to
pose and lighting. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition (CVPR), IEEE, vol 2, pp II‚Äì104Li H, Kadav A, Durdanovic I, Samet H, Graf HP (2016)
Pruning filters for efficient convnets. arXiv preprint
arXiv:160808710
Li H, Kadav A, Durdanovic I, Samet H, Graf HP (2017)
Pruning filters for efficient convnets. In: Interna-
tional Conference on Learning Representations, URL
https://openreview.net/forum?id=rJqFGTslg
Li Y, Xie S, Chen X, Dollar P, He K, Girshick R (2021)
Benchmarking detection transfer learning with vision
transformers. arXiv preprint arXiv:211111429
Lian D, Zhou D, Feng J, Wang X (2022) Scaling & shift-
ing your features: A new baseline for efficient model
tuning. In: Advances in Neural Information Process-
ing Systems (NeurIPS)
Liao N, Shi B, Cao M, Zhang X, Tian Q, Yan J (2023)
Rethinking visual prompt learning as masked visual
token modeling. arXiv preprint arXiv:230304998
Liu L, Yu BX, Chang J, Tian Q, Chen CW
(2022) Prompt-matched semantic segmentation.
arXiv preprint arXiv:220810159
Liu Z, Sun M, Zhou T, Huang G, Darrell T (2018)
Rethinking the value of network pruning. In: Inter-
national Conference on Learning Representations
Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S,
Guo B (2021) Swin transformer: Hierarchical vision
transformer using shifted windows. In: Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision, pp 10012‚Äì10022
Loshchilov I, Hutter F (2017) Decoupled weight decay
regularization. arXiv preprint arXiv:171105101
Luo JH, Wu J, Lin W (2017) Thinet: A filter level prun-
ing method for deep neural network compression. In:
Proceedings of the IEEE international conference on
computer vision, pp 5058‚Äì5066
Luo X, Xu J, Xu Z (2022) Channel importance matters
in few-shot image classification. In: International con-
ference on machine learning, PMLR, pp 14542‚Äì14559
Mahajan D, Girshick R, Ramanathan V, He K, Paluri
M, Li Y, Bharambe A, Van Der Maaten L (2018)
Exploring the limits of weakly supervised pretrain-
ing. In: Proceedings of the European conference on
computer vision (ECCV), pp 181‚Äì196
Maji S, Rahtu E, Kannala J, Blaschko M, Vedaldi A
(2013) Fine-grained visual classification of aircraft.
arXiv preprint arXiv:13065151
Manli S, Weili N, De-An H, Zhiding Y, Tom G, An-
ima A, Chaowei X (2022) Test-time prompt tuning
for zero-shot generalization in vision-language mod-
els. In: NeurIPS
Matthey L, Higgins I, Hassabis D, Lerchner A (2017)
dsprites: Disentanglement testing sprites dataset
Netzer Y, Wang T, Coates A, Bissacco A, Wu B, Ng
AY (2011) Reading digits in natural images with un-

--- PAGE 18 ---
18 Henry Hengyuan Zhao et al.
supervised feature learning
Nie X, Ni B, Chang J, Meng G, Huo C, Zhang Z,
Xiang S, Tian Q, Pan C (2022) Pro-tuning: Uni-
fied prompt tuning for vision tasks. arXiv preprint
arXiv:220714381
Nilsback ME, Zisserman A (2006) A visual vocabu-
lary for flower classification. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), IEEE, vol 2, pp 1447‚Äì1454
Pan J, Lin Z, Zhu X, Shao J, Li H (2022) St-adapter:
Parameter-efficient image-to-video transfer learning.
Advances in Neural Information Processing Systems
35:26462‚Äì26477
Parkhi OM, Vedaldi A, Zisserman A, Jawahar C (2012)
Cats and dogs. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition (CVPR), IEEE, pp 3498‚Äì3505
Rao Y, Zhao W, Liu B, Lu J, Zhou J, Hsieh CJ (2021)
Dynamicvit: Efficient vision transformers with dy-
namic token sparsification. In: NeurIPS
Recht B, Roelofs R, Schmidt L, Shankar V (2019) Do
imagenet classifiers generalize to imagenet? In: Inter-
national Conference on Machine Learning (ICML),
PMLR, pp 5389‚Äì5400
Strudel R, Garcia R, Laptev I, Schmid C (2021) Seg-
menter: Transformer for semantic segmentation. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp 7262‚Äì7272
Sung YL, Cho J, Bansal M (2022) Vl-adapter:
Parameter-efficient transfer learning for vision-and-
language tasks. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pp 5227‚Äì5237
Touvron H, Cord M, Sablayrolles A, Synnaeve G, J¬¥ egou
H (2021) Going deeper with image transformers. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp 32‚Äì42
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
Gomez AN, Kaiser  L, Polosukhin I (2017) Attention
is all you need. In: NeurIPS
Veeling BS, Linmans J, Winkens J, Cohen T, Welling M
(2018) Rotation equivariant cnns for digital pathol-
ogy. In: International Conference on Medical im-
age computing and computer-assisted intervention,
Springer, pp 210‚Äì218
Wang H, Ge S, Lipton Z, Xing EP (2019) Learning
robust global representations by penalizing local pre-
dictive power. In: NeurIPS
Wang P, Wang X, Wang F, Lin M, Chang S, Xie W,
Li H, Jin R (2021) Kvt: k-nn attention for boosting
vision transformers. arXiv preprint arXiv:210600515
Wang S, Chang J, Wang Z, Li H, Ouyang W, Tian Q
(2022) Fine-grained retrieval prompt tuning. arXivpreprint arXiv:220714465
Xiao J, Hays J, Ehinger KA, Oliva A, Torralba A
(2010) Sun database: Large-scale scene recognition
from abbey to zoo. In: 2010 IEEE computer society
conference on computer vision and pattern recogni-
tion, IEEE, pp 3485‚Äì3492
Xing Y, Wu Q, Cheng D, Zhang S, Liang G, Zhang
Y (2022) Class-aware visual prompt tuning for
vision-language pre-trained model. arXiv preprint
arXiv:220808340
Xu R, Luo F, Zhang Z, Tan C, Chang B, Huang
S, Huang F (2021) Raise a child in large lan-
guage model: Towards effective and generalizable
fine-tuning. In: Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), Association for Computational Lin-
guistics
Yang J, Zhou K, Li Y, Liu Z (2021) Generalized out-
of-distribution detection: A survey. arXiv preprint
arXiv:211011334
Yang J, Wang P, Zou D, Zhou Z, Ding K,
Peng W, Wang H, Chen G, Li B, Sun Y,
et al. (2022a) Openood: Benchmarking general-
ized out-of-distribution detection. arXiv preprint
arXiv:221007242
Yang J, Zhou K, Liu Z (2022b) Full-spectrum
out-of-distribution detection. arXiv preprint
arXiv:220405306
Yuan L, Chen Y, Wang T, Yu W, Shi Y, Jiang ZH, Tay
FE, Feng J, Yan S (2021) Tokens-to-token vit: Train-
ing vision transformers from scratch on imagenet. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp 558‚Äì567
Yuan L, Hou Q, Jiang Z, Feng J, Yan S (2022) Volo:
Vision outlooker for visual recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
Zang Y, Li W, Zhou K, Huang C, Loy CC (2022)
Unified vision and language prompt learning. arXiv
preprint arXiv:221007225
Zhai X, Puigcerver J, Kolesnikov A, Ruyssen P,
Riquelme C, Lucic M, Djolonga J, Pinto AS, Neu-
mann M, Dosovitskiy A, et al. (2019) A large-
scale study of representation learning with the vi-
sual task adaptation benchmark. arXiv preprint
arXiv:191004867
Zhang B, Jin X, Gong W, Xu K, Zhang Z, Wang P, Shen
X, Feng J (2023a) Multimodal video adapter for pa-
rameter efficient video text retrieval. arXiv preprint
arXiv:230107868
Zhang Y, Zhou K, Liu Z (2022) Neural prompt search.
arXiv preprint arXiv:220604673
Zhang Y, Zhou K, Liu Z (2023b) What makes good
examples for visual in-context learning?

--- PAGE 19 ---
Salient Channel Tuning 19
Zhao Y, Zhong Z, Zhao N, Sebe N, Lee GH (2022)
Style-hallucinated dual consistency learning for do-
main generalized semantic segmentation. In: ECCV
Zheng Z, Yue X, Wang K, You Y (2022) Prompt vi-
sion transformer for domain generalization. arXiv
preprint arXiv:220808914
Zhou J, Wang P, Wang F, Liu Q, Li H, Jin R (2021a)
Elsa: Enhanced local self-attention for vision trans-
former. arXiv preprint arXiv:211212786
Zhou K, Yang Y, Qiao Y, Xiang T (2021b) Domain
generalization with mixstyle. In: ICLR
Zhou K, Yang J, Loy CC, Liu Z (2022a) Condi-
tional prompt learning for vision-language models.
In: IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)
Zhou K, Yang J, Loy CC, Liu Z (2022b) Learning
to prompt for vision-language models. International
Journal of Computer Vision (IJCV)
Zhou K, Zhang Y, Zang Y, Yang J, Loy CC, Liu
Z (2022c) On-device domain generalization. arXiv
preprint arXiv:220907521
