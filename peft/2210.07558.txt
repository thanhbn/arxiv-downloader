# 2210.07558.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2210.07558.pdf
# File size: 644523 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DyLoRA: Parameter-Efﬁcient Tuning of Pretrained Models using
Dynamic Search-Free Lo w Rank A daptation
Mojtaba Valipour1;2Mehdi Rezagholizadeh2Ivan Kobyzev2Ali Ghodsi1
{mojtaba.valipour, ali.ghodsi}@uwaterloo.ca, {mehdi.rezagholizadeh, ivan.kobyzev}@huawei.com
1: University of Waterloo, 2: Huawei Noah’s Ark Lab
Abstract
With the ever-growing size of pretrained mod-
els (PMs), ﬁne-tuning them has become more
expensive and resource-hungry. As a remedy,
low-rank adapters (LoRA) keep the main pre-
trained weights of the model frozen and just
introduce some learnable truncated SVD mod-
ules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter-efﬁcient,
they suffer from two major problems: ﬁrst,
the size of these blocks is ﬁxed and cannot
be modiﬁed after training (for example, if we
need to change the rank of LoRA blocks, then
we need to re-train them from scratch); sec-
ond, optimizing their rank requires an exhaus-
tive search and effort. In this work, we in-
troduce a dynamic low-rank adaptation (Dy-
LoRA) technique to address these two prob-
lems together. Our DyLoRA method trains
LoRA blocks for a range of ranks instead
of a single rank by sorting the representa-
tion learned by the adapter module at different
ranks during training. We evaluate our solu-
tion on different natural language understand-
ing (GLUE benchmark) and language genera-
tion tasks (E2E, DART and WebNLG) using
different pretrained models such as RoBERTa
and GPT with different sizes. Our results show
that we can train dynamic search-free models
with DyLoRA at least 4 to 7 times (depending
to the task) faster than LoRA without signiﬁ-
cantly compromising performance. Moreover,
our models can perform consistently well on a
much larger range of ranks compared to LoRA.
1
1 Introduction
Pre-training/ﬁne-tuning has become a popular
paradigm for solving many tasks in natural lan-
guage processing (NLP) (Devlin et al., 2018; Liu
et al., 2019; Brown et al., 2020) and Computer Vi-
sion (Simonyan and Zisserman, 2014; He et al.,
2016; Howard et al., 2019; Bochkovskiy et al.,
1github.com/huawei-noah/KD-NLP/tree/main/DyLoRA2020; Chen et al., 2020; Dosovitskiy et al., 2020).
pretrained models (PMs) such as pretrained lan-
guage models (PLMs) (Devlin et al., 2018; Brown
et al., 2020), and pretrained visual-language mod-
els (Lu et al., 2019; Li et al., 2019; Su et al., 2019;
Xia et al., 2021) have advanced a lot in recent years.
With the ever-growing size of these pretrained mod-
els, ﬁne-tuning them on downstream tasks becomes
more expensive. Moreover, as the ratio of the num-
ber of parameters of models with respect to the
labeled data increases, the ﬁne-tuning process will
be more prone to overﬁtting (Karimi Mahabadi
et al., 2021). There are two categories of solutions:
ﬁrst, model compression (Jafari et al., 2021; Chen
et al., 2021); second, parameter-efﬁcient tuning
(PET) (Houlsby et al., 2019a; Karimi Mahabadi
et al., 2021; Mao et al., 2021).
There are many different model compression
techniques in the literature for Transformer-based
models such as matrix factorization (Noach and
Goldberg, 2020; Tahaei et al., 2021), prun-
ing (Wang et al., 2019), quantization (Tao et al.,
2022; Prato et al., 2020), and knowledge distilla-
tion (Hinton et al., 2015; Li et al., 2021; Jafari et al.,
2021; Passban et al., 2021; Rashid et al., 2021).
There are also different types of PET techniques
in the literature such as low-rank adapters (Wang
et al., 2020; Karimi Mahabadi et al., 2021; Houlsby
et al., 2019b; Hu et al., 2021b), and prompt-based
techniques (Lester et al., 2021).
Although model compression solutions are well-
established in recent years in the literature, apply-
ing them to large language models can be very
costly, because compression techniques usually
need to train (or ﬁne-tune) the original large model.
A case in point is knowledge distillation which re-
lies on ﬁne-tuning a large teacher model or even
pre-training the student model as suggested in (Jiao
et al., 2019). Moreover, using compression tech-
niques usually leads to degrading the model perfor-
mance. PETs can be alternatives to the compres-arXiv:2210.07558v2  [cs.CL]  19 Apr 2023

--- PAGE 2 ---
Frozen  
Pretrained  
Weights  
DyLoRAParameter Updates Forward PassFigure 1: DyLoRA: The overall diagram of our proposed method. In each iteration, we sample from a pre-deﬁned
random distribution which will help us to truncate the up-projection and down-projection matrices in the LoRA
(Hu et al., 2021a) objective.
sion methods, especially when we would like to use
the full capacity of the large pretrained models with
light training efforts (such as the language-model-
as-a-service scenario (Sun et al., 2022)). Among
PET techniques, low-rank adapters have received
much attention because, in contrast to prompt-
tuning techniques, low-rank adapters do not add to
the sequence length, get trained faster, and perform
better (Karimi Mahabadi et al., 2021). Even though
there are several low-rank adaptation techniques
in the literature, such as Adapter (Houlsby et al.,
2019b), Compacter (Karimi Mahabadi et al., 2021),
and LoRA (Hu et al., 2021b); they all suffer from
two major common problems: ﬁrst, it is not clear
how to select the size of their rank (while their per-
formance is very sensitive to this rank selection);
second, their training is static which means that if
a low-rank model is trained based on a particular
rank size, it will not work well in other rank values
(i.e. for any other rank value we need to train a
separate model).
This paper proposes a dynamic low-rank adapter
technique (DyLoRA) to address these two prob-
lems. Without loss of generality, we focus on
LoRA(Hu et al., 2021a) and train LoRA blocks
for a range of ranks instead of a single rank by
sorting out the representation learned at different
ranks during training. While our model is more
ﬂexible, it can outperform LoRA in a much wider
range of ranks without adding to the training time.
Moreover, our technique does not need extra train-
ing for searching across ranks. We summarize our
contributions in the following:•Dynamic LoRA: On top of LoRA, we devel-
oped a new algorithm (DyLoRA) that makes
it dynamic at inference time without incurring
extra costs.
•Search-free LoRA: We demonstrate that by
making a negligible compromise in perfor-
mance, it is possible to avoid the costly search
process of choosing the optimal rank for
LoRA.
2 Related Work
This section reviews low-rank adaptation tech-
niques for parameter-efﬁcient tuning and poten-
tial existing solutions to make these techniques
dynamic and search-free.
It has been shown in (Aghajanyan et al., 2020)
that for classiﬁcation tasks such as natural language
understanding (NLU), PLMs have a low intrinsic
dimension. This observation motivates the use of
low-rank adapters for parameter-efﬁcient tuning.
There are several low-rank adapters in the literature
such as LoRA (Hu et al., 2021b), Adapter (Houlsby
et al., 2019b), Compacter (Karimi Mahabadi et al.,
2021), and Parallel Adapter (PA) (He et al., 2021).
LoRA is a low-rank up-projection/down-projection
transformation without any non-linearity applied
in parallel to key and value attention matrices.
The main beneﬁt of LoRA is that the adapter
module, after training, can be integrated into the
original weight matrices of the model, which in
turn can lead to a very efﬁcient inference time.
Adapters also have a low-rank up-projection/down-
projection transformation with an intermediate non-

--- PAGE 3 ---
linearity. The Adapter module is applied in series
with the feed-forward network (FFN). Having the
adaptor module in-line with other blocks in the
model can increase the inference time of the model.
PA is a faster version of the Adapter, which can be
applied in parallel with the FFN block. The com-
pactor is a more memory-efﬁcient version of the
Adapter, which deploys the sum of Kronecker prod-
ucts to reconstruct each up-projection and down-
projection matrices. All these low-rank adapters
suffer from two major issues: ﬁrst, ﬁnding the best
rank requires heavy exhaustive training and search;
second, the tuned adapter module works well only
with a particular rank.
While there have been some efforts in the lit-
erature towards dynamic networks such as Dyn-
aBERT (Hou et al., 2020) and GradMax (Evci et al.,
2022), to the best of our knowledge, this problem
for factorized networks and low-rank adapters is
still open. DRONE (Chen et al., 2021) propose a
technique for data-aware low-rank model compres-
sion however their approach is not search-free, and
also, it is not dynamic. DynaBERT introduces a
two-stage method to train width and depth-wise
dynamic networks. However, DynaBERT requires
a ﬁne-tuned teacher model on the task to train its
sub-networks which makes it unsuitable for PET
techniques. GradMax is a technique that gradually
adds to the neurons of a network without touch-
ing the already trained neurons. But it is unclear
how GradMax can be deployed to alleviate the
rank-search problem in low-rank adapters. Wang
et al. (2019) propose a structured pruning technique
called factorized low-rank pruning (FLOP). FLOP
decomposes weight matrices of a network into the
sum of rank-1 components, which are regularized
during training to gain sparsity. It is worth men-
tioning that FLOP aims at compressing the main
model, and even if it can be used for ﬁnding a good
rank in the lower-rank representation of full-weight
matrices, the ﬁnal low-rank model will not be dy-
namic (i.e. it is trained well only for one rank and
not a range of ranks, same as LoRA.). In this paper,
we propose a new methodology for training low-
rank modules for multiple ranks simultaneously
rather than training a single-rank adapter at a time
(without changing the training budget). Inspired by
the idea of nested dropout (Rippel et al., 2014), we
pursue ordering the representations of the bottle-
neck at the low-rank adapter modules with a new
recipe. To the best of our knowledge, it is the ﬁrsttime that the concept of ordering representations
has been deployed in training PLMs.
3 Background
3.1 Nested Dropout
Inspired by the dropout (Hinton et al., 2012), nested
drop-out (Rippel et al., 2014) is a stochastic regular-
ization technique that targets enforcing ordered rep-
resentations in training auto-encoders. The nested
dropout, adds an implicit bias (which does not exist
in dropout) to favor order in training. For example,
in dropout, we can randomly drop any nodes or
units in the network, but in nested dropout, if we
randomly select kthunit, then we keep all the units
indexed from 1tokand drop the units with indices
larger thank. Therefore, nested dropout tends to-
ward accommodating more important information
in lower indices while learning representations.
Following the notations of (Rippel et al., 2014),
nested dropout assumes an auto-encoder mapping
ofNtraining examplesfyigN
i=12Y,YRDto
their corresponding representations fxigN
i=12X,
XRKusing the function f:Y!Xwith pa-
rameters; and then decoding these representations
using another function g :X!Ywith parame-
ters to reconstruct the inputs. The reconstruction
loss can be deﬁned as follows:
C(; ) =NX
i=1jjyi g (f(yi))jj2: (1)
Suppose we want to randomly drop some units in
our representation vector x. In this regard, we sam-
ple a random variable bpB(:);b2f1;2;:::;Kg
from a pre-deﬁned categorical distribution pB(:)
and truncate the functions fandg to keep their
corresponding units indexed from 1 to band drop-
pingb+1toKindices. Let’s deﬁne the b-truncated
version of the vector xasx#band the b-truncated
version of the functions fandg asf#bandg #b
respectively. In this case, the reconstruction loss is
redeﬁned for the b-truncated model as follows:
C(; ) =EpB[C#b(; )] =KX
b=1pB(b)C#b(; )
where
C#b(; ) =NX
i=1jjyi g #b(f#b(yi))jj2:
(2)

--- PAGE 4 ---
In the ﬁnal stage, the parameters of this model can
be obtained by solving the following optimization
problem.
(; ) =argmin
; C(; ):(3)
While our work in this paper is inspired by the
feature of ordering information suggested in nested
dropout, we can distinguish our work from nested
dropout in several aspects:
1.The nested dropout technique is used to add
order information to a vector representation;
however, we are adding order information to
the low-rank matrix decomposition to make
it work across a range of ranks instead of a
single rank.
2.Our training algorithm differs from nested
dropout in the choice of the distribution func-
tionpB(:), and we propose a more efﬁcient
individual loss for each truncated matrix com-
pared to the linear summation loss (check
equations 2 and 11 in the original paper (Rip-
pel et al., 2014)) in nested dropout. The origi-
nal proposal for the nested dropout was to use
a batch with mixed truncated examples. To
enhance efﬁciency and resolve suboptimality,
we propose to ﬁx truncation in the entire batch
as part of our approach.
3.2 LoRA: Low-rank Adapters
In LoRA (Hu et al., 2021a), some pretrained
weights of dense layers of PLMs are summed with
parallel linear low-rank adapter modules. During
ﬁne-tuning, the original pretrained weights are kept
frozen; LoRA modules can be updated instead. For
example, let’s assume that W02Rmdis a pre-
trained weight matrix in the network which is ac-
companied by a LoRA module W=WupWdw
whereWup2Rmr,Wdw2Rrd, andr
min(m;d). Then, the output of this layer can be
obtained as
h=W0x+ Wx=W0x+
rWupWdwx:(4)
Bear in mind that the Wupmatrix is initialized as
a zero matrix, and the Wdwmatrix is initialized
as a zero-mean Gaussian distribution where is a
constant scale hyper-parameter.
In LoRA, the rank ris a hyperparameter that
should be tuned for each task. Moreover, LoRA
is astatic low-rank adapter that works only with a
particular size of r, which has been trained on it.4 Our Method: DyLoRA
In this section, we introduce our solution to get
dynamic low-rank adapters that can be trained and
deployed well on a range of ranks instead of a
single particular rank (with a ﬁxed training budget).
This ﬂexibility can free us from searching for the
best ranks by training the model multiple times.
Without loss of generality, we explain our so-
lution on top of LoRA as one of the prominent
low-rank adapter techniques in the literature. In
each LoRA module, we have an up-projection
(Wup2Rmr) and a down-projection matrix
(Wdw2Rrd). Let’s assume that we would like to
train the LoRA module to operate in the range of
r2Range [rmin;rmax]whererminandrmaxcan
be treated as new hyper-parameters. To make the
LoRA module work in a range of ranks instead of
a single rank, we need to ensure that increasing or
decreasing the rank will not signiﬁcantly hamper
the model’s performance. One way to implement
such behavior would be by sorting the information
content of different ranks in the training process
of LoRA modules. In this regard, at each train-
ing step, we sample bpB(:);b2frmin;rmin+
1;:::;rmaxgform a pre-deﬁned categorical distri-
bution (which has a support in Range [rmin;rmax])
and truncate WdwandWupmatrices accordingly.
Wdw#b=Wdw[1 :b;:]
Wup#b=Wup[:;1 :b](5)
Wdw#bandWup#bare b-truncated versions of Wdw
andWuprespectively (see Fig. 1 for the visualiza-
tion). Moreover, let’s deﬁne Wb
dwas thebthrow of
Wdw;Wb
upcorresponds to the bthcolumn ofWup.
Wb
dw=Wdw[b;:]
Wb
up=Wup[:;b](6)
Then, the forward pass of this truncated LoRA mod-
ule during training will be calculated as following:
h=W0x+
bWup#bWdw#bx (7)
For simplicity, let’s assume that we have only
one LoRA module in the network (the one which
is described in Eq. 7). Let’s ﬁrst consider the
regular static loss function ( LS) of the network
f(x;Wdw;Wup)withWdwandWuptunable pa-
rameters for Ngiven input-output pairs (x;y) =

--- PAGE 5 ---
(xi;yi)N
i=1:
min
Wdw;WupLS(x;y;Wdw;Wup),
NX
i=1l(f(xi;Wdw;Wup);yi):(8)
wherel(f;y)is a loss function that measures the
divergence of network predictions compared with
the target labels. Then, let’s extend the training
loss to make the network dynamic considering the
b-truncation process. We can deﬁne our dynamic
loss functionLDYas follows.
LDY
#b=NX
i=1l(f(xi;Wdw#b;Wup#b);yi):(9)
Bear in mind that, our loss function has a major dif-
ference from the nested dropout loss, which makes
it more efﬁcient. The nested dropout loss is in the
form ofPrmax
b=rminpB(b)LDY
#b(x;y;Wdw#b;Wup#b)
which requires to sum the loss over the entire possi-
ble range of ranks and it is computationally expen-
sive. To overcome this computational restriction,
we replace it by optimizing the model parameters
for each target rank individually at each time step.
We show that this scheme quite works well.
The other difference with nested dropout is that
in the parameter update phase, we add a new mode
(so-called frozen ) as a hyper-parameter to our train-
ing. This new mode suggests to only update the
bthcorresponding row and column sampled in the
truncation phase (i.e. a single row or column will
be updated at a time to prevent the learning param-
eters from being forgotten at previous time steps.).
With a minor performance cost, this approach can
improve the efﬁciency of our algorithm even fur-
ther.
Wb
dw Wb
dw rWb
dwLDY
#b
Wb
up Wb
up rWbupLDY
#b(10)
Table 4 shows the impact of only updating "b"
versus updating the columns and rows from 1 to
b. The summary of our technique is described in
Algorithm 1.
5 Experiments
In this section, we describe the experiments used
to evaluate our DyLoRA model on both natural lan-
guage understanding (NLU) and natural languageAlgorithm 1 DyLoRA - Training
Require:
r2Range[rmin,rmax];i: the number of training
iterations;: a scaling factor; pB: probability
distribution function for rank selection; X2
Rdn: all input features to LORA; W02Rmd
the original frozen pretrained weight matrix
Require:Wdw2Rrd;Wup2Rmr, FROZEN:
whether to keep the lower ranks frozen when
updating the higher ranks
while t <ido:
Forward:
// sample a speciﬁc rank, during test is given
bpB(:)
// truncate down-projection matrix
Wdw#b=Wdw[:b,:]
Wb
dw=Wdw[b,:]
// truncate up-projection matrix
Wup#b=Wup[:,:b]
Wb
up=Wup[:,b]
// calculate the LoRA output
h=W0X+
bWup#bWdw#bX
Backward:
ifFROZEN then
// only update the unique parameters
of the selected rank
Wb
dw Wb
dw rWb
dwLDY
#b
Wb
up Wb
up rWbupLDY
#b
else
Wdw#b Wdw#b rWb
dw#bLDY
#b
Wup#b Wup#b rWb
up#bLDY
#b
end if
end while
generation (NLG) tasks. To be fair with the orig-
inal LoRA method, we try to keep the setting of
our experiments similar to the LoRA paper (Hu
et al., 2021a). Therefore similarly, we chose the
pretrained RoBERTa (Liu et al., 2019) base model
as the backbone of the LoRA and DyLoRA exper-
iments for the GLUE benchmark (Development
Set), and GPT-Medium for the NLG tasks. For our
experiments, we did not use any hyper-parameter
tuning, nor did we search the validation epochs, nor
did we use MLNI trick (use the MLNI checkpoint
instead of the pretrained weights) to enhance the
model’s performance. More details about the hyper-
parameters is available in Table 8 in Appendix B.
In total, we conducted more than 200 experiments
and evaluated more than 1600 models, details of

--- PAGE 6 ---
Model: RoBERTa-Base
Task Rank=1 Rank=2 Rank=4 Rank=8 Rank=16 Rank=32
QQP (Accuracy) 89.14 89.96 90.33 90.69 90.95 91.02
SST-2 (Accuracy) 93.58 94.15 94.38 94.84 94.27 94.5
MRPC (Accuracy) 87.25 87.75 88.24 87.25 86.76 89.22
CoLA (Mathews) 61.84 57.78 61.57 63.81 63.07 62.82
Table 1: The effect of the rank of the low-rank adaptation matrix over the performance of the model. In this exper-
iment, all the other hyperparameters are ﬁxed, and we only changed the rank of the LoRA model. In this search
space, Underline shows the minimum performance rank, and the bold number shows the maximum performance
rank.
Accuracy Accuracy F1 Mathews Accuracy Accuracy Accuracy Pearson
Model MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg
Rank = 1
LoRA 34:603:6969:617:9983:473:9025:579:7153:002:9544:307:50 57:555:51 76:076:06 54.90
DyLoRA (Frozen) 85:360:2693:510:4990:750:7056:951:5491:700:2887:870:17 66:798:54 89:950:24 82.86
DyLoRA 85:590:0793:230:6391:580:6957:932:1291:950:1488:370:15 74:801:48 90:300:13 84.22
Rank = 2
LoRA 40:536:1782:755:0888:001:8143:304:6763:422:9959:216:13 68:881:26 85:511:94 66.45
DyLoRA (Frozen) 85:740:2893:760:5291:090:4556:882:0992:030:2288:210:0763:9012:8590:250:15 82.73
DyLoRA 86:020:0693:810:3091:660:4659:911:8892:390:2589:330:05 76:031:61 90:600:09 84.97
Rank = 3
LoRA 58:956:0290:001:2789:661:2556:781:8879:264:8072:584:09 72:492:30 88:800:29 76.07
DyLoRA (Frozen) 85:780:2593:760:2691:780:8958:860:3292:170:18 88:400:070:906:14 90:500:29 84.02
DyLoRA 86:700:0994:110:3391:560:8660:972:0192:770:2189:760:07 77:112:97 90:690:14 85.46
Rank = 4
LoRA 72:105:2591:560:3489:620:9258:533:9385:091:2080:783:73 73:072:29 89:280:72 80.00
DyLoRA (Frozen) 85:930:1993:850:3391:280:7159:251:0592:270:1688:520:08 71:122:46 90:530:18 84.10
DyLoRA 86:820:0494:400:1392:060:4659:811:7192:910:3189:800:10 77:402:72 90:860:06 85.53
Rank = 5
LoRA 78:613:9792:820:4690:750:9660:373:1088:970:9085:261:56 73:212:17 89:900:30 82.49
DyLoRA (Frozen) 85:950:1793:780:2691:280:6459:411:3092:300:1788:560:09 71:482:92 90:600:20 84.17
DyLoRA 87:000:1094:290:4191:730:6060:521:0793:010:2890:040:10 76:902:11 90:970:20 85.56
Rank = 6
LoRA 83:021:5993:490:8891:280:6361:942:2790:320:7687:541:51 76:681:16 90:120:12 84.30
DyLoRA (Frozen) 85:980:1693:760:4691:120:4358:951:1092:460:1488:680:13 72:642:44 90:640:23 84.28
DyLoRA 86:970:2094:270:3791:440:6460:161:7093:010:2190:070:14 77:331:66 91:030:20 85.53
Rank = 7
LoRA 85:440:7893:620:3591:270:7362:192:6691:880:2389:510:30 75:521:41 90:350:24 84.97
DyLoRA (Frozen) 86:080:1493:970:1791:020:7058:760:9492:300:1088:770:06 73:501:67 90:680:15 84.38
DyLoRA 86:820:1094:270:3391:380:5959:511:7592:990:2690:040:06 77:911:58 91:070:19 85.50
Rank = 8
LoRA 86:820:1894:010:3091:480:7362:081:3792:390:3990:420:02 74:510:41 90:480:24 85.27
DyLoRA (Frozen) 86:100:0493:690:4191:190:7958:520:9592:470:1888:820:06 73:292:49 90:680:14 84.35
DyLoRA 86:760:1394:360:3891:380:8359:511:8493:000:3289:910:08 77:550:59 91:050:19 85.44
Best (Rank)
LoRA 87.03(8) 94.50(6) 92.25(7) 66.05 (7) 92.81(8) 90.45 (8) 77.98(6) 90.87(8) 86.49
DyLoRA (Frozen) 86.18(7) 94.50 (2) 92.93(3) 61.57(5) 92.70 (6) 88.88(8) 75.81(7) 90.89 (6) 85.43
DyLoRA 87.17(6) 94.72 (7) 92.79(8) 63.32 (3) 93.56 (8) 90.17 (6) 80.14(4) 91.36 (7) 86.66
Full Rank
Fine Tune87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4
Table 2: In this table, the task is to ﬁnd a low-rank adaptation matrix that works with different ranks at inference
time given a ﬁxed budget (training time).
which can be found in the attachments.
5.1 Baselines
•Fine Tune : To show a relative upper bound
for the performance of our proposed method,
we ﬁne-tuned all the parameters in the model.Even though we have a large number of train-
able parameters, this can help us better under-
stand how higher-rank models perform.
•LoRA : As a baseline to DyLoRA, we em-
ployed the original LoRA model with their
tuned hyperparameters (Hu et al., 2021a). As

--- PAGE 7 ---
Accuracy F1 Accuracy Pearson
Model (Rank) Trainable Params SST-2 MRPC QNLI STS-B A VERAGE
Fine Tune125M 94.8 90.2 92.8 91.2 92.25
FLOP80M 92.09 88.61 89.05 88.18 89.48
LoRA (1) 0.628M 93.58 91.93 91.98 90.85 92.09
Maximum Rank: rmax= 8
DyLoRA (1) 0.628M 93:230:6391:580:6991:950:1490:300:13 91.77
DyLoRA (8) 0.887M 94:360:3891:380:8393:000:3291:050:19 92.45
Table 3: This table compares DyLoRA with compression-based algorithms. As indicated by *, we reported "Fine
Tune" and FLOP from their original papers, (Liu et al., 2019) and (Wang et al., 2019). To the best of our knowledge,
experiments were conducted under the same experimental setting. We count all the trainable parameters including
classiﬁer, unlike LoRA paper (Hu et al., 2021a) which they count only LoRA speciﬁc parameters.
Maximum Rank: rmax= 8
Accuracy F1 Mathews Accuracy Accuracy Pearson
bPB:Distribution Updated Parameters SST-2 MRPC CoLA QNLI RTE STS-B A VERAGE
Rank=8
Geometric (p=0.15)Wdw#b,Wup#b 93:970:3390:841:1558:951:9592:740:13 74:800:90 90:660:15 83.66
Wb
dw,Wb
up 93:600:2490:500:4258:191:1792:260:12 71:911:74 90:200:36 82.78
UniformWdw#b,Wup#b 94:360:3891:380:8359:511:8493:000:32 77:550:59 91:050:19 84.47
Wb
dw,Wb
up 93:690:4191:190:7958:520:9592:470:18 73:292:49 90:680:14 83.31
Rank=1
Geometric (p=0.15)Wdw#b,Wup#b 93:530:4791:360:7259:431:1292:240:08 73:653:55 90:330:14 83.42
Wb
dw,Wb
up 93:580:2690:810:8358:551:1392:270:2868:5211:8890:600:31 82.39
UniformWdw#b,Wup#b 93:230:6391:580:6957:932:1291:950:14 74:801:48 90:300:13 83.30
Wb
dw,Wb
up 93:510:4990:750:7056:951:5491:700:28 66:798:54 89:950:24 81.61
Table 4: Ablation Study - In this experiment, our goal is to demonstrate how the introduced distribution can affect
the performance of DyLoRA.
a result, most of the experiments have been
conducted in a favorable manner for LoRA.
•FLOP : Due to its ﬂexibility, Factorized Low
Rank Pruning (FLOP) (Wang et al., 2019) can
be applied to any matrix multiplication and,
therefore, can be used to avoid the search in
our problem. However, this baseline lacks the
dynamic properties of DyLoRA. We used it
to show regularization-based techniques’ per-
formance and pros and cons.
5.2 LoRA rank selection problem
There is no clear guidance on how to determine
the rank for the LoRA algorithm. It is evident in
the LoRA paper (Hu et al., 2021a) that the perfor-
mance of models varies a lot with different ranks
(e.g. check Tables 15, and 18 in the LoRA paper),
and does not indicate any clear trend. We also ob-
serve the same problem in the GLUE benchmark.
We may argue that theoretically, the rank with the
best performance is always the highest. High ranks,
however, introduce additional parameters into the
adaptive process and this might be undesirable. In
practice, as demonstrated in Table 1, the most ef-
fective rank differs depending on the task. Forexample, based on the MRPC results, the rank with
the lowest performance is 16 while the rank with
the highest performance is 32. This is different
from SST-2, in which rank 1 is the least performing
rank and rank 8 is the most effective rank. Many
factors can contribute to this difference, including
but not limited to the size of the dataset, hyperpa-
rameter selections, hardware conﬁgurations and the
optimization.
5.3 Dynamic low rank adaptation
For example, suppose we have a neural network
that we wish to deploy on various devices with dif-
ferent conﬁgurations. The use of higher ranks may
pose a problem for very sensitive devices as they
have a greater number of parameters. Therefore,
we must either train several models with different
conﬁgurations or ﬁnd the most optimal rank. The
cost associated with this is signiﬁcant, as even in
the setting of LoRA, we are required to ﬁnd the
best rank for each task and each device. Using
DyLoRA, however, one needs to train one model
per task and, as our method is adaptive at inference
time, we can deploy it according to our needs. In
Table 2, we demonstrate the dynamic properties of

--- PAGE 8 ---
DyLoRA. In order to ensure a fair comparison, all
LoRA and DyLoRA models in this table have the
same model size, we used the same code and eval-
uation process, and all models were trained to the
same extent. In LoRA, we lose performance when
performing inferences for the lower ranks. This
occurs because the model has been trained only for
rank 8 during training. In DyLoRA, we preserve
a high level of performance for lower ranks while
competing well with LoRA on rank 8.
Model Time SST-2 (r)MRPC (r)
Maximum Rank: rmax = 64
LoRA (Search) 7x 95.3(64) 89.71(64)
DyLoRA (Frozen) 1x 94.38 (7) 89.95(34)
Maximum Rank: rmax = 32
LoRA (Search) 6x 94.84 (32) 88.73(16)
DyLoRA (Frozen) 1x 94.38 (7) 89.71(5)
Table 5: In this table, the search space of rank is larger
compared to the previous experiment and the goal is to
ﬁnd the most optimal rank for the low-rank adaptation
of a pre-rained RoBERTa-Base. For LoRA (Search),
we ran experiments for ranks=1,2,4,8,16,32,64 and we
reported the best results. In the Exhaustive Search, one
has to search all the ranks from 1 to 64, which means
it will cost 64 times more than our proposed method.
The lower the rank the better, and the higher the perfor-
mance is the better.
5.4 Search-free low rank adaptation
The process of selecting a particular rank can be
expensive as previously mentioned. In Table 5, we
present an experiment that illustrates the costs asso-
ciated with such a search for LoRA and DyLoRA.
As an example, if one naively wanted to search the
entire range of ranks (for example, 64 in the exper-
iment), then they would have to train and evaluate
64 distinct models in order to determine the proper
rank. It becomes even more expensive if one search
the entire rank space. In the case of uniform search,
this cost is less, yet still more expensive (7 times in
the experiment) than our proposed method. There-
fore, for LoRA (Search), we ran experiments for
ranks=1,2,4,8,16,32,64 and we reported the best
results. The results demonstrate that our proposed
method performs competitively at a much lower
cost.
5.5 Robustness of DyLoRA
As illustrated in Table 2, DyLoRA is quite robust
to randomness and can produce consistently good
results due to stable convergence.5.6 Regularization and Pruning
An alternative method of avoiding the search prob-
lem is using regularization/pruning techniques to
determine the intrinsic rank of the weight matrix.
In this way, we can reduce the number of param-
eters of the original matrices; however, we will
not have a dynamic model during inference. To
illustrate the difference between such methods and
DyLoRA, we reported the performance of one of
these models, FLOP (Wang et al., 2019), in Table
3. FLOP utilizes low-rank factorization to create
new matrices representing the original weight ma-
trix. Thus, they will have fewer total parameters
but require more trainable parameters to reach a
comparable performance to DyLoRA.
5.7 Generative Tasks
In this experiment, we evaluate the performance
of our model on different natural language gen-
eration (NLG) tasks such as the E2E NLG Chal-
lenge (Novikova et al., 2017), DART (Nan et al.,
2020) and WebNLG (Gardent et al., 2017). The
results of the E2E task are shown in Table 6 and
due to the space limit, the results of the other two
tasks are demonstrated in Appendix C. The genera-
tive tasks demonstrate a similar pattern as the NLU
task, showing that our model is able to work well
at wider range of ranks compared to LoRA.
5.8 Ablation study
In this subsection, we investigate the impact of two
design choices in DyLoRA: ﬁrst, the new distri-
butionPBhyper-parameter in our technique; sec-
ond, the impact of updating Wb
dwandWb
upparam-
eters instead of the entire Wdw#bandWup#b. The
distribution PBchanges the relative importance
of the different ranks during the training process.
To examine the impact of the chosen distribution
on DyLoRA’s performance, we used two distribu-
tions, geometric and uniform. As shown in Table
4, the geometric distribution, provides a much bet-
ter method for optimizing the lower ranks, since it
pays much more attention to the lower ranks during
training, and uniform distribution will give better
performance over all ranks. We chose to use uni-
form distribution in most of our experiments to
avoid adding another hyperparameter which is a re-
quirement of the geometric distribution. Moreover,
we demonstrate that it is possible to ensure that the
optimization of rank bwill not negatively affect the
performance of the lower ranks ( 1tob 1), while

--- PAGE 9 ---
Model (Method) Updated Params Trainable Params E2E NLG Challenge
BLEU NIST MET ROUGE-L CIDEr
Rank=1
GPT-2 M (LoRA) 0.09M 3.38 1.18 9.23 18.79 0.12
GPT-2 M (DyLoRA) Wb
dw,Wb
up 0.09M 67:920:208:650:0644:910:3869:070:322:380:04
GPT-2 M (DyLoRA) Wdw#b,Wup#b 0.09M 68:860:558:720:0445:810:4070:330:642:430:04
Rank=2
GPT-2 M (LoRA) 0.19M 46.99 6.39 34.19 56.10 1.27
GPT-2 M (DyLoRA) Wb
dw,Wb
up 0.19M 68:810:498:750:0245:230:2269:810:302:410:01
GPT-2 M (DyLoRA) Wdw#b,Wup#b 0.19M 68:971:038:750:0745:880:5570:070:862:430:04
Rank=3
GPT-2 M (LoRA) 0.29M 63.68 8.46 42.37 65.84 2.24
GPT-2 M (DyLoRA) Wb
dw,Wb
up 0.29M 68:411:008:690:1045:310:6469:750:692:420:02
GPT-2 M (DyLoRA) Wdw#b,Wup#b 0.29M 69:330:268:760:0546:190:2270:560:432:460:01
Rank=4
GPT-2 M (LoRA) 0.39M 69.88 8.81 46.81 72.10 2.53
GPT-2 M (DyLoRA) Wb
dw,Wb
up 0.39M 68:360:418:700:0245:460:5669:910:502:430:01
GPT-2 M (DyLoRA) Wdw#b,Wup#b 0.39M 69:190:438:750:0346:260:4770:780:632:460:02
Fine-Tune
GPT-2 M (FT)354M 68.2 8.62 46.2 71.0 2.5
Table 6: For all metrics, higher is better. Rows with * have been reported based on the LoRA paper. Unlike (Hu
et al., 2021a), we included the classiﬁer number of parameters in our trainable parameters count.
performing reasonably well. As mentioned, this
can be accomplished by only updating the unique
parameters associated with rank rthat do not over-
lap with lower ranks.
In addition, in Table 7, we demonstrate the result
of using our individual loss (Eq. 9) vs. the nested
dropout original objective function in an equal set-
ting. As shown, our proposed objective function
is both effective and efﬁcient. Furthermore, it is
important to note that the summation loss is not
scalable when many ranks are involved. We also
discussed the time complexity of LoRA and Dy-
LoRA in Appendix A.
Maximum Rank: rmax= 8
Loss Training Time CoLA
LDY
#b645.82s 52.64PpB(b)LDY
#b1175.69s 54.12
Table 7: This experiment shows the impact of choos-
ing individual loss vs. summation loss functions on
our training. The average performance across all possi-
ble ranks (1,2,...,8) is reported. For summation loss to
be computationally more feasible, smaller epochs were
chosen. A total of seven GPUs were used in this exper-
iment.
6 Conclusion
In this paper, we presented our solution DyLoRA to
address two problems in low-rank adapters regard-
ing rank selection and making them dynamic. We
showed that DyLoRA can select the rank withoutrequiring multiple re-training and is able to make
LoRA dynamic at inference time. As a result, we
can avoid the process of searching for the most
optimal ranks for many real-life scenarios. It has
been demonstrated that DyLoRA performance is
comparable with LoRA, yet we can support a wider
range of ranks without adding additional time and
effort.
Limitations
According to LoRA (Hu et al., 2021a), a proper
choice of the scalar can improve the results. In
order to determine what is the best choice, further
investigation is required. Despite our demonstra-
tion that uniform distribution can be as effective
as speciﬁc geometric distribution, further investiga-
tion is necessary to evaluate the effect of different
distributions on different downstream tasks. As
shown in this paper, our algorithm works over a
wide range of ranks, but further research is needed
to understand the impact of choosing a particular
range.
7 Acknowledgement
We would like to use DyLoRA with Mindspore2,
which is a new framework for deep learning com-
puting.
2mindspore.cn

--- PAGE 10 ---
References
Armen Aghajanyan, Luke Zettlemoyer, and Sonal
Gupta. 2020. Intrinsic dimensionality explains the
effectiveness of language model ﬁne-tuning. arXiv
preprint arXiv:2012.13255 .
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-
Yuan Mark Liao. 2020. Yolov4: Optimal speed
and accuracy of object detection. arXiv preprint
arXiv:2004.10934 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu,
Heewoo Jun, David Luan, and Ilya Sutskever. 2020.
Generative pretraining from pixels. In International
conference on machine learning , pages 1691–1703.
PMLR.
Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-
Jui Hsieh. 2021. Drone: Data-aware low-rank com-
pression for large nlp models. Advances in neural
information processing systems , 34:29321–29334.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.
An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint
arXiv:2010.11929 .
Utku Evci, Max Vladymyrov, Thomas Unterthiner,
Bart van Merriënboer, and Fabian Pedregosa. 2022.
Gradmax: Growing neural networks using gradient
information. arXiv preprint arXiv:2201.05125 .
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The webnlg
challenge: Generating text from rdf data. In Pro-
ceedings of the 10th International Conference on
Natural Language Generation , pages 124–133.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2021. Towards a
uniﬁed view of parameter-efﬁcient transfer learning.
arXiv preprint arXiv:2110.04366 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 770–
778.Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7).
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .
Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao
Chen, and Qun Liu. 2020. Dynabert: Dynamic bert
with adaptive width and depth. Advances in Neural
Information Processing Systems , 33:9782–9793.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019a. Parameter-efﬁcient transfer learning for nlp.
InInternational Conference on Machine Learning ,
pages 2790–2799. PMLR.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019b. Parameter-efﬁcient transfer learning for nlp.
InInternational Conference on Machine Learning ,
pages 2790–2799. PMLR.
Andrew Howard, Mark Sandler, Grace Chu, Liang-
Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al.
2019. Searching for mobilenetv3. In Proceedings
of the IEEE/CVF international conference on com-
puter vision , pages 1314–1324.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021a. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021b. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma,
and Ali Ghodsi. 2021. Annealing knowledge distil-
lation. In Proceedings of the 16th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Main Volume , pages 2493–2504,
Online. Association for Computational Linguistics.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling bert for natural language
understanding. arXiv preprint arXiv:1909.10351 .
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efﬁcient low-rank
hypercomplex adapter layers. Advances in Neural
Information Processing Systems , 34:1022–1035.

--- PAGE 11 ---
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou,
and Xu Sun. 2021. Dynamic knowledge distilla-
tion for pre-trained language models. arXiv preprint
arXiv:2109.11295 .
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Hsieh, and Kai-Wei Chang. 2019. Visualbert: A
simple and performant baseline for vision and lan-
guage. arXiv preprint arXiv:1908.03557 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
olinguistic representations for vision-and-language
tasks. Advances in neural information processing
systems , 32.
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Ma-
dian Khabsa. 2021. Unipelt: A uniﬁed frame-
work for parameter-efﬁcient language model tuning.
arXiv preprint arXiv:2110.07577 .
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit
Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-
gru Tang, Aadit Vyas, Neha Verma, Pranav Kr-
ishna, et al. 2020. Dart: Open-domain struc-
tured data record to text generation. arXiv preprint
arXiv:2007.02871 .
Matan Ben Noach and Yoav Goldberg. 2020. Com-
pressing pre-trained language models by matrix de-
composition. In Proceedings of the 1st Confer-
ence of the Asia-Paciﬁc Chapter of the Association
for Computational Linguistics and the 10th Interna-
tional Joint Conference on Natural Language Pro-
cessing , pages 884–889.
Jekaterina Novikova, Ond ˇrej Dušek, and Verena Rieser.
2017. The e2e dataset: New challenges for end-to-
end generation. arXiv preprint arXiv:1706.09254 .
Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh,
and Qun Liu. 2021. ALP-KD: attention-based layer
projection for knowledge distillation. In Thirty-Fifth
AAAI Conference on Artiﬁcial Intelligence, AAAI
2021, Thirty-Third Conference on Innovative Appli-
cations of Artiﬁcial Intelligence, IAAI 2021, The
Eleventh Symposium on Educational Advances in
Artiﬁcial Intelligence, EAAI 2021, Virtual Event,
February 2-9, 2021 , pages 13657–13665. AAAI
Press.
Gabriele Prato, Ella Charlaix, and Mehdi Reza-
gholizadeh. 2020. Fully quantized transformer for
machine translation. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1–14.Ahmad Rashid, Vasileios Lioutas, and Mehdi Reza-
gholizadeh. 2021. Mate-kd: Masked adversarial
text, a companion to knowledge distillation. arXiv
preprint arXiv:2105.05912 .
Oren Rippel, Michael Gelbart, and Ryan Adams.
2014. Learning ordered representations with nested
dropout. In Proceedings of the 31st International
Conference on Machine Learning , volume 32 of
Proceedings of Machine Learning Research , pages
1746–1754, Bejing, China. PMLR.
Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 .
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,
Furu Wei, and Jifeng Dai. 2019. Vl-bert: Pre-
training of generic visual-linguistic representations.
arXiv preprint arXiv:1908.08530 .
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing
Huang, and Xipeng Qiu. 2022. Black-box tun-
ing for language-model-as-a-service. arXiv preprint
arXiv:2201.03514 .
Marzieh S Tahaei, Ella Charlaix, Vahid Partovi Nia,
Ali Ghodsi, and Mehdi Rezagholizadeh. 2021. Kro-
neckerbert: Learning kronecker decomposition for
pre-trained language models via knowledge distilla-
tion. arXiv preprint arXiv:2109.06243 .
Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang,
Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong.
2022. Compression of generative pre-trained lan-
guage models via quantization. arXiv preprint
arXiv:2203.10705 .
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,
Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming
Zhou, et al. 2020. K-adapter: Infusing knowl-
edge into pre-trained models with adapters. arXiv
preprint arXiv:2002.01808 .
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.
Structured pruning of large language models. arXiv
preprint arXiv:1910.04732 .
Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong
Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon
Bharti, and Ming Zhou. 2021. Xgpt: Cross-modal
generative pre-training for image captioning. In
CCF International Conference on Natural Language
Processing and Chinese Computing , pages 786–797.
Springer.
A Time complexity
The training time for DyLoRA is comparable to
that of LoRA trained once on a speciﬁc rank. Thus,
when searching the rank space for LoRA, we need
to train it multiple times, whereas our method does
not require searching the ranks. Accordingly, Dy-
LoRA’s relative time complexity is inversely pro-
portional to the number of possible ranks for which

--- PAGE 12 ---
the LoRA model must be searched. In MRPC,
DyLoRA (for all the ranks) and LoRA (only on
a single rank 8) require a total training time of
408.39 seconds and 399.95 seconds, respectively.
Consequently, when we need to train eight LoRA
models (Rank=1,2,...,8), it will result in a cost of
399.95*8=3199.6s, compared to the training time
of our model, which is only 408.39 seconds. A
more efﬁcient implementation of our algorithm
may result in a better time complexity.
B Hyperparameters
We did not use any parameter tuning nor MNLI
trick (initializing some down-streams tasks from
MNLI checkpoint instead of pretrained weights).
Therefore, we ﬁne-tuned all the datasets from orig-
inal pretrained weights. We simply followed a uni-
ﬁed hyper-parameters for all different experiments.
Unlike LoRA (Hu et al., 2021a) which reported the
median over 5 random seeds, we reported the mean
and standard deviation over 5 random seeds. See
the details in Table 8.
C GPT Experiments
A summary of the additional experiments that have
been conducted to demonstrate the effectiveness
of our proposed method for the task of language
generation is provided in Table 9.

--- PAGE 13 ---
Model Parameter Value
RoBERTa-BaseOptimizer AdamW
Warmup Ratio 0.06
LR Scheduler Linear
Batch Size 32
Epochs 30
Learning Rate (LR) 4e-4
Weight Decay 0.1
LoRA Conﬁg rq=rv= 8(unless otherwise mentioned)
LoRA 16
Max Sequence Length 512
Seeds 10, 42, 4242, 10, 1010
GPU Tesla V100-PCIE-32GB
GPT MediumOptimizer AdamW
Adam Beta2 0.999
Warmup Steps 500
Clip 0.0
LR Scheduler Linear
Batch Size 8
Epochs 5
Learning Rate (LR) 2e-4
Weight Decay 0.01
Correct Bias True
LoRA Dropout 0.1
Lable Smooth 0.1
LoRA Conﬁg rq=rv= 4
LoRA 32
Seeds 10, 42, 4242
GPU Tesla V100-PCIE-32GB
Table 8: All the hyperparameters that have been used throughout our study.

--- PAGE 14 ---
Model (Method) Trainable Params DART WebNLG
BLEU" TER# BLEU" TER#
Rank=1
GPT-2 M (LoRA) 0.09M 0.71 0.49 2.80 1.18
GPT-2 M (DyLoRA-Frozen) 0.09M 44:480:110:490:0052:090:100:400:01
GPT-2 M (DyLoRA) 0.09M 44:770:170:490:0153:040:070:400:00
Rank=2
GPT-2 M (LoRA) 0.19M 15.90 0.48 26.58 0.67
GPT-2 M (DyLoRA-Frozen) 0.19M 45:040:140:480:0152:740:310:400:01
GPT-2 M (DyLoRA) 0.09M 46:050:310:480:0054:320:090:390:01
Rank=3
GPT-2 M (LoRA) 0.29M 35.84 0.47 43.61 0.47
GPT-2 M (DyLoRA-Frozen) 0.29M 45:220:140:490:0153:030:550:400:00
GPT-2 M (DyLoRA) 0.29M 46:680:360:480:0154:480:050:390:00
Rank=4
GPT-2 M (LoRA) 0.39M 47.10 0.46 55.57 0.39
GPT-2 M (DyLoRA-Frozen) 0.39M 45:560:330:480:0053:030:010:400:00
GPT-2 M (DyLoRA) 0.39M 46:560:420:480:0154:480:450:390:00
Fine-Tune
GPT-2 M (FT)354M 46.2 0.46
Table 9: Rows with * have been reported from the LoRA paper. (Hu et al., 2021a).
