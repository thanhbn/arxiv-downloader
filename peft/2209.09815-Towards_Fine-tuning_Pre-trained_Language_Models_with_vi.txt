# 2209.09815.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2209.09815.pdf
# Kích thước tệp: 1519080 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hướng tới Tinh chỉnh Mô hình Ngôn ngữ Tiền huấn luyện với
Lan truyền Tiến và Lan truyền Ngược Số nguyên
Mohammadreza Tayaranian¹ Alireza Ghaffari¹ Marzieh S. Tahaei¹
Mehdi Rezagholizadeh¹ Masoud Asgharian² Vahid Partovi Nia¹
¹Huawei Noah's Ark Lab, Trung tâm Nghiên cứu Montreal
²Khoa Toán học và Thống kê, Đại học McGill
{mohammadreza.tayaranian, alireza.ghaffari, marzieh.tahaei}@huawei.com
{mehdi.rezagholizadeh, vahid.partovinia}@huawei.com
masoud.asgharian2@mcgill.ca

Tóm tắt
Số lượng lớn tham số của một số mô hình ngôn ngữ nổi bật, chẳng hạn như BERT, khiến việc tinh chỉnh chúng trên các tác vụ hạ nguồn trở nên tốn nhiều tài nguyên tính toán và năng lượng. Trước đây, các nhà nghiên cứu tập trung vào các loại dữ liệu số nguyên có độ rộng bit thấp hơn cho việc lan truyền tiến của mô hình ngôn ngữ để tiết kiệm bộ nhớ và tính toán. Tuy nhiên, đối với lan truyền ngược, chỉ có loại dữ liệu số thực dấu phẩy động 16-bit được sử dụng cho việc tinh chỉnh BERT. Trong nghiên cứu này, chúng tôi sử dụng số học số nguyên cho cả lan truyền tiến và lan truyền ngược trong việc tinh chỉnh BERT. Chúng tôi nghiên cứu tác động của việc thay đổi độ rộng bit số nguyên đến hiệu suất metric của mô hình. Phương pháp tinh chỉnh số nguyên của chúng tôi sử dụng số học số nguyên để thực hiện lan truyền tiến và tính toán gradient của các lớp tuyến tính, layer-norm và embedding của BERT. Chúng tôi tinh chỉnh BERT bằng phương pháp huấn luyện số nguyên trên SQuAD v1.1 và SQuAD v2.0, và benchmark GLUE. Chúng tôi chứng minh rằng hiệu suất metric của việc tinh chỉnh BERT số nguyên 16-bit phù hợp với cả baseline số thực dấu phẩy động 16-bit và 32-bit. Hơn nữa, sử dụng loại dữ liệu số nguyên 8-bit nhanh hơn và tiết kiệm bộ nhớ hơn, việc tinh chỉnh số nguyên của BERT mất trung bình 3.1 điểm so với baseline FP32.

1 Giới thiệu
Trong vài năm qua, việc tích hợp cơ chế attention vào các mô hình học sâu đã dẫn đến việc tạo ra các mô hình dựa trên transformer. BERT (Devlin et al., 2018) là một mô hình ngôn ngữ dựa trên transformer nổi bật đã cho thấy hiệu suất tốt nhất trong các tác vụ xử lý ngôn ngữ tự nhiên (NLP).

BERT yêu cầu tài nguyên bộ nhớ và tính toán cao do có số lượng tham số lớn. Có số lượng tham số lớn tạo ra thách thức cho việc suy luận, huấn luyện và cũng như tinh chỉnh mô hình này. Hơn nữa, giai đoạn huấn luyện tức là tiền huấn luyện và tinh chỉnh, bao gồm nhiều phép toán hơn so với suy luận. Cụ thể hơn, giai đoạn huấn luyện bao gồm tính toán gradient và cập nhật trọng số khiến việc huấn luyện tốn nhiều tài nguyên tính toán hơn.

Một phương pháp giảm độ phức tạp tính toán của mô hình học sâu là biểu diễn các tham số và activation của chúng trong các loại dữ liệu có độ rộng bit thấp. Điều này giảm dung lượng bộ nhớ của mô hình và cho phép tính toán hiệu quả hơn. Ví dụ, Hình 1 cho thấy các loại dữ liệu số nguyên bit thấp có thông lượng cao hơn và tiêu thụ năng lượng tốt hơn so với số thực dấu phẩy động.

Các nỗ lực nghiên cứu trước đây về lượng tử hóa số nguyên của mô hình ngôn ngữ dựa trên transformer chỉ tập trung vào lan truyền tiến và việc tính toán gradient được giữ ở loại dữ liệu số thực dấu phẩy động 32-bit (FP32) (Bhandare et al., 2019; Kim et al., 2021; Zafrir et al., 2019).

Hơn nữa, những nỗ lực trước đây để sử dụng loại dữ liệu độ rộng bit thấp cho tính toán gradient của mô hình ngôn ngữ dựa trên transformer chỉ được giới hạn ở số thực dấu phẩy động 16-bit (FP16). Phương pháp này, được gọi là huấn luyện độ chính xác hỗn hợp (Micikevicius et al., 2017), sử dụng loại dữ liệu FP16 để biểu diễn trọng số, activation và gradient trong khi sử dụng FP32 cho việc cập nhật trọng số.

Hình 1: Tiêu thụ năng lượng và độ trễ của 1 tỷ phép toán sử dụng các loại dữ liệu khác nhau, đo trên Intel®Xeon®CPU E5-2698 v4.

--- TRANG 2 ---
Ở đây chúng tôi trình bày một phương pháp tinh chỉnh số nguyên cho mô hình ngôn ngữ dựa trên transformer như BERT. Khác với các nghiên cứu trước, chúng tôi sử dụng loại dữ liệu số nguyên cho cả lan truyền tiến và tính toán gradient trong quá trình tinh chỉnh BERT. Hơn nữa, chúng tôi sử dụng định dạng điểm cố định động để biểu diễn số thực dấu phẩy động dưới dạng số nguyên.

Chiến lược ánh xạ số nguyên của chúng tôi có thể được sử dụng cùng với số thực dấu phẩy động trong tinh chỉnh và suy luận tương tự như huấn luyện độ chính xác hỗn hợp. Trong chiến lược đề xuất của chúng tôi, số học của tất cả các lớp tính toán chuyên sâu cho cả lan truyền tiến và lan truyền ngược đều được thực hiện bằng số học số nguyên trong khi các thành phần khác của mô hình, như hàm phi tuyến và cập nhật trọng số được giữ ở FP32.

Chúng tôi sử dụng phiên bản số nguyên của các lớp tính toán chuyên sâu như lớp tuyến tính, chuẩn hóa (layer-norm) và embedding.

Chúng tôi nghiên cứu tác động của các độ rộng bit khác nhau của activation đầu vào số nguyên và cho thấy rằng việc tăng độ rộng bit của hàm ánh xạ điểm cố định cải thiện hành vi hội tụ của mô hình. Điều này cho phép chúng tôi tìm ra độ rộng bit tối thiểu cần thiết cho việc tinh chỉnh số nguyên của BERT.

Các thí nghiệm tinh chỉnh của chúng tôi cho thấy BERT số nguyên 16-bit có thể phù hợp với hiệu suất metric của các phương pháp độ chính xác hỗn hợp FP16 và FP32. Chúng tôi cũng tiếp tục giảm độ rộng bit và cho thấy rằng việc tinh chỉnh số nguyên của BERT với trọng số số nguyên 8-bit và activation số nguyên 12-bit có sự sụt giảm điểm số 3.1 so với hiệu suất gốc.

Tóm lại, bài báo này đưa ra những đóng góp sau:
• Tinh chỉnh số nguyên của mô hình ngôn ngữ dựa trên transformer sử dụng số học số nguyên cho cả lan truyền tiến và lan truyền ngược của các lớp tính toán chuyên sâu như tuyến tính, layer-norm và embedding. Theo hiểu biết của chúng tôi, đây là lần đầu tiên loại dữ liệu số nguyên được sử dụng cho lan truyền ngược của mô hình ngôn ngữ tiền huấn luyện.

• Phân tích tác động của việc thay đổi độ rộng bit của định dạng điểm cố định động đến sự hội tụ của tinh chỉnh. Nhận xét 3 thảo luận rằng hành vi hội tụ của phương pháp tinh chỉnh số nguyên của chúng tôi có liên quan trực tiếp đến phương sai của ánh xạ điểm cố định động và được kiểm soát bởi độ rộng bit.

• Chúng tôi cho thấy rằng việc tinh chỉnh BERT bằng số nguyên 16-bit có thể vượt trội hơn phương pháp tinh chỉnh độ chính xác hỗn hợp FP16.

Phần còn lại của bài báo được cấu trúc như sau. Phần 2 thảo luận ngắn gọn về các nghiên cứu trước đây sử dụng loại dữ liệu độ rộng bit thấp cho suy luận và huấn luyện mô hình học sâu. Phần 3 cung cấp chi tiết về phương pháp tinh chỉnh số nguyên của chúng tôi, bao gồm các hàm ánh xạ biểu diễn và các lớp chỉ sử dụng số nguyên. Hành vi hội tụ của ánh xạ điểm cố định động được nghiên cứu trong Phần 4 bằng cách cung cấp các quan sát thực nghiệm và phân tích lý thuyết. Các thí nghiệm tinh chỉnh trên các thiết lập số nguyên và số thực dấu phẩy động khác nhau được trình bày trong Phần 5. Cuối cùng, Phần 6 kết luận các ý tưởng được đề xuất trong nghiên cứu này.

2 Các Nghiên cứu Liên quan
Trong phần này, chúng tôi thảo luận về các nghiên cứu trước đây sử dụng loại dữ liệu độ rộng bit thấp trong mô hình ngôn ngữ dựa trên transformer. Các nghiên cứu này có thể được phân loại thành hai nhóm chính. Trong nhóm đầu tiên, được gọi là suy luận bit thấp, các loại dữ liệu độ rộng bit thấp chỉ được sử dụng trong giai đoạn lan truyền tiến để cải thiện độ phức tạp tính toán và giảm sử dụng bộ nhớ trong quá trình suy luận. Trong nhóm thứ hai, còn được gọi là huấn luyện bit thấp, các loại dữ liệu độ rộng bit thấp hơn được sử dụng cho cả giai đoạn lan truyền tiến và lan truyền ngược.

2.1 Suy luận Bit thấp
Nghiên cứu trước đây về suy luận bit thấp lượng tử hóa các tham số và activation của mô hình để tăng tốc lan truyền tiến. Danh mục này được chia thành các phương pháp huấn luyện nhận biết lượng tử hóa (QAT) và lượng tử hóa sau huấn luyện (PTQ).

Trong QAT, lượng tử hóa được thực hiện trong quá trình huấn luyện, cho phép các tham số mô hình thích ứng với nhiễu lượng tử hóa. QAT dựa vào gradient FP32 độ chính xác cao để huấn luyện mô hình và thích ứng với nhiễu lượng tử hóa.

Ví dụ, (Zafrir et al., 2019) đề xuất Q8BERT lượng tử hóa các tính toán suy luận của tất cả các lớp tuyến tính và embedding của BERT thành số nguyên 8-bit và cập nhật thang đo lượng tử hóa với trung bình trượt. Tương tự, (Shen et al., 2020) đề xuất Q-BERT yêu cầu tính toán ma trận hessian cho từng nhóm tham số để được sử dụng trong tinh chỉnh độ chính xác hỗn hợp với các độ rộng bit khác nhau. (Kim et al., 2021) đề xuất I-BERT sử dụng lược đồ lượng tử hóa đồng nhất để lượng tử hóa activation đầu vào và trọng số của các thành phần khác nhau của BERT. Trong I-BERT, các hệ số tỷ lệ lượng tử hóa được tính toán dựa trên phân phối của dữ liệu huấn luyện.

Khác với QAT thực hiện lượng tử hóa các phép toán suy luận trong quá trình huấn luyện, các phương pháp Lượng tử hóa Sau Huấn luyện (PTQ) áp dụng lượng tử hóa cho các tham số khi việc huấn luyện hoàn tất. Do đó, chúng yêu cầu hiệu chỉnh hoặc điều chỉnh tham số bổ sung để thích ứng mô hình với các tham số đã lượng tử hóa.

Ví dụ, (Bhandare et al., 2019) lượng tử hóa các phép nhân ma trận của kiến trúc transformer gốc từ (Vaswani et al., 2017) thành loại dữ liệu số nguyên 8-bit. Hơn nữa, việc lượng tử hóa chỉ được thực hiện cho lan truyền tiến và yêu cầu hiệu chỉnh bổ sung bằng dữ liệu validation để điều chỉnh các ranh giới của hàm lượng tử hóa. (Zadeh et al., 2020) giới thiệu GOBO nén các trọng số đã tinh chỉnh của BERT bằng cách nhóm chúng thành hai danh mục Gaussian và outlier. Các trọng số outlier được giữ ở FP32, trong khi các trọng số Gaussian được lượng tử hóa thành bit thấp hơn. Đối với các chế độ độ rộng bit thấp hơn, TernaryBERT và BinaryBERT có thể đẩy việc lượng tử hóa xuống còn 2 và 1 bit tương ứng (Zhang et al., 2020a; Bai et al., 2020). Cả hai đều dựa vào các phương pháp như tăng cường dữ liệu và chưng cất kiến thức để thích ứng mô hình với trọng số bit thấp.

2.2 Huấn luyện Bit thấp
Nghiên cứu về huấn luyện bit thấp cố gắng thực hiện cả lan truyền tiến và tính toán gradient trong số học bit thấp. Sử dụng định dạng số độ chính xác thấp cho gradient làm giảm khả năng thích ứng các tham số của mô hình với nhiễu lượng tử hóa, nhưng tăng thông lượng và giảm dung lượng bộ nhớ.

Huấn luyện độ chính xác hỗn hợp FP16 (Micikevicius et al., 2017) là một phương pháp phổ biến hiện tại cho việc tinh chỉnh bit thấp của mô hình ngôn ngữ dựa trên transformer. Phương pháp này sử dụng loại dữ liệu FP16 trong cả lan truyền tiến và tính toán gradient, trong khi sử dụng FP32 cho việc cập nhật trọng số. Khác với huấn luyện độ chính xác hỗn hợp FP16, nghiên cứu của chúng tôi sử dụng định dạng điểm cố định động cho phép nhiều lựa chọn độ rộng bit cho loại dữ liệu. Chúng tôi cho thấy rằng phương pháp tinh chỉnh số nguyên 16-bit của chúng tôi vượt trội hơn huấn luyện độ chính xác hỗn hợp FP16 về điểm metric.

Việc sử dụng loại dữ liệu số nguyên trong huấn luyện mô hình học sâu đã được nghiên cứu trước đây cho các tác vụ computer vision. Ví dụ, (Zhang et al., 2020b) lượng tử hóa activation đầu vào, gradient và tham số của các lớp tuyến tính cho các mạng nơ-ron tích chập (CNN) khác nhau. Tương tự, (Zhao et al., 2021) thích ứng các tham số lượng tử hóa bằng cách phát hiện phân phối của gradient trong chiều kênh. Trong cả hai nghiên cứu này, lỗi lượng tử hóa được đo trong quá trình huấn luyện và được sử dụng để điều chỉnh thang đo lượng tử hóa, trong khi phương pháp của chúng tôi không yêu cầu bất kỳ thông tin nào về phân phối dữ liệu hoặc gradient. (Zhu et al., 2020) áp dụng lược đồ lượng tử hóa để huấn luyện kiến trúc CNN với "cắt gradient nhạy hướng" và tỷ lệ tốc độ học để kiểm soát lỗi lượng tử hóa của gradient. Phương pháp tinh chỉnh số nguyên của chúng tôi không yêu cầu cắt gradient và có thể theo cùng quỹ đạo mất mát với baseline số thực dấu phẩy động với cùng siêu tham số. Phương pháp đề xuất của chúng tôi cải thiện so với (Ghaffari et al., 2022) sử dụng định dạng điểm cố định động cho huấn luyện số nguyên của mô hình học sâu. Khác với (Ghaffari et al., 2022), nghiên cứu của chúng tôi nghiên cứu các độ rộng bit khác nhau cho cả trọng số và activation để tìm ra độ rộng bit tối thiểu cần thiết cho việc tinh chỉnh BERT. Hơn nữa, chúng tôi nghiên cứu phương pháp huấn luyện số nguyên trên các mô hình ngôn ngữ lớn nơi mà lượng tử hóa bit thấp được biết đến là một nhiệm vụ thách thức (Bondarenko et al., 2021). Theo hiểu biết của chúng tôi, đây là lần đầu tiên số nguyên được sử dụng cho lan truyền ngược của mô hình ngôn ngữ dựa trên transformer.

3 Phương pháp
3.1 Ánh xạ Biểu diễn
Chúng tôi sử dụng định dạng điểm cố định động (Williamson, 1991) để ánh xạ các số thực dấu phẩy động thành loại dữ liệu số nguyên. Định dạng này, còn được gọi là điểm float khối, ánh xạ các số thực dấu phẩy động thành các khối số nguyên, với mỗi khối có thang đo riêng. Để biết thêm thông tin về các định dạng số khác nhau, tham khảo Phụ lục A.

Chúng tôi sử dụng hàm ánh xạ điểm cố định tuyến tính để ánh xạ các số thực dấu phẩy động thành số nguyên. Ánh xạ điểm cố định tuyến tính chuyển đổi một tensor số thực dấu phẩy động F thành một tensor số nguyên và một hệ số tỷ lệ đơn.

--- TRANG 3 ---
Ánh xạ Điểm cố định Tuyến tính Float Tham số
Activation Đầu vào Số nguyên
Activation Đầu vào
Thang đo
Activation Đầu vào Float
Tham số Số nguyên
Phép nhân Ma trận
Ánh xạ Ngược Phi tuyến
Đầu ra
Thang đo Tham số
Cộng
Giải nén
Tensor Float
Dấu
Mantissa
Mũ
Thang đo Tensor
Tensor Số nguyên b-bit Có dấu
Làm tròn
tối đa

Hình 2: Các phép toán lan truyền tiến trong lớp tuyến tính chỉ sử dụng số nguyên. Hộp xanh lá cây sử dụng số học số nguyên và hộp đỏ sử dụng loại dữ liệu số thực dấu phẩy động. Ở đây, đầu ra số nguyên được tạo ra bằng phép nhân ma trận số nguyên và thang đo đầu ra được tạo ra bằng một phép cộng đơn. Bảng dưới cùng cho thấy ánh xạ điểm cố định tuyến tính cho các tensor đầu vào, đó là activation đầu vào và tensor tham số trong hình này.

Các số nguyên được thu được bằng cách làm tròn mantissa của số thực dấu phẩy động. Thang đo là giá trị tối đa của các mũ số thực dấu phẩy động của F. Phần dưới của Hình 2 cho thấy các phép toán bên trong của ánh xạ điểm cố định tuyến tính.

Để ánh xạ các số điểm cố định thành số thực dấu phẩy động, một hàm ánh xạ ngược phi tuyến được sử dụng. Ánh xạ ngược chuyển đổi các số nguyên thành mantissa số thực dấu phẩy động được chuẩn hóa và đóng gói mỗi số nguyên với thang đo tương ứng thành một số thực dấu phẩy động.

Chi tiết về các hàm ánh xạ biểu diễn được cung cấp trong (Ghaffari et al., 2022). Phương pháp của chúng tôi khác ở chỗ nó bao gồm các độ rộng bit khác nhau cho cả trọng số và activation để tinh chỉnh mô hình ngôn ngữ dựa trên transformer. Chúng tôi khai thác chiến lược ánh xạ này để khám phá các độ rộng bit khác nhau cho trọng số và activation nhằm tìm ra độ rộng bit tối thiểu để tinh chỉnh mô hình.

3.2 Tinh chỉnh Số nguyên
Phương pháp của chúng tôi sử dụng số học số nguyên cho trọng số, activation và gradient, trong khi việc cập nhật trọng số được giữ ở FP32. Hơn nữa, các thiết lập BERT được đề xuất của chúng tôi sử dụng phiên bản chỉ sử dụng số nguyên cho tất cả các lớp tuyến tính, layer-norm và embedding trong đó các phép toán bên trong được thực hiện bằng số học số nguyên.

3.2.1 Lớp Tuyến tính
Hình 2 mô tả cái nhìn tổng quan về các phép toán lan truyền tiến của lớp tuyến tính chỉ sử dụng số nguyên. Tất cả các tham số và activation của lớp đều được ánh xạ đầu tiên thành điểm cố định động bằng hàm ánh xạ điểm cố định tuyến tính. Trong trường hợp lớp tuyến tính, các tham số số nguyên và activation đầu vào sau đó được gửi đến hàm nhân ma trận số nguyên để tạo ra đầu ra số nguyên. Nếu cần, đầu ra số nguyên có thể được ánh xạ ngược lại thành số thực dấu phẩy động để được sử dụng bởi các lớp khác của mô hình bằng ánh xạ ngược phi tuyến.

Đối với lan truyền ngược, gradient của các tham số và activation đầu vào cũng được tính toán bằng số học số nguyên. Sử dụng phép nhân ma trận số nguyên, gradient đầu ra được nhân với activation đầu vào và tham số để tính toán gradient. Vì việc cập nhật trọng số được thực hiện ở FP32, gradient số nguyên và thang đo của chúng được chuyển qua ánh xạ ngược phi tuyến để được ánh xạ thành FP32.

3.2.2 Layer-norm
Chuẩn hóa lớp hoặc layer-norm thực hiện phép toán sau trên đầu vào X (Ba et al., 2016):

X√(σ² + ε) + β. (1)

Ở đây γ và β là các tham số trọng số và bias, σ và μ là độ lệch chuẩn và trung bình đầu vào tương ứng. Đối với lan truyền tiến của layer-norm số nguyên, chúng tôi ánh xạ X thành định dạng điểm cố định động và tính toán σ và μ bằng số học số nguyên. Lưu ý rằng phép nhân với γ và phép cộng với β cũng được thực hiện bằng số học số nguyên.

--- TRANG 4 ---
Hình 3: Điểm F1 của việc tinh chỉnh BERT sử dụng gradient và activation b-bit trên tập dữ liệu SQuAD v2.0. Đối với độ rộng bit điểm cố định 8-bit và 9-bit, chúng tôi sử dụng activation đầu vào 12-bit.

Hơn nữa, lan truyền ngược cũng sử dụng số học số nguyên để tính toán gradient cho đầu vào, γ và β.

3.2.3 Lớp Embedding
Lớp embedding là một bảng tra cứu lưu trữ các embedding. Lớp nhận một danh sách các chỉ số làm đầu vào và trả về danh sách các embedding tương ứng cho mỗi chỉ số. Lớp embedding số nguyên xử lý các embedding số nguyên và cần ít dung lượng bộ nhớ hơn để lưu trữ các giá trị này. Đối với lan truyền ngược, lớp embedding áp dụng gradient số nguyên đầu ra trực tiếp vào mỗi hàng tương ứng của bảng tra cứu.

4 Hành vi Hội tụ của Ánh xạ Điểm Cố định Động
4.1 Quan sát Thực nghiệm
Hình 2 cho thấy độ rộng bit, b, được kiểm soát bằng cách điều chỉnh số bit được làm tròn trong hàm làm tròn. Ở đây chúng tôi nghiên cứu tác động của việc thay đổi độ rộng bit số nguyên đến hiệu suất metric của mô hình.

Động cơ của việc thay đổi độ rộng bit của điểm cố định động là để kiểm soát phương sai được gây ra bởi ánh xạ điểm cố định tuyến tính. Các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng điểm cố định động với độ rộng bit 10 đạt được cùng hiệu suất với phương pháp tinh chỉnh FP32. Hình 3 minh họa điểm F1 của việc tinh chỉnh BERT trên tập dữ liệu SQuAD v2.0 so với độ rộng bit điểm cố định. Lưu ý rằng số học điểm cố định với độ rộng bit cao hơn 10 bit có thể phù hợp chặt chẽ với điểm F1 của baseline FP32, được chỉ ra bởi đường màu đỏ trong hình. Cũng lưu ý rằng trong thiết lập thí nghiệm của chúng tôi cho định dạng điểm cố định động 8-bit, chúng tôi sử dụng activation đầu vào 12-bit để thu hẹp khoảng cách điểm F1 với baseline FP32. Lý do sử dụng activation đầu vào độ rộng bit cao hơn là chúng tôi quan sát thấy activation 8-bit làm giảm đáng kể điểm F1. Hình 4 cho thấy tác động của độ rộng bit activation đầu vào đến điểm F1 khi trọng số là số nguyên 8-bit. Thay đổi độ rộng bit của activation đầu vào từ 8 bit thành 12 bit làm tăng đáng kể điểm F1. Tăng độ rộng bit activation đầu vào vượt quá 12 bit có tác động không đáng kể đến điểm F1, xác nhận rằng 12 bit là độ rộng bit tối thiểu cần thiết của activation đầu vào cho ứng dụng này với trọng số số nguyên 8-bit.

Hình 4: Điểm F1 của việc tinh chỉnh BERT sử dụng trọng số và gradient 8-bit, với độ rộng bit activation đầu vào thay đổi trên tập dữ liệu SQuAD v2.0. Lưu ý rằng Nhận xét 3 biện minh cho thí nghiệm này bằng cách sử dụng phương sai của ánh xạ điểm cố định động b-bit.

4.2 Phân tích Lý thuyết
Ở đây, chúng tôi nghiên cứu tác động của việc thay đổi độ rộng bit ánh xạ điểm cố định động đến phương pháp gradient descent ngẫu nhiên. Mục tiêu là chỉ ra mối quan hệ của độ rộng bit trọng số và activation đến sự hội tụ của huấn luyện số nguyên. Hãy xem xét phương trình cập nhật trọng số đơn giản hóa sau:

w_k+1 = w_k - α ĝ(w_k; ξ_k); (2)

trong đó ĝ(w_k; ξ_k) là gradient điểm cố định động và α là tốc độ học trong giai đoạn tinh chỉnh. Hơn nữa, chúng tôi cũng xem xét các giả định phổ biến sau trong phần tiếp theo.

--- TRANG 5 ---
Giả định 1 (Tính liên tục Lipschitz). Hàm mất mát L(w) có thể vi phân liên tục và gradient của nó thỏa mãn bất đẳng thức sau với L > 0 là hằng số Lipschitz:

L(w) ≤ L(w̄) + ∇L(w̄)^T(w - w̄) + (1/2)L||w - w̄||₂²; ∀w, w̄ ∈ ℝᵈ: (3)

Giả định 2. (i) L(w_k) bị chặn. (ii) Gradient điểm cố định động b-bit ĝ(w_k; ξ_k) là một ước lượng không thiên lệch của gradient thực của hàm mất mát ∇L(w_k)^T E_k{ĝ(w_k; ξ_k)} = ||∇L(w_k)||₂² = ||E_k{ĝ(w_k; ξ_k)}||₂²; và (iii) với gradient điểm cố định động b-bit tức là ĝ(w_k; ξ_k), tồn tại các vô hướng M > 0, M_V > 0, M_q > 0 và M_q^V > 0 sao cho đối với tất cả các lần lặp của SGD:

V_k{ĝ(w_k; ξ_k)} ≤ M + M_q + (M_V + M_q^V)||∇L(w_k)||₂²:

Trong đó M_q và M_q^V biểu thị phương sai thêm vào của ánh xạ điểm cố định động b-bit trên phương sai gradient thực. Cũng lưu ý rằng để Giả định 2 (i) đúng, chúng tôi sử dụng làm tròn ngẫu nhiên cho lan truyền ngược.

Giả sử Giả định 1 và Giả định 2 đúng, thì bất đẳng thức (4) theo sau từ (Ghaffari et al., 2022, Nhận xét 2):

E_k{L(w_k+1)} - L(w_k) ≤ -α(1 - (1/2)αL(M_G + M_q^G))||∇L(w_k)||₂² + (1/2)α²L(M + M_q);

với M_G := 1 + M_V và M_q^G := 1 + M_q^V; (4)

điều này cho thấy tác động của phương sai thêm vào của ánh xạ điểm cố định, tức là M_q^V và M_q, đến từng bước của bộ tối ưu hóa.

Nhận xét 1. Trong bất đẳng thức (4), số hạng đầu tiên, -α(1 - (1/2)αL(M_G + M_q^G))||∇L(w_k)||₂² góp phần giảm mất mát L trong khi số hạng thứ hai, (1/2)α²L(M + M_q), ngăn cản điều đó. Cũng lưu ý rằng khi M_q và M_q^G tăng, chúng ảnh hưởng tiêu cực đến việc giảm mất mát L. Điều này có nghĩa là để có hành vi hội tụ tốt, các cận phương sai ánh xạ biểu diễn, tức là M_q và M_q^G, phải được kiểm soát.

Nhận xét 2. Đối với ánh xạ điểm cố định động với số nguyên b-bit, các cận phương sai ánh xạ biểu diễn tức là M_q và M_q^G, có liên quan chặt chẽ đến độ rộng bit b. Ở đây, chúng tôi nghiên cứu hai hằng số này cho một lớp tuyến tính. Hãy ký hiệu Â là phiên bản điểm cố định động b-bit của tensor A và â_ij là phần tử thứ ij của nó. Chúng ta có thể liên hệ â_ij và a_ij với một số hạng lỗi ε như â_ij = a_ij + ε_ij^A. Đối với một lớp tuyến tính Ŷ = X̂Ŵ, việc tính toán gradient điểm cố định động b-bit trong lan truyền ngược là:

Ĉ = ∂L̂/∂Ŵ = ∂Ŷ/∂Ŵ ∂L̂/∂Ŷ = X̂^T ∂L̂/∂Ŷ = X̂^T Ĝ: (5)

Cần quan tâm đến việc tìm mối quan hệ giữa Ĉ = X̂^T Ĝ trong lan truyền ngược số nguyên và gradient thực C = X^T G. Chúng ta có thể rút ra phương sai cho mỗi phần tử ĉ_ij bằng cách khai triển các số hạng lỗi ε,

V{ĉ_ij} = V(∑_{n=1}^N x̂_ni ĝ_nj) = V(∑_{n=1}^N (x_ni + ε_ni^X)(g_nj + ε_nj^G)) ≤ V(∑_{n=1}^N x_ni g_nj) + 2σ_G² E{||X_{i:}^T||₂²} + 2σ_X² E{||G_{:j}||₂²} + Nσ_X² σ_G² = V{c_ij} + 2σ_G² E{||X_{i:}^T||₂²} + 2σ_X² E{||G_{:j}||₂²} + Nσ_X² σ_G²: (6)

Trong bất đẳng thức (6), σ_G² = max_{i,j}(V{ε_{i,j}^G}) và σ_X² = max_{i,j}(V{ε_{i,j}^X}). Cũng lưu ý ||X_{i:}^T||₂² = ∑_j x_{ji}² biểu thị chuẩn L-2 bình phương của hàng thứ i của X^T và ||G_{:j}||₂² = ∑_i g_{ij}² biểu thị chuẩn L-2 bình phương của cột thứ j của G. Hơn nữa, bằng cách định nghĩa:

{M_q := 2σ_G²(E{||X_{i:}^T||₂²} + Nσ_X²)
{M_q^V := 2σ_X²                                 (7)

Phương trình (7) cho thấy rằng M_q phụ thuộc vào phương sai của ánh xạ điểm cố định động cho activation đầu vào và gradient trong khi M_q^G chỉ phụ thuộc vào phương sai gradient điểm cố định động b-bit.

--- TRANG 6 ---
[THIS IS TABLE: Bảng 1 showing performance metrics for integer fine-tuning of BERT on selected GLUE tasks, with columns for different tasks and rows for different bit configurations]

Bảng 1: Hiệu suất metric của việc tinh chỉnh số nguyên BERT trên các tác vụ GLUE được chọn. Metric báo cáo cho QQP và MRPC là độ chính xác và điểm F1, cho QNLI, MNLI, RTE và SST-2 là độ chính xác, cho STSB là tương quan Pearson-Spearman, và cho CoLA là tương quan Matthews.

Mệnh đề 1. Đối với biểu diễn điểm cố định động của tensor Â với số nguyên b-bit, phương sai của lỗi cho phần tử ε_i^A thỏa mãn bất đẳng thức sau:

V{ε_i^A} ≤ 2^{2(scale_A - b + 2)}: (8)

Chứng minh. Sử dụng ánh xạ điểm cố định động thành số nguyên b-bit, lỗi ε_i^A thỏa mãn cận sau:

-2^{scale_A}(0.00001)_2 ≤ ε_i^A ≤ 2^{scale_A}(0.00001)_2

-2^{scale_A - b + 2} ≤ ε_i^A ≤ 2^{scale_A - b + 2}: (9)

Do đó, bất đẳng thức (8) được thu được bằng cách sử dụng bất đẳng thức Popoviciu về phương sai:

V{ε_i^A} ≤ (1/4)(2^{scale_A - b + 2} - (-2^{scale_A - b + 2}))² = 2^{2(scale_A - b + 2)}: (10)

Nhận xét 3. Bất đẳng thức (8) cho thấy rằng việc tăng độ rộng bit b trong ánh xạ điểm cố định động làm giảm phương sai của lỗi. Điều này xác nhận kết quả thí nghiệm của chúng tôi trên tập dữ liệu SQuAD v2.0 rằng với b > 10, điểm F1 có thể phù hợp với baseline FP32, xem Hình 3. Cũng lưu ý trong phương trình (7), cả M_q và M_q^V đều phụ thuộc vào phương sai ánh xạ điểm cố định động b-bit của activation đầu vào σ_X². Do đó, việc tăng b cho activation đầu vào trong khi giữ trọng số ở định dạng 8-bit phải cải thiện hành vi hội tụ. Hiện tượng này cũng được xác nhận bởi kết quả thí nghiệm của chúng tôi trên tập dữ liệu SQuAD v2.0 được minh họa trong Hình 4.

5 Kết quả Thí nghiệm
5.1 Thiết lập Thí nghiệm
Chúng tôi tinh chỉnh BERT base trên một loạt tác vụ hạ nguồn để so sánh hiệu suất của phương pháp tinh chỉnh số nguyên với các phương pháp tinh chỉnh FP16 và FP32. Thiết lập FP16 AMP sử dụng automatic mixed precision của NVIDIA và baseline FP32 là triển khai mặc định từ Pytorch.

Mô hình được tinh chỉnh trên các tác vụ được chọn của benchmark GLUE (Wang et al., 2018), cùng với Stanford Question Answering Datasets, tức là SQuAD v1.1 và SQuAD v2.0 (Rajpurkar et al., 2016).

Tất cả các thiết lập tinh chỉnh đều sử dụng cùng siêu tham số và được tinh chỉnh trong cùng số epoch. Mỗi metric báo cáo là trung bình của năm lần chạy với năm seed ngẫu nhiên khác nhau để giảm thiểu tác động của biến thiên ngẫu nhiên của kết quả. Các thí nghiệm tinh chỉnh được thực hiện dựa trên các script tinh chỉnh của thư viện Hugging Face (Wolf et al., 2019). Đối với các thí nghiệm GLUE, việc tinh chỉnh được thực hiện trong 5 epoch và tốc độ học được đặt thành 2×10⁻⁵. Ngoài ra, kích thước batch tinh chỉnh trên mỗi thiết bị được đặt thành 32. Tinh chỉnh BERT trên tập dữ liệu SQuAD được thực hiện trong 2 epoch và tốc độ học là 5×10⁻⁵ và kích thước batch tinh chỉnh trên mỗi thiết bị là 12. Tất cả các thí nghiệm được chạy trên tám GPU NVIDIA V100 với 32 gigabyte VRAM.

[THIS IS TABLE: Bảng 2 showing metric performance for fine-tuning BERT on SQuAD v1.1 and v2.0 datasets]

Bảng 2: Hiệu suất metric của việc tinh chỉnh BERT trên tập dữ liệu SQuAD v1.1 và v2.0. Đối với cả hai tập dữ liệu, metric khớp chính xác và điểm F1 được báo cáo.

5.2 Kết quả
Kết quả tinh chỉnh BERT base trên benchmark GLUE và tập dữ liệu SQuAD được trình bày trong Bảng 1 và Bảng 2 tương ứng. Benchmark GLUE chứa một loạt tác vụ hạ nguồn, được thiết kế để đánh giá một tập hợp đa dạng các khả năng hiểu ngôn ngữ của mô hình NLP. Tập dữ liệu SQuAD chứa một loạt đoạn văn bản kèm theo câu hỏi và nhiệm vụ là dự đoán khoảng của câu trả lời trong đoạn văn. Sử dụng loại dữ liệu số nguyên 16-bit, BERT có thể phù hợp hoặc vượt trội so với hiệu suất FP32 đối với tất cả các tác vụ. BERT số nguyên 16-bit cũng cho thấy hiệu suất tương tự hoặc tốt hơn so với phương pháp tinh chỉnh độ chính xác hỗn hợp FP16. Tiếp tục giảm độ rộng bit số nguyên xuống 8, việc tinh chỉnh BERT thể hiện mức giảm trung bình 1.7 điểm trên benchmark GLUE và 4.5 điểm cho tập dữ liệu SQuAD. Hơn nữa, các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng số nguyên 10-bit và 12-bit có mức giảm điểm trung bình 0.8 và 0.3 điểm cho các tác vụ GLUE, và 0.8 và 0.2 điểm cho tập dữ liệu SQuAD tương ứng.

5.3 Quỹ đạo Mất mát
Hình 5 cho thấy quỹ đạo mất mát của việc tinh chỉnh số nguyên BERT trên tập dữ liệu SQuAD v2.0 sử dụng số nguyên 16-bit và 8-bit, cùng với phương pháp FP32. Quỹ đạo mất mát tinh chỉnh của BERT sử dụng số nguyên 16-bit theo sát quỹ đạo mất mát FP32. Mặt khác, khi tinh chỉnh với tham số số nguyên 8-bit và activation đầu vào số nguyên 12-bit, quỹ đạo mất mát bị dịch chuyển nhẹ, nhưng vẫn theo cùng xu hướng với đối tác FP32.

--- TRANG 7 ---
Hình 5: Quỹ đạo mất mát tinh chỉnh số nguyên của BERT trên tập dữ liệu SQuAD v2.0 trong 2750 lần lặp.

6 Kết luận
Chúng tôi đã đề xuất một phương pháp tinh chỉnh số nguyên cho mô hình ngôn ngữ dựa trên transformer sử dụng định dạng điểm cố định động. Chúng tôi đã sử dụng loại dữ liệu điểm cố định động để biểu diễn tham số, activation đầu vào và gradient thành các giá trị số nguyên. Kết quả là, phương pháp tinh chỉnh của chúng tôi sử dụng số học số nguyên cho lan truyền tiến và lan truyền ngược của các lớp tính toán chuyên sâu như lớp tuyến tính, layer-norm và embedding của mô hình BERT. Hơn nữa, chúng tôi đã nghiên cứu rằng việc tăng độ rộng bit của định dạng điểm cố định động làm giảm phương sai của hàm ánh xạ và do đó, cải thiện sự hội tụ của phương pháp tinh chỉnh số nguyên. Chúng tôi tiến hành các thí nghiệm tinh chỉnh trên benchmark GLUE và tập dữ liệu SQuAD để so sánh hiệu suất metric của BERT số nguyên với các phương pháp tinh chỉnh độ chính xác hỗn hợp FP16 và FP32. Các thí nghiệm của chúng tôi cho thấy rằng việc tinh chỉnh số nguyên 16-bit có thể đạt được cùng hiệu suất metric với phương pháp tinh chỉnh độ chính xác hỗn hợp FP16. Ngoài ra, việc tinh chỉnh BERT với loại dữ liệu độ rộng bit thấp hơn, tức là số nguyên 8-bit, duy trì mức giảm trung bình của điểm metric trong phạm vi 3.1 điểm so với thiết lập FP16.

Hạn chế
Mặc dù phương pháp tinh chỉnh số nguyên của chúng tôi sử dụng số nguyên cho các lớp tính toán chuyên sâu của BERT, hỗ trợ số nguyên cho các lớp phi tuyến của BERT, ví dụ như activation softmax và GELU, được để lại cho công việc tương lai.

Chúng tôi đã cho thấy trong Hình 1 rằng các loại dữ liệu số nguyên nhanh hơn cho trường hợp chung. Tuy nhiên, một so sánh trực tiếp về thời gian và chi phí bộ nhớ của phương pháp tinh chỉnh số nguyên với các phương pháp FP16 và FP32 được để lại cho các nghiên cứu tương lai do thiếu quyền truy cập vào phần cứng thích hợp với hỗ trợ tensor core số nguyên.

Mặc dù có những điểm tương đồng giữa giai đoạn tinh chỉnh và tiền huấn luyện, chúng khác nhau ở các khía cạnh chính của việc huấn luyện như kích thước tập dữ liệu và số epoch. Những thách thức của việc sử dụng số học số nguyên trong giai đoạn tiền huấn luyện sẽ được nghiên cứu trong công việc tương lai.

Tài liệu tham khảo
Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.

--- TRANG 8 ---
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, và Irwin King. 2020. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.

Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun Choi, Kushal Datta, và Vikram Saletore. 2019. Efﬁcient 8-bit quantization of transformer neural machine language translation model. arXiv preprint arXiv:1906.00532.

Yelysei Bondarenko, Markus Nagel, và Tijmen Blankevoort. 2021. Understanding and overcoming the challenges of efﬁcient transformer quantization. arXiv preprint arXiv:2109.12948.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza Tayaranian, Masoud Asgharian, và Vahid Partovi Nia. 2022. Is integer arithmetic enough for deep learning training? arXiv preprint arXiv:2207.08822.

Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, và Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506–5518. PMLR.

Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. arXiv preprint arXiv:1710.03740.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, và Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 8815–8821.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.

Darrell Williamson. 1991. Dynamically scaled ﬁxed point arithmetic. In [1991] IEEE Paciﬁc Rim Conference on Communications, Computers and Signal Processing Conference Proceedings, pages 315–318. IEEE.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.

Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, và Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy efﬁcient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811–824. IEEE.

Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, và Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36–39. IEEE.

Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, và Qun Liu. 2020a. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.

Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, et al. 2020b. Fixed-point back-propagation training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2330–2338.

Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, và Yinghui Xu. 2021. Distribution adaptive int8 quantization for training cnns. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 3483–3491.

Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, và Junjie Yan. 2020. Towards uniﬁed int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1969–1979.

A Loại Dữ liệu
Trong phần này, chúng tôi cung cấp một cái nhìn tổng quan ngắn gọn về các loại dữ liệu khác nhau được đề cập trong nghiên cứu này.

Loại dữ liệu số thực dấu phẩy động được sử dụng để biểu diễn các số phân số thập phân. Một số thực dấu phẩy động nhị phân có ba thành phần: dấu (s), mantissa (m), và số mũ (e). Sử dụng các thành phần này, số thực dấu phẩy động x được biểu diễn như:

x = (-1)^s × m × 2^(e-t)

trong đó t là độ chính xác và 0 ≤ m < 2^t - 1. Một cách khác để biểu diễn số thực dấu phẩy động là:

x = (-1)^s × 2^e × (d₁/2 + d₂/4 + ... + dₜ/2^t)

--- TRANG 9 ---
trong đó dᵢ là các chữ số nhị phân của m. Đối với FP32, số mũ và mantissa là các số nguyên 8 và 23 bit.

Điểm cố định là một loại dữ liệu khác để biểu diễn các số phân số. Khác với số thực dấu phẩy động trong đó mỗi mantissa được tỷ lệ hóa bằng số mũ tương ứng, điểm cố định sử dụng một hệ số tỷ lệ đơn cho tất cả các số.

Chúng tôi sử dụng loại dữ liệu điểm cố định động trong phương pháp tinh chỉnh số nguyên. Còn được gọi là điểm float khối, định dạng này sử dụng một thang đo khác nhau cho mỗi khối số để cho phép tính linh hoạt hơn.
