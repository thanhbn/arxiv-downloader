# Các Adapter Rank Thấp Bất Khả Tri Tác Vụ cho Các Phương Ngữ Tiếng Anh Chưa Được Nhìn Thấy

Zedian Xiao
William Held
Yanchen Liu
Diyi Yang
Đại học Stanford
Viện Công nghệ Georgia
Đại học Harvard
markxiao@stanford.edu, wheld3@gatech.edu, yanchenliu@g.harvard.edu,
diyiy@cs.stanford.edu

## Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) được huấn luyện trên các kho dữ liệu có trọng số không tỷ lệ nghiêng về Tiếng Anh Mỹ Chuẩn. Kết quả là, những người nói các phương ngữ khác gặp phải nhiều thất bại hơn đáng kể khi tương tác với các công nghệ này. Trong thực tế, những người nói này thường điều chỉnh lời nói của họ để được hiểu tốt hơn. Công việc của chúng tôi chia sẻ niềm tin rằng các công nghệ ngôn ngữ nên được thiết kế để thích ứng với sự đa dạng trong các phương ngữ tiếng Anh chứ không phải ngược lại. Tuy nhiên, các nghiên cứu trước về phương ngữ gặp khó khăn trong việc tổng quát hóa cho các phương ngữ đang phát triển và mới nổi một cách có thể mở rộng. Để lấp đầy khoảng trống này, phương pháp của chúng tôi, HyperLoRA, tận dụng kiến thức ngôn ngữ học chuyên môn để cho phép thích ứng hiệu quả tài nguyên thông qua các siêu mạng. Bằng cách tách rời thông tin đặc trung phương ngữ và thông tin liên phương ngữ, HyperLoRA cải thiện khả năng tổng quát hóa cho các phương ngữ chưa được nhìn thấy theo cách bất khả tri tác vụ. Không chỉ HyperLoRA có khả năng mở rộng hơn về số lượng tham số, mà nó còn đạt được hiệu suất tốt nhất hoặc cạnh tranh nhất trên 5 phương ngữ trong setting zero-shot. Theo cách này, cách tiếp cận của chúng tôi tạo điều kiện tiếp cận công nghệ ngôn ngữ cho hàng tỷ người nói phương ngữ tiếng Anh vốn bị thiểu số hóa truyền thống.

## 1 Giới thiệu
Sự đa dạng phương ngữ xuất phát từ những khác biệt về chủng tộc, văn hóa, tôn giáo, dân tộc, khu vực, kinh tế-xã hội và độ tuổi. Xem xét việc tích hợp ngày càng rộng rãi của các LLM (Dai et al., 2019; Liu et al., 2019; Raffel et al., 2020) vào các công cụ hàng ngày, những LLM này nên được làm cho bất biến với các khác biệt phương ngữ. Điều này chưa xảy ra, thực tế, một khoảng cách đáng kể trong hiệu suất của các LLM được quan sát khi chúng được áp dụng cho các phương ngữ tiếng Anh có khoảng cách ngôn ngữ học xa với Tiếng Anh Mỹ Chuẩn (SAE) (Jurgens et al., 2017; Blodgett et al., 2018; Kiritchenko và Mohammad, 2018; Ziems et al., 2023b). Những bất đồng này làm nảy sinh các mối quan ngại về chủng tộc, dân tộc và kinh tế-xã hội cho các nhóm bị thiểu số hóa (Gururangan et al., 2022) trong kho dữ liệu huấn luyện của các công nghệ này (Hovy và Spruit, 2016; Blodgett và O'Connor, 2017; Halevy et al., 2021a). Việc hiểu và giảm thiểu những bất đồng này đặc biệt quan trọng trong việc tránh các hậu quả có hại và không mong muốn, có thể dao động từ việc từ chối chăm sóc trong các hệ thống chăm sóc sức khỏe thương mại (Obermeyer et al., 2019) đến thành kiến chủng tộc trong phát hiện ngôn từ thù địch (Davidson et al., 2019; Sap et al., 2019; Rios, 2020; Halevy et al., 2021b; Zhou et al., 2021).

Trước đây, các phương pháp mạnh mẽ phương ngữ chủ yếu tập trung vào việc lấp đầy sự thiếu hụt dữ liệu phương ngữ thông qua giám sát thủ công (Blevins et al., 2016; Blodgett et al., 2018) và các hình thức giám sát yếu (Jørgensen et al., 2016; Jurgens et al., 2017), hoặc gần đây hơn thông qua tăng cường dữ liệu tổng hợp (Multi-VALUE; Ziems et al., 2022, 2023b). Một hạn chế chung của các phương pháp này là giả định của chúng về việc có sẵn dữ liệu phương ngữ cho tất cả các tác vụ downstream. Trong thực tế, điều này không thực tế, vì việc tìm kiếm những người chú thích trong tất cả các phương ngữ đã là một thách thức (Ziems et al., 2023b). Nghiên cứu gần đây đã bắt đầu giảm gánh nặng về dữ liệu phương ngữ cụ thể cho tác vụ, chẳng hạn như bằng cách huấn luyện các adapter bất khả tri tác vụ thông qua căn chỉnh liên phương ngữ (Held et al., 2023). Trong khi các phương ngữ mới đang nổi lên và các phương ngữ hiện có đang phát triển, nhu cầu về dữ liệu trong tất cả các phương ngữ vẫn còn, cũng như các phương pháp thích ứng hiệu quả tài nguyên và bất khả tri tác vụ.

Để đạt được mục tiêu này, chúng tôi đề xuất HyperLoRA, một phương pháp thích ứng hiệu quả cho các phương ngữ mới mà không cần thêm chú thích phương ngữ. Trong việc loại bỏ sự phụ thuộc vào dữ liệu phương ngữ này, chúng tôi chuyển sang kiến thức chuyên môn hiện có về phương ngữ. Bird (2022) tuyên bố rằng chúng ta không cần phải thu hẹp khoảng cách dữ liệu trong các setting nơi kiến thức chuyên môn có sẵn. Giả định này về việc có quyền truy cập vào kiến thức chuyên môn là hợp lý vì chi phí để một chuyên gia duy nhất xác định phương ngữ của người nói thấp hơn nhiều so với việc thuê những người chú thích từ tất cả các phương ngữ. Trước đây, việc sử dụng các đặc trưng loại hình học đã thành công trong việc loại bỏ khoảng cách này trong setting đa ngôn ngữ (Ansell et al., 2021). Lấy cảm hứng từ điều này, nghiên cứu của chúng tôi điều tra liệu kiến thức chuyên môn và các đặc trưng loại hình học này có thể được tận dụng cho các phương ngữ hay không.

Một giải pháp tự nhiên trong việc tận dụng kiến thức chuyên môn này là thông qua các siêu mạng (Ha et al., 2016), đã thể hiện khả năng tổng quát hóa đáng chú ý trong thị giác máy tính và NLP (Knyazev et al., 2021; Üstün et al., 2022). Sử dụng một siêu mạng, chúng tôi điều chỉnh các adapter LoRA đặc trưng phương ngữ (Hu et al., 2021) sử dụng các đặc trưng loại hình học để thích ứng với các phương ngữ mục tiêu. Bằng cách cô lập độ phức tạp của không gian loại hình học cho siêu mạng và bằng cách tạo ra các adapter LoRA đặc trưng phương ngữ, chúng tôi giảm thiểu sự can thiệp liên phương ngữ (Wang et al., 2020) trong mô hình chính. Siêu mạng được huấn luyện trên các kho dữ liệu song song để tối ưu hóa một mục tiêu căn chỉnh hình thái cú pháp trong không gian biểu diễn, cho phép HyperLoRA học cách thích ứng với các phương ngữ độc lập với ứng dụng downstream. Mục tiêu căn chỉnh này là mới, có nguyên tắc và dễ tính toán. Quan trọng nhất, chúng tôi thấy rằng việc sử dụng hiệu quả kiến thức chuyên môn có thể tương đương với 250 chú thích mỗi phương ngữ. Cuối cùng, chúng tôi thiết kế một thước đo để đánh giá độ bao phủ của các đặc trưng phương ngữ, nhằm hiểu rõ hơn các hạn chế của việc sử dụng siêu mạng cho tổng quát hóa zero-shot cho các phương ngữ.

## 2 Nghiên cứu Liên quan

**NLP Phương ngữ** Khi áp dụng cho các phương ngữ tiếng Anh khác, các mô hình ngôn ngữ hiện có chủ yếu tập trung vào Tiếng Anh Mỹ Chuẩn (SAE) thường thể hiện hiệu suất thấp hơn đáng kể (Sap et al., 2019; Rios, 2020; Halevy et al., 2021b; Zhou et al., 2021). Nghiên cứu trước đây đã tiết lộ rằng việc prompting các LLM có thể làm giảm thêm hiệu suất trên các phương ngữ này (Ziems et al., 2023a; Liu et al., 2023). Những bất đồng này có thể củng cố thêm sự mất cân bằng quyền lực hiện có (Hovy và Spruit, 2016; Bommasani et al., 2021) và mang lại tác hại phân bổ cho các cộng đồng chủng tộc, dân tộc và kinh tế-xã hội cụ thể. Đây chính là lý do tại sao việc phát triển các phương pháp mạnh mẽ phương ngữ hiện tại có tầm quan trọng hàng đầu.

**Học Chuyển giao** Học Chuyển giao đã trở thành mô hình chiếm ưu thế trong việc chuyên biệt hóa các mô hình cho ngôn ngữ và tác vụ mục tiêu. Để đạt được hiệu quả này, nhiều mô-đun tinh chỉnh hiệu quả tham số (PEFT) (Hu et al., 2021; Houlsby et al., 2019; Zaken et al., 2022) đã được thiết kế để thích ứng hiệu quả các Mô hình Ngôn ngữ được huấn luyện trước Lớn cho các ứng dụng downstream (Pfeiffer et al., 2023). MAD-X (Pfeiffer et al., 2020b) cho thấy rằng các adapter tác vụ và ngôn ngữ riêng biệt có thể được kết hợp để đạt được chuyển giao đa tác vụ liên ngôn ngữ. Giống như MAD-X, TADA (Held et al., 2023) huấn luyện các adapter đặc trưng phương ngữ riêng biệt với các adapter tác vụ, cho phép thích ứng mô hình được huấn luyện SAE cho các phương ngữ khác nhau theo cách bất khả tri tác vụ. Tuy nhiên, các nghiên cứu này bị hạn chế bởi nhu cầu phải huấn luyện một adapter cho mỗi ngôn ngữ/phương ngữ. Để giải quyết thiếu sót này, một số nghiên cứu sử dụng siêu mạng để tạo ra các adapter đặc trưng ngôn ngữ từ các vector loại hình học ngôn ngữ (Ansell et al., 2021) và các định danh ngôn ngữ (Üstün et al., 2022), hiệu quả loại bỏ nhu cầu huấn luyện trên hàng trăm adapter ngôn ngữ. Ngoài adapter, siêu mạng cũng đã được áp dụng cho prompt-tuning (He et al., 2022) và LoRA (Phang et al., 2022). Trong khi nghiên cứu trước chủ yếu tạo ra các mô-đun để thích ứng ngôn ngữ, nghiên cứu của chúng tôi là nghiên cứu đầu tiên thực hiện thích ứng phương ngữ thông qua siêu mạng.

**Căn chỉnh liên ngôn ngữ** Căn chỉnh liên ngôn ngữ đã được quan sát trong các biểu diễn được học của các mô hình ngôn ngữ đa ngôn ngữ (Pires et al., 2019). Căn chỉnh là một tính chất đặc biệt mong muốn cho phép các mô-đun task-adapter được chia sẻ giữa các ngôn ngữ. Hơn nữa, các phương pháp căn chỉnh liên ngôn ngữ (Conneau et al., 2018, 2020) đặc biệt hiệu quả khi làm việc với các ngôn ngữ rất tương tự, khiến chúng phù hợp cho setting liên phương ngữ. Đáng ngạc nhiên, setting liên phương ngữ này vẫn chưa được khám phá đầy đủ. Trong hầu hết các setting, giám sát cấp token-to-token cho căn chỉnh không có sẵn. Các nghiên cứu trước đã giải quyết vấn đề này bằng cách phát triển các phương pháp không giám sát. Trong dòng nghiên cứu này, một số phương pháp thực hiện căn chỉnh liên ngôn ngữ bằng cách giảm thiểu khoảng cách Wasserstein gần đúng (Arjovsky et al., 2017; Romanov et al., 2019). Ngoài ra, nghiên cứu trước đã chỉ ra rằng việc tối ưu hóa trực tiếp cho khoảng cách Wasserstein được nới lỏng sử dụng Sinkhorn's Divergence cũng có thể hiệu quả cho căn chỉnh liên ngôn ngữ khi các biểu diễn đủ tin cậy có sẵn (Zhang et al., 2017). Trong setting của chúng tôi, Multi-VALUE cung cấp cho chúng tôi một lượng lớn dữ liệu huấn luyện giả phương ngữ, điều này làm cho việc thiết kế một mục tiêu căn chỉnh hình thái cú pháp trở nên khả thi.

## 3 HyperLoRA

Như một bước đầu tiên hướng tới tính mạnh mẽ phương ngữ, HyperLoRA cho phép thích ứng hiệu quả tài nguyên cho các phương ngữ mới theo cách bất khả tri tác vụ. Cách tiếp cận của chúng tôi dựa trên 4 thành phần chính: (1) chúng tôi hỗ trợ các phương ngữ có tài nguyên thấp với kiến thức ngôn ngữ học chuyên môn có thông tin được mô hình hóa bởi (2) một siêu mạng học một không gian đặc trưng ngôn ngữ chung giữa các phương ngữ. Siêu mạng được huấn luyện để tạo ra (3) các mô-đun LoRA nhẹ với (4) mục tiêu căn chỉnh biểu diễn phương ngữ và SAE bằng cách tìm kế hoạch vận chuyển tối ưu. Dưới kế hoạch vận chuyển tối ưu này, chúng ta có thể trực tiếp cắm các mô-đun LoRA vào bất kỳ tác vụ downstream nào.

### 3.1 Loại hình học Phương ngữ như Kiến thức Chuyên môn

"The man I met's girlfriend is a real beauty" là câu mà một người nói phương ngữ East Anglian sẽ nói thay vì "The girlfriend of the man I met is a real beauty". Người nói East Anglian sử dụng một cấu trúc mà dấu hiệu sở hữu được thêm vào cuối cụm danh từ. Đối với các nhà ngôn ngữ học, đây được gọi là một đặc trưng ngôn ngữ hoặc quy tắc ngôn ngữ mà người nói phương ngữ sử dụng với các tỷ lệ khác nhau và trong các ngữ cảnh khác nhau. Các chuyên gia đã phát hiện ra rằng đặc trưng này không phải là duy nhất đối với phương ngữ East Anglian và có thể được tìm thấy trong nhiều phương ngữ gần về mặt địa lý với phương ngữ East Anglian, hoặc thậm chí trong tiếng Anh Ấn Độ và tiếng Anh Hồng Kông, với mức độ phổ biến thấp hơn. Các chuyên gia từ lâu đã nghiên cứu các biến thể trong và giữa các phương ngữ qua lăng kính của các đặc trưng loại hình học này. Chúng tôi theo trực giác của Nerbonne (2009), định nghĩa các phương ngữ bằng các tập hợp độc đáo của các đặc trưng phương ngữ tương quan. Các vector đặc trưng loại hình học này có sẵn trên Electronic Atlas of Varieties of English (eWAVE; Kortmann et al., 2020)¹. Multi-VALUE áp dụng các phép biến đổi đặc trưng một cách xác suất theo sự chứng thực của chúng trong eWAVE với các tỷ lệ sau: 100% cho các đặc trưng bắt buộc, 60% cho các đặc trưng không phổ biến cũng không hiếm, 30% cho các đặc trưng hiếm và 0% cho không có thông tin hoặc vắng mặt được chứng thực. Chúng tôi tuân theo quy trình này. Cụ thể hơn, chúng tôi mô hình hóa không gian các đặc trưng ngôn ngữ cùng với các mẫu tổng hợp của chúng sử dụng một mạng neural và điều tra tính hữu ích của nó cho tổng quát hóa liên phương ngữ.

### 3.2 Siêu mạng

Chúng tôi tận dụng siêu mạng cho Low-Rank Adaptation (LoRA). LoRA (Hu et al., 2021) là một cách tiếp cận tinh chỉnh giữ các tham số mô hình đầy đủ cố định và thay vào đó cập nhật một phân tách rank thấp của các ma trận attention. Thay vì cập nhật trọng số LoRA trực tiếp, cách tiếp cận của chúng tôi học các trọng số của một siêu mạng (Ha et al., 2016), sau đó được sử dụng để tạo ra các trọng số LoRA thích hợp. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên tạo ra các adapter LoRA với siêu mạng cho thích ứng domain. Chúng tôi đưa ra một phác thảo chi tiết trong Hình 1 cho kiến trúc siêu mạng mới này để tạo ra các tham số LoRA. Cụ thể, chúng tôi trình bày ký hiệu cho kiến trúc siêu mạng của chúng tôi như sau. Cho D^k_q, U^k_q biểu thị các phép chiếu rank thấp lớp k liên quan đến query, và D^k_v, U^k_v, những cái liên quan đến value. Chúng tôi sử dụng siêu mạng g nhận đầu vào concat(d, i^k_{q,v}) trong đó d ∈ [0,1]^#features là vector đặc trưng phương ngữ và i^k_{q,v} ∈ {0, ..., 2×#blocks} là embedding vị trí phân biệt giữa các transformer block, và giữa các adapter LoRA query và value. Chúng tôi sử dụng các siêu mạng riêng biệt cho D^k_{q,v} và U^k_{q,v}. Mỗi siêu mạng được tham số hóa bởi các trọng số W_d, W_u biểu thị các phép chiếu down và up tương ứng. Cuối cùng, cho D_{q,v} (tương tự cho U_{q,v}) các phương trình siêu mạng có thể được viết là:

x = concat(d, i^k_{q,v}) (1)
D^k_{q,v}, U^k_{q,v} = g(x), g'(x) (2)

và cụ thể hơn:
D_{q,v} = MM(ReLU(MM(x, W_d)), W_u) (3)

trong đó MM biểu thị phép nhân ma trận. Phương trình 3 cũng áp dụng cho U^k_{q,v} với các trọng số tương ứng thông qua một phép tính tương tự. Huấn luyện HyperLoRA được thể hiện trong Hình 2 và Thuật toán 1.

### 3.3 Thích ứng Rank Thấp Đặc trưng Phương ngữ

Các phương pháp thích ứng liên ngôn ngữ trước đây đã tập trung vào nhiều cấu hình adapter bottleneck khác nhau được áp dụng sau multi-head attention trong lớp transformer (Lialin et al., 2023; Pfeiffer et al., 2020b, 2023). Dựa trên những nỗ lực này, chúng tôi đưa ra giả thuyết rằng thích ứng ở mức attention có thể hiệu quả đối với các biến thể hình thái cú pháp có mặt trong các phương ngữ. Giả thuyết này xuất phát từ quan sát rằng cơ chế self-attention, được biết đến với độ nhạy cảm đối với các sắc thái cú pháp, có thể phục vụ tốt hơn các biến thể cú pháp giữa và trong các phương ngữ. Tuy nhiên, cần có một kiểm tra toàn diện về các mô-đun PEFT cho phương ngữ, điều mà chúng tôi để lại cho nghiên cứu tương lai.

### 3.4 Căn chỉnh Hình thái cú pháp

Trong khi có một lượng lớn song song bitexts câu xuất phát từ dịch máy được sử dụng cho căn chỉnh liên ngôn ngữ, tương đương không tồn tại cho các phương ngữ tiếng Anh. Như một biện pháp khắc phục, chúng tôi sử dụng hệ thống dịch dựa trên quy tắc của Multi-VALUE (Ziems et al., 2023b) để tạo ra các kho dữ liệu song song cho tất cả các phương ngữ nguồn. Trong khi đánh giá Multi-VALUE được chỉ ra là có thể dự đoán hiệu suất thực tế (Ziems et al., 2023b), bản chất tổng hợp của đánh giá này là một hạn chế của nghiên cứu chúng tôi được thảo luận thêm trong phần Hạn chế.

Các kho dữ liệu được biến đổi Multi-VALUE chỉ được căn chỉnh ở mức câu. Tuy nhiên, những khác biệt chúng tôi giải quyết nằm ở mức hình thái cú pháp, đòi hỏi căn chỉnh cấp token. Để đạt được mục tiêu này, chúng tôi tận dụng các phương pháp căn chỉnh không giám sát được thảo luận trong nghiên cứu trước (Zhang et al., 2017; Alvarez-Melis và Jaakkola, 2018). Chúng tôi đo lường các biến thể cấp token thông qua khoảng cách earth mover's, được ký hiệu là W(P_DIAL, P_SAE), trong đó P_DIAL biểu thị phân phối của các biểu diễn lớp cuối phương ngữ, trong khi P_SAE tương ứng với phân phối cho SAE. Khoảng cách earth mover's, hoặc khoảng cách Wasserstein (W), có thể được xấp xỉ thông qua Sinkhorn's divergence (Feydy et al., 2018) nội suy giữa Khoảng cách Wasserstein, và Maximum Mean Discrepancy (MMD) thông qua phương trình:

S_ε(α, β) = W_ε(α, β) - 1/2 W_ε(α, α) - 1/2 W_ε(β, β)

Ở đây W_ε là khoảng cách Wasserstein được điều chỉnh entropy hiệu quả tính toán (Cuturi, 2013), được định nghĩa như sau:

W_ε(α, β) = min_{π∈Π(α,β)} ∫_{X×Y} c(x,y)dπ(x,y) + εKL(π, α⊗β)

trong đó x và y là các biểu diễn phương ngữ và SAE lớp cuối tương ứng. Và tương tự X và Y là các không gian đặc trưng cho các biểu diễn phương ngữ và SAE lớp cuối, tương ứng. π là coupling giảm thiểu chi phí c của việc di chuyển khối lượng từ phân phối α đến β. Để tính toán Sinkhorn divergence, chúng tôi sử dụng solver được cung cấp bởi Feydy et al. (2018) với ε = 0.05 và c là sai số bình phương.

**Thuật toán 1 Huấn luyện HyperLoRA**
```
Input: features {d_s}_{s∈S}, sentences {x_s}_{s∈S}, SAE representations h_SAE
Initialize M  # Main model
Initialize g  # Hypernetwork
for training step do
    s ~ S  # Sample dialect
    B_s ~ {x_s}  # Sample batch
    θ_s ← g(d_s)  # LoRA adapter
    for x_s ∈ B_s do
        h_s ← M(x_s; θ_s)  # last hidden states
    end for
    loss ← S_ε({h_s}, h_SAE)
    backpropagate loss in g
end for
Return: g
```

## 4 Thiết lập Thí nghiệm

**Datasets** Chúng tôi đánh giá phương pháp của chúng tôi trên 5 biến thể được biến đổi phương ngữ của GLUE sử dụng Multi-VALUE (Ziems et al., 2023b). Chúng tôi chọn African American Vernacular English (AAVE), Indian English (IndE), Nigerian English (NgE), Colloquial Singaporean English (CollSgE), và Chicano English (ChcE) làm các phương ngữ trọng tâm. AAVE đã là trọng tâm chính của các nghiên cứu trước trong tính mạnh mẽ phương ngữ. IndE và NgE là các phương ngữ được sử dụng rộng rãi bởi hơn một trăm triệu người nói. CollSgE đã được chỉ ra là một sự dịch chuyển phương ngữ đặc biệt khó khăn (Ziems et al., 2023b) chia sẻ ít đặc trưng ngôn ngữ với SAE chính thống, và với nhiều đặc trưng độc đáo chỉ có trong CollSgE. Mặt khác, ChcE đặc biệt gần với SAE. Trong các thí nghiệm của chúng tôi, chúng tôi tập trung vào 5 phương ngữ này. Sau đó, trong các nghiên cứu khử yếu tố, chúng tôi sẽ khám phá việc huấn luyện HyperLoRA trên các phương ngữ khác gần hơn với CollSgE để nghiên cứu tác động của các phương ngữ được sử dụng trong thời gian huấn luyện.

**Chi tiết Huấn luyện** Đối với tất cả các thí nghiệm, chúng tôi sử dụng RoBERTa Base được huấn luyện trước (Liu et al., 2019) làm mô hình backbone. Để huấn luyện HyperLoRA, chúng tôi sử dụng 1000 ví dụ WiC (Pilehvar và Camacho-Collados, 2019) từ mỗi phương ngữ nguồn. Tại thời điểm suy luận, chúng tôi cắm mô-đun LoRA được tạo ra từ siêu mạng đã học vào mô hình backbone với các adapter cụ thể tác vụ thích hợp và các classification head liên quan. Chúng tôi huấn luyện HyperLoRA với 4 phương ngữ nguồn sử dụng optimizer Adam (Kingma và Ba, 2017) với tỷ lệ học 3e-5, với scheduler tuyến tính, và sử dụng kích thước batch 16 cho 50 epoch. Chúng tôi tải mô hình với loss thấp nhất vào cuối huấn luyện. Các siêu tham số này đã được chọn thông qua tìm kiếm lưới trên các tỷ lệ học 1e-5, 3e-5, và 1e-4, kích thước batch 16, 32, và 64, và giữa 30 và 50 epoch. Đối với các adapter cụ thể tác vụ, chúng tôi trực tiếp sử dụng các adapter GLUE có sẵn từ Adapterhub (Pfeiffer et al., 2020a). Trong tất cả các thí nghiệm của chúng tôi, HyperLoRA được huấn luyện và đánh giá theo cách zero-shot. Đối với mỗi phương ngữ chưa được nhìn thấy (ví dụ, A), chúng tôi huấn luyện HyperLoRA sử dụng các phương ngữ còn lại (B, C, D, E) và đánh giá tính mạnh mẽ phương ngữ của nó đối với A. Ví dụ, trong Hình 2, HyperLoRA được huấn luyện trên AAVE, NgE, ChcE, và IndE, và được đánh giá trên phương ngữ mục tiêu CollSgE.

**Baselines** Trong việc đánh giá HyperLoRA, chúng tôi đánh giá (1) hiệu quả tài nguyên của nó so với các phương pháp phương ngữ bất khả tri tác vụ hiện tại, (2) tính mạnh mẽ phương ngữ của nó so với các mô hình được huấn luyện cho SAE, và (3) khả năng sử dụng hiệu quả kiến thức chuyên môn để thích ứng với các phương ngữ mới. Đối với mỗi câu hỏi nghiên cứu này, chúng tôi thiết lập một baseline phù hợp. Để giải quyết hiệu quả tài nguyên của phương pháp chúng tôi, chúng tôi so sánh HyperLoRA với TADA (Held et al., 2023) được huấn luyện trên số lượng ví dụ khác nhau từ phương ngữ mục tiêu. Cụ thể hơn, đối với mỗi k ∈ [10, 25, 50, 250, 500, 1000], chúng tôi huấn luyện TADA trên k mẫu WiC. Chúng tôi tuân theo TADA để sử dụng 1000 ví dụ và giữ nguyên các chi tiết huấn luyện còn lại. Để làm nổi bật tính mạnh mẽ phương ngữ của HyperLoRA, chúng tôi triển khai một baseline adapter đơn giản, được ký hiệu là SAE. Sử dụng RoBERTa-Base làm backbone, chúng tôi thêm các adapter cụ thể tác vụ được huấn luyện trên dataset GLUE gốc. Tương tự như HyperLoRA, đây là một baseline zero-shot. Cuối cùng, chúng tôi thiết lập một baseline không sử dụng kiến thức chuyên môn. Để làm điều này, chúng tôi loại bỏ thành phần siêu mạng của HyperLoRA, giữ các mô-đun LoRA và loss căn chỉnh của chúng tôi. Trái ngược với HyperLoRA, các mô-đun LoRA này là liên phương ngữ. Chúng tôi huấn luyện và đánh giá cả HyperLoRA và LoRA theo cách zero-shot tương tự.

## 5 Kết quả Thí nghiệm

### 5.1 Thích ứng Hiệu quả cho Các phương ngữ Chưa được nhìn thấy

Đầu tiên, chúng tôi làm nổi bật hiệu quả của việc sử dụng kiến thức chuyên môn trong việc thích ứng với các phương ngữ mới. Để đơn giản, chúng tôi hạn chế đánh giá của mình cho Quora Question Pairs (QQP), một trong những tác vụ có ít biến đổi nhất trong hiệu suất.

Trong Hình 3, chúng tôi cho thấy hiệu suất QQP trên cả 5 phương ngữ. HyperLoRA tìm thấy hiệu suất cạnh tranh với chi phí thấp hơn nhiều, cho thấy hiệu suất tương đương với TADA được huấn luyện trên ≈250 mẫu phương ngữ. Đối với AAVE và CollSgE, điều này thấp hơn, khoảng 50 và 25 tương ứng. Quan sát này làm nổi bật giá trị của kiến thức ngôn ngữ học chuyên môn cho thích ứng phương ngữ, vì nó có thể tương đương với việc có 250 mẫu được chú thích mỗi phương ngữ—một lợi ích đáng kể. Tầm quan trọng của phát hiện này trở nên rõ ràng khi xem xét số lượng lớn các phương ngữ hiện có, vượt quá 70, và khả năng xuất hiện của các phương ngữ mới. Việc thu thập 250 mẫu được chú thích cho mỗi phương ngữ có thể tốn kém một cách cấm đoán và thách thức về mặt tìm kiếm những người chú thích phù hợp (Ziems et al., 2023b). Chúng tôi thừa nhận rằng trong khi HyperLoRA có thể không hoàn toàn thu hẹp khoảng cách hiệu suất, nó hiệu quả giải quyết sự đánh đổi giữa hiệu suất và hạn chế tài nguyên mà không có bất kỳ ví dụ phương ngữ nào. Do đó, nó cung cấp một mức độ mạnh mẽ có giá trị với chi phí gần như không đáng kể.

### 5.2 Kết quả Chuyển giao Zero-Shot

Để đánh giá tính mạnh mẽ phương ngữ của HyperLoRA, chúng tôi so sánh HyperLoRA với baseline SAE trên cả 5 phương ngữ trong Bảng 2. Chúng tôi quan sát rằng phương pháp của chúng tôi thường đạt được hiệu suất cao hơn so với baseline SAE, ngoại trừ RTE. Đáng chú ý, HyperLoRA đạt được hiệu suất cao hơn trên hơn 4 trong 7 tác vụ. Trong việc phân tích các kết quả này, chúng tôi thấy rằng COLA, RTE, và SST2 gặp phải biến đổi lớn trong hiệu suất. Đối với các tác vụ còn lại, tức là MNLI, QNLI, QQP, và STSB, HyperLoRA đạt được hiệu suất tốt nhất hoặc cạnh tranh. Tổng thể, có một cải thiện 1.7% trong hiệu suất trung bình cho AAVE và 0.8% trong hiệu suất trung bình cho NgE.

Trong trường hợp ChcE, cách tiếp cận của chúng tôi không mang lại cải thiện hiệu suất trung bình. Đáng chú ý rằng các tác giả của Multi-VALUE (Ziems et al., 2023b) cũng gặp phải kết quả tương tự khi huấn luyện trên ChcE thay vì SAE. Sự thiếu cải thiện này có thể được quy cho những tương đồng nổi bật giữa ChcE và Colloquial American English. Tập hợp thí nghiệm này tính đến biến đổi tiềm năng trong những khác biệt giữa các phương ngữ nguồn được sử dụng để huấn luyện HyperLoRA và các phương ngữ HyperLoRA được đánh giá trên. Biến đổi này có thể giải thích những khác biệt trong cải thiện hiệu suất giữa các phương ngữ.

Như một mô-đun plug-and-play có thể được sử dụng ngay bởi bất kỳ cộng đồng nào, HyperLoRA có tiềm năng cải thiện tính mạnh mẽ của mô hình backbone được huấn luyện SAE bất kể phương ngữ.

### 5.3 Hiệu quả của Kiến thức Chuyên môn

Để xác thực đóng góp của kiến thức chuyên môn, chúng tôi so sánh HyperLoRA với baseline LoRA. Chúng tôi báo cáo kết quả trong Bảng 3. Chúng tôi quan sát rằng huấn luyện các adapter LoRA liên phương ngữ có thể ảnh hưởng tiêu cực đến hiệu suất GLUE. Khi so sánh với baseline SAE naïve, LoRA thể hiện hiệu suất kém hơn với sự giảm 3.8% và 1.8% trên COLA và RTE, tương ứng. Đối với HyperLoRA, chúng tôi thấy rằng mặc dù vẫn có sự giảm nhẹ trong hiệu suất RTE -1.8%, nó chứng minh vượt trội hơn baseline SAE. Cụ thể, HyperLoRA mang lại cải thiện 3.5% trên COLA và 0.8% trên QQP. Thông qua kiểm tra bootstrap cặp, chúng tôi xác minh rằng sự giảm trong hiệu suất RTE không có ý nghĩa thống kê, trong khi các cải thiện trên COLA và QQP có ý nghĩa thống kê. Kết luận, các phát hiện của chúng tôi gợi ý rằng việc sử dụng siêu mạng để giảm thiểu can thiệp tiêu cực, cùng với việc tận dụng kiến thức chuyên môn, chứng minh là một chiến lược hiệu quả để cải thiện chuyển giao liên phương ngữ.

## 6 Phân tích Khử yếu tố

### 6.1 Căn chỉnh Hình thái cú pháp

Để hiểu hiệu quả của mục tiêu hình thái cú pháp của chúng tôi, chúng tôi quay lại thiết lập của TADA và sửa đổi mục tiêu căn chỉnh của nó thành Sinkhorn Divergence của chúng tôi. Đối với cả TADA và mục tiêu căn chỉnh của chúng tôi, chúng tôi huấn luyện các adapter đặc trưng phương ngữ cho AAVE sử dụng 1000 mẫu song song từ dataset SAE WiC và Multi-VALUE được biến đổi AAVE Wic Dataset. Chúng tôi đánh giá các adapter này trên benchmark GLUE và báo cáo kết quả trong Bảng 4. Chúng tôi quan sát rằng cả TADA và mục tiêu căn chỉnh của chúng tôi đều vượt trội hơn adapter tác vụ SAE naïve. Trong khi chiến lược của chúng tôi đạt được +0.7% trên COLA và -1.8% trên RTE so với TADA, chúng tôi xác minh thông qua kiểm tra bootstrap cặp và thấy rằng những khác biệt này không có ý nghĩa thống kê. Do đó, không có sự khác biệt đáng kể trong hiệu suất, mục tiêu căn chỉnh hình thái cú pháp dựa trên Sinkhorn divergence của chúng tôi trình bày một vấn đề tối ưu hóa có nền tảng vững chắc có thể được giải quyết hiệu quả. Nó cung cấp các đảm bảo hội tụ mong muốn, loại bỏ sự cần thiết cho các heuristic bổ sung được sử dụng trong cách tiếp cận huấn luyện adversarial trong TADA.

### 6.2 Tác động của Các phương ngữ Nguồn

Chúng tôi nghiên cứu tác động của các phương ngữ nguồn một cách gần gũi hơn bằng cách phân tích sự khác biệt của phương ngữ mới tại thời điểm kiểm tra đối với các phương ngữ nguồn được sử dụng để huấn luyện. Sự khác biệt này của các tập hợp đặc trưng phương ngữ là tự nhiên, thực tế, nó thường được biết đến trong phương ngữ học rằng một số đặc trưng mâu thuẫn với nhau (Nerbonne, 2009). Các thước đo thường được sử dụng để đo lường sự khác biệt phương ngữ là khoảng cách địa lý và khoảng cách Manhattan áp dụng cho các vector đặc trưng phương ngữ (Ziems et al., 2023b). Tuy nhiên, các thước đo này không phù hợp trực tiếp cho setting đa nguồn. Để đạt được hiệu quả này, chúng tôi phát triển một thước đo cho độ bao phủ đặc trưng, như sau. Chúng tôi đưa ra giả thuyết rằng HyperLoRA hoạt động tốt nhất trên các phương ngữ mới khi hầu hết các đặc trưng ngôn ngữ của phương ngữ mới đã được nhìn thấy trong quá trình huấn luyện.

Coverage = 1 - ||[∑_{s∈S} d_s - d_t]_-||_1 / ||d_t||_1 (4)

trong đó d_s và d_t là các vector đặc trưng ngôn ngữ cho phương ngữ nguồn s và phương ngữ mục tiêu t, tương ứng. S biểu thị tập hợp các phương ngữ nguồn. Thước đo của chúng tôi hiệu quả tính toán tỷ lệ phần trăm của các đặc trưng có trọng số trong phương ngữ mục tiêu được bao phủ bởi các phương ngữ trong S.

Để đo lường tác động của các phương ngữ nguồn, chúng tôi tính toán khoảng cách Manhattan trung bình và điểm bao phủ cho tất cả các kết hợp của 4 phương ngữ khác với phương ngữ mục tiêu. Đối với CollSgE, chúng tôi thấy rằng tập hợp (CapeE, FijiAE, MaltE, SriLE) đạt được khoảng cách Manhattan thấp nhất, nhưng cũng có điểm bao phủ thấp. Di chuyển lên trong pareto frontier, tập hợp (MalaE, MaltE, JamE, IndSAE) đạt được khoảng cách Manhattan thấp, nhưng điểm bao phủ cao. Chúng tôi huấn luyện HyperLoRA cho hai tập hợp phương ngữ nguồn này và so sánh hiệu suất với thí nghiệm trước của chúng tôi (Phần 5). Chúng tôi báo cáo kết quả trong Bảng 5.

Chúng tôi thấy rằng cả khoảng cách Manhattan trung bình thấp hơn và bao phủ đặc trưng lớn hơn đều có thể góp phần vào cải thiện hiệu suất trên phương ngữ mục tiêu. Cụ thể, đồng thời giảm khoảng cách Manhattan và cải thiện bao phủ đặc trưng có thể dẫn đến cải thiện +2.6% trên RTE (từ NgE, AAVE, IndE, ChcE đến MalaE, MaltE, JamE, IndSAE). Tổng thể, khi phương ngữ mới đặc biệt gần về khoảng cách Manhattan và được bao phủ rộng rãi bởi các phương ngữ nguồn, chúng tôi quan sát HyperLoRA có thể dẫn đến hiệu suất cao nhất, với cải thiện +0.7% trong hiệu suất trung bình, so với baseline SAE.

Dựa trên những phát hiện này, chúng tôi chứng minh rằng khi tài nguyên tính toán bị hạn chế, việc sử dụng các heuristic này cung cấp một chiến lược đơn giản và hiệu quả để chọn các phương ngữ nguồn khi giải quyết các phương ngữ đang phát triển.

## 7 Kết luận

Trong bài báo này, chúng tôi đề xuất HyperLoRA, một phương pháp thích ứng phương ngữ bất khả tri tác vụ, nhẹ và có khả năng mở rộng cao. Nơi chỉ truy cập kiến thức chuyên môn về phương ngữ, chúng tôi cho thấy rằng HyperLoRA có thể dẫn đến cải thiện tính mạnh mẽ đối với các phương ngữ chưa được nhìn thấy trên benchmark GLUE, trên năm phương ngữ. Tại thời điểm suy luận, HyperLoRA không yêu cầu bất kỳ dữ liệu phương ngữ nào, điều này làm cho nó áp dụng rộng rãi trong các setting hạn chế tài nguyên và tính toán. Hơn nữa, HyperLoRA được huấn luyện với khối lượng dữ liệu có thể dễ dàng thay thế bằng các kho dữ liệu phương ngữ được dịch thủ công. Hiệu quả tài nguyên và tính toán này rất tạo điều kiện cho việc chiếm hữu các công nghệ ngôn ngữ trong các cộng đồng nhỏ nhưng đa dạng². Cuối cùng, bằng cách tạo ra các adapter LoRA sử dụng một siêu mạng nhẹ, cách tiếp cận của chúng tôi có tính di động cao cho các LLM với ít hơn 0.5% tham số bổ sung và không có độ trễ suy luận bổ sung nào. Những khía cạnh này cho phép HyperLoRA đạt được sự đánh đổi thuận lợi giữa chi phí huấn luyện và suy luận và tính mạnh mẽ phương ngữ. Tóm lại, HyperLoRA có tiềm năng lớn để cho phép hàng tỷ người nói phương ngữ tiếng Anh bị thiểu số hóa truyền thống truy cập công nghệ ngôn ngữ bằng ngôn ngữ ưa thích của họ.

## Hạn chế

HyperLoRA được huấn luyện trên các giả phương ngữ thu được bằng cách sử dụng các quy tắc biến đổi Multi-VALUE (Ziems et al., 2023b), là những dịch chuyển phương ngữ tổng hợp tập trung vào các khác biệt liên quan đến hình thái học và cú pháp. Điều quan trọng cần lưu ý là những dịch chuyển này không bao gồm toàn bộ các biến thể có thể có trong các phương ngữ thực tế. Do đó, chúng tôi khuyến khích nghiên cứu tương lai giải quyết hạn chế này và khám phá các biến thể xảy ra tự nhiên khác liên quan đến phương ngữ như khác biệt từ vựng, dịch chuyển chủ đề và dịch chuyển thanh ghi. Ngoài ra, trong khi HyperLoRA có thể sử dụng bất kỳ vector ngôn ngữ nào cung cấp đặc trưng chi tiết hơn của các phương ngữ trong giai đoạn kiểm tra, chúng tôi không thực hiện phân tích độ nhạy cho các vector này. Sự thiếu đảm bảo này có thể đặt ra thách thức vì các biến thể phương ngữ thực tế thường phức tạp và tinh tế hơn nhiều.

Hơn nữa, nghiên cứu của chúng tôi không bao gồm so sánh toàn diện về các kỹ thuật tinh chỉnh hiệu quả tham số khác nhau cho thích ứng phương ngữ. Chúng tôi khuyến khích nghiên cứu sâu hơn để đi sâu vào lĩnh vực này và khám phá nó.

Cuối cùng, tất cả các thí nghiệm của chúng tôi chủ yếu tập trung vào các LLM chỉ encoder. Kết quả là, điều này tạo ra khoảng cách thí nghiệm nơi chúng tôi không thể xác minh hiệu suất của phương pháp chúng tôi trên các kiến trúc encoder-decoder và chỉ decoder. Nghiên cứu tương lai nên lấp đầy khoảng cách và khám phá thêm các giải pháp thích ứng phương ngữ bất khả tri tác vụ cho các mô hình với các kiến trúc thay thế này.

## Tuyên bố Đạo đức

Như được làm nổi bật trong các hạn chế của chúng tôi, chúng tôi thừa nhận rằng chúng tôi không thể cung cấp đảm bảo về việc sử dụng HyperLoRA trong các cộng đồng nơi các biến thể nội phương ngữ phổ biến. Hạn chế này xuất phát từ thực tế rằng các phương ngữ không phải là các thực thể đồng nhất và bao gồm các biến thể đa dạng. Do đó, việc các thành viên của các cộng đồng phương ngữ này thực hiện các biện pháp phòng ngừa cần thiết khi áp dụng HyperLoRA cho các trường hợp sử dụng của họ là rất quan trọng.

## Lời cảm ơn

Chúng tôi xin cảm ơn các nhà phê bình ẩn danh và các thành viên phòng thí nghiệm SALT vì phản hồi có giá trị của họ. Công việc này được tài trợ một phần bởi tài trợ Defense Advanced Research Project Agency (DARPA) HR00112290103/HR0011260656, và tài trợ NSF IIS-2247357 và IIS-2308994.
