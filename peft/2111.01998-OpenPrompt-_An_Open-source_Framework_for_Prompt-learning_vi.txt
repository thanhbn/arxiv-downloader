# 2111.01998.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2111.01998.pdf
# Kích thước tệp: 303272 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
OpenPrompt: Một Framework Mã nguồn mở cho Prompt-learning
Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,
Zhiyuan Liuy,Hai-Tao Zhengy,Maosong Sun
Đại học Thanh Hoa, Bắc Kinh, Trung Quốc
fdingn18, hsd20, zwl19, yl-chen21 g@mails.tsinghua.edu.cn
Tóm tắt
Prompt-learning đã trở thành một paradigm mới
trong xử lý ngôn ngữ tự nhiên hiện đại, trực tiếp
thích ứng các mô hình ngôn ngữ được huấn luyện
trước (PLMs) với dự đoán kiểu cloze, mô hình hóa
tự hồi quy, hoặc sinh chuỗi từ chuỗi, mang lại
hiệu suất hứa hẹn trên các tác vụ khác nhau. Tuy
nhiên, chưa có framework triển khai chuẩn nào
của prompt-learning được đề xuất, và hầu hết các
codebase prompt-learning hiện có, thường không
được quy định, chỉ cung cấp các triển khai hạn chế
cho các tình huống cụ thể. Vì có nhiều chi tiết như
chiến lược tạo template, chiến lược khởi tạo, và
chiến lược verbalization, v.v. cần được xem xét
trong prompt-learning, các chuyên gia gặp khó khăn
trong việc nhanh chóng thích ứng các phương pháp
prompt learning mong muốn vào ứng dụng của họ.
Trong bài báo này, chúng tôi trình bày OpenPrompt,
một toolkit thống nhất dễ sử dụng để tiến hành
prompt-learning trên PLMs. OpenPrompt là một
framework thân thiện với nghiên cứu được trang bị
hiệu quả, tính mô-đun, và khả năng mở rộng, và
tính kết hợp của nó cho phép tự do kết hợp các
PLMs khác nhau, định dạng tác vụ, và các mô-đun
prompting trong một paradigm thống nhất. Người
dùng có thể thuận tiện triển khai các framework
prompt-learning và đánh giá tính tổng quát của
chúng trên các tác vụ NLP khác nhau mà không bị
hạn chế. OpenPrompt được phát hành công khai
tại https://github.com/thunlp/OpenPrompt.
1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước (PLMs)
(Han et al., 2021a; Qiu et al., 2020) đã được chứng
minh rộng rãi là hiệu quả trong hiểu và sinh ngôn
ngữ tự nhiên, mở ra kỷ nguyên mới của xử lý ngôn
ngữ tự nhiên (NLP) hiện đại. Trong giai đoạn đầu
của cuộc cách mạng này, cách tiếp cận chuẩn để
thích ứng PLMs với các tác vụ NLP cụ thể khác nhau
là paradigm pretraining-finetuning, nơi các tham số
bổ sung và mục tiêu cụ thể cho tác vụ được đưa vào
trong quy trình tuning. Tuy nhiên gần đây, paradigm
thích ứng PLMs đang thay đổi. Bắt nguồn từ T5
(Raffel et al., 2019) và GPT-3 (Brown et al., 2020),
các nhà nghiên cứu phát hiện rằng PLMs có thể được
kích thích hiệu quả bởi các prompt hoặc demonstration
văn bản, đặc biệt trong các tình huống ít dữ liệu.
Lấy ví dụ đơn giản về phân loại cảm xúc dựa trên
prompt, pipeline bao gồm một template và một
verbalizer, trong đó template được sử dụng để xử lý
văn bản gốc với một số token bổ sung, và verbalizer
chiếu các label gốc thành các từ trong từ vựng để
dự đoán cuối cùng. Giả sử template là " <text> It is<mask> ",
trong đó token <text> đại diện cho văn bản gốc, và
verbalizer là f"positive":"great", "negative":"terrible" g.
Câu " Albert Einstein was one of the greatest intellects
of his time. " sẽ đầu tiên được bọc bởi template được
định nghĩa trước thành " Albert Einstein was one of the
greatest intellects of his time. It is <mask> ". Câu được
bọc sau đó được tokenize và đưa vào PLM để dự đoán
phân phối trên từ vựng tại vị trí token <mask>. Mong
đợi rằng từ great sẽ có xác suất lớn hơn terrible.
Như minh họa ở trên, prompt-learning chiếu các tác
vụ downstream thành các mục tiêu pre-training cho
PLMs với sự hỗ trợ của các prompt mã hóa văn bản
hoặc soft. Một loạt các nghiên cứu về prompt-learning
(Liu et al., 2021a) đã được đề xuất để điều tra các
chiến lược xây dựng template (Schick and Schütze,
2021; Gao et al., 2021; Liu et al., 2021b), verbalizer
(Hu et al., 2021), tối ưu hóa (Lester et al., 2021), và
ứng dụng (Li and Liang, 2021; Han et al., 2021b;
Ding et al., 2021a) cho paradigm này.
Một vấn đề prompt-learning có thể được coi là một
sự tổng hợp của PLMs, kiến thức tiên nghiệm của
con người, và các tác vụ NLP cụ thể cần được xử lý.

--- TRANG 2 ---
Ví dụ PLM Template Verbalizer Tác vụ Tài liệu tham khảo
Naive TC MLM & Seq2Seq M. text M. One-Many Phân loại văn bản -
Naive KP LM & Seq2Seq M. text - Knowledge Probing -
Naive FET MLM M. text (meta info) M. One-Many Phân loại thực thể (Ding et al., 2021a)
PTR MLM M. text (phức tạp) M. One-One Trích xuất quan hệ (Han et al., 2021b)
P-tuning LM Soft tokens M. One-One Phân loại văn bản (Liu et al., 2021b)
Prefix-tuning LM, Seq2Seq Soft tokens - Sinh văn bản (Li and Liang, 2021)
LM-BFF MLM A. text M. One-Many Phân loại văn bản (Gao et al., 2021)
Bảng 1: Một số ví dụ được triển khai bởi OpenPrompt, trong đó M. là viết tắt của được định nghĩa thủ công
và A. là viết tắt của được sinh tự động. Lưu ý rằng các cách tiếp cận khác nhau tập trung vào các phần khác
nhau trong prompt-learning. Bổ sung cho toàn bộ pipeline, các triển khai cụ thể của chúng tôi cho các phương
pháp này được tích hợp vào các lớp cụ thể của OpenPrompt. Ví dụ, triển khai cốt lõi của KPT nằm trong
lớp KnowledgeableVerbalizer.

Do đó, khó có thể hỗ trợ các triển khai cụ thể của
prompt-learning một cách tinh tế với các thư viện
deep learning hoặc NLP hiện tại trong khi cũng
thiếu một paradigm chuẩn. Các công trình trước đây
theo đuổi cách hiệu quả nhất để triển khai prompt-
learning với ít thay đổi nhất đối với framework hiện
có cho fine-tuning truyền thống, dẫn đến khả năng
đọc kém và thậm chí là khả năng tái tạo không ổn định.
Hơn nữa, hiệu suất của một pipeline prompt-learning
biến thiên rất lớn với việc lựa chọn template và
verbalizer (Zhao et al., 2021), tạo ra nhiều rào cản
hơn cho việc triển khai. Cuối cùng, hiện tại không
có framework mã nguồn mở toàn diện nào được
thiết kế đặc biệt cho prompt-learning, điều này khiến
việc thử nghiệm các phương pháp mới và so sánh
nghiêm ngặt các cách tiếp cận trước đây trở nên khó khăn.
Để giải quyết vấn đề này, chúng tôi trình bày
OpenPrompt, một toolkit mã nguồn mở, dễ sử dụng,
và có thể mở rộng cho prompt-learning. OpenPrompt
mô-đun hóa toàn bộ framework của prompt-learning
và xem xét các tương tác giữa mỗi mô-đun. Chúng tôi
nhấn mạnh tính năng kết hợp của OpenPrompt, hỗ trợ
các kết hợp linh hoạt của các định dạng tác vụ đa dạng,
PLMs, và các mô-đun prompting. Ví dụ, chúng ta có
thể dễ dàng thích ứng prefix-tuning (Li and Liang,
2021) cho một tác vụ phân loại văn bản trong OpenPrompt.
Tính năng này cho phép người dùng đánh giá tính tổng
quát của các mô hình prompt-learning của họ trên
các tác vụ khác nhau, chứ không chỉ hiệu suất trên
các tác vụ cụ thể.
Cụ thể, trong OpenPrompt, một lớp Template được
sử dụng để định nghĩa hoặc sinh các template mã hóa
văn bản hoặc soft để bọc input gốc. Để hỗ trợ linh
hoạt các template khác nhau dưới một paradigm thống
nhất, chúng tôi thiết kế một ngôn ngữ template mới
có thể dễ dàng tiến hành tùy chỉnh cấp token cho
các thuộc tính tương ứng. Ví dụ, người dùng có thể
chỉ định token nào là embedding chia sẻ, có thể
huấn luyện, hoặc theo cách nào các token này được
xử lý hậu kỳ, mà không cần phải thực hiện các
triển khai phức tạp cho các template cụ thể. Một
Verbalizer chiếu các label phân loại thành các từ
trong từ vựng, và một PromptModel chịu trách nhiệm
cho quá trình huấn luyện và suy luận. Mỗi mô-đun
trong OpenPrompt được định nghĩa rõ ràng trong
khi vẫn duy trì tính độc lập và kết nối để các nhà
nghiên cứu có thể dễ dàng triển khai mô hình và
cải thiện có mục tiêu. Chúng tôi cũng triển khai
các baseline với OpenPrompt và đánh giá chúng
trên một phạm vi rộng các tác vụ NLP, chứng minh
hiệu quả của OpenPrompt.
Lĩnh vực prompt-learning đang trong giai đoạn
khám phá với sự phát triển nhanh chóng. Hy vọng
rằng, OpenPrompt có thể giúp người mới bắt đầu
nhanh chóng hiểu prompt-learning, cho phép các
nhà nghiên cứu triển khai hiệu quả pipeline nghiên
cứu prompt-learning, và trao quyền cho các kỹ sư
dễ dàng áp dụng prompt-learning vào các hệ thống
NLP thực tế để giải quyết các vấn đề thực tế.
OpenPrompt sẽ không chỉ mở mã nguồn tất cả
các code, mà còn tiếp tục cập nhật tài liệu để cung
cấp các hướng dẫn chi tiết.
2 Bối cảnh
Prompt-learning tiết lộ thế hệ NLP tiếp theo có thể
trông như thế nào.
Mặc dù PLMs đã đạt được thành công to lớn trên
hầu hết tất cả các tác vụ con trong NLP, một vấn đề
vẫn còn treo lơ lửng, liệu chúng ta có thực sự khai
thác đầy đủ tiềm năng của PLMs, đặc biệt là những
mô hình lớn?
Fine-tuning thông thường sử dụng các head và mục
tiêu cụ thể cho tác vụ bổ sung để thích ứng, nhưng
chiến lược này có thể gặp hai vấn đề. Một mặt, cách
tiếp cận như vậy tạo ra một khoảng cách tự nhiên
giữa model tuning và pre-training. Mặt khác, khi
số lượng tham số mô hình tăng lên, cách tiếp cận
fine-tuning này trở nên ngày càng khó khăn để

--- TRANG 3 ---
vận hành do khối lượng tính toán khổng lồ (ví dụ,
GPT-3 (Brown et al., 2020)).
Bằng cách bắt chước quá trình pre-training,
prompt-learning một cách trực quan kết nối khoảng
cách giữa pre-training và model tuning. Trên thực
tế, paradigm này có hiệu quả đáng ngạc nhiên trong
chế độ ít dữ liệu (Le Scao and Rush, 2021; Gao et al.,
2021). Ví dụ, với template phù hợp, prompt-learning
zero-shot thậm chí có thể vượt trội hơn fine-tuning
32-shot (Ding et al., 2021a). Một thuộc tính thực
nghiệm hứa hẹn khác của prompt-learning là tiềm
năng kích thích PLMs quy mô lớn. Khi nói đến mô
hình 10B, việc chỉ tối ưu hóa prompts (các tham số
của mô hình được cố định) có thể đạt được hiệu suất
tương đương với fine-tuning tất cả tham số (Lester
et al., 2021). Những nghiên cứu thực tế này ngụ ý
rằng chúng ta có thể sử dụng prompts để khai thác
kiến thức được giữ trong PLMs một cách hiệu quả
và hiệu quả hơn, dẫn đến hiểu biết sâu sắc hơn về
các nguyên tắc cơ bản của cơ chế của chúng (Wei
et al., 2021; Qin et al., 2021; Vu et al., 2021).
Từ quan điểm triển khai thực tế, prompt-learning
thực sự phức tạp và đòi hỏi nhiều cân nhắc chi tiết.
Với NLP đa mục đích dưới paradigm prompt-learning
làm mục tiêu của chúng tôi, chúng tôi trình bày
OpenPrompt, một toolkit thống nhất để triển khai
hiệu quả và hiệu quả các cách tiếp cận prompt-learning.
OpenPrompt thể hiện cái nhìn toàn diện về các chi
tiết lập trình của prompt-learning, và cho phép các
chuyên gia nhanh chóng hiểu các cơ chế và thuộc
tính thực tế của kỹ thuật này. Và người ta có thể
nhanh chóng triển khai các thuật toán prompt-learning
đại diện hiện có đã được triển khai trong gói dưới
một framework lập trình thống nhất. Hơn nữa,
OpenPrompt cho phép các nhà nghiên cứu hoặc
nhà phát triển nhanh chóng thử nghiệm các ý tưởng
mới của prompt-learning, không chỉ bao gồm các
template hoặc verbalizer được thiết kế mới, mà còn
khám phá các thuộc tính của prompt-learning, ví dụ,
tấn công đối kháng dựa trên prompt.
3 Thiết kế và Triển khai
Như đã nêu trong §1, prompt-learning là một quá
trình toàn diện kết hợp PLMs, kiến thức con người,
và các tác vụ NLP cụ thể. Ghi nhớ điều đó, triết lý
thiết kế là đồng thời xem xét tính độc lập và kết nối
lẫn nhau của mỗi mô-đun. Như minh họa trong
Hình 1, OpenPrompt cung cấp vòng đời đầy đủ
của prompt-learning dựa trên PyTorch (Paszke et al.,
2019). Trong phần này, chúng tôi đầu tiên giới thiệu
tính kết hợp của OpenPrompt, và sau đó thiết kế
chi tiết và triển khai của mỗi thành phần trong
OpenPrompt.
3.1 Tính kết hợp
Trong thế giới NLP, chúng ta thường áp dụng các
PLMs khác nhau với các hàm mục tiêu tương ứng
cho các tác vụ cơ bản khác nhau (đại khái, phân loại
và sinh). Nhưng trong prompt learning, với ý tưởng
cốt lõi của framework là bắt chước các tác vụ pre-
training trong tác vụ downstream, về cơ bản là
"dự đoán từ dựa trên ngữ cảnh", chúng ta có thể
thống nhất hơn nữa việc thực thi các tác vụ downstream.
OpenPrompt hỗ trợ kết hợp các tác vụ (phân loại
và sinh), PLMs (MLM, LM và Seq2Seq), và các
mô-đun prompt (các template và verbalizer khác
nhau) một cách linh hoạt. Ví dụ, từ quan điểm mô
hình, T5 (Raffel et al., 2019) không chỉ được sử
dụng cho dự đoán span và GPT (Brown et al., 2020)
không chỉ được sử dụng cho các tác vụ sinh. Từ
quan điểm prompting, prefix-tuning cũng có thể
được sử dụng cho phân loại, và soft prompt có thể
được sử dụng cho sinh. Tất cả các kết hợp này có
thể dễ dàng được triển khai và xác thực trên các
tác vụ NLP trong framework của chúng tôi để chúng
ta có thể hiểu rõ hơn các cơ chế liên quan.
3.2 Các mô hình ngôn ngữ được huấn luyện trước
Một ý tưởng cốt lõi của prompt-learning là sử dụng
ngữ cảnh bổ sung với các token được mask để bắt
chước các mục tiêu pre-training của PLMs và kích
thích tốt hơn các mô hình này. Do đó, việc lựa chọn
PLMs rất quan trọng đối với toàn bộ pipeline của
prompt-learning. PLMs có thể được chia thành ba
nhóm theo các mục tiêu pre-training của chúng.
Nhóm đầu tiên của PLMs sử dụng masked language
modeling (MLM) để tái tạo một chuỗi bị hỏng bởi
các token được mask ngẫu nhiên, nơi chỉ các loss
của các token được mask được tính toán. Các PLMs
điển hình với mục tiêu MLM bao gồm BERT (Devlin
et al., 2019), RoBERTa (Liu et al., 2019), v.v., và
mục tiêu như vậy được coi là phù hợp cho hiểu ngôn
ngữ tự nhiên (NLU). Nhóm thứ hai khai thác language
modeling (LM) kiểu tự hồi quy để dự đoán token
hiện tại theo các token dẫn đầu của nó. GPT-3 (Brown
et al., 2020) là một trong những tác phẩm đại diện
áp dụng mục tiêu này. Phần thứ ba là các mô hình
sequence-to-sequence (Seq2Seq), nhằm sinh một
chuỗi với decoder được điều kiện trên một encoder
riêng biệt cho chuỗi đầu vào. Các PLMs seq2seq
điển hình

--- TRANG 4 ---
包括T5 (Raffel et al., 2020)、MASS (Song et al.,
2019)和BART (Lewis et al., 2020)等。
Các PLMs khác nhau có các thuộc tính khác nhau,
dẫn đến khả năng thích ứng khác nhau cho các tác
vụ NLP khác nhau trong prompt-learning. Thực tế
trong OpenPrompt, chúng tôi hỗ trợ trực tiếp tải
PLMs từ huggingface transformers1(Wolf et al.,
2020), và PLMs được triển khai bởi các thư viện
khác sẽ được hỗ trợ trong tương lai. Khi PLM được
xác định, các nhà nghiên cứu có thể triển khai một
pipeline prompt-learning hợp lệ đã biết (ví dụ,
RoBERTa cho phân loại cảm xúc few-shot) hoặc
khám phá các ứng dụng khác của PLM có thể khai
thác tiềm năng của nó. Người dùng OpenPrompt
không cần triển khai các head mục tiêu cho các
PLMs khác nhau để tính toán loss tương ứng, một
giao diện thống nhất có thể thực hiện các hoạt động
này tự động (§ 3.6).
3.3 Tokenization
Tokenization là một bước quan trọng trong xử lý
dữ liệu cho NLP, và nó phải đối mặt với những thách
thức mới trong prompt-learning. Sau khi thiết kế
template, việc triển khai cụ thể của tokenization cho
input gốc và template được thiết kế có thể tốn thời
gian và dễ xảy ra lỗi. Đầu tiên, trong prompt-learning,
một số thông tin cụ thể như các chỉ số của entities
và masked tokens nên được xử lý cẩn thận trong
tokenization. Một số lỗi nhỏ, như không khớp chỉ
số masked token, có thể dẫn đến hậu quả nghiêm
trọng. Hơn nữa, các vấn đề concatenation và truncation
sau tokenization (templates không được truncate)
cũng nên được xử lý. Vì các PLMs khác nhau có thể
có các chiến lược tokenization khác nhau, chúng ta
cũng nên xem xét sự không nhất quán trong các chi
tiết xử lý ngữ cảnh bổ sung.
Trong OpenPrompt, chúng tôi thiết kế cụ thể mô-đun
tokenization cho prompt-learning và đơn giản hóa
đáng kể quá trình này. Bằng cách sử dụng các API
xử lý dữ liệu được đóng gói của chúng tôi, người
dùng có thể sử dụng kiểu có thể đọc được của con
người để thiết kế template và thuận tiện hoạt động
trên input và template cùng lúc. Thành phần của
chúng tôi tích hợp thông tin phức tạp từ input và
template và sau đó tiến hành tokenization. Dựa trên
việc lựa chọn PLMs (MLM, LM, và Seq2Seq),
OpenPrompt tự động chọn tokenizer phù hợp trong
prompt-learning, có thể tiết kiệm thời gian đáng kể
cho người dùng để xử lý dữ liệu liên quan đến prompt.
3.4 Templates
Như một trong những phần trung tâm của prompt-
learning, một mô-đun template bọc văn bản gốc với
template mã hóa văn bản hoặc soft. Một template
thường chứa các token ngữ cảnh (văn bản hoặc soft)

--- TRANG 5 ---
1# Ví dụ A. Hard prompt cho phân loại chủ đề
2a {"mask"} news: {"meta": "title"} {"meta": "description"}
3
4# Ví dụ B. Hard prompt cho phân loại thực thể
5{"meta": "sentence"}. In this sentence, {"meta": "entity"} is a {"mask"},
6
7# Ví dụ C. Soft prompt (khởi tạo bằng textual tokens)
8{"meta": "premise"} {"meta": "hypothesis"} {"soft": "Does the first sentence entails
the second ?"} {"mask"} {"soft"}.
9
10# Ví dụ D. Sức mạnh của quy mô
11{"soft": None, "duplicate": 100} {"meta": "text"} {"mask"}
12
13# Ví dụ E. Hỗ trợ script xử lý hậu kỳ
14# ví dụ viết một biểu thức lambda để loại bỏ dấu câu cuối trong dữ liệu
15{"meta": "context", "post_processing": lambda s: s.rstrip(string.punctuation)}. {"
soft": "It was"} {"mask"}
16
17# Ví dụ F. Mixed prompt với hai shared soft tokens
18{"meta": "premise"} {"meta": "hypothesis"} {"soft": "Does"} {"soft": "the", "soft_id
": 1} first sentence entails {"soft_id": 1} second?
19
20# Ví dụ G. Chỉ định title không nên bị truncate
21a {"mask"} news: {"meta": "title", "shortenable": False} {"meta": "description"}
Hình 2: Một số ví dụ về ngôn ngữ template của chúng tôi. Trong ngôn ngữ template của chúng tôi, chúng ta có thể sử dụng key "meta" để tham chiếu văn bản đầu vào gốc (Ví dụ B), các phần của đầu vào gốc (Ví dụ A, C, G), hoặc thông tin key khác. Chúng ta cũng có thể tự do chỉ định token nào là hard và token nào là soft (và chiến lược khởi tạo của chúng). Chúng ta có thể gán một id cho một soft token để chỉ định token nào đang chia sẻ embeddings (Ví dụ F). OpenPrompt cũng hỗ trợ xử lý hậu kỳ (Ví dụ E) cho mỗi token, ví dụ, biểu thức lambda hoặc MLP.

và các masked tokens. Trong OpenPrompt, tất cả
các template được kế thừa từ một lớp cơ sở chung
với các thuộc tính phổ quát và các phương thức
trừu tượng.
Các công trình trước đây thiết kế đa dạng các
template, bao gồm template được viết thủ công
(Schick and Schütze, 2021) và template soft thuần
túy (Lester et al., 2021). Gu et al. (2021) báo cáo
rằng hỗn hợp các token template thủ công và soft
(có thể huấn luyện) đôi khi mang lại kết quả tốt hơn
so với template thủ công riêng biệt và template soft.
Trong Liu et al. (2021b), hiệu suất hứa hẹn được
đạt được bằng cách cố định phần lớn các token
thủ công trong khi tuning một số ít các token khác.
Trong Han et al. (2021b), template được contextualize,
cần được điền với head entity và tail entity để tạo
thành một cái hoàn chỉnh, hơn nữa, đầu ra của nhiều
vị trí được sử dụng trong tính toán loss trong template
của họ. Logan IV et al. (2021) thiết kế null template
với phép concatenation đơn giản của các đầu vào
và một token <mask> được append.
Không hợp lý khi thiết kế một định dạng template
cho mỗi prompt vì nó sẽ đòi hỏi chi phí học tập
cao cho việc sử dụng thực tế. Để giải quyết vấn đề
này, trong OpenPrompt, chúng tôi thiết kế một ngôn
ngữ template để giảm bớt vấn đề, với nó chúng ta
có thể xây dựng các loại template khác nhau dưới
một paradigm thống nhất. Ngôn ngữ template của
chúng tôi lấy cảm hứng từ cú pháp dict của Python.
Và thiết kế như vậy đảm bảo tính linh hoạt và rõ
ràng cùng lúc, cho phép người dùng xây dựng các
prompt khác nhau với sự dễ dàng tương đối.
Cụ thể hơn, một node template là một văn bản (hoặc
văn bản rỗng) với mô tả các thuộc tính. Trong ngôn
ngữ template của chúng tôi, người ta tự do chỉnh
sửa các thuộc tính của mỗi token trong template,
như ký tự nào là shared embedding, ký tự được xử
lý hậu kỳ như thế nào (ví dụ bằng MLP), v.v. Chúng
tôi trình bày một số ví dụ template trong Hình 2,
và hướng dẫn chi tiết để viết template có trong tài
liệu của chúng tôi https://thunlp.github.io/OpenPrompt.
1from openprompt import
ManualVerbalizer
2
3promptVerbalizer = ManualVerbalizer(
4 classes = classes,
5 label_words = {
6 "negative": ["bad"],
7 "positive": ["good", "
wonderful", "great"],
8 },
9 tokenizer = bertTokenizer,
10)
Hình 3: Một ví dụ để định nghĩa một Verbalizer, số
lượng label words cho mỗi lớp là linh hoạt.

--- TRANG 6 ---
MLMLMPrefixNLUSeq2SeqManualSoftMixManualContextKnowNLGAutoFix PLMTrainingPLMTemplateVerbalizerTask
UnfixP-tuningPrompt-tuningPrefix-tuningPTRP-tuningSoftHình 4: Minh họa không gian xác thực của OpenPrompt. Bằng cách điều khiển các mô-đun khác nhau của framework, chúng ta có thể triển khai và đánh giá các phương pháp khác nhau trên một tập rộng các tác vụ NLP. Chúng tôi trình bày bốn ví dụ trong minh họa này, các đường có màu biểu thị luồng triển khai của phương pháp tương ứng.

3.5 Verbalizers
Khi nói đến phân loại dựa trên prompt, một lớp
verbalizer nên được xây dựng để ánh xạ các label
gốc thành các label words trong từ vựng. Khi một
PLM dự đoán một phân phối xác suất trên từ vựng
cho một vị trí được mask, một verbalizer sẽ trích
xuất các logit của label words và tích hợp các logit
của label words vào lớp tương ứng, do đó chịu trách
nhiệm cho việc tính toán loss. Hình 3 trình bày một
cách đơn giản để định nghĩa một verbalizer phân
loại cảm xúc nhị phân.
Tương tự như template, tất cả các lớp verbalizer
cũng được kế thừa từ một lớp cơ sở chung với các
thuộc tính cần thiết và các phương thức trừu tượng.
Bổ sung cho các verbalizer được định nghĩa thủ công,
chúng tôi triển khai các verbalizer tự động như
AutomaticVerbalizer và KnowledgeableVerbalizer
(Hu et al., 2021). Hơn nữa, các hoạt động quan trọng
như calibration (Zhao et al., 2021) cũng được thực
hiện trong OpenPrompt.
3.6 PromptModel
Trong OpenPrompt, chúng tôi sử dụng một đối tượng
PromptModel để chịu trách nhiệm cho việc huấn
luyện và suy luận, chứa một PLM, một đối tượng
Template, và một đối tượng Verbalizer (tùy chọn).
Người dùng có thể kết hợp linh hoạt các mô-đun
này và định nghĩa các tương tác nâng cao giữa chúng.
Một phương thức forward không phụ thuộc vào mô
hình được triển khai trong lớp cơ sở để dự đoán từ
cho các vị trí được mask. Một mục tiêu của mô-đun
này là người dùng không cần triển khai cụ thể các
head cho các PLMs khác nhau, mà sử dụng một API
thống nhất để "dự đoán từ cho các vị trí cần được
dự đoán" bất kể mục tiêu pre-training. Một ví dụ
để định nghĩa một PromptModel được trình bày
trong Hình 5.

1from openprompt import
PromptForClassification
2
3promptModel = PromptForClassification(
4 template = promptTemplate,
5 model = bertModel,
6 verbalizer = promptVerbalizer,
7)
8
9promptModel.eval()
10with torch.no_grad():
11 for batch in data_loader:
12 logits = promptModel(batch)
13 preds = torch.argmax(logits,
dim = -1)
14 print(classes[preds])
Hình 5: Một ví dụ để định nghĩa một PromptModel và
tiến hành đánh giá.

3.7 Huấn luyện
Từ quan điểm các tham số có thể huấn luyện, việc
huấn luyện prompt-learning có thể được chia thành
hai loại chiến lược. Chiến lược đầu tiên đồng thời
tune các prompt và PLM, được xác minh là hiệu
quả trong chế độ ít dữ liệu (OpenPrompt cũng cung
cấp FewshotSampler để hỗ trợ tình huống few-shot
learning). Chiến lược thứ hai là chỉ huấn luyện các
tham số của prompt và giữ PLM cố định, điều này
được coi là một phương pháp tuning hiệu quả về
tham số và được xem xét như một cách hứa hẹn
để kích thích PLMs siêu lớn. Cả hai chiến lược này
đều có thể được gọi với một cú nhấp chuột trong
mô-đun trainer (hoặc runner) của OpenPrompt.
Các mô-đun Trainer trong OpenPrompt triển khai
quá trình huấn luyện đi kèm với các thủ thuật huấn
luyện định hướng prompt, ví dụ ensemble của
template. Đồng thời, OpenPrompt hỗ trợ thực nghiệm
thông qua cấu hình để dễ dàng điều khiển nghiên
cứu thực nghiệm quy mô lớn.

--- TRANG 7 ---
4 Đánh giá
OpenPrompt nhằm hỗ trợ một tập rộng các tác vụ
NLP dưới paradigm của prompt-learning. Về mặt
đánh giá, chúng tôi sử dụng OpenPrompt để triển
khai các baseline khác nhau và đánh giá chúng trên
các tác vụ NLP tương ứng. Chúng tôi trình bày không
gian xác thực trong Hình 4. Và các tác vụ đánh giá
bao gồm WebNLG (Gardent et al., 2017) cho sinh
có điều kiện, GLUE (Wang et al., 2018) và Super-
GLUE (Wang et al., 2019) cho hiểu ngôn ngữ tự
nhiên; SemEval (Hendrickx et al., 2010) cho trích
xuất quan hệ; Few-NERD (Ding et al., 2021b) cho
phân loại thực thể chi tiết; MNLI (Williams et al.,
2017), AG's News (Zhang et al., 2015), DBPedia
(Lehmann et al., 2015) và IMDB (Maas et al., 2011)
cho phân loại văn bản; LAMA (Petroni et al., 2019)
cho knowledge probing. Các bộ xử lý của các tập
dữ liệu này đã được triển khai trong OpenPrompt,
và chúng đều được kế thừa từ một lớp DataProcessor
cơ sở chung. Để giữ kết quả cập nhật, chúng tôi
liên tục cập nhật và báo cáo các kết quả mới nhất
trên repository GitHub của chúng tôi https://github.com/thunlp/OpenPrompt.
5 Kết luận và Công việc Tương lai
Chúng tôi đề xuất OpenPrompt, một toolkit thống
nhất, dễ sử dụng và có thể mở rộng cho prompt-
learning. OpenPrompt thiết lập một framework
thống nhất với các khối được định nghĩa rõ ràng
và các tương tác linh hoạt để hỗ trợ nghiên cứu
vững chắc về prompt-learning. Ở cấp độ ứng dụng,
OpenPrompt có thể hỗ trợ các nhà nghiên cứu và
nhà phát triển triển khai hiệu quả và hiệu quả các
pipeline prompt-learning. Trong tương lai, chúng
tôi sẽ tiếp tục tích hợp các kỹ thuật và tính năng
mới vào OpenPrompt để hỗ trợ tiến trình nghiên
cứu của prompt-learning.
Tài liệu tham khảo
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of ACL, pages 4171–4186,
Minneapolis, Minnesota.
Ning Ding, Yulin Chen, Xu Han, Guangwei Xu,
Pengjun Xie, Hai-Tao Zheng, Zhiyuan Liu, Juanzi
Li, and Hong-Gee Kim. 2021a. Prompt-learning
for fine-grained entity typing. Arxiv preprint,
2108.10604.
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,
Xu Han, Pengjun Xie, Hai-Tao Zheng, and Zhiyuan
Liu. 2021b. Few-nerd: A few-shot named entity
recognition dataset. In Proceedings of ACL.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of ACL, pages 3816–3830,
Online.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The webnlg
challenge: Generating text from rdf data. In Pro-
ceedings of INLG, pages 124–133.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2021. Ppt: Pre-trained prompt tuning for few-shot
learning. arXiv preprint arXiv:2109.04332.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,
Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan,
Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu,
Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan,
Wayne Xin Zhao, and Jun Zhu. 2021a. Pre-trained
models: Past, present and future. ArXiv preprint,
abs/2106.07139.
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and
Maosong Sun. 2021b. Ptr: Prompt tuning with rules
for text classification. ArXiv preprint, 2105.11259.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. SemEval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of SemEval,
pages 33–38.
Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan
Liu, Juanzi Li, and Maosong Sun. 2021. Knowl-
edgeable prompt-tuning: Incorporating knowledge
into prompt verbalizer for text classification. ArXiv
preprint, 2108.02035.
Teven Le Scao and Alexander M Rush. 2021. How
many data points is a prompt worth? In Proceedings
of NAACL, pages 2627–2636.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick Van Kleef,
Sören Auer, et al. 2015. Dbpedia–a large-scale, mul-
tilingual knowledge base extracted from wikipedia.
Semantic web, 6(2):167–195.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. ArXiv preprint, abs/2104.08691.

--- TRANG 8 ---
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of ACL, pages
7871–7880, Online.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings ACL, pages 4582–4597, Online. Asso-
ciation for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ArXiv preprint, abs/2107.13586.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt
understands, too. arXiv preprint arXiv:2103.10385.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. ArXiv preprint, abs/1907.11692.
Robert L Logan IV, Ivana Balažević, Eric Wallace,
Fabio Petroni, Sameer Singh, and Sebastian Riedel.
2021. Cutting down on prompts and parameters:
Simple few-shot learning with language models.
arXiv preprint arXiv:2106.13353.
Andrew Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of ACL.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
high-performance deep learning library. Proceed-
ings of NeurIPS, 32:8026–8037.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton
Bakhtin, Yuxiang Wu, Alexander H Miller, and Se-
bastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066.
Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin,
Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng
Li, Maosong Sun, et al. 2021. Exploring low-
dimensional intrinsic task subspace via prompt tun-
ing. arXiv preprint arXiv:2110.07867.
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
Science China Technological Sciences, pages 1–26.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. ArXiv preprint, abs/1910.10683.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a unified text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 255–269, Online. Association for Com-
putational Linguistics.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv:1905.02450.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,
and Daniel Cer. 2021. Spot: Better frozen model
adaptation through soft prompt transfer. arXiv
preprint arXiv:2110.07904.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R Bowman. 2019. Super-
glue: A stickier benchmark for general-purpose
language understanding systems. arXiv preprint
arXiv:1905.00537.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.
Colin Wei, Sang Michael Xie, and Tengyu Ma. 2021.
Why do pretrained language models help in down-
stream tasks? an analysis of head and prompt tun-
ing.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of EMNLP, pages 38–45,
Online.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. Advances in neural information process-
ing systems.

--- TRANG 9 ---
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language models.
arXiv preprint arXiv:2102.09690.
