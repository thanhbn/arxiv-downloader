# 2308.08469.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.08469.pdf
# Kích thước file: 1324106 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Thông báo Preprint
Bài báo này đã được chấp nhận xuất bản trong ACM Transactions on Intelligent Systems
and Technology (TIST) 2025. Phiên bản cuối cùng sẽ có sẵn tại https://doi.org/
10.1145/3719207 .
1arXiv:2308.08469v6  [cs.LG]  20 Feb 2025

--- TRANG 2 ---
LLM4TS: Căn chỉnh các LLM được Pre-Train như là các
Bộ dự báo chuỗi thời gian hiệu quả về dữ liệu
Ching Chang1, Wei-Yao Wang1, Wen-Chih Peng1, Tien-Fu Chen1
1Đại học Quốc gia Yang Ming Chiao Tung, Hsinchu, Đài Loan
blacksnail789521.cs10@nycu.edu.tw, sf1638.cs05@nctu.edu.tw,
wcpeng@cs.nycu.edu.tw, tfchen@cs.nycu.edu.tw
Tóm tắt
Dự báo chuỗi thời gian đa biến là rất quan trọng trong nhiều lĩnh vực khác nhau, ví dụ: lập kế hoạch kinh tế
và dự báo thời tiết. Các mô hình deep train-from-scratch đã thể hiện hiệu suất hiệu quả
nhưng yêu cầu lượng lớn dữ liệu, điều này hạn chế khả năng ứng dụng thực tế. Gần đây,
các nhà nghiên cứu đã tận dụng khả năng chuyển giao học biểu diễn của các Mô hình Ngôn ngữ Lớn
được pre-train (LLM) để xử lý hiệu quả các tập dữ liệu phi ngôn ngữ hạn chế. Tuy nhiên,
việc kết hợp LLM với dữ liệu chuỗi thời gian đặt ra những thách thức về khả năng thích ứng hạn chế do
sự khác biệt trong thành phần giữa dữ liệu chuỗi thời gian và ngôn ngữ, và không thể xử lý
thông tin thời gian đa quy mô. Để giải quyết những thách thức này, chúng tôi đề xuất LLM4TS, một khung
cho dự báo chuỗi thời gian với các LLM được pre-train. LLM4TS bao gồm một chiến lược
fine-tuning hai giai đoạn: giai đoạn căn chỉnh chuỗi thời gian để căn chỉnh LLM với các sắc thái của
dữ liệu chuỗi thời gian, và giai đoạn fine-tuning dự báo cho các nhiệm vụ dự báo chuỗi thời gian
downstream. Hơn nữa, khung của chúng tôi có một phương pháp tổng hợp hai cấp độ mới mà
tích hợp dữ liệu thời gian đa quy mô trong các LLM được pre-train, tăng cường khả năng
diễn giải thông tin cụ thể theo thời gian của chúng. Trong các thí nghiệm trên 7 tập dữ liệu dự báo chuỗi thời gian,
LLM4TS vượt trội so với các phương pháp state-of-the-art hiện có so với các mô hình
trained-from-scratch trong các kịch bản full-shot, và cũng đạt thứ hạng cao nhất trong các kịch bản few-shot. Ngoài ra, các đánh giá so sánh với các phương pháp học biểu diễn không giám sát khác nhau
làm nổi bật hiệu quả của LLM4TS với học biểu diễn trong các nhiệm vụ dự báo. Các nghiên cứu
ablation tiếp tục xác nhận sự đóng góp của từng thành phần vào LLM4TS và nhấn mạnh
vai trò thiết yếu của việc sử dụng trọng số được pre-train của LLM để có hiệu suất tối ưu. Mã nguồn có
sẵn tại https://github.com/blacksnail789521/LLM4TS .
1 Giới thiệu
Dự báo là một nhiệm vụ quan trọng trong phân tích chuỗi thời gian đa biến, không chỉ vì khả năng hoạt động
mà không cần gán nhãn thủ công mà còn vì tầm quan trọng của nó trong các ứng dụng thực tế như lập kế hoạch kinh tế [21, 33, 6] và dự báo thời tiết [51, 23, 14]. Gần đây, nhiều mô hình deep train-from-
scratch đã được phát triển cho dự báo chuỗi thời gian [46, 28, 40, 52, 51, 44], mặc dù
một số nghiêng về học biểu diễn không giám sát [42, 45, 34, 8, 50, 5, 27] và học chuyển giao
[47, 53, 11, 36, 12]. Nói chung, các phương pháp này nhằm sử dụng các bộ học biểu diễn
thành thạo: đầu tiên trích xuất các biểu diễn phong phú từ dữ liệu chuỗi thời gian và sau đó sử dụng các
biểu diễn này để dự báo.
Đạt được một bộ học biểu diễn thành thạo đòi hỏi dữ liệu huấn luyện đầy đủ [15, 48, 37], tuy nhiên
trong các tình huống thực tế, thường thiếu các tập dữ liệu chuỗi thời gian quy mô lớn. Ví dụ, trong
sản xuất công nghiệp, dữ liệu cảm biến cho các sản phẩm khác nhau không thể được kết hợp để phân tích thêm, dẫn đến dữ liệu hạn chế cho mỗi loại sản phẩm [43, 7, 1]. Nghiên cứu gần đây đã chuyển hướng
về các LLM được pre-train trong Xử lý Ngôn ngữ Tự nhiên (NLP) [29, 3, 35], khai thác
khả năng học biểu diễn mạnh mẽ và khả năng học few-shot của chúng. Hơn nữa, các LLM này có thể
thích ứng với các tập dữ liệu phi ngôn ngữ (ví dụ: hình ảnh [26, 24], âm thanh [9, 30], dữ liệu bảng [13, 31], và
2

--- TRANG 3 ---
dữ liệu chuỗi thời gian [53, 17]) bằng cách fine-tuning với chỉ một số ít tham số và dữ liệu hạn chế. Trong khi
LLM nổi tiếng với khả năng học chuyển giao đặc biệt trên nhiều lĩnh vực khác nhau, các sắc thái
cụ thể theo lĩnh vực của dữ liệu chuỗi thời gian đưa ra hai thách thức trong việc tận dụng các mô hình này
cho dự báo chuỗi thời gian.
Thách thức đầu tiên của việc sử dụng LLM cho dự báo chuỗi thời gian là khả năng thích ứng hạn chế
với các đặc điểm độc đáo của dữ liệu chuỗi thời gian do trọng tâm ban đầu của LLM trong pre-training là
corpus ngôn ngữ. Trong khi LLM đã được chứng minh cả thực tế và lý thuyết [53] là
hiệu quả trong học chuyển giao qua nhiều phương thức khác nhau nhờ cơ chế self-
attention độc lập với dữ liệu, trọng tâm chính của chúng vào văn bản chung trong quá trình pre-training gây ra thiếu hụt trong việc nhận dạng các mẫu chuỗi thời gian quan trọng và các sắc thái quan trọng cho dự báo chính xác. Hạn chế này rõ ràng trong các lĩnh vực như khí tượng học và dự báo điện [51], nơi việc không tính đến các mẫu thời tiết và xu hướng tiêu thụ năng lượng dẫn đến các dự đoán không chính xác.
Thách thức thứ hai nằm ở khả năng hạn chế để xử lý thông tin thời gian đa quy mô.
Trong khi LLM thành thạo trong việc hiểu trình tự và ngữ cảnh của từ, chúng gặp khó khăn để
hiểu thông tin thời gian do thiếu việc sử dụng dữ liệu liên quan đến thời gian đa quy mô như
đơn vị thời gian (ví dụ: giây, phút, giờ, v.v.) và ngày cụ thể (ví dụ: ngày lễ, sự kiện
quan trọng). Thông tin thời gian này là quan trọng trong phân tích chuỗi thời gian để xác định và dự đoán
các mẫu [40, 39]; ví dụ, trong quản lý năng lượng, nó được sử dụng để giải quyết các đỉnh tiêu thụ
trong ban ngày và vào mùa hè/mùa đông, trái ngược với nhu cầu thấp hơn trong ban đêm và trong
các mùa ôn hòa hơn [51]. Điều này nhấn mạnh tầm quan trọng của các mô hình thành thạo trong việc diễn giải các mẫu thời gian đa quy mô (từ hàng giờ đến theo mùa) để dự báo nhu cầu năng lượng chính xác. Tuy nhiên, hầu hết
LLM (ví dụ: [29, 35]) được xây dựng trên kiến trúc Transformer không tự nhiên kết hợp
thông tin thời gian đa quy mô, dẫn đến các mô hình không thể nắm bắt các biến đổi quan trọng qua
các quy mô thời gian khác nhau.
Để giải quyết các vấn đề trên, chúng tôi đề xuất LLM4TS, một khung cho dự báo chuỗi thời gian
với các LLM được pre-train. Về thách thức đầu tiên, khung của chúng tôi giới thiệu một phương pháp fine-tuning hai giai đoạn: giai đoạn căn chỉnh chuỗi thời gian và giai đoạn fine-tuning dự báo. Giai đoạn đầu tiên tập trung vào việc căn chỉnh LLM với các đặc điểm của dữ liệu chuỗi thời gian bằng cách sử dụng
mục tiêu autoregressive, cho phép LLM được fine-tune thích ứng với các biểu diễn chuỗi thời gian. Giai đoạn thứ hai được kết hợp để học các nhiệm vụ dự báo chuỗi thời gian tương ứng. Bằng
cách này, mô hình của chúng tôi hỗ trợ hiệu suất hiệu quả trong các kịch bản full- và few-shot. Đáng chú ý,
trong suốt cả hai giai đoạn, hầu hết các tham số trong LLM được pre-train đều bị đóng băng, do đó bảo tồn
khả năng học biểu diễn vốn có của mô hình. Để vượt qua hạn chế của LLM trong việc
tích hợp thông tin thời gian đa quy mô, chúng tôi giới thiệu một chiến lược tổng hợp hai cấp độ mới. Phương pháp này nhúng thông tin thời gian đa quy mô vào dữ liệu chuỗi thời gian được chia patch,
đảm bảo rằng mỗi patch không chỉ đại diện cho các giá trị chuỗi mà còn bao gồm ngữ cảnh
cụ thể về thời gian quan trọng. Do đó, LLM4TS nổi lên như một bộ dự báo chuỗi thời gian hiệu quả về dữ liệu,
thể hiện hiệu suất few-shot mạnh mẽ trên nhiều tập dữ liệu khác nhau (Hình 1).
Tóm lại, các đóng góp chính của bài báo như sau:
•Căn chỉnh LLM hướng tới dữ liệu chuỗi thời gian: Theo hiểu biết tốt nhất của chúng tôi, LLM4TS
là phương pháp đầu tiên căn chỉnh các Mô hình Ngôn ngữ Lớn được pre-train với các đặc điểm
chuỗi thời gian, hiệu quả sử dụng khả năng học biểu diễn và học few-shot
hiện có.
•Thông tin thời gian đa quy mô trong LLM: Để thích ứng với thông tin cụ thể về thời gian,
một phương pháp tổng hợp hai cấp độ được đề xuất để tích hợp dữ liệu thời gian đa quy mô trong
các LLM được pre-train.
•Hiệu suất mạnh mẽ trong dự báo: LLM4TS xuất sắc trong 7 benchmark dự báo chuỗi thời gian
thực tế, vượt trội so với các phương pháp state-of-the-art, bao gồm những phương pháp được huấn luyện từ
đầu. Nó cũng thể hiện khả năng few-shot mạnh, đặc biệt với chỉ 5% dữ liệu,
3

--- TRANG 4 ---
(a) 5% dữ liệu huấn luyện (b) 10% dữ liệu huấn luyện
Hình 1: So sánh hiệu suất mô hình trong dự báo few-shot.
nơi nó vượt qua baseline tốt nhất sử dụng 10% dữ liệu. Hiệu quả này làm cho
LLM4TS rất liên quan đến các ứng dụng dự báo thực tế, thực tế.
2 Nghiên cứu liên quan
2.1 Học chuyển giao qua nhiều phương thức khác nhau với LLM
LLM đã thể hiện hiệu quả của chúng trong học chuyển giao qua nhiều phương thức khác nhau,
như hình ảnh [26, 24], âm thanh [9, 30], dữ liệu bảng [13, 31], và dữ liệu chuỗi thời gian [53, 17]. Một
động lực chính để sử dụng LLM trong nhiều phương thức khác nhau là khả năng đạt được hiệu suất đáng chú ý
với dữ liệu hạn chế [53]. Để bảo tồn khả năng học biểu diễn độc lập với dữ liệu
của chúng, hầu hết các tham số trong những LLM này được giữ cố định. Bằng chứng thực nghiệm [26, 53]
chỉ ra rằng LLM giữ hầu hết các tham số không thay đổi thường vượt trội so với những LLM được huấn luyện từ
đầu, nhấn mạnh giá trị của việc duy trì những điểm mạnh học biểu diễn đã có sẵn của các mô hình này (thêm thí nghiệm có thể tìm thấy trong Phần 5.4.3). Về mặt lý thuyết, được chỉ ra
rằng các module self-attention trong các transformer được pre-train này phát triển khả năng cho các hoạt động độc lập với dữ liệu (tương tự như phân tích thành phần chính [53]), cho phép chúng hoạt động
hiệu quả như các universal compute engines [26] hoặc general computation calculators [10]. Trong
lĩnh vực chuỗi thời gian, GPT4TS [53] sử dụng GPT-2 được pre-train và thể hiện hiệu suất mạnh
trong dự báo chuỗi thời gian dưới điều kiện few-shot mà không sửa đổi hầu hết các tham số. Time-LLM [18] lập trình lại LLM cho dự báo chuỗi thời gian bằng cách chuyển đổi chuỗi thời gian
thành các prototype văn bản và sử dụng prompts để hướng dẫn dự đoán. Nó vượt trội so với các mô hình chuyên biệt,
đặc biệt trong các cài đặt few-shot và zero-shot. TEMPO [4] điều chỉnh các mô hình giống GPT cho dự báo chuỗi thời gian bằng cách phân tách xu hướng và sử dụng prompts để thích ứng phân phối tốt hơn,
xuất sắc trong các kịch bản zero-shot. TEST [32] căn chỉnh chuỗi thời gian với các embedding LLM thông qua
tokenization và contrastive learning, cho phép dự báo chuỗi thời gian hiệu quả sử dụng các LLM được pre-train mà không cần fine-tuning. Với LLM4TS của chúng tôi, chúng tôi giải quyết các thách thức về khả năng thích ứng hạn chế
với các đặc điểm chuỗi thời gian và khó khăn trong việc xử lý thông tin thời gian đa quy mô,
từ đó nâng cao hiệu suất trong dự báo chuỗi thời gian.
4

--- TRANG 5 ---
Hình 2: Công thức hóa vấn đề cho dự báo chuỗi thời gian đa biến .
2.2 Dự báo chuỗi thời gian dài hạn
Nhiều nỗ lực đã được dành cho việc sử dụng các mô hình Transformer cho dự báo chuỗi thời gian dài hạn [51, 40, 52, 28, 49, 22]. Trong khi các mô hình dựa trên Transformer đã thu hút sự chú ý,
DLinear [46] tiết lộ rằng một mô hình tuyến tính một lớp đơn giản có thể vượt qua nhiều phương pháp
dựa trên Transformer phức tạp này. Những mô hình deep train-from-scratch này thể hiện hiệu suất
xuất sắc khi được huấn luyện trên các tập dữ liệu đầy đủ, nhưng hiệu quả của chúng giảm trong các kịch bản
dữ liệu hạn chế. Ngược lại, LLM4TS đặt ra các benchmark mới cùng với những phương pháp state-of-the-art này
trong cả kịch bản full- và few-shot.
2.3 Học biểu diễn chuỗi thời gian
Trong lĩnh vực chuỗi thời gian, self-supervised learning nổi lên như một phương pháp nổi bật cho việc học biểu diễn. Trong khi Transformers được công nhận rộng rãi như các ứng cử viên hàng đầu cho phân tích chuỗi thời gian end-to-end [41, 25, 28, 38, 2], các backbone dựa trên CNN [45] hoặc dựa trên RNN [34] liên tục
nổi bật như kiến trúc được ưa thích trong self-supervised learning chuỗi thời gian. Tuy nhiên,
khả năng vốn có của Transformers để mô hình hóa các phụ thuộc tầm xa và nắm bắt các mẫu
hoàn toàn phù hợp với dữ liệu chuỗi thời gian, điều này liên quan đến các mối quan hệ tuần tự phức tạp. Vì
giai đoạn căn chỉnh chuỗi thời gian trong LLM4TS có thể được xem như một phương pháp self-supervised learning,
chúng tôi đánh giá khả năng học biểu diễn của LLM4TS và thể hiện toàn bộ tiềm năng của
Transformers trong unsupervised representation learning, vượt qua hiệu suất của các mô hình dựa trên CNN và RNN thông thường.
3 Công thức hóa vấn đề
Cho một chuỗi thời gian đa biến hoàn chỉnh và được lấy mẫu đều, chúng tôi sử dụng một cửa sổ dữ liệu trượt
để trích xuất các mẫu tuần tự, như được minh họa trong Hình 2. Cửa sổ này di chuyển với bước
là 1 và có tổng chiều dài là Tin+Tout— bao gồm dữ liệu quá khứ xin= (d1, . . . , d Tin) với chiều dài cửa sổ nhìn lại Tin và dữ liệu tương lai xout= (dTin+1, . . . , d Tin+Tout) với chiều dài dự đoán Tout. Đối với mỗi bước thời gian t, dt đại diện cho một vector C chiều, trong đó C biểu thị
số lượng đặc trưng. Mục tiêu của chúng tôi là sử dụng dữ liệu quá khứ xin∈RTin×C để dự đoán dữ liệu tương lai
xout∈RTout×C.
4 LLM4TS được đề xuất
Hình 3 minh họa khung LLM4TS của chúng tôi, tận dụng GPT-2 được pre-train [29] như là
mô hình backbone. Chúng tôi đầu tiên giới thiệu giai đoạn căn chỉnh chuỗi thời gian, tập trung vào việc căn chỉnh
LLM với các đặc điểm của dữ liệu chuỗi thời gian sử dụng mục tiêu autoregressive (Phần
5

--- TRANG 6 ---
(a) Căn chỉnh chuỗi thời gian (b) Fine-tuning dự báo
Hình 3: Khung LLM4TS. Các số trong chuỗi thời gian được chia patch (ví dụ: 1, 2, ..., 16
trong patch đầu tiên) chỉ ra thứ tự tuần tự của các timestamps. Khung bao gồm
hai giai đoạn: (a) Căn chỉnh chuỗi thời gian, sử dụng phương pháp autoregressive để căn chỉnh
LLM được pre-train với dữ liệu chuỗi thời gian được chia patch. (b) Fine-tuning dự báo, bắt đầu với
linear probing (tức là chỉ lớp đầu ra được unfrozen), tiếp theo là full fine-tuning (tất cả các lớp
và các thành phần PEFT trong LLM đều được unfrozen).
4.1). Tiếp theo, giai đoạn fine-tuning dự báo được thiết kế để tăng cường thêm khả năng của mô hình
để xử lý các nhiệm vụ dự báo chuỗi thời gian (Phần 4.2).
4.1 Căn chỉnh chuỗi thời gian
Các LLM hiện có được pre-train trên corpus ngôn ngữ chung, có nghĩa là chúng không học được
thông tin ngữ cảnh hóa bên ngoài các lĩnh vực ngôn ngữ; do đó, giai đoạn căn chỉnh chuỗi thời gian được đề xuất để căn chỉnh LLM với các đặc điểm của dữ liệu chuỗi thời gian. Cho việc lựa chọn
GPT-2 [29] của chúng tôi như mô hình backbone, đây là một mô hình ngôn ngữ nhân quả, chúng tôi đảm bảo rằng giai đoạn này
áp dụng cùng một phương pháp huấn luyện autoregressive được sử dụng trong giai đoạn pre-training của nó. Hình
3(a) minh họa mục tiêu autoregressive trong giai đoạn căn chỉnh chuỗi thời gian: cho một chuỗi đầu vào
của dữ liệu chuỗi thời gian được chia patch (ví dụ: patch thứ 1, patch thứ 2, patch thứ 3, v.v.), mô hình backbone
tạo ra một chuỗi đầu ra được dịch chuyển một patch sang phải (ví dụ: patch thứ 2, patch thứ 3,
patch thứ 4, v.v.).
Instance Normalization Chuẩn hóa dữ liệu là thiết yếu cho hiệu suất ổn định khi thích ứng
các mô hình được pre-train qua nhiều phương thức khác nhau. Cùng với layer normalization được sử dụng trong
LLM được pre-train, chúng tôi kết hợp instance normalization để cải thiện tính nhất quán và độ tin cậy
trong việc xử lý các tập dữ liệu chuỗi thời gian đa dạng. Trong mô hình của chúng tôi, instance normalization được sử dụng
mà không kết hợp biến đổi affine có thể huấn luyện. Điều này quan trọng bởi vì khi một batch
dữ liệu được thu thập và instance normalization được áp dụng với biến đổi affine có thể huấn luyện,
dữ liệu được biến đổi kết quả trở nên không phù hợp để làm ground truth cho đầu ra. Cho rằng
một mục tiêu autoregressive được sử dụng ở giai đoạn này, việc áp dụng biến đổi affine có thể huấn luyện
là không khả thi.
Cho một mẫu chuỗi thời gian đầu vào xin∈RTin×C, chúng tôi áp dụng instance normalization (IN) để
tạo ra một mẫu chuỗi thời gian được chuẩn hóa xnormed ∈RTin×C với mean bằng không và độ lệch chuẩn đơn vị:
xnormed = IN( xin). (1)
6

--- TRANG 7 ---
Hình 4: Mã hóa thời gian đa quy mô cho dữ liệu chuỗi thời gian được chia patch . Quá trình này
bao gồm tổng hợp hai cấp độ. Ở đây, chỉ patch đầu tiên được hiển thị để đơn giản; trong thực tế,
tất cả các patches trong một batch được xử lý đồng thời. Tổng hợp cấp 1 tính toán embedding thời gian
cho mỗi đơn vị thời gian và cộng chúng lại với nhau. Tiếp theo, tổng hợp cấp 2 áp dụng phương pháp
pooling để trích xuất embedding thời gian cuối cùng.
Tokenization chuỗi thời gian Kích thước cửa sổ ngữ cảnh trong các LLM được pre-train (ví dụ: 1024 trong
GPT-2) là đủ cho các nhiệm vụ NLP nhưng không đầy đủ cho dự báo chuỗi thời gian dài hạn.
Trong các thí nghiệm của chúng tôi, chiều dài dự đoán là 720 kết hợp với kích thước cửa sổ nhìn lại là 512
dễ dàng vượt quá những giới hạn này. Để giải quyết điều này, chúng tôi áp dụng channel-independence cùng với patching[28] cho tokenization chuỗi thời gian, hiệu quả giải quyết ràng buộc kích thước cửa sổ ngữ cảnh
và đồng thời giảm độ phức tạp thời gian và không gian của Transformer một cách bậc hai.
Channel-independence chuyển đổi dữ liệu chuỗi thời gian đa biến thành nhiều dữ liệu chuỗi thời gian đơn biến,
do đó biến đổi chiều của dữ liệu thành RTin×1, với chiều channel C được hợp nhất
vào chiều batch size. Bước patching tiếp theo nhóm các bước thời gian liền kề thành một
token dựa trên patch duy nhất, giảm chiều thời gian của mẫu đầu vào từ Tin thành Tp, trong đó
Tp biểu thị số lượng patches, và đồng thời mở rộng chiều đặc trưng từ 1 thành
P, với P đại diện cho chiều dài patch.
Cho một mẫu chuỗi thời gian được chuẩn hóa xnormed ∈RTin×C, chúng tôi đầu tiên áp dụng channel-independence
(CI), và sau đó patching để tạo ra một chuỗi các patches p∈RTp×P:
p= patching(CI( xnormed )). (2)
Ba encoding cho dữ liệu chuỗi thời gian được chia patch Cho mục tiêu thích ứng một LLM được pre-train
cho dữ liệu chuỗi thời gian, lớp token encoding gốc (được thiết kế cho văn bản) trở nên không phù hợp do các phương thức không khớp. Ngoài ra, chúng tôi thiết kế một lớp multi-scale temporal encoding mới để giải quyết sự bất lực trong việc xử lý thông tin thời gian đa quy mô.
Cho một chuỗi tokens, việc áp dụng token encoding là cần thiết để căn chỉnh chiều của chúng với
chiều embedding tiềm ẩn của LLM được pre-train. Trong các thực hành NLP tiêu chuẩn, encoding này
sử dụng một bảng tra cứu có thể huấn luyện để ánh xạ tokens vào một không gian chiều cao. Tuy nhiên, phương pháp này
chỉ phù hợp với scalar tokens, trong khi dữ liệu chuỗi thời gian được chia patch của chúng tôi là vectors . Do đó,
chúng tôi bỏ lớp token encoding gốc trong LLM, và sử dụng một lớp tích chập một chiều Conv token như lớp token encoding mới của chúng tôi. Trái ngược với việc sử dụng một lớp
tuyến tính [53], chúng tôi chọn một lớp tích chập do khả năng vượt trội của nó trong việc giữ lại thông tin ngữ nghĩa cục bộ trong dữ liệu chuỗi thời gian. Điều này dẫn đến việc tạo ra token embedding
etoken∈RTp×D, trong đó D biểu thị chiều của các embeddings:
etoken = Conv token(p). (3)
7

--- TRANG 8 ---
Đối với lớp positional encoding, chúng tôi sử dụng một bảng tra cứu có thể huấn luyện Epos để ánh xạ
các vị trí patch. Điều này dẫn đến việc tạo ra positional embedding epos∈RTp×D:
epos=Epos(i), (4)
trong đó i∈RTp đại diện cho các chỉ số của các vị trí patch.
Để giải quyết thách thức mà LLM gặp phải trong việc xử lý thông tin thời gian đa quy mô, chúng tôi
giới thiệu một lớp multi-scale temporal encoding. Khi xử lý dữ liệu liên quan đến thời gian, chúng ta đối mặt
với hai thách thức do nhu cầu tổng hợp nhiều thông tin thành một biểu diễn thống nhất (Hình 4):
1. Mỗi timestamp bao gồm một loạt các thuộc tính thời gian đa quy mô (ví dụ: giây, phút,
giờ, ngày lễ, v.v.).
2. Mỗi patch bao gồm nhiều timestamps.
Để giải quyết thách thức đầu tiên liên quan đến các thuộc tính thời gian đa dạng trong một timestamp,
chúng tôi sử dụng tổng hợp cấp 1: một bảng tra cứu có thể huấn luyện cho mỗi thuộc tính thời gian (ví dụ:
Esec, Emin, ...), ánh xạ nó vào một không gian chiều cao, và sau đó cộng chúng để tạo ra
một temporal embedding duy nhất. Để đáp ứng thách thức thứ hai về nhiều timestamps trong
một patch, chúng tôi sử dụng tổng hợp cấp 2: một phương pháp pooling để trích xuất temporal embedding cuối cùng.
Đối với phương pháp pooling, chúng tôi chọn phương pháp "select first", trong đó timestamp ban đầu được
chỉ định làm đại diện cho toàn bộ patch. Điều này là do timestamp đầu tiên thường
mang thông tin quan trọng và đại diện nhất cho toàn bộ khoảng thời gian được bao phủ bởi
patch, đặc biệt trong dữ liệu chuỗi thời gian nơi các sự kiện trước đó có thể có ảnh hưởng đáng kể đến
chuỗi tiếp theo. Quá trình này tạo ra temporal embedding cuối cùng etemp∈RTp×D:
etemp= Pooling
X
a∈{sec,min,hour,...}Ea(ta)
, (5)
trong đó a đại diện cho các thuộc tính thời gian khác nhau (giây, phút, giờ, ngày lễ, v.v.), Ea
biểu thị bảng tra cứu có thể huấn luyện cho mỗi thuộc tính thời gian, ta∈RTp×P là chuỗi
các patches chứa thông tin thời gian cho thuộc tính thời gian đó, và Pooling áp dụng
phương pháp pooling cho các embeddings được tổng hợp.
Cuối cùng, các token, positional, và temporal embeddings được cộng để tạo ra embedding cuối cùng
e∈RTp×D, sau đó được đưa vào các Transformer blocks được pre-train:
e=etoken +epos+etemp. (6)
LLM được Pre-train Để bảo tồn khả năng học biểu diễn độc lập với dữ liệu của LLM,
hầu hết các tham số trong những LLM này được giữ cố định. Bằng chứng thực nghiệm [26, 53] cho thấy rằng việc huấn luyện
những LLM này từ đầu thường làm tổn hại hiệu suất, làm nổi bật tầm quan trọng của việc cố định hầu hết
các tham số để giữ lại khả năng học biểu diễn của LLM. Vì vậy, chúng tôi chọn
đóng băng hầu hết các tham số, đặc biệt là những tham số liên quan đến multi-head attention và feed-
forward layers trong Transformer block, vì chúng chịu trách nhiệm nhiều nhất cho việc học biểu diễn [53].
Đối với các tham số có thể huấn luyện còn lại trong LLM được pre-train, chúng tôi sử dụng hai phương pháp Parameter-
Efficient Fine-Tuning (PEFT) để điều chỉnh một cách có chọn lọc hoặc giới thiệu một tập hợp hạn chế các tham số có thể huấn luyện. Cụ thể, chúng tôi sử dụng Layer Normalization Tuning [26] để điều chỉnh các tham số
đã tồn tại trong Transformer blocks, làm cho biến đổi affine trong layer normalization có thể huấn luyện.
Đồng thời, chúng tôi sử dụng Low-Rank Adaptation (LoRA) [16], giới thiệu các ma trận low-rank có thể huấn luyện được áp dụng cho các ma trận query (Q) và key (K) trong cơ chế self-attention
8

--- TRANG 9 ---
. Với hai kỹ thuật PEFT này, chỉ 1.5% tổng số tham số của LLM được pre-train được sử dụng để huấn luyện.
Cho embedding e (được điều chỉnh thành chiều embedding cần thiết D bởi ba
lớp encoding), chúng tôi truyền nó vào LLM được pre-train, bao gồm một chuỗi các
Transformer blocks (TBs) được pre-train (với tổng cộng L blocks). Quá trình này tạo ra các embeddings cuối cùng
z∈RTp×D:
z= TBs( e). (7)
Sau khi được xử lý bởi LLM được pre-train, chúng tôi sử dụng một lớp đầu ra tuyến tính Wtsa∈RP×D
để biến đổi embedding đầu ra trở lại dữ liệu chuỗi thời gian được chia patch:
ˆpshifted =zW⊤
tsa, (8)
trong đó ˆpshifted ∈RTp×P đại diện cho mục tiêu dự đoán của chúng tôi, tương ứng với các patches chuỗi thời gian gốc
( p) được dịch chuyển một patch sang phải, phù hợp với mục tiêu autoregressive của
giai đoạn này. Để đảm bảo dự đoán tái tạo chính xác dữ liệu patched được dịch chuyển thực tế
pshifted ∈RTp×P, chúng tôi sử dụng Mean Squared Error (MSE) làm hàm loss:
Ltsa= MSE( pshifted ,ˆpshifted ). (9)
4.2 Fine-tuning dự báo
Sau khi căn chỉnh LLM được pre-train với dữ liệu chuỗi thời gian được chia patch trong giai đoạn căn chỉnh chuỗi thời gian, chúng tôi chuyển giao các trọng số đã huấn luyện của mô hình backbone, bao gồm những trọng số từ các
lớp encoding, sang giai đoạn fine-tuning dự báo. Khi fine-tuning mô hình backbone cho
nhiệm vụ dự báo, có hai chiến lược huấn luyện chính: full fine-tuning (trong đó tất cả các tham số mô hình
được cập nhật) và linear probing (trong đó chỉ lớp đầu ra tuyến tính cuối cùng được cập nhật).
Các nghiên cứu đã chỉ ra rằng một phương pháp tuần tự—linear probing ban đầu tiếp theo là full fine-tuning
(LP-FT, như được minh họa trong Hình 3(b))—liên tục vượt qua các chiến lược chỉ sử dụng
một trong hai phương pháp [20]. Sự vượt trội của LP-FT là do phương pháp hai giai đoạn của nó: đầu tiên tìm
một lớp đầu ra được tối ưu hóa để giảm thiểu các điều chỉnh sau này trong fine-tuning (bảo tồn hiệu quả của feature extractor cho các kịch bản out-of-distribution (OOD)), và sau đó sử dụng full fine-tuning
để thích ứng mô hình với nhiệm vụ cụ thể (tăng cường độ chính xác in-distribution (ID)) [20].
Đối với kiến trúc mô hình trong giai đoạn fine-tuning dự báo, chúng tôi bảo tồn hầu hết
cấu trúc như trong giai đoạn căn chỉnh chuỗi thời gian, bao gồm ba lớp encoding và
LLM được pre-train. Tuy nhiên, có hai khác biệt kiến trúc trong giai đoạn này: instance normalization và lớp đầu ra.
Khác biệt kiến trúc đầu tiên là trong instance normalization, nơi chúng tôi áp dụng Reversible
Instance Normalization (RevIN) [19] để tăng cường độ chính xác dự báo. RevIN bao gồm instance normalization cụ thể theo batch và denormalization tiếp theo, cả hai đều chia sẻ cùng biến đổi affine có thể huấn luyện. Bước denormalization bổ sung giải quyết sự thay đổi phân phối giữa
dữ liệu huấn luyện và kiểm tra, đây là một thách thức phổ biến trong lĩnh vực chuỗi thời gian (ví dụ: thay đổi theo mùa). Do đó, trong bước tokenization chuỗi thời gian, chúng tôi áp dụng normalization của RevIN,
tiếp theo là channel-independence và patching:
p= patching(CI(RevIN norm(xin))). (10)
Đáng chú ý, bước denormalization chỉ áp dụng được cho dữ liệu chuỗi thời gian không được chia patch; do đó, trong
giai đoạn căn chỉnh chuỗi thời gian, instance normalization tiêu chuẩn được sử dụng.
Khác biệt kiến trúc thứ hai nằm ở lớp đầu ra, có chức năng biến đổi
embedding cuối cùng z thành dữ liệu tương lai được dự đoán, được trình bày dưới định dạng chuỗi thời gian
chung (không được chia patch). Điều này bao gồm việc làm phẳng dữ liệu và truyền qua lớp đầu ra tuyến tính
9

--- TRANG 10 ---
Bảng 1: Tổng quan thống kê của 7 tập dữ liệu cho dự báo chuỗi thời gian dài hạn.
Tập dữ liệu Đặc trưng Timesteps Độ chi tiết
Weather 21 52,696 10 phút
Traffic 862 17,544 1 giờ
Electricity 321 26,304 1 giờ
ETTh1 & ETTh2 7 17,420 1 giờ
ETTm1 & ETTm2 7 69,680 5 phút
Wfft∈RTout×Tp·D, tiếp theo là sắp xếp lại, và sau đó áp dụng denormalization của RevIN để
có được dự đoán cuối cùng ˆxout∈RTout×C:
ˆxout= RevIN denorm (Rearrange((Flatten( z))W⊤
fft). (11)
Để đảm bảo rằng dự đoán này tái tạo chính xác dữ liệu tương lai xout∈RTout×C, chúng tôi sử dụng
MSE làm hàm loss:
Lfft= MSE( xout,ˆxout). (12)
5 Thí nghiệm
Tập dữ liệu Trong phân tích dự báo dài hạn của chúng tôi, chúng tôi thí nghiệm trên 7 tập dữ liệu benchmark thực tế, có thể truy cập công khai. Chúng tôi trình bày các thống kê chi tiết cho những tập dữ liệu này trong Bảng 1, bao gồm
số lượng đặc trưng, tổng chiều dài của các tập dữ liệu, và tần suất lấy mẫu của chúng.
Weather1 bao gồm dữ liệu khí hậu địa phương trải dài 4 năm cho khoảng 1.600
địa điểm ở Mỹ. Nó bao gồm 11 biến thời tiết trong mỗi bản ghi, ngoài biến mục tiêu
'wet bulb.' Traffic2 bao gồm các quan sát hàng giờ từ Bộ Giao thông California, chi tiết tỷ lệ lấp đầy đường được ghi bởi nhiều cảm biến khác nhau được đặt trên các đường cao tốc
ở vùng Vịnh San Francisco. Electricity3 bao gồm dữ liệu sử dụng điện hàng giờ cho 321 khách hàng, trải dài từ 2012 đến 2014, với 'MT 320' được đặt làm biến mục tiêu. ETT [51] tập trung
vào dữ liệu triển khai điện lực thời gian dài. Bộ sưu tập bao gồm hai tập dữ liệu được lấy mẫu hàng giờ
(ETTh1, ETTh2) và hai tập dữ liệu được lấy mẫu mỗi 15 phút (ETTm1, ETTm2), bao phủ
một khoảng thời gian hơn hai năm từ nhiều tỉnh khác nhau ở Trung Quốc. Mỗi tập dữ liệu trong chuỗi ETT
có một biến nhiệt độ dầu cùng với sáu biến tải điện.
Chỉ số đánh giá Trong dự báo chuỗi thời gian, Mean Squared Error (MSE) và Mean Absolute Error (MAE) là các chỉ số thường được sử dụng để đánh giá hiệu suất. Mean Squared Error (MSE) có thể được biểu diễn như:
MSE =1
NNX
n=1(xout−ˆxout)2. (13)
Ở đây, xout chỉ ra dữ liệu tương lai thực tế tương ứng với dữ liệu quá khứ xin, trong khi ˆ xout
đại diện cho dữ liệu tương lai được dự đoán dựa trên dữ liệu quá khứ đầu vào. N là tổng số
mẫu. Mean Absolute Error (MAE) được cho bởi:
MAE =1
NNX
n=1|xout−ˆxout|. (14)
1https://www.ncei.noaa.gov/data/local-climatological-data/
2http://pems.dot.ca.gov/
3https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014
10

--- TRANG 11 ---
Baselines Đối với dự báo chuỗi thời gian dài hạn, chúng tôi tập trung vào một loạt các mô hình
state-of-the-art. Cùng một tập hợp mô hình được sử dụng cho few-shot learning và các nghiên cứu ablation. GPT4TS
[53] sử dụng patching và channel independence để ban đầu biến đổi dữ liệu chuỗi thời gian thành tokens. Sau đó nó sử dụng một Mô hình Ngôn ngữ Lớn được pre-train (GPT-2), duy trì các trọng số được pre-train trong các lớp self-attention và feedforward của các residual blocks từ LLM được pre-train. DLinear [46] thách thức việc sử dụng phổ biến các mô hình dựa trên Transformer cho dự báo chuỗi thời gian dài hạn. Nó giới thiệu một mô hình tuyến tính một lớp đơn giản mà đáng ngạc nhiên vượt trội so với
các mô hình dựa trên Transformer phức tạp trên nhiều tập dữ liệu thực. PatchTST [28] giới thiệu
một phương pháp dựa trên Transformer cho dự báo chuỗi thời gian, tập trung vào hiệu quả bằng cách sử dụng
patching và channel-independence để biến đổi dữ liệu chuỗi thời gian thành patches. FEDformer
[52] kết hợp Transformers với phân tách seasonal-trend và frequency enhancement cho
dự báo chuỗi dài hạn hiệu quả và hiệu quả, vượt qua các hạn chế của Transformer truyền thống bằng cách nắm bắt cả xu hướng toàn cầu và cấu trúc chi tiết với độ phức tạp tuyến tính. Time-
LLM [18] lập trình lại LLMs cho dự báo chuỗi thời gian bằng cách chuyển đổi chuỗi thời gian thành
text prototypes và sử dụng prompts để hướng dẫn dự đoán. Nó vượt trội so với các mô hình chuyên biệt, đặc biệt trong các cài đặt few-shot và zero-shot. TEMPO [4] điều chỉnh các mô hình giống GPT cho dự báo chuỗi thời gian bằng cách phân tách xu hướng và sử dụng prompts để thích ứng phân phối tốt hơn, xuất sắc trong các kịch bản zero-shot. TEST [32] căn chỉnh chuỗi thời gian với LLM embeddings thông qua
tokenization và contrastive learning, cho phép dự báo chuỗi thời gian hiệu quả sử dụng LLMs được pre-train mà không cần fine-tuning.
Đối với unsupervised representation learning trong phân tích chuỗi thời gian, chúng tôi khám phá các mô hình tiên tiến
xuất sắc trong việc trích xuất các biểu diễn có ý nghĩa mà không dựa vào dữ liệu được gán nhãn. PatchTST
[28] trong ngữ cảnh này là một biến thể khác biệt với biến thể được sử dụng trong các thí nghiệm dự báo, với
nhấn mạnh vào representation learning. Nó áp dụng chiến lược MLM (Masked Language Model),
tương tự như BERT, để học các biểu diễn. BTSF [42] giới thiệu khung Bilinear Temporal-Spectral
Fusion để cải thiện representation learning chuỗi thời gian bằng cách tích hợp thông tin temporal và
spectral. Phương pháp này giảm thiểu bias lấy mẫu và tối ưu hóa biểu diễn đặc trưng thông qua
kỹ thuật augmentation và fusion cấp instance. TS2Vec [45] là khung universal đầu tiên
dành riêng cho việc học biểu diễn của dữ liệu chuỗi thời gian. Nó nhấn mạnh việc phân biệt thông tin ngữ cảnh đa quy mô ở cả cấp instance và timestamp, thể hiện hiệu quả trong nhiều
nhiệm vụ chuỗi thời gian khác nhau. TNC [34] sử dụng kiểm định Augmented Dickey-Fuller để phát hiện các khu vực lân cận temporal và thực hiện Positive-Unlabeled learning để giải quyết bias lấy mẫu. TS-TCC [8] tạo ra hai views khác nhau thông qua strong và weak
augmentations và tăng cường biểu diễn thông qua contrastive learning, tập trung vào sự khác biệt temporal và contextual giữa các views này.
Chi tiết thực hiện Đối với các thí nghiệm của chúng tôi trong dự báo chuỗi thời gian dài hạn, few-shot
learning, và các nghiên cứu ablation, chúng tôi sử dụng các cài đặt từ PatchTST [28] để so sánh một cách nhất quán. Chúng tôi đầu tiên khám phá hiệu suất của mô hình trong các kịch bản few-shot, tiếp theo là một
đánh giá toàn diện dưới cài đặt full-shot để đảm bảo phân tích kỹ lưỡng và cân bằng.
Chúng tôi đặt chiều dài cửa sổ nhìn lại Tin thành 336 hoặc 512 (báo cáo kết quả tốt nhất), và
cấu hình chiều dài patch P là 16 với stride S là 8. Đối với unsupervised representation learning,
các cài đặt được điều chỉnh nhẹ thành Tin= 512, P= 12, và S= 12. Phù hợp với cấu hình GPT4TS [53], chúng tôi chỉ sử dụng 6 lớp đầu tiên trong 12 lớp của GPT-2 base [29].
5.1 Few-Shot Learning trong dự báo chuỗi thời gian dài hạn
Bảng 2 hiển thị kết quả dự báo chuỗi thời gian dài hạn chỉ sử dụng 5% dữ liệu huấn luyện,
trong khi Bảng 3 trình bày kết quả tương tự nhưng chỉ với 10% dữ liệu huấn luyện được sử dụng. Trong các thí nghiệm của chúng tôi, các phân chia nhất quán cho tập huấn luyện, validation, và test được duy trì trên cả kịch bản full và few-shot learning. Chúng tôi cố ý giới hạn tỷ lệ dữ liệu huấn luyện xuống 5%
và 10% để đánh giá hiệu suất mô hình trong các kịch bản few-shot. Đối với mỗi tập dữ liệu, chúng tôi huấn luyện một
11

--- TRANG 12 ---
Bảng 2: Dự báo dài hạn few-shot sử dụng 5% dữ liệu huấn luyện. Đối với hầu hết
tập dữ liệu, kết quả được báo cáo qua các chiều dài dự đoán Tout∈ {96,192,336,720}. Tuy nhiên, đối với
các tập dữ liệu được đánh dấu bằng * (ETTh1, ETTh2, và Traffic), chỉ Tout∈ {96,192,336} được sử dụng
vì không có đủ dữ liệu để tạo thành một tập huấn luyện khi Tout= 720. Kết quả tốt nhất được in đậm , trong khi kết quả tốt thứ hai được gạch chân. Lưu ý rằng TEMPO* được đánh giá dưới cài đặt zero-shot vì nó là phương pháp dựa trên prompt.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.173 0.227 0.175 0.230 0.184 0.242 0.171 0.224 0.172 0.263 0.211 0.254 0.182 0.276
1920.218 0.265 0.227 0.276 0.228 0.283 0.230 0.277 0.224 0.271 0.254 0.298 0.273 0.283
3360.276 0.310 0.286 0.322 0.279 0.322 0.294 0.326 0.282 0.321 0.292 0.332 0.294 0.325Weather
7200.355 0.366 0.366 0.379 0.364 0.388 0.384 0.387 0.366 0.381 0.370 0.379 0.383 0.388
960.509 0.484 0.543 0.506 0.547 0.503 0.557 0.519 0.483 0.464 0.400 0.406 0.531 0.447
1920.717 0.581 0.748 0.580 0.720 0.604 0.711 0.570 0.629 0.540 0.426 0.421 0.750 0.533
3360.728 0.589 0.754 0.595 0.984 0.727 0.816 0.619 0.768 0.626 0.441 0.430 0.741 0.636ETTh1
720 - - - - - - - - - - - - - -
960.314 0.375 0.376 0.421 0.442 0.456 0.401 0.421 0.336 0.397 0.301 0.353 0.368 0.457
1920.365 0.408 0.418 0.441 0.617 0.542 0.452 0.455 0.406 0.425 0.355 0.389 0.407 0.486
3360.398 0.432 0.408 0.439 1.424 0.849 0.464 0.469 0.405 0.432 0.379 0.408 0.402 0.428ETTh2
720 - - - - - - - - - - - - - -
960.349 0.379 0.386 0.405 0.332 0.374 0.399 0.414 0.316 0.377 0.438 0.424 0.340 0.381
1920.374 0.394 0.440 0.438 0.358 0.390 0.441 0.436 0.450 0.464 0.461 0.432 0.473 0.451
3360.411 0.417 0.485 0.459 0.402 0.416 0.499 0.467 0.450 0.424 0.515 0.467 0.519 0.464ETTm1
7200.516 0.479 0.577 0.499 0.511 0.489 0.767 0.587 0.483 0.471 0.591 0.509 0.604 0.499
960.192 0.273 0.199 0.280 0.236 0.326 0.206 0.288 0.174 0.261 0.185 0.267 0.254 0.275
1920.249 0.309 0.256 0.316 0.306 0.373 0.264 0.324 0.215 0.287 0.243 0.304 0.265 0.286
3360.301 0.342 0.318 0.353 0.380 0.423 0.334 0.367 0.273 0.330 0.309 0.345 0.360 0.373ETTm2
7200.402 0.405 0.460 0.436 0.674 0.583 0.454 0.432 0.433 0.412 0.386 0.395 0.511 0.439
960.139 0.235 0.143 0.241 0.150 0.251 0.145 0.244 0.147 0.242 0.178 0.276 0.144 0.246
1920.155 0.249 0.159 0.255 0.163 0.263 0.163 0.260 0.158 0.241 0.198 0.293 0.180 0.248
3360.174 0.269 0.179 0.274 0.175 0.278 0.183 0.281 0.178 0.277 0.209 0.309 0.194 0.304ECL
7200.222 0.310 0.233 0.323 0.219 0.311 0.233 0.323 0.224 0.312 0.279 0.355 0.205 0.277
960.401 0.285 0.419 0.298 0.427 0.304 0.404 0.286 0.414 0.291 0.476 0.343 0.443 0.317
1920.418 0.293 0.434 0.305 0.447 0.315 0.412 0.294 0.419 0.291 0.496 0.355 0.407 0.320
3360.436 0.308 0.449 0.313 0.478 0.333 0.439 0.310 0.437 0.314 0.503 0.356 0.440 0.323Traffic
720 - - - - - - - - - - - - - -
Avg. Rank 2.120 2.200 4.200 4.000 4.680 5.080 4.760 4.640 2.720 2.920 4.400 4.280 5.000 4.640
mô hình duy nhất trong giai đoạn căn chỉnh chuỗi thời gian, sau đó được áp dụng nhất quán trên tất cả các chiều dài dự đoán. Ngược lại, trong giai đoạn fine-tuning dự báo, chúng tôi fine-tune một mô hình riêng biệt cho mỗi
chiều dài dự đoán, trong khi đảm bảo rằng tất cả các mô hình này chia sẻ cùng các siêu tham số.
Cả LLM4TS và GPT4TS [53] đều liên tục vượt qua hầu hết các mô hình train-from-scratch trong
các kịch bản dữ liệu hạn chế trên nhiều tập dữ liệu khác nhau, nhờ khả năng học biểu diễn
đã tồn tại được đóng gói trong GPT-2. Với việc bổ sung căn chỉnh chuỗi thời gian và tích hợp thông tin thời gian đa quy mô, LLM4TS nổi lên như một bộ dự báo chuỗi thời gian hiệu quả về dữ liệu tốt hơn so với GPT4TS, đạt hiệu suất tốt hơn trên tất cả các tập dữ liệu. Đáng chú ý, LLM4TS
với chỉ 5% dữ liệu vượt trội so với baseline tốt nhất sử dụng 10% dữ liệu. Đối với tập dữ liệu lớn nhất
(Traffic), PatchTST nổi lên như mô hình hàng đầu trong kịch bản full-shot, mặc dù xu hướng này
không mở rộng sang các kịch bản few-shot. Với chỉ 10% dữ liệu huấn luyện, LLM4TS vượt trội so với
PatchTST trong 5 trên 8 đánh giá, và với chỉ 5% dữ liệu huấn luyện, nó dẫn đầu trong 5 trên
6 đánh giá. Điều này cho thấy rằng trong các kịch bản few-shot, các mô hình deep train-from-scratch truyền thống nói chung vẫn kém hiệu suất so với những mô hình tận dụng các LLM được pre-train.
12

--- TRANG 13 ---
Bảng 3: Dự báo dài hạn few-shot sử dụng 10% dữ liệu huấn luyện. Chúng tôi sử dụng
các chiều dài dự đoán T∈ {96,192,336,720} cho tất cả các tập dữ liệu. Kết quả tốt nhất được in đậm , trong khi
kết quả tốt thứ hai được gạch chân. Lưu ý rằng TEMPO* được đánh giá dưới cài đặt zero-shot vì nó là phương pháp dựa trên prompt.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.158 0.207 0.163 0.215 0.171 0.224 0.165 0.215 0.161 0.210 0.211 0.254 0.163 0.213
1920.204 0.249 0.210 0.254 0.215 0.263 0.210 0.257 0.204 0.248 0.254 0.298 0.230 0.263
3360.254 0.288 0.256 0.292 0.258 0.299 0.259 0.297 0.261 0.302 0.292 0.332 0.258 0.282Weather
7200.322 0.336 0.321 0.339 0.320 0.346 0.332 0.346 0.309 0.332 0.370 0.379 0.301 0.328
960.417 0.432 0.458 0.456 0.492 0.495 0.516 0.485 0.448 0.460 0.400 0.406 0.455 0.457
1920.469 0.468 0.570 0.516 0.565 0.538 0.598 0.524 0.484 0.483 0.426 0.421 0.572 0.519
3360.505 0.499 0.608 0.535 0.721 0.622 0.657 0.550 0.589 0.540 0.441 0.430 0.578 0.531ETTh1
7200.708 0.572 0.725 0.591 0.986 0.743 0.762 0.610 0.700 0.604 0.443 0.451 0.723 0.594
960.282 0.351 0.331 0.374 0.357 0.411 0.353 0.389 0.275 0.326 0.301 0.353 0.332 0.374
1920.364 0.400 0.402 0.411 0.569 0.519 0.403 0.414 0.374 0.373 0.355 0.389 0.401 0.433
3360.374 0.416 0.406 0.433 0.671 0.572 0.426 0.441 0.406 0.429 0.379 0.408 0.408 0.440ETTh2
7200.445 0.461 0.449 0.464 0.824 0.648 0.477 0.480 0.427 0.449 0.409 0.440 0.459 0.480
960.360 0.388 0.390 0.404 0.352 0.392 0.410 0.419 0.346 0.388 0.438 0.424 0.392 0.401
1920.386 0.401 0.429 0.423 0.382 0.412 0.437 0.434 0.373 0.416 0.461 0.432 0.423 0.426
3360.415 0.417 0.469 0.439 0.419 0.434 0.476 0.454 0.413 0.426 0.515 0.467 0.471 0.444ETTm1
7200.470 0.445 0.569 0.498 0.490 0.477 0.681 0.556 0.485 0.476 0.591 0.509 0.552 0.501
960.184 0.265 0.188 0.269 0.213 0.303 0.191 0.274 0.177 0.261 0.185 0.267 0.233 0.262
1920.240 0.301 0.251 0.309 0.278 0.345 0.252 0.317 0.241 0.314 0.243 0.304 0.303 0.302
3360.294 0.337 0.307 0.346 0.338 0.385 0.306 0.353 0.274 0.327 0.309 0.345 0.359 0.341ETTm2
7200.386 0.393 0.426 0.417 0.436 0.440 0.433 0.427 0.417 0.390 0.386 0.395 0.452 0.419
960.135 0.231 0.139 0.237 0.150 0.253 0.140 0.238 0.139 0.241 0.178 0.276 0.138 0.235
1920.152 0.246 0.156 0.252 0.164 0.264 0.160 0.255 0.151 0.248 0.198 0.293 0.158 0.255
3360.173 0.267 0.175 0.270 0.181 0.282 0.180 0.276 0.169 0.270 0.209 0.309 0.176 0.275ECL
7200.229 0.312 0.233 0.317 0.223 0.321 0.241 0.323 0.240 0.322 0.279 0.355 0.230 0.311
960.402 0.288 0.414 0.297 0.419 0.298 0.403 0.289 0.418 0.291 0.476 0.343 0.415 0.317
1920.416 0.294 0.426 0.301 0.434 0.305 0.415 0.296 0.414 0.296 0.496 0.355 0.425 0.300
3360.429 0.302 0.434 0.303 0.449 0.313 0.426 0.304 0.421 0.311 0.503 0.356 0.436 0.310Traffic
7200.480 0.326 0.487 0.337 0.484 0.336 0.474 0.331 0.462 0.327 0.538 0.376 0.489 0.338
Avg. Rank 2.036 1.679 4.000 3.786 5.143 5.679 5.036 5.071 2.214 2.786 4.786 4.821 4.536 3.857
Chúng tôi cũng so sánh các phương pháp dựa trên LLM mới nhất trong lĩnh vực dự báo chuỗi thời gian:
Time-LLM, TEMPO, và TEST. Quan trọng là lưu ý rằng TEMPO được đánh giá dưới cài đặt
zero-shot, vì nó là phương pháp dựa trên prompt. Do đó, trong cả kịch bản few-shot và full-shot,
TEMPO luôn báo cáo kết quả zero-shot. Đối với các tập dữ liệu lớn hơn (như Weather,
Electricity, và Traffic), LLM4TS liên tục đạt hiệu suất tốt nhất, ngay cả với chỉ
5% hoặc 10% dữ liệu huấn luyện. TEMPO hoạt động tốt nhất trên các tập dữ liệu ETTh1 và ETTh2,
trong khi Time-LLM dẫn đầu trên các tập dữ liệu ETTm1 và ETTm2. Nhìn chung, LLM4TS thể hiện
hiệu suất vượt trội trên tất cả các phương pháp dựa trên LLM này trong cả kịch bản dữ liệu huấn luyện 5% và 10%.
5.2 Full-Shot Learning trong dự báo chuỗi thời gian dài hạn
Bảng 4 trình bày kết quả dự báo chuỗi thời gian dài hạn được tính trung bình qua một tập chiều dài dự đoán nhất quán Tout∈ {96,192,336,720} cho tất cả các tập dữ liệu. Trong khi trọng tâm chính của việc sử dụng
các LLM được pre-train là về few-shot learning, LLM4TS không chỉ xuất sắc trong lĩnh vực này mà còn vượt trội so với
13

--- TRANG 14 ---
Bảng 4: Dự báo dài hạn cho dữ liệu chuỗi thời gian đa biến. Chúng tôi sử dụng các chiều dài
dự đoán T∈ {96,192,336,720} cho tất cả các tập dữ liệu. Kết quả tốt nhất được in đậm , trong khi kết quả
tốt thứ hai được gạch chân. Lưu ý rằng TEMPO* được đánh giá dưới cài đặt zero-shot vì nó là phương pháp dựa trên prompt.
Methods LLM4TS GPT4TS DLinear PatchTST Time-LLM TEMPO* TEST
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
960.147 0.196 0.162 0.212 0.176 0.237 0.149 0.198 0.147 0.201 0.211 0.254 0.150 0.202
1920.191 0.238 0.204 0.248 0.220 0.282 0.194 0.241 0.189 0.234 0.254 0.298 0.198 0.246
3360.241 0.277 0.254 0.286 0.265 0.319 0.245 0.282 0.262 0.279 0.292 0.332 0.245 0.286Weather
7200.313 0.329 0.326 0.337 0.333 0.362 0.314 0.334 0.304 0.316 0.370 0.379 0.324 0.342
960.371 0.394 0.376 0.397 0.375 0.399 0.370 0.399 0.362 0.392 0.400 0.406 0.372 0.400
1920.403 0.412 0.416 0.418 0.405 0.416 0.413 0.421 0.398 0.418 0.426 0.421 0.414 0.422
3360.420 0.422 0.442 0.433 0.439 0.443 0.422 0.436 0.430 0.427 0.441 0.430 0.422 0.437ETTh1
7200.422 0.444 0.477 0.456 0.472 0.490 0.447 0.466 0.442 0.457 0.443 0.451 0.447 0.467
960.269 0.332 0.285 0.342 0.289 0.353 0.274 0.336 0.268 0.328 0.301 0.353 0.275 0.338
1920.328 0.377 0.354 0.389 0.383 0.418 0.339 0.379 0.329 0.375 0.355 0.389 0.340 0.379
3360.353 0.396 0.373 0.407 0.448 0.465 0.329 0.380 0.368 0.409 0.379 0.408 0.329 0.381ETTh2
7200.383 0.425 0.406 0.441 0.605 0.551 0.379 0.422 0.372 0.420 0.409 0.440 0.381 0.423
960.285 0.343 0.292 0.346 0.299 0.343 0.290 0.342 0.272 0.334 0.438 0.424 0.293 0.346
1920.324 0.366 0.332 0.372 0.335 0.365 0.332 0.369 0.310 0.358 0.461 0.432 0.332 0.369
3360.353 0.385 0.366 0.394 0.369 0.386 0.366 0.392 0.352 0.384 0.515 0.467 0.368 0.392ETTm1
7200.408 0.419 0.417 0.421 0.425 0.421 0.416 0.420 0.383 0.411 0.591 0.509 0.418 0.420
960.165 0.254 0.173 0.262 0.167 0.269 0.165 0.255 0.161 0.253 0.185 0.267 0.193 0.237
1920.220 0.292 0.229 0.301 0.224 0.303 0.220 0.292 0.219 0.293 0.243 0.304 0.257 0.264
3360.268 0.326 0.286 0.341 0.281 0.342 0.274 0.329 0.271 0.329 0.309 0.345 0.289 0.295ETTm2
7200.350 0.380 0.378 0.401 0.397 0.421 0.362 0.385 0.352 0.379 0.386 0.395 0.375 0.369
960.128 0.223 0.139 0.238 0.140 0.237 0.129 0.222 0.131 0.224 0.178 0.276 0.132 0.223
1920.146 0.240 0.153 0.251 0.153 0.249 0.157 0.240 0.152 0.241 0.198 0.293 0.158 0.241
3360.163 0.258 0.169 0.266 0.169 0.267 0.163 0.259 0.160 0.248 0.209 0.309 0.163 0.260ECL
7200.200 0.292 0.206 0.297 0.203 0.301 0.197 0.290 0.192 0.298 0.279 0.355 0.199 0.291
960.372 0.259 0.388 0.282 0.410 0.282 0.360 0.249 0.362 0.248 0.476 0.343 0.407 0.282
1920.391 0.265 0.407 0.290 0.423 0.287 0.379 0.256 0.374 0.247 0.496 0.355 0.423 0.287
3360.405 0.275 0.412 0.294 0.436 0.296 0.392 0.264 0.385 0.271 0.503 0.356 0.430 0.296Traffic
7200.437 0.292 0.450 0.312 0.466 0.315 0.432 0.286 0.430 0.288 0.538 0.376 0.463 0.315
Avg. Rank 2.036 2.214 4.786 4.750 5.536 5.357 2.571 2.750 1.643 2.143 6.607 6.250 4.214 3.679
tất cả các phương pháp deep train-from-scratch, ngay cả với việc truy cập tập dữ liệu đầy đủ, nhờ vào fine-tuning hai giai đoạn và tích hợp thông tin thời gian đa quy mô. Ngược lại, GPT4TS, mặc dù sử dụng khả năng học biểu diễn của GPT-2 được pre-train, không đạt được hiệu suất vượt trội so với các baseline train-from-scratch truyền thống trong các kịch bản full-shot. Điều này đặc biệt
rõ ràng khi xử lý khối lượng lớn dữ liệu huấn luyện. Hạn chế này chủ yếu xuất phát từ việc thiếu căn chỉnh chuỗi thời gian và việc bao gồm thông tin thời gian đa quy mô,
điều này là thiết yếu để tăng cường hiệu suất trong dự báo chuỗi thời gian. Thú vị là, đối với tập dữ liệu lớn nhất
(Traffic), PatchTST có thể vượt qua cả LLM4TS và GPT4TS. Điều này cho thấy
rằng với việc truy cập tập dữ liệu hoàn chỉnh và khối lượng dữ liệu đủ lớn, các mô hình deep train-from-scratch truyền thống đôi khi có thể vượt trội so với những mô hình tận dụng các LLM được pre-train.
Chúng tôi cũng so sánh các phương pháp dựa trên LLM mới nhất trong lĩnh vực dự báo chuỗi thời gian:
Time-LLM, TEMPO, và TEST. Quan trọng là lưu ý rằng TEMPO được đánh giá dưới cài đặt zero-shot, vì nó là phương pháp dựa trên prompt, và do đó luôn báo cáo kết quả zero-shot trong cả kịch bản few-shot và full-shot. Time-LLM nổi lên như mô hình hàng đầu, nhờ vào
phương pháp sáng tạo lập trình lại LLMs cho dự báo chuỗi thời gian bằng cách chuyển đổi thời gian
14

--- TRANG 15 ---
Bảng 5: Đánh giá unsupervised representation learning trong dự báo với linear
probing. Chúng tôi sử dụng các chiều dài dự đoán Tout∈ {24,48,168,336,720} cho tập dữ liệu ETTh1. Kết quả trung bình tốt nhất được in đậm , trong khi kết quả tốt thứ hai được gạch chân.
Methods LLM4TS PatchTST BTSF TS2Vec TNC TS-TCC
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
24 0.315 0.365 0.322 0.369 0.541 0.519 0.599 0.534 0.632 0.596 0.653 0.610
48 0.342 0.384 0.354 0.385 0.613 0.524 0.629 0.555 0.705 0.688 0.720 0.693
168 0.401 0.415 0.419 0.424 0.640 0.532 0.755 0.636 1.097 0.993 1.129 1.044
336 0.421 0.427 0.445 0.446 0.864 0.689 0.907 0.717 1.454 0.919 1.492 1.076
720 0.426 0.447 0.487 0.478 0.993 0.712 1.048 0.790 1.604 1.118 1.603 1.206ETTh1
Avg. 0.381 0.408 0.405 0.420 0.730 0.595 0.788 0.646 1.098 0.863 1.119 0.926
chuỗi thành text prototypes và sử dụng prompts để hướng dẫn dự đoán. Tuy nhiên, LLM4TS vẫn
rất gần về hiệu suất so với Time-LLM. Đáng chú ý, LLM4TS vẫn vượt trội so với Time-LLM trong
các kịch bản few-shot, thể hiện hiệu quả dữ liệu đặc biệt và tính mạnh mẽ trong các cài đặt dữ liệu hạn chế.
5.3 Unsupervised Representation Learning
Cho rằng mục tiêu autoregressive được sử dụng trong giai đoạn căn chỉnh chuỗi thời gian có thể được xem như một
pretext task trong unsupervised representation learning, chúng tôi nhằm đánh giá khả năng representation learning của LLM4TS. Để đánh giá hiệu quả của unsupervised representation learning, chúng tôi
tiến hành đánh giá tuyến tính trên dự báo chuỗi thời gian. Điều này bao gồm việc pre-train mô hình backbone
sử dụng pretext task, đóng băng trọng số của nó, và sau đó huấn luyện một lớp tuyến tính được gắn
trên nhiệm vụ dự báo downstream. Với các tham số của mô hình backbone được cố định, hiệu suất mạnh
trong dự báo phụ thuộc vào tính biểu cảm của các biểu diễn đã học. Bảng 5
hiển thị hiệu suất vượt trội của LLM4TS so với các đối thủ cạnh tranh trên tập dữ liệu ETTh1, làm nổi bật
hiệu quả của việc thích ứng LLM với các đặc điểm chuỗi thời gian trong giai đoạn căn chỉnh chuỗi thời gian. So sánh này chỉ bao gồm các phương pháp self-supervised learning, do đó
loại trừ các mô hình deep train-from-scratch được thiết kế rõ ràng cho dự báo chuỗi thời gian. Tương tự,
GPT4TS không phải là một phần của thí nghiệm này vì nó thiếu giai đoạn representation learning riêng biệt.
Biến thể của PatchTST được sử dụng ở đây khác với biến thể trong các thí nghiệm dự báo; biến thể này
tập trung vào representation learning. PatchTST kết hợp phương pháp MLM (Masked Language Model),
tương tự như BERT, để học các biểu diễn. Mặc dù vậy, LLM4TS vẫn nổi lên như
phương pháp hàng đầu trong khả năng representation learning giữa tất cả các phương pháp được đánh giá, đạt được
cải thiện trung bình 6.02% trong MSE. Trong lĩnh vực unsupervised representation
learning cho dữ liệu chuỗi thời gian, các mô hình dựa trên CNN (như BTSF, TS2Vec, TS-TCC) và
RNN (như TNC) thường được ưa chuộng hơn Transformers. Tuy nhiên, các thí nghiệm của chúng tôi tiết lộ
rằng các mô hình dựa trên Transformer có thể vượt qua những lựa chọn truyền thống này về hiệu suất với các pretext tasks được thiết kế tốt.
5.4 Nghiên cứu Ablation
5.4.1 Các thành phần chính trong LLM4TS
Hình 5 khám phá tác động của căn chỉnh chuỗi thời gian, multi-scale temporal encoding, và PEFT
trong LLM4TS, đánh giá cả kịch bản full- và few-shot trên tập dữ liệu ETTh1. Một phân tích
so sánh—có và không có các thành phần này—làm nổi bật tầm quan trọng cá nhân của chúng trong
việc tăng cường độ chính xác dự báo trong cả hai kịch bản. Đáng chú ý, LLM4TS mang lại hiệu suất đặc biệt
15

--- TRANG 16 ---
Hình 5: Nghiên cứu ablation về các thành phần chính trong LLM4TS. Mỗi ablation được tiến hành
dưới cả full- và few-shot learning với 10% dữ liệu huấn luyện. Chúng tôi báo cáo kết quả trung bình qua
các chiều dài dự đoán Tout∈ {96,192,336,720} cho tập dữ liệu ETTh1. Kết quả tốt nhất được in
đậm .
trong few-shot learning, trung bình giảm 6.2% MSE với mỗi việc kết hợp
các thành phần này.
Trong kết quả thí nghiệm, chúng tôi quan sát được ba insight chính. Thứ nhất, có một xu hướng đáng chú ý
là cải thiện MSE tăng khi chiều dài dự đoán mở rộng. Điều này chỉ ra rằng
các yếu tố cốt lõi của LLM4TS trở nên ngày càng có lợi trong các tình huống mà mức độ
khả năng dự đoán cao hơn được yêu cầu, đặc biệt rõ ràng với các chiều dài dự đoán dài hơn. Thứ hai,
các kịch bản few-shot thể hiện những lợi ích đáng kể hơn so với các kịch bản full-shot khi tích hợp những
thành phần chính này vào LLM4TS. Điều này nhấn mạnh điểm mạnh của LLM4TS như một bộ dự báo chuỗi thời gian hiệu quả về dữ liệu, một chất lượng chủ yếu được gán cho các thành phần vốn có của nó. Thứ ba, trong hai
phương pháp PEFT, LoRA tỏ ra có lợi hơn Layer Normalization. Lợi thế này được
quan sát nhất quán trong cả kịch bản full-shot và few-shot, làm nổi bật hiệu quả của LoRA trong
việc tăng cường hiệu suất của mô hình.
5.4.2 Chiến lược huấn luyện trong Fine-tuning dự báo
Như đã thảo luận trong Phần 4.2, trong khi linear probing (LP) cho thấy hiệu suất vượt trội trong các kịch bản out-of-
distribution (OOD) và full fine-tuning (FT) xuất sắc trong các kịch bản in-distribution (ID),
LP-FT có thể vượt qua FT và LP trong cả kịch bản OOD và ID. Hình 6 cho thấy rằng LP-FT
tăng cường hiệu suất trong cả full- và few-shot learning trên tập dữ liệu ETTh1, đạt được
cải thiện trung bình 0.7% trong MSE cho full-shot learning và 2.51% cho few-shot learning.
Những cải thiện tinh tế trong cả hai kịch bản có thể được gán cho số lượng hạn chế các tham số có thể huấn luyện trong mô hình backbone của LLM4TS ngay cả khi sử dụng FT, điều này thu hẹp sự phân biệt giữa LP và FT. Kết quả tiếp tục tiết lộ rằng few-shot learning thu được lợi ích lớn hơn
từ LP-FT, chủ yếu do tính dễ bị tổn thương cao hơn của nó đối với các vấn đề OOD nghiêm trọng. Ngoài ra, nhất quán với các quan sát trong nghiên cứu ablation về các thành phần chính của LLM4TS, chúng tôi
lưu ý một xu hướng tương tự nơi các chiều dài dự đoán dài hơn mang lại lợi ích đáng kể hơn trong các kịch bản few-shot.
16

--- TRANG 17 ---
Hình 6: Nghiên cứu ablation về chiến lược huấn luyện trong fine-tuning dự báo. Mỗi ab-
lation được tiến hành dưới cả full- và few-shot learning với 10% dữ liệu huấn luyện. Chúng tôi báo cáo
kết quả trung bình qua các chiều dài dự đoán Tout∈ {96,192,336,720} cho tập dữ liệu ETTh1. Kết quả tốt nhất được in đậm .
Bảng 6: Nghiên cứu ablation về hiệu quả của các trọng số được pre-train của LLM . Mỗi
ablation được tiến hành dưới few-shot learning với 10% và 5% dữ liệu huấn luyện. 'No Freeze'
đề cập đến mô hình sử dụng trọng số được pre-train của LLM mà không đóng băng bất kỳ lớp nào trong quá trình
huấn luyện, trong khi 'No Pretrain' biểu thị mô hình không sử dụng trọng số được pre-train của LLM,
ngụ ý mô hình được huấn luyện từ đầu. Chúng tôi báo cáo kết quả trung bình qua các chiều dài dự đoán
Tout∈ {96,192,336,720} cho các tập dữ liệu Weather, ETTm1, và ETTm2. Kết quả trung bình tốt nhất được in đậm .
Methods LLM4TS GPT4TS No Freeze No Pretrain
Metric MSE MAE MSE MAE MSE MAE MSE MAE
Weather 0.235 0.270 0.238 0.275 0.273 0.302 0.278 0.305
ETTm1 0.408 0.413 0.464 0.441 0.546 0.484 0.473 0.446 10%
ETTm2 0.276 0.324 0.293 0.335 0.340 0.367 0.361 0.385
Weather 0.256 0.292 0.264 0.302 0.284 0.312 0.298 0.324
ETTm1 0.413 0.417 0.467 0.450 0.562 0.496 0.470 0.452 5%
ETTm2 0.286 0.332 0.308 0.347 0.327 0.362 0.413 0.411
5.4.3 Hiệu quả của các trọng số được Pre-train của LLM
Như đã thảo luận trong Phần 2.1, hầu hết các tham số trong những LLM này được giữ cố định để bảo tồn khả năng học biểu diễn độc lập với dữ liệu của chúng. Bảng 6 cho thấy rằng LLM4TS hoạt động
tốt nhất khi hầu hết các tham số vẫn không thay đổi trên các tập dữ liệu Weather, ETTm1, và ETTm2.
Cụ thể, LLM4TS thể hiện cải thiện trung bình đáng chú ý 17.78% trong MSE so với phương pháp 'No Freeze', nơi các trọng số được pre-train được sử dụng mà không đóng băng bất kỳ
lớp nào trong quá trình huấn luyện. Hơn nữa, khi so sánh với phương pháp 'No Pretrain', nơi
mô hình được huấn luyện từ đầu mà không tận dụng các trọng số được pre-train, LLM4TS thể hiện
cải thiện trung bình thậm chí còn đáng kể hơn là 18.28% trong MSE. Điều này nhấn mạnh tầm quan trọng
của việc giữ lại những điểm mạnh đã tồn tại trong representation learning vốn có của các mô hình này, có thể gán chủ yếu cho các cơ chế self-attention trong các transformers được pre-train, điều này
tạo điều kiện cho việc phát triển các hoạt động độc lập với dữ liệu.
5.5 Chi phí huấn luyện và suy luận
Đánh giá chi phí tính toán của các mô hình dựa trên LLM là thiết yếu để xác định tính thực tế
của chúng trong các kịch bản thực tế. Trong bối cảnh này, chúng tôi so sánh LLM4TS với hai
17

--- TRANG 18 ---
Bảng 7: Tham số huấn luyện .
Model Tham số có thể huấn luyện Tổng tham số Tỷ lệ tham số có thể huấn luyện
LLM4TS 3.4M 85M 4%
PatchTST 20M 20M 100%
FEDformer 33M 33M 100%
Hình 7: So sánh thời gian huấn luyện và suy luận (tính bằng giây) cho một batch. Chúng tôi
sử dụng chiều dài dự đoán Tout= 96 cho tập dữ liệu ETTh2. Kết quả tốt nhất được in đậm .
baseline hàng đầu khác dựa trên Transformer, PatchTST và FEDformer. Chi tiết về số lượng
tham số có thể huấn luyện và tổng tham số có thể được tìm thấy trong Bảng 7. LLM4TS phân biệt
chính nó bằng cách giữ hầu hết các tham số được pre-train cố định và sử dụng hai phương pháp Parameter-Efficient Fine-
Tuning (PEFT): Layer Normalization Tuning và LoRA. Do đó, chỉ 4% tham số
của nó là có thể huấn luyện, làm cho số lượng tham số có thể huấn luyện trong LLM4TS thấp hơn đáng kể so với
những tham số trong các đối tác train-from-scratch của nó.
Thời gian thực thi cho cả huấn luyện và suy luận của LLM4TS, so với PatchTST
và FEDformer, được đánh giá sử dụng GPU NVIDIA Tesla V100, và kết quả được minh họa
trong Hình 7. Để đảm bảo so sánh công bằng trên tất cả các phương pháp, chúng tôi chuẩn hóa batch size ở
128 và đặt các chiều ẩn thành 768, phù hợp với thông số kỹ thuật của GPT-2. Đánh giá
được tiến hành trên một batch duy nhất, và đối với LLM4TS, chúng tôi cung cấp thời gian huấn luyện cho
cả hai giai đoạn, vì nó liên quan đến một quá trình huấn luyện hai giai đoạn. LLM4TS vượt trội so với các baseline
về thời gian thực thi trong cả giai đoạn huấn luyện và suy luận. Hiệu quả nâng cao này được
ghi nhận cho kiến trúc của nó, tận dụng đa số các tham số không thể huấn luyện, do đó đáng kể
giảm gánh nặng tính toán trong suốt cả giai đoạn huấn luyện và suy luận.
6 Kết luận
Trong bài báo này, chúng tôi trình bày LLM4TS, một khung cho dự báo chuỗi thời gian sử dụng các LLM được pre-train. LLM4TS sử dụng chiến lược fine-tuning hai giai đoạn, bắt đầu với giai đoạn căn chỉnh chuỗi thời gian để thích ứng LLM với các đặc điểm của dữ liệu chuỗi thời gian, tiếp theo là giai đoạn fine-tuning dự báo được thiết kế cho các nhiệm vụ dự báo chuỗi thời gian. Khung của chúng tôi cũng giới thiệu một phương pháp tổng hợp hai cấp độ mới, tích hợp dữ liệu thời gian đa quy mô trong các LLM được pre-train
18

--- TRANG 19 ---
để cải thiện việc diễn giải thông tin liên quan đến thời gian của chúng. Thông qua các thí nghiệm trên
7 tập dữ liệu dự báo chuỗi thời gian, LLM4TS thể hiện hiệu suất vượt trội so với các
phương pháp state-of-the-art hiện có, bao gồm những phương pháp được huấn luyện từ đầu, trong cả kịch bản full và few-shot.
Trong nghiên cứu tương lai, chúng tôi dự định mở rộng nghiên cứu theo hai hướng. Thứ nhất, trong khi chúng tôi chọn GPT-
2 làm LLM chính trong bài báo này để so sánh công bằng với GPT4TS, chúng tôi dự định đánh giá
các LLM gần đây hơn như GPT-3.5 và LLaMA-2 để đánh giá những tiến bộ của chúng. Thứ hai, chúng tôi nhằm
khám phá các nhiệm vụ khác, như phân loại và phát hiện bất thường. Mặc dù dự báo có
liên quan cao đến các ứng dụng thực tế mà không cần gán nhãn thủ công, việc mở rộng nó sang
các nhiệm vụ khác cho phép khả năng ứng dụng rộng hơn của khung LLM4TS.
Tài liệu tham khảo
[1] Akbar Abbaspour Ghadim Bonab. A comparative study of demand forecasting based
on machine learning methods with time series approach. Journal of applied research on
industrial engineering , 9(3):331–353, 2022.
[2] Sabeen Ahmed, Ian E. Nielsen, Aakash Tripathi, Shamoon Siddiqui, Ravi Prakash Ra-
machandran, and Ghulam Rasool. Transformers in time-series analysis: A tutorial. Circuits
Syst. Signal Process. , 42(12):7433–7466, 2023.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In NeurIPS , 2020.
[4] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan
Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting.
InThe Twelfth International Conference on Learning Representations , 2024.
[5] Ching Chang, Chiao-Tung Chan, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen.
Timedrl: Disentangled representation learning for multivariate time-series. CoRR ,
abs/2312.04142, 2023.
[6] Ananda Chatterjee, Hrisav Bhowmick, and Jaydip Sen. Stock price prediction using time
series, econometric, machine learning, and deep learning models. CoRR , abs/2111.01137,
2021.
[7] Hao-Yi Chih, Yao-Chung Fan, Wen-Chih Peng, and Hai-Yuan Kuo. Product quality pre-
diction with convolutional encoder-decoder architecture and transfer learning. In CIKM ,
pages 195–204. ACM, 2020.
[8] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli
Li, and Cuntai Guan. Time-series representation learning via temporal and contextual
contrasting. In IJCAI , pages 2352–2359. ijcai.org, 2021.
[9] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-
audio generation using instruction guided latent diffusion model. In ACM Multimedia ,
pages 3590–3598. ACM, 2023.
19

--- TRANG 20 ---
[10] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and
Dimitris Papailiopoulos. Looped transformers as programmable computers. In ICML ,
volume 202 of Proceedings of Machine Learning Research , pages 11398–11442. PMLR,
2023.
[11] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Transfer learning for financial
time series forecasting. In PRICAI (2) , volume 11671 of Lecture Notes in Computer Science ,
pages 24–36. Springer, 2019.
[12] Qi-Qiao He, Patrick Cheong-Iao Pang, and Yain-Whar Si. Multi-source transfer learning
with ensemble for financial time series forecasting. In WI/IAT , pages 227–233. IEEE, 2020.
[13] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and
David A. Sontag. Tabllm: Few-shot classification of tabular data with large language
models. In AISTATS , volume 206 of Proceedings of Machine Learning Research , pages
5549–5581. PMLR, 2023.
[14] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani,
Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (TCN) network for
an effective weather forecasting using time-series data from the local weather station. Soft
Comput. , 24(21):16453–16482, 2020.
[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol
Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR ,
abs/2203.15556, 2022.
[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In
ICLR . OpenReview.net, 2022.
[17] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu
Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series
forecasting by reprogramming large language models. CoRR , abs/2310.01728, 2023.
[18] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu
Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time
series forecasting by reprogramming large language models. In The Twelfth International
Conference on Learning Representations , 2024.
[19] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
Reversible instance normalization for accurate time-series forecasting against distribution
shift. In ICLR . OpenReview.net, 2022.
[20] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang.
Fine-tuning can distort pretrained features and underperform out-of-distribution. In ICLR .
OpenReview.net, 2022.
[21] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and
short-term temporal patterns with deep neural networks. In SIGIR , pages 95–104. ACM,
2018.
[22] Sangwon Lee, Junho Hong, Ling Liu, and Wonik Choi. Ts-fastformer: Fast transformer
for time-series forecasting. ACM Trans. Intell. Syst. Technol. , 15(2), feb 2024.
20

--- TRANG 21 ---
[23] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philo-
sophical Transactions of the Royal Society A , 379(2194):20200209, 2021.
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
InNeurIPS , 2023.
[25] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui Jiao, Yizhou Chen, Zhiguang Wang, and
Wei Song. Gated transformer networks for multivariate time series classification. CoRR ,
abs/2103.14438, 2021.
[26] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transform-
ers as universal computation engines. In AAAI , pages 7628–7636. AAAI Press, 2022.
[27] Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsu-
pervised representation learning for time series: A review. CoRR , abs/2308.01578, 2023.
[28] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series
is worth 64 words: Long-term forecasting with transformers. In ICLR . OpenReview.net,
2023.
[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1:9, 2019.
[30] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual LLM for video under-
standing. CoRR , abs/2312.06720, 2023.
[31] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm:
Can large language models understand structured table data? a benchmark and empirical
study. In Proceedings of the 17th ACM International Conference on Web Search and Data
Mining , WSDM '24, page 645–654, New York, NY, USA, 2024. Association for Computing
Machinery.
[32] Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. TEST: Text prototype aligned em-
bedding to activate LLM's ability for time series. In The Twelfth International Conference
on Learning Representations , 2024.
[33] Hiteshi Tandon, Prabhat Ranjan, Tanmoy Chakraborty, and Vandana Suhag. Coronavirus
(covid-19): Arima-based time-series analysis to forecast near future and the effect of school
reopening in india. Journal of Health Management , 24(3):373–388, 2022.
[34] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learn-
ing for time series with temporal neighborhood coding. In ICLR . OpenReview.net, 2021.
[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´ elien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and
efficient foundation language models. CoRR , abs/2302.13971, 2023.
[36] Alexandros-Menelaos Tzortzis, Sotiris Pelekis, Evangelos Spiliotis, Spiros Mouzakitis,
John E. Psarras, and Dimitris Askounis. Transfer learning for day-ahead load forecasting:
a case study on european national electricity demand time series. CoRR , abs/2310.15555,
2023.
[37] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori
Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities
of large language models. Trans. Mach. Learn. Res. , 2022, 2022.
21

--- TRANG 22 ---
[38] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang
Sun. Transformers in time series: A survey. In IJCAI , pages 6778–6786. ijcai.org, 2023.
[39] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times-
net: Temporal 2d-variation modeling for general time series analysis. In ICLR . OpenRe-
view.net, 2023.
[40] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition
transformers with auto-correlation for long-term series forecasting. Advances in Neural
Information Processing Systems , 34:22419–22430, 2021.
[41] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time
series anomaly detection with association discrepancy. In ICLR . OpenReview.net, 2022.
[42] Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iter-
ative bilinear temporal-spectral fusion. In ICML , volume 162 of Proceedings of Machine
Learning Research , pages 25038–25054. PMLR, 2022.
[43] Cheng-Han Yeh, Yao-Chung Fan, and Wen-Chih Peng. Interpretable multi-task learning
for product quality prediction with attention mechanism. In ICDE , pages 1910–1921. IEEE,
2019.
[44] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian,
Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in
time series forecasting. Advances in Neural Information Processing Systems , 36, 2024.
[45] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong,
and Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI , pages
8980–8987. AAAI Press, 2022.
[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time
series forecasting? In AAAI , pages 11121–11128. AAAI Press, 2023.
[47] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised
contrastive pre-training for time series via time-frequency consistency. Advances in Neural
Information Processing Systems , 35:3988–4003, 2022.
[48] Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need
billions of words of pretraining data? In ACL/IJCNLP (1) , pages 1112–1125. Association
for Computational Linguistics, 2021.
[49] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension de-
pendency for multivariate time series forecasting. In ICLR . OpenReview.net, 2023.
[50] Xiaochen Zheng, Xingyu Chen, Manuel Sch¨ urch, Amina Mollaysa, Ahmed Allam, and
Michael Krauthammer. Simts: Rethinking contrastive representation learning for time
series forecasting. arXiv preprint arXiv:2303.18205 , 2023.
[51] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wan-
cai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting.
InAAAI , pages 11106–11115. AAAI Press, 2021.
[52] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:
Frequency enhanced decomposed transformer for long-term series forecasting. In ICML ,
volume 162 of Proceedings of Machine Learning Research , pages 27268–27286. PMLR, 2022.
[53] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general
time series analysis by pretrained LM. In NeurIPS , 2023.
22
