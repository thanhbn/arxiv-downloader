# 2307.08941.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2307.08941.pdf
# File size: 1290209 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
MLP Fusion: Towards Efficient Fine-tuning of Dense
and Mixture-of-Experts Language Models
Mengting Ai∗, Tianxin Wei∗, Yifan Chen∗,Member, IEEE , Zeming Guo, Jingrui He, Senior Member, IEEE
Abstract —Fine-tuning a pre-trained language model (PLM)
emerges as the predominant strategy in many natural language
processing applications. However, this process is known to be
expensive, especially on edge devices with low computing power.
While general approaches (e.g. quantization and distillation) have
been widely studied to reduce the compute/memory of PLM
fine-tuning, one-shot compression techniques specifically designed
for fine-tuning remain largely unexplored. In this paper, we
investigate the neural tangent kernel (NTK)–which reveals the
gradient descent dynamics of neural networks–of the multilayer
perceptrons (MLP) modules in a PLM and propose to coin a
lightweight PLM through NTK-approximating MLP fusion . By
incorporating NTK into the compression process, MLP Fusion not
only preserves the original model’s output but also maintains its
training dynamics. To achieve this, we reconsider the MLP as
a bundle of sub-MLPs and cluster them into a given number
of centroids, which can then be restored as a compressed MLP
and surprisingly well approximate the NTK of the original PLM.
Our approach is applicable to both standard MLP modules and
Mixture-of-Experts (MoE) modules in PLMs, demonstrating its
scalability and versatility. Additionally, we provide theoretical
derivations to demonstrate how the proposed compression
preserves the NTK. Extensive experiments of PLM fine-tuning on
both natural language understanding and generation tasks are
provided to verify the effectiveness of MLP fusion. Our code is
available at https://github.com/weitianxin/MLP_Fusion.
Index Terms —Neural tangent kernel, pre-trained language
model fine-tuning, efficient machine learning, Mixture-of-Experts.
I. I NTRODUCTION
SUPERVISED fine-tuning (SFT) of pre-trained language
models (PLMs) has been the most common method to
tackle downstream natural language processing (NLP) tasks
[1], [2]. However, despite the high performance of SFT on
downstream tasks [3], users face significant computational
costs in terms of both time and space due to the large size
of PLMs. The sizes of popular PLMs have recently grown
from hundreds of millions [4] to trillions [5] of parameters,
driven by scaling laws [6]. Even the smallest BERT model [7]
Mengting Ai, Tianxin Wei, and Jingrui He are with the School of Information
Sciences, University of Illinois Urbana-Champaign, Champaign, IL 61820,
USA. E-mail: mai10@illinois.edu; twei10@illinois.edu; jingrui@illinois.edu.
Yifan Chen is with the Departments of Mathematics and Computer
Science, Hong Kong Baptist University, Kowloon, Hong Kong. E-mail:
yifanc@hkbu.edu.hk.
Zeming Guo is with the Jacobs Technion-Cornell Institute, Cornell Tech,
New York, NY 10044, USA. E-mail: zg296@cornell.edu.
*Mengting Ai, Tianxin Wei, and Yifan Chen contributed equally to this
work.
Corresponding authors: Yifan Chen and Jingrui He.
Fig. 1. NTK matrix approximation error of compression methods on the
validation set of SST2.
has over 110M parameters, not to mention the newer Llama-
series models [8], which range from 7B to 405B parameters.
Mixture-of-Experts (MoE) [9], as another product of scaling
laws, extends beyond the traditional feedforward neural network
(FFN) layer by replacing a single multilayer perceptron (MLP)
with multiple MLPs, referred to as “experts”. Sparse MoE
designs improve performance while keeping inference com-
putational costs (FLOPs) comparable to those of the original
dense model, as only a few selected experts are activated during
inference. However, during fine-tuning, the computational costs
significantly increase because each expert requires tuning. The
expert size for Mixtral [10] reaches 176.2M, and the presence of
8 or even more experts in each layer exacerbates the demands.
Various efforts have been made in various fields to compress
and harness the large-scale PLMs. A popular technique in model
compression is knowledge distillation [11]–[14, KD], which
aims to transfer knowledge from pre-trained large language
models (LLMs) to smaller models. However, this approach
requires extensive retraining, involving both the original LLM
and the compact model. There were some previous attempts
to establish one-shot model compression methods. Single-shot
pruning methods [15]–[18, sparsification] identify sub-networks
at initialization concerning certain criteria of weight magnitude
or gradient flow. However, most works on (entry-wise) pruning
focus on reducing the conceptual training or inference FLOPs,
while sparse matrix multiplication is not well supported on
modern hardware (e.g. GPUs) and even slows down the training
in wall-clock time [19].
Meanwhile, truncated singular value decomposition (SVD)
on weight matrices [20], [21] has been applied to accelerate
0000–0000/00$00.00 © 2021 IEEEarXiv:2307.08941v3  [cs.LG]  6 Jan 2025

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
large CNNs by leveraging the linear structure within the
network to eliminate redundancy. However, truncated SVD has
limited representational power and might lead to suboptimal
performance, as it significantly reduces the dimensionality of
the linear transformations in the network. LoRA [22], although
mitigating this issue by incorporating information from the
original weight matrices, does not reduce the inference cost
of the fine-tuned model. Specifically for MoE models, various
studies have introduced the concept of expert merging [23]–
[27] and expert pruning [28], as a method to reduce the number
of experts within each layer of the MoE model.
On a separate note, although efficient attention mecha-
nisms [29]–[31] have become the mainstream methodology to
accelerate the pre-training of language models, we instead focus
on the FFN sub-layers of a pre-trained transformer. Given the
ever-increasing hidden input size of PLMs, which now exceeds
ten thousand [8], [10], the significance of FFN sub-layers in
terms of computation cost has grown substantially. We observe
for regular NLP tasks where the token sequence length is no
longer than 512, the computational cost of the MLP modules
is heavier than the attention module even though the attention
module has a quadratic complexity (detailed computation and
comparison of the computational cost within the two modules
are provided in the supplementary material Appendix B-A).
This disparity is even more pronounced in MoE models.
The current limitations of general model compression
methods and the realistic need to reduce the compute in MLP
modules motivate us to develop an MLP compression technique
for efficient language model SFT. To attain competitive SFT
performance, we propose a novel perspective on PLM compres-
sion, that the compressed model is supposed to approximate
not only the model output, but also the training dynamics
of the original SFT. We turn to neural tangent kernel [32],
[33, NTK] as a proxy of the SFT dynamics and manage to
enable the compressed model to approximate the original
NTK; specifically, we dissect the MLP or MoE structure
in a PLM, connect it with model fusion [34, which layer-
wisely fuse multiple MLPs into one], and propose a novel
compression method, MLP fusion , specific to PLM fine-tuning.
As shown in Figure 1, our method “fusion” provably attains
the smallest NTK matrix approximation error on a real-world
dataset SST2 [35].
In summary, the contributions of this work are four-fold:
•We introduce the concept of NTK approximation for PLM
compression, to ensure that the compressed model can
preserve the training dynamics of the original model.
•We dissect the MLP modules in PLMs and propose a novel
data-agnostic technique, MLP fusion, which leverages clus-
tering characteristics to approximate the NTK. Theoretical
derivations are provided to demonstrate how the proposed
compression preserves the NTK.
•We demonstrate that MLP Fusion can be applied to both
traditional MLP and MoE modules within PLMs, proving
its versatility.
•We provide extensive experimental results on PLM SFT for
both natural language understanding (NLU) and generation
(NLG) tasks, validating their effectiveness and soundness.II. R ELATED WORK
There are numerous model compression methods for reduc-
ing the size of MLPs in PLMs. The first line of research, knowl-
edge distillation [36]–[38], compresses the pre-trained model
and then fine-tunes the compressed model on downstream tasks.
Techniques like mean squared error [11], optimal transport [39],
and maximum mean discrepancy (MMD) [40] are commonly
used as distillation loss terms. However, this approach requires
loading and executing the large teacher PLM, demanding
significant computational resources before fine-tuning. Another
direction involves methods applied after the SFT stage to
achieve faster inference. For example, FastBert [41] uses a
sample-wise adaptive mechanism to adjust inference time, and
DeeBERT [42] accelerates inference by allowing samples to
exit earlier. Moefication [43] splits MLP modules into sub-
networks with a router to select the appropriate sub-network
for each input. However, these methods still rely on regular
SFT and do not fully utilize PLM knowledge.
In addition to the directions above, a more lightweight
efficient fine-tuning paradigm is one-shot model compression .
As a representative, single-shot pruning methods [15]–[18],
[44] identify sub-networks at initialization concerning user-
specified criteria (e.g. weight magnitude or gradient flow) and
attain sparsity in model weights. The Lottery Ticket Hypothesis
(LTH) [45], [46] demonstrates the existence of sparse sub-
networks in DNNs and has been applied successfully to
PLMs. Recent advancements like NTK-SAP [47] and PX [48]
integrate Neural Tangent Kernel (NTK) theory into pruning,
achieving strong performance on architectures like ResNet [49].
However, pruning primarily reduces theoretical FLOPs, while
sparse matrix operations remain inefficient on modern hardware
(e.g., GPUs), leading to slower wall-clock training times.
Classical computational techniques like truncated SVD [20] and
randomized sketching [50], [51] are also intuitive for one-shot
PLM compression. LoRA [22] introduces low-rank layers atop
original ones, reducing SFT costs by updating only the added
parameters. However, LoRA does not decrease inference costs
since the final output combines the original and low-rank layers.
Detailed trade-offs for LoRA are discussed in Section V-F.
Specifically for MoE models, various studies have introduced
the concept of expert merging [23]–[27] and expert pruning
[28], as a method to reduce the number of experts within each
layer of the MoE model. However, we note that these methods
may not fully leverage the MoE structure, as reducing the
number of experts could result in significant information loss.
In Section V we implement the aforementioned methods as
baselines for a comprehensive comparison.
III. P RELIMINARIES AND NOTATIONS
The notations for MoE layers are introduced in Section III-A .
We also provide brief preliminaries to NTK in Section III-B.
A. Multilayer Perceptron and Mixture-of-Experts Modules
Across this paper, we denote the input sequence as a feature
matrix X∈Rs×p, where sis the sequence length and pis
the dimension of the MLP input/output (the dimensions of
MLP input and output agree with the embedding dimension in

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
most PLMs). The neural architecture of interest (a pre-trained
transformer) is denoted as f, which is different from the scalar
loss function ℓ. For the simplicity of notation, the output of the
whole network f(x)is assumed to be a scalar in this paper,
which is the case in regression and binary classification tasks.
Our derivation however still holds for vector/matrix output if
we analyze the output element-wise.
Specifically, an MLP in the FFN sub-layer of a transformer
can be expressed as:
H=σ(XW 1+1b⊤
1)W2+1b⊤
2, (1)
where W1∈Rp×pI,b1∈RpI(resp. W2∈RpI×p,b2∈Rp)
are the weight matrix and the bias term of the first (resp.
second) linear transform within the FFN sub-layer, and σ(·)
is the element-wise activation function. Some other constantly
used notations involve the MLP intermediate dimension pI
(the subscript Iis short for “intermediate”).
As an improvement over the MLP module above, we also
consider the classical mixture-of-experts (MoE) modules, where
each expert takes the form of an MLP above. We provide the
framework illustration of an MoE layer in Figure 2. In more
detail, each MoE layer consists of Nexperts. The k-th expert
Ek(a function to transform input vector xto a new feature)
in an FFN sub-layer is an MLP and denoted as:
Ek(X) =σ(XW(k)
1+1(b(k)
1)⊤)W(k)
2+1b(k)
2.
The output of the MoE layer is thus given by ( ⊙represents
the Hadamard product):
Hm=NX
k=1[G(X)](·,k)1⊤⊙Ek(X), (2)
where G(X) = Softmax (TopK ( XW g))∈Rs×Nreturns
the normalized sparse router gating vector of all experts for
each token: TopK( g)outputs giwhen giis within the top- k
values of g∈RN, otherwise it returns −∞. (We slightly abuse
the notation in TopK ( XW g), where TopK( ·)is row-wisely
applied to the sequence matrix XW g.) Here, Wg∈Rp×N
represents the linear transform, turning the input token xiinto
the router logit for each expert. For example, given a single
input vector token xi, the gate vector G(x) = [0 .7,0,0.3,0]
activates experts 1 and 3 with scores 0.7 and 0.3 (suppose the
number of experts N= 4 andTop2 are taken.) The gradients
(as well as the NTK) of the weights in an MLP or MoE module
are analyzed in Section IV-A.
B. Neural Tangent Kernel
NTK is a powerful theoretical technique to study the gradient
descent dynamics of neural networks [32]. It originated from
the research on infinitely wide or ultra-wide neural networks. In
applying NTK to convolutional neural networks for computer
vision tasks, it is noted that NTK can be extended to arbitrary
neural architecture fand initialization θ0, which induces the
stochastic gradient descent (SGD) NTK as [33]:
⟨∇θ0f(x;θ0),∇θ0f(z;θ0)⟩. (3)
We note that the NTK above is specific to the SGD [52]
optimizer. As for Adam [53], the most common optimizer for
Fig. 2. In this illustrative example of MoE layers, the Top- KSelector, along
with the Gate Network–often referred to as the ‘router’–selects Experts 1 and
3 based on their scores for the given input.
language model fine-tuning, its corresponding NTK in the early
stage of training (which is believed to match the nature of the
short-period fine-tuning) can be approximated by the so-called
Asymmetric SignGD Kernel [54]:
K(AS)(x, z):=⟨∇θ0f(x;θ0),sign (∇θ0f(z;θ0))⟩,(4)
we will refer to this kernel by Adam NTK and focus on the
analysis thereof in this paper, considering Adam is the dominant
optimizer in language model fine-tuning.
Recent works show directly using the NTK Equation (3)
extracted from a pre-trained model f(·)can obtain decent
performance on computer vision tasks [55], and in some cases
can capture the training dynamics of language model fine-tuning
[54]. There has already been some discussion on compressing
a trained (fine-tuned) network with NTK preserved through
pruning and quantization [47], [48], [56]. We will shortly
leverage the useful tool to guide the design of our proposed
models and serve as a sanity check as well.
IV. MLP FUSION WITH APPROXIMATE NTK
For the reader’s convenience, we first derive the concrete
form of the NTK for MLP and MoE modules in Section IV-A .
Next, Section IV-B outlines the exact form of the proposed
clustering-based MLP Fusion, followed by verification in
Section IV-C to ensure the method meets the previously stated
expectations. Finally, Section IV-D presents layer-wise tuning,
incorporating ideas from further distillation.
A. Preparation: NTK for MLP & MoE
As the gradients w.r.t. the model weights are the building
blocks of NTK, we provide the expressions of the gradients
for MLP and MoE as follows, whose calculation is based on
∇Hf/∇Hmfm∈Rs×pand the chain rule (similar to f, along
this paper fmis set as a scalar MoE model).
Gradients for MLP modules. We start with the gradients
for a classical MLP module in Equation (1). In particular,
∇W2f=σ⊤∇Hf,∇b2f= (∇Hf)⊤1
∇W1f=X⊤ 
∇HfW⊤
2
⊙σ′
∇b1f= 
∇HfW⊤
2
⊙σ′⊤1,(5)
where we by convention abuse the boldfaced notation σ, σ′
as a shorthand for σ 
XW 1+1b⊤
1
andσ′ 
XW 1+1b⊤
1

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
respectively. The computation of NTK would then be boiled
down to the proper inner products of the aforementioned
gradient terms. It is worth mentioning that the classical
model compression technique, pruning, is expected to well
approximate the NTK. Assuming the output of a pruned MLP
is close to the original one, the gradient terms will be roughly
approximated by the Hadamard product of the mask matrix
and the original gradient terms in Equation (5).
Gradients for MoE modules. We first give the matrix form
of an MoE module to ease the following gradient derivation;
to the best of our knowledge, we are the first to provide this
fundamental form for MoE modules.
For a single token xi∈Rp(also the i-th row in the
sequence matrix X), the router matrix is denoted as:
Ri:= diag( G(xi))⊗IpI=
[G(xi)]1·I
...
[G(xi)]N·I

N·pI×N·pI,
where ⊗is the Kronecker product, Iis the identity matrix. We
recall G(xi)is a sparse vector with length Nthat contains
the gate value. (Since G(xi)depends on xi,Riis different
for each token and we accordingly add the subscript i.) The
weight matrices in the MoE layer are collectively denoted as:
W1=
W(1)
1···W(N)
1
p×N·pI,
W2=
W(1)
2⊤
···
W(N)
2⊤⊤
N·pI×p.
Thei-th row of the MoE output can accordingly be expressed
as (for simplicity, the bias terms are omitted; also, in practice,
most MoE models do not contain them):
(Hm)⊤
i=σ 
x⊤
iW1
RiW2, (6)
in which Riencapsulates crucial expert knowledge and exhibits
high sparsity, as typically only a selected number of experts
are activated within each MoE layer.
The gradients of the MoE layer based on Equation (6) can
be obtained as:
∇W2fm=X
iR⊤
i·σi·(∇Hmfm)⊤
i,
∇W1fm=X
ixi 
RiW2(∇Hmfm)i
⊙σ′
i⊤,(7)
in which σi, σ′
iare respectively the i-th row of σ, σ′.
As a closing remark, we intentionally omit the gradient
for the weight matrix Wgin the router, as in practice it is
frozen during the SFT stage; this practice comes from the
observation that preserving the original PLM’s universal world
information can enhance their performance [57]–[59]. Our
findings in Section V-E empirically support this observation,
demonstrating freezing Wgcan improve model performance.
B. Methodology: MLP Fusion with Clustering
In this subsection, we primarily develop the proposed method
with the goal of ensuring that the new output eHCcan effectively
approximate the original output H(a rough approximation
analysis is provided in Appendix C in the supplement), and
Ensemble of
 FusionSub-MLPs
≈NTK as a P roxy of 
Training DynamicsFig. 3. An overview of MLP Fusion. The original MLP in Dense and MoE
LLMs can be decomposed as an ensemble of pIsub-MLPs; through MLP
fusion, we cluster the sub-MLPs and re-construct a smaller MLP, which is
shown to approximate the NTK of the original MLP and is thus expected to
enjoy a similar training dynamics to the full-size PLM.
delay the discussion on NTK approximation to Section IV-A .
We propose MLP Fusion following this intuitive purpose,
through a view that an MLP can be taken as the ensemble
of multiple bottleneck-1 sub-MLPs [60]–[62]. We rewrite the
MLP output in the following form:
H=σ(XW 1+1b⊤
1)W2+1b⊤
2
=pIX
i=1[σ(XW 1,·,i+b1,i1)W2,i,·] +1b⊤
2,
where by convention we represent the i-th column (resp. row) in
the weight matrix W1(resp. W2) asW1,·,i(resp. W2,i,·). The
summation implies that it is feasible to approximate MLP via a
few bottleneck-1 sub-MLPs (the summands on the right-hand-
side above), in a similar way to numerical methods such as
importance sampling [63] and sketching [50], [51]. Considering
the existence of the nonlinear activation function σ, we turn
to clustering and will demonstrate below how this classical
machine learning technique can be used to approximate the
above sub-MLP summation.
To obtain the “embedding” of the sub-MLPs for clustering,
we consider the original MLP as pIsupporting points, repre-
sented by a design matrix W=
W⊤
1,b1,W2
∈RpI×(2p+1).
An intuitive idea to compress an MLP is therefore representing
the empirical distribution (MLP) by the output centroids of
cclusters. We suggest to use wi= [W⊤
1,·,i,b1,i,W2,i,·]⊤
as the embedding vector for the i-th sub-MLP, as wican
uniquely decide the i-th sub-MLP. Upon the embeddings,
Lloyd’s algorithm [64] can be directly applied to solve the
k-means clustering problem, obtain cclusters {Pj}c
j=1, and
return a one-hot clustering matrix C∈Rc×pIwith elements
Cji= 1{wi∈Pj}. Normalizing Cso that the rows sum to
1, we can construct an averaging matrix ¯Cwith elements
¯Cji=1
|Pj|{wi∈Pj}, where |Pj|is the number of elements in
cluster j. so that ¯CW will return the desired centroid matrix
fW=h
fW⊤
1,eb1,fW2i
∈Rc×(2p+1). In general, conducting
clustering is minimizing the distance from a point to its closest
centroid, which partially explains our intuition that replacing
the original sub-MLPs with their corresponding centroids can

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
benefit the compression of MLPs.
Relation with Model Fusion [34]. In principle, we consider
the clustering of sub-MLPs shares the same spirit as model
fusion, which takes a single layer of MLPs as an empirical
distribution of the corresponding weights (either W1,·,i’s or
W2,i,·’s in our context) and then fuses multiple MLPs into a
new one through solving a Wasserstein barycenter problem [65].
The clustering procedure is closely related to the problem above,
as the output centroids serve as the optimal barycenters when
the number of points wis assigned to each cluster is fixed. Due
to the connection, we refer to the clustering procedure as MLP
fusion in this paper.
The derivation for MLP compression . We replace each
sub-MLP parameter vector wiwith the corresponding centroid
(equivalently, we replace WwithC⊤fW). The new output can
be naturally simplified as:
σ
XfW1+1(˜b1)⊤
C
C⊤fW2+1b⊤
2
=σ
XfW1+1(˜b1)⊤ 
CC⊤fW2+1b⊤
2,
where the above equation holds because Csimply “copies” the
centroids and thus can be taken out of the activation function.
We will shortly show in Section IV-C that the computational
properties of the one-hot clustering matrix Care indeed critical
for Adam NTK approximation.
After being taken out of the activation function, Cis then
allowed to be combined with C⊤to form the scaling matrix
referred to as P=CC⊤, which is a c×cdiagonal fixed
matrix that greatly reduces the computation compared with
the original equation. Note the architecture of the final MLP
has not yet been specified, since there are different ways to
address the scaling matrix P: it can
•either be incorporated into fW2, or
•stands alone as a constant scaling matrix.
It is worth noting that both strategies behave identically
during forward propagation; however, during backward propa-
gation, the gradient of the second method is multiplied by the
scaling matrix P. In the final version of MLP Fusion, we adopt
the form that uses the stand-alone scaling matrix P, and refer
to the variant that incorporates PintofW2as “clustering”.
The derivation for MoE module compression . As an MoE
module is composed of multiple MLPs, the proposed MLP
Fusion can be respectively applied to the experts therein, and
the derivation will be similar; we defer the presentation to the
next subsection.
C. NTK Approximation
In addition to the aforementioned goal of output approxima-
tion, we further suggest a compression method for fine-tuning
is supposed to preserve the NTK of the original model so that
the training dynamics thereof can be preserved as well.
To this end, we revisit the scaling matrix Pand render it
“stand-alone” (the second choice in the previous subsection);
with this specific design, MLP Fusion is able to approximate
the NTK and its form is formally given as:
eHC:=σ
XfW1+1eb⊤
1
PfW2+1b⊤
2 (8)where we choose fW1:=W1¯C⊤,eb1:=¯Cb1,fW2:=¯CW 2
as the new parameters for the compressed MLP, and Pis de-
signed to stand alone as a constant scaling matrix. We note that
Pii= (CC⊤)ii=P
qC2
iq=P
qCiq(Pij= (CC⊤)ij= 0
fori̸=j) represents the number of points in cluster i. From
an intuitive perspective, the backward process of multiplying
the gradient by the scaling matrix Pcan be seen as assigning
different learning rates to different clusters. This means that
larger clusters are given a larger learning rate.
As for the MoE modules , directly applying MLP Fusion to
each expert will give:
(eHm)⊤
i=σ
x⊤
ifW1
P(m)
ifW2, (9)
where P(m)
i=C(m)Ri(C(m))⊤and the concatenated cluster-
ing matrix C(m)is denoted as:
C(m)=
C1
...
CN

N·c×N·pI,
andCkis the corresponding clustering matrix specific to expert
k; to ease the notations, we also reload the compressed weight
matrices fW1,fW2as
fW1=
fW(1)
1···fW(N)
1
,
fW2=
fW(1)
2⊤
···
fW(N)
2⊤⊤
,
where fW(k)
1=W(k)
1¯C⊤
k,fW(k)
2=¯CkW(k)
2,∀k∈[N], and
similar to ¯C,¯Ckis the normalized version of Ck. A closing
remark is that the compression version in Equation (9) enjoys
the same form as the original MoE output in Equation (6), and
thus we can similarly derive the gradient for fW1,fW2in the
MoE modules as in Equation (7).
We will then show how the specific MLP (8)can serve to
approximate the NTK of the original MLP (1)(the derivation
for the MoE module (2)can be found in the supplementary
material). This expectation implies the following requirements:
first, the new output eHC(resp.eHm) is supposed to approximate
the original output H(resp. Hm), which we have heuristically
shown in the previous discussion; second, the hidden represen-
tation σand the related composition term 
∇HfW⊤
2
⊙σ′
should also be preserved.
This subsection will thus be devoted to verifying that the
proposed method can induce an Adam NTK close to the
original one. The key step is to show the inner product of
the four gradient terms in Equation (5) will approximately
remain. We prepare some additional notations to ease the
following discussions and let the compressed neural model be
equipped with the compressed MLP module as fc. The input
token sequence is denoted as XorZ, respectively.
We first make an assumption that the clustering can capture
the MLP empirical distribution, so that, to some sense,
CfW≈WandeHC≈Has derived in Section IV-B .
This assumption implies that ∇Hfcan be preserved by
∇eHCfc, since they depend on H/eHCin the same way.
We can automatically obtain ⟨∇b2f(X),sign (∇b2f(Z))⟩ ≈
⟨∇b2fc(X),sign (∇b2fc(Z))⟩.

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
We then analyze the term ⟨∇W2f(X),sign (∇W2f(Z))⟩,
where the notation ⟨·,·⟩is reloaded as the matrix inner product
⟨X,Z⟩:= Tr 
X⊤Z
. The term equals1
Trh
(∇Hf(X))⊤σx·sign 
σ⊤
z∇Hf(Z)i
,
and can be shown to approach
D
∇fW2fc(X),sign
∇fW2fc(Z)E
.
Concretely, we re-utilize the deduction ∇Hf≈ ∇eHCfcto
make it sufficient to study whether (˜σxP)·sign 
P˜σ⊤
z
can
approximate its counterpart σx·sign 
σ⊤
z
.
Analogous analyses of the matrix product
 
∇Hf(X)W⊤
2
⊙σ′
x
· 
∇Hf(Z)W⊤
2
⊙σ′
z⊤
can also be performed for the other two terms
⟨∇W1f(X),sign (∇W1f(Z))⟩and
⟨∇b1f(X),sign (∇b1f(Z))⟩.
Due to space constraints, derivations for the two terms and the
MoE modules are provided in the supplementary Appendix B-
C & B-D. Notably, the element-wise sign function in the Adam
optimizer plays a key role in NTK preservation. For regular
SGD, additional conditions and modifications are required.
Readers can find more details in Appendix B-E.
D. Layer-wise Task-specific Tuning
In the previous section, MLP Fusion manages to exploit the
potentials of the pre-trained models in a one-shot and task-
agnostic manner, where we retain the training dynamics of
neural networks through NTK preservation. To more effectively
acquire the knowledge within each task, we can leverage the
idea from distillation and intuitively design a layer-wise (and
thus lightweight) task-specific tuning module, which further
tunes the fused MLP with task-specific unsupervised training
data. Compared to classical distillation, the layer-wise tuning
lasts a shorter period (only 1epoch in our experiments in
Section V,) and we only updates the weights in the fused MLP.
To be specific, we set the tuning loss as the mean squared
error (MSE) between the layer output Hl
tin the teacher model
and the layer output Hlin the student model for layer lof the
PLM. The tuning loss is then computed as:
ℓtune=LX
l=1MSE(Hl
t,Hl) (10)
where Lis the number of layers in the PLM and MSE denotes
the mean squared error.
V. N UMERICAL RESULTS
In this section, we present the numerical results of MLP
Fusion compared to representative baselines on both NLU
and NLG tasks. We start with the experimental setup and an
1σx, σzare the shorthand for σ 
XW 1+1b⊤
1andσ 
ZW 1+1b⊤
1
respectively. Similarly, we define ˜σx:=σ
XfW1+1˜b⊤
1
and˜σz:=
σ
ZfW1+1˜b⊤
1
forfc.initial evaluation of the approximation error, followed by an
ablation study and efficiency evaluation. Detailed experimental
information is included in the supplementary material.
A. Experiment Setup
1) Backbone Models: As our experiments cover both NLU
and NLG tasks, we leverage different model types tailored to
each. Specifically, we utilize RoBERTa [66], an encoder-only
architecture, for the NLU tasks, and GPT-2 [67], a decoder-only
architecture, for the NLG tasks. For MoE models, we employ
the Switch Transformer [5], an encoder-decoder model with
16 experts in each MoE layer.
2) Baseline Methods: We mainly compare our method
with one-shot compression methods: regular fine-tuning [1],
truncated SVD [20], Pruning (single-shot unstructured prun-
ing) [15]–[18], LTH (Lottery Ticket Hypothesis) [45], [46],
GEM-MINER [68], Moefication [43]. We also compare the
structured pruning method FLOP (Factorized Low-rank Prun-
ing) [69]. In addition, we list the performance of Distil-
RoBERTa [70] as a reference for NLU tasks.
By default, all the baseline methods are set to reduce the
MLP intermediate size from 3076 to 768 or a comparable
number of parameters. Specifically, (i) truncated SVD keeps
only the tlargest singular values and the associated singular
vectors. (ii) Unstructured pruning globally removes a certain
ratio of connections by exploring the weight magnitude and
gradient. Concretely, we mask 75% of the connections to match
the compression rate (25%). As a special case of pruning,
(iii) Lottery tickets hypothesis (LTH) [45], [46], demonstrates
the existence of sparse subnetworks in DNNs, which performs
iterative sparsification during tuning to find the matching
network. We iteratively prune the MLP for one epoch to make
it computationally equivalent to the layer-wise task-specific
tuning module (introduced in Section IV-D ). The network mask
ratio is also 75% as pruning. (iv) Moefication splits the MLP
modules of a PLM into several sub-networks and designs the
additional route mechanism to decide the corresponding sub-
network for each input. Here we split the original MLP into 4
sub-networks to match the network compression ratio.
Furthermore, we introduce three machine learning techniques,
randomized sketching, MMD approximation, and regular
clustering, as extra strong baselines. We demonstrate as follows
how to apply them to MLP compression.
Sketching the Weight Matrices. The idea of Sketching is to
reduce the size of W1,W2by multiplying a matrix S:
eHS=σ(XW 1S+1b⊤
1S)S⊤W2+1b⊤
2,
where Scan be a Gaussian Sketching Matrix, which applies
Johnson–Lindenstrauss transform [71] to the weight matrices
W1,W2. We expect Sketching can more or less preserve the
information within the PLM.
Compression via Minimizing MMD. As described in Sec-
tion IV-B , we propose viewing the MLP as an empirical
distribution of sub-MLPs. Intuitively, the MLP can be com-
pressed by minimizing the MMD distance between the original
and compressed MLP distributions. This approach produces a
compressed model with fewer support points while preserving

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
TABLE I
APPROXIMATION ERROR OF EACH BASELINE METHOD ON SST2
VALIDATION SET WITH ROBERT A AS THE BACKBONE .
Approximation Error
Output NTK
Sketch 24.48 ±0.61 242757.23 ±42629.38
MMD 8.92 ±0.22 7620.49 ±527.86
SVD 5.89 ±0.00 4423.38 ±108.89
Pruning 5.18 ±0.23 6623.20 ±463.72
LTH 5.10 ±0.18 6628.73 ±462.03
Clustering 4.83±0.02 7030.91 ±561.52
MLP Fusion (Ours) 4.83±0.02 2826.59 ±155.06
key properties (details are provided in Appendix B-B in the
supplement).
Clustering without NTK approximation. Moreover, to clearly
ablate the effect of NTK approximation, we implement a
clustering-based method where the fused weights fW1,˜b1,
andfW2are replaced as follows:
fW1=W1¯C⊤P1
2,˜b1=¯Cb1P1
2,fW2=P1
2¯CW 2.
Here, P=CC⊤is a diagonal matrix. The corresponding
MLP is then defined as:
σ
XfW1+1˜b⊤
1
fW2+1b⊤
2,
which enjoys the same architecture as “Sketching” and “MMD”.
For the layer-wise task-specific tuning, we adopt the orig-
inal RoBERTa [66]/Switch Transformer [5] (GPT-2 [67] for
language generation) as the teacher and the language model
with fused MLP as the student, on the two NLU tasks. The
tuning only lasts for 1epoch so as to match the computational
cost of the pre-processing in LTH.
B. Preliminary Evaluation of Approximation Error
As a sanity check, we first perform the preliminary evaluation
of NTK approximation error for each applicable method. We
examine the output and the induced NTK of the first-layer
MLP on the validation set of SST2 with RoBERTa-base [66]
compressed by different baseline approaches. The results,
averaged over three runs, are summarized in Table I. Note
that DistilRoBERTa and Moefication were excluded from this
analysis due to their focus on different objectives. Specifically,
SVD is a deterministic method and therefore gives 0standard
deviation. We come up with the following observations: (i) Most
of the listed methods can well approximate the MLP output
with a small output distance between the original RoBERTa
and the compressed model. (ii) MLP Fusion achieves the
smallest NTK approximation error among all methods, defined
as the l2difference between the NTK kernel matrix computed
on the evaluation samples before and after compression.
This experiment verifies our proposal that MLP Fusion best
preserves the training dynamics of the original PLM.
C. Experiments on Natural Language Understanding
We provide extensive experimental comparisons based on
RoBERTa as the PLM with a set of representative baselines
on natural language understanding benchmarks SST2, MNLI,TABLE II
PERFORMANCE (ACC) ONSST2 AND MNLI VALIDATION SETS WITH
ROBERT A AS THE PLM. BOLD INDICATES THE BEST SCORE FOR EACH
METRIC ,WHILE UNDERLINED VALUES REPRESENT THE SECOND -BEST .
SST2 MNLI
RoBERTa 94.61 ±0.09 87.34 ±0.28
DistilRoBERTa 92.50 ±0.12 84.03 ±0.18
Sketch 91.90 ±0.14 83.30 ±0.11
MMD 92.54 ±0.41 84.20 ±0.24
SVD 92.55 ±0.24 85.23 ±0.04
FLOP 92.12 ±0.19 84.05 ±0.21
Pruning 92.78 ±0.17 85.82 ±0.12
LTH 92.91 ±0.15 85.96 ±0.10
GEM-MINER 92.89 ±0.16 85.51 ±0.11
Moefication 92.19 ±0.20 84.83 ±0.27
Clustering 93.01 ±0.17 85.75 ±0.04
MLP Fusion (Ours) 93.23 ±0.23 86.10±0.06
+Task-specific Tuning 93.79±0.07 86.32 ±0.06
which can be found in Table II, while additional test results
are provided in the supplementary material. Furthermore, we
present performance comparisons among various methods after
task-specific fine-tuning and two additional baselines that try
to maintain MLP output and NTK in the Appendix.
DistilRoBERTa, a lightweight version of RoBERTa with the
same training process, served as a baseline for comparison.
For all compressed methods, we reduced the intermediate
size to 25% ( 3072→768), aligning with the MLP input
size. Pruning/LTH involved masking 75% of MLP connections,
while Moefication divided the MLP into four experts to reduce
model size. All reductions were applied to the last 8 layers of
the PLM to ensure fairness.
From the table, we have the following findings: (i) MLP
Fusion outperforms all the baselines, which demonstrates the
effectiveness of our proposed approach. (ii) Without NTK
approximation, there is an obvious reduction in the performance
of Clustering, which verifies the necessity thereof. (iii) Layer-
wise task-specific tuning further enhances the performance of
MLP Fusion by incorporating task-specific knowledge. It is
worth noting that LTH and GEM-MINER also require pre-
processing when masking connections; after making them
computationally comparable to MLP Fusion, the superior
performance of our method more clearly validates its edges.
For the MoE model Switch Transformer, experiments were
conducted under similar conditions (c.f. the results in Table III).
Unlike RoBERTa, a distilled version of Switch Transformer
was unavailable. In addition to selecting the best-performing
baseline from Table II, we included M-SMoE [24], a MoE-
specific SFT method, and NTK-SAP [47], which combines
NTK with pruning. Similar conclusions can be drawn that
(i) Clustering’s performance falls behind compared to MLP
Fusion, which generally outperforms the baseline methods.
Furthermore, after additional distillation, the performance
improves even more. (ii) It is worth noting that even though
NTK-SAP produces a commendable result, the actual speed-up
is limited, as further discussed in Section V-F.

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
TABLE III
EVALUATION RESULTS OF SWITCH TRANSFORMER ON FOUR GLUE NLU TASKS (MEASURED IN ACCURACY ).BOLD INDICATES THE BEST SCORE FOR
EACH METRIC ,WHILE UNDERLINED VALUES REPRESENT THE SECOND -BEST .
SST2 MRPC CoLA MNLI
Switch Transformer 95.60 ±0.08 91.17 ±0.12 83.48 ±0.17 88.67 ±0.06
Sketch 91.25 ±0.23 69.69 ±0.38 69.12 ±0.00 83.36 ±0.28
MMD 94.19 ±0.04 87.91 ±0.29 81.50 ±0.03 87.25 ±0.02
Pruning 93.81 ±0.01 88.48 ±0.22 82.20 ±0.13 87.20 ±0.08
SVD 94.53 ±0.08 89.05 ±0.05 82.36 ±0.03 87.85 ±0.03
M-SMoE 94.84 ±0.03 89.05 ±0.06 82.36 ±0.03 87.51 ±0.03
NTK-SAP 94.88 ±0.02 89.30 ±0.12 82.61 ±0.05 87.85±0.03
Clustering 94.76 ±0.02 89.38 ±0.20 82.13 ±0.02 87.50 ±0.02
MLP Fusion 95.03 ±0.15 89.87±0.13 82.39±0.02 87.89 ±0.01
+Task-specific Tuning 95.15±0.08 89.95 ±0.08 82.65 ±0.01 87.91 ±0.02
TABLE IV
PERFORMANCE (%) OF BASELINE METHODS ON WEBNLG WITH GPT-2 AS THE PLM. BOLD INDICATES THE BEST SCORE FOR EACH METRIC ,WITH A
DOWN -ARROW DENOTING THAT LOWER VALUES ARE BETTER .UNDERLINED VALUES REPRESENT THE SECOND -BEST RESULTS .
WebNLG
BLEU MET TER ↓
S U A S U A S U A
GPT-2 57.93 22.55 42.17 0.42 0.25 0.34 0.39 0.76 0.56
Sketch 43.16 8.07 27.28 0.32 0.13 0.23 0.54 0.95 0.73
MMD 56.73 19.90 40.17 0.41 0.23 0.33 0.41 0.79 0.59
Pruning 55.21 21.80 40.55 0.40 0.25 0.33 0.42 0.76 [YF: 0.57]
Clustering 54.75 20.63 39.81 0.40 0.25 0.33 0.43 0.77 0.59
MLP Fusion (Ours) 57.12 21.03 40.79 0.41 0.25 0.33 0.41 0.78 0.58
+Task-specific Tuning 56.75 21.41 41.04 0.42 0.25 0.33 0.40 0.77 0.57
aThe letters S, U, and A in the WebNLG metric denote SEEN, UNSEEN, and ALL; instances under the SEEN categories are used for
training; instances under the UNSEEN categories are used for testing; ALL has all the instances in it.
D. Experiments on Natural Language Generation
In this part, we investigate the effectiveness of the proposed
MLP Fusion by evaluating on the natural language generation
benchmark WebNLG with a set of one-shot comparable
baselines. The results are reported in Table IV. The compress-
ing/pruning setup is the same as the NLU evaluation shown
in Section V-C. Our proposed method generally preserves the
performance of the naive fine-tuning approach most effectively.
MLP Fusion achieves an average accuracy improvement of
about 1 %compared to the baselines. Among the baselines, the
pruning method stands out due to its preservation of the original
MLP weight matrix size. However, the lack of robust support
for sparse matrix multiplication on modern GPU hardware
limits the actual efficiency gains, as detailed in Section V-F.
E. Ablation Studies
Impact of Sketch Layers. To analyze the effect of sketching
layers in PLMs, we extended our experiments beyond sketching
the last 8 layers on SST-2. Figure 4 provides insights, with
dashed lines representing the performance of RoBERTa and
DistilRoBERTa. The horizontal axis value "2" corresponds to
Fig. 4. Accuracy of each baseline method with respect to various numbers of
sketched layers on the SST2 data set.
sketching the last 2 layers. MLP Fusion consistently achieves
comparable or superior performance to the baselines, often
outperforming raw RoBERTa when fewer than 6 layers are
sketched. This demonstrates the potential of MLP Fusion to

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
Fig. 5. Training loss (dynamic) of our proposed MLP Fusion with respect to
various intermediate dimensions on the SST2 data set.
TABLE V
PERFORMANCE OF PROPOSED METHOD AND THE BASELINE METHOD AT
DIFFERENT LEVELS OF COMPRESSION .
Intermediate Dimension Sketch MLP Fusion
256 88.19 90.71
768 91.90 93.23
1536 92.09 93.46
reduce redundancy in neural networks, effectively offering
a "free lunch" in PLM fine-tuning. Moreover, MLP Fusion
consistently surpasses DistilRoBERTa as long as fewer than
10 layers are sketched. However, sketching all 12 layers
significantly reduces performance, indicating that the bottom
layers of PLMs retain critical semantic information. This
observation aligns with findings in [72].
The Choice of the Intermediate Dimension in MLP
Fusion. In our initial experiments, we set the intermediate
dimension to 768, matching the MLP input size. To explore
its impact, we analyzed the training dynamics and testing
performance of MLP Fusion across various intermediate sizes.
As shown in Figure 5, the training loss remains stable when
the intermediate dimension is 768or larger. However, reducing
the dimension below 768results in a sharp increase in loss
due to the MLP being rank-deficient, which adversely affects
performance. Testing results, presented in Table V, exhibit
a similar trend, underscoring the importance of maintaining
sufficient intermediate dimensionality.
The Influence of Freezing Router Matrix. Based on the
observation that LLM’s universal world information will impact
their performance [57]–[59], we follow [24] to freeze the router
matrix when fine-tuning the MoE model. Here, we compared
the empirical results from fine-tuning Switch Transformer on
the MRPC dataset in Table VI.
F . Efficiency Evaluation
In this subsection, we compare the efficiency of our method
with LoRA, on the MoE model Switch Transformer (in which
MLP takes a higher portion of parameters than dense models).
We provide the FLOPs and total time during SFT in Table
VII, and those during the inference stage in Table VIII. ForTABLE VI
INFLUENCE OF FREEZE ROUTER MATRIX OR NOT IN THE SFT STAGE FOR
THE MOEMODEL .
Frozen Not-frozen
MRPC 91.17 90.69
experiments during the SFT stage, we use the SST2 training
dataset for 5 rounds, with a batch size of 32 and a sequence
length of 100. For experiments during the inference stage, we
test the model on SST2 validation set with 10 epochs. We set
the config for LoRA on all MLP layers (which is different from
our top 8 MoE layers) with rank r= 8, coefficient α= 32
(the hyperparameters in LoRA).
Several conclusions could be obtained based on the results:
(i)NTK-SAP and Hardware Efficiency. As an unstructured
pruning method, NTK-SAP does not translate into hardware ef-
ficiency improvements. Despite reducing parameter counts, the
nature of unstructured pruning doesn’t align with hardware ac-
celeration capabilities, which are more optimized for structured
reductions. (ii) LoRA’s Trade-offs. While LoRA effectively
reduces memory usage during the SFT stage by modifying
only a few parameters, it fails to improve efficiency during
the inference stage. Even if the overhead from the additional
low-rank matrices can be eliminated by integrating them back
into the full matrices, the efficiency gains remain limited to the
training phase rather than inference. (iii) Structured Pruning
and SVD. These methods offer a balanced trade-off, with
structured pruning and SVD significantly reducing memory
and parameter sizes. They also perform well in inference,
making them favorable for deployment scenarios where memory
and speed are critical. (iv) MLP Fusion’s Consistency. Our
approach demonstrates similar efficiency gains as structured
pruning and SVD. It reduces memory usage and maintains
competitive runtime performance both during training and
inference, showing its robustness and suitability for deployment.
VI. C ONCLUSION
In this paper, we propose MLP fusion, a novel one-
shot model compression method that utilizes clustering to
approximate the NTK of the original PLM. We demonstrate
that the fused MLP can both well approximate the output
and attain the closest NTK to the original one compared
to other one-shot compression methods. At the same time,
MLP fusion can be applied to Mixture-of-Experts models and
obtain larger gains in space efficiency, underlining its universal
effectiveness and scalability in PLMs. One direct extension of
our work is using MLP fusion as an initialization method for
distillation. Compared to re-training from scratch, we expect the
information preserved in the fused MLP can ease the following
distillation and speed up the model convergence. We believe
MLP fusion sheds some light on the new paradigm for efficient
language model fine-tuning.
ACKNOWLEDGEMENTS
This work is supported by National Science Foundation
under Award No. IIS-1947203, IIS-2117902, IIS-2137468,

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
TABLE VII
EFFICIENCY EVALUATION DURING THE SFT STAGE .
Memory (GB) Runtime (s) TFLOPs/Model GFLOPs/MLP Layer Params (M) Trainable Params (M)
Full 24.02 6697.01 ±4.76 3.90 90.63 1,073 1,073
NTK-SAP 24.08 6687.40 ±9.81 3.37 22.66 1,073 1,073
Structured Pruning 15.46 5759.46 ±9.25 3.37 22.66 620 620
SVD 15.47 6562.39 ±4.98 3.37 22.66 619 619
LoRA 13.51 6424.75 ±4.01 2.28 31.39 1,086 123
MLP Fusion 15.46 5782.12 ±8.90 3.37 22.66 620 620
TABLE VIII
EFFICIENCY EVALUATION DURING THE INFERENCE STAGE .
Memory (GB) Runtime (s) TFLOPs/Model GFLOPs/MLP Layer Params (M)
Full 5.38 170.33 ±0.03 1.30 30.21 1,073
NTK-SAP 5.38 168.88 ±0.08 1.12 7.55 1,073
Structured Pruning 4.88 163.43 ±0.25 1.12 7.55 620
SVD 3.89 176.34 ±0.68 1.12 7.55 619
LoRA 5.38 169.53 ±0.05 1.30 30.21 1,073
MLP Fusion 4.88 162.37 ±0.14 1.12 7.55 620
and Agriculture and Food Research Initiative (AFRI) grant
no. 2020-67021-32799/project accession no.1024178 from the
USDA National Institute of Food and Agriculture. The views
and conclusions are those of the authors and should not be
interpreted as representing the official policies of the funding
agencies or the government.
REFERENCES
[1]J. Howard and S. Ruder, “Universal language model fine-tuning for
text classification,” in Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) ,
2018, pp. 328–339.
[2]M. Kale and A. Rastogi, “Text-to-text pre-training for data-to-text tasks,”
inProceedings of the 13th International Conference on Natural Language
Generation . Dublin, Ireland: Association for Computational Linguistics,
Dec. 2020, pp. 97–102.
[3]M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” in
Proceedings of NAACL-HLT , 2018, pp. 2227–2237.
[4]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language models
are few-shot learners,” arXiv preprint arXiv:2005.14165 , 2020.
[5]W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity,” The Journal
of Machine Learning Research , vol. 23, no. 1, pp. 5232–5270, 2022.
[6]J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural
language models,” 2020.
[7]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[8]H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,
E. Grave, and G. Lample, “Llama: Open and efficient foundation language
models,” 2023.
[9]N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,
and J. Dean, “Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.
[10] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel,
G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock,S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril,
T. Wang, T. Lacroix, and W. E. Sayed, “Mixtral of experts,” 2024.
[11] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” 2015, cite arxiv:1503.02531Comment: NIPS 2014 Deep
Learning Workshop.
[12] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A
survey,” International Journal of Computer Vision , vol. 129, pp. 1789–
1819, 2021.
[13] H. He, J. Wang, Z. Zhang, and F. Wu, “Compressing deep graph
neural networks via adversarial knowledge distillation,” in Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining , ser. KDD ’22. New York, NY , USA: Association
for Computing Machinery, 2022, p. 534–544. [Online]. Available:
https://doi.org/10.1145/3534678.3539315
[14] S. Kang, J. Hwang, W. Kweon, and H. Yu, “De-rrd: A knowledge
distillation framework for recommender system,” in Proceedings of
the 29th ACM International Conference on Information & Knowledge
Management , ser. CIKM ’20. New York, NY , USA: Association
for Computing Machinery, 2020, p. 605–614. [Online]. Available:
https://doi.org/10.1145/3340531.3412005
[15] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efficient neural network,” Advances in neural information
processing systems , vol. 28, 2015.
[16] N. Lee, T. Ajanthan, and P. Torr, “SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY,” in International
Conference on Learning Representations , 2019.
[17] C. Wang, G. Zhang, and R. Grosse, “Picking winning tickets before
training by preserving gradient flow,” arXiv preprint arXiv:2002.07376 ,
2020.
[18] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, “Pruning neural
networks without any data by iteratively conserving synaptic flow,”
Advances in Neural Information Processing Systems , vol. 33, pp. 6377–
6389, 2020.
[19] T. Dao, B. Chen, N. S. Sohoni, A. Desai, M. Poli, J. Grogan, A. Liu,
A. Rao, A. Rudra, and C. Ré, “Monarch: Expressive structured matrices
for efficient and accurate training,” in International Conference on
Machine Learning . PMLR, 2022, pp. 4690–4721.
[20] E. L. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus, “Exploiting
linear structure within convolutional networks for efficient evaluation,”
Advances in neural information processing systems , vol. 27, 2014.
[21] X. Wang, Y . Zheng, Z. Wan, and M. Zhang, “Svd-llm: Truncation-aware
singular value decomposition for large language model compression,”
2024. [Online]. Available: https://arxiv.org/abs/2403.07378
[22] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and
W. Chen, “Lora: Low-rank adaptation of large language models,” 2021.

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
[23] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao, “Merging experts
into one: Improving computational efficiency of mixture of experts,” in
Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing , 2023, pp. 14 685–14 691.
[24] P. Li, Z. Zhang, P. Yadav, Y .-L. Sung, Y . Cheng, M. Bansal, and T. Chen,
“Merge, then compress: Demystify efficient smoe with hints from its
routing policy,” arXiv preprint arXiv:2310.01334 , 2023.
[25] F. Xue, X. He, X. Ren, Y . Lou, and Y . You, “One student knows all
experts know: From sparse to dense,” 2022.
[26] C. Liu, C. Lou, R. Wang, A. Y . Xi, L. Shen, and J. Yan, “Deep
neural network fusion via graph matching with applications to model
ensemble and federated learning,” in Proceedings of the 39th International
Conference on Machine Learning , ser. Proceedings of Machine Learning
Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and
S. Sabato, Eds., vol. 162. PMLR, 17–23 Jul 2022, pp. 13 857–13 869.
[27] G. Stoica, D. Bolya, J. Bjorner, P. Ramesh, T. Hearn, and J. Hoffman,
“Zipit! merging models from different tasks without training,” 2024.
[28] X. Lu, Q. Liu, Y . Xu, A. Zhou, S. Huang, B. Zhang, J. Yan, and H. Li,
“Not all experts are equal: Efficient expert pruning and skipping for
mixture-of-experts large language models,” 2024. [Online]. Available:
https://arxiv.org/abs/2402.14800
[29] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efficient trans-
former,” in 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net,
2020.
[30] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,
T. Sarlós, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger,
L. Colwell, and A. Weller, “Rethinking attention with performers,” CoRR ,
vol. abs/2009.14794, 2020.
[31] Y . Chen, Q. Zeng, H. Ji, and Y . Yang, “Skyformer: Remodel self-attention
with gaussian kernel and nystr \" om method,” Advances in Neural
Information Processing Systems , vol. 34, pp. 2122–2135, 2021.
[32] A. Jacot, F. Gabriel, and C. Hongler, “Neural tangent kernel: Convergence
and generalization in neural networks,” Advances in neural information
processing systems , vol. 31, 2018.
[33] S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang,
“On exact computation with an infinitely wide neural net,” Advances in
Neural Information Processing Systems , vol. 32, 2019.
[34] S. P. Singh and M. Jaggi, “Model fusion via optimal transport,” Advances
in Neural Information Processing Systems , vol. 33, pp. 22 045–22 055,
2020.
[35] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng, and
C. Potts, “Recursive deep models for semantic compositionality over a
sentiment treebank,” in Proceedings of the 2013 conference on empirical
methods in natural language processing , 2013, pp. 1631–1642.
[36] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,” arXiv preprint
arXiv:1910.01108 , 2019.
[37] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and
Q. Liu, “Tinybert: Distilling bert for natural language understanding,”
arXiv preprint arXiv:1909.10351 , 2019.
[38] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, “Minilm: Deep
self-attention distillation for task-agnostic compression of pre-trained
transformers,” Advances in Neural Information Processing Systems ,
vol. 33, pp. 5776–5788, 2020.
[39] S. Lohit and M. Jones, “Model compression using optimal transport,”
inProceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , 2022, pp. 2764–2773.
[40] Z. Huang and N. Wang, “Like what you like: Knowledge distill via
neuron selectivity transfer,” arXiv preprint arXiv:1707.01219 , 2017.
[41] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, and Q. Ju, “Fastbert:
a self-distilling bert with adaptive inference time,” arXiv preprint
arXiv:2004.02178 , 2020.
[42] J. Xin, R. Tang, J. Lee, Y . Yu, and J. Lin, “Deebert: Dynamic early
exiting for accelerating bert inference,” in Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , 2020, pp. 2246–
2251.
[43] Z. Zhang, Y . Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, “Moefication:
Conditional computation of transformer models for efficient inference,”
arXiv preprint arXiv:2110.01786 , 2021.
[44] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning efficient
convolutional networks through network slimming,” in Proceedings of the
IEEE international conference on computer vision , 2017, pp. 2736–2744.
[45] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding sparse,
trainable neural networks,” arXiv preprint arXiv:1803.03635 , 2018.[46] T. Chen, J. Frankle, S. Chang, S. Liu, Y . Zhang, Z. Wang, and M. Carbin,
“The lottery ticket hypothesis for pre-trained bert networks,” Advances in
neural information processing systems , vol. 33, pp. 15 834–15 846, 2020.
[47] Y . Wang, D. Li, and R. Sun, “NTK-SAP: Improving neural network
pruning by aligning training dynamics,” in The Eleventh International
Conference on Learning Representations , 2023.
[48] L. Iurada, M. Ciccone, and T. Tommasi, “Finding lottery tickets in vision
models via data-driven spectral foresight pruning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) , June 2024, pp. 16 142–16 151.
[49] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” 2015.
[50] D. P. Woodruff et al. , “Sketching as a tool for numerical linear algebra,”
Foundations and Trends ®in Theoretical Computer Science , vol. 10, no.
1–2, pp. 1–157, 2014.
[51] Y . Chen, Q. Zeng, D. Hakkani-Tur, D. Jin, H. Ji, and Y . Yang,
“Sketching as a tool for understanding and accelerating self-attention for
long sequences,” in Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies . Seattle, United States: Association for
Computational Linguistics, Jul. 2022, pp. 5187–5199.
[52] H. Robbins and S. Monro, “A stochastic approximation method,” The
annals of mathematical statistics , pp. 400–407, 1951.
[53] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,
Y . Bengio and Y . LeCun, Eds., 2015.
[54] S. Malladi, A. Wettig, D. Yu, D. Chen, and S. Arora, “A kernel-based
view of language model fine-tuning,” arXiv preprint arXiv:2210.05643 ,
2022.
[55] A. Wei, W. Hu, and J. Steinhardt, “More than a toy: Random matrix
models predict how real-world neural representations generalize,” arXiv
preprint arXiv:2203.06176 , 2022.
[56] L. Gu, Y . Du, Y . Zhang, D. Xie, S. Pu, R. C. Qiu, and Z. Liao, “”lossless”
compression of deep neural networks: A high-dimensional neural tangent
kernel approach,” in Advances in Neural Information Processing Systems ,
A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.
[57] G. He, J. Chen, and J. Zhu, “Preserving pre-trained features helps
calibrate fine-tuned language models,” in The Eleventh International
Conference on Learning Representations , 2023. [Online]. Available:
https://openreview.net/forum?id=NI7StoWHJPT
[58] J. Mukhoti, Y . Gal, P. H. S. Torr, and P. K. Dokania, “Fine-tuning can
cripple your foundation model; preserving features may be the solution,”
2023.
[59] S. Dou, E. Zhou, Y . Liu, S. Gao, J. Zhao, W. Shen, Y . Zhou, Z. Xi,
X. Wang, X. Fan et al. , “Loramoe: Revolutionizing mixture of ex-perts
for maintaining world knowledge in language model alignment,” arXiv
preprint arXiv:2312.09979 , 2023.
[60] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-
Tur, “Inducer-tuning: Connecting prefix-tuning and adapter-tuning,” in
Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing . Association for Computational Linguistics, 2022.
[61] B. Wang, Y . Ren, L. Shang, X. Jiang, and Q. Liu, “Exploring extreme
parameter compression for pre-trained language models,” in International
Conference on Learning Representations , 2022.
[62] B. Yuan, C. R. Wolfe, C. Dun, Y . Tang, A. Kyrillidis, and C. Jermaine,
“Distributed learning of fully connected neural networks using independent
subnet training,” Proceedings of the VLDB Endowment , vol. 15, no. 8,
pp. 1581–1590, 2022.
[63] J. M. Hammersley and K. W. Morton, “Poor man’s monte carlo,” Journal
of the Royal Statistical Society: Series B (Methodological) , vol. 16, no. 1,
pp. 23–38, 1954.
[64] S. Lloyd, “Least squares quantization in pcm,” IEEE transactions on
information theory , vol. 28, no. 2, pp. 129–137, 1982.
[65] G. Peyré, M. Cuturi et al. , “Computational optimal transport: With
applications to data science,” Foundations and Trends ®in Machine
Learning , vol. 11, no. 5-6, pp. 355–607, 2019.
[66] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.
[67] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
“Language models are unsupervised multitask learners,” OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[68] K. Sreenivasan, J. yong Sohn, L. Yang, M. Grinde, A. Nagle, H. Wang,
E. Xing, K. Lee, and D. Papailiopoulos, “Rare gems: Finding lottery
tickets at initialization,” in Advances in Neural Information Processing
Systems , A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
[69] Z. Wang, J. Wohlwend, and T. Lei, “Structured pruning of large language
models,” in Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) . Online: Association for
Computational Linguistics, Nov. 2020, pp. 6151–6162.
[70] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a dis-
tilled version of bert: smaller, faster, cheaper and lighter,” ArXiv , vol.
abs/1910.01108, 2019.
[71] N. Ailon and B. Chazelle, “The fast johnson–lindenstrauss transform and
approximate nearest neighbors,” SIAM Journal on computing , vol. 39,
no. 1, pp. 302–322, 2009.
[72] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y . Artzi, “Revisiting
few-sample bert fine-tuning,” arXiv preprint arXiv:2006.05987 , 2020.
[73] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-
Tur, “Empowering parameter-efficient transfer learning by recognizing
the kernel structure in self-attention,” in Findings of the Association for
Computational Linguistics: NAACL 2022 . Association for Computational
Linguistics, 2022.
[74] Y . Zhao, J. Huang, J. Hu, X. Wang, Y . Mao, D. Zhang, Z. Jiang,
Z. Wu, B. Ai, A. Wang, W. Zhou, and Y . Chen, “Swift:a scalable
lightweight infrastructure for fine-tuning,” 2024. [Online]. Available:
https://arxiv.org/abs/2408.05517
[75] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in
International Conference on Learning Representations , 2018.
[76] W. B. Dolan and C. Brockett, “Automatically constructing a corpus
of sentential paraphrases,” in Proceedings of the Third International
Workshop on Paraphrasing (IWP2005) , 2005. [Online]. Available:
https://aclanthology.org/I05-5002
[77] A. Warstadt, A. Singh, and S. R. Bowman, “Neural network acceptability
judgments,” Transactions of the Association for Computational
Linguistics , vol. 7, pp. 625–641, 2019. [Online]. Available: https:
//aclanthology.org/Q19-1040
[78] C. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, “Creating
training corpora for nlg micro-planning,” in Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) . Association for Computational Linguistics (ACL),
Aug. 2017, pp. 179–188, 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017 ; Conference date: 30-07-2017
Through 04-08-2017.
[79] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311–318.
[80] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation
with improved correlation with human judgments,” in Proceedings of the
acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization , 2005, pp. 65–72.
[81] M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul, “A study
of translation edit rate with targeted human annotation,” in Proceedings
of the 7th Conference of the Association for Machine Translation in the
Americas: Technical Papers , 2006, pp. 223–231.
[82] K. Fukushima, “Cognitron: A self-organizing multilayered neural net-
work,” Biological cybernetics , vol. 20, no. 3, pp. 121–136, 1975.
[83] L. Balles and P. Hennig, “Dissecting adam: The sign, magnitude and
variance of stochastic gradients,” in International Conference on Machine
Learning . PMLR, 2018, pp. 404–413.
[84] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence
sequence-to-sequence model for speech recognition,” in 2018 IEEE
international conference on acoustics, speech and signal processing
(ICASSP) . IEEE, 2018, pp. 5884–5888.
[85] S. Geng, S. Liu, Z. Fu, Y . Ge, and Y . Zhang, “Recommendation as
language processing (rlp): A unified pretrain, personalized prompt &
predict paradigm (p5),” in Proceedings of the 16th ACM Conference on
Recommender Systems , 2022, pp. 299–315.
[86] T. Wei and J. He, “Comprehensive fair meta-learned recommender
system,” in Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining , 2022, pp. 1989–1999.
[87] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y . Shen, and T.-Y .
Liu, “Do transformers really perform badly for graph representation?”
Advances in Neural Information Processing Systems , vol. 34, pp. 28 877–
28 888, 2021.
[88] T. Wei, Y . You, T. Chen, Y . Shen, J. He, and Z. Wang, “Augmentations in
hypergraph contrastive learning: Fabricated and generative,” in Advances
in Neural Information Processing Systems , 2022.Mengting Ai is a first-year Ph.D. student at the School of Information Sciences,
University of Illinois at Urbana-Champaign. She earned her M.S. in Computer
Science from the University of Illinois at Urbana-Champaign in 2023 and her
B.S. in Computer Science from Xi’an Jiaotong University in 2022, graduating
as an outstanding student. Her research focuses on efficient machine learning,
particularly large language model compression. Mengting has published in
top-tier conferences such as KDD and actively contributes to the research
community, including serving on the AAAI 2024 program committee. Her
expertise in NLP and scalable AI systems stems from both academic research
and collaborative projects, reflecting a commitment to advancing efficient and
scalable AI technologies.
Tianxin Wei is a fourth-year Ph.D. student at the University of Illinois at
Urbana-Champaign. He earned his B.S. in Computer Science from the School
of the Gifted Young at the University of Science and Technology of China in
2020. His research focuses on trustworthy machine learning, with applications
in language modeling, information retrieval, and agriculture. He is the recipient
of the University Nomination for Apple Scholar in AI/ML 2024, the Amazon
Internship Fellowship in 2024, the Conference Presentation Award at UIUC in
2023, NeurIPS Scholar Awards in 2022 and 2023, and the ICML Grant Award
in 2023. He has authored more than 20 publications at major conferences
(ICML, NeurIPS, ICLR, KDD), and his work has earned the SIGIR Best
Paper Honorable Mention in 2021. He has served as a program committee
member for leading venues (ICML, NeurIPS, ICLR, KDD, AAAI, WSDM,
etc.). His dedication to advancing impactful research was recognized with the
Outstanding Reviewer Award for KDD 2025.
Yifan Chen (Member, IEEE) received the B.S. degree from Fudan University,
Shanghai, China, in 2018, and the PhD degree in Statistics from the University
of Illinois Urbana-Champaign in 2023. He is currently an assistant professor
in computer science and math at Hong Kong Baptist University. He is broadly
interested in developing efficient machine learning algorithms, encompassing
both statistical and deep learning models. He has published several papers in
these areas, including ICML, Neurips, KDD, etc.
Zeming Guo is currently pursuing a Master of Science at Cornell University,
where his research focuses on advancing language modeling. He earned his
Bachelor of Science in Information Science from the University of Illinois at
Urbana-Champaign in 2023. He has been working at Amazon Web Services
(AWS), where he contributed to the development of large-scale distributed
systems. His research interests include large language model reasoning,
knowledge distillation, and enhancing the efficiency of deep learning models.
He has published first-author papers at the prestigious ICML conference.
Jingrui He (Senior Member, IEEE) is a Professor at School of Information
Sciences, University of Illinois at Urbana-Champaign. She received her
PhD from Carnegie Mellon University in 2010. Her research focuses on
heterogeneous machine learning, active learning, neural bandits, and self-
supervised learning, with applications in security, agriculture, social network
analysis, healthcare, and finance. Dr. He is the recipient of the 2016 NSF
CAREER Award, the 2020 OAT Award, three times recipient of the IBM
Faculty Award in 2018, 2015 and 2014 respectively, and was selected as
IJCAI 2017 Early Career Spotlight. Dr. He has more than 180 publications
at major conferences (e.g., ICML, NeurIPS, ICLR, KDD) and journals (e.g.,
TMLR, TKDD, JMLR), and is the author of two books. Her papers have
received the Distinguished Paper Award at FAccT 2022, as well as Bests of
the Conference at ICDM 2016, ICDM 2010, and SDM 2010. Dr. He is a
Distinguished Member of ACM, a Senior Member of AAAI and IEEE. She is
also the Program Co-chair of IEEE BigData 2023.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
Supplementary Material for “MLP Fusion: Towards Efficient
Fine-tuning of Dense and Mixture-of-Experts Language Models”
APPENDIX A
DETAILS OF EXPERIMENTS
A. Experimental Setup
We evaluate the proposed MLP fusion on various downstream NLP tasks and provide a sketch of these tasks in this section.
In addition, we succinctly introduce two intuitive while non-trivial baseline methods, “Sketching” and “MMD”, in Section II.
Part of the experiment implementations are borrowed from [24], [60], [73], [74]. The code for our algorithms is available at
https://github.com/weitianxin/MLP_Fusion.
All the models in this work are implemented by PyTorch. The experiments are all conducted on one Tesla V100 32 GB GPU.
For NLU tasks, We fine-tune RoBERTa [66] with an AdamW [75] optimizer and use a polynomial learning rate scheduler to
make the learning rate linearly decay; concretely, the learning rate is linearly warmed up from 0 for the first 0.06 epoch. The
learning rate is searched in the range of {1e-5, 2e-5,4e-5, 6e-5, 8e-5}, and the batch size is fixed as 32. For NLG tasks, we
keep using AdamW optimizer to fine-tune GPT-2 [67], and a linear learning rate scheduler with a 500-step warmup duration is
used. The learning rate is tuned in the same range as above while the batch size is fixed to 8. By default, all the compared
methods reduce the MLP intermediate size to 768 or a comparable number of parameters from 3076. The reduction/sketching
is performed on the last 8layers of the PLM by default. For Clustering, we adopt the K-Means algorithm due to its simplicity
and effectiveness. To reduce the random variability in the results, the experiments are all averaged over three runs.
As for Switch Transformer, the learning rate is searched in the range of {1e-4,2e-4,3e-4,5e-4,1e-3}, the batch size within the
range of {16,32,64}, and the training epoch within the range of {3,5,10,15,20}. The details of the AdamW optimizer which is
fixed for all datasets are given in table IX.
To ensure comparability across methods, we standardize the parameter count reduction for the experts to around 75%, which
means 25% of the parameters will be retained. All the methods are performed at the top 8 MoE layers of Switch Transformer.
TABLE IX
FINE-TUNING HYPER -PARAMETERS SETTING FOR SWITCH TRANSFORMER .
Value
Optimizer AdamW
Adam ϵ 1e-08
Adam β (0.9, 0.98)
warm-up steps 8
weight decay 0.01
B. Runtime of fine-tuning after PLM compression
Since our proposed MLP fusion only differs from the sketching and mmd baselines in initialization, we focus on the runtime
evaluation of MLP fusion along with two representative methods, regular fine-tuning and pruning.
For a fair comparison, we intentionally run the two NLU tasks on a cluster server (so that no other processes will compete
with the model fine-tuning) with one core of a server CPU (Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz) on Ubuntu 18.04.
In this setting, we train the RoBERTa model for 100 steps with batch size 32.

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14
Specifically, on SST2, it will take the model with MLP fusion, pruning, and regular fine-tuning around 6746 ,18066 ,9342
seconds to finish the training, respectively; on MNLI, the time cost is around 6956 ,17060 ,18966 seconds for the training. We
remark the architecture of MLP fusion can accelerate the regular fine-tuning by 30% on SST2, and is even 2.7times faster in
MNLI, which has longer average sequence length. As for pruning, although it has a comparable prediction performance in the
two tasks, its time cost is no less than regular fine-tuning and is much higher in the more lightweight task SST2, due to some
overhead cost from its implementation.
C. The parameter count of SVD
For SVD, to make the parameters retained for each expert matrix equal, we have:
pI×k+k+k×p≈k×(pI+p)
s×pI×p=sppI,
where sis the parameter rate we retain (25% here), and kis the number of top-k singular values in SVD. For Switch Transformer,
we have pI= 4p, sok=4
5sp. For Qwen we have pI=11
16p, sok=11
27sp.
D. Details of the Datasets
We provide the details of the datasets we used in the experiment along with their license here. The statistics can be found in
Tables X and XI.
TABLE X
DATASET STATISTICS OF FINE -TUNED CLASSIFICATION TASKS .
Dataset Category Train size Test Size Classes
SST2 Sentiment Analysis 67,349 872 2
MRPC Paraphrase Identification 3,668 408 2
CoLA Linguistic Acceptability Judgment 8,551 1,043 2
MNLI Textual Entailment 392,702 9,815 3
TABLE XI
DATASET STATISTICS OF FINE -TUNED GENERATION TASKS .
Dataset Category Train size Test Size Average Text Length
WebNLG Text Generation 35,426 5,150 24.36
•SST2 [35]: SST2, the Stanford Sentiment Treebank version 2, is a popular dataset for sentiment analysis. It contains
movie review sentences labeled as positive or negative, excluding neutral sentences, providing a binary classification
task. This dataset is notable for its fine-grained annotation, as it includes sentiment labels for every subphase within the
sentence parse trees. SST2 is widely used for training and evaluating models on sentiment analysis, testing their ability to
understand nuanced emotional tones in text, with the license of CC0: Public Domain.
•MRPC [76]: The Microsoft Research Paraphrase Corpus (MRPC) evaluates models on paraphrase identification by using
sentence pairs from online news sources. MRPC is a part of the GLUE benchmark and is valuable for assessing a model’s
ability to understand and compare semantic content in sentences, especially in semantic analysis tasks. The license of
MRPC is unknown.

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15
•CoLA [77]: The Corpus of Linguistic Acceptability (CoLA) assesses models’ linguistic acceptability judgment. It
distinguishes between grammatically acceptable and unacceptable sentences, emphasizing the importance of grammatical
understanding in language comprehension and model evaluation. The license for CoLA is not specified.
•MNLI [77]: The Multi-Genre Natural Language Inference (MNLI) dataset is a diverse corpus for natural language
understanding tasks, focusing on textual entailment. It includes pairs of sentences and challenges models to determine
whether the second sentence entails, contradicts, or remains neutral to the first sentence. MNLI’s wide range of genres and
diverse content makes it a robust benchmark for evaluating models in natural language inference tasks. Most of the data
are under the OANC’s license, with the other falling under several permissive licenses, a Creative Commons Share-Alike
3.0 Unported License, and Creative Commons Attribution 3.0 Unported Licenses.
•WebNLG [78]: This dataset is composed of data/text pairs, where “data” is in a format of (subject, property, object) triple.
For the train and the validation set, there are nine categories extracted from DBpedia; while in the test set, there are five
extra unseen categories, which can partially reflect the generalizability of the methods. The input sequences in the training
set contain 1to7triples, and the lengths of most sequences are bounded by 50 (as each triple only includes three short
phrases). The official evaluation script is used in our experiments, and we report BLEU [79], METEOR [80] and TER
[81] as the metrics.
APPENDIX B
DERIVATIONS OMITTED IN THE MAIN TEXT
A. Computational costs of attention and MLP moduels
Condensing FFN sub-layers is critical to obtaining a lightweight pre-trained model. Besides self-attention sub-layers, FFN
sub-layers also take a lot of computation time and even become the actual bottleneck when the input sequence length is short.
We will verify this claim through the following derivation.
We first recall the most common setting of an MLP in PLMs. Taking RoBERTa-base as an example, the hidden dimension is
p= 768 and there are h= 12 heads in each self-attention module; the intermediate dimension in MLP is pI= 4p= 3072 . In
the self-attention sub-layer, given the length- ninput Xwe need to first compute the query, key, and value matrix Q,K,V,
which takes 3·np2operations to perform the linear transform (omitting the bias). For the core self-attention module, we will at
least need h·2n2(p/h)multiplication operations; the final linear transform will again take np2cost. The total FLOPs of a
self-attention sub-layer are around 4np2+ 2n2p.
As for the FFN sub-layer, the computational cost is clear: 2nppI= 8np2. We can check for regular nlp tasks in which the
input length nis bounded by 512,8np2proves to be larger than 4np2+ 2n2p, when p= 768 . More specifically, when input
length n <2p, the computation cost of FFN layers becomes the primary bottleneck. This condition is particularly applicable to
modern foundational language models [8], which often possess a massive hidden size even exceeding ten thousand.
B. Maximum mean discrepancies (MMD)
We start with a brief introduction to MMD. The expression of MMD between two distributions PandQis given as
MMD (P, Q) = sup
∥f∥H≤1EX∼P[f(X)]−EY∼Q[f(Y)]
=∥EX∼P[φ(X)]−EY∼Q[φ(Y)]∥H, (11)

--- PAGE 16 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16
where φ(·) :X → H is the feature map inducing the kernel function k(x, y) =⟨φ(x), φ(y)⟩Hassociated with a reproducing
kernel Hilbert space (RKHS) H. Through the strong reproducing property of the map φ, we can rewrite the the squared MMD
as
MMD2(P, Q) =∥EX∼Pφ(X)−EY∼Qφ(Y)∥2
H
=⟨EX∼Pφ(X),EX′∼Pφ(X′)⟩H+⟨EY∼Qφ(Y),EY′∼Qφ(Y′)⟩H−2⟨EX∼Pφ(X),EY∼Qφ(Y)⟩H
=EX,X′∼Pk(X, X′) +EY,Y′∼Qk(Y, Y′)−2EX∼P,Y∼Qk(X, Y), (12)
which is easier to optimize using back-propagation.
Following the empirical distribution view of MLP, we denote the original MLP as µw, a uniform discrete distribution over
the rows of the embedding matrix W, and the compressed MLP similarly as ˆµ, an empirical distribution evenly distributed
over the rows in matrix ˆW= [ˆW1,ˆb1,ˆW2]∈Rc×(2p+1)). We can then optimize the following problem
min
ˆWMMD2
µw,ˆµ
ˆW
, (13)
from which we can obtain ˆW. As in Section IV-B, we can construct the condensed MLP with ˆWas
eHM=pI
kh
σ
X(W(m)
1)⊤+1(b(m))⊤
W(m)
2i
+1b⊤
2, (14)
which additionally introduces a factor pI/csince expectations rather than sums are involved in MMD.
C. NTK preservation
In the main text, we have made the assumption that CfW≈WandeHC≈H. This assumption implies that ∇Hfcan be
preserved by ∇eHCfc, which helps obtain ⟨∇b2f(X),sign (∇b2f(Z))⟩ ≈D
∇˜b2fc(X),sign
∇˜b2fc(Z)E
.
For the remaining three term, we first address 1:=D
∇fW2fc(X),sign
∇fW2fc(Z)E
:
1= Tr
∇eHCfc(X)⊤
˜σxP·sign
P˜σ⊤
z∇eHCfc(Z)
= Tr
∇eHCfc(X)⊤
σ
XfW1+1˜b⊤
1
P·sign
P⊤σ
fW⊤
1Z⊤+˜b11⊤
∇eHCfc(Z)
(i)= Tr
∇eHCfc(X)⊤
σ
XfW1+1˜b⊤
1
CC⊤sign
σ
fW⊤
1Z⊤+˜b11⊤
∇eHCfc(Z)
(ii)= Tr
∇eHCfc(X)⊤
σ
XfW1C+1˜b⊤
1C
sign
σ
C⊤fW⊤
1Z⊤+C⊤˜b11⊤
∇eHCfc(Z)
≈Trh
(∇Hf(X))⊤σ 
XW 1+1b⊤
1
sign 
σ 
W⊤
1Z⊤+b11⊤
∇Hf(Z)i
=⟨∇W2f(X),sign (∇W2f(Z))⟩,
where equation (i)above holds since P=CC⊤and the positive diagonal matrix Pwill not impact the sign of the matrix
elements; as for equation (ii), the “copy” matrix C, as discussed in Section IV-B , is free to be brought inside both the sign
function and the activation function.
For⟨∇W1f(X),sign (∇W1f(Z))⟩, we need to verify the product
X⊤ 
∇Hf(X)W⊤
2
⊙σ′
x
·sign 
∇Hf(Z)W⊤
2
⊙σ′
z⊤Z

--- PAGE 17 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17
can be approximated by 2:=X⊤h
∇eHCfc(X)fW⊤
2P
⊙˜σ′
xi
·signh
∇eHCfc(Z)fW⊤
2P
⊙˜σ′
zi⊤
Z
, where ˜σ′
x:=
σ′
XfW1+1˜b⊤
1
,˜σz:=σ′
ZfW1+1˜b⊤
1
, and σ′(·)is the derivative of the activation function σ(·)2. We show the
derivation as follows:
2(i)=X⊤h
∇eHCfc(X)fW⊤
2
⊙˜σ′
xi
P·sign
Ph
∇eHCfc(Z)fW⊤
2
⊙˜σ′
zi⊤
Z
=X⊤h
∇eHCfc(X)fW⊤
2
⊙˜σ′
xi
CC⊤·signh
∇eHCfc(Z)fW⊤
2
⊙˜σ′
zi⊤
Z
=X⊤h
∇eHCfc(X)fW⊤
2C
⊙(˜σ′
xC)i
·signh
∇eHCfc(Z)fW⊤
2C
⊙(˜σ′
zC)i⊤
Z
≈X⊤ 
∇Hf(X)W⊤
2
⊙σ′
x
· 
∇Hf(Z)W⊤
2
⊙σ′
z⊤Z,
in which we obtain equation (i)because Pas a diagonal matrix has the same scaling effect on the Hadamard producth
∇eHCfc(X)fW⊤
2
⊙˜σ′
xi
as on one of its component ∇eHCfc(X)fW⊤
2; the rest equations simply follow the previous derivations.
For the last term ⟨∇b1f(X),sign (∇b1f(Z))⟩, we solely need to replace the above input matrix X,Zwith1⊤, and all the
derivation steps will follow.
D. NTK preservation for MoE modules
Considering there are no bias terms in common MoE modules, we solely study the NTK preservation for two weight matrices
W1,W2in this subsection. In advance of the derivation, we first restate the assumptions in Section IV-C thatfW1C(m)≈W1,
(C(m))⊤fW2≈W2, andeHm≈Hm; we also pay close attention to the standalone matrix P(m)
ifor MoE modules:
P(m)
i=C(m)Ri(C(m))⊤
=
C1[G(xi)]1IpIC⊤
1
...
CN[G(xi)]NIpIC⊤
N
=
[G(xi)]1·C1C⊤
1
...
[G(xi)]N·CNC⊤
N

N·c×N·c.
We comment that P(m)
isimilarly is also a positive diagonal matrix; intuitively, it represents the cluster sizes in each expert,
additionally weighted by the gating score in MoE routers. Moreover, due to the special property of a diagonal matrix, we
further have
P(m)
i=C(m)(C(m))⊤·eRi,whereeRi:= diag( G(xi))⊗Ic.
Denoting the compressed MoE model as ˜fm, we start with addressing 1:=D
∇fW2˜fm(X),sign
∇fW2˜fm(Z)E
as:
1= Tr" X
i
∇eHm˜fm(X)
i˜σ⊤
x,iP(m)
i!
·sign X
iP(m)
i˜σz,i
∇eHm˜fm(Z)⊤
i!#
= Tr" X
i
∇eHm˜fm(X)
iσ
x⊤
ifW1
P(m)
i!
·sign X
iP(m)
iσ
fW⊤
1zi
∇eHm˜fm(Z)⊤
i!#
(i)= Tr" X
i
∇eHm˜fm(X)
iσ
x⊤
ifW1
C(m)Ri(C(m))⊤!
sign 
C(m)(C(m))⊤X
ieRiσ
fW⊤
1zi
∇eHm˜fm(Z)⊤
i!#
.
2For simplicity we assume the activation function σ(·)is differentiable everywhere.

--- PAGE 18 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18
The last equation (i)above holds since P(m)
i=C(m)(C(m))⊤·eRi. Considering the positive diagonal matrix C(m)(C(m))⊤
will not impact the sign of the matrix elements, we have
1= Tr" X
i
∇eHm˜fm(X)
iσ
x⊤
ifW1
C(m)Ri(C(m))⊤!
·sign X
ieRiσ
fW⊤
1zi
∇eHm˜fm(Z)⊤
i!#
= Tr" X
i
∇eHm˜fm(X)
iσ
x⊤
ifW1C(m)
Ri!
·sign X
i(C(m))⊤eRiσ
fW⊤
1zi
∇eHm˜fm(Z)⊤
i!#
(ii)= Tr" X
i
∇eHm˜fm(X)
iσ
x⊤
ifW1C(m)
Ri!
·sign X
iRiσ
(C(m))⊤fW⊤
1zi
∇eHm˜fm(Z)⊤
i!#
≈Tr" X
i(∇Hmfm(X))iσ 
x⊤
iW1
Ri!
·sign X
iRiσ 
W⊤
1zi
(∇Hmfm(Z))⊤
i!#
=
∇W2fm(X),sign 
∇W2fm(Z)
,
where equation (ii)holds since
(C(m))⊤eRi=Ri(C(m))⊤,
and the “copy” matrix C(m), as discussed in Section IV-B , is free to be brought inside both the sign and the activation function.
For the rest termD
∇fW1˜fm(X),sign
∇fW1˜fm(Z)E
, we need to show
 X
ixi 
RiW2(∇Hmfm(X))i
⊙σ′
x,i⊤!
·sign X
i 
RiW2(∇Hmfm(Z))i
⊙σ′
z,i
z⊤
i!
can be approximated by
2:= X
ixih
P(m)
ifW2(∇eHm˜fm(X))i
⊙˜σ′
x,ii⊤!
·sign X
ih
P(m)
ifW2(∇eHm˜fm(Z))i
⊙˜σ′
z,ii
z⊤
i!
,
where ˜σ′
x,i:=σ′
fW⊤
1xi
,˜σz,i:=σ′
fW⊤
1zi
, and σ′(·)is the derivative of the activation function σ(·):
2(i)= X
ixih
fW2(∇eHm˜fm(X))i
⊙˜σ′
x,ii⊤
P(m)
i!
·sign X
iP(m)
ih
fW2(∇eHm˜fm(Z))i
⊙˜σ′
z,ii
z⊤
i!
= X
ixih
fW2(∇eHm˜fm(X))i
⊙˜σ′
x,ii⊤
C(m)Ri(C(m))⊤!
·
sign 
C(m)(C(m))⊤X
ieRih
fW2(∇eHm˜fm(Z))i
⊙˜σ′
z,ii
z⊤
i!
= X
ixih
fW2(∇eHm˜fm(X))i
⊙˜σ′
x,ii⊤
C(m)Ri(C(m))⊤!
·sign X
ieRih
fW2(∇eHm˜fm(Z))i
⊙˜σ′
z,ii
z⊤
i!
= X
ixih
(C(m))⊤fW2(∇eHm˜fm(X))i
⊙
(C(m))⊤˜σ′
x,ii⊤
Ri!
·
sign X
i(C(m))⊤eRih
fW2(∇eHm˜fm(Z))i
⊙˜σ′
z,ii
z⊤
i!
,
in which we obtain equation (i)because P(m)as a diagonal matrix has the same scaling effect on the Hadamard producth
fW2(∇eHm˜fm(X))i
⊙˜σ′
x,ii
as on one of its component fW2(∇eHm˜fm(X))i. Next, we again utilize the relation
(C(m))⊤eRi=Ri(C(m))⊤

--- PAGE 19 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19
and have
2= X
ixih
(C(m))⊤fW2(∇eHm˜fm(X))i
⊙
(C(m))⊤σ′
fW⊤
1xii⊤
Ri!
·
sign X
iRih
(C(m))⊤fW2(∇eHm˜fm(Z))i
⊙
(C(m))⊤σ′
fW⊤
1zii
z⊤
i!
= X
ixih
Ri(C(m))⊤fW2(∇eHm˜fm(X))i
⊙σ′
(C(m))⊤fW⊤
1xii⊤!
·
sign X
ih
Ri(C(m))⊤fW2(∇eHm˜fm(Z))i
⊙σ′
(C(m))⊤fW⊤
1zii
z⊤
i!
≈ X
ixi 
RiW2(∇Hmfm(X))i
⊙σ′
x,i⊤!
·sign X
i 
RiW2(∇Hmfm(Z))i
⊙σ′
z,i
z⊤
i!
,
and the last equation holds again due to the previous special property of RiandC(m).
E. Model requirements for SGD NTK
To preserve the regular SGD NTK, the scale of the weight parameters needs to be adjusted. We re-define the efficient MLP
model as ( fc, σx, σz,˜σx,˜σzwill also be accordingly re-defined):
eHC:=σ
XW(c)
1+1
b(c)
1⊤
W(c)
2, (15)
where W(c)
1:=fW1P1
2,b(c)
1:=P1
2˜b1andW(c)
2:=P1
2fW1incorporate the diagonal scaling matrix Pin Equation (8).
We also require the activation function to have the following property:
σ(AP) =σ(A)P,
for arbitrary non-negative diagonal matrix P, which implies σ(0) = 0 ,σ(·)is piece-wise linear on R+,R−, and σ′(x)is
piece-wise constant ( σ(1)onR+andσ(−1)onR−); as an instance, the commonly used Rectified Linear Units (ReLU)
function [82] σr(x) = max {0, x}can satisfy this requirement.
Equation (15) can keep maintaining the approximation for Hand still ⟨∇b2f(X),∇b2f(Z)⟩ ≈D
∇b(c)
2fc(X),∇b(c)
2fc(Z)E
,
as we only modify the scale of the weight matrices. We then follow the derivation in the previous subsection and similar results
are obtained
For the remaining three term, again we first address 1:=D
∇W(c)
2fc(X),∇W(c)
2fc(Z)E
:
1= Tr
∇eHCfc(X)⊤
˜σx·˜σ⊤
z∇eHCfc(Z)
= Tr
∇eHCfc(X)⊤
σ
XW(c)
1+1
b(c)
1⊤
·σ
W(c)
1⊤
Z⊤+b(c)
11⊤
∇eHCfc(Z)
(i)= Tr
∇eHCfc(X)⊤
σ
XfW1+1˜b⊤
1
P1
2P1
2σ
fW⊤
1Z⊤+˜b11⊤
∇eHCfc(Z)
= Tr
∇eHCfc(X)⊤
σ
XfW1+1˜b⊤
1
CC⊤σ
fW⊤
1Z⊤+˜b11⊤
∇eHCfc(Z)
≈⟨∇ W2f(X),∇W2f(Z)⟩,
where equation (i)holds since P1
2, as we require, is free to be brought outside the activation function; the rest derivation
simply follows the counterpart in Appendix B-C.

--- PAGE 20 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20
For⟨∇W1f(X),∇W1f(Z)⟩, we similarly need to verify the product X⊤ 
∇Hf(X)W⊤
2
⊙σ′
· 
∇Hf(Z)W⊤
2
⊙σ′⊤X
can be approximated by 2:=X⊤
∇eHCfc(X)
W(c)
2⊤
⊙˜σ′
x
·
∇eHCfc(Z)
W(c)
2⊤
⊙˜σ′
z⊤
X.
˜σ′
x:=σ′
XfW1+1˜b⊤
1
,˜σz:=σ′
ZfW1+1˜b⊤
1
We then show the derivation as follows:
2=X⊤
∇eHCfc(X)
W(c)
2⊤
⊙˜σ′
x
·
∇eHCfc(Z)
W(c)
2⊤
⊙˜σ′
z⊤
X
(i)=X⊤h
∇eHCfc(X)fW⊤
2
⊙˜σ′
xi
P1
2P1
2·h
∇eHCfc(Z)fW⊤
2
⊙˜σ′
zi⊤
X
=X⊤h
∇eHCfc(X)fW⊤
2
⊙˜σ′
xi
CC⊤·h
∇eHCfc(Z)fW⊤
2
⊙˜σ′
zi⊤
X
=X⊤
∇eHCfc(X)fW⊤
2
⊙σ′
XW(c)
1+1
b(c)
1⊤
CC⊤
·
∇eHCfc(X)fW⊤
2
⊙σ′
XW(c)
1+1
b(c)
1⊤⊤
X
(ii)=X⊤h
∇eHCfc(X)fW⊤
2
⊙σ′
XfW1+1˜b⊤
1i
CC⊤
·h
∇eHCfc(X)fW⊤
2
⊙σ′
XfW1+1˜b⊤
1i⊤
X,
in which we obtain equation (i)because P1
2as a diagonal matrix has the same scaling effect on the Hadamard product
∇eHCfc(X)
W(c)
2⊤
⊙˜σ′
x
as on one of its component ∇eHCfc(X)
W(c)
2⊤
; equation (ii)holds because σ′is piece-
wise constant and the scaling matrix P1
2will not change the signs of the elements within. Then, following the same derivation
as in the previous derivations, we have 2≈X⊤ 
∇Hf(X)W⊤
2
⊙σ′
· 
∇Hf(Z)W⊤
2
⊙σ′⊤X
For the last term ⟨∇b1f(X),∇b1f(Z)⟩, we can again replace the above input matrix Xwith1⊤, and all the derivation
steps will follow.
APPENDIX C
BOUNDING THE ERROR OF MLP OUTPUT
Recall the object of clustering is:
min
C∥W−C⊤fW∥2
F
The assumption can thus be rewritten in a mathematical manner, which is ∥W−C⊤fW∥F≤εwith small ε. Assuming
f(W,C⊤fW) =∥W−C⊤fW∥F≤ε, we can provide a standard analysis of MLP output (ignoring b1for simplicity) as
follows.
Denoting ∆:=σ(XfW1C)−σ(XW 1)and following the technical assumptions in [4] that ∥W1∥2≤C1,∥W2∥2≤
C2,∥X∥F≤CXand the activation function σ(·)isL-Lipschitz continuous. Further assuming σ(0) = 0 (the assumptions hold
for commonly used activation functions in PLMs, e.g., ReLU and GELU), we first have
∥∆∥F≤L∥XfW1C−XW 1∥F≤L∥X∥F∥fW1C−W1∥F≤LCXε.

--- PAGE 21 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21
We can then bound ∥H−eHC∥Fas
∥σ(XfW1C)C⊤fW2−σ(XW 1)W2∥F≤∥σ(XfW1C)(C⊤fW2−W2)∥F+∥(σ(XfW1C)−σ(XW 1))W2∥F
≤∥σ(XfW1C)∥F∥C⊤fW2−W2∥F+∥σ(XfW1C)−σ(XW 1)∥F∥W2∥2
=∥∆ +σ(XW 1)∥F∥C⊤fW2−W2∥F+∥∆∥F∥W2∥2
≤(∥∆∥F+∥σ(XW 1)∥F)·ε+∥∆∥F·C2
≤L(C2CX+C1CX)·ε+LCX·ε2,
where we utilize
∥fW1C−W1∥F,∥C⊤fW2−W2∥ ≤ ∥W−C⊤fW∥F≤ε
∥σ(XW 1)∥F=∥σ(XW 1)−σ(0)∥F≤L∥XW 1∥F≤L∥X∥F∥W1∥2=LC1CX,
for the derivation. From the bound, we can observe with the well-learned C⊤fWfrom clustering (small ε), the output error
(∥H−eHC∥F) will also be small.
Moreover, a similar error analysis can also be applied to Adam NTK. The error analysis ∥K −eKC∥Fof NTK kernel Kcan
be simplified as analyzing Tr[A⊤sign(B)−eA⊤sign(eB)]where A,Brepresent arbitrary matrices. The precise derivation of
the approximation error bound on Adam NTK, considering the additional assumption on ∥sign(B)−sign(eB)∥Fas described
in [83], is left as future work.
APPENDIX D
DISCUSSIONS
We are not aware of any potential negative societal impacts regarding our work to the best of our knowledge. For all the
used data sets, there is no private personally identifiable information or offensive content.
Regarding future work, beyond the combination with distillation, we also plan to explore practical compression methods in
various domains, including speech processing [84], recommender system [85], [86], and graph mining [87], [88]. The derivation
of a more precise error analysis with regard to the pre-trained model is also a challenging and promising direction.
APPENDIX E
PERFORMANCE COMPARISON OF REPRESENTATIVE METHODS AFTER TASK-SPECIFIC FINE-TUNING
TABLE XII
ACCURACY OF REPRESENTATIVE METHODS AFTER TASK-SPECIFIC FINE-TUNING ON SST2 V ALIDATION SET
Method+Task-specific Fine-tuning Accuracy
Sketch 91.86
Clustering 93.35
MMD 92.43
SVD 93.01
LTH 93.42
MLP Fusion(Ours) 93.79
From Table XII and Table II in the paper, we can observe that not all methods can benefit from the layer-wise task-specific
tuning module. For example, the accuracy of the Sketch method drops from 91.90 to 91.86. Meanwhile, our proposed MLP

--- PAGE 22 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 22
Fusion provides a promising starting point for subsequent optimization through NTK approximation. By incorporating a
layer-wise task-specific tuning module, we can further enhance its performance and still achieve the best results compared to
all other baseline methods.
APPENDIX F
EXPERIMENT RESULTS ON MORE BENCHMARK DATASETS
TABLE XIII
ACCURACY OF EACH BASELINE METHOD ON STS-B AND QNLI V ALIDATION SETS WITH ROBERT A AS THE PLM
Method STS-B(7k) QNLI(105k)
RoBERTa 91.20 92.80
DistilRoBERTa 88.30 90.80
Sketch 86.99 89.84
Clustering 88.12 90.63
LTH 87.37 90.87
MLP Fusion(Ours) 89.37 91.03
Table XIII shows the experimental results on two additional data sets QNLI and STS-B within the GLUE benchmark. We
can see the proposed method is still able to achieve the best performance over strong baselines.
APPENDIX G
PERFORMANCE COMPARISON BETWEEN PROPOSED METHOD AND MAINTAINING OUTPUT /NTK OFTHEMLP M ODEL
TABLE XIV
PERFORMANCE OF PROPOSED METHOD AND MAINTAINING OUTPUT /NTK OFTHEMLP M ODEL
Methods Accuracy
Maintain NTK Gradient 91.74
Maintain Output 92.35
MLP Fusion (Ours) 93.23
In Table XIV, we compare two additional baselines that try to maintain the MLP output and NTK of the pre-trained language
model. Our NTK approximation method MLP Fusion still achieves the best performance. The loss that attempts to maintain the
output with unsupervised data ranked second. The method that tries to maintain the NTK of the MLP model with gradient has
the lowest accuracy. This is mainly because the gradient difference in the loss is difficult to minimize since it requires operating
second-order derivatives, which can also be time-consuming. Additionally, gathering labeled data for the loss calculation can
also be burdensome.
