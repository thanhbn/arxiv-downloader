# 2305.14788.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.14788.pdf
# Kích thước tệp: 1233914 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thích ứng Mô hình Ngôn ngữ để Nén Ngữ cảnh
Alexis Chevalier∗Alexander Wettig∗Anirudh Ajith Danqi Chen
Khoa Khoa học Máy tính & Princeton Language and Intelligence
Đại học Princeton
{achevalier, anirudh.ajith}@princeton.edu
{awettig, danqic}@cs.princeton.edu
Tóm tắt
Các mô hình ngôn ngữ (LM) dựa trên Transformer là những công cụ mạnh mẽ và có thể áp dụng rộng rãi, nhưng tính hữu ích của chúng bị hạn chế bởi cửa sổ ngữ cảnh hữu hạn và chi phí tính toán đắt đỏ khi xử lý các tài liệu văn bản dài. Chúng tôi đề xuất thích ứng các LM đã được đào tạo trước thành AutoCompressors. Những mô hình ngôn ngữ này có khả năng nén các ngữ cảnh dài thành các vector tóm tắt compact, sau đó có thể truy cập bởi mô hình như các soft prompt. Các vector tóm tắt được đào tạo với một mục tiêu không giám sát, trong đó các tài liệu dài được xử lý theo từng đoạn, và các vector tóm tắt từ tất cả các đoạn trước đó được sử dụng trong mô hình hóa ngôn ngữ. Chúng tôi tinh chỉnh các mô hình OPT và Llama-2 trên các chuỗi lên đến 30.720 token và cho thấy rằng AutoCompressors có thể sử dụng các ngữ cảnh dài để cải thiện perplexity. Chúng tôi đánh giá AutoCompressors trên học tập trong ngữ cảnh bằng cách nén các ví dụ minh họa tác vụ và thấy rằng các vector tóm tắt là những thay thế tốt cho các ví dụ minh họa văn bản thuần túy, tăng độ chính xác trong khi giảm chi phí suy luận. Cuối cùng, chúng tôi khám phá lợi ích của việc tính toán trước các vector tóm tắt cho các kho dữ liệu lớn bằng cách áp dụng các vector tóm tắt vào mô hình ngôn ngữ tăng cường truy xuất và một tác vụ xếp hạng lại đoạn văn. Nhìn chung, AutoCompressors nổi lên như một giải pháp đơn giản và tiết kiệm để mở rộng cửa sổ ngữ cảnh của LM trong khi tăng tốc suy luận trên các ngữ cảnh dài.1

1 Giới thiệu
Các mô hình ngôn ngữ (LM) dựa trên Transformer (Vaswani et al., 2017) gần đây đã chứng kiến sự gia tăng mạnh mẽ về độ phổ biến và hiện đang nhận hàng triệu truy vấn, xử lý hàng tỷ token, và tạo ra văn bản cho nhiều ứng dụng đa dạng (Brown et al., 2020; Touvron et al., 2023; Zhang et al.,
*AC và AW đóng góp như nhau. Công việc này được thực hiện khi AC ở Viện Nghiên cứu Cao cấp và thăm nhóm Princeton NLP.
1Mã nguồn và mô hình của chúng tôi được công khai tại
https://github.com/princeton-nlp/AutoCompressors.
sử dụng cho mô hình hóa ngôn ngữ các vector tóm tắt
đầu vào được phân đoạn ngẫu nhiên
token tóm tắt
vector tóm tắt
LM LM LM Hình 1: AutoCompressors xử lý các tài liệu dài bằng cách đệ quy tạo ra các vector tóm tắt được truyền như soft prompt cho tất cả các đoạn tiếp theo.

2022). Với sự gia tăng độ phổ biến này đến thách thức cho các nhà nghiên cứu làm cho LM hiệu quả hơn, để tăng tốc suy luận và triển khai LM ở quy mô lớn, đồng thời tăng tính linh hoạt của chúng, do đó cho phép người dùng xử lý nhiều dữ liệu hơn theo những cách mới.

Với những mục tiêu này trong tâm trí, chúng tôi đề xuất dạy các LM đã được đào tạo trước khả năng nén văn bản thành các vector tóm tắt. Các vector tóm tắt là những soft prompt ngắn (Lester et al., 2021), ngắn hơn một hoặc hai bậc độ lớn so với văn bản thuần túy chưa được nén, được thu từ các trạng thái đầu ra của một mô hình ngôn ngữ. Các vector tóm tắt phục vụ hai mục đích chung: chúng có thể giúp mở rộng cửa sổ ngữ cảnh của mô hình ngôn ngữ đến các tài liệu rất dài với chi phí tính toán tối thiểu, và chúng giúp tăng tốc suy luận trên văn bản mà các vector tóm tắt đã được tính toán trước và cached.

Các mô hình của chúng tôi, mà chúng tôi gọi là AutoCompressors, được đào tạo với một mục tiêu học không giám sát đơn giản khuyến khích mô hình lưu trữ thông tin thiết yếu trong các vector tóm tắt. Các vector tóm tắt được tạo ra từng đoạn một từ các tài liệu dài và được sử dụng để cải thiện mô hình hóa ngôn ngữ trong các đoạn tương lai (Hình 1). Công việc của chúng tôi

--- TRANG 2 ---
xây dựng trên kiến trúc RMT được đề xuất gần đây (Bulatov et al., 2022) với một điểm khác biệt quan trọng: chúng tôi giới thiệu tích lũy tóm tắt, trong đó các vector tóm tắt từ tất cả các đoạn được nối lại để tạo ra bản tóm tắt của toàn bộ tài liệu. Chúng tôi cũng đào tạo AutoCompressors với các đầu vào được phân đoạn ngẫu nhiên để chúng có thể nén tốt hơn các ngữ cảnh có độ dài khác nhau trong các tác vụ downstream. Chúng tôi cho thấy rằng những cải tiến này cải thiện việc giữ lại thông tin tầm xa và cho phép những cách suy luận mới trên nhiều đoạn văn.

AutoCompressors có thể được khởi tạo với các LM đã được đào tạo trước để tạo ra các mô hình mạnh mẽ và linh hoạt. Chúng tôi tinh chỉnh AutoCompressors từ các mô hình OPT-2.7B (Zhang et al., 2022) và Llama-2-7B (Touvron et al., 2023) trên các chuỗi từ 6.144 đến 30.720 token với một GPU NVIDIA A100 duy nhất có bộ nhớ 80GB. Chúng tôi cho thấy rằng các vector tóm tắt có hiệu quả trong việc cải thiện perplexity trên các tài liệu dài và rằng những khả năng nén này mạnh mẽ đối với tổng quát hóa miền. Phân tích của chúng tôi cho thấy rằng AutoCompressors có thể suy luận trên các vector tóm tắt, làm cho chúng hữu ích cho một tập hợp đa dạng các ứng dụng downstream.

Chúng tôi áp dụng AutoCompressors vào học tập trong ngữ cảnh (ICL) bằng cách nén lên đến 90 ví dụ minh họa trong ngữ cảnh. Chúng tôi xem xét 11 tác vụ phân loại, bao gồm 7 tác vụ SuperGLUE (Wang et al., 2019), và chúng tôi thấy rằng các vector tóm tắt vượt trội hơn ICL few-shot với số lượng token trong ngữ cảnh tương đương trên 8 trong số 11 tác vụ.

Cuối cùng, chúng tôi khám phá hai ứng dụng mà AutoCompressors có thể giảm chi phí suy luận bằng cách tính toán trước các vector tóm tắt cho các kho dữ liệu lớn. Đầu tiên, chúng tôi áp dụng một setting cho mô hình ngôn ngữ tăng cường truy xuất (Shi et al., 2023). Chúng tôi thấy rằng với độ dài chuỗi bằng nhau, việc sử dụng các vector tóm tắt đạt được lợi ích perplexity gấp 1.5 lần so với các đoạn văn bản thuần túy, và vượt trội hơn các phương pháp tăng cường truy xuất với ngân sách tính toán tương tự. Thứ hai, chúng tôi xem xét một tác vụ xếp hạng lại đoạn văn zero-shot (Sachan et al., 2022). Chúng tôi thiết lập rằng việc xếp hạng lại các đoạn văn dựa trên các vector tóm tắt của chúng đạt được sự cân bằng tốt nhất giữa hiệu suất xếp hạng lại và thông lượng suy luận.

Tóm lại, những đóng góp chính của chúng tôi như sau: (1) Chúng tôi giới thiệu một phương pháp để mở rộng LM đến các cửa sổ ngữ cảnh dài dưới yêu cầu tính toán quy mô nhỏ bằng cách học tạo ra các vector tóm tắt. Chúng tôi đề xuất tích lũy tóm tắt và đào tạo với phân đoạn ngẫu nhiên như những tính năng chính của AutoCompressors. (2) Chúng tôi cho thấy rằng các vector tóm tắt mã hóa thông tin hữu ích cho các tác vụ downstream và có thể được sử dụng để giảm chi phí suy luận của học tập trong ngữ cảnh. (3) Chúng tôi chứng minh lợi ích của việc tính toán trước các vector tóm tắt cho các kho dữ liệu lớn và sử dụng AutoCompressors kết hợp với các retriever.

2 Công việc Liên quan
Soft prompts Soft prompt tuning là một phương pháp hiệu quả để thích ứng các Transformer đã được đào tạo trước mà không cập nhật các tham số hiện có (Lester et al., 2021; Zhong et al., 2021; Liu et al., 2022). Các embedding được khởi tạo mới được thêm vào đầu chuỗi đầu vào ("soft prompt"), và tối ưu hóa được thực hiện đối với các tham số mới này trong khi phần còn lại của mô hình được đóng băng. Đây là một trong nhiều phương pháp tinh chỉnh hiệu quả tham số (Lialin et al., 2023) và liên quan đến prefix tuning, nơi các tham số được khởi tạo mới được thêm vào các trạng thái attention thay thế (Li and Liang, 2021).

Nén prompt Wingate et al. (2022) đề xuất học một soft prompt σ để nén thông tin chứa trong một ngữ cảnh x. Với một mô hình ngôn ngữ đã được đào tạo trước pLM, họ rút ra các phần tiếp theo y∼pLM(· |x) dựa trên x và sử dụng một mục tiêu chưng cất để căn chỉnh các dự đoán của mô hình được điều kiện trên soft prompt pLM(y|σ) với các dự đoán được điều kiện trên ngữ cảnh pLM(y|x). Wingate et al. (2022) thấy rằng soft prompts giữ lại thông tin cấp cao và tạo điều kiện cho việc tạo ra có thể kiểm soát. Tuy nhiên, cách tiếp cận này yêu cầu chạy tối ưu hóa cho mỗi ngữ cảnh mới x, không có chuyển giao kiến thức giữa các ngữ cảnh tương tự. Ngược lại, AutoCompressors của chúng tôi học để dự đoán các soft prompt σ của riêng chúng như một hàm của x.

Chưng cất ngữ cảnh Một dòng công việc liên quan (Askell et al., 2021; Snell et al., 2022) nhằm chưng cất thông tin trong ngữ cảnh, ví dụ, hướng dẫn, vào một mô hình học sinh không được nhắc. Trong công việc đồng thời, Mu et al. (2023) dạy các mô hình nén hướng dẫn thành các prefix attention key-value ngắn. Cách tiếp cận của chúng tôi khác biệt bằng cách học nén bất kỳ thông tin ngữ cảnh nào, bao gồm các tài liệu dài, và tạo ra các soft prompt compact hơn.

Transformer tầm xa Một số cải tiến kiến trúc đã được đề xuất để mở rộng Transformer đến độ dài ngữ cảnh dài hơn trong khi giảm chi phí bộ nhớ cao của full attention. Những cải tiến này bao gồm hạn chế và làm thưa cửa sổ attention (Dai et al., 2019; Child et al., 2019), xấp xỉ

--- TRANG 3 ---
attention (Rae et al., 2020; Zheng et al., 2022; Choromanski et al., 2021), cũng như giới thiệu các yếu tố recurrent (Ma et al., 2022; Bulatov et al., 2022), tính toán có điều kiện (Ainslie et al., 2023), và truy xuất các token trước đó từ ngữ cảnh ở tầng đầu ra (Zhong et al., 2022). Xem Tay et al. (2022) để có một khảo sát toàn diện về các kiến trúc tầm xa hiệu quả.

Hầu hết các kiến trúc này thường yêu cầu đào tạo đắt đỏ từ đầu, hoặc sẽ lệch hướng đáng kể so với khởi tạo đã được đào tạo trước.² Hơn nữa, nhiều mô hình ngôn ngữ thiếu bias quy nạp để ngoại suy đến các chuỗi dài hơn (Press et al., 2022). Mặc dù AutoCompressors về nguyên tắc có thể được đào tạo từ đầu, chúng tôi cho thấy rằng chúng cung cấp một giải pháp đơn giản để mở rộng cửa sổ ngữ cảnh của các mô hình đã được đào tạo trước đến các chuỗi dài hơn.

3 Phương pháp
Chúng tôi mô tả cách chúng tôi thích ứng một mô hình ngôn ngữ đã được đào tạo trước để nén văn bản thành các vector tóm tắt. Một tổng quan về kiến trúc của chúng tôi được thể hiện trong Hình 1.

Vector tóm tắt AutoCompressor xây dựng trên kiến trúc RMT (Bulatov et al., 2022). Chúng tôi mở rộng từ vựng đầu vào của mô hình cơ sở bằng κ token tóm tắt đặc biệt <Sum> i và khởi tạo κ embedding đầu vào mới.³ Khi chúng tôi thêm chuỗi <Sum> 1. . .<Sum> κ vào một đầu vào, nó báo hiệu cho mô hình xuất ra các vector tóm tắt đặc biệt của ngữ cảnh trước đó. Những vector này sau đó có thể được truyền đến đoạn văn bản tiếp theo như một soft prompt có độ dài κ. Vì không gian embedding của các mô hình ngôn ngữ đã được đào tạo trước có thể trải rộng hàng nghìn chiều, chúng tôi kỳ vọng rằng cơ chế này có khả năng cao để truyền thông tin đến các đoạn tiếp theo. Hơn nữa, một soft prompt có thể nội suy giữa nhiều token embedding, và do đó đại diện cho các khái niệm trừu tượng hơn so với một token rời rạc duy nhất (Wingate et al., 2022).

Tích lũy tóm tắt Chúng tôi chia các tài liệu dài thành các đoạn S₁, . . . , Sₙ và xử lý chúng tuần tự. Bulatov et al. (2022) kết hợp thông tin từ các đoạn trước đó bằng cách thêm vào đầu bản tóm tắt nén σᵢ₋₁ được tạo ra từ Sᵢ₋₁ vào các đầu vào embedded của Sᵢ. Chúng tôi đề xuất tích lũy tóm tắt, cho phép một đường truyền thông tin trực tiếp giữa mỗi đoạn và tất cả các đoạn trước nó: chúng tôi nối các vector tóm tắt σ₁. . . , σᵢ₋₁ để tạo thành σ<ᵢ và thêm vào đầu σ<ᵢ cho Sᵢ. Lưu ý rằng độ dài của σ<ᵢ bây giờ là (i−1)κ, tăng tuyến tính với độ dài tài liệu.

Positional embedding Khi sử dụng kiến trúc Transformer cơ sở với absolute positional embedding, như kiến trúc OPT (Zhang et al., 2022), chúng tôi không thêm positional embedding vào các token tóm tắt <Sum> i, cũng như vào các vector tóm tắt. Điều này cho phép chúng tôi sử dụng tất cả position embedding đã được đào tạo trước như các token ngữ cảnh và làm cho việc mở rộng mô hình đến số lượng bước nén tùy ý trong quá trình đào tạo trở nên khả thi. Mô hình vẫn bảo toàn thứ tự của các token tóm tắt do các token embedding riêng biệt của chúng.

Nếu Transformer cơ sở sử dụng relative positional embedding, như RoPE (Su et al., 2022), chúng tôi áp dụng positional embedding cho các token và vector tóm tắt mà không có bất kỳ sửa đổi nào thêm.

3.1 Đào tạo Vector Tóm tắt
Chúng tôi sử dụng một cách tiếp cận đào tạo không giám sát đơn giản khuyến khích mô hình học nén ngữ cảnh qua nhiều bước.

Mục tiêu đào tạo Viết (x^i₁, . . . , x^i_{mᵢ}) cho đoạn Sᵢ với mọi i≤n, trong đó mᵢ là số lượng token trong Sᵢ. Điều kiện trên các vector tóm tắt nối σ<ᵢ, chúng tôi chiếu các đầu ra Transformer với đầu mô hình hóa ngôn ngữ để thu được xác suất token tiếp theo p(x^i_t|x^i₁, . . . , x^i_{t-1}, σ<ᵢ). Chúng tôi tối thiểu hóa loss cross-entropy trên toàn bộ tài liệu:

L=−1/N Σⁿᵢ₌₁ Σᵐⁱₜ₌₁ log p(x^i_t|x^i₁, . . . , x^i_{t-1}, σ<ᵢ).

trong đó N là tổng số token. Mục tiêu này giữ lại khả năng của mô hình ngôn ngữ đã được đào tạo trước trên đoạn đầu tiên S₁ và nó khuyến khích mô hình lưu trữ thông tin hữu ích trong các vector tóm tắt, mà các đoạn tương lai có thể tận dụng để đưa ra dự đoán token tốt hơn.

Không giống như Wingate et al. (2022), chúng tôi không đào tạo với mục tiêu chưng cất kiến thức, vì LM đã được đào tạo trước có cửa sổ ngữ cảnh hạn chế như một giáo viên, trong khi học sinh AutoCompressor học xử lý các tài liệu dài hơn nhiều.

Phân đoạn ngẫu nhiên Chúng tôi thay đổi ngẫu nhiên độ dài mᵢ của các đoạn Sᵢ trong quá trình đào tạo, tuân theo điều kiện rằng mỗi đoạn phù hợp với

--- TRANG 4 ---
cửa sổ ngữ cảnh của mô hình. Điều này cho phép AutoCompressors nén các tài liệu có độ dài khác nhau và cải thiện hiệu suất dưới đánh giá với các đoạn có độ dài cố định (xem Hình 2).

BPTT với stop-gradient Chúng tôi sử dụng backpropagation through time (BPTT) và gradient checkpointing (Chen et al., 2016) cho mỗi đoạn để giảm kích thước của đồ thị tính toán. Ngoài ra, chúng tôi tính toán và cache các vector tóm tắt và dừng gradient của chúng sau 2 bước nén, tương tự như caching các trạng thái attention trong quá khứ trong đào tạo Transformer-XL (Dai et al., 2019). Điều này giả định rằng để học nén thông tin hữu ích trong Sᵢ, việc dự đoán các token trong Sᵢ₊₁ liền kề là đủ. Trong Hình 2, chúng tôi xác nhận rằng điều này không gây ra penalty khi dự đoán các đoạn dài, trong khi giảm thêm yêu cầu bộ nhớ GPU.

4 Đánh giá Mô hình hóa Ngôn ngữ
Trong phần này, chúng tôi đào tạo AutoCompressors và đánh giá khả năng mô hình hóa ngôn ngữ tầm xa của chúng bằng cách lấy mẫu các chuỗi dài mà chúng tôi chia thành các đoạn 2.048 token. Chúng tôi cố định đoạn cuối cùng và nén n đoạn trước đó. Chúng tôi theo dõi perplexity của đoạn cuối cùng khi điều kiện trên các vector tóm tắt cho mỗi n.

Chúng tôi tiến hành các thí nghiệm chính và ablation với các mô hình OPT (Zhang et al., 2022) có 1.3B hoặc 2.7B tham số, được tinh chỉnh trên 2B token từ Pile (Gao et al., 2020). Trong Phần 4.1, chúng tôi đánh giá một AutoCompressor trên các chuỗi 8.000 token và so sánh với một mô hình RMT tương đương và một baseline Extended Full Attention. Trong Phần 4.2, chúng tôi tinh chỉnh một AutoCompressor trên các chuỗi 30.000 token để chứng minh tính khả thi trên các chuỗi rất dài. Cuối cùng, trong Phần 4.3, chúng tôi mở rộng AutoCompressors bằng cách tinh chỉnh một mô hình Llama-2-7B trên 15B token từ RedPajama (TogetherAI, 2023). Các siêu tham số mô hình đầy đủ và thông tin dữ liệu có thể được tìm thấy trong Phụ lục A.

4.1 Thí nghiệm trên Chuỗi 8K-Token
Setting Chúng tôi khởi tạo tất cả các mô hình với mô hình OPT 2.7B-parameter và tinh chỉnh trên 2B token từ 4 miền từ Pile (Gao et al., 2020). AutoCompressor của chúng tôi sử dụng κ= 50 token tóm tắt và được tinh chỉnh với tích lũy tóm tắt trên bốn đoạn, mỗi đoạn từ 1.024 đến 2.048 token. Nén 2.048 token thành 50 vector tóm tắt đạt được tỷ lệ nén 40 token cho mỗi vector tóm tắt. Chúng tôi sử dụng các baseline sau:

1. Chúng tôi tinh chỉnh một baseline OPT-2.7B trên dữ liệu của chúng tôi. Mô hình này bị giới hạn ở các chuỗi 2.048 token do đào tạo trước.

2. Extended full attention: Chúng tôi tinh chỉnh OPT-2.7B trên các chuỗi lên đến 4.096 token bằng cách mở rộng positional embedding của mô hình. Chúng tôi khởi tạo các embedding cho vị trí [2049 ..4096] với các embedding cho vị trí [1..2048]. Chúng tôi không thể mở rộng ngữ cảnh vượt quá 4.096 token do hạn chế bộ nhớ GPU.

3. RMT-2.7B: Chúng tôi tinh chỉnh một mô hình RMT trên dữ liệu của chúng tôi với κ= 50 vector tóm tắt.

4. AutoCompressor của chúng tôi được tinh chỉnh trên các tài liệu lên đến 6.144 token. Mỗi tài liệu được phân đoạn ngẫu nhiên thành bốn đoạn sao cho hai đoạn đầu tiên cộng lại bằng 3.072 token. Độ dài của mỗi đoạn từ 1.024 đến 2.048 token. Chúng tôi sử dụng κ= 50 vector tóm tắt và tích lũy tóm tắt. Chúng tôi dừng gradient mỗi hai bước nén.

Tất cả các mô hình được đánh giá trên các tài liệu được lấy mẫu từ Pile với độ dài cố định 8.192 token. Chúng tôi lấy mẫu 610 tài liệu từ mỗi miền sau: Books3, FreeLaw, GitHub, Wikipedia (trong miền), và ArXiv, Gutenberg, HackerNews, YoutubeSubtitles (ngoài miền).

Kết quả Chúng tôi trình bày kết quả trong Bảng 1. Chúng tôi thấy rằng AutoCompressor hưởng lợi từ các ngữ cảnh dài 6.144 token và luôn vượt trội hơn mô hình RMT.

Chúng tôi cũng thấy rằng AutoCompressor hưởng lợi từ các chuỗi ngắn hơn nhiều so với những gì thấy trong quá trình đào tạo, không giống như RMT. Xem cũng Hình 2 và Bảng 6 cho tính hữu ích của phân đoạn ngẫu nhiên.

Mặc dù extended full attention hoạt động tốt nhất trên các chuỗi dài 4.096, chúng tôi quan sát thấy một sự đánh đổi cho các ngữ cảnh ngắn hơn nơi AutoCompressors đạt được hiệu suất tốt nhất. Chúng tôi cũng nhấn mạnh rằng AutoCompressor chú ý đến tối đa 150 soft prompt bổ sung trong quá trình đánh giá, trong khi mô hình full attention được cung cấp thêm 2.048 token.

Những xu hướng này giữ nguyên cho cả đánh giá trong miền và ngoài miền. Tuy nhiên, khoảng cách giữa AutoCompressor và baseline full-attention tăng lên trong setting ngoài miền, cho thấy rằng các vector tóm tắt tổng quát hóa ít hơn một chút so với các attention head đã được đào tạo trước.

4.2 Thí nghiệm trên Chuỗi 30K-Token
Setting Chúng tôi tinh chỉnh OPT-1.3B và OPT-2.7B như AutoCompressors trên 2B token nhưng đào tạo trên các chuỗi 30.720 token với 20 bước nén.⁴ Chúng tôi sử dụng 50 token tóm tắt, phân đoạn ngẫu nhiên, và stop-gradient như trước. Chúng tôi cũng

⁴Do sự khan hiếm của các chuỗi rất dài trong Pile, chúng tôi chỉ đào tạo trên dữ liệu từ miền Books3, và sử dụng miền Gutenberg như đánh giá ngoài miền.

--- TRANG 5 ---
[Bảng 1: Held-out perplexity trên 2.048 token, trong khi thay đổi độ dài của ngữ cảnh trước đó (tất cả các thí nghiệm dựa trên các mô hình OPT-2.7B). Đối với RMT và AutoCompressor, chúng tôi điều kiện trên các vector tóm tắt. Chúng tôi cũng báo cáo các lợi ích perplexity so với baseline OPT đã tinh chỉnh không có ngữ cảnh bổ sung, đạt 6.28 trong miền và 8.53 ngoài miền (các lợi ích được hiển thị bằng số màu). †: Mặc dù extended full attention (Extended FA) đạt được perplexity tương tự hoặc hơi tốt hơn, nó sử dụng lên đến 2.048 token bổ sung và không thể mở rộng thêm. Tuy nhiên, AutoCompressor chỉ sử dụng 50×3 = 150 vector tóm tắt để xử lý 6.144 token ngữ cảnh.]

[Bảng 2: Kết quả đánh giá cho AutoCompressors được đào tạo trên các chuỗi 30.720 token và được đánh giá trên Books3 (trong miền) và Gutenberg (ngoài miền). Chúng tôi đào tạo với một GPU NVIDIA A100 duy nhất và báo cáo bộ nhớ CUDA cần thiết để tinh chỉnh sử dụng một chuỗi duy nhất mỗi batch. AutoCompressors yêu cầu ít bộ nhớ hơn vì chúng tôi dừng gradient sau hai đoạn.]

[Bảng 3: Kết quả đánh giá cho AutoCompressor của chúng tôi được đào tạo từ Llama-2 7B trên các chuỗi 6.144 token. Đối với AutoCompressor, chúng tôi điều kiện trên các vector tóm tắt. Đối với Llama-2 và Extended Full Attention (Extended FA), chúng tôi điều kiện trên các token văn bản thuần túy.]

tinh chỉnh một mô hình RMT từ OPT-1.3B, để sử dụng như một baseline. Chúng tôi không thể tinh chỉnh một baseline RMT 2.7B-parameter vì phương pháp RMT dẫn đến lỗi out-of-memory.

Tất cả các mô hình được đánh giá trên 2.048 token held-out cuối cùng của các tài liệu có kích thước 30.720 token bằng cách nén tất cả các đoạn 2.048-token trước đó.

Kết quả Chúng tôi thu thập kết quả trong Bảng 2. Việc đánh giá cho thấy rằng cả hai mô hình AutoCompressor đều học để sử dụng toàn bộ 28K token để giảm perplexity, trong khi baseline RMT không hưởng lợi từ việc nhân đôi số lượng token ngữ cảnh từ 14K lên 28K. Điều này cho thấy rằng tích lũy tóm tắt có hiệu quả trong việc nắm bắt các phụ thuộc tầm xa trong tài liệu. Chúng tôi cũng báo cáo yêu cầu bộ nhớ CUDA để tinh chỉnh mỗi mô hình trong Bảng 2. Chúng tôi đào tạo với một GPU NVIDIA A100 có bộ nhớ 80GB. Việc dừng gradient giảm bộ nhớ CUDA và làm cho việc tinh chỉnh AutoCompressor từ OPT-2.7B trở nên khả thi, trong khi tinh chỉnh với RMT dẫn đến out-of-memory ở quy mô đó.

4.3 Mở rộng AutoCompressors lên Llama-2
Setting Chúng tôi tinh chỉnh một mô hình Llama-2 7B-parameter như một AutoCompressor trên một GPU duy nhất bằng cách đóng băng mô hình và chỉ tối ưu hóa các embedding token tóm tắt và các trọng số attention thông qua LoRA (Hu et al., 2022). Mô hình được đào tạo trên 15B token từ RedPajama (TogetherAI, 2023), được chia thành các chuỗi 6.144 token, và chúng tôi sử dụng 50 token tóm tắt, phân đoạn ngẫu nhiên, và stop-gradient. Chúng tôi cũng tinh chỉnh một baseline Extended Full Attention trên cùng tập dữ liệu. Cửa sổ ngữ cảnh của mô hình đã được đào tạo trước được mở rộng bằng cách tăng giá trị θ trong RoPE theo (Rozière et al., 2023).

Chúng tôi so sánh cả hai mô hình với mô hình Llama-2-7B đã được đào tạo trước, có cửa sổ ngữ cảnh 4.096 token. Tất cả các mô hình được đánh giá trên 2.048 token cuối cùng của các tài liệu 8.192-token.

Kết quả Chúng tôi thu thập kết quả trong Bảng 3. AutoCompressor hưởng lợi từ toàn bộ ngữ cảnh để giảm perplexity: nén một ngữ cảnh 4.096-token thành 100 vector tóm tắt đạt được perplexity tương tự như baseline Extended Full Attention với 512 token văn bản thuần túy, và nén một ngữ cảnh 6.144-token thành 150 vector tóm tắt cải thiện perplexity thêm một chút. Hơn nữa, chúng tôi thấy rằng các vector tóm tắt bảo toàn perplexity khi các ngữ cảnh ngắn được nén.

Tuy nhiên, Llama-2 và baseline Extended Full At-

--- TRANG 6 ---
[Tiếp tục với đoạn cuối của trang 6 và các trang tiếp theo...]

tention vượt trội hơn Auto Compressor khi các ngữ cảnh dài hơn được cung cấp. Cần có nghiên cứu thêm để xây dựng các vector tóm tắt bảo toàn tất cả thông tin ngữ cảnh.

4.4 Phân tích
Ablation Chúng tôi đào tạo các mô hình OPT-2.7B không có phân đoạn ngẫu nhiên, tích lũy tóm tắt, hoặc stop gradient. Kết quả được thể hiện trong Hình 2. Chúng tôi thấy rằng phân đoạn ngẫu nhiên dẫn đến nén tốt hơn các đoạn ngắn, nhưng vẫn cải thiện perplexity khi nén nhiều đoạn 2048 token. Như mong đợi, tích lũy tóm tắt giúp cải thiện perplexity vượt quá một đoạn được nén. Chúng tôi cũng xác nhận rằng việc dừng gradient không ảnh hưởng đến hiệu suất mặc dù giảm yêu cầu bộ nhớ GPU. Trong Bảng 2, chúng tôi cũng cho thấy rằng việc dừng gradient giúp giảm bộ nhớ GPU.

Chúng tôi cũng đào tạo AutoCompressors với κ= 20, 50, 70 hoặc 100 token tóm tắt và báo cáo kết quả perplexity held-out trong Bảng 7 ở Phụ lục. Đáng ngạc nhiên, chúng tôi thấy rằng hiệu suất không tăng với các soft prompt dài hơn, và κ= 50 hoạt động tốt nhất tổng thể. Chúng tôi giả thuyết rằng việc học một số lượng lớn hơn các vector tóm tắt có thể yêu cầu ngân sách đào tạo lớn hơn.

Phân tích cấp token Chúng tôi tìm cách hiểu rõ hơn cách các vector tóm tắt có lợi cho các dự đoán token riêng lẻ. Trong Hình 5 ở Phụ lục, chúng tôi cho thấy các lợi ích perplexity tại mỗi vị trí token cho AutoCompressor với các vector tóm tắt và cho baseline extended full-attention.

Chúng tôi thấy rằng việc điều kiện trên các vector tóm tắt cải thiện perplexity trên tất cả 2048 vị trí token. Chúng tôi quan sát thấy rằng baseline extended full attention vượt trội hơn AutoCompressor ở đầu chuỗi, trong khi AutoCompressor đạt được hiệu suất tốt nhất về cuối chuỗi. Điều này cho thấy rằng các vector tóm tắt có hiệu quả trong việc nắm bắt các phụ thuộc văn bản tầm xa.

Trong Phụ lục D, chúng tôi cho thấy các ví dụ về câu và token hưởng lợi nhiều nhất từ các vector tóm tắt. Chúng tôi thấy rằng các vector tóm tắt chứa thông tin nổi bật, như tên hoặc ngày tháng, và rằng mô hình có thể suy luận trên các vector tóm tắt. Điều này xác nhận rằng các vector tóm tắt là những bản tóm tắt hữu ích của văn bản được nén.

5 Nén Ví dụ Minh họa cho Học tập Trong Ngữ cảnh
Trong phần này, chúng tôi nghiên cứu tính hữu ích của các vector tóm tắt để thực hiện các tác vụ downstream. Chúng tôi cho thấy rằng các ví dụ minh họa trong ngữ cảnh có thể được nén một cách đáng tin cậy thành các vector tóm tắt để cải thiện hiệu suất đồng thời tăng hiệu quả trên một tập hợp đa dạng các benchmark NLP.

Đánh giá Chúng tôi đánh giá khả năng học tập trong ngữ cảnh của AutoCompressor dựa trên Llama-2-7B từ Phần 4.3 trên mười một tập dữ liệu phân loại và trả lời câu hỏi trắc nghiệm. Đối với mỗi tập dữ liệu, chúng tôi đánh giá hiệu quả của việc nén 1, 2 hoặc 3 đoạn ví dụ minh họa thành 50, 100 hoặc 150 vector tóm tắt. Đối với mỗi đoạn, chúng tôi bao gồm càng nhiều ví dụ minh họa càng tốt cho đến khi chúng tôi đạt 750 token. Đối với SST-2, điều này tương ứng với 30 ví dụ minh họa mỗi đoạn trung bình. Chúng tôi so sánh cách tiếp cận nén này với kết quả thu được bằng cách prompt mô hình sử dụng 150 và 750 token ví dụ minh họa văn bản thuần túy.

Chúng tôi sử dụng contextual calibration (Zhao et al., 2021) và class-balanced sampling khi những kỹ thuật này cải thiện hiệu suất trên tập validation. Đối với mỗi tập dữ liệu, chúng tôi báo cáo độ chính xác trung bình và độ lệch chuẩn trên 7 seed ngẫu nhiên. Các setting chi tiết cho mỗi tập dữ liệu có thể được tìm thấy trong Bảng 11.

Trong Bảng 12 ở Phụ lục, chúng tôi cũng so sánh hiệu suất ICL của các mô hình AutoCompressor dựa trên OPT-2.7B với baseline RMT và OPT-2.7B đã được đào tạo trước, và bao gồm hiệu suất của mô hình Llama-2-7B đã được đào tạo trước.

Kết quả Chúng tôi cho thấy kết quả đánh giá trong Bảng 4. Kết quả cho thấy rằng các vector tóm tắt liên tục cải thiện hiệu suất so với baseline zero-shot. Hơn nữa, các vector tóm tắt tăng độ chính xác

--- TRANG 7 ---
[Bảng 4: Đánh giá hiệu suất ICL của mô hình Llama-2 7B. Mỗi bản tóm tắt dài 50 token và tương ứng với một đoạn có giá trị 750 token ví dụ minh họa. Chúng tôi cũng báo cáo độ chính xác khi prompt AutoCompressor với 150 và 750 token ví dụ minh họa văn bản thuần túy như baselines. Lưu ý rằng đối với BoolQ và MultiRC, các ví dụ minh họa quá dài để phù hợp với 150 token.]

so với 150 token ví dụ minh họa thuần túy trên 8/11 tác vụ. Trên 8 tác vụ (AG News, SST-2, BoolQ, WiC, WSC, CB, COPA và MultiRC), các vector tóm tắt cũng vượt trội hơn ICL với 750 token ví dụ minh họa văn bản thuần túy. Các vector tóm tắt nổi lên như một thay thế mạnh mẽ cho các ví dụ minh họa văn bản thuần túy, khi chúng tăng độ chính xác trong khi giảm chi phí suy luận.

Trong Bảng 12 (Phụ lục E), chúng tôi thấy rằng AutoCompressor OPT-2.7B đạt được độ chính xác cao hơn baseline RMT trên 8 trong số 11 tác vụ và rằng mô hình RMT không hưởng lợi từ nhiều bước nén. Điều này cho thấy rằng tích lũy tóm tắt là một cơ chế hiệu quả để nén các ví dụ minh họa trong ngữ cảnh. Chúng tôi cũng quan sát thấy rằng AutoCompressor Llama-2 đã tinh chỉnh của chúng tôi có độ chính xác zero-shot tệ hơn đáng kể trên một số tác vụ so với khởi tạo Llama-2, và hiệu suất ICL hơi tệ hơn. Chúng tôi nghi ngờ rằng điều này là do sự không khớp miền trong dữ liệu tinh chỉnh của chúng tôi và corpus đào tạo trước Llama-2.

6 Nén Kho dữ liệu Truy xuất cho Suy luận Hiệu quả
Chúng tôi nghiên cứu tính hữu ích của việc tính toán trước các vector tóm tắt cho các bộ sưu tập tài liệu lớn. Những vector này có thể được lưu trữ và sau đó được truy xuất để suy luận hiệu quả. Vì suy luận thường đắt hơn lưu trữ, cách tiếp cận này có tiềm năng đạt được những sự đánh đổi thực tế tốt.

6.1 Mô hình hóa Ngôn ngữ Tăng cường Truy xuất
Các mô hình ngôn ngữ tăng cường truy xuất cải thiện dự đoán token bằng cách truy xuất thông tin từ một kho dữ liệu. Một số cách tiếp cận đã được đề xuất để truyền kiến thức bên ngoài vào tầng đầu vào (Guu et al., 2020; Shi et al., 2023), các tầng trung gian (Borgeaud et al., 2022) hoặc ở tầng đầu ra (Khandelwal et al., 2020; Zhong et al., 2022).

[Hình 3: Mô hình hóa ngôn ngữ tăng cường truy xuất hiệu quả với AutoCompressors. Các kho dữ liệu lớn có thể được tiền xử lý thành các vector tóm tắt nén có thể được lưu trữ một cách rẻ. Khi truy xuất, các bản tóm tắt nén được kết hợp để truy cập hiệu quả nhiều tài liệu trong một lần forward pass duy nhất.]

REPLUG Nghiên cứu tình huống của chúng tôi tập trung vào REPLUG (Shi et al., 2023), là một phương pháp đơn giản để kết hợp một mô hình ngôn ngữ đã được đào tạo trước với một retriever có sẵn để cải thiện hiệu suất mô hình hóa ngôn ngữ. Với quyền truy cập vào một corpus bên ngoài C, REPLUG truy xuất k đoạn văn D={d₁, . . . , dₖ} dựa trên một đoạn x để tính điểm cho đoạn tiếp theo y. Xác suất tổng thể cho y được tính bằng cách ensemble các dự đoán dựa trên các đoạn văn khác nhau:

p(y|x,D) = Σ_{d∈D} λ(d, x)·p(y|CONCAT(d, x)),

trong đó λ(d, x) là các điểm số tương tự chuẩn hóa từ retriever và CONCAT(d, x) biểu thị sự nối của p và x. Phương pháp này gây ra một overhead đáng kể, vì nó yêu cầu k lần forward pass qua các chuỗi CONCAT(d, x, y).

Fused Summaries Chúng tôi giới thiệu một setting cho mô hình hóa ngôn ngữ tăng cường truy xuất gần với fusion-in-decoder (Izacard and Grave, 2021). Chúng tôi nối các vector tóm tắt của các đoạn văn được truy xuất D để tạo thành các vector tóm tắt kết hợp, σ_D = CONCAT(σ_{dₖ}, . . . , σ_{d₁}), trong đó dₖ, . . . , d₁ được sắp xếp từ ít liên quan nhất đến liên quan nhất. Điều này giống với

--- TRANG 8 ---
tích lũy tóm tắt như được mô tả trong Phần 3. Chúng tôi cũng thấy hữu ích việc làm mượt điểm số xác suất và sắp xếp lại các đoạn văn được truy xuất dựa trên các vector tóm tắt của chúng (Phụ lục F). Hình 3 cung cấp tổng quan về cách tiếp cận của chúng tôi.

Fused Passages Chúng tôi thiết lập một baseline để kết hợp các vector tóm tắt bằng cách nối các đoạn văn bản thuần túy và tính toán xác suất được làm mượt, xem Phụ lục F. Không giống như các vector tóm tắt, phương pháp này bị giới hạn bởi cửa sổ ngữ cảnh của mô hình.

Thí nghiệm Chúng tôi đánh giá AutoCompressor OPT-2.7B được giới thiệu trong Phần 4.1 mà không có bất kỳ tinh chỉnh bổ sung nào. Tương tự như Shi et al. (2023), chúng tôi truy xuất từ Pile. Chúng tôi sử dụng Books3, FreeLaw, GitHub, Wikipedia, Gutenberg, ArXiv, HackerNews, và YoutubeSubtitles. Chúng tôi index 10B token cho mỗi miền, được chia thành các đoạn văn 512 hoặc 50 token.

Chúng tôi lấy mẫu các đoạn 256 token từ dữ liệu validation Pile, sử dụng 128 token đầu tiên làm ngữ cảnh x cho truy xuất và 128 token cuối cùng y cho đánh giá. Chúng tôi sử dụng mô hình Contriever (Izacard et al., 2022) để truy xuất, và truy xuất top 10 đoạn văn. Chúng tôi cũng loại bỏ trùng lặp dữ liệu bằng cách loại bỏ các đoạn văn trùng lặp với x bởi 64 token.

Kết quả Kết quả được trình bày trong Bảng 5. Chúng tôi thấy rằng Fused Summaries vượt trội hơn Fused Passages và REPLUG khi các đoạn văn 50-token được truy xuất. Chúng tôi đo thông lượng một cách thực nghiệm và cho thấy rằng đối với 10 tài liệu được truy xuất, Fused Summary Vectors vẫn tiết kiệm. Chúng tôi lưu ý rằng việc nén các tập dữ liệu 10B token dẫn đến không gian đĩa 5TB mỗi miền khi được lưu trữ ở định dạng half-precision.⁵ Do đó Fused Summaries đạt được một sự đánh đổi tốt giữa chi phí lưu trữ và thông lượng.

⁵Để so sánh, việc lưu trữ đầu ra transformer tại mọi token duy nhất (ví dụ, trong setting encoder-decoder) sẽ chiếm 51 TB, và lưu trữ tất cả trạng thái attention sẽ là 3.276 TB.

Hơn nữa, Fused Summaries vượt trội hơn REPLUG top-2 với các đoạn văn 512-token và thấy sự tăng thông lượng 1.7x, điều này cho thấy rằng mô hình hưởng lợi từ sự đa dạng của các tài liệu được nén. Tuy nhiên, REPLUG top-10 vượt trội hơn Fused Summaries. Chúng tôi để lại cho công việc tương lai việc khám phá cách tạo ra các vector tóm tắt chất lượng cao hơn để sử dụng tốt hơn các đoạn văn được nén.

Chúng tôi lưu ý rằng việc kết hợp các vector tóm tắt có hiệu quả mặc dù có sự không khớp trong đào tạo vì chúng tôi rút ra các vector tóm tắt độc lập từ các tài liệu riêng biệt. Hơn nữa, mô hình AutoCompressor của chúng tôi chỉ được đào tạo để tích lũy 3 tập vector tóm tắt, nhưng nó hưởng lợi từ việc kết hợp các vector tóm tắt của lên đến 10 tài liệu.

[Bảng 5: Lợi ích PPL (%) từ các setting mô hình hóa ngôn ngữ tăng cường truy xuất khác nhau, so với baseline không truy xuất. Chúng tôi đánh giá AutoCompressor OPT-2.7B và báo cáo thông lượng trên một GPU NVIDIA A100 duy nhất cho mỗi phương pháp mà không batch các ví dụ. Fused Summaries vượt trội hơn Fused Passages và REPLUG với các đoạn văn 50-token. Hơn nữa, Fused Summaries top-10 vượt trội hơn REPLUG top-2 với các đoạn văn 512-token trong khi cũng đạt được sự tăng thông lượng 1.7×.]

6.2 Xếp hạng lại Đoạn văn Không giám sát
Cuối cùng, chúng tôi xem xét nghiên cứu tình huống về xếp hạng lại đoạn văn, trong đó một retriever có sẵn nhanh như BM25 truy xuất một tập hợp lớn các đoạn văn ứng viên, và một bộ xếp hạng lại có khả năng hơn tinh chỉnh thứ hạng để tăng thứ hạng của các đoạn văn liên quan nhất.

Phương pháp Sachan et al. (2022) giới thiệu một phương pháp hiệu quả để tận dụng các mô hình ngôn ngữ làm bộ xếp hạng lại mà không cần giám sát hoặc tinh chỉnh bổ sung. Với một truy vấn q và một tập hợp các đoạn văn ứng viên {p₁, . . . , pₖ}, mô hình ngôn ngữ tính điểm likelihood của truy vấn q được điều kiện trên prompt "Passage: {pᵢ}. Please write a question based on this passage." cho mỗi đoạn văn pᵢ và xếp hạng lại các đoạn văn dựa trên các điểm số.

Thí nghiệm Chúng tôi xem xét tác vụ xếp hạng lại các đoạn văn BM25 trên tập test NQ (Balachandran et al., 2021) và so sánh AutoCompressors có sẵn với 20 và 50 token tóm tắt với các mô hình OPT đã được đào tạo trước từ 125M đến 2.7B tham số. Chúng tôi tính toán trước các vector tóm tắt cho 21M đoạn văn từ một corpus Wikipedia (Karpukhin et al.,

--- TRANG 9 ---
2020), điều này yêu cầu không gian đĩa 2.1TB và 5.4TB ở half precision cho 20 và 50 vector tóm tắt tương ứng. Chúng tôi đo chất lượng của kết quả xếp hạng lại bằng cách sử dụng Recall@20.

Kết quả Kết quả được thể hiện trong Hình 4. Chúng tôi đo thông lượng cho các truy vấn riêng lẻ không được batch trên một GPU NVIDIA A100 80GB duy nhất và giả định rằng độ trễ của việc tải các vector tóm tắt là không đáng kể. Mặc dù các đoạn văn chỉ dài 100 từ, dẫn đến tỷ lệ nén thấp, các vector tóm tắt tăng tốc đáng kể suy luận, trong khi hy sinh hiệu suất ít hơn so với các mô hình nhỏ hơn. Điều này dẫn đến một sự đánh đổi Pareto-optimal giữa tính toán và hiệu suất và chứng minh rằng các vector tóm tắt thường giữ lại đủ thông tin từ một đoạn văn để đánh giá mức độ liên quan của nó đối với một truy vấn cụ thể.

[Hình 4: Chúng tôi so sánh AutoCompressors (hình vuông) trong một setting xếp hạng lại đoạn văn không giám sát với các mô hình ngôn ngữ đã được đào tạo trước (hình tròn). Số trên mỗi điểm dữ liệu cho thấy có bao nhiêu đoạn văn được truy xuất bởi BM25 được xếp hạng lại, và trục dọc cho thấy hiệu suất Recall@20 của hệ thống xếp hạng lại trên tập test NQ. Chúng tôi xem xét thông lượng trên một GPU NVIDIA A100 duy nhất và giả định rằng nhiều truy vấn không thể được batch. Bằng cách tận dụng các vector tóm tắt được tính toán trước cho các đoạn văn, AutoCompressors dẫn đến các giải pháp xếp hạng lại nằm trên biên Pareto của recall so với tính toán.]

7 Kết luận
Chúng tôi đã giới thiệu một chiến lược đào tạo để thích ứng các LM đã được đào tạo trước thành AutoCompressors, nén đệ quy các ngữ cảnh thành các vector tóm tắt. Các thí nghiệm của chúng tôi chỉ ra rằng các vector tóm tắt giữ lại thông tin ngữ cảnh quan trọng, rằng chúng có thể mã hóa các ví dụ minh họa trong ngữ cảnh, và rằng chúng có thể được sử dụng trong các setting truy xuất. Các vector tóm tắt cũng có thể được tính toán trước, cached và tái sử dụng. Điều này cung cấp lợi ích hiệu quả thực tế bằng cách giảm kích thước của cửa sổ attention. Công việc tương lai đáng kể vẫn còn trong việc mở rộng AutoCompressors đến các mô hình lớn hơn và cải thiện chất lượng của các vector tóm tắt để thu hẹp thêm khoảng cách với full attention trên các ngữ cảnh tầm xa.

Hạn chế
1. Chúng tôi chỉ áp dụng AutoCompressors cho các mô hình OPT lên đến 2.7B tham số và một mô hình Llama có 7B tham số. Công việc tương lai cần thiết lập cách AutoCompressors hoạt động đối với các mô hình lớn hơn nữa. Khi chiều của vector tóm tắt tăng lên, có triển vọng giữ lại nhiều thông tin hơn cho mỗi vector.

2. Kết quả của chúng tôi cho thấy rằng các vector tóm tắt bỏ qua một số thông tin hữu ích có thể truy cập thông qua full attention. Ngoài ra, các mô hình không phải lúc nào cũng hưởng lợi từ việc tăng số lượng vector tóm tắt. Chúng tôi nghi ngờ rằng tín hiệu đào tạo để học các vector tóm tắt một cách hiệu quả có thể bị hạn chế bởi việc các mô hình đã được đào tạo trước rất giỏi trong việc đưa ra dự đoán từ các token văn bản thuần túy trong đoạn hiện tại. Công việc tương lai cần thiết để cải thiện tối ưu hóa này.

3. Tích lũy tóm tắt vẫn dẫn đến độ phức tạp bậc hai với số lượng đoạn tăng lên, mặc dù ở tỷ lệ thấp hơn nhiều so với full attention. Công việc tương lai có thể khám phá các cách để kết hợp nhiều vector tóm tắt một cách hiệu quả hơn.

Lời cảm ơn
Chúng tôi cảm ơn Mengzhou Xia, Howard Chen, Vishvak Murahari, Aatmik Gupta, Zirui Wang, Jiatong Yu, và các thành viên của nhóm Princeton NLP vì những thảo luận hữu ích và phản hồi có giá trị. Nghiên cứu này được hỗ trợ bởi giải thưởng NSF CAREER (IIS-2239290), học bổng Sloan Research Fellowship, và giải thưởng Data Science Research từ Adobe. AC cũng biết ơn sự hỗ trợ từ Quỹ Nghiên cứu Minerva.

Tài liệu tham khảo
[Phần tài liệu tham khảo tiếp tục với các trích dẫn học thuật...]

[Tiếp tục với các trang còn lại của tài liệu...]
