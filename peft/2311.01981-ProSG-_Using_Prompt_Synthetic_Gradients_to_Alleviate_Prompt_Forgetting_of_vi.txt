# 2311.01981.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2311.01981.pdf
# Kích thước tệp: 697473 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ProSG: Sử dụng Gradient Tổng hợp Prompt để Giảm thiểu Quên Prompt của 
Các Mô hình Ngôn ngữ Giống RNN
Haotian Luo*Kunming Wu*Cheng Dai†
Sixian Ding Xinhao Chen
Đại học Tứ Xuyên
haotianluo2002@gmail.com
Tóm tắt
Các mô hình ngôn ngữ giống RNN đang nhận được sự quan tâm trở lại
từ các nhà nghiên cứu NLP trong những năm gần đây và một số mô hình đã
đạt được tiến bộ đáng kể, cho thấy hiệu suất có thể so sánh với các transformer truyền thống. Tuy nhiên, do bản chất tuần hoàn của RNN, loại mô hình ngôn ngữ này chỉ có thể lưu trữ thông tin trong một tập hợp các vector trạng thái có độ dài cố định.
Hệ quả là, chúng vẫn gặp phải tình trạng hay quên mặc dù đã có nhiều cải tiến và tối ưu hóa, khi được đưa ra các hướng dẫn hoặc prompt phức tạp. Vì việc tạo sinh có prompt là chức năng chính và được quan tâm nhất của các LM, việc giải quyết vấn đề quên trong quá trình tạo sinh không nghi ngờ gì là có tầm quan trọng cực kỳ lớn. Trong bài báo này, tập trung vào việc giảm thiểu việc quên prompt trong quá trình tạo sinh, chúng tôi đề xuất một kiến trúc để dạy mô hình ghi nhớ prompt trong quá trình tạo sinh bằng gradient tổng hợp. Để buộc mô hình ghi nhớ prompt, chúng tôi suy ra các trạng thái mã hóa prompt, sau đó chuyển đổi nó thành việc sửa đổi tham số mô hình bằng cách sử dụng phép xấp xỉ gradient thứ hạng thấp, từ đó mã hóa cứng prompt vào các tham số mô hình tạm thời. Chúng tôi xây dựng một bộ dữ liệu cho các thí nghiệm, và kết quả đã chứng minh tính hiệu quả của phương pháp chúng tôi trong việc giải quyết vấn đề hay quên trong quá trình tạo sinh có prompt. Chúng tôi sẽ công bố toàn bộ mã nguồn khi được chấp nhận.

Giới thiệu
Transformer (Vaswani et al. 2023) đã lâu thống trị lĩnh vực NLP trên nhiều miền khác nhau do hiệu suất xuất sắc và tính song song trong huấn luyện, nhưng độ phức tạp O(N) cho mỗi bước tạo sinh và chi phí bộ nhớ lưu trữ các cặp key-value lịch sử làm cho transformer kém hiệu quả hơn khi chuỗi đầu ra ngày càng dài. Do đó, các nhà nghiên cứu đã nỗ lực rất nhiều để khám phá cấu trúc hiệu quả hơn, ví dụ như transformer không có attention (AFT) (Zhai et al. 2021). Gần đây, các nhà nghiên cứu đã thành công trong việc phát triển các mô hình mạnh mẽ có cả khả năng huấn luyện song song và suy luận đệ quy, ví dụ như RWKV (Peng et al. 2023) và RetNet (Sun et al. 2023).

Các mô hình như RWKV và RetNet kết hợp ưu điểm của cả transformer và RNN, đảm bảo rằng độ phức tạp huấn luyện không thay đổi trong khi đạt được một quá trình suy luận với độ phức tạp O(N). Thông qua các thí nghiệm rộng rãi, loại mô hình này một cách đáng ngạc nhiên cho thấy hiệu suất có thể so sánh với các transformer truyền thống.

Tuy nhiên, cũng chính vì bản chất tuần hoàn của các mô hình như vậy, có nghĩa là nén thông tin lịch sử thành một tập hợp các vector ẩn trạng thái có độ dài cố định, khiến mô hình không thể giữ lại toàn bộ thông tin lịch sử. Do đó, nó chắc chắn sẽ dẫn đến một loạt các vấn đề quên. Lấy việc tạo sinh có prompt làm ví dụ, với prompt: "Viết một câu chuyện về Tom", kết quả có thể đáp ứng kỳ vọng của chúng ta. Nhưng khi prompt không phải là một hướng dẫn đơn giản mà bao gồm hai bước thao tác, việc quên có thể xảy ra. Ví dụ, với prompt: "Viết một câu chuyện về Tom, câu chuyện này nên có kết thúc bi thảm" sau khi mô hình tạo sinh nội dung tương đối dài, nó có thể quên phần sau của prompt đã cho, và không viết kết thúc bi thảm như mong đợi. Chúng tôi gọi đây là "quên prompt". Chúng tôi sử dụng thuật ngữ này để định nghĩa hiện tượng mà một mô hình ngôn ngữ, khi được đưa ra một prompt cụ thể để tạo sinh, hoặc quên các chi tiết cụ thể của prompt hoặc không tạo sinh theo các yêu cầu được đưa ra bởi prompt. Bảng 1 cho thấy thêm các trường hợp quên prompt.

Vì việc cho phép mô hình tuân theo prompt được coi là mục tiêu tối quan trọng của chức năng quan trọng nhất của một mô hình ngôn ngữ (Liu et al. 2021), việc giải quyết vấn đề quên prompt trong quá trình tạo sinh của mô hình ngôn ngữ chắc chắn là bắt buộc và cần được giải quyết.

Để thuận tiện cho việc mô tả và công thức hóa vấn đề, chúng tôi đã định nghĩa thuật ngữ "prompt đa giai đoạn" để chỉ một prompt bao gồm nhiều hơn một tiểu prompt hoặc quá trình (các ví dụ được hiển thị trong Bảng 1). Việc quên xảy ra do độ dài của vector trạng thái mô hình là cố định. Trong quá trình nhập văn bản vào mô hình, có những tình huống mà mô hình không thể xác định liệu có nên giữ lại thông tin thu được tại thời điểm hiện tại hay không. Khi nhiều văn bản được nhập vào, thông tin quan trọng cũng có nguy cơ bị quên. Vấn đề này đặc biệt có hại đối với các tác vụ tạo sinh có prompt, là chức năng chính của các mô hình ngôn ngữ được huấn luyện trước lớn.

Trong bài báo này, nhằm giảm thiểu việc quên prompt trong quá trình tạo sinh, chúng tôi khai thác một cấu trúc gọi là ProSG (Prompt Synthetic Gradient), mã hóa thông tin prompt vào mô hình tạm thời trong một lần chạy tạo sinh để tăng chất lượng tạo sinh. Đầu tiên, để mô hình hóa

--- TRANG 2 ---
Mẫu Prompt Hành vi Đầu ra
A Viết một câu chuyện về học sinh Noah,
và viết một bản tóm tắt cho câu chuyện
ở cuối.Quên tóm tắt câu chuyện ở 
cuối đầu ra.Noah là một học sinh thông minh luôn
mơ ước ... Cuối cùng, anh ấy
hiện đang làm bác sĩ tại một bệnh viện
nơi anh ấy cảm thấy hài lòng với những gì
anh ấy làm mỗi ngày.
B Liệt kê 10 tên nhân vật, và viết
một câu chuyện sử dụng những tên này.Liệt kê chính xác 10 tên nhưng quên 
viết câu chuyện.1. Sophia - Một phụ nữ tốt bụng.
2. Jack - Một người đàn ông quyến rũ thích
khám phá. ... 10. Max -
Một người đàn ông đáng tin cậy luôn
ở bên bạn bè và gia đình.
C Cho một dãy số: 1, 0,
1, 0, 0, 6, 2, 0, 0, 7, 2, 6, 0, 1, 2, 9.
Bây giờ hãy đọc lại nó.Quên các số đã cho. Dãy số là: 0, 1, 0, 1, 2, 0, 0,
1, 2, 6, 7, 2, 9
Bảng 1: Ví dụ về quên prompt

Trạng thái LLM LLM
promptđầu ra
Gradient
Tổng hợp
Hình 1: Tổng quan Kiến trúc.

vấn đề tốt hơn, chúng tôi xây dựng một bộ dữ liệu, chứa 21k prompt đa giai đoạn và các câu trả lời tương ứng theo mô hình chung của tinh chỉnh hướng dẫn. Thứ hai, một mô-đun thích ứng được sử dụng để tính toán gradient của prompt và áp dụng thay đổi trong việc tạo sinh tiếp theo. Mô-đun thích ứng này là một tập hợp các mạng nơ-ron sử dụng kỹ thuật thích ứng thứ hạng thấp (Hu et al. 2021) nhận trạng thái mã hóa prompt làm đầu vào và tạo ra gradient tổng hợp (Jaderberg et al. 2017) làm đầu ra. Kiến trúc thô được hiển thị trong Hình 1.

Chúng tôi tiến hành các thí nghiệm rộng rãi và đầy đủ để đánh giá tính hiệu quả của cấu trúc và bộ dữ liệu mới của chúng tôi. Kết quả thí nghiệm cho thấy phương pháp của chúng tôi dẫn đến hiệu suất tạo sinh xuất sắc về cả các chỉ số tự động và đánh giá của con người. Tóm lại, các đóng góp của chúng tôi như sau:

(1) Chúng tôi đánh giá hiện tượng quên prompt và xây dựng bộ dữ liệu hướng dẫn đa giai đoạn (MuSI);
(2) Chúng tôi đề xuất một khung làm việc tạo điều kiện cho sự hợp tác tốt hơn với các bộ dữ liệu bằng cách mã hóa prompt tạm thời vào các tham số mô hình, do đó giảm thiểu đáng kể vấn đề quên prompt.

Công trình liên quan
Mô hình ngôn ngữ giống RNN
Sau những nỗ lực của các nhà nghiên cứu, hiện tại có hai mô hình giống RNN tương đối thành công, đó là RWKV (Peng et al. 2023) và RetNet (Sun et al. 2023). Loại mô hình này kết hợp ưu điểm của cả transformer và RNN, đảm bảo rằng tính song song huấn luyện không thay đổi trong khi đạt được quá trình suy luận với độ phức tạp tuyến tính. Mặc dù hai mô hình này có hiệu suất tốt, là RNN, chúng vẫn tương đối bị hạn chế bởi dung lượng bộ nhớ cố định, là hệ quả trực tiếp của các vector trạng thái cố định. Nhược điểm này dẫn đến hiệu ứng quên đáng chú ý, đặc biệt khi đưa ra hướng dẫn đa giai đoạn cho mô hình để tạo sinh. Mô hình quên thông tin về prompt đã cho khi quá trình tạo sinh tiến triển, hoặc thậm chí trước khi việc tạo sinh bắt đầu. Mặc dù hai mô hình đều tuyên bố có hiệu suất tốt, các nhà nghiên cứu của RWKV đã phát hành mã nguồn trong thời gian tương đối dài. Vì vậy chúng tôi chọn RWKV để tiến hành hầu hết công việc nghiên cứu của mình.

ICL là Tinh chỉnh Ngầm định
ICL (học trong ngữ cảnh) (Dong et al. 2023) là một khả năng nổi lên của các mô hình ngôn ngữ lớn cho phép chúng thích ứng với nhiều tác vụ khác nhau khi được đưa ra một tập hợp các ví dụ minh họa. Tính chất này đã được chứng minh trong các LLM như GPT-3 (Brown et al. 2020). (Dai et al. 2023) cung cấp một cái nhìn mới về việc giải thích ICL như một quá trình meta-tối ưu hóa, xem xét LM như một meta-optimizer tạo ra meta-gradient theo các ví dụ minh họa thông qua tính toán tiến. Sau đó gradient sẽ được áp dụng cho LM gốc thông qua attention để xây dựng mô hình ICL.

Được truyền cảm hứng từ quan điểm này, chúng tôi nảy ra ý tưởng xem xét việc tạo sinh có hướng dẫn như một quá trình tinh chỉnh ngầm định, có nghĩa là mô hình có thể được xem là được tối ưu hóa bởi gradient từ prompt. Vì vậy chúng tôi có thể tổng hợp

--- TRANG 3 ---
Mô tả các loại 
prompt khác nhauGPT-3.5
… 
…… 
…GPT-3.5… 
…
Câu trả lời cho mỗi promptHãy cho tôi
prompt thỏa mãn điều kiện này: 
[bao gồm hai hướng dẫn phụ],
Dưới đây là
một số ví dụ:
(1) viết một câu chuyện mà...
(2) viết một bài thơ mà...
Ví dụ Giai đoạn 1: Tạo promptCác loại prompt khác nhauThám tử Johnson được biết đến với 
bản năng nhạy bén và quyết tâm 
không lay chuyển. …Viết một câu chuyện ngắn về một thám tử 
giải quyết một vụ án bí ẩn.
Giai đoạn 2: Tạo câu trả lời
Hình 2: Xây dựng Bộ dữ liệu.

gradient do prompt mang lại để đạt được mục tiêu giảm thiểu việc quên.

Gradient Tổng hợp
Gradient Tổng hợp (Jaderberg et al. 2017) là một phương pháp được sử dụng để tách rời các mô-đun nơ-ron, nhằm tăng tốc quá trình huấn luyện. Trong việc huấn luyện mạng nơ-ron truyền thống, thông tin gradient cần được truyền từ lớp đầu ra đến lớp đầu vào thông qua lan truyền ngược (Rumelhart, Hinton, và Williams 1986). Tuy nhiên, gradient tổng hợp xấp xỉ thông tin gradient thực bằng cách giới thiệu một mô hình bổ sung. Thông thường, mô hình gradient tổng hợp nhận đầu ra của các lớp trung gian của mạng nơ-ron làm đầu vào và tạo ra một gradient xấp xỉ, sau đó được truyền đến lớp trước của mạng. Trong khung làm việc của chúng tôi, chúng tôi sử dụng kỹ thuật này để xấp xỉ gradient được tạo ra bởi prompt.

Thích ứng Thứ hạng Thấp
LoRA (Hu et al. 2021) là một kỹ thuật để giảm độ phức tạp tính toán và yêu cầu bộ nhớ khi tinh chỉnh các mô hình. LoRA đã được chứng minh mang lại kết quả xuất sắc trong nhiều tác vụ tinh chỉnh khác nhau trong lĩnh vực NLP.

Chúng tôi đưa ra giả định rằng các tham số của mô hình chỉ cần những sửa đổi nhỏ để đạt được mức độ ghi nhớ thông tin prompt nhất định có khả năng nâng cao việc tạo sinh. Vì vậy chúng tôi sử dụng thích ứng thứ hạng thấp khi tính toán gradient tổng hợp prompt để giảm chi phí bộ nhớ của khung làm việc.

Phương pháp
Bộ dữ liệu
Hướng dẫn Thông thường vs. Hướng dẫn Đa giai đoạn Hầu hết hướng dẫn thông thường chỉ bao gồm một thao tác/bước, ví dụ, "Viết một câu chuyện về Tom". Bộ dữ liệu được sử dụng bởi Alpaca là bộ dữ liệu điển hình chứa chủ yếu các hướng dẫn đơn giản. Trong khi hướng dẫn đa giai đoạn bao gồm nhiều hơn một thao tác/bước, ví dụ, "Viết một câu chuyện về Tom, và cũng tạo một bản tóm tắt ở cuối". Đối với các mô hình ngôn ngữ truyền thống sử dụng kiến trúc transformer, hai loại hướng dẫn có sự khác biệt nhẹ, vì transformer có thể bảo toàn thông tin hoàn chỉnh về hướng dẫn bằng cách lưu trữ các cặp K-V. Tuy nhiên, nó hoàn toàn khác đối với các mô hình giống RNN vì bản chất tuần hoàn dẫn đến việc mất thông tin không thể tránh khỏi trong quá trình tạo sinh, điều này vô hiệu hóa khả năng "nhìn lại" hướng dẫn của các mô hình giống RNN như transformer. Hệ quả là, hiện tượng quên của mô hình giống RNN đặc biệt rõ ràng khi xử lý các hướng dẫn đa giai đoạn.

Bộ dữ liệu Hướng dẫn Đa giai đoạn (MuSI) Để xử lý quá trình quên tốt hơn, chúng tôi tận dụng ChatGPT để thu thập một bộ dữ liệu có tên là Bộ dữ liệu Hướng dẫn Đa giai đoạn (MuSI), chứa một số hướng dẫn đa giai đoạn và các câu trả lời tương ứng, và sau đó chúng tôi loại bỏ thủ công dữ liệu không tuân thủ. Cuối cùng, chúng tôi thu thập 22k cặp hướng dẫn-câu trả lời. Cần nhấn mạnh rằng các prompt được chọn cho bộ dữ liệu của chúng tôi chứa rất ít kiến thức chuyên môn cụ thể, cho phép so sánh công bằng hơn với các mô hình ngôn ngữ lớn khác trong các lĩnh vực công cộng sau khi tinh chỉnh.

--- TRANG 4 ---
12n-1n
𝑥012n-1n
𝑥1...
...
...
... 12n-1n
𝑥𝑛𝑦0
1(+𝛥𝑊1)2(+𝛥𝑊2)(n-1)(+𝛥𝑊𝑛−1)n(+𝛥𝑊𝑛)
𝑦01(+𝛥𝑊1)2(+𝛥𝑊2)(n-1)(+𝛥𝑊𝑛−1)n(+𝛥𝑊𝑛)
𝑦1𝑦2 𝑦1
...
...
...
...Tổng hợp
Gradient
... ... ... ... ... ... ...iLớp RNN thứ i
trước khi tổng hợp gradientLớp Tuyến tính Đầu ra
iLớp RNN thứ i
sau khi tổng hợp gradient
Trạng thái ẩn
𝑥𝑖
𝑦𝑖Token prompt
Token đầu ra
𝛥𝑊𝑖 Gradient prompt
Hình 3: Kiến trúc mô hình.

Toàn bộ quá trình xây dựng MuSI được minh họa trong Hình 2.

Tổng quan Kiến trúc của chúng tôi bao gồm hai phần, một trong số đó là LM giống RNN nền tảng, được đặt là RWKV vì nó đã phát hành nhiều tài nguyên nhất. Phần khác là mô-đun tổng hợp gradient prompt G. Đối với mỗi lần tạo sinh, chúng tôi trước tiên đưa chuỗi prompt X, chứa n token {x1,x2, ...,xn} vào RNN LM, sau đó vector trạng thái ẩn Hn chứa thông tin prompt sẽ được sử dụng để tạo gradient tổng hợp. Tóm gọn, các trạng thái ẩn sẽ được sử dụng bởi mô-đun tổng hợp gradient làm đầu vào, và đầu ra là gradient xấp xỉ thứ hạng thấp ∆W, được mong đợi là truyền đạt thông tin prompt một cách nhất quán trong quá trình tạo sinh bằng cách thêm vào tham số gốc W như một số gia. Do đó trong việc tạo sinh tiếp theo, mỗi lần chuyển tiếp sản xuất token đầu ra thứ k yk sẽ được tăng cường với số gia tham số ∆W. Toàn bộ quá trình có thể được công thức hóa như sau:

{H0,H1...,Hn}=RNN (x1,x2, ...,xn) (1)
∆W =G(Hn),yk=RNN (+∆W )(yk−1) (2)

RWKV RWKV là một mô hình giống RNN chủ yếu bao gồm một chồng các khối dư, mỗi khối được tạo thành bởi một khối phụ trộn thời gian và một khối phụ trộn kênh với cấu trúc tuần hoàn. Trong bài báo này, chúng tôi thêm mô-đun tổng hợp gradient vào khối trộn thời gian. Khối trộn thời gian có thể được công thức hóa như:

rt=Wr·(µrxt+ (1−µr)xt−1) (3)
kt=Wk·(µkxt+ (1−µk)xt−1) (4)
vt=Wv·(µvxt+ (1−µv)xt−1) (5)
wkv t=Pt−1
i=1e−(t−1−i)w+kivi+eu+ktvtPt−1
i=1e−(t−1−i)w+ki+eu+kt(6)
ot=Wo·(σ(rt)⊙wkv t) (7)

Quá trình tạo sinh sẽ được sửa đổi bởi gradient tổng hợp của chúng tôi sau khi tính toán gradient prompt. Do đó tính toán time-mix sẽ là:

rt= (Wr+ ∆Wr)·(µrxt+ (1−µr)xt−1) (8)
kt= (Wk+ ∆Wk)·(µkxt+ (1−µk)xt−1)(9)
vt= (Wv+ ∆Wv)·(µvxt+ (1−µv)xt−1)(10)
wkv t=Pt−1
i=1e−(t−1−i)w+kivi+eu+ktvtPt−1
i=1e−(t−1−i)w+ki+eu+kt(11)
ot= (Wo+ ∆Wo)·(σ(rt)⊙wkv t) (12)

Tính toán của các phần khác trong mô hình RWKV vẫn không thay đổi.

Nhúng Trạng thái Do các không gian đặc trưng riêng biệt của các ma trận cho các trạng thái key, value, receptance, và output, chúng tôi sử dụng cùng một mô-đun tổng hợp gradient để xấp xỉ gradient của bốn tham số trong cùng một lớp. Cách tiếp cận này giúp giảm số lượng tham số và cho phép tính toán song song. Phương pháp này tăng tốc tính toán, nhưng nó cũng có một khuyết điểm chí mạng: các tham số khác nhau nên nhận được các cập nhật khác nhau khi tính toán gradient tổng hợp prompt. Để khắc phục vấn đề này, chúng tôi giới thiệu nhúng trạng thái.

Nhúng trạng thái là một tập hợp các vector nhúng đã học cho mỗi trạng thái khác nhau được lưu trữ trong mỗi lớp, sẽ được thêm vào đầu vào x□ trước khi được đưa vào mô-đun cốt lõi (chồng biến đổi S) của mô-đun tổng hợp gradient G.

Nhúng trạng thái cho phép sử dụng cùng một mô-đun để xử lý đầu vào trong bốn chế độ khác nhau, cho phép một mô-đun duy nhất cung cấp gradient tổng hợp với các tính chất riêng biệt cho bốn loại trạng thái.

Chồng Biến đổi Chồng biến đổi S là một tập hợp các khối giống hệt nhau, là một mô hình tuần tự chứa các lớp tuyến tính, hàm kích hoạt, và layer norm. Cụ thể, một khối biến đổi bao gồm một lớp tuyến tính, một hàm kích hoạt ReLU, một lớp tuyến tính khác, và một layer norm. Chúng tôi cũng thêm một kết nối dư giữa đầu ra và đầu vào. Đối với một khối đơn. Chồng biến đổi là mô-đun cốt lõi được sử dụng để tổng hợp gradient.

Ma trận Thứ hạng thấp Đầu ra Hình dạng của vector trạng thái là 1×E, và hình dạng của ma trận trạng thái là E×E. Cần lưu ý rằng chúng tôi mở rộng trạng thái thành một vector có hình dạng 1×nE, trong đó n được chọn từ 1, 2 hoặc 3. Vector trạng thái không thay đổi sau khi được xử lý bởi chồng biến đổi và sau đó sẽ được định hình lại thành n×E. Dựa trên giả định rằng việc ghi nhớ prompt có thể đạt được ở mức độ nhất định bởi các thay đổi bậc thấp trong không gian tham số mô hình, chúng tôi sử dụng kỹ thuật thích ứng thứ hạng thấp. Để biến đổi hình dạng đầu ra thành E×E, chúng tôi thiết kế một ma trận thứ hạng thấp đầu ra đã học B có hình dạng E×n, sẽ nhân với đầu ra của chồng biến đổi S.

Mô-đun Tổng hợp Gradient Mô-đun tổng hợp gradient của chúng tôi bao gồm một chồng biến đổi S, một tập hợp nhúng trạng thái e, và một tập hợp ma trận thứ hạng thấp đầu ra B. Mô-đun này được hiển thị trong Hình 4. Vì thiết kế chi tiết của trạng thái ẩn khác nhau theo kiến trúc mô hình, công thức của chúng tôi được chỉ định với RWKV để hiển thị chi tiết mô-đun rõ ràng hơn. Trạng thái của mỗi lớp sẽ được đưa vào Mô-đun Tổng hợp Gradient G và tính toán gradient. Mỗi lớp thực hiện cùng một thao tác, để công thức hóa toàn bộ quá trình một cách chi tiết, chúng tôi chọn lớp thứ i làm ví dụ. Trong các thí nghiệm của chúng tôi, chúng tôi chọn đầu vào x của mỗi lớp, mã hóa prompt để thêm với 4 nhúng trạng thái khác nhau:

xk=x+ek (13)
xv=x+ev (14)
xr=x+er (15)
xo=x+eo (16)
(17)

trong đó k, v, r, o lần lượt đại diện cho key, value, receptance và output. Và sau đó mỗi x□ sẽ được xử lý bởi chồng biến đổi và tạo ra A□:

A□=S(x□) (18)
(19)

Sau đó, ma trận thứ hạng thấp A□ sẽ nhân với B□ tương ứng và tạo ra ∆W□.

∆W□=B□×A□ (20)
(21)

Chiến lược Huấn luyện
Huấn luyện Song song Trong quá trình huấn luyện, mô-đun tổng hợp gradient yêu cầu truy cập vào các trạng thái ẩn của prompt X đã mã hóa, cụ thể là vector trạng thái cuối cùng. Để đạt được điều này, chúng tôi ban đầu đệm một số prompt {X1,X2, ...,Xk}, có độ dài {l1,l2, ...,lk} và nhập một batch vào mô hình, do đó thu được chuỗi vector trạng thái {H1,H2, ...,Hk},Hi∈RL×E, trong đó L là độ dài đệm và E là chiều kênh. Tuy nhiên, do độ dài prompt khác nhau và việc đưa vào các token đệm, thông tin trong vector ở cuối prompt có thể bị mất. Thứ cần thiết thực tế là {Hl1
1,Hl2
2, ...,Hlk
k},Hj
i∈RE thay vì {HL
1,HL
2, ...,HL
k}. Do đó, trong quá trình xử lý, điều quan trọng là phải truyền độ dài gốc của prompt. Điều này cho phép chúng tôi sử dụng vector tại vị trí tương ứng với độ dài gốc của prompt như vector trạng thái cần thiết và cũng đảm bảo tính nhất quán giữa huấn luyện và suy luận.

Huấn luyện Hai giai đoạn Chúng tôi sử dụng phương pháp huấn luyện hai giai đoạn, trong đó giai đoạn đầu tiên bao gồm việc tinh chỉnh mô hình ngôn ngữ nền tảng, với một loss huấn luyện, và giai đoạn thứ hai tập trung vào việc huấn luyện mô-đun tổng hợp gradient để tăng cường hơn nữa khả năng ghi nhớ của nó. Chính thức, đặt bộ dữ liệu là D với kích thước N. giai đoạn đầu tiên nhằm tối đa hóa log-likelihood (ký hiệu bởi JRNN) của mục tiêu trên tất cả các mẫu huấn luyện của D, tức là,

JRNN=NX
nTnX
tlogPθ1(xn,t|xn,<t) (22)

trong đó xn,t đại diện cho từ thứ t của mẫu thứ n. Tn biểu thị độ dài từ của mẫu yn. θ1 là tham số của mô hình. Giai đoạn thứ hai cũng nhằm tối đa hóa log-likelihood của tất cả các mẫu huấn luyện, nhưng tham số của mô hình nền tảng θ1 được đóng băng, có thể được công thức hóa như sau:

JS=NX
nTnX
tlogPθ1,θ2(xn,t|xn,<t) (23)

Trong công thức này, JS biểu thị log-likelihood của giai đoạn thứ hai, tối ưu hóa mô-đun tổng hợp gradient, có tham số θ2

--- TRANG 5 ---
Hình 4: Mô-đun tổng hợp gradient.

Thí nghiệm
Bộ dữ liệu
Do thiếu một bộ dữ liệu phù hợp có thể đáp ứng hiệu quả các yêu cầu của hướng dẫn đa giai đoạn hiện tại, tất cả các thí nghiệm của chúng tôi đều được tiến hành trên bộ dữ liệu MuSI.

Phương pháp/Mô hình Cạnh tranh
Chúng tôi sẽ so sánh mô hình với các mô hình có kích thước tương tự, và cũng các mô hình được công nhận rộng rãi với nhiều tham số hơn có khả năng tạo ra đầu ra thỏa mãn. Chúng tôi mô tả một số mô hình được chọn như sau:

GPT-2, Vicuna, ChatGLM GPT-2 (Radford et al. 2019), Vicuna (Chiang et al. 2023), ChatGML (Zeng et al. 2022) đều là các transformer decoder-only được huấn luyện trước. Chúng có khả năng hiểu ngữ cảnh của văn bản đầu vào và tạo ra văn bản đầu ra mạch lạc và logic. Là transformer, chúng giữ lại toàn bộ chuỗi token lịch sử. Do đó, chúng tôi coi ba mô hình này là chuẩn mực.

RWKV Đối với RWKV, chúng tôi thiết kế ba thí nghiệm: (1) Phiên bản tinh chỉnh gốc của RWKV-4-World; (2) Tinh chỉnh MuSI; (3) Tinh chỉnh MuSI cộng với kiến trúc ProSG. Thiết lập thí nghiệm này cho phép chúng tôi phân tích liệu MuSI và ProSG có đóng vai trò trong việc giảm thiểu quên prompt hay không.

Chỉ số Đánh giá
Chúng tôi áp dụng ba chỉ số phân tích: Perplexity, Độ chính xác, và Đánh giá của Con người.

Perplexity Perplexity, hay PPL, là một thước đo định lượng chỉ ra mức độ tốt của một mô hình ngôn ngữ trong việc dự đoán một tập hợp từ cho trước dựa trên dữ liệu huấn luyện của nó. Các giá trị perplexity thấp hơn gợi ý rằng mô hình tốt hơn trong việc đưa ra các dự đoán chính xác.

Perplexity của các mô hình ngôn ngữ là số mũ của cross-entropy và được sử dụng để chỉ ra khả năng dự đoán của mô hình. Giả sử chúng ta có một tập dữ liệu kiểm tra được ký hiệu là D, bao gồm N mẫu, mỗi mẫu được biểu diễn là xi, trong đó xi là một chuỗi văn bản đã được tokenize. Mô hình dự đoán phân phối xác suất pi(y) cho token tiếp theo đối với mỗi chuỗi xi, trong đó y đại diện cho token tiếp theo. Đối với mỗi mẫu xi, công thức để tính cross-entropy là:

H(xi) =−1
MMX
j=1logpi(yj)

Ở đây, M là số token trong mẫu xi, yj là token thứ j trong mẫu, và pi(yj) là xác suất dự đoán của token thứ j bởi mô hình. Sau đó, tổng hợp cross-entropy cho tất cả các mẫu để thu được cross-entropy cho toàn bộ bộ dữ liệu:

H(D) =NX
i=1H(xi)

Cuối cùng, perplexity (PPL) có thể được tính toán bằng cách sử dụng cross-entropy:

PPL (D) =eH(D)

Độ chính xác Chúng tôi định nghĩa độ chính xác là liệu nội dung được tạo ra có tuân theo prompt đã cho hay không. Nếu một đầu ra trả lời tất cả các câu hỏi hoặc xem xét tất cả các hướng dẫn phụ trong prompt đã cho, đầu ra này sẽ nhận được một điểm; ngược lại, nó sẽ nhận được số không.

Phương pháp đánh giá này có hai ưu điểm sau:(1) cho phép chúng tôi tập trung vào việc phân tích liệu việc tạo sinh của mô hình có tuân theo prompt hay không, từ đó xác định mức độ quên trong quá trình tạo sinh. (2) bằng cách không tập trung vào việc học phong cách ngôn ngữ của bộ dữ liệu của mô hình trong quá trình tinh chỉnh, chúng tôi có thể so sánh nó với các mô hình khác chưa được tinh chỉnh bởi MuSI.

Tuy nhiên, việc phân tích thủ công một khối lượng lớn đầu ra của mô hình để kiểm tra tính tuân thủ khá thách thức và tốn thời gian. Do đó, chúng tôi tận dụng ChatGPT với các prompt phù hợp để đạt được mục tiêu phân tích tự động thô. Thông qua xác nhận của chúng tôi, cách tiếp cận này đã chứng minh độ chính xác tương đối đáng tin cậy như một đánh giá tự động thô.

Đánh giá của Con người Vì chúng tôi quan sát thấy rằng ChatGPT không đưa ra được các phán đoán chính xác về một số câu hỏi nhất định, chẳng hạn như những câu hỏi liên quan đến tính toán toán học, và cũng vì mục đích đạt được phân tích toàn diện và chi tiết hơn cho ba biến thể RWKV, chúng tôi đã tiến hành đánh giá thủ công. Chúng tôi đã yêu cầu năm người chú thích phân tích và so sánh

--- TRANG 6 ---
Mô hình Perplexity Độ chính xác Đánh giá của Con người
Trôi chảy Chính xác Chất lượng
Vicuna-7B – 0.760 – – –
ChatGLM-6B – 0.938 – – –
GPT-2-0.4B (MuSI) – 0.893 – – –
RWKV-4-0.4B 5.480 0.534 4.03 3.42 3.45
RWKV-4-0.4B (MuSI) 3.583 0.698 4.10 3.57 3.58
RWKV-4-0.4B (MuSI, ProSG) 3.161 0.761 4.22 3.87 3.78
Bảng 2: Kết quả thí nghiệm. PPL của ba mô hình ngôn ngữ đầu tiên không được tính toán. Bởi vì chúng có cách tokenize khác nhau, kết quả không thể so sánh được.

chất lượng, độ trôi chảy, và độ chính xác của kết quả được tạo ra một cách mù quáng. Phạm vi điểm số từ 0 đến 5 được thiết lập cho cả ba chỉ số.

Ba chỉ số này có định nghĩa sau:
1) Trôi chảy: để đo lường độ trôi chảy của các câu được tạo ra và xác định bất kỳ trường hợp nào của việc tạo sinh lặp lại.
2) Chính xác: giống như đã đề cập trong phần phụ trước.
3) Chất lượng: để đánh giá liệu các câu trả lời được cung cấp có mạch lạc logic, chi tiết phù hợp, và phù hợp với sở thích của con người hay không.

Kết quả
Kết quả Đánh giá Tự động Bảng 2 trình bày hiệu suất của tất cả các mô hình. Từ bảng, chúng ta có thể quan sát thấy rằng RWKV, sau khi tinh chỉnh với bộ dữ liệu MuSI, đã đạt được điểm số độ chính xác cao hơn. Điều này cho thấy rằng bộ dữ liệu của chúng tôi thực sự đã giảm thiểu việc quên prompt của mô hình. Hơn nữa, việc sử dụng kiến trúc ProSG đã nâng cao khả năng ghi nhớ của mô hình, có thể được suy ra từ sự cải thiện trong các chỉ số. Các phương pháp của chúng tôi làm cho hiệu suất của mô hình gần hơn nhiều với ba transformer, có khả năng ghi nhớ hoàn chỉnh. Điều này hoàn toàn phù hợp với kỳ vọng của chúng tôi.

Kết quả Đánh giá của Con người Kết quả đánh giá của con người được báo cáo ở phần bên phải của Bảng 2. Chúng ta có thể thấy rằng, cả bộ dữ liệu và kiến trúc của chúng tôi đều đóng góp vào việc cải thiện hiệu suất của mô hình. Kết quả phù hợp với chỉ số tự động.

Nghiên cứu Trường hợp Để có được bài trình bày trực quan về kết quả được tạo ra, chúng tôi chọn một vài prompt và ghi lại đầu ra của một số mô hình khác nhau, được hiển thị trong Bảng 3. Do hạn chế về không gian, chúng tôi chỉ hiển thị 1 mẫu và hiển thị các phần chính của kết quả, bỏ qua nhiều chi tiết ít quan trọng hơn. Kết quả đầy đủ có thể được tìm thấy trong tài liệu bổ sung. Chúng ta có thể quan sát thấy một sự cải thiện đáng kể trong đầu ra của mô hình sau khi kết hợp kiến trúc ProSG, đạt được hiệu ứng mong muốn của việc ghi nhớ prompt. Điều này một lần nữa chứng minh tính hiệu quả của khung làm việc của chúng tôi.

Kết luận
Trong bài báo này, chúng tôi phân tích cụ thể vấn đề quên prompt trong quá trình tạo sinh. Chúng tôi xây dựng một bộ dữ liệu hướng dẫn đa giai đoạn (MuSI) và đề xuất một kiến trúc dựa trên tổng hợp gradient (ProSG) để giảm thiểu việc quên prompt. Các thí nghiệm rộng rãi chứng minh rằng cách tiếp cận của chúng tôi thành công trong việc nâng cao khả năng ghi nhớ prompt của mô hình, cho phép các tác vụ tạo sinh tốt hơn dưới sự kiểm soát của prompt.

Hạn chế
Mặc dù phương pháp của chúng tôi đã đạt được kết quả đầy hứa hẹn, do hạn chế về tài nguyên, chúng tôi không thể tiến hành các thí nghiệm liên quan trên các mô hình ngôn ngữ lớn hơn như 13B hoặc 33B. Nghiên cứu trong tương lai nên được thực hiện để khám phá việc quên prompt và giải pháp tương ứng của các mô hình ngôn ngữ lớn.

Tài liệu tham khảo
[Các tài liệu tham khảo được dịch từ tiếng Anh sang tiếng Việt]

--- TRANG 7 ---
Hướng dẫn
Ba loại thói quen tập thể dục phổ biến là gì? Hãy mô tả ngắn gọn từng loại và đề xuất một loại cho người muốn cải thiện thể lực tim mạch.

RWKV-4-0.4B (MuSI, ProSG)
Ba loại thói quen tập thể dục phổ biến là:
1. Tập thể dục tim mạch: Loại này ...
2. Tập sức mạnh: Loại này ...
3. Yoga: Yoga là một ...
Đối với người muốn cải thiện thể lực tim mạch, tôi sẽ đề xuất sự kết hợp của các bài tập aerobic và các bài tập tập sức mạnh để cải thiện thể lực tim mạch và sức khỏe tổng thể.

RWKV-4-0.4B (MuSI)
Ba loại thói quen tập thể dục phổ biến là cardio, tập sức mạnh, và các bài tập linh hoạt.
1. Tập thể dục Tim mạch: Điều này bao gồm ...
2. Tập Sức mạnh: Điều này bao gồm ...
3. Tập Linh hoạt: Điều này bao gồm ...

RWKV-4-0.4B
1. Đi bộ: Đi bộ là một bài tập ít tác động ...
2. Đạp xe: Đạp xe là một bài tập ít tác động ...
3. Bơi lội: Bơi lội là một bài tập ít tác động ...

Bảng 3: Nghiên cứu trường hợp

--- TRANG 8 ---
A.; Sutskever, I.; và Amodei, D. 2020. Các Mô hình Ngôn ngữ là Người Học Vài Mẫu. arXiv:2005.14165.

Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; và Xing, E. P. 2023. Vicuna: Một Chatbot Mã nguồn Mở Gây Ấn tượng với GPT-4 với 90%* Chất lượng ChatGPT.

Dai, D.; Sun, Y.; Dong, L.; Hao, Y.; Ma, S.; Sui, Z.; và Wei, F. 2023. Tại sao GPT Có thể Học Trong Ngữ cảnh? Các Mô hình Ngôn ngữ Thực hiện Ngầm định Gradient Descent như Meta-Optimizer. arXiv:2212.10559.

Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; Li, L.; và Sui, Z. 2023. Một Khảo sát về Học Trong Ngữ cảnh. arXiv:2301.00234.

Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; và Chen, W. 2021. LoRA: Thích ứng Thứ hạng Thấp của Các Mô hình Ngôn ngữ Lớn. arXiv:2106.09685.

Jaderberg, M.; Czarnecki, W. M.; Osindero, S.; Vinyals, O.; Graves, A.; Silver, D.; và Kavukcuoglu, K. 2017. Giao diện Nơ-ron Tách rời sử dụng Gradient Tổng hợp. Trong Precup, D.; và Teh, Y. W., biên tập, Kỷ yếu Hội nghị Quốc tế lần thứ 34 về Học Máy, tập 70 của Kỷ yếu Nghiên cứu Học Máy, 1627–1635. PMLR.

Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; và Neubig, G. 2021. Tiền-huấn luyện, Prompt, và Dự đoán: Một Khảo sát Hệ thống về Các Phương pháp Prompting trong Xử lý Ngôn ngữ Tự nhiên. arXiv:2107.13586.

Peng, B.; Alcaide, E.; Anthony, Q.; Albalak, A.; Arcadinho, S.; Cao, H.; Cheng, X.; Chung, M.; Grella, M.; GV, K. K.; He, X.; Hou, H.; Kazienko, P.; Kocon, J.; Kong, J.; Koptyra, B.; Lau, H.; Mantri, K. S. I.; Mom, F.; Saito, A.; Tang, X.; Wang, B.; Wind, J. S.; Wozniak, S.; Zhang, R.; Zhang, Z.; Zhao, Q.; Zhou, P.; Zhu, J.; và Zhu, R.-J. 2023. RWKV: Tái phát minh RNN cho Kỷ nguyên Transformer. arXiv:2305.13048.

Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; và Sutskever, I. 2019. Các Mô hình Ngôn ngữ là Người Học Đa nhiệm Không giám sát.

Rumelhart, D. E.; Hinton, G. E.; và Williams, R. J. 1986. Học biểu diễn bằng cách lan truyền ngược lỗi. nature, 323(6088): 533–536.

Sun, Y.; Dong, L.; Huang, S.; Ma, S.; Xia, Y.; Xue, J.; Wang, J.; và Wei, F. 2023. Mạng Giữ lại: Một Người Kế nhiệm của Transformer cho Các Mô hình Ngôn ngữ Lớn. arXiv:2307.08621.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; và Polosukhin, I. 2023. Attention Là Tất cả Những gì Bạn Cần. arXiv:1706.03762.

Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm-130b: Một mô hình được huấn luyện trước song ngữ mở. arXiv preprint arXiv:2210.02414.

Zhai, S.; Talbott, W.; Srivastava, N.; Huang, C.; Goh, H.; Zhang, R.; và Susskind, J. 2021. Một Transformer Không có Attention. arXiv:2105.14103.
