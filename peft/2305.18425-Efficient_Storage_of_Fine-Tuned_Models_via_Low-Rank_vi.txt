Lưu trữ Hiệu quả của Các Mô hình Tinh chỉnh thông qua
Xấp xỉ Hạng Thấp của Phần dư Trọng số

Tóm tắt
Trong bài báo này, chúng tôi trình bày một phương pháp hiệu quả để lưu trữ các mô hình tinh chỉnh bằng cách tận dụng các đặc tính hạng thấp của phần dư trọng số. Quan sát quan trọng của chúng tôi là phần dư trọng số trong các mô hình tham số hóa quá mức lớn thể hiện các đặc tính hạng thấp mạnh mẽ hơn nữa. Dựa trên hiểu biết này, chúng tôi đề xuất Mã hóa Phần dư Hiệu quả (ERE), một cách tiếp cận mới đạt được lưu trữ hiệu quả của trọng số mô hình tinh chỉnh bằng cách xấp xỉ phần dư trọng số hạng thấp. Hơn nữa, chúng tôi phân tích tính bền vững của phần dư trọng số và đẩy giới hạn của hiệu quả lưu trữ bằng cách sử dụng lượng tử hóa bổ sung và phân bổ hạng theo lớp. Kết quả thực nghiệm của chúng tôi chứng minh rằng phương pháp của chúng tôi giảm đáng kể dấu chân bộ nhớ trong khi bảo toàn hiệu suất trong các nhiệm vụ và phương thức khác nhau. Chúng tôi phát hành mã nguồn của mình.

1 Giới thiệu
Tinh chỉnh các mô hình được huấn luyện trước quy mô lớn đã trở thành một kỹ thuật được áp dụng rộng rãi trong các nhiệm vụ như mô hình hóa ngôn ngữ, nhận dạng giọng nói và tạo hình ảnh. Nó đã được chứng minh là hiệu quả trong việc đạt được hiệu suất tối ưu trên các nhiệm vụ đa dạng. Tuy nhiên, khi số lượng tham số mô hình tiếp tục tăng theo cấp số nhân, đạt tới hàng tỷ, việc lưu trữ các tham số tinh chỉnh đầy đủ cho mỗi nhiệm vụ cụ thể trở nên không thực tế và không hiệu quả. Để giải quyết vấn đề này, các cách tiếp cận Tinh chỉnh Hiệu quả Tham số (PEFT) đã được giới thiệu, nhằm chỉ điều chỉnh một tập con của các tham số gốc và bảo toàn các tham số được điều chỉnh cho các nhiệm vụ riêng lẻ.

Trong khi các phương pháp PEFT hiệu quả và cho thấy kết quả đầy hứa hẹn có tính cạnh tranh với các mô hình tinh chỉnh đầy đủ, chúng thường dựa vào các mô-đun phức tạp được thiết kế thủ công hoặc các siêu tham số có chiều cố định cho mỗi lớp mô hình. Ví dụ, các phương pháp như Compacter, LoRA, và prefix-tuning giới thiệu thêm sự phức tạp bằng cách yêu cầu các hạng cụ thể hoặc tiền tố token, và việc đạt được hiệu suất tối ưu với các phương pháp này thường đòi hỏi tìm kiếm siêu tham số mở rộng. Nghiên cứu gần đây cũng đã chỉ ra rằng ngay cả với việc kết hợp triệt để các tiên nghiệm cụ thể về mô hình/nhiệm vụ/tài nguyên mở rộng, PEFT gặp khó khăn trong việc vượt trội hơn tinh chỉnh đầy đủ trong các tình huống tài nguyên thấp đến trung bình dưới ngân sách tính toán. Nhược điểm của PEFT trở nên rõ ràng hơn khi các phương pháp này không tương thích với các kỹ thuật hiện có được thực hiện trên tinh chỉnh trọng số đầy đủ.

Vì những lý do này, người ta có thể kết luận sử dụng tinh chỉnh đầy đủ cho tinh chỉnh hạ lưu của họ. Nếu vậy, liệu chúng ta vẫn có thể giải quyết vấn đề dấu chân bộ nhớ không hiệu quả trong thiết lập tinh chỉnh đầy đủ? Để giải quyết vấn đề này, chúng tôi đề xuất một cách tiếp cận khác trong bài báo này tập trung vào phần dư trọng số: sự khác biệt giữa các mô hình tinh chỉnh và các đối tác cơ sở của chúng. Đáng chú ý, chúng tôi đã quan sát thấy rằng phần dư trọng số của các mô hình tham số hóa quá mức, đã được thừa nhận về bản chất hạng thấp của chúng, thể hiện các đặc tính hạng thấp rõ rệt hơn nữa. Hơn nữa, các tham số phần dư riêng lẻ này thể hiện tính bền vững, cho thấy rằng sự đóng góp của chúng vào sự dịch chuyển dữ liệu hạ lưu là rất dư thừa.

Dựa trên các phát hiện của chúng tôi, chúng tôi đề xuất Mã hóa Phần dư Hiệu quả (ERE), bao gồm việc tinh chỉnh đầy đủ mô hình và lưu trữ hiệu quả phần dư trọng số dưới định dạng hạng thấp. Chúng tôi cũng sử dụng lượng tử hóa vector theo hạng, sử dụng xấp xỉ hạng thấp dựa trên độ dịch chuyển trọng số của các lớp riêng lẻ, được đo thông qua phân phối phổ xấp xỉ của chúng. Kết quả của phương pháp chúng tôi được thể hiện trong Hình 1.

Chúng tôi đã tiến hành các thí nghiệm trên nhiều nhiệm vụ, bao gồm Hiểu Ngôn ngữ Tự nhiên (NLU), Mô hình hóa Ngôn ngữ (LM), và tạo hình ảnh, để đánh giá hiệu quả của phương pháp được đề xuất. Trong nhiệm vụ NLU, chúng tôi đã chứng minh rằng phần dư trọng số có thể được nén lên đến 6,0% của trọng số tinh chỉnh đầy đủ trong khi đạt được hiệu suất tương đương. Cụ thể, chúng tôi đạt được độ chính xác trung bình 89,2 trên tiêu chuẩn Đánh giá Hiểu Ngôn ngữ Tổng quát (GLUE), rất gần với hiệu suất của trọng số gốc, 89,4. Chúng tôi cũng chứng minh rằng ERE có thể thành công trong việc bảo toàn chất lượng tạo sinh gốc với lưu trữ hiệu quả đáng kể trong tạo hình ảnh và các nhiệm vụ LM. Chúng tôi tóm tắt các đóng góp của mình dưới đây.

• Chúng tôi quan sát thấy rằng sự khác biệt trong các mô hình tinh chỉnh, đặc biệt là những mô hình có thay đổi tham số nhỏ, có thể được xấp xỉ với bộ nhớ ít hơn đáng kể so với mô hình cơ sở.
• Chúng tôi tiếp tục quan sát thấy rằng các lớp khác nhau yêu cầu ngân sách xấp xỉ khác nhau. Vì mục đích này, chúng tôi đề xuất ERE, một cách tiếp cận mới cho phép phân bổ tài nguyên phân biệt giữa các lớp để có được lưu trữ tối ưu.
• Vì phương pháp của chúng tôi không phụ thuộc vào mô hình, chúng tôi chứng minh hiệu quả của ERE trên các nhiệm vụ và phương thức khác nhau. Phương pháp của chúng tôi đạt được lưu trữ hiệu quả trong khi duy trì hiệu suất.

2 Các Công trình Liên quan

2.1 Học Chuyển giao
Học chuyển giao bao gồm tinh chỉnh một mô hình được huấn luyện trước trên một nhiệm vụ hạ lưu cụ thể sử dụng một tập dữ liệu được gán nhãn nhỏ hơn. Cách tiếp cận này tận dụng kiến thức thu được trong quá trình huấn luyện trước, cho phép mô hình nhanh chóng thích ứng với nhiệm vụ mới với yêu cầu dữ liệu và tính toán giảm. Học chuyển giao đã được chứng minh là rất hiệu quả trong việc nâng cao hiệu suất của các mô hình ngôn ngữ trên các nhiệm vụ NLU khác nhau. Hơn nữa, đã có những nỗ lực đáng kể để tùy chỉnh các mô hình tạo sinh, chẳng hạn như GANs và các mô hình dựa trên khuếch tán.

2.2 Tinh chỉnh Hiệu quả Tham số
Các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) đã nổi lên như một giải pháp thay thế để đạt được học chuyển giao hiệu quả, vì tinh chỉnh đầy đủ trọng số mô hình được huấn luyện trước có thể tốn kém về mặt tính toán. LoRA là một phương pháp PEFT nổi bật sử dụng lớp thắt cổ chai tương tự như adapter. Nó yêu cầu một hạng xác định trước r với tiên nghiệm α để huấn luyện, điều này xác định quy mô cập nhật tham số hóa lại. Hạng cố định r được áp dụng đồng nhất cho tất cả các lớp, giả định yêu cầu ngân sách bằng nhau cho mỗi lớp. Các công trình gần đây như AdaLoRA và DyLoRA đã giải quyết hạn chế này bằng cách phân bổ ngân sách động dựa trên tầm quan trọng của các lớp riêng lẻ. Mặc dù chúng đầy hứa hẹn, chúng giới thiệu những khó khăn triển khai thậm chí còn khó hơn.

Ngược lại, phương pháp được đề xuất của chúng tôi không được tích hợp vào quy trình huấn luyện. Do đó, khi mô hình được huấn luyện, chúng tôi có sự linh hoạt để chọn cách nén phần dư trọng số dựa trên các yếu tố ERE.

3 Phân tích về Phần dư Trọng số

3.1 Động lực Hạng Hiệu quả của Δθ

Hãy ký hiệu trọng số mô hình được huấn luyện trước là θ, và mô hình tinh chỉnh là θ′. Chúng tôi tiến hành phân tích chi tiết về động lực huấn luyện của trọng số phần dư, ký hiệu là Δθ = θ′ − θ trong quá trình tinh chỉnh mô hình RoBERTa-Large trên các nhiệm vụ NLU khác nhau. Chúng tôi điều tra hạng hiệu quả của phần dư trọng số và so sánh chúng với các mạng được khởi tạo ngẫu nhiên. Phù hợp với các tài liệu trước đây, chúng tôi phát hiện rằng các mô hình tham số hóa quá mức lớn, bao gồm các mô hình RoBERTa được huấn luyện trước, thể hiện cấu trúc hạng thấp khi huấn luyện tiến triển.

Thú vị, chúng tôi quan sát thấy rằng phần dư trọng số sở hữu hạng hiệu quả thấp hơn đáng kể. Điều này được chứng minh trong Hình 2, nơi chúng tôi vẽ biểu đồ độ lớn của các giá trị kỳ dị được chuẩn hóa bởi giá trị kỳ dị tối đa theo thời gian. Phân phối giá trị riêng nhanh chóng hội tụ về chế độ hạng thấp, cho thấy hạng hiệu quả nhỏ hơn nhiều so với các tham số cơ sở.

Hơn nữa, chúng tôi phát hiện rằng trong cùng các lớp, hạng hiệu quả của phần dư trọng số là không thể dự đoán và động, như được minh họa trong Hình 3. Phát hiện này giúp giải thích nhu cầu tìm kiếm siêu tham số mở rộng và hiệu suất không nhất quán của PEFT trên các nhiệm vụ, như đã được chứng minh trong các công trình trước đây.

Mặc dù hành vi tương tự có thể đã được giả thuyết dựa trên các nghiên cứu trước đây, theo hiểu biết tốt nhất của chúng tôi, nghiên cứu của chúng tôi là nghiên cứu đầu tiên quan sát một cách có hệ thống hành vi này.

3.2 Tính bền vững của Δθ
Chúng tôi quan sát thấy rằng phần dư trọng số tinh chỉnh ít nhạy cảm hơn đáng kể với nhiễu ngẫu nhiên so với trọng số được huấn luyện trước. Bằng cách giới thiệu nhiễu log-normal vào mỗi tham số, chúng tôi đánh giá tác động của nhiễu lên biểu diễn đầu ra của mô hình. Chúng tôi gây nhiễu cho trọng số hoặc phần dư trọng số bằng cách sử dụng θ̃′ = (θ + Δθ) exp(Z) hoặc θ̃′ = θ + exp(Z)Δθ, trong đó Z ~ N(0, σ²) và đo sự thay đổi trong biểu diễn bằng khoảng cách góc. Từ Hình 4a, người ta có thể thấy rằng nhiễu gây nhiễu trên phần dư không làm gì hoặc rất ít độ lệch biểu diễn thực tế ở mức độ nhất định.

Dựa trên các phát hiện trước đây về tính hạng thấp của phần dư trọng số, chúng tôi kết luận rằng phần dư trọng số dư thừa đáng kể đối với tác động tối thiểu của chúng. Để minh họa điều này, chúng tôi thực hiện xấp xỉ hạng thấp trên cả trọng số đầy đủ và phần dư trọng số, so sánh sự khác biệt kết quả trong biểu diễn đầu ra bằng độ tương tự cosine. Hình 4b làm nổi bật sự tương phản rõ rệt giữa các tham số cơ sở, thể hiện độ lệch đáng kể khi giảm 20% hạng của chúng, và phần dư trọng số, không gây ra sự khác biệt mạnh mẽ như vậy.

4 ERE: Mã hóa Phần dư Hiệu quả

Dựa trên các quan sát của chúng tôi, chúng tôi giới thiệu Mã hóa Phần dư Hiệu quả (ERE) như một phương pháp nén phần dư trọng số thay vì trọng số tinh chỉnh đầy đủ. Bằng cách tận dụng các kỹ thuật xấp xỉ hạng thấp, được hướng dẫn bởi các yêu cầu của các lớp riêng lẻ (như đã quan sát trong §3.1), chúng tôi đạt được lưu trữ thậm chí hiệu quả hơn của phần dư trọng số.

4.1 Xấp xỉ Hạng thấp và Phân bổ Ngân sách
Trước tiên chúng tôi xấp xỉ Δθ bằng tích của ba ma trận, U, D, và V, trong đó U và V là các ma trận Stiefel hạng thấp, và D là ma trận đường chéo. Mặc dù giải pháp này có thể được thu được thông qua Phân tích Giá trị Kỳ dị, một số lớp yêu cầu hạng ít hơn đáng kể để xấp xỉ chính xác Δθ. Chúng tôi có thể sử dụng Phân phối Phổ Thực nghiệm (ESD) riêng lẻ để xác định lượng hạng phù hợp cho Δθ sau tinh chỉnh. Với ngân sách tham số cố định M, mục tiêu của chúng tôi là phân phối hạng theo cách giảm thiểu chuẩn Frobenius trung bình cho giá trị. Gọi xấp xỉ hạng k tốt nhất có thể của phần dư trọng số w = Δθ là wₖ = Σᵢ₌₁ᵏ σᵢuᵢvᵢᵀ. Lỗi Frobenius bởi xấp xỉ như vậy là

|w - wₖ|_F = |Σᵢ₌ₖ₊₁ⁿ σᵢuᵢvᵢᵀ|_F = √(Σᵢ₌ₖ₊₁ⁿ σᵢ²)

trong đó w là ma trận m×n với m ≤ n. Mục tiêu của chúng tôi là giảm thiểu tổng lỗi xấp xỉ hạng thấp của mọi lớp. Vì hạng r của mỗi lớp đi kèm với ngân sách tham số (n+m)r, mục tiêu của chúng tôi có thể được biểu thị như sau:

min_r Σᵢ₌₁ᴺ Σₗ₌ᵣᵢ₊₁^min(nᵢ,mᵢ) σₗ² (Tổng Lỗi Bình phương)
chịu ràng buộc Σᵢ₌₁ᴺ rᵢ(nᵢ+mᵢ) ≤ M (Ngân sách)

Ở đây, nᵢ×mᵢ là các hình dạng của ma trận trọng số của lớp i, và N là số lượng tất cả các lớp có thể tinh chỉnh cho mô hình. Lưu ý rằng công thức này là một trường hợp đặc biệt của bài toán knapsack nổi tiếng và do đó là NP-Complete. Do đó để tìm giải pháp xấp xỉ, chúng tôi xấp xỉ fᵢ(r) = Σₗ₌ᵣ₊₁^min(nᵢ,mᵢ) σₗ² log-tuyến tính dựa trên ESD của lớp i, áp dụng thư giãn liên tục trên các biến hạng, và giải quyết bài toán đã thư giãn trong công thức Lagrangian và tìm kiếm nhị phân. Xem §A để biết giải thích chi tiết về thuật toán của chúng tôi.

4.2 Tối ưu hóa Thêm
Mặc dù ý tưởng chung của chúng tôi được mô tả ở trên, chúng tôi có thể triển khai một số sửa đổi để nâng cao hiệu suất hơn nữa. Đầu tiên, để giải quyết khả năng đánh giá quá mức của một tác động của hạng so với tác động khác, chúng tôi kết hợp hạng tiên nghiệm. Điều này bao gồm việc đồng nhất hóa giải pháp hạng đã thư giãn r ∈ ℝᴺ bằng cách lấy r'ᵢ = (1-α)rᵢ + αrₐᵥg, trong đó rₐᵥg được định nghĩa là Σᵢ₌₁ᴺ rₐᵥg(nᵢ+mᵢ) = M. Lưu ý rằng việc đồng nhất hóa này không vi phạm các ràng buộc ngân sách. Chúng tôi đã xác định thực nghiệm α = 0.5 hoạt động tốt. Phương pháp heuristic này cần thiết vì công thức của chúng tôi, nhằm giảm tổng chuẩn Frobenius, không được căn chỉnh hoàn hảo với mục tiêu cuối cùng là tái tạo biểu diễn đặc trưng cuối cùng. Một mục tiêu tinh vi hơn vẫn là hướng nghiên cứu tiềm năng trong tương lai.

Thứ hai, chúng tôi quan sát thấy rằng lượng tử hóa tích cực có thể được áp dụng mà không mất hiệu suất đáng kể, phù hợp với các phát hiện trong các nghiên cứu khác. Theo cách tiếp cận của họ, chúng tôi sử dụng lượng tử hóa làm tròn gần nhất cho U và V, trong khi dành độ chính xác nửa cho D. Tuy nhiên, chúng tôi gặp phải một vấn đề tiềm ẩn khi giải lượng tử hóa U và V, vì độ chính xác của chúng có thể thấp hơn đáng kể so với D, vi phạm giả định của chúng tôi rằng U và V là các phần tử Stiefel. Để giải quyết điều này, chúng tôi thuận tiện chiếu U và V đã giải lượng tử hóa trở lại lên đa tạp Stiefel gần nhất. Các chi tiết thêm và kết quả ablation có thể được tìm thấy trong §B.

5 Thí nghiệm

5.1 ERE trên Hiểu Ngôn ngữ Tự nhiên
Để đánh giá hiệu quả của phương pháp ERE của chúng tôi, trước tiên chúng tôi tiến hành đánh giá trên tiêu chuẩn NLU chuẩn, GLUE. Chúng tôi tinh chỉnh mô hình RoBERTa-Large được huấn luyện trước được cung cấp trên từng nhiệm vụ riêng lẻ, sử dụng thiết lập tinh chỉnh mô hình đơn và nhiệm vụ đơn. Chúng tôi huấn luyện và đánh giá mỗi mô hình với framework Fairseq. Chúng tôi sử dụng các cấu hình huấn luyện chính thức để tái tạo hiệu suất của mô hình tinh chỉnh đầy đủ và áp dụng ERE trên phần dư trọng số giữa các mô hình được huấn luyện trước và tinh chỉnh. Chúng tôi tiến hành đánh giá sử dụng các thiết lập hạng khác nhau cho ERE và báo cáo cả độ chính xác tổng thể và các yêu cầu lưu trữ tương ứng. Lưu ý rằng, chúng tôi lưu trọng số mô hình với độ chính xác đầy đủ ngoại trừ các bộ phân loại mới được thêm vào vì chúng không có đối tác để nén phần dư. Chúng tôi cũng báo cáo kết quả khi các kỹ thuật Lượng tử hóa BIT (BITQ) và Xấp xỉ Hạng Thấp (LRA) được áp dụng độc lập. Đối với tất cả các thí nghiệm áp dụng ERE, r biểu thị hạng tiên nghiệm, và b có nghĩa là mức lượng tử hóa bit. Lưu ý rằng, kỹ thuật lượng tử hóa bit được sử dụng trong BITQ khác với ERE. Trong ERE, chỉ các thành phần U và V của phần dư được phân tích hạng thấp được lượng tử hóa như đã đề cập §4.2, trong khi BITQ lượng tử hóa tất cả các phần tử của phần dư trọng số.

Bảng 1 minh họa rằng ERE (r=256, b=4) dẫn đến một mô hình giảm đáng kể, chỉ yêu cầu 6,0% không gian lưu trữ gốc, với tác động tối thiểu đến suy giảm hiệu suất. Kết quả cho thấy rằng nén phần dư có thể đạt được hiệu quả ngay cả khi chỉ sử dụng các kỹ thuật BITQ hoặc LRA. Tuy nhiên, điều quan trọng cần lưu ý là các phương pháp này yêu cầu lưu trữ lớn hơn so với ERE. Tuy nhiên, bất chấp những ưu điểm của phương pháp chúng tôi trong lưu trữ hiệu quả trọng số mô hình, tồn tại khả năng thất bại trong việc tái tạo chính xác các biểu diễn của mô hình gốc khi nén phần dư với hạng và bit cực thấp, đặc biệt trong các nhiệm vụ như MNLI và QQP. Chúng tôi giả thuyết rằng điều này là do các nhiệm vụ này thực hiện cập nhật trọng số lớn hơn nhiều so với các nhiệm vụ khác, trong đó MNLI và QQP mất khoảng 37 lần thời gian huấn luyện trên máy GPU V100 đơn, trong khi duy trì cùng kích thước batch. Một khả năng khác có thể là tinh chỉnh RoBERTa với các nhiệm vụ NLU yêu cầu một bộ phân loại được khởi tạo mới không giống như các nhiệm vụ khác như tạo text-to-image và mô hình hóa ngôn ngữ. Hơn nữa, đáng chú ý là việc áp dụng ERE mang lại hiệu suất được cải thiện trong một số nhiệm vụ (MNLI, CoLA, và RTE). Cải thiện này có thể được quy cho việc loại bỏ các yếu tố dư thừa hoặc làm suy giảm hiệu suất thông qua mã hóa phần dư trọng số với ERE.

5.2 ERE trên Tạo Hình ảnh
Phần này trình bày các thí nghiệm của chúng tôi về nhiệm vụ tạo hình ảnh sử dụng ERE. Chúng tôi sử dụng các Mô hình Khuếch tán Tiềm ẩn (LDMs) và StyleGANs được huấn luyện trước và tinh chỉnh đầy đủ có sẵn công khai. Đối với các mô hình dựa trên khuếch tán, chúng tôi áp dụng ERE trên các mô hình tinh chỉnh có sẵn công khai như Disney diffusion, GTA5 diffusion, Ghibli diffusion. Đối với các mô hình dựa trên khuếch tán, chúng tôi cố định seed với 0 và 50 bước suy luận và thang đo hướng dẫn 7.5 để lấy mẫu. Chúng tôi lấy mẫu ngẫu nhiên 40 hình ảnh từ một tập prompt cố định, và đo khoảng cách Học Độ tương tự Vá Hình ảnh Cảm nhận (LPIPS) trên các hình ảnh được tạo ra để đo sự tái tạo chính xác của các mô hình tinh chỉnh. Tương tự đối với các mô hình StyleGAN, chúng tôi thực hiện thích ứng được điều khiển bởi CLIP thông qua phương pháp NADA và nén các mô hình được thích ứng với ERE. Chúng tôi cũng lấy mẫu 40 hình ảnh với seed cố định 0 và đo khoảng cách LPIPS của chúng. Kết quả, như được hiển thị trong Hình 5 và Bảng 2, chứng minh rằng ERE (r=4, b=4) có thể thành công bảo toàn hầu hết thông tin ngữ nghĩa. Hơn nữa, việc tăng số lượng hạng dẫn đến chất lượng cảm nhận được cải thiện.

5.3 ERE trên Mô hình hóa Ngôn ngữ
Trong phần này, chúng tôi trình bày đánh giá ERE trên Mô hình Ngôn ngữ Lớn (LLM), Dolly. Dolly là một mô hình tinh chỉnh đầy đủ dựa trên Pythia, sử dụng tập dữ liệu bao gồm khoảng 15k hướng dẫn. Chúng tôi đánh giá hai mô hình với 3 tỷ (3B) và 7 tỷ (7B) tham số với 8 nhiệm vụ đánh giá zero-shot sử dụng framework đánh giá LM mã nguồn mở.

Bảng 3 trình bày kết quả áp dụng ERE cho mô hình LLM tinh chỉnh đầy đủ. Nó cho thấy rằng mô hình với ERE vượt trội hơn mô hình gốc trong các nhiệm vụ đánh giá zero-shot. Tuy nhiên, có một xu hướng suy giảm hiệu suất đáng chú ý vượt quá một hạng nhất định. Cụ thể, đối với mô hình 7B, hiệu suất bắt đầu suy giảm sau hạng 4, trong khi đối với mô hình 3B, nó bắt đầu suy giảm sau hạng 8. Chúng tôi giả thuyết điều này là do mục đích ban đầu của dolly là tuân theo các prompt hướng dẫn, thay vì đạt được hiệu suất zero-shot là khả năng vốn có của baseline được huấn luyện trước. Do đó, chúng tôi diễn giải hiện tượng này là ERE cho phép khôi phục khả năng tổng quát hóa của mô hình trong khi bảo toàn khả năng hiểu prompt hướng dẫn. Hỗ trợ diễn giải này, Hình 6 chứng minh rằng mô hình với ERE giữ lại khả năng hiểu prompt hướng dẫn, trong khi baseline được huấn luyện trước thì không.

6 Thảo luận và Hạn chế

6.1 Tại sao chúng ta cần ERE khi chúng ta có PEFT?
Trong phần này, chúng tôi nhấn mạnh rằng mặc dù PEFT đã cho thấy hiệu suất ấn tượng, có những tình huống mà việc quay trở lại tinh chỉnh đầy đủ vẫn cần thiết, làm cho ERE trở thành một giải pháp thay thế có giá trị.

Thứ nhất, việc tích hợp các phương pháp PEFT khác làm gián đoạn đồ thị tính toán tại thời điểm suy luận có thể thách thức và tốn kém. Nhiều phương pháp PEFT sửa đổi luồng tính toán các biến và được thiết kế đặc biệt cho một kiến trúc mô hình duy nhất. Ví dụ, việc tích hợp prefix tuning với Paint-with-words hoặc Adapters với ControlNet đặt ra những khó khăn đáng kể do sự không tương thích kiến trúc của chúng. Ngược lại, tinh chỉnh đầy đủ không giới thiệu bất kỳ sự phức tạp bổ sung nào về mặt tương thích.

Thứ hai, bằng cách chỉ dựa vào PEFT, người dùng có thể bỏ qua nghiên cứu mở rộng được tiến hành dưới giả định tinh chỉnh trọng số đầy đủ. Nghiên cứu này bao gồm các lĩnh vực chuyên biệt, chẳng hạn như học tập liên tục tập trung tiên nghiệm, và các phương pháp trung bình trọng số.

6.2 So sánh giữa ERE và LoRA
Chúng tôi thực hiện phân tích so sánh giữa ERE và LoRA, nhằm mô tả điểm mạnh và điểm yếu của cả hai phương pháp. Cả hai phương pháp đều có đặc điểm chung trong việc tận dụng bản chất hạng thấp của phần dư trọng số (Δθ). Để so sánh công bằng, chúng tôi triển khai LoRA trong Fairseq sử dụng triển khai chính thức. Chúng tôi đầu tiên tinh chỉnh đầy đủ mô hình cho mỗi nhiệm vụ GLUE và tính tổng thời gian huấn luyện. Sau đó chúng tôi huấn luyện mô hình RoBERTa-Large được tiêm LoRA trong cùng ngân sách thời gian huấn luyện. Chúng tôi tuân theo hầu hết các siêu tham số trong bài báo LoRA, và khởi tạo các tham số bộ phân loại với cùng seed và sử dụng cùng GPU (V100), framework (Fairseq), và phiên bản PyTorch (2.0).

Trong Bảng 4, chúng tôi quan sát thấy rằng độ chính xác tổng thể của LoRA tương đương với mô hình tinh chỉnh đầy đủ. Tuy nhiên, cả ERE và mô hình tinh chỉnh đầy đủ đều đạt được hiệu suất tốt hơn. Mặt khác, LoRA yêu cầu lưu trữ nhỏ hơn so với ERE để đạt được hiệu suất cạnh tranh. Tuy nhiên, điều quan trọng cần lưu ý là việc so sánh trực tiếp với ERE và LoRA có thể thách thức vì phương pháp tinh chỉnh đầy đủ cập nhật nhiều lớp hơn bao gồm lớp embedding từ và bias. Trọng số mô hình tinh chỉnh đầy đủ có thể được lưu trữ hiệu quả với ERE, yêu cầu 6,0% lưu trữ gốc trong khi bảo toàn hiệu suất tương tự và 1,7% cho cùng hiệu suất so với LoRA.

Kết quả là, tùy thuộc vào nhiệm vụ cụ thể và yêu cầu, người ta có thể chọn LoRA do hiệu suất cạnh tranh và yêu cầu lưu trữ nhỏ hơn. Tuy nhiên, trong một số tình huống mà việc tối đa hóa hiệu suất là quan trọng, cách tiếp cận tinh chỉnh đầy đủ và ERE có thể được ưa thích. Hơn nữa, đáng chú ý là ERE có thể được áp dụng cho trọng số thu được thông qua LoRA, cung cấp tính linh hoạt và tùy chọn bổ sung để đạt được sự cân bằng tối ưu giữa hiệu suất và hiệu quả lưu trữ. Điều này làm nổi bật tính linh hoạt và lợi ích tiềm năng của ERE kết hợp với các phương pháp PEFT.

6.3 Hạn chế
Các hạn chế của bài báo chúng tôi bao gồm những thách thức tiềm ẩn của ERE trong các tình huống nén cực đoan, đặc biệt khi kích thước mô hình không đủ lớn. Hạn chế này trở nên rõ rệt hơn với số lượng cập nhật cao hoặc tốc độ học cao tương đối với kích thước mô hình. Ngoài ra, mặc dù chúng tôi chứng minh hiệu quả của phương pháp trên các nhiệm vụ và kiến trúc mô hình khác nhau, chúng tôi chưa thiết lập khả năng áp dụng của nó trong các cấu hình tinh chỉnh đa dạng. Chúng tôi thừa nhận nhu cầu điều tra thêm trong các thiết lập này như một hướng nghiên cứu tương lai.

7 Kết luận
Bài báo này cung cấp phân tích kỹ lưỡng về phần dư trọng số và giới thiệu ERE như một cách tiếp cận hiệu quả để lưu trữ hiệu quả mà không hy sinh hiệu suất. Các thí nghiệm của chúng tôi chứng minh hiệu quả của ERE trên các nhiệm vụ như NLU, LM, và tạo hình ảnh. Chúng tôi tin rằng ERE cung cấp một giải pháp thay thế hấp dẫn cho PEFT về việc giảm dấu chân, do tính linh hoạt, đơn giản, và hiệu suất cạnh tranh của chúng.
