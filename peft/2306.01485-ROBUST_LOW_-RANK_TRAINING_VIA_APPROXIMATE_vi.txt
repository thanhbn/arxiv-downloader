# 2306.01485.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2306.01485.pdf
# Kích thước tệp: 1009722 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
HUẤN LUYỆN CẤP THẤP ROBUSTNESS THÔNG QUA CÁC RÀNG BUỘC TRỰC GIAO GẦN ĐÚNG
Dayana Savostianova
Gran Sasso Science Institute
67100 L'Aquila (Italy)
dayana.savostianova@gssi.itEmanuele Zangrando
Gran Sasso Science Institute
67100 L'Aquila (Italy)
emanuele.zangrando@gssi.it
Gianluca Ceruti
EPF Lausanne
1015 Lausanne (Switzerland)
gianluca.ceruti@epfl.chFrancesco Tudisco
Gran Sasso Science Institute
67100 L'Aquila (Italy)
francesco.tudisco@gssi.it

TÓM TẮT
Với sự tăng trưởng của kích thước mô hình và dữ liệu, đã có những nỗ lực rộng rãi nhằm thiết kế các kỹ thuật cắt tỉa để giảm nhu cầu tài nguyên của các pipeline học sâu, trong khi vẫn duy trì hiệu suất mô hình. Để giảm cả chi phí suy luận và huấn luyện, một hướng nghiên cứu nổi bật sử dụng phân tích nhân tử ma trận cấp thấp để biểu diễn các trọng số mạng. Mặc dù có thể duy trì độ chính xác, chúng tôi quan sát thấy rằng các phương pháp cấp thấp có xu hướng làm tổn hại độ robustness của mô hình chống lại các nhiễu loạn đối kháng. Bằng cách mô hình hóa robustness theo số điều kiện của mạng thần kinh, chúng tôi lập luận rằng việc mất robustness này là do các giá trị kỳ dị bùng nổ của các ma trận trọng số cấp thấp. Do đó, chúng tôi giới thiệu một thuật toán huấn luyện cấp thấp robust duy trì các trọng số của mạng trên đa tạp ma trận cấp thấp trong khi đồng thời áp dụng các ràng buộc trực giao gần đúng. Mô hình kết quả giảm cả chi phí huấn luyện và suy luận trong khi đảm bảo điều kiện tốt và do đó có robustness đối kháng tốt hơn, mà không làm tổn hại đến độ chính xác của mô hình. Điều này được chứng minh bằng bằng chứng số rộng rãi và bởi định lý xấp xỉ chính của chúng tôi cho thấy mạng cấp thấp robust được tính toán xấp xỉ tốt mô hình đầy đủ lý tưởng, với điều kiện tồn tại một mạng con cấp thấp có hiệu suất cao.

1 Giới thiệu
Học sâu và mạng thần kinh đã đạt được thành công lớn trong nhiều ứng dụng trong thị giác máy tính, xử lý tín hiệu và tính toán khoa học, để kể một vài. Tuy nhiên, độ robustness của chúng đối với các nhiễu loạn của dữ liệu đầu vào có thể ảnh hưởng đáng kể đến an ninh và độ tin cậy và tạo ra một nhược điểm lớn cho việc ứng dụng thực tế của chúng. Hơn nữa, các yêu cầu về bộ nhớ và tính toán cho cả giai đoạn huấn luyện và suy luận khiến chúng trở nên không thực tế trong các môi trường ứng dụng có tài nguyên hạn chế. Trong khi đã có một nghiên cứu rộng rãi về các phương pháp cắt tỉa và robustness đối kháng để giải quyết hai vấn đề này một cách riêng lẻ, thì ít công việc hơn đã được thực hiện để thiết kế các mạng thần kinh vừa tiết kiệm năng lượng vừa robust. Thực tế, trong nhiều phương pháp, hai vấn đề này dường như cạnh tranh với nhau vì hầu hết các kỹ thuật cải thiện robustness đối kháng đều yêu cầu mạng lớn hơn [43,73,32,37,42] hoặc các hàm mất mát đòi hỏi tính toán nhiều hơn, và do đó các giai đoạn huấn luyện tốn kém hơn [12,21,40,62].
Công việc hạn chế có sẵn cho đến nay về các mạng được cắt tỉa robust chủ yếu tập trung vào việc giảm chi phí bộ nhớ và tính toán của giai đoạn suy luận, trong khi duy trì robustness đối kháng [52,26,71,38,18].
Tuy nhiên, giai đoạn suy luận chỉ chiếm một phần rất hạn chế trong chi phí của toàn bộ pipeline học sâu, thay vào đó chủ yếu bị chi phối bởi giai đoạn huấn luyện. Giảm cả chi phí suy luận arXiv:2306.01485v1 [cs.LG] 2 Jun 2023

--- TRANG 2 ---
và huấn luyện là một mục tiêu đầy thử thách nhưng đáng mong muốn, đặc biệt là trong tầm nhìn về một AI dễ tiếp cận hơn và việc sử dụng hiệu quả trên các thiết bị có tài nguyên hạn chế và kết nối hạn chế như máy bay không người lái hoặc vệ tinh.

Một số kỹ thuật hiệu quả nhất để giảm chi phí huấn luyện cho đến nay đã dựa trên tham số hóa trọng số cấp thấp [51,67,27]. Các phương pháp này khai thác cấu trúc cấp thấp nội tại của các ma trận tham số và các ma trận dữ liệu lớn nói chung [55,65,45,16]. Do đó, giả định một cấu trúc cấp thấp cho các trọng số của mạng thần kinh W=USV⊤, các quy trình huấn luyện kết quả chỉ sử dụng các yếu tố riêng lẻ nhỏ U, S, V. Điều này dẫn đến chi phí huấn luyện tỷ lệ tuyến tính với số neuron, trái ngược với việc tỷ lệ bậc hai yêu cầu bởi việc huấn luyện trọng số đầy đủ. Mặc dù giảm đáng kể các tham số huấn luyện, các phương pháp này đạt được độ chính xác tương đương với các mạng đầy đủ gốc.
Tuy nhiên, robustness của chúng đối với các nhiễu loạn đối kháng vẫn chưa được khám phá rộng rãi cho đến nay.

Trong bài báo này, chúng tôi quan sát thấy rằng robustness đối kháng của các mạng cấp thấp thực sự có thể xấu đi so với baseline đầy đủ. Bằng cách mô hình hóa robustness của mạng theo số điều kiện của mạng thần kinh, chúng tôi lập luận rằng việc mất robustness này là do số điều kiện bùng nổ của các ma trận trọng số cấp thấp, có các giá trị kỳ dị tăng rất lớn để phù hợp với độ chính xác baseline và để bù đắp cho việc thiếu tham số. Do đó, để giảm thiểu tính không ổn định ngày càng tăng này, chúng tôi thiết kế một thuật toán huấn luyện mạng chỉ sử dụng các yếu tố cấp thấp U, S, V trong khi đồng thời đảm bảo số điều kiện của mạng vẫn nhỏ. Để đạt được điều này, chúng tôi diễn giải bài toán tối ưu hóa mất mát như một dòng gradient thời gian liên tục và sử dụng các kỹ thuật từ lý thuyết tích phân hình học trên đa tạp [66,29,51,11] để dẫn xuất ba dòng gradient chiếu riêng biệt cho U, S, V, riêng lẻ, đảm bảo số điều kiện của mạng vẫn bị ràng buộc trong dung sai mong muốn 1 +τ, trong suốt các epoch.
Đối với một hằng số nhỏ cố định ε >0, điều này được thực hiện bằng cách ràng buộc các giá trị kỳ dị của các ma trận cấp nhỏ trong một dải hẹp [s−ε, s+ε] xung quanh một giá trị s, được chọn để xấp xỉ tốt nhất các giá trị kỳ dị gốc.

Chúng tôi cung cấp nhiều đánh giá thực nghiệm trên các kiến trúc và bộ dữ liệu khác nhau, nơi các mạng cấp thấp robust được so sánh với nhiều baseline khác nhau. Kết quả cho thấy rằng kỹ thuật được đề xuất cho phép chúng tôi tính toán từ đầu các trọng số cấp thấp với các giá trị kỳ dị bị ràng buộc, giảm đáng kể nhu cầu bộ nhớ và chi phí tính toán của huấn luyện trong khi đồng thời duy trì hoặc cải thiện cả độ chính xác và độ chính xác robust của mô hình gốc. Bên cạnh bằng chứng thực nghiệm, chúng tôi cung cấp một định lý xấp xỉ chính cho thấy rằng nếu tồn tại một mạng cấp thấp hiệu suất cao với các giá trị kỳ dị bị ràng buộc, thì thuật toán của chúng tôi tính toán nó với sai số xấp xỉ bậc nhất.

Bài báo này tập trung vào mạng thần kinh feedforward. Tuy nhiên, các kỹ thuật và phân tích của chúng tôi áp dụng một cách đơn giản cho các bộ lọc tích chập được định hình lại dưới dạng ma trận, như được thực hiện trong ví dụ [51,67,27]. Các cách khác tồn tại để thúc đẩy tính trực giao của các bộ lọc tích chập, ví dụ [56,61,72], mà chúng tôi không xem xét trong công việc này.

2 Công việc liên quan
Robustness của mạng thần kinh chống lại các nhiễu loạn đối kháng đã được nghiên cứu rộng rãi trong cộng đồng học máy. Đã được biết rằng robustness đối kháng của một mạng thần kinh có liên quan chặt chẽ đến tính liên tục Lipschitz của nó [58,12,64,21], xem thêm Phần 3. Theo đó, huấn luyện mạng thần kinh với hằng số Lipschitz bị ràng buộc là một chiến lược được sử dụng rộng rãi để giải quyết vấn đề. Nhiều công trình đã nghiên cứu các kiến trúc Lipschitz [64,61,33,56], và một số đảm bảo robustness được chứng nhận đã được đề xuất [21,57,47]. Trong khi việc chia tỷ lệ từng lớp để áp đặt ràng buộc 1-Lipschitz là một khả năng, phương pháp này có thể dẫn đến gradient biến mất và đã được biết rằng một cách hiệu quả hơn để giảm hằng số Lipschitz và tăng robustness được đạt được bằng cách thúc đẩy tính trực giao trên từng lớp [5,12]. Bên cạnh robustness, các hằng số Lipschitz nhỏ và các lớp trực giao được biết là dẫn đến cải thiện các giới hạn tổng quát hóa [10,41] và gradient có thể diễn giải hơn [63]. Tính trực giao cũng được chứng minh là cải thiện sự lan truyền tín hiệu trong các mạng (rất) sâu [69, 48].

Nhiều phương pháp khác nhau để tích hợp các ràng buộc trực giao trong mạng thần kinh sâu đã được phát triển qua các năm. Các phương pháp tiếp cận ví dụ đáng chú ý bao gồm các phương pháp dựa trên regularization và landing [1,12], tham số hóa rẻ của nhóm trực giao [35,6,34,44,46], các sơ đồ gradient descent Riemannian và chiếu [9, 3, 2].

--- TRANG 3 ---
Song song với việc phát triển các phương pháp để thúc đẩy tính trực giao, một hướng nghiên cứu tích cực đã phát triển để phát triển các chiến lược huấn luyện hiệu quả để áp dụng trọng số cấp thấp. Không giống như các chiến lược cắt tỉa thúc đẩy tính thưa thớt chủ yếu nhằm giảm các tham số cần thiết cho suy luận [20,8,19,39,28], các mô hình mạng thần kinh cấp thấp được thiết kế để huấn luyện trực tiếp trên đa tạp tham số thấp của các ma trận cấp thấp và đặc biệt hiệu quả để giảm số lượng tham số cần thiết bởi cả giai đoạn suy luận và huấn luyện. Tương tự như huấn luyện trực giao, các phương pháp cho huấn luyện cấp thấp bao gồm các phương pháp dựa trên regularization [24,27], cũng như các phương pháp dựa trên tham số hóa hiệu quả của đa tạp cấp thấp sử dụng SVD hoặc phân tích cực [67,70], và các mô hình huấn luyện dựa trên tối ưu hóa Riemannian [51, 53].

Bằng cách kết hợp huấn luyện cấp thấp với các ràng buộc trực giao gần đúng, trong công việc này chúng tôi đề xuất một chiến lược đồng thời áp dụng robustness trong khi chỉ yêu cầu một tỷ lệ giảm của các tham số mạng trong quá trình huấn luyện. Phương pháp dựa trên một công thức vi phân dòng gradient của bài toán huấn luyện, và việc sử dụng lý thuyết tích phân hình học để dẫn xuất các phương trình chi phối của các yếu tố cấp thấp. Với công thức này, chúng tôi có thể giảm độ nhạy cảm của mạng trong quá trình huấn luyện với chi phí gần như không, tạo ra các mạng thần kinh cấp thấp có điều kiện tốt. Các phát hiện thực nghiệm của chúng tôi được hỗ trợ bởi một định lý xấp xỉ cho thấy rằng, nếu mạng đầy đủ lý tưởng có thể được xấp xỉ bởi một mạng cấp thấp, thì phương pháp của chúng tôi tính toán một xấp xỉ tốt. Điều này phù hợp với công việc gần đây cho thấy sự tồn tại của các mạng cấp thấp hiệu suất cao trong ví dụ các mô hình tuyến tính sâu [45,16,7,23].
Hơn nữa, vì tính trực giao giúp huấn luyện các mạng thực sự sâu, các mô hình trực giao cấp thấp có thể được sử dụng để giảm thiểu tác động của độ sâu hiệu quả tăng lên khi huấn luyện các mạng cấp thấp [50].

3 Số điều kiện của một mạng thần kinh
Robustness đối kháng của một mô hình mạng thần kinh f có thể được đo bằng độ nhạy cảm trường hợp xấu nhất của f đối với các nhiễu loạn nhỏ của dữ liệu đầu vào x. Theo nghĩa tuyệt đối, điều này quy về việc đo hằng số Lipschitz toàn cục và cục bộ tốt nhất của f đối với các khoảng cách phù hợp, như được thảo luận trong nhiều bài báo [58,21,14,12]. Tuy nhiên, vì mô hình và dữ liệu có thể giả định các giá trị lớn tùy ý và nhỏ tùy ý nói chung, một thước đo tương đối của độ nhạy cảm của f có thể có nhiều thông tin hơn. Nói cách khác, nếu chúng ta giả định một nhiễu loạn δ có kích thước nhỏ so với x, chúng ta muốn định lượng sự thay đổi tương đối lớn nhất trong f(x+δ), so với f(x). Đây là một vấn đề điều kiện hóa đã được biết đến, như chúng tôi xem xét tiếp theo, và tự nhiên dẫn đến khái niệm số điều kiện của một mạng thần kinh.

Trong thiết lập tuyến tính, số điều kiện của một ma trận là một thước đo tương đối được chấp nhận rộng rãi của độ nhạy cảm trường hợp xấu nhất của các bài toán tuyến tính đối với nhiễu trong dữ liệu. Đối với một ma trận A và chuẩn toán tử ma trận ∥A∥= supx̸=0∥Ax∥/∥x∥, số điều kiện của A được định nghĩa là cond(A) =∥A∥∥A+∥, trong đó A+ biểu thị pseudo-inverse của A. Lưu ý rằng việc xác minh rằng cond(A)≥1 là ngay lập tức. Bây giờ, nếu ví dụ u và uε là các nghiệm của hệ phương trình tuyến tính Au=b, khi A và b là dữ liệu chính xác hoặc khi chúng bị nhiễu loạn với nhiễu δA,δb có chuẩn tương đối ∥δA∥/∥A∥ ≤ε và ∥δb∥/∥b∥ ≤ε, tương ứng, thì giới hạn sai số tương đối sau đây tồn tại

∥u−uε∥
∥u∥≲cond(A)ε .

Do đó, các nhiễu loạn nhỏ trong dữ liệu A, b dẫn đến các thay đổi nhỏ trong nghiệm nếu và chỉ nếu A có điều kiện tốt, tức là cond(A) gần bằng một.

Như trong trường hợp tuyến tính, có thể định nghĩa khái niệm số điều kiện cho các hàm tổng quát f, [22, 49]. Hãy bắt đầu bằng việc định nghĩa tỷ lệ sai số tương đối của một hàm f:Rd→Rm tại điểm x:

R(f, x;δ) =∥f(x+δ)−f(x)∥
∥f(x)∥.∥δ∥
∥x∥(1)

Để tính đến kịch bản trường hợp xấu nhất, số điều kiện cục bộ của f tại x được định nghĩa bằng cách lấy sup của (1) trên tất cả các nhiễu loạn có kích thước tương đối ε, tức là sao cho ∥δ∥ ≤ε∥x∥, trong giới hạn của ε nhỏ. Cụ thể,

cond(f;x) = lim
ε↓0sup
δ̸=0:∥δ∥≤ε∥x∥R(f, x;δ).

--- TRANG 4 ---
Đại lượng này là một thước đo cục bộ của điều kiện hóa "vô cùng nhỏ" của f xung quanh điểm x. Thực tế, một phép tính trực tiếp cho thấy rằng

∥f(x+δ)−f(x)∥
∥f(x)∥≲cond(f;x)ε , (2)

miễn là ∥δ∥ ≤ε∥x∥. Do đó, cond(f;x) cung cấp một dạng hằng số Lipschitz cục bộ tương đối cho f đặc biệt cho thấy rằng, nếu ∥δ∥/∥x∥ nhỏ hơn cond(f;x)−1, chúng ta mong đợi sự thay đổi hạn chế trong f khi x bị nhiễu loạn với δ. Một kết luận tương tự được đạt được bằng cách sử dụng hằng số Lipschitz cục bộ tuyệt đối trong ví dụ [21]. Tương tự như trường hợp tuyệt đối, một hằng số Lipschitz tương đối toàn cục có thể được đạt được bằng cách xem xét trường hợp xấu nhất trên x, thiết lập

cond(f) = sup
x∈Xcond(f;x).

Rõ ràng, cùng một giới hạn (2) tồn tại cho cond(f). Lưu ý rằng điều này thực sự tổng quát hóa trường hợp tuyến tính, vì khi f(x) =Ax chúng ta có cond(f) = cond(f, x) = cond(A).

Khi f là một mạng thần kinh, cond(f) là một hàm của các trọng số mạng và robustness có thể được áp dụng bằng cách giảm cond(f) trong khi huấn luyện. Thực tế, cond(f) là tương đương tương đối của hằng số Lipschitz của mạng và do đó các chứng chỉ robustness dựa trên Lipschitz tiêu chuẩn [36,64,21] có thể được diễn đạt lại theo cond(f). Tuy nhiên, đối với các hàm f tổng quát và các chuẩn ∥ · ∥ tổng quát, cond(f) có thể (rất) tốn kém để tính toán, có thể không khả vi, và cond(f)>1 có thể tồn tại [22]. May mắn thay, đối với mạng thần kinh feedforward, điều sau đây tồn tại (chứng minh và chi tiết bổ sung được chuyển đến Phụ lục B trong tài liệu bổ sung)

Mệnh đề 1. Cho X là không gian đặc trưng và cho f(x) =zL+1 là một mạng với L lớp tuyến tính
zi+1=σi(Wizi),i= 1, . . . , L . Khi đó,

cond(f) = sup
x∈X\{0}cond(f;x)≤LY
i=1sup
x∈X\{0}cond(σi;x) LY
i=1cond(Wi)
.

Đặc biệt, đối với X điển hình và các lựa chọn điển hình của σi, bao gồm σi∈ {leakyReLU ,sigmoid ,tanh ,
hardtanh ,softplus ,siLU}, chúng ta có

supx∈X\{0}cond(σi;x)≤C <+∞

cho một hằng số dương C >0 chỉ phụ thuộc vào hàm kích hoạt σi.

Lưu ý rằng đối với các phi tuyến theo từng phần tử σ, số điều kiện cond(σ;x) có thể được tính toán một cách đơn giản. Thực tế, khi σ là Lipschitz, vấn đề có thể được rút gọn thành một hàm một chiều, và nó xuất phát trực tiếp từ định nghĩa của nó rằng (xem thêm [60])

cond(f;x) = sup
νx∈∂σ(x)|νx||x||σ(x)|−1, x ∈R

trong đó ∂σ(x) biểu thị gradient tổng quát Clarke [13] của σ tại điểm x. Do đó, ví dụ, nếu σ là LeakyRelu với độ dốc α, chúng ta có cond(σ) = 1 ; nếu σ là sigmoid logistic (1 +e−x)−1 và không gian đặc trưng X chỉ chứa các điểm không âm, thì cond(σ)≤supx≥0|x|e−x(1 +e−x)−1≤1/e.

Từ Mệnh đề 1 chúng ta thấy rằng khi f là một mạng feedforward, để giảm số điều kiện của f thì đủ để giảm điều kiện hóa của tất cả các trọng số của nó. Khi ∥ · ∥ =∥ · ∥ 2 là chuẩn Euclidean L2, chúng ta có cond2(W) =smax(W)/smin(W), tỷ lệ giữa giá trị kỳ dị lớn nhất và nhỏ nhất của W. Điều này ngụ ý rằng các ma trận trọng số trực giao, ví dụ, được điều kiện hóa tối ưu đối với metric L2. Do đó, một hệ quả đáng chú ý và đã được biết đến của Mệnh đề 1 là việc áp đặt các ràng buộc trực giao trên Wi cải thiện robustness của mạng [12, 34, 25, 46].

Trong khi các ràng buộc trực giao được nghiên cứu rộng rãi trong tài liệu, các ma trận trực giao không phải là những ma trận duy nhất được điều kiện hóa tối ưu. Thực tế, cond2(W) = 1 cho bất kỳ W nào với các giá trị kỳ dị hằng số. Trong phần tiếp theo, chúng tôi sẽ sử dụng quan sát này để thiết kế một thuật toán cấp thấp và chi phí thấp huấn luyện các mạng có điều kiện tốt bằng cách đảm bảo cond2(W)≤1+τ, cho tất cả các lớp W và một dung sai mong muốn τ >0.

--- TRANG 5 ---
02000 4000 6000 8000
# bước tối ưu hóa101số điều kiện lớpBaseline
02000 4000 6000 8000
# bước tối ưu hóa100101102103số điều kiện lớpDòng gradient cấp thấp
02000 4000 6000 8000
# bước tối ưu hóa100101102103104105106107số điều kiện lớpDLRT
0 200 400 600
# bước tối ưu hóa101102103104105106107số điều kiện lớpPhân tích lớpHình 1: Sự tiến hóa của số điều kiện các lớp trong quá trình huấn luyện cho LeNet5 trên MNIST. Từ trái sang phải: mô hình baseline cấp đầy đủ tiêu chuẩn; [67] huấn luyện cấp thấp vanilla; [51] huấn luyện cấp thấp động dựa trên dòng gradient; [70] huấn luyện cấp thấp thông qua regularization. Tất cả các chiến lược huấn luyện cấp thấp được thiết lập với tỷ lệ nén 80% (tỷ lệ phần trăm tham số được loại bỏ so với mô hình baseline đầy đủ).

4 Huấn luyện cấp thấp robust

4.1 Tính không ổn định của mạng cấp thấp
Các phương pháp cấp thấp là những chiến lược phổ biến để giảm lưu trữ bộ nhớ và chi phí tính toán của cả giai đoạn huấn luyện và suy luận của các mô hình học sâu [51,67,27]. Tận dụng cấu trúc cấp thấp nội tại của các ma trận tham số [7,55,45,16], các phương pháp này huấn luyện một mạng con với các ma trận trọng số được tham số hóa như W=USV⊤, cho các ma trận "cao và mỏng" U, V với r cột, và một ma trận r×r nhỏ S. Huấn luyện các ma trận trọng số cấp thấp đã được chứng minh là hiệu quả giảm các tham số huấn luyện trong khi duy trì hiệu suất tương đương với mô hình đầy đủ. Tuy nhiên, trong khi nhiều đóng góp đã phân tích và tinh chỉnh các phương pháp cấp thấp để phù hợp với độ chính xác của mô hình đầy đủ, độ chính xác robust của các mô hình cấp thấp đã bị bỏ qua phần lớn trong tài liệu.

Ở đây chúng tôi quan sát thấy rằng việc giảm cấp của lớp thực sự có thể làm xấu đi robustness của mạng. Chúng tôi lập luận rằng hiện tượng này có thể quy cho số điều kiện bùng nổ của mạng. Trong Hình 1 chúng tôi vẽ sự tiến hóa của số điều kiện cond2 cho bốn lớp nội bộ của LeNet5 trong quá trình huấn luyện sử dụng các chiến lược huấn luyện cấp thấp khác nhau và so sánh chúng với mô hình đầy đủ. Trong khi số điều kiện của mô hình đầy đủ tăng vừa phải với số lần lặp, số điều kiện của các lớp cấp thấp tăng vọt một cách đáng kể. Tính không ổn định giá trị kỳ dị này dẫn đến hiệu suất robustness kém của các phương pháp, như quan sát thấy trong đánh giá thực nghiệm của Phần 5.

Trong phần sau, chúng tôi thiết kế một mô hình huấn luyện cấp thấp cho phép áp đặt các ràng buộc huấn luyện đơn giản nhưng hiệu quả, ràng buộc số điều kiện của mạng được huấn luyện trong dung sai mong muốn 1 +τ, và cải thiện robustness mạng mà không ảnh hưởng đến chi phí huấn luyện cũng như suy luận.

4.2 Dòng gradient cấp thấp với các giá trị kỳ dị bị ràng buộc

Cho W∈Rn×m là ma trận trọng số của một lớp tuyến tính trong f. Đối với một số nguyên r≤min{m, n} cho Mr={W: rank(W) =r} là đa tạp của các ma trận cấp-r mà chúng tôi tham số hóa như

Mr=
USV⊤:U∈Rn×r, V∈Rm×r với các cột trực chuẩn , S∈Rr×r khả nghịch	
.

Rõ ràng, các giá trị kỳ dị của W=USV⊤∈ Mr trùng với các giá trị kỳ dị của S. Đối với s, ε sao cho 0< ε < s , định nghĩa Σs(ε) là tập hợp các ma trận với các giá trị kỳ dị trong khoảng [s−ε, s+ε].
Lưu ý rằng Σs(0) là một đa tạp Riemannian được đạt được về cơ bản bằng một phép chia tỷ lệ s của đa tạp Stiefel tiêu chuẩn và bất kỳ A∈Σs(0) nào đều được điều kiện hóa tối ưu, tức là cond2(A) = 1 . Do đó, ε có thể được diễn giải như một tham số xấp xỉ kiểm soát mức độ gần của Σs(ε) với đa tạp "tối ưu" Σs(0). Để tăng cường robustness mạng, trong phần sau chúng tôi sẽ ràng buộc ma trận trọng số tham số S với Σs(ε). Với ràng buộc này, chúng ta có cond2(W)≤(s−ε)−1(s+ε) = 1 + τ, với τ= 2(s−ε)−1ε, để dung sai τ về điều kiện hóa của mạng có thể được điều chỉnh bằng cách chọn tham số xấp xỉ ε một cách phù hợp.

Cho hàm mất mát L, chúng tôi quan tâm đến bài toán tối ưu hóa có ràng buộc

minL s.t. W=USV⊤∈ Mr và S∈Σs(ε), cho tất cả các lớp W . (3)

--- TRANG 6 ---
Để tiếp cận (3), chúng tôi sử dụng các lập luận tiêu chuẩn từ lý thuyết tích phân hình học [66,29] để thiết kế một sơ đồ huấn luyện chỉ cập nhật các yếu tố U, S, V và gradient của L đối với U, S, V, mà không bao giờ tạo thành các trọng số đầy đủ cũng như các gradient đầy đủ. Để đạt được điều này, theo [51], trước tiên chúng tôi diễn đạt lại việc tối ưu hóa L đối với mỗi lớp W như một dòng gradient thời gian liên tục

˙W(t) =−∇WL(W(t)), (4)

trong đó "dấu chấm" biểu thị đạo hàm theo thời gian và trong đó chúng tôi viết L như một hàm của W duy nhất, để ngắn gọn. Dọc theo nghiệm của phương trình vi phân trên, mất mát giảm và một điểm dừng được tiếp cận khi t→ ∞ . Bây giờ, nếu chúng ta giả định W∈ Mr, thì ˙W∈TWMr, không gian tiếp tuyến của Mr tại điểm W. Do đó, để đảm bảo toàn bộ quỹ đạo W(t)∈ Mr, chúng ta có thể xem xét dòng gradient chiếu

˙W(t) =−PW(t)∇WL(W(t)), trong đó PW biểu thị phép chiếu trực giao (trong không gian xung quanh của ma trận) lên TWMr. Tiếp theo, chúng ta nhận thấy rằng phép chiếu PW∇WL có thể được định nghĩa bằng cách áp đặt tính trực giao đối với bất kỳ điểm Y∈TWMr nào, cụ thể là

⟨PW∇WL − ∇ WL, Y⟩= 0 cho tất cả Y∈TWMr

trong đó ⟨·,·⟩ là tích vô hướng Frobenius. Như đã thảo luận trong ví dụ [29,51], các phương trình trên kết hợp với biểu diễn đã biết của TWMr tạo ra một hệ thống ba phương trình dòng gradient cho các yếu tố riêng lẻ 


˙U=−G1(U), G 1(U) =P⊥
U∇UL(USV⊤)(SS⊤)−1
˙V=−G2(V), G 2(V) =P⊥
V∇VL(USV⊤)(S⊤S)−⊤
˙S=−G3(S), G 3(S) =∇SL(USV⊤)(5)

trong đó P⊥
U= (I−UU⊤) và P⊥
V= (I−V V⊤) là các toán tử chiếu lên không gian trực giao với span của U và V, tương ứng.

Dựa trên hệ thống dòng gradient trên, chúng tôi đề xuất một sơ đồ huấn luyện tại mỗi lần lặp và cho mỗi lớp được tham số hóa bởi bộ ba {U, S, V} tiến hành như sau:

1. cập nhật U và V bằng cách tích phân số các dòng gradient ˙U=−G1(U) và ˙V=−G2(V)
2. chiếu U, V kết quả lên đa tạp Stiefel của các ma trận với r cột trực chuẩn
3. cập nhật trọng số r×r S bằng cách tích phân ˙S=−G3(S)
4. cho một dung sai robustness cố định τ, chiếu S được tính toán lên Σs(ε), chọn s và ε sao cho
•s là xấp xỉ hằng số tốt nhất với S⊤S, tức là s= argminα∥S⊤S−α2I∥F
•ε sao cho cond2 của phép chiếu của S không vượt quá 1 +τ

Lưu ý rằng các hệ số s,ε tại điểm 4 có thể được đạt được một cách rõ ràng bằng cách thiết lập s=qPr
j=1sj(S)2/r, moment thứ hai của các giá trị kỳ dị sj(S) của S, và ε=τs/(2 +τ). Cũng lưu ý rằng, trong các phương trình vi phân cho U, S, V trong (5), bốn bước trên có thể được thực hiện song song cho mỗi trong ba biến. Pseudocode chi tiết của sơ đồ huấn luyện được trình bày trong Thuật toán 1. Chúng tôi kết thúc với một số nhận xét về việc thực hiện của nó.

Nhận xét, chi tiết thực hiện và hạn chế
Mỗi bước của Thuật toán 1 yêu cầu ba bước tối ưu hóa tại dòng 2, 4, 6. Các bước này có thể được thực hiện bằng cách sử dụng các bộ tối ưu hóa bậc nhất tiêu chuẩn như SGD với momentum hoặc ADAM. Các kỹ thuật tiêu chuẩn có thể được sử dụng để chiếu lên đa tạp Stiefel tại dòng 3 và 5 của Thuật toán 1, xem ví dụ [4,66]. Ở đây, chúng tôi sử dụng phân tích QR. Đối với phép chiếu lên Σs(ε) tại dòng 9, chúng tôi tính toán SVD của yếu tố nhỏ S và thiết lập thành s+ε hoặc s−ε các giá trị kỳ dị rơi ngoài khoảng [s−ε, s+ε]. Lưu ý rằng, khi τ= 0, tức là khi chúng ta yêu cầu điều kiện hóa hoàn hảo cho trọng số lớp W=USV⊤, thì SVD của S có thể được thay thế bằng một bước QR hoặc bất kỳ phép chiếu đa tạp Stiefel nào khác. Thực vậy, chúng ta có thể thiết lập tương đương s=p
trace(S⊤S)/r, và sau đó chiếu lên Σs(0) bằng cách điều chỉnh tỷ lệ bằng một yếu tố s phép chiếu của S lên đa tạp Stiefel. Trong trường hợp này, hệ thống (5) được đơn giản hóa hơn nữa, vì chúng ta có thể thay thế (SS⊤)−1 và (S⊤S)−⊤ bằng vô hướng 1/s2.

Nhìn chung, mạng cấp thấp nén có r(n+m+r) tham số cho mỗi lớp, trong đó n và m là số lượng neuron đầu vào và đầu ra. Do đó, chọn r sao cho 1−r(n+m+r)/(nm) =α

--- TRANG 7 ---
Thuật toán 1: Pseudocode của sơ đồ huấn luyện Cấp Thấp có Điều kiện tốt Robust (CondLR)
Đầu vào: Tỷ lệ nén được chọn, tức là cho mỗi lớp W chọn một cấp r;
Trọng số các lớp ban đầu được tham số hóa như W=USV⊤, với S∼r×r;
Moment giá trị kỳ dị thứ hai của S, s=pP
ksk(S)2/r
Dung sai điều kiện hóa τ >0
1foreach lần lặp và mỗi lớp do (mỗi khối song song)
2 U←một bước tối ưu hóa với gradient G1 và điểm khởi tạo U
3 U←chiếu U lên đa tạp Stiefel với r cột trực chuẩn
4 V←một bước tối ưu hóa với gradient G2 và điểm khởi tạo V
5 V←chiếu V lên đa tạp Stiefel với r cột trực chuẩn
6 S←một bước tối ưu hóa với gradient G3 và điểm khởi tạo S
7 s←pP
ksk(S)2/r, căn bậc hai của moment thứ hai của các giá trị kỳ dị của S
8 ε←τs/(2 +τ)
9 S←chiếu S lên Σs(ε)

có thể tạo ra tỷ lệ nén mong muốn 0< α < 1 trên số lượng tham số mạng, tức là số lượng tham số người ta loại bỏ so với baseline đầy đủ. Ví dụ, trong các thí nghiệm của chúng tôi, chúng tôi sẽ chọn r sao cho α= 0.5 hoặc α= 0.8.

Hạn chế. Vì tham số cấp r phải được chọn trước cho mỗi lớp của mạng, một hạn chế của phương pháp được đề xuất là nhu cầu tiềm năng về việc tinh chỉnh tham số đó, mặc dù phân tích được đề xuất trong Bảng 1 cho thấy hiệu suất cạnh tranh cho cả tỷ lệ nén 50% và 80%. Ngoài ra, nếu kích thước lớp n×m không đủ lớn, tỷ lệ nén 1−r(n+m+r)/(nm) có thể bị hạn chế.
Do đó phương pháp hoạt động tốt chỉ cho các mạng đủ rộng. Cuối cùng, một cách tiêu chuẩn để đạt được hiệu suất đối kháng tốt hơn sẽ là kết hợp robustness dựa trên điều kiện hóa được đề xuất với các chiến lược huấn luyện đối kháng [15,68,59]. Tuy nhiên, chi phí tạo ra các ví dụ đối kháng trong quá trình huấn luyện không phải là không đáng kể, đặc biệt khi dựa trên các cuộc tấn công nhiều bước, và do đó cách tích hợp huấn luyện đối kháng mà không ảnh hưởng đến lợi ích thu được với nén cấp thấp không đơn giản.

4.3 Đảm bảo xấp xỉ
Các phương pháp tối ưu hóa trên đa tạp của các ma trận cấp thấp được biết là bị ảnh hưởng bởi hình học nội tại cứng của đa tạp ràng buộc có độ cong rất cao xung quanh các điểm mà W∈ Mr gần như kỳ dị [4,66,29]. Điều này ngụ ý rằng ngay cả những thay đổi rất nhỏ trong W có thể tạo ra các không gian tiếp tuyến rất khác nhau, và do đó các quỹ đạo huấn luyện khác nhau, như được thể hiện bởi kết quả dưới đây:

Bổ đề 1 (Giới hạn độ cong, Bổ đề 4.2 [29]). Cho W∈ Mr cho smin(W)>0 là giá trị kỳ dị nhỏ nhất của nó. Cho bất kỳ W′∈ Mr gần với W tùy ý và bất kỳ ma trận B nào, điều sau đây tồn tại

∥PWB−PW′B∥F≤C smin(W)−1∥W−W′∥F,

trong đó C >0 chỉ phụ thuộc vào B.

Trong thuật ngữ dòng gradient của chúng tôi, hiện tượng này được thể hiện bởi sự hiện diện của nghịch đảo ma trận trong (5).
Trong khi đây thường là một vấn đề có thể ảnh hưởng đáng kể đến hiệu suất của các bộ tối ưu hóa cấp thấp (xem thêm Phần 5), bước regularization được đề xuất áp dụng các giá trị kỳ dị bị ràng buộc cho phép chúng ta di chuyển dọc theo các đường tránh các điểm cứng. Sử dụng quan sát này, ở đây chúng tôi cung cấp một giới hạn về chất lượng của mạng thần kinh cấp thấp được tính toán thông qua Thuật toán 1, với điều kiện tồn tại một quỹ đạo tối ưu dẫn đến một mạng gần như cấp thấp. Chúng tôi nhấn mạnh rằng giả định này phù hợp với công việc gần đây cho thấy sự tồn tại của các mạng cấp thấp hiệu suất cao trong ví dụ các mô hình tuyến tính sâu [45, 16, 7, 23].

Giả sử việc huấn luyện được thực hiện thông qua gradient descent với tỷ lệ học λ >0, và cho W(t) là dòng gradient đầy đủ (4). Hơn nữa, giả sử rằng cho t∈[0, λ] và một ε >0 cho trước, cho mỗi lớp tồn tại E(t) và fW(t)∈ Mr∩Σs(ε) sao cho

W(t) =fW(t) +E(t),

--- TRANG 8 ---
trong đó s là moment thứ hai của các giá trị kỳ dị của W(t) và E(t) là một nhiễu loạn có biến thiên thời gian bị ràng buộc, cụ thể là ∥˙E(t)∥ ≤η. Nói cách khác, chúng ta giả định tồn tại một quỹ đạo huấn luyện dẫn đến một ma trận trọng số W(t) gần như cấp thấp với các giá trị kỳ dị gần như hằng số. Bởi vì giá trị s bị ràng buộc bởi cấu trúc, ma trận phụ thuộc tham số fW(t) có các giá trị kỳ dị thể hiện giới hạn dưới vừa phải. Do đó, W(t) cách xa vùng cứng của (5) và chúng ta đạt được giới hạn sau, dựa trên [29] (chứng minh được chuyển đến Phụ lục C trong tài liệu bổ sung)

Định lý 1. Cho UkSkV⊤
k là một nghiệm của (5) được tính toán với k bước của Thuật toán 1. Giả sử rằng
•Khởi tạo cấp thấp U0S0V⊤
0 trùng với xấp xỉ cấp thấp fW(0).
•Chuẩn của gradient đầy đủ bị ràng buộc, tức là, ∥∇WL(W(t))∥ ≤µ.
•Tỷ lệ học bị ràng buộc như λ≤s−ε
4√2µη.

Khi đó, giả sử không có lỗi số, giới hạn sai số sau đây tồn tại

∥UkSkV⊤
k−W(λk)∥ ≤3λη .

5 Thí nghiệm
Chúng tôi minh họa hiệu suất của Thuật toán 1 trên nhiều trường hợp thử nghiệm khác nhau. Tất cả các thí nghiệm có thể được tái tạo với mã có sẵn trong tài liệu bổ sung. Để đánh giá hiệu suất nén và robustness kết hợp của phương pháp được đề xuất, chúng tôi so sánh nó với cả baseline đầy đủ và cấp thấp.

Cho tất cả các mô hình, chúng tôi tính toán độ chính xác tự nhiên và độ chính xác robust. Cho {(xi, yi)}i=1,...,n là tập hợp các hình ảnh thử nghiệm và các nhãn tương ứng và cho f là mô hình mạng thần kinh, với đầu ra f(x) trên đầu vào x. Chúng tôi định lượng độ chính xác robust của tập thử nghiệm như:

robust_acc(δ) =1
nPn
i=1 1{yi}(f(xi+δi))

trong đó δ= (δi)i=1,...,n là các nhiễu loạn đối kháng liên quan đến mỗi mẫu. Lưu ý rằng, trong trường hợp không bị nhiễu loạn với ∥δi∥= 0, định nghĩa độ chính xác robust hoàn toàn trùng với định nghĩa độ chính xác thử nghiệm. Trong các thí nghiệm của chúng tôi, các nhiễu loạn đối kháng được tạo ra bởi phương pháp dấu hiệu gradient nhanh [17], do đó chúng có dạng δi=ϵsign(∇xL(f(xi), yi)), trong đó ϵ kiểm soát cường độ nhiễu loạn, vì ∥δi∥∞=ϵ. Vì các hình ảnh trong thiết lập của chúng tôi có các mục nhập đầu vào trong [0,1], đầu vào bị nhiễu loạn sau đó được kẹp vào khoảng đó. Lưu ý rằng, vì lý do tương tự, giá trị của ϵ kiểm soát trong trường hợp của chúng tôi kích thước tương đối của nhiễu loạn.

Bộ dữ liệu. Chúng tôi xem xét các bộ dữ liệu MNIST và CIFAR10 [30] cho mục đích đánh giá. Bộ đầu tiên chứa 60,000 hình ảnh huấn luyện và bộ cuối cùng chứa 50,000 hình ảnh huấn luyện. Tất cả các bộ dữ liệu có 10,000 hình ảnh thử nghiệm và 10 lớp. Không có augmentation dữ liệu được thực hiện.

Mô hình. Chúng tôi sử dụng LeNet5 [31] cho bộ dữ liệu MNIST và VGG16 [54] cho CIFAR10. Kiến trúc tổng quát cho tất cả các mạng được sử dụng được bảo tồn trên các mô hình, trong khi các cấu trúc lưu trữ trọng số và khung tối ưu hóa khác nhau.

Phương pháp. Mạng baseline của chúng tôi là mạng được thực hiện với việc triển khai tiêu chuẩn. Cayley SGD [35] và Projected SGD [3] là các phương pháp dựa trên tối ưu hóa Riemannian huấn luyện các trọng số mạng trên đa tạp Stiefel của các ma trận với các cột trực chuẩn. Do đó, cả hai phương pháp đều đảm bảo cond2(W) = 1 cho tất cả các lớp. Phương pháp đầu tiên sử dụng ước lượng lặp của biến đổi Cayley, trong khi phương pháp sau sử dụng phép chiếu dựa trên QR để thu hồi gradient Riemannian lên đa tạp Stiefel. Cả hai phương pháp đều không có nén và sử dụng các ma trận trọng số đầy đủ. DLRT, SVD prune, và Vanilla là các phương pháp cấp thấp đảm bảo nén các tham số mô hình trong quá trình huấn luyện. DLRT [51] dựa trên một mô hình dòng gradient cấp thấp tương tự như Thuật toán 1 được đề xuất. SVD prune [70] dựa trên một mất mát regularized với một số hạng phạt thúc đẩy cấp thấp. Phương pháp này được thiết kế để nén các cấp của mạng sau khi huấn luyện, nhưng chúng tôi áp đặt một tỷ lệ nén cố định từ đầu để so sánh công bằng.
"Vanilla" biểu thị phương pháp cấp thấp rõ ràng, tham số hóa các lớp như W=UV⊤ và thực hiện các bước descent xen kẽ trên U và V, như được thực hiện trong ví dụ [67,27]. Tất cả các mô hình được triển khai với tỷ lệ nén huấn luyện cố định α= 0.5 và α= 0.8, tức là chúng tôi chỉ sử dụng 50% và 20% số tham số mà mô hình đầy đủ sẽ sử dụng trong quá trình huấn luyện, tương ứng. Cuối cùng, chúng tôi triển khai CondLR như trong Thuật toán 1 cho ba lựa chọn của dung sai điều kiện hóa τ∈ {0,0.1,0.5}. Chúng tôi cũng triển khai một phiên bản sửa đổi của

--- TRANG 9 ---
Bảng 1: Kết quả so sánh phương pháp
LeNet5 MNIST VGG16 Cifar10 c.r.
(%)Nhiễu loạn tương đối ϵ 0.0 0.02 0.04 0.06 0.0 0.002 0.004 0.006
Baseline 0.9908 0.9815 0.9647 0.9328 0.9087 0.7515 0.5847 0.4640 0
Cayley SGD 0.9872 0.9812 0.9695 0.9497 0.8965 0.7435 0.5802 0.4508 0
Projected SGD 0.9665 0.9047 0.7908 0.6260 0.8584 0.6967 0.5402 0.4124 0CondLRτ= 0 0.9884 0.9819 0.9717 0.9582 0.9131 0.7457 0.5611 0.4157 50
τ= 0.1 0.9873 0.9825 0.9759 0.9668 0.9117 0.7539 0.6201 0.5212 50
τ= 0.5 0.9865 0.9791 0.9726 0.9612 0.8999 0.7262 0.5940 0.487 50
mean 0.9606 0.9414 0.9156 0.8773 0.8793 0.6846 0.4861 0.3459 50
DLRT 0.9907 0.981 0.9608 0.9245 0.8367 0.6079 0.4342 0.3233 50
Vanilla 0.987 0.9748 0.9488 0.9094 0.8995 0.6801 0.4917 0.3856 50
SVD prune 0.9876 0.9718 0.9458 0.8966 0.8981 0.6750 0.4753 0.3737 50CondLRτ= 0 0.9881 0.9802 0.9694 0.9505 0.9054 0.7476 0.5718 0.4237 80
τ= 0.1 0.9877 0.9794 0.9673 0.9477 0.9063 0.7079 0.5115 0.3760 80
τ= 0.5 0.9858 0.9754 0.9585 0.9288 0.8884 0.6936 0.5082 0.3787 80
mean 0.9816 0.9730 0.9597 0.9395 0.8698 0.6627 0.4659 0.3213 80
DLRT 0.9888 0.9747 0.9503 0.9067 0.8438 0.6178 0.4184 0.2883 80
Vanilla 0.9839 0.9660 0.9314 0.8687 0.8806 0.6540 0.4505 0.3234 80
SVD prune 0.9847 0.9650 0.9261 0.8513 0.8845 0.6355 0.4195 0.3068 80

Thuật toán 1 trong đó S được chiếu trực tiếp lên đa tạp Stiefel Σ1(0), tức là tham số s được cố định bằng một.

Huấn luyện. Mỗi phương pháp và mô hình được huấn luyện trong 120 epoch của stochastic gradient descent với kích thước minibatch là 128. Chúng tôi sử dụng tỷ lệ học 0.1 cho LeNet5 và 0.05 cho VGG16 với momentum 0.3 và 0.45, tương ứng, và một bộ lập lịch tỷ lệ học với hệ số = 0.4 tại epoch 70 và 100.

Kết quả. Để so sánh, chúng tôi đo độ chính xác robust cho tất cả các tổ hợp được chọn của bộ dữ liệu, mô hình và phương pháp với ngân sách nhiễu loạn tương đối ϵ∈ {0,0.01,0.02,0.03,0.04,0.05,0.06} cho MNIST và ϵ∈ {0,0.001,0.002,0.003,0.004,0.005,0.006} cho CIFAR10. Kết quả được tóm tắt trong Bảng 1 cho MNIST và CIFAR10 và một tập con của ngân sách nhiễu loạn. Tập hợp kết quả đầy đủ được hiển thị trong Bảng 2–3 trong tài liệu bổ sung. Trong các bảng, chúng tôi làm nổi bật (màu xám) phương pháp có hiệu suất tốt nhất cho mỗi phạm vi tỷ lệ nén α∈ {0%,50%,80%}, và phương pháp tốt nhất tổng thể (in đậm).

Các thí nghiệm cho thấy rằng phương pháp được đề xuất bảo tồn một cách có ý nghĩa độ chính xác so với baseline cho tất cả các bộ dữ liệu; hơn nữa, robustness được cải thiện từ baseline cho tỷ lệ nén 50% và bổ sung trong trường hợp MNIST cho tỷ lệ nén 80%. So với các phương pháp trực giao không nén, CondLR với tỷ lệ nén 50% cũng vượt trội hơn Cayley SGD và Projected SGD.
Đồng thời CondLR được nén theo thiết kế, vì vậy chúng tôi giảm chi phí bộ nhớ cùng với việc tăng robustness. Như dự đoán trong Phần 4.1, các phương pháp cấp thấp không có regularization làm xấu đi số điều kiện và do đó robustness của mô hình. Điều này phù hợp với kết quả của Bảng 1, nơi chúng tôi quan sát thấy rằng nén không có regularization không hoạt động tốt về độ chính xác robust, đặc biệt DLRT, Vanilla và SVD prune thể hiện sự giảm hiệu suất so với baseline và, do đó, CondLR. Cụ thể, cho mỗi tỷ lệ nén cố định α phương pháp của chúng tôi thể hiện robustness cao nhất; hơn nữa, robustness của CondLR với α= 0.8 cao hơn robustness của bất kỳ phương pháp nào khác với tỷ lệ nén thậm chí thấp hơn, α= 0.5.

Như một đánh giá thực nghiệm tiếp theo, chúng tôi phân tích robustness của các mô hình huấn luyện cấp thấp đối với các giá trị kỳ dị nhỏ. Như được thể hiện bởi Định lý 1, CondLR không bị ảnh hưởng bởi độ cong dốc của đa tạp cấp thấp Mr, tức là độ chính xác và tỷ lệ hội tụ của CondLR không xấu đi khi điều kiện hóa của các ma trận trọng số bùng nổ. Ngược lại, các giá trị kỳ dị nhỏ có thể ảnh hưởng đáng kể đến

--- TRANG 10 ---
0200 400 600 800 1000 1200 1400
Bước tối ưu hóa0.00000.00250.00500.00750.01000.01250.01500.01750.0200Mất mát
0200 400 600 800 1000 1200 1400
Bước tối ưu hóa0.00.20.40.60.81.0Độ chính xác
0200 400 600 800 1000 1200 1400
Bước tối ưu hóa103108101310181023102810331038Giới hạn trên cond(f)CondLR,=0
DLRT
SVD Prune
VanillaHình 2: Sự tiến hóa của mất mát, độ chính xác, và Q
icond(Wi) cho Lenet5 trên bộ dữ liệu MNIST, cho các lớp ban đầu có điều kiện xấu có các giá trị kỳ dị bị buộc phải giảm theo cấp số nhân với các lũy thừa của hai.

các mô hình huấn luyện cấp thấp khác. Trong Hình 2 chúng tôi cho thấy hành vi của mất mát, độ chính xác, và số điều kiện như các hàm của các bước lặp, khi mạng được khởi tạo với các ma trận lớp có điều kiện xấu. Chính xác, tương tự như những gì được thực hiện trong [27], chúng tôi lấy mẫu ngẫu nhiên các trọng số Gaussian ban đầu, chúng tôi tính toán SVD của chúng để định nghĩa các yếu tố U và V ban đầu, và sau đó buộc các giá trị kỳ dị của yếu tố S ban đầu giảm theo cấp số nhân. Chúng tôi quan sát thấy rằng CondLR và DLRT (cũng dựa trên một công thức dòng gradient cấp thấp) là robust nhất, trong khi hiệu suất của các phương pháp khác xấu đi đáng kể. Điều này xác nhận rằng, như cũng được quan sát trong [27,51], hầu hết các phương pháp cấp thấp yêu cầu các lựa chọn khởi tạo được tinh chỉnh cụ thể, trong khi mô hình huấn luyện cấp thấp robust được đề xuất đảm bảo hiệu suất vững chắc độc lập với việc khởi tạo.

A Kết quả bổ sung
Trong phần này, chúng tôi báo cáo kết quả về độ chính xác tự nhiên và robust cho phạm vi rộng hơn của ngân sách nhiễu loạn ϵ∈ {0,0.01,0.02,0.03,0.04,0.05,0.06} cho LeNet5 trên MNIST, và ϵ∈ {0,0.001,0.002,0.003,0.004,0.005,0.006} cho VGG16 trên CIFAR10. Xem Bảng 2 và Bảng 3. Kết quả xác nhận những phát hiện tương tự như những kết quả được báo cáo trong các phần trước.

Bảng 2: LeNet5 MNIST
Nhiễu loạn tương đối ϵ0.0 0.01 0.02 0.03 0.04 0.05 0.06 cr (%)
Baseline 0.9908 0.9866 0.9815 0.9751 0.9647 0.9512 0.9328 0
Cayley SGD 0.9872 0.9844 0.9812 0.9767 0.9695 0.9600 0.9497 0
Projected SGD 0.9665 0.9409 0.9047 0.8520 0.7908 0.7111 0.6260 0CondLRτ= 0.0 0.9884 0.9850 0.9819 0.9772 0.9717 0.9665 0.9582 50
τ= 0.1 0.9873 0.9848 0.9825 0.9795 0.9759 0.9719 0.9668 50
τ= 0.5 0.9865 0.9827 0.9791 0.9764 0.9726 0.9682 0.9612 50
mean 0.9868 0.9837 0.9809 0.9781 0.9728 0.9681 0.9614 50
DLRT 0.9907 0.9863 0.9810 0.9716 0.9608 0.9454 0.9245 50
vanilla 0.9870 0.9816 0.9748 0.9621 0.9488 0.9307 0.9094 50
SVD prune 0.9876 0.9803 0.9718 0.9625 0.9458 0.9235 0.8966 50CondLRτ= 0.0 0.9881 0.9842 0.9802 0.9752 0.9694 0.9615 0.9505 80
τ= 0.1 0.9877 0.9839 0.9794 0.9739 0.9673 0.9581 0.9477 80
τ= 0.5 0.9858 0.9812 0.9754 0.9683 0.9585 0.9459 0.9288 80
mean 0.9816 0.9770 0.9730 0.9676 0.9597 0.9491 0.9395 80
DLRT 0.9888 0.9836 0.9747 0.9646 0.9503 0.9317 0.9067 80
vanilla 0.9839 0.9756 0.9660 0.9506 0.9314 0.9033 0.8687 80
SVD prune 0.9847 0.9760 0.9650 0.9477 0.9261 0.8937 0.8513 80

--- TRANG 11 ---
B Chứng minh Mệnh đề 1
Bổ đề 2. Cho ϕ: (Y,∥ · ∥ Y)→(Z,∥ · ∥ Z) và ψ: (X,∥ · ∥ X)→(Y,∥ · ∥ Y) là các ánh xạ liên tục giữa các không gian Banach hữu hạn chiều, và giả sử cond(ψ) và cond(ϕ) được định nghĩa tốt. Khi đó bất đẳng thức sau tồn tại:

cond(ϕ◦ψ)≤cond(ϕ) cond(ψ)

Chứng minh. Để chứng minh bổ đề này, chúng tôi sẽ đi qua định nghĩa của R(ϕ◦ψ, x, δ ). Hãy nhớ lại rằng nó được định nghĩa như

R(ϕ◦ψ, x, δ ) =∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥x∥X
∥δ∥X∥ϕ◦ψ(x)∥Z

Bây giờ, nếu ψ(x+δ)−ψ(x) = 0 cho δ̸= 0, thì R(ϕ◦ψ, x, δ ) = 0 . Vì chúng ta quan tâm đến supremum, chúng ta có thể giới hạn đến δ sao cho ψ(x+δ)−ψ(x)̸= 0. Hơn nữa, bằng cách nhân và chia cho ∥ψ(x)∥Y chúng ta được:

R(ϕ◦ψ, x, δ ) =∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥ψ(x)∥Y
∥ψ(x+δ)−ψ(x)∥Y∥ϕ◦ψ(x)∥Z∥ψ(x+δ)−ψ(x)∥Y∥x∥X
∥δ∥X∥ψ(x)∥Y

Bây giờ, nếu chúng ta định nghĩa cond(f, x, ε ) = sup
δ̸=0:∥δ∥X≤εR(f, x, δ ), chúng ta lấy supremum cả hai vế trên tập {δ∈X|∥δ∥X≤ε} và chúng ta có thể giới hạn trên nó với tích của suprema của hai khối

sup
δ̸=0:∥δ∥X≤εR(ϕ◦ψ, x, δ ) = cond(ψ, x, ε ) sup
δ̸=0:∥δ∥X≤ε∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥ψ(x)∥Y
∥ψ(x+δ)−ψ(x)∥Y∥ϕ◦ψ(x)∥Z
=⋆

Bây giờ, cho η=ψ(x+δ)−ψ(δ), chúng ta có thể viết lại phần thứ hai của phương trình cuối như một dạng hạn chế của cond(ϕ, ψ(x), ε) đến tập hợp cụ thể của các hướng nhiễu loạn có dạng η. Do đó, chúng ta có thể giới hạn dưới nó như:

⋆≤cond(ψ, x, ε ) sup
η̸=0:∥η∥Y≤ε∥ϕ(ψ(x) +η)−ϕ(ψ(x))∥Z∥ψ(x)∥Y
∥η∥Y∥ϕ(ψ(x))∥Z
=
= cond(ψ, x, ε ) cond(ϕ, ψ(x), ε)

Bảng 3: VGG16 Cifar10
Nhiễu loạn tương đối ϵ0.0 0.001 0.002 0.003 0.004 0.005 0.006 cr (%)
Baseline 0.9087 0.8375 0.7515 0.6643 0.5847 0.5194 0.4640 0
Cayley SGD 0.8965 0.8278 0.7435 0.6612 0.5802 0.5087 0.4508 0
Projected SGD 0.8584 0.7810 0.6967 0.6169 0.5402 0.4743 0.4124 0CondLRτ= 0.0 0.9131 0.8371 0.7457 0.6515 0.5611 0.4848 0.4157 50
τ= 0.1 0.9117 0.8331 0.7539 0.6883 0.6201 0.5656 0.5212 50
τ= 0.5 0.8999 0.8180 0.7262 0.6550 0.5940 0.5397 0.4877 50
mean 0.8793 0.7851 0.6846 0.5729 0.4861 0.4116 0.3459 50
DLRT 0.8367 0.7163 0.6079 0.5130 0.4342 0.3702 0.3233 50
vanilla 0.8995 0.7991 0.6801 0.5721 0.4917 0.4299 0.3856 50
SVD prune 0.8981 0.7923 0.6750 0.5662 0.4753 0.4161 0.3737 50CondLRτ= 0.0 0.9054 0.8314 0.7476 0.6574 0.5718 0.4912 0.4237 80
τ= 0.1 0.9063 0.8134 0.7079 0.6070 0.5115 0.4330 0.3760 80
τ= 0.5 0.8884 0.7992 0.6936 0.5897 0.5082 0.4377 0.3787 80
mean 0.8698 0.7719 0.6627 0.5558 0.4659 0.3854 0.3213 80
DLRT 0.8438 0.7269 0.6178 0.5103 0.4184 0.3489 0.2883 80
vanilla 0.8806 0.7721 0.6540 0.5395 0.4505 0.3767 0.3234 80
SVD prune 0.8845 0.7646 0.6355 0.5143 0.4195 0.3552 0.3068 80

--- TRANG 12 ---
Theo giả thiết, cond(ϕ) và cond(ψ) là hữu hạn, vì vậy chúng ta có thể lấy giới hạn ε↓0 để có:

cond(ϕ◦ψ, x)≤cond(ϕ, ψ(x)) cond(ψ, x)

Cuối cùng, lấy supremum trên x trên cả hai thành viên của phương trình cuối, chúng ta có thể giới hạn trên nó với số điều kiện gốc không có ràng buộc về các hướng:

cond(ϕ◦ψ)≤sup
xcond(ϕ, ψ(x)) cond(ψ, x)≤cond(ϕ) cond(ψ)

và do đó kết luận.

Bây giờ, chứng minh của Mệnh đề 1 xuất phát trực tiếp bằng cách áp dụng Bổ đề trên một cách đệ quy.

Chứng minh Mệnh đề1. Bằng cách định nghĩa Ti(z) =Wiz, chúng ta có thể viết mạng thần kinh f như

f(x) = (σL◦TL◦σL−1◦ ··· ◦ T1)(x)

cho các hàm kích hoạt phi tuyến σi. Do đó, cho L= 1, luận đề xuất phát trực tiếp từ Bổ đề2, miễn là cond(σ1)≤C <∞. Cho L >1, người ta có thể mở ra cấu trúc thành phần của f từ bên trái, định nghĩa ϕ(z) = (σL◦TL)(z) và ψ(x) = (σL−1◦ ··· ◦ T1)(x). Khi đó bằng cách sử dụng Bổ đề2 chúng ta có rằng

cond(f)≤cond(ϕ) cond(ψ)≤cond(σL) cond(TL) cond(ψ).

Bây giờ, vì ψ là một mạng có độ sâu L−1, chúng ta có thể tiến hành quy nạp để đạt được rằng cond(f)≤ CQ
icond(Ti), với C= ΠL
i=1cond(σi), và chúng ta kết luận.

Lưu ý rằng kết quả trên chỉ có ý nghĩa nếu cond(σ) = supx∈Xcond(σ;x)<∞. Khi σ là Lipschitz, sử dụng công thức

cond(f;x) = sup
νx∈∂σ(x)∥νx∥∥x∥∥σ(x)∥−1,

với ∂ là toán tử gradient tổng quát Clarke [13], chúng tôi quan sát dưới đây rằng điều này đúng cho một danh sách rộng các hàm kích hoạt σ và không gian đặc trưng X.

•LeakyReLU. Cho x∈R,α >0, cho σ(x) = max {0, x}+αmin{0, x}. Khi đó bất kỳ νx∈∂σ(x) nào sao cho νx= 1 nếu x >0;νx=α nếu x <0;νx= [min(α,1),max(α,1)] nếu không. Do đó

cond(σ) = sup
x̸=0cond(σ;x) = sup
x̸=0sup
β∈∂σ(x)|x||1x>0+α1x<0+β1x=0|
|max{0, x}+αmin{0, x}|= max(α,1)

•Tanh. Cho x∈R, cho σ(x) = tanh(x). Khi đó σ′(x) =1
cosh2(x) và do đó

cond(σ) = sup
x|x|
|tanh(x)||cosh2(x)|= sup
x|4x|
|ex−e−x||ex+e−x|= 1

Vì maximum của cond(σ, x) đạt được tại zero, nơi hàm có thể được mở rộng bằng tính liên tục.

•Hardtanh. Cho x∈[−a, a] và a >0, cho σ(x) =a1x>a−a1x<−a+x1x∈[−a,a]. Khi đó, chúng ta có rằng ∂σ(x) trùng với các giá trị đạo hàm tại tất cả các điểm trừ x=±a. Tại hai điểm đó, chúng ta có ∂σ(±a) = [0 ,1]. Do đó, cho bất kỳ νx∈∂σ(x) nào, chúng ta có

cond(σ) = sup
x∈[−a,a]|νx||x|
|σ(x)|≤sup
x∈[−a,a]|x|
|σ(x)|=a

•Logistic sigmoid. Cho x∈R cho σ(x) = (1 + e−x)−1. Khi đó σ′(x) =σ(x)(1−σ(x)) và do đó

cond(σ;x) =|x|(1−σ(x)) =|x|e−x(1 +e−x)−1.

Do đó, khi x≥0, chúng ta có |x|e−x≤1/e và (1 +e−x)≥1, do đó cond(σ;x)≤1/e.

•Softplus. Cho x∈R, cho σ(x) = ln(1 + ex). Khi đó σ′(x) =S(x) = (1 + e−x)−1 và cond(σ;x) = |x|S(x)σ(x)−1. Do đó, cho x≥0, chúng ta có cond(σ;x)≤1.

•SiLU. Cho x∈R cho σ(x) =x(1 +e−x)−1=xS(x). Khi đó, σ′(x) =S(x) +xS(x)(1−S(x)) và do đó cho bất kỳ x≥0 nào chúng ta có

cond(σ;x) =|1 +x(1−S(x))| ≤1 +1
e

--- TRANG 13 ---
C Chứng minh Định lý 1
Trong phần sau, chứng minh của kết quả xấp xỉ chính được trình bày. Chúng tôi gạch dưới rằng phần cốt lõi của chứng minh dựa trên [29, Định lý 5.2]. Để hoàn chỉnh, chúng tôi tái đề xuất ở đây các yếu tố chính của lập luận. Chúng tôi tham khảo độc giả quan tâm đến [29] và các tài liệu tham khảo trong đó để biết thêm chi tiết.

Chứng minh. Cho Y(t) là nghiệm của (5) tại thời gian t∈[0, λ]. Đầu tiên, chúng tôi quan sát rằng các dòng con chiếu của W(t) =fW(t) +E(t) và fW(t) thỏa mãn các phương trình vi phân


˙Y=P(Y)˙fW+P(Y)˙E ,
˙fW=P
fW˙fW .

trong đó P(·) biểu thị phép chiếu trực giao vào không gian tiếp tuyến của đa tạp cấp thấp Mr. Tiếp theo, chúng tôi quan sát rằng các đồng nhất thức sau tồn tại

(P(Y)−P(fW))˙fW=−(P⊥(Y)−P⊥(fW))˙fW=−P⊥(Y)˙fW=−P⊥(Y)2˙fW .

trong đó P⊥(·) =I−P(·) đại diện cho phép chiếu trực giao bổ sung. Cái sau ngụ ý rằng

⟨Y−fW,(P(Y)−P(fW))˙fW⟩=⟨P⊥(Y)(Y−fW),(P(Y)−P(fW))˙fW⟩.

Cho γ= 32µ(s−ε)−2. Nó xuất phát từ [29, Bổ đề 4.2] rằng

⟨Y−fW,˙Y−˙fW⟩=⟨P⊥(Y)(Y−fW),(P(Y)−P(fW))˙fW⟩+⟨Y−fW, P (Y)˙E⟩
≤γ∥Y−fW∥3+η∥Y−fW∥.

Hơn nữa, chúng tôi nhắc nhở rằng

⟨Y−fW,˙Y−˙fW⟩=1
2d
dt∥Y−fW∥2=∥Y−fW∥d
dt∥Y−fW∥.

Do đó, sai số e(t) =∥Y(t)−fW(t)∥ thỏa mãn bất đẳng thức vi phân

˙e≤γe2+η, e (0) = 0 .

Sai số e(t) cho t∈[0, λ] thừa nhận một giới hạn trên được cho bởi nghiệm của

˙z=γz2+η, z (0) = 0 .

Bài toán giá trị ban đầu vi phân cuối cùng thừa nhận một nghiệm đóng được cho bởi

z(t) =p
η/γtan (tp
ηγ),

trong đó số hạng cuối bị ràng buộc bởi 2tη cho t√γη≤1. Chứng minh do đó kết luận như sau

∥Y(t)−W(t)∥ ≤ ∥ Y(t)−fW(t)∥+∥E(t)∥ ≤2tη+tη= 3tη ,

trong đó ước lượng cuối xuất phát từ đồng nhất thức tích phân E(t) =Rt
0˙E(s)ds.

Tài liệu tham khảo
[1]P. Ablin và G. Peyré. Tối ưu hóa nhanh và chính xác trên đa tạp trực giao không có rút gọn.
Trong Hội nghị quốc tế về Thống kê trí tuệ nhân tạo, trang 5636–5657. PMLR, 2022.
[2]P.-A. Absil, R. Mahony, và R. Sepulchre. Thuật toán tối ưu hóa trên đa tạp ma trận. Princeton
University Press, 2008.
[3]P.-A. Absil và J. Malick. Rút gọn giống phép chiếu trên đa tạp ma trận. SIAM Journal on
Optimization, 22(1):135–158, 2012.
[4]P.-A. Absil và I. V. Oseledets. Rút gọn cấp thấp: một khảo sát và kết quả mới. Computational
Optimization and Applications, 62(1):5–29, 2015.

--- TRANG 14 ---
[5]C. Anil, J. Lucas, và R. Grosse. Sắp xếp xấp xỉ hàm lipschitz. Trong Hội nghị quốc tế về Học máy, trang 291–301. PMLR, 2019.
[6]M. Arjovsky, A. Shah, và Y. Bengio. Mạng thần kinh tái phát tiến hóa đơn vị. Trong Hội nghị quốc tế về học máy, trang 1120–1128. PMLR, 2016.
[7]S. Arora, N. Cohen, W. Hu, và Y. Luo. Regularization ngầm trong phân tích nhân tử ma trận sâu. Advances in Neural Information Processing Systems, 32, 2019.
[8]A. Ashok, N. Rhinehart, F. Beainy, và K. M. Kitani. Học N2n: Nén mạng đến mạng thông qua học tăng cường gradient chính sách. Trong Hội nghị quốc tế về Biểu diễn học, 2018.
[9]N. Bansal, X. Chen, và Z. Wang. Chúng ta có thể thu được nhiều hơn từ regularization trực giao trong việc huấn luyện mạng sâu không? Advances in Neural Information Processing Systems, 31, 2018.
[10] P. L. Bartlett, D. J. Foster, và M. J. Telgarsky. Giới hạn biên được chuẩn hóa phổ cho mạng thần kinh. Advances in neural information processing systems, 30, 2017.
[11] G. Ceruti và C. Lubich. Một bộ tích phân robust không thông thường cho xấp xỉ cấp thấp động. BIT Numerical Mathematics, 62(1):23–44, 2022.
[12] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, và N. Usunier. Mạng parseval: Cải thiện robustness đối với các ví dụ đối kháng. Trong Hội nghị quốc tế về Học máy, trang 854–863. PMLR, 2017.
[13] F. H. Clarke. Tối ưu hóa và phân tích không trơn. SIAM, 1990.
[14] J. Cohen, E. Rosenfeld, và Z. Kolter. Robustness đối kháng được chứng nhận thông qua làm mịn ngẫu nhiên. Trong hội nghị quốc tế về học máy, trang 1310–1320. PMLR, 2019.
[15] J. Ding, T. Bu, Z. Yu, T. Huang, và J. Liu. Snn-rat: Mạng thần kinh spiking tăng cường robustness thông qua huấn luyện đối kháng được regularized. Advances in Neural Information Processing Systems, 35:24780–24793, 2022.
[16] R. Feng, K. Zheng, Y. Huang, D. Zhao, M. Jordan, và Z.-J. Zha. Giảm cấp trong mạng thần kinh sâu. arXiv:2206.06072, 2022.
[17] I. J. Goodfellow, J. Shlens, và C. Szegedy. Giải thích và khai thác các ví dụ đối kháng, 2015.
[18] S. Gui, H. Wang, H. Yang, C. Yu, Z. Wang, và J. Liu. Nén mô hình với Robustness đối kháng: Một khung tối ưu hóa thống nhất. 2019.
[19] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, và S. Han. AMC: AutoML cho nén và tăng tốc mô hình trên thiết bị di động. Trong Proceedings of the European conference on computer vision, trang 784–800, 2018.
[20] Y. He, X. Zhang, và J. Sun. Cắt tỉa kênh để tăng tốc mạng rất sâu. Trong IEEE International Conference on Computer Vision, trang 1389–1397, 2017.
[21] M. Hein và M. Andriushchenko. Đảm bảo chính thức về robustness của một bộ phân loại chống lại thao tác đối kháng. Advances in neural information processing systems, 30, 2017.
[22] D. J. Higham. Số điều kiện và số điều kiện của chúng. Linear Algebra and its Applications, 214:193–213, 1995.
[23] M. Huh, H. Mobahi, R. Zhang, B. Cheung, P. Agrawal, và P. Isola. Thiên kiến đơn giản cấp thấp trong mạng sâu. Transactions on Machine Learning Research, 2023.
[24] Y. Idelbayev và M. A. Carreira-Perpinán. Nén cấp thấp của mạng thần kinh: Học cấp của mỗi lớp. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 8049–8059, 2020.
[25] K. Jia, D. Tao, S. Gao, và X. Xu. Cải thiện việc huấn luyện mạng thần kinh sâu thông qua ràng buộc giá trị kỳ dị. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 4344–4352, 2017.
[26] A. Jordao và H. Pedrini. Về tác động của cắt tỉa đối với robustness đối kháng. Trong 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). IEEE Computer Society, 2021.

--- TRANG 15 ---
[27] M. Khodak, N. Tenenholtz, L. Mackey, và N. Fusi. Khởi tạo và regularization của các lớp thần kinh được phân tích nhân tử. Trong Hội nghị quốc tế về Biểu diễn học, 2021.
[28] H. Kim, M. U. K. Khan, và C.-M. Kyung. Nén mạng thần kinh hiệu quả. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 12569–12577, 2019.
[29] O. Koch và C. Lubich. Xấp xỉ cấp thấp động. SIAM Journal on Matrix Analysis and Applications, 29(2):434–454, 2007.
[30] A. Krizhevsky, G. Hinton, et al. Học nhiều lớp đặc trưng từ hình ảnh nhỏ. 2009.
[31] Y. LeCun, L. Bottou, Y. Bengio, và P. Haffner. Học dựa trên gradient được áp dụng để nhận dạng tài liệu. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[32] H. Lee, S. Han, và J. Lee. Huấn luyện viên đối kháng sinh tạo: Phòng thủ đối với các nhiễu loạn đối kháng với gan, 2017.
[33] K. Leino, Z. Wang, và M. Fredrikson. Mạng thần kinh robust toàn cầu. Trong Hội nghị quốc tế về Học máy, trang 6212–6222. PMLR, 2021.
[34] M. Lezcano-Casado và D. Martınez-Rubio. Ràng buộc trực giao rẻ trong mạng thần kinh: Một tham số hóa đơn giản của nhóm trực giao và unitary. Trong Hội nghị quốc tế về Học máy, trang 3794–3803. PMLR, 2019.
[35] J. Li, L. Fuxin, và S. Todorovic. Tối ưu hóa Riemannian hiệu quả trên đa tạp Stiefel thông qua biến đổi Cayley. 2019.
[36] Q. Li, S. Haque, C. Anil, J. Lucas, R. B. Grosse, và J.-H. Jacobsen. Ngăn chặn suy giảm gradient trong mạng tích chập bị ràng buộc lipschitz. Advances in neural information processing systems, 32, 2019.
[37] Y. Li, Z. Yang, Y. Wang, và C. Xu. Giãn nở kiến trúc thần kinh cho robustness đối kháng. Trong Advances in Neural Information Processing Systems, 2021.
[38] N. Liao, S. Wang, L. Xiang, N. Ye, S. Shao, và P. Chu. Đạt được robustness đối kháng thông qua tính thưa thớt. Machine Learning, trang 1–27, 2022.
[39] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, và K. Kavukcuoglu. Biểu diễn phân cấp cho tìm kiếm kiến trúc hiệu quả. Trong Hội nghị quốc tế về Biểu diễn học, 2018.
[40] X. Liu, Y. Li, C. Wu, và C.-J. Hsieh. Adv-BNN: Cải thiện phòng thủ đối kháng thông qua mạng thần kinh bayesian robust. Trong Hội nghị quốc tế về Biểu diễn học, 2019.
[41] P. M. Long và H. Sedghi. Giới hạn tổng quát hóa cho mạng thần kinh tích chập sâu. Trong Hội nghị quốc tế về Biểu diễn học, 2020.
[42] D. Madaan, J. Shin, và S. J. Hwang. Cắt tỉa thần kinh đối kháng với triệt tiêu lỗ hổng tiềm ẩn. Trong ICML, 2020.
[43] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, và A. Vladu. Hướng tới các mô hình học sâu chống lại các cuộc tấn công đối kháng. Trong Hội nghị quốc tế về Biểu diễn học, 2018.
[44] K. D. Maduranga, K. E. Helfrich, và Q. Ye. Mạng thần kinh tái phát unitary phức sử dụng biến đổi cayley có tỷ lệ. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 33, trang 4528–4535, 2019.
[45] C. H. Martin và M. W. Mahoney. Tự regularization ngầm trong mạng thần kinh sâu: Bằng chứng từ lý thuyết ma trận ngẫu nhiên và ý nghĩa cho việc học. Journal of Machine Learning Research, 22(165):1–73, 2021.
[46] E. Massart. Regularizer trực giao trong học sâu: làm thế nào để xử lý ma trận hình chữ nhật? Trong 2022 26th International Conference on Pattern Recognition (ICPR), trang 1294–1299. IEEE, 2022.
[47] L. Meunier, B. J. Delattre, A. Araujo, và A. Allauzen. Một quan điểm hệ thống động cho mạng thần kinh lipschitz. Trong Hội nghị quốc tế về Học máy, trang 15484–15500. PMLR, 2022.
[48] J. Pennington, S. Schoenholz, và S. Ganguli. Hồi sinh sigmoid trong học sâu thông qua đẳng cự động: lý thuyết và thực hành. Advances in neural information processing systems, 30, 2017.

--- TRANG 16 ---
[49] J. R. Rice. Một lý thuyết về điều kiện. SIAM Journal on Numerical Analysis, 3(2):287–310, 1966.
[50] D. A. Roberts, S. Yaida, và B. Hanin. Các nguyên lý của Lý thuyết Học sâu. Cambridge University Press, tháng 5 năm 2022.
[51] S. Schotthöfer, E. Zangrando, J. Kusch, G. Ceruti, và F. Tudisco. Vé số cấp thấp: tìm kiếm mạng thần kinh cấp thấp hiệu quả thông qua phương trình vi phân ma trận. Trong Advances in Neural Information Processing Systems, 2022.
[52] V. Sehwag, S. Wang, P. Mittal, và S. Jana. Hydra: Cắt tỉa mạng thần kinh robust đối kháng. NeurIPS, 2020.
[53] U. Shalit, D. Weinshall, và G. Chechik. Học trực tuyến trong đa tạp của ma trận cấp thấp. Advances in neural information processing systems, 23, 2010.
[54] K. Simonyan và A. Zisserman. Mạng tích chập rất sâu cho nhận dạng hình ảnh quy mô lớn. arXiv preprint arXiv:1409.1556, 2014.
[55] S. P. Singh, G. Bachmann, và T. Hofmann. Hiểu biết phân tích về cấu trúc và cấp của bản đồ Hessian mạng thần kinh. Trong Advances in Neural Information Processing Systems, tập 34, 2021.
[56] S. Singla và S. Feizi. Tích chập trực giao xiên. Trong Hội nghị quốc tế về Học máy, trang 9756–9766. PMLR, 2021.
[57] S. Singla, S. Singla, và S. Feizi. Cải thiện robustness l2 xác định trên cifar-10 và cifar-100. Trong Hội nghị quốc tế về Biểu diễn học, 2021.
[58] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, và R. Fergus. Tính chất kỳ lạ của mạng thần kinh. Trong Hội nghị quốc tế về Biểu diễn học (ICLR), 2014.
[59] D. Terjék. Regularization lipschitz đối kháng. Trong Hội nghị quốc tế về Biểu diễn học, 2020.
[60] L. N. Trefethen và D. Bau. Đại số tuyến tính số. SIAM, 1997.
[61] A. Trockman và J. Z. Kolter. Trực giao hóa các lớp tích chập với biến đổi cayley. Trong Hội nghị quốc tế về Biểu diễn học, 2021.
[62] T. Tsiligkaridis và J. Roberts. Về huấn luyện đối kháng frank-wolfe. Trong ICML 2021 Workshop on Adversarial Machine Learning, 2021.
[63] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, và A. Madry. Robustness có thể mâu thuẫn với độ chính xác. Trong Hội nghị quốc tế về Biểu diễn học, 2018.
[64] Y. Tsuzuku, I. Sato, và M. Sugiyama. Huấn luyện biên lipschitz: Chứng nhận có thể mở rộng của bất biến nhiễu loạn cho mạng thần kinh sâu. Advances in neural information processing systems, 31, 2018.
[65] M. Udell và A. Townsend. Tại sao ma trận dữ liệu lớn xấp xỉ cấp thấp? SIAM Journal on Mathematics of Data Science, 1(1):144–160, 2019.
[66] A. Uschmajew và B. Vandereycken. Phương pháp hình học trên đa tạp ma trận và tensor cấp thấp. Handbook of variational methods for nonlinear geometric data, trang 261–313, 2020.
[67] H. Wang, S. Agarwal, và D. Papailiopoulos. Pufferfish: mô hình hiệu quả giao tiếp không có chi phí thêm. Proceedings of Machine Learning and Systems, 3:365–386, 2021.
[68] Y.-L. Wu, H.-H. Shuai, Z.-R. Tam, và H.-Y. Chiu. Chuẩn hóa gradient cho mạng đối kháng sinh tạo. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 6373–6382, 2021.
[69] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. Schoenholz, và J. Pennington. Đẳng cự động và lý thuyết trường trung bình của CNN: Cách huấn luyện mạng tích chập vanilla 10.000 lớp. Trong Hội nghị quốc tế về Học máy, trang 5393–5402. PMLR, 2018.
[70] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, và Y. Chen. Học mạng thần kinh sâu cấp thấp thông qua regularization trực giao vector kỳ dị và sparsification giá trị kỳ dị. Trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), trang 2899–2908, 2020.

--- TRANG 17 ---
[71] S. Ye, K. Xu, S. Liu, H. Cheng, J.-H. Lambrechts, H. Zhang, A. Zhou, K. Ma, Y. Wang, và X. Lin. Robustness đối kháng vs. nén mô hình, hay cả hai? Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), tháng 10 năm 2019.
[72] T. Yu, J. Li, Y. Cai, và P. Li. Xây dựng tích chập trực giao một cách rõ ràng. Trong Hội nghị quốc tế về Biểu diễn học, 2022.
[73] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, và M. Jordan. Đánh đổi có nguyên tắc lý thuyết giữa robustness và độ chính xác. Trong Hội nghị quốc tế về học máy, trang 7472–7482. PMLR, 2019.
