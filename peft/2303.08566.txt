# 2303.08566.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.08566.pdf
# File size: 2108138 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning
Haoyu He1Jianfei Cai1Jing Zhang2Dacheng Tao2Bohan Zhuang1†
1ZIP Lab, Monash University2The University of Sydney
Abstract
Visual Parameter-Efficient Fine-Tuning (PEFT) has be-
come a powerful alternative for full fine-tuning so as to
adapt pre-trained vision models to downstream tasks, which
only tunes a small number of parameters while freezing the
vast majority ones to ease storage burden and optimiza-
tion difficulty. However, existing PEFT methods introduce
trainable parameters to the same positions across differ-
ent tasks depending solely on human heuristics and neglect
the domain gaps. To this end, we study where to intro-
duce and how to allocate trainable parameters by proposing
a novel Sensitivity-aware visual Parameter-efficient fine-
Tuning (SPT) scheme, which adaptively allocates trainable
parameters to task-specific important positions given a de-
sired tunable parameter budget. Specifically, our SPT first
quickly identifies the sensitive parameters that require tun-
ing for a given task in a data-dependent way. Next, our
SPT further boosts the representational capability for the
weight matrices whose number of sensitive parameters ex-
ceeds a pre-defined threshold by utilizing existing struc-
tured tuning methods, e.g., LoRA [23] or Adapter [22],
to replace directly tuning the selected sensitive parameters
(unstructured tuning) under the budget. Extensive exper-
iments on a wide range of downstream recognition tasks
show that our SPT is complementary to the existing PEFT
methods and largely boosts their performance, e.g., SPT im-
proves Adapter with supervised pre-trained ViT-B/16 back-
bone by 4.2% and 1.4% mean Top-1 accuracy, reaching
SOTA performance on FGVC and VTAB-1k benchmarks, re-
spectively. Source code is at https://github.com/
ziplab/SPT .
1. Introduction
To effectively adapt the pre-trained representations to the
downstream tasks, the de-facto choice is full fine-tuning,
which initializes the model with the pre-trained weights and
tunes all the parameters. However, vanilla full fine-tuning
needs to store a separate instance of parameters for each
†Corresponding author. E-mail: bohan .zhuang @gmail .com
TaskTAdapter(a)Any Structured Tuning MethodAny Structured Tuning Method
TaskT(b)Task1……Task1
AdapterFigure 1: (a) Existing PEFT methods, such as Adapter [22]
introduce trainable parameters to the same positions for all
downstream tasks. These methods design task-agnostic po-
sitions to employ trainable parameters relying on heuris-
tics and neglect consideration of the distinct domain gaps
and characteristics for the downstream tasks. (b) Our
Sensitivity-aware visual Parameter-efficient fine-Tuning
(SPT) introduces trainable parameters to the task-specific
important positions and allocates them with both unstruc-
tured and structured tuning granularities, simultaneously.
For structured tuning, SPT can exploit any existing struc-
tured tuning methods, such as LoRA [23] or Adapter [22].
Red lines and blocks represent trainable parameters and
modules, while blue lines represent frozen parameters.
task and each deployment scenario. It can be extremely
storage-intensive as the storage cost grows linearly with the
number of possible cases, considering there are vast vari-
eties of downstream tasks and dynamic deployment envi-
ronments, especially when deploying the large vision mod-
els [13, 34, 58] to mobile systems. For example, even stor-
ing a single large pre-trained ViT-H [13] model on a local
disk requires at least 2.3GB, while the Top-10 U.S. apps
required only collectively 2.2GB in May 2021.1
Notably, an emerging solution is to replace vanilla
fine-tuning with visual Parameter-Efficient Fine-Tuning
(PEFT) [24, 10, 65, 25], which only tunes a small num-
1https://sensortower.com/blog/ios-app-size-growth-2021arXiv:2303.08566v2  [cs.CV]  31 Aug 2023

--- PAGE 2 ---
ber of trainable parameters while freezing the vast ma-
jority ones that are shared by multiple tasks. As PEFT
approaches exhibit less than 1% of trainable parameters,
the storage burden is largely alleviated. Another attrac-
tive property of PEFT is that tuning fewer parameters eases
the optimization difficulty and mitigates the overfitting is-
sue when adapting large pre-trained models on the target
dataset, thereby achieving comparable or even better perfor-
mance than vanilla fine-tuning [24]. Although promising,
the existing PEFT approaches introduce trainable parame-
ters to the same positions for all downstream tasks, relying
on human heuristics and neglecting task-specific domain
gaps and characteristics, which limits their performance.
For instance, in a task-agnostic manner, Prompt Tuning-
deep [24] and Adapter [22] respectively add trainable pa-
rameters to multi-head self-attention and feed-forward net-
work layers for all distinct tasks as depicted in Figure 1 (a).
To address this fundamental challenge, we explore where
to introduce andhow to allocate trainable parameters un-
der a desired parameter budget by presenting a novel
Sensitivity-aware visual Parameter-efficient fine- Tuning
(SPT) scheme that identifies the task-specific important po-
sitions to adaptively allocate trainable parameters. Since the
pre-trained weights at distinct positions have varying contri-
butions for different downstream tasks [60, 28, 39], we first
propose a new criterion to quickly identify the task-specific
sensitive parameters that require tuning in a data-dependent
way. Inspired by model pruning metrics [46, 37, 3, 4], we
propose to measure the parameter sensitivity with the loss
reduction when being tuned, which can be approximated
by a first-order Taylor expansion derived within a single
forward and backward pass ahead of fine-tuning in one-
shot. Our sensitivity criterion is simple and effective, which
can identify the task-specific important positions to intro-
duce trainable parameters for any backbone quickly. For
instance, calculating the sensitivity for ViT-B/16 backbone
takes only 5.5 seconds with a single GPU on any of the
VTAB-1k datasets.
With our criterion, we empirically observe that the pro-
portions of the sensitivity parameters for each block indeed
vary markedly across different tasks in Section 4.4. To al-
locate the trainable parameters under a desired trainable pa-
rameter budge, an intuitive solution is to directly tune the
most sensitive weight connections, which we name unstruc-
tured tuning. Despite its simplicity and flexibility, unstruc-
tured tuning only tunes a few parameters which still lacks
representational capability and is challenging to bridge the
domain gap. To this end, we propose to further incorpo-
rate structured tuning to replace unstructured tuning at the
sensitive weight matrices whose numbers of sensitive pa-
rameters exceed a pre-defined threshold to improve the rep-
resentational capability under a similar parameter budget.
Structured tuning can be implemented by any parameter-efficient structured tuning methods [23, 10, 25, 24] that di-
rectly adjust the hidden representations, e.g., inserting an
adapter module sequentially after the sensitive weight ma-
trices. Therefore, our SPT adaptively combines both un-
structured and structured tuning granularity and allocates
trainable parameters with high flexibility and representa-
tional capability for each distinct downstream task.
This paper has the following key contributions. 1) We
make the pioneering exploration to identify the task-specific
important positions under the PEFT setting, which is fast,
effective, versatile to be applied to various backbones with
different pre-training strategies, and orthogonal to the exist-
ing PEFT methods. 2) Based on the sensitivity criterion, we
propose a trainable parameter allocation strategy that adap-
tively combines both unstructured and structured tuning un-
der a desired parameter budget to achieve high flexibility,
large capacity, and favorable trade-off between parameter
efficiency and accuracy. 3) Extensive experiments on a total
of 24 downstream recognition tasks with both plain and hi-
erarchical vision Transformer backbones under supervised
and self-supervised pre-trainings show that our SPT is com-
plementary to the existing PEFT methods and boosts their
performance by large margins. For instance, SPT improves
Adapter [22] by 4.2% mean Top-1 accuracy, outperforming
the SOTA PEFT methods on the FGVC benchmark.
2. Related Work
Parameter-efficient fine-tuning. Full fine-tuning is the
most predominant approach when adapting a large-scale
pre-trained model to downstream tasks, where the model
is initialized from the pre-trained weights with all pa-
rameters trainable. Yet, when a model becomes larger,
parameter-efficient fine-tuning [29, 30] is highly desirable,
which tunes only a tiny portion of parameters to allevi-
ate the storage burden. The general PEFT approaches
can be categorized into addition-based PEFT methods and
reparameterization-based PEFT methods.
Addition-based PEFT attaches additional trainable pa-
rameters to the backbone and only tunes these parame-
ters. Apart from Prompt tuning [24] and Adapter [22], re-
cent addition-based methods study connecting or combin-
ing existing PEFT methods. For instance, He et al . [19]
connect Prompt tuning and Adapter and provide a unified
view that all PEFT approaches share the same design to ad-
just the hidden representations. Zhang et al . [65] search
for the optimal configurations to combine multiple PEFT
approaches following once-for-all scheme [6, 56]. Since
the additional parameters require extra computations com-
pared to full fine-tuning, a few recent works [48, 50] design
specific architectures to avoid storing the intermediate ac-
tivations, thereby alleviating the fine-tuning memory cost.
However, it is noteworthy that enhancing training efficiency
is not the primary objective of our work.

--- PAGE 3 ---
Reparameterization-based PEFT aims to avoid extra
computational costs by tuning parameters that are inher-
ently in or can be reparameterized into the backbone during
inference. Prior works select the parameters that are inher-
ently in the backbone, including the bias terms [61], the last
several layers [60, 5], and weight connections [15, 66]. To
reparameterize new parameters into the backbone [18, 31],
representative work LoRA [23] optimizes two low-rank ma-
trices which can be further merged into the weight matri-
ces. In contrast to the aforementioned works, we argue the
importance of tuning parameters at task-specific important
positions and quickly identify them with our proposed pa-
rameter sensitivity criterion before tuning, which is comple-
mentary to and provides valuable guidance for the existing
PEFT methods. Moreover, our SPT can also be inference-
efficient when implementing structured tuning with any
reparameterization-based structured tuning method. Re-
cently, SSF [31] is proposed to introduce trainable scaling
and shifting parameters that can be absorbed into the previ-
ous linear layers. However, it cannot scale to higher train-
able parameter budgets and requires a complex and time-
consuming hyper-parameter search for learning rate, weight
decay, and drop-path rate on each individual dataset, thus is
not directly comparable to our method.
Task-specific transfer learning. The effectiveness
of transferring pre-trained models to downstream tasks
strongly depends on the relationship between the source and
target tasks [45, 55, 28, 42]. This has motivated the com-
munity to explore the optimal pre-training data [12, 59],
model [49, 40], and weights [17, 57] for the target task. To
seek suitable task-specific pre-training data , Cui et al. [12]
select the source domain data from the top-k most simi-
lar classes measured by Earth Mover’s Distance; Yoon et
al. [59] weight each class in the source domain with rein-
forcement learning; and Puigcerver et al. [43] first train a di-
verse set of experts and then select the most relevant expert
for each target task. Another line of work selects a suitable
pre-trained model for the target task ahead of fine-tuning
by measuring the transferability of pre-trained models to the
target domain with interclass covariance between the source
data and target classes [2] or conditional cross-entropy [49]
between the source and target labels. Considering the trans-
ferability of the feature representations at distinct layers
for the same pre-trained model is different [60, 39], re-
cent works [16, 47] endeavour transfer task-specific weights
by freezing some pre-trained weights and fine-tuning the
rest. For example, the task-specific fine-tuned weights
are selected by learning a policy network with Gumbel-
Softmax [17], optimizing a sparse mask with L0norm [15],
and learning binary gates for each parameter [66]. Our
SPT also adaptively selects task-specific parameters. In
contrast to the previous work, we 1) derive task-specific
important positions prior to fine-tuning with only a singleforward and backward pass, which is computationally ef-
ficient; 2) mask the gradients for insensitive parameters in
unstructured tuning with fixed binary masks, thereby having
more affordable fine-tuning memory than optimizing learn-
able binary masks in [15, 66]. Moreover, we are pioneering
work to adaptively allocate task-specific trainable parame-
ters with both fine-grained unstructured and coarse-grained
structured tuning granularities to achieve both high flexibil-
ity and representational capability.
3. Method
Our sensitivity-aware visual parameter-efficient fine-
tuning consists of two stages. In the first stage, SPT mea-
sures the task-specific sensitivity for the pre-trained param-
eters (Section 3.1). Based on the parameter sensitivity and a
given parameter budget, SPT then adaptively allocates train-
able parameters to task-specific important positions (Sec-
tion 3.2).
3.1. Task-specific Parameter Sensitivity
Recent research has observed that pre-trained backbone
parameters exhibit varying feature patterns [44, 38] and crit-
icality [64, 9] at distinct positions. Moreover, when trans-
ferred to downstream tasks, their efficacy varies depending
on how much pre-trained features are reused and how well
they adapt to the specific domain gap [60, 28, 39]. Moti-
vated by these observations, we argue that not all parame-
ters contribute equally to the performance across different
tasks in PEFT and propose a new criterion to measure the
sensitivity of the parameters in the pre-trained backbone for
a given task.
Specifically, given the training dataset Dtfor the
t-th task and the pre-trained model weights w=
{w1, w2, . . . , w N} ∈RNwhere Nis the total number of
parameters, the objective for the task is to minimize the em-
pirical risk: minwE(Dt,w). We denote the parameter sen-
sitivity set as S={s1, . . . , s N}and the sensitivity snfor
parameter wnis measured by the empirical risk difference
when tuning it:
sn=E(Dt,w)−E(Dt,w|wn=w∗
n), (1)
where w∗
n= argmin
wn(E(Dt,w)). We can reparameterize
the tuned parameters as w∗
n=wn+ ∆ wn, where ∆wnde-
notes the update for wnafter tuning. Here we individually
measure the sensitivity of each parameter, which is reason-
able given that most of the parameters are frozen during
fine-tuning in PEFT. However, it is still computationally in-
tensive to compute Eq. (1) for two reasons. Firstly, getting
the empirical risk for Nparameters requires forwarding the
entire network Ntimes, which is time-consuming. Sec-
ondly, it is challenging to derive ∆wn, as we have to tune
each individual wnuntil convergence.

--- PAGE 4 ---
Top-𝜏ParamsParameterSensitivity𝒮Foreachweightmatrix𝑾,∑!"#$!"×$#$%𝑴! 	≥ σ&'(	?UnstructuredTuning
AnyStructuredTuningMethodFigure 2: Overview of our trainable parameter allocation strategy. With the parameter sensitivity set S, we first get the
top-τsensitive parameters. Instead of directly tuning these sensitive parameters, we also boost the representational capability
by replacing unstructured tuning with structured tuning at sensitive weight matrices that have a large number of sensitive
parameters, which can be implemented by an existing structured tuning method, e.g., LoRA [23] and Adapter [22]. Red lines
and blocks represent trainable parameters and modules, while blue lines represent frozen parameters.
Algorithm 1 Computing task-specific parameter sensitivi-
ties
Input: Pre-trained model with network parameters w,
training set Dtfor the t-th task, and number of training
samples Cused to calculate the parameter sensitivities
Output: Sensitivity set S={s1, . . . , s N}
Initialize S={0}N
fori∈ {1, . . . , C }do
Get the i-th training sample of Dt
Compute loss E
Compute gradients g
forn∈ {1, . . . , N }do
Update sensitivity for the n-th parameter: sn=
sn+g2
n
end for
end for
To overcome the first barrier, we simplify the empirical
loss by approximating snin the vicinity of wby its first-
order Taylor expansion
s(1)
n=−gn∆wn, (2)
where the gradients g=∂E/∂w, and gnis the gradient
of the n-th element of g. To address the second barrier,
following [33, 8], we take the one-step unrolled weight as
the surrogate for w∗
nand approximate ∆wnin Eq. (2) with
a single step of gradient descent. We can accordingly get
s(1)
n≈g2
nϵ, where ϵis the learning rate. Since ϵis the same
for all parameters, we can eliminate it when comparing the
sensitivity with the other parameters and finally get
s(1)
n≈g2
n. (3)
Therefore, the sensitivity of a parameter can be effi-
ciently measured by its potential to reduce the loss on
the target domain. Note that although our criterion
draws inspiration from pruning work [37], it is distinct
from it. [37] measures the parameter importance by
the squared change in loss when removing them, i.e.,
(E(Dt,w)−E(Dt,w|wn= 0))2and finally derives the
parameter importance by (gnwn)2, which is different from
our formulations in Eqs. (1) and (3).
In practice, we accumulate Sfrom a total number of
Ctraining samples ahead of fine-tuning to generate accu-rate sensitivity as shown in Algorithm 1, where Cis a pre-
defined hyper-parameter. In Section 4.3, we show that em-
ploying only 400 training samples is sufficient for getting
reasonable parameter sensitivity, which requires only 5.5
seconds with a single GPU for any VTAB-1k dataset with
ViT-B/16 backbone [13].
3.2. Adaptive Trainable Parameters Allocation
Our next step is to allocate trainable parameters based
on the obtained parameter sensitivity set Sand a desired
parameter budget τ. A straightforward solution is to di-
rectly tune the top- τmost sensitive unstructured connec-
tions (parameters) while keeping the rest frozen, which we
name unstructured tuning. Specifically, we select the top- τ
most sensitive weight connections in Sto form the sensi-
tive weight connection set T. Then, for a weight matrix
W∈Rdin×dout, we can get a binary mask M∈Rdin×dout
computed by
Mj=1Wj∈ T
0Wj/∈ T, (4)
where WjandMjare the j-th element in WandM,
respectively. Accordingly, we can train the sensitive param-
eters by gradient descent and the updated weight matrix can
be formulated as W′←W−ϵgW⊙M, where gWis the
gradient for W.
However, considering PEFT approaches generally limit
the proportion of trainable parameters to less than 1%, tun-
ing only a small number of unstructured weight connections
might not have enough representational capability to han-
dle the downstream datasets with large domain gaps from
the source pre-training data. Therefore, to improve the rep-
resentational capability, we propose to replace unstructured
tuning with structured tuning at the sensitive weight matri-
ces that have a high number of sensitive parameters. To pre-
serve the parameter budget, we can implement structured
tuning with an existing efficient structured tuning PEFT
method [23, 10, 22, 25] that learns to directly adjust all hid-
den dimensions at once. We depict an overview of our train-
able parameter allocation strategy in Figure 2. For exam-
ple, we can employ the low-rank reparameterization trick
LoRA [23] to the sensitive weight matrices and the one-step
update for Wcan be formulated as

--- PAGE 5 ---
W′=W+WdownWup ifPdin×dout
j=0Mj≥σopt
W−ϵgW⊙M otherwise,
(5)where Wdown∈Rdin×randWup∈Rr×doutare two learn-
able low-rank matrices to approximate the update of Wand
rank ris a hyper-parameter where r≪min(din, dout).
In this way, we perform structured tuning on Wwhen
its number of sensitive parameters exceeds σopt, whose
value depends on the pre-defined type of structured tuning
method. For example, since implementing structured tun-
ing with LoRA requires 2×din×dout×rtrainable pa-
rameters for each sensitive weight matrix, we set σLoRA←
2×din×dout×rto ensure that the number of trainable
parameters introduced by structured tuning is always equal
to or lower than the number of sensitive parameters.
In this way, our SPT adaptively incorporates both struc-
tured and unstructured tuning granularities to enable higher
flexibility and stronger representational power, simultane-
ously. In Section 4.3, we show that structured tuning is
important for the downstream tasks with larger domain
gaps and both unstructured and structured tuning contribute
clearly to the superior performance of our SPT.
4. Experiments
4.1. Experimental Setup
Datasets and metrics. We evaluate our SPT on total 24
downstream tasks in two groups following [24]. 1) FGVC is
a benchmark for fine-grained visual classification, including
CUB-200-2011 [54], NABirds [52], Oxford Flowers [41],
Stanford Cars [14], and Stanford Dogs [26] datasets. Each
FGVC dataset contains between 55 to 200 classes and a few
thousand images for train, validation, and test. We follow
the validation splits in [24] if the validation set is unavail-
able. 2) VTAB-1k [62] is a large-scale transfer learning
benchmark consisting of a collection of 19 visual classifi-
cation tasks. VTAB-1k can further be divided into three
groups, including Natural tasks with natural images, Spe-
cialized tasks with images captured by specialized equip-
ment, e.g., medical images, and Structured tasks with im-
ages mostly generated from synthetic environments. Each
of the VTAB-1k dataset has only 800 training and 200 val-
idation samples, while the test set sizes vary. We use top-1
accuracy (%) averaged within each group as our main met-
ric following [24].
Pre-trained backbones. We conduct experiments on the
plain vision Transformer backbone ViT-B/16 [13] that is
pre-trained on ImageNet [27] with different pre-training
strategies following [24], including supervised pre-training
and self-supervised pre-training with MAE [20] and MoCo
v3 [11] following [24]. We also conduct experiments on
the representative hierarchical vision Transformer backbone
Swin-B [34] under supervised pre-training.
Contenders. We categorize the baseline methods intoaddition-based and reparameterization-based PEFT meth-
ods as introduced in Section 2. Unless specified, all baseline
methods keep the backbone frozen. Addition-based meth-
ods require extra computations during inference, includ-
ing M LP-k, PROMPT -SHALLOW [24], P ROMPT -DEEP [24],
ADAPTER -k[22], A DAPT FORMER [10], and NOAH [65].
Reparameterization-based methods have no additional com-
putational overhead during inference, including L INEAR ,
PARTIAL -k, BIAS[61], and L ORA-k[23]. Here krepre-
sents the number of bottleneck dimension in A DAPTER -k
and L ORA-k. We also compare with full fine-tuning which
is denoted by F ULL. We introduce the details of these meth-
ods in the supplementary material.
We also introduce two variants of our SPT: addition-
based SPT-A DAPTER and reparameterization-based SPT-
LORA. SPT-A DAPTER directly adjusts the hidden repre-
sentations that are computed by sensitive weight matrices
following [22], while SPT-L ORA approximates updating
the sensitive weight matrices following [23]. For the two
variants, we follow the exact weight initializations that are
described in [23] and follow [65] to set the bottleneck di-
mension as 8.
Implementation details. Following [65], we use the
AdamW optimizer [36] with cosine learning rate decay and
set the batch size, learning rate, and weight decay as 64,
1×10−3, and1×10−4, respectively. We also follow [65] for
the standard data augmentation pipeline. We set the number
of training samples Cused to calculate our parameter sen-
sitivities in Algorithm 1 universally to be 800 for the main
experiments.
4.2. Main Results
We compare our method with the baseline methods un-
der different backbones, pre-training strategies, and tasks.
Visual recognition with ViT backbone. First, our pro-
posed SPT-A DAPTER and SPT-L ORA achieve the best
performance under different trainable parameter budgets
with supervised pre-trained ViT-B/16 backbone, as shown
in Table 1 and Figure 3 (a). For instance, SPT-A DAPTER
outperforms the SOTA method NOAH by a clear margin of
0.9% mean top-1 accuracy over the 19 VTAB-1k datasets
with fewer trainable parameters. We speculate that our SPT
variants allocate trainable parameters at task-specific posi-
tions compared to the heuristically selected positions in the
baseline methods, which contributes to our superior perfor-
mance. We also observe that our SPT-A DAPTER and SPT-
LORA achieve large performance gains over A DAPTER and
LORA variants, respectively. For example, SPT-A DAPTER
and SPT-L ORA with 0.41% trainable parameters respec-
tively improve A DAPTER -8 and L ORA-8 significantly by
4.0% and 3.3% mean accuracy on the FGVC benchmark.
This suggests that identifying task-specific important posi-
tions and combining both unstructured and structured tun-
ing granularities with SPT are complementary to the exist-

--- PAGE 6 ---
ViT-B/16 Total FGVC VTAB-1k
(85.8M) params Tuned / Total Mean Acc. Tuned / Total Natural Specialized Structured Mean Acc.
FULL 24.02× 100% 88.5 100% 75.9 83.4 47.6 69.0
Addition-based methods
MLP-3 1.35× 1.50% 79.8 1.42% 67.8 72.8 30.6 57.1
PROMPT -SHALLOW 1.04× 0.31% 84.6 0.13% 76.8 79.7 47.0 67.8
PROMPT -DEEP 1.18× 0.98% 89.1 1.14% 78.5 82.4 55.0 72.0
ADAPTER -8 1.06× 0.39% 85.5 0.23% 79.0 84.1 58.5 73.9
ADAPTER -32 1.19× 0.95% 85.6 0.71% 79.6 84.0 58.3 74.0
ADAPT FORMER 1.09× 0.44% 85.1 0.36% 80.6 85.4 58.5 74.8
NOAH - - - 0.52% 80.2 84.9 61.3 75.5
SPT-A DAPTER (Ours) 1.08× 0.41% 89.5 0.30% 81.3 85.3 60.8 75.8
SPT-A DAPTER (Ours) 1.10× 0.47% 89.8 0.44% 82.0 85.8 61.4 76.4
Reparameterization-based methods
LINEAR 1.02× 0.12% 79.3 0.04% 68.9 77.2 26.8 57.6
PARTIAL -1 3.00× 8.38% 82.6 8.30% 69.4 78.5 34.2 60.7
BIAS 1.05× 0.13% 88.4 0.13% 73.3 78.3 44.1 65.2
LORA-8 1.07× 0.55% 86.0 0.23% 79.5 84.6 60.5 74.9
LORA-16 1.18× 0.90% 84.8 0.69% 79.8 84.9 60.2 75.0
SPT-L ORA (Ours) 1.08× 0.41% 89.3 0.31% 81.5 85.6 60.7 75.9
SPT-L ORA (Ours) 1.15× 0.60% 90.1 0.63% 81.9 85.9 61.3 76.4
Table 1: Comparisons on FGVC and VTAB-1k [62] benchmarks using supervised pre-trained ViT-B/16 backbone pre-trained
on ImageNet-21k. “Total params” denotes the ratio of the total number of parameters needed for all downstream tasks relative
to the one for the pre-trained backbone, and “Tuned/Total” denotes the fraction of trainable parameters. Top-1 accuracy (%)
is reported. The best result is in bold , and the second-best result is underlined .
ViT-B/16 Total VTAB-1k MAE VTAB-1k MoCo v3
(85.8M) Params Tuned / Total Natural Specialized Structured Mean Acc. Tuned / Total Natural Specialized Structured Mean Acc.
FULL 38.02× 100% 59.3 79.7 53.8 64.3 100% 72.0 84.7 42.0 69.6
Addition-based methods
ADAPTER -8 1.08× 0.23% 57.2 78.4 54.7 63.4 0.23% 27.6 70.9 48.4 49.0
ADAPTER -32 1.28× 0.95% 55.3 78.8 53.3 62.5 0.99% 74.2 82.7 47.7 68.2
PROMPT -SHALLOW 1.02× 0.12% 40.0 69.7 27.5 45.7 0.12% 67.3 82.3 37.6 62.4
PROMPT -DEEP 1.05× 0.23% 36.0 60.6 26.6 41.1 0.07% 70.3 83.0 42.4 65.2
SPT-A DAPTER (Ours) 1.07× 0.26% 64.8 82.4 60.4 69.2 0.08% 76.1 84.9 60.1 73.7
SPT-A DAPTER (Ours) 1.13× 0.41% 65.6 82.7 60.7 69.7 0.30% 76.6 85.0 61.7 74.4
Reparameterization-based methods
LINEAR 1.02× 0.04% 18.9 52.7 23.7 32.1 0.04% 67.5 81.1 30.3 59.6
PARTIAL -1 4.16× 8.30% 58.4 78.3 47.6 61.5 8.30% 72.3 84.6 47.9 68.3
BIAS 1.06× 0.13% 54.6 75.7 47.7 59.3 0.13% 72.9 81.1 53.4 69.2
LORA-8 1.08× 0.23% 57.5 77.7 57.7 64.3 0.23% 21.2 66.7 45.1 44.3
LORA-16 1.28× 0.69% 57.3 77.1 59.9 64.8 0.69% 16.0 64.0 48.7 42.9
SPT-L ORA (Ours) 1.11× 0.29% 63.8 81.6 60.0 68.5 0.30% 76.5 85.4 63.0 75.0
SPT-L ORA (Ours) 1.23× 0.69% 65.4 82.4 61.5 69.8 0.50% 76.5 86.0 63.6 75.3
Table 2: Comparisons on VTAB-1k [62] benchmark using self-supervised ViT-B/16 backbone pre-trained by MAE [20] and
MoCo v3 [11]. “Total params” denotes the ratio of the total number of parameters needed for all downstream tasks relative
to the one for the pre-trained backbone, and “Tuned/Total” denotes the fraction of trainable parameters. Top-1 accuracy (%)
is reported. The best result is in bold , and the second-best result is underlined .
ing PEFT methods and boost their performance.
Second, SPT variants outperform baseline methods
and full fine-tuning by significant margins with the self-
supervised pre-trained ViT-B/16 backbones. As shown in
Table 2, existing PEFT approaches exhibit inferior results
than full fine-tuning with the self-supervised pre-trainedbackbones MAE and MoCo v3. It is worth noting that
previous PEFT methods yield inconsistent results with the
backbones of different pre-training strategies. In contrast,
SPT variants consistently outperform full fine-tuning. In
particular, SPT-A DAPTER achieves remarkable 5.8% and
5.5% mean top-1 accuracy gains over the best-performing

--- PAGE 7 ---
MethodTuned /
TotalNatural Specialized StructuredMean /
Acc.
FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5
PROMPT -SHALLOW 0.04% 79.9 82.5 37.8 66.7
PROMPT -DEEP 0.23% 76.8 84.5 53.4 71.6
ADAPTER -8 1.18% 81.7 87.3 61.2 76.7
SPT-A DAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
LINEAR 0.04% 73.5 80.8 33.5 62.6
PARTIAL -1 2.15% 73.1 81.7 35.0 63.3
LORA-8 1.18% 81.7 87.2 60.1 76.3
SPT-L ORA (ours) 0.49% 83.1 87.4 60.4 77.2
Table 3: Comparisons on VTAB-1k [62] benchmark with
supervised pre-trained Swin-B [34]. “Tuned/Total” denotes
the fraction of trainable parameters. Top-1 accuracy (%) is
reported. The best result is in bold .
baseline method on VTAB-1k benchmark with only 0.26%
and 0.08% trainable parameters for MAE and MoCo v3 pre-
trained backbones, respectively. Moreover, our observation
in supplementary material suggests that self-supervised pre-
trained ViT backbones have more diverse sensitivity distri-
butions and a higher variance in sensitivity across differ-
ent tasks than the supervised pre-trained one. This leads to
the conjecture that baseline methods which assign trainable
parameters to the same positions for all tasks may fail to
mitigate the distinct domain gaps in individual downstream
datasets, whereas our SPT allocates trainable parameters to
task-specific positions accurately.
Visual recognition with Swin and ConvNeXt backbones.
From Table 3, we observe that our SPT-L ORA and SPT-
ADAPTER also achieve SOTA performance with Swin-B
backbone on all dataset groups. We also follow VPT [24] to
apply SPT variants to ResNet-alike architecture ConvNeXt-
Base [35] and report the results in Table 4. We observe that
SPT variants achieve better trade-offs between accuracy and
parameter efficiency than the baseline methods. The results
further demonstrate the versatility and effectiveness of our
SPT.
Semantic segmentation. We follow VPT [24] to con-
duct semantic segmentation on ADE20k dataset. Following
the settings of [24], we apply our SPT to SETR-PUP [67]
framework with ViT-L backbone and only allocate train-
able parameters to the backbone with the head fully fine-
tuned. We report the mIoU results in Table 5. Notably,
SPT-L ORA and SPT-A DAPTER outperform the baseline
methods by large margins, indicating our SPT can be gen-
eralized to the semantic segmentation task. We report the
results for conducting only structured tuning for both SPT-
LORA and SPT-A DAPTER as it yields higher mIoU.
4.3. Ablation Study
Effect of the sensitivity criterion. We investigate the ef-
fectiveness of our sensitivity criterion on VTAB-1k by em-Method Tuned/Total Natural Specialized Structured Mean Acc.
FULL 100% 78.0 83.7 60.4 74.0
LoRA 0.79% 82.2 84.7 64.1 77.0
Adapter 0.47% 83.1 84.9 64.6 77.5
SPT-LoRA (ours) 0.57% 83.4 86.7 65.9 78.7
SPT-Adapter (ours) 0.36% 83.7 86.2 65.3 78.4
Table 4: Comparisons on VTAB-1k [62] benchmark
with supervised pre-trained ConvNeXt-Base [35] backbone.
“Tuned/Total” denotes the fraction of trainable parameters.
Top-1 accuracy (%) is reported. The best result is in bold .
Method mIoU-s.s. (%) mIoU-m.s. (%) Trainable Param.
VPT 44.0 45.6 15.8M
LoRA 43.9 45.9 14.7M
Adapter 44.4 46.6 14.6M
SPT-LoRA (ours) 45.4 47.5 14.6M
SPT-Adapter (ours) 45.2 47.2 14.6M
Table 5: Semantic Segmentation: Comparisons on
ADE20k [68] val with SETR [67] on ViT-L backbone. We
report both single-scale (s.s.) and multi-scale (m.s.) mIoU
results. The best result is in bold .
ploying structured tuning methods from [24, 22, 23] to the
task-specific sensitive weight matrices. Note that we do
not conduct unstructured tuning to ensure fair comparisons.
The results are presented in Figure 3 (b). Our criterion
brings consistent 1.1%, 1.6%, and 0.8% performance gains
for P ROMPT -DEEP , ADAPTER -32, and L ORA-16, respec-
tively, which demonstrates the effectiveness and versatility
of our sensitivity criterion to identify accurate task-specific
important positions.
Effect of structured and unstructured tuning. We inves-
tigate the effectiveness of unstructured and structured tun-
ing individually on VTAB-1k. The results are presented in
Table 6. We start by applying A DAPTER -8 to the sensitive
weight matrices identified by our sensitivity criterion (SPT-
ADAPTER w/o unstructured). We observe that our sensitiv-
ity criterion boosts the performance of all the dataset groups
by clear margins, which again demonstrates the importance
of our sensitivity criterion. Next, we observe that allocating
the trainable parameters to the unstructured sensitive weight
connections also brings accuracy improvement to the Natu-
ral and Specialized datasets from A DAPTER -8. However,
we find that structured tuning is especially important for
achieving good performance on Structured datasets. To fur-
ther investigate this phenomenon, we observe that Struc-
tured datasets have larger domain gaps from the pre-training
source domain [27] compared to Natural and Specialized
datasets as visualized in Figure 3 (c). We hence conjecture
that structured tuning has a higher representational capa-
bility than unstructured tuning which facilitates mitigating
the large domain gaps during fine-tuning (see the supple-
mentary material for visual examples). Finally, we observe

--- PAGE 8 ---
(b)(c)(a)LORA-8 74.9ADAPTER-32 + TSM75.6ADAPTER-8 73.9PROMPT-DEEP + TSM 73.1PROMPT-DEEP 67.8LORA-16 + TSM 75.8ADAPTER-32 74.0LORA-16 75.0PROMPT-DEEP 72.0Figure 3: (a) Accuracy vs. parameter efficiency with supervised pre-trained ViT-B/16 backbone on VTAB-1k [62]. SPT
variants perform favorably against the other PEFT approaches and are more scalable. (b) Applying other PEFT structured
tuning methods [24, 22, 23] to the task-specific sensitive weight matrices (denoted by TSM) identified by our criterion with
supervised pre-trained ViT-B/16 backbone on VTAB-1k. Our criterion brings consistent performance gain. (c) Domain vs.
performance gaps for different dataset groups in VTAB-1k [62]. The blue bars show the domain gaps between the source
domain (ImageNet [27]) and target domains, which are measured by Maximum Mean Discrepancy (MMD) distance [51].
The red line represents the performance gaps between SPT-A DAPTER w/o unstructured and w/o structured, using supervised
pre-trained ViT-B/16 backbone. The dataset groups are Natural, Specialized, and Structured. Structured tuning is important
for achieving good performance on Structured datasets with larger domain gaps.
MethodTuned /
TotalNatural Specialized StructuredMean /
Acc.
ADAPTER -8 0.23% 79.0 84.1 58.5 73.9
SPT-A DAPTER w/o unstructured 0.29% 81.2 85.1 60.3 75.5
SPT-A DAPTER w/o structured 0.34% 81.2 85.0 59.6 75.3
SPT-A DAPTER 0.30% 81.3 85.3 60.8 75.8
Table 6: Ablation study on structured and unstructured tun-
ing only with supervised pre-trained ViT-B/16 backbone.
Top-1 accuracy (%) is reported. We set different parameter
constraints to align the fractions of trainable parameters for
these cases. The best result is in bold
that incorporating both structured and unstructured tuning
at task-specific important positions achieves the highest per-
formance on all dataset groups.
Effect of number of training samples Cto get parame-
ter sensitivity. We investigate the effect of the number of
training images Cfor calculating our parameter sensitiv-
ity (Algorithm 1 of the main paper). We randomly sample
training samples and report the mean results over three runs
in Table 8. We find that our SPT is robust to the number
of training samples Cand randomly sampling 400 out of a
total of 800 training samples is sufficient to obtain accurate
task-specific important positions, e.g., calculating the sensi-
tivity for ViT-B/16 backbone takes only 5.5 seconds with a
single GPU on any of the VTAB-1k datasets and this com-
putation is required only once.
Computational cost analysis. We investigate the com-
putational cost of SPT-L ORA by comparing with full
fine-tuning, addition-based method P ROMPT -DEEP , and
reparameterization-based method L ORA-16. The results
are presented in Table 7. We observe that P ROMPT -DEEP
has higher inference latency and inference GPU memory
due to the additional prompts. In contrast, since the up-MethodInference Latency
(ms/img)Inference Memory
(GB)Fine-tuning Memory
(GB)
FULL 2.8 1.3 11.9
PROMPT -DEEP 3.8 1.9 13.2
LORA-16 2.8 1.3 8.2
SPT-L ORA w/o unstructured 2.8 1.3 8.3
SPT-L ORA 2.8 1.3 9.8
Table 7: Cost comparisons with ViT-B/16 backbone on the
VTAB-1k [62] benchmark with around 0.70% fractions of
the trainable parameters. We report the latency (ms/img)
and the peak memory usage (GB) with image resolution
224×224 on a single GeForce 3090 GPU. The best result
is in bold .
C 240 400 560 800
Mean Acc. 76.3 76.4 76.4 76.4
Table 8: Effect of the number of training samples used to get
the sensitivity for SPT-L ORA with supervised pre-trained
ViT-B/16 backbone on VTAB-1k [62]. Top-1 accuracy (%)
is reported. The best result is in bold .
dated parameters after fine-tuning can be reparameterized
and merged into the pre-trained model, our SPT-L ORA,
SPT-L ORA w/o unstructured tuning, and L ORA-16 are
more efficient than P ROMPT -DEEP during inference. How-
ever, we observe that our SPT-L ORA has slightly higher
fine-tuning memory than the full fine-tuning and L ORA-16
which is taken up by updating the unstructually-tuned pa-
rameters with sparse gradients in Eq. (5). Additionally, for
memory-intense scenarios, one can employ SPT-LoRA w/o
unstructured tuning for improved performance (0.8% Top-1
accuracy on VTAB-1k) and similar fine-tuning memory as
LoRA, as shown in Figure 3 (b) and Table 6.

--- PAGE 9 ---
Supervisedpre-trainedViT-B
𝑾!𝑾"𝑾#𝑾$%&𝑾'𝑾$%(𝑾!𝑾"𝑾#𝑾$%&𝑾'MAEpre-trainedViT-B
𝑾!𝑾"𝑾#𝑾$%&𝑾$%(𝑾'𝑾$%(MoCoV3pre-trainedViT-BFigure 4: Parameter sensitivity patterns under 0.4M trainable parameter budget for ViT-B/16 backbone with different pre-
training strategies on three sample tasks from VTAB-1k [63]. The proportions exhibit task-specific varying patterns in terms
of network depth (upper figures) and task-agnostic similar patterns in terms of operations (lower figures).
4.4. Observations on Sensitivity Patterns
Our sensitivity criterion identifies task-specific impor-
tant positions, which can reveal the contributions of the pre-
trained weights to different downstream tasks during trans-
fer learning. We visualize the proportions of the sensitive
parameters for the supervised pre-trained ViT-B/16 back-
bone under 0.4M trainable parameter budget in Figure 4.
First, we investigate the most sensitive blocks, whose num-
bers of sensitive parameters are summed and normalized
over the 12 ViT-B/16 blocks. We observe that the patterns
of the sensitive parameter proportions vary markedly across
different tasks, which echoes the observations made in [17].
This suggests that we should not introduce trainable param-
eters to the same positions for each individual task but al-
locate trainable parameters at task-specific ones as we pro-
posed. Next, we investigate the most insensitive weight ma-
trices within a block. A ViT block consists of a query Wq,
a keyWk, a value Wv, and an output Woweight matrices
in the multi-head self-attention layer and two weight matri-
cesWfc1andWfc2in the feed-forward network as elab-
orated in [53, 13]. We observe that the query Wqand key
Wkweight matrices have the lowest proportions of sensi-
tive parameters for all three sample tasks. Since Wqand
Wkare responsible for learning the attention scores which
indicate the pairwise similarity among the patches, we spec-
ulate that although domain changes, the patch relationships
learned during pre-training can be efficiently reused when
transferred to downstream classification tasks.5. Conclusion
In this paper, we have explored identifying and allo-
cating trainable parameters to task-specific important po-
sitions for visual parameter-efficient tuning. Specifically,
we have proposed a novel criterion to quickly measure the
sensitivity of the pre-trained parameters for each specific
task before fine-tuning. Based on the parameter sensitiv-
ity, we have proposed a trainable parameter allocation strat-
egy that adaptively combines both unstructured and struc-
tured tuning under a desired trainable parameter budget, en-
abling high representational capability and flexibility. Fi-
nally, we have conducted extensive experiments on a to-
tal of 24 downstream recognition tasks with both plain and
hierarchical vision Transformer backbones under different
pre-training strategies to demonstrate the versatility and ef-
fectiveness of our proposed SPT. Notably, we have shown
that our approach is complementary to the existing PEFT
methods and improves their performance significantly. In
the future, we will explore adapting large vision models
to more downstream tasks with SPT, e.g., dense prediction
and vision-and-language tasks, and improve the training ef-
ficiency of SPT for on-device training [7, 32].
Acknowledgement. We thank Jing liu and Ziyi Liu for their
helpful discussions. This research is partially supported by
Monash FIT Start-up Grant. Dr. Jing Zhang is supported by
the Australian Research Council project FL-170100117.

--- PAGE 10 ---
References
[1] A. F. Agarap. Deep learning using rectified linear units
(relu). arXiv preprint arXiv:1803.08375 , 2018. 12
[2] Y . Bao, Y . Li, S.-L. Huang, L. Zhang, L. Zheng, A. Zamir,
and L. Guibas. An information-theoretic approach to trans-
ferability in task transfer learning. In ICIP , pages 2309–
2313. IEEE, 2019. 3
[3] G. Bender, P.-J. Kindermans, B. Zoph, V . Vasudevan, and
Q. Le. Understanding and simplifying one-shot architecture
search. In ICML , pages 550–559. PMLR, 2018. 2
[4] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Smash:
one-shot model architecture search through hypernetworks.
InICLR , 2018. 2
[5] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix ´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR , pages 221–230, 2017. 3
[6] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once-for-all:
Train one network and specialize it for efficient deployment.
InICLR , 2020. 2, 12
[7] H. Cai, C. Gan, L. Zhu, and S. Han. Tinytl: Reduce mem-
ory, not parameters for efficient on-device learning. NeurIPS ,
33:11285–11297, 2020. 9
[8] H. Cai, L. Zhu, and S. Han. ProxylessNAS: Direct neural
architecture search on target task and hardware. In ICLR ,
2019. 4
[9] N. S. Chatterji, B. Neyshabur, and H. Sedghi. The intrigu-
ing role of module criticality in the generalization of deep
networks. In ICLR , 2020. 3
[10] S. Chen, C. Ge, Z. Tong, J. Wang, Y . Song, J. Wang, and
P. Luo. Adaptformer: Adapting vision transformers for scal-
able visual recognition. NeurIPS , 2022. 1, 2, 4, 5, 12
[11] X. Chen, S. Xie, and K. He. An empirical study of training
self-supervised vision transformers. In ICCV , pages 9640–
9649, 2021. 5, 6, 12, 15
[12] Y . Cui, Y . Song, C. Sun, A. Howard, and S. Belongie. Large
scale fine-grained categorization and domain-specific trans-
fer learning. In CVPR , pages 4109–4118, 2018. 3
[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,
G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image
is worth 16x16 words: Transformers for image recognition
at scale. In ICLR , 2021. 1, 4, 5, 9
[14] T. Gebru, J. Krause, Y . Wang, D. Chen, J. Deng, and L. Fei-
Fei. Fine-grained car detection for visual census estimation.
InAAAI , 2017. 5, 15
[15] D. Guo, A. Rush, and Y . Kim. Parameter-efficient transfer
learning with diff pruning. In ACL-IJCNLP , 2021. 3
[16] Y . Guo, Y . Li, L. Wang, and T. Rosing. Adafilter: Adaptive
filter fine-tuning for deep transfer learning. In AAAI , pages
4060–4066, 2020. 3
[17] Y . Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and
R. Feris. Spottune: transfer learning through adaptive fine-
tuning. In CVPR , pages 4805–4814, 2019. 3, 9[18] T. Hao, H. Chen, Y . Guo, and G. Ding. Consolidator: Mer-
gable adapter with group connections for vision transformer.
InICLR , 2023. 3
[19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig.
Towards a unified view of parameter-efficient transfer learn-
ing. In ICLR , 2022. 2
[20] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, and R. Girshick.
Masked autoencoders are scalable vision learners. In CVPR ,
pages 16000–16009, 2022. 5, 6, 12, 14
[21] D. Hendrycks and K. Gimpel. Gaussian error linear units
(gelus). arXiv preprint arXiv:1606.08415 , 2016. 12
[22] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone,
Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and
S. Gelly. Parameter-efficient transfer learning for nlp. In
ICML , pages 2790–2799, 2019. 1, 2, 4, 5, 7, 8, 12
[23] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li,
S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adap-
tation of large language models. In ICLR , 2022. 1, 2, 3, 4, 5,
7, 8, 12
[24] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hari-
haran, and S.-N. Lim. Visual prompt tuning. In ECCV , 2022.
1, 2, 5, 7, 8, 12
[25] S. Jie and Z.-H. Deng. Convolutional bypasses are better vi-
sion transformer adapters. arXiv preprint arXiv:2207.07039 ,
2022. 1, 2, 4
[26] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei.
Novel dataset for fine-grained image categorization. In
CVPRW , 2011. 5, 15
[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
sification with deep convolutional neural networks. NeurIPS ,
25, 2012. 5, 7, 8, 12, 16
[28] A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and
P. Liang. Fine-tuning can distort pretrained features and un-
derperform out-of-distribution. In ICLR , 2022. 2, 3
[29] B. Lester, R. Al-Rfou, and N. Constant. The power of scale
for parameter-efficient prompt tuning. In EMNLP , 2021. 2
[30] X. L. Li and P. Liang. Prefix-tuning: Optimizing continu-
ous prompts for generation. In IJCNLP , pages 4582–4597.
Association for Computational Linguistics, 2021. 2
[31] D. Lian, D. Zhou, J. Feng, and X. Wang. Scaling & shift-
ing your features: A new baseline for efficient model tuning.
NeurIPS , 2022. 3
[32] J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, C. Gan, and
S. Han. On-device training under 256kb memory. In
NeurIPS , 2022. 9
[33] H. Liu, K. Simonyan, and Y . Yang. DARTS: Differentiable
architecture search. In ICLR , 2019. 4
[34] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo. Swin transformer: Hierarchical vision transformer
using shifted windows. In ICCV , 2021. 1, 5, 7
[35] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and
S. Xie. A convnet for the 2020s. In CVPR , pages 11976–
11986, 2022. 7

--- PAGE 11 ---
[36] I. Loshchilov and F. Hutter. Fixing weight decay regulariza-
tion in adam, 2018. 5
[37] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz.
Importance estimation for neural network pruning. In CVPR ,
pages 11264–11272, 2019. 2, 4
[38] M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat,
F. Shahbaz Khan, and M.-H. Yang. Intriguing properties of
vision transformers. NeurIPS , 34:23296–23308, 2021. 3
[39] B. Neyshabur, H. Sedghi, and C. Zhang. What is being trans-
ferred in transfer learning? NeurIPS , 33:512–523, 2020. 2,
3
[40] C. Nguyen, T. Hassner, M. Seeger, and C. Archambeau.
Leep: A new measure to evaluate transferability of learned
representations. In ICML , pages 7294–7305. PMLR, 2020.
3
[41] M.-E. Nilsback and A. Zisserman. Automated flower clas-
sification over a large number of classes. In ICVGIP , pages
722–729. IEEE, 2008. 5, 15
[42] J. Plested and T. Gedeon. Deep transfer learning for image
classification: a survey. arXiv preprint arXiv:2205.09904 ,
2022. 3
[43] J. Puigcerver, C. Riquelme, B. Mustafa, C. Renggli, A. S.
Pinto, S. Gelly, D. Keysers, and N. Houlsby. Scal-
able transfer learning with expert models. arXiv preprint
arXiv:2009.13239 , 2020. 3
[44] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and
A. Dosovitskiy. Do vision transformers see like convolu-
tional neural networks? NeurIPS , 34:12116–12128, 2021.
3
[45] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Diet-
terich. To transfer or not to transfer. In NeurIPSW , 2005.
3
[46] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training
very deep networks. NeurIPS , 28, 2015. 2
[47] X. Sun, R. Panda, R. Feris, and K. Saenko. Adashare:
Learning what to share for efficient deep multi-task learning.
NeurIPS , 33:8728–8740, 2020. 3
[48] Y .-L. Sung, J. Cho, and M. Bansal. Lst: Ladder side-tuning
for parameter and memory efficient transfer learning. In
NeurIPS , 2022. 2
[49] A. T. Tran, C. V . Nguyen, and T. Hassner. Transferability and
hardness of supervised classification tasks. In ICCV , pages
1395–1405, 2019. 3
[50] C.-H. Tu, Z. Mai, and W.-L. Chao. Visual query tuning:
Towards effective usage of intermediate representations for
parameter and memory efficient transfer learning. In CVPR ,
2023. 2
[51] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
arXiv preprint arXiv:1412.3474 , 2014. 8
[52] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry,
P. Ipeirotis, P. Perona, and S. Belongie. Building a bird
recognition app and large scale dataset with citizen scientists:The fine print in fine-grained dataset collection. In CVPR ,
pages 595–604, 2015. 5
[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need. NeurIPS , pages 5998–6008, 2017. 9
[54] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. Tech. Rep. CNS-
TR-2011-001, California Institute of Technology , 2011. 5
[55] Z. Wang, Z. Dai, B. P ´oczos, and J. Carbonell. Characteriz-
ing and avoiding negative transfer. In CVPR , pages 11293–
11302, 2019. 3
[56] H. Wu, J. Xu, J. Wang, and M. Long. Autoformer: Decom-
position transformers with auto-correlation for long-term se-
ries forecasting. NeurIPS , 34:22419–22430, 2021. 2
[57] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and
F. Huang. Raise a child in large language model: Towards
effective and generalizable fine-tuning. In EMNLP , 2021. 3
[58] Y . Xu, Q. Zhang, J. Zhang, and D. Tao. Vitae: Vision trans-
former advanced by exploring intrinsic inductive bias. In
NeurIPS , 2021. 1
[59] J. Yoon, S. Arik, and T. Pfister. Data valuation using rein-
forcement learning. In ICML , pages 10842–10851. PMLR,
2020. 3
[60] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? NeurIPS , 27,
2014. 2, 3
[61] E. B. Zaken, Y . Goldberg, and S. Ravfogel. Bitfit: Sim-
ple parameter-efficient fine-tuning for transformer-based
masked language-models. In ACL, pages 1–9, 2022. 3, 5,
12
[62] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen,
C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neu-
mann, A. Dosovitskiy, et al. A large-scale study of represen-
tation learning with the visual task adaptation benchmark.
arXiv preprint arXiv:1910.04867 , 2019. 5, 6, 7, 8, 12, 13,
14, 15, 16
[63] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen,
C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neu-
mann, A. Dosovitskiy, et al. A large-scale study of represen-
tation learning with the visual task adaptation benchmark.
arXiv preprint arXiv:1910.04867 , 2019. 9
[64] C. Zhang, S. Bengio, and Y . Singer. Are all layers created
equal? arXiv preprint arXiv:1902.01996 , 2019. 3
[65] Y . Zhang, K. Zhou, and Z. Liu. Neural prompt search. arXiv
preprint arXiv:2206.04673 , 2022. 1, 2, 5, 12
[66] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch ¨utze. Masking as
an efficient alternative to finetuning for pretrained language
models. In EMNLP , 2020. 3
[67] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu,
J. Feng, T. Xiang, P. H. Torr, et al. Rethinking semantic
segmentation from a sequence-to-sequence perspective with
transformers. In CVPR , pages 6881–6890, 2021. 7
[68] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
ralba. Scene parsing through ade20k dataset. In CVPR , pages
633–641, 2017. 7

--- PAGE 12 ---
Appendix
We organize our supplementary material as follows.
• In Section 6, we introduce more details about the contenders.
• In Section 7, we show more sensitivity patterns for ViT-B/16 with various pre-training strategies.
• In Section 8, we show some dataset samples from ImageNet [27] and VTAB-1k [62].
• In Tables 9 and 10, we show per-task results for our SPT variants on FGVC and VTAB-1k benchmarks, respectively.
6. More Details of Contenders
• FULL: fully tunes all the backbone and classification head parameters.
• L INEAR : freezes all the backbone parameters and only tunes a linear classification head.
• B IAS[61]: freezes all the backbone parameters except for the bias terms and also tunes the linear classification head.
• PARTIAL -k: freezes all the backbone parameters except for the last klayers and also tunes the linear classification head
as described in [24].
• M LP-k: freezes all the backbone parameters and tunes the classification head which is implemented by a trainable
k-layer multi-layer perceptron as described in [24].
• PROMPT -SHALLOW [24]: freezes all the backbone parameters while introducing additional trainable prompts to the
input space of the pretrained ViT.
• PROMPT -DEEP [24]: freezes all the backbone parameters while appending additional trainable prompts to the sequence
in the multi-head self-attention layer of each ViT block.
• A DAPTER -k[22]: freezes all the backbone parameters while adding a down projection, a ReLU [21] non-linearity, and
an up projection layer sequentially in the feed-forward network (FFN) of each visual Transformer block. We follow the
training details of [65] to achieve better performance.
• L ORA-k[23]: freezes all the backbone parameters while adding a concurrent branch including two low-rank matrices
to the weight matrices in the multi-head self-attention layers to approximate efficiently updating them. The low-rank
matrices can be merged into the backbone weights after fine-tuning. We follow the training details of [65] to achieve
better performance.
• A DAPT FORMER [10]: freezes all the backbone parameters while adding a concurrent branch including a down projec-
tion, a ReLU [1] non-linearity, an up projection layer, and a pre-defined scaling factor to the FFN layer of each ViT
block.
• NOAH [65]: searches for an optimal configuration with a once-for-all [6] network that includes trainable prompts,
adapter modules, and LoRA modules, which requires a longer training schedule than the other VPET methods.
7. More Parameter Sensitivity Patterns
We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo
V3 [11]) and datasets sampled from FGVC benchmark [24]. We visualize the proportions of the sensitive parameters under
0.4M trainable parameter budget. Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViT-
B/16 are shown in Figures 5, 6, 7. Visualizations of sampled FGVC datasets with supervised pre-trained ViT-B/16 are shown
in Figure 8. We find our observations in the main paper are general: the proportions of the sensitive parameter exhibit: 1)
dataset-specific varying patterns in terms of network depth; and 2) dataset-agnostic similar patterns in terms of operations.
We empirically find that the self-supervised pre-trained backbones have higher sensitivity variances than the supervised pre-
trained one across the 19 downstream tasks. In particular, the variance of ViT-B/16 pre-trained with MAE [20] is twice as
large as that of the supervised pre-trained ViT-B/16. We speculate that our SPT variants can better handle the large variances
for self-supervised pre-trained backbones (Table 2 of the main paper) by identifying task-specific positions to introduce the
trainable parameters.
8. Dataset Samples for the Source and Target Domains
We visualize some sampled images from the source domain (ImageNet [27]) and the target domains (VTAB-1k [62]) in
Figure 10. We observe that the images from the Natural tasks of VTAB-1k are relatively more similar to the source domain
compared to those from the Structured tasks of VTAB-1k, which aligns with our observation that Structured tasks have large
domain gaps. As structured tuning improves the performance of Structured datasets (Section 4.3 of the main paper), we
speculate that structured tuning facilitates mitigating such large domain gaps.

--- PAGE 13 ---
Figure 5: The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget with supervised pre-
trained ViT-B/16 backbone. We sample six tasks from VTAB-1k [62].
Tuned / Total CUB-200-2011 NABirds Oxford Flowers Stanford Dogs Stanford Cars Mean Acc.
FULL 100% 87.3 82.7 98.8 89.4 84.5 88.5
Addition-based methods
MLP-3 1.50% 85.1 77.3 97.9 84.9 53.8 79.8
PROMPT -SHALLOW 0.31% 86.7 78.8 98.4 90.7 68.7 84.6
PROMPT -DEEP 0.98% 88.5 84.2 99.0 90.2 83.6 89.1
ADAPTER -8 0.39% 87.3 84.3 98.4 88.8 68.4 85.5
ADAPTER -32 0.95% 87.2 84.3 98.5 89.6 68.4 85.6
ADAPT FORMER 0.44% 84.7 75.2 97.9 84.7 83.1 85.1
SPT-A DAPTER 0.41% 89.1 83.3 99.2 90.5 85.6 89.5
SPT-A DAPTER 0.47% 89.1 83.3 99.2 91.1 86.2 89.8
Reparameterization-based methods
LINEAR 0.12% 85.3 75.9 97.9 86.2 51.3 79.3
PARTIAL -1 8.38% 85.6 77.8 98.2 85.5 66.2 82.6
BIAS 0.13% 88.4 84.2 98.8 91.2 79.4 88.4
LORA-8 0.55% 84.9 79.0 98.1 88.1 79.8 86.0
LORA-16 0.90% 85.6 79.8 98.9 87.6 72.0 84.8
SPT-L ORA 0.41% 88.6 82.8 99.4 91.4 84.5 89.3
SPT-L ORA 0.60% 88.6 83.4 99.5 91.4 87.3 90.1
Table 9: Per-task results on the FGVC benchmark from Table 1 of the main paper. “Tuned / Total” denotes the fraction of the
trainable parameters. Top-1 accuracy (%) is reported. The best result is in bold , and the second-best result is underlined .

--- PAGE 14 ---
Figure 6: The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget with MAE [20] pre-
trained ViT-B/16 backbone. We sample six tasks from VTAB-1k [62].

--- PAGE 15 ---
Figure 7: The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget for MoCo v3 [11] pre-
trained ViT-B/16 backbone. We sample six tasks from VTAB-1k [62].
𝑾!𝑾"𝑾#𝑾$%&𝑾$%'𝑾(
Figure 8: Sensitivity patterns under 0.4M trainable parameters for Oxford Flowers [41], Stanford Cars [14], and Stanford
Dogs [26]. We show the proportions of the sensitive parameters for the query Wq, keyWk, value Wv, andWoweight
matrices in the multi-head self-attention layer and two weight matrices Wfc1andWfc2in the feed-forward network.

--- PAGE 16 ---
Supervised MAE MoCo v3
Pre-training strategy0.0000.0010.0020.0030.0040.005VarianceFigure 9: Comparisons of sensitivity variances across backbones with different pre-training strategies on VTAB-1k.
(a)
(e)
Flowers102PetsSun397
Clevr-CountDMLab
sNORB-Azim(b)(c)
(d)(f)
ImageNet
Figure 10: Dataset samples from ImageNet [27] and VTAB-1k [62]. Samples from Natural tasks of VTAB-1k ((a), (b), and
(c)) are relatively more similar to the source ImageNet samples compared to the ones from Structured tasks of VTAB-1k ((d),
(e), and (f)).

--- PAGE 17 ---
Natural Specialized StructuredTuned / Total
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Mean Acc.
Camelyon
EuroSAT
Resisc45
Retinopathy
Mean Acc.
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Mean Acc.
FULL 100% 68.9 87.7 64.3 97.2 86.9 87.4 38.8 75.9 79.7 95.7 84.2 73.9 83.4 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 47.6
Addition-based methods
MLP-3 1.50% 63.8 84.7 62.3 97.4 84.7 32.5 49.2 67.8 77.0 88.0 70.2 56.1 72.8 47.8 32.8 32.3 58.1 12.9 21.2 15.2 24.8 30.6
PROMPT -SHALLOW 0.31% 77.7 86.9 62.6 97.5 87.3 74.5 51.2 76.8 78.2 92.0 75.6 72.9 79.7 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 47.0
PROMPT -DEEP 0.98% 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0
ADAPTER -8 0.39% 69.2 90.1 68.0 98.8 89.9 82.8 54.3 79.0 84.0 94.9 81.9 75.5 84.1 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 58.5
ADAPTER -32 0.71% 68.7 92.2 69.8 98.9 90.3 84.2 53.0 79.6 83.2 95.4 83.2 74.3 84.0 81.9 63.9 48.7 80.6 76.2 47.6 30.8 36.4 58.3
NOAH 0.50% 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3
SPT-A DAPTER 0.30% 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8
SPT-A DAPTER 0.44% 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4
Reparameterization-based methods
LINEAR 0.12% 63.4 85.0 63.2 97.0 86.3 36.6 51.0 68.9 78.5 87.5 68.6 74.0 77.2 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 26.8
PARTIAL -1 8.38% 66.8 85.9 62.5 97.3 85.5 37.6 50.6 69.4 78.6 89.8 72.5 73.3 78.5 41.5 34.3 33.9 61.0 31.3 32.8 16.3 22.4 34.2
BIAS 0.13% 72.8 87.0 59.2 97.5 85.3 59.9 51.4 73.3 78.7 91.6 72.9 69.8 78.3 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 44.1
LORA-8 0.55% 67.1 91.4 69.4 98.8 90.4 85.3 54.0 79.5 84.9 95.3 84.4 73.6 84.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 60.5
LORA-16 0.90% 68.1 91.4 69.8 99.0 90.5 86.4 53.1 79.8 85.1 95.8 84.7 74.2 84.9 83.0 66.9 50.4 81.4 80.2 46.6 32.2 41.1 60.2
SPT-L ORA 0.31% 72.3 93.0 72.5 99.3 91.5 86.2 55.5 81.5 85.0 96.2 85.1 75.9 85.6 83.7 66.4 52.5 80.2 80.1 51.1 30.1 41.3 60.7
SPT-L ORA 0.63% 73.5 93.3 72.5 99.3 91.5 87.9 55.5 81.9 85.7 96.2 85.9 75.9 85.9 84.4 67.6 52.5 82.0 81.0 51.1 30.2 41.3 61.3
Table 10: Per-task results on the VTAB-1k benchmark from Table 1 of the main paper. “Tuned / Total” denotes the fraction of the trainable parameters. Top-1
accuracy (%) is reported.
