# 2311.09179.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2311.09179.pdf
# Kích thước tệp: 301966 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SiRA: Hỗn hợp Thưa thớt của Thích ứng Hạng thấp
Yun Zhu∗Nevan Wichers∗Chu-Cheng Lin∗Xinyi Wang∗Tianlong Chen†
Lei Shu∗Han Lu∗Canoee Liu∗Liangchen Luo∗Jindong Chen∗Lei Meng∗
Google∗, CSAIL@MIT†

Tóm tắt
Điều chỉnh Hiệu quả Tham số đã là một phương pháp nổi bật để thích ứng Mô hình Ngôn ngữ Lớn với các tác vụ hạ nguồn. Hầu hết các nghiên cứu trước đây xem xét việc thêm các tham số có thể huấn luyện dày đặc, trong đó tất cả các tham số được sử dụng để thích ứng với tác vụ nhất định. Chúng tôi thấy điều này kém hiệu quả thực nghiệm khi sử dụng ví dụ của LoRA rằng việc giới thiệu thêm tham số có thể huấn luyện không giúp ích. Được thúc đẩy bởi điều này, chúng tôi nghiên cứu tầm quan trọng của việc tận dụng tính toán "thưa thớt" và đề xuất SiRA: hỗn hợp thưa thớt của thích ứng hạng thấp. SiRA tận dụng Hỗn hợp Thưa thớt của Chuyên gia (SMoE) để tăng hiệu suất của LoRA. Cụ thể, nó thực thi định tuyến chuyên gia topk với giới hạn dung lượng hạn chế số lượng token tối đa mà mỗi chuyên gia có thể xử lý. Chúng tôi đề xuất một cơ chế dropout chuyên gia mới và đơn giản trên mạng gating để giảm vấn đề quá khớp. Thông qua các thí nghiệm rộng rãi, chúng tôi xác minh SiRA hoạt động tốt hơn LoRA và các phương pháp hỗn hợp chuyên gia khác trên các tác vụ đơn khác nhau và cài đặt đa tác vụ.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (Thoppilan et al., 2022; Passos et al., 2023; Ouyang et al., 2022; Touvron et al., 2023) (LLMs) đã thể hiện khả năng ấn tượng trong một loạt các tác vụ. Tuy nhiên, việc thích ứng các mô hình mục đích chung này với các tác vụ hạ nguồn có tài nguyên thấp đặc biệt quan trọng. Để làm được điều này, điều chỉnh hiệu quả tham số (PET) (Hu et al., 2021; Li và Liang, 2021; Lester et al., 2021; Houlsby et al., 2019; Zhang et al., 2023; Zaken et al., 2021; Chen et al., 2022), giới thiệu các trọng số cụ thể cho tác vụ vào mô hình nền tảng đông lạnh để thực hiện gradient descent, tránh việc quên thảm khốc (Luo et al., 2023) của việc tinh chỉnh và cung cấp chất lượng tốt hơn và chi phí thấp hơn so với học trong ngữ cảnh (Liu et al., 2022). LoRA (Hu et al., 2021) là một phương pháp PET được áp dụng rộng rãi, tương ứng với yunzhu@google.com, đạt hiệu suất cao bằng cách tận dụng ma trận hạng thấp.

Một câu hỏi đối với người dùng LoRA hoặc phương pháp PET khác là nên sử dụng bao nhiêu tham số có thể huấn luyện. Trong trường hợp LoRA, điều này được kiểm soát bởi hạng của ma trận hạng thấp. Và bằng cách tăng giá trị này, có thể cung cấp thêm tính toán để phù hợp với các tác vụ cụ thể. Tuy nhiên, đã được chỉ ra rằng ma trận hạng cao hơn sẽ không mang lại chất lượng tốt hơn cho mô hình do tính không ổn định của việc huấn luyện (Chen et al., 2022), điều mà chúng tôi xác minh trong Hình 2 ở Phụ lục 8.1. Điều này tạo ra một nút thắt cổ chai ẩn cho chất lượng của mô hình ngay cả khi chúng ta có đủ ngân sách tính toán, và việc cải thiện chất lượng của LoRA vẫn là một thách thức.

Mặt khác, việc tận dụng Hỗn hợp Thưa thớt của Chuyên gia trong mạng neural đã được nghiên cứu rộng rãi như một sự thay thế cho Mạng FeedForward, với các phương pháp khác nhau để tìm phép gán tối ưu giữa chuyên gia và token, bao gồm học tăng cường (Bengio et al., 2015), chương trình tuyến tính (Lewis et al., 2021), quy tắc cố định (Roller et al., 2021), gating top-1 (Fedus et al., 2022), gating top-2 (Lepikhin et al., 2020), gating top-k (Shazeer et al., 2017), lựa chọn chuyên gia ngược (Zhou et al., 2022), và phép gán mềm (Puigcerver et al., 2023). Một số nghiên cứu gần đây đã đề xuất sử dụng mô hình hỗn hợp chuyên gia trên việc điều chỉnh hiệu quả tham số (Wang et al., 2022; Zadouri et al., 2023). Tuy nhiên, các nghiên cứu trước đây đã bỏ qua tiềm năng của SMoE thưa thớt, có lẽ do các vấn đề như bỏ token (Puigcerver et al., 2023) và quá khớp (Elbayad et al., 2022).

Để giải quyết điều này, chúng tôi nghiên cứu việc tận dụng tính toán "thưa thớt" cho PET trong bài báo này và đề xuất SiRA, Hỗn hợp Thưa thớt của Thích ứng Hạng thấp. SiRA thực thi định tuyến chuyên gia top k, cải thiện việc sử dụng tài nguyên và tính toán và cũng tạo điều kiện cho việc kiểm soát dung lượng chi tiết hơn của một đầu vào nhất định. SiRA bao gồm arXiv:2311.09179v1 [cs.CL] 15 Nov 2023

--- TRANG 2 ---
ba thành phần quan trọng: ràng buộc dung lượng chấp nhận việc bỏ token, một mất mát phụ trợ để phạt việc sử dụng quá mức một số ít chuyên gia, và một cơ chế dropout chuyên gia mới. Chúng hoạt động cùng nhau để đảm bảo cân bằng tải phù hợp và giải quyết vấn đề quá khớp.

Chúng tôi đã tiến hành các thí nghiệm rộng rãi để xác minh hiệu suất của SiRA, đạt được hiệu suất tốt hơn so với LoRA (Hu et al., 2021) và các biến thể MoE của nó Adamix (Wang et al., 2022), MoLoRA (Zadouri et al., 2023) trên một loạt các điểm chuẩn tác vụ đơn và đa tác vụ. Nghiên cứu ablation của chúng tôi cho thấy số lượng chuyên gia được sử dụng và dung lượng trên mỗi chuyên gia cải thiện hiệu suất, điều này chứng minh lợi thế của việc "thưa thớt". Đáng chú ý, dropout chuyên gia đóng vai trò quan trọng, và nó hiệu quả hơn SMoE-dropout (Chen et al., 2023a).

2 Nghiên cứu Liên quan
Điều chỉnh Hiệu quả Tham số (PET) Điều chỉnh Hiệu quả Tham số có nhiều biến thể khác nhau như Adapters (Houlsby et al., 2019), Prefix Tuning (Li và Liang, 2021; Liu et al., 2021), Prompt Tuning (Lester et al., 2021), P-tuning (Liu et al., 2023), attention-injection (Zhang et al., 2023), LoRA (Hu et al., 2021; Dettmers et al., 2023), và kết hợp các phương pháp PET (Mao et al., 2021). Trong bài báo này, chúng tôi tập trung vào LoRA vì nó đã được tìm thấy đạt kết quả tốt hơn, mặc dù các phương pháp có thể được áp dụng cho các biến thể khác cũng như vậy.

MoE cho phương pháp PET Dọc theo giao điểm của PET và MoE, Adamix (Wang et al., 2022) và MoLoRA (Zadouri et al., 2023) giống nhất với công việc của chúng tôi. Adamix chọn ngẫu nhiên một chuyên gia trong quá trình huấn luyện và tính trung bình tất cả chuyên gia trong quá trình suy luận. Mặc dù hiệu quả, phương pháp này giống như việc tính trung bình checkpoint (Gao et al., 2022) bởi vì các chuyên gia được chọn ngẫu nhiên, họ không học cách chuyên môn hóa. Quan trọng hơn, phương pháp ngẫu nhiên có thời gian huấn luyện dài hơn đáng kể, được nhân với số lượng chuyên gia được sử dụng. MoLoRA áp dụng MoE mềm đầy đủ lên trên LoRA, trong đó tất cả chuyên gia được tính trung bình bằng cách sử dụng một gating đã học. So với nghiên cứu này, phương pháp của chúng tôi có thể đạt hiệu quả tốt hơn. Thứ nhất, SiRA không cần thời gian dài hơn để huấn luyện so với LoRA tiêu chuẩn, nhờ sự hội tụ nhanh của SMoE (Fedus et al., 2022). Thứ hai, tính thưa thớt được thực thi trong SiRA giúp tiết kiệm tài nguyên huấn luyện và tính toán suy luận so với MoLoRA.

Một hướng khác của nghiên cứu MoE là cho đa tác vụ, như Task-MoE (Kudugunta et al., 2021) và Skill Selection (Ponti et al., 2023). Các phương pháp này giả định task-id bên ngoài như một đầu vào bổ sung cho việc huấn luyện và suy luận. Mặc dù chúng tôi thí nghiệm với MoE trong cài đặt đa tác vụ, nó không yêu cầu task-id của đầu vào.

3 Hỗn hợp Thưa thớt của Thích ứng Hạng thấp
Để tăng dung lượng của LoRA (Hu et al., 2021) bằng cách sử dụng Hỗn hợp Chuyên gia (MoE) mà không thêm quá nhiều chi phí tính toán, chúng tôi đề xuất Hỗn hợp Thưa thớt của Chuyên gia của Thích ứng Hạng thấp (SiRA), tận dụng nhiều bộ adapter LoRA nhẹ như chuyên gia trong khi thực thi tính thưa thớt khi sử dụng các chuyên gia.

Hình 1 cho thấy minh họa của SiRA. Lớp MoE cho adapter bao gồm E chuyên gia, mỗi chuyên gia có trọng số LoRA riêng, W1, ..., WE. Wk là tích của hai ma trận hạng thấp Wk = BkAk. Chúng tôi cũng giả định mô hình nền tảng cơ bản có W0 như là trọng số đông lạnh, đại diện cho phép chiếu query, key, value, hoặc output. Tiếp theo, chúng tôi sẽ giới thiệu từng thành phần của khung làm việc thực thi tính thưa thớt của các chuyên gia LoRA.

Expert Gating Để giảm chi phí tính toán, SiRA chỉ kích hoạt một tập con của tất cả các mô-đun chuyên gia. Chính thức, trong mỗi lần truyền xuôi, chúng tôi chọn K trong số E chuyên gia sử dụng điểm số đầu ra của mạng gating θg. Quá trình được biểu thị toán học như Phương trình (1) và (2), trong đó s biểu thị chỉ số token của chuỗi xs và Gs,e là đầu ra mạng gating tại token thứ s chuyên gia thứ e.

G(xs) = TopK(softmax(θT_g xs)) (1)
ys = ∑^E_{e=1} Gs,e We(xs) + W0(xs) (2)

Experts Dropout Chúng tôi đề xuất một cách thực tế để làm cho gate cân bằng hơn với một gate dropout đơn giản. Cụ thể, chúng tôi giới thiệu dropout vào đầu ra gating G như được thể hiện trong Phương trình 3.

G(xs) = TopK(Dropout(softmax(θT_g xs))) (3)

--- TRANG 3 ---
Lớp Chiếu Attention
Topk
Attention
Chiếu
Trọng số Tổng có Trọng số Cộng
Dung lượng Đã sử dụng ……
Trọng số LoRA
……
Chuyên gia Tràn
Dropout
Gate Hình 1: SiRA: Hỗn hợp Gated Thưa thớt của LoRA.

Expert Token Capacity Chúng tôi thực thi các ràng buộc dung lượng cho chuyên gia theo GShard (Lepikhin et al., 2020). Cụ thể, chúng tôi hạn chế số lượng token được xử lý bởi mỗi chuyên gia không được vượt quá ngưỡng được định trước. Khi dung lượng đạt được, một chuyên gia chỉ đơn giản bỏ các token tràn. Nếu tất cả K chuyên gia đạt dung lượng token của họ trước khi tất cả token trong một ví dụ huấn luyện được xử lý, các token còn lại sẽ chỉ được mã hóa bằng tham số mô hình đông lạnh W0.

Auxiliary Loss Chúng tôi định nghĩa một số hạng mất mát phụ trợ để khuyến khích cân bằng tải giữa các chuyên gia khác nhau (Shazeer et al., 2017). Chúng tôi định nghĩa tổng số token là S, và có E chuyên gia. Chúng tôi ký hiệu ce là số lượng token được định tuyến đến chuyên gia e. Bằng cách sử dụng gate trung bình trên mỗi chuyên gia me = Mean_s(Dropout(softmax(θT_g xs))) như một xấp xỉ có thể vi phân, chúng tôi định nghĩa mất mát aux trong Phương trình 4.

l_aux = (1/E) ∑^E_{e=1} (ce/S) * me (4)

4 Thí nghiệm
4.1 Thiết lập Đánh giá
Baseline và Cấu hình Thí nghiệm Chúng tôi đặc biệt so sánh mô hình của chúng tôi với LoRA tiêu chuẩn (Hu et al., 2021), Adamix (Wang et al., 2022) và MoLoRA (Zadouri et al., 2023). Lưu ý rằng các phương pháp adapter khác không được so sánh vì phương pháp SiRA là trực giao và có thể được áp dụng lên trên chúng cũng như vậy. Chúng tôi chọn PALM2-FLAN XXS (Passos et al., 2023) làm mô hình nền tảng. Chúng tôi tuân theo cấu hình mặc định trong (Hu et al., 2021) để tiêm trọng số LoRA vào các phép chiếu attention và đặt hạng nội tại là 4. Chúng tôi sử dụng 16 chuyên gia theo mặc định trên tất cả baseline. Để biết cấu hình huấn luyện và lựa chọn mô hình, xem Phụ lục 8.2.

Bộ dữ liệu và Độ đo Chúng tôi đánh giá trên các bộ dữ liệu sau:¹

XTREME-UP Bộ dữ liệu XTREME-UP (Ruder et al., 2023) là một bộ dữ liệu đa tác vụ đa ngôn ngữ, tập trung vào các tình huống dữ liệu khan hiếm của các ngôn ngữ ít được đại diện. Trong nghiên cứu này, chúng tôi chọn hai ngôn ngữ ít được đại diện—Swahili(SW) và Bengali(BN)—và đánh giá trên một số tác vụ NLP trong đó hai ngôn ngữ này có dữ liệu huấn luyện và đánh giá. Chúng tôi tuân theo Ruder et al. (2023) cho các phần chia và độ đo đánh giá của từng tác vụ.

FinQA FinQA (Chen et al., 2021) là một bộ dữ liệu QA trong lĩnh vực tài chính. Khả năng lý luận phức tạp là cần thiết để trả lời chính xác những câu hỏi này. Lưu ý rằng các câu trả lời của bộ dữ liệu FinQA là các chương trình của một DSL số học đặc biệt. Trong nghiên cứu này, chúng tôi chỉ đánh giá dựa trên các độ đo khớp hình thức bề mặt, tức là điểm khớp chính xác và F1.

ForumSum ForumSum (Khalman et al., 2021) là một bộ dữ liệu tóm tắt cuộc trò chuyện đa dạng và chất lượng cao với các tóm tắt được viết bởi con người, trong đó các cuộc trò chuyện được thu thập từ nhiều diễn đàn internet khác nhau. Chúng tôi báo cáo BLEURT (Sellam et al., 2020), ROUGEL, và điểm F1.

4.2 Hiệu suất của SiRA
Chúng tôi đánh giá tất cả hiệu suất tác vụ đơn trong Bảng 1. Lưu ý rằng vì FinQA là một tác vụ khó với lý luận tài chính, do đó điểm khớp chính xác và f1 tương đối thấp. Chúng ta có thể nhận thấy rằng SiRA vượt trội hơn tất cả các baseline khác ở hầu hết các tác vụ, với ít hơn 1% tham số bổ sung. Đáng chú ý khi so sánh với MoLoRA, SiRA đạt hiệu suất tốt hơn liên tục trên tất cả các tác vụ. Điều này chứng minh rằng MoE "thưa thớt" tốt hơn MoE "đầy đủ". Adamix cho thấy một số lợi thế nhỏ trên tác vụ Semantic Parsing, nhưng tổng thể thua SiRA trên tất cả các tác vụ khác.

Chúng tôi cũng đã tiến hành thí nghiệm trên hai cài đặt đa tác vụ trên ngôn ngữ swahili (SW) và bengali(BN), và hai cài đặt đa ngôn ngữ cho tác vụ QA trong ngôn ngữ và tác vụ QA qua ngôn ngữ. Chúng tôi

¹ Vì mô hình cơ sở của chúng tôi (Chung et al., 2022) đã được tiếp xúc với nhiều bộ dữ liệu công cộng trong quá trình huấn luyện, chúng tôi chọn bộ dữ liệu chưa được tiêu thụ.

--- TRANG 4 ---
Bảng 1: So sánh Hiệu suất cho Tác vụ Đơn
Phương pháp δParams FinQA (EN) ForumSum (EN) SP (SW) QA-in (SW) NER (SW) SP (BN) QA-in (BN) QA-cross (BN)
em f1 bleurt rougeL f1 accuracy f1 span-f1 accuracy f1 f1
LoRA 0.043% 5.0 5.6 96.70 33.97 23.54 27.63 82.08 88.95 33.52 80.34 76.81
Adamix 0.689% 5.6 6.0 95.95 35.10 23.88 33.22 81.24 89.00 39.03 81.70 76.07
MoLoRA 0.746% 5.6 6.4 97.05 34.37 24.79 32.50 82.33 89.33 36.28 79.06 76.75
SiRA 0.746% 5.8 6.6 97.14 35.67 25.83 32.52 83.00 89.95 38.61 82.10 76.93

Bảng 2: So sánh Hiệu suất cho Đa Tác vụ
Phương pháp δparams SW Multitask BN Multitask
SP(accuracy) QA-in(f1) NER(span-f1) Average SP(accuracy) QA-in(f1) QA-cross(f1) Average
LoRA 0.043% 28.06 77.71 88.28 64.69 32.06 79.27 75.03 62.12
Adamix 0.689% 35.14 76.99 89.01 67.10 38.41 79.49 75.09 64.33
MoLoRA 0.746% 33.44 79.91 88.92 65.66 35.98 78.14 76.37 63.49
SiRA 0.746% 33.98 81.26 89.04 68.10 37.71 82.17 75.50 65.13

Bảng 3: So sánh Hiệu suất cho Tác vụ Đa ngôn ngữ.
Phương pháp δparams QA-in (9) QA-cross (25)
LoRA 0.043% 85.09 69.41
Adamix 0.689% 84.75 70.42
MoLoRA 0.746% 85.14(WIP) 69.70(WIP)
SiRA 0.746% 86.38 70.86

Bảng 4: Nghiên cứu ablation tự thực hiện trên siêu tham số topK(K) và dung lượng chuyên gia(C) trên ForumSum.
Configs bleurt rougeL f1
K=2 96.87 34.51 24.73
K=4 96.60 34.66 25.34
K=6 96.75 34.73 24.55
K=8 96.76 35.31 25.64
K=10 97.51 35.10 25.19
K=12 96.96 34.49 24.24
C=2 96.33 34.15 24.13
C=4 96.60 34.66 25.34
C=6 97.14 35.67 25.83
C=8 97.31 34.97 25.24
C=10 97.25 34.75 25.57
C=12 96.50 34.44 23.94

báo cáo các số liệu trong Bảng 2 và Bảng 3 tương ứng. Xu hướng tổng thể tương tự như những gì chúng tôi tìm thấy trong các tác vụ đơn. SiRA đạt hiệu suất trung bình tốt nhất trong tất cả các baseline.

4.3 Nghiên cứu Ablation
Nghiên cứu ablation Tính toán Chúng tôi chia sẻ các nghiên cứu ablation trên ForumSum trong Bảng 4. Chúng tôi chọn một cấu hình đơn giản làm cơ sở (k=4, C=4) và sau đó thay đổi từng cái trong khi giữ nguyên phần còn lại. Đầu tiên chúng tôi thay đổi top K từ 2 đến 12, với dung lượng C=K. Và sau đó chúng tôi cố định K=4, và thay đổi dung lượng chuyên gia từ 2 đến 12. Một phát hiện thú vị là việc tăng tính toán hoặc dung lượng sẽ không luôn tăng điểm số và có một "vùng thoải mái" mà chúng ta cần tìm ra bằng việc điều chỉnh mô hình. Điều này cũng chứng minh tại sao phương pháp dựa trên MoE "đầy đủ" không tốt bằng SiRA. SiRA cung cấp kiểm soát chi tiết hơn về tính toán.

Bảng 5: Nghiên cứu ablation gating trên ForumSum.
Phương pháp bleurt rougeL f1
SiRA 97.14 35.67 25.83
- aux loss 96.37 35.09 25.11
- Expert Dropout 97.09 34.73 24.55
+ SMoE-Dropout 96.30 34.24 24.32

Nghiên cứu ablation Gating Chúng tôi cũng cung cấp các nghiên cứu ablation về gating trong Bảng 5. Cụ thể, chúng tôi so sánh SiRA với 3 trường hợp: 1/ loại bỏ mất mát aux, 2/ loại bỏ gate dropout, và 3/ sử dụng dropout định tuyến tĩnh SMoE-Dropout (Chen et al., 2023a) thay thế. Kết quả cho thấy gating đã học vẫn tốt hơn gating cố định, và cả gate dropout và mất mát aux đều giúp cải thiện hiệu suất của SiRA.

Gate học được gì Chúng tôi sử dụng thí nghiệm đa tác vụ Swahili để nghiên cứu gate đang học gì. Chúng tôi đo entropy trung bình của mỗi phân phối trọng số gate trước khi TopK được áp dụng. Entropy trung bình cho tác vụ QA (trong ngôn ngữ) giảm từ 1.6 xuống 1.13 nats trong quá trình huấn luyện. Điều này chỉ ra rằng mô hình học cách cho các gate nhất định trọng số nhiều hơn khi nó huấn luyện.

Chúng tôi cũng đo các hệ số tương quan trung bình giữa mỗi chỉ số tác vụ và mỗi chỉ số gate tương tự như (Chen et al., 2023b). Chúng tôi chuyển đổi chỉ số tác vụ thành mã hóa one hot cho việc này. Ở cuối quá trình huấn luyện, tương quan trung bình khoảng

--- TRANG 5 ---
.025, không có ý nghĩa. Tương quan giữa gate và ngôn ngữ trong thí nghiệm đa ngôn ngữ cũng không có ý nghĩa. Điều này cho thấy cơ chế gating của chúng tôi không học cách định tuyến các tác vụ khác nhau đến các gate khác nhau.

5 Danh sách Việc cần làm
Bản thảo này hiện đang được phát triển tích cực. Những nỗ lực sắp tới của chúng tôi bao gồm việc có thêm kết quả và phân tích, và cải thiện việc viết. Chúng tôi nhiệt tình chào đón các đề xuất, hiểu biết sâu sắc và phê bình mang tính xây dựng từ cộng đồng nghiên cứu. Nếu bạn có bất kỳ phản hồi hoặc ý tưởng nào có thể cải thiện chất lượng và tác động của công việc chúng tôi, vui lòng không ngần ngại liên hệ với tác giả chính. Đóng góp của bạn rất quý giá đối với chúng tôi, và chúng tôi mong muốn tích hợp các quan điểm đa dạng để tinh chỉnh và nâng cao nghiên cứu của mình.

6 Kết luận
Bài báo này giới thiệu SiRA, một biến thể Hỗn hợp Thưa thớt của Chuyên gia của LoRA. SiRA thực thi định tuyến chuyên gia top k với ràng buộc dung lượng cho mỗi chuyên gia. Chúng tôi cũng thiết kế một cơ chế dropout chuyên gia mới trên mất mát phụ trợ để giảm vấn đề quá khớp. Chúng tôi đã tiến hành các thí nghiệm rộng rãi để xác minh hiệu suất của SiRA, đạt hiệu suất tốt hơn LoRA và các biến thể MoE của nó trên các cài đặt tác vụ đơn và đa tác vụ khác nhau.

7 Hạn chế
SiRA đang tốn thêm chi phí phục vụ để phục vụ với các tham số bổ sung trên chuyên gia và gating, so với LoRA hoặc Adamix. Làm thế nào để giảm thiểu chi phí phục vụ là một vấn đề thách thức mà chúng tôi hy vọng giải quyết trong các nghiên cứu tương lai.

Lời cảm ơn
Chúng tôi xin gửi lời cảm ơn đến Abhanshu Sharma, Hassan Mansoor, Qifei Wang, Victor Cărbune, v.v. cho những đóng góp có giá trị của họ.

Tài liệu tham khảo
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, và Doina Precup. 2015. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297.

Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, và Shangsong Liang. 2022. Revisiting parameter-efficient tuning: Are we really there yet? arXiv preprint arXiv:2202.07962.

Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, và Zhangyang Wang. 2023a. Sparse moe as the new dropout: Scaling dense and self-slimmable transformers. arXiv preprint arXiv:2303.01610.

Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, và William Yang Wang. 2021. FinQA: A dataset of numerical reasoning over financial data. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 3697–3711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik G. Learned-Miller, và Chuang Gan. 2023b. Mod-squad: Designing mixtures of experts as modular multi-task learners. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, trang 11828–11837. IEEE.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, và Jason Wei. 2022. Scaling instruction-finetuned language models.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.

Maha Elbayad, Anna Sun, và Shruti Bhosale. 2022. Fixing moe over-fitting on low-resource languages in multilingual machine translation. arXiv preprint arXiv:2212.07571.

William Fedus, Barret Zoph, và Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270.

Yingbo Gao, Christian Herold, Zijian Yang, và Hermann Ney. 2022. Revisiting checkpoint averaging for neural machine translation. arXiv preprint arXiv:2210.11803.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea

--- TRANG 6 ---
Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. Trong International Conference on Machine Learning, trang 2790–2799. PMLR.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

Misha Khalman, Yao Zhao, và Mohammad Saleh. 2021. ForumSum: A multi-speaker conversation summarization dataset. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, trang 4592–4599, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, và Orhan Firat. 2021. Beyond distillation: Task-level mixture-of-experts for efficient inference. arXiv preprint arXiv:2110.03742.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.

Brian Lester, Rami Al-Rfou, và Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. 2021. Base layers: Simplifying training of large, sparse models. Trong International Conference on Machine Learning, trang 6265–6274. PMLR.

Xiang Lisa Li và Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, và Jie Tang. 2021. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, và Jie Tang. 2023. Gpt understands, too. AI Open.

Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, và Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747.

Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, và Madian Khabsa. 2021. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và các cộng sự. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.

Alex Passos, Andrew Dai, Bryan Richter, Christopher Choquette, Daniel Sohn, David So, Dmitry (Dima) Lepikhin, Emanuel Taropa, Eric Ni, Erica Moreira, Gaurav Mishra, Jiahui Yu, Jon Clark, Kathy Meier-Hellstern, Kevin Robinson, Kiran Vodrahalli, Mark Omernick, Maxim Krikun, Maysam Moussalem, Melvin Johnson, Nan Du, Orhan Firat, Paige Bailey, Rohan Anil, Sebastian Ruder, Siamak Shakeri, Siyuan Qiao, Slav Petrov, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, và Zack Nado. 2023. Palm 2 technical report. Báo cáo kỹ thuật, Google Research.

Edoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, và Siva Reddy. 2023. Combining parameter-efficient modules for task-level generalisation. Trong Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, trang 687–702.

Joan Puigcerver, Carlos Riquelme, Basil Mustafa, và Neil Houlsby. 2023. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951.

Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, và các cộng sự. 2021. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555–17566.

Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel A. Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana L. Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David I. Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle, Melvin Johnson, Dmitry Panteleev, và Partha Talukdar. 2023. Xtreme-up: A user-centric scarce-data benchmark for under-represented languages.

Thibault Sellam, Dipanjan Das, và Ankur P Parikh. 2020. Bleurt: Learning robust metrics for text generation. Trong Proceedings of ACL.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.

--- TRANG 7 ---
Noam Shazeer và Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. Trong International Conference on Machine Learning, trang 4596–4604. PMLR.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, và các cộng sự. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, và các cộng sự. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, và Jianfeng Gao. 2022. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. arXiv preprint arXiv:2210.17451.

Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, và Sara Hooker. 2023. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. arXiv preprint arXiv:2309.05444.

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. 2021. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199.

Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Yu Qiao. 2023. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199.

Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, và các cộng sự. 2022. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103–7114.

8 Phụ lục
8.1 Tác động của hạng LoRA
Chúng tôi nghiên cứu tác động của hạng LoRA trong Hình 2.

8.2 Huấn luyện và Lựa chọn Mô hình
Trong quá trình tinh chỉnh có giám sát, SFT, chúng tôi sử dụng 8 chip Tensor Processing Units (TPU) V3 để tinh chỉnh. Kích thước batch là 64, và số bước huấn luyện tối đa là 30000. Chúng tôi sử dụng trình tối ưu hóa Adafactor (Shazeer và Stern, 2018) với tỷ lệ học 0.0005. Cả độ dài chuỗi đầu vào và đầu ra đều được đặt để khớp với yêu cầu bộ dữ liệu. Tỷ lệ dropout huấn luyện là 0.05. Tỷ lệ dropout chuyên gia được đặt thành 0.5. Chúng tôi đã thực hiện tìm kiếm siêu tham số để tìm cấu hình mô hình tốt nhất. Chúng tôi giải mã trên các tập xác thực của từng tác vụ mỗi 100 bước. Và chúng tôi báo cáo kết quả kiểm tra từ các checkpoint tốt nhất theo điểm số xác thực. Đối với kết quả đa tác vụ, checkpoint được chọn dựa trên trung bình các độ đo của từng tác vụ. Đối với các số được báo cáo trong phần 4.2, chúng tôi sử dụng topk K=4 làm mặc định. Tuy nhiên, chúng tôi thấy K=8 tốt hơn cho đa tác vụ BN và cài đặt đa ngôn ngữ QA (trong ngôn ngữ), và K=12 tốt hơn cho thí nghiệm QA (qua ngôn ngữ).

Tham số Có thể huấn luyện (%) RougeL
33343536
0.25 0.50 0.75 1.00 1.25 SiRA MoLoRA Adamix LoRA

Hình 2: SiRA so với LoRA trên Tác vụ ForumSum. Chúng tôi tăng hạng của LoRA (hạng=4, 8, 16, 32, 64, 128) và báo cáo RougeL như một độ đo. Đáng chú ý là việc tăng hạng không giúp cải thiện hiệu suất. SiRA (hạng=4) có thể đạt chất lượng cao hơn bằng cách tận dụng hỗn hợp thưa thớt của chuyên gia.
