# 2210.08823.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2210.08823.pdf
# File size: 3323529 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Scaling & Shifting Your Features:
A New Baseline for Efﬁcient Model Tuning
Dongze Lian1Daquan Zhou1;2Jiashi Feng2Xinchao Wang1
1National University of Singapore2ByteDance
{dongze,xinchao}@nus.edu.sg {zhoudaquan21,jshfeng}@gmail.com
Abstract
Existing ﬁne-tuning methods either tune all parameters of the pre-trained model
(full ﬁne-tuning), which is not efﬁcient, or only tune the last linear layer (linear
probing), which suffers a signiﬁcant accuracy drop compared to the full ﬁne-tuning.
In this paper, we propose a new parameter-efﬁcient ﬁne-tuning method termed as
SSF, representing that researchers only need to Scale and Shift the deep Features
extracted by a pre-trained model to catch up with the performance of full ﬁne-
tuning. In this way, SSF also surprisingly outperforms other parameter-efﬁcient
ﬁne-tuning approaches even with a smaller number of tunable parameters. Fur-
thermore, different from some existing parameter-efﬁcient ﬁne-tuning methods
(e.g., Adapter or VPT) that introduce the extra parameters and computational cost
in the training and inference stages, SSF only adds learnable parameters during
the training stage, and these additional parameters can be merged into the original
pre-trained model weights via re-parameterization in the inference phase. With the
proposed SSF, our model obtains 2.46% (90.72% vs.88.54%) and 11.48% (73.10%
vs. 65.57%) performance improvement on FGVC and VTAB-1k in terms of Top-1
accuracy compared to the full ﬁne-tuning but only ﬁne-tuning about 0.3M parame-
ters. We also conduct amounts of experiments in various model families (CNNs,
Transformers, and MLPs) and datasets. Results on 26 image classiﬁcation datasets
in total and 3 robustness & out-of-distribution datasets show the effectiveness of
SSF. Code is available at https://github.com/dongzelian/SSF .
1 Introduction
With the popularity of the data-driven methods in the deep learning community, the dataset scale
and the model size have both got huge explosions. There is a tendency to explore large models and
then adopt these pre-trained models in downstream tasks to achieve better performance and faster
convergence, which gradually becomes a common way.
However, the current procedure depends on full ﬁne-tuning heavily, where all the parameters of the
model are updated. It inevitably causes the model to be over-ﬁtted to the small target dataset and
thus cannot be used for other tasks after the ﬁne-tuning. As a result, the device will need to save
a dedicated set of model parameters for each task, which causes a huge amount of storage space,
especially for today’s large models ( e.g., ViT-G/14 [16] 1.8G, CoAtNet [10] 2.4G).
A simple solution for the above problem is linear probing [ 26], where only the last head layer is
ﬁne-tuned. However, this practice usually yields inferior performance compared to the full ﬁne-tuning
proxy. Motivated by the success of the parameter-efﬁcient ﬁne-tuning strategy with prompt in the
ﬁeld of natural language processing (NLP) [ 36,49,38,30], the recent work implements a similar
proxy on vision tasks [ 44], termed as Visual Prompt Tuning (VPT). Speciﬁcally, VPT [ 44] proposes
to insert learnable prompts as inputs and append them to the original image tokens. These prompts
Equal contribution.
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.08823v3  [cs.CV]  15 Jan 2023

--- PAGE 2 ---
Method Acc.Params.
(M)Uniﬁed
parameter spaceNo extra
inference params.
Full ﬁne-tuning 93.82 85.88 X X
Linear probing 88.70 0.08 X X
Adapter [36] 93.34 0.31 X 
VPT [44] 93.17 0.54 
SSF (ours) 93.99 0.28 X X
Table 1: Characteristics of different ﬁne-
tuning methods. Acc. means the Top-1 ac-
curacy (%) on CIFAR-100 with a pre-trained
ViT-B/16 for tuning. Params. means the learn-
able parameters at ﬁne-tuning. Our SSF has
a uniﬁed learnable parameter space and does
not require extra inference parameters while
obtaining superior performance.
Figure 1: Performance comparisons of seven ﬁne-
tuning methods with a pre-trained ViT-B/16 model
on the FGVC dataset and VTAB-1k benchmark.
Our SSF (red dots) achieves state-of-the-art per-
formance only with about 0.3M average learnable
parameters.
will interact with the image tokens by performing self-attention and are updated during the ﬁne-tuning
process. In this manner, a signiﬁcant performance improvement can be achieved in downstream tasks
compared to a linear probing proxy. Nevertheless, compared to the full ﬁne-tuning and linear probing,
it additionally raises two issues: i) VPT tunes the number of prompts for different tasks, which
introduces a task-dependent learnable parameter space. The ﬁne-tuning performance is sensitive to
the number of prompts for each task and needs to be carefully designed. Too few or too many prompts
might either degrade the accuracy of ﬁne-tuning or increase the redundancy of the computation
(e.g., 200 prompts on Clevr/count vs.1 prompt on Flowers102); ii) VPT [ 44], as well as other
Adapter-based methods [ 36,60], introduces additional parameters and computational cost in the
inference phase compared to the original pre-trained model. For instance, VPT introduces additional
inputs for self-attention with image tokens. Adapter-based methods insert additional modules into
the pre-trained model. These methods change the speciﬁc backbone architecture or the input of the
network, which might result in frequent structure modiﬁcations and heavy workload, especially for
those models that are already deployed in edge devices ( e.g., mobile phones).
To cope with the above issues, we attempt to ﬁnd a general proxy for parameter-efﬁcient ﬁne-
tuning, where the learnable parameter space is uniﬁed (task-independent) and no additional inference
parameters are introduced. Inspired by some feature modulation methods [ 80,40,66], we propose
a new parameter-efﬁcient ﬁne-tuning method named SSF, where you only need to Scale and Shift
your deep Features extracted by a pre-trained model for ﬁne-tuning. The intuition behind our
approach come from the fact that the upstream datasets and downstream datasets have different
data distributions [ 71]. Therefore, it is difﬁcult to apply the model weights trained in the upstream
dataset to the downstream dataset. For instance, a naive linear probing strategy with keeping the
weights of backbone frozen will cause performance degradation. To alleviate the above problem,
SSF introduces scale parameters and shift parameters, which could be considered as variance and
mean to modulate the features of the downstream dataset extracted with the pre-trained model on
the upstream dataset, such that the modulated feature falls in a discriminative space. These scale
parameters and shift parameters do not depend on any input and have a uniﬁed learnable parameter
space for different tasks. Another advantage of SSF is that it only introduces linear transformations
because we scale and shift the extracted features. These linear transformations could be further
merged into the original pre-trained weight via model re-parameterization [ 15] in the inference phase,
thus avoiding the extra parameters and FLOPs for downstream tasks. For a deployed model in edge
devices, only the updated weights after ﬁne-tuning need to be uploaded instead of changing the
backbone architecture. Table 1 shows the speciﬁc characteristics comparisons between SSF and other
ﬁne-tuning methods. SSF is simple, effective, and efﬁcient, which also conforms to Occam’s Razor
principle. Therefore, we explore this new baseline and ﬁnd that it surprisingly outperforms all other
parameter-efﬁcient ﬁne-tuning methods.
We evaluate our method on 26 classiﬁcation datasets in total and 3 robustness & out-of-distribution
datasets. SSF obtains state-of-the-art performance compared to other parameter-efﬁcient ﬁne-tuning
methods with the trainable parameters and accuracy trade-off (Table 1 and Figure 1). Compared
to the full ﬁne-tuning, our method obtains 2.46% (90.72% vs.88.54%) and 11.48% (73.10% vs.
65.57%) performance improvement on FGVC and VTAB-1k in terms of Top-1 accuracy but only
with about 0.3M trainable parameters. Furthermore, our SSF does not require additional parameters
during the inference phase. It is plug-and-play and is very easy to extend to various model families
(CNNs, Transformers, and MLPs). Our SSF establishes a new baseline and we hope that it brings
more insight into the ﬁeld of the efﬁcient model tuning.
2

--- PAGE 3 ---
2 Related Work
2.1 Model Families
Convolution has been used for a long time as the main module to extract the image features in
computer vision tasks, and CNN-based architectures have been studied [ 70,29,83,57,84,55,87]
with extension on graph-based data [ 86,85,54]. Recently, another architecture family, Transformer,
has gained widespread attention owing to its great success in NLP [ 77,13,38]. Following this
direction, Dosovitskiy et al. [16] ﬁrst employ a transformer in the domain of computer vision and
introduce a new architecture paradigm, ViT, which achieves promising results [ 88,69]. Subsequently,
various transformer-based models, such as DeiT [ 74] and Swin Transformer [ 56], are introduced and
shown to be effective on a variety of tasks such as object detection, semantic segmentation, action
recognition [ 58],etc. In another line, Tolstikhin et al. [73] propose a pure MLP-based architecture,
and subsequent papers [ 35,50] have interestingly demonstrated that the MLP-based architectures
can catch up to transformers. However, in addition to the well-designed modules, their excellent
performance is also attributed to the deployment of large-scale models. Given a large-scale model
pre-trained on a large dataset, how to perform parameter-efﬁcient ﬁne-tuning in downstream tasks is
essential but is currently less explored. In this paper, we propose SSF as a new baseline and show its
promising performance with comprehensive validation in a wide variety of tasks.
2.2 Pre-training and Fine-tuning
Early models [ 29,39,37,83,72] are usually pre-trained on the ImageNet-1K dataset, and then
ﬁne-tuned on downstream tasks to achieve faster convergence [ 27] or better performance. Such a
procedure is called pre-training and ﬁne-tuning, or transfer learning. Recent works tend to employ
larger models ( e.g., ViT [ 16] and Swin Transformer V2 [ 56]) and train them on larger datasets
(e.g., ImageNet-21K and JFT-300M) in pursuit of better performance. Both in the domains of NLP
and computer vision, these large models [ 13,56,68,25,94,95] achieve enormous performance
improvements compared to the small-scale models and provide pre-trained weights for downstream
tasks. Some other works attempt to explore how to efﬁciently ﬁne-tune the pre-trained models
[23,96] on the target tasks. For instance, given a target task, SpotTune [ 23] investigates which
layers need to be ﬁne-tuned. Touvron et al. [75] ﬁnd that ﬁne-tuning the weights of the attention
layers and freezing weights of the other parts is sufﬁcient to adapt the vision transformers to other
downstream tasks. Some works also propose to insert adapters into the network to ﬁne-tune in a
parameter-efﬁcient way. These adapters can be a small non-linear network [ 36], a hyper-network
that generates model weights [ 61], or a compactor [ 60] which performs a low-rank decomposition
to reduce the parameters. Some works have also tried to only update the bias term [ 3,90]. More
recently, VPT [ 44] proposes to insert a small number of learnable parameters (prompts) and optimize
them while freezing the backbone, which achieves signiﬁcant performance improvement compared to
the full ﬁne-tuning. During the submission of this work, some methods [ 5,92] are also proposed for
parameter-efﬁcient ﬁne-tuning, e.g., inserting a adapter module or neural prompt search. Different
from all the above works, we propose to scale and shift deep features extracted by a pre-trained
model, which is simple but effective and outperforms other parameter-efﬁcient ﬁne-tuning methods.
2.3 Feature Modulation
Many works have attempted to modulate features to obtain better performance. The most relevant ones
to our work are various normalization methods [ 41,1,80]. BN, LN, and GN usually normalize the
features and then transform them linearly with scale and shift factors to modulate feature distribution,
which has been veriﬁed to be effective in amounts of tasks. STN [ 43] introduces a learnable module
to spatially transform feature maps. In the ﬁeld of image generation, AdaIN [ 40] generates scale
and shift factors to characterize speciﬁc image styles. Self-modulation [ 6] shows GANs beneﬁt
from self-modulation layers in the generator. In vision-language tasks, Conditional BN [ 11] and
FiLM [ 66] are often utilized to modulate the features of two modalities. Unlike some algorithms
such as BN, our SSF is not limited to the modulation of normalization layer, and it has a different
motivation that is to alleviate the distribution mismatch between upstream tasks and downstream tasks
for parameter-efﬁcient ﬁne-tuning. As a comparison, we also conduct experiments in Sec. 4.3 and
show that our SSF is more effective compared to only tuning the normalization layer. Compared to
STN, AdaIN, FiLM and so on, our method is input-independent and these scale and shift parameters
model the distribution of the whole dataset so that they can be absorbed into the original pre-trained
model weights in the inference phase.
3

--- PAGE 4 ---
2.4 Model Re-parameterization
Model re-parameterization has been a common practice to improve inference efﬁciency. One
of the representative techniques is batch normalization folding used in the model compression
algorithms [ 42]. The parameters introduced by the batch normalization layers [ 41] are merged into
the convolutional layers usually stacked before them. This technique is further utilized to merge
different branches of networks into a new branch [ 89,15,14]. Similarly, our SSF fully adopts linear
transformations, which allows the scale and shift parameters in the training phase to be merged into
the original pre-trained model weights, thus avoiding the introduction of the extra parameters and
computational cost during the inference phase.
3 Approach
3.1 Preliminaries
Transformers. In a vision transformer (ViT) [ 16], an RGB image I2R3HWis divided into
NNnon-overlapping patches, and then these image patches appended a class token are fed into
an embedding layer followed by the L-layer vision transformer blocks with self-attention as the core
operation. The input x2R(N2+1)d, wheredis the embedding dimension, is ﬁrst transformed to
keysK2R(N2+1)d, valuesV2R(N2+1)d, and queries Q2R(N2+1)d. After that, we can
calculate a global self-attention by
Attention(Q;K;V ) = Softmax(QKT
p
d)V: (1)
The output of the attention layer will be fed to a two-layer MLP to extract information in the channel
dimension.
Adapter. Adapter [ 36] is inserted into the transformer layer for efﬁcient ﬁne-tuning. It is a bottleneck
module with a few trainable parameters, which contains a down-projection to reduce the feature
dimension, a non-linear activation function, and an up-projection to project back to the original
dimension. Therefore, given the input x2R(N2+1)d, the output is calculated by
out = [Wup(WdownxT)]T; (2)
whereWdown2Rd0d(whered0d),, andWup2Rdd0represent the down-projection matrix,
non-linear function, and up-projection matrix, respectively.
VPT. VPT [ 44] inserts some learnable parameters ( i.e., prompts) into the input space after the
embedding layer. These prompts interact with the original image tokens by performing self-attention.
During the ﬁne-tuning, the weights of the backbone network are kept frozen and only the parameters
of the prompts are updated. VPT-Shallow inserts prompts in the ﬁrst layer while VPT-Deep inserts
prompts in all the layers of the transformer. Assuming that the input is x2R(N2+1)d, denote the
inserted prompts as p2Rnd, wherenis the number of prompts, the combined tokens x0is
x0= [x;p]; (3)
wherex02R(N2+n+1)dwill be fed into the transformer block for self-attention (Eq. (1)).
3.2 Scaling and Shifting Your Features for Fine-tuning
Different from the above methods, we introduce both the scale and shift factors to modulate deep
features extracted by a pre-trained model with linear transformation to match the distribution of
a target dataset, as mentioned in Sec. 1. Five main properties are covered in our method: i) SSF
achieves on-par performance with the full ﬁne-tuning strategy; ii) all downstream tasks can be
inputted to the model independently without relying on any other task; iii) the model only needs
to ﬁne-tune very few parameters; iv) unlike VPT [ 44], which adjusts the number of prompts for
each task, the set of parameters for ﬁne-tuning in SSF does not change as the task changes, making
it feasible to further ﬁne-tune the parameters later by adding more tasks for multi-task learning or
continuous learning2; v) thanks to the linear transformation, SSF avoids the introduction of the extra
parameters and computational cost during the inference phase, making our method zero overhead.
2It provides more ﬂexibility, which is not a contradiction to ii).
4

--- PAGE 5 ---
…
𝛾𝛽(c)SSF-ADA…OP1SSF-ADAOPiSSF-ADAOPKSSF-ADAHeadLayer……OP1OPiOPKHeadLayerscaleshiftInputOutput(a)Training
(b)Pre-trainedmodel&InferenceFrozenTrainableFeatureFigure 2: The overall pipeline of SSF. (a) Training
pipeline via SSF, where an OP means an operation,
e.g., MSA, MLP or LN. (b) A pre-trained model
or inference pipeline. (c) Our SSF-ADA.The design of SSF. SSF performs the linear
transformation to modulate the features for
parameter-efﬁcient ﬁne-tuning as shown in Fig-
ure 2. In Figure 2 (a), given a model pre-trained
in the upstream task, we insert SSF-ADA3after
each operation (OP) of the network to modulate
features. There are KOPs in total and these op-
erations might contain multi-head self-attention
(MSA), MLP and layer normalization (LN), etc.
During the ﬁne-tuning, the pre-trained weights
in these operations are kept frozen and the SSF-
ADA parameters are kept updated. The spe-
ciﬁc SSF-ADA structure is shown in Figure2 (c),
where the features output from the previous oper-
ation are performed dot product with a scale fac-
tor and then summed with a shift factor, which
are input-independent. Formally, given the input
x2R(N2+1)d, the output y2R(N2+1)d(is
also the input of the next operation) is calculated
by
y=x+; (4)
where2Rdand2Rdare the scale and shift factors, respectively. is the dot product.
Re-parameterization. Since SSF-ADA is a completely linear transformation, we can re-parameterize
it by absorbing the scale and shift terms into the previous linear layer as follows
y=x+=(wt+b) += (w)t+b+; (5)
wherewandbare the weight and bias terms, respectively. represents the ‘convolution’ operation
in the convolutional layer or the ‘multiplication’ operation in the MLP layer. tis the input of the
previous linear layer. Since wandbare frozen and andare updated in the ﬁne-tuning, and
can be merged into the original parameter space ( wandb) in the inference stage through the above
formulation. From this perspective, our SSF-ADA makes it possible to perform downstream tasks
without adding any extra parameters and computational costs, as shown in Figure2 (b).
Discussion. The ﬁrst question is why we want the input andto be input-independent. As FiLM
[66] and AdaIN [ 40] show, we could obtain andby conditioning an image sample, however, this
might cause two shortcomings. One is that we want andto be input-independent to represent the
distribution of the whole downstream dataset so that we can modify the previous weight distribution
to ﬁt the downstream dataset by modulating the feature. Secondly, the conditional input requires the
introduction of some additional networks ( e.g., MLPs) to generate and, which introduces more
trainable parameters. More importantly, to better generate and, a non-linear activation function
might be required, which will lead to the intractability of the re-parameterization. Therefore, we
directly perform a fully linear transformation to merge the andfactors into the original pre-trained
weights, so that weights can be easily uploaded to the edge devices without any modiﬁcation of the
backbone architecture.
The second question is which operations should be followed by SSF-ADA. Our experience is that
you can insert SSF-ADA after each operation with a linear coefﬁcient in ViT. Although we can search
for some optimal layers or operations with Neural Architecture Search (NAS) [ 67,53,24,51], to
reduce the number of the trainable parameters, we believe that our method will produce better results
(or not worse than NAS) without introducing too many trainable parameters that can be merged for
inference, as will be shown in Sec. 4.3.
3.3 Complexity Analysis
We also compare the complexity of Adapter, VPT and our SSF. Take a ViT as an example, the
dimension and number of the tokens are dandN2. Assuming that Adapter projects features from
d-dim tod0-dim (where d0d) so that the extra trainable parameters are 2dd0in each layer,
3Here, we refer to our proposed method as SSF and the speciﬁc module as SSF-ADA.
5

--- PAGE 6 ---
Method Adapter VPT-Shallow VPT-Deep SSF (ours)
# Extra Params. 2Ldd0(1) nd(1) nLd (1) mLd (0)
# Extra FLOPs 2N2Ldd0(1) 2n(2N2+n)d(1) 2n(2N2+n)Ld(1) mN2Ld(0)
Table 2: The complexity comparisons of Adapter [ 36], VPT
[44] and our SSF. ‘(1)’: the same parameters and FLOPs for
training and inference; ‘(0)’: no additional parameters and
FLOPs are required for inference.VPT insertsnprompts to obtain ndex-
tra parameters in each layer, and SSF
inserts SSF-ADA after each operation
with a linear coefﬁcient to obtain md
extra parameters in each layer, when
the total number of layers is L, the
complexity of Adapter, VPT and SSF
is shown in Table 2. The speciﬁc num-
ber of additional parameters used by
Adapter, VPT and SSF depends on the values of d0,nandm. However, in practice, SSF outperforms
Adapter and VPT-Deep even with slightly fewer parameters in the training stage as we will see in
Sec. 4. Further, in the inference stage, borrowing the model re-parameterization strategy, the extra
parameters and FLOPs of SSF are zero. However, the complexity of Adapter and VPT remain the
same compared to the training, which establishes the strengths of our approach.
4 Experiments
4.1 Experimental Settings
Datasets . We mainly conduct our experiments on a series of datasets that can be categorized into
three types as detailed below:
FGVC . Following VPT [ 44], we employ ﬁve Fine-Grained Visual Classiﬁcation (FGVC) datasets to
evaluate the effectiveness of our proposed SSF, which consists of CUB-200-2011 [ 79], NABirds [ 76],
Oxford Flowers [64], Stanford Dogs [46] and Stanford Cars [18].
VTAB-1k . VTAB-1k benchmark is introduced in [ 91], which contains 19 tasks from diverse domains:
i) Natural images that are captured by standard cameras; ii) Specialized images that are captured
by non-standard cameras, e.g., remote sensing and medical cameras; iii) Structured images that are
synthesized from simulated environments. This benchmark contains a variety of tasks ( e.g., object
counting, depth estimation) from different image domains and each task only contains 1,000 training
samples, thus is extremely challenging.
General Image Classiﬁcation Datasets . We also validate the effectiveness of SSF on general image
classiﬁcation tasks. We choose the CIFAR-100 [ 47] and ImageNet-1K [ 12] datasets as evaluation
datasets, where CIFAR-100 contains 60,000 images with 100 categories. ImageNet-1K contains
1.28M training images and 50K validation images with 1,000 categories, which are very large datasets
for object recognition.
Models . For a fair comparison, we follow VPT [ 44] and mainly select ViT-B/16 [ 16] model pre-
trained on ImageNet-21K as the initialization for ﬁne-tuning. In addition, we also generalize our
method to backbones of different model families, including the recent Swin Transformer [ 56] (Swin-
B), ConvNeXt-B [ 57] and AS-MLP-B [ 50]. The former builds a hierarchical transformer-based
architecture, and the latter two belong to CNN-based architecture and MLP-based architecture
respectively.
Baselines . We ﬁrst compare our method with the two basic ﬁne-tuning methods: i) full ﬁne-tuning,
where all parameters of the models are updated at ﬁne-tuning; ii) linear probing, where only the
parameters of the classiﬁcation head (an MLP layer) are updated. We also compare our method with
recent parameter-efﬁcient ﬁne-tuning methods: iii) Adapter [ 36], where a new adapter structure with
up-projection, non-linear function, and down-projection is inserted into the transformer and only the
parameters of this new module are updated; iv) Bias [ 90], where all the bias terms of parameters are
updated; v) VPT [ 44], where the prompts are inserted into transformers as the input tokens and they
are updated at ﬁne-tuning.
Implementation Details. For the FGVC datasets, we process the image with a randomly resize crop
to224224and a random horizontal ﬂip for data augmentation. For VTAB-1k, we directly resize
the image to 224224, following the default settings in VTAB [ 91]. For CIFAR-100 and ImageNet-
1K, we follow the ﬁne-tuning setting of ViT-B/16 in [ 16], where the stronger data augmentation
strategies are adopted. We employ the AdamW [ 59] optimizer to ﬁne-tune models for 100 epochs for
CIFAR-100, and 30 epochs for ImageNet-1K. The cosine decay strategy is adopted for the learning
rate schedule, and the linear warm-up is used in the ﬁrst 10 epochs for CIFAR-100 and 5 epochs for
ImageNet-1K.
6

--- PAGE 7 ---
MethodDataset CUB-200
-2011NABirdsOxford
FlowersStanford
DogsStanford
CarsMeanParams.
(M)
Full ﬁne-tuning 87.3 82.7 98.8 89.4 84.5 88.54 85.98
Linear probing 85.3 75.9 97.9 86.2 51.3 79.32 0.18
Adapter [36] 87.1 84.3 98.5 89.8 68.6 85.67 0.41
Bias [90] 88.4 84.2 98.8 91.2 79.4 88.41 0.28
VPT-Shallow [44] 86.7 78.8 98.4 90.7 68.7 84.62 0.25
VPT-Deep [44] 88.5 84.2 99.0 90.2 83.6 89.11 0.85
SSF (ours) 89.5 85.7 99.6 89.6 89.2 90.72 0.39
Table 3: Performance comparisons on ﬁve FGVC datasets with ViT-B/16 models pre-trained on
ImageNet-21K.
Natural Specialized Structured
MethodDataset
CIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/loc
dSprites/ori
SmallNORB/azi
SmallNORB/ele
Mean
Params. (M)
Full ﬁne-tuning [44] 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.57 85.84
Linear probing [44] 63.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.6 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 52.94 0.04
Adapter [36] 74.1 86.1 63.2 97.7 87.0 34.6 50.8 76.3 88.0 73.1 70.5 45.7 37.4 31.2 53.2 30.3 25.4 13.8 22.1 55.82 0.27
Bias [90] 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.05 0.14
VPT-Shallow [44] 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 64.85 0.11
VPT-Deep [44] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 69.43 0.60
SSF (ours) 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 73.10 0.24
Table 4: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on
ImageNet-21K.
4.2 Performance Comparisons on Image Classiﬁcation
We compare the performance of our SSF and other baseline methods in 26 image classiﬁcation
tasks and the results on FGVC and VTAB-1k are shown in Table 3 and Table 4 (also see Figure
1), respectively, and the results on CIFAR-100 and ImageNet-1K are shown in Table 5, which are
evaluated in Top-1 accuracy (%). In these three tables, the bold font shows the best accuracy of all
methods and the underline font shows the second best accuracy.
We have the following ﬁndings by observing them: i) In Table 3 and Table 4, where the last column
is the average of the ﬁne-tuned parameters for each method on the corresponding datasets, our
SSF outperforms VPT [ 44] and other parameter-efﬁcient ﬁne-tuning methods, and even achieves
better performance than full ﬁne-tuning, which is mainly owing to the linear transformation applied
on the features. Speciﬁcally, SSF obtains 1.81% (90.72% vs. 89.11%) and 2.46% (90.72% vs.
88.54%) accuracy improvement on ﬁve FGVC datasets, and 5.29% (73.10% vs. 69.43%) and
11.48% (73.10% vs. 65.57%) improvement on the VTAB-1k benchmark compared to VPT and full
ﬁne-tuning. Meanwhile, SSF also uses fewer trainable parameters compared to VPT-Deep in both
datasets (0.39M vs. 0.85M, 0.24M vs. 0.60M). SSF maintains a uniﬁed learnable parameter space
for different tasks with a few parameters while VPT [ 44] needs to design the different number of
prompts for each task, which also shows the conciseness of our approach; ii) In Table 5, i.e., in
CIFAR-100 and ImageNet-1K, SSF and other parameter-efﬁcient ﬁne-tuning methods have difﬁculty
in achieving the similar performance to the full ﬁne-tuning, probably because these datasets have
sufﬁcient data to prevent over-ﬁtting of the model, especially in ImageNet-1K. In contrast, in the
VTAB-1k benchmark, the amount of data is not very large ( e.g., only 1,000 training images), which
might cause over-ﬁtting of the model for the full ﬁne-tuning. Nevertheless, in CIFAR-100 and
ImageNet-1K, our SSF still outperforms previous parameter-efﬁcient ﬁne-tuning methods (Adapter,
Bias, and VPT), which shows the effectiveness of our method; iii) In Table 5, the results of our SSF
with Swin Transformer, ConvNeXt, and AS-MLP models consistently outperform those of other
parameter-efﬁcient ﬁne-tuning methods, which also veriﬁes the effectiveness of SSF on a wide variety
of models.
Computational cost. To validate the efﬁciency of our method, we show the computational cost of
SSF in Figure 3. We employ a batch size of 16 for the training stage and inference stage, and use
mixed precision training. All running results in Figure 3 are measured in a single GeForce RTX
2080Ti GPU. We can see that SSF has similar training time and training memory with VPT but with
7

--- PAGE 8 ---
Model ViT-B/16 [16] Swin-B [56] ConvNeXt-B [57] AS-MLP-B [50]
MethodDataset
CIFAR-100
Params. (M)
ImageNet-1K
Params. (M)
CIFAR-100
Params. (M)
ImageNet-1K
Params. (M)
CIFAR-100
Params. (M)
ImageNet-1K
Params. (M)
CIFAR-100
Params. (M)
Full ﬁne-tuning 93.82 85.88 83.58 86.57 93.85 86.85 85.20 88.03 94.14 87.67 85.80 88.85 89.96 86.83
Linear probing 88.70 0.08 82.04 0.77 89.27 0.10 83.25 1.03 89.20 0.10 84.05 1.03 79.04 0.10
Adapter [36] 93.34 0.31 82.72 1.00 92.49 0.33 83.82 1.26 92.86 0.45 84.49 1.37 88.01 0.33
Bias [90] 93.39 0.18 82.74 0.87 92.19 0.24 83.92 1.16 92.80 0.23 84.63 1.16 87.46 0.26
VPT-Shallow [44] 90.38 0.23 82.08 0.92 90.02 0.13 83.29 1.05 - - - - - -
VPT-Deep [44] 93.17 0.54 82.45 1.23 92.62 0.70 83.44 1.63 - - - - - -
SSF (ours) 93.99 0.28 83.10 0.97 93.06 0.37 84.40 1.29 93.45 0.36 84.85 1.28 88.28 0.37
Table 5: Performance comparisons on CIFAR-100 and ImageNet-1K with various model families,
where ViT-B/16, Swin-B, and ConvNeXt-B are pre-trained on ImageNet-21K, and AS-MLP-B is
pre-trained on ImageNet-1K.
Full fine-tuning Linear probingAdapterBias
VPT-ShallowVPT-Deep SSF (ours)020406080100120140Training time (ms)
Full fine-tuning Linear probingAdapterBias
VPT-ShallowVPT-Deep SSF (ours)012345Training memory (G)
Full fine-tuning Linear probingAdapterBias
VPT-ShallowVPT-Deep SSF (ours)01020304050Test time (ms)
Full fine-tuning Linear probingAdapterBias
VPT-ShallowVPT-Deep SSF (ours)0.000.250.500.751.001.251.501.752.00Test memory (G)
Figure 3: Computational cost of different tuning methods. From left to right: training time, training
memory, test time, and test memory.
less inference time and inference memory. Here, we show the computational cost of VPT with 200/50
prompts (the same number of prompts to obtain the performance in Table 5) for VPT-Shallow and
VPT-Deep, respectively. When adding the number of prompts, the time cost and memory will be
larger but our SSF achieves zero-overhead inference, which is more advantageous.
4.3 The Impacts of Different Designs
As the core operation of SSF, we thoroughly evaluate how SSF-ADA affects results, e.g., the insertion
locations, the initialization of SSF-ADA and its components. We conduct experiments to analyze
the impacts of different designs for ﬁne-tuning. All experiments are implemented with pre-trained
ViT-B/16 models on CIFAR-100 and the results are shown in Table 6.
The impact of the number of layers. We directly insert SSF-ADA into different layers to evaluate
the effect of inserting layers, and the results are shown in table 6a. The values in the #layers column
indicate the number of layers with SSF-ADA, where #layers-0 represents linear probing. From the
ﬁrst and second rows, we ﬁnd that the results will improve from 88.70% to 92.69% and grow with a
small number of trainable parameters (0.08M vs.0.11M) when only inserting SSF-ADA into the ﬁrst
two layers. Keep adding SSF-ADA in the subsequent layers will make the results better. The growth
of the results is almost linear with the number of layers of inserted SSF-ADA. Therefore, we directly
choose to insert SSF-ADA into all (12) layers of vision transformer to bring the best results (93.99%)
with 0.28M trainable parameters.
The impact of the different insertion locations. Based on the different operations of ViT, we
evaluate the impact of the insertion locations of SSF-ADA. We separately remove SSF-ADA after
these operations and the results are shown in Table 6b. We ﬁnd that removing the SSF-ADA in the
MLP operation achieves inferior results than removing those in the Attention operation (93.46% vs.
93.69%) with comparable trainable parameters (0.19M vs.0.21M), which suggests that performing
feature modulation for the MLP operation might be more important. Although one can use NAS to
search for the importance of different operations and thereby insert SSF-ADA in speciﬁc locations,
the results might not be better than inserting SSF-ADA in all operations. Therefore, in order to obtain
excellent performance, we do not perform NAS but directly insert SSF-ADA into all operations.
8

--- PAGE 9 ---
#layers Acc. Params.
0 88.70 0.08
2 92.69 0.11
4 93.30 0.15
8 93.60 0.22
12 (ours) 93.99 0.28
(a)location Acc. Params.
w/o. mlp 93.46 0.19
w/o. attn 93.69 0.21
w/o. embed 93.91 0.28
w/o. norm 93.80 0.25
ours 93.99 0.28
(b)initialization Acc.
random 90.11
constant 93.91
uniform 93.87
trunc_normal 93.93
normal (ours) 93.99
(c)case Acc. Params.
w/o. scale 93.49 0.18
w/o. shift 93.74 0.18
only norm 93.26 0.11
scalar scale 93.59 0.18
ours 93.99 0.28
(d)
Table 6: The impacts of different designs. (a) The impact of the number of layers with SSF-ADA. (b)
The impacts of the different insertion locations of SSF-ADA. (c) The impacts of initialization. (d)
The impacts of different components. Acc.: Top-1 accuracy (%); Params.: parameters (M).
The impact of initialization. We also investigate how different ways of initializing the scale and
shift factors affect performance in Table 6c. In our experiments, we ﬁrst randomly initialize both scale
and shift parameters with a mean value of zero, but ﬁnd that the performance is inferior (90.11%)
and cannot converge in some experiments. After that, we randomly initialize the scale factor with a
mean value of one and ﬁnd better performance, which implies that the weights of a pre-trained model
should not be completely disrupted in the ﬁne-tuning, instead, we should start from this pre-trained
model to optimize our model. Experiments show that using the normal initialization achieves the best
performance, where the mean values of the scale factor and shift factor are one and zero, respectively.
The impact of different components. We also evaluate the impacts of different components in
SSF-ADA and the results are shown in Table 6d. We ﬁnd that removing the scale term yields worse
performance than removing the shift term with the same trainable parameters, which shows that the
scale term might be more important than the shift term. Also, note that the difference between ‘w/o.
scale’ and the ‘Bias’ method in Table 5 is that we ﬁne-tune the model with an additional shift term in
‘w/o. scale’, while ‘Bias’ ﬁne-tunes the model based on the original biases, suggesting that ﬁne-tuning
the model in a res-like manner can obtain slightly better performance (93.49% vs. 93.39%). We also
try to only ﬁne-tune all scale and shift factors in the normalization layer (LN), or ﬁne-tune the model
with SSF but set the scale term as a scalar. These experiments yield inferior performance than SSF
(93.26% vs. 93.99%, 93.59% vs. 93.99%), but could probably be considered as an alternative due to
the fact that they only use about half of the trainable parameters of SSF.
4.4 Performance Comparisons on Robustness and OOD Datasets
MethodDatasetIN-1K (") IN-A (") IN-R (") IN-C (#)
Full ﬁne-tuning 83.58 34.49 51.29 46.47
Linear probing 82.04 33.91 52.87 46.91
Adapter [36] 82.72 42.21 54.13 42.65
Bias [90] 82.74 42.12 55.94 41.90
VPT-Shallow [44] 82.08 30.93 53.72 46.88
VPT-Deep [44] 82.45 39.10 53.54 43.10
SSF (ours) 83.10 45.88 56.77 41.47
Table 7: Performance comparisons on robustness and
out-of-distribution datasets. ‘IN’ means ImageNet. The
performance on IN-1K, IN-A and IN-R is evaluated in
Top-1 accuracy (%). The performance on IN-C is evalu-
ated in mCE (mean corruption error). The lower ( #), the
better.We also conduct experiments to analyze
the robustness and Out-Of-Distribution
(OOD) ability of our SSF method with
the following datasets: ImageNet-A,
ImageNet-R and ImageNet-C. Please refer
to Appendix A.2 for their details. We per-
form the robustness and OOD evaluation
on these three datasets with the ﬁne-tuned
models on ImageNet-1K. All experimen-
tal results are listed in Table 7.
From this table, we can see that our SSF
obtains better performance than VPT and
other parameter-efﬁcient ﬁne-tuning meth-
ods on three datasets, which shows our
ﬁne-tuning method has stronger robust-
ness and out-of-distribution generalization. Furthermore, although SSF has lower accuracy than full
ﬁne-tuning on ImageNet-1K, the performance on ImageNet-A, ImageNet-R and ImageNet-C is better,
which also shows the performance between ImageNet-1K and ImageNet-A/R/C is not absolutely
positive relevant. Such improvements in robustness and OOD datasets might come from the fact that
SSF freezes most of the pre-trained parameters, which maximally preserves the knowledge learned
from the large-scale dataset and thus maintains a better generalization ability.
4.5 Visualization and Analysis
Although our goal is to modulate the features extracted by a pre-trained model, the scale and shift
parameters are input-independent indeed. Therefore, these parameters can also be regarded as
encoding information of the whole downstream dataset. After re-parameterization, these scale and
9

--- PAGE 10 ---
Layer1Layer1Layer4Layer8Layer11
Layer1Layer4Layer8Layer11
(a) Pre-trained model vs. Fine-tuned model via SSF.
Layer1
Layer1Layer4Layer8Layer11
Layer1Layer4Layer8Layer11 (b) Pre-trained model vs. Full ﬁne-tuned model.
Figure 4: Comparisons of parameter distribution between the original pre-trained model and different
ﬁne-tuning methods. The ﬁrst row shows weight distribution and the second row is bias distribution.
The blue histograms show the original pre-trained model, and the orange ones show the ﬁne-tuned
model via SSF in (a) and full ﬁne-tuned model in (b).
shift parameters are absorbed into the original model weights. To better understand information
learned by the SSF, we visualize the distributions of weights and biases before and after ﬁne-
tuning via SSF in Figure 4a. We can see that the scale and shift parameters adjust the original
weights and biases, and change the distribution of weights and biases to ﬁt the downstream task.
0 2 4 6 8 10
Layers0.20.30.40.50.60.70.80.91.0Feature similarity
Linear probing
VPT-Deep
SSF (ours)
Figure 5: The visualization of the
feature similarities between full
ﬁne-tuning and linear probing, full
ﬁne-tuning and VPT-Deep, full ﬁne-
tuning and SSF, in different layers
of ViT-B/16.As a comparison, we also visualize the original weight distribu-
tion and the weight distribution after full ﬁne-tuning in Figure
4b, from which we can ﬁnd an interesting phenomenon that
full ﬁne-tuning does not change the distribution of weights and
biases much, but probably only a small portion of the values is
changed. It is worth noting that although SSF does not match
the weight distribution of full ﬁne-tuning, it achieves better
performance (93.99% vs. 93.82% in Table 5) on CIFAR-100.
To further investigate why SSF can achieve superior perfor-
mance, beyond weight distribution, we also visualize the fea-
ture similarities between full ﬁne-tuning and linear probing, full
ﬁne-tuning and VPT-Deep, full ﬁne-tuning and SSF, as shown
in Figure 5. In the last layer, SSF has the most similar feature to
full ﬁne-tuning and the accuracy is also the closest. This shows
that even if the weight distribution learned by SSF is different
from full ﬁne-tuning, SSF is also able to extract the features of
the images in the downstream task very well, which validates
the effectiveness of our method.
5 Conclusion
In this paper, we focus on parameter-efﬁcient ﬁne-tuning and propose an SSF method to scale and
shift the features extracted by a pre-trained model. The intuition behind our method comes from
alleviating the distribution mismatch between upstream tasks and downstream tasks by modulating
deep features. SSF surprisingly outperforms other parameter-efﬁcient ﬁne-tuning approaches with
a small number of learnable parameters. Besides, the introduced scale and shift parameters during
the ﬁne-tuning can be merged into the original pre-trained model weights via re-parameterization in
the inference phase, thereby avoiding extra parameters and FLOPs. With the proposed SSF method,
our model obtains 2.46% (90.72% vs.88.54%) and 11.48% (73.10% vs. 65.57%) performance
improvement on FGVC and VTAB-1k in terms of Top-1 accuracy compared to the full ﬁne-tuning
but only ﬁne-tuning about 0.3M parameters. Experiments on 26 image classiﬁcation datasets in total
and 3 robustness & out-of-distribution datasets with various model families (CNNs, Transformers,
and MLPs) show the effectiveness of SSF, which establishes a new baseline.
Acknowledgement
The authors acknowledge the support from the Singapore National Research Foundation (“Cog-
niVision – Energy-autonomous always-on cognitive and attentive cameras for distributed real-time
vision with milliwatt power consumption” grant NRF-CRP20-2017-0003) – www.green-ic.org/
CogniVision . Xinchao Wang is the corresponding author.
10

--- PAGE 11 ---
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[2]Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich
Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv
preprint arXiv:1612.03801 , 2016.
[3]Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for
efﬁcient on-device learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems , volume 33, pages 11285–11297.
Curran Associates, Inc., 2020.
[4]Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun,
Wansen Feng, Ziwei Liu, Jiarui Xu, et al. MMDetection: Open mmlab detection toolbox and
benchmark. arXiv preprint arXiv:1906.07155 , 2019.
[5]Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping
Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. arXiv preprint
arXiv:2205.13535 , 2022.
[6]Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative
adversarial networks. In International Conference on Learning Representations (ICLR) , 2019.
[7]Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classiﬁcation:
Benchmark and state of the art. Proceedings of the IEEE , 105(10):1865–1883, 2017.
[8]Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 3606–3613, 2014.
[9]MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox
and benchmark. https://github.com/open-mmlab/mmsegmentation , 2020.
[10] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. Advances in Neural Information Processing Systems , 34:3965–3977,
2021.
[11] Harm De Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C
Courville. Modulating early visual processing by language. Advances in Neural Information
Processing Systems , 30, 2017.
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 248–255, 2009.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,
2018.
[14] Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and Guiguang Ding.
Repmlp: Re-parameterizing convolutions into fully-connected layers for image recognition.
arXiv preprint arXiv:2105.01883 , 2021.
[15] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.
Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13733–13742, 2021.
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. In International
Conference on Learning Representations (ICLR) , 2021.
[17] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE
transactions on pattern analysis and machine intelligence , 28(4):594–611, 2006.
[18] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-
grained car detection for visual census estimation. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , volume 31, 2017.
[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The
kitti dataset. The International Journal of Robotics Research , 32(11):1231–1237, 2013.
[20] Andrea Gesmundo and Jeff Dean. An evolutionary approach to dynamic introduction of tasks
in large-scale multitask learning systems. arXiv preprint arXiv:2205.12755 , 2022.
[21] Andrea Gesmundo and Jeff Dean. munet: Evolving pretrained deep neural networks into
scalable auto-tuning multitask systems. arXiv preprint arXiv:2205.10937 , 2022.
[22] Ben Graham. Kaggle diabetic retinopathy detection competition report. University of Warwick ,
pages 24–26, 2015.
11

--- PAGE 12 ---
[23] Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio
Feris. Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 4805–4814, 2019.
[24] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun.
Single path one-shot neural architecture search with uniform sampling. In European Conference
on Computer Vision , pages 544–560. Springer, 2020.
[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 16000–16009, 2022.
[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9729–9738, 2020.
[27] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In Proceedings
of the IEEE/CVF International Conference on Computer Vision , pages 4918–4927, 2019.
[28] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV) , pages 2961–2969, 2017.
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 770–778, 2016.
[30] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-
efﬁcient ﬁne-tuning for vision transformers. arXiv preprint arXiv:2203.16329 , 2022.
[31] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
dataset and deep learning benchmark for land use and land cover classiﬁcation. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217–2226, 2019.
[32] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,
Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness:
A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8340–8349, 2021.
[33] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261 , 2019.
[34] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 15262–15271, 2021.
[35] Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng.
Vision permutator: A permutable mlp-like architecture for visual recognition. arXiv preprint
arXiv:2106.12368 , 2021.
[36] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning
for nlp. In International Conference on Machine Learning , pages 2790–2799. PMLR, 2019.
[37] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, To Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.
[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu
Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
[39] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 4700–4708, 2017.
[40] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance
normalization. In Proceedings of the IEEE international conference on computer vision , pages
1501–1510, 2017.
[41] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning , pages
448–456. PMLR, 2015.
[42] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efﬁcient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2704–2713, 2018.
[43] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks.
Advances in neural information processing systems , 28, 2015.
12

--- PAGE 13 ---
[44] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,
and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision (ECCV) ,
2022.
[45] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 2901–2910, 2017.
[46] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for
ﬁne-grained image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained
Visual Categorization (FGVC) , volume 2. Citeseer, 2011.
[47] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[48] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition
with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004. , volume 2, pages
II–104. IEEE, 2004.
[49] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation.
arXiv preprint arXiv:2101.00190 , 2021.
[50] Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architec-
ture for vision. In International Conference on Learning Representations (ICLR) , 2022.
[51] Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang,
and Shenghua Gao. Towards fast adaptation of neural architectures with meta learning. In
International Conference on Learning Representations (ICLR) , 2020.
[52] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European
Conference on Computer Vision (ECCV) , pages 740–755. Springer, 2014.
[53] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In
International Conference on Learning Representations (ICLR) , 2019.
[54] Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming catastrophic forgetting in graph
neural networks. In AAAI Conference on Artiﬁcial Intelligence , 2021.
[55] Songhua Liu, Jingwen Ye, Sucheng Ren, and Xinchao Wang. Dynast: Dynamic sparse trans-
former for exemplar-guided image generation. In Proceedings of the European Conference on
Computer Vision , 2022.
[56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV) , 2021.
[57] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2022.
[58] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin
transformer. arXiv preprint arXiv:2106.13230 , 2021.
[59] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations (ICLR) , 2019.
[60] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient
low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647 , 2021.
[61] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.
Parameter-efﬁcient multi-task ﬁne-tuning for transformers via shared hypernetworks. arXiv
preprint arXiv:2106.04489 , 2021.
[62] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentan-
glement testing sprites dataset, 2017. URL https://github. com/deepmind/dsprites-dataset ,
2020.
[63] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
Reading digits in natural images with unsupervised feature learning. 2011.
[64] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large
number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image
Processing , pages 722–729. IEEE, 2008.
[65] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012
IEEE conference on computer vision and pattern recognition , pages 3498–3505. IEEE, 2012.
[66] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , volume 32, 2018.
13

--- PAGE 14 ---
[67] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture
search via parameters sharing. In International conference on machine learning , pages 4095–
4104. PMLR, 2018.
[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021.
[69] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang. Shunted self-
attention via multi-scale token aggregation. In IEEE Conference on Computer Vision and
Pattern Recognition , 2022.
[70] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. In International Conference on Learning Representations (ICLR) , 2015.
[71] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 30, 2016.
[72] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. In International Conference on Machine Learning (ICML) , pages 6105–6114, 2019.
[73] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. MLP-Mixer:
An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601 , 2021.
[74] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. In
International Conference on Machine Learning (ICML) , 2021.
[75] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Hervé Jégou. Three
things everyone should know about vision transformers. arXiv preprint arXiv:2203.09795 ,
2022.
[76] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro
Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with
citizen scientists: The ﬁne print in ﬁne-grained dataset collection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , pages 595–604, 2015.
[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems (NeurIPS) , 2017.
[78] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation
equivariant cnns for digital pathology. In International Conference on Medical image computing
and computer-assisted intervention , pages 210–218. Springer, 2018.
[79] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The
caltech-ucsd birds-200-2011 dataset. 2011.
[80] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV) , pages 3–19, 2018.
[81] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:
Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference
on computer vision and pattern recognition , pages 3485–3492. IEEE, 2010.
[82] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing
for scene understanding. In Proceedings of the European Conference on Computer Vision
(ECCV) , pages 418–434, 2018.
[83] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , July 2017.
[84] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In
Proceedings of the European Conference on Computer Vision , 2022.
[85] Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional
networks. In Conference on Neural Information Processing Systems , 2020.
[86] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling knowledge
from graph convolutional networks. In IEEE Conference on Computer Vision and Pattern
Recognition , 2020.
[87] Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli Song, and Xinchao
Wang. Learning with recoverable forgetting. In Proceedings of the European Conference on
Computer Vision , 2022.
[88] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and
Shuicheng Yan. Metaformer is actually what you need for vision. In IEEE Conference on
Computer Vision and Pattern Recognition , 2022.
14

--- PAGE 15 ---
[89] Sergey Zagoruyko and Nikos Komodakis. Diracnets: Training very deep neural networks
without skip-connections. arXiv preprint arXiv:1706.00388 , 2017.
[90] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient
ﬁne-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 ,
2021.
[91] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A
large-scale study of representation learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867 , 2019.
[92] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint
arXiv:2206.04673 , 2022.
[93] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 633–641, 2017.
[94] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou,
and Jiashi Feng. DeepViT: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886 ,
2021.
[95] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima Anandkumar, Jiashi Feng,
and Jose M Alvarez. Understanding the robustness in vision transformers. arXiv preprint
arXiv:2204.12451 , 2022.
[96] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for
vision-language models. arXiv preprint arXiv:2109.01134 , 2021.
15

--- PAGE 16 ---
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes] See abstract, introduction and experiments.
(b) Did you describe the limitations of your work? [Yes] See appendix.
(c)Did you discuss any potential negative societal impacts of your work? [Yes] See
appendix.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See abstract
and experiments.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See experiments.
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A] A part of the experiments is tested several times.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See experiments.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e)Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [Yes] See appendix.
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
16

--- PAGE 17 ---
A Detailed Descriptions for the Evaluation Datasets
A.1 Image Classiﬁcation
We show the detailed descriptions of image classiﬁcation as follows. The train/val/test splits and the
classes are shown in Table 8.
Dataset Description #Classes Train size Val size Test size
Fine-Grained Visual Classiﬁcation (FGVC)
CUB-200-2011 [79] Fine-grained bird species recognition 200 5,394?600?5,794
NABirds [76] Fine-grained bird species recognition 55 21,536?2,393?24,633
Oxford Flowers [64] Fine-grained ﬂower species recognition 102 1,020 1,020 6,149
Stanford Dogs [46] Fine-grained dog species recognition 120 10,800?1,200?8,580
Stanford Cars [18] Fine-grained car classiﬁcation 196 7,329?815?8,041
Visual Task Adaptation Benchmark (VTAB-1k) [91]
CIFAR-100 [47]
Natural100
800/1,000 20010,000
Caltech101 [17] 102 6,084
DTD [8] 47 1,880
Flowers102 [64] 102 6,149
Pets [65] 37 3,669
SVHN [63] 10 26,032
Sun397 [81] 397 21,750
Patch Camelyon [78]
Specialized2
800/1,000 20032,768
EuroSAT [31] 10 5,400
Resisc45 [7] 45 6,300
Retinopathy [22] 5 42,670
Clevr/count [45]
Structured8
800/1,000 20015,000
Clevr/distance [45] 6 15,000
DMLab [2] 6 22,735
KITTI/distance [19] 4 711
dSprites/location [62] 16 73,728
dSprites/orientation [62] 16 73,728
SmallNORB/azimuth [48] 18 12,150
SmallNORB/elevation [48] 9 12,150
General Image Classiﬁcation Datasets
CIFAR-100 [47]General image classiﬁcation100 50,000 - 10,000
ImageNet-1K [12] 1,000 1,281,167 50,000 150,000
Robustness and Out-of-Distribution Dataset
ImageNet-A [34]
Robustness & OOD200 7,500
ImageNet-R [32] 200 30,000
ImageNet-C [33] 1,000 7550,000
Table 8: The statistics of the various datasets.?: Since there are no public train/val splits in these
datasets, we follow VPT [ 44] for random train/val split. This table is partially borrowed from VPT
[44].
FGVC . Following VPT [ 44], we employ ﬁve Fine-Grained Visual Classiﬁcation (FGVC) datasets to
evaluate the effectiveness of our proposed SSF, which consists of CUB-200-2011 [ 79], NABirds [ 76],
Oxford Flowers [64], Stanford Dogs [46] and Stanford Cars [18].
VTAB-1k . VTAB-1k benchmark is introduced in [ 91], which contains 19 tasks from diverse domains:
i) Natural images that are captured by standard cameras; ii) Specialized images that are captured
by non-standard cameras, e.g., remote sensing and medical cameras; iii) Structured images that are
synthesized from simulated environments. This benchmark contains a variety of tasks ( e.g., object
counting, depth estimation) from different image domains and each task only contains 1,000 training
samples, thus is extremely challenging.
General Image Classiﬁcation Datasets . We also validate the effectiveness of SSF on general image
classiﬁcation tasks. We choose the CIFAR-100 [ 47] and ImageNet-1K [ 12] datasets as evaluation
datasets, where CIFAR-100 contains 60,000 images with 100 categories. ImageNet-1K contains
1.28M training images and 50K validation images with 1,000 categories, which are very large datasets
for object recognition.
17

--- PAGE 18 ---
A.2 Robustness and OOD
ImageNet-A is introduced in [ 34], where 200 classes from 1,000 classes of ImageNet-1K are chosen
and the real-world adversarial samples that make the ResNet model mis-classiﬁed are collected.
ImageNet-R [32] contains rendition of 200 ImageNet-1K classes and 30,000 images in total.
ImageNet-C [33] consists of the corrupted images, including noise, blur, weather, etc.The perfor-
mance of model on ImageNet-C show the robustness of model.
A.3 Detection and Segmentation
We also conduct experiments on downstream tasks beyond image classiﬁcation, such as object
detection, instance segmentation and semantic segmentation. We employ the COCO dataset [ 52] for
evaluation based on mmdetection [4] framework for the object detection and instance segmentation.
COCO contains 118K training images for training and 5K images for validation, which is one of
the most challenging object detection datasets. We use Mask R-CNN [ 28] with Swin Transformer
backbone to perform our experiments, following the same training strategies as Swin Transformers
[56]. For semantic segmentation, we employ the ADE20K dataset [ 93] for evaluation based on
mmsegmentation [ 9] framework. ADE20K contains 20,210 training images and 2,000 validation
images. Following Swin Transformer [ 56], we use UperNet [ 82] with Swin Transformer backbone .
All models are initialized with weights pre-trained on ImageNet-1K for detection and segmentation.
COCO with Mask R-CNN ADE20K with UPerNet
MethodDataset
APbAPb
50APb
75APmAPm
50APm
75mIoU MS mIoU
Full ﬁne-tuning 43.7 66.6 47.7 39.8 63.3 42.7 44.5 45.8
Linear probing 31.7 55.7 32.5 31.2 53.0 32.2 35.7 36.8
VPT-Deep [44] 33.8 57.6 35.3 32.5 54.5 33.9 37.0 37.9
SSF (ours) 34.9 58.9 36.1 33.5 55.8 34.7 38.9 39.8
Table 9: Performance of different ﬁne-tuning methods on the COCO val2017 dataset and ADE20K
dataset, where APband APmare the average precision of object detection and instance segmenta-
tion, respectively. mIoU and MS mIoU show single-scale and multi-scale inference of semantic
segmentation.
B Experiments on Detection and Segmentation
We also conduct experiments on broader downstream tasks, e.g., object detection, instance segmen-
tation, and semantic segmentation. For object detection and instance segmentation, we perform
experiments on the COCO dataset with Mask R-CNN [ 28], where Swin-T pre-trained on ImageNet-
1K is adopted as the backbone. The speciﬁc hyper-parameter setup and data augmentation refer to
Swin Transformer [ 56] and mmdetection [ 4]. We perform i) full ﬁne-tuning; ii) linear probing, where
the weights at the backbone layers are frozen and only weights at the neck and head layers are updated;
iii) VPT-Deep; iv) SSF. All models are trained with 1x schedule (12 epochs). The results are shown
in Table 9. We can see that SSF outperforms linear probing and VPT-Deep [ 44] on the COCO dataset
in terms of object detection and instance segmentation. For semantic segmentation, we perform
experiments on the ADE20K dataset with UperNet [ 82] and Swin-T pre-trained on ImageNet-1K.
The results in Table 9 show that SSF outperforms linear probing and VPT-Deep [ 44]. However, for
both datasets, SSF still has a large gap compared to the full ﬁne-tuning, which might be due to the
fact that detection and segmentation tasks are fundamentally different from classiﬁcation tasks. Only
ﬁne-tuning a few parameters in the backbone will result in inferior performance. How to introduce
trainable parameters for parameter-efﬁcient ﬁne-tuning in object detection and segmentation will be
the future work.
C Visualizations
C.1 Feature Distribution
We also visualize the feature distribution of different ﬁne-tuning methods via t-SNE on the CIFAR-100
dataset. All ﬁne-tuning methods are based on a ViT-B/16 pre-trained on the ImageNet-21K datasets.
The results are shown in Figure 6. Our SSF achieves better feature clustering results compared to
linear probing and VPT-Deep. Besides, since our method and full ﬁne-tuning have similar accuracy
18

--- PAGE 19 ---
(93.99% vs.93.82%), it is difﬁcult to distinguish them in terms of feature distribution, which also
shows the effectiveness of our method.
LinearprobingAcc.=88.70%Fullfine-tuningAcc.=93.82%SSF(ours)Acc.=93.99%VPT-DeepAcc.=93.17%
Figure 6: t-SNE visualization of different ﬁne-tuning methods, including linear probing, VPT-Deep,
our SSF, and full ﬁne-tuning (best viewed in color).
C.2 Attention Map
We also visualize the attention maps of different ﬁne-tuning methods, as shown in Figure 7. All
models are ﬁne-tuned on ImageNet-1K with ViT-B/16 pre-trained on ImageNet-21K. The speciﬁc
experimental results refer to Table 5 in the main text. We ﬁnd that VPT-Deep has more concentrated
attention on the object in some images ( e.g., the ﬁrst two lines), but lacks suitable attention on some
other images ( e.g., the last two lines). In contrast, SSF tends to obtain attention similar to the full
ﬁne-tuning but also generates the failure prediction such as the second row.
ImageLinearprobingSSF(ours)VPT-DeepFullfine-tuning
Figure 7: Visualization of attention maps. From left to right, each column shows the RGB image,
linear probing, VPT-Deep, our SSF and full ﬁne-tuning.
19

--- PAGE 20 ---
D Limitations and Societal Impacts
Regarding the limitations of this work, we currently focus on sharing backbone parameters among
different tasks while treating each task independently of the rest of the tasks involved. However, some
recent papers ( e.g., [21,20]) show that by correlating multiple tasks together during the ﬁne-tuning,
the performance for every single task can be further improved. However, recent works treat this
relationship among tasks as a black box that inevitably suffers a huge computational cost. Thus, we
believe an efﬁcient method to ﬁnd positive task relationships could be a meaningful direction for
further exploration.
This work has the following societal impact. SSF can effectively save parameters compared to the
full ﬁne-tuning so that the approach can quickly transfer large models pre-trained on large datasets to
downstream tasks, which saves computational resources and carbon emissions. Thanks to the linear
transformation and re-parameterization, we do not need to change the deployed backbone architecture
when the model is transferred to the downstream task. Only a set of weights need to be replaced,
which is also more convenient compared to the methods that introduce additional parameters such as
VPT [ 44]. However, like other ﬁne-tuning methods, SSF is also based on a pre-trained model, which
will probably also cause a violation of the use of ﬁne-tuning methods if this upstream pre-trained
model is trained on some illegal data.
20
