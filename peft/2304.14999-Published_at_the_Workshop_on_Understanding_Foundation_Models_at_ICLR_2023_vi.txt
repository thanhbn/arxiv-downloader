# 2304.14999.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2304.14999.pdf
# Kích thước file: 186141 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023
PHÂN TÍCH THỰC NGHIỆM VỀ ĐIỂM MẠNH VÀ 
ĐIỂM YẾU CỦA CÁC KỸ THUẬT PEFT CHO LLM
George Pu, Anirudh Jain, Jihan Yin & Russell Kaplan
Scale AI
fgeorge.pu,anirudh.jain,jihan.yin,russell.kaplan g@scale.com
TÓM TẮT
Khi các mô hình nền tảng tiếp tục mở rộng quy mô theo cấp số nhân về kích thước, các phương pháp thích ứng hiệu quả trở nên ngày càng quan trọng. Tinh chỉnh hiệu quả tham số (PEFT), một lớp kỹ thuật gần đây chỉ yêu cầu sửa đổi một tỷ lệ nhỏ các tham số mô hình, hiện là phương pháp phổ biến nhất để thích ứng các mô hình ngôn ngữ lớn (LLM). Một số kỹ thuật PEFT gần đây đã được đề xuất với các đánh đổi khác nhau. Chúng tôi cung cấp một điểm chuẩn toàn diện và thống nhất của các kỹ thuật PEFT khác nhau trên một LLM đại diện, mô hình FLAN-T5, và đánh giá hiệu suất mô hình trên các quy mô dữ liệu khác nhau của các tập dữ liệu phân loại và sinh. Dựa trên điều này, chúng tôi cung cấp một khung để chọn các kỹ thuật tinh chỉnh tối ưu dựa trên loại tác vụ và khả năng dữ liệu có sẵn. Trái với niềm tin phổ biến, chúng tôi cũng chứng minh thực nghiệm rằng các kỹ thuật PEFT hội tụ chậm hơn tinh chỉnh đầy đủ trong các tình huống dữ liệu ít, và đưa ra giả định về lượng dữ liệu cần thiết để các phương pháp PEFT vừa hoạt động tốt vừa hội tụ hiệu quả. Cuối cùng, chúng tôi tối ưu hóa thêm các kỹ thuật PEFT này bằng cách chọn lọc các phần của mô hình để huấn luyện, và phát hiện rằng các kỹ thuật này có thể được áp dụng với ít tham số hơn đáng kể trong khi vẫn duy trì và thậm chí cải thiện hiệu suất.

1 GIỚI THIỆU
Khi các mô hình ngôn ngữ lớn được áp dụng rộng rãi, việc huấn luyện và triển khai hiệu quả trở thành yêu cầu quan trọng để cho phép sử dụng phổ biến. Mỗi tác vụ mà một LLM được tinh chỉnh yêu cầu một bộ trọng số hoàn toàn khác. Khi các mô hình mở rộng đến hàng trăm tỷ tham số, việc lưu trữ một bộ trọng số khác nhau cho mỗi mô hình trở nên không hiệu quả và tốn kém, trong khi việc tải lại tất cả trọng số cho các tác vụ khác nhau quá chậm. Các kỹ thuật tinh chỉnh hiệu quả tham số nhằm giải quyết vấn đề này bằng cách sửa đổi một phần rất nhỏ trọng số so với kích thước mô hình đầy đủ trong khi giữ phần còn lại của mô hình đóng băng (Mao et al., 2021).

Tại thời điểm suy luận, nhiều bản thích ứng của cùng một mô hình có thể được phục vụ cùng nhau bằng cách nhanh chóng hoán đổi các mô-đun con nhỏ thay vì tất cả trọng số. Bối cảnh hiện tại của các kỹ thuật PEFT đang phát triển nhanh chóng và một số kỹ thuật PEFT gần đây đã được đề xuất – mỗi kỹ thuật đều tuyên bố có ưu điểm hơn những kỹ thuật khác ở các khía cạnh khác nhau. Tuy nhiên, do các kỹ thuật này đều được đánh giá riêng lẻ trên các mô hình và tập dữ liệu khác nhau, không rõ khi nào nên sử dụng một kỹ thuật thay vì kỹ thuật khác một cách phù hợp. Công trình này tìm cách cung cấp một khung để đánh giá cách sử dụng PEFT hiệu quả bằng cách đánh giá thực nghiệm kỹ thuật nào hoạt động tốt trong loại tác vụ nào và các kỹ thuật này mở rộng như thế nào với dữ liệu. Hơn nữa, thông qua nghiên cứu loại bỏ, công trình của chúng tôi tìm cách hiểu phần nào của mô hình quan trọng nhất để huấn luyện cho một loại tác vụ và kỹ thuật nhất định, dẫn đến thích ứng hiệu quả hơn và giảm số lượng tham số. Các đóng góp chính của chúng tôi là:

1. Thực hiện so sánh và phân tích kỹ lưỡng các phương pháp PEFT tiên tiến hiện tại trên mô hình FLAN-T5 trên các kích thước dữ liệu và loại tác vụ khác nhau (sinh/phân loại), đánh giá nhiều chiều bao gồm độ chính xác, tốc độ hội tụ và các chỉ số liên quan khác.

2. Thực hiện các nghiên cứu loại bỏ để hiểu rõ hơn tầm quan trọng tương đối của việc cập nhật các phần khác nhau của mô hình khi thích ứng LLM, xem xét thứ tự lớp và độ chi tiết mô-đun con, và tối ưu hóa thêm các kỹ thuật PEFT để giảm số lượng tham số được huấn luyện và cuối cùng cải thiện hiệu quả.

1arXiv:2304.14999v1  [cs.CL]  28 Apr 2023

--- TRANG 2 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Hình 1: Tổng thời gian chạy thực nghiệm được chuẩn hóa (thời gian PEFT / thời gian tinh chỉnh đầy đủ) với các kỹ thuật PEFT khác nhau trong cả tình huống tài nguyên thấp (trái) và tài nguyên trung bình (phải). Chúng tôi giới hạn các thực nghiệm tài nguyên cao ở một epoch, do đó loại trừ các kết quả này. Ngoài ra, chúng tôi thực hiện dừng sớm trong tất cả các thực nghiệm nơi tiêu chí dừng được định nghĩa bởi mất mát xác thực không còn giảm.

2 BỐI CẢNH VÀ PHƯƠNG PHÁP LUẬN
Chúng tôi cung cấp bối cảnh về các mô hình ngôn ngữ lớn và tinh chỉnh hiệu quả tham số trong phụ lục A.1 và chi tiết các nguyên tắc hướng dẫn việc lựa chọn phương pháp và mô hình của chúng tôi trong phụ lục A.2. Thực nghiệm của chúng tôi tập trung vào mô hình FLAN-T5-XL, và chúng tôi khám phá hiệu quả của bốn kỹ thuật tinh chỉnh khác nhau - LoRA, (IA)3, prompt tuning, và BitFit - và so sánh hiệu suất của chúng với một mô hình được tinh chỉnh đầy đủ được huấn luyện trên các phân chia train/val/test giống hệt nhau. Chúng tôi thực hiện đánh giá toàn diện về các phương pháp tinh chỉnh hiệu quả tham số (PEFT) và thiết lập một khung để tạo điều kiện lựa chọn kỹ thuật phù hợp nhất trong bất kỳ tình huống nhất định nào.

Để tối ưu hóa và cải thiện thêm các kỹ thuật PEFT hiện có, chúng tôi điều tra phần nào của mô hình quan trọng nhất trong quá trình tinh chỉnh. So với BitFit và prompt tuning, chúng tôi thực hiện nghiên cứu loại bỏ với LoRA và (IA)3 do tính linh hoạt cấu hình của chúng (ví dụ: khối attention, lớp dày đặc, lớp transformer được sửa đổi). Chúng tôi phân tích hiệu ứng của LoRA và (IA)3 trên lớp nào áp dụng kỹ thuật PEFT (ví dụ: sớm vs muộn vs ngẫu nhiên) và tác động của việc loại bỏ các mô-đun con cụ thể, chẳng hạn như vector attention và kích hoạt lớp.

3 THỰC NGHIỆM
3.1 THIẾT LẬP THỰC NGHIỆM
Mô phỏng các tình huống công nghiệp hiện tại, chúng tôi quyết định đánh giá các mô hình trên nhiều tập dữ liệu phân loại và sinh khác nhau và nhiều quy mô dữ liệu khác nhau. Đối với phân loại, chúng tôi chọn AG news (Zhang et al., 2015), có bốn lớp và hơn 100.000 mẫu, và CoLA (Warstadt et al., 2019), có hai lớp và khoảng 10.000 mẫu. Đối với sinh, chúng tôi chọn tập dữ liệu E2E (Novikova et al., 2017), trong lĩnh vực nhà hàng với 50.000 mẫu, và SAMSum (Gliwa et al., 2019) cho tóm tắt đối thoại trừu tượng với khoảng 15.000 mẫu. Để thống nhất các thực nghiệm này trên các tập dữ liệu, chúng tôi chọn ba quy mô dữ liệu: tài nguyên thấp (nhiều nhất 100 điểm dữ liệu), tài nguyên trung bình (nhiều nhất 1.000 điểm dữ liệu), và tài nguyên cao (nhiều nhất 10.000 điểm dữ liệu). Siêu tham số và thông số kỹ thuật triển khai được liệt kê trong phụ lục A.3.

3.2 PHÂN TÍCH CÁC KỸ THUẬT PEFT
Các kỹ thuật PEFT hội tụ chậm hơn tinh chỉnh đầy đủ trong các tình huống tài nguyên thấp/trung bình. Trong Hình 1, chúng tôi bất ngờ quan sát thấy rằng tinh chỉnh đầy đủ liên tục hội tụ nhanh hơn các kỹ thuật PEFT và dẫn đến hiệu suất mô hình cao hơn, ngoại trừ với BitFit trên CoLA trong phân chia tài nguyên trung bình. Trong tài nguyên thấp, chúng tôi quan sát thấy rằng tinh chỉnh đầy đủ có tốc độ tăng 73% trên AG News, 87% trên CoLA, 66% trên E2E NLG, và 60% trên SAMSum. Trong tài nguyên trung bình, chúng tôi quan sát thấy rằng tinh chỉnh đầy đủ có tốc độ tăng 46% trên AG News, 16% trên CoLA, 37% trên E2E NLG, và 64% trên SAMSum. Các giá trị cụ thể được liệt kê trong Bảng 4. Những kết quả này cho thấy rằng đối với các tập dữ liệu tài nguyên thấp hơn, nếu chúng ta ưu tiên tốc độ huấn luyện và ít quan tâm đến ràng buộc phần cứng, tinh chỉnh đầy đủ là lựa chọn tốt hơn. Tuy nhiên,

--- TRANG 3 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Bảng 1: Điểm chuẩn FLAN-T5 trên các phân chia dữ liệu, đo độ chính xác (khớp chuỗi chính xác) cho AG News/CoLA và ROUGE-L (dãy con chung dài nhất) cho E2E NLG/SAMSum, trong đó giá trị cao hơn là tốt hơn.

PEFT/Tập dữ liệu AG News CoLA E2E NLG SAMSum
Tinh chỉnh đầy đủ (thấp) 0.8588 0.699 0.46464 0.3876
(IA)3(thấp) 0.67 0.6973 0.29508 0.32924
LoRA (thấp) 0.8612 0.76436 0.48257 0.41197
BitFit (thấp) 0.8808 0.7203 0.48825 0.41914
Prompt Tune (thấp) 0.60684 0.68199 0.0258 0.00472
Tinh chỉnh đầy đủ (trung bình) 0.9212 0.79119 0.46523 0.40908
(IA)3(trung bình) 0.9208 0.78161 0.48483 0.43183
LoRA (trung bình) 0.9148 0.81418 0.48364 0.43283
BitFit (trung bình) 0.9156 0.78736 0.48539 0.42822
Prompt Tune (trung bình) 0.7312 0.76245 0.44173 0.3792
Tinh chỉnh đầy đủ (cao) 0.934 0.81417 0.48051 0.43356
(IA)3(cao) 0.9252 0.80842 0.4789 0.43998
LoRA (cao) 0.9264 0.83333 0.4756 0.43485
BitFit (cao) 0.9192 0.8295 0.47973 0.43098
Prompt Tune (cao) 0.872 0.80567 0.45942 0.3914

thật thú vị là hầu hết các phương pháp PEFT thực sự hội tụ nhanh hơn khi có nhiều dữ liệu hơn. Chúng tôi giả định rằng điều này là do, ở số lượng dữ liệu thấp hơn, tinh chỉnh đầy đủ nhanh chóng học và overfit với tập dữ liệu nhỏ hơn trong khi các phương pháp PEFT học không ổn định, trong khi ở kích thước dữ liệu cao hơn, các phương pháp PEFT ổn định hơn và học tốt hơn cấu trúc dữ liệu cơ bản.

Chúng tôi điểm chuẩn kết quả hỗn hợp nơi không có phương pháp PEFT tối ưu rõ ràng nào hoạt động tốt nhất, nhưng dựa trên khối lượng dữ liệu, có một khung quyết định rõ ràng giữa tinh chỉnh đầy đủ và PEFT. Từ Bảng 1, không có phương pháp tinh chỉnh tối ưu nào tồn tại dựa trên tác vụ, nhưng có những tình huống cụ thể nơi một phương pháp tốt hơn nhiều. Ví dụ, BitFit và LoRA hoạt động tốt nhất trong các tình huống tài nguyên thấp/trung bình, và tinh chỉnh đầy đủ tăng hiệu suất tương đối khi chúng ta tăng lượng dữ liệu lên mẫu cao hơn. Tồn tại sự khác biệt rõ ràng giữa tốc độ và hiệu suất, nơi PEFT có tốc độ tệ hơn nhưng hiệu suất tốt hơn trong tài nguyên thấp và ngược lại khi dữ liệu mở rộng.

Cho các kết quả hỗn hợp, chúng tôi cung cấp phân tích bổ sung về LoRA, (IA)3 và BitFit trên các chiều thời gian và không gian để cung cấp khung toàn diện cho việc chọn phương pháp tinh chỉnh tối ưu. Ngoài ra, chúng tôi điểm chuẩn các kết quả này với các đường cơ sở tinh chỉnh đầy đủ. Bảng 5 cho cảm nhận về phương pháp hiệu quả nhất về bộ nhớ trong quá trình huấn luyện và cho các ứng dụng hạ nguồn. Bảng 6 cho cảm nhận về phương pháp hiệu quả nhất về chi phí đối với tiêu thụ phần cứng. Chúng tôi tính toán các giá trị bằng cách chia kết quả hiệu suất trong Bảng 1 cho số lượng tham số có thể điều chỉnh hoặc tổng thời gian chạy tính bằng giây.

Để tóm tắt các phát hiện của chúng tôi cho các tình huống hạn chế tài nguyên khác nhau, chúng tôi khuyến nghị tinh chỉnh đầy đủ trên các phân chia dữ liệu thấp/trung bình và các kỹ thuật PEFT trong tài nguyên cao cho các tình huống hạn chế thời gian. Đối với các tình huống hạn chế bộ nhớ, chúng tôi quan sát thấy rằng BitFit và (IA)3 hoạt động tốt nhất trong các tình huống tài nguyên thấp, và (IA)3 hoạt động tốt nhất trong các tình huống tài nguyên trung bình/cao. Đối với các tình huống hạn chế hiệu suất, chúng tôi khuyến nghị (IA)3, LoRA hoặc BitFit cho tài nguyên thấp/trung bình và tinh chỉnh đầy đủ trong tài nguyên cao. Ngoài ra, (IA)3 sử dụng phép nhân ma trận theo phần tử và có chi phí bộ nhớ ít nhất của các phương pháp khác vì chúng ta có thể nhân trọng số mô hình với các vector (IA)3 nên không cần thêm hoặc lưu trữ tham số bổ sung. Các tình huống tài nguyên thấp hơn phổ biến nhất trong các trường hợp sử dụng sản xuất, và nếu số lượng mẫu dữ liệu rất nhỏ, kỹ thuật prompt hoặc học trong ngữ cảnh là các lựa chọn thay thế khả thi.

--- TRANG 4 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Bảng 2: Loại bỏ trên các thành phần con cụ thể để khám phá các yếu tố quan trọng đối với hiệu suất trên các tác vụ hạ nguồn. Độ chính xác cho phân loại và ROUGE-L cho sinh, trong đó giá trị cao hơn là tốt hơn.

PEFT/Tập dữ liệu AG News CoLA E2E NLG SAMSum
(IA)3(tất cả lớp) 0.9208 0.78161 0.48483 0.43183
LoRA (tất cả lớp) 0.9148 0.814176 0.48364 0.43283
(IA)3(chọn lớp đầu) 0.60 0.6877 0.462 0.4154
LoRA (chọn lớp đầu) 0.9104 0.70306 0.4823 0.43462
(IA)3(chọn lớp sau) 0.9212 0.7796 0.48550 0.4128
LoRA (chọn lớp sau) 0.8892 0.82183 0.48553 0.43028
(IA)3(chọn 50% lớp ngẫu nhiên) 0.918 0.74712 0.40 0.416
LoRA (chọn 50% lớp ngẫu nhiên) 0.9096 0.78735 0.4834 0.43109
LoRA (bỏ self-attention) 0.8928 0.7107 0.48055 0.415
LoRA (bỏ enc/dec-attention) 0.9164 0.8007 0.47966 0.43218
LoRA (bỏ query/output attention) 0.9152 0.8084 0.4815 0.43459
(IA)3(bỏ attention) 0.6972 0.7413 0.40 0.2359
(IA)3(bỏ kích hoạt dày đặc) 0.918 0.7643 0.481544 0.4292

3.3 KẾT QUẢ LOẠI BỎ
Suy giảm hiệu suất với thích ứng mô-đun chọn lọc ảnh hưởng đến (IA)3 nhiều hơn, trong khi LoRA mạnh mẽ hơn. Chúng tôi có thể chỉ ra rằng số lượng tham số có thể được giảm 50% trong khi vẫn duy trì hiệu suất, dẫn đến một mô hình hiệu quả và thích ứng hơn. Thú vị là, kết quả loại bỏ của chúng tôi có hiệu suất hạ nguồn cao hơn LoRA hoặc (IA)3 được áp dụng cho tất cả các lớp. Suy giảm hiệu suất khá không đáng kể với tham số giảm, cho thấy rằng chúng ta có thể giảm thêm số lượng tham số xuống ít hơn so với những gì các kỹ thuật ban đầu đề xuất (ví dụ: PEFT ngẫu nhiên hoặc lớp sau).

Attention rất quan trọng trong các tác vụ phân loại và sinh và đặc biệt đúng với (IA)3 so với LoRA. Việc bỏ self-attention làm tổn hại LoRA trong các tác vụ phân loại nhiều hơn so với việc loại bỏ query/output vectors và encoder-decoder attention. Đối với các mô hình hoạt động tương đương với kỹ thuật đầy đủ, chúng tôi có thể giảm tham số ít nhất một nửa và bao gồm số lượng tham số cho mỗi biến thể trong Bảng 3.

Thích ứng các lớp đầu hoạt động kém hơn so với các lớp sau hoặc ngẫu nhiên trong (IA)3. Trong lựa chọn lớp sau hoặc ngẫu nhiên, hiệu suất tốt hơn nhiều trong (IA)3 so với việc sửa đổi các lớp đầu. Loại bỏ hoạt động tốt nhất với số lượng tham số tối thiểu là chọn và PEFT-ing các lớp sau. Những phát hiện này phù hợp với tầm quan trọng của việc sửa đổi các biểu diễn sau, tương tự như các mô hình học chuyển giao.

4 KẾT LUẬN
Trong bài báo này, chúng tôi điểm chuẩn LLM trên nhiều chiều, phát triển hiểu biết về nơi các kỹ thuật PEFT LLM nên được thích ứng, và cung cấp khung về việc chọn kỹ thuật tối ưu. Trên các đường cơ sở của chúng tôi, chúng tôi thấy rằng các phương pháp PEFT thường hoạt động tốt hơn ở mức tài nguyên thấp/trung bình so với tinh chỉnh có giám sát đầy đủ, nhưng chậm hội tụ hơn. Trên các loại bỏ của chúng tôi, chúng tôi khám phá tầm quan trọng của sửa đổi cấp attention và chọn các lớp sau trong hiệu suất hạ nguồn. Chúng tôi có thể cắt tỉa và tối ưu hóa thêm LoRA đến mức nhỏ hơn với độ chi tiết cụ thể mô-đun/lớp, tăng khả năng thích ứng của nó mà không hy sinh hiệu suất.

Không có phương pháp nào hoàn toàn tốt hơn, nhưng chúng tôi nhằm thiết lập hướng dẫn thực nghiệm cho phương pháp tinh chỉnh nào sử dụng cho các tác vụ hạ nguồn dựa trên khuyến nghị trong 3.2 và 3.3. Đánh đổi hiệu quả-hiệu suất phụ thuộc vào từng trường hợp sử dụng, nhưng chúng tôi chứng minh thực nghiệm rằng có những giảm tham số đáng kể có thể được thực hiện trên các kỹ thuật PEFT ban đầu mà không ảnh hưởng đến hiệu suất mô hình. Nghiên cứu này cung cấp phân tích toàn diện về các phương pháp PEFT và cơ hội tối ưu hóa thêm để làm cho việc áp dụng các kỹ thuật này có hệ thống và dễ tiếp cận hơn.

--- TRANG 5 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

TÀI LIỆU THAM KHẢO
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. Revisiting parameter-efficient tuning: Are we really there yet? arXiv preprint arXiv:2202.07962, 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019.

Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained language model adaptation. arXiv preprint arXiv:2106.03164, 2021.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.

Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638, 2022.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577, 2021.

Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

--- TRANG 6 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506, 2020.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.

Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015.

--- TRANG 7 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

A PHỤ LỤC
A.1 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN
Các mô hình ngôn ngữ lớn (LLM) gần đây đã bùng nổ về độ phổ biến, trên nhiều kiến trúc Transformer và kỹ thuật tối ưu hóa khác nhau. Trong bối cảnh LLM, các mô hình có thể là phân biệt hoặc sinh, trong đó mô hình đầu học các ranh giới quyết định và mô hình sau mô hình hóa phân phối của các lớp. Devlin et al. (2018) giới thiệu BERT, một mô hình phân biệt, bao gồm các mạng encoder được tối ưu hóa trên mô hình ngôn ngữ che và dự đoán câu tiếp theo. Các biến thể khác là RoBERTA và ALBERT (Liu et al., 2019; Lan et al., 2019). Các mô hình sinh cũng tồn tại cho các biến thể chỉ decoder như GPT (Brown et al., 2020), GPT-neo (Black et al., 2022), và BLOOM (Scao et al., 2022), và các biến thể encoder-decoder như T5 và T0 (Raffel et al., 2020; Sanh et al., 2021). Tinh chỉnh hướng dẫn cũng thu hút sự quan tâm gần đây, được áp dụng trong FLAN-T5 và FLAN-PaLM (Wei et al., 2021; Chung et al., 2022). Tuy nhiên, các mô hình này bao gồm hàng tỷ tham số, làm cho chúng tốn kém để huấn luyện và phục vụ.

Tinh chỉnh Hiệu quả Tham số (PEFT) Để huấn luyện và phục vụ hiệu quả hơn, các kỹ thuật PEFT đang phát triển phổ biến do khả năng được thích ứng và huấn luyện nhanh chóng. Trong bối cảnh PEFT, có một số lựa chọn thiết kế kiến trúc về nơi các (sub)modules bổ sung nên được gắn vào mô hình ngôn ngữ được huấn luyện trước, điều này khác nhau về việc sửa đổi mô hình cơ bản, tính năng và tham số. Một số công trình gần đây bao gồm các lớp adapter, sửa đổi lớp transformer bằng cách thêm các lớp feedforward mới bao gồm down-projection, phi tuyến và up-projection (Houlsby et al., 2019). Ở cấp độ tính năng, Lester et al. (2021) giới thiệu prompt tuning, thêm các prompt mềm vào đầu văn bản đầu vào. Ở cấp độ tham số, Hu et al. (2021) giới thiệu LoRA, thích ứng các trọng số attention (query, key, value, và output vector). Tương tự, (IA)3 sửa đổi trọng số attention key và value cùng với kích hoạt feedforward với phép nhân theo phần tử (Liu et al., 2022).

Các phương pháp này thường liên quan đến việc đóng băng mô hình cơ bản, nhưng các kỹ thuật khác phát sinh trong đó chỉ một phần của LLM được thay đổi. Ví dụ, Zaken et al. (2021) giả định rằng tinh chỉnh phơi bày kiến thức được tạo ra bởi huấn luyện LM và giới thiệu BitFit, tinh chỉnh các số hạng bias của mô hình. Công trình trước cũng chứng minh tầm quan trọng và sự mạnh mẽ của điều chỉnh dựa trên adapter trong các tình huống dữ liệu ít hơn, trong khi ít nhạy cảm với tốc độ học (He et al., 2021). Ngoài ra, Chen et al. (2022) thực hiện đánh giá PEFT với RoBERTa và tìm thấy hiệu suất tốt hơn của các kỹ thuật này trên các tác vụ dữ liệu ít hơn, tầm quan trọng của tinh chỉnh và cạm bẫy của prefix tuning. Chúng tôi nhằm mở rộng các phát hiện này với các mô hình lớn hơn (từ hàng trăm triệu đến hàng tỷ tham số) và kết hợp (IA)3 trong điểm chuẩn của chúng tôi.

A.2 LỰA CHỌN THIẾT KẾ
Các lựa chọn thiết kế dựa trên nhu cầu công nghiệp và thực hành phổ biến để triển khai các chiến lược tinh chỉnh hiệu quả tham số với suy luận DeepSpeed. Chúng tôi muốn khám phá các phương pháp PEFT trong khi khám phá chiều rộng của các thay đổi kiến trúc và duy trì chiều sâu trong đánh giá để hiểu phương pháp/hiệu ứng tương ứng trên các trường hợp sử dụng hạ nguồn của chúng tôi. Trong giai đoạn khám phá mô hình của chúng tôi, chúng tôi chạy các thực nghiệm ban đầu trên các kích thước mô hình khác nhau cùng với các mô hình tự hồi quy, chẳng hạn như GPT-Neo, BLOOM và các biến thể T5. Chúng tôi thấy hiệu suất tệ hơn đáng kể với các mô hình khác so với FLAN-T5 trên tất cả tập dữ liệu và chỉ số, vì vậy chúng tôi chọn FLAN-T5 làm LLM đại diện.

Do ràng buộc tính toán và chi phí, chúng tôi không thể thực hiện các thực nghiệm của mình trên toàn bộ tập hợp kiến trúc mô hình và phương sai mẫu trên nhiều thử nghiệm. Tuy nhiên, chúng tôi có thể xác thực tác động nhỏ của phương sai có thể quy cho các thực nghiệm ban đầu của chúng tôi và các kiến trúc encoder-decoder và decoder, chẳng hạn như GPT-Neo và các biến thể T5. Theo kinh nghiệm, khi chúng tôi chạy các thực nghiệm này hai lần với các seed ngẫu nhiên khác nhau, xu hướng hội tụ và hiệu suất tương tự. Ngoài ra, chúng tôi đảm bảo cùng các tập val/test với giá trị seed ngẫu nhiên và giới hạn kích thước ở 2500 mẫu trên cả hai phân chia. Để duy trì các cập nhật bước tương tự, chúng tôi giới hạn tài nguyên thấp ở 10 epoch, tài nguyên trung bình ở 5 epoch và tài nguyên cao ở một epoch trong khi cũng kết hợp dừng sớm với mất mát mô hình trên tập xác thực. Cho các phân chia dữ liệu ngẫu nhiên, một số điểm chuẩn tập dữ liệu có thể bao gồm kết quả hỗn hợp do tính ngẫu nhiên, nhưng chúng tôi đảm bảo tất cả các phương pháp được đánh giá trên cùng các phân chia.

--- TRANG 8 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Chúng tôi triển khai LoRA, (IA)3, prompt tuning và BitFit và điểm chuẩn với một mô hình được tinh chỉnh đầy đủ trên cùng các phân chia train/val/test. Tất cả các kỹ thuật PEFT này sửa đổi ít hơn 1% tham số của mô hình, và do ràng buộc bộ nhớ, chúng tôi sử dụng một tập cấu hình cố định cho mỗi phương pháp PEFT và siêu tham số mô hình dựa trên thực hành tốt nhất. Chúng tôi chọn các kỹ thuật PEFT này vì chúng tập trung vào các cấp độ thích ứng kiến trúc khác nhau, chẳng hạn như ở cấp tham số với LoRA và (IA)3, cấp tính năng với prompt tuning, và cấp mô hình với BitFit. Thông số kỹ thuật triển khai ở phần tiếp theo. Về mặt thực tế, (IA)3 dễ triển khai hơn BitFit cho hầu hết các mô hình do các thư viện tối ưu hóa như DeepSpeed fusion các hoạt động inter-transformation lại với nhau (Rasley et al., 2020), làm cho việc tách số hạng bias khó hơn.

A.3 SIÊU THAM SỐ VÀ THÔNG SỐ KỸ THUẬT TRIỂN KHAI
Chúng tôi sử dụng gói HuggingFace Transformer để triển khai mô hình FLAN-T5-XL của chúng tôi, cùng với ban đầu đánh giá kích thước mô hình và kiến trúc decoder với các mô hình GPT-Neo và GPT-J. Để tối ưu hóa, chúng tôi sử dụng bộ lập lịch tốc độ học LambdaLR với bộ tối ưu AdamW, yêu cầu chỉ định tốc độ học ban đầu, warmup và annealing tuyến tính. Ngoài ra, chúng tôi sử dụng kích thước batch 16 với kích thước tích lũy gradient là 4. Chúng tôi chạy tất cả các thực nghiệm của mình với 4 A10 sử dụng song song mô hình.

Đối với siêu tham số PEFT, chúng tôi tuân theo nghệ thuật trước trong việc cấu hình siêu tham số mặc định trên các kiến trúc mô hình và kỹ thuật PEFT. Do ràng buộc thời gian và bộ nhớ, chúng tôi sử dụng một tập siêu tham số cố định cho tất cả các thực nghiệm tài nguyên thấp/trung bình/cao của chúng tôi trên các loại tập dữ liệu. Đối với prompt tuning, các tác giả thấy 20-100 token prompt hoạt động tốt nhất, và chúng tôi chọn 100 trong quá trình thích ứng. Đối với LoRA, chúng tôi tuân theo bài báo gốc với rank chiều 2, và (IA)3 khởi tạo số hạng scaling thành một. BitFit phụ thuộc vào các tính năng đầu ra của lớp cụ thể và được khởi tạo như một tensor của các số không. Trong prompt tuning, chúng tôi sửa đổi embeddings đầu vào để hỗ trợ embeddings liên tục/mềm được thêm vào đầu văn bản đầu vào. Trong LoRA và (IA)3, chúng tôi tuân theo Liu et al. (2022) trong việc biến đổi thích ứng matrix-multiplication LoRA trong các khối attention để cũng hỗ trợ scaling theo phần tử cho (IA)3 để rescale key và values trong cơ chế attention. Chúng tôi phân biệt các submodules, chẳng hạn như các khối "EncDecAttention", "SelfAttention" và "DenseReluDense" của T5. Đối với BitFit, chúng tôi thích ứng tất cả các số hạng bias trong các lớp tuyến tính để yêu cầu gradients.

Cho các ràng buộc tính toán và chi phí của chúng tôi, chúng tôi sử dụng siêu tham số từ công trình trước, tất cả đã thực hiện tìm kiếm siêu tham số cho các giá trị tối ưu và tổng quát. Thông tin thêm có thể được tìm thấy trong các bài báo FLAN-T5, prompt-tuning, (IA)3, LoRA và BitFit trong tài liệu tham khảo. Đối với tinh chỉnh đầy đủ, chúng tôi đặt LR ban đầu thành 3e5, warmup LR thành 3e4, và annealing LR thành 3e5. Đối với LoRA, (IA)3 và BitFit, chúng tôi đặt LR ban đầu thành 3e4, warmup LR thành 3e3, và annealing LR thành 3e4. Đối với prompt tuning, chúng tôi đặt LR ban đầu thành 3e3, warmup LR thành 3e1, và annealing LR thành 3e2.

A.4 SỐ LƯỢNG THAM SỐ CỦA CÁC KỸ THUẬT PEFT VÀ NGHIÊN CỨU LOẠI BỎ
Đối với số lượng tham số của các đường cơ sở, chúng tôi sử dụng mô hình FLAN-T5-XL, có 2.849.757.184 tham số mô hình. Cho các thông số kỹ thuật triển khai của chúng tôi, phương pháp (IA)3 có 933.888 tham số. Phương pháp LoRA có 3.538.944 tham số. Prompt tuning có 204.800 tham số và mở rộng theo n * h, trong đó h là kích thước trạng thái ẩn và n là độ dài của prefix. Cuối cùng, BitFit sửa đổi 1.179.648 tham số. Kết quả là, tất cả các phương pháp này sử dụng một phần nhỏ của tổng trọng số tham số. Ngoài ra, chúng tôi bao gồm thông tin về số lượng tham số cụ thể trên các thực nghiệm loại bỏ của chúng tôi trong Bảng 3, thay đổi dựa trên thành phần con cụ thể được sửa đổi (ví dụ: lớp).

--- TRANG 9 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Bảng 3: Số lượng tham số trên các biến thể loại bỏ, bao gồm (IA)3 và LoRA được áp dụng trên các lớp/modules đầy đủ.

Kỹ thuật Loại bỏ Số lượng Tham số
Tổng Tham số (không PEFT) 2849757184
(IA)3(lớp/modules đầy đủ) 933888
LoRA (lớp/modules đầy đủ) 3538944
(IA)3(chọn lớp đầu) 466944
LoRA (chọn lớp đầu) 1769472
(IA)3(chọn lớp sau) 466944
LoRA (chọn lớp sau) 1769472
(IA)3(chọn lớp ngẫu nhiên) 466944
LoRA (chọn lớp ngẫu nhiên) 1769472
LoRA (bỏ self-attention) 1179648
LoRA (bỏ enc/dec-attention) 2359296
LoRA (bỏ query/output attention) 1769472
(IA)3(bỏ attention) 344064
(IA)3(bỏ kích hoạt dày đặc) 589824

A.5 KẾT QUẢ BỔ SUNG
Trong điểm chuẩn của chúng tôi, chúng tôi phân tích các kết quả khác nhau được đề cập trong Phần 3.2. Bảng 4 đề cập đến các số chính xác được sử dụng trong tính toán thời gian hội tụ chính xác từ Hình 1. Ngoài ra, chúng tôi cung cấp các bảng được sử dụng trong việc xác định khung tinh chỉnh phụ thuộc vào số lượng tham số được sử dụng và tổng thời gian hội tụ dựa trên dừng sớm với mất mát xác thực. Cho rằng prompt tuning hoạt động tệ hơn đáng kể so với các kỹ thuật PEFT khác, chúng tôi loại trừ các kết quả này khỏi so sánh.

Bảng 4: Bảng hội tụ FLAN-T5 trên epochs và tổng thời gian chạy (tính bằng giây), trong đó giá trị thấp hơn là tốt hơn. Thấp đề cập đến tài nguyên thấp, trong khi trung bình đề cập đến phân chia dữ liệu tài nguyên trung bình. Chúng tôi giới hạn các thực nghiệm tài nguyên cao ở một epoch, do đó loại trừ các kết quả này.

PEFT/Tập dữ liệu AG News CoLA E2E NLG SAMSum
Tinh chỉnh đầy đủ (thấp) Ep. 4; 1249 Ep. 2; 89 Ep.3; 2845 Ep. 3; 1951
(IA)3(thấp) Ep. 9; 6130 Ep. 9; 725 Ep. 9; 6307 Ep. 9; 4096
LoRA (thấp) Ep. 5; 3146 Ep. 6; 687 Ep. 9; 9897 Ep. 9; 6758
BitFit (thấp) Ep. 8; 2738 Ep. 7; 349 Ep. 9; 9426 Ep. 9; 5385
Prompt Tune (thấp) Ep. 7; 6797 Ep. 9; 1124 Ep. 4; 8609 Ep. 3; 3340
Tinh chỉnh đầy đủ (trung bình) Ep. 2; 1782 Ep. 2; 608 Ep. 3; 3877 Ep. 1; 2089
(IA)3(trung bình) Ep. 4; 2565 Ep. 3; 537 Ep. 4; 5731 Ep. 3; 4234
LoRA (trung bình) Ep. 3; 3303 Ep. 3; 1231 Ep. 3; 7185 Ep. 4; 5627
BitFit (trung bình) Ep. 3; 2027 Ep. 3; 507 Ep. 4; 5712 Ep. 4; 4101
Prompt Tune (trung bình) Ep. 2; 5338 Ep. 2; 652 Ep. 3; 6152 Ep. 2; 2681

--- TRANG 10 ---
Được xuất bản tại Workshop on Understanding Foundation Models tại ICLR 2023

Bảng 5: Đo mối quan hệ của độ chính xác và tham số được điều chỉnh với mỗi kỹ thuật PEFT so với các đường cơ sở tinh chỉnh đầy đủ của chúng tôi. Hiệu suất (độ chính xác hoặc ROUGE-L) được chia cho số lượng tham số có thể điều chỉnh của mỗi thực nghiệm, được chuẩn hóa thành mỗi một trăm nghìn tham số được sử dụng.

PEFT/Tập dữ liệu AG News CoLA E2E NLG SAMSum
Tinh chỉnh đầy đủ (thấp) 3.01E-05 2.45E-05 1.63E-05 1.36E-05
(IA)3(thấp) 0.0717 0.0746 0.03159 0.03525
LoRA (thấp) 0.0243 0.0216 0.0136 0.01164
BitFit (thấp) 0.0746 0.06106 0.04139 0.03553
Tinh chỉnh đầy đủ (trung bình) 3.23E-05 2.77E-05 1.63E-05 1.44E-05
(IA)3(trung bình) 0.0985 0.0836 0.05191 0.04624
LoRA (trung bình) 0.0258 0.023 0.01366 0.01223
BitFit (trung bình) 0.07761 0.06674 0.04118 0.0363
Tinh chỉnh đầy đủ (cao) 3.28E-05 2.86E-05 1.69E-05 1.52E-05
(IA)3(cao) 0.0991 0.0865 0.05128 0.04711
LoRA (cao) 0.0262 0.0235 0.01343 0.01228
BitFit (cao) 0.0779 0.0703 0.04066 0.03643

Bảng 6: Đo mối quan hệ của độ chính xác và tổng thời gian chạy với mỗi kỹ thuật PEFT so với các đường cơ sở tinh chỉnh đầy đủ của chúng tôi. Hiệu suất (độ chính xác hoặc ROUGE-L) được chia cho tổng thời gian chạy của mỗi thực nghiệm (tính bằng phút).

PEFT/Tập dữ liệu AG News CoLA E2E NLG SAMSum
Tinh chỉnh đầy đủ (thấp) 0.04125 0.47124 0.0098 0.0119
(IA)3(thấp) 0.00597 0.05771 0.0028 0.00482
LoRA (thấp) 0.01592 0.0583 0.0029 0.00365
BitFit (thấp) 0.0193 0.12383 0.0031 0.00467
Tinh chỉnh đầy đủ (trung bình) 0.03205 0.07807 0.00719 0.01174
(IA)3(trung bình) 0.02139 0.08733 0.00507 0.00612
LoRA (trung bình) 0.01662 0.03968 0.00403 0.00461
BitFit (trung bình) 0.0271 0.09317 0.00509 0.00626
