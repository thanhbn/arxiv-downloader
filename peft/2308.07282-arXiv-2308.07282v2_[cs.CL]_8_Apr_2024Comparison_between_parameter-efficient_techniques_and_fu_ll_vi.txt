# 2308.07282.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.07282.pdf
# Kích thước tệp: 356424 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2308.07282v2  [cs.CL]  8 Thg 4 2024So sánh giữa các kỹ thuật hiệu quả tham số và tinh chỉnh đầy đủ: Một nghiên cứu tình huống về phân loại bài báo tin tức đa ngôn ngữ

Olesya Razuvayevskaya1/YinYang*, Ben Wu1/YinYang, João A. Leite1/YinYang, Freddy Heppell1/YinYang, Ivan Srba2,
Carolina Scarton1, Kalina Bontcheva1, Xingyi Song1
1Khoa Khoa học Máy tính, Đại học Sheffield, Sheffield, Vương quốc Anh
2Viện Công nghệ Thông minh Kempelen, Bratislava, Slovakia
/YinYangNhững tác giả này đóng góp như nhau cho công trình này.
* o.razuvayevskaya@sheffield.ac.uk

Tóm tắt
Adapter và Low-Rank Adaptation (LoRA) là những kỹ thuật tinh chỉnh hiệu quả tham số được thiết kế để làm cho việc huấn luyện các mô hình ngôn ngữ hiệu quả hơn. Kết quả trước đây đã chứng minh rằng những phương pháp này thậm chí có thể cải thiện hiệu suất trên một số tác vụ phân loại. Bài báo này bổ sung cho nghiên cứu hiện có bằng cách điều tra cách những kỹ thuật này ảnh hưởng đến hiệu suất phân loại và chi phí tính toán so với tinh chỉnh đầy đủ. Chúng tôi tập trung cụ thể vào các tác vụ phân loại văn bản đa ngôn ngữ (phát hiện thể loại, framing và kỹ thuật thuyết phục; với độ dài đầu vào khác nhau, số lượng lớp dự đoán và độ khó phân loại), một số trong đó có dữ liệu huấn luyện hạn chế. Ngoài ra, chúng tôi tiến hành phân tích sâu về hiệu quả của chúng qua các kịch bản huấn luyện khác nhau (huấn luyện trên dữ liệu đa ngôn ngữ gốc; trên bản dịch sang tiếng Anh; và trên tập con dữ liệu chỉ tiếng Anh) và các ngôn ngữ khác nhau. Những phát hiện của chúng tôi cung cấp những hiểu biết có giá trị về khả năng áp dụng của các kỹ thuật tinh chỉnh hiệu quả tham số, đặc biệt cho phân loại đa nhãn và các tác vụ đa ngôn ngữ không song song nhằm phân tích các văn bản đầu vào có độ dài khác nhau.

9 tháng 4, 2024 1/45

--- TRANG 2 ---
Giới thiệu
Sự phát triển của các mô hình ngôn ngữ đã dẫn đến sự gia tăng đáng kể trong số lượng tham số có thể huấn luyện cần thiết để tinh chỉnh các mô hình như vậy, với các mô hình tiên tiến bao gồm hàng triệu hoặc thậm chí hàng tỷ tham số [1,2]. Điều này đặt ra một ràng buộc nghiêm trọng cho quá trình tinh chỉnh các mô hình như vậy, thường dựa vào tài nguyên tính toán đáng kể. Do đó, nhiều nỗ lực nghiên cứu gần đây tập trung vào việc phát triển các kỹ thuật huấn luyện hiệu quả hơn [3–5]. Các phương pháp có thể giảm chi phí tính toán làm cho các mô hình ngôn ngữ dễ tiếp cận hơn đối với các nhà nghiên cứu và thực hành viên có tài nguyên tính toán hạn chế, và giảm lượng khí thải carbon trong quá trình huấn luyện của chúng.

Trong nghiên cứu này, chúng tôi điều tra hiệu quả của các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) trong các kịch bản phân loại văn bản đa ngôn ngữ, đơn ngôn ngữ và xuyên ngôn ngữ. Chúng tôi đã bao gồm các kỹ thuật PEFT đã được đánh giá trên khối lượng dữ liệu lớn (>20B) trong nghiên cứu trước [6] – cụ thể là LoRA, adapter, BitFit, prefix tuning và prompt tuning. Vì phần lớn các kỹ thuật này trước đây được đánh giá trên các mô hình dựa trên transformer (BERT và RoBERTa) cho các tác vụ phân loại văn bản, chúng tôi chọn tập con của những kỹ thuật có thể áp dụng cho các mô hình như vậy – cụ thể là LoRA, adapter và BitFit. Chúng tôi loại trừ prefix tuning [7] và prompt tuning [8] vì những kỹ thuật này được áp dụng cho các mô hình Large Language Models (LLMs) sinh tạo cho các tác vụ sinh văn bản từ văn bản và không thể so sánh trực tiếp với ba phương pháp nêu trên.

Chi tiết hơn, tinh chỉnh dựa trên adapter đại diện cho một họ các kỹ thuật hiệu quả hoạt động bằng cách đóng băng một mô hình ngôn ngữ được huấn luyện trước và thêm một số lượng nhỏ tham số có thể huấn luyện vào các lớp của mô hình ngôn ngữ [9–11]. Điều này giảm đáng kể thời gian huấn luyện với chi phí là một hình phạt hiệu suất nhỏ hoặc không có.

Một phương pháp khác để giảm số lượng tham số có thể huấn luyện dựa trên việc thực hiện Low-Rank Adaptation (LoRA) [12]. Ý tưởng chính đằng sau phương pháp LoRA là đóng băng các trọng số của các mô hình ngôn ngữ được huấn luyện trước và chèn các ma trận phân tách hạng thấp vào các lớp transformer. BIas-Term FIne-Tuning (BitFit) là một kỹ thuật PEFT chỉ sử dụng bias term và lớp phân loại tuyến tính đặc thù tác vụ trong quá trình huấn luyện, bỏ qua hầu hết các tham số trong các lớp encoder-decoder [3]. Điều này dẫn đến việc giảm đáng kể chi phí tính toán vì các bias terms chỉ chiếm một phần nhỏ (lên đến 0,1%) của tất cả các tham số của các mô hình.

9 tháng 4, 2024 2/45

--- TRANG 3 ---
Các nghiên cứu trước báo cáo rằng, ngoài việc giảm chi phí tính toán, các phương pháp dựa trên adapter có thể vượt trội hơn tinh chỉnh đầy đủ (FFT) trong các thiết lập xuyên ngôn ngữ zero-shot [11,13,14]. Tuy nhiên, các tác vụ được đề cập trong những nghiên cứu này tập trung vào các văn bản đa ngôn ngữ song song [13], có nghĩa là suy luận không được thực hiện theo cách zero-shot thực tế [14], trên các đầu vào ngắn [11] hoặc trong các lĩnh vực rất cụ thể [14]. Cho rằng các phương pháp adapter được biết đến là có khả năng hạn chế trong việc xử lý các đầu vào văn bản dài do dành một phần độ dài chuỗi cho việc thích ứng [12], có động lực để đánh giá phương pháp này trong các thiết lập xuyên ngôn ngữ và đa ngôn ngữ không song song và trên các độ dài đầu vào khác nhau. Mặt khác, LoRA không giảm độ dài chuỗi đầu vào, và các so sánh trước đây giữa FFT và LoRA cho thấy rằng, ngoài việc hiệu quả về tham số, LoRA có thể, đối với một số mô hình nhất định, vượt trội hơn FFT [12]. Tuy nhiên, hiện tại có một khoảng trống nghiên cứu về việc đánh giá LoRA trong các kịch bản huấn luyện đa ngôn ngữ tương tự, điều này tạo ra động lực để bao gồm PEFT này trong nghiên cứu của chúng tôi. Không có bằng chứng trước đây về khả năng xuyên ngôn ngữ của BitFit, tuy nhiên, kỹ thuật PEFT này được chứng minh có hiệu suất tương đương với adapters và cũng có thể so sánh với FFT trên benchmark GLUE. Điều này đã thúc đẩy việc bao gồm phương pháp BitFit trong nghiên cứu này.

Do đó, nghiên cứu này được thúc đẩy bởi việc thiếu đánh giá nhất quán của các kỹ thuật PEFT trong các tác vụ phân loại zero-shot đa ngôn ngữ và xuyên ngôn ngữ được thực hiện trên các văn bản dài không song song. Khoảng trống trong nghiên cứu hiện tại này được giải quyết thông qua một điều tra so sánh có hệ thống về cách các kỹ thuật adapter, LoRA và FitBit hoạt động trên các tác vụ phân loại đa nhãn đa ngôn ngữ, cả về hiệu suất phân loại và chi phí tính toán. Trước tiên chúng tôi thực hiện một nghiên cứu ablation trên một tác vụ phân loại để xác định các kỹ thuật PEFT thể hiện hiệu suất tốt nhất trong các kịch bản đa ngôn ngữ và xuyên ngôn ngữ. Những phương pháp PEFT hoạt động tốt nhất đó sau đó được điều tra thêm trong ba kịch bản huấn luyện (đa ngôn ngữ và xuyên ngôn ngữ) trên ba tác vụ phân loại đa dạng.

Cụ thể, chúng tôi nghiên cứu hành vi của những PEFT này trên ba tác vụ phân loại bài báo tin tức đa nhãn đa ngôn ngữ được giới thiệu như các tác vụ phụ riêng biệt của SemEval 2023 Shared Task 3 gần đây [15]: phát hiện thể loại tin tức, framing và kỹ thuật thuyết phục.

9 tháng 4, 2024 3/45

--- TRANG 4 ---
Cả ba tác vụ đều dựa trên dữ liệu đa ngôn ngữ và chứa các ngôn ngữ 'chưa thấy', tức là các ngôn ngữ không có sẵn trong tập huấn luyện, nhưng có mặt trong tập kiểm tra. Động lực thêm cho nghiên cứu này đến từ sự thành công của ba phương pháp hoạt động tốt nhất ban đầu của chúng tôi trong mỗi tác vụ phụ khác nhau này [16,17]. Phương pháp tốt nhất của chúng tôi cho tác vụ phụ 1 dựa trên một ensemble của các mô hình FFT và dựa trên adapter, cũng như lựa chọn checkpoint đặc thù ngôn ngữ. Phương pháp tác vụ phụ 2 tốt nhất của chúng tôi dựa trên các ensemble đơn ngôn ngữ và đa ngôn ngữ, một trong số đó kết hợp các phương pháp FFT và adapter với huấn luyện trước thích ứng tác vụ [18]. Cuối cùng, các mô hình của chúng tôi cho tác vụ phụ 3 bao gồm lựa chọn ngưỡng phân loại đặc thù ngôn ngữ và việc kết hợp dữ liệu không gán nhãn vào corpus huấn luyện. Nhìn chung, chúng tôi thấy rằng adapters cải thiện hiệu suất trong một số kịch bản đơn ngôn ngữ nhất định trong tác vụ phụ 1 và cho các kịch bản đa ngôn ngữ trong tác vụ phụ 2.

Những đóng góp chính của bài báo này như sau:

• Cung cấp một đánh giá về PEFT trên các tác vụ phân loại văn bản đa dạng so với các mô hình được tinh chỉnh đầy đủ.

• So sánh chi phí tính toán cho việc huấn luyện các mô hình FFT và PEFT trên ba tác vụ phụ vì một đánh giá như vậy chưa được thực hiện trước đây trên những tập dữ liệu và tác vụ này. Chúng tôi chỉ ra rằng PEFT giảm đáng kể số lượng tham số có thể huấn luyện (giữa 140 và 280 lần ít tham số hơn) và đạt được thời gian huấn luyện ngắn hơn (giữa 32% và 44%). Không giống như các phân tích trước đây giữa bottleneck adaptors, LoRA và FFT, so sánh của chúng tôi là mới trong việc cung cấp thống kê chi tiết hơn, chẳng hạn như việc sử dụng VRAM đỉnh và thời lượng huấn luyện tương đối. Thống kê sau đặc biệt quan trọng vì quá trình huấn luyện thường tốn thời gian hơn so với suy luận.

• Thực hiện so sánh sâu về hiệu suất của PEFT trong các kịch bản huấn luyện khác nhau, điều tra cả huấn luyện đa ngôn ngữ chung và hai loại kịch bản huấn luyện ngôn ngữ nguồn đơn. Chúng tôi đánh giá cách mỗi phương pháp hoạt động trên các ngôn ngữ đã thấy và tổng quát hóa cho những ngôn ngữ chưa thấy.

• Cải thiện kết quả SemEval 2023 cao nhất ban đầu. Đối với tác vụ phụ 3,

9 tháng 4, 2024 4/45

--- TRANG 5 ---
chúng tôi đạt được hiệu suất tốt hơn trên tám trong số chín ngôn ngữ so với kết quả hàng đầu trong bảng xếp hạng chính thức. Đối với tác vụ phụ 1 và 2, kết quả báo cáo ở đây chủ yếu có thể so sánh với các bài nộp SemEval 2023 ban đầu của chúng tôi mặc dù thực tế là trong nghiên cứu này chúng tôi sử dụng các mô hình ít phức tạp hơn đáng kể (các giải pháp ban đầu sử dụng nhiều bước phù hợp với tác vụ phụ và các ensemble phức tạp).

Công trình liên quan

Các kỹ thuật tinh chỉnh hiệu quả tham số

Như đã đề cập ở trên, PEFT có hiệu quả tính toán do hạn chế số lượng tham số có thể huấn luyện. Tất cả các kỹ thuật như vậy đóng băng mô hình được huấn luyện trước, nhưng khác nhau về vị trí của các tham số có thể huấn luyện được chèn vào. Chúng tôi tập trung cụ thể vào Bottleneck adapters và Low-Rank Adaptation (LoRA).

Bottleneck adapters [10] có cấu trúc tương tự như autoencoders. Trạng thái ẩn của transformer h trước tiên được chiếu xuống một số chiều nhỏ hơn dbottleneck với ma trận Wdown, đi qua hàm phi tuyến f, và sau đó được chiếu lên chiều ban đầu với ma trận Wup, và một kết nối dư r. Điều này được định nghĩa chính thức trong Eq 1.

h←Wup·f(Wdown·h)+r (1)

Vị trí của lớp adapter phụ thuộc vào loại adapter. Houlsby adapters [10] đặt các lớp adapter sau khối multi-head attention và feed-forward, trong khi Pfeiffer adapters [19] chỉ đặt lớp adapter sau khối feed-forward. Mặc dù việc thêm lớp bổ sung giảm số lượng tham số có thể huấn luyện và do đó tăng tốc độ tinh chỉnh, nó cũng tăng số lượng tham số tổng thể vĩnh viễn, làm chậm suy luận.

LoRA[12] thêm các ma trận phân tách hạng thấp vào query (Q), key(K), value (V) và các ma trận W0 được huấn luyện trước của các lớp con self-attention của transformer. Cho một lớp được biểu diễn như phép nhân ma trận h←W0x (trong đó W0∈Rd×k), trong quá trình tinh chỉnh, giá trị của W0 được sửa đổi bởi một số ∆W. LoRA

9 tháng 4, 2024 5/45

--- TRANG 6 ---
biểu diễn delta này như phân tách hạng thấp ∆W=BA (trong đó B∈Rd×r,A∈Rr×k) của hạng r≪min(d,k). Ở đây, W0 được đóng băng, trong khi B và A được khởi tạo ngẫu nhiên và cập nhật trong quá trình tinh chỉnh. Phân tách được chia tỷ lệ bởi siêu tham số α và hạng r, do đó đưa ra biểu thức mới trong Phương trình 2.

h←W0x+α/r BAx (2)

Khi giai đoạn tinh chỉnh hoàn thành, các ma trận bổ sung có thể được loại bỏ bằng cách đơn giản hóa W0, A và B thành một ma trận đơn W'0, do đó có cùng số lượng tham số như mô hình được huấn luyện trước ban đầu. Điều này giải quyết vấn đề tăng thời gian suy luận. Khi thực hiện tìm kiếm siêu tham số, siêu tham số α có thể được cố định vì nó tỷ lệ thuận với tốc độ học [20].

Các mô hình Transformer bao gồm sáu ma trận trọng số, W0, WK, WV, WQ, và hai ma trận trong lớp multilayer perceptron (MLP). Về nguyên tắc, có thể thích ứng bất kỳ số lượng ma trận trọng số nào, tuy nhiên, các tác giả tuyên bố rằng chỉ thích ứng các ma trận self-attention (W0, WK, WV, WQ) tạo ra kết quả tương đương với việc thích ứng tất cả các lớp [12].

Đánh giá sử dụng thích ứng LoRA trong các phần khác của mô hình (tất cả lớp attention, tất cả lớp feed-forward, tất cả lớp, và lớp đầu ra attention và feed-forward) tiết lộ rằng việc chèn thích ứng LoRA trong tất cả các lớp dẫn đến hiệu suất cao nhất, và trong cấu hình này siêu tham số r không có tác động [20].

BitFit[3] chỉ sử dụng một tỷ lệ phần trăm nhỏ các tham số mô hình được gọi là bias terms. Điều này cho phép giảm đáng kể số lượng tham số có thể huấn luyện.

Mỗi đầu self-attention tại lớp l bao gồm các lớp tuyến tính key (K), query(Q) và value (V). Không giống như trong phương pháp tinh chỉnh FFT, chỉ các bias terms bl được xem xét trong quá trình tinh chỉnh:

Qm,l(x) = Wm,l_q x + bm,l_q
Km,l(x) = Wm,l_k x + bm,l_k  
Vm,l(x) = Wm,l_v x + bm,l_v (3)

Nhiều đầu attention sau đó được kết hợp sử dụng cơ chế attention và được đưa vào MLP với layer-norm (LN). Các bias từ các lớp query, key và value,

9 tháng 4, 2024 6/45

--- TRANG 7 ---
attention và normalization là những tham số duy nhất được tinh chỉnh của mạng. Các ma trận W(l),·· và các tham số MLP khác được giữ đóng băng. Điều này giảm số lượng tham số có thể huấn luyện xuống 0,08%–0,09%.

Các tác giả bổ sung chỉ ra cách số lượng tham số có thể huấn luyện có thể được giảm thêm bằng cách chỉ tập trung vào các bias terms từ lớp query và lớp MLP thứ hai.

Công trình liên quan về so sánh adapters, LoRA và BitFit với FFT

Mặc dù giảm đáng kể số lượng tham số có thể huấn luyện, bottleneck adapters trước đây được phát hiện có tác động tiêu cực tối thiểu đến hiệu suất của các mô hình được tinh chỉnh cho các tác vụ phân loại câu đơn giản. Cụ thể, đánh giá trên tập dữ liệu benchmark GLUE [21] (một bộ sưu tập các tác vụ phân loại câu và cặp câu) cho thấy hiệu suất bottleneck adapter nằm trong phạm vi 0,8% hiệu suất của FFT, trong khi chỉ huấn luyện 3,6% tham số [10]. Thêm vào đó, trong lĩnh vực dịch máy, Bapna và Firat [9] thấy rằng adapters tạo ra kết quả tương đương hoặc thậm chí tốt hơn so với FFT.

Adapters nhận được sự chú ý đặc biệt trong bối cảnh của các tác vụ đa ngôn ngữ, với nhiều nghiên cứu khác nhau báo cáo lợi thế nhất quán của các mô hình adapter so với FFT. Cụ thể, Pfeiffer et al. [19] đề xuất một kiến trúc adapter mô-đun dựa trên adapter, kết hợp các adapter đặc thù tác vụ trong ngôn ngữ nguồn với các 'language' adapter được huấn luyện trên dữ liệu không gán nhãn trong ngôn ngữ đích sử dụng mục tiêu masked language modelling (MLM). Các tác giả báo cáo rằng framework này có thể vượt trội hơn các mô hình ngôn ngữ được tinh chỉnh đầy đủ truyền thống trong suy luận chuyển giao xuyên ngôn ngữ cho phần lớn các cặp ngôn ngữ. Khác với Pfeiffer et al. [19], He et al. [11] điều tra khả năng xuyên ngôn ngữ zero-shot của adapters mà không huấn luyện thêm chúng trên dữ liệu ngôn ngữ đích không gán nhãn. Các tác giả thấy rằng adapters vẫn vượt trội hơn FFT trên nhận dạng thực thể có tên (NER), gắn thẻ từ loại (POS) và các tác vụ Natural Language Inference (XNLI) xuyên ngôn ngữ. Họ báo cáo rằng phương pháp trước đặc biệt có lợi trong các tác vụ ít tài nguyên và xuyên ngôn ngữ, vì nó giảm thiểu các hiệu ứng quên bằng cách tối thiểu hóa sự khác biệt

9 tháng 4, 2024 7/45

--- TRANG 8 ---
giữa các biểu diễn của mô hình được tinh chỉnh và mô hình được huấn luyện trước.

Không giống như các nghiên cứu trước chủ yếu tập trung vào phân tích các văn bản ngắn, Chalkidis et al. [13] điều tra hiệu suất của các mô hình adapter trên các tài liệu pháp lý dài. Nhất quán với các công trình trước đây, các tác giả thấy rằng bottleneck adapters vượt trội hơn FFT, và cung cấp khả năng xuyên ngôn ngữ zero-shot tốt hơn. Những phát hiện của họ dựa trên tập dữ liệu MultiEURLEX, bao gồm 65.000 văn bản luật EU bằng 23 ngôn ngữ, được phân loại ở nhiều mức độ chi tiết (giữa 21 và 567 danh mục). Ở một số khía cạnh, tập dữ liệu này có thể so sánh với tập dữ liệu cho tác vụ phụ 3 mà chúng tôi khám phá trong công trình này, vì cả hai tập dữ liệu đều đa nhãn và đa ngôn ngữ, và có số lượng nhãn tương đương ở mức thấp nhất của MultiEURLEX. Tuy nhiên phong cách của luật EU tự nhiên cứng nhắc hơn nhiều và rất khác với các bài báo tin tức. Thêm vào đó, do phương pháp thu thập dữ liệu cụ thể được sử dụng cho MultiEURLEX, tập dữ liệu này không có khả năng chứa văn bản không liên quan. Cuối cùng, tập dữ liệu MultiEURLEX chứa dữ liệu đa ngôn ngữ song song, có nghĩa là mô hình thực hiện dự đoán zero-shot xuyên ngôn ngữ trên ngôn ngữ đích đã 'thấy' văn bản này tại thời điểm huấn luyện bằng ngôn ngữ khác.

Hạn chế sau của tập dữ liệu MultiEURLEX trở thành trọng tâm của nghiên cứu bởi Xenouleas et al. [14] người đặt câu hỏi liệu những phát hiện của Chalkidis et al. có tổng quát hóa cho các tập dữ liệu không song song hay không. Khi tập dữ liệu được sửa đổi để chỉ bao gồm các tài liệu không song song, các tác giả thấy rằng các phương pháp dựa trên dịch (translate-test và translate-train) vượt trội hơn các mô hình đa ngôn ngữ. Tuy nhiên, nhất quán với nghiên cứu trước đây, các tác giả quan sát thấy rằng adapters vẫn vượt trội hơn FFT trong mỗi thiết lập, xuyên ngôn ngữ và dựa trên dịch. Chúng tôi lưu ý rằng điều này có thể phụ thuộc vào lĩnh vực và liệu các thuộc tính liên quan có bị ảnh hưởng đáng kể bởi dịch hay không: các tài liệu pháp lý có nhiều khả năng được đại diện đúng trong ngôn ngữ đích hơn, ví dụ, các thuộc tính ngôn ngữ học cụ thể của ngôn ngữ báo hiệu một số kỹ thuật thuyết phục nhất định. Thêm vào đó, các thí nghiệm đa ngôn ngữ chung được tiến hành bởi Xenouleas et al. [14] không loại bỏ bất kỳ ngôn ngữ nào để thực hiện suy luận xuyên ngôn ngữ zero-shot, điều này làm cho phương pháp này không thể so sánh với các phương pháp đơn ngôn ngữ do sự khác biệt về kích thước của tập huấn luyện.

Tính đến các hạn chế của nghiên cứu trước đây về độ dài đầu vào, tính đặc thù cao của các lĩnh vực, và tính song song của dữ liệu đa ngôn ngữ, chúng tôi mục tiêu

9 tháng 4, 2024 8/45

--- TRANG 9 ---
thực hiện so sánh rộng hơn của các mô hình adapter với FFT trên nhiều tác vụ khác nhau và trên các văn bản đầu vào không song song không bị giới hạn trong một lĩnh vực cụ thể và có độ dài khác nhau.

Một hạn chế quan trọng khác của công trình trước đây là tất cả các nghiên cứu được đề cập ở trên đánh giá khả năng chuyển giao xuyên ngôn ngữ theo cách một-đến-nhiều, trong khi thiết lập huấn luyện đa ngôn ngữ chung không thực hiện suy luận xuyên ngôn ngữ zero-shot, làm cho nó không thể so sánh với các kịch bản xuyên ngôn ngữ zero-shot đơn ngôn ngữ. Để lấp đầy khoảng trống này và mang lại hiểu biết về cách dữ liệu huấn luyện đa ngôn ngữ chung có thể ảnh hưởng đến khả năng xuyên ngôn ngữ của các phương pháp, chúng tôi mục tiêu giới thiệu suy luận chung nhiều-đến-nhiều vào các kịch bản so sánh của chúng tôi trong khi giữ một số ngôn ngữ nhất định là 'chưa thấy' cho tất cả các kịch bản huấn luyện.

Theo hiểu biết của chúng tôi, không có so sánh nào về LoRA và BitFit với phương pháp FFT trong kịch bản đa ngôn ngữ tương tự tồn tại. Tuy nhiên, cho rằng có bằng chứng trước đây rằng LoRA và BitFit có thể, đối với một số tác vụ nhất định, vượt trội hơn FFT [3,12], có động lực rõ ràng để thực hiện đánh giá tiên phong của những kỹ thuật PEFT này trong các kịch bản đa ngôn ngữ. Kỹ thuật LoRA có thể đặc biệt hứa hẹn cho mục tiêu của chúng tôi do khả năng hoạt động tốt hơn trên các văn bản dài hơn so với các phương pháp adapter do thực tế là LoRA không giảm độ dài chuỗi đầu vào [12].

Mô tả tập dữ liệu và tác vụ

Để chọn các tác vụ và tập dữ liệu phù hợp cho mục tiêu của chúng tôi, chúng tôi phân tích các corpus từ khảo sát gần đây nhất về các tập dữ liệu đa ngôn ngữ [22] theo dõi thời gian thực dữ liệu đa ngôn ngữ. Chúng tôi chọn các tập dữ liệu cho các tác vụ tương tự của phân loại và phân tích cảm xúc, và lọc ra các tập dữ liệu chứa dữ liệu đa ngôn ngữ cho các đầu vào ngắn - lemmas, cặp từ, câu, tweets và các tuyên bố ngắn. Sau đó chúng tôi phân tích thủ công từng tập dữ liệu dựa trên cách dữ liệu đa ngôn ngữ được thu thập, và lọc ra các tập dữ liệu đa ngôn ngữ song song. Cuối cùng, chúng tôi lọc ra các tập dữ liệu tập trung vào các lĩnh vực hẹp cụ thể. Phương pháp này thu hẹp phạm vi quan tâm của chúng tôi xuống tập dữ liệu được tạo gần đây như một phần của SemEval-2023 Task 3: "Phát hiện thể loại, framing, và các kỹ thuật thuyết phục trong tin tức trực tuyến trong thiết lập đa ngôn ngữ" [15].

Trước SemEval 2023, một số tác vụ phát hiện thông tin sai lệch và tuyên truyền đa ngôn ngữ thách thức liên quan khác đã được giải quyết trong SemEval

9 tháng 4, 2024 9/45

--- TRANG 10 ---
(https://semeval.github.io) các tác vụ chia sẻ, bao gồm phát hiện nội dung siêu đảng phái [23], mỉa mai [24], và một tập nhỏ hơn các kỹ thuật thuyết phục trong dữ liệu văn bản [25] và đa phương thức [26]. Shared Task 3 trong thách thức SemEval 2023 mở rộng công trình trước đây về các kỹ thuật thuyết phục bằng cách giới thiệu các loại kỹ thuật thuyết phục mới, cũng như giải quyết hai tác vụ phụ liên quan khác, cụ thể là phân loại thể loại tin tức và phát hiện framing.

Tác vụ phụ 1: Phân loại thể loại tin tức. Cho một bài báo tin tức, xác định xem đó là báo cáo tin tức khách quan, một bài bình luận, hay châm biếm.

Tác vụ phụ 2: Phát hiện Framing. Cho một bài báo tin tức, xác định một hoặc nhiều trong số mười bốn chiều framing được sử dụng: Kinh tế, Năng lực và tài nguyên, Đạo đức, Công bằng và bình đẳng, Tính hợp pháp, hiến pháp và luật học, Đề xuất và đánh giá chính sách, Tội phạm và hình phạt, An ninh và quốc phòng, Sức khỏe và an toàn, Chất lượng cuộc sống, Bản sắc văn hóa, Dư luận công, Chính trị, Quy định bên ngoài và danh tiếng. Tập hợp các kỹ thuật framing được sử dụng trong tác vụ chia sẻ này được định nghĩa theo một phân loại học có sẵn [27].

Tác vụ phụ 3: Phát hiện Kỹ thuật Thuyết phục. Cho một đoạn văn của một bài báo tin tức, xác định không hoặc nhiều trong số 23 kỹ thuật thuyết phục được sử dụng (xem S1 Appendix để có danh sách chi tiết các kỹ thuật). Tập hợp các kỹ thuật đại diện cho một phần mở rộng của phân loại học được sử dụng trong các tập dữ liệu SemEval trước đây [26,28]. Tác vụ cũng cung cấp 6 danh mục cấp cao bao gồm các kỹ thuật thuyết phục tương tự. Mặc dù tác vụ ở mức đoạn văn, mỗi bài báo có ít nhất một đoạn văn được gán nhãn.

Cần lưu ý rằng ba trong số các hệ thống tham gia bài tập đánh giá SemEval-2023 Task 3 [15] ban đầu đã sử dụng adapters. Các nhóm HHU [29] và NAP [30] chỉ tham gia tác vụ phụ 3, trong đó họ sử dụng adapters, trong khi SheffieldVeraAI [16] áp dụng adapters cho tác vụ phụ 1 và 2. Phân tích hiệu suất ban đầu trong những tác vụ phụ này cho thấy hiệu ứng của adapters không nhất quán qua các tác vụ phụ khác nhau. Cụ thể, adapters đạt hiệu suất trung bình cao hơn cho các mô hình đơn ngôn ngữ trong tác vụ phụ 1, trong khi cản trở hiệu suất của các mô hình đơn ngôn ngữ trong tác vụ phụ 2 nhưng đạt kết quả tốt hơn ở đó cho các mô hình đa ngôn ngữ. Bằng chứng này

9 tháng 4, 2024 10/45

--- TRANG 11 ---
cung cấp động lực mạnh mẽ hơn nữa để hiểu rõ hơn về hiệu quả của các phương pháp adapter qua một loạt các tác vụ phân loại có độ khó khác nhau.

Ba tác vụ phụ này sử dụng dữ liệu chồng chéo rộng rãi, tức là cùng các bài báo đầu vào, tuy nhiên, khác nhau về các thuộc tính và thống kê tóm tắt cho các tập dữ liệu, được tóm tắt trong Bảng 1.

Bảng 1. Thuộc tính của các tác vụ phát hiện thể loại, framing và kỹ thuật thuyết phục

Tiêu chí so sánh | Tác vụ phụ 1 | Tác vụ phụ 2 | Tác vụ phụ 3
---|---|---|---
Loại tác vụ | Phát hiện thể loại | Phát hiện Framing | Kỹ thuật thuyết phục
Loại đầu vào | Toàn bộ tài liệu | Toàn bộ tài liệu | Đoạn văn
Mức độ chi tiết | Đa lớp | Đa nhãn | Đa nhãn
Metric chấm điểm chính thức | F1macro | F1micro | F1micro
Số lượng lớp | 3 | 14 | 23
Số token trung bình | 1,157 | 1,157 | 74
Kích thước tập huấn luyện | 1,234 | 1,238 | 10,927
Số ngôn ngữ nguồn | 6 | 6 | 6
Số ngôn ngữ đích | 9 | 9 | 9
Tính chủ quan của tác vụ | Cao | Thấp | Trung bình
Mất cân bằng dữ liệu | 12.7 | 4.6 | 44

Chúng tôi ước tính mất cân bằng dữ liệu như tỷ lệ các mẫu lớp cho lớp thường xuyên nhất trong tập huấn luyện so với lớp hiếm nhất. Dưới đây, chúng tôi thảo luận một số thuộc tính cá nhân chi tiết hơn.

Mất cân bằng lớp: Ngoài việc đặt ra các thách thức phân loại đa ngôn ngữ và đa lớp, dữ liệu cho ba tác vụ phụ này rất mất cân bằng, điều này thêm độ khó. Cụ thể, phân phối lớp cho tác vụ phụ 1 rất lệch, với 76% rơi vào lớp opinion và satire chỉ chiếm chưa đến 6% dữ liệu. Đối với tác vụ phụ 2, phân phối các lớp cũng không đều, nhưng ít lệch hơn so với tác vụ phụ 1. Frame phổ biến nhất là Political xuất hiện trong 49.4% các bài báo huấn luyện. Frame ít phổ biến nhất là Cultural Identity xuất hiện trong chỉ 10.8% các bài báo. Cuối cùng, trong tác vụ phụ 3 loaded language, doubt và name calling là những kỹ thuật thuyết phục phổ biến nhất, chiếm 22%, 15.6% và

9 tháng 4, 2024 11/45

--- TRANG 12 ---
12.8% các đoạn văn huấn luyện, tương ứng. 20 lớp còn lại, trung bình, chiếm 2.5% tập huấn luyện, tổng cộng 49.6%. Đặc biệt, appeal to time, whataboutism và red herring là những kỹ thuật thuyết phục ít thường xuyên nhất, đại diện cho 0.5%, 0.5% và 0.7% các đoạn văn huấn luyện, tương ứng.

Thống kê tập dữ liệu: tính đa ngôn ngữ, kích thước và độ dài đầu vào: Ba tập dữ liệu được cung cấp cho mỗi ngôn ngữ và tác vụ: huấn luyện có gán nhãn và phát triển (trừ các ngôn ngữ chưa thấy), và kiểm tra không gán nhãn.

Các nhà tổ chức tác vụ cung cấp dữ liệu kiểm tra bằng chín ngôn ngữ: Tiếng Anh (EN), Tiếng Pháp (FR), Tiếng Đức (DE), Tiếng Georgia (KA), Tiếng Hy Lạp (EL), Tiếng Ý (IT), Tiếng Ba Lan (PL), Tiếng Nga (RU), và Tiếng Tây Ban Nha (ES). Ba trong số các ngôn ngữ (Georgia, Hy Lạp và Tây Ban Nha) là các ngôn ngữ 'bất ngờ', có nghĩa là không có dữ liệu huấn luyện có gán nhãn tương ứng tồn tại trong tập dữ liệu. Do đó, để đưa ra dự đoán cho những ngôn ngữ này, tập kiểm tra của chúng phải được dịch sang một ngôn ngữ 'đã thấy', hoặc một phương pháp đa ngôn ngữ có khả năng hỗ trợ đánh giá zero-shot phải được áp dụng. Đối với 6 ngôn ngữ còn lại, dữ liệu huấn luyện và xác thực có gán nhãn được bao gồm trong tập dữ liệu.

Phải lưu ý rằng các nhà tổ chức tác vụ chưa công bố nhãn vàng cho tập kiểm tra để ngăn các nhà nghiên cứu overfitting hệ thống của họ. Điều này có nghĩa là phân tích lỗi chi tiết chỉ có thể được thực hiện trên sáu ngôn ngữ mà các tập phát triển có sẵn.

Bảng 2, 3 và 4 cho thấy phân tích chi tiết của dữ liệu huấn luyện, phát triển và kiểm tra được sử dụng trong tác vụ phụ 1, 2 và 3 tương ứng. Độ dài trung bình, được tính bằng số lượng token, được ước tính sử dụng tokenizer cho mô hình RoBERTa-large [31], vì đây là mô hình chúng tôi sử dụng trong các thí nghiệm của mình. Đối với các tập huấn luyện và phát triển trong tác vụ phụ 3, độ dài trung bình được tính cho các đoạn văn có ít nhất một kỹ thuật thuyết phục được gán. Đối với các tập kiểm tra trong tác vụ phụ 3, độ dài trung bình bao gồm mọi đoạn văn do thiếu nhãn tiêu chuẩn vàng cho dữ liệu kiểm tra. Đây là lý do tại sao số lượng ví dụ trong các tập kiểm tra cho tác vụ phụ 3 cao hơn đáng kể so với các tập huấn luyện và phát triển. Tuy nhiên, không phải tất cả những ví dụ này đều được mong đợi chứa ít nhất một kỹ thuật thuyết phục.

9 tháng 4, 2024 12/45

--- TRANG 13 ---
Bảng 2. Thống kê dữ liệu theo ngôn ngữ cho tác vụ phụ 1: Phát hiện thể loại.

Ngôn ngữ | Số lượng ví dụ | Số token trung bình
---|---|---
| Huấn luyện | Phát triển | Kiểm tra | Huấn luyện | Phát triển | Kiểm tra
EN | 433 | 83 | 54 | 1,307 | 1,066 | 978
FR | 158 | 54 | 50 | 1,241 | 1,025 | 927
DE | 132 | 45 | 50 | 995 | 913 | 1,203
IT | 226 | 77 | 61 | 975 | 856 | 958
PL | 144 | 50 | 47 | 1,354 | 1,438 | 1,935
RU | 142 | 49 | 72 | 1020 | 797 | 547
ES | 0 | 0 | 30 | N/A | N/A | 838
EL | 0 | 0 | 64 | N/A | N/A | 1,071
KA | 0 | 0 | 29 | N/A | N/A | 429

Bảng 3. Thống kê dữ liệu theo ngôn ngữ cho tác vụ phụ 2: Phát hiện Framing.

Ngôn ngữ | Số lượng ví dụ | Số token trung bình
---|---|---
| Huấn luyện | Phát triển | Kiểm tra | Huấn luyện | Phát triển | Kiểm tra
EN | 433 | 83 | 54 | 1,307 | 1,066 | 978
FR | 158 | 53 | 50 | 1,196 | 1,059 | 927
DE | 132 | 45 | 50 | 1,008 | 875 | 1,203
IT | 227 | 76 | 61 | 965 | 885 | 958
PL | 145 | 49 | 47 | 1,369 | 1,397 | 1,935
RU | 143 | 48 | 72 | 1,009 | 827 | 547
ES | 0 | 0 | 30 | N/A | N/A | 838
EL | 0 | 0 | 64 | N/A | N/A | 1,071
KA | 0 | 0 | 29 | N/A | N/A | 429

9 tháng 4, 2024 13/45

--- TRANG 14 ---
Bảng 4. Thống kê dữ liệu theo ngôn ngữ cho tác vụ phụ 3: Kỹ thuật thuyết phục.

Ngôn ngữ | Số lượng ví dụ | Số token trung bình
---|---|---
| Huấn luyện | Phát triển | Kiểm tra | Huấn luyện | Phát triển | Kiểm tra
EN | 3,610 | 1,103 | 11,466 | 88 | 37 | 65
FR | 1,693 | 437 | 7,140 | 97 | 108 | 91
DE | 1,251 | 405 | 11,060 | 95 | 87 | 76
IT | 1,742 | 594 | 8,302 | 99 | 91 | 99
PL | 1,228 | 415 | 14,084 | 109 | 122 | 90
RU | 1,232 | 310 | 8,414 | 85 | 80 | 66
ES | 0 | 0 | 1,320 | N/A | N/A | 76
EL | 0 | 0 | 3,792 | N/A | N/A | 72
KA | 0 | 0 | 640 | N/A | N/A | 78

Tác vụ phụ 1 và 2 sử dụng cùng tập bài báo trong tập kiểm tra, trong khi tập tích lũy của các bài báo được sử dụng trong các tập phát triển và huấn luyện cũng giống hệt nhau cho hai tác vụ phụ này, việc gán chúng vào một tập nhất định thay đổi một chút. Đây là lý do tại sao, như chúng ta có thể thấy từ Bảng 2 và 3, thống kê dữ liệu cho những tác vụ phụ này khá tương tự. Như có thể thấy, phân phối của các ví dụ huấn luyện qua các ngôn ngữ không đều, với EN chiếm gần 4 lần nhiều bài báo như DE, RU và PL. Chúng ta cũng có thể quan sát thấy rằng độ dài trung bình của các bài báo phụ thuộc rất nhiều vào ngôn ngữ, với các bài báo trong tập kiểm tra cho Georgia (KA) ngắn hơn hơn 4.5 lần so với các bài báo trong tập kiểm tra cho Ba Lan (PL). Khía cạnh này quan trọng cho các thí nghiệm của chúng tôi vì nó cho thấy rằng các mô hình có nhiều khả năng bỏ qua thông tin quan trọng cho một số ngôn ngữ nhất định so với những ngôn ngữ khác, do hạn chế của các mô hình transformer về độ dài đầu vào. Một quan sát khác là tập huấn luyện không phải lúc nào cũng đại diện cho tập kiểm tra. Ví dụ, các bài báo trong tập kiểm tra cho Nga (RU) ngắn hơn gấp đôi, trung bình, so với các bài báo trong tập huấn luyện cho ngôn ngữ này. Sự khác biệt về độ dài đầu vào giữa dữ liệu huấn luyện và kiểm tra ít đáng kể hơn cho tác vụ phụ 3, sử dụng các đoạn văn làm đầu vào, và cho đó tất cả các đầu vào đều nằm trong giới hạn của các mô hình transformer.

Chúng tôi phân tích các ngôn ngữ trong các tập huấn luyện và kiểm tra về lượng tài nguyên. Đối với mỗi ngôn ngữ, chúng tôi xem xét số lượng ví dụ huấn luyện trong ngôn ngữ đó hoặc trong một ngôn ngữ từ cùng họ ngôn ngữ và lượng dữ liệu huấn luyện trước trong ngôn ngữ đó cho mô hình mà chúng tôi sử dụng trong các thí nghiệm của mình. Phân tích của chúng tôi đặt Georgia là một ngoại lệ rõ ràng và một ngôn ngữ ít tài nguyên, có đáng kể ít dữ liệu huấn luyện trước hơn so với 8 ngôn ngữ khác và không có dữ liệu huấn luyện bằng Georgia hoặc ngôn ngữ liên quan. Mặt khác, tiếng Anh là ngôn ngữ có nhiều tài nguyên nhất, hưởng lợi từ hầu hết dữ liệu huấn luyện và huấn luyện trước và dữ liệu huấn luyện bổ sung trong một ngôn ngữ liên quan. Chi tiết của phân tích này được cung cấp trong S4 Appendix.

Tính chủ quan của tác vụ: Như được chỉ định trong các hướng dẫn chú thích [32], tính chủ quan khác nhau qua các tác vụ phụ. Tác vụ phụ 1 dựa vào phân tích rất tinh tế của toàn bộ bài báo và kiến thức thường thức, vì, như được đề cập bởi các nhà tổ chức, các bài báo châm biếm thường "có xu hướng bắt chước các bài báo thật" và đề cập đến "các cá nhân, tổ chức và sự kiện thế giới thực", trong khi sự phân biệt giữa báo cáo có ý kiến và khách quan có thể nằm ở những cách nhất định mà các phóng viên có xu hướng cân bằng các ý kiến được báo cáo. Các tác giả cũng nhấn mạnh rằng "ranh giới giữa ý kiến và báo cáo đôi khi có thể bị mờ" và "một bài báo tin tức chứa một số đoạn văn bản nhỏ, ví dụ, một câu xuất hiện châm biếm" thường không kích hoạt thể loại châm biếm. Tác vụ phát hiện framing có xu hướng dựa vào thông tin ngôn ngữ học nhất định hơn vì các hướng dẫn chú thích yêu cầu các người chú thích chỉ định các đoạn văn bản chính xác tương ứng với một frame nhất định. Các tác giả bổ sung cung cấp ví dụ về các chủ đề thảo luận kích hoạt các frame nhất định. Ví dụ, "chi phí, lợi ích, hoặc các tác động tài chính khác" thường là dấu hiệu của frame "kinh tế". Cuối cùng, tác vụ phụ 3 là tác vụ chi tiết nhất vì nó cung cấp chú thích mức đoạn văn của các kỹ thuật tuyên truyền. Việc phát hiện các kỹ thuật thuyết phục thường dựa vào một cấu trúc lập luận nhất định và các trigger ngôn ngữ học, chẳng hạn như việc đề cập đến một thực thể được coi là có thẩm quyền (kỹ thuật "appeal to authority"), liên kết một đối thủ với một nhóm, sự kiện hoặc khái niệm có ý nghĩa tiêu cực ("guilt by association" technique), "cụm danh từ, tính từ tạo thành nhãn và/hoặc tên" (kỹ thuật "name calling or labeling") hoặc "các đoạn văn bản lặp lại cùng thông điệp hoặc thông tin đã được giới thiệu trước đó" (kỹ thuật "repetition"). Đánh giá khách quan hơn về tính chủ quan không thể thực hiện được do thiếu điểm thỏa thuận giữa các người chú thích [15].

9 tháng 4, 2024 15/45

--- TRANG 16 ---
Thu thập tập dữ liệu: Dữ liệu được trích xuất từ cả các nguồn truyền thông chính thống và thay thế, được thu thập thông qua các bộ tổng hợp tin tức (ví dụ: Google News, Europe Media Monitor) và các tổ chức kiểm tra sự thật (ví dụ: MediaBiasFactCheck, NewsGuard), tương ứng. Tất cả các bài báo tin tức đều được xuất bản từ năm 2020 đến giữa năm 2022. Văn bản của mỗi bài báo được trích xuất tự động từ nguồn HTML của mỗi trang web bằng cách sử dụng công cụ thu thập văn bản Trafilatura [33] hoặc quy trình đặc thù trang web. Đáng chú ý, quy trình này dễ xảy ra lỗi vì đôi khi nó bao gồm nội dung văn bản không phải là một phần của chính bài báo tin tức, chẳng hạn như các cuộc thăm dò web, biểu mẫu đăng ký bản tin, và thông tin tác giả. Đối với tiếng Anh, một tập dữ liệu có sẵn trước đây cũng được sử dụng [28], nhưng các nhà tổ chức của tác vụ chia sẻ không làm cho nó đủ rõ ràng về dữ liệu tiếng Anh nào khác được bao gồm trong tập dữ liệu mới.

Phương pháp

Kịch bản huấn luyện

Trong khi trọng tâm chính của chúng tôi là kịch bản tinh chỉnh đa ngôn ngữ, chúng tôi giới thiệu hai thiết lập bổ sung trong đó các mô hình được huấn luyện chỉ trên dữ liệu tiếng Anh để điều tra liệu hiệu quả của mỗi phương pháp huấn luyện có khác nhau tùy thuộc vào thành phần và kích thước của tập huấn luyện hay không.

Ba kịch bản huấn luyện khác nhau này được tóm tắt dưới đây:

• Đa ngôn ngữ chung (nhiều-đến-nhiều): các mô hình được tinh chỉnh sử dụng tất cả dữ liệu huấn luyện trong 6 ngôn ngữ gốc.

• Tiếng Anh + Bản dịch (một-đến-nhiều): các mô hình được tinh chỉnh trên tất cả dữ liệu huấn luyện tiếng Anh gốc và bản dịch tiếng Anh của dữ liệu huấn luyện trong 5 ngôn ngữ khác. Quan trọng là phải đề cập rằng dữ liệu kiểm tra trong kịch bản này được giữ trong các ngôn ngữ gốc, có nghĩa là các dự đoán trên tất cả các ngôn ngữ ngoại trừ tiếng Anh được thực hiện theo cách xuyên ngôn ngữ zero-shot.

• Chỉ tiếng Anh (một-đến-nhiều): các mô hình được tinh chỉnh chỉ trên dữ liệu tiếng Anh gốc trong tập huấn luyện. Tương tự như kịch bản 'Tiếng Anh + Bản dịch', tập kiểm tra không được dịch sang tiếng Anh.

9 tháng 4, 2024 16/45

--- TRANG 17 ---
Việc lựa chọn ba kịch bản huấn luyện ở trên được thúc đẩy bởi thực tế là chúng tôi muốn đánh giá hiệu ứng của hai yếu tố khác nhau trên mỗi kỹ thuật huấn luyện:

1. Kịch bản huấn luyện 'Tiếng Anh + Bản dịch' cho phép đánh giá hiệu ứng của tính đa ngôn ngữ. Cụ thể, chúng tôi muốn so sánh hiệu ứng của việc có dữ liệu huấn luyện đa ngôn ngữ với kịch bản mà dữ liệu huấn luyện chỉ có sẵn trong một ngôn ngữ. Bằng cách dịch các ngôn ngữ khác sang tiếng Anh, chúng tôi tạo ra một tập dữ liệu bao gồm cùng số lượng ví dụ huấn luyện nhưng không có sự đa dạng ngôn ngữ. Điều này loại bỏ khả năng rằng sự khác biệt về hiệu suất qua ba phương pháp có thể do kích thước của dữ liệu huấn luyện cho mỗi ngôn ngữ. Đồng thời, dịch máy, như một mô hình chuyển giao cụ thể cho học tập xuyên ngôn ngữ [34], có thể tạo ra một mức độ nhiễu nào đó và do đó phá vỡ sự tương ứng cần thiết giữa mẫu gốc và được dịch.

2. Kịch bản huấn luyện 'Chỉ tiếng Anh' cho phép phân tích hiệu ứng của kích thước dữ liệu huấn luyện trên mỗi phương pháp. Điều này có thể đạt được bằng cách so sánh hiệu suất trên dữ liệu huấn luyện 'Chỉ tiếng Anh' với hiệu suất trên dữ liệu 'Tiếng Anh + Bản dịch', trong đó sự khác biệt duy nhất giữa hai cái là số lượng ví dụ huấn luyện. Tuy nhiên, kịch bản huấn luyện này không thể so sánh trực tiếp với kịch bản huấn luyện đa ngôn ngữ, vì nó không loại bỏ khả năng rằng sự khác biệt về hiệu suất của phương pháp có thể do các đặc điểm ngôn ngữ học khác nhau của dữ liệu huấn luyện đa ngôn ngữ.

Kỹ thuật huấn luyện

Trong việc lựa chọn các phương pháp PEFT cho phân tích sâu hơn của chúng tôi, chúng tôi thực hiện một nghiên cứu ablation chi tiết trên tác vụ phụ 1, nơi chúng tôi so sánh các phương pháp adapter, LoRA và BitFit trong các kịch bản phân loại đa ngôn ngữ và xuyên ngôn ngữ (xem S2 Appendix). Như có thể thấy, phương pháp BitFit không đạt được hiệu suất có thể so sánh với các phương pháp LoRA và adaptor cho bất kỳ ngôn ngữ nào trong bất kỳ kịch bản phân loại nào. Do đó chúng tôi loại trừ kỹ thuật PEFT này khỏi các thí nghiệm tiếp theo của chúng tôi.

Chúng tôi thử nghiệm với XLM-RoBERTa Large [35], sử dụng các kỹ thuật huấn luyện sau:

9 tháng 4, 2024 17/45

--- TRANG 18 ---
• Tinh chỉnh đầy đủ (FFT): Tất cả các tham số của mô hình được cập nhật trong quá trình tinh chỉnh.

• Low-Rank Adaptation (LoRA): Các tham số của mô hình được đóng băng và các ma trận LoRA (key, query, value) được thêm vào cả các lớp MLP và attention.

• Bottleneck Adapter (Adapter): Các tham số của mô hình được đóng băng và các bottleneck adapters trong cấu hình Pfeiffer [19] được thêm vào tất cả các lớp. Như chúng tôi thảo luận thêm, việc lựa chọn cấu hình adapter được đề xuất bởi các thí nghiệm ablation của chúng tôi đã chứng minh cấu hình Pfeiffer vượt trội hơn phiên bản Houlsby trung bình (S3 Appendix).

Phần tiếp theo mô tả phương pháp đằng sau mỗi kỹ thuật huấn luyện và kịch bản huấn luyện.

Thiết lập thí nghiệm

Lựa chọn mô hình và siêu tham số: Khi chọn kích thước mô hình, chúng tôi tính đến so sánh trước đây của các phương pháp PEFT dựa trên LoRA và adapter cho các kích thước khác nhau của mô hình RoBERTa qua một loạt rộng các tác vụ và tìm thấy sự ưu tiên rõ ràng cho kích thước mô hình lớn hơn [12,19]. Đối với cả phương pháp FFT và PEFT, chúng tôi bổ sung tiến hành so sánh XLM-RoBERTa Base với XLM-RoBERTa Large trên các tác vụ phụ của chúng tôi trong kịch bản huấn luyện mặc định bao gồm tất cả dữ liệu cho mỗi tác vụ phụ ở dạng gốc (kịch bản 'Đa ngôn ngữ chung'). Kết quả của so sánh này được trình bày trong S3 Appendix. Như có thể quan sát, một mô hình lớn thể hiện hiệu suất trung bình tốt hơn nhất quán qua tất cả ba tác vụ phụ. Do đó, dựa trên cả bằng chứng trước đây cho các kỹ thuật PEFT và các nghiên cứu ablation của chúng tôi cho FFT, chúng tôi quyết định sử dụng XLM-RoBERTa Large trong thiết lập thí nghiệm chính của chúng tôi.

Lựa chọn giữa cấu hình Pfeiffer [36] và Houlsby [10] của phương pháp adapter, chúng tôi thực hiện so sánh cả hai adapter cho kịch bản huấn luyện 'Đa ngôn ngữ chung'. Kết quả của so sánh này được trình bày trong S5 Appendix. Như có thể thấy, Pfeiffer adapter cho thấy lợi thế trung bình nhẹ so với Houlsby trong tất cả ba tác vụ phụ. Do đó chúng tôi sử dụng cấu hình Pfeiffer trong phần còn lại của các thí nghiệm.

9 tháng 4, 2024 18/45

--- TRANG 19 ---
Đối với các siêu tham số tốt nhất, trước tiên chúng tôi thực hiện tìm kiếm cho mỗi kịch bản huấn luyện và kỹ thuật huấn luyện trong mỗi tác vụ phụ. Tìm kiếm được thực hiện trên tập phát triển gốc như được cung cấp bởi các nhà tổ chức SemEval 2023 Task 3. Cấu hình tốt nhất thu được cho mỗi phương pháp có thể được tìm thấy trong Bảng 5. Người ta cần ghi nhớ rằng mục tiêu của chúng tôi là tối đa hóa hiệu suất mô hình trên mỗi kịch bản huấn luyện cho mỗi tác vụ phụ thay vì tối thiểu hóa chi phí tính toán. Nói cách khác, hiệu quả tham số lớn hơn có thể thực hiện được, nhưng với chi phí là hiệu suất mô hình.

Bảng 5. Siêu tham số.

Tinh chỉnh đầy đủ (FFT)
Siêu tham số | Tác vụ phụ 1 | Tác vụ phụ 2 | Tác vụ phụ 3
---|---|---|---
Tốc độ học | 1.00E-05 | 3.00E-05 | 3.40E-04
Kích thước batch | 16 | 8 | 32

Low-Rank Adaptation (LoRA)  
Siêu tham số | Tác vụ phụ 1 | Tác vụ phụ 2 | Tác vụ phụ 3
---|---|---|---
Tốc độ học | 7.00E-06 | 3.00E-4 | 1.59E-03
Kích thước batch | 16 | 8 | 32
Hạng | 8 | 8 | 2
Lớp | tất cả | tất cả | tất cả
Dropout | 5.00E-01 | 5.00E-01 | 5.00E-01
Ma trận attention | k,q,v | k,q,v | k,q,v

Bottleneck Adapter
Siêu tham số | Tác vụ phụ 1 | Tác vụ phụ 2 | Tác vụ phụ 3
---|---|---|---
Tốc độ học | 3.16E-05 | 2.00E-4 | 4.30E-04
Kích thước batch | 16 | 8 | 32
Hệ số giảm | 4 | 8 | 8

Đối với bottleneck adapter, chúng tôi sử dụng cấu hình Pfeiffer mặc định [36] bằng cách thêm adapter sau mỗi lớp con 'ffn'. Mặc dù các nghiên cứu ablation trước đây cho thấy rằng 4 lớp thấp nhất ít đóng góp vào hiệu suất [20], mục tiêu của chúng tôi là tối đa hóa hiệu suất và làm cho thiết lập có thể so sánh với LoRA nơi chúng tôi sử dụng tất cả các lớp.

9 tháng 4, 2024 19/45

--- TRANG 20 ---
Tiền xử lý văn bản: Như được hiển thị trong Bảng 1, tác vụ phụ 1 và 2 có số lượng token trung bình trên mỗi bài báo là 1,157. Do đó trong những tác vụ phụ đó, 80.0% bài báo bị cắt ngắn ở tối đa 512 token. Ngược lại, tác vụ phụ 3 trình bày số lượng token trung bình trên mỗi chuỗi là 74, do đó tất cả các câu huấn luyện được mã hóa đầy đủ mà không mất thông tin. Đối với tác vụ phụ 1, các bài báo dài hơn 512 token được tách thành các câu, sau đó được lấy mẫu tuần tự từ đầu và cuối bài báo, bảo toàn thứ tự gốc, cho đến khi đạt tối đa 512 token. Phương pháp cắt ngắn như vậy được thúc đẩy bởi các thí nghiệm của chúng tôi trên dữ liệu tác vụ phụ 1 trong giai đoạn thi đấu của SemEval 2023 Task 3 [16]. Phương pháp này mang lại cải thiện đáng kể trong điểm F1 macro so với thiết lập chỉ đơn giản cắt ngắn văn bản thành 512 token đầu tiên. Cải thiện này có thể được giải thích bởi thực tế là các hướng dẫn cho người chú thích con người trong tác vụ phụ 1 nhấn mạnh tầm quan trọng của các câu có ý kiến có xu hướng được tìm thấy về phía cuối các bài báo.

Chúng tôi thực hiện tiền xử lý văn bản cho tác vụ phụ 1 và 2 bằng cách áp dụng các bước sau cho tất cả các ngôn ngữ:

• một dấu chấm được thêm vào cuối mỗi tiêu đề;
• các câu trùng lặp trực tiếp theo sau nhau được loại bỏ;
• ký hiệu @ được loại bỏ khỏi bất kỳ handle Twitter nào;
• các liên kết đến trang web và hình ảnh cũng được loại bỏ.

Các bài báo tiếng Anh được tiền xử lý thêm như sau:

• văn bản khuyến khích chia sẻ trên các nền tảng truyền thông xã hội khác nhau được loại bỏ khỏi cuối các bài báo;
• các câu khuyến khích sự tham gia của người dùng vào các cuộc thăm dó trực tuyến, bình luận, hoặc quảng cáo cũng được loại bỏ;
• các câu quy định điều khoản sử dụng của trang web được loại bỏ;
• loại bỏ các câu chỉ ra việc cấp phép và chứa các cụm từ như 'reprinted with permission', 'posted with permission' và 'all rights reserved';

9 tháng 4, 2024 20/45

--- TRANG 21 ---
• các câu nêu chi tiết tiểu sử tác giả cũng được loại bỏ.

Đối với tác vụ phụ 3, các thí nghiệm sơ bộ không tìm thấy cải thiện hiệu suất khi tiền xử lý văn bản được áp dụng, do đó các thí nghiệm của chúng tôi cho tác vụ phụ này sử dụng trực tiếp văn bản gốc. Quan trọng, đối với các thí nghiệm tác vụ phụ 3, chúng tôi bao gồm các câu không có nhãn được gán vào dữ liệu huấn luyện bằng cách gán cho chúng một vector số không để chỉ ra rằng chúng không thuộc về bất kỳ lớp nào. Phương pháp này được chỉ ra là cải thiện đáng kể hiệu suất phân loại trên tác vụ phụ này trong các thí nghiệm ban đầu của chúng tôi [16,17]. Kích thước của tập huấn luyện được hiển thị trong Bảng 1 (10,927 ví dụ) dựa trên số lượng ví dụ có nhãn. Khi các câu không nhãn được thêm vào, dữ liệu huấn luyện cho tác vụ phụ 3 tăng lên 20,704 instance.

Các tác vụ phụ đa nhãn 2 và 3 sử dụng ngưỡng tin cậy lần lượt là 50% và 30%, sau khi áp dụng hàm kích hoạt sigmoid cho các logit. Ngưỡng tin cậy cho tác vụ phụ 3 cố ý thấp hơn và được chọn theo các thí nghiệm trước đây của chúng tôi [17], tiết lộ rằng việc hiệu chỉnh cẩn thận của nó có thể ảnh hưởng đáng kể đến hiệu suất của mô hình.

Kịch bản huấn luyện: Chúng tôi thử nghiệm với ba kịch bản huấn luyện được mô tả trước đây. Tất cả các mô hình được huấn luyện trên phần chia huấn luyện gốc được cung cấp bởi các nhà tổ chức tác vụ, sử dụng dữ liệu trong tất cả 6 ngôn ngữ đã thấy; tất cả dữ liệu EN và bản dịch sang tiếng Anh của dữ liệu trong 5 ngôn ngữ khác; hoặc chỉ sử dụng phần EN của dữ liệu huấn luyện. Mỗi mô hình sau đó được đánh giá trên phần chia kiểm tra của nhà tổ chức tác vụ (6 ngôn ngữ đã thấy và 3 ngôn ngữ bất ngờ), mà không dịch.

Phù hợp với nghiên cứu trước đây [11,14,37], chúng tôi định nghĩa ngôn ngữ 'chưa thấy' là ngôn ngữ 'đích' mà dự đoán xuyên ngôn ngữ zero-shot được thực hiện và khác với ngôn ngữ 'nguồn' mà việc tinh chỉnh đặc thù tác vụ của mô hình transformer được thực hiện. Ba ngôn ngữ tập kiểm tra chưa thấy - Hy Lạp, Georgia, và Tây Ban Nha - cho phép chúng tôi đánh giá khả năng học chuyển giao xuyên ngôn ngữ zero-shot của các kỹ thuật huấn luyện được huấn luyện trong thiết lập đầy đủ đa ngôn ngữ đầu tiên. Trong hai kịch bản huấn luyện khác ('Tiếng Anh + Bản dịch' và 'Chỉ tiếng Anh'), 8 ngôn ngữ còn lại (FR, DE, IT, PL, RU, ES, EL và KA) cung cấp hiểu biết về hiệu suất của các mô hình trong thiết lập xuyên ngôn ngữ zero-shot.

9 tháng 4, 2024 21/45

--- TRANG 22 ---
Metrics đánh giá: Hiệu suất của các kỹ thuật huấn luyện khác nhau sau đó được so sánh sử dụng hai tập tiêu chí: (1) hiệu quả tài nguyên tính toán; (2) hiệu suất phân loại. Đối với cái sau, cả F1 micro và F1 macro đều được báo cáo như metrics hiệu suất cho tất cả ba tác vụ phụ. Tuy nhiên, phải lưu ý rằng các nhà tổ chức SemEval 2023 Task 3 chỉ sử dụng F1 macro như metric chấm điểm chính thức cho tác vụ phụ 1, trong khi tác vụ phụ 2 và 3 chỉ sử dụng F1 micro. Do đó, nơi phân tích đặc thù ngôn ngữ chi tiết hơn được thực hiện trong bài báo này, chỉ metric chính thức tương ứng cho mỗi tác vụ phụ được cung cấp.

Trung bình và độ lệch chuẩn được tính toán trên ba khởi tạo seed ngẫu nhiên khác nhau.

Hiệu quả tài nguyên được đo thông qua bốn metrics: (i) lượng VRAM đỉnh được sử dụng trong quá trình huấn luyện; (ii) tăng tốc tương đối so với phương pháp được tinh chỉnh đầy đủ, là số bước huấn luyện trên giây Nm/tm của phương pháp tương ứng (LoRA hoặc Adapter) chia cho số bước huấn luyện trên giây của phương pháp được tinh chỉnh đầy đủ NFFT/tFFT (Phương trình 4); (iii) số lượng tham số có thể huấn luyện; và (iv) số lượng tham số không thể huấn luyện.

S∗=Nm/tm / NFFT/tFFT (4)

Chi tiết triển khai: Tất cả các thí nghiệm được thực hiện với framework AdapterHub [36].

Để thu được dữ liệu 'Tiếng Anh + Bản dịch', chúng tôi dịch tất cả dữ liệu huấn luyện và phát triển có sẵn sang tiếng Anh sử dụng Google Cloud Translation API. Việc lựa chọn hệ thống Dịch máy (MT) được thúc đẩy bởi báo cáo mở rộng gần đây về đánh giá các hệ thống MT [38]. Kết quả qua tất cả 11 cặp ngôn ngữ và 9 lĩnh vực được phân tích bởi các tác giả cho thấy rằng Google Translate là hệ thống tiên tiến dựa trên điểm COMET. Chúng tôi tin rằng lựa chọn này giúp giảm nhiễu tiềm năng gây ra bởi dịch, tuy nhiên, khó để định lượng hiệu ứng của nhiễu cho trường hợp chung của các bài báo tin tức, vì, như báo cáo cho thấy, hiệu suất của các hệ thống thay đổi rất nhiều tùy thuộc vào lĩnh vực. Hơn nữa, trong khi chúng tôi có ước tính hiệu suất cho các cặp EN-DE, EN-FR, EN-ES và EN-IT (có thể so sánh qua tất cả các lĩnh vực), chúng tôi không thể tìm thấy kết quả liên quan cho 4 cặp ngôn ngữ còn lại (EN-PL, EN-KA, EN-RU, EN-EL).

Mã của chúng tôi có sẵn trên GitHub (https://github.com/GateNLP/PEFT_FFT_multilingual) và kho lưu trữ Zenodo (https://doi.org/10.5281/zenodo.10066649).

Kết quả

Phân tích kết quả của chúng tôi được cấu trúc xung quanh ba câu hỏi nghiên cứu chính sau:

RQ1: Hiệu suất phân loại và chi phí tính toán của mỗi kỹ thuật huấn luyện khác nhau như thế nào cho mỗi tác vụ phụ?

RQ2: Các kịch bản huấn luyện (xác định sự đa dạng của các ngôn ngữ trong tập huấn luyện và kích thước của nó) ảnh hưởng đến hiệu suất của mỗi kỹ thuật huấn luyện như thế nào?

RQ3: Các kỹ thuật huấn luyện so sánh với nhau như thế nào cho mỗi kịch bản huấn luyện và ngôn ngữ?

Tầm quan trọng của câu hỏi nghiên cứu được đặt ra trong nghiên cứu này được thúc đẩy bởi các hạn chế của nghiên cứu trước đây về hiệu suất của các kỹ thuật PEFT trong các tác vụ phân loại mức bài báo đa ngôn ngữ.

RQ1 cung cấp phân tích cấp cao của mỗi kỹ thuật huấn luyện dưới kịch bản huấn luyện tốt nhất. So sánh này quan trọng vì nó cung cấp việc kiểm tra đầu tiên theo hiểu biết của chúng tôi về các kỹ thuật PEFT và FFT trên các văn bản không song song không bị hạn chế trong một lĩnh vực hẹp và có độ dài từ đoạn văn đến bài báo dài. RQ này đặc biệt mới đối với phương pháp LoRA vì, như chúng tôi nhấn mạnh ở trên, không có so sánh nào của LoRA với các phương pháp adapter hoặc FFT được thực hiện trước đây cho tác vụ phân loại đa nhãn đa ngôn ngữ. Chúng tôi bổ sung cung cấp so sánh chi phí tính toán vì một so sánh như vậy không được thực hiện trước đây cho bất kỳ tác vụ phụ nào trong nghiên cứu này. Không thể giả định rằng kết quả trước đây sẽ giữ cho các tác vụ của chúng tôi cho rằng hiệu quả tính toán của các phương pháp PEFT có thể nhạy cảm với độ dài đầu vào [12]. Không giống như các phân tích trước đây giữa bottleneck adaptors, LoRA và FFT thực hiện phân tích độ trễ suy luận bất khả tri tác vụ hoặc cung cấp số lượng tham số có thể huấn luyện cho các tác vụ cụ thể [12], so sánh của chúng tôi tập trung vào thống kê chi tiết hơn, chẳng hạn như việc sử dụng VRAM đỉnh và thời lượng huấn luyện tương đối. Thống kê sau đặc biệt quan trọng vì quá trình huấn luyện thường tốn thời gian hơn so với suy luận, và mối quan hệ giữa số lượng tham số có thể huấn luyện và thời gian huấn luyện không tỷ lệ tuyến tính.

RQ2 cung cấp hiểu biết tốt hơn về cách hiệu suất của mỗi kỹ thuật huấn luyện thay đổi trong các kịch bản huấn luyện khác nhau. Như đã đề cập trong phần 'Công trình liên quan', các thí nghiệm trước đây [14] được tiến hành trên các kiểm tra đa ngôn ngữ không song song kết luận rằng các phương pháp dựa trên dịch zero-shot (translate-train và translate-test) vượt trội hơn các phương pháp xuyên ngôn ngữ cho cả kỹ thuật huấn luyện FFT và adapter. Tuy nhiên, các phương pháp xuyên ngôn ngữ zero-shot của họ bị giới hạn trong các kịch bản một-đến-nhiều, làm cho khó so sánh công bằng các thiết lập huấn luyện dựa trên dịch với các kịch bản đa ngôn ngữ chung do sự khác biệt về kích thước của các tập huấn luyện. Chúng tôi mục tiêu giải quyết khoảng trống này bằng cách thay đổi tuần tự một khía cạnh của quá trình huấn luyện, bằng cách đầu tiên loại bỏ tính đa ngôn ngữ của tập huấn luyện trong khi giữ cùng kích thước, và sau đó giảm kích thước và loại bỏ các văn bản được dịch có thể nhiễu từ tập huấn luyện. Thêm vào đó, các thí nghiệm đa ngôn ngữ chung trước đây [14] không loại bỏ bất kỳ ngôn ngữ nào để thực hiện suy luận xuyên ngôn ngữ zero-shot, điều này cũng làm cho không thể so sánh phương pháp này với các phương pháp đơn ngôn ngữ. Cuối cùng, chúng tôi thêm LoRA vào tập hợp các phương pháp PEFT và cung cấp hiểu biết mới về hiệu suất của kỹ thuật này dưới các kịch bản khác nhau trong các tác vụ đa ngôn ngữ.

RQ3 tập trung vào một chiều khác của vấn đề và cố gắng trả lời phương pháp nào được ưu tiên tùy thuộc vào lượng và loại dữ liệu huấn luyện. Phân tích này được thúc đẩy bởi công trình trước đây bởi He et al. [11] trên các văn bản ngắn kết luận rằng các phương pháp adapter đặc biệt có lợi cho các tác vụ ít tài nguyên và xuyên ngôn ngữ. Do đó chúng tôi điều tra cách các phương pháp PEFT và FFT so sánh với nhau khi chúng tôi giảm số lượng tài nguyên huấn luyện trong một số ngôn ngữ nhất định và giảm lượng dữ liệu huấn luyện tổng thể.

9 tháng 4, 2024 24/45

--- TRANG 25 ---
So sánh các thuộc tính tính toán và hiệu suất của các kỹ thuật huấn luyện

Để trả lời câu hỏi nghiên cứu đầu tiên (RQ1), chúng tôi chọn kịch bản huấn luyện tốt nhất cho mỗi phương pháp và tác vụ phụ và so sánh hiệu suất của mô hình FFT với hiệu suất của các phương pháp LoRA và adapter cho mỗi tác vụ phụ. Kết quả của so sánh này được báo cáo trong Bảng 6. Chúng tôi báo cáo điểm trung bình sau ba lần chạy với các seed ngẫu nhiên khác nhau cùng với độ lệch chuẩn. Độ lệch chuẩn là căn bậc hai của trung bình các độ lệch bình phương từ trung bình.

9 tháng 4, 2024 25/45

--- TRANG 26 ---
Bảng 6. Hiệu suất và chi phí tính toán cho mỗi tác vụ phụ và kỹ thuật huấn luyện.

Tác vụ phụ 1: Phát hiện thể loại
| | FFT | LoRA | Adapter |
|---|---|---|---|
| F1macro* | 59.9±3.1 | 57.9±6.3 | 58.0±2.0 |
| F1micro | 61.7±7.5 | 60.2±3.9 | 62.8±5.7 |
| Kịch bản huấn luyện tốt nhất | Đa ngôn ngữ chung | Đa ngôn ngữ chung | Đa ngôn ngữ chung |
| Sử dụng VRAM đỉnh | ∼39GB | ∼24GB | ∼28GB |
| Thời gian huấn luyện tương đối so với FFT | 1 | 0.59 | 0.67 |
| Tham số có thể huấn luyện | ∼560M | ∼3.2M | ∼26M |
| Tham số không thể huấn luyện | 0 | ∼560M | ∼560M |

Tác vụ phụ 2: Phát hiện Framing
| | FFT | LoRA | Adapter |
|---|---|---|---|
| F1macro | 49.2±7.4 | 45.3±7.1 | 47.1±7.9 |
| F1micro* | 56.7±6.1 | 53.4±6.0 | 54.8±7.1 |
| Kịch bản huấn luyện tốt nhất | Đa ngôn ngữ chung | Đa ngôn ngữ chung | Đa ngôn ngữ chung |
| Sử dụng VRAM đỉnh | ∼23GB | ∼18GB | ∼14GB |
| Thời gian huấn luyện tương đối so với FFT | 1 | 0.68 | 0.56 |
| Tham số có thể huấn luyện | ∼560M | ∼4M | ∼7M |
| Tham số không thể huấn luyện | 0 | ∼560M | ∼560M |

Tác vụ phụ 3: Kỹ thuật thuyết phục
| | FFT | LoRA | Adapter |
|---|---|---|---|
| F1macro | 23.7±5.0 | 23.7±6.9 | 20.6±6.3 |
| F1micro* | 41.8±8.6 | 42.9±9.5 | 42.2±9.5 |
| Kịch bản huấn luyện tốt nhất | Đa ngôn ngữ chung | Đa ngôn ngữ chung | Đa ngôn ngữ chung |
| Sử dụng VRAM đỉnh | ∼20GB | ∼13GB | ∼16GB |
| Thời gian huấn luyện tương đối so với FFT | 1 | 0.56 | 0.71 |
| Tham số có thể huấn luyện | ∼560M | ∼2M | ∼7M |
| Tham số không thể huấn luyện | 0 | ∼560M | ∼560M |

Điểm và metrics hiệu suất tốt nhất xuất hiện in đậm. Metric chính cho một tác vụ phụ nhất định được đánh dấu bằng dấu hoa thị (∗).

9 tháng 4, 2024 26/45

--- TRANG 27 ---
Kết quả chứng minh rằng:

(1) FFT và adapters hoạt động tốt hơn trong tác vụ phụ 1 và 2, trong khi LoRA hoạt động tốt hơn cho tác vụ phụ 3. Chúng tôi quan sát thấy rằng đối với các văn bản dài hơn, chẳng hạn như các bài báo được phân tích trong tác vụ phụ 1 và 2, phân loại dựa trên FFT và adapter thể hiện kết quả tốt hơn trung bình so với LoRA. Đồng thời, LoRA trung bình vượt trội hơn FFT và adapters cho tác vụ phụ 3, được huấn luyện trên các văn bản ngắn hơn.

(2) Kịch bản huấn luyện 'Đa ngôn ngữ chung' hoạt động tốt nhất, bất kể tác vụ phụ và kỹ thuật huấn luyện. Chúng tôi quan sát một mô hình của kịch bản huấn luyện 'Đa ngôn ngữ chung' đạt kết quả tốt nhất cho tất cả ba tác vụ phụ cũng như tất cả ba kỹ thuật huấn luyện. Điều này có nghĩa là, nói chung, huấn luyện các mô hình trên các tập dữ liệu lớn hơn với nhiều ngôn ngữ khác nhau, có thể có lợi cho cả phương pháp FFT và PEFT được áp dụng cho các tác vụ với các thuộc tính khác nhau. Tuy nhiên, hiệu ứng này không được quan sát nhất quán cho tất cả các kết hợp của kịch bản huấn luyện và kỹ thuật huấn luyện (phân tích chi tiết hơn cho từng kịch bản huấn luyện cá nhân được cung cấp trong phần tiếp theo).

(3) LoRA và adapters có thể tiết kiệm chi phí tính toán đáng kể. Theo thiết kế, PEFT giảm số lượng tham số có thể huấn luyện đáng kể (giữa 140 và 280 lần ít tham số hơn). Kết quả là, đối với tác vụ phụ 1 và 3, việc sử dụng LoRA dẫn đến giảm đáng kể trong tiêu thụ bộ nhớ — từ 39GB xuống 24GB (38%), và từ 20GB xuống 13GB (35%) tương ứng. Đối với tác vụ phụ 2, adapter đạt hiệu quả bộ nhớ tốt nhất, trong khi giảm việc sử dụng VRAM đỉnh từ 23GB xuống 14GB (39%). Mô hình tương tự có thể được quan sát cho tổng thời gian huấn luyện, giảm xuống 56-71% thời gian huấn luyện FFT.

(4) Tiết kiệm chi phí tính toán dẫn đến hiệu suất thấp hơn, tuy nhiên tồn tại một số ngoại lệ. Tiết kiệm việc sử dụng VRAM và rút ngắn thời gian huấn luyện tự nhiên được phản ánh trong hiệu suất thấp hơn so với FFT. Adapters liên tục bị vượt trội bởi FFT trong tất cả ba tác vụ phụ. Tuy nhiên, trong trường hợp tác vụ phụ 3, LoRA không chỉ đạt kết quả có thể so sánh cao, mà còn vượt trội hơn FFT cho hầu hết các ngôn ngữ (tuy nhiên, sự khác biệt không có ý nghĩa thống kê — phân tích chi tiết hơn được cung cấp trong các phần tiếp theo). Đối với tác vụ phụ 2 và 3, chúng tôi cũng quan sát độ lệch chuẩn cao hơn của kết quả, ngụ ý sự bất ổn cao hơn của tinh chỉnh khi PEFT được áp dụng.

9 tháng 4, 2024 27/45

--- TRANG 28 ---
So sánh hiệu ứng của kịch bản huấn luyện trên mỗi kỹ thuật huấn luyện

Để trả lời câu hỏi thứ hai (RQ2), chúng tôi so sánh các kỹ thuật huấn luyện FFT, LoRA và adapter qua ba kịch bản huấn luyện được giới thiệu ở trên, cụ thể là 'Đa ngôn ngữ chung', 'Tiếng Anh + Bản dịch' và 'Chỉ tiếng Anh'. Cần lưu ý rằng trong hai kịch bản sau tất cả các ngôn ngữ trong tập kiểm tra đều chưa thấy (trừ tiếng Anh) vì mô hình không có quyền truy cập vào dữ liệu huấn luyện trong những ngôn ngữ đó. Kết quả được đo bằng các metrics tác vụ phụ chính thức (F1 macro cho tác vụ phụ 1 và F1 micro cho tác vụ phụ 2 và 3) và được hiển thị trong Bảng 7, 8 và 9.

Bảng 7. Tác vụ phụ 1: Phát hiện thể loại - Điểm F1 macro trung bình ±1 STD.

[Bảng chi tiết với các điểm số cho từng ngôn ngữ và kịch bản huấn luyện]

Bảng 8. Tác vụ phụ 2: Phát hiện Framing - Điểm F1 micro trung bình ±1 STD.

[Bảng chi tiết với các điểm số cho từng ngôn ngữ và kịch bản huấn luyện]

Bảng 9. Tác vụ phụ 3: Kỹ thuật thuyết phục - Điểm F1 micro trung bình ±1 STD.

[Bảng chi tiết với các điểm số cho từng ngôn ngữ và kịch bản huấn luyện]

9 tháng 4, 2024 28/45

--- TRANG 29 ---
[Tiếp tục Bảng 8 và 9 với dữ liệu chi tiết]

9 tháng 4, 2024 29/45

--- TRANG 30 ---
(1) Sự đa dạng của các ngôn ngữ trong tập huấn luyện cải thiện hiệu suất trung bình của kỹ thuật FFT. Đối với tất cả ba tác vụ phụ, chúng tôi quan sát hiệu suất phân loại trung bình tốt hơn đáng kể khi dữ liệu huấn luyện được cung cấp trong 6 ngôn ngữ khác nhau gốc so với việc cung cấp cùng lượng dữ liệu chỉ bằng tiếng Anh. Khi nhìn vào các ngôn ngữ cá nhân, hiệu ứng này đúng cho tất cả các ngôn ngữ trong tác vụ phụ 1 ngoại trừ tiếng Anh (ngôn ngữ duy nhất được thấy trong kịch bản huấn luyện 'Tiếng Anh + Bản dịch') và Tây Ban Nha (một trong những ngôn ngữ chưa thấy trong thiết lập đa ngôn ngữ chung). Hiệu suất giảm trên tập kiểm tra tiếng Anh trong kịch bản 'Đa ngôn ngữ chung' cho tác vụ này có thể được giải thích với hiệu ứng 'suy luận tiêu cực' được nhấn mạnh trong các nghiên cứu trước đây [39]. Tuy nhiên, chúng tôi không quan sát hiệu ứng này một cách nhất quán qua các tác vụ phụ. Cụ thể, đối với tác vụ phụ 2, ngoại lệ duy nhất là tiếng Pháp, hưởng lợi từ thiết lập huấn luyện đơn ngôn ngữ. Đáng chú ý, hiệu suất trên tập kiểm tra tiếng Anh giảm trong kịch bản 'Tiếng Anh + Bản dịch' cho tác vụ phụ 2, mặc dù được huấn luyện trên nhiều dữ liệu hơn trong ngôn ngữ này. Cuối cùng, đối với tác vụ phụ 3, tiếng Pháp là ngôn ngữ duy nhất trong tập kiểm tra hưởng lợi từ việc được huấn luyện trên dữ liệu tiếng Anh đơn ngôn ngữ, điều này nhất quán với tác vụ phụ 2.

(2) Sự đa dạng của các ngôn ngữ trong tập huấn luyện có hiệu ứng không nhất quán trên hiệu suất của kỹ thuật huấn luyện LoRA qua ba tác vụ phụ. Trong khi chúng tôi quan sát giảm hiệu suất phân loại trung bình trong kịch bản huấn luyện 'Tiếng Anh + Bản dịch' cho tác vụ phụ 2 và 3, thiết lập này cải thiện kết quả trung bình cho tác vụ phụ 1. Đối với tác vụ phụ 3, hiệu ứng này đúng cho mọi ngôn ngữ trong tập kiểm tra, trong khi đối với tác vụ phụ 2, LoRA cải thiện hiệu suất trên EN và FR khi được huấn luyện sử dụng dữ liệu tiếng Anh đơn ngôn ngữ. Đối với tác vụ phụ 1, LoRA hưởng lợi từ dữ liệu huấn luyện đa ngôn ngữ khi đưa ra dự đoán trên 5 trong số 9 ngôn ngữ (DE, IT, PL, RU, và KA) và ưu tiên thiết lập huấn luyện tiếng Anh đơn ngôn ngữ cho 4 ngôn ngữ còn lại (EN, FR, EL và ES). Sự khác biệt đặc biệt cao đối với Tây Ban Nha, dẫn đến hiệu suất trung bình hơi tốt hơn trong kịch bản huấn luyện đơn ngôn ngữ cho tác vụ phụ 1.

(3) Sự đa dạng của các ngôn ngữ trong tập huấn luyện có hiệu ứng không nhất quán trên hiệu suất phân loại adapter qua tất cả các tác vụ. Trong khi kịch bản huấn luyện 'Tiếng Anh + Bản dịch' cải thiện không đáng kể hiệu suất trung bình trong tác vụ phụ 1, nó giảm hiệu suất trung bình trong tác vụ phụ 2 và 3. Adapters

9 tháng 4, 2024 30/45

--- TRANG 31 ---
hưởng lợi từ kịch bản huấn luyện đơn ngôn ngữ khi đưa ra dự đoán trên 6 trong số 9 ngôn ngữ (EN, FR, IT, RU, ES và EL) trong tác vụ phụ 1. Đối với tác vụ phụ 2 và 3, điều này chỉ đúng cho 3 ngôn ngữ (IT, EL và KA) và 2 ngôn ngữ (EN và ES) tương ứng.

(4) Giảm kích thước của tập huấn luyện làm giảm hiệu suất của phương pháp FFT, LoRA cũng như adapter trên tất cả các ngôn ngữ đã thấy và zero-shot. Chúng tôi quan sát giảm đáng kể hiệu suất của FFT và cả hai PEFT trong kịch bản huấn luyện 'Chỉ tiếng Anh' so với thiết lập 'Tiếng Anh + Bản dịch' qua tất cả các tác vụ phụ và cho mọi ngôn ngữ trong mỗi tác vụ phụ. Kết quả này đặc biệt đáng chú ý đối với tập kiểm tra tiếng Anh vì nó chỉ ra rằng ngay cả dữ liệu được dịch có thể nhiễu cũng có thể cải thiện hiệu suất trên một ngôn ngữ nhất định so với việc sử dụng tập dữ liệu nhỏ hơn nhưng chất lượng tốt hơn.

Tóm lại, hiệu ứng của việc loại bỏ sự đa dạng ngôn ngữ khỏi tập huấn luyện không nhất quán qua các tác vụ phụ SemEval 2023 và kỹ thuật huấn luyện. Tác vụ phụ 1, cụ thể, thể hiện cải thiện hiệu suất nhẹ cho các phương pháp LoRA và adapter khi được huấn luyện trên dữ liệu 'Tiếng Anh + Bản dịch'. Tất cả các kỹ thuật huấn luyện cho thấy hiệu suất giảm đáng kể khi được huấn luyện trên dữ liệu chỉ tiếng Anh gốc, chứng minh tầm quan trọng của kích thước dữ liệu huấn luyện.

Trong phần tiếp theo, chúng tôi so sánh kết quả của ba kỹ thuật huấn luyện trong mỗi ba kịch bản huấn luyện.

So sánh các kỹ thuật huấn luyện cho mỗi kịch bản huấn luyện và ngôn ngữ

Để trả lời câu hỏi nghiên cứu thứ ba (RQ3), chúng tôi phân tích liệu hiệu suất của các phương pháp FFT, LoRA và adapter có nhất quán qua các kịch bản huấn luyện hay các kịch bản nhất định dễ bị thiếu dữ liệu huấn luyện và hoặc sự đa dạng ngôn ngữ trong dữ liệu huấn luyện hơn. Phân tích này, trên cơ sở kết quả được trình bày trong Bảng 7, 8 và 9 cho tác vụ phụ 1, 2 và 3 tương ứng, bổ sung cho RQ2 trước đây.

(1) FFT vượt trội hơn các phương pháp LoRA và adapter cho tác vụ phụ 1 và 2 trong kịch bản huấn luyện 'Đa ngôn ngữ chung', trong khi đối với tác vụ phụ 3, chỉ dự đoán zero-shot trên các ngôn ngữ chưa thấy hưởng lợi từ phương pháp FFT.

9 tháng 4, 2024 31/45

--- TRANG 32 ---
Chúng tôi quan sát thấy rằng FFT mang lại hiệu suất tốt nhất trong thiết lập đa ngôn ngữ cho hầu hết các ngôn ngữ đã thấy và chưa thấy trong tác vụ phụ 1 và 2. Trong khi sự khác biệt giữa FFT và LoRA thường ít hơn 1% cho tác vụ phụ 1, tác vụ phụ 2 thể hiện sự ưu tiên rõ ràng cho phương pháp phân loại FFT.

(2) Trong kịch bản huấn luyện 'Tiếng Anh + Bản dịch', adapters vượt trội hơn FFT qua tất cả các tác vụ phụ và cho thấy hiệu suất tốt hơn hoặc ngang bằng so với LoRA. Sự ưu tiên đặc biệt rõ ràng cho adapters có thể được quan sát cho tác vụ phụ 2, nơi đa số các ngôn ngữ đã thấy và chưa thấy hưởng lợi từ kỹ thuật huấn luyện này. Đối với tác vụ phụ 1 và 2, hầu hết các dự đoán zero-shot trên các ngôn ngữ chưa thấy ưu tiên FFT. Hiệu suất của phương pháp adapter đặc biệt nhất quán trên tiếng Anh, ngôn ngữ duy nhất được thấy trong kịch bản huấn luyện này, với tất cả các tác vụ phụ ưu tiên bộ phân loại adapter.

(3) Trong kịch bản huấn luyện 'Chỉ tiếng Anh' với ít dữ liệu hơn, sự khác biệt giữa FFT, LoRA và adapters trở nên ít rõ ràng hơn qua tất cả các tác vụ phụ. Trong thiết lập này, sự khác biệt về hiệu suất phân loại trung bình giữa FFT, adapters và LoRA qua tất cả các tác vụ phụ thường không đáng kể, với ít hơn 1% của một phương pháp so với phương pháp khác. Ví dụ, sự khác biệt giữa phương pháp adapter và LoRA cho tác vụ phụ 1 là 0.2%, và hiệu suất trung bình của FFT và LoRA giống nhau và chỉ khác nhau ở khoảng tin cậy. Tương tự, đối với tác vụ phụ 2, FFT có thể so sánh với phương pháp adapter, và đối với tác vụ phụ 3, FFT rất gần về hiệu suất với LoRA. Chúng tôi quan sát rằng đối với tác vụ phụ 1 và 2, adapters hoạt động tốt hơn cho EN (ngôn ngữ duy nhất được thấy), trong khi đối với tác vụ phụ 3, LoRA mang lại hiệu suất tốt hơn cho ngôn ngữ được thấy (EN).

(4) Hiệu suất tốt nhất tổng thể qua tất cả các kịch bản huấn luyện và kỹ thuật huấn luyện được đạt trong kịch bản huấn luyện 'Đa ngôn ngữ chung'. Trong khi phương pháp FFT hoạt động tốt hơn cho tác vụ phụ 1 và 2 trong thiết lập này, tác vụ phụ 3 cho thấy cải thiện rõ ràng khi được huấn luyện sử dụng phương pháp LoRA. Trong khi tác vụ phụ 1 và 2 nhất quán trong việc ưu tiên FFT cho cả ngôn ngữ đã thấy và chưa thấy (ES, EL, KA), tác vụ phụ 3 ưu tiên LoRA khi đưa ra dự đoán chỉ trên các ngôn ngữ đã thấy. Đối với các ngôn ngữ chưa thấy, tác vụ phụ 3 đồng ý với tác vụ phụ 1 và 2 trong việc ưu tiên phương pháp phân loại FFT.

Một số ngôn ngữ thể hiện sự ưu tiên mạnh mẽ hướng tới các kỹ thuật huấn luyện và kịch bản huấn luyện nhất định:

9 tháng 4, 2024 32/45

--- TRANG 33 ---
(1) Tiếng Anh nhất quán ưu tiên phương pháp phân loại adapter qua tất cả các kịch bản huấn luyện trong tác vụ phụ 1.

(2) Phương pháp FFT mang lại hiệu suất tốt nhất trên các dự đoán zero-shot của Georgia qua tất cả các thiết lập trong tác vụ phụ 1 và 3.

(3) Trong kịch bản huấn luyện 'Đa ngôn ngữ chung', tiếng Đức thể hiện sự ưu tiên nhất quán cho kỹ thuật huấn luyện adapter qua tất cả ba tác vụ phụ. Cụ thể, tiếng Đức là ngôn ngữ duy nhất trong tác vụ phụ 2 nơi FFT không tạo ra hiệu suất tốt nhất cho kịch bản huấn luyện 'Đa ngôn ngữ chung'. Nó cũng là trường hợp duy nhất nơi các mô hình adapter thể hiện hiệu suất tốt hơn so với LoRA cho các ngôn ngữ đã thấy cho tác vụ phụ 3 trong kịch bản huấn luyện chung.

Chúng tôi cũng quan sát rằng LoRA, trong kịch bản huấn luyện 'Đa ngôn ngữ chung', cho thấy hiệu suất tốt nhất tổng thể cho tác vụ phụ 3. Vì phương pháp này không được sử dụng bởi bất kỳ nhóm nào tham gia tác vụ chia sẻ, chúng tôi muốn xem xét cách phương pháp này so sánh với kết quả bảng xếp hạng chính thức sau cuộc thi. Bảng 10 thể hiện điểm của hệ thống chiến thắng cho tác vụ phụ 3 cùng với điểm đạt được trong tác vụ phụ 3 trong những thí nghiệm này. Như có thể thấy, phương pháp LoRA cho tác vụ phụ 3 vượt trội hơn hầu hết kết quả của các hệ thống chiến thắng. Chúng tôi đạt được tăng hiệu suất lên đến 19.63% cho tất cả các ngôn ngữ ngoại trừ Georgia (KA). 6 trong số 9 ngôn ngữ (FR, IT, PL, RU, ES và EL) đạt kết quả tốt nhất trong kịch bản huấn luyện 'Đa ngôn ngữ chung' khi áp dụng LoRA. Không ngạc nhiên, điểm tốt nhất cho tiếng Anh được đạt trong kịch bản một-đến-một 'Chỉ tiếng Anh'. Sự tăng này cũng dẫn đến việc xếp hạng đầu tiên trong 8 trong số 9 ngôn ngữ.

9 tháng 4, 2024 33/45

--- TRANG 34 ---
Bảng 10. So sánh bảng xếp hạng tập kiểm tra chính thức tác vụ phụ 3.

Ngôn ngữ | Nhóm hạng 1 | F1micro (Hạng 1) | F1micro (Của chúng tôi) | Tăng F1micro | Xếp hạng cuối cùng (Của chúng tôi)
---|---|---|---|---|---
EN | APatt | 0.37562 | 0.44937 | 19.63% | 1
FR | NAP | 0.46869 | 0.49238 | 5.05% | 1
DE | KInITVeraAI | 0.51304 | 0.54174 | 5.30% | 1
IT | KInITVeraAI | 0.55019 | 0.59919 | 8.91% | 1
PL | KInITVeraAI | 0.43037 | 0.44964 | 4.48% | 1
RU | KInITVeraAI | 0.38682 | 0.42635 | 10.22% | 1
ES | TeamAmpa | 0.38106 | 0.40674 | 6.74% | 1
EL | KInITVeraAI | 0.26733 | 0.27668 | 3.38% | 1
KA | KInITVeraAI | 0.45714 | 0.448 | -2.00% | 2

Để tóm tắt phân tích này, chúng tôi quan sát rằng các mô hình adapter và LoRA tạo ra kết quả có thể so sánh hoặc thậm chí tốt hơn cho tác vụ phụ 3 hoặc trong các kịch bản khi dữ liệu huấn luyện khan hiếm hoặc chỉ có sẵn trong một ngôn ngữ. Đây là một kết quả hứa hẹn cho rằng những phương pháp này yêu cầu ít bộ nhớ và thời gian huấn luyện hơn, như đã thảo luận trong phần trước đây.

Hạn chế và thảo luận

Kết quả được báo cáo trong nghiên cứu này bị hạn chế việc sử dụng một tập dữ liệu được chú thích với các tác vụ phân loại khác nhau. Phân tích thêm sẽ có lợi để kiểm tra những phát hiện này trên một tập đa dạng các corpus và tác vụ. Thêm vào đó, chúng tôi kiểm tra hiệu ứng của một kỹ thuật adapter, điều này ngăn chúng tôi tổng quát hóa những phát hiện của chúng tôi cho tất cả các loại phương pháp adapter. Cuối cùng, vì tác vụ phụ 1 và 2 được thực hiện ở mức bài báo nơi độ dài trung bình của bài báo hơn 512 token (Bảng 1), nhiều tín hiệu có mặt trong phần còn lại của các bài báo có thể bị bỏ qua bởi tất cả các phương pháp được so sánh trong nghiên cứu này.

Những phát hiện của chúng tôi cung cấp hiểu biết mới về hiệu quả của các kỹ thuật PEFT trong các tác vụ phân loại đa ngôn ngữ. Cụ thể, chúng tôi thấy rằng kịch bản huấn luyện đa ngôn ngữ trung bình cải thiện hiệu suất trên tất cả các tác vụ phụ. Tuy nhiên, các phương pháp hoạt động tốt nhất trong thiết lập này khác nhau tùy thuộc vào tác vụ phụ.

Kết quả của chúng tôi về phương pháp LoRA là mới vì kỹ thuật PEFT này không được điều tra trước đây trong các tác vụ đa ngôn ngữ. Trong khi chúng tôi quan sát rằng tác vụ phụ 1 và 2 hưởng lợi từ phương pháp FFT trong kịch bản huấn luyện 'Đa ngôn ngữ chung', kết quả của chúng tôi thể hiện hành vi đặc biệt thú vị của phương pháp LoRA trong kịch bản huấn luyện này cho tác vụ phụ 3, nơi nó nhất quán vượt trội hơn phương pháp FFT. Trong khi cần phân tích kỹ lưỡng để kết luận tại sao điều này xảy ra, sự khác biệt trong các thuộc tính của những tác vụ này mà chúng tôi trình bày trong Bảng 1 có thể mang lại ánh sáng cho câu hỏi này. Thứ nhất, tác vụ phụ 3 được huấn luyện trên và áp dụng cho các văn bản ngắn hơn nhiều so với tác vụ phụ 1 và 2, trong khi có nhiều ví dụ huấn luyện hơn đáng kể. Thêm vào đó, tác vụ phụ 3 được đặc trưng bởi số lượng lớp cao (23 so với 14 trong tác vụ phụ 2 và 3 trong tác vụ phụ 1) và mất cân bằng dữ liệu nghiêm trọng hơn nhiều, như được hiển thị trong Bảng 1. Bản chất của các tác vụ cũng khá khác nhau. Tác vụ phát hiện framing dựa ít vào kiến thức thường thức và ngữ dụng học, trong khi tác vụ phụ 1 và 3 chủ quan hơn đối với các chuyên gia con người. Tất cả những yếu tố này có thể ảnh hưởng đến hiệu suất của mỗi phương pháp trong các tác vụ phụ tương ứng, và cần nghiên cứu thêm để đưa ra kết luận về các loại tác vụ phân loại hưởng lợi từ phương pháp LoRA.

Chúng tôi quan sát phương pháp adapter chỉ vượt trội hơn FFT trong kịch bản xuyên ngôn ngữ zero-shot khi một lượng dữ liệu đủ được cung cấp trong ngôn ngữ nguồn (kịch bản 'Tiếng Anh + Bản dịch') trong khi kém hiệu quả hơn nhiều khi dữ liệu huấn luyện chứa nhiều ngôn ngữ khác nhau hoặc bị hạn chế và đơn ngôn ngữ. Đây là một hiểu biết quan trọng bổ sung cho các nghiên cứu trước đây báo cáo phương pháp adapter luôn vượt trội hơn FFT trong suy luận xuyên ngôn ngữ zero-shot [11,13]. Kết quả của chúng tôi cũng thách thức kết luận trước đây bởi Xenouleas et al. [14] cho thấy rằng các phương pháp adapter có lợi toàn cầu cho các tác vụ ít tài nguyên và xuyên ngôn ngữ. Thêm vào đó, kết quả của chúng tôi không đồng ý với các báo cáo trước đây rằng phương pháp FFT luôn bị vượt trội bởi phương pháp adapter trong suy luận xuyên ngôn ngữ zero-shot [11,13]. Ngược lại với điều này, chúng tôi thấy rằng đối với các ngôn ngữ 'chưa thấy' nhất quán qua tất cả các kịch bản huấn luyện ('ES', 'EL' và 'KA'), FFT vượt trội hơn phương pháp adapter trong tất cả các kịch bản huấn luyện cho tác vụ phụ 1 và tác vụ phụ 3, và cho hai trong ba kịch bản trong tác vụ phụ 2, 'Đa ngôn ngữ chung' và 'Chỉ tiếng Anh'. Cuối cùng, nghiên cứu của chúng tôi cung cấp so sánh đầu tiên theo hiểu biết của chúng tôi về kịch bản 'Đa ngôn ngữ chung' với kịch bản đơn ngôn ngữ dựa trên dịch về suy luận xuyên ngôn ngữ zero-shot.

Trong khi tác vụ phụ 3 hoạt động tốt hơn cho phương pháp LoRA trong thiết lập đa ngôn ngữ trung bình, điều này không nhất quán qua các ngôn ngữ đã thấy và zero-shot, với FFT hoạt động tốt hơn trong thiết lập xuyên ngôn ngữ zero-shot. Sự ưu tiên này đặc biệt mạnh trong các trường hợp khi lượng dữ liệu huấn luyện bị hạn chế. Điều này có thể chỉ ra thực tế rằng phương pháp adapter có thể không tốt trong tổng quát hóa xuyên ngôn ngữ, tuy nhiên, cần phân tích thêm trên nhiều tác vụ đa dạng hơn.

Trong các tác vụ với dữ liệu rất lệch, phương pháp adapter có thể dễ bị thiên về lớp thường xuyên nhất hơn so với FFT, vì có thể quan sát từ Bảng 6, điểm F1 micro cho tác vụ phụ 1 cao hơn đáng kể cho phương pháp adapter so với FFT, trong khi FFT dẫn đến F1 macro cao hơn. Tuy nhiên, phân tích lỗi chi tiết để xác nhận hoặc bác bỏ giả định này hiện tại không thể thực hiện được vì nhãn tiêu chuẩn vàng cho tập kiểm tra không được công bố.

Các ngôn ngữ ít tài nguyên có thể có sự ưu tiên cho FFT trong tất cả các kịch bản huấn luyện khi dự đoán được thực hiện theo cách xuyên ngôn ngữ zero-shot. Giả định này được đề xuất bởi thực tế rằng Georgia là ngôn ngữ duy nhất nhất quán ưu tiên phương pháp FFT qua tất cả các kịch bản huấn luyện và cho tất cả các tác vụ phụ.

Thú vị, hiệu suất trên Georgia trong tác vụ phụ 1 và 2 cao hơn so với các ngôn ngữ đã thấy, mặc dù là ít tài nguyên và zero-shot. Một lý do tiềm năng cho quan sát này có thể là thực tế rằng, như đã được chỉ ra trước đây trong Bảng 2 và 3, các văn bản trong tập kiểm tra cho Georgia, trung bình, nằm trong giới hạn của XLM-R và ngắn hơn nhiều so với độ dài đầu vào cho 8 ngôn ngữ khác. Điều này có thể giải thích tại sao cùng hiệu ứng không được quan sát cho tác vụ phụ 3, nơi tất cả các đầu vào đều nằm trong giới hạn token transformer và tương đối ngắn. Tuy nhiên, với việc thiếu nhãn tiêu chuẩn vàng cho các ngôn ngữ bất ngờ, không thể loại bỏ các lý do khác cho hiện tượng này, vì nó cũng có thể được giải thích với việc thiếu các lớp đặc biệt khó dự đoán trong tập kiểm tra cho Georgia cho cả hai tác vụ phụ.

9 tháng 4, 2024 36/45

--- TRANG 37 ---
Kết luận

Trong công trình này, chúng tôi thực hiện phân tích đầu tiên (theo hiểu biết của chúng tôi) về hiệu suất của kỹ thuật Low-Rank Adaptation (LoRA) và so sánh của nó với các phương pháp adapter và tinh chỉnh đầy đủ (FFT) trong kịch bản đa lớp đa ngôn ngữ.

Chúng tôi thấy rằng các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT), LoRA và bottleneck adapter, cung cấp hiệu quả tính toán đáng kể so với FFT về thời gian huấn luyện, số lượng tham số có thể huấn luyện và lượng bộ nhớ VRAM cần thiết. Cụ thể, chúng giảm số lượng tham số có thể huấn luyện giữa 140 và 280 lần và đạt được thời gian huấn luyện ngắn hơn giữa 32% và 44%.

So sánh giữa các phương pháp LoRA và adapter về hiệu quả tham số cho thấy rằng hiệu suất của chúng phụ thuộc vào tác vụ phụ nhất định và siêu tham số được sử dụng. Quan sát này phù hợp với kết quả của nghiên cứu trước đây bởi He et al. [11], người thấy lợi ích của phương pháp adapter phụ thuộc vào tác vụ. Trong khi chúng tôi quan sát LoRA hiệu quả hơn so với phương pháp adapter cho các tác vụ phát hiện thể loại và framing của bài báo tin tức, phương pháp adapter mất ít thời gian trung bình hơn và sử dụng ít tham số huấn luyện hơn cho cái sau.

Hơn nữa, chúng tôi thấy hiệu suất của các phương pháp phụ thuộc rất nhiều vào kịch bản huấn luyện. Phương pháp Adapter hoạt động tốt hơn so với LoRA và FFT trong kịch bản thiếu sự đa dạng ngôn ngữ trong tập huấn luyện qua các tác vụ phụ.

Sự khác biệt giữa tất cả ba phương pháp trở nên không đáng kể, thường ít hơn 1% trung bình, khi kích thước của dữ liệu huấn luyện giảm. Điều này chỉ ra rằng có thể đạt được hiệu quả tính toán cao bằng cách sử dụng các phương pháp PEFT mà không mất nhiều về hiệu suất phân loại trong thiết lập này. Nhiều thí nghiệm hơn về kết quả này liên quan đến việc giảm dần kích thước của tập huấn luyện sẽ có lợi trong tương lai để tìm ngưỡng khi hiệu suất qua các phương pháp khớp hoặc khi các phương pháp PEFT trở nên hiệu quả phân loại hơn.

Hiệu suất trên các ngôn ngữ chưa thấy thường phụ thuộc rất nhiều vào kịch bản huấn luyện. Chúng tôi thấy rằng FFT hoạt động tốt hơn so với các phương pháp PEFT trong dự đoán xuyên ngôn ngữ zero-shot khi được huấn luyện trên tập dữ liệu đa ngôn ngữ chung, điều này khác với kết quả được báo cáo bởi Chalkidis et al. [13]. Tuy nhiên, chúng tôi quan sát hiệu ứng được báo cáo bởi các tác giả trong kịch bản huấn luyện đơn ngôn ngữ, nơi phương pháp adapter hoạt động tốt hơn trên các ngôn ngữ zero-shot.

Cuối cùng, thiết lập LoRA đa ngôn ngữ chung cho phép chúng tôi cải thiện đáng kể kết quả chính thức của chúng tôi trên tác vụ phụ SemEval 2023 3 (phát hiện kỹ thuật thuyết phục) và vượt trội hơn hầu hết kết quả tốt nhất bảng xếp hạng chính thức, xếp hạng đầu tiên trong tất cả các ngôn ngữ ngoại trừ Georgia, nơi chúng tôi ở vị trí thứ hai so với kết quả bảng xếp hạng chính thức.

Thông tin hỗ trợ

S1 Appendix. Danh sách đầy đủ các danh mục và phân cấp của chúng cho tác vụ phụ 3.

S2 Appendix. Nghiên cứu ablation cho các kỹ thuật PEFT.

S3 Appendix. Lựa chọn kích thước mô hình cho XLM-RoBERTa.

S4 Appendix Lượng tài nguyên theo ngôn ngữ.

S5 Appendix So sánh adapters Houlsby và Pfeiffer qua ba tác vụ phụ.

Lời cảm ơn

Chúng tôi cảm ơn Joanna Wright đã cung cấp ý kiến và đề xuất của cô ấy về bản thảo của công trình này.

Tài liệu tham khảo

1. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong: North American Chapter of the Association for Computational Linguistics; 2019. Có sẵn từ: https://api.semanticscholar.org/CorpusID:52967399.

9 tháng 4, 2024 38/45

--- TRANG 39 ---
2. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research. 2020;21(1):5485–5551.

3. Ben Zaken E, Goldberg Y, Ravfogel S. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. Trong: Muresan S, Nakov P, Villavicencio A, editors. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Dublin, Ireland: Association for Computational Linguistics; 2022. p. 1–9. Có sẵn từ: https://aclanthology.org/2022.acl-short.1.

4. Jiang H, He P, Chen W, Liu X, Gao J, Zhao T. SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. Trong: Jurafsky D, Chai J, Schluter N, Tetreault J, editors. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics; 2020. p. 2177–2190. Có sẵn từ: https://aclanthology.org/2020.acl-main.197.

5. Xu R, Luo F, Zhang Z, Tan C, Chang B, Huang S, et al. Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Trong: Moens MF, Huang X, Specia L, Yih SWt, editors. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics; 2021. p. 9514–9528. Có sẵn từ: https://aclanthology.org/2021.emnlp-main.749.

6. Lialin V, Deshpande V, Rumshisky A. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:230315647. 2023;.

7. Li XL, Liang P. Prefix-Tuning: Optimizing Continuous Prompts for Generation. Trong: Zong C, Xia F, Li W, Navigli R, editors. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:

9 tháng 4, 2024 39/45

--- TRANG 40 ---
Long Papers). Online: Association for Computational Linguistics; 2021. p. 4582–4597. Có sẵn từ: https://aclanthology.org/2021.acl-long.353.

8. Lester B, Al-Rfou R, Constant N. The Power of Scale for Parameter-Efficient Prompt Tuning. Trong: Moens MF, Huang X, Specia L, Yih SWt, editors. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics; 2021. p. 3045–3059. Có sẵn từ: https://aclanthology.org/2021.emnlp-main.243.

9. Bapna A, Firat O. Simple, Scalable Adaptation for Neural Machine Translation. Trong: Inui K, Jiang J, Ng V, Wan X, editors. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics; 2019. p. 1538–1548. Có sẵn từ: https://aclanthology.org/D19-1165.

10. Houlsby N, Giurgiu A, Jastrzebski S, Morrone B, De Laroussilhe Q, Gesmundo A, et al. Parameter-Efficient Transfer Learning for NLP. Trong: Chaudhuri K, Salakhutdinov R, editors. Proceedings of the 36th International Conference on Machine Learning. vol. 97 of Proceedings of Machine Learning Research. PMLR; 2019. p. 2790–2799. Có sẵn từ: https://proceedings.mlr.press/v97/houlsby19a.html.

11. He R, Liu L, Ye H, Tan Q, Ding B, Cheng L, et al. On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation. Trong: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Online: Association for Computational Linguistics; 2021. p. 2208–2222. Có sẵn từ: https://aclanthology.org/2021.acl-long.172.

12. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al. LoRA: Low-Rank Adaptation of Large Language Models. Trong: International Conference on

9 tháng 4, 2024 40/45

--- TRANG 41 ---
Learning Representations; 2022. Có sẵn từ: https://openreview.net/forum?id=nZeVKeeFYf9.

13. Chalkidis I, Fergadiotis M, Androutsopoulos I. MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. Trong: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics; 2021. p. 6974–6996. Có sẵn từ: https://aclanthology.org/2021.emnlp-main.559.

14. Xenouleas S, Tsoukara A, Panagiotakis G, Chalkidis I, Androutsopoulos I. Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classification. Trong: Proceedings of the 12th Hellenic Conference on Artificial Intelligence. SETN '22. New York, NY, USA: Association for Computing Machinery; 2022. Có sẵn từ: https://doi.org/10.1145/3549737.3549760.

15. Piskorski J, Stefanovitch N, Da San Martino G, Nakov P. SemEval-2023 Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup. Trong: Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). Toronto, Canada: Association for Computational Linguistics; 2023. p. 2343–2361. Có sẵn từ: https://aclanthology.org/2023.semeval-1.317.

16. Wu B, Razuvayevskaya O, Heppell F, Leite JA, Scarton C, Bontcheva K, et al. SheffieldVeraAI at SemEval-2023 Task 3: Mono and Multilingual Approaches for News Genre, Topic and Persuasion Technique Classification. Trong: Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). Toronto, Canada: Association for Computational Linguistics; 2023. p. 1995–2008. Có sẵn từ: https://aclanthology.org/2023.semeval-1.275.

17. Hromadka T, Smolen T, Remis T, Pecher B, Srba I. KInITVeraAI at SemEval-2023 Task 3: Simple yet Powerful Multilingual Fine-Tuning for Persuasion Techniques Detection. Trong: Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). Toronto, Canada:

9 tháng 4, 2024 41/45

--- TRANG 42 ---
Association for Computational Linguistics; 2023. p. 629–637. Có sẵn từ: https://aclanthology.org/2023.semeval-1.86.

18. Gururangan S, Marasović A, Swayamdipta S, Lo K, Beltagy I, Downey D, et al. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks. Trong: Jurafsky D, Chai J, Schluter N, Tetreault J, editors. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics; 2020. p. 8342–8360. Có sẵn từ: https://aclanthology.org/2020.acl-main.740.

19. Pfeiffer J, Vulić I, Gurevych I, Ruder S. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Trong: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics; 2020. p. 7654–7673. Có sẵn từ: https://aclanthology.org/2020.emnlp-main.617.

20. Dettmers T, Pagnoni A, Holtzman A, Zettlemoyer L. QLoRA: Efficient finetuning of quantized llms. arXiv preprint arXiv:230514314. 2023;.

21. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Trong: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels, Belgium: Association for Computational Linguistics; 2018. p. 353–355. Có sẵn từ: https://aclanthology.org/W18-5446.

22. Yu X, Chatterjee T, Asai A, Hu J, Choi E. Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources. Trong: Goldberg Y, Kozareva Z, Zhang Y, editors. Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics; 2022. p. 3725–3743. Có sẵn từ: https://aclanthology.org/2022.findings-emnlp.273.

23. Kiesel J, Mestre M, Shukla R, Vincent E, Adineh P, Corney D, et al. SemEval-2019 Task 4: Hyperpartisan News Detection. Trong: Proceedings of the 13th International Workshop on Semantic Evaluation. Minneapolis, Minnesota,

9 tháng 4, 2024 42/45

--- TRANG 43 ---
USA: Association for Computational Linguistics; 2019. p. 829–839. Có sẵn từ: https://aclanthology.org/S19-2145.

24. Abu Farha I, Oprea SV, Wilson S, Magdy W. SemEval-2022 Task 6: iSarcasmEval, Intended Sarcasm Detection in English and Arabic. Trong: Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022). Seattle, United States: Association for Computational Linguistics; 2022. p. 802–814. Có sẵn từ: https://aclanthology.org/2022.semeval-1.111.

25. Da San Martino G, Barrón-Cedeño A, Wachsmuth H, Petrov R, Nakov P. SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles. Trong: Proceedings of the Fourteenth Workshop on Semantic Evaluation. Barcelona (online): International Committee for Computational Linguistics; 2020. p. 1377–1414. Có sẵn từ: https://aclanthology.org/2020.semeval-1.186.

26. Dimitrov D, Bin Ali B, Shaar S, Alam F, Silvestri F, Firooz H, et al. SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images. Trong: Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021). Online: Association for Computational Linguistics; 2021. p. 70–98. Có sẵn từ: https://aclanthology.org/2021.semeval-1.7.

27. Card D, Boydstun AE, Gross JH, Resnik P, Smith NA. The Media Frames Corpus: Annotations of Frames Across Issues. Trong: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Beijing, China: Association for Computational Linguistics; 2015. p. 438–444. Có sẵn từ: https://aclanthology.org/P15-2072.

28. Da San Martino G, Yu S, Barrón-Cedeño A, Petrov R, Nakov P. Fine-Grained Analysis of Propaganda in News Article. Trong: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics; 2019. p. 5636–5646. Có sẵn từ: https://aclanthology.org/D19-1565.

9 tháng 4, 2024 43/45

--- TRANG 44 ---
29. Billert F, Conrad S. HHU at SemEval-2023 Task 3: An Adapter-based Approach for News Genre Classification. Trong: Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). Toronto, Canada: Association for Computational Linguistics; 2023. p. 1166–1171. Có sẵn từ: https://aclanthology.org/2023.semeval-1.162.

30. Falk N, Eichel A, Piccirilli P. NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as Data Augmentation Strategies for Detecting Persuasion Techniques. Trong: Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). Toronto, Canada: Association for Computational Linguistics; 2023. p. 1433–1446. Có sẵn từ: https://aclanthology.org/2023.semeval-1.198.

31. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Computing Research Repository. 2019;arXiv:1907.11692. doi:10.48550/ARXIV.1907.11692.

32. Piskorski J, Stefanovitch N, Bausier VA, Faggiani N, Linge J, Kharazi S, et al. News Categorization, Framing and Persuasion Techniques: Annotation Guidelines. European Commission, Ispra, JRC132862; 2023.

33. Barbaresi A. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. Trong: Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. Association for Computational Linguistics; 2021. p. 122–131. Có sẵn từ: https://aclanthology.org/2021.acl-demo.15.

34. Pikuliak M, Šimko M, Bieliková M. Cross-lingual learning for text processing: A survey. Expert Systems with Applications. 2021;165:113765. doi:https://doi.org/10.1016/j.eswa.2020.113765.

35. Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzmán F, et al. Unsupervised Cross-lingual Representation Learning at Scale. Trong: Proceedings of the 58th Annual Meeting of the Association for Computational

9 tháng 4, 2024 44/45

--- TRANG 45 ---
Linguistics. Online: Association for Computational Linguistics; 2020. p. 8440–8451. Có sẵn từ: https://aclanthology.org/2020.acl-main.747.

36. Pfeiffer J, Rückié A, Poth C, Kamath A, Vulić I, Ruder S, et al. AdapterHub: A Framework for Adapting Transformers. Trong: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics; 2020. p. 46–54. Có sẵn từ: https://aclanthology.org/2020.emnlp-demos.7.

37. Pires T, Schlinger E, Garrette D. How Multilingual is Multilingual BERT? Trong: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics; 2019. p. 4996–5001. Có sẵn từ: https://aclanthology.org/P19-1493.

38. Savenkov K, Lopez M. The State of the Machine Translation 2022. Trong: Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track). Orlando, USA: Association for Machine Translation in the Americas; 2022. p. 32–49. Có sẵn từ: https://aclanthology.org/2022.amta-upg.4.

39. Wang Z, Lipton ZC, Tsvetkov Y. On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment. Trong: Webber B, Cohn T, He Y, Liu Y, editors. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics; 2020. p. 4438–4450. Có sẵn từ: https://aclanthology.org/2020.emnlp-main.359.

9 tháng 4, 2024 45/45

--- TRANG 46 ---
S1 Appendix - Phân loại danh mục tác vụ phụ 3

Mô tả của mỗi danh mục được cung cấp trong bài báo tác vụ gốc [15].

• Tấn công danh tiếng
  – Gọi tên hoặc Gán nhãn  
  – Có tội bằng liên kết
  – Gieo nghi ngờ
  – Kêu gọi đạo đức giả
  – Đặt câu hỏi về danh tiếng

• Biện minh
  – Vẫy cờ
  – Kêu gọi thẩm quyền
  – Kêu gọi sự phổ biến
  – Kêu gọi giá trị
  – Kêu gọi nỗi sợ, định kiến

• Chuyển hướng
  – Đối thủ rơm
  – Con cá trích đỏ
  – Whataboutism

• Đơn giản hóa
  – Đơn giản hóa quan hệ nhân quả
  – Lưỡng nan giả hoặc Không có lựa chọn
  – Đơn giản hóa hậu quả

• Kêu gọi
  – Khẩu hiệu
  – Kẻ giết cuộc trò chuyện

9 tháng 4, 2024 a/g

--- TRANG 47 ---
  – Kêu gọi thời gian

• Từ ngữ thao túng
  – Ngôn ngữ mang tính
  – Làm mơ hồ, Mơ hồ có chủ ý, Gây nhầm lẫn
  – Phóng đại hoặc Thu nhỏ
  – Lặp lại

9 tháng 4, 2024 b/g

--- TRANG 48 ---
S2 Appendix - nghiên cứu ablation cho các kỹ thuật PEFT

Trong phần này, chúng tôi thực hiện nghiên cứu ablation để tìm hiểu cách mỗi kỹ thuật PEFT trong ba kỹ thuật hoạt động trong các kịch bản đa ngôn ngữ, xuyên ngôn ngữ và đơn ngôn ngữ trên tác vụ phụ 1. Như có thể thấy, trong khi LoRA và Adapter thường thể hiện hành vi tương đương trong tất cả các kịch bản huấn luyện, phương pháp BitFit đạt kết quả thấp hơn đáng kể. Quan trọng, trong khi kết quả trong thiết lập đa ngôn ngữ có thể so sánh, sự khác biệt giữa BitFit và hai PEFT khác đặc biệt đáng chú ý cho hai thiết lập xuyên ngôn ngữ ('Tiếng Anh + Bản dịch' và 'Chỉ tiếng Anh').

Bảng S2. Tác vụ phụ 1: Phát hiện thể loại - Điểm F1 macro trung bình ±1 STD cho ba kỹ thuật PEFT trong 3 kịch bản huấn luyện.

[Bảng chi tiết với dữ liệu cho BitFit, LoRA và Adapter qua các kịch bản huấn luyện khác nhau]

9 tháng 4, 2024 c/g

--- TRANG 49 ---
S3 Appendix - Lựa chọn kích thước mô hình cho XLM-RoBERTa

Kết quả so sánh giữa các kích thước khác nhau của mô hình RoBERTa được cung cấp trong các Bảng S3.1, S3.2, S3.3. Như có thể thấy, đối với tất cả các kỹ thuật huấn luyện, kết quả trung bình nhất quán cao hơn đáng kể đối với kích thước lớn của mô hình cho tất cả ba tác vụ phụ.

[Các bảng S3.1, S3.2, S3.3 so sánh Base và Large sizes cho FFT, LoRA và Adapter methods]

9 tháng 4, 2024 d/g

--- TRANG 50 ---
[Tiếp tục các bảng so sánh]

9 tháng 4, 2024 e/g

--- TRANG 51 ---
S4 Appendix - Lượng tài nguyên theo ngôn ngữ

Bảng S3 cho thấy lượng tài nguyên chúng tôi xem xét khi xác định các ngôn ngữ ít tài nguyên cho tác vụ của chúng tôi. Đối với lượng dữ liệu huấn luyện trước, chúng tôi tóm tắt thống kê từ [35]. Chúng tôi cũng xem xét sự tồn tại của dữ liệu để tinh chỉnh trong một ngôn ngữ nhất định hoặc liệu dữ liệu huấn luyện có sẵn trong một ngôn ngữ từ cùng nhóm hay không (cột có tên "Language family"). Dựa trên tiêu chí của chúng tôi, ngôn ngữ Georgia là một ngoại lệ rõ ràng với dữ liệu huấn luyện trước đáng kể ít hơn và không có dữ liệu huấn luyện trong ngôn ngữ đó.

Bảng S4. So sánh lượng tài nguyên cho mỗi ngôn ngữ trong tác vụ phụ 1, 2, và 3

[Bảng chi tiết về dữ liệu huấn luyện trước, dữ liệu huấn luyện và họ ngôn ngữ]

9 tháng 4, 2024 f/g

--- TRANG 52 ---
S5 Appendix - So sánh adapters Houlsby và Pfeiffer qua ba tác vụ phụ

Bảng S4 cho thấy so sánh cấu hình Housby và Pfeiffer của adapter cho mỗi tác vụ phụ trong các thí nghiệm của chúng tôi trong kịch bản huấn luyện 'Đa ngôn ngữ chung' cho phương pháp FFT. Như có thể thấy, sự khác biệt không nhất quán qua tất cả các ngôn ngữ, nhưng trung bình, mỗi tác vụ phụ hưởng lợi từ thiết lập adapter Pfeiffer.

Bảng S5. So sánh adapters Houlsby và Pfeiffer cho XLM-RoBERTa Large trong kịch bản 'Đa ngôn ngữ chung'

[Bảng chi tiết so sánh Houlsby và Pfeiffer cho các tác vụ phụ khác nhau]

9 tháng 4, 2024 g/g
