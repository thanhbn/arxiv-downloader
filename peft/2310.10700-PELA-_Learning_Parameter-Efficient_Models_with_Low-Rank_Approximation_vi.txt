# 2310.10700.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2310.10700.pdf
# Kích thước tệp: 997331 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PELA: Học các Mô hình Tiết kiệm Tham số với Xấp xỉ Hạng Thấp
Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli
Đại học Quốc gia Singapore
Tóm tắt
Việc áp dụng mô hình lớn đã được đào tạo trước cho các tác vụ hạ lưu là không khả thi trong điều kiện tài nguyên hạn chế. Các phương pháp chiếm ưu thế gần đây để giải quyết vấn đề hiệu quả liên quan đến việc thêm một số tham số có thể học được vào mô hình backbone cố định. Tuy nhiên, chiến lược này dẫn đến nhiều thách thức hơn trong việc tải các mô hình lớn để tinh chỉnh hạ lưu với tài nguyên hạn chế. Trong bài báo này, chúng tôi đề xuất một phương pháp mới để tăng hiệu quả tham số của các mô hình đã được đào tạo trước bằng cách giới thiệu một giai đoạn đào tạo trước trung gian. Để làm điều này, đầu tiên chúng tôi sử dụng xấp xỉ hạng thấp để nén mô hình lớn ban đầu và sau đó thiết kế một mô-đun chưng cất đặc trưng và một mô-đun chính quy hóa nhiễu trọng số. Các mô-đun này được thiết kế đặc biệt để cải thiện mô hình hạng thấp. Cụ thể, chúng tôi chỉ cập nhật mô hình hạng thấp trong khi cố định các tham số backbone trong quá trình đào tạo trước. Điều này cho phép sử dụng trực tiếp và hiệu quả mô hình hạng thấp cho các tác vụ tinh chỉnh hạ lưu. Phương pháp được đề xuất đạt được cả hiệu quả về tham số cần thiết và thời gian tính toán trong khi duy trì kết quả tương đương với những thay đổi tối thiểu đối với kiến trúc backbone. Cụ thể, khi được áp dụng cho ba mô hình Transformer chỉ dành cho thị giác và một mô hình Transformer thị giác-ngôn ngữ, phương pháp của chúng tôi thường cho thấy chỉ giảm khoảng ~0.6 điểm hiệu suất trong khi giảm kích thước tham số ban đầu từ 1/3 đến 2/3. Mã nguồn đã được công bố tại liên kết.

1. Giới thiệu
Việc đào tạo trước một mô hình lớn và tinh chỉnh nó cho từng tác vụ cụ thể đã trở thành một mô hình de facto trong nhiều lĩnh vực nghiên cứu đa dạng [9, 10, 59]. Mặc dù đã đạt được hiệu suất đáng kể, việc xây dựng các mô hình như vậy thường phải đánh đổi bằng việc tăng sử dụng bộ nhớ và thời gian đào tạo dài hơn. Bất chấp những thách thức này, những tiến bộ gần đây trong việc đánh giá luật quy mô [27] và khả năng phát sinh [61] của việc đào tạo trước ngôn ngữ đã thúc đẩy thêm sự quan tâm của các nhà thực hành trong việc phát triển và sử dụng các mô hình lớn.

Vì việc triển khai các mô hình này cho các tác vụ hạ lưu thường không khả thi, các nghiên cứu gần đây đã chuyển sang tránh việc tinh chỉnh toàn bộ mô hình.

Trực giác Nghiên cứu
34
…×𝑁𝑁
×𝑁𝑁 Các lớp Transformer
PELA
Nén
Hình 1. Tổng quan và hiệu suất của phương pháp PELA được đề xuất của chúng tôi. Trái: Sử dụng PELA, chúng tôi nén các trọng số có thể huấn luyện của mô hình ViT điển hình trong khi bảo toàn kiến trúc tổng thể của nó. Phải: So sánh mô hình Gốc và PELA của chúng tôi về kích thước mô hình tương đối và chỉ số độ chính xác trên ba bộ Transformer đã được đào tạo trước.

Các phương pháp điển hình thường giới thiệu thêm một số tham số có thể học được vào mô hình backbone trong khi cố định phần còn lại, ví dụ: adapter [51] và prompt tuning [24] thêm các tham số có thể điều chỉnh vào các vị trí token ở giữa và ngoại vi của Transformer tương ứng. Tuy nhiên, phương pháp này không tránh khỏi dẫn đến hai nhược điểm sau. Thứ nhất, tiềm năng của các mô hình lớn đã được đào tạo trước không được khai thác đầy đủ vì phần lớn các tham số không được điều chỉnh với các mục tiêu tác vụ hạ lưu. Thứ hai, việc tải mô hình đã được đào tạo trước trở nên gánh nặng hơn nữa đối với các nhà nghiên cứu có tài nguyên hạn chế. Ngược lại, các phương pháp thông thường như chưng cất kiến thức (KD) [19, 20, 50] và lượng tử hóa [7, 22] có thể giảm bớt một phần vấn đề này. Tuy nhiên, hiện tại không có phương pháp được thiết lập để xây dựng một mô hình sinh viên có hiệu suất cao của KD và các toán tử không khả vi của lượng tử hóa thường làm cho việc thực hiện lan truyền ngược ít khả thi hơn.

Bài báo này nhắm đến việc phát triển một phương pháp có hiệu quả tham số cao để giúp tinh chỉnh tác vụ hạ lưu, như minh họa trong Hình 1. Bằng hiệu quả tham số, chúng tôi đề cập đến một mô hình nén với kích thước giảm (ví dụ: nhỏ hơn 2×), dễ triển khai, hiệu quả về tính toán, và có những ưu điểm thay đổi kiến trúc tối thiểu. Phương pháp của chúng tôi cung cấp một mô hình nén đã được đào tạo trước mà các tác vụ hạ lưu có thể thực hiện tinh chỉnh trực tiếp trên đó, trái ngược với các phương pháp trước đây như LoRA [21], adapter [51], và prompt tuning [24]. Cụ thể, phương pháp này được thiết kế đặc biệt để giải quyết vấn đề quá tham số hóa [1], trong đó chúng tôi sử dụng xấp xỉ hạng thấp để thay thế ma trận trọng số đã được đào tạo trước trong mỗi phép toán nhân ma trận bằng hai ma trận hạng thấp. Bằng cách này, cả kích thước mô hình ban đầu và thời gian tinh chỉnh đều được giảm đáng kể. Tuy nhiên, việc sử dụng phương pháp ngây thơ này để thực hiện tinh chỉnh cho kết quả kém hài lòng hơn (tham khảo Phần 4.4). Chúng tôi cho rằng điều này là do hai lý do: Việc phân tách trực tiếp với xấp xỉ hạng thấp không thể học hiệu quả các biểu diễn phân biệt ở cấp độ thể hiện; và phân phối đặc trưng trung gian bị nhiễu sau thao tác này, dẫn đến hiệu suất không tối ưu.

Để tiếp cận vấn đề này, chúng tôi đề xuất tận dụng hoàn toàn mô hình đã được đào tạo trước thông qua hai mô-đun. Việc triển khai bao gồm hai nhánh mô hình song song: một bao gồm mô hình đã được đào tạo trước với các tham số cố định trong quá trình đào tạo trước, trong khi nhánh kia là mô hình hạng thấp với các tham số có thể điều chỉnh. Dựa trên khung này, mô-đun đầu tiên của chúng tôi chưng cất kiến thức đặc trưng từ mô hình lớn đã được đào tạo trước sang mô hình hạng thấp nén của chúng tôi về mỗi lớp Transformer. Mô-đun khác giúp ràng buộc thay đổi trọng số trong một bán kính nhiễu được xác định trước. Hai mô-đun này giúp mô hình hạng thấp bắt chước phân phối đặc trưng của mô hình lớn đã được đào tạo trước, từ đó tăng cường khả năng phân biệt của nó. Trong quá trình tinh chỉnh, chúng tôi đơn giản sử dụng mô hình hạng thấp như một sự thay thế cho mô hình lớn ban đầu để đạt được hiệu quả tham số và tính toán cho các tác vụ hạ lưu.

Theo như chúng tôi biết, tài liệu về việc đạt được sự đánh đổi hiệu quả-hiệu suất mong muốn bằng cách sử dụng xấp xỉ hạng thấp trên các trọng số Transformer đã được đào tạo trước là khá hạn chế. Chúng tôi áp dụng phương pháp của mình cho ba Transformer chỉ dành cho thị giác tức là DeiT [54], DeiT-III-Large [56] và SwinT [39], với 1/2 đến 2/3 tham số của mô hình ban đầu; và một Transformer thị giác-ngôn ngữ - ALBEF [33] trong đó kích thước tham số được giảm xuống còn 1/3 của mô hình ban đầu. Sau đó, chúng tôi tiến hành các thí nghiệm rộng rãi trên một loạt các tác vụ hạ lưu, bao gồm phân loại hình ảnh, phân đoạn ngữ nghĩa, và phát hiện đối tượng cho các Transformer chỉ dành cho thị giác; Suy luận thị giác, định vị thị giác, truy xuất đa phương thức, và trả lời câu hỏi thị giác cho Transformer thị giác-ngôn ngữ. Phương pháp của chúng tôi đạt được hiệu suất có tính so sánh cao với mô hình backbone, với sự khác biệt chủ yếu khoảng 0.6 điểm, mặc dù chỉ sử dụng 1/3 đến 2/3 FLOPs ban đầu. Ngoài ra, lợi ích hiệu quả tham số này còn cho phép mô hình mở rộng với kích thước batch lớn hơn, dẫn đến hiệu suất cải thiện đôi khi thậm chí vượt trội hơn các backbone.

2. Công trình Liên quan
2.1. Học Tiết kiệm Tham số
Hiệu quả từ lâu đã là một vấn đề hấp dẫn trong nhiều lĩnh vực nghiên cứu [26, 49]. Sau khi bước vào kỷ nguyên học biểu diễn sâu, những cải tiến tiến bộ trong cộng đồng của chúng ta thường phải đánh đổi với số lượng lớn tham số mô hình, độ trễ và dấu chân [41, 58]. Với mối quan tâm này, các nỗ lực trước đây chủ yếu dành cho ba hướng phân biệt: chưng cất kiến thức (KD), lượng tử hóa, và cắt tỉa. Được coi là thuật toán nén mô hình có nguyên tắc, KD sớm nhằm chuyển giao kiến thức từ một mô hình giáo viên cồng kềnh sang một mô hình sinh viên nhẹ thông qua căn chỉnh logit lớp [20, 46]. Trọng tâm gần đây đã chuyển sang chuyển giao kiến thức dựa trên đặc trưng do lợi thế hiệu suất của nó so với những phương pháp dựa trên logit thông thường [19, 25, 45, 69]. Ví dụ, [25, 50] chưng cất kiến thức từ các trạng thái ẩn và ma trận chú ý, mà mặt khác, cũng có thể bỏ qua các mục tiêu đào tạo không có logit. Tuy nhiên, việc chọn đặc trưng từ những lớp nào để căn chỉnh vẫn còn thách thức vì không có sự khớp lớp giáo viên-sinh viên từ cơ sở lý thuyết. Lượng tử hóa, từ một góc độ khác của học hiệu quả, ánh xạ các tham số bit lớn hơn thành những tham số nhỏ hơn, ví dụ: số thực dấu phẩy động 32-bit thành số nguyên 8-bit [44]. Loại phương pháp này không phụ thuộc vào cấu trúc mô hình, điều này làm cho nó linh hoạt trong các mạng neural khác nhau [7, 22, 36]. Nhược điểm chính nằm ở việc giảm hiệu suất và khả năng không khả thi cho lan truyền ngược. Khác với hai loại trên, cắt tỉa được tận dụng để loại bỏ các thành phần không cần thiết hoặc ít quan trọng trong các mô hình [58]. Bằng cách loại bỏ một số kết nối [65] hoặc tham số [29], mạng dày đặc ban đầu giảm thành một mạng thưa thớt, trong đó dung lượng cần thiết để lưu trữ cũng như lượng tính toán sẽ giảm đi.

Các phương pháp dựa trên Transformer đã thành công trong các lĩnh vực nghiên cứu đa dạng kể từ khi được giới thiệu [9, 57]. Những mô hình này thường liên quan đến hàng tỷ tham số, do đó, thúc đẩy một số phương pháp cụ thể làm việc để giải quyết vấn đề hiệu quả tham số [30]. Chiến lược điển hình là thêm một số tham số có thể học được trong khi cố định phần lớn backbone Transformer trong quá trình đào tạo hạ lưu. Ví dụ, prompt tuning thêm một số tham số cụ thể cho tác vụ vào không gian đầu vào [24]; các mô hình Adapter giới thiệu một số thành phần MLP có thể học được vào mỗi lớp Transformer [51]; và việc tinh chỉnh chỉ bias cũng đã được chứng minh hiệu quả để duy trì hiệu suất tốt của các mô hình ngôn ngữ lớn [72].

2.2. Xấp xỉ Hạng Thấp
Xấp xỉ hạng thấp nhằm phân tách một ma trận thành hai ma trận nhỏ hơn, tuân theo ràng buộc rằng các ma trận kết quả có hạng giảm [47, 48]. Một ưu điểm chính của thuật toán này là nén dữ liệu, theo đó công việc trước đây đã áp dụng nó cho phân tích thành phần chính [42] và đề xuất [13, 18].

Liên quan đến Mạng Neural Tích chập (CNN), một số phương pháp áp dụng xấp xỉ hạng thấp cho mỗi bản đồ đặc trưng thông qua phân tách tensor bậc cao hơn [12, 52, 73]. Việc phân tách động các ma trận có thể huấn luyện cũng đã thu hút nhiều sự chú ý [66, 68, 70]. Một số nghiên cứu khác đã khám phá các khía cạnh khác của xấp xỉ hạng thấp, như học hạng [23], tối ưu hóa ràng buộc [32], và sử dụng nó cụ thể trong ma trận nhúng token [5, 30] hoặc tính toán tự chú ý trong Transformer [60]. LoRA [21] mô hình hóa phần dư của các tham số với xấp xỉ hạng thấp, trong đó chỉ các ma trận được phân tách mới được khai thác để đào tạo hạ lưu và do đó nó đạt được các tham số có thể huấn luyện giảm đáng kể. Mặc dù có lợi ích, phương pháp LoRA vẫn có hạn chế, vì nó đòi hỏi việc lưu trữ và tải lại các trọng số đã được đào tạo trước lớn trong đĩa cứng và bộ nhớ GPU tương ứng. Nói cách khác, chỉ các tham số có thể huấn luyện mới được giới thiệu có cường độ nhỏ hơn so với các tham số đầy đủ được cập nhật cho tinh chỉnh, làm cho nó tương tự như adapter [14, 51] và prompt tuning [24]. Không giống như các phương pháp hiện có, phương pháp của chúng tôi sử dụng xấp xỉ hạng thấp trong quá trình đào tạo trước để hoàn toàn thay thế các trọng số đã được đào tạo trước bằng các ma trận hạng thấp giảm. Kết quả là, chúng tôi đạt được cả mục tiêu hiệu quả bộ nhớ và tính toán cho các tác vụ tinh chỉnh hạ lưu.

3. Phương pháp
Transformer đã phát triển thành một khối xây dựng cơ bản của nhiều mô hình thị giác hiện đại [10, 17]. Lấy Vision Transformer (ViT) tinh túy làm ví dụ. ViT đầu tiên chia một hình ảnh RGB I∈R³×H×W thành M×M patch không chồng lấp. Cùng với một token lớp, các patch hình ảnh này sau đó được đưa vào N lớp với tự chú ý như hoạt động cơ bản. Để làm điều này, một tập hợp các ma trận truy vấn, khóa và giá trị được biến đổi từ nhúng patch thành các đặc trưng token X∈R^(M²+1)×d, trong đó d biểu thị kích thước nhúng, theo sau là một số lớp feedforward và kết nối dư. Cốt lõi của chúng nằm ở lớp được kết nối đầy đủ, thường được bao bọc trong việc ước tính điểm chú ý và các hoạt động MLP - W^T X+b, trong đó W∈R^(din×dout) là ma trận trọng số có thể học và b∈R^dout biểu thị bias, và din=d cho lớp đầu tiên.

3.1. Xấp xỉ Hạng Thấp
Quá tham số hóa là một vấn đề phổ biến trong các mô hình lớn hiện đại [1]. Trong công việc này, chúng tôi nhằm giải quyết vấn đề này bằng cách giảm số lượng tham số mô hình. Được truyền cảm hứng từ sự thành công của xấp xỉ hạng thấp trong các lĩnh vực khác [12, 52], chúng tôi đề xuất áp dụng kỹ thuật này trực tiếp cho các phép toán nhân ma trận trong ViT,

Baseline Low-Rank PELA 4050607080 Accuracy (%)
Thay đổi Hiệu suất
0.0 0.5 1.0 0123 Baseline
0.0 0.5 1.0 0123 Low-Rank

Hình 2. So sánh hiệu suất của ba mô hình và thống kê tương tự đặc trưng cấp độ thể hiện. Trái: Chúng tôi sử dụng mô hình DeiT làm baseline và hiển thị hiệu suất của các biến thể xấp xỉ hạng thấp trực tiếp và PELA của nó. Các hình con giữa và phải minh họa tương tự đặc trưng cấp độ thể hiện của các biến thể mô hình DeiT và hạng thấp trực tiếp tương ứng.

W^T X ≈ (UV^T)^T X
= V(U^T X), (1)

trong đó U∈R^(din×dlr) và V∈R^(dout×dlr) là các ma trận hạng thấp, và dlr đại diện cho hạng mong muốn của W. Lưu ý rằng các ma trận trọng số trong một mô hình học sâu thường có hạng đầy đủ, tức là rank(W) = min(din, dout). Trong điều kiện như vậy, chúng tôi tìm cách xấp xỉ bằng ma trận ban đầu và cố ý chọn dlr nhỏ hơn, ví dụ: 1/4 min(din, dout). Phương trình thứ hai luôn đúng trong các mạng neural do luật kết hợp tự nhiên. Tính chất này cho phép chúng tôi đạt được hiệu quả tính toán mà không cần khôi phục ma trận trọng số ban đầu W sau khi áp dụng xấp xỉ hạng thấp. Chúng tôi sử dụng phương pháp SVD nổi tiếng [31] để thực hiện xấp xỉ hạng thấp như sau:

SVD(W^T) = U*ΣV*, (2)

trong đó Σ∈R^(din×dout) là ma trận chéo hình chữ nhật với các số thực không âm trên đường chéo, và các giá trị kỳ dị được sắp xếp theo thứ tự giảm dần đơn điệu; U*∈R^(din×din) và V*∈R^(dout×dout) là các ma trận unitary phức. Sau đó chúng tôi chính thức hóa các ma trận hạng thấp bằng cách sử dụng phép biến đổi sau:

U = U*[:,:dlr]Σ^(1/2)[:dlr,:dlr],
V = (Σ^(1/2)[:dlr,:dlr]V*[:dlr,:])^T, (3)

trong đó [:,:dlr] có nghĩa là chúng tôi cắt bớt ma trận đã cho với dlr cột hàng đầu và các thao tác cắt bớt khác cũng có thể được suy ra dễ dàng.

Quan sát sơ bộ. Chúng tôi áp dụng xấp xỉ hạng thấp này cho các lớp được kết nối đầy đủ của các mô hình đã được đào tạo trước. Thật không may, quá trình này mang lại kết quả kém mong muốn hơn, ví dụ: độ chính xác giảm từ 81% xuống 61% như thấy trong Hình 2. Điều này chỉ ra rằng mô hình hạng thấp nén không học hiệu quả biểu diễn phân biệt cấp độ thể hiện. Hơn nữa, chúng tôi thấy rằng các đặc trưng được học sau hạng thấp bị giới hạn trong một không gian đặc trưng hẹp. Cụ thể, hai hình con bên phải trong Hình 2 cho thấy rằng tương tự đặc trưng của mỗi lớp của mô hình hạng thấp cao hơn đáng kể so với trước đây.

Để vượt qua điều này, chúng tôi đề xuất tận dụng hoàn toàn mô hình lớn đã được đào tạo trước và khai thác nó để hướng dẫn việc đào tạo mô hình hạng thấp. Cụ thể, như thể hiện trong Hình 3, đầu tiên chúng tôi thực hiện xấp xỉ hạng thấp trên mô hình đã được đào tạo trước và giữ lại cả hai mô hình. Các tham số của mô hình lớn đã được đào tạo trước được cố định trong khi chúng tôi chỉ đào tạo mô hình hạng thấp. Phương pháp của chúng tôi bao gồm thêm hai mô-đun: chưng cất đặc trưng để căn chỉnh các đặc trưng giữa hai mô hình này, nhiễu trọng số chính quy để ràng buộc mối quan hệ của ma trận khôi phục và ma trận ban đầu. Chúng tôi đặt tên phương pháp này là PELA, được gọi là các mô hình Tiết kiệm Tham số cho Xấp xỉ Hạng Thấp. Theo hiểu biết tốt nhất của chúng tôi, có nghiên cứu hạn chế về việc xây dựng một mô hình hạng thấp hiệu quả dựa trên Transformer đã được đào tạo trước. Do đó, chúng tôi nhằm giải quyết khoảng trống này bằng cách điều tra tiềm năng của xấp xỉ hạng thấp để đạt được sự đánh đổi hiệu quả-hiệu suất tối ưu.

3.2. Chưng cất Đặc trưng
Như đã nhấn mạnh trong tiểu mục trước, xấp xỉ hạng thấp có thể thay đổi phân phối đặc trưng của mô hình đã được đào tạo trước. Để giải quyết vấn đề này, chúng tôi sử dụng chưng cất kiến thức dựa trên đặc trưng, điều này đã được chứng minh hiệu quả trong việc căn chỉnh các đặc trưng giữa các mô hình [45]. Tuy nhiên, việc nén hạng thấp được thực hiện trên mỗi phép toán nhân ma trận, thay vì các khối hoặc lớp Transformer cụ thể. Việc trực tiếp chưng cất kiến thức từ tất cả các đặc trưng đầu ra của mô hình ban đầu dẫn đến nhiều lộn xộn hơn vì một số nén hạng thấp đã được bao bọc trong tính toán tự chú ý. Nhờ kết nối dư theo lớp của Transformer, chúng tôi sử dụng một sự thỏa hiệp trong công việc này - đơn giản là căn chỉnh các đặc trưng token của mỗi lớp. Từ cái nhìn tổng quát về các mô hình Transformer điển hình, mất mát chưng cất đặc trưng được định nghĩa như sau:

Lfd = Σ(i=1 to N) D(Ms(Xi_s), Mt(Xi_t)),
= 1/2N Σ(i=1 to N) ||Ms(Xi_s) - Mt(Xi_t)||², (4)

trong đó Xi_s và Xi_t biểu thị các đặc trưng token lớp thứ i của mô hình hạng thấp nén và mô hình ban đầu tương ứng; M là một phép biến đổi chuyển đổi đặc trưng sang không gian đặc trưng mục tiêu và chúng tôi sử dụng ánh xạ đồng nhất trong triển khai của chúng tôi. Bằng cách này, các đặc trưng đầu ra từ mỗi lớp ViT của mô hình hạng thấp được mong đợi sẽ chia sẻ phân phối tương tự với mô hình lớn đã được đào tạo trước tương ứng.

Một cái nhìn thay thế từ chưng cất kiến thức. Các nghiên cứu gần đây đã chỉ ra rằng chưng cất kiến thức dựa trên đặc trưng vượt trội đáng kể so với chưng cất dựa trên logit thông thường [19]. Tuy nhiên, cách thiết kế một mô hình sinh viên và chuyển giao kiến thức từ mô hình giáo viên vẫn còn thách thức vì khá khó khăn để xác định khớp đặc trưng giáo viên-sinh viên. Phương pháp của chúng tôi cung cấp một giải pháp gọn gàng cho vấn đề này do hai lý do sau: 1) Không giống như các phương pháp trước đây (ví dụ: [25, 50] loại bỏ thủ công một số lớp nhất định), xấp xỉ hạng thấp là không tốn công sức và đơn giản để nén mô hình giáo viên cồng kềnh thành một mô hình sinh viên nhẹ. 2) Tồn tại một sự tương ứng tự nhiên giữa mô hình giáo viên và mô hình sinh viên vì chúng tôi chưa thay đổi quá mức các kiến trúc mô hình.

3.3. Nhiễu Trọng số Chính quy
Một xấp xỉ hạng thấp lý tưởng là học một ma trận xấp xỉ của ma trận ban đầu tuân theo ràng buộc hạng giảm. Điều này dẫn đến một tiến thoái lưỡng nan hiệu quả-hiệu suất - Một hạng lớn hơn tương ứng với một lỗi tái tạo thấp hơn và ngược lại. Trực quan, chúng tôi liên kết việc tái tạo ma trận với việc nhiễu trọng số, điều này tương đối mới so với vấn đề độ bền nhiễu đặc trưng/đầu vào [62]. Kết quả là, một hạng nhỏ hơn trong phương pháp của chúng tôi, từ góc độ khác, có thể được coi là nhiều nhiễu trọng số hơn. Để giảm ảnh hưởng tiêu cực của những tham số bị nhiễu này, chúng tôi sử dụng chuẩn l∞ để ràng buộc lỗi tái tạo:

{||Ŵ(k) - W(k)||∞ ≤ ε,
Ŵ(k) = U(k)(V(k))T, ∀k ∈ [K]} (5)

trong đó ε đại diện cho bán kính nhiễu, W(k) là ma trận trọng số ban đầu, và [K] biểu thị tập chỉ số trọng số. Cho ε, việc bảo tồn độ bền mạng neural chống lại nhiễu trọng số có thể được đặt như vấn đề tối ưu hóa sau [62]:

Lrwp = Σ(k=1 to |[K]|) (||Ŵ(k) - W(k)||∞ - ε). (6)

3.4. Đào tạo
Hai mô-đun trên cho phép chúng tôi nắm bắt khả năng phân biệt hấp dẫn của các mô hình lớn đã được đào tạo trước. Để có được một mô hình hạng thấp nhỏ gọn, chúng tôi xem xét toàn diện các mục tiêu từ cả đào tạo trước cơ bản và hai mô-đun được đề xuất của chúng tôi:

L = Lbase + αLfd + βLrwp, (7)

trong đó α và β là các siêu tham số trọng số mất mát và Lbase là các hàm mất mát của các tác vụ đào tạo trước ban đầu. Nó có thể là mất mát phân loại của một ViT điển hình, hoặc các mất mát khớp thị giác-văn bản và mô hình hóa ngôn ngữ có mask của một mô hình thị giác-ngôn ngữ. Sau đó chúng tôi tối ưu hóa mô hình của mình trên cùng các tập dữ liệu như mô hình đã được đào tạo trước, chẳng hạn như ImageNet [8]. Sau giai đoạn đào tạo trước trung gian này, mô hình hạng thấp của chúng tôi được triển khai một cách suôn sẻ cho tinh chỉnh hạ lưu vì kiến trúc mô hình hiếm khi được thay đổi. Trái ngược với các phương pháp hiện có như prompt tuning [24], adapter [51], và LoRA [21], các phương pháp này yêu cầu cả mô hình lớn đã được đào tạo trước và các tham số tinh chỉnh, chúng tôi chỉ giữ mô hình hạng thấp để suy luận hiệu quả và sử dụng tham số (xem Hình 3 để so sánh trực quan).

3.5. Phân tích Độ phức tạp
Trước khi phân tích độ phức tạp của phương pháp chúng tôi, đầu tiên chúng tôi để dlr = 1/κ × din×dout/(din+dout), trong đó κ là một số dương và chúng tôi đặt tên nó là tỉ lệ nén. Chúng tôi chọn sử dụng tỉ lệ nén phổ quát cho tất cả các phép toán nhân ma trận để đơn giản trong khi để lại việc khám phá các tỉ lệ động cho các lớp khác nhau như công việc tương lai.

Hãy xem xét trường hợp κ = 2 và một đặc trưng patch đơn x∈R^din cho tinh chỉnh hạ lưu. Nhớ lại Phương trình 1, việc nhân ma trận ban đầu mất O(din × dout) để thao tác. Tuy nhiên, với phương pháp PELA của chúng tôi, độ phức tạp thời gian này giảm xuống O((din + dout) × dlr) = 1/2 O(din × dout). Tương tự, vì hoạt động chủ yếu trong các Transformer hiện tại là nhân ma trận (loại trừ một số tham số chuẩn hóa lớp rất ít và tham số bias), kích thước mô hình do đó cũng giảm khoảng một nửa từ quy mô ban đầu của nó. Đây là lý do tại sao phương pháp của chúng tôi khác biệt đáng kể so với các phương pháp hiệu quả gần đây khác như LoRA [21], trong đó kích thước mô hình tổng thể thực sự tăng lên.

4. Thí nghiệm
4.1. Các Baseline Học Hiệu quả Phổ biến
Chúng tôi đánh giá PELA của chúng tôi so với bốn baseline hiệu quả: TinyBERT [25] và MaskAlign [67] từ nhóm chưng cất kiến thức dựa trên đặc trưng; ToMe [2] - một phương pháp cắt tỉa token thị giác mạnh mẽ gần đây; và LoRA [21], một baseline học chuyển giao tiết kiệm tham số được sử dụng rộng rãi. Tuy nhiên, chúng tôi loại trừ một số thí nghiệm do một số không tương thích nhất định, chẳng hạn như sử dụng ToMe cho mô hình Swin và cho tác vụ định vị thị giác.

4.2. Thí nghiệm trên Mô hình Chỉ dành cho Thị giác
4.2.1 Mô hình Baseline và Kết quả
Chúng tôi áp dụng phương pháp của mình cho các mô hình DeiT-Base [54] và Swin-Base [39] được sử dụng rộng rãi. Để đảm bảo bao phủ toàn diện, chúng tôi cũng chọn DeiT-III-Large [56] lớn hơn về kích thước mô hình và đòi hỏi thời gian đào tạo lâu hơn nhiều. Tỉ lệ nén là 1/2 và 1/3 cho các mô hình DeiT và Swin tương ứng. Sau xấp xỉ hạng thấp, chúng tôi đào tạo mô hình của mình trên tập dữ liệu ImageNet-1k [8] và đánh giá nó trên tập validation tương ứng, và báo cáo kết quả trong Bảng 1. Như mong đợi, các tham số mô hình và FLOPs cho suy luận được giảm đáng kể theo mỗi tỉ lệ nén tương ứng. Mặt khác, độ chính xác giảm của hai mô hình cơ sở là 0.8% và 1.0% tương ứng. Thậm chí đối với mô hình tương đối lớn hơn DeiT-III-Large, phương pháp của chúng tôi chỉ đánh đổi 1.0% độ chính xác với một nửa tham số và FLOPs. Hơn nữa, PELA của chúng tôi vượt trội hơn các baseline học hiệu quả khác với một khoảng cách đáng chú ý.

4.2.2 Tác vụ Hạ lưu và Kết quả
Sau khi các backbone được đào tạo trước trên tập dữ liệu ImageNet, theo các nghiên cứu trước đây [15, 17], chúng tôi tiếp tục đánh giá hiệu suất mô hình trên các tác vụ phân đoạn ngữ nghĩa và phát hiện đối tượng hạ lưu.

Kết quả được trình bày trong Bảng 2 và Bảng 3, minh họa hiệu quả của phương pháp chúng tôi trong việc thực hiện các tác vụ phân đoạn ngữ nghĩa và phát hiện đối tượng tương ứng. Trong khi phương pháp của chúng tôi được hưởng lợi từ các yêu cầu bộ nhớ và tính toán giảm, việc tham gia của các framework và head hạ lưu hạn chế mức độ mà những lợi ích này có thể được thực hiện khi so sánh với phân loại vanilla. Ví dụ, FLOPs giảm cho Swin-Base trên phát hiện đối tượng trong Bảng 3 là 18% so với 30% trước đây trong Bảng 1. Tuy nhiên, phương pháp của chúng tôi vẫn hoạt động có thể so sánh với mỗi mô hình tương ứng, cho thấy hiệu quả của nó trong việc cân bằng sự đánh đổi giữa hiệu quả và độ chính xác. Đáng chú ý, PELA của chúng tôi vượt trội đáng kể so với LoRA về cả hiệu suất mô hình và kích thước mô hình.

4.3. Thí nghiệm trên Mô hình Thị giác-Ngôn ngữ
4.3.1 Mô hình Baseline và Tác vụ VL Hạ lưu
Các phương pháp đào tạo trước thị giác-ngôn ngữ truyền thống [6, 53] thường sử dụng các đặc trưng CNN được trích xuất trước cho biểu diễn hình ảnh, thường yêu cầu chú thích hộp bao chính xác. Ngược lại, ALBEF [33] tận dụng ViT để trích xuất đặc trưng thị giác trong quá trình đào tạo trước và đã thể hiện hiệu suất xuất sắc trên nhiều tác vụ VL khác nhau. Do đó, chúng tôi chọn ALBEF làm testbed đánh giá để đánh giá hiệu quả của phương pháp chúng tôi. Hơn nữa, bản chất all-in Transformer của ALBEF cho phép chúng tôi dễ dàng đạt được nén nhiều hơn. Trong bối cảnh này, chúng tôi sử dụng 1/3 tham số của mô hình ALBEF ban đầu.

Chúng tôi sử dụng bốn tác vụ thị giác-ngôn ngữ hạ lưu trong công việc này, bao gồm Truy xuất Hình ảnh-Văn bản, SNLI-VE, VG, và VQA. Một giới thiệu chi tiết về các tác vụ này có thể được tìm thấy trong tài liệu bổ sung. Đối với các thí nghiệm, chúng tôi tuân thủ nghiêm ngặt việc triển khai ALBEF ngoại trừ việc giảm kích thước batch do hạn chế tài nguyên.

4.3.2 Kết quả Tổng thể
Kết quả trên các tác vụ hạ lưu này được báo cáo trong Bảng 4, 5, và 6. Từ những bảng này, chúng tôi có ba quan sát quan trọng sau. 1) Phương pháp gần đây ALBEF [33] đã thể hiện những cải tiến hiệu suất đáng kể so với các phương pháp thông thường như LXMERT [53] và UNITER [6]. Tuy nhiên, hiệu suất vượt trội được đạt được với cái giá của việc tăng tham số và FLOPs, chủ yếu do việc sử dụng ViT cồng kềnh có thể huấn luyện để xử lý hình ảnh. So với các baseline, sử dụng Transformer phổ quát cho cả thị giác và ngôn ngữ, như UNITER [6], ALBEF cung cấp các đặc trưng thị giác vượt trội nhưng giới thiệu kích thước mô hình và độ phức tạp tính toán lớn hơn. 2) Phương pháp PELA của chúng tôi giúp giảm bớt vấn đề này thông qua xấp xỉ hạng thấp. Như có thể quan sát, PELA có thể đạt được hiệu suất có thể so sánh với ALBEF trong khi chỉ sử dụng 1/3 tham số và FLOPs. Điều này dẫn đến sự giảm đáng kể về kích thước mô hình và tính toán, với hầu hết sự suy giảm hiệu suất giới hạn chỉ một điểm. 3) Liên quan đến việc so sánh với các baseline học hiệu quả, phương pháp PELA của chúng tôi liên tục đạt được hiệu suất tốt hơn trong hầu hết các trường hợp. Ngoại lệ duy nhất là đối với các tác vụ truy xuất, trong đó PELA thể hiện hiệu suất mô hình kém hơn một chút so với LoRA. Tuy nhiên, điều quan trọng cần lưu ý là LoRA yêu cầu số lượng tham số mô hình và FLOPs lớn hơn.

4.4. Nghiên cứu Loại bỏ
Hiệu quả của hai mô-đun. Đầu tiên chúng tôi nghiên cứu hiệu suất mô hình của việc phân tách trực tiếp các trọng số đã được đào tạo trước bằng xấp xỉ hạng thấp. Tuy nhiên, như được chỉ ra trong Bảng 7, phương pháp này dẫn đến sự sụt giảm đáng kể về hiệu suất, có thể do sự thay đổi trong phân phối đặc trưng. Sau đó chúng tôi thêm hai mô-đun được đề xuất của chúng tôi vào mô hình hạng thấp và quan sát những cải tiến hiệu suất. Bằng cách kết hợp hai mô-đun với nhau, mô hình của chúng tôi thường có thể vượt trội hơn các biến thể khác, chứng minh hiệu quả của phương pháp được đề xuất.

Thay đổi hiệu suất w.r.t. tỉ lệ nén. Việc đào tạo các mô hình lớn thường liên quan đến sự đánh đổi giữa hiệu quả và hiệu suất. Để chứng minh điều này, chúng tôi đào tạo mô hình của mình bằng các tỉ lệ nén khác nhau với ít epoch hơn để đơn giản hóa quá trình và trình bày kết quả trong Hình 4. Biểu đồ này chỉ ra rằng một tỉ lệ nén nhỏ hơn, tức là một mô hình lớn hơn, thường mang lại hiệu suất tốt hơn. Tuy nhiên, một mô hình quá nhỏ, chẳng hạn như một mô hình được nén xuống 1/10 kích thước ban đầu, có thể không có khả năng đạt được kết quả thỏa đáng.

4.5. Hiệu quả Đào tạo trước & Mở rộng Mô hình
Hiệu quả Đào tạo trước. Một người có thể lo ngại về các vấn đề hiệu quả trong quá trình đào tạo trước. Để giải quyết vấn đề này, chúng tôi tận dụng mô hình DeiT-Base và đánh giá các chỉ số hiệu quả đào tạo trước của nó, và hiển thị kết quả trong Bảng 8. Cụ thể, chúng tôi sử dụng mô hình hạng thấp đơn giản vì nó đã mang lại hiệu suất mô hình đầy hứa hẹn. Mặc dù các mô hình khác có thể kích hoạt thời gian đào tạo lâu hơn, trong bối cảnh này, như thể hiện trong bảng, phương pháp PELA của chúng tôi vượt trội hơn mô hình ban đầu về cả chi phí bộ nhớ GPU và độ trễ đào tạo.

Mở rộng Mô hình Hạ lưu. Phương pháp của chúng tôi sinh ra một mô hình nhỏ gọn hơn so với mô hình lớn đã được đào tạo trước ban đầu, dẫn đến dư thừa bộ nhớ cho phép chúng tôi đào tạo các mô hình hạ lưu với kích thước batch lớn hơn. Để chứng minh hiệu quả của phương pháp chúng tôi, chúng tôi tăng kích thước batch cho cả DeiT-Base và Swin-Base trên tác vụ phân đoạn ngữ nghĩa, như thể hiện trong Bảng 9. Các thí nghiệm của chúng tôi cho thấy kết quả đầy hứa hẹn, với sự cải thiện đáng kể về hiệu suất mô hình cho cả hai mô hình, đạt được cải thiện mIoU tuyệt đối 0.57% và 0.78% tương ứng. Hơn nữa, phương pháp được đề xuất của chúng tôi cũng vượt trội hơn baseline Swin-Base ban đầu sử dụng PELA+, làm nổi bật một lợi thế khác của phương pháp được đề xuất của chúng tôi.

5. Kết luận và Công việc Tương lai
Trong công việc này, chúng tôi đề xuất một phương pháp đào tạo trước tiết kiệm tham số đơn giản nhưng hiệu quả sử dụng xấp xỉ hạng thấp làm cốt lõi. Thậm chí với tính đơn giản của nó, phương pháp của chúng tôi đạt được hiệu suất cạnh tranh với các baseline trong khi đạt được hiệu quả tham số và tính toán được cải thiện đáng kể. Những lợi thế này cho phép mở rộng mô hình về độ sâu mô hình, chiều rộng, và kích thước batch đào tạo của tinh chỉnh tác vụ hạ lưu. Công việc này làm nổi bật những lợi ích tiềm năng của việc giải quyết vấn đề quá tham số hóa của các trọng số có thể học. Ngoài ra, chúng tôi tin rằng việc nén các đặc trưng trung gian là một hướng trực giao đầy hứa hẹn để giảm độ phức tạp mô hình. Do đó, chúng tôi dự định điều tra các kỹ thuật nén đặc trưng, chẳng hạn như cắt tỉa token thị giác, để xây dựng thêm một mô hình nhẹ hơn trong nghiên cứu tương lai.

Tài liệu tham khảo
[1] Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik Stöger, Mahdi Soltanolkotabi, và Soheil Feizi. Understanding over-parameterization in generative adversarial networks. Trong ICLR, 2021. 2, 3
[2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, và Judy Hoffman. Token merging: Your vit but faster. Trong ICLR, 2023. 5, 6, 7
[3] Zhaowei Cai và Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. Trong CVPR, trang 6154–6162. IEEE, 2018. 6
[4] Chun-Fu Richard Chen, Quanfu Fan, và Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. Trong ICCV, trang 357–366. IEEE, 2021. 6
[5] Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, và Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. Trong NeurIPS, trang 11011–11021, 2018. 3
[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, và Jingjing Liu. UNITER: universal image-text representation learning. Trong ECCV, trang 104–120. Springer, 2020. 6, 7
[7] Matthieu Courbariaux, Yoshua Bengio, và Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. Trong NIPS, trang 3123–3131, 2015. 1, 2
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong CVPR, trang 248–255. IEEE, 2009. 5, 6
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. Trong NAACL, trang 4171–4186. ACL, 2019. 1, 2
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong ICLR. OpenReview.net, 2021. 1, 3, 6
[11] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, và Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Trong NeurIPS, 2020. 7
[12] Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, và Jianguo Li. Network decoupling: From regular to depthwise separable convolutions. Trong BMVC, trang 248. BMV A Press, 2018. 3
[13] Yangyang Guo, Zhiyong Cheng, Jiazheng Jing, Yanpeng Lin, Liqiang Nie, và Meng Wang. Enhancing factorization machines with generalized metric learning. TKDE, 34(8):3740–3753, 2022. 3
[14] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. Towards a unified view of parameter-efficient transfer learning. Trong ICLR, 2022. 3
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong CVPR, trang 770–778. IEEE, 2016. 6
[16] Kaiming He, Georgia Gkioxari, Piotr Dollár, và Ross Girshick. Mask r-cnn. Trong ICCV, trang 2961–2969. IEEE, 2017. 6
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross B. Girshick. Masked autoencoders are scalable vision learners. Trong CVPR, trang 15979–15988. IEEE, 2022. 3, 6
[18] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, và Tat-Seng Chua. Neural collaborative filtering. Trong WWW, trang 173–182. ACM, 2017. 3
[19] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, và Jin Young Choi. A comprehensive overhaul of feature distillation. Trong ICCV, trang 1921–1930. IEEE, 2019. 1, 2, 4
[20] Geoffrey E. Hinton, Oriol Vinyals, và Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. 1, 2
[21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. Trong ICLR. OpenReview.net, 2022. 2, 3, 5, 6, 7
[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, và Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. JMLR, 18:187:1–187:30, 2017. 1, 2
[23] Yerlan Idelbayev và Miguel Á. Carreira-Perpiñán. Low-rank compression of neural nets: Learning the rank of each layer. Trong CVPR, trang 8046–8056. IEEE, 2020. 3
[24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning. Trong ECCV, trang 709–727. Springer, 2022. 1, 2, 3, 5
[25] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. Tinybert: Distilling BERT for natural language understanding. Trong Findings of EMNLP, trang 4163–4174. ACL, 2020. 2, 5, 6, 7
[26] Roberto J. Bayardo Jr. Efficiently mining long patterns from databases. Trong SIGMOD, trang 85–93. ACM, 1998. 2
[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language models. CoRR, 2020. 1
[28] Wonjae Kim, Bokyung Son, và Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. Trong ICML, trang 5583–5594. PMLR, 2021. 7
[29] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, Minghai Qin, và Yanzhi Wang. Spvit: Enabling faster vision transformers via latency-aware soft token pruning. Trong ECCV, trang 620–640. Springer, 2022. 2
[30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. Trong ICLR. OpenReview.net, 2020. 2, 3
[31] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas M. Breuel, Jan Kautz, và Yale Song. Parameter efficient multimodal transformers for video representation learning. Trong ICLR, 2021. 3
[32] Chong Li và C.-J. Richard Shi. Constrained optimization based low-rank approximation of deep neural networks. Trong ECCV, trang 746–761. Springer, 2018. 3
[33] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, Caiming Xiong, và Steven Chu-Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Trong NeurIPS, trang 9694–9705, 2021. 2, 6, 7
[34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, và Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019. 7
[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, và Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. Trong ECCV, trang 121–137. Springer, 2020. 7
[36] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, và Shi Gu. BRECQ: pushing the limit of post-training quantization by block reconstruction. Trong ICLR. OpenReview.net, 2021. 2
[37] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C. Lawrence Zitnick. Microsoft COCO: common objects in context. Trong ECCV, trang 740–755. Springer, 2014. 6
[38] Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Dechao Meng, và Qingming Huang. Adaptive reconstruction network for weakly supervised referring expression grounding. Trong ICCV, trang 2611–2620. IEEE, 2019. 7
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong ICCV, trang 9992–10002. IEEE, 2021. 2, 5, 6
[40] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, và Stefan Lee. 12-in-1: Multi-task vision and language representation learning. Trong CVPR, trang 10434–10443. IEEE, 2020. 7
[41] Gaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller, faster, and better. CoRR, abs/2106.08962, 2021. 2
[42] Dimitris S. Papailiopoulos, Alexandros G. Dimakis, và Stavros Korokythakis. Sparse PCA through low-rank approximations. Trong ICML, trang 747–755. JMLR.org, 2013. 3
[43] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, và Piotr Dollár. Designing network design spaces. Trong CVPR, trang 10428–10436. IEEE, 2020. 6
[44] Babak Rokh, Ali Azarpeyvand, và Alireza Khanteymoori. A comprehensive survey on model quantization for deep neural networks. CoRR, 2022. 2
[45] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Hints for thin deep nets. Trong ICLR, 2015. 2, 4
[46] Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. 2
[47] M. Schuermans, Philippe Lemmerling, và Sabine Van Huffel. Structured weighted low rank approximation. Numerical Linear Algebra with Applications, 11(5-6):609–618, 2004. 2
[48] Nathan Srebro và Tommi S. Jaakkola. Weighted low-rank approximations. Trong ICML, trang 720–727. AAAI Press, 2003. 2
[49] Trevor Strohman và W. Bruce Croft. Efficient document retrieval in main memory. Trong SIGIR, trang 175–182. ACM, 2007. 2
[50] Siqi Sun, Yu Cheng, Zhe Gan, và Jingjing Liu. Patient knowledge distillation for BERT model compression. Trong EMNLP, trang 4322–4331. ACL, 2019. 1, 2, 5
[51] Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. VL-ADAPTER: parameter-efficient transfer learning for vision-and-language tasks. Trong CVPR, trang 5217–5227. IEEE, 2022. 1, 2, 3, 5
[52] Cheng Tai, Tong Xiao, Xiaogang Wang, và Weinan E. Convolutional neural networks with low-rank regularization. Trong ICLR, 2016. 3
[53] Hao Tan và Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers. Trong EMNLP, trang 5099–5110. ACL, 2019. 6, 7
[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. Trong ICML, trang 10347–10357. PMLR, 2021. 2, 5, 6
[55] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, và Hervé Jégou. Augmenting convolutional networks with attention-based aggregation. arXiv preprint arXiv:2112.13692, 2021. 6
[56] Hugo Touvron, Matthieu Cord, và Hervé Jégou. Deit iii: Revenge of the vit. Trong ECCV, trang 516–533. Springer, 2022. 2, 5, 6
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong NIPS, trang 5998–6008, 2017. 2
[58] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, và Yun Fu. Recent advances on neural network pruning at initialization. Trong IJCAI, trang 5638–5645. ijcai.org, 2022. 2
[59] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, và Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Trong ICML, trang 23318–23340. PMLR, 2022. 1
[60] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. 3
[61] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, và William Fedus. Emergent abilities of large language models. CoRR, 2022. 1
[62] Tsui-Wei Weng, Pu Zhao, Sijia Liu, Pin-Yu Chen, Xue Lin, và Luca Daniel. Towards certificated model robustness against weight perturbations. Trong AAAI, trang 6356–6363. AAAI Press, 2020. 5
[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, và Jian Sun. Unified perceptual parsing for scene understanding. Trong ECCV, trang 418–434. Springer, 2018. 6
[64] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, và Kaiming He. Aggregated residual transformations for deep neural networks. Trong CVPR, trang 1492–1500. IEEE, 2017. 6
[65] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, và Fei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. Trong EMNLP, trang 9514–9528. ACL, 2021. 2
[66] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, và Hongkai Xiong. TRP: trained rank pruning for efficient deep neural networks. Trong IJCAI, trang 977–983. ijcai.org, 2020. 3
[67] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, và Jiebo Luo. Stare at what you see: Masked image modeling without reconstruction. Trong CVPR, trang 22732–22741. IEEE, 2023. 5, 6, 7
[68] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, và Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification. Trong CVPR Workshops, trang 2899–2908. IEEE, 2020. 3
[69] Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, và Yu Li. Vitkd: Practical guidelines for vit feature knowledge distillation. CoRR, abs/2209.02432, 2022. 2
[70] Xiyu Yu, Tongliang Liu, Xinchao Wang, và Dacheng Tao. On compressing deep models by low rank and sparse decomposition. Trong CVPR, trang 67–76. IEEE, 2017. 3
[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. Trong ICCV, trang 558–567. IEEE, 2021. 6
[72] Elad Ben Zaken, Yoav Goldberg, và Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. Trong ACL, trang 1–9. ACL, 2022. 2
[73] Xiangyu Zhang, Jianhua Zou, Kaiming He, và Jian Sun. Accelerating very deep convolutional networks for classification and detection. TPAMI, 38(10):1943–1955, 2016. 3
[74] Zhu Zhang, Zhou Zhao, Zhijie Lin, Jieming Zhu, và Xiuqiang He. Counterfactual contrastive learning for weakly-supervised vision-language grounding. Trong NeurIPS, 2020. 7
[75] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, và Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 127:302–321, 2019. 6
