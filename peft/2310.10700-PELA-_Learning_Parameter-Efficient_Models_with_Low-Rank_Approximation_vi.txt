# 2310.10700.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/peft/2310.10700.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 997331 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
PELA: Há»c cÃ¡c MÃ´ hÃ¬nh Tiáº¿t kiá»‡m Tham sá»‘ vá»›i Xáº¥p xá»‰ Háº¡ng Tháº¥p
Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli
Äáº¡i há»c Quá»‘c gia Singapore
TÃ³m táº¯t
Viá»‡c Ã¡p dá»¥ng mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cho cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u lÃ  khÃ´ng kháº£ thi trong Ä‘iá»u kiá»‡n tÃ i nguyÃªn háº¡n cháº¿. CÃ¡c phÆ°Æ¡ng phÃ¡p chiáº¿m Æ°u tháº¿ gáº§n Ä‘Ã¢y Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u quáº£ liÃªn quan Ä‘áº¿n viá»‡c thÃªm má»™t sá»‘ tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ o mÃ´ hÃ¬nh backbone cá»‘ Ä‘á»‹nh. Tuy nhiÃªn, chiáº¿n lÆ°á»£c nÃ y dáº«n Ä‘áº¿n nhiá»u thÃ¡ch thá»©c hÆ¡n trong viá»‡c táº£i cÃ¡c mÃ´ hÃ¬nh lá»›n Ä‘á»ƒ tinh chá»‰nh háº¡ lÆ°u vá»›i tÃ i nguyÃªn háº¡n cháº¿. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i Ä‘á»ƒ tÄƒng hiá»‡u quáº£ tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c báº±ng cÃ¡ch giá»›i thiá»‡u má»™t giai Ä‘oáº¡n Ä‘Ã o táº¡o trÆ°á»›c trung gian. Äá»ƒ lÃ m Ä‘iá»u nÃ y, Ä‘áº§u tiÃªn chÃºng tÃ´i sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p Ä‘á»ƒ nÃ©n mÃ´ hÃ¬nh lá»›n ban Ä‘áº§u vÃ  sau Ä‘Ã³ thiáº¿t káº¿ má»™t mÃ´-Ä‘un chÆ°ng cáº¥t Ä‘áº·c trÆ°ng vÃ  má»™t mÃ´-Ä‘un chÃ­nh quy hÃ³a nhiá»…u trá»ng sá»‘. CÃ¡c mÃ´-Ä‘un nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh háº¡ng tháº¥p. Cá»¥ thá»ƒ, chÃºng tÃ´i chá»‰ cáº­p nháº­t mÃ´ hÃ¬nh háº¡ng tháº¥p trong khi cá»‘ Ä‘á»‹nh cÃ¡c tham sá»‘ backbone trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c. Äiá»u nÃ y cho phÃ©p sá»­ dá»¥ng trá»±c tiáº¿p vÃ  hiá»‡u quáº£ mÃ´ hÃ¬nh háº¡ng tháº¥p cho cÃ¡c tÃ¡c vá»¥ tinh chá»‰nh háº¡ lÆ°u. PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘áº¡t Ä‘Æ°á»£c cáº£ hiá»‡u quáº£ vá» tham sá»‘ cáº§n thiáº¿t vÃ  thá»i gian tÃ­nh toÃ¡n trong khi duy trÃ¬ káº¿t quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i nhá»¯ng thay Ä‘á»•i tá»‘i thiá»ƒu Ä‘á»‘i vá»›i kiáº¿n trÃºc backbone. Cá»¥ thá»ƒ, khi Ä‘Æ°á»£c Ã¡p dá»¥ng cho ba mÃ´ hÃ¬nh Transformer chá»‰ dÃ nh cho thá»‹ giÃ¡c vÃ  má»™t mÃ´ hÃ¬nh Transformer thá»‹ giÃ¡c-ngÃ´n ngá»¯, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i thÆ°á»ng cho tháº¥y chá»‰ giáº£m khoáº£ng ~0.6 Ä‘iá»ƒm hiá»‡u suáº¥t trong khi giáº£m kÃ­ch thÆ°á»›c tham sá»‘ ban Ä‘áº§u tá»« 1/3 Ä‘áº¿n 2/3. MÃ£ nguá»“n Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ táº¡i liÃªn káº¿t.

1. Giá»›i thiá»‡u
Viá»‡c Ä‘Ã o táº¡o trÆ°á»›c má»™t mÃ´ hÃ¬nh lá»›n vÃ  tinh chá»‰nh nÃ³ cho tá»«ng tÃ¡c vá»¥ cá»¥ thá»ƒ Ä‘Ã£ trá»Ÿ thÃ nh má»™t mÃ´ hÃ¬nh de facto trong nhiá»u lÄ©nh vá»±c nghiÃªn cá»©u Ä‘a dáº¡ng [9, 10, 59]. Máº·c dÃ¹ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ, viá»‡c xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y thÆ°á»ng pháº£i Ä‘Ã¡nh Ä‘á»•i báº±ng viá»‡c tÄƒng sá»­ dá»¥ng bá»™ nhá»› vÃ  thá»i gian Ä‘Ã o táº¡o dÃ i hÆ¡n. Báº¥t cháº¥p nhá»¯ng thÃ¡ch thá»©c nÃ y, nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y trong viá»‡c Ä‘Ã¡nh giÃ¡ luáº­t quy mÃ´ [27] vÃ  kháº£ nÄƒng phÃ¡t sinh [61] cá»§a viá»‡c Ä‘Ã o táº¡o trÆ°á»›c ngÃ´n ngá»¯ Ä‘Ã£ thÃºc Ä‘áº©y thÃªm sá»± quan tÃ¢m cá»§a cÃ¡c nhÃ  thá»±c hÃ nh trong viá»‡c phÃ¡t triá»ƒn vÃ  sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»›n.

VÃ¬ viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh nÃ y cho cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u thÆ°á»ng khÃ´ng kháº£ thi, cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ chuyá»ƒn sang trÃ¡nh viá»‡c tinh chá»‰nh toÃ n bá»™ mÃ´ hÃ¬nh.

Trá»±c giÃ¡c NghiÃªn cá»©u
34
â€¦Ã—ğ‘ğ‘
Ã—ğ‘ğ‘ CÃ¡c lá»›p Transformer
PELA
NÃ©n
HÃ¬nh 1. Tá»•ng quan vÃ  hiá»‡u suáº¥t cá»§a phÆ°Æ¡ng phÃ¡p PELA Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i. TrÃ¡i: Sá»­ dá»¥ng PELA, chÃºng tÃ´i nÃ©n cÃ¡c trá»ng sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cá»§a mÃ´ hÃ¬nh ViT Ä‘iá»ƒn hÃ¬nh trong khi báº£o toÃ n kiáº¿n trÃºc tá»•ng thá»ƒ cá»§a nÃ³. Pháº£i: So sÃ¡nh mÃ´ hÃ¬nh Gá»‘c vÃ  PELA cá»§a chÃºng tÃ´i vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÆ°Æ¡ng Ä‘á»‘i vÃ  chá»‰ sá»‘ Ä‘á»™ chÃ­nh xÃ¡c trÃªn ba bá»™ Transformer Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c.

CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘iá»ƒn hÃ¬nh thÆ°á»ng giá»›i thiá»‡u thÃªm má»™t sá»‘ tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ o mÃ´ hÃ¬nh backbone trong khi cá»‘ Ä‘á»‹nh pháº§n cÃ²n láº¡i, vÃ­ dá»¥: adapter [51] vÃ  prompt tuning [24] thÃªm cÃ¡c tham sá»‘ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh vÃ o cÃ¡c vá»‹ trÃ­ token á»Ÿ giá»¯a vÃ  ngoáº¡i vi cá»§a Transformer tÆ°Æ¡ng á»©ng. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng trÃ¡nh khá»i dáº«n Ä‘áº¿n hai nhÆ°á»£c Ä‘iá»ƒm sau. Thá»© nháº¥t, tiá»m nÄƒng cá»§a cÃ¡c mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c khÃ´ng Ä‘Æ°á»£c khai thÃ¡c Ä‘áº§y Ä‘á»§ vÃ¬ pháº§n lá»›n cÃ¡c tham sá»‘ khÃ´ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vá»›i cÃ¡c má»¥c tiÃªu tÃ¡c vá»¥ háº¡ lÆ°u. Thá»© hai, viá»‡c táº£i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trá»Ÿ nÃªn gÃ¡nh náº·ng hÆ¡n ná»¯a Ä‘á»‘i vá»›i cÃ¡c nhÃ  nghiÃªn cá»©u cÃ³ tÃ i nguyÃªn háº¡n cháº¿. NgÆ°á»£c láº¡i, cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng nhÆ° chÆ°ng cáº¥t kiáº¿n thá»©c (KD) [19, 20, 50] vÃ  lÆ°á»£ng tá»­ hÃ³a [7, 22] cÃ³ thá»ƒ giáº£m bá»›t má»™t pháº§n váº¥n Ä‘á» nÃ y. Tuy nhiÃªn, hiá»‡n táº¡i khÃ´ng cÃ³ phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c thiáº¿t láº­p Ä‘á»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh sinh viÃªn cÃ³ hiá»‡u suáº¥t cao cá»§a KD vÃ  cÃ¡c toÃ¡n tá»­ khÃ´ng kháº£ vi cá»§a lÆ°á»£ng tá»­ hÃ³a thÆ°á»ng lÃ m cho viá»‡c thá»±c hiá»‡n lan truyá»n ngÆ°á»£c Ã­t kháº£ thi hÆ¡n.

BÃ i bÃ¡o nÃ y nháº¯m Ä‘áº¿n viá»‡c phÃ¡t triá»ƒn má»™t phÆ°Æ¡ng phÃ¡p cÃ³ hiá»‡u quáº£ tham sá»‘ cao Ä‘á»ƒ giÃºp tinh chá»‰nh tÃ¡c vá»¥ háº¡ lÆ°u, nhÆ° minh há»a trong HÃ¬nh 1. Báº±ng hiá»‡u quáº£ tham sá»‘, chÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n má»™t mÃ´ hÃ¬nh nÃ©n vá»›i kÃ­ch thÆ°á»›c giáº£m (vÃ­ dá»¥: nhá» hÆ¡n 2Ã—), dá»… triá»ƒn khai, hiá»‡u quáº£ vá» tÃ­nh toÃ¡n, vÃ  cÃ³ nhá»¯ng Æ°u Ä‘iá»ƒm thay Ä‘á»•i kiáº¿n trÃºc tá»‘i thiá»ƒu. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cung cáº¥p má»™t mÃ´ hÃ¬nh nÃ©n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c mÃ  cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u cÃ³ thá»ƒ thá»±c hiá»‡n tinh chá»‰nh trá»±c tiáº¿p trÃªn Ä‘Ã³, trÃ¡i ngÆ°á»£c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y nhÆ° LoRA [21], adapter [51], vÃ  prompt tuning [24]. Cá»¥ thá»ƒ, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» quÃ¡ tham sá»‘ hÃ³a [1], trong Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p Ä‘á»ƒ thay tháº¿ ma tráº­n trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trong má»—i phÃ©p toÃ¡n nhÃ¢n ma tráº­n báº±ng hai ma tráº­n háº¡ng tháº¥p. Báº±ng cÃ¡ch nÃ y, cáº£ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh ban Ä‘áº§u vÃ  thá»i gian tinh chá»‰nh Ä‘á»u Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p ngÃ¢y thÆ¡ nÃ y Ä‘á»ƒ thá»±c hiá»‡n tinh chá»‰nh cho káº¿t quáº£ kÃ©m hÃ i lÃ²ng hÆ¡n (tham kháº£o Pháº§n 4.4). ChÃºng tÃ´i cho ráº±ng Ä‘iá»u nÃ y lÃ  do hai lÃ½ do: Viá»‡c phÃ¢n tÃ¡ch trá»±c tiáº¿p vá»›i xáº¥p xá»‰ háº¡ng tháº¥p khÃ´ng thá»ƒ há»c hiá»‡u quáº£ cÃ¡c biá»ƒu diá»…n phÃ¢n biá»‡t á»Ÿ cáº¥p Ä‘á»™ thá»ƒ hiá»‡n; vÃ  phÃ¢n phá»‘i Ä‘áº·c trÆ°ng trung gian bá»‹ nhiá»…u sau thao tÃ¡c nÃ y, dáº«n Ä‘áº¿n hiá»‡u suáº¥t khÃ´ng tá»‘i Æ°u.

Äá»ƒ tiáº¿p cáº­n váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t táº­n dá»¥ng hoÃ n toÃ n mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c thÃ´ng qua hai mÃ´-Ä‘un. Viá»‡c triá»ƒn khai bao gá»“m hai nhÃ¡nh mÃ´ hÃ¬nh song song: má»™t bao gá»“m mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vá»›i cÃ¡c tham sá»‘ cá»‘ Ä‘á»‹nh trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c, trong khi nhÃ¡nh kia lÃ  mÃ´ hÃ¬nh háº¡ng tháº¥p vá»›i cÃ¡c tham sá»‘ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh. Dá»±a trÃªn khung nÃ y, mÃ´-Ä‘un Ä‘áº§u tiÃªn cá»§a chÃºng tÃ´i chÆ°ng cáº¥t kiáº¿n thá»©c Ä‘áº·c trÆ°ng tá»« mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c sang mÃ´ hÃ¬nh háº¡ng tháº¥p nÃ©n cá»§a chÃºng tÃ´i vá» má»—i lá»›p Transformer. MÃ´-Ä‘un khÃ¡c giÃºp rÃ ng buá»™c thay Ä‘á»•i trá»ng sá»‘ trong má»™t bÃ¡n kÃ­nh nhiá»…u Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c. Hai mÃ´-Ä‘un nÃ y giÃºp mÃ´ hÃ¬nh háº¡ng tháº¥p báº¯t chÆ°á»›c phÃ¢n phá»‘i Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c, tá»« Ä‘Ã³ tÄƒng cÆ°á»ng kháº£ nÄƒng phÃ¢n biá»‡t cá»§a nÃ³. Trong quÃ¡ trÃ¬nh tinh chá»‰nh, chÃºng tÃ´i Ä‘Æ¡n giáº£n sá»­ dá»¥ng mÃ´ hÃ¬nh háº¡ng tháº¥p nhÆ° má»™t sá»± thay tháº¿ cho mÃ´ hÃ¬nh lá»›n ban Ä‘áº§u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tham sá»‘ vÃ  tÃ­nh toÃ¡n cho cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u.

Theo nhÆ° chÃºng tÃ´i biáº¿t, tÃ i liá»‡u vá» viá»‡c Ä‘áº¡t Ä‘Æ°á»£c sá»± Ä‘Ã¡nh Ä‘á»•i hiá»‡u quáº£-hiá»‡u suáº¥t mong muá»‘n báº±ng cÃ¡ch sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p trÃªn cÃ¡c trá»ng sá»‘ Transformer Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c lÃ  khÃ¡ háº¡n cháº¿. ChÃºng tÃ´i Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh cho ba Transformer chá»‰ dÃ nh cho thá»‹ giÃ¡c tá»©c lÃ  DeiT [54], DeiT-III-Large [56] vÃ  SwinT [39], vá»›i 1/2 Ä‘áº¿n 2/3 tham sá»‘ cá»§a mÃ´ hÃ¬nh ban Ä‘áº§u; vÃ  má»™t Transformer thá»‹ giÃ¡c-ngÃ´n ngá»¯ - ALBEF [33] trong Ä‘Ã³ kÃ­ch thÆ°á»›c tham sá»‘ Ä‘Æ°á»£c giáº£m xuá»‘ng cÃ²n 1/3 cá»§a mÃ´ hÃ¬nh ban Ä‘áº§u. Sau Ä‘Ã³, chÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i trÃªn má»™t loáº¡t cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u, bao gá»“m phÃ¢n loáº¡i hÃ¬nh áº£nh, phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a, vÃ  phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng cho cÃ¡c Transformer chá»‰ dÃ nh cho thá»‹ giÃ¡c; Suy luáº­n thá»‹ giÃ¡c, Ä‘á»‹nh vá»‹ thá»‹ giÃ¡c, truy xuáº¥t Ä‘a phÆ°Æ¡ng thá»©c, vÃ  tráº£ lá»i cÃ¢u há»i thá»‹ giÃ¡c cho Transformer thá»‹ giÃ¡c-ngÃ´n ngá»¯. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cÃ³ tÃ­nh so sÃ¡nh cao vá»›i mÃ´ hÃ¬nh backbone, vá»›i sá»± khÃ¡c biá»‡t chá»§ yáº¿u khoáº£ng 0.6 Ä‘iá»ƒm, máº·c dÃ¹ chá»‰ sá»­ dá»¥ng 1/3 Ä‘áº¿n 2/3 FLOPs ban Ä‘áº§u. NgoÃ i ra, lá»£i Ã­ch hiá»‡u quáº£ tham sá»‘ nÃ y cÃ²n cho phÃ©p mÃ´ hÃ¬nh má»Ÿ rá»™ng vá»›i kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t cáº£i thiá»‡n Ä‘Ã´i khi tháº­m chÃ­ vÆ°á»£t trá»™i hÆ¡n cÃ¡c backbone.

2. CÃ´ng trÃ¬nh LiÃªn quan
2.1. Há»c Tiáº¿t kiá»‡m Tham sá»‘
Hiá»‡u quáº£ tá»« lÃ¢u Ä‘Ã£ lÃ  má»™t váº¥n Ä‘á» háº¥p dáº«n trong nhiá»u lÄ©nh vá»±c nghiÃªn cá»©u [26, 49]. Sau khi bÆ°á»›c vÃ o ká»· nguyÃªn há»c biá»ƒu diá»…n sÃ¢u, nhá»¯ng cáº£i tiáº¿n tiáº¿n bá»™ trong cá»™ng Ä‘á»“ng cá»§a chÃºng ta thÆ°á»ng pháº£i Ä‘Ã¡nh Ä‘á»•i vá»›i sá»‘ lÆ°á»£ng lá»›n tham sá»‘ mÃ´ hÃ¬nh, Ä‘á»™ trá»… vÃ  dáº¥u chÃ¢n [41, 58]. Vá»›i má»‘i quan tÃ¢m nÃ y, cÃ¡c ná»— lá»±c trÆ°á»›c Ä‘Ã¢y chá»§ yáº¿u dÃ nh cho ba hÆ°á»›ng phÃ¢n biá»‡t: chÆ°ng cáº¥t kiáº¿n thá»©c (KD), lÆ°á»£ng tá»­ hÃ³a, vÃ  cáº¯t tá»‰a. ÄÆ°á»£c coi lÃ  thuáº­t toÃ¡n nÃ©n mÃ´ hÃ¬nh cÃ³ nguyÃªn táº¯c, KD sá»›m nháº±m chuyá»ƒn giao kiáº¿n thá»©c tá»« má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn cá»“ng ká»nh sang má»™t mÃ´ hÃ¬nh sinh viÃªn nháº¹ thÃ´ng qua cÄƒn chá»‰nh logit lá»›p [20, 46]. Trá»ng tÃ¢m gáº§n Ä‘Ã¢y Ä‘Ã£ chuyá»ƒn sang chuyá»ƒn giao kiáº¿n thá»©c dá»±a trÃªn Ä‘áº·c trÆ°ng do lá»£i tháº¿ hiá»‡u suáº¥t cá»§a nÃ³ so vá»›i nhá»¯ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn logit thÃ´ng thÆ°á»ng [19, 25, 45, 69]. VÃ­ dá»¥, [25, 50] chÆ°ng cáº¥t kiáº¿n thá»©c tá»« cÃ¡c tráº¡ng thÃ¡i áº©n vÃ  ma tráº­n chÃº Ã½, mÃ  máº·t khÃ¡c, cÅ©ng cÃ³ thá»ƒ bá» qua cÃ¡c má»¥c tiÃªu Ä‘Ã o táº¡o khÃ´ng cÃ³ logit. Tuy nhiÃªn, viá»‡c chá»n Ä‘áº·c trÆ°ng tá»« nhá»¯ng lá»›p nÃ o Ä‘á»ƒ cÄƒn chá»‰nh váº«n cÃ²n thÃ¡ch thá»©c vÃ¬ khÃ´ng cÃ³ sá»± khá»›p lá»›p giÃ¡o viÃªn-sinh viÃªn tá»« cÆ¡ sá»Ÿ lÃ½ thuyáº¿t. LÆ°á»£ng tá»­ hÃ³a, tá»« má»™t gÃ³c Ä‘á»™ khÃ¡c cá»§a há»c hiá»‡u quáº£, Ã¡nh xáº¡ cÃ¡c tham sá»‘ bit lá»›n hÆ¡n thÃ nh nhá»¯ng tham sá»‘ nhá» hÆ¡n, vÃ­ dá»¥: sá»‘ thá»±c dáº¥u pháº©y Ä‘á»™ng 32-bit thÃ nh sá»‘ nguyÃªn 8-bit [44]. Loáº¡i phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng phá»¥ thuá»™c vÃ o cáº¥u trÃºc mÃ´ hÃ¬nh, Ä‘iá»u nÃ y lÃ m cho nÃ³ linh hoáº¡t trong cÃ¡c máº¡ng neural khÃ¡c nhau [7, 22, 36]. NhÆ°á»£c Ä‘iá»ƒm chÃ­nh náº±m á»Ÿ viá»‡c giáº£m hiá»‡u suáº¥t vÃ  kháº£ nÄƒng khÃ´ng kháº£ thi cho lan truyá»n ngÆ°á»£c. KhÃ¡c vá»›i hai loáº¡i trÃªn, cáº¯t tá»‰a Ä‘Æ°á»£c táº­n dá»¥ng Ä‘á»ƒ loáº¡i bá» cÃ¡c thÃ nh pháº§n khÃ´ng cáº§n thiáº¿t hoáº·c Ã­t quan trá»ng trong cÃ¡c mÃ´ hÃ¬nh [58]. Báº±ng cÃ¡ch loáº¡i bá» má»™t sá»‘ káº¿t ná»‘i [65] hoáº·c tham sá»‘ [29], máº¡ng dÃ y Ä‘áº·c ban Ä‘áº§u giáº£m thÃ nh má»™t máº¡ng thÆ°a thá»›t, trong Ä‘Ã³ dung lÆ°á»£ng cáº§n thiáº¿t Ä‘á»ƒ lÆ°u trá»¯ cÅ©ng nhÆ° lÆ°á»£ng tÃ­nh toÃ¡n sáº½ giáº£m Ä‘i.

CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Transformer Ä‘Ã£ thÃ nh cÃ´ng trong cÃ¡c lÄ©nh vá»±c nghiÃªn cá»©u Ä‘a dáº¡ng ká»ƒ tá»« khi Ä‘Æ°á»£c giá»›i thiá»‡u [9, 57]. Nhá»¯ng mÃ´ hÃ¬nh nÃ y thÆ°á»ng liÃªn quan Ä‘áº¿n hÃ ng tá»· tham sá»‘, do Ä‘Ã³, thÃºc Ä‘áº©y má»™t sá»‘ phÆ°Æ¡ng phÃ¡p cá»¥ thá»ƒ lÃ m viá»‡c Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u quáº£ tham sá»‘ [30]. Chiáº¿n lÆ°á»£c Ä‘iá»ƒn hÃ¬nh lÃ  thÃªm má»™t sá»‘ tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c trong khi cá»‘ Ä‘á»‹nh pháº§n lá»›n backbone Transformer trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o háº¡ lÆ°u. VÃ­ dá»¥, prompt tuning thÃªm má»™t sá»‘ tham sá»‘ cá»¥ thá»ƒ cho tÃ¡c vá»¥ vÃ o khÃ´ng gian Ä‘áº§u vÃ o [24]; cÃ¡c mÃ´ hÃ¬nh Adapter giá»›i thiá»‡u má»™t sá»‘ thÃ nh pháº§n MLP cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ o má»—i lá»›p Transformer [51]; vÃ  viá»‡c tinh chá»‰nh chá»‰ bias cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ Ä‘á»ƒ duy trÃ¬ hiá»‡u suáº¥t tá»‘t cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n [72].

2.2. Xáº¥p xá»‰ Háº¡ng Tháº¥p
Xáº¥p xá»‰ háº¡ng tháº¥p nháº±m phÃ¢n tÃ¡ch má»™t ma tráº­n thÃ nh hai ma tráº­n nhá» hÆ¡n, tuÃ¢n theo rÃ ng buá»™c ráº±ng cÃ¡c ma tráº­n káº¿t quáº£ cÃ³ háº¡ng giáº£m [47, 48]. Má»™t Æ°u Ä‘iá»ƒm chÃ­nh cá»§a thuáº­t toÃ¡n nÃ y lÃ  nÃ©n dá»¯ liá»‡u, theo Ä‘Ã³ cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ Ã¡p dá»¥ng nÃ³ cho phÃ¢n tÃ­ch thÃ nh pháº§n chÃ­nh [42] vÃ  Ä‘á» xuáº¥t [13, 18].

LiÃªn quan Ä‘áº¿n Máº¡ng Neural TÃ­ch cháº­p (CNN), má»™t sá»‘ phÆ°Æ¡ng phÃ¡p Ã¡p dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p cho má»—i báº£n Ä‘á»“ Ä‘áº·c trÆ°ng thÃ´ng qua phÃ¢n tÃ¡ch tensor báº­c cao hÆ¡n [12, 52, 73]. Viá»‡c phÃ¢n tÃ¡ch Ä‘á»™ng cÃ¡c ma tráº­n cÃ³ thá»ƒ huáº¥n luyá»‡n cÅ©ng Ä‘Ã£ thu hÃºt nhiá»u sá»± chÃº Ã½ [66, 68, 70]. Má»™t sá»‘ nghiÃªn cá»©u khÃ¡c Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c khÃ­a cáº¡nh khÃ¡c cá»§a xáº¥p xá»‰ háº¡ng tháº¥p, nhÆ° há»c háº¡ng [23], tá»‘i Æ°u hÃ³a rÃ ng buá»™c [32], vÃ  sá»­ dá»¥ng nÃ³ cá»¥ thá»ƒ trong ma tráº­n nhÃºng token [5, 30] hoáº·c tÃ­nh toÃ¡n tá»± chÃº Ã½ trong Transformer [60]. LoRA [21] mÃ´ hÃ¬nh hÃ³a pháº§n dÆ° cá»§a cÃ¡c tham sá»‘ vá»›i xáº¥p xá»‰ háº¡ng tháº¥p, trong Ä‘Ã³ chá»‰ cÃ¡c ma tráº­n Ä‘Æ°á»£c phÃ¢n tÃ¡ch má»›i Ä‘Æ°á»£c khai thÃ¡c Ä‘á»ƒ Ä‘Ã o táº¡o háº¡ lÆ°u vÃ  do Ä‘Ã³ nÃ³ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n giáº£m Ä‘Ã¡ng ká»ƒ. Máº·c dÃ¹ cÃ³ lá»£i Ã­ch, phÆ°Æ¡ng phÃ¡p LoRA váº«n cÃ³ háº¡n cháº¿, vÃ¬ nÃ³ Ä‘Ã²i há»i viá»‡c lÆ°u trá»¯ vÃ  táº£i láº¡i cÃ¡c trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c lá»›n trong Ä‘Ä©a cá»©ng vÃ  bá»™ nhá»› GPU tÆ°Æ¡ng á»©ng. NÃ³i cÃ¡ch khÃ¡c, chá»‰ cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n má»›i Ä‘Æ°á»£c giá»›i thiá»‡u cÃ³ cÆ°á»ng Ä‘á»™ nhá» hÆ¡n so vá»›i cÃ¡c tham sá»‘ Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c cáº­p nháº­t cho tinh chá»‰nh, lÃ m cho nÃ³ tÆ°Æ¡ng tá»± nhÆ° adapter [14, 51] vÃ  prompt tuning [24]. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c Ä‘á»ƒ hoÃ n toÃ n thay tháº¿ cÃ¡c trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c báº±ng cÃ¡c ma tráº­n háº¡ng tháº¥p giáº£m. Káº¿t quáº£ lÃ , chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c cáº£ má»¥c tiÃªu hiá»‡u quáº£ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cho cÃ¡c tÃ¡c vá»¥ tinh chá»‰nh háº¡ lÆ°u.

3. PhÆ°Æ¡ng phÃ¡p
Transformer Ä‘Ã£ phÃ¡t triá»ƒn thÃ nh má»™t khá»‘i xÃ¢y dá»±ng cÆ¡ báº£n cá»§a nhiá»u mÃ´ hÃ¬nh thá»‹ giÃ¡c hiá»‡n Ä‘áº¡i [10, 17]. Láº¥y Vision Transformer (ViT) tinh tÃºy lÃ m vÃ­ dá»¥. ViT Ä‘áº§u tiÃªn chia má»™t hÃ¬nh áº£nh RGB IâˆˆRÂ³Ã—HÃ—W thÃ nh MÃ—M patch khÃ´ng chá»“ng láº¥p. CÃ¹ng vá»›i má»™t token lá»›p, cÃ¡c patch hÃ¬nh áº£nh nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o N lá»›p vá»›i tá»± chÃº Ã½ nhÆ° hoáº¡t Ä‘á»™ng cÆ¡ báº£n. Äá»ƒ lÃ m Ä‘iá»u nÃ y, má»™t táº­p há»£p cÃ¡c ma tráº­n truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ Ä‘Æ°á»£c biáº¿n Ä‘á»•i tá»« nhÃºng patch thÃ nh cÃ¡c Ä‘áº·c trÆ°ng token XâˆˆR^(MÂ²+1)Ã—d, trong Ä‘Ã³ d biá»ƒu thá»‹ kÃ­ch thÆ°á»›c nhÃºng, theo sau lÃ  má»™t sá»‘ lá»›p feedforward vÃ  káº¿t ná»‘i dÆ°. Cá»‘t lÃµi cá»§a chÃºng náº±m á»Ÿ lá»›p Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§, thÆ°á»ng Ä‘Æ°á»£c bao bá»c trong viá»‡c Æ°á»›c tÃ­nh Ä‘iá»ƒm chÃº Ã½ vÃ  cÃ¡c hoáº¡t Ä‘á»™ng MLP - W^T X+b, trong Ä‘Ã³ WâˆˆR^(dinÃ—dout) lÃ  ma tráº­n trá»ng sá»‘ cÃ³ thá»ƒ há»c vÃ  bâˆˆR^dout biá»ƒu thá»‹ bias, vÃ  din=d cho lá»›p Ä‘áº§u tiÃªn.

3.1. Xáº¥p xá»‰ Háº¡ng Tháº¥p
QuÃ¡ tham sá»‘ hÃ³a lÃ  má»™t váº¥n Ä‘á» phá»• biáº¿n trong cÃ¡c mÃ´ hÃ¬nh lá»›n hiá»‡n Ä‘áº¡i [1]. Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i nháº±m giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch giáº£m sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh. ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« sá»± thÃ nh cÃ´ng cá»§a xáº¥p xá»‰ háº¡ng tháº¥p trong cÃ¡c lÄ©nh vá»±c khÃ¡c [12, 52], chÃºng tÃ´i Ä‘á» xuáº¥t Ã¡p dá»¥ng ká»¹ thuáº­t nÃ y trá»±c tiáº¿p cho cÃ¡c phÃ©p toÃ¡n nhÃ¢n ma tráº­n trong ViT,

Baseline Low-Rank PELA 4050607080 Accuracy (%)
Thay Ä‘á»•i Hiá»‡u suáº¥t
0.0 0.5 1.0 0123 Baseline
0.0 0.5 1.0 0123 Low-Rank

HÃ¬nh 2. So sÃ¡nh hiá»‡u suáº¥t cá»§a ba mÃ´ hÃ¬nh vÃ  thá»‘ng kÃª tÆ°Æ¡ng tá»± Ä‘áº·c trÆ°ng cáº¥p Ä‘á»™ thá»ƒ hiá»‡n. TrÃ¡i: ChÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh DeiT lÃ m baseline vÃ  hiá»ƒn thá»‹ hiá»‡u suáº¥t cá»§a cÃ¡c biáº¿n thá»ƒ xáº¥p xá»‰ háº¡ng tháº¥p trá»±c tiáº¿p vÃ  PELA cá»§a nÃ³. CÃ¡c hÃ¬nh con giá»¯a vÃ  pháº£i minh há»a tÆ°Æ¡ng tá»± Ä‘áº·c trÆ°ng cáº¥p Ä‘á»™ thá»ƒ hiá»‡n cá»§a cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh DeiT vÃ  háº¡ng tháº¥p trá»±c tiáº¿p tÆ°Æ¡ng á»©ng.

W^T X â‰ˆ (UV^T)^T X
= V(U^T X), (1)

trong Ä‘Ã³ UâˆˆR^(dinÃ—dlr) vÃ  VâˆˆR^(doutÃ—dlr) lÃ  cÃ¡c ma tráº­n háº¡ng tháº¥p, vÃ  dlr Ä‘áº¡i diá»‡n cho háº¡ng mong muá»‘n cá»§a W. LÆ°u Ã½ ráº±ng cÃ¡c ma tráº­n trá»ng sá»‘ trong má»™t mÃ´ hÃ¬nh há»c sÃ¢u thÆ°á»ng cÃ³ háº¡ng Ä‘áº§y Ä‘á»§, tá»©c lÃ  rank(W) = min(din, dout). Trong Ä‘iá»u kiá»‡n nhÆ° váº­y, chÃºng tÃ´i tÃ¬m cÃ¡ch xáº¥p xá»‰ báº±ng ma tráº­n ban Ä‘áº§u vÃ  cá»‘ Ã½ chá»n dlr nhá» hÆ¡n, vÃ­ dá»¥: 1/4 min(din, dout). PhÆ°Æ¡ng trÃ¬nh thá»© hai luÃ´n Ä‘Ãºng trong cÃ¡c máº¡ng neural do luáº­t káº¿t há»£p tá»± nhiÃªn. TÃ­nh cháº¥t nÃ y cho phÃ©p chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tÃ­nh toÃ¡n mÃ  khÃ´ng cáº§n khÃ´i phá»¥c ma tráº­n trá»ng sá»‘ ban Ä‘áº§u W sau khi Ã¡p dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p. ChÃºng tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p SVD ná»•i tiáº¿ng [31] Ä‘á»ƒ thá»±c hiá»‡n xáº¥p xá»‰ háº¡ng tháº¥p nhÆ° sau:

SVD(W^T) = U*Î£V*, (2)

trong Ä‘Ã³ Î£âˆˆR^(dinÃ—dout) lÃ  ma tráº­n chÃ©o hÃ¬nh chá»¯ nháº­t vá»›i cÃ¡c sá»‘ thá»±c khÃ´ng Ã¢m trÃªn Ä‘Æ°á»ng chÃ©o, vÃ  cÃ¡c giÃ¡ trá»‹ ká»³ dá»‹ Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± giáº£m dáº§n Ä‘Æ¡n Ä‘iá»‡u; U*âˆˆR^(dinÃ—din) vÃ  V*âˆˆR^(doutÃ—dout) lÃ  cÃ¡c ma tráº­n unitary phá»©c. Sau Ä‘Ã³ chÃºng tÃ´i chÃ­nh thá»©c hÃ³a cÃ¡c ma tráº­n háº¡ng tháº¥p báº±ng cÃ¡ch sá»­ dá»¥ng phÃ©p biáº¿n Ä‘á»•i sau:

U = U*[:,:dlr]Î£^(1/2)[:dlr,:dlr],
V = (Î£^(1/2)[:dlr,:dlr]V*[:dlr,:])^T, (3)

trong Ä‘Ã³ [:,:dlr] cÃ³ nghÄ©a lÃ  chÃºng tÃ´i cáº¯t bá»›t ma tráº­n Ä‘Ã£ cho vá»›i dlr cá»™t hÃ ng Ä‘áº§u vÃ  cÃ¡c thao tÃ¡c cáº¯t bá»›t khÃ¡c cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c suy ra dá»… dÃ ng.

Quan sÃ¡t sÆ¡ bá»™. ChÃºng tÃ´i Ã¡p dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p nÃ y cho cÃ¡c lá»›p Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Tháº­t khÃ´ng may, quÃ¡ trÃ¬nh nÃ y mang láº¡i káº¿t quáº£ kÃ©m mong muá»‘n hÆ¡n, vÃ­ dá»¥: Ä‘á»™ chÃ­nh xÃ¡c giáº£m tá»« 81% xuá»‘ng 61% nhÆ° tháº¥y trong HÃ¬nh 2. Äiá»u nÃ y chá»‰ ra ráº±ng mÃ´ hÃ¬nh háº¡ng tháº¥p nÃ©n khÃ´ng há»c hiá»‡u quáº£ biá»ƒu diá»…n phÃ¢n biá»‡t cáº¥p Ä‘á»™ thá»ƒ hiá»‡n. HÆ¡n ná»¯a, chÃºng tÃ´i tháº¥y ráº±ng cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c há»c sau háº¡ng tháº¥p bá»‹ giá»›i háº¡n trong má»™t khÃ´ng gian Ä‘áº·c trÆ°ng háº¹p. Cá»¥ thá»ƒ, hai hÃ¬nh con bÃªn pháº£i trong HÃ¬nh 2 cho tháº¥y ráº±ng tÆ°Æ¡ng tá»± Ä‘áº·c trÆ°ng cá»§a má»—i lá»›p cá»§a mÃ´ hÃ¬nh háº¡ng tháº¥p cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i trÆ°á»›c Ä‘Ã¢y.

Äá»ƒ vÆ°á»£t qua Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t táº­n dá»¥ng hoÃ n toÃ n mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vÃ  khai thÃ¡c nÃ³ Ä‘á»ƒ hÆ°á»›ng dáº«n viá»‡c Ä‘Ã o táº¡o mÃ´ hÃ¬nh háº¡ng tháº¥p. Cá»¥ thá»ƒ, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 3, Ä‘áº§u tiÃªn chÃºng tÃ´i thá»±c hiá»‡n xáº¥p xá»‰ háº¡ng tháº¥p trÃªn mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vÃ  giá»¯ láº¡i cáº£ hai mÃ´ hÃ¬nh. CÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh trong khi chÃºng tÃ´i chá»‰ Ä‘Ã o táº¡o mÃ´ hÃ¬nh háº¡ng tháº¥p. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i bao gá»“m thÃªm hai mÃ´-Ä‘un: chÆ°ng cáº¥t Ä‘áº·c trÆ°ng Ä‘á»ƒ cÄƒn chá»‰nh cÃ¡c Ä‘áº·c trÆ°ng giá»¯a hai mÃ´ hÃ¬nh nÃ y, nhiá»…u trá»ng sá»‘ chÃ­nh quy Ä‘á»ƒ rÃ ng buá»™c má»‘i quan há»‡ cá»§a ma tráº­n khÃ´i phá»¥c vÃ  ma tráº­n ban Ä‘áº§u. ChÃºng tÃ´i Ä‘áº·t tÃªn phÆ°Æ¡ng phÃ¡p nÃ y lÃ  PELA, Ä‘Æ°á»£c gá»i lÃ  cÃ¡c mÃ´ hÃ¬nh Tiáº¿t kiá»‡m Tham sá»‘ cho Xáº¥p xá»‰ Háº¡ng Tháº¥p. Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, cÃ³ nghiÃªn cá»©u háº¡n cháº¿ vá» viá»‡c xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh háº¡ng tháº¥p hiá»‡u quáº£ dá»±a trÃªn Transformer Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Do Ä‘Ã³, chÃºng tÃ´i nháº±m giáº£i quyáº¿t khoáº£ng trá»‘ng nÃ y báº±ng cÃ¡ch Ä‘iá»u tra tiá»m nÄƒng cá»§a xáº¥p xá»‰ háº¡ng tháº¥p Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± Ä‘Ã¡nh Ä‘á»•i hiá»‡u quáº£-hiá»‡u suáº¥t tá»‘i Æ°u.

3.2. ChÆ°ng cáº¥t Äáº·c trÆ°ng
NhÆ° Ä‘Ã£ nháº¥n máº¡nh trong tiá»ƒu má»¥c trÆ°á»›c, xáº¥p xá»‰ háº¡ng tháº¥p cÃ³ thá»ƒ thay Ä‘á»•i phÃ¢n phá»‘i Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i sá»­ dá»¥ng chÆ°ng cáº¥t kiáº¿n thá»©c dá»±a trÃªn Ä‘áº·c trÆ°ng, Ä‘iá»u nÃ y Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ trong viá»‡c cÄƒn chá»‰nh cÃ¡c Ä‘áº·c trÆ°ng giá»¯a cÃ¡c mÃ´ hÃ¬nh [45]. Tuy nhiÃªn, viá»‡c nÃ©n háº¡ng tháº¥p Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn má»—i phÃ©p toÃ¡n nhÃ¢n ma tráº­n, thay vÃ¬ cÃ¡c khá»‘i hoáº·c lá»›p Transformer cá»¥ thá»ƒ. Viá»‡c trá»±c tiáº¿p chÆ°ng cáº¥t kiáº¿n thá»©c tá»« táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh ban Ä‘áº§u dáº«n Ä‘áº¿n nhiá»u lá»™n xá»™n hÆ¡n vÃ¬ má»™t sá»‘ nÃ©n háº¡ng tháº¥p Ä‘Ã£ Ä‘Æ°á»£c bao bá»c trong tÃ­nh toÃ¡n tá»± chÃº Ã½. Nhá» káº¿t ná»‘i dÆ° theo lá»›p cá»§a Transformer, chÃºng tÃ´i sá»­ dá»¥ng má»™t sá»± thá»a hiá»‡p trong cÃ´ng viá»‡c nÃ y - Ä‘Æ¡n giáº£n lÃ  cÄƒn chá»‰nh cÃ¡c Ä‘áº·c trÆ°ng token cá»§a má»—i lá»›p. Tá»« cÃ¡i nhÃ¬n tá»•ng quÃ¡t vá» cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘iá»ƒn hÃ¬nh, máº¥t mÃ¡t chÆ°ng cáº¥t Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

Lfd = Î£(i=1 to N) D(Ms(Xi_s), Mt(Xi_t)),
= 1/2N Î£(i=1 to N) ||Ms(Xi_s) - Mt(Xi_t)||Â², (4)

trong Ä‘Ã³ Xi_s vÃ  Xi_t biá»ƒu thá»‹ cÃ¡c Ä‘áº·c trÆ°ng token lá»›p thá»© i cá»§a mÃ´ hÃ¬nh háº¡ng tháº¥p nÃ©n vÃ  mÃ´ hÃ¬nh ban Ä‘áº§u tÆ°Æ¡ng á»©ng; M lÃ  má»™t phÃ©p biáº¿n Ä‘á»•i chuyá»ƒn Ä‘á»•i Ä‘áº·c trÆ°ng sang khÃ´ng gian Ä‘áº·c trÆ°ng má»¥c tiÃªu vÃ  chÃºng tÃ´i sá»­ dá»¥ng Ã¡nh xáº¡ Ä‘á»“ng nháº¥t trong triá»ƒn khai cá»§a chÃºng tÃ´i. Báº±ng cÃ¡ch nÃ y, cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u ra tá»« má»—i lá»›p ViT cá»§a mÃ´ hÃ¬nh háº¡ng tháº¥p Ä‘Æ°á»£c mong Ä‘á»£i sáº½ chia sáº» phÃ¢n phá»‘i tÆ°Æ¡ng tá»± vá»›i mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c tÆ°Æ¡ng á»©ng.

Má»™t cÃ¡i nhÃ¬n thay tháº¿ tá»« chÆ°ng cáº¥t kiáº¿n thá»©c. CÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ chá»‰ ra ráº±ng chÆ°ng cáº¥t kiáº¿n thá»©c dá»±a trÃªn Ä‘áº·c trÆ°ng vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i chÆ°ng cáº¥t dá»±a trÃªn logit thÃ´ng thÆ°á»ng [19]. Tuy nhiÃªn, cÃ¡ch thiáº¿t káº¿ má»™t mÃ´ hÃ¬nh sinh viÃªn vÃ  chuyá»ƒn giao kiáº¿n thá»©c tá»« mÃ´ hÃ¬nh giÃ¡o viÃªn váº«n cÃ²n thÃ¡ch thá»©c vÃ¬ khÃ¡ khÃ³ khÄƒn Ä‘á»ƒ xÃ¡c Ä‘á»‹nh khá»›p Ä‘áº·c trÆ°ng giÃ¡o viÃªn-sinh viÃªn. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cung cáº¥p má»™t giáº£i phÃ¡p gá»n gÃ ng cho váº¥n Ä‘á» nÃ y do hai lÃ½ do sau: 1) KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y (vÃ­ dá»¥: [25, 50] loáº¡i bá» thá»§ cÃ´ng má»™t sá»‘ lá»›p nháº¥t Ä‘á»‹nh), xáº¥p xá»‰ háº¡ng tháº¥p lÃ  khÃ´ng tá»‘n cÃ´ng sá»©c vÃ  Ä‘Æ¡n giáº£n Ä‘á»ƒ nÃ©n mÃ´ hÃ¬nh giÃ¡o viÃªn cá»“ng ká»nh thÃ nh má»™t mÃ´ hÃ¬nh sinh viÃªn nháº¹. 2) Tá»“n táº¡i má»™t sá»± tÆ°Æ¡ng á»©ng tá»± nhiÃªn giá»¯a mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  mÃ´ hÃ¬nh sinh viÃªn vÃ¬ chÃºng tÃ´i chÆ°a thay Ä‘á»•i quÃ¡ má»©c cÃ¡c kiáº¿n trÃºc mÃ´ hÃ¬nh.

3.3. Nhiá»…u Trá»ng sá»‘ ChÃ­nh quy
Má»™t xáº¥p xá»‰ háº¡ng tháº¥p lÃ½ tÆ°á»Ÿng lÃ  há»c má»™t ma tráº­n xáº¥p xá»‰ cá»§a ma tráº­n ban Ä‘áº§u tuÃ¢n theo rÃ ng buá»™c háº¡ng giáº£m. Äiá»u nÃ y dáº«n Ä‘áº¿n má»™t tiáº¿n thoÃ¡i lÆ°á»¡ng nan hiá»‡u quáº£-hiá»‡u suáº¥t - Má»™t háº¡ng lá»›n hÆ¡n tÆ°Æ¡ng á»©ng vá»›i má»™t lá»—i tÃ¡i táº¡o tháº¥p hÆ¡n vÃ  ngÆ°á»£c láº¡i. Trá»±c quan, chÃºng tÃ´i liÃªn káº¿t viá»‡c tÃ¡i táº¡o ma tráº­n vá»›i viá»‡c nhiá»…u trá»ng sá»‘, Ä‘iá»u nÃ y tÆ°Æ¡ng Ä‘á»‘i má»›i so vá»›i váº¥n Ä‘á» Ä‘á»™ bá»n nhiá»…u Ä‘áº·c trÆ°ng/Ä‘áº§u vÃ o [62]. Káº¿t quáº£ lÃ , má»™t háº¡ng nhá» hÆ¡n trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i, tá»« gÃ³c Ä‘á»™ khÃ¡c, cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  nhiá»u nhiá»…u trá»ng sá»‘ hÆ¡n. Äá»ƒ giáº£m áº£nh hÆ°á»Ÿng tiÃªu cá»±c cá»§a nhá»¯ng tham sá»‘ bá»‹ nhiá»…u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng chuáº©n lâˆ Ä‘á»ƒ rÃ ng buá»™c lá»—i tÃ¡i táº¡o:

{||Å´(k) - W(k)||âˆ â‰¤ Îµ,
Å´(k) = U(k)(V(k))T, âˆ€k âˆˆ [K]} (5)

trong Ä‘Ã³ Îµ Ä‘áº¡i diá»‡n cho bÃ¡n kÃ­nh nhiá»…u, W(k) lÃ  ma tráº­n trá»ng sá»‘ ban Ä‘áº§u, vÃ  [K] biá»ƒu thá»‹ táº­p chá»‰ sá»‘ trá»ng sá»‘. Cho Îµ, viá»‡c báº£o tá»“n Ä‘á»™ bá»n máº¡ng neural chá»‘ng láº¡i nhiá»…u trá»ng sá»‘ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t nhÆ° váº¥n Ä‘á» tá»‘i Æ°u hÃ³a sau [62]:

Lrwp = Î£(k=1 to |[K]|) (||Å´(k) - W(k)||âˆ - Îµ). (6)

3.4. ÄÃ o táº¡o
Hai mÃ´-Ä‘un trÃªn cho phÃ©p chÃºng tÃ´i náº¯m báº¯t kháº£ nÄƒng phÃ¢n biá»‡t háº¥p dáº«n cá»§a cÃ¡c mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Äá»ƒ cÃ³ Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh háº¡ng tháº¥p nhá» gá»n, chÃºng tÃ´i xem xÃ©t toÃ n diá»‡n cÃ¡c má»¥c tiÃªu tá»« cáº£ Ä‘Ã o táº¡o trÆ°á»›c cÆ¡ báº£n vÃ  hai mÃ´-Ä‘un Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i:

L = Lbase + Î±Lfd + Î²Lrwp, (7)

trong Ä‘Ã³ Î± vÃ  Î² lÃ  cÃ¡c siÃªu tham sá»‘ trá»ng sá»‘ máº¥t mÃ¡t vÃ  Lbase lÃ  cÃ¡c hÃ m máº¥t mÃ¡t cá»§a cÃ¡c tÃ¡c vá»¥ Ä‘Ã o táº¡o trÆ°á»›c ban Ä‘áº§u. NÃ³ cÃ³ thá»ƒ lÃ  máº¥t mÃ¡t phÃ¢n loáº¡i cá»§a má»™t ViT Ä‘iá»ƒn hÃ¬nh, hoáº·c cÃ¡c máº¥t mÃ¡t khá»›p thá»‹ giÃ¡c-vÄƒn báº£n vÃ  mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ cÃ³ mask cá»§a má»™t mÃ´ hÃ¬nh thá»‹ giÃ¡c-ngÃ´n ngá»¯. Sau Ä‘Ã³ chÃºng tÃ´i tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh cá»§a mÃ¬nh trÃªn cÃ¹ng cÃ¡c táº­p dá»¯ liá»‡u nhÆ° mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c, cháº³ng háº¡n nhÆ° ImageNet [8]. Sau giai Ä‘oáº¡n Ä‘Ã o táº¡o trÆ°á»›c trung gian nÃ y, mÃ´ hÃ¬nh háº¡ng tháº¥p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c triá»ƒn khai má»™t cÃ¡ch suÃ´n sáº» cho tinh chá»‰nh háº¡ lÆ°u vÃ¬ kiáº¿n trÃºc mÃ´ hÃ¬nh hiáº¿m khi Ä‘Æ°á»£c thay Ä‘á»•i. TrÃ¡i ngÆ°á»£c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ nhÆ° prompt tuning [24], adapter [51], vÃ  LoRA [21], cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y yÃªu cáº§u cáº£ mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c vÃ  cÃ¡c tham sá»‘ tinh chá»‰nh, chÃºng tÃ´i chá»‰ giá»¯ mÃ´ hÃ¬nh háº¡ng tháº¥p Ä‘á»ƒ suy luáº­n hiá»‡u quáº£ vÃ  sá»­ dá»¥ng tham sá»‘ (xem HÃ¬nh 3 Ä‘á»ƒ so sÃ¡nh trá»±c quan).

3.5. PhÃ¢n tÃ­ch Äá»™ phá»©c táº¡p
TrÆ°á»›c khi phÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i, Ä‘áº§u tiÃªn chÃºng tÃ´i Ä‘á»ƒ dlr = 1/Îº Ã— dinÃ—dout/(din+dout), trong Ä‘Ã³ Îº lÃ  má»™t sá»‘ dÆ°Æ¡ng vÃ  chÃºng tÃ´i Ä‘áº·t tÃªn nÃ³ lÃ  tá»‰ lá»‡ nÃ©n. ChÃºng tÃ´i chá»n sá»­ dá»¥ng tá»‰ lá»‡ nÃ©n phá»• quÃ¡t cho táº¥t cáº£ cÃ¡c phÃ©p toÃ¡n nhÃ¢n ma tráº­n Ä‘á»ƒ Ä‘Æ¡n giáº£n trong khi Ä‘á»ƒ láº¡i viá»‡c khÃ¡m phÃ¡ cÃ¡c tá»‰ lá»‡ Ä‘á»™ng cho cÃ¡c lá»›p khÃ¡c nhau nhÆ° cÃ´ng viá»‡c tÆ°Æ¡ng lai.

HÃ£y xem xÃ©t trÆ°á»ng há»£p Îº = 2 vÃ  má»™t Ä‘áº·c trÆ°ng patch Ä‘Æ¡n xâˆˆR^din cho tinh chá»‰nh háº¡ lÆ°u. Nhá»› láº¡i PhÆ°Æ¡ng trÃ¬nh 1, viá»‡c nhÃ¢n ma tráº­n ban Ä‘áº§u máº¥t O(din Ã— dout) Ä‘á»ƒ thao tÃ¡c. Tuy nhiÃªn, vá»›i phÆ°Æ¡ng phÃ¡p PELA cá»§a chÃºng tÃ´i, Ä‘á»™ phá»©c táº¡p thá»i gian nÃ y giáº£m xuá»‘ng O((din + dout) Ã— dlr) = 1/2 O(din Ã— dout). TÆ°Æ¡ng tá»±, vÃ¬ hoáº¡t Ä‘á»™ng chá»§ yáº¿u trong cÃ¡c Transformer hiá»‡n táº¡i lÃ  nhÃ¢n ma tráº­n (loáº¡i trá»« má»™t sá»‘ tham sá»‘ chuáº©n hÃ³a lá»›p ráº¥t Ã­t vÃ  tham sá»‘ bias), kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh do Ä‘Ã³ cÅ©ng giáº£m khoáº£ng má»™t ná»­a tá»« quy mÃ´ ban Ä‘áº§u cá»§a nÃ³. ÄÃ¢y lÃ  lÃ½ do táº¡i sao phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ gáº§n Ä‘Ã¢y khÃ¡c nhÆ° LoRA [21], trong Ä‘Ã³ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tá»•ng thá»ƒ thá»±c sá»± tÄƒng lÃªn.

4. ThÃ­ nghiá»‡m
4.1. CÃ¡c Baseline Há»c Hiá»‡u quáº£ Phá»• biáº¿n
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ PELA cá»§a chÃºng tÃ´i so vá»›i bá»‘n baseline hiá»‡u quáº£: TinyBERT [25] vÃ  MaskAlign [67] tá»« nhÃ³m chÆ°ng cáº¥t kiáº¿n thá»©c dá»±a trÃªn Ä‘áº·c trÆ°ng; ToMe [2] - má»™t phÆ°Æ¡ng phÃ¡p cáº¯t tá»‰a token thá»‹ giÃ¡c máº¡nh máº½ gáº§n Ä‘Ã¢y; vÃ  LoRA [21], má»™t baseline há»c chuyá»ƒn giao tiáº¿t kiá»‡m tham sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i. Tuy nhiÃªn, chÃºng tÃ´i loáº¡i trá»« má»™t sá»‘ thÃ­ nghiá»‡m do má»™t sá»‘ khÃ´ng tÆ°Æ¡ng thÃ­ch nháº¥t Ä‘á»‹nh, cháº³ng háº¡n nhÆ° sá»­ dá»¥ng ToMe cho mÃ´ hÃ¬nh Swin vÃ  cho tÃ¡c vá»¥ Ä‘á»‹nh vá»‹ thá»‹ giÃ¡c.

4.2. ThÃ­ nghiá»‡m trÃªn MÃ´ hÃ¬nh Chá»‰ dÃ nh cho Thá»‹ giÃ¡c
4.2.1 MÃ´ hÃ¬nh Baseline vÃ  Káº¿t quáº£
ChÃºng tÃ´i Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh cho cÃ¡c mÃ´ hÃ¬nh DeiT-Base [54] vÃ  Swin-Base [39] Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i. Äá»ƒ Ä‘áº£m báº£o bao phá»§ toÃ n diá»‡n, chÃºng tÃ´i cÅ©ng chá»n DeiT-III-Large [56] lá»›n hÆ¡n vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  Ä‘Ã²i há»i thá»i gian Ä‘Ã o táº¡o lÃ¢u hÆ¡n nhiá»u. Tá»‰ lá»‡ nÃ©n lÃ  1/2 vÃ  1/3 cho cÃ¡c mÃ´ hÃ¬nh DeiT vÃ  Swin tÆ°Æ¡ng á»©ng. Sau xáº¥p xá»‰ háº¡ng tháº¥p, chÃºng tÃ´i Ä‘Ã o táº¡o mÃ´ hÃ¬nh cá»§a mÃ¬nh trÃªn táº­p dá»¯ liá»‡u ImageNet-1k [8] vÃ  Ä‘Ã¡nh giÃ¡ nÃ³ trÃªn táº­p validation tÆ°Æ¡ng á»©ng, vÃ  bÃ¡o cÃ¡o káº¿t quáº£ trong Báº£ng 1. NhÆ° mong Ä‘á»£i, cÃ¡c tham sá»‘ mÃ´ hÃ¬nh vÃ  FLOPs cho suy luáº­n Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ theo má»—i tá»‰ lá»‡ nÃ©n tÆ°Æ¡ng á»©ng. Máº·t khÃ¡c, Ä‘á»™ chÃ­nh xÃ¡c giáº£m cá»§a hai mÃ´ hÃ¬nh cÆ¡ sá»Ÿ lÃ  0.8% vÃ  1.0% tÆ°Æ¡ng á»©ng. Tháº­m chÃ­ Ä‘á»‘i vá»›i mÃ´ hÃ¬nh tÆ°Æ¡ng Ä‘á»‘i lá»›n hÆ¡n DeiT-III-Large, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i chá»‰ Ä‘Ã¡nh Ä‘á»•i 1.0% Ä‘á»™ chÃ­nh xÃ¡c vá»›i má»™t ná»­a tham sá»‘ vÃ  FLOPs. HÆ¡n ná»¯a, PELA cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline há»c hiá»‡u quáº£ khÃ¡c vá»›i má»™t khoáº£ng cÃ¡ch Ä‘Ã¡ng chÃº Ã½.

4.2.2 TÃ¡c vá»¥ Háº¡ lÆ°u vÃ  Káº¿t quáº£
Sau khi cÃ¡c backbone Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c trÃªn táº­p dá»¯ liá»‡u ImageNet, theo cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [15, 17], chÃºng tÃ´i tiáº¿p tá»¥c Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a vÃ  phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng háº¡ lÆ°u.

Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 2 vÃ  Báº£ng 3, minh há»a hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i trong viá»‡c thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a vÃ  phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng tÆ°Æ¡ng á»©ng. Trong khi phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« cÃ¡c yÃªu cáº§u bá»™ nhá»› vÃ  tÃ­nh toÃ¡n giáº£m, viá»‡c tham gia cá»§a cÃ¡c framework vÃ  head háº¡ lÆ°u háº¡n cháº¿ má»©c Ä‘á»™ mÃ  nhá»¯ng lá»£i Ã­ch nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n khi so sÃ¡nh vá»›i phÃ¢n loáº¡i vanilla. VÃ­ dá»¥, FLOPs giáº£m cho Swin-Base trÃªn phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng trong Báº£ng 3 lÃ  18% so vá»›i 30% trÆ°á»›c Ä‘Ã¢y trong Báº£ng 1. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i váº«n hoáº¡t Ä‘á»™ng cÃ³ thá»ƒ so sÃ¡nh vá»›i má»—i mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng, cho tháº¥y hiá»‡u quáº£ cá»§a nÃ³ trong viá»‡c cÃ¢n báº±ng sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a hiá»‡u quáº£ vÃ  Ä‘á»™ chÃ­nh xÃ¡c. ÄÃ¡ng chÃº Ã½, PELA cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i LoRA vá» cáº£ hiá»‡u suáº¥t mÃ´ hÃ¬nh vÃ  kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh.

4.3. ThÃ­ nghiá»‡m trÃªn MÃ´ hÃ¬nh Thá»‹ giÃ¡c-NgÃ´n ngá»¯
4.3.1 MÃ´ hÃ¬nh Baseline vÃ  TÃ¡c vá»¥ VL Háº¡ lÆ°u
CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã o táº¡o trÆ°á»›c thá»‹ giÃ¡c-ngÃ´n ngá»¯ truyá»n thá»‘ng [6, 53] thÆ°á»ng sá»­ dá»¥ng cÃ¡c Ä‘áº·c trÆ°ng CNN Ä‘Æ°á»£c trÃ­ch xuáº¥t trÆ°á»›c cho biá»ƒu diá»…n hÃ¬nh áº£nh, thÆ°á»ng yÃªu cáº§u chÃº thÃ­ch há»™p bao chÃ­nh xÃ¡c. NgÆ°á»£c láº¡i, ALBEF [33] táº­n dá»¥ng ViT Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng thá»‹ giÃ¡c trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c vÃ  Ä‘Ã£ thá»ƒ hiá»‡n hiá»‡u suáº¥t xuáº¥t sáº¯c trÃªn nhiá»u tÃ¡c vá»¥ VL khÃ¡c nhau. Do Ä‘Ã³, chÃºng tÃ´i chá»n ALBEF lÃ m testbed Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i. HÆ¡n ná»¯a, báº£n cháº¥t all-in Transformer cá»§a ALBEF cho phÃ©p chÃºng tÃ´i dá»… dÃ ng Ä‘áº¡t Ä‘Æ°á»£c nÃ©n nhiá»u hÆ¡n. Trong bá»‘i cáº£nh nÃ y, chÃºng tÃ´i sá»­ dá»¥ng 1/3 tham sá»‘ cá»§a mÃ´ hÃ¬nh ALBEF ban Ä‘áº§u.

ChÃºng tÃ´i sá»­ dá»¥ng bá»‘n tÃ¡c vá»¥ thá»‹ giÃ¡c-ngÃ´n ngá»¯ háº¡ lÆ°u trong cÃ´ng viá»‡c nÃ y, bao gá»“m Truy xuáº¥t HÃ¬nh áº£nh-VÄƒn báº£n, SNLI-VE, VG, vÃ  VQA. Má»™t giá»›i thiá»‡u chi tiáº¿t vá» cÃ¡c tÃ¡c vá»¥ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong tÃ i liá»‡u bá»• sung. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m, chÃºng tÃ´i tuÃ¢n thá»§ nghiÃªm ngáº·t viá»‡c triá»ƒn khai ALBEF ngoáº¡i trá»« viá»‡c giáº£m kÃ­ch thÆ°á»›c batch do háº¡n cháº¿ tÃ i nguyÃªn.

4.3.2 Káº¿t quáº£ Tá»•ng thá»ƒ
Káº¿t quáº£ trÃªn cÃ¡c tÃ¡c vá»¥ háº¡ lÆ°u nÃ y Ä‘Æ°á»£c bÃ¡o cÃ¡o trong Báº£ng 4, 5, vÃ  6. Tá»« nhá»¯ng báº£ng nÃ y, chÃºng tÃ´i cÃ³ ba quan sÃ¡t quan trá»ng sau. 1) PhÆ°Æ¡ng phÃ¡p gáº§n Ä‘Ã¢y ALBEF [33] Ä‘Ã£ thá»ƒ hiá»‡n nhá»¯ng cáº£i tiáº¿n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng nhÆ° LXMERT [53] vÃ  UNITER [6]. Tuy nhiÃªn, hiá»‡u suáº¥t vÆ°á»£t trá»™i Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c vá»›i cÃ¡i giÃ¡ cá»§a viá»‡c tÄƒng tham sá»‘ vÃ  FLOPs, chá»§ yáº¿u do viá»‡c sá»­ dá»¥ng ViT cá»“ng ká»nh cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘á»ƒ xá»­ lÃ½ hÃ¬nh áº£nh. So vá»›i cÃ¡c baseline, sá»­ dá»¥ng Transformer phá»• quÃ¡t cho cáº£ thá»‹ giÃ¡c vÃ  ngÃ´n ngá»¯, nhÆ° UNITER [6], ALBEF cung cáº¥p cÃ¡c Ä‘áº·c trÆ°ng thá»‹ giÃ¡c vÆ°á»£t trá»™i nhÆ°ng giá»›i thiá»‡u kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lá»›n hÆ¡n. 2) PhÆ°Æ¡ng phÃ¡p PELA cá»§a chÃºng tÃ´i giÃºp giáº£m bá»›t váº¥n Ä‘á» nÃ y thÃ´ng qua xáº¥p xá»‰ háº¡ng tháº¥p. NhÆ° cÃ³ thá»ƒ quan sÃ¡t, PELA cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cÃ³ thá»ƒ so sÃ¡nh vá»›i ALBEF trong khi chá»‰ sá»­ dá»¥ng 1/3 tham sá»‘ vÃ  FLOPs. Äiá»u nÃ y dáº«n Ä‘áº¿n sá»± giáº£m Ä‘Ã¡ng ká»ƒ vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  tÃ­nh toÃ¡n, vá»›i háº§u háº¿t sá»± suy giáº£m hiá»‡u suáº¥t giá»›i háº¡n chá»‰ má»™t Ä‘iá»ƒm. 3) LiÃªn quan Ä‘áº¿n viá»‡c so sÃ¡nh vá»›i cÃ¡c baseline há»c hiá»‡u quáº£, phÆ°Æ¡ng phÃ¡p PELA cá»§a chÃºng tÃ´i liÃªn tá»¥c Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p. Ngoáº¡i lá»‡ duy nháº¥t lÃ  Ä‘á»‘i vá»›i cÃ¡c tÃ¡c vá»¥ truy xuáº¥t, trong Ä‘Ã³ PELA thá»ƒ hiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh kÃ©m hÆ¡n má»™t chÃºt so vá»›i LoRA. Tuy nhiÃªn, Ä‘iá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  LoRA yÃªu cáº§u sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh vÃ  FLOPs lá»›n hÆ¡n.

4.4. NghiÃªn cá»©u Loáº¡i bá»
Hiá»‡u quáº£ cá»§a hai mÃ´-Ä‘un. Äáº§u tiÃªn chÃºng tÃ´i nghiÃªn cá»©u hiá»‡u suáº¥t mÃ´ hÃ¬nh cá»§a viá»‡c phÃ¢n tÃ¡ch trá»±c tiáº¿p cÃ¡c trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c báº±ng xáº¥p xá»‰ háº¡ng tháº¥p. Tuy nhiÃªn, nhÆ° Ä‘Æ°á»£c chá»‰ ra trong Báº£ng 7, phÆ°Æ¡ng phÃ¡p nÃ y dáº«n Ä‘áº¿n sá»± sá»¥t giáº£m Ä‘Ã¡ng ká»ƒ vá» hiá»‡u suáº¥t, cÃ³ thá»ƒ do sá»± thay Ä‘á»•i trong phÃ¢n phá»‘i Ä‘áº·c trÆ°ng. Sau Ä‘Ã³ chÃºng tÃ´i thÃªm hai mÃ´-Ä‘un Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i vÃ o mÃ´ hÃ¬nh háº¡ng tháº¥p vÃ  quan sÃ¡t nhá»¯ng cáº£i tiáº¿n hiá»‡u suáº¥t. Báº±ng cÃ¡ch káº¿t há»£p hai mÃ´-Ä‘un vá»›i nhau, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i thÆ°á»ng cÃ³ thá»ƒ vÆ°á»£t trá»™i hÆ¡n cÃ¡c biáº¿n thá»ƒ khÃ¡c, chá»©ng minh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t.

Thay Ä‘á»•i hiá»‡u suáº¥t w.r.t. tá»‰ lá»‡ nÃ©n. Viá»‡c Ä‘Ã o táº¡o cÃ¡c mÃ´ hÃ¬nh lá»›n thÆ°á»ng liÃªn quan Ä‘áº¿n sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t. Äá»ƒ chá»©ng minh Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘Ã o táº¡o mÃ´ hÃ¬nh cá»§a mÃ¬nh báº±ng cÃ¡c tá»‰ lá»‡ nÃ©n khÃ¡c nhau vá»›i Ã­t epoch hÆ¡n Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh vÃ  trÃ¬nh bÃ y káº¿t quáº£ trong HÃ¬nh 4. Biá»ƒu Ä‘á»“ nÃ y chá»‰ ra ráº±ng má»™t tá»‰ lá»‡ nÃ©n nhá» hÆ¡n, tá»©c lÃ  má»™t mÃ´ hÃ¬nh lá»›n hÆ¡n, thÆ°á»ng mang láº¡i hiá»‡u suáº¥t tá»‘t hÆ¡n. Tuy nhiÃªn, má»™t mÃ´ hÃ¬nh quÃ¡ nhá», cháº³ng háº¡n nhÆ° má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c nÃ©n xuá»‘ng 1/10 kÃ­ch thÆ°á»›c ban Ä‘áº§u, cÃ³ thá»ƒ khÃ´ng cÃ³ kháº£ nÄƒng Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ thá»a Ä‘Ã¡ng.

4.5. Hiá»‡u quáº£ ÄÃ o táº¡o trÆ°á»›c & Má»Ÿ rá»™ng MÃ´ hÃ¬nh
Hiá»‡u quáº£ ÄÃ o táº¡o trÆ°á»›c. Má»™t ngÆ°á»i cÃ³ thá»ƒ lo ngáº¡i vá» cÃ¡c váº¥n Ä‘á» hiá»‡u quáº£ trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i táº­n dá»¥ng mÃ´ hÃ¬nh DeiT-Base vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c chá»‰ sá»‘ hiá»‡u quáº£ Ä‘Ã o táº¡o trÆ°á»›c cá»§a nÃ³, vÃ  hiá»ƒn thá»‹ káº¿t quáº£ trong Báº£ng 8. Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh háº¡ng tháº¥p Ä‘Æ¡n giáº£n vÃ¬ nÃ³ Ä‘Ã£ mang láº¡i hiá»‡u suáº¥t mÃ´ hÃ¬nh Ä‘áº§y há»©a háº¹n. Máº·c dÃ¹ cÃ¡c mÃ´ hÃ¬nh khÃ¡c cÃ³ thá»ƒ kÃ­ch hoáº¡t thá»i gian Ä‘Ã o táº¡o lÃ¢u hÆ¡n, trong bá»‘i cáº£nh nÃ y, nhÆ° thá»ƒ hiá»‡n trong báº£ng, phÆ°Æ¡ng phÃ¡p PELA cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh ban Ä‘áº§u vá» cáº£ chi phÃ­ bá»™ nhá»› GPU vÃ  Ä‘á»™ trá»… Ä‘Ã o táº¡o.

Má»Ÿ rá»™ng MÃ´ hÃ¬nh Háº¡ lÆ°u. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i sinh ra má»™t mÃ´ hÃ¬nh nhá» gá»n hÆ¡n so vá»›i mÃ´ hÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c ban Ä‘áº§u, dáº«n Ä‘áº¿n dÆ° thá»«a bá»™ nhá»› cho phÃ©p chÃºng tÃ´i Ä‘Ã o táº¡o cÃ¡c mÃ´ hÃ¬nh háº¡ lÆ°u vá»›i kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n. Äá»ƒ chá»©ng minh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i, chÃºng tÃ´i tÄƒng kÃ­ch thÆ°á»›c batch cho cáº£ DeiT-Base vÃ  Swin-Base trÃªn tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a, nhÆ° thá»ƒ hiá»‡n trong Báº£ng 9. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y káº¿t quáº£ Ä‘áº§y há»©a háº¹n, vá»›i sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» hiá»‡u suáº¥t mÃ´ hÃ¬nh cho cáº£ hai mÃ´ hÃ¬nh, Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n mIoU tuyá»‡t Ä‘á»‘i 0.57% vÃ  0.78% tÆ°Æ¡ng á»©ng. HÆ¡n ná»¯a, phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÅ©ng vÆ°á»£t trá»™i hÆ¡n baseline Swin-Base ban Ä‘áº§u sá»­ dá»¥ng PELA+, lÃ m ná»•i báº­t má»™t lá»£i tháº¿ khÃ¡c cá»§a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i.

5. Káº¿t luáº­n vÃ  CÃ´ng viá»‡c TÆ°Æ¡ng lai
Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p Ä‘Ã o táº¡o trÆ°á»›c tiáº¿t kiá»‡m tham sá»‘ Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p lÃ m cá»‘t lÃµi. Tháº­m chÃ­ vá»›i tÃ­nh Ä‘Æ¡n giáº£n cá»§a nÃ³, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh vá»›i cÃ¡c baseline trong khi Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tham sá»‘ vÃ  tÃ­nh toÃ¡n Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ. Nhá»¯ng lá»£i tháº¿ nÃ y cho phÃ©p má»Ÿ rá»™ng mÃ´ hÃ¬nh vá» Ä‘á»™ sÃ¢u mÃ´ hÃ¬nh, chiá»u rá»™ng, vÃ  kÃ­ch thÆ°á»›c batch Ä‘Ã o táº¡o cá»§a tinh chá»‰nh tÃ¡c vá»¥ háº¡ lÆ°u. CÃ´ng viá»‡c nÃ y lÃ m ná»•i báº­t nhá»¯ng lá»£i Ã­ch tiá»m nÄƒng cá»§a viá»‡c giáº£i quyáº¿t váº¥n Ä‘á» quÃ¡ tham sá»‘ hÃ³a cá»§a cÃ¡c trá»ng sá»‘ cÃ³ thá»ƒ há»c. NgoÃ i ra, chÃºng tÃ´i tin ráº±ng viá»‡c nÃ©n cÃ¡c Ä‘áº·c trÆ°ng trung gian lÃ  má»™t hÆ°á»›ng trá»±c giao Ä‘áº§y há»©a háº¹n Ä‘á»ƒ giáº£m Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh. Do Ä‘Ã³, chÃºng tÃ´i dá»± Ä‘á»‹nh Ä‘iá»u tra cÃ¡c ká»¹ thuáº­t nÃ©n Ä‘áº·c trÆ°ng, cháº³ng háº¡n nhÆ° cáº¯t tá»‰a token thá»‹ giÃ¡c, Ä‘á»ƒ xÃ¢y dá»±ng thÃªm má»™t mÃ´ hÃ¬nh nháº¹ hÆ¡n trong nghiÃªn cá»©u tÆ°Æ¡ng lai.

TÃ i liá»‡u tham kháº£o
[1] Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik StÃ¶ger, Mahdi Soltanolkotabi, vÃ  Soheil Feizi. Understanding over-parameterization in generative adversarial networks. Trong ICLR, 2021. 2, 3
[2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, vÃ  Judy Hoffman. Token merging: Your vit but faster. Trong ICLR, 2023. 5, 6, 7
[3] Zhaowei Cai vÃ  Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. Trong CVPR, trang 6154â€“6162. IEEE, 2018. 6
[4] Chun-Fu Richard Chen, Quanfu Fan, vÃ  Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. Trong ICCV, trang 357â€“366. IEEE, 2021. 6
[5] Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, vÃ  Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. Trong NeurIPS, trang 11011â€“11021, 2018. 3
[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, vÃ  Jingjing Liu. UNITER: universal image-text representation learning. Trong ECCV, trang 104â€“120. Springer, 2020. 6, 7
[7] Matthieu Courbariaux, Yoshua Bengio, vÃ  Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. Trong NIPS, trang 3123â€“3131, 2015. 1, 2
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, vÃ  Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong CVPR, trang 248â€“255. IEEE, 2009. 5, 6
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ  Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. Trong NAACL, trang 4171â€“4186. ACL, 2019. 1, 2
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, vÃ  Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong ICLR. OpenReview.net, 2021. 1, 3, 6
[11] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, vÃ  Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Trong NeurIPS, 2020. 7
[12] Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, vÃ  Jianguo Li. Network decoupling: From regular to depthwise separable convolutions. Trong BMVC, trang 248. BMV A Press, 2018. 3
[13] Yangyang Guo, Zhiyong Cheng, Jiazheng Jing, Yanpeng Lin, Liqiang Nie, vÃ  Meng Wang. Enhancing factorization machines with generalized metric learning. TKDE, 34(8):3740â€“3753, 2022. 3
[14] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, vÃ  Graham Neubig. Towards a unified view of parameter-efficient transfer learning. Trong ICLR, 2022. 3
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, vÃ  Jian Sun. Deep residual learning for image recognition. Trong CVPR, trang 770â€“778. IEEE, 2016. 6
[16] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, vÃ  Ross Girshick. Mask r-cnn. Trong ICCV, trang 2961â€“2969. IEEE, 2017. 6
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, vÃ  Ross B. Girshick. Masked autoencoders are scalable vision learners. Trong CVPR, trang 15979â€“15988. IEEE, 2022. 3, 6
[18] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, vÃ  Tat-Seng Chua. Neural collaborative filtering. Trong WWW, trang 173â€“182. ACM, 2017. 3
[19] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, vÃ  Jin Young Choi. A comprehensive overhaul of feature distillation. Trong ICCV, trang 1921â€“1930. IEEE, 2019. 1, 2, 4
[20] Geoffrey E. Hinton, Oriol Vinyals, vÃ  Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. 1, 2
[21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, vÃ  Weizhu Chen. Lora: Low-rank adaptation of large language models. Trong ICLR. OpenReview.net, 2022. 2, 3, 5, 6, 7
[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, vÃ  Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. JMLR, 18:187:1â€“187:30, 2017. 1, 2
[23] Yerlan Idelbayev vÃ  Miguel Ã. Carreira-PerpiÃ±Ã¡n. Low-rank compression of neural nets: Learning the rank of each layer. Trong CVPR, trang 8046â€“8056. IEEE, 2020. 3
[24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, vÃ  Ser-Nam Lim. Visual prompt tuning. Trong ECCV, trang 709â€“727. Springer, 2022. 1, 2, 3, 5
[25] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, vÃ  Qun Liu. Tinybert: Distilling BERT for natural language understanding. Trong Findings of EMNLP, trang 4163â€“4174. ACL, 2020. 2, 5, 6, 7
[26] Roberto J. Bayardo Jr. Efficiently mining long patterns from databases. Trong SIGMOD, trang 85â€“93. ACM, 1998. 2
[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, vÃ  Dario Amodei. Scaling laws for neural language models. CoRR, 2020. 1
[28] Wonjae Kim, Bokyung Son, vÃ  Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. Trong ICML, trang 5583â€“5594. PMLR, 2021. 7
[29] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, Minghai Qin, vÃ  Yanzhi Wang. Spvit: Enabling faster vision transformers via latency-aware soft token pruning. Trong ECCV, trang 620â€“640. Springer, 2022. 2
[30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, vÃ  Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. Trong ICLR. OpenReview.net, 2020. 2, 3
[31] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas M. Breuel, Jan Kautz, vÃ  Yale Song. Parameter efficient multimodal transformers for video representation learning. Trong ICLR, 2021. 3
[32] Chong Li vÃ  C.-J. Richard Shi. Constrained optimization based low-rank approximation of deep neural networks. Trong ECCV, trang 746â€“761. Springer, 2018. 3
[33] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, Caiming Xiong, vÃ  Steven Chu-Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Trong NeurIPS, trang 9694â€“9705, 2021. 2, 6, 7
[34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, vÃ  Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019. 7
[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, vÃ  Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. Trong ECCV, trang 121â€“137. Springer, 2020. 7
[36] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, vÃ  Shi Gu. BRECQ: pushing the limit of post-training quantization by block reconstruction. Trong ICLR. OpenReview.net, 2021. 2
[37] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, vÃ  C. Lawrence Zitnick. Microsoft COCO: common objects in context. Trong ECCV, trang 740â€“755. Springer, 2014. 6
[38] Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Dechao Meng, vÃ  Qingming Huang. Adaptive reconstruction network for weakly supervised referring expression grounding. Trong ICCV, trang 2611â€“2620. IEEE, 2019. 7
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, vÃ  Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong ICCV, trang 9992â€“10002. IEEE, 2021. 2, 5, 6
[40] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, vÃ  Stefan Lee. 12-in-1: Multi-task vision and language representation learning. Trong CVPR, trang 10434â€“10443. IEEE, 2020. 7
[41] Gaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller, faster, and better. CoRR, abs/2106.08962, 2021. 2
[42] Dimitris S. Papailiopoulos, Alexandros G. Dimakis, vÃ  Stavros Korokythakis. Sparse PCA through low-rank approximations. Trong ICML, trang 747â€“755. JMLR.org, 2013. 3
[43] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, vÃ  Piotr DollÃ¡r. Designing network design spaces. Trong CVPR, trang 10428â€“10436. IEEE, 2020. 6
[44] Babak Rokh, Ali Azarpeyvand, vÃ  Alireza Khanteymoori. A comprehensive survey on model quantization for deep neural networks. CoRR, 2022. 2
[45] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, vÃ  Yoshua Bengio. Fitnets: Hints for thin deep nets. Trong ICLR, 2015. 2, 4
[46] Victor Sanh, Lysandre Debut, Julien Chaumond, vÃ  Thomas Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. 2
[47] M. Schuermans, Philippe Lemmerling, vÃ  Sabine Van Huffel. Structured weighted low rank approximation. Numerical Linear Algebra with Applications, 11(5-6):609â€“618, 2004. 2
[48] Nathan Srebro vÃ  Tommi S. Jaakkola. Weighted low-rank approximations. Trong ICML, trang 720â€“727. AAAI Press, 2003. 2
[49] Trevor Strohman vÃ  W. Bruce Croft. Efficient document retrieval in main memory. Trong SIGIR, trang 175â€“182. ACM, 2007. 2
[50] Siqi Sun, Yu Cheng, Zhe Gan, vÃ  Jingjing Liu. Patient knowledge distillation for BERT model compression. Trong EMNLP, trang 4322â€“4331. ACL, 2019. 1, 2, 5
[51] Yi-Lin Sung, Jaemin Cho, vÃ  Mohit Bansal. VL-ADAPTER: parameter-efficient transfer learning for vision-and-language tasks. Trong CVPR, trang 5217â€“5227. IEEE, 2022. 1, 2, 3, 5
[52] Cheng Tai, Tong Xiao, Xiaogang Wang, vÃ  Weinan E. Convolutional neural networks with low-rank regularization. Trong ICLR, 2016. 3
[53] Hao Tan vÃ  Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers. Trong EMNLP, trang 5099â€“5110. ACL, 2019. 6, 7
[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, vÃ  HervÃ© JÃ©gou. Training data-efficient image transformers & distillation through attention. Trong ICML, trang 10347â€“10357. PMLR, 2021. 2, 5, 6
[55] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, vÃ  HervÃ© JÃ©gou. Augmenting convolutional networks with attention-based aggregation. arXiv preprint arXiv:2112.13692, 2021. 6
[56] Hugo Touvron, Matthieu Cord, vÃ  HervÃ© JÃ©gou. Deit iii: Revenge of the vit. Trong ECCV, trang 516â€“533. Springer, 2022. 2, 5, 6
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, vÃ  Illia Polosukhin. Attention is all you need. Trong NIPS, trang 5998â€“6008, 2017. 2
[58] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, vÃ  Yun Fu. Recent advances on neural network pruning at initialization. Trong IJCAI, trang 5638â€“5645. ijcai.org, 2022. 2
[59] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, vÃ  Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Trong ICML, trang 23318â€“23340. PMLR, 2022. 1
[60] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, vÃ  Hao Ma. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. 3
[61] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, vÃ  William Fedus. Emergent abilities of large language models. CoRR, 2022. 1
[62] Tsui-Wei Weng, Pu Zhao, Sijia Liu, Pin-Yu Chen, Xue Lin, vÃ  Luca Daniel. Towards certificated model robustness against weight perturbations. Trong AAAI, trang 6356â€“6363. AAAI Press, 2020. 5
[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, vÃ  Jian Sun. Unified perceptual parsing for scene understanding. Trong ECCV, trang 418â€“434. Springer, 2018. 6
[64] Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, vÃ  Kaiming He. Aggregated residual transformations for deep neural networks. Trong CVPR, trang 1492â€“1500. IEEE, 2017. 6
[65] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, vÃ  Fei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. Trong EMNLP, trang 9514â€“9528. ACL, 2021. 2
[66] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, vÃ  Hongkai Xiong. TRP: trained rank pruning for efficient deep neural networks. Trong IJCAI, trang 977â€“983. ijcai.org, 2020. 3
[67] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, vÃ  Jiebo Luo. Stare at what you see: Masked image modeling without reconstruction. Trong CVPR, trang 22732â€“22741. IEEE, 2023. 5, 6, 7
[68] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, vÃ  Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification. Trong CVPR Workshops, trang 2899â€“2908. IEEE, 2020. 3
[69] Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, vÃ  Yu Li. Vitkd: Practical guidelines for vit feature knowledge distillation. CoRR, abs/2209.02432, 2022. 2
[70] Xiyu Yu, Tongliang Liu, Xinchao Wang, vÃ  Dacheng Tao. On compressing deep models by low rank and sparse decomposition. Trong CVPR, trang 67â€“76. IEEE, 2017. 3
[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, vÃ  Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. Trong ICCV, trang 558â€“567. IEEE, 2021. 6
[72] Elad Ben Zaken, Yoav Goldberg, vÃ  Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. Trong ACL, trang 1â€“9. ACL, 2022. 2
[73] Xiangyu Zhang, Jianhua Zou, Kaiming He, vÃ  Jian Sun. Accelerating very deep convolutional networks for classification and detection. TPAMI, 38(10):1943â€“1955, 2016. 3
[74] Zhu Zhang, Zhou Zhao, Zhijie Lin, Jieming Zhu, vÃ  Xiuqiang He. Counterfactual contrastive learning for weakly-supervised vision-language grounding. Trong NeurIPS, 2020. 7
[75] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, vÃ  Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 127:302â€“321, 2019. 6
