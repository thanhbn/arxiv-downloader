# 2210.03265.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2210.03265.pdf
# File size: 1481032 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Polyhistor: Parameter-Efﬁcient Multi-Task
Adaptation for Dense Vision Tasks
Yen-Cheng Liu
Georgia Tech
ycliu@gatech.eduChih-Yao Ma
Meta
cyma@meta.comJunjiao Tian
Georgia Tech
jtian73@gatech.eduZijian He
Meta
zijian@meta.com
Zsolt Kira
Georgia Tech
zkira@gatech.edu
Abstract
Adapting large-scale pretrained models to various downstream tasks via ﬁne-tuning
is a standard method in machine learning. Recently, parameter-efﬁcient ﬁne-tuning
methods show promise in adapting a pretrained model to different tasks while
training only a few parameters. Despite their success, most existing methods
are proposed in Natural Language Processing tasks with language Transform-
ers, and adaptation to Computer Vision tasks with Vision Transformers remains
under-explored, especially for dense vision tasks. Further, in multi-task settings,
individually ﬁne-tuning and storing separate models for different tasks is inefﬁcient.
In this work, we provide an extensive multi-task parameter-efﬁcient benchmark and
examine existing parameter-efﬁcient ﬁne-tuning NLP methods for vision tasks. Our
results on four different dense vision tasks showed that existing methods cannot
be efﬁciently integrated due to the hierarchical nature of the Hierarchical Vision
Transformers. To overcome this issue, we propose Polyhistor andPolyhistor-Lite ,
consisting of Decomposed HyperNetworks andLayer-wise Scaling Kernels , to
share information across different tasks with a few trainable parameters. This
leads to favorable performance improvements against existing parameter-efﬁcient
methods while using fewer trainable parameters. Speciﬁcally, Polyhistor achieves
competitive accuracy compared to the state-of-the-art while only use 10% of
their trainable parameters. Furthermore, our methods show larger performance
gains when large networks and more pretraining data are used.
1 Introduction
Foundation models trained with large-scale datasets have shown the success of adapting to a variety
of downstream NLP and vision tasks [ 1]. As the state-of-the-art foundation models grow to billion
or even trillion parameter models [ 2,3,4,5,6], individually ﬁne-tuning all parameters of the model
wastes signiﬁcant computational resources. Further, for multi-task models, both ﬁne-tuning and stor-
ing separate models for multiple tasks become infeasible on devices with low computation resources.
To alleviate this issue, several works [ 7,8,9] have proposed parameter-efﬁcient ﬁne-tuning methods
to derive a better trade-off between trainable parameters and accuracy on downstream tasks. By only
training a small amount of parameters, these existing methods can substantially narrow the accuracy
gap compared to the baseline that ﬁne-tunes all parameters. However, these existing approaches
mainly focus on NLP tasks [ 10,11,12] or single-task adaptation on image classiﬁcation [ 9], and
Polyhistor :someone gifted or learned in multiple disciplines.
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.03265v1  [cs.CV]  7 Oct 2022

--- PAGE 2 ---
their applicability to more complicated vision tasks is unclear. On the other hand, the single-task
adaptation methods [ 7,10,11,12,8] still need to learn and store task-wise parameters, and the
number of trainable parameters increase with respect to the number of tasks.
Therefore, in this paper, we ﬁrst conduct a thorough study on how the existing successful parameter-
efﬁcient methods on NLP tasks perform on vision tasks , particularly on more challenging dense
vision tasks ( e.g., semantic segmentation, normals estimation). Second, based on our ﬁndings, we
then design a novel parameter-efﬁcient method for adaptation to multiple dense vision tasks .
Our method leverage shared modules across tasks and encourage the model to use shared information
in a more parameter-efﬁcient manner.
To start with, we ﬁrst evaluate the existing parameter-efﬁcient methods in NLP on dense vision
problems. We chose to apply these methods to hierarchical vision transformers (HVTs) considering
their state-of-the-art results on many per-pixel vision tasks [ 13,14]. Through our extensive studies, we
ﬁndtwolimitations in these works. First, adapter-based methods [ 11,15], which have shown strong
performance on NLP parameter-efﬁcient adaptation benchmarks, cannot be efﬁciently integrated with
HVTs. This is because the parameter usage of adapters in later transformer blocks grows quadratically
with respect to the layer scale (see Fig. 1c). Second, the state-of-the-art multi-task parameter-efﬁcient
method [ 16] applies a hyper-network to produce the weights of adapters and shares information
across different NLP tasks, while we ﬁnd it inherently requires a large number of trainable parameters
in the hyper-network (see Sec. 4.1 for further discussion).
To address the above limitations, we propose Polyhistor-Lite , which consist of two main components,
Decomposed Lite-HyperNetworks andLayer-wise Scaling Kernels . These two methods reduce the
trainable parameters in two aspects respectively, including parameter reduction for hyper-networks in
multi-tasking architecture and parameter reduction for adapters used in HVTs.
Speciﬁcally, to reduce the parameter usage of the multi-task architecture, we decompose a hyper-
network into a pair of separate hyper-networks. Unlike the existing approach, where a relatively large
hyper-network is used to produce long vector that can be reshaped into the weight matrix of adapter,
our decomposed hyper-networks individually produce two low-rank matrices that are multiplied to
construct the adapter weights. As a result, we can rely on this low-rank approximation to reduce the
parameter usage in the hyper-network yet maintain its performance on downstream tasks. In addition,
to enable the hypernetworks shared across layers in HVTs, we factorize an adapter weight matrix
into two kernels, including Template Kernels and Scaling Kernels. These two kernels are multiplied
via Kronecker Product to ﬁt in with different sizes of adapters, and this is achieved by controlling
the sizes of Scaling Kernels based on the scaling of the layer/adapter (and using the same size of
Template Kernels across layers). In this way, the parameters of adapter weights can be effectively
reduced with a minimal sacriﬁce on the accuracy of downstream tasks.
To benchmark the problem, we construct a uniﬁed framework with the same implementation details
and provide a comprehensive and fair comparison between existing parameter-efﬁcient adaptation
works in NLP on our multi-tasking dense vision problems. We also demonstrate that, with the
integration of our proposed Decomposed HyperNetworks andLayer-wise Scaling Kernels , we can
achieve a much better trade-off between trainable parameters and accuracies compared to the existing
methods. Speciﬁcally, most of existing methods struggled to match the performance of the simple
baseline, which individually ﬁne-tunes the entire network for each task, while our method achieves
better results than the simple baseline while only training less than 10% of the parameters in
a model. Compared with the state-of-the-art multi-tasking parameter-efﬁcient adaptation method,
Hyperformer [ 16], our method achieves competitive performance improvement with 90% reduction
in the trainable parameters of their method. Interestingly, we also observed that our proposed method
brings high-performance improvement when applied to the network pre-trained on the larger dataset
(ImageNet-22k). We will publicly release our code to facilitate future research.
To sum up, we list our contributions as follows:
•To the best of our knowledge, we are the ﬁrst to address parameter-efﬁcient multi-task
adaptation for vision tasks. We develop a uniﬁed framework to benchmark several parameter-
efﬁcient ﬁne-tuning NLP methods on dense vision tasks.
•We propose a novel method — Polyhistor-Lite that achieves signiﬁcant performance im-
provement with very low training parameters compared to existing methods.
2

--- PAGE 3 ---
•We observe that our method can bring further performance improvements when applied to
models with larger pre-trained dataset or with larger backbones.
2 Related Works
Parameter-efﬁcient Learning aims to adapt a pre-trained model to a new task by only training a small
number of parameters. The most straightforward method is to freeze the pre-trained encoder and
only ﬁne-tune the last layer, while, in terms of the accuracy of downstream tasks, it is still far from
full ﬁne-tuning. Thus, to achieve a better trade-off between accuracy and the number of tunable
parameters, several works [ 7,10,11,12,16,9,8] have proposed more parameter-efﬁcient methods,
and we summarize these works in the following paragraphs.
Single-Task Parameter-efﬁcient Adaptation. Several works build upon the Adapter [ 11], which
is a bottleneck-like module that is placed across the architecture and trained while the rest of the
original model is frozen. By changing the dimension of hidden vectors, one can easily control the
trade-off between trainable parameters and accuracy. For example, Houlsby et al. [11] proposes
to apply two adapter modules placed after the attention layers and the MLP layers respectively,
while Pfeiffer et al. [15] only use adapters after MLP layers and show more parameter-efﬁciency.
Furthermore, PHM-Layer [ 12] learns two types of matrices, one "slow" matrix shared across layers
and the other "fast" matrix learned individually in different layers, to produce the adapter weight via
Kronecker Product [ 17]. Compacter [ 12] further reduces the parameters by decomposing the slow
matrix into two rank-one vectors. Different from their goal of sharing slow matrix across layers, we
apply Kronecker Product to efﬁciently scale up adapters to different layer scales.
In addition, there are other parameter-efﬁcient learning works. BitFit [ 7] shows simply tuning biases
in all layers improves against the linear probing. Some other works ﬁne-tune learnable vectors, such
as learnable vectors in input word embeddings [ 18] and learnable vectors integrated with keys/values
in each layer of transformers [ 19]. LoRA [ 10] produces two low-rank matrices, which are multiplied
and served as a residual of attention weight matrices. While the above methods show favorable results
with using fewer trainable parameters, the goal of these works is single-task adaptation.
Multi-Task Parameter-efﬁcient Adaptation. When multiple tasks are learned jointly, one can share
some homogeneous information across different tasks and save the parameter usage by removing
the duplicated learned features. To this end, Hyperformer [ 16] introduces a hyper-network, which
takes as input task embeddings and produces the weights of adapters in different tasks. Since only the
parameters in the hyper-network need to be trained, the number of trainable parameters in task-wise
components can thus be reduced in the multi-tasking setup. On the other hand, Sung et al. [20] shows
that simply adding a single adapter on language transformer and sharing the adapter across tasks can
achieve promising results in vision-and-language cross-modal tasks ( e.g., image captioning).
Parameter-efﬁcient Adaptation for Vision. Despite the promising results, most parameter-efﬁcient
learning methods are evaluated on language transformers and NLP benchmarks, and parameter-
efﬁcient learning on Vision Transformer [ 9] is still an under-explored topic. A recent work, Visual
Prompt Tuning (VPT) [ 9], initiates the study of parameter-efﬁcient learning on Vision Transformers,
and it follows the idea of prompt tuning in language tasks and prepends and ﬁne-tunes some extra
learnable vectors in the input space of pre-trained Vision Transformers. VPT focuses on single-task
adaptation, while our work focuses on multi-task adaptation.
To fairly compare different parameter-efﬁcient learning methods, He et al. [21] present an empirical
study and re-evaluate parameter-efﬁcient learning methods (BitFit, Adapters, Preﬁx Tuning, and
LoRA) under the same experiment conﬁgurations for NLP tasks. Inspired by their work, we imple-
ment the aforementioned parameter-efﬁcient NLP methods (and include more latest works [ 12,16,9])
on our dense vision tasks, conduct comparative experiments, and fairly compare these methods.
3 Background
Hierarchical Vision Transformers. The Vision Transformer [ 22] is based on transformer architec-
tures [ 22] and operates on a set of patch tokens obtained from an image. As a variant of Vision
Transformer, the Hierarchical Vision Transformer [ 13,14,23,24,25,26] produces multi-scale feature
representations, and its hierarchical structure extracts ﬁne-grained information and better handles
3

--- PAGE 4 ---
(a)(b)
(c)
Figure 1: Illustration of (a) Hierarchical Vision Transformer and (b) Adapter. (c) When applying
adapters in a Hierarchical Vision Transformer, the number of parameters grows quadratically with
the respect to the block scale. Note that Cindicates the dimension of adapter input vectors, nis the
bottleneck size of adapters, and drepresents the input size of adapters.
images with scale and size variation. These properties contribute to the promising results in several
per-pixel vision tasks, including semantic segmentation [ 13,14,26], depth estimation [ 27], and
saliency detection [ 28]. As shown in Fig. 1a, a Hierarchy Vision Transformer (HVT) consists of
several transformer layers, and each transformer layer is mainly composed of an attention layer and
an MLP layer. Different from other transformers ( e.g., ViT [ 22]), a distinct characteristic of HVTs is
its pyramid-like feature maps generated from different transformer blocks as shown in Fig. 1a.
Adapters. Several parameter-efﬁcient adaptation works [ 11,15,12,21] build upon Adapter [ 11],
which is a bottleneck-like module placed in transformer layers as shown in Fig. 1b. These layers are
learnable parameters, while the rest of the model is frozen during ﬁne-tuning. The Adapter fa()
consists of a down-projection layer Wdown2Rdn, a non-linearity function (), a up-projection
layerWup2Rnd, and a skip connection from the input of the adapter hin2Rd.
hout=fa(hin;W) =(hinWdown )Wup+hin; (1)
wherehout2Rdis the output of the adapter and W= [Wdown ;W|
up]2Rd2nrepresents all
learnable parameters in the adapter.
4 Method
Problem Setting. Given a Hierarchical Vision Transformer pre-trained on large-scale image datasets
(e.g., ImageNet [ 29]), our goal is to train a small number of parameters and adapt the model to the
multi-tasking setting, where training data of Ntasks are obtained during the training stage. Following
the existing works in NLP, the criteria of parameter-efﬁcient multi-tasking learning includes the
accuracy of downstream tasks and the numbers of training parameters.
Method Overview. We aim to improve the parameter efﬁciency in two aspects: (1) to efﬁciently
share homogeneous information across tasks via lightweight hyper-networks (Section 4.1) and
(2) to efﬁciently scale up adapter weights in different transformer blocks of Hierarchical Vision
Transformers (Section 4.2). These two components are combined to improve the trade-off between
accuracy and training parameters in multi-tasking per-pixel vision tasks (Section 4.3).
4.1 Polyhistor : Decomposed Lightweight Hyper-networks for Multi-task Adaptation
With the goal of jointly adapting multiple NLP tasks in a parameter-efﬁcient manner, a prior work,
Hyperformer [ 16], builds upon a group of adapters in different tasks and extracts task-sharing
information via a hyper-network shared across different tasks. Speciﬁcally, a group of task and layer-
wise adapters with weight parameters fWt
ljt= 1;:::;N;l= 1;:::;Lgare individually inserted into
each layerlof the model with Llayers for allNtasks. Then, instead of individually learning the
4

--- PAGE 5 ---
weights of these adapters via backpropagation, Hyperformer constructs a layer-wise hyper-network
^Wl, which takes as input a learnable task embedding Vtand produce the weights of adapters Wt
l.
Wt
l= (Vt^Wl)2Rd2n;
Vt2Rk;^Wl2Rk2dn;() :R2dn!Rd2n(2)
where ()maps a vector with size of 2dnto a matrix with size of d2n.
While Hyperformer has shown promising results on multi-task NLP benchmarks, its effectiveness on
vision tasks is unclear. In addition, since the hyper-network produces the vectorization of an adapter
weight matrix ( i.e.,Wt
l2Rd2n), the output dimension of the hyper-network is the order of O(dn)
and the size of the hyper-network becomes the order of O(dnk), wheredandnare dimensions of
the input and bottleneck vectors and kis the size of task embeddings. For dense vision tasks, the
size of input vectors is usually large ( e.g.,1024 in SwinTransformer-Base). When the bottleneck
dimension is set to be proportional to the input dimension ( e.g.,n=d
, where>1is a constant),
the size of hyper-network is then quadratically increased with respect to input vectors O(kd2).
To alleviate this issue, we propose to decompose a single hyper-network ^Wlinto a pair of lightweight
hyper-networksf^Wp
l;^Wq
lg, each of which only produces a low-rank matrix. We then multiply the
matrices to obtain the adapter weight as shown in the top of Fig. 2a.
Wt
l=rX
i=1piq|
i=p(Vt^Wp
l)q(Vt^Wq
l)|;
p(Vt^Wp
l)2Rdr;q(Vt^Wq
l)2R2nr;
Vt2Rk;^Wp
l2Rkdr;^Wq
l2Rk2nr;
p() :Rdr!Rdr;q() :R2nr!R2nr;(3)
where p()andq()are matrix reshape functions and ris a matrix rank which is hyper-parameter
tuned according the computational budget. Note that the matrix rank ris usually much smaller than
dimensions of an adapter nord(i.e.,r<<n<d ).
In this way, with the low-rank decomposition and approximation, a heavy hyper-network ^Wl2
Rk2dncan be reduced to two lightweight hyper-networks f^Wp
l2Rkdr;^Wq
l2Rk2nrg, and
the number of trainable parameters can be reduced from 2kdn tok(d+ 2n). The number of trainable
parameters in hyper-networks is reduced from quadratic to linear increasing with respect to the input
size ( i.e.,O(kd2)!O (kd)). We will discuss how this translate into practical usage and demonstrate
that our method can signiﬁcantly save the number of training parameters while achieving competitive
performance compared to the vanilla Hyperformer in Sec. 5.
4.2 Layer-Wise Scaling Kernels for Hierarchical Vision Transformers
Although the number of learnable parameters is reduced by sharing two lightweight hyper-networks
across tasks, each layer of transformer still requires a pair of layer-wise hyper-networks. To prevent
the number of parameters from growing linearly with respect to the number of layers in a transformer,
one can share a hyper-network across not only different tasks but also different layers (similar to
the recently introduced Hyperformer++ [ 16] in NLP). However, since adapters in different blocks of
Hierarchical Vision Transformers have different dimensions, such a property restricts us from using
the same pair of hyper-networks to produce the weights of adapters in different transformer layers.
To overcome this issue, we introduce Layer-wise Scaling Kernels to enable sharing a hyper-network
across layers. To be more speciﬁc, as shown in Fig. 1, Hierarchical Vision Transformers have four
transformers blocks, and each transformers block (indexed by b) has multiple transformer layers with
a block-wise scale sb= 2b 1and a output size (H
4sb;W
4sb;sbC). However, the increasing channel
sizes in transformer blocks cause two problems. First, as shown in Fig. 1, the sizes of adapter weights
increase quadratically with respect to the channel size/input size. As a result, the adapters in the later
blocks would require much more trainable parameters than the earlier blocks. Second, as mentioned
above, due to the different sizes of adapters in different transformer layers, a single hyper-network
cannot produce multiple adapters in different layers and thus cannot be directly shared across layers.
5

--- PAGE 6 ---
(a) (b)
Figure 2: Illustration of our Polyhistor andPolyhistor-Lite . (a) We propose Polyhistor , which
applies Decomposed HyperNetworks to reduce the number of training parameters in multi-task
adaptation (Section 4.1). We also introduce Layer-wise Scaling Kernels to efﬁciently scale up
Template Kernels for different scales of adapters (Section 4.2). (b) By combining Decomposed
HyperNetowrks and Layer-wise Scaling Kernels, our Polyhistor-Lite can efﬁciently address multi-
task adaptation in per-pixel vision tasks (Section 4.3).
As shown in the bottom of Figure 2a, to address these two problems, we propose to factorize the
weight matrix of a adapter Wlof the layerlinto a set of Template Kernels ~Wi
land Scaling Kernels
i
l. To efﬁciently scale up adapter weights in different transformer layers, we resort to Hypercomplex
multiplication [17] and use Kronecker Product to integrate these types of matrices.1
Wl=s (l)X
i=1~Wi
l
i
l;
~Wi
l2Rd2n;i
l2Rs (l)s (l);Wl2Rds (l)2ns (l);(4)
wheres (l)and (l)are the scale and the index of transformer block where transformer layer lis
located, and
is Kronecker Product matrix operation. The sizes of Template Kernels are the same
across layers, while the sizes of Scaling Kernels depend on the block-wise scale s (l).
In other words, the purpose of Scaling Kernels is to scale up the Template Kernels and ﬁt them into
the adapters in layers with different scales. This decomposition not only reduces the parameter usage
in a single adapter but also shows the potential to reduce parameters with a shared hyper-network.
4.3 Polyhistor-Lite : Lightweight Hypernetworks for Hierarchical Vision Transformers
We have described how to reduce the training parameters for multi-tasking adaptation in Section 4.1
and Hierarchical Vision Transformers in Section 4.2. To perform parameter-efﬁcient adaptation for
multi-tasking per-pixel vision tasks, we integrate these two components to obtain our ﬁnal framework.
Speciﬁcally, as shown in Fig. 2b, we use a single pair of hyper-networks f^Wp;^Wqgshared across
different layers and different tasks. For the task t, the hyper-networks take as input a set of trainable
layer embeddingsf~Vi
lgs (l)
i=1and a task embedding ~Vtand produces two low-rank matrices, which
1For the ease of understanding, we omit the task index t.
6

--- PAGE 7 ---
are multiplied and derive a set of Template Kernels of adapters in different layers and tasks.
~Wt;i
l=p([~Vt;~Vi
l]^Wp)q([~Vt;~Vi
l]^Wq)|;8i= 1;:::; (l)
p([~Vt;~Vi
l]^Wp)2Rdr;q([~Vt;~Vi
l]^Wq)2R2nr;
~Vt;~Vi
l2Rk
2;^Wp2Rkdr;^Wq2Rk2nr;
p() :Rdr!Rdr;q() :R2nr!R2nr:(5)
To derive the parameters of an adapter in each layer, we learn another set of Scaling Kernels and
combine them with the Template Kernels via Kronecker Product.
Wt
l;=s (l)X
i=1~Wt;i
l
t;i
l;8t= 1;:::;T
~Wt;i
l2Rd2n;t;i
l2Rs (l)s (l);Wt
l2Rds (l)2ns (l):(6)
With the integration of Lightweight HyperNetworks and Layer-wise Scaling Kernels, our framework
can efﬁciently reduce the trainable parameters in multi-task adaptation for dense vision tasks. We
provide two variants of our method. Polyhistor solely uses the Decomposed HyperNetworks, and
Polyhistor-Lite combines both Decomposed HyperNetworks and Layer-Wise Scaling Kernels.
5 Experiments
5.1 Implementation Details
Dataset. We follow prior works [ 30,31] on multi-task learning for dense prediction tasks and consider
PASCAL-Context [ 32] to construct our multi-task efﬁcient adaptation for per-pixel benchmark. We
evaluate all methods on four per-pixel tasks, 21-class semantic segmentation, 7-class human part
segmentation, surface normals estimation, and saliency detection. Our evaluation metrics include
the mean intersection-over-union (mIoU) for semantic segmentation, human part segmentation, and
saliency detection and the mean error (mErr) for surface normals estimation.
Model Architecture. For the encoder, we use Swin-Transformer [ 13] due to its strong performance
in different vision tasks and the popularity in the vision community. Our decoders for different dense
tasks are based on the All-MLP decoder of Segformer [ 14], which uses simple linear layers and
bilinear upsampling layer to efﬁciently perform dense vision tasks, and we adapt the number of
output dimension to different tasks.
Training. To train our model, we use the commonly-used losses for each task. Speciﬁcally, we use
the standard per-pixel cross-entropy for semantic segmentation and human part segmentation, L1
loss for surface normals estimation, and balanced cross-entropy for saliency detection. For a fair
comparison, we experiment on a uniﬁed codebase implementation with the same loss functions and
training iterations for all baselines and our method.
5.2 Baselines
Single-task full ﬁne-tuning uses an individual pretrained model for each task, and Fine-tuning
decoders freezes the feature backbone and only ﬁne-tunes task-wise decoders for different tasks.
For single-task adaptation methods (Bitﬁt, VPT, PHM-Layer, Compacter, Compacter++, Adapter,
Low-rank Adapter, and LoRA), we place task-wise modules for each task.
Bitﬁt [7] tune the biases in all layers, and, speciﬁcally for Swin-Transformer, and we also tune biases
in patch merging layers and patch projection layers.
VPT [9] inserts tunable embeddings in the ﬁrst input layer (VPT-shallow) and all layers (VPT-deep),
and we select the best hyper-parameter ( i.e.,50embeddings per layer) for all results.
PHM layer [12] shares a slow matrix for all layers and learns a fast matrix for each layer and place
modules after attention and MLP layers, Compacter [12] further decomposes the fast matrix into
two low-rank vectors, and Compacter++ [12] only places modules after MLP layers.
7

--- PAGE 8 ---
12
 10
 8
 6
 4
 2
 0 2
Relative Improvment to Single-T ask Fine-Tuning (%)0100101102Trainable Parameters (M)Single-task Full Fine-tuning
Train decoders 
Multi-task Full Fine-tuning
Bitfit
Relative Bias
LoRA
VPT-shallow
VPT-deep
Low-rank adapters
PHM layer
Compacter
Adapter
Shared adapter
Hyperformer
Polyhistor (Ours)
Polyhistor-Lite (Ours)
6
 4
 2
 0 2
Relative Improvment to Single-T ask Fine-Tuning (%)0100101Trainable Parameters (M)
Compacter++
Compacter
Adapter
Shared Adapter
Hyperformer
Polyhistor (Ours)
Polyhistor-Lite (Ours)(a) (b)
Figure 3: (a) Our Polyhistor uses less than one-tenth of the state-of-the-art multi-task adaptation
method ( i.e.,Hyperformer [ 16]) in terms of trainable parameters in the encoder. (b) Tuning hyper-
parameters on the baseline methods leads to limited improvements, and we achieve the best trade-off
between the trainable parameters and accuracy on downstream tasks. Details are listed in Appendix.
LoRA [10] applied the low-rank decomposition on attention layers, and we select rank r= 4and the
adapter output scale ( i.e.,4), which performs the best.
Adapter [11,21] placed task-wise bottleneck-like modules into transformer layers, and Shared-
Adapter [20] share an adapter across different tasks.
Hyperformer [16] applied a hyper-network and produce the weights for adapter, and we present the
results with different adapter bottleneck dimensions. Since adapters in different layers have different
dimensions, Hyperfomer++ cannot be simply adapted to Hierarchical Vision Transformers.
5.3 Experiment results on Multi-Task Adaptation
We evaluate all methods by computing relative improvements against Single-Task Full Fine-tuning
and averaging across four tasks. Since all methods use and train all parameters of task-wise decoders,
we provide two values of trainable parameters, one for the encoder and the other for the whole model.
As presented in Fig. 3 and Table 1, among all experimental methods, Hyperformer performs the
best and achieves +2:64% on average for the four downstream tasks, but it requires 72M trainable
parameters in the encoder. On the other hand, our Polyhistor achieves competitive results ( +2:34%),
while we only need 6:41M trainable parameters in the encoder, which is less than one-tenth of the
Hyperformer. Our Polyhistor-Lite can further reduce the trainable parameters to 0:41M by integrating
the Layer-wise Scaling Kernels and sharing the hypernetwork across layers, and it achieves +1:74%
and is higher than all other methods using a similar amount of trainable parameters ( e.g., BitFit, VPT,
Shared Adapter, PHM layer, Compacter, LoRA, and Low-rank Adapter).
We also found that, while the prior parameter-efﬁcient adaptation vision method, VPT, presented
promising results on single-task image classiﬁcation [ 9], it does not show signiﬁcant improvements
against the baseline that only ﬁne-tuning decoders in our multi-task dense vision benchmark. A
potential reason is that, compared to the image classiﬁcation benchmarks which focus more on the
same task with input shifts, our benchmark focuses more on different task outputs. This makes VPT,
which adds the learnable parameters in the input space, unable to address the difference in the output
space and adapt to the new tasks.
While Hyperformer achieves the best performance improvement against Single-Task Full Fine-tuning,
it requires a larger number of parameters than other methods. This leads to a natural question: Does
reducing the parameters in Hyperformer derive a better trade-off between the number of tunable
parameters and performance improvements? Therefore, we decrease its size and examine whether it
can maintain its performance with fewer trainable parameters. We also did a similar experiment on
the other existing methods, ( e.g., Adapter, Shared Adapter, Compacter, and Compacter++), in which
we increase the number of tuning parameters by increasing the dimension of the hidden vector in the
adapter modules (increasing the down-projection ratio). As shown in Fig. 3b, we ﬁnd that simply
tuning the hyper-parameters in baseline methods cannot obtain a better tradeoff between the number
of trainable parameters and performance improvement. For example, when the number of tuning
parameters in the encoder of Hyperformer is reduced to 20:15M, its averaged relative improvement
8

--- PAGE 9 ---
Table 1: Experimental results on Multi-Task Adaptation. We use SwinTransformer-Tiny as the feature
backbone. uprepresents relative improvement or gap to the Single-task Full Fine-tuning. Results
with the symbol"=#indicate higher/lower is better.
Number of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder/All Seg. "H.Part"Sal."Normals# up
Single-task Full Fine-tuning 110.07 / 112.62 67.21 61.93 62.35 17.97 0.00 %
Fine-tuning Decoders 0.00 / 2.55 63.14 52.37 58.39 20.89 -11.02 %
Multi-task Full Fine-tuning 27.51 / 30.06 68.71 62.13 64.18 17.35 2.23 %
Bitﬁt [7] 0.30 / 2.85 68.57 55.99 60.64 19.42 -4.60 %
Relative bias [13] 0.09 / 2.64 63.51 52.35 57.74 21.07 -11.40 %
VPT-shallow [9] 0.02 / 2.57 62.96 52.27 58.31 20.90 -11.18 %
VPT-deep [9] 0.88 / 3.43 64.35 52.54 58.15 21.07 -10.85 %
PHM layer [12] 0.59 / 3.14 68.55 56.28 60.35 19.23 -4.34 %
Compacter [12] 0.23 / 2.78 68.08 56.41 60.08 19.22 -4.55 %
Compacter++ [12] 0.11 / 2.66 67.26 55.69 59.47 19.54 -5.84 %
LoRA [10] 0.32 / 2.87 70.12 57.73 61.90 18.96 -2.17 %
Adapter [21] 8.69 / 11.24 69.21 57.38 61.28 18.83 -2.71 %
Low-rank adapter 0.34 / 2.89 68.31 56.53 60.29 19.36 -4.54 %
Shared Adapter [20] 2.20 / 4.74 70.21 59.15 62.29 19.26 -1.83 %
Hyperformer [16] 72.77 /75.32 71.43 60.73 65.54 17.77 2.64 %
Polyhistor (Ours) 6.41 / 8.96 70.87 59.54 65.47 17.47 2.34 %
Polyhistor-Lite (Ours) 0.41 / 2.96 70.24 59.12 64.75 17.40 1.74 %
(a) (b)
Figure 4: Using the feature backbone with (a) larger size or (b) more pretraining data leads to larger
improvements against Single-task Full Fine-tuning. All results are produced by our Polyhistor-Lite .
For a fair comparison, both the backbones shown in (a) are pretrained on the ImageNet-1k, and both
the backbones shown in (b) are based on the SwinTrasformer-Base.
shrinks to only 0:14%. These experiments show that our methods achieve a better performance
improvement with a fewer number of parameters than the baseline methods.
5.4 Ablation studies and analyses
Different feature backbones. To verify that our proposed method is also applicable to other larger-
size model architecture, we also experiment SwinTransformer-Base. For a fair comparison, we use the
same pretraining dataset, ImageNet-1k [ 29], on both SwinTransformer-Tiny and SwinTransformer-
Base. As shown in Fig. 4a, we ﬁnd that our method achieves a larger improvement against Single-task
Full Fine-tuning when a larger feature backbone is used, and this shows a potential of obtaining more
improvements when applying our method to a larger model architecture.
Different Pretraining data. We also examine how our proposed method performs when different
pretraining data are used. Speciﬁcally, we apply our method to SwinTransformer-Base pretrained
with ImageNet-1k and ImageNet-22k. As demonstrated in Fig. 4b, we ﬁnd that our method obtains a
larger performance improvement when more pretraining data was used, and this shows a potential of
deriving more improvements by using more pretraining data.
Varying trainable parameters with different pretraining data. In Fig. 4a, we showed that using
the model with more pretraining data ( i.e.,ImageNet-22k) data can lead to a higher performance gain
compared to the model with less pretraining data ( i.e.,ImageNet-1k). To investigate this phenomenon,
9

--- PAGE 10 ---
4 5 6 7 8
Relative improvment to Single-task Full Fine-tuning (%)0.00.20.40.60.81.01.21.4Trainable Parameters (M)
Polyhistor-Lite (1k)
Polyhistor-Lite (22k)Figure 5: Our Polyhistor-Lite using more pretraining data can lead to higher relative improvements to
the Single-task Fine-tuning baseline, and this trend is consistent across varying amounts of trainable
parameters. Note that Single-task Fine-tuning baselines for Polyhistor-Lite (1k) and Polyhistor-
Lite(22k) are trained on ImageNet-1k and ImageNet-22k models respectively.
we vary the size of our Polyhistor-Lite and measure the performance gain of the models with different
pretraining data.
As shown in Fig. 5, we ﬁnd that the model pretrained on ImageNet-22k can use less trainable
parameters to achieve similar performance gains compared to the model pretrained on ImageNet-1k.
In addition, under the same amount of trainable parameters, the model pretrained on ImageNet-22k
can consistently outperform the model pretrained on ImageNet-1k. This suggests that, with more
pretraining data, feature extractors can learn more diverse representations, so that we can use less
trainable parameters and adapt better to different downstream tasks.
Dimensions of task embeddings. In addition, we also conduct analysis to examine how our method
performs with different sizes of task embeddings. As presented in Table 2, when large sizes of task
embeddings are used, the averaged improvement against single-task ﬁne-tuning becomes larger.
Table 2: Ablation study on the sizes of task embeddings. We vary the sizes of task embeddings k
from 16to64on our Polyhistor-Lite . All results in this table are based on SwinTransformer-Tiny
pretrained on ImageNet-1k.
Size of Num. of Trainable Parameters Performance of Each Downstream Task Averaged Results
Task Embeddings k Encoder / All Seg. "H.Seg."Sal."Normals# up
Single-task Full Fine-tuning - 110.07 / 112.62 67.21 61.93 62.35 17.97 0.00 %
Polyhistor-Lite 16 0.23 / 2.78 69.67 58.38 63.55 18.05 -0.15 %
Polyhistor-Lite 32 0.29 / 2.84 69.80 58.72 64.14 17.73 0.72 %
Polyhistor-Lite 64 0.41 / 2.96 70.24 59.12 64.75 17.40 1.74 %
We present more ablation studies, analyses, and implementation details in Appendix.
6 Conclusion
We proposed Polyhistor andPolyhistor-Lite — parameter-efﬁcient ﬁne-tuning methods for dense
vision tasks. We showed that most of the existing parameter-efﬁcient single-task adaptation methods
achieved lower performance compared with Single-task Full Fine-tuning, and the state-of-the-art
multi-task adaptation method achieve favorable results while using a large number of tunable parame-
ters. Compared to these existing approaches, our proposed methods do not only achieve a competitive
performance gain to the state-of-the-art but also only use a very limited amount of tunable parameters.
The potential limitation of our method is searching for suitable hyper-parameters, which is a common
limitation among all parameter-efﬁcient learning methods.
Acknowledgments. Yen-Cheng Liu and Zsolt Kira were partly supported by DARPA’s Learning
with Less Labels (LwLL) program under agreement HR0011-18-S-0044, as part of their afﬁliation
with Georgia Tech.
10

--- PAGE 11 ---
References
[1]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[3]Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent
Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when
pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360 , 2022.
[4]Anuroop Sriram, Abhishek Das, Brandon M Wood, Siddharth Goyal, and C Lawrence Zitnick.
Towards training billion parameter graph neural networks for atomic simulations. arXiv preprint
arXiv:2203.09697 , 2022.
[5]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
[6]William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
[7]Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient
ﬁne-tuning for transformer-based masked language-models. In In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (ACL) , 2022.
[8]Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff
pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , 2021.
[9]Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,
and Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119 , 2022.
[10] Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. In International Conference on
Learning Representations , 2022.
[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning
for nlp. In Proceedings of the International Conference on Machine Learning (ICML) , 2019.
[12] Rabeeh Karimi mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-
rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems
(NeurIPS) , 2021.
[13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV) , 2021.
[14] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.
Segformer: Simple and efﬁcient design for semantic segmentation with transformers. In
Advances in Neural Information Processing Systems (NeurIPS) , 2021.
[15] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,
Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:
System Demonstrations , 2020.
[16] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.
Parameter-efﬁcient multi-task ﬁne-tuning for transformers via shared hypernetworks. In Annual
Meeting of the Association for Computational Linguistics , 2021.
[17] Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, and Jie Fu.
Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multipli-
cations with 1=nparameters. In Proceedings of the International Conference on Learning
Representations (ICLR) , 2021.
11

--- PAGE 12 ---
[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient
prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing , pages 3045–3059, 2021.
[19] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation.
InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers) , 2021.
[20] Mohit Bansal Yi-Lin Sung, Jaemin Cho. Vl-adapter: Parameter-efﬁcient transfer learning for
vision-and-language tasks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , 2022.
[21] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. To-
wards a uniﬁed view of parameter-efﬁcient transfer learning. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2022.
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[23] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image
transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) ,
2021.
[24] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.
Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV) , 2021.
[25] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé
Jégou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference.
InProceedings of the IEEE International Conference on Computer Vision (ICCV) , 2021.
[26] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV) , 2021.
[27] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan
Misra. Omnivore: A Single Model for Many Visual Modalities. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.
[28] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In
Proceedings of the IEEE International Conference on Computer Vision (ICCV) , 2021.
[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision , 115(3):211–252, 2015.
[30] Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Mti-net: Multi-scale task
interaction networks for multi-task learning. In Proceedings of the European Conference on
Computer Vision (ECCV) , 2020.
[31] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai, and L. Van Gool.
Multi-task learning for dense prediction tasks: A survey. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2021.
[32] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challenge. International Journal of Computer Vision
(IJCV) , 2010.
[33] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-
supervised learning with swin transformers. arXiv preprint arXiv:2105.04553 , 2021.
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
12

--- PAGE 13 ---
Appendix
A Feature backbones with different pretraining data.
Comparison to baselines. We also experiment with other baseline methods on SwinTransformer-
Base pretrained on ImageNet-1k and ImageNet-22k. As shown in Table 3 and 4, we ﬁnd that
the trend of all methods are similar to the results on SwinTransformer-Tiny. Speciﬁcally, most
baseline methods have a performance gap with Hyperformer and our Polyhistor-Lite . Compared to
Hyperformer, our Polyhistor-Lite use less than 5%of their trainable parameters in the encoder to
achieve similar or even better results. By tuning the adapter down-projection ratio and using fewer
trainable parameters, our Polyhistor-Lite still obtains a higher performance gain against the baseline
methods with low trainable parameters ( e.g., Compacter++, Bitﬁt, VPT, PHM layer, LoRA, Adapter).
These results show that our proposed Polyhistor-Lite can derive a better trade-off against all existing
parameter-efﬁcient works in different pretrained feature backbones.
B Ranks of hyper-network output.
Another important hyper-parameter in our model is ranks of hyper-network outputs. We thus
experiment Polyhistor and vary the rank of output matrices, and, as shown in Table 5, we ﬁnd the
model can derive a better results with a large rank.
C Experiment results of tuning baseline methods.
In Fig. 3b of the main paper, we presented the results of different baseline methods with different
hyper-parameters. For a clearer comparison between baseline methods, We also provide the exact
values of all results in Table 6. It is worth noting that simply tuning the baseline methods only leads
to limited improvements of baseline methods, and our proposed Polyhistor andPolyhistor-Lite still
achieve a better trade-off between performance gain and trainable parameters.
Table 3: Experimental results on Multi-Task Adaptation. We use SwinTransformer-Base pretrained on
ImageNet-1k as the feature backbone. uprepresents relative improvement or gap to the Single-task
Full Fine-tuning. Results with the symbol "=#indicate higher/lower is better. represents the
down-projection ratio of adapters ( i.e.,. the ratio of adapter input dto hidden vectors n,=d=n).
Number of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder/All Seg. "H.Part"Sal."Normals# up
Single-task Full Fine-tuning 346.96 / 350.01 67.88 64.47 61.26 18.85 0.00 %
Fine-tuning Decoders 0.00 / 3.04 68.98 55.57 58.37 21.36 -7.55 %
Bitﬁt [7] 0.20 / 3.24 71.93 59.12 60.67 20.08 -2.46 %
Relative bias [13] 0.06 / 3.11 68.44 55.70 57.27 21.63 -8.51 %
VPT-deep [9] 2.41 / 5.45 68.80 55.85 58.25 21.37 -7.57 %
PHM layer [12] 1.89 / 4.94 71.93 59.11 59.71 20.35 -3.21 %
Compacter++ [12] 0.25 / 3.29 72.00 59.11 59.73 20.41 -3.25 %
LoRA [10] 0.87 / 4.31 74.10 61.57 63.87 18.55 2.63 %
Adapter [21] 3.64 / 6.68 73.29 60.30 62.42 18.66 1.10 %
Low-rank adapter 0.34 / 2.89 72.13 59.10 59.81 20.28 -3.01 %
Hyperformer [16] 60.88 /63.92 73.60 63.82 67.31 16.90 6.91 %
Polyhistor-Lite (Ours;= 1) 1.29 / 4.34 73.70 63.32 66.50 16.93 6.38 %
Polyhistor-Lite (Ours;= 2) 0.62 / 3.67 73.69 63.04 66.56 17.30 5.80 %
Polyhistor-Lite (Ours;= 4) 0.39 / 3.43 73.57 62.04 65.84 17.70 4.55 %
Polyhistor-Lite (Ours;= 8) 0.29 / 3.34 73.92 62.15 65.37 17.70 4.53 %
Polyhistor-Lite (Ours;= 32 ) 0.24 / 3.28 73.80 61.32 64.64 17.92 3.57 %
D Experiment results of other hierarchical vision transformers.
As shown in Table 7, we further apply our method and other baseline methods to the Pyramid Vision
Transformer [ 26]. We ﬁnd our Polyhistor can achieve comparable results to Hyperformer while
using much fewer trainable parameters. Polyhistor-Lite can further reduce trainable parameters and
achieves higher accuracy than all other methods using a similar amount of trainable parameters (e.g.,
13

--- PAGE 14 ---
Table 4: Experimental results on Multi-Task Adaptation. We use SwinTransformer-Base pretrained
onImageNet-22k as the feature backbone. uprepresents relative improvement or gap to the Single-
task Full Fine-tuning. Results with the symbol "=#indicate higher/lower is better. represents the
down-projection ratio of adapters ( i.e.,. the ratio of adapter input dto hidden vectors n,=d=n).
Number of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder/All Seg. "H.Part"Sal."Normals# up
Single-task Full Fine-tuning 346.96 / 350.01 70.72 67.47 61.00 18.73 0.00 %
Fine-tuning Decoders 0.00 / 3.04 73.33 60.56 59.13 21.38 -5.94 %
Bitﬁt [7] 0.20 / 3.24 76.42 64.89 62.05 19.03 1.09 %
Relative bias [13] 0.06 / 3.11 72.86 60.64 58.44 21.51 -6.53 %
VPT-deep [9] 2.41 / 5.45 74.21 61.41 58.80 21.61 -5.90 %
PHM layer [12] 1.89 / 4.94 76.33 64.59 60.43 20.23 -1.32 %
Compacter++ [12] 0.25 / 3.29 75.99 64.65 60.42 20.01 -1.13 %
LoRA [10] 0.87 / 4.31 78.24 66.95 64.70 18.07 4.86 %
Adapter [21] 3.64 / 6.68 77.22 65.95 63.80 18.38 3.35 %
Low-rank adapter 0.34 / 2.89 75.65 64.75 60.50 20.03 -1.21 %
Hyperformer [16] 60.88 /63.92 78.41 68.94 67.50 16.80 6.91 %
Polyhistor-Lite (Ours;= 1) 1.29 / 4.34 77.91 68.02 66.89 16.54 8.08 %
Polyhistor-Lite (Ours;= 32 ) 0.24 / 3.28 77.74 66.33 65.03 17.65 5.15 %
Table 5: Ablation study on the sizes of ranks in hypernetwork output matrices. We vary the dimensions
of ranksrfrom 1ton
2on our Polyhistor . Note thatnis the dimension of hidden vectors in adapters.
All results in this table are based on SwinTransformer-Tiny pretrained on ImageNet-1k.
Dimension of Num. of Trainable Parameters Performance of Each Downstream Task Averaged Results
Ranksr Encoder/ All Seg. "H.Seg."Sal."Normals# up
Single-task Full Fine-tuning - 110.07 / 112.62 67.21 61.93 62.35 17.97 0.00 %
Polyhistor 1 2.38 / 4.93 70.31 58.61 64.14 17.98 0.52 %
Polyhistor n/8 4.08 / 6.63 71.18 59.52 65.04 17.81 1.70 %
Polyhistor n/4 6.41 / 8.96 70.87 59.54 65.47 17.47 2.34 %
Polyhistor n/2 11.08 / 13.63 71.31 60.15 65.46 17.40 2.84 %
BitFit, PHM layer, Compacter, LoRA, and Low-rank Adapter). This trend is aligned with what we
found in Swin Transformer. We show that our method generalizes to different backbones.
E Experiment results of self-supervised models.
We conduct an experiment using the self-supervised Swim Transformer-Tiny (MoBY-Tiny [ 33]), and,
for a fair comparison, we also run all baseline with MoBY-Tiny and report the results in the Table 8.
We ﬁnd our proposed method can achieve similar or even better results compared to the Hyperformer
[2] while using much less trainable parameters.
F Discussion on difference to Visual Prompt Tuning [9]
We summarize the difference between Visual Prompt Tuning and our method in the following points.
Different Problem Settings : Visual Prompt Tuning focuses on single-task parameter-efﬁcient adap-
tation, while our proposed method focuses on multi-task parameter-efﬁcient adaptation. Our goal is
to perform a parameter-efﬁcient adaptation for multiple tasks and share the beneﬁcial information
across multiple vision tasks.
Different types of parameter-efﬁcient methods : Visual Prompt Tuning adds learnable parameters
along with the visual embeddings, while our proposed method utilizes a shared hyper-network to
produce the adapter weights for different tasks. Also, the insertion locations of learnable parameters
are different (VPT: input space, Ours: parallel to fully-connected layers).
14

--- PAGE 15 ---
Table 6: Limited improvements from tuning hyper-parameters on baseline method. uprepresents
relative improvement or gap to the Single-task Full Fine-tuning. Results with the symbol "=#
indicate higher/lower is better. Results with the symbol "=#indicate higher/lower is better. 
represents the down-projection ratio of adapters ( i.e.,. the ratio of adapter input dto hidden vectors n,
=d=n).
Num. of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder / All Seg. "H.Seg."Sal."Normals# up
Single-task Full Fine-tuning 110.07 / 112.62 67.21 61.93 62.35 17.97 0.00 %
Fine-tuning Decoders 0.00 / 2.55 63.14 52.37 58.39 20.89 -11.02 %
Compacter++ ( = 1) [12] 0.14 / 2.69 67.33 55.68 59.50 19.66 -5.98 %
Compacter++ ( = 2) [12] 0.11 / 2.66 67.26 55.69 59.47 19.54 -5.84 %
Compacter++ ( = 8) [12] 0.09 / 2.64 67.19 55.85 59.48 19.56 -5.96 %
Compacter ( = 1) [12] 0.28 / 2.83 67.94 56.23 60.18 19.25 -4.69 %
Compacter ( = 2) [12] 0.23 / 2.78 68.08 56.41 60.08 19.22 -4.55 %
Compacter ( = 8) [12] 0.19 / 2.74 68.15 56.16 60.12 19.37 -4.83 %
Adapter (= 1) [21] 17.32 / 19.87 69.13 57.35 61.17 18.79 -2.75 %
Adapter (= 2) [21] 8.69 / 11.24 69.21 57.38 61.28 18.83 -2.71 %
Adapter (= 4) [21] 4.37 / 6.92 68.93 57.33 61.24 18.95 -3.03 %
Adapter (= 8) [21] 2.21 / 4.76 69.04 57.34 61.25 18.86 -2.86 %
Adapter (= 16 ) [21] 1.13 / 3.68 69.03 57.22 61.17 18.91 -3.01 %
Shared Adapter ( = 1) [20] 4.35 / 6.89 70.57 59.43 62.54 19.07 -1.21 %
Shared Adapter ( = 2) [20] 2.20 / 4.74 70.21 59.15 62.29 19.26 -1.83 %
Shared Adapter ( = 4) [20] 1.12 / 3.66 70.02 58.87 62.09 19.35 -2.22 %
Shared Adapter ( = 8) [20] 0.58 / 3.12 69.63 58.54 61.74 19.61 -2.99 %
Hyperformer ( = 8) [16] 72.77 / 75.32 71.43 60.73 65.54 17.77 2.64 %
Hyperformer ( = 16 ) [16] 37.69 / 40.24 71.28 60.19 65.82 17.89 2.31 %
Hyperformer ( = 32 ) [16] 20.15 / 22.70 71.12 59.71 64.41 19.06 -0.14 %
Polyhistor (Ours) 6.41 / 8.96 70.87 59.54 65.47 17.47 2.34 %
Polyhistor-Lite (Ours) 0.41 / 2.96 70.24 59.12 64.75 17.40 1.74 %
Table 7: Experimental results on Multi-Task Adaptation. We use Pryramid Vision Transformer-Small
as the feature backbone. uprepresents relative improvement or gap to the Single-task Full Fine-
tuning. Results with the symbol "=#indicate higher/lower is better. represents the down-projection
ratio of adapters ( i.e.,. the ratio of adapter input dto hidden vectors n,=d=n).
Number of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder/All Seg. "H.Part"Sal."Normals# up
Single-task Full Fine-tuning 95.88 / 97.99 68.81 61.27 62.67 17.55 0.00 %
Fine-tuning Decoders 0.00 / 2.11 64.86 51.18 61.54 19.55 -8.85 %
Bitﬁt [7] 0.22 / 2.34 71.41 55.71 64.08 18.69 -2.38 %
Adapter [21] 0.79 / 2.90 71.94 56.38 64.16 18.75 -1.97 %
LoRA [10] 0.30 / 2.41 71.89 56.90 64.27 18.48 -1.35 %
Low-rank adapter 0.25 / 2.36 70.72 55.34 63.39 18.70 -3.08 %
PHM layer [12] 0.42 / 2.53 70.81 55.02 63.51 18.75 -3.20 %
Compacter++ [12] 0.09 / 2.20 70.29 54.80 63.16 18.82 -3.71 %
Hyperformer [16] 14.03 /16.14 70.81 57.76 65.49 17.75 0.14 %
Polyhistor-Lite (Ours;= 1) 5.21 / 7.32 71.00 57.52 65.83 17.83 0.13 %
Polyhistor-Lite (Ours;= 32 ) 0.29 / 2.40 70.93 56.71 65.00 17.95 -0.73 %
G Implementation Details
For a fair comparison between different methods, we use batch size 12and train for 60epochs for
each task. We use Adam optimizer [ 34] with the learning rate 1e 4and the weight decay 1e 4,
and the learning rate is linearly decreased with respect to the training iteration.
We followed the prior multi-tasking learning work [ 31] to use task-wise weighting on different losses,
while we found that using the uniform weights on the losses has similar results as the task-wise
weighting. We also applied the same data augmentations, RandomHorizontalFlip ,RandomScale
15

--- PAGE 16 ---
Table 8: Experimental results on Multi-Task Adaptation. We use MoBY-Tiny [ 33] as the feature
backbone. uprepresents relative improvement or gap to the Single-task Full Fine-tuning. Results
with the symbol"=#indicate higher/lower is better. represents the down-projection ratio of
adapters ( i.e.,. the ratio of adapter input dto hidden vectors n,=d=n).
Number of Trainable Parameters Performance of Each Downstream Task Averaged Results
Encoder/All Seg. "H.Part"Sal."Normals# up
Single-task Full Fine-tuning 110.07 / 112.62 65.52 61.78 62.05 18.14 0.00 %
Fine-tuning Decoders 0.00 / 2.55 59.64 52.97 59.60 19.88 -9.21 %
Bitﬁt [7] 0.30 / 2.85 63.43 54.90 59.50 19.80 -6.90 %
VPT-shallow [9] 0.02 / 2.57 59.50 52.84 59.48 19.88 -9.36 %
VPT-deep [9] 0.88 / 3.43 56.15 50.30 57.22 20.71 -13.72 %
Adapter [21] 8.69 / 11.24 65.00 56.66 60.84 18.64 -3.45 %
LoRA [10] 0.32 / 2.87 65.64 57.66 62.29 18.47 -1.99 %
Low-rank adapter 0.34 / 2.89 63.30 55.24 59.72 19.14 -5.82 %
PHM layer [12] 0.59 / 3.14 63.21 54.99 59.70 19.13 -5.95 %
Compacter++ [12] 0.11 / 2.66 62.31 54.69 59.43 19.58 -7.14 %
Hyperformer [16] 19.29 /44.25 66.50 58.97 66.02 17.61 1.56 %
Polyhistor (Ours) 6.41 / 8.96 67.69 59.32 65.15 17.43 2.05 %
Polyhistor-Lite (Ours) 0.41 / 2.96 67.23 58.90 64.62 17.72 1.09 %
with the range [0:75;1:25],RandomRotate with the range [ 20;20], and Resize to(512;512),
which are used in the prior work [31].
For the hyper-parameters of Polyhistor , we set the input dimension of adapter das the dimension of
hidden vectors in SwinTransformers, and the down-projection ratio is set as =d=n= 16 . For the
low-rank output matrix of hyper-networks, we set the rank as n=4, wherenis bottleneck size. We set
the size of task embeddings as 64.
As for the hyper-parameters of Polyhistor-Lite , we also set the input dimension of adapter das
the dimension of hidden vectors in SwinTransformers, and the down-projection ratio is set as
=d=n= 2. For the low-rank output matrix of hyper-networks, we set the rank as n=4, wherenis
bottleneck size. We set the size of task embeddings as 64.
16
