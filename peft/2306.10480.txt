# 2306.10480.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2306.10480.pdf
# File size: 4112926 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 1
IF2Net: Innately Forgetting-Free Networks for
Continual Learning
Depeng Li, Tianqi Wang, Bingrong Xu, Kenji Kawaguchi, Zhigang Zeng, Fellow, IEEE ,
and Ponnuthurai Nagaratnam Suganthan, Fellow, IEEE
Abstract —Continual learning can incrementally absorb new concepts without interfering with previously learned knowledge. Motivated
by the characteristics of neural networks, in which information is stored in weights on connections, we investigated how to design an
Innately Forgetting-Free Network (IF2Net) for continual learning context. This study proposed a straightforward yet effective learning
paradigm by ingeniously keeping the weights relative to each seen task untouched before and after learning a new task. We first
presented the novel representation-level learning on task sequences with random weights. This technique refers to tweaking the drifted
representations caused by randomization back to their separate task-optimal working states, but the involved weights are frozen and
reused (opposite to well-known layer-wise updates of weights). Then, sequential decision-making without forgetting can be achieved by
projecting the output weight updates into the parsimonious orthogonal space, making the adaptations not disturb old knowledge while
maintaining model plasticity. IF2Net allows a single network to inherently learn unlimited mapping rules without telling task identities at
test time by integrating the respective strengths of randomization and orthogonalization. We validated the effectiveness of our approach
in the extensive theoretical analysis and empirical study.
Index Terms —Continual learning, catastrophic forgetting, random representation learning, parsimonious orthogonal space
✦
1 I NTRODUCTION
CONTINUAL learning (CL) is the ability to learn and
remember a sequence of tasks, one at a time, with
all data of the current task available but not previous or
future tasks [1]. However, even the current state-of-the-
art deep neural networks (DNNs) would suffer from se-
vere degeneration of recognition performance, known as
the catastrophic forgetting phenomenon, when tasks are
sequentially trained [2], [3]. The primary causes may be
twofold. (i) DNNs are essentially connectionist models built
by a target task, in which the acquired information is stored
in connecting weights between adjacent layers. A previously
learned mapping of an old task would be erased if one
directly uses the standard gradient descent for learning new
mappings. Consequently, the parameter-overwritten model
would only remember the most recently learned task well.
(ii) One of the most basic paradigms in machine learning
is that test data are similarly distributed as the training
data, called the independent and identically distributed
(IID). However, Non-IID occurs when each task appears
in sequences, e.g., input distributions are changing or/and
•D. P . Li, T. Q. Wang, and Z. G. Zeng are with the School of Artificial
Intelligence and Automation, with the Institute of Artificial Intelligence,
Huazhong University of Science and Technology, and also with the Key
Laboratory of Image Processing and Intelligent Control of Education
Ministry of China, Wuhan 430074, China. E-mail: {dpli, tianqiwang,
zgzeng}@hust.edu.cn.
•B. R. Xu is with the School of Automation, Wuhan University of
Technology, Wuhan 430070, China. E-mail: bingrongxu@whut.edu.cn.
•K. Kawaguchi is with the School of Computing, National University of
Singapore, 117417, Singapore. E-mail: kawaguch@csail.mit.edu.
•P . N. Suganthan is with the School of Electrical and Electronic Engineer-
ing, Nanyang Technological University, Singapore 639798, and also with
the KINDI Center for Computing Research, College of Engineering, Qatar
University, Doha, Qatar. E-mail: epnsugan@ntu.edu.sg.
Manuscript received December 25, 2022.
(Corresponding author: Z. G. Zeng.)unknown new categories emerge in future tasks. This leads
to the crosstalk among labels at the output layer when it
comes to decision-making.
Recently, CL has received increasing interest from the
machine learning community [4], [5], [6], [7], [8]. Prior work
that is widely known on this topic falls into three families.
First, replay-based approaches, e.g., exemplar replay, refer
to straightforward maintaining a limited quantity of sam-
ples from each task seen and then blending them with that
of a new task to retrieve the learned knowledge [9], [10].
Though aided with exemplar buffers, a bounded memory
budget typically carries inadequate task-specific knowledge,
and the learning would still be biased toward the current
mapping. Furthermore, these methods are infeasible when
the data from previous tasks are not accessible due to
memory constraints or privacy issues [11], [12]. Their gener-
ative replay counterparts, alternatively, aim at yielding past
observations in pseudo samples by using data generators
[13], [14]. However, this transfers the stress from an assessed
model to the generative network, making it demanding for
the external module to precisely recover these old map-
pings [15]. Second, regularization-based approaches attempt
to restrict weights without major changes by imposing
penalties layer-by-layer [16], [17]. To this end, accumulative
regularization terms are utilized to connection weights such
that the updates only occur in constrained directions during
sequential training [12], [18]. Although these methods avoid
storing data, the objectives of past tasks make a model
rather inflexible to find the optimal parameters when the
number of tasks is large [19]. Third, dynamic architecture-
based approaches progressively expand a network where
the newly assigned weights are exclusively in charge of
new tasks [20], [21], [22]. This equates to allocating extra
capacity for each task, and the memory complexity scalesarXiv:2306.10480v1  [cs.LG]  18 Jun 2023

--- PAGE 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 2
with the number of tasks [23]. Interestingly, catastrophic
forgetting cannot be intentionally attributed to insufficient
network capacity because the same-sized networks can well
accommodate multiple tasks when trained in an interleaved
fashion or by using the data of all tasks seen so far [24], [25].
Numerous follow-up works focused on the deficiencies
observed above and further enhanced the CL performance
in various ways, such as diversifying the limited exemplars
[26], [27], constraining the magnitude of weight changes
[28], [29], and compressing the extra expanded network
size [30], [31], as summarized later in the related work
section. However, we find that, in most CL approaches, the
layer-wise updates of weights optimized by using stochastic
gradient descent inevitably either interfere with old knowl-
edge or obstruct learning for incoming tasks. This motivates
our study on harnessing highly plastic weights and explor-
ing the weight invariance during sequential training, and,
most importantly, designing a simple network that is inher-
ently robust against forgetting. Hence the name, innately
forgetting-free network (IF2Net). The core idea is to retain
the weights relative to each seen task untouched all the time,
which is not well studied to the best of our knowledge.
In each training session, IF2Net, with randomization and
orthogonalization, provides a systematic solution for CL
challenges, such that it potentially learns unlimited parallel
input-to-output mappings under the task-agnostic setting.
The investigation presented can be regarded as an ex-
tended study of the existing CL families, providing an
innovative idea and direction for future research on CL. The
main contributions and strengths of this work include:
•A randomization-based representation-level learning
technique is first developed to perfectly reproduce
the representations from task sequences. We realized
it via the forward passes unsupervised, relaxing the
computational and memory requirements compared
with the back-propagation algorithm. This allowed,
for the first time, random weights (and biases) to be
recycled to yield discriminative representations.
•We introduced orthogonalization-based decision-
making into the final output layer to break through
the stability-plasticity dilemma. Consequently, the
output weights are updated in the direction orthogo-
nal to the subspace spanned by just obtained features
from the penultimate layer, eliminating the inflex-
ibility in the original layer-by-layer practice to the
fullest. The parsimonious orthogonal space guaran-
tees output weights unaffected relative to all tasks
seen. In particular, we mathematically derived the or-
thogonal projection matrix for modulating gradients
and provided proof to its learning without forgetting.
•IF2Net is memory-efficient compared with replay
and dynamic architecture-based methods because it
requires no additional exemplars/parameters as new
tasks arrive. Besides, the proposed method is com-
patible with most, if not all, CL methods due to its
generality and, therefore, prompts those respective
techniques in achieving new state-of-the-art results,
e.g., together with the typical elastic weight consoli-
dation (EWC) for a marginal boost.
•We demonstrated the effectiveness of our approachin the class-incremental learning scenario, evaluated
by three metrics, and extensive experiments sug-
gest that IF2Net consistently outperformed the prior
state-of-the-art methods on MNIST, FashionMNIST,
CIFAR-100, Stanford Dogs, CUB-200, and ImageNet-
Subset benchmarks.
The remainder of this paper is organized as follows.
Section 2 introduces related works. In Section 3, we present
the CL setups and then detail the proposed IF2Net in Section
4. Section 5 conducts extensive experiments on six bench-
mark datasets to assess the superiority of our approach.
Finally, we conclude our paper and suggest future research
in Section 6.
2 R ELATED WORK
Continual Learning. A learning paradigm, also known as
lifelong/incremental learning, has been proposed to strike a
balance between stability and plasticity [32], [33]. It places
a single model in a dynamic environment where it must
adapt a stream of tasks to make new predictions without
forgetting [25], [34]. There are three main categories of recent
CL algorithms [4], [5], [32], [35].
Replay-based approaches construct an exemplar buffer
to save samples from previous tasks and train with current
task data. As a representative, gradient episodic memory
(GEM) obtains a positive transfer of knowledge to previ-
ous tasks by independently constraining the loss of each
episodic memory non-increase [10]. Incremental learning
with dual memory (IL2M) utilizes both bounded exemplar
images and past class statistics to rectify the network pre-
dictions [36]. Rainbow memory (RM) focuses on the classifi-
cation uncertainty to select hard samples [37]. Furthermore,
[26] exploits the abundant semantic-irrelevant information
from unlabeled data to diversify the limited exemplars.
However, replay-based approaches either generally deterio-
rate the performance with a smaller buffer size or are even-
tually not applicable to cases where data privacy should
be taken into account [11], [12]. Our approach does not
rely on the exemplar buffer to tackle forgetting since it can
inherently accommodate the previously learned mappings,
thus satisfying the data privacy criterion.
Regularization-based approaches limit the model plas-
ticity on learning important parameters of previous tasks
by adding extra regularization terms. Elastic weight consol-
idation (EWC) is a trailblazer of this family, which employs
a sequential Bayesian estimation and a Fisher information
matrix to regularize weight updates [16]. Representative ap-
proaches following it include synaptic intelligence (SI) [17],
memory aware synapses (MAS) [12], and each network pa-
rameter is associated with the weight importance computed
by different strategies. Under the Bayesian framework, an-
other set of methods takes the posterior distribution of
network parameters as the implicit penalty [38], [39], [40].
Although these methods address forgetting to some extent
without storing past examples, they cannot perform satis-
factorily in challenging settings or with complex datasets.
By contrast, our approach could alleviate their demanding
requirement of parameter optimization and take with them
to set a new state of the art.

--- PAGE 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 3
Dynamic architecture-based approaches dynamically
adapt network architectures by expansion or mask opera-
tion to accommodate knowledge needed for novel tasks. A
progressive neural network (PNN) was proposed to grad-
ually add new branches for all layers horizontally [41]. A
random path selection network (RPS-Net) leverages parallel
modules at each layer in which a possible searching space is
formed to contain previous task-specific knowledge [32] for
mitigating the uncontrollable enlargement of network size.
The model parameters in additive parameter decomposition
(APD) are decomposed into shared and task-specific ones
using masks [42]. Efficient feature transformation (EFT) pro-
poses a compact task-specific framework to overcome catas-
trophic forgetting [20]. Per-class learning (PCL) employs a
small-size multi-head setup where it branches an exclusive
output layer for the classes learned so far via one-class
learning [23], [43]. With prompt-based learning, [44] designs
a query mechanism to dynamically look up a subset of task-
relevant prompts by slightly introducing additional prompt
parameters. On the contrary, our approach is characterized
by a non-growing network, meaning that IF2Net could be
constant in the number of parameters.
Neural Networks with Random Weights. In conven-
tional single-task learning, training a single-hidden layer
feed-forward neural network with random weights (NN-
RW) has demonstrated great potential in developing easy-
implementation models, during which only the output
weights need to be tuned [45], [46], [47]. Over decades of
research, NN-RW is receiving more and more attention [48],
[49], [50], [51], [52]. Readers may refer to [53], [54], [55],
[56] for a comprehensive review of randomized learning
techniques. Here, a crucial technical issue lies in randomly
assigning the input weights (and biases) such that the
randomized learner model shares a universal approxima-
tion capability in either a deterministic or nondeterministic
manner. Randomly assigning input weights (and biases)
from a fixed probability distribution sounds simple and
fast and has been commonly adopted by random vector
functional-link network (RVFLN) [45], extreme learning ma-
chine (ELM) [57], stochastic configuration network (SCN)
[49], and broad learning system (BLS) [50]. The resulting
neural networks with fixed random hidden parameters may
have a universal approximation capability in a probability
sense, provided that an appropriate distribution (e.g., a
uniform range [r,−r], r > 0) is properly set in advance [58],
[59], [60]. This means a certain “support range” leading to
a randomized universal approximator exists and is data-
dependent for a given task. Inspired by this technical point,
we aimed to extend the fixed random hidden parameters
to overcome catastrophic forgetting. However, no work has
been presented so far to CL with the help of a randomized
learning technique due to its random nature (performance
fluctuation), shallow architecture (inferior representation),
and IID condition (same as DNNs). This motivated our in-
vestigation on building stable representation-level learning
with randomized multiple-hidden layers on task sequences
to fill the gap. To this end, we would significantly improve
NN-RW, as explained in Sections 4.2 and 4.5.1.
Orthogonal Gradient Descent. Lately, some literature
has started to consider the orthogonal gradient descent
strategy, a back-propagation variant [61], by keeping theinput-to-output mappings fixed [28], [62], [63]. To this
end, network weights are sequentially optimized in the
orthogonal space of each linear layer’s previously learned
inputs/features. The pioneering works include conceptor-
aided backpropagation (CAB) [28] and orthogonal weights
modification (OWM) [62], which resorts to different math-
ematical theories to compute the orthogonal subspace ap-
proximately. Concretely, an orthogonal projection matrix in
each layer of networks is constructed to preserve the learned
knowledge. Furthermore, the gradients in neural networks
are projected to the orthogonal direction of all previously
learned features by the orthogonal projection matrix during
learning a new task. Unlike CAB and OWM, we only ap-
plied it to the final output layer for decision-making while
simplifying the form of the orthogonal projector. Detailed
differences and strengths will be discussed in Sections 4.3,
4.5.2, and 4.5.3.
3 C ONTINUAL LEARNING SETUP
Definition. We focused the scope of our work on the
most prevalent yet challenging class-incremental learning
scenario, namely CIL [64], [65]. A model in this scenario
solves each task seen so far and infers which task it would be
encountered with, which includes the common real-world
problem of incrementally learning new classes of objects.
Mathematically, let us start by defining the CIL. Given
the supervised learning datasets Dt={(Xt,Yt)|Xt∈
RNt×Mt,Yt∈RNt×Ct}of task t(t= 1,2, . . .), where Xt
is the input, Ytis the label, Ntis the number of samples, Mt
andCtare the dimensions of input data and output classes,
respectively. Assumed a model f(Xt−1;θt−1) (t≥2)
trained on previous task(s), parameterized by its connection
weights θt−1, the objective is to train an updated model
f(Xt;θt)that can accommodate the newly emerging Ct
classes [66], that is, f(XT;θT)needs to remember how
to perform well on the cumulative C=PT
t=1Ctclasses.
Meanwhile, the data of previous tasks Dt(t= 1,2, . . . , T −
1) forf(XT;θT)is unavailable except for the replay-based
approaches/baselines, e.g., a handful of samples in exem-
plar buffers can be revisited. At test time, the resulting
model would be fed samples from any of tasks 1 to T
without telling the task identifier/oracle and evaluated by
the standard CL metrics formulated as follows.
Metrics. We monitored three statistics to reflect the qual-
ity of CL methods (all higher is better). First, the final average
test accuracy (ACC) reported in the majority of literature is
defined as:
ACC =1
TTX
t=1RT,t (1)
where RT,tmeans the test classification accuracy of a model
on task tafter training on task T. ACC measures how the
average performance of the model degrades as it learns new
tasks and directly shows the test performance of a model on
all tasks seen so far. Second, backward transfer (BWT) [10] is
defined as the difference in accuracies between when a task
is first trained and after training on the final task, averaged
over all tasks:

--- PAGE 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 4
BWT =1
T−1T−1X
t=1RT,t−Rt,t (2)
It indicates a model’s ability in knowledge retention. As
a result, the lower BWT value corresponds to catastrophic
forgetting, while BWT = zero is considered forgetting-free.
Third, forward transfer (FWT) [67] defines the average im-
provement in accuracy on a new task over an independently
trained model on that task, which is expressed as:
FWT =1
T−1TX
t=2Rt,t−Rind
t (3)
where Rind
tis the test classification accuracy of an inde-
pendent model trained only on task t. FWT estimates how
well a model uses previously learned knowledge to improve
classification accuracy on newly seen tasks. Indeed, BWT
and FWT complement ACC, e.g., if two models have similar
ACC, the preferable one shares the larger BWT and FWT.
4 I NNATELY FORGETTING -FREENETWORK
4.1 Overview
This section explains how we defied the interference among
sequential tasks by designing an innately forgetting-free
network termed IF2Net. To this end, we incorporated
dual projections (randomization and orthogonalization) into
IF2Net such that it could keep the weights relative to
each seen task untouched before and after learning a new
task. In particular, the CL process is primarily mediated
by the randomization-based representation-level learning
in several hidden layers and the orthogonalization-based
decision-making in the final output layer. An overview is
shown in Fig. 1. To obtain a good grasp of the learner model
produced by IF2Net, theoretical verification of two key
properties, such as convergence analysis of representation
approximation and proof of learning-without-forgetting de-
cision, are provided with rigorous mathematical deductions,
showing the feasibility of the proposed method under cer-
tain conditions. After this, we will highlight the prime and
original contributions with much in-depth discussion.
4.2 Randomization based Representation-Level Learn-
ing
The motivation for introducing randomization-based
representation-level learning is threefold. First, directly re-
training a network modifies its connecting weights away
from optimal solutions to old tasks with new learning over-
writing existing representations. Second, even if additional
“memories” (such as exemplar buffers, model weights, and
regularization matrices) of past tasks are used to retrieve
previously learned knowledge, the layer-wise updates of
weights optimized by stochastic gradient descent either
interfere with old knowledge or obstruct learning for incom-
ing tasks. Third, NN-RW with fixed random hidden param-
eters can share a universal approximation property given a
task [68], that is, one can deemphasize the importance of
weights by randomly assigning the input weights (and bi-
ases). Therefore, instead of optimizing hidden-layer weights
Output layer  Hidden Lyers Pre-trained CNN
0
Node block
Weights via 
randomizationWeights via 
orthogonalization
Current Task
FC1 FC2 FC3
Input/Output node Extracted features
Parsimonious 
orthogonal space0
Frozen LayersRandomization based 
Representation -Level
LearningNew 
InputNew 
Input
Output 
⊥
⊥
⊥
⊥
⊥
Legend:
Orthogonalization 
based Decision 
Making
11
nV
1
2V
1
1V
2
1V
2
2V
22
nV
33
nV
3
2V
3
1V
None for toy tasks;
ResNet for challenging tasks  Fig. 1. The proposed pipeline (top panel) and an IF2Net architecture
used in this study (bottom panel). IF2Net takes a new input image
and passes it through frozen layers of a fixed pre-trained CNN to
obtain rich representations. It then proceeds to randomization-based
representation-level learning with reused random weights (and biases)
connecting FC1-FC3, termed hidden layers. The output weight in the
output layer is updated in the parsimonious orthogonal space spanned
by discriminative features obtained from FC3 to make learning-without-
forgetting decisions. Dashed boxes indicate components that are not
required (tasks can be sequentially fed with the original inputs Xor
extracted features XFwhen it is necessary).
of a fixed network, we instead emphasize the required acti-
vations (representations) that perform well over a sequence
of tasks. This technique challenges common practice to push
the drifted representations caused by randomization back
to their separate task-optimal working states, during which
the involved random weights (and biases) are reused. The
details are as follows.
4.2.1 Representation-level learning process
Before the representation-level learning starts, IF2Net lever-
ages the representative features from a pre-trained CNN
(although it is not required); however, instead of tuning the
parameters during the CL process, IF2Net keeps the pre-
trained model fixed since the early layers of CNN can be
highly transferable [69], [70], and incrementally learns a set
of rich representations from incoming tasks when used, re-
sembling what [23], [44], [62], [70], [71] has been performed.
Hence, the input of IF2Net can be the original samples X
from toy tasks (MNIST and FashionMNIST) or the extracted
features XFfrom high-dimensional tasks (Stanford Dogs,
CUB-200, CIFAR-100, and ImageNet-Subset). We used Xto
denote both variables to simplify the notation.
The topology used in representation-level learning com-
prises several commonly used fully connected (FC) layers,
but the training process is entirely different from that in pre-
vious work. With the help of randomly initialized weights
(and biases) in the hidden layers, the inputs/features are
transformed into a random representation space, which
can help exploit hidden information among the training
samples. In particular, each layer l(l= 0,1, . . . , L −1)in
this subpart randomly allocates Slnodes that are composed

--- PAGE 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 5
Fig. 2. Randomization-based representation-level learning process.
Take FC1 as an example. When Xt(t= 1,2, . . .) is present, we
first used the randomly assigned weights (dotted arrow in black, left) to
potentially yield inferior responses called drifted representations (solid
arrow in black, right). Then, they were tweaked by unsupervised fine-
tuning back to their separate task-optimal working states (solid arrow in
red, right). The forward representation-level learning can perform well
on task sequences without weight modification. Best viewed in color.
ofnlgroups of node blocks, with each group slnodes to
diversify their counterparts of output representations. In
the following, we used a simple three-layer fully connected
implementation (see the Hidden Layers located in the bot-
tom panel of Fig. 1) to describe the representation-level
learning process. First, we defined the forward function of
jth (j= 1,2, . . . , n l+1) node block at layer l+ 1as follows:
Vl+1
j=σ(˜VlWl
j+ 1Ntbl
j) (4)
where ˜Vl∈RNt×Slis the tweaked activations (represen-
tations) from preceding layer l, matrix Wl
j∈RSl×sl+1and
row vector bl
j∈R1×sl+1are the randomly assigned weights
within a proper scope setting (e.g., [−1,1]) [59], 1Nt∈RNt
is the column vector with every entry being one, and σ(·)is
the activation function at each layer, respectively. Note that
˜V0=Xtandθ= (Wl
j,bl
j). However, the above single
forward passes potentially lead to drifted representations
Vl+1
j (j= 1,2, . . . , n l+1) caused by randomization [49],
[59], as depicted in Fig. 2.
To address this issue, we formulated it as an optimiza-
tion problem, and the objective function is defined as:
˜θ∈argmin
(˜Wl
j,˜bl
j)∥Vl+1
j˜Wl
j+1Nt˜bl
j−˜Vl∥2
2+∥˜Wl
j∥1+∥˜bl
j∥1(5)
where ˜θ= (˜Wl
j,˜bl
j)is the intermediate solution, ∥ · ∥ 1and
∥ · ∥ 2are the L1andL2norm, respectively. Conceptually,
(5) can serve as a feature processor in hierarchical neural
networks, where the randomized response Vl+1
j is used to
approximate the input ˜Vlby minimizing the reconstruction
errors [50], [72]. Then, the drifted representation Vl+1
j can
be tweaked by replacing raw θwith fine-tuned ˜θ, that is,
˜Vl+1
j=σ(˜Vl˜Wl
j+ 1Nt˜bl
j). Hence, this forward pass can
manage to slightly fine-tune each bunch of randomized
responses Vl+1
j (j= 1,2, . . . , n l+1) in an unsupervised
manner but freeze the involved weight θ= (Wl
j,bl
j),
such that the drifted representations of each task could al-
ways be dragged back to their separate/initial task-optimal
working states ˜Vl+1
j (see Fig. 2) and thus flawlessly repro-
duce representations of each task seen so far, as explained
later in Section 4.2.2. Meanwhile, we can recursively obtain
the tweaked activations ˜Vl+1= [˜Vl+1
1,˜Vl+1
2, . . . , ˜Vl+1
nl+1]
(a)
 (b)
 (c)
(d)
 (e)
 (f)
Fig. 3. t-SNE visualization based on Split FashionMNIST benchmark.
Each color represents a class, and two classes are incrementally fed
into the hidden layers as an independent task. In this task sequence,
training data of all the classes together are first visualized as a reference,
followed by the tweaked representations after incrementally learning two
classes per session. (a) Mixed raw sample space. (b)-(f) Well-clustered
representation space that corresponds to the projected outputs of FC3.
IF2Net can yield distinguishable representations from a sequence of
tasks for the final decision-making. Best viewed in color.
in layer l+ 1 by concatenating all nl+1groups of node
blocks. We will validate the efficiency of multiple node
blocks in Section 5.3.2. The above randomization-based
representation-level learning demonstrates that reused ran-
dom weights (and biases) are feasible for obtaining optimal
representations from task sequences.
The intuition behind representation-level learning is that
most CL methods are driven by weight-level optimization,
which is not necessarily the underlying way that connec-
tionist models distinguish and remember knowledge. The
involved weights in hidden layers can be transitional or dis-
posable. In this sense, rather than judging CL performance
with globally optimized weights, we can measure it with
the current representations. Hence, the latter is comparable
to some approaches for single-task learning. Consider the
FashionMNIST as an example. Fig. 3 depicts the t-SNE
visualization [73] of the raw sample space and the final pro-
jected representations that correspond to the outputs of FC3,
which indicates the good potential of randomization-based
representation-level learning. The distinguishable represen-
tations obtained from a sequence of tasks are then used for
decision-making in the final output layer.
4.2.2 Convergence analysis
Here, we provided an iterative algorithm based on the
discrete-time projection neural network [74] to solve the
optimization problem in (5) and then provided a theoretical
analysis to explain whether the proposed representation-
level learning converges under certain conditions.
Letg(u):Rn→Rnbe a projection operator defined on
a closed convex set Ω={u⊆Rn:li≤ui≤hi, i=
1,2, . . . , n }then,
g(ui) =

hiui> hi
uili≤ui≤hi
liui< li(6)

--- PAGE 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 6
which is a piecewise linear function. The property for
problem-solving is presented in the following lemma:
Lemma 1. [75] Assume there is a full-rank matrix A. For
anyx∈Rn,Ax=bif and only if Zx=q, where
Z=AT(AAT)−1Aandq=AT(AAT)−1b.
With these preliminaries, we can generally rewrite the
objective function in (5) as:
arg min
x∗:1
2∥Zx∗−qt∥2
2+λ∥x∗∥1 (7)
where t∈N+is the task index, qtis the desired input vector,
Zis the drifted representations, λis a positive scalar weight,
andx∗is the surrogate associated with ˜θ. The convergence
property of the proposed representation-level learning can
be stated in the following theorem:
Theorem 1. Fort∈N+,x∗∈Rnis an optimal solution to
problem (7) if and only if there exits y∗∈Rnsuch that
x∗andy∗satisfy:
(
Zx∗−qt+λy∗= 0
y∗=g(y∗+x∗).(8)
Proof 1. x∗∈Rnis an optimal solution to problem (7) if and
only if there exits y∗∈Rnsuch that x∗andy∗satisfy
the following condition:
Zx∗−qt+λy∗= 0, t= 1,2, . . . (9)
where y∗= [y∗
1, y∗
2, . . . , y∗
n]Tis a subgradient of ∥x∗∥1
with each component y∗
j(j= 1,2, . . . , n )defined as:
y∗
j

= 1 x∗
j>0
∈[−1,1] x∗
j= 0
=−1 x∗
j<0(10)
Then, y∗satisfies:
y∗=g(y∗+x∗) (11)
where g(·)is a projection operator from Rn→[−1,1]n
as defined in (6) with li=−1andhi= 1. □
Without loss of generality, the solution to (7) can be
iteratively described as follows [74]:
(
x(k+ 1) = x(k)−γ(Zx(k))−qt+λg(y(k) +x(k))
y(k+ 1) = g(y(k) +x(k+ 1))
(12)
where γ > 0is the gain of the network, x(k)is the state
vector, and y(k)is the output vector, respectively.
The aforementioned equation theoretically analyzed
how the proposed representation-level learning technique
can converge the randomized responses of different tasks
to their optimum values. We presented pseudo-codes of
the representation-level learning process in Algorithm 1
to clarify the algorithm implementation. In addition, we
noted that it achieves the same performance if (7) is solved
in an alternative manner, such as by extending the fast
iterative shrinkage-thresholding algorithm (FISTA) [76] to
representation-level learning.Algorithm 1 Randomization based Representation-Level
Learning
Input: Datasets Dt={(Xt,Yt)}of task t; A base network
with Lhidden layers; Number of node blocks nland
nodes in each block sl; The positive scalars λandγ.
Output: Resulting representations of Lth layer VL.
1:# initialize parameters from a uniform range
2:forl= 0,1, . . . , L −1do
3: forj= 1,2, . . . , n l+1do
4: Randomly assign weights Wl
jand biases bl
j;
5: end for
6:end for
7:# representation-level learning on sequential tasks
8:fort= 1,2, . . . , T do
9: forl= 0,1, . . . , L −1do
10: Obtain drifted representations Vl+1based on (4);
11: Tweak drifted representations based on (5);
12: end for
13:end for
4.3 Orthogonalization based Decision-Making
In the CL context, the final output layer, dubbed a classi-
fier, must accommodate the changing input distributions
or/and new unknown categories emerging in future tasks,
which is vital to both previously learned and incoming
tasks. We implemented an orthogonalization algorithm that
theoretically achieved zero-forgetting decisions by making
updated weights that do not disturb old knowledge while
maintaining model plasticity. In the following, we explained
why we expect to form a parsimonious orthogonal space for
decision-making, how we leveraged orthogonalization and
implemented output weight updates efficiently.
4.3.1 Parsimonious orthogonal space
Using the orthogonal gradient descent method, network
weights can be sequentially optimized in the orthogonal
space of all previously learned inputs/features for each FC
layer [28], [62], [63]. However, because the trained weights
are bounded to the counterpart space formed by the pre-
vious tasks, minimizing the mean squared error or cross-
entropy loss would lead to two major problems:
First, layer-wise orthogonalization entails massive com-
putational and memory requirements that are proportional
to the number of network layers. It needs to store and
update the projection matrix for each layer, accounting for
why the orthogonal gradient descent algorithm sometimes
requires sufficient memory to maintain space, especially in
models with many network layers and nodes. More impor-
tantly, it is more vulnerable to increasing the generalization
bound error during sequential training, which adversely
affects the knowledge retention of previous tasks.
Second, layer-wise orthogonalization makes it difficult
to find the optimal parameters when CL tasks are chal-
lenging. A standard gradient descent usually suffers from
slow convergence and traps at a local minimum, let alone
its orthogonal implementation, by updating the network
weights along the constrained direction. Particularly, with
the backpropagation of the entire network layer-by-layer,
the constraint is likely to be greatly amplified, degenerating
the new model’s ability to retain knowledge.

--- PAGE 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 7
Unlike these methods, we only applied it to the output
weights for decision-making and refined the form of the
orthogonal projection matrix, as presented in the subsequent
section. Hence, the nature of parsimony mainly referred to
two aspects. (i) For an individual task, it only works on
the final output layer, as opposed to the commonly used
layer-wise operations. This simplification is performed well
by representation-level learning. In this way, our approach
relaxed the computational and memory requirements, as
we do not orthogonalize the weights of reused neurons in
hidden layers and eliminated the inflexibility to the fullest.
(ii) For the sequential tasks, it only cares for one orthogonal
projector based on the representations of the penultimate
layer. By contrast, layer-wise orthogonalization requires the
projector at each layer to be delivered and updated during
learning a new task. As a result, the parsimonious orthogo-
nal space would be better for guaranteeing that the output
weights are impervious relative to all the tasks seen.
4.3.2 Orthogonal projection matrix
We first presented the decision-making process with proof
of its learning property without forgetting, followed by
mathematically deriving the orthogonal projection matrix
for modulating gradients. The details are as follows.
When finishing representation-level learning on task t
(t= 1,2, . . .), IF2Net can sequentially obtain distinguish-
able representations at FC3 termed ˜V3. Hereafter, we de-
noted ˜V3byVt∈RNt×S3for simplicity, where the subscript
tdenotes the task index and S3is the total number of nodes
at FC3. In contrast to the softmax layer, we evaluated the
output weights by solving a linear equation system. Hence,
the learning objective for IF2Net is the following regularized
least-squares minimization problem:
arg min
βt:∥Vtβt−Yt∥2
F+µ∥βt∥2
F (13)
where ∥ · ∥Fis the Frobenius norm, βtis the output weight
matrix, Ytis the training sample label, and µis the trade-
off coefficient. Thus, the corresponding network prediction
ˆYt=Vtβtcan be made at the final output layer by project-
ing the output weight updates ∆βtinto the parsimonious
orthogonal space, where the update direction is guided
by an orthogonal projection matrix Pt(see Fig. 4). When
learning a new task, old output weights are updated in
the orthogonal direction of learned representations to avoid
infringing upon the previously acquired information, even
after experiencing a sequence of tasks. Specially,
βt=βt−1−ηPt∆βt (14)
where ηis the learning rate. Hence, during the decision-
making process, let nonzero matrix Ptsatisfy AtPt=O
(t= 1,2, . . .), in which At= [VT
1,VT
2, . . . ,VT
t−1]Tconsists
of all the previously tweaked representations. The mathe-
matical proof is as follows:
Suppose there have been t(t= 1,2, . . . , T −1) tasks seen
so far, and when IF2Net is trained on Tth task, its current
prediction is ˆYT=VTβTin which βTis updated with (14)
to maintain model plasticity on something new, and each Pt
can be constructed in the following form:
High 
ACC
Low 
ACCTask 1 Task 2
Legend: Stochastic  direction of output weight  update
Orthogonal direction of output weight  updateParsimonious orthogonal space 
spanned by 
Stochastic gradient descent for  
output weight  update
Orthogonal gradient descent for  
output weight  update
2
Initial  position
2
2
2
1
1 Initial  position
Fig. 4. Orthogonalization-based decision-making process. We first pro-
jected the output weight updates ∆βtof task tinto the parsimonious
orthogonal space of all previous tasks, spanned by representations from
the penultimate layer At. Then, an orthogonal projection matrix Ptfor
modulating gradients guarantees output weights unaffected relative to
all tasks seen. For example, the β2updated by stochastic gradient
descent failed to recognize task 1 while it would perform well on tasks
seen by orthogonal gradient descent. Best viewed in color.
Pt=I−AT
t(AtAT
t+αI)−1At
=α(AT
tAt+αI)−1(15)
where αis a relatively small constant [62], [63]. At the test
time, the final model can predict ˆYtfrom any of the tasks 1
toT−1without telling the task identifier, that is,
ˆYt=VtβT
=Vt(βT−1−ηPT∆βT)
=Vt(βt−ηTX
k=t+1Pk∆βk)
=Vtβt−ηTX
k=t+1VtPk∆βk
=Vtβt(16)
Note that AkPk=O(k=t+ 1, . . . , T ) and Vtbelongs
toAk. This completes the proof of the learning-without-
forgetting decisions of IF2Net. □
From the algorithm perspective, however, it is difficult
to find an orthogonal projection matrix Ptthat can always
satisfy AtPt=O(t= 1,2, . . .). To make the computa-
tion of the projector more efficient, we used an iterative
method to approximate the parsimonious orthogonal space
spanned by Vt, much akin to the recursive least squares
(RLS) [77], [78], which can be used to train feed-forward
neural networks to achieve fast convergence. Thus, we

--- PAGE 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 8
regarded the first krows of Vtas a mini-batch; that is,
Vt(k) = [v1,v2, . . . ,vk]T. Similarly, Vt(k+ 1)TVt(k+ 1) =
Vt(k)TVt(k) +vT
k+1vk+1. We denoted Pt(k)as the iterative
form associated with Vt(k). Hence, (15) can be expressed as:
Pt(k+ 1) = α(Vt(k+ 1)TVt(k+ 1) + αI)−1
=α(Vt(k)TVt(k) +vT
k+1vk+1+αI)−1(17)
Then, we applied the Woodbury matrix identity [79] as a
solution to (17), as is presented below. Let AandBbe two
positive-definite M×Mmatrices related to
A=B−1+CD−1CT(18)
where Dis a positive-definite N×Nmatrix and Cis
anM×Mmatrix. Using the matrix inversion lemma, we
obtained
A−1=B−BC(D+CTBC)−1CTB (19)
By specifying B−1=Vt(k)TVt(k) +αIin (17), we have
B−1=αPt(k)−1. Furthermore, let C=vT
k+1andD=I.
According to (18)-(19), we have
Pt(k+ 1) = Pt(k)−Pt(k)vT
k+1vk+1Pt(k)
αI+vk+1Pt(k)vT
k+1(20)
Computing the orthogonal projector using (20) has two
main strengths. (i) Iterative implementation in an efficient
online manner avoids the matrix-inverse operation defined
in (15), which significantly facilitates the update process.
(ii) Each batch is treated as a different task and breaks
the limitation of storing representations from all previous
tasks, for example, V1,V2, . . ., because only the currently
learned Vtand the most recently updated Pt(k)are needed;
therefore, further accelerates the processing. Algorithm 2
illustrates the decision-making process of IF2Net.
Remark 1. We noted that taking the “remembering”
into consideration, the above orthogonalization-based
decision-making can absorb a new input-to-output map-
ping without interfering with the previously learned
ones. However, the process may be less effective for
“learning.” This is because orthogonal regularization
updates the output weights along the constrained direc-
tion, and it is potentially unfavorable to find the optima
during sequential training, especially for task 1, which
functions as the pre-training or initialization operation.
To alleviate this, we retrospect the optimization problem
in (13), and a closed-form solution for parameter initial-
ization is given by:
β∗
1= (VT
1V1+µI)−1VT
1Y1, N1≥S3 (21)
Alternatively,
β∗
1=VT
1(V1VT
1+µI)−1Y1, N1< S 3 (22)
The computational complexity introduced by the matrix
inversion operation can be circumvented by using ei-
ther (21) or (22), depending on the sample size N1or
total representation dimension S3. Empirically, we can
further start with a selected mini-batch to compute the
closed-form solution, followed in the same way as thesubsequent updates of the output weight βt(t≥2).
In Section 5.3.1, we demonstrated the effectiveness of
this analytical initialization for output weight. There-
fore, given the constrained update direction resulting
from layer-wise orthogonalization, we simplified it to
one layer for the final decision-making and introduced
an effective initialization manipulation, eliminating the
inflexibility to the fullest extent.
Algorithm 2 Orthogonalization based decision-making
Input: Datasets Dt={(Xt,Yt)}of task t; A classifier with
a single-head output layer; The positive scalars µ,η, and
α; Rrepresentations of penultimate layer Vt.
Output: Network predictions on tasks seen so far ˆYt.
1:# decision-making on sequential tasks
2:fort= 1,2, . . . , T do
3: ift= 1 then
4: Initialize β∗
1based on (21) or (22);
5: Iteratively approximate Ptbased on (20);
6: else
7: # updates in the parsimonious orthogonal space
8: Orthogonalize gradient by Pt∆βt;
9: Update βtbased on (14);
10: Iteratively approximate Ptbased on (20);
11: end if
12:end for
4.4 Integrating the Strengths of External Conditions
To highlight the generality of our approach, we fur-
ther explored IF2Net’s compatibility with prior work and
leveraged them as external conditions, such as replay-,
regularization-, and dynamic architecture-based strategies,
achieving a marginal boost in learning performance. Given
space limitations, we considered the typical EWC algorithm
as an example and rename it IF2Net-EWC. Both IF2Net and
IF2Net-EWC adopted the same representation-level learn-
ing in the hidden layers but slightly different policies for
updating the output weights during sequential training.
Especially, suppose there have been T−1 (T≥2)
tasks so far. When the Tth task is presented, the improved
learning objective for IF2Net-EWC can be represented as:
arg min
βT:1
2NT∥VTβT−YT∥2
F+µ
2T−1X
t=1∥Qt⊙(βT−βT−1)∥2
F
s.t.:βT=βT−1−ηPT∆βT
(23)
where NTis the number of current training samples, ⊙
is the element-wise product, βT−1is the most recently
learned output weight, and Ft=Qt⊙Qtis the Fisher
information matrix that indicates the parameter importance,
as is formulated below.
Ft=1
NtNtX
p=1∇βTlogp(βT|Dt)∇βTlogp(βT|Dt)T(24)
Thus, IF2Net-EWC retains the knowledge of old tasks by
constraining the output weights to stay in a low-error region

--- PAGE 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 9
while learning a new task, which only allows the new task
to change its output weights that are not important for
old tasks in the parsimonious orthogonal space. In other
words, each output weight with small importance does not
significantly affect the performance and can, therefore, be
changed along the orthogonal direction to minimize the
objective function, while, ideally, it should be left unchanged
with large importance. Moreover, Dtno longer needs to be
revisited or stored once Ftis obtained. In summary, IF2Net-
EWC further incorporates the update quota according to
the parameter importance to implement selective weight
consolidation compared to IF2Net.
Remark 2. Intuitively, only a single term should be main-
tained and anchored at the output weights associated
with the latest task T−1because the most recently
learned weights βT−1inherit the previous weights βt
(t= 1,2, . . . , T −2). Alternatively, multiple penalty
terms can be incorporated into (23) by replacing βT−1
withβt. The latter forces a model to remember older
tasks more vividly by double counting the data from
previous tasks, which might compensate for the fact that
older tasks are more difficult to remember [80]. As a
price, the algorithm exhibits linear growth in memory
requirements as the number of tasks increases. Hence,
we combined only IF2Net with a single term, as formu-
lated in (23).
4.5 Discussion
4.5.1 Can we directly use NN-RW for CL?
As described in the related work section, NN-RW randomly
assigns input weights (and biases), and only the output
weights need to be tuned during the single-task learning
process. Although some useful strategies, such as the su-
pervisory mechanism in SCN [49], and the lasso algorithm
in BLS [50], have been introduced to better leverage the
randomization nature, NN-RW and its variants are data-
dependent and mainly perform well on the IID condition,
making it unfeasible in the CL context. Specifically, the
randomized learning technique can be used to extract dis-
criminative features for a given task, but it fails to work
in a sequence of tasks. This is because the discriminative
information learned for a new task may not be sufficiently
discriminative between old tasks and between old and new
classes [23]. More importantly, NN-RW with task-specific
output weights limits its application to the single-task learn-
ing process, and no work has been presented to guide the
updates of output weights to address forgetting. Therefore,
further improvements are needed to extend NN-RW to the
CL context, and our work fills this gap to a certain degree.
4.5.2 Relationship between IF2Net and orthogonal gradi-
ent decent methods
We now discuss the relationship between the proposed ap-
proach, IF2Net, and the representative orthogonal gradient
descent method, OWM [62]. Both serve the same purpose
of defying catastrophic forgetting within neural networks.
However, there are three main differences between IF2Net
and OWM, which yield substantial performance distinc-
tions. (i) Rather than orthogonal regularization updates ofall the network weights layer-by-layer, we proposed lever-
aging randomization-based representation-level learning in
several hidden layers, as discussed in Section 4.2. It is theo-
retically analyzed that the proposed technique can make the
randomized responses of different tasks converge to their
separate optima. (ii) We then applied orthogonalization
to the final output layer for decision-making because the
feature map extracted from deeper layers is more likely
to contain task-specific information, and the deeper layer
can easily forget previous knowledge [26]. Particularly, we
projected the output weight updates into the parsimonious
orthogonal space spanned by the obtained response from
the preceding layer, that is, our method only needs to main-
tain one orthogonal projection matrix, while the number of
projectors in OWM is equal to that of the network layers.
Our method, together with a closed-form solution for pa-
rameter initialization, can facilitate the convergence process
and eliminate inflexibility in original layer-wise practice.
(iii) Our approach was compatible with most, if not all,
CL methods because of its generality, meaning that one can
easily combine IF2Net with the respective techniques in the
existing CL methods to achieve new state-of-the-art results.
4.5.3 Comparison through the lens of generalization bound
We also discussed the generalization performance to un-
derstand why our method is better than OWM. Based
on Rademacher complexity [81], two supportive lemmas
are presented to explain our intuitive argument, that is,
orthogonalization-based decision-making in the final output
layer is more prone to result in a lower generalization bound
error during sequential training.
This section focused on Rademacher complexity, which
is a standard tool for binding the generalization error (and
hence the sample complexity) of given classes of predictors
[82], [83]. Formally, given a real-valued function class H
and set of data points x1, . . . , x m∈X, we defined the
(empirical) Rademacher complexity ˆRm(H)as:
ˆRm(H) =Eϵ
sup
h∈H1
mmX
i=1h(xi)ϵi
(25)
where ϵ= (ϵ1, . . . , ϵ m)is a vector uniformly distributed in
{-1,+1}andmis the sample size. Bartlett and Mendelson
[82] provided the following generalization bound for the
Lipschitz loss functions:
Lemma 2. [82] Assume the loss ℓis Lipschitz (with respect
to its first argument) with the Lipschitz constant Lℓand
thatℓis bounded by c. For any δ > 0and with a
probability of at least 1−δsimultaneously for all h∈ H ,
we have
L(h)≤ˆL(h) + 2LℓˆRm(H) +cs
log(1/δ)
2m
where L(h) =E[ℓ(h(x), y)]is the expected loss of hand
ˆL(h) =1
mPm
i=1ℓ(h(xi), yi)is the empirical loss.
We can introduce a bound on the Rademacher complex-
ity term using Lemma 2 .
Lemma 3. [84] Let Hdbe the class of real-valued networks
of depth dover the domain X={x:∥x∥ ≤ B}in
Euclidean space, where each parameter matrix Wjhas

--- PAGE 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 10
a Frobenius norm at most MF(j), and with activation
functions satisfying Lemma 1 in [84]. Then,
ˆRm(Hd)≤1
mdY
j=1MF(j)·q
2 log(2) d+ 1vuutmX
i=1∥xi∥2
≤Bp
2 log(2) d+ 1Qd
j=1MF(j)
√m.
where W1, . . . ,Wdin each of the dlayers have
the Frobenius norm ∥ · ∥ Fupper bounded by
MF(1), . . . , M F(d), respectively.
Let us consider these lemmas in the continual learning
context. Suppose that t(t= 1,2, . . . , T −1) tasks have been
trained. During sequential training, one can ensure that the
empirical loss ˆL(h)on the right-hand side of Lemma 2 is un-
changed by the orthogonal decent gradient method. How-
ever, the Rademacher complexity term ˆRm(Hd)can still
increase for task twhen we update network weights at task
T. Thus, while networks do not forget what was learned, the
generalization error for the previous tasks could potentially
increase when weights are updated for later tasks. More-
over, we can use the bound of the Rademacher complexity
term in Lemma 3 . For our method, d= 1 and domain X
refer to the last hidden layer, whereas for OWM, d >1and
domain Xrefer to the input layer. As a result, this leads to a
major difference in how much they increase the Rademacher
complexity term for task t < T −1during training for task
T. From Lemma 3 , it is expected that the proposed IF2Net
does not significantly increase the Rademacher complexity
term compared to OWM. We validated this hypothesis by
plotting the value of the Rademacher complexity term for
our method and OWM in Section 5.3.4.
5 E XPERIMENTS
In this section, we presented extensive experiments to
validate the superiority of the proposed approach using
three evaluation metrics, six standard benchmark datasets,
and 17 baseline methods in the CIL scenario. First, we
introduced the experimental setting. We then provided the
experimental results and discussion, following which we
empirically analyzed the effectiveness of the core designs
in our algorithm.
5.1 Experimental Setting
Protocols. For a fair comparison, we carefully selected the
compared methods in the same environment. (i) Only sam-
ples of current task tare available, except for replay-based
methods, while test samples may come from any of tasks
1 totat test time, during which task identity is unknown.
(ii) Without using a fixed task sequence, we ran each bench-
mark multiple times with randomly shuffled task orderings
and then reported the means and/or standard deviations
of these results. Hence, repeated runs will enter tasks with
agnostic orders prior to training, which is more practical
in an open-ended environment. (iii) We gave preference
to the most well-recognized and best-performing baseline
methods in the “single-head” setting and referred to theoriginal codebases for implementation and hyperparameter
selection to ensure the best possible performance.
Datasets. We followed the common practice in the
CIL scenario to simulate changing input distributions and
emerging new classes by splitting benchmark datasets
[20], [23], [26], [44], [64], [66], [85], including MNIST,
FashionMNIST, Stanford Dogs, CUB-200, CIFAR-100, and
ImageNet-Subset. For convenience, we used the nomencla-
ture [DATASET]- C/T to denote a task sequence with C
classes evenly divided into Ttasks in the CIL scenario; for
example, the suffix indicates that a model needs to recognize
C/T new classes in each task.
Compared methods . We compared our approach with
both classic and the latest CL baselines, which covered GEM
[10], LOGD [86], IL2M [36], FS-DGPM [87], ARI [88] from
replay-based approaches, EWC [16], SI [17], MAS [12], OL-
EWC [89], BiC [90], OWM [62] from regularization-based
approaches, and RPS-Net [32], EFT [20], PCL [23] from
dynamic architecture-based approaches. Additionally, we
compared it with a representative NN-RW [50], a naive
approach of simply fine-tuning each new task ( None ; can be
seen as lower bound), and a network that is always trained
using the data of all tasks so far ( Joint ; can be seen as upper
bound).
Architectures. In our experiments, all methods used
similar-sized neural network architectures. For MNIST and
FashionMNIST, a fully connected network with three hid-
den layers was used. No pre-trained CNN is used for
toy tasks, as a simple model already generated very
good results. For Stanford Dogs, CUB-200, CIFAR-100,
and ImageNet-Subset, a standard ResNet (e.g., ResNet-18,
ResNet-56) was employed to provide well-extracted fea-
tures, similar to [23], [44], [62], [70], [71] where a feature
extractor was pre-trained by the selected partial categories.
Then, the remaining classes for which the feature extractor
was not trained are sequentially fed into a simple model to
learn different mappings. For example, it took 100 classes
from CUB-200 to pre-train with ResNet-56 and then start
consecutively learning another 100 classes that the pre-
trained model has not encountered. This is in line with the
characteristics of human learning that prepares sufficiently
for challenging tasks, instead of having no any prior knowl-
edge. A pre-trained CNN is equally used in our method and
all the baselines.
Hyper-parameter. For all baselines, we employed the
open-source code released by their authors or a popular
third-party code. Particularly, we used the SGD optimizer
with an initial learning rate ( η) of 0.01 and a batch size of 64
in our experiments. For the replay-based methods, we kept
a random exemplar set of 4.4k for MNIST, FashionMNIST,
CUB-200, and ImageNet-Subset, and restricted the exem-
plar memory budget to 2k samples for Stanford Dogs and
CIFAR-100 by following the setting in [10], [86]. Dynamic
structure-based methods will have the same magnitude as
a base network after learning all tasks or with no limits on
their expansion size. For regularization-based methods, the
trade-off is from set {100, 1000, 10000, 100000 }. Note that
the other hyperparameters are with reference to the original
settings by default. In our method, the hyperparameter
settings included λ= 0.01andγ= 0.4in (12), µ= 2−30in
(13), and α= 0.1in (20).

--- PAGE 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 11
TABLE 1
Performance comparisons on MNIST -10/5 and FashionMNIST -10/5 both measured by three evaluation metrics. All the results are (re) produced
under 5 runs, with the mean and standard deviation reported. We also mark the best results in Bold and second-best results in Italic .
MethodMNIST-10/5 FashionMNIST-10/5
Metric Metric
ACC BWT FWT ACC BWT FWT
None (lower bound) ∼19.91 - - ∼19.81 - -
Joint (upper bound) ∼98.56 - - ∼96.61 - -
NN-RW [50] 19.92 ±0.23 −0.9926±0.0029 0.0002±0.0004 19.88±0.26 −0.9921±0.0028 0.0003±0.0004
EWC [16] 38.68 ±3.44 −0.6958±0.0272 −0.0442±0.0141 37.97 ±7.58 −0.6864±0.0247 −0.0547±0.0731
MAS [12] 44.61 ±6.62 −0.0589±0.0674 −0.5873±0.0651 34.91 ±5.47 −0.6095±0.0859 −0.1950±0.0298
OL-EWC [89] 57.38 ±4.04 −0.3773±0.0584 −0.1128±0.0451 54.09 ±4.03 −0.4014±0.0760 −0.1376±0.0499
SI [17] 69.44 ±4.37 −0.0436±0.0900 −0.2943±0.0935 52.11 ±2.22 −0.4871±0.0609 −0.0824±0.0420
EFT [20] 82.53 ±1.15 −0.0856±0.0685 −0.0953±0.0428 74.79 ±1.23 −0.1328±0.0537 −0.0875±0.0227
OWM [62] 87.16 ±0.65 −0.0561±0.0138 −0.0901±0.0171 80.32 ±0.95 −0.1434±0.0268 −0.0953±0.0206
GEM [10] 94.10 ±0.37 −0.0323±0.0054 −0.0154±0.0024 81.95±1.86 −0.0881±0.0258 −0.0811±0.0252
FS-DGPM [87] 89.12 ±1.14 −0.0841±0.0123 −0.0403±0.0070 80.89 ±0.74 −0.1177±0.0182 −0.1073±0.0154
BiC [90] 93.93 ±0.58 −0.0390±0.0069 −0.0285±0.0044 82.36 ±0.72 −0.1084±0.0309 −0.0847±0.0243
ARI [88] 93.60 ±0.57 −0.0418±0.0013 −0.0283±0.0024 82.89 ±0.83 −0.0966±0.0104 −0.1056±0.0091
PCL [23] 94.14 ±0.67 −0.0314±0.0256 −0.0289±0.0109 83.27 ±0.81 −0.1238±0.0135 −0.0257±0.0108
RPS-Net [32] 94.53 ±1.92 −0.0237±0.0102 −0.0302±0.0087 84.18 ±1.60 −0.0285±0.0124 −0.0417±0.0085
LOGD [86] 94.87 ±0.59 −0.0393±0.0068 −0.0718±0.0233 84.39±0.47 −0.0918±0.0315 −0.0615±0.0067
IL2M [36] 95.51±0.42 −0.0418±0.0046 −0.0314±0.0323 82.38 ±2.04 −0.1483±0.03452 −0.0451±0.0192
IF2Net 96.16±0.54 −0.0044±0.0008 −0.0231±0.0059 95.09±0.47 −0.0189±0.0074 −0.0267±0.0112
IF2Net-EWC 96.21 ±0.71 −0.0103±0.0074 −0.0272±0.0041 95.01 ±0.61 −0.0125±0.0061 −0.0329±0.0146
5.2 Experimental Results
5.2.1 Experiments on toy examples
MNIST-10/5 and FashionMNIST-10/5. Table 1 lists the com-
parative results of MNIST-10/5 and FashionMNIST-10/5.
Both the None and NN-RW, made available for single-
task learning, only remember the most recently learned
task, and the previously trained ones have been thoroughly
forgotten. In contrast, the proposed method achieved the
highest ACC (96.16%, 95.09%), approaching Joint , and BWT
(-0.0044, -0.0189), approaching zero, proving that IF2Net is a
forgetting-free method. The complementary FWT values of
the GEM and PCL were better than those of IF2Net. How-
ever, their ACC values are lower than ours. We observed
that the performance of EWC, MAS, OL-EWC, and SI is
significantly inferior to the others, as the CIL scenario is
particularly difficult for regularization-based methods. Most
replay-based methods achieved marginally worse ACC than
IF2Net, especially for IL2M and LOGD on the MNIST-10/5
task sequence. Additionally, using randomly shuffled task
ordering is more desirable in an open-ended environment,
which can play a role in model fairness. In this way, the
standard deviations reported that our method also has low-
order sensitivity, with similar accuracies for each task se-
quence regardless of the random task orderings. Finally, the
results of IF2Net-EWC showed a marginal boost in learning
performance, indicating its generality and compatibility.5.2.2 Experiments on challenging tasks
CUB-{100/5, 100/10, 100/20 }.Three more challenging task
sequences split by the CUB were used, and the comparative
results were plotted in a stacked bar chart (see Fig. 5), where
a model must incrementally recognize fine-grained visual
categorization tasks. Our method outperformed the selected
state-of-the-art methods on ACC while being equipped with
a competitive standard deviation. Particularly, the proposed
IF2Net improved ACC with the second-best method IL2M
by an absolute margin of 5.48%, and the standard deviations
of GEM, OWM, and BiC were slightly superior to ours,
but their ACC was much inferior to IF2Net. Hence, the
results on the CUB- {100/5, 100/10, 100/20 }task sequences
highlighted our method as a promising tool for alleviating
forgetting. The standard deviation of PCL was high because
it employed a multi-head setup where it branched into an
exclusive output layer for the classes learned so far. Con-
sequently, it is demanding that PCL correctly matched the
corresponding heads for decision-making, given a sequence
of fine-grained visual tasks.
Stanford Dogs- {60/5, 60/10 }.Similarly, the results of the
task sequences split by Stanford Dogs were consistent with
those of the CUB, as depicted in Fig. 6. Note that the lower
part of each sub-figure used the left vertical coordinate
axis to report ACC, whereas the upper part used the right
vertical coordinate axis to report the BWT and FWT results
of the corresponding methods. Take Fig. 6a as an example,

--- PAGE 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 12
Method CUB-100/5 std CUB-100/10 std CUB-100/20 std summation
OWM 34.96 0.3734.48 1.2533.15 0.71 102.59 2.33
RPS-Net 35.80 0.4135.09 0.4535.67 0.56 106.56 1.42
FS-DGPM 36.34 0.6136.47 0.8236.64 1.03 109.45 2.46
GEM 37.62 0.3337.59 0.5636.96 0.97 112.17 1.86
ARI 37.86 0.4537.55 0.37 37.13 0.67 112.54 1.49
PCL 38.26 1.2537.82 2.0637.84 1.78 113.92 5.09
BiC 38.24 0.5538.18 0.5637.98 0.49 114.4 1.6
LOGD 38.25 0.3438.44 0.7539.20 1.41 115.89 2.5
IL2M 39.01 0.6738.31 1.1338.80 0.61 116.12 2.41
Ours 40.07 0.641.76 0.9439.77 0.34 121.6 1.88
34.96 35.80 36.34 37.62 37.86 38.26 38.24 38.25 39.01 40.07 
34.48 35.09 36.47 37.59 37.55 37.82 38.18 38.44 38.31 41.76 
33.15 35.67 36.64 36.96 37.13 37.84 37.98 39.20 38.80 39.77 
OWMRPS-NetFS-DGPMGEMARIPCLBiCLOGDIL2MOurs
CUB -200CUB -100/5 CUB -100/10 CUB -100/20
Fig.X   不同算法在 CUB200上依照不同任务划分进行的综合评估
图示中不同颜色的分段对应 split5，split10，split20的不同任务划分得到的 ACC，叠加后得到的总值对应算法在本数据集上的评估
我们使用了更加具有挑战性的数据集进行实验， CUB200是细粒度分类数据集，类别之间更难以区分。对于
其中的 100类，在 ResNet -56上运用 Adam优化器训练 100个epoch，得到预训练模型后，提取其余
任务序列作用在我们的方法和所有的基线中。进一步地，我们将 100类特征划分为 5，10，
估，并将结果堆叠得到图 x，需要注意的是，由于 ACC本身就是百分制的结果，不需要对各个结果额外进行归一化，直接叠加的
结果就是可比且直观的。结果表明， IF2NET在细粒度分类的 CIL场景也有着一定的优势，较之次好的方式，整体上也存在
评估分数领先，并且有着相对而言较为稳定的实验结果。我们同时注意到，分别考察不同任务划分下的
Fig. 5. Test accuracy (%) on CUB- {100/5, 100/10, 100/20 }task se-
quences. All the results are (re) produced under 5 runs, with the sep-
arate mean and accumulative standard deviation reported.
-1.00-0.90-0.80-0.70-0.60-0.50-0.40-0.30-0.20-0.100.00
0.000.050.100.150.200.250.300.350.400.450.500.550.600.650.70Stanford Dogs -60/5 ACC BWT FWT
(a)
-1.00-0.90-0.80-0.70-0.60-0.50-0.40-0.30-0.20-0.100.00
0.000.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.80Stanford Dogs -60/10
ACC2 BWT2 FWT2
(b)
Fig. 6. Performance comparisons on Stanford Dogs measured by three
evaluation metrics. All the results are (re) produced under 5 runs. (a)
Performance after sequentially learning five 12-class tasks. (b) Perfor-
mance after sequentially learning ten 6-class tasks.
and the proposed method exceeded the other baselines on
ACC, among which IL2M is still the strongest baseline with
2.29% lower than ours. Meanwhile, IF2Net achieved the best
BWT value and second-best FWT value, which implies its
strong ability to retain and transfer knowledge across fine-
grained visual categorization tasks. This is also reflected in
the comparison in Fig. 6b.
CIFAR- {60/10, 60/20 }.The performance of different CL
methods on task sequences split by CIFAR-100 is reported
in Table 2, which focused on the changing trend of ACC
with an increasing number of tasks. The results indicated
the superior performance of the proposed method in the
CIL scenario. Although it started with a minor difference inTABLE 2
Test accuracy (%) on CIFAR-60/10 task sequence. We mark the best
results in Bold and the second-best in Italic . Note that the reported
tasktaccuracy is an average of all 1,2, . . . , t tasks seen so far.
MethodTask Number
1 2 3 4 5 6 7 8 9 10
OWM 85.36 66.23 55.50 49.39 48.16 45.28 42.34 39.76 38.10 38.19
ARI 83.53 67.92 57.56 54.25 51.83 48.42 46.35 41.06 40.94 38.27
FS-DGPM 83.45 74.08 64.44 59.66 53.13 48.80 47.28 45.06 41.83 39.42
RPS-Net 83.83 66.91 65.05 64.25 58.16 52.83 49.04 47.47 45.31 43.01
LOGD 85.03 72.41 66.55 61.87 59.20 54.11 55.76 54.12 51.53 49.00
GEM 84.96 74.53 65.22 60.96 60.07 59.14 56.31 53.35 52.96 49.83
IL2M 85.11 76.44 69.84 65.42 63.19 58.63 54.35 54.10 50.87 51.78
BiC 84.60 72.13 67.64 64.80 60.12 58.42 56.10 54.00 53.77 52.03
Ours 84.71 74.58 68.11 65.45 63.70 59.94 59.38 59.63 57.22 56.98
30.0035.0040.0045.0050.0055.0060.0065.0070.0075.0080.0085.0090.0095.00100.00
1234567891011121314151617181920CIFAR -60/20
ARI
OWM
FS-DGPM
RPS-Net
IL2M
LOGD
GEM
BiC
Ours
Fig. 7. Test accuracy (%) on CIFAR-60/20 task sequence, where each
method needs to incrementally learn 3 new classes per task.
the first task, our method showed a relatively gentle ACC
degradation during sequential training. Furthermore, com-
pared with the second-best ACC from IL2M, our method
achieved a relative gain of 4.95% after the end of learning.
In addition to Table 2, Fig. 7 vividly depicts the consistent
results by incrementally learning three new classes per task.
ImageNet- {100/10, 100/25, 100/50 }.Table 3 compares
different methods on task sequences split by ImageNet-
Subset, where the catastrophic forgetting problem becomes
more challenging in either a tough individual task or a
rather long task sequence. The results show that IF2Net
surpasses the selected state-of-the-art methods with margins
of 2.86%, 2.08%, and 1.53% for ImageNet- {100/10, 100/25,
100/50 }, respectively. Overall, extensive experiments vali-
dated the competitive performance of IF2Net against state-
of-the-art methods, which are inherently immune to catas-
trophic forgetting by functionally maintaining the weights
relative to each seen task untouched all the time.

--- PAGE 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 13
TABLE 3
Large-scale experiments on ImageNet in the CIL scenario that are
measured by average test accuracy (%). The methods are run under 5
random task orderings, with the mean and standard deviation reported.
MethodTask Sequence
ImageNet-100/10 ImageNet-100/25 ImageNet-100/50
FS-DGPM 35.08 ±2.04 34.96 ±1.96 31.42 ±2.33
ARI 36.99 ±1.17 34.12 ±1.55 30.68 ±2.42
RPS-Net 40.25 ±2.65 36.62 ±2.89 29.33 ±3.45
GEM 46.23 ±1.45 44.62 ±1.13 42.78 ±1.12
IL2M 49.86 ±1.57 48.05 ±1.25 47.52 ±1.04
Ours 52.72 ±1.65 50.13 ±1.57 49.05 ±2.75
5.3 Ablation Study
Before concluding our work, we conducted ablation studies
to provide a deeper understanding of our method. To this
end, the empirical investigation of the effectiveness of core
designs in IF2Net is detailed as follows.
5.3.1 Advantage of output weight initialization using a
closed-form solution
As stated earlier, we used analytic initialization instead of
random initialization for task 1; that is, we started with a
selected mini-batch for computing the closed-form solution
β∗
1, followed in the same way as the subsequent updates
of the output weight βt(t≥2). To investigate the benefits
of this strategy, we compared two types of output weight
initializations. Specifically, different ηand epochs were
considered in the ablation study, including (i) randomly
initialized output weights and (ii) starting with a selected
mini-batch for analytically computing the output weights.
Table 4 presents comparative results based on the Fashion-
MNIST task sequence. We observed that with a proper η,
analytic initialization can be converged more quickly than
its random counterpart, as measured by the loss value, test
accuracy for task 1, and ACC value for all. This means that
it can effectively alleviate the restriction of the orthogonal
gradient descent in the direction of the parameter update.
Meanwhile, analytic initialization leveraging 2.5k (approx-
imately 20% of all available) samples from Task 1 can
yield competitive results. By contrast, IF2Net with random
initialization makes it difficult to find the optima even after
ten epochs.
5.3.2 Efficiency of the node blocks in hidden layers
We proposed randomization-based representation-level
learning, in which several node blocks were randomly allo-
cated to better exploit hidden information among the train-
ing samples. Taking FC1 as an example, Table 5 illustrates
the extent to which different combinations contributed to
the learning performance. We observed that with the fixed
100 hidden nodes in FC1, IF2Net equipped with too small
or too large node blocks would significantly degrade the
values of ACC, BWT, and FWT. By contrast, some pairs (10,
10) and (25, 4) used in our experiments performed well,
implying that appropriate node blocks are indispensablefor the proposed representation-level learning process. Ef-
ficiency analysis provided some prior knowledge of node
block selection, and it does not mean that the settings listed
were the optimal choice. In other words, user-specified fine-
tuning based on the recommended examples might yield
better single and overall performance results.
5.3.3 Impact of increasing orthogonalized layers
One of the most apparent differences between IF2Net and
OWM is that the latter only induces orthogonalization to
the final output layer instead of layer-wise operations. To
explore the effect of gradually adding orthogonalized layers,
we denote the corresponding cases: (a) Four Layers/OWM;
(b) FC1-Three Layers; (c) FC1-FC2-Two layers; (d) FC1-FC2-
FC3-One Layer/IF2Net with random initialization; (e) FC1-
FC2-FC3-One Layer/IF2Net with analytic initialization. Ta-
ble 6 compares different cases on FashionMNIST in which
GPU memory usage is proportional to the number of or-
thogonalized layers. Furthermore, with only one or two
orthogonalized layer(s), IF2Net performed well on the four
metrics. Interestingly, the ACC produced from Case (d) is
lower than that of Case (c) and Case (e), and replacing ran-
dom initialization with its analytic implementation achieves
counterattack results. Hence, we applied output-layer or-
thogonalization to the final decision-making process.
5.3.4 Superiority in generalization bound error
We followed the same setting in the previous section such
that one can correspond to the Rademacher complexity for
different cases. Specifically, we randomly took the value in
{-1,+1}with equal probability as the training labels of each
task and computed the Rademacher complexity based on
(25). Then, we accumulated the results of all previous tasks
during each training session, measuring how the function
class (current model) incrementally fit the randomly sam-
pled labels. Fig. 8 plotted the value of Rademacher complex-
ity term for our method and OWM over the whole train-
ing sessions. We observed that the generalization bound
error increases with the number of new tasks, as stated in
Lemma 2 . Especially, the IF2Net with analytic initialization
corresponding to Case (e) used one orthogonalized layer in
the final output layer and obtained the best generalization
bound error. Contrastingly, for example, the more orthogo-
nalized the layers, the faster the error increases, as stated in
Lemma 3 . Hence, this ablation study distinguished why our
method is better than OWM and proved its superiority in
generalization bound error.
6 C ONCLUSIONS
Most continual learning approaches emphasize employ-
ing external (not inherent) conditions, such as exemplar
buffers/data generators, additional objectives, and exclu-
sive branches to alleviate forgetting, rather than focusing
on an inherent implementation of avoiding overwritten
weights within connectionist models. To this end, this study
introduced two simple projections, randomization-based
representation-level learning and orthogonalization-based
decision-making, to innately overcome forgetting in a back-
bone network. The theoretical verification for convergence
analysis of representation approximation and proof of the

--- PAGE 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 14
TABLE 4
Empirical analysis of the output weight initialization using a closed-form solution on FashionMNIST -10/5. Under different learning rates ( η) and
epochs, the results were measured by the loss of learning task 1, the accuracy of learning task 1, and all. We marked the best results in bold .
Note that the number 1k, 2.5k, and 5k were the selected mini-batch for analytic initialization.
Output Weights ηEpoch 1 Epoch 5 Epoch 10
Loss-task1 ACC-task1 ACC-All Loss-task1 ACC-task1 ACC-All Loss-task1 ACC-task1 ACC-All
Random Initialization0.02 7.0902 0.9651 0.9022 2.5381 0.9750 0.9056 3.0367 0.9696 0.8689
0.002 6.6710 0.7933 0.6367 4.4575 0.7843 0.6553 2.0224 0.8975 0.6173
Analytic Initialization (1k)0.002 0.5013 0.9698 0.9281 0.2815 0.9713 0.9183 0.1369 0.9833 0.9189
0.0002 0.1444 0.9615 0.9237 0.0929 0.9717 0.9293 0.0634 0.9833 0.9200
Analytic Initialization (2.5k) 0.0002 0.0923 0.9730 0.9465 0.0548 0.9848 0.9502 0.0489 0.9900 0.9411
Analytic Initialization (5k) 0.0002 0.0910 0.9735 0.9505 0.0553 0.9810 0.9580 0.0550 0.9830 0.9482
TABLE 5
Empirical analysis of the node blocks on FashionMNIST -10/5. With
different combinations, the results were measured by three evaluation
metrics. The randomly allocated nodes in FC1 were composed of n1
groups of node blocks, with each group s1nodes.
Node Block Metric
n1 s1 ACC BWT FWT
100 1 11.18 ±7.66 0.0002 ±0.0012 -0.4751 ±0.0821
50 2 62.18 ±22.95 -0.1115 ±0.0075 -0.3191 ±0.1946
25 4 95.41 ±1.35 -0.0072 ±0.0018 -0.0316 ±0.0123
10 10 94.76 ±0.27 -0.0140 ±0.0083 -0.0357 ±0.0115
5 20 91.68 ±1.74 -0.0342 ±0.0084 -0.0554 ±0.0138
1 100 75.76 ±2.92 -0.0845 ±0.0104 -0.2022 ±0.0322
TABLE 6
Empirical analysis of increasing several orthogonalized layers on
FashionMNIST -10/5. In different cases, the results are measured by
four evaluation metrics. Note that the metric GPU (MB) refers to
memory usage during learning the last task.
CaseMetric
ACC BWT FWT GPU
(a) 0.8058 ±0.0101 -0.2216 ±0.0140 -0.0197 ±0.0055 11635
(b) 0.9214 ±0.0052 -0.0212 ±0.0067 -0.0125 ±0.0042 5281
(c) 0.9428 ±0.0045 -0.0158 ±0.0112 -0.0165 ±0.0043 3435
(d) 0.9090 ±0.0201 -0.0292 ±0.0217 -0.0758 ±0.0528 829
(e) 0.9526 ±0.0088 -0.0071 ±0.0060 -0.0289 ±0.0173 829
learning-without-forgetting decision were provided with
rigorous mathematical deductions. The resulting method
avoided forward and backward passes of backpropagation,
and one could retain weights of the hidden layers invariable
and exclusively adapt to that of the output layer during
sequential training. Thus, the proposed method perfectly
reproduced what was previously learned and relaxed the
computational and memory requirements.
Recent studies have highlighted the potential impor-
tance of leveraging a pre-trained model in the continual
12 45 3
Training Sessions00.250.500.751.001.251.501.752.00Generalization Bound Error Rademacher Complexity
(a)
(b)
(c)
(d)
(e)Fig. 8. Empirical analysis of generalization bound error based on
Rademacher complexity. Note that the legend used here is the same
as that of the previous setting.
learning context. Based on this, we used a simple archi-
tecture to better analyze a network’s behavior throughout
this study, as challenging class-incremental learning can be
performed with small adaptations. It is natural to consider
whether IF2Net can be scaled to end-to-end implementation
for challenging tasks. The purpose of this study was to
provide a systematic solution with dual projections (ran-
domization and orthogonalization) for continual learning
challenges, and we leave it for future research.
ACKNOWLEDGMENTS
This work was supported by the National Key R&D Pro-
gram of China under Grant 2021ZD0201300, the Funda-
mental Research Funds for the Central Universities under
Grant YCJJ202203012, the State Scholarship Fund of China
Scholarship Council under Grant 202206160045, and the
National Natural Science Foundation of China under Grants
U1913602, 61936004, and 62206204.
REFERENCES
[1] S. Thrun and T. M. Mitchell, “Lifelong robot learning,” Robotics
and Autonomous Systems , vol. 15, no. 1-2, pp. 25–46, 1995.

--- PAGE 15 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 15
[2] M. McCloskey and N. J. Cohen, “Catastrophic interference in
connectionist networks: The sequential learning problem,” in Psy-
chology of Learning and Motivation . Elsevier, 1989, vol. 24, pp.
109–165.
[3] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio,
“An empirical investigation of catastrophic forgetting in gradient-
based neural networks,” arXiv preprint arXiv:1312.6211 , 2013.
[4] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter,
“Continual lifelong learning with neural networks: A review,”
Neural Networks , vol. 113, pp. 54–71, 2019.
[5] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia,
A. Leonardis, G. Slabaugh, and T. Tuytelaars, “A continual learn-
ing survey: Defying forgetting in classification tasks,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , vol. 44, no. 7,
pp. 3366–3385, 2021.
[6] Z. Wang, C. Chen, and D. Dong, “A dirichlet process mix-
ture of robust task models for scalable lifelong reinforce-
ment learning,” IEEE Transactions on Cybernetics , 2022, doi:
10.1109/TCYB.2022.3170485.
[7] B. Yang, M. Lin, Y. Zhang, B. Liu, X. Liang, R. Ji, and Q. Ye, “Dy-
namic support network for few-shot class incremental learning,”
IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022,
doi: 10.1109/TPAMI.2022.3175849.
[8] X. Zhang, T. Zhao, J. Chen, Y. Shen, and X. Li, “Epicker is
an exemplar-based continual learning approach for knowledge
accumulation in cryoem particle picking,” Nature Communications ,
vol. 13, no. 1, pp. 1–10, 2022.
[9] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “iCaRL:
Incremental classifier and representation learning,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2017, pp. 2001–2010.
[10] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for
continual learning,” in Advances in Neural Information Processing
Systems , vol. 30, 2017, pp. 6470–6479.
[11] R. Shokri and V . Shmatikov, “Privacy-preserving deep learning,”
inProceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security , 2015, pp. 1310–1321.
[12] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuyte-
laars, “Memory aware synapses: Learning what (not) to forget,” in
Proceedings of the European Conference on Computer Vision , 2018, pp.
139–154.
[13] G. M. Van de Ven and A. S. Tolias, “Generative replay with
feedback connections as a general strategy for continual learning,”
arXiv preprint arXiv:1809.10635 , 2018.
[14] M. Zhai, L. Chen, F. Tung, J. He, M. Nawhal, and G. Mori, “Life-
long GAN: Continual learning for conditional image generation,”
inProceedings of the IEEE/CVF International Conference on Computer
Vision , 2019, pp. 2759–2768.
[15] L. Wang, B. Lei, Q. Li, H. Su, J. Zhu, and Y. Zhong, “Triple-memory
networks: A brain-inspired method for continual learning,” IEEE
Transactions on Neural Networks and Learning Systems , vol. 33, no. 5,
pp. 1925–1934, 2021.
[16] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al. , “Overcoming catastrophic forgetting in neural networks,”
Proceedings of the National Academy of Sciences , vol. 114, no. 13, pp.
3521–3526, 2017.
[17] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through
synaptic intelligence,” in International Conference on Machine Learn-
ing, 2017, pp. 3987–3995.
[18] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang,
and C.-C. J. Kuo, “Class-incremental learning via deep model
consolidation,” in Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision , 2020, pp. 1131–1140.
[19] A. Chaudhry, A. Gordo, P . Dokania, P . Torr, and D. Lopez-Paz,
“Using hindsight to anchor past knowledge in continual learning,”
inProceedings of the AAAI Conference on Artificial Intelligence , 2021,
pp. 6993–7001.
[20] V . K. Verma, K. J. Liang, N. Mehta, P . Rai, and L. Carin, “Ef-
ficient feature transformations for discriminative and generative
continual learning,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 13 865–13 875.
[21] J. Xu, J. Ma, X. Gao, and Z. Zhu, “Adaptive progressive contin-
ual learning,” IEEE Transactions on Pattern Analysis and Machine
Intelligence , vol. 44, no. 10, pp. 6715–6728, 2022.
[22] M. Perkonigg, J. Hofmanninger, C. J. Herold, J. A. Brink, O. Pi-
anykh, H. Prosch, and G. Langs, “Dynamic memory to alleviatecatastrophic forgetting in continual learning with medical imag-
ing,” Nature Communications , vol. 12, no. 1, pp. 1–12, 2021.
[23] W. Hu, Q. Qin, M. Wang, J. Ma, and B. Liu, “Continual learning
by using information of each class holistically,” in Proceedings of
the AAAI Conference on Artificial Intelligence , 2021, pp. 7797–7805.
[24] J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly, “Why
there are complementary learning systems in the hippocampus
and neocortex: insights from the successes and failures of connec-
tionist models of learning and memory,” Psychological Review , vol.
102, no. 3, pp. 419–457, 1995.
[25] Z. Li and D. Hoiem, “Learning without forgetting,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , vol. 40, no. 12, pp.
2935–2947, 2017.
[26] Y.-M. Tang, Y.-X. Peng, and W.-S. Zheng, “Learning to imagine:
Diversify memory for incremental learning using unlabeled data,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 9549–9558.
[27] B. Zhang, Y. Guo, Y. Li, Y. He, H. Wang, and Q. Dai, “Mem-
ory recall: A simple neural network training framework against
catastrophic forgetting,” IEEE Transactions on Neural Networks and
Learning Systems , vol. 33, no. 5, pp. 2010–2022, 2022.
[28] X. He and H. Jaeger, “Overcoming catastrophic interference using
conceptor-aided backpropagation,” in International Conference
on Learning Representations , 2018. [Online]. Available: https:
//openreview.net/forum?id=B1al7jg0b
[29] I. Paik, S. Oh, T. Kwak, and I. Kim, “Overcoming catastrophic
forgetting by neuron-level plasticity control,” in Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 34, no. 04, 2020, pp.
5339–5346.
[30] Q. Gao, Z. Luo, D. Klabjan, and F. Zhang, “Efficient
architecture search for continual learning,” IEEE Transac-
tions on Neural Networks and Learning Systems , 2022, doi:
10.1109/TNNLS.2022.3151511.
[31] Z. Zhang, Y. Chen, and C. Zhou, “Self-growing binary activation
network: A novel deep learning model with dynamic architec-
ture,” IEEE Transactions on Neural Networks and Learning Systems ,
2022, doi: 10.1109/TNNLS.2022.3176027.
[32] J. Rajasegaran, M. Hayat, S. Khan, F. S. Khan, and L. Shao,
“Random path selection for incremental learning,” in Advances in
Neural Information Processing Systems , 2019, pp. 12 669–12 679.
[33] S. Wen, A. Rios, Y. Ge, and L. Itti, “Beneficial perturbation network
for designing general adaptive artificial intelligence systems,”
IEEE Transactions on Neural Networks and Learning Systems , vol. 33,
no. 8, pp. 3778–3791, 2008.
[34] H. Jung, J. Ju, M. Jung, and J. Kim, “Less-forgetting learning in
deep neural networks,” arXiv preprint arXiv:1607.00122 , 2016.
[35] T. Lesort, V . Lomonaco, A. Stoian, D. Maltoni, D. Filliat, and
N. Dıaz-Rodrıguez, “Continual learning for robotics,” arXiv
preprint arXiv:1907.00182 , pp. 1–34, 2019.
[36] E. Belouadah and A. Popescu, “IL2M: Class incremental learning
with dual memory,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2019, pp. 583–592.
[37] J. Bang, H. Kim, Y. Yoo, J.-W. Ha, and J. Choi, “Rainbow memory:
Continual learning with a memory of diverse samples,” in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2021, pp. 8218–8227.
[38] C. V . Nguyen, Y. Li, T. D. Bui, and R. E. Turner,
“Variational continual learning,” in International Conference
on Learning Representations , 2018. [Online]. Available: https:
//openreview.net/forum?id=BkQqq0gRb
[39] S. Swaroop, C. V . Nguyen, T. D. Bui, and R. E. Turner, “Improving
and understanding variational continual learning,” arXiv preprint
arXiv:1905.02099 , 2019.
[40] N. Loo, S. Swaroop, and R. E. Turner, “Generalized variational
continual learning,” arXiv preprint arXiv:2011.12328 , 2020.
[41] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirk-
patrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive
neural networks,” arXiv preprint arXiv:1606.04671 , 2016.
[42] J. Yoon, S. Kim, E. Yang, and S. J. Hwang,
“Scalable and order-robust continual learning with addi-
tive parameter decomposition,” in International Conference
on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=r1gdj2EKPB
[43] W. Hu, M. Wang, Q. Qin, J. Ma, and B. Liu, “Hrn: A holistic
approach to one class learning,” Advances in Neural Information
Processing Systems , vol. 33, pp. 19 111–19 124, 2020.

--- PAGE 16 ---
JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, DECEMBER 2022 16
[44] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su,
V . Perot, J. Dy, and T. Pfister, “Learning to prompt for continual
learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 139–149.
[45] Y.-H. Pao, G.-H. Park, and D. J. Sobajic, “Learning and general-
ization characteristics of the random vector functional-link net,”
Neurocomputing , vol. 6, no. 2, pp. 163–180, 1994.
[46] Y.-H. Pao and Y. Takefuji, “Functional-link net computing: theory,
system architecture, and functionalities,” Computer , vol. 25, no. 5,
pp. 76–79, 1992.
[47] P . N. Suganthan and R. Katuwal, “On the origins of
randomization-based feedforward neural networks,” Applied Soft
Computing , vol. 105, p. 107239, 2021.
[48] L. Zhang and P . N. Suganthan, “Visual tracking with convolutional
random vector functional link network,” IEEE Transactions on
Cybernetics , vol. 47, no. 10, pp. 3243–3253, 2016.
[49] D. Wang and M. Li, “Stochastic configuration networks: Funda-
mentals and algorithms,” IEEE Transactions on Cybernetics , vol. 47,
no. 10, pp. 3466–3479, 2017.
[50] C. L. P . Chen and Z. Liu, “Broad learning system: An effective and
efficient incremental learning system without the need for deep
architecture,” IEEE Transactions on Neural Networks and Learning
Systems , vol. 29, no. 1, pp. 10–24, 2017.
[51] Q. Wang, W. Dai, P . Lin, and P . Zhou, “Compact incremental
random weight network for estimating the underground airflow
quantity,” IEEE Transactions on Industrial Informatics , vol. 18, no. 1,
pp. 426–436, 2021.
[52] C. Huang, M. Li, F. Cao, H. Fujita, Z. Li, and X. Wu, “Are graph
convolutional networks with random weights feasible?” IEEE
Transactions on Pattern Analysis and Machine Intelligence , 2022, doi:
10.1109/TPAMI.2022.3183143.
[53] L. Zhang and P . N. Suganthan, “A comprehensive evaluation of
random vector functional link networks,” Information Sciences , vol.
367, pp. 1094–1105, 2016.
[54] S. Scardapane and D. Wang, “Randomness in neural networks:
an overview,” Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery , vol. 7, no. 2, p. e1200, 2017.
[55] W. Cao, X. Wang, Z. Ming, and J. Gao, “A review on neural
networks with random weights,” Neurocomputing , vol. 275, pp.
278–287, 2018.
[56] X. Gong, T. Zhang, C. L. P . Chen, and Z. Liu, “Research review
for broad learning system: algorithms, theory, and applications,”
IEEE Transactions on Cybernetics , vol. 52, no. 9, pp. 8922–8950, 2022.
[57] G.-B. Huang, Q. Zhu, and C. K. Siew, “Extreme learning machine:
theory and applications,” Neurocomputing , vol. 70, no. 1-3, pp. 489–
501, 2006.
[58] M. Li and D. Wang, “Insights into randomized algorithms for neu-
ral networks: Practical issues and common pitfalls,” Information
Sciences , vol. 382, pp. 170–178, 2017.
[59] W. Dai, D. Li, P . Zhou, and T. Chai, “Stochastic configuration
networks with block increments for data modeling in process
industries,” Information Sciences , vol. 484, pp. 367–386, 2019.
[60] W. Dai, X. Zhou, D. Li, S. Zhu, and X. Wang, “Hybrid parallel
stochastic configuration networks for industrial data analytics,”
IEEE Transactions on Industrial Informatics , vol. 18, no. 4, pp. 2331–
2341, 2021.
[61] S. Singhal and L. Wu, “Training feed-forward networks with
the extended kalman algorithm,” in International Conference on
Acoustics, Speech, and Signal Processing . IEEE, 1989, pp. 1187–1190.
[62] G. Zeng, Y. Chen, B. Cui, and S. Yu, “Continual learning of context-
dependent processing in neural networks,” Nature Machine Intelli-
gence , vol. 1, no. 8, pp. 364–372, 2019.
[63] X. Li and W. Wang, “GopGAN: Gradients orthogonal projection
generative adversarial network with continual learning,” IEEE
Transactions on Neural Networks and Learning Systems , 2021, doi:
10.1109/TNNLS.2021.3093319.
[64] Y.-C. Hsu, Y.-C. Liu, A. Ramasamy, and Z. Kira, “Re-evaluating
continual learning scenarios: A categorization and case for strong
baselines,” arXiv preprint arXiv:1810.12488 , 2018.
[65] M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov,
and J. van de Weijer, “Class-incremental learning: survey and
performance evaluation on image classification,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 2022, doi:
10.1109/TPAMI.2022.3213473.
[66] G. M. Van de Ven and A. S. Tolias, “Three scenarios for continual
learning,” arXiv preprint arXiv:1904.07734 , 2019.[67] P . Pan, S. Swaroop, A. Immer, R. Eschenhagen, R. Turner, and
M. E. E. Khan, “Continual deep learning by functional regu-
larisation of memorable past,” in Advances in Neural Information
Processing Systems , vol. 33, 2020, pp. 4453–4464.
[68] D. Needell, A. A. Nelson, R. Saab, and P . Salanevich, “Random
vector functional link networks for function approximation on
manifolds,” arXiv preprint arXiv:2007.15776 , 2020.
[69] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable
are features in deep neural networks?” Advances in Neural Informa-
tion Processing Systems , vol. 27, 2014.
[70] T. L. Hayes, K. Kafle, R. Shrestha, M. Acharya, and C. Kanan,
“Remind your neural network to prevent catastrophic forgetting,”
inEuropean Conference on Computer Vision . Springer, 2020, pp.
466–483.
[71] T.-Y. Wu, G. Swaminathan, Z. Li, A. Ravichandran, N. Vasconcelos,
R. Bhotika, and S. Soatto, “Class-incremental learning with strong
pre-trained models,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 9601–9610.
[72] P . Vincent, H. Larochelle, Y. Bengio, and P .-A. Manzagol, “Extract-
ing and composing robust features with denoising autoencoders,”
inInternational Conference on Machine Learning , 2008, pp. 1096–1103.
[73] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.”
Journal of Machine Learning Research , vol. 9, no. 11, 2008.
[74] B. Xu, Q. Liu, and T. Huang, “A discrete-time projection neural
network for sparse signal reconstruction with application to face
recognition,” IEEE Transactions on Neural Networks and Learning
Systems , vol. 30, no. 1, pp. 151–162, 2018.
[75] Q. Liu and J. Wang, “ L1-minimization algorithms for sparse
signal reconstruction based on a projection neural network,” IEEE
Transactions on Neural Networks and Learning Systems , vol. 27, no. 3,
pp. 698–707, 2015.
[76] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding
algorithm for linear inverse problems,” SIAM Journal on Imaging
Sciences , vol. 2, no. 1, pp. 183–202, 2009.
[77] S. S. Haykin, Adaptive filter theory . Pearson Education India, 2002.
[78] Y. Engel, S. Mannor, and R. Meir, “The kernel recursive least-
squares algorithm,” IEEE Transactions on Signal Processing , vol. 52,
no. 8, pp. 2275–2285, 2004.
[79] G. H. Golub and C. F. Van Loan, Matrix Computations . JHU Press,
2013.
[80] F. Husz ´ar, “On quadratic penalties in elastic weight consolida-
tion,” arXiv preprint arXiv:1712.03847 , 2017.
[81] S. M. Kakade, K. Sridharan, and A. Tewari, “On the complexity
of linear prediction: Risk bounds, margin bounds, and regulariza-
tion,” in Advances in Neural Information Processing Systems , vol. 21,
2008, pp. 793–800.
[82] P . L. Bartlett and S. Mendelson, “Rademacher and gaussian com-
plexities: Risk bounds and structural results,” Journal of Machine
Learning Research , vol. 3, no. Nov, pp. 463–482, 2002.
[83] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learn-
ing: From Theory to Algorithms . Cambridge University Press, 2014.
[84] N. Golowich, A. Rakhlin, and O. Shamir, “Size-independent sam-
ple complexity of neural networks,” in Conference on Learning
Theory . PMLR, 2018, pp. 297–299.
[85] G. M. van de Ven, H. T. Siegelmann, and A. S. Tolias, “Brain-
inspired replay for continual learning with artificial neural net-
works,” Nature Communications , vol. 11, no. 1, pp. 1–14, 2020.
[86] S. Tang, D. Chen, J. Zhu, S. Yu, and W. Ouyang, “Layerwise
optimization by gradient decomposition for continual learning,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2021, pp. 9634–9643.
[87] D. Deng, G. Chen, J. Hao, Q. Wang, and P .-A. Heng, “Flattening
sharpness for dynamic gradient projection memory benefits con-
tinual learning,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 18 710–18 721, 2021.
[88] R. Wang, Y. Bao, B. Zhang, J. Liu, W. Zhu, and G. Guo,
“Anti-retroactive interference for lifelong learning,” arXiv preprint
arXiv:2208.12967 , 2022.
[89] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W.
Teh, R. Pascanu, and R. Hadsell, “Progress & compress: A scalable
framework for continual learning,” in International Conference on
Machine Learning , 2018, pp. 4528–4537.
[90] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu,
“Large scale incremental learning,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2019, pp.
374–382.
