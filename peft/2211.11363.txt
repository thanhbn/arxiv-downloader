# 2211.11363.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2211.11363.pdf
# File size: 774627 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AF Adapter: Continual Pretraining for Building
Chinese Biomedical Language Model
Yongyu Yan
School of Info. Sci. & Engi.
East China University of Sci. & Tech.
Shanghai, China
y30221069@mail.ecust.edu.cnKui Xue
Shanghai AI Lab.
Shanghai, China
xuekui@pjlab.org.cnXiaoming Shi
Shanghai AI Lab.
Shanghai, China
shixiaoming@pjlab.org.cn
Qi Ye
School of Info. Sci. & Engi.
East China University of Sci. & Tech.
Shanghai, China
yeh qi1125@ecust.edu.cnJingping Liu
School of Info. Sci. & Engi.
East China University of Sci. & Tech.
Shanghai, China
jingpingliu@ecust.edu.cnTong Ruan
School of Info. Sci. & Engi.
East China University of Sci. & Tech.
Shanghai, China
ruantong@ecust.edu.cn
Abstract —Continual pretraining is a popular way of building
a domain-specific pretrained language model from a general-
domain language model. In spite of its high efficiency, continual
pretraining suffers from catastrophic forgetting, which may harm
the model’s performance in downstream tasks. To alleviate the
issue, in this paper, we propose a continual pretraining method
for the BERT-based model, named Attention-FFN Adapter.
Its main idea is to introduce a small number of attention
heads and hidden units inside each self-attention layer and
feed-forward network. Furthermore, we train a domain-specific
language model named AF Adapter based RoBERTa for the
Chinese biomedical domain. In experiments, models are applied
to downstream tasks for evaluation. The results demonstrate
that with only about 17% of model parameters trained, AF
Adapter achieves 0.6%, 2% gain in performance on average,
compared to strong baselines. Further experimental results show
that our method alleviates the catastrophic forgetting problem
by 11% compared to the fine-tuning method. Code is available
at https://github.com/yanyongyu/AF-Adapter.
Index Terms —Continual pretraining, Chinese biomedical nat-
ural language processing, Adapter tuning
I. I NTRODUCTION
Currently, a vast volume of Chinese biomedical literature
emerges, including medical diagnosis dialogues on Chinese
medical communities and medical knowledge in Chinese
encyclopedias. As an example, DXY1(a Chinese medical
community) contains millions of medical dialogues between
5.5 million patient users and 2 million doctor users, while
Baidu Encyclopedia2(a Chinese encyclopedia) includes over
30 million articles, covering a wide range of topics related to
health, disease, and medical treatments. As a result, there is a
growing demand for precise Chinese biomedical text mining
tools to effectively extract valuable information from Chinese
medical literature.
Natural language processing (NLP) methods greatly im-
prove the automatic text mining from Chinese literature ac-
1https://portal.dxy.cn/
2https://baike.baidu.com/curately. These methods have undergone several stages, in-
cluding algorithms based on small-scale expert knowledge [1],
shallow machine learning algorithms [2], and deep learning
algorithms [3] [4]. Among these deep learning algorithms,
recently, pretrained language models (PLMs) [5] [6] have
become increasingly popular, as they demonstrate impressive
performance. Specifically, PLMs are trained on a vast amount
of text data to learn better representations of natural language,
providing a strong foundation for downstream NLP tasks.
However, directly applying PLMs on general domains to
biomedical text mining suffers from the word distribution
bias between general and biomedical texts, which harms
performances. To alleviate the issue, PLMs in the biomedical
domain are introduced. Methods of PLMs on specific domains
are divided into two ways, pretraining from scratch [7] and
continual pretraining [8] based on general-domain language
models. Pretraining from scratch means training directly on
a specialized corpus with a specialized vocabulary. Simulta-
neously, continual pretraining first initialize with PLMs on
general-domain and then resume pre-training on a domain-
specialized corpus. Compared with pretraining from scratch,
continual pretraining benefits from higher training efficiency.
In this work, a continual pretraining manner is adopted.
In spite of the high training efficiency, continual pretraining
suffers from catastrophic forgetting [9] [10] [11]. Catastrophic
forgetting means the model trained on new corpora tends to
forget the knowledge of previous data. For example, in Fig. 1,
fine-tuned RoBERTa encounters catastrophic forgetting, mak-
ing mistakes to predict masked words. Catastrophic forgetting
of commonsense knowledge harms performance.
To alleviate the issue, there are three main approaches.
The first one is training-based methods, including discrimina-
tive fine-tuning, slanted triangular learning rates, and gradual
unfreezing [12]. The second one is parameter reserve [13],
which keeps randomly selected pretrained parameters. The
third one is adapter-based tuning methods [14] [15], whicharXiv:2211.11363v2  [cs.CL]  20 Oct 2023

--- PAGE 2 ---
健康[MASK][MASK]学是对健康、疾病，及医疗保健的心理和行为过程的研究。
(Health psychology is the study of psychological and behavioral processes in health, illness, and healthcare.)
世界卫生日旨在引起世界对[MASK][MASK]、健康工作的关注。
(World Health Day aims to draw the world's attention to sanitation and health work.)
Input
健康心理学是对健康、疾病，及医疗保健的心理和行为过程的研究。
世界卫生日旨在引起世界对卫生、健康工作的关注。
健康心义学是对健康、疾病，及医疗保健的心理和行为过程的研究。
世界卫生日旨在引起世界对饮生、健康工作的关注。Output of RoBERTa
Output of fine-tuned RoBERTa
健康心理学是对健康、疾病，及医疗保健的心理和行为过程的研究。
世界卫生日旨在引起世界对卫生、健康工作的关注。
Output of AF Adapter based RoBERTaFig. 1. Outputs of RoBERTa, fine-tuned RoBERTa, and AF Adapter based
RoBERTa for two input items.
inserts additional layers after specific layers of pretrained
models. Adapter-based tuning methods better mitigate the
forgetting problem than other methods, thus becoming popular
recently.
Despite the good performance, the adding-layer manner
of the current adapter-based tuning methods adds depth to
the networks. The input needs to be fed forward to more
layers and thus is more likely to be forgotten. To alleviate
the issue, we propose a layer-extending continual pretraining
method, named Attention- FFNAdapter (AF Adapter), which
extends the attention matrixes and feed-forward networks
(FFN). Specifically, its essential idea is to introduce a small
number of heads and hidden units inside each self-attention
and FFN layer of BERT. In this method, only the added
parameters are trainable, and the original pretrained parameters
are frozen. This layer-extension method does not increase
network layer depth, thus alleviating the input information
forgetting problem. Then, AF Adapter based RoBERTa is
obtained by fine-tuning a pretrained language model on the
medical domain with AF Adapter. To estimate the feasibility
of AF Adapter, AF Adapter based RoBERTa is compared
with other models in the downstream NLP tasks. Experimental
results demonstrate that with only 17% of model parameters
trained, AF Adapter based RoBERTa achieves 0.6%, 2% gain
in performance compared with state-of-the-art models, on
average.
Contributions. The contributions in this paper are summa-
rized as follows:
•We propose a layer-extending continual pretraining
method called AF Adapter (Attention-FFN Adapter),
which aims to extend attention heads and hidden units
for the BERT-based model. The method further allevi-
ates catastrophic forgetting compared with layer-adding
methods.
•We propose a Chinese biomedical-domain pretrained lan-
guage model called AF Adapter based RoBERTa, which
is trained using AF Adapter and biomedical corpus. AF
Adapter based RoBERTa contributes greatly to down-
stream medical tasks.
•Our method achieves 0.6%, 2% gain in performance onaverage, compared to strong baselines on the CBLUE
benchmark. Besides, AF Adapter alleviates the catas-
trophic forgetting problem by 11% compared to the fine-
tuning method.
II. R ELATED WORK
In this section, we provide a brief overview of the existing
approaches in the fields of language modeling and domain-
specific pretraining. We present a summary of different meth-
ods and techniques that have been employed in these areas.
A. Language Modeling
In natural language processing, pretraining language models
with large amounts of unannotated data have proven to be a
successful strategy for transfer learning. It is effective to learn
general language representations from language models and
transfer the model to specific downstream tasks by fine-tuning.
Language models are usually trained on particular pre-
training tasks using encyclopedic and newsletter corpus. The
pretraining tasks are crucial for learning general language
representations. For self-supervised pretraining, the widely-
used tasks are as follows:
Language Modeling (LM) is a classic probabilistic density
estimation problem, used in e.g., GPT-2 [16], ULMFiT [12]
and SiATL [17]. The next output token depends on the joint
probability of the previous tokens. A drawback of unidirec-
tional language modeling is that the representation of each
token encodes only the leftward context tokens. However, bet-
ter contextual representations of text should encode contextual
information from both directions.
Masked Language Modeling (MLM) is a pretraining
task to overcome the drawback of unidirectional language
modeling. MLM first masks out some tokens in the input text
and then trains the model to predict the masked tokens. MLM
is used in BERT [5], MASS [18], T5 [19], RoBERTa [6], etc.
Permuted Language Modeling (PLM) is a language mod-
eling task on a random permutation of the input text, used by
XLNet [20], BART [21]. They observed that certain special
tokens, such as [MASK], are not present during downstream
tasks. To bridge the gap between pre-training and fine-tuning,
the model is trained to predict some tokens in permuted token
sequence, depending on the rest of the tokens.
Contrastive Learning (CTL) assumes some observed
pairs of text which are more semantically similar than ran-
domly sampled text. Common CTL tasks include Next Sen-
tence Prediction (NSP) [5] [22], Sentence Order Prediction
(SOP) [23] [24].
B. Domain-Specific Pretraining
Specialized domains like biomedicine pose challenges for
general-domain pretrained language models due to the follow-
ing reasons:
•General-domain pretrained language models are trained
on datasets primarily sourced from encyclopedias and
newsletters, making it difficult to estimate their perfor-
mance on biomedical domain text.

--- PAGE 3 ---
InputOutput
Transformer Layer N
Transformer Layer 1Feed Forward Network
Multi-Head
Self-AttentionAdded HeadAdded FFN…
…Extended ATT
Concat
& Linear
Scaled
Dot-Product
Attention
Linear … …… …… …
…
added headsLinear 2
Linear 1Activation
(GeLU)…
……
……
…Extended FFN
added ffn
… Input Tensor… Output Tensor
… Input Tensor… Output Tensor
original attention heads original feed forward network… … …
frozen hidden units trainable added unitsFig. 2. The illustration of AF Adapter. AF Adapter introduces a small number of additional heads and hidden units to each self-attention layer and feed-
forward network of the BERT-based model. The added parameters of each layer are independent. The heads and hidden units with orange color are trainable,
and all parameters with grey color from the original model are frozen.
•Word distributions differ significantly between general-
domain text and biomedical-domain text, including the
presence of biomedical terms.
•Biomedical text in Chinese biomedicine exhibits complex
phrase combinations and structures.
To address these challenges, researchers have proposed
studies focusing on biomedical domain-specific pretraining,
which can be categorized into two main types.
Domain-specific pretraining from scratch is based
on large-scale biomedical corpora. For instance, PubMed-
BERT [7] and BioGPT [25] are trained on PubMed-based
corpora, containing 3.1 billion words and 15 million items
respectively. One notable advantage of domain-specific pre-
training from scratch is the ability to customize the model’s
vocabulary to the domain. This allows for more appropriate
tokenization of medical terms, avoiding fragmented subword
representations.
Continual pretraining of a general-domain pretrained
model is a common approach for pretraining a biomedical
model. This approach involves initializing with a standard
model pretrained on encyclopedia and newsletter corpora
and then continuing the pretraining process using biomedical
corpora. Continual pretraining benefits from the knowledge ac-
quired during general-domain pretraining. Recent studies have
demonstrated that injecting extra knowledge information can
enhance the model [26], such as knowledge acquisition [27].
However, sequential task training may lead to catastrophic
forgetting, where the model forgets the general-domain knowl-
edge [9] [10] [11]. BioBERT is an example of this approach,
where continual pretraining is initialized with weights from
BERT and conducted using the PubMed corpus [8]. In the
Chinese context, PCL-MedBERT3is trained based on BERT
using biomedical text and medical QA corpora.
3https://www.ihub.org.cn/html/2020/news 0824/47.htmlIII. P RELIMINARIES
In this section, we provide a concise review of two key
components in the BERT architecture: the multi-head self-
attention mechanism and the feed-forward network. We briefly
explain their roles and functions within the BERT model.
A. BERT
The BERT model architecture is built upon a multi-layer
bidirectional Transformer encoder structure [28]. Each encoder
consists of a stack of identical blocks that incorporate multi-
head self-attention and feed-forward networks. The multi-head
self-attention layer can be formulated as follows:
Q(x) =xWQ+bQ,
K(x) =xWK+bK,
V(x) =xWV+bV,(1)
where WQ∈Rdmodel×hdk,WK∈Rdmodel×hdk,WV∈
Rdmodel×hdv,bQ∈Rdk,bK∈RdkandbV∈Rdvrepresent
the weight matrices and biases. dk=dv=dmodel/h,dmodel
is the model dimension, and his the number of attention heads.
TheQ,K, and Vare then split into hparts to calculate the
multi-head attention. This can be expressed as:
head j=Attention (Qj, Kj, Vj),
MultiHead (x) = [head 1, . . . , head h]WO,(2)
where Q= [Q1:···:Qh],K= [K1:···:Kh],V= [V1:
···:Vh]andWO∈Rhdv×dmodel . The operation “ :” stand for
the column-wise concatenation.
After the multi-head attention, the feed-forward network
is employed, consisting of two linear transformations with a
GeLU activation function in between:
FFN(x) =GeLU (xW1+b1)W2+b2, (3)
where W1∈Rdmodel×dff,W2∈Rdff×dmodel ,b1∈Rdffand
b2∈Rdmodel represent the weight matrices and biases applied
to the input xin the feed-forward network.

--- PAGE 4 ---
… Input Tensor… Output Tensor
…
Q K VAttention
Q K VAttention
Q K VAttention
…
Q K VAttention
Original Linear Units Added Linear UnitsOriginal Linear Units Added Linear Units
ConcatFig. 3. The attention layer architecture of AF Adapter.
IV. M ETHODOLOGY
In this section, AF Adapter is introduced to train domain-
specific PLMs based on general-domain PLMs. The overall
architecture of AF Adapter is illustrated in Fig. 2.
A. Model Architecture
Following adapter tuning [14] [29], to alleviate catastrophic
forgetting in the fine-tuning stage, additional trainable domain-
specific parameters are inserted into the extension of the
attention and FFN layer, while parameters in the original
model are fixed to preserve the knowledge of the general
domain.
Extended Attention Layer. A key component of BERT is
the multi-head scaled dot-product self-attention mechanism.
In this work, the self-attention layer is extended to alleviate
catastrophic forgetting. Specifically, as shown in Fig. 3, a
number of “domain-specific heads” in each attention layer
are added. Note that added domain-specific heads in different
layers are independent of each other. To extend the self-
attention mechanism, iadditional attention heads are added.
Formally, the input into the multi-head self-attention layer is
denoted as x.xis firstly fed into linear layers, and mapped
intoQ′,K′,V′. The extended weight matrices are denoted
as[WQ:W′
Q],[WK:W′
K], and [WV:W′
V].WQ,WK,
andWVare matrices from the original pretrained model, as
described in Equation 1. Then, Q′(x)∈Rdmodel×(h+i)dk,
K′(x)∈Rdmodel×(h+i)dk,V′(x)∈Rdmodel×(h+i)dvare
obtained,
Q′(x) =x[WQ:W′
Q] + [bQ:b′
Q],
K′(x) =x[WK:W′
K] + [bK:b′
K],
V′(x) =x[WV:W′
V] + [bV:b′
V],(4)
where the projections are parameter matrices W′
Q∈
Rdmodel×idk,W′
K∈Rdmodel×idk,W′
V∈Rdmodel×idv,b′
Q∈
Ridk,b′
K∈Ridkandb′
V∈Ridv.dk,dv,dmodel ,hare
hyperparameters described in Equation 1.
… Input Tensor… Output Tensor
Original Linear Units Added Linear UnitsOriginal Linear Units Added Linear Units
Activation (GeLU)Fig. 4. The feed-forward network architecture of AF Adapter.
Then, the Q′,K′andV′are divided into h+iparts for
calculating the multi-head attention,
head′
j=Attention (Q′
j, K′
j, V′
j),
where Q′= [Q1:···:Qh:Q′
1:···:Q′
i],
K′= [K1:···:Kh:K′
1:···:K′
i],
V′= [V1:···:Vh:V′
1:···:V′
i].(5)
Finally, the outputs of these heads are concatenated and then
fed to linear transformations,
MultiHead( x)
= [head 1, ..., head h, head′
1, ..., head′
i][WO⊥W′
O],(6)
where W′
O∈R(dk×i)×dmodel , and head jis the jth head
described in Equation 2. The operation “ ⊥” represents the
row-wise concatenation.
Extended Feed-Forward Network. For FFN in the model,
a multi-layer perceptron with one hidden layer, we add a
number of hidden units in each FFN layer with GeLU activa-
tion between two linear transformations, as shown in Fig. 4.
Formally, we add “domain-specific hidden units” of size aby
extending W1andW2to[W1:W′
1]and[W2⊥W′
2],
FFN(x)
= GeLU( x[W1: W′
1] + [b 1: b′
1])[W 2⊥W′
2] + b 2+ b′
2,
(7)
where W′
1∈Rdmodel×a,b′
1∈Ra,W′
2∈Ra×dmodel , and
b′
2∈Rdmodel .W1,W2,b1andb2are metrices described in
Equation 3.
B. Pretraining Details
Pretraining Tasks. To continually pretraining the BERT-
based model, the masked language modeling (MLM) is uti-
lized to pretrain the model. In MLM, a subset of input tokens
is randomly replaced with a special token (e.g., [MASK] ),
and MLM is designed to predict these tokens. The training
objective is the cross-entropy loss between the original tokens
and the predicted ones. As same as BERT and RoBERTa, 15%
of the input tokens are chosen, among which a random 80%
are replaced by [MASK] , 10% are left unchanged, and 10%
are randomly replaced by a token from the vocabulary.

--- PAGE 5 ---
TABLE I
STATISTICS OF CORPORA FOR MODEL PRETRAINING .
Corpus Type # of Sentences # of Tokens
Biomedical Question Answering 4,842k 93M
Medical Encyclopedia 793k 33M
Electronic Medical Record 1,762k 45M
Whole Word Masking. In the original BERT, the text is
split into subwords and tokenized into tokens. The whole word
masking mitigates the drawback of masking only a part of
the whole word. For the whole word masking in Chinese, the
traditional Chinese word segmentation tool “Jieba” is utilized
to split the sentence into several words and provide extra word
information to the data collator before masking.
Training Strategy. Fine-tuning with domain-specific cor-
pora is a standard method to continually pretrain the general-
domain models. However, this method suffers from the catas-
trophic forgetting problem [9].
To alleviate the issue, we first make progress in the model
architecture and do not train all parameters in the model. Only
the domain-specific parameters added in the self-attention
and FFN ( W′
Q,W′
K,W′
V,W′
O,W′
1,W′
2) are trained and
all parameters inherited from the general-domain model are
frozen.
Corpora are preprocessed into datasets before training. Para-
graphs are split into sentences by separators, and sentences are
grouped to the max sequence length of the model.
V. E XPERIMENTS
In this section, we provide the details of our experimen-
tal setup and present the results obtained from the evalua-
tion. We pretrain the domain-specific models using collected
datasets and then evaluate them on downstream tasks from the
CBLUE [30] benchmark. Furthermore, we conduct an ablation
study on different pretraining techniques and investigate con-
vergence and stability. Additionally, we perform a catastrophic
forgetting analysis to evaluate the models’ ability to retain
previously learned knowledge when trained on new tasks.
A. Pretraining Data and Settings
Datasets . To evaluate the performance of our method, we
collect a variety of Chinese biomedical corpora. These corpora
include Chinese biomedical question answering, Chinese med-
ical encyclopedia from Baidu Encyclopedia and Wikipedia,
and Electronic Medical Records from Fudan University Shang-
hai Cancer Center. Details of the pretraining corpora used
for our experiments are presented in Table I. We use these
corpora to pretrain our model before fine-tuning on evaluation
tasks. By leveraging these diverse and comprehensive datasets,
we aim to improve the model’s ability to understand natural
language text in the biomedical domain.
Backbone. We perform experiments based on the Chinese
RoBERTa-wwm-ext-base model [31], which is obtained from
the HuggingFace Hub. Specifically, the model originally con-
sists of 12 transformer encoder layers, each with a hidden sizeTABLE II
STATISTICS OF CBLUE TASKS .
Task Type Train Dev Test Metric
CMeEE NER 15,000 5,000 3,000 Micro-F1
CMeIE RE 14,339 3,585 4,482 Micro-F1
CHIP-CDN NORM 6,000 2,000 10,000 Micro-F1
CHIP-STS TS 16,000 4,000 10,000 Macro-F1
CHIP-CTC TC 22,962 7,682 10,192 Macro-F1
KUAKE-QIC TC 6,931 1,955 1,994 Acc
KUAKE-QTR NLI 24,174 2,913 5,465 Acc
KUAKE-QQR NLI 15,000 1,599 1,596 Acc
of 768. Each layer also includes 12 self-attention heads and a
feed-forward network with an intermediate size of 3,072. We
add one additional attention head to each layer and increase the
intermediate size of the feed-forward network by 1,024. We
transfer all the model parameters from the Chinese RoBERTa-
wwm-ext-base model to our modified architecture.
Pretraining Settings. We pretrain our model using the orig-
inal vocabulary provided with the Chinese RoBERTa model.
We use the AdamW optimizer with warm-up and weight
decay. Specifically, we begin with a learning rate of zero,
which increases linearly to a peak rate of 4×10−4over
the first 1000 steps of training. The learning rate then decays
linearly to zero over the remaining steps. We train the model
for 100,000 steps using a total batch size of 512, spread across
two NVIDIA A100 (80G) GPUs with a batch size of 64 and
gradient accumulation steps of 4. We use the Chinese whole-
word masking (WWM) technique during pretraining, with a
masking rate of 15% and a maximum sequence length of 512
tokens.
Baselines. The baseline models used in our experiment are
of similar sizes and have been widely used. For comparison
of PLMs, the models include BERT-base [5], BERT-wwm-
ext-base [31], RoBERTa-wwm-ext-base [31], PCL-MedBERT,
and MacBERT-base [32]. All these models have undergone
pre-training on large-scale corpora. For comparison of pre-
training techniques, the baselines include Fine-Tuning, FL-
Tuning [29], and LoRA [33]. We compare the performance of
our model against these baselines to evaluate its effectiveness.
B. Evaluation Tasks
We compare models by applying them to the downstream
NLP tasks, specifically the Chinese Biomedical Language Un-
derstanding Evaluation (CBLUE [30]) benchmark. As shown
in Table II, the benchmark divides eight tasks into six cat-
egories: medical named entity recognition (NER), medical
relationship extraction (RE), medical diagnosis normalization
(NORM), medical textual similarity (TS), text classification
(TC), and natural language inference (NLI).
•Named Entity Recognition (NER) is the task of tagging
entities in text with their corresponding type. CMeEE
task provides a pre-defined schema including nine entity
categories.

--- PAGE 6 ---
TABLE III
COMPARISON RESULTS OF DIFFERENT MODELS ON CBLUE TASKS .
Model CMeEE CMeIE CDN CTC STS QIC QTR QQR Avg.
BERT-base 62.1 54.0 55.4 69.2 83.0 84.3 60.0 84.7 69.0
BERT-wwm-ext-base 61.7 54.0 55.4 70.1 83.9 84.5 60.9 84.4 69.4
RoBERTa-wwm-ext-base 62.4 53.7 56.4 69.4 83.7 85.5 60.3 82.7 69.3
PCL-MedBERT 60.6 49.1 55.8 67.8 83.8 84.3 59.3 82.5 67.9
MacBERT-base 60.7 53.2 57.7 67.7 84.4 84.9 59.7 84.0 69.0
AF Adapter based RoBERTa (ours) 62.984 54.932 56.484 69.655 83.632 85.858 61.099 84.587 69.904
Our AF Adapter based RoBERTa achieves the highest average score among all competitors. The values are presented in percentage (%).
TABLE IV
EVALUATION OF PRETRAINING TECHNIQUES .
Techniques Trainable Parameters CMeEE CMeIE CDN CTC STS QIC QTR QQR Avg.
Fine-Tuning 102M(100%) 61.000 50.847 54.738 66.154 79.618 81.004 55.243 75.879 65.560
FL-Tuning fl=1024 19M(15.59%) 62.147 52.977 56.136 68.308 83.033 84.805 62.892 83.522 69.275
LoRA r=128 21M(17.19%) 61.747 36.333 49.859 62.150 82.177 80.894 60.586 80.954 64.338
AF Adapter (ours) 21M(17.21%) 62.984 54.932 56.484 69.655 83.632 85.858 61.099 84.587 69.904
•Relationship Extraction (RE) aims to extract semantic
relationships between two or more entities of a certain
type from unstructured text into a number of semantic
categories. There are 53 relations defined in the CMeIE
task, including 10 synonymous sub-relationships and 43
other sub-relationships.
•Lexical Normalization (NORM) is the task of trans-
forming a non-standard text into a standard register.
Clinically, there might be up to hundreds of different syn-
onyms for the same diagnosis, symptoms, or procedures.
The task aims to find the standard phrases for the given
clinical term.
•Textual Similarity (TS) deals with determining how se-
mantically similar two pieces of text are. The CHIP-STS
task aims to evaluate the generalization ability between
disease types on Chinese disease questions and answer
data.
•Text Classification (TC) includes simple short text clas-
sification and sentence intent classification. For CHIP-
CTC, the task is to classify clinical trials’ eligibility
criteria. For KUAKE-QIC, the task is to classify each of
them into one of 11 pre-defined medical intent categories.
•Natural Language Inference (NLI) is the task of de-
termining whether a hypothesis is true (entailment) or
false (contradiction) or undetermined (neutral) given a
premise. For the KUAKE-QTR and KUAKE-QQR tasks,
the dataset is used to estimate the relevance between the
query and the title or another query.
For the named entity recognition task, we use BIO nota-
tion [34], which differentiates the beginning (B), the inside (I)
of entities, and the outside (O) of entities. For the relationship
extraction task, we split it into two steps: first, we recognize
the subject and object entities and then extract the semantic
relationships between them. For the lexical normalization task,
we use the recall and ranking approach. For other tasks, weuse simple classification architecture to predict the result.
The evaluation process is similar to the CBLUE benchmark
toolkit [30], and we leverage the hyperparameters provided by
the CBLUE RoBERTa baseline.
C. Main Results
In this section, we present the comparative analysis of
different PLMs and pretraining techniques. We summarize
the main findings and highlight the performance differences
observed among the models and techniques.
1) Comparison of PLMs: For comparison, we use the
CBLUE’s public results of BERT-base, BERT-wwm-ext-base,
RoBERTa-wwm-ext-base, PCL-MedBERT, and MacBERT-
base baseline. Table III shows the performances of Chinese
NLP tasks of the CBLUE benchmark.
For the NER task, our AF Adapter based RoBERTa achieve
a score of 62.984%, outperforming the other models. This
demonstrates its ability to accurately extract biomedical en-
tities from unstructured text.
In the RE task, we observe that AF Adapter based RoBERTa
outperforms the backbone model by about 1.2%, surpassing
the other models as well. This highlights its effectiveness in
extracting semantic relationships between entities of a certain
type, demonstrating its superior performance in biomedical
information extraction.
Regarding the NORM, TS, and TC tasks, AF Adapter
based RoBERTa achieves similar performance compared to
the backbone model. Notably, our model outperforms the other
models in the QIC task, showcasing its advantage in accurately
classifying medical intents.
In the NLI tasks, our model achieves the best performance,
surpassing the backbone model by approximately 1.9%. This
signifies its ability to determine the logical relationships be-
tween query and title, highlighting its superiority in resolving
the challenges for search engines.

--- PAGE 7 ---
0 20000 40000 60000 80000 100000
Steps0.40.60.81.01.21.4LossFine-Tuning
FL Tuning
LoRA
AF AdapterFig. 5. Convergence comparison of Fine-Tuning, FL-Tuning, LoRA and AF
Adapter. The curves in dark color are obtained by smoothing the loss curves
(light color). The smoothing function is α∗previous smoothed value +
(1−α)∗current value , where α= 0.6is the smooth weight.
Overall, our AF Adapter based RoBERTa demonstrates
strong performance across multiple task types and performs
the best among models on average. It outperforms the back-
bone model RoBERTa-wwm-ext-base and medically domain-
specific model PCL-MedBERT by 0.6%, and 2%, respectively.
While achieving similar performance in NORM, TS, and TC
tasks, our model stands out in the NER, RE, and NLI tasks.
These results highlight the versatility and effectiveness of our
AF Adapter based RoBERTa in addressing a wide range of
biomedical language understanding challenges.
2) Comparison of Pretraining Techniques: To investigate
the impact of pretraining techniques on domain-specific mod-
els, we conduct several experiments using Chinese RoBERTa-
wwm-ext-base as the backbone model. With the original vo-
cabulary and the same corpus as AF Adapter based RoBERTa,
we continually pretrain the model using Fine-Tuning and
other methods. To ensure a fair comparison, we adjust the
hyperparameters of the techniques, excluding Fine-Tuning, to
achieve a similar size of trainable parameters. The evaluation
results of the pretraining techniques are presented in Table IV.
The experimental results highlight the superior performance
of the pretrained model using AF Adapter. Compared to
the Fine-Tuning approach, AF Adapter achieves remarkable
results while training only about 17% of the model parameters.
By adding additional parameters, AF Adapter consistently
outperforms LoRA by an average margin of 5.5%. Notably, our
experiments reveal that the Fine-Tuning method leads to sub-
optimal performance. These findings suggest that continuing
pretraining without considering domain-specific knowledge
and representations may fail to effectively balance domain-
specific information with general-domain representations.
D. Detailed Analysis
In this subsection, we delve into a detailed analysis of the
convergence and stability of the pretraining techniques in our
study. Additionally, we conduct an analysis of catastrophicTABLE V
CATASTROPHIC FORGETTING ANALYSIS .
Model Accuracy Diff
RoBERTa 87.752 -0
Fine-Tuning based RoBERTa 70.964 -16.788
FL-Tuning based RoBERTa 83.555 -4.197
LoRA based RoBERTa 78.231 -9.521
AF Adapter based RoBERTa (ours) 82.428 -5.324
forgetting, which assesses the extent each method retains
previously learned knowledge while adapting to a new domain.
Convergence and Stability Analysis. We examine the
convergence and stability of pretraining techniques throughout
the pretraining process and present the results in Fig. 5.
From the results, we observe that our AF Adapter exhibits
a significant decrease in loss compared to LoRA. This slower
convergence indicates that the model requires more iterations
to reach an optimal solution. One possible explanation for
this observation is that AF Adapter incorporates a “parallel”
structure and the additional parameters enable the model to
better learn and focus on the domain-specific information.
Alongside convergence, our AF Adapter demonstrates con-
sistent and stable performance, maintaining a smooth and
gradual decrease in loss. In contrast, FL-Tuning and Fine-
Tuning exhibit more fluctuations and variations in loss values.
This stability analysis further supports the robustness of our
AF Adapter approach in learning domain-specific information.
Catastrophic Forgetting Analysis. To assess the forgetting
problem caused by sequential task training, we randomly
choose 10k general-domain samples from the “WuDao” cor-
pus [35]. These samples are preprocessed using a masking
rate of 15%, ensuring consistent inputs across all compared
models. We evaluate the accuracy between Chinese RoBERTa-
wwm-ext-base, Fine-Tuning based, FL-Tuning based, LoRA
based, and AF Adapter based RoBERTa trained in Sec-
tion V-C2. The experimental result, presented in Table V,
demonstrates the models’ performance in terms of catastrophic
forgetting. Our model achieves only -5.324% accuracy reduc-
tion compared to the original RoBERTa. Additionally, it is
evident that AF Adapter based RoBERTa outperforms Fine-
Tuning based RoBERTa by a significant margin, achieving an
accuracy of 82.428 compared to 70.964. This indicates that
our model better retains previously learned knowledge when
trained on new tasks. The model trained using fine-tuning
exhibits a substantial drop in accuracy, indicating a significant
forgetting of general knowledge. This aligns with its overall
poor performance in CBLUE, as shown in Table IV.
VI. C ONCLUSION
In this paper, we propose a continual pretraining approach,
named AF Adapter, for building a domain-specific pretrained
language model. By incorporating additional attention heads
and hidden units within the BERT-based model, AF Adapter
enables effective learning of domain-specific knowledge while
leveraging the general language representations. We also

--- PAGE 8 ---
present a Chinese biomedical-domain pretrained language
model called AF Adapter based RoBERTa, which is trained
using AF Adapter and biomedical corpus. The evaluation of
AF Adapter based RoBERTa on the CBLUE benchmark show-
cases its superior performance, outperforming other models of
“similar size” by an average of 0.98%. Moreover, our analysis
reveals that AF Adapter exhibits robust convergence and sta-
bility compared to other pretraining techniques. These findings
highlight the effectiveness and potential of AF Adapter in
domain-specific pretraining for language models.
ACKNOWLEDGMENT
The authors thank the anonymous reviewers for their
valuable suggestions. This work is supported by the Na-
tional Key Research and Development Program of China
(2021YFC2701800, 2021YFC2701801).
REFERENCES
[1] Christopher Manning and Hinrich Schutze. Foundations of statistical
natural language processing . MIT press, 1999.
[2] Elizabeth D Liddy. Natural language processing. 2001.
[3] Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik,
Jingbo Shang, Curtis Langlotz, and Jiawei Han. Cross-type biomedical
named entity recognition with deep multi-task learning. Bioinformatics ,
35(10):1745–1752, 2019.
[4] Sangrak Lim and Jaewoo Kang. Chemical–gene relation extraction using
recursive neural network. Database , 2018, 2018.
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
Bert: Pre-training of deep bidirectional transformers for language un-
derstanding. arXiv preprint arXiv:1810.04805 , 2018.
[6] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692 , 2019.
[7] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,
Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.
Domain-specific language model pretraining for biomedical natural
language processing. ACM Transactions on Computing for Healthcare
(HEALTH) , 3(1):1–23, 2021.
[8] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical
language representation model for biomedical text mining. Bioinformat-
ics, 36(4):1234–1240, 2020.
[9] Michael McCloskey and Neal J Cohen. Catastrophic interference in
connectionist networks: The sequential learning problem. In Psychology
of learning and motivation , volume 24, pages 109–165. Elsevier, 1989.
[10] Roger Ratcliff. Connectionist models of recognition memory: constraints
imposed by learning and forgetting functions. Psychological review ,
97(2):285, 1990.
[11] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua
Bengio. An empirical investigation of catastrophic forgetting in gradient-
based neural networks. arXiv preprint arXiv:1312.6211 , 2013.
[12] Jeremy Howard and Sebastian Ruder. Universal language model fine-
tuning for text classification. arXiv preprint arXiv:1801.06146 , 2018.
[13] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effec-
tive regularization to finetune large-scale pretrained language models.
arXiv preprint arXiv:1909.11299 , 2019.
[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Syl-
vain Gelly. Parameter-efficient transfer learning for nlp. In International
Conference on Machine Learning , pages 2790–2799. PMLR, 2019.
[15] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying
Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. On the effectiveness of
adapter-based tuning for pretrained language model adaptation. arXiv
preprint arXiv:2106.03164 , 2021.
[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
Ilya Sutskever, et al. Language models are unsupervised multitask
learners. OpenAI blog , 1(8):9, 2019.[17] Alexandra Chronopoulou, Christos Baziotis, and Alexandros Potami-
anos. An embarrassingly simple approach for transfer learning from
pretrained language models. arXiv preprint arXiv:1902.10547 , 2019.
[18] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass:
Masked sequence to sequence pre-training for language generation.
arXiv preprint arXiv:1905.02450 , 2019.
[19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al.
Exploring the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(140):1–67, 2020.
[20] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive
pretraining for language understanding. Advances in neural information
processing systems , 32, 2019.
[21] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Ab-
delrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettle-
moyer. Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461 , 2019.
[22] Lajanugen Logeswaran and Honglak Lee. An efficient framework for
learning sentence representations. arXiv preprint arXiv:1803.02893 ,
2018.
[23] Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based
objectives for fast unsupervised sentence representation learning. arXiv
preprint arXiv:1705.00557 , 2017.
[24] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised
learning of language representations. arXiv preprint arXiv:1909.11942 ,
2019.
[25] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung
Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for
biomedical text generation and mining. Briefings in Bioinformatics ,
23(6), 2022.
[26] Matthew E Peters, Mark Neumann, Robert L Logan IV , Roy Schwartz,
Vidur Joshi, Sameer Singh, and Noah A Smith. Knowledge enhanced
contextual word representations. arXiv preprint arXiv:1909.04164 , 2019.
[27] Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen,
Wei Zhang, and Huajun Chen. Long-tail relation extraction via knowl-
edge graph embeddings and graph convolution networks. arXiv preprint
arXiv:1903.01306 , 2019.
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention
is all you need. Advances in neural information processing systems , 30,
2017.
[29] Jingping Liu, Yuqiu Song, Kui Xue, Hongli Sun, Chao Wang, Li-
han Chen, Haiyun Jiang, Jiaqing Liang, and Tong Ruan. Fl-tuning:
Layer tuning for feed-forward network in transformer. arXiv preprint
arXiv:2206.15312 , 2022.
[30] Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin
Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, et al. Cblue:
A chinese biomedical language understanding evaluation benchmark.
arXiv preprint arXiv:2106.08087 , 2021.
[31] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang.
Pre-training with whole word masking for chinese bert. IEEE/ACM
Transactions on Audio, Speech, and Language Processing , 29:3504–
3514, 2021.
[32] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and
Guoping Hu. Revisiting pre-trained models for Chinese natural lan-
guage processing. In Findings of the Association for Computational
Linguistics: EMNLP 2020 , pages 657–668, Online, November 2020.
Association for Computational Linguistics.
[33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank
adaptation of large language models. arXiv preprint arXiv:2106.09685 ,
2021.
[34] Lance A Ramshaw and Mitchell P Marcus. Text chunking using
transformation-based learning. In Natural language processing using
very large corpora , pages 157–176. Springer, 1999.
[35] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo
Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-
scale chinese corpora for pre-training language models. AI Open , 2:65–
68, 2021.
