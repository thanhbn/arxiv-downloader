# 2310.09753.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2310.09753.pdf
# File size: 2820897 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
When can transformers reason with abstract symbols?
Enric Boix-Adserà*1,2Omid Saremi1Emmanuel Abbe1,3
Samy Bengio1Etai Littwin1Joshua Susskind1
1Apple2MIT3EPFL
eboix@mit.edu,emmanuel.abbe@epfl.ch
{osaremi,bengio,elittwin,jsusskind}@apple.com
April 17, 2024
Abstract
We investigate the capabilities of transformer models on relational reasoning tasks. In these
tasks, models are trained on a set of strings encoding abstract relations, and are then tested
out-of-distribution on data that contains symbols that did not appear in the training dataset.
We prove that for any relational reasoning task in a large family of tasks, transformers learn the
abstract relations and generalize to the test set when trained by gradient descent on sufficiently
large quantities of training data. This is in contrast to classical fully-connected networks, which
we prove fail to learn to reason. Our results inspire modifications of the transformer architecture
that add only two trainable parameters per head, and that we empirically demonstrate improve
data efficiency for learning to reason.
1 Introduction
As large language models (LLMs) are trained with increasing quantities of data, they begin to
exhibit the ability to reason mathematically [Kap+20; Yua+23]. Why does more data help an LLM
learn to reason? And can we make LLMs more data-efficient at learning to reason?
In this paper, we study relational reasoning with abstract symbols, which is a basic capability
that has been hypothesized to underlie more complex abilities in human cognition [Fod75; New80;
SKM84; Mar98; Hol12; Kri+13; WSC20]. One example is in mathematics or computer science,
where relational reasoning is necessary to parse a proof or a program: variable names are abstract
symbols and the functionality of the proof or program only depends on how they relate to each
other and not on the variable names themselves.
Our contributions are threefold: (i) we formalize relational reasoning through “template tasks”;
(ii) we conduct an analysis of when transformers can learn template tasks when trained by gradient
descent and show a separation with classical fully-connected neural network architectures; (iii) we
propose modifications to transformers that improve data efficiency for learning to reason.
1.1 Capturing relational reasoning with template tasks
Building on a line of work in neuroscience [Mar98; MK16; KRS18; WSC20; Ker+22; Alt+23;
WHL23; Gei+23], we formalize a framework of reasoning tasks called template tasks .
1arXiv:2310.09753v2  [cs.CL]  16 Apr 2024

--- PAGE 2 ---
(a)
123421 (b)
123421
1234? (c)
123421
1234?Figure 1: Tasksfrom[Rav38; WSC20]whichfall
under our theory. Networks are trained with one
alphabet of symbols and then tested on held-out
symbols. Details in Appendix A.
Regression setting In the regression setting, a template task is specified by a collection of
“template” strings labeled by real numbers, which are used to generate the train and test data. The
simplest way to describe these is through an example. Consider, for instance, the templates
“α=1;β=-1;print( α)”→label=+1 and “ α=1;β=-1;print( β)”→label=-1 .(1)
These are used to generate the datasets in Figure 2, where every sample (xi, yi)∈ Xk×Yis formed
by picking a template and replacing the placeholders α, β(which we call “wildcards”) with variable
names. Memorizing the training data is easy [Zha+21b], but we wish to measure reasoning: will
the model learn to treat the variable names as abstract symbols, enabling generalization beyond
its training distribution? To evaluate this, we adopt an out-of-distribution setting, where the train
and test data distributions differ [Mar98; Abb+23]. The test dataset consists of the same programs,
but with new variable names never seen during training . Bytesting on symbols unseen in the train
set, we measure the ability of an LLM to learn logical rules on the relations between symbols. To
succeed, the LLM must effectively infer the templates from training data, and at test time match
samples to the corresponding templates to derive their labels.
(a) Train data (b) Test data (c) Transformer performance
xi yi
a=1;b=-1;print(a) +1
c=1;a=-1;print(a) -1
f=1;c=-1;print(f) +1
h=1;q=-1;print(q) -1
. . . . . .xtest
i ytest
i
R=1;A=-1;print(R) +1
Q=1;V=-1;print(V) -1
. . . . . .
101
102
103
104
Number of training samples0.00.51.01.52.0T est lossTransformer
Transformer + KQ identity
Figure 2: (a,b) Variable names in the test data never appear in the train data (indicated by lower/upper-
case names). (c) Remarkably, as the training set size increases, the LLM’s ability to reason outside of
its training data improves, as it learns to use the relations between the variable names to classify, instead
of simply memorizing the training data. Our theory motivates a modified transformer architecture (see
Observation 1.2), which solves the reasoning task with less training data. Details in Appendix A.
Apart from programming tasks as in Figure 2, this framework captures several natural problems:
•Same/different task . The simplest relational reasoning task is when the templates are “ αα”
and “ αβ” labeled by +1and−1. This encodes learning to classify two symbols as equal
(e.g., AA,BB) or as distinct (e.g., AB,BC), even when the symbols were unseen in the
training data. This task has been studied empirically in animal behavior [MK16] and in
neural networks [KRS18; WSC20].
•Word problems . Word problems often have building blocks that follow simple templates. For
example, the template “If αgives β5γ, how many γdoes βhave?” labeled by +5, could
generate the data “If Alice gives Bob 5 oranges, how many oranges does Bob have?” or the
data “If Rob gives Ada 5 apples, how many apples does Ada have?”
2

--- PAGE 3 ---
•Psychometric tests . Psychometric tests of relational reasoning, which have recently been used
to probe LLMs [Rav38; WSC20; Alt+23; Ker+22; WHL23; Web+23], are often template
tasks. Figure 1 illustrates some examples.
Next-token-prediction setting In the next-token-prediction setting, there is one extra layer of
complexity: each sample is labeled with a symbol. For the LLM to generalize to symbols unseen
at train time, not only must it learn to track the value stored in a variable, but it also must learn
to predict labels at test time that might not occur in its training data. For example, the train and
test datasets in Figure 3 are generated by:
“α="γ";β="δ";print( α)”→label= γand “ α="γ";β="δ";print( β)”→label= δ ,(2)
where α, β, γ, δ are wildcards. Other problems covered by these tasks include:
•Programming . Thetemplate“ print(" α")” labeledwith αgenerates (print("A") ,A)or(print("dog") ,dog),
and so an LLM that learns on the corresponding task can robustly evaluating print statements
on symbols not seen in the training data.
•Mathematical functions . For example, the set of templates {ααα, αβα, ααβ, βαα }labeled by
αencode the task of outputting the majority token in a length-3 string with a vocabulary of
two symbols. Similarly, for length- kstrings, the task of outputting the majority element can
be encoded with 2k−1templates.
(a) Train data (b) Test data (c) Transformer performance
xi yi
a="d";b="q";print(a) d
c="r";a="w";print(a) w
f="y";c="u";print(f) y
h="o";q="s";print(q) s
. . . . . .xtest
i ytest
i
R="F";A="Z";print(R) F
Q="B";V="A";print(V) A
. . . . . .
101
102
103
104
105
Number of training samples0.00.51.01.52.0T est errorTransformer
Transformer + KQ,VO identity
Figure 3: (a,b) The labels are symbols. (c) We propose a modified that transformer learns the reasoning
task with less data (see Observation 1.2 and Theorem 1.4). Details in Appendix A.
1.2 Main results
The phenomenon from Figures 2 and 3 that we seek to understand is: why does the out-of-
distribution performance of the transformer architecture improve as the number of training samples
increases? We analyze the regression and next-token-prediction settings separately.
(1) MLPs fail to generalize to unseen symbols A classical criticism of connectionism by
[Mar98] is that neural networks do not learn relational reasoning when trained. We support this
criticisminAppendixIbyprovingthatclassicalMLParchitectures(a.k.a. fully-connectednetworks)
trained by SGD or Adam will not generalize in template tasks on symbols unseen during training,
even in the regression setting. This failure to reason relationally occurs regardless of the training
data size. The proof uses a permutation equivariance property of MLP training [Ng04; Sha18;
LZA20; Abb+22; AB22].
3

--- PAGE 4 ---
(2) Transformers generalize to unseen symbols, but require large data diversity Nev-
ertheless, we prove that he criticism of [Mar98] is not valid for modern transformer architectures
[Vas+17]. We analyze the training dynamics of a transformer model and establish that it can learn
to reason relationally:
Theorem1.1 (InformalTheorem3.4) .For any regression template task, a wide-enough transformer
architecture trained by gradient flow on sufficiently many samples generalizes on unseen symbols.
Here the key points are: (a) Universality . The transformer architecture generalizes on symbols
unseen in train data regardless of which and how many templates are used to define the reasoning
task.(b) Large enough number of samples . Our theoretical guarantees require the training dataset
size to be large, and even for very basic tasks like the two-template task in Figure 2, good general-
ization begins to occur only at a very large number of training samples considering the simplicity
of the task. This raises the question of how the inductive bias of the transformer can be improved.
The proof of Theorem 1.1 inspires a parametrization modification that empirically lowers the
quantity of data needed by an order of magnitude. A standard transformer attention head that
takes in an input X∈Rk×dembis given by
smax( XW KWT
QXT)XW VWT
O, (3)
where WK,WQ,WV,WOaretrainableparameters. Ourmodificationmakesiteasierforthetrans-
former to access the incidence matrix XXT∈Rk×kof the input, which is invariant to permutations
of the symbol alphabet and can be used to solve the relational reasoning task:
Observation 1.2. Adding one trainable parameter ato each attention head so that WKWT
Qis
replaced by WKWT
Q+aIimproves transformers’ data-efficiency on template tasks.
(3) Transformers fail at copying unseen symbols The story is slightly different for next-
token-prediction tasks, because of the bottleneck of learning to output a symbol that was never
seen in the training dataset. Transformers’ performance degrades as the model grows (an “inverse
scaling” law [McK+23]). Large transformers fail even for the task of copying the input.
Theorem 1.3 (Informal Theorem 4.1) .Transformers with large embedding dimension fail to gen-
eralize on unseen symbols for the copy-task outputting label “ α” on template “ α”.
However, we propose adding an attention-modulated skip connection , which corrects this failure,
making it easy for the transformer to learn to copy data between its residual streams:
Theorem 1.4 (Informal Theorem 4.2) .Adding one trainable parameter bto each head so that
WVWT
Ois replaced by WVWT
O+bImakes transformers generalize on the task of Theorem 1.3.
(4) Experiments We conclude with experimental validation of our architecture modifications,
and find that they improve data efficiency on relational reasoning tasks by an order of magnitude,
and improve language-modeling performance when training the GPT-2 architecture on Wikitext.
1.3 Related literature
A spate of recent work studies whether and how LLMs perform various reasoning tasks, each fo-
cusing on one component of reasoning: these include recognizing context-free grammars [Zha+23;
AL23], learning sparse functions [Ede+22], learning compositionally [Hup+20], generalizing out-of-
distribution when learning Boolean functions [Abb+23], performing arithmetic [Nan+23], learning
4

--- PAGE 5 ---
in context [Gar+22; Ahn+23; ZFB23], and evaluating indexing [Zha+21a]. Our setting is closest to
that of empirical work studying neural networks on relational reasoning tasks [Gei+23; Web+23].
For example, the four tasks in [WSC20], the matrix digits task in [WHL23], the SET game task
in [Alt+23], and most of the tasks in [Ker+22] (with the exception of the relational games tasks),
are examples of regression template tasks that fall under our theory. Furthermore, [KRS18] shows
experimentally that MLPs fail on the same/different template task, and we provide a proof for
this in Appendix I. There is also a literature on modifying training to improve relational reason-
ing: [Web+20] proposes applying Temporal Context Normalization during training, and [San+17;
San+18; PPW18; Sha+20; WSC20; Ker+22; Alt+23] propose new architectures. Finally, some
recent works in mechanistic interpretability look for subnetworks within trained networks that are
responsible for tasks such as variable binding [Ols+22; Dav+23]. In contrast, our focus is on prov-
ing when the transformer architecture learns or fails to learn, and on applying this theoretical
understanding to improve its data efficiency for relational reasoning.
2 Formal definition of template tasks
We formally define regression template tasks. For next-token prediction, see Appendix J.
Definition 2.1. Atemplate is a string z∈(X ∪ W )k, where Xis an alphabet of tokens, and
Wis an alphabet of “wildcards”. A substitution map is an injective function s:W → X. We
write sub(z, s)∈ Xkfor the string where each wildcard is substituted with the corresponding token:
sub(z, s)i=ziifzi∈ X, and sub(z, s)i=s(zi)ifzi∈ W. The string x∈ Xkmatches the template
zifx= sub( z, s)for some substitution map sand also s(W)∩ {zi}i∈[k]=∅: i.e., the substituted
tokens did not already appear in the template z.
Example Using Greek letters to denote the wildcards and Latin letters to denote regular tokens,
the template “ ααβST” matches the string “QQRST”, but not“QQQST” (because the substitution
mapisnotinjective)and not“QQSST” (because βisreplacedbySwhichisalreadyinthetemplate).
A template task’s training data distribution is generated by picking a template randomly from
a distribution, and substituting its wildcards with a random substitution map.
Definition 2.2. A template data distribution D=D(µtmplt,{µsub,z}z, f∗, σ)is given by
•a template distribution µtmpltsupported on templates in (X ∪ W )k,
•for each z∈supp( µtmplt), a distribution µsub,zover substitution maps s:W → X,
•template labelling function f∗: supp( µtmplt)→R, and a label-noise parameter σ≥0.
We draw a sample (x, y) = (sub( z, s), f∗(z) +ξ)∼ D, by drawing a template z∼µtmplt, a
substitution map s∼µsub,z, and label noise ξ∼ N(0, σ2).
Finally, we define what it means for a model to solve the template task and generalize on unseen
symbols; namely, the model should output the the correct label for any string x∈ Xkmatching a
template, regardless of whether the string is in the support of the training distribution.
Definition 2.3. A (random) estimator ˆf:Xk→Rgeneralizes on unseen symbols with (ϵ, δ)-
error if the following is true. For any x∈ Xkthat matches a template z∈supp( µtmplt), we
have
(ˆf(x)−f∗(z))2≤ϵ ,
with probability at least 1−δover the randomness of the estimator ˆf.
5

--- PAGE 6 ---
Example If the training data is generated from a uniform distribution on templates “ αα” with la-
bel1and“ αβ” forlabel-1,thenitmightconsistofthedatasamples {(AA,1),(BB,1),(AB,−1),(BA,−1)}.
An estimator that generalizes to unseen symbols must correctly label string CCwith +1and string
CDwith−1, even though these strings consist of symbols that do not appear in the training set.
This is a nontrivial reasoning task since it requires learning to use the relations between the symbols
to classify rather than the identities of the symbols.
3 Analysis for template tasks in the regression setting
We establish that one-layer transformers of large enough width generalize to unseen symbols, when
trained with enough data on regression template tasks. It is important to note that this is not
true for all architectures, as we prove in Appendix I that MLPs trained by SGD or Adam will not
succeed.
3.1 Transformer random features kernel
The one-layer transformer architecture that we analyze consists of an embedding layer, a multihead
attention mechanism, an MLP layer, and an unembedding layer wU. This is written mathematically
in Appendix H. We analyze training only the final wUlayer of the transformer, keeping the other
weights fixed at their random Gaussian initialization. Surprisingly, even though we only train
the final layer of the transformer, this is enough to guarantee generalization on unseen symbols.
Taking the width and embedding and head dimensions to infinity, and the step size to 0, the SGD
training algorithm with weight decay converges to kernel gradient flow with the following kernel
Ktransin the infinitely-wide, infinitely-small-step-size limit. Here and throughout the remainder
of the paper, we interchangeably denote an input by a string x∈ Xkor a matrix X∈Rk×m
constructed by stacking the one-hot vectors X= [ex1, . . . ,exk]Tof the string’s tokens. ϕ:R→Ris
the MLP activation layer, β, γ∈Rare hyperparameters controlling the temperature and magnitude
of positional activations.
Ktrans(X,Y) =Eu,v[ϕ(u)ϕ(v)]foru, v∼N(0,Kattn(X,X)Kattn(X,Y)
Kattn(Y,X)Kattn(Y,Y)
)(4)
where Kattn(X,Y) =Em(X),m(Y)[smax( βm(X))T(XYT+γ2I)smax( βm(Y))]
[m(X),m(Y)]∼N(0,XXT+γ2I XYT+γ2I
Y XT+γ2I Y YT+γ2I
).
The function outputted by kernel gradient flow is known to have a closed-form solution in terms
of the samples, the kernel, and the weight-decay parameter λ, which we recall in Proposition 3.1.
Proposition3.1 (Howkernelgradientflowgeneralizes; seee.g.,[Wel13].) .Let(X1, y1), . . . , (Xn, yn)
be training samples. With the square loss and ridge-regularization of magnitude λ, kernel gradient
flow with kernel Kconverges to the following solution
ˆf(X) =yT(ˆK+λI)−1k(X), (5)
where y= [y1, . . . , y n]∈Rnare the train labels, ˆK∈Rn×nis the empirical kernel matrix and has
entries ˆKij=K(Xi,Xj), and k(X)∈Rnhas entries ki(X) =K(Xi,X).
6

--- PAGE 7 ---
3.2 Transformers generalize on unseen symbols
We prove that transformers will generalize out-of-distribution on unseen symbols when trained on
template tasks. We require the templates in the distribution µtmpltto be “disjoint”, since otherwise
the correct label for a string xis not uniquely defined, as xcould match more than one template:
Definition 3.2. Two templates z,z′∈(X ∪ W )karedisjoint if nox∈ Xkmatches both zand
z′.
Furthermore, in order to ensure that the samples are not all copies of each other (which would
not help generalization), we have to impose a diversity condition on the data.
Definition 3.3. Thedata diversity is measured by ρ= min z∈supp( µtmplt)mint∈X1
Ps∼µsub,z[t∈s(W)].
When the data diversity ρis large, then no token is much more likely than others to be substi-
tuted. If ρis on the order of the number of samples n, then most pairs of data samples will not be
equal.
Theorem 3.4 (Transformers generalize on unseen symbols) .Letµtmpltbe supported on a finite set
of pairwise-disjoint templates ending with [CLS] tokens. Then, for almost any β, γ, b 1, b2parameters
(except for a Lebesgue-measure-zero set), the transformer random features with ϕ(t) = cos( b1t+b2)
generalizes on unseen symbols.1Formally, there are constants c, C > 0and ridge regularization
parameter λ >0that depend only β, γ, b 1, b2, µtmplt, f∗, σ, such that for any xmatching a template
z∈supp( µtmplt)the kernel ridge regression estimator ˆfin(5)with kernel Ktranssatisfies
|ˆf(x)−f∗(z)| ≤Cp
log(1/δ)/n+Cp
1/ρ ,
with probability at least 1−δ−exp(−cn)over the random samples.
The first term is due to the possible noise in the labels. The second term quantifies the amount
of sample diversity in the data. Both the sample diversity and the number of samples must tend to
infinity for an arbitrarily small error guarantee.
Proof sketch (1) In Lemma 3.5 we establish with a sufficient condition for kernel ridge regression
to generalize on unseen symbols. (2) We prove that Ktranssatisfies it.
(1) Sufficientcondition . Let µtmpltbesupportedontemplates z1, . . . ,zr. LetR=∪i∈[k],j∈[r]{zj,i}
be the tokens that appear in the templates. Let [n] =I1⊔ I2⊔ ··· ⊔ I nbe the partition of the
samples such that if a∈ Ijthen sample (xa, ya)is drawn by substituting the wildcards of template
zj. Two samples xa,xbthat are drawn from the same template zjmay be far apart as measured
by the kernel: i.e., the kernel inner product K(xa,xb)may be small. However, these samples will
have similar relationship to most other samples:
K(xa,xi) =K(xb,xi)for most i∈[n]. (6)
Specifically, if the wildcards of xa,xbandxiare substituted by disjoint sets of tokens that do not
appear in the templates, then (6) holds. Therefore, as the sample diversity ρincreases, the empirical
kernel matrix ˆKbecomes approximately block-structured with blocks Ij× Ij′. For most samples
xa,xbcorresponding to template zj, and most xa′,xb′corresponding to template zj′we have
K(xa,xa′) =K(xb,xb′) =K(sub(zj, s),sub(zj′, s′)) := Nj,j′, (7)
1We analyze the shifted and rescaled cosine activation function ϕ(t) = cos( b1t+b2)out of technical convenience,
but conjecture that most non-polynomial activation functions should succeed.
7

--- PAGE 8 ---
ˆK=I1I2
N=[K(aa,bb)K(aa,bc)K(aa,bc)K(ab,cd)]
I1
I2∈Rn×n, N=K(AA, BB )K(AA, BC )
K(BC, AA )K(AB, CD )
=
N=[K(aa,bb)K(aa,bc)K(aa,bc)K(ab,cd)] ∈R2×2
Figure 4: Illustration of structure of ˆKandNfor the same/different task, which has r= 2templates
z1=ααandz2=αβ. As the sample diversity ρincreases and the number of samples nincreases, the
empirical kernel matrix ˆK∈Rn×nbecomes approximately (r×r)-block-structured, and within each block
most of the entries are given by N∈Rr×r; exceptions where this is not true, including the diagonals, are
drawn in black. Furthermore, the spectrum of ˆKis increasingly determined by the spectrum of N, and
ifNis nonsingular then the top eigenspace increasingly aligns with the span of the indicator vectors on
I1, . . . ,Ir.
where s, s′:W → X are substitution maps satisfying
s(W)∩s′(W) = 0and s(W)∩ R=s′(W)∩ R=∅. (8)
One can check that (7) and (8) uniquely define a matrix N∈Rr×rwhich gives the entries in
the blocks of ˆK, with one block for each pair of templates.2See Figure 4.
If the matrix Nis nonsingular and the number of samples is large, then the span of the top r
eigenvectors of ˆKwill align with the span of the indicator vectors on the sets I1, . . . ,Ir. Further-
more, when testing a string xtestthat matches template zj, but might not have appeared in the
training set, it holds that for most a∈ Ij, we have
k(xtest) = [K(xtest,x1), . . . , K (xtest,xn)]≈[K(xa,x1), . . . , K (xa,xn)] = ˆKa,:.
In words, the similarity relationship of xtestto the training samples is approximately the same as
the similarity relationship of xato the training samples. So the kernel ridge regression solution (5)
approximately equals the average of the labels of the samples corresponding to template zj, which
in turn is approximately equal to the template label by a Chernoff bound,
yT(ˆK+λI)−1k(xtest)≈1
|Ij|X
a∈Ijyi≈f∗(zj). (9)
Therefore, kernel ridge regression generalizes on xtest. It is important to note that the number of
samples needed until (9) is a good approximation depends on the nonsingularity of N. This yields
the sufficient condition for kernel ridge regression to succeed (proof in Appendix C).
Lemma 3.5 (Informal Lemma C.3) .IfNis nonsingular, then (5)generalizes to unseen symbols.
(2)Ktranssatisfies the sufficient condition . We now show that for anycollection of disjoint
templates z1, . . . ,zr, the matrix Ntrans:=N∈Rr×rdefined with kernel K=Ktransis nonsingular.
The challenging is that Ktransdoes not have a closed-form solution because of the expectation over
softmax terms in its definition (4). Therefore, our analysis of the transformer random feature kernel
is, to the best of our knowledge, the first theoretical analysis showing that the transformer random
features learn a nontrival class of functions of sequences. We proceed by analyzing the MLP layer
and the attention layer separately, observing that a“weak” condition on Kattncan be lifted into
the “strong” result that Ntransis nonsingular. The intuition is that as long as Kattnis not a very
degenerate kernel, it is unlikely that the MLP layer has the cancellations that to make Ntrans
nonsingular.
2This assumes a “token-symmetry” property of Kthat is satisfied by transformers; details in the full proof.
8

--- PAGE 9 ---
Lemma 3.6 (Nonsingularity of Ntrans).Suppose for every non-identity permutation τ∈Sr\ {id},
X
i∈[r]Kattn(sub(zi, s),sub(zi, s′))̸=X
i∈[r]Kattn(sub(zi, s),sub(zτ(i), s′)), (10)
where s, s′are the substitution maps in the definition of Ntransin(8). Let the MLP layer’s activation
function be ϕ(t) = cos( b1t+b2). Then for almost any choice of b1, b2(except for a Lebesgue-measure-
zero set), the matrix Ntransis nonsingular.
This is proved in Appendix E, by evaluating a Gaussian integral and showing Ntranshas Van-
dermonde structure. Although we use the cosine activation function, we conjecture that this result
holds for most non-polynomial activation functions. Next, we prove the condition on Nattn.
Lemma 3.7 (Non-degeneracy of Kattn).The condition (10)holds for Lebesgue-almost any β, γ.
The proof is in Appendix F. First, we prove the analyticity of the kernel Kattnin terms of the
hyperparameters βandγ. Because of the identity theorem for analytic functions, it suffices to show
at least one choice of hyperparameters βandγsatisfies (10) for all non-identity permutations τ.
Since Kattndoes not have a closed-form solution, we find such a choice of βandγby analyzing the
Taylor-series expansion of Kattnaround β= 0andγ= 0up to order-10 derivatives.
3.3 Improving transformer data-efficiency with WKWT
Q+aIparametrization
Can we use these insights to improve transformers’ data-efficiency in template tasks? In the proof,
the nonsingularity of Nin Lemma 3.5 drives the model’s generalization on unseen symbols. This
suggests that an approach to improve data-efficiency is to make Nbetter-conditioned by modifying
the transformer parametrization. We consider here the simplest task, with templates “ αα” and “ αβ”
labeled with +1and−1, respectively. For tokens A, B, C, D ∈ X, the matrix Nis
N=K(AA, BB )K(AA, BC )
K(BC, AA )K(AB, CD )
IfKisaninner-productkernel, K(x,x′) =κ(P
i∈[k]1(xi=x′
i)),asfromanMLP,then K(AA, BB ) =
K(AA, BC ) = K(BC, AA ) = K(AB, CD ) = κ(0), soNis singular and generalization is not
achieved. Intuitively, every sample xihas approximately the same “similarity profile to other data”
ˆKi,:= [K(xi,x1), . . . , K (xi,xn)], so the kernel method cannot identify the samples that come from
the same template as xtest. In contrast, the transformer kernel (4) succeeds by using information
about the incidence matrix XXT, which differs between templates, and does not depend on the
symbol substitution. We thus propose to emphasize the incidence matrix XXTby reparametrizing
each head to WKWT
Q+aI, where ais a trainable parameter. This adds a scaling of XXTin the
attention, and can empirically improve data efficiency by an order of magnitude on several template
tasks (see Figures 2 and 3, as well as additional experiments in Appendix B).
4 Analysis for template tasks in next-token-prediction setting
We switch gears to the next-token prediction setting with the cross-entropy loss, where the output
label may be a token as in the example of Figure 3; formal definition is in Appendix J. The simplest
task consists of template “ α” labeled by “ α”. An example train set is {(A, A),(B, B),(C, C)}, where
A, B, C ∈ Xare tokens, and then we test with (xtest, ytest) = (D, D )which is not in the train set.
This task captures the ability of a model to learn how to copy a symbol, which is important for
9

--- PAGE 10 ---
(a) Vanilla transformer (b) Transformer with WVWT
O+bI
101
102
103
Number of training samples0.00.51.01.5T est errordim=64
dim=128
dim=256
dim=512
dim=1024
dim=2048
dim=4096
101
102
103
Number of training samples0.00.51.01.5T est errordim=64
dim=128
dim=256
dim=512
dim=1024
dim=2048
dim=4096
101
102
103
Number of training samples0.00.51.01.5T est error
Figure 5: (a) Transformers fail on the copying task as embedding dimension dembgrows (Theorem 4.1);
(b) Success when reparametrizing WVWT
OasWVWT
O+bI(Theorem 4.2). Details in Appendix A.
LLMs that solve problems with multi-stage intermediate computations and must copy these to later
parts of a solution [CIS21]. From now on, we only consider this “copying” task.
We consider an architecture fattn(x;θ)with just a multi-head attention layer, and we tie the
embedding and unembedding weights as in practice [Bro+20]. Define the train loss and test loss
as follows, where ℓis the cross-entropy loss and xtestis a token unseen in the training data:
Ltrain(θ) =1
nPn
i=1ℓ(fattn(xi;θ), yi)andLtest(θ) =ℓ(fattn(xtest), ytest). We prove this network
does not generalize on unseen symbols when trained, as we take the embedding dimension large.
Our evidence is from analyzing the early time of training, and showing that the test loss on unseen
symbols does not decrease.
Theorem 4.1 (Failure of transformers at copying) .For any learning rates such that −∂Ltrain
∂t|t=0=
O(1), we must have that∂Ltest
∂t|t=0→0asdemb→ ∞.
The proof idea is that since the input string has length k= 1, the architecture simplifies: all
softmaxes in the attention heads output 1, and the network is a sum of attention heads of the form
XW EWVWT
OWT
E. Atearlytimestheevolutionoftheweights WVWT
Owillroughlylieinthespan
of{WT
EexieT
xiWE}i∈[n], which as the embedding dimension becomes large will be approximately
orthogonal to the direction WT
EextesteT
xtestWEthat would lower the test loss. This suggests the
following modification to transformers allows them to copy symbols never seen at training:
Theorem 4.2 (Adding one parameter allows copying) .After reparametrizing the attention (3)so
that in each head WVWT
Ois replaced by WVWT
O+bIwhere bis a trainable parameter, there are
learning rates such that −∂Ltrain
∂t|t=0=O(1)and−∂Ltest
∂t|t=0= Ω(1)asdemb→ ∞.
Figures 3 and 5 illustrate the benefit of this additional per-head parameter on the copying
task. It is not equivalent to adding a trainable skip connection as in ResNet [He+16]. Instead, the
addition of bhIencodes an attention-modulated skip connection that allows copying tokens between
the transformer’s streams. A related modification of adding a head with the hardcoded XXTas
its attention matrix was proposed in [Zha+22].
5 Experiments
Figures 2 and 3 (and additional experiments in Appendix B) show that our reparametrizations
can give a significant data-efficiency benefit on template tasks. Figure 6 shows they can also give
improvements on real data. In Figure 7, we see that pretraining outperforms random initialization
on a template task. This might be explained by several heads of the pretrained model with diagonals
stronger from other weights (originally observed in [TK23]). These learned diagonals resemble our
proposed transformer modifications and so might be driving the data-efficiency of fine-tuning a
10

--- PAGE 11 ---
pretrained model. Appendix B provides extensive experiments on the effect of hyperparameters,
inductive biases of different models, and varying levels of task difficulty.
Dataset GPT-2 GPT-2 + trainable identity scalings (ours)
Wikitext2 64.00 60.46
Wikitext103 16.83 16.40
Figure 6: Perplexity of GPT-2 trained from random initialization with Adam learning rate 3e-4 for 20
epochs on Wikitext (smaller perplexity is better). GPT-2 has 117M parameters, and we add an extra 288
parameters (2 per head). Interestingly, even though the task is Wikipedia modeling, and therefore is not a
pure reasoning task, the transformer modifications still give an improvement.
Effect of pretraining WKWT
QHead 12, Layer 5 WVWT
OHead 12, Layer 11
101
102
103
104
Number of training samples0.00.51.01.52.0T est lossGPT-2 from scratch
GPT-2 pretrained
0 25 50 750
20
40
60
80
0 25 50 750
20
40
60
80
Figure 7: Left: Pretrained versus randomly-initialized GPT-2 test loss when fine-tuned on αβαvs.αββ
template task. Right: some GPT-2 pretrained heads have strong diagonals (zoomed to 100x100 top-left
corner).
6 Discussion
We show that transformers are a universal architecture for template tasks in the regression setting:
when trained with gradient descent with enough training data they learn to reason relationally.
However, transformersarenotoptimal–empiricallytheyrequirelargeamountsofdatatolearnbasic
tasks, and in the next-token-prediction setting they fail at copying unseen symbols. Thus, we have
proposed architectural modifications to improve their inductive bias towards logical reasoning. It
seems promising to explore other reasoning tasks (for example, reasoning with syllogisms, reasoning
by symmetry, and compositional reasoning). It may also be fruitful to study data augmentation
approaches (e.g., concatenating the tensorization XXTto the input, so as to encourage use of
relational information). Additionally, tight quantitative upper and lower bounds on the data and
width of the architecture needed, depending on the template task, are an interesting open direction.
References
[AB22] Emmanuel Abbe and Enric Boix-Adsera. “On the non-universality of deep learning:
quantifying the cost of symmetry”. In: Advances in Neural Information Processing Sys-
tems35 (2022), pp. 17188–17201.
[Abb+22] Emmanuel Abbe et al. “An initial alignment between neural network and target is
needed for gradient descent to learn”. In: International Conference on Machine Learn-
ing. PMLR. 2022, pp. 33–52.
11

--- PAGE 12 ---
[Abb+23] Emmanuel Abbe et al. “Generalization on the unseen, logic reasoning and degree cur-
riculum”. In: arXiv preprint arXiv:2301.13105 (2023).
[Ahn+23] KwangjunAhnetal.“Transformerslearntoimplementpreconditionedgradientdescent
for in-context learning”. In: arXiv preprint arXiv:2306.00297 (2023).
[AL23] Zeyuan Allen-Zhu and Yuanzhi Li. “Physics of Language Models: Part 1, Context-Free
Grammar”. In: arXiv preprint arXiv:2305.13673 (2023).
[Alt+23] Awni Altabaa et al. “Abstractors: Transformer Modules for Symbolic Message Passing
and Relational Reasoning”. In: arXiv preprint arXiv:2304.00195 (2023).
[Bro+20] Tom Brown et al. “Language models are few-shot learners”. In: Advances in neural
information processing systems 33 (2020), pp. 1877–1901.
[CB18] Lenaic Chizat and Francis Bach. “On the global convergence of gradient descent for
over-parameterized models using optimal transport”. In: Advances in neural informa-
tion processing systems 31 (2018).
[CIS21] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. “The neural data router: Adap-
tive control flow in transformers improves systematic generalization”. In: arXiv preprint
arXiv:2110.07732 (2021).
[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. “On lazy training in differentiable
programming”. In: Advances in neural information processing systems 32 (2019).
[Cyb89] George Cybenko. “Approximation by superpositions of a sigmoidal function”. In: Math-
ematics of control, signals and systems 2.4 (1989), pp. 303–314.
[Dav+23] Xander Davies et al. “Discovering variable binding circuitry with desiderata”. In: arXiv
preprint arXiv:2307.03637 (2023).
[Ede+22] Benjamin L Edelman et al. “Inductive biases and variable creation in self-attention
mechanisms”.In: InternationalConference onMachine Learning .PMLR.2022,pp.5793–
5831.
[Fod75] Jerry A Fodor. The language of thought . Vol. 5. Harvard university press, 1975.
[Gar+22] Shivam Garg et al. “What can transformers learn in-context? a case study of simple
function classes”. In: Advances in Neural Information Processing Systems 35 (2022),
pp. 30583–30598.
[Gei+23] Atticus Geiger et al. “Relational reasoning and generalization using nonsymbolic neural
networks.” In: Psychological Review 130.2 (2023), p. 308.
[He+16] Kaiming He et al. “Deep residual learning for image recognition”. In: Proceedings of the
IEEE conference on computer vision and pattern recognition . 2016, pp. 770–778.
[Hol12] Keith J Holyoak. “Analogy and relational reasoning”. In: The Oxford handbook of think-
ing and reasoning (2012), pp. 234–259.
[Hup+20] Dieuwke Hupkes et al. “Compositionality decomposed: How do neural networks gener-
alise?” In: Journal of Artificial Intelligence Research 67 (2020), pp. 757–795.
[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler. “Neural tangent kernel: Con-
vergence and generalization in neural networks”. In: Advances in neural information
processing systems 31 (2018).
[Kap+20] Jared Kaplan et al. “Scaling laws for neural language models”. In: arXiv preprint
arXiv:2001.08361 (2020).
12

--- PAGE 13 ---
[Ker+22] Giancarlo Kerg et al. “On neural architecture inductive biases for relational tasks”. In:
arXiv preprint arXiv:2206.05056 (2022).
[KP02] Steven G Krantz and Harold R Parks. A primer of real analytic functions . Springer
Science & Business Media, 2002.
[Kri+13] Trenton Kriete et al. “Indirection and symbol-like processing in the prefrontal cortex
and basal ganglia”. In: Proceedings of the National Academy of Sciences 110.41 (2013),
pp. 16390–16395.
[KRS18] Junkyung Kim, Matthew Ricci, and Thomas Serre. “Not-So-CLEVR: learning same–
different relations strains feedforward neural networks”. In: Interface focus 8.4 (2018),
p. 20180011.
[LZA20] Zhiyuan Li, Yi Zhang, and Sanjeev Arora. “Why are convolutional nets more sample-
efficient than fully-connected nets?” In: arXiv preprint arXiv:2010.08515 (2020).
[Mar98] Gary F Marcus. “Rethinking eliminative connectionism”. In: Cognitive psychology 37.3
(1998), pp. 243–282.
[McK+23] Ian R McKenzie et al. “Inverse Scaling: When Bigger Isn’t Better”. In: arXiv preprint
arXiv:2306.09479 (2023).
[Mit20] Boris Samuilovich Mityagin. “The zero set of a real analytic function”. In: Mathematical
Notes107.3-4 (2020), pp. 529–530.
[MK16] Antone Martinho III and Alex Kacelnik. “Ducklings imprint on the relational concept
of “same or different””. In: Science353.6296 (2016), pp. 286–288.
[MMM19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. “Mean-field theory of two-
layers neural networks: dimension-free bounds and kernel limit”. In: Conference on
Learning Theory . PMLR. 2019, pp. 2388–2464.
[MMN18] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. “A mean field view of the
landscape of two-layer neural networks”. In: Proceedings of the National Academy of
Sciences 115.33 (2018), E7665–E7671.
[Nan+23] Neel Nanda et al. “Progress measures for grokking via mechanistic interpretability”. In:
arXiv preprint arXiv:2301.05217 (2023).
[New80] Allen Newell. “Physical symbol systems”. In: Cognitive science 4.2 (1980), pp. 135–183.
[Ng04] Andrew Y Ng. “Feature selection, L1 vs. L2 regularization, and rotational invariance”.
In:Proceedings of the twenty-first international conference on Machine learning . 2004,
p. 78.
[Ols+22] Catherine Olsson et al. “In-context learning and induction heads”. In: arXiv preprint
arXiv:2209.11895 (2022).
[PPW18] Rasmus Palm, Ulrich Paquet, and Ole Winther. “Recurrent relational networks”. In:
Advances in neural information processing systems 31 (2018).
[Rav38] John C Raven. “Progressive matrices: A perceptual test of intelligence”. In: London:
HK Lewis 19 (1938), p. 20.
[RV18] Grant Rotskoff and Eric Vanden-Eijnden. “Parameters as interacting particles: long
time convergence and asymptotic error scaling of neural networks”. In: Advances in
neural information processing systems 31 (2018).
13

--- PAGE 14 ---
[San+17] Adam Santoro et al. “A simple neural network module for relational reasoning”. In:
Advances in neural information processing systems 30 (2017).
[San+18] Adam Santoro et al. “Relational recurrent neural networks”. In: Advances in neural
information processing systems 31 (2018).
[Sha+20] Murray Shanahan et al. “An explicitly relational neural network architecture”. In: In-
ternational Conference on Machine Learning . PMLR. 2020, pp. 8593–8603.
[Sha18] Ohad Shamir. “Distribution-specific hardness of learning neural networks”. In: The
Journal of Machine Learning Research 19.1 (2018), pp. 1135–1163.
[SKM84] Richard E Snow, Patrick C Kyllonen, and Brachia Marshalek. “The topography of
ability and learning correlations”. In: Advances in the psychology of human intelligence
(1984), pp. 47–103.
[SS22] Justin Sirignano and Konstantinos Spiliopoulos. “Mean field analysis of deep neural
networks”. In: Mathematics of Operations Research 47.1 (2022), pp. 120–152.
[TK23] Asher Trockman and J Zico Kolter. “Mimetic Initialization of Self-Attention Layers”.
In:arXiv preprint arXiv:2305.09828 (2023).
[Vas+17] Ashish Vaswani et al. “Attention is all you need”. In: Advances in neural information
processing systems 30 (2017).
[Web+20] Taylor Webb et al. “Learning representations that support extrapolation”. In: Interna-
tional conference on machine learning . PMLR. 2020, pp. 10136–10146.
[Web+23] Taylor W Webb et al. “The Relational Bottleneck as an Inductive Bias for Efficient
Abstraction”. In: arXiv preprint arXiv:2309.06629 (2023).
[Wel13] Max Welling. “Kernel ridge regression”. In: Max Welling’s classnotes in machine learn-
ing(2013), pp. 1–3.
[WHL23] Taylor Webb, Keith J Holyoak, and Hongjing Lu. “Emergent analogical reasoning in
large language models”. In: Nature Human Behaviour (2023), pp. 1–16.
[WSC20] Taylor W Webb, Ishan Sinha, and Jonathan D Cohen. “Emergent symbols through
binding in external memory”. In: arXiv preprint arXiv:2012.14601 (2020).
[YH21] Greg Yang and Edward J Hu. “Tensor programs iv: Feature learning in infinite-width
neural networks”. In: International Conference on Machine Learning . PMLR. 2021,
pp. 11727–11737.
[Yua+23] Zheng Yuan et al. “Scaling relationship on learning mathematical reasoning with large
language models”. In: arXiv preprint arXiv:2308.01825 (2023).
[ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. “Trained Transformers Learn Linear
Models In-Context”. In: arXiv preprint arXiv:2306.09927 (2023).
[Zha+21a] Chiyuan Zhang et al. “Pointer value retrieval: A new benchmark for understanding the
limits of neural network generalization”. In: arXiv preprint arXiv:2107.12580 (2021).
[Zha+21b] Chiyuan Zhang et al. “Understanding deep learning (still) requires rethinking general-
ization”. In: Communications of the ACM 64.3 (2021), pp. 107–115.
[Zha+22] Yi Zhang et al. “Unveiling transformers with lego: a synthetic reasoning task”. In: arXiv
preprint arXiv:2206.04301 (2022).
[Zha+23] Haoyu Zhao et al. “Do Transformers Parse while Predicting the Masked Word?” In:
arXiv preprint arXiv:2303.08117 (2023).
14

--- PAGE 15 ---
Contents
1 Introduction 1
1.1 Capturing relational reasoning with template tasks . . . . . . . . . . . . . . . . . . . 1
1.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Related literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Formal definition of template tasks 5
3 Analysis for template tasks in the regression setting 6
3.1 Transformer random features kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Transformers generalize on unseen symbols . . . . . . . . . . . . . . . . . . . . . . . . 7
3.3 Improving transformer data-efficiency with WKWT
Q+aIparametrization . . . . . . . 9
4 Analysis for template tasks in next-token-prediction setting 9
5 Experiments 10
6 Discussion 11
A Details for figures in main text 16
B Additional experiments 17
B.1 Effect of transformer hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 Effect of complexity of task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.3 Effect of inductive bias of model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C Proof of Theorem 3.4 27
C.1 Part 1. General sufficient condition for good test loss . . . . . . . . . . . . . . . . . . 27
C.2 Part 2. Analyzing the transformer random features kernel . . . . . . . . . . . . . . . 28
C.3 Concluding the proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D Sufficient condition for kernel method to generalize on unseen symbols (Proof of
Lemma C.3) 29
D.1 Deferred proofs of claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.2 Remark: explicit dependence on ∥N−1∥. . . . . . . . . . . . . . . . . . . . . . . . . 34
E Nonsingularity of random features after MLP layer (Proof of Lemma 3.6) 34
F Analysis of attention layer features (Proof of Lemma 3.7) 36
F.1 Low-order derivatives of attention kernel . . . . . . . . . . . . . . . . . . . . . . . . . 36
F.2 Simplifying terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2.1 Assuming [1TX]R= [1TY]R. . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.2 Assuming [X][k]×R= [Y][k]×R. . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.3 Assuming 1TXXT1 = 1TY YT1. . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2.4 Assuming 1TXXT= 1TY YT. . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.3 Proof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
15

--- PAGE 16 ---
G Analyticity of attention kernel (technical result) 41
G.1 Technical lemmas for quantifying power series convergence . . . . . . . . . . . . . . . 42
G.2 Application of technical lemmas to attention kernel . . . . . . . . . . . . . . . . . . . 43
H Derivation of transformer kernel 45
H.1 Transformer architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
H.2 Random features kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
H.3 Informal derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
I MLPs fail to generalize on unseen symbols 48
J Deferred details for next-token-prediction template tasks 51
J.1 Definition of next-token-prediction template tasks . . . . . . . . . . . . . . . . . . . . 51
J.2 Failure of transformers to copy and modification that succeeds . . . . . . . . . . . . . 51
A Details for figures in main text
Code is available at https://github.com/eboix/relational-reasoning/ .
Psychometric tasks We describe how the tasks in Figure 1 fall under the template framework.
•(a) Distribution of 3 . The task is to complete the bottom row so that the set of elements is the
same as in the top row (answer: 2). To input this task into a language model, a token is used
to represent each symbol. The example in the figure matches template “ αβγ γα □ϵαβγ”, with
label +2. There are other templates for this task, corresponding to different arrangements of
the objects, such as “ αβγ βγ □αγϵβ” with label +1, and “ αβγ γβ □ϵβαγ” with label +3. In
total there are 144 templates, since the first 3 elements of the template are always αβγ, and
then there are 6 choices for the permutation in the next row, and finally 24 choices for the
permutation in the final row.
•(b) Relational match-to-sample . The task is to match the first row to one of two alternative
patterns (answer: 1). Again, a token is used to represent each symbol. The example in the
figure matches “ αββ γδδ ϵϵτ ” with label +1. A simple combinatorial calculation gives a total
of 40 templates (5 possible patterns in the first row, times 2 choices for whether the first
option or the second option is correct, times 4 choices for the pattern of alternative option).
•(c) Raven’s progressive matrices . A standard Raven’s progressive matrices task [Rav38] (an-
swer: three dark circles). For each of the dimensions of shape, number, and color, we have a
“distribution of 3” task with a symbolic label. For example, for the shapes in the figure, the
task is “ αβγ βγα γβ ?” with label α. Since another possibility is for each row to be constant
(as in, e.g., the case of numbers), another possible template is “ ααα βββ γγ ?” with label γ,
and so there is a total of 36+1 = 37 possible templates per dimension. This discussion assumes
that the only patterns in the progressive matrices are distribution of 3, and constant. If pro-
gressions are also allowed as in [WHL23], these can be incorporated by adding corresponding
templates.
16

--- PAGE 17 ---
Transformer performance In all experiments, standard transformer architectures are used. In
Figure 2, The architecture is a 2-layer transformer with 16 heads per layer, embedding dimension
128, head dimension 64, MLP dimension 256, trained with Adam with learning rate 1e-3 and batch-
size 1024. The ntraining samples are chosen by picking the variable names at random from an
alphabet of ntokens. The test set is the same two programs but with disjoint variable names. The
reported error bars are on average over 5 trials. The learning rate for each curve is picked as the one
achieving best generalization in {10−5,10−4,10−3,10−2}. In Figure 3, the setting is the same except
that the transformer is 4-layer transformer and has embedding dimension 512. In Figure 5 the same
hyperparameters as in Figure 2 are used. In order to measure the generalization performance of
the learned model on unseen symbols, we evaluate it on a test set and a validation set which each
consist of 100 samples drawn in the same way as the training dataset, but each using a disjoint
alphabet of size 100. Therefore, there is no overlap in the support of the train, test, and validation
distributions. We use the validation loss to select the best epoch of training out of 1000 epochs. We
report the test loss on this saved model.
B Additional experiments
We report extensive additional experiments probing the template task framework. In each of these,
the training dataset consists of nrandom training samples. Each sample is drawn according to a
template distribution. The following are template tasks on which we test.
•αβαvs.αββtask. Uniform on two templates αβαandαββwith labels 1, -1 respectively and
αandβare wildcards.
•αβαβvs.ααββtask. Same as above, except with templates αβαβandααββ.
•Length- kmajority task . Uniformon 2k−1templates α×{α, β}k−1where αandβarewildcards.
A template zhas label 1 if its first token occurs in the majority of the rest of the string, and
-1 otherwise. Namely, f∗(z) =(
1,|{i:z1=zi}|>(k+ 1)/2
−1,otherwise.
•Random template task . A certain number rof templates are drawn uniformly from (W ∪X )k,
conditioned on being pairwise distinct. The task is the uniform distribution over these r
templates, with random Gaussian labels centered and scaled so that the trivial MSE is 1.
For any of these tasks, we generate ntraining samples as follows. We substitute the wildcards for
regular tokens using a randomly chosen injective function s:W → X where Xis an alphabet of size
n(which is the same size as the number of samples). For example, if a given sample is generated
from template αβαwith substitution map smapping s(A) = 12,s(B) = 5, then the sample will be
[12,5,12]. Error bars are over 5 trials, unless otherwise noted.
B.1 Effect of transformer hyperparameters
We test a standard transformer architecture on the αβαvs.αββtask, varying some of the hyper-
parameters of the transformer to isolate their effect while keeping all other hyperparameters fixed.
The base hyperparameters are depth 2, embedding dimension 128, head dimension 64, number of
heads per layer 16, trained with Adam with minibatch size 1024 for 1000 epochs. Our experiments
are as follows:
•Learning rate and n. In Figure 8 we vary the learning rate and n.
17

--- PAGE 18 ---
•Learning rate and depth . In Figure 9 and Figure 10, we vary the learning rate and the depth,
forn= 512andn= 1024, respectively.
•Learning rate and number of heads . In Figure 11 and 12, we vary the learning rate and number
of heads, for n= 512andn= 1024, respectively.
•Learning rate and embedding dimension . InFigure13wevarythelearningrateandembedding
dimension for n= 1024.
•Learning rate and batch size . In Figure 14, we vary the learning rate and batch-size for
n= 512. In Figure 16 we vary the batch-size and nfor learning rate 0.001.
•Training just the last layer . In Figure 15, we train just the last layer, and see that the network
does learn to generalize out of distribution, as predicted by our theory. However, the number
of samples and number of epochs needed is larger than when all parameters are trained. We
train for 10000 epochs and have 64 heads per layer in this experiment.
B.2 Effect of complexity of task
We test an out-of-the-box transformer architecture with depth 2, embedding dimension 128, head
dimension 64, number of heads 16, trained with Adam with batch-size 1024 for 1000 epochs, on
various template tasks.
•Comparing difficulty of various tasks . Figure 17 we plot the performance on various simple
tasks.
•Random tasks . InFigures18,19, 20, and21, wetestonrandomtemplatetasks, andinvestigate
the effects of template length, wildcard alphabet size, regular token alphabet size, number of
templates.
B.3 Effect of inductive bias of model
We provide experiments probing the effect of the inductive bias of the model:
•Different architectures . In Figure 22, we plot the test loss for different architectures on the
αβαvs.αββtemplate task, including transformers with trainable identity perturbations to
WQWT
K, toWVWT
O, to both WQWT
KandWVWT
O, or to neither. Figure 23 illustrates on the
beneficial effect of the transformer modification for the majority task with different lengths,
lowering the amount of data needed by an order of magnitude.
•Size of model . In Figure 24 we compare the test loss of fine-tuning small, medium and large
pretrained GPT-2 networks on the αβαvs.αββtemplate task.
•MLP with XXTdata augmentation vs. transformer . In Figure 25, we compare the test loss
of a transformer with the test loss of an MLP where the input data has been augmented by
concatenating vec(XXT), which is a data augmentation that improves performance under the
NTK criterion similarly to the discussion in Section 3.3 and the discussion section.
18

--- PAGE 19 ---
104
103
102
101
learning rate0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss vs. learning rate and number of samples
n 16
n 32
n 64
n 128
n 256
n 512
n 1024
n 2048
n 4096
n 8192
104
103
102
101
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochsTransformer train loss vs. learning rate and number of samples
n 16
n 32
n 64
n 128
n 256
n 512
n 1024
n 2048
n 4096
n 8192Figure 8: Learning rate versus n= number of samples = training alphabet size. Taking too large
or too small of a learning rate can hurt generalization even when the train loss is close to zero.
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and depth, at n = 512
depth 1
depth 2
depth 4
depth 8
depth 16
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and depth, at n = 512
depth 1
depth 2
depth 4
depth 8
depth 16
Figure 9: Learning rate vs. depth at n= 512. No clear relationship between depth and generaliza-
tion. Too large or too small of a learning rate can hurt generalization.
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and depth, at n = 1024
depth 1
depth 2
depth 4
depth 8
depth 16
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and depth, at n = 1024
depth 1
depth 2
depth 4
depth 8
depth 16
Figure 10: Learning rate vs. depth at n= 1024. Unlike n= 512case, in previous figure, larger
depth typically performs better.
19

--- PAGE 20 ---
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and number of heads, at n = 512
number of heads 1
number of heads 4
number of heads 16
number of heads 64
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and number of heads, at n = 512
number of heads 1
number of heads 4
number of heads 16
number of heads 64Figure 11: Learning rate vs. number of heads per layer at n= 512. More heads are better than
one head.
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and number of heads, at n = 1024
number of heads 1
number of heads 4
number of heads 16
number of heads 64
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and number of heads, at n = 1024
number of heads 1
number of heads 4
number of heads 16
number of heads 64
Figure 12: Learning rate vs. number of heads at n= 1024. More heads are better.
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and embedding dimension, at n = 1024
embed dim 32
embed dim 128
embed dim 512
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and embedding dimension, at n = 1024
embed dim 32
embed dim 128
embed dim 512
Figure 13: Learning rate vs. embedding dimension at n= 1024. Smaller embedding dimension is
generally better.
20

--- PAGE 21 ---
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss vs. learning rate and batch size, at n = 512
batch_size 32
batch_size 128
batch_size 512
104
103
102
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss vs. learning rate and batch size, at n = 512
batch_size 32
batch_size 128
batch_size 512Figure 14: Learning rate vs. batch-size at n= 512. Smaller batch size is better.
102103104
n = train alphabet size = # samples0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss when training last layer vs. init. magnitude
Initialization scale multiplier 1
Initialization scale multiplier 8
Initialization scale multiplier 64
102103104
n = train alphabet size = # samples0.000.250.500.751.001.251.501.752.00Train lossTransformer train loss when training last layer vs. init. magnitude
Initialization scale multiplier 1
Initialization scale multiplier 8
Initialization scale multiplier 64
Figure 15: Training just the final unembedding layer suffices for the transformer to generalize out
of distribution, as predicted by our theory. However, the number of samples and number of epochs
needed is larger than when all parameters of the network are trained. Understanding why training
all parameters gives better performance than training just the last layer is an interesting future
direction. We report results for 3 different magnitudes of initializing the weights of attention mech-
anism (1 times, 8 times, and 64 times the standard initialization), and find that larger initialization
helps, which we conjecture is due to the softmax being in the saturated regime, which leads to more
weight on the relational features.
21

--- PAGE 22 ---
101102103
batch size0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss vs. batch size and number of samples
n 16
n 32
n 64
n 128
n 256
n 512
n 1024
101102103
learning rate0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochsTransformer train loss vs. learning rate and number of samples, batch size 1024
n 16
n 32
n 64
n 128
n 256
n 512
n 1024Figure 16: Batch size vs. n= number of training samples = training alphabet size. Smaller batch
size is generally better, which is most visible at n= 512.
101102103104
n = train alphabet size = # samples0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss on various tasks
ABA vs. ABB, lr 0.001
AABB vs. ABAB, lr 0.001
length-2 majority, lr 0.001
length-4 majority, lr 0.001
length-6 majority, lr 0.001
length-8 majority, lr 0.001
101102103104
n = train alphabet size = # samples0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochsTransformer train loss on various tasks
ABA vs. ABB, lr 0.001
AABB vs. ABAB, lr 0.001
length-2 majority, lr 0.001
length-4 majority, lr 0.001
length-6 majority, lr 0.001
length-8 majority, lr 0.001
Figure 17: Test and train loss of transformer for various tasks. The αβαvs.αββtask consists
of two templates αβαandαββwith labels +1, -1. The ααββvs.αβαβtask has templates +1,
-1. For each k, the length- kmajority task consists of all templates in {α} × { α, β}k−1, where each
template has label 1 if αoccurs more times in the last k−1entries, and label +1 if αoccurs fewer
times in the last k−1entries. The trivial model that outputs 0 always will achieve test loss of 1.
22

--- PAGE 23 ---
2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
template length0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss on random tasks with different lengths
n 8
n 32
n 128
n 512
n 2048
n 8192
2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
template length0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochsTransformer train loss on random tasks with different lengths
n 8
n 32
n 128
n 512
n 2048
n 8192Figure18: Performanceontaskscorrespondingoftwo,distinctrandomtemplateswithtwowildcards
α, β, and with labels 1,−1, respectively. Performance degrades as the template length increases.
2 3 4 5 6 7
wildcard alphabet size0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss on random tasks with different wildcard alphabet sizes
n 8
n 32
n 128
n 256
n 512
n 1024
n 2048
n 8192
2 3 4 5 6 7
wildcard alphabet size0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss on random tasks with different wildcard alphabet sizes
n 8
n 32
n 128
n 256
n 512
n 1024
n 2048
n 8192
Figure 19: Performance on tasks corresponding of two random templates of length 5, labeled with
1,−1, respectively. Each template is sampled randomly from W5, conditioned on the two templates
being distinct. We vary the wildcard alphabet size |W|. Performance generally degrades as the
wildcard alphabet size increases.
23

--- PAGE 24 ---
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
regular alphabet size0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss on random tasks with different regular alphabet sizes
n 8
n 32
n 128
n 256
n 512
n 1024
n 2048
n 8192
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
regular alphabet size0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss on random tasks with different regular alphabet sizes
n 8
n 32
n 128
n 256
n 512
n 1024
n 2048
n 8192Figure 20: Performance on tasks corresponding of two random templates of length 5, labeled with
1,−1, respectively. Each template is sampled randomly from (W ∪ X )5, conditioned on the two
templates being distinct. We keep |W|= 2and vary the regular token alphabet size |X|between 0
and 2. Performance quickly improves as the regular token alphabet size increases.
2 4 6 8 10 12 14
number of templates0.000.250.500.751.001.251.501.752.00T est loss
Transformer test loss on random tasks with different numbers of templates
n 8
n 32
n 64
n 128
n 256
n 512
n 1024
n 2048
n 4096
n 8192
2 4 6 8 10 12 14
number of templates0.000.250.500.751.001.251.501.752.00Best train loss in 1000 epochs
Transformer train loss on random tasks with different numbers of templates
n 8
n 32
n 64
n 128
n 256
n 512
n 1024
n 2048
n 4096
n 8192
Figure 21: Performance on tasks corresponding of two random templates of length 5, labeled with
1,−1, respectively. Each template is sampled randomly from (W ∪ X )5, conditioned on the two
templates being distinct. We keep |W|= 2and vary the regular token alphabet size |X|between 0
and 2. Performance quickly improves as the regular token alphabet size increases.
24

--- PAGE 25 ---
101102103104
n = number of samples = train alphabet size0.000.250.500.751.001.251.501.752.00T est lossComparison of architectures, test loss
Transformer, lr 0.001
Transformer + KQ identity, lr 0.001
Transformer + VO identity, lr 0.001
Transformer + KQ, VO identity, lr 0.001
Depth-3 FCNet, lr 0.001
RNN, lr 0.001
LSTM, lr 0.001
101102103104
n = number of samples = train alphabet size0.000.250.500.751.001.251.501.752.00Train lossComparison of architectures, train loss
Transformer, lr 0.001
Transformer + KQ identity, lr 0.001
Transformer + VO identity, lr 0.001
Transformer + KQ, VO identity, lr 0.001
Depth-3 FCNet, lr 0.001
RNN, lr 0.001
LSTM, lr 0.001Figure 22: Different architectures on αβαvs.αββtask. Transformer outperforms the other archi-
tectures, especially with the reparametrization that prioritizes identities in heads.
101102103104
n = train alphabet size = #samples0.000.250.500.751.001.251.501.752.00T est lossTransformer test loss on majority tasks
majority_len=2, lr 0.001
majority_len=4, lr 0.001
majority_len=6, lr 0.001
majority_len=8, lr 0.001
101102103104
n = train alphabet size = #samples0.000.250.500.751.001.251.501.752.00T est lossTransformer + KQ, VO identity, test loss on majority tasks
majority_len=2, lr 0.001
majority_len=4, lr 0.001
majority_len=6, lr 0.001
majority_len=8, lr 0.001
Figure 23: Comparison of test loss of architectures on length- kmajority task with different k.
Left: vanilla transformer architecture. Right: transformer architecture plus the trainable identity
scalings on each attention head’s WKWT
QandWVWT
Omatrices. Notice that again the transformer
reparametrization lowers the amount of data needed by at least an order of magnitude.
101102
n = number of train samples = train alphabet size0.000.250.500.751.001.251.501.752.00T est lossGPT-2 pretrained by size, test loss
gpt2-small pretrained, lr 0.001
gpt2-medium pretrained, lr 0.0001
gpt2-large pretrained, lr 0.001
101102
n = number of train samples = train alphabet size0.000.250.500.751.001.251.501.752.00Train lossGPT-2 pretrained by size, train loss
gpt2-small pretrained, lr 0.001
gpt2-medium pretrained, lr 0.0001
gpt2-large pretrained, lr 0.001
Figure 24: Pretrained GPT-2 of different sizes fine-tuned on αβαvs.αββtask.
25

--- PAGE 26 ---
101102103104
n = number train samples = size train alphabet0.000.250.500.751.001.251.501.752.00T est lossMLP vs. MLP with XXT data augmentation, test loss
MLP, lr 0.001
MLP + XXT data augmentation, lr 0.001
101102103104
n = number train samples = size train alphabet0.000.250.500.751.001.251.501.752.00Train lossMLP vs. MLP with XXT data augmentation, train loss
MLP, lr 0.001
MLP + XXT data augmentation, lr 0.001Figure 25: Test loss of MLP with XXTdata augmentation, where it is concatenated to input,
versus MLP without data augmentation, versus transformer.
26

--- PAGE 27 ---
C Proof of Theorem 3.4
There are two main parts to the proof. First, in Section C.1 we establish a lemma with a sufficient
condition for a kernel method to have good test loss. Second, in Section C.2 we prove that the trans-
former random features kernel Ktranssatisfies this condition for almost any β, γ, b 1, b2parameters.
We conclude in Section C.3.
Remark C.1. The reason that we state our result with mean-squared error loss is that we have
the closed-form solution (5) for the function that the kernel method learns in terms of its kernel
and the data. Such an expression is not known for the cross-entropy loss.
C.1 Part 1. General sufficient condition for good test loss
We restrict ourselves to token-symmetric kernels, which are kernels whose values are unchanged if
the tokens are relabeled by a permutation.
DefinitionC.2 (Token-symmetrickernel) .Kistoken-symmetricifforanypermutation π:X → X
we have K(x,y) =K([π(x1), . . . , π (xk)],[π(y1), . . . , π (yk)]).
Token-symmetry is a mild condition, as most network architectures used in practice (includ-
ing transformers) have token-symmetric neural tangent kernels at initialization. We emphasize
that token-symmetry is not sufficient for good test loss since MLPs are a counterexample (see
Appendix I.)
Tostatethesufficientconditionforgoodtestloss,let {z1, . . . ,zr}= supp( µtmplt)bethetemplate
distribution support. Define also the set R=∪i∈[k],j∈[r]{zj,i}of tokens that appear in the templates.
Finally, define N∈Rr×rby
Nij=K(sub(zi, s),sub(zj, s′)), (11)
where s, s′:W → X are substitution maps satisfying
s(W)∩s′(W) = 0and s(W)∩ R=s′(W)∩ R=∅. (12)
Onecancheckthatbecauseofthetoken-symmetryofthekernel K, thematrix Nisuniquely-defined
regardless of the substitution maps s, s′chosen, as long as they satisfy (12).
Lemma C.3 (It suffices for Nto be nonsingular) .IfKis a token-symmetric kernel, and Nis
nonsingular, then kernel ridge regression achieves vanishing test loss.
Formally, there are constants c, C > 0and ridge regularization parameter λ >0depending only
onµtmplt,σ,|W|,∥N−1∥and∥K∥∞= max xK(x,x), such that for any xmatching a template
z∈supp( µtmplt)the kernel ridge regression estimator ˆfin(5)with kernel Ksatisfies
|ˆf(x)−f∗(z)| ≤Cr
log(1/δ)
n+Cr1
ρ,
with probability at least 1−δ−exp(−cn)over the random samples.
The proof is in Appendix D, but we develop an intuition here on why the nonsingularity of the
matrix Nis important. Let [n] =I1⊔ I2⊔ ··· ⊔ I nbe the partition of the samples such that if
i∈ Ijthen sample (xi, yi)is drawn by substituting the wildcards of template zjwith substitution
map si:W → X. We show that for any string xmatching template zj, the kernel ridge regression
27

--- PAGE 28 ---
solution (5) is approximately equal to the average of the labels of the samples corresponding to
template j,
yT(ˆK+λI)−1k(x)≈1
|Ij|X
i∈Ijyi≈f∗(zj). (13)
In order to see why this is true, consider the regime in which the sample diversity is very high, i.e.,
ρ≫1. Since ρis large, any particular token is highly unlikely to be substituted. This has the
following implications:
•For most sample pairs i̸=i′∈[n], the maps siandsi′have disjoint range: si(W)∩s′
i(W).
•For most samples i∈[n], the substituted tokens are not in the templates: si(W)∩ R=∅.
These are the same conditions as in (8). So by the token-symmetry of the kernel, for most pairs of
samples the empirical kernel matrix is given by N:
ˆKi,i′:=K(xi,xi′) =Nj,j′for most i∈ Ij, i′∈ Ij′.
So ifNis nonsingular, then ˆKhasrlarge eigenvalues, and n−rmuch smaller eigenvalues. This
turns out to be sufficient for (9) to hold. We refer the reader to Appendix D for more details.
C.2 Part 2. Analyzing the transformer random features kernel
We show that the transformer random features kernel Ktranssatisfies the sufficient condition of
Lemma C.3 for vanishing test loss. It is clear that the kernel is token-symmetric because the
definition is invariant to the permutation relabelings of the tokens. The difficult part is to show
that the matrix Ntrans:=Ndefined with kernel K=Ktransin (11) is nonsingular. The main
challenge is that the transformer kernel does not have a known closed-form solution because of the
softmax terms in its definition (4). Furthermore, the result is especially challenging to prove because
it must hold for anycollection of disjoint templates z1, . . . ,zr.
We analyze the MLP layer and the attention layer of the transformer separately. We observe
that a “weak” condition on Kattncan be lifted into the “strong” result that Ntransis nonsingular.
Intuitively, as long as Kattnis not a very degenerate kernel, it is very unlikely that the MLP layer
has the cancellations that would be needed to make Ntransnonsingular.
Lemma C.4 (Nonsingularity of Ntrans, restatement of Lemma 3.6) .Suppose for every non-identity
permutation τ∈Sr\ {id},
X
i∈[r]Kattn(sub(zi, s),sub(zi, s′))̸=X
i∈[r]Kattn(sub(zi, s),sub(zτ(i), s′)), (14)
where s, s′are the substitution maps in the definition of Ntransin(12). Let the MLP layer’s
activation function be ϕ(t) = cos( b1t+b2). Then for almost any choice of b1, b2(except for a
Lebesgue-measure-zero set), the matrix Ntransis nonsingular.
This lemma is proved in Appendix E, by explicitly evaluating the Gaussian integral, which is
possible since the activation function is the cosine function. Although in our proof we use the
cosine activation function, we conjecture that this result should morally hold for sufficiently generic
non-polynomial activation functions. Next, we prove the condition on Nattn.
28

--- PAGE 29 ---
Lemma C.5 (Non-degeneracy of Kattn, restatement of Lemma 3.7) .The condition (14)holds for
Lebesgue-almost any β, γ.
The proof is in Appendix F. First, we prove the analyticity of the kernel Kattnin terms of
the hyperparameters βandγwhich control the softmax inverse temperature and the positional
embeddings. Because of the identity theorem for analytic functions, it suffices to show at least one
choice of hyperparameters βandγsatisfies (14) for all non-identity permutations τ. Since Kattn
does not have a closed-form solution, we find such a choice of βandγby analyzing the Taylor-series
expansion of Kattnaround β= 0andγ= 0up to order-10 derivatives, which happens to suffice.
C.3 Concluding the proof of Theorem 3.4
By Lemma C.3, it suffices to prove the nonsingularity of the matrix Ntransdefined in (11) with
kernel K=Ktrans. Lemma 3.6 gives a condition for nonsingularity that holds for almost any b1, b2.
Lemma 3.7 proves this condition for almost any β, γ. Therefore, Theorem 3.4 follows.
D Sufficient condition for kernel method to generalize on unseen
symbols (Proof of Lemma C.3)
We restate and prove Lemma C.3. Let Kbe a token-symmetric kernel as in Definition C.2. Let
µtmpltbe a distribution supported on disjoint templates z1, . . . ,zrand define R=∪i∈[r],j∈[k]{zi,j}.
Recall the definiton of the matrix N∈Rr×rwith
Ni,i′=K(sub(zi, s),sub(zi′, s′)).
forsubstitutionmaps s:W → X,s′:W → X satisfying s(W)∩s′(W) =s(W)∩R=s′(W)∩R=∅.
Recall that this is well-defined by the token-symmetry of the kernel K.
Lemma D.1 (Restatement of Lemma C.3) .Suppose that Kis token-symmetric and Nis non-
singular. Then there are constants 0< c < C and0< c′< C′depending only on µtmplt,σ,|W|,
∥N−1∥and∥K∥∞= max xK(x,x)such that the following holds. Consider any regularization pa-
rameter λ∈[c′n, C′n], and any string xmatching template z∈supp( µtmplt). Then with probability
≥1−δ−exp(−cn), the kernel ridge regression estimator ˆfachieves good accuracy on x:
|ˆf(x)−f∗(z)| ≤Cr
log(1/δ)
n+Cr1
ρ.
Proof.Note that some proofs of helper claims are deferred to Section D.1. Let (x1, y1), . . . , (xn, yn)
be the samples seen by the kernel method. We know from (5) that kernel ridge regression outputs
the estimator
ˆf(x) =yT(ˆK+λI)−1v(x), (Kernel ridge regression)
where the empirical kernel matrix ˆK∈Rn×nis
ˆKi,j=K(xi,xj),
andy= [y1, . . . , y n], and v(x) = [K(x1,x), . . . , K (xn,x)]∈Rn.
29

--- PAGE 30 ---
Idealized estimator when sample diversity is high If the sample diversity is sufficiently high,
then for most pairs of samples i̸=i′∈[n], it will be the case that xiandxi′do not share any of
the wildcard substitution tokens. In other words, the wildcard substitution map used to form xi
will have disjoint range from the wildcard substitution map used to form xi′. This means that we
should expect the estimator ˆfto perform similarly to the following idealized estimator:
ˆfideal(x) =yT(ˆKideal+λI)+videal(x), (15)
where ˆKideal∈Rn×nandvideal(x)∈Rnare idealized versions of ˆKandv(x), formed below. They
correspond to the limit of infinitely-diverse samples, when all token substitution maps have disjoint
range. For each j∈[r], letIj⊆[n]be the indices of samples xiformed by substituting from
template zj. For any i∈ Ij, i′∈ Ij′, let
ˆKideal
i,i′=Nj,j′, (16)
Also, similarly define videal(x)∈Rn. For any i∈ Ij, let
videal
i(x) =K(sub(zj, s),x), (17)
where s:W → X is a substitution map with s(W)∩ R=s(W)∩ {xi}i∈[k]=∅, i.e., it does not
overlap with the templates or with xin the tokens substituted for the wildcards. The expressions
(16) and (17) are well-defined because of the token-symmetry of the kernel.
If the sample diversity is high, then we show that the idealized estimator ˆfidealis indeed close
to the kernel ridge regression solution ˆf.
Claim D.2 (Idealized estimator is good approximation to true estimator) .Suppose ∥K∥∞=
maxx|K(x,x)|<∞. Then there are constants C, c > 0depending only on |W|,∥K∥∞, k, rsuch
that the following holds. For any x, with probability at least 1−exp(−cn),
|ˆfideal(x)−ˆf(x)| ≤C
λ+Cn
λ√ρ,
where ρis defined in Definition 3.3 and measures the diversity of the substitution map distribution.
Analyzing the idealized estimator using its block structure The matrix ˆKidealhas block
structure with blocks I1, . . . ,Ir. Namely, it equals ˆKi,i′=Nj,j′for all i∈ I j, i′∈ I j′. Simi-
larly,videal(x)also has block structure with blocks I1, . . . ,Ir. This structure allows us to analyze
estimator ˆfidealand to prove its accuracy.
In order to analyze the estimator, we prove the following technical claim. The interpretation of
this claim is that if xmatches template za, then videal(x)is equal to any of the rows in ˆKidealthat
correspond to template a. In other words, we should have (ˆKideal)+videal(x) =1Ia/|Ia|, which is
the indicator vector for samples that come from template a. The following technical claim is a more
robust version of this observation.
ClaimD.3. Letxbe a stringthat matches template za. Suppose that 0< λ < τ := min j∈[r]|Ij|/∥N−1∥.
Then (ˆKideal+λI)is invertible and the following are satisfied
∥(ˆKideal+λI)−1videal(x)∥ ≤s
1
|Ia|(τ
τ−λ),
30

--- PAGE 31 ---
and, letting 1Ia∈Rnbe the indicator vector for set Ia,
∥1Ia
|Ia|−(ˆKideal+λI)−1videal(x)∥ ≤s
1
|Ia|(τ
τ−λ−1).
Using the above technical claim, we can prove that ˆfidealis an accurate estimator. The insight
is that since (ˆKideal+λI)−1videal(x)is approximately the indicator vector 1Ia/|Ia|for samples
corresponding to template a, the output of the idealized estimator is the average of the labels for
samples corresponding to template a.
Claim D.4 (Idealized estimator gets vanishing test loss on unseen symbols) .There are c, C > 0
depending only on |W|, µtmplt, σ,∥K∥∞such that the following holds for any 0< λ < cn/ ∥N−1∥.
Letxbe any string that matches template z∈supp( µtmplt). Then, for any δ >0, with probability
≥1−δ−exp(−cn)over the random samples, the idealized estimator has error upper-bounded by
|ˆfideal(x)−f∗(z)| ≤Cr
log(1/δ)
n.
Proof of Claim D.4. LetE1be the event that |Ij| ≥nµtmplt(zj)/2for all j∈[r], i.e., all templates
are well-represented in the dataset. By a Hoeffding bound,
P[E1]≥1−exp(−cn).
Suppose that xmatches template za. By Claim D.3, under event E1, there is a constant C >0
such that
|ˆfideal(x)−f∗(za)|=|yT(ˆKideal+λI)−1videal(x)−f∗(za)|
≤ |yT1Ia
|Ia|−f∗(za)|+s
1
|Ia|(τ
τ−λ−1)
≤ |yT1Ia
|Ia|−f∗(za)|+Cr
1
n.
We conclude since P[|yT1Ia
|Ia|−f∗(za)|> Cq
log(1 /δ)
n|E1]≤δby a tail bound for Gaussians.
Putting the elements together to conclude the proof of the lemma Combined, Claims D.2
and D.4 imply the lemma if we take λ= Θ( n), then we obtain error O(p
log(1/δ)/n+p
1/ρ)with
probability at least 1−δ−exp(−Ω(n)).
D.1 Deferred proofs of claims
Proof of Claim D.3. Letw1, . . . ,wnbe an orthogonal basis of eigenvectors for ˆKidealwith eigenval-
uesν1, . . . , ν n. Notice that these are also eigenvectors of ˆKideal+λI. Because of the block structure
ofˆKideal, its eigenvectors and eigenvalues have a simple form. Define
M= diag([p
|I1|, . . . ,p
|Ir|])Ndiag([p
|I1|, . . . ,p
|Ir|]).
The nonzero eigenvalues of ˆKidealcorrespond to the nonzero eigenvalues of M, because for any
eigenvector u∈RrofMthere is a corresponding eigenvector of ˆKidealwith the same eigenvalue
31

--- PAGE 32 ---
by letting each of the blocks Ijconsist of copies of the entry uj/p
|Ij|. Therefore, all nonzero
eigenvalues of ˆK−1have magnitude at least
|ν1|, . . . ,|νn| ≥1/∥M−1∥ ≥min
j∈[r]|Ij|/∥N−1∥=τ > λ.
SoˆKideal+λIis invertible, which is the first part of the claim. Write1Ia
|Ia|in the eigenbasis as
1Ia
|Ia|=X
iciwi,
for some coefficients ci. By construction,
videal(x) =ˆKideal1Ia
|Ia|=X
iνiciwi,
so
∥(ˆKideal+λI)−1videal(x)∥2=∥X
iνi
νi+λciwi∥2=X
i(νi
νi+λ)2c2
i
≤max
i(νi
νi+λ)21
|Ia|≤max
i(τ
τ−λ)2.
Similarly,
∥1Ia
|Ia|−(ˆKideal+λI)−1videal(x)∥2=∥X
i(1−νi
νi+λ)ciwi∥2=X
i(1−νi
νi+λ)2c2
i
≤max
i(1−νi
νi+λ)21
|Ia|≤max
i(1−τ
τ−λ)2.
Claim D.5 (Bound on difference between kernel regressions) .Suppose that ˆKis p.s.d and that
(ˆKideal+λI)−1videal(x)is well-defined. Then, for any λ >0,
|ˆfideal(x)−ˆf(x)| ≤∥y∥
λ(∥videal(x)−v(x)∥+∥ˆK−ˆKideal∥∥(ˆKideal+λI)−1videal(x)∥)
Proof of Claim D.5. By triangle inequality,
|ˆf(x)−ˆfideal(x)|=∥yT(ˆK+λI)−1v(x)−yT(ˆKideal+λI)−1videal(x)∥
(a)
≤ ∥y∥ · ∥(ˆK+λI)−1v(x)−(ˆK+λI)−1videal(x)∥| {z }
Term 1
+∥y∥ · ∥(ˆK+λI)−1videal(x)−(ˆKideal+λI)−1videal(x)∥| {z }
Term 2
The first term can be upper-bounded because ∥(ˆK+λI)−1∥ ≤ ∥ (λI)−1∥= 1/λ, so
Term 1 ≤∥videal(x)−v(x)∥
λ
32

--- PAGE 33 ---
The second term can be upper-bounded by
Term 2 =∥(ˆK+λI)−1((ˆK+λI)(ˆKideal+λI)−1−(ˆKideal+λI)(ˆKideal+λI)−1)videal(x)∥
=∥(ˆK+λI)−1(ˆK−ˆKideal)(ˆKideal+λI)−1videal(x)∥
≤1
λ∥ˆK−ˆKideal∥∥(ˆKideal+λI)−1videal(x)∥.
Proof of Claim D.2. LetE1be the event that |Ij| ≥nµtmplt(zj)for all j∈[r]. By Hoeffding, there
is a constant c >0such that P[E1]≥1−exp(−cn). By Claim D.3, under event E1, there is a
constant C >0such that
∥(ˆKideal+λI)−1videal(x)∥ ≤C√n. (18)
Next, recall the parameter ρused to measure the spread of the substitution map distributions
{µsub,z}z∈supp( µtmplt), as defined in (3.3). For each i∈[n], letsi:W → X be the substitution map
used to generate the sample xi. Let P1be the number of samples (i, i′)such that their substitution
maps overlap, or have range that overlaps with the regular tokens in the templates. Formally:
P1=|{1≤i < i′≤n:si(W)∩si′(W)̸=∅orsi(W)∩ R ̸=∅orsi′(W)∩ R ̸=∅}|.
Similarly, let P2be the number of samples that (i, i′)such that their substitution maps overlap with
that used to generate x, or they overlap with the regular tokens in the templates:
P2=|{1≤i≤n:si(W)∩ R ̸=∅orsi(W)∩ {xj}j∈[k]̸=∅}|.
By the definition of ρ, we can upper-bound the expected number of “bad” pairs P1and “bad” indices
P2by:
E[P1]≤
X
i,i′∈[n]X
w,w′∈WP[si(w) =si′(w′)]
+nX
i∈[n]X
t∈RP[t∈si(W)]≤Cn2
ρ+Cn
ρ≤Cn2
ρ
E[P2]≤X
i∈[n]X
t∈{xj}j∈[k]∪RP[t∈si(W)]≤Cn
ρ.
By Hoeffding’s inequality, the event E2that P1≤Cn2
ρandP2≤Cn
ρoccurs with probability
≥1−exp(−cn). Under event E2,
∥ˆK−ˆKideal∥ ≤C+Cn/√ρand ∥v(x)−videal(x)∥ ≤Cp
n/ρ . (19)
By Claim D.5 and (18) and (19), under events E1, E2, and using that ∥y∥ ≤C√n, we have
|ˆfideal(x)−ˆf(x)| ≤C√n
λ(Cp
n/ρ+ (C+Cn/√ρ)C√n)≤C
λ+Cn
λ√ρ.
33

--- PAGE 34 ---
D.2 Remark: explicit dependence on ∥N−1∥
In the case that ρ=∞, let us obtain explicit dependence on ∥N−1∥in the bound of Lemma D.1.
Lemma D.6. Suppose that Kis token-symmetric and Nis nonsingular. Suppose also that ρ=
∞. Then there are constants 0< c < C and0< c′< C′depending only on µtmplt,σ,|W|,
and∥K∥∞= max xK(x,x)such that the following holds. Consider any regularization parameter
λ∈[c′n/∥N−1∥, C′n/∥N−1∥], and any string xmatching template z∈supp( µtmplt). Then with
probability ≥1−δ−exp(−cn), the kernel ridge regression estimator ˆfachieves good accuracy on
x:
|ˆf(x)−f∗(z)| ≤Cr
log(1/δ)
n+C∥N−1∥
n.
Proof.First, byClaimD.2, wehave |ˆfideal(x)−ˆf(x)| ≤C
λ. Next, byClaimD.4, wehave |ˆfideal(x)−
f∗(z)| ≤Cq
log(1 /δ)
n.
E Nonsingularity of random features after MLP layer (Proof of
Lemma 3.6)
Consider a kernel K2formed from a kernel K1as follows:
K2(x,y) =Eu,v∼Σ1(x,y)[ϕ(u)ϕ(v)],Σ1(x,y) =K1(x,x)K1(x,y)
K1(x,y)K1(y,y)
.
Here ϕ:R→Ris a nonlinear activation function. Such a random features kernel arises in a
neural network architecture by appending an infinite-width MLP layer with Gaussian initialization
to a neural network with random features with kernel K1.
We wish to prove that a certain matrix N∈Rr×rgiven by
Nij=K2(xi,yj), (20)
is nonsingular, where x1, . . . ,xr,y1, . . . ,yrare inputs. The intuition is that if ϕis a “generic”
activation function, then only a weak condition on K1is required for the matrix Nto be invertible.
We provide a general lemma that allows us to guarantee the invertibility if the activation function is
a shifted cosine, although we conjecture such a result to be true for most non-polynomial activation
functions ϕ. This is a generalization of Lemma 3.6, so it implies Lemma 3.6.
Lemma E.1 (Criterion for invertibility of N).Consider the matrix N∈Rr×rdefined in (20)where
x1, . . . ,xrandy1, . . . ,yrare inputs. Suppose that for all nontrivial permutations τ∈Sr\ {id}we
have
X
i∈[r]K1(xi,yi)̸=X
i∈[r]K1(xi,yτ(i))). (21)
Suppose also that the MLP activation function is ϕ(t) = cos( kt+c)for two hyperparameters k,c.
Then, Nis nonsingular for all (k, c)∈R2except for a Lebesgue-measure-zero subset of R2.
34

--- PAGE 35 ---
Proof.Letf(k, c) := det( N). We wish to show that {(k, c) :f(k, c) = 0 }is a measure-zero
set. By Claim E.2, is an analytic function of candk, and by the identity theorem for analytic
functions [Mit20], it suffices to show that f̸≡0. Fixing c=π/4, by Claim E.2,
K2(x,y) =1
2exp(−k2
2(K1(x,x) +K1(y,y)−2K1(x,y))).
Therefore
f(k, π/4) =X
τ∈Srsgn(τ)Y
i∈[r]K2(xi,yτ(i))
=e−k2
2(P
i∈[r]K1(xi,xi)+K1(yi,yi))X
τ∈Srsgn(τ) exp( k2X
i∈[r]K1(xi,yτ(i))).
It remains to prove that as a function of kwe have
X
τ∈Srsgn(τ) exp( k2X
i∈[r]K1(xi,yτ(i)))̸≡0,
This holds because for any distinct c1, . . . , c lthe functions exp(c1t), . . . , exp(clt)are linearly inde-
pendent functions of t, since their Wronskian is a rescaled Vandermonde determinant
exp(c1t) . . . exp(clt)
d
dtexp(c1t). . .d
dtexp(clt)
......
dl−1
dtl−1exp(c1t). . .dl−1
dtl−1exp(clt)= exp(lX
i=1cit)1. . . 1
c1. . . c l
......
cl−1
1 . . . cl−1
l
= exp(lX
i=1cit)Y
1≤i<j≤l(cj−ci)̸≡0
Below is the technical claim used in the proof of the lemma.
Claim E.2. LetU, V∼N(0,a ρ
ρ b
). Then for any k, c∈R,
E[cos(kU+c) cos( kV+c)] =1
2e−1
2k2(a+b)(e−k2ρcos(2 c) +ek2ρ).
Proof.By Mathematica, we have the following Gaussian integrals
E[eikU+ikV] =E[e−ikU−ikV] =e−1
2k2(a+b+2ρ),
E[eikU−ikV] =E[e−ikU+ikV] =e−1
2k2(a+b−2ρ).
Since cos(kt+c) = (eikt+ic+e−ikt−ic)/2,
E[cos(kU+c) cos( kV+c)] =1
4E[(eikU+ic+e−ikU−ic)(eikV+ic+e−ikV−ic)]
=1
4(e−1
2k2(a+b+2ρ)(e2ic+e−2ic) + 2e−1
2k2(a+b−2ρ))
=1
2e−1
2k2(a+b)(e−k2ρcos(2 c) +ek2ρ).
35

--- PAGE 36 ---
F Analysis of attention layer features (Proof of Lemma 3.7)
For any inputs X, Y, we write the kernel of the random features of the attention layer as
Kattn(X, Y) =Em(X),m(Y)[smax( βm(X))T(XYT+γ2I)smax( βm(Y))]
m(X),m(Y)∼N(0,XXT+γ2I XYT+γ2I
Y XT+γ2I Y YT+γ2I
),
as stated Section 3.1; see also Section H for the derivation of this kernel in the infinite-width limit of
the transformer architecture. For shorthand, we write κX,Y(β, γ) =Kattn(X,Y)to emphasize the
attention kernel’s dependence on the hyperparameters βandγwhich control the softmax’s inverse
temperature and the weight of the positional embeddings, respectively.
We prove Lemma 3.7, which is that Kattnsatisfies the property (10) required by Lemma 3.6 for
the transformer random features kernel to succeed at the template task.
Namely, consider any disjoint templates z1, . . . ,zrand two substitution maps s, s′:W → X
•that have disjoint range: s(W)∩s′(W) =∅,
•and the substituted tokens do not overlap with any of the tokens in the templates: s(W)∩R=
s′(W)∩ R=∅where R=∪i∈[r],j∈[k]{z(i)
j}.
Then we define Xi,Yi∈Rk×mto be the strings (where we abuse notation slightly by viewing
them as matrices with one-hot rows) after substituting zibys, s′respectively:
Xi= sub( zi, s)Yi= sub( zi, s′).
Lemma F.1 (Restatement of Lemma 3.7) .Define gτ(β, γ) =P
i∈[r]κXi,Yτ(i)(β, γ). Then for all
but a Lebesgue-measure-zero set of (β, γ)∈R2we have gid(β, γ)̸=gτ(β, γ)for all permutations
τ̸= id.
No closed-form expression is known for κX,Y(β, γ), so our approach is to analyze its Taylor series
expansion around β=γ= 0. Our proof proceeds in stages, where, in each stage, we examine a
higher derivative and progressively narrow the set of τthat might possibly have gτ(β, γ) =gid(β, γ).
In Section F.1, we list certain low-order derivatives of κX,Y(β, γ)that will be sufficient for our
analysis. In Section F.2, we analyze some of the terms in these expressions. In Section F.3 we put
the previous lemmas together to prove Lemma F.1.
To avoid notational overload, in this section we will not use bolded notation to refer to the
matrices X,Y, but rather use the lowercase X, Y.
F.1 Low-order derivatives of attention kernel
In the following table we collect several relevant derivatives of∂i
∂βi∂j
∂γjκX,Y(0,0)fori≤6andj≤4.
For each i,jwe use c1, c2, . . .to denote constants that depend only on k, and on the derivative i, j
being computed. Certain constants that are important for the proof are provided explicitly. These
derivatives were computed using a Python script available in our code. The colors are explained in
Section F.2.
Derivative Expansion
κX,Y(0,0) = + c11TXYT1
36

--- PAGE 37 ---
∂2
∂β2∂2
∂γ2κX,Y(0,0) = + c11TXYT1 +c2tr(XYT)
∂4
∂β4κX,Y(0,0) = + c11TXYT1 +c21TXXTXYT1 +c31TXYTY YT1 +c41TXXTXXTXYT1
+c5(1TXYT1)(1TXXT1) +c61TXYTY XTXYT1 +c7(1TXYT1)(1TXYT1)
+c81TY XTXYTY YT1 +c9(1TXYT1)(1TY YT1)
+c10(1TXXTXYT1)(1TXXT1) +c11(1TXYTY YT1)(1TXXT1)
+c12(1TXYT1)(1TXXTXYT1) +c13(1TXYTY YT1)(1TXYT1)
+c14(1TXXTXYT1)(1TY YT1) +c15(1TXYTY YT1)(1TY YT1)
+c16(1TXYT1)(1TXXT1)(1TXXT1) +c17(1TXYT1)(1TXXTXXT1)
+c18(1TXYT1)(1TXYT1)(1TXXT1) +c19(1TXYT1)(1TXYT1)(1TXYT1)
+c20(1TXYT1)(1TXXT1)(1TY YT1) +c21(1TXYT1)(1TXYT1)(1TY YT1)
+c22(1TXYT1)(1TY YT1)(1TY YT1) +c23(1TXYT1)(1TY YTY YT1)
∂4
∂β4∂2
∂γ2κX,Y(0,0) = + c11TXYT1 +c2tr(XYT) +c31TXXTXYT1 +c4tr(XXTXYT)
+c51TXYTY YT1 +c6tr(XYTY YT) +c7(1TXYT1)(1TXXT1)
+c8(tr(XYT))(1TXXT1) +c9(1TXYT1)(1TXYT1) +c10(1TXYT1)(tr(XYT))
+c11(1TXYT1)(1TY YT1) +c121TXYTXYT1 +c13(tr(XYT))(1TY YT1)
+c141TY XTY YT1 +c151TXXTY XT1 +c161TXXTY YT1
+c17(1TY YT1)(1TXXT1)
∂6
∂β6∂4
∂γ4κX,Y(0,0) = + c11TXYT1 +c2tr(XYT) +c31TXXTXYT1 +c4tr(XXTXYT)
+c51TXYTY YT1 +c6tr(XYTY YT) +c7(1TXYT1)(1TXXT1)
+c8(tr(XYT))(1TXXT1) +c9(tr(XYT))(1TXYT1) +c10(1TXYT1)(1TY YT1)
+c11(1TXYT1)(1TXYT1) +c121TXYTXYT1 +c13(tr(XYT))(1TY YT1)
+c141TXXTY XT1 +c151TY XTY YT1 +c16tr(XYTXYT)
+c17(tr(XYT))(tr(XYT)) +c18+c191TXXT1 +c201TXXTXXT1
+c211TXXTY YT1 +c221TY YT1 +c23(1TXXT1)(1TXXT1)
+c24(1TY YT1)(1TXXT1) +c25tr(XXTY YT) +c261TY YTY YT1
+c27(1TY YT1)(1TY YT1)
Furthermore,
•in the expression for κX,Y(0,0)we have c1= 1/k2>0,
•in the expression for∂2
∂β2∂2
∂γ2κX,Y(0,0), we have c2= 8/k2>0,
•in the expression for∂4
∂β4κX,Y(0,0), we have c20= 24/k6>0,
•in the expression for∂4
∂β4∂2
∂γ2κX,Y(0,0), we have c16= 48/k4>0,
•and in the expression for∂6
∂β6∂4
∂γ4κX,Y(0,0), we have c25= 17280 /k4>0.
F.2 Simplifying terms
LetX∈Rk×mandY∈Rk×mbe matrices with one-hot rows (i.e., all entries are zero except for
one).
For the submatrix corresponding to rows Sand columns T, we use the notation [X]S×T∈RS×T.
Ifvis a vector, then the subvector consisting of indices Iis[v]I.
LetR ⊆ [m]be a set containing the intersection of the column support of XandY: i.e., for
alli∈[m]\ R, either [X][k]×i=0or[Y][k]×i=0. We analyze the terms in the expressions of
Section F.1 below.
37

--- PAGE 38 ---
F.2.1 Assuming [1TX]R= [1TY]R
Suppose that [1TX]R= [1TY]R. Then any of the pink terms can be written as a function of only
Xor only Y.
•1TXYT1 =∥[1TX]R∥2
•1TXXTXYT1 = 1TXdiag(1TX)YT1 = (1TX)⊙2·(1TY) =∥[1TX]R∥3
3
•1TXYTY YT1 = 1TXdiag(1TY)YT1 = (1TX)·(1TY)⊙2=∥[1TX]R∥3
3
•1TXXTXXTXYT1 = 1TXdiag(1TX)diag(1TX)YT1 =∥[1TX]R∥4
4
•1TXYTY XTXYT1 = 1TXdiag(1TY)diag(1TX)YT1 =∥[1TX]R∥4
4
•1TY XTXYTY YT1 = 1TYdiag(1TX)diag(1TY)YT1 =∥[1TX]R∥4
4
•trace( XXTXYT) = trace( Xdiag(1TX)YT) =P
i∈[k]P
v∈[m]Xiv(1TX)vYiv=P
i∈[k]P
v∈RXiv(1TX)v=
1TXdiag(1TX)1R=∥[1TX]R∥2
•trace( XYTY YT) =∥[1TY]R∥2=∥[1TX]R∥2
F.2.2 Assuming [X][k]×R= [Y][k]×R
Suppose that X[k]×R=Y[k]×R(i.e., the restriction of XandYto the Rrows is equal). Then any
of the orange terms can be written as a function of only Xor only Y.
•tr(XYT) =P
v∈[m]P
i∈[k]XivYiv=P
v∈RP
i∈[k]X2
iv= 1TX1R= 1TY1R
•1TXYTXYT1 =P
a,b,c∈[k]1(xa=yb)1(xb=yc) = 1TX[k]×R(Y[k]×R)TX[k]×R(Y[k]×R)T1
= 1TX[k]×R(X[k]×R)TX[k]×R(X[k]×R)T
•1TXXTY XT1 =P
a,b,c1(xa=xb)1(yb=xc) =P
a,b,c1(xa=xb)1(yb=xc∈ R)
=P
a,b,c1(xa=xb∈ R)1(yb=xc∈ R) =P
a,b,c1(xa=xb∈ R)1(xb=xc∈ R) =
1TX[k]×R(X[k]×R)TX[k]×R(X[k]×R)T1
•1TY XTY YT1 = 1TX[k]×R(X[k]×R)TX[k]×R(X[k]×R)T1
•trace( XYTXYT) =P
a,b1(xa=yb)1(xb=ya) =P
a,b1(xa=yb∈ R)1(xb=ya∈ R) =P
a,b1(xa=xb∈ R) = trace(( X[k]×R)(X[k]×R)T)
F.2.3 Assuming 1TXXT1 = 1TY YT1
Suppose that 1TXXT1 = 1TY YT1. Then any of the blue terms can be written as a function of
only Xor only Y.
•1TXXT1 = 1TY YT1
•1TY YT1 = 1TXXT1
38

--- PAGE 39 ---
F.2.4 Assuming 1TXXT= 1TY YT
Suppose that 1TXXT= 1TY YT. Then any of the teal terms can be written as a function of only
Xor only Y.
•1TXXTY YT1 =∥1TXXT∥2=∥1TY YT∥2
F.3 Proof of Lemma F.1
We combine the above calculations to prove Lemma F.1.
Proof.By the technical Lemma G.1, we know that gτ(β, γ)is an analytic function for each τ.
Therefore, by the identity theorem for analytic functions [Mit20], it suffices to show that for each
τ∈Sr\ {id}we have gid(β, γ)̸≡gτ(β, γ).
Stage1.Matchingregulartokendegreedistributions.
Claim F.2. Ifgid(0,0) = gτ(0,0), then [1TXi]R= [1TYτ(i)]Rfor all i∈[r].
Proof.From the table in Section F.1, there is a positive constant c1>0such that
gτ(0,0) = c1X
i∈[r]1TXiYT
τ(i)1 =c1X
i∈[r][1TXi]R[YT
τ(i)1]R
(a)
≤X
i∈[r]∥[1TXi]R∥∥[1TYτ(i)]R∥
(b)
≤sX
i∈[r]∥[1TXi]R∥2sX
i∈[r]∥[1TYτ(i)]R∥2
=X
i∈[r]∥[1TXi]R∥2,
where (a) is by Cauchy-Schwarz and holds with equality if and only if [1TXi]R∝[1TYτ(i)]Rfor all i.
Similarly (b) is by Cauchy-Schwarz and holds with equality if and only if ∥[1TXi]R∥=∥[1TYτ(i)]R∥
for all i. Notice that (a) and (b) hold with equality if τ= id, since [1TXi]R= [1TYi]Rfor all i.
Stage2.Matchingregulartokenpositions.
Claim F.3. If∂2
∂β2∂2
∂γ2gτ(0,0) =∂2
∂β2∂2
∂γ2gid(0,0)and[1TXi]R= [1TYτ(i)]Rfor all i∈[r], then we
must have [Xi][k]×R= [Yτ(i)][k]×Rfor all i∈[r].
Proof.For a constant c2>0,
∂2
∂β2∂2
∂γ2gτ(0,0) =X
i∈[r]c11TXiYT
τ(i)1 +c2trace( XiYT
τ(i))
=
c1X
i∈[r]∥[1TXi]R∥2
+
c2X
i∈[r]trace( Xi(Yτ(i))T)
,
39

--- PAGE 40 ---
by the calculation in Section F.2.1. The first sum does not depend on τ, so we analyze the second
sum. Here,
c2X
i∈[r]trace( XiYT
τ(i)) =c2X
i∈[r]X
a∈[k][XiYT
τ(i)]aa
=c2X
i∈[r]X
v∈RX
a∈[k][Xi]av[Yτ(i)]av
(a)
≤c2s
(X
i∈[r]X
v∈RX
a∈[k]([Xi]av)2)(X
i∈[r]X
v∈RX
a∈[k]([Yτ(i)]av)2
=c2X
i∈[r]1TXi1R,
where (a) is by Cauchy-Schwarz and holds with equality if and only if X(i)
av=cY(τ(i))
avfor some
constant c. We must have c= 1because of the CLS token, so (a) holds with equality if and only if
[Xi][k]×R= [Yτ(i)][k]×Rfor all i∈[r]. Specifically (a) holds with equality if τ= id.
Stage3.Matchingwildcardtokendegreehistogramnorm.
ClaimF.4. Suppose that [1TXi]R= [1TYτ(i)]R, and that∂4
∂β4gτ(0,0) =∂4
∂β4gid(0,0). Then 1TXiXT
i1 =
1TYτ(i)YT
τ(i)1for all i∈[r].
Proof.Use[1TXi]R= [1TYτ(i)]Rand the calculations in Section F.2.1 for the pink terms. Every
term of∂4
∂β4gτ(0,0)can be written as depending only on one of XiorYτ(i), with the exception of
thec20term. Namely, we have
∂4
∂β4gτ(0,0) =X
i∈[r]a(Xi) +b(Yτ(i))
+c20(1TXiYT
τ(i)1)(1TXiXT
i)(1TYτ(i)YT
τ(i)1),
for some functions a, b. Since τis a permutation, only the term with coefficient c20depends on τ.
Here, c20>0. This term corresponds to
c20X
i∈[r](1TXiYT
τ(i)1)(1TXiXT
i1)(1TYτ(i)YT
τ(i)1)
=c20X
i∈[r]∥[1TXi]R∥∥1TYτ(i)]R∥(1TXiXT
i1)(1TYτ(i)YT
τ(i)1)
(a)
≤s
(X
i∈[r]∥[1TXi]R∥2(1TXiXT
i1)2)(X
i∈[r]∥1TYτ(i)]R∥2(1TYτ(i)YT
τ(i)1)2
=X
i∈[r]∥[1TXi]R∥2(1TXiXT
i1)2
where (a) is by Cauchy-Schwarz and holds with equality if and only if ∥[1TXi]R∥21TXiXi1 =
c∥[1TYτ(i)]R∥21TYτ(i)YT
τ(i)1for all iand some constant c. This constant c= 1because the former is
apermutationofthelatterover i∈[r]. Since ∥[1TXi]R∥2=∥[1TYi]R∥2≥1byassumptionandsince
we have the CLS token, we know that (a) holds with equality if and only if 1TXiXT
i1 = 1TYτ(i)YT
τ(i)1
for all i∈[r]. This is the case for τ= idby construction of XiandYi.
40

--- PAGE 41 ---
Stage4.Matchingwildcarddegreedistributions.
Claim F.5. Suppose that [Xi][k]×R= [Yτ(i)][k]×Rand1TXiXT
i1 = 1TYτ(i)YT
τ(i)1for all i∈[r].
Suppose also that∂4
∂β4∂2
∂γ2gτ(0,0) =∂4
∂β4∂2
∂γ2gid(0,0). Then 1TXiXT
i= 1TYτ(i)YT
τ(i)for all i∈[r].
Proof.Similarly to the proof of the previous claim, because of the calculations in Sections F.2.1,
F.2.2 and F.2.3 for the pink, orange, and blue terms, respectively, we can write∂4
∂β4∂2
∂γ2as a sum of
terms that each depends on either XiorYτ(i), plusP
i∈[r]c161TXiXT
iYτ(i)YT
τ(i)1. This latter sum is
the only term that depends on τ, and the constant c16satisfies c16>0. Similarly to the previous
claim, by Cauchy-Schwarz
X
i∈[r]c161TXiXT
iYτ(i)YT
τ(i)1≤X
i∈[r]c16∥1TXiXT
i∥∥Yτ(i)YT
τ(i)1∥,
with equality if and only if 1TXiXT
i= 1TYτ(i)YT
τ(i)for all i, since {XiXT
i}iis a permutation of
{Yτ(i)YT
τ(i)}i. This condition holds for τ= id.
Stage5.Matchingwildcardpositions.
Claim F.6. Suppose that [Xi][k]×R= [Yτ(i)][k]×Rand1TXiXT
i= 1TYτ(i)YT
τ(i)for all i∈[r].
Suppose also that∂6
∂β6∂4
∂γ4gτ(0,0) =∂6
∂β6∂4
∂γ4gid(0,0). Then XiXT
i=Yτ(i)YT
τ(i)for all i∈[r].
Proof.Write∂6
∂β6∂4
∂γ4gτ(0,0)as a sum of terms each depending only on either XiorYτ(i)by using
the calculations in Sections F.2.1, F.2.3, F.2.2, and F.2.4 to handle the pink, orange, blue, and teal
terms, plus (for c25>0),
X
i∈[r]c25trace( XiXT
iYτ(i)YT
τ(i))≤X
i∈[r]c25∥XiXT
i∥F∥Yτ(i)YT
τ(i)∥F,
with equality if and only if XiXT
i=Yτ(i)YT
τ(i)for all i∈[r]. This equality holds if τ= id, concluding
the claim.
Combine the above four claims to conclude that if gτ(β, γ)≡gid(β, γ), then we have XiXT
i=
Yτ(i)YT
τ(i)and[Xi][k]×R= [Yτ(i)][k]×Rfor all i, soτ= id.
G Analyticity of attention kernel (technical result)
We prove the analyticity of κX,˜X(β, γ) =Kβ,γ
attn(X,˜X)as function of βandγ.
Lemma G.1 (Analyticity of Kattn).For any X,˜X, the function κX,˜Xis analytic in R2.
Proof.Note that we can write
m:=m(X) =Xζ+γp,˜m:=m(˜X) =˜X˜ζ+γp,
where ζ,˜ζ∼ N(0, Im)andp∼ N(0, Ik)are independent Gaussians. So we can rewrite κX,˜Xas
κX,˜X(β, γ) =Eζ,˜ζ,p[f(β, γ;ζ,˜ζ,p)],
41

--- PAGE 42 ---
where
f(β, γ;ζ,˜ζ,p) =sT(X˜XT+γ2I)˜s.
and
s= smax( βXζ+βγp)T,˜s= smax( β˜X˜ζ+βγp).
The main obstacle is to prove the technical Lemma G.9, which states that for any k1, k2, we
have
Eζ,˜ζ,p[|∂k1
∂βk1∂k2
∂γk2f(β, γ;ζ,˜ζ,p)|]≤C(1 +γ2)k1!k2!(C(|β|+|γ|)k1+k2)
So by smoothness of fand dominated convergence, we know that we can differentiate under the
integral sign, and
|dk1
dβk1dk2
dγk2κX,X′(β, γ)|=|Eζ,˜ζ,p[∂k1
∂βk1∂k2
∂γk2f(β, γ;X,˜X,ζ,˜ζ,p)]|
≤C(1 +γ2)k1!k2!(C(|β|+|γ|)k1+k2).
Because of the bound on the derivatives and its smoothness, κX,X′(β, γ)is real-analytic.
The proof of the technical bound in Lemma G.9 is developed in the subsections below.
G.1 Technical lemmas for quantifying power series convergence
In order to show that the values of the attention kernel are real-analytic functions of in terms of
β, γ, we will need to make quantitative certain facts about how real-analyticity of is preserved under
compositions, products, and sums. For this, we introduce the notion of the convergence-type of a
real-analytic function.
Definition G.2 (Quantifying power series convergence in real-analytic functions) .LetU⊆Rm
be an open set. We say that a real-analytic function f:U→Rhas(τ1, τ2)-type for functions
τ1:U→R>0andτ2:U→R>0if the following holds. For any ζ0, consider the power series of f
around ζ0,
X
µaζ0,µ(ζ−ζ0)µ.
Then for any ζsuch that ∥ζ−ζ0∥∞≤τ1(ζ0)this power series converges absolutely.
X
µs.t.|µ|≥1|aζ0,µ||ζ−ζ0|µ≤τ2(ζ0).
We provide rules for how convergence type is affected by compositions, products, and sums.
Lemma G.3 (Composition rule for type; quantitative version of Proposition 2.2.8 of [KP02]) .Let
U⊆Rmand let V⊆Rbe open. Let f1, . . . , f n:U→Vbe real-analytic with (τ1, τ2)-type, and
letg:Vn→Rbe real-analytic with (σ1, σ2)-type. Then the composition h=g◦(f1, . . . , f n)is
real-analytic with (min( τ1,(σ1◦f)·τ1
τ2), σ2◦f)-type.
42

--- PAGE 43 ---
Proof.Fix some ζ0and let y0= [f1(ζ0), . . . , f n(ζ0)], and let a(i)
ζ0,µbe the coefficients of the power
series expansion for fiaround ζ0. Define ρ= min(1 , σ1(y0)/τ2(ζ0)). Then, for any ζsuch that
∥ζ−ζ0∥∞≤ρτ1(ζ0)andi∈[n]we have
X
µs.t.|µ|≥1|a(i)
ζ0,µ||ζ−ζ0|µ≤X
µs.t.|µ|≥1|a(i)
ζ0,µ|ρ|µ|τ1(ζ0)|µ|≤ρτ2(ζ0)≤σ1(y0).
So, lettingP∞
νby0,ν(y−y0)νbe the series expansion of garound y0, we have the following absolute
convergence
∞X
ν,s.t.|ν|≥1by0,νnY
i=1X
µs.t.|µ|≥1|a(i)
ζ0,µ||ζ−ζ0|µνi
≤σ2(y0).
So we may rearrange the terms of
∞X
νby0,νnY
i=1
X
µs.t.|µ|≥1a(i)
ζ0,µ(ζ−ζ0)µ
νi
.
as we please, and we get an absolutely convergent series for g◦faround ζ0.
Lemma G.4 (Sum and product rules for type) .Letf:Rm→Randg:Rm→Rbe real-
analytic functions of (τ1, τ2)-type and (σ1, σ2)-type respectively. Then h=f+gis real-analytic of
(min( τ1, σ1), τ2+τ2)-type, and h=fgis real-analytic of (min( τ1, σ1), τ2σ2+τ2|g|+|f|σ2)-type
Proof.Both of these are straightforward from the definition.
Lemma G.5 (Derivative bound based on type) .Letf:Rm→Rbe real-analytic with (τ1, τ2)-type.
Then, for any multi-index µ,
|∂|µ|
∂ζµf(ζ0)| ≤τ2(ζ0)
τ1(ζ0)|µ|µ!
Proof.Letaζ0,µbe the coefficients of the power series of fatζ0. Since fis of(τ1, τ2)-type, we have
X
µs.t.|µ|≥1|aζ0,µ||τ1(ζ0)||µ|≤τ2(ζ0).
Since all terms in the sum are nonnegative, for all µwith|µ| ≥1,
|aζ0,µ| ≤τ2(ζ0)·(1/τ1(ζ0))|µ|.
The lemma follows by Remark 2.2.4 of [KP02], which states∂|µ|
∂ζνf(ζ0)|=|aζ0,µ|µ!.
G.2 Application of technical lemmas to attention kernel
We now use the above general technical lemmas to specifically prove that the attention kernel is
analytic in terms of βandγ.
Lemma G.6. For any j∈[m], the function f:Rm→Rgiven by f(ζ) = smax( ζ)jis real-analytic
of(1/(2e2),1)-type
43

--- PAGE 44 ---
Proof.Write f=g◦hforg:R>0→Randh:Rk→R>0given by g(y) = 1 /y, and h(ζ) =Pm
i=1eζi−ζj.
The power expansion of g(y)around y0∈R>0, is given by
g(y) =∞X
k=0(−1)k+1
yk+1
0(y−y0)k,
so one can see that gis of (ρ1, ρ2)-type for ρ1(y0) =y0/2andρ2(y0) = 1 /y0. Finally, write the
series expansion for h(ζ)around ζ0
h(ζ) = 1 + e−ζjX
i∈[m]\{j}eζi= 1 +X
i∈[m]\{j}(∞X
l=0e−ζ0,j(ζ0,j−ζj)l
l!)(∞X
k=0eζ0,i(ζi−ζ0,i)k
k!)
Note that this expansion converges absolutely for all ζ, as the absolute series is
1 +X
i∈[m]\{j}(∞X
l=0e−ζ0,j|ζ0,j−ζj|l
l!)(∞X
k=0eζ0,i|ζi−ζ0,i|k
k!)
= 1 +X
i∈[m]\{j}e−ζ0,j+ζ0,i+|ζi−ζ0,i|+|ζj−ζ0,j|
≤e2∥ζ−ζ0∥∞h(ζ).
Specifically, his of (1, e2h)-type. So by the composition rule of Lemma G.3, it must be that fis
real-analytic of (τ1, τ2)-type for τ1= min(1 ,(ρ1◦h)·1
e2h) = 1 /(2e2)andτ2=ρ2◦h= 1/h≤1.
Lemma G.7. For any j∈[m]andX,ζ,p, the function f:R2→Rgiven by f(β, γ) =
smax( βXζ+βγp)jis real-analytic of (min(1 ,1/(2e2∥Xζ∥∞+ 2e2(|β|+|γ|)∥p∥∞),1)-type.
Proof.Write f=g◦hforg:Rm→Randh:R2→Rmgiven by g(v) = smax( v)jand
h(β, γ) =βXζ+βγp. We know from Lemma G.6 that gis real-analytic of (1/(2e2),1)-type. And
it is easy to see that his real-analytic of (1,∥Xζ∥∞+(|β|+|γ|)∥p∥∞)-type. Apply the composition
rule of Lemma G.3 to conclude.
Lemma G.8. For any X,˜X,ζ,˜ζ,p, the function f:R2→Rgiven by f(β, γ) = smax( βXζ+
βγp)T(X˜XT+γ2I)smax( β˜X˜ζ+βγp)is real-analytic and of type
(min(1 ,1
2e21
∥Xζ∥∞+ (|β|+|γ|)∥p∥∞,1
2e21
∥˜X˜ζ∥∞+ (|β|+|γ|)∥p∥∞), C(1 +γ2)),
where Cis a constant depending on the context length k.
Proof.Each entry of (X˜XT+γI)is real-analytic in γand of (1, γ)-type. So by combining with
Lemma G.7 the product rule and sum rule (Lemma G.4), and the fact that each entry of the smax
is at most one.
Asaconsequence,wecanboundthederivativesof f(β, γ;X,˜X,ζ,˜ζ,p) = smax( βXζ+βγp)T(X˜XT+
γ2I)smax( β˜X˜ζ+βγp), which was what we needed to prove Lemma G.1.
Lemma G.9. For any k1, k2≥0,
|∂k1
∂βk1∂k2
∂γk2f(β, γ;X,˜X,ζ,˜ζ,p)|
≤C(1 +γ2) max(1 ,((2e2)(∥Xζ∥∞+∥˜X˜ζ∥∞+ (|β|+|γ|)∥p∥∞))k1+k2)k1!k2!.
Proof.Direct consequence of Lemma G.5 and Lemma G.8.
44

--- PAGE 45 ---
H Derivation of transformer kernel
Westatethetransformerarchitectureandinformallyderiveitsrandomfeatureskernelintheinfinite-
width limit.
H.1 Transformer architecture
We consider a depth-1 transformer architecture (without skip connections or layernorm, for simplic-
ity). Thisarchitecturehas Hheads,eachwithparameters WK,h,WQ,h,WV,h,WO,h∈Rdhead×demb,
and embedding layer WE∈Rm×demb, positional embeddings P∈Rk×demb, an MLP layer with pa-
rameters WA,WB∈Rdmlp×demb, and a final unembedding layer with weights wU∈Rdemb. The
network takes in X∈Rk×mand outputs
ftrans(X;θ) =wT
Uz2 (Unembedding)
where
z2=1pdmlpWT
Bσ(1√dembWAz1)∈Rdemb (MLP layer)
z1=1√
HX
h∈[H]AT
hek∈Rdemb (Attention layer output at CLS token)
Ah= smax(βZ0WT
K,hWQ,hZT
0
demb√dhead)Z0WT
V,hWO,h√dheaddemb∈Rk×demb (Attention heads)
Z0=XW E+γP∈Rk×demb. (Embedding layer)
Here β, γ≥0are two hyperparameters that control the inverse temperature of the softmax and
the strength of the positional embeddings, respectively. Note that only the output of the attention
layer at the final kth position CLS token is used, since this is a depth-1 network. The smaxis a
softmax applied row-wise.
H.2 Random features kernel
The derivation of this kernel assumes that every string xends with a special [CLS] classification
token that does not appear elsewhere in the string. We choose that initialization so that each of
the entries of the intermediate representations Z0,z1,z2is of order Θ(1). In order to accomplish
this, we initialize WE,P,WK,h,WQ,h,WV,h,WO,h,WA,WBwith i.i.d. N(0,1)entries.
We also initialize wU= 0, and only train wUwhile maintaining the rest of parameters at
initialization. The random features kernel corresponding to training wUis
ˆKtrans(X,Y) =z2(X)Tz2(Y)/demb,
where we view z2as a function of the input (either XorY), and depending on the randomly-
initialized parameters of the network.
In the limit of infinitely-many heads H, infinite embedding dimension demband MLP dimension
dmlpand head dimension dhead, the kernel ˆKtranstends to a deterministic limit Ktrans, which can be
recursively computed (see, e.g., [JGH18]). Assuming that the final token of both XandYis the
45

--- PAGE 46 ---
same token (i.e., a CLS token), the deterministic limiting kernel Ktransis given by:
Ktrans(X,Y) =Eu,v[σ(u)σ(v)]foru, v∼N(0,Kattn(X,X)Kattn(X,Y)
Kattn(Y,X)Kattn(Y,Y)
)(22)
where Kattn(X,Y) =Em(X),m(Y)[smax( βm(X))T(XYT+γ2I)smax( βm(Y))]
m(X),m(Y)∼N(0,(1 +γ2)XXT+γ2I XYT+γ2I
Y XT+γ2I Y YT+γ2I
).
Notice that the covariance matrix in the above definition of the distribution of m(X),m(Y)
is rescaled compared to that in the main text in Section 3.1, but this is inessential, since we can
simply reparametrize βasβ7→β/p
1 +γ2to recover the expression in the main text.
H.3 Informal derivation
We provide an informal derivation of (22) below. Informally, by law of large numbers we have the
following almost sure convergence
ˆKtrans(X,Y) =z2(X)Tz2(Y)
demb=σ(1√dembWAz1(X))TWBWT
Bσ(1√dembWAz1(Y))
dembdmlp
demb→∞→σ(1√dembWAz1(X))Tσ(1√dembWAz1(Y))
dmlp
dmlp→∞→Eu,v[σ(u)σ(v)]foru, v∼N(0,Kattn(X,X)Kattn(X,Y)
Kattn(Y,X)Kattn(Y,Y)
)
:=Ktrans(X,Y),
46

--- PAGE 47 ---
where Kattnis the kernel corresponding to the attention layer in the infinite-width limit, defined as:
ˆKattn(X,Y) :=zT
1(X)zT
1(Y)
demb=P
h,h′∈[H]eT
kAh(X)Ah′(Y)Tek
Hdemb
=1
Hdheadd2
embX
h,h′∈[H]eT
ksmax(βZ0(X)WT
K,hWQ,hZ0(X)T
demb√dhead)Z0(X)WT
V,hWO,h
·WT
O,h′WV,h′Z0(Y)Tsmax(βZ0(Y)WT
K,h′WQ,h′Z0(Y)T
demb√dhead)Tek
dhead→∞,demb→∞→1
HX
h∈[H]eT
ksmax(βZ0(X)WT
K,hWQ,hZ0(X)T
demb√dhead)(XYT+γ2I)
·smax(βZ0(Y)WT
K,hWQ,hZ0(Y)T
demb√dhead)Tek
H→∞→E[eT
ksmax(βZ0(X)WT
K,hWQ,hZ0(X)T
demb√dhead)(XYT+γ2I)
·smax(βZ0(Y)WT
K,hWQ,hZ0(Y)T
demb√dhead)Tek]
=E[smax(βeT
kZ0(X)WT
K,hWQ,hZ0(X)T
demb√dhead)(XYT+γ2I)
·smax(βeT
kZ0(Y)WT
K,hWQ,hZ0(Y)T
demb√dhead)T]
demb→∞,dhead→∞→ Em(X),m(Y)[smax( βm(X))T(XYT+γ2I)smax( βm(Y))]
:=Kattn(X,Y),
where
m(X),m(Y)∼N(0,(1 +γ2)XXT+γ2I XYT+γ2I
Y XT+γ2I Y YT+γ2I
),
because due to the randomness in WK,handWQ,hwe have that
Z0(X)WT
Q,hWK,hZ0(X)Tek
demb√dhead
and
Z0(Y)WT
Q,hWK,hZ0(Y)Tek
demb√dhead
are jointly Gaussian with covariance:
Σ(X,Y) =EWK,h,WQ,h,WE,P[Z0(X)WT
Q,hWK,hZ0(X)Tek
demb√dheadeT
kZ0(Y)WT
K,hWQ,hZ0(Y)T
demb√dhead], .
47

--- PAGE 48 ---
Since this is an expectation over products of jointly Gaussian variables, for any i, j∈[k]we can
calculate:
Σi,j(X,Y) =EWE,P[1
d2
embX
r,s∈[demb][Z0(X)]ir[Z0(Y)]jstrace( Z0(X)TekeT
kZ0(Y))]
=EWE,P[1
d2
embX
r,s,t∈[demb][Z0(X)]ir[Z0(Y)]js[Z0(X)]kt[Z0(Y)]kt]
=EWE,P[1
d2
embX
r,s,t∈[demb][XW E+γP]ir[Y W E+γP]js[XW E+γP]kt[Y W E+γP]kt]
(a)=1
d2
embX
r,s∈[demb]EWE,P[[XW E+γP]ir[Y W E+γP]js]
·X
t∈[demb]EWE,P[[XW E+γP]kt[Y W E+γP]kt] +O(1/demb)
=1
dembX
r,s∈[demb]EWE,P[[XW E+γP]ir[Y W E+γP]js]·(1 +γ2) +O(1/demb)
(a)=1
dembX
r∈[demb]EWE,P[[XW E+γP]ir[Y W E+γP]jr]·(1 +γ2) +O(1/demb)
= [XYT]ij+γ2δij·(1 +γ2) +O(1/demb),
where in (a) we use that [XW E+γP]aband[Y W E+γP]abare independent of [XW E+γP]cd
and[Y W E+γP]cdunless b=d. So
Σ(X,Y)demb→∞→ (1 +γ2)·(XYT+γ2I).
I MLPs fail to generalize on unseen symbols
A natural question is whether classical architectures such as the MLP architecture (a.k.a., fully-
connectednetwork)wouldexhibitthesameemergentreasoningpropertieswhentrainedwithenough
data. In this section, we prove a negative result: an SGD-trained or Adam-trained MLP will not
reach good test performance on the template task. This is in sharp contrast to the positive result
for transformers proved in the previous section.
MLP architecture The input to the MLP is a concatenation of the token one-hot encodings.
The MLP alternates linear transformations and nonlinear elementwise activations. Formally, the
MLP has weights θ={W1, . . . ,WL,w}and outputs
fMLP(x;θ) =wTzL(x;θ)∈Rwhere (23)
zℓ(x;θ) =ϕ(Wℓzℓ−1(x;θ))∈Rdforℓ≥1
z0(x;θ) =z0(x) = [ex1, . . . ,exk]∈Rkm.
We consider training the MLP with SGD.
Definition I.1 (One-pass SGD training) .The learned weights θtafter tsteps of SGD training are
the random weights given by initializing θ0so that each of W0
1, . . . ,W0
L,w0have i.i.d. Gausian
entries, and then updating with θt=θt−1−ηt∇θ(fMLP(xt;θ)−yt)2|θ=θt−1for(xt, yt)∼ Dand
some step size ηt>0.
48

--- PAGE 49 ---
We show that SGD-trained MLPs fail at the template task since they do not generalize well in
the case when the templates consist only of wildcard tokens. In words, if the template labels f∗are
a non-constant function, the MLP will not reach arbitrarily low error no matter how many training
steps are taken. Let Xuns⊂ Xbe the subset of tokens not seen in the train data. We assume that
|Xuns| ≥k, which guarantees that for any template there is at least one string matching it where
all the wildcards are substituted by tokens in Xuns. Under this condition:
Theorem I.2 (Failure of MLPs at generalizing on unseen symbols) .Suppose that the label function
f∗is non-constant, and that all templates in the support of µtmpltconsist only of wildcards: z∈ Wk
for all z∈supp( µtmplt). Then, for any SGD step tthere is a string x∈(Xuns)kthat matches a
template z∈supp( µtmplt)such that
Eθt[(fMLP(x;θt)−f∗(z))2]≥c >0,
where cis constant that depends only on µtmpltandf∗.
The proof relies on the key observation that SGD-training of MLPs satisfies a permutation
invariance property [Ng04]. This property guarantees that MLP cannot consistently distinguish
between the unseen tokens, and therefore, in expectation over the weights θt, outputs the same
value for any sequence x∈(Xuns)k. We make four remarks.
Remark I.3. MLPs are universal approximators [Cyb89], so there are choices of weights θsuch
thatfMLP(·;θ)has good generalization on unseen symbols. The theorem proves that these weights
are not found by SGD.
Remark I.4. The theorem does not assume that training is in the NTK regime, i.e., it holds even
for nonlinear training dynamics.
Remark I.5. The theorem also holds for training with Adam, gradient flow, and minibatch-SGD,
since the permutation-invariance property of MLP training also holds for these.
Remark I.6. As a sanity check, we verify that MLP kernel does not meet the sufficient condition
for generalizing on unseen symbols from Lemma 3.5. The kernel for an MLP is an inner product
kernel of the form KMLP(x,x′) =κ(Pk
i=11(xi=x′
i))for a function κ:R→R. Therefore, the
matrix N∈Rr×rhas all of its entries equal to Nij=κ(0), so it is singular and the condition of
Lemma 3.5 is not met.
We now prove Theorem I.2. We first show that trained MLPs cannot differentiate between
tokens in the set Xuns. Let X=Xseen⊔ Xunsbe the partition of tokens into those seen and not
seen in the train data. Here Xseenis defined as the smallest set such that x∈ Xk
seenalmost surely
for(x, y)∼ D.
Lemma I.7 (Trained MLPs cannot distinguish unseen tokens) .For any number of SGD steps t,
and any learning rate schedule η1, . . . , η t, the learned MLP estimator cannot distinguish between
sequences of unseen tokens. Formally, for any x1,x2∈ Xk
uns, we have
Eθt[fMLP(x1;θt)] =Eθt[fMLP(x2;θt)].
Proof of Lemma I.7. The proof of this result is based on a well-known permutation-invariance prop-
erty of MLPs trained by SGD. This property has previously been used to show sample complexity
lower bounds for learning with SGD-trained MLPs [Ng04; LZA20], as well as time-complexity lower
49

--- PAGE 50 ---
bounds [Sha18; Abb+22; AB22]. In this lemma, we use the permutation invariance property to
show poor out-of-distribution generalization of SGD-trained MLPs.
First, construct a permutation Π∈Rkm×kmsuch that Πz0(x1) =z0(x2), but which also
satisfies that for any ˜x∈(Xseen)kwe have Πz0(˜x) =z0(˜x). This permutation can be easily
constructed since neither x1norx2contains tokens in Xseen. Next, define the following network
fΠ
MLP, analogously to (23) but with the first-layer inputs permuted by Π
fΠ
MLP(x;θ) =wTzΠ
L(x;θ)∈Rwhere
zΠ
ℓ(x;θ) =ϕ(WℓzΠ
ℓ−1(x;θ))∈Rdforℓ≥1
zΠ
0(x;θ) =zΠ
0(x) = Π[ ex1, . . . ,exk]∈Rkm.
Now let us couple the weights θ0, . . . ,θtfrom SGD training of fMLPon dataset D, with the
weights θΠ,0, . . . ,θΠ,tfrom SGD training of fΠ
MLPon dataset D. The coupling is performed induc-
tively on the time step, and we can maintain the property that θτ=θΠ,τfor all t. For the base case
τ= 0, we set θ0=θΠ,0. For the inductive step, τ≥1, we update the weights with the gradient
from some sample (xτ, yτ). Since xτ∈(Xseen)kalmost surely, we know that z0(xτ) =zΠ
0(xτ)
almost surely, which means that θτ=θΠ,τalmost surely. We conclude the equality in distribution
of the weights
θtd=θΠ,t. (24)
Next, let us inductively couple the weights θ0, . . . ,θtwith the weights θΠ,0, . . . ,θΠ,tin a different
way, so as to guarantee that for any time 0≤τ≤t, we have
Wτ
1=WΠ,τ
1ΠandWτ
ℓ=WΠ,τ
ℓfor all 2≤ℓ≤Landwτ=wΠ,τ.
almost surely. The base case τ= 0follows because the distribution of W0
1andWΠ,0
1is equal and
is also invariant to permutations since it is Gaussian. For the inductive step, couple the sample
updates so that SGD draws the same sample (xτ, yτ)∼ D. One can see from the chain rule that
the invariant is maintained. We conclude the equality in distribution of the weights
θt={Wt
1, . . . ,Wt
L,wt}d={WΠ,t
1Π,WΠ,t
2, . . . ,WΠ,t
L,wΠ,t} (25)
Combining (24) and (25), we get
θt={Wt
1, . . . ,Wt
L,wt}d={Wt
1Π,Wt
2, . . . ,Wt
L,wt},
which,since Πz0(x1) =z0(x2), immediately implies
fMLP(x1;θt) =fMLP(x2;{Wt
1Π,Wt
2, . . . ,Wt
L,wt})d=fMLP(x2;θt),
which proves the lemma.
Theorem I.2 follows as a consequence. Note that the key lemma proved above only relied on
a permutation invariance property of SGD on MLPs that also holds for Adam training, gradient
flow training, and SGD with minibatch (see [LZA20]). Therefore, the result holds for training with
those algorithms as well, beyond just SGD.
50

--- PAGE 51 ---
Proof of Theorem I.2. Pick any two templates z,z′∈supp( µtmplt)such that f∗(z)̸=f∗(z′). Recall
thatz,z′∈ Wkby assumption. Since we assumed that |Xuns| ≥k, there are strings x,x′∈
Xk
unsmatching templates zandz′, respectively. Furthermore, by Lemma I.7, if we define a=
Eθt[fMLP(x;θt)] =Eθt[fMLP(x′;θt)], we have
max(Eθt[(fMLP(x;θt)−f∗(z))2],Eθt[(fMLP(x′;θt)−f∗(z′))2])
≥max(( a−f∗(z))2,(a−f∗(z′))2)
≥1
4(f∗(z)−f∗(z′))2=c >0.
J Deferred details for next-token-prediction template tasks
J.1 Definition of next-token-prediction template tasks
In next-token-prediction template tasks, the output is a token in X, with the cross-entropy loss for
multiclass classification. The formal definition of these tasks is:
Definition J.1 (Multi-class prediction version of template) .The data distribution Dmulticlass =
Dmulticlass (µtmplt,{µsub,z}, f∗)is specified by: (i) a template distribution µtmpltsupported on (X ∪
W)k; (ii) for each template z, a distribution µsub,zover substitution maps s:W → X ; (iii) a
labelling function f∗: supp( µtmplt)→ X ∪ W . A sample (x, y)∈ Xk× Xdrawn from Dmulticlass is
drawn by taking x= sub( z, s)andy= sub( f∗(z), s), where z∼µtmpltands∼µsub,z.
J.2 Failure of transformers to copy and modification that succeeds
We provide the deferred proofs for Section 4.
Attention layer architecture For simplicity in this section we consider a transformer with the
attention layer only, since the MLP layer does not play a role in the ability to copy unseen symbols.
Our architecture has Hheads with parameters WK,h,WQ,h,WV,h,WO,h∈Rdhead×demb, an em-
bedding/unembedding layer WE∈Rm×demb, positional embeddings P∈Rk×demb, an MLP layer
with parameters WA,WB∈Rdmlp×demb, a final unembedding layer , and an activation function ϕ.
The network takes in X∈Rk×mand outputs
fattn(X;θ) =WEz1∈Rm(Unembedding layer)
where
z1=X
h∈[H]AT
hek
Ah= smax( βZ0WT
K,hWQ,hZT
0)Z0WT
V,hWO,h∈Rk×demb (Attention heads)
Z0=XW E+γP∈Rk×demb. (Embedding layer)
and we tie the embedding and unembedding weights, as often done in practice, for example in
GPT-2 [Bro+20]. Here β, γ≥0are two hyperparameters that control the inverse temperature of
the softmax and the strength of the positional embeddings, respectively.
51

--- PAGE 52 ---
Simplification in our case We consider here a next-token prediction setup, where there is no
final [CLS] token appended to the string. Namely, given a string x∈ Xk, this is inputted to the
network as a stacked matrix of one-hot vectors for the tokens of the string X= [ex1, . . . ,exk]. We
study a very basic template task: template “ α” labeled by α, where αis a wildcard. An example
dataset generated from this template could be {(A, A),(B, B),(C, C)}, where A, B, C ∈ Xare
tokens. Because the template has length k= 1,X∈Rk×mis a one-hot vector encoding the input
token. Furthermore, the softmax output is always a 1×1matrix with the entry 1, so the architecture
simplifies to
fattn(X;θ) =WE(X
h∈[H]WT
O,hWV,h)(WT
EXT+γPT). (26)
We initialize the entries of PandWEbe i.i.d. N(0,1/demb), the entries of WO,hbeN(0,1/(demb)),
and the entries of WV,hbeN(0,1/dhead), so that as demb→ ∞the variance of the output vanishes
asO(1/demb)as in the mean-field scaling [MMN18; MMM19; SS22; CB18; RV18; YH21].
Derivationofkernelsdrivingdynamicsatsmalltimes Despitethesimplicityofthetask, the
architecture does not generalize well on unseen symbols. Our evidence for this will be by analyzing
the early times of training. For these times, the dynamics are governed by the neural tangent kernel
(NTK) of the network at initialization [JGH18; COB19]. Let us derive the neural tangent kernel
of this architecture. This is a network with output of dimension m, so for each i, j∈[m]we will
derive Kij,O(X,X′), Kij,V(X,X′), Kij,P(X,X′), Kij,E(X,X′)which give the dynamics at small
times for training the {WO,h}h∈[H], the{WV,h}h∈[H], theWP, and the WEweights at small times,
respectively. Writing WE= [wE,1, . . . ,wE,m]⊤, by the law of large numbers,
Kij,O(X,X′) =X
h∈[H]∂[fattn(X;θ)]i
∂WO,hT∂[fattn(X′;θ)]j
∂WO,h
∝1
HX
h∈[H](XW E+γP)WT
V,hWV,h(WT
EXT+γPT)wT
E,iwE,j
dhead→∞,demb→∞→ δij(δx1,x′
1+γ2)
Kij,V(X,X′) =X
h∈[H]∂[fattn(X;θ)]i
∂WV,hT∂[fattn(X′;θ)]j
∂WV,h
∝demb
dheadX
h∈[H]wT
E,iWT
O,hWO,hwE,j(XW E+γP)T(X′WE+γP)
dhead→∞→wT
E,iwE,j(XW E+γP)T(X′WE+γP)
demb→∞→ δij(δx1,x′
1+γ2)
Kij,P(X,X′) =∂[fattn(X;θ)]i
∂PT∂[fattn(X′;θ)]j
∂P
=γ2w⊤
E,iwE,jdemb→∞→ γ2δij
52

--- PAGE 53 ---
Kij,E(X,X′) =∂[fattn(X;θ)]i
∂WET∂[fattn(X′;θ)]j
∂WE
=δij(XW E+γP)(X
h∈[H]WT
V,hWO,h)(X
h∈[H]WT
O,hWV,h)(WT
E(X′)T+γPT)
+δx1,x′
1wT
E,i(X
h∈[H]WT
O,hWV,h)(X
h∈[H]WT
V,hWO,h)wT
E,j
+δi,x′
1wT
E,j(X
h∈[H]WT
O,hWV,h)(X
h∈[H]WT
O,hWV,h)(wE,x1+γPT)
+δx1,jwT
E,i(X
h∈[H]WT
O,hWV,h)(X
h∈[H]WT
O,hWV,h)(wE,x′
1+γPT)
dhead→∞,demb→∞,H→∞→ δij(2δx1,x′
1+γ2),
since only the first two terms do not vanish as the embedding dimension and number of heads go
to infinity.
Training loss and testing loss Let(x1, y1), . . . , (xn, yn)∈ X ×X be a training set of data points
drawn from this task, where due to the structure of the template task each of the context strings is
length-1 and we have xi=yi. We will test the model on a data point (xtest, ytest), which does not
appear in the test set: i.e., xtest=ytest̸∈ {x1, . . . , x n}.
The training loss is given by
Ltrain(θ) =1
nnX
i=1ℓ(fattn(xi;θ), yi),
where ℓis the cross-entropy loss, and the test loss is given by
Ltest(θ) =ℓ(fattn(xtest), ytest).
Theorem J.2. For any learning rates ηO, ηV, ηP, ηEsuch that |∂Ltrain
∂t|=O(1)asdemb, dhead, and
H→ ∞, we have |∂Ltest
∂t| ≤o(1). In other words, the error for generalization on unseen symbols
does not decrease during training for infinite-width transformers.
Proof.Consider training with gradient flow with learning rates ηO, ηV, ηP, ηEon the parameters
{WO,h}h∈[H],{WV,h}h∈[H],WP, and WE, respectively. In the limit as demb→ ∞we have
fattn(X;θ0)→0, so
∂Ltrain
∂θ|θ=θ0=1
nnX
i=1(1
m1−exi)T∂fattn(Xi;θ)
∂θ|θ=θ0.
So at time t= 0, the training loss decreases as
∂Ltrain
∂t|t=0→ −1
n2X
i,i′∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xi′)
·(ηVKjj′,V(Xi,Xi′) +ηOKjj′,O(Xi,Xi′)
+ηPKjj′,P(Xi,Xi′) +ηEKjj′,E(Xi,Xi′)).
53

--- PAGE 54 ---
So we must take ηO=O(1/H), ηV=O(demb/dhead),ηP=O(1), and ηE=O(1)for us to have
∂Ltrain
∂t=O(1)be bounded by a constant that does not grow with demb,dhead, and H.
Under these choices of learning rates, the test loss on token xtestwhich is not in the training
dataset {x1, . . . , x n}, evolves as
∂Ltest
∂t|t=0→ −1
nX
i∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xtest)
·(ηVKjj′,V(Xi,Xtest) +ηOKjj′,O(Xi,Xtest)
+ηPKjj′,P(Xi,Xtest) +ηEKjj′,E(Xi,Xtest))
→ −1
nX
i∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xtest)
·((dhead
dembηV+HηO)δj,j′(δxi,xtest+γ2)
+ηPγ2δj,j′+ 2HηEδj,j′(δxi,xtest+γ2))
=−γ2
nX
i∈[n]X
j∈[m](1/m−δj,xi)(1/m−δj,xtest)·(dhead
dembηV+HηO+ηP+ 2ηE)
=−C
nX
i∈[n]X
j∈[m](1/m−δj,xi)(1/m−δj,xtest)
=−C/m +C/m +C/m =C/m≥0.
On the other hand, now we consider the fattnarchitecture where in each head we replace
WT
V,hWO,hwithWT
V,hWO,h+bhI, where bhis a trainable parameter and I∈Rdemb×dembis
the identity matrix:
f′
attn(X;θ) =WEz1∈Rm(Unembedding layer)
where
z′
1=X
h∈[H](A′
h)Tek
A′
h= smax( βZ0WT
K,hWQ,hZT
0)Z0(WT
V,hWO,h+bhI)∈Rk×demb (Attention heads)
Z0=XW E+γP∈Rk×demb. (Embedding layer)
Again, for the case of k= 1that we consider, the network simplifies considerably to
f′
attn(X;θ) =WE(X
h∈[H]WT
O,hWV,h+bhI)(WT
EXT+γPT). (27)
We initialize bh= 0for all h, so that the neural tangent kernels Kij,O, Kij,V, Kij,P, Kij,Eare the
same as above. Now we also have a neural tangent kernel for training the parameters {bh}h∈[H]:
Kij,b(X,X′) =X
h∈[H]∂[fattn(X;θ)]i
∂bh∂[fattn(X′;θ)]j
∂bh
∝w⊤
E,i(WT
EXT+γPT)(XW E+γPT)wE,j
demb→∞→ δi,x1δj,x′
1
54

--- PAGE 55 ---
We prove that under this parametrization the test loss does decrease with training, which shows
that adding this trainable identity scaling allows transformers to succeed at this task.
TheoremJ.3. There is a choice of learning rates ηb, ηV, ηO, ηE, ηPsuch that as demb, dhead, H→ ∞
we have |∂Ltrain
∂t| |t=0=O(1)and−∂Ltest
∂t|t=0= Ω(1).
Proof.Training just the parameters {bh}h∈[H]with learning rate ηb(keeping the learning rates
ηV, ηO, ηP, ηE= 0, so the training loss decreases as
∂Ltrain
∂t|t=0→ −ηb
n2X
i,i′∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xi′)Kjj′,b(Xi,Xi′),
so we should take ηb= Θ(1 /H)for the train loss have derivative on the order of Θ(1). The test loss
decreases as:
∂Ltest
∂t|t=0→ −ηb
nX
i∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xtest)Kjj′,b(Xi,Xtest)
→ −Hηb
nX
i∈[n]X
j,j′∈[m](1/m−δj,xi)(1/m−δj′,xtest)δj,xiδj′,xtest
=−Hηb
nX
i∈[n](1/m−1)(1/m−1)
=−Hηb(1−1/m)2
= Ω(1) ,
forηb= Ω(H), asdemb, H→ ∞.
55
