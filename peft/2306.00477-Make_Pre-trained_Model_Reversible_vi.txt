# 2306.00477.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2306.00477.pdf
# Kích thước tệp: 1229567 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Làm cho Mô hình Tiền huấn luyện Có thể Đảo ngược:
Từ Tham số đến Tinh chỉnh Hiệu quả Bộ nhớ
Baohao Liao Shaomu Tan Christof Monz
Phòng thí nghiệm Công nghệ Ngôn ngữ, Đại học Amsterdam
{b.liao, s.tan, c.monz}@uva.nl
Tóm tắt
Tinh chỉnh hiệu quả tham số (PEFT) của các mô hình ngôn ngữ tiền huấn luyện (PLM) đã
nổi lên như một phương pháp tiếp cận rất thành công, chỉ huấn luyện một số lượng nhỏ các
tham số mà không làm giảm hiệu suất và trở thành mô hình học习 de-facto với kích thước
ngày càng tăng của PLM. Tuy nhiên, các phương pháp PEFT hiện tại không hiệu quả về
bộ nhớ, vì chúng vẫn yêu cầu lưu trữ hầu hết các kích hoạt trung gian để tính toán gradient,
tương tự như tinh chỉnh đầy đủ. Một cách hiệu quả để giảm bộ nhớ kích hoạt là áp dụng mô
hình có thể đảo ngược, do đó các kích hoạt trung gian không cần thiết phải được lưu trữ và
có thể được tính toán lại. Tuy nhiên, việc sửa đổi PLM thành biến thể có thể đảo ngược
không đơn giản, vì mô hình có thể đảo ngược có kiến trúc khác biệt so với các PLM hiện
được phát hành. Trong bài báo này, trước tiên chúng tôi nghiên cứu yếu tố quan trọng nào
cho sự thành công của các phương pháp PEFT hiện tại, và nhận ra rằng việc bảo tồn điểm
khởi đầu của PLM khi khởi tạo phương pháp PEFT là cần thiết. Với phát hiện này, chúng
tôi đề xuất tinh chỉnh hiệu quả bộ nhớ (MEFT) chèn các adapter vào PLM, bảo tồn điểm
khởi đầu của PLM và làm cho nó có thể đảo ngược mà không cần tiền huấn luyện bổ sung.
Chúng tôi đánh giá MEFT trên benchmark GLUE và năm nhiệm vụ hỏi đáp với các backbone
khác nhau, BERT, RoBERTa, BART và OPT. MEFT giảm đáng kể bộ nhớ kích hoạt lên đến
84% so với tinh chỉnh đầy đủ với một lượng tham số có thể huấn luyện không đáng kể.
Hơn nữa, MEFT đạt được điểm số tương tự trên GLUE và điểm số có thể so sánh trên các
nhiệm vụ hỏi đáp như tinh chỉnh đầy đủ. Một phát hiện tương tự cũng được quan sát thấy
cho nhiệm vụ phân loại hình ảnh.

1 Giới thiệu
Các mô hình tiền huấn luyện quy mô lớn đã đạt được thành công lớn
trong nhiều lĩnh vực và ứng dụng khác nhau [1,2,3,4,5,6,7,8].
Khi khả năng của chúng tiếp tục phát triển, các mô hình ngôn ngữ
tiền huấn luyện (PLM) được phát hành đã tăng trưởng theo cấp số
nhân về kích thước, thậm chí đạt quy mô 100 tỷ tham số [3,9,10,11,
12]. Do đó, điều này đặt ra những thách thức chưa từng có trong
việc tận dụng hiệu quả các mô hình này cho các nhiệm vụ downstream
do tài nguyên tính toán hạn chế.

Một phương pháp tiếp cận phổ biến trong lịch sử để điều chỉnh PLM
cho các nhiệm vụ downstream là cập nhật tất cả các tham số tiền
huấn luyện, tinh chỉnh đầy đủ. Mặc dù tinh chỉnh đầy đủ đã mang
lại nhiều kết quả tiên tiến, khả năng áp dụng của nó bị hạn chế trong
các môi trường bị ràng buộc về lưu trữ. Ràng buộc này phát sinh từ
việc duy trì một bản sao hoàn chỉnh của mô hình được tinh chỉnh cho
mỗi nhiệm vụ. Một phương pháp điều chỉnh thay thế

--- TRANG 2 ---
là tinh chỉnh hiệu quả tham số (PEFT) [13,14,15,16,17,18,19] bao gồm việc cập nhật có chọn lọc một số lượng nhỏ các tham số cụ thể cho nhiệm vụ trong khi giữ đóng băng phần lớn các tham số của PLM. PEFT mang lại lợi thế đáng kể trong việc giảm yêu cầu lưu trữ bằng cách chỉ lưu một PLM chung cùng với các tham số đã sửa đổi cho mỗi nhiệm vụ. Ngoài việc tiết kiệm lưu trữ, PEFT đạt được hiệu suất có thể so sánh với tinh chỉnh đầy đủ, tạo ra sự quan tâm đáng kể trong việc áp dụng PEFT.

Mặc dù có lợi thế về hiệu quả tham số, các phương pháp PEFT hiện tại vẫn gặp thách thức về hiệu quả bộ nhớ [20,21]. PEFT cần thiết phải lưu trữ các kích hoạt trung gian, tương tự như yêu cầu của tinh chỉnh đầy đủ, để tính toán gradient của các tham số có thể huấn luyện. Thông thường, chúng tiêu thụ hơn 70% bộ nhớ kích hoạt của tinh chỉnh đầy đủ (xem Hình 1). Vì các kích hoạt đóng góp đáng kể vào yêu cầu bộ nhớ trong quá trình huấn luyện, có những trường hợp việc tinh chỉnh PLM quy mô lớn với PEFT không khả thi do ràng buộc bộ nhớ. Để giải quyết vấn đề này, một phương pháp tiếp cận thường được sử dụng là coi PLM như một bộ trích xuất đặc trưng, chẳng hạn như chưng cất kiến thức sang mô hình nhỏ hơn [22,23], thêm các lớp có thể huấn luyện bổ sung lên trên [20] hoặc căn chỉnh [21,24] với nó, v.v. Các phương pháp này tránh được nhu cầu lưu trữ các kích hoạt của PLM vì đồ thị tính toán gradient không đi qua PLM. Tuy nhiên, các phương pháp này thường yêu cầu tiền huấn luyện bổ sung hoặc thể hiện khoảng cách hiệu suất đáng kể so với tinh chỉnh đầy đủ khi sử dụng cùng mô hình cơ bản [20, 21].

Trong bài báo này, chúng tôi đề xuất một phương pháp mới gọi là tinh chỉnh hiệu quả bộ nhớ (MEFT) để sửa đổi PLM theo cách hiệu quả về tham số và bộ nhớ, mà không yêu cầu tiền huấn luyện bổ sung. Ban đầu, chúng tôi nghiên cứu yếu tố quan trọng cho sự thành công của các phương pháp PEFT hiện tại và xác định rằng việc khởi tạo đúng cách các tham số mới được thêm vào là cần thiết để duy trì tính liên tục thông tin từ PLM (§2). Tận dụng hiểu biết này, chúng tôi thiết kế ba phương pháp MEFT cho phép sửa đổi PLM thành biến thể có thể đảo ngược, do đó nó chỉ cần thiết lưu trữ đầu ra cuối cùng và cho phép tính toán lại các kích hoạt trung gian trong quá trình lan truyền ngược (§3). Do đó, MEFT giảm đáng kể bộ nhớ cần thiết để lưu trữ các kích hoạt (xem Hình 1).

Để xác nhận hiệu quả của các phương pháp MEFT của chúng tôi, chúng tôi tiến hành đánh giá rộng rãi trên benchmark GLUE [25] với BERT [1], RoBERTa [2] và BART [26] (§4). Kết quả thực nghiệm nhất quán cho thấy các phương pháp MEFT của chúng tôi vượt trội hơn cả tinh chỉnh đầy đủ và các baseline PEFT mạnh về hiệu quả tham số và bộ nhớ. Đáng chú ý, các phương pháp của chúng tôi đạt được điểm số tương tự như tinh chỉnh đầy đủ trong khi chỉ cập nhật 0,2% tham số và tiết kiệm lên đến 84% bộ nhớ kích hoạt. Hơn nữa, chúng tôi đánh giá MEFT trên năm nhiệm vụ hỏi đáp với mô hình lớn hơn, OPT [9]. Kết quả cho thấy phương pháp của chúng tôi đạt được điểm số có thể so sánh như tinh chỉnh đầy đủ trong khi tiết kiệm 50% bộ nhớ kích hoạt và chỉ cập nhật 0,64% tham số. Một phát hiện tương tự cũng được quan sát thấy trên nhiệm vụ phân loại hình ảnh, SVHN [27]. Tổng thể, các thí nghiệm này thiết lập hiệu quả của MEFT như một phương pháp tiếp cận hiệu quả về tham số và bộ nhớ mạnh mẽ mà không làm giảm hiệu suất.

2 Kiến thức cơ bản

Trong phần này, chúng tôi nhằm cung cấp kiến thức nền tảng cần thiết bằng cách trả lời các câu hỏi sau: (1) Tại sao các PEFT hiện tại không đủ hiệu quả bộ nhớ (§2.1)? (2) Yếu tố quan trọng nào cho sự thành công của PEFT (§2.2)? (3) Mô hình có thể đảo ngược có những thách thức gì (§2.3)?

2.1 Tinh chỉnh hiệu quả tham số không đủ hiệu quả bộ nhớ

Cho một perceptron đa lớp N: hN=fN(fN−1(...(f2(f1(h0)))...)) với h0 là đầu vào ban đầu, lớp thứ n là hn=fn(hn−1) =σn(Wnhn−1) bao gồm hàm phi tuyến σn và ma trận trọng số Wn, trong đó số hạng bias được bỏ qua để đơn giản. Ký hiệu xn=Wnhn−1, trong lan truyền ngược với mất mát L, gradient của Wn được tính toán với quy tắc chuỗi như sau:

∂L/∂Wn=∂L/∂hN(∏i=n+1 to N ∂hi/∂xi ∂xi/∂hi−1)∂hn/∂xn ∂xn/∂Wn=∂L/∂hN(∏i=n+1 to N σ'iWi)σ'nhn−1 (1)

trong đó σ' là đạo hàm của σ và việc tính toán σ'n yêu cầu xn. Do đó, {xi}i=n to N được lưu trữ trong quá trình truyền xuôi để có được gradient của Wn, ngay cả khi {Wi}i>n bị đóng băng.

--- TRANG 3 ---
Trong quá trình huấn luyện, dung lượng bộ nhớ đỉnh chủ yếu được chiếm bởi ba thành phần: tham số của mô hình {Wn}N n=1, trạng thái tối ưu hóa có kích thước gấp ba lần kích thước của các tham số có thể huấn luyện đối với Adam [28] (một cho gradient và hai cho moment), và các kích hoạt. Dung lượng bộ nhớ cho cả ba thành phần đều liên quan đến độ sâu và chiều rộng của mô hình. Ngoài ra, dung lượng bộ nhớ cho các kích hoạt cũng liên quan đến một số cài đặt huấn luyện, như kích thước batch và độ dài chuỗi.

So với tinh chỉnh đầy đủ, các phương pháp PEFT hiện tại, chẳng hạn như (Houlsby và Pfeiffer) Adapters [14,16], LoRA [17], (IA)³[29], Prompt-Tuning [19] và Prefix-Tuning [15], điều chỉnh một số lượng nhỏ tham số, làm cho kích thước trạng thái tối ưu hóa trở nên không đáng kể. Tuy nhiên, dung lượng bộ nhớ cần thiết cho các kích hoạt không được giảm đáng kể. Như được hiển thị trong Hình 2a, trong đó chúng tôi đặt kích thước batch là 64 và độ dài chuỗi là 512 trên RTE [30,31,32,33] với BERT base[1], bộ nhớ kích hoạt của tất cả các phương pháp PEFT là >75% so với tinh chỉnh đầy đủ, ngay cả với <1% tham số có thể huấn luyện.

2.2 Khởi tạo có ý nghĩa quan trọng đối với tinh chỉnh hiệu quả tham số

Các mô hình tiền huấn luyện học các biểu diễn chung và phân tán đủ để tạo điều kiện cho việc học downstream của biểu diễn nhiệm vụ được nén cao [36], tức là cung cấp một điểm khởi đầu vững chắc cho việc huấn luyện các nhiệm vụ downstream. Khi sửa đổi PLM với PEFT, chúng tôi giả thuyết rằng người ta cần bảo tồn điểm khởi đầu này ở đầu quá trình huấn luyện để có hiệu suất tốt hơn.

Giả thuyết Điểm Khởi đầu. Khi sửa đổi mô hình tiền huấn luyện bằng cách thêm các tham số mới, người ta cần khởi tạo các tham số mới theo cách bảo tồn điểm khởi đầu từ mô hình tiền huấn luyện ở đầu quá trình huấn luyện, sao cho việc tinh chỉnh mô hình đã sửa đổi có thể phù hợp với hiệu suất của tinh chỉnh đầy đủ.

Chính thức hơn, giả sử fn là một lớp PLM và hn=fn(hn−1), đầu ra từ lớp đã sửa đổi f'n, h'n=f'n(hn−1), nên gần với hn ở đầu quá trình huấn luyện. Tức là h'n=hn+δ, trong đó ∥δ∥ → 0. Trực quan, chúng ta muốn h'n≈hn, vì h'n là đầu vào cho lớp PLM (đã sửa đổi) tiếp theo. Nếu chúng khác nhau, tính liên tục biểu diễn sẽ bị phá vỡ. Mặc dù hầu hết các phương pháp PEFT [14,16,17,29] khởi tạo các module được thêm vào theo cách này, chúng tôi không thể tìm thấy một nghiên cứu kỹ lưỡng về tầm quan trọng của việc khởi tạo này trong tài liệu hiện có. Trong phần này, chúng tôi khám phá tầm quan trọng của việc khởi tạo PEFT cho hai phương pháp, LoRA và (IA)³[29].

LoRA và (IA)³ đại diện cho hai phương pháp phổ biến để giới thiệu các tham số mới, bao gồm các phép toán cộng và tỷ lệ, tương ứng. Cho một ma trận trọng số tiền huấn luyện W∈Rd×d, LoRA sửa đổi nó như h'= (W+α/r WdownWup)h, trong đó Wdown∈Rd×r và Wup∈Rr×d là các tham số có thể huấn luyện được thêm vào, α là hệ số tỷ lệ không đổi và thường r≪d. Khởi tạo mặc định của LoRA là Wdown∼ N(0, σ²) và Wup=0. Theo cách này, WdownWup=0 và điểm khởi đầu từ PLM được bảo tồn hoàn hảo. (IA)³ sửa đổi W bằng cách nhân nó với một vector có thể huấn luyện l∈Rd như h'= (l⊙W)h, trong đó ⊙ đại diện cho phép nhân theo phần tử. Khởi tạo mặc định của (IA)³ là l=1, cũng làm cho điểm khởi đầu không bị động chạm.

Để tạo điều kiện cho quá trình khởi tạo của LoRA, chúng tôi chọn các giá trị ban đầu sau: Wdown =1, Wup=c và α= 1, trong đó c là ma trận với tất cả các phần tử bằng giá trị khởi tạo c, dẫn đến α/r WdownWup=c. Khi c= 0, điểm khởi đầu từ PLM được bảo tồn. Bằng cách điều chỉnh c, chúng tôi kiểm soát mức độ khởi hành từ điểm khởi đầu. Tương tự, chúng tôi thay thế l bằng l'=αl=αc cho (IA)³.

Trong Hình 2b, chúng tôi huấn luyện các tham số mới được thêm vào trên RoBERTa base[2] cho bốn nhiệm vụ (CoLA [37], STS-B [38], MRPC [39] và RTE [30,31,32,33]). Đối với LoRA (r= 8), mặc dù chúng tôi sửa đổi phương pháp khởi tạo, kết quả của chúng tôi (c= 0) rất gần với khởi tạo mặc định. Khi điểm khởi đầu bị phá vỡ bởi c≠ 0 (α= 1), tất cả kết quả thậm chí còn tệ hơn mô hình được khởi tạo ngẫu nhiên. Tuy nhiên, khi chúng tôi đặt α= 0³ để bảo tồn điểm khởi đầu, tất cả kết quả trở nên tốt hơn nhiều so với những kết quả với α= 1. Đối với (IA)³, khi chúng tôi giảm c từ 1 (khởi tạo mặc định) xuống 0, kết quả (α= 1) trở nên ngày càng tệ hơn. Tuy nhiên, khi chúng tôi đặt α= 1/c để bảo tồn điểm khởi đầu, tất cả kết quả trở nên tốt hơn. Một số trong số chúng thậm chí còn tốt hơn khởi tạo mặc định. Tất cả các kết quả nói trên cho thấy rằng việc bảo tồn điểm khởi đầu từ PLM ở đầu quá trình huấn luyện là quan trọng khi áp dụng hoặc thiết kế phương pháp PEFT. Một sơ đồ khởi tạo khác được trình bày trong Hình 10 dẫn đến phát hiện tương tự.

2.3 Thách thức của mạng neural có thể đảo ngược

Nhắc lại mô hình có thể đảo ngược [41] trong Hình 3a, người ta có thể tái tạo đầu vào từ đầu ra như sau:

h¹n+1=λh¹n+Fn(h²n)
h²n+1=βh²n+Gn(h¹n+1)

h²n= (h²n+1− Gn(h¹n+1))/β
h¹n= (h¹n+1− Fn(h²n))/λ (2)

trong đó λ và β là các hệ số tỷ lệ. Về mặt lý thuyết, Fn và Gn có thể là hai hàm tùy ý (mạng con). Cho một mạng đa lớp có thể đảo ngược, các kích hoạt trung gian cho mỗi lớp trong quá trình truyền xuôi không cần thiết phải được lưu trữ. Người ta chỉ cần lưu trữ các đầu ra cuối cùng, sau đó tái tạo các kích hoạt trung gian và tính toán gradient từng lớp một theo cách ngược lại (Xem Listing 1 trong §Phụ lục). Theo cách này, dung lượng bộ nhớ cần thiết cho các kích hoạt có thể được giảm đáng kể và không có mối quan hệ với độ sâu của mô hình, tức là O(1) thay vì O(N).

Để nghiên cứu tính ổn định huấn luyện của mô hình có thể đảo ngược, chúng tôi chạy thí nghiệm trên RevViT [40]⁴. RevViT có cùng kiến trúc với Reformer [42], ngoại trừ việc áp dụng lớp convolutional ở đầu để chiếu một hình ảnh thành một chuỗi vector. Khi chạy RevViT, người ta vẫn có thể lưu trữ các kích hoạt trung gian và coi nó như một mô hình không thể đảo ngược. Chúng tôi gọi gradient được tính toán theo cách này là gradient vanilla. Người ta cũng có thể huấn luyện RevViT theo cách có thể đảo ngược, và gradient tương ứng được gọi là gradient có thể đảo ngược. Chúng tôi đưa cùng nhiễu ngẫu nhiên vào cùng RevViT hai lần để có được gradient tham số từ lớp convolutional, theo cách vanilla và có thể đảo ngược. Sau đó chúng tôi tính toán sự khác biệt tuyệt đối giữa hai gradient này và báo cáo các giá trị tối đa và trung bình. Theo cách này, chúng tôi muốn kiểm tra xem gradient vanilla có thể được tái tạo theo cách có thể đảo ngược hay không. Nếu lỗi tái tạo lớn, điều đó có nghĩa là gradient vanilla không thể được phục hồi theo cách có thể đảo ngược do tính ổn định số học, có thể gây ra huấn luyện không ổn định hoặc hiệu suất kém.

Như được hiển thị trong Hình 3b, với số lượng lớp tăng trong RevViT, lỗi tái tạo trở nên lớn hơn, nhưng vẫn khoảng 10⁻⁸ là không đáng kể. Tuy nhiên, RevViT nhạy cảm với các hệ số tỷ lệ, λ và β. Khi cả hai hệ số tỷ lệ hoặc một trong số chúng nhỏ hơn 1, lỗi tái tạo tăng đáng kể. Chúng tôi cũng khám phá việc khởi tạo các lớp tuyến tính trong RevViT và thấy rằng độ lệch chuẩn hoặc trung bình lớn hơn có thể gây ra lỗi tái tạo lớn hơn. Tóm lại, số lượng lớp lớn hơn, các hệ số tỷ lệ nhỏ hơn (<1) và độ lệch chuẩn hoặc trung bình lớn hơn cho việc khởi tạo có xu hướng gây ra lỗi tái tạo lớn hơn, có thể dẫn đến huấn luyện không ổn định hoặc hiệu suất thấp của mô hình có thể đảo ngược. Cuối cùng nhưng không kém phần quan trọng, RevViT [40] phát hiện rằng các kết nối residual bên trong F và G làm giảm hiệu suất của Transformer có thể đảo ngược [43]⁵.

3 Tinh chỉnh hiệu quả bộ nhớ

Bài báo này nhằm sửa đổi PLM thành biến thể có thể đảo ngược mà không cần tiền huấn luyện bổ sung, để PLM vẫn có thể được tinh chỉnh với dung lượng bộ nhớ hạn chế. Nguyên tắc hướng dẫn cơ bản đằng sau thiết kế của chúng tôi là: bảo tồn điểm khởi đầu từ PLM ở mức độ lớn nhất có thể (thảo luận trong §2.2). Trong phần này, chúng tôi đề xuất ba phương pháp để sửa đổi PLM thành mô hình có thể đảo ngược.

3.1 MEFT 1: Lớp PLM làm F, adapter làm G

Như được hiển thị trong Hình 4c, chúng tôi thiết kế F là lớp tiền huấn luyện với adapter, trong đó vị trí chèn cho adapter được mượn từ He et al. [18]. G đơn giản là một adapter. Chúng tôi khởi tạo các adapter là Wdown,Wup∼ N(0, σ²), tương tự cho các phương pháp sau. Theo cách này, đầu ra từ adapter gần với 0 ở đầu quá trình huấn luyện, do đó hn≈ Fn(hn−1). Đối với thảo luận sau, chúng tôi chỉ tập trung vào đầu quá trình huấn luyện, đảm bảo thiết kế của chúng tôi bảo tồn điểm khởi đầu từ PLM.

h0 và h1 là đầu vào và đầu ra từ lớp đầu tiên của PLM mà không có bất kỳ sửa đổi nào, tương ứng. Tức là h0 là biểu diễn sau các lớp embedding vị trí và từ của PLM. Chúng tôi gán h¹0=h²0=h0, tương tự cho các phương pháp sau. Ở đầu quá trình huấn luyện (xem Hình 4a), h¹1=λh¹0+F1(h²0) =λh0+F1(h0)≈λh0+h1, h²1=βh²0+G1(h¹1) =βh0+G1(h¹1)≈ βh0, trong đó phép tính gần đúng đúng vì việc khởi tạo adapter của chúng tôi.

Hiện tại, h¹1 và h²1 không như mong muốn. Khi chúng tôi đưa h¹1 và h²1 vào lớp có thể đảo ngược thứ 2, đặc biệt khi chúng tôi đưa h²1 vào F2, tính liên tục biểu diễn⁶ bị phá vỡ, vì h²1≠h1. Chúng tôi giới thiệu hai sửa đổi để giải quyết vấn đề này: (1) Chúng tôi đặt λ→0, để h¹1≈h1. (2) Sau đó chúng tôi đổi thứ tự của h¹1 và h²1 trước khi đưa vào lớp có thể đảo ngược tiếp theo, tức là làm cho h¹1≈βh0 và h²1≈h1. Theo cách này, h²1 bảo tồn điểm khởi đầu. Chúng tôi không yêu cầu h¹1 bảo tồn bất kỳ điểm khởi đầu nào, vì nó được đưa vào G2 không phải là lớp tiền huấn luyện.

Với cùng thiết kế nói trên cho lớp có thể đảo ngược thứ 2, chúng tôi có được h¹2≈βh1 và h²2≈h2. Tương tự, h¹n≈βhn−1 và h²n≈hn, có nghĩa là h²n luôn bảo tồn điểm khởi đầu từ PLM. Đưa h²n vào lớp có thể đảo ngược tiếp theo, Fn+1, không phá vỡ tính liên tục biểu diễn. Sau tất cả các lớp, chúng tôi đưa h'N= (h¹N+h²N)/2 vào head cụ thể cho nhiệm vụ là một lớp hoàn toàn mới, tương tự cho các phương pháp sau⁷.

3.2 MEFT 2: Adapter làm F, lớp PLM làm G

Ngược lại với MEFT 1, chúng tôi thiết kế F là adapter và G là lớp PLM với adapter cho MEFT 2 (xem Hình 4c). Trong trường hợp này, chúng tôi cần đảm bảo rằng đầu vào của G bảo tồn điểm khởi đầu. Hãy cũng bắt đầu với lớp đầu tiên, h¹1=λh¹0+F1(h²0) =λh0+F1(h0)≈λh0, h²1=βh²0+G1(h¹1) =βh0+G1(h¹1)≈βh0+G1(λh0), trong đó phép tính gần đúng đúng vì việc khởi tạo adapter của chúng tôi.

Để bảo tồn điểm khởi đầu từ PLM, chúng tôi đặt λ→1, β→0 và đổi thứ tự của h¹1 và h²1 trước khi đưa vào lớp có thể đảo ngược tiếp theo. Khi đặt λ→1, chúng tôi đảm bảo tính liên tục biểu diễn được bảo tồn cho G1, dẫn đến h²1≈βh0+h1. Khi β→0 và thứ tự của h¹1 và h²1 được đổi, h¹1≈h1 và h²1≈h0. Theo cách này, h¹1 bảo tồn điểm khởi tạo, và chúng tôi sẽ không phá vỡ tính liên tục biểu diễn khi đưa nó vào G2 trong lớp có thể đảo ngược tiếp theo. Với cùng cài đặt cho mỗi lớp, h¹n≈hn và h²n≈hn−1, do đó h¹n luôn bảo tồn điểm khởi đầu.

3.3 MEFT 3: Khối attention làm F, khối MLP làm G

Như được hiển thị trong Hình 4d, chúng tôi cũng có thể thiết kế F là khối attention tiền huấn luyện với adapter và G là khối MLP tiền huấn luyện với adapter. Cũng bắt đầu với lớp đầu tiên, chúng tôi có được h¹1=λh¹0+F1(h²0) =λh0+F1(h0), h²1=βh²0+G1(h¹1) =βh0+G1(h¹1).

λ→0 là cần thiết, do đó h¹1 tương tự với đầu ra ban đầu từ khối attention tiền huấn luyện, và có thể được đưa vào G1 để bảo tồn điểm khởi đầu. β→0 cũng cần thiết, do đó h²1≈h1, và có thể được đưa vào F2 trong lớp có thể đảo ngược tiếp theo. Theo mặc định, chúng tôi đặt λ=β→0. Đối với MEFT 3, người ta không cần đổi thứ tự của h¹1 và h²1 trước khi đưa vào lớp có thể đảo ngược tiếp theo. Đối với mỗi lớp, h¹n gần với đầu ra ban đầu từ khối attention của lớp PLM tương ứng, và h²n≈hn.

So với RevNet vanilla [41] trong đó λ=β= 1, chúng tôi cẩn thận gán các giá trị khác nhau cho λ và β để bảo tồn điểm khởi đầu từ PLM, và đổi thứ tự của các đầu ra trước khi đưa vào lớp tiếp theo (nếu cần thiết) để bảo tồn tính liên tục biểu diễn. Chúng tôi tóm tắt các cài đặt cho cả ba phương pháp MEFT trong Bảng 1.

4 Thí nghiệm

4.1 Thiết lập thí nghiệm

Tập dữ liệu và đánh giá. Chúng tôi đánh giá MEFT trên tám nhiệm vụ biểu diễn chuỗi và năm nhiệm vụ chuỗi-đến-chuỗi. Tất cả các nhiệm vụ biểu diễn chuỗi đều từ benchmark GLUE [25]. Các nhiệm vụ chuỗi-đến-chuỗi là các benchmark hỏi đáp, bao gồm OpenBookQA [44], PIQA [45], ARC (dễ và thách thức) [46] và SciQ [47]. Chúng tôi hiển thị thống kê của các tập dữ liệu này trong Bảng 8 trong Phụ lục. Đối với benchmark GLUE, chúng tôi báo cáo độ chính xác trên MNLI, QQP, QNLI, SST-2, MRPC và RTE, hệ số tương quan Pearson trên STS-B (nếu không đề cập đặc biệt) và hệ số tương quan Matthews [48] trên CoLA. Chúng tôi báo cáo độ chính xác trên tất cả các nhiệm vụ hỏi đáp. Ngoài ra, chúng tôi báo cáo tất cả kết quả trên tập phát triển như baseline của chúng tôi.

Mô hình. Chúng tôi sử dụng các mô hình chỉ encoder (BERT base[1], RoBERTa large[2] và BART large encoder [26]) làm mô hình cơ bản cho tất cả các nhiệm vụ GLUE, và các mô hình chỉ decoder (OPT 1.3B và OPT 6.7B[9]) cho các nhiệm vụ hỏi đáp. (Xem Bảng 9 trong Phụ lục để biết chi tiết mô hình.)

Baseline. Baseline quan trọng nhất là tinh chỉnh đầy đủ (Full FT) cập nhật tất cả tham số. Houlsby Adapter (AdapterH) [14], Pfeiffer Adapter (AdapterP) [16], Prefix-Tuning [15] và LoRA [17] được chọn làm baseline PEFT. Ngoài ra, hai phương pháp PEFT thống nhất, MAM [18] và AutoPEFT [49], kết hợp nhiều phương pháp PEFT cũng được chọn làm baseline PEFT. Cuối cùng, hai phương pháp điều chỉnh dựa trên đặc trưng, Y-Tuning [20] và LST[21], nhằm giảm bộ nhớ huấn luyện phục vụ như baseline hiệu quả bộ nhớ. Chúng tôi báo cáo kết quả baseline từ các bài báo gốc nếu có thể.

Triển khai. Để hiệu quả tính toán, chúng tôi đặt β= 1 cho MEFT 1, λ= 1 cho MEFT 2, và chỉ điều chỉnh các hệ số cần thiết →0 (xem Bảng 1). Sau khi có được giá trị tối ưu, tức là 0.1, chúng tôi sử dụng giá trị này cho tất cả ba phương pháp MEFT, nhiệm vụ và backbone. Trên benchmark GLUE, chúng tôi quét tốc độ học trong {3, 4, 5} ·10⁻⁴, kích thước batch trong {16, 32} và số epoch trong {10, 20} cho các nhiệm vụ có >10k mẫu huấn luyện. Đối với các nhiệm vụ tài nguyên thấp có <10k mẫu huấn luyện, chúng tôi quét tốc độ học trong {5, 6, 7, 8} ·10⁻⁴, kích thước batch trong {16, 32} và số epoch trong {20, 40}. Các không gian tìm kiếm lưới này được lấy cảm hứng từ các baseline của chúng tôi, đặc biệt là LoRA [17]. Chúng tôi sử dụng cài đặt Adam [28] mặc định với tỷ lệ khởi động là 6%. Nếu hiệu suất của mô hình trên tập phát triển không được cải thiện trong 5 epoch, chúng tôi dừng huấn luyện. Chúng tôi chạy cùng một nhiệm vụ của một phương pháp trong không gian tìm kiếm lưới nói trên ba lần với các seed ngẫu nhiên khác nhau, chọn kết quả tốt nhất từ mỗi lần chạy, và báo cáo trung bình và độ lệch chuẩn của các kết quả tốt nhất này. Đối với tất cả các nhiệm vụ hỏi đáp, chúng tôi quét tốc độ học trong {1, 3, 5, 7} ·10⁻⁴, kích thước batch trong {8, 16, 32} và số epoch trong {3, 5, 10}, và giữ các cài đặt khác như nhau, được lấy cảm hứng từ [50]. Độ dài chuỗi cho tất cả các nhiệm vụ được đặt là 512, 128, 128 và 128 cho BERT base, RoBERTa large, BART large và OPT như baseline của chúng tôi, tương ứng. Chúng tôi chạy tất cả thí nghiệm trên framework Transformers [34] trên một GPU NVIDIA RTX A6000 duy nhất với bộ nhớ 48GB. Nhìn chung, một lần chạy đơn của bất kỳ nhiệm vụ nào có thể được hoàn thành trong vòng 8 giờ, và hầu hết các nhiệm vụ có thể được hoàn thành trong một giờ. Cài đặt tinh chỉnh được tóm tắt trong Bảng 7.

4.2 Kết quả và thảo luận

Tầm quan trọng của việc khởi tạo MEFT. Ở đầu, chúng tôi tiếp tục kiểm tra giả thuyết điểm khởi đầu trên các MEFT của chúng tôi bằng cách điều chỉnh các hệ số tỷ lệ, λ và β. Như được mô tả bởi các đường đứt nét trong Hình 5, sự suy giảm hiệu suất rõ ràng khi các hệ số tỷ lệ khác biệt so với giá trị mong muốn là

--- TRANG 8 ---
0 (như được chỉ ra trong Bảng 1). Tuy nhiên, khi chúng đủ nhỏ (0.05 hoặc 0.1), kết quả thậm chí còn tốt hơn tinh chỉnh đầy đủ. Đối với hầu hết các phương pháp MEFT (MEFT 1 và MEFT 3), giá trị tối ưu cho các hệ số tỷ lệ là 0.1. Vì vậy chúng tôi sử dụng giá trị này cho tất cả các phương pháp MEFT trong các thí nghiệm sau.

MEFT với gradient vanilla là các phương pháp PEFT mạnh. Mặc dù MEFT có kiến trúc có thể đảo ngược, chúng ta vẫn có thể coi chúng như các mô hình không thể đảo ngược và lưu trữ các kích hoạt trung gian trong quá trình tinh chỉnh. Theo cách này, chúng đơn giản là các phương pháp PEFT. Trong Bảng 2, tất cả các phương pháp MEFT, sử dụng gradient vanilla, nhất quán vượt trội hơn cả tinh chỉnh đầy đủ và các phương pháp tiếp cận baseline khác với một biên độ đáng kể. Ví dụ, MEFT 3 vượt trội hơn Full FT 1% và baseline PEFT tốt nhất (AutoPEFT) 0.7% trên BERT base. MEFT 1 vượt trội hơn Full FT 0.7% trên RoBERTa large.

Khoảng cách hiệu suất của MEFT giữa gradient vanilla và có thể đảo ngược. Trong Hình 5, kết quả của MEFT với gradient có thể đảo ngược (đường liền) thường thấp hơn những kết quả với gradient vanilla (đường đứt nét). Nhắc lại thảo luận trong §2.3, các hệ số tỷ lệ nhỏ hơn (<1) và các kết nối residual trong F và G có thể gây ra lỗi tái tạo lớn hơn do tính ổn định số học. Khi sửa đổi PLM, chúng ta không thể loại bỏ các kết nối residual khỏi nó và phải đặt các hệ số tỷ lệ →0 do giả thuyết điểm khởi đầu, điều mà chúng tôi tin là lý do chính cho sự sụt giảm hiệu suất. Tuyên bố của chúng tôi được hỗ trợ thêm bởi MEFT 3 có sự sụt giảm rõ ràng nhất trong tất cả các MEFT. So với MEFT 1 và MEFT 2 chỉ có kết nối residual trong F hoặc G, cả F và G của MEFT 3 đều có kết nối residual. Ngoài ra, chúng ta phải đặt cả λ và β gần với 0 cho MEFT 3, điều này cũng gây ra lỗi tái tạo lớn hơn so với chỉ đặt một hệ số tỷ lệ (xem Hình 3b giữa). Vì MEFT 3 với gradient có thể đảo ngược thực hiện tệ nhất trong tất cả các MEFT, chúng tôi chỉ chạy nó trên BERT base do tài nguyên hạn chế. Như mong đợi, MEFT 1 được huấn luyện trong FP32 vượt trội hơn khi được huấn luyện trong FP16 trên cả RoBERTa large và BART large (xem Bảng 2), vì FP16 gây ra thêm sự bất ổn định.

MEFT có thể đảo ngược trên mô hình sâu. Do giả thuyết điểm khởi đầu, kết nối residual từ PLM vẫn còn và các hệ số tỷ lệ được đặt gần với 0. Với số lượng lớp tăng, sự bất ổn định huấn luyện dự kiến sẽ trở nên nghiêm trọng hơn (xem Hình 3b trái). Như được hiển thị trong Hình 6, khi tất cả các lớp RoBERTa có thể đảo ngược (số lượng lớp có thể đảo ngược là 24), điểm số giảm đáng kể. Để giải quyết vấn đề này, chúng tôi đề xuất ba cài đặt trong Hình 7: (1) Lưu trữ các kích hoạt cho các lớp trên (gradient vanilla) và áp dụng các lớp nông có thể đảo ngược (gradient có thể đảo ngược). (2) Đóng băng một số lớp PLM nông, tức là coi các lớp nông như một bộ trích xuất đặc trưng. (3) Kết hợp hai cài đặt trên. Đáng chú ý, chúng ta phải đặt các lớp có thể đảo ngược dưới các lớp vanilla do tính ổn định số học. Nếu chúng ta đảo ngược thứ tự, lỗi tái tạo được chuyển đến các lớp vanilla.

Chúng tôi chỉ khám phá hai cài đặt đầu tiên trên RoBERTa và sẽ thảo luận về cài đặt thứ ba trong phần sau, vì RoBERTa large không chứa nhiều lớp. Trong Hình 6, khi chúng tôi áp dụng cài đặt đầu tiên (freeze: false) cho RoBERTa large, điểm số trung bình trở nên tốt hơn khi số lượng lớp có thể đảo ngược giảm, vượt trội hơn tinh chỉnh đầy đủ khi nó ≤16. Tuy nhiên, bộ nhớ kích hoạt cũng tăng với số lượng lớp vanilla tăng, vì các lớp vanilla yêu cầu lưu trữ các kích hoạt. Theo mặc định, chúng tôi đặt số lượng lớp có thể đảo ngược là 16 cho RoBERTa large trong Bảng 2. Đối với cài đặt thứ hai (freeze: true), kết quả luôn tệ hơn tinh chỉnh đầy đủ. Tuy nhiên, bộ nhớ kích hoạt của nó vẫn như nhau vì tất cả các lớp có thể huấn luyện đều có thể đảo ngược.

MEFT có hiệu quả tham số và bộ nhớ với hiệu suất mạnh. Hãy quay lại Bảng 2. Mặc dù có khoảng cách trong MEFT giữa gradient vanilla và có thể đảo ngược, MEFT có thể đảo ngược vẫn đạt được kết quả mạnh so với các baseline trước đó. Trên BERT base, MEFT 1 và MEFT 2 có thể đảo ngược đạt được điểm số trung bình tương tự như Full FT, hơi tệ hơn phương pháp PEFT tốt nhất, AutoPEFT (83.2 so với 83.5). Tuy nhiên, MEFT có thể đảo ngược chỉ yêu cầu khoảng 21% và 24% bộ nhớ kích hoạt của Full FT và PEFT. Trên RoBERTa large, MEFT 1 có thể đảo ngược (FP32) đạt được điểm số tương tự như Full FT và vượt trội hơn tất cả các phương pháp PEFT, trong khi chỉ yêu cầu 37% và 48% bộ nhớ kích hoạt của Full FT và PEFT.

Do tài nguyên tính toán hạn chế, chúng tôi chỉ tiến hành thí nghiệm trên phương pháp MEFT tốt nhất, MEFT 1, trên BART large khi so sánh với các phương pháp hiệu quả bộ nhớ khác. Ngoài ra, chúng tôi không sử dụng không gian tìm kiếm lưới của riêng chúng tôi trên BART large. Thay vào đó, chúng tôi áp dụng cùng không gian tìm kiếm lưới như LST, đặt tốc độ học trong {3·10⁻⁴, 1·10⁻³, 3·10⁻³}, kích thước batch là 100 và số epoch là 20. Theo cách này, chúng tôi muốn xác nhận tính mạnh mẽ của MEFT. Tương tự, MEFT 1 vượt trội hơn các baseline hiệu quả bộ nhớ với biên độ lớn trong khi chỉ yêu cầu 29% bộ nhớ kích hoạt của LST. Ngoài ra, LST yêu cầu chưng cất kiến thức để khởi tạo các lớp được thêm vào và không ổn định [21].

MEFT được huấn luyện trong FP32 so với FP16, và sự đánh đổi thời gian-bộ nhớ. Mặc dù MEFT có thể đảo ngược được huấn luyện trong FP32 vượt trội hơn những mô hình được huấn luyện trong FP16, vẫn có một số thảo luận đáng chú ý về chúng: (1) Dung lượng bộ nhớ cần thiết bởi MEFT có thể đảo ngược được huấn luyện trong FP32 và FP16 là như nhau. Trong Bảng 2, MEFT 1 và MEFT 1(FP32) có cùng bộ nhớ kích hoạt trên BART large, vì các kích hoạt được tính toán lại trong lan truyền ngược luôn ở FP32 do huấn luyện độ chính xác hỗn hợp [51]. Tức là PyTorch [52] chỉ cho phép FP32 trong lan truyền ngược; (2) FP16 vẫn có lợi cho việc huấn luyện PLM lớn. Trong Bảng 2, sự khác biệt bộ nhớ đỉnh và kích hoạt là khoảng kích thước backbone trong FP32 cho PEFT và MEFT. Nếu người ta có thể giảm kích thước backbone bằng cách tải trong FP16, chúng ta có thể giảm thêm bộ nhớ đỉnh; (3) Huấn luyện trong FP16 nhanh hơn huấn luyện trong FP32 (khoảng 1:1.2) do truyền xuôi. Ngoài ra, vì MEFT có thể đảo ngược tính toán lại các kích hoạt, chúng yêu cầu nhiều thời gian huấn luyện hơn, khoảng gấp đôi thời gian huấn luyện cho MEFT với gradient vanilla.

Kết quả trên các mô hình lớn hơn và sâu hơn. Ở đây chúng tôi khám phá cài đặt thực tế hơn (cài đặt thứ ba trong Hình 7) trên các mô hình lớn hơn và sâu hơn, OPT 1.3B và OPT 6.7B, trong Bảng 3. Trên OPT 1.3B với 24 lớp, chúng tôi đặt số lượng lớp đóng băng, có thể đảo ngược và vanilla là 8. Trên OPT 6.7B với 32 lớp, chúng tôi sử dụng 8 lớp có thể đảo ngược và vanilla, giống như OPT 1.3B. Để so sánh công bằng, chúng tôi đóng băng 8 lớp PLM đầu tiên và sửa đổi 16 lớp còn lại với LoRA. MEFT 1 có thể so sánh với LoRA, trong khi chỉ yêu cầu 65% bộ nhớ kích hoạt của LoRA. Mặc dù hơi tệ hơn Full FT (-0.3%), bộ nhớ kích hoạt của MEFT 1

--- TRANG 9 ---
chỉ bằng một nửa so với Full FT. Khi sử dụng cùng bộ nhớ kích hoạt như Full FT bằng cách chạy trên OPT 6.7B, MEFT 1 vượt trội hơn Full FT với biên độ lớn.

Chuyển sang nhiệm vụ phân loại hình ảnh. Mặc dù chúng tôi chỉ tập trung vào các nhiệm vụ NLP, MEFT có thể được chuyển sang các nhiệm vụ khác, thậm chí các kiến trúc khác. Chúng tôi để việc chuyển MEFT sang các kiến trúc khác cho công việc tương lai, và ở đây áp dụng MEFT cho ViT [53] cho nhiệm vụ phân loại hình ảnh, tức là SVHN [27]. Chúng tôi tuân theo công thức huấn luyện chính từ AdaptFormer [54], ngoại trừ việc thay đổi tối ưu hóa từ SGD sang AdamW, đặt norm gradient tối đa là 0.3. Đối với các siêu tham số của MEFT 1, chúng tôi đặt r= 64 và λ= 0.3 (λ nhỏ hơn không ổn định cho huấn luyện). Tương tự như kết quả NLP, MEFT 1 đạt được độ chính xác có thể so sánh như AdaptFormer trong khi tiết kiệm một lượng lớn dung lượng bộ nhớ trong Bảng 4.

Để biết thêm kết quả về việc so sánh MEFT với gradient checkpointing, so sánh MEFT với các phương pháp lượng tử hóa, và kết hợp MEFT với các phương pháp hiệu quả bộ nhớ khác, vui lòng xem Phụ lục §E. Ngoài ra, do giới hạn trang, chúng tôi đặt các công trình liên quan chi tiết trong Phụ lục §A, và thảo luận về hạn chế của công việc chúng tôi trong Phụ lục §B.

5 Kết luận

Trong bài báo này, chúng tôi đề xuất ba phương pháp tinh chỉnh hiệu quả bộ nhớ (MEFT), tinh chỉnh PLM theo cách hiệu quả tham số và bộ nhớ mà không yêu cầu tiền huấn luyện bổ sung và phù hợp với hiệu suất của tinh chỉnh đầy đủ. MEFT sửa đổi kiến trúc PLM với các adapter và làm cho nó có thể đảo ngược, bằng cách tuân theo giả thuyết điểm khởi đầu là cần thiết cho PEFT. Vì vậy MEFT không yêu cầu lưu trữ các kích hoạt trung gian trong quá trình huấn luyện và giảm đáng kể dung lượng bộ nhớ được chiếm bởi các kích hoạt. Khi áp dụng MEFT cho các mô hình khác nhau, BERT, RoBERTa và BART, trên benchmark GLUE, MEFT đạt được điểm số tương tự như tinh chỉnh đầy đủ và các baseline mạnh khác, trong khi tiết kiệm lên đến 84% bộ nhớ kích hoạt. Một câu chuyện tương tự cũng được quan sát thấy khi áp dụng MEFT cho các mô hình lớn hơn và sâu hơn, OPT, trên năm nhiệm vụ hỏi đáp. MEFT đạt được điểm số có thể so sánh như tinh chỉnh đầy đủ và chỉ tiêu thụ 50% bộ nhớ kích hoạt của nó. Tuy nhiên, do việc tính toán lại các kích hoạt, MEFT yêu cầu thời gian huấn luyện hơi nhiều hơn các phương pháp PEFT khác và đưa ra điểm số hơi thấp hơn khi được huấn luyện trong FP16 thay vì FP32. Trong tương lai, chúng tôi quan tâm đến việc áp dụng MEFT cho các lĩnh vực khác, như thị giác máy tính và nhận dạng giọng nói tự động, và cho các backbone lớn hơn khác cho nhiều nhiệm vụ chuỗi-đến-chuỗi.

Lời cảm ơn

Chúng tôi cảm ơn tất cả các reviewer về phản hồi tuyệt vời của họ. Chúng tôi cũng cảm ơn đồng nghiệp Yan Meng về bài đánh giá hữu ích của cô ấy về bản thảo của chúng tôi. Nghiên cứu này được tài trợ một phần bởi Tổ chức Khoa học Hà Lan (NWO) theo số dự án VI.C.192.080.

--- TRANG 11 ---
Tài liệu tham khảo

[1]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: tiền huấn luyện các transformer hai chiều sâu để hiểu ngôn ngữ tự nhiên. Trong Jill Burstein, Christy Doran, và Thamar Solorio, biên tập, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), trang 4171–4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.

[2]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: Một phương pháp tiền huấn luyện BERT được tối ưu hóa mạnh mẽ. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.

[3]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Khám phá giới hạn của học chuyển giao với một transformer văn bản-đến-văn bản thống nhất. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

[4]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross B. Girshick. Các bộ mã hóa tự động có mặt nạ là các học viên thị giác có thể mở rộng. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, trang 15979–15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10.1109/CVPR52688.2022.01553.

[5]Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, và Han Hu. Simmim: một framework đơn giản cho mô hình hóa hình ảnh có mặt nạ. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, trang 9643–9653. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00943. URL https://doi.org/10.1109/CVPR52688.2022.00943.

[6]Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, và Michael Auli. wav2vec 2.0: Một framework để học tự giám sát các biểu diễn giọng nói. Trong Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, và Hsuan-Tien Lin, biên tập, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html.

[7]Jiasen Lu, Dhruv Batra, Devi Parikh, và Stefan Lee. Vilbert: Tiền huấn luyện các biểu diễn visiolinguistic không phụ thuộc nhiệm vụ cho các nhiệm vụ thị giác và ngôn ngữ. Trong Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, và Roman Garnett, biên tập, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, trang 13–23, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html.

[8]Hao Tan và Mohit Bansal. LXMERT: học các biểu diễn bộ mã hóa đa phương thức từ transformer. Trong Kentaro Inui, Jing Jiang, Vincent Ng, và Xiaojun Wan, biên tập, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, trang 5099–5110. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1514. URL https://doi.org/10.18653/v1/D19-1514.

[9]Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. OPT: các mô hình ngôn ngữ transformer tiền huấn luyện mở. CoRR, abs/2205.01068, 2022. doi: 10.48550/arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068.

[10] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel

--- TRANG 12 ---
Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, và et al. BLOOM: Một mô hình ngôn ngữ đa ngôn ngữ truy cập mở 176b-tham số. CoRR, abs/2211.05100, 2022. doi: 10.48550/arXiv.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100.

[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.

[12] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, và Donald Metzler. Thống nhất các mô hình học ngôn ngữ. CoRR, abs/2205.05131, 2022. doi: 10.48550/arXiv.2205.05131. URL https://doi.org/10.48550/arXiv.2205.05131.

[13] Baohao Liao, Yan Meng, và Christof Monz. Tinh chỉnh hiệu quả tham số mà không giới thiệu độ trễ mới. Trong Anna Rogers, Jordan L. Boyd-Graber, và Naoaki Okazaki, biên tập, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, trang 4242–4260. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.233. URL https://doi.org/10.18653/v1/2023.acl-long.233.

[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Học chuyển giao hiệu quả tham số cho NLP. Trong Kamalika Chaudhuri và Ruslan Salakhutdinov, biên tập, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 của Proceedings of Machine Learning Research, trang 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html.

[15] Xiang Lisa Li và Percy Liang. Prefix-tuning: Tối ưu hóa các lời nhắc liên tục để tạo sinh. Trong Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli, biên tập, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, trang 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021.acl-long.353.

[16] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. Adapterfusion: Tổ hợp nhiệm vụ không phá hủy để học chuyển giao. Trong Paola Merlo, Jörg Tiedemann, và Reut Tsarfaty, biên tập, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, trang 487–503. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.eacl-main.39. URL https://doi.org/10.18653/v1/2021.eacl-main.39.

[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Thích ứng hạng thấp của các mô hình ngôn ngữ lớn. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.

[18] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. Hướng tới một cái nhìn thống nhất về học chuyển giao hiệu quả tham số. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.

[19] Brian Lester, Rami Al-Rfou, và Noah Constant. Sức mạnh của quy mô cho điều chỉnh lời nhắc hiệu quả tham số. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih, biên tập, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,

--- TRANG 13 ---
2021, trang 3045–3059. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.243. URL https://doi.org/10.18653/v1/2021.emnlp-main.243.

[20] Yitao Liu, Chenxin An, và Xipeng Qiu. Y-tuning: Một mô hình điều chỉnh hiệu quả cho các mô hình tiền huấn luyện quy mô lớn thông qua học biểu diễn nhãn. CoRR, abs/2202.09817, 2022. URL https://arxiv.org/abs/2202.09817.

[21] Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. LST: điều chỉnh bên thang để học chuyển giao hiệu quả tham số và bộ nhớ. Trong NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/54801e196796134a2b0ae5e8adef502f-Abstract-Conference.html.

[22] Geoffrey E. Hinton, Oriol Vinyals, và Jeffrey Dean. Chưng cất kiến thức trong một mạng neural. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531.

[23] Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, một phiên bản chưng cất của BERT: nhỏ hơn, nhanh hơn, rẻ hơn và nhẹ hơn. CoRR, abs/1910.01108, 2019. URL http://arxiv.org/abs/1910.01108.

[24] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Yu Qiao. Llama-adapter: Tinh chỉnh hiệu quả của các mô hình ngôn ngữ với attention khởi tạo không. CoRR, abs/2303.16199, 2023. doi: 10.48550/arXiv.2303.16199. URL https://doi.org/10.48550/arXiv.2303.16199.

[25] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. GLUE: Một benchmark đa nhiệm vụ và nền tảng phân tích để hiểu ngôn ngữ tự nhiên. Trong 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.

[26] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. BART: tiền huấn luyện chuỗi-đến-chuỗi khử nhiễu để tạo sinh ngôn ngữ tự nhiên, dịch thuật và hiểu. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel R. Tetreault, biên tập, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, trang 7871–7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL https://doi.org/10.18653/v1/2020.acl-main.703.

[27] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, và Andrew Y. Ng. Đọc chữ số trong hình ảnh tự nhiên với học đặc trưng không giám sát. Trong NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.

[28] Diederik P. Kingma và Jimmy Ba. Adam: Một phương pháp tối ưu hóa ngẫu nhiên. Trong Yoshua Bengio và Yann LeCun, biên tập, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.

[29] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin Raffel. Tinh chỉnh hiệu quả tham số few-shot tốt hơn và rẻ hơn học trong ngữ cảnh. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, biên tập, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=rBCvMG-JsPd.

[30] Ido Dagan, Oren Glickman, và Bernardo Magnini. Thách thức nhận dạng kéo theo văn bản PASCAL. Trong Joaquin Quiñonero Candela, Ido Dagan, Bernardo Magnini, và Florence d'Alché-Buc, biên tập, Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, volume 3944 của Lecture Notes in Computer Science, trang 177–190. Springer, 2005. doi: 10.1007/11736790_9. URL https://doi.org/10.1007/11736790_9.

[31] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, và Danilo Giampiccolo. Thách thức nhận dạng kéo theo văn bản pascal thứ hai. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 01 2006.

[32] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, và Bill Dolan. Thách thức nhận dạng kéo theo văn bản PASCAL thứ ba. Trong Satoshi Sekine, Kentaro Inui, Ido Dagan,

--- TRANG 14 ---
Bill Dolan, Danilo Giampiccolo, và Bernardo Magnini, biên tập, Proceedings of the ACL-PASCAL@ACL 2007 Workshop on Textual Entailment and Paraphrasing, Prague, Czech Republic, June 28-29, 2007, trang 1–9. Association for Computational Linguistics, 2007. URL https://aclanthology.org/W07-1401/.

[33] Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, và Danilo Giampiccolo. Thách thức nhận dạng kéo theo văn bản PASCAL thứ năm. Trong Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November 16-17, 2009. NIST, 2009. URL https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf.

[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander Rush. Transformers: Xử lý ngôn ngữ tự nhiên tiên tiến. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.

[35] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, và Sayak Paul. Peft: Các phương pháp tinh chỉnh hiệu quả tham số tiên tiến. https://github.com/huggingface/peft, 2022.

[36] Armen Aghajanyan, Sonal Gupta, và Luke Zettlemoyer. Tính chiều nội tại giải thích hiệu quả của việc tinh chỉnh mô hình ngôn ngữ. Trong Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli, biên tập, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, trang 7319–7328. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.568. URL https://doi.org/10.18653/v1/2021.acl-long.568.

[37] Alex Warstadt, Amanpreet Singh, và Samuel R. Bowman. Các phán đoán chấp nhận của mạng neural. Trans. Assoc. Comput. Linguistics, 7:625–641, 2019. doi: 10.1162/tacl_a_00290. URL https://doi.org/10.1162/tacl_a_00290.

[38] Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio, và Lucia Specia. Semeval-2017 nhiệm vụ 1: Tương tự văn bản ngữ nghĩa - đánh giá tập trung đa ngôn ngữ và xuyên ngôn ngữ. CoRR, abs/1708.00055, 2017. URL http://arxiv.org/abs/1708.00055.

[39] William B. Dolan và Chris Brockett. Tự động xây dựng một kho ngữ liệu của các cặp câu paraphrase. Trong Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing, 2005. URL https://aclanthology.org/I05-5002/.

[40] Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, và Jitendra Malik. Transformer thị giác có thể đảo ngược. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, trang 10820–10830. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01056. URL https://doi.org/10.1109/CVPR52688.2022.01056.

[41] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, và Roger B. Grosse. Mạng residual có thể đảo ngược: Lan truyền ngược mà không lưu trữ các kích hoạt. Trong Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett, biên tập, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 2214–2224, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html.

[42] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: Transformer hiệu quả. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB.

[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention là tất cả những gì bạn cần. Trong Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett, biên tập, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

[44] Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Một bộ áo giáp có thể dẫn điện không? Một tập dữ liệu mới cho hỏi đáp sách mở. Trong Ellen Riloff, David Chiang, Julia Hockenmaier, và Jun'ichi Tsujii, biên tập, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, trang 2381–2391. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1260. URL https://doi.org/10.18653/v1/d18-1260.

[45] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. PIQA: lý luận về common sense vật lý trong ngôn ngữ tự nhiên. Trong The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, trang 7432–7439. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239.

[46] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Bạn nghĩ rằng bạn đã giải quyết hỏi đáp? hãy thử arc, thách thức lý luận AI2. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.

[47] Johannes Welbl, Nelson F. Liu, và Matt Gardner. Crowdsourcing các câu hỏi khoa học nhiều lựa chọn. Trong Leon Derczynski, Wei Xu, Alan Ritter, và Tim Baldwin, biên tập, Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, trang 94–106. Association for Computational Linguistics, 2017. doi: 10.18653/v1/w17-4413. URL https://doi.org/10.18653/v1/w17-4413.

[48] Brian W Matthews. So sánh cấu trúc thứ cấp được dự đoán và quan sát của lysozyme phage t4. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975.

[49] Han Zhou, Xingchen Wan, Ivan Vulic, và Anna Korhonen. Autopeft: Tìm kiếm cấu hình tự động cho tinh chỉnh hiệu quả tham số. CoRR, abs/2301.12132, 2023. doi: 10.48550/arXiv.2301.12132. URL https://doi.org/10.48550/arXiv.2301.12132.

[50] Guangxuan Xiao, Ji Lin, và Song Han. Offsite-tuning: Học chuyển giao mà không có mô hình đầy đủ. CoRR, abs/2302.04870, 2023. doi: 10.48550/arXiv.2302.04870. URL https://doi.org/10.48550/arXiv.2302.04870.

[51] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, và Hao Wu. Huấn luyện độ chính xác hỗn hợp. CoRR, abs/1710.03740, 2017. URL http://arxiv.org/abs/1710.03740.

[52] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, và Adam Lerer. Tự động vi phân trong pytorch. 2017.

[53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. Một hình ảnh đáng giá 16x16 từ: Transformer cho nhận dạng hình ảnh ở quy mô. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.

[54] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, và Ping Luo. Adaptformer: Điều chỉnh transformer thị giác cho nhận dạng thị giác có thể mở rộng. Trong NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html.

[55] Baohao Liao, David Thulke, Sanjika Hewavitharana, Hermann Ney, và Christof Monz. Mặt nạ nhiều hơn và mặt nạ sau: Tiền huấn luyện hiệu quả của các mô hình ngôn ngữ có mặt nạ bằng cách tách token [MASK]. Trong Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang, biên tập, Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, trang 1478–1492. Association for Computational Linguistics, 2022.

--- TRANG 16 ---
doi: 10.18653/v1/2022.findings-emnlp.106. URL https://doi.org/10.18653/v1/2022.findings-emnlp.106.

[56] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, và Tie-Yan Liu. MASS: tiền huấn luyện chuỗi có mặt nạ đến chuỗi để tạo sinh ngôn ngữ. Trong Kamalika Chaudhuri và Ruslan Salakhutdinov, biên tập, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 của Proceedings of Machine Learning Research, trang 5926–5936. PMLR, 2019. URL http://proceedings.mlr.press/v97/song19d.html.

[57] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. Học biểu diễn xuyên ngôn ngữ không giám sát ở quy mô. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel R. Tetreault, biên tập, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, trang 8440–8451. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.747. URL https://doi.org/10.18653/v1/2020.acl-main.747.

[58] Alexis Conneau và Guillaume Lample. Tiền huấn luyện mô hình ngôn ngữ xuyên ngôn ngữ. Trong Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, và Roman Garnett, biên tập, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, trang 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html.

[59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Các mô hình ngôn ngữ là những học viên đa nhiệm vụ không giám sát. OpenAI blog, 1(8):9, 2019.

[60] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Các mô hình ngôn ngữ là những học viên few-shot. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.

[61] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Luật mở rộng cho các mô hình ngôn ngữ neural. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.

[62] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, và Laurent Sifre. Huấn luyện các mô hình ngôn ngữ lớn tối ưu tính toán. CoRR, abs/2203.15556, 2022. doi: 10.48550/arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556.

[63] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.

[64] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,

--- TRANG 17 ---
Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, và Quoc Le. Lamda: Các mô hình ngôn ngữ cho ứng dụng hội thoại. CoRR, abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.

[65] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. Các mô hình ngôn ngữ được tinh chỉnh là những học viên zero-shot. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR.

[66] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, và Colin Raffel. Tổng quát hóa xuyên ngôn ngữ thông qua tinh chỉnh đa nhiệm vụ. CoRR, abs/2211.01786, 2022. doi: 10.48550/arXiv.2211.01786. URL https://doi.org/10.48550/arXiv.2211.01786.

[67] Barret Zoph và Quoc V. Le. Tìm kiếm kiến trúc neural với học tăng cường. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=r1Ue8Hcxg.

[68] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, và Madian Khabsa. Unipelt: Một framework thống nhất cho điều chỉnh mô hình ngôn ngữ hiệu quả tham số. Trong Smaranda Muresan, Preslav Nakov, và Aline Villavicencio, biên tập, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, trang 6253–6264. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.433. URL https://doi.org/10.18653/v1/2022.acl-long.433.

[69] Tianqi Chen, Bing Xu, Chiyuan Zhang, và Carlos Guestrin. Huấn luyện mạng sâu với chi phí bộ nhớ dưới tuyến tính. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174.

[70] Jonathan Frankle và Michael Carbin. Giả thuyết vé số: Tìm kiếm các mạng neural thưa thớt, có thể huấn luyện. Trong 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.

[71] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, và Michael Carbin. Kết nối chế độ tuyến tính và giả thuyết vé số. Trong Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 của Proceedings of Machine Learning Research, trang 3259–3269. PMLR, 2020. URL http://proceedings.mlr.press/v119/frankle20a.html.

[72] Animesh Koratana, Daniel Kang, Peter Bailis, và Matei Zaharia. LIT: huấn luyện biểu diễn trung gian đã học để nén mô hình. Trong Kamalika Chaudhuri và Ruslan Salakhutdinov, biên tập, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 của Proceedings of Machine Learning Research, trang 3509–3518. PMLR, 2019. URL http://proceedings.mlr.press/v97/koratana19a.html.

[73] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, và Yuxiong He. Zero-offload: Dân chủ hóa huấn luyện mô hình tỷ tham số. Trong Irina Calciu và Geoff Kuenning, biên tập, 2021 USENIX Annual Technical Conference, USENIX ATC 2021, July 14-16, 2021, trang 551–564. USENIX Association, 2021. URL https://www.usenix.org/conference/atc21/presentation/ren-jie.

[74] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. GPTQ: lượng tử hóa sau huấn luyện chính xác cho các transformer sinh tạo tiền huấn luyện. CoRR, abs/2210.17323, 2022. doi: 10.48550/arXiv.2210.17323. URL https://doi.org/10.48550/arXiv.2210.17323.

--- TRANG 18 ---
[75] Yelysei Bondarenko, Markus Nagel, và Tijmen Blankevoort. Hiểu và vượt qua các thách thức của lượng tử hóa transformer hiệu quả. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih, biên tập, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, trang 7947–7969. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.627. URL https://doi.org/10.18653/v1/2021.emnlp-main.627.

[76] Tim Dettmers, Mike Lewis, Sam Shleifer, và Luke Zettlemoyer. Tối ưu hóa 8-bit thông qua lượng tử hóa theo khối. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=shpkpVXzo3h.

[77] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Tinh chỉnh hiệu quả của llm được lượng tử hóa. CoRR, abs/2305.14314, 2023. doi: 10.48550/arXiv.2305.14314. URL https://doi.org/10.48550/arXiv.2305.14314.

[78] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. Deepspeed: Tối ưu hóa hệ thống cho phép huấn luyện các mô hình học sâu với hơn 100 tỷ tham số. Trong Rajesh Gupta, Yan Liu, Jiliang Tang, và B. Aditya Prakash, biên tập, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, trang 3505–3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703.

--- TRANG 19 ---
A Các công trình liên quan

Các mô hình ngôn ngữ tiền huấn luyện. PLM, được huấn luyện trên các tập dữ liệu rộng lớn cho một nhiệm vụ chung như dự đoán các từ bị che [1,2,3,26,55,56,57,58] hoặc từ tiếp theo [59,60] trong một câu, đóng vai trò quan trọng trong việc tạo điều kiện cho việc chuyển giao kiến thức đến các nhiệm vụ downstream. Chúng đã thể hiện thành tích đáng chú ý trong nhiều ứng dụng khác nhau, liên tục mang lại kết quả tiên tiến. Hơn nữa, việc mở rộng quy mô PLM đã được chứng minh là mang lại những cải tiến có thể dự đoán được về hiệu suất cho các nhiệm vụ downstream này [61,62]. Do đó, kích thước của các PLM được phát hành đã tăng trưởng liên tục, đạt quy mô chưa từng có là 100 tỷ tham số [9,10,11,60,63,64]. Các PLM quy mô lớn như vậy tiết lộ khả năng phi thường, cho phép học zero-shot hoặc trong ngữ cảnh [59, 60] cho một phổ rộng các nhiệm vụ. Tuy nhiên, học chuyển giao vẫn là một phương pháp tiếp cận phổ biến để triển khai hiệu quả các mô hình này trong các tình huống nhiệm vụ mới [29,65,66], điều này đặt ra yêu cầu tài nguyên tính toán chưa từng có.

Tinh chỉnh hiệu quả tham số. Với sự xuất hiện của các PLM quy mô lớn, một phương pháp mới nhằm giảm yêu cầu lưu trữ, PEFT, đã được đề xuất [14,15,19]. PEFT thêm và huấn luyện một số lượng nhỏ tham số trong khi phù hợp với hiệu suất của tinh chỉnh đầy đủ. Có nhiều cách khác nhau để thêm tham số mới. Ví dụ, Houlsby et al. [14] và Pfeiffer et al. [16] chèn các module bottleneck nhỏ (adapter) vào PLM. LoRA [17] tiêm các ma trận phân tách hạng vào các trọng số tiền huấn luyện. HiWi [13] chèn các tham số tiền huấn luyện vào adapter hạng thấp. (IA)³[29] thu phóng trọng số tiền huấn luyện với một vector có thể huấn luyện. Các phương pháp dựa trên lời nhắc [15,19] thêm một chuỗi vector có thể huấn luyện vào word embedding hoặc các thành phần attention. Gần đây, một số phương pháp thống nhất, kết hợp nhiều phương pháp PEFT theo cách heuristic [18] hoặc với kỹ thuật tìm kiếm kiến trúc neural [49,67,68], cũng đã được đề xuất. Mặc dù PEFT tiết kiệm lưu trữ với biên độ lớn so với tinh chỉnh đầy đủ, chúng vẫn yêu cầu dung lượng bộ nhớ tương tự trong quá trình huấn luyện như tinh chỉnh đầy đủ [20, 21] vì bộ nhớ kích hoạt.

Huấn luyện hiệu quả bộ nhớ. Huấn luyện hiệu quả bộ nhớ nhằm giảm dung lượng bộ nhớ trong quá trình huấn luyện. Các mạng neural có thể đảo ngược [40,41,42] giảm bộ nhớ kích hoạt bằng cách tính toán lại các kích hoạt với các đầu ra trong quá trình lan truyền ngược. Gradient checkpointing [69] đánh đổi tính toán cho bộ nhớ bằng cách loại bỏ một số kích hoạt trung gian và phục hồi chúng từ một lượt truyền xuôi bổ sung. Bộ nhớ kích hoạt là O(1) và O(√N) cho các mạng neural có thể đảo ngược và gradient checkpointing, tương ứng. MEFT là phương pháp đầu tiên được đề xuất để sửa đổi PLM thành biến thể có thể đảo ngược. Khi áp dụng MEFT trên mô hình sâu hơn, người ta có thể sử dụng gradient checkpointing để giảm thêm bộ nhớ kích hoạt cho các lớp với gradient vanilla.

Nén mạng, như pruning [70,71] và chưng cất kiến thức [22,23,72], tiết kiệm dung lượng bộ nhớ cho cả huấn luyện và suy luận. Chúng nén PLM thành mô hình nhỏ hơn bằng cách xóa các tham số không quan trọng hoặc chưng cất kiến thức từ PLM sang mô hình nhỏ hơn. Coi PLM như một bộ trích xuất đặc trưng và tránh tính toán gradient của nó cũng là một cách hiệu quả để giảm bộ nhớ kích hoạt [20,21]. Tuy nhiên, các phương pháp này thường yêu cầu tiền huấn luyện bổ sung trước khi tinh chỉnh, hoặc đạt được hiệu suất thấp hơn so với tinh chỉnh đầy đủ khi sử dụng cùng PLM.

B Hạn chế

Chúng tôi thừa nhận hạn chế chính của công việc này là chúng tôi chỉ đánh giá các phương pháp đề xuất trên một lượng hạn chế các nhiệm vụ và không tiến hành thí nghiệm trên các mô hình encoder-decoder. Lý do chính cho số lượng nhiệm vụ hạn chế là tài nguyên tính toán của chúng tôi bị ràng buộc. Ngoài ra, tiêu chí chính cho việc lựa chọn các mô hình cơ bản là chúng tôi có thể tìm thấy nhiều baseline mạnh trên chúng mà không cần tái tạo. BERT và RoBERTa đáp ứng tiêu chí này rất tốt trên benchmark GLUE. Về mô hình encoder-decoder, gần đây có xu hướng rõ ràng áp dụng mô hình chỉ decoder trên các nhiệm vụ chuỗi-đến-chuỗi. Do đó, chúng tôi áp dụng OPT trong bài báo này và dự định bao gồm LLAMA [11] cho dữ liệu instruction-finetuning trong tương lai.

Một hạn chế khác của MEFT là điểm số thấp hơn khi được huấn luyện trong FP16 và trên mô hình sâu hơn. Chúng tôi đã thảo luận về vấn đề này trong §4.2. Tóm lại, lỗi tái tạo nhiều hơn được giới thiệu bởi FP16 do phạm vi số học của nó và bởi mô hình sâu hơn do tích lũy lỗi. May mắn thay, kết quả vẫn có thể so sánh với các baseline PEFT khi được huấn luyện trong FP16. Ngay cả khi được huấn luyện trong FP32, dung lượng bộ nhớ kích hoạt không tăng so với FP16. Người ta chỉ cần dành thêm thời gian huấn luyện trong FP32 khi sử dụng cùng kích thước batch như trong FP16 (khoảng 20% thời gian huấn luyện nhiều hơn). Tuy nhiên, vì MEFT giảm dung lượng bộ nhớ, kích thước batch lớn hơn trong quá trình huấn luyện là có thể, điều này có thể tiết kiệm một số thời gian huấn luyện. Đối với các mô hình sâu hơn, chúng tôi cung cấp cài đặt thực tế và hiệu quả trong Hình 7.

Cuối cùng nhưng không kém phần quan trọng, khi tinh chỉnh các mô hình lớn hơn, như OPT 1.3B và OPT 6.7B[9], dung lượng bộ nhớ đỉnh được chiếm bởi các tham số mô hình hơn là kích hoạt (xem Bảng 3). Người ta cần kết hợp các kỹ thuật khác với MEFT để giảm dung lượng bộ nhớ đỉnh, như tải mô hình trong FP16 hoặc thậm chí trong int8 thay vì FP32, kết hợp MEFT với ZeRO [73] như trong Bảng 6.

C Thiết kế từng bước cho MEFT 1

Để người đọc dễ hiểu, trong phần này, chúng tôi giải thích MEFT 1 từng bước. Trước tiên, hãy nhấn mạnh lại các nguyên tắc hướng dẫn cho thiết kế của chúng tôi: (1) Đối với mỗi lớp có thể đảo ngược, chúng ta phải có hai đầu vào và hai đầu ra như trong Hình 3a. (2) Chúng ta cần tuân theo giả thuyết điểm khởi đầu. Tức là bất cứ khi nào chúng ta sửa đổi lớp PLM, chúng ta cần đảm bảo lớp đã sửa đổi có đầu ra gần như tương tự với lớp PLM ban đầu nếu chúng ta đưa cùng đầu vào của lớp PLM ban đầu vào lớp đã sửa đổi ở đầu quá trình huấn luyện. Nếu các đầu ra không tương tự, chúng trở nên khác biệt hơn nữa sau nhiều lớp, phá vỡ việc khởi tạo của PLM.

Như được hiển thị trong Hình 8a, đối với lớp PLM đầu tiên, h0 là đầu vào và h1 là đầu ra. Trong Hình 8b, các đầu vào cho lớp có thể đảo ngược đầu tiên là h¹0=h²0=h0. Nhắc lại kiến trúc của F1 trong Hình 4c (trên), chúng tôi đơn giản chèn adapter song song với hai lớp feed-forward liên tiếp, và khởi tạo adapter là Wdown, Wup∼ N(0,0.02²), dẫn đến h1≈ F1(h²0) vì h²0=h0. Nếu chúng tôi đặt λ→0, h¹1=λh¹0+F1(h²0)≈h1. Theo cách này, h¹1 đóng vai trò bảo tồn điểm khởi đầu.

Bây giờ hãy xem xét h²1. Do việc khởi tạo adapter của chúng tôi, đầu ra từ G1 (G1 đơn giản là một adapter như trong Hình 4c (dưới)) gần với 0. Vì vậy h²1=βh²0+G1(h¹1)≈βh0+0=βh0. Sau khi đổi thứ tự của h¹1 và h²1, h¹1≈βh0 và h²1≈h1.

Đối với lớp có thể đảo ngược thứ hai, nếu chúng ta không đổi thứ tự của h¹1 và h²1, nó trông như Hình 8c. Đầu vào cho F2 là βh0, điều này phá vỡ tính liên tục biểu diễn của PLM vì đầu vào cho F2 tiền huấn luyện nên gần với h1. Nếu chúng ta đổi thứ tự của chúng như trong Hình 8d, chúng ta bảo tồn tính liên tục biểu diễn. Và nó dẫn đến h¹2=λβh0+F2(h1)≈h2 do λ→0 và h2≈ F2(h1). Tương tự như lớp có thể đảo ngược đầu tiên, h²2≈βh1. Sau khi đổi, h¹2≈βh1 và h²2≈h2. Tương tự, đối với lớp có thể đảo ngược thứ n, h¹n≈βhn−1 và h²n≈hn.

Sau lớp cuối cùng, chúng tôi đơn giản lấy trung bình của hai đầu ra là h'N= (h¹N+h²N)/2, và đưa h'N vào head cụ thể cho nhiệm vụ, như lớp phân loại. Quy trình thiết kế tương tự cho MEFT 2 và MEFT 3. Tóm lại, việc đổi thứ tự chủ yếu để bảo tồn tính liên tục biểu diễn, và việc đặt các hệ số tỷ lệ gần với 0 chủ yếu để bảo tồn điểm khởi đầu.

D Chi tiết triển khai của các nhiệm vụ hỏi đáp

So với các nhiệm vụ GLUE trong đó tất cả các nhiệm vụ đều là nhiệm vụ phân loại và các head phân loại được khởi tạo ngẫu nhiên, các nhiệm vụ hỏi đáp là các nhiệm vụ chuỗi-đến-chuỗi và cần lớp đầu ra tiền huấn luyện chia sẻ cùng tham số với lớp word embedding. Lớp đầu ra yêu cầu tính liên tục của biểu diễn. Tức là ở đầu quá trình huấn luyện, đầu vào cho lớp đầu ra, h'N, nên gần với hN. Do đó, chúng ta cần thực hiện sửa đổi cho h'N thay vì sử dụng h'N= (h¹N+h²N)/2.

Ở đây chúng tôi giới thiệu hệ số tỷ lệ mới γ và yêu cầu γ→0. Đối với MEFT 1, vì h²N≈hN (xem Bảng 1), chúng tôi đặt h'N=γh¹N+h²N≈h²N≈hN. Tương tự, h'N=h¹N+γh²N≈h¹N≈hN cho MEFT 2, và h'N=γh¹N+h²N≈h²N≈hN cho MEFT 3. Không có bất kỳ điều chỉnh nào, chúng tôi đặt γ= 0.1 như các hệ số tỷ lệ được điều chỉnh khác theo mặc định.

E Thêm kết quả

E.1 So sánh với gradient checkpointing

Trước đây, chúng tôi chỉ nêu lý thuyết rằng bộ nhớ kích hoạt cho mạng có thể đảo ngược và gradient checkpointing là O(1) và O(√N), tương ứng. Ngoài ra, chúng tôi không so sánh chi tiết thời gian huấn luyện của MEFT với PEFT. Ở đây chúng tôi cung cấp một số kết quả thực nghiệm để bạn hiểu rõ hơn.

Trong Hình 9, chúng tôi so sánh bộ nhớ kích hoạt và thông lượng giữa MEFT, LoRA với gradient checkpointing và Full FT với gradient checkpointing. Thông lượng cho cả ba phương pháp ở cùng mức, chênh lệch tối đa 12% giữa LoRA và MEFT khi độ dài chuỗi là 128 và kích thước batch là 32. Với độ dài chuỗi tăng, khoảng cách trở nên hẹp hơn đến 7.5%. Đáng chú ý, thông lượng cho LoRA không có gradient checkpointing là 52.7 mẫu/giây. Với gradient checkpointing, nó là 36.1 mẫu/giây, 69% thông lượng ban đầu. Đối với MEFT với cùng cài đặt, nó là 33.5 mẫu/giây, 64% thông lượng của LoRA không có gradient checkpointing. Tóm lại, thông lượng của MEFT ở cùng mức với LoRA với gradient checkpointing, và khoảng 64% thông lượng của LoRA không có gradient checkpointing. Ngoài ra, bộ nhớ kích hoạt của MEFT luôn là giới hạn dưới trong ba phương pháp này. Khoảng cách giữa LoRA với gradient checkpointing và MEFT trở nên lớn hơn với độ dài chuỗi và kích thước batch tăng.

E.2 So sánh với các phương pháp lượng tử hóa

Lượng tử hóa là phương pháp trực giao với MEFT, giảm dung lượng bộ nhớ của việc huấn luyện hoặc suy luận bằng cách giảm kích thước tham số xuống ít bit hơn và sử dụng phép nhân ma trận độ chính xác bit thấp. Có chủ yếu ba phương pháp lượng tử hóa khác nhau: (1) Lượng tử hóa sau huấn luyện

--- TRANG 22 ---
[74,75] lượng tử hóa mô hình đã huấn luyện sau tiền huấn luyện hoặc tinh chỉnh; (2) Tối ưu hóa bit thấp hơn [76] lưu trữ trạng thái tối ưu hóa với độ chính xác thấp hơn và chỉ khử lượng tử hóa nó cho việc tối ưu hóa, tương tự như huấn luyện độ chính xác hỗn hợp FP16 hoặc BF16 nhưng với bit thấp hơn; (3) LLM đóng băng bit thấp hơn với LoRA, tức là QLoRA [77], áp dụng lượng tử hóa 4-bit để nén LLM. Trong quá trình tinh chỉnh, QLoRA lan truyền ngược gradient thông qua LLM đóng băng được lượng tử hóa 4-bit vào các adapter hạng thấp. Đáng chú ý, kiểu dữ liệu tính toán cho QLoRA là BF16. Nó khử lượng tử hóa các trọng số thành kiểu dữ liệu tính toán để thực hiện các lượt truyền xuôi và ngược.

Ở một mức độ nào đó, cả ba phương pháp này đều trực giao với phương pháp của chúng tôi và có thể được kết hợp với MEFT: (1) Lượng tử hóa sau huấn luyện chủ yếu để tham khảo và có thể được áp dụng cho bất kỳ mô hình đã huấn luyện nào; (2) Adam 8-bit cũng có thể được áp dụng cho bất kỳ mô hình nào được huấn luyện dựa trên gradient; (3) QLoRA là sự kết hợp của (1) và (2). Đối với QLoRA, chúng tôi tiến hành một số thí nghiệm trên BERT base với cài đặt mặc định như Hình 9. Như được hiển thị trong Bảng 5, MEFT 1 tiết kiệm nhiều bộ nhớ kích hoạt nhất trong khi có thông lượng tương tự như LoRA với gradient checkpointing. Lý do cho bộ nhớ kích hoạt lớn hơn của QLoRA so với LoRA là nó có bước khử lượng tử hóa bổ sung, điều này cũng gây ra thông lượng nhỏ nhất của nó.

E.3 Kết hợp MEFT với ZeRO

ZeRO [73] tiết kiệm bộ nhớ bằng cách phân vùng các tham số và trạng thái tối ưu hóa của mô hình giữa các GPU hoặc giữa GPU và CPU. Phương pháp này trực giao với MEFT, vì MEFT tiết kiệm bộ nhớ từ các kích hoạt. Chúng tôi tiến hành một số thí nghiệm trên OPT 1.3B bằng cách kết hợp phương pháp của chúng tôi với DeepSpeed [78] ZeRO stage 3 offload các tham số của mô hình và trạng thái tối ưu hóa sang CPU. Như được hiển thị trong Bảng 6, ZeRO giảm đáng kể dung lượng bộ nhớ từ các tham số của mô hình, do đó giảm bộ nhớ đỉnh của MEFT từ 28.2GB xuống 6.4GB.

--- TRANG 23 ---
Bảng 8: Thống kê của các tập dữ liệu

Nhiệm vụ RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI-m MNLI-mm
#Huấn luyện 2.5k 3.7k 5.8k 8.6k 67.4k 104.7k 363.8k 392.7k
#Phát triển 0.3k 0.4k 1.5k 1k 0.9k 5.5k 40.4k 9.8k 9.8k

Nhiệm vụ OpenBookQA PIQA ARC-E ARC-C SciQ
#Huấn luyện 5.0k 16.1k 2.3k 1.1k 11.7k
#Phát triển 0.5k 3.1k 2.4k 1.2k 1k

Bảng 9: Thống kê của các mô hình

Mô hình #Tham số #Lớp d model Kích thước trong FP32 (GB)
BERT base 110M 12 768 0.4
BART large encoder 205M 12 1024 0.8
RoBERTa large 355M 24 1024 1.4
OPT 1.3B 1.3B 24 2048 5.2
OPT 6.7B 6.7B 32 4096 25.6

--- TRANG 24 ---
def backward_pass(self, y1, y2, dy1, dy2):
    with torch.enable_grad():
        y1.requires_grad = True
        # Các kích hoạt trung gian của G được lưu trữ
        g_y1 = self.G(y1)
        # Lấy gradient của y1
        g_y1.backward(dy2, retain_graph=True)
    
    with torch.no_grad():
        x2 = (y2 - g_y1) / self.x2_factor
        # Tiết kiệm bộ nhớ, tương tự cho phía dưới
        del g_y1, y2
        dy1 += y1.grad
        # Tiết kiệm bộ nhớ
        y1.grad = None
    
    with torch.enable_grad():
        x2.requires_grad = True
        # Các kích hoạt trung gian của F được lưu trữ
        f_x2 = self.F(x2)
        # Lấy gradient của x2
        f_x2.backward(dy1, retain_graph=False)
    
    with torch.no_grad():
        x1 = (y1 - f_x2) / self.x1_factor
        del f_x2, y1
        dy2 *= self.x2_factor
        # dy2 =dx2, tiết kiệm bộ nhớ bằng cách sử dụng cùng biến
        dy2 += x2.grad
        x2.grad = None
        # dy1 =dx1
        dy1 *= self.x1_factor
        x2 = x2.detach()
        return x1, x2, dy1, dy2

Listing 1: Lượt truyền ngược cho mỗi Lớp. Bộ nhớ đỉnh xảy ra tại Dòng 10 hoặc Dòng 25, tùy thuộc vào việc mạng con G lớn hơn F hay ngược lại. Trong mã, chúng tôi sử dụng x1, x2, y1, y2, x1_factor, x2_factor để đại diện cho h¹n−1, h²n−1, h¹n, h²n, λ và β, tương ứng.
