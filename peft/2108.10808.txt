# 2108.10808.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2108.10808.pdf
# File size: 6606241 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Greenformers: Improving Computation and Memory
Eciency in Transformer Models via Low-Rank
Approximation
by
SAMUEL CAHYAWIJAYA
A Thesis Submitted to
The Hong Kong University of Science and Technology
in Partial FulÔ¨Ållment of the Requirements for
the Degree of Master of Philosophy
in the Department of Electronic and Computer Engineering
August 2021, Hong KongarXiv:2108.10808v1  [cs.LG]  24 Aug 2021

--- PAGE 2 ---
Authorization
I hereby declare that I am the sole author of the thesis.
I authorize the Hong Kong University of Science and Technology to lend this thesis to
other institutions or individuals for the purpose of scholarly research.
I further authorize the Hong Kong University of Science and Technology to reproduce
the thesis by photocopying or by other means, in total or in part, at the request of other
institutions or individuals for the purpose of scholarly research.
SAMUEL CAHYAWIJAYA
24 August 2021
ii

--- PAGE 3 ---
Greenformers: Improving Computation and Memory
Eciency in Transformer Models via Low-Rank
Approximation
by
Samuel Cahyawijaya
This is to certify that I have examined the above M.Phil. thesis
and have found that it is complete and satisfactory in all respects,
and that any and all revisions required by
the thesis examination committee have been made.
Prof. Pascale FUNG, Thesis Supervisor
Prof. Andrew Wing On POON, Head of Department
Thesis Examination Committee
1. Prof. Stuart GIETEL-BASTEN Division of Social Science
2. Prof. Pascale FUNG Department of Electronic and Computer Engineering
3. Prof. Qifeng CHEN Department of Electronic and Computer Engineering
Department of Electronic and Computer Engineering
August 2021
iii

--- PAGE 4 ---
Acknowledgments
I would never have completed this work without the help from many people. First of
all, I thank my advisor, Professor Pascale FUNG, for her years of mentoring, advice, and
encouragement. I have learned from her how to develop, evaluate, express, and defend
my ideas. These skills are important for my later PhD study. I thank the members of
my thesis committee, Professor Qifeng Chen, and my thesis chairperson Professor Stuart
Gietel-Basten, for their insightful comments on improving this work.
I thank my colleagues in HKUST ‚Äì Dr. Genta Indra Winata, Andrea Madotto, Dai Wen-
liang, Yu Tiezheng, Xu Yan, Lin Zhaojiang, Zihan Liu, Etsuko Ishii, Yejin Bang, Dr. Xu
Peng, Dr. Ilona Christy Unarta, Kharis Setiasabda, Bryan Wilie, Karissa Vincentio, Jacque-
line Cheryl Sabrina, Darwin Lim, Kevin Chandra, and many others. We have Ô¨Ånished a
lot of valuable works and develop many insightful ideas altogether. In daily life, we have
been very good friends. Without them, my graduate study in HKUST would not be so
colorful. Last but not least, I thank my parents and my brothers, for their support and
encouragement along my MPhil study in HKUST.
iv

--- PAGE 5 ---
Table of Contents
Title Page ii
Authorization Page ii
Signature Page iii
Acknowledgments iv
Table of Contents v
List of Figures viii
List of Tables ix
Abstract x
Chapter 1 Introduction 1
1.1 Motivation and Research Problems 1
1.2 Thesis Outline 4
Chapter 2 Preliminaries and Related Work 6
2.1 Transformer Model 6
2.1.1 Transformer Components 7
2.1.2 Transformer Layers 8
2.2 EfÔ¨Åcient Transformer 9
2.2.1 General EfÔ¨Åciency Methods 9
2.2.2 EfÔ¨Åcient Transformer 12
2.3 Low-Rank Approximation and Matrix Factorization 16
2.3.1 Singular Value Decomposition 16
2.3.2 Non-negative Matrix Factorization 17
2.3.3 Pre-Training Low-Rank Matrix Factorization 18
v

--- PAGE 6 ---
Chapter 3 Greenformers: EfÔ¨Åcient Transformer Model via Low-Rank Approxima-
tion 20
3.1 Methodology 20
3.1.1 Low-Rank Transformer: EfÔ¨Åcient Transformer with Factorized Lin-
ear Projection 20
3.1.2 Linformer: EfÔ¨Åcient Transformer with Factorized Attention Mecha-
nism 23
3.2 Experimental Setup 25
3.3 Result and Discussion 26
3.3.1 EfÔ¨Åciency comparison to Transformer Model 26
3.3.2 Short and Long Sequence EfÔ¨Åciency 27
3.3.3 Size EfÔ¨Åciency 28
3.3.4 Effectiveness of LRT and Linformer model 29
3.3.5 Impact of Greenformers 29
3.4 Conclusion 31
Chapter 4 Low-Rank Transformer for Automatic Speech Recognition 32
4.1 Methodology 33
4.2 Experimental Setup 34
4.2.1 Dataset 34
4.2.2 Hyperparameters 35
4.2.3 Baselines 35
4.2.4 Training and Evaluation 35
4.3 Result and Discussion 36
4.3.1 Evaluation Performance 36
4.3.2 Space and Time EfÔ¨Åciency 37
4.4 Conclusion 38
Chapter 5 Linformer for Alzheimer‚Äôs Disease Risk Prediction 39
5.1 Preliminaries 41
5.2 Methodology 42
5.2.1 Sequence Representation 42
5.2.2 Subword Tokenization 43
5.3 Experimental Setup 45
vi

--- PAGE 7 ---
5.3.1 Dataset Construction 45
5.4 Result and Discussion 47
5.4.1 Evaluation Performance 47
5.4.2 Effects of Sequence Length 48
5.4.3 EfÔ¨Åciency Contribution 49
5.5 Conclusion 50
Chapter 6 Conclusion 51
vii

--- PAGE 8 ---
List of Figures
2.1 Transformer model encoder-decoder architecture 6
2.2 Illustration of scaled dot-product attention and multi-head attention 8
2.3 EfÔ¨Åciency Methods for Transformer 10
2.4 Methods for inference efÔ¨Åciency 10
2.5 Comparison of different DeepSpeed ZeRO allocation strategies 12
2.6 Illustration of random, Ô¨Åxed, and global attention patterns 14
2.7 Illustration of learnable pattern approaches 14
2.8 Illustration of recurrence approaches 15
2.9 Illustration of Singular Value Decomposition 17
2.10 Illustration of Non-negative Matrix Factorization 18
2.11 Categorization of Non-negative Matrix Factorization Algorithms 19
3.1 Low-Rank Transformer Architecture 21
3.2 Low-Rank Transformer Unit 22
3.3 Comparison of the attention mechanism in original transformer model and
Linformer model 24
3.4 Inference speed up and memory efÔ¨Åciency of LRT and Linformer com-
pared to the Transformer baseline 26
3.5 Speed up and memory compression ratios of LRT and Linformer 27
3.6 Size comparison of LRT, Linformer, and Transformer model across different
Rank 28
3.7 Training speed up and memory efÔ¨Åciency of LRT compared to the Trans-
former baseline 30
4.1 ConÔ¨Åguration of our VGGish network 33
4.2 Low-Rank Transformer Architecture 34
4.3 Training and validation losses on AiShell-1 data. [116] 37
5.1 DNA sequence representation for haploid and diploid chromosome 41
5.2 Mutation patterns in genomic sequence 42
5.3 SentencePiece with BPE subword tokenization 44
5.4 Performance of Linformer model across different pretraining steps 48
5.5 Performance of Linformer (200k) model across different length of non-
coding region. 49
viii

--- PAGE 9 ---
List of Tables
1.1 Cost of training a transformer-based model 2
1.2 Comparison of different efÔ¨Åciency methods 3
3.1 Evaluation comparison on MNIST dataset 29
3.2 Cost efÔ¨Åciency of applying Low-Rank Transformer to the pre-training phase
of BERTBASE model 30
4.1 Duration statistics for AiShell-1 and HKUST datasets 35
4.2 Results on AiShell-1 and HKUST test sets 36
4.3 Compression rate and inference speed-up of LRT vs. Transformer (large) 38
5.1 List of all diplotype tokens. 43
5.2 Finetuning result on Alzheimer‚Äôs disease prediction 48
5.3 Maximum sequence length with Linformer and Subword Tokenization 49
ix

--- PAGE 10 ---
Greenformers: Improving Computation and Memory
Eciency in Transformer Models via Low-Rank
Approximation
by
SAMUEL CAHYAWIJAYA
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology
ABSTRACT
In this thesis, we introduce Greenformers, a collection of model efÔ¨Åciency methods to
improve the model efÔ¨Åciency of the recently renowned transformer models with a low-
rank approximation approach. The development trend of deep learning models tends to
results in a more complex and larger model. Although it leads to a better and more ac-
curate prediction, the resulting model becomes even more costly, as it requires weeks of
training with a huge amount of GPU resources. Particularly, the size and computational
cost of transformer-based models have increased tremendously since its Ô¨Årst debut in 2017
from100 million parameters up to 1.6 trillion parameters in early 2021. This computa-
tionally hungry model also incurs a substantial cost to the environment and even reaches
an alarming level of carbon footprint. Some of these models are so massive that it is even
impossible to run the model without a GPU cluster.
Greenformers improve the model efÔ¨Åciency of transformer models by applying low-
rank approximation approaches. SpeciÔ¨Åcally, we propose a low-rank factorization ap-
proach to improve the efÔ¨Åciency of the transformer model called Low-Rank Transformer.
x

--- PAGE 11 ---
We further compare our model with an existing low-rank factorization approach called
Linformer. Based on our analysis, the Low-Rank Transformer model is suitable for im-
proving both the time and memory efÔ¨Åciency in processing short-sequence ( 6512) input
data, while the Linformer model is suitable for improving the efÔ¨Åciency in processing
long-sequence input data ( >512). We also show that Low-Rank Transformer is more suit-
able for on-device deployment, as it signiÔ¨Åcantly reduces the model size. Additionally,
we estimate that applying LRT to the existing BERT BASE model can signiÔ¨Åcantly reduce
the computational, economical, and environmental costs for developing such models by
more than 30% of its original costs.
Our Low-Rank Transformer can signiÔ¨Åcantly reduce the computational time and mem-
ory usage on the speech recognition task. SpeciÔ¨Åcally, our Low-Rank Transformer can
halve the size of the model and increase the speed by up to 1.35x in the GPU and 1.25x in
the CPU while maintaining the performance of the model compared to the original trans-
former model. Our Ô¨Ånding suggests that transformer models tend to be over-parameterized
and our Low-Rank Transformer can help to mitigate the over-parameterization problem,
yielding a more efÔ¨Åcient model with a better generalization.
Additionally, we extend the possibility of applying a low-rank approximation ap-
proach to a genomics study for Alzheimer‚Äôs disease risk prediction. We apply sequence
modeling techniques with the Linformer model to predict Alzheimer‚Äôs disease in the Chi-
nese cohort. We deÔ¨Åne our problem as a long sequence classiÔ¨Åcation problem with various
lengths up to 33,000 nucleotides long. Our result shows that Linformer models with Sub-
word Tokenization can process very long sequence data and boost the evaluation perfor-
mance by up to 5% AUC compared to the existing FDA-approved risk scoring model and
other deep learning variants. Based on our analysis, we further conclude that the choice
of tokenization approach can also provide a huge computation and memory efÔ¨Åciency as
much as the efÔ¨Åcient model approach, which makes consideration of choosing tokeniza-
tion approach more prominent for developing a more efÔ¨Åcient transformer model.
xi

--- PAGE 12 ---
CHAPTER 1
Introduction
1.1 Motivation and Research Problems
Starting from AlexNet [57] in 2012, deep learning models such as convolution neural
network, recurrent neural network, and transformer have made signiÔ¨Åcant progression
in various Ô¨Åelds. Along with its remarkable adoption and growth, the computational
cost required for developing a deep learning model also rises signiÔ¨Åcantly at an unprece-
dented rate. From 2012 to 2018, the computational cost required is estimated to increase by
300,000x [95]. From 2018 onward, the development of the transformer-based NLP model
has shown an even sharper trend. Starting with ELMo [84] with 100M parameters in
2018, followed by BERT [25] with 340M parameters and [88] with 1.5B parameters in
2019. Recently, two other gigantic models have been released: 1) GPT-3 [12] with 175B
parameters and 2) Switch Transformer [28] with 1.6T parameters. This enormous model
size requires a huge amount of computational cost. This computationally hungry model
also incurs a substantial cost to the environment and even reaches an alarming level of
carbon footprint [100]. Some of these models are so massive that it is even impossible to
run the model in real-time without a GPU cluster.
As shown in Table 1.1, the growing trend of transformer-based models is so massive.
Within just 3 years, the computational cost for training the most massive transformer-
based model has increase by around 20,000 times from 0.181 petaÔ¨Çop/s-day for training
the Transformer BIG[110] model to 3,640 petaÔ¨Çop/s-day for training the GPT-3 [12] model.
This enormous computational cost leads to a massive increase in terms of computation
cost, economic cost, and CO 2emission. For instance, in 2017, the price of developing
the original transformer models is less than $1,000 USD with less than 100 kg of CO 2
emission. While in 2020, GPT-3 model costs around $4,600,000 USD with about 552 tons
of CO 2emission. This massive growth of computational requirement of developing a
1

--- PAGE 13 ---
ModelRelease Compute Cost Economical Cost CO 2emission
Year (petaÔ¨Çop/s-day) (USD) (kg)
Transformer BASE 2017 0.0081$41 - $140411.84
Transformer BIG 2017 0.1811$289 - $981487.14
BERTBASE 2018 2.242$2,074 - $6,9124652.34
BERTLARGE 2018 8.962$8,296 - $27,648?2,609.2?
GPT-2 (1.5B) 2018 10 - 1003$12,902 - $43.0084N/A
GPT-3 2020 3,6403$4,600,000y552,0004
Table 1.1. Cost of training a transformer-based model.1Hernandez et al (2020) [42].
2A√üenmacher et al. (2019) [3].3Brown et al (2020) [12].4Strubell et al (2019) [100]. ?
is estimated based on the computational cost comparison to the BERT BASE model.yis
retrieved from1.
transformer-based model is concerning and has attracted considerable attention in recent
years.
Several responses have been made to address this problem and raise people‚Äôs aware-
ness to improve the efÔ¨Åciency of a deep learning model and reduce the overall carbon
footprint. SustainNLP is a shared task [111] released with the goal of building energy-
efÔ¨Åcient solutions for the NLP model. Schwartz et al. [95] explored a methodology to
measure efÔ¨Åciency and introduce the term Green AI which refers to AI research that fo-
cuses on increasing computational efÔ¨Åciency. Dodge et al. (2019) [26] introduces a method
for estimating the model performance as a function of computation cost. Strubell et al.
(2019) analyzes the expected carbon footprint of NLP research and provide actionable
suggestions to reduce computational costs and improve equity. Hernandez et al. (2020)
[42] shows the importance of both hardware and algorithmic efÔ¨Åciency in reducing the
computational cost of developing a deep learning model and provide recommendations
for reporting modeling efÔ¨Åciency in AI research. Recent benchmarks[105, 114, 14] are also
considering the model efÔ¨Åciency as one of the metrics to compare the models listed on the
benchmarks.
Different efforts have been done to improve the hardware and algorithm efÔ¨Åciency
of developing a deep learning model. As shown in Table 2 in Hernandez et al. (2020)
[42], 44x less computation cost is required to achieve the same level of performance as
AlexNet. While the architectural shift from recurrent neural network models such as
1https://lambdalabs.com/blog/demystifying-gpt-3/
2

--- PAGE 14 ---
MethodRequire Training Fine-T uning Inference
Prior Model EfÔ¨Åcient EfÔ¨Åcient EfÔ¨Åcient
Distillation 3 7 7 /3 3
Pruning 3 7 7 /3 3
Quantization 3 7 7 3
Fast Adaptation 7/3 7 3 7
Data Cleansing 7 3 3 7
Distributed Training 7 3 3 7
Mixed-Precision 7 3 3 3
EfÔ¨Åcient Model 7 3 3 3
Table 1.2. Comparison of different efÔ¨Åciency methods
Seq2seq [102] and GNMT [49] to transformer model [110] leads to an increase of com-
putational efÔ¨Åciency by 61x and 9x respectively. All these improvements are made pos-
sible through numbers of efÔ¨Åciency methods such as distillation [43, 124, 101], prun-
ing [61, 40, 66, 39, 31, 121], quantization [46, 124, 5], fast adaptation [29, 120, 117], data
cleansing [64, 56, 7], distributed training [6, 91, 90], mixed-precision [75], and efÔ¨Åcient
model [96, 20, 116, 118, 103, 112, 52, 18].
As shown in Table 1.2, depending on when it is applied, an efÔ¨Åciency method can
provide different effects to the computational cost on different modeling phases. Distil-
lation, pruning, and quantization can reduce the computational cost drastically during
the inference phase, but these methods require having a prior model which makes the
overall training cost higher. For distillation and pruning, the Ô¨Åne-tuning process could
also become more efÔ¨Åcient as the distillation and pruning can be applied beforehand.
Mixed-precision and efÔ¨Åcient models decrease the computational cost during both train-
ing, Ô¨Åne-tuning, and inference. Nevertheless applying mixed-precision during inference
might produce a slight inconsistent prediction as a result of rounding error. Unlike the
other approaches, fast adaptation, data cleansing, and distributed training do not reduce
the computational cost of the model, but these approaches can reduce the actual training
time during the training and/or Ô¨Åne-tuning in different ways. Fast adaptation methods
allow the model to learn a representation that can quickly adapt to a new task, hence re-
quiring only a fraction of data in the Ô¨Åne-tuning phase. Data cleansing makes training and
Ô¨Åne-tuning phases more efÔ¨Åcient by reducing the number of samples to be trained by the
model. Recent development in distributed training [6, 91, 90] allows better resource al-
3

--- PAGE 15 ---
location and distribution strategy which ends up with signiÔ¨Åcant memory reduction and
faster training time.
In this thesis, with the spirit to alleviate the enormous cost of developing transformer-
based models, we introduce Greenformers. Greenformers is a collection of efÔ¨Åcient meth-
ods for improving the efÔ¨Åciency of the transformer model in terms of computation cost,
memory cost, and/or the number of parameters. We focus our Greenformers on im-
proving the transformer model efÔ¨Åciency with low-rank approximation methods as low-
approximation can not only greatly improve both computational and memory efÔ¨Åciency,
but also reducing the number of the model parameters signiÔ¨Åcantly. SpeciÔ¨Åcally, we ex-
plore two different two low-rank model variants to increase the efÔ¨Åciency of a transformer-
based model: 1) low-rank factorized linear projection transformer called low-rank trans-
former (LRT) [116] and 2) low-rank factorized self-attention transformer called Linformer [112].
For LRT, we conduct our experiment on speech recognition tasks where we utilize low-
rank approximation to factorize the model parameters. With this approach, the model size
and computational cost can be decreased signiÔ¨Åcantly, yielding a more efÔ¨Åcient speech
recognition model. For Linformer, we conduct our experiment on long-sequence ge-
nomics data. SpeciÔ¨Åcally, we experiment on Alzheimer‚Äôs disease risk prediction in the
Chinese cohort. In this task, given a long genomics sequence, our goal is to predict the
risk of the subject of getting Alzheimer‚Äôs disease. We formulate this task as a sequence
classiÔ¨Åcation task with an input sequence length of 30,000 long. We evaluate our ap-
proach by comparing the result with the existing FDA-approved risk-scoring biomark-
ers. Additionally, we conduct deeper analysis to analyze the efÔ¨Åciency of our low-rank
transformer variants compared to the original transformer model and provide insights on
which low-rank variant works best given a certain input characteristic.
1.2 Thesis Outline
The contents of this thesis are focused on the application of efÔ¨Åcient modeling techniques
for transformer models via low-rank approximation. The rest of the thesis is divided into
four chapters and organized as follows:
Chapter 2 (Preliminaries and Related Work) introduces the background and impor-
4

--- PAGE 16 ---
tant preliminaries and related works on transformer model, efÔ¨Åciency methods, and
low-rank matrix approximation.
Chapter 3 (EfÔ¨Åcient Transformer Model via Low-Rank Approximation) presents two
low-rank transformer variants that reduce the memory usage and computational
cost of the original transformer model.
Chapter 4 (Low-Rank Transformer for Speech Recognition) shows the effectiveness
of low-rank transformer models in speech recognition tasks.
Chapter 5 (Linformer for Alzheimer‚Äôs Disease Risk Prediction) shows the applica-
bility of the efÔ¨Åcient Linformer model on long-genome understanding tasks for pre-
dicting Alzheimer‚Äôs disease in the Chinese cohort.
Chapter 6 (Conclusion) summarizes this thesis and the signiÔ¨Åcance of the low-rank
approximation in transformer models and discusses the possible future research di-
rections.
5

--- PAGE 17 ---
CHAPTER 2
Preliminaries and Related Work
2.1 Transformer Model
EmbeddingSelf AttentionAdd & NormAdd & Norm
Feed-Forward
N√ó
Transformer Encoder
Encoder InputEmbeddingSelf AttentionAdd & NormAdd & Norm
Feed-Forward
M√ó
Transformer Decoder
Decoder InputDecoder Output
Add & Norm
Cross AttentionEncoder Output
Figure 2.1. Transformer model encoder-decoder architecture.
Transformer is a neural network architecture based on the attention mechanism. Com-
pared to the recurrent neural network (RNN) [37], the transformer model can process
a sequence in parallel which enables the model to take full advantage of modern fast
computing devices such as TPUs and GPUs. A transformer model consists of a stack of
transformer layers where each layer consists of a self-attention and a position-wise feed-
forward network. To avoid gradient vanishing two residual connections are added, one
after the self-attention and another one after the feed-forward network. Normalization
layer is also added after the residual connection to stabilize the hidden state which allows
the model to converge faster [4]. The aforementioned transformer layer is known as a
6

--- PAGE 18 ---
transformer encoder layer. There is another kind of transformer layer, i.e, transformer de-
coder layer, which is very similar to transformer encoder layer with an additional cross-
attention in between the self-attention and the feed-forward network. The depiction of
transformer encoder layer, transformer decoder layer, and the two types of layer interact
is shown in Figure 2.3.
2.1.1 Transformer Components
Scaled Dot-Product Attention The attention-mechanism in the Transformer is computed
with scaled dot-product attention [110, 115]. The scaled dot-product attention accepts a
query sequence Q2RNdk, a key sequence K2RNdk, and a value sequence V2RNdv
as its input, and produce an output O2RNdv. Scaled dot-product attention is done by
Ô¨Årst Ô¨Ånding the similarity of QandKwith a dot-product operation scaled with a factor
of1pdkto reduce the magnitude, and then apply a softmax operation to get the probabil-
ity distribution over different locations. The probability distribution is then used as the
attention weight of Vto get the output sequence O. Scaled-dot product attention can be
formulated as:
Attention (Q,K,V) =softmax (QKT
pdk)V (2.1)
Multi-Head Attention The attention in Transformer is also multi-headed. Multi-head
attention split the dkanddvinQ,K, andVinto multiple heads hwith equal size. For
each head, the scaled dot-product attention is applied. The results are then concatenated
and projected to get the output O. Unlike single-head attention, Multi-head attention
allows the model to jointly attend to information from different representation subspaces
at different positions [110]. The depiction of scaled dot-product attention and multi-head
attention is shown in Figure 2.2.
Position-wise Feed Forward Network Position-wise feed-forward network is computed
after the self-attention in the transformer encoder layer and the cross-attention in the
7

--- PAGE 19 ---
Figure 2.2. Illustration of scaled dot-product Left attention and multi-head attention Right
transformer decoder layer. Position-wise feed-forward network consists of two linear lay-
ers that are applied to each position and with an activation function in between. The orig-
inal transformer model uses ReLU [79] as the activation function, while more recent pre-
trained transformer model such as BERT [25], GPT-2 [88], and GPT-3 [12] use GELU [41] as
their activation function which proven to yield a better evaluation performance. Position-
wise feed-forward network can be formulated as:
FFN(x) =Act(xW 1+b1)W2+b2 (2.2)
wherexdenotes the input vector, Act denotes the activation function, W1andb1de-
note the weight and bias of the Ô¨Årst linear layer, and W2andb2denote the weight and
bias of the second linear layer.
2.1.2 Transformer Layers
Transformer Encoder and Transformer Decoder There are two types of transformer lay-
ers: 1) Transformer encoder and 2) Transformer decoder. The transformer encoder layer
8

--- PAGE 20 ---
process the input sequence Xencin a non-autoregressive way and produce an output se-
quenceYenc. This allows the transformer encoder layer to be computed in parallel over
different sequence positions during both the training and inference. While the Trans-
former decoder layer process the input sequence Xdecin an autoregressive way which
makes the inference step should run sequentially as it produces output for one position
for each time step. While the training process can be done in parallel by performing au-
toregressive masking when the attention is computed.
Self-Attention and Cross-Attention As shown in Figure 2.3, the transformer encoder
layer only has a self-attention while the transformer decoder layer consists of two different
kinds of attention, i.e, self-attention and cross-attention. On the self-attention mechanism,
theQ,K, andVsequences are generated by a learned projections weight WQ,WK, andWV
from the input sequence, respectively. While in the cross-attention, the query sequence Q
is generated from the decoder input Xdecand the key and value sequences, KandV, are
generated from the encoder output Yenc.
2.2 EfÔ¨Åcient Transformer
Transformer model has shown a great performance in many different Ô¨Åelds [20, 73, 119,
115, 82]. Aside from its great progression, the improvement usually comes with an in-
crease in the size of the model which yields a much larger model and more expensive
computation and memory requirements [25, 88, 97, 28, 12]. Multiple efÔ¨Åciency methods
have been developed to improve the efÔ¨Åciency of a deep learning model. Some efÔ¨Åciency
methods are architecture-independent, while some others are more speciÔ¨Åc. Several efÔ¨Å-
ciency methods can also be used in conjunction with other types of efÔ¨Åciency methods. In
the following section, we will brieÔ¨Çy describe each of the general efÔ¨Åciency methods and
then provide a deeper explanation of architecture-speciÔ¨Åc transformer efÔ¨Åciency methods.
2.2.1 General EfÔ¨Åciency Methods
As shown in Figure 4.3, in general, an efÔ¨Åciency method can be divided into four cat-
egories based on where the efÔ¨Åciency takes place: 1) inference efÔ¨Åciency, 2) Ô¨Åne-tuning
9

--- PAGE 21 ---
Low Rank
ApproximationDistillation
Pruning
Quantization
Fast
Adaptation
Data
Cleansing
Distributed
Training
Mixed
Precision
Efficient
TransformerInference
EfficiencyFine-T une
EfficiencyTraining
EfficiencyAll-Phases
EfficiencyTransformer
Efficiency Method
Kernel Method
Fixed / Random
Pattern
Learnable
PatternRecurrence
MemoryPerformer
Low-Rank
Transformer
LinformerLinear
Transformer
Synthesizer
ReformerRouting
TransformerTransformer-XL
Compressive
TransformerLongformerETC
Set T ransformerSparse 
Transform er
BigBirdBlockwise
Transformer
(Wang et al , 2020)(Winata et al , 2019)
(Kitaev et al , 2020)(Zaheer et al , 2021)(Tay et al , 2021)(Child et al , 2019)(Choromanski et al , 2021)
(Katharopoulos et al , 2020)(Qiu et al , 2019)
(Ainslie et al , 2020)
(Beltagy et al, 2020)Image
Transform er
(Parmer et al , 2019)(Dair et al , 2019)
(Rae et al , 2019)
Sinkhorn
Transformer
(Tay et al , 2020)
(Lee et al , 2019)(Roy et al , 2020)Figure 2.3. EfÔ¨Åciency Methods for Transformer. In this thesis we focus on architecture-spe-
ciÔ¨Åc low-rank approximation method for transformer model.
efÔ¨Åciency, 3) training efÔ¨Åciency, and 4) All-phases efÔ¨Åciency.
StudentT eacher (a) (b) (c)
Figure 2.4. Methods for inference efÔ¨Åciency. (a) Distillation [17]. (b) Pruning [17], (c)
Quantization [50]
Inference efÔ¨Åciency Inference efÔ¨Åciency methods such as distillation [43, 94, 101, 17],
pruning [66, 121, 40, 92, 39, 39, 17], and quantization [46, 124, 5, 50] can be used for im-
proving the efÔ¨Åciency during the inference phase by reducing the model size during the
Ô¨Åne-tuning process. Recent approaches [101, 121] introduce a task-agnostic distillation
and pruning which can further improve the efÔ¨Åciency during the Ô¨Åne-tuning phase. Dis-
tillation reduces the model size by generating a smaller student model which is taught by
the pre-trained teacher model. Pruning reduces the model size by removing unimportant
10

--- PAGE 22 ---
connections according to a criterion such as based on its magnitude [92]. Quantization
decreases the model size by quantizing the 32-bit Ô¨Çoating-point parameters into a lower
bit-depth such as 8-bit integer quantization [46], 3-bit quantization in Ternary BERT [124],
and 2-bit quantization in Binary BERT [5]. The illustration for distillation, pruning, and
quantization is shown in Figure 2.4.
Fine-tuning efÔ¨Åciency Although the goal of fast adaptation or few-shot learning meth-
ods [29, 120, 117] is to improve model generalization of the model, these approaches also
help to improve the efÔ¨Åciency on the Ô¨Åne-tuning phase as it only allows the model to be
trained with only a tiny amount of data during the Ô¨Åne-tuning. Fast adaptation is done by
learning a representation that can generalize well over different tasks by optimizing the
model on multiple tasks with a meta-learning approach [29]. Nevertheless, this method
requires building a prior model which incurs some additional cost to build the model
beforehand.
Training efÔ¨Åciency Training efÔ¨Åciency methods improve the model efÔ¨Åciency in both
pre-training and Ô¨Åne-tuning phases. There are two different approaches to achieve train-
ing efÔ¨Åciency: 1) data cleansing and 2) distributed training. Data cleansing approaches [64,
56, 7] improve the efÔ¨Åciency during the training and Ô¨Åne-tuning phase by removing data
point that is considered as unimportant based-on a certain criterion. While, recent dis-
tributed training approaches [6, 91, 90] allow better resource allocation and distribution
strategy which signiÔ¨Åcantly reduces the memory usage and enables faster training and
Ô¨Åne-tuning. Figure 2.5 shows the distributed training allocation with different DeepSpeed
ZeRO [90] conÔ¨Ågurations compared to the normally distributed training allocation ap-
proach.
All-Phases efÔ¨Åciency There are two methods that can provide efÔ¨Åciency across all phases,
i.e, mixed-precision and efÔ¨Åcient model. mixed-precision [75] is mostly used to decrease
the computational cost mainly during both training, Ô¨Åne-tuning by reducing the bit-depth
of the model parameter similar to the quantization approach. But unlike quantization
which reduces the bit-depth from 32-bit Ô¨Çoating point to lower bit integer, mixed-precision
11

--- PAGE 23 ---
Figure 2.5. Comparison of different DeepSpeed ZeRO allocation strategy with the dis-
tributed training baseline [90]
only reduces the precision of Ô¨Çoating-point from 32-bit to 16-bit and only changes the bit-
depth on certain layer types. Although mixed-precision is mainly used only on training
and Ô¨Åne-tuning, It can also be applied on the inference phase, although it might yield
an erroneous prediction due to the effect of rounding error. While, model efÔ¨Åciency can
describe any architectural model efÔ¨Åciency methods such as sparse computation, low-
rank approximation, kernel methods, and many others. A more speciÔ¨Åc description of the
transformer model efÔ¨Åciency approach is elucidated in Section 2.2.2.
2.2.2 EfÔ¨Åcient Transformer
In the recent years, many research works have tried to build an efÔ¨Åcient variant of the
transformer model. Extending from Tay et al. (2020) [107], we categorized different
model-based efÔ¨Åciency approaches for transformer model into six categories as shown
in Figure 4.3, i.e, kernel method, Ô¨Åxed/random pattern, learnable pattern, recurrence,
memory, and low-rank approximation.
Kernel Method Kernel methods [67, 52] reformulate the scaled dot-product attention
mechanism by applying kernel tricks which allows the model to avoid a costly NN
softmax operation. Kernel methods rewrite the scaled dot-product attention in the fol-
12

--- PAGE 24 ---
lowing way:
Attention (Q,K,V) =softmax (QKT
pdk)V (2.3)
Attention (Q,K,V)k=PN
i=1sim(Qk,KT
i)ViPN
j=1sim(Qk,Kj)Tpdk(2.4)
Attention (Q,K,V)k=PN
i=1(Qk)(KT
i)ViPN
k=1(Qk)(KT
j)pdk(2.5)
Attention (Q,K,V)k=(Qk)PN
i=1(KT
i)Vi
(Qk)q
dkPN
j=1(KT
j)(2.6)
Attention (Q,K,V) =(Q)((KT)V)
(Q)pdkPN
j=1(KT
j)(2.7)
WhereQ2Rndk,K2Rndk,V2Rndvdenote the query, key, and value sequences,
respectively. Ki2RdkandVi2Rdvdenotes the key and value at position i,sim(.)
denotes the similarity function which in this case represented as an exponent of the dot-
product of the two vectors, and (.)is the feature map of the function sim. The kernel
trick in Equation 2.5 can be applied with the kernel sim(.)and the feature map (.)as
sim(.)satisfy the properties of a valid kernel function which has to be symmetric and
positive semi-deÔ¨Ånite.
Fixed/Random Pattern Fixed/random pattern reduces the time complexity of the atten-
tion mechanism by manipulating the attention matrix into a sparse matrix to limit the Ô¨Åeld
of view of the attention to Ô¨Åxed, predeÔ¨Åned patterns such as local windows and block pat-
terns with Ô¨Åxed strides which are easily parallelized with GPU and TPU devices. Long-
former [10] applies a strided local window pattern. Blockwise transformer [86] and image
transformer [82] apply a Ô¨Åx block pattern. ETC [1] combines local window pattern with
additional global attention on several tokens. Sinkhorn Transformer [104] uses the block
pattern approach to generate a local group. BigBird [123] applies a combination of local
window patterns, random patterns, and global patterns on several tokens. The illustration
of the random, window, and global attention patterns are shown in Figure 2.7.
13

--- PAGE 25 ---
Figure 2.6. Illustration of random (a), Ô¨Åxed (b), and global (c) attention patterns [123]
Learnable Pattern Similar to Ô¨Åxed/random patterns, the learnable pattern tries to Ô¨Ånd
the sparse attention matrix to reduce the time complexity. Sinkhorn Transformer [ ?]
generates a learnable pattern from a generated local group to sort and Ô¨Ålter out some of the
groups to reduce the computation cost. Reformer [55] performs a hash-based similarity
function called locality sensitivity hashing (LSH) to cluster tokens into several groups
and compute the attention locally within each group. Similarly, Routing Transformer [93]
cluster tokens into several groups by performing k-means clustering. The depiction of the
learnable pattern approach is shown in Figure ??.
Sinkhorn T ransformer Routing T ransformer Reformer
Figure 2.7. Illustration of learnable pattern approaches. (Left) Sinkhorn Transformer [104],
(Center) Routing Transformer [93], and (Right) Reformer [55]
Recurrence The recurrence method is in some sense similar to the combination of block
pattern with local window pattern, as this method computes a segment-level recurrence to
connect multiple blocks of sequence. Transformer-XL [21] provides a segment-level recur-
rence mechanism that allows the current segment to attend to the previous segment. Com-
14

--- PAGE 26 ---
Transformer -XL Compr essive T ransformerFigure 2.8. Illustration of recurrence approaches. (Left) Transformer-XL [21] and (Right)
Compressive Transfoemr [89]
pressive Transformer [89] extends Transformer-XL capability to attend to long-distance
past segments by encoding the past segment into a Ô¨Åne-grained memory segment. The
illustration of Transformer-XL and Compressive Transformer are shown in Figure 2.8.
Memory A memory approach leverages a side memory module that can access multiple
tokens at once. A global token is common for the memory approach which can be seen
as a form of memory that can access the entire sequence. The global token is incorpo-
rated in Set Tranformer [63], ETC [1], Longformer [10], and Bigbird [123]. Compressive
Transformer [89] uses a form of memory module to encode the past segments. While the
k-means centroids in the Routing Transformer can also be seen as a parameterized mem-
ory module that can access the entire sequence.
Low-Rank Approximation Low-rank approaches reduce the computational cost and
memory usage by leveraging low-rank approximation on the parameters or activations of
the model. Low-rank transformer (LRT) [116] reduces the computational cost of a trans-
former model by performing low-rank approximation on the weight of the linear layer in-
side the transformer model. Although LRT does not reduce the space and time complexity
of the model, it improves the efÔ¨Åciency by signiÔ¨Åcantly shrink down the model parame-
ters. Linformer [112] reduces the space and time complexity of the attention mechanism
from O(n2) to O(n) with low-rank approximation to the NNattention matrix. Linformer
projects the sequence length of the key and value sequence to a lower-dimensional space.
More detail on LRT and Linformer model is described in Chapter 3.
15

--- PAGE 27 ---
2.3 Low-Rank Approximation and Matrix Factorization
We denote a real-valued matrix M2Rmnwith rankr,r6min(m,n). A low-rank
approximation of Mis denoted as ÀÜM=UVTwhereU2Rmk,V2Rkn, andk <<
min(m,n). Given the matrix M, such factorization can be estimated by solving a mini-
mization problem where the cost function is the measure of Ô¨Åtness of between the matrix
Mand the product of the low-rank matrices ÀÜM[36]. Distance function such as Frobenius
normkXkF=√àP
iP
jXijis often use to solve the minimization problem. We can deÔ¨Åne
the minimization problem as:
ÀÜM=argmin
ÀÜMM-ÀÜM
F(2.8)
(U,V) =argmin
U,VM-UVT
F(2.9)
The quadratic minimization problem can be solved through different methods such as
singular value decomposition (SVD) [35] or non-negative matrix factorization (NMF) [62].
Additionally, recent works in matrix factorization [58, 60, 33, 78] apply weight factoriza-
tion to the model parameter before the model is trained and yield a comparable or even
better result with fewer parameters and computational cost.
2.3.1 Singular Value Decomposition
SVD is an iterative approach to SVD decomposes a given a matrix M2Rmnwith rankr
intoM=UVTwhereU2Rmmis called as left-singular vectors, V2Rnnis called as
right-singular vectors, and 2Rmnis a diagonal matrix consisting the singular values
of the matrix Mon its Ô¨Årstrdiagonal and 0 otherwise. In this form, SVD is useful for many
applications in linear algebra including calculating pseudo inverse, solving least-square
system, and determining the rank, range, and null space of the matrix.
With the low-rank approximation, we can instead perform SVD with a pre-deÔ¨Åned
rankkwhich denotes the number of k-highest singular values to consider and produce
a much smaller U,V, andmatrices. Based on Eckart-Young theorem, the best rank- k
16

--- PAGE 28 ---
=
M(r=3)mn
Umm
V Œ£n
mn
n
=
M(k=2)mn
Umk
V Œ£k
kn
kSVD
Rank-k SVD ApproximationFigure 2.9. Illustration of Singular Value Decomposition
SVD approximation to a matrix Min Frobenius norm is attained by B=uiiviand its
error iskM-BkF=√àPr
i=k+12
iwhereuiis the column vector of matrix U,viis column
vector of matrix V, andiis diagonal entry of . To get two matrices as deÔ¨Åned before,
we can simply apply the dot-product of Uas the Ô¨Årst matrix and use the Vas the second
matrix. The rank- kSVD approximation can also be used for denoising data as it removes
the eigenvectors with smaller eigenvalues which can be considered as noise [38]. The
depiction of SVD and rank- kSVD approximation is shown in Figure 2.9.
2.3.2 Non-negative Matrix Factorization
NMF is another factorization method which factorize a matrix V2RmnintoV=WH
whereW2Rmp,H2Rpn, andp << min (m,n). Unlike SVD, NMF imposes non-
negative constraint to all three matrices [113]. There are several solvers to Ô¨Ånd Wand
Hfor NMF and the most common method is called multiplicative update rule [62]. In
multiplicative update rule, WandHare initialized with non-negative values and then
iteratively performs element-wise update to HandWwith the following equations:
17

--- PAGE 29 ---
=
V(p=3)mn
Wmp
Hn
pFigure 2.10. Illustration of Non-negative Matrix Factorization
H[i,j]  H[i,j](WTV)[i,j]
(WWTH)[i,j](2.10)
W[i,j]  W[i,j](VHT)[i,j]
(WHHT)[i,j](2.11)
The iterative update runs until it is stable or a certain pre-deÔ¨Åned stopping criterion
such as maximum iteration count is reached. A depiction of NMF factorization is shown
in Figure 2.10.
NMF algorithms can be divided into 4 categories: Basic NMF (BNMF), Constrained
NMF (CNMF), Structured NMF (SNMF), and Generalized NMF (GNMF). CNMF imposes
some additional constraints as regularization to the BNMF. SNMF enforces other charac-
teristics or structures in the solution to the NMF learning problem of BNMF. While, GNMF
generalizes BNMF by breaching the intrinsic non-negativity constraint to some extent, or
changes the data types, or alters the factorization pattern. Each NMF category is further
divided into several subclasses as shown in Figure 2.11.
2.3.3 Pre-Training Low-Rank Matrix Factorization
Both SVD and NMF require the information of the matrix Mto be known for the methods
to take place. This means that both SVD and NMF can only be applied to factorize the
weight matrix of the model once the training process is completed. This limits the ap-
plicability of the low-rank method to only increase the efÔ¨Åciency of the inference phase.
Other works in low-rank matrix factorization explore the possibility to perform low-rank
18

--- PAGE 30 ---
Figure 2.11. Categorization of Non-negative Matrix Factorization Algorithms [113]
factorization to the weight matrix of the model before training the model. SpeciÔ¨Åcally,
given a transformation layer of the form y=f(xW), wherexis andin-dimensional in-
put,yis andout-dimensional output, and W2Rdindoutis the weight matrix of the
functionf, we can decrease the number of the parameters of function fby expressing W
as the product of two smaller matrices W=UV, whereU2Rdink,V2Rkdout, and
k << min (din,dout). By choosing a small k, we can achieve a substantial reduction in
the number of parameters, memory usage, and computation cost. We call this method as
in-training low-rank factorization method.
With the in-training low-rank factorization, we can simply replace Wwith two smaller
matricesUandVand compute derivatives for UandVinstead ofWduring the training.
This approach has been explored in the previous work [23] with a multi-layer percep-
tron network and their experimental results suggest that this approach is very unstable
and lead to a higher error rate. Nevertheless, more recent works [58, 116, 60, 33, 78] ap-
ply similar methods to different model architecture to reduce the model parameters and
reduce its computational cost. These works suggest that training more advance neural
network model architectures, such as CNN and RNN, with randomly initialized low-
rank factorized weight matrix can result in a factorized model that works as good as the
non-factorized counterpart while gaining a signiÔ¨Åcant reduction in terms of the number
of parameters, computational cost, and memory usage.
19

--- PAGE 31 ---
CHAPTER 3
Greenformers: Ecient Transformer Model via
Low-Rank Approximation
In this chapter, we explore two Greenformers models that apply low-rank approximation
to the transformer model: 1) low-rank approximation on the linear layer of the trans-
former model, and 2) low-rank approximation on the self-attention mechanism of the
transformer model. We called the Ô¨Årst variant as Low-Rank Transformer (LRT) [116],
while the second variant as Linformer [112]. Both variants can help to improve the ef-
Ô¨Åciency of the transformer model signiÔ¨Åcantly and we conduct a study on these two vari-
ants to get a better understanding of the drawbacks of each model and Ô¨Ånd the best con-
dition to utilize each model.
We conduct a thorough analysis to compare the efÔ¨Åciency of the original transformer
model, LRT, and Linformer. We compare the three variants in terms of memory usage
and computational cost on different characteristics of the input sequence. We further an-
alyze the effect of low-rank factor ron the efÔ¨Åciency improvement of LRT and Linformer
compared to the baseline transformer model. Lastly, we conclude our analysis by pre-
senting the beneÔ¨Åts and limitations of each low-rank transformer model and provide a
recommendation of the best setting to use each model variant.
3.1 Methodology
3.1.1 Low-Rank Transformer: EfÔ¨Åcient Transformer with Factorized Lin-
ear Projection
To reduce the computational cost of the transformer model [110], we extend the idea of the
in-training low-rank factorization method [58, 33, 23] and introduce the low-rank variant
of the transformer model called LRT. SpeciÔ¨Åcally, we apply low-rank matrix factorization
to the linear layers in a transformer model. We call the factorized linear layer as linear
20

--- PAGE 32 ---
EmbeddingLRMHAAdd & NormAdd & Norm
LRFF
N√ó
Transformer Encoder
Encoder InputEmbeddingMasked LRMHAAdd & NormAdd & Norm
LRFF
M√ó
Transformer Decoder
Decoder InputDecoder Output
Add & Norm
LRMHAEncoder OutputFigure 3.1. Low-Rank Transformer Architecture. Low-Rank Transformer consists of Nlay-
ers of low-rank transformer encoder and Mlayers of low-rank transformer decoder. [116]
encoder-decoder (LED) unit. The matrix factorization is applied by approximating the
matrix W2Rmnin the linear layer using two smaller matrices, E2Rmrand D2Rrn
such that WED. The matrix Wrequiresmnparameters and mnFLOPS, while Eand
Drequirerm+rn=r(m+n)parameters and rm+rn=r(m+n)Ô¨Çops. If we choose
the rank to be very low r<<m ,n, the number of parameters and FLOPS in Eand Dare
much smaller compared to W. The design of our LED unit is shown in Figure 3.2 (Left).
As shown in Figure 3.1, our proposed LRT model consists of Nlayers of the LRT en-
coder to reÔ¨Åne the low-level input features into higher-level features, and Mlayers of the
LRT decoder to generate the output sequence. Each LRT encoder layer consists of low-
rank multi-head attention (LRMHA) layer followed by a low-rank feed-forward (LRFF)
layer. While each LRT decoder layer consists of a masked LRMHA layer that uses causal
masking to prevent attention to the future time-step, followed by an LRMHA layer to per-
form a cross-attention mechanism with the output from the LRT encoder, and followed by
an LRFF layer.
Low-Rank Multi-Head Attention
To reduce the computational in the multi-head attention layer, we introduce LRMHA
layer. We apply low-rank approximation to LRMHA by factorizing the query projection
21

--- PAGE 33 ---
 
 
 √ó  √ó  √ó 
LEDhead h
head 2
head 1LED
VKQLED LEDScaled Dot-Product AttentionConcatenateOutput
residual
connectionLayer Norm
Layer Norm
LED
LEDReLU
xresidual
connectionFigure 3.2. Low-Rank Transformer Unit. Left: Linear Encoder-Decoder (LED), Cen-
ter: Low-Rank Multi-head Attention (LRMHA), and Right: Low-Rank Feed-forward
(LRFF). [116]
layersWQ
i2Rdmodeldk, key projection layers WK
i2Rdmodeldk, value projection layers
WV
i2Rdmodeldv, and the output projection layer WO2Rdvdmodel . As shown in Fig-
ure 3.2 (Center), our LRMHA layer accepts a query sequence Q, a key sequence K, and a
value sequence V. Similar to the original transformer model, we add a residual connection
from the input query sequence Qwith the result from the output projection. SpeciÔ¨Åcally,
we formulate our LRMHA layer as follow:
Attention (Q,K,V) =Softmax (QKT
pdkV), (3.1)
hdi=Attention (QEQ
iDQ
i,KEK
iDK
i,VEV
iDV
i), (3.2)
f(Q,K,V) =LayerNorm (Concat (h1,,hH)EODO+Q), (3.3)
wherefdenotes the LRMHA function, hidenotes the head of i,Hdenotes the number
of heads, LayerNorm denotes layer normalization function, Concat denotes concatenation
operation between multiple heads, and EQ
i2Rdmodeldr;DQ
i2Rdrdk;EK
i2Rdmodeldr;
DK
i2Rdrdk;EV
i2Rdmodeldr;DV
i2Rdrdv;EO2Rdvdr;DO2Rdrdmodel are the
22

--- PAGE 34 ---
low-rank encoder-decoder matrices for query projection WQ
i, key projection WK
i, value
projectionWV
iand output projection WO.dmodel ,dr,dk, anddvdenote dimensions of
hidden size, rank, key, and value, respectively.
Low-Rank Feed-Forward
As shown in Figure 3.2 (Right), LRFF consists of two LED units and a ReLU activation
function is applied in between the two LED units. Similar to the original transformer
model, A residual connection is added in LRFF by connecting the input sequence xwith
the output from the second LED unit to alleviate the gradient vanishing issue. SpeciÔ¨Åcally,
we deÔ¨Åne our LRFF unit as follow:
g(x) =LayerNorm (max(0,xE1D1)E2D2+x), (3.4)
wheregdenotes the low-rank feed-forward (LRFF) function and E12Rdmodeldr;
D12Rdrdinner ;E22Rdinnerdr;D22Rdrdmodel are the low-rank encoder-decoder
matrices for the Ô¨Årst and the second LED units respectively.
3.1.2 Linformer: EfÔ¨Åcient Transformer with Factorized Attention Mech-
anism
Linformer [112] performs a low-rank approximation on the self-attention mechanism of
the transformer model. With this approach, the space and time complexity is reduced
from O(n2) to O(n), thus enabling the model to process an even longer sequences com-
pared to the quadratic-attention mechanism.
As shown in Figure 3.3, Linformer reduces the complexity of attention mechanism by
performing low-rank approximation to the self-attention layer of a transformer model.
Mathematically, A self-attention layer accepts an input sequence X2Rndan output
another sequence X2Rndv. In the original transfomer model, the input sequence X
is Ô¨Årst projected into three different sequences: query Q2Rndk, keyK2Rndk, and
valueV2Rndvby using query projection WQ2Rddk, key projection WK2Rddk,
and value projection WV2Rddv, respectively. A scaled-dot product attention is then
23

--- PAGE 35 ---
AAGTCCTCAATGACTTAG... ...EmbeddingSelf AttentionAdd & NormAdd & Norm
Feed-ForwardContextualized Embedding
N
InputQ:ndk
X:ndQueryKey
KT:
dknValue
V:ndvPairwise
Similarities
√¶(S) :nnOutput
Y:ndv
InputQ:ndk
X:ndQueryKey
ÀÜKT:
dkkValue
ÀÜV:kdvLow-Rank
Pairwise
Similarities
√¶(S)
nkOutput
Y:ndvTransformer Model Transformer Attention Linformer Attention
Reduce Complexity
O(N2)!O(N)
Figure 3.3. Comparison of the attention mechanism in original transformer model and
Linformer model
applied from the three sequences to produce the sequence Y. The formula for calculating
the self-attention mechanism in Transformer is as follow:
Q=XWQ,K=XWK,V=XWV(3.5)
Y=Softmax (QKT
pdkV) (3.6)
While in Linformer model, after Xis projected to Q,K, andV, another projection from
K2RndkandV2RndvwithWÀÜK2RknandWÀÜV2Rkn, respectively. The projection
produces a low-rank key sequence ÀÜK2Rkdkand a low-rank value sequence ÀÜV2Rkdv.
The complete formula for calculating the self-attention mechanism in Linformer is as fol-
low:
Q=XWQ,K=XWK,V=XWV(3.7)
ÀÜK=WÀÜKK,ÀÜV=WÀÜVV (3.8)
Y=Softmax (QÀÜKT
pdkÀÜV) (3.9)
24

--- PAGE 36 ---
The additional projection steps reduce the space and time complexity of computing Y
from O(n2) to O(nk) which can also be written as O( n) askis a constant. If we choose the
rankkto be very low k<<n , the Linformer model could get a huge reduction in terms
of both computation time and memory usage.
3.2 Experimental Setup
We capture all metrics that impact the efÔ¨Åciency for both training and inference. Specif-
ically, we report 5 efÔ¨Åciency metrics, i.e, 1) the time required to perform forward and
backward passes to measure the time efÔ¨Åciency in the training step, 2) the memory usage
required to perform forward pass with all of the cached activations for performing back-
ward pass to measure the memory efÔ¨Åciency on the training step, 3) the time required to
perform a forward pass without dropout and caching any activations to measure the time
efÔ¨Åciency during the inference step, 4) the memory usage required to perform forward
pass without caching any activations to measure the memory efÔ¨Åciency on the inference
step, and 5) the number of parameters over different low-rank factor rto measure the
storage efÔ¨Åciency of the model.
We compare the efÔ¨Åciency of our methods with the standard Transformer model. We
benchmark the speed and memory usage on a single 12GB NVIDIA TITAN X (Pascal)
GPU card. For the input sequence, we randomly generate data up to a certain sequence
lengthnand calculate each metric averaged across 30 runs each. Similar to [112], when
measuring the time efÔ¨Åciency, we select batch size based on the maximum batch size that
can Ô¨Åt the GPU memory for the standard transformer model. We Ô¨Åx the hyperparameter
for all models to ensure a fair comparison between each model. SpeciÔ¨Åcally, for each
model variant, we use 2-layer encoder layers, with hidden dimension of 768, feed-forward
size of 3072, and number of heads of 12.
Additionally, we compare the effectiveness between LRT, Linformer, and Transformer
models on the MNIST dataset to provide empirical evidence of the effectiveness of the
two efÔ¨Åcient transformer variations. Following [106], we Ô¨Çatten image data in MNIST
from 2828 pixels into sequences with a length of 784. We append additional an [CLS]
token as the sequence preÔ¨Åx. We employ a Transformer model with 4 transformer encoder
25

--- PAGE 37 ---
layers, model dimension of 256, 8 heads, and feed-forward size of 1024. For LRT and
Linformer, we utilize low-rank factorization with rank r=64.
3.3 Result and Discussion
3.3.1 EfÔ¨Åciency comparison to Transformer Model
Low-Rank Transformer (LRT)
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length2.34 2.30 2.31 2.17 1.97 1.63 0.91 0.80
2.32 2.35 2.32 2.30 2.00 1.60 1.12 0.73
2.37 2.36 2.35 2.29 2.13 1.64 1.10 0.71
2.24 2.26 2.20 2.19 2.04 1.63 1.12 0.76
1.90 1.90 1.88 1.87 1.75 1.48 1.13 0.77
1.62 1.61 1.60 1.59 1.52 1.34 1.07 0.78
1.33 1.32 1.32 1.32 1.27 1.16 0.97 0.76
1.12 1.14 1.16 1.12 1.09 1.03 0.91 0.76
0.83 0.85 0.85 0.84 0.83 0.81 0.75 0.68Speed Up
0.00.51.01.52.02.53.0
Speed Up
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length2% 3% 4% 7% 13% 26% 51% 102%
3% 4% 5% 8% 14% 27% 51% 102%
5% 5% 7% 10% 16% 28% 52% 102%
8% 9% 10% 13% 18% 30% 53% 100%
18% 19% 20% 23% 28% 38% 59% 102%
39% 39% 40% 42% 46% 54% 69% 99%
69% 69% 69% 70% 72% 76% 84% 101%
89% 89% 89% 89% 90% 91% 94% 100%
97% 97% 97% 97% 97% 98% 98% 100%Memory Reduction
0.00.20.40.60.81.01.21.4
Compression Rate
Linformer Model
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length1.00 0.99 1.00 0.98 0.93 0.81 0.81 0.73
1.08 1.09 1.09 1.07 1.03 0.95 0.86 0.70
1.12 1.11 1.11 1.09 1.05 1.00 0.91 0.76
1.15 1.15 1.13 1.13 1.11 1.04 0.95 0.82
1.23 1.22 1.22 1.22 1.19 1.14 1.04 0.89
1.36 1.37 1.36 1.35 1.32 1.27 1.17 1.01
1.66 1.67 1.66 1.65 1.61 1.54 1.43 1.23
2.19 2.17 2.26 2.17 2.13 2.05 1.90 1.64
4.11 4.10 4.08 4.03 3.88 3.79 3.46 3.02Speed Up
0.00.51.01.52.02.53.0
Speed Up 
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length100% 100% 100% 100% 100% 101% 103% 108%
100% 100% 100% 100% 100% 101% 103% 110%
100% 100% 100% 100% 100% 101% 104% 113%
100% 100% 100% 100% 100% 100% 105% 114%
95% 95% 95% 96% 96% 98% 103% 117%
80% 80% 80% 80% 80% 81% 88% 106%
51% 51% 51% 51% 51% 52% 59% 77%
25% 25% 25% 25% 26% 26% 31% 43%
11% 11% 11% 11% 11% 12% 14% 21%Memory Compression
0.00.20.40.60.81.01.21.4
Compression Rate
Figure 3.4. Inference speed up and memory efÔ¨Åciency of LRT (Top) and Linformer (Bot-
tom) compared to the Transformer baseline. (Left) Computation speed up against the
Transformer model, >1 indicates faster or equal speed and slower otherwise. (Right)
Memory usage compared to the Transformer model in percentage. Lower value indicates
better memory compression.
We compare the computation and memory efÔ¨Åciency of our models to the Transformer
baseline. As shown in Figure 3.4, LRT shows signiÔ¨Åcant improvement in terms of com-
putation cost when the sequence length is 6512 and rank 6128. In terms of memory
26

--- PAGE 38 ---
usage, LRT reduces 50% of the memory usage when the sequence length is 6512 and
rank6128. While for the Linformer model, we observe a signiÔ¨Åcant speedup and mem-
ory reduction when the sequence length is >256 and achieve even higher speed up when
the sequence length is larger due to its linear complexity. We also observe a similar trend
for both LRT and Linformer models when simulating the training step with both forward
and backward passes. This indicates that LRT is potential as an alternative of Transformer
model when the sequence length is short ( 6512) while Linformer is potential as an alter-
native of Transformer model when the sequence length is >512.
3.3.2 Short and Long Sequence EfÔ¨Åciency
Speed Up Ratio
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length-1.20 -1.20 -1.19 -1.17 -1.14 -1.01 -0.78 -0.67
-1.17 -1.17 -1.16 -1.16 -1.10 -0.91 -0.68 -0.50
-1.14 -1.14 -1.13 -1.12 -1.10 -0.87 -0.61 -0.34
-1.04 -1.03 -1.02 -1.02 -0.99 -0.80 -0.53 -0.31
-0.78 -0.77 -0.78 -0.78 -0.75 -0.59 -0.42 -0.20
-0.46 -0.46 -0.47 -0.47 -0.45 -0.35 -0.22 -0.06
-0.03 -0.03 -0.04 -0.04 -0.04 0.02 0.10 0.18
0.45 0.45 0.45 0.44 0.44 0.46 0.50 0.51
1.14 1.14 1.14 1.13 1.12 1.11 1.09 1.05Training Step
1.5
1.0
0.5
0.00.51.01.5
LRT vs Linformer (log scaled)
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length-0.9 -0.8 -0.8 -0.8 -0.7 -0.7 -0.1 -0.1
-0.8 -0.8 -0.8 -0.8 -0.7 -0.5 -0.3 -0.0
-0.7 -0.7 -0.8 -0.7 -0.7 -0.5 -0.2 0.1
-0.7 -0.7 -0.7 -0.7 -0.6 -0.5 -0.2 0.1
-0.4 -0.4 -0.4 -0.4 -0.4 -0.3 -0.1 0.2
-0.2 -0.2 -0.2 -0.2 -0.1 -0.1 0.1 0.3
0.2 0.2 0.2 0.2 0.2 0.3 0.4 0.5
0.7 0.6 0.7 0.7 0.7 0.7 0.7 0.8
1.6 1.6 1.6 1.6 1.5 1.5 1.5 1.5Inference Step
1.5
1.0
0.5
0.00.51.01.5
LRT vs Linformer (log scaled)
Memory Compression Ratio
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length-3.3 -3.2 -3.0 -2.7 -2.1 -1.4 -0.7 0.0
-2.7 -2.6 -2.5 -2.3 -2.0 -1.4 -0.7 0.0
-1.9 -1.9 -1.9 -1.8 -1.6 -1.3 -0.7 -0.0
-1.1 -1.1 -1.1 -1.0 -1.0 -0.8 -0.6 -0.1
-0.2 -0.2 -0.2 -0.2 -0.2 -0.1 -0.1 -0.1
0.5 0.5 0.5 0.5 0.5 0.4 0.3 0.2
1.2 1.2 1.2 1.1 1.1 1.0 0.9 0.7
1.9 1.9 1.8 1.8 1.8 1.7 1.5 1.3
2.5 2.5 2.5 2.5 2.4 2.3 2.2 1.9Training Step
3
2
1
0123
LRT vs Linformer (log scaled)
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length-4.0 -3.6 -3.2 -2.6 -2.0 -1.4 -0.7 -0.1
-3.6 -3.3 -3.0 -2.5 -1.9 -1.3 -0.7 -0.1
-3.1 -2.9 -2.7 -2.3 -1.8 -1.3 -0.7 -0.1
-2.5 -2.5 -2.3 -2.1 -1.7 -1.2 -0.7 -0.1
-1.7 -1.6 -1.6 -1.4 -1.2 -0.9 -0.6 -0.1
-0.7 -0.7 -0.7 -0.6 -0.6 -0.4 -0.3 -0.1
0.3 0.3 0.3 0.3 0.3 0.4 0.3 0.3
1.3 1.3 1.3 1.3 1.3 1.3 1.1 0.8
2.2 2.2 2.2 2.2 2.1 2.1 1.9 1.6Inference Step
3
2
1
0123
LRT vs Linformer (log scaled)
Figure 3.5. Speed up (Top) and memory compression (Bottom) ratios of LRT and Lin-
former. (Left) Ratio in the training step. (Right) Ratio in the Inference step. Negative
values indicate LRT yields better speed up or memory compression, while positive values
indicate Linformer yields better speed up or memory compression.
27

--- PAGE 39 ---
As shown in Figure 3.5, the LRT model yields better computation and memory efÔ¨Å-
ciency when the sequence length is 6256, while the Linformer model yields better com-
putation and memory efÔ¨Åciency when the sequence length is >1024. For sequence length
512, LRT offers faster during the training step and provides better memory compression
on the inference step, while Linformer offers a faster inference step and better memory
reduction on the training step. Although this depends on the choice of other hyperparam-
eters such as hidden size and number of heads, the trend indicates that the LRT model
can improve the efÔ¨Åciency better when the input sequence is short, while the Linformer
model can be an efÔ¨Åcient alternative when the sequence length is long. With a larger hid-
den size conÔ¨Åguration, we can expect that the efÔ¨Åciency of the LRT model will increase
even further. Additionally, as the sequence length of the Linformer model is pre-deÔ¨Åned
as one of its parameters, it is also worth mentioning that variation of sequence length in
the dataset might also affect the training and inference efÔ¨Åciency of the Linformer model.
3.3.3 Size EfÔ¨Åciency
64 128 256 512 1024 2048 4096
Rank6.29 M12.58 M25.17 M50.33 M100.66 M201.33 M402.65 M805.31 M#ParametersTransformer
LRT
Linformer (len=512)
Linformer (len=4096)
Figure 3.6. Size comparison of LRT, Linformer, and Transformer model across different
Rank. We visualize both axis in log 2scale.
Other than computation and memory efÔ¨Åciency, the size of a model is also an impor-
tant factor to consider for the deployment of a model especially in the on-device setting
as it affects the amount of storage and networking cost required to deliver the model. As
shown in Figure 3.6, with the additional projection layer, the Linformer model always in-
28

--- PAGE 40 ---
creases the model size regardless of its rank. On the other hand, the LRT model provides
signiÔ¨Åcant size reduction when the rank is small. For instance, with Rank r=64, the
model size can be reduced up to 8smaller and the size increases linearly along with
the rank. This means the LRT model is a more viable option for on-device computing
compared to Linformer and Transformer models.
3.3.4 Effectiveness of LRT and Linformer model
Model #Parameters Training Acc. Test Acc.
Transformer 11.2M 98.72% 96.95%
LRT 9.3M 99.04% 97.31%
Linformer 11.6M 98.89% 96.82%
Table 3.1. Evaluation comparison between Transformer, LRT, and Linformer models on
MNIST dataset
As shown in Table 3.1, both LRT and Linformer models perform as well as the Trans-
former models with much smaller computing time and memory cost on the MNIST dataset.
This suggests that efÔ¨Åciency methods via low-rank approximation, both LRT and Lin-
former, can signiÔ¨Åcantly improve the efÔ¨Åciency of the model without any loss of evalua-
tion performance. Additionally, the number of parameters for the LRT model can achieve
a slightly better score in terms of training and evaluation with much smaller parameters
compared to the original Transformer model. This suggests that the bottleneck layer from
the low-rank approximation can further improve the generalization of the model which
increases the overall score of the low-rank factorized model.
3.3.5 Impact of Greenformers
To further see the direct impact of Greenformers‚Äô efÔ¨Åciency methods, we will apply the
training efÔ¨Åciency of the LRT model to the existing transformer-based model. As the hy-
perparameter setting of the transformer model used in the analysis follows BERT BASE [25]
model, we can directly apply the training efÔ¨Åciency factor shown in Figure 3.7 to improve
the efÔ¨Åciency of the pre-training cost of the BERT BASE model. BERT model is pre-trained
for 1,000,000 steps with a sequence length of 128 for 90% of the time and a sequence length
29

--- PAGE 41 ---
Training EfÔ¨Åciency of Low-Rank Transformer (LRT)
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length2.85 2.86 2.84 2.76 2.55 2.01 1.42 0.99
3.01 3.00 2.95 2.92 2.69 2.05 1.43 0.96
3.01 3.00 2.95 2.92 2.75 2.08 1.43 0.92
2.80 2.80 2.76 2.73 2.59 2.04 1.43 0.98
2.38 2.37 2.37 2.35 2.25 1.85 1.42 0.98
2.01 2.00 2.01 2.00 1.93 1.67 1.35 0.99
1.64 1.64 1.66 1.65 1.61 1.46 1.23 0.99
1.41 1.41 1.41 1.40 1.38 1.30 1.15 0.99
1.20 1.20 1.20 1.20 1.19 1.15 1.08 0.99Speed Up
0.00.51.01.52.02.53.0
Speed Up
4 8 16 32 64 128 256 512
Rank16
32
64
128
256
512
1024
2048
4096Sequence Length4% 4% 5% 6% 13% 25% 50% 102%
7% 7% 8% 10% 13% 25% 50% 102%
14% 15% 16% 17% 21% 27% 50% 102%
33% 33% 34% 36% 39% 46% 60% 101%
63% 63% 64% 65% 68% 74% 85% 109%
83% 83% 84% 84% 86% 89% 95% 108%
94% 94% 94% 95% 95% 97% 100% 107%
98% 98% 98% 98% 99% 99% 101% 103%
100% 100% 100% 100% 100% 100% 101% 102%Memory Compression
0.00.20.40.60.81.01.21.4
Compression Rate
Figure 3.7. Training speed up and memory efÔ¨Åciency of LRT compared to the Transformer
baseline. (Left) Computation speed up against the Transformer model, >1 indicates faster
or equal speed and slower otherwise. (Right) Memory usage compared to the Trans-
former model in percentage. Lower value indicates better memory compression.
of 512 for the rest 10% of the time. We calculate the overall efÔ¨Åciency factor of the LRT
model compared to BERT BASE model with the following formula:
Effr=X
kPkEk,r (3.10)
WhereEffrdenotes the overall efÔ¨Åciency running LRT with rank r,Pkdenotes the per-
centage of time running pre-training with ksequence long, and Ek,rdenotes the efÔ¨Åciency
factor of running LRT on a sequence with length kand rankr. By applying LRT with rank
r=f32, 64, 128, 256 g, the model can achieve overall efÔ¨Åciency of 2.63, 2.50, 1.98, 1.48, re-
spectively, which can provide signiÔ¨Åcant computational, economical, and environmental
costs reduction as shown in Table 3.2.
ModelLow-Rank EfÔ¨Åciency Computational Cost Economical Cost CO 2emission
Factor Factor (petaÔ¨Çop/s-day) (USD) (kg)
BERTBASE - 1.00 2.24 $2,074 - $6,912 652.3
LRT-BERT BASE 256 1.48 1.52 $1406 - $4686 442.2
LRT-BERT BASE 128 1.98 1.13 $1045 - $3484 328.8
LRT-BERT BASE 64 2.50 0.90 $831 - $2768 261.2
LRT-BERT BASE 32 2.63 0.85 $789 - $2629 248.1
Table 3.2. Cost efÔ¨Åciency of applying Low-Rank Transformer to the pre-training phase of
BERTBASE model
30

--- PAGE 42 ---
3.4 Conclusion
In this chapter, we explore two Greenformers models which apply low-rank approxima-
tion to the Transformer model, i.e, Low-Rank Transformer (LRT) and Linformer. We con-
duct a thorough analysis to assess the efÔ¨Åciency of LRT and Linformer variants compared
to the Transformer model. Based on our analysis, we Ô¨Ågure out that LRT can be a better
alternative to the Transformer model when the sequence length is short ( 6512), while
Linformer can be a better alternative when the sequence length is long ( >512). Linformer
efÔ¨Åciency depends on the variation of input size and it is much more efÔ¨Åcient when the
input size is static. In terms of the number of parameters, we show that LRT is more suit-
able for on-device applications as it can reduce the storage requirements and the network
cost signiÔ¨Åcantly. Additionally, we show that both LRT and Linformer models can per-
form as well as the Transformer model. Lastly, we show that by applying LRT efÔ¨Åciency
methods, we can signiÔ¨Åcantly decrease the cost of training a transformer-based model,
BERT. These results demonstrate the importance of applying Greenformers which can re-
duce the computational, economical, and environmental costs of developing the model
without sacriÔ¨Åcing the performance of the model in the downstream task.
31

--- PAGE 43 ---
CHAPTER 4
Low-Rank Transformer for Automatic Speech
Recognition
Traditional HMM-based models have been outperformed by end-to-end automatic speech
recognition (ASR) models. End-to-end ASR models allow a simpler end-to-end learn-
ing mechanism as it provides only a single model structure for replacing learning acous-
tic, pronunciation, and language models. Furthermore, end-to-end ASR models require
only paired acoustic and text data, without additional prior knowledge, such as phoneme
set and word dictionary. End-to-end attention-based recurrent neural network (RNN)
models such as LAS [16], joint CTC-attention model [53], and attention-based seq2seq
model [108] have signiÔ¨Åcantly outperformed the traditional approaches by large mar-
gins in multiple speech recognition datasets. In recent years, fully-attentional transformer
speech recognition models [27, 67] have further improved the attention-based RNN mod-
els in terms of performance and training speed. Transformer models reduce the training
time of attention-based RNN models as it enables parallel computation along the time di-
mension. However, the total computational cost of both model structures remains high,
especially in the streaming scenario in which the data is fed periodically, which makes the
existing approach impractical for on-device deployment.
In this chapter, we extend the low-rank transformer (LRT) [116] to develop a compact
and more generalized model for speech recognition tasks. We show that our LRT model
can signiÔ¨Åcantly reduce the number of parameters and the total computational cost of the
model. Additionally, our experiment result suggests that our factorization can help the
model to generalize better which enables the model to achieve a slightly higher evaluation
score compared to the non-factorized model.
32

--- PAGE 44 ---
4.1 Methodology
We extend the Low-Rank Transformer (LRT) model [116] to enable processing audio data
for speech recognition tasks. Our LRT variant accepts audio data in form of spectrograms
of the power spectral density as its input and produces a sequence of characters as its
output. The spectrogram S2Rftis computed from the magnitude squared of the short-
time Fourier-transform (STFT) output, where fdenotes the length of the Fourier vector
andtdenotes the length of time segments. We extend LRT by adding a speciÔ¨Åc module
for processing audio information before feeding the audio information into the LRT en-
coder layer. SpeciÔ¨Åcally, a small VGGish network [99] is incorporated to extract low-level
audio features from the spectrogram input. Our VGGish network consists of 4 convolu-
tion layers with 2 max-pooling layers and produces an output tensor I2Rf
4t
4c, where
c represents the number of output channels and in our conÔ¨Åguration c=128. We then
merge the frequency dimensionf
4and the channel dimension cproducing from the the
tensorIproducing a matrix X2Rtcf
4and feed the matrix Xinto the LRT encoder layers.
The illustration of the VGGish network is shown in Figure 4.1.
3x3 conv , 64
3x3 conv , 64
max pool 1/2
3x3 conv , 128
3x3 conv , 128
max pool 1/2Low-Level
Audio Feature
Figure 4.1. ConÔ¨Åguration of our VGGish network
To generate the character output, we create a vocabulary of characters consisting of all
the characters in the dataset and add a projection layer after the output layer of the LRT
decoder layers. SpeciÔ¨Åcally, we add an output projection W2Rdmodeldvocab followed
by a softmax layer to calculate the probability of the output grapheme at a speciÔ¨Åed time-
step. The depiction of our speech recognition LRT model is shown in Figure 4.2.
33

--- PAGE 45 ---
LRMHALRFFLRMHA
Masked
LRMHA
SpectrogramVGG
Outputs
(shifted right)Character
EmbeddingsM xx NLRFFLinearPredictions
(Grapheme)
Positional
Encoding
SoftmaxFigure 4.2. Low-Rank Transformer Architecture. Low-Rank Transformer consists of Nlay-
ers of low-rank transformer encoder and Mlayers of low-rank transformer decoder. [116]
4.2 Experimental Setup
4.2.1 Dataset
We conduct our experiments on two speech recognition datasets: 1) a multi-accent Man-
darin speech dataset, AiShell-1 [13], and 2) a conversational telephone speech recogni-
tion dataset, HKUST [70]. We initialize our vocabulary with all characters in each corpus
such that there is no out-of-vocabulary (OOV) and we add three additional special tokens:
<PAD>,<SOS>, and<EOS>for training and inference purposes. We preprocess the raw
audio data into spectrograms with a hop length of 20ms, a window size of 10ms, and a
length of FFT vector of 320. Table 4.1 shows the duration statistics of the datasets.
34

--- PAGE 46 ---
Dataset Split Durations (h)
AiShell-1Train 150
Valid 10
Test 5
HKUSTTrain 152
Valid 4.2
Test 5
Table 4.1. Duration statistics for AiShell-1 and HKUST datasets
4.2.2 Hyperparameters
To analyze the performance and efÔ¨Åciency trade-off over different rank r, we conduct
experiment on our LRT model with three different settings for rank r, i.e,r=100,r=75
andr=50. For all experimental settings, we use VGG network with 6 convolutional
layers, two LRT encoder layers, and four LRT decoder layers, yielding three different
models: LRT( r=100) with 12M parameters, LRT( r=75) with 10M parameters, and
LRT(r=50) with 8M parameters.
4.2.3 Baselines
For comparison with our LRT models, we develop three transformer models with two
transformer encoder layers and four transformer decoder layers. We use different dimmodel
anddiminner settings for the three models, producing Transformer (large) with 23M pa-
rameters, Transformer (medium) with 12M parameters, and Transformers (small) with
8M parameters. In terms of model size, our LRT( r=100) model is comparable to the
Transformer (medium) model and LRT( r=50) model is comparable to the Transformer
(small) model. For additional comparison, we provide results gathered from the previous
works for each dataset.
4.2.4 Training and Evaluation
In the training phase, we use the cross-entropy loss as the objective function. We optimize
all models by using Adam optimizer [54]. In the evaluation phase, we generate the se-
quence with beam-search by selecting the best sub-sequence scored using the probability
35

--- PAGE 47 ---
Model Params CER
Hybrid approach
TDNN-HMM [13]* - 8.5%
End-to-end approach
Attention Model [69]* - 23.2%
+ RNNLM [69]* - 22.0%
CTC [68]* 11.7M 19.43%
Framewise-RNN [68]* 17.1M 19.38%
ACS + RNNLM* [69] 14.6M 18.7%
Transformer (large) 25.1M 13.49%
Transformer (medium) 12.7M 14.47%
Transformer (small) 8.7M 15.66%
LRT (r=100) 12.7M 13.09%
LRT (r=75) 10.7M 13.23%
LRT (r=50) 8.7M 13.60%Model Params CER
Hybrid approach
DNN-hybrid [44]* - 35.9%
LSTM-hybrid (with perturb.) [44]* - 33.5%
TDNN-hybrid, lattice-free MMI
(with perturb.) [44]*- 28.2%
End-to-end approach
Attention Model [44]* - 37.8%
CTC + LM [74]* 12.7M 34.8%
MTL + joint dec. (one-pass) [44]* 9.6M 33.9%
+ RNNLM (joint train) [44]* 16.1M 32.1%
Transformer (large) 22M 29.21%
Transformer (medium) 11.5M 29.73%
Transformer (small) 7.8M 31.30%
LRT (r=100) 11.5M 28.95%
LRT (r=75) 9.7M 29.08%
LRT (r=50) 7.8M 30.74%
Table 4.2. Results on AiShell-1 (left) and HKUST (right) test sets.* The evaluation score is
retrieved from the previous studies. We approximate the number of parameters based on
the description in the previous studies.
of the sentence P(Y). The probability of the sentence P(Y)is calculated with the following
equation:
P(Y) =Ptrans(YjX) +√à
wc(Y), (4.1)
whereis the parameter to control the decoding probability from the decoder Ptrans(YjX),
andis the parameter to control the effect of the word count wc(Y). We use=1,=0.1,
and a beam size of 8. We evaluate our model by calculating the character error rate (CER).
We run our experiment on a single GeForce GTX 1080Ti GPU and Intel Xeon E5-2620 v4
CPU cores.
4.3 Result and Discussion
4.3.1 Evaluation Performance
As shown from the experiment results in Table 4.2, LRT models gain some improvement
compared to the original transformer model. For instance, LRT ( r=100) model achieves
36

--- PAGE 48 ---
01234training lossLRT(r=50)LRT(r=100)Transformer(small)Transformer(medium)
0 10 20 30 40
epoch01234validation lossLRT(r=50)LRT(r=100)Transformer(small)Transformer(medium)Figure 4.3. Training and validation losses on AiShell-1 data. [116]
better score compared to Transformer (large) model with only around 50% of its param-
eters. LRT (r=100) model outperforms all original transformer models both in AiShell-1
and HKUST test sets, with a 13.09% CER and a 28.95% CER, respectively. Compared to
TDNN-HMM, our LRT reduces the gap between the HMM-based hybrid and fully end-to-
end approaches without leveraging any data perturbation strategy or external language
model. Interestingly, as shown in Figure 4.3, LRT models achieve lower validation loss
compared to the Transformer (large) model, which suggests that LRT models regularize
better compared to the original transformer model.
4.3.2 Space and Time EfÔ¨Åciency
In terms of space efÔ¨Åciency, as shown in Table 4.2, in the AiShell-1 test set, our LRT ( r=50)
model achieves similar performance to the Transformer (large) model with only around
35% of the Transformer (large) model parameters. While in the HKUST test set, our LRT
(r=75) model achieves similar performance to the Transformer (large) with only 45%
of its parameters. These facts suggest that low-rank approximation can halve the space
requirement of transformer models without hurting their performance. In terms of time
efÔ¨Åciency, as shown in Table 4.3, LRT ( r=75) models gain generation time speed-up by
37

--- PAGE 49 ---
Table 4.3. Compression rate and inference speed-up of LRT models compared to the Trans-
former (large) baseline. CER and j¬ØXjdenote the improvement, and the mean length of
generated sequences.
dataset r CER compress.speed-upj¬ØXj
GPU CPU only
AiShell-1 baseline 0 0 1 1 23.08
100 0.40% 49.40% 1.17x 1.15x 23.15
75 0.26% 57.37% 1.23x 1.16x 23.17
50 -0.11% 65.34% 1.30x 1.23x 23.19
HKUST baseline 0 0 1 1 22.43
100 0.26% 47.72% 1.21x 1.14x 22.32
75 0.13% 55.90% 1.26x 1.15x 22.15
50 -1.53% 64.54% 1.35x 1.22x 22.49
1.23x-1.26x in the GPU and 1.15x-1.16x in the CPU compared to the Transformer (large)
baseline model. While LRT ( r=50) model gain generation time speed-up by 1.30x-1.35x
in the GPU and 1.22x-1.23x in the CPU. These suggest that low-rank approximation can
boost the generation speed of the transformer model by around 1.25x in the GPU and
1.15x in the CPU without hurting its performance and even lower rank can be applied to
further increase the speed-up ratio although it might degrade the overall performance of
the model.
4.4 Conclusion
In this chapter, we show the application of the low-rank transformer (LRT) model on
automatic speech recognition tasks. LRT is a memory-efÔ¨Åcient and fast neural architecture
that compress the network parameters and boosts the speed of the inference time by up to
1.35x in the GPU and 1.23x in the CPU, as well as the speed of the training time for end-
to-end speech recognition. Our LRT improves the performance even though the number
of parameters is reduced by 50% compared to the baseline transformer model. Similar to
the previous result in the MNIST dataset, LRT could generalize better than uncompressed
vanilla transformers and outperforms those from existing baselines on the AiShell-1 and
HKUST datasets in an end-to-end setting without using additional external data.
38

--- PAGE 50 ---
CHAPTER 5
Linformer for Alzheimer's Disease Risk Prediction
Recent development in genome modeling shows that deep learning models, including
attention-based variant, could achieve pretty impressive performance on multiple hap-
loid regulatory elements prediction tasks in genomics outperforming the other models by
a large margin [125, 123, 47, 109 ?, 77, 122]. Extending this approach for phenotype pre-
diction, such as disease risk prediction, might provide huge advantages from improving
the prediction accuracy and creating a new set of analytical tools for better understanding
the genomics interaction of a certain phenotype characteristic. In this chapter, we explore
the possibility of applying the Linformer model on Alzheimer‚Äôs disease risk prediction
tasks using genomics sequences. Alzheimer‚Äôs disease is one of the most devastating brain
disorders in the elderly. It is estimated that nearly 36 million are affected by Alzheimer‚Äôs
disease globally and the number is expected to reach 115 million by 2050 [45]. Disease risk
prediction for Alzheimer‚Äôs disease can help to prolong the health span of an individual
with potential Alzheimer‚Äôs disease trait and enable research to get a better understanding
of the disease progression from a very early stage.
Compared to haploid regulatory elements prediction, genomic modeling for disease
risk prediction requires the model to capture a much larger scale of the input sequence.
In regulatory elements prediction, regions are usually short and indicated by a certain
pattern that is located inside regions. For instance, promoter is about 100-1000 nts [51]
long and is associated with core promoter motifs such as TATA-box and CAAT-box, and
GC-box [11, 72]. While enhancer is about 50 - 1,500nts long and is associated with GATA-
box and E-box [11, 2, 81]. On the other hand, disease risk prediction requires the model
to capture the aberration within at less than a single gene and the average gene length
inside the human genome is around 24,000 base pairs long [32]. This sequence will be
even longer when we would like to consider neighboring regulatory elements such as
promoter, and the remote regulatory elements such as enhancer and silencer, which could
be located up to one million base pairs away from the corresponding gene[83]. Moreover,
39

--- PAGE 51 ---
most animals genomes, including humans, are diploid. This will not only increase the
size of the data but also requires additional methods to encode and process the diploid
information as the haploid sequences (haplotypes) of a diploid chromosome is not directly
observable.
In this chapter, we tackle the two problems mentioned above and develop the Ô¨Årst-
ever disease risk prediction model for Alzheimer‚Äôs disease from genome sequence data.
To tackle the long input sequence, we propose two solutions: 1) we apply Linformer [112],
a variant of transformer model with linear attention, to enable processing long sequences,
and 2) we extend SentencePiece tokenization to aggregate multiple nucleotides into a sin-
gle token for different diploid sequence representations. We show that SentencePiece tok-
enization can reduce the computational cost-effectively by producing a set of tokens con-
structed from several base pairs. We thus employ the Linformer model with a low-rank
self-attention mechanism which allows the model to reduce the memory bottleneck from
O(n2)toO(n)space complexity which signiÔ¨Åcantly increases the capacity of the model
to process longer input sequence. These two solutions allow the model to capture 35
longer sequence compared to the prior works from 1,000nts to 35,0000nts long.
For processing the diploid chromosome, we introduce a diplotype representation, where
we encode a diploid chromosome sequence into 66 diplotype tokens comprising combi-
nation of 11 tokens which represents 4 nucleotide tokens ( A,G,T, and C), unspeciÔ¨Åed
token ( N), and insertion-deletion tokens ( AI,GI,TI,CI,NI, and DEL). Our experiment
result suggests that linear attention-mechanism with diplotype token representation can
effectively model long diplotype for disease prediction and can produce a higher score
compared to the Food and Drug Administration (FDA) approved feature set for predict-
ing Alzheimer‚Äôs disease. We further analyze that our model can capture important single
nucleotide polymorphisms (SNPs) which are located inside the gene and the regulatory
region. Our analysis approach can be further utilized to provide an explainable analysis
of the linear attention-based model.
40

--- PAGE 52 ---
G A T C A G AT
C
GAG A T C A
G A G C A(a) (b) (c)Figure 5.1. DNA sequence representation for haploid and diploid chromosome. (a) Hap-
lotype representation of a haploid chromosome (b) Two haplotypes representation of a
diploid chromosome. (c) Diplotype representation of a diploid chromosome.
5.1 Preliminaries
A human genome consists of 23 pairs of chromosomes, each chromosome comprises tens
to hundreds of millions of nucleotides long sequence. There are four different kind of
nucleotides: adenine ( A), guanine ( G), thymine ( T), and cytosine ( C) each connected to its
reverse complement, i.e., Awith TandCwith G, which construct a deoxyribonucleic acid
(DNA). Some regions inside a chromosome encode the template for creating a protein
sequence. These regions are called genes. As human chromosomes are diploid (paired),
there are also two copies of the same gene inside a human body. Each copy of a gene is
called an allele.
In 1911, the terms ‚Äúphenotype‚Äù and ‚Äúgenotype‚Äù are introduced[48]. Phenotype refers
to the physical or observable characteristics of an individual , while genotype refers to a
genetic characteristic of an individual. As the deÔ¨Ånition of genotype is rather broad and
Ô¨Çuid depending on the context, in this work, we use the term diploid genotype (diplo-
type) as the sequence of nucleotides from a paired chromosome and haploid genotype
(haplotype) as the sequence of nucleotides from a single chromosome. SpeciÔ¨Åcally, we
deÔ¨Åne a haplotype as a sequence H= (h1,h2, ...,hn), wherehidenotes the nucleotide cor-
responding to the position iand a diplotype as a sequence D= (d1,d2, ...,xdn), wheredi
denotes the pair-nucleotide corresponding to the position i.
To deÔ¨Åne a paired (diploid) chromosomes, we can represent the diploid chromosome
as either: 1) as a set of two haplotypes S=H1,H2whereH1 andH2 are the two haplotypes,
one for each chromosome; or 2) as a diplotype which can be visualized as an assembly
graphG= (B,E), whereBis a set of nucleotide, and Edenotes the edge connecting each
nucleotide. Intuitively, for a diploid chromosome, two haplotypes representation would
be the most biologically plausible way to representing a diploid chromosome sequence
41

--- PAGE 53 ---
T  A  T  A  A
T  A  C  A  AT  A  T  A  A
T  A  T  T  G  C   A  AT  A  T  A  A
T  A  ASubstitution Insertion Deletion
R R R
H H HFigure 5.2. Mutation patterns in genomic sequence. R denotes a reference sequence and
H denotes a haplotype compared to the reference.
as it matches the physical representation of the diploid chromosome. Unfortunately, due
to the nature of primer design in sequencing [85, 22], these haplotypes cannot be directly
observed with a standard sequencing technique. Figure 5.1 shows the example of each
sequence representation.
5.2 Methodology
Aside from using the Linformer model to improve the transformer‚Äôs efÔ¨Åciency, the choice
of the sequence representation and tokenization also play an important role. In the follow-
ing section, we describe how we represent a diploid sequence as a diplotype and elucidate
the subword tokenization method which can further improve the efÔ¨Åciency of the model.
5.2.1 Sequence Representation
In this work, we represent our diploid sequence with diplotype representation as shown
in Figure 5.1 (c). In diplotype representation, we represent each position as a diplotype
token. A diplotype token represents the combination of haplotype tokens including mu-
tation patterns. As shown in Figure 5.2, there are three mutation patterns that can occur
on a genomic sequence, i.e., substitution, insertion, and deletion. A substitution mutation
is commonly called as Single Nucleotide Polymorphism (SNP), while insertion and dele-
tion are commonly called as indel . To accommodate all possible mutations, the diplotype
token vocabulary is extended from the haplotype vocabulary in three steps. First, inser-
tion and deletion patterns are introduced by extending the Ô¨Åve tokens in the haplotype
vocabulary ( A,T,C,G,N) to 11 tokens ( A,T,C,G,N,DEL,AI,TI,CI,GI,NI).DEL denotes
deletion and AI,TI,CI,GI, and NIdenote insertion that comes with the corresponding
A,T,C,G,Ntokens. Second, a combination of the 11 tokens are added, e.g., A-T,C-A,
42

--- PAGE 54 ---
Map of Diplotype Tokens
A A DEL-A ËÖåA-C ÂóÑAI-C Áà∏C-G ÂöìCI-G ÊáÇ
C C DEL-AI ÊãîA-G ÈòøAI-CI ÊØîC-N ÊãÜCI-GIÁ≠î
G G DEL-CÂêÉA-NÂëµAI-G ÂÖ´C-T Á§§CI-N Ëææ
N N DEL-CI Êê≠A-T ÈîïAI-GI Èú∏C-CI ËΩ¶CI-NIÁ¨¨
T T DEL-G ÊÉ≥A-AIÂêñAI-N Â∑¥C-GI Â∫äCI-T Áò©
AI B DEL-GI È¶ôA-CI ‰ø∫AI-NI ÈÄºC-NI Á©øCI-TI Ê≤ì
CI D DEL-N Â≠¶A-GI ÂÆâAI-T ÊääC-TI Âá∫G-GI È´ò
GI H DEL-NI ËôöA-NI Ê°àAI-TIÁ¨îGI-N Ëù¶G-N Áªô
NI O DEL-T ÂæêA-TI ÊåâN-NI ËÆ∑GI-NIÂêàG-NI ËÇ°
TI U DEL-TI ÈúÄNI-T ÂñîN-TÂì™GI-T ËôæG-T ‰∏™
DEL X T-TI ÊãìNI-TI ‰æ¨N-TI Â®úGI-TI ÁõíG-TI ËØ•
Table 5.1. List of all diplotype tokens.
G-T,A-TI ,C-AI , and C-TI ; producing 66 tokens in total. Third, the diplotype tokens are
mapped into a single character code to allow tokenization over a sequence of diplotype
tokens. The complete map of diplotype tokens vocabulary is shown in Table 5.1.
5.2.2 Subword Tokenization
Before feeding the sequence into the model, tokenization methods such as k-mer tokeniza-
tion [76, 47] and gapped k-mer tokenization [34, 98] is commonly incorporated to enrich
the token representation. In this work, we incorporate a subword tokenization method
called SentencePiece[59]. SentencePiece is a subword tokenization method that doesn‚Äôt
require any language-speciÔ¨Åc prior. SentencePiece enables the application of subword
tokenization beyond the NLP Ô¨Åeld. Internally, SentencePiece uses a slightly modiÔ¨Åed ver-
sion of either Byte-pair-encoding (BPE), WordPiece, or Unigram subword tokenizations to
generate the vocabulary and tokenize the sequence. Aside from the capability provided
by the internal subword tokenization module, SentencePiece provides two additional pre-
processing phases: 1) character normalization and 2) white-space preservation. In the
character normalization phase, SentencePiece transforms a Unicode character into its se-
mantically equivalent decomposed-Unicode character following a certain normalization
standard. In white-space preservation phase, SentencePiece escapes the <space> char-
acter into a meta symbol _ (U+2581). The escaped sequence is then used as the input to
the internal subword tokenization method for further processing. For our experiment,
43

--- PAGE 55 ---
(a)
"ATGATG", "CTGCTG",
"GTGGTG", "CT ACTA""T", "G", "A","C",
"TG","CT","A T","GT"
Subword
Tokenizer
Training DataModel
(b)
"TGAGXCTG"Subword
Tokenizer
Sample SentenceTokenized Sentence
"TG", "A", "G",
"[UNK]","C", "TG"
Character
NormalizationWhitespace
Preservation
Sequence 
PreprocessingSequence 
PreprocessingMerge
RulesvocabularyFigure 5.3. SentencePiece with BPE subword tokenization. (a) Training phase. (b) Infer-
ence phase. The vocabulary is ordered by the frequency in descending order.
we utilize SentencePiece with NFCK1normalization standard and BPE tokenization for
performing the subword tokenization.
SentencePiece tokenization consists of two phases: training and inference. During the
training phase, given training data D, SentencePiece tokenization builds a vocabulary
with a dynamic length token representation. As SentencePiece supports several subword
tokenization algorithms, the vocabulary generation process is slightly different from one
to another. For BPE tokenization, the vocabulary is built bottom-up starting from single-
length character and iteratively add longer character pairs based on the most-frequent
pairs in D. For WordPiece tokenization, the vocabulary is also built-in bottom-up manner.
But different from BPE tokenization, WordPiece scores character pairs by their likelihood
inDinstead of its frequency. Unlike BPE and WordPiece, the Unigram method starts
with a large combination of tokens and iteratively reduce the vocabulary size by reduc-
ing symbols with the least contribution to the loss of the Unigram language model build
fromDThe training process of SentencePiece tokenization is stopped when it reaches a
speciÔ¨Åed vocabulary size. From the training phase, SentencePiece tokenization produces
a model Ô¨Åle consisting of the vocabulary and the rules to tokenize a sequence. During
the inference phase, given a sequence data S, SentencePiece tokenizes Sinto a set of to-
kens (t1,t2, ...,tn)according to the tokenization rules. Figure 5.3 shows the sample of the
1https://unicode.org/reports/tr15/
44

--- PAGE 56 ---
training and inference phase for the SentencePiece with BPE subword tokenization.
5.3 Experimental Setup
We conduct our experiment for Alzheimer‚Äôs disease risk prediction on the Chinese Co-
hort by comparing the effectiveness of the Linformer model with other baseline models.
In the following sections, we describe the SentencePiece training of the diplotype tokens,
the pre-training phase of the Linformer model, and the Ô¨Åne-tuning and evaluation of Lin-
former models.
5.3.1 Dataset Construction
We construct a dataset of genome sequences for predicting a speciÔ¨Åc disease called late-
onset Alzheimer‚Äôs disease (LOAD) [87] on Chinese Cohort. We collect the sequencing
data from 678 Hong Kong elderly with a minimum age of 65. The subjects are diagnosed
with any dementia-related issue through Montreal Cognitive Assessment (MoCA) test
[80]. The score is adjusted according to the demography information, i.e., age, gender,
and education year of the subject, and the Ô¨Ånal diagnosis is made by medical professionals
to decide whether the subject has cognitive impairment. From the 678 subjects, we Ô¨Ålter
out subjects with mild cognitive impairment (MCI) and other types of dementia that are
not related to Alzheimer‚Äôs Disease. After the Ô¨Åltering, we end up with 626 subjects with
386 subjects labeled as Alzheimer‚Äôs disease carrier (AD) and 240 subjects labeled as non-
carrier (NC). It is also worth noting that AD diagnosis through memory test is not always
accurate [15] and currently, a deÔ¨Ånite AD diagnosis can only be possible through post-
mortem diagnosis [24]. For each subject, we collect genome sequence information from
around the APOE region [126] located in chromosome 19 for each subject. The sequencing
is done by using the Ilumina platform with a read depth of 5 and a read length of 150. We
align the sequence data with bamtools [8] using hg19 reference genome [19] to align the
sequence data. For generating diplotype, we use pysam v0.15.4 and samtools v1.7 [65] to
generate the proÔ¨Åle for each position from the aligned read data and taking top-2 values
from the proÔ¨Åle to form the diplotype.
45

--- PAGE 57 ---
SentencePiece Training
For diplotype SentencePiece tokenizer, we build a training corpus generated from the
GrCh37 (hg19) human reference genome2and the dbSNP1543. We use SNPs which are
labeled COMMON4in the dbSNP154 to introduce mutation patterns into the training cor-
pus. We Ô¨Åll the remaining position with the sequence from GrCh37 (hg19) human ref-
erence genome to generate a complete diplotype of the chromosome sequence. As our
Ô¨Åne-tuning only focuses on the APOE region, we construct the training data for Sentence-
Piece from only the chromosome 19 sequence. We convert the chromosome 19 sequence
into a set of sentences S. To build the sentences S, we Ô¨Årst random sample k=50 starting
points from range 0 to 5,000 from the 5‚Äô end. From each starting point, we iteratively take
consecutive sequences with a random length from minimum length a=500 to maximum
lengthb=1000 until the sequence reaches the end of the corresponding chromosome. All
random samplings are taken uniformly. The sentence Sis then used as the training corpus
for SentencePiece training. SpeciÔ¨Åcally, we build a SentencePiece model for diplotype se-
quences with BPE tokenization to construct the vocabulary. We random sample 3,000,000
sentences to build a vocabulary of size 32000 with 5 and 66 base tokens for haplotype
and diplotype vocabulary respectively. We add three additional special tokens ‚Äú[UNK]‚Äù,
‚Äú[CLS]‚Äù, and ‚Äú[SEP]‚Äù to be used by the model in the pre-training and Ô¨Åne-tuning phases.
Pre-Training Phase
Following the success of the pre-training approach in many different Ô¨Åelds such as NLP ,
computer vision, and speech processing, we perform pre-training to allow the model to
learn the structure of a genome sequence. We use the same data as the one used for
generating the SentencePiece models, but we concatenate several sentences into a single
input sequence with a maximum sequence length of 4,096 tokens. Similar to language
model pre-training in NLP , we use a masked language modeling (MLM) objective to train
our model. For our model, we utilize a 12 layers Linformer model with 12 heads, hidden
2https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz
3https://ftp.ncbi.nih.gov/snp/redesign/archive/b154
4ACOMMON SNP is deÔ¨Åned as an SNP that has at least one 1000Genomes population with a minor allele of
frequency (MAF) >= 1% and for which 2 or more founders contribute to that minor allele frequency.
46

--- PAGE 58 ---
size of 768, feed-forward size of 3,072, drop out of 0.1, and low-rank factor of 256. We
use AdamW optimizer [71] to optimize our model with initial learning rate of 1e -4 and
apply a step-wise learning rate decay with =0.999991. We run the pre-training of our
model for 200,000 steps with a batch size of 256.
Fine-T uning and Evaluation Details
To evaluate the pre-trained models, we run a Ô¨Åne-tuning process to adapt the model for
Alzheimer‚Äôs disease risk prediction task. We feed the diplotype sequence and project
the output from [CLS] token to get the prediction logits. We Ô¨Åne-tune the model for
a maximum of 30 epochs with the early stopping of 5, the learning rate of 1 -5, and
step-wise learning rate decay with =0.9995. We use cross-entropy loss and AdamW
optimizer to optimize our model. We run the Ô¨Åne-tuning process 5 times with different
stratify splitting for train, validation, and valid. We evaluate our models by calculating
the AUC and F1-score from the model predictions and the phenotype label.
Baseline Model
For comparison with our Linformer model, we incorporate a baseline logistic regression
model trained with APOE- 3 and APOE- 4 variants which correspond to rs7412 and
rs429358 SNPs within the APOE region[9]. These features have been approved by the
Food and Drug Administration (FDA)5to be used as the measurand for risk of develop-
ing late-onset Alzheimer‚Äôs disease (LOAD).
5.4 Result and Discussion
5.4.1 Evaluation Performance
As shown from the experiment results in Table 5.2, the pre-trained Linformer model gains
some improvement compared to the baseline model. The model without pretraining
achieves a lower AUC score and a higher F1-score compared to the baseline model, this is
5https://www.accessdata.fda.gov/cdrh_docs/reviews/K192073.pdf
47

--- PAGE 59 ---
Model AUC (%) F1-Score (%)
Baseline Model
FDA-approved feats. 58.88 49.24
Linformer Model
Linformer (w/o pretraining) 56.49 56.83
Linformer (50k steps) 62.24 61.33
Linformer (100k steps) 62.94 61.17
Linformer (150k steps) 64.10 61.67
Linformer (200k steps) 64.35 61.57
Table 5.2. Finetuning result on Alzheimer‚Äôs disease prediction. 100k and 200k denote the
number of pre-training steps.
Figure 5.4. Performance of Linformer model across different pretraining steps
probably due to the choice of probability threshold in the regression model which might
overÔ¨Åt the validation data. Furthermore, we see a clear uptrend for longer training which
suggests that understanding the pattern in the genomic sequence can provide beneÔ¨Åt on
the disease risk prediction task. Figure 5.4 shows the score improvement of the Linformer
model corresponds to the pretraining steps.
5.4.2 Effects of Sequence Length
We conduct further analysis to compare the effect of adding different non-coding region
size to the model performance. We Ô¨Årst Ô¨Åne-tune the model with only the APOE gene
sequence (3611nts), and gradually adding the upstream and downstream non-coding re-
gions. We experiment with additional 1000, 3000, 5000, 10000, and 15000 sequence lengths
from both upstream and downstream resulting in a sequence of length 5611nts, 9611nts,
48

--- PAGE 60 ---
Figure 5.5. Performance of Linformer (200k) model across different length of non-coding
region. 0 indicates the model is only trained on the APOE gene sequence without any
non-coding region. Green triangle indicates the mean score over 5 runs.
13611nts, 23611nts, and 33611nts, respectively. As shown in Figure 5.5, the performance
peaks when we provide an additional 5000 sequence length to the model, and it performs
much worse when we only use the APOE gene or when we add even more non-coding
regions. This result aligns with the top three promoters and enhancers with the highest
GeneHancer score [30], of which the three regions are located within the range of 5000nts
upstream. This indicates our model can extract valid features from the sequence, although
it is not robust against noises.
5.4.3 EfÔ¨Åciency Contribution
Baseline w/ Linformer w/ Subword w/ Linformer +
Tokenization Subword Tokenization
1024 4096 9,000 35,000
Table 5.3. Maximum sequence length with Linformer and Subword Tokenization.
As shown in Table 5.3, with our approach we can encode a sequence by up to 35000nts
with a 12-layer transformer model. This capability is achieved through two different
49

--- PAGE 61 ---
methods: 1) subword tokenization and 2) Linformer model. Subword tokenization re-
duces the memory usage and computational cost by aggregating unitary tokens into a
combination of tokens, based on our experiment, the average token length in our subword
tokenizer is 8.7, which yields, on average, reduction the sequence length by a factor of
8.7 on a linear-attention mechanism. On the other hand, the Linformer model reduces
the space and time complexity from O( n2) to O(n) and based on our experiment it can
process long sequence by 4longer compared to the original with the same memory
budget. This beneÔ¨Åt will deÔ¨Ånitely increase if we compare it with a much larger sequence
length. In this case, we can conclude that our approach can process 35 longer sequence
by leveraging subword tokenization which reduces the sequence length by a factor of
8.7, and the Linformer model which increases the sequence length limit to 4the orig-
inal transformer model.
5.5 Conclusion
In this chapter, we show the application of the Linformer model on the Alzheimer‚Äôs dis-
ease prediction task. Linformer is a memory-efÔ¨Åcient and fast transformer variant archi-
tecture that reduce the space and time complexity of the transformer model from O( n2)
to O(n). Along with subword tokenization, we can increase the input sequence limit to
35000nts compared to 1000nts in most of the prior works. With additional pre-training,
the Linformer model outperforms the existing baseline model with the pre-extracted fea-
ture using APOE- 3 and APOE- 4 variants. Our analysis on extending non-coding re-
gions suggests that our Linformer model can capture the required features in the regu-
latory regions correctly although the model is prone to noises. Lastly, we show that efÔ¨Å-
ciency can also come from the choice of the tokenization approach which in this case, we
show that subword tokenization can reduce the length of the input sequence signiÔ¨Åcantly
by a factor of 8.7.
50

--- PAGE 62 ---
CHAPTER 6
Conclusion
In this thesis, in light of improving the efÔ¨Åciency of the Transformer model, we introduce
Greenformers. Greenformers is a collection of efÔ¨Åcient methods for improving the efÔ¨Å-
ciency of the transformer model in terms of computation, memory, and/or the number of
parameters. We focus our Greenformers on improving the transformer model efÔ¨Åciency
with the low-rank approximation method as low-approximation can not only greatly im-
prove both computational and memory efÔ¨Åciency, but also reducing the number of the
model parameters signiÔ¨Åcantly. We incorporate two efÔ¨Åcient low-rank transformer model
alternatives in Greenformers, i.e., Low-Rank Transformer (LRT) and Linformer. We con-
duct multiple experiments to show the effectiveness of applying LRT and Linformer in
comparison to the original Transformer model.
Firstly, we conduct a comprehensive study on these two Transformer model variants
by performing a thorough efÔ¨Åciency analysis to get a deeper understanding of the best
data characteristic that is suitable to apply each model variant. Based on our analysis, the
LRT model is suitable for improving both the time and memory efÔ¨Åciency in processing
short-sequence ( 6512) input data, while the Linformer model is suitable for improving
the efÔ¨Åciency in processing long-sequence input data ( >512). We also show that LRT
is more suitable for on-device deployment, as it signiÔ¨Åcantly reduces the model size as
the rank decreases. Additionally, we compare the evaluation performance of the Trans-
former, LRT, and Linformer model in the MNIST digit recognition task where we show
that LRT and Linformer can retain the evaluation performance of the Transformer model
while signiÔ¨Åcantly increase its computational and memory efÔ¨Åciency. Lastly, we estimate
the efÔ¨Åciency factor for applying LRT to the BERT BASE model which shows a signiÔ¨Åcant
reduction for developing a pre-trained BERT BASE model in terms of computational, eco-
nomical, and environmental cost by more than 30% of the original cost.
Secondly, we present the effectiveness of our LRT model in the speech recognition task.
Our LRT model can reduce the model size and speed up the model compared to the orig-
51

--- PAGE 63 ---
inal transformer model while maintaining similar performance. We conduct experiments
on two speech recognition datasets, AiShell-1 and HKUST, to analyze the effect of rank r
on the performance and efÔ¨Åciency trade-off of our LRT model. Our result suggests that
with a model compression rate up to 60%, there is no degradation in terms of the eval-
uation performance and the low-rank approximation can even work as a regularizer and
increase the generalization of the model. With LRT, we can gain speed improvement up
to 1.35x in the GPU and 1.23x in the CPU compared to the original transformer model.
This Ô¨Ånding suggests that ASR models tend to be overparameterized and by reducing the
noisy parameters through low-rank approximation, we can produce a model with better
generalization.
Lastly, we extend the applicability of efÔ¨Åcient transformer models to genomics studies.
We apply the sequence modeling technique with transformer models to predict Alzheimer‚Äôs
disease in the Chinese cohort. We represent the problem as a long sequence prediction
problem, where we feed the model with a genotype of individual sequencing data from
a well-studied APOE region in chromosome 19. We utilize the Linformer model to han-
dle long sequences up to 35,000 nucleotides long and we show that self-supervised pre-
training of a Linformer model can help to boost the performance of the model by around
5% AUC compared to the baseline model. We further show that our Linformer model can
capture the common features in the region which is known to be related to Alzheimer‚Äôs
disease. Additionally, we show that the tokenization technique can also provide a huge
computation and memory efÔ¨Åciency by chunking the sequence which reduces the overall
sequence length to be processed by the model.
The main contribution of this thesis is to introduce Greenformers and emphasizes its
effectiveness in improving the efÔ¨Åciency of the Transformer model by providing a huge
reduction in terms of storage, memory, and computational cost without sacriÔ¨Åcing the
overall quality of the model. Additionally, this approach can open up further research di-
rection on long sequence processing tasks, such as long document summarization, video
processing, and genomics tasks. We hope in the future other Greenformers can be ex-
tended to cover more methods for improving the efÔ¨Åciency of a Transformer model and is
further applied to the existing Transformer model such as BERT and GPT2 models.
52

--- PAGE 64 ---
List of Publications
(* denotes equal contribution)
Genta Indra Winata, Samuel Cahyawijaya , Zihan Liu, Zhaojiang Lin, Andrea Madotto,
Pascale Fung. "Are Multilingual Models Effective in Code-Switching?" In Proceed-
ings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,
2021.
Wenliang Dai* Liu, Samuel Cahyawijaya *, Zihan Liu, Pascale Fung. "Multimodal
End-to-End Sparse Model for Emotion Recognition." In Proceedings of the 2021 Con-
ference of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, 2021.
Ye Jin Bang*, Etsuko Ishii*, Samuel Cahyawijaya *, Ziwei Ji*, Pascale Fung, "Model
Generalization on COVID-19 Fake News Detection", 1st International Workshop on
Combating Online Hostile Posts in Regional Languages during Emergency Situa-
tion, CONSTRAINT 2021 co-located with 35th AAAI Conference on ArtiÔ¨Åcial Intel-
ligence, AAAI 2021.
Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya , An-
drea Madotto, Pascale Fung, "CrossNER: Evaluating Cross-Domain Named Entity
Recognition", 35th AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2021.
Zihan Liu, Genta Indra Winata, Samuel Cahyawijaya , Andrea Madotto, Zhaojiang
Lin, Pascale Fung. "On the Importance of Word Order Information in Cross-lingual
Sequence Labeling." In AAAI, 2021.
Andrea Madotto, Samuel Cahyawijaya ,Genta Indra Winata , Yan Xu, Zihan Liu,
Zhaojiang Lin, Pascale Fung. "Learning Knowledge Bases with Parameters for Task-
Oriented Dialogue Systems." In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Findings, 2020.
53

--- PAGE 65 ---
Bryan Wilie*, Karissa Vincentio*, Genta Indra Winata*, Samuel Cahyawijaya *, Xi-
aohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri
Bahar, Ayu Purwarianti. "IndoNLU: Benchmark and resources for evaluating in-
donesian natural language understanding." In Proceedings of the 1st Conference of
the Asia-PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the
10th International Joint Conference on Natural Language Processing, 2020.
Genta Indra Winata*, Samuel Cahyawijaya *, Zhaojiang Lin, Zihan Liu, Peng Xu,
Pascale Fung. "Meta-transfer learning for code-switched speech recognition." In Pro-
ceedings of the 58th Annual Meeting of the Association for Computational Linguis-
tics, 2020.
Genta Indra Winata*, Samuel Cahyawijaya *, Zihan Liu*, Zhaojiang Lin, Andrea
Madotto, Peng Xu, Pascale Fung. "Learning Fast Adaptation on Cross-Accented
Speech Recognition." In INTERSPEECH, 2020.
Genta Indra Winata*, Samuel Cahyawijaya *, Zhaojiang Lin, Zihan Liu, and Pas-
cale Fung. "Lightweight and EfÔ¨Åcient End-to-End Speech Recognition Using Low-
Rank Transformer." In ICASSP 2020-2020 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pp. 6144-6148. IEEE, 2020.
54

--- PAGE 66 ---
Bibliography
[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,
Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: En-
coding long and structured inputs in transformers. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 268‚Äì284,
Online, November 2020. Association for Computational Linguistics.
[2] Kathleen P . Anderson, Scott C. Crable, and Jerry B Lingrel. Multiple proteins bind-
ing to a GATA-e box-GATA motif regulate the erythroid kr√ºppel-like factor (EKLF)
gene. Journal of Biological Chemistry , 273(23):14347‚Äì14354, June 1998.
[3] Matthias A√üenmacher and Christian Heumann. On the comparability of pre-trained
language models, 2020.
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
[5] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael
Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization, 2020.
[6] Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabatt, Sam Sheif-
fer, Anjali Sridhar, and Min Xu. Fairscale: A general purpose modular pytorch
library for high performance and large scale training. https://github.com/
facebookresearch/fairscale , 2021.
[7] Yejin Bang, Etsuko Ishii, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Model
generalization on covid-19 fake news detection. In Tanmoy Chakraborty, Kai Shu,
H. Russell Bernard, Huan Liu, and Md Shad Akhtar, editors, Combating Online Hos-
tile Posts in Regional Languages during Emergency Situation , pages 128‚Äì140, Cham,
2021. Springer International Publishing.
[8] D. W. Barnett, E. K. Garrison, A. R. Quinlan, M. P . Stromberg, and G. T. Marth. Bam-
Tools: a c++ api and toolkit for analyzing and managing BAM Ô¨Åles. Bioinformatics ,
27(12):1691‚Äì1692, April 2011.
55

--- PAGE 67 ---
[9] Micha√´l E. Belloy, Valerio Napolioni, and Michael D. Greicius. A quarter century
of APOE and alzheimer‚Äôs disease: Progress to date and the path forward. Neuron ,
101(5):820‚Äì838, March 2019.
[10] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document
transformer, 2020.
[11] E. M. Blackwood. Going the distance: A current view of enhancer action. Science ,
281(5373):60‚Äì63, July 1998.
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Process-
ing Systems , volume 33, pages 1877‚Äì1901. Curran Associates, Inc., 2020.
[13] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-
source mandarin speech corpus and a speech recognition baseline. In Oriental CO-
COSDA 2017 , page Submitted, 2017.
[14] Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xi-
aohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar,
Masayu Leylia Khodra, Ayu Purwarianti, and Pascale Fung. Indonlg: Benchmark
and resources for evaluating indonesian natural language generation, 2021.
[15] Juliana Cecato, Jos√© Eduardo Martinelli, Ivan Aprahamian, and M√¥nica Yassuda.
P3-069: Moca contributions to differential diagnosis among normal controls, mild
cognitive impairment and alzheimer‚Äôs disease in brazil. Alzheimer‚Äôs & Dementia ,
7(4S_Part_15):S535‚ÄìS535, 2011.
[16] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and
spell: A neural network for large vocabulary conversational speech recognition. In
56

--- PAGE 68 ---
2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
pages 4960‚Äì4964. IEEE, 2016.
[17] Liyang Chen, Yongquan Chen, Juntong Xi, and Xinyi Le. Knowledge from the orig-
inal network: restore a better pruned network with knowledge distillation. Complex
& Intelligent Systems , January 2021.
[18] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian
Weller. Rethinking attention with performers. In International Conference on Learning
Representations , 2021.
[19] Deanna M. Church, Valerie A. Schneider, Tina Graves, Katherine Auger, Fiona Cun-
ningham, Nathan Bouk, Hsiu-Chuan Chen, Richa Agarwala, William M. McLaren,
Graham R.S. Ritchie, Derek Albracht, Milinn Kremitzki, Susan Rock, Holland
Kotkiewicz, Colin Kremitzki, Aye Wollam, Lee Trani, Lucinda Fulton, Robert Ful-
ton, Lucy Matthews, Siobhan Whitehead, Will Chow, James Torrance, Matthew
Dunn, Glenn Harden, Glen Threadgold, Jonathan Wood, Joanna Collins, Paul
Heath, Guy GrifÔ¨Åths, Sarah Pelan, Darren Grafham, Evan E. Eichler, George We-
instock, Elaine R. Mardis, Richard K. Wilson, Kerstin Howe, Paul Flicek, and Tim
Hubbard. Modernizing reference genome assemblies. PLoS Biology , 9(7):e1001091,
July 2011.
[20] Wenliang Dai, Samuel Cahyawijaya, Zihan Liu, and Pascale Fung. Multimodal end-
to-end sparse model for emotion recognition. In NAACL , 2021.
[21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
Salakhutdinov. Transformer-XL: Attentive language models beyond a Ô¨Åxed-length
context. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pages 2978‚Äì2988, Florence, Italy, July 2019. Association for Computa-
tional Linguistics.
[22] Mohd Nazif Darawi, Chin Ai-Vyrn, Kalavathy Ramasamy, Philip Poi Jun Hua,
Tan Maw Pin, Shahrul Bahyah Kamaruzzaman, and Abu Bakar Abdul Majeed.
57

--- PAGE 69 ---
Allele-speciÔ¨Åc polymerase chain reaction for the detection of alzheimer‚Äôs disease-
related single nucleotide polymorphisms. BMC Medical Genetics , 14(1), February
2013.
[23] Misha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando
de Freitas. Predicting parameters in deep learning. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems , volume 26. Curran Associates, Inc., 2013.
[24] Michael A. DeTure and Dennis W. Dickson. The neuropathological diagnosis of
alzheimer‚Äôs disease. Molecular Neurodegeneration , 14(1):32, Aug 2019.
[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-
training of deep bidirectional transformers for language understanding. In Proceed-
ings of the 2019 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,
pages 4171‚Äì4186, Minneapolis, Minnesota, June 2019. Association for Computa-
tional Linguistics.
[26] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith.
Show your work: Improved reporting of experimental results. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages
2185‚Äì2194, Hong Kong, China, November 2019. Association for Computational Lin-
guistics.
[27] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence
sequence-to-sequence model for speech recognition. In 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5884‚Äì5888. IEEE,
2018.
[28] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to
trillion parameter models with simple and efÔ¨Åcient sparsity, 2021.
[29] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for
fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors,
58

--- PAGE 70 ---
Proceedings of the 34th International Conference on Machine Learning , volume 70 of Pro-
ceedings of Machine Learning Research , pages 1126‚Äì1135. PMLR, 06‚Äì11 Aug 2017.
[30] Simon Fishilevich, Ron Nudel, Noa Rappaport, Rotem Hadar, Inbar Plaschkes,
Tsippi Iny Stein, Naomi Rosen, Asher Kohn, Michal Twik, Marilyn Safran, Doron
Lancet, and Dana Cohen. GeneHancer: genome-wide integration of enhancers and
target genes in GeneCards. Database , 2017, January 2017.
[31] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse,
trainable neural networks. In ICLR . OpenReview.net, 2019.
[32] Gilad Fuchs, Yoav Voichek, Sima Benjamin, Shlomit Gilad, Ido Amit, and Moshe
Oren. 4sudrb-seq: measuring genomewide transcriptional elongation rates and ini-
tiation frequencies within cells. Genome Biology , 15(5), May 2014.
[33] Fei Gao, Lijun Wu, L. Zhao, Tao Qin, Xueqi Cheng, and Tie-Yan Liu. EfÔ¨Åcient se-
quence learning with group recurrent networks. In NAACL-HLT , 2018.
[34] Mahmoud Ghandi, Dongwon Lee, Morteza Mohammad-Noori, and Michael A.
Beer. Enhanced regulatory sequence prediction using gapped k-mer features. PLoS
computational biology , 10(7):e1003711‚Äìe1003711, Jul 2014.
[35] G. H. Golub and C. Reinsch. Singular value decomposition and least squares solu-
tions. Numer. Math. , 14(5):403‚Äì420, April 1970.
[36] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.) . Johns Hop-
kins University Press, USA, 1996.
[37] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks . 2011.
[38] Qiang Guo, Caiming Zhang, Yunfeng Zhang, and Hui Liu. An efÔ¨Åcient svd-based
method for image denoising. IEEE Transactions on Circuits and Systems for Video Tech-
nology , 26(5):868‚Äì880, 2016.
[39] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and
connections for efÔ¨Åcient neural network. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems , volume 28. Curran Associates, Inc., 2015.
59

--- PAGE 71 ---
[40] Babak Hassibi and David Stork. Second order derivatives for network pruning:
Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles, editors, Advances in
Neural Information Processing Systems , volume 5. Morgan-Kaufmann, 1993.
[41] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regular-
izers with gaussian error linear units. CoRR , abs/1606.08415, 2016.
[42] Danny Hernandez and Tom B. Brown. Measuring the algorithmic efÔ¨Åciency of neu-
ral networks, 2020.
[43] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a
neural network. In NIPS Deep Learning and Representation Learning Workshop , 2015.
[44] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan. Advances in joint ctc-
attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm.
Proc. Interspeech 2017 , pages 949‚Äì953, 2017.
[45] Rose Ann Huynh and Chandra Mohan. Alzheimer‚Äôs disease: Biomarkers in the
genome, blood, and cerebrospinal Ô¨Çuid. Frontiers in Neurology , 8, March 2017.
[46] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of
neural networks for efÔ¨Åcient integer-arithmetic-only inference. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2018.
[47] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. DNABERT: pre-trained
bidirectional encoder representations from transformers model for DNA-language
in genome. September 2020.
[48] W. Johannsen. The genotype conception of heredity. The American Naturalist ,
45(531):129‚Äì159, March 1911.
[49] Melvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng
Chen, Nikhil Thorat, Fernanda Vi√©gas, Martin Wattenberg, Greg Corrado, Macduff
Hughes, and Jeffrey Dean. Google‚Äôs multilingual neural machine translation sys-
tem: Enabling zero-shot translation. Transactions of the Association for Computational
Linguistics , 5:339‚Äì351, 2017.
60

--- PAGE 72 ---
[50] S. Jung, Changyong Son, Seohyung Lee, JinWoo Son, Jae-Joon Han, Youngjun Kwak,
Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by opti-
mizing quantization intervals with task loss. 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 4345‚Äì4354, 2019.
[51] Shaul Karni. Analysis of biological networks : Transcriptional networks-promoter
sequence analysis. Tel Aviv University, 2007.
[52] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret.
Transformers are RNNs: Fast autoregressive transformers with linear attention. In
Hal Daum√© III and Aarti Singh, editors, Proceedings of the 37th International Confer-
ence on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pages 5156‚Äì5165. PMLR, 13‚Äì18 Jul 2020.
[53] Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint ctc-attention based end-to-
end speech recognition using multi-task learning. In 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) , pages 4835‚Äì4839. IEEE,
2017.
[54] Diederik P . Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learn-
ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings , 2015.
[55] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efÔ¨Åcient trans-
former. In International Conference on Learning Representations , 2020.
[56] Sosuke Kobayashi, Sho Yokoi, Jun Suzuki, and Kentaro Inui. EfÔ¨Åcient estimation
of inÔ¨Çuence of a training instance. In Proceedings of SustaiNLP: Workshop on Sim-
ple and EfÔ¨Åcient Natural Language Processing , pages 41‚Äì47, Online, November 2020.
Association for Computational Linguistics.
[57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation
with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems ,
volume 25. Curran Associates, Inc., 2012.
61

--- PAGE 73 ---
[58] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for lstm networks. ICLR
Workshop , 2017.
[59] Taku Kudo and John Richardson. SentencePiece: A simple and language indepen-
dent subword tokenizer and detokenizer for neural text processing. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations , pages 66‚Äì71, Brussels, Belgium, November 2018. Association for
Computational Linguistics.
[60] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
and Radu Soricut. Albert: A lite bert for self-supervised learning of language repre-
sentations. In International Conference on Learning Representations , 2020.
[61] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touret-
zky, editor, Advances in Neural Information Processing Systems , volume 2. Morgan-
Kaufmann, 1990.
[62] Daniel Lee and H. Sebastian Seung. Algorithms for non-negative matrix factoriza-
tion. In T. Leen, T. Dietterich, and V . Tresp, editors, Advances in Neural Information
Processing Systems , volume 13. MIT Press, 2001.
[63] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye
Teh. Set transformer: A framework for attention-based permutation-invariant neu-
ral networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-
ings of the 36th International Conference on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 3744‚Äì3753. PMLR, 09‚Äì15 Jun 2019.
[64] Nayeon Lee, Zihan Liu, and Pascale Fung. Team yeon-zi at SemEval-2019 task 4:
Hyperpartisan news detection by de-noising weakly-labeled data. In Proceedings of
the 13th International Workshop on Semantic Evaluation , pages 1052‚Äì1056, Minneapo-
lis, Minnesota, USA, June 2019. Association for Computational Linguistics.
[65] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan, N. Homer, G. Marth, G. Abeca-
sis, and R. Durbin and. The sequence alignment/map format and SAMtools. Bioin-
formatics , 25(16):2078‚Äì2079, June 2009.
62

--- PAGE 74 ---
[66] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning
Ô¨Ålters for efÔ¨Åcient convnets. CoRR , abs/1608.08710, 2016.
[67] Jie Li, Xiaorui Wang, Yan Li, et al. The speechtransformer for large-scale mandarin
chinese speech recognition. In ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages 7095‚Äì7099. IEEE, 2019.
[68] Mohan Li, Yuanjiang Cao, Weicong Zhou, and Min Liu. Framewise supervised
training towards end-to-end speech recognition models: First results. Proc. Inter-
speech 2019 , pages 1641‚Äì1645, 2019.
[69] Mohan Li, Min Liu, and Hattori Masanori. End-to-end speech recognition with
adaptive computation steps. In ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages 6246‚Äì6250. IEEE, 2019.
[70] Yi Liu, Pascale Fung, Yongsheng Yang, Christopher Cieri, Shudong Huang, and
David Graff. Hkust/mts: A very large scale mandarin telephone speech corpus. In
Proceedings of the 5th International Conference on Chinese Spoken Language Processing ,
ISCSLP‚Äô06, page 724‚Äì735, Berlin, Heidelberg, 2006. Springer-Verlag.
[71] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Inter-
national Conference on Learning Representations , 2019.
[72] M Lundin, J O Nehlin, and H Ronne. Importance of a Ô¨Çanking AT-rich region in
target site recognition by the GC box-binding zinc Ô¨Ånger protein MIG1. Molecular
and Cellular Biology , 14(3):1979‚Äì1985, March 1994.
[73] Andrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan Liu,
Zhaojiang Lin, and Pascale Fung. Learning knowledge bases with parameters for
task-oriented dialogue systems. In Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 2372‚Äì2394, Online, November 2020. Association for
Computational Linguistics.
[74] Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom Ko, Florian Metze, and
Alexander Waibel. An empirical exploration of ctc acoustic models. In 2016 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages
2623‚Äì2627. IEEE, 2016.
63

--- PAGE 75 ---
[75] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu. Mixed precision training. In International Conference on
Learning Representations , 2018.
[76] Xu Min, Wanwen Zeng, Ning Chen, Ting Chen, and Rui Jiang. Chromatin acces-
sibility prediction via convolutional long short-term memory networks with k-mer
embedding. Bioinformatics , 33(14):i92‚Äìi101, 07 2017.
[77] Xu Min, Wanwen Zeng, Shengquan Chen, Ning Chen, Ting Chen, and Rui Jiang.
Predicting enhancers with deep convolutional neural networks. BMC Bioinformatics ,
18(S13), November 2017.
[78] A. Mirza, Mine Kerpicci, and S. Kozat. EfÔ¨Åcient online learning with improved lstm
neural networks. Digit. Signal Process. , 102:102742, 2020.
[79] Vinod Nair and Geoffrey E. Hinton. RectiÔ¨Åed linear units improve restricted boltz-
mann machines. In Proceedings of the 27th International Conference on International
Conference on Machine Learning , ICML‚Äô10, page 807‚Äì814, Madison, WI, USA, 2010.
Omnipress.
[80] Ziad S. Nasreddine, Natalie A. Phillips, Val√É¬©rie B√É¬©dirian, Simon Charbonneau,
Victor Whitehead, Isabelle Collin, Jeffrey L. Cummings, and Howard Chertkow.
The montreal cognitive assessment, MoCA: A brief screening tool for mild cognitive
impairment. Journal of the American Geriatrics Society , 53(4):695‚Äì699, April 2005.
[81] S. Ogilvy, R. Ferreira, S. G. Piltz, J. M. Bowen, B. Gottgens, and A. R. Green.
The SCL +40 enhancer targets the midbrain together with primitive and deÔ¨Ånitive
hematopoiesis and is regulated by SCL and GATA proteins. Molecular and Cellular
Biology , 27(20):7206‚Äì7219, October 2007.
[82] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
Alexander Ku, and Dustin Tran. Image transformer. In Jennifer Dy and Andreas
Krause, editors, Proceedings of the 35th International Conference on Machine Learning ,
volume 80 of Proceedings of Machine Learning Research , pages 4055‚Äì4064. PMLR, 10‚Äì
15 Jul 2018.
64

--- PAGE 76 ---
[83] Len A. Pennacchio, Wendy Bickmore, Ann Dean, Marcelo A. Nobrega, and Gill
Bejerano. Enhancers: Ô¨Åve essential questions. Nature Reviews Genetics , 14(4):288‚Äì
295, March 2013.
[84] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In
Proc. of NAACL , 2018.
[85] Tran Thu Ha Pham, Quang Binh Tran, Chonlaphat Sukasem, Van Dinh Nguyen,
Chi Hieu Chu, Thi Quynh Nga Do, Ngoc Phuong Mai Tran, and Thanh Huong
Phung. A novel allele-speciÔ¨Åc pcr protocol for the detection of the hla-c*03:02 allele,
a pharmacogenetic marker, in vietnamese kinh people. The Application of Clinical
Genetics , Volume 14:27‚Äì35, February 2021.
[86] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen tau Yih, Sinong Wang, and Jie Tang.
Blockwise self-attention for long document understanding, 2020.
[87] Gil D. Rabinovici. Late-onset alzheimer disease. Continuum (Minneapolis, Minn.) ,
25(1):14‚Äì33, Feb 2019.
[88] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners. 2019.
[89] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timo-
thy P . Lillicrap. Compressive transformers for long-range sequence modelling. In
International Conference on Learning Representations , 2020.
[90] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory
optimizations toward training trillion parameter models. In Proceedings of the Inter-
national Conference for High Performance Computing, Networking, Storage and Analysis ,
SC ‚Äô20. IEEE Press, 2020.
[91] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed:
System optimizations enable training deep learning models with over 100 billion pa-
rameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowl-
edge Discovery &amp; Data Mining , KDD ‚Äô20, page 3505‚Äì3506, New York, NY, USA,
2020. Association for Computing Machinery.
65

--- PAGE 77 ---
[92] R. Reed. Pruning algorithms-a survey. IEEE Transactions on Neural Networks ,
4(5):740‚Äì747, 1993.
[93] Aurko Roy*, Mohammad Taghi Saffar*, David Grangier, and Ashish Vaswani. EfÔ¨Å-
cient content-based sparse attention with routing transformers, 2020.
[94] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter. In 5th Workshop on
Energy EfÔ¨Åcient Machine Learning and Cognitive Computing @ NeurIPS 2019 , 2019.
[95] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun.
ACM , 63(12):54‚Äì63, November 2020.
[96] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. EfÔ¨Å-
cient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision (WACV) , pages 3531‚Äì3539, Jan-
uary 2021.
[97] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language
models using model parallelism, 2020.
[98] Avanti Shrikumar, Eva Prakash, and Anshul Kundaje. GkmExplain: fast and ac-
curate interpretation of nonlinear gapped k-mer SVMs. Bioinformatics , 35(14):i173‚Äì
i182, July 2019.
[99] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for
large-scale image recognition. In ICLR , 2015.
[100] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy con-
siderations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , pages 3645‚Äì3650, Florence, Italy, July 2019.
Association for Computational Linguistics.
[101] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny
Zhou. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In
66

--- PAGE 78 ---
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,
pages 2158‚Äì2170, Online, July 2020. Association for Computational Linguistics.
[102] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with
neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems , volume 27.
Curran Associates, Inc., 2014.
[103] Mingxing Tan and Quoc Le. EfÔ¨ÅcientNet: Rethinking model scaling for convolu-
tional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
Proceedings of the 36th International Conference on Machine Learning , volume 97 of Pro-
ceedings of Machine Learning Research , pages 6105‚Äì6114. PMLR, 09‚Äì15 Jun 2019.
[104] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse Sinkhorn
attention. In Hal Daum√© III and Aarti Singh, editors, Proceedings of the 37th Interna-
tional Conference on Machine Learning , volume 119 of Proceedings of Machine Learning
Research , pages 9438‚Äì9447. PMLR, 13‚Äì18 Jul 2020.
[105] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
Jinfeng Rao, Liu Yang, Sebastian Ruder, and Don Metzler. Long range arena : A
benchmark for efÔ¨Åcient transformers. In ICLR 2021 , 2021.
[106] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena
: A benchmark for efÔ¨Åcient transformers. In International Conference on Learning
Representations , 2021.
[107] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. EfÔ¨Åcient transformers:
A survey, 2020.
[108] Zolt√°n T√ºske, George Saon, Kartik Audhkhasi, and Brian Kingsbury. Single
Headed Attention Based Sequence-to-Sequence Model for State-of-the-Art Results
on Switchboard. In Proc. Interspeech 2020 , pages 551‚Äì555, 2020.
[109] Ramzan Kh. Umarov and Victor V . Solovyev. Recognition of prokaryotic and eu-
karyotic promoters using convolutional deep learning neural networks. PLOS ONE ,
12(2):e0171410, February 2017.
67

--- PAGE 79 ---
[110] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems , volume 30. Curran Asso-
ciates, Inc., 2017.
[111] Alex Wang and Thomas Wolf. Overview of the SustaiNLP 2020 shared task. In
Proceedings of SustaiNLP: Workshop on Simple and EfÔ¨Åcient Natural Language Processing ,
pages 174‚Äì178, Online, November 2020. Association for Computational Linguistics.
[112] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-
attention with linear complexity, 2020. cite arxiv:2006.04768.
[113] Yu-Xiong Wang and Yu-Jin Zhang. Nonnegative matrix factorization: A comprehen-
sive review. IEEE Transactions on Knowledge and Data Engineering , 25(6):1336‚Äì1353,
2013.
[114] Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiao-
hong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri
Bahar, and Ayu Purwarianti. IndoNLU: Benchmark and resources for evaluating
Indonesian natural language understanding. In Proceedings of the 1st Conference of
the Asia-PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th In-
ternational Joint Conference on Natural Language Processing , pages 843‚Äì857, Suzhou,
China, December 2020. Association for Computational Linguistics.
[115] Genta Indra Winata. Multilingual transfer learning for code-switched language and
speech neural modeling. arXiv preprint arXiv:2104.06268 , 2021.
[116] Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale
Fung. Lightweight and efÔ¨Åcient end-to-end speech recognition using low-rank
transformer. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 6144‚Äì6148, 2020.
[117] Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, and
Pascale Fung. Meta-transfer learning for code-switched speech recognition. pages
3770‚Äì3776, July 2020.
68

--- PAGE 80 ---
[118] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea
Madotto, and Pascale Fung. Are multilingual models effective in code-switching?
InProceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-
Switching , pages 142‚Äì153, Online, June 2021. Association for Computational Lin-
guistics.
[119] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea
Madotto, and Pascale Fung. Are multilingual models effective in code-switching?
InProceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-
Switching , pages 142‚Äì153, Online, June 2021. Association for Computational Lin-
guistics.
[120] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea
Madotto, Peng Xu, and Pascale Fung. Learning Fast Adaptation on Cross-Accented
Speech Recognition. In Proc. Interspeech 2020 , pages 1276‚Äì1280, 2020.
[121] Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking network
pruning ‚Äì under the pre-train and Ô¨Åne-tune paradigm. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pages 2376‚Äì2382, Online, June 2021. Association for
Computational Linguistics.
[122] Qijin Yin, Mengmeng Wu, Qiao Liu, Hairong Lv, and Rui Jiang. Deephistone: a deep
learning approach to predicting histone modiÔ¨Åcations. BMC Genomics , 20(2):193,
Apr 2019.
[123] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
and Amr Ahmed. Big bird: Transformers for longer sequences. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems , volume 33, pages 17283‚Äì17297. Curran Associates, Inc.,
2020.
[124] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun
Liu. TernaryBERT: Distillation-aware ultra-low bit BERT. In Proceedings of the 2020
69

--- PAGE 81 ---
Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 509‚Äì
521, Online, November 2020. Association for Computational Linguistics.
[125] Jian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with
deep learning‚Äìbased sequence model. Nature Methods , 12(10):931‚Äì934, August 2015.
[126] Xiaopu Zhou, , Yu Chen, Kin Y. Mok, Timothy C. Y. Kwok, Vincent C. T. Mok, Qi-
hao Guo, Fanny C. Ip, Yuewen Chen, Nandita Mullapudi, Paola Giusti-Rodr√≠guez,
Patrick F. Sullivan, John Hardy, Amy K. Y. Fu, Yun Li, and Nancy Y. Ip. Non-coding
variability at the APOE locus contributes to the alzheimer‚Äôs risk. Nature Communi-
cations , 10(1), July 2019.
70
