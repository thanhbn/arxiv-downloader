# 2309.09958.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.09958.pdf
# Kích thước tệp: 126855 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2309.09958v1  [cs.CV]  18 Sep 2023Một Nghiên Cứu Thực Nghiệm về Mở Rộng Quy Mô Các Mô Hình Đa Phương Tiện Lớn được Điều Chỉnh Hướng Dẫn
Yadong Lu∗1, Chunyuan Li∗2, Haotian Liu3, Jianwei Yang2, Jianfeng Gao2, Yelong Shen1
1Microsoft Azure AI2Microsoft Research3University of Wisconsin–Madison
Tóm tắt
Điều chỉnh hướng dẫn thị giác gần đây đã cho thấy tiến triển đáng khuyến khích với các mô hình đa phương tiện lớn (LMM) mã nguồn mở như LLaVA và MiniGPT-4. Tuy nhiên, hầu hết các nghiên cứu hiện tại về LMM mã nguồn mở được thực hiện sử dụng các mô hình có 13B tham số hoặc nhỏ hơn. Trong bài báo này, chúng tôi trình bày một nghiên cứu thực nghiệm về việc mở rộng quy mô LLaVA lên đến 33B và 65B/70B, và chia sẻ các phát hiện từ những khám phá của chúng tôi trong độ phân giải hình ảnh, trộn dữ liệu và các phương pháp huấn luyện hiệu quả tham số như LoRA/QLoRA. Những yếu tố này được đánh giá thông qua tác động của chúng đến khả năng đa phương tiện và ngôn ngữ khi hoàn thành các nhiệm vụ thực tế trong tự nhiên. Chúng tôi nhận thấy rằng việc mở rộng quy mô LMM liên tục cải thiện hiệu suất mô hình và nâng cao khả năng ngôn ngữ, và hiệu suất của việc điều chỉnh LoRA/QLoRA cho LMM có thể so sánh được với hiệu suất của việc tinh chỉnh toàn bộ mô hình. Ngoài ra, nghiên cứu làm nổi bật tầm quan trọng của độ phân giải hình ảnh cao hơn và việc trộn dữ liệu đa phương tiện-ngôn ngữ để cải thiện hiệu suất LMM, và điều chỉnh hướng dẫn thị giác đôi khi có thể cải thiện khả năng ngôn ngữ thuần túy của LMM. Chúng tôi hy vọng nghiên cứu này làm cho nghiên cứu LMM tiên tiến ở quy mô lớn hơn trở nên dễ tiếp cận hơn, do đó giúp thiết lập các baseline mạnh mẽ hơn cho nghiên cứu tương lai. Mã nguồn và các checkpoint sẽ được công khai.

1 Giới thiệu
Các nghiên cứu gần đây về các mô hình đa phương tiện lớn (LMM) [9,10] đã tập trung vào các phương pháp điều chỉnh hướng dẫn thị giác [12]. Kết quả rất hứa hẹn: ví dụ, dự án mã nguồn mở Large Language and Vision Assistant (LLaVA) cho thấy rằng việc huấn luyện một mô hình ngôn ngữ lớn (LLM) 7B với dữ liệu tuân theo hướng dẫn đa phương tiện trong 3 giờ trên 8 GPU A-100 dẫn đến một LMM có khả năng hiểu và suy luận thị giác mạnh mẽ trong tự nhiên: tái tạo một số ví dụ hấp dẫn nhất của mô hình GPT-4 đa phương tiện độc quyền của OpenAI [14]. Một ý tưởng tương tự được khám phá trong công trình đồng thời MiniGPT-4 [20]. Nó đã nhanh chóng trở thành một chủ đề nghiên cứu nổi bật, thúc đẩy sự phát triển của nhiều mô hình, benchmark và ứng dụng mới [10]. Tuy nhiên, chi phí tính toán cao đã khiến hầu hết các nghiên cứu hiện tại sử dụng các LLM 7B và 13B. Do đó, tác động của việc mở rộng quy mô mô hình một cách đáng kể lên ví dụ 33B và 65B vẫn chưa được khám phá.

Nghiên cứu này nhằm mục đích lấp đầy khoảng trống này bằng cách điều tra thực nghiệm các mô hình ngôn ngữ có kích thước lớn hơn cho LMM, chia sẻ những hiểu biết từ các thí nghiệm mở rộng quy mô của chúng tôi và thiết lập các baseline mạnh mẽ hơn sử dụng LLaVA quy mô lớn hơn cho nghiên cứu tương lai. Cụ thể, chúng tôi khám phá tác động của các kích thước mô hình lớn hơn, phương pháp điều chỉnh mô hình và trộn dữ liệu đối với hiệu suất mô hình, và trình bày các phát hiện và khuyến nghị của chúng tôi. Công thức mở rộng quy mô dẫn đến hiệu suất tiên tiến (SoTA) mới trên LLaVA-Bench [12] và MM-VET [19]. Chúng tôi hy vọng rằng các phát hiện và các checkpoint LLaVA lớn hơn của chúng tôi sẽ cung cấp một tài liệu tham khảo cho nghiên cứu tương lai về điều chỉnh hướng dẫn thị giác.

*Những tác giả này đóng góp ngang nhau vào công trình này
Bản thảo. Công việc đang tiến hành

--- TRANG 2 ---
2 Thiết lập Thí nghiệm
Các Checkpoint Mô hình. Để nghiên cứu tác động của việc mở rộng quy mô LLM đối với khả năng đa phương tiện, chúng tôi tăng kích thước mô hình ngôn ngữ lên 33B và 65B [15], ngoài các mô hình 7B và 13B được sử dụng cho LMM hiện tại.

•LLaVA-33B Chúng tôi sử dụng checkpoint Vicuna-33B mã nguồn mở1[16] để thực hiện huấn luyện hai giai đoạn. Dữ liệu huấn luyện là khoảng 125K cuộc trò chuyện được thu thập từ ShareGPT.com.

•LLaVA-65B Do thiếu checkpoint Vicuna-65B công khai, chúng tôi tiến hành huấn luyện riêng mô hình Vicuna-65B, sử dụng dữ liệu ShareGPT mà chúng tôi đã xử lý độc lập. Dữ liệu này chứa 159M token được sử dụng trong quá trình huấn luyện. Để so sánh, số lượng token được báo cáo sử dụng trong huấn luyện Vicuna 33B là 370M2.

Sau khi có LLM được điều chỉnh hướng dẫn, chúng tôi tuân theo [12] để thực hiện huấn luyện LLaVA lightning hai giai đoạn: (i)Giai đoạn 1: Tiền huấn luyện để Căn chỉnh Đặc trưng. Lớp chiếu tuyến tính được huấn luyện, nó ánh xạ đặc trưng thị giác (các đặc trưng trước lớp cuối cùng của bộ mã hóa hình ảnh được tiền huấn luyện) vào không gian nhúng từ của LLM. Cụ thể hơn, chiều chiếu là 1024 →6656 cho mô hình 33B và 1024 →8192 cho mô hình 65B, tương ứng. Trong giai đoạn này, chúng tôi sử dụng tập con cân bằng khái niệm của dữ liệu LAION-CC-SBU với 558K mẫu. (ii)Giai đoạn 2: Điều chỉnh Hướng dẫn Thị giác. Chúng tôi sử dụng bộ dữ liệu hướng dẫn đa phương tiện LLaVA-80K cho giai đoạn tinh chỉnh. Các lịch trình huấn luyện khác nhau được khám phá để cho phép mô hình tuân theo các hướng dẫn đa dạng để hoàn thành các nhiệm vụ trong tự nhiên, như sẽ được chi tiết bên dưới.

Phương pháp Điều chỉnh. Chúng tôi khám phá cả các mô-đun có thể huấn luyện và trộn dữ liệu huấn luyện để điều chỉnh hướng dẫn thị giác hiệu quả và hiệu suất của các mô hình lớn.

•Các mô-đun có thể huấn luyện. Ngoài việc điều chỉnh lớp chiếu tuyến tính, hai sơ đồ được xem xét để điều chỉnh LLM: (i)Tinh chỉnh toàn bộ mô hình của LLM và (ii)Các phương pháp huấn luyện hiệu quả tham số. Đối với cái sau, LoRA [7] và QLoRA [4] được sử dụng để cho phép chúng tôi điều chỉnh các mô hình lớn với tài nguyên tính toán hạn chế. Điều này nhằm mục đích hiểu sâu về sự đánh đổi giữa chi phí huấn luyện và hiệu suất mô hình.

•Trộn dữ liệu. Thông thường chỉ dữ liệu hướng dẫn đa phương tiện được sử dụng trong Giai đoạn-2. Chúng tôi tiếp tục xem xét việc trộn dữ liệu hướng dẫn chỉ ngôn ngữ ShareGPT với dữ liệu hướng dẫn đa phương tiện LLaVA-80K để hiểu sâu về sự đánh đổi giữa khả năng ngôn ngữ và đa phương tiện của mô hình.

Siêu tham số. Trong quá trình huấn luyện của cả hai giai đoạn, chúng tôi sử dụng thư viện DeepSpeed3 và sử dụng bộ tối ưu hóa ZeRO3, ngoại trừ các lần chạy QLoRA chúng tôi sử dụng ZeRO2. Chúng tôi sử dụng độ dài chuỗi tối đa là 2048. Đối với Giai đoạn 1, chúng tôi huấn luyện cả mô hình 33B và 65B với tốc độ học 1×10−4 không có weight decay, và tốc độ học với phân rã tuyến tính và khởi động tuyến tính cho 3% tổng số bước huấn luyện. Đối với Giai đoạn 2, chúng tôi sử dụng tốc độ học 2×10−5 trong tinh chỉnh toàn bộ để huấn luyện 1 epoch cho tất cả các mô hình trong tinh chỉnh toàn bộ, và tốc độ học 1×10−4 cho các lần chạy LoRA/QLoRA. Chúng tôi đã tiến hành một loạt tìm kiếm siêu tham số và đối với các lần chạy LoRA, và thấy rằng alpha LoRA lớn hơn hoặc tương đương tốc độ học lớn hơn là rất quan trọng để có hiệu suất tốt nhất. Cụ thể, chúng tôi sử dụng alpha LoRA bằng 2 lần rank LoRA, và tốc độ học 1×10−4, hoạt động tốt nhất cho tất cả các mô hình. Đối với tinh chỉnh toàn bộ, chúng tôi sử dụng tổng kích thước batch 512 trên 4 node A100, trong đó mỗi node này được trang bị 8 GPU A100-80G. Đối với các lần chạy LoRA/QLoRA, chúng tôi sử dụng tổng kích thước batch 64 trên 1 node A100 cho mô hình 33B và 2 node cho mô hình 65B.

3 Kết quả
Đầu tiên chúng tôi so sánh các checkpoint lớn của chúng tôi trên hai benchmark gần đây được thiết kế đặc biệt cho LMM, sau đó báo cáo các phát hiện của chúng tôi trong quá trình mở rộng quy mô các mô hình LLaVA.

--- TRANG 3 ---
Mô hình Suy luận Trò chuyện Chi tiết Tổng thể
Bard-0718 78.7 83.7 69.7 77.8
Bing-Chat-0629 90.1 59.6 52.2 71.5
LLaVA-13B (beam=1) 81.7 64.3 55.9 70.1
LLaVA-13B (beam=5) 84.3 68.4 59.9 73.5
LLaVA-33B (beam=1) 82.9 70.2 62.6 73.9
LLaVA-33B (beam=5) 83.5 72.6 61.9 74.8
LLaVA-65B (beam=1) 87.3 63.8 62.3 74.2
LLaVA-65B (beam=5) 88.7 59.4 65.7 74.4
Bảng 1: So sánh hiệu suất trên LLaVA-Bench. Kích thước beam search tại 1 và 5 được báo cáo.

Mô hình Nhận diện OCR Kiến thức Sinh tạo Không gian Toán học Tổng cộng
Kết quả của các LMM mã nguồn mở khác nhau được báo cáo trong bài báo MM-VET [19]
LLaMA-Adapter v2-7B [5] 16.8 7.8 2.5 3.0 16.6 4.4 13.6±0.2
OpenFlamingo-9B [1,2] 24.6 14.4 13.0 12.3 18.0 15.0 21.8±0.1
MiniGPT-4-8B [20] 27.4 15.0 12.8 13.9 20.3 7.7 22.1±0.1
BLIP-2-12B [11] 27.5 11.1 11.8 7.0 16.2 5.8 22.4±0.2
LLaVA-7B [12] 28.0 17.1 16.3 18.9 21.2 11.5 23.8±0.6
MiniGPT-4-14B [20] 29.9 16.1 20.4 22.1 22.2 3.8 24.4±0.4
Otter-9B [8] 28.4 16.4 19.4 20.7 19.3 15.0 24.6±0.2
InstructBLIP-14B [3] 30.8 16.0 9.8 9.0 21.1 10.5 25.6±0.3
InstructBLIP-8B [3] 32.4 14.6 16.5 18.2 18.6 7.7 26.2±0.2
LLaVA-13B [12] 30.9 20.1 23.5 26.4 24.3 7.7 26.4±0.1
MM-ReAct-GPT-3.5 [18] 24.2 31.5 21.5 20.7 32.3 26.2 27.9±0.1
LLaVA-7B (LLaMA-2) [12] 32.9 20.1 19.0 20.1 25.7 5.2 28.1±0.4
LLaVA-13B (V1.3, 336px) [12] 38.1 22.3 25.2 25.8 31.3 11.2 32.5±0.1
LLaVA-13B (LLaMA-2) [12] 39.2 22.7 26.5 29.3 29.6 7.7 32.9±0.1
MM-ReAct-GPT-4 [18] 33.1 65.7 29.0 35.0 56.8 69.2 44.6±0.2
Kết quả với các lần chạy thí nghiệm của chúng tôi
LLaVA-13B (LLaMA-2) 38.4 21.0 26.3 28.8 28.0 7.7 32.6±0.1
LLaVA-33B 38.5 25.0 26.2 28.2 29.2 7.7 32.9±0.3
LLaVA-33B (Data Mixing) 37.7 27.1 26.2 28.6 28.1 11.5 34.1±0.3
LLaVA-65B 39.2 28.2 26.2 28.3 33.0 15.0 35.5±0.3
LLaVA-65B (Data Mixing) 41.8 27.9 30.4 32.3 30.5 7.3 36.4±0.2

Bảng 2: Hiệu suất của các LMM mã nguồn mở khác nhau trên MM-VET. Lưu ý rằng MM-ReAct không phải là một mô hình đa phương tiện đơn lẻ, nó là một hệ thống được xây dựng trên việc liên kết các công cụ thị giác thông qua GPT-3.5 hoặc GPT-4, mà chúng tôi đính kèm như một tham khảo. Lần chạy thí nghiệm của chúng tôi trên LLaVA-13B (LLaMA-2) cho kết quả rất tương tự với cùng checkpoint được báo cáo trong bài báo MM-VET, cho thấy rằng các pipeline đánh giá của chúng tôi là nhất quán.

3.1 So sánh trên Benchmark
LLaVA-Bench. LLaVA-Bench (In-the-Wild)4[12] là một bộ dữ liệu đánh giá đa dạng gồm 24 hình ảnh với tổng cộng 60 câu hỏi, bao gồm các cảnh trong nhà và ngoài trời, meme, tranh vẽ, phác thảo. Mỗi hình ảnh được ghép với một mô tả chi tiết được tuyển chọn thủ công và một loạt câu hỏi được lựa chọn thích hợp liên quan đến các kịch bản trò chuyện thị giác mở. Mỗi câu hỏi thuộc về một trong ba loại nhiệm vụ: các cuộc trò chuyện chứa câu hỏi nhận diện thị giác đơn giản & Q&A, các mô tả chi tiết đặc trưng cho hình ảnh bằng một đoạn văn dài, và một nhiệm vụ suy luận phức tạp tập trung vào suy ra các hàm ý từ một hình ảnh. GPT-4 ngôn ngữ (gpt4-0314) được sử dụng để chấm điểm cho các câu trả lời được sinh ra. Điểm tương đối giữa đầu ra mô hình và phản hồi vàng được báo cáo. Chúng tôi so sánh LLaVA với các hệ thống trò chuyện thị giác thương mại bao gồm Microsoft BingChat5 và Google Bard6 trên LLaVA-Bench [12].

--- TRANG 4 ---
Kết quả được trình bày trong Bảng 1. Các checkpoint 33B và 65B vượt trội hơn mô hình LLaVA-13B và Bing Chat. Mặc dù thực tế là LLaVA-Bench nhỏ (do đó việc so sánh có thể không có ý nghĩa thống kê), kết quả rất đáng khuyến khích: so với LMM lớn, LMM mã nguồn mở nhỏ hiệu quả về chi phí hơn nhiều để được triển khai trong các ứng dụng thực tế. Với việc tăng độ trễ suy luận không đáng kể, chúng tôi có thể cải thiện đáng kể hiệu suất cho tất cả các kích thước mô hình bằng cách tăng kích thước beam search từ 1 lên 5. Kết quả của chúng tôi cho thấy rằng các mô hình LLaVA lớn hơn thường thể hiện hiệu suất tốt hơn trong các nhiệm vụ liên quan đến suy luận phức tạp và tạo ra các mô tả chi tiết, điều này đòi hỏi khả năng ngôn ngữ mạnh mẽ từ LLM lớn hơn. Ngoài ra, các mô hình LLaVA lớn hơn đạt được kết quả tương đương với BingChat trong các nhiệm vụ trò chuyện đa lượt, đa phương tiện đòi hỏi khả năng hiểu hình ảnh mạnh mẽ.

MM-VET. MM-VET [19] được thiết kế dựa trên giả định rằng khả năng hấp dẫn của việc giải quyết các nhiệm vụ phức tạp thường được đạt được bởi một LMM tổng quát có thể tích hợp nhiều khả năng thị giác-ngôn ngữ (VL) đa dạng. MM-Vet chứa 200 hình ảnh và 218 câu hỏi (mẫu), nhằm mục đích đánh giá 6 khả năng VL cốt lõi (nhận diện, OCR, kiến thức, sinh tạo ngôn ngữ, nhận thức không gian, và toán học) và sự kết hợp của chúng. Để đánh giá, một bộ đánh giá dựa trên LLM (gpt4-0613) được sử dụng để chấm điểm các đầu ra mở có các hình thức khác nhau. Trong Bảng 2, chúng tôi báo cáo kết quả trên MM-VET. Hiệu suất được cải thiện liên tục từ 13B đến 33B và 65B. Mô hình LLaVA lớn nhất cải thiện hiệu suất SoTA trong số các LMM mã nguồn mở từ đầu đến cuối. Những cải thiện đáng kể nhất được quan sát thấy khi đánh giá khả năng kiến thức và sinh tạo, tiếp theo là nhận diện và OCR. Hiệu suất về không gian và toán học vẫn tương đương. Kết quả cho thấy rằng khả năng LLM được cải thiện là công cụ quan trọng trong việc lưu trữ nhiều kiến thức hơn trong các trọng số và dẫn đến khả năng phản hồi ngôn ngữ mạnh mẽ hơn.

3.2 Mở rộng quy mô LLaVA
Các thí nghiệm được tiến hành để trả lời ba câu hỏi nghiên cứu.

1⃝ Yếu tố mở rộng quy mô nào quan trọng? Chúng tôi nghiên cứu đóng góp tương đối của ba yếu tố mở rộng quy mô đối với việc cải thiện hiệu suất của LLaVA. Kết quả được tóm tắt trong Bảng 3(a).

•Kích thước mô hình. Tăng kích thước mô hình liên tục cải thiện hiệu suất tổng thể. Chúng tôi suy đoán rằng kích thước dữ liệu lớn hơn là cần thiết để huấn luyện một mô hình lớn hơn. Ví dụ, nếu chúng tôi chỉ huấn luyện trên dữ liệu LLaVA-80K, chúng tôi thấy lợi ích nhỏ hơn khi kích thước mô hình trở nên lớn hơn.

•Độ phân giải hình ảnh. Bằng cách cố định bộ mã hóa hình ảnh CLIP ViT, chúng tôi so sánh các biến thể được tiền huấn luyện để nhận độ phân giải hình ảnh 224×224 và 336×336, và thấy rằng độ phân giải cao hơn liên tục mang lại cải thiện 2-3 điểm trên tất cả bốn kích thước LLM.

•Trộn dữ liệu. Các mô hình lớn hơn có xu hướng có khả năng cao hơn trong việc khớp với dữ liệu hướng dẫn. Bằng cách trộn dữ liệu hướng dẫn chỉ ngôn ngữ (ShareGPT) với LLaVA-80K, chúng tôi có thể cải thiện hiệu suất mô hình 2 điểm, so với việc huấn luyện chỉ trên dữ liệu hướng dẫn đa phương tiện.

Trong Bảng 3(b), chúng tôi trình bày kết quả của chúng tôi trên MM-Bench [13], chứa một tập hợp 2,974 câu hỏi, đánh giá kỹ năng suy luận của mô hình trong sáu danh mục. Sự kết hợp của ba yếu tố cải thiện mô hình LLaVA 7B cơ sở, được báo cáo trong [13].

2⃝ Khi nào nên xem xét phương pháp huấn luyện hiệu quả tham số? Khi kích thước mô hình tăng, việc xem xét sử dụng các phương pháp điều chỉnh hiệu quả hơn so với tinh chỉnh toàn bộ mô hình trở nên cần thiết. LoRA và QLoRA là các phương pháp điều chỉnh hiệu quả tham số nổi tiếng. Như được hiển thị trong Bảng 4, chúng tôi báo cáo chi phí tính toán sử dụng giờ GPU trên mỗi node, bởi vì đơn vị này có thể tương đương với giá $13.63/giờ (dòng ND A100 v4) trên Azure7. Tổng chi phí có thể được ước tính bằng cách nhân #giờ và #epoch.

Trong Bảng 4(a), chúng tôi huấn luyện cả mô hình 33B và 65B với rank LoRA 8 và 64 trong 1 epoch trên bộ dữ liệu điều chỉnh hướng dẫn LLaVA-80K. Đối với các mô hình có 33B tham số trở lên, khi chúng tôi tăng giá trị rank LoRA, chúng tôi nhận thấy sự tăng lên về cả hiệu suất và chi phí cho đến khi điều chỉnh toàn bộ mô hình đạt được hiệu suất tối đa cho một kích thước mô hình cụ thể. Trong trường hợp mô hình 13B, chúng tôi thấy rằng rank 64 có thể mang lại hiệu suất tương đương với điều chỉnh toàn bộ mô hình. Chi phí liên quan nhiều hơn đến tổng số tham số hơn là số lượng tham số có thể huấn luyện. Sự tăng chi phí do tăng rank LoRA cho một kích thước mô hình nhất định nhỏ hơn đáng kể so với sự tăng chi phí do mở rộng kích thước mô hình. Ví dụ, tăng rank LoRA từ 8 lên 64 gần như khớp với hiệu suất khi điều chỉnh LoRA một mô hình 65B với cùng rank, nhưng chỉ cần 50% chi phí huấn luyện của mô hình 65B. Trong thực tế, chúng tôi thấy rằng điều chỉnh mô hình 33B cung cấp sự đánh đổi tốt giữa chi phí và hiệu suất.

Các biến thể LoRA khác nhau có hiệu suất tương tự, và QLoRA yêu cầu chi phí bộ nhớ GPU thấp hơn và chi phí thời gian chạy hơn so với LoRA. Khi các mô hình lớn (ví dụ, 65B) được huấn luyện với chế độ DeepSpeed ZeRO2, chúng có thể vừa với GPU với QLoRA, trong khi gây ra vấn đề OOM với LoRA. Trong các thí nghiệm, chúng tôi thấy rằng các siêu tham số của LoRA có tác động lớn đến hiệu suất: (i) Tốc độ học lớn và giá trị alpha của LoRA cải thiện kết quả đáng kể. Ví dụ, Với cùng rank=64, chúng tôi giảm tốc độ học= 2×10−5 và alpha=16, hiệu suất giảm từ 71.8 xuống 65.5 trên LLaVA-Bench. (ii) Trong cùng cài đặt, rank lớn dẫn đến cải thiện ít. ví dụ, chúng tôi tăng rank từ 64 lên 128 và 512, nó cải thiện từ 65.5 lên 66.1 và 68.1, tương ứng.

3⃝ Một LMM có khả năng mạnh mẽ trong cả ngôn ngữ và đa phương tiện? Chúng tôi mở rộng đánh giá của chúng tôi trong hai khía cạnh: (i) MM-VET được thêm vào để đo lường khả năng đa phương tiện tích hợp của LMM; (ii) Khả năng ngôn ngữ thuần túy của LMM được đo lường sử dụng Vicuna-80 [16] và MMLU [6], trong đó cái trước đánh giá khả năng tuân theo hướng dẫn trong các nhiệm vụ ngôn ngữ thực tế, cái sau đánh giá khả năng ngôn ngữ đa tác vụ đa ngôn ngữ. Kết quả được hiển thị trong Bảng 5, trong đó tất cả các mô hình được tinh chỉnh toàn bộ mô hình.

So với Vicuna khởi tạo trọng số LLM của LLaVA, thật đáng ngạc nhiên khi quan sát thấy rằng LLaVA, sau khi được huấn luyện chỉ trên dữ liệu hướng dẫn đa phương tiện, thể hiện khả năng ngôn ngữ tương đương. Trộn dữ liệu hướng dẫn ngôn ngữ có thể tăng khả năng đa phương tiện của LLaVA, nhưng không phải khả năng ngôn ngữ. Điều này một phần được quy cho việc bao gồm các câu hỏi suy luận phức tạp và câu trả lời dài trong LLaVA-Instruct-158K, điều này giúp duy trì khả năng ngôn ngữ của LLaVA.

--- TRANG 5 ---
Kích thước Hình ảnh Trộn Dữ liệu 7B 13B 33B 65B
224×224 ✗ 63.6 67.1 69.3 70.3
336×336 ✗ 65.9 70.1 72.0 72.3
336×336 ✓ – – 73.9 74.2
(a) Điểm hiệu suất trên LLaVA-Bench.

Checkpoint Kích thước Hình ảnh Trộn Dữ liệu Tổng thể LR AR RR FP-S FP-C C P
LLaVA-7B 224 ×224 ✗ 36.2 15.9 53.6 28.6 41.8 20.0 40.4
LLaVA-33B 336 ×336 ✓ 55.7 23.3 74.0 46.0 51.5 50.4 67.2
LLaVA-65B 336 ×336 ✓ 56.0 24.4 72.3 49.3 50.5 51.2 68.1

(b) Điểm hiệu suất trên MM-Bench. Các kỹ năng để đánh giá bao gồm suy luận logic (LR), suy luận thuộc tính (AR), suy luận quan hệ (RR), nhận thức thể hiện đơn chi tiết (FP-S), nhận thức thể hiện chéo chi tiết (FP-C), và nhận thức thô (CP).

Bảng 3: Hiệu suất mở rộng quy mô kích thước mô hình, độ phân giải hình ảnh và trộn dữ liệu.

7B 13B 33B 65B
Rank LoRA Toàn bộ 64 Toàn bộ 8 64-QLoRA 64 Toàn bộ 64 Toàn bộ
Hiệu suất ↑ 65.9 70.1 70.1 70.3 71.6 71.8 72.0 72.2 72.3
Thời gian (Giờ GPU trên mỗi node) ↓ 1.3 2.1 2.3 4.62 4.68 4.79 5.80 9.17 13.50
# Tham số Có thể Huấn luyện (B) ↓ 7 0.26 13 0.06 0.49 0.49 33 0.81 65

Bảng 4: Sự đánh đổi giữa hiệu suất và chi phí tính toán giữa các kích thước mô hình khác nhau và các phương pháp huấn luyện trên dữ liệu LLaVA-80K. "Toàn bộ" cho biết tinh chỉnh toàn bộ mô hình. "Thời gian" được báo cáo là tổng giờ GPU để hoàn thành huấn luyện 1 epoch (thời gian chạy ×#GPU) chia cho 8 (#GPU trên mỗi node). Tất cả các mô hình được huấn luyện trên dữ liệu LLaVA-80K, kết quả được thu được thông qua việc lấy trung bình 3 lần chạy đánh giá lặp lại với cùng thiết lập trên LLaVA-Bench.

Các biến thể LoRA khác nhau có hiệu suất tương tự, và QLoRA yêu cầu chi phí bộ nhớ GPU thấp hơn và chi phí thời gian chạy hơn so với LoRA. Khi các mô hình lớn (ví dụ, 65B) được huấn luyện với chế độ DeepSpeed ZeRO2, chúng có thể vừa với GPU với QLoRA, trong khi gây ra vấn đề OOM với LoRA. Trong các thí nghiệm, chúng tôi thấy rằng các siêu tham số của LoRA có tác động lớn đến hiệu suất: (i) Tốc độ học lớn và giá trị alpha của LoRA cải thiện kết quả đáng kể. Ví dụ, Với cùng rank=64, chúng tôi giảm tốc độ học= 2×10−5 và alpha=16, hiệu suất giảm từ 71.8 xuống 65.5 trên LLaVA-Bench. (ii) Trong cùng cài đặt, rank lớn dẫn đến cải thiện ít. ví dụ, chúng tôi tăng rank từ 64 lên 128 và 512, nó cải thiện từ 65.5 lên 66.1 và 68.1, tương ứng.

--- TRANG 6 ---
Mô hình Trộn Dữ liệu Đa phương tiện Ngôn ngữ
LLaVA-Bench MM-VET Vicuna-80 MMLU
Vicuna-13B - - - 79.9 55.8
LLaVA-13B ✗ 70.1 32.5 79.6 55.0
Vicuna-33B - - - 85.6 59.0
LLaVA-33B ✗ 72.0 32.9 85.3 56.1
LLaVA-33B ✓ 73.9 34.1 80.3 58.6
Vicuna-65B - - - 83.2 62.5
LLaVA-65B ✗ 72.3 35.5 84.5 62.6
LLaVA-65B ✓ 74.2 36.4 82.6 62.2
LLaMA-2-70B-Chat - - - 84.7 63.1
LLaVA-70B ✓ 69.8 35.4 81.3 65.1

Bảng 5: Hiệu suất về cả khả năng đa phương tiện và ngôn ngữ.

Chúng tôi cũng huấn luyện LLaVA-70B dựa trên checkpoint LLaMA-2-70B-Chat [15], và thấy rằng kết quả hỗn hợp về khả năng đa phương tiện và ngôn ngữ. Thú vị là, chúng tôi cải thiện LLaMA-2-70B-Chat 2.4 điểm trên MMLU, mang lại điểm MMLU tổng thể là 65.1, đây là hiệu suất tốt nhất cho kích thước mô hình 70B, theo [17] và Bảng xếp hạng Chatbot Arena8. Theo hiểu biết tốt nhất của chúng tôi, đây là kết quả được báo cáo đầu tiên cho thấy điều chỉnh hướng dẫn thị giác cải thiện khả năng ngôn ngữ của LMM quy mô lớn.

4 Kết luận và Hạn chế
Chúng tôi trình bày một nghiên cứu thực nghiệm về việc mở rộng quy mô kích thước mô hình ngôn ngữ cho LMM. Các phát hiện chính của chúng tôi là:
(i) Mở rộng quy mô LMM liên tục cải thiện hiệu suất mô hình, dẫn đến cải thiện đáng kể trong khả năng ngôn ngữ, chủ yếu do kích thước mô hình LLM tăng lên. Chúng tôi để lại cho công việc tương lai về cách mở rộng quy mô bộ mã hóa thị giác để tăng cường khả năng thị giác và cải thiện hiệu suất mô hình trên các nhiệm vụ nhận diện và hiểu thị giác. (ii) Các phương pháp hiệu quả tham số như LoRA/QLoRA là các giải pháp khả thi để tinh chỉnh các LLM quy mô lớn để có sự đánh đổi hiệu suất-chi phí tốt trong một số cài đặt thực tế với bộ nhớ GPU hạn chế. Chúng tôi quan sát thấy rằng hiệu suất của LoRA/QLoRA có thể so sánh với hiệu suất của việc tinh chỉnh toàn bộ mô hình, thiết lập hiệu quả của chúng thông qua việc giảm chi phí đáng kể trong cả huấn luyện và phục vụ mô hình. (iii) Nghiên cứu của chúng tôi về tuyển chọn dữ liệu huấn luyện cho thấy rằng việc lựa chọn độ phân giải hình ảnh phù hợp và trộn dữ liệu đa phương tiện-ngôn ngữ cho huấn luyện mô hình có thể cải thiện đáng kể hiệu suất của LMM kết quả. Chúng tôi cũng cho thấy lần đầu tiên rằng điều chỉnh hướng dẫn thị giác có thể cải thiện khả năng ngôn ngữ của LMM. Lưu ý rằng các bộ dữ liệu huấn luyện được sử dụng trong nghiên cứu này là nhỏ. Vì vậy, các phát hiện của chúng tôi vẫn còn sơ bộ. Trong công việc tương lai, chúng tôi sẽ thử nghiệm sử dụng các bộ dữ liệu lớn hơn nhiều để điều tra chi tiết xem liệu và làm thế nào các phương pháp khác nhau của việc lựa chọn và trộn dữ liệu huấn luyện có thể cải thiện chất lượng của LMM lớn hơn nhiều.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, và cộng sự. Flamingo: một mô hình ngôn ngữ thị giác cho học tập few-shot. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 3

[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, và cộng sự. Openﬂamingo: Một framework mã nguồn mở để huấn luyện các mô hình ngôn ngữ thị giác autoregressive lớn. arXiv preprint arXiv:2308.01390, 2023. 3

--- TRANG 7 ---
[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, và Steven Hoi. Instructblip: Hướng tới các mô hình ngôn ngữ thị giác đa mục đích với điều chỉnh hướng dẫn, 2023. 3

[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Tinh chỉnh hiệu quả các llm được lượng tử hóa. arXiv preprint arXiv:2305.14314, 2023. 2

[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, và cộng sự. Llama-adapter v2: Mô hình hướng dẫn thị giác hiệu quả tham số. arXiv preprint arXiv:2304.15010, 2023. 3

[6] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Đo lường hiểu biết ngôn ngữ đa tác vụ lớn. arXiv preprint arXiv:2009.03300, 2020. 5

[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Thích ứng thấp hạng của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2106.09685, 2021. 2

[8] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, và Ziwei Liu. Otter: Một mô hình đa phương tiện với điều chỉnh hướng dẫn trong ngữ cảnh. arXiv preprint arXiv:2305.03726, 2023. 3

[9] Chunyuan Li. Các mô hình đa phương tiện lớn: Ghi chú về hướng dẫn CVPR 2023. arXiv preprint arXiv:2306.14895, 2023. 1

[10] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, và Jianfeng Gao. Các mô hình nền tảng đa phương tiện: Từ chuyên gia đến trợ lý đa mục đích. arXiv preprint, 2023. 1

[11] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping tiền huấn luyện ngôn ngữ-hình ảnh với các bộ mã hóa hình ảnh đông lạnh và các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2301.12597, 2023. 3

[12] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Điều chỉnh hướng dẫn thị giác, 2023. 1,2,3

[13] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, và cộng sự. Mmbench: Mô hình đa phương tiện của bạn có phải là người chơi toàn diện không? arXiv preprint arXiv:2307.06281, 2023. 4

[14] OpenAI. Báo cáo kỹ thuật gpt-4, 2023. 1

[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, và cộng sự. Llama 2: Các mô hình nền tảng mở và được tinh chỉnh để trò chuyện. arXiv preprint arXiv:2307.09288, 2023. 2,6

[16] Vicuna. Vicuna: Một chatbot mã nguồn mở gây ấn tượng gpt-4 với chất lượng 90%* chatgpt. https://vicuna.lmsys.org/, 2023. 2,5

[17] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, và cộng sự. Lạc đà có thể đi xa đến đâu? khám phá trạng thái của việc điều chỉnh hướng dẫn trên các tài nguyên mở. arXiv preprint arXiv:2306.04751, 2023. 6

[18] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, và Lijuan Wang. Mm-react: Prompting chatgpt cho suy luận và hành động đa phương tiện, 2023. 3

[19] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, và Lijuan Wang. Mm-vet: Đánh giá các mô hình đa phương tiện lớn cho khả năng tích hợp. arXiv preprint arXiv:2308.02490, 2023. 1,3,4

[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Nâng cao hiểu biết ngôn ngữ thị giác với các mô hình ngôn ngữ lớn tiên tiến. arXiv preprint arXiv:2304.10592, 2023. 1,3

--- TRANG 8 ---
Hình này "lora_loss.png" có sẵn ở định dạng "png" từ:
http://arxiv.org/ps/2309.09958v1
