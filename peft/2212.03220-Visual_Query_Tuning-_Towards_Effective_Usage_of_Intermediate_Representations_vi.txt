# Điều chỉnh Truy vấn Trực quan: Hướng tới Sử dụng Hiệu quả Biểu diễn Trung gian 
cho Học chuyển giao Hiệu quả về Tham số và Bộ nhớ

Cheng-Hao Tu* Zheda Mai* Wei-Lun Chao
Đại học Bang Ohio, ftu.343, mai.145, chao.209 g@osu.edu

Tóm tắt
Các đặc trưng trung gian của một mô hình đã được tiền huấn luyện đã được chứng minh là có thông tin cho việc đưa ra dự đoán chính xác trên các tác vụ hạ nguồn, ngay cả khi backbone của mô hình được giữ đóng băng. Thách thức chính là làm thế nào để sử dụng các đặc trưng trung gian này với số lượng khổng lồ của chúng. Chúng tôi đề xuất điều chỉnh truy vấn trực quan (VQT), một phương pháp đơn giản nhưng hiệu quả để tổng hợp các đặc trưng trung gian của Vision Transformers. Thông qua việc giới thiệu một số lượng nhỏ các token "truy vấn" có thể học được cho mỗi lớp, VQT tận dụng cơ chế hoạt động bên trong của Transformers để "tóm tắt" các đặc trưng trung gian phong phú của mỗi lớp, sau đó có thể được sử dụng để huấn luyện các đầu dự đoán của các tác vụ hạ nguồn. Vì VQT giữ nguyên các đặc trưng trung gian và chỉ học cách kết hợp chúng, nó có hiệu quả bộ nhớ trong huấn luyện, so với nhiều phương pháp tinh chỉnh hiệu quả tham số khác học cách thích ứng đặc trưng và cần lan truyền ngược qua toàn bộ backbone. Điều này cũng gợi ý về vai trò bổ sung giữa VQT và những phương pháp đó trong học chuyển giao. Về mặt thực nghiệm, VQT liên tục vượt trội so với phương pháp tiên tiến hiện tại sử dụng các đặc trưng trung gian cho học chuyển giao và vượt trội hơn tinh chỉnh đầy đủ trong nhiều trường hợp. So với các phương pháp hiệu quả tham số thích ứng đặc trưng, VQT đạt được độ chính xác cao hơn nhiều dưới các ràng buộc về bộ nhớ. Quan trọng nhất, VQT tương thích với những phương pháp này để đạt được độ chính xác cao hơn nữa, khiến nó trở thành một tiện ích bổ sung đơn giản để tăng cường thêm học chuyển giao. Mã nguồn có sẵn tại https://github.com/andytu28/VQT .

1. Giới thiệu
Học chuyển giao bằng cách thích ứng các mô hình lớn đã được tiền huấn luyện cho các tác vụ hạ nguồn đã trở thành một tiêu chuẩn trên thực tế để có hiệu suất cạnh tranh, đặc biệt là khi các tác vụ hạ nguồn có dữ liệu hạn chế [37, 59]. Nói chung, có hai cách để thích ứng một mô hình đã được tiền huấn luyện [15, 27]: cập nhật backbone của mô hình để có những nhúng đặc trưng mới (đầu ra của lớp gần cuối) hoặc kết hợp lại các nhúng đặc trưng hiện có, tương ứng với hai phương pháp phổ biến là tinh chỉnh và thăm dò tuyến tính. Tinh chỉnh, hay cụ thể hơn là tinh chỉnh đầy đủ, cập nhật toàn bộ các tham số của mô hình theo cách end-to-end dựa trên tập dữ liệu mới. Mặc dù tinh chỉnh liên tục vượt trội hơn thăm dò tuyến tính trong các tác vụ khác nhau [54], nó yêu cầu chạy gradient descent cho tất cả các tham số và lưu trữ một mô hình được tinh chỉnh riêng biệt cho mỗi tác vụ, khiến nó tốn kém về mặt tính toán và không hiệu quả về tham số. Những vấn đề này trở nên nổi bật hơn với các mô hình dựa trên Transformer có số lượng tham số tăng theo cấp số nhân [17, 26, 46]. Thay vào đó, thăm dò tuyến tính chỉ huấn luyện và lưu trữ các đầu dự đoán mới để kết hợp lại các đặc trưng trong khi giữ backbone đóng băng. Mặc dù có hiệu quả về mặt tính toán và tham số, thăm dò tuyến tính thường kém hấp dẫn hơn do hiệu suất kém hơn.

Một số công trình gần đây đã cố gắng vượt qua tình trạng tiến thoái lưỡng nan này trong học chuyển giao. Một công trình đại diện là của Evci và cộng sự [15], những người cho rằng thành công của tinh chỉnh là do tận dụng các đặc trưng "trung gian" của các mô hình đã được tiền huấn luyện và đề xuất cho phép thăm dò tuyến tính trực tiếp truy cập vào các đặc trưng trung gian. Một số công trình khác cũng đã chứng minh tính hiệu quả của phương pháp như vậy [14,15]. Tuy nhiên, với nhiều đặc trưng trung gian trong mỗi lớp, hầu hết các phương pháp này yêu cầu pooling để giảm chiều, có thể loại bỏ thông tin hữu ích trước khi đầu dự đoán có thể truy cập vào nó.

Để sử dụng tốt hơn các đặc trưng trung gian, chúng tôi đề xuất Điều chỉnh Truy vấn Trực quan (VQT), một phương pháp đơn giản nhưng hiệu quả để tổng hợp các đặc trưng trung gian của các mô hình dựa trên Transformer như Vision Transformers (ViT) [13]. Một Transformer thường chứa nhiều lớp Transformer, mỗi lớp bắt đầu với một mô-đun Multi-head self-attention (MSA) hoạt động trên các token đặc trưng trung gian (thường >100 token) được xuất ra bởi lớp trước. Mô-đun MSA biến đổi mỗi token đặc trưng bằng cách truy vấn tất cả các token khác, theo sau bởi sự kết hợp có trọng số của các đặc trưng của chúng.

Có tính đến những cơ chế hoạt động bên trong như vậy, VQT giới thiệu một số lượng nhỏ các token "truy vấn" có thể học được cho mỗi lớp, sau đó thông qua mô-đun MSA có thể "tóm tắt" các đặc trưng trung gian của lớp trước để giảm chiều. Các đặc trưng đầu ra của những token truy vấn này sau mỗi lớp có thể được sử dụng bởi thăm dò tuyến tính để đưa ra dự đoán. So với pooling chỉ đơn giản lấy trung bình các đặc trưng trên các token, VQT thực hiện một sự kết hợp có trọng số mà trọng số có thể thích ứng, phụ thuộc vào các đặc trưng và các token truy vấn đã học, và có khả năng cao hơn để nắm bắt thông tin hữu ích cho tác vụ hạ nguồn.

Thoạt nhìn, VQT có thể trông tương tự bề ngoài với Visual Prompt Tuning (VPT) [23], một phương pháp học chuyển giao gần đây cũng giới thiệu các token có thể học được bổ sung (tức là prompts) cho mỗi lớp của Transformers, nhưng chúng khác nhau về cơ bản trong hai khía cạnh. Thứ nhất, VQT của chúng tôi chỉ sử dụng các token bổ sung để tạo ra các truy vấn, không phải keys và values, cho mô-đun MSA. Do đó, nó không thay đổi các đặc trưng trung gian của một Transformer. Ngược lại, các token bổ sung trong VPT tạo ra queries, keys, và values, và do đó có thể được truy vấn bởi các token khác và thay đổi các đặc trưng trung gian của chúng. Thứ hai, và quan trọng hơn, trong khi VQT của chúng tôi tận dụng các đầu ra tương ứng của các token bổ sung như các đặc trưng trung gian được tóm tắt, VPT trong phiên bản Deep của nó hoàn toàn bỏ qua các đặc trưng đầu ra như vậy. Nói cách khác, hai phương pháp này đi theo những con đường khác nhau về cơ bản để tiếp cận học chuyển giao: VQT học cách tận dụng các đặc trưng trung gian hiện có, trong khi VPT nhằm thích ứng các đặc trưng trung gian. Như sẽ được chứng minh trong phần 4, hai con đường này có những thế mạnh bổ sung và có thể tương thích để tiếp tục khai thác sức mạnh của học chuyển giao. Đáng chú ý rằng hầu hết các phương pháp gần đây hướng tới học chuyển giao hiệu quả tham số (PETL), chẳng hạn như Prefix Tuning [30] và AdaptFormer [10], đều có thể được coi là thích ứng các đặc trưng trung gian [19]. Do đó, những thế mạnh bổ sung đã đề cập ở trên vẫn áp dụng.

Bên cạnh sự khác biệt về cách tiếp cận học chuyển giao, một sự khác biệt khác giữa VQT và nhiều phương pháp PETL khác, bao gồm cả VPT, là việc sử dụng bộ nhớ trong huấn luyện. Trong khi nhiều phương pháp trong số đó đóng băng (phần lớn) mô hình backbone và chỉ học cách điều chỉnh hoặc thêm một số tham số, thực tế là các đặc trưng trung gian được cập nhật ngụ ý việc cần thiết của một lan truyền ngược đầy đủ qua toàn bộ backbone, điều này tốn nhiều bộ nhớ. Ngược lại, VQT giữ nguyên tất cả các đặc trưng trung gian và chỉ học cách kết hợp chúng. Việc học các token truy vấn do đó bỏ qua nhiều đường dẫn trong lan truyền ngược tiêu chuẩn, giảm dấu chân bộ nhớ 76% so với VPT.

Chúng tôi xác thực VQT trên các tác vụ nhận dạng thị giác hạ nguồn khác nhau, sử dụng một ViT đã được tiền huấn luyện [13] làm backbone. VQT vượt trội so với phương pháp SOTA sử dụng các đặc trưng trung gian [15] và tinh chỉnh đầy đủ trong hầu hết các tác vụ. Chúng tôi tiếp tục chứng minh tính tương thích mạnh mẽ và có lợi ích qua lại giữa VQT và các phương pháp PETL hiện có sử dụng các backbone đã được tiền huấn luyện khác nhau, bao gồm tiền huấn luyện tự giám sát và tiền huấn luyện ngôn ngữ-hình ảnh. Cuối cùng, VQT đạt được độ chính xác cao hơn nhiều so với các phương pháp PETL khác trong chế độ bộ nhớ thấp, cho thấy rằng nó là một phương pháp hiệu quả bộ nhớ hơn.

Tóm lại, những đóng góp chính của chúng tôi là:
1. Chúng tôi đề xuất VQT để tổng hợp các đặc trưng trung gian của Transformers cho thăm dò tuyến tính hiệu quả, có đặc trưng là học chuyển giao hiệu quả tham số và bộ nhớ.
2. VQT tương thích với các phương pháp PETL khác thích ứng các đặc trưng trung gian, tăng cường thêm hiệu suất.
3. VQT mạnh mẽ với các thiết lập tiền huấn luyện khác nhau, bao gồm tiền huấn luyện tự giám sát và tiền huấn luyện ngôn ngữ-hình ảnh.

2. Công trình Liên quan
Transformer. Thành công lẫy lừng của các mô hình Transformer [46] trong xử lý ngôn ngữ tự nhiên (NLP) [48] đã khơi dậy sự quan tâm ngày càng tăng trong việc áp dụng những mô hình này trong các lĩnh vực thị giác và đa phương thức [26]. Kể từ khi Vision Transformer (ViT) [13] được đề xuất, các phương pháp dựa trên Transformer đã chứng minh những tiến bộ ấn tượng trong các tác vụ thị giác khác nhau, bao gồm phân loại hình ảnh [35, 44, 51], phân đoạn hình ảnh [40, 49], phát hiện đối tượng [7, 58], hiểu video [2, 36], xử lý điểm mây [16, 56], và một số trường hợp sử dụng khác [9, 50]. Vì các mô hình Transformer giả định kiến thức tiên nghiệm tối thiểu về cấu trúc của vấn đề, chúng thường được tiền huấn luyện trên các tập dữ liệu quy mô lớn [8, 11, 39]. Cho rằng các mô hình Transformer lớn hơn đáng kể so với các đối tác mạng nơ-ron tích chập của chúng, ví dụ ViT-G (1843M tham số) [53] so với ResNet-152 (58M tham số) [21], cách thích ứng các Transformers đã được tiền huấn luyện cho các tác vụ hạ nguồn một cách hiệu quả về tham số và bộ nhớ vẫn là một vấn đề mở quan trọng.

PETL. Những năm qua đã chứng kiến thành công lớn của học chuyển giao hiệu quả tham số (PETL) trong NLP, nhằm thích ứng các mô hình ngôn ngữ lớn đã được tiền huấn luyện (PLMs) cho các tác vụ hạ nguồn [6, 25]. Thông thường, các phương pháp PETL chèn các mô-đun có thể học nhỏ vào PLMs và tinh chỉnh những mô-đun này với các tác vụ hạ nguồn trong khi đóng băng các trọng số đã được tiền huấn luyện của PLMs [3,5,19,22,29,33,38,41,43,47,57]. Sự thống trị hiện tại của các mô hình Transformer trong lĩnh vực thị giác đã thúc đẩy sự phát triển của các phương pháp PETL trong ViT [10, 23, 24, 31, 34, 55]. Gần đây, Visual Prompt Tuning (VPT) [23] đã được đề xuất để thêm các prompts có thể học được vào các nhúng đầu vào của mỗi lớp Transformer. AdaptFormer [10] chèn các lớp kết nối đầy đủ có cấu trúc nút thắt song song với khối MLP trong một lớp Transformer. Convpass [24] chèn một mô-đun nút thắt tích chập trong khi NOAH [55] thực hiện tìm kiếm kiến trúc nơ-ron trên các phương pháp PETL hiện có. Không giống như tất cả các phương pháp đã đề cập ở trên cập nhật các đặc trưng đầu ra của mỗi lớp Transformer, VQT của chúng tôi tập trung vào việc tận dụng các đặc trưng trung gian bị đóng băng. Do đó, VQT tương thích với hầu hết các phương pháp PETL hiện có và có hiệu quả bộ nhớ.

Học chuyển giao với các đặc trưng trung gian. Các đặc trưng trung gian của một mô hình đã được tiền huấn luyện chứa thông tin phong phú và có giá trị, có thể được tận dụng trong các tác vụ khác nhau như phát hiện đối tượng [4, 18, 32] và phát hiện OOD [28], v.v. Gần đây, nhiều công trình [12,14,15,42] đã chứng minh tính hiệu quả của những đặc trưng này trong học chuyển giao. Ở phía NLP, LST [42] huấn luyện một mạng Transformer nhẹ nhận các đặc trưng trung gian làm đầu vào và tạo ra các đặc trưng đầu ra cho dự đoán. Ở phía CV, Evci và cộng sự [15] cho rằng thành công của tinh chỉnh là do khả năng tận dụng các đặc trưng trung gian và đề xuất Head2Toe để chọn các đặc trưng từ tất cả các lớp cho học chuyển giao hiệu quả. Eom và cộng sự [14] đề xuất sử dụng các đặc trưng trung gian để hỗ trợ học chuyển giao cho phân loại đa nhãn. Tuy nhiên, do số lượng lớn các đặc trưng trung gian, hầu hết các phương pháp dựa vào thao tác pooling để giảm chiều, có thể làm méo mó hoặc loại bỏ thông tin hữu ích. Quan sát này thúc đẩy chúng tôi giới thiệu VQT, học cách tóm tắt các đặc trưng trung gian theo tác vụ hạ nguồn.

3. Phương pháp
Chúng tôi đề xuất Visual Query Tuning (VQT) để thích ứng các Transformers đã được tiền huấn luyện cho các tác vụ hạ nguồn trong khi giữ backbone đóng băng. VQT giữ nguyên tất cả các đặc trưng trung gian và chỉ học cách "tóm tắt" chúng cho thăm dò tuyến tính bằng cách giới thiệu các token "truy vấn" có thể học được cho mỗi lớp.

3.1. Kiến thức Cơ bản
3.1.1 Vision Transformer
Vision Transformers (ViT) [13] thích ứng các mô hình dựa trên Transformer [46] từ NLP vào các tác vụ thị giác, bằng cách chia một hình ảnh I thành một chuỗi N các miếng cố định kích thước {I(n)}N n=1 và coi chúng như các token NLP. Mỗi miếng I(n) đầu tiên được nhúng vào một vector D chiều x(n) 0 với mã hóa vị trí. Chuỗi các vector sau đó được thêm vào đầu với một vector "CLS" x(Class) 0 để tạo ra đầu vào Z0 = [x(Class) 0;x(1) 0;...;x(N) 0]∈RD×(1+N) cho ViT. Chúng ta sử dụng chỉ số trên/dưới để chỉ mục token/lớp.

Thông thường, một ViT có M lớp, được ký hiệu là {Lm}M m=1. Cho đầu vào Z0, lớp đầu tiên L1 tạo ra đầu ra Z1=L1(Z0) = [x(Class) 1;x(1) 1;...;x(N) 1]∈RD×(N+1), có cùng kích thước với Z0. Nghĩa là, Z1 có 1 + N token đặc trưng, và mỗi token tương ứng với cùng một cột trong Z0. Quá trình xử lý theo lớp như vậy sau đó tiếp tục tạo ra đầu ra của lớp tiếp theo, Zm=Lm(Zm-1) với m= 2;...;M, lấy đầu ra của lớp trước làm đầu vào. Cuối cùng, vector "CLS" x(Class) M trong ZM được sử dụng làm đặc trưng cho dự đoán. Lấy phân loại làm ví dụ, nhãn dự đoán ŷ=Head(x(Class) M) được tạo ra bởi một bộ phân loại tuyến tính (tức là một lớp kết nối đầy đủ).

Chi tiết của mỗi lớp Transformer. Phương pháp của chúng tôi tận dụng cơ chế hoạt động bên trong của các lớp Transformer. Dưới đây, chúng tôi cung cấp một nền tảng ngắn gọn.

Mỗi lớp Transformer bao gồm một khối Multi-head Self-Attention (MSA), một khối Multi-Layer Perceptron (MLP), và một số thao tác khác bao gồm chuẩn hóa lớp và kết nối dư. Không mất tính tổng quát, hãy xem xét một khối self-attention đơn đầu và bỏ qua những thao tác bổ sung đó.

Cho đầu vào Zm-1 cho Lm, khối self-attention đầu tiên chiếu nó thành ba ma trận, cụ thể là Query Qm, Key Km, và Value Vm,
Qm=WqZm-1;Km=WkZm-1;Vm=WvZm-1.                    (1)

Mỗi ma trận có 1 + N cột, tương ứng với mỗi cột (tức là token) trong Zm-1. Sau đó, đầu ra của Lm, tức là Zm, có thể được tính bằng:
Zm=MLPm◦MSAm(Zm-1);                                 (2)

trong đó MSAm(Zm-1) = VmSoftmax(K⊤m Qm/√D).        (3)

Softmax được tính trên các phần tử của mỗi cột; MLPm được áp dụng cho mỗi cột của MSAm(Zm-1) một cách độc lập.

3.1.2 Học chuyển giao: Thăm dò Tuyến tính, Tinh chỉnh, và Sử dụng Đặc trưng Trung gian

Để thích ứng một ViT đã được tiền huấn luyện cho các tác vụ hạ nguồn, thăm dò tuyến tính đóng băng toàn bộ mô hình backbone nhưng đầu dự đoán: nó bỏ qua Head gốc và học một cái mới. Tinh chỉnh, bên cạnh thăm dò tuyến tính, cho phép mô hình backbone được cập nhật cũng như vậy.

Một số công trình gần đây đã chứng minh tính hiệu quả của việc sử dụng các đặc trưng trung gian trong học chuyển giao, bằng cách cho phép thăm dò tuyến tính truy cập trực tiếp vào chúng [14, 15]. Công trình tiên phong HEAD2TOE [15] lấy các đặc trưng trung gian từ Z0 và bốn bước riêng biệt trong mỗi lớp Transformer: đặc trưng sau chuẩn hóa lớp, sau khối MSA, và bên trong và sau khối MLP. Vì mỗi đặc trưng có 1 + N token, HEAD2TOE nhóm các token theo chỉ số của chúng và thực hiện average pooling để giảm chiều. Các đặc trưng kết quả — trên mỗi nhóm, bước, và lớp — sau đó được nối với nhau để thăm dò tuyến tính. Để giảm thêm chiều, HEAD2TOE sử dụng group lasso [1, 52] để chọn đặc trưng.

Chúng tôi lưu ý rằng trong khi việc giảm chiều thứ hai được thúc đẩy bởi các tác vụ hạ nguồn, việc giảm chiều thứ nhất (tức là pooling) thì không, có thể vô tình loại bỏ thông tin hữu ích. Thiếu sót này thúc đẩy chúng tôi phát triển Visual Query Tuning (VQT) để sử dụng hiệu quả các đặc trưng trung gian.

3.2. Visual Query Tuning (VQT)
Chúng tôi đề xuất thay thế thao tác average pooling trong HEAD2TOE bằng cơ chế "tóm tắt" nội tại trong Transformers. Chúng tôi lưu ý rằng khối MSA được giới thiệu trong Phương trình 3 về cơ bản thực hiện trung bình có trọng số của các đặc trưng Value V trên các token, trong đó các trọng số được xác định bởi các cột của K⊤Q. Nghĩa là, nếu chúng ta có thể thêm các "cột" bổ sung vào K⊤Q, khối MSA sẽ xuất ra các kết hợp có trọng số bổ sung của V. Trong trường hợp đặc biệt mà vector được thêm vào K⊤Q có các phần tử giống hệt nhau (ví dụ, một vector toàn số không), trung bình có trọng số sẽ giảm xuống thành một trung bình đơn giản. Nói cách khác, average pooling có thể được coi như một đầu ra đặc biệt của lớp MSA.

Có tính đến insight này, chúng tôi đề xuất học và thêm các cột bổ sung Q' vào Q. Chúng tôi nhận ra ý tưởng này bằng cách giới thiệu một số lượng nhỏ T các token "truy vấn" có thể học được Pm-1 = [p(1) m-1;...;p(T) m-1] vào đầu vào của mỗi lớp Transformer Lm. Xem Hình 1b để minh họa. Khác với đầu vào gốc Zm-1 trải qua ba phép chiếu được giới thiệu trong Phương trình 1, Pm-1 chỉ trải qua phép chiếu bởi Wq,
Q' m=WqPm-1.                                         (4)

Bằng cách thêm Q' m vào Qm theo cột, chúng tôi sửa đổi phép tính của khối MSA gốc trong Phương trình 3 bằng
VmSoftmax(K⊤m[Qm;Q' m]/√D) =                        (5)
[VmSoftmax(K⊤m Qm/√D);VmSoftmax(K⊤m Q' m/√D)].

Nửa thứ hai (màu xanh) tương ứng với các đặc trưng MSA được tóm tắt mới bởi các token truy vấn có thể học được Pm-1. Sau đó sau khối MLP MLPm, những đặc trưng này dẫn đến các đặc trưng được tóm tắt mới Z' m∈RD×T từ lớp Lm.

Chúng ta có thể nối những đặc trưng được tóm tắt mới này qua các lớp, Z' m∈RD×T với m= 1;...;M, cùng với vector "CLS" cuối cùng x(Class) M, để thăm dò tuyến tính. Chúng tôi đặt tên cho phương pháp của mình là Visual Query Tuning (VQT), phản ánh thực tế rằng các token mới được thêm vào Pm với m= 0;...;M-1 chỉ phục vụ cho các cột bổ sung trong ma trận Query.

Tính chất của VQT. Như được chỉ ra trong Phương trình 5, các token truy vấn mới được giới thiệu không thay đổi các đặc trưng MSA mà ViT đã được tiền huấn luyện có được (tức là nửa đầu). Điều này ngụ ý rằng VQT giữ nguyên tất cả các đặc trưng trung gian gốc (ví dụ, Zm) nhưng chỉ học cách kết hợp lại chúng.

Huấn luyện VQT. Cho dữ liệu huấn luyện của tác vụ hạ nguồn, các token truy vấn {Pm}M-1 m=0 được học end-to-end với đầu dự đoán mới, truy cập trực tiếp vào các đầu ra {Z' m+1}M-1 m=0 của những token truy vấn này.

Để giảm thêm chiều của {Z' m+1}M-1 m=0, chúng tôi tùy chọn sử dụng group lasso, theo HEAD2TOE [15]. Cụ thể, chúng tôi trước tiên học các token truy vấn mà không có group lasso. Sau đó chúng tôi đóng băng chúng và áp dụng group lasso để chọn các đặc trưng hữu ích từ {Z' m+1}M-1 m=0. Chúng tôi cũng đã khám phá nhiều cách khác nhau để giảm chiều trong Phụ lục C.4.

3.3. So sánh với Các Công trình Liên quan
So sánh với HEAD2TOE [15]. Chúng tôi liệt kê một số khác biệt chính giữa HEAD2TOE và VQT của chúng tôi. Thứ nhất, so với HEAD2TOE lấy các đặc trưng trung gian từ nhiều bước trong một lớp Transformer, VQT chỉ lấy các đặc trưng trung gian được tóm tắt mới sau mỗi lớp. Thứ hai, và quan trọng hơn, VQT sử dụng một cách khác để kết hợp các đặc trưng trung gian qua các token. Nói chung, có hai cách để kết hợp một tập hợp các vector đặc trưng {x(n)∈RD}N n=1: nối và average pooling. Cách trước giả định rằng các vector khác nhau có ý nghĩa khác nhau ngay cả ở cùng một chiều, phù hợp cho các đặc trưng qua các lớp. Cách sau giả định rằng cùng một chiều có ý nghĩa tương tự với các vector khác nhau nên chúng có thể được so sánh và lấy trung bình, phù hợp cho các đặc trưng qua các token. Một nhược điểm cụ thể của cách trước là chiều (tức là không hiệu quả). Đối với cách sau, đó là mất mát thông tin hữu ích tiềm năng vì nó kết hợp các đặc trưng mù quáng với các tác vụ hạ nguồn (tức là không hiệu quả). HEAD2TOE lấy một hỗn hợp của hai cách này để kết hợp các đặc trưng trên các token, và có thể gặp một (hoặc cả hai) nhược điểm. Ngược lại, VQT tận dụng cơ chế nội tại của self-attention để tổng hợp các đặc trưng một cách thích ứng, có điều kiện trên các đặc trưng và các token truy vấn có thể học được, khiến nó trở thành một cách hiệu quả và có hiệu lực hơn để giải quyết nhiều đặc trưng trung gian trong mỗi lớp.

So sánh với Visual Prompt Tuning (VPT). Thoạt nhìn, VQT có thể gợi nhớ đến VPT [23], nhưng chúng khác nhau về cơ bản như được nhấn mạnh trong phần 1 và Hình 1. Ở đây, chúng tôi cung cấp thêm một số chi tiết và minh họa.

VPT trong phiên bản deep (VPT-Deep) giới thiệu các token có thể học được Pm-1 = [p(1) m-1;...;p(T) m-1] vào đầu vào của mỗi lớp Transformer Lm, tương tự như VQT. Tuy nhiên, không giống như VQT chỉ sử dụng Pm-1 để truy vấn, VPT-Deep coi Pm-1 giống như các token đầu vào khác Zm-1 và tạo ra các ma trận Query, Key, và Value tương ứng,
Q' m=WqPm-1;K' m=WkPm-1;V' m=WvPm-1.

Những ma trận này sau đó được thêm vào những ma trận gốc từ Zm-1 (cf. Phương trình 1) trước self attention,
Q̃m = [Qm;Q' m];K̃m = [Km;K' m];Ṽm = [Vm;V' m];
khiến đầu ra của khối MSA là
ṼmSoftmax(K̃⊤m Q̃m/√D) =                             (6)
[ṼmSoftmax(K̃⊤m Qm/√D);ṼmSoftmax(K̃⊤m Q' m/√D)].

So với Phương trình 3 và Phương trình 5, nửa đầu của ma trận trong Phương trình 6 thay đổi, ngụ ý rằng tất cả các đặc trưng trung gian cũng như vector "CLS" cuối cùng x(Class) M được cập nhật theo các token có thể học được Pm-1. Ngược lại, VQT giữ nguyên những đặc trưng (trung gian) này.

Có lẽ tinh tế hơn nhưng quan trọng, VPT-Deep kết thúc bằng việc loại bỏ nửa thứ hai của ma trận trong Phương trình 6. Nói cách khác, VPT-Deep không khai thác các đặc trưng được tóm tắt mới bởi Q' m, khiến nó về mặt khái niệm tương tự như Prefix Tuning [30]. Vui lòng xem Hình 1 để so sánh cạnh nhau giữa VQT và VPT-Deep.

Những khác biệt đã đề cập ở trên gợi ý một sự phân biệt thú vị giữa VQT và VPT: VQT học cách tận dụng các đặc trưng trung gian hiện có, trong khi VPT học cách thích ứng các đặc trưng trung gian. Trong tiểu mục 4.3, chúng tôi chứng minh một thế mạnh cụ thể của VQT, đó là chuyển giao các mô hình đã được tiền huấn luyện tự giám sát.

So sánh và Tương thích với các phương pháp PETL. Trên thực tế, hầu hết các phương pháp PETL hiện có điều chỉnh hoặc thêm một tập hợp nhỏ các tham số vào mô hình backbone đều cập nhật các đặc trưng trung gian [19]. Do đó, VQT của chúng tôi có khả năng bổ sung cho chúng và có thể được sử dụng để tăng cường hiệu suất của chúng. Trong tiểu mục 4.3, chúng tôi khám phá ý tưởng này bằng cách giới thiệu các token truy vấn có thể học được cho những phương pháp này.

Hiệu quả bộ nhớ trong huấn luyện. Như được chỉ ra trong [42], khi học các tham số mới được thêm vào, hầu hết các phương pháp PETL yêu cầu lưu trữ các kết quả lan truyền ngược trung gian, điều này không hiệu quả về bộ nhớ đối với các mô hình lớn dựa trên Transformer. Đối với VQT, vì nó giữ nguyên tất cả các đặc trưng trung gian và chỉ học cách (i) điều chỉnh các token truy vấn (ii) và thăm dò tuyến tính các đầu ra tương ứng của chúng, việc huấn luyện bỏ qua nhiều đường dẫn lan truyền ngược tốn kém, giảm đáng kể dấu chân bộ nhớ. Xem tiểu mục 4.4 để biết chi tiết.

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Tập dữ liệu. Chúng tôi đánh giá hiệu suất học chuyển giao trên VTAB-1k [54], bao gồm 19 tác vụ phân loại hình ảnh được phân loại thành ba nhóm: Natural, Specialized, và Structured. Nhóm Natural bao gồm các hình ảnh tự nhiên được chụp bằng máy ảnh tiêu chuẩn. Nhóm Specialized chứa các hình ảnh được chụp bằng thiết bị chuyên dụng cho mục đích viễn thám và y tế. Nhóm Structured đánh giá khả năng hiểu cấu trúc cảnh, chẳng hạn như đếm đối tượng và ước tính độ sâu 3D. Theo [54], chúng tôi thực hiện phân chia 80/20 trên 1000 hình ảnh huấn luyện trong mỗi tác vụ để tìm kiếm siêu tham số. Kết quả được báo cáo (độ chính xác phân loại top-1) được thu được bằng cách huấn luyện trên 1000 hình ảnh huấn luyện và đánh giá trên tập kiểm tra gốc.

Thiết lập tiền huấn luyện. Chúng tôi sử dụng ViT-B/16 [13] làm backbone. Thiết lập tiền huấn luyện tuân theo các baseline so sánh tương ứng. Khi so sánh với Head2Toe, chúng tôi sử dụng backbone được tiền huấn luyện có giám sát ImageNet-1K. Khi điều tra tính tương thích với các phương pháp PETL khác, backbone được tiền huấn luyện có giám sát ImageNet-21K được sử dụng. Để chứng minh tính mạnh mẽ của VQT với các thiết lập tiền huấn luyện khác nhau, chúng tôi cũng đánh giá VQT trên các backbone được tiền huấn luyện tự giám sát (MAE) [20] và ngôn ngữ-hình ảnh (CLIP) [39]. Vui lòng xem Phụ lục B để biết thêm chi tiết.

4.2. Hiệu quả của VQT
Để đánh giá hiệu suất học chuyển giao của VQT, chúng tôi so sánh VQT với các phương pháp cố định toàn bộ backbone (thăm dò tuyến tính và HEAD2TOE) và tinh chỉnh đầy đủ, cập nhật tất cả các tham số mạng end to end. Để so sánh công bằng, chúng tôi khớp số lượng tham số có thể điều chỉnh trong VQT với số lượng trong HEAD2TOE (chi tiết được bao gồm trong Phụ lục B.3). Nhìn chung, VQT cải thiện hơn thăm dò tuyến tính 12,6% và vượt trội hơn HEAD2TOE và tinh chỉnh đầy đủ lần lượt 3% và 2,1%, về hiệu suất trung bình trên 19 tác vụ, điều này chứng minh sức mạnh của việc sử dụng các đặc trưng trung gian và hiệu quả của VQT trong việc tóm tắt chúng. Trong danh mục Natural, VQT vượt trội hơn HEAD2TOE và tinh chỉnh lần lượt 2,8% và 7,5%, và vượt trội hơn chúng trong danh mục Specialized lần lượt 1,6% và 2,8%. Như được chỉ ra trong [15,54], các danh mục Natural và Specialized có sự gần gũi lĩnh vực mạnh hơn với lĩnh vực nguồn (ImageNet) vì chúng đều là những hình ảnh thực được chụp bằng máy ảnh. Do đó, backbone đã được tiền huấn luyện có thể tạo ra các đặc trưng trung gian liên quan hơn cho các lĩnh vực tương tự. Ngoại lệ duy nhất là danh mục Structured bao gồm các hình ảnh nhân tạo được render từ môi trường mô phỏng, khác biệt đáng kể so với ImageNet. Mặc dù VQT tiếp tục cải thiện HEAD2TOE, tinh chỉnh cho thấy tăng cường 2,9% so với VQT, cho thấy rằng nếu chúng ta cần thích ứng với một lĩnh vực mục tiêu khác biệt hơn, chúng ta có thể xem xét điều chỉnh một phần nhỏ của backbone để tạo ra các đặc trưng được cập nhật cho dữ liệu mới trước khi áp dụng kỹ thuật VQT của chúng ta. Phụ lục C.1 chứa thêm so sánh giữa HEAD2TOE và VQT.

4.3. Tính tương thích với các Phương pháp PETL trong Các Phương pháp Tiền huấn luyện Khác nhau
Như đã đề cập trong tiểu mục 3.3, hầu hết các phương pháp PETL hiện có và VQT đi theo những con đường khác nhau về cơ bản để tiếp cận học chuyển giao: các phương pháp PETL tập trung vào việc thích ứng mô hình để tạo ra các đặc trưng được cập nhật, trong khi VQT nhằm tận dụng tốt hơn các đặc trưng. Dựa trên tính bổ sung về khái niệm này, chúng tôi điều tra liệu chúng có thể được kết hợp để khai thác sức mạnh của học chuyển giao. Hơn nữa, để chứng minh tính mạnh mẽ của tính tương thích, chúng tôi đánh giá hiệu suất trên ba backbone đã được tiền huấn luyện khác nhau: được tiền huấn luyện tự giám sát (MAE với ImageNet-1K) [20], được tiền huấn luyện ngôn ngữ-hình ảnh (CLIP) [39] và được tiền huấn luyện có giám sát (ImageNet-21K).

Cụ thể, chúng tôi tập trung vào hai phương pháp được đề xuất gần đây: AdaptFormer [10] và VPT [23]. AdaptFormer chèn các lớp kết nối đầy đủ trong cấu trúc nút thắt song song với khối MLP trong mỗi lớp Transformer [10]; VPT thêm các token có thể học được vào đầu vào của mỗi lớp Transformer.

Để trang bị cho AdaptFormer [10] và VPT [23] với VQT của chúng tôi, trước tiên, chúng tôi cập nhật mô hình đã được tiền huấn luyện với AdaptFormer hoặc VPT để mô hình có thể tạo ra các đặc trưng trung gian liên quan cho tác vụ hạ nguồn. Sau đó chúng tôi thêm T= 1 token truy vấn vào đầu vào của mỗi lớp để tóm tắt các đặc trưng trung gian đã được cập nhật. Đối với AdaptFormer, chúng tôi sử dụng chiều nút thắt mặc định 64; đối với VPT, chúng tôi sử dụng số lượng token được thêm vào tốt nhất cho mỗi tác vụ được báo cáo trong paper của họ.

Chúng tôi tóm tắt kết quả trong Bảng 2, trong đó mỗi hàng hiển thị kết quả cho một backbone đã được tiền huấn luyện, và mỗi cột hiển thị kết quả cho một danh mục dữ liệu. Nói chung, AdaptFormer và VPT được hưởng lợi từ VQT trong hầu hết các tình huống qua các danh mục dữ liệu khác nhau và các backbone đã được tiền huấn luyện. Sự cải thiện nổi bật hơn trong backbone MAE. Vì tiền huấn luyện MAE sử dụng mục tiêu tái tạo thay vì mục tiêu phân loại hoặc contrastive, chúng tôi đưa ra giả thuyết rằng một số đặc trưng trung gian hữu ích cho phân loại có thể không được truyền tới lớp cuối cùng. Với sự giúp đỡ của VQT, AdaptFormer và VPT có thể tận dụng các đặc trưng trung gian một cách ngắn gọn và hiệu quả hơn. Ngoài ra, VQT cũng được hưởng lợi từ AdaptFormer và VPT. Trong tiểu mục 4.2, chúng tôi thấy rằng việc áp dụng trực tiếp VQT cho backbone đã được tiền huấn luyện có thể không hiệu quả cho danh mục Structured do sự gần gũi lĩnh vực thấp. Với các đặc trưng trung gian được cập nhật bởi AdaptFormer và VPT, VQT có thể tóm tắt những đặc trưng liên quan hơn này để cải thiện kết quả cho nhóm Structured.

Tóm lại, kết quả thí nghiệm minh họa rằng VQT và các phương pháp PETL bổ sung và có lợi ích qua lại, với tiềm năng tiếp tục khai thác sức mạnh của chuyển giao. Chúng tôi cung cấp kết quả chi tiết của các backbone đã được tiền huấn luyện khác nhau trong Phụ lục C.6 và so sánh tính tương thích giữa HEAD2TOE và VQT trong Phụ lục C.7.

Để xác nhận rằng sự cải thiện đã đề cập ở trên không chỉ đơn giản đến từ việc tăng số lượng tham số có thể điều chỉnh, chúng tôi mở rộng các mô-đun được thêm vào của AdaptFormer bằng cách tăng chiều nút thắt d̂ từ 64 lên 128 và 256 để khớp với số lượng tham số có thể điều chỉnh của AdaptFormer khi nó được trang bị với VQT. Như được hiển thị trong Hình 2, AdaptFormer với VQT vượt trội đáng kể so với AdaptFormer với các mô-đun được thêm vào lớn hơn khi số lượng tham số có thể điều chỉnh tương tự. Điều này tiếp tục chứng minh thế mạnh bổ sung của VQT và AdaptFormer: sự cải thiện bằng cách tận dụng các đặc trưng trung gian được tóm tắt bởi VQT không thể đạt được bằng cách chỉ đơn giản tăng độ phức tạp của các mô-đun được chèn vào AdaptFormer.

4.4. Huấn luyện Hiệu quả Bộ nhớ
Trong khi nhiều phương pháp PETL giảm số lượng tham số có thể điều chỉnh, chúng không thể cắt giảm nhiều dấu chân bộ nhớ trong quá trình huấn luyện, và do đó, việc đánh giá các phương pháp PETL thường bỏ qua tiêu thụ bộ nhớ. Tuy nhiên, trong các tình huống thực tế, một mô hình thường được yêu cầu thích ứng với dữ liệu mới trên các thiết bị edge vì lý do riêng tư, đòi hỏi nhu cầu về các phương pháp có thể được huấn luyện với bộ nhớ hạn chế. Điều này thúc đẩy chúng tôi phân tích thêm sự đánh đổi giữa độ chính xác và bộ nhớ cho VPT, AdaptFormer, và VQT.

Như đã thảo luận trong tiểu mục 3.3, VPT và AdaptFormer yêu cầu lưu trữ các kết quả lan truyền ngược trung gian để cập nhật các tham số được thêm vào của chúng, trong khi VQT bỏ qua lan truyền ngược tốn kém vì nó giữ nguyên tất cả các đặc trưng trung gian. Để đánh giá hiệu suất của chúng trong chế độ bộ nhớ thấp, chúng tôi chỉ thêm các tham số được chèn vào của chúng vào một vài lớp cuối cùng để khớp với việc sử dụng bộ nhớ. Hình 3a hiển thị hiệu suất của VQT, VPT, và AdaptFormer dưới các siêu tham số tốt nhất của chúng mà không có ràng buộc bộ nhớ; Hình 3b mô tả sự đánh đổi giữa độ chính xác và bộ nhớ cho những phương pháp này. Khi bộ nhớ không phải là một ràng buộc, VPT và AdaptFormer vượt trội hơn VQT một chút, nhưng chúng tiêu thụ nhiều bộ nhớ (GB) hơn VQT lần lượt 3,8x và 5,9x, như chúng ta có thể thấy trong Hình 3b.

Khi bộ nhớ là một ràng buộc (phía trái của Hình 3b), chúng ta thấy sự giảm độ chính xác mạnh mẽ của AdaptFormer và VPT. Mặc dù chúng vẫn vượt trội hơn thăm dò tuyến tính, VQT vượt trội hơn chúng đáng kể, cho thấy rằng VQT là một phương pháp hiệu quả bộ nhớ hơn nhờ vào cơ chế chỉ-truy-vấn của nó.

4.5. Thảo luận
Tầm quan trọng của lớp cho mỗi danh mục. Vì VQT tận dụng các đặc trưng trung gian được tóm tắt để dự đoán, chúng tôi điều tra lớp nào tạo ra các đặc trưng quan trọng hơn cho mỗi danh mục. Trong Hình 4, chúng tôi hiển thị điểm số quan trọng của mỗi lớp được tính bằng cách lấy trung bình tầm quan trọng của đặc trưng trong lớp. Các đặc trưng trong các lớp sâu hơn quan trọng hơn cho danh mục Natural, trong khi các đặc trưng từ tất cả các lớp gần như quan trọng bằng nhau cho danh mục Specialized. Ngược lại, VQT phụ thuộc mạnh vào token CLS cho danh mục Structured. Chúng tôi đưa ra giả thuyết rằng sự gần gũi lĩnh vực thấp giữa ImageNet và danh mục Structured có thể khiến các đặc trưng trung gian ít liên quan hơn, và mô hình cần phụ thuộc nhiều hơn vào token CLS.

Các kích thước dữ liệu hạ nguồn khác nhau. Chúng tôi nghiên cứu thêm hiệu quả của VQT dưới các kích thước dữ liệu huấn luyện khác nhau. Chúng tôi giảm kích thước huấn luyện của VTAB xuống {10%, 20%, 30%, 50%, 70%} và so sánh VQT với Tinh chỉnh và Thăm dò tuyến tính trong Hình 6a. Mặc dù tinh chỉnh vượt trội hơn VQT một chút trên dữ liệu 100%, VQT liên tục hoạt động tốt hơn khi chúng ta tiếp tục giảm dữ liệu huấn luyện. Trong trường hợp dữ liệu 10%, nơi chúng ta chỉ có 2 hình ảnh cho mỗi lớp trung bình, Thăm dò tuyến tính có được độ chính xác tốt nhất, nhưng sự cải thiện của nó giảm dần và hoạt động tệ hơn nhiều so với VQT khi có thêm dữ liệu. Những kết quả này cho thấy rằng VQT thuận lợi hơn trong một phạm vi rộng các kích thước dữ liệu huấn luyện.

Số lượng token truy vấn. Vì chúng tôi chỉ sử dụng một token truy vấn cho VQT trong các thí nghiệm trước, bây giờ chúng tôi nghiên cứu hiệu suất của VQT sử dụng nhiều token truy vấn hơn trên VTAB-1k. Hình 6b cho thấy rằng nhiều token truy vấn hơn có thể cải thiện VQT, nhưng độ chính xác giảm khi chúng ta thêm hơn 40 token. Chúng tôi đưa ra giả thuyết rằng việc tăng quá mức độ phức tạp của mô hình gây ra overfitting do dữ liệu hạn chế trong VTAB-1k.

Trực quan hóa. Hình 5 hiển thị trực quan hóa t-SNE [45] của token CLS và các đặc trưng được tóm tắt của chúng tôi cho ba tác vụ (SVHN, EuroSAT, và Clevr-Dist), một tác vụ từ mỗi danh mục. So với chỉ token CLS, việc thêm các đặc trưng được tóm tắt làm cho toàn bộ đặc trưng có thể tách biệt hơn, cho thấy sức mạnh của việc sử dụng các đặc trưng trung gian và hiệu quả của các token truy vấn của chúng tôi trong việc tóm tắt chúng. Chúng tôi cung cấp trực quan hóa của các tác vụ khác trong Phụ lục C.5.

5. Kết luận
Chúng tôi đã giới thiệu Visual Query Tuning, một phương pháp đơn giản nhưng hiệu quả để tổng hợp các đặc trưng trung gian của Vision Transformers. Bằng cách giới thiệu một tập hợp các token "truy vấn" có thể học được cho mỗi lớp, VQT tận dụng cơ chế nội tại của Transformers để "tóm tắt" các đặc trưng trung gian phong phú trong khi giữ nguyên các đặc trưng trung gian, cho phép nó có một quá trình huấn luyện hiệu quả bộ nhớ mà không cần lan truyền ngược qua toàn bộ backbone. Về mặt thực nghiệm, VQT vượt trội hơn HEAD2TOE, phương pháp SOTA sử dụng các đặc trưng trung gian, và chúng tôi chứng minh tính tương thích mạnh mẽ và có lợi ích qua lại giữa VQT và các phương pháp PETL khác. Hơn nữa, VQT là một phương pháp hiệu quả bộ nhớ hơn và đạt được hiệu suất cao hơn nhiều trong chế độ bộ nhớ thấp. Trong khi VQT chỉ tập trung vào việc tóm tắt các đặc trưng trong mỗi lớp, chúng tôi hy vọng công trình của chúng tôi có thể mở đường cho việc khám phá những cách hiệu quả hơn để sử dụng các đặc trưng qua các lớp và tận dụng các đặc trưng trung gian trong học chuyển giao cho các tác vụ khác, chẳng hạn như phát hiện đối tượng, phân đoạn ngữ nghĩa và phân loại video.

Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi NSF (IIS-2107077, OAC-2118240, và OAC-2112606) và Cisco Research. Chúng tôi biết ơn về các tài nguyên tính toán của Ohio Supercomputer Center. Chúng tôi cảm ơn Yu Su (OSU) vì cuộc thảo luận hữu ích. Chúng tôi cảm ơn Menglin Jia và Luming Tang (Cornell) vì chia sẻ mã nguồn.
