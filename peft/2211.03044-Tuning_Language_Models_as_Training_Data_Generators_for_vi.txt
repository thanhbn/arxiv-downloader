# 2211.03044.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2211.03044.pdf
# Kích thước tệp: 1590775 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Điều chỉnh Mô hình Ngôn ngữ như Bộ sinh dữ liệu huấn luyện cho
Học ít mẫu tăng cường bằng Bổ sung dữ liệu
Yu Meng¹ Martin Michalski¹ Jiaxin Huang¹ Yu Zhang¹ Tarek Abdelzaher¹ Jiawei Han¹

Tóm tắt
Các nghiên cứu gần đây đã tiết lộ khả năng học ít mẫu thú vị của các mô hình ngôn ngữ được huấn luyện trước (PLMs): Chúng có thể nhanh chóng thích ứng với một nhiệm vụ mới khi được tinh chỉnh trên một lượng nhỏ dữ liệu có nhãn được định dạng dưới dạng prompts, mà không cần các chú thích cụ thể cho nhiệm vụ phong phú. Mặc dù có hiệu suất hứa hẹn, hầu hết các phương pháp học ít mẫu hiện có chỉ học từ tập huấn luyện nhỏ vẫn kém hơn huấn luyện có giám sát đầy đủ một khoảng cách không nhỏ. Trong công trình này, chúng tôi nghiên cứu học ít mẫu với PLMs từ một góc độ khác: Chúng tôi đầu tiên điều chỉnh một PLM tự hồi quy trên các mẫu ít-shot và sau đó sử dụng nó như một bộ sinh để tổng hợp một lượng lớn các mẫu huấn luyện mới để bổ sung tập huấn luyện gốc. Để khuyến khích bộ sinh tạo ra các mẫu phân biệt nhãn, chúng tôi huấn luyện nó thông qua likelihood cực đại có trọng số trong đó trọng số của mỗi token được tự động điều chỉnh dựa trên một mục tiêu meta-learning phân biệt. Một PLM phân loại sau đó có thể được tinh chỉnh trên cả các mẫu ít-shot và tổng hợp với regularization để có khả năng tổng quát hóa và ổn định tốt hơn. Phương pháp FewGen của chúng tôi đạt được kết quả tốt hơn tổng thể trên bảy nhiệm vụ phân loại của benchmark GLUE so với các phương pháp học ít mẫu hiện có, cải thiện các phương pháp không bổ sung 5+ điểm trung bình, và vượt trội hơn các phương pháp bổ sung 3+ điểm trung bình.

1. Giới thiệu
Nghiên cứu gần đây đã chứng minh tiềm năng học ít mẫu hấp dẫn của các mô hình ngôn ngữ được huấn luyện trước (PLMs) (Brown et al., 2020; Clark et al., 2020; Devlin et al., 2019; He et al., 2021; Liu et al., 2019; Meng et al., 2021a; 2022b) trên các nhiệm vụ hiểu ngôn ngữ tự nhiên (NLU) (Wang et al., 2019; 2018): Thay vì dựa vào các chú thích cụ thể cho nhiệm vụ phong phú, PLMs có thể hiệu quả tận dụng một tập nhỏ các mẫu huấn luyện để nhanh chóng học một nhiệm vụ mới. Hiệu quả dữ liệu huấn luyện như vậy thường được đạt được bằng cách định dạng các nhiệm vụ downstream dưới dạng prompts (Brown et al., 2020; Gao et al., 2021; Scao & Rush, 2021; Schick & Schütze, 2021a;d), cho phép PLM thích ứng khả năng mô hình hóa ngôn ngữ thu được thông qua pretraining với các nhiệm vụ downstream.

Thành công của các phương pháp dựa trên prompt đã kích thích nhiều khám phá theo hướng học ít mẫu hiệu quả với PLMs: Các mẫu huấn luyện được chuyển đổi thành prompts ngôn ngữ tự nhiên có thể được sử dụng để trực tiếp tinh chỉnh PLMs (Gao et al., 2021; Schick & Schütze, 2021a) hoặc làm các minh chứng trong ngữ cảnh để tạo điều kiện suy luận tốt hơn (Liu et al., 2022b; Min et al., 2022b). Các phương pháp gần đây nhằm tự động hóa việc thiết kế prompts bằng tìm kiếm dựa trên gradient (Shin et al., 2020) hoặc tham số hóa prompts dưới dạng embeddings liên tục có thể học (Lester et al., 2021; Zhang et al., 2022; Zhong et al., 2021). Các nghiên cứu khác điều tra và giải quyết các vấn đề cụ thể trong học ít mẫu dựa trên prompt (Liu et al., 2022a; Tam et al., 2021; Zhao et al., 2021).

Mặc dù đáng chú ý, hiệu suất mô hình vẫn có khoảng cách không nhỏ so với các mô hình có giám sát đầy đủ được huấn luyện trên dữ liệu có nhãn khổng lồ. Thật vậy, huấn luyện các mô hình sâu về bản chất đòi hỏi dữ liệu—khả năng tổng quát hóa của mô hình thường hưởng lợi từ nhiều mẫu huấn luyện hơn (Baum & Haussler, 1988).

Trong công trình này, chúng tôi nghiên cứu học ít mẫu với PLMs từ một góc độ khác: Thay vì đề xuất các phương pháp mới để tinh chỉnh trên các mẫu ít-shot, chúng tôi tập trung vào việc sinh ra dữ liệu huấn luyện chất lượng dựa trên các mẫu ít-shot và sử dụng các mẫu huấn luyện tổng hợp này để tinh chỉnh các mô hình phân loại. Được thúc đẩy bởi sức mạnh sinh văn bản mạnh mẽ của các PLMs tự hồi quy (Brown et al., 2020; Keskar et al., 2019; Raffel et al., 2019), một số nghiên cứu trước đây mở rộng tập huấn luyện bằng cách sinh ra các văn bản mới làm mẫu huấn luyện. Họ hoặc tinh chỉnh bộ sinh trên tập huấn luyện ban đầu với mục tiêu likelihood cực đại tiêu chuẩn (Anaby-Tavor et al., 2020; Kumar et al., 2020) hoặc sử dụng các mẫu huấn luyện làm minh chứng (Yoo et al., 2021). Tuy nhiên, các phương pháp này không mô hình hóa rõ ràng sự khác biệt giữa các nhãn khác nhau và có thể gặp khó khăn trong việc sinh ra các mẫu huấn luyện chính xác liên quan đến các nhãn mong muốn cho các nhiệm vụ NLU thách thức.

Trong bài báo này, chúng tôi khám phá cách sử dụng hiệu quả các mẫu ít-shot để điều chỉnh PLMs để sinh ra các mẫu huấn luyện phân biệt nhãn chất lượng cao. Các đóng góp của chúng tôi như sau: (1) Chúng tôi phân tích các vấn đề của việc sử dụng likelihood cực đại tiêu chuẩn để điều chỉnh bộ sinh và đề xuất một mục tiêu likelihood cực đại có trọng số meta bằng cách tự động học các trọng số token nhấn mạnh tính phân biệt nhãn. (2) Chúng tôi đề xuất một quy trình huấn luyện đơn giản và hiệu quả để tinh chỉnh PLMs phân loại trên dữ liệu được sinh bằng cách giảm thiểu nhiễu nhãn. (3) Dưới cùng một cài đặt học ít mẫu, phương pháp FewGen của chúng tôi vượt trội hơn các phương pháp hiện có 3+ điểm trung bình trên bảy nhiệm vụ phân loại của benchmark GLUE (Wang et al., 2018). Các nghiên cứu ablation xác thực tính hiệu quả của mục tiêu huấn luyện có trọng số meta được đề xuất và phương pháp tinh chỉnh bộ phân loại của chúng tôi.

2. Công trình liên quan
Học ít mẫu với PLMs. Học ít mẫu đã thu hút nhiều sự chú ý gần đây do giả định tài nguyên tối thiểu của nó—Mà không cần dữ liệu chú thích khổng lồ mà chỉ tận dụng một vài mẫu huấn luyện (ví dụ, 16 mỗi nhãn), các phương pháp ít-shot có thể được áp dụng rộng rãi trong nhiều tình huống thực tế nơi việc có được chú thích quy mô lớn không khả thi. Tinh chỉnh tiêu chuẩn của PLMs cho học ít mẫu thường có hiệu suất kém vì các mẫu huấn luyện hạn chế có thể không đủ để tối ưu hóa các tham số trong đầu phân loại mới được giới thiệu. Để tái sử dụng khả năng mô hình hóa ngôn ngữ của PLMs mà không giới thiệu các tham số được khởi tạo ngẫu nhiên, các phương pháp dựa trên prompt (Brown et al., 2020; Gao et al., 2021; Hu et al., 2022; Logan IV et al., 2021; Min et al., 2022a; Schick & Schütze, 2021a;b;d; Tam et al., 2021) định dạng các mẫu huấn luyện dưới dạng mẫu prompt ngôn ngữ tự nhiên để các nhiệm vụ downstream khác nhau có thể được giải quyết như một vấn đề dự đoán token. Chúng tận hưởng hiệu quả dữ liệu huấn luyện được cải thiện so với tinh chỉnh tiêu chuẩn trong các chế độ dữ liệu thấp (Scao & Rush, 2021) và đạt được hiệu suất học ít mẫu đáng chú ý. Các phát triển sau đó trong các phương pháp dựa trên prompt thay thế việc thiết kế thủ công mẫu prompt bằng tìm kiếm hoặc học tự động (Cui et al., 2022; Hambardzumyan et al., 2021; Lester et al., 2021; Liu et al., 2021b; Zhang et al., 2022; Zhong et al., 2021). Cũng có các nghiên cứu tập trung vào các vấn đề cụ thể (Liu et al., 2022a; Tam et al., 2021; Zhao et al., 2021) trong các phương pháp dựa trên prompt. Thay vì đề xuất các phương pháp tinh chỉnh cho học ít mẫu, chúng tôi nghiên cứu cách sinh ra các mẫu huấn luyện chất lượng như các bổ sung bằng cách học từ các mẫu ít-shot.

Bổ sung dữ liệu. Các phương pháp bổ sung dữ liệu (Chen et al., 2020; Huang et al., 2022; Lee et al., 2021; Meng et al., 2021b; Miyato et al., 2017; Xie et al., 2020) nhằm tạo ra các mẫu tương tự với những mẫu hiện có để tập huấn luyện được mở rộng có thể hỗ trợ tổng quát hóa mô hình. Các phương pháp sớm đơn giản sử dụng các quy tắc được thiết kế thủ công (ví dụ, hoán đổi hoặc chèn tokens) để thay đổi ở mức từ trên các mẫu đã cho để tạo ra những mẫu mới (Wei & Zou, 2019). Các phương pháp sau đó tận dụng sức mạnh sinh mạnh mẽ của PLMs để tổng hợp các mẫu mới từ đầu. Với một tập huấn luyện, PLMs có thể được tinh chỉnh trên các mẫu có nhãn để học xác suất sinh có điều kiện nhãn (Kumar et al., 2020; Lee et al., 2021; Yang et al., 2020) hoặc lấy dữ liệu có nhãn làm minh chứng (Wang et al., 2021; Yoo et al., 2021) để sinh ra các mẫu tương tự liên quan đến cùng nhãn. Trong công trình này, chúng tôi nghiên cứu cách điều chỉnh hiệu quả các bộ sinh trên dữ liệu huấn luyện ít-shot để tạo ra dữ liệu mới—tinh chỉnh tiêu chuẩn của PLMs trên một tập nhỏ dữ liệu huấn luyện dễ bị overfitting, và mô hình kết quả có thể gặp khó khăn trong việc sinh ra dữ liệu huấn luyện chính xác, đa dạng và mới. Chúng tôi giải quyết thách thức này bằng cách tận dụng prefix-tuning và đề xuất một mục tiêu điều chỉnh bộ sinh có trọng số meta mới nhấn mạnh các token phân biệt nhãn.

Sinh văn bản có kiểm soát. Sinh các mẫu huấn luyện cho các nhãn khác nhau có thể được xem như một dạng sinh văn bản có kiểm soát (Hu et al., 2017), mà mục tiêu là sinh ra nội dung văn bản với ngữ nghĩa, phong cách hoặc thuộc tính mong muốn. Kiểm soát như vậy có thể được thực hiện thông qua các giai đoạn khác nhau của huấn luyện và triển khai PLM: Trong quá trình pretraining, các mã kiểm soát (Keskar et al., 2019) có thể được sử dụng như hướng dẫn rõ ràng để huấn luyện mô hình sinh ra các văn bản cụ thể theo domain/thuộc tính; tinh chỉnh PLMs với dữ liệu cụ thể theo thuộc tính cũng có thể cấp kiểm soát mức cao (ví dụ, các chủ đề hoặc cảm xúc nhất định (Ziegler et al., 2019)), kiểm soát mức tinh (ví dụ, các từ hoặc cụm từ cụ thể (Chan et al., 2021)) hoặc cả hai (Khalifa et al., 2021); tại thời gian suy luận, kiểm soát các thuộc tính mong muốn cũng có thể được thực thi mà không cập nhật các tham số PLM (Dathathri et al., 2020; Krause et al., 2021; Kumar et al., 2021; Liu et al., 2021a; Pascual et al., 2021; Yang & Klein, 2021). Liên quan cụ thể hơn đến ý tưởng sinh dữ liệu huấn luyện với các mô hình ngôn ngữ, các phương pháp sớm trong phân loại văn bản sử dụng các mô hình ngôn ngữ dựa trên bag-of-words hoặc LSTM (Meng et al., 2018; 2019) để sinh ra các văn bản có điều kiện lớp làm dữ liệu huấn luyện. Gần đây, một số nghiên cứu khám phá tinh chỉnh các PLMs tự hồi quy (Anaby-Tavor et al., 2020; Yang et al., 2020) với mục tiêu mô hình hóa ngôn ngữ tiêu chuẩn trên tập huấn luyện hoặc sử dụng các prompt cụ thể theo nhãn (Gao et al., 2023; Meng et al., 2022a; Schick & Schütze, 2021c; Wang et al., 2021; Ye et al., 2022) để điều hướng sinh văn bản về phía nhãn mong muốn. Trong công trình này, chúng tôi phân tích các vấn đề với việc điều chỉnh trực tiếp PLMs trên các mẫu ít-shot với mục tiêu likelihood cực đại tiêu chuẩn và đề xuất một biến thể có trọng số của mục tiêu khuyến khích PLM tập trung vào các token phân biệt nhãn.

Meta-Learning cho việc trọng số mẫu. Ý tưởng trọng số các mẫu huấn luyện trong tính toán loss xuất phát từ các tình huống mất cân bằng lớp (Wang et al., 2017) và nhãn nhiễu (Hendrycks et al., 2018)—Bằng cách gán trọng số cao hơn cho các mẫu từ các lớp thiểu số hoặc trọng số thấp hơn cho các mẫu nhiễu, quá trình học bị ảnh hưởng ít hơn bởi các vấn đề mất cân bằng/nhiễu nhãn. Meta-learning (Andrychowicz et al., 2016; Finn et al., 2017; Franceschi et al., 2018; Wu et al., 2018) là một cách để tự động học trọng số cho mỗi mẫu. Cụ thể, một mục tiêu meta, thường được định nghĩa là loss trên một tập validation sạch không thiên vị (Ren et al., 2018; Shu et al., 2019), có thể được sử dụng để học các trọng số mẫu trở thành siêu tham số kiểm soát việc tối ưu hóa các tham số mô hình. Công trình của chúng tôi có động lực và công thức khác nhau của mục tiêu meta cho huấn luyện có trọng số theo token: Không phải tất cả các token trong một mẫu huấn luyện đều phân biệt nhãn như nhau. Do đó chúng tôi thiết kế một mục tiêu meta để nhấn mạnh sự phân biệt giữa các nhãn khác nhau (thay vì sử dụng validation loss làm mục tiêu meta) để học các trọng số token.

3. Phương pháp
3.1. Khái niệm cơ bản
Tổng quan. Chúng tôi xem xét cài đặt học ít mẫu nghiêm ngặt (Perez et al., 2021): Tập huấn luyện Dtrain = {(x; y)i} gồm K mẫu huấn luyện mỗi nhãn trong đó x = [x1; x2; :::; xn] là một chuỗi văn bản với n tokens. Tập phát triển Ddev có cùng kích thước với Dtrain. Không có quyền truy cập vào dữ liệu không nhãn cụ thể theo nhiệm vụ bổ sung. Số lượng mẫu huấn luyện K được giả định là rất nhỏ (ví dụ, K = 16), khiến việc huấn luyện một mô hình phân loại C tổng quát hóa tốt với dữ liệu chưa thấy trở nên thách thức. Để giảm thiểu vấn đề khan hiếm dữ liệu huấn luyện, chúng tôi đầu tiên huấn luyện một PLM tự hồi quy trên Dtrain, và sau đó sử dụng nó như một bộ sinh G để tổng hợp thêm các mẫu mới Dgen = {(x̃; ỹ)i} bổ sung cho tập huấn luyện gốc. Cuối cùng, một PLM phân loại C được tinh chỉnh trên cả Dtrain và Dgen để thực hiện nhiệm vụ. Một tổng quan về FewGen được hiển thị trong Hình 1.

Sinh văn bản với PLMs tự hồi quy. Trong tinh chỉnh tiêu chuẩn cho sinh văn bản, một PLM tự hồi quy G được huấn luyện thông qua loss sinh likelihood cực đại của mỗi token trong chuỗi x có điều kiện trên các token trước đó:

min 1/n Σ(j=1 to n) -log p(xj|x<j);
p(xj|x<j) = exp(e^T_j hj) / Σ(j'=1 to |V|) exp(e^T_j' hj)

trong đó xác suất sinh token p(·) thường được tham số hóa sử dụng embeddings token e và các trạng thái ẩn h của một mô hình Transformer (Vaswani et al., 2017). Sau khi huấn luyện, G có thể được sử dụng để sinh ra các văn bản mới bằng cách lặp lại việc lấy mẫu tokens từ phân phối xác suất sinh của nó.

Prefix-Tuning. Không giống như tinh chỉnh cập nhật tất cả các tham số mô hình của một PLM, prefix-tuning (Li & Liang, 2021) đóng băng tất cả các tham số Transformer được pretraining và chỉ tối ưu hóa các vector prefix p được prepend vào mỗi lớp Transformer. Chúng tôi sử dụng prefix-tuning để huấn luyện Gp trên Dtrain vì (1) nó cung cấp hiệu quả tốt hơn tinh chỉnh cho các tập dữ liệu nhỏ (Li & Liang, 2021) và (2) các mô hình sinh cho các nhãn khác nhau có thể chia sẻ cùng các tham số Transformer backbone với chỉ các vector prefix khác nhau, giảm đáng kể yêu cầu bộ nhớ cho các nhiệm vụ phân loại đa lớp.

3.2. Điều chỉnh Bộ sinh Văn bản Phân biệt Nhãn

Động lực. Để mô hình hóa xác suất sinh văn bản có điều kiện p(x|yl) trên các nhãn khác nhau, một cách đơn giản là tham số hóa một mô hình sinh Gpl cho mỗi nhãn yl thông qua một tập các vector prefix p = {pl}^L_l=1 để p(x|yl) = ppl(x), và sau đó điều chỉnh pl trên các mẫu huấn luyện x với nhãn yl:

min pl Lgen; Lgen(pl) = 1/n Σ(j=1 to n) -log ppl(xj|x<j)     (1)

Tuy nhiên, một phương pháp như vậy chỉ tối ưu hóa likelihood sinh p(x|yl) mà không tính đến tính phân biệt nhãn p(yl|x) vốn quan trọng để sinh ra các mẫu huấn luyện rõ ràng nhằm hỗ trợ nhiệm vụ phân loại cuối cùng. Các nhiệm vụ NLU thách thức có thể có các phân phối tương tự nhau giữa các nhãn khác nhau, với những khác biệt rất tinh tế được phản ánh bởi một vài token chính. Ví dụ, một văn bản đánh giá tiêu cực "a movie where the ending feels like a cop-out" có thể ngay lập tức trở thành tích cực chỉ bằng cách thay đổi từ cuối "cop-out" thành "revelation". Thật vậy, chúng tôi thấy rằng những khác biệt tinh tế như vậy giữa các nhãn khác nhau có thể không được nắm bắt hiệu quả khi sử dụng mục tiêu sinh tiêu chuẩn trong Eq. (1) nơi mỗi token đóng góp như nhau vào loss tổng thể. Như được hiển thị trong Hình 2, một loss phân biệt Ldisc (được định nghĩa trong Eq. (2)) thậm chí có thể tăng trong quá trình huấn luyện—Có thể là các mẫu chủ đạo trong các mẫu huấn luyện không phân biệt nhãn (ví dụ, một tập dữ liệu đánh giá phim có thể thường xuyên đề cập đến "the movie"), khiến các bộ sinh của các nhãn khác nhau cuối cùng hội tụ về các phân phối tương tự, đặc biệt khi có ít mẫu huấn luyện mỗi nhãn.

Để thúc đẩy việc sinh ra các văn bản phân biệt nhãn, chúng tôi khuyến khích mỗi token xj có khả năng cao hơn được sinh ra dưới nhãn tương ứng yl thay vì các nhãn khác (tức là, tối đa hóa ppl(xj|x<j) và tối thiểu hóa ppl'(xj|x<j) với l' ≠ l) thông qua một loss phân biệt Ldisc:

Ldisc(p) = 1/n Σ(j=1 to n) L^j_disc(p);
L^j_disc(p) = ppl(xj|x<j) / Σ(l'=1 to L) ppl'(xj|x<j)     (2)

Mặc dù có thể trực tiếp kết hợp Ldisc với Lgen để huấn luyện Gp nhằm thực thi sự phân biệt giữa các nhãn khác nhau, việc làm như vậy sẽ dẫn đến hai hậu quả không mong muốn: (1) Một siêu tham số cần được giới thiệu để cân bằng trọng số của hai losses, mà giá trị tối ưu của nó có thể thay đổi theo nhiệm vụ; và (2) việc cập nhật trực tiếp các tham số bộ sinh với loss phân biệt Ldisc sẽ làm xấu đi chất lượng mô hình hóa ngôn ngữ của bộ sinh, khiến nó dễ sinh ra các văn bản ít trôi chảy và mạch lạc hơn sau khi huấn luyện.

Điều chỉnh Bộ sinh Likelihood Cực đại có Trọng số. Để bảo tồn việc học sinh của Gp trong khi nhấn mạnh các token phân biệt nhãn, chúng tôi giả định mỗi token được liên kết với một trọng số trong loss likelihood cực đại. Một cách trực quan, khi mục tiêu của chúng ta là sinh ra các văn bản phân biệt giữa các nhãn khác nhau làm mẫu huấn luyện, không phải tất cả các token đều nên đóng góp như nhau vào việc huấn luyện bộ sinh. Ví dụ, đối với các nhiệm vụ phân loại cảm xúc, người ta mong đợi "good/bad" phân biệt nhãn hơn "the movie", và cái trước nên được chú ý nhiều hơn trong quá trình huấn luyện. Do đó, việc tổng quát hóa Lgen trong Eq. (1) thành Lw-gen như sau bằng cách giả định một trọng số wj được đưa ra cho mỗi token là tự nhiên.

min pl Lw-gen; Lw-gen(pl; w) = Σ(j=1 to n) wj L^j_gen(pl);     (3)
L^j_gen(pl) = -log ppl(xj|x<j)

Lưu ý rằng trong Lw-gen, w được giả định là siêu tham số dưới đó pl được tối ưu hóa. Khi wj giống nhau cho mọi token, Eq. (3) sẽ tương đương với Eq. (1). Mặc dù có thể thiết kế thủ công các quy tắc trọng số để đặt w nhằm thúc đẩy việc học phân biệt nhãn, chúng có thể đòi hỏi kiến thức cụ thể theo nhiệm vụ và điều chỉnh không tầm thường.

Để tạo điều kiện việc học tự động các trọng số này w, chúng tôi đề xuất tham số hóa chúng như các siêu tham số có thể học sử dụng ý tưởng meta-learning.

Thiết lập Meta Weight Learning. Để tự động học các trọng số token như siêu tham số, chúng tôi công thức hóa một bài toán tối ưu hóa hai cấp sử dụng ý tưởng meta-learning. Mục tiêu bên trong Lw-gen tối ưu hóa các tham số bộ sinh p cho trước các trọng số token wj:

Lw-gen(p; ω) = Σ(j=1 to n) wj(ω) L^j_gen(p);
p*(ω) = arg min p Lw-gen;

trong đó các trọng số token wj(ω) được tham số hóa và học thông qua một mạng trọng số gω (chi tiết về việc triển khai của nó trong Phụ lục A). Các tham số mạng trọng số ω được huấn luyện với một mục tiêu bên ngoài Ldisc:

Ldisc(p*(ω)) = 1/n Σ(j=1 to n) L^j_disc(p*(ω));
ω* = arg min ω Ldisc     (4)

Dưới công thức tối ưu hóa hai cấp trên, loss phân biệt Ldisc không được sử dụng để trực tiếp cập nhật các tham số bộ sinh, mà để tự động học các trọng số token được sử dụng như siêu tham số bởi mục tiêu bên trong Lw-gen. Khi các trọng số token được huấn luyện để tối thiểu hóa Ldisc, bộ sinh tập trung nhiều hơn vào các token phân biệt nhãn.

Chúng tôi sử dụng một chiến lược tối ưu hóa trực tuyến (Shu et al., 2019) thay vì các vòng lặp tối ưu hóa lồng nhau để tối ưu hóa ω và p* cho hiệu quả huấn luyện. Nó cũng đảm bảo hội tụ đến các điểm tới hạn của cả Lw-gen và Ldisc dưới các điều kiện nhẹ. Chúng tôi khởi tạo các tham số prefix p sử dụng các prompt ngôn ngữ tự nhiên, và chi tiết có thể được tìm thấy trong Phụ lục B. Quy trình huấn luyện tổng thể được hiển thị trong Thuật toán 1.

Phân tích Meta Weight Learning. Để nghiên cứu cách các trọng số token được học trong quá trình huấn luyện, chúng tôi phân tích các gradient của các tham số mạng trọng số ω được tối ưu hóa thông qua Eq. (4) (chi tiết dẫn xuất trong Phụ lục C):

∂Ldisc(p̂^(t)(ω))/∂ω|ω=ω^(t) = -α Σ(j=1 to n) dj ∂wj(ω)/∂ω|ω=ω^(t);
dj = ∂Ldisc/∂p̂|p̂=p̂^(t) · ∂L^j_gen(p)/∂p|p=p^(t)

Có thể thấy rằng hướng gradient descent của ω được xác định bởi một tổng các hướng gradient ascent trọng số token (tức là, ∂wj(ω)/∂ω) được trọng số bởi một vô hướng dj, trong đó dj đặc trưng cho sự tương tự giữa gradient của mục tiêu phân biệt và gradient của mục tiêu sinh trên token thứ j. Do đó, các trọng số meta sẽ cao hơn trên những token nơi việc tối ưu hóa mục tiêu sinh của chúng có lợi hơn cho việc tối thiểu hóa mục tiêu phân biệt, để thông tin phân biệt nhãn được nhấn mạnh tốt hơn.

3.3. Tinh chỉnh Bộ phân loại

Với bộ sinh được huấn luyện Gp, chúng ta có thể tổng hợp các mẫu huấn luyện mới Dgen bổ sung Dtrain để tinh chỉnh một PLM phân loại C. Thách thức chính để tận dụng hiệu quả Dgen là nhiễu nhãn (tức là, một số mẫu được sinh có thể không chính xác liên quan đến nhãn tương ứng) có thể làm xấu đi hiệu suất mô hình nếu học có giám sát tiêu chuẩn được sử dụng trực tiếp. Chúng tôi đề xuất một quy trình huấn luyện đơn giản chống nhiễu để cải thiện khả năng tổng quát hóa và ổn định của huấn luyện: Đầu tiên tinh chỉnh C trên Dtrain với huấn luyện có giám sát tiêu chuẩn, và sau đó tiếp tục tinh chỉnh nó trên Dgen bằng cách áp dụng label smoothing (Szegedy et al., 2016) và temporal ensembling (Laine & Aila, 2017) như regularization, theo (Meng et al., 2022a). Cụ thể, với một mẫu huấn luyện (x̃; ỹ) ∈ Dgen, chúng tôi tối thiểu hóa loss phân loại sau:

Lclass(θ) = -Σ(l=1 to L) ql log(pθ(x̃)l) - λ Σ(l=1 to L) zl log pθ(x̃)l / zl;     (5)

trong đó ql = 1(l = ỹ)(1 - ε) + ε/L và ε là trọng số label smoothing; pθ(x̃) là dự đoán mô hình trên x̃; λ là trọng số regularization cho temporal ensembling; và z là các dự đoán mô hình trung bình tích lũy di động. Chúng tôi cũng sử dụng dự đoán ensembled z để lọc ra các mẫu tổng hợp nhiễu: Chúng tôi chỉ bao gồm những mẫu để huấn luyện nơi z đồng ý mạnh mẽ với nhãn ỹ (tức là, zỹ > τ trong đó τ > 0 là một tham số ngưỡng). Trong Eq. (5), số hạng phân loại đầu tiên là cross-entropy loss với nhãn được làm mượt; số hạng regularization thứ hai tương ứng với temporal ensembling, yêu cầu dự đoán mô hình hiện tại gần với các dự đoán tích lũy trước đó của nó. Điều này không chỉ vô hiệu hóa sự dao động trong dự đoán mô hình để có ổn định huấn luyện tốt hơn khi nhiễu nhãn có mặt (Nguyen et al., 2020) mà còn giúp ngăn chặn catastrophic forgetting (Kirkpatrick et al., 2017) của thông tin đã học trước đó từ tập huấn luyện ít-shot Dtrain. Vui lòng tham khảo Phụ lục B để biết chi tiết về việc triển khai temporal ensembling. Quy trình tổng thể của việc tinh chỉnh bộ phân loại được tóm tắt trong Thuật toán 2.

4. Thiết lập Thực nghiệm

Nhiệm vụ Downstream và Metrics. Chúng tôi tiến hành đánh giá trên tất cả các nhiệm vụ của benchmark GLUE (Wang et al., 2018) (chi tiết hơn trong Phụ lục D) ngoại trừ STS-B là một nhiệm vụ hồi quy. Chúng tôi tuân theo cùng một phân chia dữ liệu và giao thức đánh giá như (Gao et al., 2021): Cả Dtrain và Ddev đều chứa 16 mẫu mỗi nhãn và được lấy mẫu từ tập huấn luyện gốc với 5 seeds ngẫu nhiên khác nhau. Các tập phát triển gốc được sử dụng để kiểm tra. Đối với tất cả các kết quả được báo cáo, chúng tôi bao gồm trung bình và độ lệch chuẩn trên 5 phân chia Dtrain/Ddev khác nhau. Điểm F1 được sử dụng như metric cho QQP và MRPC, Matthews correlation cho CoLA, và accuracy cho các nhiệm vụ còn lại.

Mô hình và Cài đặt Huấn luyện. FewGen là một phương pháp sinh dữ liệu huấn luyện và có thể được sử dụng với bất kỳ phương pháp tinh chỉnh nào trên bất kỳ mô hình phân loại nào. Chúng tôi sử dụng các PLMs có kích thước vừa phải để đảm bảo kết quả của chúng tôi có thể tái tạo trên phần cứng nghiên cứu điển hình: CTRL (~1.6B tham số) (Keskar et al., 2019) như bộ sinh G và RoBERTa Large (356M tham số) (Liu et al., 2019) như bộ phân loại C. Chúng tôi sử dụng prefix-tuning để huấn luyện G và tinh chỉnh dựa trên prompt để huấn luyện C. Để đơn giản, chúng tôi sử dụng phiên bản prompt thủ công cơ bản nhất của LM-BFF (Gao et al., 2021). Ngoại lệ duy nhất là CoLA mà chúng tôi sử dụng tinh chỉnh tiêu chuẩn vì dữ liệu đầu vào có thể nằm ngoài phân phối của C (Gao et al., 2021). Việc điều chỉnh siêu tham số được thực hiện trên Ddev. Chi tiết hơn trong Phụ lục B.

Các Phương pháp So sánh. Các baseline không bổ sung bao gồm zero-shot prompting, tinh chỉnh tiêu chuẩn, học trong ngữ cảnh, và các phương pháp học ít mẫu mạnh sau: Bốn phiên bản của LM-BFF (Gao et al., 2021), P-Tuning (Liu et al., 2021b) và DART (Zhang et al., 2022). Chúng tôi cũng so sánh với các phương pháp bổ sung dữ liệu cho học ít mẫu: MixText (Chen et al., 2020), sử dụng hệ thống back translation để sinh ra paraphrases (bổ sung kiểu UDA (Xie et al., 2020)), một phương pháp minh chứng ít-shot GPT3Mix (Yoo et al., 2021), và tinh chỉnh tiêu chuẩn của bộ sinh trên các mẫu ít-shot với prompts. Để so sánh công bằng, tất cả các phương pháp bổ sung sử dụng LM-BFF (Man.) để tinh chỉnh một bộ phân loại RoBERTa Large. Chúng tôi cũng bao gồm kết quả của tinh chỉnh có giám sát đầy đủ. Chi tiết hơn về các baseline bổ sung trong Phụ lục E.

5. Đánh giá

5.1. Kết quả Chính

Chúng tôi trình bày kết quả của FewGen và các baselines trong Bảng 1. FewGen đạt được hiệu suất tốt hơn tổng thể trên các nhiệm vụ GLUE, trung bình cao hơn 5+ điểm so với phương pháp học ít mẫu tốt nhất trước đó không có bổ sung, và tốt hơn 3+ điểm so với GPT3Mix (Yoo et al., 2021) sử dụng mô hình bộ sinh lớn hơn 100 lần (~175B) so với FewGen.

So sánh với Back Translation. Sử dụng back translation để paraphrase các mẫu ít-shot không cải thiện kết quả—điều này có thể là do nó không tạo ra các mẫu đủ khác biệt so với tập huấn luyện ít-shot. Thành công của UDA (Xie et al., 2020) được dựa trên các bổ sung từ dữ liệu không nhãn phong phú cải thiện khả năng tổng quát hóa của bộ phân loại. Tuy nhiên, dưới thiết lập học ít mẫu nghiêm ngặt, không có quyền truy cập vào dữ liệu không nhãn cụ thể theo nhiệm vụ bổ sung (Gao et al., 2021), khiến các phương pháp dựa trên paraphrase khó tạo ra các mẫu huấn luyện đủ đa dạng chỉ dựa trên tập ít-shot nhỏ. Các mẫu huấn luyện mới được tạo ra bởi phương pháp FewGen của chúng tôi không giới hạn ở các paraphrases của các mẫu ít-shot, vì bộ sinh được huấn luyện thông qua prefix-tuning để bảo tồn kiến thức pretraining của PLM, dựa trên đó các mẫu huấn luyện mới có thể được tổng hợp.

So sánh với GPT3Mix. Kích thước khổng lồ của GPT3 khiến việc điều chỉnh trên các mẫu ít-shot trở nên thách thức. Do đó, GPT3Mix (Yoo et al., 2021) sử dụng các mẫu ít-shot làm minh chứng để tạo ra các bổ sung. Một phương pháp như vậy gặp phải hai hạn chế: (1) Không có bất kỳ cập nhật tham số nào cho PLM, khả năng học của nó không được tận dụng đầy đủ để thích ứng với tập huấn luyện ít-shot. (2) PLM chỉ có thể sử dụng một tập con nhỏ của các mẫu ít-shot tại một thời điểm để tạo ra mỗi bổ sung, vì số lượng minh chứng mà mô hình nhận được bị giới hạn bởi độ dài chuỗi đầu vào tối đa của nó. Điều này khiến chất lượng của các bổ sung được tạo ra nhạy cảm hơn với các mẫu huấn luyện được rút ra ngẫu nhiên. Mặt khác, phương pháp FewGen của chúng tôi có thể sử dụng toàn bộ tập ít-shot để điều chỉnh PLM và đạt được kết quả phân loại tốt hơn tổng thể với một PLM nhỏ hơn nhiều (<1% kích thước của mô hình GPT3) có thể được triển khai dễ dàng hơn nhiều trong thực tế.

5.2. Nghiên cứu Ablation

Mức tăng hiệu suất tổng thể do FewGen mang lại so với một đối tác không bổ sung có thể được thấy bằng cách so sánh FewGen với LM-BFF (Man.) sử dụng cùng bộ phân loại và phương pháp tinh chỉnh chỉ trên Dtrain. Chúng tôi tiếp tục phân tích tính hiệu quả của mỗi thành phần quan trọng trong FewGen thông qua các ablation sau: (1) Sử dụng Lgen tiêu chuẩn trong Eq.(1) thay vì Lw-gen được đề xuất trong Eq. (3) để điều chỉnh bộ sinh (w. Lgen); (2) sử dụng Lgen và Ldisc được kết hợp trực tiếp để điều chỉnh bộ sinh (w. Lgen+Ldisc); (3) không áp dụng label smoothing trong Eq. (5) (−label smooth); (4) không áp dụng temporal ensembling trong Eq. (5) (−temporal ensemble); (5) trực tiếp tinh chỉnh mô hình phân loại trên sự kết hợp của Dgen và Dtrain (w. fine-tune on Dtrain ∪ Dgen). Như được hiển thị trong Bảng 2, (1) & (2) sử dụng loss likelihood cực đại tiêu chuẩn hoặc sự kết hợp của losses sinh và phân biệt để điều chỉnh bộ sinh đều mang lại dữ liệu huấn luyện chất lượng thấp hơn và dẫn đến hiệu suất phân loại giảm; (3) & (4) không áp dụng các kỹ thuật regularization để tinh chỉnh bộ phân loại dễ bị nhiễu nhãn hơn trong các mẫu được sinh; (5) tinh chỉnh bộ phân loại trên sự kết hợp của Dgen và Dtrain kém hơn đáng kể phương pháp tinh chỉnh hai bước của chúng tôi.

5.3. Phân tích các Hàm Loss cho Điều chỉnh Bộ sinh

Như được hiển thị trong Bảng 2, việc lựa chọn loss bộ sinh có tác động đáng kể đến chất lượng dữ liệu tổng hợp và do đó hiệu suất mô hình cuối cùng. Chúng tôi tiến hành phân tích sâu hơn để so sánh các quá trình huấn luyện của bộ sinh dưới ba hàm loss sau và các mẫu được sinh kết quả: (1) Lgen là loss mô hình hóa ngôn ngữ tiêu chuẩn; (2) Lgen+Ldisc trực tiếp thêm loss phân biệt vào huấn luyện bộ sinh; và (3) Lw-gen là mục tiêu có trọng số meta của chúng tôi. Hình 3 hiển thị loss phân biệt Ldisc và loss mô hình hóa ngôn ngữ tiêu chuẩn trên tập phát triển được giữ lại trong suốt quá trình huấn luyện. Mặc dù sử dụng Lgen+Ldisc giúp giảm loss phân biệt, nó đi kèm với chi phí cản trở mô hình hóa ngôn ngữ—loss bộ sinh trên tập phát triển cao. Sử dụng mục tiêu có trọng số meta Lw-gen của chúng tôi không chỉ khuyến khích tính phân biệt mà còn giảm thiểu overfitting, mang lại loss tập validation thấp nhất. Điều này có thể là do mô hình nhận được thông tin đối chiếu từ các nhãn khác giúp mô hình hóa chính xác hơn các văn bản với nhãn đích.

Phân tích Định lượng. Ngoài hiệu suất mô hình phân loại cuối cùng phản ánh gián tiếp chất lượng dữ liệu tổng hợp, chúng tôi bổ sung tiến hành phân tích định lượng trực tiếp hơn về các mục tiêu huấn luyện bộ sinh khác nhau. Chúng tôi sử dụng hai metrics: (1) Độ chính xác của các văn bản được sinh, được đánh giá bởi các mô hình RoBERTa Large có giám sát đầy đủ được tinh chỉnh trên các tập huấn luyện gốc của mỗi nhiệm vụ. Chúng tôi chọn áp dụng đánh giá tự động như vậy thay vì đánh giá con người vì nó hiệu quả và đáng tin cậy—các mô hình RoBERTa Large có giám sát đầy đủ có độ chính xác tương đương hoặc tốt hơn các baselines con người theo benchmark GLUE. (2) Perplexity của bộ sinh trên các tập kiểm tra, phản ánh mức độ mô hình hóa phân phối nhiệm vụ của bộ sinh. Như được hiển thị trong Bảng 3, sử dụng Lw-gen để huấn luyện bộ sinh vượt trội một cách nhất quán so với việc sử dụng Lgen hoặc Lgen+Ldisc, cả về độ chính xác văn bản được sinh và khả năng mô hình hóa ngôn ngữ.

So sánh Lw-gen với Lgen, các trọng số meta được học tự động nhấn mạnh các token phân biệt trong huấn luyện bộ sinh và giúp bộ sinh nắm bắt những khác biệt ngữ nghĩa tinh tế giữa các nhãn khác nhau, dẫn đến chất lượng mô hình hóa ngôn ngữ tốt hơn và dữ liệu tổng hợp phân biệt hơn.

So sánh Lw-gen với Lgen+Ldisc, mục tiêu huấn luyện bộ sinh không bị tác động trực tiếp bởi mục tiêu phân biệt, do đó tránh được vấn đề can thiệp gradient trong học đa nhiệm vụ (Standley et al., 2019)—gradient để tối ưu hóa xác suất sinh p(x|yl) sẽ bị can thiệp bởi gradient tối ưu hóa xác suất phân biệt p(yl|x) nếu Lgen+Ldisc được sử dụng. Do đó, sử dụng Lw-gen dẫn đến chất lượng mô hình hóa ngôn ngữ tốt hơn và kết quả sinh trôi chảy và mạch lạc hơn.

Phân tích Định tính. Chúng tôi trình bày các kết quả sinh cụ thể cho ba nhãn của MNLI bởi các mô hình được huấn luyện với ba hàm loss khác nhau trong Bảng 4. Mô hình được huấn luyện với Lgen tạo ra các câu trôi chảy và mạch lạc, nhưng các câu được sinh không chính xác liên quan đến các nhãn mong muốn (tức là, kết quả sinh "entailment" và "contradiction" thực chất là neutral đối với câu đã cho), thiếu tính phân biệt nhãn. Khi Lgen+Ldisc được sử dụng, các mẫu được sinh của các nhãn khác nhau phân biệt hơn, nhưng cũng trở nên ít tự nhiên và mạch lạc hơn do khả năng mô hình hóa ngôn ngữ của mô hình bị cản trở. Bộ sinh được điều chỉnh với Lw-gen tạo ra các mẫu vừa mạch lạc vừa phân biệt nhãn. Kết quả sinh cụ thể hơn cho mỗi nhiệm vụ có thể được tìm thấy trong Phụ lục F.

5.4. Trực quan hóa các Trọng số Token đã học

Để hiểu cách các trọng số token được học tự động trong quá trình điều chỉnh bộ sinh, chúng tôi trực quan hóa các trọng số đã học trong Hình 4. Các token có trọng số cao hơn (ví dụ, "weak" trong ví dụ đầu tiên và "hates" trong ví dụ thứ hai) được học là những token quan trọng quyết định mối quan hệ của câu thứ hai với câu đầu tiên (tức là, nhãn của mẫu huấn luyện). Với những token như vậy được nhấn mạnh trong quá trình huấn luyện, bộ sinh được khuyến khích nắm bắt thông tin phân biệt nhãn giúp sinh ra các mẫu huấn luyện rõ ràng.

6. Thảo luận và Kết luận

Cân nhắc Đạo đức. Mặc dù có sức mạnh sinh văn bản và biểu diễn ấn tượng của PLMs, chúng cũng có thể đi kèm với rủi ro (Bender et al., 2021; Bender & Koller, 2020; Brown et al., 2020) sinh ra thông tin sai lệch (Pagnoni et al., 2021) hoặc làm trầm trọng thêm thiên vị (Prabhumoye et al., 2018). Thay vì cải thiện kiến trúc PLM hoặc kỹ thuật sinh, công trình của chúng tôi tập trung vào việc sử dụng các PLMs hiện có để tạo ra dữ liệu huấn luyện cho các nhiệm vụ NLU. Trong thực tế, phương pháp của chúng tôi có thể được kết hợp với bất kỳ chiến lược giảm thiểu và sửa chữa thiên vị nào (Gehman et al., 2020; Ma et al., 2020) để giảm các tác động bất lợi của PLMs.

Hạn chế. So với các phương pháp học ít mẫu trực tiếp huấn luyện mô hình phân loại trên tập huấn luyện nhỏ, FewGen đòi hỏi điều chỉnh một PLM bộ sinh và sử dụng nó để tổng hợp các mẫu huấn luyện mới, dẫn đến chi phí tính toán cao hơn và thời gian chạy lâu hơn. Tuy nhiên, chúng tôi tin rằng phương pháp của chúng tôi có thể mang lại nhiều lợi ích hơn tác hại—khi kích thước dữ liệu huấn luyện nhỏ trở thành nút thắt cổ chai hiệu suất cho các nhiệm vụ NLU, một giải pháp đơn giản nhưng tốn kém là có được thêm chú thích con người. Phương pháp của chúng tôi có thể thay thế hoặc giảm nỗ lực con người trong các quá trình tạo dữ liệu huấn luyện như vậy.

Kết luận. Trong công trình này, chúng tôi đề xuất FewGen, tận dụng các mẫu huấn luyện ít-shot để điều chỉnh một PLM bộ sinh để tổng hợp dữ liệu huấn luyện mới. Dữ liệu được sinh sau đó có thể được sử dụng kết hợp với các mẫu ít-shot để tinh chỉnh một mô hình phân loại để có khả năng tổng quát hóa tốt hơn. Để nhấn mạnh thông tin phân biệt nhãn trong quá trình điều chỉnh bộ sinh, chúng tôi đề xuất một mục tiêu likelihood cực đại có trọng số trong đó các trọng số token được học tự động thông qua một mục tiêu meta phân biệt. Vì các mẫu được sinh có thể chứa nhiễu nhãn, chúng tôi đề xuất một quy trình huấn luyện đơn giản đầu tiên huấn luyện các bộ phân loại trên tập huấn luyện ít-shot và sau đó trên tập được sinh bằng cách áp dụng regularization cho khả năng chống nhiễu. Trên bảy nhiệm vụ phân loại từ benchmark GLUE, FewGen vượt trội đáng kể so với các phương pháp hiện có dưới cùng cài đặt học ít mẫu. Tính hiệu quả của mỗi thành phần quan trọng trong FewGen được xác thực thông qua các nghiên cứu ablation. Các hướng tương lai có thể bao gồm: Sử dụng PLMs lớn hơn làm bộ sinh và bộ phân loại, huấn luyện chung cả hai mô hình với các dự đoán tin cậy cao của nhau, cải thiện độ mạnh mẽ của các mô hình được huấn luyện trên dữ liệu tổng hợp, và phát triển các metrics hệ thống để đánh giá chất lượng của các mẫu huấn luyện được sinh.

Lời cảm ơn
Nghiên cứu được hỗ trợ một phần bởi Chương trình KAIROS của US DARPA số FA8750-19-2-1004 và Chương trình INCAS số HR001121C0165, National Science Foundation IIS-19-56151, IIS-17-41317, và IIS 17-04532, và Viện Molecule Maker Lab: Một chương trình Viện Nghiên cứu AI được hỗ trợ bởi NSF dưới Giải thưởng số 2019897, và Viện Hiểu biết Không gian Địa lý thông qua Môi trường Khám phá Tích hợp (I-GUIDE) bởi NSF dưới Giải thưởng số 2118329. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào được thể hiện ở đây là của các tác giả và không nhất thiết đại diện cho quan điểm, được thể hiện rõ ràng hay ngụ ý, của DARPA hoặc Chính phủ Hoa Kỳ. Yu Meng được hỗ trợ bởi Google PhD Fellowship. Chúng tôi cảm ơn các nhà đánh giá ẩn danh cho phản hồi có giá trị và sâu sắc.
