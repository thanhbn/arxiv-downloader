# Mở rộng Mô hình Ngôn ngữ Tiền huấn luyện thành Ki��u trúc Sâu hơn
# thông qua Kiến trúc Hiệu quả Tham số

Peiyu Liu1;3, Ze-Feng Gao1, Yushuo Chen1;3, Wayne Xin Zhao1;3y, và Ji-Rong Wen1;2;3
1Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Trường Thông tin, Đại học Nhân dân Trung Quốc
3Phòng thí nghiệm Trọng điểm Bắc Kinh về Phương pháp Quản lý và Phân tích Dữ liệu Lớn
{liupeiyustu,zfgao,jrwen}@ruc.edu.cn,
batmanfly@gmail.com,chenyushuo1999@foxmail.com

## Tóm tắt

Trong bài báo này, chúng tôi đề xuất một phương pháp có hiệu quả tham số cao để mở rộng mô hình ngôn ngữ tiền huấn luyện (PLM) thành độ sâu mô hình lớn hơn. Không giống như các nghiên cứu trước đây chia sẻ tất cả tham số hoặc sử dụng các khối bổ sung, chúng tôi thiết kế kiến trúc chia sẻ tham số có khả năng hơn dựa trên toán tử tích ma trận (MPO). Phân tích MPO có thể tổ chức lại và phân tích thông tin của ma trận tham số thành hai phần: phần chính chứa thông tin chủ yếu (tensor trung tâm) và phần bổ sung chỉ có một tỷ lệ nhỏ tham số (tensor phụ trợ). Dựa trên sự phân tích này, kiến trúc của chúng tôi chia sẻ tensor trung tâm trên tất cả các lớp để giảm kích thước mô hình và đồng thời giữ các tensor phụ trợ đặc trưng cho từng lớp (cũng sử dụng bộ điều hợp) để tăng cường tính linh hoạt thích ứng. Để cải thiện việc huấn luyện mô hình, chúng tôi đề xuất thêm thuật toán khởi tạo ổn định được thiết kế riêng cho kiến trúc dựa trên MPO. Các thí nghiệm mở rộng đã chứng minh hiệu quả của mô hình đề xuất trong việc giảm kích thước mô hình và đạt hiệu suất cạnh tranh cao.

## 1 Giới thiệu

Gần đây, mô hình ngôn ngữ tiền huấn luyện (PLM) đã đạt được thành công lớn trong nhiều tác vụ NLP bằng cách khám phá kiến trúc mô hình ngày càng lớn hơn (Raffel et al., 2020; Radford et al., 2019). Đã được chứng minh rằng có thể tồn tại quy luật tỷ lệ giữa kích thước mô hình và khả năng mô hình đối với PLM (Kaplan et al., 2020), thu hút nhiều nỗ lực để tăng cường hiệu suất bằng cách mở rộng kích thước mô hình (Chowdhery et al., 2022; Wang et al., 2022).

Như một phương pháp đơn giản, chúng ta có thể trực tiếp tăng số lượng lớp của mạng Transformer để cải thiện khả năng mô hình (Wang et al., 2022; Huang et al., 2020). Tuy nhiên, kiến trúc rất sâu thường tương ứng với kích thước mô hình lớn đáng kể, dẫn đến chi phí cao cả về tính toán và lưu trữ (Gong et al., 2019). Và, việc triển khai mạng sâu trong các môi trường hạn chế tài nguyên là khó khăn, mặc dù nó thường có khả năng mô hình mạnh hơn. Do đó, có nhu cầu cấp thiết để phát triển cách thức hiệu quả tham số cho việc mở rộng độ sâu mô hình.

Để giảm tham số trong mạng sâu, chia sẻ trọng số đã được chứng minh là rất hữu ích để thiết kế kiến trúc Transformer nhẹ (Zhang et al., 2022; Lan et al., 2019). Như một công trình đại diện bằng chia sẻ tham số giữa các lớp, ALBERT (Lan et al., 2019) chỉ giữ mười phần trăm tổng số tham số của BERT trong khi duy trì hiệu suất tương đương. Mặc dù ý tưởng chia sẻ tham số đơn giản nhưng (ở một mức độ nào đó) hiệu quả, đã được phát hiện rằng trọng số giống hệt nhau giữa các lớp khác nhau là nguyên nhân chính gây suy giảm hiệu suất (Zhang et al., 2022). Để giải quyết vấn đề này, các khối bổ sung được thiết kế để nâng cao sự đa dạng tham số trong mỗi lớp (Nouriborji et al., 2022). Tuy nhiên, chúng vẫn sử dụng kiến trúc cứng nhắc của trọng số lớp được chia sẻ, có khả năng mô hình hạn chế.

Ngoài ra, việc tối ưu hóa mô hình rất sâu là khó khăn, đặc biệt khi có các thành phần được chia sẻ. Mặc dù các nghiên cứu gần đây (Wang et al., 2022; Huang et al., 2020) đề xuất các phương pháp khởi tạo cải tiến, chúng không xem xét trường hợp có chia sẻ tham số, do đó có thể dẫn đến hiệu suất không tối ưu trên kiến trúc chia sẻ tham số.

Để giải quyết những thách thức này, trong bài báo này, chúng tôi đề xuất một phương pháp có hiệu quả tham số cao để mở rộng PLM thành kiến trúc mô hình sâu hơn. Như đóng góp cốt lõi, chúng tôi đề xuất kiến trúc chia sẻ tham số dựa trên toán tử tích ma trận (MPO) cho mạng Transformer sâu. Thông qua phân tích MPO, ma trận tham số có thể được phân tích thành các tensor trung tâm (chứa thông tin chủ yếu) và tensor phụ trợ (chứa thông tin bổ sung). Phương pháp của chúng tôi chia sẻ các tensor trung tâm của ma trận tham số trên tất cả các lớp để giảm kích thước mô hình, và đồng thời giữ các tensor phụ trợ đặc trưng cho từng lớp để tăng cường tính linh hoạt thích ứng. Để huấn luyện kiến trúc sâu như vậy, chúng tôi đề xuất phương pháp khởi tạo dựa trên MPO bằng cách sử dụng kết quả phân tích MPO của ALBERT. Hơn nữa, đối với các tensor phụ trợ của các lớp cao hơn (nhiều hơn 24 lớp trong ALBERT), chúng tôi đề xuất thiết lập tham số với hệ số tỷ lệ được dẫn xuất từ phân tích lý thuyết. Chúng tôi chứng minh lý thuyết rằng nó có thể giải quyết vấn đề bất ổn định huấn luyện bất kể độ sâu mô hình.

Công trình của chúng tôi cung cấp cách thức chia sẻ tham số mới cho việc mở rộng độ sâu mô hình, có thể được áp dụng một cách tổng quát cho các mô hình dựa trên Transformer khác nhau. Chúng tôi tiến hành thí nghiệm mở rộng để đánh giá hiệu suất của mô hình MPOBERT đề xuất trên benchmark GLUE so với PLM với kích thước mô hình khác nhau (tiny, small và large). Kết quả thí nghiệm đã chứng minh hiệu quả của mô hình đề xuất trong việc giảm kích thước mô hình và đạt hiệu suất cạnh tranh. Với ít tham số hơn BERT BASE, chúng tôi mở rộng độ sâu mô hình lên gấp 4 lần và đạt điểm GLUE cao hơn 0,1 điểm so với BERT LARGE.

## 2 Công trình liên quan

**Toán tử Tích Ma trận.** Toán tử tích ma trận (còn gọi là toán tử tensor-train (Oseledets, 2011)) được đề xuất để biểu diễn hiệu quả hơn cấu trúc tuyến tính của mạng nơ-ron (Gao et al., 2020a), sau đó được sử dụng để nén mạng nơ-ron sâu (Novikov et al., 2015), mạng nơ-ron tích chập (Garipov et al., 2016; Yu et al., 2017), và LSTM (Gao et al., 2020b; Sun et al., 2020a). Dựa trên phân tích MPO, các nghiên cứu gần đây đã thiết kế các phương pháp tinh chỉnh và nén nhẹ cho PLM (Liu et al., 2021), và phát triển kiến trúc MoE hiệu quả tham số (Gao et al., 2022). Khác với các công trình này, công trình của chúng tôi nhằm phát triển PLM rất sâu với kiến trúc nhẹ và huấn luyện ổn định.

**PLM Hiệu quả Tham số.** Các nỗ lực hiện tại để giảm tham số của PLM có thể được phân loại rộng rãi thành ba hướng chính: chưng cất kiến thức, cắt tỉa mô hình, và chia sẻ tham số. Đối với các phương pháp dựa trên chưng cất kiến thức (Sanh et al., 2019; Sun et al., 2020b,b; Liu et al., 2020), PLM được chưng cất thành mạng sinh viên với ít tham số hơn nhiều. Đối với các phương pháp dựa trên cắt tỉa, chúng cố gắng loại bỏ các thành phần ít quan trọng hơn (Michel et al., 2019; Wang et al., 2020) hoặc trọng số rất nhỏ (Chen et al., 2020). Hơn nữa, phương pháp chia sẻ tham số được đề xuất thêm bằng cách chia sẻ tất cả tham số (Lan et al., 2019) hoặc kết hợp các thành phần phụ trợ cụ thể (Reid et al., 2021; Nouriborji et al., 2022). Khác với các công trình này, chúng tôi thiết kế kiến trúc dựa trên MPO có thể giảm kích thước mô hình và cho phép tính linh hoạt thích ứng, bằng cách phân tích ma trận gốc.

**Tối ưu hóa cho Mô hình Sâu.** Mặc dù việc tăng số lượng lớp để mở rộng kích thước mô hình là đơn giản, nhưng việc tối ưu hóa mạng rất sâu là khó khăn do vấn đề bất ổn định huấn luyện. Một số nghiên cứu đã đề xuất các chiến lược khác nhau để vượt qua khó khăn này cho việc huấn luyện mạng Transformer sâu, bao gồm Fixup (Zhang et al., 2019) bằng cách tái tỷ lệ đúng cách khởi tạo tiêu chuẩn, T-Fixup (Huang et al., 2020) bằng cách đề xuất sơ đồ khởi tạo trọng số, và DeepNorm (Wang et al., 2022) bằng cách giới thiệu hàm chuẩn hóa mới. Để so sánh, chúng tôi nghiên cứu cách tối ưu hóa kiến trúc sâu dựa trên MPO với chiến lược chia sẻ tham số, và khám phá việc sử dụng PLM được huấn luyện tốt để khởi tạo, có trọng tâm khác với công trình hiện tại.

## 3 Phương pháp

Trong phần này, chúng tôi mô tả phương pháp MPOBERT đề xuất để xây dựng PLM sâu thông qua kiến trúc có hiệu quả tham số cao. Phương pháp của chúng tôi tuân theo paradigm chia sẻ trọng số cổ điển, đồng thời giới thiệu cơ chế có nguyên tắc để chia sẻ tham số có thông tin giữa các lớp và cũng cho phép thích ứng trọng số đặc trưng cho từng lớp.

### 3.1 Tổng quan về Phương pháp của chúng tôi

Mặc dù chia sẻ trọng số đã được khám phá rộng rãi để xây dựng PLM nhỏ gọn (Lan et al., 2019), các nghiên cứu hiện tại hoặc chia sẻ tất cả tham số giữa các lớp (Lan et al., 2019) hoặc kết hợp các khối bổ sung để hỗ trợ việc chia sẻ (Zhang et al., 2022; Nouriborji et al., 2022). Chúng hoặc có khả năng mô hình hạn chế với kiến trúc cứng nhắc hoặc yêu cầu nỗ lực bổ sung để duy trì.

Xem xét các vấn đề trên, chúng tôi thúc đẩy phương pháp của mình theo hai khía cạnh. Thứ nhất, chỉ các tham số có thông tin nên được chia sẻ giữa các lớp, thay vì tất cả tham số. Thứ hai, nó không nên ảnh hưởng đến khả năng nắm bắt các biến đổi đặc trưng cho từng lớp. Để đạt được điều này, chúng tôi sử dụng phân tích MPO từ vật lý đa thể (Gao et al., 2020a) để phát triển kiến trúc hiệu quả tham số bằng cách chia sẻ các thành phần có thông tin giữa các lớp và giữ các thành phần bổ sung đặc trưng cho từng lớp (Phần 3.2). Như một vấn đề tiềm ẩn khác, việc tối ưu hóa PLM sâu là khó khăn do huấn luyện không ổn định (Wang et al., 2022), đặc biệt khi có chia sẻ trọng số (Lan et al., 2019). Chúng tôi đề xuất thêm một phương pháp đơn giản nhưng hiệu quả để ổn định việc huấn luyện MPOBERT (Phần 3.3). Tiếp theo, chúng tôi giới thiệu chi tiết kỹ thuật của phương pháp.

### 3.2 Lớp Transformer dựa trên MPO

Trong phần này, chúng tôi trước tiên giới thiệu phân tích MPO và giới thiệu cách sử dụng nó để xây dựng PLM sâu hiệu quả tham số.

#### 3.2.1 Phân tích MPO

Cho ma trận trọng số W ∈ R^(I×J), phân tích MPO (Gao et al., 2020a) có thể phân tích ma trận thành tích của n tensor bằng cách định hình lại hai kích thước I và J:

W_{i_1,...,i_n;j_1,...,j_n} = T^{(1)}[i_1,j_1]...T^{(n)}[i_n,j_n]    (1)

trong đó chúng ta có I = ∏_{k=1}^n i_k, J = ∏_{k=1}^n j_k, và T^{(k)}[i_k,j_k] là tensor 4 chiều với kích thước d_{k-1} × i_k × j_k × d_k trong đó d_k là chiều liên kết nối T^{(k)} và T^{(k+1)} với d_0 = d_n = 1. Để đơn giản, chúng tôi bỏ qua chiều liên kết trong Phương trình (1). Khi n là số lẻ, tensor giữa chứa nhiều tham số nhất (với chiều liên kết lớn nhất), trong khi kích thước tham số của phần còn lại giảm dần với khoảng cách tăng dần đến tensor giữa. Theo (Liu et al., 2021), chúng tôi đơn giản hóa thêm kết quả phân tích của ma trận thành tensor trung tâm C (tensor giữa) và tensor phụ trợ {A_i}_{i=1}^{n-1} (tensor còn lại).

Như một ưu điểm chính, sự phân tích như vậy có thể tổ chức lại và tập hợp thông tin của ma trận một cách hiệu quả (Gao et al., 2020a): tensor trung tâm C có thể mã hóa thông tin cần thiết của ma trận gốc, trong khi tensor phụ trợ {A_i}_{i=1}^{n-1} phục vụ như bổ sung để tái tạo chính xác ma trận.

#### 3.2.2 Mở rộng dựa trên MPO sang Mô hình Sâu

Dựa trên phân tích MPO, bản chất của phương pháp mở rộng của chúng tôi là chia sẻ tensor trung tâm giữa các lớp (nắm bắt thông tin cần thiết) và giữ tensor phụ trợ đặc trưng cho từng lớp (mô hình hóa các biến đổi đặc trưng cho từng lớp). Hình 2 cho thấy tổng quan kiến trúc của MPOBERT đề xuất.

**Chia sẻ Tham số Giữa các Lớp.** Để giới thiệu kiến trúc của chúng tôi, chúng tôi xem xét cấu trúc đơn giản hóa của L lớp, mỗi lớp bao gồm một ma trận duy nhất. Với phân tích MPO bậc năm (tức là, n = 5), chúng ta có thể thu được kết quả phân tích cho ma trận trọng số (W^{(l)}), ký hiệu là {C^{(l)}, A_1^{(l)}, A_2^{(l)}, A_3^{(l)}, A_4^{(l)}}_{l=1}^L, trong đó C^{(l)} và {A_i^{(l)}}_{i=1}^4 là tensor trung tâm và tensor phụ trợ của lớp thứ l. Phương pháp của chúng tôi là đặt tensor trung tâm được chia sẻ C giữa các lớp, có nghĩa là C^{(l)} = C (∀l = 1...L). Như được chỉ ra trong Gao et al. (2020a), tensor trung tâm chứa tỷ lệ chính của tham số (hơn 90%), và do đó phương pháp của chúng tôi có thể giảm lớn tham số khi mở rộng PLM thành kiến trúc rất sâu. Lưu ý rằng chiến lược này có thể dễ dàng áp dụng cho nhiều ma trận trong lớp Transformer, và chúng tôi bỏ qua thảo luận cho phần mở rộng đa ma trận. Một phần mở rộng khác là chia sẻ tensor trung tâm bằng cách nhóm các lớp khác nhau. Chúng tôi triển khai MPOBERT nhóm lớp, được gọi là MPOBERT+, chia các lớp thành nhiều phần và đặt tensor trung tâm được chia sẻ duy nhất trong mỗi nhóm.

**Thích ứng Trọng số Đặc trưng cho Lớp.** Không giống như ALBERT (Lan et al., 2019), kiến trúc dựa trên MPO của chúng tôi cho phép thích ứng đặc trưng cho từng lớp bằng cách giữ tensor phụ trợ đặc trưng cho từng lớp ({A_i^{(l)}}_{i=1}^4). Các tensor phụ trợ này được phân tích từ ma trận gốc, thay vì các khối bổ sung (Zhang et al., 2022). Chúng chỉ chứa tỷ lệ rất nhỏ tham số, không tăng đáng kể kích thước mô hình. Trong khi đó, một ưu điểm khác của phân tích MPO là các tensor này có tương quan cao thông qua chiều liên kết, và một nhiễu loạn nhỏ trên tensor phụ trợ có thể phản ánh toàn bộ ma trận (Liu et al., 2021). Nếu tác vụ hạ nguồn yêu cầu tính đặc trưng lớp nhiều hơn, chúng ta có thể kết hợp thêm bộ điều hợp thứ hạng thấp (Hu et al., 2021) cho việc thích ứng đặc trưng cho từng lớp. Cụ thể, chúng tôi ký hiệu W_{Adapter}^{(l)} là bộ điều hợp thứ hạng thấp cho W^{(l)}. Theo cách này, W^{(l)} có thể được công thức hóa như một tập hợp tensor: {C^{(l)}, A_1^{(l)}, A_2^{(l)}, A_3^{(l)}, A_4^{(l)}, W_{Adapter}^{(l)}}. Quy mô tham số của bộ điều hợp, L × r × d_{total}, được xác định bởi số lượng lớp L, thứ hạng r, và hình dạng của ma trận gốc (d_{total} = d_{in} + d_{out} là tổng của chiều đầu vào và đầu ra của Lớp Transformer). Vì chúng tôi sử dụng bộ điều hợp thứ hạng thấp, chúng ta có thể kiểm soát hiệu quả số lượng tham số bổ sung từ bộ điều hợp.

### 3.3 Huấn luyện Ổn định cho MPOBERT

Với phương pháp dựa trên MPO ở trên, chúng ta có thể mở rộng PLM thành kiến trúc sâu hơn theo cách có hiệu quả tham số cao. Tuy nhiên, như được chỉ ra trong các nghiên cứu trước (Lan et al., 2019; Wang et al., 2022), việc tối ưu hóa PLM rất sâu là khó khăn, đặc biệt khi có các thành phần được chia sẻ. Trong phần này, chúng tôi giới thiệu thuật toán huấn luyện đơn giản nhưng ổn định cho MPOBERT và sau đó thảo luận về cách nó giải quyết vấn đề bất ổn định huấn luyện.

#### 3.3.1 Khởi tạo Mạng dựa trên MPO

Công trình hiện tại đã phát hiện rằng khởi tạo tham số quan trọng cho việc huấn luyện mô hình sâu (Huang et al., 2020; Zhang et al., 2019; Wang et al., 2022), có thể giúp giảm thiểu bất ổn định huấn luyện. Để tối ưu hóa MPOBERT mở rộng tốt hơn, chúng tôi đề xuất phương pháp khởi tạo được thiết kế đặc biệt dựa trên kiến trúc dựa trên MPO ở trên.

**Khởi tạo với Phân tích MPO.** Vì MPOBERT chia sẻ các thành phần toàn cục (tức là, tensor trung tâm) trên tất cả các lớp, ý tưởng của chúng tôi là sử dụng PLM được huấn luyện tốt hiện có dựa trên chia sẻ trọng số để cải thiện khởi tạo tham số. Ở đây, chúng tôi sử dụng ALBERT 24 lớp đã phát hành với tất cả tham số được chia sẻ giữa các lớp. Ý tưởng chính là thực hiện phân tích MPO trên ma trận tham số của ALBERT, và thu được các tensor trung tâm và phụ trợ tương ứng. Tiếp theo, chúng tôi thảo luận về khởi tạo MPOBERT theo hai khía cạnh. Đối với tensor trung tâm, chúng tôi trực tiếp khởi tạo chúng (từng cái cho mỗi ma trận) bằng tensor trung tâm dẫn xuất từ kết quả phân tích MPO của ALBERT. Vì chúng được chia sẻ toàn cục, chỉ cần một bản sao duy nhất để khởi tạo bất kể độ sâu lớp. Tương tự, đối với tensor phụ trợ, chúng ta có thể trực tiếp sao chép tensor phụ trợ từ kết quả phân tích MPO của ALBERT.

**Tỷ lệ Khởi tạo.** Một vấn đề tiềm ẩn là ALBERT chỉ cung cấp kiến trúc 24 lớp, và chiến lược như vậy không còn hỗ trợ khởi tạo cho kiến trúc hơn 24 lớp (không có tensor phụ trợ tương ứng). Như giải pháp của chúng tôi, chúng tôi mượn ý tưởng trong Wang et al. (2022) tránh bùng nổ cập nhật bằng cách kết hợp hệ số tỷ lệ bổ sung và nhân các giá trị được khởi tạo ngẫu nhiên cho tensor phụ trợ (những cái trong các lớp cao hơn 24 lớp) với hệ số (2L)^(-1/4), trong đó L là số lượng lớp. Tiếp theo, chúng tôi trình bày phân tích lý thuyết về tính ổn định huấn luyện.

#### 3.3.2 Phân tích Lý thuyết

Để hiểu vấn đề bất ổn định huấn luyện từ góc độ lý thuyết, chúng tôi xem xét mô hình dựa trên Transformer F(x; W) với x và W là đầu vào và tham số, và xem xét cập nhật một bước ΔF. Theo Wang et al. (2022), cập nhật mô hình lớn (ΔF) ở đầu huấn luyện có thể gây ra bất ổn định huấn luyện của mô hình Transformer sâu. Để giảm thiểu vấn đề bùng nổ cập nhật, cập nhật nên được giới hạn bởi hằng số, tức là, ||ΔF|| = O(1). Tiếp theo, chúng tôi nghiên cứu cách ΔF được giới hạn với MPOBERT.

**Giới hạn Cập nhật dựa trên MPO.** Không mất tính tổng quát, chúng tôi xem xét trường hợp đơn giản của phân tích MPO bậc thấp: n = 3 trong Phương trình (1). Theo phương pháp dẫn xuất trong Wang et al. (2022), chúng tôi đơn giản hóa ma trận W, A₁, C và A₂ thành các đại lượng vô hướng w, u, c, v, có nghĩa là tham số wₗ tại lớp thứ l có thể được phân tích như wₗ = uₗcₗvₗ. Dựa trên các ký hiệu này, chúng tôi xem xét mô hình dựa trên Transformer L-lớp F(x; w) (w = {w₁, w₂, ..., wₗ}), trong đó mỗi lớp con được chuẩn hóa với Post-LN: xₗ₊₁ = LN(xₗ + Gₗ(xₗ; wₗ)). Sau đó chúng ta có thể chứng minh ||ΔF|| thỏa mãn (xem Định lý A.1 trong Phụ lục):

||ΔF|| ≤ ∑ᴸₗ₌₁(c₁vₗ||∇uₗuₗ|| + c₁uₗ||∇vₗvₗ|| + vₗuₗ||∇c₁c₁||)    (2)

Phương trình trên giới hạn cập nhật mô hình theo các tensor trung tâm và phụ trợ. Vì tensor trung tâm (cₗ) có thể được khởi tạo bằng trọng số tiền huấn luyện, chúng ta có thể đơn giản hóa thêm giới hạn trên bằng cách giảm chúng. Với một số dẫn xuất (Xem Hệ quả A.2 trong Phụ lục), chúng ta có thể thu được (v²ᵢ + u²ᵢ)(uₗvₗ) = O(1/L) để đảm bảo rằng ||ΔF|| = O(1). Để đơn giản, chúng tôi đặt uᵢ = vᵢ = (2L)^(-1/4) để giới hạn độ lớn của mỗi cập nhật độc lập với số lượng lớp L. Trong triển khai, chúng tôi trước tiên áp dụng phương pháp Xavier để khởi tạo, và sau đó tỷ lệ các giá trị tham số với hệ số (2L)^(-1/4).

**So sánh.** Nghiên cứu trước đây đã chỉ ra rằng sử dụng các giá trị được thiết kế cho khởi tạo ngẫu nhiên có thể cải thiện việc huấn luyện mô hình sâu (Huang et al., 2020; Zhang et al., 2019; Wang et al., 2022). Các phương pháp này nhằm cải thiện khởi tạo kiến trúc Transformer tổng quát để huấn luyện từ đầu. Để so sánh, chúng tôi khám phá việc sử dụng trọng số tiền huấn luyện và sử dụng kết quả phân tích MPO để khởi tạo. Đặc biệt, Gong et al. (2019) đã chứng minh hiệu quả của việc xếp chồng các lớp nông tiền huấn luyện cho mô hình sâu trong việc tăng tốc hội tụ, cũng cho thấy tính ưu việt về hiệu suất của trọng số tiền huấn luyện so với khởi tạo ngẫu nhiên.

#### 3.3.3 Huấn luyện và Tăng tốc

Để cụ thể hóa phương pháp của chúng tôi, chúng tôi tiền huấn luyện mô hình BERT 48 lớp (tức là, MPOBERT 48). Để so sánh công bằng với BERT BASE và BERT LARGE, chúng tôi áp dụng cùng kho liệu tiền huấn luyện (BOOKCORPUS (Zhu et al., 2015) và English Wikipedia (Devlin et al., 2018)) và tác vụ tiền huấn luyện (mô hình hóa ngôn ngữ có mặt nạ, và dự đoán thứ tự câu). Chúng tôi trước tiên thực hiện phân tích MPO trên trọng số của ALBERT và sử dụng thuật toán khởi tạo trong Phần 3.3.1 để đặt trọng số tham số. Trong quá trình huấn luyện, chúng tôi cần giữ bản sao cập nhật của tensor trung tâm và tensor phụ trợ: chúng tôi tối ưu hóa chúng theo các tác vụ tiền huấn luyện theo cách đầu cuối và kết hợp chúng để dẫn xuất ma trận tham số gốc cho tính toán tiến (tốn chi phí tương đối nhỏ của phép nhân ma trận song song).

Thông thường, tốc độ của quá trình tiền huấn luyện bị ảnh hưởng bởi ba yếu tố chính: băng thông số học, băng thông bộ nhớ, hoặc độ trễ. Chúng tôi sử dụng thêm một loạt cách tối ưu hóa hiệu quả để tăng tốc tiền huấn luyện, như huấn luyện độ chính xác hỗn hợp với FP16 (giảm băng thông bộ nhớ và số học) và triển khai hợp nhất của kích hoạt và chuẩn hóa (giảm độ trễ). Cuối cùng, chúng tôi có thể huấn luyện MPOBERT 48 lớp với chi phí thời gian 3,8 ngày (so với chi phí không tối ưu hóa 12,5 ngày) trên cấu hình máy chủ của chúng tôi (8 card GPU NVIDIA V100 và bộ nhớ 32GB). Thêm chi tiết huấn luyện có thể được tìm thấy trong phần thiết lập thí nghiệm Phần 4.1 và Phụ lục A.2 (Bảng 6 và Thuật toán 1).

## 4 Thí nghiệm

Trong phần này, chúng tôi trước tiên thiết lập thí nghiệm và sau đó đánh giá hiệu quả của MPOBERT trên nhiều tác vụ khác nhau với các cài đặt mô hình khác nhau.

### 4.1 Thiết lập Thí nghiệm

**Thiết lập Tiền huấn luyện.** Đối với kiến trúc, chúng tôi ký hiệu số lượng lớp là L, kích thước ẩn là H, và số lượng đầu tự chú ý là A. Chúng tôi báo cáo kết quả trên bốn kích thước mô hình: MPOBERT 12 (L=12, H=768, A=12), MPOBERT 24 (L=24, H=1024, A=16), MPOBERT 48 (L=48, H=1024, A=16) và MPOBERT 48+ triển khai chia sẻ tham số giữa các lớp trong ba nhóm riêng biệt như được thảo luận trong tiểu mục 3.2.2. Chúng tôi tiền huấn luyện tất cả các mô hình với kích thước batch 4096 trong 10k bước. Mã của chúng tôi sẽ được phát hành sau thời gian đánh giá.

**Bộ dữ liệu Tinh chỉnh.** Để đánh giá hiệu suất mô hình của chúng tôi, chúng tôi tiến hành thí nghiệm trên benchmark GLUE (Wang et al., 2018) và SQuAD v1.1 (Rajpurkar et al., 2016). Vì tinh chỉnh thường nhanh, chúng tôi chạy tìm kiếm tham số toàn diện và chọn mô hình hoạt động tốt nhất trên tập phát triển để đưa ra dự đoán trên tập kiểm tra.

**Mô hình Baseline.** Chúng tôi so sánh MPOBERT đề xuất với các PLM sâu cạnh tranh hiện tại và mô hình hiệu quả tham số. Để so sánh công bằng, chúng tôi chia mô hình thành ba danh mục chính dựa trên kích thước mô hình:

**Mô hình Tiny (#To < 50M).** ALBERT 12 (Lan et al., 2019) là PLM đại diện nhất đạt kết quả cạnh tranh chỉ với 11M.

**Mô hình nhỏ (50M < #To < 100M).** Chúng tôi xem xét PLM (T5 12) và mô hình nén (MobileBERT (Sun et al., 2020b), DistilBERT (Sanh et al., 2019) và TinyBERT (Jiao et al., 2019)).

**Mô hình cơ bản (#To > 100M).** Chúng tôi so sánh với BERT 12, XLNet 12, RoBERTa 12 và BART 12 cho danh mục này. Lưu ý rằng chúng tôi chỉ bao gồm các biến thể cơ bản có kích thước mô hình tương tự để so sánh công bằng.

### 4.2 Kết quả Chính

**Cài đặt Giám sát Hoàn toàn.** Chúng tôi trình bày kết quả của MPOBERT và các mô hình baseline khác trên GLUE và Squad cho tinh chỉnh trong Bảng 1.

Đầu tiên, chúng tôi đánh giá hiệu suất của MPOBERT so với các mô hình khác có số lượng tham số tương tự. Cụ thể, đối với mô hình tiny, MPOBERT 24 vượt trội hơn ALBERT 24, và đạt cải thiện đáng kể trên cả tập phát triển (85.7 so với 84.0) và tập kiểm tra (82.6 so với 81.2). Điều này làm nổi bật lợi ích của việc tăng khả năng từ tham số đặc trưng cho từng lớp (tức là, tensor phụ trợ và bộ điều hợp đặc trưng cho từng lớp) trong MPOBERT. Hơn nữa, đối với mô hình nhỏ và cơ bản, MPOBERT 48 lớp liên tục đạt kết quả tốt hơn T5 12 và tất cả mô hình hiệu quả tham số, đồng thời cũng đạt kết quả tương đương với các PLM 12 lớp khác với số lượng tham số giảm. Điều này chứng minh lợi ích đáng kể của việc mở rộng theo độ sâu mô hình với tham số đặc trưng cho từng lớp trong MPOBERT.

Thứ hai, chúng tôi đánh giá hiệu quả tham số của MPOBERT bằng cách so sánh với các PLM khác trong cùng độ sâu mô hình. Ví dụ, khi xem xét mô hình với L=12 lớp, MPOBERT đạt kết quả tương đương hoặc thậm chí vượt trội (+1.7 cho BERT 12 và +0.4 cho XLNet 12) PLM trong khi có ít tham số hơn. Điều này làm nổi bật thêm ưu điểm của phương pháp hiệu quả tham số của MPOBERT trong việc xây dựng mô hình sâu.

**Cài đặt Tinh chỉnh Đa tác vụ.** Để chứng minh hiệu quả của mô hình chia sẻ tham số đề xuất trong việc học biểu diễn được chia sẻ trên nhiều tác vụ, chúng tôi tinh chỉnh MPOBERT, BERT và ALBERT trên benchmark GLUE đa tác vụ và báo cáo kết quả trong Bảng 2. Cụ thể, chúng tôi thiết kế hai nhóm thí nghiệm. (1) Mô hình sâu so với nông. So sánh với BERT 12, MPOBERT 48 có các lớp Transformer sâu hơn nhiều nhưng vẫn ít tổng số tham số hơn (tức là, 75M so với 110M). Chúng tôi thấy rằng MPOBERT 48 đạt điểm GLUE trung bình cao hơn 1.4 điểm so với BERT 12. (2) Chia sẻ tensor trung tâm so với chia sẻ tất cả trọng số. So sánh với ALBERT 12, MPOBERT 12 chỉ chia sẻ một phần trọng số, tức là, tensor trung tâm, trong khi ALBERT 12 chia sẻ tất cả trọng số. Chúng tôi thấy rằng chia sẻ tensor trung tâm có thể cải thiện hiệu quả kết quả trung bình hơn chia sẻ tất cả trọng số (82.0 so với 81.4 cho MRPC).

**Cài đặt Học Few-shot.** Chúng tôi đánh giá hiệu suất của mô hình đề xuất, MPOBERT, trong cài đặt học few-shot (Huang et al., 2022) trên hai tác vụ, SST-2 và MNLI, sử dụng số lượng ví dụ được gắn nhãn hạn chế. Kết quả trong Bảng 3 cho thấy rằng MPOBERT vượt trội hơn BERT, vốn gặp phải over-fitting, và ALBERT, không hưởng lợi từ số lượng tham số giảm. Các kết quả này chứng minh thêm tính ưu việt của mô hình đề xuất trong việc khai thác tiềm năng của khả năng mô hình lớn dưới các kịch bản dữ liệu hạn chế.

### 4.3 Phân tích Chi tiết

**Phân tích Phương pháp Khởi tạo.** Thí nghiệm này nhằm loại trừ hiệu ứng của trọng số tiền huấn luyện được khởi tạo trên kết quả tinh chỉnh. Chúng tôi vẽ đồ thị hiệu suất của mô hình trên SST-2 theo các bước huấn luyện. Cụ thể, chúng tôi so sánh hiệu suất của MPOBERT sử dụng các phương pháp khởi tạo khác nhau (Xavier trong Hình 3(a) và trọng số phân tích của ALBERT trong Hình 3(b)) cho tiền huấn luyện. Kết quả chứng minh rằng tiền huấn luyện MPOBERT từ đầu yêu cầu khoảng 50k bước để đạt hiệu suất tương đương với BERT BASE, trong khi khởi tạo với trọng số phân tích của ALBERT tăng tốc đáng kể sự hội tụ và dẫn đến cải thiện rõ ràng trong 10k bước huấn luyện đầu tiên. Ngược lại, lợi ích từ tiền huấn luyện liên tục cho ALBERT là không đáng kể. Các kết quả này đảm bảo rằng các cải thiện quan sát được trong MPOBERT không chỉ do việc sử dụng trọng số tiền huấn luyện được khởi tạo.

**Phân tích Ablation.** Để đánh giá tác động riêng lẻ của các thành phần trong mô hình MPOBERT, chúng tôi tiến hành nghiên cứu ablation bằng cách loại bỏ hoặc bộ điều hợp đặc trưng cho từng lớp hoặc chiến lược chia sẻ tham số giữa các lớp. Kết quả được hiển thị trong Bảng 4, cho thấy rằng việc loại bỏ một trong hai thành phần dẫn đến giảm hiệu suất của mô hình, làm nổi bật tầm quan trọng của cả hai thành phần trong chiến lược đề xuất. Mặc dù kết quả cũng chỉ ra rằng chia sẻ tham số giữa các lớp đóng vai trò quan trọng hơn trong hiệu suất mô hình.

**So sánh Hiệu suất theo Thứ hạng Adapter.** Để so sánh tác động của thứ hạng adapter trong bộ điều hợp đặc trưng cho từng lớp đối với hiệu suất của MPOBERT, chúng tôi huấn luyện MPOBERT với các thứ hạng khác nhau (4, 8 và 64) và đánh giá mô hình trên các tác vụ hạ nguồn trong Bảng 5. Kết quả chứng minh rằng thứ hạng 8 là đủ cho MPOBERT, điều này chứng minh thêm sự cần thiết của bộ điều hợp đặc trưng cho từng lớp. Tuy nhiên, chúng tôi cũng quan sát thấy giảm hiệu suất của biến thể với thứ hạng adapter 64. Điều này minh họa rằng việc tăng thêm thứ hạng có thể tăng nguy cơ over-fitting trong quá trình tinh chỉnh. Do đó, chúng tôi đặt thứ hạng 8 cho MPOBERT trong kết quả chính.

**Phân tích Mẫu Ngôn ngữ học.** Để điều tra các mẫu ngôn ngữ học được nắm bắt bởi MPOBERT, BERT và ALBERT, chúng tôi tiến hành một bộ tác vụ thăm dò, theo phương pháp của Tenney et al. (2019). Các tác vụ này được thiết kế để đánh giá việc mã hóa thông tin bề mặt, cú pháp và ngữ nghĩa trong biểu diễn của mô hình. Kết quả được hiển thị trong Hình 4, cho thấy rằng BERT mã hóa cú pháp cục bộ hơn ở các lớp thấp hơn và ngữ nghĩa phức tạp hơn ở các lớp cao hơn, trong khi ALBERT không thể hiện xu hướng rõ ràng như vậy. Tuy nhiên, MPOBERT thể hiện hành vi theo lớp tương tự như BERT trong một số tác vụ (tức là, tác vụ 0, 2, 4), và kết quả cải thiện ở các lớp thấp hơn cho các tác vụ khác (tức là, tác vụ 3) tương tự như ALBERT. Kết quả chứng minh rằng MPOBERT nắm bắt thông tin ngôn ngữ học khác với các mô hình khác, và tham số theo lớp của nó đóng vai trò quan trọng trong sự khác biệt này.

## 5 Kết luận

Chúng tôi phát triển MPOBERT, một mô hình ngôn ngữ tiền huấn luyện hiệu quả tham số cho phép mở rộng hiệu quả của mô hình sâu mà không cần tham số bổ sung hoặc tài nguyên tính toán. Chúng tôi đạt được điều này bằng cách giới thiệu lớp Transformer dựa trên MPO và chia sẻ tensor trung tâm giữa các lớp. Trong quá trình huấn luyện, chúng tôi đề xuất các phương pháp khởi tạo cho tensor trung tâm và phụ trợ, dựa trên phân tích lý thuyết để giải quyết các vấn đề ổn định huấn luyện. Hiệu quả của MPOBERT được chứng minh thông qua các thí nghiệm đa dạng, như giám sát, đa tác vụ, và few-shot nơi nó liên tục vượt trội hơn các mô hình cạnh tranh khác.

## Hạn chế

Kết quả trình bày trong nghiên cứu của chúng tôi bị hạn chế bởi một số tác vụ và bộ dữ liệu xử lý ngôn ngữ tự nhiên được đánh giá, và cần nghiên cứu thêm để hiểu đầy đủ về khả năng diễn giải và tính mạnh mẽ của mô hình MPOBERT. Ngoài ra, có tính chủ quan trong việc lựa chọn tác vụ và bộ dữ liệu hạ nguồn, mặc dù chúng tôi sử dụng các phân loại được công nhận rộng rãi từ tài liệu. Hơn nữa, các ràng buộc tính toán hạn chế khả năng nghiên cứu hiệu suất mở rộng của mô hình MPOBERT ở độ sâu lớn hơn như 96 lớp hoặc nhiều hơn. Đây là lĩnh vực cho nghiên cứu tương lai.

## Tuyên bố Đạo đức

Việc sử dụng kho ngữ liệu lớn để huấn luyện mô hình ngôn ngữ lớn có thể đặt ra các mối quan tâm về đạo đức, đặc biệt là liên quan đến tiềm năng thiên vị trong dữ liệu. Trong nghiên cứu của chúng tôi, chúng tôi thực hiện các biện pháp phòng ngừa để giảm thiểu vấn đề này bằng cách chỉ sử dụng các nguồn dữ liệu huấn luyện tiêu chuẩn, như BOOKCORPUS và Wikipedia, được sử dụng rộng rãi trong huấn luyện mô hình ngôn ngữ (Devlin et al., 2018; Lan et al., 2019). Tuy nhiên, quan trọng cần lưu ý rằng khi áp dụng phương pháp của chúng tôi cho các bộ dữ liệu khác, tiềm năng thiên vị phải được xem xét cẩn thận và giải quyết. Cần nghiên cứu thêm và chú ý đến vấn đề này trong các nghiên cứu tương lai.
