# SODA : Thích ứng Hiệu quả Tham số Nhận biết Phổ cho Mô hình Khuếch tán

Xinxi Zhang1,*Song Wen1,*Ligong Han1,*,†Felix Juefei-Xu2Akash Srivastava3
Junzhou Huang4Hao Wang1Molei Tao5Dimitris Metaxas1
1Rutgers University2New York University3MIT-IBM Watson AI Lab4UT Arlington5Georgia Tech
*Đóng góp bằng nhau†Trưởng dự án, Tác giả liên hệ

Hình 1: SODA đạt được chất lượng hình ảnh và sự đồng bộ văn bản vượt trội trên các hình ảnh đầu vào và lời nhắc đa dạng, như thay đổi nền, thay đổi kết cấu, và tổng hợp tư thế mới. Ngoài ra, SODA có thể tạo ra những hình ảnh đồng bộ với lời nhắc theo phong cách cho trước được chỉ định bởi một hình ảnh phong cách đầu vào.

## Tóm tắt

Việc thích ứng các mô hình sinh sáng quy mô lớn đã được huấn luyện trước theo cách hiệu quả tham số đang được quan tâm. Các phương pháp truyền thống như thích ứng thứ hạng thấp đạt hiệu quả tham số bằng cách áp đặt ràng buộc nhưng có thể không tối ưu cho các tác vụ yêu cầu khả năng biểu diễn cao. Chúng tôi đề xuất một khung thích ứng nhận biết phổ mới cho các mô hình sinh sáng. Phương pháp của chúng tôi điều chỉnh cả giá trị kỳ dị và các vector cơ sở của chúng trong các trọng số đã được huấn luyện trước. Sử dụng tích Kronecker và các bộ tối ưu hóa Stiefel hiệu quả, chúng tôi đạt được thích ứng hiệu quả tham số của các ma trận trực giao. Cụ thể, chúng tôi giới thiệu Thích ứng Phân tích Trực giao Phổ (SODA), cân bằng hiệu quả tính toán và khả năng biểu diễn. Các đánh giá rộng rãi trên các mô hình khuếch tán văn bản-thành-hình ảnh chứng minh hiệu quả của SODA, cung cấp một thay thế nhận biết phổ cho các phương pháp tinh chỉnh hiện có.

## 1 Giới thiệu

Việc thích ứng các mô hình nền tảng sinh sáng thị giác quy mô lớn đã được huấn luyện trước, như Stable Diffusion [35, 10, 5], theo cách hiệu quả tham số, ngày càng thu hút sự chú ý trong cộng đồng nghiên cứu. Những mô hình sinh sáng này, đã chứng minh khả năng đáng kể trong việc tạo ra hình ảnh chất lượng cao, có thể tốn nhiều tính toán và yêu cầu tài nguyên bộ nhớ đáng kể. Để làm cho những mô hình này dễ tiếp cận hơn và có thể thích ứng với các ứng dụng khác nhau [19, 47, 4, 44], các nhà nghiên cứu đang tập trung vào các phương pháp tinh chỉnh những mô hình này một cách hiệu quả mà không cần huấn luyện lại toàn bộ mạng.

Thích ứng hiệu quả tham số [28, 8, 42] không chỉ giảm chi phí tính toán mà còn cho phép triển khai mô hình nhanh hơn và linh hoạt hơn trên các tác vụ và bộ dữ liệu khác nhau.

Tiềm năng cho việc tinh chỉnh hiệu quả tham số đã được làm nổi bật thông qua các xác thực rộng rãi, chứng minh khả năng thích ứng các mô hình cơ sở với dữ liệu khác nhau, cho phép cải tiến và tùy chỉnh phù hợp với các tác vụ cụ thể và đặc điểm người dùng. Những phương pháp này cho phép kiến trúc mô hình cơ bản vẫn giữ nguyên phần lớn trong khi chèn hoặc điều chỉnh một tập con nhỏ các tham số. Cách tiếp cận này có lợi vì nó bảo tồn kiến thức đã được huấn luyện trước trong khi giới thiệu các điều chỉnh cụ thể cho tác vụ. Bản chất nhẹ của các tham số được tối ưu hóa cũng tạo điều kiện cho việc tích hợp liền mạch, làm cho có thể đạt được hiệu suất cao mà không có chi phí tính toán liên quan đến việc huấn luyện lại mô hình đầy đủ.

Hiệu quả của những phương pháp này thường đạt được bằng cách giới thiệu cấu trúc hoặc ràng buộc vào không gian tham số. Ví dụ, Thích ứng Thứ hạng Thấp (LoRA) [17] ràng buộc các thay đổi tham số ở dạng thứ hạng thấp, trong khi Adapter Kronecker (KronA) [9] ràng buộc các thay đổi ma trận trọng số là một tích Kronecker. Bằng cách áp đặt những ràng buộc này, quá trình thích ứng trở nên dễ quản lý hơn và hiệu quả về mặt tính toán. LoRA, ví dụ, hoạt động dưới giả định rằng các điều chỉnh cần thiết đối với các tham số của mô hình về bản chất là thấp chiều, điều này đơn giản hóa quá trình tối ưu hóa. Tuy nhiên, ràng buộc thứ hạng thấp này có thể không phải lúc nào cũng tối ưu, đặc biệt đối với các tác vụ yêu cầu khả năng biểu diễn cao hơn, vì nó có thể hạn chế khả năng của mô hình trong việc nắm bắt các mẫu phức tạp trong dữ liệu.

Mặc dù có tính đơn giản và hiệu quả của LoRA, ràng buộc thứ hạng thấp của nó có thể không tối ưu cho các tác vụ yêu cầu khả năng biểu diễn cao. Cụ thể, đối với xấp xỉ thứ hạng r của ma trận W, nghiệm tối ưu tương ứng với r giá trị kỳ dị lớn nhất và các vector kỳ dị liên quan của chúng, điều mà LoRA không sử dụng một cách rõ ràng. Hạn chế này cho thấy rằng có những hướng có giá trị tiềm năng trong không gian tham số, được biểu diễn bởi các vector kỳ dị, không được khai thác. Nhận ra khoảng trống này, chúng tôi đề xuất tận dụng thông tin phổ đầy đủ của các ma trận trọng số đã được huấn luyện trước trong quá trình tinh chỉnh, từ đó nâng cao khả năng thích ứng và hiệu suất của mô hình.

Trong bài báo này, chúng tôi đề xuất một cách tiếp cận mới để thích ứng nhận biết phổ của các mô hình sinh sáng. Phương pháp của chúng tôi tận dụng không gian phổ của các trọng số đã được huấn luyện trước, điều chỉnh cả giá trị kỳ dị và vector kỳ dị trong quá trình tinh chỉnh. Bằng cách tập trung vào cả độ lớn và hướng của những thành phần phổ này, chúng tôi có thể đạt được thích ứng tinh tế và hiệu quả hơn. Để đảm bảo hiệu quả tham số, chúng tôi sử dụng tích Kronecker để xoay các vector kỳ dị, do đó sửa đổi cả độ lớn và hướng của chúng. Cách tiếp cận này cho phép chúng tôi duy trì sự cân bằng giữa hiệu quả tính toán và khả năng nắm bắt các biểu diễn dữ liệu phức tạp, làm cho phương pháp của chúng tôi đặc biệt phù hợp cho các tác vụ chiều cao. Những đóng góp của chúng tôi như sau:

• Chúng tôi đề xuất một khung tinh chỉnh cho việc cá nhân hóa các mô hình khuếch tán văn bản-thành-hình ảnh bằng cách sử dụng phổ của các tham số đã được huấn luyện trước.

• Chúng tôi giới thiệu SODA, Thích ứng Phân tích Trực giao Phổ, một công thức hiệu quả tham số của khung tinh chỉnh nhận biết phổ cho các mô hình sinh sáng tận dụng tích Kronecker và điều chỉnh cùng lúc độ lớn và hướng của các vector kỳ dị của tham số trong quá trình tinh chỉnh.

• Chúng tôi tiến hành đánh giá rộng rãi cho phương pháp của mình về tùy chỉnh các mô hình khuếch tán văn bản-thành-hình ảnh, và chứng minh rằng nó là một thay thế hấp dẫn cho các phương pháp tinh chỉnh hiệu quả tham số truyền thống không nhận biết phổ.

## 2 Nghiên cứu liên quan

**Cá nhân hóa khuếch tán.** Cá nhân hóa khuếch tán [39, 16, 41, 38, 33, 35] [12, 36] nhằm học hoặc tái tạo các khái niệm hoặc chủ thể sử dụng các mô hình khuếch tán đã được huấn luyện trước, với một hoặc vài hình ảnh cho trước.

Một số nghiên cứu [36, 23, 24] tinh chỉnh các mô hình khuếch tán đã được huấn luyện trước trên các hình ảnh chứa các khái niệm hoặc chủ thể mong muốn. DreamBooth [36] đề xuất tinh chỉnh toàn bộ tập hợp trọng số để biểu diễn các chủ thể hoặc khái niệm như các định danh duy nhất, có thể được sử dụng để tổng hợp hình ảnh của các kịch bản hoặc phong cách khác nhau. CustomDiffusion [23] gợi ý rằng việc tinh chỉnh chỉ các lớp chú ý chéo là đủ để học các khái niệm mới, dẫn đến hiệu suất tốt hơn trong việc tạo ra thành phần đa khái niệm. Lee et al. đề xuất DCO [24], tinh chỉnh các mô hình khuếch tán mà không mất khả năng tổng hợp của các mô hình đã được huấn luyện trước bằng cách sử dụng các mô hình phần thưởng ngầm. Một hướng nghiên cứu khác [12, 11] tập trung vào việc tối ưu hóa các embedding từ. Cụ thể, Textual Inversion [12] đề xuất tối ưu hóa các embedding từ để nắm bắt các khái niệm duy nhất trong khi đóng băng các trọng số mô hình khuếch tán đã được huấn luyện trước. Ngoài ra, một số nghiên cứu [13, 43, 2] kết hợp việc tối ưu hóa embedding từ và trọng số mô hình khuếch tán. Tuy nhiên, các phương pháp tinh chỉnh mô hình thường liên quan đến một số lượng lớn tham số, có thể không hiệu quả và dễ bị overfitting.

**Tinh chỉnh hiệu quả tham số.** Sự phát triển nhanh chóng của các mô hình nền tảng, chứa một số lượng lớn tham số, đã làm cho việc tinh chỉnh những mô hình này trên các bộ dữ liệu nhỏ trở nên thách thức do số lượng tham số lớn liên quan. Để giải quyết các vấn đề về hiệu quả và overfitting trong tinh chỉnh mô hình, các kỹ thuật Tinh chỉnh Hiệu quả Tham số (PEFT) đã được đề xuất. Một hướng nghiên cứu PEFT tập trung vào tinh chỉnh Adapter [34, 18, 6], bao gồm việc chèn các lớp có thể huấn luyện bên trong các lớp của mô hình đã được huấn luyện trước. Một hướng nghiên cứu khác khám phá Thích ứng Thứ hạng Thấp (LoRA) [17, 48, 7, 22, 49, 15]. LoRA [17, 37] đề xuất học các trọng số dư bằng cách xây dựng chúng thông qua phép nhân của hai ma trận thứ hạng thấp, từ đó giảm đáng kể số lượng tham số được học. Các phương pháp khác cũng đã được phát triển, như SVDiff [14], thực hiện phân tích giá trị kỳ dị trên các ma trận trọng số đã được huấn luyện trước và chỉ tinh chỉnh các giá trị kỳ dị, và OFT [32, 25], duy trì năng lượng hyperspherical của mô hình đã được huấn luyện trước bằng cách nhân với một ma trận trực giao có thể huấn luyện. Ngoài ra, KronA [9, 27] xây dựng các ma trận trọng số dư sử dụng tích Kronecker của hai ma trận kích thước nhỏ. Tuy nhiên, các phương pháp nêu trên không tận dụng đầy đủ kiến thức tiên nghiệm được nhúng trong các trọng số đã được huấn luyện trước. Trong nghiên cứu này, chúng tôi nâng cao các phương pháp PEFT hiện có bằng cách giới thiệu Thích ứng Phân tích Trực giao Phổ. Trong khi một nghiên cứu đồng thời [46] cũng sử dụng cách tiếp cận nhận biết phổ, nó khác với chúng tôi vì nó chỉ tập trung vào việc tinh chỉnh không gian phổ hàng đầu.

## 3 Phương pháp

### 3.1 Kiến thức cơ bản

**Thích ứng Thứ hạng Thấp (LoRA).** Các mô hình khuếch tán văn bản-thành-hình ảnh bao gồm nhiều trọng số lớn đã được huấn luyện trước. Chúng tôi tuân theo triển khai Stable Diffusion của LoRA [37] và chỉ tinh chỉnh các ma trận chiếu tuyến tính trong các lớp chú ý chéo, một trong số đó được ký hiệu là W₀ ∈ Rᵐˣⁿ. Thay đổi trọng số trong quá trình tinh chỉnh được ký hiệu là ΔW. Thích ứng Thứ hạng Thấp (LoRA) giả định thứ hạng thấp của các gia số trọng số của mạng và phân tích mỗi ma trận gia số thành tích của hai ma trận thứ hạng thấp ΔW = BA, trong đó A ∈ Rʳˣⁿ và B ∈ Rᵐˣʳ. Do đó, chúng tôi có thể suy ra lan truyền xuôi sau:

h = W₀x + ΔWx = W₀x + BAx,                    (1)

trong đó h và x tương ứng biểu diễn đầu ra và đầu vào của W.

**Tinh chỉnh Trực giao (OFT).** Tinh chỉnh mô hình khuếch tán thường yêu cầu hiệu quả và bảo tồn kiến thức tiên nghiệm. Để nâng cao kiến thức tiên nghiệm, OFT [32] đề xuất giữ lại năng lượng hyperspherical trong cấu trúc quan hệ theo cặp giữa các neuron. Chi tiết, nó học một ma trận trực giao để thực hiện cùng một phép biến đổi cho tất cả các neuron trong mỗi lớp, điều này giữ cho góc giữa tất cả các neuron trong mỗi lớp không thay đổi. Cập nhật trọng số được biểu diễn bởi

W = W₀R,                                        (2)

trong đó R là một ma trận trực giao và W₀ là trọng số đã được huấn luyện trước. Để giảm số lượng tham số có thể huấn luyện, OFT gốc sử dụng cấu trúc đường chéo khối để làm cho nó hiệu quả tham số, trong đó R := diag(R₁, R₂, ..., Rᵣ) và mỗi Rᵢ là một ma trận trực giao kích thước nhỏ.

### 3.2 Tối ưu hóa trên Đa tạp Stiefel

Một đa tạp Stiefel St(n, m) := {V ∈ Rⁿˣᵐ : V^T V = Iᵐˣᵐ} là tập hợp các ma trận n×m (n ≤ m) với mỗi cột trực giao với tất cả các cột khác. Xem xét bài toán tối ưu hóa min_{V∈St(n,m)} f(V), đó là tìm một ma trận V tối thiểu hóa một hàm mục tiêu cho trước f(V) với điều kiện ràng buộc rằng V nằm trên đa tạp Stiefel. Bài toán tối ưu hóa này có nhiều ứng dụng, như OFT ở trên. Để đảm bảo rằng tham số được tối ưu hóa ở lại trên đa tạp Stiefel, OFT gốc sử dụng tham số hóa Cayley, R = (I + S)(I - S)⁻¹ trong đó S là một ma trận nghiêng đối xứng. Khi đó R là một ma trận trực giao và OFT chỉ cần tối ưu hóa S. Ở đây chúng tôi sử dụng bộ tối ưu hóa Stiefel được giới thiệu trong [21], bảo tồn cấu trúc đa tạp và giữ động lượng trong không gian tiếp tuyến. Cho một ma trận trực giao V, chúng tôi có thể sử dụng bộ tối ưu hóa Stiefel để giữ nó trong đa tạp Stiefel. Tuy nhiên, tối ưu hóa trực tiếp V không hiệu quả tham số. Ở đây chúng tôi sử dụng tích Kronecker, chúng tôi tận dụng nhận xét sau:

**Nhận xét.** Nếu V₁, V₂, ..., Vᵣ là các ma trận trực giao, thì tích Kronecker của chúng ∏ᵢ₌₁ʳ Vᵢ = V₁ ⊗ V₂ ⊗ ··· ⊗ Vᵣ cũng là trực giao.

Khi đó, tích Kronecker của nhiều ma trận trực giao kích thước nhỏ có thể tạo ra một ma trận trực giao kích thước tương đối lớn, vì vậy số lượng tham số được giảm. Công thức như vậy hiệu quả hơn OFT, chúng tôi gọi nó là Tinh chỉnh Trực giao Kronecker (KOFT). Tuy nhiên, không phải lúc nào cũng có thể tìm thấy một phân tích Kronecker cho bất kỳ ma trận trực giao nào cho trước. Do đó, lấy cảm hứng từ OFT, chúng tôi học một ma trận xoay R để điều chỉnh V₀, V_R := V₀R. Bây giờ, vì R được khởi tạo như đồng nhất I, chúng tôi luôn có thể tham số hóa nó bằng tích Kronecker, V_R = V₀(R₁ ⊗ R₂). Cấu trúc đường chéo khối chia sẻ như được áp dụng bởi OFT gốc tương ứng với Iᵣₓᵣ ⊗ R. So sánh với của chúng tôi, điều này thưa thớt hơn. Vậy tại sao chúng tôi muốn xem xét tối ưu hóa trong đa tạp Stiefel? Một mặt, các ma trận trực giao tự nhiên xuất hiện trong phân tích số. Mặt khác, các nghiên cứu trước đây [1, 3] đã chỉ ra rằng việc áp đặt tính trực giao lên các tham số mô hình tạo điều kiện cho việc học bằng cách hạn chế gradient nổ/biến mất và cải thiện độ bền vững.

### 3.3 Tinh chỉnh Nhận biết Phổ

LoRA và OFT đã cho thấy hiệu suất đầy hứa hẹn trong việc thích ứng các mô hình đã được huấn luyện trước với các tác vụ xuôi dòng. Tuy nhiên, LoRA bỏ qua kiến thức tiên nghiệm trong các trọng số đã được huấn luyện trước, trong khi OFT chỉ sử dụng thông tin góc giữa các neuron, không khám phá đầy đủ kiến thức trong các trọng số đã được huấn luyện trước. Để sử dụng tốt hơn phổ của các trọng số đã được huấn luyện trước, chúng tôi đề xuất Thích ứng Phân tích Trực giao Phổ (SODA). Trước tiên chúng tôi phân tích ma trận trọng số đã được huấn luyện trước W₀ trong mỗi lớp thành một thành phần phổ W^spec₀ và một ma trận cơ sở W^basis₀, được công thức hóa là W₀ = W^spec₀W^basis₀. Sau đó chúng tôi cập nhật phổ trong thành phần phổ W^spec₀ và ma trận cơ sở W^basis₀ riêng biệt. Phổ trong thành phần phổ W^spec₀ được tối ưu hóa bằng bộ tối ưu hóa gradient descent. Vì ma trận cơ sở luôn trực giao, chúng tôi có thể sử dụng bộ tối ưu hóa Stiefel để tối ưu hóa nó trên đa tạp Stiefel. Tuy nhiên, tối ưu hóa trực tiếp W^basis₀ sẽ dẫn đến số lượng tham số có thể huấn luyện lớn như tinh chỉnh trọng số đầy đủ. Xem xét rằng cả tích ma trận và tích Kronecker của các ma trận trực giao cũng là trực giao, chúng tôi xây dựng một ma trận trực giao R = ∏ᵢ₌₁ʳ Rᵢ, trong đó Rᵢ là một ma trận trực giao kích thước nhỏ. Khi đó trọng số cập nhật của chúng tôi có thể được công thức hóa là:

W = (W^spec₀ ⊕ ΔS)W^basis₀R,                     (3)

trong đó các tham số có gạch chân là có thể huấn luyện. ΔS biểu thị phổ gia số, trong đó toán tử ⊕ biểu diễn việc cộng phổ gia số vào phổ trong ma trận thành phần phổ W^spec₀. Phổ gia số ΔS được cập nhật bằng bộ tối ưu hóa gradient descent, trong khi mỗi ma trận trực giao Rᵢ được cập nhật bằng bộ tối ưu hóa Stiefel, đảm bảo rằng ràng buộc trực giao được duy trì trong quá trình tối ưu hóa. Ở đây chúng tôi xem xét hai phương pháp phân tích, SVD và phân tích LQ/QR:

**Phân tích Giá trị Kỳ dị (SVD).** Nếu chúng tôi phân tích W₀ = U₀Σ₀V₀^T trong đó Σ₀ = diag(σ) là các giá trị kỳ dị và V₀^T là một ma trận trực giao, SVDiff [14] đề xuất tinh chỉnh các giá trị kỳ dị, hoặc tinh chỉnh các dịch chuyển phổ δ,

W = U₀Σ_δV₀^T với Σ_δ := diag(ReLU(σ + δ)).      (4)

Chúng tôi đề xuất tinh chỉnh các vector kỳ dị V cũng như vậy. Tuy nhiên, tinh chỉnh trực tiếp V sẽ không hiệu quả tham số. Do đó chúng tôi tận dụng tích Kronecker V_R := V₀R = V₀(∏ᵢ₌₁ʳ Rᵢ),

W^SODA-SVD = U₀Σ_δV_R^T với Σ_δ = diag(ReLU(σ + δ)), V_R := V₀(∏ᵢ₌₁ʳ Rᵢ).  (5)

**Phân tích LQ/QR (QR).** Chúng tôi có thể phân tích thay thế W₀ = L₀Q₀, trong đó L₀ là một ma trận tam giác dưới và Q₀ là một ma trận trực chuẩn. Tương tự như SVD, đường chéo của L₀ là các giá trị riêng của L₀ và chúng tôi đề xuất tinh chỉnh cả L₀ và Q₀,

W^SODA-QR = L_δQ_R với L_δ := L₀ + diag(δ), Q_R := Q₀(∏ᵢ₌₁ʳ Rᵢ).  (6)

### 3.4 Phân tích

**Số lượng tham số.** Một so sánh về số lượng tham số có thể điều chỉnh cho các cách tiếp cận khác nhau được hiển thị trong Bảng 1. Để đơn giản hóa ký hiệu, ở đây chúng tôi giả định m = n và các khối xoay nhỏ được chia đều (n/r cho OFT, và n^(1/r) cho SODA). Đối với KOFT và SODA, chúng tôi có thể quan sát rằng số lượng tham số giảm mạnh khi r tăng (chúng tôi sử dụng r = 3 trong các thí nghiệm của chúng tôi).

**Gradient của các giá trị kỳ dị.** Cho một ma trận W ∈ Rᵐˣⁿ và phân tích giá trị kỳ dị của nó W = UΣV^T. Nếu chúng tôi ký hiệu đạo hàm của loss l w.r.t. đầu ra h là δh = ∂l/∂h, thì gradient của W bằng δW = δhx^T, và

δΣ = U^T(δhx^T)V = (U^Tδh)(V^Tx)^T, và δΣᵢᵢ = ⟨uᵢ, δh⟩⟨vᵢ, x⟩,  (7)

trong đó ⟨·,·⟩ ký hiệu tích vô hướng, và uᵢ và vᵢ là cột thứ i của U và V, tương ứng. Như có thể thấy, gradient của các giá trị kỳ dị bao gồm hai phần: a) "từ trái sang phải", gradient δh được chiếu lên các cột của U; b) "từ phải sang trái", đầu vào x được chiếu lên các cột của V và chỉ thành phần thứ i ảnh hưởng đến σᵢ.

Nếu chúng tôi xem xét thay đổi của ma trận trọng số sau một bước nhỏ là ΔW, thì nếu chúng tôi chỉ tinh chỉnh các giá trị kỳ dị σ, chúng tôi có thể tính toán thay đổi hiệu quả của ma trận trọng số ΔW',

ΔW' = UΔΣ V^T và                                (8)
ΔΣ = (U^TΔWV) ⊙ Iᵐˣⁿ.                          (9)

Chúng tôi có thể quan sát rằng

||ΔW'||²_F = ||ΔΣ||²_F = ||(U^TΔWV) ⊙ Iᵐˣⁿ||²_F ≤ ||U^TΔWV||²_F = ||ΔW||²_F.  (10)

Dấu ≤ xuất phát từ thực tế rằng tích Hadamard ⊙ che giấu các phần tử không nằm trên đường chéo do đó thu nhỏ chuẩn Frobenius. Thực tế, vì chúng tôi sẽ che giấu hầu hết các phần tử, ||ΔW'||²_F có xu hướng nhỏ hơn nhiều so với ||ΔW||²_F và đây là lý do tại sao chúng tôi đặt tốc độ học lớn cho σ.

Bảng 1: So sánh Số lượng Tham số cho Các Phương pháp Khác nhau.

| Phương pháp | LoRA | OFT | KOFT | SODA |
|-------------|------|-----|------|------|
| Số lượng tham số | 2n·r | n²/r hoặc n²/r² (chia sẻ) | r·n^(2/r) | n + r·n^(2/r) |

## 4 Thí nghiệm

Chúng tôi sử dụng Stable Diffusion XL (SDXL) [31] như mô hình T2I khuếch tán đã được huấn luyện trước. Chúng tôi tiến hành thí nghiệm về cá nhân hóa chủ thể (Phần 4.1), cá nhân hóa phong cách (Phần 4.1), và các nghiên cứu ablation (Phần 4.3). Trong tất cả các thí nghiệm, chúng tôi huấn luyện các bộ mã hóa văn bản và UNet của mô hình SDXL bằng cách thay thế tất cả các mô-đun tuyến tính trong các lớp chú ý và chú ý chéo với PEFT tương ứng.

### 4.1 Cá nhân hóa Chủ thể

**Thiết lập thí nghiệm.** Đối với Cá nhân hóa Chủ thể, chúng tôi tinh chỉnh mô hình SDXL trên bộ dữ liệu DreamBooth [36] theo khung Direct Consistency Optimization (DCO) [24]. Để so sánh công bằng, chúng tôi điều chỉnh tốc độ học tốt nhất cho mỗi phương pháp và kiểm tra mỗi phương pháp với ba tốc độ học khác nhau. Điều này cho phép chúng tôi đánh giá hiệu suất của chúng một cách toàn diện. Thiết lập thí nghiệm chi tiết có thể được tìm thấy trong Phụ lục.

**Baseline.** Chúng tôi so sánh các phương pháp của mình với các baseline mạnh bao gồm LoRA [37] và OFT [32]. Để so sánh công bằng và giữ số lượng tham số xấp xỉ như nhau, chúng tôi đặt thứ hạng r = 1 cho LoRA và r = 3 cho KOFT và SODA, tức là R = R₁ ⊗ R₂ ⊗ R₃.

**Kết quả định lượng.** Chúng tôi báo cáo Độ tương tự Hình ảnh-Văn bản (↑, sử dụng SigLIP [45]) để đo lường độ trung thực và Độ tương tự Hình ảnh (↑, sử dụng DINOv2 [30]) để đo lường độ tin cậy hoặc bảo tồn danh tính. Thông tin chi tiết về các lời nhắc đánh giá và chỉ số có thể được tìm thấy trong Phụ lục. Chúng tôi vẽ đường cong Pareto bao gồm các điểm số với các tốc độ học khác nhau. Đường cong này minh họa sự đánh đổi giữa độ trung thực và độ tin cậy cho phương pháp được đánh giá. Phía trên bên phải của đường cong là lý tưởng, cho thấy rằng phương pháp có thể đạt được tạo ra thành phần đồng bộ với lời nhắc trong khi bảo tồn danh tính của chủ thể.

Hình 6 cho thấy so sánh LoRA (★), các phương pháp của chúng tôi, SODA-SVD (●) và SODA-QR (▲). Thú vị là, LoRA không thể đẩy biên giới lên phía trên bên phải, cho thấy rằng LoRA có xu hướng overfit với chủ thể và gặp khó khăn trong việc tạo ra hình ảnh đồng bộ với lời nhắc trong khi bảo tồn danh tính chủ thể. Điều này gợi ý rằng việc điều chỉnh cùng lúc độ lớn và hướng của trọng số đã được huấn luyện trước đã phân tích có thể sử dụng tốt hơn các prior mô hình khi thích ứng với các khái niệm mới mà không overfit. So với OFT (■), các phương pháp của chúng tôi SODA-SVD (●) và SODA-QR (▲) mô tả biên giới trên-phải trong cả độ tương tự hình ảnh-văn bản và độ tương tự hình ảnh, chứng minh hiệu quả của chúng. Điều này gợi ý rằng việc tích hợp tinh chỉnh phổ có thể nâng cao hiệu suất hơn nữa. Thú vị là, các phương pháp của chúng tôi, SODA-SVD (●) và SODA-QR (▲), trùng lặp với nhau, gợi ý rằng tinh chỉnh Σ có hiệu ứng rất tương tự với tinh chỉnh đường chéo của R.

**Kết quả định tính.** Trong Hình 3, chúng tôi cung cấp so sánh định tính giữa các cách tiếp cận của chúng tôi (SODA-SVD và SODA-QR) và các baseline (OFT và LoRA). Chúng tôi quan sát rằng các lời nhắc liên quan đến thay đổi nền (f) hoặc thay đổi phong cách (g) được xử lý tốt bởi tất cả các phương pháp. Những lời nhắc này có thể dễ dàng hơn vì chúng không yêu cầu hiểu biết sâu sắc về chủ thể; ngay cả việc overfit vẫn có thể tạo ra hình ảnh đồng bộ với lời nhắc. Tuy nhiên, đối với các lời nhắc yêu cầu thay đổi hình dạng (e) hoặc kết cấu (a-d), LoRA gặp khó khăn với việc tạo ra thành phần do overfit đối tượng. Các phương pháp của chúng tôi và OFT hoạt động tốt hơn đáng kể so với LoRA trên các lời nhắc yêu cầu thay đổi kết cấu hoặc hình dạng. Ví dụ, trong (a), (c), và (e), các phương pháp của chúng tôi chứng minh việc tạo ra thành phần vượt trội. Điều này gợi ý lợi ích của việc tận dụng phổ của các trọng số đã được huấn luyện trước, không chỉ điều chỉnh cơ sở một cách trực giao. Những quan sát này cũng được hỗ trợ bởi các kết quả định lượng được thảo luận ở trên. Thú vị là, với các đối tượng như chó (d), LoRA đôi khi thành công trong việc tạo ra hình ảnh thành phần tốt thay đổi kết cấu của chủ thể. Chúng tôi giả thuyết điều này là do các mô hình đã được huấn luyện trước có prior mạnh cho những đối tượng như vậy, làm cho LoRA dễ dàng hơn trong việc tìm các điểm tối ưu hóa biểu diễn đối tượng tốt mà không overfit.

### 4.2 Cá nhân hóa Phong cách

**Thiết lập thí nghiệm.** Đối với cá nhân hóa phong cách, chúng tôi thí nghiệm trên các hình ảnh phong cách từ bộ dữ liệu StyleDrop [40], chúng tôi tinh chỉnh tất cả các phương pháp peft trên 10 hình ảnh tham chiếu phong cách và tạo ra các hình ảnh thành phần cho phong cách tương ứng. Kết quả này được hiển thị trong Hình 4, Và chúng tôi tuân theo [40] để trộn mô hình chủ thể cá nhân hóa và mô hình phong cách để tạo ra hình ảnh của một chủ thể cá nhân hóa trong một phong cách cá nhân hóa. chúng tôi chọn ngẫu nhiên 10 chủ thể từ bộ dữ liệu Dreambooth [36] và trộn trọng số peft tinh chỉnh dư của chúng ΔW₁ với trọng số peft tinh chỉnh dư phong cách ΔW₂. Để trộn, chúng tôi sử dụng trộn số học (Merge) [40], tức là ΔW = ΔW₁ + ΔW₂, kết quả được hiển thị trong Hình 7. Các thiết lập thí nghiệm khác tuân theo Phần 4.1.

**Baseline.** Chúng tôi so sánh các phương pháp của mình với LoRA với thứ hạng r = 1, và SVDiff [14]. Đối với LoRA và SVDiff, chúng tôi có thể trực tiếp lấy trọng số dư đã trộn bằng cách trộn trọng số dư của mô hình cá nhân hóa chủ thể và mô hình phong cách. Đối với Các Phương pháp của chúng tôi, chúng tôi tính toán trọng số dư bằng cách trừ trọng số SVD hoặc QR đã tinh chỉnh khỏi trọng số đã được huấn luyện trước và trộn trọng số dư.

**Kết quả.** Hình 4 cho thấy so sánh kết quả giữa LoRA, SVDiff và Của chúng tôi (SODA-SVD và SODA-QR). Các phương pháp của chúng tôi có thể tạo ra hình ảnh đồng bộ với lời nhắc trong phong cách tham chiếu, trong khi SVDiff quan sát thấy over-fit với hình ảnh, và LoRA tạo ra artifacts. Hình 7 cho thấy kết quả của các hình ảnh được tạo ra bằng cách trộn mô hình chủ thể và mô hình phong cách. Các mô hình của chúng tôi có thể tạo ra hình ảnh phù hợp phong cách trong khi bảo tồn danh tính của chủ thể cá nhân hóa. trong khi SVDiff có xu hướng overfit với chủ thể và không bảo tồn tốt phong cách, và lora cũng overfit với chủ thể và tạo ra artifact. Ngoài ra, chúng tôi cho thấy việc tạo ra thành phần mới của chủ thể và phong cách kết hợp trong Hình 5

### 4.3 Nghiên cứu Ablation

Chúng tôi cũng tiến hành các nghiên cứu ablation về nhận thức phổ và tối ưu hóa trên Đa tạp Stiefel để xác thực thiết kế và lựa chọn của chúng tôi.

**Nhận thức Phổ.** a) Chúng tôi nghiên cứu hiệu ứng của tinh chỉnh phổ và tinh chỉnh trực giao. Chúng tôi chọn 10 chủ thể từ bộ dữ liệu DreamBooth [36] và so sánh phương pháp của chúng tôi (SODA-SVD), kết hợp tinh chỉnh phổ và tinh chỉnh trực giao, và chỉ tinh chỉnh phổ (SVD), chỉ tinh chỉnh trực giao (Kronecker orthogonal Adapter), và một phiên bản dư của SODA-SVD. Hình 8a cho thấy chỉ trực giao và chỉ phổ có hiệu suất tương tự nhau trong khi của chúng tôi hoạt động tốt hơn. b) Chúng tôi cũng nghiên cứu lựa chọn phương pháp tinh chỉnh phổ. Trong Hình 8b, chúng tôi so sánh hiệu suất của tinh chỉnh phổ sử dụng SVD và phân tích LQ/QR. Kết quả chứng minh rằng tinh chỉnh phổ SVD hoạt động tốt hơn một chút so với LQ/QR. Hình 9 trực quan hóa tác động của các lựa chọn ràng buộc đầu ra khác nhau (không ReLU, softplus, và ReLU) trên tinh chỉnh phổ SVD. Kết quả cho thấy rằng sử dụng ReLU đạt được hiệu suất tốt nhất.

**Tối ưu hóa trên đa tạp Stiefel.** Chúng tôi tiến hành ablation về các phương pháp tối ưu hóa khác nhau trên Đa tạp Stiefel. Chúng tôi so sánh OFT gốc, KOFT (OFT với tích Kronecker), KOFT-Cayley (OFT với tích Kronecker và tham số hóa Cayley), và OFT-Stiefel (OFT với bộ tối ưu hóa Stiefel). Từ Hình 8c, chúng tôi quan sát rằng bộ tối ưu hóa Stiefel [21] hoạt động tốt hơn các phương pháp khác khi sử dụng tốc độ học nhỏ, trong khi các phương pháp khác hoạt động tương tự nhau. Điều này chứng minh rằng bộ tối ưu hóa Stiefel có thể đạt được hiệu suất tương đương với tham số hóa Cayley, với tính linh hoạt bổ sung cho phép ma trận không vuông. Hơn nữa, bộ tối ưu hóa Stiefel thể hiện độ bền vững lớn hơn, vì nó đạt được hiệu suất tốt hơn với tốc độ học nhỏ hơn so với các phương pháp khác và đạt được hiệu suất tương đương với tốc độ học lớn hơn.

## 5 Kết luận và Thảo luận

Trong bài báo này, trước tiên chúng tôi xác định các hạn chế của các phương pháp PEFT trước đây, không được thiết kế để tận dụng đầy đủ kiến thức tiên nghiệm trong các trọng số đã được huấn luyện trước. Để giải quyết vấn đề này, chúng tôi đề xuất tinh chỉnh hiệu quả tham số nhận biết phổ, một cách tiếp cận mới tận dụng phổ của các tham số đã được huấn luyện trước. Cụ thể, chúng tôi giới thiệu Thích ứng Phân tích Trực giao Phổ (SODA), thực hiện cùng lúc tinh chỉnh phổ và trực giao. Các thí nghiệm về cá nhân hóa khuếch tán chứng minh rằng phương pháp của chúng tôi hoạt động tốt hơn các phương pháp PEFT trước đây. Hơn nữa, các nghiên cứu ablation xác thực hiệu quả của các thành phần riêng lẻ của cách tiếp cận SODA được đề xuất của chúng tôi.

**Hạn chế.** Phương pháp tinh chỉnh được đề xuất của chúng tôi chạy chậm hơn trong quá trình huấn luyện so với LoRA do bộ tối ưu hóa Stiefel. Nghiên cứu tương lai sẽ tập trung vào việc tăng tốc các thuật toán tối ưu hóa và áp dụng chúng cho các mô hình ngôn ngữ lớn.

**Lời cảm ơn.** Chúng tôi muốn cảm ơn Haizhou Shi về những thảo luận có giá trị.
