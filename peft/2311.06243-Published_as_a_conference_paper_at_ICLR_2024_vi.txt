# 2311.06243.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2311.06243.pdf
# Kích thước tệp: 11654715 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TINH CHỈNH TRỰC GIAO HIỆU QUẢ THAM SỐ THÔNG QUA
PHÂN TÍCH NHÂN TỬ BƯỚM
Weiyang Liu1,2,*Zeju Qiu1,*Yao Feng1,3,†Yuliang Xiu1,†Yuxuan Xue4,†Longhui Yu2,†Haiwen Feng1
Zhen Liu5Juyeon Heo2Songyou Peng1,3Yandong Wen1Michael J. Black1Adrian Weller2,6Bernhard Schölkopf1
1Viện Max Planck cho Hệ thống Thông minh - Tübingen2Đại học Cambridge3ETH Zürich
4Đại học Tübingen5Mila, Université de Montréal6Viện Alan Turing boft.wyliu.com
TÓM TẮT
Các mô hình nền tảng lớn đang trở nên phổ biến, nhưng việc huấn luyện chúng từ đầu có chi phí cực kỳ đắt đỏ. Do đó, việc thích ứng hiệu quả các mô hình mạnh mẽ này cho các nhiệm vụ hạ nguồn ngày càng trở nên quan trọng. Trong bài báo này, chúng tôi nghiên cứu một mô hình tinh chỉnh có nguyên tắc - Tinh chỉnh Trực giao (OFT) - để thích ứng nhiệm vụ hạ nguồn. Mặc dù thể hiện khả năng tổng quát hóa tốt, OFT vẫn sử dụng một số lượng khá lớn các tham số có thể huấn luyện do tính chiều cao của ma trận trực giao. Để giải quyết vấn đề này, chúng tôi bắt đầu bằng việc xem xét OFT từ góc độ truyền tải thông tin, và sau đó xác định một số tiêu chí quan trọng cho phép hiệu quả tham số tốt hơn. Lấy cảm hứng từ cách thuật toán biến đổi Fourier nhanh Cooley-Tukey cho phép truyền tải thông tin hiệu quả, chúng tôi đề xuất một tham số hóa trực giao hiệu quả sử dụng cấu trúc bướm. Chúng tôi áp dụng tham số hóa này vào OFT, tạo ra một phương pháp tinh chỉnh hiệu quả tham số mới, được gọi là Bướm Trực giao (BOFT). Bằng cách bao gồm OFT như một trường hợp đặc biệt, BOFT giới thiệu một khung tinh chỉnh trực giao tổng quát. Cuối cùng, chúng tôi tiến hành một nghiên cứu thực nghiệm rộng rãi về việc thích ứng các transformer thị giác lớn, mô hình ngôn ngữ lớn, và mô hình khuếch tán văn bản thành hình ảnh cho các nhiệm vụ hạ nguồn khác nhau trong thị giác và ngôn ngữ.

1 GIỚI THIỆU
Các mô hình gần đây như ChatGPT [4,9] và Stable Diffusion [73], thể hiện khả năng tổng quát hóa đáng chú ý của các mô hình nền tảng lớn. Sự gia tăng nhanh chóng trong hiệu suất của các mô hình như vậy đi kèm với sự gia tăng đáng kể về số lượng tham số (ví dụ, GPT-3 có khoảng 175B tham số). Kết quả là, việc huấn luyện một mô hình nền tảng từ đầu ngày càng trở nên thách thức đối với các nhà nghiên cứu. Do đó, tiến bộ rộng rãi trong lĩnh vực này đòi hỏi khả năng thích ứng các mô hình như vậy mà không cần huấn luyện lại từ đầu. Nghĩa là, chúng ta phải có thể thích ứng hiệu quả các mô hình nền tảng hiện có cho các nhiệm vụ hạ nguồn. Có chủ yếu ba cách để thực hiện thích ứng nhiệm vụ hiệu quả: tinh chỉnh mô hình [6,23,29,67,69,92,97], trong đó kiến trúc mô hình không thay đổi và một tập con các tham số mô hình được tinh chỉnh; điều chỉnh bộ thích ứng [24,28,48,65,71], trong đó các tham số có thể huấn luyện bổ sung được thêm vào mô hình gốc; và điều chỉnh prompt [39,42], trong đó các token tiền tố có thể huấn luyện bổ sung được gắn vào đầu vào. Trong số các phương pháp này, tinh chỉnh mô hình nổi bật như một phương pháp đơn giản nhưng mạnh mẽ, và quan trọng hơn, không gây ra độ trễ suy luận.

Nguyên tắc cơ bản đằng sau tinh chỉnh mô hình là đảm bảo rằng mô hình được huấn luyện trước và mô hình được tinh chỉnh tương tự nhau dựa trên các phép đo nhất định, sao cho kiến thức huấn luyện trước được bảo tồn. Ví dụ, các phương pháp tinh chỉnh mô hình hiện tại thường áp dụng tốc độ học nhỏ. Cách tiếp cận đặc biệt này khuyến khích sự khác biệt nhỏ giữa mô hình được huấn luyện trước và mô hình được tinh chỉnh. Với tính chất có cấu trúc của ma trận trọng số, một cách tiếp cận có nguyên tắc hơn cố gắng bảo tồn thông tin quan hệ của ma trận trọng số, tức là các góc cặp đôi giữa các neuron. Insight này dẫn đến một khung tinh chỉnh mô hình mới, được biết đến với tên Tinh chỉnh Trực giao (OFT) [67], trong đó các neuron trong cùng một lớp được biến đổi bởi cùng một ma trận trực giao sao cho các góc cặp đôi giữa các neuron được chứng minh là được bảo tồn trong suốt quá trình tinh chỉnh. Mặc dù OFT đã thể hiện khả năng tổng quát hóa và hội tụ đầy hứa hẹn để tinh chỉnh các mô hình khuếch tán văn bản thành hình ảnh [67], số lượng tham số có thể huấn luyện trong OFT có thể khá lớn do tính chiều cao của ma trận trực giao. Để giải quyết vấn đề này, OFT giới thiệu cấu trúc khối chéo để giảm số lượng tham số. Tuy nhiên, hiệu quả tham số cũng đi kèm với một cái giá - ma trận trực giao có một mẫu thưa thớt cố định và phép biến đổi trực giao được áp dụng độc lập trong các khối khác nhau. Mẫu thưa thớt khối chéo này, mặc dù tiết kiệm tham số, có thể gây ra một số bias quy nạp không mong muốn (ví dụ, ma trận trực giao khối chéo giảm khả năng biểu đạt và không thể xấp xỉ các phép biến đổi tuyến tính cổ điển), và quan trọng hơn, cách tìm một mẫu thưa thớt tốt vẫn là một bí ẩn.

Chìa khóa để giải quyết vấn đề này là tạo ra một ma trận trực giao dày đặc, trong khi vẫn hiệu quả về tham số. Mặc dù điều này có vẻ không khả thi ở cái nhìn đầu tiên vì một ma trận trực giao dày đặc d-chiều cần O(d²) tham số, chúng tôi chọn một hướng đi mới để tạo ra một ma trận trực giao dày đặc với nhiều ma trận thưa thớt được phân tích nhân tử. Cách tiếp cận này được hướng dẫn bởi insight rằng số lượng tham số có thể huấn luyện có thể được giảm bằng cách đánh đổi thời gian tính toán cho không gian. Vì chúng ta biểu diễn ma trận trực giao bằng phép nhân của các ma trận thưa thớt, phép nhân phải được thực hiện lặp đi lặp lại trong mỗi lần lặp huấn luyện. Để đặt phân tích nhân tử ma trận vào viễn cảnh, chúng tôi diễn giải việc tạo ra một ma trận trực giao dày đặc như một vấn đề truyền tải thông tin. Dưới công thức vấn đề này, tạo ra một ma trận trực giao dày đặc bằng cách nhân các ma trận thưa thớt có thể được hiểu như truyền tải thông tin trên một đồ thị có cấu trúc lưới. Khung truyền tải thông tin này cho phép chúng ta thiết kế nhiều cách có thể để thực hiện phân tích nhân tử ma trận thưa thớt hạn chế số lượng tham số có thể huấn luyện trong khi vẫn đủ biểu đạt để tạo ra các ma trận dày đặc.

Để đạt được hiệu quả tham số trong khung truyền tải thông tin của chúng tôi, chúng tôi lấy cảm hứng từ các cấu trúc bướm trong thuật toán biến đổi Fourier nhanh Cooley-Tukey [12] trong đó thông tin từ các nút khác nhau có thể được trao đổi hiệu quả [36]. Cụ thể, đồ thị bướm trong thuật toán Cooley-Tukey vốn có gây ra một cách để thực hiện phân tích nhân tử ma trận thưa thớt, được gọi là phân tích nhân tử bướm. Với phân tích nhân tử bướm, chúng ta có thể tạo ra một ma trận dày đặc d-chiều với một tích của O(log d) ma trận thưa thớt, mỗi ma trận có O(d) phần tử khác không. Bằng cách đảm bảo rằng mỗi ma trận thưa thớt là trực giao, chúng ta đi đến một tham số hóa trực giao hiệu quả chỉ với O(d log d) tham số, đây là một sự giảm đáng kể từ tham số hóa ban đầu. Bằng cách tận dụng tham số hóa trực giao hiệu quả như vậy, chúng tôi đề xuất một phương pháp tinh chỉnh hiệu quả tham số mới - Bướm Trực giao (BOFT). BOFT bao gồm OFT như một trường hợp đặc biệt và cung cấp một khung tinh chỉnh trực giao tổng quát. Có một đặc điểm chung cho cấu trúc khối chéo và cấu trúc bướm - tính thưa thớt. Cả hai cấu trúc đều giới thiệu tính thưa thớt dữ liệu vào ma trận trực giao để giảm số lượng tham số có thể huấn luyện hiệu quả. Thật thú vị khi đối chiếu cách tiếp cận của chúng ta với cấu trúc hạng thấp trong LoRA [29]; cả ma trận hạng thấp và ma trận thưa thớt đều là các họ chính của ma trận có cấu trúc [5] đạt được hiệu quả tham số.

So với cấu trúc khối chéo mà OFT sử dụng để đánh đổi giữa khả năng biểu đạt và tính đều đặn, BOFT sử dụng cấu trúc bướm để xây dựng một phép nội suy mượt mà hơn giữa các ma trận từ nhóm trực giao đầy đủ (khả năng biểu đạt) và ma trận đơn vị (tính đều đặn). Điều này cho phép chúng ta tìm một lớp giả thuyết nhỏ hơn trong nhóm trực giao cho các nhiệm vụ hạ nguồn. Với việc sử dụng rộng rãi cấu trúc bướm trong nhiều phép biến đổi tuyến tính nhanh, chẳng hạn như biến đổi Fourier rời rạc và biến đổi cosine rời rạc, chúng tôi lập luận rằng cách tiếp cận có cấu trúc của chúng ta đối với hiệu quả tham số sẽ giới thiệu một bias quy nạp ngầm có lợi cho khả năng tổng quát hóa và ngăn chặn hiện tượng quá khớp. Trực giác của chúng tôi đến từ tính chất mà cấu trúc bướm có thể dễ dàng khôi phục nhiều phép biến đổi tuyến tính cổ điển trong khi cấu trúc khối chéo trong OFT không thể khôi phục bất kỳ phép biến đổi nào. Những đóng góp của chúng tôi được liệt kê dưới đây:

• Chúng tôi nghiên cứu vấn đề hiệu quả tham số trong tinh chỉnh trực giao với một khung truyền tải thông tin mới. Khung này biến đổi nhiệm vụ tạo ra một ma trận trực giao dày đặc hiệu quả tham số thành một vấn đề truyền tải thông tin trong một đồ thị có cấu trúc lưới.

• Lấy cảm hứng từ cấu trúc bướm trong thuật toán Cooley-Tukey, chúng tôi đề xuất Bướm Trực giao, một phương pháp tinh chỉnh trực giao hiệu quả tham số, trong khung truyền tải thông tin.

• Chúng tôi cung cấp một số insight lý thuyết về lý do tại sao BOFT có thể giảm đáng kể số lượng tham số có thể huấn luyện trong khi vẫn mang lại khả năng biểu đạt tốt và tính ổn định huấn luyện. Nhờ phân tích nhân tử ma trận, BOFT cũng đi kèm với một phép nội suy trọng số hấp dẫn (xem Hình 10).

• Lần đầu tiên, chúng tôi áp dụng tinh chỉnh trực giao cho các nhiệm vụ khác nhau ngoài việc tạo văn bản thành hình ảnh có thể kiểm soát [67], cho thấy tiềm năng to lớn của nó như một phương pháp tinh chỉnh mô hình chung. Để làm được điều này, chúng tôi áp dụng BOFT cho một số ứng dụng thích ứng từ thị giác máy tính đến xử lý ngôn ngữ tự nhiên. BOFT vượt trội hơn các phương pháp tiên tiến hiện tại với một biên độ đáng kể, xác nhận hiệu quả tham số vượt trội và khả năng tổng quát hóa của nó.

2 CÔNG TRÌNH LIÊN QUAN

Tinh chỉnh hiệu quả tham số (PEFT). Khi các mô hình nền tảng (ví dụ, [4,35,68,73]) trở nên ngày càng lớn và mạnh mẽ, việc tinh chỉnh các mô hình này cho các nhiệm vụ hạ nguồn theo cách hiệu quả tham số đã khơi dậy sự quan tâm đáng kể [18,45,82]. Trong số nhiều phương pháp PEFT [1,3,8,10,19,21,23,24,28–31,33,39,42,46,49,54,56,78,79,86,88,92,98], các phương pháp dựa trên tham số hóa lại [1,19,29] có liên quan nhất đến công trình của chúng tôi. LoRA [29] cập nhật ma trận trọng số được huấn luyện trước bằng cách thêm một tích của hai ma trận hạng thấp, đạt được hiệu suất đầy hứa hẹn trên các nhiệm vụ ngôn ngữ tự nhiên. Vì hạng của tất cả các ma trận được thêm vào được đặt thành một hằng số trong LoRA, một số phương pháp [84,95,97] điều chỉnh động hạng cho các lớp khác nhau sao cho ngân sách tham số được phân bổ đầy đủ. Do tính đơn giản của nó, tham số hóa lại trọng số hạng thấp như vậy đã trở nên rất phổ biến [6,16,102]. Lấy cảm hứng từ cách năng lượng siêu cầu đặc trưng cho tính tổng quát hóa [50,52], [67] đề xuất tinh chỉnh trực giao, một phương pháp thay thế nhưng hiệu quả để tinh chỉnh các mô hình khuếch tán văn bản thành hình ảnh. Cụ thể, OFT học một ma trận trực giao để biến đổi các neuron của cùng một lớp, và nó đạt được khả năng tổng quát hóa mạnh hơn và huấn luyện ổn định hơn một cách nhất quán so với LoRA. Mặc dù có hiệu suất mạnh, OFT nói chung có nhiều tham số có thể huấn luyện hơn LoRA. Do đó, việc làm cho OFT hiệu quả tham số hơn là một mục tiêu hữu ích. Hơn nữa, liệu OFT có áp dụng được cho một phổ rộng hơn các nhiệm vụ thích ứng (ngoài việc kiểm soát các mô hình khuếch tán văn bản thành hình ảnh [67]) hay không vẫn chưa được biết. BOFT cải thiện hiệu quả tham số của OFT thông qua phân tích nhân tử bướm. Nhờ điều này, chúng ta hiện có thể chứng minh sức mạnh của tinh chỉnh trực giao trong các nhiệm vụ thích ứng chung.

Cấu trúc bướm. Thuật toán Cooley-Tukey radix-2 đệ quy giảm biến đổi Fourier rời rạc N-điểm thành hai biến đổi Fourier rời rạc N/2-điểm, và quá trình này gây ra một cấu trúc bướm có thể được viết như một tích của nhiều ma trận thưa thớt (tích này cũng được gọi là ma trận bướm). Ma trận bướm đã được sử dụng để tham số hóa ma trận trực giao để tránh xoay trong phép khử Gauss và cải thiện hiệu quả [63], để ổn định việc huấn luyện các mạng neural tái phát [32] và trong xấp xỉ kernel [60]. [13,14] học các phép biến đổi tuyến tính nhanh với tham số hóa bướm. [7,15] sử dụng ma trận bướm để cho phép huấn luyện hiệu quả các mạng neural. Cấu trúc bướm cũng được tìm thấy hữu ích trong phép nhân ma trận-vector nhanh [57,61], xấp xỉ ma trận thưa dữ liệu [43], và truyền mạng [36,40,70]. Trái ngược với công trình trước đó, chúng tôi tập trung vào việc khai thác cấu trúc bướm để nâng cao hiệu quả tham số của OFT.

3 MỘT QUAN ĐIỂM TRUYỀN TẢI THÔNG TIN VỀ TINH CHỈNH TRỰC GIAO

Ma trận Trọng số
Được huấn luyện trước
W d×n
dx +Ma trận Trọng số
Được huấn luyện trước
W n×d ... Ma trận Trực giao
R b×b r
d×d
Ma trận Hạng thấp
AB
(a) Cấu trúc Hạng thấp trong LoRA (b) Cấu trúc Trực giao trong OFTAB
0 0

Hình 1: So sánh tham số hóa lại giữa LoRA và OFT. Chúng tôi bắt đầu với một số kiến thức cơ bản về OFT. Để tinh chỉnh ma trận trọng số được huấn luyện trước, OFT tham số hóa lại ma trận trọng số mới như tích của một ma trận trực giao có thể học và ma trận trọng số được huấn luyện trước đã đóng băng. So với LoRA cập nhật trọng số bằng ma trận hạng thấp cộng thêm, OFT sử dụng ma trận trực giao nhân để cập nhật trọng số. Để đạt hiệu quả tham số, LoRA sử dụng cấu trúc hạng thấp, và ngược lại, OFT ban đầu sử dụng cấu trúc trực giao khối chéo [67] (càng nhỏ kích thước của các khối chéo, OFT càng hiệu quả tham số). Một so sánh trực quan được đưa ra trong Hình 1.

Động lực để áp dụng phép biến đổi trực giao để tinh chỉnh ma trận trọng số là để bảo tồn các góc cặp đôi của neuron [50,52,67], sao cho kiến thức ngữ nghĩa từ huấn luyện trước có thể được bảo tồn phần lớn. Cụ thể, OFT tối ưu hóa một ma trận trực giao R∈Rd×d cho một lớp tuyến tính được huấn luyện trước W0∈Rd×n, và sửa đổi forward pass từ z=(W0)⊤x thành z=(RW0)⊤x, trong đó x∈Rd và z∈Rn là vector đầu vào và đầu ra, tương ứng. Để đạt được khởi tạo zero, OFT khởi tạo R như một ma trận đơn vị. Để đảm bảo tính trực giao của R trong suốt quá trình tinh chỉnh, chúng ta theo [52,67] để sử dụng tham số hóa Cayley, tức là, R=(I+Q)(I-Q)^(-1) trong đó Q là ma trận skew-symmetric với Q=-Q⊤. Để hiệu quả tham số, cấu trúc khối chéo được giới thiệu bằng cách tham số hóa ma trận trực giao R như diag(R1,R2,···,Rr) trong đó Ri∈Rb×b,∀i là ma trận trực giao nhỏ và br=d. Hiệu quả tham số mang lại bởi cấu trúc khối chéo đi kèm với một cái giá - nó giới thiệu một giả định rằng các chiều của một neuron (tức là, một vector cột của ma trận trọng số W0) được chia thành r nhóm và các chiều trong các nhóm khác nhau được biến đổi riêng biệt sử dụng các ma trận trực giao khác nhau. Mặc dù hiệu quả thực nghiệm của cấu trúc khối chéo, việc chia các chiều của một neuron thành r nhóm dựa trên chỉ số của chúng không có ý nghĩa gì, điều này làm cho ma trận trực giao dày đặc trở nên mong muốn. Một câu hỏi tự nhiên phát sinh: Chúng ta có thể xây dựng một ma trận trực giao dày đặc mà không mất hiệu quả tham số không?

Để giải quyết câu hỏi này, chúng tôi đề xuất tạo ra một ma trận trực giao dày đặc với một tích của nhiều ma trận trực giao thưa thớt. Để cung cấp một góc nhìn thống nhất nhưng trực quan để nghiên cứu mẫu thưa thớt

4

của phân tích nhân tử ma trận trực giao, chúng tôi đóng khung vấn đề tạo ra một ma trận trực giao dày đặc trong OFT như một vấn đề truyền tải thông tin. Cụ thể, tạo ra một ma trận dày đặc R∈Rd×d bằng một tích của m ma trận vuông R=BmBm-1···B1 có thể được xem như truyền tải thông tin trong một lưới với d×(m+1) nút, như minh họa trong Hình 2. Động lực đằng sau quan điểm truyền tải thông tin đến từ quan sát rằng một ma trận vuông dày đặc d-chiều có thể được diễn giải như một kết nối dày đặc từ d nút đến d nút khác. Đối với ma trận R, nếu phần tử Rij bằng không, thì nó chỉ ra rằng thông tin từ nút thứ j không thể truyền đến nút thứ i. Nếu Rij khác không, thì thông tin có thể được truyền tải. Do đó, biểu diễn ma trận dày đặc R với nhiều ma trận BmBm-1···B1 cũng có thể được diễn giải như thực hiện trao đổi thông tin tuần tự dựa trên đồ thị được gây ra bởi Bi,∀i. Thông tin truyền theo B1 trước và Bn cuối cùng. Như một ví dụ cụ thể trong Hình 2, chúng ta xem xét phân tích nhân tử R=B5B4B3B2B1 có mẫu thưa thớt và đồ thị được gây ra được trực quan hóa. Đồ thị trong Hình 2 là kết quả của việc mở rộng phép nhân ma trận. Trong đồ thị được gây ra, ma trận Bi được xem như ma trận kết nối từ các nút mức thứ i đến các nút mức thứ (i+1). Cụ thể hơn, phần tử (j1, j2) của Bi biểu thị liệu có cạnh có hướng từ nút thứ j2 trong mức thứ i đến nút thứ j1 trong mức thứ (i+1) hay không (không có nghĩa là không có cạnh). Để B5B4B3B2B1 là ma trận dày đặc, mọi nút trong mức đầu tiên phải có thể truyền thông tin đến tất cả các nút trong mức thứ 6. Nếu chúng ta chỉ xem xét R=B4B3B2B1 tương ứng với các nút nguồn trong mức đầu tiên và các nút nhận trong mức thứ 5, thì chúng ta tìm thấy rằng thông tin từ nút 1 không thể được truyền đến nút 3. Do đó, mẫu thưa thớt của B4B3B2B1 có một phần tử bằng không ở vị trí (3,1). Xem xét R=B3B2B1 và R=B2B1, sự tương ứng tương tự tồn tại giữa đồ thị được gây ra và mẫu thưa thớt. Nói chung, để một ma trận R∈Rd×d là dày đặc, m ma trận phân tích nhân tử Bm,···,B1 cần tương ứng với một tập hợp các cạnh có hướng trên lưới d×(m+1) trong đó một cạnh có hướng chỉ có thể kết nối hai nút giữa các mức liền kề (tức là, các cột), sao cho thông tin từ mọi nút trong mức đầu tiên có thể được truyền đến mọi nút trong mức thứ (m+1).

1 2 3 4
1 2 3 4

Hình 3: Một ví dụ về cấu trúc khối chéo trong OFT. Quan điểm truyền tải thông tin có thể giúp chúng ta hiểu rõ hơn về mẫu thưa thớt của ma trận phân tích nhân tử trong OFT. Hình 3 trực quan hóa cấu trúc khối chéo của R trong OFT ban đầu. Mặc dù giảm số lượng tham số có thể huấn luyện, cấu trúc khối chéo không thể xây dựng một ma trận dày đặc R. Mục tiêu của chúng ta là tạo ra một ma trận trực giao dày đặc với m ma trận trực giao thưa thớt, sử dụng càng ít tham số có thể huấn luyện hiệu quả càng tốt. Dưới quan điểm truyền tải thông tin, các tiêu chí chung hướng đến mục tiêu của chúng ta là (i) kết nối dày đặc: mọi nút trong mức đầu tiên có ít nhất một đường dẫn đến mọi nút trong mức cuối cùng, và (ii) cạnh tự do tối thiểu: tổng số cạnh phải càng nhỏ càng tốt dưới ràng buộc trực giao. Chúng ta lưu ý rằng tính trực giao tiêm một ràng buộc tinh tế vào các cạnh giữa các mức liền kề. Ví dụ, để mỗi ma trận Bi có hạng đầy đủ (một điều kiện cần thiết của tính trực giao), chúng ta cần có d cạnh để tạo thành một song ánh giữa tất cả các nút trong mức thứ i và tất cả các nút trong mức thứ (i+1), điều này làm cho số lượng cạnh giữa các mức liền kề ít nhất là d (ví dụ, 4 cho ví dụ trong Hình 2). Những d cạnh này là cần thiết cho tính trực giao và không nên được tính vào số lượng cạnh, bởi vì những phần tử này không thể huấn luyện (ví dụ, đối với một ma trận trực giao d×d với d phần tử khác không, những phần tử này chỉ có thể là ±1). Bởi vì ma trận trực giao yêu cầu ít tham số hơn ma trận đầy đủ, ràng buộc trực giao sẽ mang lại phụ thuộc bổ sung giữa các cạnh. Như một ví dụ, đối với ma trận trực giao 2×2, một số không ở vị trí (1,1) sẽ ngụ ý một số không khác ở vị trí (2,2) (tức là, một cạnh thiếu có thể ngụ ý một cạnh thiếu khác). Do đó, đối với mỗi tập hợp các kết nối cạnh khả thi, tính trực giao đôi khi có thể thêm hoặc loại bỏ một số cạnh. Bằng cách trực quan hóa mẫu khác không của ma trận trực giao được tạo thành, khung truyền tải thông tin đặc biệt hữu ích trong OFT, bởi vì chúng ta chỉ quan tâm đến các phần tử khác không có thể huấn luyện của R và các giá trị cụ thể của chúng không quan trọng.

Một kết nối dày đặc ngây thơ giữa hai mức cần O(d²) cạnh (tức là, một ma trận trực giao dày đặc duy nhất), mang lại d²-d cạnh (với d=4, nó là 12 cạnh). Hình 2 đưa ra một ví dụ về phân tích nhân tử ma trận khả thi và nó cần 10 cạnh tổng cộng, thực tế ít hơn một ma trận trực giao dày đặc duy nhất. Khung này cho phép chúng ta nghiên cứu hiệu quả tham số của OFT từ góc độ đồ thị, và chúng ta có thể dễ dàng đưa ra các phân tích nhân tử khả thi với khung này. Chúng tôi lấy cảm hứng từ một cấu trúc tô-pô thú vị từ thuật toán Cooley-Tukey, được gọi là đồ thị bướm [12], có thể kết nối dày đặc d nút nguồn và d nút nhận một cách hiệu quả với O(d log d) cạnh. Ví dụ, cấu trúc tô-pô trong Hình 2 cần 10 cạnh để đạt kết nối dày đặc, trong khi mạng bướm chỉ cần 8 cạnh. Tiếp theo, chúng tôi giới thiệu cách cấu trúc bướm có thể cải thiện hiệu quả tham số.

4 THAM SỐ HÓA TRỰC GIAO BẰNG PHÂN TÍCH NHÂN TỬ BƯỚM

Mức 1 Mức 2 Mức 3 Mức 4
B(8,8)1
2
3
41
2
3
41
2
3
41
2
3
4
1
2
3
41
2
3
41
2
3
41
2
3
4
~B(8,4)~B(8,2)~

Hình 4: Cấu trúc bướm (d=8). Cấu trúc bướm ban đầu được sử dụng trong thuật toán Cooley-Tukey để thực hiện biến đổi Fourier nhanh. Trong biến đổi Fourier, một thay đổi cục bộ trong miền tần số có thể gây ra thay đổi toàn cục trong miền không gian, về mặt khái niệm tương tự với vấn đề truyền tải thông tin của chúng ta - mọi nút trong mức đầu tiên có thể truyền thông tin đến tất cả các nút trong mức cuối cùng. Cấu trúc bướm cũng trở thành một cấu trúc tô-pô mạng máy tính phổ biến [41,75] được sử dụng để trao đổi thông tin hiệu quả. Giả sử rằng k≥2 là lũy thừa của 2, chúng ta bắt đầu bằng cách định nghĩa yếu tố bướm BF(k) như

BF(k) = 
diag(d1) diag(d2)
diag(d3) diag(d4)
 ∈Rk×k, (1)

trong đó di∈Rk/2,∀i là một số vector. Với d=2N, chúng ta sau đó định nghĩa ma trận bướm d-chiều B(d)∈Rd×d một cách đệ quy như

B(d) = ~B(d,d)·
B1(d/2) 0
0 B2(d/2)

= ~B(d,d)~B(d,d/2)···~B(d,2), (2)

trong đó B1(d/2) và B2(d/2) là hai ma trận bướm d/2-chiều. Chúng ta sau đó định nghĩa thành phần bướm như ~B(d,k)=diag(BF1(k),···,BFd/k(k)) là ma trận khối chéo có kích thước d×d với kích thước khối k, trong đó BFi(k),∀i là các yếu tố bướm được định nghĩa trong Phương trình 1. Bây giờ chúng ta đã sẵn sàng sử dụng ma trận bướm để tham số hóa một ma trận trực giao. Để đạt được điều này, chúng ta chỉ cần đảm bảo rằng tất cả các yếu tố nhân ~B(d,k),∀k trong ma trận bướm B(d) là trực giao. Chúng ta đầu tiên xem xét ma trận khối chéo ~B(d,2) với kích thước khối 2, và chúng ta có thể dễ dàng đảm bảo ~B(d,2) là trực giao với phép biến đổi Cayley (hoặc phép quay 2-chiều) để tham số hóa mỗi khối, giống như [52,67]. Mẫu khác không của mọi thành phần bướm có thể được xem như một hoán vị của mẫu khác không của ~B(d,2), vì vậy tất cả các thành phần bướm có thể được tham số hóa dễ dàng như ma trận trực giao. Điều này cho chúng ta một tham số hóa hiệu quả của ma trận trực giao được xây dựng dựa trên nhiều ma trận trực giao 2×2. Chúng ta tổng quát hóa ma trận bướm theo [7], và định nghĩa một thành phần bướm khối ~Bb(d,k) trong đó mỗi phần tử trong di,∀i trở thành một ma trận b×b. Để đảm bảo thành phần bướm khối ~Bb(d,2) là trực giao, chúng ta tham số hóa mỗi ma trận khối 2b×2b là trực giao. Mẫu khác không của các thành phần bướm khác ~Bb(d,k), k>2 là hoán vị theo khối của mẫu khác không của ~Bb(d,2) và do đó có thể được biến thành ma trận trực giao tương tự. Kết hợp các phần, forward pass trong BOFT là

z = R(m,b)·W0⊤x, s.t.
R(m,b) = ∏(i=1 to m) ~Bb(d,i) & ~Bb(d,j)⊤~Bb(d,j) = ~Bb(d,j)~Bb(d,j)⊤ = Id| {z }
∀j∈[1,m]
,

trong đó chúng ta ký hiệu ~Bb(d,2m-i+1) như ~Bb(d,i) để đơn giản, và Id là ma trận đơn vị có kích thước d. Ma trận trực giao R(m,b)∈Rd×d được tạo thành từ một tích của nhiều thành phần bướm trực giao. Để thuận tiện, chúng ta ký hiệu BOFT với R(m,b/2) như BOFT(m,b), trong đó b≥2. Khi m=1, thì BOFT(1,b) giảm xuống OFT khối chéo [67] với kích thước khối b. BOFT(1,d) giảm xuống OFT ban đầu [67] với ma trận trực giao đầy đủ không ràng buộc. BOFT(log2d/b,b) sử dụng ma trận bướm khối Bb/2(d) như R, và mang lại một ma trận trực giao dày đặc R. Nói chung, BOFT(m,b) cần 1/2(b-1)dm tham số có thể huấn luyện hiệu quả để tinh chỉnh một lớp tuyến tính có kích thước d×n. Nếu chúng ta sử dụng ma trận bướm, tức là, m=log d, b=2, BOFT sử dụng O(d log d) tham số. Ngược lại, OFT ban đầu với ma trận trực giao dày đặc đầy đủ sử dụng O(d²) tham số, và OFT khối chéo với số khối r sử dụng O(bd). Do đó, OFT ban đầu phải sử dụng kích thước khối b=d để tạo ra một ma trận trực giao dày đặc, trong khi BOFT có thể sử dụng bất kỳ b nào để đạt được điều này.

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Khởi tạo đơn vị cho BOFT. Các phương pháp tinh chỉnh thường bắt đầu với mô hình được huấn luyện trước chính xác sao cho mô hình được tinh chỉnh sẽ không lệch quá nhiều so với mô hình được huấn luyện trước. Ví dụ, LoRA sử dụng khởi tạo zero cho các trọng số hạng thấp. Trong BOFT, chúng tôi khởi tạo tất cả các thành phần bướm với ma trận đơn vị (tức là, ma trận skew-symmetric được khởi tạo như zero trong phép biến đổi Cayley).

Dropout Nhân cho BOFT. LoRA [29] tiếp tục triển khai một lớp Dropout cho việc cập nhật trọng số hạng thấp để ngăn chặn hiện tượng quá khớp. Dropout thông thường [77] hoạt động tự nhiên cho LoRA, nhưng không cho BOFT do việc cập nhật trọng số nhân của chúng ta. Để giải quyết điều này, chúng tôi đề xuất một Dropout Nhân cho BOFT. Bởi vì ma trận trực giao R(m,b) được tạo thành từ m thành phần bướm trực giao có thể được hoán vị dễ dàng thành ma trận trực giao khối chéo 2b×2b. Dropout Nhân đầu tiên chọn ngẫu nhiên p1 phần trăm của các thành phần bướm và p2 phần trăm của các khối chéo trong mỗi thành phần bướm, và sau đó thay thế những khối này như ma trận đơn vị.

5 NHỮNG INSIGHT VÀ THẢO LUẬN HẤP DẪN

Khả năng biểu đạt của BOFT. Cấu trúc bướm cùng với hoán vị có thể hoàn hảo khôi phục nhiều phép biến đổi tuyến tính nhanh cổ điển [13,14] (ví dụ, biến đổi Fourier nhanh, biến đổi Hadamard), nhưng việc ma trận bướm trực giao của chúng ta có thể xấp xỉ một ma trận trực giao tổng quát tốt như thế nào vẫn chưa được biết. Chúng tôi bắt đầu bằng việc tiến hành một mô phỏng để xấp xỉ một ma trận trực giao dày đặc ngẫu nhiên [2] với kích thước 512×512. Kết quả trong Hình 5 được lấy trung bình trên 10 seed ngẫu nhiên. Trục y biểu thị lỗi xấp xỉ, và trục x biểu thị số lượng tham số có thể huấn luyện hiệu quả. Mỗi đường cong với cùng màu biểu thị BOFT với cùng kích thước khối, và điểm ngoài cùng bên trái là lỗi của BOFT(1,b) (tức là, OFT khối chéo ban đầu với kích thước khối b). BOFT nói chung mang lại hiệu quả tham số tốt hơn OFT. Ví dụ, khả năng biểu đạt của BOFT(9,2) tốt hơn BOFT(1,16) nhưng có ít tham số hơn nhiều. BOFT với b nhỏ hơn và m lớn hơn nói chung hiệu quả tham số hơn. Ví dụ, BOFT(6,4) sử dụng ít tham số hơn nhiều nhưng mang lại lỗi xấp xỉ tương tự với BOFT(2,16). Nói chung, ma trận bướm đại diện cho một tập con có cấu trúc hơn của nhóm trực giao (so với cấu trúc khối chéo), điều này làm cho BOFT có thể chứng minh được là biểu đạt hơn OFT với cùng kích thước khối.

Định lý 1 (Khả năng biểu đạt của BOFT). BOFT biểu đạt hơn OFT với cùng kích thước khối. Để ma trận bướm xấp xỉ tất cả ma trận trực giao có kích thước d, chúng ta có thể nhân ma trận bướm với Bd-1,1(d)B⊤d-1,2(d)···B1,1(d)B⊤1,2(d), trong đó Bi,j(d),∀i,∀j là ma trận bướm.

Định lý 1 gợi ý một tổng quát hóa đơn giản cho BOFT - ma trận trực giao cuối cùng được tổng quát hóa thành RG(m1,b1,m2,b2,l)=Rl,1(m1,b1)R⊤l,2(m2,b2)···R1,1(m1,b1)R⊤1,2(m2,b2) trong đó R⊤i,j(m,b) biểu thị ma trận trực giao được sử dụng trong BOFT. Khi m1=m2=log d, b1=b2=2 và l=d-1, thì RG(m1,b1,m2,b2,l) có thể đại diện cho toàn bộ nhóm trực giao. Tổ hợp ma trận như vậy cũng được gọi là hệ thống kaleidoscope [14]. Tuy nhiên, chúng tôi lưu ý rằng khả năng biểu đạt tốt hơn không luôn dẫn đến hiệu suất tốt hơn trong tinh chỉnh, vì tinh chỉnh đầy đủ, mặc dù có khả năng biểu đạt toàn cầu, thường mang lại hiệu suất không thỏa đáng. Sự đánh đổi giữa khả năng biểu đạt và tính đều đặn là chìa khóa cho khả năng tổng quát hóa của tinh chỉnh mô hình. BOFT mở rộng không gian tham số tinh chỉnh với các tiên nghiệm cấu trúc, điều này cho phép chúng ta tìm một sự đánh đổi tốt hơn giữa khả năng biểu đạt và tính đều đặn.

Tính chất phổ. Tinh chỉnh trực giao nói chung mang lại tính chất phổ tốt hơn LoRA, bởi vì nó hoàn hảo bảo tồn chuẩn phổ của ma trận trọng số được huấn luyện trước W0. Chúng ta có thể thấy điều này bằng phân rã giá trị kỳ dị: W0=UΣV⊤ trong đó U,V là ma trận trực giao và Σ là ma trận chéo giá trị kỳ dị. Cả OFT và BOFT đều nhân một ma trận trực giao R vào bên trái và thu được trọng số được tinh chỉnh RUΣV⊤, điều này không ảnh hưởng đến giá trị kỳ dị lớn nhất (tức là, chuẩn phổ của W0). Việc bảo tồn như vậy đã được chứng minh là có lợi rất lớn cho tính ổn định huấn luyện và tổng quát hóa [58, 90]. Chúng tôi giới thiệu thêm các tính chất toán học thú vị trong Phụ lục G.

Tinh chỉnh trực giao như học tương tự song tuyến tính. BOFT có thể được viết như học tương tự song tuyến tính w0iRx trong đó w0i là neuron thứ i (tức là, vector cột) của ma trận trọng số W0. BOFT có thể được xem như học ma trận tương tự song tuyến tính R với tính đều đặn mạnh (tức là, R cần là trực giao), điều này kết nối nội tại với học metric khoảng cách [89] và dạng song tuyến tính [72].

Bias quy nạp và tổng quát hóa trong BOFT. Vì R(m,b) trong BOFT thường đại diện cho một tập con có cấu trúc của nhóm trực giao ràng buộc lớp giả thuyết, BOFT sẽ tự nhiên gây ra một bias quy nạp. Chúng tôi lập luận rằng bias quy nạp có cấu trúc được gây ra bởi phân tích nhân tử bướm có lợi cho tổng quát hóa, vì nó có một mẫu cấu trúc chung của nhiều phép biến đổi tuyến tính cổ điển [14], chẳng hạn như biến đổi Fourier rời rạc, biến đổi sine/cosine rời rạc và biến đổi Hadamard. Hơn nữa, phân tích nhân tử ma trận thưa thớt trong BOFT cũng có thể mang lại một số bias quy nạp ngầm [22, 44, 51].

So sánh với huấn luyện thưa thớt dựa trên bướm. Có khá nhiều công trình [7,13–15] nghiên cứu huấn luyện thưa thớt với tham số hóa bướm. Chúng thường tập trung vào tham số hóa lại ma trận trọng số trực tiếp với tham số hóa bướm và huấn luyện mạng neural từ đầu. [15] xem xét tinh chỉnh trọng số được huấn luyện trước bằng cách đầu tiên chiếu trọng số lên một biến thể của ma trận bướm và sau đó tối ưu hóa các thành phần được chiếu cho các nhiệm vụ hạ nguồn. BOFT đề xuất một chiến lược tinh chỉnh rất khác biệt biến đổi trọng số với ma trận trọng số được chia sẻ lớp.

6 ỨNG DỤNG VÀ KẾT QUẢ THỰC NGHIỆM

Chúng tôi áp dụng BOFT để tinh chỉnh mô hình ngôn ngữ lớn (DeBERTaV3 [25], Llama-2 [81]), mô hình nền tảng thị giác (DINOv2 [62], SAM [35]), và mô hình tạo sinh văn bản thành hình ảnh (Stable Diffusion [73]) trên các nhiệm vụ hạ nguồn khác nhau. Để đảm bảo so sánh công bằng, chúng tôi sử dụng chính xác cùng cài đặt cho tất cả các baseline được so sánh. Kết quả được lấy trung bình trên 5 seed ngẫu nhiên, và các cải thiện đã vượt qua các kiểm tra có ý nghĩa với p<0.05. Chi tiết thực nghiệm và thêm kết quả được cung cấp trong phụ lục.

6.1 THÍCH ỨNG CỦA MÔ HÌNH NGÔN NGỮ LỚN (LLMS)

Phương pháp # Param MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B Tất cả
Tinh chỉnh đầy đủ 184M 89.90 95.63 69.19 92.40 94.03 83.75 89.46 91.60 88.25
BitFit [92] 0.1M 89.37 94.84 66.96 88.41 92.24 78.70 87.75 91.35 86.20
H-Adapter [28] 1.22M 90.13 95.53 68.64 91.91 94.11 84.48 89.95 91.48 88.28
P-Adapter [65] 1.18M 90.33 95.61 68.77 92.04 94.29 85.20 89.46 91.54 88.41
LoRA r=8[29] 1.33M 90.65 94.95 69.82 91.99 93.87 85.20 89.95 91.60 88.50
AdaLoRA [97] 1.27M 90.76 96.10 71.45 92.23 94.55 88.09 90.69 91.84 89.46
OFT b=16 0.79M 90.33 96.33 73.91 92.10 94.07 87.36 92.16 91.91 89.77
BOFTm=2 b=8 0.75M 90.25 96.44 72.95 92.10 94.23 88.81 92.40 91.92 89.89

Bảng 1: Kết quả trên tập phát triển GLUE. Chúng tôi báo cáo độ chính xác khớp cho MNLI, tương quan Matthew cho CoLA, tương quan trung bình cho STS-B và độ chính xác cho các nhiệm vụ khác.

Hiểu ngôn ngữ tự nhiên. Để đánh giá hiệu suất của BOFT trên thích ứng LLM, chúng tôi đầu tiên tinh chỉnh mô hình DeBERTaV3-base được huấn luyện trước [25] trên benchmark GLUE [87], bao gồm một số nhiệm vụ phân loại câu đại diện và được sử dụng rộng rãi để đánh giá khả năng hiểu ngôn ngữ tự nhiên [17,25,53]. Kết quả được trình bày trong Bảng 1. "# Param" trong bảng biểu thị tổng số tham số có thể huấn luyện hiệu quả cho mỗi phương pháp. Chúng tôi lưu ý rằng OFT [67] với kích thước khối 16 là BOFT(1,16). Người ta có thể quan sát rằng tinh chỉnh trực giao hoạt động tốt hơn các phương pháp tiên tiến hiện tại. Quan trọng hơn, BOFT vượt trội hơn OFT trong khi vẫn sử dụng ít tham số hơn.

MMLU (5-shot) MMLU (0-shot)
Phương pháp # Param Hums. STEM Social Other Avg. Hums. STEM Social Other Avg.
Llama-2-7B - 43.0 36.9 51.6 52.1 45.7 38.8 33.3 46.8 45.0 40.8
LoRA r=16 0.125% 42.9 38.5 54.5 53.8 47.0 42.5 37.1 51.5 52.3 45.5
LoRA r=32 0.25% 42.9 38.7 54.6 54.7 47.3 42.5 36.7 52.8 52.7 45.9
OFT b=16 0.13% 44.0 38.9 54.2 54.3 47.5 44.0 36.7 52.9 52.0 46.2
BOFTm=2 b=8 0.12% 44.5 39.0 54.4 55.1 47.9 44.3 37.4 53.1 52.8 46.7

Bảng 2: Độ chính xác (%) trên MMLU. "# Param" biểu thị tỷ lệ phần trăm của các tham số được tinh chỉnh.

Hiểu ngôn ngữ đa nhiệm vụ lớn. Chúng tôi sử dụng Alpaca [80] như tập dữ liệu tinh chỉnh của chúng tôi và đánh giá cả hiệu suất zero-shot và few-shot trên tập dữ liệu MMLU [27] bao gồm 57 nhiệm vụ ngôn ngữ. Tất cả các phương pháp sử dụng mô hình Llama-2-7B được huấn luyện trước [81]. Kết quả trong Bảng 2 cho thấy cải thiện nhất quán so với LoRA, nhưng BOFT sử dụng ít tham số hơn. Đáng chú ý, BOFT(2,8) tạo ra một ma trận trực giao khối chéo với kích thước khối 16, và vẫn vượt trội hơn OFT với cùng kích thước khối (tức là, BOFT(1,16)) với một biên độ đáng kể. Kết quả này ngụ ý rằng cấu trúc bướm có thể kết hợp một bias quy nạp có thể tổng quát hóa.

Phương pháp # Param GSM8K MATH
Llama-2-7B - 14.6 2.5
LoRA r=32 0.25% 50.2 7.8
OFT b=16 0.13% 50.1 8.4
BOFTm=2 b=8 0.12% 50.6 8.6

Bảng 3: Kết quả trên GSM8K và MATH.

Trả lời câu hỏi toán học. Chúng tôi cũng đánh giá phương pháp của chúng tôi trong trả lời câu hỏi toán học sử dụng hai benchmark thách thức: GSM8K [11] và MATH [27]. Đối với tất cả các phương pháp tinh chỉnh, chúng tôi sử dụng MetaMathQA-40K [91] như tập dữ liệu tinh chỉnh, và mô hình Llama-2-7B [81] như backbone được huấn luyện trước. Như có thể quan sát trong Bảng 3, BOFT xuất sắc trong lý luận toán học trên cả hai tập dữ liệu. Chúng tôi lưu ý rằng mặc dù cải thiện trên tập dữ liệu MATH thực tế khá thách thức, BOFT đạt được hơn 10% cải thiện tương đối so với LoRA trong khi chỉ sử dụng một nửa số lượng tham số có thể huấn luyện cho LoRA. Hơn nữa, BOFT vượt trội hơn OFT ngay cả với cùng số khối hiệu quả, một lần nữa xác minh rằng cấu trúc bướm có thể giới thiệu một bias quy nạp có thể tổng quát hóa. Chúng tôi cũng cung cấp một nghiên cứu trường hợp của một vài câu hỏi trong Phụ lục E.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Tự nhiên Chuyên môn Có cấu trúc# param (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Trung bình

Tinh chỉnh đầy đủ 304.4 67.6 91.7 77.9 99.7 93.7 92.8 52.3 88.1 96.1 90.9 77.2 67.2 59.8 58.1 82.8 83.6 62.0 36.9 39.4 74.6
Linear Probing 0 73.2 90.9 78.1 99.7 95.2 40.3 59.3 84.2 92.9 86.8 75.6 48.1 44.4 45.9 65.4 25.5 37.0 18.5 30.9 62.7
BitFit [92] 0.27 78.5 91.7 80.4 99.7 95.0 67.3 60.2 85.2 96.1 90.7 75.7 84.1 63.0 52.7 78.9 83.8 61.9 28.0 37.7 74.2
FacTtt r=16 [31] 0.12 76.2 89.4 77.3 99.7 94.7 89.6 58.9 87.1 94.3 88.7 74.0 83.1 63.3 56.2 83.1 61.7 37.1 23.3 32.6 72.1
FacTtk r=32 [31] 0.12 75.0 89.1 78.6 99.7 95.0 92.1 58.9 86.1 94.6 89.5 74.2 84.3 62.0 57.7 85.2 68.4 38.3 31.2 44.2 73.9
LoRA r=4[29] 1.77 77.2 92.8 80.3 99.7 94.8 92.7 59.5 88.3 96.4 91.4 77.4 74.7 62.4 58.1 85.2 85.8 57.2 31.8 37.2 76.6
GLoRA r=4[6] 4.87 80.1 93.7 80.2 99.7 94.4 89.6 59.9 85.9 96.0 91.0 76.2 61.8 62.3 56.9 85.8 65.7 57.2 37.0 41.4 74.5
OFT b=16 2.10 77.7 91.9 80.1 99.7 94.7 92.9 59.3 88.4 96.4 91.5 77.2 81.0 64.7 60.5 84.0 92.2 61.1 34.8 40.3 77.3
BOFTm=2 b=8 1.99 78.1 92.5 80.6 99.7 95.0 93.0 59.9 88.9 96.6 91.6 77.3 84.5 64.9 61.4 84.1 93.9 62.0 36.2 40.0 77.9
BOFTm=4 b=4 1.77 78.2 91.4 79.6 99.7 94.9 92.8 59.4 88.1 96.4 91.6 76.2 81.9 65.4 60.0 84.5 92.9 61.3 37.1 39.3 77.4
BOFTm=6 b=2 1.11 78.3 91.5 79.9 99.7 95.0 92.0 60.2 88.2 96.5 91.4 77.2 80.5 64.1 61.4 85.0 91.6 60.8 34.0 38.5 77.1

Bảng 4: Kết quả (%) trên benchmark VTAB-1K. "# param" chỉ định số lượng tham số có thể huấn luyện của mỗi phương pháp. Độ chính xác trung bình được thu được bằng cách lấy trung bình trên tất cả 19 nhiệm vụ. Kết quả tốt nhất được đánh dấu bằng "in đậm", và kết quả tốt thứ hai/thứ ba được đánh dấu bằng "gạch chân".

6.2 THÍCH ỨNG CỦA MÔ HÌNH NỀN TẢNG THỊ GIÁC

Mô hình # ParamDIS COIFT HRSOD ThinObject Trung bình
mIoU mBIoU mIoU mBIoU mIoU mBIoU mIoU mBIoU mIoU mBIoU
SAM (baseline) 0 62.0 52.8 92.1 86.5 90.2 83.1 73.6 61.8 79.5 71.1
Tinh chỉnh SAM 4.06M 78.9 70.3 93.9 89.3 91.8 83.4 89.4 79.0 88.5 80.5
HQ-SAM [34] 1.33M 78.6 70.4 94.8 90.1 93.6 86.9 89.5 79.9 89.1 81.8
OFT-SAM b=16 0.07M 77.8 69.1 94.9 90.3 92.6 85.5 91.2 80.6 88.9 81.4
BOFT-SAMm=4 b=4 0.04M 78.2 69.7 94.9 90.5 93.1 86.0 91.7 80.1 89.5 81.6
BOFT-SAMm=2 b=8 0.06M 78.4 70.3 94.7 90.1 93.0 86.5 91.7 81.8 89.5 82.2

Bảng 5: Kết quả trên HQSeg-44K [34] (DIS [66], COIFT [47], HRSOD [93], ThinObject [47]).

Học chuyển giao trên VTAB-1K. Chúng tôi đánh giá hiệu suất tinh chỉnh của BOFT trên benchmark VTAB-1K [94], đã được sử dụng rộng rãi để đánh giá các thuật toán học chuyển giao hiệu quả tham số. VTAB-1K bao gồm 19 nhiệm vụ phân loại hình ảnh được chia thành ba danh mục: hình ảnh tự nhiên, nhiệm vụ chuyên môn (ví dụ, viễn thám và hình ảnh y tế), và nhiệm vụ có cấu trúc (ví dụ, dự đoán độ sâu và hướng). Trong VTAB-1k, mỗi tập dữ liệu cung cấp 800 mẫu tập huấn luyện được gán nhãn, một tập con của tập huấn luyện gốc của chúng. Chúng tôi sử dụng chúng để tinh chỉnh mô hình cơ sở và độ chính xác phân loại Top-1 trên tập kiểm tra gốc tương ứng của chúng được sử dụng như thước đo hiệu suất. Đáng chú ý, tất cả các phương pháp được so sánh không gây ra độ trễ suy luận, vì vậy chúng có cùng tốc độ suy luận. Bởi vì lớp phân loại cuối cùng sẽ luôn được huấn luyện lại và các tham số có thể huấn luyện của lớp tuyến tính phân loại đó khác nhau giữa các nhiệm vụ khác nhau, chúng tôi theo thông lệ chung và không tính chúng khi báo cáo tổng số tham số có thể huấn luyện cho mỗi phương pháp. Khác với [6], chúng tôi sử dụng một vision transformer được huấn luyện trước lớn hơn nhiều [62] (DINOv2-large) với hơn 300M tham số. Độ chính xác được đưa ra trong Bảng 4. Chúng tôi quan sát rằng tinh chỉnh trực giao đạt được độ chính xác kiểm tra tổng thể tốt nhất trên benchmark VTAB-1K, và BOFT với m=4, b=4 một lần nữa đạt được hiệu suất tốt nhất. Đáng chú ý, sự nâng cao hiệu suất của BOFT vừa ổn định vừa nhất quán giữa các nhiệm vụ, vì hầu như tất cả kết quả của chúng tôi đều vượt trội hơn baseline tinh chỉnh đầy đủ đơn giản nhất. BOFT kém hơn một chút so với tinh chỉnh đầy đủ trên ba nhiệm vụ: dSpr-Ori (-0.7%), Caltech101 (-0.3%) và sNORB-Ele (-0.1%). Ngược lại, LoRA kém đáng kể so với tinh chỉnh đầy đủ trên sNORB-Azim và dSpr-Ori tới 5%. Những kết quả này xác thực hiệu quả của BOFT cho vision transformer.

SAM
 BOFT-SAM

Hình 6: So sánh định tính giữa SAM và BOFT-SAM.

Phân đoạn chất lượng cao với SAM. Mô hình Phân đoạn Bất cứ thứ gì (SAM) [35] là một mô hình nền tảng thị giác cho phân đoạn hình ảnh có thể nhắc nhở, thể hiện khả năng zero-shot ấn tượng. SAM bao gồm ba thành phần chính: một bộ mã hóa hình ảnh được huấn luyện trước để tạo ra một embedding đặc trưng của hình ảnh đầu vào, một bộ mã hóa prompt để nhúng prompt, và một bộ giải mã mask để ánh xạ những embedding đầu vào này thành một mask phân đoạn. Mặc dù hiệu suất ấn tượng trong phân đoạn hình ảnh chung, SAM thiếu khả năng thực hiện phân đoạn có độ chính xác cao trong các tình huống thách thức. Để giải quyết điều này, HQ-SAM [34] đề xuất huấn luyện một HQ-Output Token bổ sung và một mô-đun fusion đặc trưng toàn cục-cục bộ trên một tập dữ liệu phân đoạn chất lượng cao, HQSeg-44K [34], để cải thiện chất lượng mask, đạt được hiệu suất tiên tiến trong phân đoạn chất lượng cao. Sử dụng cùng tập dữ liệu và hàm mất mát như HQ-SAM, chúng tôi tinh chỉnh SAM ban đầu với BOFT trong 10 epoch. Cụ thể, chúng tôi chỉ áp dụng BOFT cho tất cả các lớp tuyến tính của bộ giải mã mask của SAM, trong khi giữ phần khác của SAM đông lạnh. Chúng tôi so sánh với việc tinh chỉnh toàn bộ bộ giải mã mask, huấn luyện các mô-đun HQ-SAM [34] và tinh chỉnh các lớp tuyến tính với BOFT. Bảng 5 cho thấy rằng BOFT-SAM chỉ sử dụng 3% tham số có thể huấn luyện được sử dụng trong HQ-SAM, và vẫn khớp với hiệu suất của nó. Hơn nữa, vì trọng số nhân được học bởi BOFT có thể được kết hợp trở lại với trọng số của SAM, BOFT-SAM có chính xác cùng tốc độ suy luận như SAM, trong khi, ngược lại, HQ-SAM có các mô-đun bổ sung ảnh hưởng đến tốc độ suy luận của nó.

6.3 KIỂM SOÁT MÔ HÌNH KHUẾCH TÁN VĂN BẢN THÀNH HÌNH ẢNH

Phương pháp # Param Lỗi ↓
LoRA r=16 2.52M 8.878
LoRA r=64 10.08M 8.062
LoRA r=128 20.17M 8.038
OFT r=16 2.71M 8.876
OFT r=4 10.50M 6.537
OFT r=2 20.89M 6.407
BOFTm=2 r=32 2.66M 8.070
BOFTm=6 r=32 7.69M 6.731
BOFTm=5 r=16 12.93M 6.387
BOFTm=4 r=8 20.76M 5.667

Bảng 6: Lỗi landmark khuôn mặt giữa tín hiệu điều khiển và dự đoán.

Vì OFT ban đầu được sử dụng để kiểm soát các mô hình khuếch tán văn bản thành hình ảnh [67], chúng tôi cũng đánh giá BOFT với cùng nhiệm vụ để so sánh tốt hơn. Chúng tôi tinh chỉnh Stable Diffusion được huấn luyện trước [73] cho hai nhiệm vụ: tạo sinh có thể kiểm soát (ví dụ, [59,96]) và tạo sinh dựa trên chủ thể (ví dụ, [74]). Tạo sinh có thể kiểm soát cho phép thêm tín hiệu điều khiển không gian vào các mô hình khuếch tán văn bản thành hình ảnh. Tạo sinh dựa trên chủ thể nhằm tổng hợp hình ảnh của một chủ thể trong các bối cảnh mới bằng cách tinh chỉnh trên một vài hình ảnh của chủ thể đó để học một định danh duy nhất. Chúng tôi theo cùng cài đặt như [67] để đánh giá tạo sinh có thể kiểm soát. Để dễ so sánh với OFT, nơi cấu trúc khối được đặc trưng bởi số lượng khối r, chúng tôi cũng sử dụng số lượng khối để đặc trưng BOFT (thay vì kích thước khối b). Bởi vì rd=b, r lớn hơn chỉ ra ít tham số hơn. Ví dụ, đối với BOFT với r=32 để tạo ra một ma trận trực giao dày đặc, chúng ta cần có m=6.

Chúng tôi bắt đầu bằng việc so sánh LoRA, OFT và BOFT với ngân sách tham số nhỏ (ít hơn 3M tham số). Chúng ta thấy từ Bảng 6 rằng BOFT với r=32, m=2 mang lại hiệu suất tốt hơn đáng kể so với cả LoRA và OFT với số khối 16 dưới ngân sách tham số 3M. Dưới cài đặt ngân sách nhỏ này, chúng tôi cũng cung cấp so sánh định tính giữa LoRA (r=16), OFT (r=16) và BOFT (r=32, m=2) trong Hình 7. Chúng tôi cũng đánh giá cách BOFT hoạt động với ma trận trực giao dày đặc sử dụng r=16, m=4 và r=8, m=4. Chúng tôi quan sát rằng BOFT với r=8, m=4 đạt được hiệu suất tốt nhất và vượt trội đáng kể so với LoRA với số lượng tham số tương tự.

Cuối cùng, chúng tôi tiến hành một nghiên cứu ablation về cách số lượng thành phần bướm m ảnh hưởng đến hiệu suất của tạo sinh có thể kiểm soát. Chúng tôi đầu tiên cố định số khối là r=32, và sau đó thay đổi số lượng thành phần bướm trong BOFT từ 0 (tức là, OFT với số khối 32) đến 6 (BOFT với ma trận trực giao dày đặc). Hình 8 cho thấy rằng BOFT với m lớn hơn mang lại hiệu suất điều khiển tốt hơn cho việc tinh chỉnh Stable Diffusion. Thú vị hơn, chúng tôi cũng thấy rằng, với cùng số lượng khối, số lượng thành phần bướm tăng thường dẫn đến hội tụ nhanh hơn và ổn định hơn. Điều này ngụ ý rằng tinh chỉnh trực giao với ma trận trực giao dày đặc hơn hội tụ nhanh hơn trong việc tinh chỉnh các mô hình khuếch tán văn bản thành hình ảnh. Điều này cũng khớp với trực giác của chúng ta rằng ma trận trực giao dày đặc có thể biến đổi neuron hiệu quả hơn do truyền tải thông tin hiệu quả hơn.

BOFT cũng hoạt động nhất quán tốt hơn cả LoRA và OFT ban đầu trong tạo sinh dựa trên chủ thể. Một so sánh định tính được đưa ra trong Hình 9 và Phụ lục C. Đối với tất cả các phương pháp được so sánh, chúng tôi sử dụng các siêu tham số tốt nhất có thể. Chúng tôi quan sát thực nghiệm rằng BOFT nói chung có thể nắm bắt nhiều đặc điểm bản sắc nội tại của chủ thể đầu vào hơn, và do đó, các hình ảnh được tạo ra

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

a [V] bowl with a wheat field in the background
a [V] bowl with a city in the background
LoRA OFT BOFTa shiny [V] backpack
a [V] backpack on top of pink fabric
LoRA OFT BOFT
Hình ảnh đầu vào Hình ảnh đầu vào

a [V] sneaker with the Eiffel Tower in the background
 a cube shaped [V] toy
a [V] toy with a tree and autumn leaves in the background
LoRA OFT BOFTa [V] sneaker on the beach
LoRA OFT BOFTHình ảnh đầu vào Hình ảnh đầu vào

Hình 9: So sánh định tính của tạo sinh dựa trên chủ thể. Hình này được xem tốt nhất ở dạng kỹ thuật số, có màu và được phóng to đáng kể.

hợp lý hơn về mặt thị giác trong việc bảo tồn bản sắc chủ thể. Chúng ta có thể thấy từ Hình 9 rằng OFT ban đầu cũng cho thấy hiệu suất tốt trong việc bảo tồn bản sắc chủ thể trong khi LoRA có khả năng tuân theo prompt văn bản tốt hơn. Ngược lại rõ rệt, BOFT có thể đạt được điều tốt nhất của cả hai thế giới bằng cách đồng thời thể hiện khả năng bảo tồn bản sắc chủ thể tốt cũng như khả năng tuân theo prompt văn bản chính xác. Đáng chú ý, đối với trường hợp đồ chơi vịt dưới bên trái trong Hình 9, chúng tôi quan sát rằng BOFT có thể nắm bắt bản chất của đồ chơi và tạo ra một đồ chơi hình khối với màu sắc tương tự về mặt khái niệm.

BOFT đi kèm với nội suy trọng số miễn phí. Chúng tôi có một khám phá đáng ngạc nhiên nhưng thú vị phân biệt duy nhất BOFT với các phương pháp hiện có trong tạo sinh có thể kiểm soát. BOFT bao gồm nhiều ma trận trực giao (tức là, nhiều thành phần bướm), và tích của những ma trận này cho ra mô hình được tinh chỉnh hoàn chỉnh. Tuy nhiên, điều gì sẽ xảy ra nếu chúng ta đặt các thành phần bướm trực giao được huấn luyện thành ma trận đơn vị từng cái một mà không huấn luyện lại? Nếu chúng ta đặt tất cả các thành phần bướm thành đơn vị, mô hình giảm xuống Stable Diffusion. Nếu không có thành phần bướm nào được đặt thành đơn vị, thì chúng ta có mô hình BOFT-tinh chỉnh đầy đủ. Sau khi huấn luyện BOFT, cấu trúc của nhiều thành phần bướm cung cấp cho chúng ta một nội suy trọng số miễn phí trên manifold trực giao. Chúng tôi thực hiện nội suy trọng số cho tất cả các lớp BOFT-tinh chỉnh trong Stable Diffusion. Cụ thể, chúng tôi sử dụng BOFT với m=5, r=16, vì vậy chúng ta có 6 thành phần bướm. Chúng tôi đặt các thành phần bướm từng cái một thành ma trận đơn vị, bắt đầu từ phía tay trái. Kết quả được đưa ra trong Hình 10. Đáng ngạc nhiên, mặc dù những trọng số nội suy này chưa được huấn luyện lại, chúng vẫn có thể tạo ra hình ảnh hợp lý. Thực tế, khi chúng ta đặt nhiều thành phần bướm hơn thành đơn vị, mô hình nội suy tạo ra kết quả nội suy mượt mà, từ hình ảnh được điều khiển landmark đến hình ảnh Stable Diffusion không được điều khiển. Những kết quả này xác thực tốt rằng không gian trọng số giả thuyết (tức là, không gian mô hình) trong BOFT có thể bảo tồn tốt ngữ nghĩa và loại bỏ hiệu quả nhiều cực tiểu cục bộ xấu.

7 NHẬN XÉT KẾT LUẬN VÀ HẠN CHẾ

Bài báo của chúng tôi đề xuất Bướm Trực giao, một phương pháp tinh chỉnh hiệu quả tham số chung cho các mô hình nền tảng dựa trên cấu trúc bướm. Insight chính để có hiệu quả tham số tốt hơn là tham số hóa một ma trận trực giao dày đặc với tích của nhiều ma trận trực giao thưa thớt. Để dễ dàng tìm các phân tích nhân tử ma trận khả thi, chúng tôi đề xuất một khung truyền tải thông tin đồ thị. Dưới khung này, chúng tôi thấy rằng cấu trúc bướm có thể đạt hiệu quả các tiêu chí mong muốn của chúng ta về phân tích nhân tử ma trận trực giao thưa thớt. Chúng tôi chứng minh hiệu quả thực nghiệm của BOFT trong việc tinh chỉnh mô hình ngôn ngữ lớn, mô hình thị giác lớn và mô hình tạo sinh văn bản thành hình ảnh. Các thực nghiệm của chúng tôi cũng xác thực tính ưu việt của BOFT như một phương pháp tinh chỉnh mô hình chung.

Mặc dù có hiệu quả thực nghiệm, BOFT không hề hoàn hảo. Vì ma trận trực giao cuối cùng trong BOFT là tích của nhiều ma trận trực giao, chi phí runtime huấn luyện hơi lớn hơn OFT. Cách cải thiện runtime huấn luyện của BOFT vẫn là một vấn đề mở. May mắn thay, sau giai đoạn tinh chỉnh, các ma trận trực giao được học bởi BOFT có thể được nhân trực tiếp vào mô hình được huấn luyện trước và không có độ trễ suy luận bổ sung. Hơn nữa, liệu mạng bướm có phải là cách hiệu quả nhất để truyền tải thông tin hay không cũng chưa được biết. Khung truyền tải thông tin của chúng tôi tiếp tục cho phép chúng ta lấy cảm hứng từ một lĩnh vực nghiên cứu riêng biệt - mạng máy tính, nơi hiệu quả của một cấu trúc tô-pô mạng để truyền tải thông tin được nghiên cứu rất nhiều. Chúng tôi mong đợi rằng các cấu trúc mạng hiệu quả hơn có thể được sử dụng để tạo ra các ma trận trực giao dày đặc.

CÔNG NHẬN VÀ ĐÓNG GÓP CỦA TÁC GIẢ

Các tác giả muốn chân thành cảm ơn Peter Kulits vì việc đọc hiệu đính cẩn thận, Tim Z. Xiao và nhiều đồng nghiệp khác tại Viện Max Planck cho Hệ thống Thông minh vì vô số gợi ý hữu ích.

Công trình này được hỗ trợ bởi Bộ Giáo dục và Nghiên cứu Liên bang Đức (BMBF): Trung tâm AI Tübingen, FKZ: 01IS18039B, và bởi Cụm Học máy Xuất sắc, số EXC 2064/1 – Số dự án 390727645. WL được hỗ trợ bởi Quỹ Nghiên cứu Đức (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP XX, số dự án: 276693517. YF và SP được hỗ trợ một phần bởi Trung tâm Max Planck ETH cho Hệ thống Học. Yuliang Xiu được tài trợ bởi chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh châu Âu dưới thỏa thuận tài trợ Marie Skłodowska-Curie số 860768 (CLIPE). AW công nhận sự hỗ trợ từ Học bổng AI Turing dưới grant EP/V025279/1, Viện Alan Turing, và Leverhulme Trust thông qua CFI. MJB đã nhận được quỹ quà tặng nghiên cứu từ Adobe, Intel, Nvidia, Meta/Facebook, và Amazon. MJB có lợi ích tài chính trong Amazon, Datagen Technologies, và Meshcapade GmbH. Trong khi MJB là cố vấn cho Meshcapade, nghiên cứu của ông trong dự án này được thực hiện hoàn toàn tại, và được tài trợ hoàn toàn bởi, Hội Max Planck.

Weiyang Liu và Zeju Qiu đóng góp như nhau với tư cách tác giả chính chung. Yao Feng, Yuliang Xiu, Yuxuan Xue và Longhui Yu đóng góp như nhau với tư cách tác giả thứ hai chung. Các tác giả với đóng góp như nhau được liệt kê theo thứ tự bảng chữ cái và được phép thay đổi thứ tự của họ một cách tự do trên sơ yếu lý lịch và trang web.

Michael J. Black, Adrian Weller và Bernhard Schölkopf cùng giám sát dự án, cung cấp hỗ trợ tính toán hào phóng và đóng góp đáng kể vào dự án (bao gồm nhưng không giới hạn trong thảo luận ý tưởng, gợi ý hướng đi và viết bài báo).

Weiyang Liu khởi tạo ý tưởng cốt lõi, tổ chức dự án, đồng phát triển phương pháp hiện tại, đồng giám sát các thực nghiệm, và viết bản thảo. Zeju Qiu đồng khởi tạo ý tưởng cốt lõi, đồng phát triển phương pháp hiện tại, triển khai hầu hết các prototype, tiến hành các thực nghiệm GLUE, VTAB-1K và tinh chỉnh SAM, đồng giám sát các thực nghiệm và đóng góp vào việc viết bản thảo. Yao Feng triển khai một phiên bản BOFT nhanh trong CUDA, đóng góp vào phát triển phương pháp, và tiến hành các thực nghiệm tinh chỉnh đa phương thức. Yuliang Xiu dẫn dắt nỗ lực thực nghiệm trong việc kiểm soát các mô hình khuếch tán văn bản thành hình ảnh, tiến hành các thực nghiệm về cả tạo sinh có thể kiểm soát (ControlNet) và dựa trên chủ thể (DreamBooth), đóng góp vào phát triển phương pháp và viết bài báo.

Yuxuan Xue đóng góp vào phát triển phương pháp và các thực nghiệm của mô hình nền tảng thị giác. Longhui Yu dẫn dắt nỗ lực thực nghiệm trong MMLU và trả lời câu hỏi toán học và đóng góp vào phát triển phương pháp. Haiwen Feng đóng góp vào phát triển phương pháp và các thực nghiệm tạo sinh văn bản thành hình ảnh. Juyeon Heo tiến hành các thực nghiệm về tính bền vững. Tất cả các thành viên trong nhóm đều có những đóng góp cần thiết cho việc phát triển phương pháp và viết bài báo.

Mô hình ngôn ngữ lớn: các thực nghiệm được dẫn dắt chung bởi Zeju Qiu và Longhui Yu. Weiyang Liu đóng góp vào việc debug mô hình.

Mô hình nền tảng thị giác: các thực nghiệm được dẫn dắt bởi Zeju Qiu. Yuxuan Xue đóng góp vào các thực nghiệm baseline và debug mô hình. Weiyang Liu đóng góp vào việc debug mô hình.

Mô hình khuếch tán văn bản thành hình ảnh: các thực nghiệm được dẫn dắt bởi Yuliang Xiu. Zeju Qiu, Weiyang Liu và Haiwen Feng đóng góp vào việc debug mô hình.

11
