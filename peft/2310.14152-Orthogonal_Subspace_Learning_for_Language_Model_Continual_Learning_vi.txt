# Học tập không gian con trực giao cho việc học liên tục mô hình ngôn ngữ

Xiao Wang⋆∗, Tianze Chen⋆∗, Qiming Ge⋆, Han Xia⋆,
Rong Bao⋆,Rui Zheng⋆,Qi Zhang⋆†,Tao Gui♦†,Xuanjing Huang⋆♣
⋆Trường Khoa học máy tính, Đại học Phúc Đán, Thượng Hải, Trung Quốc
♦Viện Ngôn ngữ học và Ngôn ngữ hiện đại, Đại học Phúc Đán, Thượng Hải, Trung Quốc
♣Viện Kiểu hình con người quốc tế (Thượng Hải)
{xiao_wang20,qz,tgui}@fudan.edu.cn

## Tóm tắt

Được hưởng lợi từ kho dữ liệu khổng lồ và phần cứng tiên tiến, các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng đáng chú ý trong việc hiểu và tạo sinh ngôn ngữ. Tuy nhiên, hiệu suất của chúng giảm sút trong các tình huống gặp phải nhiều tác vụ tuần tự, còn được gọi là quên thảm khốc. Trong bài báo này, chúng tôi đề xuất phương pháp thích ứng hạng thấp trực giao (O-LoRA), một cách tiếp cận đơn giản và hiệu quả cho việc học liên tục trong mô hình ngôn ngữ, giảm thiểu hiệu quả việc quên thảm khốc trong khi học các tác vụ mới. Cụ thể, O-LoRA học các tác vụ trong các không gian con vector (hạng thấp) khác nhau được giữ trực giao với nhau để giảm thiểu sự can thiệp. Phương pháp của chúng tôi chỉ tạo ra chi phí tham số bổ sung tối thiểu và không yêu cầu lưu trữ dữ liệu người dùng để phát lại. Kết quả thực nghiệm trên các tiêu chuẩn học liên tục cho thấy phương pháp của chúng tôi vượt trội hơn các phương pháp hiện đại nhất. Hơn nữa, so với các phương pháp trước đây, phương pháp của chúng tôi xuất sắc trong việc bảo tồn khả năng tổng quát hóa của LLM trên các tác vụ chưa thấy.

## 1 Giới thiệu

Học các tác vụ tuần tự là rất quan trọng để phát triển các mô hình NLP thực tế (Wang et al., 2023b; Xi et al., 2023), vì nó cho phép sự tiến hóa liên tục khi gặp các tác vụ hoặc kiến thức mới. Mặc dù các mô hình được huấn luyện trước (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020; OpenAI, 2023) đã đạt được thành công to lớn trên các tác vụ tĩnh (Wang et al., 2022a, 2023c), việc học nhiều tác vụ tuần tự, thường được gọi là học liên tục, vẫn là một thách thức (Wu et al., 2022; Luo et al., 2023). Khi mô hình học các tác vụ mới, nó có xu hướng quên hoặc mất đi kiến thức mà nó đã tiếp thu cho các tác vụ trước đó, dẫn đến hiện tượng được gọi là quên thảm khốc (McCloskey và Cohen, 1989).

Hướng cập nhật ban đầu
Không gian con gradient của các tác vụ trước đó
Hướng cập nhật trực giao

Hình 1: Hình minh họa làm nổi bật trực giác của cách tiếp cận của chúng tôi. O-LoRA giảm thiểu việc quên thảm khốc kiến thức tác vụ trong quá khứ bằng cách hạn chế việc cập nhật gradient của tác vụ hiện tại trực giao với không gian con gradient của các tác vụ trong quá khứ.

Các nghiên cứu học liên tục hiện tại (Ke và Liu, 2022; Wang et al., 2023a) có thể được phân loại chủ yếu thành các cách tiếp cận dựa trên diễn tập, dựa trên chính quy hóa, và dựa trên kiến trúc. Các cách tiếp cận dựa trên diễn tập (Lopez-Paz và Ranzato, 2017; de Masson D'Autume et al., 2019) cho phép truy cập vào bộ đệm bộ nhớ với các ví dụ từ các tác vụ trước đó và huấn luyện mô hình cùng với tác vụ hiện tại. Thật không may, việc lưu trữ và phát lại dữ liệu từ các tác vụ trước đó có thể gây ra lo ngại về quyền riêng tư, đặc biệt khi có liên quan đến thông tin nhạy cảm hoặc có thể nhận dạng cá nhân. Các cách tiếp cận dựa trên chính quy hóa (Kirkpatrick et al., 2017; Li và Hoiem, 2017; Smith et al., 2023) đưa ra các hạng mục bổ sung trong hàm mất mát để phạt các thay đổi trong trọng số quan trọng, nhằm bảo vệ các tác vụ đã học trước đó. Chúng thường gặp khó khăn trong việc xử lý các chuỗi tác vụ dài. Các cách tiếp cận dựa trên kiến trúc (Wang et al., 2023e; Razdaibiedina et al., 2023) mở rộng động khả năng của mô hình hoặc cô lập các trọng số mô hình hiện tại để giảm sự can thiệp.

Tuy nhiên, các phương pháp như vậy về cơ bản học các mô hình chuyên gia khác nhau cho các tác vụ khác nhau, hạn chế khả năng tổng quát hóa của chúng đối với các tác vụ chưa thấy.

Các phương pháp hiện tại thường cập nhật tất cả các tác vụ trong một không gian vector chung, ảnh hưởng trực tiếp đến đầu ra lớp ẩn của mô hình. Các nghiên cứu gần đây (Farajtabar et al., 2020; Saha et al., 2021) đã làm nổi bật một cách tiếp cận đầy hứa hẹn để giải quyết vấn đề này. Bằng cách thực hiện các bước gradient theo hướng trực giao với các không gian con gradient liên kết với các tác vụ trong quá khứ, chúng ta có thể giảm thiểu hiệu quả việc quên thảm khốc vì nó ngăn chặn sự can thiệp với các hàm mất mát của tác vụ trong quá khứ. Tuy nhiên, các phương pháp trước đây hoặc yêu cầu lưu trữ dữ liệu lịch sử (Chaudhry et al., 2019), điều này gây ra lo ngại về quyền riêng tư dữ liệu, hoặc gradient dữ liệu lịch sử (Farajtabar et al., 2020), điều này trở nên không thực tế đối với các mô hình quy mô lớn.

Trong nghiên cứu này, chúng tôi đề xuất phương pháp thích ứng hạng thấp trực giao (O-LoRA)¹, một cách tiếp cận đơn giản và hiệu quả cho việc học liên tục trong mô hình ngôn ngữ. Cái nhìn quan trọng của chúng tôi có nguồn gốc từ bản chất của LoRA: các mô hình lớn được huấn luyện trước chủ yếu tinh chỉnh trong một không gian con hạng thấp cụ thể. Với tiền đề này, chúng tôi đưa ra giả thuyết rằng các không gian con gradient từ các tác vụ trước đó có thể được bắt giữ hiệu quả bởi các tham số LoRA. Trong bối cảnh học liên tục, chúng tôi dần dần học các tác vụ mới trong một không gian con trực giao trong khi cố định các tham số LoRA được học từ các tác vụ trong quá khứ. Hình 1 cung cấp một biểu diễn trực quan về cách O-LoRA giảm thiểu việc quên thảm khốc.

Phương pháp của chúng tôi cung cấp ba lợi thế: (1) Thân thiện với quyền riêng tư dữ liệu: Chúng tôi không yêu cầu lưu trữ dữ liệu người dùng để phát lại, giải quyết các lo ngại liên quan đến quyền riêng tư. (2) Thân thiện với tham số mô hình: Bằng cách chỉ đưa ra chi phí tối thiểu của các tham số bổ sung, cách tiếp cận của chúng tôi cho phép việc học các tác vụ mới mà không làm tổn hại đến hiệu suất của các tác vụ trước đó. (3) Thân thiện với tổng quát hóa: Phương pháp của chúng tôi không dựa vào ID tác vụ trong quá trình kiểm tra, làm cho nó tương thích với mô hình điều chỉnh hướng dẫn (Wang et al., 2022b), do đó bảo tồn khả năng tổng quát hóa của LLM trên các tác vụ chưa thấy.

Những đóng góp chính của chúng tôi được tóm tắt như sau:

• Chúng tôi giới thiệu O-LoRA, một cách tiếp cận đơn giản và hiệu quả cho việc học liên tục trong mô hình ngôn ngữ, dần dần học các tác vụ mới trong các không gian con trực giao.

• Phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp SOTA trước đây trên các tiêu chuẩn học liên tục chuẩn.

• Kết quả thực nghiệm cho thấy phương pháp của chúng tôi bảo tồn khả năng tổng quát hóa của các mô hình ngôn ngữ lớn trên các tác vụ chưa thấy, điều mà các phương pháp trước đây thiếu.

## 2 Nền tảng

### 2.1 Thiết lập học liên tục

Học liên tục (Ke và Liu, 2022; Wang et al., 2023b) tập trung vào việc phát triển các thuật toán học để tích lũy kiến thức trên dữ liệu không ổn định. Trong học liên tục có giám sát, một chuỗi các tác vụ {D1, . . . , DT} đến theo cách streaming. Mỗi tác vụ Dt = {xᵢᵗ, yᵢᵗ}ⁿᵢ₌₁ chứa một tập dữ liệu đích riêng biệt, trong đó xᵢᵗ ∈ Xt, yᵢᵗ ∈ Yt. Một mô hình duy nhất cần thích ứng với chúng tuần tự, chỉ với quyền truy cập vào Dt tại tác vụ thứ t. Nói chung, với mô hình dự đoán hΘ được tham số hóa bởi Θ, học liên tục tìm cách tối ưu hóa cho mục tiêu sau trên tất cả các tác vụ:

max_Θ ∑ᵀₖ₌₁ ∑_(x,y)∈Dₖ log pΘ(y|x)     (1)

Trong nghiên cứu này, chúng tôi giải quyết một thiết lập thách thức hơn. Trong giai đoạn huấn luyện, mô hình bị cấm truy cập bất kỳ dữ liệu lịch sử nào. Trong giai đoạn kiểm tra, mô hình dự đoán nhãn của một mẫu mà không biết nó thuộc về tác vụ nào.

### 2.2 LoRA

Khi các mô hình được huấn luyện trước (PTM) thích ứng với các tác vụ cụ thể, Hu et al. (2021) đã chứng minh rằng việc cập nhật trọng số trong PTM thể hiện "chiều nội tại" thấp. Đối với ma trận trọng số được huấn luyện trước Winit ∈ Rᵈˣᵏ, LoRA hạn chế việc cập nhật của nó bằng cách biểu diễn nó với một phân rã hạng thấp Winit + ΔW = Winit + AB, trong đó A ∈ Rᵈˣʳ, B ∈ Rʳˣᵏ, và hạng r ≪ min(d, k). Winit vẫn cố định trong quá trình huấn luyện và không nhận được cập nhật gradient, trong khi A và B chứa các tham số có thể huấn luyện. Để minh họa việc truyền tải tiến được sửa đổi của LoRA, xem xét thao tác h = Winitx. Với LoRA, việc truyền tải tiến được sửa đổi trở thành:

h = Winitx + ΔWx = Winitx + ABx     (2)

## 3 Thích ứng hạng thấp trực giao

Trong phần này, chúng tôi giới thiệu O-LoRA, được minh họa trong Hình 2. Đầu tiên, chúng tôi áp dụng điều chỉnh hướng dẫn làm mô hình huấn luyện của chúng tôi. Sau đó, chúng tôi dần dần học các tác vụ mới trong một không gian con trực giao trong khi giữ cố định các tham số LoRA cho các tác vụ trong quá khứ. Cuối cùng, chúng tôi tiến hành phân tích so sánh phương pháp của chúng tôi với các phương pháp hiện tại.

### 3.1 Lược đồ hướng dẫn

Khả năng tuân theo hướng dẫn là cần thiết cho LLM như một giao diện giữa con người và mô hình AI (Wang et al., 2022b; Ouyang et al., 2022; Wang et al., 2023d). Chúng tôi chọn điều chỉnh hướng dẫn làm mô hình huấn luyện của chúng tôi vì hai lý do: 1) Kết hợp chuyên môn của con người: Các mô hình có thể tận dụng kiến thức trước đó và được hưởng lợi từ chuyên môn của con người bằng cách cung cấp các hướng dẫn rõ ràng, dẫn đến việc học hiệu quả hơn. 2) Tăng cường tổng quát hóa: Hướng dẫn rõ ràng giúp mô hình nắm bắt các nguyên tắc cơ bản, cho phép tổng quát hóa tốt hơn đối với các tình huống chưa thấy.

Tất cả các hướng dẫn tác vụ tuân theo cùng một lược đồ thống nhất, bao gồm 1) Định nghĩa tác vụ cung cấp hướng dẫn chi tiết về cách một văn bản đầu vào (ví dụ, một câu hoặc một tài liệu) được mong đợi được ánh xạ tới một văn bản đầu ra. 2) Các tùy chọn là các ràng buộc nhãn đầu ra cho một tác vụ, đại diện cho tập hợp các đầu ra có thể được tạo ra bởi mô hình cho một đầu vào nhất định. 3) Văn bản là câu đầu vào của một thể hiện tác vụ. Chuỗi này sau đó được đưa vào mô hình ngôn ngữ được huấn luyện trước cùng với hướng dẫn tác vụ và các tùy chọn. 4) Câu trả lời là đầu ra mong đợi của mẫu đã cho.

### 3.2 Học liên tục trong các không gian con trực giao

Các phương pháp trước đây thể hiện một đặc điểm chung: tất cả các tác vụ trải qua cập nhật trong một không gian vector chung, ảnh hưởng trực tiếp đến đầu ra lớp ẩn của mô hình. Quên thảm khốc xảy ra trong mạng nơ-ron khi các cập nhật gradient đối với một tác vụ mới được áp dụng cho mô hình mà không xem xét các tác vụ trước đó.

Farajtabar et al. (2020) đề xuất phương pháp Hạ cấp Gradient Trực giao (OGD) để giảm thiểu vấn đề này, hạn chế các tham số di chuyển trong không gian trực giao với các gradient của các tác vụ trước đó. Với quyền truy cập hạn chế vào dữ liệu tác vụ trước đó, OGD xấp xỉ gradient hiện tại của dữ liệu trước đó với gradient trong các tham số hội tụ trước đó. Tuy nhiên, OGD cần lưu trữ gradient của tất cả dữ liệu trước đó. Điều này có thể đặc biệt khó xử lý đối với các mô hình ngôn ngữ quy mô lớn với hàng tỷ tham số (Raffel et al., 2020; Brown et al., 2020), đã trở thành tiêu chuẩn trong lĩnh vực NLP.

Liệu có thể xấp xỉ hướng gradient của các tác vụ trước đó mà không lưu trữ các gradient lịch sử? Trong nghiên cứu này, chúng tôi tận dụng không gian con hạng thấp của LoRA (Hu et al., 2021) như một đại diện cho không gian con gradient của các tác vụ trong quá khứ. Cái nhìn cơ bản của chúng tôi có nguồn gốc từ bản chất của LoRA: các mô hình lớn được huấn luyện trước chủ yếu tinh chỉnh trong một không gian con hạng thấp cụ thể. Đặc điểm hành vi này cho thấy rằng các tham số LoRA không chỉ là các điều chỉnh số mà còn đóng gói các hướng cập nhật mô hình quan trọng. Do đó, chúng tôi đưa ra giả thuyết rằng các không gian con gradient của các tác vụ trước đó được đại diện một cách súc tích bởi các tham số LoRA. Bằng cách học trong một không gian con trực giao với không gian con LoRA liên kết với các tác vụ trước đó, chúng ta có thể ngăn chặn sự can thiệp với các hàm mất mát của tác vụ trong quá khứ, do đó giảm thiểu việc quên thảm khốc.

Chúng tôi đề xuất O-LoRA, dần dần học các tác vụ mới theo hướng trực giao với không gian con LoRA của các tác vụ trong quá khứ trong khi cố định các tham số trước đó. Đối với mỗi tác vụ, chúng tôi giới thiệu một tập hợp các tham số LoRA được ký hiệu là {At, Bt}, trong đó A ∈ Rᵈˣʳ, B ∈ Rʳˣᵏ, và hạng r ≪ min(d, k). Chúng tôi xấp xỉ không gian con cập nhật tham số Ut cho tác vụ thứ t như không gian con được trải bởi các vector cột của At:

At = [a¹t, a²t, ..., aʳt]     (3)
Ut = span{a¹t, a²t, ..., aʳt}     (4)

Đặt Bt = [b¹t, b²t, ..., bʳt], trong đó bⁱt ∈ Bt đại diện cho các hệ số trọng số tuyến tính của các vector cột trong At.

Để đảm bảo tính trực giao giữa không gian con U và không gian con W, chúng ta cần thỏa mãn: <u, w> = 0, ∀u ∈ U, w ∈ W. (5)

Do đó, việc đạt được tính trực giao giữa các không gian con LoRA của tác vụ i (Ui) và tác vụ t (Ut) có thể được biểu diễn như:

Oi,t = AᵀiAt = 0.     (6)

Cuối cùng, mục tiêu huấn luyện của chúng tôi được định nghĩa như:

∑(x,y)∈Dt log pΘ(y|x) + λ₁ ∑ᵗ⁻¹ᵢ₌₁ Lorth(Ai, At)     (7)

Lorth(Ai, At) = ∑j,k ||Oi,t[j, k]||²     (8)

trong đó Oi,t[j, k] biểu thị phần tử tại hàng thứ j và cột thứ k của Oi,t, và λ₁ là trọng số của mất mát trực giao. Trong quá trình huấn luyện, để giảm thiểu việc quên kiến thức trong quá khứ, chúng tôi cố định các tham số LoRA trước đó {Ai, Bi|i < t}.

Theo Hu et al. (2021), chúng tôi chỉ áp dụng LoRA cho các trọng số chú ý của truy vấn (Wq) và giá trị (Wv).

Trong khi số lượng tham số LoRA tăng theo số lượng tác vụ trong quá trình huấn luyện, chúng ta có thể hợp nhất các cập nhật tương ứng với các tham số LoRA vào các tham số ban đầu để tránh lạm phát bộ nhớ GPU.

Winit := Winit + ∑ᵗᵢ₌₁ AiBi.     (9)

### 3.3 So sánh giữa O-LoRA và các phương pháp khác

Trong phần này, chúng tôi so sánh O-LoRA với các phương pháp học liên tục hiện tại khác qua một số chiều: không cần diễn tập, hiệu quả tham số, khả năng sẵn có của task-id trong quá trình suy luận, và khả năng áp dụng cho các tác vụ chưa thấy. Như được thể hiện trong Bảng 1, O-LoRA thể hiện ba lợi thế riêng biệt: thân thiện với quyền riêng tư dữ liệu, thân thiện với tham số mô hình, và thân thiện với tổng quát hóa.

Thân thiện với quyền riêng tư dữ liệu. Các phương pháp dựa trên diễn tập (de Masson D'Autume et al., 2019; Huang et al., 2021), dựa vào việc lưu trữ dữ liệu tác vụ trong quá khứ trong bộ đệm và phát lại nó trong quá trình huấn luyện, không phù hợp cho các tình huống có lo ngại về quyền riêng tư dữ liệu. Ngoài ra, khi số lượng tác vụ huấn luyện tăng lên, chi phí huấn luyện các tác vụ mới sử dụng các phương pháp dựa trên phát lại cũng tăng lên. Ngược lại, phương pháp của chúng tôi không yêu cầu lưu trữ dữ liệu lịch sử, giảm bớt lo ngại về quyền riêng tư dữ liệu. Hơn nữa, vì chúng tôi chỉ sửa đổi mất mát huấn luyện, không có chi phí huấn luyện bổ sung nào phát sinh.

Thân thiện với tham số mô hình. Nhiều phương pháp trước đây (Kirkpatrick et al., 2017; Farajtabar et al., 2020) huấn luyện toàn bộ tham số mô hình cho mỗi tác vụ, trong khi phương pháp của chúng tôi chỉ đưa ra các tham số bổ sung tối thiểu cho mỗi tác vụ. O-LoRA có yêu cầu thấp hơn về tài nguyên tính toán và bộ nhớ GPU trong quá trình huấn luyện. Ngoài ra, vì việc huấn luyện LoRA đóng băng các tham số mô hình được huấn luyện trước, nó ít có khả năng quên kiến thức thu được trong quá trình huấn luyện trước.

Thân thiện với tổng quát hóa. Các phương pháp truyền thống (Kirkpatrick et al., 2017; Chaudhry et al., 2018; Wang et al., 2022c), được thiết kế chủ yếu cho các tác vụ phân loại, thường không đạt được tổng quát hóa tốt cho các tác vụ chưa thấy do tập trung hẹp vào tác vụ cụ thể. Ngược lại, O-LoRA sử dụng điều chỉnh hướng dẫn (Wang et al., 2022b) làm mô hình huấn luyện của nó. Bằng cách kết hợp các hướng dẫn hoặc minh họa rõ ràng, mô hình có thể nắm bắt các nguyên tắc hoặc ràng buộc cơ bản của một tác vụ. Hướng dẫn rõ ràng này giúp mô hình tổng quát hóa vượt ra ngoài các ví dụ cụ thể trong dữ liệu huấn luyện, cho phép nó xử lý các tình huống chưa thấy hiệu quả hơn. Việc tích hợp chuyên môn của con người thông qua điều chỉnh hướng dẫn tăng cường khả năng tổng quát hóa của O-LoRA.

## 4 Thực nghiệm

### 4.1 Thiết lập thực nghiệm

#### 4.1.1 Bộ dữ liệu

Tiêu chuẩn CL chuẩn. Chúng tôi đánh giá cách tiếp cận của chúng tôi bằng cách sử dụng tiêu chuẩn CL cho mô hình ngôn ngữ, bao gồm năm bộ dữ liệu phân loại văn bản được giới thiệu bởi Zhang et al. (2015): AG News, đánh giá Amazon, đánh giá Yelp, DBpedia và Yahoo Answers. Chúng tôi áp dụng thiết lập CL cho mô hình T5, theo LFPT5 (Qin và Joty, 2021), và khám phá ba thứ tự khác nhau của tiêu chuẩn. Phụ lục A.2 cung cấp các chi tiết tác vụ, và các chuỗi tác vụ được sử dụng trong thực nghiệm của chúng tôi được cung cấp trong Phụ lục A.3.

Số lượng lớn tác vụ. Hiệu suất của phương pháp chúng tôi trên các chuỗi tác vụ dài hơn, tạo ra thách thức lớn hơn, được đánh giá thông qua thực nghiệm trên tiêu chuẩn học liên tục của 15 bộ dữ liệu (Razdaibiedina et al., 2023). Điều này bao gồm năm tác vụ từ tiêu chuẩn CL, bốn từ tiêu chuẩn GLUE (MNLI, QQP, RTE, SST2) (Wang et al., 2018), năm từ tiêu chuẩn SuperGLUE (WiC, CB, COPA, MultiRC, BoolQ) (Wang et al., 2019), và bộ dữ liệu đánh giá phim IMDB (Maas et al., 2011). Theo Razdaibiedina et al. (2023), chúng tôi chọn 1000 mẫu ngẫu nhiên để huấn luyện mỗi tác vụ và giữ lại 500 mẫu mỗi lớp để xác thực.

Tạo sinh các tác vụ chưa thấy. Để đánh giá tác động của cách tiếp cận của chúng tôi đối với khả năng tổng quát hóa của LLM, chúng tôi ban đầu huấn luyện LLM trên bộ dữ liệu Alpaca (Taori et al., 2023), một bộ dữ liệu điều chỉnh hướng dẫn đa tác vụ mã nguồn mở. Sau đó chúng tôi sử dụng LLM được huấn luyện trước cho việc huấn luyện tuần tự trên tiêu chuẩn CL chuẩn (Zhang et al., 2015). Tiêu chuẩn zero-shot của chúng tôi, MMLU (Hendrycks et al., 2020), bao gồm 57 chủ đề qua các lĩnh vực khác nhau như STEM, nhân văn, và khoa học xã hội, đánh giá kiến thức thế giới và khả năng giải quyết vấn đề qua các cấp độ khó khăn khác nhau.

#### 4.1.2 Thước đo

Đặt ai,j là độ chính xác kiểm tra trên tác vụ thứ i sau khi huấn luyện trên tác vụ thứ j, thước đo để đánh giá là Độ chính xác trung bình (AA), độ chính xác trung bình của tất cả các tác vụ sau khi huấn luyện trên tác vụ cuối cùng, (1/T)∑ᵀᵢ₌₁ ai,T

#### 4.1.3 Đường cơ sở

Chúng tôi đánh giá O-LoRA so với 10 phương pháp đường cơ sở. Quan trọng là, trong số các đường cơ sở này, chỉ các phương pháp dựa trên prompt là ngoại lệ; tất cả các phương pháp khác đều sử dụng khung LoRA. Sự thống nhất này trong nền tảng đảm bảo các thiết lập tham số nhất quán giữa O-LoRA và các phương pháp so sánh của nó, đảm bảo một so sánh công bằng.

• SeqFT (de Masson D'Autume et al., 2019): huấn luyện tất cả tham số mô hình trên một chuỗi tác vụ (không thêm bất kỳ chính quy hóa nào hoặc phát lại mẫu từ các tác vụ trước đó).

• SeqLoRA: các tham số LoRA kích thước cố định được huấn luyện trên một chuỗi tác vụ (không thêm bất kỳ chính quy hóa nào hoặc phát lại mẫu từ các tác vụ trước đó).

• IncLoRA: học tăng dần các tham số LoRA mới trên một chuỗi tác vụ tuần tự (không thêm bất kỳ chính quy hóa nào hoặc phát lại mẫu từ các tác vụ trước đó).

• Replay: tinh chỉnh toàn bộ mô hình với bộ đệm bộ nhớ, và phát lại mẫu từ các tác vụ cũ khi học tác vụ mới để tránh quên.

• EWC (Kirkpatrick et al., 2017): tinh chỉnh toàn bộ mô hình với mất mát chính quy hóa ngăn chặn cập nhật các tham số có thể can thiệp với các tác vụ đã học trước đó.

• LwF (Li và Hoiem, 2017): hạn chế lớp biểu diễn chung tương tự với trạng thái ban đầu của nó trước khi học tác vụ mới.

• L2P (Wang et al., 2022c): sử dụng đầu vào để động chọn và cập nhật prompt từ pool prompt theo cách instance-wise.

• LFPT5 (Qin và Joty, 2021): liên tục huấn luyện một soft prompt đồng thời học giải quyết các tác vụ và tạo ra mẫu huấn luyện, sau đó được sử dụng trong phát lại kinh nghiệm.

• ProgPrompt (Razdaibiedina et al., 2023): áp dụng soft prompt cụ thể cho từng tác vụ riêng biệt, tuần tự nối nó với các prompt đã học trước đó. Về bản chất, nó huấn luyện các mô hình cá nhân cho mỗi tác vụ, tận dụng ID tác vụ để chọn mô hình phù hợp trong quá trình suy luận.

• PerTaskFT: huấn luyện một mô hình riêng biệt cho mỗi tác vụ.

• MTL: huấn luyện một mô hình trên tất cả các tác vụ như học đa tác vụ. Phương pháp này là giới hạn trên của học liên tục.

#### 4.1.4 Chi tiết triển khai

O-LoRA là một phương pháp CL bất khả tri mô hình có thể được sử dụng với bất kỳ mô hình dựa trên transformer nào. Trong các thực nghiệm của chúng tôi, chúng tôi sử dụng hai mô hình ngôn ngữ được áp dụng bởi các dòng nghiên cứu trước đây trong CL cho NLP: mô hình T5 encoder-decoder (Raffel et al., 2020) và mô hình LLaMA decoder-only (Touvron et al., 2023). Để so sánh O-LoRA với các phương pháp CL gần đây (Wang et al., 2022c; Qin và Joty, 2021), chúng tôi sử dụng mô hình T5-large được huấn luyện trước. Để xác thực tác động của cách tiếp cận của chúng tôi đối với khả năng tổng quát hóa của LLM cho các tác vụ chưa thấy, chúng tôi sử dụng mô hình LLaMA-7B được huấn luyện trước. Tất cả kết quả thực nghiệm được báo cáo là trung bình của 3 lần chạy. Để biết thêm các thiết lập chi tiết, tham khảo Phụ lục A.1.

### 4.2 Kết quả chính

Bảng 2 trình bày một so sánh hiệu suất của O-LoRA và các phương pháp học liên tục cơ sở trên hai tiêu chuẩn CL. Theo LFPT5, chúng tôi báo cáo kết quả của ba lần chạy độc lập với các thứ tự tác vụ khác nhau trên tiêu chuẩn CL.

Kết quả trên các tiêu chuẩn học liên tục chuẩn. Trên tất cả các thứ tự tác vụ của tiêu chuẩn CL chuẩn, O-LoRA liên tục vượt trội hơn các phương pháp trước đây với biên độ đáng kể. Nhìn chung, O-LoRA đạt được cải thiện hiệu suất hơn 24% so với LFPT5, phương pháp tiên tiến nhất trước đây. Cách tiếp cận của chúng tôi thể hiện hiệu suất tương đương với học đa tác vụ và vượt trội đáng kể so với PerTaskFT, cho thấy rằng phương pháp của chúng tôi không chỉ hiệu quả tránh được việc quên thảm khốc mà còn tận dụng kiến thức từ các tác vụ trong quá khứ để học hiệu quả các tác vụ mới.

Hiệu suất với số lượng lớn tác vụ. Trên tiêu chuẩn thách thức hơn với số lượng lớn tác vụ, O-LoRA vượt trội hơn phương pháp tiên tiến nhất, LFPT5, về hiệu suất trung bình qua ba thứ tự tác vụ. Trong khi ProgPrompt hoạt động tốt hơn phương pháp của chúng tôi trong việc xử lý các tác vụ chuỗi dài, các ràng buộc vốn có của nó không thể bị bỏ qua. ProgPrompt bị ràng buộc chặt chẽ với các tác vụ mà nó được huấn luyện và dựa nhiều vào ID tác vụ trong quá trình suy luận, hạn chế khả năng tổng quát hóa và làm cho nó ít thích ứng hơn đối với LLM. Đáng chú ý là hầu hết tất cả các phương pháp học liên tục hiện tại hoạt động thấp hơn đáng kể so với PerTaskFT và MTL, cho thấy rằng học liên tục cho một số lượng lớn tác vụ vẫn là một vấn đề thách thức.

Tác động đến khả năng tổng quát hóa của LLM. Chúng tôi điều tra tác động của O-LoRA đối với khả năng tổng quát hóa của các mô hình ngôn ngữ lớn thông qua các thực nghiệm học liên tục. Chúng tôi bắt đầu với mô hình ngôn ngữ LLaMA-7B được tinh chỉnh trên bộ dữ liệu Alpaca, sau đó kiểm tra các mô hình có và không có ràng buộc O-LoRA trên tiêu chuẩn MMLU. Với MMLU là bài toán phân loại bốn lựa chọn, độ chính xác 25% tương đương với đoán ngẫu nhiên. Theo Bảng 3, các mô hình không có O-LoRA (Alpaca-LoRA-CL, Alpaca-LoRA-inc-CL) đạt được độ chính xác 23,3% và 28,6% tương ứng, tương đương với các phỏng đoán ngẫu nhiên. Ngược lại, các mô hình có O-LoRA trung bình đạt 33,6% độ chính xác, thể hiện hiệu quả của O-LoRA trong việc duy trì khả năng tổng quát hóa cho các tác vụ chưa thấy.

### 4.3 Thảo luận

O-LoRA có bảo tồn mất mát của các tác vụ trước đó trong khi huấn luyện các tác vụ mới không? Chúng tôi đánh giá hiệu quả của không gian con hạng thấp của LoRA trong việc xấp xỉ không gian con gradient của các tác vụ trước đó. Trong đánh giá của chúng tôi, chúng tôi áp dụng ràng buộc trực giao với trọng số 0,5 (λ₁ = 0,5). Để so sánh, không có ràng buộc (λ₁ = 0), các tham số LoRA mới được thêm vào cho các tác vụ mới với LoRA lịch sử, và các tham số mô hình được giữ cố định. Như Hình 3 cho thấy, ràng buộc O-LoRA giúp giữ mất mát của các mẫu trước đó ở mức thấp, chứng minh rằng ràng buộc O-LoRA hiệu quả chống lại việc quên thảm khốc.

O-LoRA ảnh hưởng đến đầu ra của mỗi lớp trong mô hình như thế nào? Chúng tôi kiểm tra sự biến đổi trong các trạng thái ẩn đối với các mẫu tác vụ trong quá khứ trong các mô hình được huấn luyện có và không có ràng buộc O-LoRA, sử dụng mô hình T5-base. Hình 4 chứng minh rằng ràng buộc O-LoRA giảm thiểu các biến đổi, do đó giảm việc quên kiến thức nội tại. Chúng tôi phát hiện rằng các lớp thấp hơn mã hóa kiến thức ngữ nghĩa chung hơn có thể được chia sẻ qua các tác vụ. Ngược lại, các lớp cao hơn mã hóa kiến thức ngữ nghĩa cụ thể cho tác vụ và thay đổi nhiều trong quá trình học tác vụ mới. Bộ giải mã có thể nắm bắt thông tin liên quan từ các biểu diễn ngữ nghĩa phong phú này, chứng minh tác động tối thiểu của phương pháp chúng tôi đối với các tác vụ trong quá khứ.

Các PLM khác nhau ảnh hưởng đến hiệu suất như thế nào? Chúng tôi đánh giá hiệu suất của các mô hình qua các kích thước tham số khác nhau (T5-base, T5-large, T5-XL) và các kiến trúc riêng biệt (T5, LLaMA) sử dụng tiêu chuẩn học liên tục chuẩn. Các phát hiện của chúng tôi như sau: 1) Trong chuỗi T5, độ chính xác trung bình của O-LoRA cải thiện khi kích thước tham số tăng lên. 2) Các kích thước mạng lớn hơn xuất hiện để chống lại việc quên thảm khốc, tiếp cận mức độ thành thạo của học đa tác vụ. 3) Đáng chú ý là, ngay cả với số lượng tham số lớn hơn trong mô hình LLaMA-7B, mô hình T5-3B đăng ký độ chính xác trung bình cao hơn. Điều này ngụ ý rằng các kiến trúc encoder-decoder có thể chống lại việc quên tốt hơn.

Hạng tối ưu r cho O-LoRA là gì? Để điều tra ảnh hưởng của tham số hạng (r) đối với hiệu suất của O-LoRA, chúng tôi tiến hành thực nghiệm sử dụng T5-Base trên tiêu chuẩn CL chuẩn. Bảng 5 trình bày kết quả của các giá trị r khác nhau. Tăng hạng r cải thiện độ chính xác trung bình của mô hình đến một mức độ nhất định. Tuy nhiên, chúng tôi quan sát thấy rằng không có sự khác biệt đáng kể về hiệu suất giữa r=2 và r=16, cho thấy rằng không gian gradient của mô hình có chiều nội tại tương đối thấp.

## 5 Nghiên cứu liên quan

### 5.1 Học liên tục

Học liên tục (Ke và Liu, 2022; Wang et al., 2023a) nhằm phát triển các thuật toán học có thể tích lũy kiến thức trên dữ liệu không ổn định. Các nghiên cứu hiện tại có thể được phân loại rộng rãi thành các cách tiếp cận dựa trên diễn tập, dựa trên chính quy hóa, và dựa trên kiến trúc. Để thảo luận sâu về học liên tục trong kỷ nguyên của các mô hình ngôn ngữ lớn, độc giả có thể tham khảo (Wang et al., 2023b).

Các cách tiếp cận dựa trên diễn tập (Lopez-Paz và Ranzato, 2017; de Masson D'Autume et al., 2019; Han et al., 2020; Bai et al., 2022) tận dụng bộ đệm bộ nhớ lưu trữ các ví dụ từ các tác vụ trước đó, huấn luyện mô hình cùng với tác vụ hiện tại. Phát lại kinh nghiệm (ER) (Rolnick et al., 2019) là một chiến lược chung được sử dụng trong các cách tiếp cận dựa trên diễn tập và phục vụ như một đường cơ sở mạnh. Tuy nhiên, việc lưu trữ và phát lại dữ liệu từ các tác vụ trước đó gây ra lo ngại về quyền riêng tư, đặc biệt khi xử lý thông tin nhạy cảm.

Các cách tiếp cận dựa trên chính quy hóa (Kirkpatrick et al., 2017; Li và Hoiem, 2017; Farajtabar et al., 2020; Smith et al., 2023) kết hợp các hạng mục bổ sung vào hàm mất mát để phạt các thay đổi trong trọng số quan trọng. Ví dụ, Hạ cấp Gradient Trực giao (OGD) (Farajtabar et al., 2020) hạn chế các tham số di chuyển trong không gian trực giao được định nghĩa bởi các gradient của các tác vụ trước đó. Tuy nhiên, OGD yêu cầu lưu trữ gradient của tất cả dữ liệu lịch sử, điều này trở nên không khả thi đối với các mô hình ngôn ngữ lớn. Một nghiên cứu khác giới thiệu C-LoRA (Smith et al., 2023) cho việc học liên tục của hình ảnh có điều kiện văn bản, chính quy hóa sự tương tự của các tham số LoRA mới với các phiên bản lịch sử, hạn chế tính dẻo dai học tập của chúng đối với các tác vụ mới.

Các cách tiếp cận dựa trên kiến trúc (Wang et al., 2023e; Razdaibiedina et al., 2023) tập trung vào việc mở rộng động khả năng mô hình hoặc cô lập các trọng số mô hình hiện tại để giảm thiểu sự can thiệp giữa các tác vụ mới và cũ. Progressive Prompts (Razdaibiedina et al., 2023) học các prompt riêng biệt cho mỗi tác vụ đến và tuần tự nối chúng với các prompt đã học trước đó. Tuy nhiên, các cách tiếp cận như vậy về cơ bản huấn luyện các mô hình chuyên gia riêng biệt cho các tác vụ khác nhau, hạn chế khả năng tổng quát hóa của chúng đối với các tác vụ chưa thấy.

Ngược lại với các phương pháp hiện tại, cách tiếp cận của chúng tôi cung cấp những lợi thế độc đáo về quyền riêng tư dữ liệu, hiệu quả tham số mô hình, và khả năng tổng quát hóa, như đã thảo luận trong các phần trước.

### 5.2 Điều chỉnh hiệu quả tham số

Điều chỉnh hiệu quả tham số (PET) (He et al., 2021) đã nổi lên như một hướng nghiên cứu quan trọng nhằm tối ưu hóa hiệu suất mô hình trong khi giảm thiểu tài nguyên tính toán và nỗ lực chú thích. Các phương pháp khác nhau đã được đề xuất để đạt được hiệu quả tham số trong điều chỉnh, bao gồm adapters (Houlsby et al., 2019), học prompt (Lester et al., 2021), LoRA (Hu et al., 2021), và tinh chỉnh các tập con của mô hình (Zaken et al., 2021). Một cách tiếp cận đặc biệt hứa hẹn là sử dụng các adapter hạng thấp, đã thể hiện hiệu quả trong việc thích ứng mô hình với các tác vụ mới với tham số bổ sung tối thiểu.

Dựa trên LoRA, chúng tôi đề xuất một kiến trúc mạng nơ-ron học liên tục hiệu quả trong nghiên cứu này. Cách tiếp cận của chúng tôi bao gồm việc xếp lớp các adapter hạng thấp trên các ma trận phép chiếu key và value của các khối transformer. Bằng cách tận dụng lợi ích của các adapter hạng thấp, chúng tôi nhằm tạo ra sự cân bằng giữa hiệu suất mô hình và hiệu quả tính toán trong bối cảnh học liên tục.

## 6 Kết luận

Trong bài báo này, chúng tôi giới thiệu O-LoRA, một cách tiếp cận mới tận dụng việc học không gian con trực giao cho việc học liên tục trong mô hình ngôn ngữ. O-LoRA một cách có hệ thống giải quyết việc quên thảm khốc bằng cách áp dụng chiến lược học tăng dần trong các không gian con trực giao. Được phân biệt bởi các cân nhắc về quyền riêng tư dữ liệu, sử dụng tham số mô hình hiệu quả, và tổng quát hóa mạnh mẽ đối với các tác vụ mới, phương pháp của chúng tôi nổi bật. Các đánh giá thực nghiệm nhấn mạnh hiệu quả của O-LoRA trong việc giải quyết các phức tạp của học liên tục.

## Hạn chế

Mặc dù phương pháp của chúng tôi đã thể hiện hiệu quả trong các đánh giá thực nghiệm, có một số hạn chế cần xem xét. Thứ nhất, hiệu suất và khả năng áp dụng của nó trong các tình huống phức tạp hơn với số lượng lớn tác vụ, chẳng hạn như hàng trăm tác vụ, cần được điều tra thêm. Ngoài ra, mặc dù phương pháp của chúng tôi không dựa vào nhận dạng tác vụ trong quá trình suy luận, nó vẫn yêu cầu nhận dạng tác vụ trong quá trình huấn luyện để huấn luyện các tham số LoRA khác nhau cho mỗi tác vụ. Khám phá các phương pháp cho việc huấn luyện bất khả tri tác vụ sẽ là một hướng tương lai có giá trị. Bằng cách giải quyết những hạn chế này, chúng ta có thể nâng cao khả năng mở rộng và khả năng bất khả tri tác vụ của cách tiếp cận chúng tôi, tiến thêm lĩnh vực học liên tục cho mô hình ngôn ngữ.

## Lời cảm ơn

Các tác giả bày tỏ lòng biết ơn đến các nhà đánh giá ẩn danh cho những bình luận sâu sắc của họ. Chúng tôi cũng muốn ghi nhận Hang Yan. Mặc dù anh ấy không tham gia trực tiếp vào nghiên cứu này, hướng dẫn cơ bản của anh ấy trong học liên tục đã là then chốt cho nghiên cứu của chúng tôi. Nghiên cứu này được tài trợ một phần bởi Chương trình Lãnh đạo nghiên cứu học thuật Thượng Hải 22XD1401100.

## Tài liệu tham khảo

[Các tài liệu tham khảo được giữ nguyên như trong bản gốc]

## Phụ lục A

### A.1 Chi tiết triển khai

Tất cả các thực nghiệm của chúng tôi trên các mô hình t5 được tiến hành trên một máy được trang bị 8 NVIDIA GeForce RTX 3090 và được triển khai sử dụng kho lưu trữ DeepSpeed. Đối với tất cả các thứ tự của luồng tác vụ, chúng tôi huấn luyện các mô hình với một epoch, tỷ lệ học không đổi là 1e-3, kích thước batch là 64 (kích thước batch là 8 mỗi GPU), tỷ lệ dropout là 0.1, và tỷ lệ decay trọng số là 0. Chỉ các giá trị của λ₁ và λ₂ khác nhau giữa thứ tự 1 đến 6. Đối với thứ tự 1, thứ tự 2 và thứ tự 3, chúng tôi đặt λ₁ = 0.5, 0.5, 0.5, 0.5, λ₂ = 0, 0, 0, 0. Đối với mỗi tác vụ trong thứ tự 4 (MNLI, CB, WiC, COPA, QQP, BoolQA, RTE, IMDB, Yelp, Amazon, SST-2, DBpedia, Agnews, MultiRC, Yahoo), chúng tôi đặt λ₁ = 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 5, 5, 5, 5, và λ₂ = 0, 0, 0.1, 0, 0, 0, 0.3, 0.1, 0.05, 0, 0.1, 0.1, 0.1, 0, 0.1 tương ứng. Đối với thứ tự 5 (MultiRC, BoolQA, WiC, MNLI, CB, COPA, QQP, RTE, IMDB, SST-2, DBpedia, Agnews, Yelp, Amazon, Yahoo), chúng tôi đặt λ₁ = 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, và λ₂ = 0, 0.1, 0, 0.1, 0.1, 0, 0.1, 0.3, 0.1, 0.5, 0, 0.1, 0, 0.1, 0.1 tương ứng. Đối với thứ tự 6 (Yelp, Amazon, MNLI, CB, COPA, QQP, RTE, IMDB, SST-2, DBpedia, Agnews, Yahoo, MultiRC, BoolQA, WiC), chúng tôi đặt λ₁ = 0.5, 0.5, 0.02, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, và λ₂ = 0, 0, 0, 0.1, 0, 0, 0.3, 0, 0.1, 0.1, 0, 0.1, 0, 0.1, 0.3 tương ứng.

### A.2 Bộ dữ liệu

Bảng 4 cho thấy chi tiết của 15 bộ dữ liệu chúng tôi đã sử dụng cho các thực nghiệm CL của chúng tôi, cùng với các thước đo đánh giá của chúng. Nhìn chung, chúng tôi đã sử dụng các bộ dữ liệu từ tiêu chuẩn CL (Zhang et al., 2015), tiêu chuẩn GLUE (Wang et al., 2018) và SuperGLUE (Wang et al., 2019), và thêm bộ dữ liệu đánh giá phim IMDB, theo (Razdaibiedina et al., 2023).

### A.3 Thứ tự chuỗi tác vụ

Chúng tôi báo cáo các thứ tự tác vụ được sử dụng cho các thực nghiệm CL của chúng tôi qua các mô hình T5 và LLaMA trong Bảng 5.

### A.4 Hướng dẫn tác vụ

Bảng 6 cho thấy các prompt cho các tác vụ khác nhau. NLI biểu thị suy luận ngôn ngữ tự nhiên, bao gồm MNLI, RTE và CB. SC biểu thị phân tích cảm xúc, bao gồm Amazon, Yelp, SST-2 và IMDB. TC biểu thị phân loại chủ đề, bao gồm AG News, Dbpedia và Yahoo.

### A.5 Kết quả chi tiết của MMLU Zero-shot
