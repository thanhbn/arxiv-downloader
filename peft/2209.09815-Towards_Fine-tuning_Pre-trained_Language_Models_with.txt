# 2209.09815.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2209.09815.pdf
# File size: 1519080 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Towards Fine-tuning Pre-trained Language Models with
Integer Forward and Backward Propagation
Mohammadreza Tayaranian1Alireza Ghaffari1Marzieh S. Tahaei1
Mehdi Rezagholizadeh1Masoud Asgharian2Vahid Partovi Nia1
1Huawei Noah‚Äôs Ark Lab, Montreal Research Center
2Department of Mathematics and Statistics, McGill University
{mohammadreza.tayaranian, alireza.ghaffari, marzieh.tahaei}@huawei.com
{mehdi.rezagholizadeh, vahid.partovinia}@huawei.com
masoud.asgharian2@mcgill.ca
Abstract
The large number of parameters of some
prominent language models, such as BERT,
makes their Ô¨Åne-tuning on downstream tasks
computationally intensive and energy hungry.
Previously researchers were focused on lower
bit-width integer data types for the forward
propagation of language models to save mem-
ory and computation. As for the backward
propagation, however, only 16-bit Ô¨Çoating-
point data type has been used for the Ô¨Åne-
tuning of BERT. In this work, we use integer
arithmetic for both forward and back propa-
gation in the Ô¨Åne-tuning of BERT. We study
the effects of varying the integer bit-width on
the model‚Äôs metric performance. Our integer
Ô¨Åne-tuning uses integer arithmetic to perform
forward propagation and gradient computation
of linear, layer-norm, and embedding layers of
BERT. We Ô¨Åne-tune BERT using our integer
training method on SQuAD v1.1 and SQuAD
v2., and GLUE benchmark. We demonstrate
that metric performance of Ô¨Åne-tuning 16-bit
integer BERT matches both 16-bit and 32-bit
Ô¨Çoating-point baselines. Furthermore, using
the faster and more memory efÔ¨Åcient 8-bit in-
teger data type, integer Ô¨Åne-tuning of BERT
loses an average of 3.1 points compared to the
FP32 baseline.
1 Introduction
Over the past few years, integration of attention
mechanisms into deep learning models led to the
creation of transformer based models. BERT (De-
vlin et al., 2018) is a prominent transformer based
language model which has shown state-of-the-art
performance in natural language processing (NLP)
tasks.
BERT requires high memory and computational
resources due to its large number of parameters.
Having large number of parameters incurs chal-
lenges for inference, training, and also Ô¨Åne-tuning
*Equal contribution.
FP32 Int32 Int16 Int8050100150JoulesCPU Energy Consumption for 1 Billion Operations
Addition
Multiplication
FP32 Int32 Int16 Int80.00.51.01.5SecondsCPU Latency for 1 Billion Operations
Addition
MultiplicationFigure 1: Energy consumption and latency of 1 billion
operations using various data types, measured on an
Intel¬ÆXeon¬ÆCPU E5-2698 v4.
of this model. Moreover, the training phase i.e. pre-
training and Ô¨Åne-tuning, involves more operations
compared to the inference. More speciÔ¨Åcally, the
training phase includes gradient computation and
weight update that make the training more compu-
tationally intensive.
One method of reducing the computational com-
plexity of deep learning models is to represent their
parameters and activations in low bit-width data
types. This reduces the memory footprint of the
model and enables more efÔ¨Åcient computations.
For instance, Figure 1 shows that low-bit integer
data types have higher throughput and better energy
consumption compared to Ô¨Çoating-point.
Previous research attempts at integer quantiza-
tion of transformer based language models were
only focused on forward propagation and the gradi-
ent computation were kept in 32-bit Ô¨Çoating-point
data type (FP32) (Bhandare et al., 2019; Kim et al.,
2021; Zafrir et al., 2019).
Furthermore, earlier efforts for using low bit-
width data types for gradient computation of trans-
former based language models has only been lim-
ited to 16-bit Ô¨Çoating-point (FP16). This method,arXiv:2209.09815v2  [cs.LG]  12 Feb 2023

--- PAGE 2 ---
known as mixed precision training (Micikevicius
et al., 2017), uses FP16 data type to represent
weights, activations and gradients while using FP32
for the weight update.
Here we present an integer Ô¨Åne-tuning method
for transformer based language models such as
BERT. Unlike previous works, we use integer data
types for both forward propagation and gradient
computation during the Ô¨Åne-tuning of BERT. More-
over, we use the dynamic Ô¨Åxed-point format to
represent Ô¨Çoating-point numbers as integers.
Our integer mapping strategy can be used along-
side Ô¨Çoating-point numbers in Ô¨Åne-tuning and in-
ference similar to mixed precision training. In our
proposed strategy, the arithmetic of all the compute
intensive layers for both forward and back propaga-
tion are performed using integer arithmetic while
other components of the model, such as nonlinear
functions and the weight updates are kept in FP32.
We use integer versions of compute intensive lay-
ers such as linear, normalization (layer-norm), and
embedding layers.
We study the effect of various bit-widths of the
integer input activation and show that increasing
the bit-width of the Ô¨Åxed-point mapping function
improves the convergence behaviour of the model.
This enables us to Ô¨Ånd the minimum bit-width re-
quired for integer Ô¨Åne-tuning of BERT.
Our Ô¨Åne-tuning experiments show that 16-bit
integer BERT is able to match the metric perfor-
mance of mixed precision FP16 and FP32 methods.
We also further reduce the bit-widths and show
that integer Ô¨Åne-tuning of BERT with 8-bit integer
weights and 12-bit integer activations has a score
drop of 3:1compared to the original performance.
To summarize, this paper makes the following
contributions:
‚Ä¢Integer Ô¨Åne-tuning of transformer based lan-
guage models that uses integer arithmetic for
both the forward and back propagation of
compute intensive layers such as linear, layer-
norm, and embedding. To the best of our
knowledge, this is the Ô¨Årst time that integer
data type is used for back propagation of pre-
trained language models.
‚Ä¢Analyzing the effect of changing the bit-width
of dynamic Ô¨Åxed-point format on the conver-
gence of Ô¨Åne-tuning. Remark 3 discusses that
the convergence behaviour of our integer Ô¨Åne-
tuning is directly related to the variance of dy-namic Ô¨Åxed-point mapping and is controlled
by the bit-width.
‚Ä¢We show that Ô¨Åne-tuning BERT using 16-
bit integer numbers is able to outperform the
FP16 mixed precision Ô¨Åne-tuning method.
The rest of this paper is structured as follows.
Section 2 brieÔ¨Çy discusses previous works in which
low bit-width data types are used for inference and
training of deep learning models. Section 3 pro-
vides details of our integer Ô¨Åne-tuning method, in-
cluding the representation mapping functions and
integer-only layers. The convergence behaviour
of the dynamic Ô¨Åxed-point mapping is studied in
Section 4 by providing empirical observations and
theoretical analysis. The Ô¨Åne-tuning experiments
on various integer and Ô¨Çoating-point setups are pre-
sented in Section 5. Finally, Section 6 concludes
the ideas proposed in this work.
2 Related Works
In this section we discuss the previous works that
use low bit-width data types in transformer based
language models. These works could be catego-
rized into two major groups. In the Ô¨Årst group,
called low-bit inference, the low bit-width data
types are used only in the forward propagation
phase to improve computational complexity and
reduce memory usage during the inference. In the
second group, also known as low-bit training, lower
bit-width data types are used for both the forward
and back propagation phases.
2.1 Low-bit Inference
Previous research on low-bit inference quantize the
model parameters and activations to speed up the
forward propagation. This category is itself divided
into quantization-aware training (QAT) and post-
training quantization (PTQ) methods.
In QAT, quantization is performed during train-
ing, allowing the model parameters to adapt to the
quantization noise. QAT relies on high-precision
FP32 gradients to train the model and adapt it to
the quantization noise.
For instance, (Zafrir et al., 2019) proposed
Q8BERT which quantizes the inference compu-
tations of all linear and embedding layers of BERT
to 8-bit integers and updates the quantization scale
with a moving average. Similarly, (Shen et al.,
2020) suggested Q-BERT which requires the com-
putation of hessian matrix for each group of param-

--- PAGE 3 ---
eters to be used in a mixed precision Ô¨Åne-tuning
with different bit-widths. (Kim et al., 2021) pro-
posed I-BERT that uses a uniform quantization
scheme to quantize input activations and weights
of various components of BERT. In I-BERT, the
quantization scaling factors are computed based on
the distribution of the training data.
Unlike QAT that performs quantization of in-
ference operations during training, Post-Training
Quantization (PTQ) methods apply quantization
to the parameters when the training is completed.
Thus, they require extra calibration or parameter
tuning to adapt the model to the quantized parame-
ters.
For instance, (Bhandare et al., 2019) quantized
the matrix multiplications of the original trans-
former architecture from (Vaswani et al., 2017) to
8-bit integer data type. Moreover, the quantization
is done only for the forward propagation and re-
quires extra calibration using validation data to tune
the boundaries of the quantization function. (Zadeh
et al., 2020) introduced GOBO which compresses
the Ô¨Åne-tuned weights of BERT by grouping them
into two categories of Gaussian and outlier. The
outlier weights are kept in FP32, while the Gaus-
sian weights are quantized to lower bits. For lower
bit-width regimes, TernaryBERT and BinaryBERT
are able to push the quantization to 2 and 1 bits
respectively (Zhang et al., 2020a; Bai et al., 2020).
They both rely on methods such as data augmenta-
tion and knowledge distillation to adapt the model
to the low-bit weights.
2.2 Low-bit Training
Research on low-bit training try to perform both
the forward propagation and gradient computation
in low-bit arithmetic. Using low precision number
formats for gradients reduces the model‚Äôs ability to
adapt the parameters to the quantization noise, but
increases the throughput and reduces the memory
footprint.
FP16 mixed precision training (Micikevicius
et al., 2017) is a common method currently for
low-bit Ô¨Åne-tuning of transformer based language
models. This method uses FP16 data type in both
forward propagation and gradient computation,
while using FP32 for the weight update. Unlike
FP16 mixed precision training, our work uses dy-
namic Ô¨Åxed-point format which allows for multiple
choices of bit-width for the data type. We show that
our 16-bit integer Ô¨Åne-tuning method outperformsFP16 mixed precision training in terms of metric
score.
Using integer data types in the training of deep
learning models has been previously studied for
the computer vision tasks. For instance, (Zhang
et al., 2020b) quantized the input activations, gradi-
ents and parameters of the linear layers for various
convolutional neural networks (CNN). Similarly,
(Zhao et al., 2021) adapted the quantization pa-
rameters by detecting the distribution of the gra-
dients in the channel dimension. In both these
works the quantization error is measured during
training and is used to adjust the quantization scale,
whereas our method does not require any informa-
tion about distribution of data or gradients. (Zhu
et al., 2020) applied a quantization scheme to train
CNN architectures with ‚Äúdirection sensitive gradi-
ent clipping‚Äù and learning rate scaling to control
the quantization error of gradients. Our integer
Ô¨Åne-tuning method does not require gradient clip-
ping and can follow the same loss trajectory as
the Ô¨Çoating-point baseline with the same hyper-
parameters. Our proposed method improves upon
(Ghaffari et al., 2022) which uses dynamic Ô¨Åxed-
point format for integer training of deep learning
models. Unlike (Ghaffari et al., 2022), our work
studies various bit-widths for both weights and ac-
tivations to Ô¨Ånd the minimum bit-width required
for Ô¨Åne-tuning BERT. Furthermore, we study in-
teger training method on large language models
where low-bit quantization is known to be a chal-
lenging task (Bondarenko et al., 2021). To the best
of our knowledge, this is the Ô¨Årst time where inte-
ger numbers are used for the back propagation of
transformer based language models.
3 Methodology
3.1 Representation Mapping
We use the dynamic Ô¨Åxed-point format
(Williamson, 1991) to map the Ô¨Çoating-point
numbers to integer data type. This format, also
known as block Ô¨Çoating-point, maps Ô¨Çoating-point
numbers to blocks of integer numbers, with
each block having its unique scale. For more
information on various number formats refer to
Appendix A.
We use a linear Ô¨Åxed-point mapping function
to map Ô¨Çoating-point numbers to integer numbers.
The linear Ô¨Åxed-point mapping converts a Ô¨Çoating-
point tensor Fto a tensor of integers and a single
scale factor.

--- PAGE 4 ---
Linear Fixed-point
Mapping  Float  
Parameter
Integer  
Input ActivationInput Activation
ScaleFloat  
Input ActivationInteger  
Parameter
Matrix
Multiplication  
Non-linear Inverse
Mapping  
OutputParameter  
ScaleAdd Unpack Floating-
point  
Float T ensorSign
Mantissa
Exponent Tensor ScaleSigned b-bit
Integer T ensorb-bit
Rounding  
maximum  Figure 2: Forward propagation operations in an integer-only linear layer. Green boxes use integer arithmetic and
red boxes use Ô¨Çoating-point data type. Here, the integer output is generated using an integer matrix multiplication
and the output scale is generated by a single add operation. The bottom panel shows the linear Ô¨Åxed-point mapping
for the input tensors, that are the input activation and the parameter tensor in this Ô¨Ågure.
The integers are obtained by rounding the
Ô¨Çoating-point mantissas. The scale is the maximum
of the Ô¨Çoating-point exponents of F. The bottom
section of Figure 2 shows the internal operations
of the linear Ô¨Åxed-point mapping.
To map the Ô¨Åxed-point numbers to Ô¨Çoating-point,
a non-linear inverse mapping function is used. The
inverse mapping converts integer numbers into nor-
malized Ô¨Çoating-point mantissas and packs each
integer with its corresponding scale into a Ô¨Çoating-
point number.
Details of the representation mapping functions
are provided in (Ghaffari et al., 2022). Our method-
ology differs in that it includes various bit-widths
for both weights and activations for the Ô¨Åne-tuning
of transformer based language models. We exploit
this mapping strategy to explore various bit-widths
for weights and activations in order to Ô¨Ånd the min-
imum bit-width for Ô¨Åne-tuning the model.
3.2 Integer Fine-tuning
Our method uses integer arithmetic for weights,
activations and gradients, while the weight update
is kept in FP32. Moreover, our proposed BERT
setups use integer-only versions for all the linear,
layer-norm and embedding layers in which internal
operations are performed with integer arithmetic.
3.2.1 Linear Layer
Figure 2 depicts a high-level view of forward prop-
agation operations of the integer-only linear layer.
All the parameters and activations of the layer areÔ¨Årst mapped to dynamic Ô¨Åxed-point using the linear
Ô¨Åxed-point mapping function. In the case of linear
layer, the integer parameters and input activations
are then sent to an integer matrix multiplication
function to generate the integer output. If needed,
the integer output could be mapped back to Ô¨Çoating-
point to be used by other layers of the model using
the non-linear inverse mapping.
For back propagation, the gradients of the param-
eters and input activations are also computed using
integer arithmetic. Using integer matrix multipli-
cation, the output gradients are multiplied by input
activations and parameters to compute the gradi-
ents. Since the weight update is performed in FP32,
the integer gradients and their scales are passed to
the non-linear inverse mapping to be mapped to
FP32.
3.2.2 Layer-norm
The layer normalization or layer-norm performs
the following operation on its input X(Ba et al.,
2016):
X p
2++. (1)
Hereandare the weight and bias parameters,
andandare input standard deviation and mean
respectively. For the forward propagation of inte-
ger layer-norm we map Xto dynamic Ô¨Åxed-point
format and compute andusing integer arith-
metic. Note that multiplication to and addition
withare also performed using integer arithmetic.

--- PAGE 5 ---
8 910 11 12 13 14 15 16
Dynamic Fixed-point bit-width505560657075F1 ScoreBERT on SQuAD v2.0 Dataset
Dynamic Fixed-point
FP32 BaselineFigure 3: F1 score of Ô¨Åne-tuning BERT using b-bit gra-
dients, and activations on SQuAD v2.0 dataset. For the
8-bit and 9-bit Ô¨Åxed-point bit-widths, we use 12-bit in-
put activations.
Moreover, the back propagation also uses integer
arithmetic to compute the gradients for the input,
, and.
3.2.3 Embedding Layer
The embedding layer is a lookup table that stores
embeddings. The layer takes a list of indices as
input and returns the list of corresponding embed-
dings for each index. The integer embedding layer,
handles integer embeddings and needs less mem-
ory footprint to store these values. For the back
propagation, the embedding layer applies the out-
put integer gradients directly to each corresponding
row of the lookup table.
4 Convergence Behaviour of Dynamic
Fixed Point Mapping
4.1 Empirical Observations
Figure 2 shows that the bit-width, b, is controlled by
adjusting the number of rounded bits in the round-
ing function. Here we study the effect of changing
the integer bit-width on the metric performance of
the model.
The motivation of varying the bit-width of the dy-
namic Ô¨Åxed-point is to control the variance induced
by the linear Ô¨Åxed-point mapping. Our experiments
show that using dynamic Ô¨Åxed-point with a bit-
width of 10 achieves the same performance as the
FP32 Ô¨Åne-tuning method. Figure 3 demonstrates
the F1 score of Ô¨Åne-tuning BERT on SQuAD v2.0
dataset against the Ô¨Åxed-point bit-width. Note that
the Ô¨Åxed-point arithmetic with a bit-width higher
8 910 11 12 13 14 15 16
Input Activation bit-width505560657075F1 ScoreBERT with 8-bit Dynamic Fixed-point
on SQuAD v2.0 DatasetFigure 4: F1 score of Ô¨Åne-tuning BERT using 8-bit
weights and gradients, with varying input activation bit-
width on SQuAD v2.0 dataset. Note that Remark 3
justiÔ¨Åes this experiment using the variance of b-bit dy-
namic Ô¨Åxed-point mapping.
than 10 bits is able to closely match the F1 score of
the FP32 baseline, that is indicated by the red line
in the Ô¨Ågure. Also note that in our experimental
setup for the 8-bit dynamic Ô¨Åxed-point format, we
use 12-bit input activations to close the F1 score
gap with the FP32 baseline. The reason for using
higher bit-width input activations is that we ob-
served 8-bit activation dramatically reduces the F1
score. Figure 4 shows the effect of input activa-
tion bit-width on the F1 score when the weights
are 8-bit integers. Changing the bit-width of the
input activation from 8 bits to 12 bits signiÔ¨Åcantly
increases the F1 score. Increasing the input activa-
tion bit-width beyond 12 bits has a negligible effect
on the F1 score, conÔ¨Årming that 12 bits is the mini-
mum required bit-width of the input activations for
this application with 8-bit integer weights.
4.2 Theoretical Analysis
Here, we study the effect of varying dynamic Ô¨Åxed-
point mapping bit-width on the stochastic gradient
descent method. The goal is to show the relation
of weight and activation bit-widths on the conver-
gence of integer training. Let us consider the fol-
lowing simpliÔ¨Åed weight update equation
wk+1=wk+ ^g(wk;k); (2)
where ^g(wk;k)is the dynamic Ô¨Åxed-point gradi-
ent and is the learning rate during the Ô¨Åne-tuning
phase. Furthermore, we also consider the following
common assumptions in sequel.

--- PAGE 6 ---
Assumption 1 (Lipschitz-continuity). The loss
functionL(w)is continuously differentiable and its
gradients satisÔ¨Åes the following inequality where
L>0is the Lipchitz constant
L(w)6L( w)+rL( w)>(w w)
+1
2Ljjw wjj2
2;
8w;w2Rd: (3)
Assumption 2. (i)L(wk)is bounded. (ii) b-bit
dynamic Ô¨Åxed-point gradients ^g(wk;k)is an unbi-
ased estimator of the true gradients of the loss func-
tionrL(wk)>Ekf^g(wk;k)g=jjrL(wk)jj2
2=
jjEkf^g(wk;k)gjj2
2;and (iii) with the b-bit dy-
namic Ô¨Åxed-point gradients i.e. ^g(wk;k), there
exist scalars M>0,MV>0,Mq>0and
Mq
V>0such that for all iterations of SGD
Vkf^g(wk;k)g
6M+Mq+ (MV+Mq
V)jjrL(wk)jj2
2:
WhereMqandMq
Vdenote the added variance of
b-bit dynamic Ô¨Åxed-point mapping on the true gra-
dient variance. Also note that in order for Assump-
tion 2 (i) to hold true, we use stochastic rounding
for back propagation.
Suppose Assumption 1 andAssumption 2 are
true, then inequality (4)follows from (Ghaffari
et al., 2022, Remark 2)
EkfL(wk+1)g L (wk)
6 (1 1
2L(MG+Mq
G))jjrL(wk)jj2
2
+1
22L(M+Mq);
withMG:= 1 +MVandMq
G:= 1 +Mq
V;
(4)
which shows the effect of added variance of Ô¨Åxed
point mapping, i.e. Mq
VandMq, on each step of
the optimizer.
Remark 1. In inequality (4), the Ô¨Årst term,
 (1 1
2L(MG+Mq
G))jjrL(wk)jj2
2contribute
to decreasing the loss Lwhile the second term,
1
22L(M+Mq), prevents it. Also note that when
MqandMq
Gare increased, they negatively affect
the descent of the loss L. This means for a good
convergence behaviour, representation mappingvariance bounds, i.e. MqandMq
G, must be con-
trolled.
Remark 2. For dynamic Ô¨Åxed-point mapping with
b-bit integers, the representation mapping variance
bounds i.e.MqandMq
G, are closely related to the
bit-widthb. Here, we study these two constants for
a linear layer. Let us denote ^Aas theb-bit dynamic
Ô¨Åxed-point version of tensor Aand^aijas itsijth
element. We can relate ^aijandaijwith an error
termsuch as ^aij=aij+A
ij. For a linear layer
^Y=^X^W, the computation of the b-bit dynamic
Ô¨Åxed-point gradients in the back propagation is
^C=@^L
@^W=@^Y
@^W@^L
@^Y=^X>@^L
@^Y=^X>^G:(5)
It is of interest to Ô¨Ånd the relation between ^C=
^X>^Gin the integer back propagation and the true
gradients C=X>G. We can derive the variance
for each element ^cijby expanding the error terms
,
Vf^cijg=V(NX
n=1^xni^gnj)
=V(NX
n=1(xni+X
ni)(gnj+G
nj))
6V(KX
n=1xnignj)
+2
GEfjjX>
i:jj2
2g+2
XEfjjG:jjj2
2g
+N2
X2
G
=Vfcijg+2
GEfjjX>
i:jj2
2g
+2
XEfjjG:jjj2
2g+N2
X2
G:
(6)
In inequality (6),2
G= maxi;j(VfG
i;jg)and
2
X= maxi;j(VfX
i;jg). Also notejjX>
i:jj2
2=PJ
jx2
jidenotes the squared L-2 norm of the ithrow
ofX>andjjG:jjj2
2=PI
ig2
ijdenotes the squared
L-2 norm of the jthcolumn ofG. Furthermore, by
deÔ¨Åning
(
Mq:=2
G(EfjjX>
i:jj2
2g+N2
X)
Mq
V:=2
X(7)
Equation (7)shows thatMqdepends on variance
of dynamic Ô¨Åxed-point mapping for input activa-
tions and gradients while Mq
Gonly depends on b-bit
dynamic Ô¨Åxed-point gradients variance.

--- PAGE 7 ---
QQP QNLI MNLI SST-2 STSB RTE MRPC CoLA Average
FP32 91.0/88.0 91.1 84.2 92.5 88.3 63.8 82.5/87.8 57.2 82.6
FP16 AMP 90.9/87.9 91.2 84.1 92.4 88.3 64 82.1/87.7 57.5 82.6
16-bit integer 91.0/88.0 91.2 84.2 92.5 88.3 64.5 82.3/87.6 57.7 82.7
12-bit integer 90.9/88.0 91.2 84.0 92.6 87.9 63.5 81.3/87.4 56.7 82.4
10-bit integer 90.8/87.8 91.0 84.0 92.5 87.5 62.7 78.4/85.8 57.6 81.8
8-bit integer 90.1/86.8 90.8 83.7 92.3 87 61.8 76.8/84.7 55.0 80.9
Table 1: Metric performance of integer Ô¨Åne-tuning of BERT on selected GLUE tasks. The reported metric for
QQP and MRPC is accuracy and F1 score, for QNLI, MNLI, RTE, and SST-2 is accuracy, for STSB is the Pearson-
Spearman correlation, and for CoLA is the Matthews correlation.
Proposition 1. For dynamic Ô¨Åxed-point representa-
tion of tensor ^Awithb-bit integers, the variance of
error for element isatisÔ¨Åes the following inequality
VfA
ig622(escaleA b+2): (8)
Proof .Using dynamic Ô¨Åxed-point mapping to b-bit
integers, the error A
isatisÔ¨Åes the following bound
 2escaleA(0:000001) 2|{z}
b 16A
i62escaleA(0:000001) 2|{z}
b 1
 2escaleA b+26A
i62escaleA b+2:
(9)
Thus, the inequality (8)is obtained by using
Popoviciu‚Äôs inequality on variances
VfA
ig61
4(2escaleA b+2 ( 2escaleA b+2))2
622(escaleA b+2): (10)
Remark 3. Inequality (8)shows that increasing bit-
widthbin dynamic Ô¨Åxed-point mapping reduces
the variance of the error. This conÔ¨Årms our ex-
perimental results on SQuAD v2.0 dataset that for
b > 10, F1 score can match FP32 baseline, see
Figure 3. Also note in equation (7), bothMqand
Mq
Vdepend onb-bit dynamic Ô¨Åxed-point mapping
variance of input activation 2
X. Hence, increas-
ingbfor input activations while keeping weights
in 8-bit format must improve the convergence be-
haviour. This phenomenon is also conÔ¨Årmed by
our experimental results on SQuAD v2.0 dataset
demonstrated in Figure 4.
5 Experimental Results
5.1 Experimental Setup
We Ô¨Åne-tuned BERT base on a series of down-
stream tasks to compare the performance of our in-
teger Ô¨Åne-tuning method with FP16 and FP32 Ô¨Åne-
tuning methods. FP16 AMP setup uses NVIDIA‚ÄôsSQuAD v1.1 SQuAD v2
FP32 80.5/88.0 70.6/73.8
FP16 AMP 79.9/87.6 70.6/73.9
16-bit integer 80.7/88.0 70.6/73.9
12-bit integer 79.8/87.6 70.5/73.8
10-bit integer 78.4/86.6 69.8/73.2
8-bit integer 75.6/84.5 65.5/69.2
Table 2: Metric performance of Ô¨Åne-tuning BERT on
SQuAD v1.1 and v2.0 datasets. For both datasets the
exact match metrics and F1 scores are reported.
automatic mixed precision1and the FP32 baseline
is the default implementation from Pytorch.
The model is Ô¨Åne-tuned on selected tasks of
GLUE benchmark (Wang et al., 2018), along with
the Stanford Question Answering Datasets, i.e.
SQuAD v1.1 and SQuAD v2.0 (Rajpurkar et al.,
2016).
All the Ô¨Åne-tuning setups use the same hyper-
parameters and are Ô¨Åne-tuned for the same number
of epochs. Each reported metric is the average
of Ô¨Åve runs with Ô¨Åve different random seeds to
mitigate the effects of random variation of the re-
sults. The Ô¨Åne-tuning experiments are performed
based on the Ô¨Åne-tuning scripts of the Hugging
Face library (Wolf et al., 2019). For GLUE exper-
iments the Ô¨Åne-tuning is performed for 5 epochs
and the learning rate is set to 210 5. Also, the
per-device Ô¨Åne-tuning batch-size is set to 32. Fine-
tuning BERT on SQuAD datasets is done for 2
epochs and the learning rate is 510 5and the
per-device Ô¨Åne-tuning batch-size is 12. All experi-
ments are run on eight NVIDIA V100 GPUs with
32 gigabytes of VRAM.
1https://developer.nvidia.com/automatic-mixed-precision

--- PAGE 8 ---
0 500 1000 1500 2000 2500
Fine-tuning Steps12345Fine-tuning LossBERT on SQuAD v2.0 Dataset
8-bit
16-bit
FP32 BaselineFigure 5: Integer Ô¨Åne-tuning loss trajectory of BERT
on SQuAD v2.0 dataset for 2750 iterations.
5.2 Results
The results of Ô¨Åne-tuning BERT base on GLUE
benchmark and SQuAD datasets are presented in
Table 1 and Table 2 respectively. GLUE benchmark
contains a series of downstream tasks, designed to
evaluate a diverse set of language understanding
abilities of NLP models. SQuAD datasets con-
tain a series of text passages accompanied by a
question and the task is to predict the span of the
answer in the passage. Using 16-bit integer data
type, BERT is able to either match or outperform
the FP32 performance for all tasks. The 16-bit inte-
ger BERT also shows similar or better performance
compared to the FP16 mixed precision Ô¨Åne-tuning
method. Further reducing the integer bit-width to 8,
Ô¨Åne-tuning BERT exhibits an average of 1.7 point
drop on GLUE benchmark and 4.5 point drop for
SQuAD datasets. Moreover, our experiments show
that using 10-bit and 12-bit integers has average
score drops of 0.8 and 0.3 points for GLUE tasks,
and 0.8 and 0.2 point for SQuAD datasets respec-
tively.
5.3 Loss Trajectory
Figure 5 shows the loss trajectory of integer Ô¨Åne-
tuning BERT on SQuAD v2.0 dataset using 16-bit
and 8-bit integers, along with FP32 method. The
Ô¨Åne-tuning loss trajectory of BERT using 16-bit
integer closely follows the FP32 loss trajectory. On
the other hand, when Ô¨Åne-tuning with 8-bit integer
parameters and 12-bit integer input activations, the
loss trajectory is slightly shifted, but follows the
same trend of its FP32 counterpart.6 Conclusion
We proposed an integer Ô¨Åne-tuning method for
transformer based language models using dynamic
Ô¨Åxed-point format. We used dynamic Ô¨Åxed-point
data type to represent parameters, input activations
and gradients in integer values. As a result, our
Ô¨Åne-tuning method uses integer arithmetic for the
forward and back propagation of compute intensive
layers such as linear, layer-norm and embedding
layers of BERT model. Furthermore, we studied
that increasing the bit-width of the dynamic Ô¨Åxed-
point format reduces the variance of the mapping
function and thus, improves the convergence of our
integer Ô¨Åne-tuning method. We conduct Ô¨Åne-tuning
experiments on GLUE benchmark and SQuAD
datasets to compare the metric performance of our
integer BERT with FP16 mixed precision and FP32
Ô¨Åne-tuning methods. Our experiments show that
the 16-bit integer Ô¨Åne-tuning is able to achieve the
same metric performance as the FP16 mixed pre-
cision Ô¨Åne-tuning method. In addition, Ô¨Åne-tuning
BERT with lower bit-width data types, i.e. 8-bit
integer, maintains an average drop of metric score
within 3.1 points of the FP16 setup.
Limitations
Although our integer Ô¨Åne-tuning method uses inte-
ger numbers for compute intensive layers of BERT,
integer support for non-linear layers of BERT, e.g.
softmax and GELU activation, are left for future
work.
We have shown in Figure 1 that the integer data
types are faster for the general case. However, a
direct comparison of the time and memory cost of
our integer Ô¨Åne-tuning method with the FP16 and
FP32 methods is left for future works due to lack
of access to a proper hardware with integer tensor
core support.
Despite the similarities between Ô¨Åne-tuning and
pre-training phases, they differ in key aspects of
training such as dataset size and number of epochs.
The challenges of using integer arithmetic in the
pre-training phase will be studied in the future
work.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450 .

--- PAGE 9 ---
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,
Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
2020. Binarybert: Pushing the limit of bert quanti-
zation. arXiv preprint arXiv:2012.15701 .
Aishwarya Bhandare, Vamsi Sripathi, Deepthi
Karkada, Vivek Menon, Sun Choi, Kushal Datta,
and Vikram Saletore. 2019. EfÔ¨Åcient 8-bit quantiza-
tion of transformer neural machine language trans-
lation model. arXiv preprint arXiv:1906.00532 .
Yelysei Bondarenko, Markus Nagel, and Tijmen
Blankevoort. 2021. Understanding and overcoming
the challenges of efÔ¨Åcient transformer quantization.
arXiv preprint arXiv:2109.12948 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza
Tayaranian, Masoud Asgharian, and Vahid Par-
tovi Nia. 2022. Is integer arithmetic enough
for deep learning training? arXiv preprint
arXiv:2207.08822 .
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W
Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-
only bert quantization. In International conference
on machine learning , pages 5506‚Äì5518. PMLR.
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev,
Ganesh Venkatesh, et al. 2017. Mixed precision
training. arXiv preprint arXiv:1710.03740 .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low
precision quantization of bert. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol-
ume 34, pages 8815‚Äì8821.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems , 30.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Darrell Williamson. 1991. Dynamically scaled Ô¨Åxed
point arithmetic. In [1991] IEEE PaciÔ¨Åc Rim Con-
ference on Communications, Computers and Signal
Processing Conference Proceedings , pages 315‚Äì318.
IEEE.Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R√©mi Louf, Morgan Fun-
towicz, et al. 2019. Huggingface‚Äôs transformers:
State-of-the-art natural language processing. arXiv
preprint arXiv:1910.03771 .
Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,
and Andreas Moshovos. 2020. Gobo: Quantiz-
ing attention-based nlp models for low latency and
energy efÔ¨Åcient inference. In 2020 53rd Annual
IEEE/ACM International Symposium on Microarchi-
tecture (MICRO) , pages 811‚Äì824. IEEE.
OÔ¨År Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy EfÔ¨Åcient Machine
Learning and Cognitive Computing-NeurIPS Edi-
tion (EMC2-NIPS) , pages 36‚Äì39. IEEE.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020a. Ternarybert:
Distillation-aware ultra-low bit bert. arXiv preprint
arXiv:2009.12812 .
Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu,
Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zi-
dong Du, Tian Zhi, et al. 2020b. Fixed-point
back-propagation training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 2330‚Äì2338.
Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya
Zhang, Zhenyu Gu, and Yinghui Xu. 2021. Distri-
bution adaptive int8 quantization for training cnns.
InProceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence , volume 35, pages 3483‚Äì3491.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu,
Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie
Yan. 2020. Towards uniÔ¨Åed int8 training for con-
volutional neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 1969‚Äì1979.
A Data Types
In this section we provide a brief overview of vari-
ous data types mentioned in this work.
Floating-point data type is used to represent dec-
imal fractional numbers. A binary Ô¨Çoating-point
number has three components of sign ( s), mantissa
(m), and exponent ( e). Using these components,
Ô¨Çoating-point number xis shown as:
x= ( 1)sm2e t
wheretis the precision and 0m2t 1. An-
other way of representing Ô¨Çoating-point numbers
is as
x= ( 1)s2e(d1
2+d1
4+:::+dt
2t)

--- PAGE 10 ---
wherediare binary digits of m. For FP32, exponent
and mantissa are 8 and 23 bit integer numbers.
Fixed-point is another data type for representing
fractional numbers. Unlike Ô¨Çoating-point numbers
where each mantissa is scaled using its respective
exponent, Ô¨Åxed-point uses a single scale factor for
all the numbers.
We use the dynamic Ô¨Åxed-point data type in our
integer Ô¨Åne-tuning method. Also known as block
Ô¨Çoating-point, this format uses a different scale for
each block of numbers to allow for more Ô¨Çexibility.
