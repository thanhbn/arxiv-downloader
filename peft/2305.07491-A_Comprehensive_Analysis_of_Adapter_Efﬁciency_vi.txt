# Phân tích toàn diện về hiệu quả Adapter
Nandini Mundra1;2Sumanth Doddapaneni1;2Raj Dabre3
Anoop Kunchukuttan1;2;4Ratish Puduppully5Mitesh M. Khapra1;2
1Viện Công nghệ Ấn Độ, Madras2AI4Bharat
3Viện Nghiên cứu Công nghệ Thông tin và Truyền thông Quốc gia4Microsoft
5Viện Nghiên cứu Infocomm (I2R), ASTAR, Singapore
Tóm tắt
Adapter đã được định vị như một phương pháp tinh chỉnh hiệu quả tham số (PEFT), trong đó một số lượng tối thiểu các tham số được thêm vào mô hình và tinh chỉnh. Tuy nhiên, adapter chưa được phân tích đầy đủ để hiểu liệu PEFT có chuyển thành lợi ích trong hiệu quả huấn luyện/triển khai và khả năng duy trì/mở rộng không. Thông qua các thí nghiệm rộng rãi trên nhiều adapter, tác vụ và ngôn ngữ trong các thiết lập có giám sát và chuyển giao zero-shot đa ngôn ngữ, chúng tôi cho thấy rõ ràng rằng đối với các tác vụ Hiểu ngôn ngữ tự nhiên (NLU), hiệu quả tham số trong adapter không chuyển thành lợi ích hiệu quả so với tinh chỉnh toàn bộ mô hình. Cụ thể hơn, adapter tương đối tốn kém để huấn luyện và có độ trễ triển khai cao hơn một chút. Hơn nữa, lợi ích về khả năng duy trì/mở rộng của adapter có thể đạt được bằng các phương pháp đơn giản hơn như huấn luyện đa tác vụ thông qua tinh chỉnh toàn bộ, cũng cung cấp thời gian huấn luyện tương đối nhanh hơn. Do đó, chúng tôi khuyến nghị rằng đối với các mô hình có kích thước vừa phải cho các tác vụ NLU, người thực hành nên dựa vào tinh chỉnh toàn bộ hoặc huấn luyện đa tác vụ thay vì sử dụng adapter. Mã nguồn của chúng tôi có sẵn tại https://github.com/AI4Bharat/adapter-efficiency.

1 Giới thiệu
Tiền huấn luyện tiếp theo bởi tinh chỉnh (Devlin et al., 2019; Liu et al., 2019b) là mô hình được sử dụng phổ biến nhất trong NLP, nhưng khi các mô hình tiền huấn luyện tăng kích thước, tinh chỉnh toàn bộ mô hình (tinh chỉnh toàn bộ) trở nên tốn kém. Duy trì một bản sao của mô hình cho mỗi tác vụ là tốn kém, và tinh chỉnh hiệu quả tham số (PEFT) đã trở thành một lĩnh vực nghiên cứu tích cực tập trung vào tinh chỉnh một số lượng tối thiểu tham số trong khi vẫn đạt được hiệu suất tương đương với tinh chỉnh toàn bộ. Tác giả liên hệ: Nandini Mundra (cs21s041@cse.iitm.ac.in)

Hình 1: So sánh 10 adapter khác nhau với các baseline đơn giản hơn như tinh chỉnh toàn bộ (FT) và học đa tác vụ (MTL). Trong hình trên, trục y thể hiện hiệu suất zero-shot trung bình trên tất cả các tác vụ và tất cả các ngôn ngữ. Trong hình dưới, trục y thể hiện hiệu suất tiếng Anh trung bình trên tất cả các tác vụ. Các từ viết tắt được sử dụng là- 'H' - Houlsby, 'B' - Bapna, 'HP' - Houlsby Parallel1, 'BP'- Bapna Parallel, 'PT'- Prefix Tuning, 'L'- LoRA, 'C' - Compacter, 'AD'- Adapter Drop, 'AF' - Adapter Fusion, 'ME' - MADX-en, 'MH' - MADX-hi, 'FT' - Fine-tuning, 'MTL'- Multi-task-learning.

Tinh chỉnh adapter (Houlsby et al., 2019), thường liên quan đến tinh chỉnh các lớp feedforward nhỏ được chèn vào mô hình, là phương pháp PEFT phổ biến nhất. Với số lượng tham số cần tinh chỉnh ít hơn đáng kể, adapter rất hữu ích trong các tình huống mà mô hình tiền huấn luyện quá lớn để thực hiện tinh chỉnh tất cả các tham số của nó. Hơn nữa, sự có sẵn của các framework như Adapter-hub (Pfeiffer et al., 2020a), được xây dựng trên Transformers (Wolf et al., 2020), đã giúp các nhà nghiên cứu dễ dàng thí nghiệm với các phương pháp PEFT và triển khai mô hình của họ.

Mặc dù adapter rõ ràng là hiệu quả tham số, chúng tôi lập luận rằng, trên thực tế, hiệu quả còn nhiều hơn chỉ là số lượng tham số được tinh chỉnh. Ví dụ, một mô hình hiệu quả tham số sẽ yêu cầu nhiều phép toán dấu phẩy động (FLO) hơn, do các tham số bổ sung được thêm vào và điều này sẽ ảnh hưởng đến độ trễ. Ngoài ra, số bước đến hội tụ sẽ dẫn đến tính không hiệu quả tính toán - chúng tôi thấy rằng adapter cần nhiều bước hơn để hội tụ so với tinh chỉnh toàn bộ. Mặc dù adapter có thể dễ dàng được sử dụng để mở rộng mô hình hiện có cho các tác vụ mới, hiệu quả về tổng chi phí khi kết hợp nhiều tác vụ thường không được nghiên cứu.

Do đó, chúng tôi tin rằng cần có một nghiên cứu toàn diện về adapter so với các baseline đơn giản hơn để trả lời câu hỏi sau: Adapter thực sự hiệu quả ở những gì?

Chúng tôi khuyến nghị rằng để trả lời câu hỏi này, người ta nên nhìn xa hơn số lượng tham số và xem xét các chỉ số hiệu quả khác, chẳng hạn như, (i) thời gian huấn luyện và tính toán (FLO), (ii) khả năng triển khai thông qua độ trễ suy luận (iii) và khả năng duy trì. Các nghiên cứu hiện tại đã xem xét một hoặc nhiều chỉ số trên nhưng thiếu một nghiên cứu toàn diện so sánh nhiều adapter phổ biến trên các tác vụ khác nhau qua các ngôn ngữ, đặc biệt là trong bối cảnh đa ngôn ngữ. Một baseline đơn giản hơn là học đa tác vụ (MTL) (Liu et al., 2019a), trong đó một mô hình duy nhất được huấn luyện chung cho tất cả các tác vụ thông qua các head phân loại cụ thể cho tác vụ. Hầu hết các nghiên cứu về adapter không so sánh với MTL, khiến việc có được bức tranh rõ ràng về tiện ích thực sự của adapter trở nên khó khăn.

Trong công trình này, chúng tôi cố gắng xây dựng một bức tranh rõ ràng hơn bằng cách thí nghiệm với 10 adapter khác nhau và 6 tác vụ Hiểu ngôn ngữ tự nhiên (NLU) trải rộng 11 ngôn ngữ Ấn Độ. Chúng tôi tập trung vào chuyển giao zero-shot, trong đó chúng tôi tinh chỉnh mô hình chỉ trên dữ liệu huấn luyện tiếng Anh. Chúng tôi so sánh adapter với tinh chỉnh toàn bộ và học đa tác vụ (MTL) và thấy rằng, hoàn toàn trái ngược với niềm tin phổ biến, các baseline đơn giản hơn này hiệu quả hơn theo nhiều trục. Công trình của chúng tôi cũng đặt ra một framework để đánh giá adapter theo nhiều chiều. Các phát hiện chính của công trình chúng tôi theo các chiều này, như được tóm tắt trong Hình 1 như sau:

Hiệu quả tính toán: Adapter không hiệu quả tính toán và cần trung bình 325.6% tính toán nhiều hơn (được đo bằng FLO) so với tinh chỉnh toàn bộ, chủ yếu vì chúng mất thời gian hội tụ lâu hơn 20.2%.

Overhead suy luận: Adapter chèn các lớp mới và do đó lượng tính toán cũng như kích thước của mô hình triển khai tăng lên một chút so với tinh chỉnh toàn bộ.

Khả năng duy trì và mở rộng: Chúng tôi thấy rằng thay vì thêm adapter mới cho tác vụ mới, sử dụng MTL, nơi chúng tôi kết hợp dữ liệu tác vụ mới với 10% dữ liệu của các tác vụ trước đó, không chỉ cho hiệu suất tương đương mà còn tương đương về mặt tính toán trong khi được hưởng lợi từ chuyển giao đa tác vụ. Vì MTL chỉ cần một head phân loại cụ thể cho tác vụ mới, nó có thể là một giải pháp thay thế tuyệt vời có thể duy trì và mở rộng cho adapter.

Hiệu suất tác vụ: Chúng tôi cho thấy rằng cả adapter và MTL đều có thể đạt được hiệu suất tương đương với tinh chỉnh toàn bộ trong cả thiết lập trong ngôn ngữ và zero-shot đa ngôn ngữ. Các phát hiện của chúng tôi cung cấp một bức tranh thực tế về adapter cho NLU và cho thấy rằng mặc dù chúng thực sự hiệu quả tham số, chúng gặp phải các hạn chế tính toán có thể được giải quyết bằng các phương pháp như MTL. Chúng tôi hy vọng rằng các quan sát của chúng tôi sẽ thúc đẩy các nghiên cứu sâu hơn về adapter và giúp phát triển các phương pháp PEFT giải quyết các hạn chế hiện tại của adapter.

2 Nghiên cứu liên quan
Tinh chỉnh hiệu quả tham số (PEFT): Zoph et al. (2016) là một trong những nghiên cứu sớm nhất về PEFT bằng cách cho thấy rằng tinh chỉnh một phần của mô hình tiền huấn luyện giảm yêu cầu bộ nhớ và giúp tránh overfitting. Mặc dù đơn giản, việc xác định phần nào của mô hình nên được tinh chỉnh liên quan đến tìm kiếm toàn diện. Tuy nhiên, điều này đã thúc đẩy nghiên cứu về việc chèn các thành phần có thể tinh chỉnh vào mô hình tiền huấn luyện, nổi bật nhất là các nghiên cứu về Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Hu et al., 2022) là các lớp feedforward nhỏ được chèn sau các lớp self-attention và/hoặc feedforward của mô hình Transformer (Vaswani et al., 2017). Prompt có thể học (Li and Liang, 2021), là các tham số được thêm vào key và value của các lớp attention, cũng có thể được coi là adapter thông qua một công thức đơn giản (He et al., 2022). Các nghiên cứu như compacter (Mahabadi et al., 2021) và IA3(Liu et al., 2022) tập trung thêm vào việc giảm kích thước của adapter. Mặt khác, các nghiên cứu về AdapterFusion (Pfeiffer et al., 2021), và MAD-X (Pfeiffer et al., 2020b) tập trung nhiều hơn vào khả năng học chuyển giao của adapter. Tuy nhiên, các nghiên cứu này chủ yếu tập trung vào hiệu quả tham số và bỏ qua các khía cạnh khác của hiệu quả, như thời gian huấn luyện, khả năng triển khai, khả năng duy trì, và hiệu quả chuyển giao đa ngôn ngữ. AdapterDrop (Rücklé et al., 2021) đề xuất giảm thời gian huấn luyện adapter nhưng bỏ qua các khía cạnh nêu trên, một khoảng trống mà chúng tôi lấp đầy trong bài báo này. Trong khi nghiên cứu của chúng tôi đại diện cho sự so sánh thực nghiệm về hiệu suất trong ngôn ngữ, zero-shot và thời gian hội tụ của phương pháp PEFT, học đa tác vụ, và phương pháp tinh chỉnh, nghiên cứu trước đây đã xem xét tính không ổn định của phương pháp PEFT. Chen et al. (2022) đã chứng minh tính không ổn định của PEFT liên quan đến khởi tạo trọng số, thời gian huấn luyện, và thứ tự dữ liệu huấn luyện. Họ cũng so sánh hiệu suất của phương pháp PEFT và tinh chỉnh đối với kích thước tập dữ liệu khác nhau. Theo vấn đề giao thức bị hỏng2 như được đề cập trong bài báo (Chen et al., 2022, Phần 2), chúng tôi đã sử dụng tập dev và test khác nhau cho tất cả các thí nghiệm. Ngoài việc tập trung vào quan sát rằng tinh chỉnh không thể được thay thế hoàn toàn bởi PEFT, nghiên cứu của chúng tôi cũng đã chứng minh rằng học đa tác vụ có thể là một giải pháp thay thế cho phương pháp PEFT.

Mô hình tiền huấn luyện đa ngôn ngữ: Kể từ khi giới thiệu BERT (Devlin et al., 2019), là mô hình tiền huấn luyện tận dụng dữ liệu đơn ngôn ngữ, đã có sự cải thiện đáng kể trong hiệu suất của các tác vụ NLP downstream như phân tích cảm xúc, trả lời câu hỏi và suy luận ngôn ngữ tự nhiên. Điều này tiếp theo là các mô hình tiền huấn luyện đa ngôn ngữ lớn như mô hình bất khả tri nhóm ngôn ngữ XLM-R (Conneau et al., 2020), và các mô hình cụ thể cho nhóm ngôn ngữ IndicBERT (Doddapaneni et al., 2022; Kakwani et al., 2020), IndoBERT (Koto et al., 2020), AfriBerta (Ogueji et al., 2021), v.v. Các mô hình đa ngôn ngữ cho phép chuyển giao đa ngôn ngữ, cho phép các mô hình được tinh chỉnh trên một ngôn ngữ và được đánh giá zero-shot trên các ngôn ngữ khác. Hiệu quả của chuyển giao thông qua tinh chỉnh chưa nhận được sự chú ý đúng mức, và công trình của chúng tôi tập trung vào khía cạnh này trong cả mô hình tinh chỉnh toàn bộ và PEFT.

Học đa tác vụ (MTL): MTL tập trung vào tinh chỉnh toàn bộ một mô hình cho nhiều tác vụ (Caruana, 1993) nhưng chỉ mới thấy sự áp dụng đáng kể gần đây (Wei et al., 2021; Muennighoff et al., 2022). MTL được hưởng lợi từ chuyển giao đa tác vụ, mà chúng tôi cũng phân tích trong bài báo này (x4.4). Tổng quan chung về MTL trong deep learning có thể được tìm thấy trong Ruder (2017) và Zhang et al. (2022).

3 Thiết lập thí nghiệm
Bây giờ chúng tôi mô tả các phương pháp tinh chỉnh, tác vụ, tập dữ liệu, ngôn ngữ, mô hình tiền huấn luyện, và thiết lập huấn luyện.

3.1 Phương pháp tinh chỉnh
Sau đây là các phương pháp tinh chỉnh chúng tôi thí nghiệm.

3.1.1 Phương pháp không phải Adapter
Tinh chỉnh toàn bộ (Devlin et al., 2019) là phương pháp tiêu chuẩn, trong đó tất cả các tham số được cập nhật.
Học đa tác vụ (Liu et al., 2019a) tương tự như tinh chỉnh toàn bộ, ngoại trừ việc nó sử dụng encoder chung cho tất cả các tác vụ, với mỗi tác vụ có "head" cụ thể cho tác vụ.

3.1.2 Phương pháp Adapter
Houlsby Adapter (Houlsby et al., 2019) liên quan đến việc chèn các lớp feedforward bottleneck bổ sung, sau các lớp con self-attention và FFN. Chúng tôi thí nghiệm với cả hai, sequential và parallel (Houlsby sequential và Houlsby parallel) adapter (He et al., 2022).

Bapna Adapter (Bapna and Firat, 2019) chèn adapter chỉ sau lớp con FFN. Chúng tôi lại sử dụng cả hai phiên bản sequential và parallel (Bapna sequential và Bapna parallel).

LoRA (Hu et al., 2022) chèn các ma trận low-rank có thể huấn luyện cho các ma trận query và value trong khối self-attention để xấp xỉ các cập nhật trọng số.

Compacter (Mahabadi et al., 2021) điều chỉnh trọng số của mạng neural sử dụng các lớp adapter hypercomplex low-rank compact.

Prefix-Tuning (Li and Liang, 2021) được lấy cảm hứng từ prefix văn bản. Ở đây, k vector prefix có thể huấn luyện được thêm vào trước Keys (K) và values (V) trong khối self-attention.

MAD-X (Pfeiffer et al., 2020b) là phương pháp cho học chuyển giao đa ngôn ngữ tiền huấn luyện adapter cụ thể cho ngôn ngữ cho kiểm tra đa ngôn ngữ và adapter cụ thể cho tác vụ cho tác vụ đích.

AdapterFusion (Pfeiffer et al., 2021) sử dụng adapter được huấn luyện trên các tác vụ khác cho học chuyển giao như các lớp bổ sung trong mô hình cho tác vụ downstream. Lớp fused được huấn luyện cho tác vụ đích.

AdapterDrop (Rücklé et al., 2021) nhằm giảm chi phí tính toán của việc huấn luyện adapter bằng cách loại bỏ ngẫu nhiên một tập con các adapter trong mỗi lần lặp huấn luyện.

Trong khi LoRA và prefix-tuning ban đầu không được coi là adapter, He et al. (2022) đã chỉ ra rằng chúng có thể được tái công thức như adapter và do đó tất cả các phương pháp PEFT chúng tôi nghiên cứu trong bài báo này thực chất là adapter.

3.2 Tác vụ, tập dữ liệu và ngôn ngữ
Chúng tôi tập trung vào 6 tác vụ hiểu ngôn ngữ tự nhiên đa ngôn ngữ từ benchmark IndicXTREME (Doddapaneni et al., 2022) trải rộng 18 ngôn ngữ từ 4 họ ngôn ngữ. Các tác vụ này có thể được phân loại rộng rãi thành phân loại câu (4), phân loại token (1), và trả lời câu hỏi (1). Chúng tôi đưa ra tổng quan trong Bảng 1, bao gồm kích thước corpus và chỉ số (Accuracy hoặc F1) được sử dụng để đánh giá. Trừ khi được đề cập rõ ràng, chúng tôi chỉ huấn luyện và xác thực trên dữ liệu tiếng Anh và đánh giá trên tập test tiếng Anh (có giám sát/trong ngôn ngữ) cũng như tập test ngôn ngữ Ấn Độ trong IndicXTREME (zero-shot). Vui lòng tham khảo Phụ lục 6.1 để biết chi tiết về tác vụ và ngôn ngữ.

3.3 Mô hình tiền huấn luyện
Chúng tôi chủ yếu thí nghiệm với IndicBERT v2 (Doddapaneni et al., 2022) được huấn luyện trên corpus IndicCorp v2 và hỗ trợ 23 ngôn ngữ Ấn Độ và tiếng Anh. Nó được huấn luyện với mục tiêu Masked Language Modeling (MLM) (Devlin et al., 2019). Chúng tôi cũng thực hiện ablation với các phiên bản BASE và LARGE của XLM-R (Conneau et al., 2020) trên tập con ngôn ngữ được chọn.

Tiền huấn luyện adapter ngôn ngữ MAD-X được thực hiện sử dụng tập dữ liệu IndicCorp v2 (Doddapaneni et al., 2022) với mục tiêu MLM cho 11 ngôn ngữ Indic và tiếng Anh với 6.5M câu được lấy mẫu cho mỗi ngôn ngữ.

3.4 Chi tiết huấn luyện
Tất cả mô hình được huấn luyện với Adapter-hub (Pfeiffer et al., 2020a). Tất cả thí nghiệm được thực hiện trên GPU Nvidia A100-SXM4 40GB và kết quả được báo cáo bằng cách thực hiện một lần chạy duy nhất. Chúng tôi sử dụng các thiết lập được khuyến nghị/mặc định trong Adapter-hub nhưng khi có thể, chúng tôi thực hiện điều chỉnh siêu tham số trên tập phát triển để xác định siêu tham số tối ưu. Bảng 2 đưa ra không gian tìm kiếm và siêu tham số hiệu suất tốt nhất cho Houlsby, Bapna, LoRA và Prefix-Tuning. Đối với MAD-X, chúng tôi đã sử dụng cấu hình mặc định như trong Adapter-hub cho cả adapter ngôn ngữ và tác vụ, như được thể hiện trong Bảng 2. Đối với Adapter-fusion, chúng tôi đã huấn luyện mỗi adapter tác vụ theo kiểu ST-A (adapter tác vụ đơn) (Pfeiffer et al., 2021).

Đối với tất cả các tác vụ sử dụng mô hình IndicBERT, chúng tôi huấn luyện mô hình tối đa 50 epoch với độ kiên nhẫn dừng sớm 3 epoch. Chúng tôi sử dụng 2,000 bước khởi động cho tất cả các tác vụ và thiết lập, ngoại trừ MTL, nơi chúng tôi sử dụng 20,000 bước khởi động do kích thước tăng của dữ liệu huấn luyện. Để so sánh công bằng qua tất cả các thiết lập, chúng tôi sử dụng kích thước batch là 32 ví dụ với tốc độ học 3e-5 và weight decay 0.1. Đối với MTL, chúng tôi thấy rằng weight decay 0.01 cho kết quả tốt nhất. Đối với tất cả các thí nghiệm, FLO được báo cáo được cung cấp bởi thư viện HF transformers (Wolf et al., 2020).

4 Kết quả
Bây giờ chúng tôi báo cáo kết quả so sánh các khía cạnh hiệu quả khác nhau của phương pháp adapter và không adapter. Bảng 3 và 5 tương ứng thể hiện kết quả trong ngôn ngữ và đa ngôn ngữ (huấn luyện trên tiếng Anh và kiểm tra trên Indic) được lấy trung bình qua các ngôn ngữ Indic. Xem Phụ lục 6.2 để biết hiệu suất từng ngôn ngữ. Chúng tôi trình bày các quan sát chính trong các phần con sau.

4.1 Hiệu quả tham số
Adapter hiệu quả tham số, nhưng không có adapter nào là tốt nhất: Rõ ràng là không có adapter nào thực hiện tốt nhất trong tất cả các tác vụ. Quan sát này đúng trong cả thiết lập trong ngôn ngữ và đa ngôn ngữ, nơi một phương pháp thực hiện tốt nhất trong thiết lập trong ngôn ngữ nhưng có thể không phải là tốt nhất trong thiết lập đa ngôn ngữ. Compacter và LORA liên tục cho hiệu suất thấp nhất, có thể do số lượng tham số nhỏ mà chúng tinh chỉnh (chúng chỉ thêm 0.2% - 0.3% tham số có thể điều chỉnh vào mô hình). Mặt khác, Adapter Fusion, Prefix Tuning, và MADX thêm từ 1.1% đến 7.9% tham số có thể điều chỉnh nhưng vẫn thực hiện kém so với Houlsby adapter, chỉ thêm 0.9% tham số. Nói chung, chúng tôi khuyến nghị Houlsby adapter vì nó có xu hướng thực hiện tốt qua nhiều tác vụ và ngôn ngữ trung bình.

4.2 Hiệu quả tính toán
Chúng tôi tính tổng số FLO cho tất cả phương pháp cho tất cả tác vụ và báo cáo phần trăm tăng tương đối so với tinh chỉnh toàn bộ trong Bảng 4. Chi tiết cụ thể tác vụ về hội tụ mô hình và FLO tuyệt đối (Bảng 8) có sẵn trong Phụ lục.

Tinh chỉnh toàn bộ là nhanh nhất với biên độ đáng kể. Trong khi các phương pháp adapter hiệu quả tham số, chúng không hiệu quả tính toán khi tinh chỉnh. Trên thực tế, chúng tiêu thụ nhiều FLO hơn để hội tụ và đạt hiệu suất tương đương với tinh chỉnh toàn bộ. AdapterDrop (hàng 8 trong Bảng 3) thể hiện mức tăng FLO ít nhất (97.6%) nhưng cũng gặp phải hiệu suất giảm. MAD-X (hàng 10, 11) tốn kém nhất (1042.5%-1025.7%) về FLO nhưng vẫn cho kết quả kém so với tinh chỉnh toàn bộ. Adapter thực hiện tốt nhất (Houlsby, hàng 1) cũng rất tốn kém về mặt tính toán. Các kết quả này thể hiện rõ ràng rằng adapter rất tốn kém về mặt tính toán trong khi đạt hiệu suất tương đương hoặc tệ hơn so với tinh chỉnh toàn bộ.

MTL là giải pháp thay thế hiệu quả chi phí cho adapter vì nó chỉ sử dụng 20% FLO nhiều hơn so với tinh chỉnh toàn bộ trong khi đạt hiệu suất tương đương với adapter tốt nhất (Houlsby cho 83.9% & MTL cho 83.4%). Hơn nữa, MTL thể hiện hiệu suất đa ngôn ngữ trung bình tốt nhất so với adapter cũng như tinh chỉnh toàn bộ. Cần lưu ý rằng MTL có lợi đáng kể cho tác vụ paraphrasing thông qua chuyển giao đa tác vụ, thể hiện mức tăng hiệu suất 16.9% accuracy so với tinh chỉnh toàn bộ trong thiết lập đa ngôn ngữ (thí nghiệm trong các phần sau cho thấy paraphrasing được hưởng lợi từ tác vụ NLI). Do đó, nếu tập hợp đầy đủ các tác vụ cần hỗ trợ được biết trước, MTL đơn giản hơn và tương đương với adapter trong hiệu suất downstream, trong khi hiệu quả chi phí hơn. Sanh et al. (2022) cho thấy MTL cho phép tổng quát hóa tác vụ zero-shot, nâng cao thêm tính hấp dẫn của MTL so với adapter.

4.3 Overhead suy luận
Bảng 3 cũng cho thấy mức tăng thời gian suy luận cho các phương pháp khác nhau so với tinh chỉnh toàn bộ. MTL không thêm bất kỳ overhead nào so với tinh chỉnh toàn bộ vì không có tham số mới nào được thêm vào mô hình. Mặt khác, adapter có overhead không tầm thường trong thời gian suy luận do tham số bổ sung. Phương pháp Bapna parallel và LoRA cho thấy mức tăng thời gian suy luận ít nhất (lần lượt là 21.2% và 23.1%), vì chúng là adapter parallel. Bapna parallel có thời gian suy luận ít hơn Houlsby parallel vì nó có gần như một nửa số tham số. Phương pháp adapter fusion có thời gian suy luận cao nhất vì nó kết hợp tất cả sáu adapter tác vụ và có lớp fused bổ sung. Nó cũng có số lượng tham số bổ sung tối đa. Mặc dù Compacter có số lượng tham số ít nhất, thời gian suy luận của nó nhiều hơn 100.5% so với tinh chỉnh vì các ma trận trọng số hypercomplex low-rank compact được chuyển đổi thành ma trận high-rank thông qua tích Kronecker. Các ma trận high-rank này thực sự được sử dụng trong quá trình forward pass và quy trình hai bước này làm chậm suy luận.

4.4 Khả năng duy trì và mở rộng
Ưu điểm chính của adapter là khả năng 'plug-and-play' module, do đó giúp dễ dàng mở rộng mô hình tiền huấn luyện cho các tác vụ mới mà không cần tạo bản sao cho tác vụ mới hoặc ảnh hưởng đến hiệu suất trên các tác vụ khác. Điều này giảm yêu cầu bộ nhớ tại thời gian suy luận và làm cho hệ thống modular hơn, có thể duy trì và mở rộng. Chúng tôi đã thấy rằng các mô hình MTL cung cấp hiệu suất tương tự mà không có tham số bổ sung và với chi phí tính toán thấp hơn so với adapter. Để xem liệu chúng cũng có thể dễ dàng mở rộng, chúng tôi thí nghiệm với thiết lập sau.

Chúng tôi giữ lại một tác vụ (tác vụ đích) và tinh chỉnh mô hình tiền huấn luyện trên các tác vụ còn lại (tạo ra mô hình MTL 1). Tiếp theo, chúng tôi tiếp tục tinh chỉnh mô hình trên tác vụ đích cũng như 10% dữ liệu từ các tác vụ mà mô hình đã thấy. Một mẫu từ các tác vụ cũ được bao gồm trong hỗn hợp tinh chỉnh để tránh catastrophic forgetting (McCloskey and Cohen, 1989; French, 1999). Để so sánh, chúng tôi cũng thực hiện tinh chỉnh tiếp tục chỉ trên tác vụ đích (mô hình: MTL +tgt) cũng như tinh chỉnh trên tất cả các tác vụ có sẵn (mô hình: MTL).

Kết quả của các thí nghiệm này được thể hiện trong Bảng 6 cho thiết lập đa ngôn ngữ (và Bảng 9 trong Phụ lục cho thiết lập trong ngôn ngữ). Chúng tôi thấy rằng hiệu suất của tác vụ đích có thể so sánh với cả tinh chỉnh toàn bộ và MTL với tất cả các tác vụ. Do đó, các tác vụ mới có thể được thêm vào mô hình MTL hiện tại trong khi vẫn giữ hiệu suất tương tự như full FT hoặc MTL. Hơn nữa, chúng tôi thấy rằng mô hình MTL +tgt+old cũng giữ hiệu suất cho các tác vụ cũ. Chúng tôi cũng thấy rằng nếu dữ liệu mẫu từ các tác vụ đã hỗ trợ không được sử dụng, mô hình gặp phải catastrophic forgetting (mô hình: MTL +tgt). Do đó, một sự điều chỉnh đơn giản của MTL có thể hỗ trợ nhiều tác vụ một cách có thể mở rộng.

Chi phí tính toán tinh chỉnh cho MTL +tgt+old là tổng chi phí tính toán cho (a) tinh chỉnh MTL 1 và (b) tinh chỉnh tiếp tục cần thiết để mở rộng mô hình cho tác vụ đích. Trong Bảng 6, cột "% "FLO" báo cáo phần trăm tăng trong tổng FLO (tổng của (a) và (b)) so với tổng FLO tinh chỉnh (tức là tổng FLO tinh chỉnh trên tất cả tác vụ). Như quan sát được, giữ lại tác vụ sentiment, và sau đó học liên tục tác vụ sentiment cùng với 10% dữ liệu của các tác vụ hiện tại chỉ mất 2.3% FLO tương đối nhiều hơn. Chi phí tối đa được thực hiện bởi tác vụ NER với 68.2% FLO tương đối nhiều hơn. Giữ lại một tác vụ và sau đó thêm tác vụ được giữ lại trung bình mất 27.4% FLO tương đối nhiều hơn, trong khi thêm tất cả các tác vụ cùng lúc mất 20.2% FLO tương đối nhiều hơn. Tuy nhiên, điều này vẫn hiệu quả chi phí hơn so với các phương pháp adapter thực hiện tốt nhất. Ví dụ, Houlsby adapter yêu cầu khoảng 311% tính toán nhiều hơn so với tinh chỉnh toàn bộ. Do đó, chúng tôi thấy khả năng duy trì của MTL hiệu quả chi phí.

Tuy nhiên, hiệu suất đa ngôn ngữ trung bình cho khả năng duy trì MTL (như được thể hiện trong Bảng 6), hơi tăng do việc bao gồm tác vụ paraphrase. Nếu hiệu suất MTL trung bình được tính toán mà không có tác vụ paraphrase (tức là chỉ xem xét năm tác vụ còn lại), một sự giảm nhỏ trong hiệu suất được quan sát.

4.5 Ảnh hưởng của kích thước mô hình
Để nghiên cứu thêm về ảnh hưởng của kích thước mô hình lên các adapter khác nhau, chúng tôi thí nghiệm với hai mô hình tiền huấn luyện khác nhau được huấn luyện trên cùng dữ liệu tiền huấn luyện nhưng chỉ khác nhau về kích thước mô hình. Cụ thể, chúng tôi so sánh các mô hình XLMR-base và XLMR-large (Conneau et al., 2020) có lần lượt 270M và 550M tham số. Chúng tôi đánh giá các adapter trên các tác vụ XNLI, XQuAD và NER từ benchmark XTREME (Hu et al., 2020). Chúng tôi sử dụng tập dữ liệu tiếng Anh để huấn luyện và kiểm tra hiệu suất zero-shot đa ngôn ngữ trên 14 ngôn ngữ cho XNLI và WikiANN và 11 ngôn ngữ cho XQuAD.

Kết quả được thể hiện trong Bảng 7. Chúng tôi có thể thấy rằng khi kích thước mô hình tăng, thời gian thích ứng tương đối so với thời gian tinh chỉnh toàn bộ giảm. Do đó, đối với các mô hình ngôn ngữ lớn, chúng tôi có thể thấy xu hướng adapter ngày càng hiệu quả chi phí. Thực tế, nghiên cứu gần đây về các mô hình ngôn ngữ lớn đã cho thấy adapter có triển vọng (Yong et al., 2022). Tuy nhiên, các mô hình lớn hơn vẫn cần tính toán nặng và triển khai chúng vẫn thách thức. Trong trường hợp này, có một dòng nghiên cứu chưng cất LLM sau đó có thể được tinh chỉnh (Ganesan et al., 2021). Vì adapter không có nhiều hiệu quả tính toán trong các mô hình nhỏ hơn, tinh chỉnh toàn bộ hoặc MTL là những đối thủ xuất sắc.

4.6 Điểm mấu chốt
Hình 1 cho thấy tóm tắt thống nhất về hiệu suất tác vụ và tính toán tinh chỉnh cần thiết cho các phương pháp khác nhau được thảo luận trong bài báo. Tóm tắt các quan sát đã thảo luận trước đây, chúng tôi thấy rằng MTL vượt trội hoặc có thể so sánh với tất cả adapter trong thiết lập trong ngôn ngữ và zero-shot đa ngôn ngữ (đặc biệt là đối với các mô hình nhỏ hơn). Do đó, chúng tôi khuyến nghị rằng MTL nên được coi là giải pháp thay thế cho adapter trong các tình huống hạn chế nơi các mô hình tương đối nhỏ hơn được ưa thích, ngân sách tính toán bị hạn chế và tính mở rộng quan trọng.

5 Kết luận
Trong bài báo này, chúng tôi đã tiến hành phân tích toàn diện về adapter qua các ngôn ngữ và tác vụ khác nhau để đánh giá ưu điểm của chúng về hiệu quả huấn luyện/triển khai và khả năng duy trì/mở rộng. Chúng tôi so sánh adapter với các phương pháp baseline đơn giản hơn, bao gồm tinh chỉnh và học đa tác vụ, trong thiết lập có giám sát/trong ngôn ngữ cũng như zero-shot đa ngôn ngữ, và thấy rằng các phương pháp đơn giản hơn này hiệu quả tính toán hơn và có hiệu quả triển khai tốt hơn, trong khi đạt hiệu suất tương đương với adapter. Ngoài ra, chúng tôi đã tiến hành các thí nghiệm rộng rãi để cho thấy rằng học đa tác vụ là giải pháp thay thế hiệu quả chi phí hơn tương đối so với adapter về khả năng duy trì, vì nó cho phép mô hình được mở rộng cho các tác vụ mới với chi phí thấp hơn adapter. Do đó, chúng tôi đề xuất rằng các baseline đơn giản hơn nên được sử dụng cho các mô hình có kích thước vừa phải, vì chúng hiệu quả hơn adapter.

Lời cảm ơn
Chúng tôi muốn cảm ơn Bộ Điện tử và Công nghệ Thông tin4 của Chính phủ Ấn Độ cho khoản tài trợ hào phóng thông qua dự án Digital India Bhashini5. Chúng tôi cũng cảm ơn Trung tâm Phát triển Tính toán Tiên tiến6 để cung cấp thời gian tính toán trên siêu máy tính Param Siddhi. Chúng tôi cũng cảm ơn Nilekani Philanthropies cho khoản tài trợ hào phóng hướng tới xây dựng tập dữ liệu, mô hình, công cụ và tài nguyên cho các ngôn ngữ Indic. Chúng tôi cũng cảm ơn Microsoft cho khoản tài trợ hỗ trợ nghiên cứu về các ngôn ngữ Indic.

Hạn chế
Chúng tôi xác định các hạn chế sau của công trình:
• Nghiên cứu của chúng tôi bị hạn chế trong NLU và một số quan sát của chúng tôi có thể không áp dụng trong thiết lập Sinh ngôn ngữ tự nhiên (NLG). Trong khi đối với NLU chuyển giao đa ngôn ngữ thông qua tinh chỉnh toàn bộ hiệu quả như adapter, trong NLG tinh chỉnh toàn bộ cho NLG đa ngôn ngữ zero-shot không đáng tin cậy do nguy cơ catastrophic forgetting. Do đó, adapter có thể quan trọng hơn cho NLG (Vu et al., 2022).
• Chúng tôi chủ yếu tập trung vào các mô hình tiền huấn luyện nhỏ hơn vì các mô hình lớn hơn yêu cầu tài nguyên tính toán đáng kể mà không phải ai cũng có thể tiếp cận, và do đó, các phát hiện của chúng tôi có thể không áp dụng cho các mô hình lớn hơn với hàng tỷ tham số. Tuy nhiên, nghiên cứu tích cực về nén các mô hình tiền huấn luyện cho thấy rằng tinh chỉnh các mô hình tiền huấn luyện compact sẽ vẫn là một lĩnh vực nghiên cứu quan trọng.
• Phân tích của chúng tôi tập trung vào 6 tác vụ NLU, tương đối ít hơn so với tổng số tác vụ trong các benchmark như BIG-Bench (Srivastava et al., 2022). Mặc dù tập trung vào số lượng tác vụ lớn hơn sẽ tăng độ tin cậy của các nghiên cứu, trọng tâm của chúng tôi về hiệu suất đa ngôn ngữ có nghĩa là chúng tôi hiện tại bị hạn chế bởi tính sẵn có của dữ liệu benchmarking trong các ngôn ngữ khác cho số lượng lớn tác vụ này.

Tuyên bố đạo đức
Tất cả các tập dữ liệu được sử dụng trong nghiên cứu này đều có sẵn công khai, và không có người chú thích nào được thuê để thu thập dữ liệu. Chúng tôi xác nhận rằng các tập dữ liệu chúng tôi sử dụng không chứa bất kỳ nội dung có hại nào. Chúng tôi đã trích dẫn các tập dữ liệu và các nghiên cứu có liên quan được sử dụng trong nghiên cứu này.
