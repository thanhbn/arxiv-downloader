# Điều chỉnh thưa thớt cho tăng tốc suy luận
của các mô hình ngôn ngữ lớn

Eldar Kurtic∗
IST Austria
eldar.kurtic@ista.ac.at

Denis Kuznedelev∗
Skoltech & Yandex
denis.kuznedelev@skoltech.ru

Elias Frantar
IST Austria
elias.frantar@ista.ac.at

Michael Goin
Neural Magic
michael@neuralmagic.com

Dan Alistarh
IST Austria & Neural Magic
dan.alistarh@ista.ac.at

## Tóm tắt
Chúng tôi xem xét bài toán điều chỉnh thưa thớt chính xác của các mô hình ngôn ngữ lớn (LLM), tức là điều chỉnh các LLM đã được huấn luyện trước trên các tác vụ chuyên biệt, đồng thời tạo ra độ thưa thớt trong trọng số của chúng. Về mặt độ chính xác, chúng tôi quan sát thấy rằng điều chỉnh tiêu chuẩn dựa trên hàm mất mát có thể không khôi phục được độ chính xác, đặc biệt là ở độ thưa thớt cao. Để giải quyết vấn đề này, chúng tôi thực hiện một nghiên cứu chi tiết về các hàm mất mát kiểu chưng cất, xác định một phương pháp chưng cất dựa trên L2 mà chúng tôi gọi là SquareHead cho phép khôi phục chính xác ngay cả ở độ thưa thớt cao hơn, trên tất cả các loại mô hình. Về mặt hiệu quả thực tế, chúng tôi cho thấy rằng các LLM thưa thớt có thể được thực thi với tốc độ nhanh hơn bằng cách tận dụng độ thưa thớt, cho cả thời gian chạy CPU và GPU. Trong khi phương pháp tiêu chuẩn là tận dụng độ thưa thớt để giảm tính toán, chúng tôi quan sát thấy rằng trong trường hợp các LLM bị giới hạn bởi bộ nhớ, độ thưa thớt cũng có thể được tận dụng để giảm băng thông bộ nhớ. Chúng tôi trình bày kết quả end-to-end cho thấy tốc độ tăng do độ thưa thớt, đồng thời khôi phục độ chính xác, trên T5 (dịch ngôn ngữ), Whisper (dịch giọng nói), và các mô hình GPT mở (MPT cho sinh văn bản). Đối với sinh văn bản MPT, chúng tôi cho thấy lần đầu tiên rằng điều chỉnh thưa thớt có thể đạt 75% độ thưa thớt mà không giảm độ chính xác, cung cấp tốc độ end-to-end đáng chú ý cho cả suy luận CPU và GPU, và nhấn mạnh rằng độ thưa thớt cũng tương thích với các phương pháp lượng tử hóa. Các mô hình và phần mềm để tái tạo kết quả của chúng tôi được cung cấp trong Mục 6.

## 1 Giới thiệu
Các mô hình Transformer lớn [1] đã trở nên phổ biến và được áp dụng rộng rãi nhờ hiệu suất đột phá trên nhiều tác vụ thách thức. Để giải quyết chi phí thời gian chạy cao của chúng, một số kỹ thuật tăng tốc đã được phát triển, ví dụ [2,3,4,5]. Để tăng tốc suy luận, kỹ thuật phổ biến nhất là lượng tử hóa, ví dụ [6,7,3,5]: các LLM có thể được lượng tử hóa xuống 4 bit mỗi trọng số (hoặc hơn) gần như không mất mát, và điều này có thể được tận dụng để tăng tốc suy luận, ví dụ [3,5]. Tuy nhiên, các phương pháp lượng tử hóa đang đạt đến giới hạn độ chính xác ở khoảng 3 bit mỗi trọng số: tại điểm này, độ chính xác có vẻ khó khôi phục với các kỹ thuật hiện tại [8, 9, 10].

Một giải pháp nén quan trọng thay thế cho lượng tử hóa là độ thưa thớt trọng số [11], bao gồm việc cắt tỉa các kết nối LLM riêng lẻ bằng cách đặt chúng thành không. Đối với các mô hình nhỏ hơn, ví dụ BERT [12], đã biết [13,14] rằng mức độ thưa thớt cao có thể được áp dụng trong quá trình điều chỉnh, tức là quá trình mà mô hình đã được huấn luyện trước được thích ứng với một tác vụ "hạ nguồn", chẳng hạn như trả lời câu hỏi hoặc phân loại văn bản, dẫn đến tốc độ tăng đáng kể. Tuy nhiên, chưa biết liệu các kỹ thuật tương tự có hoạt động ở quy mô của các LLM hay không.

Trong công việc sơ bộ này, chúng tôi nghiên cứu điều chỉnh thưa thớt hiệu quả cho các LLM, trên ba ứng dụng hiện đại: chuyển đổi giọng nói sử dụng Whisper [15], chuyên biệt hóa cho một ngôn ngữ cụ thể, dịch máy sử dụng T5 [16], chuyên biệt hóa cho một cặp ngôn ngữ cụ thể [17], và lý luận cấp cao hơn sử dụng mô hình MPT kiểu GPT mở [18], chuyên biệt hóa trên tác vụ Grade-School Math (GSM) [19].

Trong bối cảnh này, những đóng góp của chúng tôi như sau:

• Chúng tôi quan sát thấy rằng điều chỉnh thưa thớt ngây thơ [13], theo sau điều chỉnh dày đặc trong khi dần dần áp đặt độ thưa thớt, là thách thức để áp dụng cho các LLM do sự bất ổn trong huấn luyện, biểu hiện dưới nhiều hình thức: 1) đột biến hàm mất mát ở các giá trị độ thưa thớt cao hơn, dẫn đến phân kỳ, 2) khôi phục kém, vì lượng dữ liệu điều chỉnh tương đối nhỏ có sẵn cho tác vụ con có thể không đủ để khôi phục độ chính xác; hoặc 3) trang bị quá mức, vì lặp lại nhiều lần trên dữ liệu điều chỉnh hạn chế dẫn đến hàm mất mát huấn luyện thấp, nhưng lỗi xác nhận cao.

• Để giải quyết vấn đề này, chúng tôi điều tra việc điều chỉnh các mô hình thưa thớt thu được thông qua SparseGPT [20] sử dụng các hàm mất mát khác nhau kết hợp cross-entropy tiêu chuẩn, chưng cất kiến thức đầu ra [21], và một loại chưng cất kiến thức ℓ2 theo token được lấy cảm hứng từ [22,23,24] mà chúng tôi gọi là SquareHead. Chúng tôi cho thấy rằng chưng cất SquareHead liên tục khôi phục độ chính xác, ngay cả ở độ thưa thớt cao.

• Về mặt thực tế, chúng tôi cho thấy rằng các mô hình thưa thớt thu được có thể được thực thi với tốc độ suy luận nhanh hơn. Đối với suy luận CPU, chúng tôi tận dụng thời gian chạy suy luận DeepSparse [25] để có được tốc độ tăng trên cả ba ứng dụng. Ngoài ra, tập trung vào suy luận sinh tạo, chúng tôi trình bày tốc độ tăng GPU thông qua định dạng thưa thớt N:M nhận biết GPU, và tốc độ tăng CPU đáng chú ý, ví dụ 7,5x với mất mát độ chính xác tối thiểu so với baseline FP32, đồng thời tận dụng độ thưa thớt và lượng tử hóa.

Cùng nhau, kết quả của chúng tôi cho thấy rằng, với bộ kỹ thuật phù hợp, độ thưa thớt có thể được áp dụng thành công trong kịch bản điều chỉnh LLM thách thức, và điều này có thể dẫn đến tốc độ tăng đáng kể trên các tình huống thực tế, cả trên CPU và GPU.

## 2 Phương pháp luận

### 2.1 Điều chỉnh thưa thớt
Thưa thớt hóa. Để có được một tập hợp các mô hình thưa thớt đáp ứng các yêu cầu nén mục tiêu, chúng tôi tăng mức độ thưa thớt dần dần trong khi điều chỉnh mô hình trên tác vụ quan tâm. Trừ khi nói khác, chúng tôi bắt đầu từ một danh sách các mức độ thưa thớt mong muốn, theo thứ tự tăng dần, và lặp lại việc cắt tỉa và điều chỉnh mô hình trong khi tuân theo công thức điều chỉnh ban đầu trong mỗi chu kỳ tiếp theo.

Chiến lược chưng cất. Các LLM rất khó huấn luyện và điều chỉnh, và chúng tôi thấy điều này đặc biệt đúng khi độ thưa thớt được áp đặt trong một chu kỳ điều chỉnh thường ngắn. Chọn hàm mất mát "đúng" cho điều chỉnh đặc biệt quan trọng, và do đó chúng tôi điều tra các phương pháp chưng cất kiến thức (KD). Mô hình học sinh thưa thớt được huấn luyện để bắt chước hành vi của một giáo viên dày đặc, đã được điều chỉnh trên tác vụ mục tiêu. Chiến lược KD phổ biến nhất thêm một hạng mất mát đo lường phân kỳ KL giữa đầu ra của học sinh và giáo viên [21]. Tuy nhiên, chúng tôi thu được kết quả tốt hơn bằng cách đi xa hơn và chưng cất các biểu diễn trung gian.

Chưng cất đầu ra tiêu chuẩn sử dụng phân kỳ KL giữa logit học sinh và giáo viên làm hàm mất mát:

Llogit = DKL(θt||θs) = (1/B×seq) ∑i 1[i ∉ P] ∑i 1[i ∉ P] pθt(xi) log(pθt(xi)/pθs(xi))    (1)

Ở trên, B là kích thước batch, seq là độ dài chuỗi, pθt(xi) và pθs(xi) biểu thị xác suất đầu ra cho mô hình giáo viên và học sinh tương ứng. Ký hiệu 1[i ∉ P] có nghĩa là hàm mất mát cho token đệm P bị loại bỏ.

Để chuyển giao các biểu diễn trung gian, chúng tôi kiểm tra hàm mất mát lỗi bình phương trung bình (MSE) được chuẩn hóa trên mỗi biểu diễn đặc trưng, mà chúng tôi gọi đơn giản là SquareHead:

Llfeat = MSE(flt, fls) / MSE(flt, 0), shape(fls) = shape(flt) = B×seq×dmodel    (2)

Ở đây, flt và fls biểu thị bản đồ đặc trưng của lớp thứ l của mô hình giáo viên và học sinh, và MSE biểu thị lỗi bình phương trung bình được tính là MSE(X, Y) = (1/N) ∑i=0N (xi - yi)2, cho các vector N-chiều X và Y. Chúng tôi loại bỏ các hạng tương ứng với token đệm. Động lực cho việc chuẩn hóa là độ lớn của các kích hoạt có thể khác nhau rất nhiều giữa các lớp khác nhau, do đó tập trung quy trình tối ưu hóa để tối ưu hóa các lớp có chuẩn lớn nhất. Chúng tôi quan sát thấy rằng đối với một số mô hình, hàm mất mát MSE không có chuẩn hóa dẫn đến sự bất ổn trong huấn luyện. Tổng hàm mất mát đặc trưng là tổng của các hàm mất mát theo lớp trên tất cả các khối encoder/decoder.

Chưng cất SquareHead. Tổng hàm mất mát chưng cất SquareHead là tổng của hàm mất mát tác vụ ban đầu và hạng SquareHead với trọng số bằng nhau: L = Ltask + Lfeat.

Công việc trước đây. Như đã nói trước đó, các biến thể của hàm mất mát này đã được sử dụng bởi [22,23,24] trong bối cảnh nén các mô hình quy mô nhỏ hơn trong quá trình điều chỉnh, chẳng hạn như các mô hình kiểu BERT [12] cho các tác vụ điều chỉnh trả lời câu hỏi hoặc phân loại cảm xúc. Trong bối cảnh điều chỉnh BERT, đã biết rằng các biến thể của chưng cất kiến thức có thể giúp giảm mất mát độ chính xác trong quá trình điều chỉnh, ví dụ [13,14,24,23,26]. Tương đối với dòng công việc này, trong bài báo này chúng tôi xác định một loại hàm mất mát chưng cất có hiệu quả nhất quán cho điều chỉnh thưa thớt chính xác của các mô hình lớn.

### 2.2 Tăng tốc thời gian chạy từ độ thưa thớt

Bây giờ chúng tôi chuyển sự chú ý đến câu hỏi về việc có được tốc độ tăng thời gian chạy thực tế từ một mô hình được điều chỉnh thưa thớt. Chiến lược tiêu chuẩn là tận dụng độ thưa thớt giảm tải tính toán: vì chúng ta có thể bỏ qua các phép nhân với không, độ thưa thớt cao hơn có nghĩa là tính toán suy luận thấp hơn. Tuy nhiên, tận dụng độ thưa thớt tương đối thấp phát sinh trong các mô hình sâu chính xác để có được lợi ích tính toán được biết là khó khăn, đặc biệt trên GPU [27, 28].

Trong trường hợp của các LLM, chúng ta có một con đường bổ sung cho tăng tốc thưa thớt, vì các mô hình này thường bị giới hạn bởi bộ nhớ [3,5]. Tức là, một phần đáng kể của thời gian chạy sinh tạo có thể được thực hiện bằng cách tải trọng số (từ bộ nhớ đến thanh ghi) để thực hiện tính toán lớp.

Để tận dụng điều này, chúng ta có thể lưu trữ trọng số thưa thớt ở dạng nén, và giải nén chúng một cách tức thời trong khi thực hiện tính toán lớp. Điều này thậm chí có thể liên quan đến phép nhân với không, miễn là chúng không phải cụ thể hóa trọng số trong bộ nhớ chậm. Chúng tôi trình bày kết quả trên thời gian chạy CPU và GPU tận dụng các biến thể của phương pháp này. Để thực thi CPU, chúng tôi tận dụng thời gian chạy DeepSparse [25] triển khai suy luận nhận biết độ thưa thớt trong cả tình huống giới hạn bởi bộ nhớ và tính toán.

Phác thảo triển khai GPU. Đối với suy luận GPU, chúng tôi cho thấy tiềm năng tăng tốc thông qua triển khai sau đây cho các mẫu độ thưa thớt "đồng nhất" như 16:32 (50%), 16:64 (75%) và 16:128 (87,5%). (Để rõ ràng, 16:32 có nghĩa là 16 số khác không trong mọi khối 32 trọng số liên tiếp.) Chúng tôi triển khai một kernel CUDA tùy chỉnh nơi mỗi threadblock tìm nạp các tile 16×#threads trọng số vào shared memory và sau đó tích lũy đầu ra tích vector-ma trận một phần bằng cách giải nén các bitmask INT32 tương ứng để xác định khi nào trọng số nên được nhân với đầu vào.

Được thực thi trên GPU NVIDIA A6000 cho ma trận 4096×12288 (hình dạng của lớp chiếu QKV trong mô hình MPT-7B), kernel của chúng tôi tạo ra tốc độ tăng 1,82× so với thực thi FP16 dày đặc, gần với kỳ vọng lý thuyết 1,77× (tương đương với 9 bit mỗi trọng số) cho biểu diễn bitmask của độ thưa thớt.

Chúng tôi hiển thị kết quả tự tăng tốc, tức là so với baseline dày đặc tương ứng, cho ma trận 4096×12288 này cả trên CPU và GPU trong Hình 1, quan sát thấy rằng cả hai triển khai đều đạt được tốc độ tăng đáng kể. Để tham khảo, chúng ta có thể cắt tỉa mô hình sinh tạo MPT-7B cả trong định dạng 16:32 và 16:64, và hoàn toàn khôi phục độ chính xác trên GSM8K (xem Bảng 4 trong Mục 4).

## 3 Ứng dụng 1: Nén mô hình dịch T5 và Whisper

Thiết lập thí nghiệm. Chúng tôi xem xét các mức độ thưa thớt tương ứng với tỷ lệ nén 2x, 3x, 4x, 5x, 6x, 7x, 8x, và 10x, áp dụng đồng nhất mỗi lớp với bộ cắt tỉa SparseGPT [20]. Chúng tôi so sánh điều chỉnh thưa thớt với ba biến thể của hàm mất mát. Cross Entropy (hàm mất mát ban đầu, Ltask), Standard KD (Ltask + Llogit), và SquareHead KD (Ltask + Lfeat). Chúng tôi báo cáo các siêu tham số trong Phụ lục A. Tốc độ tăng được báo cáo cho thực thi end-to-end trong động cơ suy luận DeepSparse [25] trên CPU Intel Sapphire Rapids với 8 nhân (AWS m7i.4xlarge). Trong phần sau, so sánh giữa các chiến lược chưng cất (hình bên trái) được thực hiện mà không cắt tỉa đầu ra cuối cùng, trong khi đầu ra mô hình ngôn ngữ được cắt tỉa trong các thí nghiệm thời gian chạy (bảng bên phải), vì điều này có tác động không nhỏ đến tổng thời gian suy luận.

Nén cho dịch ngôn ngữ sử dụng T5. Chúng tôi bắt đầu bằng cách điều tra việc thưa thớt hóa mô hình T5 đã được huấn luyện trước [29], được điều chỉnh trên tập con tiếng Anh-Đức phổ biến của WMT14 [30]. Theo thực hành tiêu chuẩn, chúng tôi tính điểm BLEU trên tập xác nhận của phần tách tương ứng như một thước đo độ chính xác của mô hình. Hình 2 cho thấy sự đánh đổi giữa độ chính xác và độ thưa thớt cho các hàm mất mát khác nhau. Bảng 1 cho thấy điểm cho biến thể hàm mất mát hoạt động tốt nhất (SquareHead KD), cùng với tốc độ tăng tự thân. Baseline dày đặc cần 370ms để mã hóa/giải mã 128 token.

Chúng tôi quan sát thấy rằng huấn luyện thưa thớt chỉ với Cross Entropy và Standard KD trở nên bất ổn ở độ thưa thớt cao và chỉ SquareHead KD có thể tạo ra các mô hình có độ thưa thớt cao với hiệu suất hợp lý. Một ví dụ về đường cong hàm mất mát huấn luyện được trình bày trong Phụ lục C.

Nén chuyển giọng nói thành văn bản sử dụng Whisper. Trong Hình 3 và Bảng 2, chúng tôi nghiên cứu nén của Whisper-Small (244M) [31] cho Nhận dạng giọng nói tự động (ASR) trên tập con tiếng Hindi (Hi) của CommonVoice 11.0 [32]. Chúng tôi báo cáo Tỷ lệ lỗi từ (WER) như một thước đo tiêu chuẩn của hiệu suất ASR. Baseline CPU dày đặc của chúng tôi mất 882ms để phiên âm một chuỗi âm thanh 15 giây và bảng phân tích chi tiết được trình bày trong Phụ lục D.

Thảo luận. Kết quả cho thấy rằng hàm mất mát SquareHead KD cải thiện so với các phương pháp tiêu chuẩn trong hai tình huống điều chỉnh này, đặc biệt là ở độ thưa thớt cao hơn, nơi các phương pháp sau có xu hướng phân kỳ. (Chúng tôi cung cấp phân tích hiện tượng này trong Phụ lục B.) Nhìn chung, chúng ta có thể tạo ra mức độ thưa thớt vừa phải (50-70%) trong khi duy trì hoặc thậm chí cải thiện độ chính xác, dẫn đến tốc độ tăng 1,5-2x. Chúng ta có thể có được tốc độ tăng cao hơn, lên đến 2,67x, với độ thưa thớt 80-90%, với giá của tỷ lệ lỗi cao hơn.

## 4 Ứng dụng 2: Nén mô hình sinh tạo

Tiếp theo, chúng tôi điều tra việc nén các mô hình sinh tạo, cụ thể là mô hình được huấn luyện trước kiểu GPT mã nguồn mở Mosaic MPT-7B [18]. Chúng tôi tập trung vào GSM8K [33], một tập dữ liệu với chất lượng cao và các bài toán toán học cấp tiểu học đa dạng, theo công thức được cung cấp bởi [34]. Trên tác vụ này, ở chế độ zero-shot, mô hình baseline hoàn toàn thất bại, với điểm số 0%, trong khi ở đánh giá 8-shot nó chỉ đạt 6,8%. Những kết quả này cho thấy rằng mô hình cần thiết sự tinh chỉnh bổ sung thông qua điều chỉnh có giám sát (SFT).

Thiết lập thí nghiệm. Đầu tiên, chúng tôi điều chỉnh MPT-7B thông qua SFT để có được một baseline dày đặc có độ chính xác cao và cạnh tranh, mà chúng tôi sử dụng làm giáo viên trong các lần chạy chưng cất kiến thức. Sau đó, chúng tôi áp dụng cắt tỉa không cấu trúc một lần với SparseGPT cho các mục tiêu độ thưa thớt 40%, 50%, 60%, 70%, và 80%, đồng nhất trên tất cả các lớp, tương ứng với tỷ lệ nén 1,7x, 2,0x, 2,5x, 3,3x, và 5,0x. Chúng tôi khám phá cách các kỹ thuật điều chỉnh thưa thớt khác nhau giúp khôi phục độ chính xác của mô hình baseline dày đặc. Chúng tôi cung cấp mô tả chi tiết về các siêu tham số trong Phụ lục A.2. Sau điều chỉnh, chúng tôi điều tra tính tương thích của các mô hình thưa thớt với lượng tử hóa thông qua lượng tử hóa sau huấn luyện xuống INT8. Để đạt được điều này, chúng tôi tận dụng thư viện SparseML [35] và lượng tử hóa xuống 8-bit các trọng số và kích hoạt của tất cả các ma trận trọng số tuyến tính, và hai phép nhân ma trận batch trong các lớp attention. Để đánh giá độ chính xác trên tác vụ GSM8K, chúng tôi sử dụng giao thức đánh giá tiêu chuẩn hóa thông qua Language Model Evaluation Harness [36].

Thảo luận. Kết quả độ chính xác cho các hàm mất mát khác nhau được hiển thị trong Hình 4. Chúng thể hiện xu hướng rất tương tự với hai ứng dụng trước, cho thấy rằng SquareHead KD vượt trội hơn cả hàm mất mát CE tiêu chuẩn và KD tiêu chuẩn cho điều chỉnh thưa thớt. Việc các mô hình được chưng cất có thể vượt trội hơn baseline dày đặc ở độ thưa thớt thấp có thể được giải thích do tác dụng của việc áp dụng chưng cất, nhưng cũng có thể do tác dụng điều hòa của độ thưa thớt thấp. Chúng tôi quan sát thấy rằng, với SquareHead KD, chúng ta có thể có được các mô hình FP32 với độ thưa thớt 70% và 75% về cơ bản không có mất mát (về độ chính xác kiểm tra) so với mô hình dày đặc, mặc dù cắt tỉa được thực hiện trong một lần. Đáng nhấn mạnh rằng điều chỉnh với SquareHead KD cũng cải thiện độ chính xác của mô hình dày đặc, từ 28,2 lên 33,0.

Bây giờ chúng tôi kiểm tra sự đánh đổi giữa tốc độ tăng và độ chính xác, được trình bày trong Bảng 3 cho cả mô hình FP32 và INT8. Trong FP32, độ thưa thớt vừa phải (60-70%) có thể đạt được mà không mất mát, dẫn đến tốc độ tăng 2-2,5x, trong khi độ thưa thớt cao hơn (80%) dẫn đến mất mát độ chính xác cao hơn. Chuyển sang INT8, chúng tôi quan sát thấy sự giảm độ chính xác nhất quán 1-2% ở mỗi mức độ thưa thớt, do lượng tử hóa sau huấn luyện. Đồng thời, việc mất mát độ chính xác này được đi kèm với cải thiện hiệu suất lớn, vì lợi ích do hai phương pháp nén này tạo ra được cộng dồn. Về mặt tuyệt đối, mô hình INT8 70% có thể thực thi với 7,7 token/giây đáng chú ý trên một nhân của CPU AMD Ryzen, và 20,9 token/giây trên 4 nhân. Tốc độ tăng của mô hình INT8 thưa thớt 60% không mất mát là khoảng 6,7x, và chúng ta có thể đạt tốc độ tăng giải mã 9,08x ở INT8 80%, với giá của giảm 7% độ chính xác.

## 5 Thảo luận

Chúng tôi đã cho thấy các kết quả sớm cho thấy rằng độ thưa thớt có thể là một phương pháp tăng tốc hiệu quả cho suy luận LLM, tập trung vào các ứng dụng "hiện đại" như dịch giọng nói và ngôn ngữ, và sinh văn bản theo các hướng dẫn phức tạp. Quan trọng là, nghiên cứu của chúng tôi cho thấy rằng độ thưa thớt tương thích với lượng tử hóa trong cài đặt sinh tạo bị giới hạn bởi bộ nhớ, và cùng nhau các kỹ thuật này có thể dẫn đến hiệu suất đáng chú ý cho các thiết bị bị giới hạn tính toán. Dựa trên công việc trước đây, chúng tôi đã xác định một phương pháp chưng cất tổng quát để khôi phục độ chính xác trong khi tạo ra độ thưa thớt cao trên dữ liệu điều chỉnh hạn chế. Trong công việc tương lai, chúng tôi dự định mở rộng nghiên cứu sơ bộ này bằng cách khám phá điều chỉnh thưa thớt cho các mô hình và tác vụ quy mô lớn hơn trong cài đặt sinh tạo, mức độ lượng tử hóa cao hơn, cũng như cài đặt có liên quan thực tế là cắt tỉa trên tác vụ huấn luyện trước [37], theo sau là điều chỉnh mô hình đã được thưa thớt hóa trên tập dữ liệu chuyên biệt.

## 6 Tái tạo kết quả

Để thúc đẩy khả năng tái tạo kết quả của chúng tôi, chúng tôi cung cấp các tài nguyên sau:

• Mã cho điều chỉnh thưa thớt của các mô hình T5 và Whisper có thể tìm thấy tại đây: https://github.com/IST-DASLab/TACO4NLP.

• Mã cho điều chỉnh thưa thớt của các mô hình kiểu GPT (ví dụ MPT) có thể tìm thấy tại đây: https://github.com/IST-DASLab/SparseFinetuning.

• Các mô hình MPT được phát hành tại https://sparsezoo.neuralmagic.com/?datasets=gsm8k&ungrouped=true

• Tốc độ tăng CPU cho suy luận sinh tạo có thể được tái tạo bằng cách làm theo hướng dẫn tại https://github.com/neuralmagic/deepsparse/tree/main/research/mpt và như một Bộ sưu tập HuggingFace.

• Mã GPU sẽ được cung cấp tại https://github.com/IST-DASLab/sparsegpt/.

## Lời cảm ơn

Các tác giả cảm ơn nhóm Nghiên cứu Học máy Neural Magic cho các cuộc thảo luận hữu ích trong quá trình phát triển bài báo này. Chúng tôi cũng muốn cảm ơn Eugenia Iofinova cho những nhận xét hữu ích về phiên bản trước của bản thảo này, và Artur Niederfahrenhorst cho những đề xuất hữu ích về điều chỉnh trên tập dữ liệu GSM.
