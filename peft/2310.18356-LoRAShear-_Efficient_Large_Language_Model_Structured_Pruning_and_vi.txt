# LoRAShear: Cắt tỉa có cấu trúc hiệu quả cho các mô hình ngôn ngữ lớn và phục hồi tri thức

Tianyi Chen1 Tianyu Ding1 Badal Yadav1 Ilya Zharkov1 Luming Liang1
1Microsoft
{tiachen,tianyuding,bayadav,zharkov,lulian}@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLMs) đã biến đổi bối cảnh trí tuệ nhân tạo, trong khi kích thước khổng lồ của chúng đặt ra những thách thức đáng kể về mặt chi phí tính toán. Chúng tôi giới thiệu LoRAShear, một phương pháp hiệu quả mới để cắt tỉa có cấu trúc LLMs và phục hồi tri thức. Với các LLMs tổng quát, LoRAShear đầu tiên tạo ra các đồ thị phụ thuộc trên các mô-đun LoRA để khám phá các cấu trúc loại bỏ tối thiểu và phân tích phân phối tri thức. Sau đó nó tiến hành cắt tỉa có cấu trúc tiến bộ trên các bộ điều hợp LoRA và cho phép chuyển giao tri thức tự nhiên để bảo tồn tốt hơn thông tin trong các cấu trúc dư thừa. Để phục hồi tri thức bị mất trong quá trình cắt tỉa, LoRAShear nghiên cứu kỹ lưỡng và đề xuất một lược đồ tinh chỉnh động với các bộ điều hợp dữ liệu động để thu hẹp hiệu quả khoảng cách hiệu suất với các mô hình đầy đủ. Kết quả số liệu chứng minh rằng chỉ sử dụng một GPU trong vòng vài ngày GPU, LoRAShear đã giảm hiệu quả dấu chân của LLMs 20% với chỉ 1,0% suy giảm hiệu suất và vượt trội đáng kể so với các phương pháp tiên tiến nhất. (Mã nguồn sẽ được công bố sớm.)

## 1. Giới thiệu

Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) (Zhao et al., 2023; Hadi et al., 2023) đã đánh dấu một cột mốc quan trọng trong sự phát triển của trí tuệ nhân tạo. Những mô hình này, được phân biệt bởi kích thước tham số rộng lớn, đã thể hiện các khả năng nổi sinh (Wei et al., 2022), xúc tác các đột phá không chỉ trong lĩnh vực xử lý ngôn ngữ tự nhiên mà còn trong các nhiệm vụ thuộc nhiều lĩnh vực khác nhau (Driess et al., 2023). Điều này đã mở ra những khả năng mới để tiến tới Trí tuệ nhân tạo tổng quát (AGI) (Everitt et al., 2018; Bubeck et al., 2023).

Tuy nhiên, kích thước khổng lồ của LLMs, thường dao động từ hàng chục đến hàng trăm tỷ tham số (Touvron et al., 2023), gây ra chi phí tính toán đáng kể cả về sức mạnh xử lý và yêu cầu bộ nhớ.

Cắt tỉa có cấu trúc là một cách hiệu quả để cung cấp các DNN nhỏ gọn thông qua việc xác định và loại bỏ các cấu trúc dư thừa sau đó phục hồi tri thức bị mất (Han et al., 2015; Chen et al., 2021b). Tuy nhiên, việc áp dụng nó lên LLMs đang phải đối mặt với những thách thức đáng kể, do các yêu cầu về tài nguyên tính toán khổng lồ và các bộ dữ liệu đào tạo không khả dụng của cả bộ dữ liệu tiền đào tạo và tinh chỉnh theo hướng dẫn (Brown et al., 2020). Do đó, các mô hình có thể được phân loại rộng rãi như cắt tỉa dưới tài nguyên hạn chế hoặc đầy đủ. Đối với thiết lập tài nguyên hạn chế, các công trình cắt tỉa gần đây (Ma et al., 2023; Zhang et al., 2023; Sun et al., 2023) sử dụng Low-Rank-Adaptor (LoRA) (Hu et al., 2021) trong quá trình cắt tỉa và giai đoạn tinh chỉnh theo hướng dẫn để giảm yêu cầu tài nguyên, nhưng vẫn phải đối mặt với suy giảm hiệu suất đáng kể so với LLMs đầy đủ. Đối với thiết lập tài nguyên đầy đủ, Sheared-LLaMA (Xia et al., 2023) thực hiện cắt tỉa có cấu trúc trên các LLMs gốc để trực tiếp đạt được các đối tác nhỏ gọn vượt trội hơn các LLMs có kích thước bằng nhau được đào tạo từ đầu, trong khi yêu cầu sức mạnh GPU đáng kể có thể không khả thi cho người dùng công cộng.

Chúng tôi đề xuất LoRAShear, một khung cắt tỉa có cấu trúc mới cho LLMs trong thiết lập tài nguyên hạn chế để thu hẹp đáng kể khoảng cách hiệu suất giữa các LLMs đã cắt tỉa với các phiên bản đầy đủ của chúng. So với các công trình hiện có, LoRAShear có hai lợi thế chính để bảo tồn và phục hồi tri thức bị mất tốt hơn trong quá trình cắt tỉa.

Thứ nhất, chúng tôi đề xuất một Lora Half-Space Projected Gradient (LHSPG) mới để cho phép cắt tỉa có cấu trúc tiến bộ với chuyển giao tri thức tự nhiên qua các mô-đun LoRA. Thứ hai, chúng tôi đề xuất một giai đoạn phục hồi tri thức động để thực hiện tinh chỉnh đa giai đoạn theo cách của cả tiền đào tạo và tinh chỉnh theo hướng dẫn. Ngoài ra, LoRAShear có thể áp dụng lên các LLMs tổng quát với việc thực hiện phân tích đồ thị phụ thuộc trên LLMs với các mô-đun LoRA (Chen et al., 2023b).

Chúng tôi bây giờ tóm tắt những đóng góp chính của chúng tôi như sau:

• **Phân tích đồ thị phụ thuộc trên LLMs với các mô-đun LoRA**. Để tự động cắt tỉa có cấu trúc các LLMs tổng quát, việc khám phá các cấu trúc loại bỏ tối thiểu là cần thiết thông qua phân tích đồ thị phụ thuộc. LLMs với LoRA đặt ra những thách thức bổ sung, vì các cấu trúc mục tiêu không thể đào tạo được nhưng các mô-đun LoRA phụ trợ có thể học được, được bỏ qua bởi các thuật toán hiện có (Chen et al., 2023b; Ma et al., 2023). Chúng tôi đề xuất một thuật toán đồ thị mới để xây dựng các đồ thị phụ thuộc bao gồm các nhóm nút chồng chéo và các nhóm nút kết hợp, và phân vùng các biến có thể đào tạo tương ứng.

• **Cắt tỉa có cấu trúc tiến bộ thông qua LHSPG**. Chúng tôi đề xuất một thuật toán tối ưu hóa thưa thớt có cấu trúc mới LoRA Half-Space Projected Gradient (LHSPG) để thực hiện cắt tỉa có cấu trúc tiến bộ. LHSPG tận dụng thông tin từ các mô-đun LoRA và hiệu quả tạo ra độ thưa thớt có cấu trúc mong muốn trên các biến gốc. LHSPG chuyển giao tri thức được lưu trữ trong các cấu trúc tương đối dư thừa sang các cấu trúc quan trọng để bảo tồn tri thức của các LLMs được tiền đào tạo tốt hơn.

• **Phục hồi tri thức động**. Để phục hồi thêm tri thức sau cắt tỉa tiến bộ, chúng tôi đề xuất một cơ chế phục hồi tri thức động. Thay vì chỉ tham gia vào tinh chỉnh theo hướng dẫn như các công trình cắt tỉa tài nguyên hạn chế hiện có, chúng tôi thích ứng xây dựng một tập con từ các bộ dữ liệu tiền đào tạo dựa trên phân phối hiệu suất để phục hồi tri thức tổng quát bị mất trong quá trình cắt tỉa. Sau đó chúng tôi thực hiện tinh chỉnh theo hướng dẫn thông thường để phục hồi chuyên môn đặc thù lĩnh vực và khả năng hướng dẫn của các LLMs đã cắt tỉa.

• **Kết quả thí nghiệm**. Chúng tôi chứng minh hiệu quả của LoRAShear trên LLAMAv1 mã nguồn mở. Bằng cách sử dụng một GPU A100 trong vòng vài ngày GPU, so với mô hình đầy đủ, LLAMAv1 đã cắt tỉa 20% chỉ giảm không đáng kể 1% hiệu suất, và LLAMAv1 đã cắt tỉa 50% bảo tồn 82% hiệu suất trên các điểm chuẩn đánh giá. Trong khi đó, kết quả của chúng tôi vượt trội đáng kể so với các phương pháp tiên tiến nhất hiện có.

## 2. Công trình liên quan

Trong khi cắt tỉa (Han et al., 2015) đã được thiết lập tốt trong các mạng nơ-ron sâu truyền thống (DNNs), việc áp dụng nó lên LLMs đặt ra những thách thức độc đáo. Không giống như các DNNs nhỏ hơn, đặc thù cho từng nhiệm vụ (Ding et al., 2021; 2022), LLMs có một số lượng lớn tham số và yêu cầu tài nguyên tính toán đáng kể (Brown et al., 2020). Hơn nữa, việc chúng tổng quát hóa tốt trên nhiều nhiệm vụ là rất quan trọng (Xia et al., 2023). Gần đây, các phương pháp cắt tỉa khác nhau đã được phát triển đặc biệt cho LLMs, thường rơi vào hai danh mục chính: không có cấu trúc và có cấu trúc.

**Cắt tỉa không có cấu trúc**. Các phương pháp cắt tỉa không có cấu trúc (Dong et al., 2017; Chen et al., 2020; 2021a) tập trung vào việc đặt các trọng số riêng lẻ không quan trọng trong mô hình về không. Phương pháp chi tiết này rất đơn giản và thường duy trì hiệu suất tốt, ngay cả với tỷ lệ nén cao. Tuy nhiên, nó dẫn đến các ma trận trọng số thưa thớt không phù hợp với các bộ gia tốc phần cứng, làm cho chúng kém hiệu quả hơn trong triển khai thực tế. Trong lĩnh vực LLMs, một số kỹ thuật mới đã xuất hiện.

SparseGPT (Frantar & Alistarh, 2023) sử dụng một quá trình cập nhật trọng số tinh vi liên quan đến các cập nhật Hessian bậc hai đồng bộ, bỏ qua việc đào tạo lại truyền thống. Ngược lại, Wanda (Sun et al., 2023) đạt được độ thưa thớt cao mà không cần đào tạo lại nào, đơn giản bằng cách cắt tỉa các trọng số có độ lớn nhỏ nhất nhân với các kích hoạt đầu vào tương ứng. PST (Li et al., 2022), tuy nhiên, kết hợp cắt tỉa không có cấu trúc với tinh chỉnh hiệu quả, cắt tỉa cả trọng số LoRA và mô hình được tiền đào tạo. Nhược điểm của phương pháp này là nhu cầu về một mặt nạ sử dụng nhiều bộ nhớ khớp với hình dạng của các trọng số được tiền đào tạo.

**Cắt tỉa có cấu trúc**. Các phương pháp cắt tỉa có cấu trúc (Chen et al., 2021b; 2023a;b) tập trung vào việc loại bỏ toàn bộ nhóm tham số, chẳng hạn như nơ-ron hoặc lớp, thay vì các trọng số riêng lẻ. Phương pháp cấp nhóm này thân thiện với phần cứng vì nó duy trì các ma trận trọng số dày đặc. Thách thức chính là chọn cấu trúc nào để loại bỏ mà không làm tổn hại đến hiệu suất mô hình. Trong bối cảnh LLMs, một số kỹ thuật gần đây nhằm mục đích triển khai hiệu quả hơn và tăng tốc suy luận.

Ví dụ, LLM-Pruner (Ma et al., 2023) đề xuất một thuật toán phát hiện phụ thuộc để xác định và loại bỏ các cấu trúc liên kết không quan trọng, tiếp theo là một giai đoạn hậu đào tạo nhanh chóng với dữ liệu hạn chế. Tuy nhiên, phương pháp này sử dụng nhiều bộ nhớ vì nó yêu cầu thông tin gradient đầy đủ và không tương thích với LoRA, đòi hỏi một giai đoạn hậu đào tạo riêng biệt để phục hồi tri thức. Ngược lại, LoRAPrune (Zhang et al., 2023) tích hợp LoRA với cắt tỉa có cấu trúc lặp lại, đạt được cả tinh chỉnh hiệu quả tham số và tăng tốc phần cứng trực tiếp. Phương pháp này cũng hiệu quả về bộ nhớ, chỉ dựa vào trọng số và gradient của LoRA cho tiêu chí cắt tỉa, không giống như LLM-Pruner sử dụng gradient đầy đủ.

Gần đây nhất, Sheared-LLaMA (Xia et al., 2023) nhằm mục đích cắt tỉa mô hình đến một kiến trúc mục tiêu được định nghĩa bởi các mô hình được tiền đào tạo hiện có. Sau đó nó đào tạo mô hình đã cắt tỉa bằng cách sử dụng dữ liệu được tải động, dựa trên tỷ lệ giảm mất mát của từng lĩnh vực, dẫn đến việc sử dụng dữ liệu hiệu quả hơn và cải thiện hiệu suất nhanh hơn. Tuy nhiên, Sheared-LLaMA phân bổ tài nguyên tính toán đáng kể cho việc tiền đào tạo tiếp theo để phục hồi hiệu suất.

Trong công trình này, chúng tôi trình bày LoRAShear, một phương pháp cắt tỉa có cấu trúc hiệu quả của LLMs trong khi phục hồi tri thức. So với các phương pháp hiện có, phương pháp của chúng tôi tận dụng độc đáo một thuật toán đồ thị mới để tạo ra các đồ thị phụ thuộc cho cả LLM gốc và các mô-đun LoRA. Chúng tôi giới thiệu thêm một thuật toán tối ưu hóa thưa thớt có cấu trúc sử dụng thông tin từ các mô-đun LoRA để cập nhật trọng số, từ đó tăng cường việc bảo tồn tri thức. Sau cắt tỉa, chúng tôi sử dụng phương pháp đào tạo hai giai đoạn bao gồm cả tiền đào tạo và tinh chỉnh để phục hồi tri thức tổng quát và đặc thù lĩnh vực một cách hiệu quả.

## 3. LoRAShear

LoRAShear thiết kế chuyên biệt một quy trình toàn diện từ đầu đến cuối để nén các LLMs được tiền đào tạo và cung cấp phục hồi tri thức hiệu quả. Phác thảo được trình bày như Thuật toán 1. Với một LLM tổng quát M, chúng tôi đầu tiên phân tích kiến trúc của nó, tạo ra đồ thị phụ thuộc của nó, và phân vùng các biến có thể đào tạo của nó thành một tập hợp nhóm G theo các cấu trúc loại bỏ tối thiểu đã khám phá (Mục 3.1). Sau đó chúng tôi phân tích tri thức phân phối trên các cấu trúc loại bỏ tối thiểu để loại trừ những cấu trúc có tác động lớn đến hiệu suất mô hình khỏi việc cắt tỉa (Mục 3.2). Tiếp theo, cắt tỉa có cấu trúc tiến bộ được thực hiện trên các cấu trúc có thể cắt tỉa thông qua LHSPG đề xuất của chúng tôi để xác định các cấu trúc dư thừa và chuyển giao tri thức được lưu trữ trong các cấu trúc dư thừa trở lại vào các đối tác quan trọng (Mục 3.3), và xây dựng một LLM nén M* thông qua việc tự động loại bỏ các cấu trúc dư thừa (Mục 3.4). Tri thức bị mất trong quá trình cắt tỉa sau đó được phục hồi thông qua một giai đoạn phục hồi tri thức động để tóm tắt hiệu suất của M* nén đến LLM đầy đủ (Mục 3.5).

**Thuật toán 1** Phác thảo của LoRAShear.
1: **Đầu vào**. Một LLM tiền đào tạo tổng quát M.
2: Khám phá các cấu trúc loại bỏ tối thiểu của M thông qua tạo và phân tích đồ thị phụ thuộc (V,E). Phân vùng các biến có thể đào tạo của M thành G.
3: Phân tích phân phối tri thức trên mỗi nhóm nút trong đồ thị phụ thuộc.
4: Cắt tỉa có cấu trúc tiến bộ bằng LHSPG để xác định các cấu trúc dư thừa và chuyển giao tri thức bị mất.
5: Xây dựng mô hình nén để xóa bỏ sự dư thừa để tạo thành LLM nhỏ gọn nén M*.
6: Tinh chỉnh động để phục hồi tri thức bị mất.
7: **Đầu ra**. LLM nhỏ gọn hiệu suất cao M*.

### 3.1. Khám phá cấu trúc loại bỏ tối thiểu

**Thuật toán 2** Khám phá cấu trúc loại bỏ tối thiểu.
1: **Đầu vào**. Một LLM M để được nén và tinh chỉnh.
2: Xây dựng đồ thị theo dõi (E,V) của M.
3: Thiết lập các nhóm nút N_composed cho các toán tử kết hợp thông qua duyệt (E,V) và cây mô-đun của M.
4: Thiết lập các nhóm nút N_basic cho các toán tử còn lại.
5: Xây dựng phụ thuộc giữa N_composed và N_basic.
6: Phân vùng các biến có thể đào tạo thành các cấu trúc loại bỏ tối thiểu và tạo thành G.
7: **Trả về** các nhóm biến có thể đào tạo G.

Với một LLM mục tiêu M, bước đầu tiên là khám phá các cấu trúc loại bỏ tối thiểu, được định nghĩa là các đơn vị có thể được loại bỏ trực tiếp mà không ảnh hưởng đến chức năng của các DNNs còn lại. Việc khám phá như vậy được thực hiện bằng cách phân tích các đồ thị theo dõi và tạo ra các đồ thị phụ thuộc trên các toán tử cơ bản trong OTOv2 (Chen et al., 2023b). Trong những đồ thị phụ thuộc này, mỗi nhóm nút chỉ ra các toán tử phụ thuộc và cần được cắt tỉa cùng nhau nếu có các biến có thể đào tạo và rời rạc với nhau trong các DNNs bình thường. Tuy nhiên, LLMs với các mô-đun LoRA dễ dàng phá vỡ những thuật toán đó vì trong những mô hình như vậy, chỉ các mô-đun LoRA mới có thể đào tạo được, và các biến LLM gốc được cố định nhưng có thể cắt tỉa. Để giải quyết vấn đề này, chúng tôi đặc biệt giới thiệu toán tử kết hợp và các nhóm nút chồng chéo.

Toán tử kết hợp đề cập đến các toán tử được lắp ráp bởi nhiều toán tử cơ bản như các mô-đun LoRA bao gồm hai toán tử tuyến tính, nhưng cần được xem xét như một toàn thể. Mỗi toán tử kết hợp như vậy nên tạo thành một nhóm nút, chẳng hạn như Linear-1-LoRA-A-Linear-1-LoRA-B trong Hình 2a. Các nhóm nút chồng chéo N_composed tồn tại vì các nút đi ra của chúng vẫn cần tuân theo sự phụ thuộc giữa các toán tử liền kề khác trong đồ thị theo dõi, ví dụ, Linear-1-LoRA-B thuộc về hai nhóm nút được đánh dấu màu xanh lá cây và xanh lam đồng thời. Sau đó chúng tôi xem xét chung các nhóm nút cơ bản và kết hợp N_basic và N_composed và phân vùng các biến có thể đào tạo của M thành một tập hợp nhóm G, trong đó mỗi nhóm g∈G tương ứng với một cấu trúc loại bỏ tối thiểu.

### 3.2. Phân tích phân phối tri thức

Do quá trình đào tạo phổ quát, tri thức được phân phối không đều trên tất cả các nhóm nút trong đồ thị phụ thuộc. Một số nhóm nút đóng vai trò quan trọng hơn đáng kể so với các nhóm khác, dẫn đến sự sụp đổ hiệu suất nếu cắt tỉa chúng. Trong khi đó, trong thiết lập tài nguyên hạn chế, tri thức sau khi sụp đổ sẽ không dễ dàng được phục hồi. Do đó, trước khi tham gia vào giai đoạn cắt tỉa có cấu trúc tiến bộ, chúng tôi phân tích phân phối tri thức để xác định vị trí các nhóm nút nên được loại trừ khỏi việc cắt tỉa.

**Thuật toán 3** Phân tích phân phối tri thức.
1: **Đầu vào**. Phân vùng biến có thể đào tạo G, các nhóm nút N_composed ∪ N_basic, một tập hợp tỷ lệ cắt tỉa P, một bộ dữ liệu đánh giá D_eval, và tỷ lệ không thể cắt tỉa γ.
2: **đối với** mỗi nhóm nút trong N_composed ∪ N_basic **làm**
3: Cắt tỉa các nhóm dựa trên một proxy đã chỉ định và P.
4: Tính độ lệch hiệu suất dựa trên D_eval.
5: Khôi phục các nhóm đã cắt tỉa về trạng thái ban đầu.
6: **kết thúc cho**
7: Sắp xếp độ lệch hiệu suất trên N_composed ∪ N_basic.
8: Đánh dấu các nhóm trong G liên quan đến các nhóm nút có độ lệch lớn nhất dựa trên γ như không thể cắt tỉa G_unprunable.
9: Đánh dấu các nhóm còn lại trong G như có thể cắt tỉa G_prunable.
10: **Trả về** các nhóm biến có thể cắt tỉa và không thể cắt tỉa G_prunable ∪ G_unprunable.

Như đã nêu trong Thuật toán 3, chúng tôi lặp lại duyệt tất cả các nhóm nút, và cắt tỉa từng nhóm dựa trên một tỷ lệ cắt tỉa đã chỉ định nhưng giữ các nhóm còn lại không thay đổi. Sau đó chúng tôi đánh giá độ lệch đầu ra giữa mỗi LLM đã cắt tỉa so với mô hình đầy đủ trên một bộ dữ liệu đánh giá được chọn trước. Những nhóm có độ lệch lớn nhất γ|N_composed ∪ N_basic| được đánh dấu là không thể cắt tỉa, các nhóm biến tương ứng tạo thành G_unprunable. Những nhóm còn lại được đánh dấu là có thể cắt tỉa, nơi các biến có thể đào tạo tạo thành G_prunable.

### 3.3. Cắt tỉa có cấu trúc tiến bộ thông qua LHSPG

Bước tiếp theo là tiến hành cắt tỉa có cấu trúc tiến bộ trên các nhóm biến có thể cắt tỉa G_prunable. Để thực hiện điều này, chúng tôi đề xuất một bộ tối ưu hóa thưa thớt có cấu trúc mới LoRA Half-Space Projected Gradient (LHSPG) để tạo ra độ thưa thớt có cấu trúc trên các tham số mô hình gốc dựa trên thông tin tối ưu hóa trên các mô-đun LoRA phụ trợ. Có hai điểm chính của LHSPG, tức là (i) xác định và loại bỏ hiệu quả các cấu trúc dư thừa thông qua việc chiếu chúng lên không, và (ii) chuyển giao tri thức được lưu trữ trong các cấu trúc tương đối dư thừa được cắt tỉa trở lại cho các đối tác quan trọng để bảo tồn tri thức của các LLMs đầy đủ tốt hơn.

**Bài toán mục tiêu**. Chúng tôi công thức hóa việc cắt tỉa có cấu trúc tiến bộ như bài toán tối ưu hóa thưa thớt có cấu trúc sau đây (3) trên LLMs với các mô-đun LoRA.

minimize_{A,B} f(A,B), s.t. Card {g∈G_prunable|[x]_g = 0} = K, (3)

trong đó A và B là các tập hợp của các ma trận con phân tách LoRA, có thể đào tạo được trong quá trình cắt tỉa có cấu trúc. Chúng tôi tìm cách tạo ra độ thưa thớt nhóm trên các biến gốc với mức độ thưa thớt mục tiêu là K.

**Phác thảo**. Phác thảo của LHSPG được trình bày trong Thuật toán 4. Chúng tôi đầu tiên khởi động các biến LoRA trong các nhóm có thể cắt tỉa G_prunable thông qua gradient descent ngẫu nhiên (SGD) hoặc các biến thể của nó như AdamW để thu thập thông tin gradient. Sau đó chúng tôi tiến bộ xác định các cấu trúc dư thừa trong P giai đoạn tối ưu hóa thưa thớt. Để tiến hành, chúng tôi tính mức độ thưa thớt nhóm mục tiêu được tạo ra cho mỗi giai đoạn. Trong mỗi giai đoạn p, chúng tôi sắp xếp các nhóm có thể cắt tỉa dựa trên một số proxy độ quan trọng được chỉ định trước và chọn những nhóm có điểm số độ quan trọng thấp nhất làm nhóm dư thừa cho giai đoạn hiện tại G̃_p.

**Thuật toán 4** Cắt tỉa có cấu trúc tiến bộ thông qua LHSPG
1: **Đầu vào**. biến tiền đào tạo x_0, tốc độ học α, các bước khởi động T_w, giai đoạn tiến bộ P, độ dài giai đoạn T_p, mức độ thưa thớt nhóm mục tiêu K, và phân vùng biến G_prunable ∪ G_unprunable.
2: Khởi động T_w bước thông qua SGD hoặc các biến thể của nó (AdamW).
3: Khởi tạo các nhóm dư thừa G_redundant ← ∅.
4: Khởi tạo các nhóm quan trọng G_important ← G.
5: Tính mức độ thưa thớt cho mỗi giai đoạn cắt tỉa K̃ := K/T_p.
6: **đối với** mỗi giai đoạn cắt tỉa p = 0,1,···,P-1 **làm**
7: Chọn G̃_p trong G_important với điểm số độ quan trọng thấp nhất K̃.
8: Cập nhật G_redundant ← G_redundant ∪ G̃_p.
9: Cập nhật G_important ← G_important / G̃_p.
10: **cho** t = 0,1,···,T_p-1 **làm**
11: Cập nhật LoRA B và A thông qua SGD hoặc các biến thể của nó.
    B_{t+1} ← B_t - α_k∇_{B_t}f
    A_{t+1} ← A_t - α_k∇_{A_t}f (1)
12: Tính lần lặp thử nghiệm [x̃_{t+1}]_{G̃_p} cho mỗi g∈G̃_p.
    [x̃_{t+1}]_g ← [x_t + γB_{t+1}A_{t+1}]_g - λ_g[x_t]_g/‖[x_t]_g‖ (2)
13: Thực hiện chiếu Half-Space trên [x̃_{t+1}]_{G̃_p}.
14: Cập nhật [x_{t+1}]_{G̃_p} ← [x̃_{t+1}]_{G̃_p}.
15: Cập nhật [B_{t+1}]_{G̃_p} ← 0.
16: **nếu** t = T_p-1 **thì**
17: Hợp nhất [B_{t+1}A_{t+1}]_{G_important} vào [x]_{G_important}.
18: **kết thúc nếu**
19: **kết thúc cho**
20: **kết thúc cho**
21: **Trả về** lần lặp cuối cùng x*_{LHSPG}.

Sau đó chúng tôi tính lần lặp thử nghiệm trên các biến LoRA trong A và B thông qua SGD hoặc các biến thể của nó. Đối với các nhóm dư thừa G̃_p, chúng tôi tiến hành gradient descent thông qua xấp xỉ LoRA và phạt trên độ lớn biến tỷ lệ thuận với λ_g, được chọn dựa trên độ dài của mỗi giai đoạn cắt tỉa. Một phép chiếu Half-Space tiếp theo được thực hiện trên lần lặp thử nghiệm để chiếu các nhóm biến với việc hy sinh ít nhất trên hàm mục tiêu.

Trong suốt quá trình, các nhóm dư thừa được chiếu tiến bộ lên không, trong quá trình chiếu, các mô-đun LoRA cho các đối tác quan trọng đang hấp thụ tri thức thông qua việc tối thiểu hóa các hàm mất mát. Kết quả là, việc cắt tỉa có cấu trúc tiến bộ không chỉ xác định hiệu quả và chiếu các nhóm biến dư thừa lên không, mà còn bảo tồn tri thức được lưu trữ trong các cấu trúc dư thừa ở mức độ lớn nhất. Một lần lặp cuối cùng x*_{LHSPG} được trả về cho bước tiếp theo.

### 3.4. Xây dựng LLM nén

Với giải pháp của LHSPG, LoRAShear tự động xây dựng một LLM đã cắt tỉa có cấu trúc M* thông qua việc tự động xóa bỏ các cấu trúc tương ứng với các nhóm dư thừa trong G_prunable. Toàn bộ quy trình được thực hiện thông qua hai lần duyệt đồ thị phụ thuộc. Lần duyệt đầu tiên lặp lại mỗi nhóm nút và cắt tỉa các cấu trúc dọc theo chiều chính. Lần duyệt thứ hai xóa bỏ các cấu trúc dọc theo chiều phụ dựa trên trạng thái đã cắt tỉa của các cấu trúc đến.

### 3.5. Phục hồi tri thức động

**Thuật toán 5** Phục hồi tri thức động.
1: **Đầu vào**. bộ dữ liệu tiền đào tạo D_pretraining, bộ dữ liệu tinh chỉnh theo hướng dẫn D_instruct, và một LLM đã cắt tỉa M*.
2: Thiết lập các bộ dữ liệu xác thực cho D_pretraining và D_instruct là D^val_pretraining và D^val_instruct, tương ứng.
3: **cho** D ∈ {D_pretraining, D_instruct} **làm**
4: **trong khi** chưa hội tụ **làm**
5: Xây dựng động D̃ ⊆ D bằng đánh giá.
6: Tinh chỉnh M* với LoRA trên D̃.
7: **kết thúc trong khi**
8: **kết thúc cho**
9: **Trả về** LLM đã cắt tỉa được phục hồi tri thức M*.

Bước cuối cùng là phục hồi tri thức bị mất sau cắt tỉa và khôi phục khả năng của LLM. Để đạt được sự phục hồi thành công, điều cần thiết là phải hiểu cách LLM thu được tri thức của mình. Tri thức được thu được thông qua một quá trình hai giai đoạn: tiền đào tạo trên các kho văn bản rộng lớn và đa dạng, tiếp theo là tinh chỉnh với các bộ dữ liệu hướng dẫn cụ thể. Tri thức thu được được lưu trữ như các biến trong LLM, nhưng những biến này bị loại bỏ trong quá trình cắt tỉa. Do đó, để lấy lại tri thức, cần có một quá trình hậu đào tạo, bao gồm cả bộ dữ liệu tiền đào tạo và tinh chỉnh theo hướng dẫn.

Do tính chất rộng lớn và đa dạng của các bộ dữ liệu tiền đào tạo, các phương pháp cắt tỉa có cấu trúc hiện có, đặc biệt là những phương pháp được thiết kế cho tài nguyên hạn chế, chỉ dựa vào các bộ dữ liệu tinh chỉnh theo hướng dẫn. Tuy nhiên, phương pháp này thường dẫn đến sự suy giảm đáng kể về hiệu suất. Để giảm thiểu thách thức này, chúng tôi giới thiệu một khung phục hồi tri thức động, được trình bày như Thuật toán 5.

Đặc biệt, với các bộ sưu tập dữ liệu tiền đào tạo và tinh chỉnh theo hướng dẫn D_pretraining và D_instruct, chúng tôi đầu tiên lấy mẫu đồng đều các tập con từ chúng để xác thực D^val_pretraining và D^val_instruct. Sau đó chúng tôi xem xét việc phục hồi tri thức trên các bộ dữ liệu tiền đào tạo. Để tiến hành, chúng tôi đầu tiên đánh giá sự suy giảm hiệu suất trên các nguồn khác nhau thông qua D^val_pretraining. Dựa trên phân phối độ lệch hiệu suất, chúng tôi xây dựng một tập con D̃_pretraining ⊆ D_pretraining.

Tiêu chí để chọn các mẫu bao gồm ưu tiên các danh mục gặp phải sự suy giảm đáng kể hơn trong khi đảm bảo sự đại diện cân bằng của các mẫu từ các nguồn có sự suy giảm tối thiểu để ngăn chặn tình trạng overfitting. Sau đó, chúng tôi sử dụng LoRA để tinh chỉnh mô hình đã cắt tỉa. Nếu kết quả đánh giá không hội tụ, chúng tôi lặp lại quá trình xây dựng tập con tiếp theo từ D_pretraining cho đến khi đạt được sự hội tụ. Sau việc phục hồi tri thức từ giai đoạn tiền đào tạo, chúng tôi áp dụng cùng một phương pháp cho các bộ dữ liệu tinh chỉnh theo hướng dẫn. Phương pháp lặp lại này cuối cùng tạo ra LLM đã cắt tỉa được tối ưu hóa cao M*.

## 4. Thí nghiệm số liệu

Để chứng minh hiệu quả của LoRAShear, chúng tôi cung cấp kết quả sơ bộ trên LLAMAv1 mã nguồn mở (Touvron et al., 2023). Nhiều kết quả thí nghiệm hơn sẽ có trong các phiên bản tương lai.

### 4.1. Lựa chọn bộ dữ liệu

**Bộ dữ liệu tiền đào tạo**. Chúng tôi theo Touvron et al. để thu thập các bộ dữ liệu tiền đào tạo hoặc các lựa chọn thay thế cho tiếng Anh. Đặc biệt, chúng tôi chọn OpenWebText (Aaron Gokaslan, 2019) như một lựa chọn thay thế cho các bộ dữ liệu English CommonCrawl và C4. Chúng tôi chọn một dump Wikipedia đã xử lý năm 2022 (Foundation). Gutenberg (Gerlach & Font-Clos, 2020) và BookCorpus (Zhu et al., 2015) cũng được sử dụng trong bộ sưu tập của chúng tôi. Đối với mỗi bộ dữ liệu, chúng tôi tiến hành tiền xử lý tiêu chuẩn để xóa bỏ các ký tự bất thường và chỉ giữ lại các đoạn văn chứa hơn 64 token.

**Bộ dữ liệu tinh chỉnh theo hướng dẫn**. Để so sánh công bằng, chúng tôi theo các công trình cắt tỉa LLM có cấu trúc hiện có (Ma et al., 2023; Zhang et al., 2023) trong thiết lập tài nguyên hạn chế để sử dụng bộ dữ liệu Alpaca (Taori et al., 2023), bao gồm 52.000 hướng dẫn và minh họa được tạo ra bởi công cụ text-davinci-003 của OpenAI. Alpaca thường xuyên được sử dụng để thực hiện tinh chỉnh hướng dẫn tổng quát cho các mô hình ngôn ngữ và làm cho LLM tuân theo hướng dẫn tốt hơn.

### 4.2. Kết quả thí nghiệm

**Phân tích phân phối tri thức**. Phân phối tri thức được phân tích trên LLAMAv1 được trình bày trong Hình 4. Với một bộ dữ liệu đánh giá, chúng tôi thực hiện Thuật toán 3 để phân tích tri thức phân phối trên các cấu trúc loại bỏ tối thiểu trong mỗi nhóm nút. Sau khi đo độ lệch đầu ra, rõ ràng là tri thức được phân phối không đều trên các nhóm nút khác nhau. Những nhóm nút đầu tiên và cuối cùng đóng vai trò quan trọng hơn so với các nhóm khác đối với dự đoán mô hình. Trong quá trình cắt tỉa, sẽ tốt hơn nếu tránh cắt tỉa những nhóm nút nhạy cảm nhất này vì việc tính toán điểm số độ quan trọng vẫn có thể cắt tỉa một số cấu trúc loại bỏ tối thiểu của chúng có thể dẫn đến sự suy giảm hiệu suất đáng kể.

**Kết quả cắt tỉa**. Bây giờ chúng tôi trình bày kết quả định lượng của LoRAShear và so sánh với các phương pháp khác trên điểm chuẩn đánh giá được tính toán thông qua lm-evaluation-harness (Gao et al., 2021). Như được trình bày trong Bảng 1, dưới cùng tỷ lệ cắt tỉa 20%, LoRAShear vượt trội đáng kể so với các phương pháp khác từ 2,2%-5,0% độ chính xác và chỉ giảm không đáng kể 1% so với LLAMAv1 đầy đủ. Chúng tôi bổ sung thực hiện một nghiên cứu loại bỏ chỉ tận dụng cùng bộ dữ liệu tinh chỉnh theo hướng dẫn, tức là Alpaca, để phục hồi tri thức bị mất. Hiệu suất trong thiết lập này vẫn vượt trội hơn các phương pháp khác, ngụ ý hiệu quả của việc cắt tỉa có cấu trúc tiến bộ thông qua LHSPG để chuyển giao và bảo tồn tri thức. Dưới tỷ lệ cắt tỉa cao 50%, sự vượt trội của LoRAShear vẫn duy trì. Đặc biệt, dưới cả việc cắt tỉa có cấu trúc tiến bộ tiếp theo là phục hồi tri thức thông qua các bộ dữ liệu tiền đào tạo và tinh chỉnh theo hướng dẫn, hiệu suất của chúng tôi tốt hơn đáng kể so với các phương pháp tiên tiến nhất hiện có.

## 5. Kết luận

Chúng tôi đề xuất một LoRAShear mới để thực hiện cắt tỉa có cấu trúc hiệu quả và phục hồi tri thức cho các LLMs tổng quát trong thiết lập tài nguyên hạn chế. LoRAShear có ba điểm chính: (i) nó tự động khám phá các cấu trúc loại bỏ tối thiểu trên LLMs với các mô-đun LoRA; (ii) thực hiện cắt tỉa có cấu trúc tiến bộ thông qua một bộ tối ưu hóa thưa thớt có cấu trúc mới LHSPG tạo ra độ thưa thớt có cấu trúc trên các biến gốc thông qua thông tin được lưu trữ trong các mô-đun LoRA; và (iii) được trang bị với một giai đoạn phục hồi tri thức động để lấy lại tri thức từ cả bộ dữ liệu tiền đào tạo và tinh chỉnh theo hướng dẫn. Kết quả số liệu xác nhận hiệu quả, chỉ giảm không đáng kể 1% hiệu suất so với mô hình đầy đủ dưới tỷ lệ cắt tỉa 20% và bảo tồn 82% hiệu suất dưới tỷ lệ cắt tỉa 50% so với LLMs đầy đủ. Nhiều thí nghiệm hơn sẽ có trong các phiên bản cập nhật.
