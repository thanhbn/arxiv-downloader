# SinkLoRA: Hiệu quả Nâng cao và Khả năng Trò chuyện
cho các Mô hình Ngôn ngữ Lớn Ngữ cảnh Dài
Hengyu Zhang∗
hzha0195@student.moansh.edu
Tóm tắt
Mở rộng chức năng của mô hình Transformer để phù hợp với độ dài chuỗi dài hơn đã trở thành một thách thức quan trọng. Việc mở rộng này rất quan trọng không chỉ để cải thiện các tác vụ như dịch thuật ngôn ngữ và xử lý ngữ cảnh dài mà còn để cho phép các ứng dụng mới như chatbot, tạo mã và tạo nội dung đa phương tiện. Trở ngại chính là cơ chế self-attention, có độ phức tạp bậc hai với độ dài chuỗi về thời gian tính toán và yêu cầu bộ nhớ. LongLoRA đã đề xuất shifted sparse attention (S2-Attn), hiệu quả cho phép mở rộng ngữ cảnh và dẫn đến việc tiết kiệm tính toán đáng kể với hiệu suất tương tự như fine-tuning với vanilla attention. Tuy nhiên, LongLoRA vẫn chưa hiệu quả bằng vanilla attention, chỉ đạt 39% cải thiện perplexity so với full attention. Sự kém hiệu quả này do việc dịch chuyển chu kỳ được áp dụng trong các mẫu attention head khác nhau, gây ra sự hỗn loạn trong cấu trúc attention head hoặc trao đổi thông tin không cần thiết giữa các nhóm token. Để giải quyết những vấn đề này, chúng tôi đề xuất SinkLoRA, có phân chia công việc tốt hơn. Cụ thể, (1) chúng tôi phát triển SF-Attn với thuật toán phân đoạn và tái lắp ráp để tỷ lệ thuận trả lại các nhóm attention head được dịch chuyển chu kỳ về trạng thái không dịch chuyển cùng với global attention của "sink attention tokens", đạt 92% cải thiện perplexity so với full attention sau fine tuning, và (2) áp dụng thuật toán nén KV cache SOTA H2O để tăng tốc suy luận. Hơn nữa, chúng tôi đã tiến hành supervised fine-tuning với SinkLoRA sử dụng tập dữ liệu LongAlpaca-plus do chúng tôi tự thu thập. Tất cả mã, mô hình, tập dữ liệu và demo của chúng tôi đều có tại https://github.com/Dexter-GT-86/SinkLoRA.

1 Giới thiệu
Nâng cao chức năng của các mô hình Transformer để xử lý độ dài chuỗi dài hơn đã trở nên quan trọng cho nhiều ứng dụng, bao gồm dịch thuật ngôn ngữ, xử lý ngữ cảnh dài, chatbot, tạo mã và tạo nội dung đa phương tiện. Thách thức chính nằm ở cơ chế self-attention, có độ phức tạp bậc hai với độ dài chuỗi, dẫn đến yêu cầu thời gian tính toán và bộ nhớ đáng kể [4,41,21]. Để giải quyết thách thức này, một số phương pháp đã được đề xuất. Longformer và BigBird sử dụng kết hợp các cơ chế attention local, global và sparse để quản lý ngữ cảnh dài, giảm độ phức tạp xuống O(n) [4,41]. Reformer giới thiệu locality-sensitive hashing (LSH) để xấp xỉ attention bằng cách hash các token tương tự vào cùng một bucket, do đó giảm độ phức tạp tính toán [21]. LSG Attention kết hợp attention local, sparse và global để xử lý hiệu quả ngữ cảnh dài trong khi giảm thiểu overhead tính toán [9].

Bất chấp những tiến bộ này, việc quản lý tương tác ngữ cảnh dài trong các ứng dụng thực tế vẫn là một thách thức đáng kể. Công trình gần đây, chẳng hạn như LongLoRA, mở rộng cửa sổ ngữ cảnh của LLaMA2 từ 4096 lên 32768 token sử dụng Position Interpolation mà không cần tài nguyên GPU hoặc TPU đáng kể [6]. Tuy nhiên, hiệu quả của LongLoRA bị hạn chế, chỉ đạt 39% cải thiện perplexity so với full attention do cấu trúc attention head hỗn loạn và trao đổi thông tin không cần thiết giữa các nhóm token.

Để giải quyết những vấn đề này, chúng tôi đề xuất SinkLoRA, cung cấp phân chia công việc tốt hơn. Điều này bao gồm việc phát triển Sink Fixed Attention (SF-Attn), một thuật toán phân đoạn và tái lắp ráp mà, cùng với global attention của "sink attention tokens", đạt 92% cải thiện perplexity của full attention sau fine-tuning. Ngoài ra, chúng tôi áp dụng thuật toán nén KV cache tiên tiến, Heavy Hitter Oracle (H2O), để tăng tốc suy luận [43, 16, 25].

Chúng tôi đã nâng cao thêm SinkLoRA thông qua supervised fine-tuning sử dụng tập dữ liệu LongAlpaca-Plus do chúng tôi tự thu thập, bao gồm 28.000 mục từ các nguồn khác nhau, bao gồm Natural Questions, RedPajama [8], Book Summarization và LongQA [6], đảm bảo một bộ sưu tập đa dạng và toàn diện cho long instruction tuning.

Tóm lại, những đóng góp của công trình chúng tôi như sau:
• Chúng tôi trình bày SinkLoRA, một phương pháp hiệu quả về bộ nhớ và hiệu quả để mở rộng độ dài ngữ cảnh của LLaMA2 và LLaMA3, đại diện cho một bản cập nhật hoàn chỉnh của LongLoRA. Phương pháp này cải thiện hiệu quả fine-tuning và cung cấp một chiến lược suy luận triển khai linh hoạt.
• Chúng tôi giới thiệu SF-Attn, một phương pháp fine-tuning kết hợp thuật toán phân đoạn & tái lắp ráp và global attention. Phương pháp này dễ thực hiện, chính xác và hiệu quả về bộ nhớ, mà không tăng độ phức tạp tính toán. Bằng cách trực tiếp sửa đổi mẫu attention, SF-Attn hiệu quả phân phối lại điểm attention, giảm sự nhấn mạnh quá mức vào các token ban đầu qua các nhóm token khác nhau.
• Chúng tôi đạt được việc triển khai hiệu quả các mô hình ngôn ngữ lớn (LLM) tính toán chuyên sâu trong môi trường sản xuất bằng cách sử dụng phương pháp KV caching Heavy Hitter Oracle (H2O). Phương pháp này lưu trữ trạng thái key-value của các token được tạo trước đó, giảm đáng kể nhu cầu tính toán lặp lại và do đó giảm độ trễ trong tạo autoregressive. Cải tiến này cho phép một chiến lược suy luận linh hoạt và hiệu quả hơn, giảm overhead tính toán trong khi duy trì hiệu suất mô hình.
• SinkLoRA của chúng tôi hoạt động thuận lợi so với các phương pháp tiên tiến. Chúng tôi đánh giá hiệu suất của nó trên các tập dữ liệu PG19, Proof-pile và LongBench, thể hiện hiệu quả của nó. Cụ thể, đối với LLaMA2 7B, SinkLoRA vượt trội hơn LongLoRA và có thể cạnh tranh với LongChat-13B.

1.1 Động lực cho Nghiên cứu
Động lực 1: Nâng cao Điểm Attention cho Các Token Ban đầu
Các nghiên cứu trước đây đã chứng minh hiện tượng Attention Sink, trong đó một số token nhất định, thường là các token ban đầu trong chuỗi, nhận được điểm attention cao một cách không cân xứng trong quá trình tính toán của mô hình [39]. Điều này thường xảy ra vì các token này hiển thị với tất cả các token tiếp theo, dẫn đến attention đáng kể ngay cả khi chúng thiếu tầm quan trọng về ngữ nghĩa, đặc biệt trong các mô hình ngôn ngữ autoregressive [33].

Cơ chế Sparse Shifted Attention được triển khai trong LongLoRA [6] cố gắng giải quyết điều này bằng cách chuyển điểm attention cao từ các token ban đầu này sang các token khác trước đó nhận được attention thấp hơn. Sự dịch chuyển này làm giảm sự nhấn mạnh quá mức vào các token ban đầu. Để cải thiện thêm điều này, chúng tôi cần phát triển một phương pháp trực tiếp sửa đổi mẫu attention. Bằng cách áp dụng kỹ thuật này, chúng tôi có thể hiệu quả phân phối lại điểm attention, do đó giảm sự nhấn mạnh quá mức vào các token ban đầu qua các nhóm token khác nhau.

Động lực 2: Duy trì Các Token Ban đầu Trong Quá trình Fine-Tuning
Khái niệm attention sink cũng được sử dụng trong Streaming LLM [39] để cải thiện khả năng xử lý văn bản dài của mô hình. Bằng cách giữ lại các cặp Key-Value (KV) của một vài token ban đầu (attention sink) cùng với các token gần đây nhất, mô hình đảm bảo điểm attention và hiệu suất ổn định ngay cả đối với các chuỗi mở rộng. Lấy cảm hứng từ phương pháp này, chúng tôi nhằm mục đích mang tư duy này từ đào tạo vào suy luận. Nghiên cứu của chúng tôi nhằm sửa đổi quá trình fine-tuning để các token ban đầu attention tới tất cả các token khác, do đó tích lũy thêm điểm attention và nâng cao khả năng của mô hình để xử lý các chuỗi dài.

Động lực 3: Triển khai Linh hoạt Chiến lược Suy luận
Việc triển khai hiệu quả các mô hình ngôn ngữ lớn (LLM) tính toán chuyên sâu trong môi trường sản xuất thường dựa vào Key-Value (KV) caching [16]. KV caching lưu trữ trạng thái key-value của các token được tạo trước đó, giảm đáng kể nhu cầu tính toán lặp lại và do đó giảm độ trễ trong tạo autoregressive. Tuy nhiên, LongLoRA [6] chỉ giữ lại cơ chế self-attention tiêu chuẩn ban đầu trong quá trình suy luận. Để giải quyết hạn chế này, cần thiết phải áp dụng một chức năng KV cache tùy chọn. Cải tiến này cho phép một chiến lược suy luận linh hoạt và hiệu quả hơn, giảm overhead tính toán trong khi duy trì hiệu suất mô hình.

2 Công trình Liên quan
2.1 Transformer Ngữ cảnh Dài
Trở ngại chính trong việc mở rộng các mô hình Transformer để xử lý độ dài chuỗi dài hơn nằm ở cơ chế self-attention, có độ phức tạp bậc hai với độ dài chuỗi về thời gian tính toán và yêu cầu bộ nhớ. Gánh nặng tính toán bậc hai này đã thúc đẩy các nỗ lực nghiên cứu đáng kể tập trung vào việc phát triển các mô hình Transformer sparse hiệu quả hơn. Các ví dụ đáng chú ý bao gồm Longformer [4] và BigBird [41], sử dụng kết hợp các cơ chế attention local, global và sparse để quản lý ngữ cảnh dài, do đó giảm độ phức tạp xuống O(n). Các mô hình này đạt được sự cân bằng giữa duy trì đủ ngữ cảnh để hiểu và quản lý tải tính toán. Để đạt được độ phức tạp O(n log n), một số phương pháp đã được đề xuất. Fixed Window Attention [7] sử dụng một cửa sổ kích thước cố định cho attention, giới hạn tính toán attention trong một cửa sổ ngữ cảnh hạn chế. Reformer [21] giới thiệu locality-sensitive hashing (LSH) để xấp xỉ attention bằng cách hash các token tương tự vào cùng một bucket, do đó giảm độ phức tạp tính toán. LSG Attention [9], được điều chỉnh từ BigBird, kết hợp attention local, sparse và global để xử lý hiệu quả ngữ cảnh dài trong khi giảm thiểu overhead tính toán. Equipping Transformer [40] đề xuất một chiến lược đọc mới được gọi là random access, cho phép Transformer xử lý hiệu quả các tài liệu dài mà không cần kiểm tra mọi token. Phương pháp này cho thấy kết quả hứa hẹn qua các giai đoạn pretraining, fine-tuning và suy luận, thể hiện hiệu quả của nó trong việc xử lý ngữ cảnh mở rộng. Bất chấp những tiến bộ này, khả năng của các phương pháp này trong việc quản lý cuộc trò chuyện ngữ cảnh dài, chẳng hạn như những cuộc trò chuyện cần thiết trong ứng dụng chat, vẫn hạn chế. Điều này nêu bật một thách thức đang diễn ra trong việc nâng cao khả năng xử lý ngữ cảnh của các mô hình Transformer cho các ứng dụng tương tác và thời gian thực.

2.2 LLM Ngữ cảnh Dài
Những tiến bộ gần đây trong Mô hình Ngôn ngữ Lớn (LLM) đã mở rộng đáng kể khả năng của chúng, bao gồm xử lý đầu vào ngữ cảnh dài. Math Word Problems (MWP) đã thể hiện hiệu suất đáng chú ý trong việc giải quyết các câu hỏi toán học sử dụng LLM [34]. Hơn nữa, việc tận dụng LLM cho truy vấn SQL đã cho thấy triển vọng trong việc tối ưu hóa phân bổ tài nguyên, mặc dù vẫn kém hiệu quả hơn so với cơ sở dữ liệu quan hệ truyền thống [42]. LongLoRA [6], sử dụng Position Interpolation [5], đã thành công mở rộng cửa sổ ngữ cảnh của Llama 2 từ 4096 lên 32768 token mà không cần tài nguyên GPU hoặc TPU đáng kể. Llama 3 của Meta, có tới 70 tỷ tham số, đại diện cho một tiến bộ đáng kể trong LLM mã nguồn mở, cung cấp các cải tiến về hiệu quả tính toán, công cụ tin cậy và an toàn, và hợp tác với các nền tảng lớn [38]. Các mô hình mã nguồn mở như BLOOM [22], OPT [18] và Falcon [28] tiếp tục thách thức các mô hình độc quyền, mặc dù các mô hình như Vicuna [29] và Alpaca [1] vẫn tụt hậu so với các đối tác mã nguồn đóng ở một số khía cạnh. Bất chấp những tiến bộ này, việc quản lý hiệu quả các tương tác ngữ cảnh dài vẫn là một thách thức đáng kể, đòi hỏi nghiên cứu và phát triển liên tục để giải quyết độ phức tạp trong các ứng dụng LLM ngữ cảnh dài.

2.3 Nén KV-Cache
Nén kích thước KV cache khó khăn hơn so với giảm kích thước trọng số vì chúng nhạy cảm hơn và phụ thuộc vào đầu vào của mô hình. Một phương pháp hiệu quả về chi phí cho nén KV cache là token dropping [25,43,16], thiết lập một chính sách quan trọng để giữ lại các KV quan trọng và loại bỏ những KV không quan trọng. Jiang et al. [20] và Xiao et al. [39] đề xuất bảo tồn các token local với vị trí chuỗi hiện tại, vì chúng quan trọng cho việc tạo. Ví dụ, H2O [43] và FastGen [16] đề xuất giảm kích thước KV cache bằng cách loại bỏ token dựa trên điểm attention của chúng. Tương tự, SparQ [32] loại bỏ token theo độ sparse của điểm attention và cũng xem xét lỗi trong value cache được cắt tỉa.

Các phương pháp cắt tỉa này thường hiệu quả cho các tác vụ tóm tắt và suy luận zero-shot. Zhang et al. [43] và Liu et al. [25] khuyến nghị xác định một tập hợp nhỏ các token có ảnh hưởng, được gọi là heavy-hitter, để duy trì tốt hơn chất lượng tạo. Ge et al. [16] đã chứng minh thực nghiệm rằng các attention head khác nhau ưu tiên các token khác nhau và đã phát triển một chính sách quan trọng thích ứng để loại bỏ KV. Tuy nhiên, các phương pháp này có thể gây ra các vấn đề đáng kể vì ngữ cảnh chứa trong các KV bị loại bỏ hoàn toàn bị bỏ qua.

3 Phương pháp của Chúng tôi: SinkLoRA
3.1 Nền tảng
LongLoRA. LongLoRA, được giới thiệu bởi Chen et al. (2023) [6], là một phương pháp fine-tuning sáng tạo nhằm mở rộng hiệu quả kích thước cửa sổ ngữ cảnh của các mô hình ngôn ngữ lớn (LLM). Tận dụng Position Interpolation [5], LongLoRA xây dựng trên các low-rank adaptation được giới thiệu bởi LoRA [17] và kết hợp Shifted Sparse Attention (S2-Attn) [6]. Lấy cảm hứng từ Swin Transformer [24], S2-Attn quản lý ngữ cảnh mở rộng bằng cách phân chia tổng ngữ cảnh thành nhiều nhóm và thực hiện tính toán attention độc lập trong mỗi nhóm. Để đảm bảo tính liên tục, nó dịch chuyển một nửa số attention head bằng một nửa kích thước nhóm. Phương pháp này hiệu quả mô phỏng các hoạt động ngữ cảnh dài sử dụng attention span ngắn trong quá trình đào tạo, cho phép xử lý các ngữ cảnh lớn hơn đáng kể mà không tăng đáng kể overhead tính toán. Hơn nữa, LongLoRA được thiết kế để tương thích với các kỹ thuật và cơ sở hạ tầng tối ưu hóa LLM hiện có, chẳng hạn như Flash-Attention2 [11,10], do đó nâng cao khả năng sử dụng mà không cần sửa đổi hệ thống lớn. Phương pháp này cũng nhấn mạnh việc sử dụng tham số hiệu quả, đòi hỏi điều chỉnh tối thiểu đối với các lớp embedding và normalization có thể học được, đại diện cho một phần nhỏ của tổng tham số mô hình. Hiệu quả này rất quan trọng để mở rộng lên các mô hình lớn hơn trong khi duy trì tính thực tế của LongLoRA để nâng cao hiệu suất trong các tác vụ đòi hỏi hiểu biết ngữ cảnh sâu.

Attention Sink. Trong các mô hình ngôn ngữ lớn autoregressive (LLM), một hiện tượng thú vị được gọi là "attention sink" [39] được quan sát thấy, trong đó các token ban đầu nhận được một lượng điểm attention không cân xứng, bất kể mức độ liên quan về ngữ nghĩa của chúng với tác vụ. Các token ban đầu này, mặc dù thiếu tầm quan trọng về ngữ nghĩa đáng kể, có xu hướng tích lũy điểm attention cao. Hiện tượng này phát sinh do bản chất của hàm Softmax được sử dụng trong cơ chế attention, đảm bảo rằng điểm attention qua tất cả các token tổng cộng bằng một. Trong các tình huống mà ít token có liên quan mạnh mẽ với truy vấn hiện tại, mô hình phân phối lại attention cho các token có sẵn, thường mặc định cho các token ban đầu. Với bản chất autoregressive của LLM, trong đó mỗi token dự đoán token tiếp theo trong chuỗi, các token ban đầu luôn hiển thị với hầu hết tất cả các token tiếp theo, vốn dĩ đào tạo chúng trở thành mục tiêu ưa thích cho attention, do đó hoạt động như "attention sink". Hiểu biết này nêu bật một thách thức cơ bản trong cơ chế attention của các mô hình autoregressive và đề xuất các lĩnh vực để cải thiện chiến lược phân phối attention trong mô hình hóa ngôn ngữ.

Heavy-Hitter Oracle. Heavy-Hitter Oracle (H2O) [43] là một phương pháp SOTA giải quyết thách thức giảm dấu chân bộ nhớ của KV cache trong các mô hình ngôn ngữ. Phương pháp này dựa trên quan sát rằng một tập hợp con nhỏ của các token, được gọi là "Heavy Hitter" (H2), đóng góp đáng kể vào tổng điểm attention trong các mô hình ngôn ngữ. Phân tích cho thấy các token H2 này thường xuất hiện cùng nhau trong văn bản, làm cho sự hiện diện của chúng là kết quả tự nhiên của cấu trúc văn bản. Thực nghiệm chỉ ra rằng việc loại bỏ các token này có thể làm suy giảm đáng kể hiệu suất mô hình, nhấn mạnh tầm quan trọng của chúng. Phương pháp H2O kết hợp hiểu biết này với một chính sách loại bỏ cache động tối ưu cân bằng việc giữ lại các token gần đây và các token H2 quan trọng. Chiến lược này đảm bảo sử dụng bộ nhớ hiệu quả trong khi duy trì động lực tính toán cần thiết cho hiệu suất mô hình mạnh mẽ.

3.2 ÁP DỤNG Sink Fixed Attention để Cải thiện Quá trình Fine Tune
3.2.1 Nghiên cứu thử nghiệm
Để xác nhận phương pháp của chúng tôi, chúng tôi đã tiến hành một nghiên cứu thử nghiệm để đánh giá hiệu quả của các cơ chế attention khác nhau dưới các độ dài ngữ cảnh khác nhau.

Trong Bảng 1, chúng tôi thiết lập một baseline tiêu chuẩn bằng cách đào tạo và kiểm tra với full attention và fine-tuning, luôn mang lại chất lượng tốt qua các độ dài ngữ cảnh khác nhau. Thử nghiệm đầu tiên của chúng tôi liên quan đến đào tạo với short attention (Sparse Shifted Attention), được đại diện bởi mẫu trong Hình 2(a). Trong thử nghiệm này, do chi phí tính toán cao liên quan đến các mô-đun self-attention cho ngữ cảnh dài, để giải quyết vấn đề này, chúng tôi đã giới thiệu Sink Fixed Attention (SF-Attn) reassembly vào group window attention, như được thể hiện trong Hình 2(b). Giai đoạn này đảm bảo tính liên tục và tương tác tốt hơn giữa các token bằng cách tái lắp ráp mẫu attention, được mô tả trong Hình 2(c). Phương pháp này nâng cao cơ chế attention bằng cách làm cho các token ban đầu attention tới tất cả các token, do đó cung cấp global attention. Kết quả là, SF-Attn thể hiện cải thiện đáng kể so với Sparse Shifted Attention, giảm perplexity đáng chú ý qua tất cả các độ dài ngữ cảnh mục tiêu, đặc biệt là ở 8192 token.

Những phát hiện từ nghiên cứu thử nghiệm này chỉ ra rằng SF-Attn, thông qua các điều chỉnh reassembly và global attention, hiệu quả cân bằng hiệu suất và hiệu quả tính toán. Điều này làm cho nó trở thành một giải pháp hứa hẹn cho việc mở rộng độ dài ngữ cảnh của các mô hình ngôn ngữ lớn trong các ứng dụng thực tế. không có trao đổi thông tin giữa các nhóm khác nhau.

Chi tiết về SF-Attn sẽ được trình bày trong Phần 3.2.2

3.2.2 Sink Fixed Attention
Sink Fixed Attention bao gồm hai phần chính: Thuật toán Phân đoạn & Tái lắp ráp, Global Attention.

Thuật toán Phân đoạn & Tái lắp ráp
Như được thể hiện trong Hình 4, tensor đầu vào được chia thành hai nửa. Nửa thứ hai được chia thành hai phần: một phần ở nguyên chỗ trong khi phần khác được dịch chuyển và bao quanh. Các phần này sau đó được kết hợp lại với nhau theo một thứ tự mới. Tensor được tái lắp ráp này được xử lý thông qua cơ chế self-attention tiêu chuẩn. Cuối cùng, tensor đầu ra trải qua một quá trình tách, dịch chuyển và kết hợp lại tương tự trước khi đi qua một hoạt động feed forward để tạo ra đầu ra cuối cùng. Phương pháp này đơn giản hóa tính toán attention và cải thiện hiệu suất của mô hình bằng cách tập trung vào các khu vực chính của đầu vào. Điều này làm cho sparse shifted attention tương tự như fixed window attention. Chúng tôi cung cấp mã kiểu PyTorch trong Thuật toán 1.

Global Attention
Chúng tôi chọn bốn token ban đầu đầu tiên làm "sink attention tokens". Khi đào tạo các mô hình Llama-2-7B và Llama-3-8B, thiết lập này tuân theo cấu trúc của StreamingLLM [39]. Sử dụng chiến lược global attention từ Longformer và BigBird [4,41], chúng tôi làm cho các sink attention token attention tới tất cả các token qua chuỗi, và tất cả các token trong chuỗi attention tới các sink attention token. Cụ thể, sau Thuật toán Phân đoạn & Tái lắp ráp, chúng tôi thu được bản đồ điểm attention của tất cả các token trong cửa sổ cố định hiện tại dưới dạng một phiên bản được dịch chuyển ngược lại.

Thêm g token mới (sink attention token) vào chuỗi hiện tại. Tương ứng, xây dựng một ma trận kề mới B∈[0,1](N+g)×(N+g). Với i∈{1,2,...,g} và B(g+i,g+j)=A(i,j)∀i,j∈{1,...,N}, nó có B(i,:)=1 và B(:,i)=1.

Bằng cách làm điều này, chúng tôi giữ cho các sink attention token attention trong tính toán điểm attention trước khi đi vào bên trong các lớp ẩn. Vì số lượng các token như vậy nhỏ so với và độc lập với n, độ phức tạp của attention local và global kết hợp vẫn là O(n log n).

3.3 Áp dụng Thuật toán KV Cache để Tăng tốc Quá trình Suy luận
Trong phần này, chúng tôi áp dụng thuật toán H2O cho LongLoRA. Codebase LongLoRA [6] chỉ cung cấp giải pháp full attention cho giai đoạn suy luận. Chúng tôi kết hợp hệ thống loại bỏ H2O [43] bằng cách tạo một class mới để quản lý các giá trị KV cache trong bộ nhớ, do đó tối ưu hóa quá trình suy luận. Việc tích hợp này được minh họa trong Hình 3.

4 Thực nghiệm
4.1 Thiết lập Thực nghiệm
4.1.1 MÔI TRƯỜNG
Tất cả các thực nghiệm của chúng tôi được tiến hành trên một máy được trang bị 2×A100 GPU. Chúng tôi sử dụng PyTorch [27] làm framework chính để đào tạo tất cả các mô hình, tích hợp DeepSpeed [31] và Flash-Attention2 [10] để tối ưu hóa hiệu suất. Theo mặc định, DeepSpeed được sử dụng ở giai đoạn 2, trong khi giai đoạn 3 được dành riêng cho các thực nghiệm liên quan đến độ dài ngữ cảnh tối đa. Gradient checkpointing, một kỹ thuật tiêu chuẩn trong codebase Peft [12], được sử dụng để quản lý bộ nhớ hiệu quả. Cần lưu ý rằng trong khi các thực nghiệm của chúng tôi chủ yếu sử dụng 2×A100 GPU, việc sử dụng RTX 4090 GPU cũng khả thi cho một số tác vụ nhất định, chẳng hạn như fine-tuning các mô hình 7B đến kích thước ngữ cảnh 8192.

4.1.2 Mô hình
Chúng tôi mở rộng các mô hình được đào tạo trước 7B chat version của Llama2 [37] và 8B của Llama3 [38].

4.1.3 Quy trình Đào tạo
Để đào tạo mô hình, chúng tôi sử dụng mô hình Llama-2 và llama-3 với độ chính xác bf16 được kích hoạt và độ dài chuỗi tối đa dựa trên yêu cầu. Flash attention [10] được sử dụng để tính toán hiệu quả. Low-rank training được kích hoạt, và mô hình trải qua ba epoch đào tạo với batch size 1 cho đào tạo và 2 cho đánh giá mỗi thiết bị. Gradient accumulation được đặt thành 1 bước. Chiến lược đánh giá bị vô hiệu hóa, trong khi mô hình được lưu mỗi 512 bước với tổng giới hạn 2 checkpoint được lưu. Learning rate được đặt thành 2e-5 không có weight decay, và một giai đoạn warmup 20 bước được bao gồm. Learning rate tuân theo lịch trình constant với warmup. Log được ghi lại mỗi bước, và quá trình đào tạo được tối ưu hóa sử dụng cấu hình DeepSpeed (stage2.json) [31] với độ chính xác tf32 được kích hoạt.

4.1.4 Tập dữ liệu
Tập dữ liệu "LongAlpaca-Plus", chứa 28.000 mục, được cấu trúc với trọng tâm đáng kể trên các nguồn khác nhau, đây là phiên bản cập nhật của LongAlpaca [6]. Natural Questions chiếm 44% tập dữ liệu, tổng cộng 12.320 mục, có nguồn gốc từ một tập con dữ liệu Natural Questions. RedPajama đóng góp 27% tập dữ liệu với 7.560 mục, cũng có nguồn gốc từ tập con cụ thể của nó. Book Summarization bao gồm 18% tập dữ liệu với 5.040 mục, và LongQA đại diện cho phần nhỏ nhất ở 11%, với 3.080 mục. Phân phối này đảm bảo một tập dữ liệu đa dạng và toàn diện cho các tác vụ long instruction tuning. Tổng quan về tập dữ liệu LongAlpaca-Plus được cung cấp trong Phần B.

4.1.5 Chỉ số
Perplexity Perplexity, một chỉ số quan trọng trong xử lý ngôn ngữ tự nhiên (NLP), định lượng sự không chắc chắn dự đoán của mô hình ngôn ngữ, có mối quan hệ nghịch đảo với xác suất được gán cho chuỗi từ thực tế. Có nguồn gốc từ lý thuyết thông tin, perplexity đo lường hiệu quả của hệ thống truyền thông, và trong NLP, nó phản ánh mức độ hiểu biết của mô hình ngôn ngữ về các mẫu ngôn ngữ [19]. Về mặt toán học, nó được định nghĩa là

Perplexity = e^(-1/N ∑_{i=1}^N log p(w_i|w_{i-1},...,w_1))     (1)

trong đó p(x) là phân phối xác suất trên tất cả các chuỗi có thể x. Được sử dụng rộng rãi trong việc đánh giá các mô hình ngôn ngữ, từ các phương pháp n-gram ban đầu đến các kiến trúc neural tiên tiến, perplexity hỗ trợ so sánh hiệu suất mô hình, điều chỉnh trong quá trình phát triển và nâng cao các tác vụ như dịch máy và tạo văn bản [19], [14]. Như một biện pháp định lượng, nó cung cấp cái nhìn sâu sắc về cải tiến mô hình và hiệu quả của các cấu hình khác nhau, làm cho nó trở thành một công cụ không thể thiếu trong đánh giá mô hình ngôn ngữ [15].

Passkey Retrieval Chúng tôi sử dụng định dạng tương tự được cung cấp trong LongLoRA [6] cho passkey retrieval. Tài liệu tuân theo cấu trúc này:

Trong một lượng lớn văn bản ngoài lề, thông tin quan trọng được che giấu. Xác định và ghi nhớ thông tin chính này, vì bạn sẽ được kiểm tra về nó.

Đây là một ví dụ cấu trúc tài liệu:
———————————————————————————————————————–
Hoa đang nở. Cây cao. Dòng sông chảy. Cứ tiếp tục. Tiến lên và lên trên.
(lặp lại M lần)
Ghi chú Quan trọng: Passkey là 84729. Hãy nhớ số này. 84729 là passkey.
Hoa đang nở. Cây cao. Dòng sông chảy. Cứ tiếp tục. Tiến lên và lên trên.
(lặp lại N lần)
Passkey là gì? Passkey là...————————————————————————————————————– Độ dài của tài liệu thay đổi dựa trên giá trị của M và N. Số passkey, chẳng hạn như 84729, được tạo ngẫu nhiên và thay đổi với mỗi lần kiểm tra.

LongBench LongBench [3] là một benchmark song ngữ, đa tác vụ được thiết kế để đánh giá khả năng hiểu ngữ cảnh dài của các mô hình ngôn ngữ lớn (LLM). Nó tập trung vào việc đánh giá hiệu suất của các mô hình trong việc xử lý đầu vào văn bản mở rộng bằng cả tiếng Anh và tiếng Trung. Benchmark bao gồm 21 tập dữ liệu bao gồm sáu danh mục tác vụ: single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks và code completion.

4.2 Kết quả Chính
Kết quả Perplexity.
Phân tích dữ liệu từ Bảng 2 và Bảng 3, chúng tôi quan sát thấy như sau:

Bảng 2 trình bày đánh giá perplexity trên tập kiểm tra proof-pile sử dụng ba cơ chế attention: Full-Attn, S2-Attn và SF-Attn cho kích thước mô hình 7B và 8B. Mô hình 7B-Llama2, với độ dài ngữ cảnh đào tạo 8192, cho thấy rằng cơ chế SF-Attn luôn vượt trội hơn các cơ chế attention khác qua tất cả các độ dài ngữ cảnh mục tiêu (4096, 6144 và 8192). Cụ thể, các giá trị perplexity cho SF-Attn là 8.60, 8.17 và 7.85, tương ứng, cho thấy hiệu suất hiệu quả hơn. Tương tự, mô hình 8B-Llama3, được đào tạo với cùng độ dài ngữ cảnh, thể hiện điểm perplexity thấp hơn khi sử dụng cơ chế SF-Attn: 5.11, 4.86 và 4.66 cho độ dài ngữ cảnh mục tiêu 4096, 6144 và 8192, tương ứng. Điều này chứng minh rằng cơ chế SF-Attn cung cấp cải thiện đáng chú ý về perplexity so với cả Full-Attn và S2-Attn.

Bảng 3 tóm tắt độ dài ngữ cảnh tối đa mà chúng tôi có thể fine-tune cho các kích thước mô hình khác nhau sử dụng một máy đơn 2x A100. Cả mô hình Llama2 và Llama3 đều được đánh giá sử dụng cùng thiết lập đào tạo. Mô hình 7B-Llama2 cho thấy kết quả perplexity là 8.73, 8.55, 8.30, 8.13 và 8.05 cho độ dài ngữ cảnh đánh giá 2048, 4096, 8192, 12288 và 16384, tương ứng. Mô hình 8B-Llama3 đạt được giá trị perplexity 5.11, 5.02, 4.89, 4.77 và 4.72 cho cùng độ dài ngữ cảnh đánh giá.

Những kết quả này chỉ ra rằng cơ chế SF-Attn vượt trội hơn cả cơ chế Full-Attn và S2-Attn về perplexity, đặc biệt đối với độ dài ngữ cảnh lớn hơn. Điều này xác nhận hiệu quả và hiệu suất của thuật toán H2O trong framework LongLoRA.

Kết quả Passkey Retrieval.
Phân tích dữ liệu từ Bảng 4 và Hình 5, chúng tôi quan sát thấy như sau:

Bảng 4 trình bày đánh giá topic retrieval với LongChat. Tác vụ này liên quan đến việc truy xuất các chủ đề mục tiêu từ các cuộc trò chuyện rất dài với độ dài ngữ cảnh 3k, 6k, 10k, 13k và 16k. Mô hình của chúng tôi, được fine-tune trên Llama3 8B, đạt hiệu suất tương đương với LongChat-13B tiên tiến với chi phí fine-tuning thấp hơn. Cụ thể, mô hình của chúng tôi duy trì độ chính xác 1.00 cho độ dài ngữ cảnh lên đến 10k và cho thấy giảm nhẹ xuống 0.98 và 0.96 cho độ dài ngữ cảnh 13k và 16k, tương ứng. Điều này chứng minh tính mạnh mẽ và hiệu quả của mô hình chúng tôi trong việc xử lý các tác vụ truy xuất ngữ cảnh dài.

Hình 5 so sánh độ chính xác passkey retrieval giữa Llama2 7B, LongLoRA 7B và mô hình 7B của chúng tôi được fine-tune trên độ dài ngữ cảnh 32.768. Mô hình của chúng tôi không cho thấy sự suy giảm độ chính xác truy xuất lên đến 33k hoặc 36k, vượt qua giới hạn độ dài ngữ cảnh của LongLoRA, là 30k và 34k. Điều này chỉ ra rằng mô hình của chúng tôi có thể xử lý độ dài ngữ cảnh dài hơn với độ chính xác cao hơn so với các mô hình hiện có.

Kết quả LongBench.
Phân tích dữ liệu từ Bảng 6, chúng tôi quan sát thấy GPT-3.5-Turbo luôn vượt trội hơn các mô hình khác qua các tác vụ khác nhau, đạt điểm trung bình cao nhất (44.0). Điều này chỉ ra hiệu suất tổng thể vượt trội của nó. Đáng chú ý, mô hình của chúng tôi, Ours-7B, đạt điểm trung bình cao thứ hai (38.8), thể hiện hiệu suất cạnh tranh. Cụ thể, Ours-7B xuất sắc trong tác vụ Code với điểm dẫn đầu 59.2, vượt qua GPT-3.5-Turbo (54.1).

Bất chấp những điểm mạnh này, có những lĩnh vực cần cải thiện, đặc biệt trong tác vụ Synthetic trong đó Ours-7B ghi điểm 22.3, thấp hơn đáng kể so với 37.8 của GPT-3.5-Turbo. Điều này nêu bật nhu cầu nâng cao khả năng của mô hình chúng tôi trong các tác vụ synthetic để tiếp tục thúc đẩy hiệu suất tổng thể. Nhìn chung, trong khi GPT-3.5-Turbo vẫn là người dẫn đầu về hiệu suất, mô hình của chúng tôi cho thấy kết quả hứa hẹn và tiềm năng tối ưu hóa.

Kết quả H2O Inference
Trong phân tích này, chúng tôi kiểm tra hiệu suất và hiệu quả của các phương pháp KV cache khác nhau cho mô hình LLaMA-3-8B, như được mô tả trong Hình 5 và Bảng 6.

Hình 5 trình bày một phân tích so sánh giữa mô hình baseline sử dụng full cache, phương pháp H2O và chiến lược "Local", sử dụng các KV embedding gần đây nhất. Đánh giá được tiến hành trên tập dữ liệu OpenBookQA. Đáng chú ý, phương pháp Heavy-Hitter Oracle (H2O) duy trì mức độ chính xác cao ngay cả với ngân sách KV cache giảm, chứng minh hiệu quả của nó trong việc bảo tồn hiệu suất mô hình. Ngược lại, phương pháp Local thể hiện sự suy giảm đáng kể về độ chính xác khi ngân sách KV cache giảm.

Bảng 6 chi tiết về giờ suy luận cần thiết cho LLaMA-3-8B dưới các ngân sách KV cache khác nhau. Phương pháp H2O giảm đáng kể thời gian suy luận so với phương pháp Full qua tất cả các ngân sách cache. Ví dụ, ở ngân sách KV cache 100%, phương pháp H2O giảm thời gian suy luận từ 4.7 giờ (Full) xuống 4.4 giờ. Việc giảm này thậm chí còn đáng kể hơn ở ngân sách KV cache thấp hơn, nhấn mạnh hiệu quả của phương pháp H2O trong môi trường có tài nguyên hạn chế.

Kết luận, phương pháp H2O cung cấp sự cân bằng tối ưu giữa duy trì độ chính xác cao và giảm thời gian suy luận, do đó đại diện cho một chiến lược có giá trị để tối ưu hóa các mô hình ngôn ngữ lớn như LLaMA-3-8B. Khả năng của phương pháp này trong việc cung cấp tiết kiệm tính toán đáng kể trong khi bảo tồn hiệu suất nêu bật tiềm năng của nó cho các ứng dụng rộng hơn trong các cài đặt hạn chế tài nguyên.

4.3 Nghiên cứu Ablation
Ablation về các bước đào tạo SF-Attn
Để đánh giá hiệu quả của các thành phần khác nhau trong phương pháp đào tạo SF-Attn của chúng tôi, chúng tôi đã tiến hành một loạt các nghiên cứu ablation. Bảng 7 trình bày kết quả perplexity cho các phương pháp đào tạo khác nhau sử dụng tập validation của tập dữ liệu PG-19. Phương pháp SF-Attn đơn lẻ cho perplexity 8.64, thiết lập baseline để so sánh. Giới thiệu global attention cho S2-Attn dẫn đến perplexity 8.91, cho thấy rằng global attention đơn lẻ không cải thiện đáng kể hiệu suất. Kết hợp S2-Attn với Thuật toán Phân đoạn và Tái lắp ráp (S&R) dẫn đến perplexity 8.86, cho thấy cải thiện vừa phải so với S2-Attn đơn lẻ. Phương pháp baseline S2-Attn đơn lẻ có perplexity cao hơn là 9.09 so với baseline SF-Attn, chứng minh nhu cầu cải tiến bổ sung để đạt hiệu suất tốt hơn. Những kết quả này chỉ ra rằng trong khi phương pháp SF-Attn đơn lẻ có hiệu quả, kết hợp nó với các kỹ thuật bổ sung như Thuật toán S&R có thể mang lại cải thiện về perplexity.

Các nghiên cứu ablation tiếp theo được tiến hành để phân tích tác động của các thành phần riêng lẻ của Thuật toán Phân đoạn và Tái lắp ráp (S&R). Bảng 8 cung cấp kết quả perplexity cho những thực nghiệm này. Phương pháp baseline S2-Attn có perplexity 9.09. Khi mô hình chỉ áp dụng bước "shift up", perplexity tăng lên 9.35, cho thấy rằng bước này đơn lẻ không đủ để cải thiện hiệu suất. Chỉ áp dụng bước "shift back" dẫn đến perplexity 9.60, cho thấy bước này đơn lẻ cũng không đủ. Thuật toán Phân đoạn và Tái lắp ráp hoàn chỉnh đạt perplexity 8.86, chứng minh hiệu quả của việc kết hợp cả hai bước dịch chuyển. Những nghiên cứu ablation này nêu bật tầm quan trọng của Thuật toán Phân đoạn và Tái lắp ráp trong việc nâng cao hiệu suất của phương pháp đào tạo SF-Attn. Bằng cách tích hợp cả bước "shift up" và "shift back", Thuật toán S&R hiệu quả giảm perplexity và cải thiện hiệu suất mô hình.

Ablation về Biến thể SF-Attn
Tập trung cụ thể vào Sparse Fixed Attention (SF-Attn) và các biến thể của nó. Hình 7 minh họa các mẫu attention khác nhau: Sparse Fixed Attention, Stride Attention và Random Attention. Những biến thể này đại diện cho các chiến lược khác nhau để phân phối attention qua chuỗi token. Sparse Fixed Attention sử dụng một mẫu sparse cố định, Stride Attention phân phối attention theo mẫu stride, và Random Attention phân bổ attention ngẫu nhiên.

Bảng 9 trình bày kết quả fine-tuning mô hình Llama2 7B trên các chuỗi với độ dài ngữ cảnh mục tiêu khác nhau (4096, 6144 và 8192) và đánh giá trên tập validation PG19. Hiệu suất được đo lường qua các thiết lập khác nhau của cơ chế attention. Các quan sát chính bao gồm rằng Full Attention (Full-Attn) cho thấy hiệu suất nhất quán nhưng tốn kém về tính toán. S²-Attention (S²-Attn) hoạt động tốt hơn một chút so với Full Attention, đặc biệt ở độ dài ngữ cảnh dài hơn. Sparse Fixed Attention duy trì hiệu suất cạnh tranh và hiệu quả hơn. Stride và Random Attention cũng hoạt động tốt nhưng hơi thấp hơn Sparse Fixed Attention. Phương pháp SF-Attention, kết hợp các yếu tố của những chiến lược này, đạt được sự cân bằng tốt nhất với điểm perplexity thấp nhất, cho thấy nó là cơ chế attention hiệu quả và hiệu suất nhất để xử lý các chuỗi dài.

5 Thảo luận
5.1 Thất bại của Hoạt động Shift
Hoạt động shift của Sparse Shift Attention [6] được lấy cảm hứng từ Swin Transformer [24]. Nó nhằm mục đích tạo điều kiện cho việc trao đổi thông tin nhiều hơn giữa các nhóm token khác nhau. Tuy nhiên, phương pháp này cũng dẫn đến vấn đề rò rỉ thông tin.

Vấn đề rò rỉ thông tin có thể phát sinh vì hoạt động shift cho phép các token từ các phân đoạn khác nhau chia sẻ thông tin quá tự do. Mặc dù ý định là nâng cao khả năng của mô hình trong việc tích hợp thông tin từ các phần khác nhau của chuỗi, nó vô tình khiến các token truy cập thông tin mà chúng không nên biết ở giai đoạn xử lý cụ thể đó. Việc tiếp xúc sớm với thông tin này có thể làm gián đoạn quá trình học của mô hình, dẫn đến overfitting hoặc liên kết không chính xác.

Hơn nữa, hoạt động shift có thể làm mờ các ranh giới riêng biệt giữa các nhóm token, khiến mô hình mất dấu vết của ngữ cảnh cụ thể trong mỗi nhóm. Điều này có thể dẫn đến suy giảm hiệu suất, vì mô hình có thể gặp khó khăn trong việc duy trì hiểu biết nhất quán về ngữ cảnh local, điều này rất quan trọng để diễn giải và tạo ra các chuỗi dài một cách chính xác.

Tóm lại, mặc dù hoạt động shift nhằm mục đích cải thiện trao đổi thông tin, nó vô tình gây ra rò rỉ thông tin bằng cách cho phép các token truy cập sớm và tích hợp thông tin từ các phân đoạn khác nhau. Điều này làm suy yếu khả năng của mô hình trong việc duy trì các ranh giới ngữ cảnh riêng biệt và có thể tác động tiêu cực đến hiệu suất của nó trong các tác vụ đòi hỏi hiểu biết ngữ cảnh chính xác.

5.2 Thành công của Việc Làm cho Sink Attention Token Hoạt động Toàn cầu
Phương pháp được mô tả trong phần sau chi tiết về việc triển khai thành công các sink attention token hoạt động toàn cầu, nâng cao khả năng của mô hình trong việc xử lý các chuỗi dài một cách hiệu quả.

Những kết quả thành công quan sát được từ việc triển khai sink attention token có thể được quy cho một số yếu tố chính. Đầu tiên, bằng cách chỉ định các token cụ thể làm sink attention token attention tới tất cả các token khác trong chuỗi, mô hình có thể hiệu quả nắm bắt và tích hợp thông tin từ toàn bộ chuỗi. Cơ chế global attention này đảm bảo rằng thông tin quan trọng không bị mất, ngay cả trong các chuỗi rất dài.

Hơn nữa, bằng cách đảm bảo rằng tất cả các token trong chuỗi cũng attention tới các sink attention token, mô hình có thể duy trì hiểu biết nhất quán về ngữ cảnh chuỗi. Luồng attention hai chiều này cho phép mô hình củng cố thông tin quan trọng ở nhiều giai đoạn, nâng cao khả năng hiểu và giữ lại ngữ cảnh tổng thể.

Độ phức tạp hiệu quả O(n log n) đạt được thông qua phương pháp này, do số lượng sink attention token tương đối nhỏ, đảm bảo rằng khả năng nâng cao này không đi kèm với chi phí tăng đáng kể overhead tính toán. Sự cân bằng này giữa duy trì attention toàn diện và hiệu quả tính toán có thể là một yếu tố chính trong việc cải thiện hiệu suất quan sát thấy trong các mô hình sử dụng kỹ thuật này.

Tóm lại, việc triển khai các sink attention token hoạt động toàn cầu cho phép mô hình duy trì hiểu biết mạnh mẽ và nhất quán về các chuỗi dài, đảm bảo thông tin quan trọng được attention tới trong suốt các giai đoạn xử lý, do đó dẫn đến hiệu suất cải thiện trong các tác vụ đòi hỏi hiểu biết ngữ cảnh rộng lớn.

5.3 Sự Tiện lợi của Việc Áp dụng Chức năng Nén KV Cache
Như các kết quả được thể hiện trong kiểm tra suy luận H2O cho thấy, phương pháp H2O có thể duy trì độ chính xác ngay cả khi ngân sách KV cache bị giảm một nửa. Tuy nhiên, việc tiết kiệm thời gian đạt được với phương pháp này là đáng kể, giảm thời gian suy luận với hệ số 1.5. Khả năng này cho phép triển khai linh hoạt các phương pháp suy luận dựa trên tài nguyên tính toán có sẵn. Khả năng điều chỉnh ngân sách KV cache mà không ảnh hưởng đến độ chính xác đảm bảo rằng các mô hình ngôn ngữ lớn có thể được sử dụng hiệu quả và hiệu suất trong môi trường hạn chế tài nguyên, tối ưu hóa cả hiệu suất và hiệu quả tính toán.

5.4 Kết quả Tốt của Chatbot
Như được thể hiện trong ví dụ so sánh khả năng chat trong Phần C, chúng tôi quan sát các phản hồi từ hai phiên bản của mô hình LongLoRA đối với một câu hỏi liên quan đến câu chuyện.

Đầu vào của người dùng mô tả một câu chuyện liên quan đến một ngư dân già tên Tom, người mạo hiểm ra biển xa hơn đến một nơi được gọi là "Blue Deep" với hy vọng tìm thấy cá sau vài tuần không thành công. Câu hỏi hỏi tại sao Tom quyết định mạo hiểm ra biển xa hơn bao giờ hết.

Phản hồi của mô hình LongLoRA-7B tập trung vào sự tuyệt vọng và lo lắng của Tom về việc không bắt được cá, nhấn mạnh việc anh ta liên tục trở về với lưới trống và quyết tâm nuôi gia đình. Phản hồi này chính xác nắm bắt các yếu tố chính của động lực của Tom như được mô tả trong câu chuyện.

Mặt khác, mô hình SinkLoRA-7B cung cấp giải thích chi tiết hơn, nêu bật sự biến mất kéo dài của cá và quyết định kết quả của Tom về việc chấp nhận rủi ro mạo hiểm đến "Blue Deep". Nó cũng đề cập đến hy vọng của Tom tìm thấy cá ở đó, cung cấp hiểu biết toàn diện hơn về động lực của anh ta.

Lý do cho các mức độ chi tiết và ngữ cảnh khác nhau trong các phản hồi có thể được quy cho sự khác biệt về đào tạo và fine-tuning giữa hai mô hình. Mô hình SinkLoRA-7B có thể đã được đào tạo trên tập dữ liệu đa dạng hơn hoặc trải qua fine-tuning bổ sung để hiểu tốt hơn và tạo ra các phản hồi phong phú về ngữ cảnh. Đào tạo bổ sung này có thể cho phép nó cung cấp các câu trả lời sắc thái và chi tiết hơn, nắm bắt các sự tinh tế của câu chuyện một cách hiệu quả hơn.

Kết luận, cả hai mô hình đều thành công xác định động lực chính của Tom, nhưng mô hình SinkLoRA-7B cung cấp giải thích kỹ lưỡng và phong phú về ngữ cảnh hơn. Sự so sánh này nhấn mạnh hiệu quả của khả năng chat trong việc hiểu và phản hồi chính xác các câu hỏi dựa trên câu chuyện. Những khác biệt quan sát được nêu bật tầm quan trọng của đào tạo rộng lớn và fine-tuning trong việc nâng cao hiệu suất mô hình và chất lượng phản hồi.

6 Kết luận và Công trình Tương lai
Trong nghiên cứu này, chúng tôi đã giới thiệu SinkLoRA, một cải tiến đáng kể so với LongLoRA ban đầu, được thiết kế để cải thiện hiệu quả và hiệu suất của các mô hình ngôn ngữ lớn (LLM) trong việc xử lý các chuỗi ngữ cảnh dài. SinkLoRA giải quyết những hạn chế của mô hình trước bằng cách triển khai Sink Fixed Attention (SF-Attn) và sử dụng các kỹ thuật nén KV cache tiên tiến như Heavy-Hitter Oracle (H2O). Phương pháp SF-Attn đề xuất của chúng tôi hiệu quả phân phối lại điểm attention, giảm sự nhấn mạnh quá mức vào các token ban đầu và cải thiện độ chính xác tổng thể của mô hình. Phương pháp này, kết hợp với thuật toán phân đoạn và tái lắp ráp, cho phép xử lý tốt hơn các ngữ cảnh mở rộng mà không tăng độ phức tạp tính toán. Việc tích hợp nén KV cache H2O tiếp tục tăng tốc suy luận, làm cho SinkLoRA trở thành một giải pháp hiệu quả cao để triển khai LLM trong môi trường hạn chế tài nguyên.

Công trình tương lai sẽ tập trung vào việc tiếp tục tối ưu hóa các cơ chế attention và khám phá khả năng tương thích của SinkLoRA với các loại LLM và position encoding khác. Chúng tôi cũng lên kế hoạch điều tra các kỹ thuật quản lý KV cache tiên tiến hơn để nâng cao tính linh hoạt và hiệu quả của các quá trình suy luận. Mục tiêu là tiếp tục cải thiện hiệu suất và khả năng mở rộng của LLM, cho phép ứng dụng của chúng trong một phạm vi rộng hơn các tác vụ và môi trường. Tóm lại, SinkLoRA đại diện cho một bước tiến đáng kể trong việc phát triển các kỹ thuật xử lý ngữ cảnh dài hiệu quả cho các mô hình ngôn ngữ lớn, cung cấp các hướng hứa hẹn cho nghiên cứu và ứng dụng tương lai.
