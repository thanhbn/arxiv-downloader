# 2309.09958.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.09958.pdf
# File size: 126855 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.09958v1  [cs.CV]  18 Sep 2023An Empirical Study of Scaling Instruction-Tuned
Large Multimodal Models
Yadong Lu∗1, Chunyuan Li∗2, Haotian Liu3, Jianwei Yang2, Jianfeng Gao2, Yelong Shen1
1Microsoft Azure AI2Microsoft Research3University of Wisconsin–Madison
Abstract
Visual instruction tuning has recently shown encouraging p rogress with open-
source large multimodal models (LMM) such as LLaV A and MiniG PT-4. How-
ever, most existing studies of open-source LMM are performe d using models with
13B parameters or smaller. In this paper we present an empiri cal study of scal-
ing LLaV A up to 33B and 65B/70B, and share our ﬁndings from our explorations
in image resolution, data mixing and parameter-efﬁcient tr aining methods such
as LoRA/QLoRA. These are evaluated by their impact on the mul ti-modal and
language capabilities when completing real-world tasks in the wild. We ﬁnd that
scaling LMM consistently enhances model performance and im proves language
capabilities, and performance of LoRA/QLoRA tuning of LMM a re comparable
to the performance of full-model ﬁne-tuning. Additionally , the study highlights
the importance of higher image resolutions and mixing multi modal-language data
to improve LMM performance, and visual instruction tuning c an sometimes im-
prove LMM’s pure language capability. We hope this study mak es state-of-the-art
LMM research at a larger scale more accessible, thus helping establish stronger
baselines for future research. Code and checkpoints will be made public.
1 Introduction
Recent studies on large multimodal models (LMM) [ 9,10] have been focused on the methods of
visual instruction tuning [12]. The results are promising: e.g., the open-source project Large Lan-
guage and Vision Assistant (LLaV A) shows that training a 7B l arge language model (LLM) with
multimodal instruction-following data for 3 hours on 8 A-10 0 GPUs leads to a LMM with strong
visual understanding and reasoning capabilities in the wil d: reproducing some of the most appealing
examples of the proprietary OpenAI multimodal GPT-4 model [ 14]. A similar idea is explored in
its co-current work MiniGPT-4 [ 20]. It has rapidly become a prominent research topic, spurrin g the
development of numerous new models, benchmarks, and applic ations [ 10]. However, the high com-
pute cost has led most existing studies to utilize 7B and 13B L LMs. Thus, the impact of signiﬁcantly
scaling up the model size to e.g., 33B and 65B remains unexplored.
This study aims to ﬁll this gap by empirically investigating language models of larger sizes for LMM,
sharing insights of our scaling experiments and establishi ng stronger baselines using larger-scale
LLaV A for future research. Speciﬁcally, we explore the impa ct of larger model sizes, model tuning
and data mixing methods on model performance, and present ou r ﬁndings and recommendations.
The scaling recipe leads to new state-of-the-art (SoTA) per formance on LLaV A-Bench [ 12] and
MM-VET [ 19]. We hope that our ﬁndings and larger LLaV A checkpoints woul d provide a reference
for future research on visual instruction tuning.
*These authors contributed equally to this work
Preprint. Work in progress

--- PAGE 2 ---
2 Experiment Setup
Model Checkpoints. To study the impact of scaling up LLM on multimmodal capabili ties, we
increase the language model size to 33B and 65B [ 15], in addition to the 7B and 13B models used
for existing LMM.
•LLaV A-33B We employ the open source Vicuna-33B checkpoint1[16] to preform the two-
stage training. The training data is around 125K conversati ons collected from ShareGPT.com .
•LLaV A-65B Due to a lack of public 65B Vicuna checkpoint, we conduct our o wn training of
the Vicuna-65B model, utilizing ShareGPT data that we have i ndependently processed. This
data contains 159M tokens used during training. As a compari son, the reported number of
tokens used in training Vicuna 33B is 370M2.
Once the instruction-tuned LLM is given, we follow [ 12] to perform the two-stage LLaV A lightning
training: (i)Stage 1: Pre-training for Feature Alignment. The linear projection layer is trained,
which maps the visual feature (the features before the last l ayer of the pre-trained image encoder)
to word embedding space of LLM. More specifcally, the projec tion dimension is 1024 →6656 for
the 33B model and 1024 →8192 for the 65B model, respectively. In this stage, we use th e concept-
balanced subset of LAION-CC-SBU data with 558K samples. (ii)Stage 2: Visual Instruction
Tuning. We use the LLaV A-80K multimodal instruct dataset for the ﬁne -tuning stage. Various
training schedules are explored to enable the model to follo w the diverse instructions to complete
tasks in the wild, as to be detailed below.
Tuning Methods. We explore both the trainable modules and training data mixi ng for efﬁcient
and effective visual instruct tuning of large models.
•Trainable modules. In addition to tuning the linear projection layer, two schem es are consid-
ered to tune the LLM: (i)Full-model ﬁne-tuning of LLM and (ii)Parameter-efﬁcient training
methods. For the latter, LoRA [ 7] and QLoRA [ 4] are employed to allow us to tune large mod-
els with limited compute resource. This aims to gain an in-de pth understanding of the trade-off
between the training cost and model performance.
•Data mixing. Typically only the multimodal instruction data is used in St age-2. We further
consider mixing the language-only instruct data ShareGPT w ith the LLaV A-80K multimodal
instruction data to gain an in-depth understanding of the tr ade-off between models’ language
and multimodal capabilities.
Hyper-parameters. In the training process of both stages, we utilize the DeepSp eed library3and
employ the ZeRO3 optimizer, except for QLoRA runs we use ZeRO 2. We use a maximum sequence
length of 2048. For Stage 1, we train both the 33B and 65B model s with a learning rate of 1×10−4
with no weight decay, and a learning rate with linear decay an d linear warmup for 3% of training
steps in total. For Stage 2, we use a learning rate of 2×10−5in full ﬁne-tuning to train 1 epoch
for all the models in full ﬁnetuning, and a learning rate of 1×10−4for the LoRA/QLoRA runs.
We conducted a set of hyperparameter search and for LoRA runs , and found larger LoRA alpha or
equivalently larger learning rate was crucial to get the bes t performance. Speciﬁcally, we use LoRA
alpha equals 2 times the LoRA rank, and a learning rate of 1×10−4, which works the best for all the
models. For full ﬁne-tuning, we use a total batch size of 512 o n 4 A100 nodes, where each of these
nodes is equipped with 8 A100-80G GPUs. For LoRA/QLoRA runs, we use a total batchsize of 64
on 1 A100 node for 33B model and 2 nodes for 65B model.
3 Results
We ﬁrst compare our large checkpoints on two recent benchmar ks which are speciﬁcally designed
for LMM, then report our ﬁndings in the course of scaling up LL aV A models.
1https://huggingface.co/lmsys/vicuna-33b-v1.3
2https://github.com/lm-sys/FastChat/blob/main/docs/v icuna_weights_version.md
3https://github.com/microsoft/DeepSpeed
2

--- PAGE 3 ---
Models Reasoning Conversation Detail Overall
Bard-0718 78.7 83.7 69.7 77.8
Bing-Chat-0629 90.1 59.6 52.2 71.5
LLaV A-13B (beam=1) 81.7 64.3 55.9 70.1
LLaV A-13B (beam=5) 84.3 68.4 59.9 73.5
LLaV A-33B (beam=1) 82.9 70.2 62.6 73.9
LLaV A-33B (beam=5) 83.5 72.6 61.9 74.8
LLaV A-65B (beam=1) 87.3 63.8 62.3 74.2
LLaV A-65B (beam=5) 88.7 59.4 65.7 74.4
Table 1: The performance comparison on LLaV A-Bench. Beam se arch sizes at 1 and 5 are reported.
Model Rec OCR Knowledge Generation Spatial Math Total
Results of various open-source LMM on reported in the MM-VET paper [ 19]
LLaMA-Adapter v2-7B [ 5] 16.8 7.8 2.5 3.0 16.6 4.4 13.6±0.2
OpenFlamingo-9B [ 1,2] 24.6 14.4 13.0 12.3 18.0 15.0 21.8±0.1
MiniGPT-4-8B [ 20] 27.4 15.0 12.8 13.9 20.3 7.7 22.1±0.1
BLIP-2-12B [ 11] 27.5 11.1 11.8 7.0 16.2 5.8 22.4±0.2
LLaV A-7B [ 12] 28.0 17.1 16.3 18.9 21.2 11.5 23.8±0.6
MiniGPT-4-14B [ 20] 29.9 16.1 20.4 22.1 22.2 3.8 24.4±0.4
Otter-9B [ 8] 28.4 16.4 19.4 20.7 19.3 15.0 24.6±0.2
InstructBLIP-14B [ 3] 30.8 16.0 9.8 9.0 21.1 10.5 25.6±0.3
InstructBLIP-8B [ 3] 32.4 14.6 16.5 18.2 18.6 7.7 26.2±0.2
LLaV A-13B [ 12] 30.9 20.1 23.5 26.4 24.3 7.7 26.4±0.1
MM-ReAct-GPT-3.5 [18] 24.2 31.5 21.5 20.7 32.3 26.2 27.9±0.1
LLaV A-7B (LLaMA-2) [ 12] 32.9 20.1 19.0 20.1 25.7 5.2 28.1±0.4
LLaV A-13B (V1.3, 336px) [ 12]38.1 22.3 25.2 25.8 31.3 11.2 32.5±0.1
LLaV A-13B (LLaMA-2) [ 12] 39.2 22.7 26.5 29.3 29.6 7.7 32.9±0.1
MM-ReAct-GPT-4 [18] 33.1 65.7 29.0 35.0 56.8 69.2 44.6±0.2
Results with our own experiment runs
LLaV A-13B (LLaMA-2) 38.4 21.0 26.3 28.8 28.0 7.7 32.6±0.1
LLaV A-33B 38.5 25.0 26.2 28.2 29.2 7.7 32.9±0.3
LLaV A-33B (Data Mixing) 37.7 27.1 26.2 28.6 28.1 11.5 34.1±0.3
LLaV A-65B 39.2 28.2 26.2 28.3 33.0 15.0 35.5±0.3
LLaV A-65B (Data Mixing) 41.8 27.9 30.4 32.3 30.5 7.3 36.4±0.2
Table 2: Performance of various open-source LMM on MM-VET. N ote that MM-ReAct is not an
single multimodal model, it is a system built on chaining vis ual tools via GPT-3.5 or GPT-4, which
we append as a reference. Our experiment run on LLaV A-13B (LL aMA-2) yields very similar score
with the same checkpoint reported in MM-VET paper, indicati ng that our evaluation pipelines are
consistent.
3.1 Comparisons on Benchmarks
LLaV A-Bench. LLaV A-Bench (In-the-Wild)4[12] is a diverse evaluation dataset consisting of 24
images with 60 questions in total, including indoor and outd oor scenes, memes, paintings, sketches.
Each image is paired with a manually-curated, detailed desc ription and a set of properly-selected
questions related to open-ended visual chat scenarios. Eac h questions belongs to one of three types
of tasks: conversations that contain simple visual recogni tion & QA questions, detailed descriptions
that characterize the image with a long paragraph, and a comp lex reasoning task that focuses on de-
ducing implications from an image. Language GPT-4 ( gpt4-0314 ) is used to score to the generated
answers. The relative scores between the model output and go ld response are reported. We com-
pare LLaV A against the commercial visual chat systems inclu ding Microsoft BingChat5and Google
Bard6on LLaV A-Bench [ 12].
4https://github.com/haotian-liu/LLaVA/blob/main/docs /LLaVA_Bench.md
5https://www.bing.com/chat
6https://bard.google.com/
3

--- PAGE 4 ---
The results are presented in Table 1. The 33B and 65B checkpoints outperform the 13B LLaV A
model and Bing Chat. Despite the fact that LLaV A-Bench is sma ll (thus the comparison might not be
statistically signiﬁcant), the results are encouraging: c ompared to large LMM, small open-sourced
LMM are far more cost-effective to be deployed in real-world applications. With negligible increase
of inference latency, we can signiﬁcantly improve the perfo rmance for all model sizes by increasing
the beam search size from 1 to 5. Our results show that larger L LaV A models generally exhibit
better performance in tasks involving complex reasoning an d generating detailed descriptions, which
requires strong language competencies from larger LLM. In a ddition, larger LLaV A models obtain
comparable results to BingChat in multi-turn, multi-modal conversation tasks that require strong
image understanding capability.
MM-VET. MM-VET [ 19] is designed based on the assumption that the intriguing cap ability of
solving complicated tasks is often achieved by a generalist LMM which is able to integrate a varity of
vision-language (VL) capabilities. MM-Vet contains 200 im ages and 218 questions (samples), aim-
ing to evaluate6 core VL capabilities (recognition, OCR, kn owledge, language generation, spatial
awareness, and math) and their combinations. For evaluatio n, an LLM-based evaluator ( gpt4-0613 )
is used to score open-ended outputs of different forms. In Ta ble2, we report the results on MM-
VET. The performance is consistently improved from 13B to 33 B and 65B. The largest LLaV A
model improves SoTA performance among the end-to-end open- source LMM. The most signiﬁcant
improvements are observed when evaluating the capabilitie s of knowledge and generation, followed
by recognition and OCR. The performance on spatial and math r emains comparable. The result
reveals that the improved LLM capability is instrumental in storing more knowledge in the weights
and leading to a stronger language responding capability.
3.2 Scaling up LLaV A
The experiments are conducted to answer three research ques tions.
1/circlecopyrtWhich scaling factor matters? We study the relative contribution of three scaling-up fact ors
to the performance improvement of LLaV A. The results are sum marized in Table 3(a).
•Model size. Increasing the model size consistently improves the overal l performance. We
conjecture that larger data size is essential to train a larg er model. For example, if we only train
on LLaV A-80K data, we see smaller gain when model size become s larger.
•Image resolution. By ﬁxing the CLIP ViT image encoder, we compare the variants t hat are
pre-trained to take image resolution 224×224and336×336, and ﬁnd that the higher resolution
consistently yields 2-3 points improvement across all four LLM sizes.
•Data mixing. Larger models tend to have higher capability of ﬁtting the in struction data.
By mixing the language-only instruction data (ShareGPT) wi th LLaV A-80K, we can improve
model performance by 2 points, compared to training on multi modal instruction data only.
In Table 3(b), we present our result on MM-Bench [ 13], which contains a set of 2,974 questions,
which evaluate models’ reasoning skills of six categories. The combination of the three factors
improve the baseline LLaV A 7B model, reported in [ 13].
2/circlecopyrtWhen should the parameter-efﬁcient training method be cons idered? As model size in-
creases, it becomes necessary to consider using tuning meth ods that are more efﬁcient than full-
model ﬁne-tuning. LoRA and QLoRA are well-known parameter- efﬁcient tuning methods. As
shown in Table 4, we report compute cost using GPU hours per node , because the unit can be equiv-
alent to the price $13.63/hour (ND A100 v4 series) on Azure7. The total cost can be estimated by
multiplying the #hours and #epochs.
In Table 4(a), we train both the 33B and 65B model with LoRA rank 8 and 64 f or 1 epoch on the
LLaV A-80K instruction-tuning dataset. For models with 33B parameters and above, as we increase
the LoRA rank values, we notice an increase in both performan ce and cost until full-model tuning
reaches its maximum performance for a speciﬁc model size. In the case of the 13B model, we
ﬁnd that a rank of 64 can deliver comparable performance to fu ll-model tuning. The cost is more
related to the total number of parameters than the number of t rainable parameters. The cost increase
7https://azure.microsoft.com/en-us/pricing/details/m achine-learning/
4

--- PAGE 5 ---
Image Size Data Mixing 7B 13B 33B 65B
224×224 ✗ 63.6 67.1 69.3 70.3
336×336 ✗ 65.9 70.1 72.0 72.3
336×336 ✓ – – 73.9 74.2
(a) Performance scores on LLaV A-Bench.
Checkpoint Image Size Data Mixing Overall LR AR RR FP-S FP-C C P
LLaV A-7B 224 ×224 ✗ 36.2 15.9 53.6 28.6 41.8 20.0 40.4
LLaV A-33B 336 ×336 ✓ 55.7 23.3 74.0 46.0 51.5 50.4 67.2
LLaV A-65B 336 ×336 ✓ 56.0 24.4 72.3 49.3 50.5 51.2 68.1
(b) Performance scores on MM-Bench. The skills to evaluate i nclude logic reasoning (LR), attribute reason-
ing (AR), relation reasoning (RR), ﬁne-grained single-ins tance perception (FP-S), ﬁne-grained cross-instance
perception (FP-C), and coarse perception (CP).
Table 3: The performance to scale up model size, image resolu tion and data mixing.
7B 13B 33B 65B
LoRA Rank Full 64 Full 8 64-QLoRA 64 Full 64 Full
Performance ↑ 65.9 70.1 70.1 70.3 71.6 71.8 72.0 72.2 72.3
Time (GPU Hours per node) ↓ 1.3 2.1 2.3 4.62 4.68 4.79 5.80 9.17 13.50
# Trainable Parameters (B) ↓ 70.26 13 0.06 0.49 0.49 33 0.81 65
Table 4: The trade-off between performance and compute cost among different model sizes and
traing methods on LLaV A-80K data. “Full” indicates the full -model ﬁne-tuning. “Time” is reported
as the total GPU hours to ﬁnish 1 epoch training (running time ×#GPUs) divided by 8 (#GPUs
per node). All models are trained on LLaV A-80K data, results are obtained through averaging 3
repeated evaluation runs with same set up on LLaV A-Bench.
due to raising the LoRA rank for a given model size is signiﬁca ntly smaller than the cost increase
by enlarging model sizes. For example, increasing the LoRA r ank from 8 to 64 nearly matches
the performance as LoRA ﬁne-tuning a 65B model with same rank , but only requires 50% of 65B
model’s training cost. In practice we ﬁnd that tuning 33B mod el provide a good trade-off between
cost and performance.
Different LoRA variations have similar performance, and QL oRA requires lower GPU memory
cost and running-time cost than LoRA. When large models ( e.g., 65B) are trained with DeepSpeed
ZeRO2 mode, they can ﬁt into GPU with QLoRA, while yield the OO M issue with LoRA. In the
experiments, we ﬁnd that the hyperparameters of LoRA have a l arge impact of performance: (i)
Large learning rate and alpha value of LoRA improves the resu lts signiﬁcantly. For example, With
the same rank=64, we reduce the learning rate= 2×10−5and alpha=16, the performance decrease
from 71.8 to 65.5 on LLaV A-Bench. (ii)Under the same setting, large ranks leads to little improve-
ment. e.g., we increase the rank from 64 to 128 and 512, it improves from 65 .5 to 66.1 and 68.1,
respectively.
3/circlecopyrtA LMM with strong capabilities in both language and multimod al? We expand our eval-
uation in two aspects: (i)MM-VET is added to measure the integrated multimodal capabi lities of
LMM;(ii)The pure language ability of LMM is measured using Vicuna-80 [16] and MMLU [ 6],
where the former evaluates the instruct-following ability in real-world language tasks, the latter eval-
uates the multilingual multi-task language ability. The re sults are shown in Table 5, where all models
are full-model ﬁne-tuned.
Compared to Vicuna which initializes the LLM weights of LLaV A, it is surprising to observe that
LLaV A, after being trained solely on multimodal instructio n data, exhibits a comparable language
capability. Mixing language instruction data can boost LLa V A’s multimodal ability, but not the lan-
guage ability. This is partially attributed to the inclusio n of complex reasoning questions, and long-
form answers in LLaV A-Instruct-158K, which helps maintain the language capabilities of LLaV A.
5

--- PAGE 6 ---
Model Data MixMultimodal Language
LLaV A-Bench MM-VET Vicuna-80 MMLU
Vicuna-13B - - - 79.9 55.8
LLaV A-13B ✗ 70.1 32.5 79.6 55.0
Vicuna-33B - - - 85.6 59.0
LLaV A-33B ✗ 72.0 32.9 85.3 56.1
LLaV A-33B ✓ 73.9 34.1 80.3 58.6
Vicuna-65B - - - 83.2 62.5
LLaV A-65B ✗ 72.3 35.5 84.5 62.6
LLaV A-65B ✓ 74.2 36.4 82.6 62.2
LLaMA-2-70B-Chat - - - 84.7 63.1
LLaV A-70B ✓ 69.8 35.4 81.3 65.1
Table 5: Performance on both multimodal and language capabi lities.
We also train LLaV A-70B based on the LLaMA-2-70B-Chat check point [ 15], and ﬁnd that mixed
results on multimodal and language abilities. Interesting ly, we improve LLaMA-2-70B-Chat by 2.4
points on MMLU, yielding an overall MMLU score of 65.1, which is the best performance for the
70B model size, according to [ 17] and the Chatbot Arena Leaderboard8. To the best of our knowl-
edge, this is the ﬁrst reported result which shows visual ins tructing tuning improve language ability
of large-scale LMM.
4 Conclusions and Limitations
We present an empirical study of scaling the language model s ize for LMM. Our main ﬁndings are:
(i)Scaling LMM consistently enhances model performance, resu lting in signiﬁcant improvements
in language capabilities, primarily due to the increased LL M model size. We leave it to future work
how to scale the vision encoder to enhance the visual capabil ities and improve model performance on
vision recognition and understanding tasks. (ii)Parameter-efﬁcient methods such as LoRA/QLoRA
are viable solutions to ﬁnetune large-scale LLMs for a good p erformance-cost trade-off in some
real-world settings with limited GPU memory. We observe tha t LoRA/QLoRA’s performance are
comparable to that of ﬁne-tuning the full model, establishi ng their effectiveness through signiﬁcant
cost reduction in both model training and serving. (iii)Our study of training data curation reveals
that properly selecting image resolutions and mixing multi modal-language data for model training
can signiﬁcantly improve the performance of the resultant L MM. We also show for the ﬁrst time that
visual instruction tuning can improve LMM’s language capab ility. Note that the training datasets
used in this study is small. So, our ﬁndings are still prelimi nary. In future work, we will experiment
using much larger datasets to investigate in detail whether and how different methods of training
data selection and mixing can improve the quality of much lar ger LMM.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Anto ine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Rey nolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in Neural Information Processing Systems ,
35:23716–23736, 2022. 3
[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yus uf Hanafy, Wanrong Zhu, Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openﬂamingo: An open-
source framework for training large autoregressive vision -language models. arXiv preprint
arXiv:2308.01390 , 2023. 3
8https://huggingface.co/spaces/lmsys/chatbot-arena-l eaderboard
6

--- PAGE 7 ---
[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tio ng, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip : Towards general-purpose vision-
language models with instruction tuning, 2023. 3
[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Z ettlemoyer. Qlora: Efﬁcient ﬁne-
tuning of quantized llms. arXiv preprint arXiv:2305.14314 , 2023. 2
[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Gen g, Aojun Zhou, Wei Zhang, Pan
Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parame ter-efﬁcient visual instruction
model. arXiv preprint arXiv:2304.15010 , 2023. 3
[6] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Ma ntas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language un derstanding. arXiv preprint
arXiv:2009.03300 , 2020. 5
[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Z hu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2
[8] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingka ng Yang, and Ziwei Liu. Otter: A
multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023.
3
[9] Chunyuan Li. Large multimodal models: Notes on CVPR 2023 tutorial. arXiv preprint
arXiv:2306.14895 , 2023. 1
[10] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Lin jie Li, Lijuan Wang, and Jianfeng
Gao. Multimodal foundation models: From specialists to gen eral-purpose assistants. arXiv
preprint , 2023. 1
[11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. B lip-2: Bootstrapping language-
image pre-training with frozen image encoders and large lan guage models. arXiv preprint
arXiv:2301.12597 , 2023. 3
[12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
1,2,3
[13] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Z hang, Wangbo Zhao, Yike Yuan,
Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your mul ti-modal model an all-around
player? arXiv preprint arXiv:2307.06281 , 2023. 4
[14] OpenAI. Gpt-4 technical report, 2023. 1
[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shrut i Bhosale, et al. Llama 2: Open
foundation and ﬁne-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 2,6
[16] Vicuna. Vicuna: An open-source chatbot impressing gpt -4 with 90%* chatgpt quality.
https://vicuna.lmsys.org/ , 2023. 2,5
[17] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hess el, Tushar Khot, Khyathi Raghavi
Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Bel tagy, et al. How far
can camels go? exploring the state of instruction tuning on o pen resources. arXiv preprint
arXiv:2306.04751 , 2023. 6
[18] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Eh san Azarnasab, Faisal Ahmed,
Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react : Prompting chatgpt for
multimodal reasoning and action, 2023. 3
[19] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Ke vin Lin, Zicheng Liu, Xinchao
Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabil-
ities. arXiv preprint arXiv:2308.02490 , 2023. 1,3,4
[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohame d Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced larg e language models. arXiv preprint
arXiv:2304.10592 , 2023. 1,3
7

--- PAGE 8 ---
This figure "lora_loss.png" is available in "png"
 format from:
http://arxiv.org/ps/2309.09958v1
