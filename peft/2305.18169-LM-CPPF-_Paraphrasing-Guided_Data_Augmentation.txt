# 2305.18169.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.18169.pdf
# File size: 437732 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LM-CPPF: Paraphrasing-Guided Data Augmentation
for Contrastive Prompt-Based Few-Shot Fine-Tuning
Amirhossein Abaskohi1, Sascha Rothe2, Yadollah Yaghoobzadeh1,3
1School of Electrical and Computer Engineering
College of Engineering, University of Tehran, Tehran, Iran
2Google DeepMind, Zürich, Switzerland
3Tehran Institute for Advanced Studies, Khatam University, Iran
amir.abaskohi@ut.ac.ir, rothe@google.com, y.yaghoobzadeh@ut.ac.ir
Abstract
In recent years, there has been significant
progress in developing pre-trained language
models for NLP. However, these models of-
ten struggle when fine-tuned on small datasets.
To address this issue, researchers have pro-
posed various adaptation approaches. Prompt-
based tuning is arguably the most common
way, especially for larger models. Previous
research shows that adding contrastive learning
to prompt-based fine-tuning is effective as it
helps the model generate embeddings that are
more distinguishable between classes, and it
can also be more sample-efficient as the model
learns from positive and negative examples si-
multaneously. One of the most important com-
ponents of contrastive learning is data augmen-
tation, but unlike computer vision, effective
data augmentation for NLP is still challenging.
This paper proposes LM-CPPF, Contrastive
Paraphrasing-guided Prompt-based Fine-tuning
of Language Models, which leverages prompt-
based few-shot paraphrasing using generative
language models, especially large language
models such as GPT-3 and OPT-175B, for data
augmentation. Our experiments on multiple
text classification benchmarks show that this
augmentation method outperforms other meth-
ods, such as easy data augmentation, back trans-
lation, and multiple templates.1
1 Introduction
Pre-trained language models (PLMs) are trained
on large-scaled corpora in a self-supervised fash-
ion. They have fundamentally changed the NLP
community in the past few years by achieving im-
pressive results in various Tasks (Devlin et al.,
2018; Radford et al., 2018; Yang et al., 2019; Chi-
ang et al., 2022). However, when PLMs are fine-
tuned on small datasets, their performance declines.
Researchers have proposed various techniques to
adapt PLMs to these scenarios (Snell et al., 2017;
1Our implementation is publicly available at: https://
github.com/AmirAbaskohi/LM-CPPFSung et al., 2018). In addition to performance,
fine-tuning PLMs to learn a new task is parame-
ter inefficient, because an entirely new model is
required for every task (Houlsby et al., 2019).
By the introduction of GPT-3 (Brown et al.,
2020b) with 175B parameters, it has been shown
that Large Language Models (LLMs) are efficient
few-shot learners as they can use their knowledge
more effectively. One of the key features of these
LLMs is their ability to perform multiple tasks us-
ing prompts. A language prompt is a piece of text
that is added to the input query to help the model
make more accurate predictions. In addition, LLMs
can be fine-tuned for specific tasks using few exam-
ples. This has made them powerful tools for NLP
tasks, especially in few-shot scenarios. However,
that might not be practical for many situations be-
cause of the model size. Therefore, there is a need
to adapt smaller PLMs to work in a similar way to
LLMs.
Prompt-based fine-tuning is a method for adapt-
ing PLMs to specific tasks or domains by providing
a prompt (Schick and Schütze, 2020a,b). This ap-
proach has been shown to be effective in various
NLP tasks, including text classification (Han et al.,
2021; Wang et al., 2022) and question answering
(Yao et al., 2022). However, it can be challeng-
ing to achieve strong performance when only a
few examples are available for each task. Gao
et al. (2020) introduced a prompt-based fine-tuning
method called LM-BFF for RoBERTa (Liu et al.,
2019) to tackle this issue. Their approach includes
automated prompt generation and a more effective
way of using task examples in fine-tuning.
Building on the success of LM-BFF and consid-
ering contrastive learning’s promising results both
in computer vision (Chen et al., 2020) and NLP
(Chen et al., 2020; Miao et al., 2021), Jian et al.
(2022) present a contrastive learning framework to
improve LM-BFF. They propose a Supervised Con-
trastive Learning (SCL) approach (Khosla et al.,arXiv:2305.18169v3  [cs.CL]  5 Jul 2023

--- PAGE 2 ---
2020) that classifies inputs using different aug-
mented views of the data. These views are created
using different templates for their demonstrations
when building prompts.
In this paper, we show that while SCL at the
feature space can be beneficial, the use of differ-
ent templates can limit the full potential of this
approach. We propose LM-CPPF (Contrastive
Paraphrasing-guided Prompt-based Fine-tuning of
Language Models), in which we integrate the
knowledge of LLMs like GPT-3 and OPT-175B
(Zhang et al., 2022) to build different views using
paraphrasing. These models can generate para-
phrases of a sentence with different syntax, not just
by changing the lexicalization. Previous studies
have considered generating paraphrases a challeng-
ing and costly NLP task (Siddique et al., 2020;
Garg et al., 2021; Zhou and Bhat, 2021). How-
ever, PLMs can generate paraphrases easily and
effectively using in-context learning with few ex-
amples. Although prior research has studied para-
phrase generation with PLMs (Roy and Grangier,
2019; Hegde and Patil, 2020), to the best of our
knowledge, this is the first time that large LLMs are
utilized to generate paraphrases with prompts as an
augmentation method. Our experiments on six dif-
ferent text classification tasks demonstrate that LM-
CPPF outperforms the previous SOTA methods
of data augmentation in prompt-based fine-tuning,
including Easy Data Augmentation (EDA) (Wei
and Zou, 2019), Back Translation (BT) (Sugiyama
and Yoshinaga, 2019), and multiple templates (Jian
et al., 2022).
2 Related Works
LLMs like GPT-3 (Brown et al., 2020a) can per-
form NLP tasks with few examples and natural
prompts. But smaller models are not efficient
with this approach and there are data sparsity and
prompt sensitivity issues. To address these chal-
lenges, Gao et al. (2021) propose LM-BFF, a frame-
work that leverages a large PLM to automatically
generate task-specific prompts for smaller models.
It improves their few-shot performance on different
NLP tasks. Some work have enhanced LM-BFF
with different prompt tuning methods. For example,
Zhou et al. (2022) present a dual context-guided
continuous prompt tuning method that uses the
language context and connects discrete and con-
tinuous prompt tuning. Jian et al. (2022) integrate
contrastive learning and data augmentation withLM-BFF. In their contrastive part, in addition to
comparing different instances from the same or
different classes, they introduced a novel prompt-
specific augmentation method. In their approach,
they change the template of the prompt. In this pa-
per, we use few-shot paraphrasing with LLMs for
contrastive prompt-tuning, which fine-tunes mod-
els with natural prompts.
Paraphrasing is the task of expressing the same
meaning with different words or structures. It can
be used to create training data with increased di-
versity and naturalness for NLP tasks, such as text
classification (Xie et al., 2020), natural language
inference (Kumar et al., 2019), and text summariza-
tion (Loem et al., 2022), surpassing the limitations
of traditional approaches. Paraphrasing helps with
data scarcity and model generalization. There are
different ways to generate paraphrases for data aug-
mentation. One is back-translation (Sennrich et al.,
2016), which uses a translation system to convert a
sentence to another language and back. Another is
to use paraphrasing models trained on parallel para-
phrase datasets (Wieting and Gimpel, 2018; Zhu
et al., 2022). PLMs can also generate paraphrases
by using large-scale corpora, but they may produce
paraphrases that are not semantically consistent or
relevant. LLMs can reduce this problem as they
encode and generate language better. In this paper,
we generate paraphrases by carefully prompting
LLMs and then use them for data augmentation.
3 Method
Background Contrastive learning’s success re-
lies on data augmentation, which creates new views
of the input data. Contrastive learning has been uti-
lized for various tasks in deep learning (Le-Khac
et al., 2020; Conde and Turgutlu, 2021; Abaskohi
et al., 2022); however, most NLP data augmenta-
tion methods may influence semantics which re-
sults in limited improvement. For instance, EDA’s
synonym substitution may create entirely new sam-
ples since words do not have equal senses (Keselj,
2009). In addition to these augmentation methods,
the approach used in Jian et al. (2022) cannot be
counted as data augmentation as the sample is still
the same and only the template for the verbalizer
changes. Although it is a creative approach de-
signed specifically for the prompt-based method
of LM-BFF, it is limited in performance even com-
pared to EDA in several benchmarks. Furthermore,
it requires an expert to create multiple templates

--- PAGE 3 ---
Figure 1: Our method, LM-CPPF, includes two objectives: (I) MLM and (II) Supervised Contrastive Learning. The
target sentence is the first sentence in each prompt with a [MASK] token. The target sentence of Sent_0 is used to
train our model and calculate the MLM loss. We build Sent_3, whose target sentence is a paraphrase of Sent_0’s
target sentence. Sent_1 and Sent_2, sampled from the dataset, have target sentences in the same and different classes
as Sent_0, respectively.
for each task, which makes it challenging for newly
emerged tasks. Here we propose leveraging LLMs
to generate paraphrases and introduce LM-CPPF,
a novel approach aimed at addressing the chal-
lenges associated with contrastive prompt-based
fine-tuning of PLMs.
Few-shot paraphrasing Paraphrasing is one of
the best methods for data augmentation in NLP.
One of the most popular approaches for paraphras-
ing is back-translation (BT) (Sugiyama and Yoshi-
naga, 2019) due to its simplicity and efficiency.
Nonetheless, BT’s performance depends a lot on
the intermediary language. In this paper, we, in-
stead, use a combination of prompt-learning and
LLMs for paraphrasing. In few-shot paraphrasing,
an LLM rewrites a sentence given an instruction
and a few examples. We believe that LLMs gener-
ate high-quality paraphrases due to their encoded
semantic and sentence structure knowledge. We
utilize GPT-3 (Brown et al., 2020b) or OPT-175B
(Zhang et al., 2022) via their official APIs2for
generating paraphrases.
To avoid violating the prompt-based fine-tuning
settings, we do not include any additional task data
in generating our paraphrases. Following the few-
shot setting in LM-BFF, we assume to have access
to a PLM M, datasets Dtrain, andDtestwith label
spaceYwhere there are only K= 16 examples per
class in Dtrain. We use this setting for both prompt-
based few-shot paraphrasing and fine-tuning. To
2OPT-175B: opt.alpa.ai and GPT-3: openai.com/apigenerate paraphrases, excluding the one sample
that we want to paraphrase, we use QuillBot3to
create paraphrases for our prompts for the remain-
ing 15 samples in the same class of Dtrain. We
leverage two types of prompts for paraphrasing: (I)
Only Demonstration: Here, the samples and their
paraphrased versions are given using the templates
in Table C.3 to demonstrate the task of paraphras-
ing. (II) Demonstrations with Instruction: In
addition to the previous method, this one includes
instructions at the beginning of the prompt, defin-
ing paraphrasing before demonstrations. These
instructions can be seen in Table C.4.
Contrastive prompt-based fine-tuning LM-
CPPF consists of two steps. The first step in-
volves calculating the Masked Language Modeling
(MLM) loss by using the target sentence in the
given template, the specific demonstrations in the
prompt, and the verbalizer matched with the target
sentence’s label. We calculate the supervised con-
trastive loss in the second step by comparing the
target prompt with another sample with the same
template but different random demonstrations. This
comparison sample can be in the same or a different
class as the target prompt. When the comparison
sample belongs to a different class, it is randomly
sampled from the dataset. However, in cases where
the comparison sample belongs to the same class,
an alternative approach is employed. This involves
either selecting another sample from the same class
3quillbot.com

--- PAGE 4 ---
Task LM-BFFLM-BFF+ LM-BFF+ LM-CPPF LM-CPPF LM-CPPF LM-CPPF
SupConLoss Multi-templates GPT-3 OPT GPT-2 FT GPT-2
SST-2 89.5 90.3 91.0 92.3 91.8 91.1 91.4
SST-5 48.5 49.6 50.3 52.8 52.2 51.4 51.6
MNLI 62.3 63.2 64.8 68.4 66.2 65.6 65.8
CoLA 6.9 9.6 11.6 14.1 13.3 10.7 11.8
QNLI 61.2 65.4 67.2 69.2 68.5 67.5 67.8
CR 89.7 89.9 90.2 91.4 91.1 90.2 90.7
Table 1: Performance of LM-CPPF and our baselines in six datasets. LM-BFF+Multi-templates refers to Jian
et al. (2022). LM-BFF+SupConLoss uses the same architecture of LM-BFF+Multi-templates, but without any
data augmentation, just integrating supervised contrastive and MLM loss functions. Two cases are available for
GPT-2: the pre-trained model and the GPT-2 fine-tuned (FT) on ParaNMT-50M (Wieting and Gimpel, 2018) dataset.
LM-BFF, LM-BFF+Multi-template, and LM-CPPF (on average for all models used for paraphrasing) have 0.77 and
1.02, and 1.65 standard deviations on average for each task, respectively.
within the dataset or applying data augmentation
techniques, paraphrasing in our case, to augment
the target sample in order to create a new view of it.
In both of these cases, the demonstrations are not
the same. Figure 1 illustrates the fine-tuning pro-
cess, and Algorithm D.1 shows our methodology
when paraphrasing creates a new view of the target
sample. See Appendix D for more information.
4 Experiments
Evaluation datasets and protocol Our method
is evaluated on six different classification tasks
from LM-BFF (Liu et al., 2021). The reported num-
bers represent the average accuracy from five runs
using Roberta-base (Liu et al., 2019). In Section
4.1 where LLMs are compared for paraphrasing,
we also employed pre-trained and fine-tuned GPT-
2 as an additional model for paraphrasing, allowing
us to leverage smaller models in our experiments.
For the fine-tuning of GPT-2 specifically for para-
phrasing, we utilized the ParaNMT-50M (Wieting
and Gimpel, 2018) dataset. More details regarding
the training process can be found in Appendix A.
4.1 Paraphrasing in Prompt Fine-tuning
This section presents the results of our fine-tuning
approach using paraphrasing on various NLP tasks.
As shown in Table 1, LM-CPPF improves the
model’s accuracy on all tasks compared to the
baseline method of LM-BFF+Multi-templates (Jian
et al., 2022). Comparing the standard deviation of
our model in five runs and the standard deviations
of LM-BFF and LM-BFF + Multi-templates, we
see that LM-CPPF has a higher standard devia-
tion as it uses an intermediary model for generat-
ing paraphrases. In contrast, LM-BFF + Multi-templates integrates templates that have nearly
equal performance (Jian et al., 2022).
We also compare the effect of using GPT-3, OPT-
175B, and GPT-2 as our language model for few-
shot paraphrasing. We did two experiments with
GPT-2 large: (I) Using a pre-trained version of
GPT-2 where the weights are not tuned at all (II)
Fine-tuned GPT-2 where the model has been fine-
tuned on the ParaNMT-50M dataset. The results in
Table 1 indicate that GPT-3 outperforms OPT-175B
in all tasks and GPT-2 has a lower performance,
which was predictable since it has significantly
fewer parameters. Also, fine-tuned GPT-2 shows
a better performance which suggests that GPT-2’s
knowledge after pre-training is not enough for do-
ing a task like paraphrasing. About the LLMs,
although both models have 175B parameters, OPT-
175B has a 1/7carbon footprint of GPT-3, and it
is also freely available (Zhang et al., 2022). Conse-
quently, we base our further analysis on OPT-175B.
4.2 Few-shot Paraphrasing vs. Other Data
Augmentation Methods
In this section, we present an experimental compar-
ison of the performance of the few-shot paraphras-
ing approach and other data augmentation meth-
ods, including BT and EDA. The results are shown
in Table 2. The BT approach is evaluated using
different intermediary languages (Arabic, French,
Deutsch, Chinese, and Hindi). The results indicate
that BT’s performance is slightly different across
languages, with Chinese showing the highest per-
formance. In general, paraphrasing approaches,
including BT, are better in comparison to EDA.
In SST-2 and CR, where the samples are usually
simple sentences, BT shows weaker performance

--- PAGE 5 ---
TaskFew-shot Back TraslationSR RI RS RD EDAParaphrasing AR FR DE ZH HI
SST-2 91.8 90.8 90.6 90.4 90.7 90.3 90.5 89.5 90.8 91.3 90.4
SST-5 52.2 49.2 49.3 49.1 49.6 48.3 47.9 49.3 49.3 48.2 48.2
MNLI 66.2 64.3 63.1 63.8 65.4 62.2 62.9 63.2 61.7 60.2 60.3
CoLA 13.3 6.7 6.8 6.4 7.1 5.9 6.3 5.8 5.8 5.1 5.1
QNLI 68.5 66.5 66.2 65.8 66.6 64.3 66.1 65.9 66.3 65.6 63.3
CR 91.1 88.5 88.6 88.4 88.7 87.9 89.8 89.1 89.3 89.6 89.7
Table 2: Comparing the accuracy of our few-shot paraphrasing approach with the Back Translation (BT) and Easy
Data Augmentation (EDA) methods. EDA includes Synonym Replacement (SR), Random Insertion (RI), Random
Swap (RS), and Random Deletion (RD). EDA in the results is combination of all of the four mentioned methods. BT
and EDA standard deviations are 1.31 and 1.4 on average, respectively, while our approach has a standard deviation
of 1.65.
TaskTemplate Number
1 2 3 4 5 6
SST-2 91.8 91.2 91.4 89.1 92.1 92.4
SST-5 52.2 53.1 52.7 53.4 53.6 54.1
MNLI 66.2 65.9 66.9 66.1 66.2 66.4
CoLA 13.3 12.7 13.2 13.8 13.4 13.6
QNLI 68.5 68.4 68.6 68.5 68.8 69.3
CR 91.1 91.2 91.3 91.5 91.7 92.2
Table 3: Performance of different paraphrasing prompt
demonstration templates.
than EDA. We believe the reason is that BT can
be more effective for longer sequences because
longer sequences usually contain more context and
nuanced meaning. Moreover, EDA employs ad-
ditional knowledge from another PLM in certain
actions, such as synonym substitution, similar to
BT and few-shot paraphrasing.
The few-shot paraphrasing approach introduced
in this work outperforms both BT and EDA. This
confirms that using PLM’s knowledge properly in
paraphrasing is an effective and efficient data aug-
mentation method. In few-shot paraphrasing, we
instruct the model to generate paraphrases that dif-
fer in lexicalization and sentence structure.
4.3 Prompt Template Evaluation
As the heart of our method is the few-shot para-
phrase generation done by LLMs, we investigate
the impact of different paraphrasing prompt demon-
strations and instruction templates on the perfor-
mance of our model. Table 3 shows that the last
template presented in Table C.3 is better in al-
most all tasks. This template, “<Original Text>, in
other words <Paraphrased>”, uses a complete and
concrete sentence, unlike other templates, which
use specific tokens, such as “[Original]”, to dis-Task w/o InstructTemplate Number
1 2 3 4 5
SST-2 92.4 93.1 93 92.8 93.2 92.7
SST-5 54.1 54.7 54.5 54.2 54.9 54.3
MNLI 66.9 67.8 67.5 67.1 68.2 67.2
CoLA 13.6 13.1 13.2 12.6 13.3 12.8
QNLI 69.3 69.8 70.1 69.5 70.2 69.6
CR 92.2 93.1 92.8 92.6 93.3 92.4
Table 4: Performance of different paraphrasing prompt
instruction templates on various NLP tasks.
tinguish between the original and the paraphrased
version. Also, we compare different instruction
templates presented in Table C.4. As we aimed to
report our best result in each task here, we used the
best demonstration template for any particular task,
which was determined in Table 3. Table 4 shows
that the fourth template achieves the best perfor-
mance, as it precisely describes the task with its
instruction “Generate a paraphrase of the following
text using different words and sentence structures
while still conveying the same meaning”.
5 Conclusion
Our experiments demonstrated the effectiveness of
using few-shot paraphrasing as a data augmentation
method for contrastive prompt-based fine-tuning
of PLMs. It outperformed other data augmenta-
tion methods in text classification tasks, such as
EDA, multiple templates, and back translation. We
also found that our approach is effective with GPT-
3 or OPT-175b models in generating paraphrases.
Overall, LM-CPPF improves the performance of
LM-BFF by large margins using contrastive learn-
ing applied on paraphrases generated by LLMs.

--- PAGE 6 ---
Limitations
Our approach relies on the performance of the few-
shot paraphrasing. This results in two limitations
for our approach. One limitation is the difficulty
in accessing GPT-3 and OPT-175b models. These
models currently need to be more widely avail-
able. OPT-175B has a free version but it is very
slow. Another limitation is the need for annotated
demonstrations for few-shot paraphrasing. While
there are available models and tools, like QuillBot,
that can be used for this purpose, their quality is not
comparable to GPT-3 and OPT-175b. This can limit
the power of these tools in our approach. Using
human knowledge to paraphrase the demonstration
can help these large models generate high-quality
paraphrases but it is expensive.
Ethics Statement
The research conducted in this paper has been car-
ried out in accordance with the ethical principles
of ACL. We have ensured that our experiments do
not harm any individuals or groups and have ob-
tained informed consent from all participants. As
mentioned in the paper, we also tried to base our
main experimentation on the more environmentally-
friendly option, OPT-175B.
References
Amirhossein Abaskohi, Fatemeh Mortazavi, and Hadi
Moradi. 2022. Automatic speech recognition for
speech assessment of persian preschool children.
arXiv preprint arXiv:2203.12886 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020a. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020b. Language models are few-shot learners.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey Hinton. 2020. A simple framework forcontrastive learning of visual representations. In In-
ternational conference on machine learning , pages
1597–1607. PMLR.
Cheng-Han Chiang, Yung-Sung Chuang, and Hung-yi
Lee. 2022. Recent advances in pre-trained language
models: Why do they work and how do they work. In
Proceedings of the 2nd Conference of the Asia-Pacific
Chapter of the Association for Computational Lin-
guistics and the 12th International Joint Conference
on Natural Language Processing: Tutorial Abstracts ,
pages 8–15, Taipei. Association for Computational
Linguistics.
Marcos V Conde and Kerem Turgutlu. 2021. Clip-art:
Contrastive pre-training for fine-grained art classifi-
cation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
3956–3960.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723 .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830.
Sonal Garg, Sumanth Prabhu, Hemant Misra, and
G Srinivasaraghavan. 2021. Unsupervised con-
textual paraphrase generation using lexical con-
trol and reinforcement learning. arXiv preprint
arXiv:2103.12777 .
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and
Maosong Sun. 2021. Ptr: Prompt tuning with rules
for text classification.
Chaitra Hegde and Shrikumar Patil. 2020. Unsuper-
vised paraphrase generation using pre-trained lan-
guage models. arXiv preprint arXiv:2006.05477 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Yiren Jian, Chongyang Gao, and Soroush V osoughi.
2022. Contrastive learning for prompt-based few-
shot language learners. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 5577–5587, Seattle,
United States. Association for Computational Lin-
guistics.

--- PAGE 7 ---
Vlado Keselj. 2009. Speech and language processing
daniel jurafsky and james h. martin (stanford univer-
sity and university of colorado at boulder) pearson
prentice hall, 2009, xxxi+ 988 pp; hardbound, isbn
978-0-13-187321-6.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-
pervised contrastive learning. Advances in Neural
Information Processing Systems , 33:18661–18673.
Alex Krizhevsky. 2014. One weird trick for paralleliz-
ing convolutional neural networks. arXiv preprint
arXiv:1404.5997 .
Ashutosh Kumar, Satwik Bhattamishra, Manik Bhan-
dari, and Partha Talukdar. 2019. Submodular
optimization-based diverse paraphrasing and its ef-
fectiveness in data augmentation. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 3609–3619, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Phuc H Le-Khac, Graham Healy, and Alan F Smeaton.
2020. Contrastive representation learning: A frame-
work and review. Ieee Access , 8:193907–193934.
Shikun Liu, Shuaifeng Zhi, Edward Johns, and An-
drew J Davison. 2021. Bootstrapping semantic seg-
mentation with regional contrast. arXiv preprint
arXiv:2104.04465 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Mengsay Loem, Sho Takase, Masahiro Kaneko, and
Naoaki Okazaki. 2022. ExtraPhrase: Efficient data
augmentation for abstractive summarization. In Pro-
ceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies: Student
Research Workshop , pages 16–24, Hybrid: Seattle,
Washington + Online. Association for Computational
Linguistics.
Deshui Miao, Jiaqi Zhang, Wenbo Xie, Jian Song, Xin
Li, Lijuan Jia, and Ning Guo. 2021. Simple con-
trastive representation adversarial learning for nlp
tasks. arXiv preprint arXiv:2111.13301 .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Aurko Roy and David Grangier. 2019. Unsupervised
paraphrasing without translation. arXiv preprint
arXiv:1905.12752 .Timo Schick and Hinrich Schütze. 2020a. Exploit-
ing cloze questions for few shot text classification
and natural language inference. arXiv preprint
arXiv:2001.07676 .
Timo Schick and Hinrich Schütze. 2020b. It’s not just
size that matters: Small language models are also
few-shot learners. arXiv preprint arXiv:2009.07118 .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In Proceedings of the 54th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 86–96,
Berlin, Germany. Association for Computational Lin-
guistics.
AB Siddique, Samet Oymak, and Vagelis Hristidis.
2020. Unsupervised paraphrasing via deep reinforce-
ment learning. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge
Discovery & Data Mining , pages 1800–1809.
Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. Ad-
vances in neural information processing systems , 30.
Amane Sugiyama and Naoki Yoshinaga. 2019. Data
augmentation using back-translation for context-
aware neural machine translation. In Proceedings
of the Fourth Workshop on Discourse in Machine
Translation (DiscoMT 2019) , pages 35–44.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang,
Philip HS Torr, and Timothy M Hospedales. 2018.
Learning to compare: Relation network for few-shot
learning. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages
1199–1208.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.
Contrastive multiview coding. In European confer-
ence on computer vision , pages 776–794. Springer.
Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan,
Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang,
and Ming Gao. 2022. Towards unified prompt tuning
for few-shot text classification.
Jason Wei and Kai Zou. 2019. Eda: Easy data augmenta-
tion techniques for boosting performance on text clas-
sification tasks. arXiv preprint arXiv:1901.11196 .
John Wieting and Kevin Gimpel. 2018. ParaNMT-50M:
Pushing the limits of paraphrastic sentence embed-
dings with millions of machine translations. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 451–462, Melbourne, Australia. As-
sociation for Computational Linguistics.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-
ong, and Quoc V . Le. 2020. Unsupervised data aug-
mentation for consistency training.

--- PAGE 8 ---
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. Advances in neural informa-
tion processing systems , 32.
Yuan Yao, Bowen Dong, Ao Zhang, Zhengyan Zhang,
Ruobing Xie, Zhiyuan Liu, Leyu Lin, Maosong Sun,
and Jianyong Wang. 2022. Prompt tuning for discrim-
inative pre-trained language models. arXiv preprint
arXiv:2205.11166 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Jianing Zhou and Suma Bhat. 2021. Paraphrase genera-
tion: A survey of the state of the art. In Proceedings
of the 2021 Conference on Empirical Methods in
Natural Language Processing , pages 5075–5086.
Jie Zhou, Le Tian, Houjin Yu, Zhou Xiao, Hui Su, and
Jie Zhou. 2022. Dual context-guided continuous
prompt tuning for few-shot learning. In Findings of
the Association for Computational Linguistics: ACL
2022 , pages 79–84, Dublin, Ireland. Association for
Computational Linguistics.
Hongyu Zhu, Yan Chen, Jing Yan, Jing Liu, Yu Hong,
Ying Chen, Hua Wu, and Haifeng Wang. 2022.
DuQM: A Chinese dataset of linguistically perturbed
natural questions for evaluating the robustness of
question matching models. In Proceedings of the
2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 7782–7794, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
A Evaluation Setting
We used a learning rate of 1e−5for MLM loss like
LM-BFF. Although contrastive learning algorithms
often perform better with larger batch training, due
to resource limitations, we had to use half the batch
size suggested in Jian et al. (2022) for various tasks
in the SCL phase. As recommended in Krizhevsky
(2014), we used sqrt(0.5)≈0.7of the learning
rates mentioned in Jian et al. (2022) for this phase.
Therefore, we report baselines with our smaller
batch size. Our method uses a single template
for each task’s prediction. The primary prompts
are listed in Appendix B. For the prompts used
in the paraphrasing phase, with the exception of
experiments in Section 4.3, we used randomly se-
lected templates from the suggested prompts listed
in Table C.3. In all of the experiments, we used
OPT-175B, except one of the results mentioned in
Section 4.1, where we compared OPT-175B and
GPT-3 in paraphrasing.We show the batch size and learning rate for
SupCon in Table A.1. It is important to note that
the results of LM-BFF presented in the main paper
were obtained using the same large batch size as
our method to ensure fair comparisons.
We fine-tuned with a batch size that fits into GPU
memory and is divisible by the total number of
examples in the task. Experiments were conducted
on one NVIDIA RTX-3090 with 24 GB memory
using the RoBERTa-base model. Furthermore, as
per LM-BFF, we fine-tuned for a maximum of 1000
steps.
Task Batch Size Learning Rate
SST-2 8 7e−7
SST-5 20 7e−6
MNLI 12 7e−6
CoLA 8 7e−6
QNLI 8 7e−6
CR 16 7e−6
Table A.1: Batch size and learning rate for SupCon loss
used for each task.
For the GPT-2 experiments in Table 1, we fol-
lowed the same intructions for generating para-
phrases as we used for GPT-3 and OPT-175. In
fine-tuning GPT-2, we fine-tuned our model on
ParaNMT-50M (Wieting and Gimpel, 2018) with
the batch size of 32 and learning rate of 1e−3for 5
epochs.
B Task Prompts
The primary prompts utilized for each task in our
experiments are displayed in Table B.2. They were
handpicked by LM-BFF (Gao et al., 2021).
C Paraphrasing Prompts
To find the best prompt for paraphrasing, we
checked different corpus available online and found
out how the paraphrasing examples are introduced.
We generated our prompts by using this informa-
tion and our manual modification in these tem-
plates.
In this demonstration prompt, we did not provide
any explanations or descriptions for the specific
transformation applied to the input to produce the
output. Instead, we labeled the original sample and
its paraphrase. For instance, we used the token
[Original] to indicate the original sentence in the
dataset and the token [Paraphrase] to indicate the

--- PAGE 9 ---
Task Template Verbalizers
SST-2 <S1>It was [MASK] . positive: great, negative: terrible
SST-5 <S1>It was [MASK] . v.positive: great, positive: good, neutral: okay, negative: bad, v.negative: terrible
MNLI <S1>? [MASK] , <S2> entailment: Yes, netural: Maybe, contradiction: No
CoLA <S1>This is [MASK] . grammatical: correct, not_grammatical: incorrect
QNLI <S1>? [MASK] , <S2> entailment: Yes, not_entailment: No
CR <S1>It was [MASK] . positive: great, negative: terrible
Table B.2: Primary templates and verbalizers (label words) used in our experiments.
paraphrased sample. Table C.3 shows the templates
we used for this approach.
Demonstration Template
Original:<Original Text>
Paraphrase:<Paraphrased Text>
[Original]:<Original Text>
[Paraphrase]:<Paraphrased Text>
Original:<Original Text>
Rewrite:<Paraphrased Text>
[Original]:<Original Text>
[Rewrite]:<Paraphrased Text>
Here is the original source: <Original Text>
Here is the paraphrase: <Paraphrased Text>
<Original Text>, in other words <Paraphrased
Text>
Table C.3: The templates that were used to give ex-
amples of how the paraphrasing should be done to the
pre-trained language model.
In instruction for prompts, we provided exam-
ples and simple instructions to the language models.
The instructions were used to ask the model to gen-
erate paraphrases before presenting them with ex-
amples. Table C.4 shows the instructions we used
to explain the task to the model at the beginning of
our prompts.
D Contrastive Prompt-based Fine-tuning
Details
Contrastive prompt-based fine-tuning contains two
main steps: (1) Masked Language Modeling and
(2) Contrastive Learning.
Masked Language Modeling (MLM) Loss. A
classification task is approached as a Masked Lan-
guage Modeling(MLM) problem in prompt-based
methods. The input consists of a sentence (sent)
and a template with a mask (temp) (i.e., xprompt =
sent, temp ([MASK ])), and the goal is to deter-
mine the best token to fill in the [MASK ]. This
results in a MLM loss, represented as LMLM =
MLM (xprompt , y), where yis the word label as-Instructions
Summarize the following text in your own words
Rewrite the following text that expresses the same
idea in a different way
Generate a paraphrase of the following text that
expresses the same ideas in a different way
Generate a paraphrase of the following text using
different words and sentence structures while still
conveying the same meaning
Generate a summary or paraphrase of the
following text that captures the essence of the
ideas in a concise manner
Table C.4: The instructions that were used before giving
examples to the language model to describe the para-
phrasing task.
sociated with xprompt . LM-BFF (Gao et al., 2021)
uses demonstrations of label words to improve the
results. The input for this approach includes the
sentence ( sent 0) and the masked template ( temp 0)
with a mask ([MASK]. The input also contains an
additional sentence ( sent i) with the same template
(temp 0) with its own verbalizer ( word i) for those
sentences. The label words are sampled from the
training set. The classification loss is then calcu-
lated using this input.
The language model first encodes the input
sentence xininto a sequence of tokens, which
are then mapped to a sequence of hidden states
h1, h2, ..., h L.Ldenotes the length of the se-
quence, and the dimension of the hidden states
is denoted by d. For example, in prompt-
based fine-tuning, if the input sentence ( xin) is
“France missed the world cup in penalties,” the
corresponding prompt xprompt would be [CLS]
xin,[MASK ].[SEP]. The model then determines
whether it is more likely to place the appropriate
verbalizer at the [MASK] position. It has been
found that fine-tuning with this fill-in-the-blank
framework is superior to standard fine-tuning. The
prediction of the model Mfor a class y∈ Y can
be expressed by mapping the label space Y to the

--- PAGE 10 ---
Algorithm D.1 Learning from MLM and SupCon
with Paraphrasing
1:Input:
2:Training set: Dtrain
3:MLM model: M
4:Function to concatenate two strings: Concat
5:Cross Entropy loss: CE
6:Supervised Contrastive loss: SupCon
7:Paraphrase function: Paraphrase
8:Function that samples from a dataset and puts
it in the specific template: Sample
9:// The third parameter of this function specifies
10:// whether to pus [MASK]or the verbalizer of
11:// the label
12:Template For Prompts: Template
13:MaxStep = 1000
14:Preparing Samples:
15:fori < MaxStep do
16: sent, y =Sample( Dtrain,Template , false)
17: demo 1=Sample( Dtrain,Template , true)
18: demo 2=Sample( Dtrain,Template , true)
19: demo 3=Sample( Dtrain,Template , true)
20: demo 4=Sample( Dtrain,Template , true)
21: demo in1=Concat( demo 1,demo 2,)
22: demo in2=Concat( demo 3,demo 4,)
23: xin1=Concat( T(sent),T(demo in1))
24: xin2=Concat( T(Par(sent )),T(demo in2))
25: ▷MLM Learning:
26: output 1=M(xin1)
27: LMLM = CE( output 1, y)
28: LMLM .backward()
29: optimizer.step()
30: ▷Contrastive Learning:
31: output 2=M(xin2)
32: LSupCon = SupCon( output 1, output 2)
33: LSupCon .backward()
34: optimizer.step()
35:end for
label words, where V(y)represents the label word
for class y. This can be written as:
p(y|xin) =p([MASK ] =V(y)|xin)
=exp(wV(y).h[MASK ])P
y′∈Yexp(wV(y′).h[MASK ])(1)
where the weight vector of the MLM head is de-
noted by w.
In LM-BFF, the authors add demonstrations to
the input xprompt to improve the model’s under-
standing of verbalizers. As a result, the input toLM-BFF is in the following form:
T(xin)⊕ T(x1
in, y1)⊕...⊕ T(xk
in, yk)(2)
where T(xi
in, yi)illustrates the i-th demonstra-
tion in the template mathcalT with where the ac-
tual verbalizer of the samples replaces the [MASK].
Also, kis the number of demonstrations we want
to use in our prompts. This paper uses random sam-
pling to select demonstrations from the training set.
The MLM loss is calculated as follows:
LMLM =X
(xin,y)∈Dtrain−log[p(y|xin)](3)
Supervised Contrastive Loss. Supervised Con-
trastive Learning is a specific form of contrastive
learning (Chen et al., 2020; Tian et al., 2020; Liu
et al., 2021) that clusters two augmented batches at
the class level in the feature space and calculates
the contrastive loss using Equation 4:
LSupCon = (x′
1, x′
2, y) (4)
where x′
1andx′
2are the augmented version of the
input batch xandyis the actual label of the batch.
To use SupCon on multiple views of an input
text, we first need to obtain two views of the text:
xin1=T(sent)⊕ T(demo 1)⊕ T(demo 2)(5)
xin2=T(Par(sent))⊕T(demo 3)⊕T(demo 4)
(6)
where xin1is the same as xprompt +demo in LM-
BFF and Tis a function that formats the sentence
according to a specific template. Instead of using a
new template in which the newly generated sample
does not provide a new perspective, we use the
few-shot paraphrasing ( Par) function. Also, verb
stands for the verbalizer used for the actual label of
the sample. Now using Equation 4 on two views,
we can calculate the total loss:
LTotal =LSupCon +LMLM (7)
Algorithm D.1 shows an overview of our method
which uses contrastive few-shot fine-tuning with
few-shot paraphrasing. It is important to mention
that learning from LSupCon requires one additional
forward and backward pass, which increases the
computational cost by a factor of 1.5. However,
the cost is still the same as Jian et al. (2022)’s
model due to the O(1)time complexity of the
Paraphrase function. Figure 1 shows the fine-
tuning procedure for one prompt sample and its
new view created using few-shot paraphrasing.
