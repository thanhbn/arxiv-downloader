# Platypus: Cải tiến Nhanh, Rẻ và Mạnh mẽ
cho các LLM

Chúng tôi trình bày Platypus, một họ các Mô hình Ngôn ngữ Lớn (LLM) được tinh chỉnh và hợp nhất đã đạt được hiệu suất mạnh nhất và đứng ở vị trí đầu tiên trong Bảng xếp hạng LLM Mở của HuggingFace *tại thời điểm viết bài. Trong công trình này, chúng tôi mô tả (1) bộ dữ liệu được tuyển chọn Open-Platypus của chúng tôi, là một tập con của các bộ dữ liệu mở khác và chúng tôi công bố cho cộng đồng (2) quy trình tinh chỉnh và hợp nhất các mô-đun LoRA để bảo toàn kiến thức nền mạnh mẽ của các LLM đã được huấn luyện trước, đồng thời đưa kiến thức chuyên ngành cụ thể lên bề mặt (3) nỗ lực kiểm tra rò rỉ dữ liệu thử nghiệm và ô nhiễm trong dữ liệu huấn luyện, có thể cung cấp thông tin cho nghiên cứu trong tương lai. Cụ thể, họ Platypus đạt được hiệu suất mạnh trong các chỉ số LLM định lượng trên các kích thước mô hình, dẫn đầu bảng xếp hạng LLM Mở toàn cầu trong khi chỉ sử dụng một phần nhỏ dữ liệu tinh chỉnh và tổng lượng tính toán được yêu cầu cho các LLM tinh chỉnh tối tân khác. Đặc biệt, một mô hình Platypus 13B có thể được huấn luyện trên một GPU A100 duy nhất sử dụng 25k câu hỏi trong 5 giờ. Điều này là minh chứng cho chất lượng của bộ dữ liệu Open-Platypus của chúng tôi, và mở ra cơ hội cho nhiều cải tiến hơn trong lĩnh vực này.

## 1 Giới thiệu

Nghiên cứu của chúng tôi tập trung vào việc cải thiện hiệu suất của các Mô hình Ngôn ngữ Lớn (LLM) cơ sở bằng cách tinh chỉnh các mô hình sử dụng điều chỉnh hiệu quả tham số (PEFT) trên một bộ dữ liệu được tuyển chọn nhỏ nhưng mạnh mẽ là Open-Platypus. Công trình này diễn ra trong bối cảnh của những tiến bộ gần đây trong lĩnh vực LLM. Sự phát triển nhanh chóng của các mô hình này được khởi xướng bởi sự xuất hiện của các định luật mở rộng. Ngay sau đó, các mô hình 100B+ tham số như PaLM và GPT-3 đã được đề xuất. Các mô hình chuyên biệt theo nhiệm vụ xuất hiện tiếp theo, như Galactica cho các nhiệm vụ khoa học. Chinchillia được giới thiệu cùng với một phương pháp định luật mở rộng mới chuyển trọng tâm từ kích thước mô hình sang số lượng token được xử lý.

Để thách thức sự thống trị của các mô hình nguồn đóng như GPT-3.5 và GPT-4 của OpenAI, Meta đã phát hành các mô hình LLaMa gốc, hiện được biết đến với hiệu quả tính toán trong quá trình suy luận. Các sáng kiến nguồn mở như BLOOM và Falcon cũng đã được phát hành để thách thức sự thống trị của các đối tác nguồn đóng. Gần đây, Meta AI đã phát hành các mô hình LLaMa-2. Ngay sau khi phát hành ban đầu, mô hình 70B tham số đã được StabilityAI tinh chỉnh để tạo ra StableBeluga2 sử dụng bộ dữ liệu kiểu Orca. Khi quy mô của cả kiến trúc mạng và bộ dữ liệu huấn luyện đã tăng, áp lực hướng tới việc sử dụng LLM như các công cụ tổng quát có khả năng xử lý một loạt rộng các nhiệm vụ đã tăng cường. Đối với các mô hình lớn nhất, khả năng tổng quát của chúng làm cho chúng phù hợp với nhiều nhiệm vụ NLP, với các mô hình nhỏ hơn gặp khó khăn trong việc duy trì cùng mức độ đa dạng.

Một số chiến lược đã được sử dụng để cố gắng thu hẹp khoảng cách này. Một phương pháp nổi bật được gọi là chưng cất kiến thức nhằm chuyển giao kiến thức từ một mô hình giáo viên lớn, hiệu suất cao hơn sang một mô hình học sinh nhỏ hơn, bảo toàn hiệu suất trong khi giảm chi phí tính toán. Gần đây, phương pháp phổ biến nhất liên quan đến chưng cất kiến thức từ một bộ dữ liệu huấn luyện lớn thành một bộ nhỏ, một lần nữa làm cho nó ít tốn kém về mặt tính toán hơn so với các phương pháp truyền thống. Các phương pháp này cũng có xu hướng tận dụng điều chỉnh hướng dẫn, đã được chứng minh là một phương pháp hiệu quả để cải thiện hiệu suất tổng quát của LLM. Các dự án như Alpaca của Stanford và WizardLM cung cấp khung cho việc tạo ra dữ liệu định dạng hướng dẫn chất lượng cao. Tinh chỉnh các mô hình cơ sở trên các loại bộ dữ liệu này và áp dụng phương pháp tự hướng dẫn đã dẫn đến những cải thiện đáng kể trong cả hiệu suất định lượng và định tính của chúng.

Phương pháp Hỗn hợp Chuyên gia sử dụng tính toán có điều kiện, kích hoạt các phần mạng dựa trên các ví dụ riêng lẻ. Kỹ thuật này tăng cường khả năng mô hình mà không có sự gia tăng tuyến tính trong tính toán. Các biến thể thưa, như Switch Transformer, kích hoạt các chuyên gia được chọn cho mỗi token hoặc ví dụ, giới thiệu tính thưa của mạng. Những mô hình như vậy xuất sắc trong khả năng mở rộng qua các lĩnh vực và duy trì trong học tập liên tục, như được thấy với Expert Gate. Tuy nhiên, định tuyến chuyên gia không hiệu quả có thể dẫn đến huấn luyện thiếu và chuyên môn hóa không đều của các chuyên gia.

Theo sau sự xuất hiện gần đây của LoRA là QuantizedLoRA (QLoRA), đã được công nhận là một phương pháp hiệu quả và tiết kiệm chi phí. Các tác giả của QLoRA đồng thời phát hành Guanaco, một họ mô hình mới. Các mô hình Guanaco tốt nhất hiện xếp thứ 7 và 12 trên bảng xếp hạng Hugging Face tại thời điểm viết bài. Tuy nhiên, quyết định ban đầu của chúng tôi sử dụng LoRA xảy ra trước khi phát hành QLoRA, và chúng tôi giữ nguyên vì nó đã chứng minh hiệu quả trong quy trình làm việc hiện có của chúng tôi—tức là tương thích và thành công trong việc hợp nhất mô hình. Vì mục tiêu tương lai của chúng tôi bao gồm giảm thời gian và chi phí huấn luyện, chúng tôi sẽ rất hào hứng sử dụng LoRA được lượng tử hóa trong pipeline của mình và so sánh kết quả.

Các phương pháp khác đã tập trung vào huấn luyện LLM trong các nhiệm vụ cụ thể như lập trình, lý luận định lượng và kiến thức y sinh. Việc huấn luyện chuyên biệt này có những ưu điểm riêng. Bằng cách tập trung vào các lĩnh vực hẹp hơn, các mô hình này có thể đạt được tỷ lệ chính xác cao hơn và đầu ra phù hợp hơn trong các lĩnh vực tương ứng.

Một hạn chế lớn của phương pháp này, đặc biệt đối với các mô hình chuyên ngành được phái sinh từ các mô hình lớn được huấn luyện trước, là quá trình tinh chỉnh có thể tốn thời gian và tốn kém. Công trình của chúng tôi tìm cách giải quyết những vấn đề này bằng cách tập trung vào việc tinh chỉnh công thức huấn luyện nhằm duy trì lợi ích của điều chỉnh hướng dẫn, tức là cải thiện tổng quát, đồng thời cũng truyền đạt kiến thức chuyên ngành cụ thể. Chúng tôi thấy rằng các bộ dữ liệu chuyên ngành tăng hiệu suất trên một danh mục nhiệm vụ được chọn, khi kết hợp với việc hợp nhất sẽ giảm đáng kể thời gian huấn luyện. Các đóng góp cốt lõi của chúng tôi như sau:

• Open-Platypus, một bộ dữ liệu quy mô nhỏ bao gồm một tập con được tuyển chọn của các bộ dữ liệu văn bản công cộng. Bộ dữ liệu tập trung vào việc cải thiện kiến thức STEM và logic của LLM, và được tạo thành từ 11 bộ dữ liệu nguồn mở. Nó chủ yếu bao gồm các câu hỏi được thiết kế bởi con người, với chỉ ~10% câu hỏi được tạo ra bởi LLM. Lợi thế chính của Open-Platypus là, với kích thước và chất lượng của nó, nó cho phép hiệu suất rất mạnh với thời gian và chi phí tinh chỉnh ngắn và rẻ. Cụ thể, một người có thể huấn luyện mô hình 13B của riêng mình trên một GPU A100 duy nhất sử dụng 25k câu hỏi trong 5 giờ.

• Mô tả quy trình loại trừ tương tự của chúng tôi để giảm kích thước bộ dữ liệu cũng như giảm sự dư thừa dữ liệu.

• Cái nhìn chi tiết về hiện tượng luôn hiện hữu của việc ô nhiễm các bộ huấn luyện LLM mở với dữ liệu có trong các bộ thử nghiệm LLM quan trọng, và mô tả quy trình lọc dữ liệu huấn luyện của chúng tôi để tránh cạm bẫy này.

• Mô tả quy trình lựa chọn và hợp nhất của chúng tôi cho các mô-đun LoRA tinh chỉnh chuyên biệt.

## 2 Phương pháp

### 2.1 Tuyển chọn Open-Platypus

Các quyết định của chúng tôi về việc lựa chọn dữ liệu để tinh chỉnh các mô hình LLaMa-2 được ảnh hưởng bởi (1) Giả thuyết Căn chỉnh Bề mặt được trình bày bởi LIMA, nêu rằng kiến thức mô hình hầu như hoàn toàn được học trong quá trình huấn luyện trước, và với dữ liệu huấn luyện tối thiểu có thể đạt được kết quả xuất sắc trong việc căn chỉnh đầu ra mô hình; (2) bài báo giới thiệu LLaMa2 trong đó họ nêu rằng các mô hình cơ sở chưa đạt đến độ bão hòa; và (3) công trình của họ, nhấn mạnh tầm quan trọng của dữ liệu đầu vào chất lượng cao để huấn luyện các mô hình hiệu quả. Đưa vào thực tế, và ghi nhớ mục tiêu tối ưu hóa thời gian huấn luyện và hiệu suất mô hình, phương pháp tinh chỉnh các mô hình LLaMa-2 của chúng tôi là sự pha trộn cân bằng của ba điểm trên. Bằng cách tập trung vào chiều sâu trong các lĩnh vực cụ thể, tính đa dạng của lời nhắc đầu vào, và giữ kích thước của bộ huấn luyện nhỏ, chúng tôi nhằm tối đa hóa độ chính xác và tính liên quan của đầu ra mô hình. Để đạt được điều này, chúng tôi đã tuyển chọn một bộ dữ liệu được lọc nội dung, điều chỉnh hướng dẫn rút ra từ nhiều bộ dữ liệu nguồn mở khác nhau. Trong bối cảnh này, 'được lọc nội dung' đề cập đến lựa chọn của chúng tôi cho bộ huấn luyện gần như chỉ bao gồm dữ liệu liên quan đến lĩnh vực quan tâm của chúng tôi, cụ thể là STEM.

Open-Platypus được tạo thành từ 11 bộ dữ liệu nguồn mở, được chi tiết trong Bảng 1. Nó chủ yếu bao gồm các câu hỏi được thiết kế bởi con người, với ~10% câu hỏi được tạo ra bởi LLM. Với trọng tâm của chúng tôi vào STEM và logic, chúng tôi chủ yếu rút ra từ các bộ dữ liệu hướng tới những chủ đề đó, bổ sung chúng với nội dung được lọc từ khóa từ các bộ dữ liệu có phạm vi chủ đề rộng hơn, cụ thể là Openassistant-Guanaco và airoboros. Xương sống của Open-Platypus là một phiên bản sửa đổi của MATH đã được bổ sung với các giải pháp từng bước mở rộng từ PRM800K.

Chúng tôi sử dụng định dạng điều chỉnh hướng dẫn Alpaca, trong đó mỗi câu hỏi được cấu trúc với một hướng dẫn, đầu vào và đầu ra. Trong nhiều trường hợp, đầu vào để trống. Tuy nhiên, đối với một số bộ dữ liệu bao gồm các câu hỏi trắc nghiệm, cụ thể là ARB và ReClor, chúng tôi đã tích hợp bối cảnh định dạng { Chọn A, B, C, hoặc D } làm đầu vào cho mỗi câu hỏi. Đối với ScienceQA, chúng tôi đã chọn bao gồm các câu trả lời dạng dài cho các câu hỏi trắc nghiệm, hoàn toàn bỏ qua tuyên bố rõ ràng về lựa chọn đúng.

### 2.2 Loại bỏ các câu hỏi tương tự & trùng lặp

Sau khi thu thập dữ liệu từ một số nguồn, chúng tôi đã chạy nó qua một quy trình khử trùng lặp để giảm thiểu khả năng ghi nhớ. Đầu tiên, chúng tôi loại bỏ tất cả các hướng dẫn trùng lặp từng từ, sau đó loại bỏ các hướng dẫn có độ tương tự cosine 80% với các embedding SentenceTransformers của các hướng dẫn khác trong bộ huấn luyện của chúng tôi. Trong cả hai trường hợp, chúng tôi mặc định giữ cặp câu hỏi-câu trả lời có câu trả lời dài dòng hơn. Động lực của chúng tôi đằng sau điều này là các câu trả lời dài hơn có khả năng dẫn đến những giải thích chi tiết hơn và/hoặc các giải pháp từng bước.

### 2.3 Kiểm tra Ô nhiễm

Một thành phần cốt lõi trong phương pháp của chúng tôi xoay quanh việc đảm bảo rằng không có câu hỏi thử nghiệm benchmark nào vô tình rò rỉ vào bộ huấn luyện, điều này là một hiện tượng khá phổ biến. Chúng tôi tìm cách ngăn chặn việc ghi nhớ dữ liệu thử nghiệm làm méo mó kết quả benchmark. Với suy nghĩ đó, chúng tôi đã cho phép một số sự khoan dung trong việc xác định xem các câu hỏi có nên được đánh dấu là trùng lặp và loại bỏ khỏi bộ huấn luyện hay không. Cho phép một số linh hoạt trong việc xác định các câu hỏi đáng ngờ thừa nhận rằng có nhiều cách để diễn đạt một truy vấn, và kiến thức chung về lĩnh vực có thể ngăn một câu hỏi không được coi là trùng lặp.

Để làm điều đó, chúng tôi đã phát triển các heuristic sau để hướng dẫn việc lọc thủ công các câu hỏi từ Open-Platypus có điểm tương tự > 80% với bất kỳ câu hỏi benchmark nào. Chúng tôi phân loại các rò rỉ tiềm năng thành ba nhóm: trùng lặp, vùng xám và tương tự nhưng khác biệt. Với mục đích của chúng tôi, chúng tôi nghiêng về phía thận trọng và loại bỏ tất cả chúng khỏi bộ huấn luyện.

**Trùng lặp** Các câu hỏi được đánh dấu là ô nhiễm trùng lặp về cơ bản là bản sao chính xác của các câu hỏi được tìm thấy trong các bộ thử nghiệm. Điều này bao gồm các câu hỏi huấn luyện có thêm một từ hoặc sắp xếp lại nhỏ liên quan đến một câu hỏi benchmark. Ô nhiễm trùng lặp là danh mục duy nhất chúng tôi tính là ô nhiễm "thật" và tương ứng với số lượng câu hỏi bị rò rỉ được liệt kê trong Bảng 1.

**Vùng xám** Nhóm tiếp theo, được gọi là vùng xám, bao gồm các câu hỏi không phải là bản sao chính xác và nằm trong phạm vi kiến thức chung. Mặc dù chúng tôi để lại phán quyết cuối cùng của những câu hỏi này cho cộng đồng nguồn mở, chúng tôi tin rằng chúng thường cần thiết kiến thức chuyên gia. Đáng chú ý, danh mục này bao gồm các câu hỏi có hướng dẫn giống hệt nhau nhưng câu trả lời đồng nghĩa (ví dụ, "da" so với "biểu bì" như được minh họa trong hình 2). Nó cũng bao gồm các câu hỏi mà mặc dù được diễn đạt khác nhau, có câu trả lời chính xác hoặc chi tiết có mặt trong đầu ra huấn luyện.

**Tương tự nhưng khác biệt** Danh mục cuối cùng bao gồm các câu hỏi mà mặc dù có điểm tương tự cosine cao, nhưng mang lại những câu trả lời rất khác nhau. Điều này thường có thể được quy cho những thay đổi tinh tế trong cấu trúc câu hỏi, dẫn đến những phản ứng hoàn toàn khác biệt.

### 2.4 Tinh chỉnh & hợp nhất

Sau khi tinh chỉnh bộ dữ liệu và kiểm tra ba lần về ô nhiễm, phương pháp của chúng tôi tập trung vào hai điểm chính: hiệu quả của huấn luyện Xấp xỉ Hạng Thấp (LoRA) và khả năng hợp nhất mô hình tích hợp của thư viện Điều chỉnh Hiệu quả Tham số Tối tân (PEFT). Khác với các phương pháp tinh chỉnh đầy đủ, LoRA đóng băng trọng số mô hình được huấn luyện trước và thêm các ma trận phân tách hạng vào mỗi lớp của transformer. Điều này giảm số lượng tham số có thể huấn luyện cho các nhiệm vụ hạ nguồn và theo phần mở rộng, thời gian và chi phí huấn luyện. Ví dụ, mô hình 13B của chúng tôi được tinh chỉnh sử dụng 1 A100 80GB trong 5 giờ và mô hình 70B của chúng tôi sử dụng 4 A100s 80GB trong 22 giờ. Như một điểm chuẩn để so sánh, Stanford lưu ý rằng việc tinh chỉnh đầy đủ Alpaca-7B của họ mất 3 giờ trên 8 A100s 80GB. Ngoài PEFT và LoRA, chúng tôi đã tinh chỉnh các mô hình của mình bằng thư viện transformers của Hugging Face. Như đã đề cập trước đó, chúng tôi đã sử dụng mẫu định dạng lời nhắc của Stanford Alpaca.

Các nỗ lực ban đầu của chúng tôi trong việc tinh chỉnh các mô hình tập trung vào các mô-đun attention v_proj, q_proj, k_proj và o_proj. Sau đó chúng tôi chuyển sang các mô-đun gate_proj, down_proj và up_proj như được khuyến nghị, do phân tích của họ cho thấy hiệu suất vượt trội so với các mô-đun attention, ngoại trừ các tình huống mà các tham số có thể huấn luyện là một phần rất nhỏ (<0.1%) của tổng tham số. Để nhất quán, chúng tôi đã áp dụng chiến lược này cho cả tinh chỉnh 13 và 70 tỷ tham số, tương ứng với 0.27% và 0.2% tham số có thể huấn luyện. Vui lòng xem danh sách đầy đủ các siêu tham số trong Bảng 8. Sự khác biệt duy nhất giữa các mô hình 13B và 70B của chúng tôi là tốc độ học ban đầu—chúng tôi phải giảm tốc độ học ban đầu cho mô hình 70B từ 4e-4 xuống 3e-4 vì loss đã về không sau 15 bước. Hạng LoRA xác định các chiều của các ma trận hạng thấp, và alpha LoRA là hệ số mở rộng cho các ma trận trọng số. Ma trận trọng số được mở rộng bởi lora_alpha/lora_rank, và giá trị alpha cao hơn gán nhiều trọng số hơn cho các kích hoạt LoRA. Chúng tôi đã chọn 16 vì đây là thông lệ phổ biến trong các script huấn luyện mà chúng tôi đã xem xét và chọn tỷ lệ 1:1 để không áp đảo mô hình cơ sở.

Sau khi xem xét các bộ dữ liệu trong Bảng 1, chúng tôi đã cố ý chọn không hợp nhất với bất kỳ mô hình nào được huấn luyện bằng các bộ dữ liệu bị ô nhiễm. Ví dụ, chúng tôi đã hợp nhất với LLM Dolphin-70B mới sau khi xác nhận không có câu hỏi thử nghiệm nào rò rỉ vào bộ huấn luyện. Chúng tôi đã thực hiện kiểm tra ô nhiễm trên các bộ dữ liệu được sử dụng để huấn luyện các mô hình mà chúng tôi hợp nhất với khả năng tốt nhất của mình, nhưng một số bộ dữ liệu chưa được công bố công khai. Mặc dù chúng tôi không thể đưa ra đảm bảo tuyệt đối cho bất kỳ mô hình hợp nhất nào với các bộ dữ liệu nguồn đóng, chúng tôi tiếp tục với việc cho phép nghi ngờ. Chi tiết bổ sung về các cân nhắc hợp nhất được bao gồm trong phần tiếp theo, vì điều này phụ thuộc vào kết quả benchmark tinh chỉnh.

## 3 Kết quả

Trong phần này, chúng tôi trình bày một phân tích chi tiết về hiệu suất của các mô hình, đo điểm chuẩn chúng với các mô hình tối tân khác. Mục tiêu chính của chúng tôi là phân biệt tác động của việc hợp nhất cả các mô hình rộng và ngách và đánh giá lợi thế của việc tinh chỉnh trên bộ dữ liệu của chúng tôi. Tiến về phía trước, mô hình cơ sở đề cập đến mô hình mà các adapter LoRA được hợp nhất.

Theo dữ liệu Bảng xếp hạng LLM Mở của Hugging Face từ 8/10/23 (Bảng 2), biến thể Platypus2-70B-instruct của chúng tôi đã vượt trội so với các đối thủ cạnh tranh, giành được vị trí hàng đầu với điểm trung bình 73.13. Đáng chú ý, mô hình Stable-Platypus2-13B của chúng tôi, như được thể hiện trong Bảng 3, nổi bật như mô hình 13 tỷ tham số hàng đầu với điểm trung bình 63.96.

Mục tiêu của chiến lược hợp nhất mô hình của chúng tôi là đánh giá tác động hiệp đồng của việc tích hợp với các mô hình rộng như Instruct và Beluga, hoặc các mô hình chuyên biệt như Camel. Một quan sát thú vị là với việc hợp nhất Dolphin, thay vì sử dụng các adapter Platypus thông thường, chúng tôi đã chọn Platypus đã xuất được hợp nhất với LLaMa-2 cơ sở. Quyết định này được ảnh hưởng bởi các thử nghiệm kiểm tra ô nhiễm của chúng tôi đối với bộ dữ liệu Dolphin. Dolphin-Platypus2-70-B là việc hợp nhất duy nhất không làm tốt hơn cả mô hình cơ sở và adapter. Ngoài ra, có sự chênh lệch điểm số nhỏ hơn giữa các mô hình Platypus và Dolphin cơ sở so với các mô hình khác đang được thảo luận. Điều này đã dẫn chúng tôi trở lại với Camel, mà trước đó đã cho thấy kết quả đầy hứa hẹn trong các thử nghiệm ban đầu của chúng tôi sử dụng 13B.

Sau tinh chỉnh, cả mô hình 13B và 70B đều thể hiện những cải thiện đáng kể so với các mô hình LLaMa-2 cơ sở, đặc biệt trong các benchmark ARC và TruthfulQA. Điều này đã thúc đẩy chúng tôi khám phá tiềm năng của việc hợp nhất với các biến thể tinh chỉnh khác. Trong khi các hợp nhất 70B cho thấy những biến đổi cận biên từ điểm cơ sở, các hợp nhất 13B, đặc biệt với Stable Beluga, thể hiện những nâng cao đáng kể. Ví dụ, việc hợp nhất với Stable Beluga đã vượt trội so với các mô hình thành phần của nó ít nhất 0.5% trên hầu hết các benchmark, với sự gia tăng đáng kể 2.91% trong TruthfulQA. Ngoài ra, Stable-Platypus2-13B cũng cho thấy sự gia tăng tổng thể +1.05% so với mô hình cơ sở.

Vì các câu hỏi TruthfulQA chủ yếu là các câu hỏi "kiến thức" (trái ngược với các câu hỏi "lý luận"), sự cải thiện nhất quán trong điểm TruthfulQA qua các hợp nhất cho thấy rằng việc hợp nhất các mô hình có hiệu quả mở rộng cơ sở kiến thức hơn là nâng cao khả năng lý luận. Quan sát này phù hợp với bản chất của các câu hỏi TruthfulQA, chủ yếu dựa trên kiến thức. Khẳng định của bài báo LLaMa-2 rằng độ bão hòa mô hình chưa đạt được tiếp tục hỗ trợ ý tưởng rằng việc hợp nhất có thể giới thiệu thông tin "mới" cho mô hình.

Kết quả nhấn mạnh tiềm năng của việc hợp nhất mô hình như một chiến lược để nâng cao hiệu suất. Việc lựa chọn các mô hình để hợp nhất, dù rộng hay tập trung, đóng vai trò then chốt trong việc xác định kết quả. Các thử nghiệm của chúng tôi với Dolphin, chẳng hạn, nhấn mạnh tầm quan trọng của việc thử nghiệm lặp và lựa chọn mô hình. Hiệu suất nhất quán của các mô hình như Camel-Platypus2-70B qua các benchmark khác nhau tiếp tục nhấn mạnh điểm này.

Trong các thử nghiệm ARC-Challenge, Hellaswag và TruthfulQA, mô hình Camel-Platypus2-70B thể hiện sự thay đổi tích cực đáng kể nhất với sự cải thiện +4.12% trong ARC-challenge. Điều này cho thấy rằng mô hình Camel-Platypus2-70B, khi được hợp nhất với adapter Platypus, có khả năng là sự kết hợp hiệu quả nhất cho các nhiệm vụ liên quan đến ARC-Challenge.

Đối với các thử nghiệm MMLU, kết quả đa dạng hơn. Mô hình Platypus2-70B-instruct thể hiện sự cải thiện đáng kể +18.18% trong abstract_algebra, trong khi mô hình Camel-Platypus2-13B cho thấy sự suy giảm -15.62%. Điều này chỉ ra rằng hiệu quả của việc hợp nhất thay đổi tùy thuộc vào lĩnh vực cụ thể của thử nghiệm. Đáng chú ý, trong machine_learning, mô hình Camel-Platypus2-70B thể hiện sự gia tăng đáng kể +26.32%, củng cố tiềm năng của mô hình này trong các lĩnh vực cụ thể.

Rút ra từ nội dung rộng hơn của bài báo của chúng tôi, những kết quả này nhấn mạnh tầm quan trọng của việc lựa chọn mô hình phù hợp để hợp nhất với adapter Platypus. Các nâng cao hoặc suy giảm hiệu suất không đồng nhất qua tất cả các lĩnh vực, nhấn mạnh nhu cầu đánh giá chuyên ngành trước khi hoàn thiện một hợp nhất.

### 3.1 Khảo sát sâu các nhiệm vụ metric benchmark

Phần Phụ lục chứa phân tích từng nhiệm vụ MMLU theo thay đổi phần trăm và phần trăm thay đổi. Phần thảo luận còn lại sẽ tham chiếu phần trăm thay đổi, nhưng chúng tôi bao gồm cả hai để minh bạch.

Một khảo sát sâu hơn về các metric hiệu suất của các mô hình cơ sở tiết lộ rằng hai mô hình với điểm số rất tương tự không nhất thiết hợp nhất thành một mô hình vượt trội.

**ARC-Challenge, Hellaswag, TruthfulQA-MC: Bảng 4**

• Cải thiện Đáng chú ý nhất: Mô hình Camel-Platypus2-70B trong thử nghiệm ARC-challenge thể hiện sự thay đổi tích cực cao nhất với sự cải thiện +4.12%. Điều này chỉ ra rằng đối với các nhiệm vụ liên quan đến ARC-Challenge, mô hình Camel-Platypus2-70B, khi được hợp nhất với adapter Platypus, có khả năng hiệu quả nhất.

• Người thực hiện Nhất quán: Mô hình Stable-Platypus2-13B cho thấy những thay đổi tích cực nhất quán qua cả ba thử nghiệm so với mô hình cơ sở, chỉ ra hiệu suất đáng tin cậy của nó khi được hợp nhất với adapter Platypus.

• Biến đổi trong Kết quả: Kết quả cho TruthfulQA đặc biệt đa dạng, với mô hình Stable-Platypus2-13B cho thấy sự cải thiện đáng kể +5.87%, trong khi mô hình Dolphin-Platypus2-70B cho thấy sự suy giảm -1.37%.

**MMLU: Bảng 6**

• Hiệu suất Nổi bật: Trong thử nghiệm machine_learning, mô hình Camel-Platypus2-70B thể hiện sự cải thiện đáng kể +26.32%, chỉ ra tiềm năng hiệu quả của nó trong các lĩnh vực machine learning khi được hợp nhất với adapter Platypus.

• Kết quả Đa dạng: Kết quả cho thử nghiệm formal_logic đa dạng, với mô hình Stable-Platypus2-13B cho thấy sự cải thiện đáng kể +27.27%, trong khi mô hình Camel-Platypus2-13B cho thấy sự suy giảm -2.13%.

• Lĩnh vực Nhất quán: Trong các lĩnh vực như marketing, những thay đổi qua tất cả các mô hình là tối thiểu, cho thấy rằng tác động của việc hợp nhất với adapter Platypus có thể bị hạn chế trong các lĩnh vực nhất định.

• Suy giảm Đáng kể: Thử nghiệm college_physics cho thấy sự suy giảm đáng kể cho các mô hình Platypus2-70B-instruct, Dolphin-Platypus2-70B và Camel-Platypus2-70B, với những thay đổi lần lượt là -20.93%, -13.16% và -18.42%. Điều này chỉ ra các vấn đề tương thích tiềm năng hoặc sự kém hiệu quả khi những mô hình này được hợp nhất với adapter Platypus cho các nhiệm vụ liên quan đến vật lý đại học.

Các bảng cung cấp cái nhìn toàn diện về cách các mô hình khác nhau thực hiện khi được hợp nhất với adapter Platypus qua các lĩnh vực khác nhau. Rõ ràng là hiệu quả của việc hợp nhất là chuyên biệt theo lĩnh vực, và không có giải pháp phù hợp cho tất cả. Các nhà nghiên cứu và thực hành nên cẩn thận đánh giá các nâng cao hoặc suy giảm hiệu suất trong lĩnh vực quan tâm cụ thể của họ trước khi hoàn thiện một hợp nhất.

## 4 Tác động Rộng hơn & Công việc Tương lai

Các LLM hiện đại thường yêu cầu nguồn lực tính toán đáng kể, khiến chi phí huấn luyện và suy luận của chúng trở nên hạn chế đối với những người có ngân sách hạn chế. Trong khi các kỹ thuật như lượng tử hóa và LoRA cung cấp một số cứu trợ, một quan sát đáng chú ý từ bảng xếp hạng Hugging Face là sự thành công của các mô hình nhỏ hơn trong các nhiệm vụ cụ thể, như nhập vai và trả lời câu hỏi. Có thể là chiến lược để khai thác hiệu quả của những mô hình nhỏ gọn này và hợp nhất chúng với độ chính xác của các adapter riêng lẻ. Trong hệ sinh thái đó, sự tương tự giữa đầu vào và dữ liệu huấn luyện được sử dụng như một yếu tố a posteriori, làm thiên vị các đầu ra được thông báo bởi dữ liệu tương tự. Phương pháp này về cơ bản khai thác mối tương quan giữa đầu vào và dữ liệu huấn luyện tương tự của chúng để ảnh hưởng đầu ra. Hỗn hợp Chuyên gia (MoEs) trình bày một con đường đầy hứa hẹn để tiếp tục nâng cao độ chính xác, với sự thành công của việc huấn luyện chuyên ngành. Khám phá trong tương lai cũng có thể liên quan đến việc tích hợp các bộ dữ liệu kiểu alpaca và orca, cũng như xem xét tiềm năng của QLoRA trong pipeline của chúng tôi.

Xây dựng trên quan điểm này, LIMA gợi ý một tương lai được đặc trưng bởi một loạt các bộ dữ liệu nhỏ, được tuyển chọn cẩn thận cho các lĩnh vực ngách. Lợi thế của phương pháp này rất rõ ràng: quy trình tinh chỉnh được sắp xếp hợp lý và tìm kiếm tương tự cosine nhanh chóng qua các đầu vào huấn luyện trung bình của các adapter.

Một câu hỏi hấp dẫn là khả năng áp dụng của chiến lược LIMA trong bối cảnh LoRA và PEFT. Câu hỏi này đáng được điều tra thêm trong các nghiên cứu tiếp theo. Công việc tương lai có thể đi sâu hơn vào việc hiểu những sắc thái của việc hợp nhất mô hình, đặc biệt trong bối cảnh của các mô hình có điểm cơ sở tương tự. Tiềm năng của việc tận dụng các mô hình như Lazarus, một hợp nhất LoRA thành công của 6 mô hình, cũng có thể được khám phá.

## 5 Hạn chế

Platypus, là một biến thể tinh chỉnh của LLaMa-2, kế thừa nhiều hạn chế của mô hình cơ sở trong khi giới thiệu một số thách thức độc đáo do việc huấn luyện chuyên biệt của nó. Giống như LLaMa-2, Platypus không nhận được cập nhật kiến thức liên tục sau các giai đoạn huấn luyện trước và tinh chỉnh. Cơ sở kiến thức tĩnh này có thể dẫn đến thông tin lỗi thời hoặc không đầy đủ theo thời gian. Hơn nữa, vẫn còn rủi ro Platypus tạo ra nội dung không có thật hoặc lời khuyên không đủ trình độ, đặc biệt khi đối mặt với các lời nhắc mơ hồ hoặc gây hiểu lầm.

Mặc dù Platypus đã được tinh chỉnh để cải thiện khả năng thành thạo trong STEM và logic, trọng tâm chính của nó, giống như LLaMa-2, đã tập trung vào dữ liệu tiếng Anh. Mặc dù nó có thể thể hiện một số khả năng trong các ngôn ngữ khác, khả năng thành thạo này không được đảm bảo và có thể không nhất quán do dữ liệu huấn luyện trước không phải tiếng Anh hạn chế. Ngoài ra, giống như người tiền nhiệm, Platypus có thể tạo ra nội dung có khả năng gây hại, xúc phạm hoặc thiên vị, đặc biệt khi được huấn luyện trên các bộ dữ liệu có sẵn công khai. Mặc dù đã có nỗ lực để giải quyết những vấn đề này thông qua việc làm sạch dữ liệu, các thách thức vẫn tồn tại, đặc biệt đối với các ngôn ngữ không phải tiếng Anh nơi các bộ dữ liệu toàn diện có thể thiếu.

Khả năng của Platypus, giống như các mô hình AI khác, có thể bị lạm dụng cho mục đích độc hại, như lan truyền thông tin sai lệch hoặc thăm dò các chủ đề nhạy cảm. Mặc dù mô hình của chúng tôi chỉ dành cho sử dụng phi thương mại do giấy phép của bộ huấn luyện, chúng tôi đã tuân theo Hướng dẫn Sử dụng Có trách nhiệm của Meta về tinh chỉnh. Chúng tôi chưa thực hiện bất kỳ thử nghiệm tấn công đối kháng hoặc red teaming nào, vì vậy trước khi triển khai bất kỳ ứng dụng nào của Platypus, các nhà phát triển nên thực hiện thử nghiệm an toàn và điều chỉnh phù hợp với các ứng dụng cụ thể của mô hình.

Do việc huấn luyện chuyên biệt, đặc biệt trong các câu hỏi STEM và logic, Platypus có thể thể hiện những hạn chế khi đối mặt với các chủ đề ngoài lĩnh vực chuyên môn chính của nó. Vui lòng thận trọng—điều cần thiết là tuân thủ các hướng dẫn sử dụng có trách nhiệm và xem xét các biện pháp tinh chỉnh và triển khai bổ sung để đảm bảo hiệu suất tối ưu và an toàn.

Bất kỳ người dùng nào của họ Platypus nên đảm bảo rằng không có ô nhiễm giữa dữ liệu huấn luyện Platypus và bất kỳ bộ thử nghiệm benchmark nào không được sử dụng rõ ràng trong bài báo này. Ví dụ, những người tạo ra PRM800K đã kết hợp các bộ train và test của MATH để tăng chất lượng huấn luyện. Chúng tôi đã sử dụng cả bộ train và test của PRM800K trong quá trình huấn luyện, trừ bất kỳ câu hỏi nào quá tương tự với các bộ dữ liệu benchmark.

Tất cả các hạn chế được đề cập ở trên liên quan đến các biến thể mô hình hợp nhất của chúng tôi. Một lần nữa, chúng tôi đã cố ý chọn không hợp nhất với bất kỳ mô hình nào sử dụng các bộ dữ liệu bị ô nhiễm trong quá trình huấn luyện. Mặc dù chúng tôi không thể đưa ra đảm bảo tuyệt đối, chúng tôi tiếp tục với việc cho phép nghi ngờ. Chúng tôi muốn nhấn mạnh tầm quan trọng của việc thẩm định khi chọn triển khai bất kỳ LLM hoặc bộ dữ liệu nào.

Cuối cùng, chúng tôi lưu ý rằng tìm kiếm từ khóa và tương tự cosine của embedding câu có thể không phải là các phương pháp lọc đầy đủ. Mặc dù chúng tôi tin tưởng rằng không có ô nhiễm trong dữ liệu huấn luyện đã được làm sạch của chúng tôi, nhưng không có khả năng nhưng không phải không thể một số câu hỏi đã lọt qua khe hở.

## Lời cảm ơn

Một lời cảm ơn rất đặc biệt tới cả Hugging Face, đã tạo ra một không gian nơi bất kỳ ai cũng có thể đánh giá và phát hành LLM, và Meta AI đã chia sẻ LLaMa-2, xương sống của các mô hình tinh chỉnh của chúng tôi. Chúng tôi cũng muốn cảm ơn những người tạo ra LoRA, mà không có họ chúng tôi không thể có đủ khả năng để tinh chỉnh biến thể 70B của LLaMa-2.
