# 2303.11403.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2303.11403.pdf
# Kích thước file: 2141214 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
eP-ALM: Tăng cường Nhận thức Hiệu quả cho Mô hình Ngôn ngữ
Mustafa Shukor1Corentin Dancette1Matthieu Cord1,2
1Đại học Sorbonne2Valeo.ai
{firstname.lastname }@sorbonne-universite.fr
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã từng gây ấn tượng với thế giới, với những khả năng chưa từng có xuất hiện ở các mô hình với quy mô lớn. Về mặt thị giác, các mô hình transformer (tức là ViT) đang theo cùng xu hướng này, đạt được hiệu suất tốt nhất trên các benchmark thử thách. Với sự dồi dào của các mô hình đơn phương thức như vậy, một câu hỏi tự nhiên nảy sinh; liệu chúng ta có cần phải theo xu hướng này để giải quyết các tác vụ đa phương thức không? Trong công trình này, chúng tôi đề xuất thay vào đó hướng nỗ lực vào việc thích ứng hiệu quả các mô hình hiện có, và đề xuất tăng cường Mô hình Ngôn ngữ với nhận thức. Các phương pháp hiện có để thích ứng các mô hình được đào tạo trước cho các tác vụ thị giác-ngôn ngữ vẫn dựa vào một số thành phần chính cản trở hiệu quả của chúng. Cụ thể, chúng vẫn đào tạo một số lượng lớn tham số, dựa vào việc đào tạo trước đa phương thức lớn, sử dụng các bộ mã hóa (ví dụ: CLIP) được đào tạo trên các tập dữ liệu hình ảnh-văn bản khổng lồ, và thêm chi phí suy luận đáng kể. Ngoài ra, hầu hết các phương pháp này đã tập trung vào Zero-Shot và In Context Learning, với ít hoặc không có nỗ lực nào cho việc tinh chỉnh trực tiếp. Chúng tôi điều tra nỗ lực tính toán tối thiểu cần thiết để thích ứng các mô hình đơn phương thức cho các tác vụ đa phương thức và đề xuất một thiết lập thử thách mới, cùng với các phương pháp khác nhau, để thích ứng hiệu quả các mô hình được đào tạo trước đơn phương thức. Chúng tôi chỉ ra rằng bằng cách đóng băng hơn 99% tổng số tham số, chỉ đào tạo một lớp chiếu tuyến tính, và thêm tiền tố chỉ một token có thể đào tạo, phương pháp của chúng tôi (được gọi là eP-ALM) vượt trội đáng kể so với các baseline khác trên VQA và Captioning trên các phương thức Hình ảnh, Video và Âm thanh, theo thiết lập được đề xuất. Mã nguồn sẽ có sẵn tại đây: https://github.com/mshukor/eP-ALM.

1. Giới thiệu
Việc mở rộng quy mô lớn đã dẫn đến hiệu suất vượt trội liên tục cải thiện trên các tác vụ, phương thức và miền trên các benchmark hiện tại. Hầu hết tiến bộ cho đến nay là trong các lĩnh vực thị giác và ngôn ngữ. Đối với Thị giác Máy tính, họ ViT [21] bắt đầu từ mô hình tiny với 5M tham số đến ViT-e khổng lồ [15] với 4B tham số và ViT-22B lớn nhất với 22B tham số [20]. Hấp dẫn hơn, là quy mô của các Mô hình Ngôn ngữ Lớn (LLMs), như họ BLOOM [76] và OPT [107], từ hàng trăm triệu tham số đến 175B, ngoài các mô hình khác vượt quá 100B [7, 19, 83] lên đến 1T tham số [27]. Những quy mô khổng lồ này đi kèm với nhu cầu về các tập dữ liệu đào tạo trước rất lớn và thời gian đào tạo dài.

Paradigm phổ biến hiện tại để giải quyết các tác vụ đa phương thức, đặc biệt là các tác vụ Thị giác-Ngôn ngữ, là tận dụng các mô hình được đào tạo trước, và sau đó tiếp tục đào tạo end-to-end [15, 56, 79, 81, 94] trên các tập dữ liệu hình ảnh-văn bản lớn. Tuy nhiên, chi phí đào tạo là rất lớn và không thể chi trả được cho một phần lớn cộng đồng, vì những phương pháp này vẫn đào tạo tất cả tham số mô hình, ngay cả sau khi khởi tạo, trên một lượng dữ liệu khổng lồ.

Với sự dồi dào của các mô hình đơn phương thức, một câu hỏi tự nhiên nảy sinh;
Liệu chúng ta có cần phải theo xu hướng này để giải quyết các tác vụ đa phương thức không? hay thay vào đó hướng nỗ lực vào việc thích ứng hiệu quả các mô hình hiện có?

--- TRANG 2 ---
Lấy cảm hứng từ công trình gần đây về Mô hình Ngôn ngữ Tăng cường (ALMs) [70], trong bài báo này, chúng tôi ủng hộ việc thích ứng các LMs được đào tạo trước để giải quyết các tác vụ đa phương thức. Cụ thể, bằng cách tăng cường LMs với các bộ mã hóa nhận thức.

Một số phương pháp đã lệch khỏi paradigm đào tạo end-to-end bằng cách đóng băng một số module được đào tạo trước và chỉ đào tạo các tham số thích ứng, chẳng hạn như cross-attention bổ sung [3], bộ mã hóa thị giác [90] và Adapters [23]. Mặc dù các phương pháp này đã tiến một bước lớn hướng tới các mô hình tiết kiệm tham số hơn, vẫn còn nhiều thành phần tốn kém cản trở việc áp dụng của chúng bởi cộng đồng rộng lớn, chẳng hạn như chi phí bộ nhớ và thời gian đào tạo và suy luận.

Trong công trình này, chúng tôi lập luận rằng các phương pháp hiện tại còn xa mới tối ưu và có thể tìm thấy các phương pháp thích ứng hiệu quả hơn, về mặt số lượng tham số có thể đào tạo, dữ liệu đào tạo và tính toán, để thích ứng các mô hình đơn phương thức được đào tạo trước cho các tác vụ đa phương thức. Việc căn chỉnh tốt hơn các biểu diễn thị giác và ngôn ngữ có thể giúp thiết kế các phương pháp thích ứng cực kỳ hiệu quả.

Để điều tra giả thuyết này, chúng tôi tiến thêm một bước để tận dụng hiệu quả các LLMs, và đề xuất (1) một kỹ thuật mới để thích ứng các mô hình đơn phương thức bằng cách đóng băng hơn 99% (lên đến 99.94%) tham số của chúng, cùng với (2) một thiết lập tối thiểu và thử thách để thích ứng các mô hình đơn phương thức được đào tạo trước cho các tác vụ Hình ảnh/Video/Âm thanh-Ngôn ngữ (ví dụ: VQA [33, 97], Image và Audio Captioning [14,48]). Trong thiết lập này, chúng tôi ưu tiên các mô hình chỉ đơn phương thức, tránh đào tạo trước đa phương thức hoặc các bộ mã hóa đa phương thức được đào tạo rộng rãi, và xem xét kiến trúc LLMs điển hình làm backbone. Tất cả điều đó trong khi đóng băng càng nhiều tham số mô hình càng tốt. Phương pháp được minh họa trong Hình 1.

Cụ thể, chúng tôi áp dụng mô hình OPT được phát hành công khai [107] và các bộ mã hóa đơn phương thức (ví dụ: ViT, TimeSformer [6], AST [31]), được giữ đóng băng. Chúng tôi tinh chỉnh trực tiếp các tham số thích ứng trên các benchmark có sẵn công khai của các tác vụ downstream như VQA, GQA, Image Captioning, Video QA, Video Captioning và Audio Captioning.

Dựa trên thiết lập này, chúng tôi điều tra các lựa chọn thiết kế khác nhau và đề xuất các phương pháp rất hiệu quả được hỗ trợ bởi những phát hiện thú vị sau:
•Đào tạo một lớp tuyến tính duy nhất trực tiếp trên các tập dữ liệu đa phương thức downstream, và theo cùng thiết lập, vượt trội hơn các công trình khác trên các tác vụ Hình ảnh/Video/Âm thanh-Ngôn ngữ. Với một vài tham số có thể đào tạo bổ sung và một token tiền tố đã học duy nhất, chúng tôi có thể cải thiện đáng kể hiệu suất, trong khi tôn trọng ngân sách 1% tham số có thể đào tạo, và giữ chi phí suy luận gần như không đổi.
•Phương pháp của chúng tôi có khả năng khái quát hóa tốt hơn (OOD, Zero-Shot) và tiết kiệm dữ liệu (đào tạo trên 1% dữ liệu đạt 80% hiệu suất) với kết quả few-shot tốt hơn các phương pháp khác.
•Trong khi đạt hiệu suất tốt với các mô hình ngôn ngữ quy mô nhỏ đến trung bình (tức là 350M-2.7B), cải thiện vẫn tăng bằng cách mở rộng quy mô cả mô hình thị giác và ngôn ngữ. Khi mở rộng cả hai mô hình, chúng tôi vẫn có thể vượt trội hơn các phương pháp khác chỉ với 0.06% tham số có thể đào tạo.
•Các phương pháp hiện có không hoạt động tốt trên thiết lập thử thách được đề xuất, không có đào tạo trước đa phương thức lớn.

2. Công trình liên quan
Mô hình Thị giác-Ngôn ngữ (VLMs). Trước đây, các tác vụ thị giác-ngôn ngữ đã được giải quyết bằng các mô hình được tùy chỉnh nhiều cho tác vụ cụ thể [8,26,41,44,47]. Thành công trong Học tập Tự giám sát [9,34,36,87,96] và tầm quan trọng của việc khởi tạo tốt đã thúc đẩy các nhà nghiên cứu chuyển những ý tưởng này sang VLMs và bắt đầu Đào tạo trước Thị giác-Ngôn ngữ (VLP) trên quy mô lớn video-text [28, 57, 92], các tập dữ liệu hình ảnh-văn bản trong các miền chung [16,51,55,56,65,79], cũng như các miền cụ thể, như Nấu ăn [80], Hình ảnh Y tế [71] và Trích xuất Sự kiện [58]. VLP là một bước để thoát khỏi gánh nặng tùy chỉnh bằng cách có một mô hình được đào tạo trước, được khai thác cho nhiều tác vụ downstream. Gần đây, chúng ta đã chứng kiến công trình ấn tượng tiến thêm một bước hướng tới sự thống nhất hơn, bằng cách thống nhất mô hình, mục tiêu đào tạo và định dạng đầu vào-đầu ra [15, 66, 93, 94]. Tất cả các mô hình này đào tạo hầu hết tham số mô hình, ngay cả sau khi khởi tạo, điều này trở nên tốn kém hơn với xu hướng hiện tại trong việc mở rộng dữ liệu, kích thước mô hình và tính toán [15, 103]. Một phương pháp khác cho VLM là khai thác các mô hình được đào tạo trước hiện có bằng cách giữ chúng đóng băng và chỉ đào tạo các tham số thích ứng [3, 23, 60]. Công trình này ủng hộ cách tiếp cận sau, ưu tiên hiệu quả đào tạo về mặt bộ nhớ và thời gian.

Thích ứng Mô hình Ngôn ngữ. Các Mô hình Ngôn ngữ Lớn (LLMs) [7, 19, 38, 76, 83, 107] đã gây ấn tượng với thế giới trong vài năm qua, cho thấy hiệu suất chưa từng có trên vô số tác vụ NLP. Việc mở rộng LLMs lên hàng trăm tỷ tham số đã được thúc đẩy bởi các khả năng xuất hiện một cách bất ngờ [95] ở quy mô này và dẫn đến những bước nhảy đột ngột của các chỉ số liên quan trên các tác vụ downstream khó [37, 74, 84]. Khả năng khái quát hóa này đã thúc đẩy các nhà nghiên cứu bắt đầu thích ứng những mô hình này cho các phương thức khác [3, 90], các tác vụ [35, 88, 102, 106] và các miền [82]. Hiện tại, hầu hết sự tập trung được tập trung vào khai thác LLMs cho các tác vụ thị giác-ngôn ngữ, như Flamingo [3] đào tạo 10B tham số để thích ứng một mô hình ngôn ngữ 70B tham số bị đóng băng, và các kỹ thuật hiệu quả thành công khác dựa trên prompt tuning có điều kiện thị giác (Frozen [90], PromptFuse [60], LiMBeR [69]) và adapters (MAGMA [23]).

--- TRANG 3 ---
Công trình này đã chứng minh hiệu suất tốt, cho thấy rằng có thể thiết kế các phương pháp rất hiệu quả để thích ứng các mô hình ngôn ngữ hiện có [38, 91]. Về mặt video, ít công trình đã được đề xuất, chủ yếu dựa trên Adapters [78, 101]. Gần gũi nhất với phương pháp của chúng tôi là PromptFuse [60] tinh chỉnh trực tiếp cho VQA, tuy nhiên, họ sử dụng các mô hình ngôn ngữ encoder-decoder và đào tạo một soft prompt được thêm tiền tố vào đầu vào.

Học tập Hiệu quả. Học tập Tiết kiệm Tham số là một hướng nghiên cứu thú vị bao gồm việc thích ứng các mô hình được đào tạo trước sử dụng rất ít tham số có thể đào tạo. Prompt Tuning [54] là một phương pháp như vậy thêm một vài token có thể học, hoặc Soft Prompts để bối cảnh hóa đầu vào và điều hướng đầu ra của mô hình bị đóng băng hướng tới tác vụ mong muốn. Các phương pháp khác sử dụng Adapters [4,39], là các MLP có thể đào tạo, bao gồm 2 lớp chiếu tuyến tính với activation ở giữa và được chèn vào bên trong mô hình để thích ứng các lớp self-attention và feedforward. Nhiều phương pháp khác đã được đề xuất trong bối cảnh NLP như LoRa [40], Bitfit [104], Hyperformer [67], Compacters [46] và (IA)3[62]. Những phương pháp này đã được thích ứng thành công cho các phương thức khác như hình ảnh [13, 43], hình ảnh-văn bản [85, 86, 108], với rất ít công trình về video [72] và Âm thanh [50].

Một hướng nghiên cứu khác là các kỹ thuật Tiết kiệm Dữ liệu, nơi mục tiêu là đạt hiệu suất tương tự bằng cách giảm đáng kể các tập dữ liệu đào tạo. Gần đây, một số nỗ lực đã được đề xuất cho thị giác [89], ngôn ngữ [22] và thị giác-ngôn ngữ [12,17,79], chủ yếu tập trung vào thiết kế các mục tiêu đào tạo tốt hơn [79]. Tuy nhiên, ít công trình đã được thực hiện để điều tra mối liên hệ giữa hiệu quả tham số và hiệu quả dữ liệu, điều này được xem xét trong công trình này.

3. Khung công trình
Để giải quyết các tác vụ đa phương thức, chúng tôi đề xuất tăng cường các LLMs được đào tạo trước với nhận thức thông qua các bộ mã hóa nhận thức đơn phương thức (Hình 1). Chúng tôi chi tiết phương pháp của chúng tôi như sau.

3.1. eP-ALM
Chúng tôi tăng cường một LM được đào tạo trước với nhận thức thông qua nhiều bộ mã hóa cụ thể cho từng phương thức. Các bộ mã hóa tương tác với LM thông qua các token [CLS] cụ thể cho phương thức được chiếu tuyến tính. Để dễ dàng thích ứng, chúng tôi tận dụng một số kỹ thuật tiết kiệm tham số, như Prompt Tuning. Trong phần này, chúng tôi chi tiết các nguyên tắc thiết kế của phương pháp chúng tôi, được minh họa trong Hình 2.

Mô hình Ngôn ngữ (LM) Chúng tôi áp dụng các mô hình OPT [107], là các decoder ngôn ngữ autoregressive bao gồm các lớp Self-Attention và Feed Forward. Chúng được đào tạo với mục tiêu dự đoán token tiếp theo trên 180B token chủ yếu bằng tiếng Anh và được thu thập từ các tập dữ liệu khác nhau [5, 29]. Các tác giả đã phát hành một họ mô hình với các quy mô khác nhau, bắt đầu từ 125M lên đến kích thước mô hình 175B. Bên cạnh việc là mã nguồn mở và được đào tạo trên dữ liệu tiếng Anh, các kích thước mô hình khác nhau cho phép chúng tôi dễ dàng điều tra tác động của quy mô, và giúp thiết kế các phương pháp mới với kích thước mô hình có thể chi trả.

Bộ mã hóa Nhận thức Chúng tôi chỉ ưu tiên các mô hình đơn phương thức. Đối với hình ảnh, chúng tôi sử dụng mô hình ViT vanilla [21] bao gồm các lớp Self Attention và FeedForward và được đào tạo trước để phân loại hình ảnh trên ImageNet [75]. Đối với Video, chúng tôi sử dụng TimeSformer [6] bao gồm một mô hình giống ViT được tăng cường với temporal attention và được đào tạo trước trên kinetics [10]. Đối với Âm thanh, chúng tôi áp dụng AST [31], một sự thích ứng vanilla của ViT để tiêu hóa spectrogram, được đào tạo trước trên AudioSet [30]. Mặc dù chúng tôi chỉ xem xét 3 bộ mã hóa này, việc mở rộng phương pháp cho các loại bộ mã hóa và phương thức khác là đơn giản.

Chèn Perceptual Prompt. LMs thường được điều khiển thông qua các prompt văn bản khác nhau, như câu hỏi và hướng dẫn. Ở đây, LM được điều khiển bởi cả văn bản và các bộ mã hóa nhận thức. Cụ thể, các token nhận thức được chiếu được thêm tiền tố vào các token văn bản. Việc sử dụng tất cả các token thị giác một cách ngây thơ, thêm chi phí tính toán đáng kể trong quá trình đào tạo và suy luận, do độ phức tạp bậc hai của các lớp attention với số lượng token. Điều này trở nên rõ ràng hơn với các LLMs. Để giảm thiểu điều này, chúng tôi chỉ xem xét token [CLS] của các bộ mã hóa nhận thức và thêm tiền tố nó vào các token văn bản. Điều này tăng tổng số token lên 1 điều này duy trì tốc độ suy luận gần như không đổi.

Kết nối Các Mô hình với Các lớp Tuyến tính Phân cấp Cross-Modal. Khi đóng băng các bộ mã hóa nhận thức và mô hình ngôn ngữ, số lượng tham số có thể đào tạo tối thiểu là những tham số để kết nối hai mô hình này trong khi điều chỉnh các chiều embedding trong trường hợp không khớp. Do đó, chúng tôi dựa phương pháp của chúng tôi trên ràng buộc này và chỉ đào tạo một lớp chiếu tuyến tính (kết nối đơn, Hình 2) để kết nối cả hai mô hình. Để khai thác biểu diễn phân cấp được mã hóa trong các mô hình được đào tạo trước, thay vì chỉ lấy token [CLS] của lớp đầu ra cuối cùng, chúng tôi lấy các token [CLS] từ nhiều lớp của mô hình nhận thức, và chúng tôi chèn các token này vào nhiều lớp của LM (kết nối chia sẻ). Các token đến từ các lớp sớm được chèn sớm hơn và sau đó được thay thế bởi những token đến từ các lớp sâu hơn. Chúng tôi chỉ ưu tiên các lớp sâu hơn (ví dụ: 6 lớp cuối của ViT-B/16, và 12 lớp cuối của OPT-350M) nơi các biểu diễn trừu tượng hơn và ít cụ thể cho phương thức hơn. Hơn nữa, việc sử dụng cùng một chiếu tuyến tính ở các cấp độ biểu diễn khác nhau có thể không giúp nắm bắt tính đặc thù của một phân cấp như vậy, vì mục đích này, chúng tôi cũng thử nghiệm với các lớp tuyến tính khác nhau cho mỗi cấp độ (nhiều kết nối).

Thích ứng Đa phương thức với Các Kỹ thuật Tiết kiệm Tham số. Chúng tôi khám phá một số kỹ thuật tiết kiệm tham số để dễ dàng thích ứng với các tác vụ đa phương thức. Kỹ thuật chính chúng tôi sử dụng là Prompt Tuning [54]: nó bao gồm việc thêm tiền tố các token có thể đào tạo hoặc Soft Prompts vào các token văn bản đầu vào của LM. Điều này cung cấp bối cảnh hữu ích để điều hướng đầu ra mô hình. Trái với các hard prompt được thiết kế thủ công, điều này cung cấp một phương pháp linh hoạt và dễ dàng hơn cho việc bối cảnh hóa phụ thuộc vào tác vụ. Vì lý do hiệu quả, chúng tôi chỉ thêm tiền tố 10 token có thể học. Chúng tôi cũng thử nghiệm Adapters [39] như được chi tiết sau. Phương pháp có thể được chính thức hóa như sau (đọc tốt hơn với Hình 2):

[CLS]i=C(Ei(X)), i =NE/2, ..., N E,
tj=LMj([CLS]i, pj−1, tj−1), j=NL/2, ..., N L,
(1)

trong đó [CLS]i là token nhận thức của đầu vào X được trích xuất từ lớp i của bộ mã hóa nhận thức (Ei) với NE lớp. [CLS]i được chiếu sử dụng kết nối tuyến tính C và được thêm tiền tố, cùng với Soft Prompt p vào các embedding của các token văn bản tj−1 đến từ lớp trước đó trong LM (LMj−1). Hoạt động này được lặp lại mỗi 2 lớp trong LM (với NL lớp).

3.2. Thiết lập Khung Đào tạo Hướng Hiệu quả
Các phương pháp hiện tại vẫn dựa vào nhiều thành phần tốn kém cản trở việc áp dụng của chúng bởi cộng đồng lớn. Cụ thể; chúng (1) vẫn đào tạo nhiều tham số (ví dụ: bộ mã hóa thị giác [90] và adapters [23] với ∼325M params/5.11%), (2) vẫn duy trì đào tạo trước đa phương thức với các tập dữ liệu cặp hình ảnh-văn bản trên đỉnh đào tạo trước đơn phương thức [23, 69, 90], (3) tận dụng các bộ mã hóa đa phương thức như CLIP, được đào tạo trước trên 400M cặp hình ảnh-văn bản [23, 69], (4) thêm chi phí tính toán đáng kể trong quá trình suy luận, do visual prompt dài, đặc biệt khi đánh giá với In Context Learning (ICL), điều này trở nên phổ biến với LLMs [23,69]. Trong công trình này, chúng tôi đề xuất một thiết lập mới để thích ứng các mô hình đơn phương thức cho các tác vụ downstream đa phương thức. Thiết lập này thử thách hơn và được thúc đẩy bởi việc tìm kiếm nỗ lực ít nhất cần thiết để khai thác các mô hình được đào tạo trước. Thiết lập như sau:

•Chỉ đào tạo các tham số thích ứng (ví dụ: Soft Prompt, kết nối tuyến tính), trong khi giữ càng nhiều tham số được đào tạo trước đóng băng càng tốt (tiết kiệm tham số).
•Tránh đào tạo trước đa phương thức và tinh chỉnh trực tiếp trên các tập dữ liệu đa phương thức downstream (tiết kiệm dữ liệu/tính toán).
•Chỉ sử dụng các mô hình đơn phương thức được đào tạo trước, và tránh sử dụng các bộ mã hóa đa phương thức được đào tạo trước trên các tập dữ liệu khổng lồ (tiết kiệm dữ liệu).
•Giữ suy luận nhanh (ví dụ: 1 token bổ sung), bằng cách tránh các prompt dài, và sử dụng các module nặng bổ sung (tiết kiệm tính toán).
•Sử dụng các mô hình ngôn ngữ chỉ decoder (ví dụ: OPT), kiến trúc hiện tại được áp dụng bởi LLMs (do hiệu quả đào tạo trước và khả năng sinh mở).

Cụ thể, chúng tôi chỉ đào tạo kết nối tuyến tính và soft prompt trực tiếp trên các tác vụ đa phương thức downstream. Điều này bằng với ít hơn 1% tham số có thể đào tạo mà chúng tôi có thể đẩy xa hơn đến 0.06% với các mô hình lớn.

Thiết lập Pretrain Zero-shot. Trọng tâm của công trình này là tinh chỉnh trực tiếp trên các tập dữ liệu đích. Tuy nhiên, cơ chế được đề xuất (Phần 3.1) có thể được thích ứng một cách đơn giản cho thiết lập đánh giá pretrain-zeroshot. Trong phụ lục, chúng tôi chỉ ra rằng eP-ALM vượt trội hơn các công trình trước và có khả năng cạnh tranh với SoTA gần đây theo đánh giá zero-shot.

4. Thử nghiệm
Chi tiết triển khai. Chúng tôi sử dụng OPT-2.7B trong mô hình chính của chúng tôi, eP-ALM, và chúng tôi thử nghiệm trong Phần 4.2 với các mô hình OPT có kích thước khác nhau. Chúng tôi trích xuất các token [CLS] của 6 lớp cuối của các bộ mã hóa nhận thức và thêm tiền tố chúng, sau khi chiếu tuyến tính, vào các token văn bản của 12 lớp cuối của OPT. Lưu ý rằng chúng tôi thay thế [CLS] trước đó bằng cái mới để giữ cùng số lượng token.

Đối với VQA và VideoQA, chúng tôi đưa vấn đề thành sinh mở và tính toán độ chính xác sau khi so sánh nghiêm ngặt giữa văn bản đầu ra (không cắt ngắn) và văn bản ground truth. Lưu ý rằng thiết lập này thử thách hơn so với VQA dựa trên phân loại và không có lợi cho phương pháp của chúng tôi vì mô hình có thể sinh ra các câu trả lời đúng về mặt ngữ nghĩa nhưng sử dụng từ ngữ khác nhau. Chúng tôi sử dụng một token đặc biệt (' </a> ') để phân tách câu hỏi khỏi câu trả lời. Đối với captioning, chúng tôi báo cáo điểm CIDEr và BLUE@4 được áp dụng rộng rãi. Chúng tôi tinh chỉnh với loss cross-entropy cổ điển được sử dụng để đào tạo OPT gốc cho các tác vụ VQA và Captioning. Chúng tôi sử dụng optimizer AdamW với learning rate (lr) 1e-5 được warm up đến 2e-5 sau đó giảm xuống 1e-6 sử dụng cosine scheduler. Chúng tôi đào tạo trong 8 epoch với batch size 64 (128 cho GQA) và độ phân giải hình ảnh 224. Đào tạo phương pháp của chúng tôi với OPT-2.7B cho VQA v2 có thể được thực hiện trên một GPU V100 32GB trong vài giờ. Thêm chi tiết được đưa ra trong phụ lục. Chúng tôi thấy phương pháp này nhạy cảm với phương pháp giải mã văn bản (Tab. ??). Theo các công trình khác, chúng tôi sử dụng greedy decoding với beam search cho các kết quả chính (Phần 4.1), và multinomial/random sampling cho nghiên cứu ablation (Phần 4.2).

Các biến thể eP-ALM. Mô hình chính của chúng tôi, eP-ALM (được minh họa trong Hình 2), có nhiều kết nối tuyến tính; các lớp tuyến tính đã học cụ thể cho mỗi token [CLS] được chèn vào mô hình. Ngoài Prompt Tuning. Chúng tôi cũng kiểm tra các biến thể của mô hình này: eP-ALM ada (eP-ALM với Adapters thay vì Soft Prompts), eP-ALM lin (đào tạo một kết nối tuyến tính chia sẻ với tất cả các token [CLS], và không có prompt tuning) và eP-ALM pt (lin + Soft Prompt). Đối với Adapters, chúng tôi theo các công trình khác [23] và thêm tuần tự một module adapter sau self-attention và feedforward layers trong tất cả các block của OPT. Trong khi điều này có thể cho kết quả tốt hơn, nó thêm một số lượng đáng kể tham số có thể đào tạo.

4.1. Kết quả Chính
Trong phần này, chúng tôi trình bày so sánh chính với các phương pháp khác. Chúng tôi so sánh eP-ALM với SoTA trong Phần 4.1.1, sau đó trình bày kết quả chi tiết hơn cho phương thức hình ảnh trong Phần 4.1.2, phương thức video trong Phần 4.1.3, và phương thức âm thanh trong Phần 4.1.4.

4.1.1 So sánh với SoTA
Chúng tôi bắt đầu bằng cách so sánh eP-ALM với SoTA khác đào tạo số lượng lớn tham số và thường xuyên với đào tạo trước quy mô lớn. Bảng 1 cho thấy so sánh với cả thiết lập zero-shot (ZS) và Finetuning (FT). Hiệu suất của eP-ALM nói chung cao hơn điểm ZS và vẫn thấp hơn điểm FT. Tuy nhiên, khoảng cách hiệu suất với các mô hình FT nhỏ hơn với các phương thức âm thanh và video.

4.1.2 Kết quả Hình ảnh-Văn bản
Chúng tôi sử dụng ViT-B/16 bị đóng băng được đào tạo trước trên ImageNet1K làm bộ mã hóa hình ảnh. Chúng tôi xem xét các benchmark hình ảnh-văn bản sau; VQA v2 [33], GQA [42] và COCO Caption [14]. Chúng tôi sử dụng Karpathy splits cho VQA v2 và COCO, trừ khi được chỉ định khác. Đối với các phần sau, chúng tôi sử dụng gready-decoding với beam search (số beam=1)

Baselines. Vì chúng tôi là những người đầu tiên đề xuất thiết lập này, để có so sánh công bằng, chúng tôi đã tái triển khai một số phương pháp hiện có và sử dụng cùng các mô hình thị giác (ViT-ImageNet) và ngôn ngữ (OPT) cho tất cả:
1) BPromptFuse ; tương đương với PromptFuse [60] và sử dụng Prompt Tuning (N=10). Chúng tôi thêm một chiếu tuyến tính cho token [CLS] cuối cùng. Token [CLS] được thêm tiền tố vào đầu vào của LM. Lưu ý rằng chúng tôi không thể tránh thêm một chiếu tuyến tính đã đào tạo vì có sự không khớp giữa các chiều của mô hình thị giác và ngôn ngữ.

2) BMAGMA ; tương đương với MAGMA [23] và sử dụng Adapters. Chúng tôi thêm tiền tố token [CLS] vào đầu vào của LM sau chiếu tuyến tính. Lưu ý rằng, chúng tôi chỉ xem xét token [CLS] vì chúng tôi thấy nó tốt hơn việc thêm tiền tố tất cả các token hình ảnh (eP-ALM∗MAGMA ). Chúng tôi cũng thấy rằng việc đào tạo ViT làm giảm hiệu suất, do đó chúng tôi giữ nó đóng băng có lợi cho phương pháp của họ.

3) BLimBEr ; tương đương với LimBEr [69] và chỉ đào tạo chiếu tuyến tính để chiếu các token thị giác và thêm tiền tố chúng vào văn bản đầu vào. Tương tự, chúng tôi chỉ xem xét token [CLS] vì nó cho độ chính xác tốt hơn.

So sánh với Các Công trình Khác. Dựa trên nghiên cứu của chúng tôi (Phần 4.2), chúng tôi sử dụng ViT-B/16 và OPT-2.7B trong mô hình chính của chúng tôi và trong việc tái tạo các phương pháp khác. Trong Bảng 2, chúng tôi so sánh với các công trình khác trên VQA v2, GQA, và COCO Caption. Chúng tôi vượt trội đáng kể so với các phương pháp khác với ít nhất +10 điểm trên VQA v2, +9 điểm trên GQA và chúng tôi tăng gấp đôi điểm trên COCO Caption. eP-ALM pt-L với OPT-6.7B và ViT-L cho điểm tốt nhất trong khi chỉ đào tạo 0.06% tham số mô hình.

Lưu ý rằng đối với COCO Caption, các công trình khác cho điểm rất thấp (do đó chúng tôi không báo cáo chúng).

Kết quả Few-shot: Các Mô hình Tiết kiệm Tham số cũng Tiết kiệm Dữ liệu? Trong phần này, chúng tôi điều tra mô hình của chúng tôi có thể tiết kiệm dữ liệu đến mức nào. Vì mục đích này, chúng tôi đào tạo trên một phần rất nhỏ (được lấy mẫu ngẫu nhiên) từ tập đào tạo VQA và đánh giá trên tập validation. Bảng 3, cho thấy sự vượt trội của phương pháp chúng tôi so với các baseline khác. Thú vị, chúng tôi có thể đạt 80% (41.9 vs 52.77) hiệu suất khi đào tạo trên 1% dữ liệu. Điều này xác nhận phương pháp trong các tình huống tài nguyên thấp và cho thấy rằng, ngoài việc tiết kiệm tham số, mô hình của chúng tôi cũng tiết kiệm dữ liệu.

Khái quát hóa Out of Distribution (OOD): Các Mô hình Tiết kiệm Tham số có Khái quát hóa Tốt hơn? Ở đây chúng tôi điều tra liệu phương pháp tiết kiệm tham số của chúng tôi có thể hoạt động tốt trong các tình huống OOD. Vì mục đích này, chúng tôi theo các phương pháp khác [1] và đào tạo mô hình của chúng tôi trên tập đào tạo của một benchmark nhất định, và đánh giá nó trên tập validation của benchmark khác, không có đào tạo trước đa phương thức. Chúng tôi đo khoảng cách hiệu suất, tức là sự khác biệt độ chính xác giữa một mô hình được đào tạo trên một benchmark khác và cùng mô hình được đào tạo trên benchmark đích. Bảng 4 cho thấy rằng eP-ALM, đào tạo 0.06% tổng tham số, rất cạnh tranh về độ chính xác OOD với các baseline khác, đào tạo tất cả tham số mô hình và đào tạo trước trên lượng dữ liệu lớn. Cụ thể, chúng tôi vượt trội hơn VILBERT trên VQAv2 hơn 2 điểm. Thú vị, khoảng cách OOD-IID cho eP-ALM, ít nhất thấp hơn 2 lần so với ALBEF [56] và VilBERT [65]. Điều này tiết lộ rằng phương pháp tiết kiệm tham số của chúng tôi khái quát hóa tương đối tốt trong các tình huống OOD.

4.1.3 Kết quả Video-Văn bản
Chúng tôi điều tra mức độ phương pháp của chúng tôi khái quát hóa cho các phương thức khác. Vì mục đích này, chúng tôi đánh giá eP-ALM cho Video QA trên MSRVTT-QA [97] và MSVD-QA [97] và cho Video Captioning trên MSR-VTT [98]. Để mã hóa video, chúng tôi sử dụng mô hình TimeSformer-base [6] được đào tạo trước trên Kinetics-600 [11]. Chúng tôi sử dụng 8 và 16 frame 224x224 cho VQA và captioning tương ứng.

So sánh với các công trình khác theo hiểu biết tốt nhất của chúng tôi, FrozenBiLM [101] là công trình tiết kiệm tham số duy nhất đề xuất thích ứng LMs cho các tác vụ video-ngôn ngữ. Nó sử dụng Adapters để thích ứng CLIP-ViT và Bidirectional LM bị đóng băng cho Video QA. Chúng tôi so sánh phương pháp của chúng tôi với việc tái triển khai baseline này; nơi chúng tôi chỉ đào tạo Adapters và lớp chiếu tuyến tính để chiếu token [CLS] cuối cùng và thêm tiền tố nó vào các token văn bản đầu vào. Kết quả trong Bảng 5 cho thấy rằng eP-ALM vượt trội hơn baseline này một cách đáng kể. Lý do tại sao cái sau không cho kết quả tốt có thể do việc thêm tiền tố các token thị giác vào đầu vào của OPT. Chúng tôi có thể giảm số lượng tham số và làm giảm hiệu suất một chút bằng cách sử dụng kết nối tuyến tính chia sẻ (eP-ALM vs eP-ALM pt).

Kết quả Zero-Shot Để khám phá khả năng khái quát hóa của phương pháp chúng tôi, chúng tôi đánh giá trên Zero-Shot cho VideoQA, nơi mô hình được đào tạo trên một tập dữ liệu khác với tập đích. Bảng 6 cho thấy so sánh với các phương pháp khác. eP-ALM, được đào tạo trên VQA v2 (standard split), vượt trội hơn các phương pháp khác được đào tạo trên dữ liệu nhiều hơn đáng kể. Cụ thể, eP-ALM vượt trội hơn Flamingo-3B [3] trên MSRVTT-QA hơn 2 điểm, và đạt gấp đôi điểm của FrozenBiLM [101]. Trái với một số phương pháp khác đưa tác vụ thành phân loại (dựa trên tương đồng) [100] hoặc sinh có ràng buộc thông qua masking, chỉ xem xét một tập con câu trả lời (1k hoặc 2k) [57, 101, 105], phương pháp của chúng tôi được đánh giá (với so sánh từng ký tự với ground-truth) với Sinh Mở không ràng buộc (OE Gen) và có thể sinh ra câu trả lời với độ dài tùy ý. Điều này thử thách hơn và không có lợi cho phương pháp của chúng tôi.

4.1.4 Kết quả Âm thanh-Văn bản
Chúng tôi điều tra khả năng khái quát hóa của phương pháp chúng tôi cho miền âm thanh. Bộ mã hóa là mô hình AST-base [31] được đào tạo trước để phân loại trên AudioSet [30]. Chúng tôi đánh giá trên tập dữ liệu AudioCaps [48], benchmark lớn nhất cho Audio Captioning. Chúng tôi đào tạo với mel spectrogram của 128 bins và frequency và time masking với batch size 8.

Theo hiểu biết tốt nhất của chúng tôi, không có công trình trước đó đã được đề xuất để thích ứng hiệu quả LM cho các tác vụ âm thanh-văn bản, do đó chúng tôi so sánh với SoTA đào tạo end-to-end khác chỉ lấy tín hiệu âm thanh làm đầu vào. Bảng 7 cho thấy rằng phương pháp của chúng tôi rất cạnh tranh với các công trình trước, cho thấy tiềm năng thích ứng hiệu quả LM cho phương thức âm thanh.

4.2. Nghiên cứu Ablation
Trong phần này, chúng tôi ablate các thành phần khác nhau của công trình chúng tôi.

Trong phần sau, chúng tôi chạy một số ablation cho các tác vụ Hình ảnh-Văn bản, chủ yếu trên VQA v2.

So sánh với các biến thể và baseline khác nhau. Chúng tôi bắt đầu bằng cách so sánh các biến thể khác nhau với các công trình khác trong Hình 3. Tất cả các mô hình sử dụng OPT-350M và ViT-B/16. Các phương pháp khác tụt hậu đáng kể so với mô hình của chúng tôi. B MAGMA cho kết quả tốt nhất (23.3% acc.) trong số họ, tiếp theo là B PromptFuse (18.82% acc.) và cuối cùng là B LimBEr (10.75 % acc.). Chúng tôi cũng so sánh với một baseline MAGMA khác (BMAGMA∗) thêm tiền tố tất cả các token thị giác vào đầu vào, và chúng tôi thấy sự suy giảm đáng kể so với việc chỉ truyền token [CLS]. Điều này tiết lộ rằng việc thêm tiền tố tất cả các token thị giác trực tiếp vào đầu vào cản trở việc thích ứng.

Chúng tôi có thể nhận thấy sự cải thiện nhất quán của eP-ALM khi thêm nhiều tham số có thể đào tạo. Mô hình tiết kiệm tham số nhất là eP-ALM lin có 30.72%, trong khi mô hình tốt nhất có 34.34% (với Adapters eP-ALM ada). Thú vị, eP-ALM lin chỉ với một lớp tuyến tính thành công đạt hiệu suất tốt trên thiết lập thử thách này, tiết lộ rằng không gian biểu diễn ngôn ngữ và thị giác không quá xa. Các kỹ thuật tiết kiệm tham số khác như Prompt Tuning có thể giúp đạt thêm điểm (30.72 với eP-ALM lin vs 31.27 với eP-ALM pt). Hơn nữa, sử dụng các lớp khác nhau cho mỗi token [CLS] được chèn dường như cải thiện đáng kể (31.27 với eP-ALM pt vs 33.08 với eP-ALM).

Cuối cùng, chúng tôi chỉ ra rằng eP-ALM vượt qua baseline "tinh chỉnh đầy đủ" (đường xám) tinh chỉnh tất cả tham số 1.27 điểm (31.79 vs 33.08). Điều này tiết lộ rằng đào tạo tất cả trọng số của các mô hình được đào tạo trước trên các tập dữ liệu nhỏ có thể giảm khả năng khái quát hóa của chúng và làm giảm hiệu suất.

Như một sự đánh đổi giữa hiệu suất và hiệu quả, chúng tôi ưu tiên eP-ALM mà chúng tôi tiếp tục cho nghiên cứu tiếp theo.

Mức Trích xuất và Chèn của token [CLS]. Ở đây chúng tôi điều tra token [CLS] nào để trích xuất từ ViT và vị trí tốt nhất để chèn chúng vào bên trong mô hình OPT. Bảng 8 cho thấy rằng trích xuất các token [CLS] cuối cùng (từ 6 lớp cuối) tốt hơn so với chỉ sử dụng cái cuối cùng, như được thực hiện trong các phương pháp khác (Acc 30.53 vs 33.08). Ngoài ra, sử dụng tất cả các token [CLS] dường như làm giảm hiệu suất. Hơn nữa, thêm tiền tố các token [CLS] vào tất cả các lớp OPT làm giảm nhẹ (33.08 vs 32.15), và thêm tiền tố vào đầu vào của OPT cho kết quả tồi tệ nhất. Điều này có thể chỉ ra rằng việc hợp nhất các token thị giác và văn bản ở sâu trong mô hình dễ dàng hơn, nơi các biểu diễn trừu tượng hơn, so với các lớp đầu tiên nơi chúng ta có các tính năng cụ thể cho phương thức hơn và sự không khớp biểu diễn cao hơn.

Mở rộng LM. Một câu hỏi thú vị mà chúng tôi điều tra là tác động của việc mở rộng các tham số của mô hình ngôn ngữ đối với phương pháp của chúng tôi. Lý tưởng nhất, chúng tôi muốn có một phương pháp khai thác hiệu quả LLMs cho các tác vụ và phương thức khác, mà không cần truy cập vào tài nguyên tính toán khổng lồ. Trong Bảng 4, chúng tôi chỉ ra rằng điểm tăng với kích thước mô hình với bước nhảy lớn nhất là giữa OPT-350M (33.08 vs 37.29) và OPT-1.3B (∼ ×4 kích thước mô hình). Sự cải thiện nhất quán với quy mô cho thấy hiệu quả của phương pháp khi xem xét các mô hình rất lớn.

Như một sự đánh đổi giữa hiệu suất và kích thước mô hình, chúng tôi ưu tiên OPT-2.7B và sử dụng nó cho tất cả các thử nghiệm khác.

Mở rộng Mô hình Thị giác. Chúng tôi cũng nghiên cứu mô hình hoạt động như thế nào khi mở rộng bộ mã hóa thị giác. Trong Hình 5, chúng tôi có thể nhận thấy rằng điểm tăng với kích thước của ViT. Tốt nhất là ViT-L/16 (41.36) và tồi tệ nhất là ViT-S/16 (Acc 38.73). Tuy nhiên, độ phân giải ViT hoặc số lượng patch/token hình ảnh dường như không có tác động đáng kể đến hiệu suất cuối cùng sau độ phân giải 16.

Mở rộng Tính toán. Bảng 9 cho thấy rằng phương pháp của chúng tôi mở rộng với tính toán, vì đào tạo nhiều epoch hơn dẫn đến tăng 4 điểm trong độ chính xác VQA. Thú vị với OPT-6.7B và ViT-L (eP-ALM pt-L), chúng tôi đạt điểm 43.6 bằng cách chỉ đào tạo 0.06% tham số mô hình (∼4M params).

Kết quả Định tính. Chúng tôi cho thấy một số kết quả định tính của mô hình eP-ALM với OPT-2.7B trong Hình 6. Đối với VQA, chúng tôi có thể nhận thấy rằng mô hình của chúng tôi có thể trả lời đúng các câu hỏi. Hơn nữa, một số câu trả lời phong phú và chính xác hơn so với ground truth được gắn nhãn thủ công trong tập dữ liệu. Điều này cũng tiết lộ rằng giao thức đánh giá khớp chính xác không có lợi cho việc sinh mở được tạo ra bởi mô hình của chúng tôi. Thú vị, có vẻ như mô hình đã học được phong cách trả lời trong tập đào tạo (tức là, câu trả lời ngắn và súc tích). Đối với Captioning, mô hình có thể sinh ra các câu mạch lạc mô tả hình ảnh tổng thể. Tuy nhiên, nó vẫn bỏ lỡ một số chi tiết trong hình ảnh.

5. Kết luận
Trong công trình này, chúng tôi đề xuất một thiết lập thử thách mới để thích ứng hiệu quả các mô hình đơn phương thức cho các tác vụ đa phương thức, tập trung vào việc tăng cường các LMs hiện có với nhận thức. Không có đào tạo trước đa phương thức, và với gần 4M tham số có thể đào tạo bao gồm một kết nối tuyến tính và một Soft Prompt, chúng tôi có thể thích ứng một mô hình 7B bị đóng băng và đạt độ chính xác 54.5% trên VQA v2, với sinh mở không ràng buộc. Chúng tôi xác nhận hiệu quả của phương pháp với các phương thức Hình ảnh, Video và Âm thanh. Thiết lập tinh chỉnh trực tiếp này có một số lợi thế; (a) hiệu quả dữ liệu/tính toán đào tạo, (b) đạt hiệu suất nói chung cao hơn so với thiết lập pretrain-zeroshot, (c) dễ dàng thích ứng với các tác vụ, phương thức hoặc LLMs mới, nơi không cần đào tạo trước tốn kém. Tuy nhiên, cơ chế được đề xuất trong eP-ALM có thể được thích ứng một cách đơn giản cho thiết lập này.

Mặc dù kết quả vẫn còn xa so với các phương pháp hiện đại đào tạo hầu hết tham số mô hình trên nhiều dữ liệu hơn, tỷ lệ phần trăm cực nhỏ của tham số có thể đào tạo (0.06%) và điểm tăng với kích thước mô hình và tính toán làm cho công trình này hứa hẹn hướng tới việc tìm điểm trung gian, giữa các phương pháp cực kỳ hiệu quả và cực kỳ không hiệu quả, hy vọng gần hơn với cái trước.

Phương pháp có một số hạn chế, mà chúng tôi minh họa trong phụ lục. Nói chung, mô hình gặp khó khăn trong việc nắm bắt các chi tiết tinh tế trong hình ảnh, ưu tiên sinh mạch lạc hơn là sinh thực tế, có thể ảo giác một số đối tượng không có trong hình ảnh, và thiếu lý luận thông thường. Phương pháp của chúng tôi kế thừa hầu hết các hạn chế và thiên lệch của các mô hình được đào tạo trước, đặc biệt là LM, và chỉ đào tạo một vài tham số thích ứng dường như không tránh được việc chuyển giao những thiên lệch này. Cuối cùng, mô hình được đào tạo với dự đoán token tiếp theo và có thể tạo ra văn bản mạch lạc, tuy nhiên, vẫn không rõ paradigm này có thể dẫn đến khả năng lý luận thực sự như thế nào.

6. Lời cảm ơn
Công trình này được hỗ trợ một phần bởi ANR grant VISA DEEP (ANR-20-CHIA-0022), và tài nguyên HPC của IDRIS dưới phân bổ 2022-[AD011013415] và 2023-[AD011013415R1] được thực hiện bởi GENCI. Các tác giả muốn cảm ơn Theophane Vallaeys cho cuộc thảo luận hữu ích.

Tài liệu tham khảo
[1]Aishwarya Agrawal, Ivana Kaji ´c, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, and Aida Nematzadeh. Rethinking evaluation practices in visual question answering: A case study on out-of-distribution generalization. arXiv preprint arXiv:2205.12191 , 2022. 6

[2]Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE International Conference on Computer Vision , pages 8948–8957, 2019. 15

[3]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022. 2, 6, 7

[4]Ankur Bapna and Orhan Firat. Simple, scalable adaptation for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 1538–1548, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. 3

[5]Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media , volume 14, pages 830–839, 2020. 3

[6]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? InICML , volume 2, page 4, 2021. 2, 3, 6, 16

[7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. 1, 2

[8]Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome. Murel: Multimodal relational reasoning for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 1989–1998, 2019. 2

[9]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 9650–9660, 2021. 2

[10] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340 , 2018. 3

[11] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987 , 2019. 6

[12] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18030–18040, 2022. 3

[13] Shoufa Chen, Chongjian GE, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems , 2022. 3

[14] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 , 2015. 2, 5

[15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 , 2022. 1, 2, 6

[16] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision , pages 104–120. Springer, 2020. 2

[17] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu: A recipe for effective video-and-language pretraining. arXiv preprint arXiv:2212.05051 , 2022. 3

[18] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning , pages 1931–1942. PMLR, 2021. 6

[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 1, 2

[20] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442 , 2023. 1

[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations , 2021. 1, 3

[22] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning , pages 5547–5569. PMLR, 2022. 3

[23] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma–multimodal augmentation of generative models through adapter-based finetuning. arXiv preprint arXiv:2112.05253 , 2021. 2, 4, 5, 15, 16

[24] Ays ¸eg¨ul¨Ozkaya Eren and Mustafa Sert. Audio captioning based on combined audio and semantic embeddings. In 2020 IEEE International Symposium on Multimedia (ISM) , pages 41–48, 2020. 7

[25] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833 , 2018.

[26] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Doll ´ar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, et al. From captions to visual concepts and back. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1473–1482, 2015. 2

[27] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022. 1

[28] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681 , 2021. 2

[29] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020. 3

[30] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017 , New Orleans, LA, 2017. 3, 7

[31] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021 , pages 571–575, 2021. 2, 3, 7, 17

[32] F´elix Gontier, Romain Serizel, and Christophe Cerisara. Automated audio captioning by fine-tuning bart with audioset tags. In DCASE 2021 - 6th Workshop on Detection and Classification of Acoustic Scenes and Events , Virtual, Spain, Nov. 2021. 6, 7

[33] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017. 2, 5

[34] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems , 33:21271–21284, 2020. 2

[35] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336 , 2022. 2

[36] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9729–9738, 2020. 2

[37] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations , 2021. 2

[38] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556 , 2022. 2, 3

[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pages 2790–2799. PMLR, 2019. 3, 4

[40] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. 3

[41] Ronghang Hu, Anna Rohrbach, Trevor Darrell, and Kate Saenko. Language-conditioned graph networks for relational reasoning. In Proceedings of the IEEE/CVF international conference on computer vision , pages 10294–10303, 2019. 2

[42] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700–6709, 2019. 5

[43] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII , page 709–727, Berlin, Heidelberg, 2022. Springer-Verlag. 3

[44] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10267–10276, 2020. 2

[45] Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2763–2775, Dublin, Ireland, May 2022. Association for Computational Linguistics. 6

[46] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems , 34:1022–1035, 2021. 3

[47] Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, and Yu-Wing Tai. Reflective decoding network for image captioning. In Proceedings of the IEEE/CVF international conference on computer vision , pages 8888–8897, 2019. 2

[48] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT , 2019. 2, 7, 16

[49] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 119–132, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. 7

[50] Ju-ho Kim, Jungwoo Heo, Hyun-seo Shin, Chan-yeong Lim, and Ha-Jin Yu. Integrated parameter-efficient tuning for general-purpose audio models. arXiv preprint arXiv:2211.02227 , 2022. 3, 16

[51] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583–5594. PMLR, 2021. 2

[52] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823 , 2023. 16

[53] Yuma Koizumi, Yasunori Ohishi, Daisuke Niizumi, Daiki Takeuchi, and Masahiro Yasuda. Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval. arXiv preprint arXiv:2012.07331 , 2020. 7

[54] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, 2021. 3, 4

[55] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086 , 2022. 2

[56] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems , 34, 2021. 1, 2, 6

[57] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160 , 2022. 2, 6, 7

[58] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event structures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16420–16429, 2022. 2

[59] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In CVPR , 2022. 16

[60] Sheng Liang, Mengjie Zhao, and Hinrich Sch ¨utze. Modular and parameter-efficient multimodal fusion with prompting. InFindings of the Association for Computational Linguistics: ACL 2022 , pages 2976–2985, 2022. 2, 3, 5, 6, 14

[61] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740–755. Springer, 2014. 15

[62] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638 , 2022. 3

[63] Xubo Liu, Xinhao Mei, Qiushi Huang, Jianyuan Sun, Jinzheng Zhao, Haohe Liu, Mark D Plumbley, V olkan Kilic, and Wenwu Wang. Leveraging pre-trained bert for audio captioning. In 2022 30th European Signal Processing Conference (EUSIPCO) , pages 1145–1149. IEEE, 2022. 7

[64] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 3202–3211, 2022. 16

[65] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems , 32, 2019. 2, 6

[66] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916 , 2022. 2

[67] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 565–576, 2021. 3

[68] XINHAO MEI, XUBO LIU, QIUSHI HUANG, MARK DA VID PLUMBLEY , and WENWU WANG. Audio captioning transformer. In Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2021) . 7

[69] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162 , 2022. 2, 4, 5, 15, 16

[70] Gr´egoire Mialon, Roberto Dess `ı, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi `ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842 , 2023. 2

[71] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, and Edward Choi. Multi-modal understanding and generation for medical images and text via vision-language pre-training. IEEE Journal of Biomedical and Health Informatics , 26(12):6070–6080, 2022. 2

[72] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning for action recognition. arXiv preprint arXiv:2206.13559 , 2022. 3

[73] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. Proc. Interspeech 2019 , pages 2613–2617, 2019. 17

[74] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations , 2022. 2

[75] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision , 115(3):211–252, 2015. 3

[76] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagn ´e, Alexandra Sasha Luccioni, Fran c ¸ois Yvon, Matthias Gall ´e, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022. 1, 2

[77] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 17959–17968, 2022. 6

[78] Erica K Shimomoto, Edison Marrese-Taylor, Hiroya Takamura, Ichiro Kobayashi, Hideki Nakayama, and Yusuke Miyao. Towards parameter-efficient integration of pre-trained language models in temporal video grounding. arXiv preprint arXiv:2209.13359 , 2022. 3

[79] Mustafa Shukor, Guillaume Couairon, and Matthieu Cord. Efficient vision-language pretraining with visual concepts and hierarchical alignment. In 33rd British Machine Vision Conference (BMVC) , 2022. 1, 2, 3

[80] Mustafa Shukor, Nicolas Thome, and Matthieu Cord. Structured vision-language pretraining for computational cooking. arXiv preprint arXiv:2212.04267 , 2022. 2

[81] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15638–15650, 2022. 1

[82] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302 , 2022. 2

[83] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 , 2022. 1, 2

[84] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri `a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 2022. 2

[85] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. In Advances in Neural Information Processing Systems . 3

[86] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5227–5237, 2022. 3, 14

[87] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? Advances in Neural Information Processing Systems , 33:6827–6839, 2020. 2

[88] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773 , 2022. 2

[89] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning , pages 10347–10357. PMLR, 2021. 3

[90] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems , 34:200–212, 2021. 2, 4

[91] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021. 3

[92] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. arXiv preprint arXiv:2209.07526 , 2022. 2

[93] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052 , 2022. 2, 6

[94] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 , 2022. 1, 2

[95] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 , 2022. 2

[96] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3733–3742, 2018. 2

[97] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM International Conference on Multimedia , MM '17, page 1645–1653, New York, NY , USA, 2017. Association for Computing Machinery. 2, 6

[98] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 6

[99] Xuenan Xu, Heinrich Dinkel, Mengyue Wu, Zeyu Xie, and Kai Yu. Investigating local and global information for automated audio captioning with transfer learning. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 905–909. IEEE, 2021. 7

[100] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1686–1697, 2021. 7

[101] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. In NeurIPS 2022-36th Conference on Neural Information Processing Systems , 2022. 3, 6, 7

[102] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 3081–3089, 2022. 2

[103] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917 , 2022. 2

[104] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1–9, 2022. 3

[105] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16375–16387, 2022. 7

[106] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598 , 2022. 2

[107] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 1, 2, 3

[108] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16816–16825, 2022. 3

Phụ lục
Phụ lục được tổ chức như sau; trong Phần A, chúng tôi đưa ra thêm chi tiết triển khai về các thử nghiệm mà chúng tôi tiến hành. Chúng tôi minh họa và giải thích các biến thể khác nhau của eP-ALM trong Phần B. Chúng tôi so sánh eP-ALM với các phương pháp khác theo thiết lập pretrain-zeroshot (Phần C). Sau đó chúng tôi trình bày thêm các nghiên cứu ablation về các tác vụ hình ảnh-văn bản và video-văn bản trong Phần D. Cuối cùng trong Phần E chúng tôi chỉ ra một số kết quả định tính và thảo luận về hạn chế của phương pháp được đề xuất.

A. Chi tiết Triển khai
Chúng tôi sử dụng OPT-2.7B trong mô hình cuối cùng của chúng tôi. Chúng tôi trích xuất các token [CLS] của 6 lớp cuối của các bộ mã hóa nhận thức và thêm tiền tố chúng, sau khi chiếu tuyến tính, vào các token văn bản của 12 lớp cuối của OPT. Lưu ý rằng chúng tôi thay thế [CLS] trước đó bằng cái mới để giữ cùng số lượng token.

Chúng tôi tinh chỉnh với loss cross-entropy cổ điển được sử dụng để đào tạo OPT gốc cho VQA và Captioning. Chúng tôi sử dụng optimizer AdamW với learning rate 1e-5 được warm up đến 2e-5 sau đó giảm xuống 1e-6 sử dụng cosine scheduler. Đối với Adapters, chúng tôi sử dụng Adapters tuần tự sau self-attentions và feedforward layers với hệ số downsampling 8 và ReLU activation. Đối với Soft Prompt, chúng tôi triển khai nó như một lớp embedding tuyến tính nhận số từ 0 đến độ dài của prompt (ở đây là 10). Chúng tôi cũng thử nghiệm thêm một MLP sau các prompt như được thực hiện với các phương pháp khác [86]. Chúng tôi sử dụng prompt với MLP cho hầu hết các thử nghiệm vì chúng tôi thấy rằng nó cho kết quả tốt hơn một chút. Soft prompt và adapters được đào tạo với lr cố định 1e-5. eP-ALM pt-L được đào tạo với prompt nhẹ (chỉ các token có thể đào tạo không có MLP), learning rate bắt đầu 2e-4 và learning rate cố định 1e-3 cho prompt với tổng batch size 16.

VQA/GQA: chúng tôi sử dụng một token đặc biệt cho VQA (′< /a >′) để phân tách câu hỏi khỏi câu trả lời. Chúng tôi đào tạo trong 8 epoch với batch size 64 (128 cho GQA) và độ phân giải hình ảnh 224. Đào tạo phương pháp của chúng tôi với OPT-2.7B cho VQA v2 có thể được thực hiện trên một GPU V100 32GB trong 1.8 ngày (vì bộ mã hóa nhận thức bị đóng băng, việc lưu các token đầu ra của nó có thể tiết kiệm rất nhiều thời gian đào tạo). Đối với các thử nghiệm Few-shot, chúng tôi đào tạo lâu hơn (trong 64 epoch) với learning rate bắt đầu cao hơn (1e-4 được warm up đến 2e-4 và giảm xuống 1e-5). Những cái được đánh dấu bằng ∗ được đào tạo trong 100 epoch như trong PromptFuse [60].

Image Captioning chúng tôi đào tạo trong 8 epoch với batch size 64 và độ phân giải hình ảnh 224.

Video QA: chúng tôi lấy mẫu ngẫu nhiên 8 frame có độ phân giải 224x224 cho mỗi video và đào tạo trong 25 epoch với batch size 32. Đối với các thử nghiệm Zero-Shot, chúng tôi chỉ đào tạo trong 4 epoch với learning rate bắt đầu 1e-4. Chúng tôi chỉ sử dụng spatial self-attention của TimeSformer để đào tạo trên VQA v2.

Video Captioning: chúng tôi lấy mẫu ngẫu nhiên 16 frame có độ phân giải 224x224 cho mỗi video và đào tạo trong 25 epoch với batch size 64.

Audio Captioning chúng tôi đào tạo trong 30 epoch với frequency và time masking tương ứng 24 và 96. Mel bins là 128 và độ dài âm thanh là 1024. Batch size 32. Đối với Deep Prompt, chúng tôi chèn các soft prompt mới vào tất cả 32 block của OPT (mỗi cái có độ dài 10).

B. Các biến thể eP-ALM
Chúng tôi chi tiết các biến thể khác nhau được đề xuất trong bài báo này (ở đây chúng tôi xem xét ViT-B/16 và OPT-350M cho đơn giản). Những biến thể này được minh họa trong Hình 7:

eP-ALM lin: chúng tôi trích xuất các token [CLS] từ 6 lớp cuối của ViT bị đóng băng và chèn chúng vào 12 lớp cuối của OPT bị đóng băng. Để giảm chi phí suy luận, đối với mỗi cặp lớp (ở đây là 2), chúng tôi thay thế [CLS] trước đó bằng cái mới (do đó chỉ tăng số lượng token lên 1 cho toàn bộ quá trình). Tất cả các token [CLS] thị giác được chiếu bởi một lớp chiếu tuyến tính có thể đào tạo (chia sẻ) để phù hợp với chiều của chúng với chiều của OPT.

eP-ALM pt: chúng tôi tăng cường eP-ALM lin với Prompt Tuning, bao gồm việc thêm tiền tố các token có thể đào tạo (tức là, soft prompt) vào đầu vào của LM. Điều này có thể giúp mô hình thích ứng tốt với tác vụ mới bằng cách cung cấp bối cảnh cho đầu vào văn bản. Vì lý do hiệu quả, chúng tôi chỉ thêm tiền tố 10 token có thể học.

eP-ALM: trong khi một chiếu tuyến tính hấp dẫn, nó có thể không thể nắm bắt tất cả đặc thù của các token [CLS] khác nhau. Để khắc phục điều này, chúng tôi sử dụng các chiếu khác nhau cho mỗi [CLS], trong khi giữ soft prompt.

eP-ALM ada: một thay thế khác cho Prompt Tuning là Adapters. Chúng tôi theo các phương pháp khác [23] và thêm tuần tự một module adapter (downsample, activation sau đó upsample) sau self-attention và feedforward layers trong tất cả các block của OPT. Trong khi điều này có thể cho kết quả tốt hơn, nó thêm một số lượng đáng kể tham số có thể đào tạo.

C. Thiết lập Pretrain-Zeroshot
Trọng tâm của công trình này là tinh chỉnh trực tiếp, nơi chúng tôi đề xuất một cơ chế tương tác cross-modal hiệu quả với chế độ dữ liệu thấp. Tuy nhiên, cơ chế được đề xuất có thể được thích ứng một cách đơn giản cho thiết lập đánh giá pretrain-zeroshot. Trong phần này, chúng tôi chỉ ra hiệu quả của eP-ALM với đánh giá zero-shot sau khi đào tạo trước trên CC3M. Cụ thể, chúng tôi đào tạo trước eP-ALM pt−L trên CC3M trong 4 epoch (mất 35 giờ trên 2 gpu V100 32GB), và đánh giá trên các tập dữ liệu COCO [61] và NoCaps (all) [2]. Chúng tôi thử nghiệm với ViT-L, được khởi tạo từ ImageNet và CLIP.

Bảng 10 cho thấy so sánh với các phương pháp khác. Không sử dụng bộ mã hóa CLIP, eP-ALM vượt trội đáng kể so với các công trình khác trên cả hai tập dữ liệu. Sử dụng CLIP (được đào tạo để tạo ra các tính năng thị giác được căn chỉnh với văn bản) làm giảm khoảng cách cải thiện, nơi eP-ALM vẫn vượt trội hơn tất cả các baseline trên các chỉ số B@4 và METEOR. Điều này xác nhận rằng trong trường hợp bộ mã hóa thị giác không được căn chỉnh (ví dụ: được đào tạo trước trên ImageNet) cơ chế tương tác cross-modal của chúng tôi hiệu quả để căn chỉnh cả hai phương thức.

Lưu ý rằng, eP-ALM hiệu quả hơn đáng kể so với LimBEr và MAGMA, vì họ đào tạo nhiều tham số hơn trong thời gian rất dài (∼>670 GPUhs). Vì MAGMA được đào tạo trên nhiều dữ liệu hơn, chúng tôi so sánh với MAGMA được đào tạo trên CC3M thu được từ [69].

D. Nghiên cứu Ablation
Ở đây chúng tôi trình bày một nghiên cứu ablation bổ sung.

D.1. Hình ảnh-Văn bản
Đào tạo Tất cả Tham số Ở đây chúng tôi điều tra có thể đạt được bao nhiều lợi ích bằng cách giải phóng các mô hình được đào tạo trước. Chúng tôi thử nghiệm trên VQA v2 với eP-ALM. Bảng 11 cho thấy rằng tinh chỉnh các mô hình được đào tạo trước trong eP-ALM của chúng tôi mang lại cải thiện nhẹ, mặc dù có số lượng lớn tham số có thể đào tạo. Lưu ý rằng, chúng tôi thấy rằng việc sử dụng learning rate rất nhỏ (lr=1e-7) là lựa chọn duy nhất (trong khi giữ lr 1e-5 cho các connector) để giải phóng các mô hình này mà không có sự suy giảm đáng kể.

D.2. Video-Văn bản
Video Encoder: ở đây chúng tôi so sánh các bộ mã hóa khác nhau để xử lý video. Chúng tôi so sánh TimeSformer [6] có cả spatial và temporal attention và được đào tạo để phân loại video với một baseline đơn giản, ViT được đào tạo trên ImageNet, bỏ qua động lực thời gian. Đối với ViT, chúng tôi lấy trung bình của các token [CLS] của các frame được xử lý trong khi đối với TimeSformer chúng tôi xem xét token [CLS] đơn. Bảng 12 cho thấy rằng sử dụng các bộ mã hóa cụ thể cho video cho kết quả tốt hơn đáng kể cho video captioning. Ngoài ra, chúng tôi thấy rằng sử dụng 16 frame thay vì 8 mang lại cải thiện nhẹ.

Mức chèn và trích xuất của các token [CLS]: ở đây chúng tôi chỉ ra tầm quan trọng của việc tận dụng biểu diễn phân cấp trong cả bộ mã hóa video và mô hình ngôn ngữ. Bảng 13 cho thấy kết quả trên MSVD-QA. Chúng tôi chỉ ra rằng giữ tương tác giữa các token cross-modal với các lớp cuối (lớp 19 đến 31) của OPT dẫn đến kết quả tốt hơn đáng kể. Trích xuất một số token từ các token khác nhau của TimeSformer mang lại cải thiện nhẹ. Tuy nhiên, sử dụng các video transformer phân cấp [59, 64] có thể dẫn đến kết quả tốt hơn. Chúng tôi cũng nhận thấy rằng Adapters nói chung cho kết quả tốt hơn Prompt Tuning, điều này có thể là do khi đào tạo trên video chúng tôi lấy mẫu ngẫu nhiên một số frame, ngăn cản mô hình overfit trong trường hợp các tập dữ liệu nhỏ.

D.3. Âm thanh-Văn bản
So sánh với các biến thể khác nhau. Ở đây chúng tôi so sánh các biến thể khác nhau của phương pháp chúng tôi với các baseline khác nhau cho audio captioning. Chúng tôi đánh giá trên tập dữ liệu AudioCaps [48], benchmark lớn nhất cho Audio Captioning. Chúng tôi đào tạo với mel spectrogram của 128 bins và frequency và time masking với batch size 8.

Theo hiểu biết tốt nhất của chúng tôi, không có công trình trước đó đã được đề xuất để thích ứng LM cho các tác vụ âm thanh-văn bản. Tuy nhiên, có một số công trình gần đây thích ứng các mô hình âm thanh sử dụng các kỹ thuật tiết kiệm tham số, như Deep Prompts và Adapters [50]. Bảng 14 cho thấy so sánh với các phương pháp khác nhau. Chúng tôi thấy rằng việc thêm tiền tố các token âm thanh vào đầu vào của OPT không cho hiệu suất hợp lý. Để điều tra điều này thêm, chúng tôi đào tạo một baseline khác nơi các token âm thanh được nối trong 12 lớp cuối của OPT (eP-ALM l19−31+Adapter và eP-ALM l19−31+DeepPT ). Điều này dẫn đến cải thiện đáng kể.

Time và Frequency Masking: theo các phương pháp khác [31, 73] chúng tôi đào tạo eP-ALM với time và frequency masking trên AudioCaps. Bảng 15 cho thấy rằng masking giúp ích đáng kể, tuy nhiên, sử dụng quá nhiều masking làm tổn hại hiệu suất.

E. Hạn chế
Mặc dù chúng tôi chỉ ra kết quả hấp dẫn cho đào tạo rất hiệu quả, phương pháp có một số hạn chế, mà chúng tôi minh họa trong Hình 8. Đối với VQA, chúng tôi có thể nhận thấy rằng mô hình không thể nắm bắt các chi tiết tinh tế trong hình ảnh (ví dụ: số lượng màu sắc và con ngựa vằn trong 2 ví dụ đầu tiên), điều này có thể do hạn chế tương tác với mô hình thị giác thông qua các token [CLS], thường nắm bắt thông tin tổng thể về hình ảnh. Trong trường hợp các câu hỏi khó, mô hình ưu tiên sinh mạch lạc của một câu hỏi có liên quan theo sau là câu trả lời đúng của nó, thay vì trả lời câu hỏi chính ("A: màu gì của điện thoại?? đen" trong ví dụ 3).

Đối với Captioning, mô hình dường như ưu tiên xuất ra một câu mạch lạc, mặc dù nó không hoàn toàn đúng ("nhiều" bò trong một thành phố "đông đúc"). Thứ hai, mô hình có thể ảo giác một số đối tượng không xuất hiện trong hình ảnh ("táo" trong ví dụ 2). Cuối cùng, mô hình thiếu lý luận thông thường, khiến nó không thể hiểu rằng voi không nhỏ, và việc xa camera không thay đổi thực tế này (ví dụ 3).

Phương pháp của chúng tôi kế thừa hầu hết các hạn chế và thiên lệch của các mô hình được đào tạo trước, đặc biệt là LM, và chỉ đào tạo một vài tham số thích ứng dường như không tránh được việc chuyển giao những thiên lệch này. Cuối cùng, mô hình được đào tạo với dự đoán token tiếp theo và có thể tạo ra văn bản mạch lạc, tuy nhiên, vẫn không rõ paradigm này có thể dẫn đến khả năng lý luận thực sự như thế nào.
