# 2305.15265.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.15265.pdf
# File size: 1930029 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Winner-Take-All Column Row Sampling for Memory Efficient
Adaptation of Language Model
Zirui Liu∗1, Guanchu Wang∗1, Shaochen Zhong1, Zhaozhuo Xu1, Daochen Zha1, Ruixiang
Tang1, Zhimeng Jiang2, Kaixiong Zhou1, Vipin Chaudhary3, Shuai Xu3, and Xia Hu1
1Department of Computer Science, Rice University
2Department of Computer Science, Texas A&M University
3Department of Computer and Data Sciences, Case Western Reserve University
{Zirui.Liu, Guanchu.Wang, Shaochen.Zhong, Zhaozhuo.Xu, Daochen.Zha,
Ruixiang.Tang, Kaixiong.Zhou, Xia.Hu}@rice.edu ,zhimengj@tamu.edu ,{vxc204,
sxx214}@case.edu
Abstract
With the rapid growth in model size, fine-tuning the large pre-trained language model has
become increasingly difficult due to its extensive memory usage. Previous works usually focus
on reducing the number of trainable parameters in the network. While the model parameters
do contribute to memory usage, the primary memory bottleneck during training arises from
storing feature maps, also known as activations, as they are crucial for gradient calculation.
Notably, neural networks are usually trained using stochastic gradient descent. We argue that
in stochastic optimization, models can handle noisy gradients as long as the gradient estimator
is unbiased with reasonable variance. Following this motivation, we propose a new family of
unbiased estimators called WTA-CRS , for matrix production with reduced variance, which only
requires storing the sub-sampled activations for calculating the gradient. Our work provides
both theoretical and experimental evidence that, in the context of tuning transformers, our
proposed estimators exhibit lower variance compared to existing ones. By replacing the linear
operation with our approximated one in transformers, we can achieve up to 2.7 ×peak memory
reduction with almost no accuracy drop and enables up to 6.4×larger batch size. Under
the same hardware, WTA-CRS enables better down-streaming task performance by applying
larger models and/or faster training speed with larger batch sizes. The code is available at
https://github.com/zirui-ray-liu/WTACRS/ .
1 Introduction
Pre-trained language models (LMs) with transformer architecture have achieved remarkable success
in numerous natural language processing (NLP) tasks (Vaswani et al., 2017; Devlin et al., 2018;
Chuang et al., 2023; Raffel et al., 2020b; Brown et al., 2020; Yang et al., 2023; Zha et al., 2023).
Specifically, these models are trained on vast text corpora to acquire general-purpose representations,
which are then adapted to a specific task by fine-tuning on task-specific data. In recent studies,
it has been convincingly demonstrated that significantly increasing the number of parameters in
pre-trained LMs leads to remarkable improvements in performance (Kaplan et al., 2020). As a
∗Equal contribution. The order of authors is determined by flipping a coin.
1arXiv:2305.15265v4  [cs.LG]  7 Nov 2024

--- PAGE 2 ---
result, there is now an urgent necessity to effectively adapt these models, equipped with billion-scale
parameters, to a wide range of tasks.
However, a significant disparity exists between the memory requirements of pre-trained LMs
and the capacity of current hardware, particularly GPUs. For example, even a GPU with 24GB
memory cannot accommodate the fine-tuning process of the T5-3B model (Raffel et al., 2020b) with
batch size one, which boasts three billion parameters. Without additional techniques, attempting to
fine-tune billion-scale LMs on a single GPU is impossible. Although model-parallel fine-tuning is
feasible, the majority of the time, we cannot bear the expense of acquiring multiple GPUs or the
communication overhead involved. To ensure the smooth deployment of language models during the
fine-tuning process, it is crucial to adapt them for operation on a single GPU.
To address this issue, several parameter-efficient tuning methods are proposed (Lester et al.,
2021; Sung et al., 2022; Li and Liang, 2021; Zaken et al., 2021; Hu et al., 2021; Karimi Mahabadi
et al., 2021; Houlsby et al., 2019). Specifically, adapters (Houlsby et al., 2019; Karimi Mahabadi
et al., 2021) insert a small module into the transformer blocks and only update it while keeping
other parameters fixed. Similarly, prompt tuning (Lester et al., 2021) introduces a small vector that
is concatenated with the input embeddings and updated during the tuning process. LoRA (Hu et al.,
2021) injects trainable rank decomposition matrices into the transformer block, updating them while
freezing the others. Parameter-efficient tuning methods mainly reduce the memory taken by the
optimizer states (Kingma and Ba, 2014; Hu et al., 2021). Although the optimizer states contribute
to the memory footprint, storing activations (or feature maps) is the main memory bottleneck during
training (often >70%) (Chen et al., 2021c; Jain et al., 2020; Kirisame et al., 2020; Andoorveedu
et al., 2022b). Thus, parameter-efficient methods often do not reduce memory usage by much (Sung
et al., 2022; Andoorveedu et al., 2022b).
20 30 40
Training Peak Memory (GB)75808590Average Validation Result (%)Full (T5-Large)
LoRA (T5-Large)
LST (T5-Large)
LoRA+WTA-CRS@0.3 (T5-Large)
LoRA+WTA-CRS@0.3 (T5-3B)
LoRA+WTA-CRS@0.1 (T5-Large)
LoRA+WTA-CRS@0.1 (T5-3B)
Fig. 1. Accuracy-memory trade-off
ofWTA-CRS and other memory-efficient
tuningmethods. Unlessspeciallystated,
we use the T5-Large in the figure.In parallel, we can reduce the main memory bottleneck
by reducing the activation storage in fine-tuning. Since
transformer-based models are mainly built based on the lin-
ear layer, a less-explored direction is to replace the expensive
matrix multiplication operation with its memory-efficient es-
timations using column-row sampling (CRS) (Adelman et al.,
2021; Drineas et al., 2006). The key idea of CRS is to sub-
sample tensors onto low-dimensional spaces and perform the
original operations here. Specifically, for the linear operation
between two matrices A∈Rn×mandB∈Rm×q(in the con-
text of machine learning, Ais often activations), we first
sample k(k < m) column-row pairs according to a
pre-defined distribution. Then we obtain A′∈Rn×kand
B′∈Rk×q(k < m) by picking kcolumns of Aand the cor-
responding rows of Baccording to the sampled column-row
pairs (Drineas et al., 2006). Finally, we estimate AB≈A′B′. In this way, we only need to store the
sub-matrix A′andB′in GPU memory to perform the computation. Moreover, transformer-based
models training/tuning are performed with the first-order stochastic optimizer, e.g., Adam (Kingma
and Ba, 2014). In stochastic optimization, models can work with noisy gradients, as long as the
gradient estimator is unbiased and has a reasonable variance. In view of such, we ask: why spend
resources on obtaining exact gradients when we are using stochastic optimization?
Motivated by this, we focus on obtaining unbiased gradients cheaply with approximated matrix
multiplication.
The approximation method reduces the memory usage at the cost of giving outputs with variance.
Thus there naturally exists an accuracy-memory trade-off. The main challenge is how to integrate the
2

--- PAGE 3 ---
approximated matrix multiplication into transformer with minimal gradient variance. In this paper,
we propose a new family of unbiased estimator for matrix multiplication with reduced variance,
dubbed Winner-Take-All Column-Row Sampling ( WTA-CRS ). Compared to CRS, WTA-CRS reduces
the variance of an estimator by focusing more on high-probability regions of the sampling distribution.
Moreover, WTA-CRS can serve as a drop-in replacement for the linear operation in transformers,
providing an unbiased weight gradient with reduced memory usage. As shown in Figure 1, our
method achieves better accuracy-memory trade-off than state-of-the-art memory-efficient tuning
methods, e.g., LST (Sung et al., 2022) and LoRA (Hu et al., 2021). Moreover, since WTA-CRS
executed at the operation level, it is orthogonal to most of the existing parameter-efficient tuning
methods. Our contributions are highlighted as follows:
•We design a new family of unbiased estimator for matrix multiplication with reduced variance.
We theoretically and experimentally verify that it has smaller variance than the established
one under the context of tuning transformer.
•By replacing the linear operation with WTA-CRS in transformers, we can achieve up to 2.7 ×
peak memory reduction with almost no accuracy drop, and enables up to 6.4×larger batch size.
As shown in Figure 1, WTA-CRS stands out as an exceptional solution capable of fine-tuning
T5-3B using a mere 40GB GPU memory budget, with three billion parameters. Thus, we
achieve remarkable advancements in the adaptation of LMs on downstream tasks.
•We implement WTA-CRS as a ready-to-use extension for Pytorch with an easy-to-use API that
can also be combined with other memory-saving techniques.
2 Background and Preliminary
In this section, we first analyze the memory usage of transformers. Then we introduce the background
on the approximated matrix multiplication.
2.1 The Memory Usage of Transformers
T5-Base
(S=128)T5-Base
(S=256)T5-Large
(S=128)T5-Large
(S=256)0102030405060Memory (GB)
77.0%88.3%73.4%86.1%Memory Usage Breakdown
Model
Optimizer
Activation
Other
Fig. 2. The GPU memory us-
age breakdown for fine-tuning T5
(Wang et al., 2018a), where the
batch size Bis 64 and sequential
length Sis 128 or 256.In each training step of backpropagation, it has exactly two
phases, i.e., one forward phase and one backward phase.
Transformer-based models are mainly built based on the linear
operation, which can be written as:
Forward Pass Z=GEMM(H,W),(1a)
Backward Pass ∇H=GEMM(∇Z,W⊤),(1b)
∇W=GEMM(H⊤,∇Z),(1c)
where GEMM(·,·)is the General Matrix Multiplication oper-
ation, HandZare the activation (or input feature maps) and
output feature maps, respectively. Wis the weight of the linear
layer. ∇H,∇W, and ∇Zare the gradient of H,W, and Z,
respectively. From Equation (1c), activations Hare used in
the backward phase. In commonly used deep learning framework (Abadi et al., 2016; Paszke et al.,
2019), it requires storing Hin GPU memory during the forward pass, for calculating the weight
gradient ∇Win the backward pass.
Previous works show that although the model parameters contribute to the memory footprint,
activations (e.g., storing H) are the main memory bottleneck during training (Chen et al., 2021c;
3

--- PAGE 4 ---
Jain et al., 2020; Kirisame et al., 2020; Andoorveedu et al., 2022b). To get a sense of the scale,
we show in Figure 2 that for popular transformer models like T5, activations may take roughly
73∼88%of the total memory, depending on the batch size Band sequential length S.
2.2 Approximated GEMM With Sampling
LetX∈Rn×m,Y∈Rm×qbe two matrices. The goal is to efficiently estimate the matrix production
XY. Singular Value Decomposition (SVD) outputs provably optimal low-rank estimation of XY
(Adelman et al., 2021). However, SVD is almost as expensive as matrix production itself. Instead,
the sampling algorithm is proposed to approximate the matrix product XYby sampling kcolumns
ofXand corresponding rows of Yto form smaller matrices, which are then multiplied as usual
(Drineas et al., 2006; Drineas and Kannan, 2001):
GEMM(X,Y) =mX
i=1X:,iYi,:≈kX
t=11
kpitX:,itYit,:=X′Y′, (2)
where X:,i∈Rn×1andYi,:∈R1×qare the ithcolumn and row of XandY, respectively. In
this paper, we call (X:,i,Yi,:)theithcolumn-row pair. kis the number of sampled pairs
(1≤k≤m).P={pi}m
i=1is a probability distribution over the column-row pairs. it∈ {1,···m}
is the index of the sampled column-row pair at the tthtrial. stis the scale factor. X′∈Rn×kand
Y′∈Rk×qare the normalized sub-matrices sliced according to the sampled column-row pairs.
Existing work (Drineas et al., 2006) shows X′Y′is an unbiased estimation of XY, i.e.,E[X′Y′] =
XY. Furthermore, the approximation error E[||XY−X′Y′||F]is minimized when the probabilities
{pi}m
i=1are proportional to the product of the column-row Euclidean norms (Drineas et al., 2006)
(Proof in Appendix C):
pi=||X:,i||2||Yi,:||2Pm
j=1||X:,j||2||Yj,:||2. (3)
As we analyzed in Section 2.1, storing the activation His the major memory bottleneck. If
we can replace GEMM(H⊤,∇Z)in Equation (1c) with H′⊤∇Z′following the paradigm of
Equation (2), then we only need H′instead of Hin GPU memory to compute the
gradient, which significantly decreases the memory usage of activations. This estimation
linearly reduces the memory complexity from O(nm)toO(nk). Also, the total number of floating
point operations (FLOPs) is reduced as well since the computation is executed on two smaller
matrices. For the ease of illustration, in this paper we call the distribution in Equation (3) the
column-row index distribution . In the next section, we explore how to reduce memory usage via
sampling-based matrix multiplication.
3 Methodology
In recent years, we have observed that deep neural network training can be performed almost entirely
with first-order stochastic optimization (Kingma and Ba, 2014). Thus intuitively, in stochastic
optimization we can reduce the resources spent on obtaining gradients, as long as the estimated
gradient is unbiased with reasonable variance (Chmiel et al., 2023, 2021; Oktay et al., 2020). Following
this motivation, we first design a new unbiased estimator for matrix multiplication with reduced
variance compared to the one in Equation (2) (Section 3.1 ). Then we introduce how to replace the
GEMMin Transformer with its approximated version to reduce the memory usage (Section 3.2).
4

--- PAGE 5 ---
3.1 Winner-Take-All Column-Row Sampling: A New Unbiased Estimator for
GEMM
In this section, we mathematically design a new unbiased estimator for GEMMwith reduced variance
called WTA-CRS (Winner-Take-All Column-Row Sampling). Following the notation in Section 2.2,
letX∈Rn×m,Y∈Rm×qbe two matrices. P={pi}m
i=1is the column-row index distribution in
Equation (3)1. We first define the variable f(i)as
f(i) =X:iYi:
pi,
f(i)is an unbiased estimation for the matrix production between XandY. To see this,
Ej∼P[f(j)] =mX
i=1piX:,iYi:
pi=XY.
We note that the prior approximated matrix multiplication in Equation (2) is the direct extension of
f(i)by taking the average of {f(it)}k
t=1among kindependent random trials to reduce the variance.
Here we explore an alternative approach to reduce the variance of f(i)beyond simple averaging.
Our core idea is to partition the column-row index distribution P={pi}m
i=1into two complementary
regions based on the probability mass: a high-probability region PCand a low-probability region
PD\C, where D={1,···, m}is the whole set and Cis the set of the column-row index with the
largest probability. LetCbe the set of column-row pair indices associated with |C|largest
pi.We define WTA-CRS estimator for XYas follows:
Ej∼PD\ChX
c∈Cf(c)pc+ (1−X
c∈Cpc)f(j)i
. (4)
We note that the random variable in Equation (4) is the column-row pair index j, and is
only sampled from D\C. The estimator defined in Equation (4) contains two parts. The first partP
c∈Cf(c)pchas no relationship with the random variable jand is summed deterministically. The
second part f(j)is sampled stocastically, but scaled by the factor (1−P
c∈Cpc). When P={pi}m
i=1
is concentrated on a small number of atoms, the scaling factor (1−P
c∈Cpc)for the stochastic term
should be small. Therefore, we intuitively expect the estimator to have a small variance in this case
due to a small scaling factor. In this way, we reduce the variance of an estimator by focusing more
on high-probability regions of the distribution (winner-take-all). Below we formalize this intuition
by showing the statistical property of our estimator regarding the bias and variance, respectively.
Theorem 1 (Proof in Appendix C.2) .The estimator defined in Equation (4) is an unbiased estimator
for matrix production XY, i.e,Ej∼PD\C[P
c∈Cf(c)pc+ (1−P
c∈Cpc)f(j)] =XY.
Theorem 1 states that our proposed estimator in Equation (4) is unbiased. Below we compare
our proposed estimator to the CRS estimator in Equation (2) in terms of the variance. Suppose we
have the budget of only utilizing kcolumn-row pairs for approximating the matrix production. From
the implementation perspective, the estimator defined in Equation (2) estimates GEMM(X,Y)as:
(CRS) g(X,Y) =1
kkX
t=1f(it), i 1,···, iki.i.d∼ P. (5)
1Here we note that the theoretical analysis in this section can be applied to any probability distribution, not only
limited to the one in Equation (3).
5

--- PAGE 6 ---
Our estimator defined in Equation (4) splits the budget kinto two parts. Namely, the first
part explicitly sums the expectation terms for the largest probability group C(|C|< k), while
stochastically average k− |C|samples drawn from D\Cto estimate the remaining terms, up to scale:
(WTA-CRS ) ˆg(X,Y) =X
c∈Cf(c)p(c) +1−P
c∈Cpc
k− |C|k−|C|X
j=1f(j), i 1,···, ik−|C|i.i.d∼ PD\C.(6)
Theorem 2 (Proof in Appendix C.3) .Suppose the total budget of column-row pairs is k. IfC
satisfiesX
c∈Cpc>|C|
k, (7)
then we have Var[ˆg(X,Y)]<Var[g(X,Y)]. Moreover, Var[ˆg(X,Y)]is minimized when |C|=
min|C|∈{ 0,···,k}1−P
c∈Cpc
k−|C|.
Both the left- and right-hand sides of Equation (7) depend on the size of the highest probability
group |C|, which controls the number of high probability column-row pairs that are directly added
without sampling. Below we experimentally investigate whether Equation (7) holds under the context
of fine-tuning the transformer-based model with varying |C|.
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.60.8Probability Mass 
cpc
Query Layer (k = 0.3||)
2-th layer
4-th layer
6-th layer
||
k Ratio
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.6Key Layer (k = 0.3||)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.60.8Value Layer (k = 0.3||)
Fig. 3. The probability massP
c∈Cpcversus|C|
kin Equation (7) at k= 0.3|D|. Here we visualize the
column-row index distribution of query/key/value projection layer in the T5-base model, which is
fine-tuned on RTE dataset. More similar results can be found in Appendix E.1.
Experimental analysis. As shown in Figure 3, we visualize the two terms in Equation (3) for
the column-row index distribution of query, key, and value projection in the self-attention module,
respectively (Vaswani et al., 2017). Specifically, we fix the total column-row pair budget k= 0.3|D|
and change the size of the highest probability group |C|from 0 to k. We conclude that Equation (7)
holds for most of the layers when fine-tuning transformers. Thus, we expect our WTA-CRS has better
performance than CRS for adapting transformer-based models, which is later experimentally verified
in Section 5.
3.2 Compress GEMM in Transformers with WTA-CRS
Previous work has shown that unbiasedness of the estimated gradient is crucial for the proper
convergence of stochastic gradient descent (Chmiel et al., 2023, 2021; Chen et al., 2021c; Liu et al.,
2022a). As shown in Section 2.1, we have three GEMMin the linear layer. Below we investigate how
to replace GEMMwith its approximated version in a way that the estimated gradient is unbiased.
Unbiasedness. Previous work has shown that to ensure the unbiasedness of the gradient, the
approximation can only be applied during the backward pass (Chen et al., 2021c; Liu et al., 2022b;
Adelman et al., 2021). The rationale behind this conclusion is that we have E[f(x)]̸=f(E[x])for any
6

--- PAGE 7 ---
Linear-Q Linear-KLinear-V
TensorMul-1TensorMul-2
GELU Dropout
SoftMaxDropout
Linear-O
LayerNormLinear-ULinear-DDropoutLayerNorm
Output of 
last blockQ KVNext block Layer-
Q, -K, and -V
Reshape
[B×S×Dmodel][B×Nhead×S×Dhead] [B×Nhead×Dhead×S][B×S×Dmodel]
[B×Nhead×S×S][B×Nhead×S×S]
[B×S×Dmodel][B×S×Dmodel][B×S×4Dhead][B×S×4Dmodel][B×S×Dmodel]
[B×Nhead×S×Dhead][B×Nhead×S×Dhead]Fig. 4. The diagram of a single Transformer block. The shape of activations is annotated, where
B, S, D model,Nhead, and Dheadare the batch size, sequence length, hidden size, number of attention
heads, and head dimension, respectively. WTA-CRS can be applied to the operators in green; the
activationmapsofoperatorsinbluecanbelosslesslycompressed; andthoseingrayarenotcompressed
in this paper. The idea of this figure is inspired by (Andoorveedu et al., 2022a).
non-linear function f(·), e.g., E[x2]̸=E2[x]. Thus if we replace the forward GEMMin Equation (1a),
even when the approximation method gives an unbiased estimation, i.e., E[ˆg(H,W)] =HW =Z,
the output activations (e.g., GeLU (Z)) are still biased since the activation function is non-linear,
namely,
GeLU (ˆg(H,W)) =GeLU (E[Z])̸=E[GeLU (Z)].
To ensure the unbiasness of gradient and reduce the memory usage of storing H, as shown in the
example of Figure 5, we only replace GEMMin the backward pass with its approximation
(e.g., Equation (1c)), while leaving the forward one unchanged (e.g., Equation (1a)).
We show in Appendix B that the estimated weight gradient is unbiased in this case.
GEMMApprox-
GEMMGEMMLived in GPU
memoryEquation (5)
Fig. 5. The illustration of how to deploy WTA-CRS
to linear layers. We only replace GEMMin Equa-
tion (1c) with its approximated version using
WTA-CRS . The pseudocode is given in Appendix D
Algorithm 1.Implementation. Here we present how we im-
plement WTA-CRS inEquation(6)inpractice. For
the linear layer, as we analyzed, we only replace
GEMMin Equation (1c) with its approximated ver-
sion. In this case, XandYin Equation (6) are
activation H⊤and output gradient ∇Z, respec-
tively. Given the total column-row pair budget k,
thefirststep is to build the deterministic index
setC, where each element is summed explicitly
without sampling. Note that Cis a set of indices
with the highest probabilities in Equation (3).
Thus, to build C, we only need to determine its
size, denoted as |C|, which minimizes the vari-
ance of the estimator. As Theorem 2 suggested,
we set |C|=min|C|∈{ 0,···,k}1−P
c∈Cpc
k−|C|. Thesec-
ondstep is to sample k− |C|column-row indices from the remaining distribution PD\Cto obtain
the set Cstoc, where |Cstoc|=k− |C|. Thethirdstep is to build sub-sampled H′with only rows
fromC ∪ C stoc. Note that for rows in H′fromCstoc, we need to normalize it by1−P
c∈Cpc
k−|C|according
7

--- PAGE 8 ---
to Equation (6). We illustrate the above process in Figure 5. The pseudocode to Appendix D
Algorithm 1. We leverage Liger kernel (Hsu et al., 2024) to further speedup our fine-tuning pipeline.
Scope. Here we show which operation can be replaced with its approximation version. As shown
in Figure 4, the transformer is mainly consisted of linear layer, TensorMul, and other operations
(e.g., GeLU, Dropout, LayerNorm). TensorMul in Figure 4 refers to the multiplication between two
four-dimensional tensors. Our WTA-CRS can be applied to Linear-Q, -K, -V, -O, -U, -D, TensorMul-1,
and TensorMul-2 (in green). The activations of Dropout and GELU operations (in blue) can be
losslessly compressed. The Softmax and LayerNorm operators (in gray) remain unchanged.
4 Related Work and Discussion
Due to the page limit, we discuss the related work on approximated matrix multiplication and
activation compression. Other related topics, e.g., parameter-efficient fine-tuning and gradient
checkpointing, can be found in Appendix A. We also discuss the limitation and potential negative
social impact in Appendix A.
Approximated Matrix Multiplication. In the context of neural networks, approximated
matrix multiplication methods can be broadly categorized into two main groups: (1) Butterfly-based
methods (Chen et al., 2021a; Dao et al., 2022) replace dense weight matrices with butterfly matrices.
We note that they focus on the weight matrix and are orthogonal to our research, as we concentrate on
sub-sampling the activation matrix. (2) Column-row sampling (CRS) methods (Drineas et al., 2006;
Adelman et al., 2021; Liu et al., 2022b) select important rows and columns from the input matrices
and perform the multiplication on the sampled matrix. Our work is closely aligned with this second
research line. (Adelman et al., 2021; Liu et al., 2022b) share similarities with our research in terms of
utilizing CRS for approximating matrix multiplication within neural networks. The main distinction
lies in how to select the column-row pairs. Specifically, (Adelman et al., 2021) deterministically
selects column-row pairs without scaling, whereas our estimator divides the column-row pairs into a
deterministic component and a stochastic component. As we analyzed, selecting column-row pairs
deterministically is biased. Later we show that this approach may cause a significant accuracy drop
(“Deterministic” in Figure 8 ).
Activation Quantization. The activation quantization methods focus on quantizing the
activation into low numerical precision numbers, e.g., 8-bit integers (Chen et al., 2021c; Liu et al.,
2022a,c; Wang et al., 2022b,a). Here we discuss the difference between these two works in terms
of compression ratios. According to Table 5 in (Liu et al., 2022a), when GACT is combined with
Swapping, i.e., offloading quantized activation to main memory, it achieves a peak memory usage
compression rate of 1.73×for Bert-Large. Our work also compresses the activation, but in a different
way.We emphasize that our work is orthogonal to activation quantization in the sense that our work
essentially reduces the dimension of activation. This distinction allows our method to be readily
combined with activation quantization techniques, offering the potential for even more aggressive
compression.
5 Experiments
In this section, we design experiments to answer the following research questions: RQ1:How effective
isWTA-CRS in terms of accuracy with reduced memory usage? RQ2:How sensitive is WTA-CRS
affected by its key hyper-parameters? RQ3: WTA-CRS contains two parts, i.e., the deterministic
8

--- PAGE 9 ---
Table 1: The GLUE benchmark results with T5 and Bert at different scales.
Model Method CoLA SST-2 MRPC QQP MNLI QNLI RTE STS-B AVG
BERT-BaseFull 60.9 ±1.8992.2±0.3487.9±0.4687.8±0.0183.7±0.0590.7±0.1466.4±0.3688.1±0.2782.2
LoRA 61.6 ±0.2591.7±0.1790.0±0.3486.9±0.183.6±0.0290.8±0.1768.2±0.3687.6±0.5282.6
WTA-CRS@0.3 60.7 ±0.8990.2±0.0687.0±0.1487.5±0.0383.4±0.0690.4±0.0765.9±0.1889.3±0.281.8
LoRA+WTA-CRS@0.3 61.5 ±1.0889.6±0.5289.6±0.0986.3±0.0282.8±0.3590.6±0.1667.9±0.7287.3±0.781.9
T5-BaseFull 60.1 ±0.3794.9±0.2991.5±0.2988.5±0.0787.0±0.193.3±0.0379.4±0.7890.6±0.1485.7
LoRA 60.6 ±0.9460.6±0.9492.2±0.3187.4±0.0686.2±0.0693.4±0.0380.6±0.7490.7±0.0585.7
LST 55.5 ±0.2494.0±0.1791.1±0.1887.4±0.0385.7±0.1393.4±0.072.7±0.5490.4±0.0683.8
WTA-CRS@0.3 60.9 ±0.5294.8±0.1491.1±0.3588.0±0.1186.3±0.0293.1±0.0778.7±0.5990.5±0.0585.4
LoRA+WTA-CRS@0.3 60.0 ±0.5194.4±0.1692.0±0.3887.3±0.0485.6±0.0893.2±0.0180.1±1.0290.4±0.0685.4
BERT-LargeFull 66.8 ±0.3193.5±0.2989.5±0.2688.5±0.0386.4±0.1992.1±0.2472.6±0.3690.2±0.7685.0
LoRA 65.9 ±0.2793.8±0.1790.8±0.3787.6±0.0885.9±0.0592.0±0.271.3±0.1890.3±0.0984.7
WTA-CRS@0.3 64.7 ±0.4493.5±0.089.3±0.3988.2±0.0485.2±0.0391.9±0.1273.8±0.5490.4±0.0284.6
LoRA+WTA-CRS@0.3 66.0 ±0.3393.3±0.2989.7±1.3287.6±0.0286.0±0.0791.9±0.1472.4±0.1789.7±0.0484.6
T5-LargeFull 61.3 ±1.0196.3±0.093.4±0.1389.7±0.0189.8±0.0794.2±0.0585.3±0.1791.8±0.0887.7
LoRA 63.3 ±0.2696.4±0.1493.5±0.1688.5±0.0389.5±0.0594.3±0.0784.2±0.6891.7±0.1387.7
LST 59.9 ±0.7795.8±0.0691.8±0.0888.4±0.0188.7±0.0594.2±0.0282.5±0.1891.4±0.0786.6
WTA-CRS@0.3 60.9 ±1.1896.3±0.2593.6±0.5789.3±0.0489.5±0.1294.1±0.0384.4±0.3491.3±0.0587.4
LoRA+WTA-CRS@0.3 62.9 ±1.1996.2±0.0593.6±0.4788.3±0.0289.2±0.0894.0±0.0783.9±0.9591.3±0.0387.4
T5-3BLoRA 70.1 ±0.3796.8±0.2994.0±0.2789.9±0.091.0±0.1495.6±0.0585.9±0.3692.9±0.0889.5
LoRA+WTA-CRS@0.3 71.4 ±0.3596.4±0.0694.6±0.3990.0±0.0591.0±0.0695.6±0.1286.3±0.3692.9±0.0989.8
summation part and the statistical sampling part. Are they both necessary? RQ4:How is the
fine-tuning speed affected by WTA-CRS ?
5.1 Experiment Setup
Datasets and Evaluation Protocol. Following most of the previous work, we adopt GLUE
benchmark (Wang et al., 2018b) to evaluate the effectiveness of different methods, including the
CoLA, SST-2, MRPC, QQP, MNLI, QNLI, RTE, and STS-B datasets. For the SST-2, MNLI, QNLI,
and RTE datasets, we report the validation accuracy. For CoLA, we use Matthew’s correlation as
the evaluation metric. The F1 score is reported for both MRPC and QQP tasks, while the Pearson-
Spearman correlation is used to evaluate the performance on the STS-B dataset. To evaluate the
memory usage, we report the peak GPU memory usage and compression rate during the fine-tuning
process with Huggingface API (Wolf et al., 2020).
Compared Methods and Adopted Models. We consider three methods to compare in this
paper: Full fine-tuning (Full), LoRA (Hu et al., 2021), and Ladder Side-tuning (LST) (Sung et al.,
2022). Specifically, Fulltunes all of the parameters in the model to provide an upper bound of
accuracy; LoRAinserts trainable low-rank matrices into the model to parameterize the weights’
changes;LSTinjects a trainable ladder side structure. Since WTA-CRS essentially replace the linear
operation with approximated one, we emphasize that our WTA-CRSis compatible with all
these three compared methods, i.e., they can be combined together towards smaller
memory usage. For the backbone model, we follow the previous work (Sung et al., 2022; Houlsby
et al., 2019; Hu et al., 2021) to adopt the Bert-Base (Devlin et al., 2018), Bert-Large, T5-Base,
T5-Large, and T5-3B (Raffel et al., 2020a) for evaluating the effectiveness of different methods.
Hyperparameter Settings. ForWTA-CRS, it only has one hyperparameter k, which controls the
column-row pair budget. We assign the same kto all replaceable linear operations in the model. We
consider the normalized column-row pair budget k/|D| ∈ { 0.3,0.1}, which are denoted as WTA-CRS
@0.3 and WTA-CRS@0.1, respectively. We also consider the combination of WTA-CRS and LoRA to
further reduce the memory cost of both optimizer and activations. The detailed hyperparameters
are given in Appendix F. All reported results are averaged over three random trials.
9

--- PAGE 10 ---
Table 2: Peak memory usage (GB) and compression rate of fine-tuning T5-Base and -Large. We
measure the memory usage on a single NVIDIA A100 (80GB) GPU. For T5-3B, since it is trained
using multi-GPUs with data parallel. We instead report the maximum batch size in Figure 6 for it.
FP LoRA LST WTA-CRS@0.3 WTA-CRS@0.1 LoRA+ WTA-CRS@0.3 LoRA+ WTA-CRS@0.1
T5-Base 17.66 (1 ×) 13.84 (1.3 ×) 5.50 (3.2 ×) 8.44 (2.1 ×) 7.30 (2.4 ×) 6.50 (2.7 ×) 5.44 (3.2 ×)
T5-Large 45.85 (1 ×) 36.83 (1.2 ×) 14.85 (3.1 ×) 21.58 (2.1 ×) 18.46 (2.5 ×) 17.44 (2.6 ×) 14.64 (3.13 ×)
5.2 Accuracy versus Memory Usage (RQ1)
To answer RQ1, we first analyze the trade-off between the model performance and memory saving.
The evaluation results and peak memory usage are given in Tables 1 and 2, respectively. We observe:
❶WTA-CRS achieves a superior trade-off between accuracy and memory usage compared to
baselines. Specifically, WTA-CRS has negligible accuracy drop, while the peak memory usage is reduced
by2.1× ∼ 2.7×(when combined with LoRA).
100 200
Batch-size406080Peak Mem cost (GB)
Full
LoRA
LoRA+WTA-CRS@0.3
LoRA+WTA-CRS@0.1
Fig. 6. Peak memory usage
versus the maximum batch size
of T5-3B. More similar results
are shown in Appendix E.2.As we analyzed, LoRA mainly reduces the memory of optimizer
states. Thus, although it has negligible accuracy drop, it can only
achieve ∼1.3×peak memory saving. LST can reduce memory
usage up to 3×, but its accuracy drop is much larger than LoRA
andWTA-CRS . Since WTA-CRS executes at the operation level and
focuses on the activations, we further combine LoRA with WTA-CRS
to reduce memory usage more aggressively. When combing with
LoRA, WTA-CRS achieves 2.7×memory usage saving with almost
no accuracy drop. To fully tune T5-3B, it requires 37.7GB of
memory and relies on a GPU with a capacity of 40GB or higher,
e.g. RTX8000, A100, or A40. On the other hand, LoRA+ WTA-CRS
only requires 21.6GB of memory for finetuning with a mini-batch
size of 32, which can run on a GPU with 24GB memory, e.g.
RTX3090Ti or A5000. We have experimentally confirmed this
conclusion. Under the same hardware, WTA-CRS enables the tuning of larger models, resulting in
improved down-streaming task performance. Thus as shown in Figure 1, ❷under the similar memory
budget, WTA-CRS outperforms the other methods in terms of the accuracy.
Also, according to Figure 6, for T5-3B, LoRA itself can enable 1.9×larger batch size. ❸When
combined with LoRA, WTA-CRS enables 4.8×(k=0.3|D|) to6.4×(k=0.1|D|) larger batch-size.
Influence of Row-column Pairs Budget (RQ2). As we analyzed in Section 3.2, WTA-CRS
only have one hyperparameter, i.e., the total column-row pair budgets k. We conduct the ablation
study with different budget kin Figure 7. We observe that ❹It has almost no accuracy drop when
k= 0.3|D|. And the accuracy drop is about 1%when k= 0.1|D|.Notably, for T5-3B, the accuracy
drop is only 0.4%when k= 0.1|D|, which is much smaller than T5-Base and T5-Large. This suggests
that larger models are more compressible because they have more redundant activations, which is
consistent with previous observations (Li et al., 2020).
5.3 Ablation Study (RQ3 and RQ4)
To answer RQ3,WTA-CRS is compared with two compositional methods to demonstrate its superi-
ority. Namely, (1) the Deterministic method selects row-column pairs with top kprobability of
Equation (3). We note that this is the estimator proposed in (Adelman et al., 2021). (2) The CRS
method follows Equation (3) to sample the row-column pairs. All methods are deployed to GEMMin
10

--- PAGE 11 ---
w/o LoRA w/ LoRA80.082.585.0Average Validation Result (%)85.785.4
84.585.785.4
84.7(a) T5-Base
w/o LoRA w/ LoRA848688Average Validation Result (%)87.787.4
86.587.787.487.1 (b) T5-Large
w/ LoRA868890Average Validation Result (%)89.589.8
89.1
Full
k/||=0.3
k/||=0.1
 (c) T5-3B
Fig. 7. Average validation results on GLUE dataset of WTA-CRS with varying budgets.
2 4 6 8 10
Epoch9294Accuracy (%)
(a) SST2
2 4 6 8 10
Epoch7080Accuracy (%)
 (b) MNLI
2 4 6 8 10
Epoch6080F1 score (%)
Deterministic
CRS
WTA-CRS (c) QQP
Fig. 8. Validation results of T5-Base with different methods.
the backward pass, while leaving the forward one unchanged. The experiments are conducted on the
training of T5-base language model on the SST2, MNLI, and QQP datasets; The column-row pair
budget takes k/|D|= 0.1for all methods. The validation accuracy versus training epoch is given in
Figure 8. We observe:
❺WTA-CRS outperforms all compared methods, especially as the training epoch grows. The
deterministic selection of top kcolumn-row pairs suffers from accumulation of bias error that
ultimately results in a failure of convergence. For CRS, it also enables the unbiased weight gradient.
However, as we theoretically and experimentally analyzed in Theorem 7 and Figure 3, it is worse
than WTA-CRS due to larger variance. In summary, both the deterministic and stochastic parts
contribute to the effectiveness of WTA-CRS , which is consistent with our theoretical analysis.
200 400 600
Batch-size455055Training Throughput
Full
WTA-CRS@0.3
WTA-CRS@0.1
(a) T5-Large
50 100 150 200
Batch-size121416Training Throughput
Full
WTA-CRS@0.3
WTA-CRS@0.1 (b) T5-3B
Fig. 9. Batch size versus training throughput (sen-
tences/sec) with different methods, where the se-
quential length is 128. The hardware is one single
NVIDIA-A100 (80GB).The speed of WTA-CRS(RQ4). The con-
figuration of computational infrastructure is
given in Appendix F.1. We note that WTA-CRS
does not add any extra parameters to the
model. Thus, WTA-CRS only affect the fine-
tuning speed, without affecting the inference
speed. The memory taken by the activations
is proportional to the batch size and sequential
length. Below we analyze how the fine-tuning
speed affected by WTA-CRS . As we analyzed in
Appendix A limitation, the current implemen-
tation is not heavily optimized and thus the
execution time of WTA-CRS is still slower than
the original linear operation (details are shown in Appendix E.2). However, under the same hardware,
a reduction in activation memory enables the use of larger batch sizes, thereby improving training
speed due to increased GPU utilization (Goyal et al., 2017; Sung et al., 2022). As we analyzed in
Figure 6, WTA-CRS can enlarge the available batch size by up to 4.8×larger. This enhancement is
expected to result in a acceleration of the training speed. To illustrate this relationship, Figure 9
presents a visualization of batch size against training throughput (sentences per second) for both
T5-Large and T5-3B models. We observe that ❻WTA-CRS enables faster training speed under the
11

--- PAGE 12 ---
same hardware. Specifically, on the T5-Large model, WTA-CRS@0.1 shows 1.08×higher training
throughput; and on the T5-3B model, WTA-CRS@0.3 and WTA-CRS@0.1 achieve 1.14×and1.21×
higher training throughput, respectively.
6 Conclusion
In this paper, we propose WTA-CRS , a new unbiased estimator for matrix production with reduced
variance. We theoretically and experimentally show when and why the estimator is better than the
traditional unbiased estimator in terms of the variance. In the context of adapting transformers, it
almost has no accuracy drop while reducing the peak memory usage by up to 2.7×, and it enables a
6.4×larger batch size, which in return resulting in 1.2×higher training throughput.
References
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale
machine learning. In Osdi, volume 16, pages 265–283. Savannah, GA, USA, 2016.
Menachem Adelman, Kfir Levy, Ido Hakimi, and Mark Silberstein. Faster neural network training
with approximate tensor operations. Advances in Neural Information Processing Systems , 34:
27877–27889, 2021.
Muralidhar Andoorveedu, Zhanda Zhu, Bojian Zheng, and Gennady Pekhimenko. Tempo: Accel-
erating transformer-based model training through memory footprint reduction. arXiv preprint
arXiv:2210.10246 , 2022a.
Muralidhar Andoorveedu, Zhanda Zhu, Bojian Zheng, and Gennady Pekhimenko. Tempo: Accel-
erating transformer-based model training through memory footprint reduction. arXiv preprint
arXiv:2210.10246 , 2022b.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re.
Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint
arXiv:2112.00029 , 2021a.
Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song,
Anshumali Shrivastava, and Christopher Re. MONGOOSE: A learnable LSH framework for
efficient neural network training. In International Conference on Learning Representations (ICLR) ,
2021b. URL https://openreview.net/forum?id=wWK7yXkULyh .
Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael Mahoney, and Joseph
Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training.
InInternational Conference on Machine Learning , pages 1803–1813. PMLR, 2021c.
Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic unbiased
quantization: Simple 4-bit training in deep learning. arXiv preprint arXiv:2112.10769 , 2021.
12

--- PAGE 13 ---
Brian Chmiel, Itay Hubara, Ron Banner, and Daniel Soudry. Minimum variance unbiased n:
M sparsity for the neural gradients. In The Eleventh International Conference on Learning
Representations , 2023.
Yu-Neng Chuang, Guanchu Wang, Fan Yang, Zirui Liu, Xuanting Cai, Mengnan Du, and Xia Hu.
Efficient XAI techniques: A taxonomic survey. CoRR, abs/2302.03225, 2023. doi: 10.48550/arXiv.
2302.03225. URL https://doi.org/10.48550/arXiv.2302.03225 .
Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu,
Aniruddh Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for
efficient and accurate training. In International Conference on Machine Learning , pages 4690–4721.
PMLR, 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Petros Drineas and Ravi Kannan. Fast monte-carlo algorithms for approximate matrix multiplication.
InProceedings 42nd IEEE Symposium on Foundations of Computer Science , pages 452–459. IEEE,
2001.
Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices i:
Approximating matrix multiplication. SIAM Journal on Computing , 36(1):132–157, 2006.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for
nlp. InInternational Conference on Machine Learning , pages 2790–2799. PMLR, 2019.
Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu,
Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm
training. arXiv preprint arXiv:2410.10989 , 2024. URL https://arxiv.org/abs/2410.10989 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez,
Kurt Keutzer, and Ion Stoica. Checkmate: Breaking the memory wall with optimal tensor
rematerialization. Proceedings of Machine Learning and Systems , 2:497–511, 2020.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361 , 2020.
Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank
hypercomplex adapter layers. Advances in Neural Information Processing Systems , 34:1022–1035,
2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
13

--- PAGE 14 ---
Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi
Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 ,
2020.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning.arXiv preprint arXiv:2104.08691 , 2021.
Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 , 2021.
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.
Train big, then compress: Rethinking model size for efficient training and inference of transformers.
InInternational Conference on machine learning , pages 5958–5968. PMLR, 2020.
Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen,
Zhiyuan Liu, Jie Tang, Joey Gonzalez, et al. Gact: Activation compressed training for generic
network architectures. In International Conference on Machine Learning , pages 14139–14152.
PMLR, 2022a.
Zirui Liu, Qingquan Song, Kaixiong Zhou, Ting-Hsiang Wang, Ying Shan, and Xia Hu. Towards
interaction detection using topological analysis on neural networks. CoRR, abs/2010.13015, 2020.
URL https://arxiv.org/abs/2010.13015 .
Zirui Liu, Haifeng Jin, Ting-Hsiang Wang, Kaixiong Zhou, and Xia Hu. Divaug: plug-in automated
data augmentation with explicit diversity maximization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4762–4770, 2021.
Zirui Liu, Shengyuan Chen, Kaixiong Zhou, Daochen Zha, Xiao Huang, and Xia Hu. Rsc: Ac-
celerating graph neural networks training via randomized sparse computations. arXiv preprint
arXiv:2210.10737 , 2022b.
Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, and Xia Hu. EXACT: Scalable graph neural
networks training via extreme activation compression. In International Conference on Learning
Representations , 2022c. URL https://openreview.net/forum?id=vkaMaq95_rX .
Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. Randomized
automatic differentiation. arXiv preprint arXiv:2007.10412 , 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems , 32,
2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020a.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020b.
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory
efficient transfer learning. arXiv preprint arXiv:2206.06522 , 2022.
14

--- PAGE 15 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461 , 2018a.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461 , 2018b.
Guanchu Wang, Zaid Pervaiz Bhat, Zhimeng Jiang, Yi-Wei Chen, Daochen Zha, Alfredo Costilla
Reyes, Afshin Niktash, Gorkem Ulkar, Erman Okman, Xuanting Cai, et al. Bed: A real-time
object detection system for edge devices. In Proceedings of the 31st ACM International Conference
on Information & Knowledge Management , pages 4994–4998, 2022a.
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, and Xia Hu. Towards memory
efficient training via dual activation precision. arXiv preprint arXiv:2208.04187 , 2022b.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 conference on empirical methods in natural
language processing: system demonstrations , pages 38–45, 2020.
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin,
and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv
preprint arXiv:2304.13712 , 2023.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.
Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and
Xia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158 , 2023.
Shaochen Zhong, Guanqun Zhang, Ningjia Huang, and Shuai Xu. Revisit kernel pruning with lottery
regulated grouped convolutions. In International Conference on Learning Representations , 2022.
URL https://openreview.net/forum?id=LdEhiMG9WLO .
15

--- PAGE 16 ---
Appendix
A Extended Related Work and Discussion
Parameter-Efficient Fine-tuning. Parameter-efficient tuning methods select a small subset of
parameters or insert a few parameters to a pre-trained network. Then they only update the small
subset of parameters, while keeping others fixed (Lester et al., 2021; Sung et al., 2022; Li and Liang,
2021; Zaken et al., 2021; Hu et al., 2021; Karimi Mahabadi et al., 2021; Houlsby et al., 2019; Chen
et al., 2021b). For example, Adapters (Houlsby et al., 2019; Karimi Mahabadi et al., 2021) insert a
small module into the transformer blocks and only update it. Similarly, prompt tuning (Lester et al.,
2021) introduces a small vector that is concatenated with the input embeddings. BitFit (Zaken
et al., 2021) only tunes the bias term of the model. LoRA (Hu et al., 2021) injects trainable rank
decomposition matrices into the transformer block. Although these methods are “parameter-efficient”,
they actually cannot reduce the memory usage of the model itself. This is because we still needs to
build the computation graph for the whole model. Instead, the memory usage of optimizer states will
be significantly reduced, which is in proportional to the number of trainable parameters (Kingma
and Ba, 2014).
Gradient Checkpointing. Gradient checkpointing helps decrease activation memory usage by
saving only a selection of activations. However, it demands additional computation during the
backward pass, as discarded activations must be recalculated (Kirisame et al., 2020; Jain et al., 2020).
According to the report of Checkmate2(Jain et al., 2020), it achieves “a 2.3x memory reduction when
training a BERT model with Checkmate optimizations (at 1x extra overhead for rematerialization)”.
Limitations Although WTA-CRS significantly reduces the computation of the backward pass in a
hardware-friendly way i.e., dropping entire rows/columns in the tensor, the current implementation
still hampers the execution time of linear operations. This is because the extra sampling process
and data movement counteract the acceleration. However, we note that (1) the overhead can be
greatly reduced with better implementation, e.g., using prefetch and operation-fusion technique (Liu
et al., 2022a); (2) the existing implementation can still yield a large speedup when employing larger
batch sizes (Figure 9).
Potential Negative Societal Impacts. Our research primarily focuses on reducing the memory
requirement of fine-tuning Language Models (LMs). The carbon emissions produced by LM fine-
tuning may pose environmental issues. Our next step is to further improve the efficiency of LM
fine-tuning, particularly on hardware with lower energy consumption.
B Unbiasedness of Weight Gradient
This part we directly follow the proof of Theorem 1 in ActNN (Chen et al., 2021c). For completeness,
we provide the proof sketch here that is short and easy to follow. Specifically, here we use ReLU
as the activation function for illustration convenience. We note that the conclusion in this section
holds for any non-linear activation function. Specifically, the forward pass of ReLU-Linear at the lth
2https://github.com/parasj/checkmate/issues/153
16

--- PAGE 17 ---
layer is
Z(l+1)=H(l)W(l),
H(l+1)=ReLU (Z(l+1)),
and the backward pass of ReLU is:
E[∇Z(l+1)] =E[ 1Z(l+1)>0⊙ ∇H(l+1)]
= 1Z(l+1)>0⊙E[∇H(l+1)],
where ⊙is the element-wise product and 1is the indicator function. The element-wise product
is linear operation and 1Z(l+1)>0is only related to the pre-activation Z(l+1)in the forward pass.
We only apply the approximation during the backward pass so 1Z(l+1)>0can be extracted from
the expectation. We know that for the last layer L, we have E[∇H(L)] =H(L)since we do not
apply activation at the output layer. We then can prove by induction that E[∇H(l+1)] =H(l+1)and
E[∇W(l)] =W(l)for any layer l.
C Proof
C.1 Derivation of Equation (3)
LetX∈Rn×m,Y∈Rm×qbe two matrices. The matrix multiplication XYcan be estimated as
GEMM(X,Y) =mX
i=1X:,iYi,:≈kX
t=11
kpitX:,itYit,:=X′Y′,
Equation(3)showstheapproximationerror E[||XY−X′Y′||F]isminimizedwhentheprobabilities
pi=||X:,i||2||Yi,:||2Pm
j=1||X:,j||2||Yj,:||2.
Proof.Letf(i) =X:iYi:
pi∈Rn×q. We note that f(i)is an unbiased estimation of XY. Namely,
Ej∼P[f(j)] =mX
i=1piX:,iYi:
pi=XY.
Then we have
X′Y′=1
kkX
t=1f(it), (8)
where i1,···, itare the index of the sampled column-row pairs at tthrandom trials. For each it, its
variance is
17

--- PAGE 18 ---
Var[f(it)] = Var[X:itYit:
pit]
=E[X2
:itY2
it:
p2
it]−E2[X:itYit:
pit]
=E[X2
:itY2
it:
p2
it]−(XY)2.
=mX
t=1X2
:tY2
t:
pt−(XY)2. (9)
where the first step follows from the fact that Var[x] =E[x2]−E2[x].
Then we have,
E[||XY−X′Y′||F] =nX
i=1qX
j=1E[(XY−X′Y′)2
ij]
=nX
i=1qX
j=1Var[(X′Y′)ij].
By combining Equation (8) and Equation (9) into the above equation, we have
E[||XY−X′Y′||F] =1
knX
i=1qX
j=1mX
t=1X2
itY2
tj
pt−1
k∥XY∥2
F.
=1
kmX
t=1∥X:,t∥2
2∥Yt,:∥2
2
pt−1
k∥XY∥2
F.
To minimize E[||XY−X′Y′||F], the optimal probability distribution can be obtained via solving
the following optimization problem:
min
p1,···,pmmX
t=1∥X:,t∥2
2∥Yt,:∥2
2
pt,
s.t.mX
t=1pt= 1.
The solution to the above convex problem is the distribution defined in Equation (3). Namely,
pi=||X:,i||2||Yi,:||2Pm
j=1||X:,j||2||Yj,:||2.
C.2 Unbiasedness of Our Proposed Estimator
Theorem 1 (Proof in Appendix C.2) .The estimator defined in Equation (4) is an unbiased estimator
for matrix production XY, i.e,Ej∼PD\C[P
c∈Cf(c)pc+ (1−P
c∈Cpc)f(j)] =XY.
18

--- PAGE 19 ---
Proof.
Ej∼PD\ChX
c∈Cf(c)pc+ (1−X
c∈Cpc)f(j)i
=X
c∈Cf(c)pc+ (1−X
c∈Cpc)Ej∼PD\C[f(j)]
=X
c∈Cf(c)pc+ (1−X
c∈Cpc)X
j∈D\Cpj
1−P
c∈Cpcf(j)
=X
c∈Cf(c)pc+X
j∈D\Cf(j)pj
=Ej∼P[f(j)]
=XY
C.3 Variance of Our Proposed Estimator
Theorem 2 (Proof in Appendix C.3) .Suppose the total budget of column-row pairs is k. IfC
satisfiesX
c∈Cpc>|C|
k, (7)
then we have Var[ˆg(X,Y)]<Var[g(X,Y)]. Moreover, Var[ˆg(X,Y)]is minimized when |C|=
min|C|∈{ 0,···,k}1−P
c∈Cpc
k−|C|.
Proof.Recall that the original estimator for matrix production XYis defined as
Ei∼P[f(i)]. (10)
and our proposed family of estimator is defined as:
h(j) =Ej∼PD\ChX
c∈Cf(c)pc+ (1−X
c∈Cpc)f(j)i
. (11)
We first define three independent random variables as belows:
u∼ PC, (12)
j∼ PD\C, (13)
b∼Bernoulli (1−X
c∈Cpc). (14)
According to the Law of total variance, we have
Var[f(i)] =Ebh
V ar[f(i)|b]i
+ Var bh
E[f(i)|b]i
≥Ebh
Var[f(i)|b]i
=X
c∈CpcVar[f(i)|b= 0] + (1 −X
c∈Cpc)Var[ f(i)|b= 1]
≥(1−X
c∈Cpc)Var[ f(i)|i∈ D\C ] (15)
19

--- PAGE 20 ---
where the first step follows from the fact that for any random variance x,y, we have Var[y] =
E[Var[y|x]] + Var[ E[y|x]]. Also, by Equation (11), we have
Var[h(j)] = (1 −X
c∈Cpc)2Var[f(j)|j∈ D\C ]. (16)
By combining the above two inequality, we have
Var[h(j)]≤(1−X
c∈Cpc)Var[ f(i)]. (17)
Equation (17) quantitatively shows the variance reduction of h(j)overf(i). Then we compare
our estimator ˆg(X,Y)andg(X,Y)in terms of variance.
First, because g(X,Y) =1
kPk
t=1f(it), i 1,···iki.i.d∼ P. Thus we have
Var[g(X,Y)] =1
kVar[f(i)]. (18)
Similarly, we have
Var[ˆg(X,Y)] =1
k− |C|Var[h(j)]. (19)
By combining Equation (17) into the above two equations, we have
Var[ˆg(X,Y)] =1
k− |C|Var[h(j)] (20)
≤1−P
c∈Cpc
k− |C|Var[f(i)]
≤1−P
c∈Cpc
k− |C|kVar[g(X,Y)],
where the first step follows from Equation (17). By setting1−P
c∈Cpc
k−|C|k≤1, we arrive the
conclusion that whenP
c∈Cpc>|C|
k, we have Var[ˆg(X,Y)]≤Var[g(X,Y)].
Further,1−P
c∈Cpc
k−|C|kachieves the minimal when |C|= min |C|∈{ 0,···,k}1−P
c∈Cpc
k−|C|.
D Implementation Details
The pseudocode for approximated linear layer with WTA-CRS and standard line layer is given in
Algorithm 1 and Algorithm 3, respectively. The column-row pair sampling procedure is given in
Algorithm 2. For the ease of illustration, we ignore the sequential length. As we mentioned in the
main text, we only replace the GEMMin the backward pass with WTA-CRS . According to Equation (1c),
we need the activation gradient ∇Zto perform the column-row pair sampling during the forward
pass. Thus we initialize a cache in CPU memory to store the gradient norm of activations from
the last step. When performing column-row pair selection, we need to swap the gradient norm of
activations between CPU and GPU, which will cause extra time overhead due to the data movement.
Fortunately, we note that the number of elements in the gradient norm of activations is significantly
less than the one in activations, which does not cause a significant time overhead.
20

--- PAGE 21 ---
Algorithm 1: Forward & Backward pass of Approximated Linear Layer
Hyperparameter: The total budget of column-row pairs k.
procedure Init:
Initialize Cache ∈RNas an empty matrix in main memory //Nis the total number
of samples in the dataset. Cache is used for saving the norm of output
gradient ∇Z.
end
procedure Forward Pass :
Input:activation H∈RB×D, weight W∈RD×D, indices of the current batch samples
BI={j1,···, jB}.
ctx← {} // the context which saves tensors for backward
Z=HW
H′, ind←Subsample (H,Cache[BI],k)
// Cache[ BI] is the cached gradient norm from the backward pass; ind is
the set of involved column-row pair indices
xctx← {H′,W, BI, ind }
return Z
end
procedure Backward pass :
Input: ctxfrom the forward pass, output gradient ∇Z∈RB×D
H′,W, BI, ind ←ctx
∇H=∇ZW⊤
∇Z′← ∇Z[ind]
//∇Z′∈Rk×D
∇W=H′⊤∇Z′
forjinBIdo
Cache[j] =∥∇Zj,:∥2
end
// Update the gradient norm of samples in the current batch
return ∇H,∇W
end
21

--- PAGE 22 ---
Algorithm 2: Subsample
Input:activation H∈RB×D, gradient norm z∈RB, the total budget of column-row pairs
k.
fori= 1,···, Bdo
pi←zi||Hi,:||2PB
j=1zi||Hj,:||2// The probability of column-row pairs defined in
Equation (3).
end
ˆk←min ˆk∈{0,···,k}1−P
c∈Cpc
k−ˆk, s.t.C=|ˆk|.//Cis the set of column-row pair indices
associated with |C|largest pi.
Sample k− |C|i.i.d. column-row pairs Cstoc={i1,···, ik−|C|}from the distribution PD\C
ind← C ∪ C stoc
forj∈ C stocdo
H[j,:]←H[j,:]∗1−P
c∈Cpc
(k−|C|)pj// We need to normalize the stochastic part in
Equation (6) to ensure the unbiasedness.
end
H′←H[ind]//H′∈Rk×D
return H′, ind
Algorithm 3: Forward & Backward pass of the standard Linear layer
procedure Forward Pass :
Input:activation HQ∈RBS×D, weight WQ∈RD×D, batch indices index
ctx← {} // the context which saves tensors for backward
ZQ=HQWQ
ctx← {HQ,WQ}
return ZQ
end
procedure Backward pass :
Input: ctxfrom the forward pass, output gradient ∇ZQ
HQ,WQ←ctx
∇HQ=∇ZQW⊤
Q
∇WQ=H⊤
Q∇ZQ
return ∇HQ,∇WQ
end
22

--- PAGE 23 ---
E More Experimental Results
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.6Probability Mass 
cpc
Query Layer (k = 0.1||)
2-th layer
4-th layer
6-th layer
||
k Ratio
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.6Key Layer (k = 0.1||)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.20.40.6Value Layer (k = 0.1||)
Fig. 10. The probability massP
c∈Cpcversus|C|
kin Equation (7) at k= 0.1|D|. Here we visualize the
column-row index distribution of query/key/value layer T5-base model, fine-tuned on RTE dataset.
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.250.500.75Probability Mass 
cpc
Query Layer (k = 0.5||)
2-th layer
4-th layer
6-th layer
||
k Ratio
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.250.500.75Key Layer (k = 0.5||)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
||
k Ratio
0.250.500.75Value Layer (k = 0.5||)
Fig. 11. The probability massP
c∈Cpcversus|C|
kin Equation (7) at k= 0.5|D|. Here we visualize the
column-row index distribution of query/key/value layer T5-base model, fine-tuned on RTE dataset.
0 20 40 60 80 100
# Iteration0.60.7Probability Mass 
cpc
Query Layer (||
||=0.1)
2-th layer
4-th layer
6-th layer
0 20 40 60 80 100
# Iteration0.60.7Key Layer (||
||=0.1)
0 20 40 60 80 100
# Iteration0.60.7Value Layer (||
||=0.1)
Fig. 12. The probability mass of top- 10%column-row pairs in Equation (3) versus iterations. Here
we visualize the query/key/value layer T5-base model, fine-tuned on RTE dataset.
E.1 More Experimental Analysis on Theorem 2
To evaluate Theorem 2 more comprehensively, below we also plot theP
c∈Cpcversus|C|
katk= 0.1|D|
andk= 0.5|D|in Figure 10 and 11, respectively. We also plotP
c∈Cpcversus iterations in Figure
12. We summarize that the the column-row index distribution is concentrated on a few column-row
pairs. Thus, the assumption in Theorem 2 holds under the context of fine-tuning transformers.
E.2 More Experimental Speed Analysis
Increasing the batch size can often result in faster model convergence and/or enhance the final
performance. Ideally, we should adjust the batch size according to the requirements of our model
23

--- PAGE 24 ---
MethodT5-
ATTT5-
FFT5
BlockT5-
Large
FwdFull 8 10 17 1052
WTA-CRS 22 16 37 2013
BwdFull 16 19 34 2073
WTA-CRS 15 14 30 1738
F-BFull 24 29 51 3125
WTA-CRS 37 30 67 3751
Table 3: Latency (ms) of Forward and Backward pass.
rather than being constrained by the GPU’s memory capacity. To illustrate this, we have represented
the correlation between peak memory usage and maximum mini-batch size for T5-Base, T5-Large,
and T5-3B in Figure 13. Our observations highlight that WTA-CRS effectively increases the maximum
available batch size.
We also provide the apple-to-apple speed comparison for linear operation with and without
WTA-CRS in Table 3. In Table 3, “Fwd”, “Bwd”, and “F-B” are the time of forward pass, the time
of backward pass, and the total time for both the forward and backward pass, respectively. We
summarize that under the same workload, the current implementation of WTA-CRS may roughly
slow down the linear operation about 20%. This is because the extra sampling process and data
movement counteract the acceleration (see Algorithm 1). However, we note that (1) the overhead can
be greatly reduced with better implementation, e.g., using prefetch and operation-fusion technique
(Liu et al., 2022a); (2) the existing implementation can still yield a large speedup when employing
larger batch sizes (Figure 9).
500 1000 1500
Batch-size20406080Peak Mem cost (GB)
Full
LoRA
LoRA+WTA-CRS@0.3
LoRA+WTA-CRS@0.1
(a) T5-Base
200 400 600
Batch-size20406080Peak Mem cost (GB)
Full
LoRA
LoRA+WTA-CRS@0.3
LoRA+WTA-CRS@0.1 (b) T5-Large
100 200
Batch-size406080Peak Mem cost (GB)
Full
LoRA
LoRA+WTA-CRS@0.3
LoRA+WTA-CRS@0.1 (c) T5-3B
Fig. 13. Peak memory usage versus maximum mini-batch size of T5.
F Experimental Settings
We give the detailed hyper-parameter setting in this section. Specifically, for both T5 and BERT
models, the parameters are updated with the AdamW optimizer β1= 0.9β2= 0.999ϵ= 10−8and
weight decay = 0. The the learning rate is adjusted with a linear LR Scheduler, which maintains
a constant learning rate for the initial 500 steps, and adjusts it gradually thereafter. The input
sequences are padded to the maximum length 128. WTA-CRS has a LoRA dimension 32 if it is
24

--- PAGE 25 ---
combined with LoRA. To achieve the optimal solution, the T5-Base, Large, 3B and BERT-Base and
Large models have different learning rate, training epoch number, and mini-batch size on different
datasets, which are given in Tables 5, 6, 7, respectively.
F.1 Computational Infrastructure
The computational infrastructure information is given in Table 4.
Table 4: Computing infrastructure for the experiments.
Device Attribute Value
Computing infrastructure GPU
GPU model NVIDIA-A100
GPU Memory 81251MB
CUDA Version 11.4
CPU Memory 512GB
25

--- PAGE 26 ---
Table 5: Learning rate.
Model Method CoLA SST-2 MRPC QQP MNLI QNLI RTE STS-B
BERT-BaseWTA-CRS@0.3 2e-5
LoRA+ WTA-CRS@0.3 2e-4 5e-4 2e-4 3e-4 3e-4 2e-4 2e-4 3e-4
T5-BaseWTA-CRS@0.3 3e-5 3e-6 3e-5 3e-5
WTA-CRS@0.1 3e-5
LoRA+ WTA-CRS@0.3 3e-4 3e-5 3e-4 3e-5 3e-5 3e-5 3e-4 3e-4
LoRA+ WTA-CRS@0.1 3e-4 3e-5 3e-4 3e-5 3e-5 3e-5 3e-4 3e-4
BERT-LargeWTA-CRS@0.3 2e-5
LoRA+ WTA-CRS@0.3 3e-4 2e-4 2e-4 2e-4 2e-4 2e-4 3e-4 3e-4
T5-LargeWTA-CRS@0.3 3e-5 3e-6 3e-5 3e-5
WTA-CRS@0.1 3e-5 3e-6 3e-5 3e-5
LoRA+ WTA-CRS@0.3 3e-4 3e-5 3e-4 3e-5 3e-5 3e-5 3e-4 3e-4
LoRA+ WTA-CRS@0.1 3e-4 3e-5 3e-4 3e-5 3e-5 3e-5 3e-4 3e-4
T5-3BLoRA+ WTA-CRS@0.3 3e-4 3e-5 3e-4 3e-4 3e-4 3e-5 3e-4 3e-4
LoRA+ WTA-CRS@0.1 3e-4 3e-5 3e-4 3e-4 3e-4 3e-5 3e-4 3e-4
Table 6: Training epoch number.
Model Method CoLA SST-2 MRPC QQP MNLI QNLI RTE STS-B
BERT-BaseWTA-CRS@0.3 20 20 10 10 10 10 20 10
LoRA+ WTA-CRS@0.3 60 20 20 20 20 20 40 40
T5-BaseWTA-CRS@0.3 40 10 20 10 10 10 50 20
WTA-CRS@0.1 40 10 20 10 10 10 50 20
LoRA+ WTA-CRS@0.3 40 10 20 20 20 10 50 20
LoRA+ WTA-CRS@0.1 40 10 20 20 20 10 50 20
BERT-LargeWTA-CRS@0.3 60 20 20 10 10 10 40 10
LoRA+ WTA-CRS@0.3 60 20 20 20 20 20 40 40
T5-LargeWTA-CRS@0.3 20 10 20 10 10 10 40 20
WTA-CRS@0.1 20 10 20 10 10 10 40 20
LoRA+ WTA-CRS@0.3 40 10 40 10 10 10 60 20
LoRA+ WTA-CRS@0.1 40 10 20 10 10 10 60 20
T5-3BLoRA+ WTA-CRS@0.3 40 10 20 10 10 10 60 20
LoRA+ WTA-CRS@0.1 40 10 20 10 10 10 60 20
Table 7: Training mini-batch size.
Model Method CoLA SST-2 MRPC QQP MNLI QNLI RTE STS-B
BERT-Base/LargeWTA-CRS@0.3 128 16
LoRA+ WTA-CRS@0.3 128 16
T5-Base/Large/3BWTA-CRS@0.3 100
WTA-CRS@0.1 100
LoRA+ WTA-CRS@0.3 100
LoRA+ WTA-CRS@0.1 100
26
