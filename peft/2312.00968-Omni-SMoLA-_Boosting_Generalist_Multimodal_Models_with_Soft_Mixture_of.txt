# 2312.00968.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2312.00968.pdf
# File size: 1108448 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of
Low-rank Experts
Jialin Wu Xia Hu Yaqing Wang Bo Pang Radu Soricut
Google Research
{jialinwu, xiahu, yaqingwang, bopang, rsoricut }@google.com
Abstract
Large multi-modal models (LMMs) exhibit remarkable
performance across numerous tasks. However, general-
ist LMMs often suffer from performance degradation when
tuned over a large collection of tasks. Recent research
suggests that Mixture of Experts (MoE) architectures are
useful for instruction tuning, but for LMMs of parameter
size around O(50-100B), the prohibitive cost of replicating
and storing the expert models severely limits the number
of experts we can use. We propose Omni-SMoLA, an ar-
chitecture that uses the Soft MoE approach to (softly) mix
many multimodal low rank experts, and avoids introducing
a significant number of new parameters compared to con-
ventional MoE models. The core intuition here is that the
large model provides a foundational backbone, while differ-
ent lightweight experts residually learn specialized knowl-
edge, either per-modality or multimodally. Extensive ex-
periments demonstrate that the SMoLA approach helps im-
prove the generalist performance across a broad range of
generative vision-and-language tasks, achieving new SoTA
generalist performance that often matches or outperforms
single specialized LMM baselines, as well as new SoTA spe-
cialist performance.
1. Introduction
Large multimodal models (LMMs) [7–9, 14, 33, 53]
demonstrate remarkable performance on a variety of tasks
including visual question answering, image captioning, vi-
sual document understanding, etc. To date, the best perfor-
mance on most of these tasks is achieved by so-called spe-
cialist LMMs, but their large scale makes it impractical to
deploy a multitude of such specialists at once. As a result,
so-called generalist LMMs emerge as an obvious choice,
where such a model is trained and deployed to handle a wide
range of tasks using the same set of model parameters.
Building a single generalist model to solve multiple tasks
remains challenging. A straightforward approach is to fine-tune the model parameters with supervised data represent-
ing multiple tasks. However, recent research suggests that
it causes non-negligible performance degradation compared
to the performance of a single-task specialist [7]. It is
likely that, even though these tasks share the same config-
uration of modalities (e.g., image + text as input, text as
output), what the model needs to solve for is significantly
diverse – for instance, some tasks require recognizing the
fine-grained identity of visual content, others may rely on
world-knowledge outside of the visual scene, while others
require reading and understanding texts from images.
Recent work [48] show that Mixture-of-Experts (MoE)
models stand to benefit more from instruction tuning com-
pared to dense models, and serve as good candidate archi-
tectures for building generalist large language models. Intu-
itively, this should work well because different expert mod-
ules can specialize and handle different tasks. However,
there is an obvious issue with applying the MoE design
on Transformer blocks for large-scale models: the differ-
ent transformer blocks result in replicating the model pa-
rameters using high-rank experts. This creates a situation
in which the scale of each expert model block compared to
their dense-model counterparts is much more limited.
In this work, we address the aforementioned limita-
tions by introducing Omni-SMoLA, an architecture that ef-
ficiently mixes many multi-modal low rank experts. Us-
ing this architecture, we demonstrate strong capabilities for
adapting pretrained models to tackle specialized tasks. The
core intuition is that a large pretrained (or instruction-tuned)
model provides a foundational backbone of capabilities (we
denote this model by θ∗), while different lightweight ex-
perts learn additional specializations (which can be knowl-
edge, style, or capabilities). In particular, for the modalities
considered in this paper (text & vision), the Omni-SMoLA
architecture consists of three sets of experts, focusing on
text tokens, visual tokens and multimodal tokens, respec-
tively, in order to satisfy different needs from various tasks.
In general, the SMoLA design has several important pro-
prieties. First, due to its adoption of the low rank expert
design [43] and unlike conventional MoE transformer mod-
1arXiv:2312.00968v2  [cs.CV]  2 Apr 2024

--- PAGE 2 ---
els [15, 18, 30, 48], the total parameter count is not pro-
portional to the product of expert counts and the parameter
counts in each expert as the backbone still contains major-
ity of the parameters. This allows it to bypass the limitation
on the number of experts used, which helps achieve better
generalist performance. Second, this design is potentially
compatible with any large model architecture, either dense
or MoE. And, last but not least, it allows for the freedom
to potentially adopt different model architectures between
the pretraining stage and multi-task learning (or instruction
tuning) stage.
We evaluate the Omni-SMoLA approach on a variety
of settings, starting from PaLI-3 [8] (a 5B LMM) and
PaLI-X [7] (a 55B LMM), models that have current state-
of-the-art (SOTA) performance across a wide range of
vision-language benchmarks. The settings include vari-
ous image captioning tasks and visual question answering
tasks, and we experiment with possible combinations in
terms of model specialization. We find that: (1) Omni-
SMoLA achieves better average performance compared to
full-model fine-tuning baselines for both PaLI-3 and PaLI-
X; our experiments show that it achieves new SoTA results
on multiple vision-language benchmarks, both under gen-
eralist settings and under specialist settings; (2) the perfor-
mance improves with the introduction of the Omni experts,
and also increases with the number of experts; (3) in spite of
the added modules and a large number of experts per mod-
ule, the inference speed is only slightly slower compared
the base models, indicating the efficiency of this design.
2. Related Work
2.1. Large Multi-modal Models
Inspired by the success of Large Language Model [5, 11,
13], there is a growing interest of building large multi-
modal models (LMMs) [8, 9, 14, 33] that is designed to
understand both vision and language signals simultane-
ously [14, 32]. The main approach is to integrate a pre-
trained image encoder, which represents images as a se-
quence of continuous embeddings, to autoregressive lan-
guage model [7–9, 14, 33, 53]. PaLI series of works [7–9]
integrate pretrained ViT models [ ?] to encoder-decoder
language framework. PaLM-E [14] incorporate vision en-
coders as sensor modalities to language model and enable
the model to process multiple images in text sentences in a
flexible way. BLIP-2 [33] propose a lightwieght Querying
Transformer to leverage frozen pre-trained image encoder
and language model for multimodal tasks.
2.2. Parameter-Efficient Fine-Tuning
Recently the success of scaling up model size encourage
the development of larger language models [7, 10, 44, 52].
Meanwhile, parameter-efficient fine-tuning [3, 22, 24, 42,45, 51, 52] aims to discover a more efficient solution to
adapt large models to particular downstream tasks. Instead
of full model fine-tuning which updates the entire set of
model parameters, parameter-efficient fine-tuning updates
or adds a relatively small number of parameters and leaves
the rest of model parameters fixed [52]. FISH Mask[51] ap-
plies a fixes sparse mask on model parameters and only up-
dates mask-selected parameters. Adapters [3, 22, 42, 45] in-
serts new trainable dense layers into Transformer and leave
the original model parameters frozen. Prefix-tuning [34]
and prompt-tuning [31] freeze parameters of the model
and learn continuous prompts. LoRA [24] injects trainable
low-rank decomposition matrices into every layer of Trans-
former and freezes the pretrained language model parame-
ters. In particular, LoRA shows outstanding capability to
achieve competitive or even better performance than fine-
tuning with only 0.1%trainable parameters [24, 57]
2.3. Mixture-of-Experts for Multitask Learning
Mixture-of-Experts (MoE) architectures are centered
around enhancing conditional computation capabilities and
scale parameters in neural architectures such as Transform-
ers. The MoE transformer models [17, 29, 46, 61] typi-
cally employ Nfeed-forward networks, referred to as “ex-
perts”. Each of these experts has its unique set of trainable
weights, enabling them to craft distinct representations for
each input token based on contextual information. Multi-
task learning (MTL), a popular ML topic for many years,
aims at finding solutions to simultaneously improving per-
formance on multiple tasks of interests [6, 35]. Recently,
mixture-of-experts (MoE) [25, 26, 47] approaches have be-
come a promising approach for MTL [16], benefiting from
its strategy of separating the parameter space and adopt rel-
evant model part to different tasks.
Inspired by these advances, there is an increasing inter-
est in investigating the application of MoEs in Transformer-
based large models. Some methods adopt MoE in Trans-
former structure of large language models [15, 18, 30, 48].
Gshard [30] introduces the idea of scaling Transformer in
LMMs with MoE layers, in which the feed forward layer
of every other Transformer is replaced by a Sparsely-Gated
MoE layer. This MoE Transformer structure is then used in
[15] to develop a family of Decoder-only language models,
and [48] which finds MoE modified LLM models benefits
more from instruction tuning than dense LLMs.
Some other methods explore combining MoE with
parameter-efficient fine-tuning. AdaMix [56] proposes a
mixture-of-adapters mechanism to improve per-task tuning
performance. The most relevant work is the concurrent re-
search [60] that introduces mixture of LoRA by weighted
summing of different LoRA outputs. While conceptually
similar, our SMoLA approach differs by having signifi-
cantly lower computational cost, and also allowing hun-
2

--- PAGE 3 ---
dreds of experts to handle single and multiple modalities
with negligible inference speed cost. We find that scaling to
hundreds of experts is crucial to attaining improved gener-
alist performance.
3. Methodology
3.1. Preliminaries
Low-rank Adaptation (LoRA). Low-Rank Adaptation
(LoRA) [23] is a technique designed to enhance the adapt-
ability of pretrained transformer models to new tasks with a
minor increase in trainable parameter counts. It can be ap-
plied on any linear layers, offering great compatibility with
recent large models.
We denote W∈Rd1×d2as the weight matrix for a linear
layer from the large model. LoRA introduces two low-rank
matrices Win∈Rr×d1andWout∈Rd2×rfor each layer,
where r≪min{d1,d2}. The WinandWoutare consecu-
tively applied to the input of the linear layer to project the
input to a low rank space and then project back to the out-
put space. The adapted weights W′can be represented as
W′=W+WoutWin. As the rank of WinandWoutis lim-
ited by rand typically much smaller than d1andd2, the
LoRA approach serves as a compact and efficient adapta-
tion mechanism.
Soft Mixture of Experts (Soft MoE). We briefly recap
the Soft MoE model in this section (details can be found in
[43]). The core idea is to learn a dispatcher module that can
dispatch input tokens to different experts, and a combiner
module that can combine the results from all the experts
and project them back to the original token space.
We denote the input to the transformer block as X∈
RN×d1, consisting of Ntokens. Soft MoE introduces a rout-
ing matrix Φ∈RE×d1that corresponds to Eexperts. The
dispatcher and combiner are represented by Eq. 1 and 2:
norm denotes l2 normalization and αis a learnable scalar.
D=softmax (α·norm (Φ)norm (X)T,axis=1 )(1)
C=softmax (α·norm (Φ)norm (X)T,axis=0 )(2)
Each expert model fi(usually MLP Blocks) operates on
the corresponding slice of dispatched inputs ˜xi= (DX)i,:
to produce ˜yi=fi(˜xi). Then, the combiner Cprojects the
output ˜Y= [˜y0,˜y1, ...˜yE-1]to the token space Y=CT˜Y.
3.2. SMoLA Block
Conventional MoE design employs high rank experts in
their MLP blocks that directly learn to handle different in-
puts. Therefore, these experts are parameter-heavy and re-
quire expensive pretraining. The SMoLA approach relies
on adding (to an original base model denoted as θ∗) ex-
perts that use a Soft MoE architecture, while simultane-
ously avoiding significantly increasing the parameter countby soft-mixing many zero-initialized low-rank experts. In-
tuitively, the original base model θ∗serves as a founda-
tional backbone, and the additional low-rank experts serve
as “specialists” that gather additional specialized knowl-
edge and handle different use cases.
The base model θ∗can be initialized with either pre-
trained (raw), multitask-tuned, or instruction-tuned check-
points. Using a raw checkpoint provides a more general
backbone, while a multitask-tuned checkpoint provides a
backbone focused on a required skill-set of the involved
tasks – we consider the decision of whether to use one or
the other as a backbone to be application-dependent. Our
choice for Soft MoE [43] to instantiate the SMoLA block
follows from the desirable properties this architecture ex-
hibits: fully differentiable, with no token dropping, and no
expert balance issues.
The right part of Fig 1 presents a SMoLA block. SMoLA
operates on linear layers for the maximum flexibility and
compatibility. We denote W∗(W∗∈Rd1×d2) as the weight
matrix of a linear layer in the base model θ∗andX∈RN×d1
as the input with Ntokens. Following [43], we introduce
the routing matrix Φ∈RE×d1and compute the dispatcher
D∈RE×Nand the combiner C∈RE×Nusing Eq. 1 and 2
for the Eexperts.
SMoLA adopts a LoRA-inspired approach for the expert
blocks. We introduce trainable low-rank matrices Wout
i,Win
i
for the i-th expert, producing the output ˜yias in Eq. 3.
˜yi=Wout
iWin
i(DX)T
i,: (3)
Then, the output of the SMoLA Ycombines the outputs of
each expert and the original linear outputs, as in Eq. 4.
Y=XW∗+CT[˜y0,˜y1, ...˜yE-1] (4)
3.3. Omni-SMoLA
By default, SMoLA blocks take as inputs all the tokens, re-
gardless of their modality (denoted by SMoLA MMin the next
section). However, we note that various multimodal tasks
may place a different emphasis on how different modalities
are used. For example, image captioning relies more on the
visual tokens, VQA tasks on text-heavy images and using
upstream OCR focuses more on text, while natural-image
VQA must rely on both the visual and text tokens.
Inspired by [55], SMoLA can be seamlessly configured
to only adapt tokens for selected modalities. We denote the
SMoLA blocks that only take visual tokens or text tokens
asSMoLA VorSMoLA T, respectively. SMoLA MMrefers to
the SMoLA blocks that take both visual and text tokens. As
shown in Figure 1, Omni-SMoLA (denoted by SMoLA Oin
the next section) combines via sum the original backbone
outputs with the outputs of SMoLA MMand the concatenated
outputs of SMoLA VandSMoLA T.
3

--- PAGE 4 ---
Pre-trained 
Weights (W*) 
Combiner 
Learnable Frozen Outputs 
Visual Tokens Text Tokens Visual 
SMoLA 
Text 
SMoLA 
Multi-Modal 
SMoLA 
Concatenation 
Dispatcher …
LoRA1 LoRA2 LoRA3 LoRA E 
 𝚽 SMoLA Block Figure 1. Omini-SMoLA model architecture contains three SMoLA blocks that take as input visual tokens, text tokens and multimodal
tokens. Each such block employs a large set of low-rank experts.
3.4. The Properties of Omni-SMoLA
Parameter Efficiency and Time Complexity. The inte-
gration of LoRA and Soft MoE results in a combination that
achieves a substantial reduction in the number of parameters
required for adaptation, compared to traditional MoE [30].
The low-rank matrices introduced by LoRA are of signifi-
cantly lower dimensionality than the full-rank feedforward
matrices, ensuring that the parameter increase is minimal
(and controlable via the rank hyperparameter). Not only
does this lead to a leaner model, but it also reduces memory
requirements, making it feasible to increase the number of
experts to enhance performance.
Moreover, the inference cost of applying Omni-SMoLA
is negligible. Let dmaxdenote max{d1,d2}andrdenote
the rank per expert, the time complexity of SMoLA blocks
per-layer is O(ENdmax+E(d1+d2)r). For one single
layer, it increases the cost from O(Nd1d2)toO(Nd1d2+
ENdmax+E(d1+d2)r). The number of expert Eis always
much smaller than min{d1,d2}, while the rank r(typi-
cally a small integer like 4) is much smaller than the input
tokens length, especially for multimodal settings where a
single high resolution image may easily be responsible for
thousands of visual tokens.
Alternative Scaling Dimension. Traditional scaling meth-
ods in neural networks often involve increasing the size
of the model, either by adding more layers or increasing
the dimensionality of the existing layers. The proposed
method, on the other hand, introduces an alternative scaling
dimension. By leveraging sparse activation and parameter-
efficient adaptation, the proposed method achieves scal-
ing through increasing the number of the low-rank experts,
which in turn does not result in a severe increase of total
model parameter size.
Extensibility for Future Growth. The design of the pro-
posed method inherently supports extensibility, accommo-
dating future growth and adaptations with ease. As the re-quirements of a task evolve, additional low-rank special-
ist modules can be seamlessly integrated into the architec-
ture, enhancing the model’s capability without necessitat-
ing a complete overhaul. This stands in stark contrast to
traditional scaling methods, which often require predefined
dimensions and layer numbers, limiting the model’s adapt-
ability to changing scenarios.
4. Experiments
4.1. Experimental setups
Training Mixtures. We considers three mixtures:
•Image Captioning mixture : COCO captions1[27] ,
Textcaps [49], VizWiz-Cap [21].
•VQA mixture : VQAv22[19], OK-VQA [37], VizWiz-
VQA [20], ST-VQA [4], TextVQA [50], OCRVQA [41],
InfoVQA [40], DocVQA [39], ChartQA [38], AI2D [28].
•Full mixture : combines the Image Captioning mixture
and the VQA mixture.
By default, we use the full mixture in our experiments to
simulate the scenario of mixing a wide variety of different
tasks. The only exception is Sec. 4.3.6, where we measure
the effect of using more focused mixtures.
Task Prompts. We do not use benchmark specific
prompts in order to achieve better versatility of the general-
ist models. Following [7] and [8], we use Generate the
alttext in {lang}at 0: as the captioning prompt
andAnswer in en: {question }as the VQA prompt.
1In keeping with the multilingual nature of PaLI models, here we used
a variant of the original English-only COCO captions that included trans-
lated captions for an additional 35 languages.
2Included translated questions for an additional 13 languages.
4

--- PAGE 5 ---
COCO NoCaps†VQAv2 OKVQA A-OKVQA†Sci-QA†TallyQA†
Model Kar.-test val test-dev val DA MC test simple complexSpecialistGIT2 [54] 145.0 126.9 81.7 - - - - -
BEiT-3 [55] 147.6 - 84.2 - - - - -
PaLM-E [14] 138.7 - 80.0 66.1 - - - -
InstructBLIP [12] - 123.1 - 62.1 62.1 73.4 90.7 - -
PaLI-X [7] 149.2 126.3 86.0 66.1 - - 86.0 75.6
CogVLM [58] 148.7 128.3 84.7 64.7 - - 92.7 - -GeneralistUnified-IO [36] 122.3 100.0 77.9 54.0 45.2 - - - -
Qwen-VL [2] - 121.4 79.5 58.6 - - 67.1 - -
CogVLM [58] 147.0 126.2 83.4 58.9 - - - - -
PaLI-3 FT 144.4 120.3 82.5 56.2 59.0 78.7 55.2 80.4 65.4
SMoLA48
O-PaLI-3 FT 146.5 120.3 83.6 58.2 59.8 79.3 55.8 81.8 65.1
PaLI-X FT 148.7 125.6 84.4 60.7 63.9 84.0 67.2 83.8 71.8
SMoLA48
O-PaLI-X FT 149.8 126.1 85.0 62.4 65.3 84.1 67.8 83.3 70.7
Table 1. Results on natural image captioning and question answering including COCO Captions (Karpathy split), NoCaps, VQAv2,
OKVQA, A-OKVQA, ScienceQA and TallyQA test split with end-to-end modeling without OCR pipeline input. Bold and underlined
numbers highlight best performance and best generalist performance, respectively.†denotes that there are no training examples from these
datasets during training (i.e. out-domain). The numbers in bracket denote the further per-task LoRA tuned performances. We use the same
SMoLA48
O-PaLI-X FTand SMoLA48
O-PaLI-3 FTto handle inferences in Table 1 and Table 2.
Text VizWiz Text VizWiz ST OCR Info DocAI2DChart
Model Caps Cap VQA VQA VQA VQA VQA VQA VQA
val test test test-dev test test test test test test
without OCR pipeline input
Specialist SOTA 158.8 [8] 122.7 [7] 79.5[8] 76.4[58] 84.1[8] 76.7[8] 57.8[8]‡87.6[8]‡81.2[7] 70.9[7]GeneralistUnified-IO [36] - - - 57.4 - - - - - -
Qwen-VL [2] - - 63.8 - - 75.7 - 65.1 62.3 65.7
CogVLM [58] 151.3 - 68.1 - - 74.1 - - - -
mPLUG-DocOwl [59] 111.9 - 52.6 - - - 38.2 62.2 - 57.4
SMoLA48
O-PaLI-3 FT 156.7 119.8 -∗70.4 83.8 72.8 52.4 84.5 75.6 68.9
SMoLA48
O-PaLI-X FT 144.6 120.3 70.5 71.7 78.9 71.6 49.2 80.1 81.4 71.3
with OCR pipeline input
Specialist SOTA 161.0 [8] 125.7 [7] 80.8 [7] 76.4[58] 85.7[8] 77.8[8] 62.4[8] 88.6[8] 81.4[7] 72.3[7]
SMoLA48
O-PaLI-3 FT 159.3 120.4 -∗71.0 85.9 73.9 57.3 87.4 75.5 68.9
SMoLA48
O-PaLI-X FT 154.7 124.6 81.1 73.8 86.0 74.9 65.6 90.6 81.4 73.8
Table 2. Results on benchmarks more focused on text understanding capabilities. Bold and underlined numbers highlight SOTA perfor-
mance and SOTA generalist performance, respectively.‡marks specialist results with a higher resolution of 1064 where SMoLA used 812.
We use the same SMoLA48
O-PaLI-X FTand SMoLA48
O-PaLI-3 FTto handle inferences with and without OCR pipeline input in Table 1 and
Table 2.∗Results are missing because test server is not available.
Base Models. We build SMoLA models on top of two
variants of PaLI models: PaLI-X [7] and PaLI-3 [8]. PaLI
models use contrastively pretrained ViT modules as the vi-
sual encoder to produce visual embeddings for input im-
ages; these visual embeddings are then concatenated with
text embeddings and passed to the encoder-decoder back-
bone. PaLI-X is a large-scale multimodal model that con-
tains around 55B parameters. We only experimented with
using the full-mixture in PaLI-X based experiments, where
we adopted a resolution of 672. PaLI-3 is a more nimble
variant. It is still highly performant with just around 5B pa-
rameters, achieving SOTA results on a broad range of image
captioning and VQA tasks that require text understandingcapabilities from images. For PaLI-3 based experiments,
we use a resolution of 812 for the full mixture and the im-
age captioning mixture, and 1064 for the VQA mixture.
Notation and implementation. We use SMoLAE
Y-PaLI-
3|XRAW|LoRA|FT to denote the config choices for SMoLA:
•Edenotes the number of experts for each individual
modality and for multimodal experts.
•Ydenotes the SMoLA’s modality configuration: MMorO.
• base model: PaLI-3 vs PaLI-X
• SMoLA’s initial checkpoint can be either the RAWcheck-
point of the base model, the base model tuned using LoRA
on a given training mixture, or full-model fine-tuned ( FT)
5

--- PAGE 6 ---
COCO Text VizWiz VQA OK Text VizWiz ST OCR Info DocAI2DChart Avg.
Model Cap Cap Cap v2 VQA VQA VQA VQA VQA VQA VQA VQA δ
K.test val test test-dev val val∗test-dev test test test test test test
with OCR pipeline input, except for COCO Cap, VQAv2, OKVQA
PaLI-3 Specialist 145.9 161.0 120.3 85.0 60.1 78.3 72.2 85.7 77.8 62.4‡88.6‡75.2 69.5 0.00
SMoLA48
O-PaLI-3 RAW 144.4 159.1 118.7 82.6 56.2 79.1 70.6 85.5 73.3 55.1 86.6 73.8 67.6 -2.26
PaLI-3 FT 146.2 161.0 121.1 82.5 56.4 78.7 69.9 84.9 72.7 54.3 85.9 72.8 65.8 -2.31
SMoLA96
MM-PaLI-3 FT 145.7 159.3 121.4 83.4 56.7 80.0 71.5 85.6 73.6 56.7 87.3 75.2 69.2 -1.26
SMoLA48
O-PaLI-3 FT 146.5 159.3 120.4 83.6 58.2 80.1 71.0 85.9 73.9 57.3 87.4 75.5 68.9 -1.07
K.test val test test-dev val test test-dev test test test test test test
PaLI-X Specialist 149.2 159.6 125.7 86.0‡66.1‡80.8‡74.6‡84.5‡77.3‡54.8‡86.8‡81.4‡72.3‡0.00
PaLI-X LoRA 147.3 159.3 125.1 83.5 57.4 78.9 69.6 84.8 72.3 61.4 88.3 78.8 70.9 -1.65
SMoLA48
O-PaLI-X LoRA 148.6 158.8 125.2 84.7 60.8 80.3 73.1 85.2 74.2 64.8 90.1 80.2 73.0 -0.01
PaLI-X FT 148.7 157.0 125.3 84.4 60.7 79.6 72.2 84.7 73.5 62.4 88.2 80.7 70.2 -0.88
SMoLA48
O-PaLI-X FT 149.8 154.7 124.6 85.0 62.4 81.1 73.8 86.0 74.9 65.6 90.6 81.4 73.8 +0.38
Table 3. Ablation results on image captioning and question answering benchmarks. Bold and underlined numbers highlight best perfor-
mance and best generalist performance, respectively.‡denotes the specialist results with a higher resolution of 1064 for PaLI-3 and 756
resolution for PaLI-X, where we uses 812 for PaLI-3 series and 672 for PaLI-X series.∗We use val split as TextVQA test server is broken.
using the training mixture. We use a rank of 128 for
LoRA tuning on all linear layers on the PaLI encoder.3
For simplicity, we assign the same number of experts to
each SMoLA block and use a rank of 4 per expert. SMoLA
is applied on all the linear layers in the attention and MLP
modules in PaLI encoder blocks. For example, SMoLA48
O-
PaLI-X FTwith full-mixture denotes starting with PaLI-X
finetuned on the full-mixture, and then SMoLA-tuned on
the same mixture using 48 visual-token experts, 48 text-
token experts, and 48 multimodal-token experts.
Checkpoint selection. We monitor the scores on the vali-
dation splits4every 500 iterations with at most 1,024 exam-
ples for each task and select the checkpoint with maximum
average validation scores.
4.2. Main Results
In this section, we present our main experimental results
using the full mixture. Recall that the full mixture contains
both image captioning and VQA tasks. We report SMoLA
results on the natural image tasks (as well as “out-domain”
tasks not included in the training mixture) in Table 1, and
results on tasks that focus on understanding texts in images
in Table 2. While results are split into these two tables for
easier consumption, they are from the same SMoLA-based
generalist models trained on one single mixture.
First, note that the generalist PaLI-X FT(PaLI-X fine-
tuned on the full-mixture) under-performs its specialist
counterparts (PaLI-X finetuned for each task individually)
on all the benchmark datasets shown in Table 1. Apply-
3LoRA with rank 512 did not achieve better overall performance.
4We use the Pix2Struct validation split for AI2D.ing SMoLA over PaLI-X FToutperformed the base general-
ist model across the board. It effectively shortened the gap
to specialist performances, and notably introduced a new
SOTA CIDEr score of 149.8 on COCO captioning, outper-
forming all the specialist models for that task.
It is important to note that Table 1 presents results for
both “in-domain” tasks that are included in the training mix-
ture (COCO captioning, VQAv2, and OKVQA), as well
as “out-domain” tasks (those marked with†). The in-
domain tasks simulate usecases where we are interested in
serving one single model for a set of known tasks. The
out-domain tasks simulate usecases where we want to ap-
ply a generalist model to unseen tasks in a zero-shot set-
ting. The trend we noted above holds for both cases:
SMoLA48
O-PaLI-X FToutperforms base model PaLI-X FTfor
both in-domain and out-domain tasks on average. Over-
all, SMoLA48
O-PaLI-X FTachieves new SoTA generalist re-
sults for all except NoCaps and TallyQA, and furthermore
beating fine-tuned specialist models for COCO (in-domain)
and A-OKVQA (out-domain). While PaLI-3 FTbased mod-
els overall under-performs PaLI-X FTbased models on this
set of tasks, SMoLA nonetheless improves the base model
performance consistently, demonstrating the effectiveness
of this technique for both large- and small-scale models.
Table 2 presents SMoLA results on text-heavy tasks in
two experimental setups: (a) relying solely on a model’s
text understanding capabilities from the raw pixels ( without
OCR input), and (b) including tokens extracted by an up-
stream OCR module as part of the text input ( with OCR in-
put). In the with-OCR setting, SMoLA48
O-PaLI-X FTshows
remarkable results: with one single model, it outperforms
specialist SoTA performance on 6 out of 10 datasets, yield-
ing new SoTA performance for TextVQA, ST-VQA, In-
6

--- PAGE 7 ---
VQAv2OK Text VizWiz ST OCR Info DocAI2DChart Avg.
Model VQA VQA VQA VQA VQA VQA VQA VQA δ
test-dev val test test-dev test test test test test test
without OCR pipeline input
PaLI-3 Specialist 85.0 60.1 79.5 71.9 84.1 76.7 57.8 87.6 75.2 70.0 0.0
PaLI-3 FT 82.1 57.9 79.8 69.2 84.0 72.5 55.9 87.6 74.2 68.0 -1.53
SMoLA48
O-PaLI-3 FT 83.4 57.7 80.0 70.8 84.0 73.4 57.3 87.8 75.9 70.1 -0.46
with OCR pipeline input
PaLI-3 Specialist - - 80.8 72.2 85.7 77.8 62.4 88.6 75.2 69.5
PaLI-3 FT - - 81.7 70.0 85.5 73.6 59.8 88.8 74.7 67.3
SMoLA48
O-PaLI-3 FT - - 82.2 72.0 85.8 74.6 61.1 89.3 76.0 70.4
Table 4. Generalist results using the VQA mixture. Bold numbers highlight the results outperforming single specialized PaLI-3 baselines,
and underlined numbers presents the results outperform multi-task fine-tuned baselines.
foVQA, DocVQA, AI2D and ChartQA. It also improves
over the base model PaLI-x FT(see Section 4.3.1). This indi-
cates that SMoLA is effective in enabling joint processing
of information across different modalities: text situated in
image, as well as text tokens extracted by the upstream OCR
module. In the without-OCR setting, SMoLA48
O-PaLI-3 FT
is able to take advantage of PaLI-3’s strong text under-
standing capability and achieves SOTA generalist score on
TextCaps, ST-VQA, InfoVQA, and DocVQA.
4.3. Ablation Studies
4.3.1 Different base models
In Section 4.2, we see strong performance from SMoLA48
O-
PaLI-X FT, starting from a strong checkpoint (full-model
PaLI-X finetuned). In this section, we examine the ef-
fect of switching to PaLI-X LoRA, which is LoRA-tuned on
the mixture and easier to obtain for large models. As
shown in Table 3, compared to their corresponding base
models, we find SMoLA helps both PaLI-X LoRA and PaLI-
XFTto achieve better overall results, obtaining +1.64 and
+1.26 improvements on average, respectively. While it is
slightly weaker than SMoLA48
O-PaLI-X FT, which outper-
forms per-task fine-tuned specialist models by an average
of 0.38 points, SMoLA48
O-PaLI-X LoRA still achieves com-
petitive performance versus the specialist models (on av-
erage only a difference of 0.01 point). It is worth noting
that the SMoLA design improves PaLI-X FTby +2.4 points
on DocVQA, +3.2 points on InfoVQA, and +3.6 points on
ChartQA, which all involve comprehending rich text and
symbols in images. We note some performance drop on the
TextCaps task, possibly due to overfitting and unambiguous
intention for image captioning tasks when the same prompt
is used for TextCaps and natural-image descriptions.
Table 3 also shows other ablation results on using differ-
ent starting checkpoints ( θ∗) for SMoLA. Similar observa-
tion holds for the PaLI-3-based models. For instance, apply-
ing SMoLA to the raw checkpoint ( i.e.PaLI-3 RAW) achievesbetter overall score than full model fine-tuning baseline
PaLI-3 FT, and applying SMoLA to PaLI-3 FTbrings it more
competitive against PaLI-3 specialists, outperforming per-
task finetuned baselines on 4 benchmarks. One exception is
InfoVQA where the specialist uses a higher resolution.
4.3.2 Effect of Using Multi-Modal Experts
We validate the omni experts design by comparing the av-
erage performance of using 48 experts on each combination
of modalities ( i.e.SMoLA48
O-PaLI-3 FT) to using 96 experts
on all tokens ( i.e.SMoLA96
MM-PaLI-3 FT). These two variants
introduce the same additional FLOPS during inference. As
shown in Table 3, SMoLA48
O-PaLI-3 FThas a slightly edge
in terms of average performance. This suggests that for the
same amount of extra compute, there can be a slight advan-
tage to allow modality-dependent SMoLA blocks.
4.3.3 Effect of Scaling Up the Expert Counts
Number of experts per modality1.01.11.21.3
4 16 48 144Average performance improvements over MT-FT
Figure 2. Average results of increasing number of experts.
We study the effect of scaling up the expert counts using
SMoLAE
O-PaLI-3 FT. Figure 2 plots the average improve-
ments over PaLI-3 FTusing 4, 16, 48, and 144 experts per
modality. The scores are averaged across the tasks pre-
7

--- PAGE 8 ---
sented in Table 3 on validation splits5except for InfoVQA.
With only 4 experts per modality, SMoLA already yields
around +1.1 improvements over PaLI-3 FT. Scaling up the
experts counts further improves the performance: 16, 48
and 144 experts provide 1.14, 1.2, 1.27 average points gain.
4.3.4 Further LoRA tuning to Push SOTA
We note there that the SMoLA48
O-PaLI-X FTgeneralist
can be considered a strong foundational model. With
further per-task LoRA tuning (using a rank of 4), the
SMoLA48
O-PaLI-X FTspecialists achieve better results than
the SMoLA48
O-PaLI-X FTgeneralist, yielding new SOTA re-
sults on 9 benchmark datasets: COCO caption, OKVQA,
DocVQA, InfoVQA, AI2D, ChartQA, A-OKVQA, Sci-
enceQA and TallyQA (Table 5). These new SOTA results
indicate the extensibility of the Omni-SMoLA design.
Model Split PaLI-X SOTA Ours
COCO K.test 149.2 149.2 [7] 152.1
VQA v2 test-dev 86.0 86.0 [7] 85.7
OKVQA val 66.1 66.1 [7] 66.7
VizWiz-VQA test-dev 74.6 76.4 [58] 75.9
OCRVQA test 77.3 77.8 [8] 75.7
DocVQA test 86.8 88.6 [8] 90.8
InfoVQA test 54.8 62.4 [8] 66.2
AI2D test 81.4 81.4 [7] 82.5
ChartQA test 72.3 72.3 [7] 74.6
A-OKVQADA (val) - 62.1 [12] 70.2
MC (val) - 73.4 [12] 88.2
ScienceQA test - 92.7 [58] 94.7
TallyQAsimple 86.0 86.0 [7] 86.3
complex 75.6 75.6 [7] 77.1
Table 5. Further LoRA tuning SMoLA48
O-PaLI-X FT
4.3.5 Inference Speed Comparison
We compare the inference speed by measuring the num-
ber of processed examples per second (eps) for the PaLI-
3FTmodel and SMoLA48
O-PaLI-3 FTwith a resolution of 812
in batch mode (size 128) using beam decoding (beam size
4) We use COCO caption as the evaluation task where the
length of outputs are around 10 tokens on average. We sam-
ple 18 forward batches to compute the statistics. PaLI-3 FT
processed 31.29±0.63examples per second and SMoLA48
O-
PaLI-3 FTprocessed 30.85±0.70examples per second,
yielding only 1.4% slow-down when using 48 experts in
each SMoLA block, on all linear layers in the PaLI encoder.
4.3.6 Effects of different training mixtures
We evaluate SMoLA on the VQA and captioning mixture
with PaLI-3 in order to exam its effectiveness when all train-
5We use test split for AI2D as there are only 120 examples in val split.ing tasks are under the same umbrella of either VQA or cap-
tioning. We adopt a resolution of 1064 for the VQA mixture
and 812 for the captioning mixture, and finetune the PaLI-3
raw checkpoints on each mixture as baselines.
VQA Mixture As shown in Table 4, while still underper-
forming the per-task fine-tuned specialist models by 0.46,
SMoLA improves over the PaLI-3 FTbaseline by +1.07 on
average. In particular, it helps the base model on most of
the tasks with and without OCR inputs except for a 0.2 per-
formance drop on OK-VQA. The significant performance
improvements over the PaLI-3 FTbaseline on InfoVQA and
ChartQA persist as observed when training with the full
mixture. Furthermore, it also helps the PaLI-3-based model
to achieve a new SOTA result of 82.2 on TextVQA.
Image Captioning Mixture Table 6 summarizes results of
applying SMoLA to PaLI-3 LoRA and PaLI-3 FTusing the im-
age captioning mixture. In these experiments, we use a res-
olution of 812. We observe similar trends as in the case of
using the full mixture: PaLI-3 FToutperforms PaLI-3 LoRA on
average, indicating the insufficiency of LoRA tuning on a
wide range of tasks; and SMoLA helps both the full model
fine-tuned and LoRA baseline achieve better average per-
formance. The SMoLA48
O-PaLI-3 FToutperforms the per-
task fine-tuned specialist models by 0.64 on average, and
sets a new SOTA for a generalist image-captioning system.
COCOTextCap VizWizCap Avg.
Model ocr×ocr✓ocr×ocr✓ δ
K. test val val test test
PaLI-3 Specialist 145.9 158.8 161.0 119.6 120.3 0.0
PaLI-3 LoRA 143.6 158.6 161.3 118.8 120.5 -0.56
SMoLA48
O-PaLI-3 LoRA 143.9 160.6 162.6 119.1 120.8 +0.28
PaLI-3 FT 145.0 159.9 160.9 120.3 120.9 +0.28
SMoLA48
O-PaLI-3 FT 146.5 159.5 161.7 120.5 120.6 +0.64
Table 6. Generalist results using the image captioning mixture.
Bold and underlined numbers highlight best performance and best
generalist performance, respectively.
5. Conclusion
In this work, we present Omni-SMoLA, a multimodal archi-
tecture that mixes many multi-modal experts efficiently and
achieves both high specialist and generalist performance. In
contrast to previous models for which we see performance
degradation on average when training the models on a wide
range of tasks, we show that the SMoLA low-rank experts
are able to model different skills and task, and overall im-
prove the performance of a generalist model. This finding
indicates that simple LMM fine-tuning is suboptimal for
handling a wide range of tasks, and that pairing the act of
fine-tuning with specifically-designed architecture changes
leads to better performing models.
8

--- PAGE 9 ---
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning,
2022.
[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 5
[3] Ankur Bapna and Orhan Firat. Simple, scalable adaptation
for neural machine translation. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Nat-
ural Language Processing (EMNLP-IJCNLP) . Association
for Computational Linguistics, 2019. 2
[4] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
Marc ¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-
thenis Karatzas. Scene text visual question answering. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 4291–4301, 2019. 4
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In Advances in Neural Information Pro-
cessing Systems , pages 1877–1901. Curran Associates, Inc.,
2020. 2
[6] Rich Caruana. Multitask learning. Machine learning , 28:
41–75, 1997. 2
[7] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,
Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-
eri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael
Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo
Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter,
AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin
Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer,
Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li,
Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong,
Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia An-
gelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
PaLI-X: On scaling up a multilingual vision and language
model, 2023. 1, 2, 4, 5, 8
[8] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,
Jialin Wu, Paul V oigtlaender, Basil Mustafa, Sebastian
Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, DanielSalz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong,
Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut.
PaLI-3 vision language models: Smaller, faster, stronger,
2023. 2, 4, 5, 8
[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,
Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James
Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,
Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and
Radu Soricut. PaLI: A jointly-scaled multilingual language-
image model. In The Eleventh International Conference on
Learning Representations , 2023. 1, 2
[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. PaLM: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 2
[11] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina,
Alena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben
Hutchinson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas,
Chung ching Chang, Claire Cui, Cosmo Du, Daniel De Fre-
itas Adiwardana, Dehao Chen, Dmitry (Dima) Lepikhin,
Ed H. Chi, Erin Hoffman-John, Heng-Tze Cheng, Hon-
grae Lee, Igor Krivokon, James Qin, Jamie Hall, Joe Fen-
ton, Johnny Soraker, Kathy Meier-Hellstern, Kristen Olson,
Lora Mois Aroyo, Maarten Paul Bosma, Marc Joseph Pick-
ett, Marcelo Amorim Menegali, Marian Croak, Mark D ´ıaz,
Matthew Lamm, Maxim Krikun, Meredith Ringel Morris,
Noam Shazeer, Quoc V . Le, Rachel Bernstein, Ravi Rajaku-
mar, Ray Kurzweil, Romal Thoppilan, Steven Zheng, Taylor
Bos, Toju Duke, Tulsee Doshi, Vincent Y . Zhao, Vinodku-
mar Prabhakaran, Will Rusch, YaGuang Li, Yanping Huang,
Yanqi Zhou, Yuanzhong Xu, and Zhifeng Chen. Lamda:
Language models for dialog applications. In arXiv . 2022.
2
[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500 , 2023. 5, 8
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Volume 1 (Long and Short
Papers) , pages 4171–4186, 2019. 2
[14] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Peter R. Florence. Palm-e: An embodied multimodal
9

--- PAGE 10 ---
language model. In International Conference on Machine
Learning , 2023. 1, 2, 5
[15] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi
Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient
scaling of language models with mixture-of-experts. In In-
ternational Conference on Machine Learning , pages 5547–
5569. PMLR, 2022. 2
[16] Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai
Zou, Yu Cheng, Cong Hao, Zhangyang Wang, et al. M3vit:
Mixture-of-experts vision transformer for efficient multi-
task learning with model-accelerator co-design. Advances
in Neural Information Processing Systems , 35:28441–28457,
2022. 2
[17] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. arXiv preprint arXiv:2101.03961 ,
2021. 2
[18] William Fedus, Barret Zoph, and Noam M. Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. J. Mach. Learn. Res. , 23:120:1–
120:39, 2021. 2
[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: Ele-
vating the role of image understanding in visual question an-
swering. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 6904–6913, 2017. 4
[20] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
Vizwiz grand challenge: Answering visual questions from
blind people. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3608–3617,
2018. 4
[21] Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhat-
tacharya. Captioning images taken by people who are blind.
InComputer Vision – ECCV 2020 , pages 417–434, Cham,
2020. Springer International Publishing. 4
[22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 2
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 3
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In In-
ternational Conference on Learning Representations , 2022.
2
[25] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and
Geoffrey E. Hinton. Adaptive mixtures of local experts. Neu-
ral Computation , 3:79–87, 1991. 2
[26] Michael I. Jordan and Robert A. Jacobs. Hierarchical mix-
tures of experts and the em algorithm. Neural Computation ,
6:181–214, 1993. 2[27] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In CVPR , 2015. 4
[28] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon
Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is
worth a dozen images. ArXiv , abs/1603.07396, 2016. 4
[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models
with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668 , 2020. 2
[30] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, De-
hao Chen, Orhan Firat, Yanping Huang, Maxim Krikun,
Noam M. Shazeer, and Z. Chen. Gshard: Scaling giant
models with conditional computation and automatic shard-
ing. ArXiv , abs/2006.16668, 2020. 2, 4
[31] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. In Proceed-
ings of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing , pages 3045–3059, Online and
Punta Cana, Dominican Republic, 2021. Association for
Computational Linguistics. 2
[32] Chunyuan Li. Large multimodal models: Notes on cvpr 2023
tutorial. arXiv preprint arXiv:2306.14895 , 2023. 2
[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models, 2023. 1,
2
[34] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. In Proceedings of
the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Pa-
pers) , pages 4582–4597, 2021. 2
[35] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and
Sam Kwong. Pareto multi-task learning. Advances in neural
information processing systems , 32, 2019. 2
[36] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-IO: A unified
model for vision, language, and multi-modal tasks. arXiv
preprint arXiv:2206.08916 , 2022. 5
[37] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. OK-VQA: A visual question answering
benchmark requiring external knowledge. In Proceedings
of the IEEE/cvf conference on computer vision and pattern
recognition , pages 3195–3204, 2019. 4
[38] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Ena-
mul Hoque. ChartQA: A benchmark for question answering
about charts with visual and logical reasoning. In Findings
of ACL , 2022. 4
[39] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
Docvqa: A dataset for vqa on document images. In Proceed-
ings of the IEEE/CVF winter conference on applications of
computer vision , pages 2200–2209, 2021. 4
[40] Minesh Mathew, Viraj Bagal, Rub `en Tito, Dimosthenis
Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 1697–1706, 2022. 4
10

--- PAGE 11 ---
[41] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. Ocr-vqa: Visual question answering
by reading text in images. In ICDAR , 2019. 4
[42] Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Ka-
math, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and
Iryna Gurevych. Adapterhub: A framework for adapting
transformers. Association for Computational Linguistics,
2020. 2
[43] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil
Houlsby. From sparse to soft mixtures of experts. arXiv
preprint arXiv:2308.00951 , 2023. 1, 3
[44] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-
can, Jordan Hoffmann, Francis Song, John Aslanides, Sarah
Henderson, Roman Ring, Susannah Young, et al. Scaling
language models: Methods, analysis & insights from train-
ing gopher. arXiv preprint arXiv:2112.11446 , 2021. 2
[45] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. Ad-
vances in neural information processing systems , 30, 2017.
2
[46] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. 2
[47] Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc V . Le, Geoffrey E. Hinton, and Jeff Dean.
Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer. ArXiv , abs/1701.06538, 2017. 2
[48] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Long-
pre, Jason Wei, Hyung Won Chung, Barret Zoph, William
Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen,
Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu,
Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-
experts meets instruction tuning:a winning combination for
large language models, 2023. 1, 2
[49] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
Amanpreet Singh. TextCaps: a dataset for image caption-
ing with reading comprehension. In European conference on
computer vision , pages 742–758, 2020. 4
[50] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards VQA models that can read. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 8317–8326, 2019. 4
[51] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neu-
ral networks with fixed sparse masks. Advances in Neural
Information Processing Systems , 34:24193–24205, 2021. 2
[52] Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken,
Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth
Heafield, Sara Hooker, Colin Raffel, et al. Efficient methods
for natural language processing: A survey. Transactions of
the Association for Computational Linguistics , 11:826–860,
2023. 2
[53] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. Advances in Neural
Information Processing Systems , 34:200–212, 2021. 1, 2[54] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
GIT: A generative image-to-text transformer for vision and
language. TMLR , 2022. 5
[55] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, and Furu Wei.
Image as a foreign language: Beit pretraining for all vision
and vision-language tasks. CoRR , abs/2208.10442, 2022. 3,
5
[56] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xi-
aodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jian-
feng Gao. Adamix: Mixture-of-adaptations for parameter-
efficient model tuning, 2022. 2
[57] Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang,
Geoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo
Pang, Michael Bendersky, et al. Non-intrusive adapta-
tion: Input-centric parameter-efficient fine-tuning for versa-
tile multimodal modeling. arXiv preprint arXiv:2310.12100 ,
2023. 2
[58] Wenmeng Yu Wenyi Hong Ji Qi Yan Wang Junhui Ji Zhuoyi
Yang Lei Zhao Xixuan Song Jiazheng Xu Bin Xu Juanzi
Li Yuxiao Dong Ming Ding Jie Tang Weihan Wang, Qing-
song Lv. Cogvlm: Visual expert for pretrained language
models, 2023. 5, 8
[59] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,
Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,
Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jin-
gren Zhou. mplug-2: A modularized multi-modal foundation
model across text, image and video. In International Con-
ference on Machine Learning, ICML 2023, 23-29 July 2023,
Honolulu, Hawaii, USA , pages 38728–38748. PMLR, 2023.
5
[60] Ted Zadouri, Ahmet ¨Ust¨un, Arash Ahmadian, Beyza Ermis ¸,
Acyr Locatelli, and Sara Hooker. Pushing mixture of experts
to the limit: Extremely parameter efficient moe for instruc-
tion tuning, 2023. 2
[61] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany
Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. Taming
sparsely activated transformer with stochastic experts. arXiv
preprint arXiv:2110.04260 , 2021. 2
A. Scaling up SMoLA vs LoRA
A.1. Effect of scaling up rank for LoRA
Recall that in Section ??, we studied the effect of scaling
up the expert counts for SMoLAE
O-PaLI-3 FT. Specifically,
withE=4, 16, 48, and 144 experts per modality, SMoLAE
O-
PaLI-3 FTmodels achieve 1.1, 1.14, 1.2, 1.27 improvements
over the PaLI-3 FTbaseline, where the metrics are averaged
across the tasks presented in Table 3 in the main paper on
the validation splits6except for InfoVQA. Can we achieve
similar performance improvement by using a higher rank of
LoRA on top of PaLI-3 FT?
6We use test split for AI2D as there are only 120 examples in val split.
11

--- PAGE 12 ---
We experiment with LoRA tuning on PaLI-3 FTwith rank
rof 32, 128, 384, 1536. Note that LoRA tuning with rank r
has the same extra model parameters as SMoLAr/4
MM-PaLI-
3FT, and shares the same extra compute as SMoLAr/8
O-
PaLI-3 FT. These LoRA tuning runs brought 0.58, 0.86,
0.79 and 0.59 gain over the PaLI-3 FTbaseline. At lower
ranks ( r= 32,128), LoRA tuning led to smaller gains than
SMoLAr/8
O-PaLI-3 FT, and further scaling up the rank re-
sulted in worse performance in the case of LoRA tuning.
A.2. Effective rank for LoRA
Figure 3. Number of singular values that are greater than different
thresholds for LoRA weights. The left is for LoRA with rank 384
and the right is for LoRA with rank 1536.
To help understand why higher ranks do not improve
LoRA tuning, we investigate how many ranks have the po-
tential to significantly impact the outputs. Specifically, we
consider LoRA with rank 384 and 1536 applied to PaLI-
3FT. We compute the singular values for the combined
weights of WoutWinfor the output layer of each attention
module in the encoder blocks, and present statistics over
singular values in these two settings in Figure 3. In particu-
lar, we plot the number of singular values that are greater
than 0.01%, 0.1%, 1%, 5%, 10% of the largest singular
value – the light colored lines plot this for each encoder
block and the bold line plots the counts averaged across all
encoder blocks. For LoRA with rank 1536, there are only
about 78 (5%) and 185 (12%) singular values greater than
5% and 1% of the largest singular value, the remaining di-
mensions would make little contribution to the output with
normalized inputs.
B. Visualization on the routing matrix Φ
We present a heat map visualization of ΦΦTin Figure 4.
IfΦΦTclosely resembles an identity matrix, the routing
matrix Φis more likely to route the input to different ex-
perts. We have the following observations: (a) ΦΦTfor
text experts are less similar to the identity matrix, indicat-
ing less need for specialization. One possible explanation
is that input texts in our tasks tend to be easy to process
— we trained on VQA tasks with short questions or image
captioning tasks using identical prompt. (b) ΦΦTfor early
layers in vision and multimodal experts are much closer to
(a) multimodal experts (b) vision experts (c) text experts Figure 4. Heat maps for ΦΦTfor different modalities in
SMoLA48
O-PaLI-3 FT
the identity matrix, indicating the need for different experts
to handle more diverse shallow representations.
C. Pseudo Code
While we largely inherit the Soft MoE design, we provide
the Pseudo Code for running SMoLA blocks in Figure 5.
Figure 5. Pseudo Code for a SMoLA block
12
