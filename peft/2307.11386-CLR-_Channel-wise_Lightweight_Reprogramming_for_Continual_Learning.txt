# 2307.11386.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2307.11386.pdf
# File size: 16495461 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CLR: Channel-wise Lightweight Reprogramming for Continual Learning
Yunhao Ge1†, Yuecheng Li1∗, Shuo Ni1∗, Jiaping Zhao2Ming-Hsuan Yang2, Laurent Itti1†
1University of Southern California2Google Research
∗Equal contribution as second author, †correspondence to {yunhaoge, itti }@usc.edu
Abstract
Continual learning aims to emulate the human abil-
ity to continually accumulate knowledge over sequential
tasks. The main challenge is to maintain performance on
previously learned tasks after learning new tasks, i.e., to
avoid catastrophic forgetting. We propose a Channel-wise
Lightweight Reprogramming (CLR) approach that helps
convolutional neural networks (CNNs) overcome catas-
trophic forgetting during continual learning. We show that
a CNN model trained on an old task (or self-supervised
proxy task) could be “reprogrammed” to solve a new task
by using our proposed lightweight (very cheap) reprogram-
ming parameter. With the help of CLR, we have a bet-
ter stability-plasticity trade-off to solve continual learn-
ing problems: To maintain stability and retain previous
task ability, we use a common task-agnostic immutable
part as the shared “anchor” parameter set. We then
add task-specific lightweight reprogramming parameters
to reinterpret the outputs of the immutable parts, to en-
able plasticity and integrate new knowledge. To learn se-
quential tasks, we only train the lightweight reprogram-
ming parameters to learn each new task. Reprogramming
parameters are task-specific and exclusive to each task,
which makes our method immune to catastrophic forget-
ting. To minimize the parameter requirement of repro-
gramming to learn new tasks, we make reprogramming
lightweight by only adjusting essential kernels and learn-
ing channel-wise linear mappings from anchor parameters
to task-specific domain knowledge. We show that, for gen-
eral CNNs, the CLR parameter increase is less than 0.6%
for any new task. Our method outperforms 13 state-of-
the-art continual learning baselines on a new challenging
sequence of 53 image classification datasets. Code and
data are available at https://github.com/gyhandy/
Channel-wise-Lightweight-Reprogramming .
1. Introduction
Continual Learning (CL) focuses on the problem of
learning from a stream of data, where agents continually
extend their acquired knowledge by sequentially learning
new tasks or skills, while avoiding forgetting of previous
tasks [41]. In the literature, CL is also referred to as lifelong
Figure 1. Equipped with a task-agnostic immutable CNN model,
our approach ”reprogram” the CNN layers to each new task with
lightweight task-specific parameters (less than 0.6% of the original
model) to learn sequences of disjoint tasks, assuming data from
previous tasks is no longer available while learning new tasks.
learning [10, 2, 41] and sequential learning [4]. This differs
from standard train-and-deploy approaches, which cannot
incrementally learn without catastrophic interference across
tasks [22]. How to avoid catastrophic forgetting is the main
challenge of continual learning, which requires that the per-
formance on previous learned tasks should not degrade sig-
nificantly over time when new tasks are learned. This is
also related to a general problem in neural network design,
the stability-plasticity trade-off [24], where plasticity repre-
sents the ability to integrate new knowledge, and stability
refers to the ability to retain previous knowledge [13].
Dynamic Network methods have been shown to be
among the most successful ones to solve continual learn-
ing, which usually shows great stability on previous tasks
and alleviates the influence of catastrophic forgetting by dy-
namically modify the network to solve new tasks, usually
by network expansion [41, 57, 12, 58]. For stability, a de-
sirable approach would be to fix the backbone and learn ex-
tra task-specific parameters on top of it, which will have no
catastrophic forgetting. However, the number of parameters
in such methods can quickly become very large. How to re-
duce the amount of required extra parameters is still a chal-
lenging problem. To solve the above issues, our approach is
based on three motivations:arXiv:2307.11386v1  [cs.CV]  21 Jul 2023

--- PAGE 2 ---
(1)Reuse instead of re-learn. Adversarial Reprogram-
ming [16] is a method to “reprogram” an already trained
and frozen network from its original task to solve new tasks
by perturbing the input space without re-learning the net-
work parameters. It computes a single noise pattern for
each new task. This pattern is then added to inputs for the
new task and fed through the original network. The origi-
nal network processes the combined input + noise and gen-
erates an output, which is then remapped onto the desired
new task output domain. For example, one pattern may be
computed to reprogram a network originally trained on Im-
ageNet [47] to now solve MNIST [14]. The same pattern,
when added to an image of an MNIST digit, would trigger
different outputs from the ImageNet-trained network for the
different MNIST classes (e.g., digit 0 + pattern may yield
“elephant”; digit 1 + pattern “giraffe”, etc). These outputs
can then be re-interpreted according to the new task (ele-
phant means digit 0, etc). Although the computation cost
is prohibitively high compared to baseline lifelong learning
approaches, here we borrow the reprogramming idea; but
we conduct more lightweight yet also more powerful re-
programming in the parameter space of the original model,
instead of in the input space.
(2)Channel-wise transformations may link two different
kernels. GhostNet [25] could generate more feature maps
from cheap operations applied to existing feature maps,
thereby allowing embedded devices with small memory to
run effectively larger networks. This approach is motivated
by near redundancy in standard networks: after training,
several learned features are quite similar. As such, Han et
al.[25] generate some features as linear transformations
of other features. Inspired by this, our approach augments a
network with new, linearly transformed feature maps, which
can cheaply be tailored to individual new tasks.
(3)Lightweight parameters could shift model distribu-
tion. BPN [57] adds beneficial perturbation biases in the
fully connected layers to shift the network parameter dis-
tribution from one task to another, which is helpful to
solve continual learning. This is cheaper than fine-tuning
all the weights in the network for each task, instead tun-
ing only one bias per neuron. Yet, this approach provided
good lifelong learning results. However, the method could
only handle fully connected layers and the performance is
bounded by the limited ability of the bias parameters to
change the network (only 1 scalar bias for each neuron).
Our method instead designs more powerful reprogramming
patterns (kernels) for the CNN layers, which could lead to
better performance on each new task.
Drawing from these three ideas, we propose channel-
wise lightweight reprogramming (CLR). We start with task-
agnostic immutable parameters of a CNN model pretrained
on a relatively diverse dataset (e.g., ImageNet-1k, Pascal
VOC, ...) if possible, or on a self-supervised proxy task,which requires no semantic labels. We then adaptively “re-
program” the immutable task-agnostic CNN layers to adapt
and solve new tasks by adding lightweight channel-wise lin-
ear transformation on each channel of a selected subset of
Convolutional layers (Fig 2). The added reprogramming pa-
rameters are 3x3 2D convolutional kernels, each working on
separate channels of feature maps after the original convo-
lutional layer. CLR is very cheap but still powerful, with the
intuition that different kernel filters could be reprogrammed
with a task-dependent linear transformation.
The main contributions of this work are:
• We propose a novel continual learning solution for
CNNs, which involves reprogramming the CNN lay-
ers trained on old tasks to solve new tasks by adding
lightweight task-specific reprogramming parameters.
This allows a single network to learn potentially un-
limited input-to-output mappings, and to switch on the
fly between them at runtime.
• Out method achieves better stability-plasticity balance
compared to other dynamic network continual learn-
ing methods: it does not suffer from catastrophic for-
getting problems and requires limited extra parameters
during continual learning, which is less than 0.6% of
the original parameter size for each new task.
• Our method achieves state-of-the-art performance on
task incremental continual learning on a new challeng-
ing sequence of 53 image classification datasets.
2. Related Work
Continual learning. Continual learning methods can be
broadly categorized in three approaches [13]: Replay-
based, Regularization-based, and Dynamic network-based.
(1) Replay methods use a buffer containing sampled
training data from previous tasks, as an auxiliary to a new
task’s training set. The buffer can be used either at the end
of the task training (iCaRL [44], ER [45]) or during training
(GSS, AGEM, AGEM-R, DER, DERPP [38, 3, 10, 7]). Re-
hearsal schemes [44, 46, 28, 11] explicitly retrain on a lim-
ited subset of stored samples while training on new tasks.
[38, 10] use constrained optimization as an alternative so-
lution leaving more leeway for backward/forward transfer.
Pseudo rehearsal methods use output of previous model
[45] or use generates pseudo-samples with a saved gener-
ative model [52]. Rehearsal methods have the drawback
that (1) they require an extra buffer to save samples (related
to dataset hardness), (2) they easily overfit to the subset of
stored samples during replay. (3) the performance seems
to be bounded by joint training, which is influenced by the
number of tasks.
(2) Regularization methods add an auxiliary loss term
to the primary task objective to constraint weight updates.
Data-focused methods (LwF[36], LFL[31], DMC [61]) use
knowledge distillation from previous models trained on old

--- PAGE 3 ---
tasks to constrain the model being trained on the new tasks.
Prior-based methods (EWC [32], IMM [35], SI [60], MAS
[1]) estimate the importance of network parameters to pre-
viously learned tasks, and penalize changing important pa-
rameters while training new tasks. The drawbacks of these
methods are expensive computation of importance estima-
tion and suboptimality on new tasks, especially with large
numbers of tasks.
(3) Dynamic network methods dynamically modify the
network to solve new tasks, usually by network expansion.
When no constraints apply to architecture size, one can
grow new branches for new tasks, while freezing previous
task parameters [48, 59], or dedicate a model copy to each
task [2]. To save memory, recent methods keep architecture
remains static, with fixed parts allocated to each task. Pre-
vious task parts are masked out during new task training, ei-
ther imposed at parameters level [19, 39], or unit level [51].
SUPSUP [58]), PSP [12] assign a fixed set of model param-
eters to a task and avoid over-writing them when new tasks
are learned. Dynamic network methods have been shown
to be among the most successful ones in solving continual
learning. For stability, a desirable approach would be to fix
the backbone and learn extra task-specific parameters on top
of it, which will have no catastrophic forgetting. SUPSUP
[58] uses a randomly initialized fixed backbone and learns
a task-specific supermask for each new task, while the per-
formance is bounded by the ability of the fixed backbone.
CCLL [53], and EFTs [55] use fixed backbones trained on
the first task, and learn task-specific group convolution pa-
rameters. The performance is sensitive to the first task, and
the extra group convolution hyper-parameter is not straight-
forward to train.
Our channel-wise lightweight reprogramming method
also belongs to dynamic network continual learning meth-
ods. Inspired by ghost networks, BPN, and Adversarial re-
programming, we reprogram the immutable task-agnostic
parameters and apply lightweight (computation and mem-
ory cheap) extra parameters to learn new tasks.
Meta learning. Meta learning aims to improve the learn-
ing algorithm itself, given the experience of multiple learn-
ing episodes [27]. In contrast, conventional ML improves
model predictions over multiple data instances. During
meta-learning, an outer (or upper/meta) algorithm updates
and improves the inner learning algorithm (e.g., one image
classification task). For instance, the outer objective could
be the generalization performance or learning speed of the
inner algorithm. Meta-learning has the following main dif-
ference with CL: similar to transfer learning, Meta-learning
focuses on finding a better learning strategy or initializa-
tion, and cares about the performance on a new or gener-
alized task, while performance on previous tasks is not the
interest. Thus, meta-learning could be used as a prelimi-
nary step in CLR, to optimize our shared initial parameter θin CLR. Meta-learning can learn from a single task in mul-
tiple domains [54, 20] or different tasks [21, 37, 5], which
requires access to some data from all base tasks or domains
at the same time, while our method and CL assume sequen-
tial learning and cannot access the data from previous tasks.
Also, meta-learning assumes the learned general setting will
be used in similar tasks as the base tasks, while CL does not
assume that previous tasks are similar to future tasks.
3. Proposed Method
Problem setting. In this paper, we focus on the task-
incremental setting of continual learning, where data arrives
sequentially in batches, and one batch corresponds to one
task, such as a new set of classes or a new dataset to be
learned. For a given continual learning algorithm, the goal
is to obtain a high average performance on all previously
learned tasks after learning the final task. At test time, just
like PSP [12], CCLL[53], EFTs[55], EWC [32], ER[45],
etc, we assume that a task oracle is provided during infer-
ence, showing which task a given sample belongs to.
Structure. We first introduce how our proposed Channel-
wise Lightweight Reprogramming parameters reprogram
the immutable task-agnostic backbone by conducting
channel-wise linear transformation after the original con-
volutional layer to change the original Conv-block as CLR-
Conv block, and then develop a new CLR-reprogrammed
network to solve new task. (Sec. 3.1). Then, we introduce
how CLR-reprogrammed networks can be used to solve
continual learning tasks (Sec. 3.2).
3.1. Channel-wise Lightweight Reprogramming
Our proposed Channel-wise Lightweight Reprogram-
ming method is equipped with an immutable task-agnostic
backbone and creates task-specific lightweight reprogram-
ming parameters to solve new tasks. Here, we use a fixed
backbone as a task-shared immutable structure. This dif-
fers from SUPSUP [58], which uses a randomly initial-
ized fixed backbone, and CCLL [53], EFTs [55], which use
fixed backbones trained on the first task. We use a more
general and compatible way with less requirement to ob-
tain the backbone: the fixed backbone could be pretrained
with supervised learning on a relatively diverse dataset (e.g.,
on ImageNet-1k [47], or Pascal VOC [18]), or with self-
supervised learning on proxy tasks, such as DINO [9] and
SwA V [8], which requires no semantic labels – we will see
that our approach is robust to the choice of a pretraining
dataset in the Sec. 4.5 experiments). This fixed backbone
can provide a diverse set of visual features. However, those
need to be reprogrammed later for individual tasks, which
is achieved by our CLR layers.
Specifically, we use channel-wise linear transformation
to reprogram the feature map generated by an original con-
volutional kernel, to generate a new task-specific feature
map for the new task.

--- PAGE 4 ---
Input feature maps(w’ ×h’ ×c’)Last layer Input…Output Class
…CLR!!CLR"!CLR#!…CLRConv blockCLR Conv blockconvCLRlayernormreluCLRConv block
convnormrelufeature maps before CLR layer (w ×h ×c)feature maps after CLR layer (w ×h ×c)Outputfeaturemaps(w×h ×c)Channel-wise Lightweight Reprogramming (CLR)
whcwhcw’ h’c’Figure 2. Proposed continual learning model with channel-wise lightweight reprogramming (CLR) layers. All gray blocks are fixed
parameters. (top) General network architecture. (bottom) Details of CLR reprogramming layer: for each channel k∈[1..c]of an original
w×h×cfeature map (blue), a 3x3 kernel is learned to reprogram the feature towards the new task (green), without modifying the original
conv parameters (grey).
Algorithm 1 CLR Layers
Input: Feature map X′
w×h×c(output from the original
conv layer F),3×32DCLR reprogram kernels ×c
Output: Reprogrammed feature map ˆX′w×h×c
p← ⌊3/2⌋
X′←zero-padding( X′,p)
fork∈[1..c]do
ˆX′[k]←CLR k(X′[k])X′[k]is the k-th channel
end for
Fig. 2 shows the structure of the proposed CLR. CLR is
compatible with any convolutional neural network (CNN).
A CNN usually consists of several Conv blocks (e.g., Resid-
ual block) (Fig. 2 top), which contain a convolutional layer
(conv), normalization layer (e.g., batch normalization), and
activation layer (e.g., relu). Our method treats a pretrained
CNN as a general and task-agnostic immutable parame-
ter backbone for all future tasks, so we fix its parame-
ters. (More details on the choice of a pretraining backbone
in the Sec. 4.5 experiments). To reprogram the CNN to
solve a new task in continual learning, we add lightweight
trainable parameters by changing each of the original Conv
blocks into a CLR-Conv block, thereby creating a CLR-
reprogrammed CNN (Fig. 2 top). Specifically, as shown
in Fig. 2 (top), a CLR-Conv block is obtained by adding a
channel-wise lightweight reprogramming layer (CLR layer)
after each of the original fixed conv layers. (For parame-
ter saving and preventing overfit, 1 ×1 conv layers are ex-
cluded.) Each CLR layer conducts linear transformations
on each channel of the original feature map after the fixed
conv layer to reprogram the features. Here, the linear trans-
formation is represented with 3x3 2D convolutional kernelsconducted on single channel feature map. Fig. 2 (bottom)
illustrates the details of Channel-wise linear transformation
for reprogramming. For each convolutional kernel fk(),
given the input feature X, we obtain one channel of fea-
turex′
k=fk(X)after the process, all Channel-wise fea-
tures form the output feature map X′. Our Channel-wise
reprogramming Linear transformation is applied on each
channel x′
kof the output feature map X′. For each ker-
nelfk(), we have a corresponding reprogramming 3x3 2D
kernel CLR k, which takes the single channel output x′
kas
input and conducts a linear transformation to obtain the re-
programmed feature ˆx′k:
ˆx′k=CLR k(x′
k) =CLR k(fk(X)) (1)
Algorithm 1 shows the pseudocode of the features before
and after a CLR layer. We initialize the CLR layer as an
identity transformation kernel (e.g., in a 3x3 2D kernel, the
center parameter is one and all others are zero). This set-
ting is crucial for training efficiency and performance, as
it favors keeping the general feature extractors in the origi-
nal fixed model, while at the same time it allows achieving
adaptive reprogramming, based on the loss function for the
new task.
For CLR-reprogrammed CNNs, the original conv lay-
ers in the backbone are fixed, the trainable parameters in-
clude the CLR layer after each fixed conv layer (normaliza-
tion layer is optional to train), and the last fully-connected
layer. For CLR-reprogrammed Resnet-50 [26], the train-
able CLR layer takes only 0.59% of the parameter of the
original fixed Resnet-50 backbone. This efficient param-
eter property makes CLR-reprogrammed networks easy to
deploy for continual tasks.

--- PAGE 5 ---
(a) Continual learning new tasks with CLR-reprogrammed modelTask 1
Scenes
67 classes; 15,620 imagesTask 2Task k-1Task kTask k+1Birds200 classes; 11,788 images…
Cars196 classes; 16,185 images…SVHN
10 classes; 99,289 imagesTask oracleinputOutputSelect whichCLR to use
(b) Test time with task-specific CLR parametersFixed backbone Legend:Task sequenceTask-specific CLR parameterLast layerCLR layersCLR pool…
BloodCell4 classes; 12,500 images
Figure 3. CLR-reprogrammed CNNs for continual learning. (a) In learning time, a CNN model could be reprogrammed by Channel-wise
Lightweight Reprogramming parameters to solve continual new tasks. Only the CLR layers need to be trained in each reprogramming. (b)
In test time, task oracle will select which task-specific CLR parameters to use and make the final decision.
3.2. CLR for Continual Learning
In task incremental continual learning, the model faces a
sequence of tasks. As shown in Fig. 3(a), during continual
learning, a CLR-reprogrammed model learns each task at
a time, and all tasks share the same fixed pretrained back-
bone. Each task has a trainable task-specific CLR parame-
ter set consisting of the CLR layers after each original conv
layer and the last linear layer. During testing (Fig. 3(b)), we
assume a perfect task oracle (as assumed by our baselines
shown in the next section) which tells the model which task
the test image belongs to. The fixed backbone equipped
with the corresponding task-specific CLR parameter makes
the final decision.
Due to the absolute parameter isolation in CLR (i.e.,
CLR layers are completely specific and separate for each
task, and the shared backbone is not changed at all), our
method’s performance on every task is not influenced by in-
creasing number of tasks (similar to SUPSUP [58], CCLL
[53], and EFT [55]). Theoretically, the CLR-reprogrammed
model can learn as many tasks as needed in a continual
learning setting and keep the optimal performance on each
task with no accuracy decrease when the number of tasks
increase, but with a 0.59% increase in parameters for each
new task.
4. Experiments and results
In this section, we compare our CLR-reprogrammed
model to baselines on a challenging 53-dataset with large
variance (Sec. 4.1). We evaluate task accuracy fluctuation
after learning new tasks to assess forgetting (Sec. 10). Then,
we evaluate the average accuracy on all learned tasks so
far during continual learning (Sec. 4.3). We analyze the
network parameters and computation cost during continual
learning (Sec. 4.4). We conduct ablation study to analyze
the influence of different immutable backbones (Sec. 4.5)
4.1. Dataset and Baselines:
Datasets. We use image classification as the basic task
framework. We extend the conventional benchmark 8-
dataset [1, 2] to a more challenging 53-datasets by col-
lecting more challenging classification tasks. 53-datasetsconsist of 53 image classification datasets. Each one sup-
ports one complex classification task, and the corresponding
dataset is obtained from previously published sources, e.g.,
task 1 [42]: classify scenes into 67 classes, such as kitchen,
bedroom, gas station, etc (scene dataset with 15,523 im-
ages); task 2 [56]: classify 200 types of birds, such as
Brewer Blackbird, Cardinal, Chuck will Widow, etc (birds
dataset with 11,787 images). A full list and more details
on each dataset are in supplementary materials. The 53-
datasets is a subset of SKILL-102 [23] Lifelong Learn-
ing benchmark dataset1, more details about dataset creation
and an extended version (107 tasks) is in USC-DCT (Di-
verse Classification Tasks), a broader effort in our labora-
tory. DCT can be used for many machine vision purposes,
not limited to lifelong learning.
We use 53 datasets with >1.8M images from 1,584
classes over 53 tasks for experiments. Table 1 shows the
details of our 53-dataset in comparison to other benchmark
datasets. Specifically, we compare the number of different
classification targets among different benchmarks, which
represents the diversity and difference among different con-
tinual tasks. For instance, our 53-dataset contains 5 differ-
ent classification targets: object recognition, style classifi-
cation (e.g., painting style), scene classification, counting
number (e.g., CLVER dataset [30]), and medical diagnosis
(e.g., Breast Ultrasound Dataset). To date, our 53-dataset
is one of the most challenging image classification bench-
marks for continual learning algorithms, with a large num-
ber of tasks and inter-task variance.
For the experiments below, we subsampled the dataset
to allow some of the sequential baselines to converge: we
capped the number of classes/task to 300 (only affected 1
tasks), and used either around 5,120 training images for
tasks with c≥60classes, or around 2,560for tasks with
c <60, where crepresent the number of classes. Thus, we
used 53 tasks, total 1,583 classes, total 132,625 training im-
ages and 13,863 test images. We also conduct experiments
on CIFAR-100 dataset (Appendix Fig.12). Baselines. As
discussed in Sec 1, we grant each baseline a perfect task or-
1SKILL-102 dataset http://ilab.usc.edu/andy/skill102

--- PAGE 6 ---
Comparison 53-dataset (ours) 8-dataset [1, 2] ImageNet [47] Fine grained 6 tasks [47] [35] Cifar100 [34] F-CelebA [43]
# tasks 53 8 20 6 20 10
# Classes 1,584 738 1000 1943 100 20
# Images 1,811,028 166,360 1,300,000 1,440,086 60,000 1,189
# different classification target 5 1 1 2 1 1
Mix image style (nature/artifact) ✓ ✓ ✗ ✓ ✗ ✗
Mix super/fine-class classification ✓ ✓ ✓ ✗ ✗ ✗
Table 1. Comparison of 53-dataset with other benchmark datasets including Cifar-100 [34], F-CelebA [43], Fine-grained 6 tasks [47] [35],
[40], [33], [49], [15]. Note that our 53-dataset covers the 8-dataset, F-CelebA and part of the Fine grained 6 tasks.
acle during inference. We implemented 13 baselines, which
can be roughly categorized in the following 3 categories
[13]:
(1) Dynamic Network methods contains most of the
baseline methods because our method also belongs to
it: they dynamically modify the network to solve new
tasks, usually by network expansion. We use PSP [12],
Supermask in Superposition (SUPSUP) [58], CCLL[53],
Confit[29], and EFTs[55] as the representative methods of
this category: For PSP, the model learns all 53 tasks in se-
quence, generating a new PSP key for each task. The keys
help segregate the tasks within the network in an attempt
to minimize interference. For SUPSUP, the model uses a
random initialized parameter as fixed backbone and learns
class-specific supermasks for each task, which help allevi-
ate catastrophic forgetting. During inference, different tasks
use different supermasks to make the decision.
(2) Regularization methods add an auxiliary loss term to
the primary task objective to constraint weight updates. The
extra loss can be a penalty on the parameters (EWC [32],
LwF[36], MAS [1] and SI [60]) or on the feature-space
(FDR [6]), such as using Knowledge Distillation (DMC
[61]). We use EWC, online-EWC, SI, LwF as the repre-
sentatives of this category: for EWC, one agent learns all
53 tasks in sequence, using EWC machinery to constrain
the weights when a new task is learned, to attempt to not
destroy performance on previously learned tasks.
(3) Replay methods use a buffer containing sampled
training data from previous tasks, as an auxiliary to a new
task’s training set. The buffer can be used either at the end
of the task training (iCaRL, ER [44, 45]) or during training
(GSS, AGEM, AGEM-R, DER, DERPP [38, 10, 3, 7]). We
use Episodic Memory Rehearsal (ER) as the representative
baseline of this category: One agent learns all 53 tasks in
sequence. After learning each task, it adds 10 images/class
of that task to a growing replay buffer that will later be used
to rehearse old tasks. When learning a new task, the agent
learns from all the data for that task, plus rehearses all old
tasks using the memory buffer. Additionally, SGD is a naive
baseline that just fine-tunes the entire network for each task,
with no attempt at minimizing interference. SGD-LL is a
variant that uses a fixed backbone plus a single learnable
shared last layer for all tasks with a length equal to the task
with the largest number of classes (500 classes in our set-
ting). SGD uses standard stochastic gradient descent as op-timization, which may suffer from the catastrophic forget-
ting problem.
For a fair comparison, all the 13 baseline methods use
an ImageNet-pretrained ResNet-50 [26] backbone, except
for PSP, Confit (requires ResNet-18) and SUPSUP (which
requires a randomly initialized ResNet-50 backbone).
4.2. Accuracy on the first tasks
To evaluate the performance of all methods in overcom-
ing catastrophic forgetting, we track the accuracy on each
task after learning new tasks. If the method suffers from
catastrophic forgetting, then the accuracy of the same task
will decrease after learning new tasks. A great continual
learning algorithm is expected to maintain the accuracy of
the original learning performance after learning new tasks,
which means that old tasks should be minimally influenced
by an increasing number of new tasks. Fig. 4 shows the ac-
curacy on the first, and second tasks as we learn from 1 to
53 tasks using 53-datasets, to gauge the amount of forget-
ting on early tasks as many more tasks are learned. Over-
all, our CLR-reprogrammed model maintains the highest
accuracy on these early tasks over time, and importantly,
our method (similar to CCLL [53], EFT [55] and SUPSUP
[58]) avoids forgetting and maintains the same accuracy as
the original training, no matter how many new tasks are
learned. SUPSUP is not able, even from the beginning, to
learn task 1 as well as other methods. We attribute this to
SUPSUP’s limited expressivity and capacity to learn using
masks over a random backbone, especially for tasks with
many classes. Indeed, SUPSUP can perform very well on
some other tasks, usually with a smaller number of classes
(e.g., SVHN, UMNIST Face Dataset). Baseline methods
suffer a different degree of forgetting: EWC [51], PSP [12],
ER [45], SI [60], and LwF [36] suffers severe catastrophic
forgetting in the challenging dataset. We noticed similar
performance on the second task.
4.3. Average accuracy after learning all 53 tasks
We computed the average accuracy on all tasks learned
so far after learning 1, 2, 3, ... 53 tasks. We plot the accu-
racy averaged over all tasks learned so far, as a function of
the number of tasks learned in Fig. 5. Note that the level
of difficulty for each of the 53 tasks is quite variable, as
shown in Fig. 6. Hence, the average accuracy over all tasks
so far may go up or down when a new task is added, de-

--- PAGE 7 ---
Figure 4. Accuracy on task 1 as a function of the number of tasks learned. Our approach maintains the highest accuracy on task 1 over time,
and importantly, it totally avoids catastrophic forgetting and maintains the same accuracy as the original training, no matter how many new
tasks are learned. As discussed in the approach section, this is because we explicitly isolate the task-specific parameters for all tasks and
avoid parameter interference. This is also the case for baselines SUPSUP [58], CCLL [53], and EFT [55]. Other baseline methods suffer a
different degree of catastrophic forgetting. EWC [51], PSP [12], LwF[36], SI [60] and SGD suffer severe catastrophic forgetting with this
challenging dataset. Rehearsal-based method ER performs relatively well because it has an unlimited large replay buffer, and it saves 10
images/class of the previous tasks. Yet, the overall accuracy of ER is still lower than our CLR-reprogrammed model. Rehearsal methods
also incur higher (and increasing) training costs because of the rehearsing. We noticed similar performance on the second task.
Oregon Wildlife20 classes14013 images
Fashion45 classes44000 images
Blindness5 classes13000 images			SVHN
10 classes; 99,289 images
Cars196 classes16,185 imagesFood-101
101 classes101,304 images
				100 sports100 classes14600 images
Figure 5. Average accuracy on all tasks learned so far, as a function of the number of tasks learned. Our CLR-reprogrammed approach is
able to maintain higher average accuracy than all baselines. The average accuracy increases because some of the later tasks are easier than
former tasks (i.e., later tasks have higher accuracy).
pending on whether an easier or harder task is added. The
average accuracy represents the overall performance of a
continual learning method in learning and then performing
sequential tasks. Overall our CLR-reprogrammed model
achieves the best average accuracy compared with all base-
lines. For replay-based methods, the overall performance
is lower than us even though they have a large buffer, and
the training time is increased for these methods with the in-
crease in task number. EWC, PSP, LWF, SI suffer severe
catastrophic forgetting in the challenging 53-datasets.
To show more details, we plot the accuracy of each task
after learn all 53 tasks with our CLR-reprogrammed method
in Fig. 6. This shows that 53-dataset provides a range of dif-
ficulty levels for the various tasks, and is quite hard overall.
Figure 6. Absolute accuracy per task after learning 53 tasks with
our CLR-reprogrammed CNN.
4.4. Parameter and computation cost
Achieving higher overall average accuracy during learn-
ing of sequential tasks is very important. A great contin-

--- PAGE 8 ---
MethodExtra parameter
to add 1 new taskComputation
costAverage Acc
(53-datasets)
SGD 0% 1 16.71 %
PSP [12] 5.02% 0.828 34.91 %
EWC [32] 0% 1.160 9.36 %
ONLINE EWC [50] 0% 1.011 9.47∗%
SGD-LL 0% 0.333 12.49 %
ER [45] 189.14% 3.99 53.13 %
SI [60] 0% 1.680 7.28%
LwF [36] 0% 1.333 8.23%
SUPSUP [58] 3.06% 1.334 56.69 %
EFT [55] 3.17% 1.078 67.8 %
CCLL [53] 0.62% 1.006 78.3 %
CLR (Ours) 0.59% 1.003 86.05 %
Table 2. Extra parameter expenditures and computation cost anal-
ysis. We treat the computation cost of SGD as the unit, and the
computation costs of other methods are normalized by the cost of
SGD. PSP’s low computation cost comes from using a Resnet-
18 backbone instead of Resnet-50 which is its original form. For
EWC, though the final model size does not increase, the perfor-
mance is poor and N fisher matrices are needed during training;
EWC-online updated the way of updating the fisher matrix and
only requires one fisher matrix during training. ER maintain a
memory buffer that includes five images per class from the tasks
that have already been seen, we spread the size in bytes of the
image buffer over the 53 tasks to obtain the amount of extra pa-
rameters per task. SUPSUP requires a 3MB mask for each task.
ual learning algorithm is also expected to minimize the re-
quirement of extra network parameters, and the computa-
tion cost. Table. 2 shows the required extra parameter and
computation cost for our approach and the baselines. The
”extra parameters to add one new task” represents the per-
centage compared with the original backbone (e.g., ResNet-
50) size. For instance, EWC needs no extra parameter,
while its computation cost is relatively high (to compute the
Fisher information matrix that will guide the constraining
of parameters while learning a new task), and the accuracy
is not the best. Our CLR method required only 0.59% ex-
tra parameters to learn each new task, and the computation
cost increase is small compared with baseline SGD (normal
training). Importantly, our method achieves the best average
accuracy.
4.5. Influence of different immutable backbone
Our method obtains the task-agnostic immutable param-
eters by training the CNN model on a relatively diverse
dataset with supervised learning or proxy tasks with self-
supervised learning. To investigate the influence of differ-
ent pretraining methods on the performance of continual
learning, we choose four different kinds of task-agnostic
immutable parameters trained with different datasets and
tasks. For supervised learning, besides Imagenet-1K, we
also conduct experiments with a pretrained backbone on
the Pascal-VOC image classification task (relatively smaller
one). For self-supervised learning, which needs no semanticLearning paradigm Method/datasetAverage Acc
(53-datasets)
Supervised Learning ImageNet-1k [47] 86.05 %
Supervised Learning Pascal VOC [18] 82.49 %
Self-supervised Learning SwA V [8] 85.12 %
Self-supervised Learning DINO [9] 85.77 %
Table 3. Influence of different task-agnostic immutable parame-
ter. Both supervised learning and self-supervised learning could
contribute a relatively good immutable parameter for our method,
which shows that our method is robust to different backbones.
labels, we conduct experiments with backbone trained with
DINO [9] and SwA V [8]. DINO [9] is a self-distillation
with no label framework, which utilizes multiple crop of
the image (patch) on the same model and update model’s
parameters with exponential moving average. While SwA V
[8] simultaneously clusters the data and keeps the con-
sistency between cluster assignments produced for differ-
ent augmentations of the same image. The results in Ta-
ble 3 shows that both supervised and self-supervised learn-
ing could contribute a good immutable parameters, and our
method is robust to different backbone pre-training. Note
how Pascal-VOC is a much smaller dataset, which may
explain the lower overall accuracy; with any of the other
(larger) datasets, accuracy is almost the same, suggesting
that our method is not highly dependent on a specific set of
backbone features.
5. Conclusion
We propose Channel-wise Lightweight Reprogramming
(CLR), a parameter-efficient add-on continual learning
method, to allow a single network to learn potentially un-
limited parallel input-to-output mappings and to switch on
the fly between them at runtime. CLR adds channel-wise
lightweight linear reprogramming to shift the original pre-
trained fixed parameter to each task, which is simple and
generalizable to any CNN-based model. The experiments
on continually learning 53 different and challenging tasks
show that the CLR method achieves state-of-the-art per-
formance on task incremental continual learning. Besides
high performance, CLR is also parameter-efficient, which
requires only 0.59% extra parameter to learn a new task.
Acknowledgements This work was supported by Amazon
ML Fellowship, C-BRIC (one of six centers in JUMP, a
Semiconductor Research Corporation (SRC) program spon-
sored by DARPA), DARPA (HR00112190134) and the
Army Research Office (W911NF2020053). The authors af-
firm that the views expressed herein are solely their own,
and do not represent the views of the United States govern-
ment or any agency thereof.

--- PAGE 9 ---
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings of
the European Conference on Computer Vision , pages 139–
154, 2018. 3, 5, 6
[2] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.
Expert gate: Lifelong learning with a network of experts.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 3366–3375, 2017. 1, 3, 5, 6
[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in Neural Information Processing Sys-
tems, 2019. 2, 6
[4] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuyte-
laars. Selfless sequential learning. arXiv preprint
arXiv:1806.05421 , 2018. 1
[5] Marcin Andrychowicz, Misha Denil, Sergio Gomez,
Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. Learning to learn by
gradient descent by gradient descent. Advances in Neural
Information Processing Systems , 2016. 3
[6] Ari S Benjamin, David Rolnick, and Konrad Kording. Mea-
suring and regularizing networks in function space. arXiv
preprint arXiv:1805.08289 , 2018. 6
[7] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for general
continual learning: a strong, simple baseline. Advances
in Neural Information Processing Systems , pages 15920–
15930, 2020. 2, 6
[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learning
of visual features by contrasting cluster assignments. Ad-
vances in neural information processing systems , 33:9912–
9924, 2020. 3, 8
[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021. 3, 8
[10] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with a-
gem. arXiv preprint arXiv:1812.00420 , 2018. 1, 2, 6
[11] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc’Aurelio Ranzato. On tiny episodic memo-
ries in continual learning. arXiv preprint arXiv:1902.10486 ,
2019. 2
[12] Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal,
and Bruno Olshausen. Superposition of many models into
one. arXiv preprint arXiv:1902.05522 , 2019. 1, 3, 6, 7, 8
[13] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ale ˇs Leonardis, Gregory Slabaugh, and
Tinne Tuytelaars. A continual learning survey: Defying for-
getting in classification tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 44(7):3366–3385, 2021.
1, 2, 6[14] Li Deng. The mnist database of handwritten digit images for
machine learning research. IEEE Signal Processing Maga-
zine, 29(6):141–142, 2012. 2
[15] Mathias Eitz, James Hays, and Marc Alexa. How do humans
sketch objects? ACM Transactions on Graphics (TOG) ,
31(4):1–10, 2012. 6
[16] Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-
Dickstein. Adversarial reprogramming of neural networks.
arXiv preprint arXiv:1806.11146 , 2018. 2
[17] Utku Evci, Vincent Dumoulin, Hugo Larochelle, and
Michael C Mozer. Head2toe: Utilizing intermediate rep-
resentations for better transfer learning. arXiv preprint
arXiv:2201.03529 , 2022. 15
[18] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman. The pascal visual ob-
ject classes challenge: A retrospective. International Journal
of Computer Vision , 111(1):98–136, Jan. 2015. 3, 8
[19] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori
Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and
Daan Wierstra. Pathnet: Evolution channels gradient descent
in super neural networks. arXiv preprint arXiv:1701.08734 ,
2017. 3
[20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
InInternational Conference on Machine Learning , pages
1126–1135, 2017. 3
[21] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo
Grazzi, and Massimiliano Pontil. Bilevel programming for
hyperparameter optimization and meta-learning. In Interna-
tional Conference on Machine Learning , pages 1568–1577.
PMLR, 2018. 3
[22] Robert M French. Catastrophic forgetting in connectionist
networks. Trends in Cognitive Sciences , 3(4):128–135, 1999.
1
[23] Yunhao Ge, Yuecheng Li, Di Wu, Ao Xu, Adam M. Jones,
Amanda Sofie Rios, Iordanis Fostiropoulos, shixian wen,
Po-Hsuan Huang, Zachary William Murdock, Gozde Sahin,
Shuo Ni, Kiran Lekkala, Sumedh Anand Sontakke, and Lau-
rent Itti. Lightweight learner for shared knowledge life-
long learning. Transactions on Machine Learning Research ,
2023. 5
[24] Stephen Grossberg. Studies of Mind and Brain: Neural Prin-
ciples of Learning, Perception, Development, Cognition, and
Motor Control . Springer Science & Business Media, 1982.
1
[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing
Xu, and Chang Xu. Ghostnet: More features from cheap
operations. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 1580–1589,
2020. 2
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 4, 6
[27] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and
Amos Storkey. Meta-learning in neural networks: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 44(9):5149–5169, 2021. 3

--- PAGE 10 ---
[28] David Isele and Akansel Cosgun. Selective experience re-
play for lifelong learning. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , 2018. 2
[29] Shibo Jie, Zhi-Hong Deng, and Ziheng Li. Alleviating repre-
sentational shift for continual fine-tuning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3810–3819, 2022. 6
[30] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In Proceedings
of the IEEE conference on computer vision and pattern
recognition , pages 2901–2910, 2017. 5
[31] Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim.
Less-forgetting learning in deep neural networks. arXiv
preprint arXiv:1607.00122 , 2016. 2
[32] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the National Academy of Sci-
ences , 114(13):3521–3526, 2017. 3, 6, 8
[33] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
Proceedings of the IEEE International Conference on Com-
puter Vision Workshops , pages 554–561, 2013. 6
[34] Alex Krizhevsky. Learning multiple layers of features from
tiny images. pages 32–33, 2009. 6
[35] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha,
and Byoung-Tak Zhang. Overcoming catastrophic forget-
ting by incremental moment matching. Advances in Neural
Information Processing Systems , 2017. 3, 6
[36] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 40(12):2935–2947, 2017. 2, 6, 7, 8
[37] Hanxiao Liu, Karen Simonyan, and Yiming Yang.
Darts: Differentiable architecture search. arXiv preprint
arXiv:1806.09055 , 2018. 3
[38] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. Advances in Neural
Information Processing Systems , pages 6467–6476, 2017. 2,
6
[39] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pages 7765–7773, 2018. 3
[40] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In Indian
Conference on Computer Vision, Graphics & Image Process-
ing, pages 722–729, 2008. 6
[41] German I Parisi, Ronald Kemker, Jose L Part, Christopher
Kanan, and Stefan Wermter. Continual lifelong learning with
neural networks: A review. Neural Networks , 113:54–71,
2019. 1
[42] Ariadna Quattoni and Antonio Torralba. Recognizing indoor
scenes. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 413–420, 2009. 5[43] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. In
Advances in Neural Information Processing Systems , 2017.
6
[44] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classifier
and representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
2001–2010, 2017. 2, 6
[45] Anthony Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science , 7(2):123–146, 1995.
2, 3, 6, 8
[46] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. Advances in Neural Information Processing Sys-
tems, 2019. 2
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge (2014). arXiv preprint
arXiv:1409.0575 , 2(3), 2014. 2, 3, 6, 8
[48] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671 , 2016. 3
[49] Babak Saleh and Ahmed Elgammal. Large-scale classifica-
tion of fine-art paintings: Learning the right metric on the
right feature. arXiv preprint arXiv:1505.00855 , 2015. 6
[50] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina,
Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pas-
canu, and Raia Hadsell. Progress & compress: A scalable
framework for continual learning. In International Confer-
ence on Machine Learning , pages 4528–4537. PMLR, 2018.
8
[51] Joan Serra, Didac Suris, Marius Miron, and Alexandros
Karatzoglou. Overcoming catastrophic forgetting with hard
attention to the task. In International Conference on Machine
Learning , pages 4548–4557, 2018. 3, 6, 7
[52] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. Advances in
Neural Information Processing Systems , 2017. 2
[53] Pravendra Singh, Vinay Kumar Verma, Pratik Mazumder,
Lawrence Carin, and Piyush Rai. Calibrating cnns for life-
long learning. Advances in Neural Information Processing
Systems , 33:15579–15590, 2020. 3, 5, 6, 7, 8
[54] Sebastian Thrun and Lorien Y . Pratt. Learning to learn: In-
troduction and overview. In Learning to Learn , 1998. 3
[55] Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush
Rai, and Lawrence Carin. Efficient feature transformations
for discriminative and generative continual learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13865–13875, 2021. 3, 5, 6,
7, 8
[56] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. The caltech-ucsd birds-200-2011 dataset. Technical
Report CNS-TR-2011-001, California Institute of Technol-
ogy, 2011. 5

--- PAGE 11 ---
[57] Shixian Wen, Amanda Rios, Yunhao Ge, and Laurent Itti.
Beneficial perturbation network for designing general adap-
tive artificial intelligence systems. IEEE Transactions on
Neural Networks and Learning Systems , 2021. 1, 2
[58] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu,
Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosin-
ski, and Ali Farhadi. Supermasks in superposition. Advances
in Neural Information Processing Systems , pages 15173–
15184, 2020. 1, 3, 5, 6, 7, 8
[59] Ju Xu and Zhanxing Zhu. Reinforced continual learning.
Advances in Neural Information Processing Systems , 2018.
3
[60] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , pages 3987–3995, 2017.
3, 6, 7, 8
[61] Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafet-
tin Tasci, Larry Heck, Heming Zhang, and C-C Jay Kuo.
Class-incremental learning via deep model consolidation. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 1131–1140, 2020. 2, 6

--- PAGE 12 ---
Appendix
6. Details of our 53-dataset for continual learn-
ing and method performance
Fig. 7 shows a summary of the 53 datasets we used as the
continual learning benchmark in our main paper. The figure
also shows the detailed per-task accuracy of our methods
and baselines after learning all 53 tasks in the task incre-
mental continual learning setting.
7. Channel-wise linear reprogramming ability
To further understand the performance of channel-wise
lightweight reprogramming achieved by channel-wise lin-
ear transformation, we conduct qualitative experiments to
explore the ability of CLR layer to transfer the feature map
from a Pre-trained immutable parameter weight set (starting
point) to a target parameter weight set (goal).
Usually, the Pre-trained weight is not good enough due
to the domain gap between the Pre-trained dataset/learning
paradigm and the target dataset. And a relatively good per-
formance could be achieved by either finetuning the whole
backbone on the target dataset (FINETUNE) or learning
from scratch (randomly initialized backbone) on the tar-
get task dataset (SCRATCH). We will show that with the
help of a very cheap CLR layer, a feature map after a pre-
trained (non-optimal) model could be reprogrammed to-
wards a ”relatively optimal” feature map obtained by ei-
ther finetuning the whole backbone (FINETUNE) or train-
ing from scratch (SCRATCH).
We choose two datasets: CLEVR dataset and the
Kannada-MNIST dataset. Model performance on the
CLEVR dataset reaches 46.09% with a Pre-trained ResNet-
50 backbone + linear head, 97.66% with FINETUNE, and
91.41% with SCRATCH. In this scenario, pretrain has a
large accuracy gap with FINETUNE and SCRATCH. It
would be interesting to see if the CLR layer could repro-
gram a feature map obtained from pretrain towards a feature
map obtained by FINETUNE and SCRATCH, which shows
the ability of CLR layer to fill a large domain gap.
Model performance on the Kannada-MNIST dataset
reaches 95.77% with a Pre-trained backbone + linear head,
99.62% with FINETUNE, and 100% with SCRATCH.
Here, SCRATCH performs better than FINETUNE, which
shows that the pretrained weights may have no benefit (or
even harmful) for target task learning. Here we want to
show that the CLR layer could reprogram a feature map
obtained from pretrain towards the feature map obtained
by SCRATCH. We use the feature map after the first con-
volutional layer in the different models (pretrain, FINE-
TUNE, and SCRATCH). Taking the feature map after the
pretrain model as input and the feature map after FINE-
TUNE (or SCRATCH) as output, we utilize a CLR layer(3x3 2D depthwise convolutional kernels) to learn the map-
ping, i.e. the channel-wise linear transformation between
them. The qualitative results are shown in Fig. 8. Specif-
ically, in Fig. 8, we visualize the feature map that ini-
tially has a large initial gap between pretrain and FINE-
TUNE (or SCRATCH). The results show that after channel-
wise linear transformation, the feature after pretrain could
be reprogrammed towards the goal feature (FINETUNE or
SCRATCH)
8. Bootstrapping results
Fig.5 in the main paper shows the average accuracy
as more tasks are learned. However, the gradient of the
curve is also influenced by the order of the tasks (i.e., Hard
tasks located in earlier sequence will cause average accu-
racy tends to increase, while easy tasks located in earlier
sequence will cause average accuracy tends to decrease)
which is entangled with the effect of catastrophic forgetting.
We use bootstrapping to show the tendency of average
accuracy when more tasks are learned. Specifically, for any
number of tasks ( t∈(1, ...,53)) that we want to conduct in
one continual learning setting, we randomly sample ttasks
from the 53 tasks 50,000 times with replacement and com-
pute the mean accuracy (mean) and standard deviation (std).
Fig. 9 shows the Bootstrapping statistic results, which show
the change of mean and std when we increase the total num-
ber of tasks. The X-axis represents the task number twe
want to conduct. For instance, if the continual learning task
number t=10, then we randomly sample 10 tasks from the
53-dataset and calculate the mean accuracy. We repeat the
sampling 50000 times and get the std. Y-axis shows the
mean Accuracy (solid blue line) on the sampled tasks with
replacement and std as the shaded light blue range. Since
in our CLR method, the order of task is not mattered (we
have the same performance on a specific task no matter the
sequence), this allows us to simulate what would happen if
we learn a different sample of tasks given a specific task
number t. We observe that the mean accuracy is stable and
not influenced by twhen the sample number is large. For
std, when the task number tis small, the std is relatively
large, and the std decreases with task number tincrease.
When t=53, the std becomes zero.
Fig. 10 shows the Bootstrapping statistic results with de-
tailed max and minimum accuracy logs. The X-axis repre-
sents the number of tasks tin a specific continual learning
task. Y-axis shows the mean Accuracy (solid blue line) on
the sampled tasks with replacement. The shaded light blue
range shows the min and max range in the given task num-
bertamong 50000 times task samples. We use the solid
red line to represent our reported results in the main paper
Fig.5, which filled in the shaded light blue range.

--- PAGE 13 ---
Figure 7. Statistics of the datasets and per-task accuracy of our method and baselines after learning all 53 tasks in the continual learning
setting. Ablation columns indicate our methods with different initialization weights.
9. More experiments to explore the trade-off
between parameter and performance
Several other versions of our method may include meth-
ods with higher accuracy but higher cost. Our main method
- CLR (the one in the paper) adds the CLR layer after all
the original convolutional kernels except for the 1 ×1 ker-
nels, saving many parameters.
CLR-Full version applying the CLR layer to all con-
volutional kernels in the pretrained model which reaches
85.85% in accuracy and cost 1.69 ×parameters compared
to our main method (CLR).
The CLR-Reduced version adds a smaller version of
CLR layer with 1 ×1 2D reprogramming kernels after all
1×1 original Conv kernels and normal CLR layer with 3 ×3
2D reprogramming kernels after the rest CONV kernels.
It reaches 85.7% in accuracy and costs 1.08 ×parameters
compared to our main method (CLR).
The CLR-mixed version learns a weighted combinationof the original and our reprogrammed feature maps. The in-
tuition is that we keep some proportion of the original fea-
ture and add the new features learned after reprogramming.
Specifically, A trainable parameter Adecides the weight of
the summation of the reprogrammed feature and the original
feature map. Equation 2 shows the details of the weighted
combination.
ˆx′k=A∗CLR k(x′
k) + (1−A)∗x′
k (2)
where x′
kis the kth channel of the feature map from the
original Convolutional layer and CLR kis the correspond-
ing linear transformation. It reaches 86.25% in accuracy
and costs 1.79 ×parameters compared to our main method
(CLR).
The results are shown in Fig. 11. Theoretically, more
trainable parameters could lead to better performance, and
it is true for CLR-mixed version, which achieves +0.2%
than our main method. Interestingly, the CLR-full ver-
sion achieves lower average accuracy than the main method,

--- PAGE 14 ---
Figure 8. The Figure shows quantitative results of the CLR transformation ability on CLEVR and Kannada-MNIST datasets. We visu-
alize the feature maps in the first residual group of ResNet-50 that initially has a large initial gap between pre-train and FINETUNE (or
SCRATCH). The results show that after channel-wise linear transformation, the feature after pre-train could be reprogrammed towards the
goal feature (FINETUNE or SCRATCH). Pretrained indicates the frozen Imagenet pretrained ResNet-50 backbone. Finetune is a finetuned
ResNet-50 backbone with Imagenet pretrained initialization while Scratch is a trained ResNet-50 backbone with random initialization.

--- PAGE 15 ---
Figure 9. Bootstrapping statistic results. The X-axis represents the number of tasks tin a specific continual learning task. Y-axis shows the
mean Accuracy (solid blue line) on the sampled tasks with replacement and std as the shaded light blue range.
Figure 10. Bootstrapping statistic results with detailed accuracy log. The X-axis represents the number of tasks tin a specific continual
learning task. Y-axis shows the mean Accuracy (solid blue line) on the sampled tasks with replacement. The shaded light blue range shows
the min and max range in the given task number t. We use the solid red line to represent our reported results in the main paper Fig.5, which
filled in the shaded light blue range.
while most of the per-task accuracy is higher (43 out of 53
tasks).
10. Transfer learning with CLR-based model
We apply our CLR method to the transfer learning prob-
lem, where we only care about the accuracy of the trans-
ferred dataset while do not need to maintain performance
on previous datasets.
Datasets. We use the same 53-dataset to evaluate transfer
learning performance. Specifically, we use the ImageNet
pretrained ResNet-50 model as initialization and apply our
method and 4 baseline transfer learning methods 53 times,on 53 different classification tasks.
Baselines. We have four baseline methods: 1) learn from
scratch (SCRATCH), where the backbone ResNet-50 is ran-
domly initialized with no prior knowledge, and then uses
the training set of each task to train the whole network
from scratch. 2) finetune the whole backbone and last layer
(FINETUNE), 3) finetune only the last layer (LINEAR), 4)
Head2Toe method [17] use the fixed backbone and need two
steps: 1) feature selection: train the model by adding a large
fully connection between all intermediate features and the
last layer and select the important connection by adding reg-
ularization. 2) keep the important skip connection and re-

--- PAGE 16 ---
Figure 11. Per-task accuracy of our main method and other versions of our method after learning all 53 tasks in the continual learning
setting.
Method Average Acc
LwF 24%
iCARL 49%
RPS 57%
CCLL 85%
EWC 41%
SI 52%
CLR (Ours) 94.2%
Table 4. We applied our method on CIFAR-100 dataset with 10
tasks, each containing 10 classes with comparisons to baselines
from CCLL, using ResNet-18 as the backbone.
train the added layers.
Fig. 12 shows the average accuracy on all 53 classifica-
tion tasks and the details of each task, and Fig. 13 shows
the detailed result for transfer learning. Our CLR achieves
the best average accuracy on the 53-dataset compared with
all baselines. Specifically, CLR achieves almost 5% average
improvement on 53 datasets over Head2Toe, and even larger
improvement over LINEAR, FINETUNE, and SCRATCH.
This shows the effectiveness of the CLR-based model in
transfer learning problems.
11. CIFAR-100 Result
We also show our method’s result on incremental
CIFAR-100 dataset with other previous baselines in table
4

--- PAGE 17 ---
Figure 12. Bar plot of transfer learning performance on 53-dataset.
Figure 13. Transfer learning result on 53-dataset of our method and other baselines (LINEAR, SCRATCH, FINETUNE, and Head2Toe)
