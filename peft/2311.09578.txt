# 2311.09578.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2311.09578.pdf
# File size: 369885 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying
Adithya Renduchintala Tugrul Konuk
NVIDIA
{adithyare,tkonuk,okuchaiev}@nvidia.comOleksii Kuchaiev
Abstract
We introduce Tied-LoRA, a novel paradigm
leveraging weight tying and selective training
to enhance the parameter efficiency of Low-
rank Adaptation (LoRA). Our exploration en-
compasses different plausible combinations of
parameter training and freezing, coupled with
weight tying, aimed at identifying the optimal
trade-off between performance and the count
of trainable parameters. Across 5diverse tasks
and two foundational language models with dif-
ferent parameter counts, our experiments pro-
vide comprehensive insights into the inherent
trade-offs between efficiency and performance.
Our findings reveal a specific Tied-LoRA con-
figuration that distinguishes itself by showcas-
ing comparable performance to LoRA across
multiple tasks while utilizing only a fraction
of the parameters employed by the standard
LoRA method, particularly at elevated ranks.
This underscores the efficacy of Tied-LoRA in
achieving impressive results with significantly
reduced model complexity.
1 Introduction
Large language models (LLMs) play a crucial role
in various Natural Language Processing (NLP) ap-
plications due to their proficiency. A significant fac-
tor driving their widespread adoption is the ability
to fine-tune pretrained LLMs efficiently for specific
downstream tasks. This fine-tuning process allows
the creation of specialized language models that ex-
cel in specific domains and tasks. Despite dealing
with smaller training data compared to pretraining,
the computational demand for during fine-tuning
remains high, especially for large models with bil-
lions of parameters.
Moreover, for LLM service providers, it is of-
ten necessary to cater to diverse requirements by
maintaining distinct customizations for each com-
bination of user and task in the service. For in-
stance, consider a scenario where a language model
is employed to assist users in generating content
W∈Rd×3d
A∈Rd×r ὑ7v∈R3d×1
B∈Rr×3dὑ7
xz
u∈Rr×1
Figure 1: Schematic of our Tied-Lora paradigm, the
main low-rank matrices AandBare tied across (indi-
cated by the ὑ7symbol) all the layers of the base lan-
guage model. We use the gradient shading to indicate
that these parameters can either be trained or frozen.
for social media. User X may have preferences
for formal language and professional tone, while
another user, Y , might prefer a more casual and
conversational style. Additionally, each user may
have different tasks, such as composing business
emails, translating documents, creating social me-
dia captions or drafting blog posts. To serve these
varied preferences and tasks simultaneously, the
language model needs to be finely tuned for each
specific combination of user (X or Y) and task
(email composition or translation).
As the number of users and tasks per user in-
creases, so does the complexity and cost associ-
ated with customization. Managing and storing the
various combinations of customizations for each
user-task pair can introduce additional expenses,
especially after the initial training phase. The stor-
age and retrieval of these customized models, each
tailored to specific user preferences and tasks, con-
tribute to ongoing operational costs. Therefore,
in addition to efficient utilization of customizable
parameters during training, careful consideration
must be given to the post-training phase, where thearXiv:2311.09578v2  [cs.CL]  12 Apr 2024

--- PAGE 2 ---
cost of saving and accessing these combinations
becomes a significant factor in the overall resource
management strategy. This holistic approach is cru-
cial for maintaining optimal performance across
diverse user-task combinations while keeping both
computational and operational costs in check.
In light of these challenges, developing effec-
tive customization methods that not only enhance
model performance but also reduce the number of
training parameters becomes crucial. Parameter-
efficient fine-tuning (PEFT) emerges as a valuable
approach in this context. PEFT involves refining
pretrained models with minimal parameter updates,
enabling the creation of specialized models that ex-
cel in specific domains and tasks. This streamlined
customization process not only optimizes param-
eter utilization during training but also mitigates
the costs associated with managing, storing, and
serving diverse customizations post-training.
Low-rank Adaptation (LoRA) method (Hu et al.,
2021), stands out as a popular and efficient
parameter-efficient fine-tuning (PEFT) approach,
offering a straightforward implementation and the
ability to integrate LoRA weights into the base
model post-training. Despite its advantages, the ex-
pense of LoRA training becomes more pronounced,
particularly with the growing size of base language
models. While previous efforts focused on en-
hancing LoRA’s parameter efficiency through care-
ful low-rank selection, we introduce an alterna-
tive approach. In contrast to controlling parameter
count through rank, our method incorporates sim-
ple weight tying alongside selective training. This
novel combination forms the basis for a range of
Tied-LoRA configurations, each evaluated for per-
formance across five diverse customization tasks.
Through this approach, we aim to push the bound-
aries of parameter-efficient fine-tuning, making ad-
vancements in both effectiveness and simplicity.
Our contributions are threefold:
1. We propose a range of Tied-LoRA configura-
tions that use simple weight tying in LoRA
along with selective training to boost the pa-
rameter efficiency of LoRA.
2.We study this spectrum of possible Tied-
LoRA configurations on diverse tasks that re-
semble real-world customization problems.
3.Based on the results of our study, we propose
the specific TL 6(vBὑ7uAὑ7) configuration as
the best option for maintaining performancewhile reducing parameters. This configuration
is within 1−2%of LoRA in terms of perfor-
mance and in one case beats LoRA while only
using 12.5%of the number of parameters.
2 Method
In this section, we introduce Tied-LoRA, a
paradigm for parameter-efficient fine-tuning of
large language models through low-rank weight-
update approximations, weight-tying and selec-
tive training. Our framework offers a range of
“LoRA-like” configurations through a series of de-
sign choices over selective parameter training and
weight tying, including some of the existing PEFT
methodologies available in the literature. Specifi-
cally, we use weight tying alongside pairs of projec-
tion matrices and scaling vectors that can be selec-
tively either trained or frozen. As the low-rank com-
putation path does not introduce any non-linearity,
all Tied-LoRA configurations can be merged into
the base model weights to preventing additional
latency during inference.
Table 1 provides an overview of the scenarios we
study. We refer to each configuration in our study
with TL (Tied-LoRA) followed by a subscript in-
dex (e.g., TL 1). Additionally, we also include the
template of possible training parameters ( v,B,u
andA, discussed in section 2.1). For Tied-LoRA,
the low-rank projection matrices AandBaretied
across all the layers of the base model which we
indicate using the subscriptὑ7. We also indicate if a
parameter is frozen by blue font and a trainable pa-
rameter with regular font. Thus, traditional LoRA
can be expressed as vBuA and VeRA (Kopiczko
et al., 2023) can be expressed as vBὑ7uAὑ7.
2.1 Formulation
The overall structure of the tied LoRA framework
can be seen in Figure 1. Note that the original
LoRA (Hu et al., 2021) uses a dedicated pair of
low-rank projections for each of the Q,K,Vma-
trices. However, in our formulation, Wis ad×3d
matrix that jointly projects Q,K, andVattention
matrices, where dis the hidden size of the base
language model. Therefore, our down projection
Ais ad×rshaped matrix and up projection ma-
trixBhas shape r×3d, where ris the low-rank
bottleneck dimension. Essentially, the down pro-
jection Aisshared byQ,K, andV, leading to
fewer trainable parameters ( 4dr) than the original
LoRA ( 6dr).

--- PAGE 3 ---
Method Parameters Initialization
LoRA ( vBuA ) 4Ldr A ∼ N, B= 0, u, v = 1
Vera (vBὑ7uAὑ7)L(r+ 3d) A, B∼ N, u= 1, v= 0
TL1(vBὑ7uAὑ7)dr A, B ∼ N, u, v = 1
TL2(vBὑ7uAὑ7)dr+L(r+ 3d)A, B∼ N, u= 1, v= 0
TL3(vBὑ7uAὑ7) 3dr A ∼ N, B= 0, u, v = 1
TL4(vBὑ7uAὑ7) (L+ 3d)r A ∼ N, B= 0, v, u = 1
TL5(vBὑ7uAὑ7) 4dr A ∼ N, B= 0, u, v = 1
TL6(vBὑ7uAὑ7) 4dr+L(r+ 3d)A, B∼ N, u= 1, v= 0
Table 1: Tied-LoRA configurations included in our
study. The first column shows acronyms used to identify
each Tied-LoRA configuration (i.e., method). Symbols
with subscriptὑ7indicate that it is shared across all layers
and the color blue indicates that the parameter is frozen.
Formulas for the number of trainable parameters in each
configuration as a function of number of layers L, hid-
den size d, and low-rank rare also provided.
For a linear layer with a frozen pretrained weight
matrix W, we define the layer output as
z=Wx+ ∆Wx≈Wx+α
rΛvBΛuAx,(1)
where ∆Wis the full-rank update matrix, αis a
scaling factor, AandBare low-rank projection ma-
trices, and ΛuandΛvare diagonal matrices with
diagonal elements given by uandv, respectively.
Herein, ΛvBΛuAxis the low-rank approximation
to the parameter update matrix ∆W. Unlike the
original LoRA, where αis a hyper-parameter that
can be manually set, we simply set α=r, effec-
tively removing its scaling effect.
Equation 1 is a generalized formulation for meth-
ods that utilize low-rank approximations to esti-
mate parameter updates. Particular settings of pa-
rameter updates and weight tying reduces this equa-
tion to some of the existing formulations in the lit-
erature. Setting and freezing Λu= Λ v=Iand
untying AandBresults in LoRA:
z=Wx+BAx. (2)
Similarly, randomly initializing AandBmatri-
ces and tying them across all layer leads the the
VeRA formulation (Kopiczko et al., 2023):
z=Wx+ Λ vBΛuAx, (3)
2.2 Weight Tying
The third column of Table 1 presents representa-
tions for number of trainable parameters each Tied-
Lora configuration requires. As is apparent from
the table, weight tying is a critical ingredient of
our proposed approach which drastically reducesthe number of trainable parameters. For exam-
ple, LoRA ( vBuA ) training using the 7B LLaMA-
2 (Touvron et al., 2023) language model with a typ-
ical low rank setting of 8requires ∼4.2M trainable
parameters. By merely introducing weight tying
across the 32layers of this model reduces the train-
able parameters to ∼131K, which is a 96.875%
reduction. In comparison, the Vera method results
in a reduction of 90.6%.
2.3 Selective Training
Through the flexible framework that equation 1 of-
fers, we are given the opportunity to investigate a
range training configurations. By selectively updat-
ing the components A, B, u , andvduring the train-
ing process, we can generate a variety of method-
ological variations. These variations not only ex-
hibit differences in parameter count, but they also
demonstrate distinct capabilities across a variety
of tasks. This exploration allows us to investigate
the intriguing regime of extremely low-parameter
and low-rank PEFT models. This is a key step to-
wards the customization of models, enabling them
to excel at specific tasks while maintaining a mini-
mal parameter count. Our ultimate goal here is to
harness the power of this methodology to create
highly efficient, task-specific models that achieve
high performance with reduced complexity.
3 Experiments
We now turn to evaluating the different configu-
rations possible within our Tied-LoRA paradigm.
While LoRA ( vBuA ) and PEFT methods can be
used to train models for general instruction follow-
ing (Sun et al., 2023; Lermen et al., 2023; Sun
et al., 2023), we focus our evaluations in a “task
customization” perspective, where each model is
trained on a specific task and is evaluated on a test
set from the same task.
3.1 Tasks & Datasets
To evaluate the performance of each Tied-
LoRA configuration across diverse data settings,
we utilized the following types of tasks:
Extractive QA is a common task where the
model is expected to “read” some relevant text (the
context) and answer questions. The answers are
usually exact sub-strings from the provided context.
We use SQuADv1 dataset (Rajpurkar et al., 2016)
in our experiments. Since the official test split of
this dataset does not contain ground-truth answers,

--- PAGE 4 ---
we use the validation set as our test set. We create
a validation set comprising of a random sample of
4800 examples extracted from the training set.
Summarization is a central problem in NLP
and several variations of summarization datasets
have been proposed. We employ the DialogSum
dataset (Chen et al., 2021) to study our models’ per-
formance on this task. DialogSum includes sum-
maries of real-word conversations on a diverse set
of topics and scenarios. This dataset was an attrac-
tive option as the length of the conversations and
summarizes are within the context lengths ( 4096
tokens) of the base language models.
Commonsense Natural Language Inference
(NLI) is a task designed to probe the ability
of language models to apply “commonsense rea-
soning” to choose a possible ending for a given
situation described in natural language. These
tasks are typically trivial for humans but language
models can still struggle. We use the HellaSwag
dataset (Zellers et al., 2019) to study the perfor-
mance of our proposed models on this type of task.
As HellaSwag contains multiple-choice questions,
it can be viewed as a classification problem.
Translation Machine translation is a natural lan-
guage generation task which is widely used in re-
search and industry. Translation is inherently mul-
tilingual and thus offers a challenging domain to
study our Tied-LoRA paradigm. There are several
large scale translation datasets but we focus on a
moderately sized IWSLT 2017 German-to-English
spoken language translation dataset (Cettolo et al.,
2017). With over 206ktraining examples this is
the largest dataset we study.
Mathematical Reasoning is a challenging do-
main where large language models still lag behind
human performance. Using PEFT methods on such
tasks further amplifies these challenges as there are
very few trainable parameters. In our experiments,
we use the GSM8K benchmark (Cobbe et al., 2021)
which contains 8.5K high-quality, grade-school
level math word problems. Each example in the
GSM8K benchmark contains a question and an
answer. The answers are provided with natural lan-
guage solutions which contain explanations of each
step used to obtain the final answer. The final nu-
merical answer is demarcated from the rest of the
natural language solution. We evaluate our models
by comparing these final numerical answers.3.2 Base Language Models
Although PEFT enables the base language model
to perform new tasks, the final performance heav-
ily depends on the inherent abilities learned during
pretraining. This necessitates investigating the per-
formance of Tied-LoRA on multiple base models
with different inherent capabilities. Therefore, we
use a relatively small two billion parameter, GPT-
2B-001 decoder-only model1released by NVIDIA
and the moderately large 7B LLaMA 2 model (Tou-
vron et al., 2023) released by Meta. Additionally,
these models also differ in the amount of pretrain-
ing data used. The GPT-2B-001 model was trained
on1.1trillion tokens of text from publicly avail-
able multilingual text spanning 53languages. The
LLaMA2 7B model was trained on 2trillion to-
kens of predominately English text. Both models
are auto-regressive language models with a context
size of 4096 tokens.
3.3 Implementation Details
We use the open-source NeMo Framework to im-
plement all the algorithms presented in this paper.
Our implementation is publicly available through
the NeMo GitHub repository.2We set max training
steps to 2k, but training was terminated sooner us-
ing early stopping with a patience of 10to prevent
over-fitting. We trained all configurations using
AdamW optimizer (Loshchilov and Hutter, 2017)
with a weight decay of 0.01and a cosine learning
rate schedule with 50warm-up steps.
For each Tied-Lora method we tried two learning
rates, a high rate of 1−4and a low learning rate of
1−5. While the “typical” range of the low-rank
dimension ris4−16we find that some complex
tasks benefit from higher rso we trained all our
models with a wide range of r∈ {2,4,8, . . . , 128}.
Each task was trained with a global batch size of
256and a validation check interval of 30steps. The
only exception was the IWSLT translation dataset
for which we set global batch size and validation
check interval of 1024 and60respectively. No
extensive hyper-parameter search was conducted.
We used greedy-decoding to generate the models’
predictions with a limit of 500tokens.
1https://huggingface.co/nvidia/GPT-2B-001
2https://github.com/NVIDIA/NeMo/tree/
adithyare/vera

--- PAGE 5 ---
Base Model Method Dialogsum GSM8K HellaSwag IWSLT 2017 Squad
RougeL r P% EM r P% Acc. r P% BLEU r P% EM r P%
LLaMA2 7BLoRA ( vBuA ) 40.76 8 100 32.75 64 100 91.97 16 100 41.30 8 100 88.52 2 100
Vera (vBὑ7uAὑ7) 38.77 8 9.4 27.22 64 1.2 89.91 16 4.7 40.22 8 9.4 87.69 2 37.5
TL1(vBὑ7uAὑ7) 38.73 8 0.8 27.07 64 0.8 90.03 16 0.8 40.34 8 0.8 87.72 2 0.8
TL2(vBὑ7uAὑ7) 38.69 8 10.2 27.07 64 2.0 90.11 16 5.5 40.35 8 10.2 87.67 2 38.3
TL3(vBὑ7uAὑ7) 40.20 8 2.3 17.74 64 2.3 89.38 16 2.3 39.93 8 2.3 87.34 2 2.3
TL4(vBὑ7uAὑ7) 39.46 8 2.3 21.00 64 2.3 89.46 16 2.3 40.34 8 2.3 87.06 2 2.3
TL5(vBὑ7uAὑ7) 40.62 8 3.1 30.33 64 3.1 91.75 16 3.1 40.01 8 3.1 87.11 2 3.1
TL6(vBὑ7uAὑ7) 39.24 8 12.5 31.77 64 4.3 91.15 16 7.8 41.33 8 12.5 87.97 2 40.6
Vera (vBὑ7uAὑ7) 40.07 64 9.4 29.11 16 1.2 90.47 2 4.7 40.41 16 9.4 87.69 2 37.5
TL1(vBὑ7uAὑ7) 39.74 16 1.6 29.95 16 0.2 90.52 4 0.2 40.52 64 6.3 87.72 2 0.8
TL2(vBὑ7uAὑ7) 39.81 64 15.7 28.73 16 1.4 90.32 2 4.8 40.50 128 22.0 87.67 2 38.2
TL3(vBὑ7uAὑ7) 40.20 8 2.3 24.34 4 0.1 90.27 8 1.2 40.48 16 4.7 87.62 8 9.4
TL4(vBὑ7uAὑ7) 40.17 16 4.7 25.70 8 0.3 90.18 4 0.6 40.65 16 4.7 87.72 4 4.7
TL5(vBὑ7uAὑ7) 40.62 8 3.1 30.33 64 3.1 91.75 16 3.1 41.37 16 6.3 88.22 4 6.3
TL6(vBὑ7uAὑ7) 39.71 16 15.6 31.77 64 4.3 91.90 64 17.2 41.37 32 21.9 88.49 4 43.8
GPT-2B-001LoRA ( vBuA ) 38.59 4 100 12.28 64 100 85.64 64 100 40.19 128 100 83.58 32 100
Vera (vBὑ7uAὑ7) 37.02 4 18.8 6.97 64 1.2 75.94 64 1.2 38.20 128 0.6 79.43 32 2.3
TL1(vBὑ7uAὑ7) 37.11 4 1.0 8.26 64 1.0 76.32 64 1.0 38.12 128 1.0 79.26 32 1.0
TL2(vBὑ7uAὑ7) 37.00 4 19.8 8.11 64 2.2 77.02 64 2.2 38.17 128 1.6 79.50 32 3.4
TL3(vBὑ7uAὑ7) 36.50 4 3.1 5.69 64 3.1 25.05 64 3.1 36.46 128 3.1 76.96 32 3.1
TL4(vBὑ7uAὑ7) 36.82 4 3.1 6.82 64 3.1 25.05 64 3.1 32.98 128 3.1 77.47 32 3.1
TL5(vBὑ7uAὑ7) 37.17 4 4.2 8.34 64 4.2 82.25 64 4.2 38.58 128 4.2 81.43 32 4.2
TL6(vBὑ7uAὑ7) 37.63 4 22.9 9.78 64 5.3 85.02 64 5.3 39.74 128 4.8 83.02 32 6.5
Vera (vBὑ7uAὑ7) 37.28 8 18.8 8.26 2 1.2 83.41 2 1.2 39.15 2 0.6 81.77 2 2.3
TL1(vBὑ7uAὑ7) 37.22 8 2.1 9.55 4 0.1 83.54 4 0.1 39.09 2 0.1 82.20 2 0.1
TL2(vBὑ7uAὑ7) 37.29 8 20.1 9.40 4 1.2 83.64 2 1.2 39.11 2 0.6 82.41 4 2.5
TL3(vBὑ7uAὑ7) 37.18 16 12.5 6.97 8 0.4 80.66 4 0.2 38.25 4 0.1 80.96 4 0.4
TL4(vBὑ7uAὑ7) 36.88 32 25.1 7.20 32 1.6 80.51 4 0.2 38.30 8 0.2 81.03 8 0.8
TL5(vBὑ7uAὑ7) 37.55 8 8.3 9.40 128 8.3 83.71 32 2.1 39.20 64 2.1 82.74 16 2.1
TL6(vBὑ7uAὑ7) 37.81 32 52.2 10.31 16 2.2 85.13 32 3.3 39.74 128 4.8 83.56 64 10.8
Table 2: The results entire spectrum of Tied-LoRA configurations on five tasks using LLaMA2 7B base model and
the GPT-2B-001 base model. For each base model section, the first row shows the best LoRA ( vBuA ) scores on
each task along with rank rat which the best score was achieved.
4 Results
Table 2 provides a detailed comparison of various
Tied-LoRA configurations across our 5tasks for
the LLaMA2 7B and GPT-2B-001 base models.
For each task we report the metric used, such as
RougeL (Lin and Och, 2004), Exact Match (EM),
Accuracy and BLEU (Papineni et al., 2002), and the
rankrused. For each model, the table is segmented
into two sections: The first section compares the
performance of all Tied-LoRA configurations at
thesame rank where LoRA ( vBuA ) achieved its
optimum score. The second section shows the best
performance of achieved by each configuration. In
addition to metric scores and rank ( r) we also re-
port parameter usage percentage (P%) as a compar-
ison to the parameter count of the best-performing
Lora configuration. This offers a direct measure
of efficiency, showing how each model, especially
TL5(vBὑ7uAὑ7) and TL 6(vBὑ7uAὑ7) , leverages a
smaller percentage of parameters compared to the
LoRA ( vBuA ) for achieving its results.
We can immediately see that LoRA ( vBuA ) is
the best performing model for both the 2B and7B base language models on most tasks. This
is hardly surprising as it is the most expen-
sive method with respect to trainable parame-
ters. The TL 6(vBὑ7uAὑ7) configuration demon-
strates the best overall performance among all Tied-
LoRA configurations for both model sizes and is
not far behind LoRA. For our translation task with
the LLaMA2 7B base model, TL 6(vBὑ7uAὑ7) out
performs LoRA ( vBuA ) while using 12.5%
of the number of parameters. This consistent
performance illustrates it effectiveness across
diverse model scales and task types. The
TL5(vBὑ7uAὑ7) configuration, while marginally
outperformed by TL 6(vBὑ7uAὑ7) , is notable for
its parameter efficiency. This method achieves com-
parable performance to TL 6(vBὑ7uAὑ7) for typical
ranks r= 8,16,32,64,128, but with a reduced
parameter count.
Both TL 5(vBὑ7uAὑ7) and TL 6(vBὑ7uAὑ7) show
robust performance at the same rank where tra-
ditional LoRA ( vBuA ) is optimized (i.e., per-
formed best). This implies that for systems pre-
tuned for LoRA ( vBuA ) , TL 5(vBὑ7uAὑ7) and
TL6(vBὑ7uAὑ7) can be utilized with the same rank

--- PAGE 6 ---
LoRA at Layer TL 5
(vBὑ7uAὑ7) 1 4 8 12 16 20 24 28 32
Iwslt2017 (BLEU) 37.94 38.99 39.47 38.68 38.10 35.33 33.24 28.90 22.40 41.37
Squad (EM) 85.30 86.50 87.55 85.51 80.01 71.71 65.90 60.70 56.22 87.73
GSM8K (EM) 13.04 19.33 19.56 14.93 11.67 6.14 4.92 3.56 2.80 27.07
Hellaswag (EM) 77.70 88.36 87.92 85.75 85.46 77.44 27.96 49.52 47.79 91.76
Dialogsum (RougeL) 37.75 37.80 39.17 38.73 37.26 34.55 33.54 33.02 31.64 38.73
Table 3: LoRA applied to a single layer in the transformer vs. TL 5(vBὑ7uAὑ7) . All results in this table are for a
rank of 16for the 7B base model and use the same number of trainable parameters.
configuration as a “drop-in” replacement.
On average, we observe a 1.36% decline
in TL 6(vBὑ7uAὑ7) performance compared to the
LoRA ( vBuA ) model with the LLaMA 2 7B
model. This decline, however, is marginally higher
at1.95% with the 2B model. These findings sug-
gest that the efficiency of TL 6(vBὑ7uAὑ7) config-
uration may enhance with a larger or more ca-
pable base models (such as the LLaMA2 70B
model). This hypothesis warrants a future explo-
ration which we leave for future research.
4.1 Task-Dependent Optimal Rank
From Table 2, we can see that the optimal rank
for LoRA ( vBuA ) varies significantly across dif-
ferent tasks. Furthermore, perhaps surprisingly, a
higher rank does not result in higher scores. For ex-
ample, for LoRA ( vBuA ) , a rank of 2 suffices for
achieving best performance for the SQuAD task,
while a higher rank of 64 is optimal for GSM8K.
In scenarios where traditional LoRA ( vBuA ) re-
quires a higher rank, Tied-LoRA, especially
TL5(vBὑ7uAὑ7) and TL 6(vBὑ7uAὑ7) , present an
effective alternative by delivering comparable per-
formance with substantially fewer parameters. For
instance, for GSM8K, TL 6(vBὑ7uAὑ7) needs only
4.3% of the parameters that LoRA ( vBuA ) uses,
while achieving a comparable performance (EM
score of 31.77vs.32.75for TL 6(vBὑ7uAὑ7) and
LoRA, respectively).
4.2 Layer Selection Vs. Tied-LoRA
The success of Tied-LoRA, specifically
TL5(vBὑ7uAὑ7) as seen from Table 2, begs
the question – Would adding LoRA to a single
transformer layer lead to similar performance as
TL5(vBὑ7uAὑ7) ? TL 5(vBὑ7uAὑ7) does not use
layer-specific parameters (recall the vanduare
frozen and set to 1) and has the same parameter
count as applying LoRA ( vBuA ) to a single
layer in the transformer model. To examine
this, we trained all tasks with LoRA applied toa single transformer layer’s attention projection
matrices. The obvious follow-up question is,
which layer should LoRA be applied to? We
attempt single-layer LoRA on the lowest (closest
to the input embeddings) layer, which we designate
as “Layer 1” all the way to the the highest layer,
“Layer 32” in 4 layer increments. We used the
LLaMa2 7B model for this investigation.
Table 3 compares the performances of single-
layer LoRA against TL 5(vBὑ7uAὑ7) (which uses
the same number of trainable parameters as single-
layer LoRA). The TL 5(vBὑ7uAὑ7) configuration
was considerably better than any of the layer se-
lection LoRA settings. Interestingly, we note that
when applying LoRA to a single transformer layer,
the lower layers (usually layer 4 or 8) resulted in
higher performance than higher layers. This sug-
gests that there is potentially a single low-rank up-
date that can be applied to all layers to boost perfor-
mance, but it is hard to find a low-rank update for
a single-layer that results in strong performance.
4.3 Stability Across Ranks
As indicated by Figures 2a and 2b, apart from
TL6(vBὑ7uAὑ7) , all other Tied-LoRA methods
experience a decline in performance when the rank
increases. This trend highlights a general chal-
lenge faced by Tied-LoRA configurations, with
TL6(vBὑ7uAὑ7) being an exception. Specifically,
TL3(vBὑ7uAὑ7) and TL 4(vBὑ7uAὑ7) exhibit the
most dramatic drop at higher ranks among all Tied-
LoRA configurations. We leave addressing these
limitations for future research.
Figures 2c and 2d show only the best
Tied-LoRA configurations, along with base-
lines LoRA ( vBuA ) and Vera ( vBὑ7uAὑ7) .
While TL 5(vBὑ7uAὑ7) aligns closely with
TL6(vBὑ7uAὑ7) at typical ranks of 4−16,
it also exhibits a small performance reduc-
tion at higher ranks. This pattern is repeated
for Vera ( vBὑ7uAὑ7) as well. In contrast,

--- PAGE 7 ---
TL6(vBὑ7uAὑ7) maintains high performance
across a broad range of ranks and is closest to
LoRA ( vBuA ) .
5 Related Work
Parameter-efficient fine-tuning (PEFT): Re-
cent work on PEFT of pretrained language models
has shown competitive capabilities, often match-
ing full fine-tuning performance for task-specific
model customization while utilizing significantly
fewer trainable parameters. Adapters (Houlsby
et al., 2019; Pfeiffer et al., 2021) introduce task-
specific parameters within the transformer layers
that adapt to a particular task. Prompt tuning based
methods such as P-Tuning and Prefix-Tuning (Li
and Liang, 2021; Liu et al., 2023) attempt to do
the same but via task-specific vectors that can be
appended to the inputs or at various layer represen-
tations. BitFit and IA3 (Ben Zaken et al., 2022;
Liu et al., 2022) are PEFT methods that attempt to
only alter bias vectors or scaling vectors in the base
large language model.
Low-Rank adaptation (LoRA): One of the
most popular PEFT techniques is LoRA, intro-
duced by Hu et al. (2021). LoRA employs
low-rank matrix approximations of full weights’
gradient-descent (GD) update to significantly re-
duce the number of trainable parameters. Impor-
tantly, LoRA can incorporate the low-rank updates
into the frozen base weights after the fine-tuning
process, avoiding any inference speed penalties or
model architecture changes. In summary, LoRA
paves the way for efficient fine-tuning for task-
specific customization of large models with mini-
mal computational overhead and no changes to the
model’s architecture.
Extensions to LoRA: Since its arrival, there have
been several efforts to improve the LoRA method.
These methods mostly concentrated around reduc-
ing the trainable parameters and memory footprint
while increasing the performance of the method on
downstream tasks. AdaLoRA (Zhang et al., 2023)
introduces dynamic rank adjustment for the low-
rank matrices during the fine-tuning process. The
fundamental premise of this extension is to opti-
mally distribute the parameter budget over model
layers. Chavan et al. (2023) combined the adapter
tuning with LoRA to derive a generalized frame-
work that utilized both methods for increased flexi-
bility and capability across a wide variety of tasksand datasets. Kopiczko et al. (2023) proposes the
VeRA method the freezes randomly initialized pro-
jection matrices and introduces trainable scaling
vectors that vary across layers. This method shows
similar performance to the LoRA ( vBuA ) method
while dramatically reducing the number of train-
able parameters. We view VeRA as one spe-
cific configuration which lies on end of the Tied-
LoRA spectrum.
Tangential to the efforts that aim to reduce train-
able parameters, QLoRA (Dettmers et al., 2023),
significantly reduces the memory usage of LoRA
using a 4-bit or 8-bit quantized base language
model during training. The method provides al-
gorithms and custom kernels to backpropagate gra-
dients through the frozen, quantized base model to
update low-rank matrices during training, resulting
in considerable reduction in memory usage. Com-
bining quantization and reduction in the number of
trainable parameters is a direction of future work.
Weight tying: Weight tying (Press and Wolf,
2017; Inan et al., 2017) is a common approach
that reduces the number of parameters by using the
same set of weights in different parts of the net-
work. Typically the input word embedding layer
and the output word embedding layer (sometimes
referred to as the language model head) are tied. In
this study, we apply weight tying to the low-rank
weight matrices used in LoRA, and share them
across the layers of the base language model. This
simple procedure leads to efficient training meth-
ods where the number of trainable parameters are
either unaffected by, or only increases marginally
with the number of hidden layers. As models get
deeper this approach naturally provides greater pa-
rameter reduction over original LoRA method.
Vision Transformers: Ideas similar to Tied-Lora
are also being explored in the vision based tasks.
Dong et al. (2023), for example, uses weight tying
and bottleneck adapters.
6 Conclusion & Future Work
In this paper, we introduced the Tied-
LoRA paradigm, a novel approach to enhance
the parameter efficiency of Lora by employing a
simple technique of weight-tying and selective
training of low-rank matrices.
Our empirical analysis demonstrates that the
TL6(vBὑ7uAὑ7) configuration achieves perfor-
mance comparable to Lora across various tasks,

--- PAGE 8 ---
2122232425262730405060
(a) 7B Base Model21222324252627304050
(b) 2B Base Model
21222324252627565758
(c) 7B Base Model212223242526274446485052
(d) 2B Base Model
LoRA ( vBuA ) Vera (vBὑ7uAὑ7) TL1(vBὑ7uAὑ7) TL2(vBὑ7uAὑ7)
TL5(vBὑ7uAὑ7) TL6(vBὑ7uAὑ7) TL3(vBὑ7uAὑ7) TL4(vBὑ7uAὑ7)
Figure 2: Plots showing the performance of the Tied-LoRAconfigurations averaged over tasks across all
ranks.Figures 2a and 2b display all Tied-LoRAconfigurations, while Figures 2c and 2d display the best Tied-
LoRAconfigurations with LoRA and Vera as baselines. Appendix A contains plots for each task and base model.
while utilizing only a fraction of the parameters
employed by Lora across a spectrum of low-rank
dimensions. This efficiency becomes more pro-
nounced at higher ranks, leading to a more aggres-
sive reduction in the number of trainable parame-
ters compared to Lora. Remarkably, in the trans-
lation task, TL 6(vBὑ7uAὑ7) surpassed Lora’s per-
formance while using only 12.5%of the number
of parameters. Our study highlights that the ben-
efits of this configuration are particularly evident
in tasks that leverage the inherent strengths of the
base language model, such as commonsense NLI,
extractive QA, and summarization. Tasks involv-
ing mathematical reasoning and arithmetic calcula-
tions, however, favor the sheer learning capacity of
LoRA with more parameters.
As language models continue to advance, the
Tied-LoRA configurations, with their optimized
efficiency, emerge as a promising candidate to re-
place traditional Lora in a broader range of applica-
tions. This progression underscores the relevance
of Tied-LoRA as a scalable solution in the dynamic
landscape of large language model customization.
For future research, we plan to delve into the
application of Tied-LoRA methods on larger base
models. This exploration aims to assess their scala-bility and effectiveness within the broader context
of large language models. Additionally, we in-
tend to investigate weight tying in other parameter-
efficient fine-tuning methods such as Adapters
and Prefix Tuning, both of which introduce layer-
specific parameters.
Limitations
PEFT methods are inherently sensitive to the base
large language which they are applied to as well as
the specific customization task. While we attempt
to test our methods on multiple base models com-
putation cost restricts us to only 2models (so far).
While extending our analysis to other models is
possible, extending to more tasks is more challeng-
ing as the variety of tasks is large. Furthermore,
predicting the behavior of a PEFT method on a new
task based on its performance on some existing task
is very challenging. Even in our analysis we did not
expect translation task to be an outlier (our Tied-
LoRA method out performed LoRA) because on
all the other tasks LoRA was slightly better. Thus,
we caution against very strong claims of task gener-
alization and highlight that while we show results
on diverse tasks there are still a wide range of tasks
we have not explored.

--- PAGE 9 ---
References
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. BitFit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1–9, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Jan Niehues, Sebastian Stüker, Katsuhito Sudoh,
Koichiro Yoshino, and Christian Federmann. 2017.
Overview of the IWSLT 2017 evaluation campaign.
InProceedings of the 14th International Conference
on Spoken Language Translation , pages 2–14, Tokyo,
Japan. International Workshop on Spoken Language
Translation.
Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing,
and Zhiqiang Shen. 2023. One-for-all: General-
ized lora for parameter-efficient fine-tuning. arXiv
preprint arXiv:2306.07967 .
Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang.
2021. DialogSum: A real-life scenario dialogue sum-
marization dataset. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 5062–5074, Online. Association for Computa-
tional Linguistics.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 .
Wei Dong, Dawei Yan, Zhijun Lin, and Peng Wang.
2023. Efficient adaptation of large vision trans-
former via adapter re-composing. arXiv preprint
arXiv:2310.06234 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. In 5th In-
ternational Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings . OpenReview.net.Dawid Jan Kopiczko, Tijmen Blankevoort, and
Yuki Markus Asano. 2023. Vera: Vector-
based random matrix adaptation. arXiv preprint
arXiv:2310.11454 .
Simon Lermen, Charlie Rogers-Smith, and Jeffrey
Ladish. 2023. Lora fine-tuning efficiently undoes
safety training in llama 2-chat 70b. arXiv preprint
arXiv:2310.20624 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality using
longest common subsequence and skip-bigram statis-
tics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL-
04), pages 605–612, Barcelona, Spain.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-
fel. 2022. Few-shot parameter-efficient fine-tuning
is better and cheaper than in-context learning. Ad-
vances in Neural Information Processing Systems ,
35:1950–1965.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt
understands, too. AI Open .
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume , pages
487–503, Online. Association for Computational Lin-
guistics.
Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vol-
ume 2, Short Papers , pages 157–163, Valencia, Spain.
Association for Computational Linguistics.

--- PAGE 10 ---
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Xianghui Sun, Yunjie Ji, Baochang Ma, and Xian-
gang Li. 2023. A comparative study between full-
parameter and lora-based fine-tuning on chinese in-
struction data for instruction following large language
model. arXiv preprint arXiv:2304.08109 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-
chine really finish your sentence? In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 4791–4800, Florence,
Italy. Association for Computational Linguistics.
Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and
Tuo Zhao. 2023. Adaptive budget allocation for
parameter-efficient fine-tuning. arXiv preprint
arXiv:2303.10512 .

--- PAGE 11 ---
A Breakdown of Performances with
Ranks

--- PAGE 12 ---
21222324252627383940iwslt2017
BLEU
(a) iwslt2017,2B21222324252627394041iwslt2017
BLEU
(b) iwslt2017,7B
2122232425262778808284squad
Acc.
(c) squad,2B21222324252627848688squad
Acc.
(d) squad,7B
212223242526274681012gsm8k
Acc.
(e) gsm8k,2B212223242526272530gsm8k
Acc.
(f) gsm8k,7B
21222324252627607080hellaswag
Acc.
(g) hellaswag,2B212223242526278486889092hellaswag
Acc.
(h) hellaswag,7B
21222324252627363738dialogsum
RougeL
(i) dialogsum,2B2122232425262738394041dialogsum
RougeL
(j) dialogsum,7B
LoRA ( vBuA ) Vera ( vBὑ7uAὑ7) TL1(vBὑ7uAὑ7) TL2(vBὑ7uAὑ7)
TL5(vBὑ7uAὑ7) TL6(vBὑ7uAὑ7) TL3(vBὑ7uAὑ7) TL4(vBὑ7uAὑ7)
Figure 3: Plots showing the performance of the Tied-LoRA configurations along with the baseline
LoRA ( vBuA ) for 5diverse tasks at 4different values for low-rank dimension setting. Note that we let the
plot for TL 3(vBὑ7uAὑ7) and TL 4(vBὑ7uAὑ7) go out of bounds to show details for the other curves.
