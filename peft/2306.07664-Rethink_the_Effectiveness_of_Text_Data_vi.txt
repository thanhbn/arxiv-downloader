# 2306.07664.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2306.07664.pdf
# Kích thước tệp: 317501 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Suy nghĩ lại về Hiệu quả của Tăng cường Dữ liệu Văn bản: 
Một Phân tích Thực nghiệm
Zhengxiang Shi và Aldo Lipani
Đại học College London
Gower St, London - Vương quốc Anh
Tóm tắt. Trong những năm gần đây, các mô hình ngôn ngữ (LMs) đã đạt được tiến bộ đáng kể trong việc thúc đẩy lĩnh vực xử lý ngôn ngữ tự nhiên (NLP). Tuy nhiên, tác động của các kỹ thuật tăng cường dữ liệu (DA) đối với hiệu suất tinh chỉnh (FT) của các LMs này đã là chủ đề tranh luận liên tục. Trong nghiên cứu này, chúng tôi đánh giá hiệu quả của ba phương pháp FT khác nhau kết hợp với dịch ngược qua một loạt 7 nhiệm vụ NLP đa dạng, bao gồm các loại phân loại và hồi quy, bao phủ các nhiệm vụ câu đơn và cặp câu. Trái ngược với các giả định trước đây rằng DA không góp phần nâng cao hiệu suất FT của LMs, phát hiện của chúng tôi cho thấy rằng tiếp tục tiền huấn luyện trên dữ liệu tăng cường có thể cải thiện hiệu quả hiệu suất FT của các nhiệm vụ hạ nguồn. Trong trường hợp thuận lợi nhất, tiếp tục tiền huấn luyện cải thiện hiệu suất của FT hơn 10% trong bối cảnh học ít mẫu. Phát hiện của chúng tôi nhấn mạnh tiềm năng của DA như một công cụ mạnh mẽ để tăng cường hiệu suất của LMs.

1 Giới thiệu
Trong những năm gần đây, sự phát triển của LMs đã cách mạng hóa lĩnh vực NLP [1, 2], dẫn đến tiến bộ đáng kể trong một loạt các nhiệm vụ hạ nguồn, bao gồm phân loại văn bản [3, 4], truy xuất thông tin [5, 6, 7, 8], và đa phương thức [9, 10, 11]. Mặc dù LMs đã cho thấy hiệu suất ấn tượng trong nhiều nhiệm vụ, đã có cuộc tranh luận về hiệu quả của các kỹ thuật tăng cường dữ liệu (DA) đơn giản, chẳng hạn như dịch ngược, để cải thiện hiệu suất FT.

Nghiên cứu trước đây [12] đã đánh giá các kỹ thuật DA, chẳng hạn như Dịch ngược, cho rằng những DA phổ biến không phụ thuộc nhiệm vụ này mang lại cải thiện hạn chế và không nhất quán cho các LMs tiền huấn luyện [13] trong nhiều nhiệm vụ phân loại cơ bản. Ngoài ra, [14] cho rằng hầu hết các phương pháp tăng cường trước đây chỉ mang lại lợi ích cận biên và thường không hiệu quả, chỉ ra rằng DA thường dẫn đến hiệu suất không ổn định và có thể kích hoạt chế độ thất bại, đặc trưng bởi sự sụt giảm hoặc biến động hiệu suất nghiêm trọng.

Trong nghiên cứu này, chúng tôi cung cấp một nghiên cứu thực nghiệm để đánh giá lại hiệu quả của DA văn bản với hai phương pháp FT dựa trên prompt tiên tiến [15, 16], cũng như FT dựa trên CLS thông thường [13], như được thể hiện trong Hình 1(a,b). Chúng tôi thực hiện thí nghiệm trên bảy nhiệm vụ NLP riêng biệt, bao gồm các nhiệm vụ phân loại và hồi quy liên quan đến câu đơn và cặp câu, để đánh giá hiệu quả của DA. Phát hiện của chúng tôi thách thức niềm tin được giữ trước đây rằng DA không nâng cao hiệu suất FT của LMs. Chúng tôi phát hiện rằng tiếp tục tiền huấn luyện LMs trên dữ liệu tăng cường

--- TRANG 2 ---
Tôi <mask> thích cái này <mask>. Nó tích cực Tôi thực sự thích bộ phim này. Nó <mask> Tôi <mask> thích cái này <mask>. (c) Mô hình hóa Ngôn ngữ có Mặt nạ Tôi thực sự thích bộ phim này. Bộ mã hóa(a) Tinh chỉnh dựa trên CLS Đầu CLS Lớp: 0 Bộ mã hóa(b) Tinh chỉnh dựa trên Prompt Đầu LM Verbalizer: Tích cực 0 thực sự, phim(d) Mô hình hóa Ngôn ngữ có Mặt nạ với Prompt thực sự, phim Bộ mã hóa Đầu LM Bộ mã hóa Đầu LM
Các lớp Mô hình Token Prompt Nhãn Vàng/Giả

Hình 1: Tổng quan về FT dựa trên CLS và dựa trên prompt, cùng với các mục tiêu tiền huấn luyện tiếp tục tương ứng của chúng.

dữ liệu có thể cải thiện đáng kể hiệu suất của các phương pháp FT, mang lại một lựa chọn thay thế hiệu quả để nâng cao hiệu suất mô hình trong các ứng dụng thực tế.

2 Công trình Liên quan
Phương pháp dựa trên prompt. Trong những năm gần đây, việc khám phá các phương pháp dựa trên prompt đã được tiến hành để nâng cao hiệu suất FT. PET/iPET [17] đã thích ứng FT dựa trên CLS [13] bằng cách trình bày nó như một vấn đề mô hình hóa ngôn ngữ có mặt nạ, được coi là phù hợp hơn với các mục tiêu tiền huấn luyện, như được minh họa trong Hình 1. Các nghiên cứu tiếp theo đã tiếp tục tinh chỉnh việc áp dụng các mẫu và từ nhãn thông qua các cơ chế tìm kiếm tự động [15] hoặc các prompt mềm có thể được cập nhật độc lập với bất kỳ từ nào [16].

Tiếp tục Tiền huấn luyện. Nghiên cứu trước đó, chẳng hạn như [12, 14], đã đặt câu hỏi về hiệu quả của DA đơn giản trong việc cải thiện hiệu suất FT cho các nhiệm vụ hạ nguồn. Các nghiên cứu trước [18, 19] đã chứng minh hiệu quả của việc tiếp tục tiền huấn luyện trong việc cải thiện hiệu suất mô hình ngay cả với hàng trăm ví dụ không được gắn nhãn. Tuy nhiên, hiệu quả của việc tiếp tục tiền huấn luyện trên dữ liệu tăng cường dịch ngược không rõ ràng trong bối cảnh học ít mẫu.

3 Kiến thức Nền tảng
Trong phần này, chúng tôi cung cấp một tổng quan ngắn gọn về các phương pháp FT và phương pháp tiền huấn luyện tiếp tục tương ứng của chúng. Hình 1(a) minh họa FT dựa trên CLS thông thường [13], huấn luyện vector đầu ra của token [CLS] bằng cách sử dụng một lớp đầu bổ sung. Tiền huấn luyện thêm trên các văn bản liên quan đến nhiệm vụ (xem Hình 1c) trước FT dựa trên CLS thường dẫn đến hiệu suất mô hình được cải thiện [20, 18].

Tuy nhiên, tồn tại sự khác biệt giữa mục tiêu tiền huấn luyện và mục tiêu FT dựa trên CLS, dẫn đến nghiên cứu dựa trên prompt để nâng cao hiệu suất mô hình ngôn ngữ. Hình 1(b) chứng minh rằng FT dựa trên prompt được thiết kế như một vấn đề MLM với mục tiêu dự đoán token bị che [17]. Cụ thể, văn bản đầu vào X được điều kiện hóa bằng cách sử dụng một mẫu prompt cụ thể ˜X=T(X), chứa một token đặc biệt, [MASK]. FT dựa trên prompt sau đó kết nối vector đầu ra liên quan đến token [MASK] với một từ nhãn. Xác suất dự đoán lớp y∈Y được tính như sau:

--- TRANG 3 ---
Tập dữ liệu |Y| L #Huấn luyện #Kiểm tra Loại Nhãn (nhiệm vụ phân loại)
SST-5 5 18 8,544 2,210 Cảm xúc rất tích cực, tích cực, trung tính, tiêu cực, rất tiêu cực
MR 2 20 8,662 2,000 Cảm xúc tích cực, tiêu cực
CR 2 19 1,775 2,000 Cảm xúc tích cực, tiêu cực
MPQA 2 3 8,606 2,000 Cực tính Ý kiến tích cực, tiêu cực
Subj 2 23 8,000 2,000 Tính chủ quan chủ quan, khách quan
TREC 6 10 5,452 500 Phân loại câu hỏi từ viết tắt, thực thể, mô tả, con người, địa điểm, số
STS-B R 11/11 5,749 1,500 Độ tương tự câu -

Bảng 1: Các tập dữ liệu được đánh giá trong công trình này. |Y|: số lớp cho các nhiệm vụ phân loại (với một ngoại lệ: STS-B là một nhiệm vụ hồi quy có giá trị thực trong khoảng [0,5]). L: số từ trung bình trong (các) câu đầu vào.

p(y|X) = p([MASK] = M(y)|˜X), (1)

trong đó verbalizer M: Y → V ánh xạ không gian nhãn nhiệm vụ thành các từ riêng lẻ trong từ vựng V. FT dựa trên prompt có thể sử dụng các mẫu prompt cứng hoặc mềm T, với các từ nhãn có thể là một phần của các mẫu prompt [16]. Các mẫu prompt cứng [17] đòi hỏi thiết kế cẩn thận các prompt và từ nhãn cho mỗi nhiệm vụ. Tuy nhiên, việc sử dụng prompt cứng được phát hiện là không tối ưu và nhạy cảm với việc lựa chọn prompt. Các prompt mềm [16] được đề xuất để sử dụng các token không sử dụng từ từ vựng V hoặc các token bổ sung như các embedding có thể điều chỉnh cho các mẫu prompt, có thể được huấn luyện trực tiếp với giám sát cụ thể cho nhiệm vụ. Một nghiên cứu gần đây [19] đã đề xuất tiền huấn luyện tiếp tục dựa trên prompt trước FT dựa trên prompt để nâng cao thêm hiệu suất mô hình ngôn ngữ trên các nhiệm vụ hạ nguồn, như được mô tả trong Hình 1(d).

4 Thí nghiệm
Trong phần này, chúng tôi đánh giá tác động của DA (tức là dịch ngược) trên tất cả các phương pháp so sánh. Ngoài ra, chúng tôi trình bày các tập dữ liệu và đường cơ sở.

Tập dữ liệu. Nghiên cứu của chúng tôi thực hiện một phân tích toàn diện về 7 tập dữ liệu NLP, bao gồm các nhiệm vụ phân loại và hồi quy. Chúng tôi lấy 6 nhiệm vụ câu đơn (SST-5 [21], MR [22], CR [23], MPQA [24], Subj [25], TREC [26]) và 1 nhiệm vụ cặp câu tiếng Anh (STS-B [27]), như được thể hiện trong Bảng 1. Theo [17, 15, 16, 19], chúng tôi lấy mẫu K-shot (K=16) mỗi lớp từ tập huấn luyện đầy đủ của mỗi tập dữ liệu.

Đường cơ sở. Chúng tôi huấn luyện các ví dụ K-shot bằng ba phương pháp FT khác nhau, có hoặc không kết hợp dịch ngược như DA. Các phương pháp như sau: (1) "FT dựa trên CLS": xem Hình 1a; (2) "FT dựa trên Prompt (cứng)": FT với các prompt và từ nhãn chất lượng cao được tạo thủ công hoặc tự động [17] (xem Hình 1b). Vui lòng tham khảo Bảng 2 để biết chi tiết mẫu; và (3) "FT dựa trên Prompt (mềm)": FT với các prompt mềm sử dụng các token bổ sung cho cả mẫu và từ nhãn [16], trong đó cùng một mẫu được áp dụng cho tất cả các nhiệm vụ (xem Hình 1b). Chúng tôi sử dụng các mẫu SST-5 và STS-B cho tất cả các nhiệm vụ câu đơn và nhiệm vụ cặp câu, tương ứng.

--- TRANG 4 ---
Nhiệm vụ Mẫu Từ nhãn
SST-5 <S1>Nó <mask>. rất tích cực: tuyệt vời, tích cực: tốt, trung tính: ổn, tiêu cực: tệ, rất tiêu cực: kinh khủng
MR <S1>Nó <mask>. tích cực: tuyệt vời, tiêu cực: kinh khủng
CR <S1>Nó <mask>. tích cực: tuyệt vời, tiêu cực: kinh khủng
MPQA <S1> <mask>. tích cực: tích cực, tiêu cực: tiêu cực
Subj <S1>Điều này <mask>. chủ quan: chủ quan, khách quan: khách quan
TREC <mask>:<S1> từ viết tắt: Biểu hiện, thực thể: Thực thể, mô tả: Mô tả con người: Con người, địa điểm: Địa điểm, số: Số
STS-B <S1> <mask>,<S2> có: Có, không: Không

Bảng 2: Các mẫu và từ nhãn được sử dụng cho FT dựa trên prompt.

Để so sánh hiệu quả của học giám sát trực tiếp trên dữ liệu tăng cường từ dịch ngược [28], chúng tôi sử dụng dữ liệu tăng cường làm kho ngữ liệu cho việc tiếp tục tiền huấn luyện với mục tiêu mô hình hóa ngôn ngữ có mặt nạ. Do đó, chúng tôi huấn luyện ba loại phương pháp FT này từ ba loại checkpoint khác nhau để đánh giá hiệu quả tương đối của chúng: (i) checkpoint RoBERTa-Large có sẵn [13]; (ii) checkpoint tiền huấn luyện thích ứng nhiệm vụ (TAPT) [20, 29] cho FT dựa trên CLS; và (iii) checkpoint tiền huấn luyện tiếp tục dựa trên prompt (PCP) [19] cho FT dựa trên prompt.

Chi tiết Huấn luyện. Chúng tôi thực hiện tìm kiếm lưới cho tỷ lệ học trong tập {1e-5, 2e-5, 5e-5} với kích thước batch là 8. Chúng tôi huấn luyện mô hình trong 1.000 bước, đánh giá hiệu suất mỗi 100 bước, và chọn mô hình tốt nhất dựa trên tập đánh giá. Chúng tôi tăng cường mỗi ví dụ bằng cách sử dụng bản dịch Anh-Đức và Anh-Nga, dẫn đến hai ví dụ tăng cường cho mỗi ví dụ gốc.

Tập dữ liệu SST-5 MR CR MPQA Subj TREC STS-B
Chỉ số Đánh giá (acc) (acc) (acc) (acc) (acc) (acc) (Pear.)
Đa số (đầy đủ) 23.1 50.0 50.0 50.0 50.0 18.8 -
FT dựa trên CLS 41.7±1.3 76.3±3.2 79.5±3.8 65.1±12.6 91.7±0.4 80.3±5.8 46.0±16.3
+ BT 40.8±2.0↓ 71.1±5.7↓ 78.9±3.2↓ 69.2±4.3↑ 91.0±1.9↓ 83.1±9.1↑ 51.5±22.6↑
+ TAPT 41.9±2.2↑ 76.1±7.1↓ 85.3±3.6↑ 75.3±5.0↑ 91.8±1.2↑ 83.8±6.4↑ 41.9±19.0↓
FT dựa trên Prompt (cứng) 46.7±1.5 86.2±1.2 90.7±0.8 80.8±6.9 91.0±1.1 84.7±4.4 67.7±8.1
+ BT 45.4±2.2↓ 85.5±1.3↓ 91.1±0.4↑ 82.8±5.1↑ 91.3±1.0↑ 86.1±4.3↑ 66.3±7.1↓
+ PCP 49.1±1.5↑ 87.0±1.4↑ 91.3±0.9↑ 85.9±1.9↑ 91.5±1.3↑ 86.8±3.9↑ 70.1±8.1↑
FT dựa trên Prompt (mềm) 48.0±0.7 86.8±1.4 90.8±1.3 81.2±6.8 90.3±2.1 83.0±3.0 63.7±6.8
+ BT 46.7±0.9↓ 86.1±1.4↓ 91.0±0.9↑ 82.9±1.5↑ 90.8±1.0↑ 85.8±2.6↑ 69.1±8.4↑
+ PCP 49.9±1.2↑ 85.9±1.4↓ 91.7±1.2↑ 84.6±2.0↑ 91.4±1.5↑ 86.3±2.3↑ 69.6±7.9↑

Bảng 3: Kết quả kiểm tra sử dụng RoBERTa-large, trong đó trung bình và độ lệch chuẩn được báo cáo trên 5 hạt giống. Mũi tên xanh lá cây và đỏ biểu thị các thay đổi tích cực/tiêu cực so với các đường cơ sở FT không liên quan đến dịch ngược. Hiệu suất tốt nhất trên mỗi tập dữ liệu được làm nổi bật bằng màu xanh dương.

Kết quả. Bảng 3 trình bày hiệu suất của ba phương pháp FT khác nhau, liên quan đến việc sử dụng các ví dụ tăng cường như các thể hiện huấn luyện giám sát hoặc tiền huấn luyện tiếp tục. Kết quả thí nghiệm của chúng tôi tiết lộ hai quan sát chính: (1) sử dụng các ví dụ tăng cường cho tiền huấn luyện tiếp tục (TAPT hoặc PCP) thường dẫn đến cải thiện lớn hơn so với việc sử dụng chúng trong học giám sát, và (2) tiền huấn luyện tiếp tục đôi khi dẫn đến cải thiện hiệu suất đáng kể. Chúng tôi đi sâu vào những phát hiện này dưới đây.

#1. Tiền huấn luyện tiếp tục (TAPT hoặc PCP) trên ba phương pháp FT khác nhau dẫn đến cải thiện hiệu suất trong 18 trên 21 trường hợp, trong khi sử dụng dữ liệu tăng cường cho huấn luyện giám sát dẫn đến cải thiện chỉ trong 11 trên 21 trường hợp. Hơn nữa, hiệu suất trung bình của FT với tiền huấn luyện tiếp tục là 77.0% trên tất cả các tập dữ liệu và phương pháp FT, trong khi hiệu suất trung bình của FT sử dụng huấn luyện giám sát trên dữ liệu tăng cường là khoảng 75.5%. Những kết quả này làm nổi bật lợi ích của tiền huấn luyện tiếp tục.

#2. Trong một số trường hợp, tiến hành tiền huấn luyện tiếp tục (TAPT hoặc PCP) trên LMs với dữ liệu tăng cường trước khi tiến hành FT có thể dẫn đến cải thiện đáng kể. Cụ thể, phương pháp này nâng cao hiệu suất của FT dựa trên prompt (cứng) từ 46.7% lên 49.1% trên tập dữ liệu SST-5 và từ 80.8% lên 85.9% trên tập dữ liệu MPQA. Đáng chú ý, nó tăng hiệu suất của FT dựa trên CLS từ 65.1% lên 75.3% trên tập dữ liệu MPQA, dẫn đến sự tăng giá trị tuyệt đối khoảng 6%. Những phát hiện này thách thức kết luận của nghiên cứu trước [14] cho rằng các kỹ thuật DA chỉ mang lại lợi ích nhỏ.

5 Kết luận
Tóm lại, nghiên cứu của chúng tôi thách thức quan niệm về tác động hạn chế của tăng cường dữ liệu đối với FT LMs trong các nhiệm vụ NLP. Chúng tôi cho thấy rằng tiền huấn luyện tiếp tục trên dữ liệu tăng cường có thể cải thiện hiệu quả hiệu suất mô hình.

Tài liệu tham khảo
[1] Pin Ni, Yuming Li, Gangmin Li, và Victor Chang. Các phương pháp hiểu ngôn ngữ tự nhiên dựa trên nhiệm vụ kết hợp phát hiện ý định và điền vào chỗ trống cho tương tác giọng nói IoT. Neural Computing and Applications, 2020.

[2] Pin Ni, Qiao Yuan, Raad Khraishi, Ramin Okhrati, Aldo Lipani, và Francesca Medda. Nhúng mạng nơ-ron đồ thị dựa trên eigenvector và dự đoán xếp hạng tin cậy trong mạng bitcoin. ICAIF '22, 2022.

[3] Zhengxiang Shi, Qiang Zhang, và Aldo Lipani. Stepgame: Một benchmark mới cho lý luận không gian đa bước mạnh mẽ trong văn bản. Trong AAAI 2022.

[4] Zhengxiang Shi, Pin Ni, Meihui Wang, To Eun Kim, và Aldo Lipani. Trình phân tích thành phần dựa trên attention. Trong ESANN, Bruges, Bỉ, 2022.

[5] Hossein A. Rahmani, Mohammad Aliannejadi, Mitra Baratchi, và Fabio Crestani. Mô hình hóa địa lý và thời gian kết hợp dựa trên phân rã ma trận cho đề xuất điểm quan tâm. Trong ECIR. Springer, 2020.

[6] Xiao Fu và Aldo Lipani. Priming và actions: Một phân tích trong hệ thống tìm kiếm đối thoại. SIGIR, 2023.

[7] Xiao Fu, Emine Yilmaz, và Aldo Lipani. Đánh giá paradigm cranfield cho hệ thống tìm kiếm đối thoại. ICTIR, 2022.

[8] Zhengxiang Shi, Xi Wang, và Aldo Lipani. Học tương phản tự cho đề xuất dựa trên phiên. arXiv preprint arXiv:2306.01266, 2023.

--- TRANG 6 ---
[9] Zhengxiang Shi, Yue Feng, và Aldo Lipani. Học thực hiện các hành động hoặc đặt câu hỏi làm rõ. Trong Findings of NAACL 2022.

[10] Mariya Hendriksen, Maurits Bleeker, Svitlana Vakulenko, Nanne van Noord, Ernst Kuiper, và Maarten de Rijke. Mở rộng clip cho truy xuất từ danh mục sang hình ảnh trong thương mại điện tử. Trong ECIR, 2022.

[11] Zhengxiang Shi, Jerome Ramos, To Eun Kim, Xi Wang, Hossein A Rahmani, và Aldo Lipani. Khi nào và hỏi gì thông qua trạng thái thế giới và hướng dẫn văn bản: Giải pháp thách thức iglu nlp. NeurIPS IGLU Competition Workshop, 2023.

[12] Shayne Longpre, Yu Wang, và Chris DuBois. Tăng cường dữ liệu không phụ thuộc nhiệm vụ hiệu quả như thế nào đối với các transformer tiền huấn luyện? Trong Findings of EMNLP 2020. ACL, 2020.

[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: Một phương pháp tiền huấn luyện bert được tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692, 2019.

[14] Jing Zhou, Yanan Zheng, Jie Tang, Li Jian, và Zhilin Yang. FlipDA: Tăng cường dữ liệu hiệu quả và mạnh mẽ cho học ít mẫu. Trong ACL. ACL, Tháng 5 2022.

[15] Tianyu Gao, Adam Fisch, và Danqi Chen. Làm cho các mô hình ngôn ngữ tiền huấn luyện trở thành người học ít mẫu tốt hơn. Trong ACL, trang 3816–3830, Trực tuyến, Tháng 8 2021. ACL.

[16] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, và Huajun Chen. Prompt có thể phân biệt làm cho các mô hình ngôn ngữ tiền huấn luyện trở thành người học ít mẫu tốt hơn. Trong ICLR, 2022.

[17] Timo Schick và Hinrich Schütze. Khai thác câu hỏi cloze cho phân loại văn bản ít mẫu và suy luận ngôn ngữ tự nhiên. Trong ACL. ACL, Tháng 4 2021.

[18] Zhengxiang Shi, Francesco Tonolini, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai, và Yunlong Jiao. Suy nghĩ lại về học bán giám sát với các mô hình ngôn ngữ. Trong Findings of ACL 2023, Toronto, Canada, 2023. Association for Computational Linguistics.

[19] Zhengxiang Shi và Aldo Lipani. Đừng dừng tiền huấn luyện? Làm cho tinh chỉnh dựa trên prompt trở thành người học mạnh mẽ. Trong Arxiv, 2023.

[20] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, và Noah A. Smith. Đừng dừng tiền huấn luyện: Thích ứng các mô hình ngôn ngữ với các miền và nhiệm vụ. Trong ACL, trang 8342–8360. ACL, Tháng 7 2020.

[21] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, và Christopher Potts. Các mô hình sâu đệ quy cho tính hợp thành ngữ nghĩa trên một treebank cảm xúc. Trong emnlp, 2013.

[22] Bo Pang và Lillian Lee. Nhìn thấy các ngôi sao: Khai thác mối quan hệ lớp cho phân loại cảm xúc liên quan đến thang đo xếp hạng. Trong acl, 2005.

[23] Minqing Hu và Bing Liu. Khai thác và tóm tắt đánh giá khách hàng. Trong Hội nghị quốc tế ACM SIGKDD về Khám phá tri thức và khai thác dữ liệu, 2004.

[24] Janyce Wiebe, Theresa Wilson, và Claire Cardie. Chú thích các biểu hiện ý kiến và cảm xúc trong ngôn ngữ. Language resources and evaluation, 39(2-3), 2005.

[25] Bo Pang và Lillian Lee. Một giáo dục tình cảm: Phân tích cảm xúc sử dụng tóm tắt tính chủ quan dựa trên cắt tối thiểu. Trong acl, 2004.

[26] Ellen M Voorhees và Dawn M Tice. Xây dựng một bộ sưu tập kiểm tra trả lời câu hỏi. Trong SIGIR, 2000.

[27] Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, và Lucia Specia. SemEval nhiệm vụ 1: Độ tương tự văn bản ngữ nghĩa đánh giá tập trung đa ngôn ngữ và liên ngôn ngữ. 2017.

[28] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, và Michael Auli. fairseq: Một bộ công cụ nhanh, có thể mở rộng cho mô hình hóa chuỗi. Trong NAACL-HLT, 2019.

[29] Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, và Yue Zhang. AdaPrompt: Huấn luyện mô hình thích ứng cho NLP dựa trên prompt. Trong Findings of EMNLP 2022.
