# 2304.09402.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2304.09402.pdf
# File size: 969980 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MIXPRO: Simple yet Effective Data Augmentation for
Prompt-based Learning
Bohan Li1, Longxu Dou1, Yutai Hou1, Yunlong Feng1, Honglin Mu1
Qingfu Zhu1, Qinghua Sun2,3, Wanxiang Che1♠
1Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
2Jilin Kexun Information Technology Co., Ltd., Beijing, China
3iFLYTEK Research, Beijing, China
{bhli, lxdou, ythou, ylfeng, hlmu, qfzhu}@ir.hit.edu.cn
qhsun2@iflytek.com ,car@ir.hit.edu.cn
Abstract
Prompt-based learning has shown considerable promise in reformulating various
downstream tasks as cloze problems by combining original input with a prede-
termined template. This approach demonstrates its effectiveness, especially in
few-shot learning scenarios, where the model is trained on a scarce amount of
data. Despite its successes, the limited templates and text in few-shot prompt-based
learning scenarios leave significant room for performance improvement. Moreover,
existing methods sometimes resort to model ensembles, which, while effective,
could potentially hamper model efficiency due to increased computational de-
mands (Schick & Schütze, 2021b). To address these issues, we introduce MIXPRO,
an augmentation method designed to augment both the vanilla input text and the
templates. We implement this through the token-level, the sentence-level, and the
template-level Mixup strategies. The experimental results on five few-shot datasets
show that MIXPROoutperforms other augmentation baselines, improving model
performance by an average of 5.08% compared to before augmentation.
1 Introduction
Prompt-based learning has recently received significant attention (Liu et al., 2021a). This approach
reformulates downstream tasks as cloze questions using a prompt template (Schick & Schütze,
2021b). Hard prompts, which use an actual text string suitable for human reading as the template,
have demonstrated excellent performance in a variety of downstream tasks (Brown et al., 2020a;
Gao et al., 2021a; Seoh et al., 2021; Qi et al., 2022; Zhong et al., 2021a; Ben-David et al., 2022).
The templates play a crucial role in prompt-based learning, as pre-trained language models output
labels by utilizing them to transform different tasks into statistical cloze questions (Clark et al., 2019).
Prompt-based learning can also be applied to few-shot learning settings, where the model can leverage
information from a small number of training instances directly (Schick & Schütze, 2021b).
However, the performance of few-shot prompt-based learning models can be sensitive to templates
variation (Gao et al., 2021b; van de Kar et al., 2022), with small changes causing significant perfor-
mance drops (Cao et al., 2022). Moreover, existing methods (Schick & Schütze, 2021b) using model
ensembles with different templates would constrain the model inference efficiency. Furthermore,
model performance on Natural Language Processing (NLP) tasks is often heavily dependent on the
size of the training data (Ren et al., 2021). Therefore, we argue that the limited number of samples
and templates in few-shot prompt-based learning significantly limits model performance.
♠Corresponding author.
Preprint. Under review.arXiv:2304.09402v2  [cs.CL]  11 Nov 2023

--- PAGE 2 ---
Decision Boundary Before / After Optimization /Original Samples Vicinal Samples in VRMClass AClass B
（a）Vicinal Samples by Mixup Original Samples
Interpolation LineClass B
Class A40% A + 60% B
（b）Figure 1: (a) The purpose of VRM is to optimize for both the original samples and the vicinal samples, enhancing
the model’s robustness and generalization capability. (b) Mixup can be viewed as an implementation of VRM,
creating vicinal samples by interpolation.
To address this challenge, we intend to adopt a data augmentation (DA) strategy (Chen et al., 2022b),
enhancing both vanilla input text and templates by synthesizing additional distinct samples based
on original samples (Simard et al., 2000; Szegedy et al., 2014; Wei & Zou, 2019; Li et al., 2022).
DA methods have demonstrated their superiority in many scenarios, particularly in low-resource
settings where annotations are difficult and require a significant amount of expert knowledge or
experience (Hu et al., 2019; Anaby-Tavor et al., 2020; Chen et al., 2021). Vicinal Risk Minimization
(VRM) (Chapelle et al., 2000) offers significant inspiration in this regard. Its core principle is to
optimize models not just on the original data but also in the vicinal regions surrounding that data
(Fig. 1 (a)). This approach bolsters the model’s resilience to minute data perturbations.
Inspired by VRM, in this paper, we introduce the Mixup strategy (Zhang et al., 2018b) into few-shot
prompt-based learning. As an implementation strategy of VRM, Mixup linearly blends two training
samples and their respective labels, effectively generating new vicinal samples within the data space
(Fig. 1(b)). Specifically, our method is a comprehensive three-level Mixup for Prompt-based learning
(MIXPRO), including the token-level, the sentence-level, and the template-level Mixup strategies. It
blends word embeddings, mixes hidden [MASK] prompt representations, and uses diverse templates
during training.
We conduct experiments on five Natural Language Understanding (NLU) datasets and demonstrate
thatMIXPROconsistently outperforms previous DA baselines. It improves model performance by an
average of 5.08% compared to before augmentation, with specific gains of 4.20%,9.76%, and 6.49%
on CB (de Marneffe et al., 2019), RTE (Dagan et al., 2005; Bar-Haim et al., 2006), and BoolQ (Clark
et al., 2019) respectively. Notably, MIXPROis more efficient, using only 1/ninference time, where
ndenotes the template count of the corresponding task. We also conduct ablation experiments to
analyze the contributions of three-level Mixup strategies, the necessity of augmenting both text and
templates, and the fluctuation of model performance.
Our contributions are as follows:
•We systematically investigate the challenges of few-shot prompt-based learning augmen-
tation, which include high sensitivity to templates, limited input text and templates, and
inefficient inference time cost.
•We introduce MixPro, a comprehensive DA framework for the entire prompt, which
encompasses both the vanilla input text and the templates with three-level Mixup strategies.
•Our proposed method achieves the best performance compared to other augmentation
baselines across five NLU datasets on average and significantly improves model performance
by5.08%.
2

--- PAGE 3 ---
2 Background
We start by briefly introducing the basics of prompt-based learning. Then, we introduce the vanilla
Mixup strategy (Guo et al., 2019), which serves as our baseline.
2.1 Problem Definition: Prompt-based Learning
Pre-trained language models (PLMs) like BERT (Devlin et al., 2019b) and ALBERT (Lan et al.,
2020) have become essential in NLP, showing proficiency in various tasks. Recent studies (Brown
et al., 2020b; Schick & Schütze, 2021b) demonstrate their ability for few-shot learning, transforming
input text into prompts via templates for training without parameter updates, utilizing only a few
support instances (Liu et al., 2021a).
Let’s take PET (Schick & Schütze, 2021b) as an example, which is a popular work in prompt-based
learning and also serves as our backbone. In the sentiment analysis task, we are given a vanilla input
textx=“This movie is amazing.” and we can choose a template t= “The feedback is [MASK] .” to
construct a prompt pas follows:
p=This movie is amazing. The feedback is [MASK] . (1)
We then ask the pre-trained language model to fill in the [MASK] symbol with a word (e.g., “positive”
or “negative”) from a specific set W. The chosen word w∈Wis mapped to a label through an
injective function called a verbalizer v:W→L, where Lis the label set of the corresponding task.
This label serves as the final prediction of the model.
2.2 Why Augment Few-shot Prompt-based Learning?
Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020b), LLAMA-2 (Touvron et al.,
2023), and PaLM-2 (Anil et al., 2023) enable remarkable performances across a wide range of tasks.
Additionally, they perform well even in zero/few-shot settings (Yuan et al., 2023). However, they
can sometimes struggle in specialized domains with scarce data, including fields like science (Cohan
et al., 2019), psychology (Wang et al., 2023), and medicine (Dernoncourt & Lee, 2017), as these
models largely rely on general corpora. Moreover, training LLMs in low-resource settings can
be both challenging and costly due to their vast number of parameters complicates the training
process (Yang et al., 2023). There is a data shortage issue, and training such models requires high
costs and demands advanced techniques (Yuan et al., 2023). These demands make them less practical
in certain situations.
In contrast, in low-resource scenarios, smaller and specialized PLMs (e.g., BERT (Devlin et al.,
2019a), ALBERT (Lan et al., 2019)) are good choices for rapid adaptation to downstream tasks (Yuan
et al., 2023; Li et al., 2023). This can be achieved through few-shot prompt-based learning to
transform downstream tasks into prompts given some annotated templates (Schick & Schütze, 2021a).
This technology provides adaptability to new tasks with minimal task-specific data, is computationally
efficient, saves resources, and leverages vast knowledge from PLMs.
Few-shot prompt-based learning, while promising, faces three key challenges. (1)Its performance is
greatly affected by template and training example choices, leading to significant drops with minor
changes (Nie et al., 2022). (2)Using multiple templates for training and ensembling models during
inference is necessary, but inefficient for practical applications. (3)The scarcity of templates and
text in few-shot prompt-based learning restricts model accuracy. Addressing these challenges can
enhance few-shot prompt-based learning model performance.
In this paper, we argue that the limited number of samples and templates in few-shot prompt-based
learning is the core reason. Building upon this foundation, data augmentation techniques can
mitigate the high cost of manual annotation, thereby automating further improvements in model
performance (Li et al., 2022).
2.3 The Mixup Strategy
The Mixup strategy expands training distribution by creating virtual examples from nearby examples.
By encouraging the model’s linear behavior between examples, Mixup minimizes erratic predictions
beyond the training set (Zhang et al., 2018b), yielding a more robust and generalizable model.
3

--- PAGE 4 ---
(a) Original Prompt
(b) Label -preserving and Label -flipping Augmented Prompt GenerationThis movie is amazing. The feedback is [MASK].
This movie is amazing.T5 This movie is great.
T5 This movie sucks .
T5 The feedback is [MASK]. The review is [MASK].This movie is great. The review is [MASK].
This movie sucks . The review is [MASK]. NegativePositiveNegativePositive
PositivePositiveFigure 2: Based on the original prompt (a), we generate both label-preserving and label-flipping prompts through
T5 (b). The augmentation is applied to the entire prompt, including the vanilla input text and the template (in
italics ).
To apply the Mixup strategy, a pair of samples (x, y)and(x′, y′), where xandx′denote the vanilla
input text, and yandy′are their one-hot labels, are chosen. Mixup then minimizes the sample loss
from their vicinity distribution. The vanilla Mixup strategy is defined as follows:
xmixup =λ·x+ (1−λ)·x′, (2)
ymixup =λ·y+ (1−λ)·y′, (3)
where λis the Mixup ratio drawn from a Beta distribution λ∼β(α, α), with αbeing a hyperpa-
rameter controlling the degree of mixing. The Mixup strategy generates a new synthetic sample
(xmixup , ymixup )for model training. By interpolating between pairs of training examples, Mixup
encourages the model to learn more smoothly and generalize better to unseen data.
To enhance model robustness, mixing original samples with their label-preserving orlabel-flipping
augmentations is common, enhancing the distribution and boosting performance (Cheng et al., 2020;
Hendrycks et al., 2020). We follow a similar operation in our MIXPROZhou et al. (2021). Given the
original prompt ( p=x, t) in Sec.2.1, we apply T5 (Raffel et al., 2020) to generate label-preserving
and label-flipping augmented text, and label-flipping augmented text only. We provide an example in
Fig. 2. The label-preserving augmented prompt is “The movie is great. The review is [MASK] ”, and
the label-flipping augmented prompt is “The movie sucks. The review is [MASK] ”.1 2
Algorithm 1 The proposed M IXPROalgorithm.
Input : The original dataset D= (x, y)and augmented dataset D′= (x′, y′), original template set T,
augmented template set T′, hyperparameter α, and a masked language model M.
Output : The cross-entropy loss L
foreach epoch in training epochs do
// Template-level Mixup
Sample an original and its augmented prompt tandt′fromTandT′respectively.
foreach original and augmented sample (x, y)and(x′, y′)inDandD′do
p←(x, t);p′←(x′, t′);
λ←β(α, α );
// Token-level Mixup
Ep, Ep′←the input representations of pandp′by Equ. 4-5;
Emixup←λ·Ep+ (1−λ)·Ep′;
// Sentence-level Mixup
Hp, Hp′←the hidden vectors of M(Emixup )at the [MASK] positions in pandp′;
Hmixup←λ·Hp+ (1−λ)·Hp′;
logits←MLP (Hmixup );
ymixup←λ·yp+ (1−λ)·yp′;
L ← LCE(ymixup , logits );
end
end
1We systematically compare the baselines with the proposed MIXPROfrom five perspectives in Section 4.1.2.
2Sec. 4.1.3 introduces more details of label-preserving and label-flipping prompts generation.
4

--- PAGE 5 ---
PLM
Token -level Mixup
EpEp'
The movie is great . The is .The movie [PAD] sucks . The review is .
PositiveEmixup[MASK] [MASK][MASK] [MASK] Hp'Sentence -level MixupHmixup
[MASK][MASK]LCELogitsMLP
yp'
ypymixupHp
λ·Ep+(1-λ)·Ep'λ·Hp+(1-λ)·Hp'
λ·yp+(1-λ)·yp'Label MixupForward
Backward
feedbackNegativeFigure 3: The illustration of MIXPRO. The token-level Mixup interpolates word embeddings of the original
and augmented prompts as model inputs. The sentence-level Mixup interpolates hidden vectors at two
[MASK] positions for prediction, while the original and augmented labels are mixed for loss calculation. λis a
hyperparameter setting the Mixup ratio across token-level, sentence-level, and label Mixup.
3 Approach: M IXPRO
3.1 Overview
In this section, we introduce MIXPRO, a simple yet effective DA method for prompt-based learning
to boost performance and efficiency. To begin with, we augment the original prompt pby creating
label-preserving or label-flipping prompts p′, as described in Section 2.3.3Moreover, to provide a
more comprehensive task representation and better train the model using pandp′, we propose a
three-level Mixup strategy that includes the token-level, the sentence-level, and the template-level
Mixup. We summarize the proposed M IXPROin Algorithm 1, and describe it in detail below.
3.2 Token-level Mixup
The token-level Mixup interpolates the word embeddings of the original prompt and the augmented
prompt to obtain new virtual sample representations as model inputs. Fig. 3 illustrates this process.
To start, we obtain the word embeddings for both the original prompt pand the augmented prompt p′.
The word embeddings for each token are constructed by summing its corresponding token, segment,
and position embeddings. Specifically, we can express these embeddings as:
Ep=tokp+segp+posp, (4)
Ep′=tokp′+segp′+posp′, (5)
Emixup =λ·Ep+ (1−λ)·Ep′, (6)
where tokp,segp, and pospdenote the token, segment, and position embeddings of the original
prompt p.tokp′,segp′, andposp′represent those of the augmented prompt p′. The word embeddings
of original, augmented, and interpolated prompts are Ep,Ep′, and Emixup . The Mixup ratio λis
drawn from a Beta distribution λ∼β(α, α)(Equation 2). As the hyperparameter αapproaches
zero, Emixup becomes very similar to either EporEp′. Conversely, as αapproaches +∞,Emixup
approaches the midpoint between EpandEp′. We employ the mixed Emixup to train our model.
The token-level Mixup, as illustrated in Fig. 3, is capable of handling inputs of varying lengths. We
also investigated an alternative implementation that aligns two inputs at the [MASK] position, with
performance generally comparable to the current approach. Given the existence of the sentence-level
Mixup in our method, we adopt the current token-level Mixup implementation, which uniformly
aligns inputs to the left (Cheng et al., 2020).
3We collectively refer to ppandpfin Sec. 2.3 and Fig. 2 as p′.
5

--- PAGE 6 ---
i1. Random Sample
3. Next Epoch2. Model Training
1
 2
 k…Template Set
Figure 4: In training, the template-level Mixup utilizes various templates to train a single model. At the
beginning of each epoch, a random template from the template set is chosen for the model training of that epoch.
3.3 Sentence-level Mixup
The sentence-level Mixup merges hidden vectors at [MASK] positions from the sequence representa-
tions of the model input for prediction (Fig. 3). In prompt-based learning, PLMs map these [MASK]
representations to task labels, implying these hidden states offer critical insights about the task.
To leverage this information, we interpolate the hidden vectors at the [MASK] positions and the
labels with Mixup ratio λfrom a Beta distribution λ∼β(α, α). This process can be detailed as:
Hmixup =λ·Hp+ (1−λ)·Hp′, (7)
ymixup =λ·yp+ (1−λ)·yp′. (8)
First, we encode the input representations Emixup from Section 3.2 and extract hidden vectors Hp
andHp′at the [MASK] positions. Using λ, we interpolate these to get Hmixup for prediction and
also interpolate the original and augmented labels ypandyp′for loss computation. Lastly, an MLP
layer computes the logits, determining the cross-entropy loss between ymixup and model logits.
Logits =MLP (Hmixup ), (9)
L ∼LCE(ymixup , Logits ). (10)
3.4 Template-level Mixup
During training, the template-level Mixup uses various templates for prompt creation, enhancing the
model’s learning from diverse sources. This strategy also boosts inference efficiency compared to
ensembling methods trained with separate templates (Schick & Schütze, 2021a). Fig. 4 illustrates the
template-level Mixup.
The previously mentioned token-level and sentence-level Mixup strategies use a single template during
training. Similarly, PET trains multiple models, each with its unique template. During inference,
the ensemble predictions of these models are combined to produce the final result. However, we
believe that training with one template per model might lead models to favor memorization rather
than generalization in few-shot prompt-based learning (Zhang et al., 2018b).
To overcome the constraints of using one template per model, we introduce the template-level Mixup,
utilizing various templates for prompt creation in training. For each epoch, we randomly select
a template from the template set Tand pair it with all input samples to construct corresponding
prompts as model inputs. This ensures that each input text in the dataset is exposed to all available
templates, allowing the model to holistically obtain information from the corresponding prompts.
During training, the token-level and the sentence-level Mixup techniques are applied to the prompts.
The template-level Mixup also improves the efficiency of model inference. In the backbone PET
approach, each template is used to train a separate model, and during prediction, the corresponding
ensemble prediction from multiple models is combined to produce the final result. In contrast,
MIXPROcomprehensively integrates the information from multiple templates by training a single
model on all available templates. This allows for efficient and streamlined inference since only one
model needs to be used for prediction. Specifically, assuming that a dataset has ntemplates, the
time cost of MIXPROduring inference is only 1/nof that of the backbone PET approach. This
demonstrates the potential for MIXPROto improve the efficiency of prompt-based learning without
sacrificing performance.
6

--- PAGE 7 ---
Statistics CB RTE BoolQ WSC MultiRC
Metrics Acc. / F1 Acc. Acc. Acc. EM / F1 a
# Train 32 32 32 32 32
# Dev 57 278 3 ,270 104 953
# Templates 3 4 6 3 3
Table 1: Statistics for the datasets. # Train ,# Dev , and # Templates denote sample counts in the training,
development sets, and template numbers in the dataset, respectively.
4 Experiments
We perform experiments on FewGLUE (Schick & Schütze, 2021b), a widely used few-shot bench-
mark (Zhou et al., 2021). Our results demonstrate that the proposed MIXPROis effective in improving
few-shot performance by constructing an interpolated Mixup representation between the original
prompt and the augmented prompt.
4.1 Experiment Setup
4.1.1 Tasks
FewGLUE is the few-shot version of SuperGLUE (Wang et al., 2019), including a diverse set of
natural language understanding tasks, such as question answering (BoolQ and MultiRC)(Clark et al.,
2019; Khashabi et al., 2018), word sense disambiguation (WSC)(Levesque et al., 2012), and textual
entailment (CB and RTE) (de Marneffe et al., 2019; Dagan et al., 2005; Bar-Haim et al., 2006). Each
task includes a 32-sample train set and a validation set. Successfully solving these tasks requires a
deep understanding of natural language.4For more detailed information, please refer to Table 1.
4.1.2 Baselines
We integrate a comprehensive selection of nine augmentation methods, serving as baselines for
augmenting the PET backbone. We will introduce these methods in the following, and then compare
them with the proposed M IXPROfrom five perspectives in Table 2.
•Synonym Replacement (Synonym, (Zhang et al., 2015)) uses a synonym dictionary called
WordNet (Miller, 1995) to randomly replace the original words with their synonyms.
•GloVe Replacement (GloVe, (Wang & Yang, 2015)) substitutes the original words with
nearby words from pre-trained GloVe embeddings (Pennington et al., 2014a).
•Easy Data Augmentation (EDA) performs operations like synonym replacement, random
insertion, random swap, and random deletion.5
•Back Translation (BT, (Xie et al., 2020)) translates the original text into other languages
and then back. The resulting text is combined with the original.6
•TinyBERT (Jiao et al., 2020) randomly replaces the original token in the input text with
either word predicted by a Bert-base-cased model (Devlin et al., 2018) (for singlepiece
word) or words derived by GloVe (Pennington et al., 2014b) (for multiple-piece word).
•T5-MLM (Raffel et al., 2020) randomly masks some tokens in the original input text and
fills the blanks with T5 model in a template-based data cloze.
•Mixup (Guo et al., 2019; Cheng et al., 2020) adopts the vanilla strategy in Equ. 2-3 and
interpolates one original input text with another random input text in the training set.
•FlipDA (Zhou et al., 2021) constructs both label-preserving and label-flipping augmented
data given the original input text. The implementation is based on the T5-MLM above. It
additionally applies data filtering to the augmented data.
4There are three additional datasets in FewGLUE. The experimental results of WIC (Pilehvar & Camacho-
Collados, 2019) is close to random. The task forms of ReCoRD (Zhang et al., 2018c) and COPA (Roemmele
et al., 2011) are not well suited for prompt-based augmentation. Thus, we do not conduct experiments on them.
5The implementation of synonym replacement is based on Zhang et al. (2015) above.
6We get the augmented data with 9 intermediate languages in BT-10, and with 5 in BT-6.
7

--- PAGE 8 ---
Methods Object Ext.Res Pres / Flip Mode Level
Synonym Input ✓ Pres Text Token
GloVe Input ✓ Pres Text Token
EDA Input ✓ Pres Text Token
BT-10 Input ✓ Pres Text Sent
BT-6 Input ✓ Pres Text Sent
TinyBERT Input ✓ Pres Text Token
T5-MLM Input ✓ Pres Text Token
Mixup Input - Pres & Flip Emb Sent
FlipDA Input ✓ Pres & Flip Text Token
MIXPRO Input & Tmpl ✓ Pres & Flip Text & Emb Token & Sent & Tmpl
Table 2: Comparison of MIXPROwith baseline methods. Object indicates the target of augmentation. Tmpl
represents the template. Ext.Res signifies if methods utilize external resources beyond the dataset, such as
synonyms, embeddings, or additional models. Pres / Flip highlights whether the methods use label-preserving
(Pres) or label-flipping ( Flip) augmentation. Mode indicates whether the augmentation is applied on the text
itself or its underlying embeddings ( Emb).Level shows how broadly the augmentation is applied, whether it is
on individual tokens, entire sentences ( Sent), or templates ( Tmpl ).
Hyperparameters CB RTE BoolQ WSC MultiRC
batch_size 2 2 2 4 1
grad_acc_steps 8 8 8 4 16
max_seq_length 256 256 256 128 512
max_steps 250 250 250 250 250
adam_epsilon 1e−8 1e−8 1e−8 1e−8 1e−8
learning_rate 1e−5 1e−5 1e−5 1e−5 1e−5
max_grad_norm 1.0 1 .0 1 .0 1 .0 1 .0
weight_decay 0.01 0 .01 0 .01 0 .01 0 .01
mixup_alpha 0.5 0 .5 0 .1 0 .1 0 .5
Table 3: Hyperparameters used in M IXPRO.
In this paper, we propose MIXPRO, a simple yet effective DA method for prompt-based learning.
MIXPROdraws additional virtual examples through three-level Mixup based on the label-preserving
and label-flipping augmented prompts to expand the support of the training distribution. To evaluate
the effectiveness and capabilities of MIXPRO, we undertake a comprehensive comparison against the
above baselines from five perspectives. Detailed results of this comparison can be found in Table 2.
MIXPROoptimizes the above baselines from various criteria: (1) Object: MIXPROaugments
the templates, enriching the model’s understanding of the task while simultaneously reducing its
heightened sensitivity to specific templates. (2) Ext.Res: MIXPROemploys T5 as an external
resource, which generates high-quality augmented prompts for the subsequent three-level Mixup. (3)
Pres / Flip: Label-flipping augmented data provides useful information for classification and largely
improves generalization (Zhou et al., 2021). (4) Mode: MIXPROadopt the Mixup strategy (Zhang
et al., 2018b,a) to draw additional virtual examples from the vicinity of the augmented prompt for a
comprehensive distribution (Zhang et al., 2018a). (5) Level: MIXPROadopts a three-level Mixup to
achieve multi-dimensional augmentation, aiming for enhanced performance and inference efficiency.
Overall, our proposed MIXPROis more comprehensive than the baselines and presents significant
advancements in few-shot prompt-based learning. By augmenting the entire prompt, it successfully
reduces the model’s sensitivity to template variations. By a three-level Mixup with label-flipping and
label-preserving prompts, it enhances performance and inference efficiency. Details are in Section 4.2.
4.1.3 Implementation Details
From Section 2.3, we augment prompts and include both label-preserving andlabel-flipping types.
The input text and templates are augmented separately. We generate label-preserving and label-
flipping augmented text, but label-preserving templates only. Note that we do not generate label-
flipping templates to ensure the correctness and controllability of the label corresponding to the
augmented prompt. Following Zhou et al. (2021), we use a cloze pattern to combine the input text (or
template) and label into a single sequence, and mask a fixed percentage of the tokens. A pre-trained
T5 model (Raffel et al., 2019) is used to fill in the blanks and generate an augmented sample.
8

--- PAGE 9 ---
CB RTE BoolQ WSC MultiRC Avg.
Method Acc. F1 Acc. Acc. Acc. EM F1 a -
PET 82.74 74.84 61.40 72.47 77.03 33.04 74.64 68.02
+ Synonym 83.33 78.12 59.24 74.98 78.74 34.09 75.55 69.15
+ GloVe 82.14 74.39 61.91 74.51 75.00 32.72 75.20 67.98
+ EDA 81.10 73.58 58.33 72.86 75.85 28.74 73.05 66.22
+ BT-10 82.44 77.72 55.93 74.59 - 32.06 74.69 66.24
+ BT-6 82.89 76.55 57.46 75.36 - 34.85 75.82 67.16
+ TinyBERT 85.42 82.35 58.66 72.60 78.95 30.47 73.20 68.81
+ T5-MLM 83.48 75.01 62.27 73.86 79.17 33.79 74.06 68.81
+ Mixup 83.93 79.28 62.06 75.03 68.70 34.06 74.66 68.25
+ FlipDA 86.31 82.45 70.67 76.98 78.74 36.38 76.23 72.54
+MIXPRO 86.94 82.67 71.16 78.96 79.13 36.41 76.44 73.10
Table 4: Performance of baseline methods and MIXPRObased on the backbone PET. Underline denotes values
that outperform PET. Bold denotes the best-performed ones of the task. “Avg.” is the average of scores.
For model training, we adopt the experimental settings used in PET (Schick & Schütze, 2021b) and
conduct a grid search (see Table 3). We set the batch size to 2 for CB, RTE, and BoolQ, to 4 for WSC,
and to 1 for MultiRC. We configure the gradient accumulation steps to 8 for CB, RTE, and BoolQ, to
4 for WSC, and to 16 for MultiRC. We set the max sequence length for tokenized inputs to 256 for
CB, RTE, and BoolQ, to 128 for WSC, and to 512 for MultiRC. Across all tasks, we consistently set
the max steps to 250, adam epsilon to 1×10−8, and learning rate to 1×10−5. To prevent gradients
from becoming too large, we set the max grad norm to 1.0 for all tasks, and for regularization, we set
the weight decay to 0.01across all tasks. Lastly, pivotal for the three-level Mixup, we set the mixup
alpha ( λin Equ. 6-8) to 0.5for CB, RTE, and MultiRC, and to 0.1for BoolQ and WSC.
To evaluate the effectiveness of DA methods, we augment the backbone PET (Schick & Schütze,
2021b) with M IXPROand other DA baselines. We utilize Albert-xxlarge-v2 (Lan et al., 2020) as the
PLM and measure the performance using the identical metrics in Table 1, namely Acc., F1, F1 a, and
EM. Since few-shot learning typically exhibits significant performance fluctuations (Dodge et al.,
2020; Schick & Schütze, 2021b), we use five independent seeds and report their average performance.
All of our experiments were performed on a Linux platform equipped with NVIDIA A100 (40G).
4.2 Main Results
In this paper, we conduct experiments on five different datasets, namely CB, RTE, BoolQ, WSC,
and MultiRC. We compare M IXPROwith the backbone PET and nine augmentation baselines from
Sec. 4.1.2, analyzing the results in three dimensions. Besides performance improvements, it’s
essential to consider the computational cost associated with these enhancements. Thus, we further
analyze the computational implications of these improvements.
Table 4 presents a detailed overview of our experimental findings.7(1)Overall performance: MIXPRO
achieved the highest average performance of 73.10. It not only outshines the backbone PET by
5.08%, but also surpasses the best baseline FlipDA by 0.56%;(2)Performance improvement beyond
the backbone: when compared against the backbone PET, MIXPROconsistently outperforms it across
all datasets, whereas other baseline methods, except for FlipDA, were only effective on a few tasks
(denoted with underlines); (3)Performance comparison with baselines across datasets: In terms of
inter-dataset performance, we compare the performance of all models in each dataset. MIXPROis the
best performing model on all datasets (except for WSC, where MIXPROis the second-best method),
and consistently outperforms the strongest baseline, FlipDA, on all datasets.
Based on the experimental results presented above, we argue that there are several challenges in few-
shot prompt-based learning augmentation. Specifically, models are highly sensitive to the choice of
templates, where small changes can lead to notable declines in effectiveness (Gao et al., 2021b; van de
Kar et al., 2022; Cao et al., 2022). Additionally, existing methods often employ model ensembles
across different templates and limit the model’s inference efficiency. (Schick & Schütze, 2021b).
Consequently, some traditional augmentation methods still face challenges in model performance
and inference efficiency within few-shot prompt-based learning.
7The results of baselines are obtained from Zhou et al. (2021) using the same experimental settings as ours.
9

--- PAGE 10 ---
CB RTE BoolQ WSC MultiRC Avg.
Method Acc. F1 Acc. Acc. Acc. EM F1 a -
MIXPRO 86.94 82.67 71.16 78.96 79.13 36.41 76.44 73.10
w/o Token 86.07 82.57 60.14 78.61 78.46 35.76 76.39 71.14
w/o Sent 85.35 80.59 67.22 78.84 77.88 35.73 76.04 71.66
w/o Tmpl 85.00 80.49 69.31 78.52 78.26 35.39 74.76 71.68
Table 5: Experiment results of removing the token ( Token ), sentence ( Sent), or template-level ( Tmpl ) Mixup
strategies from MIXPRO.Underline denotes values that outperform PET. Bold denotes the best-performed ones
of the task. “Avg.” is the average of scores. All results are the average over 5 seeds.
In comparison to existing methods, our proposed MIXPROleverages three-level Mixup strategies
to sample virtual examples from different perspectives. By combining information from various
templates, we further enhance the comprehensiveness of the model’s understanding. This intricate
design ensures model effectiveness and robustness across different tasks. The excellent results of
MIXPROin our experiments demonstrate its ability to address the aforementioned challenges stably.
Meanwhile, our DA method brings a certain increase in computational cost, which we consider
is acceptable. For the average training cost, there is an increase of 20.9% in training time during
each iteration (43s →52s) and an increase of 15.24% in the number of model parameters (223M →
257M) when keeping the batch size constant. For the average inference cost, MIXPROimproves the
efficiency of model inference. MIXPROapplies multiple templates to train a single model rather than
multiple ensemble models as PET does. Specifically, assuming that a dataset has ntemplates, the
time cost of M IXPROduring inference is only 1/nof that of the backbone PET approach.
In summary, the proposed MIXPROachieves excellent results across five datasets via three-level
Mixup. As an augmentation method, it experiences a certain increase in computational cost and the
number of parameters compared to the backbone. However, given the overall 5.08% improvement in
performance, the decrease in inference time, and robustness across different tasks, we consider these
computational cost increases to be within an acceptable range.8
4.3 Analysis
In this section, we address the following research questions for a deeper understanding of MIXPRO:
(1)Do the token-level, the sentence-level, or the template-level Mixup strategies make equal
contributions to the performance of M IXPRO?
(2)Is it essential to augment both input text and templates concurrently?
(3)Does MIXPROdemonstrate significant performance fluctuations across multiple seeds
compared to the backbone PET?
4.3.1 Three-level Mixup: Separate Contributions
To delve deeper into the impact of removing the token-level, the sentence-level, or the template-
level Mixup strategies on the overall performance of our model, we systematically conduct ablation
experiments and analyze the results.
The experimental results in Table 5 demonstrate that: (1)MIXPROoutperforms all other methods
on all datasets, indicating that the Mixup strategies at the token, sentence, and template levels all
contribute to performance improvement; (2)The impact of w/o Token is minimal overall. While it
had a significant negative impact on the RTE dataset (over 10% lower than MIXPRO), leading to
a corresponding decrease in average performance, it has minimal impact on most datasets. This
indicates that the token-level Mixup, which samples new distributions at the word embedding level,
is relatively shallow and, while it can improve model performance, is not as effective as the other
two Mixup strategies; (3)The impact of w/o Sent andw/o Tmpl are similar, with the former having
a slightly larger impact on average ( 0.02% lower). These findings suggest that both strategies are
effective: the sentence-level Mixup provides a deeper understanding of samples than the token-level
Mixup, while the template-level Mixup constructs a comprehensive task distribution by utilizing
multiple templates during training.
8A more detailed exploration of the robustness of M IXPROcan be found in Sec. 4.3.3.
10

--- PAGE 11 ---
CB RTE BoolQ WSC MultiRC Avg.
Method Acc. F1 Acc. Acc. Acc. EM F1 a -
MIXPRO 86.94 82.67 71.16 78.96 79.13 36.41 76.44 73.10
w/o Template Aug 84.28 80.59 65.84 78.50 77.69 34.41 75.71 71.00
w/o Text Aug 83.71 76.70 65.56 78.54 78.08 34.17 75.18 70.28
Table 6: Experiment results of removing the augmentation ( Aug) of vanilla text input or templates from MIXPRO
across five datasets. Underline denotes values that outperform PET. Bold denotes the best-performed ones of the
task. “Avg.” is the average of scores. All results are the average over 5 seeds.
00.010.020.030.040.050.06
CB-Acc. CB-F1 RTE BoolQ WSC Multi-EM Multi-F1aStandard Deviation
MetricsPET FlipDA MixPro
Figure 5: Standard deviation of performance of PET, FlipDA and MIXPROon five datasets. “MULTI-EM” and
“MULTI-F1A” denote the standard deviations of the EM score and F1 ascore of the models on MultiRc.
4.3.2 Necessity of Augmenting Text and Templates
To understand the influence of removing the augmentation for either vanilla input text or the associated
templates, we conduct a series of ablation experiments and analyze the results.
The results from the experiments, which are detailed in Table 6, shed light on several key findings: (1)
Our proposed method, MIXPRO, consistently outperforms the other two methods across all datasets.
This distinction underscores the significance of augmenting both the vanilla input text and templates.
(2)When examining the effects of w/o Template Aug andw/o Text Aug , the findings suggest a similar
impact across the five datasets. Moreover, a closer look shows that w/o Text Aug has a slightly bigger
impact, decreasing performance by an average of 0.72%. This is likely because input texts are usually
longer than templates. Therefore, augmenting the text provides richer information and achieves better
performance than augmenting the templates.
4.3.3 Measuring the fluctuation of model performance
We determine the standard deviation of the results for MIXPRO, FlipDA, and PET using five seeds
spread across five distinct datasets.
The experimental results, as illustrated in Fig. 5, bring noteworthy observations: Both augmentation
methods, FlipDA and MIXPRO, demonstrate a standard deviation across the five datasets that is
markedly lower than the backbone PET. Moreover, FlipDA exhibits a consistently smaller standard
deviation than M IXPRO, with the sole exception being the WSC dataset.
These outcomes underscore the effectiveness of data augmentation (DA) techniques in enhancing a
model’s robustness and generalizability. Specifically, MIXPRO, empowered by its three-level Mixup
strategy, establishes a more diversified data distribution, leading to superior performance metrics.
By augmenting templates and training the model concurrently on them (see Sec. 3 and Sec. 4.1.2),
MIXPROminimizes its sensitivity to particular templates. As a result, it achieves a more stable
performance, surpassing both the backbone and the strongest baseline across a range of datasets.
11

--- PAGE 12 ---
5 Related Works
5.1 Prompt-based Learning
Prompt-based learning converts tasks into cloze questions with templates (Liu et al., 2021a). It
includes two types: hard and soft prompts, which we will detail separately.
Inhard prompts , templates are human-readable text strings, as used in this paper. Two types exist:
(1)Manually created prompts based on human introspection, like those in Petroni et al. (2019) and
Brown et al. (2020b) to probe LMs, or predefined ones in few-shot learning (Schick & Schütze,
2020; Schick & Schütze, 2021c,d). (2)Automatically searched templates from natural language
phrases, including methods like prompt mining (Jiang et al., 2020), prompt paraphrasing (Yuan et al.,
2021; Haviv et al., 2021), gradient-based search (Wallace et al., 2019; Shin et al., 2020), and prompt
scoring (Davison et al., 2019).
Soft prompts use the embedding space of language models, not limited to human-interpretable
language. The advent of continuous prompts has removed the restriction that templates must be
parameterized solely by pre-trained language model parameters. Instead, templates can have their
own parameters that are adjustable based on training data from the downstream task. Common
methods include prefix tuning (Li & Liang, 2021; Lester et al., 2021; Tsimpoukelli et al., 2021),
tuning initialized with discrete prompts (Zhong et al., 2021b; Qin & Eisner, 2021; Hambardzumyan
et al., 2021), and hard-soft prompt hybrid tuning (Liu et al., 2021b; Han et al., 2021).
This paper focuses on hard prompts due to their simplicity (Brown et al., 2020b), user-
friendliness (Schick & Schütze, 2020), and competitive performance (Wen et al., 2023). Building
on Schick & Schütze (2021b), MIXPROintegrates automatic augmentation to enhance performance,
robustness, and inference efficiency. It is lightweight and requires minimal manual input.9
5.2 Data Augmentation
Data augmentation aims at producing synthetic training data in scenarios with limited data. As prompt-
based learning evolves, there are some studies investigating their integration with data augmentation.
In this section, we offer a concise overview of these studies.
Data augmentation in prompt-based learning falls into two categories: (1)Augmenting prompts :
FlipDA (Zhou et al., 2021) generates data using word substitution based on a pre-trained language
model and uses a classifier to select label-flipped data. PromptDA (Chen & Shu, 2022) derives
multiple label words for each class to enrich the label semantic space, while RAPT (Chowdhury et al.,
2022) augments soft prompts. (2)Using prompts for augmentation : PromDA (Wang et al., 2022)
uses soft prompts to generate augmented data for low-resource NLU tasks. WeakDap (Chen et al.,
2022a) explores few-shot data augmentation for dialogue understanding by prompting pre-trained
language models, and AUG-FedPrompt (Cai et al., 2022) annotates unlabeled data via a prompt-based
federated learning algorithm.
Our paper focuses on the first type, augmenting prompts , and our MIXPROdistinguishes itself from
prior work in the following ways: (1)It augments the complete prompt to enhance performance,
and(2)it specifically augments hard prompts. We detail these distinctions below. (1)Most existing
methods augment segments of a prompt rather than its entirety. For instance, FlipDA (Zhou et al.,
2021) augments only the vanilla input text, bypassing the templates. Thus, it does not truly augment in
the context of prompt-based learning. However, templates play a vital role in prompt-based learning,
and their augmentation proves crucial as shown in Tab.6. Similarly, PromptDA(Chen & Shu, 2022)
focuses only on label augmentation, limiting its diversity. (2)Some research centers on augmenting
soft prompts, diverging from our hard prompt augmentation (Chowdhury et al., 2022). Conversely,
MIXPROaugments both the vanilla input text and hard templates using the three-level Mixup. It
facilitates the sampling of new virtual distributions during training, leading to excellent performance.
9Manual annotation (Petroni et al., 2019; Brown et al., 2020b) is expert-intensive and time-costly, while
automatic generation needs ample data, unsuited for few-shot tasks (Zhou et al., 2021). We will apply data
augmentation methods to them for further improvement in the future.
12

--- PAGE 13 ---
6 Conclusion and Future Work
In this paper, we present a data augmentation method called MIXPROthat is specifically designed for
prompt-based learning. Our method employs a three-level Mixup strategy to generate augmented
prompts and comprehensively constructs virtual examples from the vicinity distribution of the original
prompts. Our experiments on five different few-shot learning datasets demonstrate that MIXPRO
leads to a substantial improvement in the backbone PET by an average of 5.08%. Furthermore, our
method achieves better results with improved efficiency during inference compared to the backbone.
In the future, we plan to further explore two directions. First, we will improve the model architecture
by simultaneously feeding the original and augmented prompts into the model and performing Mixup
at the self-attention layer in the PLMs to achieve a more natural and deeper information interaction.
Second, we will conduct more experiments to apply data augmentation to both manually annotated
and automatically generated prompts, with the goal of enhancing the performance of both.
Limitations
While deploying our method to various downstream tasks, certain limitations emerge concerning
the hyperparameter tuning process. The selection of αplays a pivotal role and visibly influences
model performance.10On the brighter side, we find the process of fine-tuning αto be relatively
straightforward and feasible in terms of computational resources. In alignment with prior
research (Guo et al., 2019; Cheng et al., 2020), we select αfrom a pre-defined, restricted set ( 1e−2,
1e−1,5e−1, and 1). Empirical evidence from our experiments across five different datasets distinctly
shows a preference for 1e−1and5e−1, underlining a manageable scope of tuning that brings
tangible benefits to model performance. Furthermore, the entire experimental setup boasts efficiency,
seamlessly running on a single V100 GPU (32G), thereby ensuring that the hardware demands remain
minimal and accessible. In summation, while the noted limitation exists, it is well within the bounds
of manageability, and addressing it opens avenues for enhancing model performance.
Concurrently, our approach inherits certain constraints from FlipDA (Zhou et al., 2021). As depicted in
Sec. 4.2, while MIXPROdemonstrates remarkable improvements over existing baseline augmentation
methods in most scenarios, its performance on the WSC task slightly trails behind some baselines.
This is attributed to the complexity of the WSC task, which necessitates the disambiguation of
multi-token word senses, a challenge for T5 when it comes to generating label-flipping augmented
prompts. The T5 model struggles with fabricating similar entities absent from the original sentence,
hindering its ability to generate the required candidate examples. Addressing this issue, particularly
devising a more adept pattern-based cloze algorithm for such tasks, remains an area for future
exploration (Rosset et al., 2020).
10λdenotes the Mixup ratio, sampled from a Beta distribution λ∼β(α, α ), where αacts as a crucial
hyperparameter governing the extent of Mixup.
13

--- PAGE 14 ---
References
Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlomov, S., Tepper, N., and
Zwerdling, N. Do not have enough data? deep learning to the rescue! In Proceedings of AAAI , pp.
7383–7390, 2020.
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A. T., Shakeri, S., Taropa, E.,
Bailey, P., Chen, Z., Chu, E., Clark, J., Shafey, L. E., Huang, Y ., Meier-Hellstern, K. S., Mishra,
G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y ., Xiao, K., Xu, Y ., Zhang, Y .,
Abrego, G. H., Ahn, J., Austin, J., Barham, P., Botha, J. A., Bradbury, J., Brahma, S., Brooks,
K. M., Catasta, M., Cheng, Y ., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crépy, C.,
Dave, S., Dehghani, M., Dev, S., Devlin, J., D’iaz, M. C., Du, N., Dyer, E., Feinberg, V ., Feng, F.,
Fienber, V ., Freitag, M., García, X., Gehrmann, S., González, L., Gur-Ari, G., Hand, S., Hashemi,
H., Hou, L., Howland, J., Hu, A. R., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M.,
Jia, W. H., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M.-L., Li,
W., Li, Y ., Li, J. Y ., Lim, H., Lin, H., Liu, Z.-Z., Liu, F., Maggioni, M., Mahendru, A., Maynez,
J., Misra, V ., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M.,
Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P., Ros, A., Roy, A., Saeta,
B., Samuel, R., Shelby, R. M., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter,
D., Vasudevan, V ., V odrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y .,
Xu, K., Xu, Y ., Xue, L. W., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D.,
Petrov, S., and Wu, Y . Palm 2 technical report. ArXiv , abs/2305.10403, 2023.
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., and Magnini, B. The second pascal
recognising textual entailment challenge. 2006.
Ben-David, E., Oved, N., and Reichart, R. PADA: Example-based Prompt Learning for on-the-fly
Adaptation to Unseen Domains. Transactions of the Association for Computational Linguistics ,
10:414–433, 04 2022. ISSN 2307-387X.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R.,
Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,
2020a.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R.,
Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
Language models are few-shot learners. In Proceedings of NeurIPS , 2020b.
Cai, D., Wu, Y ., Yuan, H., Wang, S., Lin, F. X., and Xu, M. Aug-fedprompt: Practical few-shot
federated nlp with data-augmented prompts. ArXiv , abs/2212.00192, 2022.
Cao, B., Lin, H., Han, X., Liu, F., and Sun, L. Can prompt probe pretrained language models?
understanding the invisible risks from a causal view. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5796–5808, Dublin,
Ireland, May 2022. Association for Computational Linguistics.
Chapelle, O., Weston, J., Bottou, L., and Vapnik, V . Vicinal risk minimization. Advances in neural
information processing systems , 13, 2000.
Chen, C. and Shu, K. Promptda: Label-guided data augmentation for prompt-based few shot learners.
ArXiv , abs/2205.09229, 2022.
Chen, J., Tam, D., Raffel, C., Bansal, M., and Yang, D. An empirical survey of data augmentation for
limited data learning in nlp. ArXiv , abs/2106.07499, 2021.
14

--- PAGE 15 ---
Chen, M., Papangelis, A., Tao, C., Rosenbaum, A., Kim, S., Liu, Y ., Yu, Z., and Hakkani-Tür,
D. Z. Weakly supervised data augmentation through prompting for dialogue understanding. ArXiv ,
abs/2210.14169, 2022a.
Chen, X., Li, L., Zhang, N., Liang, X., Deng, S., Tan, C., Huang, F., Si, L., and Chen, H. Decoupling
knowledge from memorization: Retrieval-augmented prompt learning. ArXiv , abs/2205.14704,
2022b.
Cheng, Y ., Jiang, L., Macherey, W., and Eisenstein, J. AdvAug: Robust adversarial augmentation for
neural machine translation. In Proceedings of ACL , pp. 5961–5970, 2020.
Chowdhury, J. R., Zhuang, Y ., and Wang, S. Novelty controlled paraphrase generation with retrieval
augmented conditional prompt tuning. In AAAI Conference on Artificial Intelligence , 2022.
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring
the surprising difficulty of natural yes/no questions. In Proceedings of ACL , pp. 2924–2936, 2019.
Cohan, A., Ammar, W., van Zuylen, M., and Cady, F. Structural scaffolds for citation intent
classification in scientific publications. ArXiv , abs/1904.01608, 2019.
Dagan, I., Glickman, O., and Magnini, B. The PASCAL recognising textual entailment challenge. In
Machine Learning Challenges , volume 3944, pp. 177–190, 2005.
Davison, J., Feldman, J., and Rush, A. Commonsense knowledge mining from pretrained models. In
Proceedings of EMNLP , pp. 1173–1178, 2019.
de Marneffe, M.-C., Simons, M., and Tonhauser, J. The commitmentbank: Investigating projection in
naturally occurring discourse. 2019.
Dernoncourt, F. and Lee, J. Y . Pubmed 200k rct: a dataset for sequential sentence classification in
medical abstracts. In International Joint Conference on Natural Language Processing , 2017.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional
transformers for language understanding. In North American Chapter of the Association for
Computational Linguistics , 2019a.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019b. Association for
Computational Linguistics.
Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., and Smith, N. A. Fine-tuning
pretrained language models: Weight initializations, data orders, and early stopping. CoRR ,
abs/2002.06305, 2020.
Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In
Proceedings of ACL , pp. 3816–3830, 2021a.
Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In
Proceedings of ACL , pp. 3816–3830, 2021b.
Guo, H., Mao, Y ., and Zhang, R. Augmenting data with mixup for sentence classification: An
empirical study. ArXiv preprint , abs/1905.08941, 2019.
Hambardzumyan, K., Khachatrian, H., and May, J. WARP: Word-level Adversarial ReProgramming.
InProceedings of ACL , pp. 4921–4933, 2021.
Han, X., Zhao, W., Ding, N., Liu, Z., and Sun, M. Ptr: Prompt tuning with rules for text classification,
2021.
15

--- PAGE 16 ---
Haviv, A., Berant, J., and Globerson, A. BERTese: Learning to speak to BERT. In Proceedings of
ACL, pp. 3618–3623, 2021.
Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A
simple data processing method to improve robustness and uncertainty. In Proceedings of ICLR ,
2020.
Hu, Z., Tan, B., Salakhutdinov, R., Mitchell, T. M., and Xing, E. P. Learning data manipulation for
augmentation and weighting. ArXiv , abs/1910.12795, 2019.
Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know?
Transactions of the Association for Computational Linguistics , 8:423–438, 2020.
Jiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. TinyBERT: Distilling
BERT for natural language understanding. In Proceedings of ACL , pp. 4163–4174, 2020.
Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., and Roth, D. Looking beyond the surface:
A challenge set for reading comprehension over multiple sentences. In Proceedings of ACL , pp.
252–262, 2018.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A lite bert for
self-supervised learning of language representations. ArXiv , abs/1909.11942, 2019.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. ALBERT: A lite BERT for
self-supervised learning of language representations. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.
Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning,
2021.
Levesque, H. J., Davis, E., and Morgenstern, L. The winograd schema challenge. In Principles of
Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference ,
2012.
Li, B., Hou, Y ., and Che, W. Data augmentation approaches in natural language processing: A survey.
AI Open , 2022.
Li, X., Xue, J.-T., Xie, Z., and Li, M. Think outside the code: Brainstorming boosts large language
models in code generation. ArXiv , abs/2305.10679, 2023.
Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings
of ACL , pp. 4582–4597, 2021.
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict:
A systematic survey of prompting methods in natural language processing. ArXiv preprint ,
abs/2107.13586, 2021a.
Liu, X., Zheng, Y ., Du, Z., Ding, M., Qian, Y ., Yang, Z., and Tang, J. GPT understands, too. ArXiv
preprint , abs/2103.10385, 2021b.
Miller, G. A. Wordnet: a lexical database for english. Communications of the ACM , 38(11):39–41,
1995.
Nie, F., Chen, M., Zhang, Z., and Cheng, X. Improving few-shot performance of language models
via nearest neighbor calibration. ArXiv , abs/2212.02216, 2022.
Pennington, J., Socher, R., and Manning, C. GloVe: Global vectors for word representation. In
Proceedings of EMNLP , pp. 1532–1543, 2014a.
Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation.
InProceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP) , pp. 1532–1543, 2014b.
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y ., and Miller, A. Language
models as knowledge bases? In Proceedings of EMNLP , pp. 2463–2473, 2019.
16

--- PAGE 17 ---
Pilehvar, M. T. and Camacho-Collados, J. WiC: the word-in-context dataset for evaluating context-
sensitive meaning representations. In Proceedings of ACL , pp. 1267–1273, 2019.
Qi, K., Wan, H., Du, J., and Chen, H. Enhancing cross-lingual natural language inference by
prompt-learning from cross-lingual templates. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1910–1923, Dublin,
Ireland, May 2022. Association for Computational Linguistics.
Qin, G. and Eisner, J. Learning how to ask: Querying LMs with mixtures of soft prompts. In
Proceedings of ACL , pp. 5203–5212, 2021.
Raffel, C., Shazeer, N. M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and
Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv ,
abs/1910.10683, 2019.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.
Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.
Res., 21:140:1–140:67, 2020.
Ren, S., Zhang, J., Li, L., Sun, X., and Zhou, J. Text AutoAugment: Learning compositional
augmentation policy for text classification. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pp. 9029–9043, Online and Punta Cana, Dominican
Republic, November 2021. Association for Computational Linguistics.
Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice of plausible alternatives: An evaluation of
commonsense causal reasoning. In Logical Formalizations of Commonsense Reasoning , 2011.
Rosset, C., Xiong, C., Phan, M. H., Song, X., Bennett, P., and Tiwary, S. Knowledge-aware language
model pretraining. ArXiv , abs/2007.00655, 2020.
Schick, T. and Schütze, H. Exploiting cloze-questions for few-shot text classification and natural
language inference. In Proceedings of ACL , pp. 255–269, 2021a.
Schick, T. and Schütze, H. It’s not just size that matters: Small language models are also few-shot
learners. In Proceedings of ACL , pp. 2339–2352, 2021b.
Schick, T. and Schütze, H. Exploiting cloze-questions for few-shot text classification and natural
language inference. In Proceedings of ACL , pp. 255–269, 2021c.
Schick, T. and Schütze, H. It’s not just size that matters: Small language models are also few-shot
learners. In Proceedings of ACL , pp. 2339–2352, 2021d.
Schick, T. and Schütze, H. Few-shot text generation with pattern-exploiting training, 2020.
Seoh, R., Birle, I., Tak, M., Chang, H.-S., Pinette, B., and Hough, A. Open aspect target sentiment
classification with natural language prompts. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pp. 6311–6322, Online and Punta Cana, Dominican
Republic, November 2021. Association for Computational Linguistics.
Shin, T., Razeghi, Y ., Logan IV , R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge
from Language Models with Automatically Generated Prompts. In Proceedings of EMNLP , pp.
4222–4235, 2020.
Simard, P. Y ., LeCun, Y ., Denker, J. S., and Victorri, B. Transformation invariance in pattern
recognition: Tangent distance and propagation. International Journal of Imaging Systems and
Technology , 11, 2000.
Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S. E., Anguelov, D., Erhan, D., Vanhoucke, V ., and
Rabinovich, A. Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , pp. 1–9, 2014.
17

--- PAGE 18 ---
Touvron, H., Martin, L., Stone, K. R., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,
Bhargava, P., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu,
D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn, A. S.,
Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V ., Khabsa, M., Kloumann, I. M., Korenev,
A. V ., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y ., Mao, Y ., Martinet, X.,
Mihaylov, T., Mishra, P., Molybog, I., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K.,
Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A.,
Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur, M., Narang, S., Rodriguez,
A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models.
ArXiv , abs/2307.09288, 2023.
Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F. Multimodal
few-shot learning with frozen language models. ArXiv preprint , abs/2106.13884, 2021.
van de Kar, M., Xia, M., Chen, D., and Artetxe, M. Don’t prompt, search! mining-based zero-
shot learning with language models. In Conference on Empirical Methods in Natural Language
Processing , 2022.
Wallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S. Universal adversarial triggers for
attacking and analyzing NLP. In Proceedings of EMNLP , pp. 2153–2162, 2019.
Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman,
S. R. Superglue: A stickier benchmark for general-purpose language understanding systems. In
Proceedings of NeurIPS , pp. 3261–3275, 2019.
Wang, B., Zhao, Y ., Lu, X., and Qin, B. Cognitive distortion based explainable depression detection
and analysis technologies for the adolescent internet users on social media. Frontiers in Public
Health , 10, 2023.
Wang, W. Y . and Yang, D. That’s so annoying!!!: A lexical and frame-semantic embedding based
data augmentation approach to automatic categorization of annoying behaviors using #petpeeve
tweets. In Proceedings of EMNLP , pp. 2557–2563, 2015.
Wang, Y ., Xu, C., Sun, Q., Hu, H., Tao, C., Geng, X., and Jiang, D. Promda: Prompt-based data
augmentation for low-resource nlu tasks. In Annual Meeting of the Association for Computational
Linguistics , 2022.
Wei, J. and Zou, K. EDA: Easy data augmentation techniques for boosting performance on text
classification tasks. In Proceedings of EMNLP , pp. 6382–6388, 2019.
Wen, Y ., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., and Goldstein, T. Hard prompts
made easy: Gradient-based discrete optimization for prompt tuning and discovery. ArXiv ,
abs/2302.03668, 2023.
Xie, Q., Dai, Z., Hovy, E. H., Luong, T., and Le, Q. Unsupervised data augmentation for consistency
training. In Proceedings of NeurIPS , 2020.
Yang, A. M., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D.,
Yang, F., Deng, F., Wang, F., Liu, F., Ai, G., Dong, G., Zhao, H., Xu, H., Sun, H., Zhang, H., Liu,
H., Ji, J., Xie, J., Dai, J., Fang, K., Su, L., Song, L., Liu, L., Ru, L., Ma, L., Wang, M., Liu, M.,
Lin, M., Nie, N., Guo, P., Sun, R., Zhang, T., Li, T., Li, T., Cheng, W., Chen, W., Zeng, X., Wang,
X., Chen, X., Men, X., Yu, X., Pan, X., Shen, Y .-B., Wang, Y ., Li, Y ., Jiang, Y ., Gao, Y ., Zhang, Y .,
Zhou, Z., and Wu, Z. Baichuan 2: Open large-scale language models. 2023.
Yuan, S., Chen, J., Fu, Z., Ge, X., Shah, S., Jankowski, C. R., Yang, D., and Xiao, Y . Distilling script
knowledge from large language models for constrained language planning. In Annual Meeting of
the Association for Computational Linguistics , 2023.
Yuan, W., Neubig, G., and Liu, P. Bartscore: Evaluating generated text as text generation, 2021.
Zhang, H., Cissé, M., Dauphin, Y . N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization.
In6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018a.
18

--- PAGE 19 ---
Zhang, H., Cissé, M., Dauphin, Y . N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization.
InProceedings of ICLR , 2018b.
Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Durme, B. V . Record: Bridging the gap between
human and machine commonsense reading comprehension. ArXiv , abs/1810.12885, 2018c.
Zhang, X., Zhao, J. J., and LeCun, Y . Character-level convolutional networks for text classification.
InProceedings of NeurIPS , pp. 649–657, 2015.
Zhong, R., Lee, K., Zhang, Z., and Klein, D. Adapting language models for zero-shot learning by
meta-tuning on dataset and prompt collections. In Findings of the Association for Computational
Linguistics: EMNLP 2021 , pp. 2856–2878, Punta Cana, Dominican Republic, November 2021a.
Association for Computational Linguistics.
Zhong, Z., Friedman, D., and Chen, D. Factual probing is [MASK]: Learning vs. learning to recall.
InProceedings of ACL , pp. 5017–5033, 2021b.
Zhou, J., Zheng, Y ., Tang, J., Li, J., and Yang, Z. Flipda: Effective and robust data augmentation for
few-shot learning, 2021.
19
