# Tinh chỉnh Soft Prompt để Tăng cường Truy xuất Dày đặc với Mô hình Ngôn ngữ Lớn

## TÓM TẮT

Truy xuất dày đặc (DR) chuyển đổi truy vấn và tài liệu thành các embedding dày đặc và đo lường độ tương tự giữa truy vấn và tài liệu trong không gian vector. Một trong những thách thức chính trong DR là thiếu dữ liệu huấn luyện đặc thù cho từng lĩnh vực. Mặc dù các mô hình DR có thể học từ các tập dữ liệu công khai quy mô lớn như MS MARCO thông qua học chuyển giao, bằng chứng cho thấy không phải tất cả các mô hình DR và lĩnh vực đều có thể hưởng lợi từ học chuyển giao. Gần đây, các nhà nghiên cứu đã sử dụng các mô hình ngôn ngữ lớn (LLM) để cải thiện các mô hình DR zero-shot và few-shot. Tuy nhiên, các hard prompt hoặc prompt do con người viết được sử dụng trong các nghiên cứu này không tối ưu và các truy vấn yếu được tạo ra thường nhạy cảm với các prompt. Để giải quyết vấn đề này, chúng tôi đề xuất tinh chỉnh soft prompt để tăng cường DR (SPTAR): đối với mỗi tác vụ, chúng tôi tận dụng tinh chỉnh soft prompt để tối ưu hóa một soft prompt đặc thù cho tác vụ trên dữ liệu ground truth hạn chế và sau đó nhắc nhở LLM gắn thẻ các tài liệu chưa được gắn nhãn với các truy vấn yếu, tạo ra các cặp tài liệu-truy vấn yếu để huấn luyện các bộ truy xuất dày đặc đặc thù cho tác vụ. Chúng tôi thiết kế một bộ lọc để chọn các cặp tài liệu-truy vấn ví dụ chất lượng cao trong prompt để cải thiện thêm chất lượng của các truy vấn được gắn thẻ yếu. Theo hiểu biết của chúng tôi, không có nghiên cứu trước đây nào sử dụng tinh chỉnh soft prompt để tăng cường các mô hình DR. Hơn nữa, khác với phần lớn các nghiên cứu hiện tại, nghiên cứu của chúng tôi dựa trên các LLM mã nguồn mở phổ biến để đảm bảo kết quả có thể tái tạo và xác định. Kết quả thực nghiệm của chúng tôi cho thấy SPTAR vượt trội hơn cả các baseline không giám sát và phương pháp tăng cường dựa trên LLM được đề xuất gần đây cho DR.

## 1 GIỚI THIỆU

Truy xuất thông tin (IR) đóng vai trò then chốt trong một loạt các ứng dụng, từ các công cụ tìm kiếm web nổi tiếng như Google và Bing đến các hệ thống đề xuất cá nhân hóa như đề xuất sản phẩm của Walmart và đề xuất bài hát của Apple Music. Các phương pháp IR truyền thống, như TF-IDF và BM25, được xây dựng dựa trên việc khớp độ tương tự ở cấp độ token, điều này đôi khi có thể thiếu sót do khoảng cách từ vựng. Khoảng cách này xảy ra khi các thuật ngữ tương tự về mặt ngữ nghĩa, chẳng hạn như từ đồng nghĩa, bị bỏ qua vì sự khác biệt về từ vựng. Sự thiếu sót này có thể ảnh hưởng đến chất lượng kết quả tìm kiếm và trải nghiệm người dùng.

Với những hạn chế này, các nhà nghiên cứu đã chuyển sang những tiến bộ trong deep learning để giải quyết khoảng cách từ vựng trong IR truyền thống. Một cách tiếp cận đáng chú ý là Truy xuất Dày đặc (DR), nhằm nắm bắt bản chất ngữ nghĩa tổng thể của nội dung thay vì tập trung vào từng token riêng lẻ. Các mô hình DR như dense passage retrieval (DPR) và ColBERT mã hóa mỗi truy vấn hoặc tài liệu thành một vector dày đặc, với số chiều được xác định bởi các mạng neural. Trong thực tế, các bộ truy xuất dày đặc tính toán trước các embedding tài liệu và xây dựng một chỉ mục nearest neighbor xấp xỉ (ANN) để tìm kiếm nhanh. Khi có một truy vấn mới, chỉ embedding của nó được tính toán và sau đó được xử lý bởi hệ thống tìm kiếm ANN. Khác với TF-IDF và BM25, DR đặt trọng tâm lớn hơn vào việc đánh giá độ tương tự của bối cảnh ngữ nghĩa tổng thể.

Mặc dù các phương pháp DR đã có những tiến bộ trong việc thu hẹp khoảng cách từ vựng, chúng vẫn bị hạn chế bởi tính khả dụng hạn chế của dữ liệu huấn luyện đặc thù cho lĩnh vực, cản trở hiệu suất của chúng trong các lĩnh vực chuyên biệt. Mặc dù một số nhà nghiên cứu đã đề xuất tận dụng học chuyển giao để giảm thiểu thách thức này, các nghiên cứu cho thấy không phải tất cả các mô hình DR và lĩnh vực đều có thể hưởng lợi từ học chuyển giao một cách bình đẳng. Gần đây, các LLM như GPT-3, LLaMA và Vicuna đã thể hiện khả năng học zero-shot và few-shot mạnh mẽ. Thay vì tinh chỉnh LLM trên dữ liệu đặc thù cho tác vụ, prompting tích hợp các hướng dẫn tác vụ (ví dụ: TL;DR dịch sang tiếng Anh) và một vài ví dụ liên quan làm đầu vào và trích xuất câu trả lời từ đầu ra của mô hình ngôn ngữ lớn (LLM). Thuật ngữ "hard prompt" và "soft prompt" đề cập đến các cách tiếp cận khác nhau để hướng dẫn hành vi của LLM trong quá trình tạo văn bản hoặc các tác vụ khác. Hard prompt liên quan đến việc sử dụng các đầu vào văn bản được xác định rõ ràng và không thể thay đổi để hướng dẫn mô hình. Prompt không liên quan đến việc huấn luyện hoặc tinh chỉnh bổ sung của mô hình. Mặt khác, soft prompt liên quan đến việc sử dụng các vector có thể huấn luyện hoặc embedding có thể học để hướng dẫn hành vi của mô hình. Khác với hard prompt, soft prompt không phải là hướng dẫn văn bản rõ ràng mà là các embedding ảnh hưởng đến đầu ra của mô hình. Các embedding này thường được học thông qua một quá trình được gọi là tinh chỉnh prompt. Các nghiên cứu hiện tại đã gợi ý rằng prompt cung cấp một phương pháp để tiêm hướng dẫn đặc thư cho tác vụ, điều này có lợi trong các chế độ ít dữ liệu. Nghiên cứu gần đây đã định lượng thêm lợi ích này thông qua việc kiểm tra toàn diện các prompt. Kết quả cho thấy các prompt được thiết kế tốt có thể giảm đáng kể sự phụ thuộc vào khối lượng lớn dữ liệu huấn luyện trên các tác vụ downstream. Cả InPars và PROMPTAGATOR đều sử dụng hard prompt để hướng dẫn LLM gắn thẻ các tài liệu chưa được gắn nhãn với các truy vấn yếu, sau đó huấn luyện các bộ truy xuất đặc thù cho tác vụ. Tuy nhiên, hard prompt có những hạn chế: a) Việc tạo ra các hard prompt hiệu quả là thách thức và thường đòi hỏi nỗ lực lặp đi lặp lại của con người, trực giác và đôi khi một chút may mắn; b) Ngay cả với các prompt được tạo thủ công, các tác vụ downstream vẫn hoạt động kém hơn các mô hình được tinh chỉnh. Ví dụ, so với hiệu suất của T5-XXL được tinh chỉnh trên SuperGLUE, GPT-3 175B few-shot có điểm số thấp hơn 17.5 điểm mặc dù sử dụng gấp 16 lần tham số. Những hạn chế này của hard prompt nhấn mạnh tính hiệu quả của chúng và việc giải quyết những thách thức này thu hút sự quan tâm của giới học thuật cũng như tạo ra giá trị công nghiệp.

Với những hạn chế của hard prompt, chúng tôi nghiên cứu một phương án thay thế. Thay vì sử dụng các từ có thể đọc được bởi con người như hard prompt, soft prompt bao gồm một tập hợp các embedding không thể nhận ra được bởi con người và được đặt trước ở đầu đầu vào của mạng neural. Trong quá trình tinh chỉnh soft prompt, các tham số của LLM được đóng băng và chỉ các tham số liên quan đến soft prompt được cập nhật. Mặc dù cả hai nghiên cứu đều chứng minh rằng soft prompt vượt trội hơn hard prompt, không có nghiên cứu nào sử dụng tinh chỉnh soft prompt để tăng cường DR. Trong bài báo này, chúng tôi đề xuất tinh chỉnh soft prompt để tăng cường DR (SPTAR). Cụ thể, đối với mỗi tác vụ, chúng tôi tận dụng tinh chỉnh soft prompt để tối ưu hóa các tham số liên quan đến soft prompt trên dữ liệu ground truth hạn chế và sau đó nhắc nhở LLM gắn thẻ các tài liệu chưa được gắn nhãn với các truy vấn yếu, tạo ra đủ các cặp tài liệu-truy vấn yếu để huấn luyện các bộ truy xuất đặc thù cho tác vụ. Hơn nữa, chúng tôi thấy rằng ngay cả với soft prompt được tối ưu hóa, chất lượng của các truy vấn yếu được tạo ra đôi khi nhạy cảm với các cặp tài liệu-truy vấn ví dụ trong prompt. Do đó, chúng tôi đã thiết kế một bộ lọc để chọn các cặp tài liệu-truy vấn ví dụ chất lượng cao trong prompt để cải thiện thêm chất lượng của các truy vấn được gắn thẻ yếu cũng như các tác vụ DR. Ngoài ra, phần lớn các nghiên cứu hiện tại đã được xây dựng trên các mô hình độc quyền ẩn sau các endpoint API mờ mịt, có thể tạo ra kết quả thực nghiệm không thể tái tạo hoặc không xác định. Thay vào đó, nghiên cứu của chúng tôi dựa trên các LLM mã nguồn mở được sử dụng rộng rãi. Các đóng góp chính của chúng tôi có thể được tóm tắt như sau:

• Theo hiểu biết của chúng tôi, nghiên cứu của chúng tôi là một trong những nỗ lực đầu tiên của LLM kết hợp với tinh chỉnh soft prompt để nâng cao các tác vụ DR.

• Chúng tôi giới thiệu một bộ lọc soft prompt được thiết kế để tuyển chọn các cặp tài liệu-truy vấn trong prompt, do đó nâng cao chất lượng tổng thể của dữ liệu yếu được tạo ra. Ngoài ra, chúng tôi thiết kế một bộ lọc BM25 để giảm nhiễu trong dữ liệu được tạo ra, cải thiện thêm hiệu suất.

• Chúng tôi tiến hành một tập hợp thực nghiệm toàn diện bao gồm bốn tập dữ liệu và bảy bộ truy xuất và re-ranker, chứng minh tính tổng quát và hiệu suất vượt trội của cách tiếp cận của chúng tôi so với một số baseline hiện đại.

• Các thực nghiệm dựa trên các LLM mã nguồn mở gần đây để đảm bảo kết quả thực nghiệm có thể tái tạo và xác định. Tất cả mã và dữ liệu đều có sẵn công khai.

## 2 NGHIÊN CỨU LIÊN QUAN

### 2.1 Truy xuất Dày đặc

DR chuyển đổi các truy vấn và tài liệu thành các vector dày đặc mà trên đó có thể xây dựng chỉ mục ANN để tìm kiếm nhanh. DPR sử dụng cấu trúc hai tháp: một mô hình BERT cho truy vấn và một mô hình khác cho tài liệu. Đối với mỗi truy vấn với một tài liệu dương và một số tài liệu âm, DPR đo lường độ tương tự giữa embedding truy vấn và embedding tài liệu rồi tối đa hóa log-likelihood của đoạn văn dương. Một biến thể của DPR là sử dụng một BERT bằng cách nối truy vấn và tài liệu làm đầu vào và trích xuất embedding truy vấn và embedding tài liệu sau khi mã hóa. Bộ mã hóa truy vấn và bộ mã hóa tài liệu của ColBERT chia sẻ cùng một BERT nhưng sử dụng một token đặc biệt khác nhau theo sau "[CLS]" để phân biệt truy vấn và tài liệu. Khác với DPR đo lường trực tiếp độ tương tự giữa embedding truy vấn và embedding tài liệu, ColBERT giới thiệu một cơ chế tương tác muộn. Cụ thể, đối với mỗi token trong truy vấn, ColBERT tính toán độ tương tự của nó với tất cả các token trong tài liệu và áp dụng pooling tối đa trên các điểm số tương tự này. Điểm số tương tự của một cặp truy vấn và tài liệu là tổng hợp của tất cả các điểm số sau pooling tối đa. Với một truy vấn có một tài liệu dương và một tài liệu âm, ColBERT được tối ưu hóa bằng mất mát cross-entropy softmax theo cặp trên các điểm số được tính toán của tài liệu dương và âm. ANCE là một bi-encoder được huấn luyện trên các tuple (truy vấn, tài liệu dương, tài liệu âm) trong đó tài liệu âm được truy xuất từ một ANN được xây dựng trên checkpoint của bước cuối cùng. TAS-B nhóm các truy vấn theo độ tương tự embedding của chúng và sử dụng kỹ thuật lấy mẫu dữ liệu huấn luyện kết hợp với chưng cất giám sát dual-teacher. Contriever huấn luyện một mô hình bi-encoder thông qua học tương phản. Thay vì huấn luyện mô hình trên tập dữ liệu được gắn nhãn, Contriever tạo ra các cặp truy vấn-tài liệu dương từ corpus chưa được gắn nhãn bằng hai chiến lược "inverse cloze tasks" và "independent cropping". ReContriever áp dụng cùng phương pháp như Contriever để tạo ra các cặp truy vấn-tài liệu yếu, nhưng ReContriever chấm điểm các cặp truy vấn-tài liệu yếu bằng chính nó trong quá trình huấn luyện và mất mát được gia trọng bởi các trọng số. BM25CE là một DR dựa trên re-ranking. BM25CE đầu tiên áp dụng BM25 để truy xuất tài liệu và sau đó sử dụng cross-encoder được huấn luyện để re-rank các tài liệu được truy xuất. Đóng góp của chúng tôi không phải là đề xuất các bộ truy xuất dày đặc mới mà là đề xuất một phương pháp mới để tăng cường các bộ truy xuất dày đặc hiện có.

### 2.2 Tăng cường Dữ liệu cho Truy xuất Dày đặc

Đối với các tập dữ liệu DR, thường chỉ một phần nhỏ tài liệu được gắn nhãn với truy vấn, ví dụ MS MARCO, một tập dữ liệu được sử dụng rộng rãi trong DR, có corpus gồm 8841823 tài liệu nhưng chỉ có 532761 cặp tài liệu-truy vấn huấn luyện. Vì DR đòi hỏi dữ liệu huấn luyện đáng kể để đạt được các embedding dày đặc chất lượng, một số nhà nghiên cứu đã chuyển sang tăng cường dữ liệu để tạo ra nhiều cặp tài liệu-truy vấn hơn để huấn luyện các embedding dày đặc tốt hơn. InPars đưa một prompt do con người viết đặc thù cho tác vụ và 3 cặp tài liệu-truy vấn ví dụ vào mô hình GPT-3 6B Curie để tạo ra 100K cặp tài liệu-truy vấn yếu và chọn top 10K truy vấn theo xác suất của truy vấn q để tăng cường dữ liệu huấn luyện. InPars sử dụng cùng mô hình truy xuất dày đặc được đề xuất trong [32], coi truy xuất như một tác vụ sequence-to-sequence bằng cách nối một truy vấn và một tài liệu làm đầu vào cho mô hình T5 và xuất điểm số liên quan. Các biến thể cải tiến của InPars, như InPars-v2 và InPars-Light, đã được giới thiệu để nâng cao phương pháp ban đầu. Giống như InPars, PROMPTAGATOR cũng đưa một prompt do con người viết đặc thù cho tác vụ và tối đa 8 cặp tài liệu-truy vấn ví dụ vào LLM để tạo ra dữ liệu yếu. Thay vì chọn các truy vấn yếu hàng đầu theo xác suất của chúng, PROMPTAGATOR đầu tiên huấn luyện một bộ lọc trên các cặp tài liệu-truy vấn chưa được làm sạch để lọc các truy vấn yếu bằng cách loại bỏ các truy vấn yếu không thể truy xuất các tài liệu được ghép đôi của chúng trong các tài liệu được truy xuất Top-k. Bằng cách lặp lại quá trình này nhiều lần, bộ lọc cải thiện đáng kể hiệu suất của bộ truy xuất dual-encoder DPR. Bên cạnh đó, PROMPTAGATOR sử dụng một LLM lớn hơn nhiều: mô hình 175B Flan mà hầu hết các nhà nghiên cứu không thể truy cập. DAR lập luận rằng phương pháp tạo ra truy vấn từ các tài liệu chưa được gắn nhãn tốn kém cũng như không thêm biến thể vào tài liệu. Để tăng cường dữ liệu một cách hiệu quả, DAR không chỉ nội suy hai biểu diễn tài liệu khác nhau liên quan đến truy vấn được gắn nhãn mà còn làm nhiễu ngẫu nhiên các biểu diễn của tài liệu được gắn nhãn trong không gian embedding. RocketQA áp dụng một bộ truy xuất cross-encoder được huấn luyện trước để truy xuất các tài liệu dương và âm cho một bộ sưu tập truy vấn mới với điểm tin cậy cao. RocketQAv2 tăng cường DR bằng cách tối ưu hóa chung mô hình DR cấu trúc bi-encoder và mô hình reranking cấu trúc cross-encoder để có phân phối đầu ra tương tự. DRAGON hợp nhất nhiều mô hình teacher bằng cách huấn luyện tiến bộ mô hình DR cơ sở.

### 2.3 LLM trong Truy xuất Dày đặc

Hầu hết tài liệu hiện tại trong lĩnh vực này khám phá tiềm năng của LLM để cải thiện các tác vụ DR thông qua các kỹ thuật tạo dữ liệu khác nhau, bao gồm tạo truy vấn, tạo độ liên quan và tạo hoán vị. PROMPTAGATOR và InPars với các biến thể InPars-v2 và InPars-Light đã được minh họa trong phần 2.2. UPR sử dụng LLM như một zero-shot reranker để re-rank các đoạn văn được truy xuất bởi các bộ truy xuất như BM25 và DPR. Với một truy vấn, đối với mỗi đoạn văn được truy xuất, UPR sử dụng một prompt "Vui lòng viết một câu hỏi dựa trên đoạn văn này" để nhắc nhở LLM và tính toán log-likelihood trung bình của các token câu hỏi có điều kiện trên tài liệu đầu vào làm điểm số liên quan. Do tài nguyên tính toán chuyên sâu cần thiết để huấn luyện LLM, tất cả các nghiên cứu này sử dụng LLM như các bộ tạo truy vấn thay vì tinh chỉnh chúng. HyDE tận dụng LLM để tăng cường truy vấn bằng cách tạo ra các tài liệu giả thuyết, nắm bắt hiệu quả các mẫu liên quan cho truy xuất không giám sát. LRL huấn luyện một zero-shot re-ranker theo danh sách tận dụng LLM mà không cần huấn luyện có giám sát đặc thù cho tác vụ. Khác với các pointwise re-ranker, LRL xem xét tất cả các tài liệu ứng viên để xác định vị trí xếp hạng tương đối của chúng. Một cách tiếp cận khác liên quan đến tạo hoán vị hướng dẫn, trong đó trọng tâm là hướng dẫn LLM trực tiếp xuất các hoán vị của đoạn văn. Các kỹ thuật chưng cất hoán vị được sử dụng để chuyển khả năng xếp hạng đoạn văn của ChatGPT vào một mô hình xếp hạng chuyên biệt nhỏ hơn. Trong khi các nghiên cứu này sử dụng LLM như các bộ tạo truy vấn mà không tinh chỉnh, cách tiếp cận SPTAR của chúng tôi có một cách tiếp cận khác. Chúng tôi đầu tiên thực hiện tinh chỉnh soft prompt để tối ưu hóa các soft prompt đặc thù cho tác vụ và sau đó sử dụng lọc dữ liệu để nâng cao chất lượng của dữ liệu yếu được tạo ra.

### 2.4 Tinh chỉnh Prompt

Tinh chỉnh prompt cung cấp một hướng đầy hứa hẹn để thích ứng các LLM được huấn luyện trước với các tác vụ cụ thể bằng cách tập trung vào việc tinh chỉnh mô-đun prompt thay vì tinh chỉnh toàn bộ mô hình. Prefix-Tuning giới thiệu một mô-đun prompt với các tham số có thể học θ xuất ra các embedding được đặt trước vào các embedding của các token đầu vào khác. Cách tiếp cận này giữ nguyên mục tiêu huấn luyện ban đầu trong khi chỉ cập nhật các tham số prefix θ thông qua gradient descent cho mỗi tác vụ. Một kỹ thuật tương tự khác, được gọi là "gisting", nén các prompt tùy ý thành một tập hợp các token "gist" ảo được nén bằng cách sử dụng một cách tiếp cận meta-learning. Dựa trên T5, Lester et al. đề xuất một phương pháp trong đó các embedding có thể học của một prompt đặc thù cho tác vụ được đặt trước vào đầu ra của encoder. Các embedding được nối sau đó được truyền qua decoder để tính toán mục tiêu huấn luyện. Cách tiếp cận này cho phép mô hình kết hợp thông tin đặc thù cho tác vụ vào quá trình giải mã. Zhou et al. giới thiệu Dual Context-guided Continuous Prompt (DCCP), sử dụng tinh chỉnh soft prompt với đầu vào kép: biểu diễn prompt nhận thức bối cảnh và biểu diễn bối cảnh nhận thức nhãn. Cách tiếp cận này tận dụng cả thông tin prompt và hiểu biết bối cảnh để nâng cao hiệu suất của mô hình. Tinh chỉnh prompt có thể hưởng lợi từ học đa tác vụ. Ví dụ, ATTEMPT được đề xuất bởi Wang et al. giới thiệu một phương pháp tinh chỉnh đa tác vụ chuyển giao kiến thức qua các tác vụ khác nhau thông qua một hỗn hợp các soft prompt. Trong bối cảnh Truy xuất Thông tin Đa ngôn ngữ, Huang et al. khám phá một cách tiếp cận giải mã soft prompt coi truy xuất trong mỗi ngôn ngữ như một tác vụ riêng biệt trong khi mô hình hóa chung chúng để nắm bắt các cấu trúc cơ bản được chia sẻ. Họ sử dụng các prompt có thể phân tách trong KD-SPD để mô hình hóa ngôn ngữ, nhấn mạnh rằng các ngôn ngữ chia sẻ các đặc điểm và khái niệm chung mặc dù có những thuộc tính độc đáo. Liên quan đến các tác vụ IR, DPTDR của Tang et al. sử dụng dual-encoder, hai mô hình RoBERTa, cho truy xuất. Nó khởi tạo dual-encoder thông qua học tương phản và thêm các soft prompt có thể học cho truy vấn và tài liệu. Cả dual-encoder và các prompt có thể học đều được cập nhật trong quá trình huấn luyện.

Ngược lại, khác với các nghiên cứu tăng cường dữ liệu DR hiện tại chỉ nhắc nhở LLM tạo ra các truy vấn yếu cho các tài liệu chưa được gắn nhãn với một vài cặp tài liệu-truy vấn được gắn nhãn làm ví dụ, chúng tôi đề xuất học các soft prompt đặc thù cho tác vụ trên một tỷ lệ nhỏ dữ liệu được gắn nhãn và một phương pháp bộ lọc soft prompt mới để chọn các cặp tài liệu-truy vấn ví dụ chất lượng cao trong prompt để cải thiện thêm các tác vụ DR. Toàn bộ pipeline tăng cường làm cho cách tiếp cận của chúng tôi khác biệt với các nghiên cứu hiện tại.

## 3 TINH CHỈNH SOFT PROMPT ĐỂ TĂNG CƯỜNG TRUY XUẤT DÀY ĐẶC

Như thể hiện trong Hình 1, SPTAR bao gồm sáu mô-đun: a) chuẩn bị dữ liệu; b) tinh chỉnh soft prompt; c) bộ lọc soft prompt; d) bộ tăng cường soft prompt; e) bộ lọc dữ liệu yếu; f) DR. Trong Phần 3.1, chúng tôi trình bày chi tiết cách tạo ra các tập dữ liệu huấn luyện và đánh giá của tinh chỉnh soft prompt. Với các tập dữ liệu huấn luyện và đánh giá, chúng tôi tiến hành tinh chỉnh soft prompt (Phần 3.2) để học một soft prompt đặc thù cho tác vụ. Để cải thiện thêm chất lượng của các truy vấn yếu được tạo ra, chúng tôi giới thiệu bộ lọc soft prompt (Phần 3.3) xác định các cặp tài liệu-truy vấn ví dụ tối ưu để tối ưu hóa prompt đặc thù cho tác vụ. Sau đó chúng tôi nhắc nhở LLM tạo ra các truy vấn yếu cho các tài liệu chưa được gắn nhãn (Phần 3.5), tạo ra đủ dữ liệu huấn luyện để huấn luyện DR. Cuối cùng, chúng tôi huấn luyện các mô hình DR (Phần 3.6) trên dữ liệu yếu đã được lọc (Phần 3.4). Các ký hiệu được sử dụng trong bài báo này được cung cấp trong Bảng 1.

### 3.1 Chuẩn bị Dữ liệu

Chúng tôi nghiên cứu việc tăng cường DR sử dụng dữ liệu hạn chế. Bước đầu tiên liên quan đến việc lấy mẫu một tập dữ liệu nhỏ mà trên đó chúng tôi tinh chỉnh một soft prompt đặc thù cho tác vụ. Chúng tôi định nghĩa tập dữ liệu D là D = {(qn, dn)}Nn=1 trong đó đối với mỗi truy vấn qn, có một tài liệu liên quan dn. Có thể tồn tại các truy vấn trùng lặp vì một truy vấn có thể có nhiều tài liệu liên quan. Tập dữ liệu đặc thù cho lĩnh vực D này được phân loại thành các tập con train, test và evaluation, được ký hiệu là Dtrain, Dtest và Deval tương ứng. Ngoài tập dữ liệu D, có một bộ sưu tập tài liệu C lớn hơn nhiều chứa tất cả các tài liệu trong D nhưng có thêm các tài liệu chưa được gắn nhãn được ký hiệu là Cunlabeled. Sau khi huấn luyện, DR mã hóa tất cả các tài liệu trong C thành vector. Khi một truy vấn mới đến, DR mã hóa truy vấn thành một vector và tìm kiếm top-k tài liệu tương tự trong không gian vector.

Chúng tôi lấy mẫu ngẫu nhiên các cặp tài liệu-truy vấn từ tập dữ liệu huấn luyện ban đầu Dtrain để xây dựng các tập dữ liệu huấn luyện và đánh giá cho mô-đun soft prompt, cụ thể là SXtrain và SYeval trong đó các chỉ số X và Y biểu thị số lượng truy vấn riêng biệt trong các tập dữ liệu huấn luyện và đánh giá tương ứng. Hàm NumPair(x) chỉ định số lượng cặp tài liệu-truy vấn cho x truy vấn riêng biệt trong tập dữ liệu. Vì mỗi truy vấn có thể có nhiều hơn một tài liệu dương, NumPair(x) có thể lớn hơn |x|. Do đó, SXtrain chứa NumPair(X) cặp tài liệu-truy vấn, tương tự, SYeval bao gồm NumPair(Y) cặp tài liệu-truy vấn. Để minh họa, trong thực nghiệm của chúng tôi, chúng tôi rút 50 truy vấn duy nhất và các tài liệu tương ứng của chúng từ tập dữ liệu huấn luyện Strain để tạo thành S50train (X = 50). Từ dữ liệu còn lại trong Strain, chúng tôi chọn ngẫu nhiên 100 truy vấn duy nhất và các tài liệu liên quan của chúng để tạo thành S100eval (Y = 100). S50train phục vụ để tối ưu hóa soft prompt, trong khi S100eval được sử dụng để đánh giá sự hội tụ của mô hình, cho phép chúng tôi kết thúc quá trình huấn luyện sớm và giảm thiểu rủi ro overfitting. Chúng tôi cũng đã thử các giá trị khác của X và ảnh hưởng của X được nghiên cứu trong Phần 5.2.5.

### 3.2 Tinh chỉnh Soft Prompt

Soft prompt giới thiệu một kỹ thuật mới để định hướng hành vi của mô hình mà không cần tinh chỉnh rộng rãi. Khác với hard prompt, là các hướng dẫn có thể đọc được bởi con người, soft prompt bao gồm các embedding được huấn luyện được tối ưu hóa cho các tác vụ cụ thể. Mô-đun tinh chỉnh soft prompt học một soft prompt đặc thù cho tác vụ trên một tỷ lệ nhỏ dữ liệu được gắn nhãn. Hình 2 (b) minh họa cấu trúc của mô-đun tinh chỉnh soft prompt, trong đó các hộp màu đỏ biểu thị các tham số θ được tối ưu hóa trong quá trình huấn luyện mô hình và các hộp màu xanh lá cây biểu thị các tham số ban đầu Φ của LLM được giữ lại trong quá trình huấn luyện. s biểu thị prompt cứng được khởi tạo với kích thước ls, như lặp lại "vui lòng tạo truy vấn cho tài liệu" cho đến khi độ dài của s bằng ls. Gọi fθ(·) là lớp embedding của prompt được thực hiện bởi một ma trận embedding được khởi tạo như các embedding của s được mã hóa bởi lớp embedding ban đầu của LLM. fθ(s) biểu thị soft prompt.

Đối với mỗi epoch huấn luyện, chúng tôi đầu tiên lấy mẫu ngẫu nhiên M cặp tài liệu-truy vấn từ tập dữ liệu huấn luyện SXtrain làm các cặp tài liệu-truy vấn ví dụ (dm, qm)Mm=1, sau đó lặp qua các cặp tài liệu-truy vấn còn lại (dj, qj)NumPair(X)−Mj=1 để tính toán mất mát. Các cặp ví dụ (dm, qm)Mm=1 được nối với mỗi cặp (dj, qj) bằng các từ khóa như "tài liệu" và "truy vấn" như cj. Cuối cùng, chúng tôi nối s với cj như một thực thể huấn luyện tj = [s; cj] và có NumPair(X) − M thực thể trong mỗi epoch. Khi tj được đưa vào mô-đun tinh chỉnh soft prompt, nó đầu tiên được token hóa thành một danh sách các ID zj được chỉ số bởi i sau đó các embedding của ID được trích xuất và đưa vào các lớp tiếp theo để tính toán các vector ẩn. fθ(·) lấy các ID của s làm đầu vào và xuất ra các embedding của nó trong khi các embedding của cj được tạo ra bởi lớp embedding ban đầu của LLM. Để đơn giản, chúng tôi giả định rằng mỗi token trong tj có một ID tương ứng trong zj. Đối với thực thể huấn luyện tj, vector ẩn của bước thời gian thứ i được định nghĩa là hj,i ∈ Rd trong đó hj,i = [h(1)j,i; ···; h(k)j,i] và k là số lượng lớp trong LLM. Hàm mục tiêu để huấn luyện được cho bởi:

maxθ log pθ,Φ(qj|tj) = maxθ ∑i∈idxqj log pθ,Φ(zj,i|hj,<i)    (1)

trong đó idxqj biểu thị các chỉ số tương ứng với các ID của dj. Ngoài ra, pθ,Φ(zj,i|hj,<i) biểu thị xác suất của token tiếp theo với ID zj,i. Đối với hàm mất mát L, chúng tôi sử dụng negative log-likelihood được định nghĩa là:

L = −log pθ,Φ(qj|tj)    (2)

Chúng tôi đã triển khai mô-đun tinh chỉnh soft prompt của chúng tôi dựa trên gói tinh chỉnh prompt công khai PEFT.

### 3.3 Bộ lọc Soft Prompt

Trong quá trình phát triển mô-đun soft prompt, chúng tôi quan sát thấy rằng việc lựa chọn các cặp tài liệu-truy vấn ví dụ (dm, qm)Mm=1 ảnh hưởng sâu sắc đến chất lượng tạo văn bản. Do đó, sau khi hoàn thành việc huấn luyện soft prompt, với các tham số đã học θ*, chúng tôi cố gắng chọn nhóm cặp tài liệu-truy vấn tốt nhất từ SXtrain làm các cặp tài liệu-truy vấn ví dụ trong bộ tăng cường soft prompt. Đối với M = 2, có 1225 (50*49/2) nhóm cặp ví dụ, điều này làm cho việc đánh giá tất cả trở nên không khả thi. Để giảm độ phức tạp tính toán, chúng tôi lấy mẫu ngẫu nhiên X nhóm cặp ví dụ từ SXtrain để đánh giá chúng trên tập dữ liệu đánh giá SYeval và nhóm cặp ví dụ có chỉ số đánh giá tốt nhất sẽ được chọn làm các cặp ví dụ trong bộ tăng cường soft prompt. Như thể hiện trong Hình 2 (c), sự khác biệt duy nhất giữa tinh chỉnh soft prompt và bộ lọc soft prompt là tập dữ liệu mà dj đến từ đó. Giả sử chúng tôi lấy mẫu X nhóm cặp tài liệu-truy vấn mỗi nhóm có M cặp tài liệu-truy vấn (dm, qm)Mm=1. Tập dữ liệu đánh giá SYeval có Num(Y) cặp tài liệu-truy vấn và các cặp ví dụ (dm, qm)Mm=1 được nối với mỗi cặp (dj, qj) bằng các từ khóa như "tài liệu" và "truy vấn" như cj. Sau đó, cj được nối với prompt được khởi tạo s như tj = [s, cj]. Chỉ số đánh giá giống như hàm mất mát L (Phương trình 2). Chúng tôi nghiên cứu hiệu quả của bộ lọc soft prompt trong Phần 5.2.3 và các cặp tài liệu-truy vấn ví dụ được lọc được ghi lại trong Phụ lục A và B.

### 3.4 Bộ lọc Dữ liệu Yếu

Cả InPars và PROPAGATE đều nhấn mạnh tầm quan trọng của việc lọc các cặp tài liệu-truy vấn yếu vì các truy vấn yếu được tạo ra không được đảm bảo luôn liên quan đến các tài liệu đầu vào. Sử dụng phương pháp từ InPars, chúng tôi làm sạch dữ liệu yếu. Sau khi có được các cặp yếu được tạo ra này (Phần 3.5), chúng tôi áp dụng lọc dựa trên BM25: Đối với mỗi truy vấn yếu, BM25 truy xuất top k tài liệu từ corpus C. Nếu tài liệu liên kết với một truy vấn yếu không nằm trong top k kết quả, cặp đó sẽ bị loại bỏ. Cách tiếp cận lọc này được ký hiệu là Fk(·). Đối với các tập dữ liệu MS MARCO và FiQA-2018, chúng tôi đã thử nghiệm với các giá trị top k khác nhau từ tập hợp {10, 30, 50, 70} và báo cáo kết quả tốt nhất. Hiệu quả của mô-đun bộ lọc dữ liệu yếu này được thảo luận trong Phần 5.2.4.

### 3.5 Bộ tăng cường Soft Prompt

Tạo ra các truy vấn chất lượng cao cho các tài liệu chưa được gắn nhãn vẫn là một thách thức đáng gờm. Mô-đun bộ tăng cường soft prompt của chúng tôi khai thác cả sức mạnh của các soft prompt đã học và bối cảnh được cung cấp bởi các cặp tài liệu-truy vấn ví dụ tốt nhất, cung cấp hiệu ứng hiệp đồng đảm bảo không chỉ sự liên quan mà còn chất lượng vượt trội của các truy vấn được tạo ra. Như thể hiện trong Hình 2 (d), với các tham số đã học θ* và nhóm cặp tài liệu-truy vấn ví dụ được lọc, bộ tăng cường soft prompt tạo ra một truy vấn yếu cho một tài liệu chưa được gắn nhãn dj được lấy mẫu từ Dunlabeled. Trong bài báo này, đối với mỗi tập dữ liệu, chúng tôi đầu tiên tạo ra hai tập dữ liệu yếu: a) Wlarge. 100K tài liệu chưa được gắn nhãn được lấy mẫu từ Dunlabeled để tạo ra các truy vấn yếu của chúng. Nếu số lượng tài liệu chưa được gắn nhãn trong Dunlabeled nhỏ hơn 100K, tất cả các tài liệu chưa được gắn nhãn được sử dụng để tạo ra các truy vấn yếu; b) Wsmall. 5000 cặp tài liệu-truy vấn được lấy mẫu từ Wlarge. Sau đó, chúng tôi lọc Wlarge và Wsmall bằng bộ lọc dữ liệu yếu, được mô tả trong Phần 3.4, để có được Fk(Wlarge) và Fk(Wsmall). Trong quá trình tạo truy vấn yếu, LLM không chỉ sử dụng các embedding soft prompt để nắm bắt thông tin đặc thù cho lĩnh vực mà còn hưởng lợi từ bối cảnh bổ sung được cung cấp bởi các cặp tài liệu-truy vấn ví dụ tốt nhất.

### 3.6 Truy xuất Dày đặc

DR phục vụ như bước kết thúc trong phương pháp của chúng tôi, nơi chúng tôi khai thác khả năng của mạng neural để truy xuất các tài liệu liên quan. Chúng tôi đã tiến hành các thực nghiệm trên năm bộ truy xuất dày đặc phổ biến: DPR, ColBERT, TAS-B, Contriever và ReContriever. Mô tả của các mô hình có thể được tìm thấy trong Phần 2.1. Đối với TAS-B, chúng tôi đã sử dụng mô hình bi-encoder được huấn luyện trước để phân cụm truy vấn và mô hình cross-encoder và ColBERTv2 cho các mô hình teacher. Đối với Contriever, chúng tôi tham khảo checkpoint được phát hành chính thức như Contriever_base và sử dụng nó để đánh giá ban đầu. Ngoài ra, chúng tôi tinh chỉnh Contriever_base như một mô hình truy xuất dày đặc bi-encoder, được ký hiệu là Contriever_be. Tương tự, đối với ReContriever, chúng tôi có ReContriever_base và ReContriever_be. Hơn nữa, chúng tôi đã tích hợp mô hình cross-encoder BM25CE, như đã được thiết lập trong tài liệu trước đây, re-rank top 1000 mục được truy xuất bởi BM25. Giống như BM25CE, chúng tôi đã sử dụng mô hình DPR như một bi-encoder để re-rank top 1000 mục được truy xuất bởi BM25, gọi phương pháp này là BM25BE.

## 4 THIẾT LẬP THỰC NGHIỆM

### 4.1 Tập dữ liệu

Các thực nghiệm được thực hiện trên bốn tập dữ liệu MS MARCO và FiQA-2018, được lấy từ BEIR và DL2019, DL2020. Mô tả của bốn tập dữ liệu có thể được tìm thấy trong Bảng 2. Chúng tôi tuân theo BEIR để báo cáo các chỉ số trên tập dữ liệu đánh giá thay vì dữ liệu kiểm tra cho MS MARCO, vì vậy, đối với MS MARCO, Dtest giống như Deval.

Như thể hiện trong Bảng 3: a) BM25, Contriever_base và ReContriever_base được đánh giá trên phần kiểm tra ban đầu Dtest; b) Các mô hình W/O Aug được huấn luyện trên các tập dữ liệu S50train và S100eval được sử dụng để tinh chỉnh soft prompt; c) Các mô hình InPars được huấn luyện trên S50train và S100eval cộng với Fk(Wlarge) (Wlarge được lọc bởi Fk, Phần 3.4) được tạo ra bởi các prompt do con người viết. d) Mô-đun tinh chỉnh soft prompt của SPTAR (SPTAR-Tuning) được huấn luyện trên S50train và đánh giá trên S100eval; Các mô hình DR của SPTAR (SPTAR-DR) được huấn luyện trên S50train và S100eval cộng với Fk(Wlarge) (Wlarge được lọc bởi Fk, Phần 3.4) được tạo ra bởi bộ tăng cường soft prompt (Phần 3.5); e) W/O Aug, InPars và SPTAR đều được đánh giá và kiểm tra trên cùng các phần để so sánh công bằng; f) Đối với Fk(·), chúng tôi đã thử k ∈ (10, 30, 50, 70) và chọn k với điểm số NDCG@10 tốt nhất trên tập dữ liệu đánh giá.

### 4.2 Chi tiết Huấn luyện

Để huấn luyện mô-đun soft prompt, chúng tôi đã thực hiện tinh chỉnh sử dụng hai LLM mã nguồn mở: LLaMA-7B và Vicuna-7B. Các siêu tham số huấn luyện cụ thể được ghi lại trong Bảng 4.

Các siêu tham số huấn luyện của các bộ truy xuất dày đặc được trong Bảng 5. Đối với ColBERT, không có early stop trong mã chính thức và chúng tôi đã lưu một checkpoint sau mỗi epoch. Sau khi huấn luyện, chúng tôi đánh giá thủ công một số checkpoint (3, 5, 10, 15, 18, 20) và báo cáo kết quả kiểm tra của checkpoint có điểm số NDCG@10 cao nhất.

### 4.3 Chỉ số Đánh giá

Trong bối cảnh các mô hình tạo văn bản, Perplexity là một chỉ số thường được sử dụng để định lượng mức độ không chắc chắn mà một mô hình ngôn ngữ thể hiện khi tạo ra các token mới. Chỉ số này được định nghĩa là average negative log-likelihood mũ của một chuỗi, và giá trị perplexity thấp hơn cho thấy mô hình ngôn ngữ chất lượng cao hơn. Perplexity được sử dụng để đánh giá các mô-đun tinh chỉnh soft prompt và bộ lọc soft prompt.

Trong đánh giá các mô hình DR của chúng tôi, chúng tôi tuân thủ các chỉ số được thiết lập trong các nghiên cứu trước đây. Đối với các tập dữ liệu MS MARCO và FiQA-2018, chúng tôi sử dụng Mean Reciprocal Rank at 10 (MRR@10) và Recall@100. Đối với các tập dữ liệu DL2019 và DL2020, chúng tôi sử dụng Mean Average Precision (MAP), Normalized Discounted Cumulative Gain at 10 (nDCG@10) và Recall@100. Đối với các mô hình BM25CE và BM25BE, cả hai đều re-rank top 1000 mục được truy xuất bởi BM25, Recall@100 được sử dụng đặc biệt để đánh giá hiệu quả re-ranking của chúng. Các chỉ số này cùng nhau cung cấp một đánh giá toàn diện về cách các truy vấn tăng cường ảnh hưởng đến hiệu suất của các mô hình DR.

### 4.4 Phương pháp Baseline

Nghiên cứu của chúng tôi kết hợp năm phương pháp baseline: BM25, Contriever_base, ReContriever_base, Without Augmentation (W/O Aug) và InPars. Các tập dữ liệu huấn luyện, đánh giá và kiểm tra được ghi lại trong Phần 4.1. Đối với BM25, chúng tôi sử dụng Anserini với các tham số Lucene mặc định (k = 0.9 và b = 0.4). TAS-B, Contriever và ReContriever được mô tả trong Phần 3.6. Sự khác biệt giữa InPars và SPTAR là hai mặt: a) InPars sử dụng prompt do con người viết trong khi SPTAR sử dụng soft prompt được tối ưu hóa; b) SPTAR có một mô-đun bộ lọc soft prompt để chọn các cặp tài liệu-truy vấn ví dụ. Để so sánh công bằng với InPars, chúng tôi chọn cùng các cặp tài liệu-truy vấn ví dụ trong prompt của SPTAR cho InPars và sử dụng prompt do con người viết ban đầu của InPars để nhắc nhở LLaMA và Vicuna có được các cặp tài liệu-truy vấn yếu. Chúng tôi thấy rằng đối với prompt do con người viết của InPars, chất lượng của các cặp tài liệu-truy vấn yếu được tạo ra của Vicuna tốt hơn nhiều so với LLaMA, vì vậy, đối với InPars, chúng tôi chọn Vicuna làm bộ tạo dữ liệu yếu. Đối với SPTAR, LLaMA tốt hơn Vicuna và chúng tôi chọn LLaMA cho SPTAR.

### 4.5 Câu hỏi Nghiên cứu

Một tập hợp thực nghiệm rộng rãi được thiết kế để giải quyết các câu hỏi nghiên cứu sau:

RQ1: Framework SPTAR được đề xuất có thể đạt được hiệu suất cải thiện trên các tác vụ DR so với các mô hình baseline không? (Phần 5.1)

RQ2: Trong quá trình tinh chỉnh soft prompt, mô-đun tinh chỉnh soft prompt có thực sự chưng cất kiến thức từ tập dữ liệu vào soft prompt đã học không? Những yếu tố nào góp phần vào các soft prompt đã học? (Phần 5.2.1)

RQ3: Chi phí của mô-đun tinh chỉnh soft prompt là gì? Mô-đun tinh chỉnh soft prompt có tăng đáng kể thời gian huấn luyện và tài nguyên tính toán không? (Phần 5.2.2)

RQ4: Bộ lọc soft prompt đóng vai trò cụ thể gì trong SPTAR? (Phần 5.2.3)

RQ5: Bộ lọc dữ liệu yếu có thể cải thiện thêm hiệu suất của các mô hình DR không? (Phần 5.2.4)

RQ6: Đối với mô-đun tinh chỉnh soft prompt của SPTAR, ảnh hưởng của kích thước dữ liệu huấn luyện X là gì? X lớn hơn có tốt hơn X nhỏ hơn không? (Phần 5.2.5)

RQ7: Đối với mô-đun bộ tăng cường soft prompt của SPTAR, ảnh hưởng của số lượng cặp tài liệu-truy vấn ví dụ M là gì? M lớn hơn có tốt hơn M nhỏ hơn không? (Phần 5.2.6)

## 5 KẾT QUẢ THỰC NGHIỆM

### 5.1 SPTAR so với Các mô hình Baseline (RQ1)

Chúng tôi đã tiến hành đánh giá phương pháp SPTAR của chúng tôi trên MS MARCO và các tập dữ liệu liên quan, bao gồm DL2019, DL2020 và FiQA 2018. Như được trình bày trong Bảng 6, phương pháp tăng cường dữ liệu SPTAR liên tục vượt trội hơn các baseline đã được thiết lập, bao gồm W/O Aug và InPars, trên một loạt các tập dữ liệu được sử dụng rộng rãi và các chỉ số truy xuất chính chỉ trừ R@100 của ColBERT của SPTAR bị InPars vượt qua một chút. Phương pháp TAS-B có được nhiều kết quả tốt nhất (4 trên 10, có 10 chỉ số trong mỗi hàng của Bảng 6) vì nó được chưng cất từ hai mô hình truy xuất dày đặc teacher (cross-encoder và ColBERT) cả hai đều được huấn luyện trên toàn bộ tập dữ liệu MS MARCO. Đối với tất cả 7 bộ truy xuất, 55 trên 70 cải thiện có ý nghĩa thống kê với p-values < 0.05. ColBERT chiếm 8 trên 15 cải thiện không đáng kể.

Độ phức tạp khớp ở cấp độ token của ColBERT giữa truy vấn và tài liệu có thể giải thích tại sao các cải tiến của SPTAR trên ColBERT không vượt qua đáng kể baseline InPars trên các tập dữ liệu liên quan đến MS MARCO. Kiến trúc đơn giản của DPR làm cho nó có khả năng tổng quát hóa tốt hơn ColBERT, điều này được chứng minh bởi thực tế là trên các tập dữ liệu liên quan đến MS MARCO, W/O Aug của DPR tốt hơn W/O Aug của ColBERT. Ngoài ra, tồn tại nhiễu trong các truy vấn được tạo ra, đặc biệt khi soft prompt được học từ một tập dữ liệu nhỏ (giống như tập dữ liệu của W/O Aug), và cơ chế tương tác ở cấp độ token của ColBERT nhạy cảm hơn DPR.

Liên quan đến các mô hình re-ranking, BM25CE và BM25BE, cả hai đều re-rank top 1000 mục được truy xuất ban đầu bởi BM25. BM25CE sử dụng cross-encoder để re-ranking, trong khi BM25BE tích hợp một mô hình DPR. Không có tăng cường dữ liệu, BM25BE đánh bại BM25CE trên 7 trên 10 chỉ số khi được huấn luyện trên một tập dữ liệu được gắn nhãn nhỏ (W/O Aug). Mô hình cross-encoder của BM25CE, đòi hỏi dữ liệu rộng rãi hơn để học hiệu quả các tương tác phức tạp và giảm thiểu rủi ro overfitting, cuối cùng đã thể hiện hiệu suất vượt trội dưới SPTAR so với BM25BE trên 6 trên 10 chỉ số.

Checkpoint được phát hành chính thức của Contriever_base được tải để đánh giá, và chúng tôi đã cải thiện thêm hiệu suất của nó bằng cách tinh chỉnh nó như một mô hình bi-encoder được ký hiệu là Contriever_be trong Bảng 6. Chúng tôi thấy cải thiện hiệu suất ngay cả khi tinh chỉnh Contriever_base trên một tập dữ liệu nhỏ vì W/O Aug của Contriever tốt hơn Contriever_base trên tất cả bốn tập dữ liệu. Phương pháp của chúng tôi, SPTAR của Contriever_be, có thể cải thiện thêm Contriever_base so với giá trị tốt thứ hai một cách đáng kể. InPars của Contriever_be chỉ đánh bại W/O Aug trên 4 trên 10 chỉ số. Tương tự, chúng tôi đánh giá ReContriever_base và ReContriever_be và cùng các mẫu như Contriever cũng được quan sát.

Bằng cách khai thác những lợi ích của tinh chỉnh soft prompt và LLM, mô hình của chúng tôi tạo ra các truy vấn yếu chất lượng cao làm tăng đáng kể các tác vụ DR. Hơn nữa, những cải thiện nhất quán được quan sát trên DPR, TAS-B, BM25BE, BM25CE, Contriever và ReContriever chứng thực khả năng áp dụng tổng quát của cách tiếp cận của chúng tôi, mở rộng ra ngoài các bộ truy xuất dày đặc cụ thể. Đáng chú ý là trong trường hợp không có dữ liệu tăng cường, tất cả các bộ truy xuất dày đặc ngoại trừ Contriever và ReContriever hoạt động tệ hơn mô hình không giám sát BM25. Điều này nhấn mạnh sự phụ thuộc đáng kể của DR vào dữ liệu được gắn nhãn đặc thù cho lĩnh vực và làm nổi bật những hạn chế của việc huấn luyện trực tiếp các bộ truy xuất dày đặc trong các tình huống với dữ liệu ground-truth hạn chế, nơi hiệu suất mong đợi có thể không đạt được.

### 5.2 Nghiên cứu Ablation

Trong phần này, mục tiêu chính của chúng tôi là đánh giá những đóng góp riêng biệt của mỗi mô-đun đối với hiệu quả tổng thể của framework SPTAR. Các thực nghiệm của chúng tôi tập trung vào việc đánh giá các chỉ số perplexity và NDCG@10. Chỉ số perplexity, được rút ra từ tập dữ liệu S100eval, cung cấp cái nhìn sâu sắc về chất lượng tạo văn bản của mô hình. Điểm số NDCG@10 mặc định trong phần này được có được bằng cách đánh giá mô hình SPTAR-DPR được huấn luyện, đánh giá và kiểm tra trên S50train + S100eval + Wsmall, Deval và Dtest tương ứng. Chúng tôi không lọc Wsmall để điểm số NDCG@10 có thể thực sự đại diện cho chất lượng của dữ liệu yếu.

#### 5.2.1 Tác động của Mô-đun Tinh chỉnh Soft Prompt (RQ2)

Để có cái nhìn sâu sắc hơn về các tham số được tối ưu hóa θ*, chúng tôi đã sử dụng thuật toán t-SNE để trực quan hóa các vector token ảo của soft prompt đã học fθ*(s) khi θ* hội tụ với các tập dữ liệu và LLM khác nhau.

Hình 3a minh họa phân phối của các vector token ảo trong không gian hai chiều. Chúng tôi đã sử dụng mô hình ngôn ngữ LLaMA-7B với độ dài token ảo ls = 50 cho thực nghiệm này. Các điểm màu đỏ và xanh dương biểu thị các tập dữ liệu MS MARCO và FiQA tương ứng. Phân tích trực quan rõ ràng cho thấy rằng các vector token ảo từ hai tập dữ liệu thể hiện các phân phối riêng biệt trong không gian hai chiều, với sự chồng chéo tối thiểu. Đáng chú ý, tại giai đoạn khởi tạo mô hình, cả hai tập dữ liệu đều chia sẻ cùng prompt s, khiến cho những thay đổi được quan sát trong phân phối vector sau khi hội tụ trở nên đặc biệt quan trọng. Những phát hiện này làm nổi bật khả năng đáng chú ý của tinh chỉnh prompt trong việc chưng cất kiến thức đặc thù cho lĩnh vực từ các tập dữ liệu vào các vector token prompt đã học. Thành tích này đặc biệt đáng chú ý trong tình huống mà dữ liệu ground-truth quá hạn chế đến mức các prompt do con người viết gặp khó khăn trong việc nắm bắt thông tin đặc thù cho lĩnh vực và kết hợp nó một cách hiệu quả vào thiết kế prompt.

Trong Hình 3b, các màu khác nhau phân biệt các LLM riêng biệt: GPT-2, LLaMA-7B và Vicuna-7B. Chúng tôi giữ tất cả các siêu tham số giống nhau ngoại trừ mô hình ngôn ngữ để đánh giá ảnh hưởng của các mô hình ngôn ngữ khác nhau lên các tham số θ. Sự phân tán của các điểm có cùng màu cho thấy mức độ cập nhật tham số trong quá trình huấn luyện. Hình 3b minh họa rõ ràng rằng đám mây điểm màu đỏ đại diện cho mô hình GPT-2 có ít phân tán hơn, với các điểm được nhóm chặt chẽ với nhau. Ngược lại, đám mây điểm màu xanh dương đại diện cho LLaMA-7B và đám mây điểm màu xanh lá cây đại diện cho Vicuna-7B thể hiện phân tán lớn hơn của các vector token ảo. Quan sát này cho thấy rằng, khi được huấn luyện trên cùng tập dữ liệu, các mô hình LLaMA-7B và Vicuna-7B cho phép mô-đun soft prompt hấp thụ nhiều kiến thức đặc thù cho lĩnh vực hơn, dẫn đến việc nâng cao việc tạo ra các truy vấn tổng hợp. Hơn nữa, những phát hiện tương tự đã được có được khi giải mã các token ảo thành các từ tương ứng. Ví dụ, sau khi huấn luyện mô hình GPT-2, chúng tôi quan sát thấy rằng soft prompt kết quả chỉ đơn thuần sao chép các token prompt được sử dụng trong quá trình khởi tạo, về bản chất là tái tạo prompt thủ công mà không có học tập bổ sung. Ngược lại, khi giải mã các vector token ảo thành từ sử dụng LLaMA-7B và Vicuna-7B, chúng tôi phát hiện rằng các mô hình này không chỉ giữ lại các token prompt ban đầu mà còn có được các biểu tượng và biểu diễn bổ sung liên quan đến văn bản có liên quan, chẳng hạn như "truy vấn", "viết lại", "lập luận", "nâng cao" và "thêm vào", cho thấy các tham số θ thực sự học kiến thức đặc thù cho tác vụ.

Trong Hình 3c, chúng tôi nhằm hiểu ảnh hưởng của các độ dài soft prompt khác nhau lên mô-đun tinh chỉnh bằng cách kiểm tra phân phối vector token ảo của soft prompt đã học. Thực nghiệm này được tiến hành trên LLaMA-7B và tập dữ liệu MS MARCO và tất cả các siêu tham số đều giống nhau ngoại trừ độ dài soft prompt. Ba độ dài 40, 50 và 80 được biểu thị bằng các màu đỏ, xanh dương và xanh lá cây tương ứng. Từ phân phối điểm trong Hình 3c, chúng tôi quan sát sự chồng chéo một phần giữa các điểm đỏ và xanh dương, cũng như một số điểm riêng biệt. Khi độ dài token ảo tăng, vùng phân phối embedding của soft prompt dài hơn bao gồm các vùng tương ứng với những cái ngắn hơn: 40 và 50. Kết quả này phù hợp với kỳ vọng của chúng tôi: với các độ dài khác nhau của soft prompt, các phân phối embedding của các token ảo của soft prompt là khác nhau. Tuy nhiên, bất kể độ dài của chúng, các phân phối của những prompt này có xu hướng có sự chồng chéo đáng kể và các vùng được chia sẻ.

Đối với RQ2, chúng tôi có kết luận: a) các tập dữ liệu có thể được phân biệt từ các soft prompt đã học, chứng minh rằng tinh chỉnh soft prompt thực sự học các soft prompt đặc thù cho tác vụ; b) cả LLM và độ dài của soft prompt đều ảnh hưởng đến các soft prompt đã học.

#### 5.2.2 Hiệu quả của Tinh chỉnh Soft-Prompt (RQ3)

Bảng 7 trình bày số lượng tham số có thể học và hiệu quả hội tụ của tinh chỉnh soft prompt cho các LLM khác nhau trên tập dữ liệu MS MARCO. Đối với mô-đun tinh chỉnh soft prompt trong SPTAR được đề xuất của chúng tôi, mặc dù số lượng lớn các tham số ban đầu Φ của LLM, Φ vẫn bị đóng băng và không cần tinh chỉnh. Các tham số có thể huấn luyện θ liên quan đến tinh chỉnh soft prompt ít hơn đáng kể. Các phần trăm trong cột thứ hai nhấn mạnh rằng việc tinh chỉnh của mô-đun soft prompt liên quan đến một tập hợp tham số θ cực kỳ nhỏ, khoảng bằng 0.003% kích thước của Φ. Đáng chú ý, kích thước của θ giữ nguyên, bất kể sự tăng trưởng của Φ. Đặc điểm này nâng cao đáng kể tính thực tiễn và hiệu quả huấn luyện của SPTAR, vì chúng ta có thể tinh chỉnh các soft prompt đặc thù cho tác vụ với một phần nhỏ tối thiểu các tham số để tối ưu hóa.

Hơn nữa, đối với một tác vụ hoặc tập dữ liệu mới, SPTAR có thể hoàn thành hiệu quả quá trình tinh chỉnh của mô-đun tinh chỉnh soft prompt trong vài epoch. Như được nhấn mạnh trong cột thứ ba của bảng, chúng tôi đã kiểm tra tốc độ hội tụ của mô hình tinh chỉnh soft prompt trên tập dữ liệu đánh giá S100eval (Phần 3.1) bằng số epoch tốt nhất và số này càng thấp thì tốc độ hội tụ càng nhanh. Trở nên rõ ràng rằng việc sử dụng một mô hình ngôn ngữ tiên tiến hơn thúc đẩy sự hội tụ của mô-đun tinh chỉnh soft prompt, chỉ cần bốn hoặc năm epoch để hội tụ. Xem xét cả số lượng θ và tốc độ hội tụ, chúng ta có thể tự tin kết luận rằng mô-đun tinh chỉnh soft prompt tận dụng những ưu điểm được cung cấp bởi LLM trong khi giảm thiểu hiệu quả việc tiêu thụ tài nguyên tính toán liên quan đến tinh chỉnh toàn bộ LLM.

Kết luận, mô hình tinh chỉnh soft prompt chỉ tinh chỉnh một phần nhỏ các tham số θ và việc huấn luyện hội tụ nhanh chóng trên LLM.

#### 5.2.3 Tác động của Mô-đun Bộ lọc Soft Prompt (RQ4)

Với các tham số đã học θ* trong mô-đun tinh chỉnh soft prompt của SPTAR, chúng tôi quan sát thấy các cặp tài liệu-truy vấn ví dụ trong mô-đun bộ tăng cường soft prompt của SPTAR có ảnh hưởng đến chất lượng của dữ liệu yếu được tạo ra, vì vậy cần thiết phải chọn một số M cặp tài liệu-truy vấn nhất định từ SXtrain. Trong phần này, chúng tôi nghiên cứu tác động của mô-đun bộ lọc soft prompt của SPTAR. Trong Bảng 8, chúng tôi báo cáo kết quả tốt nhất của SPTAR-DPR (Phần 5.2.6): a) đối với MS MARCO, chúng tôi báo cáo kết quả của SPTAR-DPR với LLaMA-7B và M = 2; b) đối với FiQA-2018, chúng tôi báo cáo kết quả của SPTAR-DPR với LLaMA-7B và M = 1. SPTAR-DPR được huấn luyện trên S50train + S100eval + Wsmall và kiểm tra trên Dtest. Các cặp ví dụ M tốt nhất và tệ nhất trong Bảng 8 được lọc bằng phương pháp được đề xuất trong Phần 3.3.

Như thể hiện trong Bảng 8, kết quả rõ ràng chứng minh rằng bộ lọc soft prompt nâng cao đáng kể hiệu suất trên tất cả các so sánh. Cụ thể, chúng tôi quan sát giảm perplexity đáng chú ý từ 12.60% đến 98.59% và cải thiện đáng kể từ 3.67% đến 11.44% trên NDCG@10 trong mô hình DPR downstream. Hơn nữa, các phát hiện thực nghiệm của chúng tôi cho thấy rằng trong khi việc sử dụng lý thuyết học trong bối cảnh, được bổ sung bởi các ví dụ hạn chế, nâng cao đáng kể chất lượng của các truy vấn yếu được tạo ra, việc lựa chọn các cặp tài liệu-truy vấn ví dụ cũng có ảnh hưởng đáng kể đến chất lượng tạo văn bản.

#### 5.2.4 Tác động của Mô-đun Bộ lọc Dữ liệu Yếu (RQ5)

Để đánh giá những cải thiện đạt được bằng cách lọc dữ liệu yếu, chúng tôi đã áp dụng các giá trị top-k khác nhau để lọc dữ liệu yếu được tạo ra Wlarge, tạo ra Fk(Wlarge). Sau đó chúng tôi đánh giá hiệu suất của mô hình SPTAR-DPR, được huấn luyện trên S50train + S100eval + Fk(Wlarge), trên Dtest. So sánh này phục vụ để định lượng các lợi ích so với cách tiếp cận không có bộ lọc dữ liệu yếu. Các tham số tối ưu, cụ thể là LLM và M, đã được xác định và giữ nguyên trong phần này để đánh giá độc quyền ảnh hưởng của top-k.

Như thể hiện trong Hình 4, trên MS MARCO, mô hình SPTAR-DPR không có bộ lọc dữ liệu có được điểm số NDCG@10 là 0.2319 trong khi điểm số tăng lên 0.2580 với bộ lọc dữ liệu top-k = 30. Trên FiQA-2018, SPTAR-DPR với bộ lọc top-k = 70 có được điểm số NDCG@10 cao nhất là 0.2404, trong khi nó có được điểm số NDCG@10 là 0.2242 không có bộ lọc dữ liệu. Những lợi ích nhất quán này trên các tập dữ liệu khác nhau nhấn mạnh hiệu quả của mô-đun bộ lọc dữ liệu yếu (Phần 3.4). Chúng tôi không nhận thấy bất kỳ mối tương quan nào giữa top-k và chỉ số NDCG@10; do đó, trong các tình huống thực tế, top-k hoạt động như một siêu tham số đòi hỏi tinh chỉnh cho mỗi tập dữ liệu.

#### 5.2.5 Tác động của Kích thước Huấn luyện X (RQ6)

Trong phần này, chúng tôi phân tích tác động của các kích thước huấn luyện X khác nhau trong mô-đun tinh chỉnh soft prompt của SPTAR. Để đánh giá tác động của X, trước tiên chúng tôi tiến hành tinh chỉnh soft prompt trên SXtrain và đánh giá perplexity trên S100eval. Đáng chú ý, perplexity phục vụ như một chỉ số nội tại để đo lường tác động của X đến chất lượng của các truy vấn yếu được tạo ra. Sau đó, chúng tôi tạo ra Wsmall và kiểm tra mô hình SPTAR-DPR được huấn luyện trên SXtrain + S100eval + Wsmall trên Dtest. Điểm số NDCG@10 được áp dụng để đo lường tác động của X đến các mô hình DR downstream, như DPR.

Như thể hiện trong Hình 5, các phát hiện kết luận một cách thuyết phục chứng minh những cải thiện đáng kể khi sử dụng tinh chỉnh soft prompt với các kích thước huấn luyện X khác nhau so với kết quả có được mà không có tinh chỉnh soft prompt (W/O trong Phần 4.1). Cụ thể, khi X = 50, perplexity giảm 99.78%, và quan sát được cải thiện ấn tượng 37.66%. Thú vị là, rõ ràng rằng việc nâng cao perplexity dễ dàng hơn so với cải thiện NDCG@10, gợi ý sự khác biệt giữa các chỉ số này.

Khác với InPars và Promptagator, chỉ sử dụng một số cặp tài liệu-truy vấn ví dụ trong các prompt do con người viết, các phát hiện của chúng tôi nhấn mạnh những lợi ích của kích thước huấn luyện X lớn hơn một chút trong tinh chỉnh soft prompt, dẫn đến hiệu suất tốt hơn. Sự vượt trội này thể hiện trong perplexity giảm và điểm số NDCG@10 được nâng cao trong các tác vụ downstream với sự gia tăng của kích thước huấn luyện X.

#### 5.2.6 Tác động của Số lượng Cặp Ví dụ M (RQ7)

Trong mô-đun bộ tăng cường soft prompt của SPTAR, khi gắn thẻ các tài liệu chưa được gắn nhãn với các truy vấn yếu, M cặp tài liệu-truy vấn ví dụ được lọc được sử dụng để hướng dẫn LLM. Trong phần này, chúng tôi khám phá tác động của M khác nhau. Ban đầu, chúng tôi chọn LLaMA-7B làm LLM và tiến hành tinh chỉnh soft prompt trên S50train, tính toán perplexity trên S100eval. Sau đó, với M cặp tài liệu-truy vấn ví dụ được lọc từ mô-đun bộ lọc soft prompt của SPTAR (Phần 3.3), chúng tôi tạo ra Wsmall. Cuối cùng, SPTAR-DPR được huấn luyện trên S50train + S100eval + Wsmall được kiểm tra trên Dtest để tính toán NDCG@10. Chúng tôi cũng đã làm cùng các thực nghiệm trên Vicuna, và chúng tôi thấy mô hình LLaMA-7B liên tục mang lại kết quả tốt hơn mô hình Vicuna-7B, bất kể M = 1 hay M = 2. Do đó, chúng tôi chỉ báo cáo kết quả trên LLaMA-7B trong Hình 6.

Như được mô tả trong Hình 6, đối với tập dữ liệu MS MARCO, M = 2 đạt được hiệu suất tốt nhất về perplexity và NDCG@10. Ngược lại, đối với tập dữ liệu FiQA-2008, M = 1 thể hiện hiệu suất vượt trội. Những kết quả này mâu thuẫn với giả định ban đầu của chúng tôi rằng M càng lớn thì perplexity và NDCG@10 càng tốt. Chúng tôi cho rằng sự không nhất quán này là do các phân phối tập dữ liệu khác nhau. Xem xét rằng nhiều tập dữ liệu QA chứa các tài liệu với nhiều truy vấn liên quan, trong đó mỗi truy vấn được xây dựng từ một tập con của tài liệu, nó ngụ ý một mức độ không chắc chắn và phức tạp cao hơn cho mô hình học. Những phức tạp này, đến lượt chúng, tạo ra các hiệu suất khác nhau trên các tập dữ liệu khác nhau. Do đó, chúng tôi thừa nhận tầm quan trọng của việc đi sâu hơn vào chủ đề này trong nghiên cứu tiếp theo.

## 6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi giới thiệu framework tinh chỉnh soft prompt để tăng cường DR (SPTAR) để giải quyết thách thức của dữ liệu huấn luyện đặc thư cho lĩnh vực hạn chế trong các tác vụ DR. Cách tiếp cận của chúng tôi khai thác tinh chỉnh soft prompt để tối ưu hóa các soft prompt trên dữ liệu ground truth hạn chế. Bằng cách nhắc nhở LLM với những soft prompt được tối ưu hóa này cũng như các cặp tài liệu-truy vấn ví dụ, chúng tôi tạo ra các truy vấn yếu cho các tài liệu chưa được gắn nhãn, kết quả là một bộ sưu tập phong phú các cặp tài liệu-truy vấn yếu để huấn luyện các bộ truy xuất dày đặc đặc thư cho lĩnh vực. Để nâng cao thêm chất lượng của các truy vấn được gắn thẻ yếu được tạo ra, chúng tôi tích hợp một bộ lọc soft prompt chọn các cặp tài liệu-truy vấn ví dụ chất lượng cao trong prompt cũng như một mô-đun bộ lọc dữ liệu yếu để làm sạch dữ liệu yếu được tạo ra.

Hiệu quả của cách tiếp cận được đề xuất của chúng tôi được xác thực thông qua các thực nghiệm toàn diện. Công việc này đại diện cho một bước đầu tiên hướng tới một hướng nghiên cứu đầy hứa hẹn. Trong công việc tương lai, chúng tôi nhằm mục đích xem xét kỹ lưỡng khả năng áp dụng rộng rãi của SPTAR bằng cách kiểm tra nó trên các tập dữ liệu đa dạng. Đáng chú ý là hàm mất mát được sử dụng ở đây là mất mát pointwise, ngụ ý việc sử dụng không tối ưu các thể hiện âm. Các nghiên cứu tương lai có thể hưởng lợi từ việc nghiên cứu các mất mát pairwise và listwise. Hơn nữa, tồn tại tiềm năng trong việc thăm dò các phương pháp tinh chỉnh soft prompt đa tác vụ để tăng cường cả hiệu quả và kết quả.
