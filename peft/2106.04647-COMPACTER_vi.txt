# 2106.04647.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2106.04647.pdf
# Kích thước tệp: 721967 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
COMPACTER:
Các Lớp Adapter Siêu Phức Hạng Thấp Hiệu Quả
Rabeeh Karimi Mahabadi
Đại học EPFL, Viện Nghiên cứu Idiap
rabeeh.karimi@idiap.ch

James Henderson
Viện Nghiên cứu Idiap
james.henderson@idiap.ch

Sebastian Ruder
DeepMind
ruder@google.com

Tóm tắt
Việc điều chỉnh các mô hình ngôn ngữ được tiền huấn luyện quy mô lớn cho các tác vụ hạ nguồn thông qua fine-tuning là phương pháp tiêu chuẩn để đạt được hiệu suất tốt nhất trên các bộ đánh giá NLP. Tuy nhiên, việc fine-tuning tất cả các trọng số của các mô hình có hàng triệu hoặc hàng tỷ tham số là không hiệu quả về mẫu, không ổn định trong các thiết lập tài nguyên thấp, và lãng phí vì nó yêu cầu lưu trữ một bản sao riêng của mô hình cho mỗi tác vụ. Các nghiên cứu gần đây đã phát triển các phương pháp fine-tuning hiệu quả về tham số, nhưng những cách tiếp cận này vẫn yêu cầu số lượng tham số tương đối lớn hoặc hoạt động kém hơn fine-tuning tiêu chuẩn. Trong nghiên cứu này, chúng tôi đề xuất COMPACTER, một phương pháp fine-tuning các mô hình ngôn ngữ quy mô lớn với sự đánh đổi tốt hơn giữa hiệu suất tác vụ và số lượng tham số có thể huấn luyện so với các nghiên cứu trước đó. COMPACTER thực hiện điều này bằng cách xây dựng trên các ý tưởng từ adapters, tối ưu hóa hạng thấp, và các lớp nhân siêu phức được tham số hóa.

Cụ thể, COMPACTER chèn các ma trận trọng số cụ thể cho tác vụ vào các trọng số của mô hình tiền huấn luyện, được tính toán hiệu quả như một tổng của các tích Kronecker giữa các trọng số "chậm" được chia sẻ và các ma trận "nhanh" hạng một được định nghĩa cho mỗi lớp COMPACTER. Bằng cách chỉ huấn luyện 0,047% các tham số của mô hình tiền huấn luyện, COMPACTER hoạt động ngang bằng với fine-tuning tiêu chuẩn trên GLUE và vượt trội hơn fine-tuning tiêu chuẩn trên SuperGLUE và trong các thiết lập tài nguyên thấp. Mã nguồn của chúng tôi được công khai tại https://github.com/rabeehk/compacter.

1 Giới thiệu
Các mô hình ngôn ngữ được tiền huấn luyện (PLMs) tiên tiến trong xử lý ngôn ngữ tự nhiên (NLP) đã sử dụng các biểu diễn được tham số hóa quá mức nặng bao gồm hàng trăm triệu hoặc hàng tỷ tham số để đạt được thành công trên một loạt các bộ đánh giá NLP [2,3,4]. Những mô hình này thường được áp dụng cho các tác vụ hạ nguồn thông qua fine-tuning [5], điều này yêu cầu cập nhật tất cả các tham số và lưu trữ một bản sao của mô hình đã fine-tuning cho mỗi tác vụ. Điều này gây ra chi phí lưu trữ và triển khai đáng kể và cản trở khả năng áp dụng các PLMs quy mô lớn vào các ứng dụng thực tế. Ngoài ra, việc fine-tuning các mô hình được tham số hóa quá mức trên các bộ dữ liệu tài nguyên thấp đã được chứng minh là không ổn định và có thể dẫn đến hiệu suất kém [6, 7].

Lấy cảm hứng từ câu nói của John von Neumann, chúng tôi đặt câu hỏi: cho rằng chúng ta đã học được các biểu diễn ngôn ngữ đa mục đích thông qua PLM (tức là chúng ta đã khớp con voi của mình), chúng ta cần bao nhiêu tham số nữa để đạt được hiệu suất tốt nhất trên các tác vụ NLP tiêu chuẩn? Cụ thể, chúng tôi nhằm phát triển các phương pháp thực tế, hiệu quả về bộ nhớ để huấn luyện một tập hợp tham số tối thiểu trong khi đạt được hiệu suất ngang bằng hoặc tốt hơn fine-tuning đầy đủ cho các mô hình NLP tiên tiến.

Với bốn tham số tôi có thể khớp một con voi, và với năm tham số tôi có thể làm nó vẫy vòi.
John von Neumann

Hội nghị lần thứ 35 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2021)
arXiv:2106.04647v2 [cs.CL] 27 Nov 2021

--- TRANG 2 ---
[Biểu đồ với trục x là "Phần trăm Tham số Được Huấn luyện cho Mỗi Tác vụ (Tương đối với T5)" từ 0.01 đến 100.00, trục y là "Điểm GLUE" từ 76 đến 88, và các vòng tròn có kích thước khác nhau đại diện cho memory footprint. Các phương pháp khác nhau được đánh dấu bằng các điểm trên biểu đồ.]

Hình 1: Điểm trung bình trên GLUE (trục y), phần trăm tham số có thể huấn luyện cho mỗi tác vụ (trục x, thang log), và dấu chân bộ nhớ (kích thước của các vòng tròn) của các phương pháp khác nhau.

[Sơ đồ mô tả kiến trúc Adapter với hai phần: bên trái là "Tích hợp Adapter trong mô hình transformer được tiền huấn luyện", bên phải là "Kiến trúc Adapter" với các thành phần như "Chiếu xuống feed forward", "Phi tuyến tính", "Lớp Adapter", "Attention đa đầu", "Adapter+", "Lớp Transformer", "Chuẩn hóa lớp", "Feed forward", "Adapter+", "Chuẩn hóa lớp", "Chiếu lên feed forward", "+"]

Hình 2: Trái: Tích hợp Adapter trong mô hình transformer được tiền huấn luyện. Phải: Kiến trúc Adapter. Theo Houlsby et al. [1], chúng tôi bao gồm các adapter sau các mô-đun attention và feed-forward. Trong quá trình huấn luyện, chúng tôi chỉ cập nhật các chuẩn hóa lớp và adapter (hiển thị màu vàng), trong khi mô hình tiền huấn luyện được cố định.

Tài liệu gần đây đã giới thiệu các phương pháp fine-tuning hiệu quả về tham số. Những cách tiếp cận này thường giữ các tham số của mô hình tiền huấn luyện cố định và giới thiệu một tập hợp tham số có thể huấn luyện cho mỗi tác vụ, đánh đổi số lượng tham số có thể huấn luyện với hiệu suất tác vụ. Ở một đầu của phổ, các prompts, tức là mô tả ngôn ngữ tự nhiên của một tác vụ, cùng với các minh chứng đã được sử dụng để đạt được hiệu suất hợp lý mà không cần cập nhật tham số nào trên một số bộ đánh giá [8] nhưng hiệu suất của chúng thường thua kém các mô hình đã fine-tuning. Chúng cũng yêu cầu các mô hình khổng lồ để hoạt động tốt nhưng việc chọn prompt tốt trở nên khó khăn hơn với các mô hình lớn hơn [9]. Các phương pháp soft prompt coi prompt như các tham số liên tục có thể huấn luyện, được thêm vào trước các đầu vào ở lớp đầu vào hoặc các lớp trung gian [10,11,12]. Tuy nhiên, những phương pháp như vậy thường yêu cầu các mô hình lớn để đạt được hiệu suất tốt và rất nhạy cảm với việc khởi tạo và không ổn định trong quá trình huấn luyện.

Các phương pháp hạng thấp có động lực lý thuyết huấn luyện một số lượng nhỏ tham số nằm trong không gian con chiều thấp sử dụng các phép chiếu ngẫu nhiên [13,14]. Tuy nhiên, việc lưu trữ các ma trận chiếu ngẫu nhiên gây ra chi phí bộ nhớ đáng kể và dẫn đến thời gian huấn luyện chậm. Ở đầu kia của phổ, các phương pháp adapter [1,15] chèn các biến đổi có thể huấn luyện ở các lớp khác nhau của mô hình tiền huấn luyện yêu cầu nhiều tham số hơn các cách tiếp cận nói trên nhưng hiệu quả hơn về bộ nhớ và đạt được hiệu suất tương đương với fine-tuning đầy đủ [1, 16].

Trong nghiên cứu này, chúng tôi đề xuất COMPACTER, một phương pháp fine-tuning các mô hình ngôn ngữ quy mô lớn với sự đánh đổi tuyệt vời giữa số lượng tham số có thể huấn luyện, hiệu suất tác vụ, và dấu chân bộ nhớ, so với các phương pháp hiện có (xem Hình 1). COMPACTER xây dựng trên các ý tưởng từ adapters [1], các phương pháp hạng thấp [13], cũng như các lớp nhân siêu phức gần đây [17]. Tương tự như adapters, COMPACTER chèn các ma trận trọng số cụ thể cho tác vụ vào trọng số của mô hình tiền huấn luyện. Mỗi ma trận trọng số COMPACTER được tính toán như tổng của các tích Kronecker giữa các trọng số "chậm" được chia sẻ và các ma trận "nhanh" hạng một được định nghĩa cho mỗi lớp COMPACTER (xem Hình 3). Kết quả là, COMPACTER đạt được độ phức tạp tham số O(k+d) so với O(kd) cho adapters thông thường, trong đó các adapter có kích thước k×d. Trong thực tế, COMPACTER huấn luyện 0,047% tham số của PLM.

Trên các bộ đánh giá GLUE [18] và SuperGLUE [19] tiêu chuẩn, COMPACTER vượt trội hơn các phương pháp fine-tuning hiệu quả về tham số khác và đạt được hiệu suất ngang bằng hoặc tốt hơn fine-tuning đầy đủ. Trong các thiết lập tài nguyên thấp, COMPACTER vượt trội hơn fine-tuning tiêu chuẩn.

Tóm lại, chúng tôi đóng góp những điều sau: 1) Chúng tôi đề xuất các lớp COMPACTER (Compact Adapter), một phương pháp hiệu quả về tham số để điều chỉnh các mô hình ngôn ngữ quy mô lớn. 2) Chúng tôi cho thấy COMPACTER đạt được hiệu suất thực nghiệm mạnh mẽ trên GLUE và SuperGLUE. 3) Chúng tôi chứng minh rằng COMPACTER vượt trội hơn fine-tuning trong các thiết lập tài nguyên thấp. 4) Chúng tôi cung cấp phân tích độ phức tạp tham số của COMPACTER, cho thấy nó yêu cầu ít tham số hơn đáng kể so với adapters và fine-tuning. 5) Chúng tôi cung cấp đánh giá có hệ thống về các phương pháp fine-tuning hiệu quả về tham số gần đây về thời gian huấn luyện và tiêu thụ bộ nhớ. Chúng tôi phát hành mã nguồn để hỗ trợ nghiên cứu tương lai.

2 Kiến thức nền tảng
Chúng tôi bắt đầu bằng cách giới thiệu kiến thức nền tảng cần thiết về tích Kronecker và các lớp adapter [1, 15].

2.1 Tích Kronecker
Tích Kronecker giữa ma trận A ∈ R^(m×f) và B ∈ R^(p×q), được ký hiệu là A ⊗ B ∈ R^(mp×fq), được định nghĩa toán học như sau:

A ⊗ B = [
a11B ... a1fB
⋮  ... ⋮
am1B ... amfB
]  (1)

trong đó aij biểu thị phần tử ở hàng thứ i và cột thứ j của A.

2.2 Các Lớp Adapter
Nghiên cứu gần đây đã chỉ ra rằng fine-tuning tất cả các tham số của mô hình ngôn ngữ có thể dẫn đến giải pháp không tối ưu, đặc biệt đối với các bộ dữ liệu tài nguyên thấp [6]. Như một giải pháp thay thế, Rebuffi et al. [15] và Houlsby et al. [1] đề xuất chuyển mô hình sang các tác vụ mới bằng cách chèn các mô-đun nhỏ cụ thể cho tác vụ gọi là các lớp adapter vào trong các lớp của mô hình tiền huấn luyện, như mô tả trong Hình 2. Sau đó họ chỉ huấn luyện các adapter và chuẩn hóa lớp, trong khi các tham số còn lại của mô hình tiền huấn luyện vẫn cố định. Cách tiếp cận này cho phép các mô hình ngôn ngữ tiền huấn luyện điều chỉnh hiệu quả với các tác vụ mới.

Mỗi lớp của mô hình transformer bao gồm hai mô-đun chính: a) một khối attention, và b) một khối feed-forward. Cả hai mô-đun đều được theo sau bởi một kết nối bỏ qua. Như thể hiện trong Hình 2, Houlsby et al. [1] đề xuất chèn một lớp adapter sau mỗi khối này trước kết nối bỏ qua.

Adapter là các kiến trúc nút thắt cổ chai. Bằng cách giữ chiều đầu ra tương tự như đầu vào, chúng không gây ra thay đổi nào đối với cấu trúc hoặc tham số của mô hình gốc. Lớp adapter Al cho lớp l bao gồm một phép chiếu xuống, Dl ∈ R^(k×d), phi tuyến GeLU [20], và phép chiếu lên Ul ∈ R^(d×k), trong đó k là chiều đầu vào, và d là chiều nút thắt cổ chai cho lớp adapter. Adapter được định nghĩa là:

Al(x) = Ul(GeLU(Dl(x))) + x  (2)

trong đó x là trạng thái ẩn đầu vào.

3 Phương pháp
Trong phần này, chúng tôi trình bày COMPACTER, một cách nhỏ gọn và hiệu quả để điều chỉnh các PLMs quy mô lớn.

Công thức bài toán Chúng tôi xem xét bài toán tổng quát về fine-tuning các mô hình ngôn ngữ quy mô lớn, trong đó chúng tôi được cho dữ liệu huấn luyện D = {(xi, yi)}^P_{i=1} với P mẫu. Chúng tôi giả sử chúng tôi cũng được cho một mô hình ngôn ngữ quy mô lớn được tiền huấn luyện f(·) được tham số hóa bởi θ tính toán đầu ra cho đầu vào xi. Mục tiêu của chúng tôi là fine-tuning f(·) một cách hiệu quả để cho phép mô hình điều chỉnh với các tác vụ mới.

3.1 Các Lớp Adapter Nhỏ gọn và Hiệu quả
Trong phần này, chúng tôi giới thiệu phiên bản hiệu quả của các lớp adapter, xây dựng trên các tiến bộ gần đây trong các lớp nhân siêu phức được tham số hóa (PHM) [17]. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên khai thác các lớp PHM để fine-tuning hiệu quả các mô hình transformer quy mô lớn. Lớp PHM có dạng tương tự như lớp kết nối đầy đủ, chuyển đổi đầu vào x ∈ R^k thành đầu ra y ∈ R^d:

y = Wx + b  (3)

--- TRANG 3 ---
trong đó W ∈ R^(k×d). Sự khác biệt chính là trong lớp PHM, W được học như một tổng của các tích Kronecker. Giả sử rằng k và d đều chia hết cho siêu tham số do người dùng định nghĩa n ∈ Z>0. Khi đó, ma trận W trong (3) được tính toán như tổng của n tích Kronecker như sau:

W = Σ(i=1 to n) Ai ⊗ Bi  (4)

trong đó Ai ∈ R^(√n×√n) và Bi ∈ R^(k/√n×d/√n). Lớp PHM có độ phức tạp tham số O(kd/n), giảm tham số tối đa 1/n [17] (xem §4).

3.2 Vượt ra ngoài Adapter Siêu phức
Nghiên cứu trước đây chỉ ra rằng một số thông tin được nắm bắt trong các mô hình tiền huấn luyện có thể bị bỏ qua khi chuyển giao [21,22]. Tương tự, sự dư thừa đã được quan sát trong thông tin được nắm bắt bởi các adapter, với các adapter ở các lớp thấp hơn ít quan trọng hơn [1]. Ngoài ra, việc chia sẻ adapter giữa các lớp dẫn đến sự sụt giảm hiệu suất tương đối nhỏ đối với một số tác vụ [23]. Được thúc đẩy bởi những hiểu biết này, chúng tôi đề xuất hai phần mở rộng sau để làm cho các adapter siêu phức hiệu quả hơn.

Chia sẻ thông tin giữa các adapter Việc chia sẻ tất cả các tham số adapter giữa các lớp nhìn chung quá hạn chế và không thể hoạt động ngang bằng với fine-tuning hoặc sử dụng adapter thông thường [23]; tuy nhiên, việc phân rã các adapter của chúng tôi thành các ma trận Ai và Bi như trong Phương trình (4) cho phép chúng tôi linh hoạt hơn. Do đó, chúng tôi chia các trọng số điều chỉnh của mình thành các tham số được chia sẻ nắm bắt thông tin chung hữu ích cho việc điều chỉnh với tác vụ mục tiêu và các tham số cụ thể cho adapter tập trung vào việc nắm bắt thông tin liên quan để điều chỉnh từng lớp riêng lẻ. Cụ thể, chúng tôi định nghĩa Ai như các tham số được chia sẻ chung cho tất cả các lớp adapter trong khi Bi là các tham số cụ thể cho adapter.

Tham số hóa hạng thấp Các phương pháp hạng thấp [13,14] đã chứng minh rằng hiệu suất mạnh mẽ có thể đạt được bằng cách tối ưu hóa một tác vụ trong không gian con hạng thấp. Tương tự, chúng tôi đưa ra giả thuyết rằng một mô hình cũng có thể được điều chỉnh hiệu quả bằng cách học các biến đổi trong không gian con hạng thấp. Để đạt được điều này, chúng tôi đề xuất tham số hóa Bi ∈ R^(k/√n×d/√n) như một ma trận hạng thấp, là tích của hai trọng số hạng thấp si ∈ R^(k/√n×r) và ti ∈ R^(r×d/√n), trong đó r là hạng của ma trận. Kết hợp cả hai phần mở rộng, chúng tôi đề xuất lớp nhân siêu phức được tham số hóa hạng thấp (LPHM):

W = Σ(i=1 to n) Ai ⊗ Bi = Σ(i=1 to n) Ai ⊗ (si*ti^T)  (5)

Nhìn chung, chúng tôi đặt r = 1 sao cho Bi là một ma trận hạng một. Tùy thuộc vào độ phức tạp của tác vụ mục tiêu, r có thể được đặt thành giá trị cao hơn. Hình 3 minh họa phương pháp của chúng tôi. Nhìn chung, lớp LPHM giảm độ phức tạp hơn nữa xuống O(k+d) (xem §4). Lớp LPHM cũng có thể được xem là tận dụng các trọng số "chậm" Ai được chia sẻ giữa các adapter và nắm bắt thông tin chung và các trọng số "nhanh" Bi học thông tin cụ thể cho adapter để điều chỉnh từng lớp riêng lẻ [25].

COMPACTER Dựa trên công thức trên, chúng tôi giới thiệu các lớp COMPACTER, thay thế các lớp chiếu xuống và chiếu lên trong adapter như sau:

Al(x) = LPHMUl(GeLU(LPHMDl(x))) + x

trong đó các trọng số chiếu lên LPHMUl được tính toán như trong (5), thay thế lớp Ul trong (2). Tương tự, các trọng số chiếu xuống LPHMDl thay thế lớp Dl. Trong khi hai adapter trong mỗi lớp của một transformer có các trọng số hạng một si và ti riêng của chúng, chúng tôi chia sẻ Ai trên tất cả các lớp và vị trí của các lớp adapter.

4 Hiệu quả về Tham số
Trong phần này, chúng tôi so sánh số lượng tham số của COMPACTER với adapter.

Tham số adapter Trong thiết lập tiêu chuẩn, hai adapter được thêm vào mỗi lớp của mô hình transformer [1]. Mỗi lớp adapter bao gồm 2kd tham số cho các ma trận chiếu xuống và chiếu lên (Ul, Dl) tương ứng trong đó k là kích thước của chiều đầu vào và d là chiều nút thắt cổ chai của adapter. Tổng số tham số cho các adapter cho mô hình transformer với L lớp của cả encoder và decoder do đó là 2L(2kd), tỷ lệ tuyến tính với cả ba biến.

Tham số PHM-ADAPTER Trong lớp PHM thông thường [17], như mô tả trong Phương trình (4), các tham số của Ai ∈ R^(√n×√n) và Bi ∈ R^(k/√n×d/√n) định nghĩa bậc tự do cho W là n(kd/n² + n²) = kd/n + n³. Với điều kiện nhẹ rằng kd > n⁴, thì kd/n chiếm ưu thế và kích thước tham số tổng thể của lớp PHM trong (4) là O(kd/n). Điều kiện này được thỏa mãn cho các giá trị thông thường của adapter, lớp PHM, và PLM quy mô lớn như T5-large, với kích thước ẩn k = 1024, kích thước ẩn adapter d ∈ {24, 32, 48, 96}, và n = 2, 4, 8, 12. Do đó, lớp PHM cung cấp giảm tham số gần 1/n so với các lớp kết nối đầy đủ tiêu chuẩn, là O(kd).

Tương tự, việc sử dụng các lớp PHM để mô hình hóa các ma trận chiếu xuống và chiếu lên cung cấp giảm tham số gần 1/n. Mỗi adapter với lớp PHM có tổng cộng 2(kd/n + n³) tham số. Đối với mô hình Transformer với L lớp, tổng số tham số của PHM-ADAPTER là 4L(kd/n + n³).

Tham số COMPACTER COMPACTER chia sẻ các ma trận trọng số được huấn luyện {Ai}ⁿᵢ₌₁ trong (5) bao gồm n³ tham số trên tất cả các lớp. COMPACTER cũng có hai trọng số hạng một cho mỗi adapter, si, ti trong (5) bao gồm k/√n + d/√n tham số, dẫn đến tổng cộng 2n(k/√n + d/√n) tham số cho các trọng số chiếu xuống và chiếu lên. Do đó, tổng số tham số của COMPACTER là 4L(k + d) + n³ cho một transformer với L lớp trong encoder và decoder.

Trong các thiết lập có số lượng lớp lớn, số hạng chiếm ưu thế là 4L(k + d). Do đó, với điều kiện nhẹ rằng 4L(k + d) > n³, COMPACTER có độ phức tạp O(k + d), hiệu quả hơn nhiều so với độ phức tạp O(kd) của adapter và O(kd/n) của PHM-ADAPTER tương ứng. Trong các thiết lập mà n lớn, số lượng tham số cho các ma trận trọng số được chia sẻ {Ai}ⁿᵢ₌₁ cho tất cả các lớp vẫn không đổi trong COMPACTER với tổng cộng n³ tham số trong khi điều này tỷ lệ tuyến tính với số lượng lớp L cho các lớp PHM và adapter. Ví dụ, trong mô hình T5 BASE với 222M tham số [3], COMPACTER chỉ học 0,047% tham số, và duy trì hiệu suất tương đương với fine-tuning đầy đủ.

5 Thí nghiệm
Bộ dữ liệu Theo Raffel et al. [3], chúng tôi đánh giá hiệu suất của các phương pháp trên các bộ đánh giá GLUE [18] và SUPERGLUE [19]. Các bộ đánh giá này bao gồm nhiều tác vụ phát hiện paraphrase (MRPC, QQP), phân loại cảm xúc (SST-2), suy luận ngôn ngữ tự nhiên (MNLI, RTE, QNLI, CB), tính chấp nhận ngôn ngữ (CoLA), trả lời câu hỏi (MultiRC, ReCoRD, BoolQ), phân biệt nghĩa từ (WiC), và hoàn thành câu (COPA). Vì các bộ test gốc không được công khai, chúng tôi theo Zhang et al. [27] và tách 1k mẫu từ tập huấn luyện mà chúng tôi sử dụng cho validation, trong khi chúng tôi sử dụng dữ liệu validation gốc như tập test. Đối với các bộ dữ liệu có ít hơn 10k mẫu (RTE, MRPC, STS-B, CoLA, COPA, WiC, CB, BoolQ, MultiRC), chúng tôi chia tập validation gốc làm đôi, sử dụng một nửa cho validation và nửa kia cho testing.

Chi tiết thí nghiệm Chúng tôi sử dụng mô hình T5 encoder-decoder tiên tiến [3] làm mô hình cơ sở cho tất cả các phương pháp trong thí nghiệm của chúng tôi. Để hiệu quả tính toán, chúng tôi báo cáo tất cả kết quả trên mô hình T5 BASE (12 lớp encoder và decoder và 222M tham số). Chúng tôi sử dụng implementation PyTorch HuggingFace [28]. Chúng tôi fine-tuning tất cả các phương pháp trong 3 epoch trên các bộ dữ liệu lớn và 20 epoch trên các bộ dữ liệu tài nguyên thấp của GLUE (MRPC, CoLA, STS-B, RTE, BoolQ, CB, COPA, WiC) để cho phép các mô hình hội tụ [27]. Đối với tất cả các phương pháp dựa trên adapter, chúng tôi thử nghiệm với các adapter có kích thước nút thắt cổ chai {96, 48, 24}. Chúng tôi lưu checkpoint mỗi epoch cho tất cả các mô hình và báo cáo kết quả cho các siêu tham số hoạt động tốt nhất trên tập validation cho mỗi tác vụ. Đối với các lớp PHM, chúng tôi sử dụng implementation PyTorch của Le et al. [29]. Chúng tôi bao gồm chi tiết cấp thấp trong Phụ lục A. Đối với các phương pháp của chúng tôi, chúng tôi thử nghiệm với n = {4, 8, 12} và báo cáo mô hình hoạt động tốt nhất. Chúng tôi bao gồm kết quả cho tất cả các giá trị của n trong Phụ lục B.

Theo Mahabadi et al. [30], chúng tôi đóng băng lớp đầu ra của mô hình tiền huấn luyện cho tất cả các tác vụ trên tất cả các phương pháp. Chúng tôi hiển thị kết quả với việc fine-tuning lớp đầu ra trong Phụ lục C. Theo Houlsby et al. [1], chúng tôi cập nhật các tham số chuẩn hóa lớp cho tất cả các phương pháp khi áp dụng được.

5.1 Baseline
Chúng tôi so sánh với một số phương pháp fine-tuning hiệu quả về tham số được đề xuất gần đây:

T5 BASE Chúng tôi so sánh phương pháp của mình với thực tiễn tiêu chuẩn là fine-tuning T5, nơi chúng tôi fine-tuning tất cả các tham số của mô hình trên từng tác vụ riêng lẻ.

ADAPTER Chúng tôi so sánh với một baseline adapter mạnh mẽ [1], thêm adapter cho mỗi tác vụ sau các mô-đun feed-forward và attention trong mỗi khối transformer của T5.

PFEIFFER-ADAPTER Pfeiffer et al. [31] đề xuất một biến thể adapter hiệu quả hơn, chỉ giữ một trong các adapter trong mỗi lớp để có hiệu quả huấn luyện tốt hơn. Chúng tôi thử nghiệm với việc giữ adapter nào và thấy rằng giữ adapter sau mô-đun self-attention trong mỗi lớp hoạt động tốt nhất.

ADAPTER-LOWRANK Chúng tôi tham số hóa trọng số của mỗi adapter như một tích của hai trọng số hạng một.

PROMPT TUNING Prompt tuning [12] là biến thể kế thừa của Li và Liang [10], thêm một prompt liên tục được khởi tạo ngẫu nhiên vào đầu vào (PROMPT TUNING-R). Chúng tôi cũng so sánh với một biến thể, khởi tạo prompts sử dụng embedding token của từ vựng mô hình ngôn ngữ tiền huấn luyện (PROMPT TUNING-T) [12].

INTRINSIC-SAID Structure Aware Intrinsic Dimension [14] fine-tuning mô hình bằng cách tái tham số hóa các tham số trong không gian con chiều thấp hơn d₀ (d₀ ≪ D): θᵢ = θᵢ,₀ + λᵢP_{d₀→m}θᵢ trong đó tham số θᵢ,₀ là các tham số của mô hình tiền huấn luyện và P ∈ R^{d₀×m} → R^D là một phép chiếu tuyến tính ngẫu nhiên qua biến đổi Fastfood [32]. Sau đó họ xem xét tổng số ma trận trọng số trong PLM, m, và gán một trọng số cho mỗi ma trận, dẫn đến 2R^{min} tổng cộng bằng cách đánh đổi m tham số từ không gian chiều thấp d₀ ∈ R^{d₀}. Khi đó, tổng số tham số có thể huấn luyện là d₀ + m ∈ R^{d₀+m}.

ADAPTER DROP Chúng tôi áp dụng phương pháp của Rücklé et al. [23], loại bỏ các adapter từ các lớp transformer thấp hơn để có hiệu quả huấn luyện tốt hơn cho T5 với ADAPTER. Do đó, chúng tôi loại bỏ adapter từ năm lớp đầu tiên của cả encoder và decoder trong T5 BASE.

BITFIT Cai et al. [33] đề xuất đóng băng các trọng số và chỉ huấn luyện các bias. Bằng cách không lưu trữ các kích hoạt trung gian, phương pháp này cho phép tiết kiệm bộ nhớ đáng kể. Ravfogel et al. [34] nghiên cứu phương pháp tương tự cho PLM chỉ fine-tuning các bias và lớp đầu ra cuối cùng.

--- TRANG 4 ---
[Sơ đồ minh họa việc tạo trọng số của hai lớp COMPACTER khác nhau: W₁ ∈ R^{d×k} (hàng đầu) và W₂ ∈ R^{d×k} (hàng thứ hai). Chúng tôi tạo W₁ và W₂ bằng cách sử dụng Wⱼ = Σⁿᵢ₌₁ Aᵢ ⊗ Bⱼᵢ = Σⁿᵢ₌₁ Aᵢ ⊗ (sⱼᵢtⱼᵢᵀ) (5), bằng cách tính tổng của các tích Kronecker của các ma trận được chia sẻ Aᵢ và các ma trận cụ thể cho adapter Bⱼᵢ, với i ∈ {1, ..., n} và chỉ số adapter j ∈ {1, 2}. Chúng tôi tạo mỗi Bⱼᵢ bằng cách nhân các trọng số hạng một độc lập. Trong ví dụ này n = 2, d = 6, và k = 8.]

Hình 3: Minh họa việc tạo trọng số của hai lớp COMPACTER khác nhau

5.2 Các Phương pháp của Chúng tôi

PHM-ADAPTER Chúng tôi học các trọng số của adapter sử dụng các lớp PHM như trong (4). Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên khai thác ý tưởng của PHM [17] để fine-tuning hiệu quả các mô hình ngôn ngữ quy mô lớn.

COMPACTER Chúng tôi học trọng số adapter sử dụng các lớp LPHM như mô tả trong (5). Chúng tôi cũng khám phá một biến thể chỉ giữ lớp COMPACTER sau lớp feed-forward trong mỗi khối transformer (COMPACTER++).

5.3 Kết quả trên Bộ đánh giá GLUE

Bảng 1 hiển thị kết quả trên GLUE với T5 BASE (xem Phụ lục E cho kết quả trên T5 SMALL). COMPACTER và COMPACTER++ vượt trội hơn tất cả các phương pháp hiệu quả về tham số trước đây và hoạt động ngang bằng với fine-tuning đầy đủ trong khi chỉ huấn luyện 0,07% và 0,047% tham số tương ứng. Bây giờ chúng tôi thảo luận chi tiết về các phương pháp khác nhau.

Các phương pháp dựa trên Adapter Đối với ADAPTER, việc không fine-tuning classifier làm tổn hại đáng kể hiệu suất (85,78 so với 86,48; cf. Phụ lục C). PFEIFFER-ADAPTER, chỉ thêm adapter sau mô-đun self-attention vượt trội hơn ADAPTER tiêu chuẩn trong khi hiệu quả hơn về tham số. ADAPTER DROP đạt hiệu suất thấp hơn fine-tuning, chứng minh rằng việc điều chỉnh các lớp thấp hơn của mô hình encoder-decoder T5 quan trọng đối với hiệu suất của nó. Ngoài ra, ADAPTER-LOWRANK không đủ biểu cảm để hoạt động tốt trên bộ đánh giá này.

Prompt tuning và BitFit Đối với PROMPT TUNING, chúng tôi quan sát độ nhạy cảm cao với việc khởi tạo và tốc độ học, như cũng được xác nhận trong [10]. Chúng tôi thử nghiệm với nhiều seed ngẫu nhiên nhưng hiệu suất thua kém đáng kể so với fine-tuning, đặc biệt trên các bộ dữ liệu tài nguyên thấp. Điều này có thể được giải thích bởi tính linh hoạt thấp của những phương pháp như vậy vì tất cả thông tin cần được chứa trong các tiền tố. Kết quả là, phương pháp chỉ cho phép tương tác hạn chế với phần còn lại của mô hình và hiệu suất tốt đòi hỏi các mô hình rất lớn [12]. Ngoài ra, việc tăng độ dài chuỗi dẫn đến chi phí bộ nhớ (xem §5.5) và số lượng token prompt bị giới hạn bởi số lượng token có thể vừa trong độ dài đầu vào tối đa của mô hình, điều này làm cho những phương pháp như vậy kém linh hoạt và không phù hợp để xử lý các ngữ cảnh lớn. Tương tự, BITFIT hoạt động kém hơn fine-tuning, đặc biệt trên các bộ dữ liệu tài nguyên thấp.

Intrinsic-SAID Thú vị là hiệu suất trung bình của INTRINSIC-SAID, chỉ fine-tuning 0,009% tham số của mô hình chỉ thấp hơn baseline fine-tuning 1,05 điểm. Tuy nhiên, phương pháp này có hai nhược điểm thực tế: a) việc lưu trữ các ma trận chiếu ngẫu nhiên dẫn đến chi phí bộ nhớ đáng kể; b) nó rất chậm để huấn luyện (xem §5.5). Mặc dù vậy, INTRINSIC-SAID cung cấp những hiểu biết về tính hiệu quả của tối ưu hóa hạng thấp của các mô hình ngôn ngữ tiền huấn luyện [14], điều này thúc đẩy sự phát triển của các phương pháp hiệu quả về tham số như COMPACTER.

COMPACTER Đối với các phương pháp được đề xuất của chúng tôi, chúng tôi quan sát rằng việc fine-tuning lớp đầu ra cho cả PHM-ADAPTER và COMPACTER++ không tạo ra sự khác biệt hiệu suất nhiều (xem Phụ lục C). PHM-ADAPTER giảm tham số của ADAPTER từ 0,83% xuống 0,179% (với n = 12), hiệu quả hơn 4,64 lần về tham số. COMPACTER giảm số lượng tham số xuống tỷ lệ đáng chú ý 0,073% trong khi đạt được kết quả tương đương với fine-tuning đầy đủ. Bằng cách loại bỏ lớp COMPACTER sau self-attention, COMPACTER++ đạt được hiệu suất tương tự, trong khi giảm tham số xuống 0,047%. Việc điều chỉnh mà không cập nhật chuẩn hóa lớp có thể là một hướng đi triển vọng để giảm tham số hơn nữa, ví dụ bằng cách xây dựng trên các tiến bộ gần đây trong các mô hình không có chuẩn hóa [35], điều mà chúng tôi để lại cho nghiên cứu tương lai.

5.4 Kết quả trên Bộ đánh giá SUPER GLUE

Bảng 2 hiển thị hiệu suất của các phương pháp trên SUPER GLUE [19]. Chúng tôi bao gồm kết quả cho tất cả các giá trị của n trong Phụ lục D. Chúng tôi quan sát một mẫu tương tự như trên GLUE trong Bảng 1. COMPACTER và COMPACTER++ hoạt động tốt hơn đáng kể so với các phương pháp fine-tuning hiệu quả về tham số khác và thậm chí vượt trội hơn fine-tuning đầy đủ trong khi chỉ huấn luyện 0,073% và 0,048% tham số.

5.5 Đánh giá Hiệu quả

Trong phần này, chúng tôi so sánh hiệu quả của các phương pháp được đề xuất với các phương pháp fine-tuning nhỏ gọn về tham số được đề xuất gần đây dưới cùng một ngân sách tính toán. Để làm điều này, chúng tôi huấn luyện tất cả các phương pháp trong 1 epoch trên bộ dữ liệu MNLI. Đối với mỗi phương pháp, chúng tôi chọn kích thước batch lớn nhất phù hợp với ngân sách cố định của bộ nhớ GPU (24 GB). Đối với tất cả các phương pháp dựa trên adapter, chúng tôi cố định kích thước adapter là 24. Đối với PROMPT TUNING, chúng tôi đặt số lượng token tiền tố là 100. Đối với INTRINSIC-SAID, chúng tôi đặt d₀ = 1400. Cuối cùng, chúng tôi đặt n = 4. Trong Bảng 3, chúng tôi báo cáo phần trăm tham số được huấn luyện cho mỗi tác vụ, thời gian huấn luyện mỗi epoch, và sử dụng bộ nhớ của mỗi phương pháp. Hơn nữa, Hình 1 hiển thị sự đánh đổi giữa hiệu suất định lượng, phần trăm tham số được huấn luyện, và dấu chân bộ nhớ.

Các cách tiếp cận của chúng tôi có một số tính chất hấp dẫn. Dựa trên phân tích của chúng tôi trong Bảng 1, COMPACTER và COMPACTER++ đạt được sự kết hợp tốt nhất giữa điểm GLUE cao trung bình trên tất cả các tác vụ, cộng với số lượng tham số thấp hơn đáng kể (0,073% và 0,047% tương ứng). Ngoài COMPACTER++ hoạt động tốt, yêu cầu bộ nhớ của nó là thứ hai tốt nhất trong tất cả các phương pháp, giảm sử dụng bộ nhớ -41,94% so với T5 BASE. COMPACTER và COMPACTER++ cũng tăng tốc huấn luyện đáng kể, -13,41% và -26,51% so với T5 BASE. Mặt khác, BITFIT, bằng cách không lưu trữ các kích hoạt trung gian, có yêu cầu bộ nhớ thấp nhất (-64,2% so với T5 BASE) và nhanh nhất (-35,06% so với T5 BASE) với chi phí hiệu suất định lượng thấp hơn (1,53 điểm thấp hơn; xem Bảng 1).

Các phương pháp dựa trên việc cắt bỏ adapter, tức là PFEIFFER-ADAPTER và ADAPTER DROP giảm chi phí bộ nhớ và cải thiện thời gian huấn luyện. Tuy nhiên, số lượng tham số của chúng gần như cao hơn một bậc so với COMPACTER++, với 9,1× và 10,5× tham số hơn tương ứng. Hơn nữa, mặc dù PFEIFFER-ADAPTER hoạt động ngang bằng với fine-tuning đầy đủ với sự suy giảm nhẹ (Bảng 1), ADAPTER DROP đạt hiệu suất thấp hơn (-0,65 ít hơn trung bình trên tất cả các tác vụ). Chúng tôi lưu ý rằng việc loại bỏ adapter từ các lớp transformer là một kỹ thuật tổng quát và có thể được áp dụng cho COMPACTER để cải thiện hiệu quả hơn nữa, điều mà chúng tôi để lại cho nghiên cứu tương lai.

Tương tự, mặc dù ADAPTER-LOWRANK giảm chi phí bộ nhớ và cải thiện thời gian huấn luyện, nó đạt hiệu suất thấp hơn (Bảng 1) (-0,68 ít hơn trung bình trên tất cả các tác vụ).

Ở đầu kia của phổ, INTRINSIC-SAID và các phương pháp PROMPT TUNING có số lượng tham số thấp nhất. Tuy nhiên, cả hai đều đi kèm với chi phí bộ nhớ cao (41,14% và 24,42% so với fine-tuning đầy đủ (T5 BASE) tương ứng), chậm nhất để huấn luyện, và hiệu suất của chúng thua kém đáng kể so với fine-tuning đầy đủ (xem Bảng 1). Đối với PROMPT TUNING, chi phí bộ nhớ cao là do thực tế rằng độ phức tạp tính toán của self-attention, yêu cầu lưu trữ ma trận attention đầy đủ cho tính toán gradient, tỷ lệ bậc hai với độ dài chuỗi [36]. Đối với INTRINSIC-SAID, yêu cầu bộ nhớ cao là do việc lưu trữ các ma trận chiếu ngẫu nhiên lớn, điều này giới hạn việc áp dụng INTRINSIC-SAID cho fine-tuning các PLM quy mô lớn. Hơn nữa, việc tính toán các phép chiếu qua biến đổi FastFood, mặc dù về mặt lý thuyết có thể trong O(D log d₀) [32], trong thực tế rất chậm ngay cả với implementation CUDA. Đối với các mô hình ngôn ngữ tiền huấn luyện với số lượng tham số lớn, việc cấp phát các phép chiếu ngẫu nhiên cho toàn bộ không gian tham số là không khả thi. Trong khi việc sử dụng biến đổi Fastfood phần nào giảm thiểu vấn đề này bằng cách giảm sử dụng bộ nhớ từ O(Dd₀) xuống O(D), vấn đề bộ nhớ với những phương pháp như vậy vẫn chưa được giải quyết.

Nhìn chung, với kích thước của các mô hình transformer quy mô lớn với hàng triệu và hàng tỷ tham số, như T5 [3], việc sử dụng bộ nhớ hiệu quả có tầm quan trọng tối quan trọng đối với các ứng dụng thực tế. COMPACTER và COMPACTER++ cung cấp sự đánh đổi tuyệt vời về hiệu suất, sử dụng bộ nhớ, và thời gian huấn luyện. Liên quan đến nguồn cảm hứng từ câu nói của von Neumann, chúng tôi do đó thấy rằng chỉ cần một số lượng tham số bổ sung tương đối nhỏ là cần thiết cho việc điều chỉnh PLM thực tế và hiệu quả.

5.6 Fine-tuning Tài nguyên Thấp

COMPACTER++ có ít tham số hơn đáng kể so với T5 BASE. Trong phần này, chúng tôi điều tra xem liệu điều này có thể giúp COMPACTER++ khái quát hóa tốt hơn trong các thiết lập hạn chế tài nguyên hay không. Chúng tôi lấy mẫu con từ mỗi bộ dữ liệu của GLUE cho các kích thước khác nhau trong phạm vi {100, 500, 1000, 2000, 4000}. Hình 4 hiển thị kết quả. COMPACTER++ cải thiện đáng kể kết quả trong thiết lập tài nguyên thấp, cho thấy fine-tuning hiệu quả hơn trong chế độ này.

6 Nghiên cứu Liên quan

Adapter Adapter gần đây đã xuất hiện như một paradigm mới để fine-tuning các mô hình ngôn ngữ tiền huấn luyện [1]. Trong một hướng nghiên cứu khác, Üstün et al. [37] đề xuất một phương pháp phân tích phụ thuộc đa ngôn ngữ dựa trên adapter và mạng tạo tham số ngữ cảnh [38], nơi họ tạo ra các tham số adapter được điều kiện hóa trên các embedding ngôn ngữ đầu vào được huấn luyện. Tuy nhiên, điều này dẫn đến một số lượng lớn tham số bổ sung so với mô hình cơ sở. Đồng thời, Mahabadi et al. [30] sử dụng một hypernetwork nhỏ gọn duy nhất cho phép tạo ra các trọng số adapter hiệu quả được điều kiện hóa trên nhiều tác vụ và lớp của mô hình transformer. Pilault et al. [39] cũng đề xuất một transformer được điều kiện hóa theo tác vụ cho học đa tác vụ ít hiệu quả hơn về tham số. Công trình nói trên bổ sung cho COMPACTER, và người ta có thể kết hợp COMPACTER với tạo tham số ngữ cảnh để tạo ra các mô-đun adapter. So với Mahabadi et al. [30], COMPACTER++ giảm tham số 6,2×.

Biểu diễn siêu phức Những tiến bộ học sâu trong lĩnh vực siêu phức đang trong giai đoạn mới hình thành, và hầu hết công trình khá gần đây [40,41,42,43,44]. Việc thay thế các phép nhân ma trận trong mạng tiêu chuẩn bằng tích Hamilton có ít bậc tự do hơn cung cấp tiết kiệm tham số lên đến 4× trong một phép nhân duy nhất [42,44]. Rất gần đây, Zhang et al. [17] mở rộng những phương pháp như vậy theo cách có thể giảm tham số của lớp kết nối đầy đủ dưới điều kiện nhẹ xuống 1/n, trong đó n là tham số do người dùng chỉ định. Theo hiểu biết của chúng tôi, không có nghiên cứu trước đây nào cố gắng tận dụng không gian siêu phức để fine-tuning hiệu quả các mô hình ngôn ngữ quy mô lớn.

Các mô hình hiệu quả về tham số khác Li et al. [13] và Aghajanyan et al. [14] nghiên cứu huấn luyện mô hình trong không gian con định hướng ngẫu nhiên chiều thấp thay vì không gian tham số gốc của chúng. Một hướng nghiên cứu gần đây khác đã chỉ ra rằng các mô hình tiền huấn luyện như BERT dư thừa về khả năng, cho phép sparsification đáng kể mà không suy giảm nhiều về các chỉ số cuối [45,46,47]. Tuy nhiên, những phương pháp như vậy vẫn không được hỗ trợ tốt bởi phần cứng hiện tại và thường hoạt động kém hơn so với các kiến trúc hiệu quả chuyên dụng [48].

7 Kết luận

Chúng tôi đã đề xuất COMPACTER, một phương pháp fine-tuning nhẹ cho các mô hình ngôn ngữ quy mô lớn. COMPACTER tạo ra các trọng số bằng cách tính tổng các tích Kronecker giữa các trọng số "chậm" được chia sẻ và các ma trận "nhanh" hạng một, cụ thể cho mỗi lớp COMPACTER. Tận dụng công thức này, COMPACTER giảm số lượng tham số trong adapter đáng kể từ O(kd) xuống O(k+d). Thông qua các thí nghiệm rộng rãi, chúng tôi chứng minh rằng mặc dù học ít hơn 2127,66× tham số so với fine-tuning tiêu chuẩn, COMPACTER đạt được hiệu suất tương đương hoặc tốt hơn trong thiết lập dữ liệu đầy đủ và vượt trội hơn fine-tuning trong các tình huống hạn chế dữ liệu.

--- TRANG 5 ---
[Tiếp tục với các trang còn lại theo cùng định dạng...]
