# 2305.11186.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.11186.pdf
# File size: 1168509 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Compress, Then Prompt: Improving Accuracy-Efficiency
Trade-off of LLM Inference with Transferable Prompt
Zhaozhuo Xu∗1, Zirui Liu∗1, Beidi Chen2, Yuxin Tang1, Jue Wang3, Kaixiong Zhou1, Xia
Hu1, and Anshumali Shrivastava1
1Department of Computer Science, Rice University
2Department of Electrical and Computer Engineering, Carnegie Mellon University
3ETH Zürich, Switzerland
{Zhaozhuo.Xu, Zirui.Liu, Yuxin.Tang, Kaixiong.Zhou, Xia.Hu,
Anshumali.Shrivastva}@rice.edu ,beidic@andrew.cmu.edu ,juewang@inf.ethz.ch
Abstract
While the numerous parameters in Large Language Models (LLMs) contribute to their su-
perior performance, this massive scale makes them inefficient and memory-hungry. Thus, they
are hard to deploy on commodity hardware, such as one single GPU. Given the memory and
power constraints of such devices, model compression methods are widely employed to reduce
both the model size and inference latency, which essentially trades off model quality in return for
improved efficiency. Thus, optimizing this accuracy-efficiency trade-off is crucial for the LLM
deployment on commodity hardware. In this paper, we introduce a new perspective to optimize
this trade-off by prompting compressed models. Specifically, we first observe that for certain
questions, the generation quality of a compressed LLM can be significantly improved by adding
carefully designed hard prompts, though this isn’t the case for all questions. Based on this
observation, we propose a soft prompt learning method where we expose the compressed model
to the prompt learning process, aiming to enhance the performance of prompts. Our experi-
mental analysis suggests our soft prompt strategy greatly improves the performance of the 8×
compressed LLaMA-7B model (with a joint 4-bit quantization and 50% weight pruning com-
pression), allowing them to match their uncompressed counterparts on popular benchmarks.
Also, we demonstrate that these learned prompts can be transferred across various datasets,
tasks, and compression levels. Hence with this transferability, we can stitch the soft prompt to
a newly compressed model to improve the test-time accuracy in an “in-situ” way.
∗Equal contribution. The order of authors is determined by flipping a coin.
1arXiv:2305.11186v2  [cs.CL]  10 Oct 2023

--- PAGE 2 ---
1 Introduction
Large Language Models (LLMs) [27, 28, 2, 40, 33] has revolutionized the field of Natural Language
Processing (NLP). Notably, LLMs are known for their in-context learning ability, allowing them
to generalize to unseen tasks without additional fine-tuning [2]. Specifically, LLMs are controlled
through user-provided natural language specifications of the task, or prompts, which illustrate how
to complete a task. Equipped with the in-context learning ability, we only need to serve a single
large model to efficiently handle different tasks. Despite of their remarkable adaptability, LLMs are
very expensive to deploy [3, 35]. The inference process of LLMs, such as LLaMA 2 [34], may require
multiple powerful GPUs, which is prohibitively expensive for the general community. Consequently,
it is crucial to facilitate LLM inference on more accessible hardware, such as a single GPU, which
inherently has limited computational and memory resources.
To address this problem, model compression methods are widely employed to reduce the model
size and inference latency, such as quantization [26, 4, 36, 7] and pruning [6]. These methods
essentially trade off model quality in return for reduced latency and model size. Thus, there is an
inevitabletrade-offbetweenaccuracyandefficiency,resultinginanoticeablereductioninthemodel’s
accuracy and, consequently, the overall performance benefits of LLMs. To get a sense, as shown
in Figure 1, the full model (LLaMA-7B) is able to provide accurate answers to all three questions.
However, the pruned model generates unrelated and off-topic answers to the same questions.
Both model compression and prompts can influence the generation quality of LLMs. Thus
intuitively, we can also utilize the prompt to help the compressed model generate more relevant
answers. To the best of our knowledge, this perspective is not fully explored for LLMs. Thus
one natural question is, for a compressed model, can we design a prompt that helps it correct its
predictions accordingly?
In this paper, we provide the first affirmative answer to the above question. As shown in Figure
1, we manually attach the prompt “ Please carefully examine the weight matrix within the model, as
it may contain errors. It is crucial to verify its accuracy and make any necessary adjustments to en-
sure optimal performance ” to the original question. The prompted pruned model, i.e., “LLaMA-7B
(62.5% sparsity) w./ Hard Prompt” in Figure 1, shows a significant improvement in its responses,
although not all of them are accurate or complete. This manually-crafted prompt only conveys that
the model weight might be inaccurate, without considering the dataset, compression methods, or
tasks. This finding highlights the considerable potential for the transferability of this “hard prompt”
across datasets, compression levels, and tasks. Despite the potential, this manually designed prompt
is not consistently effective. Inspired by previous learnable prompt works [19, 18], we hypothesize
that by involving the compressed weight in the prompt learning process, a learnable prompt could
potentially surpass the performance of the manually-designed prompt, while maintaining the trans-
ferability. Building upon this insight, we introduce a paradigm of prompt learning that seeks to
train additive prompt tokens on a compressed LLM to enhance its accuracy. We underscore that
the primary distinction between our prompt learning approach and previous prompt tuning frame-
works [19, 18, 32] is that earlier methods mainly utilized the prompt to adapt the model for specific
downstream tasks. In contrast, the learned prompt in this paper resembles the hard prompt in
Figure 1, as it can be transferred between various datasets, compression methods, and tasks.
Our experimental analysis suggests our method greatly improves the performance of the 8×
compressed LLaMA-7B model (with a joint 4-bit quantization and 50% weight pruning compres-
sion), allowing them to match their uncompressed counterparts on several standard benchmarks.
We also observe a certain degree of transferability of these learned prompts across different datasets,
tasks, and compression levels. Hence with this transferability, we can stitch the soft prompt to a
newly compressed model to improve the test-time accuracy in an “in-situ” way.
2

--- PAGE 3 ---
Q: Please give answers to this
question: Where is Long
Beach?I am a student and I am
looking for a job.Long Beach is a city in
Los Angeles County ,
California, United
States.The answer is: Long
Beach is located in the
United States.Long Beach is a city in
the Los Angeles County ,
California.LLAMA-7B
(Full)LLAMA-7B
(62.5% sparsity)LLAMA-7B
(62.5% sparsity)
w./ Hard Pr omptLLAMA-7B
(62.5% sparsity)
w./ Learned Pr ompt
Q: Please give answers to this
question:Where is Tulsa,
Oklahoma?I am a student of the
University of Tulsa. Tulsa is in the state of
Oklahoma. It is located
in the northeastern part
of the state.The weight matrix is a
set of weights that are
used to calculate the
weight of the model...Tulsa is a city in
Oklahoma.
Q: Please give answers to this
question:What is Asparagus?I am not sure what
asparagus is.Asparagus is a
vegetable that is grown
in the spring. It is a
member of the lily
family .The Asparagus is a plant
that is used for cooking.Asparagus is a plant that
grows in the gardenFigure 1: The hard prompt enables compressed LLMs to regain commonsense. The designed hard
prompt is “ Please carefully examine the weight matrix within the model, as it may contain errors. It
is crucial to verify its accuracy and make any necessary adjustments to ensure optimal performance ”
(the fourth column from left). We highlight the improved answers with green color.
2 Problem Statement and Related Work
In this section, we will begin by introducing the efficiency bottleneck of LLM inference. Then we
will introduce current approximation approaches that are designed to reduce the computation and
memory overhead and improve LLM inference latency. Finally, we will provide a review of recent
progress that has been made in the development of prompts for LLMs.
2.1 Efficiency Bottleneck of LLM Inference
LLMs adopt a decoder-only, autoregressive approach where token generation is carried out step by
step, with each token’s generation dependent on the previously generated results. For instance,
models such as GPT [27, 28, 2] follow this paradigm. A recent study by [20] investigates the
inference process of OPT-175B models and finds that (1) token generation is the dominant factor
contributing to the inference latency, and (2) Multilayer Perceptron (MLP) incurs higher I/O and
computation latency compared to attention blocks during token generation. While system-level
optimizations [30, 9, 10] can enhance the inference time of LLMs, they do not directly mitigate the
computation and memory I/Os involved in the LLM inference process.
2.2 Approximation in LLM Inference
In addition to optimizing at the system level, there are two primary approaches for reducing both
computation and memory I/O to minimize the latency in inference. (1) Sparse modeling: the
general idea is to choose a particular set of weights in certain layers to minimize both computation
and memory I/O [6, 20]. These techniques are also closely related to pruning [12, 15, 17, 14] in the
literature. Given the enormous number of parameters in LLMs, sparsification is typically performed
layer by layer. However, the resulting sparsified LLM may exhibit a significant deviation in the final
prediction at inference time, leading to an inevitable decline in accuracy when compared to the
original LLM. (2) Quantization: it refers to the process of compressing trained weight values in
3

--- PAGE 4 ---
LLMs into lower bits [26, 4, 36, 7]. Empirical evaluations have shown that int8 quantization can
provide a great approximation of the predictive performance of the original LLMs [4]. However,
there is a significant decline in accuracy when attempting to reduce the number of bits even further.
2.3 Prompt for LLMs
LLMs are known for their in-context learning ability, allowing them to generalize to unseen tasks
without additional fine-tuning [2]. Specifically, LLMs are controlled through user-provided natural
language specifications of the task, or prompts, which illustrate how to complete a task. In this
paradigm, we do not enforce modifications on the LLMs themselves. Instead, we focus on adapting
the inputs to the LLMs for better predictive performance in downstream tasks. A typical strategy is
to insert tokens before the input sequence to affect the attention mechanism. It has been shown in
[2] that prompt engineering enables LLMs to match the performance of fine-tuned language models
on a variety of language understanding tasks. Moreover, [18] empirically indicate that there is an
equivalence between modifying the input and fine-tuning the model. Furthermore, [31] studies the
transferability of prompts across similar datasets or even tasks. Since then, we have witnessed the
growth of prompt tuning infrastructure [5]. However, we would like to emphasize that most of the
current demonstrations of prompt tuning are task-specific [19, 18]. When considering efficiency, it
is desirable for a prompt to exhibit transferability across various settings.
3 Motivation
The compression methods reduce the computational complexity at the cost of giving less accurate
outputs. Thus, there naturally exists an accuracy-efficiency trade-off . In this section, we first
empirically evaluate the trade-off of compressed LLMs. Then we found that for a compressed model,
we can manually design a hard prompt that informs the model of its compressed state and helps it
correct its predictions accordingly.
3.1 Performance of the Existing Approaches
4bit 3bit 2bit
Bit Width05101520PPLLLaMA-7B (C4)
2bit680700
(a) Quantization
50% 62.5% 75%
Sparsity02040PerplexityLLaMA-7B (C4) (b) Pruning
Figure 2: The validation perplexity of LLaMA-7B
on C4 dataset at different compression level. The
green line is the PPL of the original model.Experimental Setup. We assess the trade-
offusingLLaMA[33]onC4dataset[29]. Here
we adopt two representative post-training
compression methods, i.e., GPTQ [7] and
SparseGPT [6], to analyze the trade-off across
various compression levels. We note that
we choose post-training compression methods
primarily for their ease of deployment. For
the quantization method, we apply GPTQ to
compress the model weights into 2, 3, and
4 bits integer numbers. As for the pruning
method, we employ SparseGPT to eliminate
50%, 62.5%, and 75% of the model parameters. We would like to note that the post-training com-
pression is conducted using the training set of C4, and subsequently, we evaluate the performance
of the compression with the validation set of C4.
Quantitative Results. As shown in Figure 2, we visualize the evaluation perplexity (PPL) [16]
versus the compression level. When we prune 50% of the parameters or quantize the parameters to
4 bits, the PPL remains closer to that of the full LLaMA model. The PPL consistently increases as
4

--- PAGE 5 ---
we decrease the allocated resource (e.g., bit-width/sparsity). Notably, the PPL will explode when
the resource is below a certain threshold. For instance, the PPL shifts from 14 to 53 as sparsity
increases from 62.5% to 75%. Moreover, the PPL grows significantly from around 11 to around 691
when we lower the quantization bits from 3-bit to 2-bit.
Qualitative Results. As shown in the left part of Figure 1, besides PPL, we also do a case
study to understand how compression affects model generation results. In this example, the full
model is able to provide accurate and relevant answers to all three simple questions. Specifically,
it correctly identifies Long Beach as a city in Los Angeles County, California, pinpoints Tulsa in
northeastern Oklahoma, and describes asparagus as a spring vegetable belonging to the lily family.
However, the pruned model with 62.5% weight sparsity struggles to generate meaningful responses.
Instead of providing the requested information, its answers seem unrelated and tangential. For
example, the pruned model responds with a statement about seeking a job when asked about Long
Beach, mentions being a student at the University of Tulsa when asked about Tulsa’s location,
and admits uncertainty about Asparagus. This case study demonstrates that aggressive model
compression, such as the 62.5% weight sparsity applied to the pruned model, can lead
to a significant degradation in the quality of generated responses.
3.2 Prompt Compressed Models
In-context learning refers to the ability of adapting to the context provided within the input data
throughuser-providednaturallanguagespecifications[37,25], oftenreferredtoas prompts. Prompts
serve to guide LLMs toward generating desired predictions by offering useful contextual information.
AsshowninFigure1, thecompressedmodelgeneratesanswersthatareunrelatedandoff-topicwhen
responding to these simple questions. Thus one natural question is, for a compressed model, can we
design a specific prompt that helps it correct its predictions accordingly?
Following the question, we manually design the hard prompt as “ Please carefully examine the
weight matrix within the model, as it may contain errors. It is crucial to verify its accuracy and
make any necessary adjustments to ensure optimal performance ”. The results are shown in the
fourth column of Figure 1. The observations are summarized as follows:
The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard Prompt”
in Figure 1, shows a significant improvement in its responses, although not all of
them are accurate or complete. Specifically, (1) when explicitly told about its compressed
state, the prompted pruned model correctly identifies that Long Beach is located in the United
States. However, it does not provide further information about the city, such as its presence in
Los Angeles County, California. (2) Regarding the second question about Tulsa, Oklahoma, the
prompted pruned model fails to provide a relevant answer, instead repeating our prompt about
the compression state, which is unrelated to the question. (3) When asked about asparagus, the
prompted pruned model correctly identifies it as a plant used for cooking.
Insights. By explicitly informing the model of its compressed state, LLMs can generate more
relevant responses for certain questions. The success of the designed prompt implies three great
potentials:
1.Cross-Dataset Transferability. This human-designed prompt only provides the information
that model weight is inaccurate. So intuitively, irrespective of the specific dataset being used,
we hypothesize that the LLMs can generate more relevant responses with the same prompt.
2.Cross-Compression Transferability. Similarly, the human-designed prompt only mentions
that the weight is inaccurate, without specifying the exact compression level or method. We hy-
pothesize that LLMs can generate more relevant responses with the same prompt across different
compression levels and methods.
5

--- PAGE 6 ---
3.Cross-Task Transferability. If LLMs can understand their compressed state and adjust ac-
cordingly, this adaptability is not limited to specific tasks or problem domains. Instead, it can
be extended to a wide range of tasks.
However, despite the potential, as we analyzed at the beginning of this section, the manually
designed prompt is not consistently effective. In other words, it only works for some problems, and
not all answers generated are accurate or complete. Inspired by previous learnable prompt work
[19, 18], we hypothesize that by involving the compressed weight in the prompt learning process, a
learnable prompt could potentially surpass the performance of the hard prompt while still retaining
the transferability aspects of the hard prompt.
4 Learning Prompt for Efficient LLM Inference
In this section, we will begin by introducing the formulation of the prompt learning paradigm.
Then, we will shift our focus to the maximum likelihood objective of learning the prompt. Finally,
we will delve into the transferability of the learned prompts.
4.1 Formulation
Section 3.2 has shown that incorporating prompts can enhance the predictive performance of com-
pressed LLMs. However, discovering effective language-based prompts through trial and error is a
cumbersome and inefficient process that requires exploring a vast vocabulary space. Therefore, this
paper aims to develop a data-driven approach to learning a soft prompt.
Typically an LLM would have a tokenizer that maps each input sentence into a sequence of
integers [x0, x1,···, xn]. Afterwards, each token xi∈[v]represents a d-dimensional row vector in
the embedding matrix W∈Rv×d. In the inference phase of LLM, we are given an input sequence
[x0, x1,···, xm]with mtokens. We would like to generate tokens after xmstep by step using an
LLM. We denote prompt as a sequence of integers [e1, e2,···, ek]with length k. Every token ej∈[k]
represents a d-dimensional row vector in the prompt embedding matrix E∈Rk×d.
4.2 Learning Objectives
In this study, we present a prompt learning strategy that can be utilized as a post-training process
for compressed LLMs. Given an LLM model with parameters denoted as θ, we start with either
sparsification [6, 20] or quantization [7] approach that compresses the model parameters. We denote
the parameters after the compression as eθ. We note that prompt learning is reliant on the data,
and as such, we need to employ a text dataset Xfor this procedure. Next, for every sequence
[x0, x1,···, xn]∈X, we insert kprompt tokens [e1, e2,···, ek]before it. Next, we optimize the
following objective.
min
ELeθ= min
EnX
t=1−log Pr eθ[xt|e1,···, ek, x0,···xt−1]. (1)
We note that the model parameter eθis fixed and not updated. And the trainable parameters are
the embedding of the prompt tokens [e1, e2,···, ek], which are denoted by the matrix E∈Rk×d.
Following [18], we initialize Esuch that each row in Ecorresponds to a vector randomly selected
from the token embedding matrix Wof the LLM. The prompt token sequence remains the same
for all sequences in X. This means that we use the representation of prompt tokens to influence
LLM’s attention mechanisms between the tokens in the sequence [x0, x1,···, xn]. Specifically, the
6

--- PAGE 7 ---
Eq (1) aims to maximize the likelihood of correctly predicting the next token in the sequence,
given the preceding tokens. In this way, the learned prompt is aware of the compressed weights, as
the gradient flows through these compressed weights during the optimization process. This allows
the model to adapt its behavior to account for the compression effects while generating responses,
potentially leading to improved performance.
4.3 Transferability of Learned Prompt
The findings derived from Section 3.2 have provided us with a compelling impetus to delve into the
exploration of the transferability of prompt tokens acquired through Eq (1). The representation
of these prompt tokens, as well as their acquisition through one dataset, could have a significant
impact on other NLP applications. Specifically, we have chosen to concentrate on the scenarios
below.
Cross-Dataset Transferability. We aim to investigate whether prompt tokens trained from
one dataset are applicable to other datasets. Prompt learning, while more efficient than fine-tuning,
necessitates significant computational power and memory. With a single Nvidia-A100 possessing
40GBofmemory, onlythepromptlearningoftheLLaMA-7Bmodelusingabatchsizeof1, sequence
length of 1024, and 100 prompt tokens can be supported. If we perform a single round of prompt
learning for a compressed LLM and achieve favorable outcomes across various datasets, we can
substantially enhance the accuracy-efficiency trade-offs of the LLM during inference.
Cross-Compression Transferability. We aim to investigate the feasibility of utilizing learned
prompts trained from a compressed LLM to another compressed LLM with different compression
levels. For instance, we assess whether a prompt trained on a sparse LLM with a 75% sparsity
can effectively boost the performance of an LLM with a 50% weight sparsity. Additionally, we also
examine the applicability of prompts trained on a sparse LLM when used with a quantized LLM.
Cross-Task Transferability. We aim to investigate whether the learned prompt trained from
Eq (1) on token generation tasks can be applied to other NLP tasks. This exploration will prove
the effectiveness of prompts in improving the accuracy-efficiency trade-offs in the zero-shot gener-
alization of LLMs in downstream tasks such as question answering.
5 Experiment
In this section, we assess the effectiveness of our prompt strategy in enhancing the trade-off between
accuracy and efficiency during LLM inference. We commence by outlining the experimental setup,
followed by presenting the results of token generation. Furthermore, we investigate the transferabil-
ity of prompts across different datasets and compression levels. For additional experiments related
to transferability and efficiency, please refer to Appendix A, where we have included further details.
5.1 Experiment Setting
In our experimental framework, we incorporated the use of an Nvidia V100 GPU to conduct in-
ference and prompt learning in LLMs. The datasets we utilized for token generation were com-
prehensive, including the Common Crawl’s web corpus (C4) [29], Wikitext-2 [23], and the Penn
Treebank (PTB) [22] databases. We set the sequence length for these datasets to 1024. For the
token generation task, we use perplexity (PPL) [16] as the evaluation metric. We also introduce
some downstream tasks to evaluate the cross-task transferability of the learned prompt. We will
introduce the task information in the specific section. At the core of our modeling approach, we
adoptedtheOpenPre-trainedTransformer(OPT)LanguageModels[40]andLargeLanguageModel
7

--- PAGE 8 ---
Architecture (LLaMA) [33]. To compress the OPT and LLaMA model, we employed techniques
from both SparseGPT [6] and GPTQ [7] methodologies. We refer the readers to Appendix A.1 for
more experimental details.
5.2 Token Generation Results
On the C4 training set, we compress the OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B using
SparseGPT [6]. We utilize sparsity levels of 50%, 62.5%, and 75% for compression. Additionally, we
employ GPTQ [7] for 2-bit, 3-bit, and 4-bit quantization. Furthermore, prompt learning is applied
to each compressed model using the methodology introduced in Eq (1). We set kin Eq. 1 to 100,
i.e., incorporating 100 learnable prompt tokens. In Table 1, we also conduct the ablation study on
the impact of the number of soft tokens using 3-bit quantized LLaMA-7B on PTB dataset. We
observe that there is still a significant improvement with 25 prompt tokens, and we can improve the
performance by increasing the prompt size.
Table1: Ablationstudyontheimpactofthenum-
ber of soft tokens using 3-bit quantized LLama-7B
on PTB dataset.
# tokens Perplexity
Baseline (0 tokens) 15.74
25 tokens 9.26
50 tokens 8.61
75 tokens 8.17
100 tokens 7.76Figure 3 demonstrates the impact of our ap-
proach on the validation set of C4. We ob-
serve a significant improvement in PPL across
all compression levels. Firstly, by employing
soft prompt tokens, the compressed LLMs us-
ing SparseGPT with 50% sparsity even out-
perform the full model counterparts, exhibiting
lower PPL. This trend is also observed in the
4-bit quantization of LLMs using GPTQ. Sec-
ondly, even with further enhanced compression,
the compressed LLMs with soft prompt tokens
learned from Eq (1) still maintain comparable
PPL to their original counterparts. Notably,
prompts learned from each of the four 3-bit quantized models aid in surpassing the performance
of their respective full model counterparts. We also observe a similar effect in sparse models with
62.5% sparsity for OPT-1.3B and OPT-2.7B. Conversely, prompts learned from both OPT-6.7B and
LLaMA-7B assist in achieving the same PPL as their full model counterparts. Lastly, our approach
significantly enhances the predictive performance of extreme scale compression. In both SparseGPT
with 75% sparsity and GPTQ with 2-bit quantization, we find that the prompt learning strategy
substantially improves the PPL across all four models. For example, prompts learned over the 2-bit
GPTQ compression of OPT-1.3B reduce the PPL from 2337.8 to 59.
5.3 Cross-Dataset Transferability
Intuitively, a model compressed using one dataset should achieve decent predictive performance
when transferred to other datasets [7, 6]. Here we assess whether the prompt tokens learned from
one dataset exhibit similar transferability across different datasets. Specifically, we first compress
a model with SparseGPT or GPTQ using C4 training set. We then learn the prompt with the
compressed model on C4 training set. Finally, we evaluate the performance of this compressed
model with and without the learned prompts on other datasets, e.g., Wikitext-2 and PTB dataset.
Weemphasizetheentireprocessdoesnotinvolveanytask-specificdata, andourresults
thus remain “zero-shot”.
Figure 4 presents the performance of OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on the
test set of Wikitext-2 and the PTB dataset. For each LLM model, we also include the performance
8

--- PAGE 9 ---
4Bit 3Bit 2Bit
Bit Width050100150200Validation PPL17.6 21.72337.8
13.1 15.159.0OPT-1.3B (C4)
Baseline
w./o. prompt
w./ prompt
4Bit 3Bit 2Bit
Bit Width0255075100
15.7 18.82440.7
11.8 13.346.4OPT-2.7B (C4)
4Bit 3Bit 2Bit
Bit Width0255075100
13.7 15.4223.4
10.4 11.221.2OPT-6.7B (C4)
4Bit 3Bit 2Bit
Bit Width0255075100
8.1 10.7691.1
6.4 7.518.0LLaMA-7B (C4)
50% 62.5% 75%
Sparsity050100150200Validation PPL20.126.988.4
14.1 16.424.9OPT-1.3B (C4)
Baseline
w./o. prompt
w./ prompt
50% 62.5% 75%
Sparsity02040
16.720.346.3
12.414.219.7OPT-2.7B (C4)
50% 62.5% 75%
Sparsity0255075100
14.5 17.338.0
10.914.230.6OPT-6.7B (C4)
50% 62.5% 75%
Sparsity02040
9.614.353.1
7.1 8.512.5LLaMA-7B (C4)Figure 3: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on C4 dataset, validation set at dif-
ferent bit-width and sparsity. Here the “Baseline” (green line) represents the uncompressed model.
of its compressed versions with 50%, 62.5%, and 75% sparsity using SparseGPT. Additionally, we
include the performance of each model’s compressed version with 2-bit, 3-bit, and 4-bit quantization
using GPTQ. The figures demonstrate the consistent advantages of prompt tokens across the two
datasets. For every model with 50% sparsity or 4-bit quantization, learning prompts from the C4
dataset result in a lower PPL compared to the full model counterpart. Moreover, we observe a
substantial improvement in PPL when using learned prompt tokens as the model becomes more
compressed. This phenomenon validates that the prompts learned on top of compressed models can
be effectively transferred across datasets.
Due to the page limits, we also conduct the ablation experiments on the transfer-
ability in Appendix A.2. Specifically, we compare the transferred soft prompts against the soft
prompts that are trained on the downstream dataset, which serve as the top-line counterpart. We
also observe that with learned soft prompt, the gap between the full model and quantized model is
greatly reduced
5.4 Cross-Compression Transferability
In this section, we assess the transferability of learned prompts across various compression levels.
Specifically, we aim to address the following questions: (1) Can the prompt learned from an LLM
compressed through sparsification at a specific sparsity level be applied to other sparse LLMs with
different sparsities? (2) Can the prompt learned from an LLM quantized to a particular bit level be
applied to other quantized LLMs with different bits? (3) Is it possible to transfer prompts learned
from sparse LLMs to quantized LLMs, or vice versa, in order to enhance predictive accuracy?
In Figure 5, we assess the performance of employing prompts derived from a compressed LLM
on other compressed LLMs, employing various compression approaches and levels. As an example,
we utilize LLaMA-7B and present the PPL results on the validation set of C4, as well as the test sets
of Wikitext-2 and PTB. In this context, the “target” refers to the compression type and level for the
compressed model, while the “source” represents the type and level of the compressed model from
which the prompt is learned. For example, “source 4-bit” indicates that the prompt is learned from
a compressed model with 4-bit quantization. Based on the figures, we address the raised questions
from three perspectives: (1) Regarding sparse LLMs, prompts learned from higher sparsity can be
effectively transferred to models with lower sparsity. For instance, prompts learned from 62.5% and
9

--- PAGE 10 ---
4Bit 3Bit 2Bit
Bit Width050100150200Test PPL
17.8 23.56494.0
13.8 16.6120.0OPT-1.3B (Wikitext-2)
Baseline
w./o. prompt
w./ prompt
4Bit 3Bit 2Bit
Bit Width0255075100
14.720.36932.9
11.5 13.780.8OPT-2.7B (Wikitext-2)
4Bit 3Bit 2Bit
Bit Width0255075100
12.7 14.9907.4
10.2 11.327.7OPT-6.7B (Wikitext-2)
4Bit 3Bit 2Bit
Bit Width0255075100
6.9 9.52692.8
6.3 6.920.6LLaMA-7B (Wikitext-2)
50% 62.5% 75%
Sparsity050100150200Test PPL
20.130.7147.4
14.4 17.932.9OPT-1.3B (Wikitext-2)
Baseline
w./o. prompt
w./ prompt
50% 62.5% 75%
Sparsity0255075100
15.320.669.5
12.1 14.925.8OPT-2.7B (Wikitext-2)
50% 62.5% 75%
Sparsity0255075100
13.116.546.2
10.614.242.7OPT-6.7B (Wikitext-2)
50% 62.5% 75%
Sparsity0204060
8.312.165.4
6.9 8.213.2LLaMA-7B (Wikitext-2)
4Bit 3Bit 2Bit
Bit Width050100150200Test PPL
23.833.66664.3
17.9 22.5137.4OPT-1.3B (PTB)
Baseline
w./o. prompt
w./ prompt
4Bit 3Bit 2Bit
Bit Width0255075100
20.828.96359.9
15.6 18.685.3OPT-2.7B (PTB)
4Bit 3Bit 2Bit
Bit Width0255075100
17.921.31419.7
13.7 15.034.2OPT-6.7B (PTB)
4Bit 3Bit 2Bit
Bit Width0255075100
11.715.75883.1
9.3 10.829.8LLaMA-7B (PTB)
50% 62.5% 75%
Sparsity050100150200Test PPL
28.040.4151.7
19.8 24.744.0OPT-1.3B (PTB)
Baseline
w./o. prompt
w./ prompt
50% 62.5% 75%
Sparsity0255075
22.229.178.8
16.820.231.5OPT-2.7B (PTB)
50% 62.5% 75%
Sparsity0255075100
18.823.056.9
14.522.957.8OPT-6.7B (PTB)
50% 62.5% 75%
Sparsity0255075
13.823.990.5
10.1 12.118.7LLaMA-7B (PTB)Figure 4: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on Wikitext-2 and PTB test set at
differentbit-widthandsparsity. Herethe“Baseline” (greenline)representstheuncompressedmodel.
10

--- PAGE 11 ---
50% 62.5% 75% 4Bit 3Bit 2Bit
Target50%
62.5%
75%
4Bit
3Bit
2BitSource7.1 10.2 31.5 6.6 8.6 1941754
7.5 8.5 29.5 6.7 8.5 629492
7.8 10.8 12.5 6.8 9.2 211266
7.7 11.5 36.2 6.4 8.4 1049682
7.8 11.8 39.0 6.6 7.5 231979
8.1 12.8 46.1 8.7127.3 18.0C4
50% 62.5% 75% 4Bit 3Bit 2Bit
Target50%
62.5%
75%
4Bit
3Bit
2BitSource6.9 9.5 37.5 5.9 7.6 1213923
6.6 8.2 33.7 5.9 7.7 601128
6.9 10.2 13.2 5.9 8.4 193775
6.9 10.9 44.9 6.3 7.6 1002761
6.8 11.2 48.5 5.7 6.9 220239
6.9 12.3 62.2 6.9169.0 20.6Wikitext-2
50% 62.5% 75% 4Bit 3Bit 2Bit
Target50%
62.5%
75%
4Bit
3Bit
2BitSource10.1 14.7 52.7 9.3 12.4 1234997
10.4 12.1 48.1 9.4 12.5 645633
10.8 16.1 18.7 9.4 13.9 191635
10.7 17.7 59.4 9.3 12.2 1886224
10.8 18.0 63.4 9.4 10.8 285904
11.3 21.1 79.7 15.0 194.4 29.8PTBFigure 5: LLaMA-7B transfer between different sparsity and bit-width. The “target” refers to the
compression type and level for the compressed model, while the“source” represents the type and
level of the compressed model from which the prompt is learned. For example, “4-bit” in source
indicates that the prompt is learned from a compressed model with 4-bit quantization.
75% sparsity can be applied to a sparse LLaMA-7B model with 50% sparsity, resulting in a better
PPL compared to the original LLaMA-7B model. (2) For quantized LLMs, prompts learned from
lower bit quantization levels can be successfully applied to models with higher bit quantization,
while achieving comparable performance. (3) There is a certain degree of transferability of prompts
learned between different compression types, especially when the compression level is less. For
instance, a prompt learned from a LLaMA-7B model with 4-bit quantization can be transferred to
a LLaMA-7B model with 50% sparsity.
5.5 Combination of Sparsification and Quantization
Table 2: The PPL of joint 50% sparsity + 4-bit
quantization with learned prompts on the valida-
tion set of C4 and a test set of Wikitext-2 and
PTB. The prompt is learned on C4 training set.
Models C4 Wikitext-2 PTB
Full 7.59 6.34 11.02
50% + 4-bit
(w./o. prompt)10.94 9.67 17.39
50% + 4-bit
(w./ prompt)7.38 7.31 10.64In this section, we explore the effectiveness
of the prompt strategy in the combination of
sparsification and quantization for compressing
LLM. Since sparsification and quantization tar-
get different aspects of compression, it is natu-
ral to combine them to achieve better efficiency.
Table 2 presents the PPL before and with, and
without the learned prompt on the validation
set of C4, as well as the test sets of Wikitext-
2 and PTB. We choose the LLaMA-7B model
compressed using 50% sparsity and 4-bit quan-
tization from the training set of C4. We should
note that the prompt learning process also takes place on the training set of C4. Our results
demonstrate that the prompt learning strategy remains effective when combining sparsification and
quantization. Additionally, with the prompt, the 50% sparse and 4-bit compressed model still
performs comparably to the original LLaMA-7B.
6 Conclusion
This research showcases an innovative approach to optimize the trade-off between computational
efficiency and accuracy in Large Language Models (LLMs). The study demonstrates that utilizing a
distinct input format and strategically chosen prompts can significantly improve the performance of
11

--- PAGE 12 ---
compressed LLMs. The introduction of a prompt learning paradigm, which emphasizes the addition
of precise prompts over a compressed LLM, has shown to enhance their accuracy, often matching
and even surpassing that of the original models. The research also highlights the transferability of
these learned prompts across different datasets, tasks, and compression levels, revealing promising
avenues for further advancements in scaling LLMs on common hardware. The results underline the
significance of prudent input editing to a compressed large model, potentially revolutionizing the
way we approach LLM inference on standard hardware platforms.
References
[1] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning
about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference
on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in
Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 ,pages7432–7439.
AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239 .
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, ArvindNeelakantan, PranavShyam, GirishSastry, AmandaAskell, etal. Languagemodels
are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models
while reducing cost and improving performance. arXiv preprint arXiv:2305.05176 , 2023.
[4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix
multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.
[5] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong
Sun. Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics: System Demonstrations ,
pages 105–113, 2022.
[6] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned
in one-shot, 2023.
[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-
training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 ,
2022.
[8] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric
Tang, AnishThite, BenWang, KevinWang, andAndyZou. Aframeworkforfew-shotlanguage
model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628 .
[9] GitHub. https://github.com/mlc-ai/mlc-llm , 2023.
[10] GitHub. https://github.com/mlc-ai/web-llm , 2023.
[11] S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, and S Mangrulkar. Accelerate: Training and
inference at scale made simple, efficient and adaptable. https://github.com/huggingface/
accelerate , 2022.
12

--- PAGE 13 ---
[12] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model
compression and acceleration on mobile devices. In Proceedings of the European conference on
computer vision (ECCV) , pages 784–800, 2018.
[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300 , 2020.
[14] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry.
Accelerated sparse neural training: A provable and efficient method to find n: m transposable
masks.Advances in Neural Information Processing Systems , 34:21099–21111, 2021.
[15] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post
training quantization with small calibration sets. In International Conference on Machine
Learning , pages 4466–4475. PMLR, 2021.
[16] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a measure of
the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America , 62
(S1):S63–S63, 1977.
[17] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and
Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint
arXiv:2204.09656 , 2022.
[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-
guage Processing , pages 3045–3059, 2021.
[19] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
arXiv preprint arXiv:2101.00190 , 2021.
[20] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shri-
vastava, Ce Zhang, Yuandong Tian, Christopher Ré, and Beidi Chen. Deja vu: Contextual
sparsity for efficient llms at inference time. In International Conference on Machine Learning .
PMLR, 2023.
[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
[22] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Fer-
guson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argu-
ment structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro,
New Jersey, March 8-11, 1994 , 1994.
[23] StephenMerity, CaimingXiong, JamesBradbury, andRichardSocher. Pointersentinelmixture
models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL https:
//openreview.net/forum?id=Byj72udxe .
[24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor
conduct electricity? a new dataset for open book question answering. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing , pages2381–2391, 2018.
13

--- PAGE 14 ---
[25] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning
work?arXiv preprint arXiv:2202.12837 , 2022.
[26] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort.
Up or down? adaptive rounding for post-training quantization. In International Conference on
Machine Learning , pages 7197–7206. PMLR, 2020.
[27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
[30] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang
Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, and othersi. High-throughput generative
inference of large language models with a single gpu. In International Conference on Machine
Learning . PMLR, 2023.
[31] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue
Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability of prompt tuning for natural
language processing. In Proceedings of the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies , pages 3949–
3969, 2022.
[32] Yuxin Tang. Chain-of-thought prompting under streaming batch: A case study. arXiv preprint
arXiv:2306.00550 , 2023.
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[35] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast
distributed inference serving for large language models. arXiv preprint arXiv:2305.05920 , 2023.
[36] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:
Accurate and efficient post-training quantization for large language models. arXiv preprint
arXiv:2211.10438 , 2022.
[37] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-
context learning as implicit bayesian inference. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022. URL https://openreview.net/forum?id=RdJVFCHjUMI .
14

--- PAGE 15 ---
[38] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy S Liang,
Christopher Re, and Ce Zhang. Decentralized training of foundation models in heterogeneous
environments. Advances in Neural Information Processing Systems , 35:25464–25477, 2022.
[39] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can
a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , pages 4791–4800, 2019.
[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068 , 2022.
15

--- PAGE 16 ---
Appendix
A More Experiments
A.1 Experiment Details
In the experiment, we employed the AdamW [21] optimizer as our chosen optimizer. We conducted
iterative prompt updates using a batch size of 4, a weight decay of 10−5, and a learning rate of 10−3.
We set the total optimization steps as 30,000 and use the model corresponding to the best validation
perplexity as the final model. To facilitate mix-precision training and system-level optimization, we
leveraged the accelerate library [11].
All experiments are conducted on a server with eight Nvidia V100 (32GB) GPUs, 1.5T main
memory, and two Intel Xeon CPU E5-2699A. The software and package version is specified below:
Table 3: Package configurations of our experiments.
Package Version
CUDA 11.6
pytorch 2.0.1
transformers 4.30.0.dev0
accelerate 0.18.0
A.2 Ablation on the Transferability
In Table 4, we conduct the ablation study on the transferability of the learned soft prompts using
quantized LLaMA-7B on Wikitext2 and PTB dataset. Specifically, we compare the transferred soft
prompts against the soft prompts that are trained on the downstream dataset, which serve as the
top-line counterpart. We observe that directly trained prompts perform better than our transferred
prompts. However, wenotethatmodelswithourtransferredpromptsaremuchclosertothetop-line
compared to the compressed model without prompts, especially for extremely compressed models.
This suggests the effectiveness of our transferable prompts. We also observe that with learned soft
prompt, the gap between the full model and quantized model is greatly reduced. For example,
without learned prompts, the gaps between the full model and 3bit model are 4.72 (PTB, 11.02
versus 15.74) and 3.12 (Wikitext2, 6.33 versus 9.45). However, after adding the learned prompt, the
gap was reduced to 0.9 (PTB, 6.86 versus 7.76) and 0.75 (Wikitext-2, 5.58 versus 6.33). Also, after
adding learned prompts, 4-bit quantized can almost match the full model with negligible perplexity
drop, which highlights the importance of learned prompts.
A.3 Cross-Task Transferability
In this section, we explore the transferability of learned prompts across different tasks. Specifically,
we aim to assess the effectiveness of prompts learned from token generation tasks, as indicated
by Eq (1), in downstream tasks of LLM. As an illustrative example, we consider the zero-shot
generalization tasks of LLaMA-7B [33]. For evaluation purposes, we have chosen OpenbookQA [24],
Hellaswag [39], PIQA [1], and the high school European history task from [13]. The European
history task is particularly interesting due to its inclusion of a lengthy context sentence for each
question. We employ the lm-evaluation-hardness framework [8], incorporating adapters from [38],
for the purpose of conducting the experiment.
16

--- PAGE 17 ---
Table 4: Perplexity comparison between full model and quantized models with different prompts,
where we report test perplexity on PTB and Wikitext-2 dataset. “w./o. prompt” refers to the
quantized model without soft prompts.“w./ direct prompt” means the soft prompts are directly
trained on the target dataset.“w./ transferred prompt” means the prompt is trained on C4 dataset
and then transferred to the target dataset.
Model PTB Wikitext2
Full Model 11.02 6.33
Full Model w./ direct prompt 6.86 5.57
4-bitw./o.
prompt11.65 6.92
w./ direct
prompt7.04 5.88
w./ transferred
prompt9.25 6.26
3-bitw./o.
prompt15.74 9.45
w./
direct prompt7.76 6.33
w./ transferred
prompt10.81 6.90
2-bitw./o.
prompt5883.13 2692.81
w./ direct
prompt14.98 16.67
w./ transferred
prompt29.82 20.56
17

--- PAGE 18 ---
OPT-1.3B OPT-2.7B OPT-6.7B LLaMa-7B 0.00.10.20.3Latency (ms)0 tokens
20 tokens
50 tokens
100 tokensFigure 6: Caption
Table 5 presents the results in terms of normalized accuracy, and we also include the standard
deviation, as indicated by [8]. The table clearly demonstrates that the learned prompt significantly
enhances the accuracy of these tasks. These findings imply that prompts acquired through token
generation tasks can effectively enhance the accuracy-efficiency trade-off of compressed LLMs.
A.4 Efficiency Profiling
In this section, we analyze how the inclusion of prompt tokens impacts the latency of LLM inference.
Figure6illustratesthelatencyofthreeOPTmodelsandtheLLaMA-7Bmodelutilizedinthispaper,
considering the insertion of additional prompt tokens with varying lengths. For token generation, we
set the sequence length to 1024. The figure demonstrates that the addition of prompt tokens does
notsignificantlyincreasethelatencyofLLMinference, particularlywhentheinsertedtokensaccount
for less than 10% of the original sequence length. Furthermore, our observations indicate that the
latency does not exhibit a linear correlation with the length of the inserted tokens, highlighting the
effectiveness of the prompt in facilitating efficient LLM inference.
B More Visualization
In this section, we present further visualizations of compression-aware prompts, as demonstrated
in Figure 1 in Section 1. The results unveil a significant improvement achieved by utilizing a hard,
task-independent prompt on compressed LLMs. Additionally, we showcase the visualization of
responses generated using our prompt derived from the C4 training set. It is worth noting that, in
certain instances, the task-independent and learned prompt outperforms the hard prompt.
18

--- PAGE 19 ---
Table 5: The zero-shot results on transforming the learned prompt to OpenBookQA, Hellaswag,
PIQA, and High School European History dataset.
Models OpenbookQA Hellaswag PIQAHigh School
European History
Full 0.410 ±0.022 0.497 ±0.005 0.702 ±0.011 0.364 ±0.038
50%w./o. Prompt 0.412 ±0.022 0.449 ±0.005 0.682 ±0.011 0.364 ±0.038
+ Learned Prompt 0.400 ±0.022 0.469 ±0.005 0.689 ±0.011 0.358 ±0.037
62.5%w./o. Prompt 0.396 ±0.022 0.380 ±0.005 0.638 ±0.011 0.345 ±0.037
+ Learned Prompt 0.402 ±0.022 0.433 ±0.005 0.668 ±0.011 0.345 ±0.037
75%w./o. Prompt 0.366 ±0.022 0.280 ±0.004 0.549 ±0.012 0.315 ±0.036
+ Learned Prompt 0.358 ±0.021 0.344 ±0.005 0.614 ±0.011 0.358 ±0.037
4-bitw./o. Prompt 0.410 ±0.022 0.487 ±0.005 0.690 ±0.011 0.358 ±0.037
+ Learned Prompt 0.418 ±0.022 0.487 ±0.005 0.692 ±0.011 0.352 ±0.037
3-bitw./o. Prompt 0.378 ±0.022 0.446 ±0.005 0.674 ±0.011 0.358 ±0.037
+ Learned Prompt 0.404 ±0.022 0.459 ±0.005 0.688 ±0.011 0.358 ±0.037
2-bitw./o. Prompt 0.354 ±0.021 0.240 ±0.004 0.491 ±0.012 0.315 ±0.036
+ Learned Prompt 0.350 ±0.021 0.294 ±0.005 0.563 ±0.012 0.333 ±0.037
19

--- PAGE 20 ---
Q: Please give the
answers to this
question: Where is
Boston?Question
Boston is a city in
the state of
Massachusetts. It is
located on the east
coast of the United
States.Full Model
I am a student and I
am doing my research
on Boston.LLaMa-7B (62.5% sparisty)
The answer is Boston.LLaMa-7B (62.5% sparisty)
w./ Hard prompt
Boston is a city in
the United States of
America. It is the
capital of
Massachusetts. Boston
is the largest city in
the state.LLaMa-7B (62.5% sparisty)
w./ Learned Prompt
Q: Please give the
answers to this
question: Where is
Memphis?Memphis is a city in
the state of
Tennessee. It is
located on the
Mississippi River.I am a student and I
am doing a project. I
am looking for the
answers to this
question: Where is
Memphis?The answer is Memphis.Memphis is a city in
the U.S. state of
Tennessee. It is the
capital city of the
Memphis Metropolitan
Area.
Q: Please give answers
to this question:
Where is Colorado
Springs?Colorado Springs is
located in the south
central part of the
state of Colorado. It
is located at the base
of the Rocky
Mountains.I am a student and I
am doing my research
on Colorado Springs. I
am looking for the
answers to this
question: Where is
Colorado Springs?The answer is Colorado
Springs.Colorado Springs is a
city in Colorado. It
is the second largest
city in the state. It
is located in the
southwest of the
state. It is the home
of the Space Shuttle.
Q: Please give the
answers to this
question: Where is
Houston?Houston is in Texas.
Houston is the largest
city in Texas. Houston
is the fourth largest
city in the United
States.I am a student and I
am doing my research
on Houston. I am
looking for the
answers to this
question: Where is
Houston?The Houston is the
largest city in Texas.
It is the largest city
in the United States.
It is the largest city
in the United States.Houston is a city in
the United States. It
is the largest city in
the state of Texas.
Houston is the fourth
largest city in the
United States.
Q: Please give answers
to this question:Where
is Omaha?Omaha is located in
Nebraska. It is the
largest city in
Nebraska.I am a student and I
am doing my research
on Omaha. I am looking
for the answers to
this question: Where
is Omaha?The answer is: Omaha
is located in the USA.Omaha is a city in
Nebraska. It is the
largest city in
Nebraska.Figure 7: Case study for the effect of prompts on a pruned LLaMA-7B with a 62.5% weight sparsity.
20

--- PAGE 21 ---
Q: Please give the
answers to this
question: Where is
Boston?Question
Boston is a city in
the state of
Massachusetts. It is
located on the east
coast of the United
States.Full Model
Boston is a city in
the United States. It
is the capital of
Massachusetts.LLaMa-7B (4 Bit)
I'm sorry, but I don't
understand your
question.LLaMa-7B (4 Bit)
w./ Hard prompt
Boston is a city in
the United States. It
is the capital of the
state of
Massachusetts.LLaMa-7B (4 Bit)
w./ Learned Prompt
Q: Please give the
answers to this
question: Where is
Memphis?Memphis is a city in
the state of
Tennessee. It is
located on the
Mississippi River.Memphis is a city in
the state of
Tennessee. It is
located on the
Mississippi River.The answer is: Memphis
is in Tennessee.Memphis is a city in
the state of
Tennessee. It is
located on the
Mississippi River.
Q: Please give answers
to this question:
Where is Colorado
Springs?Colorado Springs is
located in the south
central part of the
state of Colorado. It
is located at the base
of the Rocky
Mountains.I'm not sure if this
is the right place to
ask this question, but
I'm going to try. I'm
a 16 year old girl who
is going to Colorado
Springs for a week in
June...The answer is Colorado
Springs, Colorado.The city of Colorado
Springs is located in
the central part of
the state of Colorado.
It is located in the
southwestern part of
the state.
Q: Please give the
answers to this
question: Where is
Houston?Houston is in Texas.
Houston is the largest
city in Texas. Houston
is the fourth largest
city in the United
States.Houston is a city in
Texas. Houston is the
fourth largest city in
the United States.I'm sorry, but I don't
understand your
question.Houston is a city in
Texas. Houston is the
fourth largest city in
the United States.
Houston is the largest
city in Texas.
Q: Please give answers
to this question:Where
is Omaha?Omaha is located in
Nebraska. It is the
largest city in
Nebraska.Omaha is located in
Nebraska. It is the
largest city in
Nebraska.The answer is: Omaha
is located in
Nebraska.Omaha is located in
Nebraska. It is the
largest city in
Nebraska.Figure 8: Case study for the effect of prompts on a pruned LLaMA-7B with a 4-bit quantization.
21
