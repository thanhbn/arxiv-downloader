# 2303.13635.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2303.13635.pdf
# Kích thước tệp: 1187172 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1
Tối ưu hóa Hạng Thấp cho Học Sâu Hiệu quả: Tạo Cân bằng giữa
Kiến trúc Nhỏ gọn và Huấn luyện Nhanh
Xinwei Ou, Zhangxin Chen, Ce Zhu, Fellow, IEEE , và Yipeng Liu, Senior Member, IEEE
Tóm tắt —Mạng nơ-ron sâu đã đạt được thành công lớn trong
nhiều ứng dụng xử lý dữ liệu. Tuy nhiên, độ phức tạp tính toán cao
và chi phí lưu trữ làm cho học sâu khó được sử dụng trên các thiết bị
hạn chế tài nguyên, và không thân thiện với môi trường với chi phí
năng lượng lớn. Trong bài báo này, chúng tôi tập trung vào tối ưu hóa
hạng thấp cho các kỹ thuật học sâu hiệu quả. Trong miền không gian,
mạng nơ-ron sâu được nén bằng xấp xỉ hạng thấp của các tham số mạng,
điều này trực tiếp giảm yêu cầu lưu trữ với số lượng tham số mạng
nhỏ hơn. Trong miền thời gian, các tham số mạng có thể được huấn luyện
trong một vài không gian con, cho phép huấn luyện hiệu quả để hội tụ
nhanh. Nén mô hình trong miền không gian được tóm tắt thành ba
loại: phương pháp tiền huấn luyện, phương pháp tiền thiết lập và
phương pháp nhận thức nén, tương ứng. Với một loạt các kỹ thuật
tích hợp được thảo luận, như cắt tỉa thưa thớt, lượng tử hóa và mã hóa
entropy, chúng ta có thể kết hợp chúng trong một khung tích hợp với
độ phức tạp tính toán và lưu trữ thấp hơn. Bên cạnh tóm tắt những
tiến bộ kỹ thuật gần đây, chúng tôi có hai phát hiện để thúc đẩy
các công việc tương lai: một là hạng hiệu quả vượt trội hơn các biện
pháp thưa thớt khác cho nén mạng. Cái khác là cân bằng không gian
và thời gian cho mạng nơ-ron tensor hóa.
Thuật ngữ chỉ mục —nén mô hình, huấn luyện không gian con, hạng
hiệu quả, tối ưu hóa tensor hạng thấp, học sâu hiệu quả.
I. GIỚI THIỆU
MẠNG nơ-ron sâu (DNN) đã được sử dụng rộng rãi trong
nhiều ứng dụng xử lý dữ liệu, như nhận dạng giọng nói,
thị giác máy tính [70], [122], [63], xử lý ngôn ngữ tự nhiên
[134], [43], v.v. Vì cấu trúc sâu hơn hoặc rộng hơn có thể
dẫn đến hiệu suất tốt hơn, DNN dần được đặc trưng bởi
việc tham số hóa quá mức. Mặt khác, tham số hóa quá mức
gợi ý quá nhiều dư thừa trong DNN, dẫn đến việc overfitting
[53], [29]. Có chủ yếu hai thách thức trong học sâu: độ phức
tạp cao và hội tụ chậm. Độ phức tạp cao có nghĩa là có hàng
triệu tham số trong DNN, và tính toán giữa các tham số khổng
lồ và đầu vào là rườm rà, điều này nhấn mạnh nhu cầu về
các thuật toán hiệu quả để nén và tăng tốc. Ví dụ, số lượng
tham số trong VGG-16 [122] gần bảy triệu. Đối với một hình
ảnh trong tập dữ liệu ImageNet [70] với kích thước 224×224×3,
quá trình feedforward yêu cầu 30,9 tỷ phép toán điểm nổi
(FLOP). Độ phức tạp cao không thể chấp nhận được đối với
các thiết bị hạn chế tài nguyên, như điện thoại di động
Tất cả các tác giả đều thuộc Trường Kỹ thuật Thông tin và Truyền thông,
Đại học Khoa học và Công nghệ Điện tử Trung Quốc (UESTC), Thành Đô
611731 Trung Quốc (e-mails: xinweiou@std.uestc.edu.cn,
fzhangxinchen, eczhu, yipengliu g@uestc.edu.cn).
Bản thảo nhận được ×× ××, 2023; sửa đổi ×× ××, 2023.[65] và thiết bị IoT [73]. Hội tụ chậm được gây ra bởi
thuật toán lan truyền ngược (BP) thông thường, dẫn đến
việc huấn luyện tốn thời gian [1]. Ngoài ra, tốc độ hội tụ
nhạy cảm với việc thiết lập tốc độ học và cách khởi tạo
trọng số.

Có nhiều công trình cố gắng giảm độ phức tạp cao của
DNN với suy giảm hiệu suất chấp nhận được. Việc điều tra
nén mô hình có thể được tóm tắt thành hai khía cạnh: một
là giảm số lượng tham số, và cái khác là giảm độ rộng bit
trung bình của biểu diễn dữ liệu. Khía cạnh đầu tiên bao gồm
nhưng không giới hạn ở xấp xỉ hạng thấp [75], [62], [137],
[92], cắt tỉa [99], [150], chia sẻ trọng số [131], độ thưa [57],
chưng cất kiến thức [48]. Vì các kỹ thuật này có những hạn
chế riêng, tốt hơn là kết hợp chúng để khai thác đầy đủ
dư thừa trong DNN. Lượng tử hóa [40], [142] và mã hóa
entropy [48] thuộc về danh mục thứ hai, được thiết kế để
đạt được số bit thấp hơn cho mỗi tham số.

Xấp xỉ hạng thấp đã được áp dụng rộng rãi do cơ sở lý
thuyết mạnh mẽ và dễ triển khai trên phần cứng. Trong cuộc
khảo sát này, chúng tôi xem xét toàn diện lĩnh vực phát triển
nhanh chóng này bằng cách chia tối ưu hóa hạng thấp cho
nén mô hình thành ba danh mục chính: phương pháp tiền
huấn luyện, phương pháp tiền thiết lập và phương pháp
nhận thức nén. Sự khác biệt lớn nhất giữa chúng là cách
huấn luyện. Phương pháp tiền huấn luyện trực tiếp phân
tách một mô hình đã được huấn luyện trước để có khởi tạo
ấm cho định dạng nén, sau đó huấn luyện lại mô hình nén
để khôi phục hiệu suất. Không có tiền huấn luyện, phương
pháp tiền thiết lập huấn luyện một mạng được tiền thiết lập
ở định dạng nhỏ gọn từ đầu. Hoàn toàn khác với hai phương
pháp trên, phương pháp nhận thức nén rõ ràng tính đến nén
trong quá trình huấn luyện bằng cách dần dần buộc mạng
có cấu trúc hạng thấp. Mặc dù thảo luận về tối ưu hóa hạng
thấp cũng có thể được tìm thấy trong [136], chúng tôi đã
điều tra thêm cách tích hợp nó với các kỹ thuật nén khác
để theo đuổi độ phức tạp thấp hơn và khuyến nghị hạng
hiệu quả như biện pháp hiệu quả nhất được sử dụng trong
tối ưu hóa hạng thấp.

Khi dư thừa trong DNN được khai thác bằng huấn luyện
không gian con, DNN có thể hội tụ nhanh hơn mà không
mất độ chính xác. Trong học sâu, việc huấn luyện mạng
với các phương pháp tối ưu hóa bậc nhất, ví dụ SGD [118],
là phổ biến, điều này rẻ về mặt tính toán. Nhưng có một
số nhược điểm cố hữu của các phương pháp tối ưu hóa bậc
nhất, như hội tụ lý thuyết và thực nghiệm chậm. Các phương
pháp bậc hai có thể xử lý vấn đề này tốt, nhưng do gánh
nặng tính toán

--- TRANG 2 ---
2
của ma trận Hessian, các phương pháp bậc hai không áp
dụng được cho DNN. Ý tưởng chiếu tham số lên một không
gian con nhỏ được biểu diễn bởi một số biến độc lập là
một cách hiệu quả để giải quyết vấn đề này. Vì chỉ cần
tối ưu hóa một vài biến, chúng ta có thể áp dụng các phương
pháp tối ưu hóa bậc hai để đạt được hiệu quả thời gian
của học sâu.

Trong cuộc khảo sát này, trước tiên chúng tôi trình bày
một cái nhìn tổng quan toàn diện về các phương pháp phân
tách tensor khác nhau có thể áp dụng cho nén mô hình.
Tiếp theo, tối ưu hóa hạng thấp cho nén mô hình được tóm
tắt theo các phương pháp tiền thiết lập, tiền huấn luyện
và nhận thức nén. Đối với mỗi phương pháp, một cuộc
thảo luận chi tiết về các điểm chính về triển khai được
đưa ra. Tỉ mỉ hơn, chúng tôi so sánh giữa các biện pháp
độ thưa khác nhau được sử dụng trong phương pháp nhận
thức nén, và khai thác biện pháp hiệu quả nhất, tức là
hạng hiệu quả, hiếm khi được sử dụng như một bộ điều
chỉnh thưa trước đây. Ngoài ra, trong khi đã có nhiều
công trình đưa ra danh sách nén kết hợp [28], [21], ít
chú ý được dành cho việc tích hợp giữa xấp xỉ hạng thấp
và các kỹ thuật nén khác. Do đó, chúng tôi trình bày một
cuộc khảo sát tổng thể về loại tích hợp này ở đây. Sau đó,
chúng tôi giới thiệu tối ưu hóa hạng thấp cho huấn luyện
không gian con. Hơn nữa, chúng tôi là những người đầu
tiên liên kết hai loại tối ưu hóa hạng thấp này, khám phá
rằng dư thừa trong miền thời gian và miền không gian có
cùng nguồn gốc. Và có một cuộc thảo luận về cách áp
dụng huấn luyện không gian con trên mạng nơ-ron tensor
hóa để đạt được hiệu quả không gian và hiệu quả thời
gian đồng thời. Bản đồ tư duy tổng thể để đạt được học
sâu hiệu quả thông qua tối ưu hóa hạng thấp được hiển
thị trong Hình 1.

Khác với các cuộc khảo sát trước đây về tensor cho học
sâu hiệu quả [92], [88], [90], những đóng góp chính của
bài báo này có thể được tóm tắt như sau.
1) Chúng tôi đưa ra cái nhìn tổng quan chi tiết về xấp xỉ
hạng thấp cho nén mô hình, và chúng tôi thấy rằng RNN
có thể được nén hiệu quả bằng cách sử dụng Phân tách
Tucker Phân cấp (HT) và Phân tách Tích Kronecker (KPD),
CNN có thể được nén hiệu quả bằng cách sử dụng Tensor
Train (TT) và Phân tách Tích Kronecker Tổng quát (GKPD),
trong khi Tensor Ring (TR) và Phân tách Khối (BTD) có
thể nén phù hợp cả RNN và CNN.
2) Một loạt các kỹ thuật nén mạng nơ-ron có thể tích hợp
đã được thảo luận chi tiết, và một khung tích hợp được
tóm tắt để tận dụng tốt các phương pháp khác nhau.
3) Chúng tôi phân tích rằng dư thừa trong miền không gian
và miền thời gian có cùng nguồn gốc. Để tăng tốc việc
huấn luyện mạng nơ-ron tensor hóa, chúng ta nên cân
bằng giữa hiệu quả không gian và hiệu quả thời gian.
4) Sau khi thảo luận và kiểm tra các biện pháp thưa khác
nhau cho tối ưu hóa hạng thấp cho nén mạng nơ-ron sâu,
hạng hiệu quả vượt trội trong các thí nghiệm số.

Cuộc khảo sát này được tổ chức như sau. Trong Phần II,
chúng tôi đưa ra cái nhìn tổng quan về tối ưu hóa hạng
thấp cho nén mô hình. Xấp xỉ hạng thấp tích hợp với
các kỹ thuật nén khác được xem xét trong Phần III. Phần
IV giới thiệu tối ưu hóa hạng thấp cho huấn luyện không
gian con và phân tích sự ghép nối giữa hai loại tối ưu
hóa hạng thấp này.

II. TỐI ƯU HÓA HẠNG THẤP CHO NÉN MÔ HÌNH

Vì DNN được tham số hóa quá mức, có cơ hội làm cho
mạng sâu nhỏ gọn hơn. Các phương pháp nén, như lượng
tử hóa, cắt tỉa và xấp xỉ hạng thấp, có thể giảm độ phức
tạp của DNN mà không suy giảm độ chính xác đáng kể.
Trong số đó, xấp xỉ hạng thấp đã được áp dụng rộng rãi
vì cơ sở lý thuyết vững chắc của phân tách tensor. Trong
phần này, trước tiên chúng tôi sẽ giới thiệu các phương
pháp phân tách tensor khác nhau có thể áp dụng cho nén
mạng, sau đó chia các phương pháp tối ưu hóa cho xấp
xỉ hạng thấp thành ba loại: phương pháp tiền huấn luyện,
tiền thiết lập và nhận thức nén. Ngoài ra, chúng tôi thảo
luận về các biện pháp độ thưa hiệu quả.

A. Phân tách Tensor

Xấp xỉ hạng thấp có thể cung cấp tỷ lệ nén cực cao cho
mạng nơ-ron hồi quy (RNN) với sự mất mát độ chính xác
không đáng kể. Tuy nhiên, khi nói đến mạng nơ-ron tích
chập (CNN), hiệu suất nén không thỏa mãn như RNN.
Trong các tài liệu sớm, kernel tích chập 4D được định
hình lại thành ma trận và phân tách giá trị đơn (SVD)
được sử dụng để phân tách ma trận thành hai nhân tử
[151]. Tuy nhiên, thao tác định hình lại dẫn đến méo mó
thông tin cấu trúc. Do đó, phân tách tensor hiệu quả hơn
đã thu hút sự quan tâm. Phân tách Canonical-Polyadic
(CP) [92] được áp dụng để phân tách một lớp tích chập
thành bốn lớp tích chập liên tiếp, tăng tốc đáng kể CNN
[75]. Phân tách Tucker [129] có thể phân tách kernel 4D
thành một kernel 4D nhỏ gọn và hai ma trận bằng cách
khai thác dư thừa theo kênh. Dựa trên ba phân tách cổ
điển này, có nhiều phương pháp linh hoạt khác xuất hiện
bao gồm HT [42], TT [108], TR [153], BTD [26], GKPD
[46], Phân tách Tensor dựa trên Tích Bán tensor (STP)
[152], cải thiện đáng kể hiệu suất nén cho DNN. Bảng I
cho thấy hiệu suất của các phương pháp phân tách tensor
được sử dụng rộng rãi áp dụng để nén ResNet32 với tập
dữ liệu Cifar10.

BẢNG I
SO SÁNH HIỆU SUẤT NÉN CỦA CÁC PHƯƠNG PHÁP PHÂN TÁCH TENSOR
TIÊN TIẾN TRÊN RESNET32 VỚI TẬP DỮ LIỆU CIFAR 10

Phương pháp | Độ chính xác Top-1 (%) | Tỷ lệ nén
Tucker [65] | 87.70 | 5×
TT [37] | 88.3 | 4.8×
TR [137] | 90.6 | 5×
BTD [146] | 91.1 | 5×
GKPD [46] | 91.5 | 5×
HT [141] | 89.9 | 1.6×
STT [152] | 91.0 | 9×

Ở đây, chúng tôi phác thảo một số ký hiệu chính. Đối với
một lớp kết nối đầy đủ (FC), chúng tôi để W ∈ R^O×I biểu
thị ma trận trọng số của lớp này, trong đó I và O đại diện
cho số lượng nơ-ron đầu vào

--- TRANG 3 ---
3
và nơ-ron đầu ra, tương ứng. Và đối với một lớp tích chập
(Conv), chúng tôi để K ∈ R^S×C×H×W biểu thị trọng số
của kernel tích chập, trong đó S, C là số lượng bộ lọc và
kênh đầu vào, H, W là chiều cao và chiều rộng của kernel.
Trong một số trường hợp, chúng ta cần định hình lại một
tensor thành một tensor bậc cao hơn. Chúng ta giả định
rằng I₁I₂...I_d = I, O₁O₂...O_d = O, C₁C₂...C_d = C,
và S₁S₂...S_d = S. Một số toán tử toán học cần thiết được
liệt kê trong Bảng II.

BẢNG II
CÁC KÝ HIỆU ĐƯỢC SỬ DỤNG TRONG BÀI BÁO NÀY.

Ký hiệu | Mô tả
diag(·) | tạo một ma trận đường chéo bằng cách lấy vector đầu vào làm đường chéo chính
⊗ | tích Kronecker
∘ | tích ngoài vector
×ₙ | tích n-mode
⋄ₙ | tích bán tensor

Dựa trên các ký hiệu đã định nghĩa này, chúng ta có thể
so sánh giữa các phân tách tensor tiên tiến khác nhau về
khả năng nén và tăng tốc của chúng. Khi nhắm đến các
lớp FC, so sánh được hiển thị trong Bảng III. Và Bảng
IV dành cho các lớp Conv.

BẢNG III
SO SÁNH GIỮA CÁC LỚP KẾT NỐI ĐẦY ĐỦ ĐƯỢC NÉN BỞI TT,
TR, HT, BTD, STR, VÀ KPD VỀ CHI PHÍ TÍNH TOÁN VÀ TIÊU THỤ
LƯU TRỮ. LƯU Ý RẰNG I_m = max_{k∈{1,...,d}} I_k,
O_m = max_{k∈{1,...,d}} O_k, d = 2 ĐỐI VỚI KPD, r LÀ HẠNG
TỐI ĐA, R LÀ HẠNG CP CỦA BTD, VÀ t LÀ TỶ LỆ GIỮA
CHIỀU KẾT NỐI.

Phương pháp | Tính toán | Lưu trữ
FC | O(IO) | O(IO)
TT | O(dI_m max(I, O)r²) | O(dI_mO_mr²)
TR | O(d(I+O)r³) | O(d(I_m+O_m)r²)
HT | O(d min(I, O)(r³+I_mr²)) | O(dI_mO_mr+dr³)
BTD | O(dI_m max(I, O)r_dR) | O((dI_mO_mr+r_d)R)
STR | O(d(I+O)r³/t) | O(d(I_m+O_m)r²/t²)
KPD | O(IO_m+OI_m) | O(I_mO_m)

1) Phân tách Giá trị Đơn
Đối với một ma trận cho trước X ∈ R^M×N, SVD của nó có thể được viết như:
X = U diag(s) V^T                                          (1)

Để R biểu thị hạng của ma trận, R ≤ min{M, N}. Lưu ý
rằng U ∈ R^M×R và V ∈ R^N×R thỏa mãn UU^T = I và
VV^T = I, tương ứng. Và các phần tử của s ∈ R^R giảm
từ đầu đến cuối, tức là s₁ ≥ s₂ ≥ ... ≥ s_R.

BẢNG IV
SO SÁNH GIỮA CÁC LỚP TÍCH CHẬP ĐƯỢC NÉN BỞI TT, TR, HT, BTD, STR, GKPD VỀ CHI PHÍ TÍNH TOÁN VÀ TIÊU THỤ
LƯU TRỮ. LƯU Ý RẰNG C_m = max_{k∈{1,...,d}} C_k, S_m = max_{k∈{1,...,d}} S_k, d = 2 ĐỐI VỚI GKPD, k = max(k₁, k₂)
VỚI k₁k₂ = K, r LÀ HẠNG TỐI ĐA, R LÀ HẠNG CP CỦA BTD, M VÀ N LÀ CHIỀU CAO VÀ CHIỀU RỘNG CỦA BẢN ĐỒ
ĐẶC TRƯNG, VÀ t LÀ TỶ LỆ GIỮA CHIỀU KẾT NỐI.

Phương pháp | Tính toán | Lưu trữ
Conv | O(SCK²MN) | O(SCK²)
TT | O(dr max(rC_m, K²) max(C, S)MN) | O(dC_mS_mr²+K²r)
TR | O(r³(C+S) + (r³K²+r²(C+S))MN) | O((dC_mS_m+K²)r²)
HT | O(log₂ dCS(r³+r²) +SCK²MN) | O(dC_mS_mr+K²r+dr³)
BTD | O((K²r²+ (C+S)r)RMN ) | O((K²r²+ (I+O)r)R)
STR | O(r³/t³(C+S) + (r³K²+r²/t(C+S))MN) | O((dC_mS_m/t²+K²)r²)
GKPD | O(r(C_mS+S_mC)k²MN) | O(rC_mS_mk²)

Vì định dạng trọng số trong các lớp FC là một ma trận
tự nhiên, SVD có thể được sử dụng trực tiếp. Bằng cách
sử dụng SVD, lớp FC có thể được xấp xỉ bởi hai lớp liên
tiếp. Trọng số của lớp thứ nhất và thứ hai có thể được
biểu diễn bởi B = diag(√s)V^T và A = U diag(√s), tương
ứng. Đối với các lớp Conv, kernel 4D nên được định hình
lại thành ma trận 2D trước tiên. Bằng cách khai thác các
loại dư thừa khác nhau, có hai sơ đồ phân tách, một định
hình lại W thành ma trận S-by-CHW, gọi là phân tách theo
kênh [151], cái khác được gọi là phân tách theo không gian
[62] có được ma trận SH-by-CW. Sau đó, tính SVD của ma
trận đã định hình lại. Tương tự như quá trình nén các lớp
FC, hai lớp Conv được biểu diễn bởi các tensor được định
hình lại từ các nhân tử B và A có thể được sử dụng để
thay thế lớp gốc.

Tuy nhiên, cả hai phương pháp chỉ có thể khai thác một
loại dư thừa. Hơn nữa, cũng có dư thừa giữa các kênh
đầu vào. Khai thác tất cả các loại dư thừa cùng một lúc
có thể giúp chúng ta đạt được tỷ lệ nén cao hơn nhiều, điều

--- TRANG 4 ---
4
có thể đạt được bằng phân tách tensor.

2) Phân tách CP
Trong khi SVD phân tích một ma trận thành tổng của các
ma trận hạng một, phân tách CP phân tích một tensor thành
tổng của các tensor hạng một. Đối với một tensor bậc N,
X ∈ R^{I₁×I₂×...×I_N}, phân tách CP có thể được công thức
hóa như:

X = ⟦⟦ λ; A^{(1)}, A^{(2)}, ..., A^{(N)} ⟧⟧
  = ∑_{r=1}^R λ_r a_r^{(1)} ∘ a_r^{(2)} ∘ ... ∘ a_r^{(N)}    (2)

Mỗi a_r^{(n)} đại diện cho cột thứ r của A^{(n)} và λ ∈ R^R
đại diện cho tầm quan trọng của R thành phần. Hạng của
tensor X, ký hiệu là R, được định nghĩa là số nhỏ nhất
của các tensor hạng một [88], [94].

Khi sử dụng CP để nén các lớp FC, ma trận trọng số W
nên được tensor hóa đầu tiên thành một tensor bậc 2d,
W ∈ R^{O₁×O₂×...×O_d×I₁×I₂×...×I_d}. Đồng thời, vector
đầu vào x ∈ R^I nên được trình bày như một tensor bậc d,
X ∈ R^{I₁×I₂×...×I_d}. Đối với các kernel tích chập, bằng
cách thực hiện trực tiếp CP trên tensor kernel 4-D, lớp
sẽ được xấp xỉ bởi bốn lớp tích chập liên tiếp có trọng
số được biểu diễn bởi bốn ma trận nhân tử, tương ứng.

3) Phân tách Tucker
Phân tách Tucker có thể được coi là một tổng quát hóa
bậc cao của phân tích thành phần chính (PCA). Nó biểu
diễn một tensor bậc N với một tensor lõi bậc N nhân với
một ma trận cơ sở dọc theo mỗi mode. Do đó, đối với
X ∈ R^{I₁×I₂×...×I_N}, chúng ta có

X = G ×₁ A^{(1)} ×₂ A^{(2)} ×₃ ... ×_N A^{(N)}    (3)

Trong đó G ∈ R^{R₁×R₂×...×R_N} được gọi là tensor lõi.
Ở đây, "×_n" đại diện cho tích n-mode, tức là nhân một
tensor với một ma trận trong mode n. Theo phần tử, "×_n"
có thể được công thức hóa như:

(G ×₁ A^{(1)})_{i₁,r₂,...,r_N} = ∑_{r₁=1}^{R₁} G_{r₁,r₂,...,r_N} A^{(1)}_{i₁,r₁}    (4)

Các cột của ma trận nhân tử A^{(n)} ∈ R^{I_n×R_n} có thể
được coi là các thành phần chính của mode thứ n. Tensor
lõi G có thể được xem như một phiên bản nén của X hoặc
hệ số trong không gian con chiều thấp. Trong trường hợp
này, chúng ta có thể nói rằng X là một tensor hạng-(R₁, R₂, ..., R_N) [88], [94].

Trong trường hợp nén các lớp FC, tương tự như CP, cùng
một tensor hóa cho trọng số và đầu vào là cần thiết, tiếp
theo bởi việc thực hiện trực tiếp phân tách Tucker trên
tensor bậc 2d. Đối với các lớp Conv, vì kích thước không
gian của kernel quá nhỏ, chúng ta chỉ có thể sử dụng
Tucker2 [130] để tận dụng dư thừa giữa các bộ lọc và
giữa các kênh đầu vào, tạo ra tích chập 1×1, tích chập
H×W, và tích chập 1×1.

4) Phân tách Khối (BTD)
Phân tách Khối (BTD) được giới thiệu trong [26] như
một phân tách tensor mạnh mẽ hơn, kết hợp phân tách
CP và phân tách Tucker. Do đó, BTD mạnh mẽ hơn so
với phân tách CP và Tucker gốc. Trong khi CP xấp xỉ
một tensor với tổng các tensor hạng một, BTD là tổng
các tensor ở định dạng Tucker hạng thấp. Hoặc, bằng cách
nối các ma trận nhân tử trong mỗi mode và sắp xếp tất
cả các tensor lõi của mỗi subtensor thành một tensor lõi
đường chéo khối, BTD có thể được coi là một trường hợp
của Tucker. Do đó, xem xét một tensor bậc N, X ∈ R^{I₁×I₂×...×I_d},
BTD của nó có thể được viết như:

X = ∑_{n=1}^N G_n ×₁ A_n^{(1)} ×₂ A_n^{(2)} ×₃ ... ×_d A_n^{(d)}    (5)

Trong công thức này, N biểu thị hạng CP, tức là số lượng
khối, và G_n ∈ R^{R₁×R₂×...×R_d} là tensor lõi của khối
thứ n với các hạng đa tuyến tính bằng (R₁, R₂, ..., R_d).

Khi BTD được áp dụng để nén một lớp FC, lớp nhỏ gọn
thu được được gọi là Lớp Khối (BTL) [146]. Trong BTL,
tensor đầu vào X ∈ R^{I₁×I₂×...×I_d} được tensor hóa từ
vector đầu vào gốc x ∈ R^I và ma trận trọng số gốc W
được định hình lại thành W ∈ R^{O₁×I₁×O₂×I₂×...×O_d×I_d}.
Sau đó, chúng ta có thể phân tích W bằng BTD với các
tensor nhân tử {A_n^{(d)} ∈ R^{O_d×I_d×R_d}}_{n=1}^N. Bằng
cách tiến hành phép toán co tensor giữa BTD(W) và X,
tensor đầu ra Y ∈ R^{O₁×O₂×...×O_d} xuất hiện, có thể
được vector hóa như vector đầu ra cuối cùng. Đối với
các lớp Conv, được tuyên bố trong [146] rằng bằng cách
định hình lại kernel 4D thành ma trận, W ∈ R^{S×C×H×W},
lớp có thể được chuyển đổi thành BTL. Cụ thể, ma trận
nên được định hình lại thêm thành (1×H×1×W×S₁×C₁×S₂×C₂×...×S_d×C_d).

5) Phân tách Tucker Phân cấp
Phân tách Tucker Phân cấp (HT) là một biến thể phân
cấp của phân tách Tucker, lặp đi lặp lại biểu diễn một
tensor bậc cao với hai subtensor bậc thấp hơn

--- TRANG 5 ---
5
và một ma trận chuyển đổi thông qua việc tận dụng phân
tách Tucker [42], [95]. Đối với một tensor X ∈ R^{I₁×I₂×...×I_N},
chúng ta có thể đơn giản chia tập chỉ số {1, 2, ..., N} thành
hai tập con, tức là T = {t₁, t₂, ..., t_k}; S = {s₁, s₂, ..., s_{N-k}}.
Để U_{1...N} ∈ R^{I_{t₁}×I_{t₂}×...×I_{t_k}×I_{s₁}×I_{s₂}×...×I_{s_{N-k}}×1}
biểu thị ma trận được định hình lại từ X, và các ma trận
cắt ngắn U_t ∈ R^{I_{t₁}×I_{t₂}×...×I_{t_k}×R_t}, U_s ∈ R^{I_{s₁}×I_{s₂}×...×I_{s_{N-k}}×R_s}
đại diện cho ma trận cơ sở cột tương ứng của hai không
gian con. Khi đó, chúng ta có thể có:

U_{1...N} = (U_t ⊗ U_s)B_{1...N}                        (6)

trong đó B_{1...N} ∈ R^{R_t×R_s×1} được gọi là ma trận chuyển
đổi và "⊗" biểu thị tích Kronecker giữa hai ma trận. Tiếp
theo, chia tập T thành hai tập con L = {l₁, l₂, ..., l_q} và
V = {v₁, v₂, ..., v_{k-q}}. Chúng ta có thể biểu diễn U_t với
U_l ∈ R^{I_{l₁}×I_{l₂}×...×I_{l_q}×R_l}, U_v ∈ R^{I_{v₁}×I_{v₂}×...×I_{v_{k-q}}×R_v},
và B_t ∈ R^{R_l×R_v×R_t} như:

U_t = (U_l ⊗ U_v)B_t                                    (7)

Quá trình phân tích tương tự áp dụng đồng thời cho U_s.
Bằng cách lặp lại quy trình này cho đến khi tập chỉ số
không thể chia được, cuối cùng chúng ta có thể thu được
định dạng HT dạng cây của tensor đích. Một minh họa
của một phiên bản đơn giản của HT có thể được thấy
trong Hình 2.

[Hình 2. Phân tách HT]

Vì tích Kronecker trong Phương trình (7) tốn kém về mặt
tính toán, có những dạng ngắn gọn khác của HT, chẳng
hạn như dạng co rút được giới thiệu trong [141]. Dạng
này hợp nhất các tập con chỉ số thành tập vũ trụ từ dưới
lên trên. Theo cách này, một đầu vào bên ngoài có thể
được co với mỗi ma trận chuyển đổi và ma trận cắt ngắn
từng cái một. Cách này có thể tránh quy trình tái tạo trọng
số tốn kém về bộ nhớ và tính toán và các đầu ra trung
gian sẽ không quá lớn để hết bộ nhớ.

Đối với việc thực hiện nén các lớp FC bằng HT, ma trận
trọng số nên được chuyển đổi thành W ∈ R^{(I₁×O₁)×(I₂×O₂)×...×(I_d×O_d)},
và dữ liệu đầu vào được tensor hóa thành X ∈ R^{I₁×I₂×...×I_d}.
Để giảm độ phức tạp tính toán, việc tính toán chuỗi được
hiển thị trong Hình 3 được áp dụng. Tuy nhiên, vì không
có quy luật liên kết tích chập và co rút, kernel của các
lớp Conv phải được khôi phục từ định dạng HT. Nhân
tiện, để giữ cân bằng, kernel 4D nên được tensor hóa
thành W ∈ R^{(H×W)×(C₁×S₁)×(C₂×S₂)×...×(C_d×S_d)}.

[Hình 3. Tính toán chuỗi cho trường hợp bậc 4. X đại diện cho một đầu vào bậc 4. Những mũi tên này đại diện cho thứ tự co rút.]

6) Phân tách Tensor Train

Tensor Train (TT) là một trường hợp đặc biệt của HT,
đây là một định dạng HT suy biến [108], [93]. TT phân
tích một tensor bậc cao thành một tập hợp các tensor
bậc 3 hoặc bậc 2. Những tensor lõi này được kết nối
bởi toán tử co rút. Giả sử rằng chúng ta có một tensor
bậc N, X ∈ R^{I₁×I₂×...×I_N}, theo phần tử, chúng ta có
thể phân tích nó thành định dạng TT như:

X_{i₁,i₂,...,i_N} = ∑_{r₁,r₂,...,r_{N-1}} G₁_{i₁,r₁} G₂_{r₁,i₂,r₂} ... G^N_{r_{N-1},i_N}    (8)

trong đó tập hợp {G^n ∈ R^{R_{n-1}×I_n×R_n}}_{n=1}^N với R₀ = 1
và R_N = 1 được gọi là TT-cores [108]. Tập hợp các hạng
{R_n}_{n=0}^N được gọi là TT-ranks. Hình 4 đưa ra một
minh họa của một tensor bậc 4 được biểu diễn trong
định dạng TT.

[Hình 4. Một tensor bậc 4 trong định dạng TT.]

TT lần đầu tiên được áp dụng để nén các lớp FC trong
[105], nơi ma trận trọng số được định hình lại thành một
tensor bậc cao, W ∈ R^{(I₁×O₁)×(I₂×O₂)×...×(I_d×O_d)}.
Sau khi biểu diễn W trong định dạng TT, các TT-cores
kết quả {G^n ∈ R^{R_{n-1}×I_n×O_n×R_n}}_{n=1}^N có thể
được co trực tiếp với đầu vào đã tensor hóa. Được gợi
ý trong [141] rằng TT hiệu quả hơn để nén các lớp Conv
so với HT, trong khi HT phù hợp hơn để nén các lớp
FC có ma trận trọng số dễ được định hình lại thành một
tensor cân bằng hơn.

Việc sử dụng TT trên các lớp Conv được giới thiệu trong
[37], nơi tensor kernel 4D nên được định hình lại thành
kích thước (H×W)×(C₁×S₁)×(C₂×S₂)×...×(C_d×S_d) và
các bản đồ đặc trưng đầu vào được định hình lại thành
X ∈ R^{H×W×C₁×...×C_d}. Trong giai đoạn feedforward,
đầu vào được tensor hóa X sẽ được co với mỗi TT-core
từng cái một. Mặc dù TT có thể tiết kiệm đáng kể chi
phí lưu trữ, độ phức tạp tính toán có thể cao hơn so với
lớp Conv gốc. Do đó, HODEC (High-Order Decomposed
Convolution) được đề xuất trong [149] để cho phép giảm
đồng thời chi phí tính toán và lưu trữ, phân tách thêm
mỗi TT-cores thành hai tensor bậc 3.

7) Phân tách Tensor Ring

Do sự không thống nhất của các TT-cores biên, vẫn còn
một vấn đề mở là làm thế nào để sắp xếp các chiều của
tensor để tìm định dạng TT tối ưu. Để chinh phục vấn
đề này, Phân tách Tensor Ring (TR)

--- TRANG 6 ---
6
được đề xuất để thực hiện một tích đa tuyến tính vòng
tròn trên các lõi [153], [56], [87], [97]. Xem xét một tensor
cho trước, X ∈ R^{I₁×I₂×...×I_N}, theo phần tử, chúng ta
có thể công thức hóa biểu diễn TR của nó như:

X_{i₁,i₂,...,i_N} = ∑_{r₁,r₂,...,r_N} G₁_{r₁,i₁,r₂} G₂_{r₂,i₂,r₃} ... G^N_{r_N,i_N,r₁}
                   = tr(∑_{r₂,...,r_N} G₁_{:,i₁,r₂} G₂_{r₂,i₂,r₃} ... G^N_{r_N,i_N,:})    (9)

trong đó tất cả các lõi {G^n ∈ R^{R_n×I_n×R_{n+1}}}_{n=1}^N
với R_{N+1} = R₁ được gọi là TR-cores. Biểu đồ tensor
của nó cho một tensor bậc 4 được minh họa trong Hình 5.
Dạng này tương đương với tổng của R₁ định dạng TT.
Nhờ tích đa tuyến tính vòng tròn thu được bằng cách sử
dụng phép toán trace, TR đối xử với tất cả các lõi tương
đương và trở nên mạnh mẽ và tổng quát hơn nhiều so
với TT.

[Hình 5. Một tensor bậc 4 trong định dạng TR.]

Hơn nữa, do chiến lược vòng tròn, TR sửa chữa sự khác
biệt của gradient trong TT. Do đó, TR cũng phù hợp
để nén các lớp FC. Trong [137], TR lần đầu tiên được
áp dụng để nén DNN. Cụ thể, ma trận trọng số của các
lớp FC nên được định hình lại thành một tensor bậc 2d
có kích thước I₁×...×I_d×O₁×...×O₂, tiếp theo bởi việc
biểu diễn tensor trong định dạng TR. Đối với quá trình
feedforward, đầu tiên, hợp nhất d lõi đầu tiên và d lõi
cuối cùng để thu được F₁ ∈ R^{R₁×I₁×...×I_d×R_{d+1}} và
F₂ ∈ R^{R_{d+1}×O₁×...×O_d×R₁}, tương ứng. Sau đó, chúng
ta có thể tính toán co rút giữa đầu vào X ∈ R^{I₁×I₂×...×I_d}
và F₁, tạo ra một ma trận có thể được co với F₂. Tensor
đầu ra cuối cùng sẽ có kích thước O₁×O₂×...×O_d. Đối
với các lớp Conv, nếu giữ tensor kernel ở bậc 4 và duy
trì thông tin không gian, định dạng TR của nó có thể
được công thức hóa như:

K_{s,c,h,w} = ∑_{r₁=1}^{R₁} ∑_{r₂=1}^{R₂} ∑_{r₃=1}^{R₃} U_{r₁,s,r₂} V_{r₂,c,r₃} Q_{r₃,h,w,r₁}    (10)

Do đó, lớp gốc có thể được biểu diễn bởi ba lớp liên
tiếp có tensor trọng số là V, Q và U tương ứng. Nếu cần
tỷ lệ nén cao hơn, chúng ta có thể xem thêm U và V như
tensor được hợp nhất từ d tensor lõi tương ứng, với gánh
nặng tính toán bổ sung của việc hợp nhất.

8) Phân tách Tích Kronecker Tổng quát

Phân tách Tích Kronecker (KPD) có thể phân tích một
ma trận thành hai ma trận nhân tử nhỏ hơn được kết nối
bởi tích Kronecker, đã cho thấy rất hiệu quả khi được
áp dụng để nén RNN [128]. Để nén thêm các lớp Conv,
nó được tạo ra thành Phân tách Tích Kronecker Tổng
quát (GKPD) [46], biểu diễn một tensor bằng tổng của
tích Kronecker đa chiều giữa hai tensor nhân tử. Chính
thức, tích Kronecker đa chiều giữa A ∈ R^{J₁×J₂×...×J_N}
và B ∈ R^{K₁×K₂×...×K_N} được công thức hóa như:

(A ⊗ B)_{i₁,i₂,...,i_N} = A_{j₁,j₂,...,j_N} B_{k₁,k₂,...,k_N}    (11)

trong đó j_n = ⌊i_n/K_n⌋ và k_n = i_n mod K_n. Dựa trên
điều này, đối với một tensor bậc N cho trước X ∈ R^{J₁K₁×J₂K₂×...×J_NK_N},
GKPD có thể được ký hiệu như:

X = ∑_{r=1}^R A_r ⊗ B_r    (12)

trong đó R được gọi là hạng Kronecker. Để tìm xấp xỉ
tốt nhất trong GKPD với R cho trước, chúng ta có thể
chuyển đổi bài toán tối ưu hóa này thành việc tìm xấp
xỉ hạng-R tốt nhất cho một ma trận, có thể được giải
quyết bằng SVD một cách thuận tiện, thông qua việc
sắp xếp lại cẩn thận X thành một ma trận và sắp xếp
lại A và B thành vector.

Để thực hiện việc sử dụng GKPD để nén các lớp Conv,
kernel 4D được biểu diễn như:

W_{s,c,h,w} = ∑_{r=1}^R (A_r)_{s₁,c₁,h₁,w₁} ⊗ (B_r)_{s₂,c₂,h₂,w₂}    (13)

trong đó S₁S₂ = S, C₁C₂ = C, H₁H₂ = H và W₁W₂ = W.
Tích chập 2D giữa mỗi A_r ⊗ B_r và đầu vào có thể được
chuyển đổi thành một tích chập 3D có độ sâu bằng C₂,
tiếp theo bởi nhiều tích chập 2D. Hơn nữa, nhóm R tích
Kronecker có thể được xem như R kênh song song tính
toán hai bước trên một cách riêng biệt. Và được phân
tích rằng S₁ và C₂ lớn có thể giúp thu được nhiều giảm
FLOP hơn.

9) Phân tách Tensor dựa trên Tích Bán tensor

Tích bán tensor (STP) [18] là một tổng quát hóa của tích
ma trận thông thường, mở rộng tính toán của hai ma trận
phù hợp về chiều thành tính toán của hai ma trận tùy ý
về chiều. Vì co tensor dựa trên tích ma trận thông thường,
chúng ta có thể thay thế STP vào co tensor, điều này sẽ
dẫn đến các phương pháp phân tách tensor tổng quát và
linh hoạt hơn. Được đề xuất trong [152], Phân tách Tensor
dựa trên Tích Bán tensor được thiết kế để tăng cường
tính linh hoạt của phân tách Tucker, TT và TR bằng cách
thay thế tích ma trận thông thường trong co tensor bằng
STP, cho thấy hiệu quả cao hơn nhiều so với các phương
pháp gốc. Xem xét một trường hợp đặc biệt trong đó số
cột X ∈ R^{M×N×P} tỷ lệ với số hàng trong W ∈ R^{P×Q},
STP có thể được ký hiệu như:

Y = X ⋄_n W    (14)

hoặc, theo phần tử, như:

Y_{m,g(n,q)} = ∑_{p=1}^P X_{m,g(n,p)} W_{p,q}    (15)

Lưu ý rằng Y ∈ R^{M×N×Q}, "⋄_n" biểu thị STP, g(n,q) = 
(q-1)N + n và g(n,p) = (p-1)N + n là các hàm chỉ mục
lại.

--- TRANG 7 ---
7
Do đó, lấy phân tách Tucker dựa trên STP làm ví dụ, tức
là STTu, có thể được công thức hóa như:

X = G ⋄₁ A⁽¹⁾ ⋄₂ A⁽²⁾ ⋄₃ ... ⋄ₙ A⁽ᴺ⁾         (16)

trong đó G ∈ R^{R₁×R₂×...×Rₙ}, A⁽ⁿ⁾ ∈ R^{Iₙ/t×Rₙ/t²}. So với
Tucker bình thường, số lượng tham số được giảm từ
(∏ᵢ₌₀ᴺ Rᵢ + ∑ᵢ₌₁ᴺ IᵢRᵢ) xuống (∏ᵢ₌₀ᴺ Rᵢ + ∑ᵢ₌₁ᴺ IᵢRᵢ/t²).

B. Phương pháp Tối ưu hóa Hạng Thấp

Chúng ta đã giới thiệu các phương pháp phân tách tensor
khác nhau, nhưng cách áp dụng các phương pháp này
vào DNN mà không suy giảm đáng kể độ chính xác là
một bài toán tối ưu hóa, vẫn cần được thảo luận. Vì
hạng tensor càng thấp, tỷ lệ nén càng cao chúng ta sẽ
nhận được, chúng ta hy vọng rằng mỗi lớp của DNN có
thể được phân tách bằng phân tách tensor hạng rất thấp.
Tuy nhiên, khi hạng giảm, lỗi xấp xỉ tăng lên, dẫn đến
sự mất mát độ chính xác đáng kể. Do đó, có một sự đánh
đổi giữa độ chính xác và tỷ lệ nén, đây là một vấn đề
được nghiên cứu rộng rãi. Có chủ yếu ba loại phương
pháp tối ưu hóa hạng thấp để đạt được sự đánh đổi tốt:
phương pháp tiền huấn luyện, phương pháp tiền thiết
lập và phương pháp nhận thức nén (các công trình đại
diện có thể được thấy trong Bảng V). Đối với mỗi phương
pháp, chúng tôi sẽ đưa ra các điểm chính về triển khai
một cách chi tiết.

1) Phương pháp Tiền huấn luyện
Phương pháp tiền huấn luyện là phương pháp được sử
dụng sớm nhất trong tài liệu về áp dụng nén tensor cho
nén mô hình, trực tiếp phân tách một mạng đã được huấn
luyện thành định dạng nhỏ gọn, tiếp theo bởi việc tinh
chỉnh để khôi phục độ chính xác. Có hai vấn đề quan
trọng cho việc triển khai: lựa chọn hạng tensor và tính
không ổn định.

Lựa chọn hạng tensor có nghĩa là cách chọn hạng tensor
phù hợp của mỗi lớp trong một mạng. Vì mức độ dư thừa
khác nhau từ lớp này sang lớp khác, hạng của mỗi lớp
không được giả định là bằng nhau. Do đó, không giống
như việc thử nghiệm tốn thời gian, một phương pháp
lựa chọn hạng hiệu quả nên phân bổ các tài nguyên tính
toán hoặc lưu trữ hạn chế cho mỗi lớp một cách hợp lý
thông qua việc quyết định cẩn thận hạng của mỗi lớp,
trong khi đảm bảo suy giảm độ chính xác thấp nhất.

Một cách đơn giản nhưng hiệu quả là đặt hạng của mỗi
lớp tỷ lệ với số kênh đầu vào hoặc đầu ra tương ứng,
thường hoạt động tốt hơn so với việc đặt tất cả hạng
bằng nhau một cách thô. Một công cụ phân tích ma trận
xác suất được gọi là phân tích ma trận Bayesian biến
thiên (VBMF) [103] được sử dụng trong [65] để ước tính
hạng tensor của một tensor trong định dạng Tucker. Để
có được hạng mode-n, ma trận hóa mode-n tương ứng
của tensor đích được xem như một quan sát với nhiễu.
Sau đó, VBMF được sử dụng trên quan sát để lọc bỏ
nhiễu và sau đó thu được một ma trận hạng thấp. Trong
[151], bài toán lựa chọn hạng được công thức hóa như
một bài toán tối ưu hóa tổ hợp [114] với tài nguyên tính
toán hoặc bộ nhớ bị hạn chế. Hàm mục tiêu được ký
hiệu là tích của năng lượng PCA (tổng các giá trị đơn)
của mỗi lớp, vì các tác giả quan sát thực nghiệm rằng
năng lượng PCA có liên quan thô với độ chính xác phân
loại. Tương tự, thuật toán trong [83] cũng sử dụng ý
tưởng rằng lỗi xấp xỉ liên kết với sự mất mát độ chính
xác. Nhưng hiệu quả và hợp lý hơn, nó chọn lỗi xấp
xỉ tối đa của tất cả các lớp làm proxy cho độ chính xác
mô hình. Bằng cách tối thiểu hóa proxy này, được đảm
bảo rằng không có lớp nào được phân tách sẽ giảm đáng
kể độ chính xác. Cùng với ràng buộc tài nguyên, bài
toán cuối cùng là một tối ưu hóa minimax có thể được
giải quyết bằng tìm kiếm nhị phân.

Vì lỗi xấp xỉ không nhất thiết phản ánh sự mất mát
độ chính xác, các phương pháp trên chỉ có thể thu được
một sơ đồ cấu hình hạng tối ưu phụ. Để giải quyết thách
thức này, học tăng cường được sử dụng để tự động chọn
hạng [19], [120]. Trong hệ thống trạng thái-hành động-
phần thưởng được thiết lập, phần thưởng ủng hộ việc
giảm chi phí tài nguyên và phạt sự mất mát độ chính
xác. Trạng thái (một cấu hình hạng toàn cầu có thể của
tất cả các lớp) tạo ra phần thưởng tối đa có thể được
chọn làm trạng thái tiếp theo.

Tính không ổn định có nghĩa là nếu một mô hình được
xấp xỉ bởi một định dạng phân tách không ổn định như
định dạng CP và định dạng TR, nó sẽ dẫn đến khó khăn
trong việc tinh chỉnh, tức là hội tụ chậm và hội tụ đến
một cực tiểu địa phương sai. Trong [100], [50], [69],
được lưu ý rằng có một vấn đề thoái hóa gây ra tính
không ổn định trong phân tách CP. Cụ thể, khi CP biểu
diễn một tensor hạng tương đối cao trong định dạng
hạng thấp, có ít nhất hai thành phần hạng một có chuẩn
Frobenius đi đến vô cùng và triệt tiêu lẫn nhau. Do tính
không ổn định, [30], [75] thất bại trong việc phân tách
toàn bộ mạng bằng phân tách CP, vì khó tìm một tốc
độ học tinh chỉnh phù hợp. Để giải quyết thách thức này,
[5] đề xuất sử dụng Phương pháp Lũy thừa Tensor [2]
để tính phân tách CP và sử dụng tinh chỉnh lặp, tức là
phân tách từng lớp một và sau đó tinh chỉnh toàn bộ
mạng một cách lặp đi lặp lại. Các tác giả của [111] thiết
kế một quy trình để tối thiểu hóa độ nhạy (một biện pháp
cho mức độ thoái hóa) của tensor được tái tạo từ định
dạng CP để mạng phân tách với độ nhạy thấp có thể
được tinh chỉnh nhanh hơn và đạt được độ chính xác
tốt hơn. Một phương pháp trực tiếp hơn được đề xuất
trong [135] cho rằng mỗi cột của ma trận nhân tử nên
được chuẩn hóa sau mỗi cập nhật, vì chuẩn hóa có thể
cải thiện tính ổn định số trong tích ngoài vector [67].
Một vấn đề tính không ổn định tương tự cũng xảy ra
với TR [35]. Do đó, [110] đề xuất một quy trình hiệu
chỉnh độ nhạy để giải quyết vấn đề thông qua việc tối
thiểu hóa độ nhạy với ràng buộc bị chặn lỗi xấp xỉ.

2) Phương pháp Tiền thiết lập
Phương pháp tiền thiết lập có cách hiểu là một mạng
nơ-ron tensor hóa được thiết lập trước ở định dạng hạng
tensor thấp sẽ được huấn luyện từ đầu. Vì phương pháp
không yêu cầu tiền huấn luyện, nó có thể tiết kiệm rất
nhiều thời gian để có được một mô hình nén. Tuy nhiên,
phương pháp nhạy cảm với khởi tạo và khó đạt được
độ chính xác cao do khả năng mô hình hạn chế. Hơn
nữa, tương tự như phương pháp tiền huấn luyện, cũng
có vấn đề trong việc cấu hình hạng. Tóm lại, khởi tạo
phù hợp và lựa chọn hạng tensor là những vấn đề chính
với phương pháp này.

Khởi tạo đóng vai trò quan trọng trong việc cung cấp
một khởi đầu ấm cho việc huấn luyện DNN [39] cũng
như cho việc huấn luyện mạng cấu trúc hạng thấp [137],
và có thể có tác động đến độ chính xác cuối cùng ở mức
độ lớn. Một phân phối khởi tạo phù hợp được xác định
thực nghiệm cho trọng số trong một lớp là N(0, std = √(2/N)),
trong đó N biểu thị tổng số tham số trong lớp này. Đối
với một mô hình tiền thiết lập, chúng ta nên đảm bảo
rằng trọng số trong mỗi lớp được xấp xỉ bởi các tensor
nhân tử cũng tuân theo phân phối này. Ví dụ, khi một
lớp được nén bằng TR và phân phối của mỗi tensor lõi
là N(0, σ²), thì sau khi hợp nhất các d tensor lõi này,
các phần tử của tensor được hợp nhất sẽ có trung bình
0 và phương sai R^d σ^2d. Do đó, chúng ta cần đặt σ²
là (2/N)^(1/d) R^(-1) để có được một khởi tạo tốt. Tương
tự, đối với TT, phương sai của TT-cores nên là (2/N)^(1/d) R^(-1/(d-1)).
Một phân tích hệ thống hơn về khởi tạo cho bất kỳ phương
pháp phân tách tensor nào được giới thiệu trong [109].
Được gợi ý rằng bằng cách trích xuất cấu trúc Backbone
(tức là một cấu trúc chỉ chứa các chiều co, vì chỉ có
toán tử co sẽ thay đổi phương sai của trọng số) từ cấu
trúc tensor hóa gốc, một ma trận kề có thể được thu
được từ các cạnh nút của cấu trúc Backbone, có thể
được sử dụng để điều chỉnh phương sai của các tensor
nhân tử.

Lựa chọn hạng tensor hiếm khi được nghiên cứu trong
các công trình huấn luyện mạng nơ-ron tensor hóa và
thường đặt các hạng bằng nhau trong thí nghiệm, vì
khó xác minh dư thừa trong mỗi lớp mà không có mạng
tiền huấn luyện. Hiện tại, chỉ có một vài phương pháp
để giải quyết vấn đề này cho các phân tách tensor cụ
thể. Lấy cảm hứng từ tìm kiếm kiến trúc mạng nơ-ron
(NAS) [157], [79] đề xuất một mạng tensor ring tìm kiếm
tiến bộ (PSTRN), có khả năng tìm một cấu hình hạng
phù hợp cho TR một cách hiệu quả. Trong thuật toán
này, một giai đoạn tiến hóa và một giai đoạn tiến bộ
được thực hiện luân phiên. Trong khi giai đoạn tiến
hóa chịu trách nhiệm dẫn xuất các lựa chọn hạng tốt
trong không gian tìm kiếm thông qua thuật toán di truyền
đa mục tiêu NSGA-II [27], giai đoạn tiến bộ chịu trách
nhiệm thu hẹp không gian tìm kiếm trong vùng lân cận
của hạng tối ưu đến từ giai đoạn tiến hóa trước đó. Đối
với lựa chọn hạng với phân tách TT, [51] đề xuất một
mạng nơ-ron tensor hóa Bayesian hạng thấp. Các phương
pháp Bayesian luôn được sử dụng để suy luận hạng
tensor trong định dạng CP hoặc định dạng Tucker thông
qua các tiên nghiệm hạng thấp trong các nhiệm vụ hoàn
thành tensor [112], [44], [7]. Bài báo này tạo ra cách
tiếp cận này cho định dạng TT và mạng nơ-ron phi tuyến.

Một phương pháp dễ triển khai hơn, tìm kiếm Beam
sửa đổi, được đề xuất trong [34] để tìm thiết lập hạng
tối ưu, tốn thời gian tìm kiếm thấp hơn nhiều so với
tìm kiếm đầy đủ. Để xác minh tính tối ưu, nó áp dụng
độ chính xác xác thực trên một tập dữ liệu xác thực
mini-batch làm metric của nó. Phương pháp này có thể
áp dụng cho tất cả các loại phân tách tensor.

3) Phương pháp Nhận thức Nén
Phương pháp nhận thức nén là phương pháp mà thông
qua huấn luyện tiêu chuẩn và tối ưu hóa lặp, trọng số
của kernel và lớp FC dần dần có thể có cấu trúc hạng
tensor thấp mong muốn. Tức là, xem xét nén tương lai
trong giai đoạn huấn luyện tiêu chuẩn. Khi kết thúc
quá trình huấn luyện một lần này, các hạng tensor phù
hợp được học tự động, mà không cần nỗ lực thiết kế
các sơ đồ lựa chọn hạng hiệu quả. Hơn nữa, vì quá
trình huấn luyện vẫn trên cấu trúc mạng gốc thay vì
một mạng phân tách sâu hơn, dễ hội tụ về độ chính
xác cao mà không dễ bị gradient biến mất hoặc bùng
nổ. Có chủ yếu hai loại cách để thực hiện ý tưởng này,
tức là sử dụng regularization hạng thấp và giải quyết
tối ưu hóa có ràng buộc.

Regularization hạng thấp tương tự như regularization
thưa thớt luôn được sử dụng trong DNN để tránh overfitting.
Ý tưởng chính của regularization hạng thấp là thêm
regularizer hạng thấp trên trọng số trong mỗi lớp vào
hàm mất mát cơ bản. Do đó, với ràng buộc của regularizer
như vậy, các tensor trọng số dần dần sẽ có cấu trúc
hạng thấp mong muốn trong quá trình huấn luyện. Sau
đó, sau xấp xỉ hạng thấp, không cần huấn luyện lại
trong thời gian dài và không có rủi ro khôi phục không
ổn định. Đối với regularizer hạng thấp, một chỉ số để
đo mức độ hạng thấp là cần thiết. Vì việc tối thiểu hóa
hạng của một ma trận một cách rõ ràng là NP-hard,
nuclear norm [11] được sử dụng rộng rãi như một dạng
thư giãn lồi liên tục của hạng. Trong [4], tổng của nuclear
norm của các ma trận trọng số trong mỗi lớp được thêm
vào loss cross-entropy, tạo ra một bài toán tối ưu hóa
mới có thể được giải quyết bằng gradient descent ngẫu
nhiên proximal. Tương tự, [144] cũng sử dụng nuclear
norm và cùng một bài toán tối ưu hóa được giải quyết
bằng gradient descent sub-ngẫu nhiên [6]. Ngoài ra,
bài báo này nhúng xấp xỉ hạng thấp vào giai đoạn huấn
luyện để thúc đẩy cấu trúc hạng thấp.

Tuy nhiên, đối với những điều trên, SVD sẽ được thực
hiện trên mỗi bước huấn luyện, điều này không hiệu
quả, đặc biệt đối với các mô hình lớn hơn. Do đó, [145]
đề xuất huấn luyện SVD thực hiện huấn luyện trực tiếp
trên các nhân tử phân tách. Bằng cách sử dụng regularization
sparsity trên các giá trị đơn, nó có thể đạt được mục
tiêu thúc đẩy hạng thấp. Để duy trì dạng SVD hợp lệ,
regularization trực giao trên các ma trận đơn trái và
phải là cần thiết. Hơn nữa, trực giao cũng có thể hiệu
quả ngăn chặn gradient bùng nổ hoặc biến mất, do đó
đạt được độ chính xác cao hơn.

Giải quyết tối ưu hóa có ràng buộc là một phương pháp
mà thông qua việc giải quyết một bài toán tối ưu hóa
với các ràng buộc rõ ràng hoặc ngầm định trên hạng
tensor của trọng số, chúng ta có thể có được một mạng
tối ưu không chỉ có loss thấp mà còn có cấu trúc hạng
thấp. Theo cách cổ điển, [61] hình thành bài toán ràng
buộc hạng thấp như việc tối thiểu hóa tổng của loss và
một hàm chi phí bộ nhớ/tính toán nhưng ràng buộc mỗi
hạng không vượt quá hạng tối đa. Nó có thể được giải
quyết bằng thuật toán learning-compression [12]. Thuận
tiện hơn, [147] trực tiếp sử dụng ngân sách (ví dụ chi
phí bộ nhớ/tính toán) làm ràng buộc, với regularizer
hạng thấp được thêm vào hàm loss. Tuy nhiên, vì nó
biểu diễn hạng tensor bằng tổng của nuclear norm của
các ma trận triển khai trong mỗi mode, nó không thể
được tổng quát hóa cho các phương pháp phân tách
nhất định như CP và BTD. Và khi xử lý các tensor bậc
cao, sẽ có quá nhiều biến phụ trợ được sử dụng trong
thuật toán Lagrangian tăng cường, điều này sẽ ảnh hưởng
đến sự hội tụ. Không sử dụng nuclear norm, [148] chỉ
đặt cận trên của hạng, do đó nó có thể áp dụng cho
các phân tách tensor khác nhau.

Các phương pháp trên có sự đánh đổi không thỏa mãn
giữa độ chính xác và nén. Để giải quyết nhược điểm
này, thuật toán Frank Wolfe được sử dụng trong [154]
để tối ưu hóa trọng số mạng với ràng buộc hạng thấp.
Cải thiện này có lợi từ các hướng cập nhật có cấu trúc
cao của Frank Wolfe.

Đối với các phương pháp nhận thức nén, việc sử dụng
các biện pháp sparsity khác nhau làm regularizer hạng
thấp sẽ tác động lớn đến hiệu suất nén. Ví dụ, được
lưu ý trong [145] rằng biện pháp ℓ₁ (ví dụ nuclear norm)
phù hợp hơn cho tỷ lệ nén cực cao trong khi biện pháp
Hoyer hoạt động tốt hơn khi nhắm đến tỷ lệ nén tương
đối thấp. Do đó, việc khai thác một biện pháp sparsity
hiệu quả hấp dẫn cho bất kỳ tỷ lệ nén nào là cần thiết.
Đây chính xác là điểm chúng tôi muốn đưa ra dưới đây.

C. Biện pháp Sparsity

Gần đây, các nghiên cứu về phương pháp nhận thức nén
xuất hiện với số lượng lớn và nhiều thí nghiệm cho thấy
rằng với tiền đề sử dụng cùng một phương pháp phân
tách tensor, phương pháp nhận thức nén có thể vượt
trội hơn hai phương pháp khác [147], [148], [145]. Do
đó, chúng ta nên chú ý nhiều hơn đến nó. Một điều
chưa được nghiên cứu đầy đủ là biện pháp sparsity được
sử dụng. Vì nuclear norm (biện pháp ℓ₁) là dạng thư
giãn lồi cổ điển nhất của hạng, nó được sử dụng rộng
rãi. Tuy nhiên, không có bằng chứng nào cho thấy nuclear
norm là một lựa chọn hoàn hảo. Do đó, một so sánh
giữa các biện pháp sparsity phổ biến nên được thực hiện.
Tìm ra một biện pháp hiệu quả hơn có thể cải thiện đáng
kể khả năng nén của các thuật toán nhận thức nén hiện có.

1) Biện pháp sparsity phổ biến
Đối với các bài toán biểu diễn thưa, chuẩn ℓ₀ được
định nghĩa là số lượng phần tử khác không là biện pháp
truyền thống của độ thưa. Tuy nhiên, vì chuẩn ℓ₀ nhạy
cảm với nhiễu và đạo hàm của nó không chứa thông
tin, chuẩn ℓₚ với 0 < p ≤ 1 được giới thiệu để ít xem
xét các phần tử nhỏ hơn [121]. Đối với một vector x ∈ Rᴺ,
chuẩn ℓₚ của nó có thể được công thức hóa như:

ℓₚ(x) = (∑ᵢ₌₁ᴺ |xᵢ|ᵖ)^(1/p)                    (17)

Chuẩn ℓ₁, chuẩn ℓₚ với p = 1, là biện pháp sparsity được
sử dụng rộng rãi nhất. Chính thức, xem xét một vector
x ∈ Rᴺ, chuẩn ℓ₁ của nó có thể được ký hiệu như:

ℓ₁(x) = ∑ᵢ₌₁ᴺ |xᵢ|                             (18)

Chuẩn ℓ₁ được giới thiệu trong [143] như một thay thế
thực tế hơn cho chuẩn ℓ₀. Ngoài ra, để đo tốt hơn độ
thưa trong dữ liệu nhiễu, các dạng linh hoạt hơn dựa
trên chuẩn ℓ₁ được đề xuất trong [9], [59], tức là sorted
ℓ₁ norm và two-level ℓ₁ norm. Sorted ℓ₁ norm được
công thức hóa như:

ℓ₁ˢᵒʳᵗ(x) = ∑ᵢ₌₁ᴺ λᵢ|x|₍ᵢ₎                      (19)

trong đó λ₁ ≥ λ₂ ≥ ... ≥ λₙ ≥ 0. Theo cách này, độ lớn
của phần tử càng cao, hình phạt lên nó càng lớn. Ngắn
gọn hơn, two-level ℓ₁ norm chỉ xem xét hai mức hình
phạt, có thể được công thức hóa như:

ℓ₁²ˡᵉᵛᵉˡ(x) = ∑ᵢ∈I₁ λ₁|xᵢ| + ∑ⱼ∈I₂ λ₂|xⱼ|        (20)

Ở đây, chúng ta có |xᵢ| ≥ |xⱼ|, ∀i ∈ I₁, ∀j ∈ I₂.

Chỉ số Gini ban đầu được đề xuất như một biện pháp
đo bất bình đẳng tài sản [24], [98]. Sau đó, tính hữu
dụng của chỉ số Gini như một biện pháp sparsity đã
được chứng minh trong [115], [60]. Cho một vector đã
sắp xếp x ∈ Rᴺ có các phần tử tăng dần, tức là x₁ ≤ x₂ ≤ ... ≤ xₙ,
chỉ số Gini của nó được cho bởi:

G(x) = (1/2N ∑ᵢ₌₁ᴺ xᵢ/||x||₁)(N-i+1-N/2)        (21)

Lưu ý rằng nếu tất cả các phần tử bằng nhau, tức là
không có sparsity, chỉ số Gini đạt mức tối thiểu 0. Đối
với trường hợp thưa nhất, tức là chỉ có xₙ khác không,
chỉ số Gini đi đến mức tối đa 1-1/N. Về mặt đồ họa,
chỉ số Gini có thể được biểu diễn như hai lần diện tích
giữa đường cong Lorenz [98] và đường 45 độ. Mỗi điểm
trên đường cong Lorenz (x=a, y=b) có cách hiểu là
100a phần trăm hàng đầu của các phần tử đã sắp xếp
biểu thị 100b phần trăm tổng công suất. Đường độ biểu
thị trường hợp ít thưa nhất với chỉ số Gini bằng 0.
Hình 6 minh họa đường cong Lorenz cho một vector.

Biện pháp Hoyer được thiết kế trong [55] như một biện
pháp sparsity mới, đây là một phiên bản chuẩn hóa của
ℓ₂/ℓ₁. Đối với một vector cho trước x ∈ Rᴺ, biện pháp
Hoyer của nó có thể được công thức hóa như:

H(x) = (√N - ||x||₁/||x||₂)/√(N-1)              (22)

Hàm này đạt đến đơn vị nếu và chỉ nếu x chứa chỉ một
thành phần khác không duy nhất, và nhận giá trị 0 nếu
và chỉ nếu tất cả các thành phần bằng nhau, thay đổi
một cách mượt mà giữa hai cực trị.

Biện pháp sparsity được đề cập ở trên có thể được áp
dụng cho vector giá trị đơn như một biện pháp hạng
thấp của ma trận tương ứng. Có những biện pháp không
nghiêm ngặt khác cho hạng của một ma trận. Ở đây,
chúng tôi tập trung vào hạng hiệu quả [116]. Hãy xem
xét một ma trận X có kích thước M×N có vector giá
trị đơn được ký hiệu bởi σ ∈ Rᴷ với K = min{M, N},
hạng hiệu quả của nó có thể được cho bởi:

E(X) = exp(∑ᵢ₌₁ᴷ σ̂ᵢ log σ̂ᵢ)                   (23)

trong đó σ̂ᵢ = σᵢ/||σ||₁, logarithm cơ số e, và quy ước
log 0 = 0 được áp dụng. Biện pháp này được tối đa hóa
khi tất cả các giá trị đơn bằng nhau, và được tối thiểu
hóa khi giá trị đơn tối đa lớn hơn nhiều so với các giá
trị khác.

2) So sánh
Trong phương pháp nhận thức nén, việc sử dụng regularizer
sparsity trên vector giá trị đơn để khuyến khích các ma
trận trọng số nằm trong không gian con hạng thấp là
phổ biến. Nuclear norm được sử dụng thường xuyên nhất.
Tuy nhiên, nó đơn giản chỉ làm cho mọi thứ gần hơn
với không, điều này không thân thiện với việc giữ năng
lượng của các ma trận trọng số. Do đó, chúng tôi ưa
thích các biện pháp khác khuyến khích các giá trị đơn
không đáng kể (có độ lớn nhỏ) đi về không nhưng giữ
các giá trị đáng kể (có độ lớn lớn) hoặc làm cho chúng
lớn hơn để duy trì năng lượng. Do đó, chúng tôi chọn
chỉ số Gini, Hoyer và hạng hiệu quả làm đối tượng tiềm
năng, và so sánh giữa chúng cùng với nuclear norm.

Chúng tôi thực hiện thí nghiệm so sánh trên ResNet32
được huấn luyện trên tập dữ liệu Cifar10. Chúng tôi sử
dụng SVD đơn giản nhất để nén mạng, và trong giai
đoạn huấn luyện nhận thức nén, chúng tôi sử dụng các
biện pháp sparsity khác nhau trên giá trị vector đơn của
mỗi ma trận trọng số, với một siêu tham số để cân bằng
giữa độ chính xác và hạng thấp. Sau quá trình huấn
luyện này, có nhiều giá trị đơn gần bằng không có thể
được đặt thành không mà không làm suy giảm hiệu suất.
Một chỉ báo phù hợp để xác định các giá trị đơn được
giữ lại được giới thiệu trong [17], tức là chỉ báo dựa
trên chuẩn phổ. Chỉ báo này được định nghĩa là tỷ lệ
của giá trị đơn lớn nhất bị loại bỏ với giá trị đơn tối
đa. Nó hiệu quả hơn so với chỉ báo dựa trên chuẩn Frobenius
bình thường [107], vì nó có thể loại bỏ sự can thiệp
gây ra bởi các giá trị đơn nhỏ và nhiễu.

[Hình 7. Độ chính xác trên Cifar10 so với tỷ lệ nén số lượng tham số trong ResNet32. Hiệu ứng của các biện pháp sparsity khác nhau.]

Hình 7 cho thấy hiệu ứng của bốn biện pháp sparsity.
Nuclear Norm được sử dụng thường xuyên nhất cho
thấy hiệu suất tệ nhất. Với sự gia tăng tỷ lệ nén, độ
chính xác giảm mạnh. Lý do đằng sau điều này có thể
là vào thời điểm theo đuổi tỷ lệ nén cao, giá trị của λ
được tăng lên, với nhiều giá trị đơn bị áp đặt về không.
Nó phá hủy đáng kể khả năng biểu đạt của mô hình.
Hình này cũng gợi ý rằng hạng hiệu quả vượt trội hơn
các biện pháp còn lại cho bất kỳ chế độ nén nào. Cụ
thể, khi độ chính xác gần 85%, hạng hiệu quả có thể
đạt được tỷ lệ nén gần như lớn hơn 4 lần so với nuclear
norm. Và trong trường hợp 90%, nó có thể đạt được
lớn hơn 2 lần so với Hoyer. Đối với chế độ nén thấp,
hạng hiệu quả cũng có tiềm năng lớn nhất để đạt được
độ chính xác gần với bản gốc.

Đối với chỉ báo dựa trên chuẩn phổ, nếu chúng ta cần
loại bỏ hầu hết các giá trị đơn để đạt được tỷ lệ nén
cao, có hai lựa chọn: tăng giá trị của giá trị đơn tối đa
hoặc giảm giá trị của các giá trị đơn nhỏ. Tuy nhiên,
tăng giá trị của giá trị đơn tối đa 10 lần khó hơn nhiều
so với giảm giá trị của các giá trị đơn nhỏ 10 lần. Do
đó, chúng tôi ưa thích một biện pháp có thể mạnh mẽ
khuyến khích các giá trị đơn nhỏ đạt đến 0. Đây cũng
là lý do tại sao hạng hiệu quả có thể thể hiện hiệu quả
lớn.

III. CÁC KỸ THUẬT TÍCH HỢP

Ngoài xấp xỉ hạng thấp, có các sơ đồ nén khác có thể
dẫn đến việc giảm đáng kể tham số với chi phí chỉ là
một sự sụt giảm nhỏ trong độ chính xác đầu ra, như
cắt tỉa [8], chia sẻ trọng số [16], sparsification [49] và
chưng cất kiến thức [48]. Không nghi ngờ gì, việc tích
hợp các kỹ thuật giảm tham số này, tức là tích hợp song
song, có thể tăng cường thêm hiệu quả nén. Trong khi
nhiều khảo sát gợi ý tích hợp các kỹ thuật nén khác
nhau, một cuộc thảo luận chi tiết về sự kết hợp giữa
xấp xỉ hạng thấp và các sơ đồ khác vẫn còn thiếu. Ngoài
ra, không chỉ việc giảm tham số mà còn việc giảm bit
để biểu diễn tham số có thể cắt giảm đáng kể độ phức
tạp cao, có thể được thực hiện bằng lượng tử hóa và
mã hóa entropy. Lượng tử hóa có thể biểu diễn mỗi tham
số với độ rộng bit thấp hơn, và mã hóa entropy có thể
sử dụng từ mã để mã hóa các ký hiệu nguồn. Cả hai
kỹ thuật đều trực giao với các phương pháp giảm tham
số trên. Do đó, chúng ta có thể trực tiếp sử dụng chúng
trên một mô hình nhỏ gọn để có được một biểu diễn
nhỏ gọn hơn, tức là tích hợp trực giao. Bảng VI liệt kê
các công trình đại diện của các loại tích hợp khác nhau,
và Bảng VII liệt kê liệu các kỹ thuật này có thể nén
hoặc tăng tốc mô hình hay không.

A. Tích hợp Song song

Trong phần này, chúng tôi sẽ đưa ra một cuộc khảo sát
toàn diện về cách tích hợp xấp xỉ hạng thấp với các
kỹ thuật nén song song khác, bao gồm cắt tỉa, chia sẻ
trọng số, sparsification và chưng cất kiến thức. Thông
qua việc sử dụng kết hợp, chúng ta có thể theo đuổi
khả năng nén cao hơn.

1) Tích hợp với Cắt tỉa
Cắt tỉa được sử dụng để tìm các kết nối không quan
trọng trong một mạng cấu trúc đầy đủ và sau đó loại
bỏ chúng, dẫn đến một cấu trúc nhỏ gọn mà không mất
mát đáng kể độ chính xác. Cắt tỉa có thể được phân
loại theo các mức độ chi tiết khác nhau, bao gồm mức
trọng số, mức bộ lọc và mức lớp. Mức trọng số là cách
tiếp cận linh hoạt nhất [49] và có thể đạt được chi phí
bộ nhớ thấp nhất bằng cách lưu trữ trong định dạng
ma trận thưa như Compressed Sparse Column (CSC)
[48]. Tuy nhiên, nó dẫn đến khó khăn trong suy luận
do cần xác định mỗi trọng số được giữ hoặc bỏ. Tức
là, cách tiếp cận này không thể tăng tốc suy luận hoặc
tiết kiệm dấu chân bộ nhớ trừ khi được hỗ trợ bởi phần
cứng [47]. Mức lớp nhằm loại bỏ các lớp tầm thường,
không phù hợp cho mạng nông [15]. Để khắc phục những
nhược điểm này, một cách tiếp cận linh hoạt và áp dụng
được hơn, tức là mức bộ lọc, được đề xuất [58]. Mức
bộ lọc xem xét mỗi bộ lọc như một đơn vị và loại bỏ
các bộ lọc không đáng kể để có được một mô hình nhỏ
gọn nhưng với cấu trúc thông thường. Lưu ý rằng đối
với hai lớp Conv liên tiếp, việc loại bỏ một bộ lọc trong
kernel đầu tiên dẫn đến việc loại bỏ kênh đầu vào trong
kernel tiếp theo.

Cắt tỉa bộ lọc không xử lý dư thừa trong một bộ lọc,
trong khi xấp xỉ hạng thấp có thể khắc phục điều này
bằng cách biểu diễn mỗi bộ lọc trong định dạng hạng
thấp. Do đó, việc kết hợp chúng để khám phá tỷ lệ nén
cao hơn là hứa hẹn. [41] đề xuất thực hiện cắt tỉa bộ
lọc trước và sau đó sử dụng phân tách Tucker trên các
kernel đã cắt tỉa. Thí nghiệm trong [41] cho thấy rằng
cách tiếp cận kết hợp có thể đạt được tỷ lệ nén cao hơn
57% so với một trong hai. [17] trao đổi thứ tự của cắt
tỉa bộ lọc và xấp xỉ hạng thấp vì các bộ lọc nhỏ hơn
thu được bằng xấp xỉ hạng thấp có thể giảm xác suất
loại bỏ các bộ lọc thiết yếu. Ngoài ra, các công trình
trước đây chỉ ra rằng cắt tỉa bộ lọc có khả năng cắt
tỉa nhiều bộ lọc hơn trong các lớp sâu hơn, dẫn đến
chi phí tính toán vẫn cao của toàn bộ mạng [101]. Nhưng
với sự giúp đỡ của xấp xỉ hạng thấp, các lớp nông cũng
có thể được nén. Sau đó, cả nén bộ nhớ và chi phí tính
toán ở mức cao đều có thể đạt được.

Một nhánh công trình có thể đạt được xấp xỉ hạng thấp
và cắt tỉa bộ lọc đồng thời thông qua regularizer. Trong
[4], regularizer nuclear norm và regularizer sparse group
Lasso [3] được kết hợp để làm cho các ma trận trọng
số không chỉ hạng thấp mà còn thưa nhóm. Sau đó lớp
gốc có thể được biểu diễn bởi hai lớp nhỏ hơn, tiếp
theo bởi việc loại bỏ các kênh đầu vào không đáng kể
của lớp đầu tiên và các kênh đầu ra của lớp thứ hai.
Khác với phương pháp này, [117] sử dụng một loại regularizer
để đạt được cả hai động cơ. Nó biểu diễn một ma trận
trọng số bằng một ma trận cơ sở và một ma trận hệ số.
Bằng cách áp đặt regularization ℓ₂,₁ cả trên ma trận
hệ số và chuyển vị của nó, ma trận cơ sở có thể trở
thành hạng thấp và các kênh đầu ra không đáng kể được
xác định. Hoặc, cũng có một số công trình sử dụng hai
kỹ thuật trên các mô-đun khác nhau của một mạng. Ví
dụ, nhắm đến kiến trúc Transformer, [71] nén các khối
attention bằng xấp xỉ hạng thấp và áp dụng cắt tỉa cho
các khối feedforward, mang lại sự tăng cường lớn.

2) Tích hợp với Sparsification
Sparsification trong DNN tập trung vào việc làm cho
các ma trận trọng số thưa hơn để tính toán ma trận thưa
có thể được sử dụng để giảm chi phí tính toán cao. Đồng
thời, nó có thể cung cấp hiệu quả lưu trữ, vì các phần
tử khác không và vị trí của chúng có thể được ghi lại
trong định dạng Compressed Sparse Row (CSR) [48]
hoặc Ellpack Sparse Block (ESB) [89]. Có hai loại sparsification,
tức là sparsity không thường xuyên và sparsity cấu trúc.
Khi các phần tử khác không được đặt ngẫu nhiên trong
ma trận, chúng ta gọi nó là sparsity không thường xuyên,
linh hoạt nhưng có thể dẫn đến tăng tốc kém do mẫu
truy cập dữ liệu không thường xuyên của nó. Ngược
lại, sparsity cấu trúc có thể đạt được các mẫu truy cập
dữ liệu thường xuyên. Cụ thể hơn, sparsity cấu trúc
thường làm cho một loạt các phần tử liên tục trong ma
trận bằng không.

Xấp xỉ hạng thấp phân tích một ma trận thành các thành
phần nhỏ hơn, nhưng các thành phần này vẫn chứa các
phần tử nhỏ có thể được làm bằng không mà không dẫn
đến sự gia tăng đáng kể trong lỗi xấp xỉ. Do đó, việc
kết hợp xấp xỉ hạng thấp và sparsification để đạt được
nén tốt hơn là hứa hẹn. Phân tích Thành phần Chính
Thưa (SPCA) [158] là một trường hợp nổi tiếng để tích
hợp phân tích với sparsity. Ý tưởng chính của SPCA
là làm cho mỗi thành phần chính chỉ chứa một vài đặc
trưng của dữ liệu, để SPCA có thể giải thích được hơn
so với PCA. Cũng có sparse HOSVD và sparse CP được
đề xuất trong [2].

Trong [86], đã cho thấy rằng sparsity cao đáng ngạc
nhiên có thể đạt được sau phân tách hai giai đoạn. Được
tuyên bố rằng hơn 90% tham số có thể được làm bằng
không với ít hơn 1% suy giảm độ chính xác trên tập
dữ liệu ILSVRC2012. Trong thuật toán này, sparsity
và hạng thấp được đạt được bằng cách sử dụng chuẩn
ℓ₁ và chuẩn ℓ₂,₁ tương ứng trên một ma trận hệ số.
Cuối cùng, nó chuyển đổi phép toán tích chập trong các
lớp Conv thành nhân ma trận thưa, giảm đáng kể chi
phí tính toán. Sparse SVD, tức là các ma trận nhân tử
trong SVD được làm thưa, được đề xuất trong [124],
vượt trội hơn truncated SVD. Theo quan điểm rằng một
phần của các nơ-ron đầu vào và đầu ra trong một lớp
có thể không đáng kể, các hàng tương ứng của ma trận
đơn trái và phải có thể được làm bằng không. Và xem
xét tầm quan trọng của các mục trong một hàng của
ma trận đơn trái hoặc phải giảm từ trái sang phải, Sparse
SVD ưa thích loại bỏ các mục gần bên phải. Sparsity
cấu trúc kết quả cho phép các thư viện BLAS [76] được
sử dụng để tăng tốc cao hơn.

Nhắm đến RNN, [139] đề xuất Low-Rank Structured
Sparsity. Xem xét tính bất biến chiều trong thời gian,
phương pháp này sử dụng regularization ℓ₁ trên ma
trận đơn trái và phải bắt nguồn từ SVD, dẫn đến một
ma trận thưa theo cột và theo hàng mà không méo mó
chiều.

3) Tích hợp với Chia sẻ Trọng số
Chia sẻ trọng số được định nghĩa là một thao tác chia
sẻ tham số qua các kết nối khác nhau trong DNN bằng
cách khai thác dư thừa. Để thiết kế một mạng phức tạp
hơn với khả năng trích xuất đặc trưng tốt hơn, việc sao
chép hoặc cải tạo một số mô-đun được thiết kế tốt trong
một mạng nông, và sau đó thêm các mô-đun mới vào
cuối, tạo ra một mạng sâu hơn là phổ biến. Một mạng
điển hình là ResNet nổi tiếng [77]. Do sự tương đồng
này, việc khám phá một biểu diễn nhỏ gọn hơn bằng
cách chia sẻ tham số qua các mạng con tương tự này
là hứa hẹn. Đối với xấp xỉ hạng thấp, tương tự, ý tưởng
chia sẻ các tensor nhân tử qua các phân tách tensor của
các tensor trọng số tương tự cũng có thể được áp dụng.

Một minh họa đơn giản về tích hợp với chia sẻ trọng
số có thể được tìm thấy trong [82], nơi một tập hợp
các cơ sở bộ lọc 3D được chia sẻ qua một số hoặc tất
cả các lớp tích chập. Việc tìm kiếm cơ sở tương đương
với xấp xỉ hạng thấp của tất cả các kernel hình ma trận
với một ma trận cơ sở chung.

Một số phương pháp phân tách tensor tự nhiên kết hợp
chia sẻ trọng số. Ví dụ, trong Phân tách Tensor dựa
trên Tích Bán tensor đã đề cập trước đây, STP có thể
tính toán một phép nhân giữa một vector x ∈ Rᴺ và
một vector trọng số w ∈ Rᴾ, dẫn đến một vector đầu
ra y ∈ Rᴺ/ᴾ. Các N/P mục trong mỗi khối của x chia
sẻ một tham số trọng số của W.

Ngoài ra, một nhánh công trình chia sẻ các tensor nhân
tử qua các phân tách tensor của các tensor trọng số trong
các lớp khác nhau. [106] đề xuất T-Basis, xây dựng
một tập hợp các tensor bậc 3. Đối với một tensor hình
dạng tùy ý, mỗi TR-core của nó có thể được biểu diễn
như một tổ hợp tuyến tính của T-Basis. Do đó, một biểu
diễn nhỏ gọn của DNN có thể được dẫn xuất. [123] đề
xuất coupled TT, chứa một thành phần chung và một
thành phần độc lập. Thành phần chung được biểu diễn
bởi các TT-core chung cho các khối mạng tương tự, trong
khi các thành phần độc lập trong định dạng TT khác
nhau từ các lớp khác nhau để duy trì đặc điểm của mỗi
lớp.

4) Tích hợp với Chưng cất Kiến thức
Chưng cất kiến thức [52] là một giải pháp hứa hẹn,
nhằm cung cấp một số kiến thức bổ sung được học từ
các mạng giáo viên (một hoặc nhiều mạng phức tạp)
vào một mạng học sinh (mạng đơn giản hơn nhiều).
Với sự giúp đỡ của một giáo viên, học sinh có thể đạt
được độ chính xác tương đương nhưng với chi phí bộ
nhớ và tính toán thấp hơn nhiều so với giáo viên. Để
q_s và q_t biểu thị các đầu ra softmax của mạng học
sinh và mạng giáo viên, tương ứng. Mạng học sinh sẽ
được huấn luyện thông qua việc căn chỉnh q_s và q_t.
Nhưng trong trường hợp q_t gần với mã một-nóng của
nhãn thật, thông tin chứa trong các giá trị nhỏ không
thể được chuyển đến học sinh. Do đó, một thủ thuật
có tên nhiệt độ [52] được sử dụng để làm mềm phân
phối của cả q_s và q_t.

Các mạng được nén bằng xấp xỉ hạng thấp cũng là một
mạng đơn giản hơn có thể học kiến thức từ phiên bản
chưa nén. Nói chung, các mạng phân tách được khôi
phục bằng cách đơn giản tinh chỉnh để tối thiểu hóa
hàm cross-entropy. Tuy nhiên, quá trình tinh chỉnh luôn
hội tụ chậm và không thể khôi phục độ chính xác gốc
tốt. Do đó, điều này nhấn mạnh nhu cầu huấn luyện
mạng nén với thông tin từ mạng tiền huấn luyện tương
ứng. Tuy nhiên, được chứng minh trong [39] rằng khó
huấn luyện một mạng học sinh sâu hơn mạng giáo viên
với chưng cất kiến thức do hiện tượng không mong muốn
của gradient biến mất. Do đó, một chuyển giao kiến thức
(KT) mới được đề xuất trong [85], căn chỉnh cả đầu
ra và phản hồi trung gian từ một mạng giáo viên (gốc)
đến mạng học sinh (nén) của nó. Thí nghiệm cho thấy
rằng nó vượt trội hơn tinh chỉnh thông thường và chưng
cất kiến thức, đặc biệt với tỷ lệ nén cao.

Tuy nhiên, phương pháp KT vẫn tốn thời gian và có
nhu cầu về một tập huấn luyện quy mô lớn được chú
thích đầy đủ, có thể không khả thi trong thực tế. Li
et al. [80] đề xuất một chưng cất kiến thức sửa đổi chỉ
yêu cầu một vài mẫu không nhãn. Nó thêm một lớp
Conv 1×1 ở cuối mỗi khối của mạng học sinh, và căn
chỉnh các đầu ra cấp khối của giáo viên và học sinh
bằng cách ước tính các tham số của lớp Conv 1×1 sử
dụng hồi quy bình phương tối thiểu. Vì số lượng tham
số trong các lớp Conv 1×1 tương đối nhỏ, chỉ cần một
vài mẫu là cần thiết. Nó cũng cho phép hội tụ mô hình
nhanh, do đó tiết kiệm nhiều thời gian để khôi phục
độ chính xác. Sau khi học, lớp Conv 1×1 sẽ được hợp
nhất vào lớp trước đó, mà không tăng số lượng tham
số.

B. Tích hợp Trực giao

1) Lượng tử hóa
Thao tác ánh xạ dữ liệu từ độ chính xác đầy đủ đến
độ chính xác giảm được gọi là lượng tử hóa. Trong giai
đoạn huấn luyện và suy luận của DNN, việc biểu diễn
trọng số và activation ở 32-bit là phổ biến. Tuy nhiên,
việc truyền dữ liệu ở 32-bit là một gánh nặng, và Multiply-
Accumulate (MAC) sẽ được vận hành giữa các giá trị
điểm nổi 32-bit. Ngoài ra, năng lượng tiêu thụ tăng tuyến
tính đến bậc hai với số bit được sử dụng. Do đó, việc
giảm độ chính xác là cần thiết để giảm kích thước bộ
nhớ, tăng tốc và tiết kiệm năng lượng.

Có một số lợi thế đặc biệt của việc áp dụng lượng tử
hóa trên mạng nơ-ron. Thứ nhất, so với dạng liên tục,
các biểu diễn rời rạc mạnh mẽ hơn với nhiễu [13], [36]
và tương tự hơn với cách lưu trữ thông tin trong não
người [132], [127]. Thứ hai, cả khả năng tổng quát hóa
cao [64], [74] và hiệu quả cao dưới tài nguyên hạn chế
[133] của các dạng rời rạc thực sự là những gì học sâu
cần. Thứ ba, các phương pháp nén thông thường, như
xấp xỉ hạng thấp, chia sẻ trọng số và cắt tỉa, tập trung
vào nén bộ nhớ hoặc tăng tốc để nó thiếu sót trong việc
đạt được tăng tốc và nén đáng kể đồng thời cho toàn
bộ mạng, trong khi lượng tử hóa có thể chinh phục thách
thức này. Ngoài ra, được hiển thị trong [84] rằng hầu
hết các trọng số và activation trong DNN gần bằng
không, có thể thúc đẩy đáng kể khả năng nén của lượng
tử hóa. Một cuộc khảo sát chi tiết hơn về việc triển khai
lượng tử hóa trên DNN có thể được tìm thấy trong [38], [102].

Một cách trực tiếp để kết hợp xấp xỉ hạng thấp và lượng
tử hóa là xem xét mạng được nén bằng phân tách tensor
như một mạng mới, có thể được nén thêm bởi các phương
pháp lượng tử hóa khác nhau một cách bình thường.
Tuy nhiên, vì đã có một lỗi xấp xỉ bắt nguồn từ phân
tách, việc lượng tử hóa tiếp theo sẽ phải chịu suy giảm
độ chính xác nghiêm trọng. Do đó, một phương pháp
tích hợp mới xem xét phân tách hạng thấp và lượng tử
hóa đồng thời thay vì liên tiếp có tiềm năng giải quyết
thách thức.

Ý tưởng này có thể được tìm thấy trong [68], nơi cả
các nhân tử của định dạng Tucker và activation đều
được lượng tử hóa, và với sự giúp đỡ của chưng cất
kiến thức, lỗi xấp xỉ được tối thiểu hóa. Trong [72],
lượng tử hóa được giới thiệu trong phân tích thành phần
chính (PCA), nơi ma trận thành phần và ma trận hệ số
được lượng tử hóa với độ rộng bit khác nhau. Cùng với
ràng buộc sparsity trên ma trận hệ số, lỗi xấp xỉ trên
đa tạp dữ liệu bắt nguồn từ phân tách hạng thấp, sparsity
và lượng tử hóa sẽ được tối thiểu hóa bởi một phương
pháp gradient descent chiếu lặp. Ngoài ra, có một số
cách tiếp cận trực tiếp mở rộng các thuật toán phân
tách tensor cơ bản thành các phân tách tensor với các
nhân tử lượng tử hóa. Ví dụ, quantized CP-ALS được
đề xuất trong [104], trong đó mỗi lần lặp tối ưu hóa
các nhân tử được lượng tử hóa, và được hiển thị rằng
lỗi tái tạo dưới ALS và quantized ALS gần như giống
nhau.

Các phương pháp được đề cập ở trên đều nhằm xấp
xỉ một tensor với các nhân tử lượng tử hóa, không phù
hợp với phương pháp tiền thiết lập. Trong [78], một
tensor train lượng tử hóa (QTT) được sử dụng để nén
mạng nơ-ron tích chập ba chiều. TT-Cores trong mạng
nơ-ron tensor hóa được lượng tử hóa trước, và sau đó
việc lượng tử hóa quá trình feedforward cũng được thực
hiện, đạt được suy luận nhanh hơn 3× so với chỉ sử
dụng TT.

2) Mã hóa Entropy
Mã hóa entropy là một sơ đồ nén không mất mát, mã
hóa các ký hiệu nguồn với số bit thấp hơn cho mỗi ký
hiệu bằng cách khai thác phân phối xác suất của nguồn
[113]. Mã hóa entropy ban đầu được áp dụng cho nén
dữ liệu được giới thiệu để giảm thêm kích thước bộ
nhớ của DNN lượng tử hóa bằng cách biểu diễn các
trọng số lượng tử hóa với từ mã nhị phân [48]. Nó sử
dụng mã hóa Huffman để tiết kiệm thêm 20% đến 30%
lưu trữ mạng mà không mất độ chính xác.

Mã hóa Huffman là một phương pháp tối ưu về mặt lý
thuyết để mã hóa các ký hiệu nguồn độc lập đa biến,
nhưng với điều kiện tiên quyết rằng các đặc tính thống
kê của các ký hiệu nguồn đã được biết. Có một vấn đề
với DNN rằng các đặc tính thống kê của trọng số được
tính bằng biểu đồ là chuẩn bị tốn thời gian và khác
nhau cho mỗi mạng, thậm chí cho một mạng được tinh
chỉnh. Do đó, một phương pháp mã hóa không cần thống
kê chính xác hiệu quả hơn để nén DNN.

Một nhánh công trình gọi là mã hóa vũ trụ, như các
biến thể của Lempel–ZivWelch [155], [156], [138] và
biến đổi Burrows–Wheeler [33], có thể được áp dụng
để giải quyết vấn đề này. "Vũ trụ" có nghĩa là phương
pháp mã hóa này có một mô hình xác suất tổng quát
có thể được điều chỉnh nhẹ cho một lớp rộng các nguồn
đầu vào. Trong ứng dụng, Deep Context-based Adaptive
Binary Arithmetic Coder (DeepCABAC) [140], một loại
mã hóa vũ trụ, được sử dụng để mã hóa trọng số trong
DNN. Đây là nỗ lực đầu tiên áp dụng các phương pháp
mã hóa video tiên tiến (CABAC) cho DNN. So với mã
hóa Huffman, DeepCABAC cũng có lợi thế về hiệu quả
throughput cao hơn.

Tuy nhiên, cả mã hóa Huffman và DeepCABAC đều là
các sơ đồ Fixed-to-Variable (F2V) trong đó số bit cho
mỗi ký hiệu là biến đổi. Do độ dài biến đổi trong từ
mã, nó không hiệu quả cho việc sử dụng bộ nhớ khi
giải mã, và do đó dẫn đến độ trễ cao cho suy luận. Thay
vào đó, mã hóa Tunstall [14], một phương pháp Variable-
to-Fixed (V2F), được thiết kế để cố định độ dài của
mỗi từ mã để chúng ta có thể xử lý nhiều bit đồng thời
và giải mã nhiều chuỗi mã hóa song song. Được báo
cáo rằng mã hóa Tunstall có thể đạt được giải mã nhanh
hơn khoảng 6× so với mã hóa Huffman.

IV. TỐI ƯU HÓA HẠNG THẤP CHO HUẤN LUYỆN KHÔNG GIAN CON

A. Hàm Hạng Thấp

Đối với một hàm thực có thể vi phân, nếu gradient của
nó luôn nằm trong một không gian con chiều thấp cố
định, nó có thể được gọi là một hàm hạng thấp [23].
Chiều của các không gian con như vậy thấp hơn nhiều
so với số lượng biến độc lập, và được gọi là hạng của
hàm. Hàm ridge là hàm hạng thấp phổ biến nhất, được
định nghĩa là các hàm có thể được chuyển đổi thành
một hàm đơn biến bằng cách áp dụng một phép biến
đổi affine cho đối số [96]. Do đó, gradient của hàm như
vậy cũng có thể được chiếu vào một đường thẳng. Ví
dụ, hàm hồi quy bình phương tối thiểu là một hàm ridge
cổ điển có thể được coi là một hàm hạng một. Tính chất
hạng thấp của hàm ridge làm cho chúng được sử dụng
rộng rãi trong thống kê cổ điển. Chúng được sử dụng
làm hàm hồi quy trong hồi quy theo đuổi chiếu để giải
quyết lời nguyền của chiều và nhiễu trong dữ liệu [31].
Trong tính toán khoa học, vì các biến của hàm để định
lượng không chắc chắn luôn tương quan, khái niệm
không gian con hoạt động có thể được sử dụng để tiết
lộ một tập hợp các biến độc lập có sự dao động có thể
dẫn đến thay đổi đáng kể nhất [22], [91].

Tính chất hạng thấp cũng được tìm thấy trong giai đoạn
huấn luyện của DNN. Trong DNN, số lượng tham số có
thể huấn luyện luôn nhiều hơn so với số mẫu huấn luyện.
Do đó, đối với loại mô hình tham số hóa quá mức này,
có thể đoán rằng có một phần lớn tham số sẽ không
thay đổi trong suốt giai đoạn huấn luyện. Tổng quát
hơn, có một giả thuyết rằng quỹ đạo huấn luyện của
tham số nằm trong một không gian con được xây dựng
bởi một vài biến không liên quan. Tức là, việc tối ưu
hóa hàng triệu tham số có thể tương đương với tối ưu
hóa trong một không gian con nhỏ. Cũng có bằng chứng
rằng gradient của các DNN khác nhau sẽ dần dần vẫn
trong một không gian con nhỏ được bao trùm bởi một
vài eigenvector hàng đầu của Hessian [45].

B. Huấn luyện Không gian con

Trong học sâu, thách thức mà quá trình huấn luyện hội
tụ rất chậm là một trở ngại khó khăn. Sự hội tụ chậm
được gây ra bởi phương pháp bậc nhất chiếm ưu thế,
tức là các phương pháp dựa trên gradient descent. Vấn
đề này có thể được làm dịu bởi các phương pháp bậc
hai sử dụng thông tin bắt nguồn từ ma trận Hessian.
Hơn nữa, phương pháp bậc hai không nhạy cảm với
tốc độ học, vì vậy không cần thiết kế lịch trình tốc độ
học cụ thể. Tuy nhiên, do các tham số khổng lồ trong
DNN, việc tính toán ma trận Hessian là một gánh nặng
tính toán. Một số cách tiếp cận như Adam [66], RMSprop
[25] và AdaGrad [32] sử dụng một phần thông tin bậc
hai, như momentum và thông tin tích lũy, đã vượt trội
hơn hiệu suất của các phương pháp dựa trên gradient
thông thường.

Để áp dụng các phương pháp bậc hai như phương pháp
quasi-Newton [10] cho huấn luyện mạng, cách trực tiếp
là giảm số lượng tham số cần được tối ưu hóa. Xem
xét cấu trúc hạng thấp được khám phá trong DNN, việc
tối ưu hóa toàn bộ mạng trong một không gian con bằng
phương pháp quasi-Newton, mà không mất độ chính
xác, là hứa hẹn. Phương pháp Quasi-Newton dựa trên
DLDR [81] được giới thiệu để tiết kiệm 35% thời gian
huấn luyện so với SGD [118]. Cụ thể, trong thuật toán
này, DLDR được thiết kế để xác định không gian con
chiều thấp được xây dựng trong một số hướng quan
trọng có thể đóng góp đáng kể vào phương sai của hàm
loss. Nó đạt được điều này bằng cách lấy mẫu quỹ đạo
huấn luyện và sau đó thực hiện Phân tích Thành phần
Chính (PCA) để phân tích các hướng chiếm ưu thế.
Sau đó, tối ưu hóa bậc hai có thể được thực hiện trực
tiếp trong không gian con nhỏ này, dẫn đến hội tụ nhanh.

C. Dư thừa Không gian và Dư thừa Thời gian

Trong khi nén mô hình khai thác dư thừa trong mạng
để giảm độ phức tạp bộ nhớ và tính toán, huấn luyện
không gian con khai thác dư thừa để giảm thời gian
huấn luyện. Nói cách khác, mục tiêu của nén mô hình
và huấn luyện không gian con là hiệu quả không gian
và hiệu quả thời gian, tương ứng. Vì cả hai đều khai
thác dư thừa, chúng tôi tự hỏi liệu dư thừa mà chúng
xử lý có cùng nguồn gốc hay không.

Chúng tôi phân tích điều này bằng cách thực hiện huấn
luyện không gian con trên các mạng xấp xỉ hạng thấp
để xác định liệu huấn luyện không gian con có hiệu
suất kém trên các mạng nén hay không. Nếu vậy, đó
là bằng chứng rằng dư thừa giảm bởi nén mô hình không
đủ cho huấn luyện không gian con, tức là tính chất
hạng thấp trong miền thời gian biến mất.

Ở đây, chúng tôi thực hiện một thí nghiệm đơn giản
trên LetNet-300-100 với tập dữ liệu MNIST. LeNet-300-100
chứa hai lớp ẩn kết nối đầy đủ với chiều đầu ra 300
và 100, và một lớp đầu ra với chiều 10. Chúng tôi áp
dụng SVD trên hai lớp đầu tiên và sau đó tinh chỉnh.
Chúng tôi ghi lại quỹ đạo huấn luyện và thiết lập một
không gian con 5D bằng cách thực hiện PCA. Để xem
liệu không gian con nhỏ như vậy có đủ hay không, chúng
tôi chiếu trọng số lên không gian con này và tính toán
lỗi xấp xỉ chuẩn hóa. Hình 8 cho thấy rằng khi hạng
giảm, lỗi chuẩn hóa tăng gần như tuyến tính. Nó gợi
ý rằng tỷ lệ nén càng cao, không gian con với chiều
cố định thấp như vậy càng ít phù hợp. Nói cách khác,
nén mô hình giảm dư thừa mà huấn luyện không gian
con có thể khai thác.

[Hình 8. Lỗi Chuẩn hóa so với hạng khi chiếu LeNet-300-100 nén SVD lên không gian con 5D. Lỗi chuẩn hóa là tỷ lệ của chuẩn ℓ₂ của lỗi giữa tham số gốc và tham số chiếu và chuẩn ℓ₂ của tham số gốc. Hạng liên quan đến SVD. 'base' là mạng chưa nén.]

Ngoài ra, chúng ta có thể hiểu rằng sau phân tách hạng
thấp, một không gian con chiều cao hơn là cần thiết.
Như được hiển thị trong Hình 9, việc tăng chiều của
không gian con có tác động lớn hơn đối với mạng nén
cao. Dưới tất cả các thiết lập hạng, lỗi chuẩn hóa đi
về không khi chiều bằng 12. Nhưng có một sự giảm
mạnh khi chiều được tăng từ 11 lên 12 đối với hạng=20.
Tức là, một sự giảm nhẹ về chiều là nghiêm trọng đối
với một mạng nén cao. Khi một mạng được nén cực
kỳ, có ít dư thừa trong miền thời gian.

[Hình 9. Lỗi Chuẩn hóa so với chiều của không gian con dưới các hạng khác nhau (các mức độ nén khác nhau). Chiều của không gian con dao động từ 5 đến 15.]

D. Tạo Cân bằng

Vì dư thừa được khai thác bởi nén mô hình và huấn
luyện không gian con có cùng nguồn gốc, có một sự
cân bằng giữa hiệu quả không gian và hiệu quả thời
gian. Nếu chúng ta phân bổ hầu hết dư thừa cho nén
mô hình, chúng ta có thể thu được một mạng nhỏ gọn
và do đó đạt được hiệu quả không gian, nhưng ít dư
thừa còn lại cho huấn luyện không gian con. Ngược
lại, nếu chúng ta cần huấn luyện một mạng nhanh chóng,
chúng ta nên hứa hẹn phân bổ hầu hết dư thừa cho
huấn luyện không gian con.

Đối với nén mô hình, việc huấn luyện một mạng nơ-ron
tensor hóa (TNN) tốn thời gian hơn nhiều so với mạng
gốc. Do đó, có nhu cầu sử dụng huấn luyện không gian
con để tăng tốc việc huấn luyện TNN. Trực quan, đối
với một TNN nén cao, vì có ít dư thừa, việc huấn luyện
TNN như vậy trong một không gian con nhỏ là không
hiệu quả. Hình 10 cho thấy hiệu suất của huấn luyện
không gian con khi được áp dụng cho TT-based TNN
với các chế độ nén khác nhau. Mạng cơ sở là ResNet32
được huấn luyện trên tập dữ liệu Cifar10. Tất cả các
thí nghiệm chạy 15 epoch (tiết kiệm 35% thời gian của
phương pháp SGD) với phương pháp Quasi-Newton và
không gian con được cố định ở 40D. Trong hình này,
đường màu cam (trường hợp mà TT-Net được huấn
luyện theo cách bình thường) gần như là một đường
ngang, nhưng đường màu xanh lá cây (được huấn luyện
trong không gian con) giảm mạnh vào thời điểm tỷ lệ
nén cao. Nó gợi ý rằng huấn luyện không gian con có
thể được kết hợp với nén mô hình để đạt được hiệu
quả không gian-thời gian dưới chế độ nén vừa phải,
nhưng không gian nhỏ như vậy không phù hợp cho một
mạng nén cực kỳ.

[Hình 10. So sánh sự suy giảm độ chính xác khi áp dụng huấn luyện không gian con cho TT-Nets. CR biểu thị tỷ lệ nén của kích thước mô hình.]

Do đó, dưới chế độ nén cực kỳ, việc tăng chiều của
không gian con để làm dịu sự suy giảm độ chính xác
là cần thiết. Nhưng không khả thi khi tăng chiều một
cách mù quáng, vì số epoch lấy mẫu cũng sẽ tăng, tức
là giảm hiệu quả thời gian. Hình 11 cho thấy hiệu ứng
của việc tăng chiều của không gian con cho một TT-Net
nén cao. Nó chứng minh rằng khi chiều của không gian
con tăng, sự suy giảm độ chính xác của huấn luyện
không gian con giảm. Khi chiều được tăng lên 55, chúng
ta có thể đạt được độ chính xác tốt gần với bản gốc,
nhưng đáng chú ý rằng tổng thời gian (thời gian cho
huấn luyện không gian con và để lấy mẫu) gần với thời
gian huấn luyện bình thường. Tuy nhiên, trong trường
hợp chúng ta muốn huấn luyện một TNN nhỏ gọn nhanh
chóng và một sự sụt giảm nhỏ về độ chính xác có thể
được chấp nhận, việc huấn luyện mạng như vậy trong
một không gian con vừa phải là một lựa chọn tốt.

[Hình 11. Hiệu ứng của việc tăng chiều của không gian con khi huấn luyện TT-Nets trong không gian con. Đường nét đứt đại diện cho độ chính xác của việc huấn luyện TT-Nets theo cách bình thường.]

V. KẾT LUẬN VÀ HƯỚNG TƯƠNG LAI

Trong bài báo này, hai loại tối ưu hóa tensor hạng thấp
cho học sâu hiệu quả được thảo luận, tức là xấp xỉ
hạng thấp cho nén mô hình và huấn luyện không gian
con để hội tụ nhanh. Đối với xấp xỉ hạng thấp, chúng
tôi liệt kê các phương pháp phân tách tensor hiệu quả
khác nhau và giới thiệu ba loại phương pháp tối ưu
hóa. Vì biện pháp sparsity được áp dụng thường xuyên
trong xấp xỉ hạng thấp, chúng tôi so sánh giữa các
biện pháp phổ biến, và thí nghiệm cho thấy rằng hạng
hiệu quả có thể đạt được sự đánh đổi độ chính xác-nén
tốt nhất. Ngoài ra, chúng tôi điều tra cách tích hợp
xấp xỉ hạng thấp với các kỹ thuật nén khác. Sau đó,
chúng tôi đưa ra giới thiệu ngắn gọn về huấn luyện
không gian con và phân tích rằng dư thừa được khai
thác bởi huấn luyện không gian con và xấp xỉ hạng
thấp có cùng nguồn gốc. Hơn nữa, chúng tôi thảo luận
về cách kết hợp cả hai để tăng tốc việc huấn luyện mạng
nơ-ron tensor hóa.

Tuy nhiên, cho đến nay, ít công trình tập trung vào việc
tích hợp hơn ba loại kỹ thuật nén giảm tham số, điều
này hứa hẹn hơn để tận dụng tối đa dư thừa trong mạng.
Hơn nữa, có thể thiết kế một khung linh hoạt để tích
hợp tất cả các loại kỹ thuật nén.

Trong thực tế, độ phức tạp tính toán thấp không tương
đương với độ trễ thấp [126], và năng lượng tiêu thụ
bởi tính toán chỉ là một phần nhỏ của tổng năng lượng
cho suy luận [54], [125]. Nhưng hầu hết các công trình
lấy FLOP và kích thước bộ nhớ làm điểm chuẩn. Tức
là, một thuật toán tiên tiến với độ phức tạp rất thấp
có thể không được áp dụng cho các thiết bị di động
chạy bằng pin. Do đó, cần nhiều nỗ lực hơn trong việc
giảm tiêu thụ năng lượng của DNN.

Đối với huấn luyện không gian con, hiệu quả thời gian
vẫn còn hạn chế, vì phương pháp Quasi-Newton vẫn
dựa trên gradient của hàng triệu tham số gốc. Tối ưu
hóa trực tiếp trên một số biến độc lập vẫn cần được
nghiên cứu. Ngoài ra, vì quy trình lấy mẫu chiếm hầu
hết thời gian huấn luyện, có nhu cầu giới thiệu các kỹ
thuật mới để xây dựng không gian con với ít epoch
mẫu hơn. Một cách tiềm năng là biểu diễn tất cả các
tham số trong định dạng tensor và áp dụng phân tách
tensor để phân tích tốt hơn các thành phần chính, tức
là PCA bậc cao hơn [67].

LỜI CẢM ƠN
Công trình được hỗ trợ bởi Quỹ Khoa học Tự nhiên
Quốc gia Trung Quốc (NSFC, Số 62171088), Quỹ Hợp
tác Y-Kỹ từ Đại học Khoa học và Công nghệ Điện tử
Trung Quốc (Số ZYGX2021YGLH215).

TÀI LIỆU THAM KHẢO
[Danh sách 158 tài liệu tham khảo được giữ nguyên như trong bản gốc]
