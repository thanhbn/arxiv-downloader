# LoRA Land: 310 Mô hình Ngôn ngữ Lớn được Fine-tune
bằng LoRA cạnh tranh với GPT-4, Báo cáo Kỹ thuật

Justin Zhao, Timothy Wang  
Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky,  
Piero Molino, Travis Addair, Devvret Rishi  
Predibase

## Tóm tắt

Low Rank Adaptation (LoRA) đã trở thành một trong những phương pháp được áp dụng rộng rãi nhất cho Parameter Efficient Fine-Tuning (PEFT) của các Mô hình Ngôn ngữ Lớn (LLMs). LoRA giảm số lượng tham số có thể huấn luyện và việc sử dụng bộ nhớ trong khi đạt được hiệu suất tương đương với fine-tuning toàn bộ. Chúng tôi nhằm mục đích đánh giá tính khả thi của việc huấn luyện và phục vụ các LLMs được fine-tune với LoRA trong các ứng dụng thực tế. Đầu tiên, chúng tôi đo lường chất lượng của các LLMs được fine-tune với các bộ điều chỉnh low rank được lượng tử hóa trên 10 mô hình cơ sở và 31 tác vụ với tổng cộng 310 mô hình. Chúng tôi thấy rằng các mô hình fine-tune LoRA 4-bit vượt trội hơn các mô hình cơ sở 34 điểm và GPT-4 10 điểm trung bình. Thứ hai, chúng tôi điều tra các mô hình cơ sở hiệu quả nhất cho fine-tuning và đánh giá khả năng tương quan và dự đoán của các phương pháp đánh giá độ phức tạp tác vụ trong việc dự báo kết quả fine-tuning. Cuối cùng, chúng tôi đánh giá khả năng độ trễ và đồng thời của LoRAX, một máy chủ suy luận Multi-LoRA mã nguồn mở tạo điều kiện triển khai nhiều mô hình fine-tune LoRA trên một GPU duy nhất sử dụng trọng số mô hình cơ sở được chia sẻ và tải bộ điều chỉnh động. LoRAX cung cấp năng lượng cho LoRA Land, một ứng dụng web lưu trữ 25 LLMs Mistral-7B fine-tune LoRA trên một GPU NVIDIA A100 duy nhất với bộ nhớ 80GB. LoRA Land làm nổi bật chất lượng và hiệu quả chi phí của việc sử dụng nhiều LLMs chuyên biệt so với một LLM đa năng duy nhất.

## 1 Giới thiệu

Fine-tuning các Mô hình Ngôn ngữ Lớn (LLMs) [23,3] là một cách hiệu quả cao để cải thiện hiệu suất của chúng, và thêm các hành vi mong muốn hoặc loại bỏ các hành vi không mong muốn [28,12,13,29]. Low Rank Adaptation (LoRA) [14] là một trong những phương pháp được áp dụng rộng rãi nhất cho fine-tuning LLMs, cho thấy tiềm năng đáng kể trong việc cho phép các mô hình nhỏ hơn, chuyên biệt vượt trội hơn các mô hình lớn hơn, tổng quát hơn trên các tác vụ cụ thể, với một phần nhỏ tham số có thể huấn luyện, thách thức quan niệm rằng các mô hình tổng quát lớn hơn luôn vượt trội hơn các mô hình nhỏ hơn.

Bất chấp sự tiến bộ nhanh chóng và phát hành các mô hình cơ sở mới, như Gemma [36], Llama [37], và Mistral [15], tuyên bố về việc fine-tuning dễ dàng trên các tác vụ khác nhau, các đánh giá toàn diện về những mô hình này vẫn còn khan hiếm. Các benchmark kiến thức rộng và dựa trên lý luận như MMLU [11] và HellaSwag [44] thường được sử dụng trong các bảng xếp hạng như Open LLM Leaderboard [2], tuy nhiên, điều này không nhất thiết đại diện cho hiệu suất cụ thể theo tác vụ, trước hoặc sau fine-tuning. Các báo cáo kỹ thuật [36,37,15,26,35] thường để lại các cấu hình huấn luyện không được chỉ định, với các tuyên bố về việc fine-tuning dễ dàng không được đo lường. Trong khi hiệu quả của fine-tuning đã được chứng minh rộng rãi [17,45], việc thiếu thử nghiệm quy mô lớn để lại một số câu hỏi quan trọng chưa được trả lời, đặc biệt là về tính nhất quán và khả năng dự đoán của các cải thiện hiệu suất thông qua fine-tuning, và tác động của kích thước mô hình, mô hình cơ sở, và độ phức tạp tác vụ.

Các đánh giá nhạy cảm với prompting, và có sự khác biệt đáng kể trong các công thức được sử dụng trong các ấn phẩm và thư viện¹. Các báo cáo kỹ thuật thường giới thiệu hiệu suất mô hình bằng cách sử dụng các chiến lược prompting chuyên biệt, cụ thể theo tập dữ liệu như prompt nhập vai (ví dụ: "Giả sử bạn là một chuyên gia"), maj@kvoting [40], n-shot biến đổi [34], MedPrompt [25], và chain-of-thought [43] prompting. Trong khi những phương pháp này nhằm mục đích làm nổi bật khả năng tối ưu của các mô hình, việc sử dụng các kỹ thuật prompting đa dạng như vậy có thể làm cho việc so sánh trực tiếp giữa các mô hình và tác vụ trở nên thách thức.

Trong công việc này, chúng tôi tìm cách thu hẹp những khoảng trống này bằng cách tiến hành một phân tích sâu rộng về fine-tuning dựa trên LoRA trên 10 mô hình cơ sở và 31 tác vụ, tổng cộng 310 LLMs được fine-tune với LoRA. Chúng tôi cố ý duy trì rằng tất cả LLMs đều được fine-tune với cùng các tham số huấn luyện và nhấn mạnh việc truy vấn với zero hoặc single-shot, các prompt kiểu hoàn thành, với các hướng dẫn đơn giản như "Giải quyết bài toán trắc nghiệm sau". Tổng thể, điều này cung cấp một khung tiêu chuẩn hóa để so sánh và đánh giá khả năng nội tại của các mô hình cơ sở khác nhau khi được fine-tune với LoRA dưới các điều kiện nhất quán, trên các tác vụ cụ thể.

Chúng tôi cũng nhằm mục đích khám phá tính khả thi của việc phục vụ nhiều mô hình LoRA trong một ứng dụng sản xuất thế giới thực. LoRAX [1] cho phép phục vụ nhiều mô hình LoRA cùng lúc trên một GPU duy nhất bằng cách tận dụng trọng số mô hình cơ sở được chia sẻ và tải bộ điều chỉnh động [12]. Chúng tôi đo lường các số liệu độ trễ và đồng thời của thư viện này. Chúng tôi sử dụng LoRAX để triển khai 25 LLM fine-tune được phục vụ trên một A100² duy nhất trong ứng dụng web LoRA Land. Việc triển khai thành công của chúng tôi giới thiệu hiệu quả kinh tế của việc phục vụ nhiều LLMs được điều chỉnh LoRA cho các tác vụ chuyên biệt.

Cuối cùng, chúng tôi phát hành tất cả 25 mô hình fine-tune trên ứng dụng web LoRA Land và các công thức huấn luyện của chúng trên (Hugging Face) để cho phép phân tích và tái tạo thêm bởi cộng đồng.

## 2 Công việc liên quan

**Các phương pháp Parameter-Efficient Fine-Tuning (PEFT)** được thiết kế để giảm chi phí cao của fine-tuning các mô hình quy mô lớn. Chúng đạt được điều này bằng cách huấn luyện một tập con tương đối nhỏ các tham số, so với tổng số tham số, để thích ứng với các tác vụ downstream. Các chiến lược PEFT hiện có có thể được chia thành hai loại: **Các phương pháp dựa trên Prompt** thêm các token mềm bổ sung (prompts) vào đầu vào ban đầu và chỉ tập trung vào fine-tuning các vector có thể huấn luyện này [19,31,42]. **Các phương pháp dựa trên Adapter** giới thiệu các mô-đun có thể huấn luyện bổ sung vào backbone đông lạnh ban đầu [12,32,30,33]. LoRA [14] mở rộng dựa trên fine-tuning dựa trên adapter bằng cách thêm một số lượng nhỏ các ma trận low-rank có thể huấn luyện cùng với các lớp trọng số đông lạnh, điều này giới thiệu một overhead suy luận có thể bỏ qua. Các biến thể của LoRA bao gồm các công việc như [22], sử dụng phân rã SVD để loại bỏ các giá trị singular ít quan trọng hơn để cập nhật hiệu quả hơn. Một biến thể khác, DoRA [21], phân rã trọng số pre-trained thành các thành phần độ lớn và hướng trong khi áp dụng LoRA cho thành phần sau. QLoRA [8] tối ưu hóa thiết kế của LoRA thêm một bước nữa, sử dụng trọng số NF4 4-bit, lượng tử hóa kép để giảm dấu chân bộ nhớ, và các tối ưu hóa phân trang để giảm bớt các đỉnh bộ nhớ. Trong các thí nghiệm của chúng tôi, chúng tôi tập trung vào việc triển khai gốc của LoRA với lượng tử hóa 4-bit.

**Phục vụ hiệu quả các mô hình LoRA.** Các thách thức chính cho việc phục vụ nhiều mô hình fine-tune hiệu quả là:

1. **Khả năng mở rộng**: Khi nhu cầu suy luận mô hình tăng lên, hệ thống phải mở rộng hiệu quả để xử lý tải tăng. Điều này bao gồm không chỉ mở rộng các tài nguyên tính toán mà còn quản lý phân phối tải giữa các mô hình để duy trì hiệu suất.

2. **Chi phí**: Các tài nguyên tính toán cần thiết để phục vụ nhiều mô hình fine-tune có thể dẫn đến chi phí đáng kể. Quản lý hiệu quả những chi phí này trong khi duy trì hiệu suất và sự sẵn sàng cao là một thách thức lớn.

Các kỹ thuật như Segmented Gather Matrix-Vector Multiplication (SGMV) [4] nhằm mục đích giải quyết những thách thức này bằng cách tối ưu hóa cách thức tính toán được thực hiện và tài nguyên được sử dụng. Các công cụ mã nguồn mở như DeepSpeed³, FasterTransformer⁴, và vLLM [18] cũng nhằm mục đích cho phép phục vụ các mô hình fine-tune hiệu quả về chi phí và có thể mở rộng. Trong bài báo này, chúng tôi sử dụng LoRAX⁵, được thiết kế đặc biệt cho việc phục vụ hiệu quả các LLMs được fine-tune với LoRA. LoRAX hỗ trợ tải bộ điều chỉnh động để các bộ điều chỉnh có thể được tải xuống một cách không đồng bộ trong quá trình suy luận, nhiều họ mô hình như Llama [37] và Mistral [15], và các mô hình được lượng tử hóa bitsandbytes⁶.

## 3 Phương pháp luận

### 3.1 Lựa chọn tác vụ

Trong việc lựa chọn các tập dữ liệu và tác vụ cho nghiên cứu của chúng tôi, chúng tôi ưu tiên những tác vụ có thể truy cập rộng rãi qua Kaggle⁷ và HuggingFace⁸ và những tác vụ thường được sử dụng cho benchmarking như những tác vụ trên Open LLM Leaderboard [2].

Lựa chọn của chúng tôi bao gồm các tập dữ liệu như MMLU [11] cho kiến thức miền rộng, Jigsaw [6] cho kiểm duyệt nội dung, WikiSQL [46] cho tạo SQL, và các benchmark GLUE [39]. Chúng tôi phân loại các tác vụ được bao gồm bởi các tập dữ liệu này thành 5 loại:

• **Classic NLP**: Các tác vụ được lấy từ các tập dữ liệu NLP phổ biến được xuất bản giữa năm 2018 và 2022 bao gồm các tác vụ như nhận dạng thực thể có tên, data-to-text, và tạo tiêu đề.

• **Coding**: Tạo truy vấn SQL, và câu hỏi lập trình Python, chủ yếu tập trung vào thuật toán và thiết kế hướng đối tượng.

• **Knowledge**: Câu hỏi trắc nghiệm dựa trên kiến thức.

• **Reasoning**: Câu hỏi trắc nghiệm dựa trên lý luận.

• **Math**: Các bài toán từ số học, toán học.

### 3.2 Lựa chọn prompt

Các mô hình được fine-tune "CNet Technology là một công ty Đài Loan chuyên sản xuất thiết bị mạng. Danh mục sản phẩm của họ bao gồm card mạng, switch, và modem. Những công nghệ này là các thành phần thiết yếu của mạng máy tính, cho phép truyền và nhận dữ liệu giữa các thiết bị. Các sản phẩm của CNet Technology đóng góp vào cơ sở hạ tầng cung cấp năng lượng cho thế giới số, cho phép giao tiếp và kết nối liền mạch." ### Title: CNet Technology ### Body: CNet Technology là một công ty Đài Loan sản xuất thiết bị mạng như card mạng, switch và modem. Bạn được cung cấp tiêu đề và nội dung của một bài viết dưới đây. Vui lòng xác định loại của bài viết. Các tùy chọn có thể của bạn là: "Company", "EducationalInstitution", "Artist", "Athlete", "OfficeHolder", "MeanOfTransportation", "Building", "NaturalPlace", "Village", "Animal", "Plant", "Album", "Film", "WrittenWork" ### Title: CNet Technology ### Body: CNet Technology là một công ty Đài Loan sản xuất thiết bị mạng như card mạng, switch và modem. Loại của bài viết là gì? Bạn được cung cấp tiêu đề và nội dung của một bài viết dưới đây. Vui lòng xác định loại của bài viết. Các tùy chọn có thể của bạn là: "Company", "EducationalInstitution", "Artist", "Athlete", "OfficeHolder", "MeanOfTransportation", "Building", "NaturalPlace", "Village", "Animal", "Plant", "Album", "Film", "WrittenWork" ### Title: CNet Technology ### Body: CNet Technology là một công ty Đài Loan sản xuất thiết bị mạng như card mạng, switch và modem. ### Article Type: Point-blank Prompting Instruction Prompting Completion Prompting Các mô hình tự động hoàn thành Các mô hình được điều chỉnh hướng dẫn "Company" "CNet tập trung vào việc cung cấp các giải pháp mạng và truyền thông sáng tạo để đáp ứng nhu cầu của một bối cảnh công nghệ phát triển nhanh chóng." "Loại của bài viết là 'Company'." "* [x] Company * [ ] EducationalInstitution * [ ] Artist" "Company" "Company" "Company" "Company"

**Hình 3**: Ví dụ về các kiểu prompting khác nhau. Để duy trì việc sử dụng cùng các prompt khi so sánh mô hình và để đảm bảo khả năng thành công cao nhất giữa tất cả các loại mô hình (fine-tuned, auto-complete, hoặc instruction-tuned), tất cả các prompt của chúng tôi đều tuân theo kiểu hoàn thành.

Các nghiên cứu trước đó đã chứng minh tiềm năng của việc tận dụng các kỹ thuật prompt engineering, như việc sử dụng majority voting [48], bao gồm nhiều ví dụ in-context (n-shot) [34], MedPrompt [25], chain-of-thought prompting [43], v.v., để nâng cao hiệu suất mô hình trên các tác vụ cụ thể.

Trong các đánh giá của chúng tôi, chúng tôi có ý thức lựa chọn **không** sử dụng các chiến lược prompt engineering hoặc tuning bổ sung cho bất kỳ tập dữ liệu, tác vụ, hoặc mô hình cụ thể nào. Mặc dù việc sử dụng nhiều ví dụ in-context hơn hoặc một cách tiếp cận có chọn lọc hơn trong n-shot prompting có thể mang lại kết quả tốt hơn, chúng tôi ưu tiên khả năng tái tạo và việc giảm thiểu các thiên vị có thể phát sinh từ việc học in-context tùy chỉnh. Thay vào đó, chúng tôi chọn sử dụng các prompt kiểu hoàn thành zero hoặc single-shot đơn giản cho tất cả các tác vụ. Các prompt của chúng tôi được viết theo kiểu hoàn thành, được mô tả trong Hình 3, để cung cấp một so sánh công bằng giữa các mô hình fine-tuned, instruction-tuned, và auto-complete. Đối với các tác vụ phân loại, prompt bao gồm tất cả các lớp có thể để thông báo cho các phản hồi của mô hình. Đối với các tác vụ chuyên biệt hơn, nơi mà việc mô tả định dạng đầu ra mong đợi là thách thức, chúng tôi sử dụng một ví dụ in-context duy nhất – ví dụ đầu tiên từ split huấn luyện được xuất bản – để hướng dẫn mô hình.

Cuối cùng, chúng tôi tuân theo các quy ước gắn thẻ prompt quy định cho từng mô hình, như được nêu trong tài liệu tương ứng của mô hình trên HuggingFace, để đảm bảo truy vấn thích hợp của các mô hình pre-trained và instruction-tuned. Điều này bao gồm việc sử dụng "<s>[INST] ... [/INST]" cho các prompt dành cho Mistral Instruct, và "<bos><start_of_turn>user ... <end_of_turn><start_of_turn><model>" cho các mô hình instruction-tuned của Gemma. Để biết thông tin chi tiết về các mẫu prompt chính xác được áp dụng cho từng tác vụ và mô hình, vui lòng xem Phụ lục A.

### 3.3 Mô hình cơ sở

Tất cả các mô hình cơ sở được liệt kê trong Bảng 2. Chúng tôi sử dụng GPT-4 (gpt-4-0613) và GPT-3.5-Turbo (gpt-3.5-turbo-0125) làm hai baseline LLM mạnh. Việc lựa chọn mười mô hình cơ sở này được hướng dẫn bởi một số cân nhắc chính, bao gồm việc chúng được áp dụng rộng rãi trong cộng đồng AI, tính sẵn sàng với các giấy phép cho phép, và tính sẵn sàng của các báo cáo kỹ thuật. Chúng tôi đặc biệt chọn các mô hình cơ sở với ≤8 tỷ tham số để đảm bảo rằng mỗi mô hình có thể được huấn luyện hiệu quả trong giới hạn tài nguyên của một GPU A10G duy nhất.

### 3.4 Tham số huấn luyện

Mỗi mô hình được huấn luyện với các split huấn luyện được xuất bản⁹. Mỗi mô hình được huấn luyện trong 40000 bước huấn luyện với batch size 1, lượng tử hóa 4-bit sử dụng bitsandbytes và LoRA rank là 8. Chúng tôi sử dụng tối ưu hóa **pagedadam** [8], **learning rate** 0.002, và **cosine learning rate scheduler** với phần warm-up 0.03 (1200 bước huấn luyện). Gradients được áp dụng qua 16 bước tích lũy cho batch size hiệu quả là 16.

Những tham số huấn luyện này, kết hợp với gradient checkpointing, cho phép mỗi LLM được fine-tune trên một GPU A10 duy nhất với bộ nhớ 24 GB. Đối với các tác vụ mà việc huấn luyện trên độ dài chuỗi đầy đủ vẫn tạo ra lỗi GPU Out-Of-Memory (OOM), chúng tôi đầu tiên cắt ngắn các đầu vào ví dụ đến độ dài chuỗi tối đa được đặt là phân vị thứ 95 của tất cả các đầu vào tác vụ.

Để đảm bảo một cơ sở so sánh nhất quán và đơn giản trên các mô hình, không có điều chỉnh siêu tham số bổ sung nào được áp dụng cho bất kỳ tập dữ liệu, tác vụ, hoặc mô hình cơ sở cụ thể nào.

Các công thức huấn luyện cho mỗi mô hình được cung cấp dưới dạng cấu hình Ludwig [24] cho mỗi LLM fine-tuned và có thể được tìm thấy tại https://huggingface.co/predibase. Hình 4 cho thấy một ví dụ về cấu hình.

### 3.5 Đánh giá

Như được chỉ định trong Bảng 2, các mô hình được đánh giá trên split test nếu nó tồn tại và được gán nhãn, hoặc validation set nếu không¹⁰. Chúng tôi sử dụng một bộ số liệu đánh giá được điều chỉnh để đánh giá chính xác hiệu suất trên tất cả các tác vụ. Chúng tôi sử dụng accuracy cho các tác vụ phân loại, (1 - mean absolute error) cho các tác vụ hồi quy¹¹, và rouge-L¹² cho các tác vụ tạo. Tập dữ liệu WikiSQL có bộ đánh giá riêng, tuy nhiên do những thách thức trong việc tích hợp bộ đánh giá WikiSQL, chúng tôi đã áp dụng số liệu ROUGE làm proxy để đánh giá chất lượng truy vấn¹³. Đối với coding, chúng tôi sử dụng HumanEval [5]. Đối với GSM8K [7], một heuristic dựa trên regex [9] được sử dụng để trích xuất câu trả lời toán học để nhất quán với Open LLM Leaderboard [2]. Tất cả các số liệu đều trên thang điểm 0 đến 1, trong đó 0 là điểm tệ nhất có thể, và 1 là điểm tốt nhất có thể.

Các mô hình không được fine-tune thường tạo ra các đầu ra đa dạng hơn, bao gồm các artifact không mong muốn như từ ngữ bổ sung hoặc giải thích không được chỉ định trong prompt. Đối với các tác vụ phân loại, đôi khi những mô hình này sẽ tạo ra chuỗi lớp thực tế như "Yes/No", "positive/negative" hoặc "True/False" được viết ra, thay vì nhãn "1/0" thực trong tập dữ liệu ngay cả khi được hướng dẫn. Để giảm thiểu các khoản khấu trừ số liệu do tính nghiêm ngặt của việc phân tích phản hồi, chúng tôi đầu tiên sử dụng một bước trích xuất dựa trên regex để ánh xạ phản hồi của mô hình vào từ vựng ground truth. Nếu có nhiều khớp trong văn bản được tạo, khớp hợp lệ đầu tiên được sử dụng. Mã cho việc trích xuất phản hồi dựa trên regex trước số liệu có sẵn tại github.com/predibase/lora-bakeoff.

Các ràng buộc tài chính liên quan đến API LLM không phải là không đáng kể. Ví dụ, việc sử dụng GPT-4 để đánh giá bộ test WikiSQL hoàn chỉnh gồm 15.878 ví dụ sẽ tốn khoảng $400, xem xét số lượng token đầu vào trung bình (805) và đầu ra (16) trên mỗi ví dụ. Những chi phí như vậy có thể bị cấm, đặc biệt là đối với các tổ chức hoặc nhà nghiên cứu hoạt động với ngân sách hạn chế. Để quản lý chi phí trong khi duy trì sự nghiêm ngặt, chúng tôi hạn chế các đánh giá đến **1000 ví dụ đầu tiên** cho các tập dữ liệu có split đánh giá lớn hơn 1000 ví dụ. Chúng tôi thừa nhận rằng phương pháp này có thể giới thiệu thiên vị lựa chọn và ảnh hưởng đến khả năng tổng quát hóa của những phát hiện của chúng tôi. Chúng tôi khuyến nghị rằng nghiên cứu tương lai nên xem xét các đánh giá mở rộng hơn khi tài nguyên cho phép.

## 4 Kết quả

Fine-tuning LoRA cung cấp một tăng cường nhất quán và đáng kể từ fine-tuning trên các mô hình cơ sở và tác vụ, như được thấy trong Hình 5. Trước fine-tuning, GPT-4 và GPT-3.5 có hiệu suất mạnh nhất ngay từ đầu so với tất cả các mô hình cơ sở khác, với điểm tổng thể 0.599 và 0.661, tương ứng. Tăng cường hiệu suất từ fine-tuning dao động từ +26.3 đến +51.2 điểm cải thiện tùy thuộc vào mô hình cơ sở, và +38.7 trung bình (Bảng 3). Tùy thuộc vào tác vụ, LLM fine-tuned tốt nhất vượt trội hơn mô hình cơ sở tốt nhất từ +8.3 đến +67.5 điểm, +25.0 điểm trung bình (Bảng 4).

Sau fine-tuning, 301/310 mô hình vượt qua đối tác mô hình cơ sở của chúng¹⁴, trong khi 224/310 LLMs fine-tuned vượt qua benchmark được đặt bởi GPT-4 (Bảng 4). Gemma-2b là mô hình cơ sở có hiệu suất tệ nhất sau fine-tuning, nhưng cũng trải nghiệm sự nâng cao lớn nhất từ fine-tuning tổng thể, điều này gợi ý rằng các mô hình có điểm ban đầu thấp hơn có khả năng hưởng lợi nhiều nhất từ fine-tuning (Hình 1).

Theo tổng thể trung bình trên tất cả các tác vụ, tất cả các mô hình fine-tuned hoạt động tốt hơn GPT-3.5, và tất cả các mô hình fine-tuned 7B hoạt động tốt hơn GPT-4, ngoại trừ gemma-7b và gemma-7b-it. Phi-2, với chỉ 2 tỷ tham số, thể hiện hiệu suất cạnh tranh với GPT-4 sau fine-tuning, nhất quán với những phát hiện của báo cáo kỹ thuật Phi-2 [46].

Trung bình trên 31 tác vụ, hiệu suất tổng thể của các LLMs fine-tuned tốt nhất (0.756) cao hơn đáng kể so với GPT-4 (0.661) (Bảng 4). Một phân tích chi tiết về hiệu suất theo mô hình, theo tác vụ, có thể được tìm thấy trong Phụ lục C.

## 5 Thảo luận và Phân tích

### 5.1 Mô hình Cơ sở nào là tốt nhất cho Fine-tuning LoRA?

Mistral-7B và Zephyr-7b-beta nổi lên như những người dẫn đầu, mặc dù trong các danh mục khác nhau. Mistral-7B thường xuyên đạt được hiệu suất hàng đầu trên số lượng tác vụ nhiều nhất (10/31), gợi ý khả năng thích ứng cao (Hình 6). Ngược lại, Zephyr có hiệu suất trung bình tổng thể cao nhất (0.731). Mistral-7b, Mistral-7b-instruct, và Zephyr-7b-beta (bản thân nó dựa trên Mistral-7b-instruct [38]) dẫn đầu nhóm cho hiệu suất fine-tuning LoRA, trước các họ Llama, Phi, và Gemma.

### 5.2 Kích thước có quan trọng cho fine-tuning LoRA không? 2B vs. 7B

Mô hình Phi-2 với 2B tham số, sau fine-tuning, vượt trội hơn tất cả các mô hình Gemma 2B và 7B theo tổng thể trung bình, và chỉ kém 1.9 điểm so với mô hình 7B có hiệu suất cao tiếp theo, Llama-2-7b (0.677 vs. 0.696). Bất chấp điều này, chúng tôi thấy rằng các mô hình fine-tuned 7B hầu như luôn tốt hơn các mô hình fine-tuned 2B (29/31 tác vụ). Trong số các mô hình 2B tham số đặc biệt (Phi và Gemma), chúng tôi thấy rằng tất cả các mô hình instruct Gemma đều tốt hơn Phi ngay từ đầu, tuy nhiên, Phi-2 hoạt động tốt hơn tất cả các mô hình Gemma khác sau fine-tuning.

### 5.3 Fine-tuning có tốt hơn với các mô hình Instruction-tuned hay Auto-complete không?

Trong Hình 7, chúng tôi quan sát rằng trước fine-tuning, các mô hình instruction-tuned vượt trội hơn các mô hình auto-complete, mặc dù sử dụng các prompt kiểu hoàn thành. Một phân tích định tính cho thấy rằng các mô hình auto-complete có nhiều khả năng "đi chệch hướng" hơn, và tạo ra các chuỗi văn bản dài không liên quan, và các mô hình instruction-tuned thể hiện tính nhất quán cao hơn trong việc thử thách đúng tác vụ sắp tới.

Sau fine-tuning, sự khác biệt hiệu suất giữa các mô hình thu hẹp. Mô hình instruction-tuned trung bình vượt trội hơn một chút so với mô hình auto-complete trung bình với biên độ +0.009, tuy nhiên điều ngược lại là đúng khi so sánh mô hình instruction-tuned fine-tuned tốt nhất và mô hình auto-complete fine-tuned tốt nhất (-0.002). Các mô hình auto-complete, có thể do cơ sở kiến thức rộng hơn và ít chuyên biệt hơn, có thể vốn dĩ thích ứng hơn với nhiều tác vụ khác nhau. Tuy nhiên, với fine-tuning đầy đủ, cả hai loại mô hình đều đạt được mức độ hiệu suất tương đương. Chúng tôi khuyến khích nghiên cứu thêm để khám phá cách thiết kế cơ bản của các mô hình instruction-tuned ảnh hưởng đến khả năng thích ứng và hiệu quả của chúng trong fine-tuning cụ thể theo tác vụ.

### 5.4 Khi nào GPT-4 liên tục vượt trội hơn các mô hình fine-tuned?

Chúng tôi quan sát một lợi thế riêng biệt cho các LLMs fine-tuned trên các tác vụ có phạm vi hẹp, như những tác vụ trong các benchmark GLUE. Những tác vụ này, chủ yếu hướng đến phân loại, thấy các LLMs fine-tuned đạt được gần 90% độ chính xác, vượt trội hơn GPT-4. GPT-4 tiếp tục vượt trội hơn các mô hình fine-tuned trong 6 trong số 31 tác vụ, đặc biệt là trong các miền rộng hơn, phức tạp hơn như coding Python và MMLU.

### 5.5 Định lượng mối quan hệ giữa sự nâng cao chất lượng fine-tuning và độ phức tạp tác vụ

Nếu các mô hình fine-tuned hoạt động tốt hơn trên các tác vụ "hẹp" chuyên biệt và tệ hơn trên các tác vụ "rộng" hơn, chúng ta có thể thiết lập một mối quan hệ dự đoán giữa độ phức tạp của một tác vụ và hiệu quả của fine-tuning LoRA không? Việc xác định một mối quan hệ như vậy có thể cung cấp một công cụ dự đoán có giá trị để đánh giá những lợi ích tiềm năng của các cải tiến fine-tuning trên các tác vụ mới trước khi quá trình fine-tuning bắt đầu.

#### 5.5.1 Heuristics cho chất lượng fine-tuning, nâng cao chất lượng, và độ phức tạp tác vụ

Để định lượng độ phức tạp tác vụ, chúng tôi sử dụng một số heuristics:

• **Số lượng ví dụ huấn luyện**
• **Độ dài của đầu vào và đầu ra** (µ, σ, và phân vị thứ 95).
• **Khả năng nén**¹⁵ (µ và σ)
• **Tính đa dạng của nội dung**, mà chúng tôi ước tính bằng cách đo lường sự tương tự rouge-L giữa đầu vào và đầu ra) [41] (µ và σ).

Đối với các phép đo chất lượng mô hình, chúng tôi theo dõi:

• **Điểm GPT-4 cơ bản**
• **Nâng cao từ mô hình fine-tuned tốt nhất so với GPT-4** ("Max GPT-4 Lift")
• **Nâng cao fine-tuning trung bình so với mô hình cơ sở**
• **Điểm mô hình cơ sở tốt nhất mà không fine-tuning**
• **Điểm mô hình cơ sở trung bình mà không fine-tuning**
• **Điểm mô hình fine-tuned tốt nhất**
• **Điểm mô hình fine-tuned trung bình**

Tham khảo Bảng 5 để có một ví dụ hoàn chỉnh.

#### 5.5.2 Tương quan chất lượng fine-tuning và nâng cao chất lượng với độ phức tạp tác vụ

Chúng tôi tìm thấy một số tương quan thú vị gợi ý các tương tác đáng kể giữa các heuristics độ phức tạp tác vụ của chúng tôi và các phép đo hiệu suất mô hình. Những quan sát chính bao gồm:

• **Khả năng nén** thể hiện ảnh hưởng kép, tương quan tích cực với cả điểm mô hình cơ sở tốt nhất và trung bình (0.36), trong khi tương quan tiêu cực với những điểm này khi phương sai trong khả năng nén tăng (-0.37). Điều này cho thấy rằng trong khi khả năng nén đồng nhất hỗ trợ hiệu suất mô hình, tính biến đổi cao hơn trong khả năng nén có xu hướng làm giảm nó.

• **Độ dài Đầu vào và Đầu ra**: Độ dài đầu ra dài hơn và đa dạng hơn tương quan tích cực với sự nâng cao tối đa từ fine-tuning GPT-4, gợi ý rằng các tác vụ có đầu ra mở rộng và đa dạng hơn không có hại cho fine-tuning. Ngược lại, độ dài đầu vào và đầu ra dài hơn và đa dạng hơn tương quan tiêu cực với điểm mô hình cơ sở và fine-tuned tuyệt đối.

• **Sự Tương tự Rouge-L Đầu vào và Đầu ra**: Độ lệch chuẩn cao hơn trong sự tương tự Rouge-L đầu vào/đầu ra tương quan tiêu cực với cả điểm mô hình cơ sở và fine-tuned. Điều này gợi ý rằng tính biến đổi lớn hơn trong sự tương tự nội dung trong một tập dữ liệu có thể gây ra khó khăn cho việc học mô hình.

• **Số lượng ví dụ huấn luyện**: Không có tương quan đáng kể nào được tìm thấy với số lượng ví dụ huấn luyện, chỉ ra khả năng rằng một khi đạt được kích thước mẫu đủ, các ví dụ bổ sung không nhất thiết đóng góp vào việc cải thiện hiệu quả fine-tuning.

• **Các tương quan liên quan chất lượng mô hình** tiết lộ rằng điểm trung bình tốt hơn (cả cơ sở và fine-tuned) dự đoán mạnh mẽ điểm tốt nhất đạt được, gợi ý một tính nhất quán chung trong hiệu suất mô hình trên các trường hợp huấn luyện khác nhau.

Tổng thể, những quan sát này nhất quán với giả thuyết của chúng tôi rằng các tác vụ hẹp hơn dễ dàng hơn có nhiều khả năng thấy thành công với các bộ điều chỉnh fine-tuned.

#### 5.5.3 Dự đoán chất lượng fine-tuning và nâng cao chất lượng dựa trên heuristics độ phức tạp tác vụ

Chúng tôi huấn luyện các mô hình hồi quy tuyến tính để dự đoán sự nâng cao chất lượng có thể đạt được thông qua fine-tuning dựa trên adapter, sử dụng các heuristics độ phức tạp tập dữ liệu được chuẩn hóa z-score (được mô tả trong Bảng 5) làm các yếu tố dự đoán. Kết quả được tóm tắt trong Bảng 6, nơi chúng tôi thấy rằng các mô hình tuyến tính mang lại lỗi root mean squared (RMSE) từ 0.166 đến 0.092, tùy thuộc vào số liệu chất lượng mô hình được đề cập.

Việc kết hợp điểm của mô hình cơ sở trung bình mà không fine-tuning như một tính năng bổ sung cải thiện độ chính xác dự đoán cho tất cả các số liệu chất lượng mô hình (+0.004 đến +0.069). Điều này chứng minh một số khả năng dự đoán trong việc biết hiệu suất mô hình cơ sở để dự đoán những lợi ích tiềm năng từ fine-tuning. Lỗi RMSE khá thấp, gợi ý rằng các phép đo dựa trên heuristics trước về độ phức tạp tập dữ liệu có thể là chỉ số hợp lý về tác động fine-tuning tích cực.

## 6 Các Benchmark Hiệu suất của Triển khai LoRAX

Để đánh giá tính khả thi của việc phục vụ nhiều LLMs fine-tuned LoRA cùng lúc trong một ứng dụng thế giới thực, chúng tôi khởi chạy LoRA Land. LoRA Land là một ứng dụng web phục vụ 25 LLMs Mistral-7b fine-tuned được phục vụ cho hàng nghìn người dùng từ một GPU A100 duy nhất.

### 6.1 LoRAX tóm tắt

LoRA Exchange (LoRAX) [1] là một máy chủ suy luận Multi-LoRA mã nguồn mở được thiết kế đặc biệt để phục vụ nhiều mô hình fine-tuned cùng lúc sử dụng một tập hợp tài nguyên GPU được chia sẻ. So với các triển khai LLM chuyên dụng thông thường, LoRAX bao gồm ba thành phần mới:

• **Dynamic Adapter Loading**, cho phép mỗi tập hợp trọng số LoRA fine-tuned được tải từ storage just-in-time khi các yêu cầu đến trong runtime, mà không chặn các yêu cầu đồng thời.

• **Continuous Multi-Adapter Batching**, một chính sách lập lịch công bằng để tối ưu hóa throughput tổng hợp của hệ thống mở rộng chiến lược batching liên tục phổ biến để hoạt động trên nhiều tập hợp bộ điều chỉnh LoRA song song.

• **Tiered Weight Caching**, để hỗ trợ trao đổi nhanh các bộ điều chỉnh LoRA giữa các yêu cầu, và offloading trọng số bộ điều chỉnh đến CPU và disk để tránh lỗi out-of-memory.

### 6.2 Kết quả Benchmarking

Chúng tôi chạy các benchmark để hiểu tác động của việc phục vụ nhiều bộ điều chỉnh trên các số liệu liên quan, được mô tả dưới đây. Chúng tôi cũng kiểm tra khả năng mở rộng của hệ thống đối với các yếu tố sau:

• **Số lượng người dùng đồng thời gửi prompt LLM**
• **Số lượng bộ điều chỉnh được truy vấn đồng thời**
• **Số lượng token đầu vào**
• **Số lượng token đầu ra**

Các số liệu hiệu suất phục vụ LLM bao gồm: thời gian đến token đầu tiên (TFTT), tổng thời gian yêu cầu, thời gian streaming token, và throughput (token per second). Chúng tôi chạy các benchmark của mình từ một instance EC2 t3.2xlarge trong zone AWS us-west-2. Tất cả các benchmark đều dựa trên LLM Mistral-7b-instruct, được triển khai trên một GPU A100 với RAM 80GB. Script được sử dụng để benchmark hiệu suất phục vụ LLM có thể được tìm thấy trong Phụ lục B.

Sau đây là tóm tắt về thuật ngữ liên quan:

• **Tổng thời gian yêu cầu (ms)**: tổng thời gian từ khi yêu cầu được gửi đến khi token cuối cùng được stream trở lại cho client.
• **Thời gian đến token đầu tiên, TTFT (ms)**: thời gian từ khi yêu cầu được gửi đến khi token đầu tiên được client nhận
• **Thời gian streaming token (ms)**: thời gian từ khi token đầu tiên được client nhận đến khi token cuối cùng được nhận
• **Throughput (token/s)**: số token được tạo ra mỗi giây, được tính bằng (Thời gian streaming token (ms) / số token đầu ra)
• **Người dùng đồng thời**: số người dùng thực hiện yêu cầu đến LLM, đợi cho đến khi họ nhận được phản hồi đầy đủ, sau đó thực hiện yêu cầu khác cho đến khi kết thúc thời gian kiểm tra.

### 6.3 Độ trễ từ chuyển đổi adapter và người dùng đồng thời

Các benchmark được báo cáo sau đây đến từ các lần chạy 2 phút liên tục stream các yêu cầu đến triển khai LLM. Các thí nghiệm của chúng tôi cho thấy rằng khoảng thời gian hai phút cung cấp một khối lượng dữ liệu đầy đủ để có được các số liệu ổn định và đáng tin cậy.

Bảng 7 cho thấy tác động của hiệu suất truy vấn LLM được cô lập đến cơ chế chuyển đổi adapter. Trong trường hợp multi-adapter, multi-user, chúng tôi thấy rằng thời gian streaming token là giống nhau, nhưng tổng thời gian yêu cầu khác nhau 7.21ms điều này minh họa chi phí của việc xử lý yêu cầu từ 100 người dùng đồng thời dẫn đến chuyển đổi giữa 25 bộ điều chỉnh.

Để mô phỏng các tải trọng lưu lượng thực tế, chúng tôi tạo ra các tải trọng ngẫu nhiên với 30-500 token đầu vào và 1-120 token đầu ra, được mô hình hóa từ các tác vụ được xác định trong Bảng 2. Chúng tôi thay đổi số lượng người dùng đồng thời từ 1 đến 50, và các tải trọng được phát hành ngẫu nhiên giữa 25 endpoint bộ điều chỉnh khác nhau.

Khi mở rộng từ 1 đến 50 người dùng đồng thời, điều này cũng tăng tải 50 lần, thời gian trung bình đến token đầu tiên (TTFT) bị ảnh hưởng nhẹ (+21.84ms hoặc tăng 17.9%). Chúng tôi thấy giảm throughput 3.46 lần cho cùng một mức tăng tải 50 lần.

Bảng 9 cho thấy rằng không có sự khác biệt đáng kể giữa việc truy vấn LLM cơ sở so với 25 bộ điều chỉnh khi nói đến TTFT hoặc throughput. Chi phí của việc chuyển đổi adapter bị che khuất bởi thời gian cần thiết để tạo token một khi yêu cầu đến. So sánh số liệu trường hợp trung bình và số liệu p90 cho TTFT, sự khác biệt lớn nhất là giữa 121.8ms (trung bình) và 195.95ms (p90) cho mức tăng 60.87%. Ngoài ra, chúng tôi liên tục thấy rằng TTFT ở mức hoặc dưới mốc 200ms.

Về throughput, chúng tôi quan sát rằng cần khoảng 12 đến 13.5ms để tạo ra một token duy nhất trên một GPU A100 cả cho các triển khai cơ sở và các triển khai nơi trọng số bộ điều chỉnh đã được thêm vào. Điều này có nghĩa là throughput tổng hợp cho triển khai LLM trên GPU đó là giữa 74 token/s và 83 token/s.

### 6.4 Phân tích tác động hiệu suất của các replica triển khai bổ sung

Trong Bảng 10, chúng tôi chạy benchmark cho 25 bộ điều chỉnh được truy vấn đồng thời bởi 50 người dùng, với một triển khai LoRAX trên 1 replica. Sau đó chúng tôi chạy benchmark nơi chúng tôi mở rộng triển khai LoRAX thành 2 replica được đặt phía sau một load balancer round robin để định tuyến số lượng lưu lượng bằng nhau đến mỗi replica, đồng thời cũng mở rộng tải thành 100 người dùng đồng thời. Chúng tôi thấy rằng các số liệu ổn định trên toàn bộ, biểu thị rằng các replica có thể được mở rộng tuyến tính với tải để đạt được các số liệu tương đương.

## 7 Hạn chế

Thiết kế thí nghiệm của chúng tôi có nhiều hạn chế, bao gồm:

• **Phạm vi Đánh giá Hạn chế**: Các đánh giá của chúng tôi được giới hạn ở 1000 ví dụ đầu tiên của các tập dữ liệu có split đánh giá lớn hơn để quản lý chi phí trong khi duy trì sự nghiêm ngặt. Điều này có thể giới thiệu thiên vị lựa chọn và giới hạn khả năng tổng quát hóa của những phát hiện của chúng tôi. Nghiên cứu tương lai nên xem xét các đánh giá toàn diện hơn khi tài nguyên cho phép.

• **Ràng buộc Prompt Engineering**: Nghiên cứu của chúng tôi không sử dụng các kỹ thuật prompt engineering nâng cao như majority voting, n-shot prompting, hoặc các phương pháp tuning chuyên biệt như MedPrompt hoặc chain-of-thought prompting. Trong nghiên cứu này, chúng tôi ưu tiên khả năng tái tạo và giảm thiểu thiên vị từ việc lựa chọn ví dụ có chọn lọc bằng cách sử dụng các prompt zero hoặc single-shot đơn giản trên tất cả các tác vụ, tuy nhiên những kỹ thuật này đã cho thấy tiềm năng trong việc nâng cao hiệu suất cụ thể theo tác vụ.

• **Ràng buộc Huấn luyện**: Tất cả LLMs được fine-tune với cùng các tham số: 40K ví dụ, batch size 1, lượng tử hóa 4-bit, và LoRA rank 8, sử dụng tối ưu hóa adam và cosine learning rate scheduler với các cài đặt cụ thể. Huấn luyện được tiến hành trên một GPU A10 duy nhất, sử dụng gradient checkpointing để quản lý các hạn chế bộ nhớ. Đối với các tập dữ liệu mà độ dài chuỗi đầy đủ gây ra overflow bộ nhớ, chúng tôi cắt ngắn chuỗi đến độ dài phân vị thứ 95. Cách tiếp cận này có thể ảnh hưởng đến tính kỹ lưỡng của huấn luyện mô hình, đặc biệt là trên các tập dữ liệu mà 40K bước không hoàn thành ngay cả một epoch đầy đủ. Mở rộng khả năng phần cứng, tăng batch size, hoặc điều chỉnh các siêu tham số như learning rate hoặc scheduler có thể nâng cao kết quả.

• **Đa dạng Mô hình Hạn chế**: Các thí nghiệm của chúng tôi được giới hạn ở fine-tuning LoRA trên hai kích thước mô hình, 2B và 7B. Khám phá một phạm vi rộng hơn các kích thước mô hình, bao gồm các mô hình lớn hơn như 13B hoặc 70B, có thể cung cấp hiểu biết về khả năng mở rộng và hiệu quả của fine-tuning trên các khả năng tính toán khác nhau.

Chúng tôi duy trì rằng LoRA Land thành công chứng minh hiệu quả thực tế của việc huấn luyện và phục vụ một số LLMs chuyên biệt theo tác vụ cạnh tranh với GPT-4 trong một ứng dụng sản xuất được cung cấp năng lượng bởi LoRAX, bất chấp những hạn chế này.

## 8 Kết luận

Trong nghiên cứu này, chúng tôi đánh giá hiệu quả của Low Rank Adaptation (LoRA) cho fine-tuning các Mô hình Ngôn ngữ Lớn (LLMs) trên một phạm vi rộng các tác vụ và mô hình và tính khả thi của việc phục vụ nhiều LLMs fine-tuned LoRA trong sản xuất.

Về chất lượng mô hình, kết quả của chúng tôi xác nhận rằng fine-tuning LoRA nâng cao đáng kể hiệu suất LLM, vượt trội hơn các cơ sở không fine-tuned và GPT-4. Hiệu suất nổi bật của các mô hình như Mistral-7B trên nhiều tác vụ làm nổi bật tầm quan trọng của việc lựa chọn mô hình cơ sở trong thành công fine-tuning. Chúng tôi thấy rằng các heuristics độ phức tạp tập dữ liệu có thể được tận dụng hợp lý làm các yếu tố dự đoán tiềm năng của thành công fine-tuning, gợi ý rằng bản chất của tác vụ đóng một vai trò quan trọng trong hiệu quả của fine-tuning.

Bất chấp những kết quả này, các hạn chế như quy mô đánh giá, ràng buộc huấn luyện, và tính đơn giản của các cách tiếp cận prompt engineering của chúng tôi gợi ý các khu vực cho cải thiện tương lai. Chúng tôi phát hành tất cả các mô hình và thiết lập huấn luyện của mình để cộng đồng xác thực và thử nghiệm thêm.

Về phục vụ, chúng tôi chứng minh triển khai thực tế của những mô hình này sử dụng khung LoRAX thông qua ứng dụng web LoRA Land. Chúng tôi cung cấp các benchmark cho thời gian đến token đầu tiên (TFTT), tổng thời gian yêu cầu, và thời gian streaming token, và đo lường độ bền vững độ trễ của LoRAX đối với tối đa 100 người dùng đồng thời.

Tổng thể, LoRA Land nhấn mạnh chất lượng và hiệu quả chi phí của việc sử dụng nhiều LLMs chuyên biệt so với một LLM đa năng duy nhất.

## 9 Lời cảm ơn

Justin Zhao dẫn dắt nghiên cứu và viết bài báo. Justin Zhao và Timothy Wang thiết kế các thí nghiệm, tạo ra harness đánh giá, chạy thí nghiệm, và phân tích dữ liệu. Wael Abid dẫn dắt các benchmark hiệu suất LoRAX và viết phần 6 của bài báo. Piero Molino là một người ủng hộ sớm cho ý tưởng và cung cấp phản hồi về viết lách, thí nghiệm, và phân tích dữ liệu. Chúng tôi cảm ơn Martin Davis, Kabir Brar, và Jackie Ho đã thiết kế và phát triển ứng dụng web LoRA Land. Chúng tôi cảm ơn Travis Addair, Geoffrey Angus, Magdy Saleh, Noah Yoshida, Jeffrey Tang, và các contributor mã nguồn mở đã phát triển LoRAX. Chúng tôi cảm ơn Noah Yoshida và Gyanesh Mishra đã hỗ trợ triển khai. Chúng tôi cảm ơn Arnav Garg, Geoffrey Angus, Arnav Garg, Jeff Kinnison, Alex Shertinsky, Travis Addair, Piero Molino, và các contributor mã nguồn mở cho Ludwig. Chúng tôi cảm ơn Will Gorman, Michael Gonzales, và Devvret Rishi đã hỗ trợ, thảo luận, và phản hồi.
