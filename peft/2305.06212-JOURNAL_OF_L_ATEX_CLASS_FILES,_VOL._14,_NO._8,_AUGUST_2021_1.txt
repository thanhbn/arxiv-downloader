# 2305.06212.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.06212.pdf
# File size: 1602141 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Privacy-Preserving Parameter-Efficient Fine-Tuning
for Large Language Model Services
Yansong Li, Zhixing Tan , Paula Branco and Yang Liu ,Senior Member, IEEE
Abstract —Parameter-Efficient Fine-Tuning (PEFT) provides a
practical way for users to customize Large Language Models
(LLMs) with their private data in LLM service scenarios.
However, the inherently sensitive nature of private data demands
robust privacy preservation measures during the customization
of LLM services to ensure data security, maintain user trust,
and comply with stringent regulatory standards. Based on PEFT,
we propose Pr ivacy-Preserving P arameter-Efficient Fine-T uning
(RAPT ), a framework that offers privacy protection for LLM
services. RAPT adopts a local privacy approach, enabling users
to privatize their data locally using a text-to-text local differential
privacy mechanism. Since PEFT performs poorly when directly
trained on privatized data, we introduce a novel privatized token
reconstruction task that is trained jointly with the downstream
task, allowing LLMs to learn better task-dependent representa-
tions. Despite the simplicity of our framework, experiments show
that RAPT achieves competitive performance across tasks while
providing privacy guarantees against adversaries.1
Index Terms —Large Language Models, Parameter-efficient
Fine-tuning, Differential Privacy, Local Privacy
I. I NTRODUCTION
Recent years have witnessed the tremendous success of
Large Language Models (LLMs) [1, 2]. As the size of LLMs
continues to grow, it becomes increasingly difficult for individ-
ual users to deploy and run them locally. Consequently, recent
LLMs are often released as cloud services. To enable users and
developers to customize these LLM services, service providers
typically offer fine-tuning APIs2, which are generally based
on Parameter-Efficient Fine-Tuning (PEFT) methods[3, 4, 5].
PEFT provides a straightforward and practical solution for
adapting LLMs to downstream tasks while achieving compet-
itive performance compared to full fine-tuning [5].
Although PEFT APIs offer a practical way for users to
customize and utilize LLM services, the sensitive nature of
private data raises concerns about potential data leakage,
(Corresponding authors: Zhixing Tan and Yang Liu.)
Yansong Li is with the University of Ottawa, Ottawa, Canada
(email:yli627@uottawa.ca)
Zhixing Tan is with Zhongguancun Laboratory, Beijing, P.R.China (email:
zxtan@zgclab.edu.cn).
Paula Branco is with the School of Electrical Engineering and Computer
Science, University of Ottawa, Ottawa, Canada (email:pbranco@uottawa.ca).
Yang Liu is with the Department of Computer Science and Technology,
Tsinghua University, Beijing, China; Institute for AI Industry Research (AIR),
Tsinghua University, Beijing, China; Shanghai Artificial Intelligence Labora-
tory, Shanghai, China; and Jiangsu Collaborative Innovation Center for Lan-
guage Competence, Jiangsu, China (e-mail: liuyang2011@tsinghua.edu.cn).
1This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.
2For an example, please refer to https://openai.com/blog/
gpt-3-5-turbo-fine-tuning-and-api-updates.especially regarding Personally Identifiable Information (PII)
and Protected Health Information (PHI), both of which are
crucial to an individual’s privacy rights [6] and subject to
stringent legal regulations [7, 8]. To customize and use the
LLM service with PEFT APIs, users must first upload their
data to the service provider. Unfortunately, it is well-known
that the input text or even the embedding representations can
leak private information to various adversaries [9]. Therefore,
it is essential to implement a privacy preservation mechanism
while using LLMs to ensure the trustworthiness of these
services.
However, ensuring privacy preservation within the context
of LLM services is challenging. First, providing privacy pro-
tection for private data in LLM services is not straightforward.
Existing works on LLM privacy preservation [10, 11, 12,
13, 14, 15] primarily focus on centralized privacy settings ,
which depend on the service provider to safeguard users from
privacy leakage. Such settings may be inadequate in scenarios
involving an honest-but-curious service provider or a middle
eavesdropper [16]. Second, imposing privacy protection often
degrades the performance of downstream tasks, as existing
text privacy mechanisms struggle to preserve text semantics
while safeguarding private information [17], leading to a well-
known privacy-utility trade-off [18, 19, 20]. Furthermore,
Yang and Liu [21] found that PEFT methods, such as prompt
tuning, lack robustness against word perturbation. Conse-
quently, customizing LLMs with PEFT is likely to result in
significant performance degradation when privacy protection
is implemented.
To tackle the challenges mentioned above, we propose
Privacy-Preserving P arameter-Efficient Fine-T uning ( RAPT ), a
framework for customizing and utilizing LLM services with
privacy preservation. For this purpose, RAPT incorporates local
privacy settings that allow users to privatize their data di-
rectly on their devices. Specifically, RAPT utilizes Text-to-Text
(T2T) privatization [22], a form of local differential privacy
(LDP) [23], which substitutes words by injecting random per-
turbations into their representations. To mitigate performance
degradation when applying T2T privatization, RAPT intro-
duces an additional Part-of-Speech (POS) constraint, which
restricts word substitutions within their original POS roles,
thus achieving better grammatical coherence than the standard
T2T mechanism. Moreover, we propose a novel privatized
token reconstruction task, building on recent findings that the
masked language modeling objective can learn separable deep
representations [24, 25, 26], to further enhance the perfor-
mance of RAPT on downstream tasks. The objective of this task
is to recover the original content of a privatized special tokenarXiv:2305.06212v2  [cs.CL]  10 Jan 2025

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
User
LLMsServer
PEFT  Param.
Charlie generates he for registers
Attribute Infer ence
Contain Chatbot?  
- False
Contain User?  
- FalseNER Attack
CherryCharlie generates hefor registers
gets she as processorsAttacker  ModelEmbedding Inversion
Target: CharlieEavesdropper
Attack
Text: Chatbot  releases  it for users
PCT2T  Privatization
Embedding Model:  
Privacy Budget:
POS Scope: 
Intercept
Fig. 1. Overview of our RAPT framework. Users can privatize their data
locally using a Part-of-Speech constrained text-to-text privatization mecha-
nism. The service provider enables users to customize the LLM by training
on the privatized data through an API. This framework offers strong privacy
protection against potential eavesdropper attacks.
sequence from LLM representations. Unlike standard PEFT,
RAPT jointly tunes parameters for both the downstream task
and the privatized token reconstruction task. In this way, RAPT
combines the lightweight and modular benefits of PEFT with
improved utility (i.e., performance) through the use of POS
constrained T2T (PCT2T) and privatized token reconstruction.
Figure 1 provides an overview of our RAPT framework.
We conduct simulated privacy attacks and experiments on
various natural language processing and generation tasks with
different LLMs to evaluate the effectiveness of our proposed
framework. The results demonstrate that RAPT achieves com-
petitive performance while providing privacy protection across
different tasks and LLMs.
II. R ELATED WORK
Recent studies on LLM privacy protection can be broadly
categorized into centralized and local approaches.
a) Centralized Approaches: Most existing works focus
on a centralized privacy setting, relying on a central data cu-
rator to safeguard data from privacy leakage. Numerous studies
have explored how to train privacy-preserving LLMs [27,
13, 14, 28], which are beyond the scope of this work. For
instance, Kerrigan et al. [29] utilize differentially private
fine-tuning to protect private data used in fine-tuning public
language models. Yu et al. [10] investigate privacy protection
during the fine-tuning stage using lightweight methods such
asADAPTER s [3] and PREFIX -TUNING [4]. Our study differs
from these works by focusing on protecting users’ private data
when using LLM services without assuming the presence of
a central data curator.b) Local Approaches: Local approaches offer a higher
degree of privacy protection but often at the expense of
utility, such as performance on downstream tasks. Lyu et al.
[16] explore maintaining model utility under local privacy
constraints by utilizing a differentially private neural repre-
sentation method. However, their approach solely addresses
privacy protection during the inference stage. Conversely,
several studies have proposed pre-training [30, 13, 31] and
fine-tuning [32, 30, 33, 34, 35] based methods that provide
privacy protection during both training and inference stages.
These methods require fine-tuning the entire model on pri-
vatized data, which is an expensive process for LLMs. Our
approach also adopts a local privacy setting and supports
privacy protection during both fine-tuning and inference but
remains lightweight. Additionally, we introduce a novel pri-
vatized token reconstruction task to enhance the performance
of prompt tuning methods when training and inferring with
privatized data.
III. A PPROACH
In this section, we provide a detailed description of RAPT .
We begin by introducing Text-to-Text (T2T) privatization,
which allows users to privatize their data locally. Next, we
describe how to customize and utilize LLM services with
privatized data using our method.
A. Text-to-Text Privatization
Users of LLM services may face data privacy issues
arising from eavesdropping attacks or honest-but-curious ser-
vice providers. Therefore, adopting a local privacy setting is
advantageous, allowing users to perform data privatization
locally on their devices . In our framework, we utilize T2T
privatization [22], as most LLMs operate with a text-to-text
interface. T2T privatization is based on dX-privacy [36], a
distance-based relaxation of local differential privacy that is
widely used to protect textual content [22, 30].
Formally, for a given input set Xand an output set Y,dis
a distance function defined on X. A randomized mechanism
M:X → Y satisfies dX-privacy if and only if, for any two
data points x,x′∈ X, the distributions of outputs from M(x)
andM(x′)are bounded by:
P[M(x) =y]
P[M(x′) =y]≤eηd(x,x′),∀y∈ Y. (1)
where η≥0is the privacy parameter , which controls the
degree of privacy protection3.
1) The Original Text-to-Text Privatization: The original
T2T privatization approach introduced by Feyisetan et al. [22]
demonstrates that a privatized sequence ˆxcan be obtained
by systematically replacing each word wtin the sequence
x= [w1, . . . , w n]with a different word ˆwtto ensure plausible
deniability [37]. Specifically, using the L2distance and an
embedding model E∈R|V|×d, where |V|is the vocabulary
size and dis the embedding dimension, the process begins
by applying dX-privacy to the word embedding wt=E(wt),
3To avoid confusion, we follow Qu et al. [30] and use ηto distinguish ϵ,
which is a commonly used parameter in differential privacy literature.

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
where wt∈Rd. This is typically achieved by adding a random
noiseztowtwith density p(z)∝exp(−η|z|). The noise
z=lvis sampled by first selecting a scalar l∼Γ(d,1
η)and
then sampling a vector vuniformly from the unit ball space
Bd, yielding the privatized embedding:
ˆwt=wt+z. (2)
The next step involves utilizing a nearest neighbor search to
map the noisy representation ˆwtto a corresponding word ˆwt.
This mapping is formalized as follows:
ˆwt= arg min
wk∥E(wk)−ˆwt∥. (3)
2) Part-of-Speech Constrained Text-to-Text Privatization:
A major weakness of the original text-to-text privatization is
that the transformation treats each word in a sentence equally
and overlooks both the syntactic role and semantic of the
word, which may result in significant syntactic and semantic
changes to the sentence. For example, the sentence “I would
like to eat a burger” is transformed into “It rogate calhoun
to drive 25 pulitzer” after T2T privatization with moderate
privacy settings, which has both syntactic changes ( e.g., “a→
25”) and semantic changes ( e.g., “eat →drive”).
To alleviate the above issue, we propose a Part-of-Speech
constrained text-to-text privatization (PCT2T). Unlike the orig-
inal T2T, which adds random noise to all words in a sentence,
PCT2T only adds random noise to words belonging to a
predefined set of Part-of-Speech categories and transforms the
perturbation of representations into words within the same
POS category. The benefits of PCT2T are twofold. First,
with the additional POS constraint, the syntactic structure
of the sentence remains the same as the original sentence
after transformation. Second, as only words in selected POS
categories are transformed, the transformed sentence is more
semantically similar to the original sentence. By carefully
choosing POS categories, we can significantly improve the
utility while maintaining nearly the same degree of privacy
protection.
Technically speaking, we first mark the word boundary be-
fore applying tokenization. After POS tagging, for each word
wtin a selected POS category C, the representation wtof the
word wtis calculated as wt=Mean ({wk}wk∈Tok(wt)), where
Tok(wt)denotes the set of tokens of wtafter tokenization.
Finally, the projected word ˆwtis identified by using the nearest
neighbor search:
ˆwt= arg min
wk∥EC(wk)−wt∥,subject to wk∈ C,(4)
where ECdenotes the embedding for the POS category C
calculated from E. The procedure for calculating a word
embedding in ECis the same as that for calculating wt
mentioned above.
B. LLM Customization
Being lightweight and modular, PEFT methods are suitable
choices for customizing LLM services. We assume that users
customize LLM services through APIs by uploading their
private training data to service providers . For ease of
PCT2T  PrivatizationLLMs
Token Recon. 
 LM Head
Prompt
ProjectServer
UserFig. 2. Overview of the RAPT framework during LLM customization. The
process includes PCT2T privatization, PEFT, privatized token reconstruction
(Token Recon.), and a task-specific language model (LM) head.
description, we will assume that the service provider uses
prompt tuning [5] for LLM customization, though other PEFT
methods are equally applicable.
1) Prompt Tuning: Prompt tuning uses continuous vectors
to adapt LLMs and is learned through gradient descent [4, 5].
There are many variants of prompt tuning, such as PREFIX -
TUNING [4], PROMPT TUNING [5], and P- TUNING [38]. For
ease of description, we follow the notation of Lester et al. [5],
though other variants are also applicable.
Letfdenote the backbone LLM model for the LLM service.
A prompt T∈Rl×hof length lis a sequence of continuous
vectors [t1, . . . ,tl], where ti∈Rhis a tunable virtual token
embedding. Given an input sequence x= [w1, . . . , w n], the
prompt is prepended to X= [w1, . . . ,wn], which represents
the embeddings of x. We then obtain a sequence of activations
using the LLM:
H=f(JT;XK), (5)
where H∈R(l+n)×his the sequence of activations, and J. . .K
denotes the concatenation operator, his the hidden size of the
backbone LLM. The activations Hare used to predict task-
dependent labels through a language modeling head [38].
2) Privatized Token Reconstruction: Unfortunately, directly
tuning prompts on privatized data significantly degrades the
performance of the LLM on downstream tasks, even with
weak privacy protections. This implies that prompt tuning
is sensitive to the random perturbations introduced by T2T
privatization.
Recent studies on language model representations [24, 25]
found that the masked language modeling objective [1] is ef-
fective in learning separable deep representations. Inspired by
these findings, we propose a novel privatized token reconstruc-
tion task, which is trained jointly with the downstream task.
We expect that training with privatized token reconstruction
will help LLMs learn better task-dependent representations,
thereby improving their performance on downstream tasks
while preserving privacy.

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
TABLE I
PLAIN TOKENS ARE USED IN THE SST, QQP, AND TP-UK TASKS ,
RESPECTIVELY . PLAIN TOKENS ARE RANDOMLY GENERATED TOKEN
SEQUENCES THAT DO NOT NEED TO BE MEANINGFUL .
SST erect langley bain jazz business refrigerator uci hose
baking beijing edward combatants rotary lush emery
ruiz oilers halves poke reasons prospects oak contend
arthur fia bethlehem neighbourhoods loss accents ideol-
ogy groves quinlan pixel azure issues mum lucivar bihar
northeast rooney
QQP weimar hutchinson lift triumphant breaker pathways
bucket anne absent willow huntington brentford land-
marks ike accumulation powerplant lionel cyber boule-
vard christmas translation entertainment apparel thru de-
bate dissolve partner honeymoon brownish ncaa junction
walker artist carpenter clothes medici forge venues mg
deploy
TP-UK plaques vertebrae restart yates educator wood berman
sneakers points resin liner gotham raider basins al-
bert sinatra somalia tree dusty dimensional bytes strips
rangers zheng jimi performance manga bohemian brest
improves beau zion gillespie visit nagasaki bianca brow
installations airs fairfield
Privatized token reconstruction is similar to masked lan-
guage modeling. However, reconstructing a word in a priva-
tized input is infeasible because it may contain sensitive in-
formation. To address this issue, we prepend a fixed sequence
of tokens, referred to as “ plain tokens ,” to each input in the
training data. Plain tokens consist of arbitrarily chosen tokens
and can be safely sent to the service provider without concerns
about privacy leakage. Figure 2 provides an overview of RAPT
during LLM customization. Table I presents examples of plain
tokens used in various downstream tasks.
Formally, let k= [k1, . . . , k m]denote the plain tokens.
Given a set of training examples D={⟨xi, yi⟩|i=
1, . . . ,|D|}, where |D|denotes the size of the training data,
the users first prepend kto each input xiand then obtain a
privatized version M(Jk;xiK) = [ ˆk1, . . . , ˆkm,ˆw1, . . . , ˆwn]of
Jk;xiKthrough POS-constrained text-to-text privatization. Let
z=M(Jk;xiK), the LLM produces a sequence of activations
Gwith the sequence of word embeddings Zof the input z
and the prompt T:
G=f(JT;ZK), (6)
where G∈R(l+m+n)×h.
After obtaining G, we use mvectors Gl:l+m=
[gl+1, . . . ,gl+m]inGto reconstruct the plain tokens. To
achieve this, we introduce an additional reconstruction head
consisting of two linear layers. The probability distribution
for predicting the i-th token kiin the plain tokens is given by
pi=softmax (WdownWupgl+i), (7)
where Wdown∈R|Vr|×candWup∈Rc×hare the parameters
of the reconstruction head, cis the hidden size of the recon-
struction head, and |Vr|denotes the vocabulary size of the
reconstruction head. The loss function for the privatized token
reconstruction task is
Lrec=−mX
i=1logpi[ji], (8)where jidenotes the index of kiin the reconstruction head
vocabulary, and pi[ji]denotes the ji-th scalar in the vector
pi. Please note that the vocabulary of the reconstruction head
does not need to be the same as the vocabulary of the LLM.
The remaining nactivations Gl+m:l+m+nare used to
predict task-dependent labels. Assume the downstream task is
a classification task, where yj∈ {0,1}denotes the label for
thej-th class and |C|is the number of classes. The objective
function of the downstream task is formally described as
follows:
q= softmax( Whead1
nnX
i=1gl+m+i), (9)
Ltask=−|C|X
i=1yilogq[i], (10)
where Whead∈R|C|×his the function of the task-specific
language modeling head, and q∈R|C|is the predicted
probability distribution. As a result, the loss function for
training RAPT is given by
L=Ltask+Lrec. (11)
During the inference stage, the user also applies text-to-
text privatization to protect private information and obtain
predictions from the LLM service. We note that the reconstruc-
tion head is not used during the inference stage. Therefore,
{Wdown,Wup}can be discarded after training.
IV. P RIVACY EXPERIMENTS
We first study the effectiveness of RAPT against adversaries
through simulated attacks, following the methodology of Song
and Raghunathan [39]. We use BERT BASE as the backbone
LLM and conduct experiments on the UK section of the Trust-
pilot Sentiment dataset (TP-UK) [40]. For the POS constraint,
we carefully select Noun ,Verb,Pronoun , and Preposition
as the targets for privacy protection. Our focus on Nouns
andPronouns is due to their direct association with personal
identification information. Verbs are included because they
describe individual actions and behaviors, potentially revealing
sensitive patterns that require protection. Prepositions are also
considered since they provide context about the direction and
location, which could disclose personal details.
A. Attacks
To assess the effectiveness of RAPT in protecting privacy,
we simulate the following types of attacks:
1) Embedding Inversion Attack: This is a token-level attack
in which the representations of user input text can be reversed
to reveal the original input, potentially by an eavesdropping
attacker. We consider the following types of attacks in this
context [30]:
•White-Box Attack. White-box attacks assume that at-
tackers have full access to the model’s weights, archi-
tecture, and training pipeline, allowing them to obtain
gradient signals. However, we do not assume that attack-
ers have access to the complete training data. Following

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018
/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000000c
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c
/uni00000010/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000003a/uni0000004b/uni0000004c/uni00000057/uni00000048/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000033/uni00000026/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037
(a)
/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018
/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000000c
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c
/uni00000010/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000030/uni0000002f/uni00000033/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000025/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000033/uni00000026/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (b)
/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013
/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000000c
/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000013/uni00000011/uni00000016/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000013/uni00000011/uni00000017/uni00000015/uni00000018/uni00000013/uni00000011/uni00000017/uni00000018/uni00000013/uni00000013/uni00000011/uni00000017/uni0000001a/uni00000018/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000030/uni00000036/uni00000028/uni0000000c
/uni00000010/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000024/uni00000028/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000025/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000033/uni00000026/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (c)
/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018
/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000000c
/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni000000031X/uni0000000b/uni00000029/uni00000014/uni0000000c
/uni00000010/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000031/uni00000028/uni00000035/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000033/uni00000026/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (d)
/uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018
/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni0000000c
/uni00000013/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000018/uni0000001b/uni00000013/uni00000011/uni00000018/uni0000001c/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000014/uni00000013/uni00000011/uni00000019/uni00000015/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni000000031X/uni0000000b/uni00000024/uni00000046/uni00000046/uni00000011/uni0000000c
/uni00000010/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000024/uni00000057/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000033/uni00000026/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (e)
Fig. 3. Privacy experiment results are presented, with dashed lines in each graph indicating the outcomes of experiments conducted without privacy protection.
“AE” refers to the autoencoder.
the white-box inversion setting of Qu et al. [30], for
any privatized token embedding vtin the embedding
spaceRd, the attacker aims to recover the original token
embedding by solving:
wt= arg min
wk∥E(wk)−vt∥2. (12)
A nearest neighbor search is employed to identify the
closest original token embedding for each perturbed word
embedding.
•MLP-based Black-Box Attack. Black-box attacks as-
sume that attackers have access only to an API-like
service where they provide input and receive output,
without further information about the model. MLP-base
black-box attackers use a neural network to predicts the
original data from its transformed representation.
•Autoencoder-based Black-box Attack. The attacker uti-
lizes an autoencoder with an encoder ϕand a decoder
ψ. The encoder compresses the input xinto a latent
representation z=ϕ(x), and then the decoder attempts
to reconstruct the input as ˆx=ψ(z). The mean squared
error (MSE) quantifies the reconstruction error:
MSE(x,ˆx) =∥x−ˆx∥2. (13)
The autoencoder is being trained to decode the privatized
representations back to the original data, with a low MSE
indicating a potential vulnerability.
2) Attribute Inference Attack: The attacker can infer a
user’s private attribute t∈ {0,1}|C|from the hidden repre-
sentations, where |C|is the number of private classes [41].
Specifically, the objective of the attacker is defined as follows:
p=fθ 
1
nnX
i=1zi!
, (14)
LAIA=−|C|X
i=1tilogp[i], (15)
where ziis the perturbed word embeddings of the LLM, and
fθis a 2-layer MLP with 768 hidden units and a ReLU acti-
vation function, following the settings of Plant et al. [41]. The
choice of the TP-UK dataset encompasses user demographic
traits such as gender and age, making it a suitable selection
for the context of attribute inference attacks. We adopt the
framework of Plant et al. [41] to categorize age properties intosix equal-sized age range bins4with unique assigned labels,
while gender is represented as binary categorical variables
denoted by 0 and 1.
3) NER Attack: The attacker uses a Named Entity Recog-
nition (NER) tool to extract sensitive details like names,
addresses, and organizations from text [42]. We assume the
attacker uses a BERT BASE model to perform Named Entity
Recognition (NER) on input text sequences to extract sen-
sitive information. Given a sequence of input tokens x=
[x1, x2, . . . , x n], the NER model aims to assign a probability
vector pito each token xi, indicating the likelihood of that
token being classified as one of predefined entity types. The
attacker’s objective is to maximize the correct identification of
sensitive entities, which is formalized as:
pi= BERT( xi), (16)
LNER=−nX
i=1mX
j=1tijlogpi[j], (17)
where tijis a binary indicator of whether token xiis of
entity type j, andpi[j]is the probability assigned by the NER
model for this type. The BERT-based NER model is trained
to minimize the loss LNER.
B. Metrics
For all types of attacks, we use 1−Xas our evaluation
metric, where Xrepresents the attack’s success rate measured
by either accuracy or F1 score. This means that a lower success
rate (X) results in a higher evaluation metric value, indicating
better privacy protection. However, for the autoencoder-based
attack, we instead use the mean squared error (MSE) as the
metric. We define this overall metric as empirical privacy ,
where higher values signify stronger privacy protection.
C. Results
Figure 3 illustrates the empirical privacy for T2T, PCT2T,
and RAPT (PCT2T + Reconstruction) under various simu-
lated attacks. From the figure, we have the following three
observations. First, both T2T and PCT2T are effective at
preserving privacy. As the privacy parameter ηdecreases,
the empirical privacy improves significantly. In the absence
4The bins include [ ≤1955, 1955-1963, 1964-1971, 1972-1978, 1979-1985,
≥1986]

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
of privacy measures, attackers find it easy to reconstruct the
original input. Second, T2T and PCT2T are comparable in
terms of privacy protection regardless of the choice of privacy
parameter η. Although adding an additional POS constraint
seems to weaken the degree of privacy protection compared to
T2T, experiments show that the effects are not significant when
carefully choosing the POS categories. Third, further adding
a plain token reconstruction objective along with PCT2T does
not compromise privacy protection. This coincides with our
intuition because the original sentences are not reconstructed
during training. As a result, we conclude that our proposed
RAPT is effective in preserving privacy.
V. U TILITY EXPERIMENTS
We study the effectiveness of RAPT in improving utility. We
use the backbone LLM embedding layer as the embedding
model to perform privatization unless otherwise stated. Note
that other embedding models are also applicable.
A. Setup
1) Datasets, Metrics, and LLMs: We evaluate our approach
on both Natural Language Understanding (NLU) and Natural
Language Generation (NLG) tasks. For NLU, we utilize
BERT BASE [1] and T5 BASE [43] as the backbone language mod-
els. The evaluation is conducted using the Stanford Sentiment
Treebank (SST) [44], Quora Question Pairs (QQP) [45], and
the TP-UK dataset [40]. Performance on these NLU tasks is
assessed using accuracy scores.
For the Natural Language Generation (NLG) task, we
employ T5 BASE [43] as the backbone model and utilize the
WEBNLG dataset [46], which encompasses 14 distinct do-
mains. To evaluate the effectiveness of RAPT , we use BLEU
scores obtained from the official evaluation script. Specifically,
within the W EBNLG dataset, we consistently apply priva-
tization to entities present in both the input tables and the
generated text. After generation, the original and privatized
word pairs are seamlessly restored on the user’s end.
2) Baselines: We build RAPT on top of three representative
PEFT methods. We also compare the FINE -TUNING method
described in Qu et al. [30].
•PROMPT TUNING [5]. PROMPT TUNING introduces a
learnable continuous prompt for steering LLMs to down-
stream tasks.
•PREFIX -TUNING [4]. PREFIX -TUNING introduces more
parameters by using deep prompts, which prepend con-
tinuous vectors to activations in all LLM layers.
•ADAPTER [3]. ADAPTER injects trainable rank decompo-
sition matrices into each layer of the LLM.
3) Implementation Details: All experiments are performed
on a machine with an NVIDIA V100 GPU. For RAPT , all
experiments use a fixed length of 40 plain tokens that are
uniformly sampled from the user-specified POS categories
unless otherwise stated.
For NLU tasks, we employ the Adam optimizer [47] with
a fixed learning rate of 6e-5 for all methods. The batch size is
128, and we train each method for 4 epochs. To accommodate
the input data, we set the maximum sequence length to 128TABLE II
ESTIMATED WORD REPLACEMENT PROBABILITY (PR.)FOR BERT AND
T5, WITH VARYING PRIVACY PARAMETERS .
Embedding η- Pr.
BERTη 25 75 125 150 175
Pr. 0.94 0.85 0.45 0.14 0.05
T5η 1 2 3 5 7
Pr. 0.91 0.82 0.42 0.12 0.03
tokens. For PROMPT TUNING , we use a prompt length of 150,
while for PREFIX -TUNING , the prompt length is set to 10. The
hidden size cis set to 96, and the vocabulary size |Vr|of the
reconstruction head is 19,369. During inference, we report the
average score across 5 independent runs.
For the NLG task, we follow the setting of Li and Liang
for PREFIX -TUNING . We utilize the Adam optimizer with
a learning rate of 5e-5, a batch size of 5, and a prompt
length of 5. In the case of PROMPT TUNING , we employ the
Adafactor [48] optimizer with a learning rate of 1e-3 and a
prompt length of 50. During the generation phase, we employ
the beam search algorithm with a beam size of 5, a maximum
generation length of 100, a top- kvalue of 4, and a top- pvalue
of 0.9.
B. Results on NLU tasks
Table III shows the results of PROMPT TUNING ,PREFIX -
TUNING ,ADAPTER , and FINE -TUNING with and without pri-
vacy protection on three NLU tasks. Note that our proposed
RAPT approach incorporates both PCT2T privatization and
privatized token reconstruction . Table II gives word replace-
ment probabilities for different privacy parameter η, which
serves as an intuitive measure for comparing the degree of
privacy protection between different η. The columns with
η= +∞represent results without privacy protection. From
table III, we have the following observations:
•With smaller η, text-to-text privatization (both T2T and
PCT2T) offers stronger privacy guarantees but it in-
evitably hurts the performance of downstream tasks.5
•When adopting T2T privatization, we can see that both
PROMPT TUNING and PREFIX -TUNING perform poorly
across all tasks and LLMs even with the weakest privacy
protection (the largest η), which indicates that prompt
tuning methods are sensitive to random perturbations
imposed by text-to-text privatization.
•Using PCT2T privatization instead of T2T privatization
consistently improves the performance across different
choices of η, which indicates the effectiveness of PCT2T
in preserving the syntactic and semantics of the original
sentence.
•By further introducing the privatized token reconstruction
task, the performance of all four methods is signifi-
cantly improved across tasks and LLMs. Adding token
reconstruction cannot bring further improvements when
5Please note that the value of privacy parameter ηis not directly comparable
for different LLMs because the average distance between two words in the
embedding space is different for different LLMs.

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
TABLE III
ACCURACY RESULTS FOR PROMPT -TUNING , PREFIX -TUNING , ADAPTER ,AND FINE-TUNING ON THE SST, QQP, AND TP-UK TASKS ARE EVALUATED
USING T2T, PCT2T, AND PRIVATIZED TOKEN RECONSTRUCTION METHODS ,RESPECTIVELY . PRIVACY PARAMETERS ηRANGE FROM 25TO175 FOR
BERT AND FROM 1TO7FOR T5. T HE BEST RESULTS ACHIEVED WITH PRIVACY PRESERVATION ARE HIGHLIGHTED IN BOLD . OUR RAPT APPROACH
COMBINES BOTH PCT2T AND RECONSTRUCTION TECHNIQUES .
MethodSST QQP TP-UK
25 75 125 150 175 +∞ 25 75 125 150 175 +∞ 25 75 125 150 175 +∞
BERT BASE
PROMPT TUNING
+T2T Privatization 46.3 47.1 55.6 56.3 68.8 88.3 51.2 55.4 63.6 69.8 73.4 87.7 55.4 67.1 79.3 81.9 83.6 88.7
+PCT2T Privatization 51.8 52.4 60.5 60.8 72.9 88.2 56.1 59.7 68.2 73.9 78.7 87.8 60.7 71.5 85.1 85.9 88.1 88.5
+Reconstruction 53.3 53.6 62.1 63.1 73.1 88.5 56.5 60.6 69.1 76.6 80.2 87.4 62.2 73.3 86.0 87.2 88.5 88.6
PREFIX -TUNING
+T2T Privatization 49.2 50.4 60.7 64.9 71.5 90.7 61.5 67.4 74.7 78.1 80.2 88.6 73.8 76.6 81.6 83.8 84.3 90.1
+PCT2T Privatization 55.4 58.2 76.8 84.1 84.7 90.7 66.0 71.6 78.9 83.5 85.4 89.1 77.5 79.3 83.9 86.6 88.8 90.2
+Reconstruction 56.4 59.4 78.9 84.3 84.8 90.8 67.6 74.7 81.9 84.7 85.9 88.5 77.7 79.7 84.7 88.1 89.1 90.3
ADAPTER
+T2T Privatization 49.2 53.1 68.9 76.9 79.3 91.4 63.1 70.0 75.3 79.8 83.1 88.7 72.3 77.9 83.4 85.2 87.4 90.8
+PCT2T Privatization 58.3 60.6 80.2 88.1 89.8 91.9 68.8 75.3 81.1 85.4 87.5 88.8 78.8 84.9 86.7 89.1 90.5 91.1
+Reconstruction 59.7 62.4 81.1 89.6 89.9 91.8 69.2 75.4 82.9 85.6 87.6 88.9 81.0 85.3 88.4 89.9 90.5 90.7
FINE -TUNING
+T2T Privatization 49.9 55.2 79.6 85.9 89.7 92.4 66.2 72.6 81.0 86.2 88.2 91.1 75.3 79.9 86.8 90.2 90.5 92.5
+PCT2T Privatization 65.6 70.4 85.2 89.5 91.0 92.5 84.2 85.5 86.2 88.8 89.7 91.2 85.5 87.3 88.0 89.7 91.9 92.6
+Reconstruction 66.0 71.9 90.5 90.9 91.4 92.8 85.7 86.5 87.4 89.2 89.5 91.6 88.5 89.2 90.2 90.6 92.5 92.7
MethodSST QQP TP-UK
1 2 3 5 7 +∞ 1 2 3 5 7 +∞ 1 2 3 5 7 +∞
T5BASE
PROMPT TUNING
+T2T Privatization 49.3 55.6 67.4 74.9 85.6 91.1 56.3 77.8 82.2 85.5 88.7 94.3 70.2 80.5 82.7 83.6 84.3 91.3
+PCT2T Privatization 55.1 55.9 64.3 63.0 76.5 90.9 58.4 62.9 71.0 76.6 80.7 94.8 73.2 83.7 88.7 87.9 90.7 91.2
+Reconstruction 56.3 57.4 65.1 65.9 77.4 90.5 59.1 64.5 71.7 78.7 82.5 94.4 74.2 85.8 89.3 89.8 90.9 91.4
PREFIX -TUNING
+T2T Privatization 52.2 54.1 63.7 68.6 74.7 94.3 64.1 70.3 77.8 82.1 83.3 91.3 77.4 80.4 85.4 87.3 87.3 93.3
+PCT2T Privatization 59.1 61.9 79.7 86.2 86.8 94.0 69.3 73.6 81.1 85.6 87.7 91.7 79.8 81.6 86.9 89.7 91.3 93.6
+Reconstruction 60.0 63.1 81.4 87.3 86.9 94.6 71.3 75.3 83.9 87.2 88.6 91.5 80.5 82.8 87.2 90.8 91.7 92.8
ADAPTER
+T2T Privatization 51.2 70.1 80.5 90.4 91.8 92.5 65.7 87.2 90.1 91.4 92.4 95.2 77.8 85.4 89.9 90.4 90.7 92.1
+PCT2T Privatization 60.8 73.2 83.9 91.1 92.3 92.8 71.1 87.6 93.3 92.8 93.6 95.1 82.1 87.4 89.7 91.3 92.6 92.8
+Reconstruction 62.5 76.3 84.8 92.5 92.4 93.1 71.6 88.3 93.6 93.1 93.8 95.5 83.5 88.2 90.8 91.5 92.7 92.6
FINE -TUNING
+T2T Privatization 64.3 78.8 87.7 92.1 92.5 93.3 84.1 91.1 91.8 92.0 92.5 96.3 86.8 88.9 90.5 90.7 90.9 93.4
+PCT2T Privatization 68.4 79.2 89.1 93.0 93.8 94.9 86.9 91.7 93.1 94.5 95.2 96.7 88.7 89.8 90.8 91.5 91.7 93.6
+Reconstruction 69.4 80.7 91.9 93.2 93.9 94.7 88.3 92.6 93.9 94.6 95.8 96.1 89.9 90.2 90.9 91.8 91.9 93.2
TABLE IV
RESULTS OF LLAMA 3-8B AND MISTRAL -7B ON THE SST-2 TASK .
Method 25 75 125 150 175 +∞
LLAMA 3-8B
+T2T Privatization 69.8 75.6 78.6 85.5 89.6 96.6
+PCT2T Privatization 76.8 79.4 83.6 87.2 93.6 96.4
+Reconstruction 77.6 80.3 85.2 88.4 93.7 96.5
MISTRAL -7B
+T2T Privatization 65.3 72.6 75.1 83.4 87.2 94.1
+PCT2T Privatization 68.9 73.5 75.4 85.6 90.2 94.3
+Reconstruction 70.1 75.1 76.2 86.4 91.5 94.2
η= +∞. This is because reconstructing plain tokens
becomes trivial without adding privacy protection.
These results suggest that both PCT2T and privatized token
reconstruction are very effective in improving the performance
of parameter-efficient fine-tuning methods when trained on
privatized data. The results also coincide with our intuition
that using reconstruction can help LLMs learn better represen-
tations. To validate the effectiveness of our method on recent
LLMs, we further conducted experiments on the SST-2 taskTABLE V
RESULTS OF PROMPT TUNING AND PREFIX -TUNING ON THE WEBNLG
TASK WITH η= 1. THES, U, AND ACOLUMNS CORRESPOND TO THE
SEEN, UNSEEN, AND ALL CATEGORIES OF THE DATASET ,
RESPECTIVELY . ITALICIZED VALUES INDICATE THE RESULTS OBTAINED
WITHOUT PRIVACY PROTECTION .
MethodBLEU↑
S U A
PREFIX -TUNING 60.05 45.36 53.61
+T2T Privatization 19.02 15.14 16.28
+PCT2T Privatization 39.46 24.54 37.14
+Reconstruction 45.54 27.48 43.12
PROMPT TUNING 51.87 36.93 45.01
+T2T Privatization 16.38 12.71 14.38
+PCT2T Privatization 34.65 18.68 32.55
+Reconstruction 40.71 22.88 38.64
using Llama-8B and Mistral-7B. We utilized the embeddings
from the BERT BASE model for applying T2T and PCT2T
and fine-tuning the LLMs with the QLoRA method [49].
Table IV shows the results. From the results, we made the
same observations as in previous experiments, indicating that
our methods are applicable to different LLMs. As a result, we

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
/uni00000018/uni0000001a /uni00000018/uni0000001b /uni00000018/uni0000001c /uni00000019/uni00000013 /uni00000019/uni00000014 /uni00000019/uni00000015 /uni00000019/uni00000016
/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000024/uni00000046/uni00000046/uni00000011/uni00000035/uni00000024/uni00000033/uni00000037
/uni00000029/uni00000037
/uni00000026/uni00000024/uni00000033/uni00000028
/uni00000027/uni00000033/uni00000031/uni00000035
Fig. 4. Pareto frontiers for RAPT ,FINE -TUNING , CAPE, and DPNR. We
simulate an attribute inference attack to calculate empirical privacy. The
dataset used for the downstream task is the TP-UK dataset.
conclude that RAPT is effective in achieving better utility for
the preservation of LLM privacy.
C. Results on NLG tasks
Table V presents the results of PROMPT TUNING and PREFIX
TUNING on the W EBNLG task using the T5 BASE model. We
observe that performance significantly drops when T2T priva-
tization is adopted. For PCT2T, it achieves much better per-
formance compared to T2T (+15.5 BLEU points on average),
indicating the effectiveness of PCT2T in preserving the syntax
and semantics of the inputs. Combined with privatized token
reconstruction, we obtain an average improvement of +5.2
BLEU points. These results further confirm the effectiveness
of our proposed PCT2T and privatized token reconstruction
methods in preserving privacy for NLG tasks.
D. Comparison with Other Methods
We compare RAPT with other methods in terms of both
privacy and utility. We consider the following methods:
•CAPE [41]. This approach uses a Laplace-based embed-
ding perturbation coupled with an adversarial learning
objective to protect privacy. The method privatizes the
output representations of LLMs.
•DPNR [16]. The method introduces a combination of
word dropout and Laplace-based embedding perturbation.
It also privatizes the output representations of LLMs.
•Fine-Tuning [30]. This method privatizes the embedding
layer of LLMs. It applies dX-privacy to perturb input
embeddings of the user.
To ensure a fair comparison between these methods, we
use the commonly used attribute inference attack to study the
privacy-utility trade-off of different methods. For CAPE, we
utilize the BERT BASE model with privacy parameters set as
ϵ= 0.01andλ= 1.5, and the adversarial training learning
rate is set to 1e−3. For DPNR, we also use the BERT BASE
model. The word dropout rate ( µ) is set to 0.1and the privacy
parameter ϵis set to 5. For our implementation of RAPT , we
set the privacy parameter ηto100.
Figure 4 illustrates the Pareto Frontiers for different meth-
ods. We can observe that RAPT consistently achieves better
utility than CAPE, DPNR, and FINE -TUNING under the same
degree of privacy protection. As the degree of empirical
privacy increases, the performance gap between RAPT and
the other methods becomes larger. The results further confirm
that our RAPT method is effective in preserving the privacy of
LLMs.TABLE VI
RESULTS ON THE SST-2 TASK USING DIFFERENT EMBEDDING MODELS
FOR PCT2T. “P R.”DENOTES THE PROBABILITY OF REPLACING A WORD
WITH PCT2T PRIVATIZATION .
Embedding Pr. η ACC.
BERT 0.14 150 84.3
BioBERT 0.16 125 83.6
GPT-2 0.15 60 83.1
RoBERTa 0.14 50 85.7
T5 0.12 5 87.3
/uni00000014 /uni00000014/uni00000013 /uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000001b/uni00000017/uni00000011/uni00000018/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni00000018/uni00000011/uni00000018/uni00000030/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048
(a)
/uni00000018 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni0000001b/uni00000013
/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni0000004f/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni0000001b/uni00000016/uni00000011/uni00000018/uni0000001b/uni00000017/uni00000011/uni00000013/uni0000001b/uni00000017/uni00000011/uni00000018/uni00000030/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048
 (b)
/uni00000017/uni0000001b /uni0000001a/uni00000015 /uni0000001c/uni00000019 /uni00000014/uni00000017/uni00000017 /uni00000014/uni0000001c/uni00000015
/uni0000002b/uni0000004c/uni00000047/uni00000047/uni00000048/uni00000051/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni0000001b/uni00000016/uni00000011/uni00000018/uni0000001b/uni00000017/uni00000011/uni00000013/uni0000001b/uni00000017/uni00000011/uni00000018/uni0000001b/uni00000018/uni00000011/uni00000013/uni00000030/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048
 (c)
Fig. 5. The effects of the number of plain tokens, the length of plain tokens,
and the hidden size of the reconstruction head on the SST-2 task.
E. Intrinsic Evaluation
We study the characteristics of RAPT by comparing different
variants of RAPT on the SST-2 task using the BERT BASE
model. We assume that RAPT utilizes PREFIX -TUNING to steer
BERT BASE toward the SST-2 task.
1) Embedding Models: We investigate the effect of using
different embedding models in PCT2T privatization. Specif-
ically, the user side uses embedding models from GPT-2,
RoBERTa, BioBERT [50], or T5 to map the input text to
the privatized text. We adjust the privacy parameter ηfor
different embedding models to match the level of probability
for replacing a token in the text. Table VI shows the results
of our experiments. We find that using different embedding
models does not significantly affect performance on the down-
stream task, even with embedding models trained on different
domains (e.g., BERT vs. BioBERT).
2) Privatized Token Reconstruction:
a) Content of Plain Tokens: We first show that the
content of plain tokens can be chosen arbitrarily. We randomly
generate 5 plain tokens with a length of 40 and compare the
performance on the SST-2 task. Table VII shows the results.
We can see that the performances with different plain tokens
of the same length are comparable. The results suggest we can
arbitrarily choose the plain tokens used in the privatized token
reconstruction task.
b) Number of Plain Tokens: We study the effect of using
diverse plain tokens in the privatized token reconstruction task.
We first construct a set of plain tokens with the same length.
Then, during training, for each training input, we randomly
choose plain tokens and prepend them to the input. From Fig-
ure 5 (a), we find that using more plain tokens during training
slightly improves performance on SST-2. However, using one
plain token during privatized token reconstruction suffices to
improve the performance of the LLM on downstream tasks.
c) Length of Plain Tokens: We study the effect of using
plain tokens with different lengths. Intuitively, the LLM needs
to learn better representations to reconstruct longer plain

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
AllNounVerb
Pronoun
Preposition
Adjective
Symbol
DeterminerConjunction/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013
/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c
(a)
AllNounVerb
Pronoun
Preposition
Adjective
Symbol
DeterminerConjunction/uni00000014/uni00000015/uni00000015/uni00000017/uni00000016/uni00000019/uni00000017/uni0000001b/uni00000019/uni00000013
/uni00000028/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e/uni00000003/uni0000000b/uni0000003a/uni0000004b/uni0000004c/uni00000057/uni00000048/uni00000010/uni00000025/uni00000052/uni0000005b/uni0000000c (b)
AllNounVerb
Pronoun
Preposition
Adjective
Symbol
DeterminerConjunction/uni00000014/uni00000019/uni00000016/uni00000015/uni00000017/uni0000001b/uni00000019/uni00000017/uni0000001b/uni00000013
/uni00000031/uni00000028/uni00000035/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e (c)
AllNounVerb
Pronoun
Preposition
Adjective
Symbol
DeterminerConjunction/uni00000014/uni00000017/uni00000015/uni0000001b/uni00000017/uni00000015/uni00000018/uni00000019/uni0000001a/uni00000013
/uni00000024/uni00000055/uni0000004c/uni00000057/uni00000045/uni00000058/uni00000057/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e (d)
Fig. 6. The effect of using different POS categories on privacy and utility. We simulate the embedding inversion attack, NER attack, and attribute inference
attack with η= 100 . The “All” category represents selecting all POS tags for privacy protection.
TABLE VII
EFFECT OF USING DIFFERENT PLAIN TOKENS WITH THE SAME LENGTH ON
SST-2 DATASET . “ACC” DENOTES THE ACCURACY SCORE .
Tokens Choice 1 Choice 2 Choice 3 Choice 4 Choice 5
ACC 84.3 84.8 84.2 84.7 84.6
TABLE VIII
RESULTS OF PREFIX -TUNING ON SST-2 WITH VARYING PROMPT LENGTH
ANDη. ”P ARAMS .“DENOTES THE NUMBER OF PARAMETERS IN THE
PROMPT .
Length Params. η= 75 η= 125 η= 150
10 0.61M 59.4 78.9 84.3
40 0.95M 60.4 79.5 85.6
100 1.51M 61.8 80.6 87.5
tokens. Therefore, using longer plain tokens may benefit the
corresponding performance on downstream tasks. As shown
in Figure 5 (b), we find that the results match our intuition:
using longer plain tokens performs significantly better than
using shorter plain tokens.
d) Reconstruction Head: We study the hidden size and
vocabulary size of the reconstruction head. Figure 5 (c) shows
the results. Using a larger hidden size generally performs better
but introduces more parameters during training. As for the
vocabulary size, we found it is necessary to use a moderately
large vocabulary. A small vocabulary makes the prediction of
plain tokens much easier, therefore degrading the benefits of
privatized token reconstruction task.
e) Prompt Length: Table VIII shows the performance of
RAPT using different prompt length. We can see that RAPT
typically performs better as the length of the prompt increases.
The results are also coincide with previous studies [4, 5].
VI. A NALYSES
A. Embedding Distribution and Privacy Preservation
We first investigate the distribution of word embeddings
within a specific POS category and the corresponding empiri-
cal privacy using PCT2T. For each pair of words wiandwjin
/uni00000014/uni00000011/uni00000017/uni00000018 /uni00000014/uni00000011/uni00000018/uni00000013 /uni00000014/uni00000011/uni00000018/uni00000018 /uni00000014/uni00000011/uni00000019/uni00000013 /uni00000014/uni00000011/uni00000019/uni00000018
/uni00000030/uni00000048/uni00000044/uni00000051/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000028/uni00000050/uni00000053/uni00000048/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005cNoun
Verb
PronounPrepositionAdjectiveSymbol
Determiner
Conjunction/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019
/uni00000039/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000046/uni00000048
Fig. 7. Embedding distribution across different POS categories and their
corresponding empirical privacy. The x-axis represents the mean distance of
each category, while the y-axis indicates the degree of empirical privacy. The
color of each circle corresponds to the variance in distance for each POS
category.
a POS category, we compute the pairwise Euclidean distance
asdij=|wi−wj|. Then, the mean distance is calculated as
µ=2
n(n−1)n−1X
i=1nX
j=i+1dij.
The variance of the distance is calculated as
σ=2
n(n−1)n−1X
i=1nX
j=i+1(dij−µ)2.
We plot the mean and variance of the distances and the
corresponding empirical privacy for a given POS category in
Figure 7. From the figure, we can see that the categories Noun
andVerb achieve better privacy protection compared to other
categories. We also find that the mean distance and variance
for the Noun ,Verb, and Adjective categories are similar. This
finding reveals that it is necessary to differentiate between
different POS categories in text-to-text privatization.
B. Effect of POS Categories
We investigate the effect of choices of POS categories on
both utility and privacy for PCT2T. We focus on embedding

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
Fig. 8. We calculate the average distance in different layers of representations
between different methods.
inversion attacks, NER attacks, and attribute inference attacks.
The privacy parameter ηis set to 100. All utility experiments
are conducted on the SST-2 task. For POS categories, we
investigate using Noun ,Verb,Pronoun ,Preposition ,Adjective ,
Symbol ,Determiner , and Conjunction . Figure 6 presents the
results, from which we derive several key findings. First,
POS categories such as Conjunctions andDeterminers provide
relatively weaker privacy protection, indicating that not all
POS categories are equally useful for privacy preservation.
However, these categories are important for maintaining the
syntactic structure of the input sentence. This finding supports
our motivation for the PCT2T privatization approach. Second,
POS categories like Noun andVerb are effective for privacy
preservation. These categories are closely related to personal
identification information. Therefore, we include Noun and
Verb in our set of POS categories for all our experiments.
C. Geometry of Representations
We analyze the geometry of LLM representations to study
why RAPT improves the performance of downstream tasks
when trained on locally privatized data. We use PREFIX TUN -
ING as the prompt tuning method for RAPT and set η= 100 .
We use BERT BASE as the backbone LLM.
We first compute representations Oi∈Rn×hin each BERT
layer ifor PREFIX -TUNING without privatization, PREFIX -
TUNING with text-to-text privatization as Pi∈Rn×h, and
RAPT using data from SST-2 as Ri∈Rn×h. For each
layer, we project the {Oi,Pi,Ri}intoRn×2using principal
component analysis (PCA). Then we compute the average
distance dof representations between different methods. Fig-
ure 8 shows the results. We found that the learned deep
representations are similar to those learned without using
privacy protection. It is clear that the representations for RAPT
become closer to those without privacy protection as the layer
number increases. Therefore, we confirm that RAPT can learn
better representations.
D. Pareto Frontiers
We analyze the Pareto frontiers of our proposed RAPT
and prompt tuning with T2T privatization on the SST-2 task.
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni0000003a/uni0000004b/uni0000004c/uni00000057/uni00000048/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037(a)
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013
/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000030/uni0000002f/uni00000033/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000025/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (b)
/uni00000016/uni00000019 /uni00000016/uni0000001b /uni00000017/uni00000013 /uni00000017/uni00000015 /uni00000017/uni00000017 /uni00000017/uni00000019 /uni00000017/uni0000001b
/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000024/uni00000058/uni00000057/uni00000052/uni00000048/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000025/uni0000004f/uni00000044/uni00000046/uni0000004e/uni00000010/uni00000025/uni00000052/uni0000005b/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037
(c)
/uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013
/uni00000028/uni00000050/uni00000053/uni0000004c/uni00000055/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000055/uni0000004c/uni00000059/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000031/uni00000028/uni00000035/uni00000003/uni00000024/uni00000057/uni00000057/uni00000044/uni00000046/uni0000004e
/uni00000037/uni00000015/uni00000037
/uni00000035/uni00000024/uni00000033/uni00000037 (d)
Fig. 9. Pareto frontiers for RAPT and T2T privatization against four simulated
attacks. The dashed line in each graph represents the results from experiments
conducted without the use of privacy protection.
We simulate white-box attacks, MLP-based black-box attacks,
autoencoder-based black-box attacks, and NER attacks. The
backbone LLM is BERT BASE, and the privacy parameter η
ranges from 25 to 175. The results are illustrated in Figure 9.
From the figure, we find that for all attacks, RAPT consistently
outperforms T2T privatization under the same degree of pri-
vacy protection. As a result, it is clear that RAPT can achieve
a better privacy-utility trade-off than using T2T privatization.
VII. C ONCLUSION
In this work, we introduce RAPT , a framework for preserv-
ing privacy in LLM services. RAPT leverages local differential
privacy to provide robust protection while maintaining com-
petitive performance through privatized token reconstruction.
Our experiments demonstrate the effectiveness of RAPT across
various tasks, different LLMs, and multiple attack scenarios.
Additionally, empirical results reveal that different POS tags
offer varying levels of privacy protection, yet all are essential
for maintaining the syntactic structure of input sentences. This
validates the motivation behind our POS-Constrained Text-to-
Text Privatization approach.
REFERENCES
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“Bert: Pre-training of deep bidirectional transformers
for language understanding,” arXiv preprint
arXiv:1810.04805 , 2018.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al. , “Language models are few-shot learn-
ers,” Advances in neural information processing systems ,
vol. 33, pp. 1877–1901, 2020.
[3] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li,
S. Wang, L. Wang, and W. Chen, “Lora: Low-rank

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
adaptation of large language models,” arXiv preprint
arXiv:2106.09685 , 2021.
[4] X. L. Li and P. Liang, “Prefix-tuning: Optimizing
continuous prompts for generation,” in Proc. of
ACL, 2021, pp. 4582–4597. [Online]. Available: https:
//aclanthology.org/2021.acl-long.353
[5] B. Lester, R. Al-Rfou, and N. Constant, “The power
of scale for parameter-efficient prompt tuning,” arXiv
preprint arXiv:2104.08691 , 2021.
[6] K. Martin and H. Nissenbaum, “Measuring privacy:
An empirical test using context to expose confounding
variables,” Colum. Sci. & Tech. L. Rev. , vol. 18, p. 176,
2016.
[7] G. D. P. R. GDPR, “General data protection regulation,”
Regulation (EU) 2016/679 of the European Parliament
and of the Council of 27 April 2016 on the protection
of natural persons with regard to the processing of
personal data and on the free movement of such data,
and repealing Directive 95/46/EC , 2016.
[8] S. L. Pardau, “The california consumer privacy act:
Towards a european-style privacy regime in the united
states,” J. Tech. L. & Pol’y , vol. 23, p. 68, 2018.
[9] M. Coavoux, S. Narayan, and S. B. Cohen, “Privacy-
preserving neural representations of text,” arXiv preprint
arXiv:1808.09408 , 2018.
[10] D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Ka-
math, J. Kulkarni, Y . T. Lee, A. Manoel, L. Wutschitz
et al. , “Differentially private fine-tuning of language
models,” arXiv preprint arXiv:2110.06500 , 2021.
[11] W. Shi, A. Cui, E. Li, R. Jia, and Z. Yu, “Selective dif-
ferential privacy for language modeling,” arXiv preprint
arXiv:2108.12944 , 2021.
[12] R. Anil, B. Ghazi, V . Gupta, R. Kumar, and P. Ma-
nurangsi, “Large-scale differentially private bert,” arXiv
preprint arXiv:2108.01624 , 2021.
[13] S. Hoory, A. Feder, A. Tendler, S. Erell, A. Peled-
Cohen, I. Laish, H. Nakhost, U. Stemmer, A. Benjamini,
A. Hassidim et al. , “Learning and evaluating a differen-
tially private pre-trained language model,” in Findings of
the Association for Computational Linguistics: EMNLP
2021 , 2021, pp. 1178–1189.
[14] X. Li, F. Tramer, P. Liang, and T. Hashimoto, “Large
language models can be strong differentially private
learners,” arXiv preprint arXiv:2110.05679 , 2021.
[15] W. Shi, S. Chen, C. Zhang, R. Jia, and Z. Yu, “Just fine-
tune twice: Selective differential privacy for large lan-
guage models,” arXiv preprint arXiv:2204.07667 , 2022.
[16] L. Lyu, X. He, and Y . Li, “Differentially private
representation for NLP: Formal guarantee and an
empirical study on privacy and fairness,” in Proc. of
EMNLP , 2020, pp. 2355–2365. [Online]. Available:
https://aclanthology.org/2020.findings-emnlp.213
[17] S. Arnold, D. Yesilbas, and S. Weinzierl, “Guiding
text-to-text privatization by syntax,” in Proceedings of
the 3rd Workshop on Trustworthy Natural Language
Processing (TrustNLP 2023) , A. Ovalle, K.-W. Chang,
N. Mehrabi, Y . Pruksachatkun, A. Galystan, J. Dhamala,
A. Verma, T. Cao, A. Kumar, and R. Gupta,Eds. Toronto, Canada: Association for Computational
Linguistics, Jul. 2023, pp. 151–162. [Online]. Available:
https://aclanthology.org/2023.trustnlp-1.14
[18] S. Sreekumar and D. G ¨und¨uz, “Optimal privacy-utility
trade-off under a rate constraint,” in 2019 IEEE Interna-
tional Symposium on Information Theory (ISIT) . IEEE,
2019, pp. 2159–2163.
[19] J. Ye, Z. Zhu, F. Liu, R. Shokri, and V . Cevher, “Initial-
ization matters: Privacy-utility analysis of overparameter-
ized neural networks,” Advances in Neural Information
Processing Systems , vol. 36, 2024.
[20] A. Zhang, Y . Wang, and S. Guo, “On the utility-
informativeness-security trade-off in discrete task-
oriented semantic communication,” IEEE Communica-
tions Letters , 2024.
[21] Z. Yang and Y . Liu, “On robust prefix-tuning for text
classification,” arXiv preprint arXiv:2203.10378 , 2022.
[22] O. Feyisetan, B. Balle, T. Drake, and T. Diethe, “Privacy-
and utility-preserving textual analysis via calibrated mul-
tivariate perturbations,” in Proc. of WSDM , 2020, pp.
178–186.
[23] A. Evfimievski, J. Gehrke, and R. Srikant, “Limiting
privacy breaches in privacy preserving data mining,” in
Proc of SIGMOD-SIGACT-SIGART , 2003, pp. 211–222.
[24] E. V oita, R. Sennrich, and I. Titov, “The bottom-up
evolution of representations in the transformer: A
study with machine translation and language modeling
objectives,” in Proc. of ACL , 2019, pp. 4396–4406.
[Online]. Available: https://aclanthology.org/D19-1448
[25] J. Mamou, H. Le, M. Del Rio, C. Stephenson, H. Tang,
Y . Kim, and S. Chung, “Emergence of separable man-
ifolds in deep language representations,” in 37th Inter-
national Conference on Machine Learning, ICML 2020 .
International Machine Learning Society (IMLS), 2020.
[26] A. Wettig, T. Gao, Z. Zhong, and D. Chen, “Should
you mask 15% in masked language modeling?” arXiv
preprint arXiv:2202.08005 , 2022.
[27] N. Carlini, F. Tramer, E. Wallace, M. Jagielski,
A. Herbert-V oss, K. Lee, A. Roberts, T. Brown, D. Song,
U. Erlingsson et al. , “Extracting training data from large
language models,” in USENIX Security , 2021, pp. 2633–
2650.
[28] D. Yu, H. Zhang, W. Chen, J. Yin, and T.-Y . Liu, “Large
scale private learning via low-rank reparametrization,” in
Proc. of ICML . PMLR, 2021, pp. 12 208–12 218.
[29] G. Kerrigan, D. Slack, and J. Tuyls, “Differentially pri-
vate language models benefit from public pre-training,”
arXiv preprint arXiv:2009.05886 , 2020.
[30] C. Qu, W. Kong, L. Yang, M. Zhang, M. Bendersky,
and M. Najork, Natural Language Understanding with
Privacy-Preserving BERT . Association for Computing
Machinery, 2021, p. 1488–1497. [Online]. Available:
https://doi.org/10.1145/3459637.3482281
[31] Y . Ding, X. Wu, Y . Meng, Y . Luo, H. Wang, and W. Pan,
“Delving into differentially private transformer,” arXiv
preprint arXiv:2405.18194 , 2024.
[32] Y . Huang, Z. Song, D. Chen, K. Li, and S. Arora, “Tex-
thide: Tackling data privacy in language understanding

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
tasks,” in Findings of the Association for Computational
Linguistics: EMNLP 2020 , 2020, pp. 1368–1382.
[33] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and
S. Zanella-B ´eguelin, “Analyzing leakage of personally
identifiable information in language models,” in 2023
IEEE Symposium on Security and Privacy (SP) . IEEE,
2023, pp. 346–363.
[34] X. Shen, Y . Liu, H. Liu, J. Hong, B. Duan, Z. Huang,
Y . Mao, Y . Wu, and D. Wu, “A split-and-privatize
framework for large language model fine-tuning,” arXiv
preprint arXiv:2312.15603 , 2023.
[35] R. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y . Xu, Y . Du,
Y . Wang, and S. Chen, “Openfedllm: Training large lan-
guage models on decentralized private data via federated
learning,” in Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining ,
2024, pp. 6137–6147.
[36] K. Chatzikokolakis, M. E. Andr ´es, N. E. Bordenabe, and
C. Palamidessi, “Broadening the scope of differential
privacy using metrics,” in Proc. of PETS . Springer,
2013, pp. 82–102.
[37] V . Bindschaedler, R. Shokri, and C. A. Gunter, “Plausible
deniability for privacy-preserving data synthesis,” arXiv
preprint arXiv:1708.07975 , 2017.
[38] X. Liu, K. Ji, Y . Fu, Z. Du, Z. Yang, and J. Tang, “P-
tuning v2: Prompt tuning can be comparable to fine-
tuning universally across scales and tasks,” arXiv preprint
arXiv:2110.07602 , 2021.
[39] C. Song and A. Raghunathan, “Information leakage in
embedding models,” in Proc. of ACM SIGSAC , 2020, pp.
377–390.
[40] D. Hovy, A. Johannsen, and A. Søgaard, “User review
sites as a resource for large-scale sociolinguistic studies,”
inProc. of TheWebConf , 2015, pp. 452–461.
[41] R. Plant, D. Gkatzia, and V . Giuffrida, “CAPE: Context-
aware private embeddings for private language learning,”
inProc. of EMNLP , 2021, pp. 7970–7978. [Online].
Available: https://aclanthology.org/2021.emnlp-main.628
[42] Y . Yang, H. Wu, and H. Zhao, “Attack named en-
tity recognition by entity boundary interference,” arXiv
preprint arXiv:2305.05253 , 2023.
[43] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring
the limits of transfer learning with a unified text-to-
text transformer,” The Journal of Machine Learning
Research , vol. 21, no. 1, pp. 5485–5551, 2020.
[44] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Man-
ning, A. Y . Ng, and C. Potts, “Recursive deep models for
semantic compositionality over a sentiment treebank,” in
Proc. of EMNLP , 2013, pp. 1631–1642.
[45] Z. Chen, H. Zhang, X. Zhang, and L. Zhao, “Quora
question pairs,” University of Waterloo , pp. 1–7, 2018.
[46] C. Gardent, A. Shimorina, S. Narayan, and L. Perez-
Beltrachini, “The webnlg challenge: Generating text from
rdf data,” in Proceedings of the 10th International Con-
ference on Natural Language Generation , 2017, pp. 124–
133.
[47] D. P. Kingma and J. Ba, “Adam: A method for stochasticoptimization,” arXiv preprint arXiv:1412.6980 , 2014.
[48] N. Shazeer and M. Stern, “Adafactor: Adaptive learn-
ing rates with sublinear memory cost,” in International
Conference on Machine Learning . PMLR, 2018, pp.
4596–4604.
[49] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettle-
moyer, “Qlora: Efficient finetuning of quantized llms,”
Advances in Neural Information Processing Systems ,
vol. 36, 2024.
[50] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So,
and J. Kang, “Biobert: a pre-trained biomedical language
representation model for biomedical text mining,” Bioin-
formatics , vol. 36, no. 4, pp. 1234–1240, 2020.
