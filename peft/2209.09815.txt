# 2209.09815.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2209.09815.pdf
# File size: 1519080 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Towards Fine-tuning Pre-trained Language Models with
Integer Forward and Backward Propagation
Mohammadreza Tayaranian1Alireza Ghaffari1Marzieh S. Tahaei1
Mehdi Rezagholizadeh1Masoud Asgharian2Vahid Partovi Nia1
1Huawei Noah’s Ark Lab, Montreal Research Center
2Department of Mathematics and Statistics, McGill University
{mohammadreza.tayaranian, alireza.ghaffari, marzieh.tahaei}@huawei.com
{mehdi.rezagholizadeh, vahid.partovinia}@huawei.com
masoud.asgharian2@mcgill.ca
Abstract
The large number of parameters of some
prominent language models, such as BERT,
makes their ﬁne-tuning on downstream tasks
computationally intensive and energy hungry.
Previously researchers were focused on lower
bit-width integer data types for the forward
propagation of language models to save mem-
ory and computation. As for the backward
propagation, however, only 16-bit ﬂoating-
point data type has been used for the ﬁne-
tuning of BERT. In this work, we use integer
arithmetic for both forward and back propa-
gation in the ﬁne-tuning of BERT. We study
the effects of varying the integer bit-width on
the model’s metric performance. Our integer
ﬁne-tuning uses integer arithmetic to perform
forward propagation and gradient computation
of linear, layer-norm, and embedding layers of
BERT. We ﬁne-tune BERT using our integer
training method on SQuAD v1.1 and SQuAD
v2., and GLUE benchmark. We demonstrate
that metric performance of ﬁne-tuning 16-bit
integer BERT matches both 16-bit and 32-bit
ﬂoating-point baselines. Furthermore, using
the faster and more memory efﬁcient 8-bit in-
teger data type, integer ﬁne-tuning of BERT
loses an average of 3.1 points compared to the
FP32 baseline.
1 Introduction
Over the past few years, integration of attention
mechanisms into deep learning models led to the
creation of transformer based models. BERT (De-
vlin et al., 2018) is a prominent transformer based
language model which has shown state-of-the-art
performance in natural language processing (NLP)
tasks.
BERT requires high memory and computational
resources due to its large number of parameters.
Having large number of parameters incurs chal-
lenges for inference, training, and also ﬁne-tuning
*Equal contribution.
FP32 Int32 Int16 Int8050100150JoulesCPU Energy Consumption for 1 Billion Operations
Addition
Multiplication
FP32 Int32 Int16 Int80.00.51.01.5SecondsCPU Latency for 1 Billion Operations
Addition
MultiplicationFigure 1: Energy consumption and latency of 1 billion
operations using various data types, measured on an
Intel®Xeon®CPU E5-2698 v4.
of this model. Moreover, the training phase i.e. pre-
training and ﬁne-tuning, involves more operations
compared to the inference. More speciﬁcally, the
training phase includes gradient computation and
weight update that make the training more compu-
tationally intensive.
One method of reducing the computational com-
plexity of deep learning models is to represent their
parameters and activations in low bit-width data
types. This reduces the memory footprint of the
model and enables more efﬁcient computations.
For instance, Figure 1 shows that low-bit integer
data types have higher throughput and better energy
consumption compared to ﬂoating-point.
Previous research attempts at integer quantiza-
tion of transformer based language models were
only focused on forward propagation and the gradi-
ent computation were kept in 32-bit ﬂoating-point
data type (FP32) (Bhandare et al., 2019; Kim et al.,
2021; Zafrir et al., 2019).
Furthermore, earlier efforts for using low bit-
width data types for gradient computation of trans-
former based language models has only been lim-
ited to 16-bit ﬂoating-point (FP16). This method,arXiv:2209.09815v2  [cs.LG]  12 Feb 2023

--- PAGE 2 ---
known as mixed precision training (Micikevicius
et al., 2017), uses FP16 data type to represent
weights, activations and gradients while using FP32
for the weight update.
Here we present an integer ﬁne-tuning method
for transformer based language models such as
BERT. Unlike previous works, we use integer data
types for both forward propagation and gradient
computation during the ﬁne-tuning of BERT. More-
over, we use the dynamic ﬁxed-point format to
represent ﬂoating-point numbers as integers.
Our integer mapping strategy can be used along-
side ﬂoating-point numbers in ﬁne-tuning and in-
ference similar to mixed precision training. In our
proposed strategy, the arithmetic of all the compute
intensive layers for both forward and back propaga-
tion are performed using integer arithmetic while
other components of the model, such as nonlinear
functions and the weight updates are kept in FP32.
We use integer versions of compute intensive lay-
ers such as linear, normalization (layer-norm), and
embedding layers.
We study the effect of various bit-widths of the
integer input activation and show that increasing
the bit-width of the ﬁxed-point mapping function
improves the convergence behaviour of the model.
This enables us to ﬁnd the minimum bit-width re-
quired for integer ﬁne-tuning of BERT.
Our ﬁne-tuning experiments show that 16-bit
integer BERT is able to match the metric perfor-
mance of mixed precision FP16 and FP32 methods.
We also further reduce the bit-widths and show
that integer ﬁne-tuning of BERT with 8-bit integer
weights and 12-bit integer activations has a score
drop of 3:1compared to the original performance.
To summarize, this paper makes the following
contributions:
•Integer ﬁne-tuning of transformer based lan-
guage models that uses integer arithmetic for
both the forward and back propagation of
compute intensive layers such as linear, layer-
norm, and embedding. To the best of our
knowledge, this is the ﬁrst time that integer
data type is used for back propagation of pre-
trained language models.
•Analyzing the effect of changing the bit-width
of dynamic ﬁxed-point format on the conver-
gence of ﬁne-tuning. Remark 3 discusses that
the convergence behaviour of our integer ﬁne-
tuning is directly related to the variance of dy-namic ﬁxed-point mapping and is controlled
by the bit-width.
•We show that ﬁne-tuning BERT using 16-
bit integer numbers is able to outperform the
FP16 mixed precision ﬁne-tuning method.
The rest of this paper is structured as follows.
Section 2 brieﬂy discusses previous works in which
low bit-width data types are used for inference and
training of deep learning models. Section 3 pro-
vides details of our integer ﬁne-tuning method, in-
cluding the representation mapping functions and
integer-only layers. The convergence behaviour
of the dynamic ﬁxed-point mapping is studied in
Section 4 by providing empirical observations and
theoretical analysis. The ﬁne-tuning experiments
on various integer and ﬂoating-point setups are pre-
sented in Section 5. Finally, Section 6 concludes
the ideas proposed in this work.
2 Related Works
In this section we discuss the previous works that
use low bit-width data types in transformer based
language models. These works could be catego-
rized into two major groups. In the ﬁrst group,
called low-bit inference, the low bit-width data
types are used only in the forward propagation
phase to improve computational complexity and
reduce memory usage during the inference. In the
second group, also known as low-bit training, lower
bit-width data types are used for both the forward
and back propagation phases.
2.1 Low-bit Inference
Previous research on low-bit inference quantize the
model parameters and activations to speed up the
forward propagation. This category is itself divided
into quantization-aware training (QAT) and post-
training quantization (PTQ) methods.
In QAT, quantization is performed during train-
ing, allowing the model parameters to adapt to the
quantization noise. QAT relies on high-precision
FP32 gradients to train the model and adapt it to
the quantization noise.
For instance, (Zafrir et al., 2019) proposed
Q8BERT which quantizes the inference compu-
tations of all linear and embedding layers of BERT
to 8-bit integers and updates the quantization scale
with a moving average. Similarly, (Shen et al.,
2020) suggested Q-BERT which requires the com-
putation of hessian matrix for each group of param-

--- PAGE 3 ---
eters to be used in a mixed precision ﬁne-tuning
with different bit-widths. (Kim et al., 2021) pro-
posed I-BERT that uses a uniform quantization
scheme to quantize input activations and weights
of various components of BERT. In I-BERT, the
quantization scaling factors are computed based on
the distribution of the training data.
Unlike QAT that performs quantization of in-
ference operations during training, Post-Training
Quantization (PTQ) methods apply quantization
to the parameters when the training is completed.
Thus, they require extra calibration or parameter
tuning to adapt the model to the quantized parame-
ters.
For instance, (Bhandare et al., 2019) quantized
the matrix multiplications of the original trans-
former architecture from (Vaswani et al., 2017) to
8-bit integer data type. Moreover, the quantization
is done only for the forward propagation and re-
quires extra calibration using validation data to tune
the boundaries of the quantization function. (Zadeh
et al., 2020) introduced GOBO which compresses
the ﬁne-tuned weights of BERT by grouping them
into two categories of Gaussian and outlier. The
outlier weights are kept in FP32, while the Gaus-
sian weights are quantized to lower bits. For lower
bit-width regimes, TernaryBERT and BinaryBERT
are able to push the quantization to 2 and 1 bits
respectively (Zhang et al., 2020a; Bai et al., 2020).
They both rely on methods such as data augmenta-
tion and knowledge distillation to adapt the model
to the low-bit weights.
2.2 Low-bit Training
Research on low-bit training try to perform both
the forward propagation and gradient computation
in low-bit arithmetic. Using low precision number
formats for gradients reduces the model’s ability to
adapt the parameters to the quantization noise, but
increases the throughput and reduces the memory
footprint.
FP16 mixed precision training (Micikevicius
et al., 2017) is a common method currently for
low-bit ﬁne-tuning of transformer based language
models. This method uses FP16 data type in both
forward propagation and gradient computation,
while using FP32 for the weight update. Unlike
FP16 mixed precision training, our work uses dy-
namic ﬁxed-point format which allows for multiple
choices of bit-width for the data type. We show that
our 16-bit integer ﬁne-tuning method outperformsFP16 mixed precision training in terms of metric
score.
Using integer data types in the training of deep
learning models has been previously studied for
the computer vision tasks. For instance, (Zhang
et al., 2020b) quantized the input activations, gradi-
ents and parameters of the linear layers for various
convolutional neural networks (CNN). Similarly,
(Zhao et al., 2021) adapted the quantization pa-
rameters by detecting the distribution of the gra-
dients in the channel dimension. In both these
works the quantization error is measured during
training and is used to adjust the quantization scale,
whereas our method does not require any informa-
tion about distribution of data or gradients. (Zhu
et al., 2020) applied a quantization scheme to train
CNN architectures with “direction sensitive gradi-
ent clipping” and learning rate scaling to control
the quantization error of gradients. Our integer
ﬁne-tuning method does not require gradient clip-
ping and can follow the same loss trajectory as
the ﬂoating-point baseline with the same hyper-
parameters. Our proposed method improves upon
(Ghaffari et al., 2022) which uses dynamic ﬁxed-
point format for integer training of deep learning
models. Unlike (Ghaffari et al., 2022), our work
studies various bit-widths for both weights and ac-
tivations to ﬁnd the minimum bit-width required
for ﬁne-tuning BERT. Furthermore, we study in-
teger training method on large language models
where low-bit quantization is known to be a chal-
lenging task (Bondarenko et al., 2021). To the best
of our knowledge, this is the ﬁrst time where inte-
ger numbers are used for the back propagation of
transformer based language models.
3 Methodology
3.1 Representation Mapping
We use the dynamic ﬁxed-point format
(Williamson, 1991) to map the ﬂoating-point
numbers to integer data type. This format, also
known as block ﬂoating-point, maps ﬂoating-point
numbers to blocks of integer numbers, with
each block having its unique scale. For more
information on various number formats refer to
Appendix A.
We use a linear ﬁxed-point mapping function
to map ﬂoating-point numbers to integer numbers.
The linear ﬁxed-point mapping converts a ﬂoating-
point tensor Fto a tensor of integers and a single
scale factor.

--- PAGE 4 ---
Linear Fixed-point
Mapping  Float  
Parameter
Integer  
Input ActivationInput Activation
ScaleFloat  
Input ActivationInteger  
Parameter
Matrix
Multiplication  
Non-linear Inverse
Mapping  
OutputParameter  
ScaleAdd Unpack Floating-
point  
Float T ensorSign
Mantissa
Exponent Tensor ScaleSigned b-bit
Integer T ensorb-bit
Rounding  
maximum  Figure 2: Forward propagation operations in an integer-only linear layer. Green boxes use integer arithmetic and
red boxes use ﬂoating-point data type. Here, the integer output is generated using an integer matrix multiplication
and the output scale is generated by a single add operation. The bottom panel shows the linear ﬁxed-point mapping
for the input tensors, that are the input activation and the parameter tensor in this ﬁgure.
The integers are obtained by rounding the
ﬂoating-point mantissas. The scale is the maximum
of the ﬂoating-point exponents of F. The bottom
section of Figure 2 shows the internal operations
of the linear ﬁxed-point mapping.
To map the ﬁxed-point numbers to ﬂoating-point,
a non-linear inverse mapping function is used. The
inverse mapping converts integer numbers into nor-
malized ﬂoating-point mantissas and packs each
integer with its corresponding scale into a ﬂoating-
point number.
Details of the representation mapping functions
are provided in (Ghaffari et al., 2022). Our method-
ology differs in that it includes various bit-widths
for both weights and activations for the ﬁne-tuning
of transformer based language models. We exploit
this mapping strategy to explore various bit-widths
for weights and activations in order to ﬁnd the min-
imum bit-width for ﬁne-tuning the model.
3.2 Integer Fine-tuning
Our method uses integer arithmetic for weights,
activations and gradients, while the weight update
is kept in FP32. Moreover, our proposed BERT
setups use integer-only versions for all the linear,
layer-norm and embedding layers in which internal
operations are performed with integer arithmetic.
3.2.1 Linear Layer
Figure 2 depicts a high-level view of forward prop-
agation operations of the integer-only linear layer.
All the parameters and activations of the layer areﬁrst mapped to dynamic ﬁxed-point using the linear
ﬁxed-point mapping function. In the case of linear
layer, the integer parameters and input activations
are then sent to an integer matrix multiplication
function to generate the integer output. If needed,
the integer output could be mapped back to ﬂoating-
point to be used by other layers of the model using
the non-linear inverse mapping.
For back propagation, the gradients of the param-
eters and input activations are also computed using
integer arithmetic. Using integer matrix multipli-
cation, the output gradients are multiplied by input
activations and parameters to compute the gradi-
ents. Since the weight update is performed in FP32,
the integer gradients and their scales are passed to
the non-linear inverse mapping to be mapped to
FP32.
3.2.2 Layer-norm
The layer normalization or layer-norm performs
the following operation on its input X(Ba et al.,
2016):
X p
2++. (1)
Hereandare the weight and bias parameters,
andandare input standard deviation and mean
respectively. For the forward propagation of inte-
ger layer-norm we map Xto dynamic ﬁxed-point
format and compute andusing integer arith-
metic. Note that multiplication to and addition
withare also performed using integer arithmetic.

--- PAGE 5 ---
8 910 11 12 13 14 15 16
Dynamic Fixed-point bit-width505560657075F1 ScoreBERT on SQuAD v2.0 Dataset
Dynamic Fixed-point
FP32 BaselineFigure 3: F1 score of ﬁne-tuning BERT using b-bit gra-
dients, and activations on SQuAD v2.0 dataset. For the
8-bit and 9-bit ﬁxed-point bit-widths, we use 12-bit in-
put activations.
Moreover, the back propagation also uses integer
arithmetic to compute the gradients for the input,
, and.
3.2.3 Embedding Layer
The embedding layer is a lookup table that stores
embeddings. The layer takes a list of indices as
input and returns the list of corresponding embed-
dings for each index. The integer embedding layer,
handles integer embeddings and needs less mem-
ory footprint to store these values. For the back
propagation, the embedding layer applies the out-
put integer gradients directly to each corresponding
row of the lookup table.
4 Convergence Behaviour of Dynamic
Fixed Point Mapping
4.1 Empirical Observations
Figure 2 shows that the bit-width, b, is controlled by
adjusting the number of rounded bits in the round-
ing function. Here we study the effect of changing
the integer bit-width on the metric performance of
the model.
The motivation of varying the bit-width of the dy-
namic ﬁxed-point is to control the variance induced
by the linear ﬁxed-point mapping. Our experiments
show that using dynamic ﬁxed-point with a bit-
width of 10 achieves the same performance as the
FP32 ﬁne-tuning method. Figure 3 demonstrates
the F1 score of ﬁne-tuning BERT on SQuAD v2.0
dataset against the ﬁxed-point bit-width. Note that
the ﬁxed-point arithmetic with a bit-width higher
8 910 11 12 13 14 15 16
Input Activation bit-width505560657075F1 ScoreBERT with 8-bit Dynamic Fixed-point
on SQuAD v2.0 DatasetFigure 4: F1 score of ﬁne-tuning BERT using 8-bit
weights and gradients, with varying input activation bit-
width on SQuAD v2.0 dataset. Note that Remark 3
justiﬁes this experiment using the variance of b-bit dy-
namic ﬁxed-point mapping.
than 10 bits is able to closely match the F1 score of
the FP32 baseline, that is indicated by the red line
in the ﬁgure. Also note that in our experimental
setup for the 8-bit dynamic ﬁxed-point format, we
use 12-bit input activations to close the F1 score
gap with the FP32 baseline. The reason for using
higher bit-width input activations is that we ob-
served 8-bit activation dramatically reduces the F1
score. Figure 4 shows the effect of input activa-
tion bit-width on the F1 score when the weights
are 8-bit integers. Changing the bit-width of the
input activation from 8 bits to 12 bits signiﬁcantly
increases the F1 score. Increasing the input activa-
tion bit-width beyond 12 bits has a negligible effect
on the F1 score, conﬁrming that 12 bits is the mini-
mum required bit-width of the input activations for
this application with 8-bit integer weights.
4.2 Theoretical Analysis
Here, we study the effect of varying dynamic ﬁxed-
point mapping bit-width on the stochastic gradient
descent method. The goal is to show the relation
of weight and activation bit-widths on the conver-
gence of integer training. Let us consider the fol-
lowing simpliﬁed weight update equation
wk+1=wk+ ^g(wk;k); (2)
where ^g(wk;k)is the dynamic ﬁxed-point gradi-
ent and is the learning rate during the ﬁne-tuning
phase. Furthermore, we also consider the following
common assumptions in sequel.

--- PAGE 6 ---
Assumption 1 (Lipschitz-continuity). The loss
functionL(w)is continuously differentiable and its
gradients satisﬁes the following inequality where
L>0is the Lipchitz constant
L(w)6L( w)+rL( w)>(w w)
+1
2Ljjw wjj2
2;
8w;w2Rd: (3)
Assumption 2. (i)L(wk)is bounded. (ii) b-bit
dynamic ﬁxed-point gradients ^g(wk;k)is an unbi-
ased estimator of the true gradients of the loss func-
tionrL(wk)>Ekf^g(wk;k)g=jjrL(wk)jj2
2=
jjEkf^g(wk;k)gjj2
2;and (iii) with the b-bit dy-
namic ﬁxed-point gradients i.e. ^g(wk;k), there
exist scalars M>0,MV>0,Mq>0and
Mq
V>0such that for all iterations of SGD
Vkf^g(wk;k)g
6M+Mq+ (MV+Mq
V)jjrL(wk)jj2
2:
WhereMqandMq
Vdenote the added variance of
b-bit dynamic ﬁxed-point mapping on the true gra-
dient variance. Also note that in order for Assump-
tion 2 (i) to hold true, we use stochastic rounding
for back propagation.
Suppose Assumption 1 andAssumption 2 are
true, then inequality (4)follows from (Ghaffari
et al., 2022, Remark 2)
EkfL(wk+1)g L (wk)
6 (1 1
2L(MG+Mq
G))jjrL(wk)jj2
2
+1
22L(M+Mq);
withMG:= 1 +MVandMq
G:= 1 +Mq
V;
(4)
which shows the effect of added variance of ﬁxed
point mapping, i.e. Mq
VandMq, on each step of
the optimizer.
Remark 1. In inequality (4), the ﬁrst term,
 (1 1
2L(MG+Mq
G))jjrL(wk)jj2
2contribute
to decreasing the loss Lwhile the second term,
1
22L(M+Mq), prevents it. Also note that when
MqandMq
Gare increased, they negatively affect
the descent of the loss L. This means for a good
convergence behaviour, representation mappingvariance bounds, i.e. MqandMq
G, must be con-
trolled.
Remark 2. For dynamic ﬁxed-point mapping with
b-bit integers, the representation mapping variance
bounds i.e.MqandMq
G, are closely related to the
bit-widthb. Here, we study these two constants for
a linear layer. Let us denote ^Aas theb-bit dynamic
ﬁxed-point version of tensor Aand^aijas itsijth
element. We can relate ^aijandaijwith an error
termsuch as ^aij=aij+A
ij. For a linear layer
^Y=^X^W, the computation of the b-bit dynamic
ﬁxed-point gradients in the back propagation is
^C=@^L
@^W=@^Y
@^W@^L
@^Y=^X>@^L
@^Y=^X>^G:(5)
It is of interest to ﬁnd the relation between ^C=
^X>^Gin the integer back propagation and the true
gradients C=X>G. We can derive the variance
for each element ^cijby expanding the error terms
,
Vf^cijg=V(NX
n=1^xni^gnj)
=V(NX
n=1(xni+X
ni)(gnj+G
nj))
6V(KX
n=1xnignj)
+2
GEfjjX>
i:jj2
2g+2
XEfjjG:jjj2
2g
+N2
X2
G
=Vfcijg+2
GEfjjX>
i:jj2
2g
+2
XEfjjG:jjj2
2g+N2
X2
G:
(6)
In inequality (6),2
G= maxi;j(VfG
i;jg)and
2
X= maxi;j(VfX
i;jg). Also notejjX>
i:jj2
2=PJ
jx2
jidenotes the squared L-2 norm of the ithrow
ofX>andjjG:jjj2
2=PI
ig2
ijdenotes the squared
L-2 norm of the jthcolumn ofG. Furthermore, by
deﬁning
(
Mq:=2
G(EfjjX>
i:jj2
2g+N2
X)
Mq
V:=2
X(7)
Equation (7)shows thatMqdepends on variance
of dynamic ﬁxed-point mapping for input activa-
tions and gradients while Mq
Gonly depends on b-bit
dynamic ﬁxed-point gradients variance.

--- PAGE 7 ---
QQP QNLI MNLI SST-2 STSB RTE MRPC CoLA Average
FP32 91.0/88.0 91.1 84.2 92.5 88.3 63.8 82.5/87.8 57.2 82.6
FP16 AMP 90.9/87.9 91.2 84.1 92.4 88.3 64 82.1/87.7 57.5 82.6
16-bit integer 91.0/88.0 91.2 84.2 92.5 88.3 64.5 82.3/87.6 57.7 82.7
12-bit integer 90.9/88.0 91.2 84.0 92.6 87.9 63.5 81.3/87.4 56.7 82.4
10-bit integer 90.8/87.8 91.0 84.0 92.5 87.5 62.7 78.4/85.8 57.6 81.8
8-bit integer 90.1/86.8 90.8 83.7 92.3 87 61.8 76.8/84.7 55.0 80.9
Table 1: Metric performance of integer ﬁne-tuning of BERT on selected GLUE tasks. The reported metric for
QQP and MRPC is accuracy and F1 score, for QNLI, MNLI, RTE, and SST-2 is accuracy, for STSB is the Pearson-
Spearman correlation, and for CoLA is the Matthews correlation.
Proposition 1. For dynamic ﬁxed-point representa-
tion of tensor ^Awithb-bit integers, the variance of
error for element isatisﬁes the following inequality
VfA
ig622(escaleA b+2): (8)
Proof .Using dynamic ﬁxed-point mapping to b-bit
integers, the error A
isatisﬁes the following bound
 2escaleA(0:000001) 2|{z}
b 16A
i62escaleA(0:000001) 2|{z}
b 1
 2escaleA b+26A
i62escaleA b+2:
(9)
Thus, the inequality (8)is obtained by using
Popoviciu’s inequality on variances
VfA
ig61
4(2escaleA b+2 ( 2escaleA b+2))2
622(escaleA b+2): (10)
Remark 3. Inequality (8)shows that increasing bit-
widthbin dynamic ﬁxed-point mapping reduces
the variance of the error. This conﬁrms our ex-
perimental results on SQuAD v2.0 dataset that for
b > 10, F1 score can match FP32 baseline, see
Figure 3. Also note in equation (7), bothMqand
Mq
Vdepend onb-bit dynamic ﬁxed-point mapping
variance of input activation 2
X. Hence, increas-
ingbfor input activations while keeping weights
in 8-bit format must improve the convergence be-
haviour. This phenomenon is also conﬁrmed by
our experimental results on SQuAD v2.0 dataset
demonstrated in Figure 4.
5 Experimental Results
5.1 Experimental Setup
We ﬁne-tuned BERT base on a series of down-
stream tasks to compare the performance of our in-
teger ﬁne-tuning method with FP16 and FP32 ﬁne-
tuning methods. FP16 AMP setup uses NVIDIA’sSQuAD v1.1 SQuAD v2
FP32 80.5/88.0 70.6/73.8
FP16 AMP 79.9/87.6 70.6/73.9
16-bit integer 80.7/88.0 70.6/73.9
12-bit integer 79.8/87.6 70.5/73.8
10-bit integer 78.4/86.6 69.8/73.2
8-bit integer 75.6/84.5 65.5/69.2
Table 2: Metric performance of ﬁne-tuning BERT on
SQuAD v1.1 and v2.0 datasets. For both datasets the
exact match metrics and F1 scores are reported.
automatic mixed precision1and the FP32 baseline
is the default implementation from Pytorch.
The model is ﬁne-tuned on selected tasks of
GLUE benchmark (Wang et al., 2018), along with
the Stanford Question Answering Datasets, i.e.
SQuAD v1.1 and SQuAD v2.0 (Rajpurkar et al.,
2016).
All the ﬁne-tuning setups use the same hyper-
parameters and are ﬁne-tuned for the same number
of epochs. Each reported metric is the average
of ﬁve runs with ﬁve different random seeds to
mitigate the effects of random variation of the re-
sults. The ﬁne-tuning experiments are performed
based on the ﬁne-tuning scripts of the Hugging
Face library (Wolf et al., 2019). For GLUE exper-
iments the ﬁne-tuning is performed for 5 epochs
and the learning rate is set to 210 5. Also, the
per-device ﬁne-tuning batch-size is set to 32. Fine-
tuning BERT on SQuAD datasets is done for 2
epochs and the learning rate is 510 5and the
per-device ﬁne-tuning batch-size is 12. All experi-
ments are run on eight NVIDIA V100 GPUs with
32 gigabytes of VRAM.
1https://developer.nvidia.com/automatic-mixed-precision

--- PAGE 8 ---
0 500 1000 1500 2000 2500
Fine-tuning Steps12345Fine-tuning LossBERT on SQuAD v2.0 Dataset
8-bit
16-bit
FP32 BaselineFigure 5: Integer ﬁne-tuning loss trajectory of BERT
on SQuAD v2.0 dataset for 2750 iterations.
5.2 Results
The results of ﬁne-tuning BERT base on GLUE
benchmark and SQuAD datasets are presented in
Table 1 and Table 2 respectively. GLUE benchmark
contains a series of downstream tasks, designed to
evaluate a diverse set of language understanding
abilities of NLP models. SQuAD datasets con-
tain a series of text passages accompanied by a
question and the task is to predict the span of the
answer in the passage. Using 16-bit integer data
type, BERT is able to either match or outperform
the FP32 performance for all tasks. The 16-bit inte-
ger BERT also shows similar or better performance
compared to the FP16 mixed precision ﬁne-tuning
method. Further reducing the integer bit-width to 8,
ﬁne-tuning BERT exhibits an average of 1.7 point
drop on GLUE benchmark and 4.5 point drop for
SQuAD datasets. Moreover, our experiments show
that using 10-bit and 12-bit integers has average
score drops of 0.8 and 0.3 points for GLUE tasks,
and 0.8 and 0.2 point for SQuAD datasets respec-
tively.
5.3 Loss Trajectory
Figure 5 shows the loss trajectory of integer ﬁne-
tuning BERT on SQuAD v2.0 dataset using 16-bit
and 8-bit integers, along with FP32 method. The
ﬁne-tuning loss trajectory of BERT using 16-bit
integer closely follows the FP32 loss trajectory. On
the other hand, when ﬁne-tuning with 8-bit integer
parameters and 12-bit integer input activations, the
loss trajectory is slightly shifted, but follows the
same trend of its FP32 counterpart.6 Conclusion
We proposed an integer ﬁne-tuning method for
transformer based language models using dynamic
ﬁxed-point format. We used dynamic ﬁxed-point
data type to represent parameters, input activations
and gradients in integer values. As a result, our
ﬁne-tuning method uses integer arithmetic for the
forward and back propagation of compute intensive
layers such as linear, layer-norm and embedding
layers of BERT model. Furthermore, we studied
that increasing the bit-width of the dynamic ﬁxed-
point format reduces the variance of the mapping
function and thus, improves the convergence of our
integer ﬁne-tuning method. We conduct ﬁne-tuning
experiments on GLUE benchmark and SQuAD
datasets to compare the metric performance of our
integer BERT with FP16 mixed precision and FP32
ﬁne-tuning methods. Our experiments show that
the 16-bit integer ﬁne-tuning is able to achieve the
same metric performance as the FP16 mixed pre-
cision ﬁne-tuning method. In addition, ﬁne-tuning
BERT with lower bit-width data types, i.e. 8-bit
integer, maintains an average drop of metric score
within 3.1 points of the FP16 setup.
Limitations
Although our integer ﬁne-tuning method uses inte-
ger numbers for compute intensive layers of BERT,
integer support for non-linear layers of BERT, e.g.
softmax and GELU activation, are left for future
work.
We have shown in Figure 1 that the integer data
types are faster for the general case. However, a
direct comparison of the time and memory cost of
our integer ﬁne-tuning method with the FP16 and
FP32 methods is left for future works due to lack
of access to a proper hardware with integer tensor
core support.
Despite the similarities between ﬁne-tuning and
pre-training phases, they differ in key aspects of
training such as dataset size and number of epochs.
The challenges of using integer arithmetic in the
pre-training phase will be studied in the future
work.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450 .

--- PAGE 9 ---
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,
Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
2020. Binarybert: Pushing the limit of bert quanti-
zation. arXiv preprint arXiv:2012.15701 .
Aishwarya Bhandare, Vamsi Sripathi, Deepthi
Karkada, Vivek Menon, Sun Choi, Kushal Datta,
and Vikram Saletore. 2019. Efﬁcient 8-bit quantiza-
tion of transformer neural machine language trans-
lation model. arXiv preprint arXiv:1906.00532 .
Yelysei Bondarenko, Markus Nagel, and Tijmen
Blankevoort. 2021. Understanding and overcoming
the challenges of efﬁcient transformer quantization.
arXiv preprint arXiv:2109.12948 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza
Tayaranian, Masoud Asgharian, and Vahid Par-
tovi Nia. 2022. Is integer arithmetic enough
for deep learning training? arXiv preprint
arXiv:2207.08822 .
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W
Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-
only bert quantization. In International conference
on machine learning , pages 5506–5518. PMLR.
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev,
Ganesh Venkatesh, et al. 2017. Mixed precision
training. arXiv preprint arXiv:1710.03740 .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low
precision quantization of bert. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , vol-
ume 34, pages 8815–8821.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems , 30.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Darrell Williamson. 1991. Dynamically scaled ﬁxed
point arithmetic. In [1991] IEEE Paciﬁc Rim Con-
ference on Communications, Computers and Signal
Processing Conference Proceedings , pages 315–318.
IEEE.Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
towicz, et al. 2019. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv
preprint arXiv:1910.03771 .
Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,
and Andreas Moshovos. 2020. Gobo: Quantiz-
ing attention-based nlp models for low latency and
energy efﬁcient inference. In 2020 53rd Annual
IEEE/ACM International Symposium on Microarchi-
tecture (MICRO) , pages 811–824. IEEE.
Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy Efﬁcient Machine
Learning and Cognitive Computing-NeurIPS Edi-
tion (EMC2-NIPS) , pages 36–39. IEEE.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020a. Ternarybert:
Distillation-aware ultra-low bit bert. arXiv preprint
arXiv:2009.12812 .
Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu,
Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zi-
dong Du, Tian Zhi, et al. 2020b. Fixed-point
back-propagation training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 2330–2338.
Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya
Zhang, Zhenyu Gu, and Yinghui Xu. 2021. Distri-
bution adaptive int8 quantization for training cnns.
InProceedings of the AAAI Conference on Artiﬁcial
Intelligence , volume 35, pages 3483–3491.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu,
Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie
Yan. 2020. Towards uniﬁed int8 training for con-
volutional neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 1969–1979.
A Data Types
In this section we provide a brief overview of vari-
ous data types mentioned in this work.
Floating-point data type is used to represent dec-
imal fractional numbers. A binary ﬂoating-point
number has three components of sign ( s), mantissa
(m), and exponent ( e). Using these components,
ﬂoating-point number xis shown as:
x= ( 1)sm2e t
wheretis the precision and 0m2t 1. An-
other way of representing ﬂoating-point numbers
is as
x= ( 1)s2e(d1
2+d1
4+:::+dt
2t)

--- PAGE 10 ---
wherediare binary digits of m. For FP32, exponent
and mantissa are 8 and 23 bit integer numbers.
Fixed-point is another data type for representing
fractional numbers. Unlike ﬂoating-point numbers
where each mantissa is scaled using its respective
exponent, ﬁxed-point uses a single scale factor for
all the numbers.
We use the dynamic ﬁxed-point data type in our
integer ﬁne-tuning method. Also known as block
ﬂoating-point, this format uses a different scale for
each block of numbers to allow for more ﬂexibility.
