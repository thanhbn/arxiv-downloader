# 2305.13235.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.13235.pdf
# Kích thước tệp: 1067471 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SPARSE FIT: Prompting Few-shot với Sparse Fine-tuning để Tạo ra Đồng thời Dự đoán và Giải thích Ngôn ngữ Tự nhiên

Jesus Solano
ETH Zürich
jesus.solano@inf.ethz.ch

Mardhiyah Sanni
University of Edinburgh
m.o.sanni@sms.ed.ac.uk

Oana-Maria Camburu
University College London
o.camburu@ucl.ac.uk

Pasquale Minervini
University of Edinburgh
p.minervini@ed.ac.uk

Tóm tắt
Các mô hình tạo ra giải thích ngôn ngữ tự nhiên (NLE) cho dự đoán của chúng gần đây đã thu hút sự quan tâm ngày càng tăng. Tuy nhiên, cách tiếp cận này thường đòi hỏi các tập dữ liệu lớn của NLE được viết bởi con người cho các câu trả lời đúng trong thời gian huấn luyện, điều này có thể tốn kém và có thể không khả thi cho một số ứng dụng. Khi chỉ có một vài NLE có sẵn (thiết lập few-shot), việc fine-tuning các mô hình ngôn ngữ được huấn luyện trước (PLM) kết hợp với học tập dựa trên prompt gần đây đã cho thấy kết quả đầy hứa hẹn. Tuy nhiên, PLM thường có hàng tỷ tham số, khiến việc fine-tuning đầy đủ trở nên đắt đỏ. Chúng tôi đề xuất SPARSE FIT, một chiến lược fine-tuning sparse few-shot tận dụng các prompt rời rạc để tạo ra đồng thời dự đoán và NLE. Chúng tôi thực nghiệm với SPARSE FIT trên ba kích thước của mô hình ngôn ngữ T5 và bốn tập dữ liệu và so sánh nó với các kỹ thuật Parameter-Efficient Fine-Tuning (PEFT) tiên tiến hiện có. Chúng tôi phát hiện rằng việc fine-tuning chỉ 6.8% tham số mô hình dẫn đến kết quả cạnh tranh cho cả hiệu suất tác vụ và chất lượng của NLE được tạo ra so với việc fine-tuning đầy đủ mô hình và tạo ra kết quả tốt hơn trung bình so với các phương pháp PEFT khác về độ chính xác dự đoán và chất lượng NLE.

1 Giới thiệu
Mặc dù thành công to lớn của mạng neural (Chowdhery et al., 2022; Brown et al., 2020), các mô hình này thường thiếu các giải thích có thể hiểu được bởi con người cho dự đoán của chúng, điều này là tối quan trọng để đảm bảo độ tin cậy của chúng. Việc xây dựng các mô hình neural giải thích dự đoán của chúng bằng ngôn ngữ tự nhiên đã thấy sự quan tâm ngày càng tăng trong những năm gần đây (Wiegreffe và Marasovic, 2021). Giải thích Ngôn ngữ Tự nhiên (NLE) thường dễ diễn giải bởi con người và biểu cảm hơn các loại giải thích khác (Wallace et al., 2019; Wiegreffe và Marasovic, 2021). Tuy nhiên, một nhược điểm đáng kể của các mô hình này là chúng đòi hỏi các tập dữ liệu lớn của NLE được viết bởi con người tại thời điểm huấn luyện, điều này có thể tốn kém và tốn thời gian để thu thập. Để giải quyết vấn đề này, học tập few-shot của NLE gần đây đã nổi lên (Marasovic et al., 2022; Yordanov et al., 2022). Tuy nhiên, các kỹ thuật hiện tại liên quan đến việc fine-tuning toàn bộ mô hình với một vài ví dụ huấn luyện NLE. Điều này tốn kém về mặt tính toán vì các mô hình NLE hiện tại có thể có hàng tỷ tham số (Schwartz et al., 2020).

Trong bài báo này, chúng tôi nghiên cứu liệu sparse fine-tuning (tức là fine-tuning chỉ một tập con tham số), kết hợp với học tập dựa trên prompt (tức là, các hướng dẫn văn bản được cung cấp cho mô hình (Liu et al., 2021)), có thể giúp ích trong các tình huống có sự sẵn có hạn chế của các thể hiện huấn luyện với nhãn và NLE. Trong khi sparse fine-tuning đã được sử dụng trong Xử lý Ngôn ngữ Tự nhiên (NLP) (Houlsby et al., 2019; Logan et al., 2022; Zaken et al., 2022), theo hiểu biết của chúng tôi, công trình của chúng tôi là đầu tiên phân tích sparse fine-tuning trong bối cảnh tạo ra đồng thời dự đoán và NLE. Chúng tôi mở rộng chiến lược sparse fine-tuning hiện có chỉ xem xét các thuật ngữ bias (Zaken et al., 2022) thành một danh sách toàn diện của tất cả các lớp và cặp lớp trong mô hình ngôn ngữ.

Do đó, chúng tôi đề xuất SPARSE FIT, một chế độ huấn luyện few-shot dựa trên prompt hiệu quả cho các mô hình tạo ra cả dự đoán và NLE cho dự đoán của chúng. Chúng tôi thực nghiệm với SPARSE FIT trên hai mô hình ngôn ngữ được huấn luyện trước (PLM) mà trước đây đã cho thấy hiệu suất cao về hiệu suất tác vụ và tạo NLE, cụ thể là T5 (Raffel et al., 2020) và UNIFIED QA (Dong et al., 2019). Chúng tôi kiểm tra cách tiếp cận của chúng tôi trên bốn tập dữ liệu NLE phổ biến: e-SNLI (Camburu et al., 2018), ECQA (Aggarwal et al., 2021), SBIC (Sap et al., 2020), và ComVE (Wang et al., 2019), và đánh giá cả hiệu suất tác vụ và chất lượng của NLE được tạo ra, cái sau với cả các metric tự động và đánh giá của con người. Nhìn chung, SPARSE FIT cho thấy hiệu suất cạnh tranh trong các thiết lập học tập few-shot với 48 thể hiện huấn luyện. Ví dụ, fine-tuning chỉ Normalization Layer cùng với Self-attention Query Layer, chiếm 6.84% tham số của mô hình, liên tục cho hiệu suất tốt nhất (bị phạt bởi số lượng tham số được fine-tuned) trên cả T5 và UNIFIED QA trên tất cả bốn tập dữ liệu. Đáng chú ý, SPARSE FIT vượt trội hơn các mô hình parameter-efficient fine-tuning (PEFT) tiên tiến hiện tại về cả hiệu suất tác vụ và chất lượng NLE được tạo ra trong hai trong số bốn tập dữ liệu. Hơn nữa, chúng tôi thấy rằng việc fine-tuning các thành phần mô hình khác chiếm một phần nhỏ tham số cũng liên tục dẫn đến kết quả cạnh tranh; ví dụ, self-attention query (~6.8% tham số), self-attention query + LM head (~11.3%), và toàn bộ self-attention layer (~20%). Hơn nữa, chúng tôi cũng áp dụng SPARSE FIT cho các mô hình ngôn ngữ lớn hơn (tức là Llama 2-7B) và thấy rằng SPARSE FIT có hiệu suất cạnh tranh so với chiến lược PEFT tốt nhất cho tất cả các tập dữ liệu. Do đó, chúng tôi kết luận rằng sparse fine-tuning few-shot của PLM có thể đạt được kết quả cạnh tranh với việc fine-tuning toàn bộ mô hình.

2 Công trình liên quan
Học tập few-shot đề cập đến việc huấn luyện mô hình với dữ liệu được gán nhãn hạn chế cho một tác vụ nhất định (Finn et al., 2017; Vinyals et al., 2016). Nó đã được áp dụng thành công cho nhiều tác vụ như tạo chú thích hình ảnh (Dong et al., 2018), phân loại đối tượng (Ren et al., 2018), sinh trắc học hành vi (Solano et al., 2020), phân loại nút đồ thị (Satorras và Estrach, 2018), và mô hình ngôn ngữ (Vinyals et al., 2016). Các Mô hình Ngôn ngữ Lớn (LLM) đã cho thấy kỹ năng ấn tượng để học trong các tình huống few-shot (Brown et al., 2020; Chowdhery et al., 2022) nhờ vào kích thước corpus huấn luyện trước và khả năng thống kê của các mô hình (Izacard et al., 2022).

Parameter-Efficient Fine-Tuning Sử dụng fine-tuning, LLM đã cho thấy khả năng hiểu và tạo ngôn ngữ đột phá trong nhiều lĩnh vực (Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022). Tuy nhiên, trong NLP, mô hình up-stream (tức là, mô hình được fine-tuned) thường là một LLM với hàng triệu tham số, như T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), hoặc GPT-3 (Radford et al., 2018), điều này khiến chúng tốn kém về mặt tính toán để fine-tune. Điều này đã dẫn đến các cách tiếp cận được biết đến trong tài liệu như các phương pháp Parameter-efficient Fine-tuning (PEFT), chỉ fine-tune một tập nhỏ tham số của PLM hoặc một tập nhỏ tham số bổ sung để giữ hiệu suất cạnh tranh trong tác vụ downstream. Trong vấn đề này, Li và Liang (2021) đã giới thiệu Prefix-Tuning, một chiến lược tập trung vào việc thêm một vector nhỏ cụ thể cho tác vụ vào đầu vào để PLM đóng băng có thể thích ứng kiến thức của nó với các tác vụ downstream khác. Hu et al. (2022) đã phát triển LoRA, một kỹ thuật tiêm các ma trận low-rank có thể huấn luyện vào kiến trúc transformer trong khi đóng băng các trọng số mô hình được huấn luyện trước. Zhang et al. (2023) đã mở rộng điều này bằng cách đề xuất AdaLoRA, một phương pháp phân bổ thích ứng ngân sách rank giữa các ma trận low-rank trong quá trình huấn luyện theo điểm số quan trọng. Sau đó, Zaken et al. (2022) đã trình bày BitFit, một cách tiếp cận mới nhằm chỉ fine-tuning các thuật ngữ bias trong mỗi lớp của LM dựa trên transformer. Chúng tôi mở rộng công trình của họ để fine-tuning một số lớp, hoặc cặp của chúng, trong LM. Gần đây hơn, Liu et al. (2022) đã giới thiệu (IA)3, một phương pháp fine-tuning chia tỷ lệ các kích hoạt trung gian trong mô hình với các vector đã học.

Khả năng Giải thích của Mô hình Neural Một số cách tiếp cận đã được đề xuất trong tài liệu để mang một mức độ khả năng giải thích cho dự đoán của các mô hình neural, sử dụng các hình thức giải thích khác nhau, như (1) Giải thích dựa trên đặc trưng (Ribeiro et al., 2016; Shrikumar et al., 2017; Yoon et al., 2019; Sha et al., 2021), (2) Giải thích Ngôn ngữ Tự nhiên (Camburu et al., 2018; Marasović et al., 2020; Kayser et al., 2022; Majumder et al., 2022), (3) Giải thích phản thực (Akula et al., 2020), và (4) Giải thích thay thế (Alaa và van der Schaar, 2019). Trong bài báo này, chúng tôi tập trung vào các mô hình cung cấp NLE, tức là, văn bản tự do nêu lý do đằng sau một dự đoán. Là ngôn ngữ tự nhiên, NLE nên dễ diễn giải bởi con người và biểu cảm hơn các loại giải thích khác, vì chúng có thể trình bày các lập luận không có trong đầu vào (Wiegreffe và Marasovic, 2021; Camburu et al., 2021; Kaur et al., 2020). NLE đã được áp dụng cho nhiều lĩnh vực như trả lời câu hỏi (Narang et al., 2020), suy luận ngôn ngữ tự nhiên (Camburu et al., 2018), hệ thống khuyến nghị (Chen et al., 2021), học tăng cường (Ehsan et al., 2018), hình ảnh y tế (Kayser et al., 2022), lý luận thị giác-văn bản (Hendricks et al., 2018; Kayser et al., 2021; Majumder et al., 2022), và giải quyết bài toán toán học (Ling et al., 2017).

Để làm cho các mô hình neural có khả năng tạo ra NLE chính xác, cách tiếp cận phổ biến nhất là chú thích dự đoán với các giải thích được viết bởi con người và huấn luyện mô hình để tạo ra NLE bằng cách đưa chúng như một tác vụ tạo chuỗi (Camburu et al., 2018). Tuy nhiên, việc thu thập các tập dữ liệu lớn với NLE được viết bởi con người là tốn kém và tốn thời gian. Để giải quyết vấn đề này, Yordanov et al. (2022) đã đề xuất một chiến lược học chuyển giao vanilla để học từ một vài NLE nhưng nhiều nhãn trong một tác vụ bằng cách fine-tuning một PLM được huấn luyện trên một số lượng lớn NLE từ các lĩnh vực khác. Gần đây hơn, Marasovic et al. (2022) đã giới thiệu benchmark FEB cho học tập few-shot của NLE và một chiến lược fine-tuning dựa trên prompt, mà chúng tôi sử dụng làm baseline trong công trình của chúng tôi.

3 SPARSE FIT
Chúng tôi đề xuất SPARSE FIT, một chiến lược huấn luyện NLE few-shot hiệu quả tập trung vào việc fine-tuning chỉ một tập con tham số trong một LM lớn. SPARSE FIT được lấy cảm hứng từ (1) Marasovic et al. (2022), người đã sử dụng fine-tuning và prompt để thực hiện học tập few-shot của nhãn và NLE; và (2) BitFit Zaken et al. (2022), người đã cho thấy rằng việc fine-tuning chỉ các thuật ngữ bias trong một PLM dẫn đến hiệu suất cạnh tranh (và đôi khi tốt hơn) so với việc fine-tuning toàn bộ mô hình. Chúng tôi mở rộng BitFit bằng cách khám phá việc fine-tuning các thành phần khác nhau (tức là, các lớp hoặc khối) trong kiến trúc của PLM. Cụ thể, chúng tôi nghiên cứu hiệu suất self-rationalization sau khi fine-tuning các thành phần sau trong mô hình T5: (1) các khối encoder, (2) các khối decoder, (3) language model head, (4) các lớp self-attention, (5) các mạng feed-forward, (6) lớp normalization, và (7) tất cả các cặp của các thành phần trên không chứa encoder và decoder (xem Phụ lục C). Cho rằng kiến trúc của mô hình UNIFIED QA giống với T5, việc diễn giải các tham số hoạt động cũng áp dụng cho UNIFIED QA.

Chúng tôi nhằm xác định một tập hướng dẫn để xác định các thành phần nên được fine-tuned để đạt được hiệu suất cạnh tranh trong khi cập nhật số lượng tham số tối thiểu. Lưu ý rằng khi fine-tuning bất kỳ thành phần nào, hoặc cặp thành phần, chúng tôi đóng băng tất cả các tham số PLM khác và huấn luyện LM để tạo ra có điều kiện một văn bản dưới dạng "[label] because [explanation]".

Encoder Encoder T5 bao gồm N khối transformer, mỗi khối bao gồm ba lớp: self-attention, lớp kết nối đầy đủ theo vị trí, và layer normalization. Số lượng khối phụ thuộc vào biến thể T5 (12 khối cho T5-base, 24 cho T5-large, và 36 cho T5-3B). Encoder chiếm khoảng 41% tham số mô hình.

Decoder Decoder chiếm khoảng 54% tham số mô hình T5. Ngoài self-attention, lớp kết nối đầy đủ, và layer normalization, nó cũng bao gồm một lớp encoder-decoder attention trong các khối của nó, mà chúng tôi fine-tune như một phần của việc fine-tuning decoder.

LM Head Trên đầu decoder, T5 có một language modeling head để tạo văn bản dựa trên corpus. LM head chiếm khoảng 5% tổng tham số mô hình.

Attention Layer Mỗi khối transformer bắt đầu với một lớp self-attention. Có ba loại tham số trong lớp self-attention, cụ thể là, để tính ma trận query Q, ma trận key K, và ma trận value V. Chúng tôi đề xuất khám phá việc fine-tuning mỗi ma trận self-attention như một cấu hình SPARSE FIT có thể. Chúng tôi cũng khám phá việc fine-tuning toàn bộ Self-attention Layer (Q,K,V). Trung bình, tỷ lệ phần trăm tham số có thể huấn luyện liên quan đến mỗi ma trận chiếm khoảng 6% tham số mô hình. Lưu ý rằng các tham số attention trong encoder-decoder attention không được cập nhật trong thiết lập này (chúng chỉ được cập nhật cùng với decoder).

Layer Normalization Các lớp normalization nhằm cải thiện tốc độ huấn luyện của các mô hình (Ba et al., 2016). Mô hình T5 bao gồm hai thành phần Layer Normalization cho mỗi khối, một sau lớp self-attention và một sau mạng feed-forward. Không giống như bài báo gốc cho layer normalization (Ba et al., 2016), mô hình T5 sử dụng một phiên bản đơn giản hóa của layer normalization chỉ chia tỷ lệ lại các kích hoạt. Tỷ lệ phần trăm trọng số có thể học trong layer normalization là khoảng 0.2% tham số.

4 Thí nghiệm

Tập dữ liệu Chúng tôi tuân theo benchmark FEB cho học tập few-shot của NLE (Marasovic et al., 2022) và xem xét bốn tập dữ liệu NLE: e-SNLI cho suy luận ngôn ngữ tự nhiên (Camburu et al., 2018), ECQA cho trả lời câu hỏi đa lựa chọn (Aggarwal et al., 2021), ComVE cho phân loại common sense (Wang et al., 2019), và SBIC cho phân loại tính công kích (Sap et al., 2020).

--- TRANG 4 ---

SPARSE FIT ComVE ECQA SBIC e-SNLI Trung bình
Baseline
(100.00%)Acc. 80.5 ±4.5 57.6 ±2.6 70.1 ±3.4 84.8 ±2.6 73.3 ±3.3
nBERTs 74.2 ±4.2 51.7 ±2.4 67.8 ±3.3 76.9 ±2.5 67.7 ±3.1
Decoder
(54.60%)Acc. 67.3 ±6.0▽ 58.5 ±2.6 66.8 ±3.1▽ 86.6 ±1.7▽ 69.8 ±3.4
nBERTs 61.7 ±5.5▽ 52.3 ±2.4▽ 64.7 ±2.7 78.3 ±1.6▽ 64.3 ±3.0
Encoder
(40.95%)Acc. 72.6 ±6.1▽ 53.2 ±3.6▽ 62.4 ±6.5▽ 79.0 ±3.4▽ 66.8 ±4.9
nBERTs 67.1 ±5.7 47.2 ±3.2▽ 58.7 ±6.5▽ 72.4 ±3.2▽ 61.3 ±4.6
Dense.wo
(27.29%)Acc. 61.3 ±4.4▽ 56.1 ±2.1▽ 62.4 ±2.6▽ 84.0 ±1.9 65.9 ±2.8
nBERTs 56.4 ±4.1▽ 0.0±0.0▽ 59.8 ±2.6▽ 74.7 ±2.6▽ 47.7 ±2.3
Self-attention (KQV)
(20.47%)Acc. 76.2 ±4.4▽ 56.9 ±3.0 69.9 ±3.8 83.3 ±2.4▽ 71.6 ±3.4
nBERTs 70.3 ±4.0▽ 50.2 ±2.7▽ 67.4 ±3.9▽ 76.1 ±2.2▽ 66.0 ±3.2
LM head + Attention.Q
(11.28%)Acc. 74.8 ±5.0▽ 55.4 ±2.7▽ 67.1 ±5.2▽ 82.8 ±3.0▽ 70.0 ±4.0
nBERTs 69.0 ±4.6 43.7 ±4.3▽ 64.5 ±5.5 75.8 ±2.8▽ 63.2 ±4.3
LM head
(4.46%)Acc. 15.6 ±1.3▽ 58.9 ±2.3▽ 0.2±0.2▽ 86.7 ±1.8▽ 40.3 ±1.4
nBERTs 0.0 ±0.0▽ 0.0±0.0▽ 0.2±0.2▽ 0.0±0.0▽ 0.0±0.0
LayerNorm + Attention.Q
(6.84%)Acc. 74.9 ±5.3▽ 55.8 ±3.1▽ 67.0 ±4.4▽ 82.6 ±2.7▽ 70.1 ±3.9
nBERTs 69.0 ±4.8 45.9 ±3.7▽ 64.3 ±4.7 75.6 ±2.5▽ 63.7 ±3.9
Attention.K
(6.82%)Acc. 48.8 ±2.8▽ 56.7 ±2.5▽ 0.2±0.2▽ 19.6 ±11.5▽31.3 ±4.3
nBERTs 19.4 ±10.0▽ 0.0±0.0▽ 0.1±0.2▽ 0.2±0.3▽ 4.9±2.6
Attention.Q
(6.82%)Acc. 74.8 ±5.1▽ 55.5 ±3.2▽ 66.9 ±4.6▽ 82.8 ±2.6▽ 70.0 ±3.8
nBERTs 68.9 ±4.7 43.4 ±4.8▽ 64.2 ±4.8 75.8 ±2.3▽ 63.1 ±4.2
Attention.V
(6.82%)Acc. 55.5 ±3.0▽ 53.1 ±2.8▽30.1 ±10.2▽ 84.2 ±2.0 55.7 ±4.5
nBERTs 51.0 ±2.8▽ 0.0±0.0▽ 30.1 ±10.2▽ 71.7 ±3.4▽ 38.2 ±4.1
LayerNorm
(0.02%)Acc. 34.3 ±2.4▽ 59.0 ±2.4▽ 0.3±0.3▽ 86.6±1.8▽ 45.0 ±1.7
nBERTs 0.0 ±0.0▽ 0.0±0.0▽ 0.2±0.2▽ 0.0±0.0▽ 0.1±0.1

Bảng 1: Tóm tắt các cấu hình SPARSE FIT hoạt động tốt nhất cho T5-large. Chúng tôi báo cáo trung bình và độ lệch chuẩn trên 60 phân chia huấn luyện-xác thực few-shot cho metric độ chính xác và normalized BERTScore (nBERTs). Trong ngoặc là tỷ lệ phần trăm trọng số được fine-tuned cho mỗi cấu hình SPARSE FIT. Chúng tôi thể hiện bằng chữ đậm thiết lập có metric cao nhất cho mỗi tập dữ liệu, bằng màu xanh hiệu suất cao nhất trong SPARSE FIT mà không xem xét số lượng tham số, và bằng màu xanh lá cây thiết lập hoạt động tốt nhất sau khi xem xét tỷ lệ phần trăm tham số được fine-tuned. Sự đánh đổi giữa tham số và hiệu suất được tính bằng (1− %params) × nBERTs). Kiểm tra ý nghĩa được đánh giá thông qua mean t-test so với baseline: ▽ đại diện cho p-value thấp hơn 10^-2.

Phân chia Dữ liệu Học tập Few-shot Chúng tôi cũng tuân theo giao thức đánh giá few-shot được sử dụng bởi Marasovic et al. (2022). Chúng tôi sử dụng 60 phân chia huấn luyện-xác thực của họ để chạy thí nghiệm. Mỗi thí nghiệm được chạy với 48 ví dụ trong tập huấn luyện và 350 ví dụ trong tập xác thực. Lưu ý rằng, tùy thuộc vào tập dữ liệu, số lượng ví dụ huấn luyện cho mỗi nhãn khác nhau: e-SNLI có 16 ví dụ cho mỗi nhãn, ECQA 48, SBIC 24, và ComVE 24, dẫn đến 48 ví dụ huấn luyện cho tất cả các tập dữ liệu.

Quy trình Huấn luyện Theo Marasovic et al. (2022), chúng tôi fine-tune T5 (Raffel et al., 2020) và UNIFIED QA (Khashabi et al., 2020). Tùy thuộc vào thiết lập, các gradient được kích hoạt cho các tham số cụ thể (SPARSE FIT) hoặc toàn bộ mô hình (baseline). Chúng tôi báo cáo kết quả thí nghiệm cho baseline, và quan sát một hành vi nhất quán với cái được báo cáo bởi Marasovic et al. (2022). Thêm vào đó, để so sánh với các baseline PEFT khác, chúng tôi đã điều chỉnh LoRA (Hu et al., 2022), AdaLoRA (Zhang et al., 2023) và (IA)3 (Liu et al., 2022). Chúng tôi sử dụng triển khai PEFT được phát triển bởi Hugging Face (Mangrulkar et al., 2022). Trong triển khai (IA)3 của chúng tôi, chúng tôi lệch hơi so với triển khai gốc vì chúng tôi học các vector tỷ lệ cho tất cả các lớp trong mô hình thay vì học chúng chỉ cho các module attention. Điều này dẫn đến sự gia tăng hiệu suất đáng kể. Đối với các cấu hình SPARSE FIT, chúng tôi fine-tune mỗi thành phần (hoặc cặp) trong 25 epoch với kích thước batch 4 mẫu. Tương tự như Marasovic et al. (2022), chúng tôi sử dụng optimizer AdamW (Loshchilov và Hutter, 2019) với learning rate cố định 0.00003. Tạo văn bản có điều kiện được sử dụng để thực hiện suy luận trên tập xác thực. Huấn luyện và đánh giá được chạy trên NVIDIA P100, và mất trung bình 23.2 phút.

--- TRANG 5 ---

baseline
(100.0 %)decoder
(54.6 %)encoder
(40.95 %)dense.wo
(27.29 %)dense.wi
(27.29 %)
attention.q+attention.k+attention.v(20.47 %)
lm_head+attention.q(11.28 %)lm_head
(4.46 %)
layer_norm+attention.q(6.84 %) attention.k(6.82 %) attention.q(6.82 %) attention.v(6.82 %) layer_norm(0.02 %)
Thành phần Fine-tuned Sparse01020304050607080Normalized BERTscore (%)
Tác vụ Downstream
ComVE ECQA SBIC e-SNLI

Hình 1: Phân phối normalized BERTScore cho các thiết lập SPARSE FIT khác nhau của sparse fine-tuning cho T5-large. Tỷ lệ phần trăm tham số được fine-tuned được thể hiện trong ngoặc.

Đánh giá Tự động Việc đánh giá xem xét độ chính xác tác vụ và chất lượng của NLE được tạo ra. Để đánh giá tự động chất lượng của NLE, chúng tôi tuân theo Marasovic et al. (2022) và sử dụng BERTScore (Zhang et al., 2019), được Kayser et al. (2021) cho thấy tương quan tốt nhất với đánh giá của con người trong NLE. Như trong Marasovic et al. (2022), chúng tôi tính normalized BERTScore gán điểm số không cho NLE trống, hoặc NLE của các mẫu được dự đoán không chính xác (vì người ta sẽ không mong đợi, cũng như không muốn, một NLE có thể tin được nếu dự đoán là sai). Chúng tôi báo cáo trung bình và độ lệch chuẩn của độ chính xác và normalized BERTScore trên 60 phân chia huấn luyện-xác thực cho mỗi cấu hình fine-tuning.

Đánh giá Con người Ngoài normalized BERTScore, chúng tôi thực hiện đánh giá con người quy mô nhỏ hơn để đánh giá chất lượng NLE cho các cấu hình SPARSE FIT hoạt động tốt nhất. Chúng tôi sử dụng NLE liên quan đến 30 mẫu được dự đoán chính xác đầu tiên (cân bằng với số lượng lớp) trong mỗi tập xác thực cho đánh giá con người. Khung đánh giá con người của chúng tôi tuân theo những cái của Kayser et al. (2021); Marasovic et al. (2022). Để đánh giá chất lượng NLE, mỗi người chú thích được yêu cầu trả lời câu hỏi: "Giải thích có biện minh cho câu trả lời không?" và chọn một trong bốn câu trả lời có thể: có, có yếu, không yếu, hoặc không. Hơn nữa, chúng tôi cũng yêu cầu người chú thích xác định các thiếu sót chính, nếu có, của NLE được tạo ra. Các danh mục thiếu sót có thể là (1) vô nghĩa, (2) mâu thuẫn, (3) thiếu giải thích, (4) giải thích không đầy đủ, (5) lặp lại đầu vào, (6) ảo giác, (7) từ thừa ở cuối, (8) đúng nhưng không liên quan, (9) không chính xác, và (10) một từ. Một tác giả và một người chú thích bên thứ ba đã thực hiện chú thích độc lập của toàn bộ tập NLE được chọn để đánh giá (tổng cộng 600 ví dụ). Như trong Kayser et al. (2021), chúng tôi tính giá trị số cho chất lượng NLE bằng cách ánh xạ bốn câu trả lời như sau: có → 1, có yếu → 2/3, không yếu → 1/3, và không → 0; và lấy trung bình trên tất cả chú thích cho mỗi mô hình.

4.1 Kết quả
Để đánh giá SPARSE FIT, chúng tôi tính độ chính xác tác vụ và chất lượng của NLE được tạo ra. Cho rằng có 62 cấu hình có thể (các lớp đơn cộng với các cặp lớp), vì lý do không gian, phần sau cho thấy các cấu hình tốt nhất dựa trên tính chất tổng quát của mô hình. Kết quả cho tất cả cấu hình được thể hiện trong Phụ lục C.

Hiệu suất Tác vụ Chúng tôi trình bày trong Bảng 1 hiệu suất độ chính xác cho các thiết lập SPARSE FIT được chọn cho T5-large. Như có thể quan sát trong Bảng 1, một số cấu hình SPARSE FIT với rất ít tham số được fine-tuned có thể tạo ra kết quả tốt hơn đáng kể so với baseline (tức là, full fine-tuning). Ví dụ, fine-tuning Normalization Layer (LayerNorm) (0.02% tham số của mô hình) đạt được hiệu suất tác vụ tốt hơn baseline cho hai trong số bốn tập dữ liệu (e-SNLI và ECQA). Hơn nữa, chúng tôi nhất quán thấy rằng nếu hai cấu hình SPARSE FIT đạt được kết quả tổng quát tốt, việc kết hợp chúng bằng cách fine-tuning cả hai thành phần cùng nhau tạo ra kết quả tốt hơn đáng kể so với mỗi cấu hình riêng lẻ. Chúng tôi thể hiện trong Hình 10 sự phân tán của hiệu suất tác vụ cho các cấu hình SPARSE FIT. Kết quả cho T5-base và T5-3b được thể hiện trong Phụ lục C.

Chúng tôi thấy rằng độ chính xác tác vụ nhất quán cao hơn cho các LM lớn nhất cho tất cả tập dữ liệu, nhưng khoảng cách giữa T5-large và T5-3b nhỏ (<7%) so với sự gia tăng tham số có thể huấn luyện (~5x).

Chất lượng NLE Nhớ lại rằng LM được fine-tuned để tạo ra có điều kiện một văn bản dưới dạng "[label] because [explanation]". Hình 1 cho thấy normalized BERTScore cho các thiết lập SPARSE FIT được chọn như một proxy để đánh giá NLE được tạo ra sau token giải thích (tức là "because") tốt như thế nào. Đối với tất cả box plot, trục x cho thấy các cấu hình SPARSE FIT, với tỷ lệ phần trăm tham số được fine-tuned trong ngoặc. Nhìn chung, có thể quan sát rằng các thiết lập SPARSE FIT với ít tham số có thể huấn luyện (<10%), như Self-attention Query (Att.Q), LM Head + Attention Query (Att.Q+LMhead), và Layer Norm + Attention Query (Att.Q+LN), cạnh tranh với baseline cho tất cả tập dữ liệu. Hơn nữa, chúng ta có thể thấy rằng chất lượng NLE tốt nhất đạt được cho các kết hợp SPARSE FIT của hai hoặc nhiều loại thành phần (ví dụ, Att.Q). Đáng chú ý, fine-tuning khối decoder (~54% params) đạt được hiệu suất tốt hơn so với toàn bộ fine-tuning cho hai trong số bốn tập dữ liệu (e-SNLI và ECQA). Khoảng cách hiệu suất giữa hầu hết các cấu hình SPARSE FIT và baseline không vượt quá 15% cho tất cả tập dữ liệu, ngay cả đối với các chiến lược fine-tuning rất sparse.

Bất ngờ, nhiều cấu hình SPARSE FIT với độ chính xác tác vụ cao (ví dụ, LayerNorm) có normalized BERTScore gần hoặc bằng không. Điều này xảy ra vì hoặc việc tạo có điều kiện kết thúc câu sau token nhãn được tạo ra hoặc token giải thích (tức là, "because") không được tạo ra thành công. Chúng tôi điều tra thêm về hành vi này trong Phần 4.2. Chúng tôi tóm tắt kết quả về chất lượng NLE cho T5-large trong Bảng 1. Kết quả cho các kích thước mô hình T5 khác (tức là T5-base, T5-large và T5-3b) được thể hiện trong Phụ lục C.1. Chúng tôi thấy rằng normalized BERTScore nhất quán tăng với kích thước của LM. Đáng chú ý, các cấu hình SPARSE FIT tốt nhất cho T5-large cũng đạt được hiệu suất tốt nhất khi fine-tuning T5-base, nhưng chúng hơi khác cho T5-3b.

Các Baseline PEFT Khác Để so sánh SPARSE FIT với các baseline PEFT khác, chúng tôi cũng đã đánh giá LoRA (Hu et al., 2022), AdaLoRA (Zhang et al., 2023) và (IA)3 (Liu et al., 2022) cho NLE. Bảng 2 cho thấy hiệu suất downstream và chất lượng NLE cho các chiến lược PEFT khác nhau trên T5-large. Có thể thấy rằng, trung bình, SPARSE FIT vượt trội hơn các chiến lược PEFT khác. Trong khi các phương pháp PEFT này điều chỉnh ít hơn 20% của 50.45 triệu tham số được cập nhật bởi SPARSE FIT, chất lượng NLE tốt hơn đáng kể cho SPARSE FIT cho hai trong số bốn tập dữ liệu. Lưu ý rằng trong Bảng 2 SPARSE FIT có FLOPS thấp nhất. Chúng tôi giả định rằng điều này xảy ra vì SPARSE FIT không giới thiệu tham số mô hình bổ sung cũng như không tăng độ phức tạp kiến trúc của mô hình. Trong Bảng 8 trong Phụ lục C.3, chúng tôi thể hiện kết quả thêm cho LoRA được huấn luyện trên phạm vi lớn hơn số lượng tham số.

--- TRANG 6 ---

FLOPS ComVE ECQA SBIC e-SNLI Trung bình
SPARSE FIT(Att.Q+LN)
(6.84%)2.37e14 Acc. 74.86 ±5.27 55.81 ±3.12 66.99 ±4.4 82.62 ±2.73 70.07 ±3.88
nBERTs 69.02 ±4.83 45.88 ±3.72 64.29 ±4.7 75.63 ±2.51 63.7 ±3.94
AdaLoRA
(4.48%)2.87e14 Acc. 19.43 ±1.47▽59.40 ±2.28▽ 0.18 ±0.20▽ 86.66 ±1.79▽41.42 ±1.44
nBERTs 16.26 ±1.23▽48.30 ±1.85▽ 0.15 ±0.16▽ 72.19 ±1.49▽34.23 ±1.18
AdaLoRA
(1.15%)1.48e15 Acc. 69.66 ±3.47▽46.60 ±4.02▽61.80 ±2.74▽84.50 ±1.95▽65.64 ±3.05
nBERTs 64.06 ±3.19▽41.22 ±3.65▽58.91 ±2.86▽77.43 ±1.79▽60.41 ±2.87
LoRA (Att.QV , Rank=128)
(4.86%)2.88e14 Acc. 67.77 ±3.73▽43.51 ±3.57▽63.57 ±3.16▽84.26 ±1.92▽ 64.78 ±3.1
nBERTs 61.36 ±3.41▽ 0.33 ±0.41▽ 61.06 ±3.29▽76.49 ±1.75▽49.81 ±2.22
LoRA (Att.KQVO, Rank=4)
(0.32%)2.75e14 Acc. 68.96 ±3.68▽39.04 ±4.06▽62.66 ±3.46▽ 84.05 ±1.81▽ 63.68 ±3.25
nBERTs 63.48 ±3.39▽33.52 ±3.76▽59.80 ±3.66▽77.04 ±1.66▽58.46 ±3.12
(IA)3
(0.07%)2.74e14 Acc. 58.53 ±2.32▽59.14 ±2.36▽48.08 ±0.81▽86.64 ±1.85▽63.10 ±1.84
nBERTs 53.87 ±2.15▽48.08 ±1.92▽48.06 ±0.80▽72.18 ±1.54▽55.55 ±1.60

Bảng 2: So sánh hiệu suất giữa SPARSE FIT và các chiến lược PEFT khác. Chúng tôi báo cáo trung bình và độ lệch chuẩn trên 60 phân chia huấn luyện-xác thực few-shot cho metric độ chính xác và normalized BERTScore (nBERTs). Chúng tôi thể hiện bằng chữ đậm thiết lập có metric cao nhất cho mỗi tập dữ liệu. Kiểm tra ý nghĩa được đánh giá thông qua mean t-test so với SPARSE FIT: ▽ đại diện cho p-value thấp hơn 10^-2.

Các Mô hình Ngôn ngữ Lớn hơn Để đánh giá hiệu suất của SPARSE FIT trong cả các mô hình ngôn ngữ lớn hơn và các kiến trúc khác nhau, chúng tôi thực hiện một tập thí nghiệm áp dụng SPARSE FIT cho Llama 2-7B. Lưu ý rằng cách tiếp cận SPARSE FIT áp dụng cho bất kỳ kiến trúc nào (không chỉ T5 encoder-decoder) vì nó tập trung vào việc xác định lớp tối ưu để fine-tune, bất kể cấu trúc cơ bản của mô hình. Trong vấn đề này, chúng tôi đã tiến hành thí nghiệm trên một mô hình decoder-only lớn hơn (tức là Llama 2-7B). Bảng 3 cho thấy độ chính xác dự đoán trung bình và chất lượng NLE cho chiến lược SPARSE FIT tốt nhất (Att.Q+LN) và baseline PEFT hoạt động tốt nhất (AdaLora) cho Llama 2-7B. Nhìn chung, có thể quan sát rằng SPARSE FIT vượt trội hơn chiến lược PEFT khác cho tất cả tập dữ liệu. Đặc biệt, SPARSE FIT tốt nhất có chất lượng NLE tốt hơn khoảng 5% trung bình so với PEFT khác.

--- TRANG 7 ---

ComVE ECQA SBIC e-SNLI Trung bình
Baseline - Full Fine-tuning
(100% )Acc. 63.71 ±9.1411.14 ±3.1463.86 ±1.8634.91 ±0.4343.41 ±3.64
nBERTs 55.93 ±9.16 9.46 ±2.79 57.42 ±1.3228.62 ±0.8437.86 ±3.53
SPARSE FIT(Att.Q+LN)
(7.97%)Acc. 68.03 ±8.2424.53 ±3.3457.90 ±1.7040.10 ±4.0347.64 ±4.33
nBERTs 58.67 ±7.2020.60 ±2.8350.41 ±2.7234.32 ±3.5041.00 ±4.06
AdaLoRA
(0.30%)Acc. 64.23 ±2.8613.04 ±2.0957.29 ±1.8638.15 ±4.2243.18 ±2.76
nBERTs 56.16 ±2.7711.13 ±1.7950.63 ±0.3333.48 ±3.7137.85 ±2.15

Bảng 3: So sánh hiệu suất giữa SPARSE FIT và các chiến lược PEFT khác cho Llama 2-7B. Chúng tôi báo cáo trung bình và độ lệch chuẩn trên 60 phân chia few-shot cho metric độ chính xác và normalized BERTScore (nBERTs). Chúng tôi thể hiện bằng chữ đậm thiết lập có metric cao nhất cho mỗi tập dữ liệu. Kiểm tra ý nghĩa được đánh giá thông qua mean t-test so với SPARSE FIT: ▽ đại diện cho p-value thấp hơn 10^-2.

Đánh giá Con người
e-SNLI ECQA SBIC ComVE Trung bình
Full
Fine-tuning29.63 (0.43)41.92 (0.23)54.44 ±0.721.67 (0.22)36.91
SPARSE FIT
Att.Q17.28 0.38 35.35 (0.31)61.11 (0.77)28.89 (0.35)35.66
AdaLora
1.15%23.33 (0.34)34.44 (0.26)61.11 (0.69)23.34 (0.25)35.55
SPARSE FIT
Att.Q+LN38.27 (0.34) 31.31 (0.26) 58.89 (0.69)40.00 (0.2542.12

Bảng 4: Điểm số trung bình được đưa ra bởi các người chú thích con người cho SPARSE FIT hoạt động tốt nhất và các baseline PEFT khác của T5-large. Kết quả tốt nhất được in đậm. Trong ngoặc, chúng tôi thể hiện điểm số thỏa thuận giữa các người chú thích.

Đánh giá Con người Chúng tôi thể hiện trong Bảng 4 phân phối điểm số được đưa ra bởi các người chú thích con người cho chất lượng NLE được tạo ra cho các chiến lược SPARSE FIT tốt nhất và baseline PEFT hoạt động tốt nhất (AdaLora). Chúng tôi tính điểm số thỏa thuận giữa các người chú thích sử dụng metric Cohen' κ (Cohen, 1960). Nhìn chung, chúng tôi thấy rằng chất lượng NLE được tạo ra sau khi áp dụng SPARSE FIT cao hơn nhiều so với baseline và AdaLoRA cho 2 trong số 4 tác vụ. Đối với hai tác vụ khác, AdaLoRA tạo ra NLE tốt hơn với biên độ rất nhỏ. Trung bình, NLE của SPARSE FIT tốt hơn khoảng 8% so với NLE của AdaLoRA và tốt hơn 6% so với NLE full fine-tuning. Tuy nhiên, đánh giá con người cho thấy rằng NLE được tạo ra thường không đủ để giải thích dự đoán một cách chính xác. Chúng tôi thể hiện trong Hình 2 phân phối điểm số khả năng tin cậy được đưa ra bởi các người chú thích con người cho chất lượng NLE được tạo ra cho các chiến lược SPARSE FIT tốt nhất. Có thể quan sát rằng khoảng một nửa NLE không biện minh cho câu trả lời, bất kể chiến lược fine-tuning nào được sử dụng. Tương tự, tỷ lệ NLE hoàn toàn biện minh cho dự đoán gần 25% bất kể thiết lập SPARSE FIT. Chúng tôi chi tiết các thiếu sót và hạn chế của NLE được tạo ra trong Phần 4.2. Cuối cùng, chúng tôi thể hiện trong Bảng 5 kết quả đánh giá con người cho cấu hình SPARSE FIT tốt nhất và các baseline PEFT khác khi áp dụng cho Llama2-7B. Có thể thấy rằng trung bình NLE của SPARSE FIT tốt hơn khoảng 6% so với NLE của AdaLoRA và tốt hơn 21% so với NLE full fine-tuning của Llama2-7B.

100 90 80 70 60 50 40 30 20 10 0
Tỷ lệ phần trămbaseline
decoder
encoder
attentionQ
layerNorm+
attentionQThiết lập Sparse Fine-Tuning 
 
Đánh giá Con người
không không yếu có yếu có

Hình 2: Minh họa điểm số khả năng tin cậy được đưa ra bởi các người chú thích con người cho chất lượng NLE được tạo ra bởi các cấu hình SPARSE FIT khác nhau. Các người chú thích được yêu cầu trả lời câu hỏi: "Giải thích có biện minh cho câu trả lời không?"

4.2 Thảo luận
Phân tích NLE được tạo ra Trong Hình 4, chúng tôi thể hiện một bộ sưu tập ví dụ về NLE được tạo ra bởi baseline và các cấu hình SPARSE FIT hoạt động tốt nhất. Như trong các công trình trước (Camburu et al., 2018; Kayser et al., 2021; Marasovic et al., 2022), chúng tôi chỉ thể hiện các ví dụ mà nhãn được dự đoán chính xác bởi mô hình vì chúng tôi không mong đợi một mô hình dự đoán nhãn sai tạo ra giải thích đúng. Chúng tôi trình bày trong Phụ lục B một danh sách mở rộng hơn về NLE được tạo ra với SPARSE FIT.

Thiếu sót NLE Hình 3 mô tả biểu đồ tần số của các thiếu sót cho baseline và các chiến lược SPARSE FIT hoạt động tốt nhất. Có thể quan sát rằng các thiếu sót phổ biến nhất là Thiếu giải thích, Vô nghĩa, và Giải thích không đầy đủ. Đối với cấu hình SPARSE FIT tốt nhất (tức là Att.Q+LN), Giải thích không đầy đủ là lý do có nhiều lần xuất hiện nhất. Chúng tôi thể hiện sự phân tích các thiếu sót cho mỗi tập dữ liệu trong Phụ lục C.4.

Thỏa thuận Giữa các Người chú thích Như thể hiện trong Bảng 4, sự thỏa thuận giữa các người chú thích khá thấp cho tập NLE được đánh giá. Cụ thể hơn, các người chú thích đã đưa ra điểm số khác nhau cho 181 trong số 600 NLE. Tập dữ liệu có sự khác biệt đáng kể nhất là ECQA, với 63 sự khác biệt, trong khi tập dữ liệu SBIC đồng nhất nhất, với 17 sự khác biệt. Sự biến thiên giữa các người chú thích có thể xuất phát từ ba lý do nhận thức tiềm ẩn (Bourke, 2014; Niño, 2009). Lý do đầu tiên là sự bất đồng nhận thức, nói rằng các người chú thích không thể xác định một cách khách quan sự khác biệt giữa hai câu trả lời liền kề (tức là Có Yếu vs Không Yếu, hoặc Có vs. Có Yếu). Lý do thứ hai là sự bất đồng vị trí (Bourke, 2014), có thể thay đổi cách các người chú thích nhận thức kết quả của thuật toán do chủng tộc, giới tính và các yếu tố nhận dạng kinh tế xã hội khác của họ. Điều này đặc biệt quan trọng đối với tập dữ liệu SBIC, vì nó chứa nội dung công kích. Lý do thứ ba là sự bất đồng kỳ vọng, có thể khiến một người chú thích nghiêm khắc hơn về các đặc điểm làm cho một giải thích hoàn chỉnh và chính xác. Một bộ sưu tập mở rộng các ví dụ về sự bất đồng nhận thức, sự bất đồng vị trí, và sự bất đồng kỳ vọng trong Phụ lục C.5.

Tạo ra NLE Trống Như đã đề cập trước đó, một số cấu hình SPARSE FIT (ví dụ LayerNorm) có hiệu suất tác vụ cao nhưng tạo ra NLE trống hầu hết thời gian, đặc biệt là cho các tập dữ liệu e-SNLI và ECQA. Một giải thích có thể cho sự bất đồng giữa độ chính xác tác vụ cao và chất lượng NLE thấp là việc tạo ra NLE về bản chất là một vấn đề phức tạp hơn so với giải quyết các tác vụ downstream, trong đó cái trước có thể đòi hỏi fine-tuning các phần đáng kể hơn của tham số mô hình. Một giải thích khác có thể được tìm thấy bằng cách phân tích các tác vụ huấn luyện trước của PLM và quan sát rằng, trong giai đoạn huấn luyện trước, T5 được huấn luyện trên tập dữ liệu MNLI (Williams et al., 2018), bao gồm các thể hiện NLI mà không có NLE. Các trọng số T5 sau đó được huấn luyện trước trên MNLI bằng cách đưa tác vụ NLI như một vấn đề chuyển đổi chuỗi, trong đó đầu vào là một cặp giả thuyết-tiền đề, và đầu ra là nhãn. Khi chỉ một tập con nhỏ tham số được cập nhật (ví dụ, LayerNorm (0.02%)), mô hình gợi lên hành vi ban đầu của nó và dự đoán nhãn mà không tạo ra NLE. Lý luận tương tự có thể được kết luận cho ECQA vì UNIFIED QA được huấn luyện trước trên CommonsenseQA (Talmor et al., 2019), bao gồm các mẫu chỉ có câu trả lời cho câu hỏi đa lựa chọn.

--- TRANG 8 ---

Đánh giá Con người
e-SNLI ECQA SBIC ComVE Trung bình
Full
Fine-tuning17.78 38.89 47.78 40.00 36.11
SPARSE FIT
Att.Q+LN41.11 73.33 68.89 70.00 63.33
AdaLora 50.00 52.22 71.11 55.56 57.22

Bảng 5: Điểm số trung bình được đưa ra bởi các người chú thích con người cho SPARSE FIT hoạt động tốt nhất và các baseline PEFT khác của Llama-2-7B. Kết quả tốt nhất được in đậm.

baseline decoder encoder attentionQ layerNorm+
attentionQ
Thiết lập Sparse Fine-Tuning0510152025Số lượng
Đánh giá Con người
thiếu giải thích
giải thích không đầy đủ
vô nghĩa
lặp lại đầu vàotừ thừa ở cuối
ảo giác
một từ
đúng nhưng không liên quanmâu thuẫn
không chính xác
không liên quan
trống

Hình 3: Biểu đồ về các thiếu sót của NLE được tạo ra cho baseline và các cấu hình SPARSE FIT hoạt động tốt nhất tổng hợp cho tất cả tập dữ liệu.

5 Tóm tắt
Chúng tôi đã giới thiệu SPARSE FIT, một chiến lược kết hợp sparse fine-tuning với học tập dựa trên prompt để huấn luyện các mô hình NLE trong thiết lập few-shot. SPARSE FIT cho thấy hiệu suất cạnh tranh nhất quán trong khi chỉ cập nhật một tập con tối thiểu tham số (tức là Self-attention Query + Layer Normalization, có ~6.8% tham số mô hình). Chúng tôi thấy rằng sparse fine-tuning của T5-large nhất quán đạt được hiệu suất tốt hơn so với fine-tuning T5-base và hơi tệ hơn (<5%) so với T5-3b, bất kể chiến lược SPARSE FIT. Hơn nữa, ba người hoạt động tốt nhất cho T5-base đạt được bởi cùng một tập cấu hình SPARSE FIT được tìm thấy cho T5-large. So với các kỹ thuật PEFT khác, SPARSE FIT tạo ra độ chính xác dự đoán trung bình tốt hơn và chất lượng NLE. Chúng tôi hy vọng SPARSE FIT sẽ truyền cảm hứng cho cộng đồng điều tra sparse fine-tuning tại các thành phần mô hình khác nhau.

Hạn chế
Mặc dù việc tạo ra giải thích ngôn ngữ tự nhiên là một lĩnh vực nghiên cứu sôi động, vẫn không có bảo đảm rằng những giải thích như vậy phản ánh chính xác cách mô hình hoạt động bên trong (Wiegreffe et al., 2021; Camburu et al., 2020). Ví dụ, việc giải thích được tạo ra có vẻ hợp lý không có nghĩa là mô hình không dựa vào các thuộc tính được bảo vệ và các tương quan giả mạo trong dữ liệu huấn luyện để tạo ra dự đoán của nó. Do đó, chúng tôi vẫn khuyến nghị cẩn thận khi sử dụng các mô hình tự giải thích trong sản xuất, vì chúng có thể nắm bắt các thiên lệch có thể có hại từ dữ liệu huấn luyện, ngay cả khi những điều này không được đề cập trong các giải thích.

Lời cảm ơn
Chúng tôi cảm ơn Andrea Sissa đã giúp đỡ với đánh giá con người và những ý tưởng sâu sắc của cô ấy về các biến thể thỏa thuận giữa các người chú thích. Oana-Maria Camburu được hỗ trợ bởi Leverhulme Early Career Fellowship. Pasquale Minervini được tài trợ một phần bởi chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu theo thỏa thuận tài trợ số 875160, ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence) EPSRC (tài trợ số EP/W002876/1), một khoản tài trợ ngành từ Cisco, và một khoản đóng góp từ Accenture LLP.

--- TRANG 9 ---

[Phần này tiếp tục với các ví dụ cụ thể về NLE được tạo ra cho các tập dữ liệu khác nhau, bao gồm e-SNLI, ECQA, SBIC, và ComVE. Mỗi ví dụ có tiền đề/câu hỏi, nhãn/câu trả lời, giải thích gốc, và các giải thích được tạo ra bởi các phương pháp khác nhau như Baseline, Decoder, Att.Q, Att.Q + Head, và Att.Q + LN.]

--- TRANG 10 ---

[Phần này chứa danh sách tài liệu tham khảo với các tác giả, tiêu đề, và thông tin xuất bản của các nghiên cứu được trích dẫn trong bài báo.]

--- TRANG 11 ---

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---

[Phụ lục A với biểu diễn đồ họa của SPARSE FIT và các ví dụ về NLE được tạo ra]

Phụ lục A Biểu diễn Đồ họa SPARSE FIT
Trong bài báo này, chúng tôi đề xuất một chế độ huấn luyện few-shot dựa trên prompt hiệu quả cho các mô hình tạo ra cả dự đoán và NLE trên mô hình ngôn ngữ T5. Để hiểu rõ hơn về các tham số có thể huấn luyện hoạt động trong mỗi cấu hình SPARSE FIT, chúng tôi minh họa trong Hình 5 một biểu diễn đồ họa của kiến trúc T5 với các tham số hoạt động được tô màu cho Layer Normalization sparse fine-tuning. Sau khi đóng băng phần còn lại của mô hình (các lớp màu xám), tỷ lệ phần trăm tham số có thể được cập nhật trong Layer Normalization là 0.02% của toàn bộ mô hình. Xem xét rằng kiến trúc của mô hình UNIFIED QA giống với T5, việc diễn giải các tham số hoạt động cũng áp dụng cho UNIFIED QA.

[Hình 5 cho thấy sơ đồ kiến trúc T5 với các thành phần khác nhau được đánh dấu]

Phụ lục B Ví dụ về NLE được tạo ra
[Phần này chứa nhiều ví dụ cụ thể về NLE được tạo ra cho các tập dữ liệu khác nhau]

--- TRANG 14 đến TRANG 25 ---

[Các phụ lục tiếp theo chứa kết quả chi tiết cho tất cả cấu hình SPARSE FIT, so sánh hiệu suất, phân tích lỗi, và các thí nghiệm bổ sung với các kích thước mô hình khác nhau]
