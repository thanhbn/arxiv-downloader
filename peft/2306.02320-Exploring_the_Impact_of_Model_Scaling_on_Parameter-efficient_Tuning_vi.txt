# 2306.02320.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2306.02320.pdf
# Kích thước tệp: 1684671 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khám phá Tác động của Việc Mở rộng Quy mô Mô hình lên Điều chỉnh Hiệu quả Tham số
Yusheng Su1∗, Chi-Min Chan1∗, Jiali Cheng2, Yujia Qin1, Yankai Lin3, Shengding Hu1,
Zonghan Yang1,Ning Ding1,Xingzhi Sun4,Guotong Xie4,Zhiyuan Liu1†,Maosong Sun1+
1Khoa Khoa học Máy tính và Công nghệ, Đại học Thanh Hoa
2Đại học Massachusetts Lowell
3Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân
4Ping An Technology
yushengsu.thu@gmail.com
Tóm tắt
Các phương pháp điều chỉnh hiệu quả tham số (PET) có thể điều khiển hiệu quả các mô hình ngôn ngữ được huấn luyện trước cực lớn (PLM) bằng cách chỉ huấn luyện số lượng tham số tối thiểu. Các phương pháp PET khác nhau sử dụng các mô-đun có thể điều chỉnh được thiết kế thủ công khác nhau. Trong các PLM nhỏ, thường có sự khác biệt hiệu suất đáng chú ý giữa các phương pháp PET. Tuy nhiên, khi quy mô mô hình tăng lên, sự khác biệt hiệu suất trở nên không đáng kể. Do đó, chúng tôi đưa ra giả thuyết rằng việc mở rộng quy mô mô hình làm giảm tác động của sự khác biệt thiết kế đối với các phương pháp PET. Để điều tra giả thuyết này, chúng tôi giới thiệu một phương pháp PET linh hoạt hơn gọi là phương pháp Arbitrary PET (APET). Phương pháp APET tương thích với một mô-đun có thể điều chỉnh, bao gồm bất kỳ số lượng tham số nào được phân bố ở các vị trí tùy ý. Sau đó, chúng tôi sử dụng nó và tiến hành thí nghiệm trên 11 nhiệm vụ NLP trên 3 PLM đại diện. Các điều tra của chúng tôi tiết lộ rằng việc mở rộng quy mô mô hình (1) làm giảm tác động của vị trí các tham số có thể điều chỉnh đối với hiệu suất, và (2) cho phép các phương pháp điều chỉnh đạt được hiệu suất tương đương với việc tinh chỉnh toàn bộ tham số bằng cách tối ưu hóa ít tham số có thể điều chỉnh hơn. Thú vị thay, chúng tôi cũng quan sát thấy rằng các phương pháp điều chỉnh tối ưu hóa số lượng tham số có thể điều chỉnh tương tự để vượt qua hiệu suất đoán ngẫu nhiên trên các nhiệm vụ khác nhau. Chúng tôi thảo luận chung về hiện tượng này và hai phát hiện nói trên từ góc độ tối ưu hóa để hiểu các cơ chế cơ bản. Những kết luận này nâng cao hiểu biết của chúng ta về tác động của việc mở rộng quy mô mô hình lên PET và hỗ trợ trong việc thiết kế các phương pháp PET hiệu quả và hiệu suất hơn cho các PLM có quy mô khác nhau. Mã nguồn có thể được lấy từ kho GitHub này: https://github.com/yushengsu-thu/PET_Scaling .

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước (PLM), chẳng hạn như GPT (Radford et al., 2018), BERT (Devlin et al., 2019),
∗Hai tác giả đầu đóng góp như nhau.
†Tác giả liên hệ: Z.Liu và M.Sun.

--- TRANG 2 ---
và T5 (Raffel et al., 2020), đã đạt được thành công lớn trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau. Mặc dù hiệu quả, việc tinh chỉnh (FT) các PLM quy mô lớn này với toàn bộ tham số gây ra cả chi phí tính toán và lưu trữ không thể chi trả được. Để giải quyết vấn đề này, các nhà nghiên cứu đã đề xuất một loạt các phương pháp điều chỉnh hiệu quả tham số (PET) (Houlsby et al., 2019a; Li and Liang, 2021; Mahabadi et al., 2021a; Lester et al., 2021; Mahabadi et al., 2021b; Hu et al., 2022a; Ben Zaken et al., 2022; He et al., 2022b) chỉ cập nhật một mô-đun có thể điều chỉnh được chỉ định bao gồm tham số tối thiểu trong khi đóng băng các tham số còn lại trong PLM trong quá trình thích ứng mô hình.

Mặc dù các phương pháp PET đại diện hiện có này có thể giảm chi phí tính toán và lưu trữ, thường có sự khác biệt hiệu suất đáng chú ý giữa các phương pháp PET đại diện này trong các nhiệm vụ hạ nguồn. Thú vị thay, khi quy mô của PLM tăng lên, sự khác biệt hiệu suất giữa các phương pháp PET trở nên hẹp hơn, như được minh họa trong Hình 1. Những phát hiện này thú vị và đáng khám phá vì các phương pháp PET đại diện hiện có được thiết kế với các triết lý khác biệt, ví dụ, các mô-đun có thể điều chỉnh được cấu thành từ số lượng tham số có thể điều chỉnh khác nhau được phân bố ở các vị trí tùy ý. Do đó, chúng tôi đưa ra giả thuyết rằng việc mở rộng quy mô mô hình làm giảm tác động của các sự khác biệt thiết kế nói trên giữa các phương pháp PET đối với hiệu suất. Để xác thực giả thuyết này, chúng tôi tiến hành thêm hai dòng phân tích ablation:

(A1) Liệu quy mô mô hình có làm giảm sự khác biệt hiệu suất do vị trí của các tham số có thể điều chỉnh.

(A2) Liệu quy mô mô hình có làm giảm sự khác biệt hiệu suất do số lượng tham số có thể điều chỉnh.

Tuy nhiên, chỉ điều tra bốn phương pháp PET đại diện (xem Hình 1) có thể không đủ để bao quát một phạm vi đầy đủ các vị trí tham số cho các phân tích ablation (A1). Ngoài ra, các mô-đun có thể điều chỉnh của bốn phương pháp PET này bị ràng buộc để được cấu thành từ các tensor hoặc ma trận cấp độ lớp, khiến việc kiểm soát chính xác số lượng tham số có thể điều chỉnh ở mức độ tinh (tham số) trong các phân tích ablation (A2) trở nên khó khăn. Để tạo thuận lợi cho các phân tích ablation, chúng tôi phát triển một phương pháp Arbitrary Parameter-Efficient Tuning (APET) linh hoạt hơn (§ 5.1), có thể tương thích với bất kỳ số lượng tham số có thể điều chỉnh nào được phân bố ở các vị trí tùy ý.

Trong phân tích (A1), chúng tôi so sánh hiệu suất của các phương pháp APET với số lượng tham số có thể điều chỉnh bằng nhau được phân bố ở các vị trí khác nhau. Dựa trên kết quả thí nghiệm, chúng tôi quan sát thấy sự khác biệt nhỏ hơn trong hiệu suất của các phương pháp APET này trên các mô hình lớn hơn. Phát hiện này cho thấy rằng việc mở rộng quy mô mô hình làm giảm tác động do vị trí của các tham số có thể điều chỉnh đối với hiệu suất.

Trong phân tích (A2), chúng tôi so sánh hiệu suất của cùng các phương pháp APET với số lượng tham số có thể điều chỉnh khác nhau. Dựa trên kết quả thí nghiệm, chúng tôi quan sát thấy rằng việc mở rộng quy mô mô hình không làm giảm tác động do số lượng tham số có thể điều chỉnh đối với hiệu suất. Hơn nữa, chúng tôi đã quan sát hai hiện tượng thú vị khi số lượng tham số có thể điều chỉnh đạt đến hai ngưỡng: ngưỡng cao và ngưỡng thấp.

Khi số lượng tham số có thể điều chỉnh bằng ngưỡng cao, các phương pháp APET có thể đạt được hiệu suất tinh chỉnh toàn bộ tham số của mô hình backbone tương ứng, và ngưỡng cao có xu hướng thấp hơn trên các mô hình lớn hơn. Cụ thể, các phương pháp PET có thể tối ưu hóa ít tham số có thể điều chỉnh hơn để đạt được hiệu suất tinh chỉnh toàn bộ tham số trên các mô hình lớn hơn. Mặt khác, khi số lượng tham số có thể điều chỉnh vượt qua ngưỡng tham số thấp, tất cả các phương pháp APET đều vượt trội hơn hiệu suất đoán ngẫu nhiên. Chúng tôi thấy rằng các ngưỡng thấp gần như giống hệt nhau trên cùng các mô hình, ngay cả đối với các nhiệm vụ khác nhau. Điều này cho thấy rằng trên các nhiệm vụ khác nhau, các phương pháp PET có thể tối ưu hóa số lượng tham số có thể điều chỉnh tương tự trên cùng PLM để vượt qua hiệu suất đoán ngẫu nhiên.

Tóm lại, chúng tôi giới thiệu các phương pháp PET linh hoạt hơn - phương pháp APET - để tiến hành các phân tích ablation mở rộng và tiết lộ tác động của việc mở rộng quy mô mô hình đối với thiết kế PET, ví dụ, (1) vị trí của các tham số có thể điều chỉnh (§ 5.2) và (2) số lượng tham số có thể điều chỉnh (§ 5.3). (3) Hơn nữa, chúng tôi thảo luận về các phát hiện của các phân tích ablation từ góc độ tối ưu hóa (§ 6). Chúng tôi hy vọng những kết luận này không chỉ khuyến khích nhiều nhà nghiên cứu hơn khám phá tác động của việc mở rộng quy mô mô hình đối với các phương pháp điều chỉnh từ góc độ lý thuyết, mà còn cung cấp hướng dẫn để thiết kế các phương pháp điều chỉnh cho các mô hình có quy mô khác nhau.

2 Công trình Liên quan
Các Phương pháp Điều chỉnh Hiệu quả Tham số (PET)
Với các PLM lớn hơn liên tục được phát triển, việc tinh chỉnh tất cả các tham số và lưu trữ các trọng số thích ứng trở nên ngày càng cồng kềnh. Để giải quyết vấn đề này, các nhà nghiên cứu đề xuất các phương pháp PET giữ hầu hết các tham số của PLM bị đóng băng và chỉ tối ưu hóa một mô-đun có thể điều chỉnh bao gồm một vài tham số trong quá trình thích ứng hạ nguồn. Trong những năm gần đây, nhiều thiết kế khác nhau của các phương pháp PET đã xuất hiện. Ví dụ, một số phương pháp PET chèn các mô-đun có thể điều chỉnh bên ngoài sau các lớp feed-forward và attention trong PLM (Houlsby et al., 2019a; Pfeiffer et al., 2021; Mahabadi et al., 2021c); những phương pháp khác đặt trước các mô-đun có thể điều chỉnh vào các lớp attention (Li and Liang, 2021; Hu et al., 2022a) hoặc lớp embedding (Lester et al., 2021). Một dòng phương pháp PET khác chọn các tham số hiện có trong PLM (Ben Zaken et al., 2022; Guo et al., 2021) làm mô-đun có thể điều chỉnh để tối ưu hóa. Để nâng cao hơn nữa hiệu suất của các phương pháp PET, một số công trình đề xuất các chiến lược lựa chọn tự động (Hu et al., 2022c; Chen et al., 2023; Lawton et al., 2023; Zhou et al., 2023) cho các tham số có thể điều chỉnh.

--- TRANG 3 ---
Mặc dù các phương pháp PET này có các mô-đun có thể điều chỉnh khác biệt, chúng có thể được thống nhất thành một dạng tương tự. He et al. (2022a) hình thức hóa các phương pháp PET như một khung thống nhất để nghiên cứu các kết nối giữa các phương pháp PET. Yi et al. (2022) cũng tiến hành nghiên cứu tương tự và tiến một bước chỉ ra rằng việc tối ưu hóa của các phương pháp PET khác nhau có thể được thống nhất trong một không gian con tương tự. Trong bài báo này, chúng tôi tận dụng những góc nhìn thống nhất này để giải thích tác động của việc mở rộng quy mô mô hình lên PET trong cuộc thảo luận cuối cùng (§ 6).

Sức mạnh của Việc Mở rộng Quy mô Mô hình Với việc mở rộng quy mô kích thước mô hình, các PLM xuất hiện nhiều khả năng, bao gồm khả năng lý luận (Wei et al., 2022b,a), và có thể đạt được kết quả tiên tiến trong các nhiệm vụ hiểu và tạo sinh khác nhau (Du et al., 2022; Chowdhery et al., 2022).

Từ góc độ thích ứng, một số nhà nghiên cứu thấy rằng việc thực hiện một số phương pháp PET (Lester et al., 2021; Ding et al., 2023; Su et al., 2022) trên các mô hình quy mô lớn gần như có thể đạt được hiệu suất tinh chỉnh toàn bộ tham số. Trong bài báo này, chúng tôi tiến một bước thấy rằng khi quy mô mô hình tăng lên, sự khác biệt hiệu suất giữa các phương pháp PET khác biệt trở nên nhỏ hơn (§ 4). Do đó, chúng tôi nghiên cứu tác động của việc mở rộng quy mô mô hình đối với các phương pháp PET (§ 5) để hiểu sâu hiện tượng này và giải thích nó từ góc độ tối ưu hóa (§ 6).

3 Kiến thức Cơ bản
Trong phần này, chúng tôi trước tiên giới thiệu khung Transformer (§ 3.1) và PET đại diện nhất (§ 3.2).

3.1 Khung Transformer
Mô hình Transformer (Vaswani et al., 2017) là kiến trúc chính cho hầu hết các PLM mạnh mẽ. Mô hình được xếp chồng của L khối, mỗi khối bao gồm một chuỗi các lớp, bao gồm self-attention và mạng feed-forward. Trong quá trình truyền tiến qua mỗi khối, trạng thái ẩn đầu vào được áp dụng với chuỗi các lớp. Để đơn giản, chúng tôi hình thức hóa phép biến đổi của mỗi lớp như

hout=f(hin). (1)

Dưới lớp như toán tử f, trạng thái ẩn đầu vào hin∈Rs×din được biến đổi thành trạng thái ẩn đầu ra hout∈Rs×dout, trong đó s là độ dài đầu vào và din,dout là các chiều.

3.2 Điều chỉnh Hiệu quả Tham số (PET)
Các phương pháp PET khác nhau được trang bị các mô-đun θ đa dạng như được hiển thị trong Hình 1. Các mô-đun này được cấu thành từ các tham số có thể điều chỉnh W sửa đổi các lớp gốc và các phép biến đổi tương ứng trong PLM. Để so sánh, chúng tôi theo quan điểm thống nhất (He et al., 2022a; Hu et al., 2022c) để tái cấu trúc các phép biến đổi của tất cả các phương pháp PET như các sửa đổi ∆h của các trạng thái ẩn cụ thể trong các lớp PLM tương ứng như sau:

hout=f(hin) + ∆h. (2)

Trong quá trình huấn luyện, cho một nhiệm vụ hạ nguồn D={X, Y}, chúng tôi chỉ tối ưu hóa tất cả các tham số có thể điều chỉnh của mô-đun θ cho mỗi phương pháp PET để tạo ra các đầu ra mong muốn Y của một nhiệm vụ hạ nguồn trong khi đóng băng phần còn lại của các tham số Φ trong PLM M, như được hiển thị trong Hình 1. Chính thức, mục tiêu huấn luyện là tối thiểu hóa L như sau:

minθL(M(Φ,θ)(X), Y). (3)

4 Thí nghiệm Chính
Để khám phá tác động của việc mở rộng quy mô mô hình đối với các phương pháp PET này, chúng tôi trước tiên giới thiệu các nhiệm vụ được điều tra, PLM và cài đặt của các phương pháp PET đại diện hiện có trong các thí nghiệm (§ 4.1), và sau đó báo cáo các kết quả thí nghiệm chính (§ 4.2).

--- TRANG 4 ---
4.1 Cài đặt Thí nghiệm
Các Nhiệm vụ NLP Được Điều tra Chúng tôi điều tra 11 nhiệm vụ, có thể được chia thành 5 danh mục: (1) Phân tích Cảm xúc (SA), bao gồm SST-2 (Socher et al., 2013), IMDB (Maas et al., 2011), và Rotten Tomatoes (Pang and Lee, 2005); (2) Suy luận Ngôn ngữ Tự nhiên (NLI), bao gồm MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), và RTE (Bos and Markert, 2005); (3) Nhận dạng Diễn đạt (PI), bao gồm MRPC (Dolan and Brockett, 2005) và QQP (Sharma et al., 2019); (4) Hỏi đáp (QA), bao gồm NQ-Open (Lee et al., 2019); (5) Tóm tắt (SUM), bao gồm SAMSum (Gliwa et al., 2019) và Multi-News (Fabbri et al., 2019). Chi tiết thêm trong phụ lục A.

Các PLM Được Điều tra Chúng tôi sẽ thí nghiệm trên ba loạt backbone PLM: BERT (Devlin et al., 2019), BLOOM (Scao et al., 2023), và T5 (Raffel et al., 2020) đại diện cho mô hình dựa trên encoder, mô hình dựa trên decoder, và mô hình dựa trên sequence-to-sequence, tương ứng. Vì BERT có giới hạn đầu ra độ dài cố định, chúng tôi chỉ điều tra các danh mục nhiệm vụ SA, PI, và NLI trên nó. Khác biệt, các mô hình BLOOM và T5 không có giới hạn đầu ra độ dài cố định; do đó, chúng tôi điều tra tất cả các nhiệm vụ trên chúng.

Chi tiết Huấn luyện của Các Phương pháp PET Chúng tôi chọn bốn phương pháp PET đại diện: Prompt (Lester et al., 2021), BitFit (Ben Zaken et al., 2022), Adapter (Houlsby et al., 2019a), và LoRA (Hu et al., 2022a), để tiến hành các thí nghiệm phân tích. Để đảm bảo tính nhất quán của hiệu suất các phương pháp PET, chúng tôi duy trì thiết kế gốc của mỗi phương pháp, bao gồm vị trí của các tham số có thể điều chỉnh và số lượng tham số có thể huấn luyện, như được báo cáo trong các bài báo gốc tương ứng. Ngoài ra, chúng tôi huấn luyện mỗi phương pháp PET trên 11 nhiệm vụ sử dụng 3 seed ngẫu nhiên khác nhau và báo cáo hiệu suất trung bình của chúng. Chi tiết thêm về các cấu hình huấn luyện có thể được tìm thấy trong phụ lục B.

4.2 Tác động Mở rộng Quy mô Mô hình lên Các Phương pháp PET
Để điều tra tác động của việc mở rộng quy mô mô hình đối với các phương pháp PET, chúng tôi sắp xếp các Mô hình Ngôn ngữ Được huấn luyện trước (PLM) theo thứ tự tăng dần dựa trên quy mô mô hình của chúng, và chúng tôi báo cáo hiệu suất của các phương pháp PET trên mỗi loại PLM.

Kết quả được báo cáo trong Hình 2. Đầu tiên, chúng tôi có thể quan sát thấy rằng các phương pháp PET thể hiện sự khác biệt hiệu suất đáng chú ý (độ lệch chuẩn BERTSMALL; S.D. = 5.08, BERTBASE; S.D. = 3.63, BERTLARGE; S.D. = 2.65) khác nhau trên các mô hình quy mô chung (BERT SMALL và BERT BASE trong hình phụ [a]; BLOOM 560M và BLOOM 1.1B trong hình phụ [b]; T5 SMALL và T5 BASE trong hình phụ [c]). Hiện tượng này là trực quan và thể hiện tác động quan trọng của sự khác biệt thiết kế (vị trí và số lượng tham số trong mô-đun có thể điều chỉnh) đối với hiệu suất của các phương pháp PET. Phát hiện này đã được tìm thấy một cách nhất quán trong nhiều công trình trước đây (Ding et al., 2023; Hu et al., 2022c).

Tuy nhiên, chúng tôi thấy rằng khi việc mở rộng quy mô mô hình tăng lên (từ BERT SMALL đến BERT LARGE trong hình phụ [a]; từ BLOOM 560M đến BLOOM 7.1B trong hình phụ [b]; từ T5 SMALL đến T5 XXL trong hình phụ [c]), sự khác biệt hiệu suất giữa các phương pháp PET giảm đi trên tất cả các loại mô hình, như được chứng minh bởi độ lệch chuẩn (S.D.) giảm dần (từ 5.08 xuống 2.65 trên [a] BERT; từ 3.46 xuống 2.50 trên [b] BLOOM; từ 2.75 xuống 1.72 trên [c] T5). Phát hiện này ngụ ý rằng việc mở rộng quy mô mô hình lớn hơn có thể làm giảm tác động của sự khác biệt thiết kế giữa các phương pháp PET đối với hiệu suất.

--- TRANG 5 ---
5 Các Phân tích Ablation
Sự khác biệt thiết kế giữa các phương pháp PET chủ yếu nằm ở vị trí tham số và số lượng tham số của mô-đun có thể điều chỉnh. Để xác minh thêm liệu việc mở rộng quy mô mô hình có loại bỏ tương ứng tác động của các sự khác biệt nói trên đối với các phương pháp PET, chúng tôi đã tiến hành hai ablation để điều tra liệu việc mở rộng quy mô mô hình có thể làm giảm (1) tác động của vị trí tham số có thể điều chỉnh và (2) tác động của số lượng tham số có thể điều chỉnh.

Tuy nhiên, chỉ điều tra bốn phương pháp PET tương ứng nói trên là không đủ để bao phủ đủ các biến thể của vị trí tham số cho nghiên cứu ablation (1). Hạn chế này khiến chúng tôi khó kiểm soát chính xác số lượng tham số có thể điều chỉnh ở mức độ tinh (cấp độ tham số) trong nghiên cứu ablation (2). Do đó, chúng tôi phát triển một phương pháp PET linh hoạt hơn, phương pháp Arbitrary Parameter-Efficient Tuning (APET). Mô-đun có thể điều chỉnh của nó có thể là cấu trúc tùy ý (§ 5.1) tạo thuận lợi cho chúng tôi khám phá các vị trí tham số khác nhau trong nghiên cứu ablation (§ 5.2) và kiểm soát dễ dàng hơn số lượng tham số có thể điều chỉnh trong nghiên cứu ablation (§ 5.3).

5.1 Điều chỉnh Hiệu quả Tham số Tùy ý (APET)
Tương tự như các phương pháp PET, phương pháp APET được trang bị mô-đun tùy ý θ được cấu thành từ L trọng số có thể điều chỉnh W được phân bố ở bất kỳ vị trí nào của mô hình. Ở đây, APET có ba thao tác để chèn trọng số có thể điều chỉnh W vào bất kỳ vị trí nào của PLM, từ đó sửa đổi các lớp cụ thể và các phép biến đổi tương ứng của chúng như sau:

ADD Trọng số có thể điều chỉnh W sẽ được vào lớp PLM. Phép biến đổi tương ứng của một lớp PLM có thể được ký hiệu như:
hout=f(hin) +W1. (4)

CONCAT Trọng số có thể điều chỉnh W sẽ được nối với trạng thái ẩn hoặc lớp trong PLM. Phép biến đổi tương ứng của một lớp PLM có thể được ký hiệu như:
hout=f(hin) +f(W2)αhinW3W4(5)

PLUG Trọng số có thể điều chỉnh W sẽ được cắm giữa các lớp PLM. Phép biến đổi tương ứng của một lớp PLM có thể được ký hiệu như:
hout=f(hin) +σ(f(hin)W5)W6.(6)

Lưu ý rằng các trọng số có thể điều chỉnh được chèn W không bị giới hạn ở cấu trúc nói trên như được hiển thị trong Hình 3; chúng có thể là các cấu trúc tùy ý. Theo các trọng số có thể điều chỉnh được chèn và các sửa đổi tương ứng, các phép biến đổi của một lớp PLM cho phương pháp APET có thể được biểu diễn như:

hout=f(hin) + [W1, f(W2)αhinW3W4, σ(f(hin)W5)W6, ...] (7)

Bằng cách so sánh Phương trình 7 với các phương trình của Phương trình 2 được giới thiệu trước đó, rõ ràng là các phương pháp PET là những trường hợp đặc biệt của phương pháp APET. Mô-đun θ của APET được cấu thành từ các trọng số được chèn tùy ý W, có thể được biểu diễn như θ={cW1,cW2, ...,cWL}. Trong quá trình huấn luyện, chúng tôi theo Phương trình (3) chỉ tối ưu hóa θ trong khi đóng băng phần còn lại của các tham số (Φ) trong PLM.

5.2 Tác động của Sự khác biệt Vị trí Tham số đối với Hiệu suất
Để điều tra liệu việc mở rộng quy mô mô hình có thể làm giảm tác động của vị trí tham số trong PET, chúng tôi ban đầu đóng băng các yếu tố quan trọng khác, tức là số lượng tham số có thể điều chỉnh, có thể ảnh hưởng đến hiệu suất. Cho rằng các tham số có thể điều chỉnh của bốn phương pháp PET nói trên

--- TRANG 6 ---
được cố định ở cùng các vị trí, chúng tôi gặp khó khăn trong việc tiến hành chính xác một thí nghiệm để đánh giá tác động của vị trí. Dưới hạn chế này, chúng tôi sau đó sử dụng phương pháp APET để chọn tùy ý các tham số có thể điều chỉnh với các seed ngẫu nhiên khác nhau, mỗi seed ngẫu nhiên đại diện cho một phân bố tham số khác nhau, và huấn luyện chúng trên các nhiệm vụ.

Trong các thí nghiệm, chúng tôi đặt số lượng tham số có thể điều chỉnh cho các phương pháp APET thành bốn nhóm. Số lượng tham số trong mỗi nhóm (thanh: h) tương ứng với của bốn phương pháp PET nói trên (Prompt, BitFit, LoRA, Adapter). Chúng tôi ký hiệu các phương pháp APET này với số lượng tham số khác nhau như APETPrompt, APETBitFit, APETLoRA, và APETAdapter, tương ứng. Bên cạnh đó, chúng tôi tiến hành nghiên cứu ablation trên ba loạt mô hình (BERT, BLOOM, và T5) và báo cáo hiệu suất trung bình của nhiệm vụ (SST, RTE, và MRPC).

So sánh Hiệu suất Như được hiển thị trong Hình 4, có bốn nhóm so sánh trong mỗi biểu đồ phụ. Chúng tôi có thể quan sát thấy rằng khi kích thước PLM mở rộng quy mô (BERT: từ [a.1] đến [a.2]; BLOOM: từ [b.1] đến [b.2]; T5: từ [c.1] đến [c.2]), sự khác biệt hiệu suất (độ lệch chuẩn (S.D)) của các phương pháp APET trong mỗi nhóm giảm. Dựa trên phát hiện này, chúng tôi lập luận rằng các mô hình lớn hơn thể hiện hiệu quả cao hơn trong việc làm giảm tác động của sự khác biệt vị trí tham số đối với hiệu suất.

Ngoài ra, chúng tôi đã quan sát thấy rằng mặc dù có số lượng tham số có thể điều chỉnh khác nhau trong bốn nhóm khác nhau (thanh: h) của các phương pháp APET, chúng có ít sự khác biệt hiệu suất hơn trên mô hình lớn hơn. Chúng tôi sẽ đào sâu vào phát hiện này hơn nữa và cung cấp giải thích cho hiện tượng này trong § 5.3.

5.3 Tác động của Sự khác biệt Số lượng Tham số Có thể Điều chỉnh đối với Hiệu suất
Trong phần này, cho phương pháp APET dưới số lượng tham số có thể điều chỉnh khác nhau, chúng tôi quan sát hiệu suất của chúng để tiến hành nghiên cứu ablation.

Từ các kết quả được báo cáo trong Hình 5, chúng tôi có thể thấy rằng (1) trên các mô hình nhỏ hơn, ví dụ, BERT SMALL (- - -), BLOOM 560M (- - -), T5 SMALL (- - -) khi các tham số có thể điều chỉnh của các phương pháp điều chỉnh ít hơn một số nhất định, hiệu suất sẽ giảm xuống hiệu suất đoán ngẫu nhiên; (2) tương tự,

--- TRANG 7 ---
hiện tượng này vẫn đúng trên các mô hình lớn hơn, BERT LARGE (—–), BLOOM 7.1B(—–), T5 XXL(—–). Dựa trên những phát hiện này, chúng tôi có thể lập luận rằng việc mở rộng quy mô mô hình không thể loại bỏ đầy đủ tác động của số lượng tham số có thể điều chỉnh đối với hiệu suất của các phương pháp PET.

Thú vị thay, chúng tôi tìm thấy hai ngưỡng tham số cho các tham số có thể điều chỉnh trong tất cả các mô hình và đặt tên chúng là ngưỡng tham số thấp (Low, Low, Low, Low, Low, Low) cho các tham số điều chỉnh cần thiết và ngưỡng tham số cao (High, High, High, High, High, High) cho các tham số điều chỉnh cần thiết, tương ứng trong Hình 5. Khi các tham số có thể điều chỉnh nhiều hơn ngưỡng tham số thấp, phương pháp APET có thể vượt qua hiệu suất ngẫu nhiên (ví dụ, 1×100/Số loại nhãn % trên BERT, 0% trên BLOOM, và 0% trên T5); khi các tham số có thể điều chỉnh nhiều hơn ngưỡng tham số cao, phương pháp APET gần như có thể đạt được hiệu suất tinh chỉnh toàn bộ tham số (FT). Hơn nữa, chúng tôi thấy rằng việc mở rộng quy mô mô hình ảnh hưởng đến hai ngưỡng tham số. Do đó, chúng tôi khám phá hiện tượng này trong các đoạn văn sau.

Ngưỡng Cao của Các Tham số Điều chỉnh Cần thiết
Dựa trên kết quả thí nghiệm trong biểu đồ phụ [c.1] (SST2) của Hình 5, chúng tôi thấy rằng ngưỡng cao của mô hình lớn hơn luôn thấp hơn ngưỡng cao của mô hình nhỏ hơn. Hiện tượng này đúng trên tất cả các nhiệm vụ (SST2, RTE, MRPC), và cho tất cả các loạt mô hình, như được mô tả trong tất cả các biểu đồ phụ. Do đó, chúng tôi có thể kết luận rằng việc mở rộng quy mô mô hình cho phép các phương pháp điều chỉnh huấn luyện ít tham số cần thiết hơn trong khi đạt được hiệu suất tương tự của tinh chỉnh toàn bộ tham số.

Kết luận này có thể giải thích trực quan tại sao các phương pháp APET có thể đạt được hiệu suất tương đối tương tự trên các mô hình lớn hơn, đặc biệt là trên T5 XXL, như được minh họa trong [c.2] nói trên trong Hình 2. Điều này là do số lượng tham số có thể điều chỉnh trong mỗi nhóm của các phương pháp APET vượt qua các ngưỡng tham số cao trên T5 XXL; do đó, tất cả chúng đều đạt được hiệu suất tương tự của tinh chỉnh toàn bộ tham số.

Ngưỡng Thấp của Các Tham số Điều chỉnh Cần thiết
Từ các kết quả trên, chúng tôi thấy rằng các phương pháp APET sẽ vượt qua hiệu suất đoán ngẫu nhiên (0% trên T5; 0% trên BLOOM; 50% trên BERT) và ngay lập tức đạt đến 80~90% hiệu suất tinh chỉnh toàn bộ tham số khi các tham số có thể điều chỉnh nhiều hơn các ngưỡng thấp. Tuy nhiên, các ngưỡng thấp tương đối cao hơn trên các mô hình lớn hơn (BERT LARGE, BLOOM 7.1B, T5 XXL). Cụ thể, các phương pháp APET cần nhiều tham số có thể điều chỉnh hơn để vượt qua hiệu suất đoán ngẫu nhiên. Hiện tượng này là nhất quán

--- TRANG 8 ---
trên tất cả các nhiệm vụ trên tất cả các loạt mô hình. Do đó, chúng tôi có thể suy ra rằng việc mở rộng quy mô mô hình không thể giảm số lượng tham số điều chỉnh cần thiết để điều khiển PLM thực hiện các nhiệm vụ hạ nguồn.

Hơn nữa, đáng chú ý là các ngưỡng tham số thấp của các phương pháp APET gần như nằm trong cùng phạm vi trên cùng các mô hình. Cụ thể, phạm vi của các ngưỡng thấp nằm trong [8.0e+2, 3.2e+3] trên BERT LARGE, và [4.0e+2, 1.6e+3] trên BERT SMALL; [8.2e+3, 4.1e+4] trên BLOOM 7.1B, [8.2e+3, 4.1e+3] trên BLOOM 560M; [7.9e+3, 8.5e+3] trên T5 XXL, [8.0e+2, 7.9e+3] trên T5 SMALL. Chúng tôi sẽ giải thích hiện tượng này từ góc độ tối ưu hóa trong § 6.

6 Thảo luận về Kết quả Ablation từ Góc độ Tối ưu hóa

Các mục tiêu của tất cả các phương pháp điều chỉnh hiệu quả tham số (PET, APET) có thể được biểu diễn như minθL(M(Φ,θ)(X), Y) như được giới thiệu trong Phương trình (3), trong đó θ là mô-đun có thể điều chỉnh. Mô-đun θ của các phương pháp PET khác nhau bao gồm các cấu trúc khác nhau và số lượng tham số có thể điều chỉnh khác nhau. Trong bài báo này, chúng tôi điều tra tác động của việc mở rộng quy mô mô hình đối với các mô-đun khác nhau, sở hữu số lượng tham số có thể điều chỉnh khác nhau được phân bố trên nhiều vị trí. Chúng tôi thấy rằng việc mở rộng quy mô mô hình lớn hơn có thể (1) làm giảm tác động do sự khác biệt vị trí của các tham số có thể điều chỉnh (§ 5.2) và (2) làm cho các phương pháp PET tối ưu hóa ít tham số có thể điều chỉnh hơn để đạt được hiệu suất tinh chỉnh toàn bộ tham số (§ 5.3). Để hiểu sâu hơn những hiện tượng này, chúng tôi sẽ điều tra các lý do cơ bản từ góc độ tối ưu hóa. (3) Bên cạnh đó, chúng tôi cũng quan sát thấy rằng các phương pháp PET có thể tối ưu hóa gần như số lượng tham số điều chỉnh cần thiết tương tự để vượt qua hiệu suất đoán ngẫu nhiên trên cùng các mô hình backbone (§ 5.3). Mặc dù hiện tượng (3) không do việc mở rộng quy mô mô hình gây ra, chúng tôi cũng có thể giải thích nó từ góc độ tối ưu hóa. Tiếp theo, chúng tôi cùng thảo luận nó và hai phát hiện nói trên (1) và (2) trong các đoạn văn sau.

Tại sao việc mở rộng quy mô mô hình làm giảm tác động do sự khác biệt vị trí của các tham số có thể điều chỉnh đối với hiệu suất PET? Từ góc độ điều khiển tối ưu, một mô-đun có thể điều chỉnh (θ) của một phương pháp điều chỉnh có thể được xem như một bộ điều khiển (Yang and Liu, 2022; Ding et al., 2023) để điều khiển PLM hướng tới các nhiệm vụ hạ nguồn. Khi quy mô mô hình tăng lên, mô hình lớn hơn có dư thừa tham số cao hơn (Aghajanyan et al., 2021), cho phép lựa chọn tùy ý các tham số có thể điều chỉnh để điều chỉnh mà không làm giảm hiệu suất đáng kể (Desai et al., 2019; Chen et al., 2020; Prasanna et al., 2020; Evci et al., 2020); do đó, các bộ điều khiển (mô-đun) có thể có bậc tự do cao hơn.

Điều này có thể giải thích tại sao các vị trí tùy ý của các tham số có thể điều chỉnh có ít tác động hơn sao cho tất cả các phương pháp PET có thể đạt được hiệu suất tương tự trên các mô hình lớn hơn. Đáng chú ý là mặc dù phân bố của các tham số có thể điều chỉnh có ít tác động hơn đến hiệu suất, nó vẫn ảnh hưởng đến tốc độ hội tụ. Do đó, tìm kiếm phân bố tham số tốt hơn để cải thiện tốc độ hội tụ cho các phương pháp PET là một hướng đáng khám phá.

Tại sao việc mở rộng quy mô mô hình tận dụng ít tham số có thể điều chỉnh hơn để đạt được hiệu suất tinh chỉnh toàn bộ tham số? Điều chỉnh θ để định hướng PLM hướng tới các nhiệm vụ NLP hạ nguồn có thể được xem như các thích ứng. Từ góc độ không gian biểu diễn, các thích ứng của các phương pháp PET có thể được tái tham số hóa thành một không gian con thống nhất có chiều thấp (Qin et al., 2021; Aghajanyan et al., 2021; Yi et al., 2022). Aghajanyan et al. (2021) tiến một bước chứng minh rằng thích ứng trên PLM lớn hơn có thể được tái tham số hóa thành không gian chiều thấp hơn; điều này ngụ ý giải thích tại sao các phương pháp PET có thể tối ưu hóa ít tham số hơn trên các mô hình quy mô lớn hơn, ví dụ, T5 XXL, để đáp ứng hiệu suất tinh chỉnh toàn bộ tham số trên các nhiệm vụ.

Tại sao các phương pháp PET có thể tối ưu hóa số lượng tham số có thể điều chỉnh tương tự để vượt qua đoán ngẫu nhiên? Như đã nêu ở trên, các thích ứng của các phương pháp PET có thể được tái tham số hóa thành một không gian con thống nhất. Qin et al. (2021) cho thấy rằng không gian con chiều thấp này được chia sẻ giữa tất cả các nhiệm vụ NLP cho cùng các phương pháp PET. Yi et al. (2022) tiến một bước đề xuất rằng không gian con này cũng được chia sẻ giữa các phương pháp PET khác nhau. Điều này có thể ngụ ý giải thích tại sao tất cả các phương pháp PET có thể điều chỉnh số lượng tham số điều chỉnh cần thiết tương tự để vượt qua hiệu suất đoán ngẫu nhiên trên cùng các mô hình, ngay cả đối với các nhiệm vụ khác nhau (§ 5.3).

7 Kết luận
Lĩnh vực mở rộng quy mô mô hình cho LLM trình bày các hướng quan trọng và thú vị cho cộng đồng LLM. Việc tăng quy mô mô hình tiết lộ nhiều khả năng và lợi thế mới nổi. Trong công trình này, trọng tâm chính của chúng tôi là tác động của việc mở rộng quy mô mô hình liên quan đến các phương pháp PET.

--- TRANG 9 ---
Thông qua các nghiên cứu quan sát toàn diện và các cuộc thảo luận sâu sắc từ góc độ tối ưu hóa, chúng tôi có được những hiểu biết sâu sắc hơn về tác động của việc mở rộng quy mô mô hình đối với PET và lý do đằng sau các hiện tượng quan sát được. Chúng tôi tin rằng những phát hiện của chúng tôi sẽ đóng vai trò như một chất xúc tác, truyền cảm hứng cho nghiên cứu và khám phá tỉ mỉ hơn nữa trong lĩnh vực này.

8 Hạn chế
Bài báo này có thể có một số hạn chế có thể có như sau: (1) chúng tôi chỉ khám phá tác động của luật mở rộng quy mô đối với hiệu suất. Có thể có các điểm nghiên cứu khác đáng khám phá, chẳng hạn như sức mạnh của quy mô mô hình đối với tốc độ hội tụ; (2) chúng tôi nghiên cứu sức mạnh của quy mô mô hình với các thí nghiệm thực nghiệm toàn diện và giải thích các phát hiện từ góc độ tối ưu hóa. Có thể có nhiều bằng chứng lý thuyết hơn để giải thích những phát hiện thú vị này.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc]

--- TRANG 10 ---
[Tiếp tục với các tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục với các tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục với các tài liệu tham khảo]

--- TRANG 13 ---
[Tiếp tục với các tài liệu tham khảo]

--- TRANG 14 ---
A Nhiệm vụ và Tập dữ liệu
Chúng tôi sử dụng các nhiệm vụ NLP khác nhau để đánh giá các phương pháp APET, có thể được chia thành 5 danh mục sau:

Phân tích Cảm xúc (SA) Các nhiệm vụ SA đánh giá liệu một mô hình có thể dự đoán chính xác các nhãn cảm xúc của một câu đầu vào. Trong bài báo này, chúng tôi chọn SST-2 (Socher et al., 2013), IMDB (Maas et al., 2011), và Rotten Tomatoes (Pang and Lee, 2005).

Suy luận Ngôn ngữ Tự nhiên (NLI) Các nhiệm vụ NLI đánh giá khả năng của mô hình phân loại chính xác liệu một giả thuyết có thể được suy ra hay không khi cho một tiền đề. Trong bài báo này, chúng tôi chọn MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), và RTE (Bos and Markert, 2005).

Nhận dạng Diễn đạt (PI) Các nhiệm vụ PI đánh giá liệu một mô hình có thể nhận dạng chính xác các diễn đạt, có nghĩa là hai câu giống hệt nhau về nghĩa ngữ. Trong bài báo này, chúng tôi chọn MRPC (Dolan and Brockett, 2005), và QQP (Sharma et al., 2019).

Hỏi đáp (QA) Các nhiệm vụ QA đánh giá khả năng của mô hình trả lời câu hỏi. Ngữ cảnh có thể có mặt. Trong bài báo này, chúng tôi chọn NQ-Open (Lee et al., 2019), một tập dữ liệu QA thế giới mở không có ngữ cảnh.

Tóm tắt (SUM) Các nhiệm vụ SUM đánh giá khả năng của mô hình tóm tắt một đoạn văn dài thành một bản tóm tắt ngắn hơn mà không mất ngữ nghĩa của văn bản gốc. Trong bài báo này, chúng tôi chọn SamSUM (Gliwa et al., 2019), và Multi-News (Fabbri et al., 2019) trong các thí nghiệm của chúng tôi.

B Các Phương pháp Điều chỉnh Hiệu quả Tham số (PET)
Ở đây, chúng tôi trước tiên tóm tắt lại lớp PLM (transformer). Sau đó, chúng tôi mô tả chi tiết và cấu hình huấn luyện của các phương pháp PET được hiển thị trong Hình 1.

B.1 Kiến trúc Transformer
PLM thường là một chồng của nhiều lớp Transformer, mỗi lớp bao gồm một attention đa đầu và một mạng feed-forward. Attention đa đầu chứa h đầu attention hoạt động song song. Cụ thể, cho một đầu vào X∈Rn×d, đầu attention thứ i hoạt động như sau:

hi=softmax ((XWiq)(XWik)T/√(d/h))(XWiv), (8)

trong đó n là độ dài chuỗi, d là chiều ẩn, Wiq∈Rn×d là truy vấn, Wik∈Rn×d là khóa, và Wiv∈Rn×d là giá trị. Đầu ra từ mỗi đầu attention sẽ được nối và được biến đổi thêm bởi Wo∈Rd×d và được ký hiệu như:

hMHA=concat (h1,h2, ...,hh)Wo, (9)

trong đó hMHA∈Rn×d là trạng thái ẩn đầu ra của lớp attention đa đầu. Sau đó, h sẽ được đưa vào mạng feed-forward hai lớp

hFFN=σ(hW1+b1)W2+b2, (10)

trong đó W1∈Rd×dm, W2∈Rdm×d, b1∈Rdm, b2∈Rd, và dm> d là một số nguyên.

Trong quá trình truyền tiến qua mỗi khối (transformer), trạng thái ẩn đầu vào được áp dụng với chuỗi các lớp. Để đơn giản, chúng tôi hình thức hóa phép biến đổi của mỗi lớp như

hout=f(hin). (11)

Dưới lớp như toán tử f, trạng thái ẩn đầu vào hin∈Rn×d được biến đổi thành trạng thái ẩn đầu ra hout∈Rn×d, trong đó s là độ dài đầu vào, và d là chiều.

B.2 Chi tiết Triển khai của Các Phương pháp PET
Prompt Prompt-tuning (Lester et al., 2021) đặt trước Np token mềm có thể điều chỉnh, tức là embedding, vào các câu đầu vào và yêu cầu mô hình dự đoán xác suất của từ tiếp theo. Trong quá trình huấn luyện, chỉ các embedding mới được thêm vào được tối ưu hóa và mô hình backbone bị đóng băng.

BitFit BitFit (Ben Zaken et al., 2022) là một phương pháp chỉ điều chỉnh tất cả các bias term Wb∈Rd trong PLM, nằm trong các lớp self-attention và layer norm.

LoRA LoRA (Hu et al., 2022b) là một phương pháp thích ứng PLM trong không gian rank thấp. Nó down-project các trọng số attention xuống một chiều thấp hơn và up-project nó trở lại chiều gốc. Chỉ các trọng số projection này được tối ưu hóa.

Adapter Adapter (Houlsby et al., 2019b) là một phương pháp chỉ điều chỉnh các mô-đun adapter được chèn, bao gồm down projection, biến đổi phi tuyến, up projection, và một skip-connection. Đối với mỗi lớp Transformer hiện có trong PLM, các mô-đun adapter được chèn tại hai

--- TRANG 15 ---
vị trí: (1) sau lớp feed-forward đầu tiên, và (2) sau hai lớp feed-forward liên tiếp. Trong quá trình huấn luyện, chỉ các mô-đun adapter được tối ưu hóa và phần còn lại của PLM bị đóng băng.

B.3 Cấu hình Huấn luyện của Các Phương pháp PET
Mô-đun có thể điều chỉnh của một phương pháp PET θ được cấu thành từ L trọng số có thể điều chỉnh W (tất cả trọng số có thể điều chỉnh) của phương pháp PET cụ thể, có thể được biểu diễn như θ={W1,W2, ...,WL}. Chúng tôi cũng theo Phương trình (3) để huấn luyện phương pháp PET. Trong quá trình huấn luyện, chúng tôi chỉ tối ưu hóa θ trong khi đóng băng phần còn lại của các tham số trong PLM. Chúng tôi áp dụng kích thước batch là 32 và không có warm-up cho hầu hết các mô hình và nhiệm vụ PET. Độ dài đầu vào tối đa là 128 cho các nhiệm vụ câu đơn (SA) và 256 cho các nhiệm vụ đa câu (NLI, PI, QA, SUM). Độ dài tạo sinh tối đa là 1 cho các nhiệm vụ phân loại (SA, NLI, PI), 64 cho Multi-News, và 128 cho SAMSum. Trên các mô hình BERT, BLOOM, T5, chúng tôi đặt tỷ lệ học của chúng là {3e-4}, {3e-4, 5e-5}, {1e-4, 1e-3, 1e-2} tương ứng. Sau đó, chúng tôi chọn hiệu suất tốt nhất để báo cáo.

C Các Phương pháp Điều chỉnh Hiệu quả Tham số Tùy ý (APET)
Chúng tôi giới thiệu một phương pháp PET linh hoạt hơn, phương pháp Arbitrary Parameter-Efficient Tuning (APET). Mô-đun có thể điều chỉnh của nó có thể là cấu trúc tùy ý tạo thuận lợi cho chúng tôi khám phá các cấu trúc mô-đun khác nhau (vị trí tham số) và kiểm soát dễ dàng hơn số lượng tham số có thể điều chỉnh.

C.1 Chi tiết Triển khai của Các Phương pháp APET
Như chúng tôi đã giới thiệu trước đó trong § 5.1, mô-đun có thể điều chỉnh của phương pháp APET được cấu thành từ các trọng số có thể điều chỉnh. Mỗi trọng số có thể điều chỉnh có thể được biểu diễn như W. Ở đây, chúng tôi có ba thao tác để chèn trọng số có thể điều chỉnh W vào PLM để sửa đổi các lớp cụ thể và các phép biến đổi tương ứng của chúng như sau:

ADD Chúng tôi sẽ thêm trọng số có thể điều chỉnh W vào lớp PLM. Phép biến đổi tương ứng có thể được ký hiệu như hout:
f(hin) +W1. (12)

CONCAT Chúng tôi sẽ nối trọng số có thể điều chỉnh W và trạng thái ẩn hoặc lớp trong PLM. Phép biến đổi tương ứng có thể được ký hiệu như hout:
f(hin) +f(W2)αhinW3W4(13)

PLUG Chúng tôi sẽ cắm trọng số có thể điều chỉnh W giữa các lớp PLM. Phép biến đổi tương ứng có thể được ký hiệu như hout:
f(hin) +σ(f(hin)W5W6). (14)

Theo các thao tác này và các phép biến đổi tương ứng, chúng tôi có thể biểu diễn các phương pháp APET như hout:
f(hin) + [W1, f(W2)αhinW3W4, σ(f(hin)W5)W6, ...] (15)

Bằng cách so sánh Phương trình (15) với các phương trình của các phương pháp PET được giới thiệu trước đó, chúng tôi có thể thấy rõ ràng rằng các phương pháp PET là những trường hợp đặc biệt của các phương pháp APET.

--- TRANG 16 ---
C.2 Cấu hình Huấn luyện của các phương pháp APET
Mô-đun có thể điều chỉnh của một phương pháp APET θ được cấu thành từ L trọng số có thể điều chỉnh W, có thể được biểu diễn như θ={W1,W2, ...,WL}. Chúng tôi cũng theo Phương trình (3) để huấn luyện phương pháp APET. Trong quá trình huấn luyện, chúng tôi chỉ tối ưu hóa θ trong khi đóng băng phần còn lại của các tham số trong PLM.

Bên cạnh đó, chúng tôi áp dụng kích thước batch là 32 và không có warm-up cho hầu hết các mô hình và nhiệm vụ APET. Ngoài ra, độ dài đầu vào tối đa là 128 cho các nhiệm vụ câu đơn (SA) và 256 cho các nhiệm vụ đa câu (NLI, PI, QA, SUM). Độ dài tạo sinh tối đa là 1 cho các nhiệm vụ phân loại (SA, NLI, PI), 64 cho Multi-News, và 128 cho SAMSum. Trên các mô hình BERT, BLOOM, T5, chúng tôi đặt tỷ lệ học của chúng là {3e-4}, {3e-4, 5e-5}, {1e-4, 1e-3, 1e-2} tương ứng. Sau đó, chúng tôi chọn hiệu suất tốt nhất để báo cáo.

D Số lượng Tham số Có thể Điều chỉnh của APET
Ở đây, Bảng 2 hiển thị số lượng tham số có thể điều chỉnh của APET cho mỗi nhóm trong Hình 4.

E Sức mạnh của Quy mô Mô hình đối với Khả năng Chuyển giao
Hơn nữa, để khám phá liệu sức mạnh của quy mô mô hình có thể tạo thuận lợi cho khả năng tổng quát hóa của các phương pháp điều chỉnh, chúng tôi khám phá khả năng chuyển giao của các phương pháp APET giữa các nhiệm vụ NLP trong cài đặt zero-shot (Vu et al., 2022; Su et al., 2022; Ding et al., 2023). Trong các thí nghiệm, chúng tôi trước tiên huấn luyện các tham số của các phương pháp APET trên các nhiệm vụ nguồn và tái sử dụng trực tiếp chúng trên các nhiệm vụ đích trong cài đặt zero-shot. Chúng tôi sẽ điều tra hai loạt PLM T5 (T5 SMALL và T5 XXL) và BERT (BERT SMALL và BERT LARGE) và báo cáo hiệu suất tương đối.

Lưu ý rằng đối với các loại nhiệm vụ khác nhau, chúng được kỳ vọng chia sẻ các nhóm nhãn khác nhau (ví dụ đối với nhiệm vụ như SA, các nhãn thường là positive/negative, trong khi đó, đối với các nhiệm vụ như NLI, các nhãn thường là entailment/not entailment). Việc tái sử dụng các tham số được huấn luyện trên nhiệm vụ nguồn để kiểm tra trên nhiệm vụ đích sẽ tự nhiên thất bại vì mô hình không thể tạo ra các nhãn mà chúng chưa từng thấy trong giai đoạn huấn luyện. Vì mục đích này, chúng tôi thường ánh xạ các bộ nhãn gốc thành một bộ nhãn thống nhất (ví dụ negative/not entailment/false –> 0, positive/entailment/true –> 1). Việc sử dụng bộ nhãn thống nhất làm cho việc đánh giá khả năng chuyển giao của phương pháp AFP giữa các loại nhiệm vụ khác nhau trở nên khả thi bất kể sự khác biệt của các nhãn gốc.

Kết quả được hiển thị trong Hình 7, từ đó chúng tôi có thể thấy rằng các phương pháp APET (APET DISCRETE và APET ADJACENT) có thể chuyển giao sang cùng loại nhiệm vụ được thể hiện bởi màu tối hơn dọc theo đường chéo của ma trận và thường hoạt động tốt cả trên PLM quy mô nhỏ (Hình 7 (a): BERT SMALL và T5 SMALL) và PLM quy mô lớn (Hình 7 (b): BERT LARGE và T5 XXL). Tuy nhiên, màu nhạt hơn chỉ ra rằng các phương pháp APET gặp khó khăn trong việc thực hiện các loại nhiệm vụ khác nhau tổng thể, và cả PLM quy mô nhỏ và quy mô lớn đều chia sẻ hiện tượng này. Phát hiện này chỉ ra rằng sức mạnh của quy mô không nhất thiết tạo thuận lợi cho khả năng tổng quát hóa của các phương pháp AFP phù hợp với giả định phổ biến rằng ít tham số hơn thường gây ra underfitting, trong khi nhiều tham số hơn có xu hướng gây ra overfitting. Tuy nhiên, cơ chế đằng sau hiện tượng này vẫn khơi gợi mối quan tâm sâu sắc của chúng tôi và đáng mở rộng mà chúng tôi sẽ phân tích một cách có hệ thống trong công việc tương lai.

--- TRANG 17 ---
[Tiếp tục với các bảng và hình ảnh bổ sung từ phụ lục]
