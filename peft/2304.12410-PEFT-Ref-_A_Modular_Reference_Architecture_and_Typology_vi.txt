# 2304.12410.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2304.12410.pdf
# Kích thước tệp: 719772 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PEFT-Ref: Một Kiến trúc Tham chiếu Mô-đun và Hệ thống Phân loại
cho các Kỹ thuật Tinh chỉnh Hiệu quả Tham số
Mohammed Sabry
ADAPT/DCU, Dublin, Ireland
mohammed.sabry@adaptcentre.ieAnya Belz
ADAPT/DCU, Dublin, Ireland
anya.belz@adaptcentre.ie

Tóm tắt
Các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) gần đây nhằm cải thiện chi phí đáng kể của việc tinh chỉnh hoàn toàn các mô hình ngôn ngữ được đào tạo trước lớn (PLM). Khi các kỹ thuật PEFT khác nhau gia tăng, việc so sánh chúng trở nên khó khăn, đặc biệt về (i) cấu trúc và chức năng mà chúng thêm vào PLM, (ii) các loại và mức độ cải thiện hiệu quả khác nhau đạt được, (iii) hiệu suất ở các nhiệm vụ downstream khác nhau, và (iv) cách các khác biệt về cấu trúc và chức năng liên quan đến hiệu quả và hiệu suất nhiệm vụ. Để tạo điều kiện cho những so sánh như vậy, bài báo này trình bày một kiến trúc tham chiếu chuẩn hóa các khía cạnh được chia sẻ bởi các kỹ thuật PEFT khác nhau, trong khi cô lập các khác biệt đến các vị trí và tương tác cụ thể với các thành phần tiêu chuẩn. Thông qua quá trình chuẩn hóa và cô lập khác biệt này, một cái nhìn mô-đun về các kỹ thuật PEFT xuất hiện, hỗ trợ không chỉ so sánh trực tiếp các kỹ thuật khác nhau và hiệu quả cũng như hiệu suất nhiệm vụ của chúng, mà còn khám phá có hệ thống khả năng tái sử dụng và khả năng kết hợp của các loại mô-đun tinh chỉnh khác nhau. Chúng tôi trình bày cách kiến trúc tham chiếu có thể được áp dụng để hiểu các tính chất và lợi thế tương đối của các kỹ thuật PEFT, do đó thông báo việc lựa chọn kỹ thuật cho các nhiệm vụ cụ thể, và các lựa chọn thiết kế cho các kỹ thuật PEFT mới.

1 Giới thiệu
Trong vài năm qua, đã có sự gia tăng đáng kể về kích thước của các mô hình ngôn ngữ được đào tạo trước (PLM) như GPT3 (Brown et al., 2020), OPT (Zhang et al., 2022a), BLOOM (Workshop et al., 2022), và PaLM (Chowdhery et al., 2022), có hàng tỷ tham số. Sự gia tăng kích thước này đã được đi kèm với sự gia tăng tương ứng về chi phí đào tạo và triển khai các PLM lớn, với những tác động tài chính và môi trường đáng kể. Việc tái sử dụng PLM thông qua thích ứng với các nhiệm vụ downstream, thay vì đào tạo các mô hình ngôn ngữ mới cho các nhiệm vụ mới, giảm thiểu chi phí này một cách đáng kể. Tuy nhiên, tinh chỉnh hoàn toàn, phương pháp thích ứng nhiệm vụ mặc định, vẫn rất tốn kém vì nó đào tạo lại, và sau đó lưu trữ, toàn bộ mô hình.

Các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) cải thiện chi phí này bằng cách (tái)đào tạo một tập hợp tham số nhỏ hơn nhiều. Các phương pháp heuristic sửa đổi một tập con cụ thể của các tham số hiện có của mô hình, ví dụ Lee et al. (2019) tinh chỉnh một phần tư cuối của các lớp trong BERT và RoBERTa, và Zaken et al. (2022) chỉ tinh chỉnh các bias term của mô hình. Các kỹ thuật PEFT khác như Adapters (Houlsby et al., 2019), prefix tuning (Li và Liang, 2021), prompt tuning (Lester et al., 2021), và LoRA (Hu et al., 2021), thay vào đó đóng băng tất cả các tham số PLM, và thêm và đào tạo một tập hợp nhỏ các tham số mới cùng với các tham số này. Một số nghiên cứu (Ding et al., 2023; Chen et al., 2022a; He et al., 2022; Mao et al., 2022) đã phát hiện các kỹ thuật PEFT thêm tham số như vậy rất hiệu quả trong các nhiệm vụ thực tế. Đó là nhóm phương pháp PEFT này mà chúng tôi tập trung trong bài báo này.

Khi ngày càng nhiều kỹ thuật PEFT được báo cáo, việc so sánh chúng về cải thiện hiệu quả và hiệu suất ở các nhiệm vụ khác nhau trở nên khó khăn hơn, đặc biệt là khía cạnh nào của cấu trúc và chức năng của chúng liên quan đến hiệu quả và hiệu suất tốt hơn. Để giải quyết vấn đề này, chúng tôi đề xuất framework PEFT-Ref bao gồm kiến trúc tham chiếu mô-đun và hệ thống phân loại cung cấp cách chuẩn hóa để đặc trưng hóa các kỹ thuật PEFT về các tính chất cấu trúc và chức năng của chúng. Trong bài báo này, chúng tôi trình bày kiến trúc tham chiếu (Phần 2), và sử dụng nó để tạo ra hệ thống phân loại của bảy kỹ thuật PEFT hàng đầu (Phần 3), và so sánh các kỹ thuật về hiệu quả và hiệu suất (Phần 4). Chúng tôi minh họa cách điều này có thể được sử dụng để thông báo các lựa chọn thiết kế và lựa chọn kỹ thuật cho các nhiệm vụ cụ thể (Phần 5), và kết thúc với đánh giá công trình liên quan và một số kết luận (Phần 6 và 7).

--- TRANG 2 ---
2 Framework PEFT-Ref
Trong phần này, chúng tôi trình bày PEFT-Ref dưới dạng sơ đồ (Phần 2.1), và theo các tính chất phân loại mà nó định nghĩa (Phần 2.2). Kết hợp, kiến trúc tham chiếu và các tính chất được dự định để nắm bắt đầy đủ các khác biệt và tương đồng giữa các kỹ thuật PEFT khác nhau, làm cơ sở để hiểu nguyên nhân về điểm mạnh và điểm yếu tương đối của chúng, và thông báo việc lựa chọn và phát triển kỹ thuật.

2.1 Kiến trúc tham chiếu PEFT mô-đun
Hình 1 hiển thị kiến trúc tham chiếu PEFT dưới dạng sơ đồ, cho thấy cách các loại mô-đun khác nhau được tạo và đào tạo bởi các kỹ thuật PEFT khác nhau ghép vào và tương tác với kiến trúc Transformer tiêu chuẩn. Hầu hết các tính chất được định nghĩa trong phần tiếp theo cũng được mô tả trong sơ đồ (xem chú giải Hình 1).

Trong sơ đồ, các lớp lặp L× được hiển thị bên trong hộp màu xám, với luồng residual ở bên phải. Các thành phần của PLM Transformer tiêu chuẩn được hiển thị trong các hộp màu đen (không gạch ngang). Trong các lớp embedding, attention và feed-forward, và ngay sau các lớp attention và feed-forward, chúng tôi hiển thị nơi và cách các kỹ thuật PEFT khác nhau chèn các mô-đun của chúng, được chỉ ra bằng các hộp gạch ngang. Lưu ý rằng chúng thường không được kết hợp, tức là chỉ một loại mô-đun PEFT thường được chèn.

2.2 Các tính chất mô-đun của mô-đun PEFT
Hệ thống phân loại PEFT-Ref bao gồm các tính chất cấu trúc và chức năng mô-đun sau đây. Chúng tôi chuyển thể một số tên tính chất từ văn liệu tính toán mô-đun chung, một số từ công trình gần đây về tính mô-đun trong mạng nơ-ron, và một số là mới, như được chỉ ra. Phạm vi giá trị tính chất là cụ thể cho PEFT-Ref trong tất cả các trường hợp. Bảng 1 liệt kê các tính chất này và giá trị cụ thể của chúng cho bảy kỹ thuật PEFT hàng đầu.

1. Kết nối nội bộ (Clune et al., 2013; Meunier et al., 2010) – dense hoặc sparse: Kết nối nơ-ron trong các lớp của mô-đun PEFT. Kết nối nội bộ dày đặc hơn cho thấy tính mô-đun cao hơn. Tất cả các kỹ thuật PEFT hiện tại, trừ (IA)³, đều được kết nối nội bộ dày đặc. Trong Bảng 1, chúng tôi cũng hiển thị loại thành phần được kết nối dày đặc cụ thể: lớp embedding, MLP phi tuyến, MLP tuyến tính, self-attention; (IA)³ chèn một tham số vector độc lập không dày đặc cũng không thưa thớt.

2. Kết nối liên kết (Béna và Goodman, 2021; Meunier et al., 2010) – fixed:dense, fixed:sparse hoặc dynamic: Cách các mô-đun PEFT được kết nối với kiến trúc PLM. Kết nối liên kết thưa thớt hơn cho thấy tính mô-đun cao hơn. Tất cả các kỹ thuật PEFT hiện tại trừ tiny-attention adapters (Zhao et al., 2022) có kết nối liên kết fixed/dense.

3. Tham số được thích ứng (Ding et al., 2023) – addition hoặc reparameterisation: Tất cả các kỹ thuật PEFT thay đổi các tham số mô hình, bằng cách thêm chúng hoặc tái tham số hóa các thành phần hiện có trong kiến trúc PLM.

4. Chia sẻ/Ràng buộc Tham số – shared, tied, hoặc none: Trong chia sẻ tham số hai tập hợp tham số bị buộc phải giống nhau; trong ràng buộc, hai tập hợp tham số được giữ gần nhau. Chia sẻ/ràng buộc tham số có nhiều lợi thế trong điều chuẩn hóa và inductive bias (Yeh et al., 2022), bao gồm hiệu suất và ổn định tốt hơn với ít tham số hơn. Trong số các kỹ thuật PEFT hiện tại, chỉ Compacters (Karimi Mahabadi et al., 2021) chia sẻ tham số, trong các lớp tái tham số hóa của chúng.

5. Loại đầu vào (Pfeiffer et al., 2023b; Auda và Kamel, 1999) – hidden, data hoặc weights: Loại đầu vào mà các mô-đun PEFT nhận: (i) biểu diễn ẩn nhận từ khối lớp Transformer, (ii) dữ liệu trước khi vào khối, hoặc (iii) ma trận trọng số được khởi tạo mới, trong trường hợp các kỹ thuật PEFT thêm và tối ưu hóa ma trận trọng số (Prompt Tuning, Prefix Tuning, và (IA)³).

6. Dạng chèn (Pfeiffer et al., 2023b; Auda và Kamel, 1999) – sequential hoặc parallel: Liệu mô-đun tinh chỉnh có được chèn vào PLM theo tuần tự hay song song. Hầu hết các kỹ thuật chèn mô-đun theo tuần tự nhận đầu ra của khối lớp Transformer mà chúng cộng tác.

7. #Chèn – n layers hoặc all layers: Có bao nhiêu instance của mô-đun PEFT được chèn vào PLM. Tất cả các kỹ thuật PEFT hiện tại trừ prompt tuning (chỉ chèn vào lớp embedding), chèn mô-đun vào tất cả các lớp Transformer lặp L×. Prefix tuning cũng thêm tham số vào lớp embedding.

--- TRANG 3 ---
Hình 1: Kiến trúc tham chiếu PEFT mô-đun hiển thị các thành phần PLM (hộp trung tâm), các loại mô-đun PEFT khác nhau (bên trái và phải của trung tâm), và các vị trí chèn của mô-đun PEFT (hộp gạch ngang trong hộp PLM) và tương tác giữa các mô-đun PEFT và thành phần PLM (xem thêm chú giải bên trái).

8. Dạng tích hợp (Auda và Kamel, 1999) – concatenation, scaled addition, direct addition, gated addition, hoặc rescaling: Cách đầu ra của mô-đun PEFT được tích hợp vào PLM.

9. Không gian làm việc – attention layer, FFN layer hoặc embedding layer: Trong khoa học nhận thức, không gian làm việc là kênh truyền thông băng thông hạn chế trong đó các mô-đun khác nhau trao đổi thông tin (Baars, 1988). Trong AI, Goyal et al. (2022) sử dụng mô hình không gian làm việc chia sẻ để mô tả trao đổi thông tin có hệ thống giữa các vùng chuyên biệt trong mạng nơ-ron. Trong bối cảnh của chúng tôi, hầu hết các kỹ thuật PEFT sử dụng các lớp attention và/hoặc các lớp kết nối đầy đủ trong PLM làm không gian làm việc của chúng. Bảng 1 cũng chỉ ra, khi thích hợp, vị trí tương tác cụ thể trong không gian làm việc – queries/values, keys/values, (FFN) intermediate representation.

3 Đặc trưng hóa các Kỹ thuật PEFT với PEFT-Ref

Trong phần này, chúng tôi đặc trưng hóa bảy kỹ thuật PEFT hàng đầu theo các tính chất cấu trúc mô-đun PEFT-Ref. Chúng tôi tập trung vào các kỹ thuật PEFT có tính mô-đun theo nghĩa chúng thêm và đào tạo các tập hợp tham số riêng biệt về mặt topo. Các kỹ thuật như vậy đã được chứng minh là rất hiệu quả và thường được sử dụng trong các nhiệm vụ downstream và thực tế (Ding et al., 2023; Liu et al., 2022), trái ngược với các phương pháp heuristic hơn thích ứng một tập con cố định cụ thể của tham số PLM (Lee et al., 2019; Zaken et al., 2022). Bảng 1 cung cấp tổng quan về bảy kỹ thuật PEFT được đề cập trong phần này, theo các tính chất phân loại được giới thiệu trong Phần 2.2.

3.1 Prompt Tuning (PT)
Topo và chức năng nội bộ mô-đun: Prompt Tuning (PT) (Lester et al., 2021) tạo ra các embedding giống token sử dụng lớp embedding (kết nối nội bộ), sau đó được nối (dạng tích hợp) với các embedding đầu vào của PLM (không gian làm việc), như được hiển thị trong Hình 1. Quá trình tinh chỉnh tùy chỉnh các embedding giống token theo mục tiêu nhiệm vụ.

Các tính chất mô-đun và cộng tác với PLM: PT chỉ chèn tham số ở lớp embedding, nối tất cả các embedding giống token với

--- TRANG 4 ---
[Bảng 1: Tính chất cấu trúc của các mô-đun PEFT được tạo bởi bảy kỹ thuật PEFT (để mô tả các kỹ thuật xem Phần 3; để định nghĩa các tính chất xem Phần 2.2).]

các embedding đầu vào, dẫn đến kết nối liên kết fixed-dense giữa PLM và PT.

3.2 Prefix Tuning (PF)¹
Topo và chức năng nội bộ mô-đun: Trái ngược với Prompt Tuning, chỉ tạo ra các embedding giống token sử dụng lớp embedding, Li và Liang (2021) đề xuất sử dụng hai lớp tuyến tính với kích hoạt Softmax ở giữa (Hình 1).

Các tính chất mô-đun và cộng tác với PLM: Li và Liang (2021) hơn nữa mở rộng không gian làm việc thành các embedding đầu vào và keys và values của Attention trong tất cả các lớp Transformer. Các embedding giống token PF được nối với các ma trận này (tức là dạng tích hợp).² PF kết nối tất cả thông tin của nó với PLM (tức là kết nối liên kết của nó là fixed:dense).

3.3 LoRA
Topo và chức năng nội bộ mô-đun: LoRA (Hu et al., 2021) thích ứng PLM sử dụng các ma trận phân tách thứ hạng thấp. Ý tưởng là việc cập nhật tham số mô hình có thể được xấp xỉ sử dụng phân tách chiều thấp. LoRA tái tham số hóa các trọng số queries và values của Attention thành các ma trận thứ hạng thấp. Đối với mỗi cái, LoRA sử dụng hai lớp chiếu tuyến tính nhỏ (kết nối liên kết) để tái tham số hóa các trọng số. LoRA nhận cùng đầu vào mà các trọng số được tái tham số hóa nhận (tức là dạng chèn là song song).

Các tính chất mô-đun và cộng tác với PLM: LoRA tạo ra queries và values của đầu vào và cộng tác (dạng tích hợp) thông qua scaled addition (h+λ∆h) với queries và values của Attention (không gian làm việc) của đầu vào trong tất cả các lớp Transformer. LoRA gửi tất cả thông tin của nó đến không gian làm việc (tức là kết nối liên kết = fixed:dense).

3.4 Adapters
Topo và chức năng nội bộ mô-đun: Adapters (Houlsby et al., 2019) sử dụng lớp feed-forward (kết nối nội bộ) tạo bottleneck thông tin thông qua hai lớp tuyến tính chiếu thông tin xuống rồi lên, với kích hoạt ReLU ở giữa. Adapters thích ứng các biểu diễn ẩn từ các khối Attention và FNN (dạng chèn = tuần tự).

Các tính chất mô-đun và cộng tác với PLM: Adapters tích hợp kết quả của chúng với không gian làm việc (các khối Attention và FNN) thông qua direct addition (h+ ∆h). Mặc dù tồn tại các biến thể của Adapters thay đổi kết nối nội bộ hoặc #chèn, như AdapterDrop (Rücklé et al., 2021), Compacters (Karimi Mahabadi et al., 2021), và Tiny-Attention Adapters (Zhao et al., 2022), tất cả đều sử dụng direct addition để tích hợp. Adapters gửi tất cả thông tin của chúng đến không gian làm việc (kết nối liên kết = fixed:dense).

3.5 Tiny-Attention Adapters
Topo và chức năng nội bộ mô-đun: Tiny-Attention Adapters (Zhao et al., 2022) là biến thể của Adapters thay đổi kết nối nội bộ thành lớp Attention nhỏ (Hình 1).

Các tính chất mô-đun và cộng tác với PLM: Giống như Adapters, Tiny-Attention Adapters được chèn tuần tự, cộng tác thông qua direct addition với không gian làm việc của chúng, và nhận biểu diễn ẩn làm đầu vào. Tuy nhiên, chúng được chèn sau khối Attention (không gian làm việc), và gửi thông tin của chúng đến không gian làm việc một cách có chọn lọc dựa trên đầu vào (kết nối liên kết = dynamic).

3.6 Compacters
Topo và chức năng nội bộ mô-đun: Compacters (Karimi Mahabadi et al., 2021) là biến thể của Adapters với khác biệt sau. Trong lớp Adapter vanilla, W∈Rᵏˣᵈ. Ngược lại, Compacters tái tham số hóa lớp W như tổng của các tích Kronecker, với k và d chia hết cho siêu tham số n do người dùng định nghĩa. Cụ thể, tổng của n tích Kronecker là W=∑ⁿᵢ₌₁Aᵢ⊗Bᵢ, trong đó Aᵢ∈Rⁿˣⁿ và Bᵢ∈Rᵏ/ⁿˣᵈ/ⁿ.

¹Prefix tuning được xuất bản trước prompt tuning, nhưng hai cái dường như đã được phát triển đồng thời và, tình cờ, cái trước là biến thể nâng cao của cái sau.
²He et al. (2022) chứng minh rằng nối trong khối Attention có thể được xem như một dạng tích hợp gated addition; trong (1−λ)h+λ∆h, h đại diện cho chức năng PLM và ∆h đại diện cho chức năng mô-đun PEFT.

--- TRANG 5 ---
Compacters cải thiện hiệu quả tham số hơn nữa bằng cách chia sẻ trọng số của Aᵢ giữa các lớp của compacter.

Các tính chất mô-đun và cộng tác với PLM: Compacters có cùng tính chất như Adapters về cộng tác với PLM, dạng chèn, dạng tích hợp, và không gian làm việc.

3.7 (IA)³
Topo và chức năng nội bộ mô-đun: Mô-đun (IA)³ (Liu et al., 2022) bao gồm ba vector tái quy mô các khối Attention (keys, values), và FFN của lớp Transformer (Hình 1). Trong quá trình tinh chỉnh, các vector này được khởi tạo bằng một để đảm bảo rằng mô-đun không ảnh hưởng đến chức năng của PLM trước khi được hướng dẫn bởi gradient mục tiêu của nhiệm vụ.

Các tính chất mô-đun và cộng tác với PLM: (IA)³ áp dụng tái quy mô vector đã học vào không gian làm việc của nó (keys, values, và FFN trung gian) trên tất cả các lớp Transformer. Nó được chèn tuần tự và gửi tất cả thông tin của nó đến không gian làm việc (kết nối liên kết = fixed:dense).

4 So sánh Hiệu quả và Hiệu suất với PEFT-Ref

Trong phần này, chúng tôi sử dụng PEFT-Ref làm cơ sở cho một số loại so sánh khác nhau giữa bảy kỹ thuật được đặc trưng hóa trong phần trước. Trong Phần 4.1, chúng tôi xem xét kỹ hơn chính xác những cải thiện hiệu quả nào mà mỗi kỹ thuật PEFT đạt được, (i) so với tinh chỉnh hoàn toàn liên quan đến tất cả tham số PLM, và (ii) so với các kỹ thuật PEFT khác. Sau đó trong Phần 4.2, chúng tôi xem xét những gì chúng ta biết cho đến nay về hiệu suất của bảy kỹ thuật ở các nhiệm vụ benchmark khác nhau, và liên kết nó với các tính chất mô-đun của chúng.

4.1 Cải thiện hiệu quả

4.1.1 Độ phức tạp
Bảng 2 cung cấp tổng quan về các kỹ thuật PEFT về độ phức tạp thời gian mỗi token của (các) mô-đun mà chúng thêm (cột 2), và số lượng

[Bảng 2: Hiệu quả của bảy kỹ thuật PEFT được khảo sát; dₘ= chiều mô hình, dₕ= chiều mô-đun PEFT, n= số token cho prompt và prefix tuning; k, r, d = chiều đầu vào/đầu ra của mô-đun PEFT, trong đó đối với LoRA r là rank, và đối với Adapters k là chiều bottleneck. d=dₘ.T= #Input embeddings. N= Chiều giảm trong tích Kronecker.]

tham số được thêm mỗi lớp Transformer (cột 3). Độ phức tạp thời gian mô-đun (cột 2) được kiểm soát bởi kết nối nội bộ và loại đầu vào.³ Ở đây, chúng tôi chỉ tính thời gian một kỹ thuật PEFT cần để tạo ra đầu ra để cộng tác với PLM. Theo nghĩa này, ví dụ (IA)³ có độ phức tạp thời gian hằng số O(1), vì đầu ra được lấy trực tiếp từ mô-đun sau khởi tạo trọng số, và được sử dụng như một rescaler cho các kích hoạt của PLM. Chúng tôi cung cấp thêm chi tiết phân tích độ phức tạp mô-đun trong Phụ lục A.

Số lượng tham số (cột 3) chủ yếu được kiểm soát bởi loại không gian làm việc, #chèn, và kết nối nội bộ. Ví dụ, (IA)³ sử dụng ba vector để tái quy mô keys, values của Attention, và biểu diễn trung gian FFN, cho dₘ (chiều mô hình) cho keys và values mỗi cái, cộng 4dₘ cho biểu diễn trung gian FFN, tức là tổng cộng 6dₘ.

4.1.2 Hiệu quả trong đào tạo
Hiệu quả tham số không nhất thiết chuyển thành hiệu quả học tập. Ding et al. (2023) đã kiểm tra sự hội tụ của các kỹ thuật PEFT như LoRA, Adapters, Prefix Tuning, và Prompt Tuning so với tinh chỉnh hoàn toàn. Kết quả cho thấy tinh chỉnh hoàn toàn hội tụ nhanh nhất, tiếp theo là Adapters/LoRA, rồi Prefix Tuning, trong khi Prompt Tuning có tốc độ hội tụ chậm nhất. Khi PLM tăng kích thước, sự hội tụ của các kỹ thuật PEFT trở nên nhanh hơn. Kết quả của Ding et al. cũng chỉ ra rằng sự hội tụ nhạy cảm hơn với cấu trúc hơn số lượng tham số.

PEFT-Ref rõ ràng tính đến các tính chất cấu trúc kiểm soát tốc độ hội tụ, bao gồm kết nối nội/liên kết, #chèn, sản xuất đầu ra (loại đầu vào và dạng chèn), và chia sẻ tham số. Chẳng hạn, sự hội tụ chậm trong Prompt Tuning có thể được gán cho sự bất ổn do đầu ra (embedding giống token) được tối ưu hóa trực tiếp, trong khi Prefix Tuning nhạy cảm với các lựa chọn tái tham số hóa (kết nối nội bộ) tạo ra đầu ra này. Ngoài ra, một số kỹ thuật PEFT có thể có tốc độ hội tụ tương tự hoặc tốt hơn tinh chỉnh hoàn toàn tùy thuộc vào độ phức tạp nhiệm vụ.

Chen et al. (2022a) đã kiểm tra tính ổn định của hiệu suất qua các random seed khác nhau cho một số kỹ thuật PEFT bao gồm Adapters, LoRA, và Prefix Tuning, theo một nghiên cứu tương tự về tính ổn định của tinh chỉnh hoàn toàn (Dodge et al., 2020). Các tác giả phát hiện rằng các kỹ thuật PEFT này, giống như tinh chỉnh hoàn toàn, dễ bị biến động hiệu suất do khởi tạo trọng số và thứ tự dữ liệu. Hơn nữa, các tác giả đã điều tra tác động của việc kiểm soát số lượng tham số trong các kỹ thuật này đối với tính ổn định của chúng. Họ quan sát thấy rằng giảm số lượng tham số trong các kỹ thuật PEFT có thể tăng tính ổn định của chúng. Do đó, họ khuyến nghị thận trọng khi chọn các yếu tố giảm trong Adapters, rank trong LoRA, và độ dài prompt trong Prefix Tuning, và đặt chúng trong phạm vi thấp. Trong Phụ lục A, chúng tôi xem xét chi tiết hơn hiệu quả forward & backward training passes trong bối cảnh độ phức tạp mô-đun như trong Phần 4.1.1.

4.1.3 Hiệu quả lưu trữ và trong ứng dụng
Cột cuối trong Bảng 2 hiển thị số lượng tham số được thêm mỗi lớp transformer, và các biến kiểm soát nó, cho mỗi kỹ thuật PEFT trong bảy kỹ thuật. Bằng cách chỉ lưu các mô-đun PEFT cụ thể nhiệm vụ sau tinh chỉnh⁴ thay vì toàn bộ mô hình như sẽ được yêu cầu trong tinh chỉnh hoàn toàn, kích thước lưu trữ có thể giảm drastically từ gigabyte xuống vài megabyte. Hiệu quả lưu trữ này làm cho việc phục vụ nhiều người dùng và ứng dụng sử dụng một PLM độc lập duy nhất cùng với nhiều mô-đun PEFT cụ thể nhiệm vụ khác nhau trở nên khả thi.

Các tính chất cấu trúc được định nghĩa trong PEFT-Ref (ví dụ, chèn, loại đầu vào, tham số thích ứng,

³Các kỹ thuật PEFT với độ phức tạp thời gian O(1) xuất ra từ đầu vào trong một bước. Trừ các phương pháp sử dụng mạng khác để tạo trọng số, tất cả các kỹ thuật PEFT nhận trọng số làm đầu vào và tạo ra trọng số làm đầu ra có độ phức tạp thời gian O(1) theo nghĩa này.

⁴Tất cả các kỹ thuật PEFT lưu các tham số có thể điều chỉnh của chúng, ngoại lệ là Prefix Tuning, chỉ lưu các embedding giống token cuối cùng và loại bỏ mạng đã tạo ra chúng.

--- TRANG 6 ---
không gian làm việc, chia sẻ tham số) trực tiếp kiểm soát hiệu quả theo nghĩa này, do đó tạo điều kiện cho những hiểu biết về cải thiện tiềm năng. Trong Phụ lục A, chúng tôi xem xét chi tiết hơn độ trễ suy luận cho hiệu quả trong ứng dụng trong bối cảnh độ phức tạp mô-đun như trong Phần 4.1.1.

4.2 Hiệu suất nhiệm vụ
Trong Bảng 3 trong Phụ lục B, chúng tôi đã tài liệu hóa hiệu suất của các kỹ thuật PEFT khác nhau qua các nhiệm vụ khác nhau dựa trên nghiên cứu trước đây.

Trong số các kỹ thuật chúng tôi đã kiểm tra, LoRA nổi bật như người thực hiện hàng đầu trong một số nhiệm vụ, dù là lựa chọn đầu tiên hay thứ hai tốt nhất. LoRA hoạt động cộng tác với PLM để cải thiện các thành phần quan trọng, đặc biệt là các ma trận queries. Đây là kỹ thuật PEFT duy nhất cộng tác với thành phần này.

Adapters, và các biến thể của chúng, cũng thể hiện điểm hiệu suất xuất sắc, và có vẻ như các tính chất tái tham số hóa và chia sẻ tham số trong Compacter nâng cao hiệu quả của chúng.⁵ Cuối cùng, chúng tôi quan sát thấy (IA)³ thực hiện tốt hơn trong các nhiệm vụ lý luận thông thường so với LoRA và Adapters có thể được gán cho cái trước sử dụng rescaling như dạng tích hợp của nó, và cái sau sử dụng addition.

LoRA, Adapters, và Compacter sử dụng chỉ các lớp attention, hoặc các lớp attention và FFN, làm không gian làm việc của chúng và phân tích của chúng tôi chỉ ra rằng các kỹ thuật PEFT sử dụng các khối feed-forward và/hoặc Attention làm không gian làm việc của chúng được liên kết với điểm hiệu suất cao hơn.

5 Sử dụng PEFT-Ref để Hướng dẫn Lựa chọn và Phát triển Kỹ thuật

Trong phần này, chúng tôi bắt đầu từ (i) thông tin mà PEFT-Ref cung cấp về các kỹ thuật PEFT, và (ii) hiệu suất của chúng ở các nhiệm vụ downstream khác nhau, để rút ra những kết luận rộng về tính phù hợp của mỗi kỹ thuật cho các loại nhiệm vụ khác nhau (Phần 5.1).

Sau đó chúng tôi tiến thêm một bước và suy đoán cách các kỹ thuật PEFT (Phần 5.2) có thể được phát triển thêm, hoặc thậm chí kết hợp, để cải thiện tính ổn định, tốc độ hội tụ, và/hoặc hiệu suất nhiệm vụ của chúng.

5.1 Lựa chọn kỹ thuật PEFT
Prompt Tuning là kỹ thuật phù hợp cho nhiệm vụ như Named Entity Recognition, vì nó hoạt động trên lớp embedding, vốn đã có đủ thông tin ngữ cảnh để giải quyết nhiệm vụ này, sau khi truyền qua các lớp mô hình ngôn ngữ bị đóng băng. Điều này có nghĩa là việc điều kiện hóa nhiệm vụ chỉ trên embedding là đủ. Ngoài ra, Prompt Tuning có độ phức tạp lớp O(1) và số lượng tham số thấp, làm cho nó trở thành lựa chọn hiệu quả có thể đạt hiệu suất tốt ngay cả với ngân sách tính toán nhỏ.

LoRA có thể là lựa chọn phù hợp cho các nhiệm vụ Question Answering vì nó hoạt động trên không gian làm việc queries và values của attention cho phép mô hình xác định các mối quan hệ liên quan giữa các từ và cụm từ trong câu hỏi và câu trả lời (hiệu suất của LoRA trong các nhiệm vụ QA đa lựa chọn Bảng 3 hỗ trợ điều này). Ngoài ra, dạng tích hợp tunable scaling có thể hỗ trợ mô hình tận dụng tốt hơn thông tin quan trọng để giải quyết nhiệm vụ. Tiny-Attention Adapters có thể cung cấp attention bổ sung và có thể cải thiện đầu ra biểu diễn ẩn sau khối attention Transformer vì chúng được chèn tuần tự sau nó.

Các nhiệm vụ Data-to-Text và Summarisation có thể hưởng lợi từ việc sử dụng LoRA hoặc Prefix Tuning. Nghiên cứu trước đây (Li và Liang, 2021; Liu et al., 2022; Xu et al., 2022; Ding et al., 2023) đã cho thấy các kỹ thuật này cung cấp hiệu suất tương đương, nhưng sự lựa chọn giữa chúng phụ thuộc vào ngân sách tính toán có sẵn. LoRA có ít tham số hơn và độ phức tạp lớp tốt hơn so với Prefix Tuning, làm cho nó trở thành lựa chọn hiệu quả hơn, và hiệu suất của chúng trong các nhiệm vụ này có thể được giải thích bởi các tính chất của chúng trong PEFT-Ref. Công trình gần đây (Xu et al., 2022) đã đánh giá Adapters cho các nhiệm vụ generation và phát hiện rằng mặc dù chúng có hiệu suất tốt, chúng có điểm faithfulness tệ hơn tinh chỉnh hoàn toàn và Prefix Tuning. Để giải thích những kết quả này dưới ánh sáng của PEFT-Ref, có thể lưu ý rằng Adapters sử dụng khối feed-forward ngoài khối attention làm không gian làm việc của chúng. Tuy nhiên, Zhang et al. (2022b) phát hiện rằng khối feed-forward chứa nhiều redundancy. Thay đổi khối này thêm có thể dẫn đến điểm faithfulness thấp hơn cho các nhiệm vụ generation.

Kết luận, việc lựa chọn kỹ thuật PEFT phụ thuộc vào độ phức tạp của nhiệm vụ đang thực hiện. Chẳng hạn, nếu nhiệm vụ yêu cầu lý luận trên ngữ cảnh

⁵Adapters và Compacters khác nhau về chia sẻ tham số và tái tham số hóa, với Compacters hiệu quả hơn Adapters. Những tính chất này chịu trách nhiệm cho hiệu suất của chúng, như được hiển thị trong Bảng hiệu suất 3.

--- TRANG 7 ---
(Chen et al., 2022b), nên chọn phương pháp có các mô-đun attention làm không gian làm việc. Alternatively, nếu nhiệm vụ liên quan đến việc thêm các khái niệm mới vào mô hình ngôn ngữ, các mô-đun feed-forward có thể được sử dụng để lưu trữ kiến thức trong Transformer (Dai et al., 2022), do đó làm cho chúng trở thành không gian làm việc tiềm năng cho thích ứng. Đối với các nhiệm vụ đơn giản không yêu cầu bất kỳ yêu cầu nào ở trên, việc thêm thông tin cụ thể nhiệm vụ thông qua không gian làm việc embedding sẽ đủ. Tất cả những hiểu biết này có thể dễ dàng suy ra sử dụng PEFT-Ref.

5.2 Phát triển thêm các kỹ thuật PEFT
Chia sẻ/ràng buộc tham số có nhiều lợi thế trong điều chuẩn hóa và inductive bias (Yeh et al., 2022). ALBERT (Lan et al., 2020), một mô hình ngôn ngữ đạt được giảm tham số bằng cách chia sẻ và factorising tham số, đạt hiệu suất cao và ổn định với ít tham số hơn BERT. Do đó, chia sẻ tham số là tính chất hấp dẫn có thể góp phần đáng kể vào hiệu suất và tính ổn định của các kỹ thuật tinh chỉnh. Việc cho phép chia sẻ/ràng buộc tham số qua các lớp của các mô-đun khác nhau, cũng như qua các lớp Transformer, giữ tiềm năng nâng cao đáng kể hiệu suất và tính ổn định của các kỹ thuật PEFT.

Việc áp dụng tham số scaling có thể điều chỉnh trong Adapters, như trong LoRa (Hu et al., 2021), có thể cải thiện dramatically các phương pháp này vì chúng cộng tác với tất cả các khối trong lớp Transformer. Sự cộng tác đáng kể như vậy có thể cần được kiểm soát thông qua scaled addition. Chúng tôi cũng lưu ý những điều chỉnh đơn giản nhưng hiệu quả, như AdapterDrop (Rücklé et al., 2021), động loại bỏ một số lớp Adapter được gắn với tất cả các lớp Transformer trong thiết lập vanilla. Ngoài ra, tính ổn định có thể được tăng trong prompt tuning bằng cách giới thiệu layering phù hợp để tạo ra trọng số prompt để nối với embedding.

Một hướng tiềm năng khác cho phát triển là kiểm soát số lượng chèn của mô-đun PEFT bằng cách chọn các lớp cụ thể (thay vì tất cả) để chèn. Các kỹ thuật tinh chỉnh đặc tả heuristic (ví dụ Lee et al., 2022, tinh chỉnh một phần tư cuối của các lớp trong BERT và RoBERTa) đạt hiệu suất tốt có thể được sử dụng như chỉ dẫn về lớp nào cần chọn.

Các mô-đun PEFT có thể sử dụng luồng residual (tức là embedding có ngữ cảnh của chuỗi đầu vào) làm không gian làm việc, và thích ứng nó bằng cách tái tham số hóa hoặc thêm tập hợp tham số mới như vector scaling.

Ngoài ra, các kỹ thuật tinh chỉnh đặc tả heuristic như BitFit (Zaken et al., 2022) và LN-Tuning (Qi et al., 2022) tinh chỉnh các bias term và LayerNorm trong mô hình tương ứng, đại diện cho không gian làm việc tiềm năng để thiết kế các mô-đun PEFT thích ứng chúng. Lợi thế của việc sử dụng PEFT trên các đặc tả heuristic này là nó bảo tồn kiến thức của mô hình PLM về các tham số như bias và LayerNorm và cộng tác với chúng thay vì thay đổi chúng.

6 Công trình Liên quan
He et al. (2022) bao gồm xử lý các phương pháp PEFT giải quyết kiến trúc nội bộ, biểu diễn được sửa đổi, dạng chèn, và hàm composition. Tuy nhiên, để nắm bắt đầy đủ tiềm năng của các kỹ thuật PEFT từ quan điểm mô-đun, việc nắm bắt phạm vi đa dạng của các tính chất bù đắp cho những biến thể tinh tế của chúng là cần thiết: trong dạng chức năng, tất cả bốn phương pháp PEFT được xem xét đều được xử lý như có dạng Project down → Nonlinear/linear → Project up, nhưng không phải tất cả phương pháp PEFT đều có dạng này (ví dụ Prompt Tuning, (IA)³, Tiny-Attention Adapters). Hơn nữa, về biểu diễn được sửa đổi, xử lý một cách khó hiểu xử lý mô-đun Transformer tạo ra biểu diễn ẩn như chính biểu diễn ẩn (tức là nó xử lý vị trí (mô-đun Transformer) như biểu diễn ẩn).

Ngoài ra, không phải tất cả phương pháp PEFT đều sửa đổi biểu diễn ẩn. Trong công trình của chúng tôi, chúng tôi tách biệt rõ ràng giữa vị trí (Workspace trong PEFT-Ref), và biểu diễn ẩn được sửa đổi (loại đầu vào trong PEFT-Ref). Cũng không phải tất cả kỹ thuật PEFT thường được tích hợp với mô hình ngôn ngữ chỉ thông qua các dạng addition (ví dụ Prompt Tuning, (IA)³).

Pfeiffer et al. (2023b) trình bày cái nhìn thống nhất về deep learning mô-đun, tập trung vào bốn chiều chính: triển khai mô-đun, hàm routing, tập hợp mô-đun, và đào tạo mô-đun. Quan điểm này tiết lộ kết nối giữa các thread nghiên cứu độc lập trước đây và các ứng dụng khác nhau của mạng mô-đun. Trong khi Pfeiffer et al. thảo luận ngắn gọn về một số kỹ thuật PEFT dưới triển khai mô-đun, họ chỉ sử dụng loại composition để phân loại chúng (input composition cho prompt và prefix tuning, parameter composition cho LoRA, function composition cho Adapters).

Công trình khác đã khảo sát các kỹ thuật hiệu quả tham số và nghiên cứu nền tảng lý thuyết và hiệu suất của chúng trên các nhiệm vụ downstream khác nhau. Ví dụ, Ding et al. (2023) thiết kế thư viện trên thư viện Transformers (Wolf et al., 2020) để cho phép đào tạo linh hoạt, composing, gắn/tháo các kỹ thuật PEFT với PLM. Mao et al. (2022) đề xuất framework mixture of experts cho các kỹ thuật PEFT học kích hoạt kỹ thuật PEFT phù hợp nhất với nhiệm vụ.

7 Kết luận
Trong công trình được báo cáo ở đây, chúng tôi nhằm góp phần vào sự hiểu biết toàn diện hơn về lĩnh vực nghiên cứu đang phát triển nhanh chóng của các kỹ thuật PEFT. Trong bài báo này, chúng tôi đã giới thiệu framework PEFT-Ref bao gồm kiến trúc tham chiếu và hệ thống phân loại dựa trên danh mục các tính chất cấu trúc và chức năng được chuẩn hóa của các phương pháp PEFT. Chúng tôi đã cho thấy cách các kỹ thuật PEFT có thể được đặc trưng hóa theo framework và cách đặc trưng hóa như vậy cho phép so sánh trực tiếp giữa các phương pháp PEFT về cải thiện hiệu quả và hiệu suất nhiệm vụ.

Chúng tôi đã phân tích thêm các đặc trưng hóa PEFT-Ref của bảy phương pháp PEFT hàng đầu, để (i) rút ra những kết luận quan trọng về tính phù hợp của chúng cho các loại nhiệm vụ khác nhau, và (ii) trích xuất những chỉ dẫn rõ ràng để phát triển các phương pháp PEFT cải thiện trong tương lai.

PEFT-Ref cung cấp kiến trúc tham chiếu đơn giản nhưng tổng quát được thiết kế để tạo điều kiện (i) dễ nhớ các thành phần của nó, và (ii) hiểu biết so sánh về các phương pháp PEFT khác nhau. Hơn nữa, việc có cái nhìn mô-đun về các kỹ thuật PEFT khuyến khích tăng khả năng tái sử dụng của PLM cho các use case và nhiệm vụ khác nhau, và phù hợp với lời kêu gọi gần đây để xây dựng và duy trì các mô hình ngôn ngữ lớn như phần mềm mã nguồn mở.⁶

Hạn chế
Trong công trình này, mục tiêu của chúng tôi là thiết lập nền tảng vững chắc để hiểu các kỹ thuật PEFT bằng cách nhấn mạnh cái nhìn mô-đun về các tham số mà chúng thêm và/hoặc thao tác. Chúng tôi đề xuất rằng các kỹ thuật PEFT có thể được xem như các mô-đun nhỏ làm việc cộng tác với các mô-đun lớn, như mô hình ngôn ngữ, để giải quyết các nhiệm vụ cụ thể. Bằng cách áp dụng quan điểm mô-đun này, chúng ta có thể tận dụng những lợi ích cấu trúc và chức năng của tính mô-đun.

Mục tiêu chính của chúng tôi là thống nhất các kỹ thuật PEFT, đi sâu hơn vào hoạt động bên trong của chúng để có được hiểu biết toàn diện. Ngoài ra, chúng tôi tìm cách xác định các lĩnh vực mà những kỹ thuật này có thể được cải thiện và đưa ra hướng dẫn về việc đưa ra những lựa chọn thông tin khi chọn kỹ thuật cho nhiệm vụ downstream.

Về những chỉ dẫn cho phát triển tương lai, chúng tôi (chưa) cung cấp triển khai cho những cải thiện đối với phương pháp PEFT mà chúng tôi đề xuất. Về điểm mạnh tương đối của các phương pháp PEFT khác nhau, có những yếu tố khác đóng vai trò trong việc lựa chọn phương pháp nằm ngoài phạm vi của công trình hiện tại.

Cuối cùng, trong khi chúng tôi đã bao gồm các phương pháp PEFT hàng đầu trong các đặc trưng hóa mẫu của mình, chúng tôi chưa bao gồm tất cả các biến thể và phương pháp khác tồn tại. Do đó, có thể hình dung rằng việc bao gồm chúng sẽ dẫn đến sửa đổi framework, đặc biệt về phạm vi giá trị tính chất.

⁶https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html

--- TRANG 8 ---
[Phần này chứa các tài liệu tham khảo - References - từ trang 8-12]

--- TRANG 13 ---
[Bảng 3: Độ chính xác trung bình của các kỹ thuật PEFT trên các nhiệm vụ khác nhau qua các dataset. Tiny-Attention Adapters được bỏ qua khỏi Bảng do thiếu các nghiên cứu so sánh trong văn liệu đã xuất bản.]
