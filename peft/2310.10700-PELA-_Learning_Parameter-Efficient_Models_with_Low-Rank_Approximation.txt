# 2310.10700.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2310.10700.pdf
# File size: 997331 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PELA: Learning Parameter-Efficient Models with Low-Rank Approximation
Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli
National University of Singapore
Abstract
Applying a pre-trained large model to downstream tasks
is prohibitive under resource-constrained conditions. Re-
cent dominant approaches for addressing efficiency issues
involve adding a few learnable parameters to the fixed
backbone model. This strategy, however, leads to more
challenges in loading large models for downstream fine-
tuning with limited resources. In this paper, we propose a
novel method for increasing the parameter efficiency of pre-
trained models by introducing an intermediate pre-training
stage. To this end, we first employ low-rank approximation
to compress the original large model and then devise a fea-
ture distillation module and a weight perturbation regular-
ization module. These modules are specifically designed to
enhance the low-rank model. In particular, we update only
the low-rank model while freezing the backbone parameters
during pre-training. This allows for direct and efficient uti-
lization of the low-rank model for downstream fine-tuning
tasks. The proposed method achieves both efficiencies in
terms of required parameters and computation time while
maintaining comparable results with minimal modifications
to the backbone architecture. Specifically, when applied
to three vision-only and one vision-language Transformer
models, our approach often demonstrates a merely ‚àº0.6
point decrease in performance while reducing the original
parameter size by 1/3 to 2/3. The code has been released at
link.
1. Introduction
Pre-training a large model and fine-tuning it at hand has
become a de facto paradigm in diverse research fields [9,
10, 59]. While significant performance has been achieved,
building such models often compromises increased memory
usage and longer training time. Despite these challenges,
recent advances in the appreciation of the scaling law [27]
and emergent abilities [61] of language pre-training have
further fueled practitioners‚Äô interest in developing and uti-
lizing large models.
As it is usually prohibitive to deploy these models for
downstream tasks, recent studies have resorted to bypass-
Research Intuition
34
‚Ä¶√óùëÅùëÅ
√óùëÅùëÅTransformer Layers
PELA
Compression
Figure 1. Overview and performance of our proposed PELA
method. Left: Using PELA, we compress the trainable weights
of a typical ViT model while preserving its overall architecture.
Right: Comparison of the Original and our PELA w.r.t relative
model size and accuracy metric on three pre-trained Transformers.
ing the fine-tuning of the entire model. Typical approaches
often introduce a few more learnable parameters to the
backbone model while freezing the rest, e.g., adapter [51]
and prompt tuning [24] add tunable parameters to the mid-
dle and peripheral token positions of Transformers, respec-
tively. However, this approach inevitably leads to the fol-
lowing two disadvantages. First, the potential of pre-trained
large models is not fully exploited as the majority of pa-
rameters are not tuned with downstream task objectives.
Second, loading the pre-trained model becomes even more
burdensome for researchers with limited resources. In con-
trast, conventional methods such as knowledge distillation
(KD) [19, 20, 50] and quantization [7, 22] can partially al-
leviate this issue. Yet, there is currently no established ap-
proach for constructing a high-performing student model of
KD and non-differentiable operators of quantization usually
make it less feasible to perform back-propagation.
This paper targets developing a highly parameter-
efficient approach to help downstream task fine-tuning,
as illustrated in Fig. 1. By parameter-efficiency, we re-
fer to a compressed model with reduced-size ( e.g., 2√ó
smaller), easy-to-implement, computationally-efficient, and
minimal-architectural-change merits. Our method offers a
pre-trained compressed model that downstream tasks can
1arXiv:2310.10700v2  [cs.CV]  17 Nov 2023

--- PAGE 2 ---
directly perform fine-tuning on, in contrast to previous ap-
proaches such as LoRA [21], adapter [51], and prompt tun-
ing [24]. In particular, this method is specially designed to
address the over-parameterization problem [1], where we
resort to the low-rank approximation to replace the pre-
trained weight matrix in each matrix multiplication oper-
ation with two low-rank matrices. By this means, the origi-
nal model size and fine-tuning time are both fairly reduced.
However, using this naive approach to perform fine-tuning
yields less satisfactory outcomes (refer to Sec. 4.4). We at-
tribute this fact to two reasons: Directly decomposition with
low-rank approximation cannot effectively learn instance-
level discriminative representations; and the intermediate
feature distribution is perturbed after this operation, result-
ing in sub-optimal performance.
In order to approach this problem, we propose fully tak-
ing advantage of the pre-trained model via two modules.
The implementation involves two parallel model branches:
one consists of the pre-trained model with fixed parameters
during pre-training, while the other is the low-rank model
with tunable parameters. Based upon this framework, our
first module distills the feature knowledge from the large
pre-trained model to our compressed low-rank model in
terms of each Transformer layer. The other module helps
bind the weight change within a pre-defined perturbation
radius. These two modules help the low-rank model mimic
the feature distribution of the large pre-trained model,
thereby enhancing its discrimination capability. During
fine-tuning, we simply use the low-rank model as a replace-
ment for the original large one to achieve parameter and
computational efficiency for downstream tasks.
As far as we know, the literature on achieving a desirable
efficiency-effectiveness trade-off by using low-rank approx-
imation on pre-trained Transformer weights is quite limited.
We apply our method to three vision-only Transformers i.e.,
DeiT [54], DeiT-III-Large [56] and SwinT [39], with 1/2
to 2/3 of the original model parameters; and one vision-
language Transformer ‚Äì ALBEF [33] where the parameter
size is reduced to 1/3 of the original model. We then con-
duct extensive experiments on a range of downstream tasks,
including image classification, semantic segmentation, and
object detection for the vision-only Transformers; Visual
entailment, visual grounding, cross-modal retrieval, and
visual question answering for the vision-language Trans-
former. Our approach achieves performance that is highly
comparable to the backbone model, with differences mostly
around 0.6 points, despite using only 1/3 to 2/3 of the orig-
inal FLOPs. In addition, this parameter-efficiency benefit
further enables the model to scale with larger batch sizes,
leading to improved performance that sometimes even out-
performs backbones.2. Related Work
2.1. Parameter-Efficient Learning
Efficiency has long been an engaging problem in a variety
of research areas [26, 49]. After stepping into the deep rep-
resentation learning era, the progressive improvements in
our community often trade with a large number of model
parameters, latency, and footprints [41, 58]. With this con-
cern, previous efforts have been mainly devoted to three
distinctive directions: knowledge distillation (KD), quan-
tization, and pruning. Deemed as a principled model com-
pression algorithm, early KD aims to transfer the knowl-
edge from a cumbersome teacher model to a lightweight
student model via class logit alignment [20, 46]. Recent
focus has been shifted to feature-based knowledge transfer
due to its performance advantage over conventional logit-
based ones [19, 25, 45, 69]. For example, [25, 50] dis-
till the knowledge from hidden states and attention matri-
ces, which on the other hand, can also bypass the logit-free
training objectives. However, choosing features from which
layers to align remains challenging as there is no teacher-
student layer match from a theoretical basis. Quantization,
from another angle of efficient learning, maps larger bit pa-
rameters to smaller ones, e.g., 32-bit floating point to an
8-bit integer [44]. This kind of method is not dependent
on the model structures, which makes it flexible in various
neural networks [7, 22, 36]. The key downside lies in its
performance reduction and possible infeasibility for back-
propagation. Different from the above two categories, prun-
ing is leveraged to remove unnecessary or less important
components in models [58]. By removing some connec-
tions [65] or parameters [29], the original dense network
reduces to a sparse one, in which the required capacity for
storage as well as the amount of computations will dwindle.
Transformer-based approaches have succeeded in di-
verse research domains since their introduction [9, 57].
These models often involve billions of parameters, which
consequently, motivates some specific methods working on
addressing the parameter-efficiency problem [30]. The typ-
ical strategy is to add a few learnable parameters while
freezing the majority of the Transformer backbone during
downstream training. For instance, prompt tuning appends
some task-specific parameters into the input space [24];
Adapter models introduce several learnable MLP compo-
nents into each Transformer layer [51]; and fine-tuning bias
only has also been proven effective for maintaining good
performance of large language models [72].
2.2. Low-Rank Approximation
Low-rank approximation aims to decompose one matrix
into two smaller matrices, subject to the constraint that the
resulting matrices have reduced rank [47, 48]. One key
merit of this algorithm is data compression, whereby previ-
2

--- PAGE 3 ---
ous work has applied it to principal component analysis [42]
and recommendation [13, 18].
Pertaining to Convolutional Neural Networks (CNNs),
some approaches apply the low-rank approximation to each
feature map via higher-order tensor decomposition [12,
52, 73]. Dynamically decomposing trainable matrices
has also attracted much attention [66, 68, 70]. Some
more studies explored other aspects of low-rank approxi-
mation, such as rank learning [23], constrained optimiza-
tion [32], and employing it specifically in token embedding
matirx [5, 30] or self-attention computation in Transform-
ers [60]. LoRA [21] models the residual of parameters with
low-rank approximation, wherein only the newly decom-
posed matrices are exploited for downstream training and
it thus achieves significantly reduced trainable parameters.
Despite its benefits, the LoRA approach still has limitations,
as it necessitates the storage and reloading of large pre-
trained weights in hard disk and GPU memory, respectively.
In other words, only the newly introduced trainable param-
eters that are of a smaller magnitude compared to the full
parameters are updated for fine-tuning, making it similar to
adapters [14, 51] and prompt tuning [24]. Unlike existing
approaches, our method uses low-rank approximation dur-
ing pre-training to entirely replace the pre-trained weights
with reduced low-rank matrices. As a result, we achieve
both memory and computational efficiency goals for down-
stream fine-tuning tasks.
3. Method
Transformers have grown into a fundamental building block
of many modern vision models [10, 17]. Take the seminal
Vision Transformer (ViT) as an example. ViT first divides
an RGB image I‚ààR3√óH√óWintoM√óMnon-overlapping
patches. Together with a class token, these image patches
are thereafter fed into Nlayers with self-attention as the
basic operation. To this end, a set of query, key, and value
matrices are transformed from the patch embedding to to-
ken features X‚ààR(M2+1)√ód, where ddenotes the embed-
ding size, followed by several feedforward layers and resid-
ual connections. At their core lies the fully connected layer,
which is often wrapped in the attention score estimation and
MLP operations - WTX+b, where W‚ààRdin√ódoutis the
learnable weight matrix and b‚ààRdoutdenotes the bias,
anddin=dfor the first layer.
3.1. Low-rank Approximation
Over-parameterization is a common issue in modern large
models [1]. In this work, we aim to address this prob-
lem by reducing the number of model parameters. In-
spired by the success of low-rank approximation in other
domains [12, 52], we propose to apply this technique di-
Baseline Low-Rank PELA4050607080Accuracy (%)
Performance Change
0.0 0.5 1.00123Baseline
0.0 0.5 1.00123Low-RankFigure 2. Performance comparison of three models and statistics
of the instance-level feature similarity. Left: We use the DeiT
model as the baseline and show the performance of its directly
low-rank approximation and PELA variants. The middle and right
sub-figures illustrate the instance-level feature similarity of DeiT
and directly low-rank model variants, respectively.
rectly to the matrix multiplication operations in ViT,
WTX‚âà(UVT)TX
=V(UTX),(1)
where U‚ààRdin√ódlrandV‚ààRdout√ódlrare low-rank
matrices, and dlrrepresents the desired rank of W. Note
that the weight matrices in a deep learning model are of-
ten with full-rank, i.e.,rank(W) =min(din, dout). Un-
der such conditions, we seek approximately equal the orig-
inal matrix and deliberately choose a smaller dlr,e.g.,
1
4min(din, dout). The second equation constantly holds in
neural networks due to the natural associative law. This
property allows us to achieve computational efficiency
without needing to recover the original weight matrix W
after applying the low-rank approximation. We utilize the
well-known SVD approach [31] to perform the low-rank ap-
proximation as,
SVD(WT) =U‚àóŒ£V‚àó, (2)
where Œ£‚ààRdin√ódoutis a rectangular diagonal matrix with
non-negative real numbers on the diagonal, and the singu-
lar values are sorted in a monotonously decreasing order;
U‚àó‚ààRdin√ódinandV‚àó‚ààRdout√ódoutare complex unitary
matrices. We then formalize the low-rank matrices using
the following transformation,
Ô£±
Ô£≤
Ô£≥U=U‚àó
[:,:dlr]Œ£1
2
[:dlr,:dlr],
V= (Œ£1
2
[:dlr,:dlr]V‚àó
[:dlr,:])T,(3)
where [:,:dlr]implies we truncate the given matrix with the
top-dlrcolumns and other truncation operations can also be
easily deduced.
Preliminary observation. We apply this low-rank approx-
imation to the fully connected layers of pre-trained mod-
els. Unfortunately, this process delivers less desirable re-
sults, e.g., the accuracy drops from 81% to 61% as seen in
Fig. 2. This indicates that the compressed low-rank model
3

--- PAGE 4 ---
PELA -Full Model
35
‚Ä¶ √óùëÅùëÅTransformer LayersFeature Distillation
Regularized Weight Perturbation
√ó|ùëæùëæ‚àíùëºùëºùëΩùëΩùëáùëá|‚àûùëæùëæ
ùëºùëº
ùëΩùëΩLarge model with
parameters fixedLow-rank model with
parameters tunedùë•ùë•‚ÑéAdd
Fixed
ùë•ùë•‚ÑéTunable
Tunable
(a) Method overview(b) Existing approaches
(c) Our methodFigure 3. Overview of our proposed PELA and pipeline comparison with existing methods. (a) We leverage a typical ViT model as the
base for the illustration of our method. The two involved modules, i.e., Feature Distillation aligns the token features in an apple-to-apple
fashion of each layer, and Regularized Weight Perturbation bounds the recovered weight matrices. During fine-tuning on downstream
tasks, existing approaches use both the fixed pre-trained model weights and the newly added parameters (b). In contrast, our PELA keeps
only the low-rank model while excluding the large pre-trained model for efficient computation (c).
does not effectively learn instance-level discriminative rep-
resentation. Moreover, we found that the learned features
after low rank are confined in a narrow feature space. In par-
ticular, the right two subfigures in Fig. 2 demonstrate that
the feature similarity of each class of the low-rank model is
drastically higher than before.
To overcome this, we propose to take full advantage of
the large pre-trained model and harness it to guide the train-
ing of the low-rank model. Specifically, as shown in Fig. 3,
we first perform low-rank approximation on the pre-trained
model and retain both models. The parameters of the large
pre-trained model are frozen while we train only the low-
rank model. Our method further involves two modules: fea-
ture distillation to align features between these two mod-
els,regularized weight perturbation to constrain the affin-
ity of the recovered matrix and original matrix. We name
this method PELA , dubbed Parameter- Efficient models for
Low-rank Approximation . To the best of our knowledge,
there is limited research on constructing an effective low-
rank model based on pre-trained Transformers. Therefore,
we aim to address this gap by investigating the potential of
low-rank approximation to achieve an optimal efficiency-
effectiveness trade-off.
3.2. Feature Distillation
As highlighted in the previous sub-section, the low-rank
approximation can alter the feature distribution of the
pre-trained model. To address this problem, we resort
to feature-based knowledge distillation, which has been
proven effective in aligning the features between mod-els [45]. Nevertheless, the low-rank compression is per-
formed on each matrix multiplication operation, rather than
specific Transformer blocks or layers. Directly distill-
ing knowledge from all the output features of the original
model leads to more clutter as some low-rank compression
is already wrapped within the self-attention computation.
Thanks to the layer-wise residual connection of Transform-
ers, we employ a compromise in this work ‚Äì simply align-
ing the token features of each layer. From a general view
of typical Transformer models, the feature distillation loss
is defined as follows,
Lfd=NX
i=1D(Ms(Xi
s),Mt(Xi
t)),
=1
2NNX
i=1‚à• M s(Xi
s)‚àí M t(Xi
t)‚à•2,(4)
where Xi
sandXi
tdenote the i-th layer token features of
the compressed low-rank model and original model, respec-
tively; Mis a transformation that transfers the feature to
the target feature space and we employ identity mapping in
our implementation. In this way, the output features from
each ViT layer of the low-rank model are expected to share
a similar distribution with that of the corresponding large
pre-trained model.
An alternative view from knowledge distillation. Re-
cent studies have shown that feature-based knowledge dis-
tillation significantly outperforms conventional logit-based
one [19]. Nonetheless, how to design a student model
and transfer the knowledge from the teacher model remains
4

--- PAGE 5 ---
challenging as it is rather difficult to define teacher-student
feature matching. Our method offers a neat solution to this
problem due to the following two reasons: 1) Unlike pre-
vious approaches ( e.g., [25, 50] manually removing cer-
tain layers), the low-rank approximation is effortless and
straightforward to compress the cumbersome teacher model
to a lightweight student model. 2) There exists a natural cor-
respondence between the teacher model and student model
as we have not excessively altered the model architectures.
3.3. Regularized Weight Perturbation
An ideal low-rank approximation is to learn an approximat-
ing matrix of the original one subject to a reduced rank con-
straint. This leads to an efficiency-effectiveness dilemma
‚Äì A larger rank corresponds to a lower reconstruction er-
ror and vice versa. Intuitively, we associate the matrix re-
construction with that of weight perturbation, which is rel-
atively new as opposed to the feature/input perturbation ro-
bustness problem [62]. As a result, a smaller rank in our
method, from the other angle, can be seen as more weight
perturbations. To reduce the negative influence of these per-
turbed parameters, we use the l‚àû-norm for constraining the
reconstruction error,
(
‚à•ÀÜW(k)‚àíW(k)‚à•‚àû‚â§œµ,
ÀÜW(k)=U(k)(V(k))T,‚àÄk‚àà[K](5)
where œµrepresents the perturbation radius, W(k)is the orig-
inal weight matrix, and [K]denotes the weight index set.
Given œµ, preserving the neural network robustness against
weight perturbation can be cast as the following optimiza-
tion problem [62],
Lrwp=|[K]|X
k=1(‚à•ÀÜW(k)‚àíW(k)‚à•‚àû‚àíœµ). (6)
3.4. Training
The above two modules enable us to capture the compelling
discriminative capability of large pre-trained models. To
obtain a compact low-rank model, we comprehensively
consider the objectives from both the base pre-training and
our proposed two modules,
L=Lbase+Œ±Lfd+Œ≤Lrwp, (7)
where Œ±andŒ≤are loss weight hyper-parameters and Lbase
is the loss functions of the original pre-training tasks. It
can be the classification loss of a typical ViT, or vision-text
matching and masked language modeling losses of a vision-
language model. We then optimize our model on the same
datasets as the pre-trained model, such as ImageNet [8].
After this intermediate pre-training stage, our low-rank
model is smoothly deployed for downstream fine-tuningsince the model architecture is rarely altered. In contrast to
existing methods such as prompt tuning [24], adapters [51],
and LoRA [21], which require both the large pre-trained
model and the fine-tuning parameters, we only keep the
low-rank model for efficient inference and parameter usage
(see Fig. 3 for a visual comparison).
3.5. Complexity Analysis
Before analyzing the complexity of our method, we first
letdlr=1
Œ∫din√ódout
din+dout, where Œ∫is a positive number and
we name it compression ratio . We choose to use a uni-
versal compression ratio for all the matrix multiplication
operations for simplicity while leaving the exploration of
dynamic ratios for different layers as future work.
Let us consider the case where Œ∫= 2 and a single
patch feature x‚ààRdinfor downstream fine-tuning. Recall
Eqn. 1, the original matrix multiplication takes O(din√ó
dout)to operate. However, with our PELA method, this
time complexity reduces to O((din+dout)√ódlr) =
1
2O(din√ódout)1. Similarly, as the majority operation in
existing Transformers is matrix multiplication (excluding
some very few layer normalization parameters and bias pa-
rameters), the model size thus also roughly halves from
its original scale. This is why our method is significantly
different from other recent efficient approaches such as
LoRA [21], where the overall model size in fact increases.
4. Experiments
4.1. Common Efficient Learning Baselines
We evaluated our PELA against four efficient baselines:
TinyBERT [25] and MaskAlign [67] from the feature-
based knowledge distillation group; ToMe [2] - a recent
strong vision token pruning approach; and LoRA [21] ,
which is a widely used parameter-efficient transfer learning
baseline. However, we excluded some experiments due to
certain incompatibilities, such as using ToMe for the Swin
model and for the visual grounding task.
4.2. Experiments on Vision-Only Models
4.2.1 Baseline Models and Results
We applied our method to the widely used DeiT-Base [54]
and Swin-Base [39] models. To ensure comprehensive cov-
erage, we also selected DeiT-III-Large [56] which is larger
in model size and requires much longer training time. The
compression ratio is 1/2 and 1/3 for the DeiT models and
Swin, respectively. After the low-rank approximation, we
trained our model on the ImageNet-1k dataset [8] and eval-
uated it on the corresponding validation set, and report the
results in Table 1. As expected, the model parameters and
1We refer this reduced complexity only to the matrix multiplication
since we do not optimize other operations such as attention computation.
5

--- PAGE 6 ---
Table 1. Model performance of image classification on ImageNet-
1K [8] with 224x224 resolution. The parameters and FLOPs are
estimated during inference.
Method Params(M) GFLOPs Acc(%)
ViT-Base [10] 86.6 35.1 77.9
CrossViT-B [4] 105.0 40.3 82.2
T2T-ViT-24 [71] 64.1 25.5 82.3
RegNetY-16G [43] 83.6 31.9 82.9
DeiTBase [54] 86.6 33.7 81.8
TinyBert [25] 44.2 17.3 78.0
MaskAlign [67] 44.2 17.3 78.2
ToMe [2] 86.6 16.5 76.4
PELA 44.1 17.0 81.0
Swin-BaseBase [39] 87.8 30.3 83.5
TinyBert [25] 58.6 20.6 78.8
MaskAlign [67] 58.6 20.6 79.1
PELA 62.2 21.3 82.5
DeiT-III-LargeBase [56] 304.4 119.4 84.9
TinyBert [25] 156.8 61.5 79.2
MaskAlign [67] 156.8 61.5 79.5
PELA 153.2 59.8 83.9
Table 2. Model performance of semantic segmentation on the
ADE20K dataset [75] with UperNet [63].
Backbone Params(M) GFLOPs mIoU
ResNet-101 [15] 85.5 689 44.9
PatchConvNet-B60 [55] 141.0 1,258 48.1
MAE ViT-B [17] 163.9 2,343 48.1
DeiTBase [54] 121.4 320.4 45.0
LoRA [21] 124.8 331.1 40.6
TinyBert [25] 79.0 214.5 36.4
MaskAlign [67] 79.0 214.5 36.8
PELA 78.9 203.4 43.2
Swin-BaseBase [39] 121.3 798.6 47.7
LoRA [21] 124.7 822.6 44.2
TinyBert [25] 92.1 721.4 40.0
MaskAlign [67] 92.1 721.4 39.6
PELA 79.3 685.3 47.2
DeiT-III-LargeBase [56] 428.4 1,155 47.0
LoRA [21] 440.4 1,190 44.7
TinyBert [25] 280.8 784 38.1
MaskAlign [67] 280.8 784 38.4
PELA 277.2 739 45.6
FLOPs for inference are significantly reduced according to
each respective compression ratio. On the flip side, the
dropped accuracy of the two base models is 0.8% and 1.0%,
respectively. Even for the relatively larger model DeiT-III-
Large, our method only trades 1.0% accuracy with half of
the parameters and FLOPs. Moreover, our PELA surpasses
other efficient learning baselines by a notable margin.
4.2.2 Downstream Tasks and Results
After the backbones are pre-trained on the ImageNet
dataset, as per prior studies [15, 17], we further evaluated
the model performance on downstream semantic segmenta-
tion and object detection tasks.Table 3. Model performance of object detection on the MSCOCO
dataset [37] with Cascade Mask RCNN [3, 16].
Backbone Params(M) GFLOPs APbox
ResNet-50 [15] 77.3 411.0 46.3
ResNeXt-101-32 [64] 96.0 546.1 48.1
Swin-BaseBase [39] 145.0 1,501 50.1
LoRA [21] 149.0 1,547 46.1
TinyBert [25] 115.8 1,302 41.1
MaskAlign [67] 115.8 1,302 41.1
PELA 103.0 1,232 49.0
The results are presented in Table 2 and Table 3, which
illustrate the effectiveness of our method in performing
semantic segmentation and object detection tasks, respec-
tively. While our approach benefits from the reduced
memory and computation requirements, the involvement
of downstream frameworks and heads limits the extent to
which these benefits can be realized when compared to
vanilla classification. For instance, the reduced FLOPs
for Swin-Base on object detection in Table 3 are 18% as
compared to the previous 30% in Table 1. Nevertheless,
our approach still performs comparably with each respec-
tive model, demonstrating its effectiveness in balancing the
trade-off between efficiency and accuracy. Notably, our
PELA significantly outperforms LoRA in terms of both
model performance and model size.
4.3. Experiments on Vision-Language Model
4.3.1 Baseline Model and Downstream VL Tasks
Traditional visual-language pre-training approaches [6, 53]
frequently utilized pre-extracted CNN features for image
representation, often requiring precise bounding box an-
notations. In contrast, ALBEF [33] leverages ViT for vi-
sual feature extraction during pre-training and has exhib-
ited exceptional performance across a variety of VL tasks.
Therefore, we chose ALBEF as our evaluation testbed to
assess the effectiveness of our method. Furthermore, the
all-in Transformer nature of ALBEF enabled us to effort-
lessly achieve more compression. In this context, we used
1/3 of the parameters of the original ALBEF model.
We utilized four downstream vision-language tasks in
this work, including Image-Text Retrieval, SNLI-VE, VG,
and VQA. A detailed introduction to these tasks can be
found in the supplementary material. For the experiments,
we strictly followed the implementation of ALBEF except
for reducing the batch size due to resource constraints.
4.3.2 Overall Results
The results on these downstream tasks are reported in Ta-
ble 4, 5, and 6. From these tables, we have the follow-
ing three important observations. 1) The recent approach
6

--- PAGE 7 ---
Table 4. Performance comparison of text retrieval (TR) and image Retrieval (IR) on the Flickr30K and MSCOCO datasets.
Dataset Model Params TFLOPsTR IR
R@1 R@5 R@10 R@1 R@5 R@10
Flickr30KUNITER [6] 110 0.37 87.3 98.0 99.2 75.6 94.1 96.8
VILLA [11] 110 - 87.9 97.5 98.8 76.3 94.2 96.8
ALBEFBase [33] 419 7.41 93.4 99.5 99.6 80.6 95.8 98.0
LoRA [21] 431 7.49 92.1 99.2 99.0 80.2 95.6 97.7
TinyBERT [25] 230 4.66 57.6 82.8 89.9 40.8 70.6 79.4
MaskAlign [67] 230 4.66 59.0 84.2 90.9 41.1 70.4 80.5
ToMe [2] 419 2.61 74.8 92.6 96.4 62.0 86.2 91.5
PELA 173 2.58 91.6 99.3 99.6 79.7 94.8 97.5
MSCOCOUNITER [6] 110 0.37 65.7 88.6 93.8 52.9 79.9 88.0
OSCAR [35] 110 - 70.0 91.1 95.5 54.0 80.8 88.5
ALBEFBase [33] 419 7.41 72.6 91.2 95.2 54.9 80.5 88.1
LoRA [21] 431 7.49 73.2 91.7 95.9 56.5 81.3 88.9
TinyBERT [25] 230 4.66 33.6 62.1 74.8 22.6 49.8 63.2
MaskAlign [67] 230 4.66 35.7 64.9 77.3 24.2 52.5 65.5
ToMe [2] 419 2.61 56.2 82.3 90.1 41.7 71.0 81.3
PELA 173 2.58 71.6 91.0 95.3 55.1 80.8 88.3
Table 5. Model performance on visual entailment and VQA.
Params (M) and TFLOPs are counted based on the VQA model.
Model Params TFLOPsSNLI-VE VQA
val test test-dev test-std
VisualBERT [34] 134 0.37 - - 70.80 71.00
ViLT [28] 118 1.01 - - 70.94 -
LXMERT [53] 224 0.41 - - 72.42 72.54
UNITER [6] 116 0.37 78.59 78.28 72.70 72.91
12-in-1 [40] - - - 76.59 73.15 -
ALBEFBase 581 7.05 79.29 79.79 74.55 74.89
LoRA 644 7.14 79.34 79.53 71.07 -
TinyBERT 392 4.55 73.83 73.31 61.33 -
MaskAlign 392 4.55 73.74 73.48 63.85 -
ToMe 581 2.55 77.58 78.02 68.59 -
PELA 259 2.47 78.55 78.66 73.84 73.87
ALBEF [33] has demonstrated significant performance im-
provements over conventional methods like LXMERT [53]
and UNITER [6]. However, superior performance is
achieved at the expense of increased parameters and FLOPs,
mainly due to the usage of a cumbersome trainable ViT for
image processing. In comparison to the baselines, which
use a universal Transformer for both vision and language,
such as UNITER [6], ALBEF offers superior visual features
but introduces a larger model size and computational com-
plexity. 2) Our PELA method helps alleviate this problem
through the low-rank approximation. As can be observed,
PELA is able to achieve comparable performance to AL-
BEF while using only 1/3 of the parameters and FLOPs.
This translates to a significant reduction in model size and
computation, with most performance degradation limited
to just one point. 3) Regarding the comparison with effi-
cient learning baselines, our PELA approach consistently
achieves better performance in most cases. The only ex-Table 6. Model performance on the challenging weakly-
supervised visual grounding task.
Model Val TestA TestB
ARN [38] 32.78 34.35 32.13
CCL [74] 34.29 36.91 33.56
ALBEFBase 57.94 65.07 45.75
PELA 57.06 65.85 45.10
ception is for retrieval tasks, where PELA exhibits slightly
inferior model performance compared to LoRA. However,
it is important to note that LoRA requires a larger number
of model parameters and FLOPs.
4.4. Ablation Study
Effectiveness of the two modules. We first studied the
model performance of direct decomposition of pre-trained
weights using low-rank approximation. However, as indi-
cated in Table 7, this approach results in a significant drop
in performance, possibly because of the shift in feature dis-
tribution. We then added our proposed two modules to the
low-rank model and observed performance improvements.
By combining the two modules together, our model can of-
ten outperform other variants, demonstrating the effective-
ness of the proposed method.
Performance variation w.r.t. compression ratio. Training
large models often involves a trade-off between effective-
ness and efficiency. To demonstrate this, we trained our
model using different compression ratios with fewer epochs
to simplify the process and present the results in Fig. 4.
This graph indicates that a smaller compression ratio, i.e., a
larger model, typically yields better performance. However,
a model that is too small, such as one that is compressed to
7

--- PAGE 8 ---
Table 7. Ablation studies of the proposed method over five tasks. For the downstream tasks of ALBEF, we selected representative evaluation
metrics for space concerns.
Model LfdLrwpDeiT Swin ALBEF
Cls Seg Cls Seg Retrieval SNLI-VE VG
Acc mIoU Acc mIoU TR@1 IR@1 val test TestA TestB
Baseline 81.80 44.99 83.50 47.68 72.64 54.91 79.29 79.79 65.07 45.75
PELA‚úó ‚úó 61.08 24.42 77.60 28.80 65.94 49.45 76.10 76.21 61.36 41.36
‚úì ‚úó 80.90 43.67 82.89 47.28 70.66 54.55 78.35 78.07 65.70 44.90
‚úó ‚úì 80.55 42.94 82.86 47.24 71.44 54.43 78.50 78.39 66.24 44.57
‚úì ‚úì 80.96 43.24 82.54 47.21 71.26 54.75 78.55 78.66 65.86 45.10
2 4 6 8 1055606570758085Acc (%)
DeiT Classification
2 4 6 8 104050607080Acc (%)
Swin Classification
2 4 6 8 10767778Acc (%)
SNLI-VE
2 4 6 8 1065707580IR@1 (%)
Retrieval@Flickr
2 4 6 8 1040455055IR@1 (%)
Retrieval@MSCOCO
2 4 6 8 10565860626466TestA Acc (%)
Visual Grounding
Figure 4. Model performance change w.r.t. compression ratios.
Table 8. Efficiency comparison of two pre-training strategies.
Method Batch Size GPU Mem ‚Üì Latency ‚Üì
DeiT-Base64√ó412.77 GB 76.24 ms/img
DeiT-Base pela 11.34 GB 70.06 ms/img
1/10 of its original size, may not be capable of achieving
satisfactory results.
4.5. Pre-training Efficiency & Model Scaling
Pre-training Efficiency. One may be concerned about the
efficiency issues during pre-training. To address this prob-
lem, we leveraged the DeiT-Base model and evaluated its
pre-training efficiency metrics, and show the results in Ta-
ble 8. In particular, we employed the plain low-rank model
because it already delivers promising model performance.
Though other models may trigger longer training time, un-
der this context, as shown in the table, our PELA method
outperforms the original model in terms of both GPU mem-
ory cost and training latency.
Downstream Model Scaling. Our approach spawns a more
compact model compared to the original large pre-trained
one, resulting in a surplus of memory that enables us to train
downstream models with larger batch sizes. To demonstrate
the effectiveness of our method, we increased the batch size
for both DeiT-Base and Swin-Base on the semantic segmen-
tation task2, as shown in Table 9. Our experiments show
promising results, with a significant improvement in model
performance for both models, achieving an absolute mIoU
2We kept the GPU memory less than the original large baseline model
for a fair comparison.Table 9. Model scaling performance of DeiT-Base and Swin-Base
on semantic segmentation. PELA+ denotes the PELA model with
a larger batch size while maintaining similar GPU memory.
Method Batch Size Baseline PELA PELA+
DeiT-Base 16‚Üí20 44.99 43.24 43.81 +.57
Swin-Base 16‚Üí20 47.68 47.21 47.99 +.78
improvement of 0.57% and 0.78%, respectively. Moreover,
our proposed method also outperforms the original Swin-
Base baseline using PELA+, highlighting another advan-
tage of our proposed approach.
5. Conclusion and Future Work
In this work, we propose a simple yet effective parameter-
efficient pre-training approach that employs low-rank ap-
proximation as the core. Even with its simplicity, our
method achieves competitive performance with baselines
while attaining significantly improved parameter and com-
putational efficiencies. These advantages enable model
scaling in terms of model depth, width, and training batch
size of downstream task fine-tuning. This work highlights
the potential benefits of tackling the over-parameterization
problem of learnable weights. In addition to this, we believe
that the compression of intermediate features is a promis-
ing orthogonal direction for reducing model complexity.
Therefore, we plan to investigate feature compression tech-
niques, such as vision token pruning, to further build a more
lightweight model in future research.
8

--- PAGE 9 ---
References
[1] Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund
Kalibhat, Mucong Ding, Dominik St ¬®oger, Mahdi
Soltanolkotabi, and Soheil Feizi. Understanding over-
parameterization in generative adversarial networks. In
ICLR , 2021. 2, 3
[2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token
merging: Your vit but faster. In ICLR , 2023. 5, 6, 7
[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving
into high quality object detection. In CVPR , pages 6154‚Äì
6162. IEEE, 2018. 6
[4] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.
Crossvit: Cross-attention multi-scale vision transformer for
image classification. In ICCV , pages 357‚Äì366. IEEE, 2021.
6
[5] Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-
Jui Hsieh. Groupreduce: Block-wise low-rank approxima-
tion for neural language model shrinking. In NeurIPS , pages
11011‚Äì11021, 2018. 3
[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
UNITER: universal image-text representation learning. In
ECCV , pages 104‚Äì120. Springer, 2020. 6, 7
[7] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. In NIPS , pages 3123‚Äì
3131, 2015. 1, 2
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , pages 248‚Äì255. IEEE, 2009. 5, 6
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In NAACL , pages 4171‚Äì
4186. ACL, 2019. 1, 2
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR . OpenReview.net, 2021. 1, 3, 6
[11] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,
and Jingjing Liu. Large-scale adversarial training for vision-
and-language representation learning. In NeurIPS , 2020. 7
[12] Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, and Jianguo
Li. Network decoupling: From regular to depthwise separa-
ble convolutions. In BMVC , page 248. BMV A Press, 2018.
3
[13] Yangyang Guo, Zhiyong Cheng, Jiazheng Jing, Yanpeng
Lin, Liqiang Nie, and Meng Wang. Enhancing factoriza-
tion machines with generalized metric learning. TKDE , 34
(8):3740‚Äì3753, 2022. 3
[14] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified view
of parameter-efficient transfer learning. In ICLR , 2022. 3
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In CVPR ,
pages 770‚Äì778. IEEE, 2016. 6
[16] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-
shick. Mask r-cnn. In ICCV , pages 2961‚Äì2969. IEEE, 2017.
6
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll¬¥ar, and Ross B. Girshick. Masked autoencoders are scal-
able vision learners. In CVPR , pages 15979‚Äì15988. IEEE,
2022. 3, 6
[18] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia
Hu, and Tat-Seng Chua. Neural collaborative filtering. In
WWW , pages 173‚Äì182. ACM, 2017. 3
[19] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, No-
jun Kwak, and Jin Young Choi. A comprehensive overhaul
of feature distillation. In ICCV , pages 1921‚Äì1930. IEEE,
2019. 1, 2, 4
[20] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
Distilling the knowledge in a neural network. CoRR ,
abs/1503.02531, 2015. 1, 2
[21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In
ICLR . OpenReview.net, 2022. 2, 3, 5, 6, 7
[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio. Quantized neural networks:
Training neural networks with low precision weights and ac-
tivations. JMLR , 18:187:1‚Äì187:30, 2017. 1, 2
[23] Yerlan Idelbayev and Miguel ¬¥A. Carreira-Perpi Àún¬¥an. Low-
rank compression of neural nets: Learning the rank of each
layer. In CVPR , pages 8046‚Äì8056. IEEE, 2020. 3
[24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim.
Visual prompt tuning. In ECCV , pages 709‚Äì727. Springer,
2022. 1, 2, 3, 5
[25] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distill-
ing BERT for natural language understanding. In Findings
of EMNLP , pages 4163‚Äì4174. ACL, 2020. 2, 5, 6, 7
[26] Roberto J. Bayardo Jr. Efficiently mining long patterns from
databases. In SIGMOD , pages 85‚Äì93. ACM, 1998. 2
[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for
neural language models. CoRR , 2020. 1
[28] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
and-language transformer without convolution or region su-
pervision. In ICML , pages 5583‚Äì5594. PMLR, 2021. 7
[29] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei
Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao
Tang, Minghai Qin, and Yanzhi Wang. Spvit: Enabling faster
vision transformers via latency-aware soft token pruning. In
ECCV , pages 620‚Äì640. Springer, 2022. 2
[30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite
BERT for self-supervised learning of language representa-
tions. In ICLR . OpenReview.net, 2020. 2, 3
9

--- PAGE 10 ---
[31] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas M. Breuel,
Jan Kautz, and Yale Song. Parameter efficient multimodal
transformers for video representation learning. In ICLR ,
2021. 3
[32] Chong Li and C.-J. Richard Shi. Constrained optimization
based low-rank approximation of deep neural networks. In
ECCV , pages 746‚Äì761. Springer, 2018. 3
[33] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare,
Shafiq R. Joty, Caiming Xiong, and Steven Chu-Hong Hoi.
Align before fuse: Vision and language representation learn-
ing with momentum distillation. In NeurIPS , pages 9694‚Äì
9705, 2021. 2, 6, 7
[34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. Visualbert: A simple and performant
baseline for vision and language. CoRR , abs/1908.03557,
2019. 7
[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics
aligned pre-training for vision-language tasks. In ECCV ,
pages 121‚Äì137. Springer, 2020. 7
[36] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi
Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: push-
ing the limit of post-training quantization by block recon-
struction. In ICLR . OpenReview.net, 2021. 2
[37] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV , pages 740‚Äì755. Springer, 2014. 6
[38] Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha,
Dechao Meng, and Qingming Huang. Adaptive recon-
struction network for weakly supervised referring expression
grounding. In ICCV , pages 2611‚Äì2620. IEEE, 2019. 7
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , pages 9992‚Äì10002. IEEE, 2021. 2, 5, 6
[40] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi
Parikh, and Stefan Lee. 12-in-1: Multi-task vision and
language representation learning. In CVPR , pages 10434‚Äì
10443. IEEE, 2020. 7
[41] Gaurav Menghani. Efficient deep learning: A survey on
making deep learning models smaller, faster, and better.
CoRR , abs/2106.08962, 2021. 2
[42] Dimitris S. Papailiopoulos, Alexandros G. Dimakis, and
Stavros Korokythakis. Sparse PCA through low-rank ap-
proximations. In ICML , pages 747‚Äì755. JMLR.org, 2013.
3
[43] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr Doll ¬¥ar. Designing network design
spaces. In CVPR , pages 10428‚Äì10436. IEEE, 2020. 6
[44] Babak Rokh, Ali Azarpeyvand, and Alireza Khanteymoori.
A comprehensive survey on model quantization for deep
neural networks. CoRR , 2022. 2
[45] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. In ICLR , 2015. 2, 4[46] Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. Distilbert, a distilled version of BERT:
smaller, faster, cheaper and lighter. CoRR , abs/1910.01108,
2019. 2
[47] M. Schuermans, Philippe Lemmerling, and Sabine Van Huf-
fel. Structured weighted low rank approximation. Numerical
Linear Algebra with Applications , 11(5-6):609‚Äì618, 2004. 2
[48] Nathan Srebro and Tommi S. Jaakkola. Weighted low-rank
approximations. In ICML , pages 720‚Äì727. AAAI Press,
2003. 2
[49] Trevor Strohman and W. Bruce Croft. Efficient document
retrieval in main memory. In SIGIR , pages 175‚Äì182. ACM,
2007. 2
[50] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient
knowledge distillation for BERT model compression. In
EMNLP , pages 4322‚Äì4331. ACL, 2019. 1, 2, 5
[51] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. VL-
ADAPTER: parameter-efficient transfer learning for vision-
and-language tasks. In CVPR , pages 5217‚Äì5227. IEEE,
2022. 1, 2, 3, 5
[52] Cheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E. Con-
volutional neural networks with low-rank regularization. In
ICLR , 2016. 3
[53] Hao Tan and Mohit Bansal. LXMERT: learning cross-
modality encoder representations from transformers. In
EMNLP , pages 5099‚Äì5110. ACL, 2019. 6, 7
[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training
data-efficient image transformers & distillation through at-
tention. In ICML , pages 10347‚Äì10357. PMLR, 2021. 2, 5,
6
[55] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr
Bojanowski, Armand Joulin, Gabriel Synnaeve, and Herv ¬¥e
J¬¥egou. Augmenting convolutional networks with attention-
based aggregation. arXiv preprint arXiv:2112.13692 , 2021.
6
[56] Hugo Touvron, Matthieu Cord, and Herv ¬¥e J¬¥egou. Deit iii:
Revenge of the vit. In ECCV , pages 516‚Äì533. Springer,
2022. 2, 5, 6
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS , pages 5998‚Äì
6008, 2017. 2
[58] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, and Yun Fu.
Recent advances on neural network pruning at initialization.
InIJCAI , pages 5638‚Äì5645. ijcai.org, 2022. 2
[59] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. OFA: unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. In ICML , pages 23318‚Äì23340. PMLR, 2022. 1
[60] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
CoRR , abs/2006.04768, 2020. 3
[61] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-
ret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tat-
sunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,
10

--- PAGE 11 ---
and William Fedus. Emergent abilities of large language
models. CoRR , 2022. 1
[62] Tsui-Wei Weng, Pu Zhao, Sijia Liu, Pin-Yu Chen, Xue Lin,
and Luca Daniel. Towards certificated model robustness
against weight perturbations. In AAAI , pages 6356‚Äì6363.
AAAI Press, 2020. 5
[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In ECCV , pages 418‚Äì434. Springer, 2018. 6
[64] Saining Xie, Ross Girshick, Piotr Doll ¬¥ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR , pages 1492‚Äì1500. IEEE, 2017. 6
[65] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao
Chang, Songfang Huang, and Fei Huang. Raise a child in
large language model: Towards effective and generalizable
fine-tuning. In EMNLP , pages 9514‚Äì9528. ACL, 2021. 2
[66] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang,
Yingyong Qi, Yiran Chen, Weiyao Lin, and Hongkai Xiong.
TRP: trained rank pruning for efficient deep neural networks.
InIJCAI , pages 977‚Äì983. ijcai.org, 2020. 3
[67] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun,
Houqiang Li, and Jiebo Luo. Stare at what you see: Masked
image modeling without reconstruction. In CVPR , pages
22732‚Äì22741. IEEE, 2023. 5, 6, 7
[68] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel
Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep
neural networks via singular vector orthogonality regulariza-
tion and singular value sparsification. In CVPR Workshops ,
pages 2899‚Äì2908. IEEE, 2020. 3
[69] Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan,
and Yu Li. Vitkd: Practical guidelines for vit feature knowl-
edge distillation. CoRR , abs/2209.02432, 2022. 2
[70] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.
On compressing deep models by low rank and sparse decom-
position. In CVPR , pages 67‚Äì76. IEEE, 2017. 3
[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In ICCV , pages 558‚Äì567. IEEE, 2021.
6
[72] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit:
Simple parameter-efficient fine-tuning for transformer-based
masked language-models. In ACL, pages 1‚Äì9. ACL, 2022. 2
[73] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classifi-
cation and detection. TPAMI , 38(10):1943‚Äì1955, 2016. 3
[74] Zhu Zhang, Zhou Zhao, Zhijie Lin, Jieming Zhu, and Xi-
uqiang He. Counterfactual contrastive learning for weakly-
supervised vision-language grounding. In NeurIPS , 2020. 7
[75] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-
dler, Adela Barriuso, and Antonio Torralba. Semantic under-
standing of scenes through the ade20k dataset. IJCV , 127:
302‚Äì321, 2019. 6
11
