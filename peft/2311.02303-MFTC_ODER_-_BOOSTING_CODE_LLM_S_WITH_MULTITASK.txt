# 2311.02303.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2311.02303.pdf
# File size: 1084615 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MFTC ODER : BOOSTING CODE LLM S WITH MULTITASK
FINE-TUNING
A P REPRINT
Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang,
Dajun Chen ,Min Shen ,Hailian Zhou ,Wei Jiang ,Hang Yu∗,Jianguo Li∗
Ant Group, China
November 7, 2023
ABSTRACT
Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to
enhancing model’s coding capabilities through fine-tuning on pre-trained models. Previous fine-
tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant
separate fine-tuning for each task, requiring extensive training resources and posing challenges in
terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent
interconnectedness among different code-related tasks. To overcome these limitations, we present
a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning
on multiple tasks. By incorporating various loss functions, we effectively address common chal-
lenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent
convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task
fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on
a mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including
efficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed
compared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several main-
stream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation,
our MFTCoder fine-tuned model, CODEFUSE-CODELLAMA -34B , achieves an impressive pass@1
score of 74.4% on the HumaneEval benchmark, surpassing GPT-4 performance (67%, zero-shot).
MFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder
Keywords Large Language Model ·Code Generation ·Multi-task Learning
1 Introduction
The paradigm-shifting emergence of ChatGPT2, powered by both GPT-3.5 and GPT-4 OpenAI (2023), has set ablaze
the landscape of research and development in the realm of large language models (LLMs). This breakthrough has
further sparked the interest in leveraging LLMs for code understanding and generation, commonly referred to as Code
LLMs. By pretraining on extensive code data sources such as the Github public data, these Code LLMs can acquire
comprehensive contextual representations that can be applied to various code-related tasks .
While the pretraining stage of (Code) LLMs seek to ensure their generalizability to different downstream tasks, the
subsequent finetuning stage typically only adapt the (Code) LLMs to a specific task or a scenario. However, this
approach overlooks two critical challenges. Firstly, it involves resource-intensive individual finetuning of large
LLMs for each task, which hinders efficient deployment in production. Secondly, the interrelated nature of code
domain tasks suggests that joint finetuning can enhance performance compared to separate finetuning. It is
therefore imperative to conduct multitask finetuning, enabling simultaneous handling of all tasks while leveraging the
strengths of related tasks to enhance performance.
∗Corresponding Author: {hyu.hugo, lijg.zero}@antgroup.com
2https://openai.com/blog/chatgptarXiv:2311.02303v1  [cs.LG]  4 Nov 2023

--- PAGE 2 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
As an illuminating example, suppose we have two related tasks: code completion and code summarization. Code
completion involves predicting the next line of code based on a partial code snippet, while code summarization aims
to generate a concise human-readable summary of a given code snippet. Traditionally, separate models would be
fine-tuned for each task, resulting in resource-intensive duplication. However, code completion and code summarization
have inherent connections. Completion of a code snippet relies on understanding the overall functionality and purpose,
while generating an accurate summary requires comprehending the structure, dependencies, and intended functionality.
By employing multitask learning, a single model can be trained to jointly learn both tasks, leveraging shared knowledge
and patterns, leading to improved performance on both tasks. The model understands the contextual dependencies
between code elements, aiding in predicting the next snippet and generating informative summaries. Furthermore,
multitask learning offers additional benefits beyond individual task performance: the shared representation between
tasks helps mitigate overfitting, promote better generalization, and enhance the model’s ability to handle data scarcity
for specific tasks. If code completion has a larger training dataset than code summarization, the model can leverage the
abundance of completion data to enhance performance in summarization, effectively addressing data scarcity challenges.
Multitask learning even enables the model to handle unseen but related tasks without specific training data. Overall,
multitask learning allows models to jointly learn multiple related tasks, benefiting from shared knowledge, improving
performance, enhancing generalization, and handling data scarcity.
Despite the importance of multitask learning for finetuning, only a handful of existing studies have explored this approach
in the domain of NLP Raffel et al. (2023); Aghajanyan et al. (2021); Aribandi et al. (2022). These studies incorporate
multi-task data and merge it for large-scale model learning, without explicitly separating the tasks. Unfortunately, these
studies tend to prioritize tasks with larger sample sizes, disregarding tasks with smaller sample sizes. Furthermore, they
fail to ensure equal convergence speed among tasks, leading to over-optimization of some tasks and under-optimization
of others.
In this paper, we focus on multitask fine-tuing (MFT) of (Code) LLMs, in order to guarantee equitable attention to
tasks with varying sample sizes and approximately similar optimization progress. In particular, our attention is on
Code LLMs, as code domain tasks often exhibit correlations, and so we name our approach MFTCoder. We emphasize
that MFTcoder can be extended to an arbitrary set of related-NLP tasks in a straighforward manner. To enhance
the efficiency of MFTCoder, we incorporate parameter-efficient fine-tuning techniques, including LoRA Hu et al .
(2021) and QLoRA Dettmers et al .(2023). Experimental results demonstrate that multi-task models trained using the
MFT approach outperform those fine-tuned individually for each task or by merging data from multiple tasks. We
further validate the effectiveness of MFTCoder on various baseline pretrained LLMs, such as Qwen Bai et al .(2023),
Baichuan Baichuan (2023), Llama Touvron et al .(2023a), Llama 2 Touvron et al .(2023b), StarCoder Li et al .(2023a),
CodeLLama Rozière et al .(2023), and CodeGeex2 Zheng et al .(2023). Remarkably, when applying MFTCoder to
the CodeLlama-34B-Python Rozière et al .(2023) base model, it achieves a pass@1 score of 74.4% on the humanEval
evaluation dataset, even surpassing the performance of GPT-4 (67%, zero-shot) OpenAI (2023).
The main contributions of this paper can be summarized as follows:
•We propose MFTCoder, a novel multitask finetuning approach for concurrently adapting LLMs to multiple code-
related tasks. Our focus is on addressing the issues of data balance and convergence speed that commonly arise in
previous multitask finetuning methods.
•We validate MFTCoder on various baseline pretrained models, including Qwen Bai et al .(2023), Baichuan Baichuan
(2023), Llama Touvron et al .(2023a), Llama 2 Touvron et al .(2023b), StarCoder Li et al .(2023a), CodeLLama Rozière
et al.(2023), CodeFuse Di et al .(2023), and CodeGeex2 Zheng et al .(2023), demonstrating its compatibility with
different baseline models.
•Extensive experiments show that the MFT approach outperforms individual fine-tuning for each task or data merging
from multiple tasks. Notably, when implementing MFTCoder with the CodeLlama-34B-Python Rozière et al .(2023)
base model, it achieves an impressive pass@1 score of 74.4% on the humanEval evaluation dataset, surpassing the
performance of GPT-4 (67%, zero-shot) OpenAI (2023).
2 Related Works
2.1 Code LLMs
Coding capability serves as a critical criterion for evaluating general large language models (LLMs) in code-related
tasks. Notable performance on the widely-used HumanEval dataset Chen et al .(2021), a benchmark for code generation,
has been observed across various models, including LaMDA Thoppilan et al .(2022), PaLM Chowdhery et al .(2022),
PaLM 2 Anil et al .(2023), ChatGPT, and GPT-4 OpenAI (2023). In particular, GPT-4 has set a remarkable record of
67.0% pass@1 score. However, their closed-source nature limits their availability and hinders further collaborative
2

--- PAGE 3 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Figure 1: Overview of MFTCoder framework.
advancements. In contrast, recent open-source LLMs, including LLaMA Touvron et al .(2023a), LLaMA 2 Touvron et al .
(2023b), Qwen Bai et al .(2023), and Phi-1.5 Li et al .(2023b), have demonstrated notable progress in code-related tasks,
with commentable scores of 23.7%, 29.9%, 32.3%, and 41.4% respectively. Despite this progress, their performance
still lags behind the state-of-the-art closed-source models.
On the other hand, LLMs specifically designed for code-related tasks, often referred to as code LLMs, have also
undergone significant developments. Alongside closed-source Code LLMs such as Codex Chen et al .(2021),
Code-Davinci Chen et al .(2021), AlphaCode Li et al .(2022), PaLM-Coder Chowdhery et al .(2022), and PanGu-
Coder Christopoulou et al .(2022), open-source alternatives like including SantaCoder Allal et al .(2023), Phi-1.0 Gu-
nasekar et al .(2023), CodeGeeX-2 Zheng et al .(2023), StarCoder Li et al .(2023a), Code LLaMA Rozière et al .
(2023) have showcased competitive performance with their closed-source counterparts. Notably, CodeLLama-34B-
Python Rozière et al .(2023) obtains a score of 53.7% on HumanEval. Apart from pretraining, another intriguing
approach to further enhancing Code LLMs is instruction fine-tuning, as showcased by CodeT5+ Wang et al .(2023),
Phi-1.0 Gunasekar et al .(2023), OctoPack Muennighoff et al .(2023), and WizardCoder Luo et al .(2023). By leveraging
carefully curated high-quality instruction datasets, these methods exhibit the potential of fine-tuning to enhance code
generation capabilities.
2.2 Multitask Learning
Multitask learning (MTL) Caruana (1997); Crawshaw (2020) is a potent approach in machine learning that holds
significant promise for enhancing model performance and addressing diverse challenges Crawshaw (2020). By training
a single model on multiple related tasks, MTL enables the model to leverage shared knowledge and patterns, leading to
enhanced generalization and improved accuracy. MTL methods can be categorized into two groups: hard parameter
sharing Zhao et al .(2018); Liu et al .(2019b,a); Kendall et al .(2018); Liu et al .(2019c); Chen et al .(2018); Jean
et al.(2019) and soft parameter sharing Duong et al .(2015); Yang and Hospedales (2017); Long et al .(2017); Lee
et al.(2018); Sun et al .(2020); Pascal et al .(2021). Hard parameter sharing involves sharing model weights between
tasks, while soft parameter sharing incorporates task-specific models with separate weights. In the context of large
language models (LLMs), hard parameter sharing is particularly relevant, since the large number of parameters in LLMs
facilitates their ability to handle multiple related tasks with a common set of parameters. As a result, optimizing an
LLM to effectively tackle multiple tasks lies at the heart of MTL for LLMs. In recent years, notable advancements have
been made in MTL techniques. Google introduced T5 Raffel et al .(2023) in 2020, where they explored the application
of MTL techniques. Meta further introduced Mupper Aghajanyan et al .(2021) in 2021, which applies multi-task
learning between pretraining and fine-tuning, termed as pre-fine-tuning (PFT). They discovered that incorporating this
step enhances the performance of the pretrained model across various downstream tasks and significantly improves
the speed of fine-tuning. However, if the number of tasks in PFT is too small, it can have a negative impact on the
model’s performance. Therefore, it is recommended to have a minimum of 15 tasks for optimal results. Building upon
T5, Google introduced ExT5 Aribandi et al .(2022), which increased the number of tasks to 107. They found that as
long as the number of tasks in pretraining is sufficiently large, even if there may be mutual interference among tasks,
the ultimate results are still remarkably good. Ultimately, ExT5 outperformed T5 across multiple metrics. It is worth
noting that these studies mainly focused on incorporating multi-task data and merging it for the large model to learn,
without explicitly segregating the tasks. While these approaches have shown promising results, they tend to overlook
data imbalance and convergence speed issues that often arise in MTL. In this paper, we address these challenges and
propose MFTCoder, a multitask finetuning approach for LLMs that tackles these problems effectively.
3

--- PAGE 4 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Figure 2: Data Generation Approach for Code Exercises Datasets using Single-turn Conversation Scheme.
3 Approach
In this section, we will introduce our multi-task fine-tuning framework, MFTCoder3, along with the design of its key
components.
3.1 MFT Framework
MFTCoder aims to seamlessly adapt LLMs to diverse new scenarios while maximizing their performance within a
specific context. When applying MFTCoder to a new scenario, the initial step involves decomposing the scenario into
smaller tasks that correspond to targeted abilities. For instance, in the domain of code LLMs, the overarching objective
of enhancing models’ code capabilities can be further divided into specific tasks like code completion, text-to-code
generation, unit test case generation, code repair, code debugging, and even cross-language translation. Our extensive
practical experience has demonstrated that MFTCoder effectively handles multi-task scales ranging from single to
dozens or even hundreds of tasks. Each task necessitates the collection and organization of fine-tuning datasets. However,
data collection for certain tasks can pose challenges. To overcome this, MFTCoder leverages Self-Instruct Wang
et al.(2022) techniques and Agents to generate instruction datasets. With the capability to concurrently fine-tune
multiple downstream tasks, MFTCoder effectively handles substantial volumes of fine-tuning data, ensuring efficient
training. It incorporates two efficient data tokenization modes and implements PEFT (Parameter-Efficient Fine-Tuning)
techniques to enhance training efficiency. In the realm of multi-task learning, MFTCoder confronts the issue of task
imbalances, encompassing imbalanced data distribution, varying task difficulties, and divergent convergence rates. To
mitigate these challenges, MFTCoder introduces or adapts different loss functions to achieve task balance. Recognizing
that different large-scale models possess distinct strengths and capabilities, MFTCoder facilitates the selection of
suitable model architectures based on specific scenarios to achieve optimal performance. It has been adapted to popular
LLMs, including LLama Touvron et al .(2023a), LLama 2 Touvron et al .(2023b), CodeLLama Rozière et al .(2023),
Qwen Bai et al .(2023), Baichuan 1/2 Baichuan (2023), ChatGLM 2 Du et al .(2022), CodeGeeX 2 Zheng et al .(2023),
GPT-NEOX Black et al .(2022), CodeFuse Di et al .(2023), StarCoder Li et al .(2023a), AntLLM, and more. We
continuously update and expand the compatibility with additional models.
The overall framework of MFTCoder is illustrated in Figure 1. In the subsequent sections, we will provide a more
detailed exploration of these components, including instruction datasets construction, efficient tokenization modes,
PEFT fine-tuning and balanced loss functions.
3.2 Instruction Dataset Construction
For tasks with challenging data collection, We employ the Self-Instruct Wang et al .(2022) technique to generate
fine-tuning data for downstream code-related tasks in MFTCoder. This involves providing customized prompts to
GPT-3.5 or GPT-4 that clearly describe our instruction generation requirements, thereby generating instructional data.
Furthermore, we drew inspiration from the Textbook approach employed in the PHI work Gunasekar et al .(2023),
incorporating the self-instruct technique to generate Code Exercises datasets for downstream code-related tasks.
In terms of specific implementation, we have two options. One is the Agents multi-turn conversation approach achieved
through Camel Li et al .(2023c), and the other is the single-turn conversation method by directly invoking the ChatGPT
3https://github.com/codefuse-ai/MFTCoder
4

--- PAGE 5 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
API. In our multi-turn approach, we employ Camel to launch two agents, each assigned specific roles and task themes,
facilitating a dialogue between them to generate instructional data aligned with the given theme. For instance, when
generating Python exercise data, we designate the roles of ’teacher’ (simulating the user role of ChatGPT) and ’student’
(simulating the assistant role of ChatGPT) for the agents. The teacher’s responsibility is to provide exercise instructions
to the student, while the student’s task is to offer corresponding solutions to those instructions. This iterative process
continues, generating multiple exercise questions, until the task requirements are met or the maximum input length of
ChatGPT is reached. To accommodate ChatGPT’s input length limitation, we cannot directly utilize a large question as
the task theme. For instance, when creating Python exercise questions to assess students’ proficiency, we break down
the main theme into smaller Python knowledge points ( e.g.binary search tree) and initiate separate Camel sessions for
each knowledge point. For a concrete example, please refer to Appendix A.
The multi-turn approach provides high automation but can be costly due to the need for maintaining two agents, each
making multi-turn calls to the ChatGPT API. To mitigate this, we propose a more cost-effective single-turn conversation
generation approach, and the overall process is illustrated in Figure 2. We begin by creating an initial set of seeds, such
as hundreds of Python knowledge points. These seeds are then combined with prepared fixed prompt templates to
generate a set of patterned task prompts. To address the issue of reduced diversity caused by fixed templates and to
ensure accurate prompt descriptions, we utilize Camel’s task prompt refinement feature to obtain precise and diverse
task prompts. Each task prompt is used to generate a set of instructions related to the corresponding seed ( e.g.exercise
problems related to binary search trees). Using ChatGPT, we generate the corresponding solutions for the generated
instructions. Finally, we assemble and deduplicate the instructions with their respective solutions to obtain an exercise
dataset. We have open-sourced a Python Code Exercises dataset4constructed using this approach.
3.3 Efficient Tokenization Modes
Tokenization is an essential step in the pre-training and fine-tuning of LLM models, where input and output texts are
split into smaller units to be processed. It, along with the loss function, effectively defines how the data is utilized
during the training process, thus playing a crucial role in both the model’s effectiveness and training efficiency. In the
typical SFT tokenization scheme, samples within the same batch are aligned to the maximum input length (seq-length)
of the model with extra padding tokens, shown as Figure 3a. However, in practice, we have found that this approach
results in a high proportion of padding tokens. For example, when using the CodeFuse-13B Di et al .(2023) tokenizer to
process 35 downstream tasks, the average proportion of padding tokens is 92.22% (with seq-length set to 4096). This
means a significant number of tokens are used solely for alignment purposes, providing no value to the training process.
This results in lower training efficiency and wastage of offline tokenization storage space. To address this issue, we
have adopted and optimized two tokenization modes, namely dynamic padding andpack modes.
In dynamic padding mode, the micro batch window size of each GPU is determined by the maximum sample length in
the micro batch. Shorter samples are padded with additional padding tokens to match this size, as shown in Figure 3b.
Although padding tokens do not affect the model’s training effectiveness, they add computational overhead during
training, impacting the training speed. Dynamic padding mode effectively reduces the proportion of padding tokens
used, leading to faster training. In our experience, this approach can yield approximately a twofold speed improvement
compared to the traditional SFT tokenization mode (actual improvement depends on the dataset). It’s important to note
that this mode is suitable for online tokenization scenarios only.
While the dynamic padding mode reduces the micro batch window size, the pack mode, similar to Llama 2’s SFT
tokenization mode Touvron et al .(2023b), maximizes the utilization of the model’s maximum input window length
(seq-length). In the pack mode, multiple fine-tuning samples are sequentially packed into a window of seq-length,
separated by eos tokens, as shown in Figure 3c. In the figure, samples 1-4 of Figure 3a are combined and placed
in one window after one another. If a sample cannot fit in the current window, it is placed in the next window with
padding tokens filling the remaining space. For instance, in Figure 3c, sample 5 is placed in the second window with
padding tokens, while sample 6 is accommodated in the third window. The pack mode, in comparison to the dynamic
padding mode, offers even lower padding token ratio, resulting in improved training speed. Our practical experience
demonstrates that this approach reduces the average proportion of padding tokens to less than 10% among the 35
tasks mentioned earlier, leading to a substantial boost in training speed while maintaining training effectiveness. It is
important to highlight that MFTCoder supports both online and offline pack tokenization scenarios, serving not only the
SFT phase but also the pre-training stages.
4https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k
5

--- PAGE 6 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
(a) Normal SFT Mode
(b) Dynamic Padding Mode
 (c) Pack SFT Mode
Figure 3: Illustration of the differences in sample organization within a batch between normal SFT, dynmaic padding and
Pack SFT tokenization modes. The light-colored squares in the figure represent the Prompt section of the samples, while
the dark-colored squares represent the Label section (participating in loss calculation). The blank squares represent
padding section.
3.4 PEFT Efficient Fine-tuning
The prevalent large-scale models typically contain billions of parameters, while multi-task learning scenarios often
involve numerous tasks, resulting in a substantial total number of fine-tuning samples. If we were to opt for full-fledged
fine-tuning of these large models using a vast amount of data, two challenges would arise: firstly, the need for extensive
storage and computational resources; secondly, the potential risk of catastrophic forgetting during training. To address
these issues, MFTCoder incorporates the PEFT (Parameter-efficient fine-tuning) technique Houlsby et al .(2019),
enabling efficient fine-tuning to be accomplished within a short timeframe and with minimal resource requirements.
Specifically, MFTCoder supports two PEFT methods: Lora (Large-scale Language Model Low-Rank Adaptation) Hu
et al.(2021) and QLora (Quantized Large-scale Language Model Low-Rank Adaptation) Dettmers et al .(2023). The
fundamental concept of Lora is quite simple, as depicted in Figure 4. It involves adding an auxiliary branch to the
original pretrained language model. During training, the parameters W∈Rd×dof the original pretrained model remain
fixed, while only the dimensional expansion matrix A∈Rd×rand dimensional reduction matrix B∈Rr×dwithin the
auxiliary branch are trained. The matrix product BAis then added to the original model W, resulting in the newly
trained model. Due to the significantly smaller magnitude of r compared to d, the number of trainable parameters can
be dramatically reduced. Building upon LoRA, QLoRA incorporates a novel high-precision quantization technique
called NF4 and dual quantization to quantize the pretrained model to 4 bits. It also introduces a small set of learnable
low-rank adapter weights. These weights are fine-tuned by optimizing the gradients through back-propagation of the
quantized weights. As a result, QLoRA enables the fine-tuning of larger models using fewer GPU resources. As an
example, MFTCoder can fine-tune a 70B model on a single A100 with 80GB of VRAM.
6

--- PAGE 7 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Figure 4: Visualizing the Essence of Lora’s Basic Idea.
3.5 Multitask Fine-Tuning with Balanced Losses
As a multi-task learning framework, MFTCoder, as described in Section 2, faces a significant challenge of data
imbalance, task heterogeneity, and varying convergence speeds. To address these challenges, MFTCoder incorporates a
set of loss functions specifically designed to alleviate these imbalances.
To address the issue of data imbalance, we first ensure that all samples from all tasks are utilized exactly once within
a single epoch. To avoid the model favoring tasks with larger amounts of data, we introduce a weight assignment
strategy during loss computation. Specifically, we support two weight calculation schemes: one based on the number of
task samples and the other based on the number of valid tokens involved in the loss calculation. The former is more
straightforward, but it may perform poorly when dealing with tasks that have extreme differences in the number of valid
tokens, such as binary classification tasks like "yes" or "no" answering or single-choice exam tasks. On the other hand,
the latter weight assignment scheme based on the actual number of valid tokens involved in the loss calculation can
mitigate these issues. The specific formulation for weighted loss calculation is shown in Equation 1. In Equation 1, N
represents the total number of tasks, Midenotes the number of samples for the i-th task, Tijsignifies the count of valid
tokens (i.e., tokens involved in loss calculation) for the j-th sample of the i-th task, and tijkrefers to the k-th valid token
of the j-th sample for the i-th task.
L(θ) = min
θ1
NNX
i=1PMi
j=1PTij
k=1−log(pθ(tijk))
PMi
j=1Tij(1)
To address the issue of task heterogeneity, we drew inspiration from the focal loss approach and incorporated it into
MFTCoder. We implemented two different levels of focal loss functions to cater to different granularities. One operates
at the sample level, as shown in Equation 2, while the other operates at the task level, as shown in Equation 3.
L2(θ) = min
θPN
i=1PMi
j=1−αi∗(1−Pij)γ∗QijPN
i=1Mi, Pij=1
TijTijX
k=1Pijk, Qij=1
TijTijX
k=1log(Pijk) (2)
L3(θ) = min
θ1
NNX
i=1−αi∗(1−Pi)γ∗Qi, Pi=1
MiMiX
j=11
TijTijX
k=1Pijk, Qi=1
MiMiX
j=11
TijTijX
k=1log(Pijk)(3)
To address the issue of inconsistent convergence speeds, we drew inspiration from the FAMO Liu et al .(2023) approach
and innovatively applied it to calculate the validation loss. Firstly, we assumed that each task, indexed by i, has its
own original loss Li(θ). In the t-th iteration, we updated the weights of each task based on the gradients of their
corresponding validation losses, aiming to maximize the weight wifor the task with the slowest convergence speed,
shown as Equation 4. Here, gtrepresents the gradient of the weighted validation loss for all tasks, ci(α, gt)denotes the
slope (gradient) of the validation loss for the i-th task, θtdenotes the parameters of the network in the t-th iteration, α
is the learning rate, and ϵis a small constant to prevent division by zero. Furthermore, we would like to provide further
explanation on how we achieve balanced convergence. To ensure that tasks converge at a similar pace, we introduce a
7

--- PAGE 8 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 1: Various experimental models and their corresponding training data.
Experimental Model Task Desired Ability #Samples#Samples
after packing
SFT-S-C ODECOMPLETION CODE -COMPLETION Code Completion 192,547 18,811
SFT-S-T EXT2CODE TEXT 2CODE Text-to-code Generation 94,086 14,399
SFT-S-C ODECOMMENT CODE -COMMENT Code Comments Generation 645,711 134,775
SFT-S-C ODETRANS CODE -TRANS Code Translation 307,585 71,573
SFT-S-U NITTEST UNIT -TEST Unit test-case generation 390,393 77,681
SFT-M IXED Mix of the above 5 tasks All of the above 1,630,322 317,239
MFT-5T ASKS The above 5 tasks All of the above 1,630,322 317,239
dynamic balancing mechanism. At each iteration, we update the task-specific weights based on the gradients of their
validation losses. This approach aims to give more importance to tasks with slower convergence speeds, allowing
them to have a larger influence on the overall optimization process. By dynamically adjusting the task weights, we
create a balanced convergence scenario, where all tasks progress towards their optimal solutions at a similar rate. This
mechanism effectively addresses the issue of disparate convergence speeds and enhances the overall stability and
performance of the MFTCoder framework.
L4(θ) = max
gtmin
i1
αci(α, gt)−1
2∥gt∥2, gt=X
iwi
t∇Li(θt), ci(α, gt) =Li(θt)− Li(θt−αdt)
Li(θt) +ϵ(4)
By incorporating these different loss functions, MFTCoder effectively addresses the diverse requirements of various
multitask scenarios and alleviates the challenges of data imbalance, task heterogeneity, and inconsistent convergence
speeds typically encountered in existing large-scale MTL research. MFTCoder’s flexible framework provides a robust
solution to these issues, empowering the development of more efficient and accurate multitask models.
4 Evaluation
In this section, we will conduct multiple sets of experiments using MFTCoder to validate the effectiveness and
superiority of the MFT method. Specifically, we aim to address the following three research questions:
RQ1: Does the MFT model, obtained by fine-tuning multiple tasks using MFT methodology, outperform the
SFT-S(ingle) models, where each task is individually fine-tuned?
RQ2: Does the MFT model outperform the SFT-Mixed model, where multiple tasks are combined and fine-tuned
as one?
RQ3: In terms of generalization to unseen tasks, does the MFT model outperform the SFT-Mixed model?
Next, we will commence by presenting the experimental setup. Subsequently, we will showcase and delve into the
experimental results. Finally, we will culminate by summarizing and addressing the research questions raised in this
section.
4.1 Evaluation Setup
To address these three research questions, we selected 5 code-related downstream tasks and prepared the corresponding
fine-tuning data, as shown in Table 1. Table 1 presents the desired enhancements (Column III) and the number of
samples (Column IV) for each task. For instance, the CODECOMPLETION -TASK aims to improve the model’s code
completion ability and includes 192,547 fine-tuning samples. The CODETRANS -TASK aims to enhance the model’s code
translation capability and consists of 307,585 fine-tuning samples. Thus, we trained 7 models (Column I), including
individual SFT-S-* models trained for each downstream task, a combined SFT-M IXED model for the 5 task data, and
an MFT-5T ASKS model trained using the MFT method.
In the experiment, all models were configured identically except for the training data. The base model for all models
was CodeLlama-13B-Python Rozière et al .(2023). Each model was trained using 16 A100 GPUs (with 80GB VRAM),
a micro batch size of 8, and a global batch size of 128. The Adam optimizer Kingma and Ba (2017) was used with an
8

--- PAGE 9 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
initial learning rate of 2e-4, and a minimum learning rate of 1e-5. We employed the QLora-INT4 mode of MFTCoder
for fine-tuning, with a consistent fine-tuning parameter proportion of 2.52%. The positions and initial values of the
trainable parameters were also the same. All models incorporate the Data-Balance Loss (i.e., Equation 1) and employ
pack tokenization mode. Notably, when there is only one task, this loss function aligns with the conventional loss
employed in standard GPT model pre-training. To determine the convergence point for each model, we trained them
until the validation loss surpassed the loss from the current epoch for the next two consecutive epochs. This mechanism,
known as early-stopping strategy, was employed to ensure optimal convergence for each model.
4.2 Evaluation Datasets
In this paper, we utilized publicly available and representative code assessment benchmarks for comparative evaluation.
These benchmarks include:
•HumanEval Chen et al .(2021) is a widely used Python code completion evaluation dataset, meticulously curated by
researchers at OpenAI.
•HumanEval-X Zheng et al .(2023) is an extension of HumanEval, translated into multiple programming languages,
enabling multi-language code completion evaluation.
•DS-1000 Lai et al .(2022) focuses on assessing a model’s ability to perform data science analysis using Python code,
covering essential libraries such as Numpy, Pandas, TensorFlow, Pytorch, Scipy, Sklearn, and Matplotlib.
•MBPP Austin et al .(2021) comprises 1000 Python programming problems, constructed through crowdsourcing,
primarily targeting a model’s proficiency in basic Python. In this study, we selected 500 problems with ID 11-510 from
MBPP to evaluate the text-to-code generation capability, specifically generating code based on problem descriptions.
•CodeFuseEval Di et al .(2023), building upon HumanEval and HumanEval-X, further extends the evaluation to
include Chinese code completion (with Chinese docstrings), code translation, and unit test case generation capabilities,
referred to as CodeFuseEval-CN ,CodeFuseEval-CodeTrans , and CodeFuseEval-UnitTest , respectively.
Throughout these evaluation datasets, we employed "pass@1" as the evaluation metric in this paper.
4.3 Evaluation Results
In this section, we will showcase the evaluation results of seven trained models. For the SFT-S-* models, which
were trained individually for each task, we will focus on testing their specific target capabilities. For instance, we will
exclusively evaluate the performance of the SFT-S-C ODECOMPLETION model in the code completion task. On the
other hand, for the SFT-M IXED andMFT-5T ASKS models, we will assess their performance on each task and compare
it with the corresponding SFT-S-* models. Specifically, we will conduct tests to evaluate the capabilities of code
completion, text-to-code generation, code comment generation, code translation, and unit test case generation.
4.3.1 Code Completion
For code completion, we employed the HumanEval Chen et al .(2021) and HumanEval-X Zheng et al .(2023) evaluation
datasets to assess the model’s performance. HumanEval is a widely-used benchmark dataset released by OpenAI
specifically designed to evaluate the Python code completion ability of large language models. HumanEval-X, on the
other hand, is an expansion of HumanEval that enables the evaluation of large models’ code completion performance
across various programming languages. Consistent with other studies, we employed the pass@1 metric as the evaluation
measure.
We evaluated three models: SFT-S-C ODECOMPLETION ,SFT-M IXED , and MFT-5T ASKS . The performance of these
models on the HumanEval dataset is summarized in Table 2 (Column III). Results indicate that the MFT-5T ASKS
model, trained using the MFT approach, outperforms the other two models. It achieves a 2.44% higher performance
compared to the SFT-M IXED model, which was fine-tuned with mixed task data. It is worth noting that the SFT-M IXED
model does not perform as well as the SFT-S-C ODECOMPLETION model, which was trained individually for the
code-completion task.
Furthermore, we conducted a multilingual evaluation on the HumanEval-X dataset for the three models, as presented in
Table 3. The MFT-5T ASKS model demonstrates superior performance in Java and Golang, while the SFT-M IXED
model excels in C++ and JavaScript. Overall, the evaluation affirms that the MFT-5T ASKS model outperforms the
others, with an average improvement of 1.22% over the SFT-M IXED model.
Overall, in terms of code completion tasks, models trained using the MFT method outperform both individually
fine-tuned models and models fine-tuned after combining multiple tasks.
9

--- PAGE 10 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 2: Pass@1 performance on HumanEval (Code Completion) and MBPP (Text-to-Code Generation). We utilized
the greedy decoding strategy with zero-shot. The values of CodeLlama-Python-base are taken from Rozière et al .
(2023).
Model SizeHumaneval
pass@1MBPP
pass@1Average
CodeLlama-Python-base Rozière et al. (2023) 13B 43.3% 49.0% 46.15%
SFT-S-C ODECOMPLETION 13B 59.76% NA NA
SFT-S-T EXT2CODE 13B NA 54.2% NA
SFT-M IXED 13B 57.93% 53.6% 55.765%
MFT-5T ASKS 13B 60.37% 56.0% 58.185%
Table 3: Comparison of pass@1 Metric Performance on the multilingual HumanEval-X (zero-shot, greedy-decoding)
Trained Model Java C++ JavaScript Golang Average
CodeLlama-13B-Py-base 43.3% 41.46% 34.76% 38.41% 29.27%
SFT-S-C ODECOMPLETION 50.0% 39.02% 47.56% 40.23% 44.20%
SFT-M IXED 56.1% 48.17% 56.10% 37.80% 49.54%
MFT-5T ASKS 57.32% 46.34% 54.27% 45.12% 50.76%
4.3.2 Text-to-Code Generation
To evaluate the models’ ability to generate code based on descriptions, we selected the MBPP Austin et al .(2021)
evaluation dataset and used the pass@1 metric. MBPP is specifically designed to assess models’ capacity to synthesize
concise Python programs from natural language descriptions.
We tested three models, namely SFT-S-T EXT2CODE,SFT-M IXED , and MFT-5T ASKS , on the MBPP dataset,
measuring their pass@1 performance as shown in Table 2 (Column IV). Among these models, MFT-5T ASKS exhibited
the highest performance, surpassing the SFT-M IXED model by 2.4%. Similarly, in terms of the text-to-code generation
task, models fine-tuned after combining multiple tasks showed inferior performance compared to models fine-tuned
specifically for this individual task.
Overall, in terms of text-to-code generation tasks, models trained using the MFT method outperform both
individually fine-tuned models and models fine-tuned after combining multiple tasks.
4.3.3 Code Comment Generation
The objective of the code comment generation task is to have models add necessary comments to the code without
modifying the input code itself. This includes both line comments and interface comments, making the code more
readable and user-friendly.
To assess this capability, we constructed an evaluation set based on 500 MBPP test questions (id xx-xx). For each
question in the evaluation set, we had the SFT-S-C ODECOMMENT ,SFT-M IXED andMFT-5T ASKS models generate
comments for it. Subsequently, we employed GPT-4 as the referee, which has been instructed with criteria for good
comments, to determine which model performed the best. If it was not possible to determine, the output was labeled as
UNKNOWN. Finally, we counted the number of questions where each model was determined to perform the best and
calculated the corresponding proportions, shown in Table 4.
It can be observed that 38.8% of the questions were determined to be best performed by the MFT-5T ASKS model,
surpassing the second-ranked SFT-M IXED by 7.4% and the third-ranked SFT-S-C ODECOMMENT by 10.8%. Addition-
ally, 1.8% of the questions were marked as indeterminable by GPT-4. In summary, for this task, the models trained
using the MFT method exhibit the best performance.
4.3.4 Code Translation
The objective of the code translation task is to accurately and precisely translate a given code snippet implemented
in the source language into an equivalent code snippet implemented in the target language while ensuring that both
implementations possess identical functionality. Here, we utilize the CODEFUSE EVAL5Di et al .(2023) evaluation
5https://github.com/codefuse-ai/codefuse-evaluation
10

--- PAGE 11 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 4: Performance Comparison of Three Models on Code Commenting Task. GPT-4 Determines the Best Performing
Model for Each Question. This Table Presents the Proportion of Questions Where Each Model Performs the Best. In
particular, 1.8% of the evaluation cases were indeterminate for GPT-4 to determine the best-performing model.
Training Model Best identified by GPT-4
SFT-S-C ODECOMMENT 28%
SFT-M IXED 31.4%
MFT-5T ASKS 38.8%
Table 5: Comparison of pass@1 Metric Performance on the CODEFUSE EVAL -CodeTranslation Di et al .(2023)
(zero-shot, greedy-decoding)
Training Model Py2Java Py2C++ Java2Py C++2Py Java2C++ C++2Java Avg.
SFT-S-C ODETRANS 59.52% 57.40% 70.73% 62.20% 67.07% 62.80% 63.29%
SFT-M IXED 80.16% 71.20% 67.68% 72.56% 65.85% 82.31% 73.29%
MFT-5T ASKS 82.16% 77.20% 65.85% 70.73% 64.64% 84.76% 74.22%
datasets’ code translation subset to support bidirectional translation between Java, Python, and C++. In order to evaluate
the accuracy and functional equivalence of the translation results, we employ test cases that are semantically equivalent
to ones of the source program for each task. These test cases are used to verify whether the result code can run and pass
successfully, as indicated by the pass@1 criterion.
The test results of the three models are presented in Table 5. The MFT-5T ASKS model performs the best in Python-
to-Java, Python-to-C++, and C++-to-Java translations. The SFT-M IXED model excels in C++-to-Python translation,
while the SFT-S-C ODETRANS model performs the best in Java-to-Python and Java-to-C++ translations. Overall, the
MFT-5T ASKS model demonstrates superior performance, with an average improvement of 0.93% over SFT-M IXED
and 10.9% over SFT-S-C ODETRANS .This task also highlights the phenomenon that models trained using the
MFT approach outperform the other two training methods.
4.3.5 Unit Test Case Generation
The task at hand is to generate unit test cases by training a model to produce a set of test cases for a given code snippet,
such as a method or class, and verify if the provided code implementation is correct. We have opted to utilize the unittest
subset from the CODEFUSE EVAL Di et al .(2023) evaluation datasets as our test suite. We evaluate the test cases using
the pass@1 metric, which means that if a model generates test cases for a sample program and the sample program
passes all the test cases, the count of correctly generated samples increases by 1. Similar to CodeLLama Rozière et al .
(2023), we employ a greedy decoding strategy during the evaluation process.
We compared three models across Python, Java, and JavaScript for their test generation capabilities. The results in
Table 6 indicate that the MFT-5T ASKS model outperforms others in test generation for Python, with a 5.73% lead
over the second-ranked SFT-M IXED model, and a significant 10.19% lead over the third-ranked SFT-S-U NITTEST
model. In JavaScript, the MFT-5T ASKS model also excels, with a 7.93% advantage over the other models. However, in
Java, the MFT-5T ASKS model performs 5.37% better than SFT-S-U NITTEST, but slightly lags behind SFT-M IXED
by 5.44%. Overall, the MFT-5T ASKS model consistently demonstrates the highest performance, with an average
improvement of 2.74% over SFT-M IXED , and a remarkable 7.83% improvement over SFT-S-U NITTEST.
In summary, training models using the MFT approach yielded better performance compared to models fine-tuned with
the same data mixed together. Furthermore, the performance of the MFT-trained models surpassed that of the model
individually fine-tuned for the UNIT-TEST task.
Table 6: Comparison of pass@1 Metric Performance on the codefuseEV AL-TestcaseGeneration Di et al .(2023)
(zero-shot, greedy-decoding)
Trained Model Python Java JavaScript Average
SFT-S-U NITTEST 33.76% 32.43% 41.46% 35.88%
SFT-M IXED 38.22% 43.24% 41.46% 40.97%
MFT-5T ASKS 43.95% 37.8% 49.39% 43.71%
11

--- PAGE 12 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 7: Sample Text-to-SQL Test Cases from the Spider Yu et al .(2019b) dataset and Example Predictions by the
SFT-Mixed Model and MFT-5Tasks Model.
PROMPT:<|role_start|>system<|role_end|>You are a professional SQL engineer and you are writing SQL
queries for data query tasks.
<|role_start|>human<|role_end|>What are the names of the heads who are born outside the
California state?
schema:head(head_ID|name|born_state|age
<|role_start|>bot<|role_end|>
LABEL:SELECT name FROM head WHERE born_state != ’California’
SFT-M IXED :You can use the SQL SELECT statement to retrieve the names of the heads who are born outside
the California state. Here is a sample query:
“‘sql
SELECT name
FROM head
WHERE born_state <> ’California’;
“‘
This query selects the name column from the head table where the born_state column is not
equal to ’California’.
MFT-5T ASKS :SELECT name
FROM head
WHERE born_state <> ’California’;
4.3.6 Generalization on an Unseen Task
In addition to evaluating the performance of our models on tasks with training data to address RQ1 andRQ2 , we also
have RQ3 to answer, which is whether the models trained using the MFT approach exhibit better generalization on
unseen tasks compared to models trained by combining multiple datasets into a single SFT approach. To investigate
this, we selected the Text-to-SQL generation task as our testing target. The data for this task was not included in the
training of the seven existing models. Furthermore, this task is code-related but distinctly different from the previous
five downstream tasks.
We have selected two evaluation metrics, BLEU score and logical accuracy of SQL statements. The BLEU score
assesses the textual similarity between the generated outputs and the reference answers. The logical accuracy metric,
on the other hand, allows us to address the variations in SQL syntax that may occur. Specifically, Logical Accuracy
measures the proportion of test samples in the dataset where the generated SQL statements are both syntactically correct
and semantically equivalent to the reference answers.
We selected five representative text-to-SQL datasets, including WikiSQL Zhong et al .(2017), Spider Yu et al .(2019b),
CSpider Min et al .(2019), CoSQL Yu et al .(2019a), and BirdSQL Li et al .(2023d), and randomly sampled 200
examples from each dataset for evaluation. The test case examples are shown in Table 7, where the first row demonstrates
the fine-tuned data format similar to OpenAI ChatML format6. Using each sampled dataset, we tested the logical
accuracy and BLEU score of the SFT-M IXED and MFT-5T ASKS models, as shown in Table 8.
According to Table 8, MFT-5T ASKS outperforms SFT-M IXED in terms of BLEU scores on each dataset, averaging
2.78 times higher. This indicates that the generated results of MFT-5T ASKS exhibit higher similarity to the reference
answer texts. This similarity can also be observed in Table 7, where MFT-5T ASKS produces cleaner results, while
SFT-M IXED provides more explanations, which may be preferred in certain scenarios. Moreover, MFT-5T ASKS
demonstrates better performance in terms of logical accuracy, achieving an overall accuracy that is 2.18 times higher
than SFT-M IXED model, and up to 4.67 times higher on the WikiSQL dataset. Numerically, MFT-5T ASKS exhibits
superior performance compared to SFT-M IXED , indicating stronger generalization of MFT-trained models on
the Text-to-SQL task, which is an unseen task during training.
6https://github.com/openai/openai-python/blob/main/chatml.md
12

--- PAGE 13 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 8: Comparison of generalization capabilities between MFT-5T ASKS andSFT-M IXED on the Text-to-SQL task.
The evaluation metrics include SQL logical accuracy and BLEU score.
Trained Model WIKISQL SPIDER CSPIDER COSQL BiRDSQL Average
Logical Accuracy
SFT-M IXED 1.5% 2.0% 7.0% 6.5% 5.5% 4.5%
MFT-5T ASKS 7.0% (4.67x) 4.5% (2.25x) 16.5% (2.36x) 10.5%(1.62x) 10.5% (1.91x) 9.8% (2.18x)
BLEU
SFT-M IXED 0.032 0.047 0.025 0.081 0.026 0.042
MFT-5T ASKS 0.138 0.138 0.116 0.119 0.074 0.117
4.4 Evaluation Summary
We selected five downstream tasks related to code and trained a total of seven models, including SFT-S-* models
fine-tuned individually for each task, the SFT-M IXED model fine-tuned with a mixture of all task data, and the
MFT-5T ASKS model trained using the MFT method. We compared and tested the performance of each model in terms
of their target capabilities. Additionally, we evaluated the generalization performance of the MFT method and the
mixed SFT method on unseen tasks. The results can be summarized as follows:
iModels trained with the MFT method outperformed those fine-tuned individually for each task, indicating a
positive answer to RQ1.
iiModels trained with the MFT method outperformed those fine-tuned with a mixture of multiple tasks,
providing a positive answer to RQ2.
iiiModels trained with the MFT method exhibit stronger generalization capabilities on new, unseen tasks
compared to the SFT models fine-tuned with a mixture of multiple task data.
5 Application
Considering the outstanding performance of the MFT training method, we have leveraged our MFTCoder3, developed
based on this approach, to fine-tune the existing mainstream open-source LLM models. e.g.QWen Bai et al .(2023),
Baichuan Baichuan (2023), CodeGeex2 Zheng et al .(2023), Llama Touvron et al .(2023a), LLama2 Touvron et al .
(2023b), CodeLLama Rozière et al. (2023), StarCoder Li et al. (2023a).
MFTCoder supports Lora and QLora, which significantly reduces the number of model training parameters. Coupled
with dual quantization for model size compression, this ultimately leads to a substantial reduction in GPU memory
requirements. As a result, it becomes possible to fine-tune a 70B model on a single A100 GPU with ease. When
fine-tuning these models using MFTCoder, we set the trainable parameters to be within the range of 0.1% to 5% of the
total parameters. Through empirical evidence, we have found that as the proportion of trainable parameters increases,
performance improvement tends to plateau. In fact, we have observed that a trainable parameter proportion of less than
5% is often sufficient to achieve performance levels close to that of full-scale fine-tuning.
When fine-tuning these models, we configure them for multitasking with a range of 3-7 tasks. Depending on the model
size, we typically use Lora mode for models below 20B, and QLora mode for models larger than 20B. After fine-tuning,
we evaluate their performance in code completion and text-to-code generation tasks, measuring their performance
on HumanEval Chen et al .(2021) and MBPP Austin et al .(2021), as shown in Table 9 Column III and IV. We have
calculated the average improvement of MFT fine-tuning compared to the base models in terms of HumanEval and
MBPP. As shown in column 5, the improvement ranges from 6.26% to 12.75%, with the improvements on HumanEval
consistently surpassing those on MBPP. Additionally, we have also evaluated the code completion performance of the
MFTCoder fine-tuned models on the multilingual benchmark, HumanEval-X Zheng et al .(2023). The results of this
evaluation are presented in Table 10. Notably, the fine-tuned CodeFuse-CodeLLama-Python-MFT (34B) achieved an
average pass@1 of 56.88% across four languages: Java, C++, JavaScript, and Golang.
Table 9 also presents the performance of fine-tuned open-source models ( e.g.OctoPack Muennighoff et al .(2023) and
WizardCoder-Python Luo et al .(2023)) and representative closed-source models (e.g., Claude2 Anthropic (2023), GPT-
4 OpenAI (2023)) on HumanEval and MBPP. It is worth noting that our fine-tuned model, CodeFuse-CodeLLama-
34B7, based on CodeLlama-34B-Python achieves a remarkable performance of 74.4% on HumanEval, surpassing
7https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B
13

--- PAGE 14 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 9: pass@1 performance on HumanEval Chen et al .(2021) (Code Completion) and MBPP Austin et al .(2021)
(Text-to-Code Generation) after fine-tuning with MFTCoder across multiple mainstream open-source models. The
CodeFuse-*-MFT models are evaluated using a combination of greedy decoding and zero-shot testing strategy, while
the metric values for the other models are taken from their respective papers, technical reports, or open-source project
homepages.
Model SizeHumaneval
pass@1MBPP
pass@1Average
Open-source base models
QWen-base Bai et al. (2023) 14B 32.3% 40.8% 36.55%
Llama-base Touvron et al. (2023a) 65B 23.7% 37.7% 30.7%
Llama2-base Touvron et al. (2023b) 70B 29.9% 45.0% 37.45%
StarCoder-base Li et al. (2023a) 15B 33.6% 52.7% 43.15%
CodeGeex2-base Zheng et al. (2023) 6B 35.9% 42.4% 39.15%
CodeLlama-Python-base Rozière et al. (2023) 13B 43.3% 49.0% 46.15%
CodeLlama-Python-base Rozière et al. (2023) 34B 53.7% 56.2% 54.95%
MFT fine-tuned models
CodeFuse-QWen-MFT814B 48.78% 43.8% 46.29% (+9.74%)
CodeFuse-Llama-MFT 65B 34.76% 41.8% 38.28% (+7.58)
CodeFuse-Llama2-MFT 70B 40.85% 40.8% 40.83% (+3.38%)
CodeFuse-StarCoder-MFT915B 54.90% 49.60% 52.25% (+9.10%)
CodeFuse-CodeGeex2-MFT 6B 45.12% 46.2% 45.66% (+6.51%)
CodeFuse-CodeLlama-Python-MFT 13B 60.37% 56.0% 58.19% (+12.04%)
CodeFuse-CodeLLama-Python-MFT734B 74.4% 61.0% 67.70% (+12.75%)
Open-source fine-tuned models
QWen-chat Bai et al. (2023) 14B 43.9% 46.4% 45.15%
PHI-1 Gunasekar et al. (2023) 1.3B 50.6% 55.5% 53.05%
OctoCoder Muennighoff et al. (2023) 15B 46.2% NA NA
WizardCoder Luo et al. (2023) 15B 57.3% 51.8% 54.55%
Phind-CodeLlama-v2 Phind (2023) 34B 71.95% NA NA
WizardCoder-Python Luo et al. (2023) 34B 73.2% 61.2% 67.2%
Closed-source models
PanGu-Coder2 Shen et al. (2023) 15B 61.2% NA NA
Unnatural CodeLlama Rozière et al. (2023) 34B 62.2% 61.2% 61.7%
Claude2 Anthropic (2023) NA 71.2% NA NA
GPT-3.5 OpenAI (2023) 175B 48.1% 52.2% 50.15%
GPT-4 (zero-shot) OpenAI (2023) NA 67.00% NA NA
all the listed models in the table, including GPT-4 (67.00%, zero-shot) OpenAI (2023). We also evaluated
the performance of the model on other benchmarks, including multilingual HUMAN EVAL-XZheng et al .(2023),
MBPP Austin et al .(2021), DS-1000 Lai et al .(2022)and CODEFUSE EVAL Di et al .(2023), and compared it against
GPT-3.5 and GPT-4, as shown in Figure 5. CodeFuse-CodeLLama-34B outperforms GPT-4 on CODEFUSEEVAL-
UNITTESTandHUMANEVAL , matches its performance in code translation ability, but falls behind in Chinese code
completion ( CODEFUSEEVAL-CN), multi-language completion, data-science analysis ( DS-1000 ), and text-to-code
generation ( MBPP ) capabilities compared to GPT-4. However, it surpasses or equals GPT-3.5 on all evaluation datasets.
The input-output examples on each evaluation dataset can be found in Appendix C.
Furthermore, we conducted an evaluation to assess the impact of fine-tuning the models with MFTCoder and code-
related data on their performance in NLP tasks, as illustrated in Figure 6. Taking CODEFUSE-QW EN-14B as a case
study, we compared it against the base model QW EN-14B and the official model QW EN-14B- CHAT fine-tuned by
Alibaba Cloud on top of it. It is evident that CODEFUSE-QW EN-14B maintains its proficiency in NLP. In fact, it
exhibits a slight enhancement in language, reasoning, and understanding abilities compared to the other two models.
However, there is a minor decline in its examination ability when compared to the base model QW EN-14B and similar
findings are observed for the fine-tuned QW EN-14B- CHAT model.
8https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B
9https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B
14

--- PAGE 15 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Table 10: pass@1 performance on Multi-lingual HumanEval-X (Zheng et al., 2023) after fine-tuning with MFTCoder
across multiple mainstream open-source models. The metric values marked with an asterisk (*) were obtained from the
models’ corresponding papers, technical reports, or open-source project homepages, while the remaining metric values
were evaluated using a combination of greedy decoding and zero-shot testing strategy.
Model Size Python Java C++ JavaScript Golang Avgerage
QWen-base 14B 32.3%∗35.37% 30.49% 32.93% 21.34% 30.49%
CodeFuse-QWen-MFT 14B 48.78% 41.46% 38.41% 46.34% 26.83% 40.36%
Llama-base 65B 23.7%∗29.26% 20.73% 23.78% 18.9% 23.27%
CodeFuse-Llama-MFT 65B 34.76% 37.2% 29.88% 32.93% 23.78% 31.71%
Llama2-base 70B 29.9%∗39.02% 31.10% 35.98% 23.78% 31.96%
CodeFuse-Llama2-MFT 70B 40.85% 35.98% 32.32% 38.41% 27.44% 35.00%
StarCoder-base 15B 33.6%∗34.15% 25.61% 22.56% 22.56% 29.48%
CodeFuse-StarCoder-MFT 15B 54.9% 47.56 46.34% 48.17% 37.20% 46.83%
CodeGeex2-base 6B 35.9%∗30.8%∗29.3%∗32.2%∗22.5%∗30.14%
CodeFuse-CodeGeex2-MFT 6B 45.12% 45.73% 37.2% 37.2% 28.05% 38.66%
CodeLlama-Python-base 13B 43.3%∗41.46% 34.76% 38.41% 29.27% 37.44%
CodeFuse-CodeLlama-Python-MFT 13B 60.37% 57.32% 46.34% 54.27% 45.12% 52.68%
CodeLlama-34B-Python-base 34B 53.7%∗45.73% 42.68% 45.73% 31.71% 43.91%
CodeFuse-CodeLLama-Python-MFT 34B 74.4% 61.6% 54.3% 61.0% 50.6% 60.38%
HumanEvalCodeFuseEval-CNHumanEval-X
MBPP
CodeFuseEval-UnitTest
DS1000CodeFuseEval-CodeTrans1020304050607080
CodeFuse-CodeLlama-34B
GPT-3.5
GPT-4
Figure 5: Radar Chart of CodeFuse-CodeLlama-34B Model on HUMAN EVAL,HUMAN EVAL-X,MBPP ,DS-1000 ,
and CODEFUSE EVAL benchmarks compared to GPT-3.5 and GPT-4.
6 Discussion
Despite the superior performance of the MFT training method compared to the task data mixing-based SFT training
method in the aforementioned experiments, it should be noted that the effectiveness of the MFT approach is highly
dependent on the task-splitting strategy. Not all scenarios are suitable for being split into multiple tasks. For instance, in
our practical experience, we found that splitting a task based on difficulty levels and training it using the MFT method
did not yield better results compared to the task-mixed SFT training method. Moreover, training code completion
tasks as multiple tasks based on programming languages also did not outperform the mixed SFT method. Based on
our practical experience, we have concluded that tasks with distinct main desired abilities are more suitable for task
15

--- PAGE 16 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
AFQMCCHIDWicWSCCOPA
CMNLI
OCNLI
AX-b
AX-g
RTE
CSL
C3
EPRSTMTMMLUC-EvalARC-c0.20.40.60.81.0
Language Reasoning
UnderstandingExaminationCodeFuse-Qwen-14b
Qwen-14b
Qwen-14b-chat
Figure 6: Performance comparison of CODEFUSE-QW EN-14B fine-tuned with MFTCoder and code-related data,
QW EN-14B base model, and officially fine-tuned model QW EN-14B- CHAT on NLP evaluation datasets. Detailed data
can be found in Appendix D.
splitting and MFT training, whereas tasks with similar main training objectives are not well-suited for MFT training.
We plan to further investigate and establish more precise criteria for task delineation in future research.
In our task generalization experiments, we observed that models trained using the MFT method produced inference
results that were more similar to the reference answers and had more concise content. Conversely, inference results
generated by the task-mixed SFT training method contained more Chain-of-Thought (CoT) information. In certain
scenarios, the former approach was more preferred, such as in IDE plugins, while the latter approach was favored in
other scenarios, such as web assistants. As a result, we cannot simply generalize that one method is better than the
other. We are currently researching the reasons behind these performance differences.
As a multi-task learning method, MFT also faces a major challenge during the training process: inconsistent convergence
speeds among different tasks. For example, in the aforementioned experiments, the code completion task converged
much faster than the unit test-case generation task (details can be found in Appendix B). This makes it difficult to find
an optimal point that performs well on all tasks. The selected checkpoint either converges insufficiently on some tasks
or overfits on others. To address this issue, we experimented with existing multi-task learning balancing optimization
solutions such as FAMO Liu et al .(2023). However, FAMO requires dual back-propagation in each iteration, resulting
in training time being approximately doubled. Furthermore, the required number of epochs for convergence also
increases significantly, and the adjustability of the convergence speed is limited. Unfortunately, this exponentially
increased cost does not yield equivalent benefits. In response, we are currently developing a more optimal and adaptive
multi-task optimization balancing approach.
Furthermore, even after balancing the convergence speeds of multiple tasks, where the same set of parameters is updated,
it is still challenging to fundamentally eliminate the inherent conflicts in weight updates across different tasks. To
address this issue, we are currently exploring the utilization of MoE (Mixture of Experts) Chen et al .(2022) to achieve
MFT.
7 Conclusion
This paper introduces MFTCoder, a framework that supports multi-task fine-tuning, effectively addressing the challenges
of data imbalance, varying difficulty levels, and inconsistent convergence speeds through the design of various loss
functions. Experimental results demonstrate that this approach outperforms individual fine-tuning on each task or
fine-tuning on a mixed ensemble of tasks. Additionally, MFTCoder facilitates efficient training, including efficient
data utilization and PEFT training. It also provides a high-quality instruction dataset construction solution. Leveraging
MFTCoder for fine-tuning on the CodeLLama base, the CodeFuse-CodeLLama-34B model achieves an impressive
pass@1 score of 74.4% on the HumanEval dataset, surpassing the performance of GPT-4 (67%, zero-shot).
16

--- PAGE 17 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
References
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive
Multi-task Representations with Pre-Finetuning. arXiv:cs.CL/2101.11038
Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff,
Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey
Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo GarcÃa del
RÃo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar,
David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried,
Arjun Guha, Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don’t reach for the stars! arXiv:cs.SE/2301.03988
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa,
Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav
Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin
Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, ClÃ ©ment Crepy,
Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark DÃaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng,
Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,
Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li,
YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua
Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat,
Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,
Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine,
Dasha Valter, Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,
Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny
Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report. arXiv:cs.CL/2305.10403
Anthropic. 2023. Model Card and Evaluations for Claude Models .https://www-files.anthropic.com/production/
images/Model-Card-Claude-2.pdf
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran,
Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. ExT5: Towards Extreme Multi-Task
Scaling for Transfer Learning. arXiv:cs.CL/2111.10952
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael
Terry, Quoc Le, et al. 2021. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732 (2021).
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui,
Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang
Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng
Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen
Technical Report. arXiv preprint arXiv:2309.16609 (2023).
Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint arXiv:2309.10305 (2023). https://arxiv.
org/abs/2309.10305
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle
McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,
and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. arXiv:cs.CL/2204.06745
Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41–75.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri
Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,
Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. (2021).
arXiv:cs.LG/2107.03374
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive
loss balancing in deep multitask networks. In International conference on machine learning . PMLR, 794–803.
Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. 2022. Towards Understanding Mixture of Experts in Deep
Learning. arXiv:cs.LG/2208.02813
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won
Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,
17

--- PAGE 18 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr
Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with
Pathways. arXiv:cs.CL/2204.02311
Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo
Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng
Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022. PanGu-Coder: Program Synthesis with Function-Level Language Modeling.
arXiv:cs.LG/2207.11280
Michael Crawshaw. 2020. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796 (2020).
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.
arXiv:cs.LG/2305.14314
Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang
Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen
Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao
Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. 2023. CodeFuse-13B: A Pretrained
Multi-lingual Code Large Language Model. arXiv:cs.SE/2310.06266
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model
Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) . 320–335.
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing
in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the
7th international joint conference on natural language processing (volume 2: short papers) . 845–850.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero
Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al .2023. Textbooks Are All You Need. arXiv preprint arXiv:2306.11644 (2023).
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan,
and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. arXiv:cs.LG/1902.00751
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA:
Low-Rank Adaptation of Large Language Models. arXiv:cs.CL/2106.09685
Sébastien Jean, Orhan Firat, and Melvin Johnson. 2019. Adaptive scheduling for multi-task learning. arXiv preprint arXiv:1909.06434
(2019).
Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using uncertainty to weigh losses for scene geometry and
semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7482–7491.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:cs.LG/1412.6980
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang,
and Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation. ArXiv abs/2211.11501
(2022).
Hae Beom Lee, Eunho Yang, and Sung Ju Hwang. 2018. Deep asymmetric multi-task feature learning. In International Conference
on Machine Learning . PMLR, 2956–2964.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023c. CAMEL: Communicative
Agents for "Mind" Exploration of Large Scale Language Model Society. arXiv:cs.AI/2303.17760
Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo,
Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023d. Can LLM
Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. arXiv:cs.CL/2305.03111
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher
Akiki, Jia Li, Jenny Chim, et al. 2023a. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023).
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks Are All You
Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix
Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen
Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,
Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with
AlphaCode. Science 378, 6624 (dec 2022), 1092–1097. https://doi.org/10.1126/science.abq1158
Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. 2023. FAMO: Fast Adaptive Multitask Optimization. arXiv:cs.LG/2306.03792
18

--- PAGE 19 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
Shikun Liu, Edward Johns, and Andrew J Davison. 2019b. End-to-end multi-task learning with attention. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition . 1871–1880.
Shengchao Liu, Yingyu Liang, and Anthony Gitter. 2019c. Loss-balanced task weighting to reduce negative transfer in multi-task
learning. In Proceedings of the AAAI conference on artificial intelligence , V ol. 33. 9977–9978.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Multi-Task Deep Neural Networks for Natural Language
Understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 4487–4496.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. 2017. Learning multiple tasks with multilinear relationship
networks. Advances in neural information processing systems 30 (2017).
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568 (2023).
Qingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A Pilot Study for Chinese SQL Semantic Parsing. arXiv:cs.CL/1909.13293
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro
von Werra, and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models. arXiv:cs.CL/2308.07124
OpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774
Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. 2021. Maximum roaming multi-task learning. In
Proceedings of the AAAI Conference on Artificial Intelligence , V ol. 35. 9331–9341.
Phind. 2023. Phind-CodeLlama-34B-v2 .https://huggingface.co/Phind/Phind-CodeLlama-34B-v2
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2023. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:cs.LG/1910.10683
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez,
Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, et al .
2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint arXiv:2307.14936 (2023).
Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. 2020. Adashare: Learning what to share for efficient deep multi-task
learning. Advances in Neural Information Processing Systems 33 (2020), 8728–8740.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos,
Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière,
Naman Goyal, Eric Hambro, Faisal Azhar, et al .2023a. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971 (2023).
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,
Prajjwal Bhargava, Shruti Bhosale, et al .2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 (2023).
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022.
Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022).
Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code
Large Language Models for Code Understanding and Generation. arXiv:cs.CL/2305.07922
Yongxin Yang and Timothy Hospedales. 2017. Trace Norm Regularised Deep Multi-Task Learning. In 5th International Conference
on Learning Representations .
Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang,
Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent
Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki, and Dragomir Radev. 2019a. CoSQL: A Conversational Text-to-SQL
Challenge Towards Cross-Domain Natural Language Interfaces to Databases. arXiv:cs.CL/1909.05378
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman,
Zilin Zhang, and Dragomir Radev. 2019b. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain
Semantic Parsing and Text-to-SQL Task. arXiv:cs.CL/1809.08887
Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. 2018. A modulation module for multi-task learning with
applications in image retrieval. In Proceedings of the European Conference on Computer Vision (ECCV) . 401–416.
Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin
Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.
InKDD .
Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating Structured Queries from Natural Language using
Reinforcement Learning. CoRR abs/1709.00103 (2017).
19

--- PAGE 20 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
A Code Exercises Generation with Camel
Table 11: Examples of Prompts for Code Exercises Generation using Camel
Original task prompt:
Create 50 coding exercises to test and improve students’ Python programming skills. The exercises must must must focus on Binary search. The difficulty of exercises
must be from easy to medium to hard.
Specified task prompt:
A student will collaborate with a programming teacher to develop 50 Python coding exercises centered specifically on Binary search algorithms. The exercises should
progressively increase in difficulty, from easy to medium to hard, targeting various aspects of implementation and optimization techniques.
Final task prompt:
A student will collaborate with a programming teacher to develop 50 Python coding exercises centered specifically on Binary search algorithms. The exercises should
progressively increase in difficulty, from easy to medium to hard, targeting various aspects of implementation and optimization techniques.
AI Assistant sys message:
SystemMessage(role_name=’A student’, role_type = <RoleType.ASSISTANT: ’assis-
tant’>, meta_dict={’task’: ’A student will assist a programming teacher in designing a
series of 50 Python coding exercises that emphasize the application and optimization
of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
from easy to medium to hard, allowing students to strengthen their sorting skills
progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A programming teacher’},
role=’system’, content=’Never forget you are a A student and I am a A programming
teacher. Never flip roles! Never instruct me!
We share a common interest in collaborating to successfully complete a task.
You must help me to complete the task.
Here is the task: A student will assist a programming teacher in designing a series of
50 Python coding exercises that emphasize the application and optimization of Bubble
Sort algorithm. These exercises will span varying levels of difficulty, from easy to
medium to hard, allowing students to strengthen their sorting skills progressively..
Never forget our task!
I must instruct you based on your expertise and my needs to complete the task.
I must give you one instruction at a time.
You must write a specific solution that appropriately solves the requested instruction
and explain your solutions.
You must decline my instruction honestly if you cannot perform the instruction due
to physical, moral, legal reasons or your capability and explain the reasons.
Unless I say the task is completed, you should always start with:
Solution: <YOUR_SOLUTION>
<YOUR_SOLUTION> should be very specific, include detailed explanations and
provide preferable detailed implementations and examples and lists for task-solving.
Always end <YOUR_SOLUTION> with: Next request.’)AI User sys message:
SystemMessage(role_name=’A programming teacher’, role_type = <RoleType.USER:
’user’>, meta_dict = {’task’: ’A student will assist a programming teacher in designing
a series of 50 Python coding exercises that emphasize the application and optimization
of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
from easy to medium to hard, allowing students to strengthen their sorting skills
progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A programming teacher’},
role=’system’, content=’Never forget you are a A programming teacher and I am a A
student. Never flip roles! You will always instruct me.
We share a common interest in collaborating to successfully complete a task.
I must help you to complete the task.
Here is the task: A student will assist a programming teacher in designing a series of
50 Python coding exercises that emphasize the application and optimization of Bubble
Sort algorithm. These exercises will span varying levels of difficulty, from easy to
medium to hard, allowing students to strengthen their sorting skills progressively..
Never forget our task!
You must instruct me based on my expertise and your needs to solve the task ONLY
in the following two ways:
1. Instruct with a necessary input:
Instruction: <YOUR_INSTRUCTION>
Input: <YOUR_INPUT>
2. Instruct without any input:
Instruction: <YOUR_INSTRUCTION>
Input: None
The "Instruction" describes a task or question. The paired "Input" provides further
context or information for the requested "Instruction".
You must give me one instruction at a time.
I must write a response that appropriately solves the requested instruction.
I must decline your instruction honestly if I cannot perform the instruction due to
physical, moral, legal reasons or my capability and explain the reasons.
You should instruct me not ask me questions.
Now you must start to instruct me using the two ways described above.
Do not add anything else other than your instruction and the optional corresponding
input!
Keep giving me instructions and necessary inputs until you think the task is completed.
When the task is completed, you must only reply with a single word
<CAMEL_TASK_DONE>.
Never say <CAMEL_TASK_DONE> unless my responses have solved your task.’)
20

--- PAGE 21 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
B Convergence Speed of MFT Training Process
3000 4000 5000 6000 7000 8000 9000 100000.0970.0980.0990.1000.1010.1020.103
(a) Code Comment Task
3000 4000 5000 6000 7000 8000 9000 100000.2760.2780.2800.2820.284
 (b) Code Completion Task
3000 4000 5000 6000 7000 8000 9000 100000.1020.1040.1060.1080.1100.112
(c) Code Translation Task
3000 4000 5000 6000 7000 8000 9000 100000.5750.5760.5770.5780.5790.5800.5810.582
 (d) Text-to-Code Task
3000 4000 5000 6000 7000 8000 9000 100000.1850.1900.1950.2000.2050.210
(e) Unit Test-case Generation Task
3000 4000 5000 6000 7000 8000 9000 100000.1840.1860.1880.1900.1920.194 (f) Overall of 5 tasks
Figure 7: Validation Loss Convergence Speeds: A Comparative Analysis of 5 Code-related Downstream Tasks and
Overall Training Progress using the data-balanced loss function.
21

--- PAGE 22 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
C Examples of codefuse-codellama-34b across various tasks
C.1 Code Completion on HumanEval
Prompt:
<|role_start|>human<|role_end|># language: Python
from typing import List
def below_zero ( operations : List [int ]) -> bool :
""" You ’re given a list of deposit and withdrawal operations on a bank
account that starts with
zero balance . Your task is to detect if at any point the balance of
account fallls below zero , and
at that point function should return True . Otherwise it should return
False .
>>> below_zero ([1 , 2, 3])
False
>>> below_zero ([1 , 2, -4, 5])
True
"""
<|role_start|>bot<|role_end|>
CodeFuse-CodeLLama-34B:
balance = 0
for op in operations :
balance += op
if balance < 0:
return True
return False
C.2 Multilingual Code Completion on HumanEval-X
Prompt (Java):
<|role_start|>human<|role_end|>// language: Java
import java . util .*;
import java . lang .*;
class Solution {
/**
Given a string s, count the number of uppercase vowels in even indices .
For example :
countUpper (" aBCdEf ") returns 1
countUpper (" abcdefg ") returns 0
countUpper (" dBBE ") returns 0
*/
public int countUpper ( String s) {
<|role_start|>bot<|role_end|>
22

--- PAGE 23 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
CodeFuse-CodeLLama-34B:
int count = 0;
for (int i = 0; i < s. length (); i += 2) {
char c = s. charAt (i);
if ( Character . isUpperCase (c) && (c == ’A’ || c == ’E’ || c == ’I’
|| c == ’O’ || c == ’U’)) {
count ++;
}
}
return count ;
}
}
Prompt (Golang):
<|role_start|>human<|role_end|>// language: Go
import (
" strings "
)
// Create a function Encrypt that takes a string as an argument and
// returns a string Encrypted with the alphabet being rotated .
// The alphabet should be rotated in a manner such that the letters
// shift down by two multiplied to two places .
// For example :
// Encrypt (’hi ’) returns ’lm ’
// Encrypt (’ asdfghjkl ’) returns ’ewhjklnop ’
// Encrypt (’gf ’) returns ’kj ’
// Encrypt (’et ’) returns ’ix ’
func Encrypt (s string ) string {
<|role_start|>bot<|role_end|>
CodeFuse-CodeLLama-34B:
encrypted := make ([] byte , len(s))
for i, c := range s {
if c >= ’a’ && c <= ’z’ {
encrypted [i] = byte ((c-’a’+2) %26 + ’a’) // shift down two places
} else if c >= ’A’ && c <= ’Z’ {
encrypted [i] = byte ((c-’A’+2) %26 + ’A’)
} else {
encrypted [i] = byte (c) // non - alphabetic characters remain the
same
}
}
return string ( encrypted )
}
23

--- PAGE 24 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
C.3 Code Completion with Chinese prompts on CodeFuseEval-CN
Prompt:
<|role_start|>human<|role_end|># language: Python
<|role_start|>bot<|role_end|>
CodeFuse-CodeLLama-34B:
encoded_message = ’’
vowels = {’a’: ’y’, ’A’: ’Y’, ’e’: ’c’, ’E’: ’C’, ’i’: ’g’, ’I’: ’G’, ’o’
: ’m’, ’O’: ’M’, ’u’: ’s’, ’U’: ’S’}
for letter in message :
if letter . isalpha ():
if letter in vowels :
encoded_message += vowels [ letter ]
elif letter . islower ():
encoded_message += letter . upper ()
elif letter . isupper ():
encoded_message += letter . lower ()
else :
encoded_message += letter
return encoded_message
}
C.4 Text-to-Code on MBPP
Prompt:
<|role_start|>human<|role_end|> Write a function to find the number of subsequences having product smaller
than k for the given non negative array. Your code should satisfy these tests:
assert no_of_subsequences ([1 ,2 ,3 ,4] , 10) == 11
assert no_of_subsequences ([4 ,8 ,7 ,2] , 50) == 9
assert no_of_subsequences ([5 ,6 ,7 ,8] , 15) == 4
<|role_start|>bot<|role_end|>
24

--- PAGE 25 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
CodeFuse-CodeLLama-34B:
def no_of_subsequences (arr , k):
n = len( arr)
dp = [0] * n
dp [0] = 1
for i in range (1, n):
dp[i] = dp[i -1]
if arr[i] < k:
dp[i] += 1
for j in range (i):
if arr[j] < k// arr[i]:
dp[i] += dp[j]
return dp[n -1]
import unittest
class TestNoOfSubsequences ( unittest . TestCase ):
def test_no_of_subsequences ( self ):
self . assertEqual ( no_of_subsequences ([1 ,2 ,3 ,4] , 10) , 111)
self . assertEqual ( no_of_subsequences ([4 ,8 ,7 ,2] , 50) , 9)
self . assertEqual ( no_of_subsequences ([5 ,6 ,7 ,8] , 15) , 4)
if __name__ == " __main__ ":
unittest . main ()
C.5 Data Science Analysis on DS-1000
Prompt:
I’m using tensorflow 2.10.0.
I have a tensor of lengths in tensorflow, let’s say it looks like this:
[4, 3, 5, 2]
I wish to create a mask of 1s and 0s whose number of 1s correspond to the entries to this tensor, padded in front
by 0s to a total length of 8. I.e. I want to create this tensor:
[[0. 0. 0. 0. 1. 1. 1. 1.]
[0. 0. 0. 0. 0. 1. 1. 1.]
[0. 0. 0. 1. 1. 1. 1. 1.]
[0. 0. 0. 0. 0. 0. 1. 1.]]
How might I do this?
A:
<code>
import tensorflow as tf
lengths = [4, 3, 5, 2]
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>
25

--- PAGE 26 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
CodeFuse-CodeLLama-34B:
max_length = tf. reduce_max ( lengths )
# Create a tensor of 1s and 0s
mask = tf. sequence_mask ( lengths , maxlen = max_length , dtype =tf. float32 )
# Pad the mask with 0s to a total length of 8
padded_mask = tf. pad(mask , [[0 , 0], [0, 8 - max_length ]], constant_values =0)
result = padded_mask
C.6 Code Translation on CodeFuseEval-CodeTrans
Prompt:
<|role_start|>human<|role_end|>
# Translate the following Java program to Python
# Java
import java . util .*;
import java . lang .*;
class Solution {
public List <List < Integer >> getRow (List <List < Integer >> lst , int x) {
List <List < Integer >> coords = new ArrayList < >();
for (int i = 0; i < lst. size (); i ++) {
List <List < Integer >> row = new ArrayList < >();
for (int j = lst .get(i). size () - 1; j >= 0; j --) {
if (lst.get(i).get (j) == x) {
row .add( Arrays . asList (i, j));
}
}
coords . addAll (row);
}
return coords ;
}
}
# Python
<|role_start|>bot<|role_end|>
CodeFuse-CodeLLama-34B:
def get_row (lst , x):
coords = []
for i in range (len (lst)):
for j in range (len (lst[i]) -1, -1, -1):
if lst[i][j] == x:
coords . append ([i, j])
return coords
26

--- PAGE 27 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
C.7 Unit test case Generation on CodeFuseEval-UnitTest
Prompt:
<|role_start|>human<|role_end|>
Write unit tests for the following code , without providing explanations :
‘‘‘
public class ExceptionUtils {
private static final String [] CAUSE_METHOD_NAMES = {
" getCause ",
" getNextException ",
" getTargetException ",
" getException ",
" getSourceException ",
" getRootCause ",
" getCausedByException ",
" getNested ",
" getLinkedException ",
" getNestedException ",
" getLinkedCause ",
" getThrowable ",
};
public static Throwable getCause ( final Throwable throwable , String []
methodNames ) {
if ( throwable == null ) {
return null ;
}
if ( methodNames == null ) {
final Throwable cause = throwable . getCause ();
if ( cause != null ) {
return cause ;
}
methodNames = CAUSE_METHOD_NAMES ;
}
return Stream .of( methodNames ).map(m -> getCauseUsingMethodName (
throwable , m)). filter ( Objects :: nonNull ). findFirst (). orElse ( null );
}
}
‘‘‘
<|role_start|>bot<|role_end|>
27

--- PAGE 28 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
CodeFuse-CodeLLama-34B:
‘‘‘
import org. junit . Assert ;
import org. junit . Test ;
import java . util . Objects ;
import java . util . stream . Stream ;
public class ExceptionUtilsTest {
@Test
public void testGetCause_NullThrowable () {
Throwable throwable = null ;
String [] methodNames = {" getCause "};
Throwable output = ExceptionUtils . getCause ( throwable , methodNames );
Assert . assertEquals (null , output );
}
@Test
public void testGetCause_NullMethodNames () {
Throwable throwable = new RuntimeException (" test ");
String [] methodNames = null ;
Throwable output = ExceptionUtils . getCause ( throwable , methodNames );
Assert . assertEquals (null , output );
}
@Test
public void testGetCause_FoundCause () {
Throwable throwable = new RuntimeException (" test ", new
NullPointerException ());
String [] methodNames = {" getCause "};
Throwable output = ExceptionUtils . getCause ( throwable , methodNames );
Assert . assertEquals ( NullPointerException .class , output . getClass ());
}
@Test
public void testGetCause_NotFoundCause () {
Throwable throwable = new RuntimeException (" test ");
String [] methodNames = {" getCause "};
Throwable output = ExceptionUtils . getCause ( throwable , methodNames );
Assert . assertEquals (null , output );
}
@Test
public void testGetCause_DefaultMethodNames_FoundCause () {
Throwable throwable = new RuntimeException (" test ", new
NullPointerException ());
Throwable output = ExceptionUtils . getCause ( throwable );
Assert . assertEquals ( NullPointerException .class , output . getClass ());
}
@Test
public void testGetCause_DefaultMethodNames_NotFoundCause () {
Throwable throwable = new RuntimeException (" test ");
Throwable output = ExceptionUtils . getCause ( throwable );
Assert . assertEquals (null , output );
}
private Throwable getCauseUsingMethodName ( Throwable throwable , String
methodName ) {
try {
return ( Throwable ) throwable . getClass (). getMethod ( methodName ).
invoke ( throwable );
} catch ( Exception e) {
return null ;
}
}
}
‘‘‘
28

--- PAGE 29 ---
MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning A P REPRINT
D Evaluation results of CodeFuse-QWen-14B on NLP benchmark datasets
Table 12: Comparisons of the performances of CodeFuse-QWen-14B, QWen-14B, and QWen-14b-chat on several
NLP evaluation datasets. QWen-14B is a base model trained by Alibaba Cloud, QWen-14B-chat is a model fine-tuned
by themselves on top of the QWen-14B base model, and CodeFuse-QWen-14B is a model fine-tuned by us using
MFTCoder and code data.
QWen-14B QWen-14B-chat CodeFuse-QWen-14B
LANGUAGEAFQMC 69.00% 72.6% 71.99%
CHID 84.7% 72.3% 84.42%
Wic 50.9% 50.5% 55.02%
WSC 66.3% 66.3% 68.27%
Average 67.73% 65.43% 69.93%
REASONINGCOPA 93% 89% 95.00%
CMNLI 62.1% 60.3% 55.33%
OCNLI 58.2% 53.8% 49.22%
AX-b 49.5% 58.5% 67.84%
AX-g 80.9% 52.5% 82.87%
RTE 71.5% 51.6% 76.53%
Average 69.20% 60.95% 71.13%
UNDERSTANDINGCSL 54.4% 55.6% 68.50%
C3 90.8% 91.7% 91.01%
EPRSTMT 86.9% 91.2% 84.92%
Average 77.37% 79.50% 81.48%
EXAMINATIONAX-b 67.9% 66.4% 64.27%
AX-g 71.7% 71.7% 68.98%
RTE 84.4% 80.3% 84.75%
Average 74.67% 72.80% 72.67%
29
