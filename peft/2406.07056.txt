# 2406.07056.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2406.07056.pdf
# File size: 449650 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Effectively Compress KV Heads for LLM
Hao Yu1,2Zelan Yang3Shen Li3Yong Li3Jianxin Wu1,2∗
1State Key Laboratory for Novel Software Technology, Nanjing University
2School of Artificial Intelligence, Nanjing University3Alibaba Inc.
yuh@lamda.nju.edu.cn
{yangzelan.yzl,litan.ls,jiufeng.ly}@alibaba-inc.com
wujx2001@gmail.com
Abstract
The advent of pre-trained large language models (LLMs) has revolutionized various
natural language processing tasks. These models predominantly employ an auto-
regressive decoding mechanism that utilizes Key-Value (KV) caches to eliminate
redundant calculations for previous tokens. Nevertheless, as context lengths and
batch sizes increase, the linear expansion in memory footprint of KV caches
becomes a key bottleneck of LLM deployment, which decreases generation speeds
significantly. To mitigate this issue, previous techniques like multi-query attention
(MQA) and grouped-query attention (GQA) have been developed, in order to
reduce KV heads to accelerate inference with comparable accuracy to multi-head
attention (MHA). Despite their effectiveness, existing strategies for compressing
MHA often overlook the intrinsic properties of the KV caches. In this work, we
explore the low-rank characteristics of the KV caches and propose a novel approach
for compressing KV heads. In particular, we carefully optimize the MHA-to-GQA
transformation to minimize compression error, and to remain compatible with
rotary position embeddings (RoPE), we also introduce specialized strategies for
key caches with RoPE. We demonstrate that our method can compress half or
even three-quarters of KV heads while maintaining performance comparable to
the original LLMs, which presents a promising direction for more efficient LLM
deployment in resource-constrained environments.
1 Introduction
In recent years, the emergence of pre-trained large language models (LLMs) [ 16;34;39] has been a
cornerstone in redefining performance benchmarks across various natural language processing (NLP)
tasks. These models frequently exhibit capabilities that are on par with human levels of comprehension
and generation. In general, LLMs are built upon the neural structure of Transformers [ 35], which
requires a computational cost quadratic to the input sequence’s length and makes long sequence
inference intractable. To mitigate this issue, the auto-regressive decoding LLMs support Key-Value
(KV) caches, i.e., to cache the previous context’s intermediate key and value states in memory. KV
caches can avoid redundant computation of previous tokens, thereby expediting the inference process.
Nevertheless, all outputs of key and value weights for each block need to be cached during inference,
thus KV cache parameters are often extremely high. As a result, the expansion of sequence lengths
and batch sizes often causes a linear increase in memory footprint for past KV caches, further
resulting in the decoding process during LLM inference being memory-bound [ 19]. Furthermore, the
current trend to support longer contexts exacerbates this issue. Consequently, a sharp increase in KV
caches can significantly slow down model inference and such a heavy memory footprint presents a
key challenge in LLM deployments.
∗J. Wu is the corresponding author.
Preprint. Under review.arXiv:2406.07056v1  [cs.CL]  11 Jun 2024

--- PAGE 2 ---
To address these challenges, researchers proposed multi-query attention (MQA) [ 30] and grouped-
query attention (GQA) [ 1], which use only one key-value head to correspond to all or multiple query
heads, respectively. MQA and GQA can greatly reduce the sizes of KV caches during LLM inference,
and achieve comparable results as the original MHA mechanism. In particular, to effectively reduce
KV heads, the original GQA paper compares three compression strategies, i.e., randomly keeping
several heads, specifying the first head for each group, and averaging KV heads in one group. They
showed that directly mean-pooling KV head weights achieves the best accuracies. However, we find
that this compression strategy ignores the inherent characteristics of KV caches, thus resulting in
suboptimal model initialization. Therefore, all the training data and abundant GPU times are needed
to fine-tune the compressed model after averaging the KV heads. This high computing resource
requirement presents a huge challenge to compressing KV heads. As a direct consequence, attempts
to compress KV heads of pre-trained LLMs remain rare, and researchers now prefer to train GQA
models or compress KV caches directly.
In this paper, we investigate the low-rank property of KV caches and our observations reveal that
only a small number of top singular values need to be retained to keep most of the KV cache energy.
Inspired by this finding, we propose to compress KV heads with low-rank decomposition. Our
idea stems from a simple but crucial realization, i.e., when compressing a deep learning model,
we should focus on minimizing the loss of the model outputs, rather than the model weights [ 12].
We first group KV heads and perform SVD for each group of KV caches. Then we calculate low-
rank approximations for those KV caches. In general, these low-rank compression weights can be
incorporated into the original model to convert MHA into GQA pattern seamlessly. In addition, when
LLM applies RoPE, the original method of weight fusing to reduce key heads will be invalid. To
solve this issue, we propose several special strategies. In this case, although the compressed weights
could not be incorporated into the original model, those key caches are successfully compressed and
the generation speed of LLMs can still be improved.
Compared with average pooling KV head or low-rank approximating head weights directly, our
work can find a better initial weight for a compressed model, so KV caches can be effectively
compressed and fewer training samples and computing resources are needed to restore the precision
of the compressed model. We list our contributions as follows:
•We carefully study the characteristics of KV caches and prove the low-rank property of
KV caches. That is, keeping only a small number of singular values in the KV caches can
preserve most of the context information.
•To effectively compress KV caches, we propose a novel framework to reduce KV heads, i.e.,
convert the original MHA into GQA pattern by low-rank decomposition. Besides, we also
propose special strategies to deal with the attention layer with RoPE.
•Extensive experiments have proved the effectiveness of our approach. On different LLM
series models, our framework can compress half and even three-quarters of KV heads, and
maintain comparable results as the original models, proving its wide applicability and high
efficiency in different scenarios.
2 Related Work
Our work is connected to several themes in the literature, which we describe next.
2.1 Large Language Models (LLMs)
Large language models [ 3;16;34;39;7] are designed to understand and generate human languages.
In recent years LLMs have developed rapidly and consistently show excellent performances across
various NLP tasks. These breakthroughs in performance can be partly attributed to the powerful
modeling capabilities of its multi-head attention mechanism. To introduce positional information into
attention, researchers have proposed various positional embeddings. In this paper, we will show that
our framework can easily handle those models with different positional embeddings.
2

--- PAGE 3 ---
2.2 Weights Compression for Transformers
In general, the common building block of LLM is transformer [ 35], which tends to have a large number
of parameters and is computationally intensive. Weight compression technology can significantly
reduce memory footprint and speed up inference. Therefore, there is a lot of research work trying
to compress transformer models by various strategies, such as pruning, low-rank approximation,
knowledge distillation, etc. Mickel et al. [ 22] found that only a few heads have a significant effect
on translation tasks, and most heads can be pruned without any precision loss. AFM [ 12] low-rank
decomposed fully-connected (FC) weights by PCA. GQA [ 1] introduced grouped-query attention
and averaged KV head weights to convert a multi-head checkpoint into a multi-query checkpoint.
LLM-Pruner [ 37] applied gradients to estimate the importance of model weights and pruned less
significant coupled structures within the model based on this estimation. MiniLLM [ 11] proposed
the reverse KL loss to distill LLMs into smaller language models. To the best of our knowledge,
although weight compression is widespread, this is the first attempt to compress LLM’s KV heads
with low-rank approximation.
2.3 KV Cache Optimization
KV cache compression is harder than weight compression since they are more sensitive and are related
to model inputs. Some previous works tried to quantize KV caches. To achieve data-free distillation,
LLM-QAT [ 18] leveraged generations by a pre-trained model and quantized KV caches. Some
previous works dropped unimportant tokens with pre-designed criteria. H2O [40] and FastGen [10]
utilized accumulated attention scores for token importance and effectively reduced the cache size
by dropping tokens with lower scores. Scissorhands [ 19] found only some pivotal tokens have
a substantial influence at one step and significantly influence future generations. Gear [ 15] first
quantized KV caches and then employed a low-rank matrix to approximate the quantization error.
Multi-head Latent Attention (MLA) [ 7] introduced low-rank key-value joint compression, which
applied a latent vector to generate KV caches implicitly and required LLM to be trained from scratch,
while our focus is on optimizing the existing MHA mechanism. The effort to compress attention
heads remains scarce nowadays, and our work may potentially be combined with these previous
efforts to achieve a higher compression ratio.
3 Methods
We describe our framework in this section. First we introduce the traditional MHA & GQA algo-
rithms, and the naive algorithm that converts MHA to GQA. Then we present our finding that KV
caches generally emerge as a low-rank characteristic. Based on our finding, we present a KV head
compression framework that uses SVD to low-rank approximate KV caches. In addition, we also
present a comparison method that directly approximates KV head weights with SVD. Finally, we
propose several special policies to deal with RoPE.
3.1 Preliminaries
Transformer applies a multi-head attention (MHA) mechanism to capture different subspace repre-
sentations inside the input sequence. Giving an input x∈Rl×d, where landdare sequence lengths
and embedding dimensions. MHA performs the attention function in parallel with hheads, i.e.,
MHA( x) = [H1, . . . , H h]WO, (1)
Hi= Softmax( QiK⊤
i/p
dh)Vi, (2)
where [·]means concatenation operation. Qi=xWQi,Ki=xWKiandVi=xWViare query, key
and value matrices. WQi, WKiandWVi∈Rd×dhare learnable projection matrices of the i-th head.
dhis typically set to d/h. In particular, given an auto-regressive decoding LLM, the first prediction
step will generate a new token x0based on input x.KiandViat every head and every layer are
cached for subsequent generation, which results in the initial KV caches, i.e.,
K(0)= [K1, . . . , K h], (3)
V(0)= [V1, . . . , V h]. (4)
3

--- PAGE 4 ---
0.00.10.20.30.40.50.60.70.80.91.0
135791113151719212325272931RatioBlockIndexK Cache w/ RoPEK Cache w/o RoPEV Cache
0.00.10.20.30.40.50.60.70.80.91.0
135791113151719212325272931RatioBlock IndexK Cache w/ RoPEK Cache w/o RoPEV cacheFigure 1: Ratio of energy kept in each KV cache for LLaMA2-7B when 25% (left) and 50% (right)
dimensions are retained. The x-axis is the block index.
Here K(0), V(0)∈Rl×dare KV caches corresponding to each block. At step s(1≤s)of auto-
regressive decoding, LLM predicts a new token xsbased on the input and previously generated tokens.
In particular, MHA only needs to compute the query, key and value (i.e., qs, ks, vs∈R1×d) vector
for the newly generated token xsand append the corresponding key and value to the KV caches, i.e.,
K(s)=K(s−1)||ks, andV(s)=V(s−1)||vs, where K(s), V(s)∈R(l+s)×d. This strategy avoids the
recalculation of previous tokens and significantly increases the generation speed. However, the sizes
of KV caches can increase dramatically in long context inference, which results in speed degradation
due to heavy memory bandwidth overhead.
It can be seen that the sizes of KV caches are proportional to the number of KV heads. To effectively
reduce their memory footprint, previous researchers proposed multi-query attention (MQA) [ 30] and
grouped-query attention (GQA) [ 1]. In the original GQA paper, to compress MHA into the GQA
mechanism, they directly average key and value head weights in each group. hheads in an MHA
checkpoint will be divided into ggroups, where each group contains t=h/gheads. Then the new
i-th key ˜WKiand value ˜WVimatrices in GQA are calculated by
˜WKi= (WKi×t+WKi×t+1+···+WKi×t+t−1)/t, (5)
˜WVi= (WVi×t+WVi×t+1+···+WVi×t+t−1)/t, (6)
where i∈ {0,1,···, g−1}. However, after analyzing KV caches, we identify that the current
strategy is suboptimal as it overlooks the intrinsic characteristics of KV caches. Specifically, we have
discovered that KV caches typically exhibit a low-rank property. Based on our findings, we propose a
more efficient compression framework.
3.2 KV Caches are Low-rank!
To reveal the low-rank property of KV caches, we construct a small verification experiment, i.e., we
evaluate LLaMA2-7B [ 34] on the C4 [ 27] training dataset. In particular, we sample 128 sequences
from the C4 training set and each sample is 2048 tokens long. We perform model inference on
LLaMA2 and collect KV caches. KV cache sizes in each block are 262144 ×4096. Then we perform
SVD on those caches.
Figure 1 shows the percentage of the sum of these top singular values to the sum of all singular values
when 25% and 50% of the highest singular values are retained. In particular, we report the rank of
key cache that before (i.e., K Cache w/o RoPE) and after (i.e., K Cache w/ RoPE) performing RoPE.
Two conclusions can be drawn from this experiment. First, only 25% of the highest singular values
need to be retained to get most of the energy. Second, RoPE generally reduces the rank of key cache.
Those phenomenons indicate that for LLMs, KV caches are likely to be low-rank. To ensure that
most of the energy of KV caches is preserved, we only need to keep part of the output dimensions of
key and value head weights.
3.3 Effectively Transfer MHA to GQA
Now we present our compression algorithm. Inspired by PCA [ 36] and AFM [ 12], we compress KV
caches by taking advantage of low-rank approximation and then incorporate compression weights
4

--- PAGE 5 ---
Multi HeadQueriesKeysCompressLow-rankWeightMergeGrouped QueryFigure 2: Illustration of compressing key heads into GQA pattern. Note that the strategy of compress-
ing value heads is similar to this.
into the model. We illustrate how to compress the key heads in Figure 2. The strategy of reducing
value heads is similar to this, except that those compression weights are fused into the value matrices
WVand the output weight WO.
In particular, our compression algorithm needs a pre-designed dataset to calculate compression
weights. Given an MHA checkpoint with hhead, we collect KV caches for the calibration dataset
and divide the hheads’ caches into ggroups and set t=h/g, i.e.,
˜Ki= [Ki×t, Ki×t+1, . . . , K i×t+t−1] =x[WKi×t, WKi×t+1, . . . , W Ki×t+t−1], (7)
˜Vi= [Vi×t, Vi×t+1, . . . , V i×t+t−1] =x[WVi×t, WVi×t+1, . . . , W Vi×t+t−1], (8)
where ˜Ki,˜Vi∈Rl×tdhandi∈ {0,1,···, g−1}. Then, we perform SVD on those caches, i.e.,
˜Ki= ΦiΣiΨiand˜Vi= ΘiΛiΩi, where Φi,Θi∈Rl×tdhandΨi,Ωi∈Rtdh×tdhare orthonormal
matrices. Σi,Λi∈Rtdh×tdhare diagonal rectangular matrices containing singular values in the
decreasing order. Therefore, we can get the low-rank approximation of ˜Ki,˜Vias
˜Ki≈˜Ki(Ψi
dh)⊤Ψi
dh, (9)
˜Vi≈˜Vi(Ωi
dh)⊤Ωi
dh, (10)
where Ψi
dh,Ωi
dh∈Rdh×tdhare the top- dhrows of ΨiandΩi, respectively. Because KV caches
usually have abundant parameters, it is not practical to collect all caches and calculate SVD di-
rectly. Instead, we update ˜K⊤
i˜Kiand˜V⊤
i˜Viin a streamline fashion and then compute their eigen-
decompositions. Note that there is no non-linear layer between those key and value matrices and
(Ψi
dh)⊤&(Ωi
dh)⊤. Therefore, we can merge them directly and get the new i-th key and value
matrices in GQA as
˜WKi= [WKi×t, WKi×t+1, . . . , W Ki×t+t−1](Ψi
dh)⊤, (11)
˜WVi= [WVi×t, WVi×t+1, . . . , W Vi×t+t−1](Ωi
dh)⊤, (12)
where ˜WKi,˜WVi∈Rd×dh. With the above transformation, the number of KV heads can be reduced
from htog, so that the compression ratio of KV caches reaches 1−g/h. Besides, according to
Equations 1 and 2, there is also no nonlinear layer between Ψi
dhandWQ, orΩi
dhandWO. So
similarly, we can incorporate those low-rank weights into WQandWO. We set pto be the integer
part of i/t,q=i−t×pandi∈ {0,1, . . . , h −1}, then
˜WQi=WQi(Ψp
dh,q)⊤, (13)
˜WOi= Ωp
dh,qWOi, (14)
where Ψp
dh,q,Ωp
dh,q∈Rdh×dhis the q×dhthrough (q+ 1)×dhcolumns of Ψp
dhandΩp
dh.
WOi∈Rdh×drepresents the i×dhto(i+ 1)×dhrows of WO. We update the original weights
WQiandWOiinto˜WQiand˜WOi, respectively. We name this method SVD- a, as it performs SVD
on the output activations.
5

--- PAGE 6 ---
Except for this strategy which compresses KV heads with calibration sets, another low-rank compres-
sion strategy that does not require data is to directly perform SVD on KV weights, i.e.,
ˆWKi= [WKi×t, WKi×t+1, . . . , W Ki×t+t−1], (15)
ˆWVi= [WVi×t, WVi×t+1, . . . , W Vi×t+t−1]. (16)
Then we perform SVD on ˆWKiandˆWVi, i.e., ˆWKi=ˆΦiˆΣiˆΨiandˆWVi=ˆΘiˆΛiˆΩi. Similarly, we
keep the top- dhrows of ˆΨiandˆΩi, and then treat them as compressed weights to replace the original
ΨiandΩi. We treat this strategy as a baseline method and name it SVD- w, as it low-rank decomposes
model weight. Later we will show that this SVD- wis not as effective as our approach that uses KV
caches’ low-rank characteristic.
After obtaining those compressed models, we use LoRA [ 14] to fine-tune them. We will show
that by removing half or even three-quarters of KV heads, our compressed LLMs can still achieve
comparable accuracies as the original model. Note that our compression method is orthogonal to the
fine-tuning strategy. If more computing resources are available, full-parameter fine-tuning is also a
viable strategy to potentially achieve better accuracy.
3.4 Deal with RoPE
There is a special case when LLMs adopt RoPE [ 32], which inserts a relative position embedding
between WQandWK. At this time, the attention score calculation becomes
qmk⊤
n= (xmWqRd
θ,m)(xnWkRd
θ,n)⊤, (17)
where xi∈R1×dis the i-th (1≤i≤l−1) token in an input sequence x.Rd
θ,iis the rotary
embedding corresponding to xi.qmandknare the query and key vectors of xmandxn, respectively.
LLMs then view xnWkRd
θ,nas the key cache in practice. This will, however, cause the original
fusion for key cache invalid since it introduces extra position embedding between WkandWq.
To deal with this difficulty, we come up with several strategies. First, we do not divide key heads but
directly calculate Ψmatrices for the whole key cache in each block. This strategy avoids grouping
key heads, which reduces the algorithm’s constraints and can further improve the accuracy of the
compressed model. Second, instead of keeping the original key cache K, we calculate and preserve
˜K=K(Ψdh)⊤during inference. Although the compressed model is no longer in the GQA pattern,
key cache compression is achieved. Therefore, the original calculation becomes
qmk⊤
n= (xmWqRd
θ,m)Ψ⊤
dhΨdh(xnWkRd
θ,n)⊤. (18)
In particular, we will calculate (xmWqRd
θ,m)Ψ⊤
dhinstead of continuing to calculate Ψ⊤
dh˜K⊤, because
xonly contains one token during the generation process, but key cache often has many tokens. RoPE
does not involve value caches, so the compression strategy for value caches remains the same. Later
our experiments will prove that all our three strategies can reduce KV caches with faster generation.
4 Experiment
We now evaluate our methods in this section. More results can be found in the appendix.
4.1 Settings
Foundation models. We evaluate our framework on the 7B1 model of BLOOMZ [ 24], and the 7B,
13B models of LLaMA2 [ 34]. BLOOMZ is BLOOM’s [ 16] supervised fine-tuning (SFT) version
using the open-sourced xP3 [24] dataset.
Compression. For BLOOMZ-7B1, we sampled 1 ‰data from the xP3 dataset. The original xP3
includes 83.6M samples, thus we randomly sampled 83.6K sampled from it. For LLaMA2 models,
following QLoRA [ 8], we use the FLANv2 [ 20] dataset and extract 23K data from it. We concatenate
the input and output of each sample for inference, and then collect KV caches in each block to
calculate compression weights.
6

--- PAGE 7 ---
Table 1: Performances of BLOOMZ-7B1 with our KV heads compression framework.
#KV Heads Throughput (token/s)AccuracyA VG.XNLI XWinoGrad XCOPA XStoryCloze
32 8.56 39.73 51.49 51.03 54.25 47.25
16 14.41 39.48 51.47 52.97 54.06 47.85
8 23.24 39.03 50.45 51.60 52.37 46.84
4 34.78 38.45 50.29 50.27 50.89 45.92
Table 2: Performances of LLaMA2 models with our KV heads compression framework.
Model #KV Heads Throughput (token/s)Perplexity (↓) MMLU
W2 C4 0-shot 5-shot
LLaMA2-7B32 8.05 5.47 6.98 41.79 45.82
16 13.41 7.08 9.12 48.32 48.74
8 20.81 9.17 11.24 44.62 45.54
LLaMA2-13B40 5.04 4.88 6.47 52.12 55.17
20 8.60 6.51 8.01 53.65 54.71
10 13.93 8.40 9.86 49.65 50.73
Here we reduce half and three-quarters KV heads. The original BLOOMZ-7B1 and LLaMA2-7B
have MHA structures that contain 32 heads, and we compress all attention layers in the models to
16 or 8 heads. For BLOOMZ-7B1, we continue to reduce KV heads into 4. LLaMA2-13B contains
40 heads and we reduce them to 20 or 10 heads. Note that BLOOMZ uses ALiBi [ 26] and does not
involve RoPE so it is a standard GQA mechanism after compression.
Fine-tuning. We apply LoRA [ 14] to fine-tune the compressed model. For 7B size models, we set
LoRA’s r= 256 andα= 512 and fine-tune 1 epoch. For 13B size models, LoRA’s ris 512 and αis
256, and we fine-tune the compressed model for 2 epochs. In all experiments, we apply a constant
learning rate schedule and set the learning rate as 4e-5. We use 4 ×80G A100 GPU to fine-tune the
compressed model and set batch size per device as 1. We set the gradient accumulation steps as 16
and the LoRA dropout rate as 0.05. Weight decay is 0.05 and AdamW [ 21] is used. Note for the
BLOOMZ-7B1 model, we continue to apply the same 83.6K sub-dataset to fine-tune the compressed
model. For LLaMA2 models, we select 232K data from FLANv2 [20] as the training dataset.
Evaluation metrics. For BLOOMZ-7B1, we follow the evaluation metrics of the original paper,
i.e., we evaluate the zero-shot accuracy on XCOPA [ 25], XNLI [ 6], XWinoGrad [ 33] and XSto-
ryCloze [ 17] with Bulgarian (BG), German (DE), Greek (EL), Russian(RU), Thai (TH), Turkish (TR),
Japanese (JP), Estonian (ET), Haitian (HT), Italian (IT), Quechua (QU) and Burmese (MY) language
questions and English prompts. For LLaMA2 models, we evaluate the perplexity on WikiText2 [ 31]
and C4 [ 27]. We further assess the zero-shot commonsense question answering (QA) ability on
tasks covering SIQA [ 29], HellaSwag [ 38], PIQA [ 2], WinoGrande [ 28], ARC [ 5], BoolQ [ 4], and
OpenBookQA [ 23]. We also evaluate both the zero-shot and five-shot performance of the LLMs
on the Massively Multitask Language Understanding (MMLU) benchmark [ 13]. It consists of 57
language tasks including humanities, STEM, social science, etc. We adopt lm-eval-harness [ 9] to
produce the accuracy results. Besides, we also report throughput in an 80G A100 GPU. Since the
prefilling stage is computation-bound while the decoding stage is memory-bound, and here our goal is
to compress KV caches, we set the context length to 2048 and calculate throughput when generating
the 2050th token for simplicity.
4.2 Main Results
We report the number (#) of KV heads, throughput, and accuracies of BLOOMZ-7B1 and LLaMA2 in
Tables 1 and 2, respectively. Further accuracies of eight zero-shot commonsense question answering
datasets are shown in Table 3. Note that for BLOOMZ-7B1, the ‘A VG.’ column is not the direct
average accuracy of the four datasets, as each dataset contains a different number of sub-datasets.
Therefore, we report the average accuracy of those sub-datasets. Detailed results are shown in
the appendix. For LLaMA2 models, we abbreviate WikiText2, HellaSwag, WinoGrande, and
OpenBookQA to W2, HLSW, WG, and OBQA, respectively. ARC-e and ARC-c stand for ARC-easy
and ARC-challenge tasks, respectively.
7

--- PAGE 8 ---
Table 3: Accuracies in the commonsense QA datasets with different #KV heads on LLaMA2 models.
Model #KV Heads BoolQ PIQA SIQA HLSW WG ARC-e ARC-c OBQA Avg.
LLaMA2-7B32 77.77 79.05 32.91 76.00 69.22 74.58 46.25 44.20 62.50
16 83.61 77.97 32.80 72.53 73.56 78.45 49.66 46.40 64.37
8 80.58 76.77 32.91 66.67 67.88 73.32 43.34 43.80 60.66
LLaMA2-13B40 80.61 80.52 33.11 79.38 72.30 77.40 49.06 45.20 64.70
20 85.78 80.41 32.04 77.84 74.59 80.22 54.35 46.80 66.50
10 83.24 78.45 33.11 73.00 70.80 75.97 49.06 41.80 63.18
Table 4: Performances of LLaMA2 models with different initialization strategies.
#KV Heads Methods StagePerplexity (↓) MMLUQA A VG.W2 C4 0-shot 5-shot
16Mean-poolInitialization 1079.43 625.68 22.94 24.94 34.88
Fine-tune 1079.43 625.68 35.25 35.41 56.57
SVD- wInitialization 807.62 624.78 23.11 23.17 35.46
Fine-tune 10.44 12.28 42.46 39.38 62.10
SVD- aInitialization 13.57 18.57 28.05 25.97 48.87
Fine-tune 7.08 9.12 48.32 48.74 64.37
8Mean-poolInitialization 4113.79 2381.27 25.00 24.95 34.69
Fine-tune 63.32 37.87 26.63 25.30 40.79
SVD- wInitialization 3888.63 2350.09 23.42 22.94 35.43
Fine-tune 15.44 17.13 24.30 23.69 55.86
SVD- aInitialization 194.61 141.84 23.61 23.93 37.59
Fine-tune 9.17 11.24 44.62 45.54 60.66
As these results exhibited, when compressing half of KV heads, our algorithm can maintain the same
or even higher accuracy, while guaranteeing more than 50% throughput improvement at the decoding
process. This phenomenon indicates that our framework is a powerful solution in scenarios where
memory efficiency is required. When further compressing three-quarters of KV heads, BLOOM-7B1
and LLaMA2-13B lost more KV cache information, resulting in a slight accuracy drop. However,
those results are still comparable with the original models’ accuracies. These results demonstrate
that our approach is an effective strategy to compress KV heads and reduce KV cache sizes, thereby
alleviating the heavy memory bandwidth pressure during the LLM generation phase.
4.3 Ablation Studies
We further perform several analyses to explore the impact of different modules of our method.
Comparing different compression strategies. Here we compare three different compression
strategies, namely directly mean-pooling KV head weights [ 1], performing SVD for KV head weights
(i.e., SVD- w), and our methods (i.e., SVD- a). All other settings are consistent except for the
compression strategy. We apply LLaMA2-7B with 16 and 8 KV heads and report the results of direct
compression (i.e., the ‘Initialization’ rows) and further fine-tuning (i.e., the ‘Fine-tune’ rows). Due to
page limitation, we report the average accuracy of eight zero-shot commonsense QA datasets here
(i.e., the ‘QA A VG.’ column).
Table 5: Performances of LLaMA2 models with different initialization data sizes.
#KV Heads DataPerplexity (↓) MMLUQA A VG.W2 C4 0-shot 5-shot
1623K 13.57 18.57 28.05 25.97 48.87
46K 13.83 18.89 27.96 25.89 48.50
92K 13.93 18.96 28.11 25.98 48.66
232K 13.92 18.93 27.89 25.87 48.70
823K 194.61 141.84 23.61 23.93 37.59
46K 197.23 142.80 23.76 23.81 37.46
92K 202.05 145.44 23.62 23.79 37.34
232K 202.38 145.43 23.74 24.01 37.48
8

--- PAGE 9 ---
Table 6: Performances of LLaMA2 models with different fine-tuning data sizes.
#KV Heads DataPerplexity (↓) MMLUQA A VG.W2 C4 0-shot 5-shot
1623K 7.29 9.41 43.49 44.16 61.93
46K 7.38 9.35 42.42 44.62 61.53
92K 7.26 9.20 45.00 46.36 62.57
232K 7.08 9.12 48.32 48.74 64.37
823K 9.62 12.11 32.71 35.37 57.13
46K 9.48 11.84 39.93 40.10 57.53
92K 9.73 11.50 40.53 42.90 59.04
232K 9.17 11.24 44.62 45.54 60.66
Table 7: LLaMA2: different epoch v.s. data.
#KV Heads Data EpochPerplexity (↓) MMLUQA A VG.W2 C4 0-shot 5-shot
1646K 1 7.38 9.35 42.42 44.62 61.53
46K 2 7.64 9.60 45.37 45.77 62.90
92K 1 7.26 9.20 45.00 46.36 62.57
846K 1 9.48 11.84 39.93 40.10 57.53
46K 2 9.56 12.72 41.57 41.77 58.76
92K 1 9.73 11.50 40.53 42.90 59.04
As we can see from Table 4, both mean-pooling and SVD- wlead to collapse before fine-tuning, and
our method far exceeds them both before & after fine-tuning. Those results reveal the effectiveness
of our algorithm. That is, with limited computing and data resources, our SVD- acan significantly
reduce KV cache sizes and are very practical, while other strategies are not.
Influence of different data sizes for initialization and fine-tuning. Here we investigate the effect
of the size of different data sets. Similarly, we apply LLaMA2-7B with 16 and 8 KV heads as the
baseline. In particular, in our original settings, we calculate the compression weights with 23K
samples for initialization and fine-tune the compression model with 232K samples. Here we sample
46K and 92K data from the original FLANv2 dataset and calculate the compression weights. Then
based on the model compressing with 23K samples, we continue to fine-tune the model with 23K,
46K, 92K, and 232K samples respectively.
Results of different dataset sizes on model initialization and fine-tuning are shown in Tables 5 and 6,
respectively. As we can see, the data size required for initialization is not high, while the fine-tuning
process is relatively more data-hungry. During initialization, excess data does not necessarily yield
additional benefits. However, the more data for fine-tuning, the better the compression model will
perform. This phenomenon reveals that our method can further achieve higher precision if there are
more data and computational resources.
Enlarge data sizes v.s. increase training epochs. Here we investigate the effect of training time
on the results, i.e., given the same training GPU resources, whether it is more beneficial to continue
enlarging the training dataset size or increasing the training epoch. Similarly, we train the compressed
LLaMA2-7B with 2 epoch and 46K training samples, and compare the results of training 1 epoch
with 92K data. The results are shown in Table 7. It can be seen that the addition of high-quality
samples is more helpful to the model accuracies than increasing training epochs.
5 Conclusion, Limitation, and Future Work
In this paper, we proposed a low-rank decomposition framework to compress KV heads. We first
discovered that KV caches are low-rank, and based on this finding we converted the original MHA
architecture to GQA mechanism by low-rank decomposition. We also proposed several special
strategies to handle the attention layer with RoPE. With half or even three-quarters of KV cache
compressed, our approach can recover LLM’s accuracy with limited training and data resources,
while this is not possible with the previous KV head compression method.
9

--- PAGE 10 ---
Although our compression framework can significantly reduce KV caches and improve the speed
of LLM generation, our approach currently has some shortcomings. First of all, when our method
is used in the attention layer with RoPE, the key head compression weights cannot be incorporated
into the original model, resulting in additional parameters and computational overhead, which will
slightly slow down the prefill stage. So how to compress key heads with RoPE more elegantly will be
an interesting direction in the future. In addition, when heavily compressing KV heads or the model
size is too large, LLM’s accuracy will decrease to some extent. Therefore, how to better deal with
these situations is also a future direction. Besides, our approach can potentially be combined with
previous KV cache compression methods to achieve an extreme KV cache compression ratio, which
we leave as a viable direction for the future.
References
[1]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit
Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages
4895–4901, 2023.
[2]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physi-
cal commonsense in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence ,
pages 7432–7439, 2020.
[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
InAdvances in Neural Information Processing Systems 33 , volume 33, pages 1877–1901, 2020.
[4]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of
the Conference of the North American Chapter of the Association for Computational Linguistics , pages
2924–2936, 2019.
[5]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457 , 2018.
[6]Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk,
and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing , pages 2475–2485, 2018.
[7]DeepSeek-AI. DeepSeek-V2: A strong, economical, and efficient mixture-of-experts language model.
arXiv preprint arXiv:2405.04434 , 2024.
[8]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of
quantized llms. In Advances in Neural Information Processing Systems 36 , volume 36, pages 10088–10115,
2024.
[9]Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,
Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model
evaluation, 2021.
[10] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you
what to discard: Adaptive KV cache compression for LLMs. In International Conference on Learning
Representations , 2024.
[11] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language
models. In International Conference on Learning Representations , 2024.
[12] Yu Hao and Wu Jianxin. Compressing transformers: Features are low-rank, but weights are not! In
Proceedings of the AAAI Conference on Artificial Intelligence , pages 11007–11015, 2023.
[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning
Representations , 2021.
[14] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning
Representations , 2021.
10

--- PAGE 11 ---
[15] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao.
Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint
arXiv:2403.05527 , 2024.
[16] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, et al. Bloom: A 176b-parameter open-access multilingual language
model. arXiv preprint arXiv:2211.05100 , 2022.
[17] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,
Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov,
and Xian Li. Few-shot learning with multilingual generative language models. In Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing , pages 9019–9052, 2022.
[18] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,
Raghuraman Krishnamoorthi, and Vikas Chandra. LLM-QAT: Data-free quantization aware training for
large language models. arXiv preprint arXiv:2305.17888 , 2023.
[19] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis,
and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv
cache compression at test time. In Advances in Neural Information Processing Systems 36 , volume 36,
pages 52342–52364, 2023.
[20] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,
Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction
tuning. In International Conference on Machine Learning , pages 22631–22648, 2023.
[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2018.
[22] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Advances in
Neural Information Processing Systems 32 , volume 32, 2019.
[23] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?
a new dataset for open book question answering. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing , pages 2381–2391, 2018.
[24] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through
multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 15991–16111, 2023.
[25] Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vuli ´c, and Anna Korhonen.
XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pages 2362–2376, 2020.
[26] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input
length extrapolation. In International Conference on Learning Representations , 2022.
[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
Journal of machine learning research , 21(140):1–67, 2020.
[28] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial
winograd schema challenge at scale. In Communications of the ACM , pages 99–106, 2021.
[29] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense
reasoning about social interactions. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , pages 4463–4473, 2019.
[30] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint
arXiv:1911.02150 , 2019.
[31] Merity Stephen, Xiong Caiming, Bradbury James, Socher Richard, et al. Pointer sentinel mixture models.
InInternational Conference on Learning Representations , 2017.
[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.
11

--- PAGE 12 ---
[33] Alexey Tikhonov and Max Ryabinin. It’s All in the Heads: Using Attention Heads as a Baseline for
Cross-Lingual Transfer in Commonsense Reasoning. In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021 , pages 3534–3546, 2021.
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 , 2023.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems 30 , volume 30, pages 5998–6008, 2017.
[36] Jianxin Wu. Essentials of Pattern Recognition: An Accessible Approach . Cambridge University Press,
2020.
[37] Gongfan Fang Xinyin Ma and Xinchao Wang. Llm-pruner: On the structural pruning of large language
models. In Advances in Neural Information Processing Systems 36 , volume 36, pages 21702–21720, 2023.
[38] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really
finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , page 4791–4800, 2019.
[39] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, et al. OPT: Open pre-trained transformer language models. arXiv preprint
arXiv:2205.01068 , 2022.
[40] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong
Tian, Christopher Ré, Clark Barrett, Zhangyang "Atlas" Wang, and Beidi Chen. H2O: Heavy-hitter oracle
for efficient generative inference of large language models. In Advances in Neural Information Processing
Systems 36 , volume 36, pages 34661–34710, 2023.
12

--- PAGE 13 ---
A The running time of compression
Here we report the time required to compress LLaMA2 models. The results are shown in Table 8.
Numbers in the table are measured in hours. As we can see, our algorithm requires very little time
to compute the compression weights, and the bottleneck of the entire framework is the time spent
fine-tuning the compressed models.
Table 8: Hours needed to calculate compressing weights and fine-tune the model.
SizeInitialization Fine-tuningSizeInitialization Fine-tuning
Head = 16 Head = 8 Head = 16 Head = 8 Head = 20 Head = 10 Head = 20 Head = 10
7B 0.30 43.40 36.14 13B 0.68 103.47 87.01
B Detailed results of compressed models
In Table 9 we report detailed accuracies of BLOOMZ-7B1 and its compressed version on each
sub-dataset. For LLaMA2 models in the MMLU datasets, we report the accuracies in Table 10.
Table 9: Detailed accuracies of BLOOMZ-7B1 with different KV heads.
#KV Heads Dataset Average ACC.
32XNLI XWinoGrad
47.25BG DE EL RU TH TR JP RU
40.30 42.80 38.74 42.70 37.69 36.15 50.26 52.70
XCOPA XStoryCloze
ET HT IT QU TH TR MY RU
48.20 49.40 57.00 48.00 55.20 48.40 49.46 59.03
16XNLI XWinoGrad
47.85BG DE EL RU TH TR JP RU
39.86 42.92 38.51 42.62 36.99 36.00 50.43 52.51
XCOPA XStoryCloze
ET HT IT QU TH TR MY RU
50.00 51.40 57.20 51.40 59.20 48.60 50.09 58.03
8XNLI XWinoGrad
46.84BG DE EL RU TH TR JP RU
39.27 42.16 38.35 41.57 37.00 35.85 49.16 51.75
XCOPA XStoryCloze
ET HT IT QU TH TR MY RU
50.00 49.80 55.60 51.40 54.40 48.40 49.34 55.39
4XNLI XWinoGrad
45.92BG DE EL RU TH TR JP RU
38.82 40.76 37.70 41.43 36.38 35.61 48.91 51.68
XCOPA XStoryCloze
ET HT IT QU TH TR MY RU
49.20 47.40 48.80 51.20 55.60 49.40 47.85 53.94
Table 10: Detailed accuracies of LLaMA2 models with different KV heads in MMLU datasets.
Model #KV HeadsMMLU (0-shot) MMLU (5-shot)
Hums. STEM Social Other Avg. Hums. STEM Social Other Avg.
LLaMA2-7B32 39.64 34.25 47.35 47.18 41.79 43.32 36.98 51.77 52.69 45.82
16 44.99 39.42 55.96 54.81 48.32 44.48 39.93 56.68 56.26 48.74
8 41.06 37.36 52.58 49.50 44.62 41.11 38.31 54.01 51.21 45.54
LLaMA2-13B40 47.99 42.21 61.23 59.41 52.12 53.43 43.84 63.21 61.35 55.17
20 49.54 43.48 62.98 60.93 53.65 50.97 44.24 63.99 61.83 54.71
10 46.25 40.22 57.62 56.49 49.65 46.89 41.42 59.90 56.90 50.73
C Detailed results of different compression strategies
In Tables 11 and 12, we compare the influence of three different compression strategies in detail,
which can better reflect the advantage of our compression strategy. Although the model accuracy
13

--- PAGE 14 ---
collapses after direct compression, our compression model can quickly recover the accuracy after
further fine-tuning. These results demonstrate the effectiveness of our framework.
Table 11: Detailed accuracies in MMLU of LLaMA2 models with different initialization strategies.
#KV Heads Methods StageMMLU (0-shot) MMLU (5-shot)
Hums. STEM Social Other Avg. Hums. STEM Social Other Avg.
16Mean-poolInit. 24.21 21.22 21.71 23.98 22.94 24.21 21.22 21.71 23.98 22.94
Fine-tune 32.99 31.18 37.60 40.49 35.25 33.24 20.83 38.25 40.52 35.41
SVD- wInit. 24.63 21.44 21.94 23.66 23.11 24.21 21.73 21.84 24.40 23.17
Fine-tune 38.98 34.09 49.17 49.57 42.46 37.98 30.76 44.59 45.09 39.38
SVD- aInit. 25.06 29.72 31.04 27.94 28.05 25.48 24.10 24.44 30.13 25.97
Fine-tune 44.99 39.42 55.96 54.81 48.32 44.48 39.93 56.68 56.26 48.74
8Mean-poolInit. 24.82 24.52 24.80 25.97 25.00 24.23 27.40 23.56 24.23 24.95
Fine-tune 23.80 29.24 30.26 23.80 26.63 24.12 29.08 25.22 23.30 25.30
SVD- wInit. 24.00 22.49 22.62 24.27 23.42 24.40 21.38 21.61 23.62 22.94
Fine-tune 24.97 23.28 23.14 25.46 24.30 24.25 23.88 22.07 24.24 23.69
SVD- aInit. 24.08 22.90 23.53 23.69 23.61 24.08 24.55 22.91 24.07 23.93
Fine-tune 41.06 37.36 52.58 49.50 44.62 41.11 38.31 54.01 51.21 45.54
Table 12: Detailed accuracies in zero-shot commonsense QA datasets of LLaMA2 models with
different initialization strategies.
#KV Heads Methods StageCommonsense QA (0-shot)A VG.BoolQ PIQA SIQA HLSW WG ARC-e ARC-c OBQA
16Mean-poolInit. 37.80 50.76 32.70 26.47 50.28 26.68 26.71 27.60 34.88
Fine-tune 73.27 74.37 34.29 60.44 62.51 70.12 40.36 37.20 56.57
SVD- wInit. 38.47 53.10 31.58 27.88 51.62 28.83 24.23 28.00 35.46
Fine-tune 79.45 77.80 32.96 69.20 69.69 76.30 46.59 44.80 62.10
SVD- aInit. 60.21 69.26 33.27 51.93 59.67 51.60 31.83 33.20 48.87
Fine-tune 83.61 77.97 32.80 72.53 73.56 78.45 49.66 46.40 64.37
8Mean-poolInit. 37.83 49.89 33.06 26.28 48.93 26.22 27.30 28.00 34.69
Fine-tune 60.03 58.38 32.91 38.48 50.75 37.92 24.49 23.40 40.79
SVD- wInit. 42.60 50.76 34.14 26.59 51.22 26.39 25.94 25.80 35.43
Fine-tune 69.60 73.99 32.75 60.30 63.93 70.24 39.68 36.40 55.86
SVD- aInit. 46.33 56.96 32.80 30.30 51.70 32.53 24.32 25.80 37.59
Fine-tune 80.58 76.77 32.91 66.67 67.88 73.32 43.34 43.80 60.66
14
