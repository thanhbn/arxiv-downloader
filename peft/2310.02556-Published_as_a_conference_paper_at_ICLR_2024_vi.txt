# 2310.02556.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2310.02556.pdf
# Kích thước tệp: 1260654 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
NOLA: NÉN LORA BẰNG CÁCH SỬ DỤNG TỔ HỢP TUYẾN TÍNH
CỦA CƠ SỞ NGẪU NHIÊN
Soroush Abbasi Koohpayegani∗,1K L Navaneet∗,1
Parsa Nooralinejad1Soheil Kolouri2Hamed Pirsiavash1
1University of California, Davis2Vanderbilt University
TÓM TẮT
Tinh chỉnh các Mô hình Ngôn ngữ Lớn (LLM) và lưu trữ chúng cho từng tác vụ phụ
thuộc hoặc miền là không thực tế do kích thước mô hình khổng lồ (ví dụ: 350GB
trong GPT-3). Các nghiên cứu hiện tại, chẳng hạn như LoRA, thể hiện tiềm năng
của việc sửa đổi thứ hạng thấp đối với trọng số gốc của LLM, cho phép thích ứng
và lưu trữ hiệu quả cho các mô hình cụ thể theo tác vụ. Những phương pháp này
có thể giảm số lượng tham số cần thiết để tinh chỉnh LLM đi vài bậc độ lớn. Tuy
nhiên, những phương pháp này đối mặt với hai hạn chế chính: (1) số lượng tham
số bị giới hạn dưới bởi phân rã thứ hạng một, và (2) mức độ giảm bị ảnh hưởng
mạnh bởi cả kiến trúc mô hình và thứ hạng được chọn. Chúng tôi giới thiệu NOLA,
vượt qua giới hạn dưới thứ hạng một có trong LoRA. Nó đạt được điều này bằng
cách tham số hóa lại các ma trận thứ hạng thấp trong LoRA sử dụng tổ hợp tuyến
tính của các ma trận được tạo ngẫu nhiên (cơ sở) và chỉ tối ưu hóa các hệ số hỗn
hợp tuyến tính. Cách tiếp cận này cho phép chúng tôi tách rời số lượng tham số
có thể huấn luyện khỏi cả lựa chọn thứ hạng và kiến trúc mạng. Chúng tôi trình
bày kết quả thích ứng sử dụng GPT-2, LLaMA-2, và ViT trong các tác vụ xử lý
ngôn ngữ tự nhiên và thị giác máy tính. NOLA hoạt động tốt như các mô hình
LoRA với số lượng tham số ít hơn nhiều so với LoRA với thứ hạng một, khả năng
nén tốt nhất mà LoRA có thể đạt được. Đặc biệt, trên LLaMA-2 70B, phương pháp
của chúng tôi nhỏ gọn gần 20 lần so với LoRA nén nhất mà không làm giảm độ
chính xác. Mã nguồn của chúng tôi có sẵn tại: https://github.com/UCDvision/NOLA

1 GIỚI THIỆU
Các mạng neural được huấn luyện trước lớn đã thể hiện khả năng tổng quát hóa đáng chú ý trên nhiều tác vụ phụ thuộc đa dạng trong cả xử lý ngôn ngữ tự nhiên và thị giác máy tính, đạt được hiệu quả dữ liệu chưa từng có. Ví dụ, các mô hình ngôn ngữ lớn đã chứng minh khả năng tổng quát hóa vài mẫu (Brown et al., 2020) trên nhiều tác vụ khác nhau, bao gồm dịch thuật, trả lời câu hỏi, tác vụ điền chỗ trống, và lý luận. Tương tự, trong DINOv2, (Oquab et al., 2023) thể hiện cách một mô hình ViT được huấn luyện trước lớn (Dosovitskiy et al., 2020) với hơn 1B tham số mang lại các đặc trưng thị giác đa năng vượt trội cho nhiều tác vụ điểm chuẩn phụ thuộc ở cả cấp độ hình ảnh và pixel. Thông thường, những mô hình lớn được huấn luyện trước này được thích ứng với các tác vụ phụ thuộc thông qua tinh chỉnh các tham số của chúng. Tuy nhiên, tinh chỉnh và lưu trữ toàn bộ tập tham số mô hình cho mỗi tác vụ phải chịu chi phí lưu trữ đáng kể (ví dụ: 350GB cho GPT-3). Thách thức này đã thúc đẩy một lượng lớn các nghiên cứu gần đây tập trung vào tinh chỉnh hiệu quả tham số của các mô hình lớn (Hu et al., 2021; Xu et al., 2023; Dettmers et al., 2023; Chen et al., 2022; Sung et al., 2022b).

Lấy cảm hứng từ tính chiều nội tại thấp của các tham số tối ưu của mạng quá tham số hóa (Li et al., 2018; Aghajanyan et al., 2021), (Hu et al., 2021) đã đề xuất một giả thuyết quan trọng rằng sự thay đổi trong trọng số trong quá trình thích ứng/tinh chỉnh mô hình có "thứ hạng nội tại" thấp, dẫn đến sự phát triển của Thích ứng Thứ hạng Thấp (LoRA). Về bản chất, LoRA cho phép huấn luyện gián tiếp một lớp tuyến tính trong mạng neural bằng cách tối ưu hóa các ma trận phân rã thứ hạng cho sự thay đổi trọng số trong những lớp này, dẫn đến việc giảm đáng kể số lượng tham số cần thiết cho việc thích ứng.

* Đóng góp ngang nhau.
1arXiv:2310.02556v2 [cs.CL] 30 Apr 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 1: Phương pháp của chúng tôi (NOLA): Sau khi hạn chế thứ hạng của ∆W bằng cách phân rã nó thành A×B, chúng tôi tham số hóa lại A và B thành tổ hợp tuyến tính của nhiều ma trận cơ sở ngẫu nhiên. Chúng tôi đông lạnh cơ sở và W và học các hệ số tổ hợp. Để tái tạo mô hình, chúng tôi lưu trữ các hệ số và hạt giống của bộ sinh ngẫu nhiên là một đại lượng vô hướng duy nhất. NOLA dẫn đến nén nhiều hơn so với LoRA và quan trọng hơn là tách rời tỷ lệ nén khỏi thứ hạng và các chiều của W. Người ta có thể giảm số lượng tham số xuống 4 lần nhỏ hơn so với thứ hạng=1 của LoRA điều này không thể thực hiện được với LoRA do thứ hạng là một số nguyên.

(ví dụ: giảm 10.000 × tham số cho GPT-3). Đáng chú ý, LoRA đã trở nên phổ biến, và nhiều phần mở rộng khác nhau của phương pháp này đã được đề xuất kể từ khi ra đời (Xu et al., 2023; Dettmers et al., 2023). Tuy nhiên, LoRA và các phiên bản phái sinh của nó có ba hạn chế cố hữu: (1) số lượng tham số bị giới hạn dưới bởi phân rã thứ hạng một của các lớp tuyến tính, và (2) số lượng tham số bị lượng tử hóa vì thứ hạng là một số nguyên, và (3) số lượng tham số phụ thuộc vào kiến trúc của mô hình, tức là các chiều của ma trận tuyến tính, và lựa chọn thứ hạng. Trong bài báo này, chúng tôi giới thiệu một phương pháp, được ký hiệu là NOLA, mang lại những lợi ích tương tự như LoRA trong khi giải quyết các hạn chế của nó. NOLA cho phép tách rời số lượng tham số có thể huấn luyện khỏi cả lựa chọn thứ hạng và kiến trúc mạng, và nó phá vỡ giới hạn phân rã thứ hạng một của LoRA.

NOLA được lấy cảm hứng từ nghiên cứu gần đây của Nooralinejad et al. (2022), có tiêu đề PRANC. Trong nghiên cứu này, chúng tôi tham số hóa lại một mạng neural sử dụng tổ hợp tuyến tính của các trọng số được tạo giả ngẫu nhiên. Sau đó, chúng tôi huấn luyện gián tiếp các tham số mạng bằng cách tối ưu hóa các hệ số hỗn hợp tuyến tính. Cách tiếp cận này dẫn đến việc giảm đáng kể tổng số tham số cần thiết để biểu diễn mạng. Khác với PRANC, trọng tâm của chúng tôi trong NOLA là tham số hóa lại sự thay đổi của trọng số neural để tinh chỉnh các mô hình lớn. Quan trọng hơn, khác với PRANC, NOLA kết hợp những hiểu biết vô giá từ (Hu et al., 2021), khẳng định rằng sự thay đổi trọng số trong quá trình tinh chỉnh có thứ hạng nội tại thấp. Về bản chất, chúng tôi sử dụng phân rã thứ hạng được trình bày trong LoRA nhưng lắp ráp các ma trận phân rã thứ hạng như một tổ hợp tuyến tính của các ma trận giả ngẫu nhiên (tức là 'cơ sở'). Tối ưu hóa các ma trận phân rã thứ hạng trong NOLA tương tự như việc xác định các hệ số hỗn hợp tuyến tính cho các ma trận ngẫu nhiên. Thiết kế này cho phép chúng tôi tách rời số lượng tham số khỏi hình dạng của lớp tuyến tính và cũng khỏi lựa chọn thứ hạng. Hơn nữa, các ràng buộc thứ hạng thấp mang lại lợi thế đáng kể về tính toán và dấu chân bộ nhớ so với phương pháp được đề xuất trong PRANC. Hình 1 minh họa khái niệm cơ bản của NOLA.

Tại sao ít tham số hơn lại quan trọng?
Chúng tôi hình dung một tương lai nơi chúng ta phải quản lý và chuyển đổi hiệu quả giữa nhiều Mô hình Ngôn ngữ Lớn (LLM), mỗi mô hình được điều chỉnh cho các tác vụ cụ thể. Tầm nhìn này xuất phát từ nhu cầu về các LLM được tùy chỉnh với dữ liệu riêng tư và/hoặc khái niệm tạo ra một LLM phổ quát có thể triệu hồi các LLM tùy chỉnh như một hộp công cụ đa năng để giải quyết các tác vụ đa dạng (Schick et al., 2023). Tuy nhiên, hiện tại, các LLM tùy chỉnh đòi hỏi lưu trữ đáng kể, và quá trình chuyển đổi giữa chúng thiếu hiệu quả do các thao tác I/O lớn. NOLA cung cấp một giải pháp tham số hóa lại nhỏ gọn hơn có thể được lưu trữ hiệu quả trong bộ nhớ GPU, cho phép tái tạo theo yêu cầu trực tiếp trên GPU khi một tác vụ mới xuất hiện.

Lưu ý rằng mặc dù lưu trữ tham số trong bộ nhớ CPU là một lựa chọn hiệu quả về chi phí, quá trình chuyển chúng từ CPU sang GPU phát sinh thời gian và tiêu thụ điện năng đáng kể. Hơn nữa, việc truyền dữ liệu này dựa vào một tài nguyên được chia sẻ (ví dụ: bus PCIe), có thể gặp tắc nghẽn trong môi trường máy chủ bận rộn. Do đó, tối ưu hóa tính nhỏ gọn của mô hình để phù hợp với một số mô hình trong bộ nhớ GPU hạn chế được chứng minh là có lợi trong những kịch bản này. Ví dụ, 1.000 mô hình GPT-3 tùy chỉnh sử dụng LoRA cần gần 35GB bộ nhớ (giả sử LoRA nén nó với hệ số 10.000×), có thể không phù hợp trong bộ nhớ GPU cùng với chính mô hình LLM. Do đó, nén nó thêm hệ số 5 giảm xuống 7GB, có thể phù hợp trong bộ nhớ GPU, dẫn đến chuyển đổi rất hiệu quả giữa các tác vụ.

Đóng góp. Những đóng góp cụ thể của chúng tôi trong bài báo này là: 1) Một tham số hóa lại mới để nén các mô hình ngôn ngữ lớn cụ thể theo tác vụ, được ký hiệu là NOLA. 2) NOLA tách rời tỷ lệ nén khỏi thứ hạng và chiều của ma trận trọng số, mở khóa tỷ lệ nén cao hơn trong khi giữ hầu hết lợi ích của LoRA, bao gồm giảm bộ nhớ và tính toán trong thời gian huấn luyện. 3) NOLA có thể được cải thiện thêm bằng cách lượng tử hóa các hệ số và có thể được áp dụng cho các kiến trúc khác như CNN. 4) Khi áp dụng cho PRANC, NOLA tăng tốc và giảm dấu chân bộ nhớ của nó.

2 PHƯƠNG PHÁP ĐỀ XUẤT: NOLA
LoRA, viết tắt của Thích ứng Thứ hạng Thấp, là một phương pháp được chấp nhận rộng rãi để tùy chỉnh một mô hình được huấn luyện trước, chẳng hạn như GPT, cho một tác vụ cụ thể. Thay vì thay đổi tất cả các tham số được ký hiệu là W trong một lớp nhất định, LoRA duy trì các tham số được huấn luyện trước gốc W như một hằng số và học một điều chỉnh dư ∆W để tinh chỉnh mô hình cho tác vụ mới. Các tham số lớp được cập nhật kết quả sau đó được tính là W + ∆W. Khái niệm cốt lõi đằng sau LoRA là tối thiểu hóa kích thước của ∆W bằng cách hạn chế thứ hạng của nó. Trong một bối cảnh chính thức hơn, xem xét W∈Rm×n và ∆W∈Rm×n, LoRA thực hiện điều này bằng cách tham số hóa lại ∆W như tích của hai ma trận, ∆W=A×B, trong đó A∈Rm×r và B∈Rr×n, với r đại diện cho thứ hạng—một siêu tham số. Bằng cách chọn một thứ hạng tương đối nhỏ (r << min(m, n)), LoRA giảm hiệu quả việc sử dụng bộ nhớ. Tối ưu hóa này đạt được bằng cách lưu trữ A và B, thể hiện một biểu diễn nhỏ gọn hơn đáng kể so với ∆W đầy đủ. Tỷ lệ nén kết quả được định lượng là mn/r(m+n). Thật không may, tỷ lệ nén này: (1) gắn liền với hình dạng của các tham số m và n, và do đó kiến trúc mô hình và (2) bị giới hạn trên bởi mn/(m+n), tức là cho r= 1.

Trong bài báo này, chúng tôi giới thiệu một kỹ thuật tham số hóa lại mới cho ∆W có thể tách rời hiệu quả thứ hạng khỏi tỷ lệ nén, cho phép tỷ lệ nén cao hơn mn/(m+n), tương ứng với r= 1 trong khung LoRA. Để đạt được điều này, chúng tôi lấy cảm hứng từ PRANC (Nooralinejad et al., 2022) và tham số hóa lại các ma trận A và B để tồn tại trong một không gian chiều thấp hơn được xác định bởi một tập hợp các ma trận cơ sở được tạo ngẫu nhiên. Chính thức, chúng tôi biểu diễn điều này như:

A=∑(i=1 to k) αiAi, B =∑(j=1 to l) βjBj (1)

trong đó, Ai∈Rm×r và Bj∈Rr×n là các ma trận ngẫu nhiên được tạo bởi Bộ tạo số ngẫu nhiên giả với hạt giống cố định. Sau đó chúng tôi học A và B như tổ hợp tuyến tính của những ma trận ngẫu nhiên được xác định trước và đông lạnh này. Quan trọng, chính các ma trận ngẫu nhiên vẫn không đổi, và chúng tôi chỉ tối ưu hóa các vectơ hệ số α và β. Sau đó:

∆W=∑(i=1 to k) αiAi × ∑(j=1 to l) βjBj (2)

Trong thực tế, để lưu trữ ∆W cho một tác vụ cụ thể, chúng tôi chỉ cần giữ lại hạt giống (một đại lượng vô hướng duy nhất) và các vectơ hệ số α và β. Đáng chú ý, cách tiếp cận này cho phép chọn một số lượng nhỏ ma trận cơ sở (k+l), không phụ thuộc vào thứ hạng của phân tích A×B và hình dạng của ∆W, từ đó nâng cao tỷ lệ nén để vượt qua mn/(m+n).

Lượng tử hóa các hệ số α và β: Chúng tôi chủ yếu quan tâm đến việc giảm lưu trữ cho một tác vụ mới, giả sử LLM được huấn luyện trước đã có sẵn. Do đó, để giảm thêm lưu trữ, chúng tôi lượng tử hóa các hệ số α và β xuống độ chính xác thấp hơn (ví dụ: 4 bit) trong khi cơ sở ngẫu nhiên và trọng số LLM được huấn luyện trước có độ chính xác điểm nổi FP16 tiêu chuẩn. Lưu ý rằng người ta cũng có thể lượng tử hóa các ma trận A và B trong LoRA; tuy nhiên, phương pháp của chúng tôi không buộc chính A và B phải có độ chính xác thấp. Người ta có thể lượng tử hóa α và β sau khi tối ưu hóa (lượng tử hóa sau huấn luyện) hoặc trong khi tối ưu hóa chúng (huấn luyện nhận biết lượng tử hóa). Chúng tôi kỳ vọng cách sau sẽ hoạt động tốt hơn. Để học nhận biết lượng tử hóa, chúng tôi sử dụng phương pháp trong (Rastegari et al., 2016b; Jacob et al., 2018) nơi chúng tôi sử dụng α và β lượng tử hóa trong lượt truyền xuôi và cập nhật các phiên bản FP16 của α và β trong lượt truyền ngược. Hơn nữa, chúng tôi sử dụng thủ thuật Ước lượng Thẳng (STE) (Bengio et al., 2013) để ước lượng gradient.

Một số nghiên cứu gần đây đã cho thấy có thể lượng tử hóa trọng số của LLM cho mỗi tác vụ, giảm cả tính toán và lưu trữ. Tuy nhiên, những phương pháp này không phù hợp cho một số lượng lớn tác vụ vì LLM lượng tử hóa vẫn cụ thể theo tác vụ và lớn.

Hiệu quả bộ nhớ: Lưu ý rằng tùy thuộc vào số lượng ma trận cơ sở, cơ sở ngẫu nhiên có thể lớn, đòi hỏi bộ nhớ lớn. Thú vị, việc tạo ma trận ngẫu nhiên trong chính GPU rất nhanh, vì vậy tương tự như PRANC, ở mỗi lần lặp, chúng tôi tạo từng khối ma trận cơ sở tại một thời điểm, nhân chúng với các hệ số tương ứng, và loại bỏ chúng. Tạo cơ sở ngay lập tức tại thời điểm suy luận có thể giảm mạnh chi phí giao tiếp giữa CPU và GPU vì các vectơ α và β cho nhiều tác vụ có thể được lưu trữ trong bộ nhớ GPU.

Hiệu quả của NOLA so với PRANC: PRANC (Nooralinejad et al., 2022) định hình lại toàn bộ tham số mô hình hoặc mỗi lớp của nó thành một vectơ dài và tham số hóa lại điều đó bằng tổ hợp tuyến tính của các vectơ ngẫu nhiên. Tuy nhiên, như đã đề cập trong (Nooralinejad et al., 2022), phương pháp này bao gồm phép nhân các hệ số với ma trận ngẫu nhiên lớn hai lần ở mỗi lần lặp (một lần trong lượt truyền xuôi và một lần trong lượt truyền ngược), điều này rất tốn kém. Ví dụ, kích thước của ma trận ngẫu nhiên cho ResNet18 với 1000 hệ số sẽ gần 11M×1K. NOLA giảm tính toán này trong khi giữ cùng số lượng tham số bằng cách định hình lại vectơ dài thành ma trận 2D và hạn chế thứ hạng của nó. Giả sử d² trọng số và k cơ sở ngẫu nhiên, kích thước ma trận cơ sở cho PRANC sẽ là kd² trong khi NOLA với thứ hạng r giảm xuống kdr giả sử rằng mỗi thành phần trong A×B có k/2 ma trận cơ sở để giữ số lượng tham số bằng với PRANC. Sau đó, tổng tính toán cho PRANC sẽ là kd²+d²≈kd² trong khi đối với NOLA, nó sẽ là kdr+ 2dr≈kdr. Do đó, giả sử thứ hạng r nhỏ, NOLA có thể giảm thời gian huấn luyện của PRANC với hệ số lớn d/r do việc giảm tính toán ở lượt truyền xuôi và ngược. Hơn nữa, trong phần phụ lục, chúng tôi thực nghiệm cho thấy rằng NOLA cung cấp phạm vi bao phủ tốt hơn của không gian trọng số so với PRANC.

Cấu trúc của các tham số: Lưu ý rằng người ta có thể áp dụng NOLA cho các kiến trúc mô hình khác ngoài transformer bằng cách chỉ đơn giản định hình lại tensor trọng số thành ma trận 2D (tốt nhất là gần hình vuông) và sau đó nén nó. Chúng tôi làm điều này trong các thí nghiệm ResNet trong Phụ lục, nơi các ma trận trọng số là tensor 4D của các bộ lọc tích chập.

3 THỰC NGHIỆM
Ở đây, chúng tôi đánh giá NOLA trong các tác vụ học chuyển giao trong cả NLP và thị giác. Hơn nữa, trong phần phụ lục, chúng tôi đánh giá NOLA trong huấn luyện từ đầu.

3.1 NOLA TRÊN GPT-2:
Chúng tôi thích ứng các tham số của GPT-2 được huấn luyện trước với ba tập dữ liệu Tạo ngôn ngữ tự nhiên (NLG) khác nhau bằng cách tinh chỉnh các tham số sử dụng NOLA. Chúng tôi sử dụng GPT-2-Large và GPT-2-Medium trong các thí nghiệm. Chúng tôi tuân theo (Li & Liang, 2021; Hu et al., 2021) cho thiết lập thích ứng.

Tập dữ liệu: Chúng tôi sử dụng các tập dữ liệu sau cho tác vụ Tạo ngôn ngữ tự nhiên (NLG): E2E NLG Challenge (Novikova et al., 2017) phục vụ như một điểm chuẩn thường được sử dụng để đánh giá các mô hình NLG. Nó bao gồm 51.200 mẫu, phân phối như sau: 42.200 cho huấn luyện, 4.600 cho xác thực, và thêm 4.600 cho kiểm tra. DART (Nan et al., 2020) là một tập dữ liệu quan trọng khác được sử dụng để đánh giá việc tạo văn bản từ dữ liệu. Tập dữ liệu này phong phú với 82.191 ví dụ rút ra từ nhiều miền khác nhau. WebNLG (Gardent et al., 2017) là một tập dữ liệu văn bản từ dữ liệu, tự hào với 22.000 ví dụ trải rộng 14 danh mục riêng biệt. Đáng chú ý, tập kiểm tra WebNLG giới thiệu năm danh mục mới, thúc đẩy chúng tôi trình bày kết quả trên tất cả các danh mục trong tập dữ liệu này. Những tập dữ liệu này cung cấp một nền tảng toàn diện cho việc đánh giá và thử nghiệm NLG của chúng tôi.

LoRA: Trong các thí nghiệm, chúng tôi áp dụng LoRA trên cả lớp chiếu query và value trong mỗi khối attention. Vì số lượng tham số gắn liền với thứ hạng, chúng tôi điều chỉnh thứ hạng để giảm số lượng tham số. Chúng tôi so sánh với LoRA với cả thứ hạng bốn và thứ hạng một.

Các đường cơ sở khác: Hơn nữa, chúng tôi so sánh NOLA với một số đường cơ sở khác, bao gồm tinh chỉnh tất cả tham số, Adapters (Houlsby et al., 2019; Lin et al., 2020b; Pfeiffer et al., 2021; Rücklé et al., 2020), và tinh chỉnh Prefix-layer (PreLayer) (Li & Liang, 2021).

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: E2E NLG Challenge: Chúng tôi so sánh NOLA với LoRA với hai kiến trúc khác nhau: GPT-2 trung bình (M) và lớn (L). QV đề cập đến chiếu Query và Value, trong khi MLP biểu thị lớp MLP trong transformer. AdapterL và AdapterH là hai đường cơ sở Adapter được báo cáo trong (Hu et al., 2021). Các phương pháp hoạt động tốt nhất và tốt thứ hai được in đậm. Để giảm số lượng tham số trong LoRA, chúng tôi sử dụng thứ hạng thấp hơn (LoRA thứ hạng=1). Chúng tôi không thấy giảm hiệu suất khi giảm số lượng tham số có thể huấn luyện xuống 1/20 của LoRA với thứ hạng 4 trong GPT-L. Lưu ý rằng trong LoRA, người ta không thể giảm số lượng tham số dưới thứ hạng một.

GPT-2 M
Phương pháp    Các lớp     Thứ hạng    # Tham số      E2E NLG Challenge
               thích ứng   Adapter     có thể huấn luyện  BLEU NIST MET ROUGE-L CIDEr
Finetune       Tất cả lớp    -         354.920M       68.2 8.62 46.2 71.0 2.47
AdapterL       Lớp phụ       -         0.370M         66.3 8.41 45.0 69.8 2.40
AdapterL       Lớp phụ       -         11.090M        68.9 8.71 46.1 71.3 2.47
AdapterH       Lớp phụ       -         11.090M        67.3 8.50 46.0 70.7 2.44
Finetune       2 lớp cuối    -         25.190M        68.1 8.59 46.0 70.8 2.41
PreLayer       Token phụ     -         0.350M         69.7 8.81 46.1 71.4 2.49
LoRA           QV            4         0.350M         70.4 8.85 46.8 71.8 2.53
LoRA           QV            1         0.098M         68.7 8.72 45.6 70.5 2.43
NOLA (Của chúng tôi) QV      8         0.350M         70.1 8.80 46.8 71.7 2.53
NOLA (Của chúng tôi) QV      8         0.096M         70.0 8.82 46.7 71.6 2.51
NOLA (Của chúng tôi) MLP     8         0.096M         70.2 8.79 46.7 71.8 2.51
NOLA (Của chúng tôi) QV      8         0.048M         70.1 8.82 46.4 71.4 2.52
NOLA (Của chúng tôi) MLP     8         0.048M         69.4 8.71 46.5 71.5 2.47

GPT-2 L
Finetune       Tất cả lớp    -         774.030M       68.5 8.78 46.0 69.9 2.45
AdapterL       Lớp phụ       -         0.880M         69.1 8.68 46.3 71.4 2.49
AdapterL       Lớp phụ       -         23.000M        68.9 8.70 46.1 71.3 2.45
PreLayer       Token phụ     -         0.770M         70.3 8.85 46.2 71.7 2.47
LoRA           QV            4         0.770M         70.4 8.89 46.8 72.0 2.47
LoRA           QV            1         0.184M         69.9 8.81 46.7 71.6 2.53
NOLA (Của chúng tôi) QV      8         0.144M         70.5 8.85 46.8 71.7 2.54
NOLA (Của chúng tôi) MLP     8         0.144M         70.1 8.80 46.5 71.2 2.52
NOLA (Của chúng tôi) QV      8         0.072M         69.8 8.80 46.4 71.3 2.51
NOLA (Của chúng tôi) MLP     8         0.072M         69.4 8.71 46.6 71.5 2.48
NOLA (Của chúng tôi) QV      8         0.036M         70.1 8.80 46.7 71.7 2.53
NOLA (Của chúng tôi) MLP     8         0.036M         70.0 8.81 46.4 71.5 2.53

NOLA: Chúng tôi đánh giá NOLA với hai biến thể khác nhau: 1. Thích ứng các lớp MLP. 2. Thích ứng các ma trận chiếu query và value. Lưu ý rằng, khác với LoRA, chúng tôi có thể sử dụng bất kỳ số lượng tham số nào trong khi áp dụng NOLA cho bất kỳ cấu trúc trọng số nào vì số lượng tham số không gắn liền với hình dạng của tensor trọng số. Chúng tôi phân bổ số lượng tham số bằng nhau cho A và B trong mỗi lớp NOLA (tức là, k=l). Sử dụng k=l=1000 dẫn đến 0.096M tham số trong GPT-M và 0.144M tham số trong GPT-L. Ngoài ra, chúng tôi sử dụng một nửa (k=l=500) và một phần tư (k=l=250) số lượng tham số trên mỗi lớp để có được các checkpoint nhỏ hơn.

Chi tiết triển khai: Chúng tôi huấn luyện các mô hình bằng một GPU NVIDIA RTX 6000 Ada Generation duy nhất. Đối với tất cả các siêu tham số ngoại trừ tốc độ học, chúng tôi sử dụng các giá trị giống như LoRA để huấn luyện và đánh giá GPT-2. Chúng tôi huấn luyện các mô hình trong 5 epoch với tốc độ học 0.1 và không có weight decay. Chúng tôi sử dụng batch size 8. Chúng tôi sử dụng thứ hạng 8 cho NOLA trong các thí nghiệm. Như LoRA, chúng tôi chia tỷ lệ A×B với c/r, trong đó c là một siêu tham số và r là thứ hạng. Chúng tôi sử dụng giá trị mặc định c= 1.

Kết quả: Chúng tôi so sánh với LoRA và các đường cơ sở khác trong Bảng 1 và Bảng 11 trong Phụ lục. NOLA ngang bằng hoặc tốt hơn so với các phương pháp khác với cùng số lượng tham số. Trong tác vụ E2E, NOLA với 0.036M tham số đạt điểm BLEU 70.12, nhỏ gọn gấp 20 lần so với LoRA với thứ hạng 4 có 0.77M tham số và đạt điểm BLEU 70.4. Mô hình NOLA này sử dụng thứ hạng 8, không ảnh hưởng đến số lượng tham số và tăng thời gian chạy nhẹ (không đáng kể so với mô hình LLM thực tế). Lưu ý rằng mục tiêu của chúng tôi là giảm số lượng tham số mà không giảm độ chính xác, điều này được thể hiện trong Bảng 1 và 11. Chúng tôi không khẳng định rằng NOLA cải thiện độ chính xác so với các đường cơ sở. Chúng tôi thể hiện các biến thể khác nhau của NOLA (MLP, QV, v.v.) để nhấn mạnh rằng NOLA không rất nhạy cảm với việc lựa chọn biến thể.

Thời gian huấn luyện và bộ nhớ của NOLA: Tương tự như LoRA, trong thời gian suy luận, chúng tôi có thể tính A×B offline và hợp nhất nó với W. Do đó, NOLA không có overhead nào so với mô hình gốc. Trong thời gian huấn luyện, NOLA có overhead nhỏ do phép nhân các hệ số

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 2: Thời gian huấn luyện và bộ nhớ: Chúng tôi so sánh bộ nhớ huấn luyện và thời gian chạy của NOLA với LoRA. Vì việc tạo cơ sở ngẫu nhiên trên mỗi lớp có overhead nhỏ, chúng tôi có thể tạo cơ sở ngẫu nhiên một lần và chia sẻ cơ sở qua các lớp để tiết kiệm thời gian. Phiên bản NOLA này có thời gian chạy tương tự như LoRA và độ chính xác ngang bằng với NOLA với cơ sở không được chia sẻ.

Mô hình & Phương pháp  Cơ sở ngẫu nhiên  Bộ nhớ huấn luyện  Thời gian huấn luyện  # Tham số    E2E NLG Challenge
                                          (ms/batch)        có thể huấn luyện  BLEU NIST MET ROUGE-L CIDEr
GPT-2 L (LoRA r=1)     -               33.35GB           776              184K          69.89 8.81 46.70 71.64 2.53
GPT-2 L (NOLA QV)      Không chia sẻ   33.37GB           834              144K          70.46 8.85 46.77 71.68 2.54
GPT-2 L (NOLA QV)      Được chia sẻ    33.40GB           779              144K          70.32 8.85 46.74 71.71 2.54

Bảng 3: Tác động của thứ hạng trong NOLA: Chúng tôi thay đổi thứ hạng từ 1 đến 8. Lưu ý rằng chúng tôi có thể sử dụng cùng số lượng tham số trong tất cả các thứ hạng vì số lượng tham số không gắn liền với thứ hạng trong NOLA.

# Tham số   Thứ hạng   E2E NLG Challenge
huấn luyện            BLEU NIST MET ROUGE-L CIDEr
GPT-2 M (NOLA QV)
96K        8          70.03 8.82 46.74 71.64 2.51
96K        4          69.69 8.76 46.56 71.44 2.51
96K        2          70.47 8.86 46.71 71.79 2.53
96K        1          69.09 8.78 45.87 70.15 2.45
96K        8          70.03 8.82 46.74 71.64 2.51
48K        8          70.09 8.82 46.44 71.36 2.52
24K        8          68.30 8.67 45.13 68.91 2.40
12K        8          67.18 8.60 43.99 68.38 2.26
GPT-2 L (NOLA QV)
144K       8          70.46 8.85 46.77 71.68 2.54
144K       4          70.25 8.85 46.84 71.81 2.54
144K       2          69.69 8.78 46.55 71.25 2.51
144K       1          69.71 8.82 46.47 70.96 2.51

Bảng 4: Lượng tử hóa các hệ số: Lượng tử hóa tham số sau huấn luyện không làm giảm hiệu suất đến lượng tử hóa 4bit. Trong huấn luyện nhận biết lượng tử hóa, NOLA bền vững hơn với lượng tử hóa so với LoRA.

Mô hình        # Bit   E2E NLG Challenge
& Phương pháp  lượng tử BLEU NIST MET ROUGE-L CIDEr
Lượng tử hóa sau huấn luyện
GPT-2 L        16-bit  69.89 8.81 46.70 71.64 2.53
(LoRA r=1)     8-bit   69.91 8.81 46.69 71.75 2.53
               4-bit   69.63 8.75 46.32 71.24 2.48
               3-bit   62.01 8.02 42.01 67.23 2.07
               16-bit  70.46 8.85 46.77 71.68 2.54
               8-bit   70.43 8.84 46.78 71.72 2.54
GPT-2 L        4-bit   70.29 8.82 46.74 71.82 2.52
(NOLA QV)      3-bit   65.14 8.58 44.38 67.56 2.23
Huấn luyện nhận biết lượng tử hóa
GPT-2 L        3-bit   67.08 8.86 44.67 68.76 2.36
(LoRA r=1)     2-bit   56.13 4.70 35.38 63.68 1.40
               3-bit   70.14 8.82 46.58 71.61 2.53
GPT-2 L        2-bit   68.69 8.72 46.06 70.61 2.48
(NOLA QV)

với trọng số cơ sở. Chúng tôi đo thời gian chạy và dấu chân bộ nhớ của NOLA trong huấn luyện và so sánh với LoRA trong Bảng 2. Vì việc tạo cơ sở ngẫu nhiên cho mỗi lớp thêm overhead nhỏ, chúng tôi có thể chia sẻ cơ sở ngẫu nhiên qua tất cả các lớp và tạo và lưu trữ chúng chỉ một lần để cải thiện thời gian chạy. Chúng tôi đo thời gian và bộ nhớ với batch size 8. NOLA, với cơ sở ngẫu nhiên duy nhất cho mỗi lớp, chậm hơn LoRA một chút. Tuy nhiên, NOLA với cơ sở ngẫu nhiên được chia sẻ có độ chính xác ngang bằng với cơ sở ngẫu nhiên duy nhất và có thời gian chạy tương tự như LoRA.

Nghiên cứu khử yếu tố về thứ hạng của NOLA: Vì số lượng tham số tách rời khỏi thứ hạng của ma trận, chúng tôi có thể đánh giá riêng tác động của thứ hạng mà không thay đổi số lượng tham số. Chúng tôi báo cáo tác động của thứ hạng trong Bảng 3. Chúng tôi thay đổi thứ hạng từ 1 đến 8 và sử dụng c= 1.0 cho thứ hạng 4 và 8, và c= 0.5 cho thứ hạng thấp hơn. Như cũng được lưu ý bởi (Hu et al., 2021), việc hiểu tác động của thứ hạng cần nghiên cứu nghiêm ngặt hơn như công việc tương lai.

3.2 NOLA VỚI CÁC HỆ SỐ LƯỢNG TỬ HÓA, α VÀ β:
Chúng tôi đánh giá hiệu suất của NOLA với thứ hạng 4 với các tham số α và β lượng tử hóa trên tập dữ liệu E2E trong Bảng 4 trong hai thiết lập khác nhau. Đầu tiên, chúng tôi thực hiện Lượng tử hóa sau huấn luyện (PTQ) bằng cách lượng tử hóa các tham số xuống q bit sau huấn luyện. Chúng tôi quan sát không có giảm đáng kể trong cả LoRA và NOLA trong các thí nghiệm PTQ 4 bit. Thứ hai, chúng tôi đánh giá các mô hình với Huấn luyện nhận biết lượng tử hóa (QAT) nơi chúng tôi lượng tử hóa các hệ số trong lượt truyền xuôi và cập nhật chúng ở dạng không lượng tử hóa. Trong QAT với 3 bit, NOLA có giảm nhẹ 0.3 điểm trong khi LoRA có giảm 2.8 điểm trong metric BLEU. Lưu ý rằng trong NOLA, mặc dù α và β bị lượng tử hóa, A và B trong Phương trình 1 không bị lượng tử hóa vì các ma trận cơ sở, Ai và Bj, không bị lượng tử hóa. Điều này trái ngược với LoRA nơi A và B bị lượng tử hóa.

3.3 NOLA TRÊN LLAMA-2:
Tinh chỉnh với LoRA cho các LLM lớn hơn là thách thức do nhu cầu tính toán, vì vậy chúng tôi cần sử dụng QLoRA nơi mô hình cơ sở bị lượng tử hóa. Khung NOLA của chúng tôi không phụ thuộc vào lượng tử hóa của mô hình cơ sở, vì vậy đối với các thí nghiệm LLaMA-2 (Touvron et al., 2023), chúng tôi sử dụng NOLA với mô hình cơ sở lượng tử hóa xuống 8-bit trong khi các hệ số NOLA vẫn sử dụng 16-bit. Chúng tôi tinh chỉnh LLaMA-2 được huấn luyện trước

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 5: Tinh chỉnh hướng dẫn cho LLaMA-2 lượng tử hóa: NOLA tinh chỉnh LLaMA-2 70B (8-bit) với 0.57M tham số, giảm 95% so với LoRA thứ hạng một. Chúng tôi sử dụng phiên bản lượng tử hóa của LLaMA-2 cho cả LoRA và NOLA, vì vậy đường cơ sở LoRA của chúng tôi tương đương với QLoRA.

LLaMA-2 - 7B (8-bit)     LLaMA-2 - 13B (8-bit)    LLaMA-2 - 70B (8-bit)
Không tinh chỉnh LoRA NOLA  Không tinh chỉnh LoRA NOLA  Không tinh chỉnh LoRA NOLA
Thứ hạng Adapter  -    1   16    -    1   16    -    1   16
Tham số có thể huấn luyện - 2.50M 0.06M (↓97%) - 3.91M 0.14M (↓96%) - 12.94M 0.57M (↓95%)
Train Loss       1.53  0.97 1.05   1.43  0.94 0.95   1.42  0.87 0.90
Val Loss         1.74  1.04 1.01   1.59  0.96 0.97   1.53  0.92 0.90
MMLU Acc         45.3  46.5 46.5   54.8  55.3 55.3   68.9  69.5 69.4

mô hình sử dụng 8-bit QLoRA trên tập dữ liệu Alpaca (Taori et al., 2023), báo cáo cả metric loss huấn luyện và xác thực cụ thể cho tập dữ liệu Alpaca. Ngoài ra, chúng tôi sử dụng điểm chuẩn MMLU (Massively Multitask Language Understanding) (Hendrycks et al., 2021) để đánh giá hiệu suất trên một tập hợp đa dạng các tác vụ hiểu ngôn ngữ. Điểm chuẩn này bao gồm 57 tác vụ trải rộng các lĩnh vực như toán học, lịch sử, khoa học máy tính, và luật. Trên MMLU, chúng tôi tập trung vào đánh giá 5-shot (cung cấp 5 ví dụ trong ngữ cảnh), theo thực hành tiêu chuẩn.

Chi tiết triển khai: Chúng tôi áp dụng LoRA và NOLA trên tất cả các lớp (chiếu Query, Key, Value và Output và các lớp MLP) của transformer trên ba kích thước mô hình khác nhau của LLaMA-2: 7B, 13B, và 70B. Đối với các thí nghiệm LoRA, chúng tôi đặt thứ hạng r= 1 vì bài báo LoRA (Bảng 15 của nó) cho thấy rằng mô hình thứ hạng một hoạt động tốt như các thứ hạng cao hơn cho các LLM lớn hơn (ví dụ: GPT-3). Đối với NOLA, chúng tôi sử dụng r= 16 và điều chỉnh số lượng tham số được tối ưu hóa cho mỗi kích thước mô hình LLaMA-2 sử dụng các thiết lập sau: k=l= 128 cho LLaMA-2 7B, k=l= 256 cho LLaMA-2 13B, và k=l= 512 cho LLaMA-2 70B. Trong quá trình tinh chỉnh LoRA hoặc NOLA, chúng tôi tuân thủ các siêu tham số được báo cáo trong QLoRA, (Dettmers et al., 2023). Chúng tôi tối ưu hóa trong một epoch trên tập dữ liệu Alpaca với batch size 256 sử dụng bốn GPU RTX 3090. Tốc độ học là 0.0002 cho LoRA và 0.001 cho NOLA. Tương tự như LoRA, chúng tôi chia tỷ lệ A×B với c/r, trong đó c là một siêu tham số và r là thứ hạng. Chúng tôi sử dụng c= 16 cho LoRA và c= 4 cho NOLA.

Kết quả: Kết quả được trình bày trong Bảng 5. Đáng chú ý, NOLA có khả năng tinh chỉnh LLaMA-2 70B với ít hơn 0.6M tham số, đại diện cho việc giảm trung bình 95% tham số so với LoRA với thứ hạng một.

3.4 NOLA TRÊN VISION TRANSFORMERS
Chúng tôi thực hiện các thí nghiệm về tác vụ phân loại hình ảnh trên các kiến trúc ViT-B và ViT-L với cả khởi tạo có giám sát và tự giám sát (MAE).

Chi tiết triển khai: Tất cả các mạng được huấn luyện trước được lấy từ thư viện Timm (Whitman). Tất cả các cách tiếp cận được huấn luyện trong 50 epoch, và độ chính xác top-1 ở epoch cuối cùng được báo cáo. Chúng tôi sử dụng batch-size 64 và điều chỉnh tốc độ học ban đầu cho mỗi tập dữ liệu và kiến trúc cho tất cả các cách tiếp cận. Vì trọng tâm của chúng tôi là tinh chỉnh trên các tập dữ liệu nhỏ, chúng tôi sử dụng 5 và 10 ví dụ được gán nhãn trên mỗi lớp để tinh chỉnh. Vì có phương sai cao trong hiệu suất do các tập huấn luyện nhỏ, chúng tôi lấy mẫu bốn tập khác nhau của các mẫu huấn luyện trên mỗi k-shot và ba khởi tạo khác nhau cho LoRA/NOLA và báo cáo độ chính xác trung bình và độ lệch chuẩn. Tất cả các cách tiếp cận được huấn luyện với cross-entropy loss. Chi tiết bổ sung trong phần phụ lục.

Tập dữ liệu: ImageNet-21k và ImageNet-1k được sử dụng để huấn luyện trước các mô hình backbone. Chúng tôi sử dụng CIFAR10 (Krizhevsky et al., 2014), CIFAR100 (Krizhevsky et al., 2009), CUB-200-2011 (Welinder et al., 2010), Caltech-101 (Fei-Fei et al., 2004), Aircraft (Maji et al., 2013), Food101 (Bossard et al., 2014), Pets (Parkhi et al., 2012) và SUN397 (Xiao et al., 2010) để tinh chỉnh.

Đường cơ sở: Chúng tôi so sánh NOLA với ba cách tiếp cận đường cơ sở: Linear, Full-FT (tinh chỉnh đầy đủ) và LoRA (Hu et al., 2021). Trong Linear, chỉ có phần đầu phân loại cuối cùng được tối ưu hóa, trong khi trong Full-FT, toàn bộ mạng backbone cũng được tối ưu hóa. Không có tham số bổ sung nào được sử dụng trong quá trình tinh chỉnh cho bất kỳ cách tiếp cận nào trong số này. Chúng tôi áp dụng LoRA trên các ma trận chiếu Query, Key, và Value với thứ hạng đặt 4 cho ViT-B đến 1 hoặc 4 cho ViT-L. Trong các thí nghiệm sơ bộ, LoRA với thứ hạng một thấy giảm lớn trong độ chính xác. Điều này phù hợp với các thí nghiệm GPT-2 M của chúng tôi nơi các mô hình nhỏ hơn yêu cầu thứ hạng cao hơn. Chúng tôi áp dụng NOLA trên các lớp MLP và sử dụng thứ hạng 4 cho ViT-B và 1 cho ViT-L. Chúng tôi báo cáo số lượng tham số có thể huấn luyện cho mỗi cách tiếp cận loại trừ số lượng tham số đầu phân loại chung cho tất cả các cách tiếp cận. Chúng tôi cũng báo cáo đánh giá nearest-neighbor (1-NN) cho phân loại zero-shot trên các tác vụ phụ thuộc sử dụng các đặc trưng được huấn luyện trước ImageNet.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 6: Kết quả trên vision transformers. Chúng tôi tinh chỉnh các mô hình ViT được huấn luyện trước ImageNet trên nhiều tập dữ liệu nhỏ với 5 và 10 mẫu huấn luyện. Độ chính xác trung bình và độ lệch chuẩn qua 12 lần chạy được báo cáo. Số lượng tham số huấn luyện cho bộ phân loại Linear phụ thuộc vào số lượng lớp trong tập dữ liệu (0.01,0.1,0.2,0.1M tham số cho CIFAR-10, CIFAR-100, CUB và Caltech tương ứng). Chúng tôi không tính lớp tuyến tính trong các tham số có thể huấn luyện. Phương pháp hoạt động tốt nhất được in đậm trong khi tất cả các phương pháp trong vòng một điểm của phương pháp tốt nhất được gạch dưới. NOLA vượt trội LoRA với các tham số tương đương trên các tập dữ liệu và kiến trúc, đặc biệt trong chế độ dữ liệu huấn luyện thấp. Hiệu suất của NOLA với một nửa hoặc một phần ba tham số huấn luyện tương đương với LoRA. Lưu ý rằng LoRA với thứ hạng một là LoRA nhỏ gọn nhất.

Mô hình cơ sở  # Tham số    CIFAR-10      CIFAR-100     CUB-200-2011   Caltech-101
              huấn luyện    5     10      5     10      5     10       5     10
ViT-B
Nearest Neighbor           79.6  80.8    52.4  59.2    71.9  78.0     84.1  87.5
Linear         0          80.8(1.1) 85.1(1.0) 58.9(0.9) 64.5(0.7) 72.7(0.4) 79.2(0.2) 85.8(0.8) 88.5(0.4)
Full-FT        5.3M       73.9(6.5) 87.6(2.7) 61.4(2.4) 78.2(1.1) 59.7(1.9) 76.6(0.2) 87.9(0.8) 91.1(0.5)
LoRA (r=4)     141K       87.3(2.3) 93.1(0.5) 76.3(0.5) 81.6(0.9) 75.7(0.5) 82.4(0.3) 88.4(1.1) 90.8(0.5)
NOLA-MLP       47K        87.9(1.3) 92.2(0.5) 75.1(0.6) 81.3(0.8) 75.5(0.6) 81.7(0.4) 88.0(1.2) 90.6(0.5)

ViT-B-MAE
Nearest Neighbor           18.2  19.8    5.8   9.8     13.2  25.3     28.2  40.7
Linear         0          27.4(1.9) 34.5(1.4) 15.7(0.7) 22.2(0.2) 12.7(0.3) 18.4(0.3) 66.9(1.1) 76.9(0.6)
Full-FT        5.3M       41.1(4.4) 58.4(3.6) 19.7(4.8) 24.2(11.1) 23.0(3.8) 51.9(2.8) 76.4(2.3) 86.5(0.5)
LoRA (r=4)     141K       54.7(1.6) 70.1(2.2) 39.3(3.1) 52.4(1.3) 35.7(1.5) 54.0(0.6) 82.4(0.6) 87.7(0.5)
NOLA-MLP       47K        55.1(2.6) 72.1(2.7) 42.1(1.4) 53.5(1.0) 35.8(1.5) 53.9(0.6) 88.0(1.2) 90.6(0.5)

ViT-L
Nearest Neighbor           88.7  89.9    68.9  74.0    77.4  82.3     88.4  90.1
Linear         0          84.1(1.8) 88.4(1.1) 63.7(1.3) 70.6(0.9) 73.7(0.6) 79.2(0.3) 87.6(0.9) 89.9(0.4)
Full-FT        289M       77.2(2.7) 90.2(2.8) 74.0(2.3) 86.2(0.6) 73.3(0.9) 83.9(0.2) 88.7(1.0) 91.3(0.7)
LoRA (r=4)     375K       86.5(2.0) 93.8(1.0) 82.9(0.9) 87.6(0.6) 81.2(0.4) 85.3(0.3) 89.3(0.7) 91.3(0.3)
LoRA (r=1)     94K        86.3(1.3) 92.8(0.8) 82.2(0.8) 85.6(0.9) 80.6(0.3) 85.2(0.3) 89.9(1.0) 91.6(0.4)
NOLA-MLP       94K        89.0(3.6) 96.0(0.5) 83.6(0.9) 87.8(0.6) 80.8(0.6) 85.2(0.2) 90.0(0.7) 91.7(0.3)
NOLA-MLP       47K        83.9(1.8) 93.0(1.7) 81.2(1.0) 87.1(0.6) 80.7(0.5) 85.0(0.3) 89.8(0.8) 91.5(0.4)

ViT-L-MAE
Nearest Neighbor           33.5  39.2    15.2  21.9    16.9  29.2     57.4  67.6
Linear         0          40.2(2.3) 49.2(2.6) 22.6(0.9) 31.3(0.5) 15.2(0.3) 21.9(0.4) 75.2(0.5) 83.2(0.6)
Full-FT        289M       60.6(4.5) 68.3(4.0) 37.9(11.1) 52.0(16.1) 42.2(2.3) 67.1(1.1) 87.2(0.8) 90.8(0.7)
LoRA (r=4)     375K       63.5(3.0) 82.4(2.3) 50.2(6.8) 62.6(5.2) 35.2(2.9) 60.8(1.2) 87.0(0.9) 90.7(0.4)
LoRA (r=1)     94K        67.7(3.8) 83.8(1.2) 50.4(1.0) 62.5(0.6) 32.9(1.8) 56.6(1.7) 87.0(0.6) 90.8(0.4)
NOLA-MLP       94K        70.6(3.8) 86.0(1.4) 51.7(1.1) 63.8(0.8) 36.9(5.6) 61.6(1.0) 87.4(0.4) 90.9(0.5)
NOLA-MLP       47K        69.6(3.8) 84.8(1.1) 49.9(0.8) 62.8(0.7) 36.1(0.8) 58.8(1.2) 87.1(0.6) 90.9(0.4)

Bảng 7: Kết quả bổ sung trên các tác vụ thị giác. Sử dụng ViT-B, NOLA đạt được hiệu suất tương đương như LoRA (r=4) chỉ với một phần ba số lượng tham số trên bốn tập dữ liệu thách thức. Kích thước lớp tuyến tính là: 0.03M, 0.2M, 0.1M, 0.3M cho Aircraft, Food101, Pets và SUN397 tương ứng.

Phương pháp    # Tham số   Aircraft      Food101       Pets          SUN397
              huấn luyện   5 Shot 10 Shot 5 Shot 10 Shot 5 Shot 10 Shot 5 Shot 10 Shot
Nearest Neighbor          24.6   27.1    48.4   54.2    82.3   86.2    44.4   51.5
Linear         0         29.7(2.3) 36.9(2.1) 53.2(0.3) 61.5(0.1) 88.4(0.4) 91.3(0.3) 38.0(0.8) 42.7(0.4)
Full-FT        82M       31.4(2.4) 43.2(2.0) 48.6(5.1) 65.8(2.7) 82.7(1.1) 91.1(0.5) 45.0(3.3) 52.6(0.3)
LoRA (r=4)     0.141M    32.4(1.4) 43.8(1.5) 60.8(1.6) 73.1(0.6) 85.5(0.8) 91.6(0.5) 51.6(0.4) 55.6(0.3)
NOLA-MLP       0.047M    33.7(2.2) 43.3(1.4) 64.5(0.8) 72.6(0.4) 88.0(0.6) 92.2(0.3) 50.5(0.4) 55.5(0.3)

Kết quả: Kết quả về tinh chỉnh trên các tác vụ phân loại hình ảnh được trình bày trong Bảng 6. Cách tiếp cận Nearest Neighbor ngây thơ hoạt động cạnh tranh trên các tập dữ liệu CUB và Caltech. LoRA và NOLA tốt hơn đáng kể so với Linear và Full-FT trên một số thiết lập (ví dụ: CIFAR-100 5 shot trên ViT-B-MAE). Điều này có thể do overfitting của các cách tiếp cận Linear và Full-FT trên số lượng mẫu huấn luyện hạn chế. Khi sử dụng số lượng tham số huấn luyện tương tự, NOLA vượt trội LoRA trong hầu hết các thiết lập trên các kiến trúc và tập dữ liệu. Nó cũng đạt được hiệu suất tương đương với LoRA chỉ với một nửa hoặc một phần ba tham số huấn luyện của LoRA. Điều này phù hợp với các quan sát của chúng tôi trên các tác vụ NLG. Sự khác biệt giữa hai phương pháp đặc biệt đáng chú ý khi số lượng ví dụ huấn luyện nhỏ - hoặc trong thiết lập 5-shot hoặc khi số lượng lớp nhỏ, như trong CIFAR-10. Điều này gợi ý rằng sự cải thiện thu được bởi NOLA có thể do việc giảm số lượng tham số huấn luyện. Cả LoRA và NOLA đều vượt trội đáng kể và nhất quán so với các cách tiếp cận Linear và Full-FT. NOLA có thể dễ dàng được sử dụng trong các lớp MLP vì số lượng tham số huấn luyện tách rời khỏi kích thước ma trận trọng số. Việc áp dụng tương tự của LoRA sẽ yêu cầu gấp 8 lần tham số huấn luyện do kích thước lớp ẩn lớn của mô-đun MLP. Chúng tôi quan sát thực nghiệm rằng NOLA-MLP vượt trội nhẹ so với NOLA trên khối attention (xem Bảng 10 trong phụ lục). Chúng tôi cung cấp kết quả trên bốn tập dữ liệu bổ sung được sử dụng để điểm chuẩn học chuyển giao trong Bảng 7. Aircraft, Food101 và Pets là các tập dữ liệu chi tiết trong khi SUN397 là một tập dữ liệu lớn với 397 lớp. Có sự khác biệt lớn hơn trong hiệu suất của các cách tiếp cận Nearest Neighbor và Linear trên hầu hết các tập dữ liệu này

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
so với những tập trong Bảng 6, gợi ý rằng việc thích ứng với những tập dữ liệu này khó khăn hơn. Phù hợp với các quan sát trước đây của chúng tôi trong Bảng 6, NOLA chỉ với một phần ba số lượng tham số hoạt động tương đương với LoRA.

4 CÁC NGHIÊN CỨU LIÊN QUAN

Các mô hình Transformer cho thị giác và ngôn ngữ: Các mạng Transformer, được giới thiệu bởi (Vaswani et al., 2017), xuất hiện như một mô hình chuỗi-tới-chuỗi trong Xử lý ngôn ngữ tự nhiên (NLP). Thành công của chúng sớm mở rộng sang cộng đồng thị giác máy tính, với các nghiên cứu tiếp theo (Dosovitskiy et al., 2021; Touvron et al., 2021) giới thiệu mạng Vision Transformer (ViT) như một thay thế mạnh mẽ cho các backbone Mạng neural tích chập (CNN). Các Transformer chấp nhận một chuỗi token làm đầu vào. Những token này có thể là, ví dụ, embedding từ trong ngôn ngữ hoặc các patch hình ảnh trong thị giác. BERT (Devlin et al., 2019) và GPT-2 (Radford et al., 2018) trong NLP, và MAE (He et al., 2021) và DINO (Caron et al., 2021) trong thị giác máy tính huấn luyện các mạng transformer thông qua tự giám sát trên lượng lớn dữ liệu không gán nhãn. Những nghiên cứu này chứng minh rằng các mạng transformer lớn khi được huấn luyện trên kho dữ liệu khổng lồ, tổng quát hóa tốt cho các tác vụ phụ thuộc ngay cả khi tinh chỉnh trên rất ít ví dụ cụ thể theo tác vụ. Ví dụ, (Brown et al., 2020) cho thấy rằng GPT-3 với 175B tham số là một học viên vài-shot tốt. Cuối cùng, quy luật tỷ lệ được trình bày bởi (Kaplan et al., 2020) chỉ ra rằng việc tăng đồng thời dữ liệu huấn luyện và tham số mô hình có thể dẫn đến những cải thiện hiệu suất đáng kể và khả năng nổi lên trước đây không có sẵn cho các mô hình nhỏ hơn.

Tinh chỉnh hiệu quả tham số: Do khả năng tổng quát hóa vài-shot chưa từng có, các mạng neural lớn được huấn luyện trước, chẳng hạn như các mô hình nền tảng và LLM đã trở nên cực kỳ phổ biến trong những năm gần đây. Một số lượng ngày càng tăng người dùng đang tùy chỉnh những mô hình này để thích ứng chúng với các tác vụ cụ thể của họ. Tuy nhiên, với kích thước khổng lồ của những mô hình này, tinh chỉnh và lưu trữ toàn bộ tập tham số mô hình (Devlin et al., 2019; Radford et al., 2018) cho mỗi tác vụ là không thực tế. Thách thức này trở nên trầm trọng hơn khi số lượng tác vụ tăng lên. Ngoài các vấn đề về lưu trữ, overhead liên quan đến việc tải các mô hình cụ thể theo tác vụ và chuyển trọng số từ CPU sang GPU thường trở thành một điểm nghẽn tính toán trong nhiều ứng dụng. Các cách tiếp cận Tinh chỉnh hiệu quả tham số (PEFT) nhằm giải quyết những vấn đề này bằng cách xác định số lượng tham số tối thiểu cần thiết để thích ứng một mô hình lớn. Adapters (Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020b; Mahabadi et al., 2021) là các cách tiếp cận PEFT đạt được thích ứng bằng cách thêm các mô-đun nhỏ vào các lớp trung gian của mô hình. Một nhược điểm chính của Adapters là độ trễ bổ sung chúng đưa vào trong suy luận. BitFit (Zaken et al., 2021) chỉ thích ứng bias của mạng. Ladder tuning (Sung et al., 2022a) giảm dấu chân bộ nhớ trong huấn luyện bằng cách tránh lan truyền ngược qua backbone chính. IA3 (Liu et al., 2022) huấn luyện các tham số bổ sung trong mô-đun attention. Một cách tiếp cận PEFT khác được chấp nhận rộng rãi là prompt-tuning cho LLM bao gồm tối ưu hóa một tập hợp mới các token đầu vào, hoặc prompts, cho mỗi tác vụ (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). Mặc dù gợi nhớ về kỹ thuật prompt, sự khác biệt nằm ở việc huấn luyện một tập hợp cụ thể các token prompt trong prompt-tuning có thể cũng tăng độ trễ suy luận.

(Hu et al., 2021) giới thiệu LoRA, chứng minh rằng việc sửa đổi thứ hạng thấp của trọng số gốc là đủ để thích ứng một LLM với một tác vụ mới. Khác với adapters và prompt-tuning, những sửa đổi thứ hạng thấp này có thể được tích hợp vào trọng số gốc, do đó tránh overhead bổ sung trong suy luận. Tuy nhiên, LoRA có hai hạn chế chính: 1) phân rã thứ hạng một đặt ra giới hạn dưới cho các tham số cần thiết để tinh chỉnh, và 2) số lượng tham số yêu cầu phụ thuộc vào kiến trúc và lựa chọn thứ hạng. Nghiên cứu của chúng tôi, có tên NOLA, giải quyết những thách thức này bằng cách tách rời các tham số có thể huấn luyện khỏi cả lựa chọn thứ hạng và kiến trúc mạng. Một số nghiên cứu gần đây đã tìm cách nâng cao LoRA bằng cách lượng tử hóa các tham số của nó (Dettmers et al., 2023; Xu et al., 2023; Kwon et al., 2022; Gong et al., 2023), tối ưu hóa lựa chọn thiết kế của LoRA thông qua tìm kiếm kiến trúc neural (Zhang et al., 2022), hoặc phân bổ tham số động dựa trên thứ hạng yêu cầu cho mỗi ma trận trọng số (Zhang et al., 2023). Hầu hết những cải tiến này cũng tương thích với phương pháp đề xuất của chúng tôi. Thực tế, chúng tôi chứng minh rằng NOLA có thể được lượng tử hóa xuống 4-bit mà không có bất kỳ suy giảm hiệu suất nào, từ đó nhấn mạnh rằng khái niệm lượng tử hóa là riêng biệt và bổ sung cho NOLA.

Các mô hình học sâu nhỏ gọn: Một lĩnh vực có liên quan chặt chẽ đến PEFT là nén mô hình. Pruning (Kim et al., 2020; Lin et al., 2020a; Siems et al., 2021; Tiwari et al., 2021; Hayou et al., 2020; Wang et al., 2020; Li et al., 2021) và lượng tử hóa (Rastegari et al., 2016a; Lee et al., 2021) là các phương pháp chính để nén mạng neural. Các kỹ thuật như những kỹ thuật trong (Kusupati et al., 2020; Isik et al., 2022) có thể đạt được tỷ lệ pruning cao, dẫn đến nén đáng kể.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Lời cảm ơn: Nghiên cứu này được hỗ trợ một phần bởi DARPA theo Hợp đồng số HR00112190135 và HR00112290115 và các grant NSF 1845216 và 2339898.

TÀI LIỆU THAM KHẢO

Armen Aghajanyan, Sonal Gupta, và Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 7319–7328, 2021. 1

Yoshua Bengio, Nicholas Léonard, và Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. 4

Lukas Bossard, Matthieu Guillaumin, và Luc Van Gool. Food-101 – mining discriminative components with random forests. Trong European Conference on Computer Vision, 2014. 7

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 1, 9

Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, và Armand Joulin. Emerging properties in self-supervised vision transformers, 2021. 9

Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, và Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 1

Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, và Yixin Chen. Compressing neural networks with the hashing trick. Trong International conference on machine learning, trang 2285–2294. PMLR, 2015. 16

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. 16

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. 1, 2, 7, 9

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], tháng 5 2019. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805. 9

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, và cộng sự. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations, 2020. 1

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 9

Li Fei-Fei, Rob Fergus, và Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop, 2004. 7

Claire Gardent, Anastasia Shimorina, Shashi Narayan, và Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. Trong Proceedings of the 10th International Conference on Natural Language Generation, trang 124–133, 2017. 4, 17

Zhuocheng Gong, Jiahao Liu, Qifan Wang, Yang Yang, Jingang Wang, Wei Wu, Yunsen Xian, Dongyan Zhao, và Rui Yan. Prequant: A task-agnostic quantization approach for pre-trained language models, 2023. 9

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Karen Hambardzumyan, Hrant Khachatrian, và Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121 [cs], tháng 12 2020. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121. 9

Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, và Yee Whye Teh. Robust pruning at initialization. arXiv preprint arXiv:2002.08797, 2020. 9

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016. 16

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross Girshick. Masked autoencoders are scalable vision learners, 2021. 9

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 7

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], tháng 6 2019. URL http://arxiv.org/abs/1902.00751. 4, 9

Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, và cộng sự. Lora: Low-rank adaptation of large language models. Trong International Conference on Learning Representations, 2021. 1, 2, 4, 5, 6, 7, 9

Berivan Isik, Tsachy Weissman, và Albert No. An information-theoretic justification for model pruning. Trong International Conference on Artificial Intelligence and Statistics, trang 3821–3846. PMLR, 2022. 9

Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, và Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 2704–2713, 2018. 3

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language models, 2020. 9

Woojeong Kim, Suhyun Kim, Mincheol Park, và Geunseok Jeon. Neuron merging: Compensating for pruned neurons. Advances in Neural Information Processing Systems, 33:585–595, 2020. 9

Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton.. cifar-10 and cifar-100 datasets. URl: https://www.cs.toronto.edu/kriz/cifar.html, 6(1):1, 2009. 7

Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton.. The cifar-10 dataset. online: http://www.cs.toronto.edu/kriz/cifar.html, 55(5), 2014. 7, 16

Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, và Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. Trong Proceedings of the International Conference on Machine Learning, tháng 7 2020. 9

Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, và Dongsoo Lee. Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv:2210.03858, 2022. 9

Junghyup Lee, Dohyung Kim, và Bumsub Ham. Network quantization with element-wise gradient scaling. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6448–6457, 2021. 9

Brian Lester, Rami Al-Rfou, và Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691 [cs], tháng 4 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691. 9

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, và Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. Trong International Conference on Learning Representations, 2018. 1

Xiang Lisa Li và Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], tháng 1 2021. URL http://arxiv.org/abs/2101.00190. 4, 9

Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan Yang, Jincheng Ma, Qi Tian, và Rongrong Ji. Towards compact cnns via collaborative compression. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6438–6447, 2021. 9

Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, và Martin Jaggi. Dynamic model pruning with feedback. arXiv preprint arXiv:2006.07253, 2020a. 9

Zhaojiang Lin, Andrea Madotto, và Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 441–459, Online, tháng 11 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41. 4, 9

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022. 9

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, và Jie Tang. GPT Understands, Too. arXiv:2103.10385 [cs], tháng 3 2021. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385. 9

Rabeeh Karimi Mahabadi, James Henderson, và Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers, 2021. 9

Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, và Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 7

Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, và cộng sự. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020. 4, 17

Parsa Nooralinejad, Ali Abbasi, Soroush Abbasi Koohpayegani, Kossar Pourahmadi Meibodi, Rana Muhammad Shahroz Khan, Soheil Kolouri, và Hamed Pirsiavash. Pranc: Pseudo random networks for compacting deep models. Trong International Conference on Computer Vision (ICCV), 2022. 2, 3, 4, 16

Jekaterina Novikova, Ondřej Dušek, và Verena Rieser. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017. 4

Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, và cộng sự. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1

O. M. Parkhi, A. Vedaldi, A. Zisserman, và C. V. Jawahar. Cats and dogs. Trong Computer Vision and Pattern Recognition, 2012. 7

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning, 2021. 4

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, và cộng sự. Improving language understanding by generative pre-training. 2018. 9

Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, và Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. Trong European conference on computer vision, trang 525–542. Springer, 2016a. 9

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, và Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks, 2016b. 3

Sylvestre-Alvise Rebuffi, Hakan Bilen, và Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv:1705.08045 [cs, stat], tháng 11 2017. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045. 9

Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, và Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020. 4

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 2

Julien Niklas Siems, Aaron Klein, Cedric Archambeau, và Maren Mahsereci. Dynamic pruning of a neural network via gradient signal-to-noise ratio. Trong 8th ICML Workshop on Automated Machine Learning (AutoML), 2021. 9

Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005, 2022a. 9

Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 5227–5237, 2022b. 1

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 7

Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, và Deepak K Gupta. Chipnet: Budget-aware pruning with heaviside continuous approximations. arXiv preprint arXiv:2102.07156, 2021. 9

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Herve Jegou. Training data-efficient image transformers and distillation through attention, 2021. 9

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 6

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Proceedings of the 31st International Conference on Neural Information Processing Systems, trang 6000–6010, 2017. 9

Huan Wang, Can Qin, Yulun Zhang, và Yun Fu. Neural pruning via growing regularization. arXiv preprint arXiv:2012.09243, 2020. 9

Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, và Pietro Perona. Caltech-ucsd birds 200. 2010. 7

Ross Whitman. Timm. https://github.com/huggingface/pytorch-image-models/tree/main/timm. [Trực tuyến; truy cập 28-Sep-2023]. 7

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, và Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. Trong Computer Vision and Pattern Recognition, 2010. 7

Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, và Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. 1, 2, 9

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. 9

Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, và Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning, 2023. 9

Yuanhan Zhang, Kaiyang Zhou, và Ziwei Liu. Neural prompt search, 2022. 9

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A PHỤ LỤC

A.1 ĐO THỨ HẠNG CỦA CÁC GIẢI PHÁP CÓ THỂ TRONG NOLA SO VỚI PRANC:
Chọn n vectơ cơ sở (mỗi vectơ có d chiều) trong PRANC sẽ dẫn đến tất cả các ma trận học được có thể sống trong một không gian con n chiều. Tuy nhiên, vì NOLA với cùng số lượng tham số tổng cộng (k+l=n) sử dụng phân tích A×B, các giải pháp có thể có thể sống trong một không gian con chiều cao hơn. Chúng tôi thực hiện một thí nghiệm đơn giản bằng cách lấy mẫu một số vectơ hệ số ngẫu nhiên, tái tạo ma trận ∆W, định hình lại nó thành một vectơ dài (d²) chiều, và đo thứ hạng của hiệp phương sai của các mẫu để xem bao nhiều không gian toàn bộ được bao phủ bởi các mẫu. Kết quả được thể hiện trong Hình 2 cho một thí nghiệm đơn giản với d và n thay đổi. Như mong đợi, NOLA có thể bao phủ toàn bộ không gian (thứ hạng đầy đủ) sử dụng một số lượng nhỏ tham số so với PRANC. Lưu ý rằng thứ hạng trong phân tích này là về hiệp phương sai của các mẫu ma trận trọng số ngẫu nhiên có thể và không nên nhầm lẫn với thứ hạng trong công thức LoRA hoặc NOLA.

Hình 2: So sánh thứ hạng của các mẫu trong không gian con giải pháp cho PRANC và NOLA, với cùng số lượng tham số, n. "Phần trăm bao phủ" là thứ hạng không gian con chia cho thứ hạng tối đa có thể (d²), vì vậy 1.0 biểu thị thứ hạng đầy đủ. Như mong đợi, phạm vi bao phủ cho PRANC tăng tuyến tính trong khi nó bão hòa rất nhanh đối với NOLA.

A.2 NOLA TRONG HUẤN LUYỆN TỪ ĐẦU:
MLP trên MNIST (một thí nghiệm đồ chơi):
Chúng tôi tin rằng NOLA là một tham số hóa lại tốt hơn so với PRANC và có thể đạt được các cực tiểu cục bộ mà PRANC không thể đạt được. Để chứng minh điều này thực nghiệm, chúng tôi thực hiện một thí nghiệm rất đơn giản nơi chúng tôi áp dụng MLP 2 lớp cho tác vụ phân loại MNIST. Vì chúng tôi muốn đo phương pháp nào tốt hơn trong việc đạt đến các cực tiểu cục bộ và không nhất thiết là khả năng tổng quát, chúng tôi đánh giá các mô hình bằng loss huấn luyện. Ngoài ra, chúng tôi có chủ ý sử dụng một số lượng lớn neuron trong lớp ẩn để tham số hóa quá mức và tăng số lượng cực tiểu cục bộ.

Chúng tôi sử dụng một lớp tuyến tính từ 784 đặc trưng đến 256 với bias theo sau bởi ReLU và một lớp tuyến tính từ 256 đến 10 lớp. Chúng tôi sử dụng tất cả 60K mẫu để huấn luyện để báo cáo loss huấn luyện. Đối với cả PRANC và NOLA, chúng tôi sử dụng 32 tham số cho mỗi lớp. Các siêu tham số khác giống nhau cho cả hai: 200 epoch, batch size 512 với lr= 0.05. Chúng tôi sử dụng thứ hạng r= 4 cho NOLA. PRANC có loss huấn luyện cuối cùng là 0.87, trong khi NOLA đạt được loss huấn luyện 0.71. Thí nghiệm đơn giản này hỗ trợ thực nghiệm rằng NOLA có khả năng biểu diễn tốt hơn. Ngoài ra, huấn luyện PRANC hoàn thành trong 1152 giây trong khi NOLA hoàn thành trong 579 giây. Chúng tôi sẽ để lại nghiên cứu lý thuyết về so sánh này cho công việc tương lai. Hơn nữa, thí nghiệm này chứng minh thực nghiệm rằng NOLA là một phương pháp tổng quát, và thành công của nó không phụ thuộc vào kiến trúc của transformer hoặc các mô-đun attention.

CNN trên ImageNet100 và CIFAR-10:
Hơn nữa, để so sánh khả năng biểu diễn của NOLA và PRANC, chúng tôi huấn luyện NOLA từ đầu trên một tác vụ phân loại hình ảnh sử dụng kiến trúc CNN. Đối với mỗi lớp tích chập, chúng tôi định hình lại tất cả tham số của một lớp thành một ma trận (hình dạng gần hình vuông) và áp dụng NOLA cho ma trận. Sau đó, chúng tôi định hình lại nó về hình dạng gốc của tích chập. Ngoài ra, chúng tôi huấn luyện LoRA sử dụng cách tiếp cận tương tự như NOLA. Chúng tôi tuân theo một thiết lập tương tự như (Nooralinejad et al., 2022) cho các thí nghiệm về phân loại hình ảnh.

Tập dữ liệu và kiến trúc: Chúng tôi xem xét hai kiến trúc trong các thí nghiệm: ResNet20 với 270K tham số, và ResNet18 (He et al., 2016) với 11M tham số. Chúng tôi huấn luyện ResNet20 trên CIFAR10 (Krizhevsky et al., 2014), và ResNet18 trên ImageNet100 (Deng et al., 2009).

Kết quả: Chúng tôi báo cáo kết quả của ImageNet100 trong Bảng 9, và CIFAR10 trong Bảng 8. NOLA vượt trội cả PRANC và LoRA với số lượng tham số tương tự.

Chi tiết triển khai: Đối với ImageNet100 và ResNet18, chúng tôi sử dụng k=l= 2.000 cơ sở cho mỗi trong 20 mô-đun, và cho bộ phân loại (lớp tuyến tính cuối cùng), chúng tôi sử dụng k=l= 10.000, dẫn đến tổng cộng 100.000 tham số có thể huấn luyện loại trừ 9.600 tham số batchnorm. Chúng tôi sử dụng thứ hạng 64 cho tất cả các lớp. Chúng tôi huấn luyện tất cả các mô hình sử dụng tối ưu hóa Adam với tốc độ học 0.001 và batch size 256 trong 200 epoch. Đối với CIFAR-10 và ResNet20, chúng tôi sử dụng k=l= 250 cơ sở cho mỗi mô-đun tích chập, và cho lớp tuyến tính, chúng tôi sử dụng k=l= 1000 tham số. Chúng tôi sử dụng batch size 256, tối ưu hóa Adam, và tốc độ học 0.001. Chúng tôi sử dụng một GPU NVIDIA-GeForce RTX 3090 duy nhất cho tất cả các thí nghiệm.

So sánh thời gian huấn luyện: Chúng tôi đo thời gian huấn luyện của NOLA và PRANC trên một GPU NVIDIA-GeForce RTX 3090 duy nhất và batch size 256. Lưu ý rằng thời gian huấn luyện bao gồm cả lượt truyền xuôi và ngược cho mỗi batch. Trung bình, NOLA xử lý một batch trong 228ms trong khi PRANC thực hiện tương tự trong 1070 ms, vì vậy NOLA nhanh hơn 4.6 lần so với PRANC.

Bảng 8: Huấn luyện trên CIFAR10: Kết quả của phương pháp chúng tôi trên tập dữ liệu CIFAR10 và ResNet20.
Phương pháp    # Tham số  Độ chính xác
mô hình huấn luyện 269.722  88.92%
PRANC          12.752     81.5%
LoRA           13.295     81.5%
NOLA           12.876     82.4%

Bảng 9: Huấn luyện trên ImageNet100: Kết quả của phương pháp chúng tôi trên tập dữ liệu ImageNet-100 và ResNet18
Phương pháp          # Tham số  Độ chính xác
mô hình huấn luyện   11.227.812 82.1%
HashedNet (Chen et al., 2015) 129.200   52.96%
PRANC                119.200    61.08%
LoRA                 150.000    63.50%
NOLA                 109.600    64.66%

A.3 NGHIÊN CỨU KHỬ YẾU TỐ VÀ CHI TIẾT CỦA NOLA TRÊN VISION TRANSFORMERS:
Chi tiết triển khai: Chúng tôi xem xét các tốc độ học 5e−3, 1e−3 và 5e−4 cho các phương pháp LoRA, NOLA và Linear và 8e−5, 5e−5, 3e−5 và 1e−5 cho Full-FT. Thiết lập tốt nhất được chọn dựa trên hiệu suất trên tập xác thực. Để tạo tập dữ liệu k-shot, chúng tôi lấy mẫu ngẫu nhiên không thay thế từ tập huấn luyện. Đối với mỗi tập này, chúng tôi chạy với ba khởi tạo khác nhau của mạng. Quá trình này được lặp lại bốn lần và các giá trị trung bình được báo cáo.

So sánh giữa NOLA-QV và NOLA-MLP: Chúng tôi thử nghiệm với lớp NOLA trong cả mô-đun attention và MLP của vision transformer. Chúng tôi quan sát rằng áp dụng NOLA trên MLP hoạt động tốt hơn so với trên khối attention (Bảng 10). Do đó, chúng tôi sử dụng NOLA-MLP làm thiết lập mặc định. Lưu ý rằng số lượng tham số có thể huấn luyện giữ nguyên trong cả hai phiên bản. Khác với điều này, áp dụng LoRA trên khối MLP sẽ yêu cầu số lượng tham số có thể huấn luyện cao hơn đáng kể do kích thước tăng của các ma trận trọng số trong MLP so với những ma trận trong khối attention.

Bảng 10: So sánh giữa NOLA trong các khối MLP và attention: Chúng tôi quan sát rằng NOLA trên khối MLP hiệu quả hơn. Chúng tôi chọn điều này làm thiết lập mặc định.

Mô hình cơ sở # Tham số  CIFAR-10   CIFAR-100  CUB-200-2011  Caltech-101
             huấn luyện  5    10    5    10    5    10       5    10
ViT-L
NOLA-QV      47K        87.0(0.9) 91.6(0.7) 74.8(0.6) 80.4(0.9) 75.3(0.4) 81.7(0.3) 87.9(1.1) 90.6(0.5)
NOLA-MLP     47K        87.9(1.3) 92.2(0.5) 75.1(0.6) 81.3(0.8) 75.5(0.6) 81.7(0.4) 88.0(1.2) 90.6(0.5)

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A.4 KẾT QUẢ CỦA TÁC VỤ NLG TRÊN CÁC TẬP DỮ LIỆU DART VÀ WEBNLG:
Trong Bảng 11, chúng tôi báo cáo thêm kết quả tương tự như Bảng 1 sử dụng GPT-2 M và GPT-2 L trên các tập dữ liệu DART (Nan et al., 2020) và WebNLG (Gardent et al., 2017).

Bảng 11: Tập dữ liệu DART và WebNLG: Tương tự như Bảng 1, chúng tôi so sánh NOLA với các phương pháp khác. NOLA ngang bằng hoặc tốt hơn với các phương pháp khác với cùng số lượng tham số.

GPT-2 M
Phương pháp     Các lớp    Thứ hạng  # Tham số     DART              WebNLG
                thích ứng  Adapter   có thể huấn luyện BLEU↑MET↑TER↓   BLEU↑MET↑TER↓
Finetune        Tất cả lớp   -       354.000M      46.2 0.39 0.46   46.5 0.38 0.53
AdapterL        Lớp phụ      -       0.370M        42.4 0.36 0.48   50.2 0.38 0.43
AdapterL        Lớp phụ      -       11.000M       45.2 0.38 0.46   54.9 0.41 0.39
Finetune        2 lớp cuối   -       24.000M       41.0 0.34 0.56   36.0 0.31 0.72
PreLayer        Token phụ    -       0.350M        46.4 0.38 0.46   55.1 0.41 0.40
LoRA            QV           4       0.350M        47.1 0.39 0.46   54.9 0.41 0.39
LoRA            QV           1       0.098M        46.4 0.38 0.48   53.5 0.40 0.40
NOLA (Của chúng tôi) QV      8       0.096M        47.0 0.38 0.48   53.9 0.40 0.40
NOLA (Của chúng tôi) MLP     8       0.096M        47.1 0.38 0.47   54.7 0.41 0.40
NOLA (Của chúng tôi) QV      8       0.048M        45.7 0.38 0.49   53.8 0.40 0.40
NOLA (Của chúng tôi) MLP     8       0.048M        45.5 0.38 0.49   53.0 0.40 0.40

GPT-2 L
Finetune        Tất cả lớp   -       774.000M      47.0 0.39 0.46   55.5 0.42 0.42
AdapterL        Lớp phụ      -       0.880M        45.7 0.38 0.46   56.0 0.41 0.39
AdapterL        Lớp phụ      -       230.000M      47.1 0.39 0.45   57.7 0.43 0.39
PreLayer        Token phụ    -       0.770M        46.7 0.38 0.45   56.3 0.42 0.40
LoRA            QV           4       0.770M        47.5 0.39 0.45   57.1 0.43 0.38
LoRA            QV           1       0.184M        47.7 0.39 0.47   55.9 0.42 0.39
NOLA (Của chúng tôi) QV      8       0.144M        47.8 0.39 0.47   55.8 0.41 0.39
NOLA (Của chúng tôi) MLP     8       0.144M        47.8 0.39 0.47   56.0 0.42 0.39
NOLA (Của chúng tôi) QV      8       0.072M        46.4 0.38 0.48   55.5 0.41 0.38
NOLA (Của chúng tôi) MLP     8       0.072M        46.8 0.38 0.48   55.8 0.41 0.39
