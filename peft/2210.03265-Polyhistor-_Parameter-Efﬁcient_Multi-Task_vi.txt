# 2210.03265.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2210.03265.pdf
# Kích thước tệp: 1481032 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Polyhistor: Thích ứng Đa nhiệm vụ Hiệu quả Tham số
cho Các Nhiệm vụ Thị giác Dày đặc
Yen-Cheng Liu
Georgia Tech
ycliu@gatech.edu

Chih-Yao Ma
Meta
cyma@meta.com

Junjiao Tian
Georgia Tech
jtian73@gatech.edu

Zijian He
Meta
zijian@meta.com

Zsolt Kira
Georgia Tech
zkira@gatech.edu

Tóm tắt
Việc thích ứng các mô hình được đào tạo trước quy mô lớn với các nhiệm vụ hạ nguồn khác nhau thông qua tinh chỉnh là một phương pháp chuẩn trong học máy. Gần đây, các phương pháp tinh chỉnh hiệu quả tham số cho thấy tiềm năng trong việc thích ứng một mô hình được đào tạo trước với các nhiệm vụ khác nhau trong khi chỉ đào tạo một số ít tham số. Bất chấp thành công của chúng, hầu hết các phương pháp hiện tại được đề xuất trong các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên với các Transformer ngôn ngữ, và việc thích ứng với các nhiệm vụ Thị giác Máy tính với Vision Transformers vẫn còn chưa được khám phá đầy đủ, đặc biệt là đối với các nhiệm vụ thị giác dày đặc. Hơn nữa, trong các thiết lập đa nhiệm vụ, việc tinh chỉnh và lưu trữ riêng biệt các mô hình khác nhau cho các nhiệm vụ khác nhau là không hiệu quả. Trong công trình này, chúng tôi cung cấp một benchmark đa nhiệm vụ hiệu quả tham số mở rộng và kiểm tra các phương pháp tinh chỉnh hiệu quả tham số NLP hiện có cho các nhiệm vụ thị giác. Kết quả của chúng tôi trên bốn nhiệm vụ thị giác dày đặc khác nhau cho thấy rằng các phương pháp hiện tại không thể được tích hợp hiệu quả do tính chất phân cấp của các Hierarchical Vision Transformers. Để khắc phục vấn đề này, chúng tôi đề xuất Polyhistor và Polyhistor-Lite, bao gồm Decomposed HyperNetworks và Layer-wise Scaling Kernels, để chia sẻ thông tin qua các nhiệm vụ khác nhau với một số ít tham số có thể đào tạo. Điều này dẫn đến cải thiện hiệu suất thuận lợi so với các phương pháp hiệu quả tham số hiện có trong khi sử dụng ít tham số có thể đào tạo hơn. Cụ thể, Polyhistor đạt được độ chính xác cạnh tranh so với các phương pháp tiên tiến nhất trong khi chỉ sử dụng 10% tham số có thể đào tạo của chúng. Hơn nữa, các phương pháp của chúng tôi cho thấy cải thiện hiệu suất lớn hơn khi sử dụng các mạng lớn hơn và nhiều dữ liệu đào tạo trước hơn.

1 Giới thiệu
Các mô hình nền tảng được đào tạo với các tập dữ liệu quy mô lớn đã cho thấy thành công trong việc thích ứng với nhiều nhiệm vụ NLP và thị giác hạ nguồn khác nhau [1]. Khi các mô hình nền tảng tiên tiến nhất phát triển đến các mô hình tỷ hoặc thậm chí nghìn tỷ tham số [2,3,4,5,6], việc tinh chỉnh riêng lẻ tất cả tham số của mô hình lãng phí đáng kể tài nguyên tính toán. Hơn nữa, đối với các mô hình đa nhiệm vụ, cả việc tinh chỉnh và lưu trữ các mô hình riêng biệt cho nhiều nhiệm vụ trở nên không khả thi trên các thiết bị có tài nguyên tính toán thấp.

Để giảm bớt vấn đề này, một số công trình [7,8,9] đã đề xuất các phương pháp tinh chỉnh hiệu quả tham số để đạt được sự cân bằng tốt hơn giữa các tham số có thể đào tạo và độ chính xác trên các nhiệm vụ hạ nguồn. Bằng cách chỉ đào tạo một lượng nhỏ tham số, các phương pháp hiện tại này có thể thu hẹp đáng kể khoảng cách độ chính xác so với baseline tinh chỉnh tất cả tham số. Tuy nhiên, các phương pháp hiện tại này chủ yếu tập trung vào các nhiệm vụ NLP [10,11,12] hoặc thích ứng nhiệm vụ đơn lẻ trên phân loại hình ảnh [9], và khả năng áp dụng của chúng cho các nhiệm vụ thị giác phức tạp hơn chưa rõ ràng. Mặt khác, các phương pháp thích ứng nhiệm vụ đơn lẻ [7,10,11,12,8] vẫn cần học và lưu trữ các tham số theo nhiệm vụ, và số lượng tham số có thể đào tạo tăng theo số lượng nhiệm vụ.

Do đó, trong bài báo này, chúng tôi đầu tiên tiến hành một nghiên cứu kỹ lưỡng về cách các phương pháp hiệu quả tham số thành công hiện tại trên các nhiệm vụ NLP hoạt động trên các nhiệm vụ thị giác, đặc biệt là trên các nhiệm vụ thị giác dày đặc thách thức hơn (ví dụ: phân đoạn ngữ nghĩa, ước lượng pháp tuyến). Thứ hai, dựa trên các phát hiện của chúng tôi, chúng tôi sau đó thiết kế một phương pháp hiệu quả tham số mới cho việc thích ứng với nhiều nhiệm vụ thị giác dày đặc. Phương pháp của chúng tôi tận dụng các module được chia sẻ qua các nhiệm vụ và khuyến khích mô hình sử dụng thông tin được chia sẻ theo cách hiệu quả tham số hơn.

Để bắt đầu, chúng tôi đầu tiên đánh giá các phương pháp hiệu quả tham số hiện có trong NLP trên các vấn đề thị giác dày đặc. Chúng tôi chọn áp dụng các phương pháp này lên hierarchical vision transformers (HVTs) xem xét kết quả tiên tiến nhất của chúng trên nhiều nhiệm vụ thị giác per-pixel [13,14]. Thông qua các nghiên cứu mở rộng của chúng tôi, chúng tôi tìm thấy hai hạn chế trong các công trình này. Đầu tiên, các phương pháp dựa trên adapter [11,15], đã cho thấy hiệu suất mạnh mẽ trên các benchmark thích ứng hiệu quả tham số NLP, không thể được tích hợp hiệu quả với HVTs. Điều này là do việc sử dụng tham số của adapters trong các khối transformer sau tăng bậc hai theo tỷ lệ layer (xem Hình 1c). Thứ hai, phương pháp hiệu quả tham số đa nhiệm vụ tiên tiến nhất [16] áp dụng một hyper-network để tạo ra trọng số của adapters và chia sẻ thông tin qua các nhiệm vụ NLP khác nhau, trong khi chúng tôi thấy rằng nó vốn đã yêu cầu một số lượng lớn tham số có thể đào tạo trong hyper-network (xem Phần 4.1 để thảo luận thêm).

Để giải quyết các hạn chế trên, chúng tôi đề xuất Polyhistor-Lite, bao gồm hai thành phần chính, Decomposed Lite-HyperNetworks và Layer-wise Scaling Kernels. Hai phương pháp này giảm các tham số có thể đào tạo trong hai khía cạnh tương ứng, bao gồm giảm tham số cho hyper-networks trong kiến trúc đa nhiệm vụ và giảm tham số cho adapters được sử dụng trong HVTs.

Cụ thể, để giảm việc sử dụng tham số của kiến trúc đa nhiệm vụ, chúng tôi phân tách một hyper-network thành một cặp hyper-networks riêng biệt. Khác với phương pháp hiện tại, trong đó một hyper-network tương đối lớn được sử dụng để tạo ra vector dài có thể được định hình lại thành ma trận trọng số của adapter, các hyper-networks phân tách của chúng tôi riêng lẻ tạo ra hai ma trận hạng thấp được nhân để xây dựng trọng số adapter. Kết quả là, chúng tôi có thể dựa vào xấp xỉ hạng thấp này để giảm việc sử dụng tham số trong hyper-network nhưng vẫn duy trì hiệu suất của nó trên các nhiệm vụ hạ nguồn. Ngoài ra, để kích hoạt các hypernetworks được chia sẻ qua các layers trong HVTs, chúng tôi phân tích một ma trận trọng số adapter thành hai kernels, bao gồm Template Kernels và Scaling Kernels. Hai kernels này được nhân thông qua Kronecker Product để phù hợp với các kích thước khác nhau của adapters, và điều này đạt được bằng cách kiểm soát kích thước của Scaling Kernels dựa trên tỷ lệ của layer/adapter (và sử dụng cùng kích thước của Template Kernels qua các layers). Theo cách này, các tham số của trọng số adapter có thể được giảm hiệu quả với việc hy sinh tối thiểu về độ chính xác của các nhiệm vụ hạ nguồn.

Để benchmark vấn đề, chúng tôi xây dựng một framework thống nhất với cùng chi tiết triển khai và cung cấp so sánh toàn diện và công bằng giữa các công trình thích ứng hiệu quả tham số hiện tại trong NLP trên các vấn đề thị giác dày đặc đa nhiệm vụ của chúng tôi. Chúng tôi cũng chứng minh rằng, với việc tích hợp Decomposed HyperNetworks và Layer-wise Scaling Kernels đề xuất của chúng tôi, chúng tôi có thể đạt được sự cân bằng tốt hơn nhiều giữa các tham số có thể đào tạo và độ chính xác so với các phương pháp hiện tại. Cụ thể, hầu hết các phương pháp hiện tại đều gặp khó khăn trong việc đạt hiệu suất của baseline đơn giản, cái mà riêng lẻ tinh chỉnh toàn bộ mạng cho mỗi nhiệm vụ, trong khi phương pháp của chúng tôi đạt kết quả tốt hơn baseline đơn giản trong khi chỉ đào tạo ít hơn 10% tham số trong một mô hình. So với phương pháp thích ứng hiệu quả tham số đa nhiệm vụ tiên tiến nhất, Hyperformer [16], phương pháp của chúng tôi đạt cải thiện hiệu suất cạnh tranh với việc giảm 90% tham số có thể đào tạo của phương pháp của họ. Thú vị là, chúng tôi cũng quan sát thấy rằng phương pháp đề xuất của chúng tôi mang lại cải thiện hiệu suất cao khi áp dụng cho mạng được đào tạo trước trên tập dữ liệu lớn hơn (ImageNet-22k). Chúng tôi sẽ công khai phát hành mã của chúng tôi để tạo điều kiện cho nghiên cứu tương lai.

Tóm lại, chúng tôi liệt kê các đóng góp của mình như sau:
• Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là người đầu tiên giải quyết thích ứng đa nhiệm vụ hiệu quả tham số cho các nhiệm vụ thị giác. Chúng tôi phát triển một framework thống nhất để benchmark một số phương pháp tinh chỉnh hiệu quả tham số NLP trên các nhiệm vụ thị giác dày đặc.
• Chúng tôi đề xuất một phương pháp mới — Polyhistor-Lite đạt được cải thiện hiệu suất đáng kể với các tham số đào tạo rất thấp so với các phương pháp hiện tại.

--- TRANG 2 ---
• Chúng tôi quan sát thấy rằng phương pháp của chúng tôi có thể mang lại cải thiện hiệu suất thêm khi áp dụng cho các mô hình với tập dữ liệu đào tạo trước lớn hơn hoặc với các backbone lớn hơn.

2 Các Công trình Liên quan
Học hiệu quả tham số nhằm thích ứng một mô hình được đào tạo trước với một nhiệm vụ mới bằng cách chỉ đào tạo một số lượng nhỏ tham số. Phương pháp đơn giản nhất là đóng băng bộ encoder được đào tạo trước và chỉ tinh chỉnh layer cuối cùng, trong khi, về độ chính xác của các nhiệm vụ hạ nguồn, nó vẫn còn xa so với việc tinh chỉnh đầy đủ. Do đó, để đạt được sự cân bằng tốt hơn giữa độ chính xác và số lượng tham số có thể điều chỉnh, một số công trình [7,10,11,12,16,9,8] đã đề xuất các phương pháp hiệu quả tham số hơn, và chúng tôi tóm tắt các công trình này trong các đoạn sau.

Thích ứng Hiệu quả Tham số Nhiệm vụ Đơn lẻ. Một số công trình xây dựng dựa trên Adapter [11], đây là một module giống như bottleneck được đặt qua kiến trúc và được đào tạo trong khi phần còn lại của mô hình gốc bị đóng băng. Bằng cách thay đổi chiều của các vector ẩn, người ta có thể dễ dàng kiểm soát sự cân bằng giữa các tham số có thể đào tạo và độ chính xác. Ví dụ, Houlsby et al. [11] đề xuất áp dụng hai module adapter được đặt sau các attention layers và MLP layers tương ứng, trong khi Pfeiffer et al. [15] chỉ sử dụng adapters sau MLP layers và cho thấy hiệu quả tham số hơn. Hơn nữa, PHM-Layer [12] học hai loại ma trận, một ma trận "chậm" được chia sẻ qua các layers và ma trận "nhanh" khác được học riêng lẻ trong các layers khác nhau, để tạo ra trọng số adapter thông qua Kronecker Product [17]. Compacter [12] tiếp tục giảm các tham số bằng cách phân tách ma trận chậm thành hai vector hạng một. Khác với mục tiêu chia sẻ ma trận chậm qua các layers của chúng, chúng tôi áp dụng Kronecker Product để mở rộng adapters một cách hiệu quả đến các tỷ lệ layer khác nhau.

Ngoài ra, có các công trình học hiệu quả tham số khác. BitFit [7] cho thấy việc chỉ điều chỉnh bias trong tất cả các layers cải thiện so với linear probing. Một số công trình khác tinh chỉnh các vector có thể học, chẳng hạn như các vector có thể học trong word embeddings đầu vào [18] và các vector có thể học được tích hợp với keys/values trong mỗi layer của transformers [19]. LoRA [10] tạo ra hai ma trận hạng thấp, được nhân và phục vụ như một residual của các ma trận trọng số attention. Trong khi các phương pháp trên cho thấy kết quả thuận lợi với việc sử dụng ít tham số có thể đào tạo hơn, mục tiêu của các công trình này là thích ứng nhiệm vụ đơn lẻ.

Thích ứng Hiệu quả Tham số Đa Nhiệm vụ. Khi nhiều nhiệm vụ được học đồng thời, người ta có thể chia sẻ một số thông tin đồng nhất qua các nhiệm vụ khác nhau và tiết kiệm việc sử dụng tham số bằng cách loại bỏ các đặc trưng được học trùng lặp. Để đạt mục tiêu này, Hyperformer [16] giới thiệu một hyper-network, nhận đầu vào là các task embeddings và tạo ra trọng số của adapters trong các nhiệm vụ khác nhau. Vì chỉ có các tham số trong hyper-network cần được đào tạo, số lượng tham số có thể đào tạo trong các thành phần theo nhiệm vụ có thể được giảm trong thiết lập đa nhiệm vụ. Mặt khác, Sung et al. [20] cho thấy rằng việc đơn giản thêm một adapter duy nhất trên language transformer và chia sẻ adapter qua các nhiệm vụ có thể đạt kết quả hứa hẹn trong các nhiệm vụ thị giác-ngôn ngữ đa phương thức (ví dụ: tạo chú thích hình ảnh).

Thích ứng Hiệu quả Tham số cho Thị giác. Bất chấp kết quả hứa hẹn, hầu hết các phương pháp học hiệu quả tham số được đánh giá trên language transformers và benchmarks NLP, và học hiệu quả tham số trên Vision Transformer [9] vẫn là một chủ đề chưa được khám phá đầy đủ. Một công trình gần đây, Visual Prompt Tuning (VPT) [9], bắt đầu nghiên cứu về học hiệu quả tham số trên Vision Transformers, và nó theo ý tưởng của prompt tuning trong các nhiệm vụ ngôn ngữ và thêm và tinh chỉnh một số vector có thể học bổ sung trong không gian đầu vào của Vision Transformers được đào tạo trước. VPT tập trung vào thích ứng nhiệm vụ đơn lẻ, trong khi công trình của chúng tôi tập trung vào thích ứng đa nhiệm vụ.

Để so sánh công bằng các phương pháp học hiệu quả tham số khác nhau, He et al. [21] trình bày một nghiên cứu thực nghiệm và đánh giá lại các phương pháp học hiệu quả tham số (BitFit, Adapters, Prefix Tuning, và LoRA) dưới cùng cấu hình thí nghiệm cho các nhiệm vụ NLP. Được truyền cảm hứng từ công trình của họ, chúng tôi triển khai các phương pháp NLP hiệu quả tham số đã đề cập (và bao gồm thêm các công trình mới nhất [12,16,9]) trên các nhiệm vụ thị giác dày đặc của chúng tôi, tiến hành các thí nghiệm so sánh, và so sánh công bằng các phương pháp này.

3 Kiến thức Nền tảng
Hierarchical Vision Transformers. Vision Transformer [22] dựa trên kiến trúc transformers [22] và hoạt động trên một tập hợp các patch tokens thu được từ hình ảnh. Là một biến thể của Vision Transformer, Hierarchical Vision Transformer [13,14,23,24,25,26] tạo ra các biểu diễn đặc trưng đa tỷ lệ, và cấu trúc phân cấp của nó trích xuất thông tin chi tiết và xử lý tốt hơn

--- TRANG 3 ---
(a)(b)
(c)
Hình 1: Minh họa về (a) Hierarchical Vision Transformer và (b) Adapter. (c) Khi áp dụng adapters trong một Hierarchical Vision Transformer, số lượng tham số tăng bậc hai theo tỷ lệ block. Lưu ý rằng C biểu thị chiều của các vector đầu vào adapter, n là kích thước bottleneck của adapters, và d đại diện cho kích thước đầu vào của adapters.

hình ảnh với sự thay đổi tỷ lệ và kích thước. Những tính chất này đóng góp vào kết quả hứa hẹn trong một số nhiệm vụ thị giác per-pixel, bao gồm phân đoạn ngữ nghĩa [13,14,26], ước lượng độ sâu [27], và phát hiện saliency [28]. Như được thể hiện trong Hình 1a, một Hierarchy Vision Transformer (HVT) bao gồm một số transformer layers, và mỗi transformer layer chủ yếu được tạo thành từ một attention layer và một MLP layer. Khác với các transformers khác (ví dụ: ViT [22]), một đặc điểm riêng biệt của HVTs là các feature maps hình kim tự tháp được tạo từ các transformer blocks khác nhau như được thể hiện trong Hình 1a.

Adapters. Một số công trình thích ứng hiệu quả tham số [11,15,12,21] xây dựng dựa trên Adapter [11], đây là một module giống như bottleneck được đặt trong các transformer layers như được thể hiện trong Hình 1b. Các layers này là các tham số có thể học, trong khi phần còn lại của mô hình bị đóng băng trong quá trình tinh chỉnh. Adapter fa() bao gồm một down-projection layer Wdown∈R^(d×n), một hàm phi tuyến (·), một up-projection layer Wup∈R^(n×d), và một skip connection từ đầu vào của adapter hin∈R^d.

hout = fa(hin; W) = σ(hinWdown)Wup + hin; (1)

trong đó hout∈R^d là đầu ra của adapter và W = [Wdown; W^T_up]∈R^(d×2n) đại diện cho tất cả các tham số có thể học trong adapter.

4 Phương pháp
Thiết lập Bài toán. Cho một Hierarchical Vision Transformer được đào tạo trước trên các tập dữ liệu hình ảnh quy mô lớn (ví dụ: ImageNet [29]), mục tiêu của chúng tôi là đào tạo một số lượng nhỏ tham số và thích ứng mô hình với thiết lập đa nhiệm vụ, trong đó dữ liệu đào tạo của N nhiệm vụ được thu thập trong giai đoạn đào tạo. Theo các công trình hiện tại trong NLP, tiêu chí của học đa nhiệm vụ hiệu quả tham số bao gồm độ chính xác của các nhiệm vụ hạ nguồn và số lượng tham số đào tạo.

Tổng quan Phương pháp. Chúng tôi nhằm cải thiện hiệu quả tham số trong hai khía cạnh: (1) chia sẻ hiệu quả thông tin đồng nhất qua các nhiệm vụ thông qua các hyper-networks nhẹ (Phần 4.1) và (2) mở rộng hiệu quả trọng số adapter trong các transformer blocks khác nhau của Hierarchical Vision Transformers (Phần 4.2). Hai thành phần này được kết hợp để cải thiện sự cân bằng giữa độ chính xác và tham số đào tạo trong các nhiệm vụ thị giác per-pixel đa nhiệm vụ (Phần 4.3).

4.1 Polyhistor: Decomposed Lightweight Hyper-networks cho Multi-task Adaptation
Với mục tiêu thích ứng đồng thời nhiều nhiệm vụ NLP theo cách hiệu quả tham số, một công trình trước đây, Hyperformer [16], xây dựng dựa trên một nhóm adapters trong các nhiệm vụ khác nhau và trích xuất thông tin chia sẻ nhiệm vụ thông qua một hyper-network được chia sẻ qua các nhiệm vụ khác nhau. Cụ thể, một nhóm adapters theo nhiệm vụ và layer với các tham số trọng số {W^t_l | t = 1, ..., N; l = 1, ..., L} được chèn riêng lẻ vào mỗi layer l của mô hình với L layers cho tất cả N nhiệm vụ. Sau đó, thay vì học riêng lẻ trọng số của các adapters này thông qua backpropagation, Hyperformer xây dựng một hyper-network theo layer Ŵ^l, nhận đầu vào là một task embedding có thể học V^t và tạo ra trọng số của adapters W^t_l.

W^t_l = ψ(V^t Ŵ^l) ∈ R^(d×2n);
V^t ∈ R^k; Ŵ^l ∈ R^(k×2dn); ψ(·) : R^(2dn) → R^(d×2n) (2)

trong đó ψ(·) ánh xạ một vector có kích thước 2dn thành một ma trận có kích thước d×2n.

Trong khi Hyperformer đã cho thấy kết quả hứa hẹn trên các benchmarks NLP đa nhiệm vụ, hiệu quả của nó trên các nhiệm vụ thị giác chưa rõ ràng. Ngoài ra, vì hyper-network tạo ra vectorization của một ma trận trọng số adapter (tức là W^t_l ∈ R^(d×2n)), chiều đầu ra của hyper-network là bậc O(dn) và kích thước của hyper-network trở thành bậc O(dnk), trong đó d và n là chiều của đầu vào và bottleneck vectors và k là kích thước của task embeddings. Đối với các nhiệm vụ thị giác dày đặc, kích thước của các input vectors thường lớn (ví dụ: 1024 trong SwinTransformer-Base). Khi chiều bottleneck được đặt tỷ lệ thuận với chiều đầu vào (ví dụ: n = d/λ, trong đó λ > 1 là một hằng số), kích thước của hyper-network sau đó tăng bậc hai theo các input vectors O(kd²).

Để giảm bớt vấn đề này, chúng tôi đề xuất phân tách một hyper-network đơn lẻ Ŵ^l thành một cặp hyper-networks nhẹ {Ŵ^p_l, Ŵ^q_l}, mỗi cái chỉ tạo ra một ma trận hạng thấp. Chúng tôi sau đó nhân các ma trận để thu được trọng số adapter như được thể hiện ở phần trên của Hình 2a.

W^t_l = Σ^r_(i=1) p_i q^T_i = ψ_p(V^t Ŵ^p_l) ψ_q(V^t Ŵ^q_l)^T;
ψ_p(V^t Ŵ^p_l) ∈ R^(d×r); ψ_q(V^t Ŵ^q_l) ∈ R^(2n×r);
V^t ∈ R^k; Ŵ^p_l ∈ R^(k×dr); Ŵ^q_l ∈ R^(k×2nr);
ψ_p(·) : R^(dr) → R^(d×r); ψ_q(·) : R^(2nr) → R^(2n×r); (3)

trong đó ψ_p(·) và ψ_q(·) là các hàm reshape ma trận và r là một hạng ma trận là siêu tham số được điều chỉnh theo ngân sách tính toán. Lưu ý rằng hạng ma trận r thường nhỏ hơn nhiều so với các chiều của adapter n hoặc d (tức là r << n < d).

Theo cách này, với việc phân tách và xấp xỉ hạng thấp, một hyper-network nặng Ŵ^l ∈ R^(k×2dn) có thể được giảm xuống hai hyper-networks nhẹ {Ŵ^p_l ∈ R^(k×dr), Ŵ^q_l ∈ R^(k×2nr)}, và số lượng tham số có thể đào tạo có thể được giảm từ 2kdn xuống k(d + 2n). Số lượng tham số có thể đào tạo trong hyper-networks được giảm từ tăng bậc hai xuống tuyến tính theo kích thước đầu vào (tức là O(kd²) → O(kd)). Chúng tôi sẽ thảo luận cách điều này chuyển thành việc sử dụng thực tế và chứng minh rằng phương pháp của chúng tôi có thể tiết kiệm đáng kể số lượng tham số đào tạo trong khi đạt hiệu suất cạnh tranh so với Hyperformer vanilla trong Phần 5.

4.2 Layer-Wise Scaling Kernels cho Hierarchical Vision Transformers
Mặc dù số lượng tham số có thể học được giảm bằng cách chia sẻ hai hyper-networks nhẹ qua các nhiệm vụ, mỗi layer của transformer vẫn yêu cầu một cặp hyper-networks theo layer. Để ngăn số lượng tham số tăng tuyến tính theo số lượng layers trong transformer, người ta có thể chia sẻ hyper-network qua không chỉ các nhiệm vụ khác nhau mà cả các layers khác nhau (tương tự như Hyperformer++ [16] được giới thiệu gần đây trong NLP). Tuy nhiên, vì adapters trong các blocks khác nhau của Hierarchical Vision Transformers có các chiều khác nhau, tính chất như vậy hạn chế chúng ta khỏi việc sử dụng cùng một cặp hyper-networks để tạo ra trọng số của adapters trong các transformer layers khác nhau.

Để khắc phục vấn đề này, chúng tôi giới thiệu Layer-wise Scaling Kernels để cho phép chia sẻ hyper-network qua các layers. Cụ thể hơn, như được thể hiện trong Hình 1, Hierarchical Vision Transformers có bốn transformer blocks, và mỗi transformer block (được lập chỉ mục bởi b) có nhiều transformer layers với block-wise scale s_b = 2^(b-1) và kích thước đầu ra (H/(4s_b), W/(4s_b), s_b C). Tuy nhiên, việc tăng kích thước channel trong transformer blocks gây ra hai vấn đề. Đầu tiên, như được thể hiện trong Hình 1, kích thước của trọng số adapter tăng bậc hai theo kích thước channel/kích thước đầu vào. Kết quả là, các adapters trong các blocks sau sẽ yêu cầu nhiều tham số có thể đào tạo hơn so với các blocks trước. Thứ hai, như đã đề cập ở trên, do các kích thước khác nhau của adapters trong các transformer layers khác nhau, một hyper-network đơn lẻ không thể tạo ra nhiều adapters trong các layers khác nhau và do đó không thể được chia sẻ trực tiếp qua các layers.

--- TRANG 4 ---
(a) (b)
Hình 2: Minh họa về Polyhistor và Polyhistor-Lite của chúng tôi. (a) Chúng tôi đề xuất Polyhistor, áp dụng Decomposed HyperNetworks để giảm số lượng tham số đào tạo trong thích ứng đa nhiệm vụ (Phần 4.1). Chúng tôi cũng giới thiệu Layer-wise Scaling Kernels để mở rộng hiệu quả Template Kernels cho các tỷ lệ khác nhau của adapters (Phần 4.2). (b) Bằng cách kết hợp Decomposed HyperNetworks và Layer-wise Scaling Kernels, Polyhistor-Lite của chúng tôi có thể giải quyết hiệu quả thích ứng đa nhiệm vụ trong các nhiệm vụ thị giác per-pixel (Phần 4.3).

Như được thể hiện ở phần dưới của Hình 2a, để giải quyết hai vấn đề này, chúng tôi đề xuất phân tích ma trận trọng số của adapter W_l của layer l thành một tập Template Kernels W̃^i_l và Scaling Kernels α^i_l. Để mở rộng hiệu quả trọng số adapter trong các transformer layers khác nhau, chúng tôi sử dụng phép nhân Hypercomplex [17] và dùng Kronecker Product để tích hợp các loại ma trận này.

W_l = Σ^{s(l)}_{i=1} W̃^i_l ⊗ α^i_l;
W̃^i_l ∈ R^{d×2n}; α^i_l ∈ R^{s(l)×s(l)}; W_l ∈ R^{ds(l)×2ns(l)}; (4)

trong đó s(l) và β(l) là tỷ lệ và chỉ số của transformer block nơi transformer layer l được đặt, và ⊗ là phép toán ma trận Kronecker Product. Kích thước của Template Kernels giống nhau qua các layers, trong khi kích thước của Scaling Kernels phụ thuộc vào block-wise scale s(l).

Nói cách khác, mục đích của Scaling Kernels là mở rộng Template Kernels và làm cho chúng phù hợp với các adapters trong các layers với các tỷ lệ khác nhau. Việc phân tách này không chỉ giảm việc sử dụng tham số trong một adapter đơn lẻ mà còn cho thấy tiềm năng giảm tham số với hyper-network được chia sẻ.

4.3 Polyhistor-Lite: Lightweight Hypernetworks cho Hierarchical Vision Transformers
Chúng tôi đã mô tả cách giảm tham số đào tạo cho thích ứng đa nhiệm vụ trong Phần 4.1 và Hierarchical Vision Transformers trong Phần 4.2. Để thực hiện thích ứng hiệu quả tham số cho các nhiệm vụ thị giác per-pixel đa nhiệm vụ, chúng tôi tích hợp hai thành phần này để có được framework cuối cùng của chúng tôi.

Cụ thể, như được thể hiện trong Hình 2b, chúng tôi sử dụng một cặp hyper-networks đơn lẻ {Ŵ^p, Ŵ^q} được chia sẻ qua các layers và nhiệm vụ khác nhau. Đối với nhiệm vụ t, các hyper-networks nhận đầu vào là một tập layer embeddings có thể đào tạo {Ṽ^i_l}^{s(l)}_{i=1} và task embedding Ṽ^t và tạo ra hai ma trận hạng thấp, được nhân và tạo ra một tập Template Kernels của adapters trong các layers và nhiệm vụ khác nhau.

W̃^{t,i}_l = ψ_p([Ṽ^t; Ṽ^i_l] Ŵ^p) ψ_q([Ṽ^t; Ṽ^i_l] Ŵ^q)^T; ∀i = 1, ..., β(l)
ψ_p([Ṽ^t; Ṽ^i_l] Ŵ^p) ∈ R^{d×r}; ψ_q([Ṽ^t; Ṽ^i_l] Ŵ^q) ∈ R^{2n×r};
Ṽ^t, Ṽ^i_l ∈ R^{k/2}; Ŵ^p ∈ R^{k×dr}; Ŵ^q ∈ R^{k×2nr};
ψ_p(·) : R^{dr} → R^{d×r}; ψ_q(·) : R^{2nr} → R^{2n×r}; (5)

Để tạo ra tham số của adapter trong mỗi layer, chúng tôi học một tập Scaling Kernels khác và kết hợp chúng với Template Kernels thông qua Kronecker Product.

W^t_{l,β} = Σ^{s(l)}_{i=1} W̃^{t,i}_l ⊗ α^{t,i}_l; ∀t = 1, ..., T
W̃^{t,i}_l ∈ R^{d×2n}; α^{t,i}_l ∈ R^{s(l)×s(l)}; W^t_l ∈ R^{ds(l)×2ns(l)}; (6)

Với việc tích hợp Lightweight HyperNetworks và Layer-wise Scaling Kernels, framework của chúng tôi có thể giảm hiệu quả các tham số có thể đào tạo trong thích ứng đa nhiệm vụ cho các nhiệm vụ thị giác dày đặc. Chúng tôi cung cấp hai biến thể của phương pháp của chúng tôi. Polyhistor chỉ sử dụng Decomposed HyperNetworks, và Polyhistor-Lite kết hợp cả Decomposed HyperNetworks và Layer-Wise Scaling Kernels.

5 Thí nghiệm
5.1 Chi tiết Triển khai
Tập dữ liệu. Chúng tôi theo các công trình trước đây [30,31] về học đa nhiệm vụ cho các nhiệm vụ dự đoán dày đặc và xem xét PASCAL-Context [32] để xây dựng benchmark thích ứng hiệu quả đa nhiệm vụ per-pixel của chúng tôi. Chúng tôi đánh giá tất cả các phương pháp trên bốn nhiệm vụ per-pixel, phân đoạn ngữ nghĩa 21 lớp, phân đoạn bộ phận con người 7 lớp, ước lượng pháp tuyến bề mặt, và phát hiện saliency. Các metric đánh giá của chúng tôi bao gồm mean intersection-over-union (mIoU) cho phân đoạn ngữ nghĩa, phân đoạn bộ phận con người, và phát hiện saliency và mean error (mErr) cho ước lượng pháp tuyến bề mặt.

Kiến trúc Mô hình. Đối với encoder, chúng tôi sử dụng Swin-Transformer [13] do hiệu suất mạnh mẽ của nó trong các nhiệm vụ thị giác khác nhau và tính phổ biến trong cộng đồng thị giác. Các decoder của chúng tôi cho các nhiệm vụ dày đặc khác nhau dựa trên All-MLP decoder của Segformer [14], sử dụng các linear layers đơn giản và bilinear upsampling layer để thực hiện hiệu quả các nhiệm vụ thị giác dày đặc, và chúng tôi thích ứng số lượng chiều đầu ra cho các nhiệm vụ khác nhau.

Đào tạo. Để đào tạo mô hình của chúng tôi, chúng tôi sử dụng các loss thông dụng cho mỗi nhiệm vụ. Cụ thể, chúng tôi sử dụng per-pixel cross-entropy chuẩn cho phân đoạn ngữ nghĩa và phân đoạn bộ phận con người, L1 loss cho ước lượng pháp tuyến bề mặt, và balanced cross-entropy cho phát hiện saliency. Để so sánh công bằng, chúng tôi thí nghiệm trên triển khai codebase thống nhất với cùng hàm loss và iterations đào tạo cho tất cả baselines và phương pháp của chúng tôi.

5.2 Baselines
Single-task full fine-tuning sử dụng một mô hình được đào tạo trước riêng biệt cho mỗi nhiệm vụ, và Fine-tuning decoders đóng băng feature backbone và chỉ tinh chỉnh các decoder theo nhiệm vụ cho các nhiệm vụ khác nhau.

Đối với các phương pháp thích ứng nhiệm vụ đơn lẻ (Bitfit, VPT, PHM-Layer, Compacter, Compacter++, Adapter, Low-rank Adapter, và LoRA), chúng tôi đặt các module theo nhiệm vụ cho mỗi nhiệm vụ.

Bitfit [7] điều chỉnh bias trong tất cả các layers, và, cụ thể cho Swin-Transformer, chúng tôi cũng điều chỉnh bias trong patch merging layers và patch projection layers.

VPT [9] chèn các embedding có thể điều chỉnh trong input layer đầu tiên (VPT-shallow) và tất cả các layers (VPT-deep), và chúng tôi chọn siêu tham số tốt nhất (tức là 50 embeddings mỗi layer) cho tất cả kết quả.

PHM layer [12] chia sẻ một ma trận chậm cho tất cả các layers và học một ma trận nhanh cho mỗi layer và đặt các module sau attention và MLP layers, Compacter [12] tiếp tục phân tách ma trận nhanh thành hai vector hạng thấp, và Compacter++ [12] chỉ đặt modules sau MLP layers.

--- TRANG 5 ---
12
10
8
6
4
2
0
-2
Cải thiện Tương đối so với Single-Task Fine-Tuning (%)
0 100 101 102
Tham số Có thể Đào tạo (M)

Single-task Full Fine-tuning
Train decoders
Multi-task Full Fine-tuning
Bitfit
Relative Bias
LoRA
VPT-shallow
VPT-deep
Low-rank adapters
PHM layer
Compacter
Adapter
Shared adapter
Hyperformer
Polyhistor (Của chúng tôi)
Polyhistor-Lite (Của chúng tôi)

6
4
2
0
-2
Cải thiện Tương đối so với Single-Task Fine-Tuning (%)
0 100 101
Tham số Có thể Đào tạo (M)

Compacter++
Compacter
Adapter
Shared Adapter
Hyperformer
Polyhistor (Của chúng tôi)
Polyhistor-Lite (Của chúng tôi)

(a) (b)

Hình 3: (a) Polyhistor của chúng tôi sử dụng ít hơn một phần mười so với phương pháp thích ứng đa nhiệm vụ tiên tiến nhất (tức là Hyperformer [16]) về tham số có thể đào tạo trong encoder. (b) Việc điều chỉnh siêu tham số trên các phương pháp baseline dẫn đến cải thiện hạn chế, và chúng tôi đạt được sự cân bằng tốt nhất giữa các tham số có thể đào tạo và độ chính xác trên các nhiệm vụ hạ nguồn. Chi tiết được liệt kê trong Phụ lục.

LoRA [10] áp dụng việc phân tách hạng thấp trên các attention layers, và chúng tôi chọn hạng r = 4 và tỷ lệ đầu ra adapter (tức là 4), có hiệu suất tốt nhất.

Adapter [11,21] đặt các module giống bottleneck theo nhiệm vụ vào transformer layers, và Shared-Adapter [20] chia sẻ adapter qua các nhiệm vụ khác nhau.

Hyperformer [16] áp dụng hyper-network và tạo ra trọng số cho adapter, và chúng tôi trình bày kết quả với các chiều bottleneck adapter khác nhau. Vì adapters trong các layers khác nhau có các chiều khác nhau, Hyperformer++ không thể được thích ứng đơn giản với Hierarchical Vision Transformers.

5.3 Kết quả thí nghiệm về Multi-Task Adaptation
Chúng tôi đánh giá tất cả các phương pháp bằng cách tính toán cải thiện tương đối so với Single-Task Full Fine-tuning và lấy trung bình qua bốn nhiệm vụ. Vì tất cả các phương pháp sử dụng và đào tạo tất cả tham số của các decoder theo nhiệm vụ, chúng tôi cung cấp hai giá trị của tham số có thể đào tạo, một cho encoder và một cho toàn bộ mô hình.

Như được trình bày trong Hình 3 và Bảng 1, trong số tất cả các phương pháp thí nghiệm, Hyperformer hoạt động tốt nhất và đạt +2.64% trung bình cho bốn nhiệm vụ hạ nguồn, nhưng nó yêu cầu 72M tham số có thể đào tạo trong encoder. Mặt khác, Polyhistor của chúng tôi đạt kết quả cạnh tranh (+2.34%), trong khi chúng tôi chỉ cần 6.41M tham số có thể đào tạo trong encoder, ít hơn một phần mười so với Hyperformer. Polyhistor-Lite của chúng tôi có thể giảm thêm các tham số có thể đào tạo xuống 0.41M bằng cách tích hợp Layer-wise Scaling Kernels và chia sẻ hypernetwork qua các layers, và nó đạt +1.74% và cao hơn tất cả các phương pháp khác sử dụng lượng tham số có thể đào tạo tương tự (ví dụ: BitFit, VPT, Shared Adapter, PHM layer, Compacter, LoRA, và Low-rank Adapter).

Chúng tôi cũng thấy rằng, trong khi phương pháp thích ứng hiệu quả tham số thị giác trước đây, VPT, trình bày kết quả hứa hẹn trên phân loại hình ảnh nhiệm vụ đơn lẻ [9], nó không cho thấy cải thiện đáng kể so với baseline chỉ tinh chỉnh decoders trong benchmark thị giác dày đặc đa nhiệm vụ của chúng tôi. Một lý do tiềm năng là, so với các benchmark phân loại hình ảnh tập trung nhiều hơn vào cùng nhiệm vụ với input shifts, benchmark của chúng tôi tập trung nhiều hơn vào các đầu ra nhiệm vụ khác nhau. Điều này làm cho VPT, thêm các tham số có thể học trong không gian đầu vào, không thể giải quyết sự khác biệt trong không gian đầu ra và thích ứng với các nhiệm vụ mới.

Trong khi Hyperformer đạt cải thiện hiệu suất tốt nhất so với Single-Task Full Fine-tuning, nó yêu cầu một số lượng tham số lớn hơn so với các phương pháp khác. Điều này dẫn đến câu hỏi tự nhiên: Việc giảm tham số trong Hyperformer có tạo ra sự cân bằng tốt hơn giữa số lượng tham số có thể điều chỉnh và cải thiện hiệu suất không? Do đó, chúng tôi giảm kích thước của nó và kiểm tra xem nó có thể duy trì hiệu suất với ít tham số có thể đào tạo hơn không. Chúng tôi cũng đã làm thí nghiệm tương tự trên các phương pháp hiện tại khác, (ví dụ: Adapter, Shared Adapter, Compacter, và Compacter++), trong đó chúng tôi tăng số lượng tham số điều chỉnh bằng cách tăng chiều của vector ẩn trong các adapter modules (tăng tỷ lệ down-projection). Như được thể hiện trong Hình 3b, chúng tôi thấy rằng việc chỉ điều chỉnh siêu tham số trong các phương pháp baseline không thể có được sự cân bằng tốt hơn giữa số lượng tham số có thể đào tạo và cải thiện hiệu suất. Ví dụ, khi số lượng tham số điều chỉnh trong encoder của Hyperformer được giảm xuống 20.15M, cải thiện tương đối trung bình của nó

--- TRANG 6 ---
Bảng 1: Kết quả thí nghiệm về Multi-Task Adaptation. Chúng tôi sử dụng SwinTransformer-Tiny làm feature backbone. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn.

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder/Tất cả | Seg. ↑ | H.Part ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 110.07 / 112.62 | 67.21 | 61.93 | 62.35 | 17.97 | 0.00 %
Fine-tuning Decoders | 0.00 / 2.55 | 63.14 | 52.37 | 58.39 | 20.89 | -11.02 %
Multi-task Full Fine-tuning | 27.51 / 30.06 | 68.71 | 62.13 | 64.18 | 17.35 | 2.23 %
Bitfit [7] | 0.30 / 2.85 | 68.57 | 55.99 | 60.64 | 19.42 | -4.60 %
Relative bias [13] | 0.09 / 2.64 | 63.51 | 52.35 | 57.74 | 21.07 | -11.40 %
VPT-shallow [9] | 0.02 / 2.57 | 62.96 | 52.27 | 58.31 | 20.90 | -11.18 %
VPT-deep [9] | 0.88 / 3.43 | 64.35 | 52.54 | 58.15 | 21.07 | -10.85 %
PHM layer [12] | 0.59 / 3.14 | 68.55 | 56.28 | 60.35 | 19.23 | -4.34 %
Compacter [12] | 0.23 / 2.78 | 68.08 | 56.41 | 60.08 | 19.22 | -4.55 %
Compacter++ [12] | 0.11 / 2.66 | 67.26 | 55.69 | 59.47 | 19.54 | -5.84 %
LoRA [10] | 0.32 / 2.87 | 70.12 | 57.73 | 61.90 | 18.96 | -2.17 %
Adapter [21] | 8.69 / 11.24 | 69.21 | 57.38 | 61.28 | 18.83 | -2.71 %
Low-rank adapter | 0.34 / 2.89 | 68.31 | 56.53 | 60.29 | 19.36 | -4.54 %
Shared Adapter [20] | 2.20 / 4.74 | 70.21 | 59.15 | 62.29 | 19.26 | -1.83 %
Hyperformer [16] | 72.77 /75.32 | 71.43 | 60.73 | 65.54 | 17.77 | 2.64 %
Polyhistor (Của chúng tôi) | 6.41 / 8.96 | 70.87 | 59.54 | 65.47 | 17.47 | 2.34 %
Polyhistor-Lite (Của chúng tôi) | 0.41 / 2.96 | 70.24 | 59.12 | 64.75 | 17.40 | 1.74 %

(a) (b)

Hình 4: Việc sử dụng feature backbone với (a) kích thước lớn hơn hoặc (b) nhiều dữ liệu đào tạo trước hơn dẫn đến cải thiện lớn hơn so với Single-task Full Fine-tuning. Tất cả kết quả được tạo ra bởi Polyhistor-Lite của chúng tôi. Để so sánh công bằng, cả hai backbone được thể hiện trong (a) đều được đào tạo trước trên ImageNet-1k, và cả hai backbone được thể hiện trong (b) đều dựa trên SwinTransformer-Base.

thu hẹp xuống chỉ còn -0.14%. Những thí nghiệm này cho thấy rằng các phương pháp của chúng tôi đạt được cải thiện hiệu suất tốt hơn với ít tham số hơn so với các phương pháp baseline.

5.4 Nghiên cứu ablation và phân tích
Các feature backbone khác nhau. Để xác minh rằng phương pháp đề xuất của chúng tôi cũng có thể áp dụng cho kiến trúc mô hình kích thước lớn hơn, chúng tôi cũng thí nghiệm SwinTransformer-Base. Để so sánh công bằng, chúng tôi sử dụng cùng tập dữ liệu đào tạo trước, ImageNet-1k [29], trên cả SwinTransformer-Tiny và SwinTransformer-Base. Như được thể hiện trong Hình 4a, chúng tôi thấy rằng phương pháp của chúng tôi đạt được cải thiện lớn hơn so với Single-task Full Fine-tuning khi sử dụng feature backbone lớn hơn, và điều này cho thấy tiềm năng đạt được nhiều cải thiện hơn khi áp dụng phương pháp của chúng tôi cho kiến trúc mô hình lớn hơn.

Dữ liệu đào tạo trước khác nhau. Chúng tôi cũng kiểm tra cách phương pháp đề xuất của chúng tôi hoạt động khi sử dụng dữ liệu đào tạo trước khác nhau. Cụ thể, chúng tôi áp dụng phương pháp của chúng tôi lên SwinTransformer-Base được đào tạo trước với ImageNet-1k và ImageNet-22k. Như được chứng minh trong Hình 4b, chúng tôi thấy rằng phương pháp của chúng tôi có được cải thiện hiệu suất lớn hơn khi sử dụng nhiều dữ liệu đào tạo trước hơn, và điều này cho thấy tiềm năng đạt được nhiều cải thiện hơn bằng cách sử dụng nhiều dữ liệu đào tạo trước hơn.

Thay đổi tham số có thể đào tạo với dữ liệu đào tạo trước khác nhau. Trong Hình 4a, chúng tôi đã cho thấy rằng việc sử dụng mô hình với nhiều dữ liệu đào tạo trước hơn (tức là ImageNet-22k) có thể dẫn đến cải thiện hiệu suất cao hơn so với mô hình với ít dữ liệu đào tạo trước hơn (tức là ImageNet-1k). Để điều tra hiện tượng này,

--- TRANG 7 ---
4 5 6 7 8
Cải thiện tương đối so với Single-task Full Fine-tuning (%)
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Tham số Có thể Đào tạo (M)

Polyhistor-Lite (1k)
Polyhistor-Lite (22k)

Hình 5: Polyhistor-Lite của chúng tôi sử dụng nhiều dữ liệu đào tạo trước hơn có thể dẫn đến cải thiện tương đối cao hơn so với baseline Single-task Fine-tuning, và xu hướng này nhất quán qua các lượng tham số có thể đào tạo khác nhau. Lưu ý rằng các baseline Single-task Fine-tuning cho Polyhistor-Lite (1k) và Polyhistor-Lite(22k) được đào tạo trên các mô hình ImageNet-1k và ImageNet-22k tương ứng.

chúng tôi thay đổi kích thước của Polyhistor-Lite và đo lường cải thiện hiệu suất của các mô hình với dữ liệu đào tạo trước khác nhau.

Như được thể hiện trong Hình 5, chúng tôi thấy rằng mô hình được đào tạo trước trên ImageNet-22k có thể sử dụng ít tham số có thể đào tạo hơn để đạt được cải thiện hiệu suất tương tự so với mô hình được đào tạo trước trên ImageNet-1k. Ngoài ra, dưới cùng lượng tham số có thể đào tạo, mô hình được đào tạo trước trên ImageNet-22k có thể vượt trội nhất quán so với mô hình được đào tạo trước trên ImageNet-1k. Điều này cho thấy rằng, với nhiều dữ liệu đào tạo trước hơn, các feature extractors có thể học các biểu diễn đa dạng hơn, để chúng tôi có thể sử dụng ít tham số có thể đào tạo hơn và thích ứng tốt hơn với các nhiệm vụ hạ nguồn khác nhau.

Chiều của task embeddings. Ngoài ra, chúng tôi cũng tiến hành phân tích để kiểm tra cách phương pháp của chúng tôi hoạt động với các kích thước khác nhau của task embeddings. Như được trình bày trong Bảng 2, khi sử dụng kích thước lớn của task embeddings, cải thiện trung bình so với single-task fine-tuning trở nên lớn hơn.

Bảng 2: Nghiên cứu ablation về kích thước của task embeddings. Chúng tôi thay đổi kích thước của task embeddings k từ 16 đến 64 trên Polyhistor-Lite của chúng tôi. Tất cả kết quả trong bảng này dựa trên SwinTransformer-Tiny được đào tạo trước trên ImageNet-1k.

Kích thước | Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Task Embeddings k | Encoder / Tất cả | Seg. ↑ | H.Seg. ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | - | 110.07 / 112.62 | 67.21 | 61.93 | 62.35 | 17.97 | 0.00 %
Polyhistor-Lite 16 | 0.23 / 2.78 | 69.67 | 58.38 | 63.55 | 18.05 | -0.15 %
Polyhistor-Lite 32 | 0.29 / 2.84 | 69.80 | 58.72 | 64.14 | 17.73 | 0.72 %
Polyhistor-Lite 64 | 0.41 / 2.96 | 70.24 | 59.12 | 64.75 | 17.40 | 1.74 %

Chúng tôi trình bày thêm các nghiên cứu ablation, phân tích, và chi tiết triển khai trong Phụ lục.

6 Kết luận
Chúng tôi đã đề xuất Polyhistor và Polyhistor-Lite — các phương pháp tinh chỉnh hiệu quả tham số cho các nhiệm vụ thị giác dày đặc. Chúng tôi đã cho thấy rằng hầu hết các phương pháp thích ứng nhiệm vụ đơn lẻ hiệu quả tham số hiện tại đạt hiệu suất thấp hơn so với Single-task Full Fine-tuning, và phương pháp thích ứng đa nhiệm vụ tiên tiến nhất đạt kết quả thuận lợi trong khi sử dụng số lượng lớn tham số có thể điều chỉnh. So với các phương pháp hiện tại này, các phương pháp đề xuất của chúng tôi không chỉ đạt được cải thiện hiệu suất cạnh tranh so với tiên tiến nhất mà còn chỉ sử dụng lượng tham số có thể điều chỉnh rất hạn chế. Hạn chế tiềm năng của phương pháp chúng tôi là tìm kiếm các siêu tham số phù hợp, đây là hạn chế chung trong tất cả các phương pháp học hiệu quả tham số.

Lời cảm ơn. Yen-Cheng Liu và Zsolt Kira được hỗ trợ một phần bởi chương trình Learning with Less Labels (LwLL) của DARPA theo thỏa thuận HR0011-18-S-0044, như một phần của sự liên kết của họ với Georgia Tech.

--- TRANG 8 ---
Tài liệu tham khảo
[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. Về những cơ hội và rủi ro của các mô hình nền tảng. arXiv preprint arXiv:2108.07258, 2021.

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Các mô hình ngôn ngữ là người học few-shot. Advances in neural information processing systems, 33:1877–1901, 2020.

[3] Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun, Armand Joulin, và Piotr Bojanowski. Các mô hình thị giác mạnh mẽ và công bằng hơn khi được đào tạo trước trên các hình ảnh không được tuyển chọn mà không có sự giám sát. arXiv preprint arXiv:2202.08360, 2022.

[4] Anuroop Sriram, Abhishek Das, Brandon M Wood, Siddharth Goyal, và C Lawrence Zitnick. Hướng tới đào tạo các mạng thần kinh đồ thị tỷ tham số cho các mô phỏng nguyên tử. arXiv preprint arXiv:2203.09697, 2022.

[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311, 2022.

[6] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Mở rộng lên các mô hình nghìn tỷ tham số với sự thưa thớt đơn giản và hiệu quả. arXiv preprint arXiv:2101.03961, 2021.

[7] Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Tinh chỉnh hiệu quả tham số đơn giản cho các mô hình ngôn ngữ được che dựa trên transformer. Trong Kỷ yếu Hội nghị thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (ACL), 2022.

[8] Demi Guo, Alexander Rush, và Yoon Kim. Học chuyển giao hiệu quả tham số với diff pruning. Trong Kỷ yếu Hội nghị thường niên lần thứ 59 của Hiệp hội Ngôn ngữ học Tính toán và Hội nghị Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên (Tập 1: Bài báo dài), 2021.

[9] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.

[10] Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, và Weizhu Chen. Lora: Thích ứng hạng thấp của các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế về Biểu diễn Học tập, 2022.

[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Học chuyển giao hiệu quả tham số cho nlp. Trong Kỷ yếu Hội nghị Quốc tế về Học máy (ICML), 2019.

[12] Rabeeh Karimi mahabadi, James Henderson, và Sebastian Ruder. Compacter: Các adapter layers hypercomplex hạng thấp hiệu quả. Trong Advances in Neural Information Processing Systems (NeurIPS), 2021.

[13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer sử dụng shifted windows. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[14] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, và Ping Luo. Segformer: Thiết kế đơn giản và hiệu quả cho phân đoạn ngữ nghĩa với transformers. Trong Advances in Neural Information Processing Systems (NeurIPS), 2021.

[15] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, và Iryna Gurevych. Adapterhub: Một framework để thích ứng transformers. Trong Kỷ yếu Hội nghị 2020 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên: Trình diễn Hệ thống, 2020.

[16] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, và James Henderson. Tinh chỉnh đa nhiệm vụ hiệu quả tham số cho transformers thông qua các hypernetworks được chia sẻ. Trong Hội nghị thường niên của Hiệp hội Ngôn ngữ học Tính toán, 2021.

[17] Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, và Jie Fu. Vượt ra ngoài các fully-connected layers với quaternions: Tham số hóa phép nhân hypercomplex với 1/n tham số. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR), 2021.

--- TRANG 9 ---
[18] Brian Lester, Rami Al-Rfou, và Noah Constant. Sức mạnh của quy mô cho prompt tuning hiệu quả tham số. Trong Kỷ yếu Hội nghị 2021 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, trang 3045–3059, 2021.

[19] Xiang Lisa Li và Percy Liang. Prefix-tuning: Tối ưu hóa các prompts liên tục cho việc sinh. Trong Kỷ yếu Hội nghị thường niên lần thứ 59 của Hiệp hội Ngôn ngữ học Tính toán và Hội nghị Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên (Tập 1: Bài báo dài), 2021.

[20] Mohit Bansal Yi-Lin Sung, Jaemin Cho. Vl-adapter: Học chuyển giao hiệu quả tham số cho các nhiệm vụ thị giác-và-ngôn ngữ. Trong Kỷ yếu Hội nghị IEEE về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 2022.

[21] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. Hướng tới cái nhìn thống nhất về học chuyển giao hiệu quả tham số. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR), 2022.

[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. arXiv preprint arXiv:2010.11929, 2020.

[23] Weijian Xu, Yifan Xu, Tyler Chang, và Zhuowen Tu. Co-scale conv-attentional image transformers. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[24] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, và Lei Zhang. Cvt: Giới thiệu convolutions vào vision transformers. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[25] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, và Matthijs Douze. Levit: một vision transformer trong trang phục convnet cho suy luận nhanh hơn. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[26] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, và Ling Shao. Pyramid vision transformer: Một backbone linh hoạt cho dự đoán dày đặc mà không có convolutions. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[27] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, và Ishan Misra. Omnivore: Một mô hình đơn lẻ cho nhiều phương thức thị giác. Trong Kỷ yếu Hội nghị IEEE về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 2021.

[28] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, và Junwei Han. Visual saliency transformer. Trong Kỷ yếu Hội nghị Quốc tế IEEE về Thị giác Máy tính (ICCV), 2021.

[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.

[30] Simon Vandenhende, Stamatios Georgoulis, và Luc Van Gool. Mti-net: Multi-scale task interaction networks cho multi-task learning. Trong Kỷ yếu Hội nghị châu Âu về Thị giác Máy tính (ECCV), 2020.

[31] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai, và L. Van Gool. Multi-task learning cho các nhiệm vụ dự đoán dày đặc: Một khảo sát. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

[32] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, và Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV), 2010.

[33] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, và Han Hu. Self-supervised learning với swin transformers. arXiv preprint arXiv:2105.04553, 2021.

[34] Diederik P Kingma và Jimmy Ba. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. arXiv preprint arXiv:1412.6980, 2014.

--- TRANG 10 ---
Phụ lục
A Feature backbones với dữ liệu đào tạo trước khác nhau.
So sánh với baselines. Chúng tôi cũng thí nghiệm với các phương pháp baseline khác trên SwinTransformer-Base được đào tạo trước trên ImageNet-1k và ImageNet-22k. Như được thể hiện trong Bảng 3 và 4, chúng tôi thấy rằng xu hướng của tất cả các phương pháp tương tự như kết quả trên SwinTransformer-Tiny. Cụ thể, hầu hết các phương pháp baseline có khoảng cách hiệu suất với Hyperformer và Polyhistor-Lite của chúng tôi. So với Hyperformer, Polyhistor-Lite của chúng tôi sử dụng ít hơn 5% tham số có thể đào tạo của chúng trong encoder để đạt kết quả tương tự hoặc thậm chí tốt hơn. Bằng cách điều chỉnh tỷ lệ adapter down-projection và sử dụng ít tham số có thể đào tạo hơn, Polyhistor-Lite của chúng tôi vẫn có được cải thiện hiệu suất cao hơn so với các phương pháp baseline với tham số có thể đào tạo thấp (ví dụ: Compacter++, Bitfit, VPT, PHM layer, LoRA, Adapter). Những kết quả này cho thấy rằng Polyhistor-Lite đề xuất của chúng tôi có thể đạt được sự cân bằng tốt hơn so với tất cả các công trình hiệu quả tham số hiện tại trong các feature backbones được đào tạo trước khác nhau.

B Hạng của đầu ra hyper-network.
Một siêu tham số quan trọng khác trong mô hình của chúng tôi là hạng của đầu ra hyper-network. Do đó chúng tôi thí nghiệm Polyhistor và thay đổi hạng của các ma trận đầu ra, và, như được thể hiện trong Bảng 5, chúng tôi thấy mô hình có thể đạt kết quả tốt hơn với hạng lớn.

C Kết quả thí nghiệm của việc điều chỉnh các phương pháp baseline.
Trong Hình 3b của bài báo chính, chúng tôi đã trình bày kết quả của các phương pháp baseline khác nhau với các siêu tham số khác nhau. Để so sánh rõ ràng hơn giữa các phương pháp baseline, chúng tôi cũng cung cấp các giá trị chính xác của tất cả kết quả trong Bảng 6. Đáng chú ý rằng việc chỉ điều chỉnh các phương pháp baseline chỉ dẫn đến cải thiện hạn chế của các phương pháp baseline, và Polyhistor và Polyhistor-Lite đề xuất của chúng tôi vẫn đạt được sự cân bằng tốt hơn giữa cải thiện hiệu suất và tham số có thể đào tạo.

Bảng 3: Kết quả thí nghiệm về Multi-Task Adaptation. Chúng tôi sử dụng SwinTransformer-Base được đào tạo trước trên ImageNet-1k làm feature backbone. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn. λ biểu thị tỷ lệ down-projection của adapters (tức là tỷ lệ đầu vào adapter d so với các hidden vectors n, λ = d/n).

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder/Tất cả | Seg. ↑ | H.Part ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 346.96 / 350.01 | 67.88 | 64.47 | 61.26 | 18.85 | 0.00 %
Fine-tuning Decoders | 0.00 / 3.04 | 68.98 | 55.57 | 58.37 | 21.36 | -7.55 %
Bitfit [7] | 0.20 / 3.24 | 71.93 | 59.12 | 60.67 | 20.08 | -2.46 %
Relative bias [13] | 0.06 / 3.11 | 68.44 | 55.70 | 57.27 | 21.63 | -8.51 %
VPT-deep [9] | 2.41 / 5.45 | 68.80 | 55.85 | 58.25 | 21.37 | -7.57 %
PHM layer [12] | 1.89 / 4.94 | 71.93 | 59.11 | 59.71 | 20.35 | -3.21 %
Compacter++ [12] | 0.25 / 3.29 | 72.00 | 59.11 | 59.73 | 20.41 | -3.25 %
LoRA [10] | 0.87 / 4.31 | 74.10 | 61.57 | 63.87 | 18.55 | 2.63 %
Adapter [21] | 3.64 / 6.68 | 73.29 | 60.30 | 62.42 | 18.66 | 1.10 %
Low-rank adapter | 0.34 / 2.89 | 72.13 | 59.10 | 59.81 | 20.28 | -3.01 %
Hyperformer [16] | 60.88 /63.92 | 73.60 | 63.82 | 67.31 | 16.90 | 6.91 %
Polyhistor-Lite (Của chúng tôi; λ = 1) | 1.29 / 4.34 | 73.70 | 63.32 | 66.50 | 16.93 | 6.38 %
Polyhistor-Lite (Của chúng tôi; λ = 2) | 0.62 / 3.67 | 73.69 | 63.04 | 66.56 | 17.30 | 5.80 %
Polyhistor-Lite (Của chúng tôi; λ = 4) | 0.39 / 3.43 | 73.57 | 62.04 | 65.84 | 17.70 | 4.55 %
Polyhistor-Lite (Của chúng tôi; λ = 8) | 0.29 / 3.34 | 73.92 | 62.15 | 65.37 | 17.70 | 4.53 %
Polyhistor-Lite (Của chúng tôi; λ = 32) | 0.24 / 3.28 | 73.80 | 61.32 | 64.64 | 17.92 | 3.57 %

D Kết quả thí nghiệm của các hierarchical vision transformers khác.
Như được thể hiện trong Bảng 7, chúng tôi tiếp tục áp dụng phương pháp của chúng tôi và các phương pháp baseline khác lên Pyramid Vision Transformer [26]. Chúng tôi thấy Polyhistor của chúng tôi có thể đạt kết quả tương đương với Hyperformer trong khi sử dụng ít tham số có thể đào tạo hơn nhiều. Polyhistor-Lite có thể giảm thêm tham số có thể đào tạo và đạt độ chính xác cao hơn so với tất cả các phương pháp khác sử dụng lượng tham số có thể đào tạo tương tự (ví dụ:

--- TRANG 11 ---
Bảng 4: Kết quả thí nghiệm về Multi-Task Adaptation. Chúng tôi sử dụng SwinTransformer-Base được đào tạo trước trên ImageNet-22k làm feature backbone. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn. λ biểu thị tỷ lệ down-projection của adapters (tức là tỷ lệ đầu vào adapter d so với các hidden vectors n, λ = d/n).

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder/Tất cả | Seg. ↑ | H.Part ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 346.96 / 350.01 | 70.72 | 67.47 | 61.00 | 18.73 | 0.00 %
Fine-tuning Decoders | 0.00 / 3.04 | 73.33 | 60.56 | 59.13 | 21.38 | -5.94 %
Bitfit [7] | 0.20 / 3.24 | 76.42 | 64.89 | 62.05 | 19.03 | 1.09 %
Relative bias [13] | 0.06 / 3.11 | 72.86 | 60.64 | 58.44 | 21.51 | -6.53 %
VPT-deep [9] | 2.41 / 5.45 | 74.21 | 61.41 | 58.80 | 21.61 | -5.90 %
PHM layer [12] | 1.89 / 4.94 | 76.33 | 64.59 | 60.43 | 20.23 | -1.32 %
Compacter++ [12] | 0.25 / 3.29 | 75.99 | 64.65 | 60.42 | 20.01 | -1.13 %
LoRA [10] | 0.87 / 4.31 | 78.24 | 66.95 | 64.70 | 18.07 | 4.86 %
Adapter [21] | 3.64 / 6.68 | 77.22 | 65.95 | 63.80 | 18.38 | 3.35 %
Low-rank adapter | 0.34 / 2.89 | 75.65 | 64.75 | 60.50 | 20.03 | -1.21 %
Hyperformer [16] | 60.88 /63.92 | 78.41 | 68.94 | 67.50 | 16.80 | 6.91 %
Polyhistor-Lite (Của chúng tôi; λ = 1) | 1.29 / 4.34 | 77.91 | 68.02 | 66.89 | 16.54 | 8.08 %
Polyhistor-Lite (Của chúng tôi; λ = 32) | 0.24 / 3.28 | 77.74 | 66.33 | 65.03 | 17.65 | 5.15 %

Bảng 5: Nghiên cứu ablation về kích thước của hạng trong các ma trận đầu ra hypernetwork. Chúng tôi thay đổi chiều của hạng r từ 1 đến n/2 trên Polyhistor của chúng tôi. Lưu ý rằng n là chiều của các hidden vectors trong adapters. Tất cả kết quả trong bảng này dựa trên SwinTransformer-Tiny được đào tạo trước trên ImageNet-1k.

Chiều của | Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Hạng r | Encoder/ Tất cả | Seg. ↑ | H.Seg. ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | - | 110.07 / 112.62 | 67.21 | 61.93 | 62.35 | 17.97 | 0.00 %
Polyhistor 1 | 2.38 / 4.93 | 70.31 | 58.61 | 64.14 | 17.98 | 0.52 %
Polyhistor n/8 | 4.08 / 6.63 | 71.18 | 59.52 | 65.04 | 17.81 | 1.70 %
Polyhistor n/4 | 6.41 / 8.96 | 70.87 | 59.54 | 65.47 | 17.47 | 2.34 %
Polyhistor n/2 | 11.08 / 13.63 | 71.31 | 60.15 | 65.46 | 17.40 | 2.84 %

BitFit, PHM layer, Compacter, LoRA, và Low-rank Adapter). Xu hướng này phù hợp với những gì chúng tôi tìm thấy trong Swin Transformer. Chúng tôi cho thấy rằng phương pháp của chúng tôi tổng quát hóa cho các backbone khác nhau.

E Kết quả thí nghiệm của các mô hình tự giám sát.
Chúng tôi tiến hành thí nghiệm sử dụng Swim Transformer-Tiny tự giám sát (MoBY-Tiny [33]), và, để so sánh công bằng, chúng tôi cũng chạy tất cả baseline với MoBY-Tiny và báo cáo kết quả trong Bảng 8. Chúng tôi thấy phương pháp đề xuất của chúng tôi có thể đạt kết quả tương tự hoặc thậm chí tốt hơn so với Hyperformer [2] trong khi sử dụng ít tham số có thể đào tạo hơn nhiều.

F Thảo luận về sự khác biệt với Visual Prompt Tuning [9]
Chúng tôi tóm tắt sự khác biệt giữa Visual Prompt Tuning và phương pháp của chúng tôi trong các điểm sau.

Thiết lập Bài toán Khác nhau: Visual Prompt Tuning tập trung vào thích ứng hiệu quả tham số nhiệm vụ đơn lẻ, trong khi phương pháp đề xuất của chúng tôi tập trung vào thích ứng hiệu quả tham số đa nhiệm vụ. Mục tiêu của chúng tôi là thực hiện thích ứng hiệu quả tham số cho nhiều nhiệm vụ và chia sẻ thông tin có lợi qua nhiều nhiệm vụ thị giác.

Các loại phương pháp hiệu quả tham số khác nhau: Visual Prompt Tuning thêm các tham số có thể học cùng với các visual embeddings, trong khi phương pháp đề xuất của chúng tôi sử dụng một hyper-network được chia sẻ để tạo ra trọng số adapter cho các nhiệm vụ khác nhau. Ngoài ra, vị trí chèn của các tham số có thể học khác nhau (VPT: không gian đầu vào, Của chúng tôi: song song với các fully-connected layers).

--- TRANG 12 ---
Bảng 6: Cải thiện hạn chế từ việc điều chỉnh siêu tham số trên phương pháp baseline. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn. λ biểu thị tỷ lệ down-projection của adapters (tức là tỷ lệ đầu vào adapter d so với các hidden vectors n, λ = d/n).

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder / Tất cả | Seg. ↑ | H.Seg. ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 110.07 / 112.62 | 67.21 | 61.93 | 62.35 | 17.97 | 0.00 %
Fine-tuning Decoders | 0.00 / 2.55 | 63.14 | 52.37 | 58.39 | 20.89 | -11.02 %
Compacter++ (λ = 1) [12] | 0.14 / 2.69 | 67.33 | 55.68 | 59.50 | 19.66 | -5.98 %
Compacter++ (λ = 2) [12] | 0.11 / 2.66 | 67.26 | 55.69 | 59.47 | 19.54 | -5.84 %
Compacter++ (λ = 8) [12] | 0.09 / 2.64 | 67.19 | 55.85 | 59.48 | 19.56 | -5.96 %
Compacter (λ = 1) [12] | 0.28 / 2.83 | 67.94 | 56.23 | 60.18 | 19.25 | -4.69 %
Compacter (λ = 2) [12] | 0.23 / 2.78 | 68.08 | 56.41 | 60.08 | 19.22 | -4.55 %
Compacter (λ = 8) [12] | 0.19 / 2.74 | 68.15 | 56.16 | 60.12 | 19.37 | -4.83 %
Adapter (λ = 1) [21] | 17.32 / 19.87 | 69.13 | 57.35 | 61.17 | 18.79 | -2.75 %
Adapter (λ = 2) [21] | 8.69 / 11.24 | 69.21 | 57.38 | 61.28 | 18.83 | -2.71 %
Adapter (λ = 4) [21] | 4.37 / 6.92 | 68.93 | 57.33 | 61.24 | 18.95 | -3.03 %
Adapter (λ = 8) [21] | 2.21 / 4.76 | 69.04 | 57.34 | 61.25 | 18.86 | -2.86 %
Adapter (λ = 16) [21] | 1.13 / 3.68 | 69.03 | 57.22 | 61.17 | 18.91 | -3.01 %
Shared Adapter (λ = 1) [20] | 4.35 / 6.89 | 70.57 | 59.43 | 62.54 | 19.07 | -1.21 %
Shared Adapter (λ = 2) [20] | 2.20 / 4.74 | 70.21 | 59.15 | 62.29 | 19.26 | -1.83 %
Shared Adapter (λ = 4) [20] | 1.12 / 3.66 | 70.02 | 58.87 | 62.09 | 19.35 | -2.22 %
Shared Adapter (λ = 8) [20] | 0.58 / 3.12 | 69.63 | 58.54 | 61.74 | 19.61 | -2.99 %
Hyperformer (λ = 8) [16] | 72.77 / 75.32 | 71.43 | 60.73 | 65.54 | 17.77 | 2.64 %
Hyperformer (λ = 16) [16] | 37.69 / 40.24 | 71.28 | 60.19 | 65.82 | 17.89 | 2.31 %
Hyperformer (λ = 32) [16] | 20.15 / 22.70 | 71.12 | 59.71 | 64.41 | 19.06 | -0.14 %
Polyhistor (Của chúng tôi) | 6.41 / 8.96 | 70.87 | 59.54 | 65.47 | 17.47 | 2.34 %
Polyhistor-Lite (Của chúng tôi) | 0.41 / 2.96 | 70.24 | 59.12 | 64.75 | 17.40 | 1.74 %

Bảng 7: Kết quả thí nghiệm về Multi-Task Adaptation. Chúng tôi sử dụng Pyramid Vision Transformer-Small làm feature backbone. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn. λ biểu thị tỷ lệ down-projection của adapters (tức là tỷ lệ đầu vào adapter d so với các hidden vectors n, λ = d/n).

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder/Tất cả | Seg. ↑ | H.Part ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 95.88 / 97.99 | 68.81 | 61.27 | 62.67 | 17.55 | 0.00 %
Fine-tuning Decoders | 0.00 / 2.11 | 64.86 | 51.18 | 61.54 | 19.55 | -8.85 %
Bitfit [7] | 0.22 / 2.34 | 71.41 | 55.71 | 64.08 | 18.69 | -2.38 %
Adapter [21] | 0.79 / 2.90 | 71.94 | 56.38 | 64.16 | 18.75 | -1.97 %
LoRA [10] | 0.30 / 2.41 | 71.89 | 56.90 | 64.27 | 18.48 | -1.35 %
Low-rank adapter | 0.25 / 2.36 | 70.72 | 55.34 | 63.39 | 18.70 | -3.08 %
PHM layer [12] | 0.42 / 2.53 | 70.81 | 55.02 | 63.51 | 18.75 | -3.20 %
Compacter++ [12] | 0.09 / 2.20 | 70.29 | 54.80 | 63.16 | 18.82 | -3.71 %
Hyperformer [16] | 14.03 /16.14 | 70.81 | 57.76 | 65.49 | 17.75 | 0.14 %
Polyhistor-Lite (Của chúng tôi; λ = 1) | 5.21 / 7.32 | 71.00 | 57.52 | 65.83 | 17.83 | 0.13 %
Polyhistor-Lite (Của chúng tôi; λ = 32) | 0.29 / 2.40 | 70.93 | 56.71 | 65.00 | 17.95 | -0.73 %

G Chi tiết Triển khai
Để so sánh công bằng giữa các phương pháp khác nhau, chúng tôi sử dụng batch size 12 và đào tạo trong 60 epochs cho mỗi nhiệm vụ. Chúng tôi sử dụng bộ tối ưu hóa Adam [34] với learning rate 1e-4 và weight decay 1e-4, và learning rate được giảm tuyến tính theo iteration đào tạo.

Chúng tôi theo công trình học đa nhiệm vụ trước đây [31] để sử dụng trọng số theo nhiệm vụ trên các loss khác nhau, trong khi chúng tôi thấy rằng việc sử dụng trọng số đồng nhất trên các loss có kết quả tương tự như trọng số theo nhiệm vụ. Chúng tôi cũng áp dụng cùng các data augmentations, RandomHorizontalFlip, RandomScale

--- TRANG 13 ---
Bảng 8: Kết quả thí nghiệm về Multi-Task Adaptation. Chúng tôi sử dụng MoBY-Tiny [33] làm feature backbone. ↑ biểu thị cải thiện tương đối hoặc khoảng cách so với Single-task Full Fine-tuning. Kết quả với ký hiệu ↑/↓ chỉ ra cao hơn/thấp hơn là tốt hơn. λ biểu thị tỷ lệ down-projection của adapters (tức là tỷ lệ đầu vào adapter d so với các hidden vectors n, λ = d/n).

Số lượng Tham số Có thể Đào tạo | Hiệu suất của Mỗi Nhiệm vụ Hạ nguồn | Kết quả Trung bình
Encoder/Tất cả | Seg. ↑ | H.Part ↑ | Sal. ↑ | Normals ↓ | ↑

Single-task Full Fine-tuning | 110.07 / 112.62 | 65.52 | 61.78 | 62.05 | 18.14 | 0.00 %
Fine-tuning Decoders | 0.00 / 2.55 | 59.64 | 52.97 | 59.60 | 19.88 | -9.21 %
Bitfit [7] | 0.30 / 2.85 | 63.43 | 54.90 | 59.50 | 19.80 | -6.90 %
VPT-shallow [9] | 0.02 / 2.57 | 59.50 | 52.84 | 59.48 | 19.88 | -9.36 %
VPT-deep [9] | 0.88 / 3.43 | 56.15 | 50.30 | 57.22 | 20.71 | -13.72 %
Adapter [21] | 8.69 / 11.24 | 65.00 | 56.66 | 60.84 | 18.64 | -3.45 %
LoRA [10] | 0.32 / 2.87 | 65.64 | 57.66 | 62.29 | 18.47 | -1.99 %
Low-rank adapter | 0.34 / 2.89 | 63.30 | 55.24 | 59.72 | 19.14 | -5.82 %
PHM layer [12] | 0.59 / 3.14 | 63.21 | 54.99 | 59.70 | 19.13 | -5.95 %
Compacter++ [12] | 0.11 / 2.66 | 62.31 | 54.69 | 59.43 | 19.58 | -7.14 %
Hyperformer [16] | 19.29 /44.25 | 66.50 | 58.97 | 66.02 | 17.61 | 1.56 %
Polyhistor (Của chúng tôi) | 6.41 / 8.96 | 67.69 | 59.32 | 65.15 | 17.43 | 2.05 %
Polyhistor-Lite (Của chúng tôi) | 0.41 / 2.96 | 67.23 | 58.90 | 64.62 | 17.72 | 1.09 %

với phạm vi [0.75, 1.25], RandomRotate với phạm vi [-20, 20], và Resize về (512, 512), được sử dụng trong công trình trước đây [31].

Đối với các siêu tham số của Polyhistor, chúng tôi đặt chiều đầu vào của adapter d là chiều của các hidden vectors trong SwinTransformers, và tỷ lệ down-projection được đặt là λ = d/n = 16. Đối với ma trận đầu ra hạng thấp của hyper-networks, chúng tôi đặt hạng là n/4, trong đó n là kích thước bottleneck. Chúng tôi đặt kích thước của task embeddings là 64.

Đối với các siêu tham số của Polyhistor-Lite, chúng tôi cũng đặt chiều đầu vào của adapter d là chiều của các hidden vectors trong SwinTransformers, và tỷ lệ down-projection được đặt là λ = d/n = 2. Đối với ma trận đầu ra hạng thấp của hyper-networks, chúng tôi đặt hạng là n/4, trong đó n là kích thước bottleneck. Chúng tôi đặt kích thước của task embeddings là 64.
