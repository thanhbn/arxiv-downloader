# 2309.06922.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.06922.pdf
# File size: 870241 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning
Sanghyeon Kim1* Hyunmo Yang2* Younghyun Kim2* Youngjoon Hong3†
Eunbyung Park1,2†
1Department of Electrical and Computer Engineering, Sungkyunkwan University
2Department of Artificial Intelligence, Sungkyunkwan University
3Department of Mathematical Sciences, KAIST
Abstract
The recent surge in large-scale foundation models has
spurred the development of efficient methods for adapting
these models to various downstream tasks. Low-rank adap-
tation methods, such as LoRA, have gained significant at-
tention due to their outstanding parameter efficiency and
no additional inference latency. This paper investigates a
more general form of adapter module based on the analy-
sis that parallel and sequential adaptation branches learn
novel and general features during fine-tuning, respectively.
The proposed method, named Hydra, due to its multi-head
computational branches, combines parallel and sequential
branch to integrate capabilities, which is more expressive
than existing single branch methods and enables the explo-
ration of a broader range of optimal points in the fine-tuning
process. In addition, the proposed adaptation method ex-
plicitly leverages the pre-trained weights by performing a
linear combination of the pre-trained features. It allows
the learned features to have better generalization perfor-
mance across diverse downstream tasks. Furthermore, we
perform a comprehensive analysis of the characteristics of
each adaptation branch with empirical evidence. Through
an extensive range of experiments, encompassing compar-
isons and ablation studies, we substantiate the efficiency
and demonstrate the superior performance of Hydra. This
comprehensive evaluation underscores the potential impact
and effectiveness of Hydra in a variety of applications. Our
code is available on https://github.com/extremebird/Hydra
1. Introduction
The large-scale foundation models have been remark-
ably successful across a broad range of domains and
tasks [5, 12, 15, 40, 61]. Training these large-scale mod-
els from scratch is a formidable task, primarily limited to
*Equal contributions
†Corresponding authors
Figure 1. Ghraphical illustration of the model adaptation ap-
proaches. (a) fully updates allmodel parameters. (b) and (c) uti-
lize adapter module either in a parallel or a sequential manner. (d)
Hydra leverages both parallel and sequential adapter.
a selected few organizations. The main obstacles hindering
broader accessibility are the substantial model sizes, exor-
bitant computational requirements, and the unavailability of
extensive datasets. In particular, a large model size imposes
a significant computational burden even during fine-tuning
for downstream tasks. Efficiently adapting these large-scale
models to downstream tasks has emerged as the prevailing
practice in numerous applications.
Parameter Efficient Fine-tuning (PEFT) methods [7,
32–34, 47, 48, 80] efficiently fine-tune a pre-trained net-
work. Although these methods optimize a significantly
smaller number of parameters than the total parameters,
they have outperformed the full tuning in various down-
stream tasks. Among PEFT methods, adapter-based meth-
ods [7, 32, 33, 48] have demonstrated superior performance
and been widely used. They attach lightweight modules,
1arXiv:2309.06922v1  [cs.CV]  13 Sep 2023

--- PAGE 2 ---
called adapter, to a pre-trained model and optimize only
the adapter modules during fine-tuning. Recently, moti-
vated by empirical evidence of low intrinsic dimension in
model adaptation, LoRA [33] leverages linear adapter mod-
ules to eliminate the additional inference latency existed in
previous adapter-based methods [7, 32]. Furthermore, var-
ious matrix factorization techniques have been applied to
adapter modules to enhance efficiency [29, 82].
While adapter-based methods have become more effi-
cient and advanced, they have been limited to either a par-
allel or a sequential approach. The parallel (Fig. 1-(b))
and sequential (Fig. 1-(c)) approaches are represented as
f(x) +g(x)andf(x) +g(f(x)), respectively, where fis a
pre-trained module and gis an adapter module. While these
two are expressed in a similar fashion, the adapter module
of each approach is optimized with different features, xand
f(x), as inputs. In other words, task-specific features can be
acquired based on how the adapter module is attached dur-
ing fine-tuning. However, existing adapter-based methods
have not extensively explored this aspect.
This paper investigates the characteristics of each attach-
ment approach. The parallel branch is optimized over the
same input xas the pre-trained layer, leading to the learn-
ing of task-specific features that have not been pre-trained.
This aligns well with the empirical observations, as previ-
ous study [33] found that low-rank adaptation often ampli-
fies the important features relevant to specific downstream
tasks. On the other hand, the sequential branch learns to
combine general features from the pre-trained large-scale
model due to its explicit formulation g(f(x)).
Based on these characteristics, we propose a more gen-
eral form of adapter module, called Hydra1, combining par-
allel and sequential adapter modules. The proposed form is
f(x) +gp(x) +gs(f(x)), where gpandgsare parallel and
sequential adapter, respectively. This formulation is inher-
ently more expressive than single branch approaches, as it
can be reduced to one of them when gp(·) = 0 orgs(·) = 0 .
Our hypothesis is that the introduction of a more general
and expressive form enables the exploration of a broader
range of local optima. Consequently, this increased flexibil-
ity may lead to superior generalization performance for the
new tasks.
In addition, we use the linear adapter module of LoRA to
construct the proposed method while preserving its advan-
tageous properties. Therefore, the two additional computa-
tional branches of our method can be merged after training,
hence no additional latency during inference. Moreover,
thanks to the simple and versatile linear adapter structure,
the proposed module can not only be easily implemented
but also be plugged into any linear layers for parameter ef-
ficient fine-tuning purposes.
1Named after the multi-headed Greek mythological beast, rhyme with
the original LoRAIn this paper, we deeply delve into the role of both par-
allel and sequential branches. We observe that each branch
learns distinct features during fine-tuning. To elaborate, the
parallel branch tends to learn new features by exploring fea-
tures that were absent during the pre-training phase, and the
sequential branch relatively general features by exploiting
pre-trained features. The proposed method, Hydra , has un-
dergone extensive testing on popular transformer architec-
tures, and we have conducted fine-tuning experiments on
diverse datasets spanning vision and natural language tasks.
As a result, leveraging both parallel and sequential branches
further increases the fine-tuning ability of the model, sur-
passing other prevalent fine-tuning methods.
2. Related works
2.1. Transformer
Transformer is a neural network architecture that uses
multi-head self attention layers and was initially proposed
for machine translation [71]. Many large-scale pre-trained
transformers [5, 15, 45, 50, 60, 61] have exhibited outstand-
ing performance on numerous natural language process-
ing (NLP) tasks, indicating their scalability. These suc-
cesses of transformers in NLP fields inspired [18] to intro-
duce the Vision Transformer (ViT), a purely transformer-
based backbone architecture for computer vision (CV) tasks
that demonstrated promising results. Subsequently, many
transformer-based vision models [12, 17, 51, 69] have been
suggested and shown significant improvement on vision
tasks, including image classification [13, 42], dense pre-
diction [49, 84], and image generation [31, 38]. Moreover,
multi-modal training [59] and self-supervised learning [28]
also accelerate the broad use of ViT. In this paper, we apply
our method to transformer architectures widely used in both
language and vision tasks.
2.2. Adapter-based methods
Adapter-based method is one of the parameter-efficient
adaptation methods that involve training only lightweight
adapter modules without updating the original parameters
of a pre-trained model. [63] is a pioneer work that ap-
plied adapter modules for multiple vision domain adapta-
tion. Adapter [32] introduced a low-rank residual adapter
module that consists of down and up projection with inter-
mediate non-linear function.
Subsequent studies [1, 58, 64] demonstrated promising
and efficient fine-tuning performance in various NLP tasks.
Moreover, Compacter [37] leveraged the Kronecker product
decomposition and parameter sharing to the projection ma-
trices of adapter modules for more efficient adaptation. VL-
adapter [68] successfully applied various adapters to multi-
modal (vision and language) tasks, demonstrating their ver-
satility and effectiveness. Adaptformer [7] introduced a
2

--- PAGE 3 ---
parallel adapter to feed-forward networks of ViT for vi-
sual recognition tasks. While these adapter tuning meth-
ods demonstrated promising results, the additional adapter
branches, which have intermediate non-linear functions,
slow down the inference speed.
LoRA [33] proposed a low-rank adaptation module com-
posed solely of linear layers. This design allows the pa-
rameters of introduced branches to be merged in an addi-
tive manner with the pre-trained parameters at the inference
stage, ensuring no latency. Compared to existing adapter
tuning, it has shown competitive or even better adaptation
ability in NLP fields. AdaLoRA [82] further improved
LoRA with singular value decomposition (SVD) for adap-
tive budget allocation. KAdaptation [29] leveraged a low-
rank weight update manner similar to LoRA, in which the
update weights are obtained by Kronecker product of shared
matrix and low-rank matrix, for fine-tuning vision mod-
els. Also, FacT [35] proposed tensorization-decomposition
framework, which tensorize whole ViT into a single 3D ten-
sor, then apply Factor-Tuning with various tensor decompo-
sition methods, such as Tensor-Train(TT) or Tucker(TK).
SSF [48] suggested the introduction of scaling and shift-
ing factors to perform linear transformation on features after
pre-trained modules of ViT, in order to match the target dis-
tribution. Recently, RepAdapter [52] proposed a sequential
structural reparameterization scheme for low-rank adapta-
tion modules. These studies have exhibited competitive per-
formance and efficiency, not sagging the inference speed.
Building upop these single branch approaches, we leverage
parallel and sequential branches together to demonstrate su-
perior performance.
2.3. Other PEFT approaches
Besides the success of adapter-based methods, the ap-
proaches without adapter have been explored. BitFit [80]
only trained bias-terms during fine-tuning. Diff-pruning
[26] introduced a task-specific diff vector, which is adap-
tively pruned during training. Token-based tuning ap-
proaches [34, 44, 47, 65] also widely used PEFT methods.
They involves attaching supplementary tokens, also known
as prompt, to the input or intermediate sequence and fine-
tune them to guide the model’s attention towards the rele-
vant information for the new task. VPT [34] demonstrated
promising performance in vision domain by applying the
prompt tuning approach that succeeded in natural language
tasks. While token-based tuning has demonstrated promis-
ing tuning ability, the addition of new tokens causes a few
drawbacks. It reduces the available input sequence length,
potentially limiting the amount of context that the model
can process effectively. Moreover, it increases computa-
tional complexity. Moreover, applying these approaches to
models that utilize local self-attention can pose additional
challenges.3. Methods
3.1. Preliminary
LoRA [33] applies a linear adapter module on linear
(dense) layers of a pre-trained model for efficient model
adaptation. It assumes that the intrinsic rank of adaptation
matrix A∈Rd×kis low, allowing a low-rank decompo-
sition on A(rank (A)≪min(d, k)). That is, the adap-
tation matrix Ais decomposed as A=AupAdown, where
Aup∈Rd×randAdown∈Rr×kare an up-projection and
down-projection adaptation matrix, respectively. Thus, the
linear adapter module is formulated as g(x;Aup, Adown) =
AupAdownx. For brevity, henceforth, we use g(x;A)to de-
noteg(x;Aup, Adown)
For a given input feature x∈Rk, the forward pass of
LoRA is implemented as below:
h=f(x;W0, b0) +g(x;A) (1)
=W0x+b0+AupAdownx, (2)
where h∈Rdis an output vector, W0∈Rd×kis a pre-
trained weight matrix, and b0∈Rdis a bias. To effi-
ciently optimize the linear layer during fine-tuning, only the
adaptation matrices AupandAdownare trained, and the pre-
trained matrix W0and bias b0are frozen. While using the
extra parallel branch g(x;A)is efficient for fine-tuning, it
leads latency in inference. Thanks to the linearity, the for-
ward pass in Eq. (2) can be re-implemented as follow:
h= (W0+A)x+b0 (3)
=f(x;W0+A, b 0), (4)
In other words, during inference, the adapter module can be
merged into the pre-trained linear layer, ensuring no addi-
tional computational cost.
In Eq. (1), it is evident that LoRA is one of the par-
allel approaches. The linear adapter module of LoRA can
be optimized without direct dependence on the pre-trained
matrix W0. Therefore, it would facilitates the ease of learn-
ing new features that diverge from the pre-trained features.
However, there is a possibility of losing the generalization
ability of pre-trained weight matrix.
3.2. SeqLoRA
In order to compare parallel and sequential approaches,
we introduce SeqLoRA, a sequential form of LoRA, lever-
aging the idea of a low-rank adaptation on an output vector
of the pre-trained linear layer. This results in the following
forward pass:
h=f(x;W0, b0) +g(f(x;W0, b0);B) (5)
=W0x+b0+BupBdownW0x+BupBdownb0, (6)
where B∈Rd×dis an adaptation matrix, Bup∈Rd×ris
an up-projection adaptation matrix, and Bdown∈Rr×dis a
3

--- PAGE 4 ---
down-projection adaptation matrix. Similar to LoRA, only
the adapter module is optimized and the forward pass for
inference can be represented as a single linear layer:
h= (W0+BW 0)x+b0+Bb0 (7)
=f(x;W0+BW 0, b0+Bb0), (8)
We posit that LoRA and SeqLoRA are complementary
to each other. SeqLoRA can learn new features for the
downstream tasks by linearly combining features from the
pre-trained layer. While SeqLoRA has the capacity to learn
highly useful features based on the capabilities of the large-
scale pre-trained models, it may encounter limitations in
learning novel concepts or features that were absent during
the pre-training phase.
SeqLoRA shares similarities with the recently proposed
RepAdapter [52] in terms of its sequential linear adapter
module. However, we introduce it to compare with its par-
allel counterpart and utilize a component of following our
proposed method, Hydra .
3.3. Hydra
To harness the strengths of both LoRA and SeqLoRA,
we introduce Hydra , a more general form of linear adap-
tation module that integrates the capabilities of both meth-
ods. Hydra allows for the combination and utilization of the
advantageous aspects of LoRA and SeqLoRA, providing a
comprehensive and flexible framework for efficient and ef-
fective model adaptation. More precisely, it can not only
capture novel features easily but also get a broader sight
based on general pre-trained features. For Hydra , we com-
bine parallel and sequential adaptation branches, which al-
lows us the following forward pass:
h=f(x;W0, b0) +g(x;A) +g(f(x;W0, b0);B)(9)
=W0x+b0+AupAdownx
+BupBdownW0x+BupBdownb0, (10)
where A∈Rd×k,Aup∈Rd×ra,Adown∈Rra×kis an
adaptation matrix for parallel branch and its low-rank de-
composition with rank ra,B∈Rd×d,Bup∈Rd×rb,
Bdown∈Rrb×dis an adaptation matrix for sequential
branch and its low-rank decomposition with rank rb. For
simplicity, we set rank as ra=rbthroughout the paper.
Following LoRA, we use a random Gaussian initialization
for down-projection matrices, Adown andBdown, and zero
initialization for up-projection matrices, AupandBup. Con-
sequently, at the start of the training, both AandBare ini-
tialized to zero. For model adaptation, Aup,Adown,Bup, and
Bdown are trained based on the gradient descent, while W0
andb0are not updated.
As depicted in Fig. 1-(d), the implementation for train-
ing consists of three branches: pre-trained, parallel, and se-
quential. After the training, parallel and sequential branches
Figure 2. Hydra -MLP in a transformer architecture in the training
phase. Linear Adapter Module (LAM) implements down projec-
tion and up projection on its input in order.
can be merged into the pre-trained branch as follows:
h= (W0+A+BW 0)x+b0+Bb0 (11)
=f(x;W0+A+BW 0, b0+Bb0), (12)
Therefore, our method does not increase computational
complexity during inference.
In addition, it is apparent that LoRA and SeqLoRA can
be identified as specific instances of Hydra when B= 0
andA= 0, respectively. This observation establishes that
our method encompasses a more generalized framework for
task-specific adaptations. As a result, our approach provides
an enhanced modeling capacity to comprehensively capture
various adaptation scenarios during fine-tuning.
3.4. Architecture design
While our approach is designed to be compatible with
any linear layers, in this work, we focus on its application
to the MLP blocks within transformer architectures, which
have been widely used in recent large-scale models. As il-
lustrated in Fig. 2, a typical transformer block consists of a
multi-head self attention (MSA) block and an MLP block,
interleaved by non-linear activations and layer normaliza-
tion. We replace the proposed adaptation module with the
last layer of the MLP block. Since non-linear activations
are not employed in the final linear layer of the MLP, we
can avoid the potential ‘additional latency’ that could arise
during the inference. We refer to the MLP block, to which
our method is applied, as Hydra -MLP.
In addition, this design choice is also motivated by recent
studies, revealing that self-attention blocks in the transform-
ers tend to diminish high-frequency information, whereas
MLP blocks amplify it [56, 75]. As Hydra -MLP contains
4

--- PAGE 5 ---
Method
#Params (M)
Avg Acc. ( ↑)
PE (↑)
Caltech101
CIFAR10
CIFAR100
Country211
DTD
EuroSAT
FER2013
FGVCAircraft
Food101
GTSRB
HatefulMemes
KITTI-Dist
MNIST
Flowers102
OxfordPets
PatchCamelyon
SST2
Resisc45
StanfordCars
VOC2007
Full tuning 87.9 65.49 0.498 87.64 91.11 71.52 15.75 54.36 85.24 52.72 26.22 83.28 74.05 55.64 39.15 65.55 80.55 87.31 64.92 59.09 75.61 57.21 82.95
Linear-probing 0.03 66.32 0.663 90.96 90.35 67.31 17.36 62.04 72.95 51.91 29.52 83.82 56.47 55.83 40.37 77.50 92.29 88.03 59.00 59.36 78.10 68.30 84.99
Adapter [32] 1.24 65.08 0.647 90.18 90.14 73.57 16.83 57.13 67.97 41.76 30.52 83.58 58.50 48.91 37.18 80.34 90.78 86.52 59.92 58.70 79.22 67.68 82.22
LoRA [33] 0.18 61.48 0.614 87.64 90.52 69.69 17.12 50.16 74.03 51.04 20.01 83.76 42.96 55.88 48.05 61.36 74.28 85.49 63.20 57.04 62.09 54.89 80.33
Compacter [37] 0.08 62.79 0.628 89.02 79.96 44.33 28.22 52.93 50.48 35.46 41.13 78.28 66.90 47.60 57.72 85.82 88.29 79.23 61.83 64.22 63.76 64.79 75.84
Kadaptation [29] 0.08 68.92 0.689 88.96 90.03 73.92 17.53 63.97 76.25 47.45 30.04 84.38 80.71 55.86 42.29 85.20 93.19 89.05 63.39 59.18 79.96 70.21 84.49
Hydra 0.20 70.95 0.709 91.23 90.89 74.20 17.75 64.47 87.00 51.10 33.05 84.27 87.11 55.91 42.05 90.76 93.18 89.38 70.83 59.58 82.41 71.19 82.66
Table 1. ELEV ATER experiment results. We use CLIP pre-trained ViT-Base-224/32 as a backbone model.
SeqLoRA that is designed to exploit the pre-trained features
through the linear combination, our approach effectively en-
courages the model to promote useful high-frequency fea-
tures for specific downstream tasks. Unless specified, in this
paper, Hydra indicates Hydra -MLP.
4. Experimental results
We substantiate the versatility of our method Hydra
through an extensive range of experiments including both
vision and natural language tasks. Next, we analyze the
characteristics based on the utilization approach of adapter
branch and discuss the computational efficiency of the pro-
posed method. Finally, we verify the effectiveness of our
architecture design by conducting ablation studies.
4.1. Few-shot experiments
First, since various fine-tuning applications opt to fall
down to the condition of limited data accessibility, we val-
idated our proposed Hydra in a few-shot learning scenario
using 20 image classification datasets from the ELEV ATER
benchmark [46]. Each dataset comprises a distinct num-
ber of labels along their corresponding images. Following
the previous work [29], we used the CLIP pre-trained ViT-
Base-224/32 as a backbone model. And, we set the bottle-
neck rank of Hydra asra=rb= 2. Detailed experimental
settings and statistics of each dataset are reported in Appen-
dices B and C, respectively.
As shown in Tab. 1, Hydra achieved the highest accu-
racy score on 11 out of 20 datasets and surpassed the other
PEFT methods with regard to the average accuracy. Fur-
thermore, we report the PE score [46] to compare accuracy-
efficiency trade-off. The PE score is defined as follow:
PE=accuracy ·exp(−log10(p/M 0+ 1)) , (13)
where pis the number of trainable parameters, and M0is a
magnitude of pre-trained model’s parameters. We set M0=
108. We observed that our method also attained the highestPE score in Tab. 1. As a result, the proposed method is not
only effective but also efficient for few-shot learning.
4.2. VTAB-1k experiments
Next, we conducted the experiment on the VTAB-1k
benchmark [81] to compare Hydra with the state-of-the-
art PEFT methods. The VTAB-1k benchmark consists 19
vision datasets and each dataset are categorized into three
groups with different concepts, i.e., the Natural ,Special-
ized andStructured . We used ViT-Base-224/16 model pre-
trained on ImageNet-21k in a supervised manner. Follow-
ing prior works [52], we applied our Hydra module on ev-
ery layer of both projection layer in attention block and
final linear layer of MLP block with low-rank dimension
ra=rb= 2for this experiment. More details of the exper-
iments are in Appendices B and C.
We noted that Hydra excels recent PEFT methods in
Tab. 2. Compared to existing non-linear adapter meth-
ods [7, 32], our method has demonstrated enhanced perfor-
mance, avoiding any additional inference latency through
the combination of linear operations. Therefore this signi-
fies linear adapter modules can also function well in a multi-
branch approach.
Furthermore, it is noteworthy the proposed method,
which combines parallel and sequential adaptation
branches, outperforms previous single branch (either paral-
lel or sequential) approaches [33, 35, 48, 52]. To effectively
learn task-specific feature during fine-tuning, in other
words, both a parallel branch that learns novel concepts
and a sequential branch that transforms pre-trained features
need to be used in conjunction. As a result, introducing a
more comprehensive and expressive structure is helpful for
proficient task adaptation.
4.3. Natural language understanding experiments
In the field of NLP, transformers have achieved great suc-
cess, leading numerous large-scale pre-trained transformer
5

--- PAGE 6 ---
Natural Specialized Structured
Method
#Params (M)
Avg Acc.
CIFAR100
Caltech101
DTD
Flowers102
OxfordPets
SVHN
Sun397
PatchCamelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSPR-Loc
dSPR-Ori
sNORB-Azim
sNORB-Ele
Full tuning 85.8 68.9 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1
Linear-probing 0.04 57.6 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2
Adapter [32] 0.16 73.9 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6
AdaptFormer [7] 0.16 74.7 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1
LoRA [33] 0.29 74.5 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0
VPT [34] 0.53 72.0 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8
NOAH [83] 0.36 75.5 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2
SSF [48] 0.22 75.7 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9
FacT [35] 0.07 75.6 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0
RepAdapter [52] 0.22 76.1 72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0
Hydra 0.28 76.5 72.7 91.3 72.0 99.2 91.4 90.7 55.5 85.8 96.0 86.1 75.9 83.2 68.2 50.9 82.3 80.3 50.8 34.5 43.1
Table 2. VTAB-1k experiment results. All methods are conducted by ViT-Base-224/16 pre-trained on ImageNet-21k.
Method
#Params (M)
Avg.
MNLI
SST-2
MRPC
CoLA
QNLI
QQP
RTE
STS-B
Full tuning 125 86.4 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2
BitFit [80] 0.1 85.2 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8
AdapterDrop [64] 0.3 84.4 87.1 94.2 88.5 60.8 93.1 90.2 71.5 89.7
AdapterDrop [64] 0.9 85.4 87.3 94.7 88.4 62.6 93.0 90.6 75.9 90.3
LoRA [33] 0.3 87.2 87.5 95.1 89.7 63.4 93.3 90.8 86.6 91.5
Hydra 0.3 87.9 87.5 95.0 92.2 65.4 92.8 90.8 87.4 91.7
Table 3. Natural language understanding results. We report the
Matthew’s correlation for CoLA, Pearson correlation for STS-B,
and accuracy for the others.
models. Hence, many PEFT methods were initially pro-
posed for NLP tasks. Therefore, in this section, we vali-
dated that our method can effectively fine-tune a pre-trained
NLP model as well. We performed natural language un-
derstanding experiments on GLUE benchmark [73]. Fol-
lowing [33], we used the pre-trained RoBERTa (base) [50],
which originally has 125M trainable parameters from the
HuggingFace Transformers library [78]. More experimen-
tal details are in Appendices B and C.
As shown in Tab. 3, Hydra has demonstrated supe-
rior adaptation capability compared to full tuning while
requiring significantly fewer trainable parameters. Simi-
larly, akin to results from vision task experiments, the pro-posed method outperforms existing PEFT approaches. No-
tably, despite both LoRA and Hydra applying the same lin-
ear adapter module, Hydra achieves a substantial lead over
LoRA, showing a significant performance advantage (+0.7
on average). This underscores the potential of the pro-
posed method in NLP tasks as well. In essence, our multi-
branch adapter module exhibits strong performance across
domains, making it versatile and applicable in various fine-
tuning scenarios.
4.4. Analysis
Adapter-based methods can be categorized into parallel
and sequential approaches based on the attachment manner.
While the formulations ( Eqs. (1) and (5)) are similar, they
are trained in distinct way because of different input fea-
tures. The parallel branch learns new features by exploring
features that were absent during the pre-training phase. On
the other hand, the sequential branch learns relatively gen-
eral features by exploiting pre-trained features. In this sec-
tion, we delve into the attributes of the parallel and sequen-
tial branches with experimental evidence. Following that,
we also analyze Hydra from the perspective of efficiency,
an important element of the PEFT methods.
Subspace similarity of weight matrix We analyze each
branch in terms of the weight matrix. To do so, we mea-
sured the similarity between pre-trained weight matrix W0
and the weight matrices of each branch. In Eq. (11), the
weight matrices of the parallel and sequential branches are
represented as AandBW 0, respectively, for input x. Fol-
6

--- PAGE 7 ---
lowing [33], we leveraged subspace similarity defined as
below:
ϕ(M, N, i, j ) =∥Ui
MTUj
N∥2
F
min{i, j}, (14)
where matrices Ui
M∈Rd×iandUj
N∈Rd×jare formed by
extracting from first to i-th and j-th columns of the left sin-
gular matrix of matrices MandN, respectively. We evalu-
ated the similarity between the top 10 %singular directions
in the pre-trained matrix W0and the top 2 singular direc-
tions in the adaptation weight matrix AorBW 0.
In Fig. 3, we observed higher overall similarity values
between BW 0andW0compared to AandW0due to ex-
plicitly leveraging W0. This indicates the sequential branch
tends to learn general features that are relatively similar to
pre-trained features. Furthermore, the majority of similar-
ity values do not exceed 0.25 for both AandBW 0. It
implies the Hydra module enhances task-specific features
instead of the previously amplified features by W0. Thus,
our multi-branch module effectively fulfills the role of the
adapter module, which needs to learn task-specific features.
Visualization of the feature space We conducted t-
SNE [70] visualization on the embedding features of the
[CLS] token in the last transformer block after fine-
tuning. In this visualization, we visualized the embed-
ding feature, distinguishing it into pre-trained branch output
f(x;W0, b0), parallel branch output g(x;A), and sequential
branch output g(f(x;W0, b0);B). Based on this, we inter-
preted what features each branch is trained to represent.
As illustrated in Fig. 4, we can observe a notable differ-
ence in the distribution of output features between the par-
allel branch and the sequential branch. This clearly demon-
strates that each branch hold distinct characteristics. In par-
ticular, the distribution of output features in the sequential
branch primarily within the feature space of the pre-trained
branch. It indicates that the sequential branch learns fea-
tures that are similar to well-generalized pre-trained ones.
On the other hand, the parallel branch learns the unique fea-
tures that were not acquired during pre-training.
Method #Params (M)Training Time
(sec/epoch)
Full tuning 85.8 148.95
LoRA 0.22 115.44
SeqLoRA 0.21 112.19
Hydra 0.21 119.37
Table 4. Training time comparison of full tuning, single branch
approaches, and Hydra with CIFAR10 on a single RTX3090 GPU.
The results are tested with a batch size of 128 and input resolution
of 224 × 224. Reported training time(sec/epoch) is the average
across 10 epochs.
Figure 3. Normalized subspace similarity. ϕ(W0, A, i, j )and
ϕ(W0, BW 0, i, j )of each transformer block trained on CIFAR100
in VTAB-1k benchmark. (a) and (b) correspond to the 6th and 9th
blocks of ViT-Base-224/16, respectively.
Figure 4. t-SNE visualizations of [CLS] token embedding. We
used ViT-Base-224/32 trained on CIFAR100 in VTAB-1k bench-
mark. (a), (b), and (c) are the results of different random seeds,
respectively. For each result, we used 128 images as input.
Computational efficiency Here, we address the
parameter-efficiency of our Hydra with computational com-
plexity. For simplicity, we assume the input and out-
put of linear adapter module have the same dimension
d. Then, the linear adapter module has a computational
complexity of O(2rd). This is because it is defined as
g(x;A) =AupAdownx, where Aup∈Rd×r, and Adown∈
Rr×d. Therefore, the computational complexity of the sin-
gle branch methods, LoRA and SeqLoRA, is O(2rd). For
ra=rb=r,Hydra fundamentally has two branches, lead-
ing to an increase in computational complexity. However,
in all our experiments, we set ra=rb=r
2, resulting in both
time and memory complexity of O(2rd). It implies that the
computational complexity of LoRA, SeqLoRA, and Hydra
are theoretically identical.
However, when applied to the real applications, multi-
branch design of Hydra is likely to lead bottlenecks on
GPU. To figure out the bottlenecks, we compared the train-
ing time of each method on CIFAR10 dataset. For a fair
comparison, we applied all methods, including Hydra , to
MLP blocks. We used ViT-Base-224/32 model with batch
size of 128. The results are shown in Tab. 4. It demonstrates
that when the number of parameters, meaning memory con-
sumption, is similar, it can be observed that single branch
7

--- PAGE 8 ---
Method #Params (M) Avg Acc.
LoRA 0.31 75.6
SeqLoRA 0.28 75.4
Hydra 0.28 76.5
Table 5. Comparison of LoRA, Se-
qLoRA, and Hydra on VTAB-1K bench-
mark. Each method is applied to both
MSA and MLP blocks.Method #Params (M) Avg.
LoRA 0.3 87.2
SeqLoRA 0.3 87.4
Hydra 0.3 87.9
Table 6. Comparison of LoRA, Se-
qLoRA, and Hydra on GLUE benchmark.
The reported numbers represent average
performance. Each method is applied to
MLP blocks.Block #Params (M) Avg Acc.
MSA 0.20 70.45
MLP 0.20 70.95
Table 7. Ablation results for the optimal
position of Hydra module on the ELE-
V ATER benchmark.
methods are generally faster than Hydra . However, the dif-
ference is not significant, and as observed in previous exper-
iments, Hydra has demonstrated exceptional adaptation per-
formance compared to other methods. Additionally, Hydra
has the advantage of no additional inference latency. There-
fore, the adaptation branches used for fine-tuning have no
impact on the inference computational complexity.
4.5. Ablation studies
In this section, we performed ablation studies to vali-
date the rationale behind our architecture design. First, we
carried out a head-to-head comparison to fairly assess the
efficacy of our approach. Next, we verified the effective po-
sition of Hydra in the transformer architecture. Here, we
present only the summarized tables, Tabs. 5 to 7. The full
tables are reported in Appendix A.
Head-to-head comparison Hydra is a method that com-
bines parallel and sequential branches, i.e., LoRA and
SeqLoRA. We performed experiments, ablating one of
these branches, to prove the advantage of combined branch
method. To do so, we leveraged the vision experiments in
Sec. 4.2 and the natural language experiments in Sec. 4.3
with same experimental settings. For fair comparison, we
applied each method to the blocks where the adapter mod-
ule was attached in each experiment. We configured the
low-rank rto ensure a similar number of trainable parame-
ters.
As shown in Tabs. 5 and 6, Hydra exhibits the highest av-
erage performance in both experiments. In addition, while
there is no significant difference in performance between
LoRA and SeqLoRA, Hydra demonstrates a notable dis-
crepancy. This observation implies that combining LoRA
and SeqLoRA, Hydra , is more effective way than using
each. Considering our analysis that parallel and sequential
branch are complementary in nature, the proposed method
can be regarded as effectively integrating the strengths of
each branch. Consequently, the general and expressive form
of our method enables outstanding fine-tuning across di-
verse task domains, regardless of the specific domain.
Position of Hydra module Inherently, Hydra modulecan be applied to any linear layer of the transformer, such
as projection layers of MSA blocks or linear layers that re-
side in MLP blocks. We mainly applied the Hydra module
to MLP blocks, guided by the distinctive attribute of each
block. To make it more concretely, we empirically investi-
gate the optimal block for Hydra module. It was performed
on the ELEV ATER benchmark experiments in Sec. 4.1.
The results are shown in Tab. 7. We observed that when
applying Hydra module to MLP blocks, it demonstrates bet-
ter performance. Therefore, our architecture design, Hy-
dra-MLP described in Fig. 2, is reasonable. Furthermore, it
indicates that our method well transforms pre-trained fea-
tures with amplified high-frequency by MLP block into
task-specific features.
5. Conclusion
In this paper, we conduct an in-depth analysis of the roles
of each adaptation branch, parallel and sequential, which
has not been explored. We demonstrate that parallel branch
inclines towards acquiring novel features through the explo-
ration of absent features during pre-training phase, while
the sequential branch utilizes pre-trained features to capture
relatively general features.
We also propose a general and expressive adaptation
formulation, Hydra , which combines parallel and sequen-
tial adaptation branch to integrate the capabilities of both
branches. By leveraging linear adapter module, it has no
additional inference latency and can be applied to any lin-
ear layer. Furthermore, thanks to its simple structure, Hy-
dracan be easily implemented. The proposed method
demonstrates superior performance on comprehensive ex-
periments, including both vision and natural language tasks,
without bells and whistles. This shows the versatility of Hy-
drain fine-tuning applications.
Since our focus is primarily on analyzing the character-
istics of each branch and demonstrating the effectiveness of
the multi-branch approach, we utilize simple linear adapter
module. However, the form of Hydra is not influenced
by the adapter module. Therefore, existing adapter-based
8

--- PAGE 9 ---
methods can be easily extended to multi-branch variants.
We anticipate that the general and expressive formation of
Hydra will be widely adopted in the field of the parameter
efficient fine-tuning.
References
[1] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Sim-
ple, scalable adaptation for neural machine translation. arXiv
preprint arXiv:1909.08478 , 2019. 2
[2] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Si-
mon Green, V ´ıctor Vald ´es, Amir Sadik, et al. Deepmind lab.
arXiv preprint arXiv:1612.03801 , 2016. 17
[3] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Gi-
ampiccolo. The fifth pascal recognizing textual entailment
challenge. In TAC. Citeseer, 2009. 13
[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101–mining discriminative components with random
forests. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part VI 13 , pages 446–461. Springer, 2014. 16
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1, 2
[6] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio,
and Lucia Specia. Semeval-2017 task 1: Semantic textual
similarity-multilingual and cross-lingual focused evaluation.
arXiv preprint arXiv:1708.00055 , 2017. 13
[7] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt-
ing vision transformers for scalable visual recognition. arXiv
preprint arXiv:2205.13535 , 2022. 1, 2, 5, 6
[8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-
ing image scene classification: Benchmark and state of the
art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 16,
17
[9] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 3606–3613, 2014. 16,
17
[10] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of
single-layer networks in unsupervised feature learning. In
Proceedings of the fourteenth international conference on
artificial intelligence and statistics , pages 215–223. JMLR
Workshop and Conference Proceedings, 2011. 16
[11] Ido Dagan, Oren Glickman, and Bernardo Magnini. The
pascal recognising textual entailment challenge. In Machine
Learning Challenges. Evaluating Predictive Uncertainty, Vi-
sual Object Classification, and Recognising Tectual Entail-
ment: First PASCAL Machine Learning Challenges Work-
shop, MLCW 2005, Southampton, UK, April 11-13, 2005,
Revised Selected Papers , pages 177–190. Springer, 2006. 13
[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner,Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
et al. Scaling vision transformers to 22 billion parameters.
arXiv preprint arXiv:2302.05442 , 2023. 1, 2
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2
[14] Li Deng. The mnist database of handwritten digit images for
machine learning research [best of the web]. IEEE signal
processing magazine , 29(6):141–142, 2012. 16
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1, 2
[16] Bill Dolan and Chris Brockett. Automatically constructing
a corpus of sentential paraphrases. In Third International
Workshop on Paraphrasing (IWP2005) , 2005. 13
[17] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming
Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining
Guo. Cswin transformer: A general vision transformer
backbone with cross-shaped windows. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12124–12134, 2022. 2
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2
[19] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision , 88:303–338, 2010. 16
[20] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
2004 conference on computer vision and pattern recognition
workshop , pages 178–178. IEEE, 2004. 16, 17
[21] Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new
performance measure and evaluation benchmark for road de-
tection algorithms. In 16th International IEEE Conference
on Intelligent Transportation Systems (ITSC 2013) , pages
1693–1700. IEEE, 2013. 16, 17
[22] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
William B Dolan. The third pascal recognizing textual entail-
ment challenge. In Proceedings of the ACL-PASCAL work-
shop on textual entailment and paraphrasing , pages 1–9,
2007. 13
[23] Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha
Arnoud, and Vinay Shet. Multi-digit number recognition
from street view imagery using deep convolutional neural
networks. arXiv preprint arXiv:1312.6082 , 2013. 16, 17
[24] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron
Courville, Mehdi Mirza, Ben Hamner, Will Cukierski,
Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Chal-
lenges in representation learning: A report on three ma-
chine learning contests. In Neural Information Processing:
9

--- PAGE 10 ---
20th International Conference, ICONIP 2013, Daegu, Ko-
rea, November 3-7, 2013. Proceedings, Part III 20 , pages
117–124. Springer, 2013. 16
[25] Ben Graham. Kaggle diabetic retinopathy detection compe-
tition report. University of Warwick , 22, 2015. 17
[26] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-
efficient transfer learning with diff pruning. arXiv preprint
arXiv:2012.07463 , 2020. 3
[27] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. The sec-
ond pascal recognising textual entailment challenge. In Pro-
ceedings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment , volume 7, 2006. 13
[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 2
[29] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang,
and Xin Eric Wang. Parameter-efficient fine-tuning for vi-
sion transformers. arXiv preprint arXiv:2203.16329 , 2022.
2, 3, 5, 13
[30] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classification. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 12(7):2217–2226, 2019. 16, 17
[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 2
[32] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 1, 2, 5, 6
[33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 1, 2, 3, 5, 6, 7, 13
[34] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXIII , pages 709–727. Springer, 2022. 1,
3, 6
[35] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for
lightweight adaptation on vision transformer. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 37,
pages 1060–1068, 2023. 3, 5, 6
[36] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In Proceedings
of the IEEE conference on computer vision and pattern
recognition , pages 2901–2910, 2017. 17
[37] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian
Ruder. Compacter: Efficient low-rank hypercomplex adapterlayers. Advances in Neural Information Processing Systems ,
34:1022–1035, 2021. 2, 5
[38] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 2
[39] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj
Goswami, Amanpreet Singh, Pratik Ringshia, and Davide
Testuggine. The hateful memes challenge: Detecting hate
speech in multimodal memes. Advances in Neural Informa-
tion Processing Systems , 33:2611–2624, 2020. 16
[40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 1
[41] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
Proceedings of the IEEE international conference on com-
puter vision workshops , pages 554–561, 2013. 16
[42] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. 2009. 2, 16, 17
[43] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning
methods for generic object recognition with invariance to
pose and lighting. In Proceedings of the 2004 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition, 2004. CVPR 2004. , volume 2, pages II–104.
IEEE, 2004. 17
[44] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. arXiv preprint
arXiv:2104.08691 , 2021. 3
[45] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and
Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and
comprehension. arXiv preprint arXiv:1910.13461 , 2019. 2
[46] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang,
Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng
Liu, Yong Jae Lee, et al. Elevater: A benchmark and toolkit
for evaluating language-augmented visual models. Advances
in Neural Information Processing Systems , 35:9287–9301,
2022. 5, 13
[47] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. arXiv preprint
arXiv:2101.00190 , 2021. 1, 3
[48] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. Scaling & shifting your features: A new baseline
for efficient model tuning. Advances in Neural Information
Processing Systems , 35:109–123, 2022. 1, 3, 5, 6
[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 2
[50] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
10

--- PAGE 11 ---
bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019. 2, 6
[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 2
[52] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun,
Guannan Jiang, Zhiyu Wang, and Rongrong Ji. Towards
efficient visual adaption via structural re-parameterization.
arXiv preprint arXiv:2302.08106 , 2023. 3, 4, 5, 6
[53] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
fication of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
16
[54] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander
Lerchner. dsprites: Disentanglement testing sprites dataset,
2017. 17
[55] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing , pages 722–729. IEEE, 2008. 16, 17
[56] Namuk Park and Songkuk Kim. How do vision transformers
work? arXiv preprint arXiv:2202.06709 , 2022. 4
[57] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In 2012 IEEE conference on
computer vision and pattern recognition , pages 3498–3505.
IEEE, 2012. 16, 17
[58] Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,
Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-
destructive task composition for transfer learning. arXiv
preprint arXiv:2005.00247 , 2020. 2
[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 16
[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 2
[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485–5551, 2020. 1, 2
[62] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. Squad: 100,000+ questions for machine com-
prehension of text. arXiv preprint arXiv:1606.05250 , 2016.
13
[63] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. Ad-
vances in neural information processing systems , 30, 2017.
2
[64] Andreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.
Adapterdrop: On the efficiency of adapters in transformers.
arXiv preprint arXiv:2010.11918 , 2020. 2, 6[65] Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and
Andrew Jackson. Fine-tuning image transformers using
learnable memory. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
12155–12164, 2022. 3
[66] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. Recursive deep models for semantic compositional-
ity over a sentiment treebank. In Proceedings of the 2013
conference on empirical methods in natural language pro-
cessing , pages 1631–1642, 2013. 13
[67] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
Christian Igel. The german traffic sign recognition bench-
mark: a multi-class classification competition. In The 2011
international joint conference on neural networks , pages
1453–1460. IEEE, 2011. 16
[68] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter:
Parameter-efficient transfer learning for vision-and-language
tasks. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5227–5237,
2022. 2
[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International conference on machine learning ,
pages 10347–10357. PMLR, 2021. 2
[70] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research ,
9(11), 2008. 7
[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 2
[72] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co-
hen, and Max Welling. Rotation equivariant cnns for digital
pathology. In Medical Image Computing and Computer As-
sisted Intervention–MICCAI 2018: 21st International Con-
ference, Granada, Spain, September 16-20, 2018, Proceed-
ings, Part II 11 , pages 210–218. Springer, 2018. 16, 17
[73] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman. Superglue: A stickier benchmark for general-
purpose language understanding systems. Advances in neu-
ral information processing systems , 32, 2019. 6
[74] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R Bowman. Glue: A multi-task
benchmark and analysis platform for natural language un-
derstanding. arXiv preprint arXiv:1804.07461 , 2018. 13
[75] Peihao Wang, Wenqing Zheng, Tianlong Chen, and
Zhangyang Wang. Anti-oversmoothing in deep vision trans-
formers via the fourier domain analysis: From theory to
practice. arXiv preprint arXiv:2203.05962 , 2022. 4
[76] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.
Neural network acceptability judgments. arXiv preprint
arXiv:1805.12471 , 2018. 13
[77] Adina Williams, Nikita Nangia, and Samuel R Bowman. A
broad-coverage challenge corpus for sentence understanding
11

--- PAGE 12 ---
through inference. arXiv preprint arXiv:1704.05426 , 2017.
13
[78] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, R ´emi Louf, Morgan Funtowicz, et al. Transformers:
State-of-the-art natural language processing. In Proceed-
ings of the 2020 conference on empirical methods in natural
language processing: system demonstrations , pages 38–45,
2020. 6
[79] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In 2010 IEEE computer so-
ciety conference on computer vision and pattern recognition ,
pages 3485–3492. IEEE, 2010. 16, 17
[80] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:
Simple parameter-efficient fine-tuning for transformer-based
masked language-models. arXiv preprint arXiv:2106.10199 ,
2021. 1, 3, 6
[81] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo-
longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso-
vitskiy, et al. The visual task adaptation benchmark. 2019.
5, 13
[82] Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-efficient fine-
tuning. arXiv preprint arXiv:2303.10512 , 2023. 2, 3
[83] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural
prompt search. arXiv preprint arXiv:2206.04673 , 2022. 6
[84] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20k dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 633–641,
2017. 2
12

--- PAGE 13 ---
Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning
Appendix
A. Detailed results of ablation studies
Due to constraints of space, we reported only the sum-
marized tables in the main paper. Here, we present full re-
sults for each experiment.
• Tab. 8: The detailed results of head-to-head compari-
son on VTAB-1k benchmark.
• Tab. 9: The detailed results of head-to-head compari-
son on GLUE benchmark.
• Tab. 10: The detailed results of the optimal position of
Hydra module experiment on ELEV ATER benchmark.
B. Experimental details
B.1. Few-shot experiments
Following [29], we used SGD optimizer to fine-tune
the model. We set ra=rb= 2 for the Hydra module
since the rank of the LoRA in this experiment is set to 4.
We searched for the optimal learning rate and the weight
decay using the automatic hyper-parameter tuning toolkit
from ELEV ATER [46]. Every experiments of few-shot set-
ting on the ELEV ATER benchmark are conducted on a sin-
gle NVIDIA-A100 GPU. We report the hyper-parameters in
Tab. 11.
B.2. VTAB-1k experiments
We used AdamW optimizer with cosine scheduler where
warm-up epoch is set to 10. For the similar number of pa-
rameters compared with methods in benchmark, we set low
ranks ra=rb= 2 in both MSA and MLP blocks. Hydra
is trained with 100 epochs for all datasets. Detailed hyper-
parameters for each dataset are reported in Tab. 12. All of
the results are conducted on a single NVIDIA-A100 GPU.
B.3. Natural language understanding experiments
We used AdamW optimizer with a linear learning rate
decay schedule and set warm-up iteration ratio as 0.06.
Across the datasets, the low ranks are set as ra= 4 and
rb= 8. Following LoRA [33], the adaptation modules
for the MRPC, RTE and STS-B experiments were initial-
ized using the best MNLI experiment checkpoint. The
hyper-parameters specific to each dataset are presented in
Table 13. We used 4 NVIDIA-A100 GPUs for training.C. Dataset details
C.1. Few-shot experiments
We tested Hydra on 20 datasets from ELEV ATER bench-
mark [46] for few-shot experiments: Caltech101, CIFAR10,
CIFAR100, Country211, DTD, EuroSat, FER2013, FGVC
Aircraft, Food101, GTSRB, HatefulMemes, KittiDistance,
MNIST, Flowers102, OxfordPets, PatchCamelyon, SST2,
RESISC45, StanfordCars, and VOC2007. Detailed statis-
tics of each dataset are reported in Tab. 14.
C.2. VTAB-1k experiments
We assessed the accuracy using the VTAB-1k bench-
mark [81], and the statistics of each dataset are presented
in Tab. 15. The benchmark consists of various vision do-
main with three groups: CIFAR100, Caltech101, DTD,
Flowers102, OxfordPets, SVHN and SUN397 in Natural
group, PatchCamelyon, EuroSAT, Resisc45 and Retinopa-
thy in Specialized group, and Clevr, DMLab, dSprites and
SmallNORB in Structured group. The number of train data
is fixed to 1,000 for each dataset.
C.3. Natural language understanding experiments
We evaluated our method on the General Language
Understanding Evaluation (GLUE) benchmark [74]. It
covers various language understanding tasks, including
linguistic acceptability (CoLA [76]), sentiment analysis
(SST-2 [66]), paraphrase (MRPC [16], QQP2), sentence
similarity (STS-B [6]), and natural language inference
(MNLI [77], QNLI [62], RTE [3, 11, 22, 27]).
2data.quora.com/First-Quora-Dataset-Release-Question-Pairs
13

--- PAGE 14 ---
Natural Specialized Structured
Method
#Params(M)
Avg Acc.
CIFAR100
Caltech101
DTD
Flowers102
OxfordPets
SVHN
Sun397
PatchCamelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSPR-Loc
dSPR-Ori
sNORB-Azim
sNORB-Ele
LoRA 0.31 75.6 72.86 90.5 73.2 99.3 91.4 88.4 55.0 85.6 94.4 85.9 74.4 81.5 68.2 49.4 78.6 78.5 49.5 32.7 43.0
SeqLoRA 0.28 75.4 72.4 89.2 71.4 99.0 90.8 87.7 55.2 85.6 94.3 84.9 74.7 82.7 67.7 49.3 79.9 81.0 50.1 32.8 40.3
Hydra 0.28 76.5 72.7 91.3 72.0 99.2 91.4 90.7 55.5 85.8 96.0 86.1 75.9 83.2 68.2 50.9 82.3 80.3 50.8 34.5 43.1
Table 8. The detailed results of head-to-head comparison on VTAB-1k benchmark.
Method #Params(M) Avg. MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
LoRA 0.3 87.2 87.2 94.5 90.2 64.2 92.5 90.7 87.4 91.1
SeqLoRA 0.3 87.4 87.5 94.7 90.7 63.3 93.0 90.8 88.1 91.4
Hydra 0.3 87.9 87.5 95.0 92.2 65.4 92.8 90.8 87.4 91.7
Table 9. The detailed results of head-to-head comparison on GLUE benchmark.
Method
#Params(M)
Avg Acc.( ↑)
Caltech101
CIFAR10
CIFAR100
Country211
DTD
EuroSAT
FER2013
FGVCAircraft
Food101
GTSRB
HatefulMemes
KITTI-Dist
MNIST
Flowers102
OxfordPets
PatchCamelyon
SST2
Resisc45
StanfordCars
VOC2007
MSA 0.20 70.45 92.01 90.76 73.93 17.65 65.05 81.60 51.32 32.00 84.37 85.66 55.90 43.74 91.47 92.56 89.37 68.85 59.80 79.80 69.95 83.16
MLP 0.20 70.95 91.23 90.89 74.20 17.75 64.47 87.00 51.10 33.05 84.27 87.11 55.91 42.05 90.76 93.18 89.38 70.83 59.58 82.41 71.19 82.66
Table 10. The detailed results of the optimal position of Hydra module experiment on ELEV ATER benchmark.
Method
Caltech101
CIFAR10
CIFAR100
Country211
DTD
EuroSAT
FER2013
FGVCAircraft
Food101
GTSRB
HatefulMemes
KITTI-Dist
MNIST
Flowers102
OxfordPets
PatchCamelyon
SST2
Resisc45
StanfordCars
VOC2007
Batch size 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64
Epochs 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
Learning rate 1e-2 1e-3 1e-3 1e-3 1e-2 1e-2 1e-2 1e-2 1e-4 1e-1 1e-4 1e-2 1e-2 1e-2 1e-2 1e-2 1e-4 1e-2 1e-2 1e-1
Weight decay 3.16e-4 1.00e-6 1.00e+0 1.33e-6 1.00e-6 1.00e-6 1.00e-4 3.16e-6 1.00e-4 1.00e-2 1.00e+4 1.00e-6 1.00e-6 1.00e-6 1.00e-6 1.00e+0 1.00e-6 1.00e-4 3.16e-4 3.16e-1
Table 11. The hyper-parameters in few-shot experiments. Since the weight decays for each dataset are searched in the logspace, we report
them using two significant digits in the exponential notation.
14

--- PAGE 15 ---
Hyper-parameter
CIFAR100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSPR-Loc
dSPR-Ori
sNORB-Azim
sNORB-Ele
Batch size 16 32 32 64 32 16 64 64 16 64 64 64 16 64 64 16 64 64 64
Learning rate 5e-4 1e-3 1e-3 5e-3 1e-3 1e-3 1e-3 5e-3 5e-4 5e-3 5e-4 5e-4 5e-4 1e-3 5e-4 1e-3 1e-3 5e-3 5e-4
Weight decay 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-5 1e-4
Dropout 0.2 0.1 0.2 0.0 0.0 0.0 0.2 0.2 0.0 0.2 0.2 0.2 0.0 0.2 0.2 0.2 0.2 0.2 0.2
Table 12. The hyper-parameters in VTAB-1k experiments.
Hyper-parameter MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
Batch size 8 16 16 4 8 16 16 4
Epochs 30 60 40 80 25 25 80 40
Learning rate 4e-4 6e-4 6e-4 4e-4 8e-4 6e-4 8e-4 6e-4
Weight decay 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.1
Table 13. The hyper-parameters in NLU experiments.
15

--- PAGE 16 ---
Dataset #labels Train size Test size
Hateful Memes [39] 2 8,500 500
PatchCamelyon [72] 2 262,144 32,768
Rendered-SST2 [59] 2 6,920 1,821
KITTI Distance [21] 4 6,347 711
FER 2013 [24] 7 28,709 3,589
CIFAR10 [42] 10 50,000 10,000
EuroSAT [30] 10 5,000 5,000
MNIST [14] 10 60,000 10,000
STL10 [10] 10 5,000 8,000
SVHN [23] 10 73,257 26,032
VOC 2007 Classification [19] 20 2,501 4,952
Oxford-IIIT-Pets [57] 37 3,680 3,669
GTSRB [67] 43 26,640 12,630
Resisc45 [8] 45 3,150 25,200
Describable Textures [9] 47 1,880 1,880
CIFAR100 [42] 100 50,000 10,000
FGVC Aircraft [53] 100 3,334 3,333
Food101 [4] 101 75,750 25,250
Caltech101 [20] 101 3,060 6,084
Oxford-Flowers102 [55] 102 1,020 6,149
Stanford Cars [41] 196 8,144 8,041
Country-211 [59] 211 31,650 21,100
SUN397 [79] 397 19,850 19,850
Table 14. Statistics of 23 datasets used in few-shot experiments.
16

--- PAGE 17 ---
Dataset #labels Train size Test size
CIFAR100 [42] 100 10,000
Caltech101 [20] 102 6,084
Describable Textures [9] 47 1,880
Oxford-Flowers102 [55] 102 6,149
Oxford-IIIT-Pets [57] 37 3,669
SVHN [23] 10 26,032
Sun397 [79] 397 36,032
PatchCamelyon [72] 2 32,768
EuroSAT [30] 10 5,400
Resisc45 [8] 45 1,000 6,300
Retinopathy [25] 5 42,670
Clevr/count [36] 8 15,000
Clevr/distance [36] 6 15,000
DMLab [2] 6 22,735
KITTI Distance [21] 4 711
dSprites/location [54] 16 73,728
dSprites/orientataion [54] 16 73,728
SmallNORB/azimuth [43] 18 12,150
SmallNORB/elevation [43] 18 12,150
Table 15. Datasets of VTAB-1k benchmark
17
