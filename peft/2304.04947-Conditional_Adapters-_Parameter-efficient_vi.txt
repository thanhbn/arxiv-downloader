# Conditional Adapters: Học chuyển giao hiệu quả tham số với suy luận nhanh

Tao Lei∗ Junwen Bai Siddhartha Brahma Joshua Ainslie Kenton Lee Yanqi Zhou
Nan Du Vincent Y. Zhao Yuexin Wu Bo Li Yu Zhang Ming-Wei Chang
Google

## Tóm tắt

Chúng tôi đề xuất Conditional Adapter (CODA), một phương pháp học chuyển giao hiệu quả tham số cũng cải thiện hiệu quả suy luận. CODA tổng quát hóa vượt ra ngoài các phương pháp adapter tiêu chuẩn để cho phép một cách mới cân bằng tốc độ và độ chính xác bằng cách sử dụng tính toán có điều kiện. Bắt đầu với một mô hình pretrained dày đặc hiện có, CODA thêm vào kích hoạt thưa thớt cùng với một số lượng nhỏ tham số mới và một giai đoạn huấn luyện nhẹ. Các thí nghiệm của chúng tôi chứng minh rằng phương pháp CODA cung cấp một cách hiệu quả bất ngờ để chuyển giao kiến thức. Trên nhiều tác vụ ngôn ngữ, thị giác và giọng nói khác nhau, CODA đạt được tăng tốc suy luận từ 2x đến 8x so với các phương pháp Adapter tiên tiến với mất mát độ chính xác vừa phải đến không có và cùng hiệu quả tham số.

## 1 Giới thiệu

Các mô hình pretrained lớn đã đạt được kết quả đột phá nhưng trở ngại chính để triển khai chúng là chi phí thích ứng và suy luận. Do kích thước ngày càng tăng của các mô hình pretrained, ví dụ, finetuning đã trở nên ngày càng đắt đỏ vì nó yêu cầu một bản sao riêng của toàn bộ mô hình và cập nhật tất cả tham số cho mỗi tác vụ downstream. Học chuyển giao hiệu quả tham số như Adapter [Houlsby et al., 2019] và Prompt Tuning [Lester et al., 2021] đã được đề xuất để giải quyết vấn đề này. Những phương pháp này chỉ cập nhật một tập con nhỏ tham số cho mỗi tác vụ downstream, cho phép mô hình giữ lại kiến thức và tránh quên nghiêm trọng [Vu et al., 2022]. Đáng chú ý, những phương pháp này có thể phù hợp với độ chính xác của một mô hình được finetuned đầy đủ, trong khi đạt được độ chính xác tốt hơn trên phân phối dữ liệu ngoài miền [Lester et al., 2021, Awadalla et al., 2022].

Thật không may, các phương pháp học chuyển giao hiệu quả tham số tiêu chuẩn chỉ mang lại hiệu quả tham số, không phải hiệu quả suy luận. Ví dụ, trong khi chỉ một vài ma trận chiếu nhỏ được thêm vào mô hình pretrained trong phương pháp Adapter, tất cả đầu vào mô hình (như tokens) vẫn sử dụng tất cả tham số trong quá trình suy luận. Do đó, tốc độ suy luận giống nhau (hoặc thấp hơn một chút) so với phương pháp finetuning đầy đủ. Hơn nữa, các nghiên cứu trước đây đã chỉ ra rằng những phương pháp học hiệu quả tham số này hiệu quả nhất khi kích thước của mô hình pretrained lớn [Lester et al., 2021], khiến nhiều ưu điểm của những phương pháp này khó thực hiện trong thực tế.

Trong bài báo này, chúng tôi đề xuất Conditional Adapter (CODA), một phương pháp học chuyển giao hiệu quả tham số cung cấp cả hiệu quả tham số và suy luận. CODA là một tổng quát hóa của phương pháp adapter, được xây dựng với trực giác sau - chúng ta có thể coi mô hình pretrained như một nguồn kiến thức phổ quát nhưng chỉ truy vấn nó cho các đầu vào cần thiết. Hình 1 so sánh CODA với finetuning và các phương pháp adapter tiêu chuẩn. Tương tự như các phương pháp adapter tiêu chuẩn, mô hình của chúng tôi thêm và cập nhật một adapter nhỏ trong mỗi lớp, trong khi cố định các khối Transformer pretrained cho việc thích ứng downstream. Tuy nhiên, không giống như các phương pháp trước đây, CODA giả định rằng nhiều biểu diễn token đầu vào (của mỗi lớp) không quan trọng cho tác vụ dự đoán và do đó không yêu cầu tính toán nặng. Trong những trường hợp như vậy, khối Transformer pretrained có thể được bỏ qua. Cho rằng nhiều tokens không được xử lý bởi khối Transformer, CODA chạy nhanh hơn đáng kể so với các phương pháp trước đây.

Trong khi kích hoạt có điều kiện có lợi ích tốc độ rõ ràng, CODA phải học chọn các tokens quan trọng cho tính toán nặng để duy trì độ chính xác mô hình. Để đạt được điều này, chúng tôi giới thiệu một phép toán soft top-k để tính toán quyết định lựa chọn token. Phép toán soft top-k này, có thể được xem như một tổng quát hóa của softmax và một nới lỏng của hard top-k, sử dụng các kỹ thuật tối ưu hóa có điều chỉnh entropy tương tự như tối ưu hóa vận chuyển tính toán [Cuturi, 2013]. Kết quả là, đầu ra của nó có thể được tính toán bằng các vòng lặp nhanh và khả vi, cho phép lựa chọn token được tối ưu hóa trực tiếp cho hiệu suất mô hình.

Chúng tôi áp dụng CODA trên các tác vụ nặng về encoder và đánh giá hiệu quả của nó trên ba miền khác nhau - xử lý ngôn ngữ tự nhiên, thị giác máy tính và xử lý giọng nói. Nhìn chung, CODA đạt được tăng tốc suy luận từ 2 đến 8 lần so với phương pháp adapter tiêu chuẩn với mất mát độ chính xác vừa phải đến không có. Bảng 1 trình bày kết quả của chúng tôi bằng cách chọn một trong những tác vụ hoạt động tốt nhất trong mỗi miền. Chúng tôi cũng tiến hành các nghiên cứu ablation toàn diện để phân tích hiệu quả, hiệu suất và khả năng mở rộng của CODA. Ví dụ, chúng tôi phát hiện rằng chỉ với một chút đến không có pretraining router, các mô hình dense pretrained hiện có như T5 [Raffel et al., 2020] có thể được chuyển đổi hiệu quả thành các mô hình CODA để đạt được cả ưu điểm hiệu quả tham số và tốc độ.

## 2 Công trình liên quan

**Các phương pháp học chuyển giao hiệu quả tham số** Do số lượng tham số ngày càng tăng trong các mô hình Transformer pretrained, nhiều phương pháp khác nhau đã được đề xuất cho học chuyển giao với cập nhật tham số tối thiểu. Prompt tuning [Lester et al., 2021] và prefix tuning [Li và Liang, 2021] giới thiệu các token embedding ảo mới có thể được finetuned như tham số mô hình. Các phương pháp Adapter [Houlsby et al., 2019, He et al., 2021] thêm một số lượng nhỏ tham số mới, có thể học được vào mỗi lớp trong khi giữ cố định các tham số pretrained. Một phương pháp phổ biến khác, Low-Rank Adaptation [LoRA; Hu et al., 2021], chèn các ma trận phân rã low-rank có thể học được vào các tham số mô hình pretrained. Ngoài việc yêu cầu chi phí lưu trữ ít hơn, các phương pháp hiệu quả tham số đã được chứng minh là hiệu quả mẫu hơn và đạt được khái quát hóa ngoài miền tốt hơn so với finetuning tiêu chuẩn. CODA là một phương pháp adapter nhưng có thể dễ dàng kết hợp với các phương pháp hiệu quả tham số khác như LoRA để tăng tốc suy luận của chúng.

**Tính toán có điều kiện** Việc phát triển các mô hình được kích hoạt thưa thớt và có điều kiện đã là một lĩnh vực nghiên cứu rất tích cực. Ví dụ, các mô hình Mixture-of-Experts (MoE) [Shazeer et al., 2017] và nhiều tiến bộ gần đây [Du et al., 2022, Fedus et al., 2021] đã được đề xuất để mở rộng quy mô kích thước của các mô hình ngôn ngữ mà không tăng chi phí tính toán. Nhiều công trình gần đây đã khám phá các phương pháp định tuyến token tốt hơn cho các mô hình MoE, ví dụ sử dụng random hashing [Roller et al., 2021], balanced assignment [Lewis et al., 2021] và expert-choosing router [Zhou et al., 2022]. CODA áp dụng tính toán có điều kiện cho cả khối attention và feed-forward của mô hình, trong khi các mô hình MoE chỉ tập trung vào kích hoạt thưa thớt trong các khối feed-forward.

Tương tự như phương pháp của chúng tôi, nhiều phương pháp gần đây khác nhau đã đạt được hiệu quả tính toán bằng cách bỏ qua tính toán trên một tập con của các token đầu vào. Tuy nhiên, cơ chế lựa chọn có thể rất khác nhau, như sử dụng pooling [Nawrot et al., 2022], token merging [Bolya et al., 2023], token pruning [Rao et al., 2021, Yin et al., 2022], learned sigmoid gates [Bapna et al., 2020] và early exiting [Schuster et al., 2022]. Trong khi hầu hết các phương pháp token merging và pruning đã được đề xuất cho các tác vụ thị giác, chúng tôi cho thấy rằng CODA có thể áp dụng cho nhiều miền bao gồm văn bản, thị giác và giọng nói. Ngoài ra, token merging và phương pháp lựa chọn token của chúng tôi được xây dựng với các bias quy nạp và trực giác khác nhau. Token merging tận dụng sự dư thừa trong các token thị giác, trong khi lựa chọn token giả định một đỉnh của tính liên quan token. Tức là, chỉ một vài tokens là cần thiết cho tác vụ dự đoán. Một sự khác biệt lớn khác là CODA định tuyến động và cập nhật biểu diễn token trong mỗi lớp, trong khi nếu một token bị pruned (hoặc merged), nó sẽ không bao giờ được sử dụng lại bởi các lớp tiếp theo. Chúng tôi tin rằng cơ chế định tuyến token của chúng tôi phù hợp hơn cho các ứng dụng văn bản và giọng nói, như trả lời câu hỏi, nơi các tokens khác nhau có thể đóng vai trò quan trọng trong các lớp khác nhau, hoặc cho các truy vấn đầu vào khác nhau.

Cuối cùng, CODA có liên quan chặt chẽ đến một công trình đồng thời, CoLT5 [Ainslie et al., 2023], cũng sử dụng kích hoạt có điều kiện (lựa chọn token) cho hiệu quả suy luận. Trọng tâm của CoLT5 và CODA rất khác nhau. CoLT5 cụ thể điều chỉnh kiến trúc mô hình của nó cho văn bản dài (ví dụ, hơn 16k tokens), ví dụ, bằng cách kết hợp attention cục bộ với routed attention. Các mô hình CoLT5 được pretrained từ đầu và tất cả tham số được finetuned cho các tác vụ downstream. Ngược lại, CODA được khởi tạo và thích ứng trực tiếp từ một mô hình dense pretrained sẵn có, và chúng tôi tối ưu hóa hiệu suất của nó trên học chuyển giao hiệu quả tham số. Những ưu điểm của CODA và CoLT5 có thể được kết hợp cho các ứng dụng văn bản dài.

**Các mô hình Transformer hiệu quả** Nhiều biến thể Transformer hiệu quả đã được đề xuất để tăng tốc tính toán mô hình. Các ví dụ bao gồm tạo ra các biến thể attention nhanh [Wang et al., 2020a, Beltagy et al., 2020, Guo et al., 2022, Hua et al., 2022], tìm kiếm kiến trúc mạng [Press et al., 2019, So et al., 2021, Su et al., 2021] và sử dụng các mô-đun neural không-attention cho hiệu quả [Gulati et al., 2020, Lei, 2021]. CODA sử dụng tính toán có điều kiện như một phương pháp trực giao cho hiệu quả.

**Nén mô hình** Ngoài việc xây dựng các kiến trúc mô hình hiệu quả, các phương pháp nén mô hình như pruning [Han et al., 2016, Zhu và Gupta, 2017, Wang et al., 2020b, Xia et al., 2022] và distillation [Hinton et al., 2015, Kim và Rush, 2016, Turc et al., 2019, Lin et al., 2020] có thể được áp dụng để tăng tốc suy luận mô hình. So với những phương pháp này, CODA giữ lại tất cả tham số mô hình của mô hình lớn pretrained, và do đó tránh được việc retrain một mô hình mới từ đầu hoặc quên kiến thức do loại bỏ tham số. Ngoài ra, CODA có thể được xem như một phiên bản động của layer pruning vì nó có thể kích hoạt các lớp Transformer khác nhau cho mỗi token, và có thể được kết hợp thêm với distillation để giảm mất mát độ chính xác do tính toán có điều kiện.

## 3 Phương pháp

### 3.1 Kiến trúc

Trong suốt phần này và phần thí nghiệm, chúng tôi xây dựng CODA dựa trên parallel adapters [He et al., 2021]. Tuy nhiên, lưu ý rằng phương pháp của chúng tôi có thể được tổng quát hóa cho các loại adapters khác như sequential adapters [Houlsby et al., 2019] và LoRA [Hu et al., 2021]. Chúng tôi trình bày kết quả thí nghiệm bổ sung sử dụng LoRA trong Phụ lục B.3. Hình 2 minh họa kiến trúc của chúng tôi và cho thấy cách CODA tính toán đầu ra của nó bằng cách chỉ chọn một tập con nhỏ của các token đầu vào để truy vấn mô hình pretrained. Khi parallel adapters được sử dụng, CODA giới thiệu một số lượng nhỏ tham số có thể học được trong các nhánh song song, trong khi phần lớn tham số mô hình (liên quan đến các lớp Transformer pretrained) vẫn cố định. Ngoài ra, CODA chỉ gửi k = ⌈n/r⌉ tokens để xử lý nặng. Chúng tôi định nghĩa r > 1 là hệ số giảm, một hằng số (như 4) để kiểm soát việc tiết kiệm tính toán.

Tiếp theo, chúng tôi giới thiệu ngắn gọn ký hiệu của chúng tôi và mô tả tính toán của CODA chi tiết. Chúng tôi sử dụng F() để ký hiệu một mạng neural được tham số hóa và hàm tương ứng được định nghĩa bởi mạng. Ví dụ, một lớp Transformer [Vaswani et al., 2017] bao gồm một lớp con attention F_att() theo sau bởi một lớp con feed forward F_ffn(). Mỗi lớp cũng sử dụng layer normalization [Ba et al., 2016], cụ thể là LN_att() và LN_ffn(), trước khi áp dụng các hàm attention và feed forward. Chúng tôi định nghĩa X ∈ R^(n×d) là đầu vào của một lớp encoder Transformer, trong đó n là số lượng tokens đầu vào và d là kích thước ẩn của mô hình.

Cho đầu vào lớp X, trước tiên chúng tôi áp dụng layer normalization, cụ thể là X_norm = LN_att(X). Đầu vào được chuẩn hóa sẽ được xử lý bởi nhánh adapter và nhánh Transformer có điều kiện. Đầu ra của chúng sau đó được cộng và kết hợp như đầu ra cuối cùng của lớp.

**Nhánh Adapter** Gọi F_adapter() là hàm biến đổi của nhánh adapter. Đầu ra được định nghĩa là:

Z_adapter = F_adapter(X_norm)  (1)

Tương tự như các phương pháp trước đây, F_adapter() được thực hiện bằng cách sử dụng một mạng feed forward với kích thước ẩn nhỏ như 64. Kết quả là, tính toán Z_adapter chỉ tốn một số lượng nhỏ phép toán dấu phẩy động và chi phí của nó thường không đáng kể so với chi phí của nhánh Transformer nặng. Nhánh adapter không chọn tokens có điều kiện. Nói cách khác, F_adapter() được áp dụng cho tất cả tokens đầu vào X ∈ R^(n×d).

**Nhánh có điều kiện** Tính toán của nhánh có điều kiện diễn ra ba bước. Trước tiên, mỗi lớp CODA định nghĩa một hàm router F_router() để chọn k tokens cho nhánh có điều kiện. Hàm router trong mỗi lớp trả về hai đầu ra:

m, P = F_router(X_norm)  (2)

trong đó P ∈ {0,1}^(k×n) là ma trận bao gồm k one-hot vectors chỉ ra việc lựa chọn tokens. Ở đây P[i, j] = 1 khi và chỉ khi token được chọn thứ i là token đầu vào thứ j từ X_norm. m ∈ [0,1]^n là một weight mask trong đó m[j] là trọng số lựa chọn cho token đầu vào thứ j. m[j] = 0 nếu token không được chọn. Chúng tôi sẽ mô tả cách router học được việc lựa chọn chi tiết hơn sau trong phần này.

Sau khi quyết định định tuyến được đưa ra, các biểu diễn đầu vào của các tokens được chọn có thể được thu thập bằng phép nhân ma trận:

X_routed = PX_norm ∈ R^(k×d)  (3)

trong đó k hàng trong X_norm được chọn để xây dựng ma trận k-by-d X_routed. Tương tự như một lớp Transformer tiêu chuẩn, nhánh có điều kiện áp dụng các biến đổi attention và feed forward cho đầu vào được chọn:

Z̄_routed = F_att(X_routed)  (4)
Z_routed = F_ffn(LN_ffn(X_routed + Z̄_routed))  (5)

trong đó Z̄_routed, Z_routed ∈ R^(k×d) ký hiệu đầu ra của mạng attention và mạng feed forward tương ứng.

Chúng tôi xem xét hai biến thể attention khác nhau về cách chúng tính toán key-value vectors. Một biến thể áp dụng attention k-to-k sử dụng X_routed làm cả query vectors và key-value vectors. Biến thể khác áp dụng attention k-to-all sử dụng toàn bộ input vectors X_norm làm attention keys và values. Biến thể k-to-all chạy chậm hơn nhưng đạt được chất lượng cao gần với mô hình đầy đủ. Chúng tôi so sánh hiệu suất của hai biến thể trong Mục 5.

Đầu ra attention và feed-forward Z̄_routed và Z_routed được kết hợp và chiếu ngược về cùng shape với đầu vào ban đầu:

Z_cond = P^T(Z̄_routed + Z_routed) ∈ R^(n×d)  (6)

Cuối cùng Z_cond kết hợp với đầu ra adapter và đầu vào ban đầu của lớp hiện tại để tạo ra đầu ra của lớp:

Y = X + Z_adapter + m ⊙ Z_cond  (7)

m ⊙ Z_cond là một phép nhân theo phần tử mà scale các hàng của Z_cond sử dụng weight m. Phép toán này có thể được xem như một gating operation, trong đó hidden state Z_cond[i] của token thứ i được weight bởi token selection score m[i] được gán bởi router. Điều này cho phép gradient propagation từ m đến các tham số router, sao cho việc lựa chọn token có thể được tối ưu hóa jointly với các thành phần mô hình khác trong quá trình training.

**Learned router** Một thành phần quan trọng của CODA là hàm router F_router() được học để chọn một tập con tokens cho hiệu suất mô hình thuận lợi. Cho biểu diễn token X_norm, router của chúng tôi trước tiên tính toán dot-product score s = w^T X_norm, trong đó w ∈ R^d là một parameter vector liên quan đến router trong lớp này. Dot-product score s được chuẩn hóa thêm bởi một hàm f() : R^n → [0,1]^n, và clipped để tạo ra selection score m:

λ = f(s)  (8)
m = λ ⊙ Top(λ, k) ∈ R^n  (9)

Ở đây Top(λ, k) ∈ {0,1}^n là một indicator function trả về một binary mask chỉ ra top-k values trong λ. Ma trận one-hot P được định nghĩa trong (2) có thể được tạo theo Top(λ, k). Nói ngắn gọn, những giá trị cao nhất của λ sẽ được chọn bởi router.

Hàm f() phải giữ được tính khả vi đối với đầu vào của nó (s trong trường hợp này) sao cho chúng ta có thể cập nhật các tham số router w trong quá trình training. Một lựa chọn có thể cho f() là hàm sigmoid activation mà chuẩn hóa các giá trị trong s một cách độc lập. Tuy nhiên, điều này không mô hình hóa rõ ràng ràng buộc rằng chúng ta cần chọn k tokens từ n tokens có sẵn. Xem xét một trường hợp đơn giản trong đó k = 1, một lựa chọn tự nhiên cho f() sẽ là hàm softmax. Vì softmax cung cấp chuẩn hóa toàn cục trên các input scores, một gradient update để tăng một trong các scores cũng sẽ giảm các scores khác, một hiệu ứng mong muốn để học top-1 selection.

Chúng tôi giả thuyết rằng một soft top-k operator tổng quát hóa softmax nên được sử dụng cho k > 1 tổng quát. Điều này thực sự có thể bằng cách hình thức hóa soft top-k như bài toán tối ưu sau:

f(s) := arg max_λ s^T λ + ϵH(λ)
s.t. 1^T λ = k, λ[i] ∈ [0,1] ∀i = 1, ..., n  (10)

Ở đây H(λ) = Σ_{i=1}^n -λ[i] log λ[i] là một hàm entropy tổng quát (áp dụng cho bất kỳ vector positive λ nào thay vì một phân phối), và ϵ > 0 là một hệ số nhỏ.

Bài toán tối ưu này liên quan chặt chẽ đến softmax và top-k operation. Cụ thể, khi ϵ = 0, nó trở thành một linear program trả về Top(s, k) như nghiệm. Ngoài ra, khi k = 1, có thể chứng minh rằng nghiệm của nó là softmax(s/ϵ). Nói rộng ra, (10) sẽ trả về một soft top-k mask và tính smooth được kiểm soát bởi ϵ (và do đó ϵ phải positive để hoạt động như một temperature).

Bài toán (10) không có nghiệm dạng đóng cho ϵ > 0 và k > 1 tùy ý, nhưng nghiệm của nó có thể được thu thập bằng một thuật toán iterative. Cụ thể, gọi a ∈ R và b ∈ R^n là hai auxiliary variables (có thể được khởi tạo về zeros). Nghiệm có dạng λ = exp((s + b + a)/ϵ). Các giá trị của a và b có thể được thu thập bằng các cập nhật iterative sau:

a' = ϵ ln(k) - ϵ ln(Σ_{i=1}^n exp((s[i] + b[i])/ϵ))
b' = min(-s - a', 0)  (11)

Trong thực tế, chúng tôi sử dụng T = 20 iterations và hàm f(s) trả về exp((s + b + a)/ϵ) sử dụng a và b từ iteration cuối cùng. Hàm f(s) giữ được tính khả vi đối với s sử dụng những cập nhật iterative này, vì vậy chúng ta có thể train router jointly với các tham số mô hình khác. Chúng tôi cung cấp thảo luận bổ sung và đạo hàm của các cập nhật trong Phụ lục §C.

### 3.2 Training

CODA có thể được khởi tạo trực tiếp từ một mô hình Transformer hiện có. Cho một mô hình pretrained như T5 [Raffel et al., 2020], các lớp Transformer được tái sử dụng và sao chép trực tiếp trong các nhánh có điều kiện của CODA, và chỉ các tham số adapter và router được khởi tạo ngẫu nhiên. Vì pretraining một mô hình dense lớn có thể đắt đỏ, phương pháp của chúng tôi giảm chi phí training tổng thể.

Các routers và thành phần mạng neural trong CODA phải hợp tác và được tối ưu hóa cho các dự đoán mô hình chính xác. Khi dữ liệu finetuning có sẵn hạn chế, một khởi tạo ngẫu nhiên cho các tham số router (và adapter) có thể không tối ưu. Chúng tôi chứng minh rằng CODA có thể được pretrain thêm sử dụng cùng objective pretraining như mô hình dense, để tăng cường hiệu suất downstream. Quan trọng là, CODA yêu cầu ít bước training hơn đáng kể trong quá trình pretraining, vì hầu hết tham số của nó được lấy từ một mô hình đã được pretrained. Chúng tôi cho thấy rằng chi phí pretraining CODA có thể thấp hơn 10-30x so với pretraining của mô hình dense ban đầu. Chúng tôi trình bày phân tích này trong Mục 5.

Cuối cùng, chúng tôi train CODA trên các tác vụ downstream bằng cách chỉ cập nhật các tham số adapter, router và layer normalization. Kích thước của các adapters nhỏ (ví dụ 5M tham số), và mỗi khối router và layer normalization chỉ giới thiệu d tham số, trong đó d là chiều mô hình. Kết quả là, CODA vẫn hiệu quả tham số tương tự như các phương pháp adapter và prompt-tuning trước đây.

## 4 Thiết lập thí nghiệm

CODA được đánh giá trên ba miền bao gồm xử lý ngôn ngữ tự nhiên (NLP), thị giác máy tính và xử lý giọng nói, và trên một loạt các ứng dụng như classification, question answering, summarization và speech recognition. Các thí nghiệm được tổ chức như sau: Trước tiên chúng tôi chứng minh hiệu quả của CODA và tiến hành phân tích về các lựa chọn thiết kế của nó sử dụng các mô hình T5 có sẵn công khai (§5). Trong kết quả cuối cùng của chúng tôi (§6), chúng tôi pretrain các mô hình Transformer từ đầu và mở rộng đánh giá của chúng tôi sang các miền thị giác và giọng nói.

**Datasets** Chúng tôi sử dụng corpus C4 [Raffel et al., 2020] để pretrain các mô hình text. Đối với các mô hình speech, chúng tôi sử dụng corpus LibriLight [Kahn et al., 2020] để pretraining. Các mô hình vision Transformer của chúng tôi sử dụng cùng dữ liệu và quy trình training trong Pix2Struct [Lee et al., 2022]. Các dataset finetuning của chúng tôi cho các mô hình text bao gồm các dataset MNLI [Williams et al., 2018], RTE [Dagan et al., 2005, Haim et al., 2006, Giampiccolo et al., 2007, Bentivogli et al., 2009], BoolQ [Clark et al., 2019], SQuAD [Rajpurkar et al., 2016] và XSum [Narayan et al., 2018]. Các mô hình speech được đánh giá trên tác vụ speech recognition sử dụng dataset LibriSpeech [Panayotov et al., 2015]. Cuối cùng, chúng tôi sử dụng các dataset OCR-VQA [Mishra et al., 2019], DocVQA [Mathew et al., 2021], và Screen2Words [Wang et al., 2021] cho các mô hình vision.

## 5 Hiểu và Phân tích CODA

**Thiết lập** Chúng tôi trình bày một số phân tích để xác thực các lựa chọn thiết kế của CODA trong phần này. Chúng tôi khởi tạo CODA sử dụng phiên bản 1.1 release của các checkpoint T5, và thực hiện pretraining CODA sử dụng cùng setting như các mô hình T5. Trong quá trình pretraining, chúng tôi đặt routing capacity thành k = 192 cho input sequence length n = 512. Chúng tôi không tune giá trị của k cho pretraining, nhưng sẽ báo cáo kết quả sử dụng các giá trị k khác nhau trong finetuning. Chúng tôi thực hiện 100K gradient steps, tức là 10% tổng số steps được sử dụng để train các mô hình T5 dense. Chi phí tính toán tổng thể thấp hơn 20x so với full training của các mô hình dense, vì CODA chỉ áp dụng tính toán nặng trên ít hơn một nửa số tokens.

Để đơn giản, chúng tôi đánh giá trên các tác vụ classification cho nhiều nghiên cứu ablation khác nhau của CODA. Cụ thể, chúng tôi báo cáo kết quả trên các dataset MNLI, RTE và BoolQ, và test ba kích thước mô hình khác nhau bao gồm kích thước Base, Large và XL của T5. Chúng tôi sẽ mở rộng đánh giá của chúng tôi sang các tác vụ generation như question answering trong phần kết quả đầy đủ (§6).

**CODA có thể nhanh và chính xác không?** Bảng 2 trình bày kết quả finetuning của CODA. Để so sánh, chúng tôi cũng báo cáo kết quả của Parallel Adapter, tương tự như CODA ngoại trừ việc nó áp dụng các lớp Transformer đắt đỏ cho tất cả input tokens. Điều này tạo thành một upper-bound, và là một baseline mạnh đã được báo cáo là tốt nhất trong một loạt các phương pháp adapter và prompt tuning [He et al., 2021]. Như được thể hiện trong Bảng 2, CODA có thể đạt được giảm tính toán 3-5x (r = 3,5) trong các lớp Transformer với chi phí ít hơn 1.0 điểm giảm độ chính xác trung bình. Như mong đợi, biến thể k-to-all attention của chúng tôi đạt được độ chính xác tốt hơn một cách nhất quán so với biến thể k-to-k, vì nó có thể truy cập toàn bộ context attention. Mặt khác, biến thể k-to-k attention chạy nhanh hơn trong thực tế, có thể có lợi cho các tác vụ với input rất dài. Chúng tôi chọn phiên bản k-to-all trong phần kết quả cuối cùng (§6).

**Cần bao nhiêu bước pretraining?** Hình 3 vẽ độ chính xác finetuning bằng cách thay đổi số bước pretraining cho CODA. Vì CODA có thể được khởi tạo sử dụng các mô hình dense pretrained, nó yêu cầu chỉ 20K steps để có được kết quả finetuning cạnh tranh. Tất nhiên, sử dụng nhiều bước pretraining hơn có thể cải thiện độ chính xác downstream. Thực tế là CODA có thể được cập nhật nhanh chóng mà không lặp lại pretraining đắt đỏ sẽ rất có lợi trong các ứng dụng thực tế.

**Learned routing có quan trọng không?** Chúng tôi phân tích tác động của learned routing trong Bảng 3 bằng cách so sánh soft top-k router của chúng tôi với các triển khai router khác. Chúng tôi triển khai một biến thể thay thế soft top-k bằng hàm sigmoid activation, vì vậy selection weight của mỗi token kích hoạt riêng (mà không xem xét ràng buộc capacity). Như được thể hiện trong bảng, biến thể này đạt được độ chính xác tệ hơn trên hầu hết tất cả các tác vụ và kích thước mô hình được test, tệ hơn 2.0 điểm trung bình. Chúng tôi cũng triển khai một baseline "no-learning" chỉ đơn giản chọn k tokens đầu tiên, tương đương với việc truncating input sequence. Baseline này hoạt động tệ hơn nhiều, dẫn đến giảm độ chính xác hơn 10 điểm cho k nhỏ (và r lớn tương đương). Phân tích này xác nhận tầm quan trọng của việc học một routing tốt để giữ lại hiệu suất mô hình mạnh.

## 6 Kết quả đầy đủ

**Thiết lập** Trong phần này, chúng tôi áp dụng recipe training tốt nhất của chúng tôi cho tất cả các tác vụ và miền ứng dụng. Trước tiên chúng tôi pretrain các mô hình Transformer dense, theo sau bởi quy trình training CODA trong §3.2. Các mô hình speech của chúng tôi được pretrained sử dụng objective masked language modeling (MLM) tương tự như BERT [Devlin et al., 2019], và random quantized output label space [Chiu et al., 2022]. Các mô hình vision và text của chúng tôi sử dụng kiến trúc encoder-decoder tương tự như T5 nhưng kết hợp một vài thay đổi. Theo PaLM [Chowdhery et al., 2022], chúng tôi sử dụng multi-query attention [Shazeer, 2019] chia sẻ cùng key và value projection cho nhiều query heads. Chúng tôi chỉ sử dụng 6 decoder layers và tăng feed forward hidden size (để bù đắp cho việc giảm số layers). Những sửa đổi này có hiệu ứng trung tính trên chất lượng mô hình, nhưng tăng tốc auto-regressive decoding đáng kể. Chúng tôi sẽ cho thấy CODA tương thích với những thay đổi này và có thể tăng tốc suy luận thêm một hệ số lớn đáng kể. Chúng tôi cung cấp chi tiết hơn về thiết lập thí nghiệm của chúng tôi trong Phụ lục A.

**Kết quả NLP** Ngoài các dataset classification được sử dụng trong Mục 5, chúng tôi cũng đánh giá các mô hình cuối cùng của chúng tôi trên các dataset SQuAD, ReCord và XSum yêu cầu tạo ra một câu trả lời hoặc tóm tắt cho input. Bảng 4 chứa kết quả finetuning của các mô hình XL. So với baseline parallel adapter sử dụng full computation, CODA đạt được giảm tính toán 3x và 5x với chỉ 1.0 và 1.7 điểm mất trong điểm trung bình.

Hình 4 và 5 làm nổi bật xu hướng scaling của CODA. CODA chạy nhanh hơn nhiều với chất lượng hơi tệ hơn so với baseline parallel adapter. Điều này được mong đợi vì baseline xử lý tất cả tokens trong mọi lớp, trong khi CODA chỉ chọn 1/r tokens để xử lý nặng. Quan trọng là, khoảng cách chất lượng này giảm khi kích thước mô hình tăng (như được thể hiện trong Hình 4), khiến CODA trở thành lựa chọn hiệu quả tính toán cho các mô hình lớn. Thực sự, CODA có thể đánh đổi chất lượng cho tốc độ bằng cách thay đổi số lượng tokens được chọn. Hình 5 (trái) chứng minh rằng CODA đạt được đánh đổi tốc độ-chất lượng mạnh hơn nhiều so với các mô hình dense không có tính toán có điều kiện. Đường màu đen chỉ ra kết quả của Parallel Adapter khi kích thước mô hình tăng từ Small đến XL, và mỗi đường màu xanh đại diện cho đánh đổi tốc độ-chất lượng của CODA sử dụng r = 1,3,5. Hơn nữa, Hình 5 (giữa) cho thấy các mô hình CODA lớn hơn thể hiện tăng tốc suy luận cao hơn. Những quan sát này nhất quán trên các tác vụ khác. Chúng tôi cung cấp kết quả bổ sung trong Phụ lục §B.

**Kết quả speech recognition** Chúng tôi xác thực thêm hiệu suất của CODA trong miền speech. Mô hình của chúng tôi sử dụng một Transformer encoder và một 2-layer LSTM Transducer [Graves, 2012]. Tương tự như thiết lập NLP, chúng tôi test hiệu suất của mô hình speech trên 3 quy mô – Base, Large và XL (xem Phụ lục A cho chi tiết). Bảng 5 chứng minh rằng với tỷ lệ giảm đáng kể (r = 2,4), thay đổi trên word error rate (WER) nhất quán tối thiểu trên test-clean và test-other sets của LibriSpeech qua các kích thước mô hình khác nhau (và trên các sets khác trong §B.2). Hơn nữa, kết quả của chúng tôi có thể so sánh với các mô hình hoạt động hàng đầu, như w2v-BERT [Chung et al., 2021] và BEST-RQ [Chiu et al., 2022], được finetuned đầy đủ bằng cách cập nhật tất cả tham số. Hình 5 (phải) làm nổi bật lần nữa rằng áp dụng tính toán có điều kiện dẫn đến đánh đổi tốc độ-chất lượng tốt hơn so với các mô hình dense.

**Kết quả vision** Chúng tôi mở rộng thí nghiệm của chúng tôi sang các tác vụ visual liên quan đến ngôn ngữ tự nhiên trong hình ảnh, như documents và user interfaces. Các thí nghiệm của chúng tôi dựa trên Pix2Struct [Lee et al., 2022], trong đó một image-encoder-text-decoder được pretrained bằng cách học dự đoán HTML đơn giản hóa từ các screenshot trang web. Bảng 6 cho thấy kết quả trên ba tác vụ cũng được đánh giá trong bài báo Pix2Struct gốc. Trong OCRVQA và Screen2Words, chúng tôi quan sát giảm hiệu suất tương đối nhỏ khi giảm số lượng routed tokens (tức là patches). Khi capacity là 1/16 của độ dài sequence ban đầu, dẫn đến khoảng 13× speedup, chúng tôi chỉ mất khoảng 1 điểm. Chúng tôi suy đoán rằng điều này do tính thưa thớt cao cấp trong inputs cho hai tác vụ này. Đối với DocVQA, nơi có tương đối nhiều thông tin textual hơn, chúng tôi quan sát một đánh đổi hiệu suất-tốc độ dốc hơn nhưng vẫn đạt được 8× speedup với giảm 4 điểm.

Để cung cấp hiểu biết trực quan hơn về tại sao CODA hoạt động, chúng tôi visualize hành vi router cho mô hình OCR-VQA trong Hình 6. Chúng tôi cho thấy patches nào mà routers ưa thích nhất (màu ấm nhất) và ít nhất (màu lạnh nhất), cho một số lớp. Quan sát đầu tiên, ngay lập tức rõ ràng, là router tránh các patches low-frequency, tức là patches có khả năng là "whitespace", vì chúng có thể được xử lý đầy đủ bởi các lớp adapter rẻ. Quan sát thứ hai, tinh tế hơn, là router dần dần hội tụ trên một số lượng nhỏ các key patches mà chúng tôi giả thuyết phục vụ như biểu diễn cho các vùng lớn hơn. Visualization xác nhận rằng CODA có thể chọn các patches có ý nghĩa và đại diện hữu ích cho tác vụ dự đoán.

## 7 Kết luận và Hạn chế

Chúng tôi trình bày CODA, một phương pháp adapter hiệu quả tham số cho phép suy luận nhanh. CODA dựa vào tính toán có điều kiện để chọn lọc kích hoạt tính toán mô hình trên các đơn vị input quan trọng, cung cấp một cách mới để cân bằng tính biểu đạt và hiệu quả của mô hình.

Trong công việc này, chúng tôi tập trung vào các ứng dụng nặng về encoder như summarization, speech recognition và visual question answering, bằng cách áp dụng phương pháp của chúng tôi vào encoder. Một hạn chế của CODA là cơ chế routing hiện tại (tức là lựa chọn token trong một sequence cho trước) không trực tiếp áp dụng được cho các mô hình decoder-only cho token generation tự hồi quy. Việc cho phép token generation nhanh sử dụng kích hoạt có điều kiện trong các lớp decoder là một hướng thú vị chúng tôi dự định khám phá trong công việc tương lai.

## 8 Lời cảm ơn

Chúng tôi muốn cảm ơn Rama Pasumarthi, Hongkun Yu, Kelvin Guu, Zhuyun Dai, Timothy Dozat, Raphael Hoffmann, Tao Wang, Tal Schuster, Ziwei Ji, Frederick Liu và Slav Petrov vì lời khuyên và thảo luận hữu ích.
