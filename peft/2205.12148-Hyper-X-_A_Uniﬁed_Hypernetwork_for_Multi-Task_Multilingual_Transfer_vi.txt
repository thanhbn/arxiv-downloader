# Hyper-X: Một Siêu Mạng Thống Nhất cho Chuyển Giao Đa Nhiệm Đa Ngôn Ngữ

Ahmet Üstün1, Arianna Bisazza1, Gosse Bouma1,
Gertjan van Noord1, Sebastian Ruder2
1Đại học Groningen
2Google Research
a.ustun@rug.nl

## Tóm tắt

Các mô hình đa ngôn ngữ quy mô lớn có triển vọng cho việc học chuyển giao qua các nhiệm vụ và ngôn ngữ. Tuy nhiên, các phương pháp hiện có không thể tận dụng đầy đủ dữ liệu huấn luyện khi nó có sẵn trong các kết hợp nhiệm vụ-ngôn ngữ khác nhau. Để khai thác sự giám sát không đồng nhất như vậy, chúng tôi đề xuất Hyper-X, một siêu mạng duy nhất thống nhất học đa nhiệm vụ và đa ngôn ngữ với khả năng thích ứng hiệu quả. Mô hình này tạo ra trọng số cho các mô-đun adapter được điều kiện hóa trên cả embedding nhiệm vụ và ngôn ngữ. Bằng cách học cách kết hợp kiến thức đặc thù của nhiệm vụ và ngôn ngữ, mô hình của chúng tôi cho phép chuyển giao zero-shot cho các ngôn ngữ và kết hợp nhiệm vụ-ngôn ngữ chưa từng thấy. Các thí nghiệm của chúng tôi trên một tập hợp đa dạng các ngôn ngữ chứng minh rằng Hyper-X đạt được hiệu quả tốt nhất hoặc cạnh tranh khi có sẵn hỗn hợp nhiều tài nguyên, trong khi ngang bằng với các baseline mạnh trong kịch bản tiêu chuẩn. Hyper-X cũng hiệu quả hơn đáng kể về tham số và tài nguyên so với các phương pháp huấn luyện adapter riêng biệt. Cuối cùng, Hyper-X liên tục tạo ra kết quả mạnh trong các kịch bản few-shot cho các ngôn ngữ mới, cho thấy tính linh hoạt của phương pháp của chúng tôi vượt ra ngoài chuyển giao zero-shot.

## 1 Giới thiệu

Học chuyển giao qua các ngôn ngữ và nhiệm vụ từ lâu đã là một trọng tâm quan trọng trong NLP (Ruder et al., 2019). Những tiến bộ gần đây trong các transformer đa ngôn ngữ quy mô lớn (MMT; Devlin et al., 2019; Conneau et al., 2020) cho thấy thành công lớn trong lĩnh vực này. Một lợi ích của các mô hình như vậy là khả năng chuyển giao thông tin đặc thù nhiệm vụ từ một ngôn ngữ nguồn có tài nguyên cao sang một ngôn ngữ đích có tài nguyên thấp (Hình 1, 1). Ngoài ra, các mô hình như vậy có thể tận dụng kiến thức từ nhiều nhiệm vụ để có khả năng tổng quát hóa mạnh hơn (Hình 1, 2).

Theo thời gian, nhiều cộng đồng nghiên cứu đã phát triển tài nguyên cho các ngôn ngữ cụ thể mà họ tập trung (Strassel và Tracey, 2016; Nivre et al., 2018; Wilie et al., 2020). Trong thực tế, do đó thường có dữ liệu có sẵn cho các nhiệm vụ khác nhau trong hỗn hợp các ngôn ngữ khác nhau. Ví dụ, ngoài dữ liệu tiếng Anh cho cả gán thẻ POS và Nhận dạng Thực thể Có tên (NER), một treebank với chú thích POS có thể có sẵn cho tiếng Thổ Nhĩ Kỳ, trong khi dữ liệu NER có thể có sẵn cho tiếng Ả Rập. Ví dụ này được minh họa trong Hình 1, 3.

Trái ngược với các paradigm chuyển giao liên ngôn ngữ hiện có như chuyển giao zero-shot một nhiệm vụ duy nhất (Hu et al., 2020) hoặc học few-shot (Lauscher et al., 2020a), học đa nhiệm vụ trên hỗn hợp các tập dữ liệu như vậy (đa nhiệm vụ hỗn hợp ngôn ngữ) tạo ra cơ hội để tận dụng tất cả dữ liệu có sẵn và chuyển giao thông tin qua cả nhiệm vụ và ngôn ngữ đến các kết hợp nhiệm vụ-ngôn ngữ chưa từng thấy (Ponti et al., 2021).

Tuy nhiên, các chiến lược tinh chỉnh tiêu chuẩn bị hạn chế trong khả năng tận dụng dữ liệu nhiệm vụ và ngôn ngữ không đồng nhất như vậy. Cụ thể, các MMT dễ bị ảnh hưởng bởi quên thảm khốc và can thiệp (Wang et al., 2020) khi chúng được tinh chỉnh trên nhiều nguồn. Adapter (Houlsby et al., 2019), một phương án tinh chỉnh hiệu quả về tham số được sử dụng phổ biến để chuyển giao qua các nhiệm vụ (Mahabadi et al., 2021b) hoặc ngôn ngữ (Üstün et al., 2020) nhưng yêu cầu huấn luyện một adapter mới cho mỗi ngôn ngữ mới (Pfeiffer et al., 2020b).

Trong bài báo này, chúng tôi đề xuất một siêu mạng thống nhất, HYPER-X đặc biệt phù hợp với cài đặt này bằng cách tận dụng nhiều nguồn thông tin bao gồm các ngôn ngữ và nhiệm vụ khác nhau trong một mô hình duy nhất. Ý tưởng cốt lõi bao gồm việc lấy embedding ngôn ngữ và nhiệm vụ làm đầu vào, và tạo ra các tham số adapter thông qua một siêu mạng cho kết hợp nhiệm vụ-ngôn ngữ tương ứng. Bằng cách tham số hóa từng nhiệm vụ và ngôn ngữ riêng biệt, Hyper-X cho phép thích ứng với các kết hợp chưa từng thấy tại thời điểm kiểm tra trong khi khai thác tất cả các tài nguyên dữ liệu có sẵn.

Ngoài ra, Hyper-X có thể sử dụng liền mạch mô hình hóa ngôn ngữ có mặt nạ (MLM) trên dữ liệu không được gán nhãn, điều này cho phép nó thực hiện thích ứng zero-shot cho các ngôn ngữ không được bao phủ bởi MMT trong quá trình tiền huấn luyện. MLM cũng cho phép Hyper-X học biểu diễn ngôn ngữ ngay cả khi không có dữ liệu đặc thù nhiệm vụ.

Tóm lại, công trình của chúng tôi tập hợp một số "thành phần" chuyển giao thành công đã được khám phá trong tài liệu gần đây (xem Bảng 1), cụ thể là học đa nhiệm vụ, học đa ngôn ngữ, tiền huấn luyện thêm, cùng với mức độ cao về hiệu quả tính toán và thời gian.

Chúng tôi đánh giá Hyper-X cho chuyển giao liên ngôn ngữ trên hai nhiệm vụ gán nhãn chuỗi, cụ thể là gán thẻ từ loại (POS) và nhận dạng thực thể có tên (NER) trong 16 ngôn ngữ—7 trong số đó không được bao phủ trong tiền huấn luyện—qua ba thiết lập thí nghiệm được mô tả trong Hình 1. Các thí nghiệm của chúng tôi chứng minh rằng Hyper-X ngang bằng với các baseline mạnh cho chuyển giao liên ngôn ngữ từ tiếng Anh. Trong các cài đặt đa nhiệm vụ và hỗn hợp ngôn ngữ, Hyper-X cho thấy cải thiện lớn so với các baseline tiêu chuẩn và đạt được hiệu suất tương đương với mô hình dựa trên adapter kém hiệu quả hơn do khả năng tận dụng các nguồn giám sát không đồng nhất. Phân tích làm nổi bật rằng Hyper-X vượt trội về sự cân bằng hiệu quả-hiệu suất. Cuối cùng, chúng tôi đánh giá mô hình của chúng tôi trong cài đặt few-shot, nơi Hyper-X liên tục đạt được hiệu suất cạnh tranh qua các ngôn ngữ và nhiệm vụ khác nhau, điều này gợi ý khả năng sử dụng của phương pháp chúng tôi trong các kịch bản học liên tục.

## 2 Bối cảnh

### 2.1 Adapter

Adapter (Rebuffi et al., 2018) là các lớp bottleneck nhẹ được chèn vào một MMT để tinh chỉnh mô hình cho một nhiệm vụ mới (Houlsby et al., 2019), ngôn ngữ (Pfeiffer et al., 2020b) hoặc miền (Bapna và Firat, 2019). Các trọng số tiền huấn luyện của transformer vẫn được cố định và chỉ các tham số adapter được cập nhật. Thiết lập này ngăn chặn quên thảm khốc (McCloskey và Cohen, 1989) bằng cách đóng gói kiến thức chuyên biệt.

Một cách chính thức, mô-đun adapter Ai tại lớp i bao gồm một phép chiếu xuống Di ∈ R^(h×b) của đầu vào zi ∈ R^h với chiều bottleneck b, một hàm phi tuyến (ReLU) và một phép chiếu lên Ui ∈ R^(b×h):

Ai(zi) = Ui · ReLU(Di · zi) + zi (1)

trong đó mạng feed-forward này được theo sau bởi một liên kết dư nối đến đầu vào zi.

### 2.2 Siêu mạng

Một siêu mạng là một mạng tạo ra các trọng số cho một mạng chính lớn hơn (Ha et al., 2016). Khi sử dụng siêu mạng, mô hình chính học mục tiêu mong muốn (ví dụ: phân loại) trong khi siêu mạng nhận một đầu vào phụ trợ (thường là một embedding) đại diện cho cấu trúc của các trọng số và tạo ra các tham số của mô hình chính. Do đó, siêu mạng cho phép học một không gian tham số duy nhất được chia sẻ qua nhiều chiều chuyển giao như nhiệm vụ (Mahabadi et al., 2021b) hoặc ngôn ngữ (Platanios et al., 2018) trong khi cũng cho phép tái tham số hóa đặc thù đầu vào.

Cụ thể hơn, siêu mạng là một hàm sinh H nhận một embedding s^(h) ∈ R^ds đại diện cho các nguồn đầu vào, và tạo ra các tham số mô hình θ:

θ ← H(s^(h)) (2)

Trong khi H có thể là bất kỳ hàm khả vi nào, nó thường được tham số hóa như một phép biến đổi tuyến tính đơn giản (Wh) tạo ra một vector phẳng với chiều da, tương ứng với tổng số tham số mô hình. Wh được chia sẻ qua tất cả các nguồn đầu vào, cho phép chia sẻ tối đa.

## 3 Hyper-X

Chúng tôi đề xuất Hyper-X, một sự thích ứng hiệu quả của MMT bằng cách khai thác nhiều nguồn thông tin để chuyển giao đến ngôn ngữ chưa từng thấy hoặc các cặp nhiệm vụ-ngôn ngữ. Cụ thể, Hyper-X học cách kết hợp kiến thức đặc thù nhiệm vụ và ngôn ngữ dưới dạng embedding sử dụng siêu mạng. Được điều kiện hóa trên embedding nhiệm vụ và ngôn ngữ, siêu mạng tạo ra các lớp adapter tổng hợp cho kết hợp nhiệm vụ-ngôn ngữ tương ứng (ví dụ: NER trong tiếng Thổ Nhĩ Kỳ), do đó cho phép chuyển giao đến các cặp nhiệm vụ-ngôn ngữ tùy ý tại thời điểm kiểm tra. Hình 2 cung cấp tổng quan về mô hình của chúng tôi.

Bằng cách học chung từ thông tin nhiệm vụ và ngôn ngữ, Hyper-X vượt qua một số hạn chế của công trình trước đây: Không giống như các phương pháp dựa trên adapter (Pfeiffer et al., 2020b; Üstün et al., 2020) chỉ chuyển giao thông tin liên ngôn ngữ đến nhiệm vụ của task adapter, mô hình của chúng tôi có khả năng tận dụng giám sát—và chuyển giao tích cực—từ cả nhiều nhiệm vụ và ngôn ngữ. Hơn nữa, không giống như Ponti et al. (2021) yêu cầu dữ liệu có chú thích trong một trong các nhiệm vụ đích cho mỗi ngôn ngữ, Hyper-X có thể thực hiện chuyển giao zero-shot ngay cả khi không có dữ liệu có chú thích từ bất kỳ nhiệm vụ đích nào, bằng cách sử dụng MLM như một nhiệm vụ phụ trợ cho mỗi ngôn ngữ.

### 3.1 Một siêu mạng cho adapter nhiệm vụ-ngôn ngữ

Chúng tôi sử dụng siêu mạng tiêu chuẩn như hàm sinh tham số. Tuy nhiên, thay vì tạo ra toàn bộ tham số mô hình, siêu mạng của chúng tôi tạo ra các tham số cho từng lớp adapter. Cụ thể, siêu mạng H tạo ra các tham số adapter trong đó mỗi lớp adapter Ai bao gồm ma trận chiếu xuống và lên (Di, Ui):

Di, Ui ← H(s^(h)) (3)

**Tách biệt nhiệm vụ và ngôn ngữ** Trong Hyper-X, chúng tôi điều kiện hóa việc tạo tham số trên nhiệm vụ và ngôn ngữ đầu vào. Do đó, cho một kết hợp nhiệm vụ t ∈ {t1, ..., tm} và ngôn ngữ l ∈ {l1, ..., ln}, embedding nguồn chứa kiến thức từ cả hai nguồn: s^(h)(t, l). Chúng tôi tham số hóa từng nhiệm vụ và ngôn ngữ thông qua các embedding riêng biệt, điều này cho phép thích ứng với bất kỳ kết hợp nhiệm vụ-ngôn ngữ nào. Embedding nhiệm vụ và ngôn ngữ (s^(t), s^(l)) là các vector chiều thấp được học cùng với các tham số của siêu mạng. Trong quá trình huấn luyện, cho mỗi mini-batch, chúng tôi cập nhật các embedding này theo nhiệm vụ và ngôn ngữ mà mini-batch được lấy mẫu từ đó.

**MLM như nhiệm vụ phụ trợ** Hyper-X học các embedding nhiệm vụ và ngôn ngữ riêng biệt—miễn là nhiệm vụ và ngôn ngữ đã được nhìn thấy trong quá trình huấn luyện. Vì dữ liệu có chú thích trong nhiều ngôn ngữ ít đại diện bị hạn chế, chúng tôi sử dụng MLM như một nhiệm vụ phụ trợ trong quá trình huấn luyện để cho phép tính toán embedding cho mọi ngôn ngữ. Hơn nữa, MLM cho phép hiệu suất zero-shot tốt hơn cho các ngôn ngữ không được bao gồm trong tiền huấn luyện MMT (xem § 6.2 để phân tích chi tiết về tác động của MLM).

**Chia sẻ qua các lớp** Ngoài embedding nhiệm vụ và ngôn ngữ, chúng tôi học một embedding lớp s^(i) (Mahabadi et al., 2021b; Ansell et al., 2021) tương ứng với chỉ số lớp transformer i nơi mô-đun adapter tương ứng được cắm vào. Vì Hyper-X tạo ra một adapter cho mỗi lớp Transformer, việc học các embedding lớp độc lập cho phép chia sẻ thông tin qua các lớp đó. Hơn nữa, vì embedding lớp cho phép sử dụng một siêu mạng duy nhất cho tất cả các lớp Transformer, chúng giảm các tham số có thể huấn luyện, tức là kích thước của siêu mạng, theo một hệ số tương ứng với số lớp của mô hình chính.

**Kết hợp nhiều nguồn** Để kết hợp embedding ngôn ngữ, nhiệm vụ và lớp, chúng tôi sử dụng một mạng chiếu nguồn Ps đơn giản như một phần của siêu mạng của chúng tôi. Mô-đun này bao gồm hai lớp feed-forward với kích hoạt ReLU nhận phép nối của ba embedding và học một embedding kết hợp s^(p) ∈ R^dp với chiều có thể nhỏ hơn:

s^(h) = s^(l) ⊕ s^(t) ⊕ s^(i) (4)
s^(p) = Ps(s^(h)) (5)

trong đó s^(h) ∈ R^ds đề cập đến embedding được nối trước Ps, với ds = dl + dt + di. Thành phần này cho phép học cách kết hợp embedding nguồn trong khi cũng giảm tổng số tham số có thể huấn luyện.

## 4 Thí nghiệm

**Tập dữ liệu và ngôn ngữ** Chúng tôi tiến hành thí nghiệm trên hai nhiệm vụ downstream: gán thẻ từ loại (POS) và nhận dạng thực thể có tên (NER). Đối với gán thẻ POS, chúng tôi sử dụng tập dữ liệu Universal Dependencies (UD) 2.7 (Zeman et al., 2020) và đối với NER, chúng tôi sử dụng WikiANN (Pan et al., 2017) với các phần train, dev và test từ Rahimi et al. (2019). Ngoài hai nhiệm vụ này, chúng tôi cũng sử dụng mô hình hóa ngôn ngữ có mặt nạ (MLM) trên các bài báo Wikipedia như một nhiệm vụ phụ trợ. Chúng tôi giới hạn số câu từ Wikipedia xuống 100K cho mỗi ngôn ngữ, để kiểm soát tác động của kích thước tập dữ liệu và giảm thời gian huấn luyện.

Đối với việc lựa chọn ngôn ngữ, chúng tôi xem xét: (i) sự đa dạng loại hình dựa trên họ ngôn ngữ, chữ viết và thuộc tính hình thái cú pháp; (ii) sự kết hợp của các ngôn ngữ có tài nguyên cao và tài nguyên thấp dựa trên dữ liệu có sẵn trong nhiệm vụ downstream; (iii) sự hiện diện trong dữ liệu tiền huấn luyện của mBERT; và (iv) sự hiện diện của một ngôn ngữ trong hai tập dữ liệu đặc thù nhiệm vụ. Chúng tôi cung cấp chi tiết về việc lựa chọn ngôn ngữ và tập dữ liệu trong Phụ lục A.

**Thiết lập thí nghiệm** Chúng tôi đánh giá Hyper-X cho chuyển giao zero-shot trong ba cài đặt khác nhau: (1) Đơn nhiệm vụ tiếng Anh, nơi chúng tôi huấn luyện các mô hình chỉ trên dữ liệu tiếng Anh cho từng nhiệm vụ downstream riêng biệt. (2) Đa nhiệm vụ tiếng Anh, nơi các mô hình được huấn luyện trên dữ liệu POS và NER tiếng Anh cùng lúc. (3) Đa nhiệm vụ hỗn hợp ngôn ngữ, nơi chúng tôi huấn luyện các mô hình trong thiết lập đa nhiệm vụ, nhưng thay vì chỉ sử dụng dữ liệu tiếng Anh cho cả POS và NER, chúng tôi sử dụng hỗn hợp các kết hợp nhiệm vụ-ngôn ngữ. Để đo hiệu suất zero-shot trong thiết lập này, theo Ponti et al. (2021), chúng tôi tạo ra hai phân vùng khác nhau từ tất cả các kết hợp nhiệm vụ-ngôn ngữ có thể sao cho một cặp nhiệm vụ-ngôn ngữ luôn chưa từng thấy đối với một trong các phân vùng (ví dụ: NER-Turkish và POS-Arabic trong Hình 1). Chi tiết về các phân vùng và chiến lược phân vùng của chúng tôi được đưa ra trong Phụ lục A.

### 4.1 Baseline và biến thể mô hình

**mBERT** (Devlin et al., 2019) là một MMT được tiền huấn luyện cho 104 ngôn ngữ. Chúng tôi sử dụng mBERT bằng cách tinh chỉnh tất cả các tham số mô hình trên các nguồn có sẵn. Vì phương pháp tiêu chuẩn này cho phép chuyển giao liên ngôn ngữ từ cả một nguồn duy nhất hoặc một tập hợp các kết hợp nhiệm vụ-ngôn ngữ, chúng tôi so sánh nó với Hyper-X trong cả ba cài đặt. Hơn nữa, chúng tôi sử dụng mBERT như mô hình cơ sở cho cả Hyper-X và các baseline khác.

**MAD-X** (Pfeiffer et al., 2020b) là một framework mô-đun dựa trên adapter cho học chuyển giao liên ngôn ngữ dựa trên MMT. Nó kết hợp adapter đặc thù nhiệm vụ với các adapter đặc thù ngôn ngữ được huấn luyện độc lập cho mỗi ngôn ngữ sử dụng MLM. Chúng tôi huấn luyện các adapter ngôn ngữ MAD-X trên cùng dữ liệu Wikipedia được sử dụng cho Hyper-X, cho tất cả các ngôn ngữ với kiến trúc mặc định. Cuối cùng, cho thiết lập hỗn hợp ngôn ngữ, vì MAD-X gốc không cho phép huấn luyện đa nhiệm vụ tiêu chuẩn, chúng tôi huấn luyện các task adapter bằng cách sử dụng nhiều ngôn ngữ nguồn nhưng cho NER và POS riêng biệt. Chúng tôi gọi mô hình này là MAD-X MS.

**Phân tích không gian tham số** (Ponti et al., 2021) là một framework Bayesian học một bộ sinh tham số từ nhiều nhiệm vụ và ngôn ngữ cho lớp softmax trên đầu của MMT. Tuy nhiên, nếu một ngôn ngữ thiếu dữ liệu huấn luyện có chú thích, mô hình này không thể học biến ẩn cần thiết cho ngôn ngữ tương ứng. Do đó, chúng tôi chỉ đánh giá baseline này cho thiết lập đa nhiệm vụ hỗn hợp ngôn ngữ sử dụng cùng các phân vùng như Hyper-X. Chúng tôi sử dụng implementation gốc với các siêu tham số mặc định và phân tích low-rank.

**Biến thể mô hình** Chúng tôi đánh giá hai biến thể của Hyper-X để thấy tác động của kích thước siêu mạng: Mô hình Hyper-X Base tinh chỉnh 76m tham số (ds = 192), tương thích với MAD-X về tổng số tham số có thể huấn luyện, và Hyper-X Small chỉ cập nhật 13m tham số (ds = 32). Bảng 3 cho thấy số lượng tham số cùng với thời gian chạy tương ứng.

### 4.2 Chi tiết huấn luyện

Cho tất cả các thí nghiệm, chúng tôi sử dụng batch size 32 và độ dài chuỗi tối đa 256. Chúng tôi huấn luyện Hyper-X trong 100.000 bước cập nhật bằng cách sử dụng tốc độ học giảm tuyến tính 1e-4 với 4000 bước khởi động. Chúng tôi đánh giá các checkpoint mỗi 5.000 bước, và sử dụng checkpoint tốt nhất w.r.t. điểm validation trung bình để kiểm tra. Đối với các baseline, chúng tôi huấn luyện mBERT và MAD-X task adapter trong 20 epoch bằng cách sử dụng tốc độ học 1e-5 và 1e-4 tương ứng với cùng scheduler và bước khởi động. Vì MAD-X yêu cầu các language adapter tiên quyết, chúng tôi huấn luyện language adapter trong 100.000 bước cho mỗi ngôn ngữ riêng biệt.

Về kích thước mô hình, chúng tôi sử dụng chiều bottleneck 256 để học adapter cho Hyper-X. Tương tự, chúng tôi huấn luyện các language và adapter với chiều 256 và 48 cho MAD-X để tạo ra baseline có thể so sánh. Trong Hyper-X, như đầu vào cho siêu mạng, các chiều cho embedding nhiệm vụ, ngôn ngữ và lớp đều được đặt thành 64 (tổng 192). Trong quá trình huấn luyện, chúng tôi tạo ra các mini-batch đồng nhất cho mỗi kết hợp nhiệm vụ-ngôn ngữ để học các embedding tương ứng cùng với siêu mạng. Hơn nữa, theo Mahabadi et al. (2021b), chúng tôi cũng cập nhật các tham số layer-norm gốc. Trong quá trình huấn luyện đa nhiệm vụ, chúng tôi sử dụng lấy mẫu dựa trên nhiệt độ với T = 5 để cân bằng mỗi cặp nhiệm vụ-ngôn ngữ trong quá trình huấn luyện (Xem Phụ lục § B.1 để biết chi tiết).

## 5 Kết quả chuyển giao zero-shot

Bảng 2 cho thấy kết quả zero-shot tổng hợp trong NER và gán thẻ POS tương ứng. Ngoài điểm trung bình qua tất cả 15 ngôn ngữ zero-shot, chúng tôi cho thấy trung bình của 8 ngôn ngữ 'đã thấy' và 7 ngôn ngữ 'chưa thấy' riêng biệt so với phạm vi bao phủ ngôn ngữ của mBERT. Chúng tôi trình bày kết quả cho các cài đặt đơn nhiệm vụ tiếng Anh, đa nhiệm vụ tiếng Anh và đa nhiệm vụ hỗn hợp ngôn ngữ.

Nhìn chung, Hyper-X Base hoạt động ngang bằng với baseline mạnh nhất khi chuyển giao từ tiếng Anh. Trong sự hiện diện của các nguồn bổ sung, như hỗn hợp các cặp nhiệm vụ-ngôn ngữ, Hyper-X vượt trội hơn cả mBERT và phân tích không gian tham số (PSF). So với MAD-X, Hyper-X thường hoạt động tốt hơn trên các ngôn ngữ đã thấy. Chúng tôi liên hệ điều này với siêu mạng thống nhất cho phép chia sẻ tối đa giữa các ngôn ngữ và sử dụng cao hơn khả năng tiền huấn luyện trái ngược với các adapter riêng lẻ. Trên các ngôn ngữ chưa thấy, Hyper-X bị MAD-X vượt trội trong hầu hết các trường hợp. Tuy nhiên, chúng tôi nhấn mạnh rằng MAD-X yêu cầu huấn luyện các language adapter riêng biệt cho mỗi ngôn ngữ mới, điều này khiến nó kém hiệu quả về tài nguyên đáng kể so với Hyper-X (xem § 6.1).

**Đơn nhiệm vụ tiếng Anh** Khi tiếng Anh được sử dụng như ngôn ngữ nguồn duy nhất cho mỗi nhiệm vụ riêng biệt, Hyper-X (Base) hoạt động ngang bằng với MAD-X cho NER (52.7 vs 52.8 F1) nhưng thua kém cho gán thẻ POS (63.5 vs 65.4 Acc.) trung bình. Cả hai mô hình đều vượt trội đáng kể so với mBERT. Nhìn vào kết quả ngôn ngữ riêng lẻ, Hyper-X hoạt động hơi tốt hơn trên các ngôn ngữ 'đã thấy' so với MAD-X trong NER và gán thẻ POS tương ứng. Đối với các ngôn ngữ 'chưa thấy', cả MAD-X và Hyper-X đều được lợi từ MLM, dẫn đến cải thiện lớn so với mBERT. Giữa hai mô hình, MAD-X đạt điểm trung bình cao hơn trong cả NER và gán thẻ POS.

**Đa nhiệm vụ tiếng Anh** Trong cài đặt đa nhiệm vụ chỉ có dữ liệu tiếng Anh, việc tinh chỉnh mBERT cho cả hai nhiệm vụ đích cùng lúc cho kết quả hỗn hợp so với huấn luyện đơn nhiệm vụ—phù hợp với các phát hiện trước đây ghi nhận quên thảm khốc và can thiệp trong MMT (Wang et al., 2020). Mặt khác, Hyper-X Base cho thấy cải thiện nhỏ nhưng nhất quán trên đa số ngôn ngữ, với mức tăng trung bình 0.2 (F1) và 0.1 (Acc.) trong NER và gán thẻ POS tương ứng. Điều này xác nhận rằng Hyper-X có thể giảm thiểu can thiệp trong khi cho phép chia sẻ giữa các nhiệm vụ khi đủ khả năng được cung cấp.

**Đa nhiệm vụ hỗn hợp ngôn ngữ** Trong cài đặt này, hỗn hợp dữ liệu ngôn ngữ được cung cấp cho NER và POS thông qua hai phân vùng huấn luyện riêng biệt trong khi giữ mỗi cặp nhiệm vụ-ngôn ngữ chưa từng thấy trong một trong các phân vùng này. Tất cả các mô hình bao gồm mBERT đạt được điểm zero-shot tốt hơn so với các cài đặt trước đây. Trong số các baseline, phân tích không gian tham số (PSF) cho cải thiện lớn hơn so với mBERT trên cả hai nhiệm vụ, cho thấy tầm quan trọng của tham số hóa đặc thù nhiệm vụ và ngôn ngữ để thích ứng MMT. Hyper-X Base tạo ra mức tăng hiệu suất lớn nhất trong số các mô hình chỉ huấn luyện một mô hình duy nhất: nó đạt được mức tăng trung bình 9.0 (F1) và 4.3 (Acc.) cho NER và POS. Mặc dù cả PSF và Hyper-X đều cho phép thích ứng được điều kiện hóa trên hỗn hợp các kết hợp nhiệm vụ và ngôn ngữ, chúng tôi liên hệ sự khác biệt giữa PSF và Hyper-X với sự tương phản trong việc tạo tham số. PSF chỉ tạo ra tham số của lớp softmax và do đó không thể thích ứng các lớp sâu hơn của mô hình. Mặt khác, Hyper-X tạo ra các tham số lớp adapter được chèn xuyên suốt mô hình, cung cấp mức độ linh hoạt thích ứng cao hơn. Hyper-X vượt trội hơn PSF đặc biệt trên các ngôn ngữ chưa thấy vì nó được lợi từ MLM như một nhiệm vụ phụ trợ.

Cuối cùng, Hyper-X có xu hướng hoạt động hơi tốt hơn trên các ngôn ngữ đã thấy so với phiên bản đa nguồn thích ứng của MAD-X. Tuy nhiên, MAD-X vượt trội hơn Hyper-X trên các ngôn ngữ chưa thấy 1.2 (F1) và 2.8 (Acc.) cho NER và POS tương ứng. Bên cạnh những lợi ích dự kiến của các language adapter được huấn luyện độc lập trong MAD-X, chúng tôi liên hệ điều này với sự giám sát liên nhiệm vụ hạn chế cho các ngôn ngữ chưa thấy trong Hyper-X cho cài đặt này. Đặc biệt, khi nhiệm vụ đích là POS, hầu hết các ngôn ngữ chưa thấy chỉ có 100 câu có sẵn trong tập dữ liệu NER, điều này chỉ để lại ít dư địa cho cải thiện.

## 6 Phân tích

### 6.1 Hiệu quả tham số và thời gian

Bảng 3 cho thấy số lượng tham số được tinh chỉnh và thời gian huấn luyện cần thiết cho các baseline và mô hình Hyper-X. Không giống như mBERT, PSF và Hyper-X, MAD-X bao gồm 16 và 2 language và task adapter được huấn luyện độc lập tương ứng. Về hiệu quả tham số, các mô hình MAD-X và Hyper-X Base tương ứng với 43% tham số của mBERT. Tuy nhiên, về thời gian huấn luyện, Hyper-X Base chỉ được huấn luyện một lần trong khoảng 18 giờ, trái ngược với tổng thời gian huấn luyện cao đáng kể của MAD-X (116 giờ tổng cộng). Do đó, xem xét hiệu suất zero-shot cạnh tranh qua các ngôn ngữ và cài đặt khác nhau, Hyper-X Base cung cấp sự cân bằng hiệu quả-hiệu suất tốt hơn. Hơn nữa, trong trường hợp thêm nhiều ngôn ngữ, số lượng tham số và thời gian huấn luyện của MAD-X tăng tuyến tính với số ngôn ngữ mới, trong khi chi phí tính toán của Hyper-X vẫn giữ nguyên.

Như các biến thể mô hình Hyper-X, chúng tôi đánh giá hai kích thước khác nhau của source embedding (ds; 32→192). Mặc dù Hyper-X Small hiệu quả hơn nhiều về tham số (7.2% tham số của mBERT) và mất ít thời gian huấn luyện hơn một chút (16h), hiệu suất zero-shot của nó thấp hơn đáng kể so với mô hình base, đặc biệt là cho các ngôn ngữ chưa thấy. Tuy nhiên, Hyper-X Small vẫn là một phương án hợp lệ đặc biệt cho các ngôn ngữ 'đã thấy'.

### 6.2 Tác động của huấn luyện MLM phụ trợ

Hình 3 minh họa tác động của huấn luyện MLM phụ trợ trong Hyper-X Base cho cài đặt đa nhiệm vụ hỗn hợp ngôn ngữ. Vì cài đặt này cung cấp các instance huấn luyện cho mỗi nhiệm vụ và ngôn ngữ, chúng tôi đánh giá tác động của MLM bằng cách loại bỏ dữ liệu Wikipedia tương ứng trước tiên cho các ngôn ngữ 'đã thấy', sau đó cho 'tất cả' ngôn ngữ. Như được hiển thị trong hình, mặc dù sự có sẵn của dữ liệu MLM tăng nhẹ hiệu suất ngôn ngữ đã thấy, nó chủ yếu thúc đẩy điểm số trong các ngôn ngữ chưa thấy: +6.2 F1 và +10.5 Acc. cho NER và POS tương ứng. Hơn nữa, khi dữ liệu MLM bị loại bỏ chỉ cho các ngôn ngữ đã thấy, Hyper-X có thể phần lớn phục hồi hiệu suất trên các ngôn ngữ đã thấy, xác nhận hiệu ứng chi phối của MLM trên các ngôn ngữ chưa thấy.

### 6.3 Tác động của ngôn ngữ nguồn

Trong cài đặt đa nhiệm vụ hỗn hợp ngôn ngữ, chúng tôi cố ý tránh nhóm các ngôn ngữ từ cùng họ vào các phân vùng khác nhau, để hạn chế việc chuyển giao từ các instance cùng họ ngôn ngữ, và để quan sát hiệu ứng của sự giám sát liên nhiệm vụ. Tuy nhiên, chúng tôi cũng đánh giá tác động của ngôn ngữ nguồn trong thiết lập này, để đo mức độ chuyển giao tích cực tiềm năng. Để đạt mục đích này, chúng tôi chuyển đổi các phân vùng của kk, mt, yue, sao cho tất cả chúng sẽ có khả năng được lợi từ một ngôn ngữ có tài nguyên cao từ cùng họ cho cùng nhiệm vụ đích. Hình 4 và 5 cho thấy kết quả tổng hợp trong cả Hyper-X Base và mBERT. Thứ nhất, cả hai mô hình đều được lợi từ chuyển giao tích cực. Thứ hai, mặc dù mức tăng tương đối trong mBERT hơi cao hơn, Hyper-X vẫn vượt trội hơn mBERT với biên độ lớn, cho thấy tính bền vững của mô hình chúng tôi so với các phân vùng khác nhau.

### 6.4 Chuyển giao few-shot

Việc tinh chỉnh MMT với một vài instance đích đã được chứng minh là tăng hiệu suất zero-shot (Lauscher et al., 2020b). Do đó, chúng tôi đánh giá Hyper-X cho chuyển giao few-shot trên 5 ngôn ngữ—3 trong số đó có tài nguyên cao và được bao phủ bởi mBERT và 2 có tài nguyên thấp và chưa thấy. Để đạt mục đích này, chúng tôi tinh chỉnh thêm Hyper-X và các baseline tương ứng được huấn luyện ban đầu trong đa nhiệm vụ tiếng Anh bằng cách sử dụng 5, 10, 20 và 50 instance huấn luyện cho mỗi ngôn ngữ riêng biệt trên NER và gán thẻ POS (xem chi tiết trong Phụ lục §D).

Hình 6 trình bày kết quả trung bình so sánh mBERT với MAD-X. Tương tự như kết quả zero-shot, trên các ngôn ngữ đã thấy, Hyper-X liên tục cung cấp khả năng thích ứng tốt hơn cả hai baseline cho NER và POS. Trên các ngôn ngữ chưa thấy, MAD-X cho kết quả tốt nhất trung bình. Điều này là do MAD-X bắt đầu với các biểu diễn ban đầu tốt hơn cho tiếng Malta và tiếng Uyghur. Khi nhiều mẫu được cung cấp, Hyper-X giảm khoảng cách ban đầu. Nhìn chung, Hyper-X liên tục đạt được hiệu suất tốt nhất hoặc cạnh tranh trong đa số các thí nghiệm, ngoại trừ các ngôn ngữ 'chưa thấy' cho gán thẻ POS, cho thấy hiệu quả của phương pháp chúng tôi vượt ra ngoài chuyển giao zero-shot tiêu chuẩn. Cùng với hiệu quả tham số và huấn luyện, các kết quả này cho thấy Hyper-X có thể được mở rộng dễ dàng cho các ngôn ngữ mới mà không phải chịu chi phí tính toán lớn.

## 7 Công trình liên quan

**Adapter** Như một phương án hiệu quả về tham số thay cho việc tinh chỉnh tiêu chuẩn, adapter đã được sử dụng cho huấn luyện nhanh (Rücklé et al., 2021), học đa nhiệm vụ (Stickland và Murray, 2019) và tổng hợp kiến thức (Pfeiffer et al., 2021a; Wang et al., 2021; Poth et al., 2021). Hơn nữa, Mahabadi et al. (2021a) và He et al. (2022a) mở rộng adapter để có hiệu suất tốt hơn với ít tham số hơn. Trong bối cảnh chuyển giao đa ngôn ngữ, adapter cho phép phân bổ khả năng đặc thù ngôn ngữ bổ sung, do đó giảm thiểu 'lời nguyền của tính đa ngôn ngữ' (Üstün et al., 2020). Các language adapter như vậy (Pfeiffer et al., 2020b; Ansell et al., 2021) đạt được kết quả zero-shot cao khi kết hợp với task adapter và cho phép tổng quát hóa cho các ngôn ngữ chưa thấy trong quá trình tiền huấn luyện thông qua thích ứng dựa trên MLM (Pfeiffer et al., 2021b). Philip et al. (2020) và Üstün et al. (2021) cũng sử dụng adapter đơn ngôn ngữ cho NMT zero-shot và không giám sát.

**Siêu mạng trong NLP** Tay et al. (2021) đề xuất một mô hình đa nhiệm vụ sử dụng siêu mạng để điều kiện hóa trên đầu vào để học các tái tham số hóa đặc thù nhiệm vụ. Tương tự, Mahabadi et al. (2021b) tạo ra các adapter đặc thù nhiệm vụ thông qua siêu mạng. Gần đây, He et al. (2022b) sử dụng siêu mạng để tạo ra prompt. Đối với học đa ngôn ngữ, nơi các nguồn đầu vào tương ứng với embedding ngôn ngữ, Üstün et al. (2020) và Ansell et al. (2021) học các embedding này từ các vector đặc trưng loại hình của ngôn ngữ, cho phép tổng quát hóa cho các ngôn ngữ chưa thấy dựa trên siêu mạng. Với tinh thần tương tự như công trình của chúng tôi, phân tích không gian tham số (PSF; Ponti et al., 2021), học các embedding đặc thù nhiệm vụ và ngôn ngữ từ các kết hợp nhiệm vụ-ngôn ngữ đã thấy. Tuy nhiên, không giống như mô hình của chúng tôi, các embedding này được sử dụng cho tham số hóa đặc thù nhiệm vụ/ngôn ngữ trong lớp softmax.

## 8 Kết luận

Chúng tôi đã đề xuất Hyper-X, một phương pháp mới cho học chuyển giao đa nhiệm vụ đa ngôn ngữ, dựa trên siêu mạng thống nhất tận dụng các nguồn thông tin không đồng nhất, như nhiều nhiệm vụ và ngôn ngữ. Bằng cách học tạo ra các adapter tổng hợp cho mỗi kết hợp nhiệm vụ-ngôn ngữ sửa đổi các tham số của transformer đa ngôn ngữ tiền huấn luyện, Hyper-X cho phép chia sẻ thông tin tối đa và cho phép dự đoán zero-shot cho các cặp nhiệm vụ-ngôn ngữ tùy ý tại thời điểm kiểm tra. Thông qua một số thí nghiệm, chúng tôi chứng minh rằng Hyper-X cạnh tranh với nghệ thuật hiện đại khi chuyển giao từ một ngôn ngữ nguồn. Khi có sẵn hỗn hợp các nhiệm vụ và ngôn ngữ, Hyper-X vượt trội hơn một số baseline mạnh trên nhiều ngôn ngữ, trong khi hiệu quả hơn về tham số và thời gian. Cuối cùng, chúng tôi cho thấy rằng đối với chuyển giao few-shot, Hyper-X là một lựa chọn mạnh với chi phí tính toán thấp hơn so với các baseline cho việc thích ứng nhiệm vụ ban đầu.

## 9 Hạn chế

Thứ nhất, mặc dù các thí nghiệm của chúng tôi cho thấy tiềm năng của Hyper-X để được lợi từ nhiều nhiệm vụ cho chuyển giao zero-shot, cho đến nay chúng tôi đã đánh giá mô hình của chúng tôi trên một tập hợp hạn chế các nhiệm vụ: NER và gán thẻ POS, điều này có thể hạn chế khả năng tổng quát hóa của mô hình chúng tôi cho các nhiệm vụ khác.

Thứ hai, đối với chuyển giao few-shot, chúng tôi giới hạn các thí nghiệm của mình cho các ngôn ngữ mà chúng tôi học thông qua MLM và cho các nhiệm vụ hiện có. Công trình của chúng tôi không bao gồm các ngôn ngữ không có dữ liệu MLM cũng như các nhiệm vụ hoàn toàn mới. Tuy nhiên, việc học các embedding nhiệm vụ và ngôn ngữ riêng biệt tạo ra khả năng nội suy các embedding hiện có cho các ngôn ngữ hoặc nhiệm vụ mới, điều này có thể đặc biệt hiệu quả cho học few-shot. Chúng tôi để việc khám phá hai hạn chế này cho công trình tương lai.
