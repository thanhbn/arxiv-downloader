# 2305.17660.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.17660.pdf
# File size: 1054306 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Plug-and-Play Document Modules for Pre-trained Models
Chaojun Xiao1,2,3, Zhengyan Zhang1,2,3, Xu Han1,2,3âˆ—, Chi-Min Chan1,2,3, Yankai Lin4,5
Zhiyuan Liu1,2,3âˆ—, Xiangyang Li6, Zhonghua Li6, Zhao Cao6, Maosong Sun1,2,3âˆ—
1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing
2International Innovation Center of Tsinghua University, Shanghai3Quan Cheng Laboratory
4Gaoling School of Artificial Intelligence, Renmin University of China, Beijing
5Beijing Key Laboratory of Big Data Management and Analysis Methods
6Huawei Technologies Co., Ltd.
xiaocj20@mails.tsinghua.edu.cn ,{hanxu2022,liuzy,sms}@tsinghua.edu.cn
Abstract
Large-scale pre-trained models (PTMs) have
been widely used in document-oriented NLP
tasks, such as question answering. However,
the encoding-task coupling requirement results
in the repeated encoding of the same documents
for different tasks and queries, which is highly
computationally inefficient. To this end, we tar-
get to decouple document encoding from down-
stream tasks, and propose to represent each
document as a plug-and-play document mod-
ule, i.e., a document plugin, for PTMs (PlugD).
By inserting document plugins into the back-
bone PTM for downstream tasks, we can en-
code a document one time to handle multiple
tasks, which is more efficient than conventional
encoding-task coupling methods that simulta-
neously encode documents and input queries
using task-specific encoders. Extensive exper-
iments on 8 datasets of 4 typical NLP tasks
show that PlugD enables models to encode doc-
uments once and for all across different sce-
narios. Especially, PlugD can save 69% com-
putational costs while achieving comparable
performance to state-of-the-art encoding-task
coupling methods. Additionally, we show that
PlugD can serve as an effective post-processing
way to inject knowledge into task-specific mod-
els, improving model performance without any
additional model training. Our code and check-
points can be found in https://github.com/
thunlp/Document-Plugin .
1 Introduction
In recent years, large-scale pre-trained models
(PTMs) (Devlin et al., 2019; Raffel et al., 2020)
have been widely adopted and achieved break-
through performance for document-oriented NLP
tasks, such as question answering. However, due
to the tight coupling of document encoding and
concrete tasks, PTMs have to dynamically gener-
ate document representations according to specific
tasks and queries, leading to the repeated encoding
âˆ—Corresponding authors.
Where did 
Shakespeare born?Warwickshire
Document
PluginPTMPre-trained
ModelPTM
Slot
FillingQuestion
AnsweringDocument
Encoding
QA
<Shakespeare, 
spouse , ___ >Anne Hathaway
SF
Fact
Verification
QATask-specific
Models
Shakespeare was
an playwriterFV
YesFigure 1: Illustration of plug-and-play document mod-
ules. Document encoding is decoupled from concrete
tasks. By plugging document plugins into task-specific
models, we can handle multiple tasks such as question
answering, fact verification, and slot filling.
of the same documents in different applications.
For example, Wikipedia documents are commonly
used in various knowledge-intensive tasks such as
question answering (Chen et al., 2017), fact verifi-
cation (Thorne et al., 2018), and dialogue genera-
tion (Dinan et al., 2019). In this case, existing meth-
ods separately encode one document for each task
or even for each input query (e.g., a question for
question answering, a claim for fact verification),
making them highly computationally inefficient.
To this end, it raises a natural question: can we
decouple document encoding from concrete tasks,
encoding documents only once and with guaran-
teed transferability across multiple tasks?
For this question, we propose a novel framework
based on PTMs to decouple document encoding
from tasks, named PlugD. Specifically, PlugD in-
corporates plug-and-play modules to store docu-
ment information and utilizes a PTM backbone to
capture information from plugins for task reason-
ing. As shown in Figure 1, documents are encoded
into pluggable plugins once and for all before taskarXiv:2305.17660v1  [cs.CL]  28 May 2023

--- PAGE 2 ---
adaptation. The semantics and knowledge of doc-
uments can be injected into task-specific models
by plugging document plugins. During task rea-
soning, the task-specific models can activate the
information encoded in the document plugins to
handle the input queries. In this way, PlugD can
decouple the document encoding from downstream
task reasoning and reduce the computation costs.
For representing documents as pluggable mod-
ules, there are two main challenges: (1) Plugin
learning: The document plugins must be effec-
tive for various downstream tasks, requiring them
to contain sufficient semantics and knowledge.
(2) Plugin utilization: Once the document plugins
are ready, it is important for task-specific models to
capture relevant information from them effectively
for task reasoning.
As for plugin learning, we adopt a self-
supervised method, which requires document plu-
gins to provide sufficient knowledge for the PTM
to make predictions. Specifically, for each docu-
ment, we first randomly select parts of sentences as
a query and use the remaining sentences as context
to learn plugins. Then, after encoding the context
into plugins, the model is required to recover the
masked recurring spans or generate the next sen-
tences for the query based on the plugin knowledge.
As for plugin utilization, we propose two strate-
gies to utilize document plugins for downstream
tasks: plugging during tuning andplugging after
tuning1. For plugging during tuning, document plu-
gins are utilized in both tuning and inference stages.
Task data and document plugins are combined to-
gether to train task-specific models. For plugging
after tuning, document plugins are only utilized in
the inference stage to provide external knowledge.
Document plugins are adopted as a post-processing
way to inject knowledge into task-specific models
without additional training.
To verify the effectiveness of our plug-and-play
framework, we adopt Wikipedia as our document
collection and conduct experiments on 8datasets
of4typical knowledge-intensive NLP tasks. The
results show that we can generate document plug-
ins once and successfully adapt plugins to various
downstream tasks. Compared to competitive base-
lines that encode documents and task-specific in-
puts simultaneously, our plugin-based method can
save 69% computational costs with comparable
1Here tuning refers to tuning PTMs for downstream tasks,
including full-parameter fine-tuning and parameter-efficient
tuning.performance. Besides, utilizing document plug-
ins works as an effective post-processing approach
to introducing the knowledge of documents into
task-specific models and achieving performance
improvements without model training. We argue
that with the current trend of increasing the model
size of PTMs, decoupling document encoding from
concrete tasks like PlugD can be a promising di-
rection that enables large PTMs to effectively and
efficiently serve diverse downstream tasks.
2 Related Work
2.1 Plug-and-Play Modules for PTMs
Recent PTMs have shown to be effective in var-
ious downstream tasks (Devlin et al., 2019; Liu
et al., 2019; Raffel et al., 2020; Radford et al., 2018;
Brown et al., 2020; Han et al., 2021; Chowdhery
et al., 2022). However, training and tuning large-
scale PTMs for ever-increasing tasks is expensive
in computation and storage. To address this issue,
building plug-and-play modules with various capa-
bilities for PTMs has received increasing attention
recently. For instance, parameter-efficient tuning,
which is also known as delta tuning, is proposed to
perform task adaptation by fine-tuning only small
amounts of parameters and keeping other parame-
ters fixed (Zaken et al., 2022; Houlsby et al., 2019;
Lester et al., 2021; Liu et al., 2021; Hu et al., 2021;
Ding et al., 2022). The task-specific modules pos-
sess play-and-play characteristics and can effec-
tively inject task ability into PTMs. Besides, some
researchers explore combining pluggable modules
with large-scale PTMs for efficient controllable
text generation (Dathathri et al., 2020; Madotto
et al., 2020; Pascual et al., 2021), domain adap-
tation (Chronopoulou et al., 2022; Pfeiffer et al.,
2020), information retrieval (Shi et al., 2023; Yu
et al., 2023), knowledge injection (Zhang et al.,
2023), model debias (Lauscher et al., 2021), and
model integration (Xu et al., 2023; Alayrac et al.,
2022). Owing to the powerful abilities of large-
scale PTMs, these modules can effectively activate
the modelâ€™s capabilities with limited parameters.
Different from previous functional modules, we at-
tempt to build document plugins to provide knowl-
edge and context information for PTMs.
2.2 Language Representation Learning
Language representation learning is a fundamen-
tal NLP task (Bengio et al., 2013; Devlin et al.,
2019; Radford et al., 2018) that aims to effectively

--- PAGE 3 ---
represent rich semantics distributed in text and ben-
efit various downstream tasks. Previous efforts
attempt to map the language inputs into interme-
diate distributed features, such as word embed-
dings (Mikolov et al., 2013; Kiros et al., 2015;
Pennington et al., 2014; Peters et al., 2018), sen-
tence embeddings (Conneau et al., 2017; Reimers
and Gurevych, 2019; Gao et al., 2021), and doc-
ument embeddings (Dai et al., 2015; Wu et al.,
2018), which are further used as inputs of down-
stream task-specific models to generate the final
task-specific document representations. Further-
more, some researchers make preliminary explo-
ration to decouple document encoding from tasks
by freezing the part of layers of document en-
coders (Du et al., 2020; Saad-Falcon et al., 2022).
But these works only achieve semi-decoupling of
document encoding from tasks, and can only be
used for the plugging during tuning setting.
Notably, many efforts have been devoted to ex-
ploring the effective architectures, such as sparse
attention, of PTMs to encode long documents (Belt-
agy et al., 2020; Zaheer et al., 2020; Zhang et al.,
2021; Mehta et al., 2022; Tay et al., 2022). These
works are parallel to ours, and we can adopt sparse-
attention layers to further improve efficiency.
3 Methodology
In this section, we will first present the paradigm de-
scription and the overall framework of PlugD Then
we introduce the self-supervised plugin learning
method to make document plugins contain suffi-
cient semantics and two strategies about how to
utilize document modules.
3.1 Plug-and-Play Document Modules
In this paper, we focus on decoupling document
encoding with specific tasks. Different from
encoding-task coupling methods which simulta-
neously encode the documents and task-specific
queries, PlugD aims to encode documents once
and for all before task adaptation. Specifically,
given a PTM backbone Mand a document d, we
first use the PTM to encode the document into a
task-agnostic pluggable module, D, i.e., a docu-
ment plugin. Equipped with the document plugin,
the PTM is injected into the corresponding docu-
ment knowledge. Then we adopt task data to tune
the PTM to obtain task-specific models. During
inference, we can quickly obtain predictions for
an input query by inserting the relevant documentplugin into the task-specific models, avoiding re-
encoding the document.
3.2 Overall Framework
As shown in Figure 1, we design PlugD, which
consists of three components: a PTM backbone,
document plugins that provide document knowl-
edge, and task-specific models derived from the
PTM to handle specific tasks. We will present
these components below.
PTM Backbone. PTMs have been proven ef-
fective in a wide range of downstream tasks, and
raise a paradigm shift to solve multiple tasks with
one unified model (Bommasani et al., 2021; Brown
et al., 2020; Chowdhery et al., 2022). In view of
this, we further explore the decoupling of document
encoding and tasks, unifying document represen-
tations across tasks. PlugD relies on a large-scale
PTM, which can serve as a fundamental infrastruc-
ture to learn plugins from documents and as an ini-
tialization for task-specific models. Note that, for
our framework, any PTM with large-scale parame-
ters can be used as the backbone. Specifically, we
adopt a widely-used sequence-to-sequence PTM,
T5 (Raffel et al., 2020), in this paper. As the pre-
training objectives of the PTM do not involve the
document plugins, we further conduct plugin learn-
ing for the PTM so that it can generate and utilize
document plugins. The training tasks are intro-
duced in the following sections.
Document Plugin. Document plugins store doc-
ument knowledge and are obtained before utilizing
these documents for specific tasks. Inspired by re-
cent progress in model interpretation (Petroni et al.,
2019; Jiang et al., 2020; Roberts et al., 2020; Dai
et al., 2022; Mitchell et al., 2022), which claims
that the parameters of PTMs store vast amounts
of knowledge, we propose to encode the seman-
tics and knowledge of documents into pluggable
parameters. In this way, when the document plugin
is inserted into the PTM, the PTM is empowered
with the corresponding document knowledge.
Inspired by prefix-tuning (Li and Liang, 2021),
we represent documents as prefix tokens for at-
tention layers. When the document plugin is in-
serted into the backbone, we concatenate the cor-
responding prefix tokens with the hidden vectors
of task-specific queries in attention layers to pro-
vide document knowledge. Specifically, given a
document dwithLdtokens, we first encode the
document with the PTM to get the raw document

--- PAGE 4 ---
Shakespeare was born in 
Warwickshire. He married 
Susanna      At the age of 18,  
Shakespeare  married   Anne 
Hathaway.  After  the  marriage 
Anne gave birth to a daughter, 
Susanna.    Ham died of 
unknown causes at 1 August 
1596
PTM PTM<X> Shakespeare <Y> 
Anne Hathaway ...
At the age of 18, Shake-
speare married Anne ...After the marriage Anne 
gave birth to 
Recurring Span
Prediction Next Sentece
GenerationSplit the document
into the context
and query.
Learning a plugin
from the context.At the age of 18, <X> 
married <Y> ...Figure 2: The illustration of PlugD in plugin learning.
representations Hd={h1, ...,hLd}. Then, we
adopt a mapping network to project the representa-
tion vectors into prefix tokens: Pd={p1, ...,pLd},
where pi=hi+MLP (hi). The prefix tokens
are further inserted into the attention layers. Let
Hq={hq
1, ...,hq
Lq}denote the hidden vectors of
query in the attention layer. We calculate the atten-
tion output as follows:
Ho
q=Attn(HqWq,cat(Pd,Hq)Wk,cat(Pd,Hq)Wv),
(1)
where Wq,Wk, andWvare trainable parame-
ters for the self-attention layer. Then Ho
qis fed
into the feed-forward layer as the original Trans-
former (Vaswani et al., 2017) layer.
Different from encoding documents during task
adaptation or inference, prefix tokens do not in-
volve the computation of feed-forward layers.
Moreover, to better integrate the semantics of doc-
uments and queries for handling tasks, document
plugins are only inserted in the near-top layers of
the PTM backbone. Therefore, these document plu-
gins in the form of prefix tokens only increase lim-
ited computational requirements, whereas PlugD
can achieve a significant computational speedup as
a result. Due to the high storage requirement of
adding different prefix tokens to different attention
layers, we share Pdfor all attention layers. Note
that, we can also utilize other model structures,
such as bias parameter (Zaken et al., 2022) and
LoRA (Hu et al., 2021), to represent documents in
PlugD, which we leave for future work.
Task-specific Models. Task-specific models are
derived from the PTM backbone and tuned on the
supervised task data to obtain task reasoning abil-
ity. During downstream tuning, we freeze the doc-
ument plugins, and only the task-specific models
and the mapping network of the document plugins
are trainable so that the document plugins can be
reused across different tasks. We adopt two train-
ing methods for task-specific models, including
vanilla full-parameter fine-tuning and parameter-efficient tuning (PET). Note that, deploying large-
scale PTMs with full-parameter fine-tuning will
lead to exacerbated computational and storage bur-
dens for multi-task scenarios. Thus, it is worth
exploring PlugD with PET for efficient task adap-
tion in real-world applications.
Both two training methods adopt task-specific
objectives to optimize the parameters. Fine-tuning
optimizes all parameters of the PTM backbone,
while parameter-efficient tuning only optimizes
parts of the parameters and keeps other parame-
ters frozen. Specifically, we adopt adapters for
parameter-efficient tuning (Pfeiffer et al., 2021).
Given the hidden vector hâˆˆRd, where dis the
hidden size, the output of the adapter layer is cal-
culated as:
hout=h+Ï•(hW down)Wup, (2)
where WdownâˆˆRdÃ—r,WupâˆˆRrÃ—d, and râ‰ªd
refer to the bottleneck dimension.
Computational Complexity. PlugD encodes
the documents before task adaptation and thus can
reduce the computation costs. In this paragraph,
we discuss the computational complexity of PlugD
in detail. Assume the lengths of the query and
document are LqandLd, respectively. For the
traditional encoding-task coupling models, which
simultaneously encode documents and queries, the
computational complexity of the attention layer is
O((Lq+Ld)2), and the computational complex-
ity of the feed-forward layer is O(Lq+Ld). For
PlugD, the document plugins are inserted into the
attention layer, whose computational complexity
isO(Lq(Lq+Ld)). And the document plugins do
not involve the computation of the feed-forward
layer, and thus its computational complexity is
O(Lq). In real-world applications, the documents
usually are much longer than the queries. There-
fore, PlugD can achieve significant computational
speedup compared with conventional encoding-
task coupling models.

--- PAGE 5 ---
3.3 Plugin Learning
To enable the document plugins to contain suf-
ficient document knowledge, we futher explore
self-supervised plugin learning in this section. As
shown in Figure 2, we adopt two self-supervised
tasks, recurring span prediction, and next sen-
tence generation to augument the comphrension
and generation ability of PlugD. Both two tasks
require document plugins to provide context infor-
mation for the model to make the predictions. Let
d={s1, ..., s n}denote the input document with n
sentences. We perform plugin learning as:
Recurring span prediction (RSP) . Inspired by
Ram et al. (2021), we utilize recurring spans to
construct self-supervision signals. Recurring spans
occur multiple times in the documents, and usually
contain important semantics for document under-
standing. Masking the recurring spans and requir-
ing the PTM to recover them can help the PTM
to capture document semantics. Specifically, we
concatenate sentences randomly sampled from the
document das query q, and treat the remaining
sentences as context c. Then we generate the doc-
ument plugin Pcbased on c, and replace the re-
curring spans in qas special mask tokens. The
PTM is required to predict the masked spans in q
conditioned on Pc. Different from the traditional
masked language model task (Devlin et al., 2019;
Raffel et al., 2020), which mainly focuses on local
information around the masked spans, RSP usually
requires the PTM to integrate global information
from document plugins.
Next sentence generation (NSG) . To enable
the document plugins to benefit generation tasks,
we adopt NSG as a training task. We first
randomly sample three consecutive sentences
{si, si+1, si+2}from the document d. The re-
maining sentences are treated as the context c=
{s1, ..., s iâˆ’1, si+3, ..., s n}to generate the docu-
ment plugin Pc. Then we regard sias the query,
and require the PTM to generate the following two
sentences {si+1, si+2}conditioned on Pc.
These two tasks require the PTM to capture both
local information from the queries and global infor-
mation from the document plugins. Therefore, after
plugin learning, the PTM is supposed to be able
to build informative document plugins and serve
as a good initialization for task-specific models to
capture knowledge from document plugins. Both
two tasks are sequence-to-sequence tasks, and we
adopt the negative likelihood as the training ob-jectives for two tasks. The model is trained in a
multi-task fashion, and the final training loss is the
sum of the loss of two tasks. During plugin learn-
ing, the document plugins are calculated in real
time for different documents. All parameters of the
PTM are tuned for plugin learning. After that, the
document plugins can be calculated and stored for
further downstream task tuning and inference.
3.4 Plugging Strategies
To flexibly utilize the document plugins, we pro-
pose two plugging strategies:
Plugging during tuning. In this setting, the docu-
ment plugins are adopted in both the training and in-
ference of task-specific models. Given an instance
with the query and document as inputs, we first
insert the corresponding document plugin, which
is computed before fine-tuning, into the models.
Then task-specific models are trained with task-
specific objectives to capture relevant information
from the document plugins.
Plugging after tuning. In this setting, the doc-
ument plugins are adopted only in the inference
of task-specific models. Document plugins can
provide external knowledge, and serve as a post-
processing method to inject knowledge into task-
specific models. During inference, given an in-
stance, we directly insert related document plugins
into the task-specific models to achieve knowledge
injection. This setting does not require additional
training for existing task-specific models and can
be used to flexibly inject textual knowledge.
4 Experiments
4.1 Evaluation Settings
Datasets. We adopt widely-used Wikipedia arti-
cles as our document collection and select typical
knowledge-intensive tasks for evaluation. We adopt
a typical multi-task benchmark, KILT (Petroni
et al., 2021), to evaluate our models. The tasks
in KILT are grounded in the same snapshot of
Wikipedia pages. In particular, we evaluate PlugD
on a fact verification dataset, FEVER (Thorne et al.,
2018), four question answering datasets, including
Natural Questions (NQ) (Kwiatkowski et al., 2019),
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), ELI5 (Fan et al., 2019), a dialogue gen-
eration dataset, Wizard of Wikipedia (WoW) (Di-
nan et al., 2019), and two slot filling dataset, Zero
Shot RE (zsRE) (Levy et al., 2017), T-REx (El-
Sahar et al., 2018). These tasks are diverse and

--- PAGE 6 ---
ModelsFEVER NQ TriviaQA HotpotQA ELI5 WoW zsRE T-RexAvg.Acc. EM F1 EM F1 EM F1 RL F1 Acc. Acc.
Parameter-efficient Tuning
ED2LM 83.13 38.34 46.04 53.84 62.05 19.84 28.63 11.24 15.24 31.15 46.34 37.39
EmbRecy 84.59 37.42 45.43 53.02 61.05 18.98 27.70 11.57 16.91 27.20 44.16 36.73
ED2LM fâ™£81.81 35.62 44.18 52.01 59.82 19.07 27.81 11.01 15.20 27.09 44.78 35.82
EmbRecy fâ™£84.59 32.13 40.17 47.59 55.37 18.18 26.79 11.92 16.65 20.76 41.22 34.13
PlugDâ™£86.56 41.54 49.76 57.29 65.43 23.04 32.51 11.37 17.15 32.12 48.38 39.68
w/o PTâ™£86.33 40.24 47.72 57.67 64.91 22.04 31.44 11.67 17.07 30.64 48.26 39.24
UpperBound 88.20 42.60 50.86 61.77 69.14 23.84 33.71 11.80 17.92 33.65 49.96 41.22
Full-parameter Fine-tuning
ED2LM 80.59 42.07 49.79 58.94 66.68 22.80 32.32 11.66 16.10 31.77 50.84 39.35
EmbRecy 84.34 42.71 50.55 59.31 66.67 23.57 33.46 12.01 17.30 30.10 50.12 39.93
ED2LM fâ™£84.17 40.84 48.57 57.05 64.92 21.61 30.70 11.94 15.83 24.19 48.04 37.96
EmbRecy fâ™£85.04 39.89 47.58 57.91 65.37 21.59 30.92 11.92 16.69 27.82 50.28 38.89
PlugDâ™£86.34 42.53 50.42 59.46 67.07 23.46 33.07 12.30 17.61 30.99 52.22 40.61
w/o PTâ™£85.97 42.25 49.80 58.88 66.60 23.05 32.20 12.16 17.40 29.94 52.40 40.26
UpperBound 86.42 45.03 52.92 62.50 69.82 24.54 34.66 12.33 18.39 32.60 52.50 41.79
Table 1: The main results of our proposed PlugD and baselines for plugging during tuning. We boldface the best
result and underline the second-best results for each dataset. The methods that can generate task-agnostic document
representations are denoted withâ™£.
require the model to exploit document knowledge
fully. As shown in the paper of KILT, external doc-
ument knowledge can not benefit the entity linking
task. Thus, we do not use them for evaluation in
this paper. Following Petroni et al. (2021), we use
dense passage retrieval (Karpukhin et al., 2020) to
retrieve relevant documents from Wikipedia arti-
cles for each query. Please refer to Appendix for
evaluation results of document retrieval.
Metrics. Following previous work, we adopt
accuracy for the fact verification task (FEVER) and
slot filling tasks (zsRE, T-REx); exact match (EM)
and F1 score for the extractive question answer-
ing tasks (NQ, HotpotQA, TriviaQA); ROUGE-L
(RL) for the long abstractive question answering
tasks (ELI5); F1 score for the dialogue generation
task (WoW). Besides, to evaluate the overall perfor-
mance, we calculate average scores for these tasks
as an evaluation metric, in which EM scores are
used for extractive question answering tasks.
4.2 Training Details
We utilize the widely used T5-large (Raffel et al.,
2020), as our PTM backbone. For the PET train-
ing method, we set the bottleneck dimension of
adapters as 16. We insert document plugins in the
last12layers. We conduct plugin learning on a
large-scale unsupervised corpus, C4 (Raffel et al.,
2020) for 36k steps. We use Adam to optimize
our models. Due to the high computational costsof full-parameter fine-tuning, in the following ex-
periments, we adopt the PET method to train the
models unless otherwise specified. We train models
with a half-precision floating point on 8NVIDIA
A100 GPUs, and the plugin learning process takes
18 hours. Please refer to Appendix for more details.
4.3 Baselines
Plugging during tuning. Here we compare PlugD
with several representative baselines, which encode
the documents and queries with two different en-
coders. In this way, these models can reuse docu-
ment representations across different queries, but
they still need to generate different document rep-
resentations for different tasks. (1) ED2LM (Hui
et al., 2022) utilizes the encoder-decoder architec-
ture to encode the queries and documents sepa-
rately, and then the document can be pre-encoded
before inference. In particular, the documents are
inputted into the encoder, and queries are inputted
into the decoder. (2) EmbRecy (Saad-Falcon et al.,
2022) proposes to reuse the intermediate activa-
tions of the documents to achieve speedup for fine-
tuning and inference. EmbRecy caches an inter-
mediate layerâ€™s output as the document represen-
tation and the remaining near-top layers are tuned
to fuse the information of documents and queries.
(3) Besides, to meet the setting of decoupling doc-
ument encoding from tasks, we freeze the docu-
ment encoders of ED2LM and EmbRecy to make

--- PAGE 7 ---
Models Avg.FLOPs Time
G ms
ED2LM 37.39 114.9 60
EmbRecy 35.54 197.5 142
PlugD 39.68 139.3 98
UpperBound 41.22 453.1 226
Table 2: The average scores and computational costs of
PlugD and baseline models.
the document representations unified across differ-
ent tasks. We denote the two task-agnostic meth-
ods as ED2LM fandEmbRecy f. (4) As PlugD
conducts further self-supervised training for plug-
and-play representation learning, we also present
the results of PlugD without plugin learning ( w/o
PT) to show the effectiveness of the architecture
of PlugD. (5) UpperBound follows the traditional
settings, in which the queries and documents are
concatenated together and fed into the model. The
document representations generated by this base-
line are query-specific. The model needs to encode
a single document multiple times for different tasks
and different queries, which is the upper bound of
task-agnostic methods.
Plugging after tuning. We attempt to inject
unstructured textual knowledge into PTMs after
downstream tuning. Existing methods mainly focus
on enhancing PTMs with structural knowledge dur-
ing pre-training or fine-tuning (Zhang et al., 2019;
Wang et al., 2021; Bosselut et al., 2019). These
methods require retraining the task-specific models
to achieve knowledge injection, which thus cannot
be adopted in this setting. Therefore, we present
the results of the following models: (1) We adopt
T5(Raffel et al., 2020) and PlugD as the back-
bones, which are trained with only the queries as
inputs and do not utilize external document knowl-
edge in evaluation. (2) Based on the trained T5 and
PlugD, we adopt different post-processing methods
to incorporate document knowledge. For T5, we
directly concatenate the documents and queries as
inputs for evaluation ( +Concat ). For PlugD, we
insert the document knowledge with document plu-
gins ( +DPlug ). The setting is challenging as there
is a gap between the training and evaluation.
4.4 Plugging during Tuning
We present the comparison results between base-
line models and PlugD in Table 1. From this ta-
ble, we can observe that: (1) The baseline models
which generate task-agnostic document representa-ModelsFEVER NQ WoW zsRE
Acc. EM F1 F1 Acc.
T5 79.10 11.35 17.11 16.59 2.52
+Concat 76.84 14.45 22.16 14.26 19.17
âˆ† -2.26 +3.1 +5.05 -2.33 +16.65
PlugD 79.56 11.17 16.39 16.58 2.23
+DPlug 82.54 23.01 32.68 15.28 21.13
âˆ† +2.98 +11.84 +16.29 -1.03 +18.90
Table 3: The main results of our proposed PlugD and
baselines for plugging after tuning.
tions perform worse than the corresponding mod-
els which generate task-specific representations.
It indicates that decoupling document representa-
tion from concrete tasks is challenging and existing
methods cannot achieve satisfactory performance.
(2) Compared with the task-agnostic baseline mod-
els (ED2LM fand EmbRecy f), PlugD can achieve
significant performance improvements across dif-
ferent tasks. Besides, compared with ED2LM and
EmbRecy, PlugD can also achieve superior results
on many datasets, especially for parameter-efficient
tuning. In addition, ED2LM and EmbRecy need
to generate document representations for different
tasks separately. Thus they require more storage
than PlugD. In contrast, PlugD can generate infor-
mative unified representations with fewer storage
requirements and achieve superior results across
different tasks. (3) Compared with the traditional
encoding-task coupling model (UpperBound), shar-
ing document representation across different tasks
in PlugD only leads to a limited performance drop
(39.68vs.41.22, and 40.61vs.41.79on average).
And as PlugD does not need to encode documents
during downstream tuning and inference, PlugD en-
ables significant computational acceleration. The
results suggest that PlugD can effectively capture
document semantics and inject them into the PTM
to provide knowledge. (4) Even PlugD without fur-
ther plugin learning can outperform the baselines
on several datasets. It proves that PlugD benefits
from both the self-supervised tasks and the model
architecture. Besides, it also indicates that the con-
textualized document representations generated by
the original PTM (PlugD w/o PT) are powerful if
we utilize them correctly.
Computational Cost. We compare the computa-
tional cost of PlugD and baseline models. Here, we
present the floating point operations (FLOPs) and
calculation time required to process one data in in-
ference for each method. We assume that the docu-
ment, query, and answer contain 512,48, and 32to-

--- PAGE 8 ---
Datasets FEVER NQ WoW zsRE
Acc. EM F1 F1 Acc.
PlugD 86.56 41.54 49.76 17.15 32.12
w/ RSP 86.17 41.23 49.21 16.98 31.66
w/ NSG 86.03 40.80 49.06 17.62 28.92
w/o PT 86.33 40.24 47.72 17.07 30.64
Table 4: The results of ablation study.
kens, respectively. The results are shown in Table 2.
From the results, we can observe that: (1) The
methods for task-agnostic representation require
much less computational cost than encoding-task
coupling methods. Especially, our method PlugD
can achieve 3.25Ã—speed up ( 139.3GFLOPs vs.
453.1GFLOPs). (2) The methods for task-agnostic
representation generally are inferior to encoding-
task coupling methods. PlugD can achieve better
average scores than other baselines and preserve
low computational costs. (3) Both task-agnostic
and query-agnostic models need to pre-encode and
store document representations before downstream
tuning for inference speed up. However, models
generating query-agnostic and task-specific repre-
sentations require separate document representa-
tions for each task. In contrast, our PlugD generates
task-agnostic representations for all tasks, resulting
in better results and lower storage requirements.
4.5 Plugging after Tuning
The comparison results are shown in Table 3. From
the results, we can observe that: (1) Both T5
and PlugD cannot achieve consistent improvement
from post-processing knowledge injection on these
tasks. It proves that plugging after tuning is a chal-
lenging setting as there is a gap between training
and evaluation. (2) PlugD can achieve significant
improvement on FEVER, NQ, and zsRE, which
further indicates the effectiveness of PlugD. How-
ever, PlugD cannot achieve improvement on WoW.
As the ability to acquire knowledge from the doc-
ument plugins is obtained from plugin learning,
further downstream task tuning may lead the mod-
els to forget the ability. Thus, even PlugD can
not achieve consistent improvement. (3) Without
document knowledge, PlugD and T5 achieve com-
parable results. It indicates that the plugin learning
process does not improve the fundamental ability
of PTMs. The improvement achieved by PlugD in
both plugging during/after tuning settings comes
from the effective plug-and-play framework.
FEVER NQ WoW zsRE Average
T arget T askFEVER
NQ
WoW
zsRE
PlugDSource T ask96 83 86.9 88.8 90.8
95.3 92.3 87.9 95.5 93.9
95.5 85.5 88.9 91 91.7
93.6 85.6 87.2 97 91.7
100 100 100 100 100Figure 3: Relative transfer performance (transfer perfor-
mance / PlugDâ€™s performance)(%).
4.6 Ablation Study
In this section, we conduct an ablation study to ver-
ify the effectiveness of our proposed plugin learn-
ing tasks. We show the results of the models, which
are trained with only recurring span prediction task
(w/ RSP ), with only next sentence generation task
(w/ NSG ), or without plugin learning ( w/o PT ).
We evaluate the models on four datasets for the
plugging during tuning setting.
The results are shown in Table 4. We can find
that (1) PlugD without plugin learning leads to a
significant performance drop, which further indi-
cates that the proposed training tasks can help the
PTM to effectively encode the document knowl-
edge into plugins. (2) Two tasks can cooperate
with each other to improve the model performance.
Though training PlugD with only one task will
lead to performance deterioration on some tasks,
training with two tasks can achieve consistent im-
provement over the model without plugin learning.
(3) When PlugD is trained with only NSG, the
model can achieve superior results for WoW. But
the task harms the performance for FEVER and
zsRE. This is because NSG requires the model to
generate long sentences, which is similar to WoW,
while FEVER and zsRE only require short outputs.
In contrast, training with only RSP will also lead
to a performance drop for WoW. It indicates that
diverse plugin learning tasks are important for ex-
pressive document plugins.
4.7 Transferability Analysis
In this section, we want to explore the effective-
ness of supervised tasks on document representa-
tion transferability. Here we present the results
of ED2LM, which can outperform other baselines.
Specifically, we train the task-specific document en-

--- PAGE 9 ---
coder on a source task, and then reuse the encoder
on other target tasks to continually train the rest of
the model. The results are shown in Figure 3.
From the results, we can observe that 1) The
non-diagonal values of the matrix are consistently
smaller than the diagonal values. It suggests that
training the document encoder with existing super-
vised tasks can hardly benefit other target tasks.
PlugD trained with two self-supervised objectives
can provide transferable document representation
and achieve superior results. 2) The encoders
trained on the NQ dataset can outperform encoders
trained on other tasks. It indicates that training with
challenging tasks may lead to better performance.
5 Conclusion
In this paper, we explore a new paradigm, which
aims to represent documents as pluggable mod-
ules for PTMs. In this setting, we can get rid of
encoding the same document multiple times for
different tasks. The extensive experiments prove
that our proposed PlugD can significantly reduce
the computational cost and effectively inject doc-
ument knowledge into PTMs to improve perfor-
mance. In the future, we will explore more effec-
tive plugin learning tasks and further attempt to
represent knowledge graphs, and figures as plugins
to provide knowledge for PTMs.
Limitations
We discuss the limitations of PlugD in this sec-
tion: (1) We explore decoupling document encod-
ing from concrete tasks in this paper, and propose
to represent documents as pluggable modules be-
fore task adaptation. Therefore, PlugD has a higher
storage requirement than conventional encoding-
coupling methods. We encourage (2) In the experi-
ments, we adopt T5 as our PTM backbone. Actu-
ally, the proposed framework can also be applied
to more pre-trained models with various model ar-
chitectures. Besides, recent trends show that larger
models tend to build more expressive text repre-
sentations. It is worth exploring PlugD with larger
PTMs with billions of parameters to learn informa-
tive document plugins. (3) In this paper, we adopt
an external retriever to retrieve relevant documents
for each input query. Recent progress in retrieval-
augmented language models shows that training
the PTMs with an end-to-end textual knowledge re-
triever can promote downstream performance. We
believe document plugins can also serve as the ex-ternal knowledge base and enhancing PlugD with
end-to-end retrieval is a promising direction.
Acknowledgement
This work is supported by the National Key R&D
Program of China (No. 2020AAA0106502), Na-
tional Natural Science Foundation of China (No.
62236004).
Author Contributions Chaojun Xiao and Chi-
Min Chan wrote the code and conducted the exper-
iments. Chaojun Xiao, Zhengyan Zhang, and Xu
Han wrote the initial draft. Yankai Lin, Zhiyuan
Liu, and Xiangyang Li significantly edited and im-
proved the paper. Zhonghua Li, Zhao Cao, and
Maosong Sun provided valuable advice to the re-
search.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, Roman Ring, Eliza Rutherford, Serkan
Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,
Marianne Monteiro, Jacob L. Menick, Sebastian
Borgeaud, Andy Brock, Aida Nematzadeh, Sahand
Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and KarÃ©n Si-
monyan. 2022. Flamingo: a visual language model
for few-shot learning. In NeurIPS .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. CoRR ,
abs/2004.05150.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Anal. Mach. Intell. ,
35(8):1798â€“1828.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, Shyamal Buch, Dal-
las Card, Rodrigo Castellon, Niladri S. Chatterji,
Annie S. Chen, Kathleen Creel, Jared Quincy
Davis, Dorottya Demszky, Chris Donahue, Moussa
Doumbouya, Esin Durmus, Stefano Ermon, John
Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea
Finn, Trevor Gale, Lauren Gillespie, Karan Goel,
Noah D. Goodman, Shelby Grossman, Neel Guha,
Tatsunori Hashimoto, Peter Henderson, John He-
witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing
Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keel-
ing, Fereshte Khani, Omar Khattab, Pang Wei Koh,
Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,
and et al. 2021. On the opportunities and risks of
foundation models. CoRR , abs/2108.07258.

--- PAGE 10 ---
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi.
2019. COMET: commonsense transformers for auto-
matic knowledge graph construction. In Proceedings
of the ACL , pages 4762â€“4779.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Proceedings of NeurIPS .
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of ACL , pages
1870â€“1879.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR , abs/2204.02311.
Alexandra Chronopoulou, Matthew E. Peters, and Jesse
Dodge. 2022. Efficient hierarchical domain adapta-
tion for pretrained language models. In Proceedings
of NAACL-HLT , pages 1336â€“1351.
Alexis Conneau, Douwe Kiela, Holger Schwenk, LoÃ¯c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
EMNLP , pages 670â€“680.
Andrew M. Dai, Christopher Olah, and Quoc V . Le.
2015. Document embedding with paragraph vectors.
CoRR , abs/1507.07998.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neuronsin pretrained transformers. In Proceedings of ACL ,
pages 8493â€“8502.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu. 2020. Plug and play language models:
A simple approach to controlled text generation. In
Proceedings of ICLR .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of NAACL-HLT , pages
4171â€“4186.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In Proceedings of ICLR .
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,
Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei
Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong
Sun. 2022. Delta tuning: A comprehensive study of
parameter efficient methods for pre-trained language
models. CoRR , abs/2203.06904.
Jingfei Du, Myle Ott, Haoran Li, Xing Zhou, and
Veselin Stoyanov. 2020. General purpose text embed-
dings from pre-trained language models for scalable
inference. In Findings of EMNLP , volume EMNLP
2020, pages 3018â€“3030.
Hady ElSahar, Pavlos V ougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon S. Hare, FrÃ©dÃ©rique
Laforest, and Elena Simperl. 2018. T-rex: A large
scale alignment of natural language with knowledge
base triples. In Proceedings of LREC .
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
long form question answering. In Proceedings of
ACL, pages 3558â€“3567.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. In Proceedings of EMNLP , pages 6894â€“
6910.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao
Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang,
Liang Zhang, Wentao Han, Minlie Huang, Qin Jin,
Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu,
Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen,
Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. 2021.
Pre-trained models: Past, present and future. AI
Open , 2:225â€“250.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In Pro-
ceedings of ICML , volume 97, pages 2790â€“2799.

--- PAGE 11 ---
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large
language models. CoRR , abs/2106.09685.
Kai Hui, Honglei Zhuang, Tao Chen, Zhen Qin,
Jing Lu, Dara Bahri, Ji Ma, Jai Prakash Gupta,
CÃ­cero Nogueira dos Santos, Yi Tay, and Donald Met-
zler. 2022. ED2LM: encoder-decoder to language
model for faster document re-ranking inference. In
Findings of ACL , pages 3747â€“3758.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know. TACL , 8:423â€“438.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of ACL , pages 1601â€“1611.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
EMNLP , pages 6769â€“6781.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Proceedings of NeurIPS , pages 3294â€“3302.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. TACL , 7:452â€“466.
Anne Lauscher, Tobias LÃ¼ken, and Goran Glavas. 2021.
Sustainable modular debiasing of language models.
InFindings of ACL: EMNLP , pages 4782â€“4797.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of EMNLP , pages 3045â€“
3059.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-
moyer. 2017. Zero-shot relation extraction via read-
ing comprehension. In Proceedings of CoNLL , pages
333â€“342.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of ACL-IJCNLP , pages 4582â€“4597.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
CoRR , abs/2107.13586.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth
Dathathri, and Pascale Fung. 2020. Plug-and-play
conversational models. In Findings of EMNLP , vol-
ume EMNLP 2020, pages 2422â€“2433.
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and
Behnam Neyshabur. 2022. Long range lan-
guage modeling via gated state spaces. CoRR ,
abs/2206.13947.
TomÃ¡s Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NeurIPS , pages 3111â€“
3119.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D. Manning. 2022. Fast model
editing at scale. In Proceedings of ICLR .
Damian Pascual, Beni Egressy, Clara Meister, Ryan
Cotterell, and Roger Wattenhofer. 2021. A plug-
and-play method for controlled text generation. In
Findings of EMNLP , pages 3973â€“3997.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP , pages
1532â€“1543.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT , pages
2227â€“2237.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
S. H. Lewis, Majid Yazdani, Nicola De Cao, James
Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim RocktÃ¤schel, and
Sebastian Riedel. 2021. KILT: a benchmark for
knowledge intensive language tasks. In Proceedings
of NAACL-HLT , pages 2523â€“2544.
Fabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel,
Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander H. Miller. 2019. Language models
as knowledge bases? In Proceedings of EMNLP-
IJCNLP , pages 2463â€“2473.
Jonas Pfeiffer, Aishwarya Kamath, Andreas RÃ¼cklÃ©,
Kyunghyun Cho, and Iryna Gurevych. 2021.
Adapterfusion: Non-destructive task composition for
transfer learning. In Proceedings of EACL , pages
487â€“503.
Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-
tian Ruder. 2020. MAD-X: an adapter-based frame-
work for multi-task cross-lingual transfer. In Pro-
ceedings of EMNLP , pages 7654â€“7673.

--- PAGE 12 ---
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing with unsupervised learning.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. JMLR , 21:140:1â€“140:67.
Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-
son, and Omer Levy. 2021. Few-shot question an-
swering by pretraining span selection. In Proceed-
ings of ACL/IJCNLP , pages 3066â€“3079.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of EMNLP-IJCNLP , pages 3980â€“
3990.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the parame-
ters of a language model? In Proceedings of EMNLP ,
pages 5418â€“5426.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. A primer in bertology: What we know about
how BERT works. Trans. Assoc. Comput. Linguis-
tics, 8:842â€“866.
Jon Saad-Falcon, Amanpreet Singh, Luca Soldaini,
Mike Dâ€™Arcy, Arman Cohan, and Doug Downey.
2022. Embedding recycling for language models.
CoRR , abs/2207.04993.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. REPLUG: retrieval-augmented
black-box language models. CoRR , abs/2301.12652.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. 2022. Efficient transformers: A survey. ACM
Computing Surveys , 55(6):1â€“28.
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
verification. In Proceedings of NAACL-HLT , pages
809â€“819.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NeurIPS , pages 5998â€“
6008.
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,
Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin
Jiang, and Ming Zhou. 2021. K-adapter: Infusing
knowledge into pre-trained models with adapters. In
Findings of ACL , volume ACL/IJCNLP 2021, pages
1405â€“1418.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned lan-
guage models are zero-shot learners. In Proceedings
of ICLR . OpenReview.net.
Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu,
Avinash Balakrishnan, Pin-Yu Chen, Pradeep Raviku-
mar, and Michael J. Witbrock. 2018. Word moverâ€™s
embedding: From word2vec to document embedding.
InProceedings of EMNLP , pages 4524â€“4534.
Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu,
Chenguang Zhu, and Julian McAuley. 2023. Small
models are valuable plug-ins for large language mod-
els.CoRR , abs/2305.08848.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of EMNLP , pages 2369â€“2380.
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.
2023. Augmentation-adapted retriever improves gen-
eralization of language models as a zero-shot plug-in.
InProceedings of ACL .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
taÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
formers for longer sequences. In Proceedings of
NeurIPS .
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. In
Proceedings of ACL , pages 1â€“9.
Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li,
Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021.
Poolingformer: Long document modeling with pool-
ing attention. In Proceedings of ICML , volume 139,
pages 12437â€“12446.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,
Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced
language representation with informative entities. In
Proceedings of ACL , pages 1441â€“1451.

--- PAGE 13 ---
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong
Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan
Liu, Peng Li, Maosong Sun, and Jie Zhou. 2023.
Plug-and-play knowledge injection for pre-trained
language models. In Proceedings of ACL .
Junru Zhou, Zhuosheng Zhang, Hai Zhao, and Shuail-
iang Zhang. 2020. LIMIT-BERT : Linguistics in-
formed multi-task BERT. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2020, Online Event, 16-20 November 2020 , volume
EMNLP 2020 of Findings of ACL , pages 4450â€“4461.A Appendix
A.1 Discussion
In this paper, we propose to decouple document
encoding from concrete tasks, achieving encoding
documents once and for all across different tasks.
In this section, we further discuss the potential of
PlugD.
Unified Model across Multiple Tasks. Recently,
with the rapid progress of large-scale PTMs, han-
dling multiple tasks with a unified PTM has re-
ceived rapidly increasing attention. For example,
many researchers explore instruction tuning (Sanh
et al., 2022; Wei et al., 2022) to enable a uni-
fied PTM to perform multiple tasks with natural
language description. We attempt to extend the
paradigm to document representation, enhancing
the unified PTM with unified document represen-
tation across multiple tasks. In this way, we can
provide the PTM with various external knowledge
flexibly and efficiently, avoiding encoding docu-
ments multiple times for different tasks and user
input queries.
Heterogeneous Knowledge Base. Enhancing
large-scale PTMs with various knowledge is an
important topic for natural language processing.
Many researchers attempt to incorporate knowl-
edge graphs (Zhang et al., 2019; Wang et al., 2021),
linguistic knowledge (Zhou et al., 2020) into PTMs.
We argue that PlugD provides a new way for knowl-
edge injection. We can encode various knowledge,
such as images, and knowledge graphs, into the
plugins of PTMs. In this way, we can build a het-
erogeneous plugin knowledge base for PTMs to
improve downstream performance.
Continual Learning. Previous researches show
that PTMs can implicitly encode knowledge in the
model parameters (Petroni et al., 2019; Jiang et al.,
2020; Roberts et al., 2020; Dai et al., 2022; Mitchell
et al., 2022), which is not editable for continual up-
dates. PlugD provides a new way for the continual
learning of PTMs. We can insert and update new
knowledge for PTMs by continually learning and
updating new document plugins, which will be fur-
ther utilized to provide knowledge to PTMs.
A.2 Document Retrieval
In this paper, we adopt dense passage retrieval,
DPR (Karpukhin et al., 2020), to retrieve relevant
documents for each input query. Following Petroni
et al. (2021), we adopt R-Precision, Precision@k
and Recall@k, as the evaluation metrics. We adopt

--- PAGE 14 ---
Datasets FEVER NQ TriviaQA HotpotQA ELI5 WoW zsRE TRex
R-Precision 56.29 55.97 46.76 25.58 16.46 27.37 14.72 43.74
Precision@3 24.01 27.71 24.82 2.82 10.62 15.04 6.87 18.40
Recall@3 70.25 59.25 51.64 8.46 23.90 45.12 19.20 55.21
Table 5: The results of document retrieval for each dataset.
the evaluation scripts provided by the KILT paper.
Please refer to the original KILT paper for the de-
tails of the metrics. From the results, we can see
that the retrieval performance is not satisfactory
for some datasets, which may bring the noise in
the downstream tasks. And we encourage the com-
munity to develop retrievers, which can achieve
satisfactory performance across different tasks.
A.3 Impacts of Insertion Layers
Datasets FEVER NQ WoW zsRE
Acc. EM F1 F1 Acc.
PlugD (6) 85.22 39.78 48.12 17.15 28.44
PlugD (12) 86.33 40.24 47.72 17.07 30.64
PlugD (24) 86.64 40.52 48.77 16.86 29.14
Table 6: The results of PlugD with different number
of insertion layers. Here PlugD ( n) indicates that the
document plugins are inserted into the top- nlayers.
PlugD inserts the document plugins into the self-
attention layers to provide document knowledge.
As the pre-trained models tend to capture linguistic
features in the bottom layers and capture the task-
specific information in the top layers (Rogers et al.,
2020). Therefore, to reduce computational costs,
we only insert the document plugins in the top
layers. In this section, we explore the impact of
insertion layers of document plugins. We present
the results of PlugD with document plugins inserted
in the last 6layers, 12layers, and all 24 layers.
Here, we do not conduct plugin learning for PlugD
to speed up experiments.
The results are shown in Table 6. From the re-
sults, we can see that: (1) With the increasing of
insertion layers, the performance on FEVER and
NQ improves. But PlugD with document plugins in
all layers can not outperform the PlugD with docu-
ment plugins in the top layers on WoW and zsRE.
That is because the fact verification and question
answering tasks require the models to select useful
information via both lexical matching and seman-
tic matching. In contrast, the dialogue generation
and slot filling tasks rely on document semantics
to provide knowledge, and inserting the documentplugins in the bottom layers can not benefit the per-
formance. (2) The three models can obtain similar
performance on these tasks. Therefore, in order to
reduce the computational costs and maintain the
performance, we only insert document plugins in
the top 12layers for other experiments.
A.4 Impacts of Plugin Sharing across Layers
As mentioned in previous sections, PlugD inserts
the same prefix tokens for different attention layers
to save the storage. In this section, we study the
impacts of sharing plugins across different layers.
To this end, we attempt to insert different prefix
tokens for different layers. Specifically, we encode
the document dto obtain the raw hidden state Hl
d
from the l-th layer, and then adopt the mapping
network tailored to the l-th layer to map the hidden
state into the prefix tokens. The prefix tokens are
then inserted into the l-th layer for query encoding.
Similar to PlugD, we insert the representations into
the top 12layers for this model. We term the model
as All-Hidden.
The comparison results are shown in Table 7.
From the results, we can observe that All-Hidden
can achieve superior results on three datasets, in-
cluding FEVER, WoW, and zsRE. But All-Hidden
requires 12Ã—storage than PlugD, which is imprac-
tical for large-scale document collections. And
PlugD can achieve comparable performance to All-
Hidden. Therefore, to reduce the storage require-
ment, we choose to share the document plugins
across different attention layers.
A.5 Experimental Details
Model Implementation. The mapping network
of document plugins is used to map the raw docu-
ment representations into the document plugins
for different tasks. Given a hidden vector, hi,
we calculate the corresponding prefix token as
pi=hi+W2
mReLU (W1
mhi), where hiâˆˆRd,
W1
mâˆˆRdÃ—2d,W2
mâˆˆR2dÃ—d, and dâˆˆRis the
hidden size.
As for the parameter-efficient tuning method, we
adopt adapter layers to tune the model. We add
the adapters after the layernorm operation of feed-

--- PAGE 15 ---
Datasets FEVER NQ WoW zsRE
Acc. EM F1 F1 Acc.
PlugD 85.22 39.78 48.12 17.15 28.44
All-Hidden 86.73 39.46 47.71 17.28 32.20
Table 7: The comparison results of PlugD and All-
Hidden that does not share plugins across layers.
forward layers. The parameters of adapters are
randomly initialized following a zero-mean Gaus-
sian distribution with standard deviation as 10âˆ’2.
Plugin Learning. For the recurring span predic-
tion task, we first identify spans that occur multiple
times from the documents. Then we filter out the
stopwords and personal pronouns, and keep the
longest 15spans as the recurring spans for further
masking. Then we randomly sample 5sentences,
which contain the recurring spans from the docu-
ment as the query. For the next sentence generation
task, we randomly sample three consecutive sen-
tences from the documents, where the first sentence
is treated as the query, and the last two sentences
are treated as the answers. The model is trained in
a multi-task fashion, and 70% documents are used
for recurring span prediction, and 30% documents
are used for next sentence generation. The maxi-
mal length for queries and answers are set as 196
and128, respectively. We set the learning rate as
2Ã—10âˆ’5and batch size as 256.
Downstream Task Tuning. For downstream
tasks, we set the training batch size as 64.
The learning rate is selected from {10âˆ’4,5Ã—
10âˆ’4,10âˆ’3}for PET. And as full-parameter fine-
tuning require amounts of computation, we do
not conduct grid search for this setting. We set
the learning rate for full-parameter fine-tuning as
2Ã—10âˆ’5. For fact verification, we take the claims
as the input queries and take the logits of â€œyes"
and â€œno" for classification. For other tasks, we
treat them as text-to-text generation problems, and
during the inference, we adopt the greedy strategy
for decoding. The evaluation scripts are written by
our own, and will be released with the paper.
