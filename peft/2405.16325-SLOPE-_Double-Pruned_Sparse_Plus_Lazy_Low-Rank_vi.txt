# 2405.16325.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2405.16325.pdf
# Kích thước tệp: 1330155 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SLOPE: Huấn luyện Trước Bộ Điều Chỉnh Thứ Hạng Thấp Lười Biếng Cộng Thưa Thớt Cắt Tỉa Kép cho các Mô hình Ngôn ngữ Lớn

Mohammad Mozaffari Amir Yazdanbakhsh
Khoa Khoa học Máy tính Google DeepMind
Đại học Toronto Mountain View, Hoa Kỳ
mmozaffari@cs.toronto.edu ayazdan@google.com

Zhao Zhang Maryam Mehri Dehnavi
Khoa Kỹ thuật Điện và Máy tính Khoa Khoa học Máy tính
Đại học Rutgers Đại học Toronto
zhao.zhang@rutgers.edu mmehride@cs.toronto.edu

Tóm tắt
Chúng tôi đề xuất SLOPE, một phương pháp Huấn luyện Trước Bộ Điều Chỉnh Thứ Hạng Thấp Lười Biếng Cộng Thưa Thớt Cắt Tỉa Kép cho các mô hình ngôn ngữ lớn (LLM) nhằm cải thiện độ chính xác của các LLM thưa thớt đồng thời tăng tốc quá trình huấn luyện trước và suy luận cũng như giảm dung lượng bộ nhớ của chúng. Huấn luyện trước thưa thớt của các LLM làm giảm độ chính xác của mô hình, để khắc phục điều này, các nghiên cứu trước đây sử dụng các mô hình dày đặc trong quá trình tinh chỉnh. SLOPE cải thiện độ chính xác của các mô hình được huấn luyện trước thưa thớt bằng cách thêm các bộ điều chỉnh thứ hạng thấp trong 1% lần lặp cuối cùng của quá trình huấn luyện trước mà không thêm chi phí đáng kể vào quá trình huấn luyện trước và suy luận của mô hình. Ngoài ra, SLOPE sử dụng công thức lan truyền ngược cắt tỉa kép để cắt tỉa ma trận trọng số chuyển vị bằng cách sử dụng cấu trúc thưa thớt N:M để cho phép lan truyền ngược thưa thớt được tăng tốc. SLOPE tăng tốc quá trình huấn luyện và suy luận của các mô hình với hàng tỷ tham số lên đến 1.25× và 1.54× tương ứng (OPT-33B và OPT-66B) đồng thời giảm việc sử dụng bộ nhớ của chúng lên đến 0.63× và 0.61× cho huấn luyện và suy luận tương ứng.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) thể hiện tiềm năng đáng kể cho việc hiểu và tạo ra ngôn ngữ tự nhiên; tuy nhiên, chúng tốn kém để huấn luyện và thực thi do số lượng tham số lớn và khối lượng dữ liệu huấn luyện cần thiết. Quá trình huấn luyện của LLM bao gồm giai đoạn huấn luyện trước [45] và giai đoạn tinh chỉnh. Trong giai đoạn huấn luyện trước, mô hình được huấn luyện trên một tập văn bản chất lượng cao lớn [17,1] và sau đó được tinh chỉnh trên các tác vụ hạ nguồn khác nhau [57,48]. Cả hai giai đoạn đều yêu cầu lượng tính toán, bộ nhớ và truyền thông đáng kể.

Độ thưa thớt của mô hình, trong đó các phần ít quan trọng của mô hình được cắt tỉa, có thể giảm chi phí tính toán và bộ nhớ của quá trình huấn luyện trước LLM [24]. Độ thưa thớt không có cấu trúc nếu các phần tử được loại bỏ từ các vị trí tùy ý trong các tensor. Độ thưa thớt không có cấu trúc khó tăng tốc do không có hỗ trợ phần cứng/phần mềm [58]. Để giải quyết vấn đề này, độ thưa thớt có cấu trúc áp đặt các ràng buộc về nơi các phần tử không có thể xuất hiện [28,33], tạo ra các khối dày đặc của các phần tử khác không trong ma trận để tận dụng các quy trình tính toán dày đặc. Nhược điểm của các phương pháp thưa thớt có cấu trúc là chúng hạn chế sự lựa chọn cho các mẫu thưa thớt dẫn đến giảm độ chính xác trong mô hình thưa thớt so với dày đặc [9]. NVIDIA gần đây đã giới thiệu các lõi tensor thưa thớt [40] cho phần cứng của họ để tăng tốc các mẫu thưa thớt có cấu trúc linh hoạt hơn, tức là thưa thớt 2:4; hỗ trợ phần cứng cho thưa thớt N:M trong đó nhiều nhất N trong M phần tử liên tiếp là không vẫn chưa có sẵn nhưng các chuyên gia học máy đang phát triển thuật toán cho các mẫu này [29, 34, 47].

Áp dụng mặt nạ thưa thớt N:M cho một mô hình dẫn đến mất độ chính xác do sự lựa chọn hạn chế của các mẫu thưa thớt. Thay đổi mặt nạ thưa thớt một cách động trong suốt quá trình huấn luyện trước là một trong những phương pháp được đề xuất để giải quyết vấn đề này [12]. Zhou et al. [61] đề xuất một thước đo mới để tìm các mẫu thưa thớt N:M dẫn đến độ chính xác cao hơn trong mỗi lần lặp. [29] đề xuất việc sử dụng mặt nạ suy giảm để cải thiện thêm độ chính xác của các mô hình. STEP [34] đề xuất một trình tối ưu hóa mới cải thiện sự hội tụ của các mô hình với mặt nạ thích ứng. Trong khi các phương pháp thích ứng có thể cải thiện độ chính xác của các mô hình, chúng yêu cầu lưu trữ các trọng số dày đặc và có thể các thước đo bổ sung để cập nhật các mẫu thưa thớt mới, đồng thời lãng phí một phần các tính toán huấn luyện để huấn luyện các trọng số sẽ bị cắt tỉa trong các lần lặp sau. SPDF [55] và Huấn luyện Trước Thưa Thớt-Dày Đặc (FST) [26], người ta có thể bù đắp cho sự mất mát do thưa thớt gây ra bằng tinh chỉnh dày đặc. Nhưng giai đoạn tinh chỉnh dày đặc sẽ vô hiệu hóa việc tiết kiệm bộ nhớ và tính toán của các phương pháp thưa thớt trong quá trình suy luận. Được truyền cảm hứng từ điều này, chúng tôi giới thiệu các phần tử khác không bổ sung vào trọng số trong các bước cuối của quá trình huấn luyện trước. Để tránh lưu trữ một mô hình dày đặc trong quá trình suy luận trong khi vẫn có được các khả năng tương tự của một trọng số dày đặc, chúng tôi thêm các phần tử khác không dưới dạng các bộ điều chỉnh thứ hạng thấp [25]. Các thí nghiệm của chúng tôi cho thấy việc sử dụng bộ điều chỉnh thứ hạng thấp dẫn đến sự hội tụ nhanh hơn đáng kể so với khi cùng số lượng tham số có thể học được được thêm vào các trọng số thưa thớt.

Việc sử dụng thưa thớt N:M trong huấn luyện trước LLM bị hạn chế để tăng tốc lan truyền thuận trong vòng lặp huấn luyện vì cấu trúc N:M theo hàng trong mẫu thưa thớt trọng số sẽ bị mất khi các trọng số được chuyển vị trong lan truyền ngược. Các nghiên cứu trước [27,60,26] cố gắng tận dụng thưa thớt trong cả lan truyền thuận và ngược bằng cách tìm các mặt nạ có thể chuyển vị thông qua các phương pháp khác nhau: thuật toán tìm kiếm tham lam, tìm kiếm giữa các hoán vị ngẫu nhiên, và tìm kiếm giữa các kết quả của phép tích chập. Tuy nhiên, các mặt nạ có thể chuyển vị này làm giảm độ chính xác của mô hình và thêm chi phí thời gian chạy đáng kể [26], thường dẫn đến chậm lại (lên đến 8.4×). Để giải quyết các vấn đề này, chúng tôi đề xuất một công thức lan truyền ngược cắt tỉa kép với các đảm bảo hội tụ lý thuyết. Thay vì ép buộc chuyển vị trọng số phải thưa thớt N:M, phương pháp của chúng tôi chuyển vị ma trận trọng số N:M trước rồi sau đó áp đặt thưa thớt N:M. Điều này cho phép các ma trận trọng số thể hiện một phạm vi rộng hơn của các mẫu thưa thớt, dẫn đến cải thiện độ chính xác.

Phương pháp của chúng tôi, SLOPE, là một phương pháp Huấn luyện Trước Bộ Điều Chỉnh Thứ Hạng Thấp Lười Biếng Cộng Thưa Thớt Cắt Tỉa Kép cho các LLM. Nó sử dụng mặt nạ thưa thớt N:M tĩnh với công thức lan truyền ngược cắt tỉa kép để tăng tốc cả lan truyền thuận và ngược. Các đóng góp chính của SLOPE là:

• Lan truyền ngược Cắt tỉa Kép → Chúng tôi đề xuất chuyển vị một ma trận trọng số N:M đã được thưa thớt hóa (lan truyền thuận) trước khi áp đặt một vòng thưa thớt N:M khác (lan truyền ngược), cải thiện chất lượng mô hình và giảm chi phí tìm kiếm mặt nạ.

• Bộ điều chỉnh Thứ hạng Thấp Lười biếng → Chúng tôi giới thiệu các tham số bổ sung với chi phí tính toán và bộ nhớ tối thiểu, chỉ cho 1% lần lặp cuối cùng của quá trình huấn luyện trước, cải thiện khả năng của mô hình (xem Hình 1).

• Các nhân CUDA được tối ưu hóa → Chúng tôi tối ưu hóa chung các nhân thưa thớt 2:4 của Nvidia và các lời gọi thứ hạng thấp thông qua việc chia ô và lập lịch hiệu quả. Các nhân CUDA được tối ưu hóa cao của chúng tôi dẫn đến tăng tốc huấn luyện đầu cuối 1.25× và tăng tốc suy luận 1.54× trên các LLM với hàng tỷ tham số, đồng thời giảm dung lượng bộ nhớ huấn luyện và suy luận lên đến 0.63× và 0.61×, tương ứng.

2 Huấn luyện trước thưa thớt cộng thứ hạng thấp của LLM
Phương trình 1, 2, và 3 mô tả các công thức cho lan truyền thuận và ngược của lớp tuyến tính thứ i trong một mạng neural. Ở đây, tensor trọng số được ký hiệu là Wi∈Rdout×din và tensor đầu vào được ký hiệu là Xi∈Rb×din. Lan truyền thuận tạo ra một tensor đầu ra được biểu diễn là Yi∈Rb×dout. Trong tất cả các phương trình, din và dout đề cập đến các chiều đầu vào và đầu ra của lớp tương ứng.

FWD →|Yi=XiWT
i (1)
BWD −1→|∇WiL=∇YiLTXi (2)
BWD −2→|∇XiL=∇YiLWi (3)

Chiều mà dọc theo đó việc cắt tỉa N:M xảy ra tương ứng với chiều giảm trong phép nhân Ma trận-Ma trận. Nếu không có hạn chế này, phép toán Ma trận-Ma trận thưa thớt không thể được tăng tốc trên GPU [42]. Với hạn chế này trong tâm trí, để tận dụng thưa thớt trọng số trong lan truyền thuận và

--- TRANG 2 ---
lan truyền ngược, người ta cần cắt tỉa các phần tử dọc theo các cột của WT
i trong Phương trình 1 (FWD) và Wi trong Phương trình 3. Để thỏa mãn yêu cầu này, cần thiết phải cắt tỉa các phần tử của tensor trọng số Wi dọc theo cả chiều hàng và cột.

2.1 Lan truyền ngược cắt tỉa kép
Có thể sử dụng nhiều phương pháp khác nhau để khai thác thưa thớt N:M trong cả lan truyền thuận và ngược. Ví dụ, người ta có thể cắt tỉa tensor kích hoạt Xi trong FWD dọc theo chiều hàng và Wi trong BWD-2 dọc theo chiều cột. Mặc dù tồn tại các kết hợp đa dạng để cắt tỉa, trọng tâm của chúng tôi trong nghiên cứu này chủ yếu là việc thưa thớt hóa các tensor trọng số vì hai lý do: (a) việc thưa thớt hóa các tensor trọng số trực tiếp ảnh hưởng đến tài nguyên cần thiết để lưu trữ và phục vụ mô hình, và (b) các phát hiện ban đầu của chúng tôi chỉ ra rằng việc cắt tỉa các tensor trọng số trong cả lan truyền thuận và ngược có tác động bất lợi tương đối ít hơn đến chất lượng mô hình đầu cuối tổng thể. Thêm chi tiết về các thí nghiệm của chúng tôi có thể được tìm thấy trong J. Do đó, chúng tôi đặt ra một công thức lan truyền ngược cắt tỉa kép có thể tăng tốc hiệu quả các tính toán FWD và BWD-2.

Ngoài ra, chúng tôi chứng minh rằng việc hiện thực hóa các tensor trọng số đã cắt tỉa như vậy, mặc dù có tổn thất, vẫn thể hiện các tính chất hội tụ. Trong phần còn lại của bài báo này, chúng tôi biểu diễn tensor trọng số chịu cắt tỉa theo hàng là WR
i, trong khi việc cắt tỉa đồng thời theo hàng và cột (cắt tỉa kép) được trình bày là WR,C
i. Chúng tôi viết lại các phương trình huấn luyện để phù hợp với những sửa đổi này, với các thay đổi được đề xuất được làm nổi bật bằng màu xanh:

FWD →|Yi=XiWR
iT(4)
BWD −1→|∇WiL=∇YiLTXi (5)
BWD −2→|∇XiL=∇YiLWR,C
i (6)

Chúng tôi gọi công thức này là "có tổn thất" vì ma trận trọng số trải qua mất mát thông tin trong quá trình lan truyền ngược so với trạng thái của nó trong lan truyền thuận.

--- TRANG 3 ---
Huấn luyện Trước N:M Thưa Thớt với Lan Truyền Ngược Có Tổn Thất (99% lần lặp) | Huấn luyện Trước Thưa Thớt + Thứ Hạng Thấp Lười Biếng (1% lần lặp)

Cắt Tỉa Theo Hàng | Cắt Tỉa Theo Hàng và Cột

[Hình 1 chứa các sơ đồ toán học với các ký hiệu và mũi tên mô tả quá trình huấn luyện thưa thớt]

Lan Truyền Thuận | Lan Truyền Ngược

Hình 1: Đường ống huấn luyện thưa thớt trong SLOPE. Ở đây, X, Y, và W biểu thị các tensor đầu vào, đầu ra, và trọng số cho một lớp cụ thể, tương ứng. ∇·L biểu thị gradient của hàm mất mát. L và R là các thành phần thứ hạng thấp chỉ được giới thiệu trong 1% lần lặp cuối cùng. Chỉ số trên R hiển thị việc cắt tỉa theo hàng sử dụng sơ đồ N:M và R, C hiển thị cả việc thưa thớt hóa N:M theo cột và hàng, dẫn đến các số không bổ sung được áp đặt. Các phần tử màu xanh biểu thị các giá trị khác không, trong khi các phần tử màu trắng biểu thị các giá trị đã cắt tỉa, và các phần tử màu đỏ chỉ ra các số không bổ sung được giới thiệu trong lan truyền ngược.

Sử dụng công thức này để huấn luyện, chúng tôi có thể tăng tốc cả lan truyền thuận và ngược nhờ sự tồn tại của thưa thớt N:M dọc theo cả hai chiều của các tensor trọng số.

Phân tích dung lượng bộ nhớ. Việc tạo ra thưa thớt có cấu trúc N:M không chỉ cải thiện hiệu quả tính toán của các phép toán GEMM mà còn giảm dung lượng bộ nhớ để lưu trữ các tensor thưa thớt. Tuy nhiên, cần lưu ý rằng việc lưu trữ meta-data phụ trợ trở nên cần thiết, chứa thông tin về vị trí của các phần tử khác không trong một ma trận hỗ trợ. Phương trình 7 phân định số bit cần thiết để lưu trữ các chỉ số trong định dạng thưa thớt N:M, trong đó ⌈.⌉ biểu thị hàm trần. Chúng tôi trình bày các kết quả chi tiết về việc giảm dung lượng bộ nhớ trong phần 3.

nN:M
index = ⌈log2(M/N)⌉ (7)

Phân tích hội tụ. Định lý 2.1 (chứng minh trong tiểu mục T.1) cho thấy thưa thớt bổ sung do cắt tỉa kép gây ra đối với một ma trận N:M đã được cắt tỉa theo hàng ban đầu. Theo bổ đề này, chúng tôi định lượng thưa thớt tăng thêm do cắt tỉa kép với các mẫu thưa thớt 1:2, 2:4, và 2:8 lần lượt là 12.5%, 9.375%, và 3.39%. Quan sát này nhấn mạnh rằng khi giá trị M trong N:M tăng, lượng dư các phần tử không trong ma trận cắt tỉa kép giảm. Việc giảm các phần tử không này do đó ngụ ý giảm lỗi tính toán, tăng cường tính bền vững của các tính toán. Chúng tôi mở rộng thêm các hiểu biết về hiện tượng này trong Phụ lục I.

Bổ đề 2.1. Xem xét một ma trận A được khởi tạo ngẫu nhiên. Theo ký hiệu của chúng tôi, chúng tôi biểu thị phiên bản cắt tỉa theo hàng của A bằng AR và phiên bản cắt tỉa chung theo cột và hàng của A bằng AR,C. Chúng tôi sử dụng D(.) để trình bày tỷ lệ mật độ của một ma trận. Phương trình 8 cho thấy các phần tử không bổ sung trong ma trận A được giới thiệu bởi cắt tỉa kép, trong đó s=N/M.

D(AR)−D(AR,C) = Σ(j=N+1 to M) (M choose j) * s^j * (1-s)^(M-j) * (j-N)/M (8)

Định lý 2.2 phát biểu rằng việc thay đổi động của mặt nạ theo cột trong Phương trình 5 trong mỗi lần lặp huấn luyện không gây tác động có hại đến sự hội tụ của trình tối ưu hóa. Hiện tượng này có thể được quy cho sự tương đương giữa vế trái của Phương trình 9, tương ứng với Phương trình 3 [BWD-2], và hiệu ứng trung bình hóa đạt được thông qua nhiều lần lặp huấn luyện của lan truyền ngược với mặt nạ thưa thớt khác biệt. Tuy nhiên, đối với các giá trị tùy ý của N và M, 4 và 5 có thể được sử dụng trong huấn luyện với đảm bảo hội tụ (chứng minh trong tiểu mục T.1). Mặt nạ thưa thớt được chọn ngẫu nhiên tại thời điểm khởi tạo, tức là tất cả các trọng số có cùng xác suất là không hoặc khác không. Điều này là bởi vì tại thời điểm khởi tạo, vị trí của các trọng số có cường độ lớn hơn là tùy ý. Sau khi chọn mặt nạ thưa thớt tại thời điểm khởi tạo, chúng tôi giữ mặt nạ cố định trong suốt toàn bộ quá trình huấn luyện. Chính sách này đảm bảo rằng mỗi phần tử trong trọng số có cùng xác suất là khác không tại thời điểm khởi tạo và thỏa mãn giả định trong Bổ đề 2.1.

Định lý 2.2. Giả sử một hàm mất mát L(W⟩,X⟩) cho một mẫu ngẫu nhiên Xi, và xem xét một mặt nạ ngẫu nhiên Mi, Phương trình 9 đúng, trong đó E[.] là toán tử kỳ vọng và ⊙ là phép nhân theo phần tử.

EXi[∇XiL(Wi, Xi)] = (M/N) * EMi[EXi[∇YiL(Wi, Xi)(M⊙Wi)]] (9)

2.2 Bộ điều chỉnh thứ hạng thấp lười biếng
Việc cắt tỉa các tensor trọng số trong tính toán FWD và BWD-2 là mong muốn cho hiệu quả tính toán nhưng có thể có tác động có hại đến chất lượng. Để giảm thiểu tác động bất lợi này đến chất lượng mô hình, chúng tôi tăng cường ma trận trọng số cắt tỉa kép với một ma trận thứ hạng thấp. Việc phân rã ma trận trọng số cắt tỉa kép, kết hợp với ma trận thứ hạng thấp, duy trì hiệu quả tính toán của phép nhân Ma trận-Ma trận thưa thớt trong quá trình lan truyền thuận và ngược. Đồng thời, phương pháp này hứa hẹn trong việc giảm thiểu các hiệu ứng bất lợi của cắt tỉa kép đối với chất lượng mô hình tổng thể.

Xem xét ma trận trọng số dày đặc, được ký hiệu bằng Wdense∈Rdout×din, Phương trình 10 minh họa việc phân rã ma trận được đề xuất. Trong biểu thức này, Wsparse ∈Rdout×din biểu thị một ma trận cắt tỉa kép và L∈Rdout×r và R∈Rr×din là các thành phần của xấp xỉ thứ hạng thấp. Biến r biểu thị thứ hạng của xấp xỉ thứ hạng thấp này. r hoạt động như một siêu tham số

--- TRANG 4 ---
kiểm soát sự đánh đổi giữa dung lượng bộ nhớ, hiệu quả tính toán, và chất lượng mô hình.

Wdense = Wsparse + LR (10)

Việc phân rã ma trận của ma trận cắt tỉa kép kết hợp với xấp xỉ ma trận thứ hạng thấp giảm dung lượng bộ nhớ của W từ dinّdout thành dinّdout * N/M + (din+dout)r, trong đó r << min(din, dout). Tuy nhiên, độ phức tạp tính toán của phép nhân Ma trận-Ma trận dày đặc thay đổi từ b*din*dout thành b*din*dout * N/M + b(din+dout)r. Cho giá trị r nhỏ hơn đáng kể so với b, din, và dout, công thức của chúng tôi hiệu quả giảm cả dung lượng bộ nhớ và độ phức tạp tính toán của phép nhân Ma trận-Ma trận bằng một hệ số M/N×.

Chúng tôi chứng minh thực nghiệm rằng tốc độ hội tụ của các bộ điều chỉnh thứ hạng thấp vượt qua các trọng số thưa thớt. Chúng tôi quy cho hành vi này đến số lượng tham số thấp hơn đáng kể vốn có trong các bộ điều chỉnh thứ hạng thấp. Tận dụng quan sát này, chúng tôi kết hợp các bộ điều chỉnh thứ hạng thấp độc quyền trong 1% cuối cùng của các lần lặp huấn luyện. Việc sử dụng hạn chế này của các bộ điều chỉnh thứ hạng thấp dẫn đến giảm thêm chi phí huấn luyện, cụ thể về tổng số phép toán. Chúng tôi gọi việc sử dụng được đề xuất của các bộ điều chỉnh thứ hạng thấp trong các bước cuối của huấn luyện là bộ điều chỉnh thứ hạng thấp lười biếng.

2.3 Nhân thưa thớt
cuSPARSELt là một thư viện CUDA được thiết kế đặc biệt cho phép nhân Ma trận-Ma trận thưa thớt, trong đó một toán hạng trải qua cắt tỉa với mẫu thưa thớt 2:4. Tuy nhiên, thư viện này không cung cấp API cho các quy trình đại số khác như phép cộng và gán cho các tensor thưa thớt. Chúng tôi hiện đi sâu vào chi tiết của các nhân khác nhau để huấn luyện và tổng quan phương pháp triển khai của chúng tôi.

Thuật toán 1 cho thấy quá trình huấn luyện của một lớp tuyến tính đơn lẻ được lấy từ một mô hình dựa trên attention. Chúng tôi giả sử việc sử dụng suy giảm trọng số trong các trình tối ưu hóa, và sau đó thiết kế các API thưa thớt cần thiết để tạo điều kiện cho các phép toán của trình tối ưu hóa. Quá trình huấn luyện bắt đầu với khởi tạo ma trận (dòng 2) và thiết lập các định dạng thưa thớt để lưu trữ các tensor trọng số và chuyển vị tương ứng của chúng (dòng 3 và 4). Sau đó, đối với mỗi mini-batch trong tập huấn luyện, chúng tôi tính toán lan truyền thuận theo Phương trình 4 (dòng 8). Như một phần của lan truyền ngược, đạo hàm của hàm mất mát đối với kích hoạt đầu ra được tính toán (dòng 10). Tiếp theo, các gradient của hàm mất mát đối với kích hoạt đầu vào (dòng 11) và tensor trọng số (dòng 12) được tính toán sử dụng Phương trình 5 và Phương trình 2, tương ứng. Để tránh sự cần thiết phải cập nhật trọng số với các giá trị không và giảm thiểu chi phí dung lượng bộ nhớ liên quan, chúng tôi sử dụng một chiến lược trong đó chúng tôi che các gradient cho các trọng số đã cắt tỉa. Các giá trị được tính toán được lưu trữ trong định dạng thưa thớt (dòng 13). Tiếp theo, để triển khai suy giảm trọng số trong trình tối ưu hóa và giảm thiểu tác động của việc chia tỷ lệ gradient, chúng tôi tính toán giá trị của (1/γ)∇WL+αW (dòng 15). Ở đây, α là suy giảm trọng số được áp dụng trong trình tối ưu hóa, trong khi γ biểu thị hệ số chia tỷ lệ gradient cho sự ổn định số trong quá trình lan truyền ngược độ chính xác một nửa. Các giá trị cập nhật cho tensor trọng số được tính toán theo quy tắc cập nhật của trình tối ưu hóa (dòng 16). Cuối cùng, giá trị của tensor trọng số và chuyển vị của nó được cập nhật trực tiếp trong định dạng thưa thớt (dòng 17 và dòng 18). Thêm chi tiết về việc triển khai các nhân tùy chỉnh được sử dụng trong Thuật toán 1 có thể được tìm thấy trong Phụ lục K.

2.4 Tối ưu hóa thời gian chạy SLOPE
Trong khi SLOPE cải thiện việc huấn luyện và suy luận của LLM bằng cách giới thiệu các trọng số thưa thớt và bộ điều chỉnh thứ hạng thấp, một triển khai ngây thơ có thể cản trở cải thiện hiệu suất đầy đủ của nó. Cụ thể, các nhân SpMM cuSPARSELt [41] thể hiện độ nhạy cảm với các hình dạng tensor đầu vào và trọng số, và việc giới thiệu các bộ điều chỉnh thứ hạng thấp tại thời điểm suy luận có thể tăng số lượng lời gọi trong quá trình lan truyền thuận của mỗi lớp tuyến tính. Phần này bao gồm phương pháp của chúng tôi để tối ưu hóa việc triển khai SLOPE và cải thiện thêm hiệu suất mô hình.

Chia ô hiệu quả của các tensor upsample. Hình 3-(a) cho thấy tăng tốc đạt được bởi backend cuSPARSELt trên một phạm vi các hình dạng tensor thường được sử dụng trong LLM. Trong khi tăng tốc của SpMM trong các tensor downsample tăng dần khi kích thước của chúng tăng, tăng tốc của tensor upsample giảm vào khoảng hidden dimension = 4000. Để khắc phục hạn chế này, chúng tôi chia ô tensor upsample thành nhiều ma trận nhỏ hơn có kích thước bằng nhau, mỗi ma trận đều hưởng lợi từ tăng tốc cải thiện khi được nhân với đầu vào sử dụng thưa thớt 2:4. Bằng cách điều chỉnh kích thước của các ô, chúng tôi nhận ra rằng hiệu suất tốt nhất có thể đạt được bằng cách sử dụng các ô hình vuông. Kết quả của các phép nhân này sau đó được

--- TRANG 5 ---
Thuật toán 1 Thuật toán Huấn luyện Trước Thưa Thớt Tăng tốc cho một Lớp Tuyến tính
1: Đầu vào: Trọng số: W, Tập Huấn luyện: D, Suy giảm Trọng số: α, Hệ số Chia tỷ lệ Gradient: γ
2: backend.init()
3: WSparseTranspose = backend.setup(W.transpose())
4: WSparse = backend.setup(W)
5: sparseMask = (WSparse != 0) // Theo phần tử
6: for (X,Ŷ) ∈ D do
7:    // Lan Truyền Thuận
8:    Y = backend.spmm(X, WSparseTranspose)
9:    // Lan Truyền Ngược
10:   gradOutput = ∇YL
11:   gradInput = backend.spmm(gradOutput, WSparse)
12:   gradWeight = backend.matmul(gradOutput.transpose(), X)
13:   gradWeightSparse = backend.pruneAndCompress(gradWeight, sparseMask)
14:   // Trình Tối ưu hóa với Suy giảm Trọng số
15:   g = backend.sparseAdd(gradWeightSparse, WSparse, 1/γ, α)
16:   WNew = optimizer.updateWeight(g)
17:   backend.updateSparseMatrix(WSparse, WNew)
18:   backend.updateSparseMatrix(WSparseTranspose, WNew.transpose())
19: end for

nối lại. Tối ưu hóa này, như được chi tiết trong Phụ lục E, dẫn đến cải thiện tốc độ suy luận 12% và tăng tốc độ huấn luyện 4% với SLOPE.

Nhân hiệu quả cho SpMM+bộ điều chỉnh thứ hạng thấp kết hợp. Một triển khai đơn giản của các bộ điều chỉnh thứ hạng thấp yêu cầu bốn lời gọi nhân: một cho phép nhân ma trận thưa thớt, hai cho tính toán thứ hạng thấp, và một để cộng các kết quả. Ngoài ra, các thí nghiệm của chúng tôi chứng minh rằng việc nhân các ma trận với bộ điều chỉnh thứ hạng thấp không tỷ lệ tương xứng với thứ hạng của bộ điều chỉnh, dẫn đến chi phí đáng kể do cường độ số học thấp của chúng (xem Phụ lục C). Để giải quyết vấn đề này, chúng tôi giới thiệu hai tối ưu hóa: (1) nối tensor downsample vào tensor trọng số thưa thớt, giảm lời gọi nhân và tăng cường độ số học như trong Phương trình 11-trái, và (2) tận dụng nhân cuBLAS kết hợp phép nhân ma trận và phép cộng, giảm thiểu truy cập bộ nhớ cache và lời gọi nhân như trong Phương trình 11-phải. Như được chứng minh trong Phụ lục D, các tối ưu hóa này cùng nhau đóng góp vào cải thiện tăng tốc lên đến 6% trong tốc độ suy luận đầu cuối.

[Y1|Y2] = X[WT|L]; Y = Y2R + Y1 (11)

3 Kết quả thí nghiệm
Phần này đánh giá hiệu quả của SLOPE trong việc tăng tốc quá trình huấn luyện trước đồng thời đạt được tiết kiệm bộ nhớ. Do các tài nguyên tính toán đáng kể cần thiết cho việc huấn luyện trước LLM, đánh giá độ chính xác của chúng tôi chủ yếu tập trung vào các LLM quy mô nhỏ hơn lên đến 774M tham số. Tuy nhiên, kết quả tăng tốc và giảm bộ nhớ mở rộng đến một phạm vi rộng hơn của các mô hình, từ 2.6B đến 66B tham số.

3.1 Tăng tốc và tiết kiệm bộ nhớ đầu cuối: huấn luyện trước và suy luận
Chúng tôi đánh giá tăng tốc và giảm bộ nhớ của SLOPE trong quá trình huấn luyện trước và suy luận trên các LLM với kích thước tham số mô hình khác nhau. Để chứng minh khả năng mở rộng và hiệu quả của phương pháp của chúng tôi, chúng tôi đã tiến hành đo hiệu suất rộng rãi trên các mô hình OPT (2.6 B đến 66 B) và LLaMA-3-8B và Mistral-v0.3-7B. Trong tất cả các thí nghiệm, chúng tôi đã kích hoạt FlashAttention-2 [8] (Phụ lục M trình bày nghiên cứu phân tích chi tiết về tác động của FlashAttention). Để giảm thiểu tác động của các giá trị ngoại lệ, chúng tôi đã tiến hành 1.000 lần lặp cho mỗi thí nghiệm tăng tốc và báo cáo giá trị trung vị. Đối với các thí nghiệm giảm bộ nhớ, chúng tôi đã thực hiện năm lần chạy độc lập và tương tự báo cáo kết quả trung vị. Các phương pháp này được chọn để cung cấp một thước đo xu hướng trung tâm đáng tin cậy hơn trong kết quả của chúng tôi.

Chúng tôi so sánh phương pháp của mình với việc huấn luyện trước và suy luận dày đặc trực tiếp trong PyTorch, sử dụng backend cuBLAS hiệu quả. Là điểm chuẩn huấn luyện trước thưa thớt, chúng tôi so sánh công trình của mình với Huấn luyện Trước Thưa Thớt-Dày Đặc (FST) [26], phương pháp huấn luyện trước 2:4 tiên tiến nhất và công trình huấn luyện trước thưa thớt bán cấu trúc duy nhất cung cấp tăng tốc đầu cuối. Lưu ý rằng các phương pháp nhắm vào việc huấn luyện trước LLM với thưa thớt N:M thường gặp vấn đề thiếu hiệu quả do chi phí tìm kiếm mặt nạ và/hoặc thiết lập nén. Phụ lục H và Phụ lục B chi tiết việc phân tích trong Bi-Mask [60] và FST [26], tương tự sử dụng thưa thớt N:M trên cả lan truyền thuận và ngược.

Đáng chú ý, phương pháp của chúng tôi, SLOPE, khác biệt đáng kể với công trình gần đây Huấn luyện Hoàn toàn Thưa Thớt (FST) [26] trong hai khía cạnh chính. Thứ nhất, chúng tôi cắt tỉa toàn diện tất cả trọng số trong mô hình, bao gồm cả các mô-đun MLP và Self-Attention, trong khi FST chỉ cắt tỉa trọng số trong các mô-đun MLP. Thứ hai, FST sử dụng trọng số có thể chuyển vị động, điều này tạo ra chi phí tính toán và bộ nhớ bổ sung trong quá trình huấn luyện. Cuối cùng, FST đòi hỏi tinh chỉnh dày đặc (~17% của huấn luyện trước), do đó vô hiệu hóa lợi thế tăng tốc của họ trong quá trình suy luận. Ngược lại, phương pháp của chúng tôi đạt được các mô hình ngôn ngữ lớn hiệu quả và chính xác trong cả quá trình huấn luyện và suy luận mà không có những hạn chế như vậy.

Trước khi chúng tôi tiếp tục với các kết quả, chúng tôi làm rõ các ký hiệu được sử dụng trong bản thảo của chúng tôi và bài báo FST [26] trong bảng 1.

Bảng 1: Mô tả các Thuật ngữ Chính
Thuật ngữ | Mô tả
Huấn luyện Trước Thưa Thớt | Ký hiệu chung được sử dụng trong SLoPe và FST, chỉ việc sử dụng trọng số thưa thớt trong quá trình huấn luyện trước.
Tinh chỉnh Dày Đặc | Ký hiệu được sử dụng trong bài báo FST, chỉ một giai đoạn huấn luyện trước mở rộng.
Tinh chỉnh Hạ nguồn | Hiệu suất sau khi huấn luyện trước kết thúc, được sử dụng để tinh chỉnh mô hình cho các tác vụ hạ nguồn cụ thể.
FST | Kỹ thuật huấn luyện trước mở rộng tập trung vào tinh chỉnh dày đặc.
Extended SRT | Biến thể của huấn luyện trước thưa thớt được mở rộng với tinh chỉnh bổ sung.

Tăng tốc SLOPE cho huấn luyện trước và suy luận. Bảng 2 tóm tắt các tăng tốc đạt được bởi phương pháp của chúng tôi trong cả quá trình huấn luyện và suy luận. Vì hơn 99% việc huấn luyện xảy ra mà không có bộ điều chỉnh thứ hạng thấp, tăng tốc huấn luyện phần lớn độc lập với thứ hạng bộ điều chỉnh. Ngược lại, tăng tốc suy luận bị ảnh hưởng trực tiếp bởi thứ hạng bộ điều chỉnh. Cho các chiều ẩn khác nhau trên các kích thước mô hình khác nhau, chúng tôi báo cáo tăng tốc suy luận cho các tỷ lệ thứ hạng bộ điều chỉnh khác nhau: adapter-rank/hidden-dimension.

Hình 3-(a) minh họa rằng cuSPARSELt đạt được tăng tốc cao hơn cho các ma trận lớn cho đến khi nó đạt đến khả năng hiệu suất tối đa của nó (2×). Một xu hướng tương tự được quan sát trong tăng tốc huấn luyện trước và suy luận của các mô hình. Đối với các ma trận nhỏ được sử dụng trong bộ điều chỉnh thứ hạng thấp, cường độ số học thấp hơn của phép nhân bộ điều chỉnh thứ hạng thấp dẫn đến chi phí cao hơn so với phép nhân thưa thớt. Điều này là do cường độ số học thấp hạn chế việc sử dụng đầy đủ tài nguyên GPU, dẫn đến thiếu hiệu quả.

Giảm bộ nhớ SLOPE trong huấn luyện trước và suy luận. Đối với huấn luyện, việc tiêu thụ bộ nhớ của một mô hình dày đặc bao gồm trọng số, gradient, và trạng thái trình tối ưu hóa, lên đến 4×16bits cho trọng số, 4×16bits cho gradient, và 2×4×32bits cho trạng thái trình tối ưu hóa. Tuy nhiên, mô hình thưa thớt lưu trữ trọng số khác không và chỉ số hai lần (cho cả trọng số và trọng số chuyển vị), cùng với mặt nạ nhị phân, gradient, và trạng thái trình tối ưu hóa giảm. Điều này cộng lại thành 2×(16 + 3) bits (trọng số và trọng số chuyển vị), 4×8bits (mặt nạ nhị phân), 2×16bits (gradient), và 2×2×32bits (trạng thái trình tối ưu hóa). Do đó, dung lượng bộ nhớ trong quá trình huấn luyện được giảm 68%. Đối với suy luận, một mô hình dày đặc yêu cầu lưu trữ trọng số với tổng chi phí bộ nhớ là 4×16bits. Ngược lại, mô hình thưa thớt của chúng tôi

--- TRANG 6 ---
Bảng 2: Phân tích so sánh tăng tốc huấn luyện trước và suy luận đầu cuối (×) giữa SLOPE và công trình mới nhất (FST) về tăng tốc huấn luyện trước với thưa thớt 2:4 (ICML 2024) [26]. Lưu ý rằng việc thiếu tăng tốc suy luận trong FST là do việc huấn luyện trước dày đặc cuối cùng trong các lần lặp cuối, dẫn đến một mô hình dày đặc cho suy luận. E-SR-STE là viết tắt của Extended SR-STE.

MÔ HÌNH | PHƯƠNG PHÁP | HUẤN LUYỆN | SUY LUẬN
---|---|---|---
 | | KHÔNG BỘ ĐIỀU CHỈNH (r=0) | KHÔNG BỘ ĐIỀU CHỈNH (r=0) | BỘ ĐIỀU CHỈNH 1.56% | BỘ ĐIỀU CHỈNH 6.25%
OPT-66B | SLOPE | 1.20 | 1.46 | 1.43 | 1.40
 | FST | 1.06 | 1.00 | 1.00 | 1.00
OPT-30B | SLOPE | 1.22 | 1.53 | 1.53 | 1.50
 | FST | 1.07 | 1.00 | 1.00 | 1.00
OPT-13B | SLOPE | 1.25 | 1.54 | 1.39 | 1.36
 | FST | 1.10 | 1.00 | 1.00 | 1.00
OPT-6.6B | SLOPE | 1.21 | 1.46 | 1.46 | 1.43
 | FST | 1.11 | 1.00 | 1.00 | 1.00
OPT-2.6B | SLOPE | 1.13 | 1.31 | 1.25 | 1.18
 | FST | 1.09 | 1.00 | 1.00 | 1.00
LLAMA-3-8B | SLOPE | 1.16 | 1.35 | 1.33 | 1.32
 | FST | 1.09 | 1.00 | 1.00 | 1.00
MISTRAL-V0.3-7B | SLOPE | 1.15 | 1.34 | 1.32 | 1.31
 | FST | 1.07 | 1.00 | 1.00 | 1.00

Bảng 3: Phân tích so sánh giảm bộ nhớ đầu cuối (×) trong quá trình huấn luyện và suy luận giữa SLOPE và công trình mới nhất (FST) về tăng tốc huấn luyện trước với thưa thớt 2:4 (ICML 2024) [26]. Các giá trị lớn hơn 1.00× cho thấy chi phí bộ nhớ.

MÔ HÌNH | PHƯƠNG PHÁP | HUẤN LUYỆN | SUY LUẬN
---|---|---|---
 | | KHÔNG BỘ ĐIỀU CHỈNH (r=0) | KHÔNG BỘ ĐIỀU CHỈNH (r=0) | BỘ ĐIỀU CHỈNH 1.56% | BỘ ĐIỀU CHỈNH 6.25%
OPT-66B | SLOPE | 0.67 | 0.63 | 0.65 | 0.70
 | FST | 1.27 | 1.00 | 1.00 | 1.00
OPT-30B | SLOPE | 0.67 | 0.61 | 0.63 | 0.69
 | FST | 1.17 | 1.00 | 1.00 | 1.00
OPT-13B | SLOPE | 0.68 | 0.51 | 0.62 | 0.68
 | FST | 1.16 | 1.00 | 1.00 | 1.00
OPT-6.6B | SLOPE | 0.68 | 0.60 | 0.62 | 0.68
 | FST | 1.19 | 1.00 | 1.00 | 1.00
OPT-2.6B | SLOPE | 0.67 | 0.62 | 0.64 | 0.70
 | FST | 1.18 | 1.00 | 1.00 | 1.00
LLAMA-3-8B | SLOPE | 0.63 | 0.66 | 0.69 | 0.71
 | FST | 1.17 | 1.00 | 1.00 | 1.00
MISTRAL-V0.3-7B | SLOPE | 0.68 | 0.66 | 0.69 | 0.65
 | FST | 1.15 | 1.00 | 1.00 | 1.00

tối ưu hóa việc sử dụng bộ nhớ bằng cách chỉ lưu trữ các trọng số khác không và chỉ số của chúng, dẫn đến 2×16bits cho các phần tử khác không và ba bit cho chỉ số (xem phương trình 7). Điều này dẫn đến giảm 54% việc sử dụng bộ nhớ trong quá trình suy luận.

Bảng 3 trình bày việc giảm bộ nhớ cho các thứ hạng bộ điều chỉnh thứ hạng thấp khác nhau và các biến thể mô hình OPT, LLaMA-2, và Mistral. Việc giảm bộ nhớ ít hơn so với kỳ vọng lý thuyết một chút, chủ yếu do việc sử dụng bộ nhớ bổ sung từ các thành phần mô hình khác, như layer norm, và các tham số mô hình dày đặc.

3.2 Kết quả độ chính xác huấn luyện trước
Để đánh giá tác động của SLOPE đối với độ chính xác mô hình, chúng tôi đã tiến hành các thí nghiệm huấn luyện trước trên nhiều mô hình và tập dữ liệu khác nhau (chi tiết trong Phụ lục O). Trong tất cả các thí nghiệm, các đầu phân loại và lớp tuyến tính đầu tiên theo sau đầu vào đều dày đặc.

GPT2 (Small/Large). Chúng tôi đã huấn luyện trước cả biến thể nhỏ (117 M tham số) và lớn (774 M tham số) của GPT2 [46] trên tập dữ liệu OpenWebText [1]. Để có sự so sánh công bằng, chúng tôi đánh giá các mô hình trên các tác vụ zero-shot MMLU [23], Arc Challenge [6], và OpenBookQA [35] được triển khai trong Language Model Evaluation Harness [18]. Ngoài ra, chúng tôi đánh giá perplexity xác thực của các mô hình

--- TRANG 7 ---
theo cùng cài đặt thí nghiệm được mô tả trong FlashAttention [10,8]. Chúng tôi so sánh SLOPE với hai phương pháp huấn luyện trước thưa thớt tiên tiến, bao gồm (a) Wanda [51] → một kỹ thuật cắt tỉa một lần, (b) Extended SR-STE [61,26] → một phương pháp huấn luyện trước mặt nạ động cho thưa thớt N:M, phục vụ làm nền tảng cho các công trình tiếp theo [27,60,26]. Xin lưu ý rằng SR-STE chỉ hỗ trợ tối ưu hóa gradient ngẫu nhiên, và FST đã mở rộng nó cho các trình tối ưu hóa khác. Chúng tôi sử dụng phần mở rộng được cung cấp bởi FST trong công trình của chúng tôi, và gọi nó là Extended SR-STE. Sự khác biệt giữa Extended SR-STE và FST là FST yêu cầu huấn luyện trước dày đặc (tinh chỉnh) trong 17% cuối của huấn luyện trước và chỉ cắt tỉa các lớp MLP của mô hình, trong khi SR-STE hoàn toàn thưa thớt và cắt tỉa cả các lớp MLP và Self-Attention của mô hình.

[Có biểu đồ về perplexity GPT2 Small và Large theo iterations]

Hình 2: Perplexity xác thực của GPT2-Small và GPT2-Large trên OpenWebText. γw hiển thị giá trị của tham số hệ số suy giảm trong Extended SR-STE (FST).

Hình 2 so sánh perplexity xác thực và độ chính xác zero-shot của GPT2-Small và GPT2-Large trên một phạm vi các phương pháp huấn luyện trước thưa thớt với các siêu tham số khác nhau. Chúng tôi đã bổ sung thêm các bộ điều chỉnh thứ hạng thấp lười biếng vào Extended SR-STE [61] để cho thấy hiệu quả của phương pháp của chúng tôi trong các phương pháp khác và cũng so sánh cả hai phương pháp với các cài đặt tương tự hơn. Trong khi khoảng cách về perplexity luôn tồn tại giữa các mô hình thưa thớt và dày đặc, SLOPE đạt được perplexity thấp hơn so với Wanda [51] và Extended SR-STE. Ngoài ra, Bảng 4 tóm tắt độ chính xác đạt được của các mô hình trên các tác vụ zero-shot, cho thấy SLOPE liên tục đạt được độ chính xác cao hơn so với Extended SR-STE. Hơn nữa, việc thêm các bộ điều chỉnh thứ hạng thấp lười biếng có thể có lợi cho cả phương pháp huấn luyện tĩnh và động. Độ chính xác được cải thiện này xuất phát từ việc phân bổ hiệu quả ngân sách huấn luyện của SLOPE. Cụ thể, Extended SR-STE, với các mặt nạ cắt tỉa động của nó, tiêu tốn một phần đáng kể ngân sách huấn luyện của nó (ví dụ như cập nhật gradient) để cập nhật các trọng số có thể cuối cùng bị cắt tỉa và không được sử dụng trong suy luận, dẫn đến lãng phí tài nguyên. Phụ lục A cung cấp thêm chi tiết và bằng chứng hỗ trợ cho quan sát này. Kết quả xác thực bổ sung cho các thí nghiệm GPT trên tập dữ liệu GLUE cũng được cung cấp trong Phụ lục Q và P.

BERT-Large-Uncased. Chúng tôi huấn luyện trước BERT-Large-Uncased [13] (355 M tham số) và tinh chỉnh nó cho các tác vụ trả lời câu hỏi và phân loại văn bản khác nhau, theo một phương pháp tương tự như [43, 36,44] cho cả quá trình huấn luyện trước và tinh chỉnh. Phụ lục G cung cấp chi tiết về quá trình huấn luyện trước và tinh chỉnh. Chúng tôi đánh giá hiệu suất của BERT-Large-Uncased trên SQuAD v1.1 [48]

--- TRANG 8 ---
Bảng 4: Kết quả độ chính xác GPT2-Small trên các tác vụ zero-shot. Thứ hạng bộ điều chỉnh là tỷ lệ của bộ điều chỉnh thứ hạng thấp với chiều ẩn của mô hình. Đối với Extended SR-STE, chúng tôi đã sử dụng hệ số suy giảm 6e-6, vì nó dẫn đến perplexity thấp nhất trong OpenWebText. Cấu hình thưa thớt hoạt động tốt nhất được làm nổi bật bằng chữ in đậm.

PHƯƠNG PHÁP | THỨ HẠNG BỘ ĐIỀU CHỈNH | MMLU ↑ | ARC CHALLENGE ↑ | OPEN-BOOK QA ↑ | WINO-GRANDE ↑ | HELLA-SWAG ↑ | MATHQA ↑ | PIQA ↑ | RACE ↑
---|---|---|---|---|---|---|---|---|---
DENSE | N/A | 22.9 | 20.7 | 16.2 | 50.6 | 28.5 | 21.8 | 59.8 | 28.4
SLOPE | 2.1% | 23.0 | 19.3 | 16.4 | 50.8 | 27.5 | 20.8 | 57.6 | 27.2
 | 0.05% | 23.0 | 19.4 | 16.2 | 50.5 | 27.4 | 20.8 | 57.5 | 27.1
 | 0 | 23.0 | 19.3 | 16.0 | 50.1 | 27.5 | 20.8 | 57.4 | 27.1
EXTENDED SR-STE | 2.1% | 24.2 | 18.3 | 14.2 | 47.5 | 26.9 | 21.4 | 55.2 | 24.2
 | 0.05% | 24.1 | 18.4 | 14.2 | 47.5 | 26.8 | 21.2 | 54.5 | 24.2
 | 0 | 24.1 | 18.3 | 12.6 | 47.5 | 26.9 | 21.2 | 54.8 | 24.0

và các tác vụ GLUE [57]. Chúng tôi báo cáo điểm số metric trung bình cho GLUE và trình bày các metric cụ thể theo tác vụ trong Phụ lục L. Xin lưu ý rằng trong tất cả các thí nghiệm tương ứng với BERT-Large-Uncased, khi sử dụng Wanda, chúng tôi đã tinh chỉnh mô hình sau khi cắt tỉa để cải thiện độ chính xác của các mô hình, vì việc sử dụng Wanda một mình dẫn đến kết quả độ chính xác cực kỳ thấp.

Tác động của các bộ điều chỉnh thứ hạng thấp. Để hiểu tác động của các bộ điều chỉnh thứ hạng thấp đối với hiệu suất huấn luyện trước, chúng tôi đã tiến hành các nghiên cứu phân tích sử dụng thứ hạng bộ điều chỉnh thứ hạng thấp là 4, 16, và 64 cho 1% tổng số lần lặp. Các thứ hạng này biểu thị lên đến 6.25% chiều ẩn của mô hình. Bảng 5 cho thấy kết quả của các cài đặt này trên các tác vụ hạ nguồn SQuAD và GLUE. Chúng tôi trình bày các metric theo tác vụ cho GLUE trong Phụ lục L. Như mong đợi, việc thêm các bộ điều chỉnh thứ hạng thấp cải thiện độ chính xác cuối cùng của mô hình trên tất cả các tác vụ. Ngoài ra, thứ hạng cao hơn cải thiện hiệu suất của mô hình với chi phí tăng yêu cầu tính toán. Cũng đáng chú ý rằng việc kết hợp các bộ điều chỉnh thứ hạng thấp chỉ trong các lần lặp cuối (1% tổng lần lặp) là đủ để khôi phục độ chính xác huấn luyện trước.

Tốc độ hội tụ của các bộ điều chỉnh thứ hạng thấp. Chúng tôi đã đưa ra giả thuyết rằng các bộ điều chỉnh thứ hạng thấp sẽ hội tụ nhanh hơn do số lượng tham số có thể học được ít hơn đáng kể. Để kiểm tra điều này, chúng tôi đã giới thiệu các bộ điều chỉnh thứ hạng thấp trong giai đoạn thứ hai của quá trình huấn luyện trước BERT-Large-Uncased và theo dõi tốc độ hội tụ của chúng. Hình 3 cho thấy độ tương tự cosine của các bộ điều chỉnh, với bộ điều chỉnh downsample hội tụ nhanh chóng trong vòng 100 lần lặp và bộ điều chỉnh upsample hội tụ chậm hơn một chút. Mặc dù vậy, việc hạn chế huấn luyện trong 100 lần lặp vẫn mang lại kết quả tương đương trên các tác vụ hạ nguồn.

[Có hai biểu đồ: (a) về tăng tốc cuSPARSELt SpMM và (b) về độ tương tự của bộ điều chỉnh thứ hạng thấp]

Hình 3: (a) Tăng tốc đạt được bằng cách sử dụng backend cuSPARSELt trong PyTorch cho các ma trận Attention (dout=din), Upsample (dout=4din) và Downsample (dout=din/4) với kích thước batch 2048. (b) Độ tương tự cosine của các bộ điều chỉnh thứ hạng thấp và các bộ điều chỉnh đã hội tụ cho các lớp khác nhau trong mô hình. Độ tương tự cosine được tính trung bình giữa 24 lớp của BERT-Large-Uncased.

Tác động của thưa thớt N:M hỗn hợp. Để nghiên cứu độ nhạy cảm của các khối khác nhau đối với các tỷ lệ thưa thớt khác nhau và đánh giá tầm quan trọng tương đối của chúng, chúng tôi thí nghiệm trên một phạm vi cấu hình: (a) [2:4-2:4] → áp dụng đồng đều thưa thớt 2:4 trên tất cả các lớp (b) [2:4-2:8] → áp dụng mẫu thưa thớt 2:4 cho 12 khối đầu tiên và mẫu thưa thớt 2:8 cho 12 khối cuối cùng và (c) [2:8-2:4] → chúng tôi đảo ngược tỷ lệ thưa thớt cho 12 khối đầu tiên và cuối cùng. Lưu ý rằng, để giảm chi phí tính toán, chúng tôi sử dụng cùng một checkpoint dày đặc cho Giai đoạn-1 trong tất cả các cài đặt và một bộ điều chỉnh thứ hạng thấp có thứ hạng 40 cho tất cả các mô hình. Chúng tôi cũng nhân bản thí nghiệm này bằng cách sử dụng Wanda [51] và báo cáo kết quả so sánh.

Bảng 5: Kết quả SQuADv1.1 trên BERT-Large-Uncased với các thứ hạng bộ điều chỉnh khác nhau. r biểu thị tỷ lệ của bộ điều chỉnh thứ hạng thấp với chiều ẩn (1024).

TẬP DỮ LIỆU | DENSE | r=0 | r=0.39% | r=1.56% | r=6.25%
---|---|---|---|---|---
SQUAD | 90.44 | 89.1 | 89.1 | 89.2 | 89.5
GLUE | 80.22 | 77.4 | 77.7 | 77.8 | 78.2

--- TRANG 9 ---
Bảng 6: Kết quả SQuADv1.1 trên BERT-Large-Uncased cho các cài đặt thưa thớt khác nhau.

MẪU THƯA THỚT (12 KHỐI ĐẦU - 12 KHỐI CUỐI) | SQUAD SLOPE | SQUAD WANDA | GLUE SLOPE | GLUE WANDA
---|---|---|---|---
2:4-2:4 | 90.17 | 89.93 | 79.08 | 78.84
2:4-2:8 | 89.85 | 89.55 | 79.03 | 77.24
2:8-2:4 | 89.67 | 86.57 | 75.92 | 69.08

Bảng 6 tóm tắt kết quả GLUE và SQuAD cho các cài đặt này. Như kết quả cho thấy, việc tăng tỷ lệ thưa thớt làm giảm độ chính xác của mô hình trên tất cả các tác vụ. Nhưng khi 12 khối đầu tiên của mô hình bị cắt tỉa, việc giảm độ chính xác cao hơn đáng kể, đặc biệt trên tập dữ liệu GLUE. Chúng tôi kết luận rằng các khối đầu tiên của mô hình nhạy cảm hơn với thưa thớt trong quá trình huấn luyện trước, nhưng người ta có thể thưa thớt hóa các khối cuối của LLM một cách tích cực hơn. Chúng tôi quan sát một mẫu tương tự trong kết quả Wanda, nhưng Wanda hoạt động kém hơn so với SLOPE một cách nhất quán trong những trường hợp này.

Tác động của thưa thớt hóa trên các mô-đun khác nhau. Mỗi khối trong LLM bao gồm một mô-đun self-attention và một mô-đun MLP, mỗi mô-đun chứa nhiều lớp tuyến tính. Chúng tôi đã phân tích độ nhạy cảm của SLOPE đối với việc cắt tỉa từng mô-đun đó. Kết quả của chúng tôi trong Phụ lục F chứng minh rằng SLOPE có thể duy trì kết quả chất lượng cạnh tranh trong khi cắt tỉa tất cả các mô-đun trong mô hình.

4 Kết luận
Tóm lại, SLOPE cải thiện cả thời gian huấn luyện trước và suy luận đồng thời giảm dung lượng bộ nhớ với tác động không đáng kể đến hiệu suất mô hình. SLOPE đạt được những lợi ích này bằng cách sử dụng hiệu quả thưa thớt N:M và các bộ điều chỉnh thứ hạng thấp lười biếng trong cả lan truyền thuận và ngược, được hỗ trợ bởi thiết kế hiệu quả của các nhân CUDA. Ngoài ra, việc sử dụng các bộ điều chỉnh thứ hạng thấp lười biếng cho phép cân bằng dung lượng bộ nhớ và độ chính xác mô hình trên một phạm vi rộng các mô hình. Kết quả cho thấy SLOPE đạt được tăng tốc lên đến 1.25× và 1.54× cho huấn luyện trước và suy luận, tương ứng. Những tăng tốc này được đạt được trong khi phương pháp của chúng tôi giảm dung lượng bộ nhớ hiệu quả lên đến 0.63× (huấn luyện trước) và 0.61× (suy luận).

Lời cảm ơn và Tiết lộ Tài trợ
Công trình này cũng được hỗ trợ một phần bởi NSERC Discovery Grants (RGPIN-06516, DGECR00303), chương trình Canada Research Chairs, giải thưởng Ontario Early Researcher, chương trình Canada Research Chairs, Ontario Early Researcher Award, và Digital Research Alliance of Canada (www.alliancecan.ca). Công trình của Zhao Zhang được hỗ trợ bởi National Science Foundation OAC-2401246. Chúng tôi cũng ghi nhận Texas Advanced Computing Center (TACC) tại Đại học Texas ở Austin vì đã cung cấp tài nguyên HPC góp phần vào kết quả nghiên cứu được báo cáo trong bài báo này (http://www.tacc.utexas.edu). Chúng tôi bày tỏ lòng biết ơn đối với David Fleet, Karolina Dziugaite, Suvinay Subramanian, Cliff Young, và David Anugraha vì đã xem xét bài báo và cung cấp phản hồi sâu sắc. Chúng tôi cũng cảm ơn đội ngũ mở rộng tại Google DeepMind đã tạo điều kiện và hỗ trợ hướng nghiên cứu này.

--- TRANG 10 ---
[Phần này chứa danh sách tài liệu tham khảo từ [1] đến [14], được dịch như sau:]

Tài liệu tham khảo

[1] Ellie Pavlick Aaron Gokaslan, Vanya Cohen và Stefanie Tellex. OpenWebText Corpus, 2019.

[2] Dimitris Bertsimas, Ryan Cory-Wright, và Nicholas AG Johnson. Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. JMLR, 2023.

[3] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, và Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention Approximation. arXiv preprint arXiv:2110.15343, 2021.

[4] Stanley F Chen, Douglas Beeferman, và Roni Rosenfeld. Evaluation Metrics for Language Models. Carnegie Mellon University, 1998.

[5] Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, và Yuan Xie. Dynamic N:M Fine-grained Structured Sparse Attention Mechanism. Trong PPoPP, 2023.

[6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[7] Compute Canada. Compute Canada. https://computecanada.ca/.

[8] Tri Dao. Flashattention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv preprint arXiv:2307.08691, 2023.

[9] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher Re. Pixelated Butterfly: Simple and Efficient Sparse Training for Neural Network Models. arXiv preprint arXiv:2112.00029, 2021.

[10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Trong NeurIPS, 2022.

[11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.

[12] Tim Dettmers và Luke Zettlemoyer. Sparse Networks from Scratch: Faster Training without Losing Performance. arXiv preprint arXiv:1907.04840, 2019.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805, 2018.

[14] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, và Michael Carbin. Linear Mode Connectivity and the Lottery Ticket Hypothesis. Trong ICML, 2020.

[Tiếp tục với phần còn lại của bài báo bao gồm các tài liệu tham khảo khác và phụ lục...]
