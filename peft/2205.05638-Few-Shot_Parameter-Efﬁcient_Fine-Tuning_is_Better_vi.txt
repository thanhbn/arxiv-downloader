# 2205.05638.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2205.05638.pdf
# Kích thước file: 562785 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Điều chỉnh tham số hiệu quả few-shot tốt hơn
và rẻ hơn so với học trong ngữ cảnh
Haokun LiuDerek TamMohammed Muqeeth
Jay Mohta Tenghao Huang Mohit Bansal Colin Raffel
Khoa Khoa học Máy tính
Đại học North Carolina tại Chapel Hill
{haokunl,dtredsox,muqeeth,craffel}@cs.unc.edu
Tóm tắt
Học trong ngữ cảnh few-shot (ICL) cho phép các mô hình ngôn ngữ được huấn luyện trước thực hiện một nhiệm vụ chưa từng gặp mà không cần huấn luyện dựa trên gradient bằng cách đưa một số lượng nhỏ các ví dụ huấn luyện vào đầu vào. ICL phát sinh chi phí tính toán, bộ nhớ và lưu trữ đáng kể vì nó liên quan đến việc xử lý tất cả các ví dụ huấn luyện mỗi khi đưa ra dự đoán. Điều chỉnh tham số hiệu quả (PEFT) (ví dụ: các mô-đun adapter, prompt tuning, các phương pháp cập nhật thưa, v.v.) cung cấp một mô hình thay thế trong đó một tập hợp nhỏ các tham số được huấn luyện để cho phép mô hình thực hiện nhiệm vụ mới. Trong bài báo này, chúng tôi so sánh một cách nghiêm ngặt ICL few-shot và PEFT và chứng minh rằng phương pháp sau cung cấp độ chính xác tốt hơn cũng như chi phí tính toán thấp hơn đáng kể. Trên con đường đó, chúng tôi giới thiệu một phương pháp PEFT mới gọi là (IA)3 điều chỉnh tỷ lệ các kích hoạt bằng các vector đã học, đạt được hiệu suất mạnh hơn trong khi chỉ giới thiệu một lượng tham số mới tương đối nhỏ. Chúng tôi cũng đề xuất một công thức đơn giản dựa trên mô hình T0 [1] được gọi là T-Few có thể được áp dụng cho các nhiệm vụ mới mà không cần điều chỉnh hoặc sửa đổi cụ thể cho từng nhiệm vụ. Chúng tôi xác nhận hiệu quả của T-Few trên các nhiệm vụ hoàn toàn chưa thấy bằng cách áp dụng nó vào benchmark RAFT [2], đạt được hiệu suất siêu con người lần đầu tiên và vượt trội hơn so với công nghệ tiên tiến 6% tuyệt đối. Tất cả mã sử dụng trong các thí nghiệm của chúng tôi đều công khai có sẵn.1

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước đã trở thành nền tảng của xử lý ngôn ngữ tự nhiên, nhờ vào thực tế rằng chúng có thể cải thiện đáng kể hiệu quả dữ liệu trên các nhiệm vụ quan tâm – tức là, sử dụng mô hình ngôn ngữ được huấn luyện trước để khởi tạo thường tạo ra kết quả tốt hơn với ít dữ liệu được gán nhãn hơn. Một cách tiếp cận phổ biến trong lịch sử là sử dụng các tham số của mô hình được huấn luyện trước để khởi tạo trước khi thực hiện điều chỉnh dựa trên gradient trên nhiệm vụ downstream quan tâm. Mặc dù điều chỉnh đã tạo ra nhiều kết quả tiên tiến [1], nó dẫn đến một mô hình chuyên biệt cho một nhiệm vụ duy nhất với một tập hợp giá trị tham số hoàn toàn mới, có thể trở nên không thực tế khi điều chỉnh một mô hình trên nhiều nhiệm vụ downstream.

Một cách tiếp cận thay thế được phổ biến bởi [3,4] là học trong ngữ cảnh (ICL), điều này khuyến khích một mô hình thực hiện một nhiệm vụ downstream bằng cách nhập các ví dụ được gợi ý. Prompting few-shot chuyển đổi một bộ sưu tập nhỏ các cặp input-target thành các hướng dẫn và ví dụ (thường) có thể hiểu được bởi con người [3,4], cùng với một ví dụ chưa được gán nhãn mà một dự đoán được mong muốn. Đáng chú ý, ICL không yêu cầu huấn luyện dựa trên gradient và do đó cho phép một mô hình duy nhất thực hiện ngay lập tức một loạt các nhiệm vụ. Thực hiện ICL do đó chỉ dựa vào các khả năng mà một mô hình đã học trong quá trình huấn luyện trước. Những đặc điểm này đã dẫn đến rất nhiều sự quan tâm gần đây đối với các phương pháp ICL [5–10].

Đóng góp bằng nhau.
1https://github.com/r-three/t-few
Bản thảo. Đang được xem xét.arXiv:2205.05638v2 [cs.LG] 26 Aug 2022

--- TRANG 2 ---
VKQ
softmax 
DenseNonlinearity Dense
T0Susie yêu thích bánh mì chuối của bà ngoại. Susie gọi cho bà ngoại và hỏi bà gửi một ít. Bà ngoại sống rất xa. Một tuần trôi qua và bà ngoại đã làm Susie ngạc nhiên bằng cách đến thăm. Có thể có sự tiếp diễn nào cho câu chuyện? Susie rất vui mừng. Susie buồn bã.
(IA)3Các loss sử dụng trong T-FewHình 1: Sơ đồ của (IA)3 và các thành phần loss được sử dụng trong công thức T-Few. Trái: (IA)3 giới thiệu các vector đã học lk; lv, và l lần lượt điều chỉnh tỷ lệ (thông qua phép nhân theo từng phần tử, được hình dung như ) các key và value trong cơ chế attention và các kích hoạt bên trong trong mạng feed-forward theo vị trí. Phải: Ngoài loss cross-entropy tiêu chuẩn LLM, chúng tôi giới thiệu loss unlikelihood LUL làm giảm xác suất của các đầu ra không chính xác và loss chuẩn hóa độ dài LLN áp dụng loss softmax cross-entropy tiêu chuẩn cho log-xác suất chuẩn hóa độ dài của tất cả các lựa chọn đầu ra.

Mặc dù có những lợi ích thực tế của ICL, nó có một số nhược điểm lớn. Thứ nhất, xử lý tất cả các cặp input-target được gợi ý mỗi khi mô hình đưa ra dự đoán phát sinh chi phí tính toán đáng kể. Thứ hai, ICL thường tạo ra hiệu suất kém hơn so với điều chỉnh [4]. Cuối cùng, định dạng chính xác của prompt (bao gồm cả cách diễn đạt [11] và thứ tự của các ví dụ [12]) có thể có tác động đáng kể và không thể dự đoán đến hiệu suất của mô hình, vượt xa sự biến thiên giữa các lần chạy của điều chỉnh.

Công việc gần đây cũng đã chứng minh rằng ICL có thể hoạt động tốt ngay cả khi được cung cấp các nhãn không chính xác, đặt ra câu hỏi về việc có bao nhiêu việc học đang diễn ra [9].

Một mô hình bổ sung để cho phép một mô hình thực hiện một nhiệm vụ mới với cập nhật tối thiểu là điều chỉnh tham số hiệu quả (PEFT), trong đó một mô hình được huấn luyện trước được điều chỉnh bằng cách chỉ cập nhật một số lượng nhỏ các tham số được thêm vào hoặc được chọn. Các phương pháp gần đây đã phù hợp với hiệu suất của việc điều chỉnh toàn bộ mô hình trong khi chỉ cập nhật hoặc thêm một phần nhỏ (ví dụ: 0.01%) các tham số của mô hình đầy đủ [13,14]. Hơn nữa, một số phương pháp PEFT cho phép các batch nhiệm vụ hỗn hợp nơi các ví dụ khác nhau trong một batch được xử lý khác nhau [14], làm cho cả PEFT và ICL khả thi cho các mô hình đa nhiệm vụ.

Trong khi các lợi ích của PEFT giải quyết một số thiếu sót của điều chỉnh (khi so sánh với ICL), có tương đối ít sự tập trung vào việc liệu các phương pháp PEFT có hoạt động tốt khi có rất ít dữ liệu được gán nhãn hay không. Mục tiêu chính của chúng tôi trong bài báo này là đóng khoảng trống này bằng cách đề xuất một công thức – tức là, một mô hình, một phương pháp PEFT và một tập hợp siêu tham số cố định – đạt được hiệu suất mạnh trên các nhiệm vụ mới, chưa thấy trong khi chỉ cập nhật một phần nhỏ các tham số của mô hình. Cụ thể, chúng tôi dựa cách tiếp cận của mình trên mô hình T0 [1], một biến thể của T5 [15] được điều chỉnh trên hỗn hợp đa nhiệm vụ của các tập dữ liệu được gợi ý.

Để cải thiện hiệu suất trên các nhiệm vụ phân loại và lựa chọn nhiều lựa chọn, chúng tôi thêm các thành phần loss dựa trên unlikelihood [16,17] và chuẩn hóa độ dài [4]. Ngoài ra, chúng tôi phát triển (IA)3, một phương pháp PEFT nhân các kích hoạt trung gian bằng các vector đã học. (IA)3 đạt được hiệu suất mạnh hơn điều chỉnh toàn bộ mô hình trong khi cập nhật lên đến 10.000 tham số ít hơn. Cuối cùng, chúng tôi chứng minh các lợi ích của việc huấn luyện trước các tham số (IA)3 trước khi điều chỉnh [18,19]. Công thức tổng thể của chúng tôi, mà chúng tôi gọi là "T-Few", hoạt động tốt hơn đáng kể so với ICL (ngay cả đối với các mô hình lớn hơn 16 lần) và vượt trội hơn con người lần đầu tiên trên benchmark học few-shot thế giới thực RAFT [2] trong khi yêu cầu ít tính toán hơn đáng kể và cho phép các batch nhiệm vụ hỗn hợp trong quá trình suy luận. Để tạo điều kiện cho việc sử dụng T-Few trên các vấn đề mới và nghiên cứu tương lai về PEFT, chúng tôi phát hành mã của chúng tôi.1

Sau khi cung cấp nền tảng về ICL và PEFT trong phần sau, chúng tôi thảo luận về thiết kế của T-Few trong phần 3. Trong phần 4, chúng tôi trình bày các thí nghiệm so sánh T-Few với các baseline ICL mạnh. Cuối cùng, chúng tôi thảo luận về công việc liên quan trong phụ lục B và kết luận trong phần 5.

2 Nền tảng
Trong phần này, chúng tôi cung cấp một cái nhìn tổng quan về ICL và PEFT với trọng tâm vào việc đặc trưng hóa chi phí tính toán, bộ nhớ và lưu trữ trên đĩa khi đưa ra dự đoán. Chi phí thế giới thực phụ thuộc vào việc triển khai và phần cứng, vì vậy chúng tôi báo cáo chi phí theo FLOPs cho tính toán và byte cho bộ nhớ và lưu trữ, tương ứng. Công việc liên quan bổ sung được thảo luận trong phụ lục B.

2.1 Học trong ngữ cảnh few-shot (ICL)
ICL [3,4] nhằm mục đích khuyến khích một mô hình thực hiện một nhiệm vụ bằng cách đưa vào các ví dụ input-target được nối và gợi ý (gọi là "shots") cùng với một ví dụ truy vấn chưa được gán nhãn. Lấy nhiệm vụ chữ cái tuần hoàn từ Brown et al. [4] làm ví dụ, một đầu vào hoặc ngữ cảnh 4-shot sẽ là "Vui lòng sắp xếp lại các chữ cái thành một từ, và viết từ đó: asinoc = casino, yfrogg = froggy, plesim = simple, iggestb = biggest, astedro = ", mà đầu ra mong muốn sẽ là "roasted". ICL khuyến khích một mô hình ngôn ngữ tự hồi quy thực hiện nhiệm vụ này bằng cách đưa vào ngữ cảnh và lấy mẫu từ mô hình. Đối với các nhiệm vụ phân loại, mỗi nhãn được liên kết với một chuỗi (ví dụ: "positive" và "negative" cho phân tích cảm xúc) và một nhãn được gán bằng cách chọn chuỗi nhãn mà mô hình gán xác suất cao nhất. Đối với các nhiệm vụ lựa chọn nhiều lựa chọn (ví dụ: chọn giữa N câu trả lời có thể cho một câu hỏi), dự đoán của mô hình tương tự được xác định bằng cách xác định lựa chọn nào được gán xác suất cao nhất.

Lợi thế chính của ICL là nó cho phép một mô hình duy nhất thực hiện nhiều nhiệm vụ ngay lập tức mà không cần điều chỉnh. Điều này cũng cho phép các batch nhiệm vụ hỗn hợp, nơi các ví dụ khác nhau trong một batch dữ liệu tương ứng với các nhiệm vụ khác nhau bằng cách sử dụng các ngữ cảnh khác nhau trong đầu vào. ICL cũng thường được thực hiện với chỉ một số lượng hạn chế các ví dụ được gán nhãn – được gọi là học few-shot – làm cho nó hiệu quả về dữ liệu.

Mặc dù có những lợi thế này, ICL đi kèm với những nhược điểm thực tế đáng kể: Thứ nhất, việc đưa ra dự đoán đắt đỏ hơn đáng kể vì mô hình cần xử lý tất cả các ví dụ được gán nhãn trong ngữ cảnh. Cụ thể, bỏ qua độ phức tạp bậc hai của các hoạt động self-attention trong các mô hình ngôn ngữ Transformer (thường nhỏ so với chi phí của phần còn lại của mô hình [20]), xử lý k ví dụ huấn luyện cho ICL k-shot tăng chi phí tính toán khoảng k+1 lần so với việc xử lý ví dụ chưa gán nhãn một mình. Chi phí bộ nhớ tương tự tăng tỷ lệ tuyến tính với k, mặc dù trong quá trình suy luận, chi phí bộ nhớ thường bị chi phối bởi việc lưu trữ các tham số của mô hình. Riêng biệt, có một lượng nhỏ lưu trữ trên đĩa cần thiết để lưu trữ các ví dụ trong ngữ cảnh cho một nhiệm vụ nhất định. Ví dụ, lưu trữ 32 ví dụ cho một nhiệm vụ mà đầu vào và đích được gợi ý cho mỗi ví dụ dài 512 token sẽ yêu cầu khoảng 66 kilobyte lưu trữ trên đĩa (32 ví dụ 512 token 32 bit).

Ngoài các chi phí nêu trên, ICL cũng thể hiện hành vi không trực quan. Zhao et al. [12] cho thấy rằng thứ tự của các ví dụ trong ngữ cảnh có ảnh hưởng mạnh đến dự đoán của mô hình. Min et al. [9] cho thấy rằng ICL vẫn có thể hoạt động tốt ngay cả khi các nhãn của các ví dụ trong ngữ cảnh được hoán đổi (tức là làm cho không chính xác), điều này đặt ra câu hỏi về việc ICL có thực sự "học" từ các ví dụ được gán nhãn hay không.

Nhiều cách tiếp cận khác nhau đã được đề xuất để giảm thiểu những vấn đề này. Một cách để giảm chi phí tính toán là cache các vector key và value cho các ví dụ trong ngữ cảnh. Điều này có thể thực hiện được vì các mô hình ngôn ngữ Transformer chỉ decoder có mẫu masking nhân quả, vì vậy các kích hoạt của mô hình cho ngữ cảnh không phụ thuộc vào ví dụ chưa gán nhãn. Trong trường hợp cực đoan, ICL 32-shot với 512 token cho mỗi ví dụ trong ngữ cảnh sẽ dẫn đến hơn 144 gigabyte các vector key và value được cache cho mô hình GPT-3 (32 ví dụ 512 token 96 lớp 12288 d_model 32 bit mỗi cái cho các vector key và value). Riêng biệt, Min et al. [21] đề xuất ensemble ICL, nơi thay vì sử dụng xác suất đầu ra từ việc nối k ví dụ huấn luyện, các xác suất đầu ra của mô hình trên mỗi ví dụ huấn luyện (tức là ICL 1-shot cho mỗi k ví dụ) được nhân với nhau. Điều này giảm chi phí bộ nhớ không phải tham số xuống k/2 lần nhưng tăng chi phí tính toán lên 2 lần. Về hiệu suất nhiệm vụ, Min et al. [21] thấy rằng ensemble ICL vượt trội hơn biến thể nối tiêu chuẩn.

2.2 Điều chỉnh tham số hiệu quả
Trong khi điều chỉnh tiêu chuẩn cập nhật tất cả các tham số của mô hình được huấn luyện trước, đã được chứng minh rằng có thể thay vào đó cập nhật hoặc thêm một số lượng tương đối nhỏ các tham số. Các phương pháp sớm đề xuất thêm adapter [22–24], là các mạng feed-forward nhỏ có thể huấn luyện được chèn giữa các lớp trong mô hình được huấn luyện trước cố định. Kể từ đó, nhiều phương pháp PEFT tinh vi đã được đề xuất, bao gồm các phương pháp chọn một tập hợp con thưa của các tham số để huấn luyện [25,26], tạo ra các cập nhật rank thấp [13], thực hiện tối ưu hóa trong không gian con chiều thấp hơn [27], thêm các adapter rank thấp sử dụng phép nhân siêu phức [28], và nhiều hơn nữa. Liên quan, prompt tuning [14] và prefix tuning [29] nối các embedding liên tục đã học vào đầu vào hoặc kích hoạt của mô hình để khuyến khích nó thực hiện một nhiệm vụ; điều này có thể được xem như một phương pháp PEFT [30]. Các phương pháp PEFT tiên tiến có thể phù hợp với hiệu suất của việc điều chỉnh tất cả các tham số của mô hình trong khi chỉ cập nhật một phần nhỏ (ví dụ: 0.01%) các tham số của mô hình.

PEFT giảm đáng kể các yêu cầu bộ nhớ và lưu trữ cho việc huấn luyện và lưu mô hình. Ngoài ra, một số phương pháp PEFT một cách đơn giản cho phép các batch nhiệm vụ hỗn hợp – ví dụ, prompt tuning cho phép một mô hình duy nhất thực hiện nhiều nhiệm vụ đơn giản bằng cách nối các embedding prompt khác nhau vào mỗi ví dụ trong batch [14]. Mặt khác, các phương pháp PEFT tái tham số hóa mô hình (ví dụ: [27,13]) đắt đỏ hoặc khó khăn cho các batch nhiệm vụ hỗn hợp. Riêng biệt, các phương pháp PEFT khác nhau tăng tính toán và bộ nhớ cần thiết để thực hiện suy luận theo các mức độ khác nhau.

Ví dụ, adapter hiệu quả thêm các lớp (nhỏ) bổ sung vào mô hình, dẫn đến sự tăng nhỏ nhưng không thể bỏ qua chi phí tính toán và bộ nhớ. Một chi phí bổ sung phát sinh bởi PEFT là chi phí của việc điều chỉnh chính nó, phải được thực hiện một lần và sau đó được khấu hao khi mô hình được sử dụng cho suy luận. Tuy nhiên, chúng tôi sẽ cho thấy rằng PEFT có thể hiệu quả hơn đáng kể về mặt tính toán khi xem xét cả điều chỉnh và suy luận trong khi đạt được độ chính xác tốt hơn ICL.

3 Thiết kế công thức T-Few
Với việc PEFT cho phép một mô hình được thích ứng với một nhiệm vụ mới với yêu cầu lưu trữ tương đối nhỏ và chi phí tính toán, chúng tôi lập luận rằng PEFT trình bày một sự thay thế hứa hẹn cho ICL. Mục tiêu của chúng tôi do đó là phát triển một công thức cho phép một mô hình đạt được độ chính xác cao trên các nhiệm vụ mới với các ví dụ được gán nhãn hạn chế trong khi cho phép các batch nhiệm vụ hỗn hợp trong quá trình suy luận và phát sinh chi phí tính toán và lưu trữ tối thiểu. Bằng công thức, chúng tôi có nghĩa là một mô hình cụ thể và cài đặt siêu tham số cung cấp hiệu suất mạnh trên bất kỳ nhiệm vụ mới nào mà không cần điều chỉnh thủ công hoặc điều chỉnh theo từng nhiệm vụ.

Bằng cách này, chúng tôi có thể đảm bảo rằng cách tiếp cận của chúng tôi là một lựa chọn thực tế trong các cài đặt few-shot nơi dữ liệu được gán nhãn hạn chế có sẵn để đánh giá [31, 32].

3.1 Mô hình và Tập dữ liệu
Như một bước đầu tiên, chúng tôi phải chọn một mô hình được huấn luyện trước. Lý tưởng nhất, mô hình nên đạt được hiệu suất cao trên các nhiệm vụ mới sau khi điều chỉnh trên một số lượng hạn chế các ví dụ được gán nhãn. Trong các thí nghiệm sơ bộ áp dụng các phương pháp PEFT cho các mô hình được huấn luyện trước khác nhau, chúng tôi đạt được hiệu suất tốt nhất với T0 [1]. T0 dựa trên T5 [15], một mô hình Transformer encoder-decoder [33] đã được huấn luyện trước thông qua một mục tiêu mô hình hóa ngôn ngữ có mask [34] trên một corpus lớn dữ liệu văn bản chưa gán nhãn. T0 được tạo ra bằng cách điều chỉnh T5 trên hỗn hợp đa nhiệm vụ của các tập dữ liệu để cho phép khái quát hóa zero-shot, tức là khả năng thực hiện các nhiệm vụ mà không cần bất kỳ huấn luyện dựa trên gradient bổ sung nào. Các ví dụ trong các tập dữ liệu được sử dụng để huấn luyện T0 được gợi ý bằng cách áp dụng các mẫu prompt từ Public Pool of Prompts (P3 [35]), chuyển đổi mỗi ví dụ trong mỗi tập dữ liệu sang định dạng văn bản-đến-văn bản được gợi ý nơi mỗi nhãn tương ứng với một chuỗi khác nhau. Để ngắn gọn, chúng tôi bỏ qua mô tả chi tiết về T0 và T5; độc giả quan tâm có thể tham khảo Sanh et al. [1] và Raffel et al. [15]. T0 được phát hành trong các biến thể ba tỷ và mười một tỷ tham số, được gọi là "T0-3B" và đơn giản là "T0" tương ứng. Trong phần này (nơi mục tiêu của chúng tôi là thiết kế công thức T-Few thông qua thí nghiệm rộng rãi), chúng tôi sử dụng T0-3B để giảm chi phí tính toán. Đối với tất cả các mô hình và thí nghiệm, chúng tôi sử dụng Hugging Face Transformers [36].

Trong khi T0 được thiết kế cho khái quát hóa zero-shot, chúng tôi sẽ chứng minh rằng nó cũng đạt được hiệu suất mạnh sau khi điều chỉnh với chỉ một vài ví dụ được gán nhãn. Để kiểm tra khái quát hóa của T0, Sanh et al. [1] chọn một tập hợp các nhiệm vụ (và các tập dữ liệu tương ứng) để giữ lại từ hỗn hợp huấn luyện đa nhiệm vụ – cụ thể, hoàn thành câu (COPA [37], H-SWAG [38], và Story Cloze [39] datasets), suy luận ngôn ngữ tự nhiên (ANLI [40], CB [41], và RTE [42]), giải quyết đồng tham chiếu (WSC [43] và Winogrande [44]), và định nghĩa từ (WiC [45]). Đánh giá khả năng khái quát hóa sau đó có thể được thực hiện một cách đơn giản bằng cách đo hiệu suất trên các tập dữ liệu được giữ lại này.

Chúng tôi cũng sẽ sau đó kiểm tra khả năng của T-Few trong benchmark RAFT [2] trong phần 4.3, một bộ sưu tập các nhiệm vụ few-shot "thế giới thực" chưa thấy không có tập validation và một tập test được giữ lại. ANLI, WiC, WSC được cấp phép theo Creative Commons License. Winogrande được cấp phép theo giấy phép Apache. COPA theo giấy phép BSD-2 Clause. Chúng tôi không thể tìm thấy giấy phép của RTE và CB nhưng chúng là một phần của SuperGLUE nói rằng các tập dữ liệu được phép sử dụng trong bối cảnh nghiên cứu.

Để dễ dàng so sánh, chúng tôi sử dụng cùng số lượng ví dụ huấn luyện few-shot cho mỗi tập dữ liệu như Brown et al. [4], dao động từ 20 đến 70. Thật không may, các tập hợp con tập dữ liệu few-shot được sử dụng bởi Brown et al. [4] chưa được tiết lộ công khai. Để cho phép so sánh mạnh mẽ hơn, chúng tôi do đó xây dựng năm tập dữ liệu few-shot bằng cách lấy mẫu các tập hợp con với các hạt giống khác nhau và báo cáo trung vị và khoảng tứ phân vị. Chúng tôi gợi ý các ví dụ từ mỗi tập dữ liệu sử dụng các mẫu prompt từ P3 Bach et al. [35], sử dụng một mẫu prompt được lấy mẫu ngẫu nhiên cho mỗi ví dụ tại mỗi bước. Trừ khi nêu khác, chúng tôi huấn luyện mô hình của chúng tôi trong 1K bước với kích thước batch 8 và báo cáo hiệu suất ở cuối huấn luyện.

Để đánh giá, chúng tôi sử dụng "phân loại xếp hạng", nơi log-xác suất của mô hình cho tất cả các chuỗi nhãn có thể được xếp hạng và dự đoán của mô hình được coi là chính xác nếu lựa chọn xếp hạng cao nhất là câu trả lời chính xác. Đánh giá phân loại xếp hạng tương thích với cả nhiệm vụ phân loại và lựa chọn nhiều lựa chọn. Vì hiệu suất mô hình có thể thay đổi đáng kể tùy thuộc vào mẫu prompt được sử dụng, chúng tôi báo cáo độ chính xác trung vị trên tất cả các mẫu prompt từ P3 và trên các tập hợp con dữ liệu few-shot cho mỗi tập dữ liệu. Đối với tất cả các tập dữ liệu, chúng tôi báo cáo độ chính xác trên tập test hoặc tập validation khi các nhãn test không công khai (ví dụ: các tập dữ liệu SuperGLUE). Trong văn bản chính, chúng tôi báo cáo độ chính xác trung vị trên chín tập dữ liệu được đề cập ở trên. Kết quả chi tiết trên mỗi tập dữ liệu được cung cấp trong các phụ lục.

3.2 Huấn luyện Unlikelihood và Chuẩn hóa Độ dài
Trước khi điều tra các phương pháp PEFT, chúng tôi trước tiên khám phá hai thành phần loss bổ sung để cải thiện hiệu suất của việc điều chỉnh few-shot các mô hình ngôn ngữ. Các mô hình ngôn ngữ thường được huấn luyện với loss cross-entropy LLM = -1/T ∑t log p(yt|x; y<t) nơi mô hình được huấn luyện để tăng xác suất của chuỗi đích chính xác y = (y1, y2, ..., yT) cho chuỗi đầu vào x.

Để đánh giá, chúng tôi sử dụng phân loại xếp hạng (được mô tả trong phần 3.1) phụ thuộc vào cả xác suất mà mô hình gán cho lựa chọn chính xác cũng như các xác suất được gán bởi mô hình cho các lựa chọn không chính xác. Để tính đến điều này trong quá trình huấn luyện, chúng tôi xem xét việc thêm loss unlikelihood [16, 17]:

LUL = -∑n=1^N ∑t=1^T(n) log(1-p(ŷt^(n)|x; ŷ<t^(n))) / ∑n=1^N T(n)     (1)

điều này ngăn cản mô hình dự đoán token từ các chuỗi đích không chính xác, nơi ŷ(n) = (ŷ1, ŷ2, ..., ŷT(n)) là chuỗi đích không chính xác thứ n của N chuỗi đích không chính xác. Chúng tôi giả thiết rằng việc thêm LUL sẽ cải thiện kết quả trên phân loại xếp hạng vì mô hình sẽ được huấn luyện để gán xác suất thấp hơn cho các lựa chọn không chính xác, từ đó cải thiện cơ hội mà lựa chọn chính xác được xếp hạng cao nhất.

Các chuỗi đích có thể cho một ví dụ huấn luyện nhất định có thể có độ dài khác nhau đáng kể, đặc biệt trong các nhiệm vụ lựa chọn nhiều lựa chọn. Xếp hạng mỗi lựa chọn dựa trên xác suất do đó có thể "ưu tiên" các lựa chọn ngắn hơn vì xác suất được gán của mô hình cho mỗi token ≤ 1. Để khắc phục điều này, chúng tôi xem xét việc sử dụng chuẩn hóa độ dài khi thực hiện phân loại xếp hạng, điều này chia điểm số của mô hình trên mỗi lựa chọn câu trả lời có thể cho số lượng token trong lựa chọn (như được sử dụng trong GPT-3 [4]). Khi sử dụng chuẩn hóa độ dài trong quá trình đánh giá, chúng tôi giới thiệu một thành phần loss bổ sung trong quá trình huấn luyện phản ánh chặt chẽ hơn đánh giá được chuẩn hóa độ dài. Đầu tiên, chúng tôi tính toán log xác suất chuẩn hóa độ dài của một chuỗi đầu ra nhất định ℓ(x,y) = 1/T ∑t=1^T log p(yt|x; y<t). Sau đó, chúng tôi tối đa hóa log xác suất chuẩn hóa độ dài của lựa chọn câu trả lời chính xác bằng cách tối thiểu hóa loss softmax cross-entropy:

LLN = -log(exp(ℓ(x,y)) / (exp(ℓ(x,y)) + ∑n=1^N exp(ℓ(x,ŷ(n)))))     (2)

Khi huấn luyện một mô hình với LLM, LUL, và LLN, chúng tôi đơn giản cộng chúng lại. Điều này tránh việc giới thiệu bất kỳ siêu tham số nào sẽ có vấn đề để điều chỉnh trong cài đặt few-shot (nơi các tập validation có kích thước thực tế là nhỏ bởi cần thiết [31, 32]).

Chúng tôi báo cáo kết quả của việc điều chỉnh tất cả các tham số của T0-3B với và không có chuẩn hóa độ dài trên tất cả các tập dữ liệu trong phụ lục C. Chúng tôi thấy rằng việc thêm LLN cải thiện độ chính xác từ 60.7% lên 62.71% và bao gồm cả LUL và LLN cung cấp cải thiện thêm lên 63.3%. Vì các thành phần loss này cải thiện hiệu suất mà không giới thiệu bất kỳ siêu tham số bổ sung nào, chúng tôi bao gồm chúng trong công thức của chúng tôi và sử dụng chúng trong tất cả các thí nghiệm sau.

3.3 Điều chỉnh tham số hiệu quả với (IA)3
Để so sánh thuận lợi với ICL few-shot, chúng tôi cần một phương pháp PEFT có các tính chất sau: Thứ nhất, nó phải thêm hoặc cập nhật càng ít tham số càng tốt để tránh phát sinh chi phí lưu trữ và bộ nhớ. Thứ hai, nó nên đạt được độ chính xác mạnh sau khi huấn luyện few-shot trên các nhiệm vụ mới. Cuối cùng, nó phải cho phép các batch nhiệm vụ hỗn hợp, vì đó là một khả năng của ICL. Để dễ dàng cho phép các batch nhiệm vụ hỗn hợp, một phương pháp PEFT lý tưởng không nên sửa đổi chính mô hình. Nếu không, mỗi ví dụ trong một batch sẽ hiệu quả cần được xử lý bởi một mô hình hoặc đồ thị tính toán khác nhau. Một sự thay thế thuận tiện hơn được cung cấp bởi các phương pháp trực tiếp sửa đổi các kích hoạt của mô hình vì điều này có thể được thực hiện độc lập và rẻ cho mỗi ví dụ trong batch theo nhiệm vụ mà ví dụ tương ứng. Prompt tuning và các phương pháp prefix tuning [14,29] hoạt động bằng cách nối các vector đã học vào chuỗi kích hoạt hoặc embedding và do đó là các ví dụ về các phương pháp PEFT sửa đổi kích hoạt cho phép các batch nhiệm vụ hỗn hợp. Tuy nhiên, như chúng tôi sẽ thảo luận sau, chúng tôi không thể đạt được độ chính xác hợp lý với prompt tuning và thấy rằng các phương pháp PEFT hiệu quả hơn không cho phép các batch nhiệm vụ hỗn hợp. Do đó chúng tôi phát triển một phương pháp PEFT mới đáp ứng các yêu cầu của chúng tôi.

Như một sự thay thế, chúng tôi khám phá phép nhân theo từng phần tử (tức là điều chỉnh tỷ lệ) của các kích hoạt của mô hình chống lại một vector đã học. Cụ thể, chúng tôi xem xét thích ứng của dạng l ⊙ x nơi l ∈ ℝd là một vector cụ thể nhiệm vụ đã học, ⊙ biểu thị phép nhân theo từng phần tử, và x ∈ ℝT×d là một chuỗi độ dài T của các kích hoạt. Chúng tôi sử dụng "ký hiệu broadcasting" [46] để mục (i,j) thứ của l ⊙ x là ljxi,j.

Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng không cần thiết phải giới thiệu một vector điều chỉnh tỷ lệ đã học cho mỗi tập hợp kích hoạt trong mô hình Transformer. Thay vào đó, chúng tôi thấy rằng đủ để giới thiệu các vector điều chỉnh tỷ lệ trên các key và value trong các cơ chế self-attention và encoder-decoder attention và trên kích hoạt trung gian của các mạng feed-forward theo vị trí. Cụ thể, sử dụng ký hiệu từ Vaswani et al. [33], chúng tôi giới thiệu ba vector đã học lk ∈ ℝdk, lv ∈ ℝdv, và lγ ∈ ℝdff, được giới thiệu vào các cơ chế attention như:

softmax(Q(lk ⊙ K)T/√dk)(lv ⊙ V)

và trong các mạng feed-forward theo vị trí như W2(σ(lγ ⊙ (W1x))), nơi σ là phi tuyến tính của mạng feed-forward. Chúng tôi giới thiệu một tập hợp riêng biệt các vector lk, lv, và lγ trong mỗi khối lớp Transformer. Điều này thêm tổng cộng L(dk + dv + dγ) tham số mới cho một encoder Transformer L khối lớp và L(2dk + 2dv + dγ) (với các yếu tố 2 tính đến sự hiện diện của cả self-attention và encoder-decoder attention) cho một decoder L khối lớp. lk, lv, và lγ đều được khởi tạo bằng số một để hàm tổng thể được tính toán bởi mô hình không thay đổi khi chúng được thêm vào. Chúng tôi gọi phương pháp của chúng tôi là (IA)3, viết tắt của "Infused Adapter by Inhibiting and Amplifying Inner Activations".

(IA)3 làm cho các batch nhiệm vụ hỗn hợp có thể thực hiện được vì mỗi chuỗi kích hoạt trong batch có thể được nhân riêng biệt và rẻ bằng vector nhiệm vụ đã học liên kết của nó. Chúng tôi cũng lưu ý rằng, trong trường hợp một mô hình chỉ được sử dụng trên một nhiệm vụ duy nhất, các sửa đổi được giới thiệu bởi (IA)3 cũng có thể được áp dụng cho các ma trận trọng số vĩnh viễn để không cần phép nhân theo từng phần tử và kiến trúc của mô hình vẫn không thay đổi. Điều này có thể thực hiện được vì các phép nhân theo từng phần tử được thực hiện trong (IA)3 luôn xuất hiện cùng với một phép nhân ma trận, và l ⊙ Wx = (l ⊙ W)x. Trong trường hợp này, phương pháp của chúng tôi không phát sinh chi phí tính toán bổ sung so với mô hình gốc.

Để xác nhận (IA)3, chúng tôi so sánh nó với nhiều phương pháp thích ứng hiện có trong cài đặt điều chỉnh T0-3B trên các tập dữ liệu few-shot từ các nhiệm vụ được giữ lại. Cụ thể, chúng tôi so sánh với 9 phương pháp PEFT mạnh: BitFit [47] chỉ cập nhật các tham số bias; Adapter [23] giới thiệu các lớp cụ thể nhiệm vụ sau self-attention và các mạng feed-forward theo vị trí; Compacter và Compacter++ [28] cải thiện adapter bằng cách sử dụng các ma trận rank thấp và phép nhân siêu phức; prompt tuning [14] học các embedding prompt cụ thể nhiệm vụ được nối vào đầu vào của mô hình; FISH Mask [26] chọn một tập hợp con các tham số để cập nhật dựa trên thông tin Fisher xấp xỉ của chúng; Intrinsic SAID [27] thực hiện tối ưu hóa trong không gian con chiều thấp; prefix-tuning [29] học các vector cụ thể nhiệm vụ được nối vào các kích hoạt của mô hình; và LoRA [13] gán các cập nhật rank thấp cho các ma trận tham số. Ngoài ra, chúng tôi bao gồm các baseline của việc điều chỉnh toàn bộ mô hình và chỉ cập nhật các tham số chuẩn hóa lớp.

Đối với một số phương pháp cho phép thay đổi hiệu quả tham số, chúng tôi báo cáo kết quả cho các ngân sách khác nhau: độ thưa 0.2% và 0.02% cho FISH Mask, 10 và 100 vector prompt đã học cho prompt tuning, và các không gian con 20.000 hoặc 500.000 chiều cho Intrinsic SAID.

Kết quả được hiển thị trong hình 2, với kết quả chi tiết cho mỗi tập dữ liệu trong phụ lục D. Chúng tôi thấy rằng (IA)3 là phương pháp duy nhất đạt được độ chính xác cao hơn baseline điều chỉnh toàn bộ mô hình. Trong khi các phương pháp PEFT khác (ví dụ: Intrinsic SAID và prompt tuning) cập nhật hoặc giới thiệu ít tham số hơn, (IA)3 hoạt động tốt hơn đáng kể. Kết quả và cài đặt của chúng tôi khác với một số công việc trước đây về các phương pháp PEFT mà chúng tôi so sánh. Mahabadi et al. [28] báo cáo rằng Compacter và Compacter++ vượt trội hơn điều chỉnh toàn bộ mô hình, bao gồm trong cài đặt few-shot. Lester et al. [14] thấy rằng prompt tuning có thể phù hợp với điều chỉnh toàn bộ mô hình, và trong công việc tiếp theo Wei et al. [48] thấy rằng prompt tuning hoạt động tốt khi được áp dụng cho một mô hình được điều chỉnh đa nhiệm vụ trong cài đặt few-shot.

Trong cả hai trường hợp, chúng tôi thử nghiệm với các lựa chọn siêu tham số khác nhau để cố gắng phù hợp với kết quả trước đây. Chúng tôi giả thiết sự bất đồng đến từ việc chúng tôi sử dụng một mô hình và các tập dữ liệu khác nhau. Đối với prompt tuning cụ thể, chúng tôi nhận thấy rằng hiệu suất tập validation có thể dao động dữ dội trong quá trình huấn luyện, gợi ý về các vấn đề tối ưu hóa có thể.

--- TRANG 7 ---
0.001% 0.01% 0.1%
% tham số được cập nhật
50
55
60
65
Độ chính xác
Tất cả tham số
(IA)³
LoRA
BitFit
Layer Norm
Compacter
Compacter++
Prompt Tuning
Prefix Tuning
Adapter
FISH Mask
Intrinsic SAID

Hình 2: Độ chính xác của các phương pháp PEFT với LUL và LLN khi áp dụng cho T0-3B. Các phương pháp với ngân sách tham số biến đổi được biểu thị bằng các điểm đánh dấu lớn hơn và nhỏ hơn cho nhiều hoặc ít tham số hơn.

10¹² 10¹³ 10¹⁴ 10¹⁵
FLOPs mỗi ví dụ
50
55
60
65
70
Độ chính xác
T-Few
T0
T5+LM
GPT-3 6.7B
GPT-3 13B
GPT-3 175B

Hình 3: Độ chính xác của các phương pháp học few-shot khác nhau. T-Few sử dụng (IA)³ cho các phương pháp PEFT của T0, T0 sử dụng học zero-shot, và T5+LM và các biến thể GPT-3 sử dụng ICL few-shot. Trục x tương ứng với chi phí suy luận; chi tiết được cung cấp trong phần 4.2.

3.4 Huấn luyện trước (IA)³
Trong công việc gần đây, Gu et al. [18], Vu et al. [19] cho thấy rằng việc huấn luyện trước các embedding prompt trong prompt tuning có thể cải thiện hiệu suất khi điều chỉnh trên các nhiệm vụ few-shot downstream. Để huấn luyện trước, Gu et al. [18] sử dụng một bộ các nhiệm vụ tự giám sát được áp dụng cho dữ liệu văn bản chưa gán nhãn, và Vu et al. [19] xem xét việc sử dụng embedding từ một nhiệm vụ riêng biệt hoặc hỗn hợp đa nhiệm vụ. Chúng tôi theo Vu et al. [19] và đơn giản huấn luyện trước các tham số mới được giới thiệu bởi (IA)³ trên cùng hỗn hợp đa nhiệm vụ được sử dụng để huấn luyện T0. Chúng tôi huấn luyện trước trong 100.000 bước với kích thước batch 16 trước khi điều chỉnh các tham số (IA)³ trên mỗi tập dữ liệu downstream riêng lẻ. Một so sánh đầy đủ về độ chính xác với và không có huấn luyện trước (IA)³ được chi tiết trong phụ lục E. Chúng tôi thấy rằng huấn luyện trước cải thiện độ chính xác được điều chỉnh từ 64.6 lên 65.8 và do đó thêm nó vào công thức của chúng tôi.

3.5 Kết hợp các thành phần
Tóm lại, công thức T-Few được định nghĩa như sau: Chúng tôi sử dụng mô hình T0 làm backbone. Chúng tôi thêm (IA)³ để thích ứng nhiệm vụ downstream và sử dụng các tham số được khởi tạo từ việc huấn luyện trước (IA)³ trên cùng hỗn hợp đa nhiệm vụ cho T0. Như một mục tiêu, chúng tôi sử dụng tổng của loss mô hình hóa ngôn ngữ tiêu chuẩn LLM, loss unlikelihood LUL cho các lựa chọn không chính xác, và loss chuẩn hóa độ dài LLN. Chúng tôi huấn luyện trong 1.000 bước với kích thước batch 8 chuỗi sử dụng trình tối ưu hóa Adafactor [49] với tốc độ học 3e-3 và lịch trình suy giảm tuyến tính với khởi động 60 bước. Chúng tôi áp dụng các mẫu prompt cho các tập dữ liệu downstream trong quá trình huấn luyện và suy luận để chuyển đổi mỗi ví dụ thành định dạng văn bản-đến-văn bản có hướng dẫn. Quan trọng, chúng tôi áp dụng công thức này cho mỗi tập dữ liệu downstream theo cách hoàn toàn giống nhau mà không cần điều chỉnh siêu tham số hoặc sửa đổi cho từng tập dữ liệu. Điều này làm cho công thức trở thành một lựa chọn thực tế cho các cài đặt học few-shot nơi các tập validation nhỏ theo định nghĩa [31, 32].

4 Vượt trội hơn ICL với T-Few
Sau khi thiết kế và thiết lập công thức T-Few trên T0-3B, chúng tôi hiện áp dụng nó cho T0 (với 11 tỷ tham số) và so sánh hiệu suất với các baseline ICL few-shot mạnh. Từ điểm này trở đi, chúng tôi sử dụng chính xác cùng công thức và siêu tham số trên tất cả các nhiệm vụ.

4.1 Hiệu suất trên các nhiệm vụ T0
Đầu tiên, chúng tôi đánh giá T-Few trên các tập dữ liệu được giữ lại từ hỗn hợp huấn luyện của T0. Chúng tôi so sánh với học zero-shot với T0 [1] (vì chúng tôi thấy ICL few-shot hoạt động tệ hơn zero-shot đối với T0, xem phụ lục F); ICL few-shot với T5+LM [14] (mô hình ngôn ngữ dự đoán bước tiếp theo mà T0 dựa trên); và ICL few-shot với các biến thể 6.7, 13, và 175 tỷ tham số của GPT-3. Xem phụ lục F để biết thêm chi tiết về các baseline này. Độ chính xác trên các tập dữ liệu T0 được giữ lại (được mô tả trong phần 3.1) được hiển thị trong bảng 1 và hình 3, với kết quả cho mỗi tập dữ liệu được báo cáo trong phụ lục F. Chúng tôi thấy rằng T-Few vượt trội hơn tất cả các phương pháp khác với một biên độ đáng kể. Đáng chú ý, T-Few đạt được độ chính xác cao hơn 6% so với ICL few-shot với GPT-3 175B mặc dù nhỏ hơn khoảng 16 lần và vượt trội hơn các biến thể GPT-3 nhỏ hơn với biên độ thậm chí lớn hơn. T-Few cũng đạt được độ chính xác cao hơn đáng kể so với cả học zero-shot với T0 và ICL few-shot với T5+LM.

Phương pháp | FLOPs suy luận | FLOPs huấn luyện | Không gian đĩa | Độ chính xác
T-Few | 1.1e12 | 2.7e16 | 4.2 MB | 72.4%
T0 [1] | 1.1e12 | 0 | 0 B | 66.9%
T5+LM [14] | 4.5e13 | 0 | 16 kB | 49.6%
GPT-3 6.7B [4] | 5.4e13 | 0 | 16 kB | 57.2%
GPT-3 13B [4] | 1.0e14 | 0 | 16 kB | 60.3%
GPT-3 175B [4] | 1.4e15 | 0 | 16 kB | 66.6%

Bảng 1: Độ chính xác trên các nhiệm vụ T0 được giữ lại và chi phí tính toán cho các phương pháp và mô hình học few-shot khác nhau. T-Few đạt được độ chính xác cao nhất với chi phí tính toán thấp hơn 1.000 lần so với ICL với GPT-3 175B. Điều chỉnh với T-Few có chi phí khoảng bằng ICL trên 20 ví dụ với GPT-3 175B.

Phương pháp | Độ chính xác
T-Few | 75.8%
Baseline con người [2] | 73.5%
PET [50] | 69.6%
SetFit [51] | 66.9%
GPT-3 [4] | 62.7%

Bảng 2: Top-5 phương pháp tốt nhất trên RAFT tại thời điểm viết. T-Few là phương pháp đầu tiên vượt trội hơn baseline con người và đạt được độ chính xác cao hơn 6% so với phương pháp tốt nhất tiếp theo.

4.2 So sánh chi phí tính toán
Sau khi thiết lập rằng T-Few vượt trội hơn đáng kể các mô hình dựa trên ICL, chúng tôi hiện so sánh chi phí tương đối của mỗi cách tiếp cận học few-shot. Để đơn giản, chúng tôi sử dụng các ước tính FLOPs-per-token cho các mô hình ngôn ngữ dựa trên Transformer được giới thiệu bởi Kaplan et al. [20]. Cụ thể, chúng tôi ước tính rằng một Transformer chỉ decoder (ví dụ: chuỗi GPT) với N tham số sử dụng 2N FLOPs mỗi token để suy luận và 6N FLOPs mỗi token để huấn luyện. Các mô hình encoder-decoder như T0 và T5 (nơi encoder và decoder có cùng số lượng lớp và kích thước lớp) chỉ xử lý mỗi token bằng encoder hoặc decoder (mỗi cái có khoảng một nửa tham số của toàn bộ mô hình), vì vậy các ước tính FLOPs mỗi token được giảm một nửa xuống N và 3N FLOPs mỗi token để suy luận và huấn luyện. Chúng tôi lưu ý rằng FLOPs không phải là đo lường trực tiếp chi phí tính toán thế giới thực vì độ trễ, sử dụng điện năng, và các chi phí khác có thể thay đổi đáng kể tùy thuộc vào phần cứng và các yếu tố khác [52]. Tuy nhiên, chúng tôi tập trung vào FLOPs vì nó là một chỉ số độc lập phần cứng có mối tương quan chặt chẽ với chi phí thế giới thực - cài đặt phần cứng được sử dụng để chạy các phương pháp khác nhau mà chúng tôi xem xét có thể thay đổi đáng kể giữa các phương pháp. Chúng tôi tóm tắt chi phí trong bảng 1 và thảo luận chúng dưới đây. Đối với tất cả các ước tính, chúng tôi sử dụng số lượng shot trung vị (41) trên các tập dữ liệu mà chúng tôi xem xét. Đánh giá xếp hạng và loss unlikelihood của chúng tôi đều yêu cầu xử lý mọi lựa chọn đầu ra có thể để đạt được dự đoán cho một ví dụ chưa gán nhãn. Độ dài chuỗi được token hóa kết hợp trung vị cho đầu vào và tất cả các đích có thể là 103 cho các tập dữ liệu mà chúng tôi xem xét. Đối với các ví dụ trong ngữ cảnh được xử lý cho ICL few-shot, chỉ đích chính xác là cần thiết, tạo ra độ dài chuỗi trung vị là 98. Giả sử rằng các vector key và value được cache, xử lý một ví dụ đơn với ICL do đó liên quan đến việc xử lý 41×98 + 103 token. Một tóm tắt các ước tính chi phí của chúng tôi được cung cấp trong bảng 1.

Chi phí suy luận. Ngoài độ chính xác được cải thiện, lợi thế chính của việc tránh ICL few-shot là chi phí suy luận thấp hơn đáng kể. Xử lý một đầu vào đơn và tất cả các lựa chọn đích với T-Few yêu cầu 11e9×103 = 1.1e12 FLOPs, trong khi ICL few-shot với GPT-3 175B yêu cầu 2×175e9×(41×98 + 103) = 1.4e15 FLOPs – hơn 3 bậc độ lớn hơn. Chi phí suy luận với ICL sử dụng các biến thể GPT-3 nhỏ hơn cũng cao hơn đáng kể so với chi phí suy luận của T-Few. Như đã thảo luận trong phần 2.1, việc cache các vector key và value khi cùng một tập hợp các ví dụ trong ngữ cảnh được tái sử dụng có thể giảm chi phí tính toán của ICL. Tuy nhiên, điều này chỉ dẫn đến việc giảm khoảng 41×, không đủ để làm cho bất kỳ chi phí ICL GPT-3 nào thấp bằng T-Few.

Chi phí huấn luyện. Vì T-Few là phương pháp duy nhất liên quan đến việc cập nhật tham số, nó là phương pháp duy nhất phát sinh chi phí huấn luyện. Huấn luyện một mô hình encoder-decoder mười một tỷ tham số trong 1.000 bước với kích thước batch 8 chuỗi độ dài 103 yêu cầu khoảng 3×11e9×1.000×8×103 = 2.7e16 FLOPs. Mặc dù không nhỏ, điều này chỉ lớn hơn khoảng 20 lần so với FLOPs cần thiết để xử lý một ví dụ đơn với ICL few-shot sử dụng GPT-3 175B. Nói cách khác, huấn luyện T-Few có chi phí bằng việc sử dụng GPT-3 175B để xử lý 20 ví dụ với ICL few-shot. Chúng tôi cũng thấy rằng việc điều chỉnh T0 với T-Few trên một tập dữ liệu đơn chỉ mất khoảng nửa giờ trên một GPU NVIDIA A100 duy nhất. Tại thời điểm viết, điều này sẽ có chi phí khoảng 2 USD sử dụng Microsoft Azure.²

Chi phí lưu trữ. T-Few cũng phát sinh chi phí lưu trữ lớn nhất. Khi được lưu trữ như các số thực độ chính xác đơn, các tham số được thêm bởi (IA)³ chiếm 4.2 MB không gian trên đĩa. Ngược lại, các phương pháp ICL chỉ yêu cầu lưu trữ các ví dụ trong ngữ cảnh được token hóa (thường được lưu trữ như số nguyên 32-bit), dẫn đến yêu cầu không gian đĩa nhỏ hơn là 41×98×32 bit = 16 kB. Tuy nhiên, chúng tôi lưu ý rằng 4.2 MB bị lu mờ bởi kích thước trên đĩa của chính các checkpoint mô hình – lưu trữ các vector thích ứng (IA)³ cho 10.000 nhiệm vụ sẽ mất khoảng bằng không gian như checkpoint T0 (41.5 GB).

Sử dụng bộ nhớ. Trong quá trình suy luận, chi phí bộ nhớ chính phát sinh từ các tham số của mô hình. Mô hình duy nhất nhỏ hơn T0 (được sử dụng bởi T-Few) là GPT-3 6.7B; nếu không, T-Few sẽ phát sinh chi phí bộ nhớ thấp hơn trong quá trình suy luận. Chi phí bộ nhớ bổ sung phát sinh khi huấn luyện T-Few do cần cache các kích hoạt trung gian cho lan truyền ngược và cho các biến tích lũy gradient trong Adafactor. Tuy nhiên, như đã đề cập ở trên, có thể sử dụng công thức T-Few trên một GPU A100 80GB duy nhất.

4.3 Hiệu suất trên Các nhiệm vụ Few-shot Thế giới thực (RAFT)
Cho đến nay, chúng tôi đã đánh giá hiệu suất trên một bộ sưu tập các tập dữ liệu không được thiết kế rõ ràng để đánh giá việc học few-shot. Để đánh giá tốt hơn hiệu suất của T-Few trong thế giới thực, chúng tôi đánh giá cách tiếp cận của chúng tôi trên benchmark RAFT [2]. RAFT bao gồm 11 nhiệm vụ "có giá trị kinh tế" nhằm phản ánh các ứng dụng thế giới thực. Quan trọng, mỗi tập dữ liệu RAFT chỉ có 50 ví dụ huấn luyện không có tập validation và một tập test (lớn hơn) không có nhãn công khai, vì vậy không thể "gian lận" bằng cách điều chỉnh trên tập validation lớn một cách không thực tế hoặc bằng cách nhìn trộm tập test [32,31]. Chúng tôi áp dụng T-Few cho RAFT bằng cách sử dụng các prompt tiêu chuẩn được phát hành cùng với tập dữ liệu. Độ chính xác của top-5 phương pháp hiện tại được hiển thị trong bảng 2, với chi tiết thêm được cung cấp trong phụ lục H. T-Few đạt được độ chính xác tiên tiến là 75.8% và vượt trội hơn baseline con người (73.5% độ chính xác) lần đầu tiên. Mô hình tốt nhất tiếp theo (từ Schick và Schütze [50]) đạt được độ chính xác thấp hơn 6% và GPT-3 175B chỉ đạt được 62.7%. Những kết quả này xác nhận rằng T-Few có thể được áp dụng ngay lập tức cho các nhiệm vụ thế giới thực mới để đạt được hiệu suất mạnh.

4.4 Thí nghiệm ablation
Với việc các thí nghiệm thiết kế T-Few của chúng tôi trên T0-3B, chúng tôi thực hiện ablation một số thành phần của T-Few trên T0. Kết quả chi tiết được hiển thị trong phụ lục G. Mặc dù lợi ích từ việc thêm mỗi thành phần không luôn tăng đáng kể độ chính xác trên mỗi tập dữ liệu riêng lẻ, mỗi thành phần đều cải thiện hiệu suất trung bình trên các tập dữ liệu một cách nhất quán: Loại bỏ huấn luyện trước giảm độ chính xác 1.6%, loại bỏ huấn luyện unlikelihood và chuẩn hóa độ dài giảm độ chính xác 4.1%, và loại bỏ cả huấn luyện trước và các thành phần loss bổ sung của chúng tôi giảm độ chính xác 2.5%.

5 Kết luận
Chúng tôi đã giới thiệu T-Few, một công thức học few-shot hiệu quả tham số đạt được độ chính xác cao hơn ICL few-shot với chi phí tính toán thấp hơn. T-Few sử dụng (IA)³, một phương pháp PEFT mới điều chỉnh tỷ lệ các kích hoạt bên trong bằng các vector đã học. Sử dụng (IA)³ tạo ra hiệu suất tốt hơn điều chỉnh toàn bộ mô hình trong khi chỉ giới thiệu một lượng nhỏ tham số bổ sung. T-Few cũng sử dụng hai thành phần loss bổ sung khuyến khích mô hình xuất ra xác suất thấp hơn cho các lựa chọn không chính xác và tính đến độ dài của các lựa chọn câu trả lời khác nhau. Khi áp dụng T-Few nguyên vẹn (không có điều chỉnh siêu tham số cụ thể nhiệm vụ hoặc thay đổi khác) cho benchmark RAFT, chúng tôi đạt được hiệu suất siêu con người lần đầu tiên và vượt trội hơn các submission trước đó với biên độ lớn. Thông qua đặc trưng hóa chi tiết về chi phí tính toán, chúng tôi thấy rằng T-Few sử dụng ít hơn 1.000 lần FLOPs trong quá trình suy luận so với ICL few-shot với GPT-3 và chỉ yêu cầu 30 phút để huấn luyện trên một GPU NVIDIA A100 duy nhất. Vì tất cả các thí nghiệm của chúng tôi đều trên các nhiệm vụ phân loại, chúng tôi quan tâm đến việc áp dụng T-Few cho các nhiệm vụ sinh như tóm tắt và trả lời câu hỏi trong công việc tương lai.

Chúng tôi hy vọng kết quả của chúng tôi cung cấp một góc nhìn mới về cách tốt nhất để thực hiện học few-shot với các mô hình ngôn ngữ lớn.

²https://docs.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series

--- TRANG 10 ---
Tài liệu tham khảo
[1]Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[2]Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. RAFT: A real-world few-shot text classification benchmark. arXiv preprint arXiv:2109.14076, 2021.
[3]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019.
[4]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
[5]Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.
[6]Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021.
[7]Andrew Kyle Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context? ArXiv, abs/2204.02329, 2022.
[8]Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.
[9]Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
[10] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022.
[11] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247, 2021.
[12] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690, 2021.
[13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.
[14] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
[15] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020.
[16] Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955, 2021.
[17] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.
[18] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.

--- TRANG 11 ---
[19] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.
[20] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[21] Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. arXiv preprint arXiv:2108.04106, 2021.
[22] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017.
[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. arXiv preprint arXiv:1902.00751, 2019.
[24] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural machine translation. arXiv preprint arXiv:1909.08478, 2019.
[25] Demi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020.
[26] Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. arXiv preprint arXiv:2111.09839, 2021.
[27] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.
[28] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021.
[29] Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
[30] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.
[31] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. arXiv preprint arXiv:2105.11447, 2021.
[32] Avital Oliver, Augustus Odena, Colin Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. Advances in Neural Information Processing Systems, 2018.
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.
[34] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[35] Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, et al. PromptSource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.
[36] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020.

--- TRANG 12 ---
[37] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. 2011 AAAI Spring Symposium Series, 2011.
[38] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
[39] Rishi Sharma, James Allen, Omid Bakhshandeh, and Nasrin Mostafazadeh. Tackling the story ending biases in the story cloze test. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 752–757, 2018.
[40] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.
[41] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung 23, 2019.
[42] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177–190. Springer, 2005.
[43] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.
[44] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.
[45] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018.
[46] Stefan Van Der Walt, S. Chris Colbert, and Gael Varoquaux. The numpy array: a structure for efficient numerical computation. Computing in science & engineering, 13(2), 2011.
[47] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.
[48] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[49] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning. PMLR, 2018.
[50] Timo Schick and Hinrich Schütze. True few-shot learning with prompts–a real-world perspective. arXiv preprint arXiv:2111.13440, 2021.
[51] Moshe Wasserblat. Sentence transformer fine-tuning (SetFit): Outperforming GPT-3 on few-shot text-classification while being 1600 times smaller, 2021.
[52] Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The efficiency misnomer. arXiv preprint arXiv:2110.12894, 2021.
[53] Guanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021.
[54] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-Tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021.

--- TRANG 13 ---
[55] Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-Tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.
[56] Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, and Yue Zhang. AdaPrompt: Adaptive model training for prompt-based NLP. arXiv preprint arXiv:2202.04824, 2022.
[57] Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, and Tong Zhang. Black-box prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531, 2022.
[58] Daniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sameer Singh, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, et al. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. arXiv preprint arXiv:2112.08348, 2021.
[59] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. arXiv preprint arXiv:2112.08654, 2021.
[60] Zonghan Yang and Yang Liu. On robust prefix-tuning for text classification. arXiv preprint arXiv:2203.10378, 2022.
[61] Yuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, and Jian Zhang. A prompting-based approach for adversarial example generation and robustness enhancement. arXiv preprint arXiv:2203.10714, 2022.
[62] Xiaochen Liu, Yu Bai, Jiawei Li, Yinan Hu, and Yang Gao. PSP: Pre-trained soft prompts for few-shot abstractive summarization. arXiv preprint arXiv:2204.04413, 2022.
[63] Xavier Garcia and Orhan Firat. Using natural language prompts for machine translation. arXiv preprint arXiv:2202.11822, 2022.
[64] Hunter Lang, Monica Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt-based learning for large language models. arXiv preprint arXiv:2202.00828, 2022.
[65] Boshi Wang, Xiang Deng, and Huan Sun. Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach. arXiv preprint arXiv:2203.08383, 2022.
[66] Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang, Zhilin Yang, and Jie Tang. Controllable generation from pre-trained language models via inverse prompting. arXiv preprint arXiv:2103.10685, 2021.
[67] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, et al. On transferability of prompt tuning for natural language understanding. arXiv preprint arXiv:2111.06719, 2021.
[68] Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, et al. HyperPrompt: Prompt-based task-conditioning of transformers. arXiv preprint arXiv:2203.00759, 2022.
[69] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.
[70] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.
[71] Teven Le Scao and Alexander M. Rush. How many data points is a prompt worth? arXiv preprint arXiv:2103.08493, 2021.
[72] Sen Yang, Yunchen Zhang, Leyang Cui, and Yue Zhang. Do prompts solve NLP tasks using natural language? arXiv preprint arXiv:2203.00902, 2022.

--- TRANG 14 ---
[73] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.
[74] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.
[75] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. arXiv preprint arXiv:2108.13161, 2021.
[76] Rabeeh Karimi Mahabadi, Luke Zettlemoyer, James Henderson, Marzieh Saeidi, Lambert Mathias, Veselin Stoyanov, and Majid Yazdani. PERFECT: Prompt-free and efficient few-shot learning with language models. arXiv preprint arXiv:2204.01172, 2022.
[77] Nafise Sadat Moosavi, Quentin Delfosse, Kristian Kersting, and Iryna Gurevych. Adaptable adapters. arXiv preprint arXiv:2205.01549, 2022.
[78] Eleni Triantafillou, Hugo Larochelle, Richard Zemel, and Vincent Dumoulin. Learning a universal template for few-shot dataset generalization. arXiv preprint arXiv:/2105.07029, 2021.
[79] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E. Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. arXiv preprint arXiv:1906.07697, 2019.
[80] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Universal representation learning from multiple domains for few-shot classification. Proceedings of the IEEE/CVF International Conference on Computer Vision., 2021.
[81] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.

--- TRANG 15 ---
A Tài nguyên tính toán được sử dụng
Tất cả các mô hình T0-3B được huấn luyện trên A6000 48GB. Huấn luyện T0-3B với các phương pháp PEFT khác nhau mất khoảng một giờ để huấn luyện, ngoại trừ Intrinsic SAID và FishMask mỗi cái mất khoảng hai giờ để huấn luyện. Huấn luyện trước (IA)³ mất 1 ngày trên 4 A6000. Tất cả các mô hình T0 được huấn luyện trên A100 80GB từ DataCrunch³ và mất khoảng nửa giờ để huấn luyện mỗi cái. Huấn luyện trước (IA)³ mất khoảng 1 ngày trên 4 A100.

B Công việc liên quan
Hiện tại, prompt tuning là một trong những phương pháp hiệu quả tham số nhất cho các mô hình ngôn ngữ lớn [29,14,53]. Liu et al. [54] giới thiệu một số thủ thuật để cải thiện prompt tuning, An et al. [55] điều chỉnh prompt cùng với embedding đầu vào để tăng hiệu suất, và Chen et al. [56] cải thiện embedding prompt thông qua tiếp tục huấn luyện trước. Với những khó khăn tối ưu hóa khi huấn luyện embedding prompt, Diao et al. [57] gần đây sử dụng tối ưu hóa hộp đen để huấn luyện embedding prompt mà không yêu cầu gradient. Một số công việc đã phân tích prompt tuning từ góc độ khả năng diễn giải Khashabi et al. [58] và sự tương đồng với các phương pháp PEFT khác He et al. [30]. Prompt tuning đã được áp dụng cho nhiều ứng dụng khác nhau cho NLP bao gồm học liên tục [59], tính mạnh mẽ của mô hình [60,61], tóm tắt [62], dịch máy [63], đồng huấn luyện [64], thăm dò các mô hình ngôn ngữ [65,65], prompting nghịch đảo [66] và học chuyển giao [67]. He et al. [68] gần đây đề xuất việc sử dụng hypernetwork để dự đoán prompt cho các nhiệm vụ mới (thay vì huấn luyện các tham số prompt với gradient descent). Prompt tuning và các phương pháp PEFT khác cũng đã được khám phá bên ngoài bối cảnh của các mô hình ngôn ngữ (ví dụ: thị giác [22, 69] và các mô hình thị giác-và-ngôn ngữ [26]).

Riêng biệt, nhiều nghiên cứu đã xem xét điều chỉnh toàn bộ mô hình few-shot với prompt rời rạc [70]. Công việc gần đây đã phân tích huấn luyện với prompt rời rạc, chứng minh sự tăng hiệu suất với prompting khi huấn luyện trên các số lượng ví dụ khác nhau [71], thấy rằng các mô hình hoạt động tương tự khi được huấn luyện trên prompt tốt và xấu [11], và khám phá prompt nào hoạt động tốt cho cài đặt few-shot và full-shot [72]. Cũng có những nỗ lực phát triển các phương pháp tìm prompt rời rạc hiệu quả [73,74] và huấn luyện prompt sử dụng các phương pháp tương tự prompt tuning [75].

Cũng có rất nhiều công việc về cải thiện ICL. Chen et al. [5], Min et al. [6] sử dụng ICL cho meta-learning để thực hiện học few-shot trên các nhiệm vụ mới. Lampinen et al. [7] cho thấy ICL có thể cải thiện khi được cung cấp giải thích và [8] sử dụng ICL với văn bản được truy xuất từ web cho trả lời câu hỏi miền mở. Trong khi đó, Min et al. [9] phân tích cách ICL hoạt động và cho thấy rằng ICL vẫn có thể hoạt động tốt khi các nhãn không chính xác được cung cấp cho các ví dụ trong ngữ cảnh.

Với sự ra đời của các mô hình ngôn ngữ lớn với hàng tỷ tham số, đã có rất nhiều sự quan tâm gần đây đối với các phương pháp PEFT. Một lượng nhỏ công việc gần đây cũng đã bắt đầu khám phá tính tương thích của các phương pháp PEFT trong cài đặt few-shot. Mahabadi et al. [28] thấy rằng PEFT có thể vượt trội hơn điều chỉnh tiêu chuẩn trong cài đặt tài nguyên thấp. Trong công việc đồng thời, Mahabadi et al. [76] so sánh PEFT với việc sử dụng prompt rời rạc (ví dụ: PET [70]) trong quá trình điều chỉnh few-shot và thấy rằng PEFT so sánh thuận lợi. Cũng đồng thời, Moosavi et al. [77] đề xuất một framework để giới thiệu adapter mà kiến trúc và thiết kế thay đổi từ nhiệm vụ này sang nhiệm vụ khác và chứng minh kết quả được cải thiện trong cài đặt few-shot. Gu et al. [18] và Vu et al. [19] đều khám phá cách huấn luyện trước các tham số prompt tuning có thể cải thiện khi dữ liệu được gán nhãn hạn chế có sẵn. Đối với học few-shot, Triantafillou et al. [78] khám phá việc học các tham số phổ quát và phụ thuộc tập dữ liệu có thể được pha trộn để khái quát hóa. Requeima et al. [79] sử dụng các quá trình thích ứng thần kinh có điều kiện và Li et al. [80] tận dụng chưng cất từ nhiều bộ trích xuất đặc trưng cho việc học các lớp hoặc miền mới trong học few-shot.

C Kết quả đầy đủ Huấn luyện Unlikelihood và Chuẩn hóa Độ dài
Bảng 3 hiển thị kết quả đầy đủ với huấn luyện unlikelihood và chuẩn hóa độ dài.

D Kết quả PEFT đầy đủ
Chúng tôi so sánh với các phương pháp PEFT sau, sử dụng bộ lập lịch suy giảm tuyến tính với khởi động với tỷ lệ khởi động 0.06 và trình tối ưu hóa Adafactor [49]. Chúng tôi hiển thị kết quả đầy đủ cho mỗi tập dữ liệu của tất cả

³https://cloud.datacrunch.io/

--- TRANG 16 ---
[Tiếp tục phần bảng và nội dung còn lại của trang 16-23 với định dạng tương tự, bao gồm tất cả các bảng dữ liệu, kết quả thí nghiệm, và chi tiết kỹ thuật được dịch sang tiếng Việt]
