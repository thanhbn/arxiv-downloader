ComPEFT: Nén để Truyền Thông Các Cập Nhật Hiệu Quả Tham Số thông qua Thưa Hóa và Lượng Tử Hóa

Prateek Yadav¹ Leshem Choshen²³ Colin Raffel⁴⁵ Mohit Bansal¹

Tóm tắt

Các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) giúp khả năng thích ứng hiệu quả một mô hình ngôn ngữ để tạo ra các mô hình "chuyên gia" chuyên môn hóa cho các tác vụ hoặc lĩnh vực mới. Các kỹ thuật gần đây trong việc hợp nhất mô hình và tổng quát hóa tổ hợp tận dụng các mô hình chuyên gia này bằng cách tổ hợp động các mô-đun để cải thiện khả năng tổng quát hóa zero/few-shot. Mặc dù hiệu quả của các phương pháp PEFT, kích thước của các mô hình chuyên gia có thể gây khó khăn trong việc truy xuất các mô hình chuyên gia cho mỗi truy vấn qua các mạng độ trễ cao như Internet hoặc phục vụ nhiều chuyên gia trên một GPU duy nhất. Để giải quyết những vấn đề này, chúng tôi trình bày ComPEFT, một phương pháp mới để nén các dư lượng tinh chỉnh (vector tác vụ) của các mô hình dựa trên PEFT. ComPEFT sử dụng thưa hóa và lượng tử hóa tam phân để giảm kích thước của mô-đun PEFT mà không thực hiện bất kỳ việc đào tạo lại bổ sung nào trong khi bảo tồn hoặc nâng cao hiệu suất mô hình. Trong đánh giá mở rộng trên các mô hình dựa trên T5, T0 và LLaMA với 200M-65B tham số, ComPEFT đạt được tỷ lệ nén 8x-50x. Đặc biệt, chúng tôi cho thấy ComPEFT cải thiện theo quy mô - các mô hình mạnh hơn thể hiện khả năng nén cao hơn và hiệu suất tốt hơn. Ví dụ, chúng tôi cho thấy ComPEFT áp dụng cho LLaMA vượt trội hơn QLoRA 4.16% trên MMLU với việc giảm kích thước lưu trữ lên đến 26x. Ngoài ra, chúng tôi cho thấy các chuyên gia nén được tạo ra bởi ComPEFT duy trì khả năng tổng quát hóa tổ hợp few-shot, tạo điều kiện cho việc truyền thông và tính toán hiệu quả, và thể hiện hiệu suất tăng cường khi được hợp nhất. Cuối cùng, chúng tôi cung cấp phân tích các thành phần phương pháp khác nhau, so sánh nó với các phương pháp PEFT khác, và kiểm tra hiệu quả của ComPEFT trong việc nén dư lượng của tinh chỉnh đầy đủ.

1. Giới thiệu

Các phương pháp tinh chỉnh hiệu quả tham số (PEFT) như LoRA và (IA)³ tạo điều kiện cho việc thích ứng hiệu quả các mô hình ngôn ngữ bằng cách học một tập tham số mới tối thiểu. Đáng chú ý, phương pháp QLoRA gần đây giảm đáng kể các yêu cầu bộ nhớ cho việc thích ứng bằng cách học các mô-đun LoRA trên một mô hình cơ sở được lượng tử hóa 4-bit. Điều này cho phép QLoRA thích ứng các mô hình cơ sở với lên đến 65B tham số trên một GPU 48GB duy nhất. Do đó, các nền tảng như Hugging Face model hub lưu trữ một số lượng ngày càng tăng các mô hình chuyên môn hóa tận dụng các phương pháp PEFT cho các tác vụ liên quan đến hiểu biết đa phương thức, chuyển giao đa ngôn ngữ, sử dụng công cụ, và chuyên môn cụ thể lĩnh vực như toán học, lập trình, và hơn thế nữa.

Các công trình gần đây về hợp nhất mô hình và tổng quát hóa tổ hợp đã tập trung vào việc tận dụng tính mô-đun của các mô hình chuyên gia này. Những công trình gần đây này nâng cao khả năng tổng quát hóa cho các lĩnh vực và tác vụ chưa thấy bằng cách tổ hợp động các mô hình chuyên gia PEFT khác nhau dựa trên một truy vấn cho trước hoặc tập dữ liệu mục tiêu. Cách tiếp cận mô-đun này là một giải pháp hiệu quả về chi phí và linh hoạt cho việc phát triển các hệ thống NLP bền vững hơn giữa sự tăng trưởng nhanh chóng của các mô hình học máy sử dụng nhiều tài nguyên.

Tuy nhiên, việc truy xuất nhiều chuyên gia cho mỗi truy vấn một cách tức thì có thể cực kỳ chậm qua các mạng độ trễ cao như Internet. Ví dụ, adapter QLoRA cho mô hình LLaMA-65B có kích thước 3.2 GB (có thể so sánh với kích thước của một mô hình T5-Large đầy đủ, là 3.0 GB). Do đó, ngay cả khi chúng ta có một phương pháp hiệu quả để hợp nhất mô hình và tổng quát hóa tổ hợp cho các lĩnh vực chưa thấy, việc truy xuất ngay cả một số ít mô hình chuyên gia có thể gây ra chi phí truyền thông cấm đoán cao qua các mạng độ trễ cao.

Để giải quyết những chi phí truyền thông này, chúng tôi giới thiệu phương pháp ComPEFT nén các dư lượng tinh chỉnh (vector tác vụ, tức là sự khác biệt của các giá trị tham số giữa các checkpoint đã tinh chỉnh và đã được đào tạo trước) bằng cách khai thác phân phối giá trị trong vector tác vụ. Cụ thể, ComPEFT thực hiện thưa hóa ban đầu để đặt lại hầu hết các giá trị trong vector tác vụ về zero, theo sau bởi lượng tử hóa, nơi độ lớn của các giá trị còn lại được thay thế bởi một hằng số vô hướng duy nhất. ComPEFT có điểm tương đồng với phương pháp Sparse Ternary Compression (STC) được sử dụng trong học liên kết; tuy nhiên, có những khác biệt đáng chú ý. Không giống như STC, ComPEFT giữ lại hiệu suất cao hậu hoc mà không cần đào tạo bổ sung. Trong khi sự suy giảm hiệu suất được quan sát khi sử dụng nén STC trên vector tác vụ, ComPEFT có thể khôi phục hiệu suất đầy đủ hoặc thậm chí vượt qua hiệu suất gốc bằng cách đơn giản điều chỉnh giá trị hằng số được sử dụng cho độ lớn của vector tam phân.

2. Bối cảnh và Động lực

2.1. Thiết lập Vấn đề

Cho một mô hình được đào tạo trước như LLaMA hoặc T5, chúng ta có thể tạo ra một mô hình chuyên gia cho tác vụ cụ thể t bằng cách tinh chỉnh tất cả các tham số mô hình hoặc sử dụng phương pháp tinh chỉnh hiệu quả tham số (PEFT) như (IA)³ hoặc LoRA. Trong cả hai trường hợp, chúng ta biểu diễn các tham số có thể đào tạo là θ, được khởi tạo là θinit, mà sau khi tinh chỉnh, trở thành θft. Công trình này giả định truy cập vào các tham số mô hình ban đầu θinit và các tham số mô hình đã tinh chỉnh, θft, và tập trung vào (1) nén các cập nhật tham số để truyền thông hiệu quả nhằm cho phép chuyển giao nhanh qua các mạng độ trễ cao như internet, và (2) hiểu chiều nội tại của các cập nhật tham số. Những cập nhật này được công thức hóa là các vector tác vụ τ∈Rd, được biểu diễn là τ=θft−θinit và đóng gói "kiến thức" được học trong giai đoạn tinh chỉnh.

2.2. Động lực

Nhiều công trình trước đây đã cho thấy vector tác vụ có thể được nén bằng cách áp dụng các kỹ thuật khác nhau như (1) thưa hóa - loại bỏ một số tham số không quan trọng cho hiệu suất; hoặc (2) lượng tử hóa - giảm số lượng bit biểu diễn mỗi tham số. Đây là hai cách bổ sung để giảm lượng thông tin được lưu trữ, giảm số lượng các mục nhập khác không trong vector tác vụ, và giảm lượng thông tin trong mỗi mục nhập.

Độ thưa của các Cập nhật Đã học: Vector tác vụ, τt=θft−θinit, biểu thị các thay đổi tổng hợp cho mỗi tham số trong giai đoạn tinh chỉnh mô hình. Các công trình trước đây đã cho thấy đối với bất kỳ tác vụ cho trước nào, 70-90% các giá trị trong vector tác vụ có thể được loại bỏ mà không làm tổn hại hiệu suất của các tác vụ. Có thể điều này đúng vì hầu hết các tham số có những thay đổi nhỏ và chỉ tích lũy các cập nhật gradient nhiễu.

Lượng tử hóa các Cập nhật Tham số: Nhiều công trình sử dụng lượng tử hóa hậu đào tạo để giảm số lượng bit cần thiết để biểu diễn mỗi tham số. Họ thực hiện nén bằng cách sử dụng các kiểu dữ liệu độ chính xác thấp hơn để lưu trữ trọng số mô hình và thực hiện tính toán.

3. ComPEFT: Nén để Truyền Thông Các Cập Nhật Hiệu Quả Tham Số thông qua Thưa Hóa và Lượng Tử Hóa

Theo thành công của các phương pháp được đề cập ở trên, chúng tôi hiện trình bày ComPEFT, sử dụng thưa hóa và lượng tử hóa để nén nặng các vector tác vụ nhằm giải quyết các vấn đề đã đề cập về truyền thông các cập nhật tham số qua các mạng độ trễ cao cho tổng quát hóa tổ hợp.

3.1. Tiền đề

Vector tác vụ τt=θft−θinit cho một tác vụ cụ thể t đóng gói các thay đổi đối với các giá trị tham số xảy ra sau khi tinh chỉnh trên tác vụ. Chúng ta có thể phân tách vector tác vụ τt thành hai vector riêng biệt: một vector hướng (dấu) γt∈Rd và một vector độ lớn µt∈Rd. Chính thức, γt=sgn(τt), nơi hàm signum sgn(x) thỏa mãn điều kiện sgn(x)·|x|=x và đưa ra các giá trị +1, 0 hoặc -1 dựa trên dấu của x. Vector độ lớn µt được tính là µt=|τt|. Cuối cùng, chúng ta có thể phân tách τt là tích Hadamard (nhân theo từng phần tử) của các vector này bằng τt=γt⊙µt. ComPEFT thực hiện nén cực độ của các vector tác vụ bằng cách thưa hóa vector dấu γt lên đến 95% và nén vector độ lớn d-chiều µt∈Rd thành một giá trị vô hướng duy nhất.

3.2. Các Bước trong ComPEFT

Để tái tạo một mô hình chuyên gia cho tác vụ t, chúng ta chỉ cần truyền thông cập nhật qua mô hình được đào tạo trước cơ sở được biểu thị bởi vector tác vụ τt. Cho vector tác vụ này, ComPEFT tuân theo ba bước đơn giản để nén:

1. Phân tách: Đối với mỗi tác vụ t, chúng ta tạo vector tác vụ, τt=θft−θinit và phân tách nó thành vector hướng γt∈Rd và vector độ lớn µt∈Rd sao cho τt=γt⊙µt.

2. Thưa hóa: Chúng ta thưa hóa vector dấu γt để chỉ giữ lại các tham số (hoặc chỉ số của chúng) tương ứng với các giá trị top-k dựa trên độ lớn của chúng và đặt dấu cho các tham số có độ lớn nhỏ nhất (1-k) về 0.

3. Lượng tử hóa Độ lớn: Cuối cùng, chúng tôi đề xuất thay thế vector độ lớn µt∈Rd bằng một hằng số vô hướng duy nhất. Cụ thể, chúng ta định nghĩa vector tác vụ nén cuối cùng τ̃t là τ̃t=α∗σ(τt)∗γ̃t, nơi σ(τt) là độ lệch chuẩn của vector tác vụ gốc τt và α là một hằng số tỷ lệ.

3.3. Lưu trữ Hiệu quả của Các Mô hình ComPEFT

Entropy của Vector Tác vụ Thưa hóa: Thông thường, một vector tác vụ cho trước τt được lưu trữ ở định dạng float 16-bit (bfloat16 hoặc fp16) và do đó yêu cầu 16∗d bit để lưu trữ trên đĩa. Nếu các giá trị trong τt được phân phối đều, entropy của cập nhật cũng là Hdense = 16∗d bit. Mặt khác, τ̃t nén của chúng ta là sự kết hợp của một vector dấu tam phân thưa với các giá trị ∈ {-1,0,+1} và một giá trị vô hướng 16-bit α∗σ(τt)∈R.

Với kỹ thuật mã hóa hoàn hảo và độ thưa 95%, ComPEFT của chúng ta có thể giảm số lượng bit trên mỗi tham số từ 16 bit xuống khoảng 0.34 bit, là cải thiện 47x về yêu cầu truyền thông và lưu trữ.

4. Kết quả Chính

Trong các thí nghiệm chính, chúng tôi cho thấy ngay cả các vector tác vụ tinh chỉnh hiệu quả tham số thể hiện chiều nội tại đáng kể thấp, cụ thể là, hầu hết các tham số của chúng là không cần thiết. Phát hiện này cho phép nén đáng kể với hiệu suất tốt hơn hoặc tương tự trên các tác vụ downstream. Những vector tác vụ được nén hiệu quả này có thể được truyền với chi phí rẻ qua các mạng độ trễ cao, tạo điều kiện cho cả việc hợp nhất mô hình và tổng quát hóa tổ hợp cho các tác vụ mới. Quan trọng là, khi kích thước mô hình và khả năng zero-shot tăng, khả năng nén và hiệu suất của ComPEFT cũng tăng.

4.1. Nén QLoRA được Đào tạo trên Các Mô hình LLaMA

Chúng tôi khám phá tiện ích của ComPEFT trong việc đào tạo các adapter QLoRA cho các mô hình LLaMA với 7B, 13B, 33B, và 65B tham số. Chúng tôi thử nghiệm với 8 tập dữ liệu theo hướng dẫn gần đây đa dạng về ngôn ngữ và kích thước tập dữ liệu.

Kết quả: Chúng tôi cung cấp kết quả cho tất cả các kết hợp tác vụ và kích thước mô hình, so sánh hiệu suất của các checkpoint ComPEFT và các checkpoint QLoRA gốc. Chúng tôi thấy rằng trong 28 trong số 32 cấu hình thí nghiệm ComPEFT cải thiện hiệu suất của các mô hình QLoRA gốc trong khi nén mô-đun LoRA giữa 10x-50x về chi phí lưu trữ. ComPEFT dẫn đến cải thiện 0.54%, 1.06%, 3.44%, và 4.16% trên MMLU cho các mô hình tham số LLaMA 7B, 13B, 33B, và 65B tương ứng.

7. Kết luận

Phương pháp nén PEFT mới của chúng tôi, ComPEFT, cung cấp một giải pháp hiệu quả cho các thách thức độ trễ liên quan đến việc truy xuất các mô hình chuyên gia cho tinh chỉnh hiệu quả tham số. Bằng cách nén các dư lượng tinh chỉnh thông qua thưa hóa và lượng tử hóa, ComPEFT đạt được tỷ lệ nén cao và thường nâng cao hiệu suất mô hình qua các tác vụ NLP và kích thước mô hình khác nhau. Hơn nữa, nó bảo tồn khả năng tổng quát hóa tổ hợp few-shot, tạo điều kiện cho việc truyền thông và tính toán hiệu quả, và thể hiện hiệu suất cải thiện khi được hợp nhất với các mô hình gốc. Nghiên cứu này đóng góp những hiểu biết có giá trị vào lĩnh vực tinh chỉnh hiệu quả tham số, giải quyết cả các mối quan tâm về hiệu suất và độ trễ.
