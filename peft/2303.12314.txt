# 2303.12314.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.12314.pdf
# File size: 916839 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization
for Few-shot Generalization
Kaihang Pan1,2∗, Juncheng Li1†, Hongye Song2, Jun Lin2, Xiaozhong Liu3, Siliang Tang1
1Zhejiang University,2DAMO Academy, Alibaba Group,
3Worcester Polytechnic Institute
{kaihangpan, junchengli, siliang}@zju.edu.cn
{hongye.shy, linjun.lj}@alibaba-inc.com, xliu14@wpi.edu
Abstract
Prompt tuning is a parameter-efficient method,
which learns soft prompts and conditions
frozen language models to perform specific
downstream tasks. Though effective, prompt
tuning under few-shot settings on the one
hand heavily relies on a good initialization
of soft prompts. On the other hand, it can
easily overfit to few-shot training samples,
thereby undermining generalizability. Exist-
ing works leverage pre-training or supervised
meta-learning to initialize soft prompts but
they fail to data-efficiently generalize to un-
seen downstream tasks. To address the above
problems, this paper proposes a novel Self-
sUpervised meta- Prompt learning framework
with MEta-gradient Regularization for few-
shot generalization ( SUPMER ). SUPMER
leverages self-supervised meta-learning with
a diverse set of well-designed meta-training
tasks to learn a universal prompt initialization
for efficient adaptation using only unlabeled
data. Additionally, it jointly meta-learns a gra-
dient regularization function to transform raw
gradients into a domain-generalizable direc-
tion, thus alleviating the problem of overfit-
ting. Extensive experiments show that SUP-
MER achieves better performance for differ-
ent few-shot downstream tasks, and also ex-
hibits a stronger domain generalization abil-
ity. The code for SUPMER will be available at
https://github.com/beepkh/SUPMER .
1 Introduction
Recent NLP accomplishments witnessed the rapid
development of pre-trained language models
(PLMs) ( e.g., BERT Devlin et al., 2019; T5 Raf-
fel et al., 2020; GPT3 Brown et al., 2020). Fine-
tuning, which tunes the entire PLM parameters,
has achieved outstanding performances in various
NLP tasks. However, as the pre-trained model
scale increases, tuning the entire set of parameters
*Work done when interning at Alibaba DAMO Academy.
†Corresponding Author.
MRPC CB SUBJ
Dataset
(a)5055606570Accuracy (%)Method: Vanilla PT
20 40 60 80 100 120 140
Training Steps
(b)455055606570Accuracy (%)
Dataset: MRPC
vanilla PT
SUPMERFigure 1: (a) Performance of PT with different prompt
initialization. (b) Performance after different training
steps for vanilla PT and SUPMER.
would be sometimes unaffordable. More recently,
prompt-based methods, which simply insert a piece
of carefully designed text to the input ( e.g., “It was
⟨X⟩.”) and predict target words ( e.g., “great” or
“terrible”) at the mask position with frozen PLMs,
have demonstrated remarkable effectiveness. But it
has been observed that the performance of prompt-
based methods is greatly affected by the design of
prompts. In light of this, prompt tuning (PT Lester
et al., 2021), as a parameter-efficient tuning method,
is proposed to only prepend some additional learn-
able tokens called soft prompts to the input text,
with all PLM parameters freezing.
Though prompt tuning is an efficient and effec-
tive paradigm, Gu et al. (2022) shows it performs
much worse than fine-tuning under few-shot set-
tings. We argue that the performance is not satisfac-
tory mainly due to two limitations: 1) The perfor-
mance of PT is highly sensitive to the soft prompt
initialization , especially for few-shot tasks. As
shown in Figure 1 (a), different soft prompt initial-
ization leads to significant performance variations.
2) Few-shot PT risks overfitting to some spurious
correlations as soft prompts are tuned on limited
training samples, thus undermining the generaliz-
ability of PLMs. As shown in Figure 1 (b), the
performance of few-shot vanilla PT degrades sig-
nificantly in the final training steps.
Recent research mainly focused on the first limi-arXiv:2303.12314v4  [cs.CL]  23 Oct 2023

--- PAGE 2 ---
tation, leveraging pre-training or supervised meta-
learning for soft prompt initialization. A pre-
trained prompt tuning method (PPT) (Gu et al.,
2022) is proposed from the beginning, which uti-
lizes self-supervised tasks to pre-train soft prompts
and then applies them in the few-shot scenario.
However, without explicitly optimizing the fast
adaptation ability of the model, PPT suffers from a
train-test mismatch between the pre-training data
and the downstream data. So it limits generaliza-
tion to unseen few-shot tasks, especially when there
is a significant disparity in task domains or for-
mats. MetaPrompting (Hou et al., 2022), as an-
other effort, seeks assistance from model-agnostic
meta-learning (MAML Finn et al., 2017) for fast
adaptation in few-shot settings. However, in each
task, MetaPrompting requires plenty of labeled
data within certain classes to perform supervised
meta-learning for prompt initialization, which is
often inaccessible in practical few-shot scenarios.
And the learned initialization can only generalize
to the remaining classes of the same task in a few-
shot manner, exhibiting weak task transferability.
Furthermore, all these existing works ignore the
second limitation, i.e., the propensity for few-shot
prompt tuning to lead to overfitting.
To address the shortcomings of existing works,
we propose SUPMER , aSelf-sUpervised meta-
Prompt learning framework with MEta-gradient
Regularization. It leverages self-supervised meta-
learning to universally learn an efficient soft prompt
initialization, also with a meta-gradient regulariza-
tion function to mitigate overfitting. This compre-
hensive process only requires a one-time execu-
tion and enables seamless adaptation to different
downstream few-shot tasks, while also facilitating
faster convergence for downstream prompt tuning.
Specifically, to address the first limitation, we de-
sign a novel self-supervised meta-learning method
for prompt initialization, which automatically gen-
erates a diverse set of meta-training tasks from
large-scale unlabeled corpora and explicitly learns
to fast adapt across these tasks. To ensure task di-
versity, we initially design a collection of anchor
self-supervised meta-training tasks with different
formats. And then a curriculum-based task aug-
mentation method is further proposed to enrich the
task distribution dynamically in terms of the cur-
rent model capability.
For the second issue, we integrate a meta-
gradient regularization function into meta-promptlearning. As we simulate distribution shift through
task augmentation, the meta-gradient regularization
parameters are jointly optimized to align gradient
directions across different distributions within our
proposed meta-prompt learning paradigm. Con-
sequently, in downstream tasks, these optimized
parameters can be directly utilized to transform
raw gradients over few-shot samples into a domain-
generalizable direction, preventing prompt tuning
overfitting to some domain-specific correlations.
Overall, our contributions are mainly three-fold:
(1) We propose a novel self-supervised meta-
prompt learning framework to better initialize soft
prompts, where only unlabeled pre-training data
are used to construct different meta-training tasks
with curriculum-based task augmentation for fur-
ther task enrichment.
(2) We incorporate a novel meta-gradient regu-
larization function into our meta-prompt learning
framework, which meta-learns to transform the raw
gradient during few-shot learning into a domain-
generalizable direction, thus preventing prompt tun-
ing overfitting to domain-specific correlations.
(3) Comprehensive experiments on few-shot
learning and domain generalization validate the
superiority of our method, which even outperforms
full-model tuning in few-shot learning. It also ex-
hibits a stronger domain generalization ability.
2 Related Work
Soft Prompt Tuning. Soft prompt tuning is one
of the most parameter-efficient tuning methods
widely used in NLP (Liu et al., 2023) and vision-
language tasks (Zhou et al., 2022; Li et al., 2023a),
which only tunes a small number of (extra) pa-
rameters to attain strong performance. Specifi-
cally, it freezes the PLM parameters and prepends
some trainable continuous embeddings ( i.e., soft
prompts) to the input sequence (Lester et al., 2021)
or every layer of the pre-trained model (Li and
Liang, 2021; Liu et al., 2022).
To efficiently train task-adaptive soft prompts in
few-shot scenarios, some studies (Vu et al., 2022;
Asai et al., 2022; Sun et al., 2022) employ task
adaptation techniques, obtaining source prompts
from source tasks in a supervised way and interpo-
lating them into the target prompts. Other works
focus on training improved prompt initializations.
PPT (Gu et al., 2022) pre-trains the soft prompts
with some self-supervised tasks on unlabeled cor-
pora, but it doesn’t explicitly optimize the fast adap-

--- PAGE 3 ---
evaluation𝜃!Update𝐷!"##$%&
Update(2) Meta-Prompt Learning with Meta-Gradient Regularization
supportqueryDistribution Shift𝐷'"(%)𝐷'"(%)anchortask distribution
Gradient Similarity
𝐷'"(%)𝐷!"##$%&ComputeGradient
×𝑛
(1) Curriculum-based Task AugmentationMeta-trainingMeta-Gradient RegularizationTask Interpolation𝜆⋅+1−𝜆⋅=𝜆⋅+1−𝜆⋅=unlabeled data Sentence-pair classiﬁcationMulti-choice classiﬁcationSingle-sentence classiﬁcation
𝐷!"##$%&𝐷'"(%)SamplingSampling𝜃′!𝑔/1#𝑔/1"𝑔′/1"ComputeGradient𝑔/0!!
𝜃$Update𝐷*(+!,$&(3) Fast Adaptive Prompt Tuning for Few-shot Generalization𝐷&(!&ComputeGradientDownstream testing𝜽′𝒏𝑔/2"𝑔′/2"Mixing Ratio 𝜆
🔥
🔥
🔥
❄Meta-Gradient Regularizationtask distribution
🔥learnable
❄frozen Meta-training Dataself-supervised task dataFigure 2: The framework of SUPMER. We employ task interpolation to enrich the distribution of self-supervised
meta-training tasks. Concurrently, we integrate a meta-gradient regularization function into meta-prompt learning.
Furthermore, during meta-prompt learning we also dynamically adapt the mixing ratio of task interpolation,
upgrading the vanilla task augmentation into a curriculum-based one.
tation ability of the model. MetaPrompting(Hou
et al., 2022) utilizes supervised meta-learning for
soft prompt initialization, splitting each dataset into
two sets with disjoint data classes. One split is used
to initialize soft prompts while the other serves as
the downstream task. In comparison, SUPMER dif-
fers from MetaPrompting in the following ways: 1)
for each downstream task MetaPrompting focuses
on a fixed supervised dataset to reinitialize soft
prompts, whereas SUPMER can universally gener-
alize to different unseen tasks with large-scale un-
labeled corpora for initialization; 2) MetaPrompt-
ing doesn’t freeze PLM parameters, while SUP-
MER only tunes the soft prompts as the general
soft prompt tuning methods do.
Meta-Learning. Meta-learning, also known as
learning to learn, optimizes the ability to learn
new tasks quickly and efficiently, utilizing expe-
rience from previously seen tasks. It can be classi-
fied into three types: metric-based methods (Koch
et al., 2015; Vinyals et al., 2016; Snell et al.,
2017), model-based methods (Graves et al., 2014;
Mishra et al., 2018; Qiao et al., 2018), and gradient-
based methods (Hochreiter et al., 2001; Ravi and
Larochelle, 2017; Nichol et al., 2018; Li et al.,
2020). In this work, we focus on a gradient-based
meta-learning algorithm ( i.e., MAML Finn et al.,
2017). Compared to typical meta-learning methods
that rely on human-annotated meta-training tasks,
we automatically generate abundant tasks in a self-
supervised way, also integrating a meta-gradient
regularization function into MAML to steer gradi-ents towards a domain-generalizable direction.
3 Method
In this section, we describe the whole framework
of SUPMER (shown in Figure 2). With pre-defined
preliminaries, we first introduce the way to con-
struct anchor self-supervised meta tasks and the
foundation of task augmentation to densify task
distributions. Then we elaborate on the SUPMER
model, including the meta-gradient regularization
function. Finally, we upgrade the original task aug-
mentation method into a curriculum-based one. Be-
sides, we formalize all tasks in a text-to-text format
following the T5 fashion (Raffel et al., 2020).
3.1 Preliminaries
Prompt Tuning. In prompt tuning (Lester et al.,
2021), given a training sample (xi, yi)from task
Dτ, we apply a prompt template Pconverting xi
into a new sequence P(xi)and then concatenate
a set of soft prompts θto the beginning of P(xi).
And verbalizer Vplays a role in mapping yito some
corresponding label tokens V(yi)in the vocabulary
of PLMs. So the objective of prompt tuning can be
formulated as follows:
arg min
θLDτ(θ)
= arg max
θX
(xi,yi)∈Dτlogp 
⟨X⟩=V(yi)|[θ;P(xi)];θ
(1)
where θdenotes the soft prompt embedding (the
only tunable parameters in prompt tuning). ⟨X⟩let
PLMs predict target tokens at the masked positions
and[·;·]is the concatenation operation.

--- PAGE 4 ---
Model-Agnostic Meta-Learning. Assuming ac-
cess to a task distribution p(T), the goal of meta-
learning is to utilize tasks τi∼p(T), referred to as
meta-training tasks or meta tasks, to train a learn-
ing procedure that generalizes to unseen tasks from
the distribution. Model-Agnostic Meta-Learning
(MAML) (Finn et al., 2017) is a gradient-based
bi-level optimization meta-learning method, which
consists of an inner loop task-specific learning and
outer loop fast adaptation across tasks.
Specifically, a task τis composed of the support
setDs
τand the query set Dq
τ. In the inner loop of
MAML, a model learns to adapt to a new task τi
using its support set in the following way:
θ′
i=θ−α1∇θLDsτi(θ) (2)
where α1is the inner loop learning rate and θis the
model’s parameters. And the optimized parameters
θ′
iis then evaluated on the query set of task τiwith
the loss function LDq
τi. In the outer loop, this loss
across meta-training tasks is treated as the final
training loss to update θ:
θ←θ−β1∇θX
τi∼p(T)LDq
τi(θ′
i) (3)
where β1is the outer loop learning rate.
3.2 Constructing Anchor Meta Tasks
Supervised datasets with a large amount of la-
beled data are often unavailable in many NLP tasks.
While unlabeled data is more easily accessible and
generally covers broader semantic concepts. So we
utilize the unlabeled data from a large corpus to
create anchor self-supervised meta-training tasks.
The unlabeled data are first grouped into differ-
ent clusters. We utilize PLMs to derive semanti-
cally meaningful embeddings for sentences in the
corpus, and then apply unsupervised K-means to
cluster these unlabeled sentences. Based on the re-
sults of K-means, we design three different formats
of self-supervised meta-training tasks: sentence-
pair classification, multi-choice classification, and
single-sentence classification.
Specifically, sentence-pair classification in-
volves predicting whether two sentences are adja-
cent in the same document or from the same cluster
after K-means clustering. Multi-choice classifi-
cation identifies the correct sentence among sev-
eral candidates, which is either adjacent to a query
sentence or from its same cluster. And Single-
sentence classification aims to associate each sen-
tence with its correct cluster label, as determinedby K-means. On this basis, for each task format,
we distribute meta-training data into different tasks
to construct anchor meta-training tasks with well-
balanced task distributions. We group samples with
similar embeddings into the same task based on the
results of K-means. And we give a more detailed
description of anchor meta-training task construc-
tion in Appendix A.2.
3.3 Vanilla Task Augmentation
With a set of anchor meta-training tasks, in this
section we first introduce the vanilla task augmen-
tation to densify the task distribution. Extending
the idea of mixup (Zhang et al., 2018), we augment
the task set through task interpolation, which lin-
early combines features and corresponding labels
of samples from the query set in different tasks. In
§3.5 we further upgrade the vanilla task augmen-
tation method into a curriculum-based one, which
dynamically controls the task interpolation in terms
of the current model capability.
Specifically, for a task composed of the support
set and the query set, we denote the hidden repre-
sentations of the query set samples in task τkas
Hq. Given an anchor task τi, first we randomly se-
lect another task τj. While retaining the support set
ofτi, we reconstruct its query set by interpolating
on the hidden representations (Hq
i,Hq
j)and corre-
sponding labels (Yq
i,Yq
j)from the query sets in τi
andτj, which can be accomplished using mixup:
˜Hq
i= (1−λ)Hq
i+λHq
j,˜Yq
i= (1−λ)Yq
i+λYq
j
(4)
where the mixing ratio λ∈[0,1]is drawn from
a Beta distribution Beta (α, α), and αis a hyper-
parameter. The process of task augmentation not
only enriches the task distribution, but also sim-
ulates the distribution shift between the support
set and the query set within one task, as we only
leverage interpolation between the query sets of dif-
ferent anchor meta-training tasks. And in §3.4 we
will show the effect of this distribution deviation.
3.4 Meta-Prompt Learning with
Meta-Gradient Regularization
In this section we introduce the algorithm of
our meta-prompt learning framework, which is a
bi-level meta-learning paradigm learning a task-
universal soft prompt initialization θfor efficient
adaptation. And it jointly meta-learns a meta-
gradient regularization function ψϕthat transforms
raw gradients into a domain-generalizable direction
to prevent prompt tuning from overfitting.

--- PAGE 5 ---
Specifically, considering that the inner loop up-
date of MAML ( i.e., Eq. (2)) over limited samples
might overfit to some domain-specific correlations,
we propose to learn a gradient regularization func-
tionψϕ(·), making a direct transformation to the
raw gradients obtained from the support set Ds
τi.
The function first performs affine transformation
h(·)(e.g., rotation) to modulate the raw gradients
g, and then an update gate vector zis employed to
combine gandh(g)into the final gradients:
ψϕ(g) =z·h(g) + (1 −z)·g (5)
Obviously, the value of zcan be used to control
how much the transformed gradients h(g)con-
tribute to the output of ψϕ(g). We hope to deter-
mine this weight based on the input samples them-
selves, setting z=σ(WH+b), where His the
hidden representations of input samples. Formally,
now we transform Eq. (2) into:
θ′
i=θ−α1ψϕ(∇θLDsτi(θ)) (6)
After adapting the soft prompt embeddings to
the support set Ds
τi, in the outer loop we optimize
the prompt initialization θbased on these adapted
embeddings θ′via Eq. (3). Besides, meta-gradient
regularization parameters ϕare also optimized us-
ing the same loss to learn a better gradient transfor-
mation, with β2as the learning rate:
ϕ←ϕ−β2∇ϕX
τi∼p(T)LDq
τi(θ′
i)(7)
Overall, the total meta-prompt learning obejec-
tive can be formulated as follows:
arg min
θ,ϕX
τi∼p(T)LDq
τi 
θ−α1ψϕ(∇θLDsτi(θ))
(8)
Downstream Prompt Tuning. The above meta-
prompt learning framework only requires a one-
time execution. The optimized prompt initialization
θ∗and meta-gradient regularization parameters ϕ∗
are then universal for different downstream tasks.
During downstream prompt tuning, we fix ϕ∗and
further adapt θ∗to testing tasks as Eq. (6).
Analysis of SUPMER. Here we give some anal-
ysis of how SUPMER could enhance generalizabil-
ity, with more complete proof in Appendix A.1.
Given that x=θ−α1ψϕ(∇θLDs(θ))andx0=θ,
focusing on a single meta-training task, we can ap-
ply a first-order Taylor expansion around the pointx0to reformulate Eq. (8) as:
∵LDq(x) =LDq(x0) +L′
Dq(x0)(x−x0)
∴arg min
θ,ϕLDq 
θ−α1ψϕ(∇θLDs(θ))
= arg min
θ,ϕLDq(θ)−α1∇θLDq(θ)·ψϕ 
∇θLDs(θ)(9)
Based on the aforementioned discussion, we can
reach the following conclusions: (1) The update
ofθminimizes the expected loss on the query set.
(2) The optimization of both θandϕmaximizes
the inner product between the regulated gradients
from the support set and the gradients from the
query set. The inner product of two vectors is
larger if they are in a similar direction. Recalling
that we simulate the distribution shift between the
support set and the query set, the optimization of θ
andϕtries to align the gradient directions across
different distributions. To improve the alignment
between the domain-specific gradients, the gradi-
ent regularization parameters ϕare optimized to
retain some domain-invariant information of meta-
training data and then can be utilized to regulate
raw gradients obtained from few-shot samples into
a domain-generalizable direction in downstream
prompt tuning, thus avoiding overfitting to some
spurious correlations.
3.5 Curriculum-based Task Augmentation
In §3.4 we show that SUPMER can help align the
optimization direction across two distributions with
deviation, which is simulated by performing task
augmentation exclusively on the support sets. From
Eq. (4) it is evident that the mixing ratio λof mixup
controls the extent of the distribution deviation,
with a larger λresulting in a more noticeable devia-
tion. However, in the previously discussed method,
λis sampled from a fixed Beta distribution. In this
section, we propose a more flexible sampling ap-
proach, which upgrades the original task augmenta-
tion method into a curriculum-based one, gradually
increasing the task difficulty and achieving a more
reasonable distribution shift.
The curriculum-based task augmentation dynam-
ically adjusts the parameters of the Beta distribu-
tion, from which we sample the mixing ratio λ.
Specifically, a batch of meta tasks is sampled in
each training epoch. For each task, we can obtain
gradients on the support set gs
iand gradients on the
query set gq
i, along with their cosine similarity. We
leverage the average cosine similarity sk−1of all
tasks in a batch during the last epoch to derive the

--- PAGE 6 ---
Model: T5-base (220M)
Methods SST-2 SST-5 MR CR SUBJ TREC CB RTE QNLI WiC MRPC QQP A VG
Prefix Tuning 78.21.738.02.574.03.284.53.366.23.970.12.970.42.654.42.055.74.353.41.368.11.264.91.364.82.5
P-Tuning-v2 83.10.841.71.582.30.888.70.574.82.478.12.167.55.453.91.361.41.654.52.270.71.168.01.168.71.7
FT 83.61.741.22.681.70.988.30.980.01.479.81.771.91.556.92.162.30.654.61.670.20.769.51.070.01.4
PT 71.93.437.32.873.24.584.43.561.55.365.34.258.96.153.22.855.24.853.12.466.65.363.02.662.04.0
PPT 81.22.040.25.481.20.783.67.366.83.773.42.460.77.755.41.260.43.953.61.368.00.863.10.765.63.1
Unified-PPT 76.87.744.71.779.03.387.70.664.25.868.42.963.92.954.41.156.72.454.51.667.81.167.62.965.52.8
MetaPT 85.71.345.31.182.54.588.50.373.23.778.72.265.42.456.11.758.23.154.11.369.60.668.80.968.81.9
SUPMER 87.30.546.70.684.00.689.30.379.62.280.20.972.41.457.31.061.71.054.81.271.30.570.51.071.30.9
Model: Flan-T5-XL (3B)
Methods SST-2 SST-5 MR CR SUBJ TREC CB RTE QNLI WiC MRPC QQP A VG
Zero-shot Inference 89.1 52.3 83.3 80.6 57.4 87.2 76.8 75.8 85.0 50.5 77.2 77.5 74.4
Few-shot Inference 93.2 53.3 88.5 87.8 58.6 91.6 83.9 79.1 86.9 64.3 79.9 81.0 79.0
Prefix Tuning 88.62.145.42.788.72.289.11.684.63.990.62.465.46.473.42.672.34.155.51.873.92.675.02.375.22.9
P-Tuning-v2 91.21.053.51.190.00.889.81.187.73.292.30.969.12.962.34.676.83.663.12.077.21.777.31.677.52.0
FT 92.91.453.61.989.61.391.30.588.70.993.62.077.81.676.01.784.41.563.01.877.71.382.92.981.01.6
PT 88.29.245.73.485.65.688.65.181.65.185.15.665.47.261.64.069.35.954.81.171.12.770.28.672.35.3
PPT 91.02.649.12.388.81.388.03.087.61.990.81.767.14.867.12.281.51.156.11.375.82.271.63.176.22.3
Unified-PPT 89.98.347.42.889.70.989.01.586.44.388.43.269.33.563.21.174.93.259.91.774.02.476.32.075.72.9
MetaPT 93.70.852.51.390.70.789.70.688.12.491.81.872.12.274.11.377.52.558.51.676.61.881.42.478.91.6
SUPMER 95.50.455.30.791.40.590.70.790.30.893.01.587.61.581.41.088.30.665.01.778.10.885.10.483.50.9
Table 1: Results of few-shot learning. For each dataset we report the average accuracy and standard deviation over
five random seeds (zero-shot & few-shot inference produce nearly consistent results each time as they do not require
parameter tuning). Bold fonts indicate the best results. We can see SUPMER achieves better performance.
mixing ratio λkfor the current epoch k:
λk=Beta (α, bkα)
bk=m1+sk−1
2−1
m−1,
where sk−1=1
|B|·|B|X
i=1gs
i·gq
i
∥gs
i∥ · ∥gq
i∥(10)
where mis the curve parameter. In this way, when
our model is not capable of aligning the optimiza-
tion directions across different distributions at the
beginning, a smaller λis preferable to create a
smaller distribution deviation. Then λtends to
gradually increase as the model’s capability im-
proves, resulting in a larger distribution deviation
and a corresponding increase in task difficulty.
We present the pseudo-codes of SUPMER in
Appendix A.4.
4 Experiments
4.1 Experimental Setup
We evaluate our approach in two problem settings:
1) Few-shot learning with different NLP down-
stream tasks; 2) domain generalization.
Few-shot Learning. We consider 6 downstream
tasks with 12 datasets: 1) the sentiment analy-
sis datasets SST-2, SST-5 (Socher et al., 2013),
MR (Pang and Lee, 2005) and CR (Hu and Liu,
2004); 2) the subjectivity classification dataset
SUBJ (Pang and Lee, 2004); 3) the questionclassification dataset TREC (V oorhees and Tice,
2000); 4) the natural language inference datasets
CB (De Marneffe et al., 2019) and RTE (Wang
et al., 2019); 5) the question answering dataset
QNLI (Rajpurkar et al., 2016); 6) the word
sense disambiguation dataset WiC (Pilehvar and
Camacho-Collados, 2019); 7) the paraphrase de-
tection datasets MRPC (Dolan and Brockett, 2005)
and QQP. Following Karimi Mahabadi et al. (2022),
for each dataset we sample 16 instances per label
from the original training set to form training and
validation sets for few-shot learning.
Domain Generalization. Then we design a more
challenging problem about zero-shot domain gen-
eralization in the sentiment analysis task. Our ex-
periments include 6 domains across 3 datasets: 1)
the Amazon review dataset (Blitzer et al., 2007)
containing reviews about Books (B), DVDs (D),
Electronics (E) and Kitchen appliances (K); 2) the
airline review dataset (A) (Nguyen, 2015; Ziser
and Reichart, 2018); 3) the restaurant (R) domain
obtained from the Yelp dataset (Zhang et al., 2015).
We choose A as the source domain and the other
five (B, D, E, K, R) constitute the target domains.
On this basis, we sample 16 instances per label
from the training set of the source domain to tune
soft prompts. And then we directly use the soft
prompts learned from the source domain to evaluate
performance on the test set of each domain.

--- PAGE 7 ---
Model: T5-base (220M)
MethodSource TargetA VG
A B D E K R
Prefix Tuning 78.12.2 82.71.0 81.51.2 84.40.6 82.72.6 84.92.2 82.41.6
P-Tuning-v2 84.00.8 83.60.9 82.41.7 85.90.9 84.21.5 83.82.7 84.01.4
FT 84.40.2 83.90.6 81.00.6 84.10.6 85.00.7 85.30.6 84.00.6
PT 79.82.5 75.32.2 76.03.2 79.62.0 79.81.9 83.01.8 78.92.3
PPT 81.91.4 77.93.5 83.61.3 88.41.0 89.11.6 87.81.5 84.81.7
Unified-PPT 80.93.0 82.12.0 76.84.0 81.04.5 82.23.9 84.93.3 81.33.5
MetaPT 86.10.384.30.8 84.20.7 89.40.9 90.30.6 - (86.90.7)
SUPMER 85.70.585.30.685.10.490.30.691.10.590.40.488.00.5
Model: Flan-T5-XL (3B)
MethodSource TargetA VG
A B D E K R
zero-shot inference 77.8 84 .6 86 .2 86 .8 88 .6 87 .8 85.3
few-shot inference 82.5 90 .3 89 .7 92 .3 92 .2 89 .2 89.4
Prefix Tuning 83.01.1 85.31.9 83.41.1 87.31.5 86.42.4 89.81.0 85.91.5
P-Tuning-v2 85.60.4 86.72.3 86.81.8 88.62.1 90.22.3 88.92.6 87.81.9
FT 86.20.7 88.82.2 84.61.3 87.21.3 89.81.1 90.70.9 87.91.3
PT 82.61.2 79.03.9 82.51.7 84.22.3 84.52.5 84.82.1 82.92.3
PPT 85.20.9 83.23.3 89.71.8 92.41.7 93.61.5 89.61.1 89.01.7
Unified-PPT 83.00.8 82.53.0 82.04.8 86.03.0 86.62.2 85.62.0 84.32.6
MetaPT 87.10.687.42.7 89.51.7 93.80.6 94.31.1 − (90.41.3)
SUPMER 87.00.289.81.491.11.295.10.895.80.991.80.891.80.9
Table 2: Results of domain generalization. For MetaPT
we calculate the average performance only across do-
main A, B, D, E, K (without R).
4.2 Experimental Details
Baselines. Our experiments are built on a smaller-
scale model, T5-base (Raffel et al., 2020), and
then on a larger-scale instruction-tuned model,
Flan-T5-XL (Chung et al., 2022). For both two
backbone models, we use the following baselines:
(1) prompt tuning methods with the same num-
ber of tunable parameters as SUPMER: vanilla
prompt tuning ( PTLester et al., 2021), PPT (Gu
et al., 2022), Unified-PPT (Gu et al., 2022), and
MetaPT (Huang et al., 2022). (2) methods with
more tunable parameters: Prefix-Tuning (Li and
Liang, 2021), P-tuning-v2 (Liu et al., 2022), full-
model tuning ( FT). Furthermore, Given that FLAN-
T5-XL was also designed with few-shot inference
in mind, we additionally compare with two base-
line methods on FLAN-T5-XL, i.e.,zero-shot in-
ference andfew-shot inference , which directly
employ Flan-T5-XL for downstream evaluation.
We list the details of baselines in Appendix B.
Implementation Details. We solve all down-
stream tasks in a text-to-text format and run each
experiment with 5 different random seeds. For
all prompt tuning methods, we follow Lester et al.
(2021) to design soft prompts composed of 100
soft tokens, with tunable parameters far less than
full-model tuning. For our SUPMER, following
PPT (Gu et al., 2022) we sample 10GB data from
OpenWebText (Gokaslan et al., 2019), a large-
scale unlabeled corpus, to construct self-supervised
meta-training tasks. The meta-training stage only
requires a one-time execution. In downstream
prompt-tuning, we freeze the meta-gradient reg-
50 100 150
Training Steps203040506070Accuracy (%)
CB
FT
Vanilla PT
PPT
MetaPT
SUPMER
50 100 150
Training Steps3540455055606570Accuracy (%)
MRPC
FT
Vanilla PT
PPT
MetaPT
SUPMERFigure 3: The performance after different training steps
on CB and MRPC.
4 8 16 32 64
Number of Samples Per Label3436384042444648Accuracy (%)
SST-5
FT
Vanilla PT
PPT
MetaPT
SUPMER
4 8 16 32 64
Number of Samples Per Label606570758085Accuracy (%)
SUBJ
FT
Vanilla PT
PPT
MetaPT
SUPMER
Figure 4: The performance on SST-5 and SUBJ when
different numbers of training samples are available.
ularization parameters and the soft prompts are the
only tunable parameters. We give more details of
training hyper-parameters in Appendix C.
4.3 Main Result
Table 1 and Table 2 show the main results of few-
shot learning and domain generalization. From the
results, we have the following observations.
First, in few-shot learning, SUPMER achieves
better performance than all baselines on 10 of 12
datasets, whether using T5-base or Flan-T5-XL as
the backbone. And the average accuracy of SUP-
MER over all datasets reaches 71.3% on T5-base,
significantly outperforming other baselines ( e.g.,
improving the performance by +1.3 points com-
pared to FT). Notably, when utilizing the larger
Flan-T5-XL as the backbone, SUPMER demon-
strates even more substantial performance gains
(e.g., improving the average performance by +2.5
points compared to FT), which indicates that our
approach unlocks greater capabilities for stronger
models that have undergone instruction-tuning with
a higher number of parameters.
Specifically, SUPMER consistently outperforms
all other prompt tuning methods with the same
number of tunable parameters across all datasets.
This indicates that our method offers soft prompts
with better few-shot generalization ability. And it is
noteworthy to highlight that SUPMER utilizes ex-

--- PAGE 8 ---
Methods SST-2 SST-5 MR CR SUBJ TREC CB RTE QNLI WiC MRPC QQP A VG
1 SUPMER (only labeled) 87.547.083.889.975.4 79.6 67.956.6 59.0 54.6 69.2 69.570.0
2SUPMER (only unlabeled) 87.3 46.784.089.379.680.272.457.361.754.871.370.571.3
3 PPT (labeled + unlabeled) 84.7 45.0 82.487.867.2 77.4 64.355.3 61.6 53.9 68.9 67.768.0
4 MetaPT (labeled + unlabeled) 86.1 46.3 83.789.473.8 80.1 67.257.4 60.0 54.3 70.1 69.969.9
5SUPMER (labeled + unlabeled) 89.148.285.790.879.383.473.258.863.755.370.571.572.5
Table 3: Results of few-shot learning on T5-base, considering different data and methods for prompt initialization.
actly the same unlabelled data as PPT and Unified-
PPT for soft prompt initialization. Yet it consider-
ably outperforms these two baselines, demonstrat-
ing that the performance improvement is primarily
attributable to our methodology rather than the
meta-training data itself . Additionally, SUPMER
outperforms baseline methods with more tunable
parameters ( e.g., full-model tuning) on the majority
of datasets, achieving superior performance with
fewer parameters.
Second, SUPMER is superior to all baselines
in almost all domain-generalization setups. For
example, compared to MetaPT which meta-trains
soft prompts with a supervised sentiment analysis
dataset, SUPMER exhibits average gains of 1.1%
on T5-base and 1.4% on Flan-T5-XL. So it can be
inferred that SUPMER shows stronger robustness
to domain shifts, exhibiting better generalization to
unseen tasks or domains.
Third, for both few-shot learning and domain
generalization on Flan-T5-XL, SUPMER demon-
strates superior performance across almost all
datasets and domains in contrast to few-shot in-
ference. It provides further evidence that for LMs
such as Flan-T5-XL with inherent few-shot infer-
ence capabilities, our approach can significantly en-
hance their abilities in a parameter-efficient tuning
strategy, without providing any in-context exam-
ples during inference.
Fourth, SUPMER also results in lower variances
on most datasets. Few-shot learning is often notori-
ous for its instability. And in our method we keep
few-shot prompt tuning more stable.
4.4 Ablation Study
Analysis of Generalization. Figure 3 shows the
performance trend for each method after different
training steps on datasets CB and MRPC with T5-
base model. It illustrates that few-shot prompt tun-
ing converges slowly with its performance typically
showing an overall decline during the final training
steps because they may easily result in overfitting.
In comparison, SUPMER achieves faster, stronger,
and more enduring few-shot generalization. It notMethods Few-shot Learning DG
1 only sp 66.7 85 .7
2 only mc 66.9 85 .7
3 only ss 67.4 86 .7
4 w/o ta 68.6 85 .3
5 w/o curriculum 69.9 87 .0
6 w/o mgr 69.4 86 .1
7 SUPMER 71.3 88 .0
Table 4: Results of ablation study to illustrate the effect
of individual components. We report the average accu-
racy over all 12 datasets in few-shot learning and all 6
domains in domain generalization (DG).
only accelerates the convergence to the optimal per-
formance realizing fast adaptation, but also con-
sistently maintains its optimal performance across
prolonged training periods .
Effect of Sample Size. We also discuss how
the performance of SUPMER and other baselines
varies when the number of training samples in-
creases on SST-5 and SUBJ. As shown in Figure 4,
with T5-base as the underlying PLM, when the
number of training samples per label grows from
4 to 64, SUPMER is consistently better than other
prompt tuning methods. And the performance gap
between these methods is gradually reduced as the
number of training data increases.
Self-Supervised v.s. Supervised. To illustrate
that self-supervised meta-learning can better gen-
eralize to unseen tasks compared to supervised
meta-learning, we also collect a set of labeled
datasets (ensuring no overlap with downstream test-
ing datasets) to formulate meta-training tasks for
soft prompt initialization and conduct the experi-
ments of few-shot learning on T5-base. The results
are displayed in Table 3 (rows 1 and 2). As our col-
lected labeled data contains lots of sentiment anal-
ysis datasets ( e.g., Yelp5), SUPMER (only labeled)
and SUPMER (only unlabeled) reveal proximity in
their performance on sentiment analysis tasks ( i.e.,
SST-2, SST-5, MR, CR). But in other tasks, using
unlabeled data consistently achieves better results
than utilizing only labeled data, also with a higher
average accuracy over all datasets, which validates

--- PAGE 9 ---
the superiority of self-supervised meta-learning.
Effect of integrating Labeled Data. To further
explore the impact of integrating labeled data and
substantiate the efficacy of SUPMER following
this integration, we amalgamate the original un-
labeled meta-training data with our collected la-
beled data mentioned above, with a mixing ratio
of labeled to unlabeled as 1:2. The amalgamated
data is employed for constructing meta-training
tasks to meta-train SUPMER. Moreover, follow-
ing PPT (Gu et al., 2022) and MetaPT (Huang
et al., 2022), We also leverage pre-training and
vanilla MAML to initialize soft prompts using the
same amalgamated data. The experimental results
of few-shot learning on T5-base are shown in Ta-
ble 3 (rows 3-5). First, we can see that SUPMER
(labeled+unlabeled) outperforms SUPMER (unla-
beled) and SUPMER (labeled) as it allows us to
harness the high-quality advantages of labeled data
while also exploiting the broader semantic concepts
encapsulated by unlabeled data. Second, After the
integration of labeled data, SUPMER still consis-
tently demonstrates significantly superior perfor-
mance compared to baseline methods employing
the same data for prompt initialization, which fur-
ther underscores the effectiveness of SUPMER.
Effect of Individual Components. We train the
following ablation models. 1) only sp / mc / ss:
we retain sentence-pair classification / multi-choice
classification / single-sentence classification as the
only anchor meta-training task format. 2) w/o ta:
we entirely remove the task augmentation method.
3) w/o curriculum: we only retain the vanilla task
augmentation without the curriculum-based idea.
4) w/o mgr: we remove the meta-gradient regu-
larization function. All experiments follow the
settings in §4.1 and are conducted on T5-base. We
report the average accuracy of few-shot learning
and domain generalization in Table 4. More de-
tailed results are in Appendix D.
The results of Row 1-3 indicate that considering
diversified task formats during meta-training helps
efficiently generalize to different tasks as down-
stream tasks often contain various task formats.
Row 4 and Row 5 highlight that task augmenta-
tion plays an essential role in our framework, with
curriculum-based augmentation further enriching
the task distribution and realistically simulating
the distribution shift. Moreover, Row 6 validates
the superiority of meta-gradient regularization inavoiding overfitting to some domain-specific corre-
lations, thus achieving better performance.
5 Conclusion
In this paper, we present SUPMER, a self-
supervised meta-prompt learning framework with
meta-gradient regularization. With a diverse set of
well-designed self-supervised meta-training tasks,
SUPMER jointly meta-learns a universal prompt
initialization and an effective gradient regulariza-
tion function for efficient few-shot generalization.
Extensive experiments on few-shot learning and
domain generalization show that SUPMER outper-
forms other prompt methods and full-model tuning,
achieving state-of-the-art performance.
Limitations
Although SUPMER performs superbly in a variety
of problem scenarios, there still exist some limita-
tions in our work: 1) We did not conduct any data
filtering or cleaning operations to the meta-training
data, which could potentially result in the inclusion
of some biased content. 2) Our experiments are
solely conducted on English tasks, and also do not
involve some kinds of NLP tasks ( e.g., language
generation Li et al., 2022c) or vision-language
tasks (Zhang et al., 2022b; Li et al., 2022b; Zhang
et al., 2019; Li et al., 2021).
To address these limitations, in the future we
plan to conduct further cleansing and filtering on
the current meta-training data. Besides, we intend
to evaluate the few-shot performance of our frame-
work in the multilingual setting and also broaden
the scope of tasks, including retrieval (Pan et al.,
2023), language generation (Li et al., 2022c) and
vision-language tasks (Li et al., 2023b; Chen et al.,
2023; Li et al., 2022a; Zhang et al., 2022a). Fur-
thermore, we hope our work could pave the way
for future research on better leveraging parameter-
efficient methods under few-shot settings.
Acknowledgements
This work has been supported in part by the
Zhejiang NSF (LR21F020004), Key Research
and Development Projects in Zhejiang Province
(No. 2023C01030, 2023C01032), NSFC (No.
62272411), National Key Research and Develop-
ment Program of China (2018AAA0101900), Ant
Group and Alibaba-Zhejiang University Joint Re-
search Institute of Frontier Technologies.

--- PAGE 10 ---
References
Akari Asai, Mohammadreza Salehi, Matthew Pe-
ters, and Hannaneh Hajishirzi. 2022. ATTEMPT:
Parameter-efficient multi-task tuning via attentional
mixtures of soft prompts. In Proceedings of the
2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 6655–6672, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics , pages 440–447,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Nitay Calderon, Eyal Ben-David, Amir Feder, and Roi
Reichart. 2022. DoCoGen: Domain counterfactual
generation for low resource domain adaptation. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 7727–7746, Dublin, Ireland. As-
sociation for Computational Linguistics.
Dong Chen, Kaihang Pan, Guoming Wang, Yueting
Zhuang, and Siliang Tang. 2023. Improving vision
anomaly detection with the guidance of language
modality. arXiv preprint arXiv:2310.02821 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Marie-Catherine De Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Investi-
gating projection in naturally occurring discourse. In
proceedings of Sinn und Bedeutung .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th Interna-
tional Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , pages
1126–1135. PMLR.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Ste-
fanie Tellex. 2019. Openwebtext corpus.
Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401 .
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2022. PPT: Pre-trained prompt tuning for few-shot
learning. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 8410–8423, Dublin,
Ireland. Association for Computational Linguistics.
Sepp Hochreiter, A. Steven Younger, and Peter R. Con-
well. 2001. Learning to learn using gradient descent.
InArtificial Neural Networks — ICANN 2001 , pages
87–94, Berlin, Heidelberg. Springer Berlin Heidel-
berg.
Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li,
and Wanxiang Che. 2022. MetaPrompting: Learn-
ing to learn better prompts. In Proceedings of the
29th International Conference on Computational Lin-
guistics , pages 3251–3262, Gyeongju, Republic of
Korea. International Committee on Computational
Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining , pages 168–177.
Yukun Huang, Kun Qian, and Zhou Yu. 2022. Learn-
ing a better initialization for soft prompts via meta-
learning. arXiv preprint arXiv:2205.12471 .
Rabeeh Karimi Mahabadi, Luke Zettlemoyer, James
Henderson, Lambert Mathias, Marzieh Saeidi,
Veselin Stoyanov, and Majid Yazdani. 2022. Prompt-
free and efficient few-shot learning with language

--- PAGE 11 ---
models. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 3638–3652, Dublin,
Ireland. Association for Computational Linguistics.
Gregory Koch, Richard Zemel, and Ruslan Salakhut-
dinov. 2015. Siamese neural networks for one-shot
image recognition. In ICML Deep Learning Work-
shop , volume 2.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 175–184, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Juncheng Li, Minghe Gao, Longhui Wei, Sil-
iang Tang, Wenqiao Zhang, Mengze Li, Wei Ji,
Qi Tian, Tat-Seng Chua, and Yueting Zhuang. 2023a.
Gradient-regulated meta-prompt learning for gen-
eralizable vision-language models. arXiv preprint
arXiv:2303.06571 .
Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Sil-
iang Tang. 2022a. Fine-grained semantically aligned
vision-language pre-training. Advances in neural
information processing systems , 35:7290–7303.
Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao,
Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-
Seng Chua, Siliang Tang, and Yueting Zhuang.
2023b. Fine-tuning multimodal llms to follow zero-
shot demonstrative instructions. arXiv preprint
arXiv:2308.04152 .
Juncheng Li, Siliang Tang, Linchao Zhu, Haochen Shi,
Xuanwen Huang, Fei Wu, Yi Yang, and Yueting
Zhuang. 2021. Adaptive hierarchical graph reason-
ing with semantic coherence for video-and-language
inference. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 1867–
1877.
Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei
Wu, Yueting Zhuang, and William Yang Wang. 2020.Unsupervised reinforcement learning of transferable
meta-skills for embodied navigation. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12123–12132.
Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu,
Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang,
and Xin Eric Wang. 2022b. Compositional tem-
poral grounding with structured variational cross-
graph correspondence learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3032–3041.
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,
and Ji-Rong Wen. 2022c. Pretrained language mod-
els for text generation: A survey. arXiv preprint
arXiv:2201.05273 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys , 55(9):1–35.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-
iao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:
Prompt tuning can be comparable to fine-tuning
across scales and tasks. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 61–68,
Dublin, Ireland. Association for Computational Lin-
guistics.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
Pieter Abbeel. 2018. A simple neural attentive meta-
learner. In International Conference on Learning
Representations .
Quang Nguyen. 2015. The airline review dataset.
Alex Nichol, Joshua Achiam, and John Schulman.
2018. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999 .
Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei
Ji, Shuo Zhang, Jun Lin, Xiaozhong Liu, and Sil-
iang Tang. 2023. Controlretriever: Harnessing the
power of instructions for controllable retrieval. arXiv
preprint arXiv:2308.10025 .
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL-04) , pages 271–278,
Barcelona, Spain.

--- PAGE 12 ---
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL’05) , pages 115–124, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Mohammad Taher Pilehvar and Jose Camacho-Collados.
2019. WiC: the word-in-context dataset for evalu-
ating context-sensitive meaning representations. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 1267–1273,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille.
2018. Few-shot image recognition by predicting pa-
rameters from activations. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) .
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations
toward training trillion parameter models. In SC20:
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16. IEEE.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining , pages 3505–3506.
Sachin Ravi and Hugo Larochelle. 2017. Optimization
as a model for few-shot learning. In International
Conference on Learning Representations .
Timo Schick and Hinrich Schütze. 2021. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339–2352, Online. Association
for Computational Linguistics.Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. In Ad-
vances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Tianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and
Xuanjing Huang. 2022. Multi-task pre-training of
modular prompt for few-shot learning. arXiv preprint
arXiv:2210.07565 .
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, ko-
ray kavukcuoglu, and Daan Wierstra. 2016. Match-
ing networks for one shot learning. In Advances in
Neural Information Processing Systems , volume 29.
Curran Associates, Inc.
Ellen M. V oorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval , SIGIR ’00, page 200–207, New York, NY ,
USA. Association for Computing Machinery.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’,
and Daniel Cer. 2022. SPoT: Better frozen model
adaptation through soft prompt transfer. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 5039–5059, Dublin, Ireland. Association
for Computational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel Bowman. 2019. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. In Advances in Neural Information
Processing Systems , volume 32. Curran Associates,
Inc.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.

--- PAGE 13 ---
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. 2018. mixup: Beyond empirical
risk minimization. In International Conference on
Learning Representations .
Wenqiao Zhang, Jiannan Guo, Mengze Li, Haochen
Shi, Shengyu Zhang, Juncheng Li, Siliang Tang, and
Yueting Zhuang. 2022a. Boss: Bottom-up cross-
modal semantic composition with hybrid counterfac-
tual training for robust content-based image retrieval.
arXiv preprint arXiv:2207.04211 .
Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu
Zhang, Qingpeng Cai, Juncheng Li, Sihui Luo, and
Yueting Zhuang. 2022b. Magic: Multimodal rela-
tional graph adversarial inference for diverse and
unpaired text-based image captioning. In Proceed-
ings of the AAAI Conference on Artificial Intelligence ,
volume 36, pages 3335–3343.
Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang
Pu, Fei Wu, and Yueting Zhuang. 2019. Frame aug-
mented alternating attention network for video ques-
tion answering. IEEE Transactions on Multimedia ,
22(4):1032–1041.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems , volume 28. Curran Associates, Inc.
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and
Ziwei Liu. 2022. Learning to prompt for vision-
language models. International Journal of Computer
Vision , 130(9):2337–2348.
Yftah Ziser and Roi Reichart. 2018. Pivot based lan-
guage modeling for improved neural domain adap-
tation. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers) , pages 1241–1251,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

--- PAGE 14 ---
Appendices
A Additional Information for SUPMER
A.1 Complete Analysis of SUPMER
In this section, we provide a more comprehensive
and complete analysis of SUPMER. We will show
that during meta-training, the optimization of soft
prompt embeddings θand the meta-gradient regu-
larization parameters ϕtends to maximize the inner
product of gradients obtained from the support set
after regulation and gradients from the query set.
Specifically, to update the parameters θandϕ,
we should evaluate their gradients at first, denoting
them as gθandgϕ. Considering the original algo-
rithm of MAML, each task consists of a support set
and a query set. And only one step of gradient de-
scent is applied in the inner-loop optimization. To
make our statement more direct, we denote the loss
function based on the support set and the query set
asL0andL1. In SUPMER, ignoring the regular-
ized loss, only L1is directly utilized to optimize ϕ,
while θis optimized in a bi-level meta-optimization
paradigm. Here we define the following terms re-
lated to θsimilar to Nichol et al. (2018):
gθ
i=∂Li(θi)
∂θi(gradient obtained during SGD)
gθ
i=∂Li(θ0)
∂θ0(gradient at initial point)
Hθ
i=∂2Li(θ0)
∂θ2
0(Hessian at initial point)
θ1=θ0−α1ψϕ(gθ
0) (gradient descent in the inner-loop)
(11)
For each definition i∈ {0,1}andψϕ(·)is the
meta-gradient regularization operation. θ0denotes
the initial soft prompt embeddings for each step,
andθ1denotes the embeddings after the inner-loop
optimization. Obviously we have gθ
0=gθ
0. Firstly
we perform a Taylor series expansion to approxi-
mate the SGD gradients gθ
1obtained from the query
set as follows:
gθ
1=∂L1(θ1)
∂θ1
=∂L1(θ0)
∂θ0+∂2L1(θ0)
∂θ2
0(θ1−θ0) +O(||θ1−θ0||2)|{z }
=O(α2
1)
=gθ
1−α1Hθ
1ψϕ(gθ
0) +O(α2
1)
(12)
Then we analysis the gradient descent operation
in the inner-loop optimization based on the sup-
port set. Define Uas the gradient descent and we
haveU(θ0) =θ0−α1ψϕ(∂L0(θ0)
∂θ0). So we can get∂U(θ0)
∂θ0and∂U(θ0)
∂ϕas follows:
∂U(θ0)
∂θ0=∂
∂θ0(θ0−α1ψϕ(∂L0
∂θ0))
=I−α1∂ψϕ(gθ
0)
∂gθ
0·∂gθ
0
∂θ0
=I−α1∂ψϕ(gθ
0)
∂gθ
0·Hθ
0(13)
∂U(θ0)
∂ϕ=∂
∂ϕ(θ0−α1ψϕ(∂L0
∂θ0))
=−α1∂ψϕ(gθ
0)
∂ϕ(14)
So based on Eq. (12, 13, 14), we can finally approx-
imate the gradients gθandgϕas:
gθ=∂L1(θ1)
∂θ0
=∂L1(U(θ0))
∂θ0
=∂L1
∂θ1·∂U(θ0)
∂θ0
=gθ
1−α1Hθ
1ψϕ(gθ
0)−α1gθ
1·∂ψϕ(gθ
0)
∂gθ
0·Hθ
0+O(α2
1)
=gθ
1−α1∂
∂θ0(gθ
1ψϕ(gθ
0)) +O(α2
1)
gϕ=∂L1(θ1)
∂ϕ
=∂L1(U(θ0)
∂ϕ
=∂L1
∂θ1·∂U(θ0)
∂ϕ
=−α1gθ
1∂ψϕ(gθ
0)
∂ϕ+O(α2
1)
=−α1∂
∂ϕ(gθ
1ψϕ(gθ
0)) +O(α2
1)
(15)
Thus,−∂
∂θ0(gθ
1ψϕ(gθ
0))and−∂
∂ϕ(gθ
1ψϕ(gθ
0))indi-
cate the optimization direction, which increases the
inner product between gradients from the query set
and gradients from the support set after transfor-
mation. To further consolidate our analysis, we
also track the normalized gradient inner product
in the first 5000 steps during meta-training. As
shown in Figure 5, it is clear that the normalized
gradient inner product gradually increases during
meta-training.
On this basis, since there exists distribution shift
between the support set and the query set after task
augmentation, our method aligns the gradient di-
rections across different distributions, which helps
enhance model generalization. In other words, the
trainable parameters descend in a coordinated man-
ner such that the input-output correspondence is as

--- PAGE 15 ---
0 2000 4000
Training Steps0.00.20.40.60.8Gradient Inner ProductFigure 5: Normalized gradient inner products in the first
5000 steps during meta-training.
close as possible across two distributions with de-
viation. Besides, the meta-gradient regularization
parameters ϕalso retain some domain-invariant in-
formation of the meta-training data in the above
process. Considering that ϕis fixed in downstream
tasks, ϕcan be applied to encourage the alignment
between the domain-specific gradients and avoid
prompt-tuning overfitting to some domain-specific
correlations.
A.2 Constructing Anchor Meta Tasks
Given a sentence xfrom unlabeled corpora, we can
derive semantically meaningful sentence embed-
dingH=fenc
θ(x)with PLMs, e.g., T5 encoder.
And we apply K-means to cluster these unlabeled
sentences according to their embeddings:
P,{µc}= arg min
{Cc},{µc}KX
c=1X
H∈Cc∥H−µc∥2(16)
where µcindicates the learned centroid of cluster
CcandPindicates the partitions of all sentences.
K-means clustering leads to more abundant formats
and objectives of meta-training tasks. Based on the
results of K-means, we design three formats of an-
chor self-supervised meta-training tasks: sentence-
pair classification, multi-choice classification, and
single-text classification. Here we introduce each
of them in detail.
Sentence-pair Classification. Sentence-pair
classification takes a pair of sentences (x0, x1)as
input, and x0is the anchor sentence. We carry
on next sentence prediction task and sentence
similarity task in sentence-pair classification with
the label list Y= [0,1,2]. For the former one,
following Gu et al. (2022), we set two sentences
next to each other as label 0, those from the same
document but not adjacent as label 2, and those
from different documents as label 1. And forAlgorithm 1 Meta-training Process of SUPMER
1:p(T): Distribution over anchor tasks
2:fθ: PLM with soft prompt embeddings θ
3:ψϕ: Meta-gradient regularization
4:α1, β1, β2: Learning rate
5:TA: Task augmentation in Algorithm 2
6:s← −1
7:Randomly initialize θ, ϕ
8:while not done do
9: Sample a batch of task {τi}n
i=1from p(T)
10: {τi}n
i=1=TA({τi}n
i=1, p(T), s)
11: for all τi={Ds
τi,Dq
τi}do
12: Evaluate ∇θLDsτi(fθ)withDs
τi
13: Evaluate ∇θLDq
τi(fθ)withDq
τi
14: Transform ∇θLDsτi(fθ)viaψϕ(·)
15: si=∇θLDq
τi(fθ)·ψϕ(∇θLDsτi(fθ))
∥∇θLDq
τi(fθ)∥·∥ψϕ(∇θLDsτi(fθ))∥
16: θ′
i=θ−α1ψϕ(∇θLDsτi(fθ))
17: end for
18: s←P
isi/P
i1
19: θ←θ−β1∇θP
τiLDq
τi(fθ′
i)
20: ϕ←ϕ−β2∇ϕ P
τiLDq
τi(fθ′
i) +Lreg
21:end while
22:return θ, ϕ
sentence similarity task, we set two sentences
coming from the same cluster as label 0, and those
from different clusters as label 1. In this way, the
prompt template and verbalizer are designed as:
P=“s1⟨X⟩.s2”
V={0→yes,1→no,2→maybe}(17)
Multi-choice Classification. Multi-choice clas-
sification takes an anchor sentence x0as the query
and we should find the correct one in several an-
swer candidates. Here we also set two different
tasks. The first one aims to select the sentence next
tos0and the second one aims to select the sentence
which belongs to the same cluster as s0. In each
task we will set four candidates, and only one of
them is correct. We design the prompt template
and verbalizer as follows:
P=“s0? A.s1···D.s4.Answer: ⟨X⟩”
V={0→A,1→B,2→C,3→D}(18)
Single-Sentence Classification. Through K-
means clustering, each sentence is associated with
a cluster label riin{0,1}Kwhere ric= 1ifc=k
andyic= 0ifc̸=k. Here krepresents the cluster

--- PAGE 16 ---
to which the sentence belongs. We simply use ri
as the pseudo label for meta-training and construct
4-way classification tasks. As for the designing
of the verbalizer, we transform the single-sentence
classification into the format of multi-choice classi-
fication. We insert the centroid of cluster µcinto
the template and use it to represent the correspond-
ing cluster. So that we have:
P=“s0? A.⟨µc1⟩··· D.⟨µc4⟩.Answer: ⟨X⟩”
V={0→A,1→B,2→C,3→D}(19)
On this basis, for each task format, we separate
all data into different tasks to construct anchor
meta-training tasks with good task distributions.
Through K-means, sentences with similar embed-
dings are clustered into the same group. So in
sentence-pair classification and multi-choice classi-
fication, we group samples whose anchor sentence
comes from the same cluster into the same meta-
training task. And in single-sentence classification,
for each meta-training task, we randomly select N
clusters as Nclasses and then sample ksentences
for each cluster to construct a N-way k-shot classi-
fication task ( N= 4). In this way, we completely
construct all anchor meta-training tasks.
A.3 Additional Loss to Train Meta-Gradient
Regularization Parameters
In the meta-training stage, we optimize the meta-
gradient regularization parameters ϕvia Eq. (7),
utilizing the same loss which optimizes the soft
prompt embeddings. Here we introduce a regu-
larized loss to attach some additional restrictions
when updating the meta-gradient regularization pa-
rameters. Notably, a higher value of bkin Eq. (10)
indicates a higher probability of a larger distribu-
tion deviation between the support set and the query
set. Furthermore, in Eq. (5) we also tend to in-
crease zto achieve a more pronounced gradient
transformation with a more noticeable distribution
deviation. From this perspective, zhas a similar
monotonicity with bk, and they both range between
0 and 1. Thus we further add a regularized loss
Lreg=∥z−bk∥2to constrain the value of zand
finally modify Eq. (7) into:
ϕ←ϕ−β2∇ϕ X
τi∼p(T)LDq
τi(fθ′
i) +λLreg
(20)
A.4 Pseudo-Codes of SUPMER
We show the pseudo-codes for the meta-training
process of SUPMER in Alg. 1. And the process of
curriculum-based task augmentation is described
in Alg. 2.Algorithm 2 TA: Curriculum-based Task Aug-
mentation
1:{τi}n
i=1: A batch of anchor tasks
2:p(T): Distribution over anchor tasks
3:s∈[−1,1]: Avg cos-sim between gradients
4:α,m: hyper-parameters
5:s←(1 +s)/2
6:b←(ms−1)/(m−1)
7:for all τi={Ds
τi,Dq
τi}do
8: Sample task τj={Ds
τj,Dq
τj}from p(T)
9: Draw λfrom Beta (α, bα )
//Dq
τi= (Hq
i,Yq
i),Dq
τj= (Hq
j,Yq
j)
//H: the hidden representations of samples
10: ˜Hq
i= (1−λ)Hq
i+λHq
j
11: ˜Yq
i= (1−λ)Yq
i+λYq
j
12: Dq
τi←(˜Hq
i,˜Yq
i)
13:end for
14:return {τi}n
i=1
B Dataset & Baseline Details
Few-shot Learning. We conduct experiments of
few-shot learning on 6 different downstream En-
glish tasks with 12 datasets. Since some of the
test sets of the datasets are not publicly available,
following Karimi Mahabadi et al. (2022), we lever-
age the original validation sets of SST-2, CB, RTE,
QNLI, WiC, MRPC, and QQP1as substitutes for
the unavailable test sets. And the validation sets
for few-shot learning are sampled from the orig-
inal training set, ensuring no overlap with our
designated test sets. Besides, we download the
datasets of SST-2, SST-5, MR, CR, and SUBJ from
Gao et al. (2021). And the rest of the datasets
are obtained from the HuggingFace Datasets li-
brary (Lhoest et al., 2021). CB, RTE, BoolQ, and
Wic are from SuperGLUE Benchmark (Wang et al.,
2019), while QNLI, MRPC, and QQP are from
GLUE Benchmark (Wang et al., 2018) with Cre-
ative Commons license (CC BY 4.0). We give the
statistics of all these datasets in Table 5.
Domain Generalization. Similar to Calderon
et al. (2022), We evaluate on the sentiment analysis
task including 6 different domains: Airlines (A),
Books (B), DVDs (D), Electronics (E), Kitchen
appliances (K), and Restaurants (R). Each domain
has totally 2,000 manually labeled data of binary
categories for testing, including 1000 positive and
1https://quoradata.quora.com/

--- PAGE 17 ---
Dataset Task #Train #Test K
SST-2 Sentiment analysis 6920 872 2
SST-5 Sentiment analysis 8544 2210 5
MR Sentiment analysis 8662 2000 2
CR Sentiment analysis 1774 2000 2
SUBJ Subjectivity classification 8000 2000 2
TREC Question classification 5452 500 6
CB Natural language inference 250 56 3
RTE Natural language inference 2490 277 2
QNLI Question answering 104743 5463 2
WiC Word sense disambiguation 5428 638 2
MRPC Paraphrase detection 3668 408 2
QQP Paraphrase detection 363846 40430 2
Table 5: Statistics of all 12 datasets for few-shot learn-
ing. K is the number of labels. We sample N×K
instances from the original training set to construct the
few-shot training and validation sets. And #Test shows
the size of the test set.
1000 negative. We choose A as the source domain
and the other five (B, D, E, K, R) constitute the
target domains. We sample 16 instances per la-
bel from the training set of the source domain for
prompt tuning and then evaluate on the test sets of
all 6 domains.
Baselines. We first compare with baseline meth-
ods with the same number of parameters as SUP-
MER. These methods utilize prompt tuning (Lester
et al., 2021) to handle downstream tasks, with the
key distinction lying in the initialization of the soft
prompts. Vallina prompt tuning ( PTLester et al.,
2021) directly tunes the soft prompts in the down-
stream task, which are randomly initialized from
a normal distribution. PPT (Gu et al., 2022) pre-
trains soft prompts in a self-supervised way with 3
formats of pre-training tasks: sentence-pair classi-
fication, multiple-choice classification and single-
text classification. Unified-PPT (Gu et al., 2022)
formulate all these three formats into a unified task
form. MetaPT (Huang et al., 2022) using a su-
pervised sentiment analysis dataset Yelp5 as the
meta-training data and directly leveraging MAML
to initialize soft prompts.
To further demonstrate the effectiveness of
our method, we also consider baseline methods
with more tunable parameters, including Prefix-
Tuning (Li and Liang, 2021) and P-tuning-v2 (Liu
et al., 2022), which add prompts at each layer of
PLM. We also compare with full-model tuning
(FT) that fine-tunes all parameters of the PLM.
Given that FLAN-T5-XL was also designed with
few-shot inference in mind, we newly compare
with two baseline methods on FLAN-T5-XL: zero-Hyper-parameter Value
Number of clusters for each task format 250
Tasks per batch 4
Size of support set per task 32
Size of query set per task 32
Optimizer Adam
Inner loop learning rate 0.1
Outer loop learning rate 0.1
Learning rate for ϕ 1e-4
Scheduler Linear scheduler
Warm-up steps 0
Max training steps 100,000
Validation steps 2,000
Max sequence length 512
λ 1.0
m 2.0
Table 6: Hyper-parameters for SUPMER. ϕdenotes
the meta-gradient regularization parameters. λis the
coefficient of the regularized loss. And mis the curve
parameter in the curriculum-based task augmentation.
shot inference andfew-shot inference . For both
of them, we directly employ Flan-T5-XL for down-
stream evaluation, coupled with carefully designed
task instructions for each dataset. Furthermore, in
few-shot inference, we also provide an appropriate
number of few-shot examples to form a demonstra-
tion context.
C Training Details
We apply the T5 base model (Raffel et al., 2020)
(220M parameters) and Flan-T5-XL model (Chung
et al., 2022) (3B parameters) as the underlying
PLM, and use the HuggingFace Pytorch implemen-
tation (Wolf et al., 2020). We run experiments with
8 GeForce RTX 3090 24G GPUs. And the meta-
training process of SUPMER takes about 140 GPU
hours. Next we will describe the details of training
hyper-parameters in the case of leveraging T5-base
as the PLM.
C.1 Training Hyper-parameters for
Downstream Tasks
In our experiments, we leverage full-model tun-
ing and prompt tuning to solve downstream tasks,
including few-shot learning and domain general-
ization. In few-shot learning, following some prior
work (Schick and Schütze, 2021; Karimi Mahabadi
et al., 2022), we set the maximum sequence length
of each example to 256 for CR, SUBJ, CB, RTE
and WiC, and 128 for other datasets. While in do-
main generalization, the maximum sequence length
of each example is set to 256.
We run each experiment 5 times on the random

--- PAGE 18 ---
Methods SST-2 SST-5 MR CR SUBJ TREC CB RTE QNLI WiC MRPC QQP
only sp 83.61.5 42.62.2 81.71.8 86.00.6 65.82.8 71.26.6 64.62.1 57.02.5 58.43.3 53.61.5 69.91.3 66.01.0
only mc 83.41.4 44.51.9 79.35.1 88.30.5 70.54.7 66.41.4 65.93.1 54.91.3 58.71.7 54.21.8 68.80.8 67.61.3
only ss 84.51.5 45.02.0 81.50.7 88.40.5 73.33.1 79.15.8 62.12.6 53.91.0 56.51.4 53.31.3 67.71.3 63.71.7
w/o ta 84.71.0 40.13.3 81.91.8 87.20.8 73.62.8 78.83.7 66.41.9 56.60.9 59.41.8 54.32.4 69.51.1 70.21.1
w/o curriculum 86.80.8 40.82.2 82.31.3 88.40.9 74.83.1 79.71.6 71.02.1 56.50.862.61.455.41.169.70.871.31.2
w/o mgr 85.01.3 44.51.1 82.80.7 88.00.5 76.01.7 79.55.0 67.11.6 56.80.8 58.92.4 54.42.0 70.01.0 70.30.9
SUPMER 87.30.546.70.684.00.689.30.379.62.280.20.972.41.457.31.061.71.0 54.81.271.30.570.51.0
Table 7: Detailed results of ablation study for few-shot learning to illustrate the effect of individual Components.
In the first three rows we keep only one anchor task format during meta-training, and sp stands for sentence-pair
classification, mc for multi-choice classification, ss for single-sentence classification. And w/o ta means entirely
removing task augmentation, w/o curriculum only retains the vanilla task augmentation without the curriculum-based
idea. w/o mgr means removing the meta-gradient regularization method.
MethodSource Target
A B D E K R
only sp 83.41.1 82.11.4 83.00.7 88.51.2 88.90.8 88.10.7
only mc 84.00.6 82.31.2 81.50.9 88.51.0 89.30.7 88.80.7
only ss 83.60.7 84.70.8 84.20.6 88.90.9 89.70.9 89.00.3
w/o ta 83.40.8 82.01.4 81.71.5 87.80.8 88.20.6 88.60.6
w/o curriculum 84.00.5 84.70.8 83.90.6 89.60.5 90.30.8 89.71.1
w/o mgr 83.80.4 83.40.5 83.30.5 88.10.6 89.20.8 88.90.4
SUPMER 85.70.585.30.685.10.490.30.791.10.590.40.4
Table 8: Detailed results of ablation study for domain generalization to illustrate the effect of individual Components.
seed [10, 20, 30, 40, 50] and report the average ac-
curacy as well as the standard deviation. For both
full-model tuning and prompt tuning, We imple-
ment AdamW as the optimizer. We use a batch size
of 32 and train the model for 200 epochs, mean-
while evaluating the model every 10 steps. And we
report the results for hyper-parameters performing
the best on the validation set for each task.
Besides, for full-model tuning, all parameters of
PLM are fine-tuned without adding soft prompts.
We use the learning rate of [1e-5, 2e-5, 3e-5] and
choose the one obtaining the highest validation
performance. Moreover, to fine-tune the Flan-T5-
XL model, we use ZeRO (Rajbhandari et al., 2020)
stage-2 provided in DeepSpeed (Rasley et al., 2020)
to reduce GPU memory usage.
For prompt tuning, we freeze all PLM parame-
ters and only tune soft prompts composed of 100
soft tokens. As a result, the tunable parameters of
prompt tuning are only 77 Kwith T5-base and 205K
with Flan-T5-XL, updating around 3000 and 15000
times fewer parameters on T5-base and Flan-T5-Xl,
respectively, compared to full-model tuning. And
we find that prompt tuning requires a much larger
learning rate than full-model tuning. We search
for the learning rate in [1e-1, 2e-1, 3e-1] and also
choose the model with the best performance on thevalidation set.
C.2 Training Hyper-parameters for Prompt
Initialization
Pre-training for prompt initialization. Gu et al.
(2022) proposes two frameworks for unsuper-
vised prompt pre-training, named PPT and Uni-
fied PPT. PPT designs three formats of unsuper-
vised pre-training tasks (sentence-pair classifica-
tion, multiple-choice classification and single-text
classification), and Unified-PPT further formulate
them into a unified task form. We implement PPT
and Unified-PPT following the hyper-parameters
provided in Gu et al. (2022) and reset the pre-
trained language model to T5-base and Flan-T5-
XL. Specifically, for both PPT and Unified-PPT,
we sample 10GB of unlabeled data from OpenWeb-
Text to construct pre-training tasks for each task
format. And 5% data are split for validation. We
apply the “inverse square root” learning rate sched-
uler with no warm-up steps and set the learning
rate as 0.1. We set the batch size to 256 with the
max sequence length as 512, and train soft prompts
for at most 200,000 steps. We evaluate the perfor-
mance on the validation set every 2,000 steps and
choose prompts with the lowest validation loss.

--- PAGE 19 ---
MethodSource TargetA VG
A B D E K
1 SUPMER (only labeled) 86.4 84.7 84 .8 90 .0 90 .7 87.3
2 SUPMER (only unlabeled) 85.7 85.3 85 .1 90 .3 91 .187.5
3 PPT (labeled + unlabeled) 83.1 79 .0 84 .4 89 .3 90 .6 85.3
4 MetaPT (labeled + unlabeled) 86.3 85 .3 86 .7 90 .1 91 .4 88.0
5 SUPMER (labeled + unlabeled) 86.6 88 .6 88 .5 92 .7 93 .790.0
Table 9: Results of domain generalization on T5-base, considering different data and methods for prompt initializa-
tion. As our collected labeled data includes Yelp5, a sentiment analysis dataset in the domain of restaurants, we
conduct the experiments of domain generalization only across domains A, B, D, E, K (without R).
Meta-training for prompt initialization. In our
SUPMER framework, we sample 10GB of unla-
beled data from OpenWebText to construct self-
supervised meta-training tasks. We split 5% data to
construct tasks for validation. And for each task for-
mat, we first set the number of clusters to 250. We
sample 4 meta-training tasks in a batch, and train
the prompt embeddings θand the meta-gradient
regularization parameters ϕfor at most 100,000
steps. We also evaluate the performance on the
validation set every 2,000 steps, choosing θand
ϕwith the lowest validation loss for downstream
tasks. Table 6 lists all training hyper-parameters for
SUPMER. It is worth noting that for most hyper-
parameters in Table 6, we just set a default value by
experience without tuning them. we tune the hyper-
parameters which are also tuned in other baselines
(e.g., learning rate), ensuring all methods have the
same number of tunable hyper-parameters in our
experiments.
Moreover, to illustrate the superiority of
self-supervised meta-learning, we also imitate
MetaPT(Huang et al., 2022) to initialize soft
prompts via supervised meta-learning. MetaPT
uses a supervised sentiment analysis dataset Yelp5
as the meta-training data, which has 650,000 train-
ing samples only covering the domain of restau-
rants. Following Huang et al. (2022), We group all
labeled data into 10 clusters through K-means. And
we set the inner loop learning rate to 0.08, the outer
loop learning rate to 0.025 with the early stop pa-
tience as 6. Other hyper-parameters are consistent
with those in SUPMER.
D Full Results of Ablation Study
In this section, we first give detailed experimental
results of the ablation study to illustrate the effect
of individual components. We evaluate each abla-
tion model over all 12 datasets of few-shot learning
and all 6 domains of domain generalization, withT5-base as the underlying PLM. We run each ex-
periment 5 times on the random seed [10, 20, 30,
40, 50] and report the average performances as well
as the standard deviation. The detailed results of
few-shot learning and domain generalization are
shown in Table 7 and Table 8. We can see each
component is critical in our framework.
Besides, in §4.4, to explore the superiority of
self-supervised meta-learning and the impact of
integrating additional labeled data for soft prompt
initialization, we conduct experiments of few-shot
learning on T5-base, considering different data and
methods for soft prompt initialization. We also
carry out experiments of domain generation lever-
aging different data with various prompt initial-
ization methods, with the results presented in Ta-
ble 9. From Table 3 and 9, it is evident that self-
supervised meta-learning utilizing unlabeled data
exhibits enhanced adaptability to unseen tasks in
comparison to its supervised counterparts. And
amalgamating both labeled and unlabeled data for
the construction of meta-training tasks emerges
as a more advantageous strategy. When it comes
to employing both labeled and unlabeled data for
prompt initialization, SUPMER continues to show-
case markedly superior results in contrast to base-
line methods in the realms of both few-shot learn-
ing and domain generalization.
