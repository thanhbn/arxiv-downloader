# 2103.10385.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2103.10385.pdf
# File size: 1583446 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GPT Understands, Too
Xiao Liu1∗, Yanan Zheng1∗, Zhengxiao Du1, Ming Ding1, Yujie Qian2,
Zhilin Yang1†, Jie Tang1†
1Tsinghua University2Massachusetts Institute of Technology
Abstract
Prompting a pretrained language model with
natural language patterns has been proved effec-
tive for natural language understanding (NLU).
However, our preliminary study reveals that
manual discrete prompts often lead to unsta-
ble performance—e.g., changing a single word
in the prompt might result in substantial per-
formance drop. We propose a novel method
P-Tuning that employs trainable continuous
prompt embeddings in concatenation with dis-
crete prompts. Empirically, P-Tuning not only
stabilizes training by minimizing the gap be-
tween various discrete prompts, but also im-
proves performance by a sizeable margin on
a wide range of NLU tasks including LAMA
and SuperGLUE. P-Tuning is generally effec-
tive for both frozen and tuned language models,
under both the fully-supervised and few-shot
settings.
1 Introduction
Pretrained language models (PLMs; Brown et al.,
2020) have significantly advanced the performance
of natural language understanding (NLU). PLMs
are trained with different pretraining objectives,
such as masked language modeling (Devlin et al.,
2018), autoregressive language modeling (Radford
et al., 2019), seq2seq (Raffel et al., 2019), and per-
mutation language modeling (Yang et al., 2019).
PLMs can be further enhanced with prompting
(Brown et al., 2020; Schick and Schütze, 2020),
which employs manually written prompt patterns as
additional input to a language model. With prompt-
ing while PLMs are either finetuned on a small la-
beled dataset or frozen for direct inference on down-
stream tasks. Prompting has significantly improved
the performance of many NLU tasks (Brown et al.,
2020; Schick and Schütze, 2020).
†corresponding to: Zhilin Yang (zhiliny@tsinghua.edu.cn)
and Jie Tang (jietang@tsinghua.edu.cn)
∗indicates equal contribution.
Figure 1: Average scores on 7 dev datasets of Super-
GLUE using P-Tuning.
Prompt P@1
w/o PTP@1
w/ PT
[X] is located in [Y]. (original) 31.3 57.8
[X] is located in which country or state? [Y]. 19.8 57.8
[X] is located in which country? [Y]. 31.4 58.1
[X] is located in which country? In [Y]. 51.1 58.1
Table 1: Discrete prompts suffer from instability (high
variance), while P-Tuning stabilizes and improves per-
formance. Results are precision@1 on LAMA-TREx
P17 with BERT-base-cased. “PT” refers to P-Tuning,
which trains additional continuous prompts in concate-
nation with discrete prompts.
However, we observe that manual discrete
prompts suffer from a large degree of instability.
As shown in Table 1, with a frozen language model,
changing a single word in the prompt might result
in substantial performance drop. As we will show
in Section 3, when the language model is tuned,
the instability problem is alleviated but the perfor-
mance difference between different prompts is still
sizeable, especially in the few-shot setting. Such
an instability issue of discrete prompts poses a crit-
ical challenge in practice. Recent approaches of
automatic prompting have attempted to search for a
better-performing prompt given a task (Shin et al.,
2020; Gao et al., 2020; Jiang et al., 2020b), but
these methods do not change the unstable nature of
discrete prompts.
To reduce the instability of discrete prompts,
we propose a novel method P-Tuning that em-
ploys trainable continuous prompt embeddings in
concatenation with discrete prompts. Specifically,arXiv:2103.10385v2  [cs.CL]  25 Oct 2023

--- PAGE 2 ---
given a discrete prompt as the input, P-Tuning con-
catenates continuous prompt embeddings with the
discrete prompt tokens and feeds them as the input
to the language model. The continuous prompts are
updated by backpropagation to optimize the task
objective. The intuition is that continuous prompts
incorporate a certain degree of learnability into the
input, which may learn to offset the effects of mi-
nor changes in discrete prompts to improve training
stability. To further improve performance, we em-
ploy a prompt encoder using LSTMs or MLPs to
model the dependency between continuous prompt
embeddings.
We experiment with two NLU benchmarks: the
LAMA (Petroni et al., 2019) knowledge probing
and SuperGLUE (Wang et al., 2019a). On LAMA,
with the language model frozen, P-Tuning out-
performs manual discrete prompts and searched
prompts by 20+ points and 9 points respectively
with the same pretrained models. On SuperGLUE,
with the language model finetuned, P-Tuning out-
performs PET (Schick and Schütze, 2020) with
the best discrete prompts under both the fully-
supervised and few-shot settings. In addition to im-
proving performance, our results show that across
a wide range of tasks and settings, P-Tuning sub-
stantially reduces the performance gap between dif-
ferent discrete prompts, which results in improved
stability for language model adaptation.
2 Method
2.1 Issues with Discrete Prompts
Prompting employs natural language patterns as
additional inputs to pretrained language models for
adaptation to downstream tasks (Brown et al., 2020;
Schick and Schütze, 2020). Prior work (Zheng
et al., 2021) has pointed out that prompting has
achieved consistent and substantial improvements
on a number of NLP tasks. However, it still re-
mains a challenging problem of how to write high-
performing discrete prompts.
We performed preliminary experiments using
different manual prompts on the LAMA knowledge
probing task (Petroni et al., 2019), which aims to
extract triplet knowledge from a language model
by predicting the tail entities. Results in Table 1
show that manual discrete prompts lead to unstable
performance. For example, if we compare the last
two prompts in the table, changing a single word
in prompt causes a drastic decrease of 20 points in
performance.In light of the challenge, recent works propose to
automate the search procedure of discrete prompts
by mining the training corpus (Jiang et al., 2020b),
gradient-based searching (Shin et al., 2020), and us-
ing pretrained generative models (Gao et al., 2020).
However, these works aim at searching for better-
performing prompts but do not change the nature
of instability for discrete prompts. In addition to
the instability issue, searching in the discrete space
might not be able to fully leverage the gradients
from backpropagation, which will potentially result
in suboptimal solutions. To this end, we explore
the possibility of training continuous prompts to
stabilize and improve the performance of language
model adaptation.
2.2 P-Tuning
Formally, let Mbe a pretrained language model
with a hidden size of hand a vocabulary size of
|V|. Let{(xi,yi))}ibe a labeled dataset for an
NLU task, where x0:n={x0, x1, ..., x n}is an
input consisting of a sequence of discrete tokens,
andy∈ Y is a label. Our goal is to estimate the
conditional probability for classification fM(x) =
ˆp(y|x)with parameters of Meither finetuned or
frozen.
Prompting was proposed in the format of
discrete tokens (Schick and Schütze, 2020).
Let[Di]be a discrete prompt token. Each
prompt can be described as a template T=
{[D0:i],x,[D(i+1): j],y,[D(j+1): k]}, which could
organize the labeled data (including the inputs x
and the label y) into a sequence of text tokens, such
that the task could be reformulated as filling in the
blanks of the input text. For example, for the task of
predicting a country’s capital (LAMA-TREx P36),
a prompt could be “The capital of [INPUT] is [LA-
BEL].” With a piece of labeled data “(Britain, Lon-
don)”, the reformulated text would be “The capital
of Britain is [MASK].”, where “[MASK]" should
predict the given label “London”. Both discrete
prompts and discrete data are together mapped into
input embeddings:
{e(D0)...e(Di),e(x0), ...,e(xn), ...,e(Dk)}
through the pretrained embedding layer, where e∈
R|V|× d.
However, as is discussed in Section 2.1, such
discrete prompts tend to be extremely unstable
and might not be optimal with back-propagation.
Therefore, we propose P-Tuning that uses contin-
uous prompt embeddings to improve and stabilize

--- PAGE 3 ---
Pre-trained Language Model(GPT, BERT, …Prompt Encoder[P0]
<latexit sha1_base64="0DXRfnpo546JxXEWz0BMIZpwZ/g=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64="0DXRfnpo546JxXEWz0BMIZpwZ/g=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64="0DXRfnpo546JxXEWz0BMIZpwZ/g=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit><latexit sha1_base64="0DXRfnpo546JxXEWz0BMIZpwZ/g=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtOu1ekWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+EDUj6U+w==</latexit>[Pi]
<latexit sha1_base64="04Hd84/XZ1Ucnoy1lracOFTzCn0=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64="04Hd84/XZ1Ucnoy1lracOFTzCn0=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64="04Hd84/XZ1Ucnoy1lracOFTzCn0=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit><latexit sha1_base64="04Hd84/XZ1Ucnoy1lracOFTzCn0=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtMu73SLJats6WXOAzsDJWSrGhVf0EYPETykCMAQQhL24SChpwUbFmLiOpgQJwhxHWeYokDalLIYZTjEjug7oF0rY0PaK89Eqz06xadXkNLEAWkiyhOE1WmmjqfaWbG/eU+0p7rbmP5u5hUQKzEk9i/dLPO/OlWLRB+nugZONcWaUdV5mUuqu6Jubn6pSpJDTJzCPYoLwp5Wzvpsak2ia1e9dXT8TWcqVu29LDfFu7olDdj+Oc55UD8q21bZvjwuVc6yUeexh30c0jxPUMEFqqiRt8AjnvBsXBlj4864/0w1cplmF9+W8fAB2deVNA==</latexit>[Pi+1]
<latexit sha1_base64="T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64="T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64="T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit><latexit sha1_base64="T4HkTbSj3PFQ1GfAlSrlXjyhL+Y=">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIsgCCURQZdFNy4r2IfUUpI41cG8mEzEUoo7cesPuNU/Ev9A/8I7YwpqEZ2Q5My595yZe6+XBDyVtv1aMKamZ2bnivOlhcWl5RVztdxM40z4rOHHQSzanpuygEesIbkMWDsRzA29gLW8q0MVb10zkfI4OpGDhHVD9yLife67kqieWe6cha68FOGwPuoN+bYz6vbMil219bImgZODCvJVj80XnOEcMXxkCMEQQRIO4CKlpwMHNhLiuhgSJwhxHWcYoUTajLIYZbjEXtH3gnadnI1orzxTrfbplIBeQUoLm6SJKU8QVqdZOp5pZ8X+5j3UnupuA/p7uVdIrMQlsX/pxpn/1alaJPrY1zVwqinRjKrOz10y3RV1c+tLVZIcEuIUPqe4IOxr5bjPltakunbVW1fH33SmYtXez3MzvKtb0oCdn+OcBM2dqmNXnePdSu0gH3UR69jAFs1zDzUcoY4Ged/gEU94Nk6NW+POuP9MNQq5Zg3flvHwAURqluE=</latexit>[Pm]
<latexit sha1_base64="tjiKpTOvdf5v8j22gLqr1mbMF+o=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64="tjiKpTOvdf5v8j22gLqr1mbMF+o=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64="tjiKpTOvdf5v8j22gLqr1mbMF+o=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit><latexit sha1_base64="tjiKpTOvdf5v8j22gLqr1mbMF+o=">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl047KifUBbS5JO26F5MZkIpRTErT/gVn9K/AP9C++MKahFdEKSM+fec2buvW7s80Ra1mvOWFhcWl7JrxbW1jc2t4rbO/UkSoXHal7kR6LpOgnzechqkkufNWPBnMD1WcMdnat445aJhEfhtRzHrBM4g5D3uedIom5a7cCRQxFMqtNu0OkWS1bZ0sucB3YGSshWNSq+oI0eInhIEYAhhCTsw0FCTws2LMTEdTAhThDiOs4wRYG0KWUxynCIHdF3QLtWxoa0V56JVnt0ik+vIKWJA9JElCcIq9NMHU+1s2J/855oT3W3Mf3dzCsgVmJI7F+6WeZ/daoWiT5OdQ2caoo1o6rzMpdUd0Xd3PxSlSSHmDiFexQXhD2tnPXZ1JpE16566+j4m85UrNp7WW6Kd3VLGrD9c5zzoH5Utq2yfXlcqpxlo85jD/s4pHmeoIILVFEjb4FHPOHZuDLGxp1x/5lq5DLNLr4t4+ED41uVOA==</latexit>……h0
<latexit sha1_base64="XIhpn8OxNUvhZRM6e2EnUbWV7tU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64="XIhpn8OxNUvhZRM6e2EnUbWV7tU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64="XIhpn8OxNUvhZRM6e2EnUbWV7tU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit><latexit sha1_base64="XIhpn8OxNUvhZRM6e2EnUbWV7tU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1HP6ZUrTtXRy54HrgEVmNVIyi+4QR8JAuSIwBBDEg7hIaOnAxcOUuK6mBInCHEdZ7hHibQ5ZTHK8Igd03dIu45hY9orz0yrAzolpFeQ0sYBaRLKE4TVabaO59pZsb95T7WnutuE/r7xioiVGBH7l26W+V+dqkVigFNdA6eaUs2o6gLjkuuuqJvbX6qS5JASp3Cf4oJwoJWzPttak+naVW89HX/TmYpV+8Dk5nhXt6QBuz/HOQ9aR1XXqboXx5XamRl1EXvYxyHN8wQ11NFAk7yHeMQTnq26FVu5dfeZahWMZhfflvXwAeV8kBA=</latexit>hi
<latexit sha1_base64="nWnmfxj2c+WFY4VamC6wC03+Ux4=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64="nWnmfxj2c+WFY4VamC6wC03+Ux4=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64="nWnmfxj2c+WFY4VamC6wC03+Ux4=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit><latexit sha1_base64="nWnmfxj2c+WFY4VamC6wC03+Ux4=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lCSdtkPzYjJRShH8Abf6aeIf6F94Z5yCWkQnJDlz7j1n5t7rpyHPpOO8FqyFxaXlleJqaW19Y3OrvL3TypJcBKwZJGEirn0vYyGPWVNyGbLrVDAv8kPW9sfnKt6+ZSLjSXwlJynrRt4w5gMeeJKoy1GP98oVp+roZc8D14AKzGok5RfcoI8EAXJEYIghCYfwkNHTgQsHKXFdTIkThLiOM9yjRNqcshhleMSO6TukXcewMe2VZ6bVAZ0S0itIaeOANAnlCcLqNFvHc+2s2N+8p9pT3W1Cf994RcRKjIj9SzfL/K9O1SIxwKmugVNNqWZUdYFxyXVX1M3tL1VJckiJU7hPcUE40MpZn22tyXTtqreejr/pTMWqfWByc7yrW9KA3Z/jnAeto6rrVN2L40rtzIy6iD3s45DmeYIa6migSd5DPOIJz1bdiq3cuvtMtQpGs4tvy3r4AGzrkEk=</latexit>…hi+1
<latexit sha1_base64="uv7sHjM75Zjq7Y01g85optkEEAs=">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64="uv7sHjM75Zjq7Y01g85optkEEAs=">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64="uv7sHjM75Zjq7Y01g85optkEEAs=">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit><latexit sha1_base64="uv7sHjM75Zjq7Y01g85optkEEAs=">AAACynicjVHLSsNAFD2Nr1pfVZdugkUQhJKIoMuiGxcuKtgH1FKS6bQdTJMwmQgldOcPuNUPE/9A/8I7YwpqEZ2Q5My559yZe68fByJRjvNasBYWl5ZXiqultfWNza3y9k4ziVLJeINFQSTbvpfwQIS8oYQKeDuW3Bv7AW/5dxc63rrnMhFReKMmMe+OvWEoBoJ5iqjWqJeJI3faK1ecqmOWPQ/cHFSQr3pUfsEt+ojAkGIMjhCKcAAPCT0duHAQE9dFRpwkJEycY4oSeVNScVJ4xN7Rd0i7Ts6GtNc5E+NmdEpArySnjQPyRKSThPVptomnJrNmf8udmZz6bhP6+3muMbEKI2L/8s2U//XpWhQGODM1CKopNoyujuVZUtMVfXP7S1WKMsTEadynuCTMjHPWZ9t4ElO77q1n4m9GqVm9Z7k2xbu+JQ3Y/TnOedA8rrpO1b0+qdTO81EXsYd9HNI8T1HDJepomCof8YRn68qS1sTKPqVWIffs4tuyHj4AMo2RxQ==</latexit>hm
<latexit sha1_base64="MV7vt71uo3HwFtGIdDyhAdpOcgw=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64="MV7vt71uo3HwFtGIdDyhAdpOcgw=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64="MV7vt71uo3HwFtGIdDyhAdpOcgw=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit><latexit sha1_base64="MV7vt71uo3HwFtGIdDyhAdpOcgw=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl002VF2wq1lGQ6bYfmxWSilCL4A27108Q/0L/wzpiCWkQnJDlz7j1n5t7rJ4FIleO8FqyFxaXlleJqaW19Y3OrvL3TSuNMMt5kcRDLa99LeSAi3lRCBfw6kdwL/YC3/fG5jrdvuUxFHF2pScK7oTeMxEAwTxF1OeqFvXLFqTpm2fPAzUEF+WrE5RfcoI8YDBlCcERQhAN4SOnpwIWDhLgupsRJQsLEOe5RIm1GWZwyPGLH9B3SrpOzEe21Z2rUjE4J6JWktHFAmpjyJGF9mm3imXHW7G/eU+Op7zahv597hcQqjIj9SzfL/K9O16IwwKmpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hPcUmYGeWsz7bRpKZ23VvPxN9Mpmb1nuW5Gd71LWnA7s9xzoPWUdV1qu7FcaV2lo+6iD3s45DmeYIa6migSd5DPOIJz1bdiqzMuvtMtQq5ZhfflvXwAXZrkE0=</latexit>…Pseudo PromptsInput embeddingBack PropagationPre-trained Language Model(GPT, BERT, …Prompt GeneratorInput embeddingDiscrete rewardsBritain[MASK]Thecapitalofise(Britain)
<latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit>e(The)
<latexit sha1_base64="X/YIDIxg/Hse8WN1EcNWk2xxhY8=">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64="X/YIDIxg/Hse8WN1EcNWk2xxhY8=">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64="X/YIDIxg/Hse8WN1EcNWk2xxhY8=">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit><latexit sha1_base64="X/YIDIxg/Hse8WN1EcNWk2xxhY8=">AAAC23icjVHLSsNAFD2Nr1pfUcGNm2AR6qYkIuiy6MZlhb6gLSVJp20wL5KJWGJX7sStP+BW/0f8A/0L74wpqEV0QpIz595zZu69Vug6Mdf115wyN7+wuJRfLqysrq1vqJtbjThIIpvV7cANopZlxsx1fFbnDndZK4yY6Vkua1qXZyLevGJR7AR+jY9D1vXMoe8MHNvkRPXUnY5n8pE1SNmk1OHsmqe1EZsc9NSiXtbl0maBkYEislUN1Bd00EcAGwk8MPjghF2YiOlpw4COkLguUuIiQo6MM0xQIG1CWYwyTGIv6TukXTtjfdoLz1iqbTrFpTcipYZ90gSUFxEWp2kynkhnwf7mnUpPcbcx/a3MyyOWY0TsX7pp5n91ohaOAU5kDQ7VFEpGVGdnLonsiri59qUqTg4hcQL3KR4RtqVy2mdNamJZu+itKeNvMlOwYm9nuQnexS1pwMbPcc6CxmHZ0MvGxVGxcpqNOo9d7KFE8zxGBeeook7eN3jEE56VrnKr3Cn3n6lKLtNs49tSHj4ACkCYvg==</latexit>e(capital)
<latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit>e(of)
<latexit sha1_base64="flebG6e5LB7rimfipQ3vJzIFXBg=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64="flebG6e5LB7rimfipQ3vJzIFXBg=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64="flebG6e5LB7rimfipQ3vJzIFXBg=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit><latexit sha1_base64="flebG6e5LB7rimfipQ3vJzIFXBg=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gKSVJJ20wL5KJWEI37sStP+BWP0j8A/0L74wpqEV0QpIz595zZu69VuS5Cde014IyN7+wuFRcLq2srq1vlDe3WkmYxjZr2qEXxh3LTJjnBqzJXe6xThQz07c81rauzkS8fc3ixA2DSz6OWM83h4HruLbJieqXdwzf5CPLydikanB2w7PQmRz0yxWtpsmlzgI9BxXkqxGWX2BggBA2UvhgCMAJezCR0NOFDg0RcT1kxMWEXBlnmKBE2pSyGGWYxF7Rd0i7bs4GtBeeiVTbdIpHb0xKFfukCSkvJixOU2U8lc6C/c07k57ibmP6W7mXTyzHiNi/dNPM/+pELRwOTmQNLtUUSUZUZ+cuqeyKuLn6pSpODhFxAg8oHhO2pXLaZ1VqElm76K0p428yU7Bib+e5Kd7FLWnA+s9xzoLWYU3XavrFUaV+mo+6iF3soUrzPEYd52igSd4ZHvGEZ8VQbpU75f4zVSnkmm18W8rDByeCmGg=</latexit>e(is)
<latexit sha1_base64="u3KTSyivQRk08+u91kKYRwDoHq0=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64="u3KTSyivQRk08+u91kKYRwDoHq0=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64="u3KTSyivQRk08+u91kKYRwDoHq0=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit><latexit sha1_base64="u3KTSyivQRk08+u91kKYRwDoHq0=">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEuimJCLosunFZwT6gLSVJp+1gXiQTsYRu3Ilbf8CtfpD4B/oX3hlTUIvohCRnzr3nzNx77dDlsTCM15w2N7+wuJRfLqysrq1vFDe3GnGQRA6rO4EbRC3bipnLfVYXXLisFUbM8myXNe2rMxlvXrMo5oF/KcYh63rW0OcD7liCqF5xp+NZYmQPUjYpdwS7ESmPJwe9YsmoGGrps8DMQAnZqgXFF3TQRwAHCTww+BCEXViI6WnDhIGQuC5S4iJCXMUZJiiQNqEsRhkWsVf0HdKunbE+7aVnrNQOneLSG5FSxz5pAsqLCMvTdBVPlLNkf/NOlae825j+dublESswIvYv3TTzvzpZi8AAJ6oGTjWFipHVOZlLoroib65/qUqQQ0icxH2KR4QdpZz2WVeaWNUue2up+JvKlKzcO1lugnd5Sxqw+XOcs6BxWDGNinlxVKqeZqPOYxd7KNM8j1HFOWqok3eKRzzhWetot9qddv+ZquUyzTa+Le3hAzgqmG8=</latexit>e([MASK])
<latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit>Britain[MASK]e(Britain)
<latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit><latexit sha1_base64="apa0nk0gEVHoA/TQZedTMI5FqmY=">AAAC33icjVHLSsNAFD3G97vqTjfBItRNSUTQpejGpYJthbaUSTptB/NiMhFLKbhzJ279Abf6N+If6F94Z4ygFtEJSc6ce8+Zufd6SSBS5TgvY9b4xOTU9Mzs3PzC4tJyYWW1msaZ9HnFj4NYnnss5YGIeEUJFfDzRHIWegGveRdHOl675DIVcXSm+glvhqwbiY7wmSKqVVhvhEz1vM6AD0sNxa/U4FAKxUQ03G4Vik7ZMcseBW4OisjXSVx4RgNtxPCRIQRHBEU4AENKTx0uHCTENTEgThISJs4xxBxpM8rilMGIvaBvl3b1nI1orz1To/bplIBeSUobW6SJKU8S1qfZJp4ZZ83+5j0wnvpuffp7uVdIrEKP2L90n5n/1elaFDrYNzUIqikxjK7Oz10y0xV9c/tLVYocEuI0blNcEvaN8rPPttGkpnbdW2biryZTs3rv57kZ3vQtacDuz3GOgupO2XXK7ulu8eAwH/UMNrCJEs1zDwc4xgkq5H2NBzziyWLWjXVr3X2kWmO5Zg3flnX/DreZmo4=</latexit>e([MASK])
<latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit><latexit sha1_base64="lmVmOIHtBwXAqq1jPqBsI6SCI4s=">AAAC3nicjVHLSsNAFD2Nr1pfVVfiJliEuimJCLqsuhFEqGgf0JaapNMamhfJRCyluHMnbv0Bt/o54h/oX3hnTEEtohOSnDn3njNz7zUDx464pr2mlInJqemZ9Gxmbn5hcSm7vFKJ/Di0WNnyHT+smUbEHNtjZW5zh9WCkBmu6bCq2TsU8eoVCyPb9855P2BN1+h6dse2DE5UK7vWcA1+aXYGbJhvcHbNB/WT/bPj5nCrlc1pBU0udRzoCcghWSU/+4IG2vBhIYYLBg+csAMDET116NAQENfEgLiQkC3jDENkSBtTFqMMg9gefbu0qyesR3vhGUm1Rac49IakVLFJGp/yQsLiNFXGY+ks2N+8B9JT3K1PfzPxconluCT2L90o8786UQtHB3uyBptqCiQjqrMSl1h2Rdxc/VIVJ4eAOIHbFA8JW1I56rMqNZGsXfTWkPE3mSlYsbeS3Bjv4pY0YP3nOMdBZbugawX9dCdXPEhGncY6NpCnee6iiCOUUCbvGzziCc/KhXKr3Cn3n6lKKtGs4ttSHj4AZZGZnw==</latexit>(a) Discrete Prompt Search(b) P-tuningcapitale(capital)
<latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit><latexit sha1_base64="rwoN1CZfwDAAgs823DmssBjrTSU=">AAAC33icjVHLSsNAFD3GV31X3ekmWATdlEQEXRbduKxgq9CWMhmndTAvkolYSsGdO3HrD7jVvxH/QP/CO2MKahGdkOTMufecmXuvF/syVY7zOmaNT0xOTRdmZufmFxaXissr9TTKEi5qPPKj5MxjqfBlKGpKKl+cxYlggeeLU+/yUMdPr0SSyig8Ub1YtALWDWVHcqaIahfXmgFTF16nLwZbTSWuVZ+zWCrmD7bbxZJTdsyyR4GbgxLyVY2KL2jiHBE4MgQQCKEI+2BI6WnAhYOYuBb6xCWEpIkLDDBL2oyyBGUwYi/p26VdI2dD2mvP1Kg5neLTm5DSxiZpIspLCOvTbBPPjLNmf/PuG099tx79vdwrIFbhgti/dMPM/+p0LQod7JsaJNUUG0ZXx3OXzHRF39z+UpUih5g4jc8pnhDmRjnss200qald95aZ+JvJ1Kze8zw3w7u+JQ3Y/TnOUVDfKbtO2T3eLVUO8lEXsI4NbNE891DBEaqokfcNHvGEZ4tZt9addf+Zao3lmlV8W9bDB+otmqM=</latexit>Figure 2: An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone,
“Britain”) and target (red zone, “[MASK]”), the orange zone refer to the prompt. In (a), the prompt generator only
receives discrete rewards; on the contrary, in (b) the continuous prompt embeddings and prompt encoder can be
optimized in a differentiable way.
prompting. Let [ Pi] be the ithcontinuous prompt
embedding. The prompt template for P-Tuning is
as follows:
T={[P0:i],x,[P(i+1): j],y,[P(j+1): k]}
P-Tuning leverages an extra embedding function
f: [P i]→hito map the template to
{h0, ..., h i,e(x), hi+1, ..., h j,e(y), hj+1, ..., h k}
Finally, we update the embeddings {Pi}k
i=1to op-
timize a task loss function.
It is noteworthy that we can also concatenate
discrete prompts with continuous prompts, which
performs better and is adopted throughout our ex-
periments. P-Tuning is applicable to both frozen
and finetuned language models.
2.3 Prompt Encoder
In the aforementioned framework, we employ a
mapping function fto map trainable embeddings
{Pi}to model inputs {hi}. The intuition is that
by using a mapping function, it is more conve-
nient to model the dependency between different
prompt embeddings, compared to using indepen-
dent learnable embeddings. In our implementation,
we use a lightweight neural network to formulate
the function f. Specifically, we experiment with
using long short-term memory (LSTM) networks,
multi-layer perceptrons (MLPs), and the identity
mapping function in Section 3.
3 Experiments
We include two NLU benchmarks: LAMA (Petroni
et al., 2019) for knowledge probing (§ 3.1) and Su-
perGLUE (Wang et al., 2019a) for general natural
language understanding. On SuperGLUE, we con-
sider both the fully-supervised learning (§ 3.2) and
few-shot learning (§ 3.3) settings.LAMA Full SG Few SG
frozen tuned tuned
Improved ✓ ✓ ✓
Stabilized ✓ ✗ ✓
Table 2: Task settings and summary of results in our
experiments. P-tuning shows improvement over base-
lines on all task settings, and can stabilize performance
on LAMA and Few SG. For Full SG, the gap between
discrete prompts is not large and training is stable even
without P-Tuning. (Full SG: fully-supervised learn-
ing on SuperGLUE; Few SG: few-shot SuperGLUE;
Improved: overall performance improved; Stabilized:
training stabilized by minimizing difference between
discrete prompts).
On LAMA, following Shin et al. (2020); Jiang
et al. (2020b), language models are frozen and only
the discrete or continious prompts are tuned. For
SuperGLUE, following Schick and Schütze (2020);
Zheng et al. (2021), language models are tuned. In
our setting, we jointly optimize the language model
parameters and the continuous prompts. This setup
not only follows the common, standard settings in
prior work, but also allows evaluating P-Tuning
with both tuned and frozen language models.
The overall task setup and a summary of results
are shown in Table 2.
3.1 Knowledge Probing
3.1.1 Setup
Knowledge probing, or referred to as fact retrieval,
evaluates how much real-world knowledge has
language models gained from pre-training. The
LAMA (Petroni et al., 2019) dataset evaluates it
with cloze tests created from triples selected in the
knowledge bases.
Datasets and vocabulary. LAMA enforces all

--- PAGE 4 ---
Prompt type Model P@1
Original
(MP)BERT-base 31.1
BERT-large 32.3
E-BERT 36.2
DiscreteLPAQA (BERT-base) 34.1
LPAQA (BERT-large) 39.4
AutoPrompt (BERT-base) 43.3
P-tuningBERT-base 48.3
BERT-large 50.6Model MP P-tuning
BERT-base (109M) 31.7 52.3 (+20.6)
-AutoPrompt (Shin et al., 2020) - 45.2
BERT-large (335M) 33.5 54.6 (+21.1)
RoBERTa-base (125M) 18.4 49.3 (+30.9)
-AutoPrompt (Shin et al., 2020) - 40.0
RoBERTa-large (355M) 22.1 53.5 (+31.4)
GPT2-medium (345M) 20.3 46.5 (+26.2)
GPT2-xl (1.5B) 22.8 54.4 (+31.6)
MegatronLM (11B) 23.1 64.2 (+41.1)
Table 3: Knowledge probing Precision@1 on LAMA-34k (left) and LAMA-29k (right). P-tuning outperforms all
the discrete prompt searching baselines. (MP: Manual prompt; PT: P-tuning).
answers in single-token format. We first adopt
the original LAMA-TREx dataset, consisting of
41 Wikidata relations and altogether 34,039 test-
ing triples (namely LAMA-34k, which covers all
BERT vocabularies). Since different pretrained
models share distinct vocabularies, to allow direct
comparison, we follow previous work (Shin et al.,
2020) to adopt a subset that covers the intersection
of GPT’s and BERT’s vocabularies. This is caled
LAMA-29k. We again follow Shin et al. (2020) to
construct the training, development, and test data
to allow for fair comparison.
Setup. LAMA has provided a handcraft prompt
for each relation, as shown in Table 1, which are
effective but likely sub-optimal. For bidirectional
masked language models, we only need to replace
“[X]” with the subject entity and “[Y]” with the
[MASK] token; for unidirectional language models
such as GPT, following LAMA’s original setting
on Transformer-XL (Dai et al., 2019), we use the
network output just before the target position.
The number of prompt tokens and positions are
selected based on the development sets, and for
simplicity we choose the (3, sub, org_prompt, 3,
obj, 3) template for bidirectional models and (3,
sub, org_prompt, 3, obj) for unidirectional models
as this configuration performs well for most rela-
tions (where the number indicates the number of
continuous prompt tokens). Continuous prompts
are concatenated with original discrete prompts.
During the prompt training, we set the learning rate
to 1e-5 and use the Adam optimizer.
3.1.2 Main results
The results are presented in Table 3. P-tuning sig-
nificantly improves the best results of knowledge
probing from 43.3% to 50.6% on LAMA-34k and
from 45.2% to 64.2% on LAMA-29k. Moreover,P-tuning outperforms previous discrete prompt
searching approaches such as AutoPrompt (Shin
et al., 2020) and LPAQA (Jiang et al., 2020b) on
the same-size models. This confirms our intuition
in Section 2 that discrete prompts might not be
optimal.
3.2 Fully-supervised Learning
3.2.1 Setup
Dataset. To evaluate P-tuning on fully-supervised
learning tasks, we adopt the SuperGLUE bench-
mark (Wang et al., 2019b), consisting of 8 challeng-
ing natural language understanding (NLU) tasks.
We focus on 7 of them since the ReCoRD (Zhang
et al., 2018) task adopts no discrete prompts, thus
P-tuning is not directly applicable. The tasks in-
clude question answering (BoolQ (Clark et al.,
2019a) & MultiRC (Khashabi et al., 2018)), tex-
tual entailment (CB (De Marneffe et al., 2019) &
RTE (Dagan et al., 2005)), co-reference resolution
(WiC (Pilehvar and Camacho-Collados, 2018)),
causal reasoning (COPA (Roemmele et al., 2011)),
and word sense disambiguation (WSC (Levesque
et al., 2012)).
Comparison methods. We experiment with P-
tuning on both unidirectional and bidirectional
pretrained models, i.e., GPT and BERT. We
include four variants BERT-Base, BERT-Large,
GPT2-Base, and GPT-medium. For each model,
we compare standard classification finetuning,
PET (Schick and Schütze, 2020) (a typical fine-
tuning method based on manual discrete prompts)
and our P-tuning.
Configuration. We use the same metrics as
in (Wang et al., 2019b). For fully-supervised learn-
ing, we use a large training set to finetune pre-
trained models and use a development set for hyper-

--- PAGE 5 ---
(a) Fully-supervised performance with base-scale models.
MethodBoolQ CB WiC RTE MultiRC WSC COPAAvg.(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)
BERT-Base
(109M)CLS-FT 72.9 85.1 73.9 71.1 68.4 16.2 66.3 63.5 67.0 66.2
PET-FT 73.7 87.5 90.8 67.9 70.4 13.7 62.5 60.6 70.0 67.1
P-tuning 73.9 89.2 92.1 68.8 71.1 14.8 63.3 63.5 72.0 68.4
GPT2-Base
(117M)CLS-FT 71.2 78.6 55.8 65.5 67.8 17.4 65.8 63.0 64.4 63.0
PET-FT 74.8 87.5 88.1 68.0 70.0 23.5 69.7 66.3 78.0 70.2
P-tuning 75.0 91.1 93.2 68.3 70.8 23.5 69.8 63.5 76.0 70.4
(b) Fully-supervised performance with large-scale models.
MethodBoolQ CB WiC RTE MultiRC WSC COPAAvg.(Acc.) (Acc.) (F1) (Acc.) (Acc.) (EM) (F1a) (Acc.) (Acc.)
BERT-Large
(335M)CLS-FT177.7 94.6 93.7 74.9 75.8 24.7 70.5 68.3 69.0 72.5
PET-FT 77.2 91.1 93.5 70.5 73.6 17.7 67.0 80.8 75.0 73.1
P-tuning 77.8 96.4 97.4 72.7 75.5 17.1 65.6 81.7 76.0 74.6
GPT2-Med.
(345M)CLS-FT 71.0 73.2 51.2 65.2 72.2 19.2 65.8 62.5 66.0 63.1
PET-FT 78.3 96.4 97.4 70.4 72.6 32.1 74.4 73.0 80.0 74.9
P-tuning 78.9 98.2 98.7 69.4 75.5 29.3 74.2 74.0 81.0 75.6
1We report the same results taken from SuperGLUE (Wang et al., 2019a).
Table 4: Fully-supervised performance on SuperGLUE development set.
parameter and model selection. Specifically, the
AdamW optimizer with a linearly decayed learn-
ing rate is used for training. We use a learning
rate of {1e−5,2e−5,3e−5}, a batch size of
{16,32}, and a warm-up ratio of {0.0,0.05,0.1}.
For small datasets (i.e., COPA, WSC, CB, RTE),
we fine-tune pretrained models for 20 epochs. For
larger datasets (i.e., WiC, BoolQ, MultiRC), we
reduce the number of training epochs to be 10 as
the model converges earlier. Early stopping is used
to avoid over-fitting the training data.
3.2.2 Main Results
The main results of fully-supervised learning are
shown in Table 4. We observe that P-tuning can
improve fully-supervised learning performance on
both BERTs and GPTs. (1) Specifically, on the
BERT-Base model, P-tuning achieves best perfor-
mance on 5/7 tasks, while with BERT-Large, P-
tuning outperforms other methods on 4/7 tasks.
The exceptions are WiC and MultiRC, both of
which have relatively large training sets. We find
that P-tuning might not have large gains over CLS-
FT on such high-resource tasks, while benefits
more on low-resource tasks. On average, P-tuning
improves over the considered baselines. (2) On
GPT2-Base and GPT2-Medium models, P-tuning
consistently achieves the best performance on all
tasks.3.3 Few-Shot Learning
While GPT-3 has shown decent few-shot learning
potential with handcrafted prompts, it still struggles
on some of the challenging tasks (e.g., natural lan-
guage inference) (Brown et al., 2020). We are mo-
tivated to study whether P-tuning can also improve
the few-shot learning performance of pretrained
models on challenging tasks.
3.3.1 Setup
Few-shot Evaluation. The few-shot performance
is sensitive to lots of factors (e.g., the order of train-
ing examples, random seed, and prompt patterns),
and thus suffers from high variance (Zhao et al.,
2021a; Lu et al., 2021; Zhang et al., 2020). There-
fore, the few-shot evaluation strategy should make
sure that the improvements are indeed from an im-
proved method instead of variance. To this end, we
follow the FewNLU evaluation procedure (Zheng
et al., 2021) that has addressed and handled the
issue. Specifically, we use random data splits to
perform model selection only on a small labeled
set to prevent overfitting a large dev set.
Dataset. We use the few-shot SuperGLUE (also
known as FewGLUE) benchmark (Schick and
Schütze, 2020) and follow the setting in prior work
(Zheng et al., 2021) in terms of data split construc-
tion.
Baseline and Hyper-parameter. In few-shot learn-

--- PAGE 6 ---
ing, we again compare P-tuning with PET (Schick
and Schütze, 2020), which was shown to out-
perform GPT-3 on some of the tasks. Similar
to (Schick and Schütze, 2020), we use ALBERT-
xxLarge as the base model. For hyper-parameters
that are shared by PET and P-tuning (e.g., learn-
ing rate, maximum training step, evaluation fre-
quency), we use the same search space for fair
comparison. Specifically, we search the learning
rate in {1e−5,2e−5}, the maximum training
step in {250,500}, and the evaluation frequency in
{0.02,0.04}.
Construction of Prompt Patterns. For PET, we
use the same manual prompts reported by Schick
and Schütze (2020). When constructing prompt
patterns for P-tuning, based on the same manual
prompts as PET, we insert different numbers of
continuous prompt tokens into different positions,
thus formulating a number of pattern candidates.
We then select the best pattern for P-tuning using
the validation strategy of FewNLU (Zheng et al.,
2021). We also conduct further analysis of the num-
ber and the position of continuous prompt tokens
in §3.3.3.
3.3.2 Main Results
Few-Shot Performance. Table 5 shows the main
results of few-shot learning. We find that, on AL-
BERT, P-tuning consistently outperform PET on
average by more than 1 points. It outperforms
PromptTuning by more than 13 points. It proves
that by automatically learning continuous prompt
tokens, the pretrained models can achieve better
few-shot performance on NLU tasks.
3.3.3 Ablation Study
Type of Prompt Encoder Prior work (Shin et al.,
2020) proposes to simply use an MLP as the prompt
encoder, we perform further ablation analysis for
prompt encoder selection, and results are shown
in Table 8. We consider LSTM, MLP, and EMB
(i.e., we directly optimize the word embeddings
without using additional parameters). From the
results, we can see that LSTM, MLP, and EMB
all work as a prompt encoder. Results show that
both LSTM and MLP generally work well on these
tasks, while EMB is unstable and can substantially
under-perform the other two on some tasks (e.g,.
WiC and CB). To sum up, both LSTM and MLP
could be taken into account when working on new
tasks.Location of Prompt Tokens To study at which
location to insert continuous prompt tokens, we
perform experiments as Table 7 shows. From the
results, we have the following findings.
1.By comparing #1 (or #2) with #3 (or #4), we find
that it would be better if we insert continuous
prompt tokens at the location where it does not
segment the sentences. For example, in case#1,
“[P]” breaks the completeness of sentence “[Hy-
pothesis]?” while in case#3, “[P]” is located
between sentences.
2.By comparing #2 (or #3) with #4, we find that
there’s no special preference for placing on the
edge or in the middle of the inputs.
3. It is suggested to write a number of pattern can-
didates and then search over them for the best
for each task.
Number of Prompt Tokens We also study the in-
fluence of the number of prompt tokens and show
the results in Table 7. By comparing #3, #6, #7,
and #8, we can conclude that the number of prompt
tokens has a great impact on the few-shot perfor-
mance. However, it is not that a larger number of
prompt tokens would always be better. We conjec-
ture that it could be that due to the limited training
data, it becomes difficult to learn the parameters
when excessively increasing the number of contin-
uous prompt tokens. In practice, it is suggested
to search for the best number of prompt tokens
through model selection.
3.3.4 Comparison with Discrete Prompt
Search
Prior work (Gao et al., 2020) proposed to automati-
cally search discrete prompts and achieved better
results than those of manual prompts. We now
proceed to compare P-Tuning with auto-searched
discrete prompts. For fair comparison, we follow
the setting of LM-BFF (Gao et al., 2020) to also
conduct experiments on some of the GLUE tasks
(Wang et al., 2018) with RoBERTa-Large model
(Liu et al., 2019). Since the the evaluation proto-
cols have large impacts on few-shot performance,
we use the top-3 discrete prompts searched by LM-
BFF and experiment with using only the discrete
prompts and additionally applying P-Tuning. For
P-Tuning, the prompt patterns are constructed by
concatenating the same discrete prompts as well as
continuous prompts. Results in Table 9 show that
additionally incorporating continuous prompts can
further improve few-shot performance. P-Tuning is

--- PAGE 7 ---
MethodBoolQ
(Acc.)RTE
(Acc.)WiC
(Acc.)CB
(Acc.) (F1.)MultiRC
(F1a.) (EM.)WSC
(Acc.)COPA
(Acc.)Avg
Prompt Tuning 58.47 ±1.00 54.42 ±3.05 52.74 ±2.36 75.45 ±2.25 67.73 ±5.70 59.28 ±4.73 15.03 ±4.11 74.04 ±2.99 61.50 ±4.36 58.56
PET-FT 76.70 ±1.85 72.83 ±1.30 53.87 ±4.47 84.38 ±4.47 62.56 ±7.66 76.51 ±1.52 36.46 ±2.13 80.05 ±2.53 81.75 ±4.03 70.74
P-tuning 76.55 ±2.68 63.27 ±3.63 55.49 ±1.21 88.39 ±3.72 84.24 ±5.15 75.91 ±1.74 38.01 ±0.78 78.85 ±1.76 85.25 ±3.30 71.81
Table 5: The few-shot performance of PET (Schick and Schütze, 2020), Prompt Tuning (Lester et al., 2021) and our
P-tuning over seven tasks based on ALBERT. Each result is averaged over 4 runs with different data splits. Results
show that P-tuning consistently improves average few-shot performance by more than 1 point compared to PET and
by more than 13 points compared to Prompt Tuning.
Method P#0 P#1 P#2 P#3 P#4 P#5 STD
FSL
(BoolQ)PET-FT77.10 67.96 74.14 72.48 71.77 60.865.68±2.21 ±2.69 ±1.38 ±4.31 ±2.56 ±3.99
P-tuning75.41 75.11 73.43 71.35 71.31 65.863.52±3.09 ±1.61 ±2.60 ±4.57 ±8.58 ±3.80
LAMA
(P17)MP 31.3 19.8 31.4 51.1 34.0 32.7 10.1
P-tuning 57.8 57.8 58.1 58.1 58.9 58.7 0.46
Table 6: Upper table: Few-shot learning (FSL) of PET and P-tuning in terms of each pattern on SuperGLUE with
ALBERT; Lower table: Manual prompt (MP) and P-tuning performance on LAMA-P17 with BERT-base-cased.
For each column, P-tuning and compared methods share the same manual prompts, while P-tuning additionally
concatenates continuous prompt tokens. We report the standard deviation over multiple results of different patterns.
Results show that P-tuning achieves smaller standard deviation, proving that P-tuning can improve stability w.r.t.
the choice of discrete patterns.
easy to be combined with existing discrete prompts,
while further improving stability as discussed in
Section 3.4.
3.4 Stabilizing Language Model Adaptation
In the above sections, we have shown that P-Tuning
improves over performance across multiple set-
tings. Now we present results to demonstrate that
P-Tuning also stabilizes language model adapta-
tion; i.e., reducing the differences between differ-
ent prompts. As we have shown in Table 1, manual
prompts have a large impact on the performance.
When it comes to few-shot learning, the perfor-
mance gap of different prompts is prominent due
to the sensitivity of few-shot learning (Zheng et al.,
2021). Results in Table 6 show that P-tuning im-
proves the performance of the worst-performing
patterns (e.g., P#5), and achieves a smaller stan-
dard deviation over multiple patterns. Compared to
PET-FT, P-tuning increases the stability w.r.t. the
choice of patterns.
On LAMA, we observe similar a phenomenon
that while manual prompts often yield quite volatile
results, appending trainable continuous prompts on
top of the manual prompts can stabilize their per-
formances, reducing the standard deviation from
10.1 to 0.46.4 Related work
Language Model Prompting. GPT-3 (Brown
et al., 2020) uses in-context examples (Liu et al.,
2021; Zhao et al., 2021b) as a way of prompting to
transfer knowledge from pretraining to downstream
tasks. Schick and Schütze (2020) proposed to use
cloze patterns, which removes the constraint that
the masked token is the last token of the sentence.
This further minimizes the gap between pretrain-
ing and downstream tasks. To improve prompting
for NLU, recent works have proposed methods to
automatically search for high-performing prompts
by mining the training corpus (Jiang et al., 2020b),
gradient-based search (Shin et al., 2020), or using
pretrained generative models (Gao et al., 2020).
Our approach is different from these prior works
in that we resort to using continuous prompt em-
beddings, which are found to be complementary to
discrete prompts in our experiments.
Recently, some concurrent works also proposed
the use of continuous prompts. Prefix-tuning (Li
and Liang, 2021) adds continuous prompts at the
beginning of the sequence for each layer. In con-
trast to our work, prefix-tuning targets natural lan-
guage generation tasks.
In the area of NLU, a few concurrent methods
were proposed based on continuous prompts, fo-

--- PAGE 8 ---
ID Prompt Patterns of P-tuning Seg. Pos. #[P] Acc. F1. Avg.
1 [Premise] Question: [Hypothesis] [P] ? Answer: [M]. Yes Mid 1 87.95 76.70 82.33
2 [Premise] Question [P]: [Hypothesis] ? Answer: [M]. Yes Mid 1 88.39 78.57 83.48
3 [Premise] Question: [Hypothesis] ? [P] Answer: [M]. No Mid 1 89.29 79.86 84.58
4 [Premise] [P] Question: [Hypothesis] ? Answer: [M]. No Miid 1 89.73 82.15 85.94
5 [Premise] Question: [Hypothesis] ? Answer: [M]. [P] No Edge 1 87.50 83.39 85.45
6 [Premise] Question: [Hypothesis] ? [P][P] Answer: [M]. No Mid 2 88.39 84.74 86.57
7 [Premise] Question: [Hypothesis] ? [P][P][P][P] Answer: [M]. No Mid 4 88.39 85.14 86.76
8 [Premise] Question: [Hypothesis] ? [P][P][P][P][P][P][P][P] Answer: [M]. No Mid 8 83.48 73.32 78.40
Table 7: The few-shot performance of P-tuning on the CB task on ALBERT with different prompt patterns. “Seg.”
means whether the inserted prompt tokens segment complete sentences. “Pos.” indicates inserting the prompt tokens
at the edge or in the middle of the inputs. “[P]” is continuous prompt token. “[M]” is the mask token.
Task LSTM MLP EMB
WiC-ACC 56.27±1.54 55.25±3.09 53.96±3.23
CB-ACC. 81.70±7.49 88.39±3.72 82.59±3.69
CB-F1. 77.41±9.15 84.24±5.15 67.27±6.78
BoolQ-ACC. 75.41±3.09 76.46±2.84 76.87±1.69
Table 8: The few-shot performance on WiC, CB and
BoolQ tasks with ALBERT using different prompt en-
coders. Results show that both LSTM and MLP gener-
ally work well on these tasks, while EMB is unstable
and can substantially under-perform the other two on
some tasks (e.g,. WiC and CB). “EMB” means using an
identity mapping for the prompt encoder.
Task LM-BFF (Auto) P-Tuning
SST-2 92.89 92.78
MNLI 57.53 58.70
MRPC 68.26 69.49
Table 9: Few-shot performance of automatically
searched prompts and P-Tuning. We evaluated LM-
BFF (Auto) using the reported top-3 searched patterns
under our evaluation procedure. P-Tuning also uses
the same discrete prompts, in concatenation with con-
tinuous prompts. Results show that P-Tuning can be
effectively combined with existing discrete patterns and
achieve further performance improvement.
cusing on improving knowledge probing (Qin and
Eisner, 2021; Zhong et al., 2021). Lester et al.
(2021) showed that with large pretrained models,
only tuning continuous prompts with a frozen lan-
guage model achieves comparable performance to
full-model tuning.
Compared to these concurrent works on NLU,
P-Tuning reaches a unique conclusion that contin-
uous prompts improve performance and stabilize
training with either frozen or tuned models under
both the few-shot and fully-supervised settings. For
example, no concurrent works have shown thatcontinuous prompts can improve performance with
a tuned language model. Technically, P-Tuning
also has a few unique designs such as using hy-
brid continuous-discrete prompts and employing a
prompt encoder.
Knowledge in Language Models. Self-
supervised (Liu et al., 2020) pre-trained language
models (Han et al., 2021) including GPT (Rad-
ford et al., 2019), BERT (Devlin et al., 2018), XL-
Net (Yang et al., 2019), RoBERTa (Liu et al., 2019)
have been observed to learn not only contextual-
ized text representations but also linguistic and
world knowledge. (Hewitt and Manning, 2019)
demonstrates that contextualized representations
produced by language models can form a parse tree
in the embedding space. (Vig, 2019; Clark et al.,
2019b) look into the multi-head attention patterns
within transformers and discover that certain atten-
tion heads may correspond to some grammatical
functions, including co-reference and noun modi-
fiers. LAMA (Petroni et al., 2019, 2020) propose
the LAMA task that leverages cloze tests to pre-
dict the fact triples of knowledge bases to examine
language model’s ability of memorizing facts with
answers in the single-token format. In (Wang et al.,
2020), the authors investigate the attention matrices
to find evidence about knowledge triples contained
in the context. (Jiang et al., 2020a) develops a
multi-token fact retrieval dataset based on LAMA.
5 Conclusions
In this paper, we present a method P-Tuning that
uses continuous prompts in concatenation with dis-
crete prompts. P-Tuning improves performance
and stabilizes training for pretrained language
model adaptation. P-Tuning is effective with both
tuned and frozen language models under both the
few-shot and fully-supervised setings.

--- PAGE 9 ---
References
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019a. Boolq: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 2924–2936.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D Manning. 2019b. What does bert look
at? an analysis of bert’s attention. arXiv preprint
arXiv:1906.04341 .
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop ,
pages 177–190. Springer.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860 .
Marie-Catherine De Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Inproceedings of Sinn und Bedeutung , volume 23,
pages 107–124.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.arXiv preprint arXiv:1810.04805 .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723 .
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao
Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao
Han, Minlie Huang, et al. 2021. Pre-trained models:
Past, present and future. AI Open .
John Hewitt and Christopher D. Manning. 2019. A
structural probe for finding syntax in word representa-
tions. In North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL) . Association for Computa-
tional Linguistics.
Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki,
Haibo Ding, and Graham Neubig. 2020a. X-factr:
Multilingual factual knowledge retrieval from pre-
trained language models. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 5943–5959.Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham
Neubig. 2020b. How can we know what language
models know? Transactions of the Association for
Computational Linguistics , 8:423–438.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking
beyond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers) , pages 252–262.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. ArXiv , abs/2104.08691.
Hector Levesque, Ernest Davis, and Leora Morgenstern.
2012. The winograd schema challenge. In Thir-
teenth International Conference on the Principles of
Knowledge Representation and Reasoning . Citeseer.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021. What
makes good in-context examples for gpt- 3?arXiv
preprint arXiv:2101.06804 .
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang,
Li Mian, Jing Zhang, and Jie Tang. 2020. Self-
supervised learning: Generative or contrastive. arXiv
preprint arXiv:2006.08218 , 1(2).
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2021. Fantastically
ordered prompts and where to find them: Over-
coming few-shot prompt order sensitivity. CoRR ,
abs/2104.08786.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H Miller, and
Sebastian Riedel. 2020. How context affects lan-
guage models’ factual predictions. arXiv preprint
arXiv:2005.04611 .
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
ton Bakhtin, Yuxiang Wu, Alexander H Miller, and
Sebastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066 .
Mohammad Taher Pilehvar and José Camacho-Collados.
2018. Wic: 10, 000 example pairs for eval-
uating context-sensitive representations. CoRR ,
abs/1808.09121.

--- PAGE 10 ---
Guanghui Qin and J. Eisner. 2021. Learning how to ask:
Querying lms with mixtures of soft prompts. ArXiv ,
abs/2104.06599.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning , pages 90–95.
Timo Schick and Hinrich Schütze. 2020. It’s not just
size that matters: Small language models are also
few-shot learners. Computing Research Repository ,
arXiv:2009.07118.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV ,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with
automatically generated prompts. arXiv preprint
arXiv:2010.15980 .
Jesse Vig. 2019. A multiscale visualization of at-
tention in the transformer model. arXiv preprint
arXiv:1906.05714 .
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R Bowman. 2019a. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. arXiv preprint arXiv:1905.00537 .
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019b. SuperGLUE: A
Stickier Benchmark for General-Purpose Language
Understanding Systems. In NeurIPS 2019 , pages
3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2018.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding. ArXiv ,
abs/1804.07461.
Chenguang Wang, Xiao Liu, and Dawn Song. 2020.
Language models are open knowledge graphs. arXiv
preprint arXiv:2010.11967 .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding. arXiv preprint
arXiv:1906.08237 .Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng
Gao, Kevin Duh, and Benjamin Van Durme. 2018.
Record: Bridging the gap between human and ma-
chine commonsense reading comprehension. arXiv
preprint arXiv:1810.12885 .
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-
berger, and Yoav Artzi. 2020. Revisiting few-sample
BERT fine-tuning. CoRR , abs/2006.05987.
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021a. Calibrate before use: Im-
proving few-shot performance of language models.
CoRR , abs/2102.09690.
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021b. Calibrate before use: Improv-
ing few-shot performance of language models. arXiv
preprint arXiv:2102.09690 .
Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian
Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder,
and Zhilin Yang. 2021. Fewnlu: Benchmarking state-
of-the-art methods for few-shot natural language un-
derstanding.
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [mask]: Learning vs. learning to
recall. ArXiv , abs/2104.05240.
