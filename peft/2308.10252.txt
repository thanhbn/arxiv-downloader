# 2308.10252.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2308.10252.pdf
# File size: 1201295 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LMTuner: An user-friendly and highly-integrable Training Framework
for fine-tuning Large Language Models
Yixuan Weng1, Zhiqi Wang1,2, Huanxuan Liao1,2, Shizhu He1,2, Shengping Liu3, Kang Liu1,2, Jun Zhao1,2
1The Laboratory of Cognition and Decision Intelligence for Complex Systems,
Institute of Automation, Chinese Academy of Sciences
2School of Artificial Intelligence, University of Chinese Academy of Sciences
3Unisound, Beijing, China
wengsyx@gmail.com, {wangzhiqi2022, liaohuanxuan2023}@ia.ac.cn,
liushengping@unisound.com, {shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn
Abstract
With the burgeoning development in the realm
of large language models (LLMs), the demand
for efficient incremental training tailored to spe-
cific industries and domains continues to in-
crease. Currently, the predominantly employed
frameworks lack modular design, it often takes
a lot of coding work to kickstart the training of
LLM. To address this, we present "LMTuner",
a highly usable, integrable, and scalable system
for training LLMs expeditiously and with mini-
mal user-input. LMTuner comprises three main
modules - the Interaction, Training, and Infer-
ence Modules. We advocate that LMTuner’s us-
ability and integrality alleviate the complexities
in training large language models. Remarkably,
even a novice user could commence training
large language models within five minutes. Fur-
thermore, it integrates DeepSpeed frameworks
and supports Efficient Fine-Tuning methodolo-
gies like Low Rank Adaptation (LoRA), Quan-
tized LoRA (QLoRA), etc., enabling the train-
ing of language models scaling from 300M to
a whopping 130B parameters using a single
server. The LMTuner’s homepage1and screen-
cast video2are now publicly available.
1 Introduction
Large language models (LLMs) are demonstrat-
ing unprecedented performance in numerous nat-
ural language understanding and generation tasks
(Wei et al., 2022b,a; Weng et al., 2023a) thanks
to their capacity to learn from extensive text data
with generative manner (Brown et al., 2020; Scao
et al., 2022). This has led to a rising number of re-
searchers and engineers embarking on the training
of their own language models for specific industries
and domains (Taori et al., 2023; Xu et al., 2023;
Cui et al., 2023). However, training LLMs im-
poses high demands on engineering skills (Zhang
et al., 2022), and the various techniques applicable
1https://wengsyx.github.io/LMTuner/
2https://youtu.be/nsXmWOmN3rE
Interaction Module
Training Module
Inference ModuleMan-Machine Interactive Environment Interaction
Let_Tune()
Dataset
GPT-4
Model PEFT MEFTHyper-
Parameters
Medical Low OwnLlama
Llama2ChatGLMGLM-
130BLoRA LOMO QLoRALearning
RateEpochs
WarmupFigure 1: The architecture of the LMTuner. LMTuner
can be invoked through a single line of code
 .
The overall process is top-down, sequentially going
through the Interaction Module, Training Module, and
Inference Module.
to such training are mostly disparate (Dao et al.,
2022; Xi et al., 2023; Luo et al., 2023). This not
only increases the complexity of related projects
but also adds to the learning cost required for train-
ing language models.
With the progressive development of generative
large language model technology, various tech-
niques have emerged, incorporated into different
toolkits (Zhao et al., 2023b). As shown in Table 1,
when aiming for model parallelism, MegatronLM
(Shoeybi et al., 2020) stands as a preferred choice,
while bitsandbytes (Dettmers et al., 2022) serves
the purpose of model quantization, and Opendelta
(Hu et al., 2023) facilitates the implementation of
Efficient Fine-Tuning technology. Nonetheless, the
flexibility offered by these individual tools comes
at the cost of developers spending significant time
coordinating the usage of diverse tool modules,
rendering direct application challenging. To ad-
dress these challenges, several frameworks, such as
h2oGPT (Candel et al., 2023) and Lamini (Diamos
et al., 2023), have attempted to consolidate some of
these functionalities. However, these frameworks
typically integrate only a subset of the available
techniques, often lacking comprehensive coverage
of commonly used model technologies.
Therefore, in this paper, we present LMTuner, a
novel framework that significantly minimizes thesearXiv:2308.10252v1  [cs.CL]  20 Aug 2023

--- PAGE 2 ---
Highly-integrable User-friendly
Model Parallelism Quantization PEFT MEFT ZeRO Load Dataset Position Interpolation AI Assisstent Code Concise
MegatronLM (Shoeybi et al., 2020) ✓
Huggingface (Wolf et al., 2020) ✓ ✓ ✓ ✓ ✓
bitsandbytes (Dettmers et al., 2022) ✓
OpenDelta (Hu et al., 2023) ✓ ✓
Lamini (Diamos et al., 2023) ✓ ✓
h2oGPT (Candel et al., 2023) ✓ ✓ ✓ ✓
LMTuner (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Compared to other commonly used language model training systems, LMTuner system has highly integrable
and user-friendly.
obstacles by offering an easy-to-use, scalable, and
integrative modular system. As depicted in Figure
1, LMTuner is comprised of three modules. 1) The
Interaction Module enables user-friendly commu-
nication, automatically adjusting parameters based
on user needs and context, ideal for non-technical
users. 2) The Training Module autonomously pro-
cesses training using these parameters, saving users
the complexity of setup. 3) The Inference Module
utilizes the trained models for various tasks upon
training completion. With these modules, develop-
ers can effortlessly generate their desired models
and automatically pass on the parameters to the
next module for seamless training. This stream-
lines the entire process, allowing the system to be
readily deployed and utilized upon completion of
training.
In summary, our proposed LMTuner system of-
fers greater usability and flexibility, allowing users
to swiftly configure parameters for training lan-
guage models according to specific needs and ef-
fectively initiate the training. Our contributions are
as follows:
•We have proposed LMTuner, which is the
highly usable and integrated training system
for LLMs. It is free to use and license-friendly
(Apache 2.0). And we open source code at
https://github.com/WENGSYX/LMTuner .
•LMTuner boasts high usability, only needing
a single code (
 ) to be launched. It
facilitates quick start-up for large language
model training by allowing users to interact
using natural language with LMTuner.
•We have incorporated a wide range of tech-
niques suited for training large language mod-
els, including models, domain QA datasets,
Efficient Fine-Tuning methods, and specific
hyperparameters. This integration fosters the
research and development of large language
models.2 Related Work
The development of pre-training language models
has brought about numerous language model tools
and a flourishing NLP community, among which
are “Transformers” (Wolf et al., 2020). It estab-
lishes a range of model classes and provides APIs
for implementing easily extendable transformer
models. Other tools closely associated with lan-
guage model building include Fairseq (Ott et al.,
2019), MegatronLM (Shoeybi et al., 2020), Med-
ConQA (Xia et al., 2022a), OpenDelta (Hu et al.,
2023) and h2oGPT (Candel et al., 2023). Differing
from these, LMTuner is specifically designed for
training auto-regressive LLMs. With its modular
design, it allows for a free combination of different
pre-training models, datasets, model frameworks,
automatic length extrapolation settings, and PEFT
methods within one framework. With the assis-
tance of dialogue-type LLMs such as GPT-4 (Ope-
nAI, 2023), users can accomplish the entire training
process with just a single line of code.
Recent researches have proposed many direc-
tions for training LLMs (Ignat et al., 2023), includ-
ing high-quality data (Zhou et al., 2023; Gunasekar
et al., 2023), efficient fine-tuning methods (Man-
grulkar et al., 2022), model structures (Shazeer,
2019; Dao et al., 2022), and length extrapolation
(Chi et al., 2023; Tworkowski et al., 2023; Su et al.,
2022; Chen et al., 2023a). The LMTuner system
integrates the most advanced techniques in these
fields, allowing users to make choices according to
their needs. LMTuner integrates these techniques
into separate modules, facilitating user selection
and usage (Wang et al., 2023a; Dao et al., 2022;
Dettmers et al., 2023).
LLMs undergoing instruction pre-training can
align with human instructions (Ouyang et al.,
2022b; Chung et al., 2022) and have been discov-
ered to possess many capabilities not present in
smaller language models (Zhao et al., 2023a), such
as tool creation (Cai et al., 2023), environment

--- PAGE 3 ---
if ‘train thismachine ’                                                                    else STEP1:   I nteraction Module
(System Message) :
You are an AI assistant capable of training large language models. 
Firstly, you need to ask the user questions in order  ...
ARGS['model']: Default: None; Here are some available models to 
choose from: GLM -130B, ChatGLM2 -6B, llama -7b...
ARGS['learning rate']: Default 1e -5; If using LoRA or QLoRA as the 
method, please set it to 1e -3...LMTuner
(GPT -4)
LMTunerMan-Machine  Interac tiveEnvironment InteractionUser: I want to train a medical model on this machine.    Let_Tune ()
Python Env.                                True
GPUs Number                            2
GPUs Memory                            48G
ARGS.jsonHello there! I"myour AI assistant, and I"mhere to help 
you train your model.
Before we get started, it"simportant to have a clear plan 
and goal in mind
UserI want to train a medical model on this machine.
Perfect! We can definitely assist you with that. To start 
with, let's choose the dataset for your model. Since you 
mentioned you're interested in training a medical model, 
we can recommend the ‘LM -Medical -v1' dataset to you. 
Would that be OK? 
Userok
Great! Your dataset has been set to 'LM -Medical -v1'.  
Based on your GPU memory, we recommend training 
Llama2 -7B or using LoRA to train Llama2 -13B.
UserIt's okay to use Llama2 -13B.
I have set up Llama2 -13B with LoRA for you.
Based on your GPU memory, we recommend using a 
batch size of 8 and setting the max text length to 1024, 
does that work for you?
In addition, you will need to set the number of training 
epochs and how often to save the weights, such as 
every N steps. The defaults are 10 and 1000, 
respectively.
UserRight.{
'GPU Number': None , 2
'GPU Memory': None , ‘48 GB’
'data': None, ‘LM-Medical -v1’               
'model': None, ‘Llama2 -13B’
'method': None, ‘LoRA ’
'learn rate': None , 1e-04
'epoch': 10,
'lora rank': 16,
'batch size': None , 8
'max length': None , 1024
'quantization': 0,
'train thismachine': None , True
'save interval': 1000,
'save path': './checkpoints',
'gradient accumulation': 1,
’rope scaling’: False,
}1
11
3
53
4
44
5
5STEP2:   Training Module
STEP3:   Inference ModuleARGS.json Readme.md
deepspeed --include localhost:0,1 main.py  --model Llama 2-13B --
use_lora 1 --lora_rank 16 --fp16 --dataset LM-Medical -v1 --
max_seq_length 1024 --deepspeed_config ./ds_config.json ...
Dataset Model PEFT MEFTHyper -
Parameters
LM-Medical -v1 Llama2 -13B LoRAfp16
Learn rate 1e -4
Max length 1024
,,,,,,
Q: What are the manifestations of lactose intolerance? What are the 
treatment methods?
A: Lactose intolerance is a common digestive disorder where the body 
is unable to completely digest lactose, a sugar found in milk and dairy 
products. This condition results from a deficiency of the enzyme 
lactase, which is produced in the small intestine. \n\nSymptoms \nThe 
symptoms of lactose intolerance typically develop within a ...4
2
2ds_config.json
LMTuner
LMTuner
LMTunerFigure 2: Overview of the LMTuner system. When a user makes a request by
 , the LMTuner system
automatically detects the GPUs available in the environment and engages in detailed discussions with the user to
determine the configuration. The configuration is then automatically sent to the Training Module to initiate training.
During the training process, wandb is enabled to save training logs. The final trained model is sent to the Inference
Module for deployment.
exploration (Wang et al., 2023b), self-verification
(Weng et al., 2022), and complex reasoning (Weng
et al., 2023b; Zhu et al., 2023). Leveraging these ca-
pabilities of LLMs, LMTuner’s Interaction Module
can help users analyze their needs and recommend
necessary settings through a nature-language-based
interaction method. This practice of using LLMs
as interactive agents (Wang et al., 2023c) have also
been applied in interactive decision making (Yao
et al., 2023) and functioning as research assistants
(Ren et al., 2023).
3 LMTuner
LMTuner is an open-source system that offers a
command-line interface (CLI) for training LLMs:
1# Official Launch for LMTuner .
2from LMTuner import Let_Tune
3Let_Tune ()
4
5>>> [AI] Greetings ! I am your AI
assistant . Present assist training
your model . Necessary possess clear
plan , goal first .
6>>> [ ANSWER ] :
As shown in Figure 2, LMTuner allows for the
development of LLMs training through simple con-versations. This can improve engineering efficiency
when training LLMs and reduce code burden.
3.1 Interaction Module
The Interaction Module of LMTuner, utilizing GPT-
4’s (or ChatGPT) System Message and Function
features, serves as an LLM training assistant. This
module, during initialization, incorporates com-
mon training issues, parameter configurations, and
selectable methods into System Messages, thereby
streamlining the training process. The essential
function of GPT-4 within this context is to ascertain
the requisite training configurations. A function,
Set_ARGS, is available to amend the parameters
in the training configuration. To enable GPT-4
to evaluate various parameter settings, we utilize
pynvml to monitor server GPUs, integrating this
information within the System Message content.
The Interaction Module helps prevent user con-
figuration mistakes through its user-friendly inter-
face and high adaptability. As shown in Figure 3,
users can state their needs conversationally. LM-
Tuner then analyzes their words and suggests ap-
propriate training settings. This works because the
System Message contains knowledge about LLM
training, compensating for the common lack of

--- PAGE 4 ---
Llama2
7B
Llama
33B
Llama2
70B
GLM-130B
......Models
Chinese
Common
LawEnglish
CommonPEFT and MEFT Datsets Hyper-Parameters
Interaction ModuleLearning Reat=1e-5
Warmup=0.02
Batch Size=24
Seq Length=1024
        I have collected one million legal texts 
and corresponding question-answer 
dialogues, and I would like to use them to 
train a Chinese language model. Since I 
have a large amount of data, I don't need 
the model to be too large, and I hope to 
perform incremental training on all 
parameters in My 8 * A6000 server.        I have a strong desire to train a 
massive model on my personal 
dataset, but unfortunately, I am limited 
to only eight 3090s. Could you lend 
me a hand?        I would like to use Llama2-7B to 
train a medical dialogue chatbot, And I 
have two A100  GPUs.A
B
CCustomMedical
ChatGLM2
6BNormal
LoRA
QLoRA
......Learning Reat=1e-4
Warmup=0.02
Batch Size=6
Seq Length=1024LOMO
Learning Reat=1e-4
Warmup=0.02
Batch Size=32
Seq Length=2048Figure 3: Depending on the requirements specified, LMTuner can automatically suggest different training plans.
such expertise among researchers and engineers
without backgrounds in large language model train-
ing.
The Interaction Module, while offering immense
flexibility and ease of use, prevents potential issues
such as user configuration errors. As shown in
Figure 3, users with different requirements only
need to express their needs in natural language.
The LMTuner system is capable of analyzing and
recommending appropriate training parameters. If
the user is not inclined to train the model on the
current device, LMTuner will automatically gen-
erate a Readme.md file that includes environment
configuration, model processing, and training code
instructions like Figure 4, facilitating a swift setup
for the user on a new device.
Once all training parameters are finalized, LM-
Tuner saves a copy of these in an ARGS.json file.
If one wishes to initiate training quickly with the
same parameters, they only need to pass the path
name of ARGS.json to the
 function,
thereby avoiding redundant repeated dialogues.
1# Quickly Launch for LMTuner .
2from LMTuner import Let_Tune
3Let_Tune ( ARGS =’./ ARGS . config ’)
4
5>>> [ LMTuner ] We will train the model
~ Go!
6>>> [2023 -07 -19 05:18:34 ,778] [ INFO ]
[ runner .py :555: main ] cmd = python
-u -m deepspeed . launcher . launch --
world_info = xxxxx main .py --seed
1234 ......
Train the model on another server  
Welcome to LMTuner we can provide you with various training methods you want
Installation  
For details, please refer to Github Guide
Operation  
Login wandb  
Please run the following code in shell to login wandb
Copy ds_config  
Please move the originally generated ds_config.json  to the now training path.
Training operation  
Please manually execute the following code on the server to start training:wandb login xxxxxxxxxxxxxxxxxx
deepspeed --include localhost:0,1 main.py --seed 1234 --model ChatGLM2-6B --
use_lora 1 --lora_rank 16 --fp16 --dataset WENGSYX/Lingo-Medical-v1 --train-data 
WENGSYX/Lingo-Medical-v1 --valid-data WENGSYX/Lingo-Medical-v1 --max_seq_length 
270 --epochs 10  --train-iters 0  --no-load-rng --warmup .02 --checkpoint-
activations --save-interval 1000 --save "./checkpoints" --split 1 --eval-interval 
1000000 --eval-batch-size 2 --lr 2e-4 --num-workers 0 --log-interval 10 --
deepspeed_config ./ds_config.jsonFigure 4: An example of the Readme.md automatically
generated by LMTuner.
3.2 Training Module
The training module in LMTuner has highly in-
tegrated, easy to invoke, and extensible features.
Currently, the mainstream LLMs are mostly sim-
ilar in architecture, and the training process and
loss calculation are generally consistent. Therefore,
we construct the required techniques in a modular-
ized manner at the code level according to different
requirements. Meanwhile, such design also facili-
tates engineers with coding skills to directly replace
the corresponding modules through hooks. In the
rest of this section, we will introduce each technical
module separately.
Datasets. The availability of high-quality training
data is crucial for developing capable LLMs. To

--- PAGE 5 ---
facilitate affordable access to suitable datasets for
question answering, we have curated and prepared
a collection of QA datasets covering diverse do-
mains, including English, Chinese, medical, and le-
gal fields. Furthermore, to enable the development
of models with customizable names and personas,
the datasets have been augmented with synthetic
question-answer pairs inquiring about the model’s
identity (e.g. "Hi I’m [MODEL NAME]"). During
training, the [MODEL NAME] tokens can be dy-
namically substituted with the preferred name for
each model instance.
1from LMTuner . dataset import
LMTunerDataset
2
3dataset = LMTunerDataset ()
4
5# Give your model a name
6dataset . set_model_name (’LMTuner ’)
7
8# Add QA dataset samples
9dataset . add_sample ([ ’Who are you ?’,
10 "I’m LMTuner , your
personal sidekick !"])
However, despite the richness of our curated
datasets, they inherently possess certain limitations
in coverage and diversity. To augment the built-
in datasets and account for user-specific needs,
our system also provides seamless support for cus-
tomized training data. Users can simply provide
the local path to their own JSONL-formatted ques-
tion answering data files. This design choice pro-
vides greater flexibility to users, empowering them
to tailor the training distribution to their unique
application requirements. For instance, users can
provide proprietary datasets containing sensitive
or confidential information not suitable for public
release. The ability to directly use local JSONL
files avoids the need to port datasets to external
platforms.
1# Pretrained Custom-Dataset Format
2{
3 "input" : " " ,
4 "output" : " With t h e b u r g e o n i n g
development i n t h e realm of . . . " ,
5}
6
7# Instruct Custom-Dataset Format
8{
9 "input" : "Human : Who a r e you ? " ,
10 "output" : " A s s i s t a n t : I ’m LMTuner ,
your p e r s o n a l s i d e k i c k ! " ,
11}
Models. Recent advancements in natural language
processing have been enabled by the transformer ar-chitecture (Vaswani et al., 2017). The SwissArmy-
Transformer3framework facilitates efficient devel-
opment of diverse transformer models by decou-
pling reusable core components from interchange-
able model-specific modules. These lightweight
modules attach to the shared backbone via hooks,
enabling rapid iteration and customization. In con-
trast, Transformers (Wolf et al., 2020) provides op-
timized implementations of canonical architectures
and pretrained models for production use.
The LMTuner system combines these comple-
mentary strengths for flexible model development
and deployment. It utilizes SwissArmyTransformer
to construct tailored architectures and seamlessly
integrates Transformers’ pretrained models. This
synthesis of code modularization and extensive
pretrained models promises to enhance productiv-
ity, accelerate innovation, and improve real-world
language understanding. LMTuner promotes ex-
ploratory modeling by innovating new designs
built on transformer infrastructure, while benefit-
ing from cutting-edge advancements in language
model pretraining. Selectively utilizing both li-
braries stands to meaningfully advance natural lan-
guage processing systems through rapid prototyp-
ing of specialized models and accessible deploy-
ment of state-of-the-art capabilities.
Efficient Fine-Tuning . LMTuner uses ZeRO tech-
nology of Deepspeed by default to unload parame-
ters and improve training throughput. In addition,
LMTuner provides parameters-efficient fine-tuning
(PEFT) methods including LoRA (Hu et al., 2022)
and QLoRA (Dettmers et al., 2023), and memory-
efficient fine-tuning (MEFT) methods including
LOMO (Lv et al., 2023) and Quantization(Gholami
et al., 2021). They support training LLMs with low
memory usage. These methods are implemented
in a modularized manner at the code level of LM-
Tuner, so they can be easily combined and used
freely.
Position Interpolation. To better support long-
context modeling, LMTuner has integrated some
scaling of RoPE. We implemented Xpos (Sun et al.,
2022) and some recent position interpolation meth-
ods like linear interpolation (Chen et al., 2023a),
dynamic interpolation, NTK-Aware Scaled RoPE
(NTKv1) and NTK-By-Parts (NTKv2). The dy-
namic methods choose the correct scale parame-
ter based on sequence length, rather than having
to settle for a fixed trade-off between maximum
3https://github.com/THUDM/SwissArmyTransformer

--- PAGE 6 ---
sequence length and performance on shorter se-
quences, i.e., use the exact position values for the
first 2048 contexts and then recalculate the posi-
tion vectors for each new sequence length as the
model generates the markers one by one. A certain
degree of long-context modeling can be achieved
by choosing different scaling methods.
Other Details. We use the probability distribution
over sequences of tokens as the optimization objec-
tive with cross-entropy loss (Radford et al., 2019)
and use Lion optimizer (Chen et al., 2023b) by de-
fault to optimize LLMs, because it has been proven
to be more memory-efficient than Adam (Kingma
and Ba, 2017). During training, we record the loss,
learning rate, and number of tokens respectively
at each step using wandb4, and display them in
the browser through line charts, which helps users
observe the training status during training.
3.3 Inference Module
LMTuner loads the final model weights after train-
ing and generates continuations conditioned on
given contexts until reaching the maximum length.
To speed up inference, LMTuner provides model
quantization methods including INT8 and INT4
quantization (Zeng et al., 2022). By quantizing
16-bit floating-point weights into lower bitwidth in-
tegers such as 8-bit or 4-bit, the computation time
and memory usage during inference can be reduced.
LMTuner quantizes weights of selected layers in a
trained model, while keeping activations in 16-bit
floating-point format. After quantization, the infer-
ence latency on CPU and throughput on GPU can
be improved significantly with little degradation in
model quality (Liu et al., 2023).
4 A Running Case
The target audience of LMTuner is machine learn-
ing engineers and researchers across academia and
industry. Novice users can leverage LMTuner’s
guided interaction while experts retain full control
over implementation details. It compares favorably
to current systems by combining user-friendliness,
scalability, and integrability within a unified inter-
face. LMTuner is open-source under the Apache
2.0 license, allowing free use in commercial prod-
ucts. By open-sourcing LMTuner, we hope to cat-
alyze progress in large language model training and
lower the barriers to leveraging these transforma-
tive technologies. The availability of an easy-to-
4https://wandb.ai/use, highly customizable system should benefit the
broader community.
Assuming we need to train a medical LLM that
can assist in patient diagnosis, and we have two
A6000 GPUs with 48GB VRAM each, as well as
a medical QA dataset (MedDialog (Zeng et al.,
2020)). Using LMTuner, we can automatically de-
termine the training process, including the selection
of the LLama-7B model and a set of corresponding
hyperparameters5.
while Table 2 showcases the performance of the
MedDialog test set (without manual model selec-
tion). Instead, we chose the final model obtained af-
ter LMTuner training, which completed 10 epochs.
We observed that the LMTuner-trained model sur-
passes existing models in metrics such as Bleu,
Meteor, and NIST. This indicates that the trained
model is ready for direct utilization.
MethodMedDialog
BLEU-2 BLEU-4 Meteor Nist-2 Nist-4
GPT-3 (175B) + Direct 14.93 7.52 4.55 0.814 0.852
GPT-3 (175B) + CoT 9.18 4.61 3.54 0.546 0.569
Instruct-GPT (175B) + Direct 15.93 8.21 5.74 0.874 0.913
Instruct-GPT (175B) + CoT 16.49 7.80 4.94 1.020 1.059
GLM (130B) + Direct 12.16 6.02 5.31 0.577 0.601
GLM (130B) + CoT 30.02 15.61 8.73 1.909 2.017
Zeng et al. (2020) 31.67 16.88 9.57 1.981 2.076
LMTuner (Ours) 35.62 19.02 9.47 2.377 2.517
Table 2: We present the results of several language
models on the MedDialog task, including the zero-
shot (Direct) and CoT (Wei et al., 2022b, 2021) per-
formance of some LLMs (GPT-3: code-davinci-001
(Chen et al., 2021); Instruct-GPT: code-davinci-002
(Ouyang et al., 2022a)), as well as the performance of
previous Finetune state-of-the-art models.
5 Conclusion
LMTuner represents a pioneering effort to facilitate
large language model training through enhanced
usability and modularity. We believe LMTuner rep-
resents a significant step towards realizing the full
potential of large language models. By continuing
to integrate emerging techniques and community
feedback, its capabilities will only grow over time.
The availability of LMTuner as an open source
project presents exciting opportunities for LLMs
enhancement. We hope its emphasis on usability
and extensibility will meaningfully accelerate fu-
ture work in this paradigm-defining domain.
5We released the code at https://github.com/
WENGSYX/LMTuner/tree/main/Example/English_
Medical

--- PAGE 7 ---
Limitations
While LMTuner is designed to be user-friendly
and intuitive, it may not capture all specific user
requirements in its current version. The training
process might need to be adjusted or additional
techniques might need to be integrated to achieve
optimal performance for certain specialized tasks.
Some complex requirements could necessitate man-
ual code modification, which might increase the
learning curve for non-expert users. However, this
limitation is also being actively addressed through
continuous development and updates to the system
to enhance its comprehension of user requirements.
Acknowledgments
This work was supported by the Strategic Priority
Research Program of Chinese Academy of Sci-
ences (No. XDA27020100) and the National Nat-
ural Science Foundation of China (No.U1936207,
No.61976211). This work was supported by the
Youth Innovation Promotion Association CAS and
Yunnan Provincial Major Science and Technology
Special Plan Projects (No.202202AD080004).
References
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow. If you use this software, please cite it
using these metadata.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,
and Denny Zhou. 2023. Large language models as
tool makers.
Arno Candel, Jon McKinney, Philipp Singer, Pas-
cal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu,
Jeff Gambera, Mark Landry, Shivam Bansal, Ryan
Chesler, Chun Ming Lee, Marcos V . Conde, Pasha
Stetsenko, Olivier Grellier, and SriSatish Ambati.
2023. h2ogpt: Democratizing large language mod-
els.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR ,
abs/2107.03374.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
Xiangning Chen, Chen Liang, Da Huang, Esteban Real,
Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong,
Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V .
Le. 2023b. Symbolic discovery of optimization algo-
rithms.
Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky,
and Peter J. Ramadge. 2023. Dissecting transformer
length extrapolation via the lens of receptive field
analysis.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and
Li Yuan. 2023. Chatlaw: Open-source legal large
language model with integrated external knowledge
bases.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,
and Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-
cation for transformers at scale.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms.
Greg Diamos, Sharon Zhou, Samee Ibraheem, Daniel,
and Arman. 2023. Lamini: The llm engine for
rapidly customizing models. https://github.com/
lamini-ai/lamini .

--- PAGE 8 ---
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335.
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao,
Michael W. Mahoney, and Kurt Keutzer. 2021. A
survey of quantization methods for efficient neural
network inference.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,
Harkirat Singh Behl, Xin Wang, Sébastien Bubeck,
Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and
Yuanzhi Li. 2023. Textbooks are all you need.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-rank adaptation of
large language models. In International Conference
on Learning Representations .
Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv,
Zhen Zhang, Zhiyuan Liu, and Maosong Sun. 2023.
OpenDelta: A plug-and-play library for parameter-
efficient adaptation of pre-trained models. In Pro-
ceedings of the 61st Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 3:
System Demonstrations) , pages 274–281, Toronto,
Canada. Association for Computational Linguistics.
Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester,
Santiago Castro, Naihao Deng, Xinyi Gao, Aylin Gu-
nal, Jacky He, Ashkan Kazemi, Muhammad Khalifa,
Namho Koh, Andrew Lee, Siyang Liu, Do June Min,
Shinka Mori, Joan Nwatu, Veronica Perez-Rosas,
Siqi Shen, Zekun Wang, Winston Wu, and Rada Mi-
halcea. 2023. A phd student’s perspective on research
in nlp in the era of very large language models.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A
method for stochastic optimization.
Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao,
Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-
Rong Wen. 2023. Do emergent abilities exist in quan-
tized large language models: An empirical study.
Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang,
Xin Jiang, and Yang You. 2023. Came: Confidence-
guided adaptive memory efficient optimization.
Kai Lv, Yuqing Yang, Tengxiao Liu, Qi jie Gao, Qipeng
Guo, and Xipeng Qiu. 2023. Full parameter fine-
tuning for large language models with limited re-
sources.
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
Younes Belkada, and Sayak Paul. 2022. Peft: State-
of-the-art parameter-efficient fine-tuning methods.
https://github.com/huggingface/peft .OpenAI. 2023. Gpt-4 technical report.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations) ,
pages 48–53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022a.
Training language models to follow instructions with
human feedback.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022b.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Zhichu Ren, Zhen Zhang, Yunsheng Tian, and Ju Li.
2023. Crest–copilot for real-world experimental sci-
entist.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Noam Shazeer. 2019. Fast transformer decoding: One
write-head is all you need.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2020. Megatron-lm: Training multi-billion
parameter language models using model parallelism.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2022. Roformer: En-
hanced transformer with rotary position embedding.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2022. A length-extrapolatable
transformer.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .

--- PAGE 9 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr
Miło ´s. 2023. Focused transformer: Contrastive train-
ing for context scaling.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.
Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Con-
nor Holmes, Samyam Rajbhandari, Olatunji Ruwase,
Feng Yan, Lei Yang, and Yuxiong He. 2023a.
Zero++: Extremely efficient collective communica-
tion for giant model training.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-
ima Anandkumar. 2023b. V oyager: An open-ended
embodied agent with large language models.
Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi,
Wangchunshu Zhou, Shaochun Hao, Guangzheng
Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen,
Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu,
Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen,
Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. 2023c.
Interactive natural language processing.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai, and Quoc V Le. 2021. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations .Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bing Sun,
Shizhu He, Kang Liu, and Jun Zhao. 2023a. Large
language models need holistically thought in medical
conversational qa. ArXiv , abs/2305.05410.
Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,
and Jun Zhao. 2022. Large language models are
reasoners with self-verification. arXiv preprint
arXiv:2212.09561 .
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu
He, Kang Liu, and Jun Zhao. 2023b. Neural com-
prehension: Language models with compiled neural
networks. arXiv preprint arXiv:2304.01665 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu.
2023. Training transformers with 4-bit integers.
Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu,
Bin Sun, Shutao Li, and Jun Zhao. 2022a. Med-
ConQA: Medical conversational question answering
system based on knowledge graphs. In Proceedings
of the The 2022 Conference on Empirical Methods
in Natural Language Processing: System Demonstra-
tions , pages 148–158, Abu Dhabi, UAE. Association
for Computational Linguistics.
Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu,
Bin Sun, Shutao Li, and Jun Zhao. 2022b. Medconqa:
Medical conversational question answering system
based on knowledge graphs. In Proceedings of the
The 2022 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 148–158.
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
2023. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv
preprint arXiv:2304.01196 .

--- PAGE 10 ---
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,
Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng,
Xiangyu Dong, Ruoyu Zhang, et al. 2020. Med-
dialog: Large-scale medical dialogue dataset. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,
Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,
Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023a. A
survey of large language models.
Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong
Tian, Weijie Liu, Yiren Chen, Ningyuan Sun,
Haoyan Liu, Weiquan Mao, Han Guo, Weigang Gou,
Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen,
Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xi-
aoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoy-
ong Du, Linlin Shen, and Kimmo Yan. 2023b. Ten-
centPretrain: A scalable and flexible toolkit for pre-
training models of different modalities. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System
Demonstrations) , pages 217–225, Toronto, Canada.
Association for Computational Linguistics.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less
is more for alignment.
Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, and
Jun Zhao. 2023. Towards graph-hop retrieval and
reasoning in complex question answering over textual
database. arXiv preprint arXiv:2305.14211 .
A Support Models
LMTuner enables rapid deployment of large lan-
guage models ranging from 300 million to 130
billion parameters, facilitating their application to
textual data tasks. We list the models supported
for training in LMTuner in Table 4. (Please notethat some of these models are not fully allowed for
commercial use.)
LMTuner also provides capabilities to load pre-
trained models from Transformers, however this ne-
cessitates redefining the data tokenizer. In addition,
LMTuner facilitates training models from scratch
to incorporate specialized architectures such as
FlashAttention and Multi-Query Attention through
its flexible hook. The modular hook-based ap-
proach streamlines implementing customized at-
tentional mechanisms and training objectives. This
balancing of usability and flexibility makes LM-
Tuner a promising platform for rapidly prototyp-
ing.
B Support Datasets
We have supported multiple QA datasets in dif-
ferent fields for quick training. The provision of
ready-to-use domain-specific datasets eliminates
the need for users to invest significant time and
effort in dataset curation and preprocessing. With
high-quality training data covering key domains,
users can promptly initialize model training and
optimization workflows.
•For English datasets, we default to loading
the LIMA6(Zhou et al., 2023) dataset, which
includes 1,030 diverse and high-quality QA
samples.
•For Chinese datasets, we manually translated
the LIMA dataset and added an additional 60
Chinese samples covering different fields such
as Chinese history, Marxism, essay writing,
and Chinese humor, making it a QA dataset
with 1,090 samples covering multiple fields
and Chinese characteristics.
•For Chinese Medical datasets, We selected
about 60,000 medical consultation dialogues
from CMCQA (Xia et al., 2022b), which con-
tained QA samples from over 30 different de-
partments, including pediatrics, gynecology,
internal medicine, oncology and more.
C LMTuner’s GPU Memory Usage
Normally, training LLMs requires a huge amount
of GPU memory. Recent advancements in EFT
techniques such as LoRA and LOMO have enabled
the training of extremely large language models on
consumer GPUs with as little as 24GB of memory.
6https://huggingface.co/datasets/GAIR/lima

--- PAGE 11 ---
Model Size 16-bit Finetune 16-bit LOMO 16-bit LoRA 8-bit LoRA 4-bit LoRA
<= 1B 8 GB 4 GB 4 GB 6 GB 6 GB
7B 8 GB 6 GB 6 GB 8 GB 8 GB
13B 16 GB 10 GB 10 GB 12 GB 10 GB
33B 2x48 GB 24 GB 24 GB 32 GB 32 GB
70B 8x24 GB 4x48 GB 2x80 GB 2x24 GB 48 GB 2x24 GB 48 GB 2x32 GB 80 GB 2x24 GB 48 GB
130B 8x48 GB 4x80 GB 8x24 GB 4x48 GB 2x80 GB 8x24 GB 4x48 GB 2x80 GB 8x24 GB 4x48 GB 2x80 GB 8x24 GB 4x48 GB 2x80 GB
Table 3: The minimum configuration required for different model size in LMTuner (when Batchsize=1). We suggest
appropriately increasing the number of GPUs to accelerate training the models.
GPT-2 (Radford et al., 2019)
GPT-Neo-1.3B (Black et al., 2021)
ChatGLM-6B (Du et al., 2022)
ChatGLM2-6B (Du et al., 2022)
Llama-7B (Touvron et al., 2023a)
Llama-13B (Touvron et al., 2023a)
Llama-33B (Touvron et al., 2023a)
Llama-65B (Touvron et al., 2023a)
Llama2-7B (Touvron et al., 2023b)
Llama2-13B (Touvron et al., 2023b)
Llama2-70B (Touvron et al., 2023b)
GLM-130B (Zeng et al., 2022)
Table 4: LMTuner supports pre-trained language models
for rapid deployment.
In addition, tensor parallelism techniques can split
a model across different GPUs, making it possible
to scale up training of even larger LLMs. In Table 3
we list the minimum recommended GPU memory
configuration by LMTuner for training models of
different sizes.
D System Configuration and
Requirements
LMTuner ( https://github.com/WENGSYX/
LMTuner ) is an open-source system that offers
a command-line interface for training LLMs,
without requiring any coding experience. The
system requirements include: Ubuntu 14.04+,
Debian 8+, CentOS 6+, or Fedora 27+. And an
NVIDIA GPU with driver version >= 460.32.03 or
AMD GPU with ROMc >= 4.0.
In addition, a series of Python libraries, includ-
ing Apex7, Pytorch8and DeepSpeed9, are also
required. For details, please refer to https://
wengsyx.github.io/LMTuner/install.html .
After installation is complete, you can di-
rectly launch via Command-Line Interface with-
7https://github.com/NVIDIA/apex
8https://pytorch.org/
9https://github.com/microsoft/DeepSpeedout modifying any code by using the one-line code
Let_Lingo()
.
LMTuner requires the provision of an OpenAI
Key to facilitate the collaborative determination of
training parameters with GPT-4 through dialogue.
However, we have also pre-defined ten sets of con-
figuration questions related to training LLMs in a
format similar to a questionnaire, to enable users
in countries that do not support GPT-4 to quickly
initiate LMTuner10.
Finally, LMTuner will automatically generate
and run a command line to invoke deepspeed ac-
cording to the requirements without needing to
modify at the code level. This not only simplifies
the training process, but also facilitates modifying
and recording the configuration of hyperparame-
ters, helping users without basic knowledge to also
train LLMs.
10Take a look at the current list of Supported Countries and
Territories for OpenAI: https://platform.openai.com/
docs/supported-countries
