# Học chuyển đổi hiệu quả về tham số và tính toán 
cho các mô hình tiền huấn luyện thị giác-ngôn ngữ

Qiong Wu12, Wei Yu12, Yiyi Zhou12, Shubin Huang1, Xiaoshuai Sun12, Rongrong Ji12∗

1Phòng thí nghiệm trọng điểm về nhận thức tin cậy đa phương tiện và tính toán hiệu quả,
Bộ Giáo dục Trung Quốc, Đại học Hạ Môn, 361005, Trung Quốc.
2Viện Trí tuệ nhân tạo, Đại học Hạ Môn, 361005, Trung Quốc.

{qiong, weiyu}@stu.xmu.edu.cn, zhouyiyi@xmu.edu.cn,
shubinhuang@stu.xmu.edu.cn, {xssun, rrji}@xmu.edu.cn

Tóm tắt

Với ngày càng nhiều tham số và tính toán, các mô hình tiền huấn luyện thị giác-ngôn ngữ (VLP) thể hiện chi phí cấm đoán trong việc thích ứng tác vụ xuôi dòng. Những nỗ lực gần đây chủ yếu tập trung vào học chuyển đổi hiệu quả tham số (PETL) cho các mô hình VLP bằng cách chỉ cập nhật một số lượng nhỏ tham số. Tuy nhiên, chi phí tính toán quá mức vẫn gây khó khăn cho việc ứng dụng VLP. Trong bài báo này, chúng tôi hướng đến học chuyển đổi hiệu quả tham số và tính toán (PCETL) cho các mô hình VLP. Cụ thể, PCETL không chỉ cần giới hạn số lượng tham số có thể huấn luyện trong các mô hình VLP, mà còn cần giảm sự dư thừa tính toán trong quá trình suy luận, do đó cho phép một sự chuyển đổi hiệu quả hơn. Để tiếp cận mục tiêu này, chúng tôi đề xuất một phương pháp bỏ qua kiến trúc động (DAS) mới hướng đến PCETL. Thay vì trực tiếp tối ưu hóa các kiến trúc nội tại của các mô hình VLP, DAS đầu tiên quan sát tầm quan trọng của các mô-đun của chúng đối với các tác vụ xuôi dòng thông qua một quá trình dựa trên học tăng cường (RL), và sau đó bỏ qua những mô-đun dư thừa bằng các mạng nhẹ, tức là các adapter, theo các phần thưởng thu được. Trong trường hợp này, mô hình VLP có thể duy trì tốt quy mô các tham số có thể huấn luyện trong khi tăng tốc suy luận của nó trên các tác vụ xuôi dòng. Để xác thực DAS, chúng tôi áp dụng nó cho một loạt các mô hình VLP đại diện, và tiến hành các thí nghiệm mở rộng trên một tập hợp các tác vụ VL. Kết quả thí nghiệm không chỉ cho thấy những lợi thế lớn của DAS trong việc giảm độ phức tạp tính toán, ví dụ -11.97% FLOP của METER trên VQA2.0, mà còn xác nhận tính c경tranh của nó so với các phương pháp PETL hiện có về mặt quy mô tham số và hiệu suất. Mã nguồn của chúng tôi được cung cấp tại https://github.com/DoubtedSteam/DAS.

1 Giới thiệu

Được truyền cảm hứng bởi thành công lớn trong xử lý ngôn ngữ tự nhiên (NLP), việc tiền huấn luyện quy mô lớn trên các cặp hình ảnh-văn bản khổng lồ cũng trở thành tiêu chuẩn thực tế trong nghiên cứu thị giác-ngôn ngữ. Để đáp ứng các kho dữ liệu tiền huấn luyện quy mô lớn, các mô hình tiền huấn luyện thị giác-ngôn ngữ (VLP) thường áp dụng các mạng dựa trên Transformer với quy mô khổng lồ về tham số và tính toán. Trong trường hợp này, việc chuyển đổi trực tiếp các mô hình VLP này sang các tác vụ xuôi dòng là quá đắt đỏ về mặt dấu chân bộ nhớ và chi phí tính toán.

Để giảm chi phí của các mô hình tiền huấn luyện, những tiến bộ gần đây đã viện cậy vào học chuyển đổi hiệu quả tham số (PETL) để thích ứng các tác vụ xuôi dòng với chi phí phải chăng. Cụ thể, các phương pháp PETL nhằm tiết kiệm việc sử dụng bộ nhớ cho các tác vụ xuôi dòng bằng cách chỉ cập nhật một số lượng nhỏ tham số thay vì tinh chỉnh đầy đủ toàn bộ mô hình. Ví dụ, các phương pháp điều chỉnh prompt mở rộng chuỗi đầu vào với các token thủ công hoặc có thể học để thu hẹp khoảng cách giữa tiền huấn luyện và các tác vụ xuôi dòng. Các nhà thực hành cũng chèn các mạng thần kinh nhẹ gọi là Adapter vào các mô hình tiền huấn luyện, do đó chiếu các đặc trưng ẩn lên không gian ngữ nghĩa của các tác vụ xuôi dòng. Gần đây hơn, các phương pháp PETL này đã được giới thiệu thành công cho các mô hình VLP cho việc phân loại hình ảnh dựa trên prompt hoặc các tác vụ VL thông thường như trả lời câu hỏi trực quan. Mặc dù có những thành công lớn, các phương pháp PETL vẫn không thể giảm độ phức tạp tính toán của các mô hình VLP, điều này có ý nghĩa hơn đối với các ứng dụng.

Trong bài báo này, chúng tôi nghiên cứu một vấn đề mới gọi là học chuyển đổi hiệu quả tham số và tính toán (PCETL). Để đạt được việc thích ứng tác vụ xuôi dòng hiệu quả hơn, PCETL không chỉ được kỳ vọng duy trì quy mô các tham số có thể huấn luyện tương tự như PETL, mà quan trọng hơn, còn cần giảm độ phức tạp tính toán của các mô hình tiền huấn luyện, do đó tăng tốc suy luận của chúng trên các tác vụ xuôi dòng. Trong các công trình hiện có, hiệu quả của bản thân mạng chủ yếu được quy cho các thiết kế cấu trúc thủ công hoặc tự động của nó. Mặc dù độ phức tạp tính toán có thể được giảm thêm bằng các phương pháp nén, như cắt tỉa, lượng tử hóa hoặc chưng cất, những phương pháp này thường yêu cầu huấn luyện lại sau khi tối ưu hóa kiến trúc mạng, điều này không áp dụng được cho các mô hình VLP đã được tiền huấn luyện tốt trên dữ liệu khổng lồ. Một mặt, dữ liệu tiền huấn luyện quy mô lớn vẫn cần một khả năng mô hình nhất định để học những kiến thức tiên nghiệm này, do đó khó có thể đạt được sự cân bằng tốt giữa hiệu suất và chi phí tính toán cho các mục tiêu tiền huấn luyện. Mặt khác, việc thiết kế một mô hình nhỏ và hiệu quả cho từng tác vụ xuôi dòng vẫn tốn công sức và đắt đỏ, điều này cũng mâu thuẫn với mục tiêu của PETL, vì thường cần tinh chỉnh đầy đủ.

Trong trường hợp này, chúng tôi lập luận rằng chìa khóa của PCETL là khám phá sự dư thừa tham số và tính toán trong các mô hình VLP hiện có. Thông thường giả định rằng quy mô mô hình tỉ lệ thuận với độ phức tạp của tác vụ. Để phục vụ mạnh mẽ cho nhiều tác vụ xuôi dòng, các mô hình VLP được tiền huấn luyện bởi nhiều mục tiêu tiền huấn luyện dựa trên hàng chục triệu cặp hình ảnh-văn bản. Trong trường hợp này, các tham số quá mức phù hợp cho tiền huấn luyện, nhưng dễ dàng dư thừa cho một tác vụ xuôi dòng. Như được hiển thị trong Hình 1-(a), hiệu suất của METER trên VQA hầu như không bị ảnh hưởng khi bỏ qua một số lượng nhất định các lớp Transformer của nó. Kết quả thực nghiệm này cũng gợi ý rằng việc khám phá một con đường tắt trong các mô hình VLP hiện có là một cách khả thi cho PCETL.

Để đạt được mục tiêu này, chúng tôi đề xuất một phương pháp bỏ qua kiến trúc động (DAS) mới hướng đến học chuyển đổi hiệu quả cho các mô hình VLP. Bằng cách quan sát sự dư thừa mô-đun của các mô hình VLP, DAS có thể thực hiện việc định tuyến mạng con tối ưu của các mô hình VLP cho một tác vụ xuôi dòng, do đó giảm tính toán trong quá trình suy luận. Trong thực tế, DAS coi quá trình này như một vấn đề k-armed bandit, và đánh giá tầm quan trọng của từng lớp/khối VL đối với tác vụ xuôi dòng thông qua nhiều lần lấy mẫu mạng con và xác thực nhanh. Do đó, các phần thưởng thu được có thể được sử dụng để phản ánh sự dư thừa của các mô-đun VL và xác định các lớp nào sẽ được bỏ qua. Đồng thời, để đạt được hiệu quả tham số, chúng tôi cũng áp dụng các mạng nhẹ, tức là Adapter, để phục vụ việc thích ứng đặc trưng ẩn và các kết nối tắt của DAS cho các mô hình VLP.

Để xác thực DAS, chúng tôi áp dụng nó cho một tập hợp các mô hình VLP, cụ thể bao gồm METER, ViLT và LaVIN, trên ba điểm chuẩn VL, cụ thể là VQA2.0, NLVR2 và Flickr30K. Kết quả thí nghiệm không chỉ cho thấy hiệu suất cạnh tranh của DAS so với các phương pháp tinh chỉnh đầy đủ và PETL, mà còn chứng kiến lợi thế lớn của nó trong việc giảm độ phức tạp tính toán của các mô hình VLP. Ví dụ, DAS có thể giúp METER đạt được 96.60% hiệu suất của việc tinh chỉnh đầy đủ trên điểm chuẩn VQA2.0 với chỉ 1.65% tham số có thể huấn luyện, trong khi giảm 11.97% FLOP. Đối với việc triển khai thực tế của một tác vụ VL cụ thể, DAS có thể giảm lên đến 93.75% tham số của các mô hình VLP. Những kết quả này xác nhận tốt giả định của chúng tôi về sự dư thừa của các mô hình VLP trên các tác vụ xuôi dòng, và cũng xác thực thiết kế của DAS được đề xuất.

Nhìn chung, đóng góp của chúng tôi có thể được tóm tắt thành ba mặt:
• Chúng tôi đặt ra một vấn đề mới gọi là học chuyển đổi hiệu quả tham số và tính toán (PCETL) cho các mô hình VLP, không chỉ yêu cầu giữ quy mô các tham số huấn luyện mà còn cần giảm độ phức tạp tính toán của các mô hình VLP trên các tác vụ xuôi dòng.
• Chúng tôi đề xuất một phương pháp bỏ qua kiến trúc động (DAS) mới cho PCETL, có thể khám phá con đường tắt tối ưu trong các mô hình VLP với sự kết hợp của các adapter song song.
• Trên hai mô hình VLP và ba tập dữ liệu điểm chuẩn, DAS được đề xuất không chỉ giảm chi phí tính toán một cách đáng kể, ví dụ -11.97% FLOP của METER trên VQA2.0, mà còn ngang bằng với các phương pháp PETL hiện có về mặt quy mô tham số và hiệu suất.
