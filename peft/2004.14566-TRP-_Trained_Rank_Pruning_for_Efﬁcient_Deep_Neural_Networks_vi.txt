TRP: Huấn luyện Cắt tỉa Hạng để Tối ưu Mạng Nơ-ron Sâu Hiệu quả

Yuhui Xu1, Yuxi Li1, Shuai Zhang2, Wei Wen3, Botao Wang2,
Yingyong Qi2, Yiran Chen3, Weiyao Lin1, Hongkai Xiong1
1Đại học Giao thông Thượng Hải 2Nghiên cứu AI Qualcomm 3Đại học Duke

Tóm tắt

Để kích hoạt DNN trên các thiết bị biên như điện thoại di động, xấp xỉ hạng thấp đã được áp dụng rộng rãi nhờ cơ sở lý thuyết vững chắc và triển khai hiệu quả. Một số nghiên cứu trước đã cố gắng trực tiếp xấp xỉ mô hình được huấn luyện trước bằng phân tích hạng thấp; tuy nhiên, các lỗi xấp xỉ nhỏ trong tham số có thể lan truyền thành mất mát dự đoán lớn. Kết quả là, hiệu năng thường giảm đáng kể và cần nỗ lực tinh chỉnh phức tạp để khôi phục độ chính xác. Rõ ràng, việc tách biệt xấp xỉ hạng thấp khỏi quá trình huấn luyện không tối ưu. Khác với các nghiên cứu trước, bài báo này tích hợp xấp xỉ hạng thấp và điều chuẩn hóa vào quá trình huấn luyện. Chúng tôi đề xuất Huấn luyện Cắt tỉa Hạng (TRP), luân phiên giữa xấp xỉ hạng thấp và huấn luyện. TRP duy trì khả năng của mạng ban đầu trong khi áp đặt ràng buộc hạng thấp trong quá trình huấn luyện. Điều chuẩn hóa hạt nhân được tối ưu hóa bằng giảm gradient ngẫu nhiên được sử dụng để thúc đẩy thêm hạng thấp trong TRP. Mạng được huấn luyện TRP có cấu trúc hạng thấp vốn có, và được xấp xỉ với mất mát hiệu năng không đáng kể, do đó loại bỏ quá trình tinh chỉnh sau phân tích hạng thấp. Phương pháp được đề xuất được đánh giá toàn diện trên CIFAR-10 và ImageNet, vượt trội hơn các phương pháp nén trước đó sử dụng xấp xỉ hạng thấp.

1 Giới thiệu

Mạng Nơ-ron Sâu (DNN) đã cho thấy thành công đáng kể trong nhiều tác vụ thị giác máy tính. Mặc dù có hiệu năng cao trong DNN dựa trên máy chủ được hỗ trợ bởi phần cứng tính toán song song tiên tiến, hầu hết các kiến trúc hiện đại chưa sẵn sàng để triển khai trên thiết bị di động do hạn chế về khả năng tính toán, bộ nhớ và năng lượng.

Để giải quyết vấn đề này, nhiều phương pháp nén và tăng tốc mạng đã được đề xuất. Các phương pháp dựa trên cắt tỉa khám phá tính thưa thớt trong trọng số và bộ lọc. Các phương pháp dựa trên lượng tử hóa giảm độ rộng bit của tham số mạng. Phân tích hạng thấp giảm thiểu dư thừa theo kênh và không gian bằng cách phân tích mạng ban đầu thành mạng nhỏ gọn với các lớp hạng thấp. Ngoài ra, các kiến trúc hiệu quả được thiết kế cẩn thận để tạo thuận lợi cho triển khai di động của mạng nơ-ron sâu. Khác với các nghiên cứu trước, bài báo này đề xuất một cách tiếp cận mới để thiết kế mạng hạng thấp.

Mạng hạng thấp có thể được huấn luyện trực tiếp từ đầu. Tuy nhiên, việc đạt được kết quả thỏa đáng là khó khăn vì một số lý do. (1) Khả năng thấp: so với mạng hạng đầy đủ ban đầu, khả năng của mạng hạng thấp bị hạn chế, gây khó khăn trong việc tối ưu hóa hiệu năng. (2) Cấu trúc sâu: phân tích hạng thấp thường tăng gấp đôi số lượng lớp trong mạng. Các lớp bổ sung làm cho tối ưu hóa số dễ bị tổn thương hơn với việc bùng nổ và/hoặc biến mất gradient. (3) Lựa chọn hạng theo kinh nghiệm: hạng của mạng được phân tích thường được chọn làm siêu tham số dựa trên mạng được huấn luyện trước; điều này có thể không phải là hạng tối ưu cho mạng được huấn luyện từ đầu.

Thay vào đó, một số nghiên cứu trước đã cố gắng phân tích các mô hình được huấn luyện trước để có được mạng hạng thấp ban đầu. Tuy nhiên, hạng thấp được áp đặt theo kinh nghiệm có thể gây ra mất mát độ chính xác lớn và cần huấn luyện lại mạng để khôi phục hiệu năng của mạng ban đầu càng nhiều càng tốt. Một số nỗ lực đã được thực hiện để sử dụng điều chuẩn hóa thưa thớt để ràng buộc mạng vào không gian hạng thấp. Mặc dù điều chuẩn hóa thưa thớt giảm lỗi gây ra bởi phân tích ở một mức độ nào đó, hiệu năng vẫn giảm nhanh khi tỷ lệ nén tăng.

Bài báo này là phần mở rộng của [Xu et al., 2019]. Trong bài báo này, chúng tôi đề xuất một phương pháp mới, gọi là Huấn luyện Cắt tỉa Hạng (TRP), để huấn luyện mạng hạng thấp. Chúng tôi nhúng phân tích hạng thấp vào quá trình huấn luyện bằng cách dần đẩy phân bố trọng số của mạng hoạt động tốt vào dạng hạng thấp, trong đó tất cả tham số của mạng ban đầu được giữ và tối ưu hóa để duy trì khả năng của nó. Chúng tôi cũng đề xuất điều chuẩn hóa hạt nhân được tối ưu hóa bằng giảm gradient ngẫu nhiên để ràng buộc thêm trọng số trong không gian hạng thấp nhằm tăng cường TRP. Giải pháp được đề xuất được minh họa trong Hình 1.

Nhìn chung, các đóng góp của chúng tôi được tóm tắt dưới đây:

1. Một phương pháp huấn luyện mới gọi là TRP được trình bày bằng cách nhúng rõ ràng phân tích hạng thấp vào quá trình huấn luyện mạng;
2. Điều chuẩn hóa hạt nhân được tối ưu hóa bằng giảm gradient ngẫu nhiên để tăng cường hiệu năng của TRP;
3. Cải thiện tăng tốc suy luận và giảm mất mát độ chính xác xấp xỉ trong cả phương pháp phân tích theo kênh và theo không gian.

2 Các Nghiên cứu Liên quan

Nhiều nghiên cứu đã được đề xuất để tăng tốc quá trình suy luận của mạng nơ-ron sâu. Ngắn gọn, các nghiên cứu này có thể được phân loại thành ba danh mục chính: lượng tử hóa, cắt tỉa, và phân tích hạng thấp.

Lượng tử hóa: Các phương pháp lượng tử hóa trọng số bao gồm huấn luyện mô hình lượng tử hóa từ đầu hoặc chuyển đổi mô hình được huấn luyện trước thành biểu diễn lượng tử hóa. Biểu diễn trọng số lượng tử hóa bao gồm giá trị nhị phân hoặc các nhóm băm. Lưu ý rằng phương pháp của chúng tôi được lấy cảm hứng từ sơ đồ kết hợp lượng tử hóa với quá trình huấn luyện, tức là chúng tôi nhúng phân tích hạng thấp vào quá trình huấn luyện để hướng dẫn tham số một cách rõ ràng vào dạng hạng thấp.

Cắt tỉa: Tính thưa thớt không có cấu trúc và có cấu trúc được giới thiệu bằng cắt tỉa. Một số nghiên cứu đề xuất cắt tỉa các kết nối không quan trọng giữa các đơn vị nơ-ron với trọng số nhỏ trong CNN được huấn luyện trước. Nghiên cứu khác sử dụng chiến lược group Lasso để học tính thưa thớt cấu trúc của mạng. Một nghiên cứu khác áp dụng chiến lược tương tự bằng cách áp đặt rõ ràng các yếu tố tỷ lệ trên mỗi kênh để đo tầm quan trọng của mỗi kết nối và loại bỏ những kết nối có trọng số nhỏ. Trong một nghiên cứu khác, vấn đề cắt tỉa được công thức hóa như một vấn đề khôi phục dữ liệu. Các bộ lọc được huấn luyện trước được đánh trọng lại bằng cách giảm thiểu hàm mục tiêu khôi phục dữ liệu. Các kênh có trọng số nhỏ hơn bị cắt tỉa. Một nghiên cứu khác chọn bộ lọc một cách kinh nghiệm sử dụng sự thay đổi đầu ra của lớp tiếp theo như một tiêu chí.

Phân tích hạng thấp: Các mô hình ban đầu được phân tích thành các mô hình nhỏ gọn với các lớp nhẹ hơn. Một nghiên cứu xem xét cả dư thừa theo không gian và theo kênh và đề xuất phân tích bộ lọc thành hai bộ lọc bất đối xứng nối tiếp. Một nghiên cứu khác giả định rằng bản đồ đặc trưng nằm trong không gian con hạng thấp và phân tích bộ lọc tích chập thành bộ lọc k×k theo sau bởi bộ lọc 1×1 qua SVD. Một nghiên cứu khác khai thác giả định hạng thấp của các bộ lọc tích chập và phân tích tích chập thông thường thành một số cấu trúc tích chập theo độ sâu và theo điểm. Mặc dù các nghiên cứu này đạt được hiệu năng đáng chú ý trong nén mạng, tất cả đều dựa trên giả định hạng thấp. Khi giả định như vậy không được thỏa mãn hoàn toàn, lỗi dự đoán lớn có thể xảy ra.

Thay vào đó, một số nghiên cứu khác sử dụng điều chuẩn hóa thưa thớt một cách gián tiếp để hướng dẫn quá trình huấn luyện mạng nơ-ron học biểu diễn hạng thấp. Công trình của chúng tôi tương tự như phương pháp điều chuẩn hóa hạng thấp này. Tuy nhiên, ngoài việc thêm điều chuẩn hóa gián tiếp trong quá trình huấn luyện, chúng tôi áp đặt ràng buộc thưa thớt rõ ràng trong quá trình huấn luyện và chứng minh rằng cách tiếp cận của chúng tôi có thể đẩy phân bố trọng số vào dạng hạng thấp một cách khá hiệu quả.

3 Phương pháp

3.1 Kiến thức cơ bản

Một cách chính thức, các bộ lọc tích chập trong một lớp có thể được ký hiệu bởi tensor W ∈ R^(n×c×kw×kh), trong đó n và c là số lượng bộ lọc và kênh đầu vào, kh và kw là chiều cao và chiều rộng của các bộ lọc. Đầu vào của lớp tích chập Fi ∈ R^(c×x×y) tạo ra đầu ra là Fo = W ⊗ Fi. Tương quan theo kênh và tương quan theo không gian được khám phá để xấp xỉ các bộ lọc tích chập trong không gian hạng thấp. Trong bài báo này, chúng tôi tập trung vào hai sơ đồ phân tích này. Tuy nhiên, khác với các nghiên cứu trước, chúng tôi đề xuất sơ đồ huấn luyện mới TRP để có được mạng hạng thấp mà không cần huấn luyện lại sau phân tích.

3.2 Huấn luyện Cắt tỉa Hạng

Huấn luyện Cắt tỉa Hạng (TRP) được thúc đẩy bởi các chiến lược huấn luyện mạng lượng tử hóa. Một trong những sơ đồ cập nhật gradient để huấn luyện mạng lượng tử hóa từ đầu là:

wt+1 = Q(wt - η∇f(wt)) (1)

trong đó Q(·) là hàm lượng tử hóa, wt biểu thị tham số trong lần lặp thứ t. Các tham số được lượng tử hóa bởi Q(·) trước khi cập nhật gradient.

Ngược lại, chúng tôi đề xuất sơ đồ huấn luyện đơn giản nhưng hiệu quả gọi là Huấn luyện Cắt tỉa Hạng (TRP) theo cách định kỳ:

Wt+1 = {
  Wt - η∇f(Wt) nếu t%m ≠ 0
  Tz - η∇f(Tz) nếu t%m = 0
  Tz = D(Wt); z = t/m
} (2)

trong đó D(·) là toán tử xấp xỉ tensor hạng thấp, η là tỷ lệ học, t chỉ số lần lặp và z là lần lặp của toán tử D, với m là chu kỳ cho xấp xỉ hạng thấp.

Thoạt nhìn, TRP này trông rất đơn giản. Một mối quan tâm ngay lập tức phát sinh: liệu các lần lặp có thể đảm bảo hạng của các tham số hội tụ, và quan trọng hơn là sẽ không tăng khi chúng được cập nhật theo cách này? Một câu trả lời tích cực (xem Định lý 2) được đưa ra trong phân tích lý thuyết của chúng tôi sẽ chứng nhận tính hợp lệ của thuật toán này.

Đối với lượng tử hóa mạng, nếu gradient nhỏ hơn lượng tử hóa, thông tin gradient sẽ bị mất hoàn toàn và trở thành không. Tuy nhiên, điều này sẽ không xảy ra trong TRP vì toán tử hạng thấp được áp dụng trên tensor trọng số. Hơn nữa, chúng tôi áp dụng xấp xỉ hạng thấp mỗi m lần lặp SGD. Điều này tiết kiệm thời gian huấn luyện đáng kể. Như được minh họa trong Hình 1, cứ mỗi m lần lặp, chúng tôi thực hiện xấp xỉ hạng thấp trên các bộ lọc ban đầu, trong khi gradient được cập nhật trên dạng hạng thấp kết quả. Nếu không, mạng được cập nhật qua SGD bình thường. Sơ đồ huấn luyện của chúng tôi có thể được kết hợp với bất kỳ toán tử hạng thấp nào.

Trong nghiên cứu được đề xuất, chúng tôi chọn các kỹ thuật hạng thấp được đề xuất trong [Jaderberg et al., 2014] và [Zhang et al., 2016], cả hai đều chuyển đổi các bộ lọc 4 chiều thành ma trận 2D và sau đó áp dụng phân tích giá trị kỳ dị cắt ngắn (TSVD). SVD của ma trận Wt có thể được viết là:

Wt = Σ(i=1 đến rank(Wt)) σi Ui (Vi)^T (3)

trong đó σi là giá trị kỳ dị của Wt với σ1 ≥ σ2 ≥ ... ≥ σrank(Wt), và Ui và Vi là các vector kỳ dị. TSVD có tham số (Wt; ε) là để tìm số nguyên nhỏ nhất k sao cho:

Σ(j=k+1 đến rank(Wt)) (σj)^2 ≤ ε Σ(i=1 đến rank(Wt)) (σi)^2 (4)

trong đó ε là siêu tham số được định nghĩa trước của tỷ lệ cắt tỉa năng lượng, ε ∈ (0,1).

Sau khi cắt ngắn n-k giá trị kỳ dị cuối cùng, chúng tôi chuyển đổi ma trận 2D hạng thấp trở lại tensor 4D. So với việc huấn luyện trực tiếp cấu trúc hạng thấp từ đầu, TRP được đề xuất có các ưu điểm sau:

(1) Khác với việc cập nhật các bộ lọc được phân tích độc lập với việc huấn luyện mạng trong tài liệu [Zhang et al., 2016; Jaderberg et al., 2014], chúng tôi cập nhật mạng trực tiếp trên hình dạng 4D ban đầu của các tham số được phân tích, cho phép phân tích mạng và huấn luyện kết hợp bằng cách bảo tồn khả năng phân biệt của nó càng nhiều càng tốt.

(2) Vì cập nhật gradient được thực hiện dựa trên cấu trúc mạng ban đầu, sẽ không có vấn đề gradient bùng nổ và biến mất gây ra bởi các lớp bổ sung.

(3) Hạng của mỗi lớp được tự động chọn trong quá trình huấn luyện. Chúng tôi sẽ chứng minh một định lý chứng nhận sự hội tụ hạng của trọng số mạng và sẽ không tăng trong mục 3.4.

3.3 Điều chuẩn hóa Chuẩn Hạt nhân

Chuẩn hạt nhân được sử dụng rộng rãi trong các vấn đề hoàn thiện ma trận. Gần đây, nó được giới thiệu để ràng buộc mạng vào không gian hạng thấp trong quá trình huấn luyện.

min (ℓ(f(x;w) + λ Σ(l=1 đến L) ||Wl||*)) (5)

trong đó ℓ(·) là hàm mất mát mục tiêu, chuẩn hạt nhân ||Wl||* được định nghĩa là ||Wl||* = Σ(i=1 đến rank(Wl)) σi^l, với σi^l là các giá trị kỳ dị của Wl. λ là siêu tham số thiết lập ảnh hưởng của chuẩn hạt nhân. Trong [Alvarez và Salzmann, 2017], toán tử gần kề được áp dụng trong mỗi lớp độc lập để giải Eq. (5). Tuy nhiên, toán tử gần kề được tách khỏi quá trình huấn luyện và không xem xét ảnh hưởng trong các lớp.

Trong bài báo này, chúng tôi sử dụng giảm gradient ngẫu nhiên [Avron et al., 2012] để tối ưu hóa điều chuẩn hóa chuẩn hạt nhân trong quá trình huấn luyện. Cho W = UΣV^T là SVD của W và cho Utru, Vtru là U, V được cắt ngắn đến rank(W) cột hoặc hàng đầu tiên, thì UtruV^T_tru là gradient con của ||W||* [Watson, 1992]. Do đó, gradient con của Eq. (5) trong một lớp là:

∇ℓ + λUtruV^T_tru (6)

Chuẩn hạt nhân và hàm mất mát được tối ưu hóa đồng thời trong quá trình huấn luyện mạng và có thể được kết hợp thêm với TRP được đề xuất.

3.4 Phân tích Lý thuyết

Trong phần này, chúng tôi phân tích sự hội tụ hạng của TRP từ góc độ lý thuyết nhiễu loạn ma trận [Stewart, 1990]. Chúng tôi chứng minh rằng hạng trong TRP giảm đơn điệu, tức là mô hình dần hội tụ về mô hình thưa thớt hơn.

Cho A là ma trận m×n, không mất tính tổng quát, m ≥ n. Σ = diag(σ1, ..., σn) và σ1 ≥ σ2 ≥ ... ≥ σn. Σ là ma trận đường chéo được tạo bởi tất cả các giá trị kỳ dị của A.

Cho Ã = A + E là nhiễu loạn của A, và E là ma trận nhiễu. Σ̃ = diag(σ̃1, ..., σ̃n) và σ̃1 ≥ σ̃2 ≥ ... ≥ σ̃n. σ̃i là các giá trị kỳ dị của Ã. Các giới hạn nhiễu loạn cơ bản cho các giá trị kỳ dị của ma trận được đưa ra bởi:

Định lý 1. Định lý Mirsky [Mirsky, 1960]:

√(Σ_i |σ̃i - σi|²) ≤ ||E||F (7)

trong đó ||·||F là chuẩn Frobenius. Sau đó, hệ quả sau có thể được suy ra từ Định lý 1:

Hệ quả 1. Cho B là bất kỳ ma trận m×n nào có hạng không lớn hơn k, tức là các giá trị kỳ dị của B có thể được ký hiệu bởi γ1 ≥ ... ≥ γk ≥ 0 và γk+1 = ... = γn = 0. Khi đó:

||B - A||F ≥ √(Σ_i |γi - σi|²) ≥ √(Σ_(j=k+1 đến n) σj²) (8)

Dưới đây, chúng tôi sẽ phân tích quy trình huấn luyện của TRP được đề xuất. Lưu ý rằng W dưới đây đều được chuyển đổi thành ma trận 2D. Về mặt Eq. (2), quá trình huấn luyện giữa hai thao tác TSVD liên tiếp có thể được viết lại như Eq. (9):

Wt = Tz = TSVD(Wt; ε)
Wt+m = Tz - η Σ_(i=0 đến m-1) ∇f(Wt+i)
Tz+1 = TSVD(Wt+m; ε) (9)

trong đó Wt là ma trận trọng số trong lần lặp thứ t. Tz là ma trận trọng số sau khi áp dụng TSVD lên Wt. ∇f(Wt+i) là gradient được lan truyền ngược trong lần lặp thứ (t+i). ε ∈ (0,1) là ngưỡng năng lượng được định nghĩa trước. Khi đó chúng ta có định lý sau:

Định lý 2. Giả sử rằng ||∇f||F có giới hạn trên G, nếu G < √(ε/m) ||Wt+m||F, thì rank(Tz) ≥ rank(Tz+1).

Chứng minh. Chúng tôi ký hiệu σj^t và σj^(t+m) là các giá trị kỳ dị của Wt và Wt+m tương ứng. Khi đó tại lần lặp thứ t, với ngưỡng tỷ lệ năng lượng ε, thao tác TSVD cố gắng tìm chỉ số giá trị kỳ dị k ∈ [0, n-1] sao cho:

Σ_(j=k+1 đến n) (σj^t)² < ε||Wt||F²
Σ_(j=k đến n) (σj^t)² ≥ ε||Wt||F² (10)

Theo Eq. (10), Tz là ma trận hạng k, tức là n-k giá trị kỳ dị cuối cùng của Tz bằng 0. Theo Hệ quả 1, chúng ta có thể suy ra:

||Wt+m - Tz||F = ||Σ_(i=0 đến m-1) ∇f(t+i)||F ≥ √(Σ_(j=k+1 đến n) (σj^(t+m))²) (11)

Với giả sử G < √(ε/m) ||Wt+m||F, chúng ta có được:

√(Σ_(j=k+1 đến n) (σj^(t+m))²) / ||Wt+m||F ≤ ||Σ_(i=0 đến m-1) ∇f(t+i)||F / ||Wt+m||F ≤ Σ_(i=0 đến m-1) ||∇f(t+i)||F / ||Wt+m||F ≤ mG / ||Wt+m||F < √ε (12)

Eq. (12) chỉ ra rằng vì các nhiễu loạn của giá trị kỳ dị được giới hạn bởi gradient tham số, nếu chúng ta chọn ngưỡng tỷ lệ năng lượng TSVD ε một cách thích hợp, chúng ta có thể đảm bảo rằng nếu n-k giá trị kỳ dị bị cắt tỉa bởi lần lặp TSVD trước, thì trước TSVD tiếp theo, năng lượng cho n-k giá trị kỳ dị cuối cùng vẫn nhỏ hơn ngưỡng năng lượng được định nghĩa trước ε. Do đó TSVD nên giữ số lượng giá trị kỳ dị bị cắt tỉa hoặc loại bỏ nhiều hơn để đạt được tiêu chí trong Eq. (10), do đó có được ma trận trọng số có hạng thấp hơn hoặc bằng, tức là Rank(Tz) ≥ Rank(Tz+1). Chúng tôi tiếp tục xác nhận phân tích của chúng tôi về sự biến thiên của phân bố hạng trong Phần 4.

4 Thí nghiệm

4.1 Tập dữ liệu và Cơ sở

Chúng tôi đánh giá hiệu năng của sơ đồ TRP trên hai tập dữ liệu phổ biến, CIFAR-10 [Krizhevsky và Hinton, 2009] và ImageNet [Deng et al., 2009]. Tập dữ liệu CIFAR-10 bao gồm các hình ảnh tự nhiên màu với độ phân giải 32×32 và có tổng cộng 10 lớp. Tập dữ liệu ImageNet bao gồm 1000 lớp hình ảnh cho tác vụ nhận dạng. Đối với cả hai tập dữ liệu, chúng tôi áp dụng ResNet [He et al., 2016] làm mô hình cơ sở vì nó được sử dụng rộng rãi trong các tác vụ thị giác khác nhau. Chúng tôi sử dụng ResNet-20, ResNet-56 cho CIFAR-10 và ResNet-18, ResNet-50 cho ImageNet. Đối với thước đo đánh giá, chúng tôi áp dụng độ chính xác top-1 trên CIFAR-10 và độ chính xác top-1, top-5 trên ImageNet. Để đo hiệu năng tăng tốc, chúng tôi tính tỷ lệ FLOPs giữa mô hình cơ sở và mô hình được phân tích để có được tỷ lệ tăng tốc cuối cùng. Thời gian thực tế CPU và GPU cũng được so sánh. Ngoài các phương pháp phân tích cơ bản, chúng tôi so sánh hiệu năng với các thuật toán tăng tốc hiện đại khác.

4.2 Chi tiết Triển khai

Chúng tôi triển khai sơ đồ TRP với GPU NVIDIA 1080 Ti. Đối với huấn luyện trên CIFAR-10, chúng tôi bắt đầu với tỷ lệ học cơ sở 0.1 để huấn luyện 164 epoch và giảm giá trị theo hệ số 10 tại epoch thứ 82 và 122. Đối với ImageNet, chúng tôi tinh chỉnh trực tiếp mô hình với sơ đồ TRP từ cơ sở được huấn luyện trước với tỷ lệ học 0.0001 trong 10 epoch. Chúng tôi áp dụng bộ giải SGD để cập nhật trọng số và đặt giá trị suy giảm trọng số là 10^-4 và giá trị động lượng là 0.9. Cải thiện độ chính xác được kích hoạt bởi phân tích phụ thuộc dữ liệu biến mất sau tinh chỉnh. Vì vậy chúng tôi đơn giản áp dụng phân tích độc lập dữ liệu được huấn luyện lại làm phương pháp cơ bản của chúng tôi.

4.3 Kết quả trên CIFAR-10

Cài đặt. Các thí nghiệm về phân tích theo kênh (TRP1) và phân tích theo không gian (TRP2) đều được xem xét. Ngưỡng năng lượng TSVD trong TRP và TRP+Nu là 0.02 và trọng số chuẩn hạt nhân được đặt là 0.0003. Chúng tôi phân tích cả các lớp 1×1 và 3×3 trong ResNet-56.

Kết quả. Như được thể hiện trong Bảng 1, đối với cả phân tích theo không gian và theo kênh, TRP được đề xuất vượt trội hơn các phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014] trên ResNet-20 và ResNet-56. Kết quả thậm chí còn tốt hơn khi sử dụng điều chuẩn hóa hạt nhân. Ví dụ, trong phân tích theo kênh (TRP2) của ResNet-56, kết quả của TRP kết hợp với điều chuẩn hóa hạt nhân thậm chí có thể đạt được tỷ lệ tăng tốc gấp 2 lần so với [Zhang et al., 2016] với cùng mức giảm độ chính xác. TRP cũng vượt trội hơn cắt tỉa bộ lọc [Li et al., 2016] và cắt tỉa kênh [He et al., 2017]. ResNet-56 được huấn luyện TRP phân tích kênh có thể đạt 92.77% độ chính xác với 2.31× tăng tốc, trong khi [He et al., 2017] là 91.80% và [Li et al., 2016] là 91.60%. Với điều chuẩn hóa hạt nhân, phương pháp của chúng tôi có thể tăng gấp đôi tỷ lệ tăng tốc của [He et al., 2017] và [Li et al., 2016] với độ chính xác cao hơn.

4.4 Kết quả trên ImageNet

Cài đặt. Chúng tôi chọn ResNet-18 và ResNet-50 làm mô hình cơ sở. Ngưỡng năng lượng TSVD ε được đặt là 0.005. λ của điều chuẩn hóa chuẩn hạt nhân là 0.0003 cho cả ResNet-18 và ResNet-50. Chúng tôi phân tích cả các lớp tích chập 3×3 và 1×1 trong ResNet-50. TRP1 là phân tích theo kênh và TRP2 là phân tích theo không gian.

Kết quả. Kết quả trên ImageNet được thể hiện trong Bảng 2 và Bảng 3. Đối với ResNet-18, phương pháp của chúng tôi vượt trội hơn các phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014]. Ví dụ, trong phân tích theo kênh, TRP đạt được tỷ lệ tăng tốc 1.81× với 86.48% độ chính xác Top5 trên ImageNet, vượt trội hơn cả phương pháp dựa trên dữ liệu [Zhang et al., 2016] và phương pháp độc lập dữ liệu [Zhang et al., 2016] với biên độ lớn. Điều chuẩn hóa hạt nhân có thể tăng tỷ lệ tăng tốc với cùng độ chính xác.

Đối với ResNet-50, để xác thực tốt hơn tính hiệu quả của phương pháp, chúng tôi cũng so sánh TRP với các phương pháp dựa trên cắt tỉa. Với tăng tốc 1.80×, ResNet-50 được phân tích của chúng tôi có thể đạt 74.06% độ chính xác Top1 và 92.07% độ chính xác Top5, cao hơn nhiều so với [Luo et al., 2017]. TRP đạt được tăng tốc 2.23× cao hơn [He et al., 2017] với cùng mức giảm 1.4% Top5. Ngoài ra, với cùng tỷ lệ tăng tốc 2.30×, hiệu năng của chúng tôi tốt hơn [Zhou et al., 2019].

4.5 Biến thiên Hạng

Để phân tích sự biến thiên của phân bố hạng trong quá trình huấn luyện, chúng tôi tiến hành thêm thí nghiệm trên tập dữ liệu CIFAR-10 với ResNet-20 và trích xuất trọng số từ lớp tích chập res3-1-2 với phân tích theo kênh như sơ đồ TRP của chúng tôi. Sau mỗi TSVD, chúng tôi tính tỷ lệ năng lượng chuẩn hóa ER(i) cho mỗi giá trị kỳ dị i như Eq. (13).

ER(i) = σi² / Σ_(j=0 đến rank(Tz)) σj² (13)

Chúng tôi ghi lại tổng cộng 40 lần lặp TSVD với chu kỳ m = 20, tương đương với 800 lần lặp huấn luyện, và ngưỡng năng lượng ε được định nghĩa trước là 0.05. Sau đó chúng tôi hình dung sự biến thiên của ER trong Hình 2. Trong quá trình huấn luyện, chúng tôi quan sát thấy giá trị giới hạn lý thuyết max_t mG/||Wt||F ≈ 0.092 < √ε ≈ 0.223, điều này cho thấy giả sử cơ bản trong định lý 2 luôn đúng cho giai đoạn huấn luyện ban đầu.

Và hiện tượng này cũng được phản ánh trong Hình 2, lúc đầu, phân bố năng lượng gần như đồng đều w.r.t mỗi giá trị kỳ dị, và số lượng giá trị kỳ dị bị loại bỏ tăng sau mỗi lần lặp TSVD và phân bố năng lượng trở nên dày đặc hơn giữa các giá trị kỳ dị với chỉ số nhỏ hơn. Cuối cùng, phân bố hạng hội tụ đến một điểm nhất định nơi tỷ lệ năng lượng nhỏ nhất chính xác đạt đến ngưỡng ε của chúng tôi và TSVD sẽ không cắt thêm giá trị kỳ dị nào.

4.6 Nghiên cứu Khử bỏ

Để thể hiện hiệu quả của các thành phần khác nhau của phương pháp, chúng tôi so sánh bốn sơ đồ huấn luyện: các phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014], các phương pháp cơ bản kết hợp với điều chuẩn hóa chuẩn hạt nhân, TRP và TRP kết hợp với điều chuẩn hóa chuẩn hạt nhân. Kết quả được thể hiện trong Hình 3. Chúng ta có thể có các quan sát sau:

(1) Điều chuẩn hóa chuẩn hạt nhân: Sau khi kết hợp điều chuẩn hóa chuẩn hạt nhân, các phương pháp cơ bản cải thiện với biên độ lớn. Vì điều chuẩn hóa chuẩn hạt nhân ràng buộc các bộ lọc vào không gian hạng thấp, mất mát gây ra bởi TSVD nhỏ hơn so với các phương pháp cơ bản.

(2) Huấn luyện cắt tỉa hạng: Như được mô tả trong Hình 3, khi tỷ lệ tăng tốc tăng, hiệu năng của các phương pháp cơ bản và các phương pháp cơ bản kết hợp với điều chuẩn hóa chuẩn hạt nhân giảm mạnh. Tuy nhiên, TRP được đề xuất giảm rất chậm. Điều này chỉ ra rằng bằng cách tái sử dụng khả năng của mạng, TRP có thể học biểu diễn đặc trưng hạng thấp tốt hơn so với các phương pháp cơ bản. Lợi ích của điều chuẩn hóa chuẩn hạt nhân trên TRP không lớn bằng các phương pháp cơ bản vì TRP đã đưa các tham số vào không gian hạng thấp bằng cách nhúng TSVD trong quá trình huấn luyện.

4.7 Tăng tốc Thời gian Chạy Thực tế của Mạng được Phân tích

Chúng tôi tiếp tục đánh giá tăng tốc thời gian chạy thực tế của Mạng được nén như được thể hiện trong Bảng 4. Thí nghiệm của chúng tôi được thực hiện trên nền tảng với một GPU Nvidia 1080Ti và CPU Xeon E5-2630. Các mô hình chúng tôi sử dụng là ResNet-18 ban đầu và các mô hình được phân tích bởi TRP1+Nu và TRP2+Nu. Từ kết quả, chúng tôi quan sát thấy rằng trên CPU, sơ đồ TRP của chúng tôi đạt được hiệu năng tăng tốc nổi bật hơn. Nhìn chung, phân tích không gian kết hợp với sơ đồ TRP+Nu của chúng tôi có hiệu năng tốt hơn. Vì cuDNN không thân thiện với kernel 1×3 và 3×1, tăng tốc thực tế của phân tích theo không gian không rõ ràng bằng việc giảm FLOPs.

5 Kết luận

Trong bài báo này, chúng tôi đã đề xuất sơ đồ mới Huấn luyện Cắt tỉa Hạng (TRP) để huấn luyện mạng hạng thấp. Nó tận dụng khả năng và cấu trúc của mạng ban đầu bằng cách nhúng xấp xỉ hạng thấp vào quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất điều chuẩn hóa chuẩn hạt nhân được tối ưu hóa bằng giảm gradient ngẫu nhiên để tăng cường TRP. TRP được đề xuất có thể được kết hợp với bất kỳ phương pháp phân tích hạng thấp nào. Trên các tập dữ liệu CIFAR-10 và ImageNet, chúng tôi đã chỉ ra rằng các phương pháp của chúng tôi có thể vượt trội hơn các phương pháp cơ bản và các phương pháp dựa trên cắt tỉa khác trong cả phân tích theo kênh và phân tích theo không gian.
