# 2210.04382.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2210.04382.pdf
# Kích thước file: 1004773 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh hiệu quả tham số với thích ứng token đặc biệt
Xiaocong Yangy, James Y. Huangz, Wenxuan Zhouzand Muhao Chenz
yĐại học Thanh Hoa;zĐại học Nam California
yangxc.18@sem.tsinghua.edu.cn ;{huangjam,zhouwenx,muhaoche}@usc.edu
Tóm tắt
Tinh chỉnh hiệu quả tham số nhằm mục đích chỉ cập nhật một tập con nhỏ các tham số khi thích ứng mô hình được tiền huấn luyện cho các tác vụ downstream. Trong công trình này, chúng tôi giới thiệu PASTA, trong đó chúng tôi chỉ sửa đổi các biểu diễn token đặc biệt (ví dụ: [SEP] và [CLS] trong BERT) trước module self-attention tại mỗi layer trong các mô hình dựa trên Transformer. PASTA đạt hiệu suất tương đương với fine-tuning toàn bộ trong các tác vụ hiểu ngôn ngữ tự nhiên bao gồm phân loại văn bản và NER với chỉ đến 0.029% tổng số tham số được huấn luyện. Công trình của chúng tôi không chỉ cung cấp một cách tiếp cận đơn giản nhưng hiệu quả của tinh chỉnh hiệu quả tham số, có nhiều ứng dụng thực tế khi triển khai các mô hình được fine-tuning cho nhiều tác vụ, mà còn chứng minh vai trò then chốt của các token đặc biệt trong các mô hình ngôn ngữ được tiền huấn luyện.¹
1 Giới thiệu
Được xây dựng dựa trên mô hình ngôn ngữ được tiền huấn luyện (PLM; Devlin et al. 2019; Liu et al. 2019; Yang et al. 2019; Chowdhery et al. 2022), nhiều hệ thống NLP gần đây được phát triển dựa trên fine-tuning đặc thù cho tác vụ. Bằng cách này, PLM hiệu quả tận dụng kiến thức bất khả tri tác vụ được nắm bắt trong quá trình tiền huấn luyện tự giám sát và thích ứng chính nó với các tác vụ downstream. Tuy nhiên, fine-tuning toàn bộ đặt ra thách thức cho việc triển khai mô hình trong các tình huống đa tác vụ, giới hạn bộ nhớ, nơi chúng ta cần huấn luyện và lưu trữ một mô hình kích thước đầy đủ riêng biệt cho mỗi tác vụ có sự khác biệt đáng kể. Như một giải pháp thay thế, tinh chỉnh hiệu quả tham số (Ding et al., 2022) nhằm mục đích chỉ cập nhật một số lượng nhỏ tham số khi thích ứng PLM cho các tác vụ downstream trong khi giữ cố định hầu hết các tham số mô hình và chia sẻ giữa các tác vụ, do đó giảm sử dụng bộ nhớ.

Trong bài báo này, chúng tôi đề xuất Tinh chỉnh hiệu quả tham số với thích ứng token đặc biệt (PASTA), trong đó chúng tôi chỉ thêm các vector có thể huấn luyện vào các biểu diễn ẩn của token đặc biệt² tại mỗi layer trước module multi-head attention trong các PLM dựa trên Transformer. Công trình của chúng tôi được thúc đẩy bởi vai trò của token đặc biệt trong PLM. Thứ nhất, các token đặc biệt như [CLS] thu thập thông tin từ toàn bộ chuỗi đầu vào và thường được coi là biểu diễn văn bản toàn cục (Devlin et al., 2019). Đối với các tác vụ mức câu như GLUE (Wang et al., 2018), một thực hành phổ biến là thêm một head phân loại mới dựa trên biểu diễn [CLS] trong layer mô hình cuối cùng. Do đó, nếu được huấn luyện đúng cách, bằng cách cập nhật các biểu diễn [CLS], chúng ta có thể xấp xỉ kết quả của quá trình thu thập thông tin trong PLM. Thứ hai, nhiều head attention trong PLM tuân theo một mẫu thẳng đứng³, trong đó các điểm attention chủ yếu được phân bổ cho token [CLS] hoặc [SEP] (Clark et al., 2019; Kovaleva et al., 2019), như được minh họa trong Hình 1. Do đó, các cập nhật cho token đặc biệt cũng có thể được phân tán đến các token khác trong quá trình forward pass thông qua các head attention thẳng đứng (Elhage et al., 2021), cho phép PLM thích ứng với cả tác vụ câu và từ vựng.

Bằng cách tinh chỉnh ít nhất đến 0.029% tổng số tham số, PASTA đạt hiệu suất cạnh tranh ngang với fine-tuning toàn bộ và BitFit (Zaken et al., 2022) trên GLUE (§4.2). Nó cũng vượt trội hơn P-tuning v2 (Liu et al., 2022) 0.6% trên CoNLL2003 với ít hơn 20 tham số bổ sung (§4.3). Nghiên cứu ablation cho thấy chúng ta có thể giảm thêm các tham số có thể huấn luyện xuống 0.009% với chỉ một sự suy giảm hiệu suất nhỏ (§4.4), cho thấy ưu điểm của việc thích ứng biểu diễn token đặc biệt.

2 Công trình liên quan
Một khảo sát gần đây (Ding et al., 2022) phân loại ba loại phương pháp tinh chỉnh hiệu quả tham số. Các phương pháp cộng (Houlsby et al., 2019; Lester et al., 2021; Liu et al., 2022) giới thiệu một số lượng nhỏ tham số có thể huấn luyện bổ sung trong khi giữ nguyên những tham số trong PLM. Các phương pháp đặc tả (Zaken et al., 2022; Guo et al., 2021; Zhao et al., 2020) cập nhật một phần tham số trong PLM trong khi giữ cố định những tham số khác. Các phương pháp tái tham số hóa (Aghajanyan et al., 2021; Hu et al., 2021; Qin et al., 2021) sửa đổi cấu trúc của PLM thành các dạng hiệu quả tham số. Phương pháp của chúng tôi thuộc về các phương pháp dựa trên cộng và tuân theo các thiết lập cơ bản của P-tuning v2 (Liu et al., 2022), trong đó các biểu diễn ẩn được khởi tạo mới của token được chèn vào mỗi layer Transformer. Khác với hầu hết các phương pháp prompt tuning giới thiệu token mới, chúng tôi thêm các vector được giới thiệu vào trạng thái ẩn của token đặc biệt và giữ nguyên độ dài chuỗi.

Các công trình trước đây sử dụng các tác vụ probing (Wu et al., 2020) và các phương pháp pruning (Prasanna et al., 2020) để nghiên cứu vai trò của các module khác nhau bên trong BERT. Đã được chứng minh rằng sự chuyên môn hóa chức năng tồn tại trong các head self-attention của BERT (Clark et al., 2019), và các head attention thẳng đứng³ chiếm một phần lớn (Yao et al., 2021). Kovaleva et al. (2019) phát hiện rằng các head attention thẳng đứng hầu như chỉ liên quan đến attention đến token [SEP] hoặc [CLS], và Clark et al. (2019) kết luận rằng các head ở các layer đầu thường chú ý đến [CLS] trong khi ở các layer giữa chú ý đến [SEP]. Trong công trình này, chúng tôi chứng minh rằng thích ứng các biểu diễn ẩn của token đặc biệt là đủ để đưa hiệu suất của PLM lên mức của fine-tuning toàn bộ.

3 PASTA
Với một PLM lớn, mục tiêu của chúng tôi là phát triển một phương pháp tinh chỉnh hiệu quả tham số chỉ cập nhật một tập nhỏ tham số khi thích ứng với tác vụ downstream. Để đạt được mục tiêu này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả gọi là PASTA, trong đó chúng tôi huấn luyện một vector ẩn cho mỗi token đặc biệt tại mỗi layer Transformer, cùng với một bộ phân loại đặc thù cho tác vụ, trong khi đóng băng các tham số của PLM.

3.1 Thích ứng token đặc biệt
Thích ứng token đặc biệt được minh họa trong Hình 2. Mặc dù những thích ứng này không được áp dụng trực tiếp cho các token không đặc biệt, các thay đổi trong trạng thái ẩn của token đặc biệt có thể được phân tán hiệu quả đến các token khác qua self-attention trong quá trình forward pass, nhờ sự phổ biến của các head attention thẳng đứng³ trong PLM.

Cụ thể, ký hiệu đầu vào cho layer Transformer thứ l là Hl=fhl igN i=1;hl i2Rd, trong đó N là số lượng token đầu vào, d là kích thước ẩn, PASTA sửa đổi đầu vào như sau:
Hl mod=fhl i+ml igN i=1;
Hl+1=Trml(Hl mod);
trong đó Trml là layer Transformer thứ l, ml i2 Rd là thích ứng token đặc biệt của chúng tôi được định nghĩa như sau:
ml i=( 0 nếu token i không phải là token đặc biệt e(vl p) nếu token i là token đặc biệt thứ p

--- TRANG 2 ---
minh họa trong Hình 1. Do đó, các cập nhật cho token đặc biệt cũng có thể được phân tán đến các token khác trong quá trình forward pass thông qua các head attention thẳng đứng (Elhage et al., 2021), cho phép PLM thích ứng với cả tác vụ câu và từ vựng.

Multi-Head Attention
h([CLS]) Câu1 Đóng băng
Có thể huấn luyện
e(v1l)
h([SEP]) Câu2 h([SEP])
e(v2l)
e(v3l)
Add & Norm
Feed-Forward Network
Add & Norm

Hình 2: Kiến trúc của layer PASTA trong Transformer. Các kết nối skip trong Transformer không được hiển thị để rõ ràng. Tại layer l chúng tôi thêm một vector có thể huấn luyện e(vl p)2 Rd vào biểu diễn ẩn của token đặc biệt thứ p trong chuỗi đầu vào, và đóng băng các trọng số của PLM.

--- TRANG 3 ---
# Tham số  Độ phức tạp tham số  Tính nhất quán
Adapter  O(Ldr)  3
P-tuning v2  O(LdT)  3
BitFit  O(L(d+m))  3
Diff-Prune  -  7
PASTA  O(Ld)  3

Bảng 1: Độ phức tạp tham số của PASTA và các baseline. Ở đây L và d đề cập đến số lượng layer và kích thước ẩn của PLM. m và r đề cập đến kích thước trung gian của các module FFN trong Transformer và Adapter, tương ứng. T là độ dài prompt. Tính nhất quán tham số đề cập đến liệu tập hợp các tham số có thể huấn luyện có nhất quán giữa các tác vụ khác nhau hay không (Zaken et al., 2022).

trong đó e(vl p)2Rd là vector có thể huấn luyện được thêm vào biểu diễn ẩn của token đặc biệt thứ p trong chuỗi đầu vào. Trong quá trình huấn luyện tác vụ downstream, chỉ những vector ẩn được giới thiệu cho token đặc biệt và bộ phân loại đặc thù cho tác vụ được tối ưu hóa, và phần còn lại của các tham số mô hình được đóng băng.

3.2 Hiệu quả tham số và tính nhất quán
Như được hiển thị trong Bảng 1, PASTA đạt độ phức tạp tham số O(Ld)⁴ và cập nhật ít nhất 0.015%-0.029% tham số so với PLM đầy đủ khi sử dụng BERT-large hoặc RoBERTa-large làm backbone. Không giống như Adapter (Houlsby et al., 2019) học sự biến đổi của tất cả token đầu vào sử dụng FFN chia sẻ, PASTA chỉ học cập nhật đặc thù cho tác vụ của biểu diễn token đặc biệt như một bias term, giảm đáng kể dung lượng tham số cần thiết để thích ứng trên các tác vụ downstream.

Đồng thời, tập hợp các tham số được giới thiệu bởi PASTA nhất quán giữa các tác vụ khác nhau, làm cho nó hiệu quả cho triển khai dựa trên phần cứng (Zaken et al., 2022). Ngược lại, trong Diff-Prune, việc cập nhật tham số được coi là một thuật ngữ của hàm loss (Guo et al., 2021), dẫn đến các tập hợp tham số được cập nhật khác nhau trong các tác vụ riêng biệt.

4 Thí nghiệm và kết quả
Chúng tôi nghiên cứu hiệu suất downstream của PASTA và phân tích các thuộc tính của các vector ẩn được giới thiệu.

4.1 Thiết lập thí nghiệm
Các phương pháp baseline. Chúng tôi so sánh PASTA với các phương pháp tinh chỉnh hiệu quả tham số sau đây trong các nghiên cứu trước. Adapter (Houlsby et al., 2019) giới thiệu các module feed-forward mới trong các layer Transformer trong khi giữ nguyên các tham số gốc của PLM. BitFit (Zaken et al., 2022) cập nhật tất cả bias term trong PLM trong quá trình fine-tuning. Diff-Prune (Guo et al., 2021) giới thiệu penalty norm L0 trên các tham số được cập nhật để khuyến khích tính thưa thớt của các tham số được tinh chỉnh. P-tuning v2 (Liu et al., 2022) thêm các vector ẩn có thể huấn luyện trước chuỗi đầu vào tại mỗi layer trong khi giữ nguyên các tham số PLM gốc. LoRA (Hu et al., 2021) sử dụng các ma trận phân tách hạng thấp để mô hình hóa các cập nhật tham số.

Cấu hình mô hình. Chúng tôi tiến hành thí nghiệm sử dụng BERT-large và RoBERTa-large (Chúng tôi cũng báo cáo thí nghiệm với BERT-base trong Phụ lục §A). Để thuận tiện cho việc so sánh với các công trình baseline, chúng tôi lấy hầu hết các kết quả thí nghiệm từ các bài báo gốc của họ được báo cáo với BERT-large hoặc RoBERTa-large. Lưu ý rằng nhiều token [SEP] trong một chuỗi đơn (ví dụ, trong các tác vụ cặp câu như MNLI) được coi là các token đặc biệt khác nhau và có các tập hợp tham số có thể huấn luyện riêng biệt, và số lượng tham số có thể huấn luyện khác nhau giữa các tác vụ downstream tùy theo số lượng token đặc biệt được thêm vào. Chi tiết về huấn luyện và thiết lập siêu tham số được hiển thị trong Phụ lục §B.

4.2 Các tác vụ GLUE
Thiết lập tác vụ. Chúng tôi đánh giá PASTA trên benchmark GLUE⁵ được sử dụng rộng rãi (Wang et al., 2018). Để thuận tiện cho việc so sánh trực tiếp, chúng tôi sử dụng các metric giống như đã được sử dụng trong các công trình baseline (Devlin et al., 2019; Liu et al., 2019). Đối với thí nghiệm với BERT, MRPC và QQP được đánh giá bằng điểm F1, STS-B được đánh giá bằng hệ số tương quan Spearman, CoLA được đánh giá bằng Matthew's Correlation, và các tác vụ khác được đánh giá bằng độ chính xác. Đối với thí nghiệm với RoBERTa, STS-B được đánh giá bằng hệ số tương quan Pearson, CoLA được đánh giá bằng Matthew's Correlation, và các tác vụ khác được đánh giá bằng độ chính xác.

Kết quả. Bảng 2 và 3 báo cáo hiệu suất của PASTA trên benchmark GLUE với BERT-large và RoBERTa-large tương ứng. PASTA với RoBERTa-large đạt điểm trung bình giống như fine-tuning toàn bộ trên các tác vụ GLUE.

--- TRANG 4 ---
%Tham số  RTE  CoLA  STS-B  MRPC  SST-2  QNLI  MNLI(m/mm)  QQP  Trung bình
độ chính xác  mcc.  Spearman  F1  độ chính xác  độ chính xác  độ chính xác  F1
Fine-tuning toàn bộ  100%  70.1  60.5  86.5  89.3  94.9  92.7  86.7/85.9  72.1  81.6
Adapter  3.6%  71.5  59.5  86.9  89.5  94.0  90.7  84.9/85.1  71.8  81.1
Diff-Prune†  0.5%  70.6  61.1  86.0  89.7  94.1  93.3  86.4/86.0  71.1  81.5
P-tuning v2  0.29%  70.1  60.1  86.8  88.0  94.6  92.3  85.3/84.9  70.6  81.0
BitFit‡  0.08%  72.0  59.7  85.5  88.9  94.2  92.0  84.5/84.8  70.5  80.9
PASTA  0.015%-0.022%  70.8  62.3  86.6  87.9  94.4  92.8  83.4/83.4  68.6  80.9

Bảng 2: Hiệu suất mô hình BERT-large trên tập test benchmark GLUE. Các dòng với * và ** là kết quả từ Devlin et al. (2019) và Houlsby et al. (2019), và các dòng với † và ‡ là từ Guo et al. (2021) và Zaken et al. (2022) tương ứng. Chúng tôi tái thực hiện thí nghiệm P-tuning v2 trên benchmark GLUE với độ dài prompt là 20.

%Tham số  RTE  CoLA  STS-B  MRPC  SST-2  QNLI  MNLI(tổng thể)  QQP  Trung bình
độ chính xác  mcc.  Pearson  độ chính xác  độ chính xác  độ chính xác  độ chính xác  độ chính xác
Fine-tuning toàn bộ  100%  86.6  68.0  92.4  90.9  96.4  94.7  90.2  92.2  88.9
LoRA†  0.24%  87.4  68.2  92.6  90.9  96.2  94.9  90.6  91.6  89.0
PASTA  0.015%-0.029%  86.6  69.7  91.8  90.9  96.8  95.1  90.4  89.9  88.9

Bảng 3: Hiệu suất mô hình RoBERTa-large trên benchmark GLUE. Các dòng với * là kết quả từ Liu et al. (2019), và các dòng với † là từ Hu et al. (2021). Chúng tôi tuân theo thiết lập metric của các baseline và cũng báo cáo kết quả trên tập development GLUE để thuận tiện cho việc so sánh trực tiếp.

CoLA  RTE  MRPC  STS-B  CoNLL2003
PASTA  65.4  76.2  89.7  90.8  94.0
- không có [CLS]  58.8  72.6  91.4  90.2  93.7
- không có [SEP]  64.5  71.1  91.9  90.3  93.7
- vector chia sẻ  64.7  74.7  92.1  90.0  93.9
- chỉ classifier  36.5  54.2  81.5  64.9  77.4

Bảng 4: Hiệu suất nghiên cứu ablation với BERT-large trên tập development GLUE và CoNLL2003.

PASTA với BERT-large đạt điểm trung bình ngang với BitFit sử dụng ít hơn 3 lần tham số có thể huấn luyện và kết quả tương đương với các baseline có độ phức tạp tham số cao hơn khác. Các kết quả chứng minh rằng bằng cách tận dụng vai trò then chốt của token đặc biệt trong PLM, PASTA có thể hiệu quả thích ứng mô hình với các tác vụ mức câu với số lượng tham số được tinh chỉnh ít hơn đáng kể so với các phương pháp trước đây.

4.3 Nhận dạng thực thể có tên
Thiết lập tác vụ. Chúng tôi thí nghiệm với tác vụ NER trên CoNLL2003 (Tjong Kim Sang and De Meulder, 2003). Theo Devlin et al. (2019), chúng tôi công thức hóa NER như một vấn đề phân loại token.

Kết quả. Như được hiển thị trong Hình 3, PASTA với BERT-large đạt điểm F1 90.8% trên tập test CoNLL2003, vượt trội hơn P-tuning v2 (Liu et al., 2022) 0.6% với ít hơn 20 tham số có thể huấn luyện, trong khi kém fine-tuning toàn bộ 2.0%. Tuy nhiên, hiệu suất mạnh mẽ của PASTA so với P-tuning v2 cho thấy rằng mặc dù PASTA chỉ trực tiếp thích ứng token đặc biệt, biểu diễn của tất cả token đầu vào vẫn có thể được tinh chỉnh đúng cách, hỗ trợ giả thuyết của chúng tôi rằng các head attention thẳng đứng có thể phân tán các thích ứng trong trạng thái ẩn của token đặc biệt đến các token khác.

Hình 3: Kết quả NER trên CoNLL03 với BERT-large (phần trăm điểm F1 được đánh dấu trên các thanh). Mỗi phương pháp được gán nhãn với phần trăm kích thước tham số có thể huấn luyện so với fine-tuning toàn bộ trong ngoặc đơn.

4.4 Phân tích
Ablation về lựa chọn token đặc biệt. Để hiểu tác động của việc tinh chỉnh các tổ hợp khác nhau của token đặc biệt trên các tác vụ downstream, chúng tôi hạn chế thêm dung lượng tham số bổ sung của PASTA bằng cách chỉ thích ứng [CLS] hoặc [SEP], hoặc chia sẻ một vector chung trên tất cả token đặc biệt. Bảng 4 cho thấy hiệu suất của ba biến thể ablated và một baseline chỉ tinh chỉnh head phân loại trên một BERT-large cố định. Nhìn chung, chúng tôi quan sát thấy sự giảm hiệu suất trên hầu hết các tác vụ ngoại trừ trên MRPC đối với ba biến thể PASTA, và hiệu suất suy giảm đáng kể nếu chúng tôi không thích ứng bất kỳ token đặc biệt nào. Những kết quả này chứng minh vai trò quan trọng của các vector ẩn được giới thiệu cho token đặc biệt trong PASTA, trong khi lựa chọn tốt nhất của token đặc biệt được thích ứng có thể khác nhau tùy thuộc vào tác vụ.

Phân bố norm của các vector ẩn được giới thiệu. Hình 4 cho thấy phân bố norm của các vector được giới thiệu trên các tác vụ downstream. Các vector ẩn được giới thiệu học sự khác biệt của token đặc biệt giữa các mô hình được tiền huấn luyện và thích ứng, và do đó norm của những vector đó chỉ ra mức độ thay đổi tham số tại các layer khác nhau. Tương tự như mẫu thay đổi tham số trong fine-tuning toàn bộ (Kovaleva et al., 2019), PASTA thường có norm lớn hơn của các vector ẩn tại các layer gần đầu ra hơn.

Hình 4: Bản đồ phân bố norm của các vector ẩn được giới thiệu trên các tác vụ MRPC, RTE và STS-B với BERT-large. Trong mỗi biểu đồ con, cột đầu tiên hiển thị norm của các vector được giới thiệu được thêm vào [CLS] tại mỗi layer, và cột thứ hai và thứ ba là norm vector được giới thiệu tại hai token [SEP] tương ứng.

5 Kết luận
Chúng tôi trình bày PASTA, một phương pháp tinh chỉnh hiệu quả tham số chỉ sửa đổi biểu diễn token đặc biệt tại mỗi layer Transformer khi thích ứng với các tác vụ downstream. Cách tiếp cận của chúng tôi được thúc đẩy bởi quan sát rằng PLM có một lượng lớn các head attention thẳng đứng chú ý mạnh đến token đặc biệt, và những head này phân tán các cập nhật giá trị từ token đặc biệt đến tất cả các token khác. Thí nghiệm cho thấy PASTA đạt hiệu suất mạnh mẽ tương đương với fine-tuning toàn bộ trên các tác vụ câu và từ vựng với hiệu quả tham số cao. Công trình của chúng tôi không chỉ cung cấp một giải pháp hiệu quả cho tinh chỉnh hiệu quả tham số, mà còn chứng minh vai trò then chốt của token đặc biệt trong PLM.

Hạn chế
Trong công trình này, chúng tôi giả thuyết rằng các head attention thẳng đứng có thể đóng vai trò như "người phân tán thông tin" dựa trên phân tích lý thuyết về Transformer (Elhage et al., 2021). Tuy nhiên, chúng tôi vẫn chưa có các cách tiếp cận trực tiếp như các tác vụ probing và kỹ thuật đảo ngược để chứng minh giả định này. Và vì PASTA dựa vào việc thích ứng token đặc biệt, nó không thể được áp dụng cho các mô hình ngôn ngữ không thêm token đặc biệt vào chuỗi đầu vào như GPT-2 (Radford et al., 2019). Đối với kết quả thực nghiệm, chúng tôi chọn benchmark GLUE và CoNLL2003 để nghiên cứu hiệu suất trên các tác vụ hiểu ngôn ngữ. Hiệu quả của PASTA trên các tác vụ sinh ngôn ngữ chưa được kiểm tra trong công trình này do băng thông hạn chế. Cuối cùng, tương tự như các phương pháp tinh chỉnh hiệu quả tham số khác, PASTA gặp phải chi phí tính toán cao hơn so với fine-tuning toàn bộ.

Lời cảm ơn
Chúng tôi cảm ơn các nhà phản biện ẩn danh vì những bình luận và gợi ý sâu sắc của họ. Xiaocong Yang, Wenxuan Zhou và Muhao Chen được hỗ trợ bởi Quỹ Khoa học Quốc gia Hoa Kỳ Grant IIS 2105329. Tài liệu này được hỗ trợ một phần bởi chương trình DARPA MCS theo Hợp đồng số N660011924033 với Văn phòng Nghiên cứu Hải quân Hoa Kỳ, Giải thưởng Nghiên cứu Amazon, Giải thưởng Nghiên cứu Cisco và một khoản phụ cấp từ NSF Cloudbank 1925001.

Tài liệu tham khảo
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319–7328, Online. Association for Computational Linguistics.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,

--- TRANG 6 ---
Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, Florence, Italy. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. Https://transformer-circuits.pub/2021/framework/index.html.

Demi Guo, Alexander Rush, and Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896, Online. Association for Computational Linguistics.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR.

Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China. Association for Computational Linguistics.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61–68, Dublin, Ireland. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.

Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.

Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2021. Unipelt: A unified framework for parameter-efficient language model tuning.

Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When BERT Plays the Lottery, All Tickets Are Winning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3208–3229, Online. Association for Computational Linguistics.

Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, and Jie

--- TRANG 7 ---
Zhou. 2021. Exploring universal intrinsic task subspace via prompt tuning.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy. Association for Computational Linguistics.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.

Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641.

Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020. Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online. Association for Computational Linguistics.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems.

Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2021. Nlp from scratch without large-scale pretraining: A simple and efficient framework.

Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1–9, Dublin, Ireland. Association for Computational Linguistics.

Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. 2020. Masking as an efficient alternative to finetuning for pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2226–2241, Online. Association for Computational Linguistics.

A Hiệu suất của BERT-base
Bảng 5 báo cáo hiệu suất của PASTA và một phần các baseline với BERT-base làm backbone trên tập development GLUE. Chúng tôi không đặt bảng kết quả trong phần chính của công trình này vì hầu hết các baseline không báo cáo điểm tập test GLUE sử dụng BERT-base, và chỉ ba trong số họ báo cáo điểm tập development GLUE với BERT-base trong các công trình của họ. PASTA có hiệu suất thấp hơn một chút so với các baseline trung bình, trong khi nó vượt trội hơn các mô hình khác trên các tập dữ liệu nhỏ như RTE và MRPC.

B Chi tiết thực hiện
Mô hình của chúng tôi được thực hiện dựa trên Transformers của Huggingface. Chúng tôi tối ưu hóa mô hình với AdamW (Loshchilov and Hutter, 2017). Chúng tôi đặt độ dài đầu vào tối đa là 128 và sử dụng seed ngẫu nhiên cố định là 42 cho tất cả tác vụ. Thí nghiệm được thực hiện trên NVIDIA RTX A5000 trung bình 3 giờ mỗi tác vụ, và huấn luyện phân tán được sử dụng cho hầu hết các tác vụ. Bảng 6 báo cáo các siêu tham số tốt nhất cho huấn luyện mô hình. Để tìm kiếm siêu tham số, chúng tôi chọn tỷ lệ học từ {5e-4, 1e-3, 2e-3, 2.5e-3, 3e-3, 4.5e-3, 5e-3, 7e-3} và số epoch từ {50, 80, 100, 150}.

C Bản đồ attention đầy đủ
Hình 5 minh họa bản đồ attention của tất cả các head trong BERT-large. Với một mẫu ngẫu nhiên từ tập dữ liệu CoLA làm đầu vào ("Fred watered the plants flat."), có 112 head trong tổng số 384 head là các head thẳng đứng³.

--- TRANG 8 ---
Hình 5: Bản đồ attention đầy đủ của BERT-large được tiền huấn luyện với một mẫu ngẫu nhiên từ CoLA làm đầu vào. Hàng và cột đại diện cho các layer và head của mô hình tương ứng, và màu tối hơn cho thấy trọng số lớn hơn. Các head attention thẳng đứng được tô sáng màu cam.

%Tham số  RTE  CoLA  STS-B  MRPC  SST-2  QNLI  MNLI(m/mm)  QQP  Trung bình
Fine-tuning toàn bộ  100%  66.4  62.1  89.8  90.9  91.6  90.0  83.2/ -  87.4  82.7
Adapter  0.81%  71.8  61.5  88.6  89.9  91.9  90.6  83.1/ -  86.8  83.0
BitFit†  0.8%  72.3  58.8  89.2  90.4  92.1  90.2  81.4/ -  84.0  82.3
PASTA  0.015%-0.022%  73.6  57.9  88.7  91.5  91.2  89.7  77.8/78.8  80.8  81.4

Bảng 5: Hiệu suất PASTA với mô hình BERT-base trên tập development benchmark GLUE. Các dòng với * và † đề cập đến kết quả từ Mao et al. (2021) và Zaken et al. (2022) tương ứng.

RTE  CoLA  STS-B  MRPC  SST-2  QNLI  MNLI  QQP  CoNLL2003
Tỷ lệ học  4.5e-3  5e-3  2e-3  2.5e-3  7e-3  2e-3  5e-4  5e-3  3e-3
Kích thước batch  32  4  32¹  32³  32⁴  64³  32⁴  32¹  32⁴  16¹
Số token được thích ứng  3  2  3  3  2  3  3  3  2
Epoch huấn luyện  150  100  150  150  100  80  50  100  100
Hiệu suất dev tốt nhất  76.2  65.4  90.8  89.7  93.9  92.2  83.7  87.9  94.1
Epoch tốt nhất  121  63  109  136  99  42  49  97  89

Bảng 6: Chi tiết huấn luyện PASTA với BERT-large cho các tác vụ GLUE và CoNLL2003. Huấn luyện phân tán trên nhiều GPU được sử dụng khi có sẵn để giảm thời gian huấn luyện.
