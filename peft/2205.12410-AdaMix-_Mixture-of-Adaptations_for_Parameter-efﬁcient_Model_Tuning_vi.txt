# AdaMix: Hỗn hợp các Module Thích ứng cho Điều chỉnh Mô hình Hiệu quả Tham số

Yaqing Wangx, Sahaj Agarwal, Subhabrata Mukherjeey, Xiaodong Liuy,
Jing Gaox,Ahmed Hassan Awadallahy,Jianfeng Gaoy
xĐại học Purdue,Microsoft,yMicrosoft Research
{wang5075, jinggao}@purdue.edu ,
{sahagar, submukhe, xiaodl, hassanam, jfgao}@microsoft.com

## Tóm tắt

Việc điều chỉnh tiêu chuẩn các mô hình ngôn ngữ lớn được huấn luyện trước (PLM) cho các tác vụ hạ nguồn đòi hỏi cập nhật hàng trăm triệu đến hàng tỷ tham số, và lưu trữ một bản sao lớn của trọng số PLM cho mỗi tác vụ dẫn đến tăng chi phí lưu trữ, chia sẻ và phục vụ các mô hình. Để giải quyết vấn đề này, các kỹ thuật điều chỉnh hiệu quả tham số (PEFT) đã được giới thiệu trong đó các thành phần huấn luyện nhỏ được tiêm vào PLM và cập nhật trong quá trình điều chỉnh. Chúng tôi đề xuất AdaMix như một phương pháp PEFT tổng quát điều chỉnh một hỗn hợp các module thích ứng – dựa trên phương pháp PEFT cơ bản được lựa chọn – được giới thiệu trong mỗi lớp Transformer trong khi giữ đóng băng hầu hết các trọng số PLM. Ví dụ, AdaMix có thể tận dụng một hỗn hợp các adapter như Houlsby (Houlsby et al., 2019) hoặc một hỗn hợp các ma trận phân rã thứ hạng thấp như LoRA (Hu et al., 2021) để cải thiện hiệu suất tác vụ hạ nguồn so với các phương pháp PEFT tương ứng cho các tác vụ NLU và NLG có giám sát đầy đủ và few-shot. Hơn nữa, chúng tôi thiết kế AdaMix sao cho nó khớp với cùng chi phí tính toán và số lượng tham số có thể điều chỉnh như phương pháp PEFT cơ bản.

Chỉ bằng cách điều chỉnh 0.1-0.2% tham số PLM, chúng tôi cho thấy AdaMix vượt trội hơn các phương pháp điều chỉnh hiệu quả tham số SOTA và điều chỉnh mô hình đầy đủ cho cả tác vụ NLU và NLG. Mã nguồn và mô hình được cung cấp tại https://aka.ms/AdaMix.

## 1 Giới thiệu

Việc điều chỉnh tiêu chuẩn các mô hình ngôn ngữ lớn được huấn luyện trước (PLM) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020; Raffel et al., 2019) cho các tác vụ hạ nguồn đòi hỏi cập nhật tất cả tham số mô hình. Với kích thước ngày càng tăng của PLM (ví dụ: 175 tỷ tham số cho GPT-3 (Brown et al., 2020) và 530 tỷ tham số cho MT-NLG (Smith et al., 2022)), thậm chí bước điều chỉnh cũng trở nên tốn kém vì nó đòi hỏi lưu trữ một bản sao đầy đủ của trọng số mô hình cho mỗi tác vụ. Để giải quyết những thách thức này, các nghiên cứu gần đây đã phát triển các kỹ thuật điều chỉnh hiệu quả tham số (PEFT). Các phương pháp này thường hoạt động kém hơn so với việc điều chỉnh mô hình đầy đủ tiêu chuẩn, nhưng giảm đáng kể số lượng tham số có thể huấn luyện. Có nhiều loại phương pháp PEFT khác nhau, bao gồm prefix-tuning (Li and Liang, 2021) và prompt-tuning (Lester et al., 2021) để điều kiện hóa các mô hình ngôn ngữ đóng băng thông qua mô tả tác vụ ngôn ngữ tự nhiên, phép chiếu chiều thấp sử dụng adapter (Houlsby et al., 2019; Pfeiffer et al., 2020, 2021) và gần đây hơn là sử dụng xấp xỉ thứ hạng thấp (Hu et al., 2021). Hình 1 cho thấy hiệu suất của một số phương pháp PEFT phổ biến với số lượng tham số có thể điều chỉnh khác nhau. Chúng tôi quan sát thấy khoảng cách hiệu suất đáng kể so với điều chỉnh mô hình đầy đủ trong đó tất cả tham số PLM được cập nhật.

Trong bài báo này, chúng tôi trình bày AdaMix, một phương pháp hỗn hợp các module thích ứng, và cho thấy rằng nó vượt trội hơn các phương pháp PEFT SOTA cũng như điều chỉnh mô hình đầy đủ trong khi chỉ điều chỉnh 0.1-0.2% tham số PLM.

Trái ngược với các phương pháp PEFT truyền thống sử dụng một module thích ứng duy nhất trong mỗi lớp Transformer, AdaMix sử dụng nhiều module thích ứng học nhiều góc nhìn khác nhau của tác vụ đã cho. Để thiết kế hỗn hợp thích ứng này, chúng tôi lấy cảm hứng từ các mô hình mixture-of-experts (MoE) được kích hoạt thưa thớt. Trong các mô hình dày đặc truyền thống (ví dụ: BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020)), tất cả trọng số mô hình đều được kích hoạt cho mỗi ví dụ đầu vào. Các mô hình MoE tạo ra độ thưa thớt bằng cách chỉ kích hoạt một tập con các trọng số mô hình cho mỗi đầu vào.

Xem xét adapter (Houlsby et al., 2019), một trong những kỹ thuật PEFT phổ biến nhất, để minh họa phương pháp của chúng tôi. Một lớp feedforward (FFN) được giới thiệu để chiếu xuống biểu diễn ẩn về một chiều thấp d (còn gọi là chiều cổ chai) theo sau bởi một FFN chiếu lên khác để khớp với chiều của lớp tiếp theo. Thay vì sử dụng một adapter duy nhất, chúng tôi giới thiệu nhiều FFN project-up và project-down trong mỗi lớp Transformer. Chúng tôi định tuyến các ví dụ đầu vào đến một trong các FFN project-up và một trong các FFN project-down dẫn đến cùng lượng chi phí tính toán (FLOP) như khi sử dụng một adapter duy nhất. Đối với các phương pháp như LoRA (Hu et al., 2021), phân rã gradient của trọng số được huấn luyện trước thành các ma trận thứ hạng thấp (A và B), chúng tôi giới thiệu nhiều phân rã thứ hạng thấp và định tuyến các ví dụ đầu vào đến chúng tương tự như adapter.

Chúng tôi thảo luận về các cơ chế định tuyến khác nhau và cho thấy định tuyến ngẫu nhiên mang lại hiệu suất tốt trong khi loại bỏ nhu cầu giới thiệu bất kỳ tham số bổ sung nào cho việc lựa chọn module. Để giảm bớt bất ổn huấn luyện có thể phát sinh từ tính ngẫu nhiên trong việc lựa chọn các module thích ứng khác nhau trong các bước huấn luyện khác nhau, chúng tôi tận dụng chính quy hóa nhất quán và chia sẻ các module thích ứng trong quá trình định tuyến ngẫu nhiên.

Việc giới thiệu nhiều module thích ứng dẫn đến tăng số lượng tham số thích ứng. Điều này không tăng chi phí tính toán nhưng tăng chi phí lưu trữ. Để giải quyết vấn đề này, chúng tôi phát triển một cơ chế hợp nhất để kết hợp trọng số từ các module thích ứng khác nhau thành một module duy nhất trong mỗi lớp Transformer. Điều này cho phép chúng tôi giữ số lượng tham số thích ứng giống như của một module thích ứng duy nhất. Cơ chế hợp nhất của chúng tôi được lấy cảm hứng từ việc trung bình hóa trọng số mô hình model soups (Wortsman et al., 2022) và multiBERT (Sellam et al., 2022). Việc trung bình hóa trọng số của các mô hình với khởi tạo ngẫu nhiên khác nhau đã được chứng minh là cải thiện hiệu suất mô hình trong các nghiên cứu gần đây (Matena and Raffel, 2021; Neyshabur et al., 2020; Frankle et al., 2020) cho thấy các mô hình được tối ưu hóa nằm trong cùng một bồn chứa của cảnh quan lỗi. Trong khi các nghiên cứu trên hướng đến việc điều chỉnh các mô hình độc lập, chúng tôi mở rộng ý tưởng này sang điều chỉnh hiệu quả tham số với các module thích ứng được khởi tạo ngẫu nhiên và một mô hình ngôn ngữ đóng băng.

Nhìn chung, công việc của chúng tôi đóng góp những điểm sau:

(a) Chúng tôi phát triển một phương pháp mới AdaMix như một hỗn hợp các thích ứng cho điều chỉnh hiệu quả tham số (PEFT) của các mô hình ngôn ngữ lớn. Với bất kỳ phương pháp PEFT nào được lựa chọn như adapter và phân rã thứ hạng thấp, AdaMix cải thiện hiệu suất tác vụ hạ nguồn so với phương pháp PEFT cơ bản.

(b) AdaMix được huấn luyện với định tuyến ngẫu nhiên và hợp nhất module thích ứng để duy trì cùng chi phí tính toán (ví dụ: FLOP, #tham số thích ứng có thể điều chỉnh) và lợi ích của phương pháp PEFT cơ bản. Để hiểu rõ hơn cách AdaMix hoạt động, chúng tôi chứng minh mối liên hệ mạnh mẽ của nó với Mạng nơ-ron Bayes và ensemble mô hình.

(c) Chỉ bằng cách điều chỉnh 0.1-0.2% tham số của mô hình ngôn ngữ được huấn luyện trước, AdaMix là phương pháp PEFT đầu tiên vượt trội hơn các phương pháp điều chỉnh mô hình đầy đủ cho tất cả tác vụ NLU trên GLUE, và vượt trội hơn các phương pháp cạnh tranh khác cho các tác vụ NLG và NLU few-shot.

**Lợi ích thực tiễn của các phương pháp PEFT.** Lợi ích quan trọng nhất của các phương pháp PEFT đến từ việc giảm sử dụng bộ nhớ và lưu trữ. Đối với Transformer, việc tiêu thụ VRAM có thể được giảm đáng kể vì chúng ta không cần theo dõi trạng thái optimizer cho các tham số đóng băng. Các phương pháp PEFT cũng cho phép nhiều tác vụ chia sẻ cùng một bản sao của PLM đầy đủ (đóng băng). Do đó, chi phí lưu trữ để giới thiệu một tác vụ mới có thể được giảm lên đến 444 lần (từ 355MB xuống 0.8MB với bộ mã hóa RoBERTa-large trong cài đặt của chúng tôi).

Chúng tôi trình bày nền tảng về Mixture-of-Experts (MoE) và adapter trong Mục 2 của Phụ lục.

## 2 Nền tảng

### 2.1 Mixture-of-Experts

Mục tiêu của thiết kế mô hình được kích hoạt thưa thớt là hỗ trợ tính toán có điều kiện và tăng số lượng tham số của các mô hình nơ-ron như Transformer trong khi giữ số lượng phép toán dấu phẩy động (FLOP) cho mỗi ví dụ đầu vào không đổi. Các mô hình Transformer Mixture-of-Experts (MoE) (Shazeer et al., 2017; Fedus et al., 2021; Lepikhin et al., 2020; Zuo et al., 2021) đạt được điều này bằng cách sử dụng N mạng feed-forward (FFN), được gọi là "chuyên gia" ký hiệu là EN i=1, mỗi mạng với tập hợp trọng số học được riêng tính toán các biểu diễn khác nhau của một token đầu vào x dựa trên ngữ cảnh. Để thưa thớt hóa mạng để giữ FLOP không đổi, có một mạng cổng bổ sung G mà đầu ra của nó là một vectơ N-chiều thưa thớt để định tuyến mỗi token qua một số ít các chuyên gia này. Lưu ý rằng, một mô hình thưa thớt với N=1 tương ứng với chỉ một lớp FFN trong mỗi khối Transformer sẽ thu gọn về mô hình dày đặc truyền thống.

Xem xét xs là biểu diễn token đầu vào ở vị trí thứ s đến lớp MOE bao gồm các FFN chuyên gia {Eg}N i=1. Cũng xem xét win i và wout i là các ma trận chiếu đầu vào và đầu ra cho chuyên gia thứ i. Đầu ra chuyên gia Ei(xs) được cho bởi:

Ei(xs) = wout i GeLU(win i xs)                    (1)

Xem xét G(xs) là đầu ra của mạng cổng. Đầu ra của lớp MoE thưa thớt được cho bởi:

h(xs) = Σi G(xs)i Ei(xs)                           (2)

trong đó G(xs)i là logit thứ i của đầu ra G(xs) biểu thị xác suất lựa chọn chuyên gia Ei.

Để giữ số lượng FLOP trong Transformer thưa thớt bằng với Transformer dày đặc, cơ chế cổng có thể được ràng buộc để định tuyến mỗi token chỉ đến một chuyên gia FFN, tức là Σi Gt(xs)i = 1.

### 2.2 Adapter

Phương pháp chủ yếu cho thích ứng tác vụ là điều chỉnh tất cả các tham số có thể huấn luyện của PLM cho mỗi tác vụ. Điều này đặt ra những thách thức tài nguyên đáng kể cả trong quá trình huấn luyện và triển khai. Một nghiên cứu gần đây (Aghajanyan et al., 2021) cho thấy PLM có chiều nội tại thấp có thể khớp với hiệu suất của không gian tham số đầy đủ.

Để thích ứng PLM cho các tác vụ hạ nguồn với số lượng tham số nhỏ, adapter (Houlsby et al., 2019) gần đây đã được giới thiệu như một phương pháp thay thế cho việc điều chỉnh nhẹ.

Chiến lược điều chỉnh adapter khéo léo giới thiệu các tham số mới vào PLM gốc. Trong quá trình điều chỉnh, chỉ các tham số adapter được cập nhật trong khi giữ các tham số còn lại của PLM đóng băng. Adapter thường bao gồm hai lớp kết nối đầy đủ như được hiển thị trong Hình 3, trong đó lớp adapter sử dụng một phép chiếu xuống Wdown ∈ R d×r để chiếu biểu diễn đầu vào x xuống một không gian chiều thấp r (được gọi là chiều cổ chai) với d là chiều mô hình, theo sau bởi một hàm kích hoạt phi tuyến f(·), và một phép chiếu lên với Wup ∈ R r×d để chiếu các đặc trưng chiều thấp trở lại chiều gốc. Các adapter được bao quanh thêm bởi các kết nối dư.

Với thiết kế adapter trên có tham số θ, tập dữ liệu DK, một bộ mã hóa mô hình ngôn ngữ được huấn luyện trước enc với tham số θPLM, trong đó |θ| << |θPLM|, chúng ta muốn thực hiện tối ưu hóa sau để thích ứng mô hình hiệu quả:

θ* = argmin L(Dk; θPLM; θ)                          (3)

## 3 Mixture-of-Adaptations

Xem xét một tập hợp M module thích ứng được tiêm vào mỗi lớp Transformer, trong đó Aij: i ∈ {1...L}; j ∈ {1...M} biểu thị module thích ứng thứ j trong lớp Transformer thứ i. Để minh họa, chúng tôi sẽ xem xét adapter (Houlsby et al., 2019) như cơ chế điều chỉnh hiệu quả tham số (PEFT) cơ bản như một ví dụ chạy. Các nguyên tắc tương tự có thể được sử dụng cho cơ chế PEFT khác như LoRA (Hu et al., 2021) cho phân rã thứ hạng thấp như chúng tôi cho thấy trong thí nghiệm.

Chúng tôi áp dụng kiến trúc Transformer được sử dụng phổ biến (Vaswani et al., 2017) bao gồm L khối Transformer lặp lại, trong đó mỗi khối bao gồm một lớp con self-attention, một mạng feed-forward kết nối đầy đủ (FFN) và các kết nối dư xung quanh các lớp con theo sau bởi chuẩn hóa lớp. Mỗi module thích ứng Aij tương ứng với adapter (Houlsby et al., 2019) bao gồm một feedforward up Wup ij và một ma trận chiếu feedforward down Wdown ij.

### 3.1 Chính sách Định tuyến

Nghiên cứu gần đây như THOR (Zuo et al., 2021) đã chứng minh chính sách định tuyến ngẫu nhiên như định tuyến random hoạt động tốt như cơ chế định tuyến cổ điển như định tuyến Switch (Fedus et al., 2021) với các lợi ích sau. Vì các ví dụ đầu vào được định tuyến ngẫu nhiên đến các chuyên gia khác nhau, không cần cân bằng tải bổ sung vì mỗi chuyên gia có cơ hội bằng nhau được kích hoạt làm đơn giản hóa framework. Hơn nữa, không có tham số bổ sung, và do đó không có tính toán bổ sung, ở lớp Switch cho việc lựa chọn chuyên gia. Điều sau đặc biệt quan trọng trong cài đặt của chúng tôi cho điều chỉnh hiệu quả tham số để giữ các tham số và FLOP giống như của một module thích ứng duy nhất. Để phân tích hoạt động của AdaMix, chúng tôi chứng minh các kết nối với định tuyến ngẫu nhiên và trung bình hóa trọng số mô hình đến Mạng nơ-ron Bayes và ensemble mô hình trong Mục 3.5.

Trong chính sách định tuyến ngẫu nhiên cho AdaMix với adapter, ở bất kỳ bước huấn luyện nào, chúng tôi ngẫu nhiên lựa chọn một cặp ma trận chiếu feedforward up và feedforward down trong lớp Transformer thứ i như Ai = {Wup ij, Wdown ik} và Bi = {Wup ij', Wdown ik'} tương ứng. Với việc lựa chọn các module thích ứng Ai và Bi trong mỗi lớp Transformer trong mỗi bước, tất cả các đầu vào trong một batch được xử lý qua cùng một tập hợp các module. Với một biểu diễn đầu vào x trong một lớp Transformer cho trước, cặp module trên thực hiện các phép biến đổi sau:

x ← x + f(xWdown)Wup                              (4)

Định tuyến ngẫu nhiên như vậy cho phép các module thích ứng học các phép biến đổi khác nhau trong quá trình huấn luyện và thu được nhiều góc nhìn về tác vụ. Tuy nhiên, điều này cũng tạo ra thách thức về module nào sử dụng trong quá trình suy luận do giao thức định tuyến ngẫu nhiên trong huấn luyện. Chúng tôi giải quyết thách thức này bằng hai kỹ thuật sau cũng cho phép chúng tôi thu gọn các module thích ứng và có được cùng chi phí tính toán (FLOP, #tham số thích ứng có thể điều chỉnh) như của một module duy nhất.

### 3.2 Chính quy hóa Nhất quán

Xem xét A = {AL i=1} và B = {BL i=1} là các tập hợp module thích ứng (ví dụ: ma trận chiếu) được kích hoạt trong hai lần truyền xuôi ngẫu nhiên qua mạng cho một đầu vào x trên L lớp của Transformer. Mục tiêu của chính quy hóa nhất quán là cho phép các module thích ứng chia sẻ thông tin và ngăn chặn phân kỳ. Để đạt mục tiêu này, chúng tôi thêm loss nhất quán sau như một regularizer vào loss tối ưu hóa cụ thể tác vụ:

L = Σ^C_{c=1} I(x,c) log softmax(z^A_c(x)) + 1/2[KL(z^A_{(:)}(x)||z^B_{(:)}(x)) + KL(z^B_{(:)}(x)||z^A_{(:)}(x))]     (5)

trong đó I(x,c) là một chỉ số nhị phân (0 hoặc 1) nếu nhãn lớp c là phân loại đúng cho x và z^A_{(:)}(x) và z^B_{(:)}(x) là các logit dự đoán khi định tuyến qua hai tập hợp module thích ứng A và B tương ứng với KL biểu thị phân kỳ Kullback-Leibler. x là biểu diễn đầu vào từ PLM với tham số đóng băng và chỉ các tham số của module {Wup, Wdown} được cập nhật trong quá trình huấn luyện.

### 3.3 Hợp nhất Module thích ứng

Trong khi việc chính quy hóa trên giảm bớt sự không nhất quán trong việc lựa chọn module ngẫu nhiên trong quá trình suy luận, nó vẫn dẫn đến tăng chi phí phục vụ để lưu trữ nhiều module thích ứng. Các nghiên cứu trước đây trong việc điều chỉnh mô hình ngôn ngữ cho các tác vụ hạ nguồn đã cho thấy hiệu suất được cải thiện khi trung bình hóa trọng số của các mô hình khác nhau được điều chỉnh với các seed ngẫu nhiên khác nhau vượt trội hơn một mô hình được điều chỉnh duy nhất. Nghiên cứu gần đây (Wortsman et al., 2022) cũng đã cho thấy các mô hình được điều chỉnh khác nhau từ cùng một khởi tạo nằm trong cùng một bồn chứa lỗi thúc đẩy việc sử dụng tổng hợp trọng số cho tóm tắt tác vụ mạnh mẽ. Chúng tôi áp dụng và mở rộng các kỹ thuật trước đây cho việc điều chỉnh mô hình ngôn ngữ sang huấn luyện hiệu quả tham số của các module thích ứng đa góc nhìn.

Trái ngược với các kỹ thuật được đề cập trước đây như định tuyến ngẫu nhiên và chính quy hóa nhất quán được áp dụng ở giai đoạn huấn luyện, chúng tôi chỉ sử dụng hợp nhất thích ứng trong quá trình suy luận. Với một tập hợp các module thích ứng, Wup ij và Wdown ik cho i ∈ {1...L} và {j,k} ∈ {1...M}, chúng tôi đơn giản trung bình hóa trọng số của tất cả các module tương ứng (ví dụ: ma trận project-up hoặc project-down) trong mỗi lớp Transformer để thu gọn thành một module duy nhất {W'up i, W'down i}, trong đó:

W'up i ← 1/M Σ^M_{j=1} Wup ij     W'down i ← 1/M Σ^M_{j=1} Wdown ij     (6)

### 3.4 Chia sẻ Module thích ứng

Trong khi định tuyến ngẫu nhiên đến các module thích ứng đa góc nhìn tăng khả năng mô hình, nó cũng có thể ảnh hưởng đến các tác vụ hạ nguồn có ít dữ liệu được gắn nhãn để điều chỉnh nhiều tập hợp module thích ứng. Để giải quyết thách thức này, chúng tôi sử dụng một cơ chế khác để chia sẻ một số module thích ứng (ví dụ: các phép toán project-down hoặc project-up) để cải thiện hiệu quả huấn luyện. Trong cài đặt tiêu chuẩn cho adapter, chúng tôi chỉ chia sẻ các ma trận chiếu feedforward projection-up tức là Wup ij = Wup i. Chúng tôi điều tra các lựa chọn thiết kế này qua nghiên cứu ablation trong thí nghiệm của chúng tôi trong Mục 4.3 và Mục B trong Phụ lục.

### 3.5 Kết nối với Mạng nơ-ron Bayes và Ensemble Mô hình

Mạng nơ-ron Bayes (BNN) (Gal and Ghahramani, 2015) thay thế các tham số trọng số xác định của mô hình bằng một phân phối trên các tham số. Để suy luận, BNN trung bình hóa trên tất cả các trọng số có thể, còn được gọi là marginalization.

Xem xét fW(x) ∈ R^d là đầu ra d-chiều của một mạng nơ-ron như vậy trong đó likelihood mô hình được cho bởi p(y|fW(x)). Trong cài đặt của chúng tôi, W = ⟨Wup, Wdown⟩ cùng với các tham số PLM đóng băng được bỏ qua khỏi ký hiệu để đơn giản. Để phân loại, chúng ta có thể áp dụng thêm likelihood softmax vào đầu ra để có được: P(y=c|x, W) = softmax(fW(x)). Với một thực thể x, phân phối xác suất trên các lớp được cho bởi marginalization trên phân phối posterior như: p(y=c|x) = ∫W p(y=c|fW(x))p(W|X,Y)dW.

Điều này đòi hỏi trung bình hóa trên tất cả các trọng số mô hình có thể, điều này không thể thực hiện được trong thực tế. Do đó, một số phương pháp xấp xỉ đã được phát triển dựa trên các phương pháp suy luận biến phân và kỹ thuật chính quy hóa ngẫu nhiên sử dụng dropout. Trong nghiên cứu này, chúng tôi tận dụng một chính quy hóa ngẫu nhiên khác dưới dạng định tuyến ngẫu nhiên. Ở đây, mục tiêu là tìm một phân phối thay thế q(w) trong một họ phân phối có thể xử lý có thể thay thế posterior mô hình thực sự khó tính toán. Thay thế lý tưởng được xác định bằng cách tối thiểu hóa phân kỳ Kullback-Leibler (KL) giữa ứng cử viên và posterior thực.

Xem xét q(W) là chính sách định tuyến ngẫu nhiên lấy mẫu T trọng số mô hình được che {fWt}^T_{t=1} ~ q(W). Đối với các tác vụ phân loại, posterior xấp xỉ bây giờ có thể được thu được bằng tích phân Monte-Carlo (Gal et al., 2017) như:

p(y=c|x) ≈ ∫ p(y=c|fW(x))q(W)dW ≈ 1/T Σ^T_{t=1} p(y=c|fWt(x)) = 1/T Σ^T_{t=1} softmax(fWt(x))     (7)

Tuy nhiên, việc tính toán posterior xấp xỉ trên trong cài đặt của chúng tôi đòi hỏi lưu trữ tất cả các trọng số mô hình ngẫu nhiên Wt(x) điều này tăng chi phí phục vụ trong quá trình suy luận. Để giảm chi phí này, chúng tôi sử dụng kỹ thuật khác cho việc trung bình hóa trọng số thông qua hợp nhất module thích ứng trong quá trình suy luận.

Gọi L^AM_W = E_{x,y}[L(softmax(fW(x)), y)] biểu thị loss kỳ vọng với hợp nhất các trọng số thích ứng ngẫu nhiên với fW = 1/T Σ_t fWt (từ Phương trình 6) và L biểu thị loss cross-entropy. Xem xét L^Ens_W = E_{x,y}[L(1/T Σ^T_{t=1} softmax(fWt(x)), y)] biểu thị loss kỳ vọng từ ensemble mô hình ngẫu nhiên mức logit (từ Phương trình 7).

Nghiên cứu trước đây (Wortsman et al., 2022) cho thấy việc trung bình hóa trọng số của nhiều mô hình được điều chỉnh với các siêu tham số khác nhau cải thiện hiệu suất mô hình. Họ phân tích về mặt lý thuyết sự tương đồng trong loss giữa trung bình hóa trọng số (L^AM_W trong cài đặt của chúng tôi) và ensemble logit (L^Ens_W trong cài đặt của chúng tôi) như một hàm của độ phẳng của loss và độ tin cậy của dự đoán. Trong khi phân tích trên hướng đến việc trung bình hóa nhiều trọng số mô hình được điều chỉnh độc lập, chúng tôi có thể áp dụng phân tích tương tự trong cài đặt của chúng tôi hướng đến việc trung bình hóa nhiều trọng số thích ứng thu được ngẫu nhiên trong việc thu được loss L^AM_W thuận lợi. Hơn nữa, hợp nhất thích ứng giảm chi phí phục vụ trong quá trình suy luận vì chúng ta chỉ cần duy trì một bản sao của trọng số được hợp nhất thay vì ensemble logit đòi hỏi bản sao của tất cả các trọng số thích ứng.

## 4 Thí nghiệm

### 4.1 Cài đặt Thí nghiệm

**Tập dữ liệu.** Chúng tôi thực hiện thí nghiệm trên một loạt các tác vụ bao gồm tám tác vụ hiểu ngôn ngữ tự nhiên (NLU) trong benchmark General Language Understanding Evaluation (GLUE) (Wang et al., 2019) và ba tác vụ sinh ngôn ngữ tự nhiên (NLG), cụ thể là E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017) và DART (Nan et al., 2020). Đối với các tác vụ NLU và NLG, chúng tôi tuân theo cùng cài đặt như (Houlsby et al., 2019) và (Li and Liang, 2021; Hu et al., 2021), tương ứng.

**Baseline.** Chúng tôi so sánh AdaMix với việc điều chỉnh mô hình đầy đủ và một số phương pháp điều chỉnh hiệu quả tham số (PEFT) hiện đại, cụ thể là Pfeiffer Adapter (Pfeiffer et al., 2021), Houlsby Adapter (Houlsby et al., 2019), BitFit (Zaken et al., 2021), Prefix-tuning (Li and Liang, 2021), UNIPELT (Mao et al., 2021) và LoRA (Hu et al., 2021). Chúng tôi sử dụng BERT-base (Devlin et al., 2019) và RoBERTa-large (Liu et al., 2019) như bộ mã hóa cho các tác vụ NLU (kết quả trong Bảng 1 và Bảng 2), và GPT-2 (Brown et al., 2020) cho các tác vụ NLG (kết quả trong Bảng 3).

**Chi tiết triển khai AdaMix.** Chúng tôi triển khai AdaMix trong Pytorch và sử dụng gpu Tesla V100 cho thí nghiệm với cấu hình siêu tham số chi tiết được trình bày trong Mục D trong Phụ lục. AdaMix với adapter sử dụng chiều 16 và 48 với bộ mã hóa BERT-base và RoBERTa-large tuân theo cài đặt của (Hu et al., 2021; Mao et al., 2021) để so sánh công bằng. AdaMix với LoRA sử dụng thứ hạng r=4 tuân theo cài đặt của (Hu et al., 2021) để giữ cùng số lượng tham số thích ứng trong quá trình suy luận. Số lượng module thích ứng trong AdaMix được đặt là 4 cho tất cả các tác vụ và bộ mã hóa trừ khi được chỉ định khác. Tác động của chiều adapter và số lượng module thích ứng cho các tác vụ NLU được điều tra trong Bảng 9 và 10. Đối với hầu hết các thí nghiệm và phân tích ablation, chúng tôi báo cáo kết quả từ AdaMix với adapter cho các tác vụ NLU. Để chứng minh tính tổng quát của framework, chúng tôi báo cáo kết quả từ AdaMix với LoRA (Hu et al., 2021) như cơ chế PEFT cơ bản cho các tác vụ NLG.

### 4.2 Kết quả Chính

#### 4.2.1 Tác vụ NLU

Bảng 1 và 2 cho thấy so sánh hiệu suất giữa các mô hình PEFT với bộ mã hóa RoBERTa-large và BERT-base tương ứng. RoBERTa-large và BERT-base được điều chỉnh đầy đủ cung cấp hiệu suất trần. Chúng tôi quan sát AdaMix với mixture-of-adapter vượt trội đáng kể hơn các baseline hiện đại khác trong hầu hết các tác vụ với các bộ mã hóa khác nhau. AdaMix với adapter là phương pháp PEFT duy nhất vượt trội hơn việc điều chỉnh mô hình đầy đủ trên tất cả các tác vụ và điểm trung bình.

#### 4.2.2 Tác vụ NLG

AdaMix tận dụng hỗn hợp các thích ứng để cải thiện so với phương pháp PEFT cơ bản như được chứng minh trong Bảng 3 cho E2E NLG tức là AdaMix với LoRA và AdaMix với adapter vượt trội hơn LoRA (Hu et al., 2021) và adapter (Houlsby et al., 2019) tương ứng. Chúng tôi báo cáo kết quả trên DART và WebNLG trong Bảng 4 và 5 trong Phụ lục.

#### 4.2.3 NLU Few-shot

Trái ngược với cài đặt có giám sát đầy đủ trong các thí nghiệm trên, chúng tôi cũng thực hiện thí nghiệm few-shot trên sáu tác vụ GLUE tuân theo cùng cài đặt (ví dụ: số shot, phân chia train và test) và đánh giá như trong (Wang et al., 2021). Cấu hình thí nghiệm chi tiết được trình bày trong Mục A của Phụ lục. AdaMix sử dụng mixture-of-adapter với điều chỉnh dựa trên prompt (Gao et al., 2021).

Bảng 6 cho thấy so sánh hiệu suất giữa các phương pháp PEFT khác nhau với |K|=30 ví dụ được gắn nhãn với RoBERTa-large như bộ mã hóa đóng băng. Chúng tôi quan sát khoảng cách hiệu suất đáng kể đối với hầu hết các phương pháp PEFT với việc điều chỉnh dựa trên prompt mô hình đầy đủ tức là với tất cả tham số mô hình được cập nhật. AdaMix với adapter vượt trội hơn hiệu suất điều chỉnh mô hình đầy đủ cho NLU few-shot tương tự như trong cài đặt có giám sát đầy đủ. Lưu ý rằng AdaMix và LiST (Wang et al., 2021) sử dụng thiết kế adapter tương tự với điều chỉnh dựa trên prompt.

### 4.3 Nghiên cứu Ablation

Chúng tôi thực hiện tất cả phân tích ablation trên AdaMix với adapter cho điều chỉnh hiệu quả tham số.

**Phân tích hợp nhất thích ứng.** Trong nghiên cứu ablation này, chúng tôi không hợp nhất các module thích ứng và xem xét hai chiến lược định tuyến khác nhau tại thời điểm suy luận: (a) định tuyến ngẫu nhiên đầu vào đến bất kỳ module thích ứng nào, và (b) định tuyến cố định nơi chúng tôi định tuyến tất cả đầu vào đến module thích ứng đầu tiên trong AdaMix. Từ Bảng 7, chúng tôi quan sát AdaMix với hợp nhất thích ứng hoạt động tốt hơn so với bất kỳ biến thể nào khác không có cơ chế hợp nhất. Đáng chú ý, tất cả các biến thể AdaMix đều vượt trội hơn việc điều chỉnh mô hình đầy đủ.

**Trung bình hóa trọng số so với ensemble logit.** Chúng tôi so sánh AdaMix với một biến thể của ensemble logit, ký hiệu là AdaMix-Ensemble. Để thực hiện điều này, chúng tôi thực hiện bốn lần truyền định tuyến ngẫu nhiên qua mạng cho mỗi đầu vào (T=4) và trung bình hóa các logit từ các lần truyền khác nhau như logit dự đoán cuối cùng. Thời gian suy luận cho phương pháp ensemble này là 4× AdaMix. Chúng tôi chạy thí nghiệm lặp lại với ba seed khác nhau và báo cáo hiệu suất trung bình trong Bảng 7. Chúng tôi quan sát AdaMix với trung bình hóa trọng số thích ứng vượt trội hơn ensemble logit theo phân tích của chúng tôi (L^AM_W so với L^Ens_W) trong Mục 3.5.

**Phân tích chính quy hóa nhất quán.** Chúng tôi bỏ chính quy hóa nhất quán trong quá trình huấn luyện để ablation và chứng minh sự suy giảm hiệu suất đáng kể trong Bảng 8.

**Phân tích chia sẻ module thích ứng.** Chúng tôi loại bỏ chia sẻ module thích ứng trong AdaMix để ablation và giữ bốn bản sao khác nhau của các lớp FFN project-down và bốn project-up. Từ Bảng 8 chúng tôi quan sát khoảng cách hiệu suất giữa AdaMix và AdaMix w/o sharing tăng lên với việc giảm kích thước tập dữ liệu chứng minh tầm quan trọng của việc chia sẻ tham số cho các tác vụ ít tài nguyên (ví dụ: RTE, MRPC). Điều này được chứng minh thêm trong Hình 7 trong Phụ lục cho thấy hội tụ nhanh hơn và loss huấn luyện thấp hơn của AdaMix với chia sẻ so với không có chia sẻ với cùng số bước huấn luyện. Chúng tôi khám phá module thích ứng nào để chia sẻ (project-up so với project-down) trong Bảng 11 trong Phụ lục mô tả kết quả tương tự.

**Tác động của số lượng module thích ứng.** Trong nghiên cứu này, chúng tôi thay đổi số lượng module thích ứng trong AdaMix là 2, 4 và 8 trong quá trình huấn luyện. Bảng 9 cho thấy lợi ích giảm dần về hiệu suất tác vụ tổng hợp khi tăng số lượng module. Khi chúng tôi tăng độ thưa thớt và số lượng tham số có thể điều chỉnh bằng cách tăng số lượng module thích ứng, các tác vụ ít tài nguyên như RTE và SST-2 – với lượng dữ liệu được gắn nhãn hạn chế để điều chỉnh – suy giảm hiệu suất so với các tác vụ có nhiều tài nguyên như MNLI và QNLI.

**Tác động của chiều cổ chai adapter.** Bảng 10 cho thấy tác động của chiều cổ chai của adapter với các bộ mã hóa khác nhau trong AdaMix. Hiệu suất mô hình cải thiện với việc tăng số lượng tham số có thể huấn luyện bằng cách tăng chiều cổ chai với lợi ích giảm dần sau một điểm nhất định.

## 5 Nghiên cứu Liên quan

**Điều chỉnh hiệu quả tham số của PLM.** Các nghiên cứu gần đây về điều chỉnh hiệu quả tham số (PEFT) có thể được phân loại thô thành hai loại: (1) điều chỉnh một tập con các tham số hiện có bao gồm điều chỉnh đầu (Lee et al., 2019), điều chỉnh bias term (Zaken et al., 2021), (2) điều chỉnh các tham số mới được giới thiệu bao gồm adapter (Houlsby et al., 2019; Pfeiffer et al., 2020), prompt-tuning (Lester et al., 2021), prefix-tuning (Li and Liang, 2021) và thích ứng thứ hạng thấp (Hu et al., 2021). Trái ngược với các nghiên cứu trước đây hoạt động trên một module thích ứng duy nhất, AdaMix giới thiệu hỗn hợp các module thích ứng với định tuyến ngẫu nhiên trong quá trình huấn luyện và hợp nhất module thích ứng trong quá trình suy luận để giữ cùng chi phí tính toán như với một module duy nhất. Hơn nữa, AdaMix có thể được sử dụng trên bất kỳ phương pháp PEFT nào để tăng cường hiệu suất của nó.

**Mixture-of-Expert (MoE).** Shazeer et al., 2017 giới thiệu mô hình MoE với một mạng cổng duy nhất với định tuyến Top-k và cân bằng tải giữa các chuyên gia. Fedus et al., 2021 đề xuất các sơ đồ khởi tạo và huấn luyện cho định tuyến Top-1. Zuo et al., 2021 đề xuất chính quy hóa nhất quán cho định tuyến ngẫu nhiên; Yang et al., 2021 đề xuất định tuyến kTop-1 với expert-prototype, và Roller et al., 2021; Lewis et al., 2021 giải quyết các vấn đề cân bằng tải khác. Tất cả các nghiên cứu trên nghiên cứu MoE thưa thớt với việc huấn luyện trước toàn bộ mô hình từ đầu. Trái ngược, chúng tôi nghiên cứu thích ứng hiệu quả tham số của các mô hình ngôn ngữ được huấn luyện trước bằng cách chỉ điều chỉnh một số lượng rất nhỏ các tham số adapter thưa thớt.

**Trung bình hóa trọng số mô hình.** Các khám phá gần đây (Szegedy et al., 2016; Matena and Raffel, 2021; Wortsman et al., 2022; Izmailov et al., 2018) nghiên cứu tổng hợp mô hình bằng cách trung bình hóa tất cả trọng số mô hình. (Matena and Raffel, 2021) đề xuất hợp nhất các mô hình ngôn ngữ được huấn luyện trước được điều chỉnh trên các tác vụ phân loại văn bản khác nhau. (Wortsman et al., 2022) khám phá việc trung bình hóa trọng số mô hình từ các lần chạy độc lập khác nhau trên cùng một tác vụ với các cấu hình siêu tham số khác nhau. Trái ngược với các nghiên cứu trên về điều chỉnh mô hình đầy đủ, chúng tôi tập trung vào điều chỉnh hiệu quả tham số. Chúng tôi khám phá trung bình hóa trọng số để hợp nhất trọng số của các module thích ứng bao gồm các tham số có thể điều chỉnh nhỏ được cập nhật trong quá trình điều chỉnh mô hình trong khi giữ các tham số mô hình lớn cố định.

## 6 Kết luận

Chúng tôi phát triển một framework mới AdaMix cho điều chỉnh hiệu quả tham số (PEFT) của các mô hình ngôn ngữ lớn được huấn luyện trước (PLM). AdaMix tận dụng hỗn hợp các module thích ứng để cải thiện hiệu suất tác vụ hạ nguồn mà không tăng chi phí tính toán (ví dụ: FLOP, tham số) của phương pháp thích ứng cơ bản. Chúng tôi chứng minh AdaMix hoạt động và cải thiện trên các phương pháp PEFT khác nhau như adapter và phân rã thứ hạng thấp trên các tác vụ NLU và NLG.

Chỉ bằng cách điều chỉnh 0.1-0.2% tham số PLM, AdaMix vượt trội hơn việc điều chỉnh mô hình đầy đủ cập nhật tất cả tham số mô hình cũng như các phương pháp PEFT hiện đại khác.

## 7 Hạn chế

Phương pháp AdaMix được đề xuất có phần tốn kém về tính toán vì nó liên quan đến việc điều chỉnh các mô hình ngôn ngữ quy mô lớn. Chi phí huấn luyện của AdaMix được đề xuất cao hơn các phương pháp PEFT tiêu chuẩn vì quy trình huấn luyện liên quan đến nhiều bản sao của adapter. Dựa trên quan sát thực nghiệm của chúng tôi, số lượng vòng lặp huấn luyện cho AdaMix thường nằm trong khoảng 1-2 lần huấn luyện cho các phương pháp PEFT tiêu chuẩn. Điều này áp đặt tác động tiêu cực đến dấu chân carbon từ việc huấn luyện các mô hình được mô tả.

AdaMix trực giao với hầu hết các nghiên cứu điều chỉnh hiệu quả tham số (PEFT) hiện có và có thể cải thiện hiệu suất của bất kỳ phương pháp PEFT nào. Trong nghiên cứu này, chúng tôi khám phá hai phương pháp PEFT đại diện như adapter và LoRA nhưng chúng tôi đã không thí nghiệm với các kết hợp khác như prompt-tuning và prefix-tuning. Chúng tôi để lại những nghiên cứu đó cho công việc tương lai.

## 8 Lời cảm ơn

Các tác giả muốn cảm ơn các trọng tài ẩn danh vì những bình luận có giá trị và gợi ý hữu ích của họ và muốn cảm ơn Guoqing Zheng và Ruya Kang vì những bình luận sâu sắc về dự án. Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia Hoa Kỳ dưới các khoản tài trợ NSF-IIS 1747614 và NSF-IIS-2141037. Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này đều thuộc về (các) tác giả và không nhất thiết phản ánh quan điểm của Quỹ Khoa học Quốc gia.
