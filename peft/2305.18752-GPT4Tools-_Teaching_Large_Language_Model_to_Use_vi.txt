2.5%4.0%0.7%3.4%3.7% 2.3%3.7%
2.3%
2.7%
0.5%
3.8%
2.3%
2.3%
3.8%
2.2%
3.7%
2.3%
3.7%
2.3%
0.2%
2.7%
2.6%0.1%42.0%58.0%
42.0%
Phân phối Tập Huấn luyện 12.3%12.1%
12.3%
12.3%
9.0%
10.9%12.3%11.2%7.7%
92.3%7.7%
Phân phối Tập Kiểm tra

Phát hiện Đối tượng Đã cho
Phân đoạn Hình ảnh
Nhận Mô tả Ảnh
Tạo Hình ảnh Từ Văn bản Đầu vào Người dùng
Phát hiện Cạnh Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Canny
Dự đoán Độ sâu Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Độ sâu
Trả lời Câu hỏi Về Hình ảnh
Hướng dẫn Hình ảnh Sử dụng Văn bản
Phát hiện Phác thảo Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Phác thảo
Tạo Hình ảnh Có Điều kiện Trên Phân đoạn
Phát hiện Tư thế Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Tư thế
Phát hiện Hed Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Soft Hed Boundary
Dự đoán Bản đồ Bình thường Trên Hình ảnh
Tạo Hình ảnh Có Điều kiện Trên Bản đồ Bình thường
Phát hiện Đường trên Hình ảnh
Phân đoạn đối tượng đã cho
Loại bỏ Cái gì đó Từ Ảnh
Thay thế Cái gì đó Từ Ảnh
Mẫu Tiêu cực
Phát hiện Văn bản Trên Hình ảnh
Phát hiện Khuôn mặt
Nhận diện Khuôn mặt
Đánh giá Chất lượng Hình ảnh
Cắt Đối tượng Đã cho
Siêu Phân giải Hình ảnh
Phát hiện
Tạo Tài sản 3D Từ Văn bản Đầu vào Người dùng

Hình 7: Phân phối dữ liệu của GPT4Tools. Phần màu tím đề cập đến các mẫu tiêu cực, trong khi những phần khác là các mẫu tích cực.

A.1 Tập Huấn luyện
Tập huấn luyện của GPT4Tools có 71.4K dữ liệu tuân theo hướng dẫn, bao gồm 35.7K mục sử dụng công cụ. Lưu ý rằng những cặp hướng dẫn-phản hồi này được tạo ra từ 41K mục trong Y+S vì một số hành động đòi hỏi hai công cụ. Dữ liệu hướng dẫn trong tập huấn luyện liên quan đến 23 công cụ có tên được hiển thị trong Bảng 5 (được đánh dấu màu xám). Phân phối của 23 công cụ này được minh họa ở bên trái Hình 7. Chúng tôi sử dụng tập huấn luyện này để hướng dẫn mô hình ngôn ngữ gọi công cụ.

A.2 Tập Đánh giá.
Tập đánh giá bao gồm hai phần: tập xác thực và tập kiểm tra.

Xác thực. Tập xác thực có tổng cộng 1170 mẫu, bao gồm cùng các công cụ với tập huấn luyện. Số lượng của mỗi công cụ gần như là 50. Tập này chứa một số mẫu được tăng cường như tập huấn luyện. Do đó, nó được sử dụng để xác minh hiệu quả của mô hình ngôn ngữ trong việc hiểu công cụ sau khi tinh chỉnh với tập huấn luyện.

Kiểm tra. Tập kiểm tra bao gồm 8 công cụ chưa thấy bởi tập huấn luyện. Tất cả tên công cụ chưa thấy được đánh dấu màu đen và hiển thị trong Bảng 5, và định nghĩa chi tiết của chúng được hiển thị trong Bảng 6. Tổng số mẫu là 652, phân phối của chúng được hiển thị ở bên phải Hình 7. Vì tập này chỉ liên quan đến các mẫu một lượt, nó được sử dụng để đánh giá khả năng zero-shot trong việc gọi công cụ bởi mô hình ngôn ngữ.

B Prompt
Prompt Công cụ. GPT4Tools được đề xuất hỗ trợ 31 công cụ, bao gồm 23 công cụ được định nghĩa trong Visual ChatGPT [5] và 8 công cụ mới. Chúng phụ thuộc vào các mô hình tạo hình ảnh (ví dụ: ControlNet [44], Stable Diffusion [46], InstructPix2Pix [47], và Shape-E [48]), và các mô hình hiểu hình ảnh (ví dụ: SAM [49], BLIP [22], MMDetection [50], MMOCR [51], MMagic [52], Face Recognition², GroundingDINO [53], và những cái khác [54–66].). Tất cả tên công cụ được tóm tắt trong Bảng 5, trong đó văn bản màu đen là các công cụ mới được định nghĩa. Mô tả chi tiết của các công cụ mới được minh họa trong Bảng 6, trong đó prompt định nghĩa tình huống sử dụng của công cụ và đối số của nó.

Prompt Tạo. Chúng tôi khuyến khích GPT-3.5 (gpt-3.5-turbo) [3] tạo ra dữ liệu tuân theo hướng dẫn bằng cách sử dụng prompt được nêu trong Bảng 7. Sau đó, chúng tôi lọc bỏ các hướng dẫn nhiễu, như được minh họa trong Bảng 8. Dựa trên dữ liệu được giữ lại, chúng tôi thực hiện tăng cường theo các bước được mô tả trong § 3.1, dẫn đến tập dữ liệu liên quan đến công cụ.

Prompt Sử dụng Công cụ. Trong quá trình trả lời lệnh người dùng, chúng tôi khuyến khích mô hình ngôn ngữ được tinh chỉnh gọi công cụ bằng prompt được hiển thị trong Bảng 9. Trong prompt này, <nội dung hình ảnh> sẽ được

²https://github.com/ageitgey/face_recognition

--- TRANG 12 ---
Bảng 6: Chi tiết các công cụ mới.

Số | Tên Công cụ | Đầu vào | Đầu ra | Prompt
1 | Phát hiện Văn bản Trên Hình ảnh | đường dẫn hình ảnh | văn bản trên hình ảnh | Hữu ích khi bạn muốn phát hiện văn bản trong hình ảnh. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
2 | Phát hiện | đường dẫn hình ảnh | hộp giới hạn của các đối tượng | Hữu ích khi bạn muốn phát hiện tất cả các đối tượng của hình ảnh, nhưng không phát hiện một đối tượng nhất định theo văn bản. như: phát hiện tất cả các đối tượng trong hình ảnh này, hoặc phát hiện hình ảnh này. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
3 | Siêu Phân giải Hình ảnh | đường dẫn hình ảnh | đường dẫn hình ảnh | Hữu ích khi bạn muốn tăng cường độ phân giải và chất lượng của hình ảnh độ phân giải thấp. như: tăng cường hình ảnh này, khôi phục hình ảnh này. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
4 | Tạo Tài sản 3D Từ Văn bản Đầu vào Người dùng | văn bản | đường dẫn hình ảnh | Hữu ích khi bạn muốn tạo ra một tài sản 3D từ văn bản đầu vào của người dùng và lưu nó vào một tệp. như: tạo ra một tài sản 3D của một đối tượng hoặc cái gì đó. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho văn bản được sử dụng để tạo ra tài sản 3D.
5 | Cắt Đối tượng Đã cho | đường dẫn hình ảnh, tên đối tượng | đường dẫn hình ảnh | Hữu ích khi bạn muốn cắt các đối tượng đã cho trong bức tranh. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path, mô tả văn bản của đối tượng cần được cắt.
6 | Đánh giá Chất lượng Hình ảnh | đường dẫn hình ảnh | điểm chất lượng | Hữu ích khi bạn muốn đưa ra một điểm chất lượng cho hình ảnh đầu vào. như: đánh giá một điểm chất lượng cho hình ảnh này, điểm chất lượng của hình ảnh này là gì, hoặc bạn có thể đưa ra một chất lượng cho hình ảnh này. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
7 | Nhận diện Khuôn mặt | đường dẫn hình ảnh | văn bản | Hữu ích khi bạn chỉ muốn nhận diện khuôn mặt trong bức tranh. như: nhận diện ai xuất hiện trong ảnh. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
8 | Phát hiện Khuôn mặt | đường dẫn hình ảnh | đường dẫn hình ảnh | Hữu ích khi bạn chỉ muốn phát hiện hoặc gắn thẻ khuôn mặt trong bức tranh. như: tìm tất cả các khuôn mặt xuất hiện trong bức tranh. gắn thẻ ai đó trong bức tranh. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.

Bảng 7: Prompt Tạo. Trong quá trình tạo, <caption> sẽ được thay thế bằng chú thích ground-truth và hộp giới hạn. Từ màu xanh lá cây là các hướng dẫn mong muốn.

Cho một hình ảnh có đường dẫn hình ảnh là example.png. Chú thích hình ảnh: <caption>. Chú thích hình ảnh bao gồm mô tả hình ảnh chi tiết và mỗi đối tượng được ghép nối với hộp giới hạn (x1, y1, x2, y2). Đối với hộp giới hạn, (x1, y1) đề cập đến trên trái, và (x2, y2) đề cập đến dưới phải. x1 nhỏ hơn x2, và y1 nhỏ hơn y2.

Dưới đây là N công cụ thị giác. Mỗi công cụ được định nghĩa là "<tên công cụ>: <tình huống sử dụng>, và <đối số>". Vui lòng tạo ra 1 hướng dẫn thị giác cho mỗi công cụ, vì vậy bạn cần tạo ra tổng cộng N hướng dẫn thị giác.

Các hướng dẫn được tạo ra phải tuân theo định dạng "<hướng dẫn>, [<tên công cụ>, <đối số>]". Mỗi hướng dẫn phải liên quan đến chú thích và có thể được giải quyết bởi công cụ. Bạn không thể sửa đổi "<tên công cụ>", hoặc thêm bất kỳ công cụ giả nào khác không được định nghĩa. Bạn phải giữ "<đối số>" chính xác.

Công cụ:
<tên công cụ>: <tình huống sử dụng>, <đối số>

Lưu ý rằng các hướng dẫn thị giác được tạo ra của bạn phải liên quan cực kỳ đến chú thích hình ảnh. Vui lòng tạo ra các hướng dẫn phức tạp và lừa đảo càng nhiều càng tốt.

--- TRANG 13 ---
Bảng 8: Các trường hợp nhiễu trong quá trình tạo. (✗) chỉ ra các ví dụ nhiễu, trong khi (✔) chỉ ra các ví dụ đã được sửa.

Lỗi Định dạng
(✗) Phân đoạn cậu bé đang vung gậy [Phân đoạn Đối tượng Đã cho, "example.jpg, cậu bé đang vung gậy"] (Hướng dẫn không được phân tách bằng dấu phẩy.)
(✔) Phân đoạn cậu bé đang vung gậy, [Phân đoạn Đối tượng Đã cho, "example.jpg, cậu bé đang vung gậy"]

Lỗi Đối số
(✗) Làm cho hình ảnh trông như một bức tranh, [Hướng dẫn Hình ảnh Sử dụng Văn bản, "bức tranh"]
(✔) Làm cho hình ảnh trông như một bức tranh, [Hướng dẫn Hình ảnh Sử dụng Văn bản, "example.png, bức tranh"]

Lỗi Công cụ
(✗) Tạo ra một hình ảnh thực của màn hình bánh và bánh từ một phác thảo, [Tạo Hình ảnh Có Điều kiện Trên Canny, "example.png, phác thảo của màn hình bánh và bánh"]
(✔) Tạo ra một hình ảnh thực của màn hình bánh và bánh từ một phác thảo, [Tạo Hình ảnh Có Điều kiện Trên Phác thảo, "example.png, phác thảo của màn hình bánh và bánh"]

thay thế bằng chú thích hình ảnh được dự đoán nếu <đầu vào người dùng> yêu cầu nội dung hình ảnh như điều kiện tiên quyết.

C Nghiên cứu Trường hợp
Nhiễu Trong Quá trình Tạo Hướng dẫn. Mặc dù ChatGPT [3] hoặc GPT-4 [4] đã chứng minh khả năng tạo ra dữ liệu chất lượng cao [39,40], vẫn có một số nhiễu trong dữ liệu được tạo ra. Ví dụ, Bảng 8 cho thấy ba loại trường hợp có nhiễu, bao gồm mẫu có lỗi định dạng, mẫu có lỗi đối số và mẫu được gán công cụ sai. Do đó, một bước lọc thực tế và hiệu quả là cần thiết khi sử dụng dữ liệu được tạo ra bởi các mô hình ngôn ngữ lớn.

Trường hợp Xấu của GPT-3.5. Như được hiển thị trong Bảng 10 và 11, GPT-3.5 [3] gọi công cụ sai để phản hồi lệnh người dùng. Do đó, khi sử dụng một mô hình ngôn ngữ như một bộ điều khiển để xây dựng một mô hình tổng quát, nên sử dụng GPT4Tools của chúng tôi để tăng cường độ chính xác của các hành động mô hình ngôn ngữ hơn nữa.

D Cài đặt Thí nghiệm
Trong § 4, chúng tôi đánh giá khả năng sử dụng công cụ của mô hình ngôn ngữ bằng cách sử dụng một tập dữ liệu tự xây dựng. Cấu hình tinh chỉnh được ghi lại trong Bảng 12.

--- TRANG 14 ---
Bảng 9: Prompt Sử dụng Công cụ. Trong quá trình suy luận, <nội dung hình ảnh> sẽ được thay thế bằng kết quả từ công cụ chú thích hình ảnh, và <đầu vào người dùng> sẽ được điền với lệnh người dùng.

GPT4Tools có thể xử lý nhiều nhiệm vụ văn bản và thị giác khác nhau, như trả lời câu hỏi và cung cấp giải thích và thảo luận sâu sắc. Nó tạo ra văn bản giống con người và sử dụng công cụ để hiểu hình ảnh một cách gián tiếp. Khi đề cập đến hình ảnh, GPT4Tools tuân theo các quy tắc tên tệp nghiêm ngặt. Để hoàn thành các nhiệm vụ thị giác, GPT4Tools sử dụng công cụ và trung thành với đầu ra quan sát. Người dùng có thể cung cấp hình ảnh mới cho GPT4Tools với một mô tả, nhưng công cụ phải được sử dụng cho các nhiệm vụ tiếp theo.

Công cụ:
<tên công cụ>: <tình huống sử dụng>, <đối số>

Để sử dụng một công cụ, vui lòng sử dụng định dạng sau:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: hành động cần thực hiện, phải là một trong <danh sách tên công cụ>
Đầu vào Hành động: đầu vào cho hành động
Quan sát: kết quả của hành động

Khi bạn có phản hồi để nói với Con người, hoặc nếu bạn không cần sử dụng công cụ, bạn phải sử dụng định dạng:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Không
AI: [phản hồi của bạn ở đây]

Tuân theo các quy tắc tên tệp và không tạo ra tên tệp không tồn tại giả. Nhớ cung cấp tên tệp hình ảnh một cách trung thành từ quan sát công cụ cuối cùng.

Cuộc trò chuyện trước đây:
Con người: Cung cấp một hình ảnh có tên . Mô tả: <nội dung hình ảnh>
AI: Đã nhận.
Đầu vào mới: <đầu vào người dùng>

GPT4Tools cần sử dụng công cụ để quan sát hình ảnh, không tưởng tượng trực tiếp chúng. Suy nghĩ và quan sát trong cuộc trò chuyện chỉ hiển thị với GPT4Tools. Khi trả lời câu hỏi của con người, lặp lại thông tin quan trọng. Hãy suy nghĩ từng bước.

--- TRANG 15 ---
Bảng 10: Ví dụ không chính xác từ GPT-3.5 (text-davinci-003) [3].

Hướng dẫn:
GPT4Tools có thể xử lý nhiều nhiệm vụ văn bản và thị giác khác nhau, như trả lời câu hỏi và cung cấp giải thích và thảo luận sâu sắc. Nó tạo ra văn bản giống con người và sử dụng công cụ để hiểu hình ảnh một cách gián tiếp. Khi đề cập đến hình ảnh, GPT4Tools tuân theo các quy tắc tên tệp nghiêm ngặt. Để hoàn thành các nhiệm vụ thị giác, GPT4Tools sử dụng công cụ và trung thành với đầu ra quan sát. Người dùng có thể cung cấp hình ảnh mới cho GPT4Tools với một mô tả, nhưng công cụ phải được sử dụng cho các nhiệm vụ tiếp theo.

Công cụ:
> Nhận Mô tả Ảnh: hữu ích khi bạn muốn biết có gì bên trong ảnh. nhận image_path làm đầu vào. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
> Trả lời Câu hỏi Về Hình ảnh: hữu ích khi bạn cần một câu trả lời cho một câu hỏi dựa trên một hình ảnh. như: màu nền của hình ảnh cuối cùng là gì, có bao nhiêu con mèo trong hình này, có gì trong hình này. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path và câu hỏi.
> Phát hiện Đối tượng Đã cho: hữu ích khi bạn chỉ muốn phát hiện hoặc tìm ra các đối tượng đã cho trong bức tranh. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path, mô tả văn bản của đối tượng cần được tìm.
> Tạo Hình ảnh Có Điều kiện Trên Tư thế: hữu ích khi bạn muốn tạo ra một hình ảnh thực mới từ cả mô tả người dùng và một hình ảnh tư thế con người. như: tạo ra một hình ảnh thực của một con người từ hình ảnh tư thế con người này, hoặc tạo ra một hình ảnh thực mới của một con người từ tư thế này. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path và mô tả người dùng.
> Tạo Hình ảnh Có Điều kiện Trên Phác thảo: hữu ích khi bạn muốn tạo ra một hình ảnh thực mới từ cả mô tả người dùng và một hình ảnh phác thảo hoặc một hình ảnh sketch. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path và mô tả người dùng.

Để sử dụng một công cụ, vui lòng sử dụng định dạng sau:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: hành động cần thực hiện, phải là một trong [Nhận Mô tả Ảnh, Trả lời Câu hỏi Về Hình ảnh, Phát hiện Đối tượng Đã cho, Tạo Hình ảnh Có Điều kiện Trên Tư thế, Tạo Hình ảnh Có Điều kiện Trên Phác thảo]
Đầu vào Hành động: đầu vào cho hành động
Quan sát: kết quả của hành động

Khi bạn có phản hồi để nói với Con người, hoặc nếu bạn không cần sử dụng công cụ, bạn phải sử dụng định dạng:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Không
AI: [phản hồi của bạn ở đây]

Tuân theo các quy tắc tên tệp và không tạo ra tên tệp không tồn tại giả. Nhớ cung cấp tên tệp hình ảnh một cách trung thành từ quan sát công cụ cuối cùng.

Cuộc trò chuyện trước đây:
Con người: Cung cấp một hình ảnh có tên examples/hybowtyx.png. Mô tả: Một bồn tắm mở với cửa vách kính trượt. Một phòng tắm với tường xám và cửa trượt trong suốt. Một vòi sen và bồn tắm có thể tiếp cận cho người khuyết tật với cửa kính. Một kết hợp bồn tắm vòi sen với vòi sen cầm tay và giá đỡ. Một vòi sen kín với cửa sổ và bồn tắm. Hiểu hình ảnh bằng cách sử dụng công cụ.
AI: Đã nhận.
Đầu vào mới: Có những đối tượng gì trên bồn rửa?

GPT4Tools cần sử dụng công cụ để quan sát hình ảnh, không tưởng tượng trực tiếp chúng. Suy nghĩ và quan sát trong cuộc trò chuyện chỉ hiển thị với GPT4Tools. Khi trả lời câu hỏi của con người, lặp lại thông tin quan trọng. Hãy suy nghĩ từng bước.

Suy nghĩ: Tôi có cần sử dụng công cụ không?

(✗)Phản hồi:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Nhận Mô tả Ảnh
Đầu vào Hành động: examples/hybowtyx.png
Quan sát: Hình ảnh chứa một bồn tắm với cửa vách kính trượt, một phòng tắm với tường xám, cửa trượt trong suốt, vòi sen và bồn tắm có thể tiếp cận cho người khuyết tật với cửa kính, kết hợp bồn tắm vòi sen với vòi sen cầm tay và giá đỡ, và một vòi sen kín với cửa sổ và bồn tắm.

(✔)Phản hồi Ground-truth:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Trả lời Câu hỏi Về Hình ảnh
Đầu vào Hành động: examples/hybowtyx.png, có những đối tượng gì trên bồn rửa
Quan sát: [đầu ra từ công cụ vqa]

--- TRANG 16 ---
Bảng 11: Ví dụ không chính xác từ GPT-3.5 (text-davinci-003) [3].

Hướng dẫn:
GPT4Tools có thể xử lý nhiều nhiệm vụ văn bản và thị giác khác nhau, như trả lời câu hỏi và cung cấp giải thích và thảo luận sâu sắc. Nó tạo ra văn bản giống con người và sử dụng công cụ để hiểu hình ảnh một cách gián tiếp. Khi đề cập đến hình ảnh, GPT4Tools tuân theo các quy tắc tên tệp nghiêm ngặt. Để hoàn thành các nhiệm vụ thị giác, GPT4Tools sử dụng công cụ và trung thành với đầu ra quan sát. Người dùng có thể cung cấp hình ảnh mới cho GPT4Tools với một mô tả, nhưng công cụ phải được sử dụng cho các nhiệm vụ tiếp theo.

Công cụ:
> Phát hiện Văn bản Trên Hình ảnh: hữu ích khi bạn muốn phát hiện văn bản trong hình ảnh. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
> Loại bỏ Cái gì đó Từ Ảnh: hữu ích khi bạn muốn loại bỏ và đối tượng hoặc cái gì đó từ ảnh từ mô tả hoặc vị trí của nó. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path và đối tượng cần được loại bỏ.
> Phát hiện Hed Trên Hình ảnh: hữu ích khi bạn muốn phát hiện ranh giới hed mềm của hình ảnh. như: phát hiện ranh giới hed mềm của hình ảnh này, hoặc phát hiện ranh giới hed trên hình ảnh, hoặc thực hiện phát hiện ranh giới hed trên hình ảnh này, hoặc phát hiện hình ảnh ranh giới hed mềm của hình ảnh này. Đầu vào cho công cụ này phải là một chuỗi, đại diện cho image_path.
> Trả lời Câu hỏi Về Hình ảnh: hữu ích khi bạn cần một câu trả lời cho một câu hỏi dựa trên một hình ảnh. như: màu nền của hình ảnh cuối cùng là gì, có bao nhiêu con mèo trong hình này, có gì trong hình này. Đầu vào cho công cụ này phải là một chuỗi được phân tách bằng dấu phẩy của hai, đại diện cho image_path và câu hỏi.

Để sử dụng một công cụ, vui lòng sử dụng định dạng sau:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: hành động cần thực hiện, phải là một trong [Phát hiện Văn bản Trên Hình ảnh, Loại bỏ Cái gì đó Từ Ảnh, Phát hiện Hed Trên Hình ảnh, Cắt Hình ảnh, Trả lời Câu hỏi Về Hình ảnh]
Đầu vào Hành động: đầu vào cho hành động
Quan sát: kết quả của hành động

Khi bạn có phản hồi để nói với Con người, hoặc nếu bạn không cần sử dụng công cụ, bạn phải sử dụng định dạng:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Không
AI: [phản hồi của bạn ở đây]

Tuân theo các quy tắc tên tệp và không tạo ra tên tệp không tồn tại giả. Nhớ cung cấp tên tệp hình ảnh một cách trung thành từ quan sát công cụ cuối cùng.

Cuộc trò chuyện trước đây:
Cung cấp một hình ảnh có tên image/wuspouwe.png. Mô tả: Một người đàn ông đang trượt tuyết xuống dốc có tuyết phủ. Một người đàn ông đang trượt tuyết xuống núi đầy tuyết. Một người đang trượt tuyết trong khu vực có tuyết và cây cối. Một người đàn ông mặc áo khoác đỏ đang trượt tuyết xuống đồi. Một người đàn ông đang trượt tuyết một mình với cây cối ở phía sau.
AI: Đã nhận.
Đầu vào mới: Màu của áo khoác người đàn ông là gì?

GPT4Tools cần sử dụng công cụ để quan sát hình ảnh, không tưởng tượng trực tiếp chúng. Suy nghĩ và quan sát trong cuộc trò chuyện chỉ hiển thị với GPT4Tools. Khi trả lời câu hỏi của con người, lặp lại thông tin quan trọng. Hãy suy nghĩ từng bước.

Suy nghĩ: Tôi có cần sử dụng công cụ không?

(✗)Phản hồi:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Phát hiện Hed Trên Hình ảnh
Đầu vào Hành động: image/wuspouwe.png
Quan sát: Áo khoác của người đàn ông màu đỏ.

(✔)Phản hồi Ground-truth:
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Trả lời Câu hỏi Về Hình ảnh
Đầu vào Hành động: image/wuspouwe.png, màu của áo khoác người đàn ông là gì?
Quan sát: [đầu ra từ công cụ vqa]

--- TRANG 17 ---
Bảng 12: Cấu hình tinh chỉnh.

Siêu tham số | Vicuna [12] & LLaMA [37] | OPT [13]
bộ tối ưu | AdamW [45] | AdamW [45]
tỷ lệ học | 3e-4 | 1.2e-4
bước khởi động | 100 | 100
suy giảm trọng số | 0.0 | 0.0
động lượng bộ tối ưu | β1,β2=0.9, 0.999 | β1,β2=0.9, 0.999
kích thước batch | 512 | 512
epoch | 3 | 3
độ dài tối đa | 2048 | 2048
chiều attention LoRA [38] (r) | 16 | 16
scaling alpha LoRA [38] (α) | 16 | 16
dropout LoRA [38] | 0.05 | 0.05

--- TRANG 18 ---
Tài liệu tham khảo
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 1, 4

[2]Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. 1, 3

[3] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. 1, 2, 3, 4, 6, 7, 11, 13, 15, 16

[4] OpenAI. Gpt-4 technical report, 2023. 1, 3, 4, 13

[5]Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 1, 2, 3, 7, 9, 10, 11

[6]Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 1, 2, 3

[7]Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021. 2, 3

[8]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In ICLR, 2017. 6

[9]Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. 3

[10] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022. 2, 3

[11] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 2, 3, 4

[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/, 2023. 2, 3, 4, 7, 8, 9, 17

[13] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 2, 4, 7, 17

[14] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. 2, 3, 4

[15] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, 2021. 2, 4

[16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 9118–9147. PMLR, 2022. 2, 4

[17] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of GPT-3 for few-shot knowledge-based VQA. In AAAI, pages 3081–3089. AAAI Press, 2022. 2, 4

[18] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In CVPR, pages 16783–16794. IEEE, 2022. 2

--- TRANG 19 ---
[19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. 2

[20] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and Geoffrey E. Hinton. A unified sequence interface for vision tasks. In NeurIPS, 2022. 2

[21] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. 2

[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 12888–12900. PMLR, 2022. 2, 11

[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.

[24] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot learning. In NeurIPS, 2022.

[25] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2

[26] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 23318–23340. PMLR, 2022. 2

[27] Alexander Kolesnikov, André Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby. Uvim: A unified modeling approach for vision with learned guiding codes. In NeurIPS, 2022. 2

[28] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. In EMNLP (Findings), pages 2856–2878, 2021. 3

[29] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR. OpenReview.net, 2022. 3

[30] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 3

[31] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. In ACL, pages 93–104. Association for Computational Linguistics, 2022.

[32] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 3

[33] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. 3

[34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 3, 4

--- TRANG 20 ---
[35] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.

[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3, 4, 9, 10

[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 7, 17

[38] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3, 6, 7, 17

[39] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056, 2023. 4, 13

[40] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. 4, 5, 13

[41] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP, pages 3980–3990, 2019. 4

[42] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255, 2009. 6

[43] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740–755, 2014. 6

[44] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 6, 11

[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 7, 17

[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10674–10685, 2022. 11

[47] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 11

[48] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 11

[49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 11

[50] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. 11

[51] Zhanghui Kuang, Hongbin Sun, Zhizhong Li, Xiaoyu Yue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei, Yiqin Zhu, Tong Gao, Wenwei Zhang, Kai Chen, Wayne Zhang, and Dahua Lin. Mmocr: A comprehensive toolbox for text detection, recognition and understanding. arXiv preprint arXiv:2108.06543, 2021. 11

[52] MMagic Contributors. MMagic: OpenMMLab multimodal advanced, generative, and intelligent creation toolbox. https://github.com/open-mmlab/mmagic, 2023. 11

[53] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 11

[54] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In CVPR, 2021. 11

[55] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning dynamic routing for semantic segmentation. In CVPR, 2020.

--- TRANG 21 ---
[56] Lin Song, Shiwei Zhang, Gang Yu, and Hongbin Sun. Tacnet: Transition-aware context network for spatio-temporal action detection. In CVPR, 2019.

[57] Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, and Nanning Zheng. Learnable tree filter for structure-preserving feature transform. NIPS, 2019.

[58] Shiwei Zhang, Lin Song, Changxin Gao, and Nong Sang. Glnet: Global local network for weakly supervised action localization. IEEE Transactions on Multimedia, 22(10):2610–2622, 2019.

[59] Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. Fine-grained dynamic head for object detection. NIPS, 2020.

[60] Jianwen Jiang, Yu Cao, Lin Song, Shiwei Zhang, Yunkai Li, Ziyao Xu, Qian Wu, Chuang Gan, Chi Zhang, and Gang Yu. Human centric spatio-temporal action localization. In ActivityNet Workshop on CVPR, 2018.

[61] Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. NIPS, 2020.

[62] Lin Song, Songyang Zhang, Songtao Liu, Zeming Li, Xuming He, Hongbin Sun, Jian Sun, and Nanning Zheng. Dynamic grained encoder for vision transformers. NIPS, 2021.

[63] Jinrong Yang, Lin Song, Songtao Liu, Zeming Li, Xiaoping Li, Hongbin Sun, Jian Sun, and Nanning Zheng. Dbq-ssd: Dynamic ball query for efficient 3d object detection. arXiv preprint arXiv:2207.10909, 2022.

[64] Songyang Zhang, Lin Song, Songtao Liu, Zheng Ge, Zeming Li, Xuming He, and Jian Sun. Workshop on autonomous driving at cvpr 2021: Technical report for streaming perception challenge. arXiv preprint arXiv:2108.04230, 2021.

[65] Rui Yang, Lin Song, Yixiao Ge, and Xiu Li. Boxsnake: Polygonal instance segmentation with box supervision. arXiv preprint arXiv:2303.11630, 2023.

[66] Xuchong Zhang, Hongbin Sun, Shiqiang Chen, Lin Song, and Nanning Zheng. Nipm-swmf: Toward efficient fpga design for high-definition large-disparity stereo matching. IEEE Transactions on Circuits and Systems for Video Technology, 29(5):1530–1543, 2018. 11
