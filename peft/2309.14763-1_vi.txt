# 2309.14763.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.14763.pdf
# Kích thước tệp: 505328 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
1
ConPET: Điều chỉnh Tham số Hiệu quả Liên tục cho
Mô hình Ngôn ngữ Lớn
Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun và Tao Yang
Tóm tắt —Học liên tục đòi hỏi việc thích ứng liên tục của các mô hình với những nhiệm vụ mới nổi lên trong khi giảm thiểu việc quên thảm khốc các nhiệm vụ cũ. Điều này cực kỳ khó khăn đối với các mô hình ngôn ngữ lớn (LLMs) với việc điều chỉnh tham số đầy đủ do chi phí tính toán cao, tiêu thụ bộ nhớ và vấn đề quên. Được truyền cảm hứng bởi sự thành công của điều chỉnh tham số hiệu quả (PET), chúng tôi đề xuất Điều chỉnh Tham số Hiệu quả Liên tục (ConPET), một mô hình tổng quát cho việc thích ứng nhiệm vụ liên tục của LLMs với độ phức tạp huấn luyện độc lập với số lượng nhiệm vụ. ConPET bao gồm hai phiên bản với các tình huống ứng dụng khác nhau. Đầu tiên, Static ConPET có thể thích ứng các phương pháp học liên tục trước đây được thiết kế ban đầu cho các mô hình tương đối nhỏ hơn với LLMs thông qua PET và một chiến lược phát lại động, điều này làm giảm đáng kể chi phí điều chỉnh và giảm thiểu vấn đề quá khớp và quên. Hơn nữa, để duy trì khả năng mở rộng, Dynamic ConPET áp dụng các mô-đun PET riêng biệt cho các nhiệm vụ khác nhau và một bộ chọn mô-đun PET để lựa chọn tối ưu động. Trong các thí nghiệm rộng rãi của chúng tôi, việc thích ứng Static ConPET giúp nhiều phương pháp trước đây giảm quy mô tham số có thể điều chỉnh hơn 3.000 lần và vượt qua baseline chỉ PET ít nhất 5 điểm trên năm benchmark nhỏ hơn, trong khi Dynamic ConPET có lợi thế trên tập dữ liệu lớn nhất. Các mã và tập dữ liệu có sẵn tại https://github.com/Raincleared-Song/ConPET.
Từ khóa chỉ mục —Học liên tục, điều chỉnh tham số hiệu quả, mô hình ngôn ngữ lớn.

I. GIỚI THIỆU
GẦN đây, các mô hình ngôn ngữ lớn (LLMs) đã thể hiện khả năng tuyệt vời từ nhiều khía cạnh [1]–[3], trang bị cho chúng tiềm năng lớn hơn trong việc xử lý các cài đặt cụ thể cho nhiệm vụ đa dạng. Để thích ứng LLMs với các nhiệm vụ hạ nguồn, điều chỉnh tinh thường là lựa chọn đầu tiên [4]. Tuy nhiên, trong các ứng dụng thực tế, sự xuất hiện nhất quán của các tài liệu như corpus mới nhất [5], kiến thức mới [6], [7], và các công cụ không đồng nhất [8] có thể thay đổi thường xuyên các lược đồ nhiệm vụ. Điều này đòi hỏi việc thích ứng cụ thể cho nhiệm vụ liên tục của LLMs, điều này rất tốn kém và có rủi ro hiệu suất khi được tiến hành thông qua điều chỉnh tinh truyền thống do số lượng tham số LLM khổng lồ và vấn đề quên thảm khốc [9], cụ thể là giảm hiệu suất đáng kể trên các nhiệm vụ cũ sau khi thích ứng với các nhiệm vụ mới.

Mặc dù nhiều phương pháp học liên tục đã được đề xuất để xử lý những vấn đề này, các thách thức cụ thể cản trở việc thích ứng chúng với LLMs. Các phương pháp dựa trên kiến trúc động [11], [12], tăng dần quy mô mô hình 

Chenyang Song, Xu Han, Zheni Zeng, Zhiyuan Liu, và Maosong Sun thuộc Khoa Khoa học Máy tính và Công nghệ, Đại học Tsinghua, Bắc Kinh 100084, Trung Quốc.
Kuai Li, Chen Chen, và Tao Yang thuộc Khoa Nền tảng Học máy, Tencent, Bắc Kinh 100193, Trung Quốc.
E-mail: scy22@mails.tsinghua.edu.cn, liuzy@tsinghua.edu.cn
Zhiyuan Liu là tác giả liên lạc.

T1 T2…Dữ liệu Cũ
Bộ nhớ 
Đầy đủ
lấy mẫu
phân cấp
Cũ
Mới
Giới hạn
Lô 
Huấn luyện
Tk lấy mẫu Tk-1
Dữ liệu 
Mới
LLM
PET EMR + Static ConPET EMR Truyền thống
T1 T2…Dữ liệu Cũ
Tk-1
lấy mẫu
số lượng cố định
T'1 T'2 T'k-1
…
Tk
Dữ liệu 
Mới Bộ nhớ
Giới hạn
Dữ liệu Huấn luyện
(Bước k)
LLM điều chỉnh
tham số đầy đủ
Lấy mẫu Dữ liệu Thô
Có thể điều chỉnh Đông lạnh điều chỉnh tham số
hiệu quả

Hình 1. So sánh giữa EMR truyền thống [10] và EMR được thích ứng với Static ConPET. Cách sau áp dụng chiến lược phát lại động cho việc tạo dữ liệu huấn luyện và PET cho điều chỉnh LLM.

với sự phát triển của dữ liệu, gặp phải chi phí tăng tuyến tính không thể chấp nhận được do việc mở rộng không hạn chế của kiến trúc. Trong khi đó, hầu hết các phương pháp dựa trên bộ nhớ [9], [13]–[15] thường xuyên điều chỉnh lại mô hình thông qua một chiến lược phát lại cố định, nơi các ví dụ được lưu trong bộ nhớ tập đoạn giới hạn được phát lại một số lần cố định kết hợp với dữ liệu mới. Điều này khiến chúng dễ bị khả năng mở rộng thấp và quá khớp trên các ví dụ được ghi nhớ [15]. Hơn nữa, vì backbone của chúng tương đối nhỏ về quy mô như BERT [16] và RoBERTa [17], chúng áp dụng điều chỉnh tham số đầy đủ trong các công trình gốc của chúng, điều này áp đặt gánh nặng lớn lên tài nguyên tính toán cho LLMs về thời gian và bộ nhớ GPU.

Đối mặt với những thách thức này, chúng tôi đề xuất Điều chỉnh Tham số Hiệu quả Liên tục (ConPET), một mô hình tổng quát cho việc thích ứng nhiệm vụ dựa trên điều chỉnh tinh liên tục của LLMs với độ phức tạp huấn luyện độc lập với số lượng nhiệm vụ, bao gồm Static ConPET và Dynamic ConPET.

Như được hiển thị trong Hình 1, chúng tôi đầu tiên sử dụng Static ConPET, một phương pháp tổng quát để thích ứng các phương pháp liên tục dựa trên bộ nhớ truyền thống như EMR [10] với LLMs trong khi đối phó với chi phí huấn luyện cao, quá khớp và quên thảm khốc. Cụ thể, nó chứa hai thích ứng chính: (1) Thay thế điều chỉnh tinh vanilla

--- TRANG 2 ---
2
với điều chỉnh tham số hiệu quả (PET). Vì PET chỉ cập nhật các mô-đun có thể điều chỉnh quy mô nhỏ (thường chiếm dưới 1% LLM) trong khi giữ LLM gốc đông lạnh, chi phí tính toán cho việc cập nhật tham số và tiêu thụ bộ nhớ GPU có thể được giảm đáng kể [4].
(2) Sử dụng dữ liệu lịch sử thông qua một chiến lược phát lại động, thực hiện lấy mẫu mạnh mẽ một cách phân cấp từ dữ liệu lịch sử toàn bộ thay vì một bộ nhớ giới hạn để cải thiện độ bao phủ dữ liệu và giảm thiểu quá khớp và quên.
Một hạn chế về số lượng lô được lấy mẫu cũng được giới thiệu để kiểm soát độ phức tạp huấn luyện.

Được thích ứng từ các phương pháp dựa trên bộ nhớ, Static ConPET với một mô-đun PET duy nhất không thể tránh khỏi vấn đề khả năng mở rộng thấp. Do đó, chúng tôi tiếp tục đề xuất Dynamic ConPET được hiển thị trong Hình 2, một kiến trúc động mới bao gồm một LLM backbone, một bộ chọn mô-đun PET, một tập hợp các mô-đun PET cụ thể cho nhiệm vụ, và một hệ thống bộ nhớ đệm. Tương tự như các kiến trúc mixture-of-expert (MoE), Dynamic ConPET tách biệt các tham số cho các nhiệm vụ với các lược đồ khác nhau (ví dụ, các loại kiến thức riêng biệt) trong các mô-đun khác nhau, điều này tự nhiên giảm thiểu việc quên và duy trì khả năng mở rộng với số lượng nhiệm vụ tăng lên. Vì mỗi mô-đun cụ thể cho nhiệm vụ chỉ chứa một mô-đun PET nhẹ và có thể cắm được thay vì các mạng con nặng trong các phương pháp dựa trên kiến trúc động trước đây, Dynamic ConPET phù hợp hơn cho LLMs tiêu thụ bộ nhớ. Trong khi đó, bộ chọn mô-đun PET đảm bảo chi phí lan truyền tiến cố định bằng cách chọn trước một số lượng cố định các mô-đun cụ thể cho nhiệm vụ với điểm số cao nhất để tham gia vào dự đoán.

Chúng tôi tiến hành các thí nghiệm toàn diện trên nhiều tập dữ liệu trích xuất kiến thức, một kịch bản học liên tục đại diện với các loại kiến thức mới liên tục xuất hiện. Kết quả cho thấy rằng cả hai phiên bản của ConPET đều có hiệu quả trong việc thích ứng cụ thể cho nhiệm vụ liên tục của LLMs trong khi có các tình huống ứng dụng khác nhau. Phân tích thêm cho thấy hiệu quả của điều chỉnh tham số hiệu quả, chọn trước mô-đun PET, và các phân chia nhiệm vụ khác nhau.

II. CÔNG TRÌNH LIÊN QUAN

A. Điều chỉnh Tham số Hiệu quả

Để điều chỉnh tinh LLM hiệu quả hơn, điều chỉnh tham số hiệu quả được đề xuất [4], chủ yếu bao gồm ba loại:
(1) Các phương pháp dựa trên bổ sung [18]–[20] giới thiệu các tham số có thể điều chỉnh bổ sung quy mô nhỏ trong khi đông lạnh LLM gốc. (2) Các phương pháp dựa trên đặc tả [21]–[23] tối ưu hóa có chọn lọc một phần của các tham số LLM với các tham số còn lại không thay đổi. (3) Các phương pháp dựa trên tham số hóa lại [24], [25] chuyển đổi các tham số thích ứng của LLMs thành các dạng hiệu quả tham số trong quá trình tối ưu hóa. Trong công trình này, ConPET có thể được thích ứng với các phương pháp dựa trên bổ sung và một số phương pháp dựa trên tham số hóa lại. Đáng chú ý, LoRA [24], giới thiệu các tham số bổ sung để mô hình hóa sự khác biệt trọng số, được chứng minh là hoạt động tốt hơn hầu hết các phương pháp PET chính thống và do đó được áp dụng rộng rãi [4]. Do đó, các thí nghiệm của chúng tôi trong công trình này sẽ tập trung vào LoRA như một đại diện.

PET được chứng minh là tiết kiệm chi phí tính toán và tiêu thụ bộ nhớ đáng kể. Theo các công trình trước đây [26], với cùng dữ liệu hướng dẫn và GPUs, thời gian tiêu thụ bởi điều chỉnh PET trên LLaMA-7B [27] chỉ khoảng một phần tư so với điều chỉnh tham số đầy đủ.

B. Học Liên tục cho LLMs

Học liên tục nhằm mục đích dạy một mô hình xử lý tăng dần các nhiệm vụ mới nổi lên trong khi giảm thiểu vấn đề quên thảm khốc. Mặc dù có thành công của học trong ngữ cảnh trong học zero/few-shot [1], điều chỉnh tinh vẫn là một mô hình phổ biến trong việc thích ứng nhiệm vụ của LLMs [4]. Các nỗ lực hiện có về học liên tục dựa trên điều chỉnh có thể được phân loại chung thành ba loại: (1) Các phương pháp dựa trên củng cố bảo vệ các tham số quan trọng khỏi việc thay đổi đáng kể, thường được thực hiện thông qua regularization [28]–[30] hoặc chưng cất [31], [32]. Tuy nhiên, chúng hoạt động kém do thiếu việc sử dụng dữ liệu lịch sử. (2) Các phương pháp dựa trên kiến trúc động [11], [12], [33] duy trì một mô hình có quy mô tăng dần với số lượng nhiệm vụ. Bằng cách giới thiệu các tham số độc lập cho các nhiệm vụ khác nhau, chúng có thể khắc phục hiệu quả việc quên thảm khốc nhưng gặp phải sự tăng tuyến tính của chi phí huấn luyện. (3) Các phương pháp dựa trên bộ nhớ [9], [13]–[15], [34] giới thiệu bộ nhớ tập đoạn để lưu trữ và phát lại các ví dụ của các nhiệm vụ cũ. Mặc dù thông tin nhiệm vụ cũ được giữ lại một phần trong bộ nhớ, chúng dễ bị quá khớp và khả năng mở rộng thấp do việc huấn luyện lại thường xuyên trên một kiến trúc mô hình cố định thông qua chiến lược phát lại cố định. Tất cả các phương pháp trên đều áp dụng điều chỉnh tinh vanilla do quy mô nhỏ của backbone của chúng.

Xem xét chi phí đắt đỏ của việc điều chỉnh LLMs, một số công trình tích hợp điều chỉnh tham số hiệu quả với học liên tục nhưng vẫn không thể xử lý đầy đủ một số khuyết điểm của các phương pháp trước đây. Cả LFPT5 [35] và Progressive Prompts [36] đều đề xuất liên tục giới thiệu và huấn luyện các prompt mềm mới cho một nhiệm vụ mới để giải quyết việc quên thảm khốc, trong khi LFPT5 bổ sung tạo ra các ví dụ giả cho việc phát lại kinh nghiệm. Tuy nhiên, tương tự như các phương pháp dựa trên kiến trúc động được đề cập ở trên, việc tích lũy liên tục các prompt mới có thể gây ra vấn đề khả năng mở rộng. AdapterCL [37] học một Adapter [18] riêng biệt cho mỗi nhiệm vụ và chọn Adapter cụ thể cho nhiệm vụ dựa trên perplexity. Phương pháp lựa chọn như vậy gây ra sự tăng tuyến tính trong chi phí chuyển tiếp liên quan đến số lượng nhiệm vụ. LAE [38] lặp lại huấn luyện và kết hợp hai chuyên gia được ưa chuộng bởi các nhiệm vụ mới/lịch sử tương ứng, điều này dễ bị quá khớp với số lượng chuyên gia cố định.

Hơn nữa, cũng có các công trình tập trung vào vấn đề tiền huấn luyện liên tục [5], [39], [40], nhưng chúng quá tốn kém để được áp dụng cho điều chỉnh tinh LLM cụ thể cho nhiệm vụ, thường được tiến hành thường xuyên với tài nguyên thấp.

C. Mixture-of-Expert cho LLMs

Một hướng công trình khác tương tự như Dynamic ConPET là kiến trúc MoE bao gồm nhiều mạng riêng biệt, học để xử lý các tập con khác nhau của các ví dụ đầu vào hoặc chịu trách nhiệm riêng biệt. MoE đầu tiên được chứng minh là có hiệu quả trong deep learning bằng cách giới thiệu một lớp MoE xếp chồng giữa các mô-đun LSTM [41]. Sau đó, GShard [42], BASELayer [43], HashLayer [44], Switch Transformers [45],

--- TRANG 3 ---
3
MoEfication [46], và DEMIX [47] cố gắng khám phá các chiến lược triển khai và huấn luyện của MoE trong các mô hình dựa trên Transformer. Tuy nhiên, những công trình này phải sửa đổi các cấu trúc cụ thể (ví dụ, lớp FFN) trong Transformers, điều này không thể dễ dàng thích ứng với việc điều chỉnh tinh liên tục của một LLM đã được tiền huấn luyện. Một số công trình gần đây đã chứng minh rằng LLMs với MoE có thể đạt được hiệu suất tối cao khi kết hợp với điều chỉnh hướng dẫn [48], [49], minh họa tiềm năng của các loại kiến trúc như vậy.

Để cải thiện hiệu quả trong khi giảm thiểu quá khớp và quên thảm khốc trong việc thích ứng cụ thể cho nhiệm vụ liên tục của LLMs, Static ConPET thích ứng các phương pháp học liên tục dựa trên bộ nhớ sử dụng PET và chiến lược phát lại động. Hơn nữa, Dynamic ConPET xử lý khả năng mở rộng thấp thông qua một cấu trúc động với các mô-đun PET cụ thể cho nhiệm vụ và một bộ chọn. Không giống như các kiến trúc MoE trước đây, Dynamic ConPET phù hợp hơn cho việc điều chỉnh LLM vì mỗi chuyên gia là một mô-đun PET nhẹ và có tính cắm cao, có thể được điều chỉnh mà không thay đổi cấu trúc hoặc tham số LLM gốc. Cuối cùng, cả hai phiên bản ConPET đều cung cấp độ phức tạp huấn luyện độc lập với số lượng nhiệm vụ.

III. PHƯƠNG PHÁP ĐỀ XUẤT

A. Định nghĩa Nhiệm vụ

Điều chỉnh tinh liên tục của LLMs nhằm mục đích dạy LLM xử lý đồng thời một chuỗi các nhiệm vụ trong một kịch bản ứng dụng cụ thể với các tài liệu mới liên tục xuất hiện. Chúng tôi ký hiệu tập hợp các lược đồ của các tài liệu được xử lý trong nhiệm vụ thứ k là Sk, với một tập huấn luyện tương ứng Tk và một tập đánh giá Qk. Các lược đồ của các nhiệm vụ khác nhau là rời rạc.
Tại bước thứ k, với dữ liệu huấn luyện đã thấy ˜Tk=∪ki=1Ti, mô hình được yêu cầu đạt được kết quả thỏa mãn trên tập đánh giá của nhiệm vụ mới cũng như tất cả k−1 nhiệm vụ lịch sử, cụ thể là ˜Qk=∪ki=1Qi với tập lược đồ ˜Sk=∪ki=1Si.

Lấy trích xuất kiến thức làm đại diện, Sk là một tập con cụ thể của các loại kiến thức (ví dụ, loại thực thể hoặc loại quan hệ). Mỗi ví dụ trong tập dữ liệu bao gồm một câu đầu vào và một nhãn chuẩn, chỉ ra loại kiến thức được biểu thị trong đầu vào. LLM sau đó được yêu cầu dự đoán nhãn loại kiến thức với độ chính xác thỏa mãn ổn định dọc theo chuỗi nhiệm vụ.

B. Static ConPET

Static ConPET là một phương pháp tổng quát để thích ứng các phương pháp học liên tục dựa trên bộ nhớ trước đây với việc thích ứng cụ thể cho nhiệm vụ liên tục của LLMs, chủ yếu bao gồm hai phần: thích ứng PET và chiến lược phát lại động.

1) Thích ứng PET và Mã hóa Ví dụ: Vì hầu hết các phương pháp dựa trên bộ nhớ trước đây chủ yếu quan tâm đến các mô hình quy mô tương đối nhỏ [9], [10], [14], chúng tôi đầu tiên thay thế việc điều chỉnh tinh vanilla bằng PET khi áp dụng chúng cho LLMs. Cụ thể, thay vì điều chỉnh tất cả các tham số trong LLMs, chúng tôi chỉ tối ưu hóa một mô-đun PET nhỏ, trong khi LLM và các mô-đun còn lại vẫn đông lạnh. Xem xét kích thước nhỏ của các tham số có thể điều chỉnh,

BẢNG I
CÁC THIẾT LẬP CỦA HAI NHIỆM VỤ TRÍCH XUẤT KIẾN THỨC LIÊN TỤC CÓ LIÊN QUAN TRONG CÁC THÍ NGHIỆM CỦA CHÚNG TÔI.

Gõ Thực thể
Mục tiêu: Xác định loại thực thể có trong ví dụ đầu vào.
Đánh dấu Thực thể: [E1] thực thể [/E1]
Mẫu Prompt: Trong câu này, thực thể là một [MASK].

Trích xuất Quan hệ
Mục tiêu: Xác định quan hệ giữa thực thể đầu và thực thể đuôi được thể hiện trong ví dụ đầu vào.
Đánh dấu Thực thể: [E1] thực thể đầu [/E1], [E2] thực thể đuôi [/E2]
Mẫu Prompt: Trong câu này, thực thể đuôi là [MASK] của thực thể đầu.

Các đánh dấu thực thể và mẫu prompt được thêm vào cuối một ví dụ đầu vào cũng được hiển thị.

chi phí tính toán cho việc cập nhật tham số và tiêu thụ bộ nhớ GPU sẽ được giảm đáng kể.

Với sự hỗ trợ của thích ứng PET, chúng tôi có thể tiến hành mã hóa ví dụ hiệu quả hơn, nhằm mục đích tạo ra các biểu diễn có thông tin cho các nhiệm vụ hạ nguồn. Chúng tôi lấy trích xuất kiến thức, bao gồm gõ thực thể và trích xuất quan hệ, làm đại diện. Để cải thiện chất lượng của các biểu diễn, chúng tôi áp dụng một LLM như bộ mã hóa backbone của chúng tôi và tăng cường đầu vào thông qua các đánh dấu thực thể và mẫu prompt. LLM đã có được lượng lớn kiến thức thông qua huấn luyện không giám sát và có thể chuyển đổi đầu vào thành các vector trạng thái ẩn có thông tin. Theo công trình trước đây [50], chúng tôi áp dụng các đánh dấu thực thể bao quanh mỗi thực thể trong ví dụ đầu vào để chèn thông tin vị trí thực thể. Bên cạnh đó, được truyền cảm hứng bởi sự thành công của điều chỉnh prompt [51], [52], chúng tôi thêm các mẫu prompt vào cuối đầu vào và lấy trạng thái ẩn của [MASK] làm biểu diễn ví dụ. Các đánh dấu thực thể và mẫu prompt được sử dụng trong hai nhiệm vụ trích xuất kiến thức liên quan được hiển thị trong Bảng I.

Chính thức, với một ví dụ đầu vào x, LLM được tích hợp với một mô-đun PET M trước tiên mã hóa x thành biểu diễn ví dụ của nó, sau đó được chiếu lên các logits tương ứng bởi đầu tuyến tính chứa trong M. Chúng tôi từ đây ký hiệu quá trình này như sau,

s=f(M,x) (1)

trong đó f và s lần lượt đại diện cho hàm mã hóa và vector logit.

2) Chiến lược Lấy mẫu Động: Thay vì tiến hành phát lại trên các ví dụ được ghi nhớ giới hạn như các phương pháp dựa trên bộ nhớ hiện có, ConPET sử dụng dữ liệu lịch sử thông qua một chiến lược phát lại động để tránh quá khớp và kiểm soát tổng số bước huấn luyện. Cụ thể, chúng tôi loại bỏ giới hạn về không gian lưu trữ để cải thiện độ bao phủ dữ liệu. Thay vào đó, các ví dụ phát lại được chọn động từ dữ liệu toàn bộ, dưới một hạn chế về số lô tối đa tại mỗi bước, điều này đảm bảo độ phức tạp độc lập với số lượng nhiệm vụ.

Chiến lược này có thể thách thức một giả định phổ biến trong học liên tục rằng bộ nhớ bị giới hạn và do đó việc lưu dữ liệu toàn bộ là không thực tế [14]. Tuy nhiên, chúng tôi coi việc tập trung vào hạn chế

--- TRANG 4 ---
4
LLM Ví dụ 
Đầu vào
ID: 34
Bộ chọn 
Mô-đun PET
điểm chọn 
lựa MM1
M3
Mk
(giáo viên ép buộc)
…... …...…... logit thấp α
logits trên S2
Mô-đun PET
Cụ thể cho Nhiệm vụ logits trên Sk
I. Chọn trước Mô-đun PET II. Dự đoán với Mô-đun PET Hoạt động
ID Mô-đun ID Ví dụ Logits Khi nào Lưu
Sau Giai đoạn I 34 M (bộ chọn)
Sau M2 được điều chỉnh 34 M2 (cũ)
Sau Giai đoạn II 34 Mk (mới)
...... ...... ......
Hệ thống 
Bộ nhớ đệm ......
LLM
M1
M
Mk
Mô-đun Đông lạnh Mô-đun Có thể Điều chỉnh
Mô-đun Được chọn Tích hợp LLM-PET
M2
logits 
dự đoán
nhãn 
một nóng
mất mát Dynamic ConPET
chọn trước k 
mô-đun PET hàng đầu

Hình 2. Kiến trúc của ConPET khi số lượng mô-đun PET hoạt động là 2. Quá trình làm việc có thể được chia thành hai thủ tục: chọn trước mô-đun PET và dự đoán với các mô-đun PET hoạt động. Tất cả logits được tạo ra bởi một mô-đun PET cụ thể sẽ được lưu ngay lập tức bởi hệ thống bộ nhớ đệm sau khi mô-đun này hoàn thành quá trình điều chỉnh.

chi phí huấn luyện (độ phức tạp) thay vì bộ nhớ về giá cả và tiêu thụ là hợp lý hơn. Nói chung, các corpus huấn luyện hiện tại không vượt quá mức TB ngay cả đối với GPT-3 175B [1], một trong những mô hình ngôn ngữ lớn nhất hiện có. Chi phí lưu trữ dữ liệu ở quy mô như vậy thấp hơn nhiều so với một GPU V100 duy nhất, chứ chưa nói đến việc huấn luyện LLMs hiện đại thường đòi hỏi hàng trăm hoặc thậm chí hàng nghìn GPUs [1], [27]. Thêm nhiều sự thật hỗ trợ cho tuyên bố này được cung cấp trong Phụ lục A.

Một vấn đề khác của việc lưu trữ dữ liệu đầy đủ là lấy mẫu không hiệu quả thống kê [14]. Để khắc phục vấn đề này, chúng tôi áp dụng lấy mẫu phân cấp khi sử dụng dữ liệu lịch sử được lưu trữ để đảm bảo độ bao phủ bằng nhau cho các ví dụ của mỗi nhiệm vụ cũ. Cụ thể, thay vì lấy mẫu ngẫu nhiên trực tiếp, chúng tôi trước tiên tạo ra một ID nhiệm vụ cũ và sau đó chọn một ví dụ từ tập con dữ liệu của nhiệm vụ đó. Bên cạnh đó, một tỷ lệ cố định được giữ giữa các ví dụ cũ và mới trong mỗi lô huấn luyện. Theo cách này, ConPET mạnh mẽ hơn đối với vấn đề mất cân bằng dữ liệu và hiệu quả hơn về mặt thống kê về độ bao phủ bằng nhau cho mỗi nhiệm vụ lịch sử.

C. Dynamic ConPET

Mặc dù có hiệu quả của Static ConPET, vẫn tồn tại một vấn đề tiềm ẩn về khả năng mở rộng thấp. Cụ thể, dưới các nhiệm vụ hạ nguồn với các tài liệu mới nổi cực kỳ phong phú, khối lượng kiến thức cần tiếp thu có thể vượt quá khả năng của các tham số có thể điều chỉnh và do đó hiệu suất sẽ giảm. Vấn đề này có thể được làm trầm trọng hơn bởi PET do quy mô nhỏ của các mô-đun PET.

Do đó, chúng tôi giới thiệu Dynamic ConPET để giải quyết vấn đề này, bao gồm một LLM backbone, một tập hợp các mô-đun PET cụ thể cho nhiệm vụ, một bộ chọn mô-đun PET, và một hệ thống bộ nhớ đệm. Chúng tôi ký hiệu mô-đun PET cho nhiệm vụ thứ k là Mk. Quá trình làm việc tại bước thứ k có thể được tóm tắt trong hai thủ tục: (1) Chọn trước mô-đun PET (Phần III-C1): Chúng tôi huấn luyện bộ chọn mô-đun PET để phân loại các ví dụ đầu vào thành k tập lược đồ đã thấy {S1,S2, ...,Sk}. Sau đó, t mô-đun PET tương ứng với các tập lược đồ với điểm chọn lựa hàng đầu t được giữ lại như những cái hoạt động. (2) Dự đoán với các mô-đun PET hoạt động (Phần III-C2): Các mô-đun PET hoạt động tạo ra logits trên tập lược đồ riêng của chúng tương ứng, sau đó được nối để đưa ra dự đoán cuối cùng. Chúng tôi phân tích độ phức tạp huấn luyện trong Phần III-C3. Như một mô-đun phụ trợ để giải quyết các tính toán logit trùng lặp, hệ thống bộ nhớ đệm được giới thiệu trong Phần III-C4. Dynamic ConPET cũng áp dụng cùng chiến lược phát lại động.

1) Chọn trước Mô-đun PET: Để tránh sự tăng tuyến tính không thể kiểm soát của chi phí huấn luyện như trong các phương pháp dựa trên kiến trúc động trước đây, chúng tôi sử dụng chọn trước mô-đun PET để chọn một số lượng cố định các mô-đun PET quan trọng nhất. Cụ thể, tại bước thứ k, chúng tôi huấn luyện một bộ chọn mô-đun PET (cũng là một mô-đun PET) để phân biệt một số cố định t của các tập lược đồ nhiệm vụ mà mỗi ví dụ có khả năng thuộc về nhất trong số {S1,S2, ...,Sk}. Sau đó, các mô-đun PET cụ thể cho t tập lược đồ được chọn được giữ lại như những cái hoạt động để tham gia vào suy luận tiếp theo.

Chính thức, bộ chọn mô-đun PET chuyển đổi mỗi ví dụ x thành một vector điểm chọn lựa ssel có kích thước k theo Công thức 1, với phần tử thứ j đại diện cho độ tin cậy rằng x thuộc về Sj. Các phần tử hàng đầu t của ssel (với các chỉ số {i1, i2, ..., it}) xác định việc chọn lựa t mô-đun PET hoạt động {Mi1,Mi2, ...,Mit}. Bên cạnh đó, chúng tôi áp dụng chính sách giáo viên ép buộc rằng mô-đun PET chính xác tương ứng với ví dụ luôn được chọn để huấn luyện. Thông qua chọn trước, chi phí lan truyền tiến được giữ không bị ảnh hưởng bởi số lượng mô-đun PET. Các thí nghiệm trong Phần IV-G cũng chứng minh hiệu quả tích cực của nó lên hiệu suất.

2) Dự đoán với Các mô-đun PET Hoạt động: Để có được dự đoán cuối cùng, chúng tôi tích hợp thông tin đã học từ các mô-đun hoạt động {Mi1,Mi2, ...,Mit}. Chính thức, mỗi mô-đun hoạt động có được một vector logit trên tập lược đồ tương ứng của nó theo Công thức 1, trong khi các vector logit của các mô-đun không hoạt động (tức là những mô-đun không được chọn) được gán là các vector có giá trị thấp đủ giống nhau α. Sau đó chúng tôi nối các logits của tất cả các mô-đun PET và tìm ra dự đoán. Thủ tục này có thể được biểu thị bằng các công thức sau,

sj=f(Mj,x), j∈ {i1, i2, ..., it},
sj= [α, α, ..., α ], j /∈ {i1, i2, ..., it},
sj∈R|Sj|, j= 1,2, ..., k,
pred = arg max [ s1,s2, ...,sk],(2)

trong đó [·] biểu thị nối, sj tham chiếu đến vector logit được tạo ra bởi mô-đun PET thứ j, và pred có nghĩa là nhãn được dự đoán.

3) Thuật toán Chi tiết và Phân tích Độ phức tạp: Cuối cùng, chúng tôi tiến hành phân tích thuật toán chi tiết và độ phức tạp huấn luyện của nó. Quá trình huấn luyện Dynamic ConPET cho nhiệm vụ thứ k được trình bày trong Thuật toán 1. Cả bộ chọn mô-đun PET và các mô-đun cụ thể cho nhiệm vụ đều áp dụng mất mát entropy chéo tiêu chuẩn cho phân loại đa lớp như mục tiêu huấn luyện. Bộ chọn mô-đun PET được cập nhật lặp lại. Tại bước thứ k, bộ chọn được khởi tạo bởi bộ chọn bước thứ (k−1), ngoại trừ việc đầu tuyến tính của nó nên được mở rộng theo chiều để xử lý nhiều lược đồ hơn, đó là chức năng của Mở rộng Chiều.

Quá trình huấn luyện trên có độ phức tạp độc lập với số lượng nhiệm vụ. Cụ thể, chúng tôi giới hạn số lô huấn luyện của bộ chọn mô-đun PET và mô-đun cụ thể cho nhiệm vụ thứ k lần lượt là iter1 và iter2, và kích thước lô được cố định là b. Trong khi đó, t mô-đun hoạt động được giữ lại cho mỗi ví dụ. Do đó, độ phức tạp huấn luyện cho nhiệm vụ thứ k là O(b·(iter1+iter2·(t+ 1))), không tăng theo số lượng nhiệm vụ k.

4) Hệ thống Bộ nhớ đệm: Để giải quyết các tính toán logit trùng lặp, chúng tôi giới thiệu một hệ thống bộ nhớ đệm phụ trợ để lưu trữ các logits được tạo ra bởi các mô-đun PET đã được điều chỉnh. Cụ thể, mỗi mục bộ nhớ đệm bao gồm ID ví dụ, ID mô-đun PET (chỉ số của một mô-đun cụ thể cho nhiệm vụ hoặc bộ chọn), và vector logit. Khi một mô-đun PET đã được điều chỉnh gặp một ví dụ đầu vào, ConPET kiểm tra cơ sở dữ liệu sử dụng ID ví dụ và ID mô-đun PET. Nếu có sự khớp, việc tính toán tốn thời gian trong Phương trình 1 có thể được tránh. Cần lưu ý rằng hệ thống bộ nhớ đệm có sẵn cho một mô-đun cụ thể chỉ sau khi mô-đun hoàn thành quá trình điều chỉnh và vẫn cố định sau đó. Ví dụ, Bộ chọn Mô-đun PET chỉ có thể truy cập bộ nhớ đệm sau khi quá trình huấn luyện riêng của nó trong giai đoạn chọn trước kết thúc.

Thuật toán 1 Dynamic ConPET cho nhiệm vụ thứ k
Yêu cầu: Tập huấn luyện Tk của nhiệm vụ thứ k
Yêu cầu: Tất cả dữ liệu huấn luyện lịch sử ˜Tk−1
Yêu cầu: Tất cả các tập lược đồ nhiệm vụ đã thấy ˜Sk
Yêu cầu: Bộ chọn mô-đun PET cuối cùng M(k−1)sel
Yêu cầu: Số lô huấn luyện tối đa iter1 và iter2
1: M(k)sel←Mở rộng Chiều(M(k−1)sel)
2: for i←1 to iter1 do
3: Lấy mẫu Bnew ngẫu nhiên từ Tk
4: Lấy mẫu Bold phân cấp từ ˜Tk−1
5: Btot← Bnew∪ Bold
6: Cập nhật M(k)sel với mất mát phân loại k loại trên Btot
7: end for
8: Khởi tạo mô-đun cụ thể cho nhiệm vụ thứ k Mk
9: for i←1 to iter2 do
10: Lấy mẫu Bnew ngẫu nhiên từ Tk
11: Lấy mẫu Bold phân cấp từ ˜Tk−1
12: Btot← Bnew∪ Bold
13: for x in Btot do
14: Chọn t mô-đun PET hoạt động có khả năng nhất cho x
15: Dự đoán cho x với các mô-đun PET hoạt động
16: end for
17: Cập nhật Mk với mất mát phân loại |˜Sk| loại trên Btot
18: end for

IV. THÍ NGHIỆM

A. Tập dữ liệu

Cho các thí nghiệm, chúng tôi tập trung vào trích xuất kiến thức liên tục như một kịch bản thích ứng liên tục đại diện của LLMs, bao gồm gõ thực thể và trích xuất kiến thức. Cụ thể, chúng tôi giới thiệu 3 tập dữ liệu sau như benchmark cho gõ thực thể liên tục:

(1) FewNERD. FewNERD [53] là một tập dữ liệu được gán nhãn thủ công lớn với 66 loại thực thể tinh tế phân cấp. Cụ thể, chúng tôi sử dụng FewNERD (SUP) để xây dựng benchmark, bao gồm 486,044 ví dụ từ Wikipedia.

(2) OntoNotes. OntoNotes 5.0 [54] cũng là một tập dữ liệu được gán nhãn thủ công với 86 loại thực thể, 264,404 câu, và nhiều nguồn dữ liệu.

(3) BBN. So với hai tập dữ liệu trên, BBN [55] tương đối nhỏ hơn, với 111,728 ví dụ và 46 loại thực thể. Nó chủ yếu được sử dụng để kiểm tra khả năng tổng quát của ConPET trên các nhiệm vụ quy mô nhỏ.

Ngoài ra, chúng tôi kiểm tra phương pháp của chúng tôi trên 3 nhiệm vụ trích xuất quan hệ liên tục:

(1) FewRel. FewRel [56] là một tập dữ liệu quy mô lớn cho trích xuất quan hệ với 80 loại quan hệ và 56,000 ví dụ từ Wikipedia.

(2) TACRED. TACRED [57] là một tập dữ liệu trích xuất quan hệ cấp câu thu được thông qua crowdsourcing. Để đơn giản, chúng tôi loại bỏ những ví dụ có nhãn "n/a", và cuối cùng nó bao gồm 21,773 câu và 41 loại quan hệ.

--- TRANG 5 ---
5
(3) ACE 2005. Chúng tôi áp dụng phần tiếng Anh của ACE 2005 Multilingual Training Corpus [58], bao gồm 7,070 câu và 18 loại quan hệ. Xem xét quy mô nhỏ của nó, nó cũng nhằm mục đích kiểm tra khả năng tổng quát nhiệm vụ quy mô nhỏ của phương pháp chúng tôi.

Đối với mỗi tập dữ liệu, chúng tôi xây dựng chuỗi nhiệm vụ tương ứng cho học liên tục bằng cách phân chia ngẫu nhiên các loại thực thể hoặc quan hệ của nó thành các cụm (5 cụm cho ACE 2005 và 10 cụm cho những cái khác). Sau đó tập lược đồ của mỗi nhiệm vụ được chỉ định cho một trong những cụm này.

Các phương pháp chia train-validation-test của FewNERD và TACRED giống như các công trình gốc [53], [57]. Đối với 4 tập dữ liệu còn lại, chúng tôi chia chúng ngẫu nhiên thành tập huấn luyện, validation và test theo tỷ lệ 8:1:1. Thống kê chính khác của những 6 tập dữ liệu này được cung cấp trong Bảng II.

B. Nguồn Tập dữ liệu và Giấy phép

Tất cả các tài liệu văn bản được bao gồm trong 6 tập dữ liệu đều bằng tiếng Anh. Các ví dụ của FewNERD và FewRel được thu thập từ Wikipedia. Các câu trong BBN được chọn từ Penn Treebank Corpus of Wall Street Journal. Các nguồn dữ liệu của ba tập dữ liệu còn lại được trộn lẫn. TACRED được thu thập từ newswire và web collection. ACE 2005 chứa các tài liệu văn bản từ broadcast conversations, broadcast news, newsgroups, telephone conversations, và weblogs. OntoNotes 5.0 phức tạp nhất, với một corpus từ sự kết hợp của telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs, và religious texts.

Về các giấy phép và chính sách sử dụng dữ liệu, FewNERD và FewRel được phát hành dưới giấy phép CC BY-SA 4.0, trong khi OntoNotes, BBN, TACRED, và ACE 2005 được sử dụng dưới giấy phép dữ liệu Linguistic Data Consortium (LDC) như một thành viên. Tất cả các tập dữ liệu được sử dụng theo cách nhất quán với mục đích sử dụng dự định của chúng, và chúng tôi giới hạn quyền truy cập công khai với chúng theo yêu cầu của giấy phép. Thông qua lấy mẫu thủ công, chúng tôi không tìm thấy bất kỳ nội dung xúc phạm hoặc định danh nào trong những tập dữ liệu này.

C. Thiết lập Thí nghiệm

Hai thước đo đánh giá được áp dụng: độ chính xác toàn bộ và độ chính xác trung bình. Cái trước là một độ chính xác phân loại tiêu chuẩn trên tất cả dữ liệu đánh giá ˜QL, trong đó L là số lượng nhiệm vụ. Cái sau lấy trung bình các độ chính xác độc lập của mỗi nhiệm vụ đã thấy, có thể đánh giá tốt hơn khả năng giải quyết việc quên thảm khốc [15].

Chúng tôi áp dụng LLaMA-7B [27] như LLM backbone, có 32 lớp và kích thước ẩn là 4,096. LoRA [24] được sử dụng như một phương pháp PET đại diện trong các thí nghiệm của chúng tôi. Bao gồm các ma trận LoRA và một đầu tuyến tính, một mô-đun PET chứa khoảng 2M tham số, chỉ chiếm 0.03% của LLM (khoảng 6.74B tham số). Để hiệu quả, rank của các ma trận LoRA và số lượng t của các mô-đun PET hoạt động được đặt lần lượt là 4 và 1 dưới tất cả các thiết lập. Tỷ lệ giữa các ví dụ cũ và mới trong mỗi lô được cố định là 1:1.

Đối với Static ConPET, chúng tôi chủ yếu thích ứng nó với ba phương pháp dựa trên bộ nhớ sau:

(1) EMR. EMR [10] là một phương pháp học liên tục dựa trên bộ nhớ cơ bản, lưu một số lượng cố định các ví dụ cho mỗi nhiệm vụ và sau đó phát lại chúng kết hợp với dữ liệu mới khi huấn luyện một mô hình mới.

(2) EA-EMR. EA-EMR [14] là một phần mở rộng của EMR. Ngoài phát lại bộ nhớ, nó tiến hành căn chỉnh embedding ở cuối mỗi bước để giảm thiểu sự biến dạng của không gian embedding của các loại kiến thức cũ.

--- TRANG 6 ---
6
BẢNG II
THÔNG TIN THỐNG KÊ CỦA 6 BENCHMARKS CÓ LIÊN QUAN TRONG CÔNG TRÌNH NÀY.

FewNERD OntoNotes BBN FewRel TACRED ACE 2005
#train 340,383 211,523 89,365 44,800 13,012 5,648
#valid 48,759 26,440 11,150 5,600 5,436 700
#test 96,902 26,441 11,213 5,600 3,325 722
#total 486,044 264,404 111,728 56,000 21,773 7,070
#type 66 86 46 80 41 18
#cluster 10 10 10 10 10 5

"#train", "#valid", "#test", và "#total" tham chiếu đến số lượng ví dụ trong tập huấn luyện, tập validation, tập test, và toàn bộ tập dữ liệu tương ứng. "#type" đại diện cho số lượng loại kiến thức có trong một tập dữ liệu. "#cluster" biểu thị tổng số nhiệm vụ L của benchmark tương ứng.

BẢNG III
CÁC SIÊU THAM SỐ ĐƯỢC SỬ DỤNG BỞI CONPET TRÊN 6 TẬP DỮ LIỆU.

FewNERD OntoNotes BBN FewRel TACRED ACE 2005
learning rate St-ConPET 2e−5 1e−4 2e−4 2e−4 5e−4 5e−4
learning rate Dy-ConPET 1e−4 1e−4 2e−4 2e−4 5e−4 5e−4
weight decay 0.01 0.01 0.01 0.01 0.01 0.01
batch number limit 25,000 12,500 5,000 4,000 2,000 2,000
training batch size 8 8 8 8 8 8
maximum epoch number 10 10 10 10 20 20
maximum input length 256 256 256 128 128 128
example number 100 100 50 50 20 20

"St-ConPET" và "Dy-ConPET" tham chiếu đến các thiết lập với Static ConPET và Dynamic ConPET tương ứng.

(3) CRL. CRL [9], cũng là một phần mở rộng EMR, tiếp tục áp dụng học đối lập và chưng cất kiến thức khi phát lại các ví dụ được ghi nhớ để giữ lại kiến thức quan hệ cũ.

Vì việc điều chỉnh tinh tham số đầy đủ vanilla chiếm tài nguyên tính toán dồi dào với hơn 3,000 lần tham số có thể điều chỉnh, chúng tôi giới thiệu ba phương pháp trên chỉ với thích ứng PET làm baselines, áp dụng bộ nhớ giới hạn và chọn một số lượng cố định các ví dụ được ghi nhớ cho mỗi loại kiến thức thông qua phân cụm K-Means.

Ngoài ra, chúng tôi giới thiệu một thiết lập cận trên Limitless để tham khảo. Cụ thể, nó chia sẻ cùng quy trình làm việc như Dynamic ConPET, ngoại trừ việc giới hạn số lô huấn luyện của nó được loại bỏ. Do đó, tại bước thứ k, nó được huấn luyện với dữ liệu lịch sử toàn bộ ˜Tk và gặp phải độ phức tạp tăng tuyến tính O(|˜Tk|·(t+ 2)).

D. Siêu tham số và Chi tiết Triển khai

Các siêu tham số thí nghiệm được điều chỉnh thông qua grid search cho mỗi tập dữ liệu tương ứng. Bên cạnh các tham số quan trọng nhất được giới thiệu trong Phần IV-C, các tham số khác cho ConPET được liệt kê trong Bảng III. Cụ thể, đối với Dynamic ConPET, learning rate trên FewNERD được điều chỉnh thành 5e−4 cho hai bước học liên tục đầu tiên, và learning rate trên OntoNotes được sửa đổi thành 5e−4 cho ba bước đầu tiên trong tất cả các thiết lập. Giới hạn số lô tham chiếu đến hạn chế về tổng số lô huấn luyện và validation tại mỗi bước nếu số epoch tối đa được đạt tới, như đã đề cập trong Phần III-B2. Tỷ lệ giữa số lô huấn luyện và validation của ConPET được đặt là 4:1 trong suốt các thí nghiệm của chúng tôi. Trong khi bảng cung cấp số epoch tối đa, epoch tốt nhất được chọn theo độ chính xác trung bình trên dữ liệu validation. Do đó, các bước huấn luyện thực tế của mô hình epoch tốt nhất có thể thấp hơn giới hạn số lô được cung cấp. "Số lượng ví dụ" tham chiếu đến số lượng ví dụ được ghi nhớ cho mỗi loại kiến thức tại mỗi bước của phiên bản baseline chỉ PET của ba phương pháp dựa trên bộ nhớ EMR, EA-EMR, và CRL, có tần suất phát lại được điều chỉnh động để có được tổng số bước huấn luyện không thấp hơn ConPET với chiến lược lấy mẫu động.

Việc triển khai ConPET dựa trên PyTorch và mỗi instance ConPET liên quan trong các thí nghiệm của chúng tôi được huấn luyện trên một GPU A100 duy nhất trong 1 đến 48 giờ, tùy thuộc vào quy mô tập dữ liệu tương ứng. Các công cụ và gói mà chúng tôi sử dụng trong các thí nghiệm bao gồm: PyTorch, transformers, numpy, scikit-learn, tqdm, và loralib.

--- TRANG 7 ---
7
BẢNG IV
KẾT QUẢ THÍ NGHIỆM CHÍNH TRÊN SÁU TẬP DỮ LIỆU. "W" VÀ "A" ĐẠI DIỆN CHO ĐỘ CHÍNH XÁC TOÀN BỘ (%) VÀ ĐỘ CHÍNH XÁC TRUNG BÌNH (%) TƯƠNG ỨNG. EMR*, EA-EMR*, VÀ CRL* THAM CHIẾU ĐẾN PHIÊN BẢN THÍCH ỨNG STATIC CONPET CỦA EMR, EA-EMR, VÀ CRL TƯƠNG ỨNG. "DY-CONPET" CÓ NGHĨA LÀ DYNAMIC CONPET.

FewNERD OntoNotes BBN FewRel TACRED ACE 2005
W A W A W A W A W A W A
Baseline: Các phương pháp dựa trên bộ nhớ +chỉ thích ứng PET
EMR 57.77 66.03 64.46 68.26 53.30 65.56 84.75 84.75 65.43 62.99 41.27 49.66
EA-EMR 48.61 60.15 67.63 71.28 46.72 64.43 83.12 83.12 59.20 55.70 41.14 53.81
CRL 72.73 75.48 61.39 62.65 55.98 64.02 84.38 84.37 75.91 72.61 54.29 61.96
Các phương pháp dựa trên bộ nhớ +Static ConPET
EMR* 74.41 77.13 85.12 86.11 80.05 83.13 89.30 89.30 80.44 75.75 79.85 80.53
EA-EMR* 73.32 75.97 84.61 85.89 80.33 84.50 88.61 88.61 84.56 80.89 83.31 82.78
CRL* 75.72 77.31 67.94 68.22 71.03 72.00 89.48 89.48 85.29 81.03 82.55 82.53
Dynamic ConPET
Dy-ConPET 76.15 78.22 84.47 85.83 76.15 81.16 88.62 88.62 84.47 80.27 80.27 80.76
Cận trên
Limitless 79.49 77.80 88.19 87.73 89.61 85.97 90.34 90.34 87.40 82.58 84.07 83.22

Các thực thể đậm đại diện cho kết quả tốt nhất. Mỗi kết quả trong bảng này ngoại trừ thiết lập "Limitless" tốn thời gian được lấy trung bình từ một sự sao chép hai lần.

BẢNG V
CÁC ĐỘ LỆCH CHUẨN CỦA CÁC KẾT QUẢ THÍ NGHIỆM CHÍNH TỪ SỰ SAO CHÉP HAI LẦN TRÊN 6 TẬP DỮ LIỆU.

FewNERD OntoNotes BBN FewRel TACRED ACE 2005
W A W A W A W A W A W A
EMR 2.87 1.66 0.67 0.50 2.41 1.28 0.70 0.70 0.49 0.51 0.55 0.17
EA-EMR 3.32 3.33 1.90 1.73 2.12 0.34 2.17 2.17 0.07 0.41 1.11 1.22
CRL 0.14 0.41 1.95 1.84 1.20 0.69 0.17 0.17 3.13 3.21 2.54 3.25
EMR* 0.68 0.44 0.29 0.19 0.12 0.13 0.16 0.16 2.30 2.27 2.98 2.52
EA-EMR* 1.61 0.12 0.73 0.81 0.53 0.10 0.52 0.52 0.91 0.52 1.18 2.10
CRL* 0.01 0.06 0.53 0.51 0.34 0.05 0.07 0.07 0.33 0.58 0.97 1.27
Dy-ConPET 0.12 0.01 0.02 0.02 1.66 0.34 0.57 0.57 0.34 0.66 1.17 1.02

"W" và "A" đại diện cho độ chính xác toàn bộ (%) và độ chính xác trung bình (%) tương ứng.

E. Kết quả Tổng thể

Kết quả thí nghiệm chính được hiển thị trong Bảng IV. Độ chính xác trung bình của các thiết lập khác nhau tại mỗi bước được hiển thị trong Hình 3. Từ bảng và hình, chúng ta có thể đi đến những kết luận sau.

(1) Xu hướng tổng thể: Tất cả các thiết lập đều giảm độ chính xác trung bình với sự gia tăng số lượng nhiệm vụ, điều này tiết lộ những tác động và thách thức không thể tránh khỏi do việc quên thảm khốc.

(2) Hiệu quả: Cả các thiết lập Static ConPET và Dynamic ConPET đều vượt qua các baseline với biên độ lớn, điều này tiết lộ hiệu quả của chúng. Vì tất cả ba phương pháp dựa trên bộ nhớ được thích ứng với Static ConPET đều vượt qua đáng kể các phiên bản chỉ PET tương ứng, chúng ta cũng có thể chứng minh tầm quan trọng của chiến lược lấy mẫu động. Tất nhiên, khoảng cách vẫn tồn tại giữa ConPET và cận trên tham chiếu với độ phức tạp tăng tuyến tính, cho thấy rằng một không gian lớn vẫn còn để khám phá trong việc điều chỉnh tinh cụ thể cho nhiệm vụ liên tục của LLMs.

(3) Tình huống Ứng dụng: Trong khi cả Dynamic ConPET và Static ConPET đều mang lại kết quả thỏa mãn, phiên bản Static xuất sắc trên năm benchmarks. Một lý do có thể là quy mô nhỏ của những tập dữ liệu này, phù hợp trong khả năng của một mô-đun PET duy nhất, do đó giảm thiểu tác động tiêu cực của khả năng mở rộng thấp. Đáng chú ý, Static ConPET thể hiện ưu thế đáng kể trong BBN và ACE 2005, có ít lược đồ kiến thức và ví dụ nhất trong số các tập dữ liệu gõ thực thể và trích xuất quan hệ tương ứng. Ngược lại, Dynamic ConPET vượt trội hơn Static ConPET trên FewNERD với một tập lược đồ lớn và nhiều ví dụ nhất. Do đó, chúng ta có thể kết luận rằng Static ConPET phù hợp hơn cho các tình huống với dữ liệu mới nổi và lược đồ kiến thức quy mô tương đối nhỏ. Ngược lại, chúng ta cần Dynamic ConPET để xử lý các tập lược đồ lớn hơn và dữ liệu rộng lớn hơn, đòi hỏi khả năng mở rộng cao hơn của kiến trúc điều chỉnh tinh liên tục.

Tất cả điểm số hiển thị trong Bảng IV ngoại trừ thiết lập "Limitless" là giá trị trung bình của kết quả từ một sự sao chép hai lần với các hạt giống ngẫu nhiên khác nhau. Các độ lệch chuẩn của những kết quả này được hiển thị trong Bảng V. Do độ phức tạp cực cao và chi phí huấn luyện của "Limitless", chúng tôi chỉ lấy kết quả của nó từ một lần chạy duy nhất.

F. Hiệu quả của Điều chỉnh Tham số Hiệu quả

Mặc dù sự thật rằng PET có thể giảm đáng kể chi phí cập nhật tham số và bộ nhớ GPU, nó có thể dẫn đến giảm hiệu suất tổng thể [4]. Do chi phí khổng lồ của việc điều chỉnh LLaMA-7B tham số đầy đủ, chúng tôi chỉ liên quan đến nghiên cứu ablation trên BERT-Large 335M nhỏ hơn [16] như một giải pháp thay thế để chứng minh tính hợp lý của thích ứng PET. Cụ thể, chúng tôi giới thiệu các thiết lập song song trên benchmark FewNERD

--- TRANG 8 ---
8
1 2 3 4 5 6 7 8 9 10
số nhiệm vụ 60 65 70 75 80 85 90 95 độ chính xác trung bình (%)
FewNERD
Dy-ConPET
EMR*
EA-EMR*
CRL*
EMR
EA-EMR
CRL
1 2 3 4 5 6 7 8 9 10 70 80 90 100
OntoNotes
1 2 3 4 5 6 7 8 9 10 70 80 90 100
BBN
1 2 3 4 5 6 7 8 9 10 85 90 95 100
FewRel
1 2 3 4 5 6 7 8 9 10 60 70 80 90
TACRED
1 2 3 4 5 40 50 60 70 80 90
ACE 2005

Hình 3. Độ chính xác trung bình (%) của các thiết lập khác nhau tại mỗi bước trong suốt quá trình học.

cho EMR*, EA-EMR*, CRL*, và Dynamic ConPET (Dy-ConPET) không có thích ứng PET. Kết quả được hiển thị trong Bảng VI. Như có thể quan sát, mặc dù ConPET chỉ có 0.12% tham số có thể điều chỉnh và do đó tiết kiệm thời gian và tài nguyên tính toán đáng kể, sự giảm độ chính xác không đáng kể, điều này chứng minh tính hợp lý của việc sử dụng PET. Kết quả thỏa mãn trên BERT-Large cũng minh họa khả năng tổng quát của ConPET đối với các mô hình ngôn ngữ được tiền huấn luyện tương đối nhỏ hơn.

G. Hiệu quả của Chọn trước Mô-đun PET

BẢNG VII
KẾT QUẢ ABLATION LIÊN QUAN ĐẾN CHỌN TRƯỚC MÔ-ĐUN PET TRONG DYNAMIC CONPET TRÊN FEWNERD VÀ ONTONOTES.

FewNERD OntoNotes
W A W A
Dy-ConPET 76.15 78.22 84.47 85.83
w/o Sel 70.75 76.22 81.85 83.37

"w/o Sel" tham chiếu đến thiết lập song song không có kỹ thuật này.

Trong Phần III-C1, chúng tôi thảo luận về hiệu quả của chọn trước mô-đun PET về hiệu quả, đó là chọn một số lượng cố định các mô-đun PET hoạt động trong Dynamic ConPET và đảm bảo chi phí lan truyền tiến cố định. Ở đây chúng tôi tiếp tục phân tích hiệu quả của nó từ khía cạnh hiệu suất. Chúng tôi tiến hành thí nghiệm trên một thiết lập song song không có kỹ thuật này, tương ứng với tình huống với t=k tại bước thứ k. Kết quả được hiển thị trong Bảng VII.

Như có thể quan sát, hiệu suất giảm đáng kể mà không có chọn trước mô-đun PET, mặc dù thiết lập "w/o Sel" làm cho tất cả các mô-đun PET hoạt động và có độ phức tạp tăng tuyến tính O(b·(iter1+iter2·(k+ 1))). Điều này có thể được quy cho sự can thiệp lẫn nhau giữa các vector logit được tạo ra bởi k mô-đun PET. Ngay cả khi kiến trúc động có thể giữ lại khả năng phân loại trên mỗi tập lược đồ nhiệm vụ độc lập, việc phân biệt giữa các lược đồ của các nhiệm vụ khác biệt là không tầm thường. Do đó, một bộ chọn mô-đun PET một cách rõ ràng chịu trách nhiệm này có thể tăng cường đáng kể hiệu suất tổng thể cũng như giảm độ phức tạp.

H. Hiệu quả của Các phân chia Nhiệm vụ Khác nhau

BẢNG VIII
HIỆU SUẤT CỦA DYNAMIC CONPET TRÊN FEWNERD VỚI CÁC PHÂN CHIA NHIỆM VỤ KHÁC BIỆT.

W A Accsel
Tương quan 76.15 78.22 80.16
Độc lập 77.62 76.18 88.91

"Tương quan" tham chiếu đến phân chia ngẫu nhiên có độ dài 10, trong khi "Độc lập" dựa trên các loại thô với độ dài 8. Accsel biểu thị độ chính xác top-1 của chọn trước mô-đun PET.

Trong các thí nghiệm của chúng tôi, các phân chia nhiệm vụ được tạo ra bằng cách phân cụm ngẫu nhiên các lược đồ kiến thức. Tuy nhiên, những lược đồ này có thể thể hiện sự tương quan, dẫn đến các nhiệm vụ không độc lập. Những tương quan như vậy có tiềm năng tác động đến hiệu suất của học liên tục, đặc biệt là Dynamic ConPET, có kiến trúc phụ thuộc nhiều vào phân chia nhiệm vụ. Do đó, phần này sẽ tập trung vào hiệu quả của các phân chia nhiệm vụ khác nhau lên Dynamic ConPET.

Dựa trên FewNERD, có lược đồ loại thực thể phân cấp với 8 loại thô, chúng tôi xây dựng lại một chuỗi nhiệm vụ độc lập lẫn nhau mới có độ dài 8 bằng cách gán mỗi loại thô cho một nhiệm vụ, trong khi phân chia ngẫu nhiên gốc là tương quan lẫn nhau. Kết quả được hiển thị trong Bảng VIII. Trong khi độ chính xác tổng thể không thay đổi đáng kể, phân chia độc lập tăng độ chính xác chọn trước với biên độ lớn, điều này ngụ ý một sự giảm trong độ chính xác của các mô-đun PET cụ thể cho nhiệm vụ. Điều này làm nổi bật một sự đánh đổi giữa chọn trước mô-đun PET và phân loại trong nhiệm vụ hạ nguồn, vì bộ chọn thượng nguồn ưa chuộng một phân chia nhiệm vụ độc lập nhưng các mô-đun PET hạ nguồn ưa chuộng một cái tương quan. Để đạt được kết quả tối ưu, chúng ta nên căn chỉnh khả năng của mỗi mô-đun PET (bao gồm cả bộ chọn) với độ khó nhiệm vụ của nó. Ví dụ, việc phân chia thêm một mô-đun PET cụ thể cho nhiệm vụ có thể có lợi nếu nhiệm vụ vượt quá khả năng của nó.

Hơn nữa, có thể hợp lý để khám phá một chiến lược khôn ngoan hơn để duy trì các phân chia nhiệm vụ. Một cải tiến có thể là áp dụng tổ chức phân cấp nhận biết kiến thức của các mô-đun PET. Cụ thể, thay vì giới hạn số lượng lớp của các mô-đun PET thành 2 (tức là một lớp của bộ chọn PET và một lớp của các mô-đun cụ thể cho nhiệm vụ), chúng ta có thể giới thiệu một cây mô-đun PET với nhiều lớp, nơi các mô-đun PET không lá chịu trách nhiệm tiến hành chọn trước trên các nút con của nó, và các mô-đun PET lá đưa ra dự đoán cuối cùng. Trong khi đó, xem xét bản chất phân cấp của một số lược đồ kiến thức (ví dụ, các loại thực thể trong FewNERD, OntoNotes, và BBN), chúng ta có thể gán trách nhiệm của các mô-đun PET theo vị trí của chúng trong cấu trúc phân cấp kiến thức thay vì thứ tự thời gian. Do đó, kiến thức phân cấp có thể được tích hợp một cách rõ ràng và khả năng của mỗi mô-đun PET có thể dễ dàng được kiểm soát với một lược đồ kiến thức được thiết kế tốt. Chúng tôi để lại cải tiến này cho công việc tương lai.

V. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi chủ yếu thảo luận về việc thích ứng hiệu quả và có hiệu quả của LLMs với các chuỗi nhiệm vụ hạ nguồn liên tục.

--- TRANG 9 ---
9
BẢNG VI
KẾT QUẢ ABLATION LIÊN QUAN ĐẾN THÍCH ỨNG PET TRONG CONPET DỰA TRÊN BERT-LARGE VÀ BENCHMARK FEWNERD.

PET w/o PET
W A W A
EMR* 74.03 76.52 76.19 77.50
EA-EMR* 74.59 77.96 77.64 75.90
CRL* 73.80 75.87 77.29 77.70
Dy-ConPET 74.25 76.60 76.97 76.87
#param 0.4M 335M

"PET" có nghĩa là thiết lập ConPET gốc và "w/o PET" tham chiếu đến thiết lập song song không có PET. "#param" là số lượng tham số có thể điều chỉnh.

Để đạt được mục tiêu này, chúng tôi đề xuất mô hình ConPET, bao gồm hai phiên bản với độ phức tạp huấn luyện độc lập với số lượng nhiệm vụ. Static ConPET có thể thích ứng các phương pháp dựa trên bộ nhớ trước đây với LLMs thông qua PET tiết kiệm chi phí và một chiến lược lấy mẫu động mạnh mẽ hơn đối với quá khớp và quên. Ngược lại, Dynamic ConPET có khả năng mở rộng hơn đối với các tình huống với dữ liệu và lược đồ nhiệm vụ quy mô lớn nhờ kiến trúc MoE-style động của nó. Các thí nghiệm chứng minh hiệu quả và tính hợp lý của các kỹ thuật chính được sử dụng trong ConPET, với một sự giảm đáng kể trong chi phí điều chỉnh. Trong tương lai, chúng tôi sẽ mở rộng ConPET sang các tình huống học liên tục đa dạng hơn (ví dụ, học liên tục các công cụ không đồng nhất [8]) và tiếp tục cải thiện mô hình của chúng tôi bằng cách khám phá các chiến lược phân chia nhiệm vụ khôn ngoan hơn.

PHỤ LỤC
THÊM SỰ THẬT HỖ TRỢ CHO TÍNH HỢP LÝ CỦA CHIẾN LƯỢC LẤY MẪU ĐỘNG

Mặc dù giả định phổ biến về bộ nhớ giới hạn trong lĩnh vực học liên tục [14], chúng tôi vẫn coi việc kiểm soát chi phí huấn luyện thay vì bộ nhớ trong quá trình điều chỉnh tinh LLMs là hợp lý hơn, đó là triết lý cơ bản của chiến lược lấy mẫu động của chúng tôi. Tính hợp lý của phương pháp này chủ yếu nằm ở giá cả áp đảo của thời gian và tài nguyên tính toán khi so sánh với lưu trữ.

Lấy GPT-3 175B [1] làm ví dụ, có corpus huấn luyện chứa khoảng 300B token. Vì một token tiếng Anh duy nhất chứa khoảng 4 ký tự trung bình¹, tổng lưu trữ cho corpus huấn luyện của nó khoảng 1∼2TB. Quy mô bộ nhớ như vậy khá chấp nhận được cho phần lớn các máy chủ hiện đại cho nghiên cứu AI. Mặc dù các tài liệu huấn luyện cho các LLMs gần đây hơn được tin là chiếm nhiều không gian hơn, giá cả lưu trữ đã được làm cho vừa phải không quá vài chục đô la mỗi TB nói chung nhờ vào tiến bộ trong công nghệ phần cứng lưu trữ². Bên cạnh đó, vì các tập dữ liệu của hầu hết các nhiệm vụ hạ nguồn nên được lọc và chú thích để đảm bảo chất lượng cao và cung cấp giám sát cho huấn luyện, quy mô của chúng khó có thể đạt đến mức TB và thường cần ít lưu trữ hơn nhiều so với corpus huấn luyện LLM.

Mặt khác, tài nguyên tính toán cần thiết cho GPT-3 175B (được tiền huấn luyện trên GPUs V100) ở một quy mô to lớn hơn, khoảng 3.64E+03 petaflop/s-days. Ngay cả khi PET có thể tiết kiệm nhiều tài nguyên tính toán, một GPU V100 duy nhất vẫn tốn hàng nghìn đô la trong một tháng³, chưa kể đến việc LLMs với hơn hàng chục tỷ tham số có thể cần hơn một thiết bị cho điều chỉnh tinh và suy luận. Tóm lại, vấn đề lưu trữ chỉ có ý nghĩa nhỏ so với áp lực tài chính do thời gian và tài nguyên tính toán gây ra.

TÀI LIỆU THAM KHẢO
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[2] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," arXiv preprint arXiv:2109.01652, 2021. [Online]. Available: https://arxiv.org/pdf/2109.01652

[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf

[4] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen et al., "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models," arXiv preprint arXiv:2203.06904, 2022. [Online]. Available: https://arxiv.org/pdf/2203.06904.pdf

[5] X. Jin, D. Zhang, H. Zhu, W. Xiao, S.-W. Li, X. Wei, A. Arnold, and X. Ren, "Lifelong pretraining: Continually adapting language models to emerging corpora," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 4764–4780. [Online]. Available: https://aclanthology.org/2022.naacl-main.351.pdf

[6] A. Daruna, M. Gupta, M. Sridharan, and S. Chernova, "Continual learning of knowledge graph embeddings," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 1128–1135, 2021. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/9343669

[7] N. Monaikul, G. Castellucci, S. Filice, and O. Rokhlenko, "Continual learning for named entity recognition," in Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 13 570–13 577. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/17600/17407

[8] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C. Xiao, C. Han et al., "Tool learning with foundation models," arXiv preprint arXiv:2304.08354, 2023. [Online]. Available: https://arxiv.org/pdf/2304.08354

[9] K. Zhao, H. Xu, J. Yang, and K. Gao, "Consistent representation learning for continual relation extraction," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 3402–3411. [Online]. Available: https://aclanthology.org/2022.findings-acl.268.pdf

[10] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, "Continual lifelong learning with neural networks: A review," Neural Networks, vol. 113, pp. 54–71, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0893608019300231

[11] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, "Progressive neural networks," arXiv preprint arXiv:1606.04671, 2016. [Online]. Available: https://arxiv.org/pdf/1606.04671.pdf

[12] X. Gu, L. Liu, H. Yu, J. Li, C. Chen, and J. Han, "On the transformer growth for progressive BERT training," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 5174–5180. [Online]. Available: https://aclanthology.org/2021.naacl-main.406.pdf

[13] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne, "Experience replay for continual learning," Advances in Neural Information Processing Systems, vol. 32, 2019. [Online]. Available: https://proceedings.neurips.cc/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf

[14] H. Wang, W. Xiong, M. Yu, X. Guo, S. Chang, and W. Y. Wang, "Sentence embedding alignment for lifelong relation extraction," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 796–806. [Online]. Available: https://aclanthology.org/N19-1086.pdf

[15] X. Han, Y. Dai, T. Gao, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "Continual relation learning via episodic memory activation and reconsolidation," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 6429–6440. [Online]. Available: https://aclanthology.org/2020.acl-main.573.pdf

[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018. [Online]. Available: https://arxiv.org/pdf/1810.04805.pdf

--- TRANG 10 ---
10
Để đạt được mục tiêu này, chúng tôi đề xuất mô hình ConPET, bao gồm hai phiên bản với độ phức tạp huấn luyện độc lập với số lượng nhiệm vụ. Static ConPET có thể thích ứng các phương pháp dựa trên bộ nhớ trước đây với LLMs thông qua PET tiết kiệm chi phí và một chiến lược lấy mẫu động mạnh mẽ hơn đối với quá khớp và quên. Ngược lại, Dynamic ConPET có khả năng mở rộng hơn đối với các tình huống với dữ liệu và lược đồ nhiệm vụ quy mô lớn nhờ kiến trúc MoE-style động của nó. Các thí nghiệm chứng minh hiệu quả và tính hợp lý của các kỹ thuật chính được sử dụng trong ConPET, với một sự giảm đáng kể trong chi phí điều chỉnh. Trong tương lai, chúng tôi sẽ mở rộng ConPET sang các tình huống học liên tục đa dạng hơn (ví dụ, học liên tục các công cụ không đồng nhất [8]) và tiếp tục cải thiện mô hình của chúng tôi bằng cách khám phá các chiến lược phân chia nhiệm vụ khôn ngoan hơn.

PHỤ LỤC
THÊM SỰ THẬT HỖ TRỢ CHO TÍNH HỢP LÝ CỦA CHIẾN LƯỢC LẤY MẪU ĐỘNG

Mặc dù giả định phổ biến về bộ nhớ giới hạn trong lĩnh vực học liên tục [14], chúng tôi vẫn coi việc kiểm soát chi phí huấn luyện thay vì bộ nhớ trong quá trình điều chỉnh tinh LLMs là hợp lý hơn, đó là triết lý cơ bản của chiến lược lấy mẫu động của chúng tôi. Tính hợp lý của phương pháp này chủ yếu nằm ở giá cả áp đảo của thời gian và tài nguyên tính toán khi so sánh với lưu trữ.

Lấy GPT-3 175B [1] làm ví dụ, có corpus huấn luyện chứa khoảng 300B token. Vì một token tiếng Anh duy nhất chứa khoảng 4 ký tự trung bình¹, tổng lưu trữ cho corpus huấn luyện của nó khoảng 1∼2TB. Quy mô bộ nhớ như vậy khá chấp nhận được cho phần lớn các máy chủ hiện đại cho nghiên cứu AI. Mặc dù các tài liệu huấn luyện cho các LLMs gần đây hơn được tin là chiếm nhiều không gian hơn, giá cả lưu trữ đã được làm cho vừa phải không quá vài chục đô la mỗi TB nói chung nhờ vào tiến bộ trong công nghệ phần cứng lưu trữ². Bên cạnh đó, vì các tập dữ liệu của hầu hết các nhiệm vụ hạ nguồn nên được lọc và chú thích để đảm bảo chất lượng cao và cung cấp giám sát cho huấn luyện, quy mô của chúng khó có thể đạt đến mức TB và thường cần ít lưu trữ hơn nhiều so với corpus huấn luyện LLM.

Mặt khác, tài nguyên tính toán cần thiết cho GPT-3 175B (được tiền huấn luyện trên GPUs V100) ở một quy mô to lớn hơn, khoảng 3.64E+03 petaflop/s-days. Ngay cả khi PET có thể tiết kiệm nhiều tài nguyên tính toán, một GPU V100 duy nhất vẫn tốn hàng nghìn đô la trong một tháng³, chưa kể đến việc LLMs với hơn hàng chục tỷ tham số có thể cần hơn một thiết bị cho điều chỉnh tinh và suy luận. Tóm lại, vấn đề lưu trữ chỉ có ý nghĩa nhỏ so với áp lực tài chính do thời gian và tài nguyên tính toán gây ra.

TÀI LIỆU THAM KHẢO
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[2] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," arXiv preprint arXiv:2109.01652, 2021. [Online]. Available: https://arxiv.org/pdf/2109.01652

[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf

[4] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen et al., "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models," arXiv preprint arXiv:2203.06904, 2022. [Online]. Available: https://arxiv.org/pdf/2203.06904.pdf

[5] X. Jin, D. Zhang, H. Zhu, W. Xiao, S.-W. Li, X. Wei, A. Arnold, and X. Ren, "Lifelong pretraining: Continually adapting language models to emerging corpora," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 4764–4780. [Online]. Available: https://aclanthology.org/2022.naacl-main.351.pdf

[6] A. Daruna, M. Gupta, M. Sridharan, and S. Chernova, "Continual learning of knowledge graph embeddings," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 1128–1135, 2021. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/9343669

[7] N. Monaikul, G. Castellucci, S. Filice, and O. Rokhlenko, "Continual learning for named entity recognition," in Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 13 570–13 577. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/17600/17407

[8] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y. Huang, C. Xiao, C. Han et al., "Tool learning with foundation models," arXiv preprint arXiv:2304.08354, 2023. [Online]. Available: https://arxiv.org/pdf/2304.08354

[9] K. Zhao, H. Xu, J. Yang, and K. Gao, "Consistent representation learning for continual relation extraction," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 3402–3411. [Online]. Available: https://aclanthology.org/2022.findings-acl.268.pdf

[10] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, "Continual lifelong learning with neural networks: A review," Neural Networks, vol. 113, pp. 54–71, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0893608019300231

[11] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, "Progressive neural networks," arXiv preprint arXiv:1606.04671, 2016. [Online]. Available: https://arxiv.org/pdf/1606.04671.pdf

[12] X. Gu, L. Liu, H. Yu, J. Li, C. Chen, and J. Han, "On the transformer growth for progressive BERT training," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 5174–5180. [Online]. Available: https://aclanthology.org/2021.naacl-main.406.pdf

[13] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne, "Experience replay for continual learning," Advances in Neural Information Processing Systems, vol. 32, 2019. [Online]. Available: https://proceedings.neurips.cc/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf

[14] H. Wang, W. Xiong, M. Yu, X. Guo, S. Chang, and W. Y. Wang, "Sentence embedding alignment for lifelong relation extraction," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 796–806. [Online]. Available: https://aclanthology.org/N19-1086.pdf

[15] X. Han, Y. Dai, T. Gao, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "Continual relation learning via episodic memory activation and reconsolidation," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 6429–6440. [Online]. Available: https://aclanthology.org/2020.acl-main.573.pdf

[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018. [Online]. Available: https://arxiv.org/pdf/1810.04805.pdf

--- TRANG 11 ---
11
[17] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "RoBERTa: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019. [Online]. Available: https://arxiv.org/pdf/1907.11692.pdf

[18] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for NLP," in International Conference on Machine Learning. PMLR, 2019, pp. 2790–2799. [Online]. Available: http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf

[19] X. L. Li and P. Liang, "Prefix-Tuning: Optimizing continuous prompts for generation," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 4582–4597. [Online]. Available: https://aclanthology.org/2021.acl-long.353.pdf

[20] T. Gao, A. Fisch, and D. Chen, "Making pre-trained language models better few-shot learners," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 3816–3830. [Online]. Available: https://aclanthology.org/2021.acl-long.295.pdf

[21] J. Lee, R. Tang, and J. Lin, "What would elsa do? freezing layers during transformer fine-tuning," arXiv preprint arXiv:1911.03090, 2019. [Online]. Available: https://arxiv.org/pdf/1911.03090.pdf

[22] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Schütze, "Masking as an efficient alternative to finetuning for pretrained language models," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, 2020, pp. 2226–2241. [Online]. Available: https://aclanthology.org/2020.emnlp-main.174.pdf

[23] E. B. Zaken, Y. Goldberg, and S. Ravfogel, "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2022, pp. 1–9. [Online]. Available: https://aclanthology.org/2022.acl-short.1.pdf

[24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "LoRA: Low-rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021. [Online]. Available: https://arxiv.org/pdf/2106.09685.pdf

[25] Y. Qin, X. Wang, Y. Su, Y. Lin, N. Ding, Z. Liu, J. Li, L. Hou, P. Li, M. Sun et al., "Exploring low-dimensional intrinsic task subspace via prompt tuning," arXiv preprint arXiv:2110.07867, 2021. [Online]. Available: https://arxiv.org/pdf/2110.07867.pdf

[26] X. Sun, Y. Ji, B. Ma, and X. Li, "A comparative study between full-parameter and lora-based fine-tuning on chinese instruction data for instruction following large language model," arXiv preprint arXiv:2304.08109, 2023. [Online]. Available: https://arxiv.org/pdf/2304.08109.pdf

[27] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., "LLaMA: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023. [Online]. Available: https://arxiv.org/pdf/2302.13971.pdf

[28] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., "Overcoming catastrophic forgetting in neural networks," Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521–3526, 2017. [Online]. Available: https://www.pnas.org/doi/epdf/10.1073/pnas.1611835114

[29] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, "Overcoming catastrophic forgetting by incremental moment matching," in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 4655–4665. [Online]. Available: https://proceedings.neurips.cc/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf

[30] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, "Riemannian walk for incremental learning: Understanding forgetting and intransigence," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 532–547. [Online]. Available: https://openaccess.thecvf.com/content_ECCV_2018/papers/Arslan_Chaudhry_Riemannian_Walk_ECCV_2018_paper.pdf

[31] Z. Li and D. Hoiem, "Learning without forgetting," IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 12, pp. 2935–2947, 2017. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/8107520

[32] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang, and C.-C. J. Kuo, "Class-incremental learning via deep model consolidation," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 1131–1140. [Online]. Available: https://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Class-incremental_Learning_via_Deep_Model_Consolidation_WACV_2020_paper.pdf

[33] T. Chen, I. Goodfellow, and J. Shlens, "Net2Net: Accelerating learning via knowledge transfer," arXiv preprint arXiv:1511.05641, 2015. [Online]. Available: https://arxiv.org/pdf/1511.05641.pdf

[34] D. Isele and A. Cosgun, "Selective experience replay for lifelong learning," in Proceedings of the AAAI Conference on Artificial Intelligence, 2018. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/11595/11454

[35] C. Qin and S. Joty, "LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of T5," in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/pdf?id=HCRVf71PMF

[36] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, and A. Almahairi, "Progressive prompts: Continual learning for language models," in The Eleventh International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/pdf?id=UJTgQBc91

[37] A. Madotto, Z. Lin, Z. Zhou, S. Moon, P. A. Crook, B. Liu, Z. Yu, E. Cho, P. Fung, and Z. Wang, "Continual learning in task-oriented dialogue systems," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 7452–7467. [Online]. Available: https://aclanthology.org/2021.emnlp-main.590.pdf

[38] Q. Gao, C. Zhao, Y. Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, "A unified continual learning framework with general parameter-efficient tuning," arXiv preprint arXiv:2303.10070, 2023. [Online]. Available: https://arxiv.org/pdf/2303.10070.pdf

[39] Y. Qin, J. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "Elle: Efficient lifelong pre-training for emerging data," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 2789–2810. [Online]. Available: https://aclanthology.org/2022.findings-acl.220.pdf

[40] Z. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu, "Continual pre-training of language models," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/pdf?id=m_GDIItaI3o

[41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," in International Conference on Learning Representations, 2017. [Online]. Available: https://openreview.net/pdf?id=B1ckMDqlg

[42] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "GShard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020. [Online]. Available: https://arxiv.org/pdf/2006.16668

[43] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, "BASE Layers: Simplifying training of large, sparse models," in International Conference on Machine Learning. PMLR, 2021, pp. 6265–6274. [Online]. Available: http://proceedings.mlr.press/v139/lewis21a/lewis21a.pdf

[44] S. Roller, S. Sukhbaatar, J. Weston et al., "Hash layers for large sparse models," Advances in Neural Information Processing Systems, vol. 34, pp. 17 555–17 566, 2021. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2021/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf

[45] W. Fedus, B. Zoph, and N. Shazeer, "Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity," The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270, 2022. [Online]. Available: https://jmlr.org/papers/volume23/21-0998/21-0998.pdf

[46] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, "MoEfication: Transformer feed-forward layers are mixtures of experts," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 877–890. [Online]. Available: https://aclanthology.org/2022.findings-acl.71.pdf

[47] S. Gururangan, M. Lewis, A. Holtzman, N. A. Smith, and L. Zettlemoyer, "DEMix layers: Disentangling domains for modular language modeling," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 5557–5576. [Online]. Available: https://aclanthology.org/2022.naacl-main.407.pdf

[48] S. Shen, L. Hou, Y. Zhou, N. Du, S. Longpre, J. Wei, H. W. Chung, B. Zoph, W. Fedus, X. Chen, T. Vu, Y. Wu, W. Chen, A. Webson, Y. Li, V. Zhao, H. Yu, K. Keutzer, T. Darrell, and D. Zhou,

--- TRANG 12 ---
12
"Mixture-of-experts meets instruction tuning: A winning combination for large language models," arXiv preprint arXiv:2305.14705, 2023. [Online]. Available: https://arxiv.org/pdf/2305.14705.pdf

[49] T. Zadouri, A. Üstün, A. Ahmadian, B. Ermiş, A. Locatelli, and S. Hooker, "Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning," arXiv preprint arXiv:2309.05444, 2023. [Online]. Available: https://arxiv.org/pdf/2309.05444.pdf

[50] L. B. Soares, N. Fitzgerald, J. Ling, and T. Kwiatkowski, "Matching the blanks: Distributional similarity for relation learning," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2895–2905. [Online]. Available: https://aclanthology.org/P19-1279.pdf

[51] N. Ding, Y. Chen, X. Han, G. Xu, P. Xie, H.-T. Zheng, Z. Liu, J. Li, and H.-G. Kim, "Prompt-learning for fine-grained entity typing," arXiv preprint arXiv:2108.10604, 2021. [Online]. Available: https://arxiv.org/pdf/2108.10604.pdf

[52] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 3045–3059. [Online]. Available: https://aclanthology.org/2021.emnlp-main.243.pdf

[53] N. Ding, G. Xu, Y. Chen, X. Wang, X. Han, P. Xie, H. Zheng, and Z. Liu, "Few-NERD: A few-shot named entity recognition dataset," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 3198–3213. [Online]. Available: https://aclanthology.org/2021.acl-long.248.pdf

[54] R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Pradhan, L. Ramshaw, N. Xue, A. Taylor, J. Kaufman, M. Franchini et al., "OntoNotes release 5.0 ldc2013t19," Linguistic Data Consortium, Philadelphia, PA, vol. 23, 2013. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2013T19

[55] R. Weischedel and A. Brunstein, "BBN pronoun coreference and entity type corpus," Linguistic Data Consortium, Philadelphia, vol. 112, 2005. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2005T33

[56] X. Han, H. Zhu, P. Yu, Z. Wang, Y. Yao, Z. Liu, and M. Sun, "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 4803–4809. [Online]. Available: https://aclanthology.org/D18-1514.pdf

[57] Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning, "Position-aware attention and supervised data improve slot filling," in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 35–45. [Online]. Available: https://nlp.stanford.edu/pubs/zhang2017tacred.pdf

[58] C. Walker, S. Strassel, J. Medero, and K. Maeda, "ACE 2005 multilingual training corpus," Linguistic Data Consortium, Philadelphia, vol. 57, p. 45, 2006. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2006T06
