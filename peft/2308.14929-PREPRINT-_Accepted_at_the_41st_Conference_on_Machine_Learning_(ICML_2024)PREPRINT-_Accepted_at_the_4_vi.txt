# 2308.14929.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.14929.pdf
# Kích thước tệp: 2230773 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PREPRINT: Được chấp nhận tại Hội nghị Học máy lần thứ 41 (ICML 2024)PREPRINT: Được chấp nhận tại Hội nghị Học máy lần thứ 41 (ICML 2024)
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện
Samuel Horváth1Stefanos Laskaridis2Shashank Rajput3Hongyi Wang4

Tóm tắt
Mạng Nơ-ron Sâu (DNN) đã là động lực chính cho những đột phá AI trong những năm gần đây. Tuy nhiên, các mô hình này ngày càng trở nên lớn hơn khi chúng trở nên chính xác và an toàn hơn. Điều này có nghĩa là việc huấn luyện chúng ngày càng tốn kém và mất thời gian và thường chỉ tạo ra một mô hình duy nhất để phù hợp với tất cả các mục tiêu. Nhiều kỹ thuật khác nhau đã được đề xuất trong tài liệu để giảm thiểu điều này, bao gồm cắt tỉa, thưa thớt hóa, hoặc lượng tử hóa trọng số mô hình và cập nhật. Mặc dù đạt được tỷ lệ nén cao, chúng thường gây ra chi phí tính toán đáng kể trong quá trình huấn luyện hoặc dẫn đến hình phạt độ chính xác không thể bỏ qua. Thay vào đó, các phương pháp phân tích đã được tận dụng để nén hạng thấp của DNN. Tương tự, các kỹ thuật như vậy (ví dụ: SVD) thường dựa vào việc phân tích lặp đi lặp lại nặng nề của các lớp và có thể không tối ưu cho các mô hình phi tuyến, chẳng hạn như DNN. Chúng tôi tiến thêm một bước trong việc thiết kế các mô hình hạng thấp hiệu quả và đề xuất MAESTRO, một khung làm việc cho các lớp hạng thấp có thể huấn luyện. Thay vì áp dụng lặp đi lặp lại các phân tích tiên nghiệm, cấu trúc hạng thấp được tích hợp vào quá trình huấn luyện thông qua LOD, một phân tích có thứ tự hạng thấp. Không chỉ đây là lần đầu tiên sắp xếp tầm quan trọng thông qua lấy mẫu được áp dụng trên cấu trúc DNN được phân tách, mà nó còn cho phép chọn hạng ở mức độ chi tiết của lớp. Phân tích lý thuyết của chúng tôi chứng minh rằng trong các trường hợp đặc biệt LOD khôi phục phân tích SVD và PCA. Được áp dụng cho DNN, MAESTRO cho phép trích xuất các mô hình có dấu chân thấp hơn mà vẫn bảo toàn hiệu suất. Đồng thời, nó cho phép sự đánh đổi nhẹ nhàng giữa độ chính xác-độ trễ để triển khai trên các thiết bị bị hạn chế hơn mà không cần huấn luyện lại.

1Đại học Mohamed bin Zayed về Trí tuệ Nhân tạo (MBZUAI), Abu Dhabi, UAE2Brave Software, London, UK3DataBricks, San Francisco, USA4Đại học Carnegie Mellon, Pittsburgh, USA. Liên hệ với: Samuel Horváth <samuel.horvath@mbzuai.ac.ae>, Stefanos Laskaridis <mail@stefanos.cc>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy, Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

1. Giới thiệu
Học Sâu đã trải qua một sự tiếp nhận chưa từng có, với các mô hình đạt được hiệu suất ở mức (siêu-)con người trong nhiều nhiệm vụ trên các phương thức, sinh ra các trợ lý thông minh hơn (Radford et al., 2023) và các hệ thống nhận thức thị giác và tạo sinh thế hệ tiếp theo (Radford et al., 2021). Tuy nhiên, cái giá của hiệu suất này là các mô hình ngày càng trở nên lớn hơn đáng kể, với việc huấn luyện và triển khai ngày càng tốn kém (Laskaridis et al., 2024). Do đó, các kỹ thuật từ Học máy Hiệu quả trở nên ngày càng có liên quan (Wan et al., 2023), và là yêu cầu để triển khai trong các thiết bị bị hạn chế, chẳng hạn như điện thoại thông minh hoặc thiết bị IoT (Laskaridis et al., 2022).

Các kỹ thuật điển hình để nén mạng bao gồm i) lượng tử hóa, tức là giảm độ chính xác của mô hình (Wang et al., 2019) hoặc các cập nhật được truyền đạt (Seide et al., 2014; Alistarh et al., 2017), ii) cắt tỉa mô hình trong quá trình huấn luyện, ví dụ: thông qua Giả thuyết Vé số May mắn (LTH) (Frankle & Carbin, 2019), iii) thưa thớt hóa biểu diễn mạng và cập nhật, tức là bỏ qua tập con tọa độ (Suresh et al., 2017; Alistarh et al., 2018) hoặc iv) xấp xỉ hạng thấp (Wang et al., 2021; Dudziak et al., 2019), tức là giữ lại các hạng có liên quan nhất của mạng được phân tách. Mặc dù có lợi ích trong quá trình triển khai, tức là mô hình có dấu chân thấp hơn, trong nhiều trường hợp, chi phí trong thời gian huấn luyện hoặc sự suy giảm độ chính xác có thể không thể bỏ qua. Hơn nữa, nhiều kỹ thuật có thể giới thiệu nhiều siêu tham số hoặc nhu cầu tinh chỉnh để khôi phục độ chính xác bị mất.

Trong công trình này, chúng tôi tập trung vào phân tích hạng thấp huấn luyện. Cụ thể, chúng tôi chỉ ra những thách thức của các kỹ thuật (Wang et al., 2021; 2023) khi phân tách các tham số của mỗi lớp trong không gian hạng thấp và nhu cầu tìm ra các hạng tối ưu cho từng lớp tại thời điểm huấn luyện. Để giải quyết điều này, chúng tôi đề xuất LOD (Phân tích có thứ tự Hạng thấp), một phiên bản mở rộng không tầm thường của kỹ thuật Dropout có thứ tự từ Horváth et al. (2021), được áp dụng để tìm dần phân tích tối ưu cho mỗi lớp của DNN trong khi huấn luyện (Hình 1). Sự khác biệt quan trọng so với công trình trước bao gồm i) tính không đồng nhất của không gian tìm kiếm (tức là chúng tôi cho phép các hạng khác nhau cho mỗi lớp), ii) khía cạnh có thể huấn luyện của phân tích để phản ánh phân phối dữ liệu, và iii) lợi ích cho thời gian huấn luyện và triển khai mà không hy sinh độ chính xác.

--- TRANG 2 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

độ chính xác. Tuy nhiên, chúng tôi cũng cung cấp cơ chế đánh đổi độ trễ-độ chính xác để triển khai mô hình trên các thiết bị bị hạn chế hơn.

Đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi đề xuất MAESTRO1, một kỹ thuật phân tách lớp mới cho phép học các lớp hạng thấp theo cách tiến bộ trong khi huấn luyện. Chúng tôi kết hợp phân tích lớp và dropout có thứ tự thành LOD, một biến thể mở rộng của dropout có thứ tự, theo cách mới, bằng cách nhúng tầm quan trọng có thứ tự trực tiếp vào các trọng số được phân tích. Bằng cách phân tách các lớp và huấn luyện trên các mô hình hạng thấp được lấy mẫu ngẫu nhiên, chúng tôi áp dụng biểu diễn phân tách tầm quan trọng có thứ tự của mỗi lớp. Chúng tôi kết hợp điều này với một số hạng lasso nhóm phân cấp (Yuan & Lin, 2006) trong hàm mất mát để loại bỏ các hạng dư thừa và thu hẹp dần không gian hạng. Bằng cách này, chúng tôi cho phép huấn luyện hiệu quả về mặt tính toán được đạt được bởi phân tích được đề xuất mà không dựa vào các phân tích lặp không chính xác và có thể tốn kém về mặt tính toán như Phân tích Giá trị Đơn lẻ (SVD).

• MAESTRO về cơ bản là một phương pháp có động lực lý thuyết nhúng phân tích vào huấn luyện. Đầu tiên, chúng tôi chỉ ra rằng mục tiêu mới của chúng tôi có thể khôi phục i) SVD của ánh xạ tuyến tính mục tiêu cho trường hợp đặc biệt của phân phối dữ liệu đồng nhất và ii) Phân tích Thành phần Chính (PCA) của dữ liệu trong trường hợp ánh xạ đồng nhất.

• Vì phân tích của MAESTRO là một phần của quá trình huấn luyện, nó cũng tính đến phân phối dữ liệu và hàm mục tiêu, trái ngược với SVD hoạt động trực tiếp trên các trọng số đã học. Chúng tôi chỉ ra rằng vấn đề này đã xuất hiện với một mô hình tuyến tính đơn giản và khái quát hóa thực nghiệm kết quả của chúng tôi trong trường hợp DNN, bằng cách áp dụng phương pháp của chúng tôi cho các loại lớp khác nhau (bao gồm fully-connected, convolutional, và attention) trải dài trên ba bộ dữ liệu và phương thức.

• Chúng tôi minh họa rằng kỹ thuật của chúng tôi đạt được kết quả tốt hơn so với các baseline dựa trên SVD với chi phí thấp hơn. Một cách chỉ thị, MAESTRO có thể đạt được kết quả ngang bằng hoặc tốt hơn so với các phương pháp hạng thấp SOTA trên các bộ dữ liệu thị giác (CIFAR-10, ImageNet) với chi phí huấn luyện thấp hơn do thu hẹp tiến bộ, trong khi cùng lúc đó nó đạt độ phức tạp thấp hơn 6% với một phần tư chi phí tính toán và một nửa tham số của các biến thể SVD trong các mô hình Transformer.

2. Công trình Liên quan

Chủ đề Học máy Hiệu quả đã nhận được rất nhiều sự chú ý trong suốt thập kỷ qua khi các mạng ngày càng trở nên tốn kém về mặt tính toán. Chúng tôi phân biệt giữa thời gian huấn luyện và triển khai, với thời gian sau có tác động đáng kể hơn và do đó bù đắp chi phí tiềm tàng trong quá trình huấn luyện. Tuy nhiên, tối ưu hóa chi phí ngày càng có liên quan để huấn luyện các mô hình lớn, và sự ra đời của Học Liên bang (McMahan et al., 2017), huấn luyện hiệu quả ngày càng trở nên có liên quan để duy trì tính khả thi.

1Việc triển khai có thể được tìm thấy tại đây: https://github.com/SamuelHorvath/Maestro-LoD

Suy luận hiệu quả. Để triển khai hiệu quả, nhiều kỹ thuật khác nhau đã được đề xuất để tối ưu hóa kiến trúc của DNN theo cách thủ công (Howard et al., 2017) hoặc tự động (tức là NAS) (Tan & Le, 2019), chúng loại bỏ tính toán dư thừa bằng cách cắt tỉa các phần của mạng (Han et al., 2015; Carreira-Perpinán & Idelbayev, 2018; Frankle & Carbin, 2019; Chen et al., 2021; Sreenivasan et al., 2022; Li et al., 2016; Wen et al., 2016; Hu et al., 2016; Wen et al., 2016; Zhu & Gupta, 2017; He et al., 2017; Yang et al., 2017; Liu et al., 2018; Yu & Huang, 2019b), theo cách có cấu trúc hoặc không có cấu trúc, hoặc sử dụng biểu diễn độ chính xác thấp (Wang et al., 2019) của các neuron và kích hoạt. Tuy nhiên, các kỹ thuật như vậy có thể liên quan đến chi phí huấn luyện không thể bỏ qua hoặc thiếu tính linh hoạt của dấu chân biến đổi khi triển khai. Gần với phương pháp của chúng tôi, đã có các kỹ thuật tận dụng xấp xỉ hạng thấp (ví dụ: SVD) để suy luận hiệu quả (Xue et al., 2013; Sainath et al., 2013; Jaderberg et al., 2014; Wiesler et al., 2014; Dudziak et al., 2019). Cuối cùng, có một loại kỹ thuật thay đổi kích thước mạng động tại thời gian chạy để tính toán, bộ nhớ hoặc hiệu quả năng lượng, dựa trên early-exiting (Laskaridis et al., 2021) hoặc dynamic-width (Yu et al., 2019) và tận dụng sự đánh đổi độ chính xác-độ trễ.

Huấn luyện hiệu quả. Mặt khác, các kỹ thuật để huấn luyện hiệu quả trở nên rất có liên quan ngày nay khi mở rộng kích thước DNN (Hu et al., 2021) hoặc triển khai đến các thiết bị nhúng (Lin et al., 2022), và thường cung cấp lợi ích bổ sung tại thời điểm triển khai. Hướng tới mục tiêu này, đã có các phương pháp được sử dụng trong đó một phần của mạng được che (Sidahmed et al., 2021) hoặc bỏ (Alam et al., 2022; Caldas et al., 2019; Wu et al., 2018) trong quá trình huấn luyện, với mục tiêu tối thiểu hóa dấu chân huấn luyện. Tương tự như early-exiting, các biến thể multi-exit để huấn luyện hiệu quả (Kim et al., 2023; Liu et al., 2022) đã được đề xuất, và điều tương tự áp dụng cho việc mở rộng dựa trên chiều rộng (Horváth et al., 2021; Diao et al., 2021). Cuối cùng nhưng không kém phần quan trọng, trong kỷ nguyên của transformers và LLM, nơi các mạng đã mở rộng theo hàm mũ về kích thước, các kỹ thuật dựa trên PEFT, chẳng hạn như fine-tuning dựa trên adapter (Houlsby et al., 2019) (như LoRA (Hu et al., 2021)), trở nên ngày càng quan trọng và tạo ra sự khác biệt quan trọng để giải quyết các nhiệm vụ downstream.

Học biểu diễn có thứ tự. Ordered Dropout (OD) được đề xuất như một cơ chế để cắt tỉa dựa trên tầm quan trọng để trích xuất dễ dàng các mạng con được thiết kế để cho phép huấn luyện liên bang không đồng nhất (Horváth et al., 2021). Các cấu trúc tương tự được đề xuất để áp dụng trong lớp biểu diễn của autoencoders (Rippel et al., 2014) để

--- TRANG 3 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

[Phần này chứa một hình ảnh mô tả cấu trúc MAESTRO với các thành phần: Layer K+1, vlayer K, Original Mapping, Factorized Mapping, Structural Mapping, Ordered Ranks, Remove Redundant Ranks, Approximate Mapping, và LoD Low-rank ordered Decomposition]

Hình 1: Cấu trúc của MAESTRO. Để có được xấp xỉ hạng thấp, ánh xạ tuyến tính đã cho được phân tách và huấn luyện với LOD để có được biểu diễn có thứ tự có thể được cắt tỉa hiệu quả.

thực thi khả năng nhận dạng của biểu diễn đã học hoặc lớp cuối cùng của bộ trích xuất đặc trưng (Horváth et al., 2021) để học một tập hợp các đặc trưng có thứ tự cho học chuyển giao. Trái ngược với công trình trước, LOD của MAESTRO mở rộng không tầm thường biểu diễn có thứ tự theo ba cách có ý nghĩa. Đầu tiên, đây là công trình đầu tiên được áp dụng cho mạng được phân tách, được thu hẹp dần khi các hạng dư thừa hội tụ về không. Điều này được đạt được thông qua hình phạt lasso nhóm phân cấp, như được mô tả trong Phần 3.3. Thứ hai, LOD cho phép các hạng không đồng nhất (không đồng nhất) mỗi lớp, tạo ra một không gian hoạt động phong phú hơn nhiều. Cuối cùng, thông qua LOD chúng ta có thể tận dụng biểu diễn có thứ tự của các hạng tại thời gian suy luận để nén thêm mô hình, cho phép sự đánh đổi độ chính xác-độ trễ nhẹ nhàng để triển khai trên các thiết bị bị hạn chế hơn, mà không cần phải huấn luyện lại.

3. MAESTRO

Trong công trình này, chúng tôi tập trung vào các mô hình hạng thấp như một kỹ thuật để giảm độ phức tạp tính toán và yêu cầu bộ nhớ của mô hình mạng nơ-ron. Thách thức chính mà chúng tôi đối mặt là việc lựa chọn hạng tối ưu hoặc sự đánh đổi giữa hiệu quả và hạng cho lớp đã cho. Do đó, chúng tôi thiết kế một kỹ thuật huấn luyện dựa trên tầm quan trọng, MAESTRO, không chỉ học ánh xạ giữa các đặc trưng và phản hồi mà còn học phân tích của mạng được huấn luyện. Điều này được đạt được bằng cách phân tích tất cả các lớp trong mạng.

3.1. Công thức

Xấp xỉ hạng thấp. Cảm hứng của chúng tôi đến từ xấp xỉ ma trận hạng thấp của ma trận A∈Rm×n. Để đơn giản, chúng tôi giả sử rằng A có hạng tối đa r = min{m, n} với k≤r giá trị đơn lẻ khác không phân biệt ˜σ1>˜σ2> ... > ˜σk>0, với các vectơ đơn lẻ trái và phải tương ứng ˜u1,˜u2, ..., ˜uk∈Rm và ˜v1,˜v2, ..., ˜vk∈Rn. Đối với ma trận như vậy, chúng ta có thể viết lại xấp xỉ hạng l tốt nhất của nó như bài toán tối thiểu hóa sau

minU∈Rm×l,V∈Rn×l ∥∑l i=1 uiv⊤i - A∥²F (1)

trong đó ci biểu thị cột thứ i của ma trận C và ∥·∥F biểu thị chuẩn Frobenius. Chúng tôi lưu ý rằng Bài toán (1) là không lồi và không mịn. Tuy nhiên, (Ye & Du, 2021) đã chỉ ra rằng thuật toán gradient descent được khởi tạo ngẫu nhiên giải quyết bài toán này trong thời gian đa thức. Trong công trình này, chúng tôi xem xét xấp xỉ hạng tốt nhất trên tất cả các hạng. Định lý Eckart–Young–Mirsky dẫn đến mục tiêu sau

minU∈Rm×r,V∈Rn×r 1/r ∑r b=1 ∥U:bV⊤:b - A∥²F, (2)

trong đó C:b biểu thị b cột đầu tiên của ma trận C. Mục tiêu này, cho đến tỷ lệ, khôi phục chính xác SVD của A, và đối với trường hợp các giá trị đơn lẻ khác không phân biệt, nghiệm là duy nhất cho đến tỷ lệ (Horváth et al., 2021). Tuy nhiên, công thức này không tính đến phân phối dữ liệu, tức là nó không thể điều chỉnh phân tích để nắm bắt các cấu trúc cụ thể xuất hiện trong bộ dữ liệu.

LoD cho xấp xỉ hạng thấp phụ thuộc dữ liệu. Do đó, bước tiếp theo trong cấu trúc của chúng tôi là mở rộng công thức bài toán này với dữ liệu có thể cải thiện thêm nén, tái tạo và khái quát hóa và kết hợp kiến thức miền. Chúng tôi giả sử rằng dữ liệu đến từ phân phối x∼X tập trung quanh không, tức là Ex∼X[x] = 0², và phản hồi được cho bởi y=Ax. Trong trường hợp đặc biệt này, chúng ta có thể viết mất mát huấn luyện như

minU∈Rm×r,V∈Rn×r Ex,y∼X[∑r b=1 1/r ∥U:bV⊤:bx - y∥²]. (3)

Điều quan trọng cần lưu ý là công thức bài toán được giới thiệu (3) cho mạng nơ-ron với một lớp ẩn duy nhất và không có kích hoạt có thể được giải quyết bằng các thuật toán ngẫu nhiên

²Chúng tôi đưa ra giả định này để đơn giản. Nó có thể được khắc phục đơn giản bằng cách thêm một số hạng bias vào mô hình.

--- TRANG 4 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

bằng cách lấy mẫu từ phân phối dữ liệu X (lấy mẫu con) và phân phối hạng D. Khi chúng tôi áp dụng LOD cho DNN, trái ngược với bất kỳ công trình trước đó nào (Horváth et al., 2021; Rippel et al., 2014; Diao et al., 2021), công thức của chúng tôi là công thức đầu tiên áp dụng dropout có thứ tự dựa trên tầm quan trọng cho biểu diễn được phân tách của mỗi lớp, do đó bảo toàn chiều. Quan trọng hơn, chúng tôi phân tách mỗi lớp một cách độc lập và cho phép tìm hạng tối ưu cho mỗi lớp theo cách được thông báo dữ liệu. Chúng tôi thảo luận chi tiết trong đoạn tiếp theo.

Xấp xỉ hạng thấp DNN. Đối với Mạng Nơ-ron Sâu (DNN), chúng tôi tìm cách khám phá các hạng tối ưu cho một tập hợp d ánh xạ tuyến tính W1∈Rm1×n1, ..., Wd∈Rmd×nd, trong đó Wi là các tham số mô hình và d là độ sâu mô hình, ví dụ: trọng số tương ứng với các lớp tuyến tính³, bằng cách phân tách chúng như Wi=UiVi⊤. Chúng tôi thảo luận cách chúng được chọn trong phần tiếp theo. Để phân tách mạng, chúng tôi nhằm mục đích tối thiểu hóa mục tiêu sau

Ex,y∼X[1/∑d i=1 ri ∑d i=1 ∑ri b=1 l(h(U1V1⊤, ..., Ui:bVi:b⊤, ..., UdVd⊤, Wo, x), y)], (4)

trong đó ri = min{mi, ni}, l là hàm mất mát, h là DNN, và Wo là các trọng số khác mà chúng tôi không phân tách. Chúng tôi lưu ý rằng công thức của chúng tôi nhằm mục đích phân tách mỗi lớp, trong khi các phân tách qua các lớp không tương tác trực tiếp. Động lực cho phương pháp này là khám phá các cấu trúc hạng thấp trong mỗi lớp không bị ảnh hưởng bởi các sự không chính xác từ các lớp khác do nhiều xấp xỉ hạng thấp.

3.2. Phân tích Lớp

Các phần sau thảo luận về cách chúng tôi triển khai phân tích mô hình cho các kiến trúc khác nhau.

Lớp FC. Một mạng nơ-ron fully connected (FC) 2 lớp có thể được biểu diễn như f(x) = σ(σ(xW1)W2), trong đó Ws là các ma trận trọng số của mỗi lớp FC, và σ(·) là bất kỳ hàm kích hoạt tùy ý nào, ví dụ: ReLU. Ma trận trọng số W có thể được phân tích như UV⊤.

Lớp CNN. Đối với lớp tích chập với chiều, W∈Rm×n×k×k trong đó m và n là số kênh đầu vào và đầu ra, và k là kích thước của các bộ lọc tích chập. Thay vì phân tích trực tiếp trọng số 4D của lớp tích chập, chúng tôi phân tích ma trận 2D được triển khai. Triển khai tensor 4D W dẫn đến ma trận 2D với hình dạng Wunrolled ∈Rmk2×n, trong đó mỗi cột đại diện cho trọng số của bộ lọc tích chập được vector hóa. Phân tích có thể

³Chúng tôi có thể áp dụng phân tách của chúng tôi trên các loại lớp khác nhau, chẳng hạn như Linear, Convolutional và Transformers như được hiển thị trong Phần 3.2.

Thuật toán 1: MAESTRO (Quá trình Huấn luyện)
Đầu vào: epochs E, dataset D, model h được tham số hóa bởi U1∈Rm1×r1, V1∈Rn1×r1, ..., Ud∈Rmd×rd, Vd∈Rnd×rd, Wo, và siêu tham số λgl, εps
1 for t←0 to E−1 do //Epochs
2   for (x, y)∈D do //Lặp qua dataset
3     Sample (i, b)∼{(i, b)}ri b=1 d i=1; //LoD
4     L=l(h(U1V1⊤, ..., Ui:bVi:b⊤, ..., UdVd⊤, Wo, x), y) +λgl∑d i=1 ∑ri b=1 ∥Uib:∥+∥Vib:∥ //Loss
5     L.backward() // Update weights
6   end
7   for i←1 to d do
8     for b←1 to ri do
9       //rank importance thresholding
10      if ∥Vib:∥+∥Uib:∥≤εps then
11        ri=b−1 //progressive shrinking
12        break
13      end
14    end
15  end
16 end

sau đó được thực hiện trên ma trận 2D được triển khai; xem (Wang et al., 2021) để biết chi tiết.

Transformers. Một lớp Transformer bao gồm một stack các bộ mã hóa và giải mã (Vaswani et al., 2017). Bộ mã hóa và giải mã chứa ba khối xây dựng chính: lớp multi-head attention, mạng feed-forward theo vị trí (FFN), và mã hóa vị trí. Chúng tôi phân tích tất cả các ma trận trọng số có thể huấn luyện trong multi-head attention (MHA) và các lớp FFN. Phân tích lớp FFN có thể áp dụng trực tiếp chiến lược từ phân tích FC. Một lớp attention p-head học p cơ chế attention trên key, value, và query (K, V, Q) của mỗi token đầu vào:

MHA(Q, K, V) = Concat(head1, ..., headp)WO.

Mỗi head thực hiện tính toán:
headi = Attention(QW(i)Q, KW(i)K, VW(i)V)
= softmax(QW(i)QW(i)⊤KK⊤/√(d/p))VW(i)V.

trong đó d là chiều ẩn. Các trọng số có thể huấn luyện W(i)Q, W(i)K, W(i)V, i∈{1,2,...,p} có thể được phân tích bằng cách đơn giản phân tách tất cả các trọng số có thể học W· trong lớp attention và thu được U·V⊤· (Vaswani et al., 2017).

3.3. Kỹ thuật Huấn luyện

Sau khi định nghĩa phân tách của các lớp điển hình được tìm thấy trong DNN, chúng tôi chuyển sang công thức hóa quy trình huấn luyện của phương pháp chúng tôi, được mô tả chính thức trong Thuật toán 1. Huấn luyện mô hình bao gồm một quá trình lặp đi lặp lại của việc truyền tiến trên mô hình bằng cách lấy mẫu một hạng bi cho mỗi lớp được phân tách i lên đến hạng tối đa ri (dòng 3). Chúng tôi tính toán mất mát, tích hợp một thành phần lasso nhóm phân cấp bổ sung (dòng 4) và truyền ngược trên mô hình được phân tách đã lấy mẫu (dòng 5). Vào cuối mỗi epoch, chúng tôi thu hẹp dần mạng bằng cách cập nhật hạng tối đa ri, dựa trên ngưỡng tầm quan trọng εps (dòng 11). Chúng tôi cung cấp thêm chi tiết về mỗi thành phần bên dưới.

Huấn luyện hiệu quả thông qua lấy mẫu. Trong Phần 4, chúng tôi chỉ ra rằng đối với trường hợp tuyến tính (3), nghiệm tối ưu tương ứng với PCA trên bộ dữ liệu được biến đổi tuyến tính. Điều này có nghĩa là nghiệm thu được chứa các hướng trực giao. Tính chất này có lợi vì nó ngụ ý trực tiếp rằng khi chúng ta sử dụng tối ưu hóa dựa trên gradient, không chỉ gradient bằng không tại điểm tối ưu, mà gradient đối với mỗi số hạng trong Phương trình (3) cũng bằng không. Tính chất tương tự được ngụ ý trực tiếp bởi over-parametrization (Ma et al., 2018) hoặc điều kiện tăng trưởng mạnh (Schmidt & Roux, 2013). Kết quả là, điều này cho phép chúng ta chỉ lấy mẫu một số hạng tại một thời điểm và thu được nghiệm có chất lượng tương tự. Khi xem xét (4) như một mở rộng của (3), không rõ ràng liệu tính chất này vẫn còn đúng, điều này cũng ngụ ý rằng tập hợp các điểm dừng của (3) là tập con của các điểm dừng của mục tiêu ban đầu mà không có phân tách. Tuy nhiên, trong các thí nghiệm, chúng tôi quan sát thấy rằng lấy mẫu là đủ để hội tụ đến nghiệm chất lượng tốt. Nếu điều này chỉ đúng gần đúng, người ta có thể tận dụng fine-tuning để khôi phục mất mát hiệu suất.

Trích xuất hạng hiệu quả thông qua hierarchical group-lasso. Theo định nghĩa, (3) dẫn đến một tập hợp các hạng có thứ tự cho mỗi lớp. Cấu trúc có thứ tự này cho phép trích xuất và lựa chọn hạng hiệu quả. Để loại bỏ hiệu quả các hạng không quan trọng trong khi giữ lại những hạng quan trọng, do đó dẫn đến mô hình hiệu quả hơn, chúng tôi xem xét Hierarchical Group Lasso (HGL) (Lim & Hastie, 2015) ở dạng

λgl∑d i=1 ∑ri b=1 (∥Uib:∥+∥Vib:∥), (5)

trong đó Cb: biểu thị ma trận chứa tất cả các cột của C ngoại trừ b−1 cột đầu tiên.

Thu hẹp tiến bộ. HGL khuyến khích các hạng không quan trọng trở thành không và có thể được loại bỏ hiệu quả khỏi mô hình. Để tính đến điều này, đối với mỗi lớp chúng tôi loại bỏ Vib: và Uib: (tức là đặt ri=b−1) nếu ∥Vib:∥+∥Uib:∥≤εps, trong đó εps là ngưỡng được chọn trước – và là siêu tham số của phương pháp chúng tôi.

Tối ưu hóa siêu tham số. Chúng tôi cung cấp thuật toán để tìm giá trị tối ưu cho siêu tham số λgl trong Thuật toán 2. Từ đánh giá của Bảng 11-15, chiến lược này thường yêu cầu tối đa 2-3 lần nỗ lực tính toán (về mặt FLOP) so với một vòng lặp huấn luyện duy nhất với λgl được chọn tối ưu. Điều này dễ dàng hơn đáng kể so với việc điều chỉnh hạng tối đa trên mỗi lớp và các bước pre-training, nơi mô hình hạng đầy đủ đang được pre-train, như trường hợp trong các baseline hạng thấp khác. Tương đương, giá trị của εps biểu thị điểm zero hiệu quả trong thuật toán của chúng tôi, giá trị điển hình là 1e−7 trong các thí nghiệm của chúng tôi.

--- TRANG 5 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

Thuật toán 2: MAESTRO (Tối ưu hóa Siêu tham số)
Đầu vào: các ràng buộc (ví dụ: độ chính xác tối thiểu yêu cầu, số tham số tối đa), epochs E, dataset D, model h, tần suất đánh giá Eeval_every, εps, giới hạn cho HPO: largeValue, smallValue
1 λgl=largeValue
2 old_model = NULL
3 while λgl>smallValue do
4   model = RandInit(model)
5   for t←0 to E−1 do //Epochs
6     Train(model, dataset, λgl)
7     if t mod Eeval_every == 0 then
8       acc = CalculateAcc(model, dataset)
9       flops, params = MeasureFootprint(model, εps)
10      if acc≥constraints['acc'] and params ≤ constraints['params'] then
11        return model // mô hình thỏa mãn các ràng buộc đã được tìm thấy
12      end
13      if params < few_params then
14        break // mô hình quá thưa thớt
15      end
16    end
17  end
18  flops, params = MeasureFootprint(model, εps)
19  if params > constraints['params'] then
20    /** không có mô hình nào thỏa mãn các ràng buộc, trả về mô hình cuối cùng thỏa mãn các ràng buộc tham số **/
21    return old_model
22  end
23  old_model = Copy(model)
24  λgl=λgl/2
25 end

Khởi tạo. Khởi tạo là một thành phần quan trọng của quy trình huấn luyện (He et al., 2015; Mishkin & Matas, 2015). Để áp dụng các thực hành tốt nhất từ huấn luyện tiêu chuẩn không phân tích, chúng tôi tuân theo phương pháp tương tự với (Khodak et al., 2021; Wang et al., 2021), trong đó trước tiên chúng tôi khởi tạo mô hình không phân tích bằng khởi tạo tiêu chuẩn. Để khởi tạo các lớp được phân tích, chúng tôi sử dụng Phân tích Giá trị Đơn lẻ của khởi tạo không phân tích – ở dạng hạng đầy đủ – để đảm bảo rằng ma trận sản phẩm kết quả giống với phân tách tham số ban đầu. Ngoài ra, SVD là phân tích tối ưu cho trường hợp tuyến tính với dữ liệu đồng nhất. Tuy nhiên, trái ngược với phương pháp baseline thích ứng (Wang et al., 2023) chúng tôi chỉ phân tách một lần, thay vì ở mỗi lần lặp huấn luyện. Do đó, chúng tôi chỉ chạy phân tích một lần và thu hẹp dần các hạng theo cách tập trung vào dữ liệu. Điều này trái ngược với công trình liên quan (Wang et al., 2021; 2023) yêu cầu lựa chọn hạng và lớp thủ công và warm-up hạng đầy đủ để đạt được

hiệu suất mong muốn, đương nhiên với chi phí overhead huấn luyện.

3.4. Huấn luyện Một lần, Triển khai Mọi nơi

Cho đến nay, chúng tôi đã mô tả cách phương pháp của chúng tôi hoạt động để huấn luyện các mô hình hạng thấp, mang lại lợi ích về băng thông tính toán, bộ nhớ, mạng và năng lượng (Wu et al., 2022) trong quá trình huấn luyện. Tại thời điểm triển khai, người ta có thể triển khai trực tiếp mô hình cuối cùng (hạng ri cho mỗi lớp) trên thiết bị, mà chúng tôi thu được từ việc thực hiện quét ngưỡng εps trên phạm vi hiệu quả của tầm quan trọng hạng qua các lớp. Tuy nhiên, trong trường hợp chúng ta muốn chạy trên các thiết bị bị hạn chế hơn, chẳng hạn như hệ thống di động hoặc nhúng (Almeida et al., 2021), phân tích đã học cũng cho chúng ta tính linh hoạt để nén thêm mô hình theo cách đơn giản, hiệu quả đánh đổi độ chính xác cho dấu chân mô hình nhỏ hơn. Lấy cảm hứng từ (Yu & Huang, 2019a), chúng tôi đề xuất sử dụng tìm kiếm tham lam. Chúng tôi bắt đầu với mô hình hiện tại và so sánh hiệu suất mô hình qua các mô hình hạng thấp khác nhau, mỗi mô hình được tạo bằng cách loại bỏ một tỷ lệ phần trăm nhất định của các hạng từ mỗi lớp. Sau đó chúng tôi loại bỏ các hạng gây ra sự giảm hiệu suất ít nhất. Quá trình này được lặp lại cho đến khi chúng ta đạt được kích thước mong muốn hoặc ràng buộc độ chính xác. Để làm cho phương pháp này hiệu quả, chúng tôi ước tính mất mát bằng một mini-batch duy nhất với kích thước batch lớn (ví dụ: 2048). Điều này cũng tránh các vấn đề với các lớp BatchNorm; xem (Yu & Huang, 2019a) để biết chi tiết.

Tóm lại, MAESTRO bao gồm một kỹ thuật để xấp xỉ hạng thấp có thể huấn luyện trong thời gian huấn luyện mà nén dần mô hình, phản ánh phân phối dữ liệu, và một phương pháp cho phép sự đánh đổi nhẹ nhàng giữa độ chính xác và độ trễ để triển khai nhúng, bằng cách chọn những phần quan trọng nhất của mạng. Chúng tôi xác thực những tuyên bố này trong Phần 5.2 và 5.5, tương ứng.

4. Đảm bảo Lý thuyết

Trong phần này, chúng tôi điều tra thêm các tính chất lý thuyết của MAESTRO cho các ánh xạ tuyến tính, tức là thiết lập của công thức bài toán (3).

Định lý 4.1 (Không chính thức). Cho A=˜U˜Σ˜V⊤ là phân tích SVD của A. Khi đó, bài toán tối thiểu hóa (3) tương đương với PCA được áp dụng cho bộ dữ liệu được biến đổi x→˜Σ˜V⊤x, x∼X được chiếu trên không gian cột của ˜U.

Phát biểu chính thức có thể được tìm thấy trong Phụ lục C. Định lý 4.1 chỉ ra rằng MAESTRO có thể thích ứng với phân phối dữ liệu bằng cách hoạt động trực tiếp trên dữ liệu x∼X và cũng với ánh xạ mục tiêu bằng cách chiếu dữ liệu lên các vectơ đơn lẻ phải được chia tỷ lệ bởi các giá trị đơn lẻ. Đặc biệt, chúng tôi chỉ ra rằng trong trường hợp đặc biệt, khi X là phân phối đồng nhất trên quả cầu đơn vị, (3), tức là MAESTRO, khôi phục chính xác SVD cắt ngắn của A, điều này phù hợp với các kết quả trước đó (Horváth et al., 2021). Trong trường hợp A là đồng nhất, rõ ràng là MAESTRO tương đương với PCA. Chúng ta có thể thấy rằng MAESTRO có thể trích xuất hiệu quả các nghiệm hạng thấp bằng cách lọc ra các hướng tương ứng với không gian null của ánh xạ mục tiêu A và các hướng không có dữ liệu. Chúng tôi cũng xác thực bằng số cả hai trường hợp đặc biệt–PCA và SVD, bằng cách tối thiểu hóa (3) bằng stochastic gradient descent (SGD) với D là phân phối đồng nhất. Các thí nghiệm này được cung cấp trong Hình 2a và 2b. Chúng tôi cung cấp bằng chứng thêm về tính thích ứng của MAESTRO trong Phụ lục E.1 và E.2.

Chúng tôi đã chỉ ra rằng MAESTRO có thể khôi phục SVD trong trường hợp đặc biệt của mô hình tuyến tính và phân phối dữ liệu đồng nhất trên quả cầu đơn vị. Chúng tôi lưu ý rằng trong trường hợp này, SVD là tối ưu, và chúng ta không thể có được phân tích tốt hơn. Do đó, mong muốn rằng MAESTRO tương đương với SVD trong tình huống này. Tổng quát hơn, chúng tôi lập luận rằng phân tích MAESTRO nên được ưa thích hơn SVD vì những lý do sau:

• Công thức MAESTRO được xây dựng trực tiếp vào huấn luyện và được điều chỉnh để có được phân tích hạng thấp tốt nhất, trong khi SVD dựa vào giả định tuyến tính.

• SVD không tính đến dữ liệu, và ngay cả trong trường hợp NN tuyến tính, các vectơ đơn lẻ đã học có thể thể hiện thứ tự sai. Chúng tôi chứng minh vấn đề này bằng một ví dụ đơn giản trong đó chúng tôi lấy ma trận A với hạng 3. Chúng tôi xây dựng bộ dữ liệu X theo cách mà vectơ đơn lẻ thứ ba là quan trọng nhất, vectơ thứ hai là thứ hai, và vectơ đầu tiên là hướng quan trọng thứ ba. Rõ ràng, SVD không nhìn vào dữ liệu. Do đó, nó không thể nắm bắt hiện tượng này. Chúng tôi thể hiện rằng MAESTRO học thứ tự đúng; xem Hình 6 của Phụ lục.

• Việc phân tích trước các mô hình cho phép chúng ta áp dụng hình phạt lasso nhóm phân cấp (Yuan & Lin, 2006) cho các trọng số được phân tách để điều chỉnh trực tiếp hạng của các lớp khác nhau.

• SVD tốn kém về mặt tính toán và chỉ có thể chạy hiếm khi, trong khi MAESTRO được xây dựng trực tiếp vào huấn luyện và do đó không yêu cầu tính toán bổ sung. Ngoài ra, MAESTRO hỗ trợ lấy mẫu hạng để huấn luyện có thể được thực hiện hiệu quả về mặt tính toán.

5. Thí nghiệm

Chúng tôi bắt đầu phần này bằng cách mô tả thiết lập thí nghiệm của chúng tôi, bao gồm các mô hình, bộ dữ liệu và baseline mà chúng tôi so sánh MAESTRO. Sau đó chúng tôi so sánh MAESTRO với các baseline về độ chính xác và các phép toán Multiply-Accumulate (MAC) huấn luyện và thảo luận kết quả. Tiếp theo, chúng tôi phân tích hành vi của hệ thống chúng tôi một cách sâu sắc và cung cấp thông tin chi tiết bổ sung về hiệu suất của kỹ thuật chúng tôi, cùng với nghiên cứu ablation và phân tích độ nhạy đối với các siêu tham số cụ thể. Cuối cùng, chúng tôi thể hiện hiệu suất của các mô hình khi triển khai và cách chúng ta có thể tạo ra mô hình có dấu chân nhỏ hơn với một số đánh đổi độ chính xác, mà không cần fine-tune.

--- TRANG 6 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

[Hình 2a và 2b cho thấy các biểu đồ xác thực rằng MAESTRO khôi phục SVD và PCA]

Hình 2: Thể hiện thực nghiệm các tính chất lý thuyết của công thức MAESTRO.

[Hình 3a và 3b cho thấy so sánh hiệu suất giữa Maestro và các baseline khác nhau]

Hình 3: Maestro so với baseline trên CIFAR10.

[Bảng 2 hiển thị thông tin về datasets và models]

Bảng 2: Bộ dữ liệu và mô hình để đánh giá. Dấu chân mô tả các mô hình vanilla.

[Bảng 3 hiển thị kết quả so sánh Maestro với baseline trên Multi30k]

Bảng 3: Maestro so với baseline trên Multi30k.

5.1. Thiết lập Thí nghiệm

Mô hình & bộ dữ liệu. Các bộ dữ liệu và mô hình được xem xét trong thí nghiệm của chúng tôi trải dài trên bốn bộ dữ liệu, được trình bày ngắn gọn cùng với các mô hình liên quan trong Bảng 2. Chúng tôi đã triển khai giải pháp của mình trong PyTorch (Paszke et al., 2017) (v1.13.0) huấn luyện các mô hình của chúng tôi trên GPU NVidia A100 (40G). Chi tiết cho các nhiệm vụ học và siêu tham số được sử dụng được trình bày trong Phụ lục D.

Baseline. Chúng tôi đã chọn các baseline khác nhau từ tài liệu mà chúng tôi tin là gần nhất với các khía cạnh của hệ thống chúng tôi. Về mặt cắt tỉa, chúng tôi so sánh với các kỹ thuật IMP (Paul et al., 2023) và RareGems (Sreenivasan et al., 2022). Về mặt lượng tử hóa, chúng tôi so sánh với XNOR-Net (Rastegari et al., 2016). Đối với các phương pháp hạng thấp, chúng tôi so sánh với Spectral Initialisation (Khodak et al., 2021), Pufferfish (Wang et al., 2021) và Cuttlefish (Wang et al., 2023).

5.2. So sánh Hiệu suất

Chúng tôi bắt đầu bằng cách so sánh MAESTRO với các baseline được đề cập từ tài liệu trên các bộ dữ liệu và mô hình của Bảng 2⁴. Kết quả được mô tả trong Hình 3 và Bảng 3, trong khi các điểm hiệu suất bổ sung của MAESTRO cho các dấu chân mô hình khác nhau được trình bày trong Phụ lục E.3 và E.4.

So sánh với các phương pháp hạng thấp. Các phương pháp hạng thấp mà chúng tôi so sánh là Pufferfish (Wang et al., 2021) và Cuttlefish (Wang et al., 2023). Các phương pháp này cố gắng giảm thời gian chạy huấn luyện và suy luận trong khi bảo toàn độ chính xác mô hình bằng cách tận dụng các xấp xỉ hạng thấp. Đối với ResNet-18, chúng tôi đạt được 94.19 ±0.07% cho 4.08M tham số và 93.97 ±0.25% cho 2.19M tham số so với 94.17% của Pufferfish tại 3.3M tham số. Đối với VGG-19, chúng tôi đạt được +0.41pp (điểm phần trăm) độ chính xác cao hơn so với Pufferfish và -0.29pp so với Cuttlefish tại 44.8% và 93.2% kích thước, tương ứng. Cuối cùng, so sánh với spectral initialization (Khodak et al., 2021) cho VGG-19, chúng tôi đạt được +5.26pp độ chính xác cao hơn cho 87.5% kích thước tham số. Kết quả chi tiết được hiển thị trong Bảng 16. Lợi ích hiệu suất này cũng áp dụng trong trường hợp Transformers (Bảng 3), trong đó MAESTRO thực hiện tốt hơn 6% về mặt perplexity tại 25% chi phí (MAC) và 51.7%

⁴Các điểm hoạt động mà chúng tôi chọn cho MAESTRO là gần nhất thấp hơn so với baseline tương ứng về mặt dấu chân. Trong trường hợp kết quả không có mặt trong Hình 3, chúng tôi cung cấp giá trị λgp để có thể tham chiếu từ Phụ lục, Bảng 12, 13.

kích thước (tham số) so với Pufferfish. Đáng chú ý là cả Pufferfish và Cuttlefish, theo mặc định, không phân tách tất cả các lớp và có các vòng warm-up full-training, cả hai đều gây ra overhead huấn luyện và tối ưu hóa siêu tham số. Ngược lại, kỹ thuật của chúng tôi chỉ giới thiệu hai siêu tham số, cụ thể là λgl và εps, điều khiển toàn bộ quá trình huấn luyện. Chúng tôi đã mở rộng các thí nghiệm của mình lên mức ImageNet-1k (Bảng 4) và đối với cùng thiết lập phân tách đầy đủ, chúng tôi đạt được độ chính xác hơi cao hơn (+0.51pp) tại 97.8% kích thước của Pufferfish. Đối với phân tách một phần, MAESTRO thực hiện ngang bằng với Pufferfish và Cuttlefish tại chi phí huấn luyện và suy luận thấp hơn.

So sánh với các phương pháp cắt tỉa. Họ baseline tiếp theo liên quan đến LTH (Frankle & Carbin, 2019). Cụ thể, chúng tôi so sánh với IMP (Paul et al., 2023) và chứng kiến rằng MAESTRO có thể đạt được +1.25pp (λgp= 128e−6) và +0.24pp (λgp= 32e−6) độ chính xác cao hơn cho ResNet-18 và VGG-19 tương ứng. Kết quả chi tiết được hiển thị trong Bảng 16 của Phụ lục. Mặc dù chúng tôi không thể mở rộng đến kích thước mà RareGems (Sreenivasan et al., 2022) cho ResNet-18, độ thưa thớt mà họ đạt được là không có cấu trúc, mà hầu hết phần cứng hiện đại không thể tận dụng. Ngược lại, kỹ thuật của chúng tôi thực hiện độ thưa thớt có cấu trúc có thứ tự tương thích với hầu hết các mục tiêu tính toán. Mặt khác, đối với VGG-19, chúng tôi đạt được +6.82pp độ chính xác cao hơn tại 43.6% dấu chân.

--- TRANG 7 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

[Bảng 4 cho thấy so sánh Maestro với baseline trên ImageNet-1k]

[Bảng 5 cho thấy nghiên cứu ablation cho ResNet18 trên CIFAR10]

So sánh với các mô hình được lượng tử hóa. Chúng tôi cũng so sánh với XNOR-Net (Rastegari et al., 2016), mà nhị phân hóa mạng để đạt được suy luận hiệu quả. Huấn luyện tiếp tục diễn ra ở độ chính xác đầy đủ, và hiệu suất suy luận phụ thuộc vào việc triển khai hoạt động của phần cứng mục tiêu. Tuy nhiên, giả sử tỷ lệ nén 3.125%, đối với cùng kích thước mô hình, chúng tôi đạt được +1.08pp (λgp= 512 e−6) và +2.18pp (λgp= 256 e−6) độ chính xác cao hơn trên ResNet-18 và VGG-19.

5.3. Hành vi Huấn luyện của MAESTRO

Sau khi chỉ ra hiệu suất tương đối của khung làm việc chúng tôi so với các baseline được chọn, giờ đây chúng tôi chuyển sang điều tra cách phương pháp của chúng tôi hoạt động đối với sự hội tụ và các xấp xỉ hạng thấp của nó.

Hội tụ mô hình và hạng. Trong Hình 4, chúng tôi trình bày động lực huấn luyện cho MAESTRO. Hình 4a minh họa sự tiến triển của tổng hạng trong suốt các bước huấn luyện. Chúng tôi quan sát thấy rằng các hạng được cắt tỉa dần dần. Điều này phù hợp với các quan sát được thực hiện trong quá trình huấn luyện Pufferfish (Wang et al., 2021), nơi các tác giả đề xuất huấn luyện warm-start với độ chính xác đầy đủ để nâng cao hiệu suất mô hình cuối cùng. Trong tình huống của chúng tôi, chúng tôi không cần tích hợp heuristic này vì MAESTRO tự động cắt tỉa hạng. Hình 4b tiết lộ các hạng qua các lớp sau huấn luyện. Chúng tôi nhận thấy một hiện tượng thú vị: các hạng được lồng nhau cho λgl tăng dần. Điều này có thể ngụ ý ngoài thứ tự tự nhiên của các hạng trong mỗi lớp, một thứ tự toàn cục. Chúng tôi kiểm tra ngắn gọn sự xuất hiện hấp dẫn này trong phần tiếp theo, và chúng tôi dự định điều tra nó kỹ lưỡng hơn trong công việc tương lai, vì chúng tôi tin rằng điều này có thể đóng góp vào quá trình lựa chọn và lấy mẫu hạng tốt hơn. Cuối cùng, Hình 4c mô tả sự tiến triển của mất mát huấn luyện. Chúng tôi thấy rằng giả thuyết của chúng tôi rằng lấy mẫu không ảnh hưởng xấu đến huấn luyện cũng được hỗ trợ thực nghiệm.

--- TRANG 8 ---
Maestro: Khám phá các Cấu trúc Hạng thấp thông qua Phân tích có thể Huấn luyện

[Hình 4 cho thấy động lực huấn luyện của MAESTRO cho ResNet18 trên CIFAR10]

5.4. Nghiên cứu Ablation

Trong phần này, chúng tôi kiểm tra tác động của mỗi thành phần đối với hiệu suất của MAESTRO. Cụ thể, chúng tôi chạy các biến thể của phương pháp chúng tôi i) không có regularization hierarchical group lasso (HGL), ii) không có progressive shrinking (PS). Ngoài ra, chúng tôi tích hợp iii) một lượt đầy đủ hạng thấp bổ sung (b=ri) vào huấn luyện ở mỗi bước để đánh giá liệu lấy mẫu bổ sung có có lợi không. Kết quả được hiển thị trong Bảng 5. Như dự kiến, phát hiện của chúng tôi xác nhận rằng cả việc bao gồm hierarchical group lasso với λgl được điều chỉnh và progressive shrinking đều không làm tổn hại hiệu suất cuối cùng, nhưng chúng làm tăng đáng kể hiệu quả của MAESTRO. Hơn nữa, lấy mẫu nhiều hạng hơn ở mỗi bước huấn luyện không cải thiện hiệu suất cuối cùng, và thực tế, nó cản trở hiệu quả huấn luyện, làm cho nó tốn kém về mặt tính toán gấp khoảng hai lần.

5.5. Đánh đổi Độ chính xác-Độ trễ tại Thời gian Huấn luyện và Triển khai

Trong Hình 5, chúng tôi minh họa các phương pháp khác nhau để cân bằng độ trễ (được đại diện thông qua các phép toán MAC) và độ chính xác trong huấn luyện và triển khai mô hình. Hình 5a chứng minh cách MAESTRO (λgl= 0) có thể được cắt tỉa hiệu quả để triển khai bằng phương pháp tìm kiếm tham lam được thảo luận trong Phần 3.4. Chúng tôi đối chiếu điều này với việc cắt tỉa tham lam của mô hình không phân tích đã được phân tích bằng SVD. Chúng tôi tiết lộ rằng baseline đơn giản này không đạt được phân tách đã học của MAESTRO và dẫn đến sự giảm hiệu suất đáng kể. Tiếp theo, Hình 5b mô tả độ chính xác cuối cùng và số lượng tham số mô hình cho các hình phạt hierarchical group lasso khác nhau. Điều này dẫn đến sự cân bằng độ trễ-độ chính xác tối ưu cho cả huấn luyện và suy luận. Tuy nhiên, điều quan trọng cần chỉ ra rằng mỗi mô hình được huấn luyện riêng lẻ, trong khi cắt tỉa tham lam chỉ cần một chu kỳ huấn luyện duy nhất. Cuối cùng, chúng tôi đi sâu vào quan sát về các hạng lồng nhau qua λgl tăng dần. Hình 5c hiển thị hiệu suất của MAESTRO (λgl= 0) qua các hạng khác nhau được chọn bởi các mô hình nhỏ hơn MAESTRO (λgl>0). Thú vị, chúng tôi quan sát thấy rằng MAESTRO (λgl= 0) thực hiện rất tốt—ví dụ, chúng ta có thể giảm các phép toán của nó một nửa (và tham số 10×) và vẫn duy trì độ chính xác 87.7% mà không cần fine-tuning, chỉ bằng cách tái sử dụng cấu trúc hạng từ các lần chạy độc lập. Như đã đề cập trước đó, chúng tôi dự định khám phá điều này thêm trong tương lai.

[Hình 5 cho thấy đánh đổi độ chính xác-độ trễ của MAESTRO dưới các cài đặt khác nhau cho VGG19 trên CIFAR10]

6. Kết luận và Công việc Tương lai

Trong công trình này, chúng tôi đã trình bày MAESTRO, một phương pháp để xấp xỉ hạng thấp có thể huấn luyện của DNN tận dụng progressive shrinking bằng cách áp dụng một biến thể tổng quát của Ordered Dropout cho các trọng số được phân tích. Chúng tôi đã chỉ ra các đảm bảo lý thuyết của công trình chúng tôi trong trường hợp các mô hình tuyến tính và chứng minh thực nghiệm hiệu suất của nó qua các loại mô hình, bộ dữ liệu và phương thức khác nhau. Đánh giá của chúng tôi đã chứng minh rằng MAESTRO vượt trội hơn các phương pháp nén cạnh tranh với chi phí thấp hơn. Trong tương lai, chúng tôi dự định mở rộng kỹ thuật của mình để bao gồm các kỹ thuật lấy mẫu tiên tiến hơn và áp dụng nó cho các tình huống học phân tán khác nhau, chẳng hạn như Học Liên bang, nơi dữ liệu tự nhiên không độc lập hoặc phân phối đồng nhất (non-IID).

Tuyên bố Tác động

Mục tiêu của công trình chúng tôi là làm cho việc huấn luyện và triển khai DNN hiệu quả hơn, ảnh hưởng đến tổng tính toán, bộ nhớ và băng thông của hệ thống, cũng như năng lượng chúng yêu cầu để chạy các nhiệm vụ tương ứng. Huấn luyện mô hình DNN yêu cầu lượng năng lượng đáng kể, dù trong trung tâm dữ liệu hay ở biên (Wu et al., 2022; Patterson et al., 2022). Tuy nhiên, các kỹ thuật như vậy không nên được sử dụng thay thế cho việc làm cho các trung tâm dữ liệu ít xanh hơn, mà như một biện pháp bổ sung để giảm thêm dấu chân carbon của Học Sâu. Ngoài ra, vì kỹ thuật của chúng tôi liên quan đến phương pháp nhận thức huấn luyện để lựa chọn dần các hạng, nó phụ thuộc vào chất lượng dữ liệu được sử dụng trong huấn luyện. Triển khai mô hình trong thực tế cho các nhiệm vụ downstream khác nhau có thể dẫn đến hành vi khác với kết quả dự định. Do đó, nó nên được kiểm tra kỹ lưỡng trước khi triển khai để đảm bảo tuân thủ các Mục tiêu Mức độ Dịch vụ (SLO) yêu cầu, đặc biệt trong các trường hợp sử dụng quan trọng về hiệu suất, chẳng hạn như xe tự lái hoặc điều hướng UAV.

--- TRANG 9 ---

[Tiếp tục với phần References và Appendix...]
