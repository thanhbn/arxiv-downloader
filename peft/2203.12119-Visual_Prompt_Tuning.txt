# 2203.12119.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2203.12119.pdf
# File size: 3619567 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Visual Prompt Tuning
Menglin Jia∗1,2, Luming Tang∗1
Bor-Chun Chen2, Claire Cardie1, Serge Belongie3
Bharath Hariharan1, and Ser-Nam Lim2
1Cornell University2Meta AI3University of Copenhagen
Abstract. The current modus operandi in adapting pre-trained mod-
els involves updating all the backbone parameters, i.e., full fine-tuning.
This paper introduces Visual Prompt Tuning (VPT) as an efficient and
effective alternative to full fine-tuning for large-scale Transformer models
in vision. Taking inspiration from recent advances in efficiently tuning
large language models, VPT introduces only a small amount (less than
1% of model parameters) of trainable parameters in the input space while
keeping the model backbone frozen. Via extensive experiments on a wide
variety of downstream recognition tasks, we show that VPT achieves sig-
nificant performance gains compared to other parameter efficient tuning
protocols. Most importantly, VPT even outperforms full fine-tuning in
many cases across model capacities and training data scales, while re-
ducing per-task storage cost. Code is available at github.com/kmnp/vpt .
1 Introduction
For a variety of recognition applications, the most accurate results are now ob-
tained by adapting large foundation models pre-trained on massive curated or
raw data, a finding that mirrors developments in natural language processing
(NLP) [6].1At first glance,this is a success story: one can make rapid progress
on multiple recognition problems simply by leveraging the latest and greatest
foundation model. In practice, however, adapting these large models to down-
stream tasks presents its own challenges. The most obvious (and often the most
effective) adaptation strategy is full fine-tuning of the pre-trained model on the
task at hand, end-to-end. However, this strategy requires one to store and de-
ploy a separate copy of the backbone parameters for every single task. This is an
expensive and often infeasible proposition, especially for modern Transformer -
based architectures, which are significantly larger than their convolutional neu-
ral networks (ConvNet) counterparts, e.g., ViT-Huge [19] (632M parameters)
vs. ResNet-50 [31] (25M parameters). We therefore ask, what is the best way
to adapt large pre-trained Transformers to downstream tasks in terms
of effectiveness and efficiency ?
∗Equal contribution.
1As pointed out in [6], all state-of-the-art models in contemporary NLP are now
powered by a few Transformer-based models ( e.g., BERT [17], T5 [66], BART [46],
GPT-3 [7]) This also applies to vision-language field recently, i.e., CLIP [65].arXiv:2203.12119v2  [cs.CV]  20 Jul 2022

--- PAGE 2 ---
2 M. Jia et al.
Backbone
(a)Existingtuningprotocols(c)Resultsonvisualclassiﬁcationtasks(b)Visual-PromptTuning (VPT)
Head
Head
Backbone
Head-oriented:
Backbone-oriented:Prompt 
TunedFrozen
LinearPartialMLP
SidetuneAdapterBias
Full
Ours
Fig. 1. Visual-Prompt Tuning (VPT) vs. other transfer learning methods. (a) Current
transfer learning protocols are grouped based on the tuning scope: Full fine-tuning,
Head-oriented, and Backbone-oriented approaches. (b) VPT instead adds extra pa-
rameters in the input space. (c) Performance of different methods on a wide range
of downstream classification tasks adapting a pre-trained ViT-B backbone, with mean
and standard deviation annotated. VPT outperforms Full fine-tuning 20 out of 24 cases
while using less than 1% of all model parameters
One straightforward approach is to turn to other strategies that we have per-
fected for adapting ConvNets to new tasks, as in Fig. 1(a). A popular approach is
to fine-tune only a subset of the parameters, such as the classifier head [56,36,11]
or the bias terms [8]. Prior research has also looked at adding additional resid-
ual blocks (or adapters ) to the backbone [68,87]. One could implement similar
strategies for Transformers. However, in general these strategies under-perform
full fine-tuning in accuracy.
We explore a different route in this paper. Instead of altering or fine-tuning
the pre-trained Transformer itself, we modify the input to the Transformer.
Drawing inspiration from the recent advances on Prompting in NLP [50,48,45,51],
we propose a new simple and efficient method to adapt transformer models for
downstream vision tasks (Fig. 1(b)), namely Visual-Prompt Tuning (VPT).
Our method only introduces a small amount of task-specific learnable parameters
into the input space while freezing the entire pre-trained Transformer backbone
during downstream training. In practice, these additional parameters are sim-
ply prepended into the input sequence of each Transformer layer and learned
together with a linear head during fine-tuning.
On 24 downstream recognition tasks spanning different domains using a pre-
trained ViT backbone, VPT beats all other transfer learning baselines, even
surpassing full fine-tuning in 20 cases, while maintaining the advantage of stor-
ing remarkably fewer parameters (less than 1% of backbone parameters) for each
individual task (Fig. 1(c)). This result demonstrates the distinctive strength of
visual prompting: whereas in NLP, prompt tuning is only able to match full
fine-tuning performance under certain circumstances [45]. VPT is especially ef-
fective in the low-data regime, and maintains its advantage across data scales.
Finally, VPT is competitive for a range of Transformer scales and designs (ViT-
Base/Large/Huge, Swin). Put together, our results suggest that VPT is one of
the most effective ways of adapting ever-growing vision backbones.

--- PAGE 3 ---
Visual Prompt Tuning 3
2 Related Work
Transformer models [73] have gained huge success in NLP [17,66,7]. The tri-
umph of the Transformer architecture also extends to various computer vision
tasks, including image classification [19,52], object detection [9,49], semantic and
panoptic segmentation [71,89,78], video understanding [25,79,21] and few-shot
learning [18], surpassing previous state-of-the-art approaches. Transformers are
also being widely used in recent self-supervised pre-training methods [11,30,3].
Given their superior performance and much larger scale compared to ConvNets,
how to efficiently adapt Transformers to different vision tasks remains an im-
portant open problem. Our proposed VPT provides a promising path forward.
Transfer learning has been extensively studied for vision tasks in the context
of ConvNets [92] and many techniques have been introduced including side tun-
ing [87], residual adapter [67], bias tuning [8], etc. Relatively little attention has
been paid to vision Transformers adaptation and how well these aforementioned
methods perform on this brand new type of architecture remains unknown. On
the other hand, given the dominance of large-scale pre-trained Transformer-
based Language Models (LM) [17,66,7], many approaches [29,28,35] have been
proposed to efficiently fine-tune LM for different downstream NLP tasks [77,76].
Among them, we focus on the following two representative methods in our ex-
periments for benchmarking purposes: Adapters [64] and BitFit [5].
Adapters [34] insert extra lightweight modules inside each Transformer layer.
One adapter module generally consists of a linear down-projection, followed by a
nonlinear activation function, and a linear up-projection, together with a residual
connection [63,64]. Instead of inserting new modules, [8] proposed to update the
bias term and freeze the rest of backbone parameters when fine-tuning ConvNets.
BitFit [3] applied this technique to Transformers and verified its effectiveness on
LM tuning. Our study demonstrates that VPT, in general, provides improved
performance in adapting Transformer models for vision tasks, relative to the
aforementioned two well-established methods in NLP.
Prompting [50] originally refers to prepending language instruction to the input
text so that a pre-trained LM can “understand” the task. With manually chosen
prompts, GPT-3 shows strong generalization to downstream transfer learning
tasks even in the few-shot or zero-shot settings [7]. In addition to the follow-up
works on how to construct better prompting texts [70,37], recent works propose
to treat the prompts as task-specific continuous vectors and directly optimize
them via gradients during fine-tuning, namely Prompt Tuning [48,45,51]. Com-
pared to full fine-tuning, it achieves comparable performance but with 1000 ×
less parameter storage. Although prompting has also been applied to vision-
language models recently [65,91,39,84,22], prompting is still limited to the input
oftextencoders. Due to the disparity between vision and language modalities, in
this paper we ask: can the same method can be applied successfully to image en-
coders? We are the first work (see related concurrent works [69,80,14,2]) to tackle
this question and investigate the generality and feasibility of visual prompting
viaextensive experiments spanning multiple kinds of recognition tasks across
multiple domains and backbone architectures.

--- PAGE 4 ---
4 M. Jia et al.
!!Head
(a) Visual -Prompt Tuning: Deep (b) Visual -Prompt Tuning: ShallowTuned Frozen
Transformer Encoder LayerTransformer Encoder LayerTransformer Encoder Layer
…!!
!"
!#Backbone
InputHead
CLSHead
Transformer Encoder LayerTransformer Encoder LayerTransformer Encoder Layer
…!!
!"
!#CLS
Embed
"! #! !! "! #!"$
%# %%…
Fig. 2. Overview of our proposed Visual-Prompt Tuning. We explore two variants:
(a) prepend a set of learnable parameters to each Transformer encoder layer’s input
(VPT -deep ); (b) only insert the prompt parameters to the first layer’s input ( VPT -
shallow ). During training on downstream tasks, only the parameters of prompts and
linear head are updated while the whole Transformer encoder is frozen.
3 Approach
We propose Visual-Prompt Tuning ( VPT ) for adapting large pre-trained vision
Transformer models. VPT injects a small number of learnable parameters into
Transformer’s input space and keeps the backbone frozen during the downstream
training stage. The overall framework is presented in Fig. 2. We first define the
notations in Sec. 3.1, then describe VPT formally in Sec. 3.2.
3.1 Preliminaries
For a plain Vision Transformer (ViT) [19] with Nlayers, an input image is
divided into mfixed-sized patches {Ij∈R3×h×w|j∈N,1≤j≤m}.h, w are
the height and width of the image patches. Each patch is then first embedded
intod-dimensional latent space with positional encoding:
ej
0=Embed (Ij) ej
0∈Rd, j= 1,2, . . . m . (1)
We denote the collection of image patch embeddings, Ei={ej
i∈Rd|j∈
N,1≤j≤m}, as inputs to the ( i+1)-th Transformer layer ( Li+1). Together
with an extra learnable classification token ( [CLS] ), the whole ViT is formulated
as:
[xi,Ei] =Li([xi−1,Ei−1]) i= 1,2, . . . , N (2)
y=Head (xN), (3)
where xi∈Rddenote [CLS] ’s embedding at Li+1’s input space. [ ·,·] indicates
stacking and concatenation on the sequence length dimension, i.e., [xi,Ei]∈
R(1+m)×d. Each layer Liconsists of Multiheaded Self-Attention (MSA) and
Feed-Forward Networks (FFN) together with LayerNorm [1] and residual con-

--- PAGE 5 ---
Visual Prompt Tuning 5
nections [31]. A neural classification head is used to map the final layer’s [CLS]
embedding, xN, into a predicted class probability distribution y.2
3.2 Visual-Prompt Tuning (VPT)
Given a pre-trained Transformer model, we introduce a set of pcontinuous em-
beddings of dimension d,i.e.,prompts , in the input space after the Embed layer.
Only the task-specific prompts are being updated during fine-tuning, while the
Transformer backbone is kept frozen. Depending on the number of Transformer
layers involved, our approach has two variants, VPT -shallow andVPT -deep ,
as shown in Fig. 2.
VPT-Shallow. Prompts are inserted into the first Transformer layer L1only.
Each prompt token is a learnable d-dimensional vector. A collection of pprompts
is denoted as P={pk∈Rd|k∈N,1≤k≤p}, the shallow-prompted ViT is:
[x1,Z1,E1] =L1([x0,P,E0]) (4)
[xi,Zi,Ei] =Li([xi−1,Zi−1,Ei−1]) i= 2,3, . . . , N (5)
y=Head (xN), (6)
where Zi∈Rp×drepresents the features computed by the i-th Transformer
layer, and [ xi,Zi,Ei]∈R(1+p+m)×d. The colors •and•indicate learnable and
frozen parameters, respectively. Notably for ViT, xNis invariant to the location
of prompts since they are inserted after positional encoding, e.g., [x0,P,E0] and
[x0,E0,P] are mathematically equivalent. This also applies to VPT-Deep.
VPT-Deep. Prompts are introduced at every Transformer layer’s input space.
For (i+1)-th Layer Li+1, we denote the collection of input learnable prompts as
Pi={pk
i∈Rd|k∈N,1≤k≤m}. The deep-prompted ViT is formulated as:
[xi,,Ei] =Li([xi−1,Pi−1,Ei−1]) i= 1,2, . . . , N (7)
y=Head (xN). (8)
Storing Visual Prompts. VPT is beneficial in presence of multiple down-
stream tasks. We only need to store the learned prompts and classification head
for each task and re-use the original copy of the pre-trained Transformer model,
significantly reducing the storage cost. For instance, given a ViT-Base with 86
million (M) parameters and d= 768, 50 shallow prompts and deep prompts
yield additional p×d= 50×768 = 0 .038M, and N×p×d= 0.46M parameters,
amounting to only 0.04% and 0.53% of all ViT-Base parameters, respectively.
2Some Transformer architectures in Vision such as Swin [52] do not use [CLS] and
treat global pooled ENas input for Head. We follow their designs when adapting VPT
to these Transformer variants. See Appendix A for more details.

--- PAGE 6 ---
6 M. Jia et al.
4 Experiments
We evaluate VPT for a wide range of downstream recognition tasks with pre-
trained Transformer backbones across scales. We first describe our experimental
setup in Sec. 4.1, including the pre-trained backbone and downstream tasks, and
a brief introduction of alternative transfer learning methods. Then we demon-
strate the effectiveness and practical utility of our method in Sec. 4.2. We
also systematically study how different design choices would affect performance
(Sec. 4.3), which leads to an improved understanding of our approach.
4.1 Experiment Setup
Pre-trained Backbones. We experiment with two Transformer architectures
in vision, Vision Transformers (ViT) [19] and Swin Transformers (Swin [52]).
All backbones in this section are pre-trained on ImageNet-21k [16]. We follow
the original configurations, e.g., number of image patches divided, existence of
[CLS] ,etc. More details are included in Appendix A.
Baselines. We compare both variants of VPT with other commonly used fine-
tuning protocols:
(a)Full : fully update allbackbone and classification head parameters.
(b)Methods that focus on the classification head. They treat the pre-trained
backbone as a feature extractor, whose weights are fixed during tuning:
–Linear : only use a linear layer as the classification head.
–Partial -k: fine-tune the last klayers of backbone while freezing the oth-
ers, as adopted in [85,88,60,30]. It redefines the boundary of backbone and
classification head.
–Mlp-k: utilize a multilayer perceptron (MLP) with klayers, instead of a
linear layer, as classification head.
(c)Methods that update a subset backbone parameters or add new trainable
parameters to backbone during fine-tuning:
–Sidetune [87]: train a “side” network and linear interpolate between pre-
trained features and side-tuned features before being fed into the head.
–Bias [8,5]: fine-tune only the bias terms of a pre-trained backbone.
–Adapter [34,63,64]: insert new MLP modules with residual connection in-
side Transformer layers.
Downstream Tasks. We experiment on the following two collections of datasets:
FGVC consists of 5 benchmarked Fine-Grained Visual Classification tasks in-
cluding CUB-200-2011 [75], NABirds [72], Oxford Flowers [59], Stanford Dogs [41]
and Stanford Cars [23]. If a certain dataset only has train and test sets publicly
available, we randomly split the training set into train (90%) and val(10%),
and rely on valto select hyperparameters.
VTAB-1k [86] is a collection of 19 diverse visual classification tasks, which are
organized into three groups: Natural - tasks that contain natural images captured
using standard cameras; Specialized - tasks that contain images captured via
specialized equipment, such as medical and satellite imagery; and Structured
- tasks that require geometric comprehension like object counting. Each task

--- PAGE 7 ---
Visual Prompt Tuning 7
Table 1. ViT-B/16 pre-trained on supervised ImageNet-21k. For each method and each
downstream task group, we report the average test accuracy score and number of wins
in (·)compared to Full . “Total params” denotes total parameters needed for all 24
downstream tasks. “Scope” denotes the tuning scope of each method. “Extra params”
denotes the presence of additional parameters besides the pre-trained backbone and
linear head. Best results among all methods except Full arebolded .VPT outshines
the full fine-tuning 20 out of 24 cases with significantly less trainable parameters
ViT-B/16 Total Scope ExtraFGVCVTAB-1k
(85.8M) params Input Backbone params Natural Specialized Structured
Total # of tasks 5 7 4 8
(a) Full 24.02× ✓ 88.54 75.88 83.36 47.64
(b)Linear 1.02× 79.32 (0) 68.93 (1) 77.16 (1) 26.84 (0)
Partial -1 3.00× 82.63 (0) 69.44 (2) 78.53 (0) 34.17 (0)
Mlp-3 1.35× ✓ 79.80 (0) 67.80 (2) 72.83 (0) 30.62 (0)
(c)Sidetune 3.69× ✓ ✓ 78.35 (0) 58.21 (0) 68.12 (0) 23.41 (0)
Bias 1.05× ✓ 88.41 (3) 73.30 (3) 78.25 (0) 44.09 (2)
Adapter 1.23× ✓ ✓ 85.66 (2) 70.39 (4) 77.11 (0) 33.43 (0)
(ours)VPT -shallow 1.04×✓ ✓84.62 (1) 76.81 (4) 79.66 (0) 46.98 (4)
VPT -deep 1.18× 89.11 (4)78.48 (6)82.43 (2)54.98 (8)
of VTAB contains 1000 training examples. Following [86], we use the provided
800-200 split of the train set to determine hyperparameters and run the final
evaluation using the full training data. We report the average accuracy score on
test set within three runs.
We report the average accuracy on the FGVC datasets, and the average
accuracy on each of the three groups in VTAB. The individual results on each
task are in Appendix D, as are image examples of these aforementioned tasks.
4.2 Main Results
Tab. 1 presents the results of fine-tuning a pre-trained ViT-B/16 on averaged
across 4 diverse downstream task groups, comparing VPT to the other 7 tuning
protocols. We can see that:
1.VPT-Deep outperforms Full (Tab. 1 (a)) on 3 out of the 4 problem
classes (20 out of 24 tasks), while using significantly fewer total model pa-
rameters (1.18 ×vs. 24.02 ×). Thus, even if storage is not a concern ,VPT is
a promising approach for adapting larger Transformers in vision. Note that
this result is in contrast to comparable studies in NLP, where prompt tuning
matches, but does not exceed full fine-tuning [45].
2.VPT-Deep outperforms all the other parameter-efficient tuning
protocols (Tab. 1 (b,c)) across all task groups , indicating that VPT -
deep is the best fine-tuning strategy in storage-constrained environments.
3. Although sub-optimal than VPT -deep ,VPT -shallow still offers non-trivial
performance gain than head-oriented tuning methods in Tab. 1 (b), indicating
thatVPT -shallow is a worthwhile choice in deploying multi-task fine-tuned
models if the storage constraint is severe.

--- PAGE 8 ---
8 M. Jia et al.
101
10060708090
VPT vs. Linear
101
10060708090
VPT vs. Adapter
101
10060708090
VPT vs. BiasT est accuracy (%)
Fraction of downstream training dataset (in log scale)VPT-Deep VPT-Shallow Full Linear Adapter Bias
Fig. 3. Performance comparison on different downstream data scales, averaged across
5 FGVC tasks. VPT -deep is compared with Linear (left), Adapter (middle) and
Bias (right). Highlighted region shows the accuracy difference between VPT -deep
and the compared method. Results of VPT -shallow areFull presented in all plots
for easy reference. The size of markers are proportional to the percentage of tunable
parameters in log scale
200 400 60070758085
ViT-B/16ViT-L/16
ViT-H/14VTAB-Natural (7)
200 400 60070758085
ViT-B/16ViT-L/16ViT-H/14VTAB-Specialized (4)
200 400 600304050
ViT-B/16 ViT-L/16ViT-H/14VTAB-Structured (8)T est accuracy (%)
T otal backbone parameters (M)VPT-deep VPT-shallow Full Linear Bias Adapter
Fig. 4. VPT vs.Full across model scales (ViT-B, ViT-L and ViT-H), for 3 VTAB
task groups. Highlighted region shows the accuracy difference between VPT -deep and
the full fine-tuning ( Full ). The size of markers are proportional to the percentage of
trainable parameters in log scale
VPT on different downstream data size. We look at the impact of training
data size on accuracy in the FGVC tasks (VTAB has only 1k training examples).
We vary the training data between 10% and 80% and compare all methods. The
same pre-trained ViT-B is used for downstream training. Task-averaged results
for each method on different training data scales are presented in Fig. 3.
Fig. 3 shows that VPT -deep outperforms all the other baselines across data
scales. Digging deeper, methods that use less trainable parameters, i.e.,VPT ,
Linear ,Adapter ,Bias , dominate over Full in the low-data regimes. This
trend, however, is reversed when more training data is available for Linear and
Adapter . In contrast, VPT -deep still consistently outperforms Full across
training data sizes. Although Bias offers similar advantages, it still marginally
under-performs VPT -deep across the board (Fig. 3 right).
VPT on different backbone scales. Fig. 4 shows VTAB-1k performance
under 3 different backbone scales: ViT- Base/Large/Huge.VPT -deep is signif-

--- PAGE 9 ---
Visual Prompt Tuning 9
Table 2. Different Transformer architecture: Swin-B pre-trained on supervised
ImageNet-21k as backbone. For each method and each downstream task group, we
report the average test accuracy score and number of wins in ( ·)compared to Full .
The column “Total params” denotes total parameters needed for all 19 downstream
tasks. Best results among all methods except Full arebolded
Swin-B Total VTAB-1k
(86.7M) params Natural Specialized Structured
Total # of tasks 7 4 8
(a) Full 19.01× 79.10 86.21 59.65
(b)Linear 1.01×73.52 (5) 80.77 (0) 33.52 (0)
Mlp-3 1.47 ×73.56 (5) 75.21 (0) 35.69 (0)
Partial 3.77×73.11 (4) 81.70 (0) 34.96 (0)
(c) Bias 1.06×74.19 (2) 80.14 (0) 42.42 (0)
(ours)VPT -shallow 1.01×79.85 (6) 82.45 (0) 37.75 (0)
VPT -deep 1.05×76.78 ( 6) 84.53 (0) 53.35 (0)
icantly better than Linear andVPT -shallow across all 3 backbone choices
and 3 subgroups of VTAB-1k. More importantly, the advantages of VPT -deep
overFull indeed still hold as the model scale increases, i.e.,VPT -deep sig-
nificantly outperforms Full onNatural andStructured groups, while offering
nearly equivalent performance on Specialized .
VPT on hierarchical Transformers. We extend VPT to Swin [52], which
employs MSA within local shifted windows and merges patch embeddings at
deeper layers. For simplicity and without loss of generality, we implement VPT
in the most straightforward manner: the prompts are attended within the local
windows, but are ignored during patch merging stages. The experiments are con-
ducted on the ImageNet-21k supervised pre-trained Swin- Base.VPT continues
to outperform other parameter-efficient fine-tuning methods ( b, c ) for all three
subgroups of VTAB Tab. 2, though in this case Full yields the highest accuracy
scores overall (at a heavy cost in total parameters).
It is surprising that the advantage of VPT -deep overVPT -shallow di-
minishes for Natural :VPT -shallow yields slightly better accuracy scores than
full fine-tuning.
4.3 Ablation on Model Design Variants
We ablate different model design choices on the supervised ImageNet-21k pre-
trained ViT-Base and evaluate them on VTAB, with same setup in Tab. 1. See
more in Appendix B.
Prompt Location. An important distinction between VPT and other methods
is the extra learnable parameters introduced as inputs for the Transformer layers.
Fig. 5 ablates different choices on how and where to insert prompts in the input
space, and how they would affect the final performance.

--- PAGE 10 ---
10 M. Jia et al.
Embed
Prepend (default)
Embed
Add
Embed
 EmbedTuned
Frozen
Prepend -pixel Concat -channelCLS
CLSCLS
CLS
50 60 70 80 9068.9 (1)47.2 (0)71.5 (3)74.0 (4)76.8 (4)77.5 (5)78.5 (6)75.9
LinearConcat-channelPrepend-pixelAddPrepend (default)AddPrepend (default)Full (a)
(b) VPT 
DeepVPT  
ShallowVTAB-Natural (7)
60 70 80 90 10077.2 (1)65.7 (0)78.4 (1)78.0 (1)79.7 (0)80.2 (0)82.4 (2)83.4
LinearConcat-channelPrepend-pixelAddPrepend (default)AddPrepend (default)Full (a)
(b) VPT 
DeepVPT  
ShallowVTAB-Specialized (4)
20 30 40 50 60 7026.8 (0)33.0 (1)44.5 (4)38.6 (1)47.0 (4)43.2 (1)55.0 (8)47.6
LinearConcat-channelPrepend-pixelAddPrepend (default)AddPrepend (default)Full (a)
(b) VPT 
DeepVPT  
ShallowVTAB-Structured (8)
Fig. 5. Ablation on prompt location. We illustrate different location choices at top, and
present the results at bottom. For easy comparison, two blue dashed lines represent
the performance of the default VPT -deep andVPT -shallow respectively
Prepend or Add? Instead of prepending prompts to the sequence of the image
patches embeddings Eias described in Sec. 3.2, another option is to directly add
prompts element-wise to those embeddings, keeping the Transformer’s input
sequence length the same as before. Though this variant is competitive to Full
in some cases ( e.g., VTAB- Natural ), its performance generally falls behind with
the default Prepend in both deep and shallow settings. More discussion on this
phenomenon is in Appendix B.
Latent or pixel space? Instead of inserting the prompts as latent vectors for
the first Transformer layer, one could introduce prompts in the pixel level before
theEmbed layer in Eq. (1), i.e.,Prepend-pixel and Concat-channel . Fig. 5
shows that the adaption performance decreases for these two variants. For ex-
ample, the accuracy score of prepending shallow prompts before the projection
layer ( Prepend-pixel ) drops 6.9%, compared to the default prepending in the
embedding space ( Prepend ) on VTAB- Natural . The performance further deteri-
orates (even as large as 30 accuracy scores drop on VTAB- Natural ) if we instead
concatenate a new channel to the input image ( Concat-channel ). These obser-
vations suggest that it’s easier for prompts to learn condensed task-dependent
signals in the latent input space of Transformers.
Prompt Length. This is the only additional hyper-parameter needed to tune
for VPT compared to full fine-tuning. For easy reference, we also ablate two
other baselines on their individual additional hyper-parameters, i.e., number
of layers for Mlp and reduction rate for Adapter . As shown in Fig. 6, the
optimal prompt length varies across tasks. Notably, even with as few as only
oneprompt, VPT -deep still significantly outperforms the other 2 baselines,
and remains competitive or even better compared to full fine-tuning on VTAB-
Structured andNatural .

--- PAGE 11 ---
Visual Prompt Tuning 11
101
10065707580
 1510
50100
200
Full fine-tuningVTAB-Natural (7)
101
100758085
15
1050
100200Full fine-tuningVTAB-Specialized (4)
102
101
100304050
1510
50100200
Full fine-tuningVTAB-Structured (8)T est accuracy (%)
Tunable parameters w.r.t. backbone (%)VPT-deep VPT-deep (best) Adapter MLP
Fig. 6. Ablation on prompt length. We vary the number of prompts for VPT -deep
and show the averaged results for each VTAB subgroup. The averaged best VPT -deep
results for each task is also shown for easy reference
912
612
312
112
113
16
19
112
747678
Full fine-tuningVTAB-Natural (7)
912
612
312
112
113
16
19
112
808284
Full fine-tuningVTAB-Specialized (4)
912
612
312
112
113
16
19
112
40455055
Full fine-tuningVTAB-Structured (8)T est accuracy (%)
Prompt depthbottomtop
topbottom
Fig. 7. Ablation on prompt depth. We select the best prompt length for each variant
with valsets.i→jindicates the Transformer layer indices that prompts are inserted
into. The 1-st layer refers to the one closest to input. ViT-B has 12 layers in total
Prompt Depth. Fig. 7 ablates which and how many layers to insert prompts.
Each variant reports the best prompt length selected with valset.VPT ’s per-
formance is positively correlated with the prompt depth in general. Yet the ac-
curacy drops if we insert prompts from top to bottom, suggesting that prompts
at earlier Transformer layers matter more than those at later layers.
Final Output. Following the original configuration of ViT, we use the final
embedding of [CLS] ,i.e.,xN, as the classification head input, which is also the
default setting in our ViT experiments. As shown in Fig. 8, if we use the average
pooling on image patch output embeddings ENas final output ( Image-pool ), the
results essentially remain the same ( e.g., 82.4 vs. 82.3 for VTAB- Specialized ).
However, if the pooling involves final prompt outputs ZN(Prompt-pool and
Global-pool ), the accuracy could drop as large as 8 points.
5 Analysis and Discussion
Visualization. Fig. 9 shows t-SNE [55] visualizations of xN,i.e., embeddings
of[CLS] after the last Transformer layer and before the classification head, for
3 tasks in VTAB (SVNH [58], EuroSAT [32], Clevr/count [38]), one for each

--- PAGE 12 ---
12 M. Jia et al.
Last Transformer Layer
[CLS] (default)CLSPrompt ImageHead
Last Transformer LayerCLSPrompt Image
Last Transformer LayerCLSPrompt ImageHead
Last Transformer LayerCLSPrompt ImageHead Head
Prompt -pooling Image -pooling Global -poolingTuned
Frozen
40 60 80 10068.9 (1)70.2 (6)79.3 (5)75.3 (5)78.5 (6)75.9
LinearGlobal-poolImage-poolPrompt-poolCLS (default)Full (a)
(b)VPT 
DeepVTAB-Natural (7)
60 70 80 9077.2 (1)80.5 (0)82.3 (1)75.7 (2)82.4 (2)83.4
LinearGlobal-poolImage-poolPrompt-poolCLS (default)Full (a)
(b)VPT 
DeepVTAB-Specialized (4)
20 40 60 80 10026.8 (0)50.7 (7)54.5 (8)49.9 (4)55.0 (8)47.6
LinearGlobal-poolImage-poolPrompt-poolCLS (default)Full (a)
(b)VPT 
DeepVTAB-Structured (8)
Fig. 8. Ablation on final output. Illustration of different strategies is included at top,
and results of those are presented at the bottom section. For easy comparison, the blue
dashed line represents the performance of default VPT -deep
(a)SVNH (VTAB -Natural) (b)EuroSAT (VTAB -Specialized) (c)CLEVR (VTAB -Structured)
Fig. 9. t-SNE visualizations of the final [CLS] embedding xNof 3 VTAB tasks from
thetest set, from Tab. 1. VPT could produce linearly separable features without
updating backbone parameters
subgroup. All plots show that VPT -deep enables linearly separable representa-
tions while using less parameters than Full . We also observe that extra tunable
parameters for every Transformer layer ( VPT -deep ) improve the performance,
compared to VPT -shallow , which only inserts prompts for the first layer’s in-
put. Interestingly on Clevr/count (Fig. 9(c)), VPT -deep andFull recover the
underlying manifold structure of the task (counting objects in images vs. street
number or landscape recognition), unlike VPT -shallow andLinear .
Apply VPT to more vision tasks. We explore the feasibility of VPT beyond
visual classification, by evaluating ADE20K [90] semantic segmentation task with
a Transformer model, SETR-PUP [89]. It adds a standard ConvNet head to
the ViT backbone to perform segmentation. The de-facto approach is still fully
fine-tuning the pre-trained backbone together with the ConvNet head ( Full ).
We include two more protocols for comparison: only update the head layers

--- PAGE 13 ---
Visual Prompt Tuning 13
Table 3. Semantic Segmentation: ADE20k [90] validation results with SETR [89] on
ViT-L. The best mIoU scores among all methods but Full arebolded . Results of
fully fine-tuning a ResNet-101 [10] are included. SS/MS: single/multi-scale inference
Backbone ViT-L/16 ResNet-101
Method Full [89] Head Only Bias VPT -deep VPT+Bias Full [10]
mIoU -SS 48.31 35.12 43.40 42.11 44.04 45.47
mIoU -MS 50.07 37.46 45.33 44.06 45.63 46.27
Tunable params (M) 318.31 13.18 13.46 13.43 15.79 63.0
Table 4. Different pre-trained objectives: MAE [30] and MoCo v3 [11] with a ViT-B
backbone. For each method and each downstream task group, we report the average
test accuracy score and number of wins in ( ·)compared to Full . “Total params”
denotes total parameters needed for all 24 downstream tasks. Best results among all
methods except Full arebolded
MAE MoCo v3
ViT-B/16 Total VTAB-1k Total VTAB-1k
(85.8M) params Natural Specialized Structured params Natural Specialized Structured
Total # of tasks 7 4 8 7 4 8
(a) Full 19.01× 59.29 79.68 53.82 19.01× 71.95 84.72 51.98
(b)Linear 1.01×18.87 (0) 53.72 (0) 23.70 (0) 1.01×67.46 (4) 81.08 (0) 30.33 (0)
Partial -1 2.58 ×58.44 (5)78.28 (1) 47.64 ( 1) 2.58×72.31 ( 5)84.58 (2) 47.89 (1)
(c)Bias 1.03×54.55 (1) 75.68 (1) 47.70 (0) 1.03×72.89 (3) 81.14 (0) 53.43 (4)
Adapter 1.17×54.90 (3) 75.19 (1) 38.98 (0) 1.22×74.19 (4) 82.66 (1) 47.69 (2)
(ours)VPT -shallow 1.01×39.96 (1) 69.65 (0) 27.50 (0) 1.01×67.34 (3) 82.26 (0) 37.55 (0)
VPT -deep 1.04×36.02 (0) 60.61 (1) 26.57 (0) 1.01×70.27 (4) 83.04 (0) 42.38 (0)
(Head Only ), update head layers and bias vectors in the backbone ( Bias ).
In Tab. 3, we report valmIoU results with and without multi-scale inference.
Though parameter-efficient protocols could not compete with Full ,VPT is
still comparable with Bias . Notably, VPT offers competitive results to a fully
fine-tuned state-of-the-art ConvNet model (DeepLab v3+ [10]), while tuning
significantly less parameters (15M vs. 64M, respectively).
Apply VPT to more pre-training methods. In addition to the backbones
pre-trained with labeled data, we experiment with two self-supervised objectives:
MAE [30] and MoCo v3 [11]. Tab. 4 reports the results on VTAB-1k with ViT-
B. We observe that both variants of VPT surpass Linear , yet the comparisons
among other techniques are less conclusive. For MAE, other parameter-efficient
methods, e.g.,Partial -1, outperform both VPT andLinear . In the case of
MoCo v3, VPT no longer holds the best performance, though it is still com-
petitive with the others. This suggests that these two self-supervised ViTs are
fundamentally different from the supervised ones in previous sections. Exactly
why and how these differences arise remain open questions.
Apply VPT to ConvNets. We examine the idea of adding trainable pa-
rameters in the input space of ConvNets: padding both height and width by p

--- PAGE 14 ---
14 M. Jia et al.
Table 5. Apply VPT to ConvNets: ResNet-50 and ConvNeXt-Base. For each method
and each downstream task group, we report the average test accuracy score and number
of wins in ( ·)compared to Full . “Total params” denotes total parameters needed
for all 19 downstream tasks. Best results among all methods except Full arebolded
ConvNeXt-Base (87.6M) ResNet-50 (23.5M)
Total VTAB-1k Total VTAB-1k
params Natural Specialized Structured params Natural Specialized Structured
Total # of tasks 7 4 8 7 4 8
(a) Full 19.01× 77.97 83.71 60.41 19.08× 59.72 76.66 54.08
(b)Linear 1.01×74.48 (5) 81.50 (0) 34.76 ( 1) 1.08×63.75 ( 6) 77.60 ( 3) 30.96 (0)
Partial -1 2.84 ×73.76 (4) 81.64 (0) 39.55 (0) 4.69×64.34 ( 6)78.64 (2) 45.78 (1)
Mlp-3 1.47 ×73.78 (5) 81.36 ( 1) 35.68 ( 1) 7.87×61.79 ( 6) 70.77 (1) 33.97 (0)
(c) Bias 1.04×69.07 (2) 72.81 (0) 25.29 (0) 1.10×63.51 ( 6) 77.22 (2) 33.39 (0)
(ours) Visual-Prompt Tuning 1.02 ×78.48 (6)83.00 (1)44.64 (1) 1.09×66.25 (6) 77.32 (2) 37.52 (0)
learnable prompt pixels for the input image. Though this operation seems un-
conventional, we implement VPT this way given there is no obvious solution to
add location-invariant prompts similar to the Transformer counterparts. In fact
this approach has been explored before in the adversarial attack literature [20].
The value of pin our experiment is 2 orders of magnitude smaller than previous
work: e.g., 5vs. 263. Most importantly, we cast this idea in the lens of transfer
learning. See Appendix C for more discussion.
Tab. 5 presents the results for ConvNeXt-B [53] (pre-trained on ImageNet-
21k) and ResNet-50 [31] (pre-trained on ImageNet-1k), respectively. VPT works
well in a larger ConvNet backbone, ConvNeXt-B, offering accuracy gains over
other sparse tuning protocols ( b,c), and outperforming Full on 8 out of 19
cases. The advantages of VPT , however, diminish with smaller ConvNet (ResNet-
50), as there is no clear winner for all 19 VTAB-1k tasks.
6 Conclusion
We present Visual Prompt Tuning, a new parameter-efficient approach to lever-
age large vision Transformer models for a wide range of downstream tasks. VPT
introduces task-specific learnable prompts in the input space, keeping the pre-
trained backbone fixed. We show that VPT can surpass other fine-tuning pro-
tocols (often including full fine-tuning) while dramatically reducing the storage
cost. Our experiments also raise intriguing questions on fine-tuning dynamics of
vision Transformers with different pre-training objectives, and how to transfer
to broader vision recognition tasks in an efficient manner. We therefore hope
our work will inspire future research on how best to tap the potential of large
foundation models in vision.
Acknowledgement. Menglin is supported by a Meta AI research grant awarded to
Cornell University, Luming and Bharath is supported by NSF IIS-2144117, Serge is
supported in part by the Pioneer Centre for AI, DNRF grant number P1. We would
like to thank Alexander Rush, Yin Cui for valuable suggestions and discussion.

--- PAGE 15 ---
Visual Prompt Tuning 15
A Implementation Details
We use PyTorch [62] to implement all experiments on NVIDIA A100-40GB GPUs.
A.1 Classification Experiments
VPT. We use valset of each dataset to find best prompt length p, see Sec. 3.2. The
prompt length is the only VPT-specific hyper-parameter that we tune. For Transformer
backbones, the range of pis{1,5,10,50,100,200}and{1,5,10,50}for ViT and Swin,
respectively. The maximum choice of pis approximately close to the number of image
patch tokens within each MSA for both architectures (ViT: 196, Swin: 49). We also
apply a dropout of 0 .1 forVPT -deep . For ConvNets, the range of pis{1,3,5,7,9,11}.
Each prompt is randomly initialized with xavier uniform initialization scheme [26]. We
follow the original backbone’ design choices, such as the existence of the classification
tokens [CLS] , or whether or not to use the final [CLS] embeddings for the classification
head input.
Adapter. Adapters [34] insert extra lightweight modules inside each Transformer
layer. One adapter module generally consists of a linear down-projection (with a re-
duction rate r), followed by a nonlinear activation function, and a linear up-projection,
together with a residual connection. [63,64] exhaustively searched all possible config-
urations and found that only inserting adapters after the FFN “Add & LayerNorm”
sub-layer works the best. Therefore we also use this setup in our own implementation.
We sweep the reduction rate rin{8,64,256}.
Augmentation and other hyper-parameters. We adopt standard image aug-
mentation strategy during training: normalize with ImageNet means and standard
deviation, randomly resize crop to 224 ×224 and random horizontal flip for five FGVC
datasets, and resize to 224 ×224 for the VTAB-1k suite.3Tab. 6 summarizes the op-
timization configurations we used. Following [56], we conduct grid search to find the
tuning-specific hyper-parameters, learning rate, and weight decay values using valset
of each task. Following the linear scaling rule [42,27,11,30], the learning rate is set as
Table 6. Implementation details for each fine-tuning method evaluated. ⋆: we observe
that for VPT -shallow sometimes benefit from a larger base LR for 6 out of 24 tasks
evaluated, where we search from {1000.0,500.0,250.0,100.0}
Full ,Partial ,Bias,Adapter Linear ,Sidetune ,Mlp,VPT
Optimizer AdamW [54] SGD
Optimizer momentum - 0.9
base lrrange {0.001, 0.0001, 0.0005, 0.005 } {50., 25., 10., 5., 2.5, 1.,0.5, 0.25, 0.1, 0.05 }⋆
Weight decay range {0.01,0.001,0.0001,0.0}
Learning rate schedule cosine decay
Warm up epochs 10
Total epochs 100 (ViT-B, Swin-B), 50 (ViT-L/H)
3Following the default settings in VTAB, we don’t adopt other augmentations

--- PAGE 16 ---
16 M. Jia et al.
Table 7. Specifications of the various datasets evaluated.⋆: we randomly sampled the
train and valsets since there are no public splits available
Dataset Description # Classes Train Val Test
Fine-grained visual recognition tasks (FGVC)
CUB-200-2011 [75] Fine-grained bird species recognition 200 5,394⋆600⋆5,794
NABirds [72] Fine-grained bird species recognition 55 21,536⋆2,393⋆24,633
Oxford Flowers [59] Fine-grained flower species recognition 102 1,020 1,020 6,149
Stanford Dogs [41] Fine-grained dog species recognition 120 10,800⋆1,200⋆8,580
Stanford Cars [23] Fine-grained car classification 196 7,329⋆815⋆8,041
Visual Task Adaptation Benchmark (VTAB-1k) [86]
CIFAR-100 [43]
Natural100
800/1000 20010,000
Caltech101 [47] 102 6,084
DTD [13] 47 1,880
Flowers102 [59] 102 6,149
Pets [61] 37 3,669
SVHN [58] 10 26,032
Sun397 [83] 397 21,750
Patch Camelyon [74]
Specialized2
800/1000 20032,768
EuroSAT [33] 10 5,400
Resisc45 [12] 45 6,300
Retinopathy [40] 5 42,670
Clevr/count [38]
Structured8
800/1000 20015,000
Clevr/distance [38] 6 15,000
DMLab [4] 6 22,735
KITTI/distance [24] 4 711
dSprites/location [57] 16 73,728
dSprites/orientation [57] 16 73,728
SmallNORB/azimuth [44] 18 12,150
SmallNORB/elevation [44] 9 12,150
Table 8. Specifications of different pre-trained backbones used in the paper. Param-
eters (M) are of the feature extractor. “Batch size” column reports the batch size for
Linear /Partial /{Full ,Bias,Adapter }/VPT (p <100) / VPT (p≥100). All
backbones are pre-trained on ImageNet [16] with resolution 224 ×224
BackbonePre-trained
ObjectivePre-trained
Dataset# params
(M)Feature dim
d Batch SizePre-trained
Model
ViT-B/16 [19]
Supervised ImageNet-21k85 768 2048 / 1280 / 128 / 128 / 64 checkpoint
ViT-L/16 [19] 307 1024 2048 / 640 / 64 / 64 / 32 checkpoint
ViT-H/14 [19] 630 1280 1024 / 240 / 28 / 28 / 14 checkpoint
ViT-B/16 [19] MoCo v3 [11]ImageNet-1k 85 768 2048 / 1280 / 128 / 128 / 64checkpoint
ViT-B/16 [19] MAE [30] checkpoint
Swin-B [52] Supervised ImageNet-21k 88 1024 1024 / 1024 / 128 / 80 / - checkpoint
ConvNeXt-Base [53] Supervised ImageNet-21k 88 1024 1024 / 1024 / 128 / 128 / - checkpoint
ResNet-50 [31] Supervised ImageNet-1k 23 2048 2048 / 2048 / 384 / 256 / - checkpoint

--- PAGE 17 ---
Visual Prompt Tuning 17
VTAB -Structured
 VTAB -Natural
 VTAB -Specialized
 FGVC
Fig. 10. Dataset examples for all classification tasks evaluated

--- PAGE 18 ---
18 M. Jia et al.
base lr×b/256, where bis the batch size used for the particular model, and base lris
chosen from the range specified in Tab. 6. The optimal hyper-parameter values for each
experiment can be found in Appendix D.
Datasets and pre-trained backbones specifications. Tabs. 7 and 8 sum-
marize the statistics and details of the evaluated classification datasets and all the
pre-trained backbones used in the paper. Fig. 10 includes image examples of all 24
classification tasks evaluated.
A.2 Semantic Segmentation Experiments
ADE20K [90] is a challenging scene parsing benchmark with 150 fine-grained labels.
The training and validation sets contain 20,210 and 2,000 images respectively. We
utilize the public codebase MMSegmentation [15] in our implementation.4The ViT-L
backbone is supervisely pre-trained on ImageNet-21k.5
SETR [89] is a competitive segmentation framework using ViT as the encoder.
PUP is a progressive upsampling strategy consisting of consecutive convolution layers
and bilinear upsampling operations. Among multiple decoder choices, PUP works the
best according to MMSegmentation’s reproduction therefore we also use it as in our
implementation.6
When applying VPT to SETR-PUP, we only insert prompts into SETR’s ViT en-
coder backbone. For the decoder, only image patch embeddings are used as inputs and
prompt embeddings are discarded. Same as recognition tasks, only the PUP decoder
head and prompts are learned during training and the ViT backbone is frozen.
For full fine-tuning, we use the same hyper-parameters as in MMSegmentation. For
HeadOnly ,Bias, and VPT, we use the hyper-parameter sweep on learning rate {0.05,
0.005, 0.0005, 0.001 }. The optimal learning rate is 0.005 for all methods. We sweep
prompt length p∈ {1, 5, 10, 50, 100, 200 }. For VPT, we also change the learning rate
multiplier to 1.0 instead of the default 10.0, so the decoder head and prompts share
the same learning rate. Other hyper-parameters remain the same as full fine-tuning.
B Extended Analysis
Effect of expanding input sequence length. As shown in Tab. 1, by expand-
ing the input sequence with learnable prompts, VPT achieves better performance than
Full on the 20 out of 24 tasks evaluated. To investigate whether the advantage of
VPT is due to its enlarged input sequence length, we experiment on two more vari-
ants: (1) the prompts are kept frozen during fine-tuning stage ( Prompt-Fixed ). (2)
only tuning the [CLS] token ( [CLS]-Learned ). From Fig. 11 we can see that, updat-
ing prompt embeddings ( Prompt-Learned ) offers significant gains, while Prompt-Fixed
yields comparable results w.r.t. Linear . This suggests that the final performance of
VPT is mainly contributed by the learned prompt embeddings instead of the enlarged
sequence length. Updating the [CLS] token performs similarly as updating 1 prompt
([CLS] vs.Learned p=1), but still lags behind the default setting where we manually
select the best number of prompt tokens based on the valset.
4See the MMSegmentation GitHub page
5ViT-L/16 checkpoint
6MMSegmentation’s reproduction on SETR

--- PAGE 19 ---
Visual Prompt Tuning 19
Embed
Prompt -Learned (default)CLS
Embed
Prompt -FixedCLS
Embed
[CLS] -LearnedCLS
Embed
Prompt -Learned (p=1)CLSTuned
Frozen
40 60 80 10068.9 (1)70.5 (2)72.6 (2)68.7 (1)76.8 (4)56.7 (0)78.575.9
LinearLearnedp=1[CLS] LearnedFixedLearned (default)FixedLearned (default)Full (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Natural (7)
60 70 80 90 10077.2 (1)78.4 (0)78.3 (0)77.9 (1)79.7 (0)77.5 (1)82.4 (4)83.4
LinearLearnedp=1[CLS] LearnedFixedLearned (default)FixedLearned (default)Full (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Specialized (4)
20 40 60 80 10026.8 (0)34.1 (0)35.1 (0)29.3 (0)47.0 (4)27.6 (0)55.047.6
LinearLearnedp=1[CLS] LearnedFixedLearned (default)FixedLearned (default)Full (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Structured (8)
Fig. 11. Effect of expanding input sequence. Illustration of different strategies is in-
cluded at top, and results of those are presented at the bottom section. For easy com-
parison, two dark and light blue lines represent the performance of default VPT -deep
andVPT -shallow , respectively
Sharing prompts. We examine the effect of sharing parameters of prompts in
Fig. 12 by setting the same prompt embedding within Transformer layers ( Shared-intra ),
among all layers ( Shared-inter ), and for all prompts inserted in the Transformer
(Shared-all ). We can observe that: (1) Sharing prompts within layer ( Shared-intra )
performs competitively or slightly outperforms the performance of using one prompt
(Default p=1), further demonstrating the value of expanding input sequence. (2) Al-
though Shared-intra under-performs Default in general, surprisingly, Shared-inter
slightly outperforms our default VPT -deep while using similar number of trainable
parameters (total number of parameters for all VTAB tasks: 1.14 ×vs. 1.13×for
Shared-inter vs.Default , respectively). Closer examination reveals that the opti-
mal prompt length pforShared-inter is in general larger than Default ,i.e., average
prompt length on all VTAB tasks: 64.58 vs. 60.94, for Shared-inter vs.Default ,
respectively. (3) Sharing the same prompt embedding both among and within lay-
ers ( Shared-all ) deteriorates performance, but still surpass the linear probing results
across three VTAB subgroups.
Prompt initialization. In NLP, prompt tuning could benefit from more sophis-
ticated prompt initialization, as shown in [45]. We investigate if this is the case for
visual prompting as well. We utilize prototype representations for downstream target
classes so that the prompts are initialized with embeddings that enumerate the output
space. Since we want the model to produce an output embedding that is close to one
of these prototype representations given a test example, initializing prompts in this
manner might give the model some hints about the target categories thus help improve
the optimization process.
Concretely, we use the averaged final [CLS] embeddings whithin each target class
of the down-stream dataset train split. Given the pre-trained ViT with Nlayers, and
the down-stream train set with ctarget classes, for each training example, we compute
the final [CLS] embeddings, xN∈Rd. Then we average these embeddings within each

--- PAGE 20 ---
20 M. Jia et al.
Transformer LayerTransformer Layer !!"#
!!
!!"##!!"#$!!"!!#
DefaultTransformer LayerTransformer Layer !!"#
!!
Shared (all)! !Transformer LayerTransformer Layer !!"#
!!
!!"#!!"#
Shared (intra -layer)!! !!
Transformer LayerTransformer Layer !!"#
!!!"!#
Shared (inter -layer)!"!#! !Tuned
Frozen
60 70 80 9068.9 (1)71.2 (2)70.5 (2)76.8 (4)77.9 (4)78.9 (6)78.1 (5)78.3 (5)78.5 (6)75.9
LinearShared-intraDefaultp=1DefaultShared-allShared-interShared-intraDefaultp=1DefaultFull (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Natural (7)
70 80 90 10077.2 (1)79.5 (0)78.4 (0)79.7 (0)82.1 (2)83.7 (2)82.0 (1)80.7 (0)82.4 (2)83.4
LinearShared-intraDefaultp=1DefaultShared-allShared-interShared-intraDefaultp=1DefaultFull (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Specialized (4)
20 40 60 8026.8 (0)37.0 (0)34.1 (0)47.0 (4)39.0 (3)55.3 (7)47.6 (4)45.6 (3)55.0 (8)47.6
LinearShared-intraDefaultp=1DefaultShared-allShared-interShared-intraDefaultp=1DefaultFull (a)
(b)VPT 
Deep
VPT  
ShallowVTAB-Structured (8)
Fig. 12. Effect of sharing prompts. Illustration of different strategies is included at
top, and results of those are presented at the bottom section. For easy comparison, the
blue dashed line represents the performance of default VPT -deep
target class to get {ˆxk
N∈Rd|k∈N,1≤k≤c}.7Setting prompt length p=c,8we
initialize Pwith{ˆxk
N}k=c
k=1forVPT -shallow , and initialize each Piwith{ˆxk
N}k=c
k=1,
where i= 0,1, . . . , N −1, for VPT -deep .
We compare the fine-tuning performance using the above initialization strategy
(CLS) against the default random initialization ( Random ) in Fig. 13. We also report
results when we fix the prompts during the fine-tuning stage ( ·-fixed ). As shown in
Fig. 13, it’s quite surprising that our default random initialization ( Random ) works
the best in general, consistently across different subgroups of VTAB without extra
pre-processing steps described above ( CLS).CLSworks comparably in Natural andSpe-
cialized subgroups.9
Prompt depth vs. prompt length. In Fig. 7, we ablate the number of layers we
insert prompts in. For each prompt depth variant, Fig. 7 reports the results using the
best prompt length for each task (“ · → · (best)” in Fig. 14). Here we adopt another
setting where the best prompt length from 1 →12 are used for allother prompt depth
variants. Comparing both “ · → · (best)” and “ · → · ”, we observe that there are varied
sensitivities to prompt length for different depths, especially if we insert prompts in
nine layers only (3 →12, 12 →3).
7ifc >200, we further apply k-means ( k= 200) to class-averaged embeddings and
use the corresponding 200 centroid embeddings as {ˆxk
N∈Rd}k=200
k=1.
8ifc >200, we set p= 200 so that prompt length won’t be too large. In fact, for
VTAB, only the Sun397 task in the Natural subgroup has over 200 classes. See Tab. 7.
9Utilizing the per-class averaged [CLS] features, we also tried several other different
implementation variants, including using per-layer [CLS] embeddings for VPT -deep
instead of only the final output [CLS] vector. They perform either the same as or even
much worse than the CLSstrategy above, and none of them is able to out-perform the
default Random .

--- PAGE 21 ---
Visual Prompt Tuning 21
20 40 60 80 10068.9 (1)64.0 (1)75.1 (4)68.7 (1)76.8 (4)51.4 (0)77.5 (4)56.7 (0)78.5 (6)75.9
LinearCLS-fixedCLSRandom-fixedRandom (default)CLS-fixedCLSRandom-fixedRandom (default)Full (a)
(b) VPT  
 Deep VPT   
ShallowVTAB-Natural (7)
50 60 70 80 90 10077.2 (1)76.8 (0)79.1 (0)77.9 (1)79.7 (0)79.2 (1)78.8 (3)77.5 (1)82.4 (2)83.4
LinearCLS-fixedCLSRandom-fixedRandom (default)CLS-fixedCLSRandom-fixedRandom (default)Full (a)
(b) VPT  
 Deep VPT   
ShallowVTAB-Specialized (4)
0 20 40 60 8026.8 (0)28.4 (0)40.0 (1)29.3 (0)47.0 (4)27.9 (0)48.4 (6)27.6 (0)55.0 (8)47.6
LinearCLS-fixedCLSRandom-fixedRandom (default)CLS-fixedCLSRandom-fixedRandom (default)Full (a)
(b) VPT  
 Deep VPT   
ShallowVTAB-Structured (8)
Fig. 13. Effect of prompt initialization. For easy comparison, the two blue dashed line
represents the performance of default VPT -deep andVPT -shallow , respectively
912
612
312
112
113
16
19
112
72747678
Full fine-tuningVTAB-Natural (7)
912
612
312
112
113
16
19
112
78808284
Full fine-tuningVTAB-Specialized (4)
912
612
312
112
113
16
19
112
4050
Full fine-tuningVTAB-Structured (8)T est accuracy (%)
Prompt depthbottomtop (best)
bottomtop
topbottom (best)
topbottom
Fig. 14. Sensitivity to prompt length for the prompt depth experiments. We select the
best prompt length for each variant with valsets. We also include the same prompt
length for all depth choices. i→jindicates the Transformer layer indices that prompts
are inserted into. The 1-st layer refers to the one closest to input. ViT-B has a total of
12 layers
Combine VPT with Bias Tuning. Our experiments in the main paper reveal
thatBias is a competitive parameter-efficient tuning baseline ( e.g., Tab. 1 (c)). Based
on this observation, we explore another protocol where we update both prompts and
the bias terms of the pre-trained backbone, keeping everything else in the backbone
frozen ( VPT+Bias ). As shown in Tab. 9, to our surprise, incorporating Bias withVPT
does not yield superior results in general, even undermines VPT -deep for all 3 task
subgroups. This suggests that these two methods are not necessarily complementary
to each other.
Prompt ensembling. [45] demonstrated prompt’s efficiency in the context of model
ensembling. For an ensemble of kmodels, we only need to store the learnt prompt
vectors instead of kcopies of the whole fine-tuned model parameters ( e.g.,k×2.5GB for
ViT-H). Furthermore, given one test example during inference time, only one forward
pass is executed with a specially-designed batch with replicated original data but varied
prompts.
Given such advantages, we also investigate VPT’s effectiveness on prompt ensem-
bling. We train 5 different prompts for each VTAB task with different random seeds,

--- PAGE 22 ---
22 M. Jia et al.
Table 9. Combining VPT withBias with a pre-trained ViT-B in Sec. 4.2. For each
method and each downstream task group, we report the average test accuracy score
and number of wins in ( ·)compared to Full . The difference between the hybrid
methods and their VPT counterpart are color coded
Bias VPT -shallow VPT -shallow +Bias VPT -deep VPT -deep +Bias
VTAB-Natural 73.30 (3) 76.81 (4) 79.78 (5) ↑2.97 78.48 (6) 77.64 (6) ↓0.84
VTAB-Specialized 78.25 (0) 79.66 (0) 81.38 (0) ↑1.72 82.43 (2) 82.22 (2) ↓0.21
VTAB-Structured 44.09 (2) 46.98 (4) 45.89 (3) ↓1.09 54.98 (8) 53.87 (6) ↓1.11
Natural
Specialized StructuredFull
Linear
Partial
MLP
Sidetune
BIAS
Adapter
VPT-Shallow
VPT-Deep75.7 83.6 46.7
68.8 77.1 26.6
69.5 78.8 34.1
67.9 74.9 30.4
45.0 69.4 23.0
73.1 78.4 44.2
70.8 77.1 33.1
76.7 79.3 47.5
78.6 82.9 54.9Average
Natural
Specialized Structured76.5 84.3 49.2
69.2 77.9 27.3
70.0 79.5 36.0
68.4 76.7 31.8
53.4 76.5 26.1
74.2 79.6 46.3
71.2 78.8 34.9
77.1 80.0 49.8
79.9 83.8 57.8Best
Natural
Specialized Structured78.8 85.2 50.2
69.2 77.7 27.4
71.7 80.6 37.4
68.9 77.0 31.8
52.3 77.5 23.7
78.0 83.3 50.4
73.2 81.0 36.6
78.1 80.5 52.1
81.1 85.4 59.6Ensemble
Natural
Specialized Structured3.1 1.7 3.6
0.4 0.6 0.8
2.1 1.8 3.2
1.0 2.1 1.3
7.3 8.1 0.6
4.9 4.9 6.2
2.4 3.9 3.5
1.3 1.1 4.6
2.5 2.5 4.7Ensemble - Average
minmax
Fig. 15. Performance of a five-run ensemble. We report the averaged, the best among
five runs as well. Best performance is bolded in each column
using the same pre-trained ViT-B backbone and hyper-parameters as in Tab. 1. Fig. 15
shows that the ensembled VPT -deep outperforms the average or even the best single-
prompt counterparts, as well as other ensembled fine-tuning methods including Full .
Test of statistical significance. We conduct non-parametric paired one-tailed
t-test (the Wilcoxon signed-rank test [82]) on whether VPT -deep ’s performance is
greater than other fine-tuning methods across 19 VTAB tasks (the null hypothesis H0
states that the mean VTAB performance difference between VPT -deep and alternate
baseline method is zero. The alternative hypothesis H1states that VPT -deep outper-
forms the baseline method on VTAB). Tab. 10 presents the p-values of each test, with
the number of observations equal to 19 for each method compared (we use the averaged
accuracy scores among 5 runs for 19 VTAB tasks and all fine-tuning methods). For
all of the fine-tuning protocols compared, VPT -deep ’s improvements are statistically
significant ( p <0.05).
We also conduct un-paired one-tailed t-test with unequal variances (Welch’s t-
test [81]), comparing the individual runs (the number of observations = 5) for each
VTAB task ( H0states that VPT -deep and the other baseline perform the same for a
specific VTAB task, while H1states that VPT -deep outperforms the other baseline
for a specific VTAB task). Fig. 16 presents the p-values for each <VPT -deep , baseline
method >pair on each task. We reject H0on 127 out of 19 ×8 = 152 cases ( p <0.05).
Compared to Full ,VPT -deep achieves statistically significant better performance on
11 out of 19 tasks.
Effect of different fine-tuning hyper-parameters. In Fig. 17, we present
different tuning protocol’s performance on different fine-tuning hyper-parameters in-

--- PAGE 23 ---
Visual Prompt Tuning 23
Table 10. Non-parametric paired one-tailed t-test (the Wilcoxon signed-rank test) on
whether VPT -deep ’s performance is greater than other methods on 19 VTAB tasks.
Results show that, VPT -deep is indeed statistically significantly better than other
fine-tuning protocols ( p <0.05)
(a) (b) (c) (ours)
Full Linear Mlp -3Partial -1Sidetune Bias Adapter VPT -shallow
IsVPT -deep statistically
significantly better? ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
p-value 1.2e-03 2.7e-05 1.9e-06 1.9e-05 1.9e-06 1.9e-06 3.8e-06 2.7e-05
Full
Linear MLP-2Partial-1 SidetuneBias
Adapter
VPT-shallowCIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/location
dSprites/orientation
SmallNORB/azimuth
SmallNORB/elevationVTAB tasks6e-07 2e-06 3e-06 4e-06 2e-08 3e-05 8e-04 4e-01
2e-05 4e-07 2e-06 3e-06 2e-11 6e-06 2e-06 7e-05
5e-02 3e-04 2e-05 2e-04 3e-07 6e-05 1e-03 5e-05
1e-02 5e-05 1e-03 2e-04 3e-02 4e-03 6e-03 2e-03
1e-03 2e-07 1e-09 3e-05 1e-07 2e-06 5e-07 8e-07
1e+00 8e-05 5e-05 8e-05 6e-05 2e-03 8e-05 1e-01
5e-06 2e-01 2e-02 1e-01 6e-02 2e-01 1e-01 3e-01
3e-01 1e-03 1e-04 1e-03 4e-02 5e-02 8e-04 3e-05
1e-01 1e-04 9e-09 2e-08 7e-09 6e-06 2e-06 5e-06
5e-01 2e-09 3e-09 2e-08 6e-10 4e-09 1e-08 2e-07
1e+00 1e+00 3e-02 1e+00 1e-01 2e-01 6e-01 1e+00
3e-04 1e-05 3e-05 5e-06 3e-05 7e-03 4e-06 4e-05
2e-01 3e-11 1e-06 3e-05 7e-11 1e-03 1e-05 1e-02
6e-06 1e-08 3e-09 2e-10 1e-09 4e-10 1e-06 1e-04
9e-03 1e-05 5e-06 1e-05 1e-06 7e-06 1e-06 5e-02
4e-04 2e-06 3e-07 2e-07 2e-08 6e-03 6e-06 8e-02
1e-01 2e-07 6e-08 2e-06 1e-05 2e-04 1e-08 7e-07
2e-03 6e-05 2e-04 3e-04 2e-06 1e-04 1e-04 8e-04
6e-05 6e-06 2e-05 3e-05 2e-05 3e-06 8e-06 2e-024e-01
5e-02
1e+00 1e-01
2e-01 1e-01 6e-02 2e-01 1e-01 3e-01
3e-01 5e-02
1e-01
5e-01
1e+00 1e+00 1e+00 1e-01 2e-01 6e-01 1e+00
2e-01
5e-02
8e-02
1e-01p-values for un-paired one-tailed t-test with unequal variances
Not SignificantSignificant
Fig. 16. Un-paired one-tailed t-test with unequal variances (Welch’s t-test) on whether
VPT -deep ’s performance is greater than other methods for each VTAB task. Results
show that, VPT -deep is statistically significantly better than other fine-tuning proto-
cols ( p <0.05) in most instances
cluding learning rate and weight decay. For our proposed VPT -deep , we also ablate
different choices of prompt length p, which is the only hyper-parameter that needs to be
manually tuned. All experiments are evaluated on the valset of KITTI/Distance task
(VTAB- Specialized ). We observe different behaviors between Linear andVPT . Both
methods freeze backbone parameters during fine-tuning stage. Linear probing is more
sensitive to weight decay values in general, whereas VPT is influenced by both learning
rate and weight decay values. VPT with larger prompt length is also less sensitive to
the choice of learning rate.
Effect of image resolution. The original ViT paper [19] found that fine-tuning
with higher image resolutions (384 ×384) is beneficial to downstream recognition tasks.
All recognition experiments presented in the main paper are fine-tuned on 224 ×224
resolution. As shown in Tab. 11, we re-run the VTAB experiments with the same
setup as in Tab. 1 but in the 384 resolution instead of the default 224. We can see that,
VPT -deep still achieves the best performance among all parameter-efficient tuning

--- PAGE 24 ---
24 M. Jia et al.
Fig. 17. Effect of different fine-tuning hyperparameters. Evaluated on the VTAB-
Specialized : KITTI/Distance task. Other tuning methods are shaded in gray
Table 11. ViT-B/16 pre-trained on supervised ImageNet-21k, fine-tuned with reso-
lution 384 ×384. We also include VPT with image resolution 224 ×224,p= 380, so
the effective image resolution is 384 ×384. For each method and each downstream task
group, we report the average test accuracy score and number of wins in ( ·)com-
pared to Full . “Total params” denotes total parameters needed for all 24 downstream
tasks. Best results among all methods except Full arebolded
ViT-B/16 Fine-tune Total VTAB-1k
(85.8M) Resolution params Natural Specialized Structured
Total # of tasks 7 4 8
(a)Full 384 19.07× 72.57 83.05 50.86
Full 224 19.07 × 75.88 83.36 47.64
(b)Linear 1.01×66.30 (2) 76.77 (0) 27.86 (0)
Mlp-3 384 1.27 ×66.45 (3) 77.77 (0) 38.03 (0)
Partial -1 2.58 ×67.91 (4) 76.94 (0) 37.16 (0)
Sidetune 3.12×47.08 (1) 40.34 (0) 24.18 (0)
(c) Bias 384 1.03 ×70.30 (4) 76.06 (0) 45.35 (1)
Adapter 1.11×69.42 (6) 77.11 (0) 30.62 (0)
(ours)VPT -shallow (p∈ {1,5,10,50,100,200}) 384 1.02 ×75.30 (4) 78.50 (0) 46.56 (2)
VPT -deep (p∈ {1,5,10,50,100,200}) 384 1.19 ×79.37 (6) 82.86 (2) 56.36 (7)
VPT -shallow (p= 380) 224 1.07 ×75.07 (3) 79.03 (0) 46.21 (2)
VPT -deep (p= 380) 224 1.78 ×74.20 (4) 82.30 (2) 54.50 (6)

--- PAGE 25 ---
Visual Prompt Tuning 25
Table 12. Cost analysis using a ViT-B/16 pre-trained on supervised ImageNet-21k. For
each method and each downstream task group, we report the latency ( ms/img ) and
peak GPU memory usage ( GB) at both training and inference time. “Tuned params”
denotes the fraction of learnable parameters needed. “Scope” denotes the tuning scope
of each method. “Extra params” denotes the presence of additional parameters besides
the pre-trained backbone and linear head. All experiments use the same A100 GPU
ViT-B/16 Tuned Scope Extra Train Test
(85.8M) params Input Backbone Head params Latency Memory Latency Memory
(ms/img) (GB) (ms/img) (GB)
(a) Full 100% ✓ ✓ 358.7 11.7 69.7 0.87
(b)Linear 0.09% 148.9 0.9 64.4 0.87
Partial -1 8.35% ✓ 193.2 1.4 66.1 0.87
Mlp-3 1.45% ✓ 164.3 0.9 64.4 0.87
(c)Sidetune 10.09%
✓✓ 164.6 1.2 66.9 0.91
Bias 0.21% 296.9 10.1 65.6 0.87
Adapter (r= 8) 2.12% ✓ 293.4 9.9 68.2 0.87
Adapter (r= 64) 0.36% ✓ 294.4 9.8 68.3 0.87
Adapter (r= 256) 0.17% ✓ 271.4 9.8 68.0 0.87
(ours)VPT -shallow (p= 1) 0.09%
✓ ✓205.9 10.3 68.1 0.88
VPT -deep (p= 1) 0.10% 213.6 10.3 69.4 0.88
VPT -shallow (p= 200) 0.27% 350.6 25.8 138.8 1.84
VPT -deep (p= 200) 2.19% 360.1 25.8 140.8 1.85
protocols, and even outperforms full fine-tuning on 15 out of 19 tasks. Although the
increase of image resolutions doesn’t lead to better full fine-tuning performance in
general, it indeed slightly boosts VPT -deep ’s performance.
Another interesting observation from Tab. 11 is that with 224 fine-tune resolution
and a larger value of p= 380, VPT could achieve similar or better performance com-
pared to Full with 384 resolution, while using the same input sequence length yet
significantly less trainable parameters.
Empirical computational cost. One possible limitation of VPT is the extra
input sequence length for Transformers. In theory the complexity of MSA is quadratic
w.r.t. the input sequence length, but this might not be the case for real-world speed due
to hardware details like lane widths and cache sizes [19]. In Tab. 12 and Fig. 18, we study
the empirical computational cost, i.e., latency, and peak GPU memory usage at both
training and inference times, for all the fine-tuning protocols studied. All experiments
use the same A100 GPU with a batch size 64 for both training and inference. We
can see that the theoretical quadratic scaling w.r.t. sequence length barely happens to
VPT. For instance, doubling the length ( p= 200 vs.m= 198) basically only lead to
2×(instead of 4 ×) inference latency and peak GPU memory w.r.t. full fine-tuning. For
training, the latency would be largely reduced with less number of prompts.
An equivalent implementation of VPT during test time is directly prepend the
parameters to the key and value arrays inside the self-attention module of Trans-
former [48] ( VPT -prefix). While we found that such implementation does not lead
to accuracy improvement on VTAB datasets, it reduces the computation cost during
inference. Figure 19 shows the comparison with different values of p.VPT -prefix re-
duces test-time latency and peak GPU memory with a large margin especially when p
becomes large.

--- PAGE 26 ---
26 M. Jia et al.
10 15 20 25
Peak GPU memory (GB)200250300350Latency (ms / img)
p=1p=50p=100p=200Full fine-tuningTrain
0.8 1.0 1.2 1.4 1.6 1.8
Peak GPU memory (GB)80100120140Latency (ms / img)
p=1p=50p=100p=200Full fine-tuningT est
Fig. 18. Peak GPU memory and latency (ms/img) during both training (left) and
inference time (right). For easy comparison, the gray dashed lines represent latency
and memory of full fine-tuning
0 50 100 150 200
Values of p80100120
Full fine-tuningLatency (ms / img)
VPT-deep
VPT-prefix
0 50 100 150 200
Values of p1.001.251.501.75
Full fine-tuningPeak GPU memory (GB)
VPT-deep
VPT-prefix
Fig. 19. VPT-deep vs. VPT-prefix: peak GPU memory (left) and latency (right) dur-
ing inference time. For easy comparison, the gray dashed lines represent latency and
memory of full fine-tuning
C Further Discussion
VPT vs. Adversarial Reprogramming (AR). The differences are: (1) the
number of learnt parameters injected in the input space in AR literature [20] is nearly
20 times larger than ours (264k vs. 13k). VPT is significantly more parameter-efficient;
(2) AR has shown its effectiveness in ConvNet, while VPT can be applied to broader
architectures, including ViT, Swin. Furthermore, VPT is more general with the option
of diving into deeper layers of pre-trained backbone (Fig. 2), whereas AR strictly
applies to the first input layer of ConvNets. (3) another distinction is that our setting
update both prompts and classification head, while AR [20] directly use the pre-trained
classification head. Our setup is more general and could be applied to models with a
broader range of pre-training objectives ( e.g., MAE [30], which does not include a
pre-trained classification head) and broader vision tasks ( e.g., segmentation).
Visual prompt vs. textual prompt. Our paper also discover discrepancies be-
tween visual and textual prompts: we show that VPT could even outperform full-model
fine-tuning on 20 out of 24 cases, which is in contract to the NLP’s related work [45].
We also found that random initialized prompts works better in Fig. 13, and prompts at
earlier layers matters more (Figs. 7 and 14), which are also different from observation
on the NLP side [45,51]. These discrepancies indicate that visual prompting might be
fundamentally different from text prompts thus in need of further investigation.

--- PAGE 27 ---
Visual Prompt Tuning 27
101
10060708090
VPT vs. Linear
CUB
101
10060708090
VPT vs. Adapter
CUB
101
10060708090
VPT vs. Bias
CUB
101
100607080
NABirds
101
100607080
NABirds
101
100607080
NABirds
101
100708090100
Flowers102
101
100708090100
Flowers102
101
100708090100
Flowers102
101
100808590
StanfordDogs
101
100808590
StanfordDogs
101
100808590
StanfordDogs
101
10020406080
StanfordCars
101
10020406080
StanfordCars
101
10020406080
StanfordCarsT est accuracy (%)
Fraction of downstream training dataset (in log scale)VPT-Deep VPT-Shallow Full Linear Adapter Bias
Fig. 20. Effect of downstream data size, for each of FGVC tasks. The size of markers
are proportional to the percentage of tunable parameters in log scale
D Supplementary Results
Numerical results of Table 1. Tabs. 13 and 14 present per-task results for 24
classification tasks evaluated in Tab. 1.
Per-task results on training data ablations. Fig. 20 presents the per-task
results for five FGVC datasets. We observe a similar trend in Fig. 3: while all parameter-
efficient methods outperform full fine-tuning in small-to-medium data regime, VPT -
deep consistently surpasses Full across data scales for five FGVC tasks.
More t-SNE visualizations. In Fig. 21, We presents more t-SNE visualizations,
similar to Fig. 9, for all VTAB datasets with less than or equal to 20 target classes.

--- PAGE 28 ---
28 M. Jia et al.
Patch Camelyon (VTAB -Specialized) Retinopathy (VTAB -Specialized) Clevr /distance (VTAB -Structured)
dSprites /location (VTAB -Specialized) dSprites /orientation (VTAB -Specialized) SmallNORB /azimuth (VTAB -Structured)
SmallNORB /elevation (VTAB -Specialized) DMLab (VTAB -Specialized) KITTI/ distance (VTAB -Structured)
Fig. 21. More t-SNE visualization of the final [CLS] embedding xNof more VTAB
tasks. We include tasks that have less or equal to 20 target classes for visualization

--- PAGE 29 ---
Visual Prompt Tuning 29
Table 13. Per-task fine-tuning results from Tab. 1 for VTAB-1k with a pre-trained
ViT-B/16
CIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Mean
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Mean
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/location
dSprites/orientation
SmallNORB/azimuth
SmallNORB/elevation
Mean
(a) Full 68.9 87.7 64.3 97.2 86.9 87.4 38.8 75.88 79.7 95.7 84.2 73.9 83.36 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 47.64
Head-oriented
(a)Linear 63.4 85.0 63.2 97.0 86.3 36.6 51.0 68.93 (1) 78.5 87.5 68.6 74.0 77.16 (1) 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 26.84 (0)
Partial -1 66.8 85.9 62.5 97.3 85.5 37.6 50.6 69.44 (2) 78.6 89.8 72.5 73.3 78.53 (0) 41.5 34.3 33.9 61.0 31.3 32.8 16.3 22.4 34.17 (0)
Mlp-2 63.2 84.8 60.5 97.6 85.9 34.1 47.8 67.70 (2) 74.3 88.8 67.1 73.2 75.86 (0) 45.2 31.6 31.8 55.7 30.9 24.6 16.6 23.3 32.47 (0)
Mlp-3 63.8 84.7 62.3 97.4 84.7 32.5 49.2 67.80 (2) 77.0 88.0 70.2 56.1 72.83 (0) 47.8 32.8 32.3 58.1 12.9 21.2 15.2 24.8 30.62 (0)
Mlp-5 59.3 84.4 59.9 96.1 84.4 30.9 46.8 65.98 (1) 73.7 87.2 64.8 71.5 74.31 (0) 50.8 32.3 31.5 56.4 7.5 20.8 14.4 20.4 29.23 (0)
Mlp-9 53.1 80.5 53.9 95.1 82.6 24.4 43.7 61.90 (1) 78.5 83.0 60.2 72.3 73.49 (0) 47.5 27.9 28.9 54.0 6.2 17.7 10.8 16.2 26.15 (0)
Backbone-oriented
(b)Sidetune 60.7 60.8 53.6 95.5 66.7 34.9 35.3 58.21 (0) 58.5 87.7 65.2 61.0 68.12 (0) 27.6 22.6 31.3 51.7 8.2 14.4 9.8 21.8 23.41 (0)
Bias 72.8 87.0 59.2 97.5 85.3 59.9 51.4 73.30 (3) 78.7 91.6 72.9 69.8 78.25 (0) 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 44.09 (2)
Adapter -256 74.1 86.1 63.2 97.7 87.0 34.6 50.8 70.50 (4) 76.3 88.0 73.1 70.5 76.98 (0) 45.7 37.4 31.2 53.2 30.3 25.4 13.8 22.1 32.39 (0)
Adapter -64 74.2 85.8 62.7 97.6 87.2 36.3 50.9 70.65 (4) 76.3 87.5 73.7 70.9 77.10 (0) 42.9 39.9 30.4 54.5 31.9 25.6 13.5 21.4 32.51 (0)
Adapter -8 74.2 85.7 62.7 97.8 87.2 36.4 50.7 70.67 (4) 76.9 89.2 73.5 71.6 77.80 (0) 45.2 41.8 31.1 56.4 30.4 24.6 13.2 22.0 33.09 (0)
Visual-Prompt Tuning
(ours)VPT -shallow 77.7 86.9 62.6 97.5 87.3 74.5 51.2 76.81 (4) 78.2 92.0 75.6 72.9 79.66 (0) 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 46.98 (4)
Prompt length ( p) 100 5 1 200 50 200 1 79.4 5 50 50 10 28.7 100 200 100 100 100 100 200 200 137.5
Tuned / Total (%) 0.18 0.10 0.04 0.27 0.08 0.19 0.36 0.17 0.01 0.05 0.09 0.01 0.04 0.10 0.18 0.09 0.09 0.10 0.10 0.19 0.19 0.13
VPT -deep 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.48 (6) 81.8 96.1 83.4 68.4 82.43 (2) 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 54.98 (8)
Prompt length ( p) 10 10 10 1 1 50 5 12.4 100 100 10 1 52.8 50 200 100 50 10 50 200 200 107.5
Tuned / Total (%) 0.20 0.20 0.15 0.10 0.04 0.54 0.41 0.23 1.06 1.07 0.15 0.02 0.57 0.54 2.11 1.07 0.54 0.12 0.55 2.12 2.11 1.14
Table 14. Per-task fine-tuning results from Tab. 1 for five FGVC tasks, with a pre-
trained ViT-B/16
CUB-200-2011 NABirds Oxford Flowers Stanford Dogs Stanford Cars Mean
(a) Full 87.3 82.7 98.8 89.4 84.5 88.54
Head-oriented
(a)Linear 85.3 75.9 97.9 86.2 51.3 79.32 (0)
Partial -1 85.6 77.8 98.2 85.5 66.2 82.63 (0)
Mlp-2 85.7 77.2 98.2 85.4 54.9 80.28 (0)
Mlp-3 85.1 77.3 97.9 84.9 53.8 79.80 (0)
Mlp-5 84.2 76.7 97.6 84.8 50.2 78.71 (0)
Mlp-9 83.2 76.0 96.2 83.7 47.6 77.31 (0)
Backbone-oriented
(b)Sidetune 84.7 75.8 96.9 85.8 48.6 78.35 (0)
Bias 88.4 84.2 98.8 91.2 79.4 88.41 (3)
Adapter -256 87.2 84.3 98.5 89.9 68.6 85.70 (2)
Adapter -64 87.1 84.3 98.5 89.8 68.6 85.67 (2)
Adapter -8 87.3 84.3 98.4 88.8 68.4 85.46 (1)
Visual-Prompt Tuning
(ours)VPT -shallow 86.7 78.8 98.4 90.7 68.7 84.62 (1)
Prompt length ( p) 100 50 100 100 100 90
Tuned / Total (%) 0.31 0.54 0.23 0.20 0.26 0.31
VPT -deep 88.5 84.2 99.0 90.2 83.6 89.11 (4)
Prompt length ( p) 10 50 5 100 200 73
Tuned / Total (%) 0.29 1.02 0.14 1.17 2.27 0.98

--- PAGE 30 ---
30 M. Jia et al.
References
1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint
arXiv:1607.06450 (2016) 4
2. Bahng, H., Jahanian, A., Sankaranarayanan, S., Isola, P.: Visual prompting: Mod-
ifying pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274
(2022) 3
3. Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transform-
ers. In: ICLR (2022) 3
4. Beattie, C., Leibo, J.Z., Teplyashin, D., Ward, T., Wainwright, M., K¨ uttler, H.,
Lefrancq, A., Green, S., Vald´ es, V., Sadik, A., et al.: Deepmind lab. arXiv preprint
arXiv:1612.03801 (2016) 16
5. Ben Zaken, E., Goldberg, Y., Ravfogel, S.: BitFit: Simple parameter-efficient
fine-tuning for transformer-based masked language-models. In: Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers). pp. 1–9. Association for Computational Linguistics,
Dublin, Ireland (May 2022). https://doi.org/10.18653/v1/2022.acl-short.1, https:
//aclanthology.org/2022.acl-short.1 3, 6
6. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities
and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021) 1
7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,
Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are
few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin,
H. (eds.) NeurIPS. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020) 1, 3
8. Cai, H., Gan, C., Zhu, L., Han, S.: Tinytl: Reduce memory, not parameters for
efficient on-device learning. NeurIPS 33, 11285–11297 (2020) 2, 3, 6
9. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-
to-end object detection with transformers. In: ECCV. pp. 213–229. Springer (2020)
3
10. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder
with atrous separable convolution for semantic image segmentation. In: ECCV.
pp. 801–818 (2018) 13
11. Chen*, X., Xie*, S., He, K.: An empirical study of training self-supervised vision
transformers. In: ICCV (2021) 2, 3, 13, 15, 16
12. Cheng, G., Han, J., Lu, X.: Remote sensing image scene classification: Benchmark
and state of the art. Proceedings of the IEEE (2017) 16
13. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., , Vedaldi, A.: Describing textures
in the wild. In: CVPR (2014) 16
14. Conder, J., Jefferson, J., Jawed, K., Nejati, A., Sagar, M., et al.: Efficient transfer
learning for visual tasks via continuous optimization of prompts. In: International
Conference on Image Analysis and Processing. pp. 297–309. Springer (2022) 3
15. Contributors, M.: MMSegmentation: Openmmlab semantic segmentation toolbox
and benchmark. https://github.com/open-mmlab/mmsegmentation (2020) 18
16. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: CVPR (2009) 6, 16

--- PAGE 31 ---
Visual Prompt Tuning 31
17. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota
(Jun 2019) 1, 3
18. Doersch, C., Gupta, A., Zisserman, A.: Crosstransformers: spatially-aware few-shot
transfer. NeurIPS 33, 21981–21993 (2020) 3
19. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: ICLR (2020) 1, 3, 4,
6, 16, 23, 25
20. Elsayed, G.F., Goodfellow, I., Sohl-Dickstein, J.: Adversarial reprogramming of
neural networks. In: ICLR (2019) 14, 26
21. Feichtenhofer, C., Fan, H., Li, Y., He, K.: Masked autoencoders as spatiotemporal
learners. arXiv preprint arXiv:2205.09113 (2022) 3
22. Ge, C., Huang, R., Xie, M., Lai, Z., Song, S., Li, S., Huang, G.: Domain adaptation
via prompt learning. arXiv preprint arXiv:2202.06687 (2022) 3
23. Gebru, T., Krause, J., Wang, Y., Chen, D., Deng, J., Fei-Fei, L.: Fine-grained car
detection for visual census estimation. In: AAAI (2017) 6, 16
24. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti
dataset. International Journal of Robotics Research (2013) 16
25. Girdhar, R., Carreira, J., Doersch, C., Zisserman, A.: Video action transformer
network. In: CVPR. pp. 244–253 (2019) 3
26. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward
neural networks. In: AISTATS (2010) 15
27. Goyal, P., Doll´ ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tul-
loch, A., Jia, Y., He, K.: Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 (2017) 15
28. Guo, D., Rush, A., Kim, Y.: Parameter-efficient transfer learning with diff pruning.
In: Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). pp. 4884–4896. Association for Computational
Linguistics, Online (Aug 2021) 3
29. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., Neubig, G.: Towards a unified view
of parameter-efficient transfer learning. In: ICLR (2022) 3
30. He, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P., Girshick, R.: Masked autoencoders are
scalable vision learners. In: CVPR. pp. 16000–16009 (2022) 3, 6, 13, 15, 16, 26
31. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. pp. 770–778 (2016) 1, 5, 14, 16
32. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing 12(7), 2217–
2226 (2019) 11
33. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing (2019) 16
34. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-
mundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp.
In: ICML. pp. 2790–2799. PMLR (2019) 3, 6, 15

--- PAGE 32 ---
32 M. Jia et al.
35. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021) 3
36. Jia, M., Wu, Z., Reiter, A., Cardie, C., Belongie, S., Lim, S.N.: Exploring visual
engagement signals for representation learning. In: ICCV (2021) 2
37. Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: How can we know what language models
know? Transactions of the Association for Computational Linguistics 8, 423–438
(2020) 3
38. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C.,
Girshick, R.: Clevr: A diagnostic dataset for compositional language and elemen-
tary visual reasoning. In: CVPR (2017) 11, 16
39. Ju, C., Han, T., Zheng, K., Zhang, Y., Xie, W.: Prompting visual-language models
for efficient video understanding. arXiv preprint arXiv:2112.04478 (2021) 3
40. Kaggle, EyePacs: Kaggle diabetic retinopathy detection (July 2015) 16
41. Khosla, A., Jayadevaprakash, N., Yao, B., Fei-Fei, L.: Novel dataset for fine-grained
image categorization. In: First Workshop on Fine-Grained Visual Categorization,
IEEE Conference on Computer Vision and Pattern Recognition. Colorado Springs,
CO (June 2011) 6, 16
42. Krizhevsky, A.: One weird trick for parallelizing convolutional neural networks.
arXiv preprint arXiv:1404.5997 (2014) 15
43. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009) 16
44. LeCun, Y., Huang, F.J., Bottou, L.: Learning methods for generic object recogni-
tion with invariance to pose and lighting. In: CVPR (2004) 16
45. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient
prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing. pp. 3045–3059. Association for Computational
Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021) 2, 3, 7, 19,
21, 26
46. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-
anov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. In: Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics. pp.
7871–7880 (2020) 1
47. Li, F.F., Fergus, R., Perona, P.: One-shot learning of object categories. IEEE
TPAMI (2006) 16
48. Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for generation.
In: Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). pp. 4582–4597. Association for Computational
Linguistics, Online (Aug 2021) 2, 3, 25
49. Li, Y., Xie, S., Chen, X., Dollar, P., He, K., Girshick, R.: Benchmarking detection
transfer learning with vision transformers. arXiv preprint arXiv:2111.11429 (2021)
3
50. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural language processing.
arXiv preprint arXiv:2107.13586 (2021) 2, 3
51. Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can
be comparable to fine-tuning universally across scales and tasks. arXiv preprint
arXiv:2110.07602 (2021) 2, 3, 26

--- PAGE 33 ---
Visual Prompt Tuning 33
52. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin trans-
former: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 3,
5, 6, 9, 16
53. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for
the 2020s. CVPR (2022) 14, 16
54. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 15
55. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine
learning research 9(11) (2008) 11
56. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe,
A., Van Der Maaten, L.: Exploring the limits of weakly supervised pretraining. In:
ECCV (2018) 2, 15
57. Matthey, L., Higgins, I., Hassabis, D., Lerchner, A.: dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/ (2017) 16
58. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits
in natural images with unsupervised feature learning. In: NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011 (2011) 11, 16
59. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number
of classes. In: 2008 Sixth Indian Conference on Computer Vision, Graphics & Image
Processing. pp. 722–729. IEEE (2008) 6, 16
60. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving
jigsaw puzzles. In: ECCV. pp. 69–84. Springer (2016) 6
61. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.V.: Cats and dogs. In: CVPR
(2012) 16
62. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in PyTorch. In:
NeurIPS Autodiff Workshop (2017) 15
63. Pfeiffer, J., Kamath, A., R¨ uckl´ e, A., Cho, K., Gurevych, I.: Adapterfusion: Non-
destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247
(2020) 3, 6, 15
64. Pfeiffer, J., R¨ uckl´ e, A., Poth, C., Kamath, A., Vuli´ c, I., Ruder, S., Cho, K.,
Gurevych, I.: Adapterhub: A framework for adapting transformers. In: Proceed-
ings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP 2020): Systems Demonstrations. pp. 46–54. Association for Computa-
tional Linguistics, Online (2020) 3, 6, 15
65. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International Conference on Machine Learning.
pp. 8748–8763. PMLR (2021) 1, 3
66. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research 21(140), 1–67 (2020) 1, 3
67. Rebuffi, S.A., Bilen, H., Vedaldi, A.: Learning multiple visual domains with residual
adapters. NeurIPS 30(2017) 3
68. Rebuffi, S.A., Bilen, H., Vedaldi, A.: Efficient parametrization of multi-domain
deep neural networks. In: CVPR. pp. 8119–8127 (2018) 2
69. Sandler, M., Zhmoginov, A., Vladymyrov, M., Jackson, A.: Fine-tuning image
transformers using learnable memory. In: CVPR. pp. 12155–12164 (2022) 3
70. Shin, T., Razeghi, Y., Logan IV, R.L., Wallace, E., Singh, S.: Autoprompt: Eliciting
knowledge from language models with automatically generated prompts. arXiv
preprint arXiv:2010.15980 (2020) 3

--- PAGE 34 ---
34 M. Jia et al.
71. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for seman-
tic segmentation. In: CVPR. pp. 7262–7272 (2021) 3
72. Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeirotis, P., Perona,
P., Belongie, S.: Building a bird recognition app and large scale dataset with citizen
scientists: The fine print in fine-grained dataset collection. In: CVPR. pp. 595–604
(2015) 6, 16
73. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
 L., Polosukhin, I.: Attention is all you need. NeurIPS 30(2017) 3
74. Veeling, B.S., Linmans, J., Winkens, J., Cohen, T., Welling, M.: Rotation equiv-
ariant cnns for digital pathology. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention (2018) 16
75. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd birds-
200-2011 dataset. Tech. Rep. CNS-TR-2011-001, California Institute of Technology
(2011) 6, 16
76. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy,
O., Bowman, S.: Superglue: A stickier benchmark for general-purpose language
understanding systems. NeurIPS 32(2019) 3
77. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: GLUE: A multi-
task benchmark and analysis platform for natural language understanding. In:
Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Inter-
preting Neural Networks for NLP. pp. 353–355. Association for Computational Lin-
guistics, Brussels, Belgium (Nov 2018). https://doi.org/10.18653/v1/W18-5446,
https://aclanthology.org/W18-5446 3
78. Wang, H., Zhu, Y., Adam, H., Yuille, A., Chen, L.C.: Max-deeplab: End-to-end
panoptic segmentation with mask transformers. In: CVPR. pp. 5463–5474 (2021)
3
79. Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.G., Zhou, L.,
Yuan, L.: Bevt: Bert pretraining of video transformers. In: CVPR. pp. 14733–14743
(2022) 3
80. Wang, Z., Zhang, Z., Lee, C.Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy,
J., Pfister, T.: Learning to prompt for continual learning. In: CVPR. pp. 139–149
(2022) 3
81. Welch, B.L.: The generalization of ‘student’s’problem when several different pop-
ulation varlances are involved. Biometrika 34(1-2), 28–35 (1947) 22
82. Wilcoxon, F.: Individual comparisons by ranking methods. In: Breakthroughs in
statistics, pp. 196–202. Springer (1992) 22
83. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale
scene recognition from abbey to zoo. In: CVPR (2010) 16
84. Yao, Y., Zhang, A., Zhang, Z., Liu, Z., Chua, T.S., Sun, M.: Cpt: Colorful prompt
tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797
(2021) 3
85. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in
deep neural networks? NeurIPS 27(2014) 6
86. Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djo-
longa, J., Pinto, A.S., Neumann, M., Dosovitskiy, A., et al.: A large-scale study of
representation learning with the visual task adaptation benchmark. arXiv preprint
arXiv:1910.04867 (2019) 6, 7, 16
87. Zhang, J.O., Sax, A., Zamir, A., Guibas, L., Malik, J.: Side-tuning: a baseline for
network adaptation via additive side networks. In: ECCV. pp. 698–714. Springer
(2020) 2, 3, 6

--- PAGE 35 ---
Visual Prompt Tuning 35
88. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. pp. 649–
666. Springer (2016) 6
89. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,
Torr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence
perspective with transformers. In: CVPR. pp. 6881–6890 (2021) 3, 12, 13, 18
90. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:
Semantic understanding of scenes through the ade20k dataset. IJCV 127(3), 302–
321 (2019) 12, 13, 18
91. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. arXiv preprint arXiv:2109.01134 (2021) 3
92. Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q.: A
comprehensive survey on transfer learning. Proceedings of the IEEE 109(1), 43–
76 (2020) 3
