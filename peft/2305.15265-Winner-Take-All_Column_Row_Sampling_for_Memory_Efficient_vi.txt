# 2305.15265.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.15265.pdf
# Kích thước file: 1930029 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Lấy mẫu Cột-Hàng Theo Kiểu Người Thắng Nhận Tất Cả để Thích Ứng Mô Hình Ngôn Ngữ Tiết Kiệm Bộ Nhớ
Zirui Liu∗1, Guanchu Wang∗1, Shaochen Zhong1, Zhaozhuo Xu1, Daochen Zha1, Ruixiang
Tang1, Zhimeng Jiang2, Kaixiong Zhou1, Vipin Chaudhary3, Shuai Xu3, và Xia Hu1
1Khoa Khoa học Máy tính, Đại học Rice
2Khoa Khoa học Máy tính, Đại học Texas A&M
3Khoa Khoa học Máy tính và Dữ liệu, Đại học Case Western Reserve
{Zirui.Liu, Guanchu.Wang, Shaochen.Zhong, Zhaozhuo.Xu, Daochen.Zha,
Ruixiang.Tang, Kaixiong.Zhou, Xia.Hu}@rice.edu ,zhimengj@tamu.edu ,{vxc204,
sxx214}@case.edu
Tóm tắt
Với sự tăng trưởng nhanh chóng về kích thước mô hình, việc tinh chỉnh các mô hình ngôn ngữ được tiền huấn luyện lớn ngày càng trở nên khó khăn do việc sử dụng bộ nhớ rộng lớn. Các nghiên cứu trước thường tập trung vào việc giảm số lượng tham số có thể huấn luyện trong mạng. Mặc dù các tham số mô hình có góp phần vào việc sử dụng bộ nhớ, nhưng nút cổ chai bộ nhớ chính trong quá trình huấn luyện phát sinh từ việc lưu trữ các bản đồ đặc trưng, còn được gọi là kích hoạt, vì chúng rất quan trọng cho việc tính toán gradient. Đáng chú ý, các mạng nơ-ron thường được huấn luyện bằng phương pháp gradient descent ngẫu nhiên. Chúng tôi lập luận rằng trong tối ưu hóa ngẫu nhiên, các mô hình có thể xử lý gradient nhiễu miễn là bộ ước lượng gradient không thiên lệch với phương sai hợp lý. Theo động lực này, chúng tôi đề xuất một họ bộ ước lượng không thiên lệch mới được gọi là WTA-CRS, cho phép nhân ma trận với phương sai giảm, chỉ yêu cầu lưu trữ các kích hoạt được lấy mẫu con để tính toán gradient. Nghiên cứu của chúng tôi cung cấp cả bằng chứng lý thuyết và thực nghiệm rằng, trong bối cảnh tinh chỉnh transformer, các bộ ước lượng được đề xuất của chúng tôi thể hiện phương sai thấp hơn so với các bộ ước lượng hiện có. Bằng cách thay thế phép toán tuyến tính bằng phép toán xấp xỉ của chúng tôi trong transformer, chúng tôi có thể đạt được giảm bộ nhớ đỉnh lên đến 2.7× với hầu như không có sự sụt giảm độ chính xác và cho phép kích thước lô lớn hơn lên đến 6.4×. Dưới cùng phần cứng, WTA-CRS cho phép hiệu suất tác vụ xuôi dòng tốt hơn bằng cách áp dụng các mô hình lớn hơn và/hoặc tốc độ huấn luyện nhanh hơn với kích thước lô lớn hơn. Mã nguồn có sẵn tại https://github.com/zirui-ray-liu/WTACRS/.
1 Giới thiệu
Các mô hình ngôn ngữ được tiền huấn luyện (LM) với kiến trúc transformer đã đạt được thành công đáng kể trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) (Vaswani et al., 2017; Devlin et al., 2018; Chuang et al., 2023; Raffel et al., 2020b; Brown et al., 2020; Yang et al., 2023; Zha et al., 2023). Cụ thể, các mô hình này được huấn luyện trên kho dữ liệu văn bản khổng lồ để thu được biểu diễn mục đích chung, sau đó được thích ứng với một nhiệm vụ cụ thể bằng cách tinh chỉnh trên dữ liệu đặc thù nhiệm vụ. Trong các nghiên cứu gần đây, đã được chứng minh thuyết phục rằng việc tăng đáng kể số lượng tham số trong các LM được tiền huấn luyện dẫn đến những cải thiện đáng kể về hiệu suất (Kaplan et al., 2020). Kết quả là, hiện tại có nhu cầu cấp thiết phải thích ứng hiệu quả các mô hình này, được trang bị các tham số quy mô tỷ, với nhiều loại tác vụ khác nhau.
∗Đóng góp bằng nhau. Thứ tự tác giả được xác định bằng cách tung đồng xu.
1arXiv:2305.15265v4  [cs.LG]  7 Nov 2024

--- TRANG 2 ---
Tuy nhiên, một sự chênh lệch đáng kể tồn tại giữa yêu cầu bộ nhớ của các LM được tiền huấn luyện và dung lượng của phần cứng hiện tại, đặc biệt là GPU. Ví dụ, ngay cả một GPU có bộ nhớ 24GB cũng không thể chứa quá trình tinh chỉnh của mô hình T5-3B (Raffel et al., 2020b) với kích thước lô bằng một, mô hình này có ba tỷ tham số. Không có các kỹ thuật bổ sung, việc cố gắng tinh chỉnh các LM quy mô tỷ trên một GPU duy nhất là không thể. Mặc dù việc tinh chỉnh song song mô hình là khả thi, nhưng phần lớn thời gian, chúng ta không thể chịu được chi phí mua nhiều GPU hoặc chi phí giao tiếp liên quan. Để đảm bảo việc triển khai mô hình ngôn ngữ một cách mượt mà trong quá trình tinh chỉnh, điều quan trọng là phải thích ứng chúng để hoạt động trên một GPU duy nhất.

Để giải quyết vấn đề này, một số phương pháp tinh chỉnh hiệu quả tham số được đề xuất (Lester et al., 2021; Sung et al., 2022; Li and Liang, 2021; Zaken et al., 2021; Hu et al., 2021; Karimi Mahabadi et al., 2021; Houlsby et al., 2019). Cụ thể, adapter (Houlsby et al., 2019; Karimi Mahabadi et al., 2021) chèn một mô-đun nhỏ vào các khối transformer và chỉ cập nhật nó trong khi giữ các tham số khác cố định. Tương tự, prompt tuning (Lester et al., 2021) giới thiệu một vector nhỏ được nối với các embedding đầu vào và được cập nhật trong quá trình tinh chỉnh. LoRA (Hu et al., 2021) tiêm các ma trận phân rã rank có thể huấn luyện vào khối transformer, cập nhật chúng trong khi đóng băng các ma trận khác. Các phương pháp tinh chỉnh hiệu quả tham số chủ yếu giảm bộ nhớ được sử dụng bởi trạng thái bộ tối ưu (Kingma and Ba, 2014; Hu et al., 2021). Mặc dù các trạng thái bộ tối ưu góp phần vào dấu chân bộ nhớ, việc lưu trữ kích hoạt (hoặc bản đồ đặc trưng) là nút cổ chai bộ nhớ chính trong quá trình huấn luyện (thường >70%) (Chen et al., 2021c; Jain et al., 2020; Kirisame et al., 2020; Andoorveedu et al., 2022b). Do đó, các phương pháp hiệu quả tham số thường không giảm việc sử dụng bộ nhớ nhiều (Sung et al., 2022; Andoorveedu et al., 2022b).

[Hình 1. Đánh đổi độ chính xác-bộ nhớ của WTA-CRS và các phương pháp tinh chỉnh hiệu quả bộ nhớ khác. Trừ khi được nêu đặc biệt, chúng tôi sử dụng T5-Large trong hình.]

Song song với điều này, chúng ta có thể giảm nút cổ chai bộ nhớ chính bằng cách giảm việc lưu trữ kích hoạt trong tinh chỉnh. Vì các mô hình dựa trên transformer chủ yếu được xây dựng dựa trên lớp tuyến tính, một hướng ít được khám phá là thay thế phép toán nhân ma trận tốn kém bằng các ước lượng hiệu quả bộ nhớ của nó sử dụng lấy mẫu cột-hàng (CRS) (Adelman et al., 2021; Drineas et al., 2006). Ý tưởng chính của CRS là lấy mẫu con các tensor vào không gian chiều thấp và thực hiện các phép toán gốc tại đây. Cụ thể, đối với phép toán tuyến tính giữa hai ma trận A∈Rn×m và B∈Rm×q (trong bối cảnh học máy, A thường là kích hoạt), chúng ta đầu tiên lấy mẫu k (k < m) cặp cột-hàng theo một phân phối được định nghĩa trước. Sau đó chúng ta thu được A′∈Rn×k và B′∈Rk×q (k < m) bằng cách chọn k cột của A và các hàng tương ứng của B theo các cặp cột-hàng được lấy mẫu (Drineas et al., 2006). Cuối cùng, chúng ta ước lượng AB≈A′B′. Bằng cách này, chúng ta chỉ cần lưu trữ ma trận con A′ và B′ trong bộ nhớ GPU để thực hiện tính toán. Hơn nữa, việc huấn luyện/tinh chỉnh các mô hình dựa trên transformer được thực hiện với bộ tối ưu ngẫu nhiên bậc nhất, ví dụ như Adam (Kingma and Ba, 2014). Trong tối ưu hóa ngẫu nhiên, các mô hình có thể làm việc với gradient nhiễu, miễn là bộ ước lượng gradient không thiên lệch và có phương sai hợp lý. Với quan điểm này, chúng tôi hỏi: tại sao phải dành tài nguyên để thu được gradient chính xác khi chúng ta đang sử dụng tối ưu hóa ngẫu nhiên?

Được thúc đẩy bởi điều này, chúng tôi tập trung vào việc thu được gradient không thiên lệch một cách tiết kiệm với phép nhân ma trận xấp xỉ.

--- TRANG 3 ---
Phương pháp xấp xỉ giảm việc sử dụng bộ nhớ với chi phí là đầu ra có phương sai. Do đó tự nhiên tồn tại một sự đánh đổi giữa độ chính xác và bộ nhớ. Thách thức chính là làm thế nào để tích hợp phép nhân ma trận xấp xỉ vào transformer với phương sai gradient tối thiểu. Trong bài báo này, chúng tôi đề xuất một họ bộ ước lượng không thiên lệch mới cho phép nhân ma trận với phương sai giảm, được gọi là Lấy mẫu Cột-Hàng Theo Kiểu Người Thắng Nhận Tất Cả (WTA-CRS). So với CRS, WTA-CRS giảm phương sai của bộ ước lượng bằng cách tập trung nhiều hơn vào các vùng xác suất cao của phân phối lấy mẫu. Hơn nữa, WTA-CRS có thể phục vụ như một sự thay thế trực tiếp cho phép toán tuyến tính trong transformer, cung cấp gradient trọng số không thiên lệch với việc sử dụng bộ nhớ giảm. Như được hiển thị trong Hình 1, phương pháp của chúng tôi đạt được sự đánh đổi độ chính xác-bộ nhớ tốt hơn so với các phương pháp tinh chỉnh hiệu quả bộ nhớ hiện đại, ví dụ như LST (Sung et al., 2022) và LoRA (Hu et al., 2021). Hơn nữa, vì WTA-CRS được thực hiện ở cấp độ phép toán, nó trực giao với hầu hết các phương pháp tinh chỉnh hiệu quả tham số hiện có. Các đóng góp của chúng tôi được nêu bật như sau:

• Chúng tôi thiết kế một họ bộ ước lượng không thiên lệch mới cho phép nhân ma trận với phương sai giảm. Chúng tôi xác minh lý thuyết và thực nghiệm rằng nó có phương sai nhỏ hơn so với bộ ước lượng đã được thiết lập trong bối cảnh tinh chỉnh transformer.

• Bằng cách thay thế phép toán tuyến tính bằng WTA-CRS trong transformer, chúng tôi có thể đạt được giảm bộ nhớ đỉnh lên đến 2.7× với hầu như không có sự sụt giảm độ chính xác, và cho phép kích thước lô lớn hơn lên đến 6.4×. Như được hiển thị trong Hình 1, WTA-CRS nổi bật như một giải pháp đặc biệt có khả năng tinh chỉnh T5-3B chỉ với ngân sách bộ nhớ GPU 40GB, với ba tỷ tham số. Do đó, chúng tôi đạt được những tiến bộ đáng kể trong việc thích ứng các LM cho các tác vụ xuôi dòng.

• Chúng tôi triển khai WTA-CRS như một tiện ích mở rộng sẵn sàng sử dụng cho Pytorch với API dễ sử dụng cũng có thể được kết hợp với các kỹ thuật tiết kiệm bộ nhớ khác.

2 Bối cảnh và Cơ sở

Trong phần này, chúng tôi đầu tiên phân tích việc sử dụng bộ nhớ của transformer. Sau đó chúng tôi giới thiệu bối cảnh về phép nhân ma trận xấp xỉ.

2.1 Việc Sử dụng Bộ nhớ của Transformer

[Hình 2. Phân tích việc sử dụng bộ nhớ GPU để tinh chỉnh T5 (Wang et al., 2018a), trong đó kích thước lô B là 64 và độ dài tuần tự S là 128 hoặc 256.]

Trong mỗi bước huấn luyện của lan truyền ngược, có chính xác hai giai đoạn, tức là một giai đoạn tiến và một giai đoạn lùi. Các mô hình dựa trên Transformer chủ yếu được xây dựng dựa trên phép toán tuyến tính, có thể được viết như:

Giai đoạn Tiến Z=GEMM(H,W), (1a)
Giai đoạn Lùi ∇H=GEMM(∇Z,W⊤), (1b)
∇W=GEMM(H⊤,∇Z), (1c)

trong đó GEMM(·,·) là phép toán Nhân Ma trận Tổng quát, H và Z là kích hoạt (hoặc bản đồ đặc trưng đầu vào) và bản đồ đặc trưng đầu ra, tương ứng. W là trọng số của lớp tuyến tính. ∇H, ∇W, và ∇Z là gradient của H, W, và Z, tương ứng. Từ Phương trình (1c), các kích hoạt H được sử dụng trong giai đoạn lùi. Trong framework học sâu thường dùng (Abadi et al., 2016; Paszke et al., 2019), nó yêu cầu lưu trữ H trong bộ nhớ GPU trong giai đoạn tiến, để tính toán gradient trọng số ∇W trong giai đoạn lùi.

Các nghiên cứu trước cho thấy rằng mặc dù các tham số mô hình góp phần vào dấu chân bộ nhớ, kích hoạt (ví dụ, lưu trữ H) là nút cổ chai bộ nhớ chính trong quá trình huấn luyện (Chen et al., 2021c; Jain et al., 2020; Kirisame et al., 2020; Andoorveedu et al., 2022b). Để có cảm nhận về quy mô,

--- TRANG 4 ---
chúng tôi cho thấy trong Hình 2 rằng đối với các mô hình transformer phổ biến như T5, kích hoạt có thể chiếm khoảng 73∼88% tổng bộ nhớ, tùy thuộc vào kích thước lô B và độ dài tuần tự S.

2.2 GEMM Xấp xỉ Với Lấy mẫu

Cho X∈Rn×m, Y∈Rm×q là hai ma trận. Mục tiêu là ước lượng hiệu quả tích ma trận XY. Phân tích Giá trị Đơn (SVD) cho ra ước lượng rank thấp tối ưu có thể chứng minh được của XY (Adelman et al., 2021). Tuy nhiên, SVD gần như tốn kém bằng chính tích ma trận. Thay vào đó, thuật toán lấy mẫu được đề xuất để xấp xỉ tích ma trận XY bằng cách lấy mẫu k cột của X và các hàng tương ứng của Y để tạo thành các ma trận nhỏ hơn, sau đó được nhân như thường lệ (Drineas et al., 2006; Drineas and Kannan, 2001):

GEMM(X,Y) = ∑(i=1 to m) X:,i Yi,: ≈ ∑(t=1 to k) (1/kpit) X:,it Yit,: = X′Y′, (2)

trong đó X:,i ∈ Rn×1 và Yi,: ∈ R1×q là cột thứ i và hàng thứ i của X và Y, tương ứng. Trong bài báo này, chúng tôi gọi (X:,i, Yi,:) là cặp cột-hàng thứ i. k là số lượng cặp được lấy mẫu (1≤k≤m). P={pi}(i=1 to m) là phân phối xác suất trên các cặp cột-hàng. it ∈ {1,···m} là chỉ số của cặp cột-hàng được lấy mẫu tại lần thử thứ t. st là hệ số tỷ lệ. X′∈Rn×k và Y′∈Rk×q là các ma trận con được chuẩn hóa được cắt theo các cặp cột-hàng được lấy mẫu.

Nghiên cứu hiện có (Drineas et al., 2006) cho thấy X′Y′ là một ước lượng không thiên lệch của XY, tức là E[X′Y′] = XY. Hơn nữa, lỗi xấp xỉ E[||XY−X′Y′||F] được tối thiểu khi các xác suất {pi}(i=1 to m) tỷ lệ với tích của các chuẩn Euclidean cột-hàng (Drineas et al., 2006) (Chứng minh trong Phụ lục C):

pi = (||X:,i||₂||Yi,:||₂) / (∑(j=1 to m) ||X:,j||₂||Yj,:||₂). (3)

Như chúng tôi đã phân tích trong Mục 2.1, việc lưu trữ kích hoạt H là nút cổ chai bộ nhớ chính. Nếu chúng ta có thể thay thế GEMM(H⊤,∇Z) trong Phương trình (1c) bằng H′⊤∇Z′ theo mô thức của Phương trình (2), thì chúng ta chỉ cần H′ thay vì H trong bộ nhớ GPU để tính toán gradient, điều này làm giảm đáng kể việc sử dụng bộ nhớ của kích hoạt. Ước lượng này giảm tuyến tính độ phức tạp bộ nhớ từ O(nm) xuống O(nk). Ngoài ra, tổng số phép toán dấu phẩy động (FLOPs) cũng được giảm vì tính toán được thực hiện trên hai ma trận nhỏ hơn. Để dễ minh họa, trong bài báo này chúng tôi gọi phân phối trong Phương trình (3) là phân phối chỉ số cột-hàng. Trong phần tiếp theo, chúng tôi khám phá cách giảm việc sử dụng bộ nhớ thông qua phép nhân ma trận dựa trên lấy mẫu.

3 Phương pháp

Trong những năm gần đây, chúng ta đã quan sát thấy rằng việc huấn luyện mạng nơ-ron sâu có thể được thực hiện gần như hoàn toàn bằng tối ưu hóa ngẫu nhiên bậc nhất (Kingma and Ba, 2014). Do đó một cách trực quan, trong tối ưu hóa ngẫu nhiên chúng ta có thể giảm tài nguyên dành cho việc thu được gradient, miễn là gradient ước lượng không thiên lệch với phương sai hợp lý (Chmiel et al., 2023, 2021; Oktay et al., 2020). Theo động lực này, chúng tôi đầu tiên thiết kế một bộ ước lượng không thiên lệch mới cho phép nhân ma trận với phương sai giảm so với bộ ước lượng trong Phương trình (2) (Mục 3.1). Sau đó chúng tôi giới thiệu cách thay thế GEMM trong Transformer bằng phiên bản xấp xỉ của nó để giảm việc sử dụng bộ nhớ (Mục 3.2).

--- TRANG 5 ---
3.1 Lấy mẫu Cột-Hàng Theo Kiểu Người Thắng Nhận Tất Cả: Một Bộ Ước lượng Không thiên lệch Mới cho GEMM

Trong mục này, chúng tôi thiết kế toán học một bộ ước lượng không thiên lệch mới cho GEMM với phương sai giảm được gọi là WTA-CRS (Lấy mẫu Cột-Hàng Theo Kiểu Người Thắng Nhận Tất Cả). Theo ký hiệu trong Mục 2.2, cho X∈Rn×m, Y∈Rm×q là hai ma trận. P={pi}(i=1 to m) là phân phối chỉ số cột-hàng trong Phương trình (3)¹. Chúng tôi đầu tiên định nghĩa biến f(i) là

f(i) = X:i Yi: / pi,

f(i) là một ước lượng không thiên lệch cho tích ma trận giữa X và Y. Để thấy điều này,

E[j∼P[f(j)]] = ∑(i=1 to m) pi (X:,i Yi:) / pi = XY.

Chúng tôi lưu ý rằng phép nhân ma trận xấp xỉ trước đây trong Phương trình (2) là sự mở rộng trực tiếp của f(i) bằng cách lấy trung bình của {f(it)}(t=1 to k) trong k lần thử ngẫu nhiên độc lập để giảm phương sai. Ở đây chúng tôi khám phá một phương pháp thay thế để giảm phương sai của f(i) ngoài việc lấy trung bình đơn giản.

Ý tưởng cốt lõi của chúng tôi là phân vùng phân phối chỉ số cột-hàng P={pi}(i=1 to m) thành hai vùng bổ sung dựa trên khối lượng xác suất: một vùng xác suất cao PC và một vùng xác suất thấp PD\C, trong đó D={1,···, m} là tập hợp toàn bộ và C là tập hợp chỉ số cột-hàng với xác suất lớn nhất. Cho C là tập hợp các chỉ số cặp cột-hàng liên quan với |C| pi lớn nhất. Chúng tôi định nghĩa bộ ước lượng WTA-CRS cho XY như sau:

E[j∼PD\C[∑(c∈C) f(c)pc + (1−∑(c∈C) pc)f(j)]], (4)

Chúng tôi lưu ý rằng biến ngẫu nhiên trong Phương trình (4) là chỉ số cặp cột-hàng j, và chỉ được lấy mẫu từ D\C. Bộ ước lượng được định nghĩa trong Phương trình (4) chứa hai phần. Phần đầu tiên ∑(c∈C) f(c)pc không có mối quan hệ với biến ngẫu nhiên j và được tính tổng một cách xác định. Phần thứ hai f(j) được lấy mẫu ngẫu nhiên, nhưng được chia tỷ lệ bởi hệ số (1−∑(c∈C) pc). Khi P={pi}(i=1 to m) tập trung trên một số lượng nhỏ các nguyên tử, hệ số tỷ lệ (1−∑(c∈C) pc) cho thành phần ngẫu nhiên sẽ nhỏ. Do đó, chúng tôi trực quan kỳ vọng bộ ước lượng sẽ có phương sai nhỏ trong trường hợp này do hệ số tỷ lệ nhỏ. Bằng cách này, chúng tôi giảm phương sai của bộ ước lượng bằng cách tập trung nhiều hơn vào các vùng xác suất cao của phân phối (người thắng nhận tất cả). Dưới đây chúng tôi chính thức hóa trực giác này bằng cách hiển thị tính chất thống kê của bộ ước lượng của chúng tôi về tính thiên lệch và phương sai, tương ứng.

Định lý 1 (Chứng minh trong Phụ lục C.2). Bộ ước lượng được định nghĩa trong Phương trình (4) là một bộ ước lượng không thiên lệch cho tích ma trận XY, tức là E[j∼PD\C[∑(c∈C) f(c)pc + (1−∑(c∈C) pc)f(j)]] = XY.

Định lý 1 phát biểu rằng bộ ước lượng được đề xuất của chúng tôi trong Phương trình (4) là không thiên lệch. Dưới đây chúng tôi so sánh bộ ước lượng được đề xuất của chúng tôi với bộ ước lượng CRS trong Phương trình (2) về mặt phương sai. Giả sử chúng ta có ngân sách chỉ sử dụng k cặp cột-hàng để xấp xỉ tích ma trận. Từ góc độ triển khai, bộ ước lượng được định nghĩa trong Phương trình (2) ước lượng GEMM(X,Y) là:

(CRS) g(X,Y) = (1/k) ∑(t=1 to k) f(it), i₁,···,ik i.i.d∼ P. (5)

¹Ở đây chúng tôi lưu ý rằng phân tích lý thuyết trong mục này có thể được áp dụng cho bất kỳ phân phối xác suất nào, không chỉ giới hạn ở phân phối trong Phương trình (3).

--- TRANG 6 ---
Bộ ước lượng được định nghĩa trong Phương trình (4) của chúng tôi chia ngân sách k thành hai phần. Cụ thể, phần đầu tiên tính tổng rõ ràng các số hạng kỳ vọng cho nhóm xác suất lớn nhất C (|C| < k), trong khi lấy mẫu ngẫu nhiên trung bình k− |C| mẫu được rút từ D\C để ước lượng các số hạng còn lại, theo tỷ lệ:

(WTA-CRS) ĝ(X,Y) = ∑(c∈C) f(c)p(c) + (1−∑(c∈C) pc)/(k−|C|) ∑(j=1 to k−|C|) f(j), i₁,···,ik−|C| i.i.d∼ PD\C. (6)

Định lý 2 (Chứng minh trong Phụ lục C.3). Giả sử tổng ngân sách của các cặp cột-hàng là k. Nếu C thỏa mãn ∑(c∈C) pc > |C|/k, (7)

thì chúng ta có Var[ĝ(X,Y)] < Var[g(X,Y)]. Hơn nữa, Var[ĝ(X,Y)] được tối thiểu khi |C| = min|C|∈{0,···,k} (1−∑(c∈C) pc)/(k−|C|).

Cả vế trái và vế phải của Phương trình (7) đều phụ thuộc vào kích thước của nhóm xác suất cao nhất |C|, điều này kiểm soát số lượng cặp cột-hàng xác suất cao được thêm trực tiếp mà không lấy mẫu. Dưới đây chúng tôi thực nghiệm khảo sát liệu Phương trình (7) có giữ được trong bối cảnh tinh chỉnh mô hình dựa trên transformer với |C| thay đổi hay không.

[Hình 3. Khối lượng xác suất ∑(c∈C) pc so với |C|/k trong Phương trình (7) tại k = 0.3|D|. Ở đây chúng tôi trực quan hóa phân phối chỉ số cột-hàng của lớp phép chiếu query/key/value trong mô hình T5-base, được tinh chỉnh trên dataset RTE. Các kết quả tương tự khác có thể được tìm thấy trong Phụ lục E.1.]

Phân tích thực nghiệm. Như được hiển thị trong Hình 3, chúng tôi trực quan hóa hai số hạng trong Phương trình (3) cho phân phối chỉ số cột-hàng của phép chiếu query, key và value trong mô-đun self-attention, tương ứng (Vaswani et al., 2017). Cụ thể, chúng tôi cố định tổng ngân sách cặp cột-hàng k = 0.3|D| và thay đổi kích thước của nhóm xác suất cao nhất |C| từ 0 đến k. Chúng tôi kết luận rằng Phương trình (7) giữ được cho hầu hết các lớp khi tinh chỉnh transformer. Do đó, chúng tôi kỳ vọng WTA-CRS của chúng tôi có hiệu suất tốt hơn CRS để thích ứng các mô hình dựa trên transformer, điều này sau đó được xác minh thực nghiệm trong Mục 5.

3.2 Nén GEMM trong Transformer với WTA-CRS

Nghiên cứu trước đây đã cho thấy rằng tính không thiên lệch của gradient ước lượng là rất quan trọng cho sự hội tụ đúng đắn của phương pháp gradient descent ngẫu nhiên (Chmiel et al., 2023, 2021; Chen et al., 2021c; Liu et al., 2022a). Như được hiển thị trong Mục 2.1, chúng ta có ba GEMM trong lớp tuyến tính. Dưới đây chúng tôi nghiên cứu cách thay thế GEMM bằng phiên bản xấp xỉ của nó theo cách mà gradient ước lượng không thiên lệch.

Tính không thiên lệch. Nghiên cứu trước đây đã cho thấy rằng để đảm bảo tính không thiên lệch của gradient, việc xấp xỉ chỉ có thể được áp dụng trong giai đoạn lùi (Chen et al., 2021c; Liu et al., 2022b; Adelman et al., 2021). Lý do đằng sau kết luận này là chúng ta có E[f(x)] ≠ f(E[x]) với bất kỳ

--- TRANG 7 ---
[Hình 4. Sơ đồ của một khối Transformer đơn. Hình dạng của kích hoạt được chú thích, trong đó B, S, Dmodel, Nhead, và Dhead là kích thước lô, độ dài tuần tự, kích thước ẩn, số đầu attention, và chiều đầu, tương ứng. WTA-CRS có thể được áp dụng cho các toán tử màu xanh lá cây; các bản đồ kích hoạt của các toán tử màu xanh dương có thể được nén mà không mất thông tin; và những cái màu xám không được nén trong bài báo này. Ý tưởng của hình này được lấy cảm hứng từ (Andoorveedu et al., 2022a).]

hàm phi tuyến f(·), ví dụ, E[x²] ≠ E²[x]. Do đó nếu chúng ta thay thế GEMM tiến trong Phương trình (1a), ngay cả khi phương pháp xấp xỉ cho ra một ước lượng không thiên lệch, tức là E[ĝ(H,W)] = HW = Z, các kích hoạt đầu ra (ví dụ, GeLU(Z)) vẫn thiên lệch vì hàm kích hoạt là phi tuyến, cụ thể là,

GeLU(ĝ(H,W)) = GeLU(E[Z]) ≠ E[GeLU(Z)].

Để đảm bảo tính không thiên lệch của gradient và giảm việc sử dụng bộ nhớ để lưu trữ H, như được hiển thị trong ví dụ của Hình 5, chúng tôi chỉ thay thế GEMM trong giai đoạn lùi bằng phép xấp xỉ của nó (ví dụ, Phương trình (1c)), trong khi để nguyên phép toán tiến (ví dụ, Phương trình (1a)).

Chúng tôi cho thấy trong Phụ lục B rằng gradient trọng số ước lượng là không thiên lệch trong trường hợp này.

[Hình 5. Minh họa cách triển khai WTA-CRS vào các lớp tuyến tính. Chúng tôi chỉ thay thế GEMM trong Phương trình (1c) bằng phiên bản xấp xỉ của nó sử dụng WTA-CRS. Mã giả được đưa ra trong Phụ lục D Thuật toán 1.]

Triển khai. Ở đây chúng tôi trình bày cách chúng tôi triển khai WTA-CRS trong Phương trình (6) trong thực tế. Đối với lớp tuyến tính, như chúng tôi đã phân tích, chúng tôi chỉ thay thế GEMM trong Phương trình (1c) bằng phiên bản xấp xỉ của nó. Trong trường hợp này, X và Y trong Phương trình (6) là kích hoạt H⊤ và gradient đầu ra ∇Z, tương ứng. Cho tổng ngân sách cặp cột-hàng k, bước đầu tiên là xây dựng tập chỉ số xác định C, trong đó mỗi phần tử được tính tổng rõ ràng mà không lấy mẫu. Lưu ý rằng C là một tập hợp các chỉ số với xác suất cao nhất trong Phương trình (3). Do đó, để xây dựng C, chúng ta chỉ cần xác định kích thước của nó, ký hiệu là |C|, điều này tối thiểu hóa phương sai của bộ ước lượng. Như Định lý 2 đề xuất, chúng tôi đặt |C| = min|C|∈{0,···,k} (1−∑(c∈C) pc)/(k−|C|). Bước thứ hai là lấy mẫu k− |C| chỉ số cột-hàng từ phân phối còn lại PD\C để thu được tập hợp Cstoc, trong đó |Cstoc| = k− |C|. Bước thứ ba là xây dựng H′ được lấy mẫu con chỉ với các hàng từ C ∪ Cstoc. Lưu ý rằng đối với các hàng trong H′ từ Cstoc, chúng ta cần chuẩn hóa nó bằng (1−∑(c∈C) pc)/(k−|C|) theo

--- TRANG 8 ---
Phương trình (6). Chúng tôi minh họa quá trình trên trong Hình 5. Mã giả trong Phụ lục D Thuật toán 1. Chúng tôi tận dụng Liger kernel (Hsu et al., 2024) để tăng tốc hơn nữa pipeline tinh chỉnh của chúng tôi.

Phạm vi. Ở đây chúng tôi cho thấy phép toán nào có thể được thay thế bằng phiên bản xấp xỉ của nó. Như được hiển thị trong Hình 4, transformer chủ yếu bao gồm lớp tuyến tính, TensorMul, và các phép toán khác (ví dụ, GeLU, Dropout, LayerNorm). TensorMul trong Hình 4 đề cập đến phép nhân giữa hai tensor bốn chiều. WTA-CRS của chúng tôi có thể được áp dụng cho Linear-Q, -K, -V, -O, -U, -D, TensorMul-1, và TensorMul-2 (màu xanh lá cây). Các kích hoạt của phép toán Dropout và GELU (màu xanh dương) có thể được nén mà không mất thông tin. Các toán tử Softmax và LayerNorm (màu xám) giữ nguyên không thay đổi.

4 Nghiên cứu Liên quan và Thảo luận

Do giới hạn trang, chúng tôi thảo luận nghiên cứu liên quan về phép nhân ma trận xấp xỉ và nén kích hoạt. Các chủ đề liên quan khác, ví dụ, tinh chỉnh hiệu quả tham số và gradient checkpointing, có thể được tìm thấy trong Phụ lục A. Chúng tôi cũng thảo luận về hạn chế và tác động xã hội tiêu cực tiềm ẩn trong Phụ lục A.

Phép nhân Ma trận Xấp xỉ. Trong bối cảnh mạng nơ-ron, các phương pháp nhân ma trận xấp xỉ có thể được phân loại rộng rãi thành hai nhóm chính: (1) Các phương pháp dựa trên Butterfly (Chen et al., 2021a; Dao et al., 2022) thay thế ma trận trọng số dày đặc bằng ma trận butterfly. Chúng tôi lưu ý rằng chúng tập trung vào ma trận trọng số và trực giao với nghiên cứu của chúng tôi, vì chúng tôi tập trung vào việc lấy mẫu con ma trận kích hoạt. (2) Các phương pháp lấy mẫu cột-hàng (CRS) (Drineas et al., 2006; Adelman et al., 2021; Liu et al., 2022b) chọn các hàng và cột quan trọng từ ma trận đầu vào và thực hiện phép nhân trên ma trận được lấy mẫu. Nghiên cứu của chúng tôi phù hợp chặt chẽ với dòng nghiên cứu thứ hai này. (Adelman et al., 2021; Liu et al., 2022b) có những điểm tương đồng với nghiên cứu của chúng tôi về việc sử dụng CRS để xấp xỉ phép nhân ma trận trong mạng nơ-ron. Sự khác biệt chính nằm ở cách chọn các cặp cột-hàng. Cụ thể, (Adelman et al., 2021) chọn các cặp cột-hàng một cách xác định mà không có tỷ lệ, trong khi bộ ước lượng của chúng tôi chia các cặp cột-hàng thành một thành phần xác định và một thành phần ngẫu nhiên. Như chúng tôi đã phân tích, việc chọn các cặp cột-hàng một cách xác định là thiên lệch. Sau này chúng tôi cho thấy rằng phương pháp này có thể gây ra sự sụt giảm độ chính xác đáng kể ("Deterministic" trong Hình 8).

Lượng tử hóa Kích hoạt. Các phương pháp lượng tử hóa kích hoạt tập trung vào việc lượng tử hóa kích hoạt thành các số có độ chính xác số thấp, ví dụ, số nguyên 8-bit (Chen et al., 2021c; Liu et al., 2022a,c; Wang et al., 2022b,a). Ở đây chúng tôi thảo luận sự khác biệt giữa hai công trình này về mặt tỷ lệ nén. Theo Bảng 5 trong (Liu et al., 2022a), khi GACT được kết hợp với Swapping, tức là đổ kích hoạt được lượng tử hóa ra bộ nhớ chính, nó đạt được tỷ lệ nén sử dụng bộ nhớ đỉnh 1.73× cho Bert-Large. Nghiên cứu của chúng tôi cũng nén kích hoạt, nhưng theo cách khác. Chúng tôi nhấn mạnh rằng nghiên cứu của chúng tôi trực giao với lượng tử hóa kích hoạt theo nghĩa là nghiên cứu của chúng tôi về cơ bản giảm chiều của kích hoạt. Sự khác biệt này cho phép phương pháp của chúng tôi có thể dễ dàng kết hợp với các kỹ thuật lượng tử hóa kích hoạt, mang lại tiềm năng nén còn tích cực hơn nữa.

5 Thực nghiệm

Trong mục này, chúng tôi thiết kế thực nghiệm để trả lời các câu hỏi nghiên cứu sau: RQ1: WTA-CRS hiệu quả như thế nào về mặt độ chính xác với việc sử dụng bộ nhớ giảm? RQ2: WTA-CRS nhạy cảm như thế nào với các siêu tham số chính của nó? RQ3: WTA-CRS chứa hai phần, tức là phần

--- TRANG 9 ---
Bảng 1: Kết quả benchmark GLUE với T5 và Bert ở các quy mô khác nhau.

[Bảng chi tiết với các kết quả thực nghiệm về CoLA, SST-2, MRPC, QQP, MNLI, QNLI, RTE, STS-B cho các mô hình khác nhau]

tổng xác định và phần lấy mẫu thống kê. Cả hai có cần thiết không? RQ4: Tốc độ tinh chỉnh bị ảnh hưởng như thế nào bởi WTA-CRS?

5.1 Thiết lập Thực nghiệm

Datasets và Giao thức Đánh giá. Theo hầu hết nghiên cứu trước đây, chúng tôi áp dụng benchmark GLUE (Wang et al., 2018b) để đánh giá hiệu quả của các phương pháp khác nhau, bao gồm các dataset CoLA, SST-2, MRPC, QQP, MNLI, QNLI, RTE, và STS-B. Đối với các dataset SST-2, MNLI, QNLI, và RTE, chúng tôi báo cáo độ chính xác validation. Đối với CoLA, chúng tôi sử dụng tương quan Matthew làm metric đánh giá. Điểm F1 được báo cáo cho cả hai tác vụ MRPC và QQP, trong khi tương quan Pearson-Spearman được sử dụng để đánh giá hiệu suất trên dataset STS-B. Để đánh giá việc sử dụng bộ nhớ, chúng tôi báo cáo việc sử dụng bộ nhớ GPU đỉnh và tỷ lệ nén trong quá trình tinh chỉnh với Huggingface API (Wolf et al., 2020).

Các Phương pháp So sánh và Mô hình Được Áp dụng. Chúng tôi xem xét ba phương pháp để so sánh trong bài báo này: Tinh chỉnh đầy đủ (Full), LoRA (Hu et al., 2021), và Ladder Side-tuning (LST) (Sung et al., 2022). Cụ thể, Full tinh chỉnh tất cả các tham số trong mô hình để cung cấp cận trên về độ chính xác; LoRA chèn các ma trận rank thấp có thể huấn luyện vào mô hình để tham số hóa các thay đổi trọng số; LST tiêm một cấu trúc ladder side có thể huấn luyện. Vì WTA-CRS về cơ bản thay thế phép toán tuyến tính bằng phép toán xấp xỉ, chúng tôi nhấn mạnh rằng WTA-CRS của chúng tôi tương thích với tất cả ba phương pháp so sánh này, tức là, chúng có thể được kết hợp với nhau hướng tới việc sử dụng bộ nhớ nhỏ hơn. Đối với mô hình xương sống, chúng tôi theo nghiên cứu trước đây (Sung et al., 2022; Houlsby et al., 2019; Hu et al., 2021) để áp dụng Bert-Base (Devlin et al., 2018), Bert-Large, T5-Base, T5-Large, và T5-3B (Raffel et al., 2020a) để đánh giá hiệu quả của các phương pháp khác nhau.

Thiết lập Siêu tham số. Đối với WTA-CRS, nó chỉ có một siêu tham số k, kiểm soát ngân sách cặp cột-hàng. Chúng tôi gán cùng một k cho tất cả các phép toán tuyến tính có thể thay thế trong mô hình. Chúng tôi xem xét ngân sách cặp cột-hàng được chuẩn hóa k/|D| ∈ {0.3, 0.1}, được ký hiệu là WTA-CRS@0.3 và WTA-CRS@0.1, tương ứng. Chúng tôi cũng xem xét sự kết hợp của WTA-CRS và LoRA để giảm thêm chi phí bộ nhớ của cả trình tối ưu và kích hoạt. Các siêu tham số chi tiết được đưa ra trong Phụ lục F. Tất cả kết quả được báo cáo được tính trung bình trên ba lần thử ngẫu nhiên.

--- TRANG 10 ---
Bảng 2: Việc sử dụng bộ nhớ đỉnh (GB) và tỷ lệ nén của việc tinh chỉnh T5-Base và -Large. Chúng tôi đo việc sử dụng bộ nhớ trên một GPU NVIDIA A100 (80GB) duy nhất. Đối với T5-3B, vì nó được huấn luyện sử dụng multi-GPU với data parallel. Thay vào đó chúng tôi báo cáo kích thước lô tối đa trong Hình 6.

[Bảng hiển thị việc sử dụng bộ nhớ cho các phương pháp khác nhau]

5.2 Độ chính xác so với Việc sử dụng Bộ nhớ (RQ1)

Để trả lời RQ1, chúng tôi đầu tiên phân tích sự đánh đổi giữa hiệu suất mô hình và tiết kiệm bộ nhớ. Kết quả đánh giá và việc sử dụng bộ nhớ đỉnh được đưa ra trong Bảng 1 và 2, tương ứng. Chúng tôi quan sát:

❶ WTA-CRS đạt được sự đánh đổi vượt trội giữa độ chính xác và việc sử dụng bộ nhớ so với baseline. Cụ thể, WTA-CRS có sự sụt giảm độ chính xác không đáng kể, trong khi việc sử dụng bộ nhớ đỉnh được giảm 2.1× ∼ 2.7× (khi kết hợp với LoRA).

[Hình 6. Việc sử dụng bộ nhớ đỉnh so với kích thước lô tối đa của T5-3B. Các kết quả tương tự khác được hiển thị trong Phụ lục E.2.]

Như chúng tôi đã phân tích, LoRA chủ yếu giảm bộ nhớ của trạng thái trình tối ưu. Do đó, mặc dù nó có sự sụt giảm độ chính xác không đáng kể, nó chỉ có thể đạt được tiết kiệm bộ nhớ đỉnh ∼1.3×. LST có thể giảm việc sử dụng bộ nhớ lên đến 3×, nhưng sự sụt giảm độ chính xác của nó lớn hơn nhiều so với LoRA và WTA-CRS. Vì WTA-CRS thực thi ở cấp độ phép toán và tập trung vào kích hoạt, chúng tôi tiếp tục kết hợp LoRA với WTA-CRS để giảm việc sử dụng bộ nhớ một cách tích cực hơn. Khi kết hợp với LoRA, WTA-CRS đạt được tiết kiệm sử dụng bộ nhớ 2.7× với hầu như không có sự sụt giảm độ chính xác. Để tinh chỉnh đầy đủ T5-3B, nó yêu cầu 37.7GB bộ nhớ và dựa vào một GPU có dung lượng 40GB hoặc cao hơn, ví dụ RTX8000, A100, hoặc A40. Mặt khác, LoRA+ WTA-CRS chỉ yêu cầu 21.6GB bộ nhớ để tinh chỉnh với kích thước mini-batch là 32, có thể chạy trên GPU có bộ nhớ 24GB, ví dụ RTX3090Ti hoặc A5000. Chúng tôi đã xác nhận thực nghiệm kết luận này. Dưới cùng phần cứng, WTA-CRS cho phép tinh chỉnh các mô hình lớn hơn, dẫn đến hiệu suất tác vụ xuôi dòng được cải thiện. Do đó như được hiển thị trong Hình 1, ❷ dưới ngân sách bộ nhớ tương tự, WTA-CRS vượt trội hơn các phương pháp khác về mặt độ chính xác.

Ngoài ra, theo Hình 6, đối với T5-3B, bản thân LoRA có thể cho phép kích thước lô lớn hơn 1.9×. ❸ Khi kết hợp với LoRA, WTA-CRS cho phép kích thước lô lớn hơn 4.8× (k=0.3|D|) đến 6.4× (k=0.1|D|).

Ảnh hưởng của Ngân sách Cặp Hàng-cột (RQ2). Như chúng tôi đã phân tích trong Mục 3.2, WTA-CRS chỉ có một siêu tham số, tức là tổng ngân sách cặp cột-hàng k. Chúng tôi thực hiện nghiên cứu ablation với ngân sách k khác nhau trong Hình 7. Chúng tôi quan sát rằng ❹ Nó hầu như không có sự sụt giảm độ chính xác khi k = 0.3|D|. Và sự sụt giảm độ chính xác khoảng 1% khi k = 0.1|D|. Đáng chú ý, đối với T5-3B, sự sụt giảm độ chính xác chỉ là 0.4% khi k = 0.1|D|, nhỏ hơn nhiều so với T5-Base và T5-Large. Điều này cho thấy rằng các mô hình lớn hơn có khả năng nén tốt hơn vì chúng có nhiều kích hoạt dư thừa hơn, phù hợp với quan sát trước đây (Li et al., 2020).

5.3 Nghiên cứu Ablation (RQ3 và RQ4)

Để trả lời RQ3, WTA-CRS được so sánh với hai phương pháp tổng hợp để chứng minh tính ưu việt của nó. Cụ thể, (1) phương pháp Deterministic chọn các cặp hàng-cột với xác suất k hàng đầu của Phương trình (3). Chúng tôi lưu ý rằng đây là bộ ước lượng được đề xuất trong (Adelman et al., 2021). (2) Phương pháp CRS theo Phương trình (3) để lấy mẫu các cặp hàng-cột. Tất cả các phương pháp được triển khai cho GEMM trong

--- TRANG 11 ---
[Hình 7. Kết quả validation trung bình trên dataset GLUE của WTA-CRS với các ngân sách khác nhau.]

[Hình 8. Kết quả validation của T5-Base với các phương pháp khác nhau.]

giai đoạn lùi, trong khi để nguyên giai đoạn tiến. Các thực nghiệm được thực hiện trên việc huấn luyện mô hình ngôn ngữ T5-base trên các dataset SST2, MNLI, và QQP; Ngân sách cặp cột-hàng lấy k/|D| = 0.1 cho tất cả các phương pháp. Độ chính xác validation so với epoch huấn luyện được đưa ra trong Hình 8. Chúng tôi quan sát:

❺ WTA-CRS vượt trội hơn tất cả các phương pháp so sánh, đặc biệt là khi epoch huấn luyện tăng. Việc chọn xác định k cặp cột-hàng hàng đầu bị ảnh hưởng bởi sự tích lũy của lỗi thiên lệch cuối cùng dẫn đến thất bại trong hội tụ. Đối với CRS, nó cũng cho phép gradient trọng số không thiên lệch. Tuy nhiên, như chúng tôi đã phân tích lý thuyết và thực nghiệm trong Định lý 7 và Hình 3, nó tệ hơn WTA-CRS do phương sai lớn hơn. Tóm lại, cả phần xác định và phần ngẫu nhiên đều góp phần vào hiệu quả của WTA-CRS, phù hợp với phân tích lý thuyết của chúng tôi.

[Hình 9. Kích thước lô so với thông lượng huấn luyện (câu/giây) với các phương pháp khác nhau, trong đó độ dài tuần tự là 128. Phần cứng là một NVIDIA-A100 (80GB) duy nhất.]

Tốc độ của WTA-CRS (RQ4). Cấu hình của hạ tầng tính toán được đưa ra trong Phụ lục F.1. Chúng tôi lưu ý rằng WTA-CRS không thêm bất kỳ tham số bổ sung nào vào mô hình. Do đó, WTA-CRS chỉ ảnh hưởng đến tốc độ tinh chỉnh, không ảnh hưởng đến tốc độ suy luận. Bộ nhớ được sử dụng bởi kích hoạt tỷ lệ với kích thước lô và độ dài tuần tự. Dưới đây chúng tôi phân tích tốc độ tinh chỉnh bị ảnh hưởng như thế nào bởi WTA-CRS. Như chúng tôi đã phân tích trong Phụ lục A hạn chế, việc triển khai hiện tại không được tối ưu hóa mạnh mẽ và do đó thời gian thực thi của WTA-CRS vẫn chậm hơn so với phép toán tuyến tính gốc (chi tiết được hiển thị trong Phụ lục E.2). Tuy nhiên, dưới cùng phần cứng, việc giảm bộ nhớ kích hoạt cho phép sử dụng kích thước lô lớn hơn, do đó cải thiện tốc độ huấn luyện do tăng việc sử dụng GPU (Goyal et al., 2017; Sung et al., 2022). Như chúng tôi đã phân tích trong Hình 6, WTA-CRS có thể mở rộng kích thước lô có sẵn lên đến 4.8× lớn hơn. Việc cải tiến này dự kiến sẽ dẫn đến tăng tốc độ huấn luyện. Để minh họa mối quan hệ này, Hình 9 trình bày trực quan hóa kích thước lô so với thông lượng huấn luyện (câu trên giây) cho cả mô hình T5-Large và T5-3B. Chúng tôi quan sát rằng ❻ WTA-CRS cho phép tốc độ huấn luyện nhanh hơn dưới cùng phần cứng. Cụ thể, trên mô hình T5-Large, WTA-CRS@0.1 cho thấy thông lượng huấn luyện cao hơn 1.08×; và trên mô hình T5-3B, WTA-CRS@0.3 và WTA-CRS@0.1 đạt được thông lượng huấn luyện cao hơn 1.14× và 1.21×, tương ứng.

6 Kết luận

Trong bài báo này, chúng tôi đề xuất WTA-CRS, một bộ ước lượng không thiên lệch mới cho tích ma trận với phương sai giảm. Chúng tôi cho thấy lý thuyết và thực nghiệm khi nào và tại sao bộ ước lượng tốt hơn so với bộ ước lượng không thiên lệch truyền thống về mặt phương sai. Trong bối cảnh thích ứng transformer, nó hầu như không có sự sụt giảm độ chính xác trong khi giảm việc sử dụng bộ nhớ đỉnh lên đến 2.7×, và nó cho phép kích thước lô lớn hơn 6.4×, điều này về lại dẫn đến thông lượng huấn luyện cao hơn 1.2×.
