# 2307.03084.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2307.03084.pdf
# Kích thước tệp: 920410 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OpenDelta: Một Thư viện Cắm và Chạy cho Việc Điều chỉnh
Hiệu quả Tham số của Các Mô hình Được Huấn luyện Trước
Shengding Hu1,2, Ning Ding1,2, Weilin Zhao1,2, Xingtai Lv1, Zhen Zhang1
Zhiyuan Liu1,2,3∗,Maosong Sun1,2
1Dept. of Comp. Sci. & Tech., IAI, BNRIST, Tsinghua University, Beijing
2International Innovation Center of Tsinghua University, Shanghai, China
3Jiangsu Collaborative Innovation Center for Language Ability, Jiangsu Normal University
hsd20@mails.tsinghua.edu.cn
Tóm tắt
Quy mô của các mô hình được huấn luyện trước lớn (PTMs)
đặt ra những thách thức đáng kể trong việc thích ứng với
các tác vụ hạ nguồn do chi phí tối ưu hóa cao và chi phí
lưu trữ liên quan đến việc tinh chỉnh toàn bộ tham số.
Để giải quyết vấn đề này, nhiều nghiên cứu khám phá các
phương pháp điều chỉnh hiệu quả tham số, còn được khung
hóa như "delta tuning" trong Ding et al. (2022), chỉ cập
nhật một tập con nhỏ các tham số, được gọi là "delta modules",
trong khi giữ cố định các tham số của mô hình backbone.
Tuy nhiên, tính thực tiễn và linh hoạt của delta tuning
đã bị hạn chế do các triển khai hiện có trực tiếp sửa đổi
mã của các PTMs backbone và mã hóa cứng các phương
pháp delta tuning cụ thể cho từng PTM. Trong bài báo này,
chúng tôi trình bày OpenDelta1, một thư viện mã nguồn
mở vượt qua những hạn chế này bằng cách cung cấp một
triển khai cắm và chạy của các phương pháp delta tuning
khác nhau. Các kỹ thuật mới của chúng tôi loại bỏ nhu
cầu sửa đổi mã của các PTMs backbone, làm cho OpenDelta
tương thích với các PTMs khác nhau, thậm chí là mới.
OpenDelta được thiết kế để đơn giản, modular và có thể
mở rộng, cung cấp một nền tảng toàn diện cho các nhà
nghiên cứu và người thực hành để điều chỉnh các PTMs
lớn một cách hiệu quả.
1 Giới thiệu
Với sự phát triển nhanh chóng của các phương pháp học
tự giám sát trong lĩnh vực học sâu, đặc biệt là các kỹ
thuật huấn luyện trước (Peters et al., 2018; Devlin et al.,
2018; Radford et al., 2018), các mô hình được huấn luyện
trước nền tảng (Bommasani et al., 2021) (PTMs) đã trở
thành nền tảng chung cho nhiều tác vụ hạ nguồn. Và kết
quả là, nghiên cứu về các PTMs quy mô lớn đã phát triển
mạnh mẽ.
Tuy nhiên, quy mô ngày càng mở rộng của các PTMs
cũng đặt ra những trở ngại đáng kể trong việc sử dụng
thực tế. Trong việc điều chỉnh mô hình truyền thống, tất
cả các tham số của các PTMs được tối ưu hóa cho từng
tác vụ hạ nguồn, điều này trở nên ngày càng không thực
tế khi mô hình mở rộng quy mô. Thứ nhất, việc tối ưu
hóa tất cả các tham số phát sinh chi phí tính toán và bộ
nhớ cấm đoán; thứ hai, việc lưu trữ một phiên bản mô
hình đã tinh chỉnh cho mỗi tác vụ hoặc thí nghiệm làm
tăng đáng kể chi phí lưu trữ.
Để giải quyết những thách thức này, các nhà nghiên cứu
đã phát triển các phương pháp hiệu quả tham số cho việc
điều chỉnh mô hình. Các phương pháp như vậy giữ cố
định các tham số của mô hình chính và chỉ cập nhật một
tập con nhỏ các tham số trong quá trình điều chỉnh. Cách
tiếp cận này, được biết đến như "delta tuning", được mô
tả và khảo sát trong Ding et al. (2022). Các phương pháp
delta tuning khác nhau đã được đề xuất, với các loại và
vị trí khác nhau của "delta modules". Ví dụ, module
Adapter (Houlsby et al., 2019) bao gồm hai lớp chiếu
tuyến tính chiều thấp với một hàm kích hoạt, trong khi
module LoRA (Hu et al., 2021) giới thiệu một phân rã
thứ hạng thấp cho ma trận trọng số. BitFit (Zaken et al.,
2021), mặt khác, chỉ định vector bias trong các PTMs
như là các delta modules. Delta module có thể được áp
dụng vào các vị trí khác nhau (Rücklé et al., 2020; He
et al., 2022; Hu et al., 2022) để đạt được hiệu suất hoặc
hiệu quả tốt hơn.
Về mặt lý thuyết, việc kết hợp hầu hết các phương pháp
delta tuning sẽ đòi hỏi việc tái cấu trúc mô hình backbone,
một yêu cầu thông thường được thực hiện thông qua việc
thao tác mã trực tiếp. Mặc dù phương pháp này có vẻ
đơn giản, nó mang theo một số nhược điểm. Chủ yếu, nó
thiếu linh hoạt, vì các delta modules có thể theo lý thuyết
được triển khai ở nhiều vị trí khác nhau, làm cho việc sửa
đổi từng vị trí trong mã mô hình backbone trở thành một
nhiệm vụ cồng kềnh. Ngoài ra, phương pháp này không
có khả năng mở rộng, vì việc phù hợp delta tuning cho
các PTMs mới được giới thiệu đòi hỏi các sửa đổi mã
mới, đặt ra thách thức cho các nhà nghiên cứu và kỹ sư.
Trong bài báo này, chúng tôi trình bày một cách tiếp cận
mới để triển khai các phương pháp delta tuning. Cách tiếp
cận của chúng tôi sửa đổi kiến trúc của mô hình backbone
sau khi nó được tải vào bộ nhớ. Chúng tôi đề xuất bốn
kỹ thuật thiết yếu, cụ thể là địa chỉ hóa dựa trên tên,
định tuyến lại tensor động, khởi tạo thời gian chạy, và
một hệ thống trực quan hóa. Sử dụng những kỹ thuật
chính này, chúng tôi xây dựng OpenDelta, một bộ công
cụ mã nguồn mở cho delta tuning mà không sửa đổi mã
mô hình backbone. OpenDelta có một số tính năng chính.
Thứ nhất, nó đơn giản để sử dụng. Việc di chuyển từ
huấn luyện toàn bộ tham số hiện có sang delta tuning chỉ
yêu cầu ít nhất ba dòng mã. Đối với người mới bắt đầu
hoặc kỹ sư, chúng tôi cũng hỗ trợ xây dựng mô hình
delta tự động. Thứ hai, nó có tính modular, với các delta
modules được triển khai như các sub-modules độc lập có
thể được gắn vào hoặc tách khỏi các mô hình backbone.
Tính năng này cho phép các delta modules khác nhau
cùng tồn tại và hợp tác trong cùng một mô hình backbone
và phục vụ nhiều tác vụ một cách linh hoạt. Thứ ba,
OpenDelta có khả năng mở rộng cao, hỗ trợ các mô hình
được huấn luyện trước trong một loạt các framework,
bao gồm cả các triển khai chính thức từ Thư viện Huggingface
(Wolf et al., 2019) và các PTMs tùy chỉnh. Nó có thể
được sử dụng với các PTMs mới xuất hiện và tích hợp
với các framework PTMs khác để huấn luyện hiệu quả,
chẳng hạn như framework huấn luyện song song.

--- TRANG 2 ---
sửa đổi kiến trúc của mô hình backbone sau khi nó được
tải vào bộ nhớ. Chúng tôi đề xuất bốn kỹ thuật thiết yếu,
cụ thể là địa chỉ hóa dựa trên tên, định tuyến lại tensor
động, khởi tạo thời gian chạy, và một hệ thống trực quan
hóa. Sử dụng những kỹ thuật chính này, chúng tôi xây
dựng OpenDelta, một bộ công cụ mã nguồn mở cho delta
tuning mà không sửa đổi mã mô hình backbone. OpenDelta
có một số tính năng chính. Thứ nhất, nó đơn giản để sử
dụng. Việc di chuyển từ huấn luyện toàn bộ tham số hiện
có sang delta tuning chỉ yêu cầu ít nhất ba dòng mã. Đối
với người mới bắt đầu hoặc kỹ sư, chúng tôi cũng hỗ trợ
xây dựng mô hình delta tự động. Thứ hai, nó có tính modular,
với các delta modules được triển khai như các sub-modules
độc lập có thể được gắn vào hoặc tách khỏi các mô hình
backbone. Tính năng này cho phép các delta modules khác
nhau cùng tồn tại và hợp tác trong cùng một mô hình backbone
và phục vụ nhiều tác vụ một cách linh hoạt. Thứ ba, OpenDelta
có khả năng mở rộng cao, hỗ trợ các mô hình được huấn
luyện trước trong một loạt các framework, bao gồm cả
các triển khai chính thức từ Thư viện Huggingface (Wolf
et al., 2019) và các PTMs tùy chỉnh. Nó có thể được sử
dụng với các PTMs mới xuất hiện và tích hợp với các
framework PTMs khác để huấn luyện hiệu quả, chẳng
hạn như framework huấn luyện song song.

2 Công trình Liên quan
Công trình của chúng tôi liên quan đến delta tuning, cụ
thể hơn là việc triển khai các phương pháp delta tuning.
Delta Tuning. Delta tuning đề cập đến phương pháp
hiệu quả tham số để điều chỉnh một PTM lớn. Các phương
pháp delta tuning khác nhau (Houlsby et al., 2019; Zaken
et al., 2021; Li and Liang, 2021; Hu et al., 2021; Mahabadi
et al., 2021; Sung et al., 2022) khác nhau ở cả kiến trúc
của delta module và các vị trí mà các delta modules được
tích hợp vào mô hình backbone. Nhiều công trình đã cố
gắng kết nối những cách tiếp cận delta tuning khác biệt
này dưới một góc nhìn thống nhất (He et al., 2022; Ding
et al., 2022; Hu et al., 2022). Trong công trình của chúng
tôi, chúng tôi lấy cảm hứng từ quan điểm thống nhất này
và nhằm mục đích thiết kế một framework có thể hỗ trợ
các phương pháp delta tuning khác nhau trong cùng một
pipeline. Thư viện của chúng tôi bao gồm các phương
pháp delta tuning phổ biến nhất và có thể thích ứng với
các phương pháp mới khi chúng xuất hiện.
Triển khai Delta tuning. Các framework triển khai trước
đây cho delta tuning dựa vào cách tiếp cận sửa đổi mã.
Ví dụ, AdapterHub (Pfeiffer et al., 2020) sao chép một
phiên bản cụ thể của Thư viện Huggingface transformers
(Wolf et al., 2019) và triển khai một số phương pháp
delta tuning phổ biến cho một tập các PTMs được định
nghĩa trước. LoRA (Hu et al., 2021) triển khai một thư
viện hạn chế các lớp tuyến tính LoRA. Những phương
pháp này là cụ thể cho mô hình và liên quan đến các
triển khai được mã hóa cứng, điều này hạn chế khả năng
sử dụng của chúng trên các PTMs khác nhau. Ngược lại,
OpenDelta đại diện cho một tiến bộ đáng kể vì nó không
yêu cầu thay đổi mã cho mô hình backbone, làm cho nó
rất linh hoạt và có thể áp dụng rộng rãi.

3 Động cơ
Trong phần này, chúng tôi bắt đầu bằng việc trình bày
công thức thống nhất của delta tuning. Sau đó, chúng
tôi nhấn mạnh một tập các đặc điểm quan trọng của delta
tuning, tập trung vào khía cạnh triển khai, điều này nhấn
mạnh nhu cầu cấp thiết cho một bộ công cụ mới để hỗ
trợ trong nghiên cứu và phát triển các cách tiếp cận delta
tuning.

3.1 Công thức Thống nhất của Delta Tuning
Mặc dù delta tuning về nguyên tắc không bị hạn chế đối
với một loại mạng nơ-ron cụ thể, hiện tại hầu như tất
cả các phương pháp delta tuning đều được áp dụng cho
các PTMs (Devlin et al., 2019; Liu et al., 2019; Raffel
et al., 2019; Brown et al., 2020) với kiến trúc Transformers
(Vaswani et al., 2017). Một PTM M được tham số hóa
bởi Θ bao gồm nhiều sub-modules m, trong đó các biểu
diễn ẩn h được truyền qua sub-module để tạo ra biểu
diễn ẩn mới h′, tức là, h′=m(h).
Việc điều chỉnh một PTM M cho các tác vụ hạ nguồn
là cập nhật các tham số gốc Θ thành Θ′. Trong tinh chỉnh
toàn bộ tham số, tất cả các tham số có thể được cập nhật,
tức là, có thể, |∆Θ|=|Θ|. Ngược lại, delta tuning chỉ
cập nhật một phần nhỏ các tham số, tức là, |∆Θ| ≪ |Θ|.
Mặc dù có sự khác biệt mạnh mẽ trong hình thức cụ thể
của các phương pháp delta tuning, He et al. (2022) thống
nhất chúng thành các dạng đặc biệt của các sửa đổi ∆h
đối với biểu diễn ẩn h. ∆h được tạo ra bằng cách truyền
một trạng thái ẩn hδ qua một delta module mδ. Chính
thức,
h←h+ ∆h=h+mδ(hδ), (1)
trong đó ← biểu thị việc thay thế h gốc, và hδ có thể
giống hoặc khác với h.

3.2 Các Tính năng Chính cho Delta Tuning
Một số tính năng chính của các phương pháp delta tuning
có thể được quan sát từ Phương trình (1).

--- TRANG 3 ---
Địa chỉ hóa Dựa trên Tên
PTM
 
Khởi tạo Thời gian Chạy Đối tượng Delta Định tuyến lại Tensor Động
Gắn
Tách
Đóng băng
Lưu
Tải
AutoDelta
Kết hợp
... 
Delta
Module
mδ mδ mδ Hình 1: Framework tổng thể của OpenDelta. Việc xây dựng đối tượng delta xảy ra sau khi mô hình backbone
được tải.

Định tuyến lại Tensor. Tính năng đầu tiên của delta
tuning là khả năng chuyển hướng luồng các trạng thái
ẩn. Trong một mô hình được huấn luyện trước, luồng
các trạng thái ẩn tạo thành một đồ thị tĩnh, với các trạng
thái ẩn đóng vai trò như các nút và các sub-modules hoạt
động như các phép biến đổi trên các cạnh. Như được
thể hiện trong Phương trình (1), việc giới thiệu phép
biến đổi cạnh mδ chuyển hướng nút hδ và tiêm nó vào
một nút khác h, tạo ra một luồng mới của các trạng thái
ẩn không có trong kiến trúc mô hình gốc. Việc triển khai
OpenDelta nên đạt được việc định tuyến lại tensor như
vậy mà không mã hóa cứng chúng.
Tính linh hoạt. Phương trình (1) cho phép các trạng
thái ẩn đầu vào và đầu ra được đặt ở bất kỳ vị trí nào
trong mô hình backbone M. Ví dụ, AdapterDrop (Rücklé
et al., 2021) quan sát thấy rằng chỉ áp dụng các delta
modules vào nửa trên của các lớp Transformer mang lại
kết quả tốt hơn so với nửa dưới. Tính linh hoạt của các
vị trí được áp dụng cung cấp cơ hội đáng chú ý để khám
phá cấu trúc tiềm năng của các delta modules (Hu et al.,
2022). Tuy nhiên, nó cũng đặt ra thách thức cho việc
triển khai để có thể đạt được tính linh hoạt trong thực
tế phù hợp với framework lý thuyết.
Tính kết hợp. Các phương pháp delta tuning khác nhau
có thể cùng tồn tại hoặc thậm chí được kết hợp trong
cùng một mô hình backbone (Hu et al., 2022), có thể
tăng cường hiệu suất hoặc hỗ trợ học đa tác vụ (Pfeiffer
et al., 2021). Do đó, việc cho phép triển khai dễ dàng
và độc lập của mỗi phương pháp delta tuning là rất quan
trọng, đồng thời cũng cho phép kết hợp linh hoạt của
nhiều modules.
Tính động. Việc mô hình PTM backbone phục vụ như
một mô hình trung tâm cho nhiều tác vụ trong delta tuning
là phổ biến. Để phục vụ một tác vụ cụ thể, các delta
modules được gắn vào mô hình backbone, tạo ra một
chuyên gia cụ thể cho tác vụ. Khi các delta modules được
tách ra, các mô hình backbone trở lại chức năng ban đầu
của chúng như các mô hình ngôn ngữ tổng quát. Bản
chất động này của việc điều chỉnh tác vụ dựa trên delta
tuning nên được tích hợp vào OpenDelta.

4 OpenDelta
Xem xét các tính năng chính đã nêu của delta tuning,
chúng tôi trình bày OpenDelta. Chúng tôi sẽ bắt đầu
bằng việc trình bày tổng quan về OpenDelta. Tiếp theo
đó, chúng tôi sẽ đi sâu vào các triển khai chính của
framework này.

4.1 Framework
Để thực hiện delta tuning, hai điều kiện tiên quyết được
yêu cầu: một mô hình ngôn ngữ được huấn luyện trước
M và "các modules được sửa đổi", là một danh sách
được người dùng chỉ định của các sub-modules mi mà
các delta modules nên được áp dụng. Mục tiêu của chúng
tôi là xây dựng một đối tượng delta. Mục tiêu của chúng
tôi là tạo ra một đối tượng delta, là một tập hợp các delta
modules thường được đặt ở nhiều vị trí khác nhau trong
M và phục vụ như một tổng thể để điều chỉnh PTM cho
các tác vụ hạ nguồn. Chúng tôi tuân theo ba bước để
tạo ra một đối tượng delta. Thứ nhất, chúng tôi sử dụng
địa chỉ hóa dựa trên tên để có được các con trỏ đến các
modules được sửa đổi. Thứ hai, chúng tôi xây dựng một
đối tượng delta bao gồm các delta modules chưa được
khởi tạo. Thứ ba, chúng tôi sửa đổi tuyến đường của
các tensor trong các modules được sửa đổi vào các delta
modules bằng cách sử dụng kỹ thuật định tuyến lại tensor
động. Sau khi tuyến đường cập nhật của trạng thái ẩn
được thiết lập, chúng tôi thực hiện khởi tạo thời gian
chạy để khởi tạo đối tượng delta.
Sau khi đối tượng delta được xây dựng, chúng tôi gắn
nó vào mô hình backbone. Sau đó, chúng tôi cung cấp
một giao diện chức năng đơn giản để tắt việc tính toán
gradient trong các mô hình backbone và chỉ tính toán
gradient của các tham số trong đối tượng delta. Sau khi
quá trình huấn luyện hoàn thành, chúng tôi cung cấp
một giao diện đơn giản để chỉ lưu các đối tượng delta,
điều này giảm đáng kể yêu cầu lưu trữ cho mô hình
backbone.
Framework tổng thể của OpenDelta được thể hiện trong
Hình 1. Tiếp theo, chúng tôi giới thiệu các triển khai
chính hỗ trợ việc xây dựng các đối tượng delta.

4.2 Các Triển khai Chính
Framework trên được thực hiện bằng bốn triển khai
chính, tức là địa chỉ hóa dựa trên tên, định tuyến lại
tensor động, khởi tạo thời gian chạy, và hệ thống trực
quan hóa.

Địa chỉ hóa Dựa trên Tên. Thứ nhất, chúng tôi cần
có được một con trỏ đến các sub-modules mong muốn
mà các delta modules được áp dụng. Trong thực tế,
chúng tôi có thể lấy được con trỏ một cách hiệu quả
bằng cách sử dụng tên của sub-module. Vì các sub-modules
được tổ chức trong cấu trúc cây, chúng tôi thực hiện
tìm kiếm theo chiều sâu để tìm các sub-modules phù
hợp với tên được cung cấp. Việc tìm kiếm này dẫn đến
một đường dẫn đầy đủ bao gồm tất cả các tên từ gốc
đến sub-module phù hợp, khớp chính xác với sub-module.
Tuy nhiên, việc viết trực tiếp đường dẫn đầy đủ đến
các sub-modules có thể không thực tế, vì vậy chúng tôi
thiết kế một số đơn giản hóa để làm cho việc địa chỉ hóa
dễ dàng hơn và dễ đọc hơn cho con người2. Một sự đơn
giản hóa như vậy liên quan đến việc tận dụng tính lặp
lại của các lớp transformer, mà nhiều phương pháp delta
tuning giải quyết bằng cách thêm các delta modules vào
cùng một loại sub-modules trong mỗi lớp. Ví dụ, khi
người dùng chỉ định attention, họ có thể có ý định áp
dụng các delta modules vào các sub-modules attention
trong tất cả các lớp transformer. Để giải quyết nhu cầu
này, chúng tôi cung cấp một cơ chế khớp đuôi tự động
khớp các sub-modules dựa trên tên của chúng. Đối với
các cấu hình phức tạp hơn của các vị trí, chúng tôi cho
phép khớp dựa trên biểu thức chính quy và lựa chọn
dựa trên web bằng cách sử dụng giao diện web được
thiết kế tùy chỉnh của chúng tôi.

Định tuyến lại Tensor Động. Một sự khác biệt cơ bản
khiến OpenDelta khác biệt với các triển khai khác là
khả năng thêm các delta modules mà không yêu cầu
bất kỳ sửa đổi nào đối với mã của các modules backbone.
Tính năng này đòi hỏi việc định tuyến lại tensor động
thông qua các delta modules và trở lại vào mô hình backbone.
Để đạt được việc định tuyến lại này, chúng tôi bao bọc
hàm forward gốc của một sub-module bằng một hàm
wrapper và thay thế hàm forward gốc bằng hàm wrapper.
Để đảm bảo việc thay thế liền mạch, chúng tôi sử dụng
một decorator để kế thừa các thuộc tính của hàm gốc,
bao gồm I/O, doc string, v.v. Trong hàm được bao bọc,
chúng tôi triển khai ba tuyến đường riêng biệt của các
trạng thái ẩn, tính đến thứ tự của sub-module gốc và
delta module. Tuyến đường đầu tiên sử dụng trạng thái
ẩn đầu vào hin của mi như là cả mục tiêu sửa đổi và
đầu vào cho delta module. Chúng tôi truyền nó qua delta
module để có được đầu ra mδ(hin), và hợp nhất nó với
hin. Chính thức,
hin←hin+mδ(hin). (2)
Tuyến đường thứ hai sử dụng trạng thái ẩn đầu ra hout
của mi như là mục tiêu sửa đổi:
hout←hout+mδ(hout). (3)
Tuyến đường thứ ba tận dụng trạng thái ẩn đầu vào hin
như là đầu vào cho delta module, và đặt trạng thái ẩn
đầu ra hout như là mục tiêu sửa đổi:
hout←hout+mδ(hin). (4)
Mặc dù ba tuyến đường này không nhất thiết bao gồm
tất cả các mối quan hệ có thể có giữa delta module và
mô hình backbone, chúng đủ để hỗ trợ hầu hết các phương
pháp delta tuning phổ biến (như được minh họa trong
Bảng 1). Tuy nhiên, chúng tôi vẫn mở cho khả năng
kết hợp các tuyến đường bổ sung khi cần thiết.

--- TRANG 4 ---
Phương pháp Công thức Vị trí Mặc định Tuyến đường Khởi tạo Thời gian Chạy
LoRA mδ(hin) =hinAB Query, Value Pt.(4) N
Adapter mδ(hout) =σ(houtW1)W2 ATTN, FFN Pt.(3) Y
Bitfit mδ(hout) =b ATTN, FFN, LayerNorm Pt.(3) N
Prefix Tuning mδ(hout) = [ MLP(p);hout] Key, Value Pt.(3) Y
Bảng 1: Các phương pháp delta tuning và đặc điểm của chúng. Vị trí mặc định đề cập đến các vị trí mà các delta modules
được gắn vào khi không có sub-modules cụ thể nào được chỉ định. A,B,W1,W2 là các ma trận trọng số, b là vector bias.
MLP( ·) là một mạng nhận thức đa lớp. [·;·] biểu thị việc nối các tensor. σ là hàm kích hoạt. Khởi tạo Thời gian Chạy
cho thấy liệu việc triển khai có sử dụng kỹ thuật này trong OpenDelta hay không.

Khởi tạo Thời gian Chạy. Để đảm bảo rằng các ma
trận trọng số trong delta module khớp với các trạng
thái ẩn về hình dạng và chiều, chúng tôi phải tính đến
các trạng thái ẩn có hình dạng không được chỉ định trong
cấu hình mô hình. Trong các triển khai truyền thống,
điều này đòi hỏi việc kiểm tra thủ công mã của mô hình
backbone. Tuy nhiên, OpenDelta tự động hóa quá trình
này bằng cách truyền một đầu vào giả qua mô hình backbone,
cho phép các hình dạng của các trạng thái ẩn được xác
định tự động khi chúng lan truyền từ đầu vào đến đầu ra.
Hệ thống Trực quan hóa. Vì delta tuning cung cấp tính
linh hoạt và tính động, việc đảm bảo việc xây dựng đúng
các đối tượng delta bằng cách xác minh rằng các delta
modules được thêm vào như được chỉ định là rất quan
trọng. Tuy nhiên, việc in trực tiếp các mô hình được
huấn luyện trước lớn dẫn đến các đầu ra khổng lồ. Để
giải quyết vấn đề này, chúng tôi cung cấp một hệ thống
trực quan hóa tận dụng sự lặp lại trong kiến trúc transformer.
Cụ thể, chúng tôi thu gọn các lớp lặp lại và in gọn gàng
thông tin của các tham số. Với việc thêm các delta modules
vào mô hình backbone, người dùng có thể dễ dàng quan
sát những thay đổi được thực hiện trong mô hình thông
qua trực quan hóa. Một ví dụ về trực quan hóa có thể
được thấy trong Hình 3. Vì hệ thống trực quan hóa hữu
ích ngoài delta tuning, nó đã được tách thành một gói
độc lập có tên "bigmodelvis"3.

5 Sử dụng
Trong phần này, chúng tôi cung cấp các trường hợp sử
dụng của OpenDelta thể hiện ba đặc điểm của OpenDelta,
tức là tính đơn giản, tính modular, và khả năng mở rộng.

5.1 Tính đơn giản
Di chuyển từ Fine-tuning. Để tạo điều kiện cho việc
di chuyển từ tinh chỉnh toàn bộ tham số hiện có sang
delta tuning, chỉ cần một vài dòng sửa đổi mã, như được
minh họa trong Hình 2. Ban đầu, trong tinh chỉnh toàn
bộ tham số truyền thống, PTM được tải từ các thư viện
bên ngoài, chẳng hạn như Huggingface Transformers
(Dòng 1), và huấn luyện mô hình (Dòng 10). Để giới
thiệu delta tuning, dòng 3-8 được thêm vào và thực thi.
Để bắt đầu, một bước tùy chọn là trực quan hóa mô hình
backbone để xác định "modified_modules" mục tiêu.
Sau đó, một đối tượng delta, chẳng hạn như LoRA, được
tạo và gắn vào mô hình backbone. Tiếp theo, các tham
số mô hình, ngoại trừ các delta modules và đầu phân
loại được khởi tạo ngẫu nhiên, được đóng băng. Tham
số "set_state_dict=True" được sử dụng để loại bỏ các
tham số không thể huấn luyện khỏi checkpoint mô hình.
Cuối cùng, các sub-modules của backbone được trực
quan hóa để xác minh việc tạo và gắn thành công các
delta modules. Một ví dụ về kết quả trực quan hóa được
mô tả trong Hình 3.

Cơ chế AutoDelta. Việc triển khai OpenDelta hỗ trợ
các thiết kế rất phức tạp của các delta modules, phục
vụ cho các yêu cầu thí nghiệm đa dạng. Tuy nhiên, việc
cung cấp một cấu hình mặc định của các delta modules
cho các thực hành viên có thể không am hiểu cơ chế
của delta tuning là mong muốn. Tuy nhiên, các quy ước
đặt tên của các sub-modules khác nhau đáng kể giữa
các mô hình backbone khác nhau, mặc dù chúng có kiến
trúc transformer chung. Để giải quyết vấn đề này, chúng
tôi thiết lập một quy ước tên chung và sử dụng kỹ thuật
ánh xạ để ánh xạ quy ước tên cụ thể của mô hình sang
quy ước chung4. Điều này cho phép cơ chế AutoDelta
được hỗ trợ một cách liền mạch. Hình 5 minh họa rằng,
một khi loại phương pháp delta tuning được chỉ định,
các delta modules sẽ được gắn vào mô hình backbone
ở các vị trí mặc định và với các siêu tham số thích hợp.
Chúng tôi đã liệt kê các cấu hình mặc định của mỗi
phương pháp delta tuning trong Bảng 1. Hơn nữa, cơ
chế AutoDelta tạo điều kiện cho việc tải các checkpoint
đã tinh chỉnh của các delta modules, mà không cần kiến
thức rõ ràng về loại và siêu tham số của các delta modules.

1from opendelta import AutoDeltaModel,
AutoDeltaConfig
2# xây dựng một delta mới sử dụng
cấu hình mặc định.
3delta_config = AutoDeltaConfig.
from_dict({"delta_type":"lora"})
4delta_model = AutoDeltaModel.
from_config(delta_config,
backbone_model)
5# tải checkpoint delta.
6delta = AutoDeltaModel.from_finetuned(
"save_dir", backbone_model)
Hình 5: Một ví dụ về việc sử dụng cơ chế AutoDelta.

5.2 Tính Modular
Thuộc tính đáng chú ý thứ hai của OpenDelta là tính
modular. Nó mang lại khả năng gắn và tách một cách
độc lập mỗi đối tượng delta khỏi mô hình backbone,
do đó cung cấp khả năng phục vụ đa tác vụ với một
mô hình backbone duy nhất. Cụ thể, giả sử dữ liệu liên
quan đến các tác vụ khác nhau được trình bày tuần tự,
trong đó mỗi dữ liệu kích hoạt việc gắn một đối tượng
delta tương ứng vào mô hình backbone để xử lý, và
một khi hoàn thành, đối tượng delta được tách ra. Một
trường hợp minh họa chức năng này được thể hiện trong
Hình 4, trong đó ba tác vụ được xử lý tuần tự bằng một
mô hình backbone duy nhất.

5.3 Khả năng Mở rộng
Delta tuning là một trong những kỹ thuật quan trọng
cho phép sử dụng các PTMs lớn, và như vậy, chúng tôi
nỗ lực đảm bảo tính tương thích của nó với các kỹ thuật
khác như gia tốc mô hình và huấn luyện đa GPU. Cụ
thể, chúng tôi hiện tại cung cấp hỗ trợ cho framework
BMTrain5 với tối ưu hóa ZeRO-3 được kích hoạt (Rajbhandari
et al., 2020). Cũng đáng chú ý rằng chúng tôi có kế
hoạch mở rộng hỗ trợ cho các framework gia tốc mô
hình bổ sung trong tương lai.

--- TRANG 5 ---
1model = AutoModel.from_pretrained("bert-base-cased")
2
3+ from bigmodelvis import Visualization
4+ Visualization(model).structure_graph()
5+ from opendelta import LoraModel
6+ delta_model = LoraModel(backbone_model=model, modified_modules=["output.dense"
, "query"])
7+ delta_model.freeze_module(exclude=["deltas", "pooler"], set_state_dict=True)
8+ Visualization(model).structure_graph()
9
10trainer.train()
Hình 2: Một ví dụ về cách sử dụng cơ bản của OpenDelta. Dấu '+' chỉ ra mã bổ sung cần thiết để kích hoạt delta
tuning. Lưu ý rằng việc trực quan hóa có thể là tùy chọn nếu bạn quen thuộc với mô hình backbone.

Hình 3: Việc trực quan hóa trạng thái của mô hình backbone
sau khi các modules LoRA được gắn vào.

--- TRANG 6 ---
1def multi_task(delta_model, input_text):
2 global model # Chúng tôi sử dụng cùng một mô hình backbone cho các tác vụ.
3 delta_model.attach()
4 print(tokenizer.decode(model.generate(input_ids=tokenize(input_text))))
5 delta_model.detach()
6multi_task("What the commmon career of Newton ad einstein?", spelling_delta)
7# >>> "What was the common career of Newton and Einstein?"
8multi_task("What was the common career of Newton and Einstein?", topic_delta)
9# >>> "The question's topic is science."
10multi_task("What was the common career of Newton and Einstein?", question_delta
)
11# >>> "Physicists."
Hình 4: Học đa tác vụ qua OpenDelta. Do hạn chế về không gian, chúng tôi chỉ giữ lại mã cốt lõi. Để biết mã chi tiết,
vui lòng tham khảo tài liệu OpenDelta. Các chuỗi sau "> > >" thể hiện đầu ra của mô hình.

6 Kết luận
Tóm lại, OpenDelta là một thư viện cắm và chạy cho
delta tuning, cung cấp một giải pháp trực quan và modular
để điều chỉnh các PTMs lớn bằng delta tuning mà không
cần sửa đổi mã. Tính thân thiện với người dùng, tính
linh hoạt và khả năng mở rộng của thư viện làm cho nó
trở nên dễ tiếp cận và hữu ích cho cả các nhà nghiên
cứu và kỹ sư. Trong tương lai, chúng tôi dự định liên
tục cập nhật thư viện với các phương pháp delta tuning
mới và đảm bảo tính tương thích của nó với các phiên
bản mới nhất của các thư viện PTMs chính khác.

--- TRANG 7 ---
7 Lời cảm ơn
Công trình này được hỗ trợ bởi Chương trình R&D Chính
quốc gia của Trung Quốc (Số 2022ZD0116312), Quỹ
Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62236004),
Dự án Chính của Quỹ Khoa học Xã hội Quốc gia Trung
Quốc (Số 22ZD298).

Hạn chế
Mặc dù chúng tôi tin rằng OpenDelta đơn giản, dễ sử
dụng, linh hoạt và có thể mở rộng vì nó không yêu cầu
sửa đổi mã, nó vẫn bị hạn chế bởi nhiều chi tiết triển
khai. Ví dụ, một số phương pháp delta tuning, chẳng
hạn như Prefix Tuning, bị hạn chế bởi lý thuyết và chỉ
có thể được sử dụng trong các lớp Attention, làm cho
chúng không thể được chỉ định tùy ý. Đây cũng là lý
do tại sao chúng tôi không sử dụng nó như một ví dụ
trong bài báo này. Mặt khác, một số mô hình cơ bản
khác biệt đáng kể so với các triển khai chính thống, làm
cho việc sử dụng cơ chế AutoDelta trở nên khó khăn.
Do đó, chúng tôi duy trì một danh sách các mô hình đã
được kiểm tra có thể sử dụng AutoDelta, trong khi các
mô hình khác vẫn có thể sử dụng OpenDelta theo cách
tùy chỉnh. Thứ ba, mặc dù về mặt lý thuyết tương thích
với các framework gia tốc khác ngoài BMTrain, chẳng
hạn như Deepspeed, có một số chi tiết triển khai hiện
tại hạn chế tính tương thích của một số chức năng. Chúng
tôi sẽ cố gắng hết sức để giao tiếp với người bảo trì
của những gói đó để tăng tính tương thích.

Cân nhắc Đạo đức
Trong quá trình viết bài báo này, ChatGPT (OpenAI,
2022) đã được sử dụng để sửa đổi và tinh chỉnh. Tuy
nhiên, các tác giả có thể đảm bảo rằng mỗi câu trong
bài báo này đã được xem xét và kiểm tra kỹ lưỡng để
truyền đạt chính xác ý định của các tác giả.

Tài liệu tham khảo
Rishi Bommasani, Drew A Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosse-
lut, Emma Brunskill, et al. 2021. On the opportuni-
ties and risks of foundation models. arXiv preprint
arXiv:2108.07258 .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In North American Chapter of the Association
for Computational Linguistics .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, et al. 2022. Delta tuning:
A comprehensive study of parameter efficient meth-
ods for pre-trained language models. arXiv preprint
arXiv:2203.06904 .
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2022. Towards a
unified view of parameter-efficient transfer learning.
InInternational Conference on Learning Representa-
tions .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In Pro-
ceedings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. ArXiv preprint ,
abs/2106.09685.
Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang,
Yasheng Wang, Zhiyuan Liu, and Maosong Sun.
2022. Sparse structure search for delta tuning. In In
proceedings of NeurIPS .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language

--- TRANG 8 ---
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv preprint , abs/1907.11692.
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efficient low-
rank hypercomplex adapter layers. ArXiv preprint ,
abs/2106.04647.
OpenAI. 2022. Chatgpt: Optimizing language models
for dialogue.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In North American Chapter of the Associ-
ation for Computational Linguistics .
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume , pages
487–503, Online. Association for Computational Lin-
guistics.
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya
Kamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun
Cho, and Iryna Gurevych. 2020. AdapterHub: A
framework for adapting transformers. In Proceedings
of the 2020 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 46–54, Online. Association for Computational
Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. ArXiv preprint , abs/1910.10683.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations
toward training trillion parameter models. In SC20:
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16. IEEE.
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2020. Adapterdrop: On the effi-
ciency of adapters in transformers. arXiv preprint
arXiv:2010.11918 .
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2021. AdapterDrop: On the efficiency of
adapters in transformers. In Proceedings of EMNLP ,
pages 7930–7946.
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.
Lst: Ladder side-tuning for parameter and mem-
ory efficient transfer learning. arXiv preprint
arXiv:2206.06522 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998–6008.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface's transformers: State-of-
the-art natural language processing. arXiv preprint
arXiv:1910.03771 .
Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. 2021. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-
models. ArXiv preprint , abs/2106.10199.
