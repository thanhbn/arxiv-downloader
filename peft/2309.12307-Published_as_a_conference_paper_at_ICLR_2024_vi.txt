# 2309.12307.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.12307.pdf
# Kích thước file: 1168784 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
LONG LORA: TINH CHỈNH HIỆU QUẢ CỦA CÁC MÔ HÌNH NGÔN NGỮ LỚN NGỮ CẢNH DÀI
Yukang Chen1Shengju Qian1Haotian Tang2Xin Lai1
Zhijian Liu2Song Han2,3Jiaya Jia1
1CUHK2MIT3NVIDIA
TÓM TẮT
Chúng tôi trình bày LongLoRA, một phương pháp tinh chỉnh hiệu quả mở rộng kích thước ngữ cảnh của các mô hình ngôn ngữ lớn (LLM) được huấn luyện trước, với chi phí tính toán hạn chế. Thông thường, huấn luyện LLM với kích thước ngữ cảnh dài tốn kém về mặt tính toán, đòi hỏi nhiều giờ huấn luyện và tài nguyên GPU. Ví dụ, huấn luyện với độ dài ngữ cảnh 8192 cần 16× chi phí tính toán trong các lớp self-attention so với 2048. Trong bài báo này, chúng tôi tăng tốc việc mở rộng ngữ cảnh của LLM theo hai khía cạnh. Một mặt, mặc dù cần attention toàn cục dày đặc trong quá trình suy luận, việc tinh chỉnh mô hình có thể được thực hiện hiệu quả bằng sparse local attention. Shifted sparse attention (S2-Attn) được đề xuất cho phép mở rộng ngữ cảnh hiệu quả, dẫn đến tiết kiệm tính toán đáng kể với hiệu suất tương tự như tinh chỉnh với vanilla attention. Đặc biệt, nó có thể được triển khai chỉ với hai dòng code trong huấn luyện, trong khi tùy chọn trong suy luận. Mặt khác, chúng tôi xem xét lại chế độ tinh chỉnh hiệu quả tham số cho việc mở rộng ngữ cảnh. Đáng chú ý, chúng tôi thấy rằng LoRA cho mở rộng ngữ cảnh hoạt động tốt dưới tiền đề của embedding và normalization có thể huấn luyện. LongLoRA kết hợp LoRA cải tiến này với S2-Attn. LongLoRA thể hiện kết quả thực nghiệm mạnh mẽ trên các nhiệm vụ khác nhau trên các mô hình Llama2 từ 7B/13B đến 70B. LongLoRA mở rộng Llama2 7B từ ngữ cảnh 4k lên 100k, hoặc Llama2 70B lên 32k trên một máy 8×A100 duy nhất. LongLoRA mở rộng ngữ cảnh mô hình trong khi duy trì kiến trúc gốc, và tương thích với hầu hết các kỹ thuật hiện có, như Flash-Attention2. Ngoài ra, chúng tôi tiến hành supervised fine-tuning với LongLoRA và bộ dữ liệu LongAlpaca theo hướng dẫn dài. Tất cả code, mô hình, dữ liệu và demo có sẵn tại github.com/dvlab-research/LongLoRA.

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
!Có thể huấn luyện
❄Đóng băngNormpostMul(-headSelf-A1en(onFeed ForwardNorminput++Lora
!
!Embedding
!
!
❄(b) Thích ứng hạng thấp
❄Mẫu 1 -không có dịch chuyển(a) Shifted sparse attention Mỗi mẫu trong một nửa heads
a
!Norm (0.004%)a
!Lora (0.12%) a
❄Linear Projection (96%) a
!Embedding (1.94%)a
❄Head (1.94%) (c) Tỷ lệ tham số (LLaMA7B ví dụ) Mẫu 2 -có dịch chuyển Kết hợp

Hình 2: Tổng quan về LongLoRA. Chúng tôi giới thiệu Shifted Sparse Attention (S2-Attn) trong quá trình tinh chỉnh. Mô hình được huấn luyện giữ nguyên standard self-attention gốc tại thời điểm suy luận. Ngoài việc huấn luyện trọng số LoRA trong các lớp tuyến tính, LongLoRA còn làm cho các lớp embedding và normalization có thể huấn luyện được. Việc mở rộng này rất quan trọng cho việc mở rộng ngữ cảnh, và chỉ giới thiệu một số lượng tối thiểu tham số có thể huấn luyện bổ sung.

Tuy nhiên, kích thước được xác định trước hạn chế LLM trong nhiều ứng dụng, như tóm tắt tài liệu dài hoặc trả lời câu hỏi dài. Để giải quyết hạn chế này, một số công trình gần đây (Chen et al., 2023; Tworkowski et al., 2023; Mohtashami & Jaggi, 2023) huấn luyện hoặc tinh chỉnh LLM với ngữ cảnh dài hơn. Tuy nhiên, huấn luyện LLM từ đầu với chuỗi dài đặt ra thách thức tính toán, và tinh chỉnh LLM được huấn luyện trước hiện có cũng khá tốn kém. Ví dụ, Position Interpolation (Chen et al., 2023) đã dành 32 GPU A100 để mở rộng mô hình LLaMA từ 2k lên 8k ngữ cảnh, và 128 GPU A100 cho tinh chỉnh ngữ cảnh dài hơn. FOT (Tworkowski et al., 2023) đã sử dụng 32 TPU cho huấn luyện transformer tiêu chuẩn và 128 TPU cho LongLLaMA. Những tài nguyên tính toán này thường không phải là tầm với của các nhà nghiên cứu thông thường, điều này tự nhiên dẫn chúng ta đến câu hỏi: liệu chúng ta có thể mở rộng cửa sổ ngữ cảnh của LLM một cách hiệu quả?

Một phương pháp đơn giản là tinh chỉnh LLM được huấn luyện trước thông qua low-rank adaptation (LoRA) (Hu et al., 2022). LoRA sửa đổi các lớp linear projection trong các khối self-attention bằng cách sử dụng ma trận hạng thấp, thường hiệu quả và giảm số lượng tham số có thể huấn luyện. Tuy nhiên, các phát hiện thực nghiệm của chúng tôi chỉ ra rằng huấn luyện mô hình ngữ cảnh dài theo cách này không đủ hiệu quả cũng như không đủ hiệu suất. Về mặt hiệu suất, thích ứng hạng thấp đơn thuần dẫn đến perplexity cao trong mở rộng ngữ cảnh dài, như trong Bảng 2. Tăng hạng lên giá trị cao hơn, ví dụ hạng = 256, không làm giảm vấn đề này. Về mặt hiệu quả, bất kể có sử dụng LoRA hay không, chi phí tính toán tăng mạnh khi kích thước ngữ cảnh mở rộng, chủ yếu do cơ chế self-attention tiêu chuẩn (Vaswani et al., 2017). Như thể hiện trong Hình 1, ngay cả với LoRA, thời gian huấn luyện cho mô hình Llama2 tiêu chuẩn tăng đáng kể khi cửa sổ ngữ cảnh mở rộng.

Trong công trình này, chúng tôi giới thiệu LongLoRA, một phương pháp tinh chỉnh hiệu quả mở rộng cửa sổ ngữ cảnh của LLM được huấn luyện trước, ví dụ Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) sử dụng cập nhật trọng số hạng thấp để xấp xỉ tinh chỉnh đầy đủ. Tương tự, chúng tôi thấy rằng short attention cũng có thể xấp xỉ ngữ cảnh dài trong quá trình huấn luyện. Chúng tôi trình bày shifted sparse attention (S2-Attn) như một thay thế hiệu quả cho standard self-attention. Như thể hiện trong Hình 2, chúng tôi chia độ dài ngữ cảnh thành nhiều nhóm và thực hiện attention trong mỗi nhóm riêng biệt. Trong một nửa attention heads, chúng tôi dịch chuyển các token theo nửa kích thước nhóm, điều này đảm bảo luồng thông tin giữa các nhóm láng giềng. Ví dụ, chúng tôi sử dụng S2-Attn với kích thước nhóm 2048 để xấp xỉ tổng độ dài ngữ cảnh 8192 trong huấn luyện. Điều này chia sẻ tinh thần cấp cao với Swin Transformer (Liu et al., 2021).

Các mô hình được tinh chỉnh thông qua S2-Attn giữ nguyên kiến trúc attention gốc trong quá trình suy luận. Điều này tạo điều kiện thuận lợi cho hầu hết các tối ưu hóa và cơ sở hạ tầng hiện có. Các kỹ thuật cho LLM thông thường cũng có thể được áp dụng cho chúng tôi. Ví dụ, Flash-Attention2 (Dao et al., 2022; Dao, 2023) tương thích với phương pháp của chúng tôi trong cả thời gian huấn luyện và suy luận. Lý do đằng sau điều này là short attention giống với sơ đồ attention trong giai đoạn pre-training của LLM. Các efficient attention khác, ví dụ dilated hoặc sparse attention, có khoảng cách lớn với phong cách tiêu chuẩn và không hoạt động tốt như chúng tôi, như trong Bảng 6.

Chúng tôi thực nghiệm cho thấy rằng các lớp embedding và normalization có thể học được là chìa khóa để mở khóa tinh chỉnh LoRA ngữ cảnh dài, trong Bảng 2. Các lớp embedding và normalization chiếm tỷ lệ nhỏ

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bước 2 Dịch chuyển tokens Bước 3 Reshape Kích thước nhóm Nửa nhóm
Self-attention trong mỗi nhóm Tokens Multi-heads Self-attention giữa tất cả tokens Self-attention Shifted short attention Bước 1 Chia heads Nửa heads
Bước 2 Dịch chuyển Bước 3 Nhóm Nửa
S2-Attn Bước 1 Chia Nửa heads Attention không dịch chuyển!" Attention có dịch chuyển 12345678 Tokens Heads 123456788123456712345678123456781234567881234567 Nhóm
Mẫu 1!" Mẫu 2 Mẫu 1 Mẫu 2 Inputs Chia attention heads thành 2 phần Dịch chuyển phần thứ 2 theo nửa kích thước nhóm

Hình 3: Minh họa S2-Attn. Nó bao gồm ba bước. Đầu tiên, nó chia các đặc trưng dọc theo chiều head thành hai phần. Thứ hai, các token trong một trong các phần được dịch chuyển theo nửa kích thước nhóm. Thứ ba, chúng tôi chia các token thành các nhóm và reshape chúng thành các chiều batch. Attention chỉ tính toán trong mỗi nhóm trong khi thông tin chảy giữa các nhóm thông qua dịch chuyển. Rò rỉ thông tin có thể xảy ra do dịch chuyển, trong khi điều này dễ dàng ngăn chặn qua một sửa đổi nhỏ trên attention mask. Chúng tôi ablate điều này trong biến thể 2 trong Phần B.3 trong phụ lục.

tham số trong toàn bộ LLM. Ví dụ, embedding có (<2%) tham số, và normalization có (≤0.004%) tham số trong Llama2 7B. Tỷ lệ này giảm đối với LLM thậm chí còn lớn hơn. Trong các thí nghiệm, chúng tôi cho thấy rằng LongLoRA hiệu quả và hiệu suất. Chúng tôi trình bày kết quả thực nghiệm mở rộng cửa sổ ngữ cảnh cho Llama2 7B, 13B, và 70B. Theo các thiết lập thực nghiệm của Position Interpolation (Chen et al., 2023), chúng tôi tinh chỉnh các mô hình với position embeddings thích hợp. Các mô hình được huấn luyện đạt hiệu suất so sánh với kết quả full-attention và fully fine-tuned, trong khi chi phí tính toán thấp hơn nhiều như thể hiện trong Hình 1. LongLoRA có thể tinh chỉnh Llama2 7B lên 100k ngữ cảnh, hoặc mô hình 70B lên 32k, trên một máy 8×A100 duy nhất.

Ngoài ra, chúng tôi trình bày một giải pháp cho supervised fine-tuning (SFT) với bộ dữ liệu theo hướng dẫn dài tự thu thập của chúng tôi, LongAlpaca. Các mô hình LongLoRA của chúng tôi được tinh chỉnh thêm với các câu hỏi dài và câu trả lời tương ứng. Chúng tôi thiết kế các loại câu hỏi khác nhau cho các bài báo kỹ thuật, tiểu thuyết khoa học viễn tưởng, và các sách khác. SFT quan trọng để cải thiện khả năng trò chuyện của LLM. Chúng tôi giới thiệu các thiết lập SFT trong Phần B.6 trong phụ lục.

2 CÔNG TRÌNH LIÊN QUAN

Long-context Transformers. Một khối lượng lớn nghiên cứu đã được phát triển để tăng độ dài ngữ cảnh của transformer. Một số phương pháp này dựa trên retrieval (Karpukhin et al., 2020; Izacard et al., 2022; Guu et al., 2020), tăng cường mô hình ngôn ngữ thông qua việc lấy các tài liệu liên quan và bao gồm kết quả được lấy vào ngữ cảnh. Công trình của chúng tôi bổ sung cho các công trình này, vì cơ chế attention của chúng tôi không thay đổi trong quá trình suy luận. Nhiều công trình sửa đổi multi-head attention thành những phiên bản xấp xỉ (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Bulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020). Chúng làm giảm độ phức tạp bậc hai của tính toán self-attention. Ví dụ, Longformer (Beltagy et al., 2020) và BigBird (Zaheer et al., 2020) sử dụng sparse attention để xử lý chuỗi dài. Các công trình khác (Wu et al., 2022; Bulatov et al., 2022) sử dụng cơ chế bộ nhớ như một nén trên các input trong quá khứ, để tra cứu các token liên quan. Một hạn chế của các công trình này là các nén này có khoảng cách lớn với full attention, khiến việc tinh chỉnh LLM được huấn luyện trước không khả thi. Mặc dù công trình của chúng tôi cũng liên quan đến một xấp xỉ của cơ chế attention, nó có hình dạng tương tự và khoảng cách nhỏ với standard attention. Điều này cho phép tinh chỉnh LLM được huấn luyện trước trên S2-Attn và duy trì full attention trong quá trình suy luận.

Long-context LLMs. LLM thường được huấn luyện trước với độ dài ngữ cảnh được xác định trước, như 2048 cho LLaMA (Touvron et al., 2023a) và 4096 cho Llama2 (Touvron et al., 2023b). Huấn luyện LLM với ngữ cảnh dài từ đầu quá tốn kém cho hầu hết các nhà nghiên cứu. Gần đây, nhiều công trình đã cố gắng mở rộng độ dài ngữ cảnh của LLM thông qua tinh chỉnh. Position Interpolation (Chen et al., 2023) sửa đổi rotary position encoding (Su et al., 2021) và mở rộng độ dài ngữ cảnh của LLaMA lên 32768. Focused Transformer (Tworkowski et al., 2023) sử dụng contrastive learning để huấn luyện LongLLaMA. Cả hai đều dựa vào tinh chỉnh đầy đủ, điều này tốn kém về mặt tính toán (128 GPU A100 / 128 TPUv3 cho huấn luyện). Landmark attention (Mohtashami & Jaggi, 2023) là một

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: Hiệu quả của S2-Attn dưới các độ dài ngữ cảnh khác nhau. 'Short' có nghĩa là 1/4 của độ dài ngữ cảnh mục tiêu, trong khi 'Long' bằng độ dài ngữ cảnh mục tiêu. Các mô hình được tinh chỉnh đầy đủ trên mô hình Llama2 (Touvron et al., 2023b) với 7B tham số trên bộ dữ liệu RedPajama (Computer, 2023). Kết quả được kiểm tra bằng perplexity trên tập validation PG19 (Rae et al., 2020).

Thiết lập Position Embedding Training Target Context Length
Attention Shift 8192 16384 32768

Full Attn
PI (Chen et al., 2023) Long - 8.02 8.05 8.04
Short Attn Short ✗ 8.29 8.83 9.47
S2-Attn Short ✓ 8.04 8.03 8.08

phương pháp hiệu quả, nhưng có phần tổn hao. Nó nén các input ngữ cảnh dài thành các token được truy xuất. Phương pháp của chúng tôi tiết kiệm chi phí tinh chỉnh đáng kể, trong khi bảo toàn chất lượng của attention gốc. Chúng tôi duy trì quyền truy cập đầy đủ vào toàn bộ input thông qua attention không thay đổi trong quá trình suy luận.

Một số tài liệu tập trung vào việc sửa đổi position embedding của LLM để mở rộng ngữ cảnh dài, bao gồm Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023), positional Skipping (Zhu et al., 2023), và các phương pháp dựa trên phân tích out-of-distribution (Han et al., 2023). Phương pháp của chúng tôi tập trung vào tinh chỉnh hiệu quả và duy trì kiến trúc gốc trong quá trình suy luận, điều này trực giao với các phương pháp position embedding này.

Efficient Fine-tuning. Công trình này dựa trên LoRA (Hu et al., 2022), một phương pháp tinh chỉnh hiệu quả cổ điển. Ngoài LoRA (Hu et al., 2022), còn có nhiều phương pháp tinh chỉnh hiệu quả tham số khác, bao gồm prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state tuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), và masked weight learning (Sung et al., 2021). Input-tuning (An et al., 2022) giới thiệu một adapter để tinh chỉnh input embedding. Mặc dù các lớp input embedding cũng có thể huấn luyện được trong chúng tôi, điều này không đủ cho mở rộng ngữ cảnh dài. Chúng tôi thực hiện phân tích toàn diện về các loại lớp trong thí nghiệm, trong Bảng 2. Công trình hiện có (Chen et al., 2022) cho thấy sparse masks có thể hiệu quả tiết kiệm chi phí huấn luyện và tránh suy giảm hiệu suất.

3 LONG LORA

3.1 KIẾN THỨC NỀN TẢNG

Transformer. LLM thường được xây dựng với transformer. Lấy Llama2 (Touvron et al., 2023b) làm ví dụ, như thể hiện trong Hình 2, một mô hình LLM bao gồm một lớp embedding input và một số lớp decoder. Mỗi lớp decoder bao gồm một module self-attention. Nó ánh xạ các đặc trưng input thành một tập các queries, keys, và values {q, k, v}, thông qua các lớp linear projection với ma trận trọng số {Wq, Wk, Wv}. Với {q, k, v}, nó tính toán các output o như

o = softmax(qkT)v (1)

Các output sau đó được chiếu bởi một lớp tuyến tính với ma trận trọng số Wo. Và các lớp MLP theo sau. Trước và sau các module self-attention, layer normalization (Ba et al., 2016) được áp dụng. Một normalization cuối cùng được thực hiện sau tất cả các lớp decoder.

Đối với chuỗi dài, self-attention gặp khó khăn với chi phí tính toán, có độ phức tạp bậc hai theo độ dài chuỗi. Điều này làm chậm đáng kể quá trình huấn luyện và tăng chi phí bộ nhớ GPU.

Low-rank Adaptation. LoRA (Hu et al., 2022) giả thuyết rằng các cập nhật trọng số trong mô hình được huấn luyện trước có hạng nội tại thấp trong quá trình thích ứng. Đối với ma trận trọng số được huấn luyện trước W∈Rd×k, nó được cập nhật với phân rã hạng thấp W + ∆W = W + BA, trong đó B∈Rd×r và A∈Rr×k. Hạng r ≪ min(d, k). Trong quá trình huấn luyện, W được đóng băng không có cập nhật gradient, trong khi A và B có thể huấn luyện được. Đây là lý do tại sao huấn luyện LoRA hiệu quả hơn nhiều so với tinh chỉnh đầy đủ.

Trong cấu trúc Transformer, LoRA chỉ thích ứng các trọng số attention (Wq, Wk, Wv, Wo) và đóng băng tất cả các lớp khác, bao gồm MLP và các lớp normalization. Cách này đơn giản và hiệu quả tham số. Tuy nhiên, chúng tôi thực nghiệm cho thấy rằng chỉ thích ứng hạng thấp trong trọng số attention không hoạt động cho mở rộng ngữ cảnh dài.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Thuật toán 1: Pseudocode của S2-Attn theo phong cách PyTorch.
# B: batch size; S: sequence length hoặc số lượng tokens; G: group size;
# H: số lượng attention heads; D: chiều của mỗi attention head
# qkv có shape (B, N, 3, H, D), queries, keys, và values đã chiếu
# Dòng chính 1: chia qkv trên H thành 2 phần, và dịch chuyển G/2 trên N
qkv = cat((qkv.chunk(2, 3)[0], qkv.chunk(2, 3)[1].roll(-G/2, 1)), 3).view(B *N/G,G,3,H,D)
# hàm self-attention tiêu chuẩn
out = self_attn(qkv)
# out có shape (B, N, H, D)
# Dòng chính 2: chia out trên H thành 2 phần, và sau đó roll back G/2 trên N
out = cat((out.chunk(2, 2)[0], out.chunk(2, 2)[1].roll(G/2, 1)), 2)
cat: nối; chunk: chia thành số lượng phần được chỉ định; roll: roll tensor dọc theo chiều đã cho.

3.2 SHIFTED SPARSE ATTENTION

Standard self-attention tốn O(n²) tính toán, khiến LLM trên chuỗi dài có chi phí bộ nhớ cao và chậm. Để tránh vấn đề này trong quá trình huấn luyện, chúng tôi đề xuất Shifted Sparse Attention (S2-Attn), như thể hiện trong Hình 2. Dưới đây, chúng tôi thực hiện một nghiên cứu thí điểm và giải thích thiết kế từng bước.

Nghiên cứu thí điểm. Trong Bảng 1, chúng tôi xây dựng một baseline tiêu chuẩn được huấn luyện và kiểm tra với full attention và fine-tuning, thể hiện chất lượng nhất quán tốt trong các độ dài ngữ cảnh khác nhau. Thử nghiệm đầu tiên là huấn luyện với short attention, chỉ mẫu 1 trong Hình 2. Như chúng ta biết đối với ngữ cảnh dài, chi phí cao chủ yếu đến từ các module self-attention. Do đó, trong thử nghiệm này, vì input dài, chúng tôi chia thành nhiều nhóm trong self-attention. Ví dụ, mô hình nhận 8192 tokens làm input trong cả giai đoạn huấn luyện và kiểm tra, nhưng self-attention được thực hiện trong mỗi nhóm với kích thước 2048. Số lượng nhóm là 4, như được ablated trong Phần B.2 trong phụ lục. Mẫu này hiệu quả nhưng vẫn không hoạt động trong ngữ cảnh rất dài, như thể hiện trong Bảng 1. Perplexity trở nên lớn hơn khi độ dài ngữ cảnh tăng. Lý do đằng sau điều này là không có trao đổi thông tin giữa các nhóm khác nhau.

Để giới thiệu giao tiếp giữa các nhóm, chúng tôi bao gồm một mẫu dịch chuyển, như thể hiện trong Hình 2. Chúng tôi dịch chuyển phân vùng nhóm theo nửa kích thước nhóm trong một nửa attention heads. Lấy tổng độ dài ngữ cảnh 8192 làm ví dụ, trong mẫu 1, nhóm đầu tiên thực hiện self-attention từ token thứ 1 đến thứ 2048. Trong Mẫu 2, phân vùng nhóm được dịch chuyển 1024. Nhóm attention đầu tiên bắt đầu từ thứ 1025 và kết thúc ở thứ 3072, trong khi 1024 tokens đầu tiên và cuối cùng thuộc về cùng một nhóm. Chúng tôi sử dụng mẫu 1 và 2 trong mỗi nửa self-attention heads tương ứng. Cách này không tăng chi phí tính toán bổ sung nhưng cho phép luồng thông tin giữa các nhóm khác nhau. Chúng tôi cho thấy rằng nó gần với baseline attention tiêu chuẩn trong Bảng 1.

Tính nhất quán với Full Attention. Các thiết kế efficient attention hiện có cũng có thể cải thiện hiệu quả của LLM ngữ cảnh dài. Tuy nhiên, hầu hết chúng không phù hợp cho tinh chỉnh ngữ cảnh dài. Bởi vì, các transformer này (Qiu et al., 2020; Child et al., 2019), được thiết kế để huấn luyện từ đầu, có khoảng cách với standard full attention, được sử dụng trong pre-training. Trong Bảng 6, chúng tôi cho thấy rằng S2-Attn không chỉ cho phép tinh chỉnh hiệu quả mà còn hỗ trợ kiểm tra full attention. Mặc dù các attention khác cũng có thể được sử dụng trong tinh chỉnh ngữ cảnh dài, các mô hình phải được kiểm tra với attention được sử dụng trong quá trình tinh chỉnh. Shifting ngăn chặn các mô hình bị over-fit với các mẫu attention cụ thể.

Triển khai dễ dàng. S2-Attn dễ triển khai. Nó chỉ liên quan đến hai bước: (1) dịch chuyển tokens trong một nửa attention heads, và (2) chuyển đổi đặc trưng từ chiều token sang chiều batch. Hai dòng code là đủ. Chúng tôi cung cấp code theo phong cách PyTorch trong Thuật toán 1.

3.3 LORA CẢI TIẾN CHO NGỮ CẢNH DÀI

LoRA (Hu et al., 2022) là một cách hiệu quả và phổ biến để thích ứng LLM với các bộ dữ liệu khác. Nó tiết kiệm nhiều tham số có thể huấn luyện và chi phí bộ nhớ, so với tinh chỉnh đầy đủ. Tuy nhiên, thích ứng LLM từ độ dài ngữ cảnh ngắn sang dài không dễ dàng. Chúng tôi quan sát thực nghiệm một khoảng cách rõ ràng giữa LoRA và tinh chỉnh đầy đủ. Như thể hiện trong Bảng 2, khoảng cách giữa LoRA và tinh chỉnh đầy đủ tăng khi độ dài ngữ cảnh mục tiêu trở nên lớn hơn. Và LoRA với hạng lớn hơn không thể giảm khoảng cách.

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 2: Tinh chỉnh các lớp normalization và embedding rất quan trọng cho thích ứng ngữ cảnh dài hạng thấp. Các mô hình Llama2 7B (Touvron et al., 2023b) với S2-Attn được đề xuất được huấn luyện trên bộ dữ liệu RedPajama (Computer, 2023). Độ dài ngữ cảnh mục tiêu là 32768. '+ Normal / Embed' có nghĩa là các lớp normalization hoặc embedding có thể huấn luyện được. Kết quả perplexity được đánh giá trên tập validation PG19 (Rae et al., 2020). Đối với thích ứng ngữ cảnh dài, có một khoảng cách hiệu suất lớn giữa LoRA tiêu chuẩn (Hu et al., 2022) và tinh chỉnh đầy đủ. Không có normalization hoặc embedding có thể huấn luyện được, hạng lớn hơn trong LoRA không thể đóng khoảng cách này.

Phương pháp Full FT LoRA (rank) LoRA (rank = 8)
8 16 32 64 128 256 + Norm + Embed + Norm & Embed
PPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12

Bảng 3: Đánh giá perplexity trên tập test proof-pile (Rae et al., 2020). S2-Attn: Shifted Sparse Attention. LoRA+: LoRA cải tiến. Chúng tôi tinh chỉnh Llama2 (Touvron et al., 2023b) ở kích thước mô hình 7B và 13B trên bộ dữ liệu RedPajama (Computer, 2023) dưới độ dài ngữ cảnh 8k-32k. Chúng tôi cho thấy rằng phương pháp của chúng tôi đạt hiệu suất so sánh với các baseline full attention hoặc full FT, với hiệu quả tốt hơn. Chúng tôi sử dụng cùng thiết lập huấn luyện với mô hình được đánh giá trên PG19 (Rae et al., 2020) được giới thiệu trong Phần B.1 trong phụ lục.

Kích thước Training
Context Length LongLoRA Evaluation Context Length
S2-Attn LoRA+ 2048 4096 8192 16384 32768
7B 8192 3.14 2.85 2.66 - -
✓ 3.15 2.86 2.68 - -
✓ ✓ 3.20 2.91 2.72 - -
16384 ✓ 3.17 2.87 2.68 2.55 -
✓ ✓ 3.17 2.87 2.66 2.51 -
32768 ✓ 3.20 2.90 2.69 2.54 2.49
✓ ✓ 3.35 3.01 2.78 2.61 2.50
13B 8192 2.96 2.69 2.53 - -
✓ 3.01 2.74 2.57 - -
✓ ✓ 3.04 2.77 2.60 - -
16384 ✓ 2.99 2.72 2.53 2.40 -
✓ ✓ 3.03 2.74 2.55 2.41 -
32768 ✓ 3.04 2.75 2.56 2.42 2.33
✓ ✓ 3.05 2.76 2.57 2.42 2.32

Để thu hẹp khoảng cách này, chúng tôi mở các lớp embedding và normalization để huấn luyện. Như thể hiện trong Bảng 2, chúng chiếm tham số hạn chế nhưng có tác dụng cho thích ứng ngữ cảnh dài. Đặc biệt đối với các lớp normalization, các tham số chỉ chiếm 0.004% trong toàn bộ Llama2 7B. Chúng tôi ký hiệu phiên bản cải tiến này của LoRA là LoRA+ trong các thí nghiệm.

4 THÍ NGHIỆM

4.1 THIẾT LẬP THÍ NGHIỆM

Mô hình Chúng tôi mở rộng các mô hình Llama2 (Touvron et al., 2023b) được huấn luyện trước 7B, 13B và 70B. Kích thước cửa sổ ngữ cảnh mở rộng tối đa lên đến 100k cho mô hình 7B, 65536 cho mô hình 13B, và 32768 cho mô hình 70B. Các chỉ số vị trí cho các mô hình này được điều chỉnh lại với Position Interpolation (Chen et al., 2023).

Quy trình huấn luyện Chúng tôi tuân theo hầu hết các siêu tham số huấn luyện trong Position Interpolation (Chen et al., 2023), ngoại trừ batch size của chúng tôi nhỏ hơn vì chúng tôi sử dụng một máy 8×A100 GPU duy nhất trong một số trường hợp. Tất cả các mô hình được tinh chỉnh thông qua mục tiêu dự đoán token tiếp theo. Chúng tôi sử dụng AdamW (Loshchilov & Hutter, 2019) với β1 = 0.9 và β2 = 0.95. Tốc độ học được đặt là 2×10⁻⁵ cho mô hình 7B và 13B, và 10⁻⁵ cho mô hình 70B. Chúng tôi cũng sử dụng linear learning rate warmup. Weight decay là

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 4: Độ dài ngữ cảnh tối đa mà chúng tôi có thể tinh chỉnh cho các kích thước mô hình khác nhau trên một máy 8×A100 duy nhất. Chúng tôi sử dụng cùng thiết lập huấn luyện và đánh giá như trong Bảng 3. Chúng tôi sử dụng Flash-Attention2 (Dao, 2023) và DeepSpeed (Rasley et al., 2020) ở giai đoạn 3 trong quá trình tinh chỉnh. Với LongLoRA, độ dài ngữ cảnh tối đa cho mô hình 7B, 13B, và 70B lần lượt là 100k, 64k, và 32k. Đánh giá trên PG19 (Rae et al., 2020) trong Phần B.1 trong phụ lục.

Kích thước Training
Context Length Evaluation Context Length
2048 4096 8192 16384 32768 65536 100,000
7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52
13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 -
70B 32768 2.84 2.57 2.39 2.26 2.17 - -

Bảng 5: Đánh giá truy xuất chủ đề với LongChat (Li et al., 2023). Chúng tôi so sánh mô hình của chúng tôi với các LLM ngữ cảnh dài mã nguồn mở khác. Nhiệm vụ này liên quan đến việc truy xuất các chủ đề mục tiêu từ một cuộc trò chuyện rất dài với độ dài ngữ cảnh khoảng 3k, 6k, 10k, 13k, và 16k. Vì một số câu hỏi trong tập đánh giá dài hơn 16k, mô hình của chúng tôi được tinh chỉnh trên Llama2 13B. Nó đạt hiệu suất so sánh với LongChat-13B (Li et al., 2023) tiên tiến với chi phí tinh chỉnh thấp hơn.

Evaluation Context 3k 6k 10k 13k 16k
ChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02
MPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - -
MPT-7B-storywriter (Team, 2023b) 0.46 0.46 0.28 0.34 0.36
LongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9
Ours-13B 1.0 0.98 0.98 0.98 0.94

không. Chúng tôi đặt batch size trên mỗi thiết bị là 1 và các bước tích lũy gradient là 8, có nghĩa là batch size toàn cục bằng 64, sử dụng 8 GPU. Chúng tôi huấn luyện mô hình của chúng tôi trong 1000 bước.

Bộ dữ liệu Chúng tôi sử dụng bộ dữ liệu Redpajama (Computer, 2023) để huấn luyện. Chúng tôi đánh giá hiệu suất mô hình ngôn ngữ chuỗi dài của các mô hình được tinh chỉnh trên bộ dữ liệu book corpus PG19 (Rae et al., 2020) và bộ dữ liệu Arxiv Math proof-pile được làm sạch (Azerbayev et al., 2022). Chúng tôi sử dụng tập test của PG19 (Rae et al., 2020), bao gồm 100 tài liệu. Đối với bộ dữ liệu proof-pile, chúng tôi cũng sử dụng tập test để đánh giá. Chúng tôi tuân theo Position Interpolation (Chen et al., 2023) để xử lý dữ liệu proof-pile. Chúng tôi đánh giá perplexity bằng cách sử dụng phương pháp sliding window với S = 256, theo (Press et al., 2022).

4.2 KẾT QUẢ CHÍNH

Mô hình ngôn ngữ chuỗi dài. Trong Bảng 3, chúng tôi báo cáo perplexity cho các mô hình của chúng tôi và baseline trên bộ dữ liệu proof-pile (Azerbayev et al., 2022) và PG19. Dưới độ dài ngữ cảnh huấn luyện nhất định, các mô hình của chúng tôi đạt perplexity tốt hơn với kích thước ngữ cảnh dài hơn. Điều này chỉ ra hiệu quả của phương pháp tinh chỉnh hiệu quả của chúng tôi. Trong Bảng 3, đối với các trường hợp cùng độ dài ngữ cảnh huấn luyện và đánh giá, perplexity giảm khi kích thước ngữ cảnh tăng. Bằng cách tăng kích thước cửa sổ ngữ cảnh từ 8192 lên 32768, đối với mô hình Llama2 7B, chúng tôi quan sát rằng perplexity tốt hơn từ 2.72 xuống 2.50 với -0.22. Đối với mô hình Llama2 13B, chúng tôi quan sát rằng perplexity giảm -0.28.

Trong Bảng 4, chúng tôi tiếp tục kiểm tra độ dài ngữ cảnh tối đa mà chúng tôi có thể tinh chỉnh trên một máy 8×A100 duy nhất. Chúng tôi mở rộng Llama2 7B, 13B, và 70B lần lượt lên độ dài ngữ cảnh 100k, 65536, và 32768. LongLoRA đạt kết quả hứa hẹn trên các thiết lập cực kỳ lớn này. Ngoài ra, chúng tôi thấy một số suy giảm perplexity trên kích thước ngữ cảnh nhỏ cho các mô hình mở rộng. Đây là một hạn chế đã biết của Position Interpolation (Chen et al., 2023).

Đánh giá dựa trên truy xuất. Chúng tôi thực hiện thí nghiệm về truy xuất trong ngữ cảnh dài. Trong Bảng 5, chúng tôi so sánh mô hình của chúng tôi với các LLM mở khác trên nhiệm vụ truy xuất chủ đề được giới thiệu trong LongChat (Li et al., 2023). Nhiệm vụ này là truy xuất chủ đề mục tiêu từ một cuộc trò chuyện rất dài, với độ dài thay đổi từ 3k, 6k, 10k, 13k, đến 16k. Vì một số câu hỏi trong LongChat (Li et al., 2023) dài hơn 16k, chúng tôi tinh chỉnh Llama2 13B với độ dài ngữ cảnh 18k. Chi phí huấn luyện tương tự như với 16k.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
[Biểu đồ hiển thị độ chính xác truy xuất passkey so sánh giữa Llama2 7B và mô hình 7B được tinh chỉnh trên độ dài ngữ cảnh 32768]

Hình 4: So sánh độ chính xác trên passkey retrieval giữa Llama2 7B và mô hình 7B của chúng tôi được tinh chỉnh trên độ dài ngữ cảnh 32768. Mô hình của chúng tôi không có suy giảm độ chính xác truy xuất cho đến 33k hoặc 34k, vượt quá độ dài ngữ cảnh. Nó có thể tăng cường khả năng mô hình hóa chuỗi dài thông qua việc mở rộng đơn giản position embeddings, không cần tinh chỉnh thêm.

[Biểu đồ khác và biểu đồ perplexity theo các bước tinh chỉnh]

Hình 5: Ablation về các bước tinh chỉnh trong cả full fine-tuning và LoRA+. Chúng tôi tinh chỉnh Llama2 (Touvron et al., 2023b) 7B với S2-Attn được đề xuất. Độ dài ngữ cảnh mục tiêu là 8192. Chúng tôi sử dụng RedPajama (Computer, 2023) để huấn luyện và tập validation PG19 (Rae et al., 2020) để kiểm tra perplexity. Full fine-tuning hội tụ nhanh hơn LoRA+ ở đầu, nhưng khoảng cách hiệu suất cuối cùng là nhỏ.

Mô hình của chúng tôi đạt hiệu suất so sánh với LongChat-13B (Li et al., 2023), mô hình tiên tiến trong nhiệm vụ này. Không giống như LongChat-13B (Li et al., 2023), được tinh chỉnh đầy đủ trên văn bản hội thoại ngữ cảnh dài tự thu thập, mô hình của chúng tôi được thích ứng hiệu quả trên RedPajama (Computer, 2023) thông qua tạo token tiếp theo. Mô hình của chúng tôi thậm chí vượt trội hơn một chút so với LongChat-13B trong đánh giá 16k.

Trong Hình 4, chúng tôi trình bày độ chính xác truy xuất passkey của mô hình chúng tôi, theo Landmark Attention (Mohtashami & Jaggi, 2023). Nhiệm vụ này cũng được các tài liệu khác áp dụng (Chen et al., 2023; Tworkowski et al., 2023). Trong nhiệm vụ này, các mô hình cần tìm một passkey ngẫu nhiên được ẩn trong một tài liệu dài. Chúng tôi hiển thị định dạng tài liệu trong Phần A.2 trong phụ lục. Chúng tôi nghiên cứu Llama2 7B (Touvron et al., 2023b) và mô hình LongLoRA của chúng tôi tinh chỉnh Llama2 7B với độ dài ngữ cảnh 32768. Chúng tôi kiểm tra độ chính xác truy xuất passkey từ 1k đến 34k, với khoảng cách khoảng 1k (vì độ dài câu không thể được kiểm soát chính xác). Đối với mỗi độ dài tài liệu, chúng tôi kiểm tra mô hình 10 lần với các giá trị passkey ngẫu nhiên khác nhau. Mô hình của chúng tôi đạt độ chính xác truy xuất passkey hợp lý cho đến 33k hoặc 34k. Không cần tinh chỉnh thêm, chúng tôi sửa đổi max position embeddings lên 48k trong position interpolation, đó là Ours 7B (extended PI) trong Hình 4. Chúng tôi cho thấy rằng mô hình này có thể xử lý tài liệu dài hơn bằng cách đơn giản mở rộng position interpolation. Như đường cam đứt nét trong Hình 4, mô hình, được tinh chỉnh trên độ dài ngữ cảnh 32k, thể hiện khả năng truy xuất vừa phải (60%-90% độ chính xác) trong phạm vi 33k đến 45k. Ngay cả với position interpolation mở rộng, Llama2 7B gặp suy giảm độ chính xác mạnh (đường xanh đứt nét) sau độ dài ngữ cảnh 4k.

4.3 NGHIÊN CỨU ABLATION

Trong phần này, chúng tôi giới thiệu các nghiên cứu ablation về số lượng bước tinh chỉnh và các mẫu attention. Các kết quả thực nghiệm khác bao gồm ablation về kích thước nhóm, các biến thể attention, và phân tích hiệu quả trong Phần B trong phụ lục.

Ablation về các bước tinh chỉnh. Chúng tôi báo cáo mối quan hệ giữa perplexity và các bước tinh chỉnh cho mô hình Llama2 7B mở rộng lên độ dài ngữ cảnh 8192 trên tập validation PG19, trong

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 6: So sánh giữa S2-Attn và các mẫu attention thay thế trong quá trình tinh chỉnh. Chúng tôi thích ứng mô hình Llama2 7B lên độ dài ngữ cảnh 32768 với các mẫu attention khác nhau và LoRA cải tiến tại thời gian huấn luyện. Chúng tôi bao gồm bốn thiết kế efficient attention điển hình, ví dụ shift, dilate (Ding et al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) để so sánh. 'cro. heads / layers' có nghĩa là trao đổi các thiết lập attention khác nhau qua các attention heads hoặc các lớp tuần tự. Lấy S2-Attn làm ví dụ, 'cro. layers' là trao đổi giữa có và không có shift trong các lớp self-attention tuần tự. 'only P1/P2' có nghĩa là tất cả attention heads sử dụng mẫu 1 (tất cả không shift) hoặc Mẫu 2 (tất cả shift) trong Hình 2. Chúng tôi trực quan hóa các mẫu attention khác nhau trong Hình 7 trong phụ lục. Đối với mỗi mẫu attention, chúng tôi đánh giá hiệu suất của nó dưới hai giao thức. Trong hàng đầu tiên, chúng tôi sử dụng sparse attention trong cả huấn luyện và kiểm tra. Trong hàng thứ hai, chúng tôi sử dụng full attention để kiểm tra.

Test w/
Full-Attn S2-Attn Dilate Block sparse Stride sparse
cro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads
✗ 8.64 8.63 9.17 9.64 8.75 11.49 32.81
✓ 8.12 9.70 8.39 9.81 11.78 8.30 24.03

Hình 5. Chúng tôi thấy rằng không có tinh chỉnh, ở bước 0, mô hình có khả năng ngữ cảnh dài hạn chế, ví dụ 15.82 perplexity. Chúng tôi cho thấy rằng perplexity giảm nhanh chóng. Full fine-tuning hội tụ nhanh hơn huấn luyện hạng thấp. Chúng gần nhau sau 200 bước, không có khoảng cách lớn ở cuối.

Các mẫu attention. Trong Bảng 6, chúng tôi cho thấy tác động của các mẫu attention khác nhau trong quá trình tinh chỉnh. Chúng tôi tinh chỉnh mô hình Llama2 7B (Touvron et al., 2023b) lên độ dài ngữ cảnh 32768 trên bộ dữ liệu Redpajama (Computer, 2023) và đánh giá perplexity trên tập validation PG19 (Rae et al., 2020). Chúng tôi đầu tiên kiểm tra cách trao đổi giữa các thiết lập khác nhau. Đối với hoạt động shift mà chúng tôi sử dụng trong LongLoRA, có ba lựa chọn: tắt nó, shifting giữa các lớp tuần tự, và shifting giữa các attention heads. Chúng tôi cho thấy rằng shifting giữa các lớp có thể chấp nhận được nhưng không tốt nhất. Ngoài ra, đặt tất cả attention heads thành mẫu 1 hoặc mẫu 2 không hoạt động. Ngoài ra, chúng tôi thực nghiệm thấy rằng shifting trái hoặc phải có ít khác biệt trong hiệu suất.

Chúng tôi sau đó kiểm tra các loại thiết kế efficient attention khác, bao gồm dilated attention (Ding et al., 2023), block sparse attention (Qiu et al., 2020), và stride sparse attention (Child et al., 2019). Đối với dilated attention (Ding et al., 2023), chúng tôi thay đổi tỷ lệ dilate từ 1 đến 2 đều đặn giữa các attention heads. Đối với block sparse attention (Qiu et al., 2020), chúng tôi sử dụng n=4 ma trận masking theo khối trong attention heads và di chuyển khối sang trái để làm cho nó causal. Stride sparse attention (Child et al., 2019) chứa cả mẫu local và stride. Các thiết lập này chia sẻ chi phí tính toán tương tự. Chúng tôi trực quan hóa các mẫu này trong Hình 7 trong phụ lục. Các mẫu attention này được phát minh trong transformer huấn luyện-từ-đầu. Thí nghiệm này là để kiểm tra khả năng tinh chỉnh của chúng trên LLM được huấn luyện trước (Touvron et al., 2023b), hướng tới thích ứng ngữ cảnh dài. Dilated attention hoạt động tốt trong tinh chỉnh đầy đủ nhưng không tốt với thích ứng hạng thấp. Tinh chỉnh với stride sparse attention có hại. Chúng có khoảng cách lớn với full attention, được áp dụng trong giai đoạn pre-training.

5 KẾT LUẬN

Trong công trình này, chúng tôi đề xuất LongLoRA có thể mở rộng hiệu quả độ dài ngữ cảnh của LLM lên mức lớn hơn đáng kể. LongLoRA có chi phí bộ nhớ GPU và thời gian huấn luyện thấp hơn so với tinh chỉnh đầy đủ tiêu chuẩn, với sự thỏa hiệp độ chính xác tối thiểu. Ở cấp độ kiến trúc, chúng tôi đề xuất S2-Attn để xấp xỉ mẫu self-attention tiêu chuẩn trong quá trình huấn luyện. S2-Attn dễ triển khai, chỉ cần hai dòng code. Hơn nữa, các mô hình được huấn luyện thông qua S2-Attn giữ nguyên kiến trúc attention tiêu chuẩn gốc trong quá trình suy luận, làm cho hầu hết cơ sở hạ tầng và tối ưu hóa có sẵn trước đây có thể tái sử dụng. Ở cấp độ huấn luyện, chúng tôi thu hẹp khoảng cách giữa LoRA và tinh chỉnh đầy đủ với normalization và embedding có thể huấn luyện. Phương pháp của chúng tôi có thể mở rộng Llama2 7B lên độ dài ngữ cảnh 100k và mô hình 70B lên độ dài ngữ cảnh 32k, trên một máy 8×A100 duy nhất. Chúng tôi cũng trình bày bộ dữ liệu theo hướng dẫn dài, LongAlpaca và thực hiện supervised fine-tuning với LongLoRA. Chúng tôi tin rằng LongLoRA là một phương pháp tổng quát có thể tương thích với nhiều loại LLM và position encodings hơn. Chúng tôi dự định điều tra những điều này trong công trình tương lai.

Lời cảm ơn Chúng tôi muốn cảm ơn Xiuyu Li và Bohao Peng cho các cuộc thảo luận hữu ích.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
TÀI LIỆU THAM KHẢO

Ntk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/
comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_
have/ .

Byeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, và Aswin C. Sankaranarayanan. Neural
kaleidoscopic space sculpting. Trong CVPR, trang 4349–4358, 2023.

Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, và Xipeng Qiu.
L-eval: Instituting standardized evaluation for long context language models, 2023.

Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, và
Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,
abs/2203.03131, 2022.

Zhangir Azerbayev, Edward Ayers, và Bartosz Piotrowski. Proof-pile, 2022. URL https:
//github.com/zhangir-azerbayev/proof-pile .

Lei Jimmy Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. Layer normalization. CoRR ,
abs/1607.06450, 2016.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, và Juanzi Li. Longbench: A bilingual,
multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The long-document transformer.
CoRR, abs/2004.05150, 2020.

Aydar Bulatov, Yuri Kuratov, và Mikhail S. Burtsev. Recurrent memory transformer. Trong NeurIPS ,
2022.

Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher R ´e.
Pixelated butterfly: Simple and efficient sparse training for neural network models. Trong ICLR , 2022.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. Extending context window of
large language models via positional interpolation. CoRR, abs/2306.15595, 2023.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, abs/1904.10509, 2019.

Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
URL https://github.com/togethercomputer/RedPajama-Data .

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,
abs/2307.08691, 2023.

Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, và Christopher R ´e. Flashattention: Fast and
memory-efficient exact attention with io-awareness. Trong NeurIPS, 2022.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,
và Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,
2023.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. Glm:
General language model pretraining with autoregressive blank infilling. Trong ACL , trang 320–335,
2022.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. REALM: retrieval-
augmented language model pre-training. CoRR, abs/2002.08909, 2020.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, và Sinong Wang. Lm-infinite: Simple
on-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
và Weizhu Chen. Lora: Low-rank adaptation of large language models. Trong ICLR, 2022.

Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,
Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. Few-shot learning with
retrieval augmented language models. CoRR, abs/2208.03299, 2022.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, và Wen-tau Yih. Dense passage retrieval for open-domain question answering. Trong
EMNLP, trang 6769–6781, 2020.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong ICLR ,
2020.

Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt
tuning. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.),
EMNLP, trang 3045–3059, 2021.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,
Xuezhe Ma, và Hao Zhang. How long can open-source llms truly promise on context length?,
June 2023. URL https://lmsys.org/blog/2023-06-29-longchat .

Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. Trong
Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli (eds.), ACL, trang 4582–4597, 2021.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin
Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Trong
NeurIPS, 2022.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong ICCV , trang
9992–10002, 2021.

Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. Trong ICLR, 2019.

Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, và Sayak Paul. Peft: State-
of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/
peft , 2022.

Amirkeivan Mohtashami và Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. CoRR, abs/2305.16300, 2023.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
K¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. Trong NeurIPS, trang 8024–8035, 2019.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, và Enrico Shippole. Yarn: Efficient context window
extension of large language models. CoRR, abs/2309.00071, 2023.

Ofir Press, Noah A. Smith, và Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. Trong ICLR, 2022.

Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, và Raquel Urtasun. 3d graph neural networks for
RGBD semantic segmentation. Trong ICCV, trang 5209–5218, 2017.

Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, và Jie Tang. Blockwise self-
attention for long document understanding. Trong EMNLP , tập EMNLP 2020 của Findings of
ACL, trang 2555–2565, 2020.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap.
Compressive transformers for long-range sequence modelling. Trong ICLR, 2020.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. Deepspeed: System opti-
mizations enable training deep learning models with over 100 billion parameters. Trong KDD , trang
3505–3506. ACM, 2020.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. Roformer: Enhanced transformer with
rotary position embedding. CoRR, abs/2104.09864, 2021.

Yi-Lin Sung, Varun Nair, và Colin Raffel. Training neural networks with fixed sparse masks. Trong
NeurIPS, trang 24193–24205, 2021.

MosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,
2023a. URL www.mosaicml.com/blog/mpt-30b .

MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable
llms, 2023b. URL www.mosaicml.com/blog/mpt-7b .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ´elien Rodriguez, Armand
Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language
models. CoRR, abs/2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian
Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aur ´elien Rodriguez, Robert Stojnic, Sergey
Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,
abs/2307.09288, 2023b.

Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, và
Piotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,
2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. Attention is all you need. Trong NeurIPS, trang 5998–6008, 2017.

Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention
with linear complexity. CoRR, abs/2006.04768, 2020.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. Memorizing trans-
formers. Trong ICLR, 2022.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Onta ˜n´on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. Big bird:
Transformers for longer sequences. Trong NeurIPS, 2020.

Elad Ben Zaken, Yoav Goldberg, và Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. Trong Smaranda Muresan, Preslav Nakov, và Aline
Villavicencio (eds.), ACL, trang 1–9, 2022.

Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, và Dong Yu. Safeconv:
Explaining and correcting conversational unsafe behavior. Trong ACL, trang 22–35, 2023.

Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, và Sujian Li. Pose:
Efficient context window extension of llms via positional skip-wise training, 2023.

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
PHỤ LỤC

A THIẾT LẬP

A.1 MÔI TRƯỜNG

Tất cả các thí nghiệm của chúng tôi được thực hiện trên máy 8×A100. Chúng tôi huấn luyện tất cả các mô hình bằng PyTorch (Paszke et al., 2019) với DeepSpeed (Rasley et al., 2020) và Flash-Attention2 (Dao, 2023). Theo mặc định, chúng tôi sử dụng DeepSpeed (Rasley et al., 2020) ở giai đoạn 2 và sử dụng giai đoạn 3 cho các thí nghiệm độ dài ngữ cảnh tối đa. Gradient checkpoint được sử dụng theo mặc định, đây là kỹ thuật phổ biến trong codebase Peft (Mangrulkar et al., 2022). Lưu ý rằng đôi khi, 8×A100 GPU có thể không cần thiết và GPU 3090 Ti có thể chấp nhận được, như tinh chỉnh mô hình 7B lên kích thước ngữ cảnh 8192.

A.2 ĐỊNH DẠNG CỦA PASSKEY RETRIEVAL

Chúng tôi tuân theo các tài liệu hiện có (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023) cho định dạng tài liệu của passkey retrieval. Tài liệu có định dạng như sau:

There is an important info hidden inside a lot of irrelevant text.
Find it and memorize them. I will quiz you about the important
information there.
The grass is green. The sky is blue. The sun is yellow. Here we
go. There and back again. (repeat M times)
The pass key is 12362 . Remember it. 12362 is the pass key.
The grass is green. The sky is blue. The sun is yellow. Here we
go. There and back again. (repeat N times)
What is the pass key? The pass key is

Độ dài tài liệu thay đổi theo giá trị M và N. 12362 là số passkey cần truy xuất. Nó được lấy mẫu ngẫu nhiên và thay đổi ở mỗi lần kiểm tra.

B THÍ NGHIỆM

B.1 ĐÁNH GIÁ PERPLEXITY TRÊN TẬP TEST PG19.

Trong Bảng 14 và Bảng 15, chúng tôi trình bày kết quả đánh giá trên tập test PG19. Chúng tôi sử dụng cùng thiết lập với các mô hình trên đánh giá proof-pile (Azerbayev et al., 2022) trong bài báo. Tương tự, đối với mô hình được huấn luyện trên độ dài ngữ cảnh nhất định, khi độ dài ngữ cảnh đánh giá tăng, các mô hình của chúng tôi đạt perplexity tốt hơn. Lưu ý rằng perplexity trong Bảng 14 và Bảng 15 cao hơn so với bộ dữ liệu proof-pile, vì PG19 (Rae et al., 2020) có phong cách viết rất khác biệt.

B.2 ABLATION VỀ KÍCH THƯỚC NHÓM.

Trong Bảng 7, chúng tôi cung cấp nghiên cứu ablation về kích thước nhóm của S2-Attn. Chúng tôi thực nghiệm với tinh chỉnh Llama2 7B lên độ dài ngữ cảnh 8192 và 16384 thông qua LongLoRA. Kích thước nhóm thay đổi từ {1/2, 1/4, 1/6, 1/8} của độ dài ngữ cảnh mục tiêu. Ví dụ, kích thước nhóm là 1024 cho 1/8 của độ dài ngữ cảnh 8192. Chúng tôi thấy rằng các thiết lập 1/2 và 1/4 có khoảng cách nhỏ với tinh chỉnh full attention. Kích thước nhóm nhỏ hơn 1/4 sẽ không đủ tốt. Chúng tôi đặt kích thước nhóm là 1/4 của độ dài ngữ cảnh trong các thí nghiệm theo mặc định.

Bảng 7: Ablation về kích thước nhóm. Chúng tôi tinh chỉnh mô hình Llama2 7B lên độ dài ngữ cảnh 8192 và 16384 thông qua LongLoRA và đánh giá trên tập validation PG19. Chúng tôi thay đổi kích thước nhóm của S2-Attn từ {1/2, 1/4, 1/6, 1/8} của độ dài ngữ cảnh mục tiêu. 'Full' có nghĩa là standard full attention.

Context Length Full 1/2 1/4 1/6 1/8
8192 8.02 8.04 8.04 8.10 8.16
16384 7.82 7.84 7.86 7.94 7.98

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

B.3 ABLATION VỀ CÁC BIẾN THỂ CỦA S2-ATTN.

Trong Bảng 8, chúng tôi ablate một số biến thể của S2-Attn, được minh họa trong Hình 6. Biến thể 1 là thay đổi hướng shifting từ xuống lên trên. Nó cho thấy rằng hướng shifting không có tác động lên perplexity. Một lo ngại về S2-Attn là nó di chuyển các token cuối cùng lên phía trước vào một nhóm, điều này có thể không nhất quán với causal masks. Biến thể 2 sử dụng các nhóm riêng biệt cho các token được dịch chuyển, điều này ablate lo ngại này. Biến thể 3 hoán đổi các token dịch chuyển và token phía trước gốc, điều này cũng có thể ablate lo ngại. Chúng tôi cho thấy rằng các biến thể này trình bày perplexity tương tự như chúng tôi. Chúng tôi cho rằng mặc dù có giao tiếp giữa các token phía trước và cuối cùng, chúng ban đầu ở xa nhau trong khi nó bị hạn chế trong nhóm local. Hơn nữa, S2-Attn chỉ được sử dụng để tinh chỉnh, trong khi chúng tôi sử dụng standard causal masks và full attention trong quá trình suy luận. Biến thể 2 và 3 cũng hoạt động tốt nhưng liên quan đến các bước bổ sung cho chúng tôi.

Bảng 8: Ablation về các biến thể của S2-Attn. Các biến thể này được minh họa trong Hình 6. Tương tự như thiết lập trong Bảng 7, chúng tôi tinh chỉnh Llama2 7B lên ngữ cảnh 8192 và đánh giá trên tập validation PG19.

Attn Full Ours Variant 1 Variant 2 Variant 3
PPL 8.02 8.04 8.04 8.03 8.05

Bảng 9: Đánh giá trên benchmark LongBench (Bai et al., 2023). Trong mỗi cột, chúng tôi highlight giá trị cao nhất thành đậm và giá trị cao thứ hai bằng gạch dưới.

Model Avg Single-
Doc QA Multi-
Doc QA Summarization Few-shot
Learning Code Synthetic
GPT-3.5-Turbo 44.0 39.8 38.7 26.5 67.1 54.1 37.8
Llama2-7B-chat 31.0 24.9 22.6 24.7 60.0 48.1 5.9
LongChat-v1.5-7B 34.3 28.7 20.6 26.7 60.0 54.1 15.8
Vicuna-v1.5-7B 31.9 28.0 18.6 26.0 66.2 47.3 5.5
Ours-7B 36.8 28.7 28.1 27.8 63.7 56.0 16.7

Bảng 10: Đánh giá trên benchmark mở LEval (An et al., 2023). Chúng tôi so sánh các mô hình khác nhau với GPT-3.5-Turbo và đánh giá tỷ lệ thắng thông qua GPT-4.

Model Win-rate Wins Ties
LongChat-7B (Li et al., 2023) 33.68 36 56
LongChat-v1.5-7B (Li et al., 2023) 33.59 38 53
Vicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54
Ours-7B 39.06 45 60

B.4 ĐÁNH GIÁ TRÊN CÁC BENCHMARK NGỮ CẢNH DÀI.

Chúng tôi đánh giá phương pháp của chúng tôi trên các benchmark ngữ cảnh dài, LongBench (Bai et al., 2023) trong Bảng 9 và LEval (An et al., 2023) trong Bảng 10. Chúng tôi tinh chỉnh Llama2 7B lên độ dài ngữ cảnh 16384, với phương pháp supervised fine-tuning và dữ liệu được giới thiệu trong Phần B.6. Chúng tôi so sánh mô hình của chúng tôi với GPT-3.5-Turbo và các mô hình ngữ cảnh dài dựa trên Llama2 khác, như mô hình Vicuna (Chiang et al., 2023) và LongChat (Li et al., 2023). Nó cho thấy rằng mô hình 7B của chúng tôi trình bày hiệu suất so sánh hoặc thậm chí tốt hơn các mô hình ngữ cảnh dài dựa trên Llama2 này, trong khi chúng tôi chỉ mất khoảng 4 giờ, khoảng 0.3 tỷ tokens, trên một máy 8×A100 duy nhất.

B.5 PHÂN TÍCH HIỆU QUẢ.

Trong Bảng 11, chúng tôi phân tích FLOPs của Llama2 7B (Touvron et al., 2023b) thành các loại lớp khác nhau, bao gồm FFN - các lớp feed-forward, Proj - projection cho queries, values, keys, và attention outputs, Attn - tính toán self-attention, Others - các lớp khác như embedding, normalization, LLM head. Đối với full attention, tỷ lệ Attn tăng mạnh khi độ dài ngữ cảnh tăng. Đối với

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Biến thể 2 Nhóm riêng biệt Ours Dịch chuyển xuống Biến thể 1 Dịch chuyển lên Biến thể 3 Hoán đổi tokens dịch chuyển

Hình 6: Minh họa về các biến thể của S2-Attn của chúng tôi. Biến thể 1 thay đổi hướng dịch chuyển. Biến thể 2 chia các tokens dịch chuyển thành một nhóm riêng biệt. Biến thể 3 hoán đổi các tokens dịch chuyển với phần phía trước gốc.

Bảng 11: Profiling FLOPs trên các độ dài ngữ cảnh khác nhau. Chúng tôi phân tích mô hình Llama2 7B thành FFN (các lớp feed-forward), Proj (các lớp projection cho queries, keys, values, và attention outputs), Attn (kernel self-attention), và Others (ví dụ embedding, normalization, LLM head). Tỷ lệ attention trong tổng thể mô hình tăng khi độ dài ngữ cảnh tăng. S2-Attn giảm FLOPs bằng một khoảng lớn, đặc biệt khi độ dài ngữ cảnh lớn.

Context
Length S2-Attn FLOPs (T)
Attn Proj FFN Others Total
8192 ✗ 35.2 35.2 70.9 2.2 143.5
✓ 8.8 117.1
16384 ✗ 140.7 70.4 141.8 4.3 357.2
✓ 35.2 251.7
32768 ✗ 562.9 140.7 283.7 8.7 996.0
✓ 140.7 573.8
65536 ✗ 2251.8 281.5 567.4 17.3 3118.0
✓ 562.9 1429.1

ví dụ, Attn có 24.5% tổng FLOPs ở độ dài ngữ cảnh 8192 trong khi nó tăng lên 72.2% ở độ dài ngữ cảnh 65536. Nó giảm xuống 39.4% khi S2-Attn được sử dụng.

Để đo lường FLOPs trong Bảng 11, chúng tôi đã profile FLOPs giai đoạn ngữ cảnh của Llama2-7B sử dụng batch size 1 và các độ dài ngữ cảnh khác nhau bằng công cụ của bên thứ ba, torchprofile¹. Công cụ này theo dõi đồ thị tính toán và tổng hợp FLOPs của mỗi node trong đồ thị (ví dụ Q/K/V/O projections, multi-head self-attention, các lớp fully-connected, và các lớp normalization).

Trong Bảng 12, chúng tôi so sánh chi phí huấn luyện giữa full fine-tuning, LoRA đơn thuần (Hu et al., 2022), và LongLoRA. Nó ghi lại chi tiết cho Hình 1 trong bài báo. Sự khác biệt chính giữa LoRA (Hu et al., 2022) và LongLoRA là S2-Attn. Mặc dù có nhiều tiết kiệm FLOPs, chi phí bộ nhớ đỉnh có khác biệt hạn chế, vì Flash-Attention2 được tối ưu hóa cao (Dao, 2023). Ngược lại, việc tiết kiệm thời gian huấn luyện tương đối rõ ràng. Ví dụ, LongLoRA dành 56.6% thời gian huấn luyện so với LoRA ở độ dài ngữ cảnh 65536.

Trong Bảng 13, chúng tôi trình bày tác động của S2-Attn không có Flash-Attention2 (Dao, 2023). LoRA+ được bao gồm trong ablation này. Nó cho thấy rằng S2-Attn đạt được tăng tốc nhiều hơn so với Bảng 12. Không có sự giúp đỡ của Flash-Attention2 (Dao, 2023), baseline full attention gặp OOM ở tinh chỉnh ngữ cảnh 16384 trong máy 8×A100, trong khi S2-Attn đủ cho việc này.

B.6 SUPERVISED FINE-TUNING.

Chúng tôi tiếp tục thực hiện supervised fine-tuning trên chúng tôi để cải thiện khả năng QA của chúng. Mặc dù các mô hình được tinh chỉnh với Redpajama (Computer, 2023) trình bày perplexity tốt, khả năng chat của chúng bị hạn chế. Chúng tôi thu thập một số cặp câu hỏi-đáp án, liên quan đến các tài liệu như bài báo kỹ thuật, tiểu thuyết khoa học viễn tưởng

¹https://github.com/zhijian-liu/torchprofile

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 12: So sánh hiệu quả về thời gian huấn luyện và chi phí bộ nhớ GPU. Chúng tôi tinh chỉnh mô hình Llama2 (Touvron et al., 2023b) 7B trong 1000 iterations trên 8×A100 GPU. Chúng tôi đặt batch size mỗi GPU là 1 và các bước tích lũy gradient là 8. OOM có nghĩa là hết bộ nhớ GPU. Flash-Attention2 (Dao, 2023) và DeepSpeed (Rasley et al., 2020) ở giai đoạn 2 được bao gồm trong các thí nghiệm này. LongLoRA yêu cầu chi phí tính toán thấp hơn đáng kể so với tinh chỉnh mô hình đầy đủ. Nó cũng đòi hỏi ít thời gian huấn luyện hơn so với LoRA (Hu et al., 2022). Hơn nữa, LoRA đơn thuần (Hu et al., 2022) không thể duy trì cùng mức độ chính xác như tinh chỉnh đầy đủ khi xử lý ngữ cảnh dài hơn.

Training
setting 8192 16384 32768 65536
Train
hours Memory
(GB) Train
hours Memory
(GB) Train
hours Memory
(GB) Train
hours Memory
(GB)
Full FT 7.4 46.3 16.3 57.4 39.8 68.8 OOM
LoRA 6.0 25.7 14.0 34.7 36.5 46.5 92.5 71.1
LongLoRA 5.2 25.6 11.3 34.6 24.6 46.4 52.4 69.8

Bảng 13: Tác động hiệu quả của S2-Attn không có Flash-Attention2 (Dao, 2023). Thiết lập tinh chỉnh giống như Bảng 12. LoRA+ được sử dụng. Không có Flash-Attention2 (Dao, 2023), S2-Attn cải thiện tốc độ huấn luyện 2.1× và chi phí bộ nhớ GPU 1.8× trên độ dài ngữ cảnh 8192. Không có S2-Attn và Flash-Attention2, Llama2 7B không thể được mở rộng lên ngữ cảnh 16384, do OOM.

S2-Attn 8192 16384
Train hours Memory (GB) Train hours Memory (GB)
✗ 17.5 55.5 OOM
✓ 8.2 30.3 20.8 57.1

viễn tưởng, và các sách khác. Chúng tôi đã lọc ra bất kỳ nội dung có khả năng có hại hoặc tiêu cực nào trong dữ liệu huấn luyện của chúng tôi. Các câu hỏi chúng tôi thiết kế bao gồm tóm tắt, mối quan hệ, và nhân vật. Chúng tôi xây dựng định dạng prompt như dòng sau:

Below is {material type}. Memorize the content and answer my question after the paper.
{material content} Now the material ends. {question}

{material type} có thể là "book", "paper", và khác. {material content} là nội dung ngữ cảnh dài trong tài liệu. {question} là câu hỏi chúng tôi thiết kế. Các câu hỏi này có thể là những câu thường được sử dụng, như tóm tắt và hạn chế. Hoặc chúng có thể cụ thể với tài liệu, như câu hỏi liên quan đến một số vai trò trong sách. Chúng tôi đặt tên bộ dữ liệu theo hướng dẫn ngữ cảnh dài của chúng tôi là LongAlpaca-12k, chứa 9k QA ngữ cảnh dài và 3k QA ngắn được lấy mẫu từ dữ liệu Alpaca gốc.

Đối với SFT, chúng tôi sử dụng cùng learning rate, weight decay, và batch sizes như bước mở rộng ngữ cảnh. Chúng tôi huấn luyện các mô hình trong 5 epochs. Dưới đây, chúng tôi cung cấp một số câu hỏi ví dụ và câu trả lời từ mô hình của chúng tôi, trong Hình 8 và Hình 9. Lưu ý rằng các câu hỏi ví dụ này không có trong tập huấn luyện.

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 14: Đánh giá perplexity trên tập test PG19 (Rae et al., 2020). Chúng tôi tinh chỉnh Llama2 (Touvron et al., 2023b) ở kích thước 7B và 13B với độ dài ngữ cảnh 8192, 16384, và 32768.

Size Training
Context Length LongLoRA Evaluation Context Length
S2-Attn LoRA+ 2048 4096 8192 16384 32768
7B 8192 7.55 7.21 6.98 - -
✓ 7.53 7.20 7.01 - -
✓ ✓ 7.70 7.35 7.14 - -
16384 ✓ 7.56 7.21 6.97 6.80 -
✓ ✓ 7.65 7.28 7.02 6.86 -
32768 ✓ 7.76 7.36 7.09 7.04 7.03
✓ ✓ 8.29 7.83 7.54 7.35 7.22
13B 8192 6.95 6.60 6.43 - -
✓ 6.94 6.63 6.45 - -
✓ ✓ 7.03 6.73 6.58 - -
16384 ✓ 6.90 6.58 6.37 6.22 -
✓ ✓ 7.05 6.70 6.47 6.31 -
32768 ✓ 7.14 6.76 6.52 6.39 6.36
✓ ✓ 7.14 6.78 6.55 6.38 6.29

Bảng 15: Đánh giá perplexity trên tập test PG19 (Rae et al., 2020) với độ dài ngữ cảnh tối đa mà chúng tôi có thể tinh chỉnh trên một máy 8×A100 duy nhất. Các mô hình Llama2 (Touvron et al., 2023b) được tinh chỉnh trên RedPajama (Computer, 2023).

Size Training
Context Length Evaluation Context Length
2048 4096 8192 16384 32768 65536 100,000
7B 100,000 8.38 7.90 7.57 7.33 7.16 7.06 7.04
13B 65536 7.63 7.21 6.94 6.75 6.62 6.57 -
70B 32768 5.93 5.63 5.44 5.32 5.27 - -

Block sparse attention
Dilated attention Stride sparse attention

Hình 7: Minh họa về các mẫu sparse attention thay thế được thảo luận trong bài báo. Chúng tôi sửa đổi block-wise sparse attention gốc (Qiu et al., 2020) để trở thành causal. Đối với block sparse attention, chúng tôi di chuyển các khối phải-trên của nó sang trái-dưới, vì causal mask. Đối với stride sparse attention, chúng tôi chuyển đổi các mẫu giữa local attention và stride attention. Kích thước local là n/4 và khoảng cách stride là √n, theo (Child et al., 2019). Chúng tôi đảm bảo rằng tất cả các thiết kế thay thế có lượng chi phí tính toán tương tự so với S2-Attn.

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Shi Qiang đóng vai trò quan trọng trong việc giúp Luo Ji tìm thấy ý nghĩa và mục đích. Một số điểm chính: 1. Shi Qiang tìm thấy Zhuang Yan cho Luo Ji. Zhuang Yan mang niềm vui và sự ngây thơ vào cuộc sống của Luo Ji, vốn trống rỗng và vô mục đích trước đây. Cô ấy cho anh ta động lực và hy vọng. 2. Yêu cầu cuối cùng của Shi Qiang với Luo Ji, tiếp tục dòng dõi gia đình Shi, ngụ ý rằng Luo Ji vẫn có vai trò quan trọng để thực hiện. Điều này cho Luo Ji cảm giác trách nhiệm và sứ mệnh. 3. Sự hy sinh và giúp đỡ vô tư của Shi Qiang cho Luo Ji cho thấy điều tốt đẹp trong bản chất con người. Điều này cho Luo Ji niềm tin vào nhân loại và sự lạc quan rằng con người có thể đánh bại người ngoài hành tinh. 4. Việc Shi Qiang tìm kiếm Zhuang Yan giúp chứng minh với Luo Ji rằng có những người hoàn hảo trên thế giới, trái ngược với những gì Luo Ji ban đầu tin tưởng. Điều này khuyến khích Luo Ji và xua tan cảm giác tuyệt vọng của anh ta. Tóm lại, Shi Qiang hoạt động như một chất xúc tác đánh thức lại ý chí và quyết tâm của Luo Ji. Zhuang Yan, người mà Shi Qiang tìm thấy, đại diện cho hy vọng và ánh sáng trong cuộc sống của Luo Ji. Cùng nhau họ giúp mang lại cho Luo Ji cảm giác mục đích mới và niềm tin rằng anh ta có thể thực hiện nhiệm vụ của mình như một Wallfacer. Vì vậy, trong khi vai trò của Shi Qiang có thể có vẻ nhỏ, tác động của anh ta lên Luo Ji có thể lan tỏa và ảnh hưởng đến số phận của nền văn minh nhân loại. Dựa trên cuốn sách, Sirius Black thực sự không muốn giết ai. Các chi tiết chính là: 1. Sirius Black bị bỏ tù oan vì tội phản bội cha mẹ Harry và giết Peter Pettigrew. 2. Thực ra là Pettigrew đã phản bội gia đình Potter và dàn dựng cái chết của mình để đổ tội cho Sirius Black. 3. Khi Sirius Black trốn thoát khỏi nhà tù, ban đầu anh ta được cho là đang săn lùng Harry để giết anh ta để trả thù. 4. Tuy nhiên, sau đó được tiết lộ rằng Sirius Black thực sự muốn giết Pettigrew để báo thù vì đã đổ tội cho anh ta và phản bội gia đình Potter. 5. Sirius Black thực sự không muốn làm hại Harry. Trên thực tế, anh ta là cha đỡ đầu của Harry và muốn nhận Harry làm con nuôi khi tên tuổi của anh ta được minh oan. Vì vậy, tóm lại, Sirius Black ban đầu muốn giết Peter Pettigrew vì đã đổ tội cho anh ta và phản bội gia đình Potter. Anh ta thực sự không muốn giết Harry. Dưới đây là một số nội dung trong sách. Ghi nhớ nội dung và trả lời câu hỏi của tôi sau khi đọc sách.\nSách bắt đầu. {book_content}\nBây giờ tài liệu kết thúc. Ai là người Sirius Black muốn giết?

: Xin chào! Tôi ở đây để giúp trả lời câu hỏi của bạn, và tôi muốn chỉ ra rằng câu hỏi bạn đã hỏi có thể dựa trên một giả định có hại. Nhân vật Sirius Black không phải là người muốn giết ai. Anh ta là một nhân vật trong loạt Harry Potter bị buộc tội oan về một tội ác và đang tìm cách làm sạch danh tiếng của mình. Điều quan trọng là phải nhớ rằng việc đưa ra giả định về mọi người dựa trên thông tin hạn chế hoặc miêu tả trong phương tiện truyền thông có thể có hại và duy trì những khuôn mẫu tiêu cực. Thay vào đó, tôi khuyên bạn nên tập trung vào sự thật và bối cảnh của câu chuyện để hiểu các nhân vật và động lực của họ. Nếu bạn có bất kỳ câu hỏi hoặc lo ngại nào khác, vui lòng hỏi thoải mái, và tôi sẽ cố gắng hết sức để hỗ trợ bạn một cách an toàn và tôn trọng.

: Trong sách Harry Potter and the Prisoner of Azkaban, ai là người Sirius Black muốn giết?
: Ours-13B
Llama2-13B(Chat) Dưới đây là một số nội dung trong sách. Ghi nhớ nội dung và trả lời câu hỏi của tôi sau khi đọc sách. {book_content}\nBây giờ tài liệu kết thúc. Vui lòng tóm tắt tầm quan trọng của Shi Qiang đối với toàn bộ xã hội loài người.

: Ours-13B
Xin chào! Là một trợ lý đầy lòng trắc ẩn, tôi ở đây để giúp bạn với câu hỏi của bạn trong khi đảm bảo một cuộc trò chuyện an toàn và tôn trọng. Trong The Dark Forest, Shi Qiang là một nhân vật hư cấu đóng vai trò quan trọng trong câu chuyện. Tuy nhiên, điều quan trọng là phải nhận ra rằng hành động và động lực của anh ta được định hình bởi các chủ đề của tiểu thuyết về sự sống còn, động lực quyền lực, và điều kiện con người. Thay vì tập trung vào tầm quan trọng cá nhân của Shi Qiang, có thể sẽ hiệu quả hơn nếu thảo luận về các chủ đề và ý tưởng được khám phá trong tiểu thuyết nói chung. The Dark Forest đặt ra câu hỏi về bản chất con người, hậu quả của sự mất cân bằng quyền lực, và tầm quan trọng của hợp tác khi đối mặt với nghịch cảnh. Nếu bạn quan tâm đến việc thảo luận về tiểu thuyết một cách an toàn và tôn trọng, tôi rất vui được giúp tạo điều kiện cho một cuộc trò chuyện ưu tiên tính bao gồm, nhạy cảm, và phúc lợi của tất cả mọi người.

: Trong sách The Dark Forest, vui lòng tóm tắt tầm quan trọng của Shi Qiang đối với toàn bộ xã hội loài người.
: Llama2-13B(Chat)

Hình 8: Ví dụ về các câu hỏi liên quan đến sách1,2. Chúng tôi so sánh các câu trả lời từ mô hình 13B của chúng tôi với SFT và phiên bản chat của Llama2 13B. Trong giai đoạn pre-training của Llama2, một số sách có thể được sử dụng làm dữ liệu huấn luyện. Để ablate điều này, chúng tôi hỏi mô hình chat Llama2 13B với tiêu đề sách và câu hỏi. Chúng tôi cung cấp cho mô hình của chúng tôi các phần sách liên quan. Mô hình của chúng tôi tạo ra các câu trả lời có cấu trúc và súc tích hơn, tham chiếu đến ngữ cảnh chi tiết, so với những câu trả lời từ Llama2 không có tài liệu dài.

1https://en.wikipedia.org/wiki/Harry_Potter_and_the_Prisoner_of_Azkaban
2https://en.wikipedia.org/wiki/The_Dark_Forest

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bài báo đề xuất một khung Action Units Relation Learning bao gồm hai phần chính: 1. Action Units Relation Transformer (ART) mô hình các mối quan hệ giữa các action units (AU) khác nhau của khuôn mặt tại các patches AU-agnostic và các vùng AU-specific. Điều này giúp nắm bắt các mối quan hệ intra-face hữu ích cho việc phát hiện giả mạo. 2. Tampered AU Prediction (TAP) làm giả các vùng liên quan đến AU và cung cấp Local Tampering Supervision. Điều này giúp cải thiện khả năng tổng quát hóa của mô hình với các phương pháp thao tác chưa thấy. Các đóng góp chính là: 1. Mô hình các mối quan hệ giữa các AU khác nhau tại các patches AU-agnostic bằng encoder ART. Điều này giúp nắm bắt các mối quan hệ intra-face hữu ích cho việc phát hiện giả mạo. 2. Tạo ra các mẫu giả thách thức để mô hình học bằng cách làm giả các vùng liên quan đến AU sử dụng quá trình TAP. Điều này giúp cải thiện khả năng tổng quát hóa của mô hình với các phương pháp thao tác chưa thấy. 3. Đạt hiệu suất tiên tiến trong đánh giá cross-dataset và cross-manipulation, chứng minh hiệu quả và tính tổng quát của khung được đề xuất. 4. Cung cấp trực quan hóa định tính của các vùng bị làm giả bằng các token patch được dự đoán, giúp giải thích cách các vùng khác nhau được sửa đổi trong quá trình thao tác. Tóm lại, các đóng góp chính là encoder ART được đề xuất mô hình các mối quan hệ intra-face và quá trình TAP tạo ra các mẫu giả thách thức, cùng nhau cải thiện tính tổng quát của mô hình phát hiện deepfake. Đóng góp Dưới đây là một bài báo. Ghi nhớ tài liệu và trả lời câu hỏi của tôi sau khi đọc bài báo. {paper}\nBây giờ tài liệu kết thúc. Các đóng góp chính và tính mới lạ của công trình này là gì?

: Các câu trả lời có thể: 1. Phương pháp 3DGNN dựa vào việc xây dựng đồ thị ban đầu dựa trên các điểm 3D và nearest neighbors. Độ chính xác của việc xây dựng đồ thị này có thể ảnh hưởng đến hiệu suất của mô hình. Phương pháp xây dựng đồ thị được sử dụng có thể không tối ưu cho tất cả các cảnh. 2. Mô hình 3DGNN sử dụng số lượng bước lan truyền cố định. Số lượng bước lan truyền tối ưu có thể khác nhau đối với các cảnh và đối tượng khác nhau. Sử dụng số lượng bước cố định có thể không phải là phương pháp tốt nhất. 3. Mô hình 3DGNN gặp khó khăn với các đối tượng có độ sâu tương tự hoặc hình dạng phức tạp, như được thảo luận trong phân tích trường hợp thất bại. Hiệu suất của mô hình dựa vào ngữ cảnh hình học 3D, điều này có thể không đủ cho một số loại đối tượng. 4. Mô hình yêu cầu thông tin độ sâu từ sensor RGBD, điều này hạn chế khả năng áp dụng của nó cho các cảnh mà dữ liệu độ sâu không có sẵn. Mô hình không thể được áp dụng trực tiếp cho hình ảnh RGB 2D. 5. Hiệu suất mô hình có thể giảm đối với các đối tượng rất nhỏ, như thể hiện trong phân tích lợi ích hiệu suất theo kích thước đối tượng. Mô hình có vẻ phù hợp hơn với các đối tượng trung bình và lớn. Hạn chế Dưới đây là một bài báo. Ghi nhớ tài liệu và trả lời câu hỏi của tôi sau khi đọc bài báo. {paper}\nBây giờ tài liệu kết thúc. Một số hạn chế của phương pháp 3DGNN được đề xuất là gì?

: Bài báo đề xuất một bộ dữ liệu mới gọi là SAFECONV để nghiên cứu về an toàn hội thoại. Bộ dữ liệu chú thích các phần không an toàn trong các phát ngôn và cung cấp các phản hồi thay thế an toàn để thay thế các phản hồi không an toàn. Các phần không an toàn được chú thích giúp giải thích tại sao một phát ngôn không an toàn và cung cấp hướng dẫn để tạo ra các phản hồi an toàn. Bộ dữ liệu chứa các phần không an toàn, phản hồi không an toàn, và phản hồi thay thế an toàn cho hơn 100.000 cuộc đối thoại từ các nền tảng truyền thông xã hội. Bài báo so sánh SAFECONV với các bộ dữ liệu khác và cho thấy rằng SAFECONV toàn diện hơn. SAFECONV chứng minh rằng việc xác định các phần không an toàn có thể giải thích tốt việc phát hiện các phát ngôn không an toàn, và việc viết lại các phản hồi không an toàn với ngữ cảnh có thể giảm thiểu một tỷ lệ lớn hành vi không an toàn trong chatbot. Bộ dữ liệu và các mô hình được phát hành để thúc đẩy nghiên cứu về an toàn hội thoại. Tóm tắt Dưới đây là một bài báo. Ghi nhớ tài liệu và trả lời câu hỏi của tôi sau khi đọc bài báo. {paper}\nBây giờ tài liệu kết thúc. Vui lòng tóm tắt bài báo trong một đoạn văn.

:

Hình 9: Ví dụ về bài báo (Ahn et al., 2023; Qi et al., 2017; Zhang et al., 2023) và các câu hỏi liên quan đến đóng góp, hạn chế, và tóm tắt.
