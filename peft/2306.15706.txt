# 2306.15706.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2306.15706.pdf
# File size: 4012948 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Approximated Prompt Tuning for Vision-Language Pre-trained Models
Qiong Wu12, Shubin Huang1, Yiyi Zhou12, Pingyang Dai1, Annan Shu3, Guannan Jiang3,
Rongrong Ji12
1Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
Ministry of Education of China, Xiamen University, 361005, P.R. China.
2Institute of Artificial Intelligence, Xiamen University, 361005, P.R. China.
3Intelligent Manufacturing Department, Contemporary Amperex Technology Co. Limited
{qiong, shubinhuang }stu.xmu.edu.cn, {zhouyiyi, pydai }xmu.edu.cn,
{shuan01, jianggn }catl.com, rrjixmu.edu.cn
Abstract
Prompt tuning is a parameter-efficient way to deploy large-
scale pre-trained models to downstream tasks by adding
task-specific tokens. In terms of vision-language pre-trained
(VLP) models, prompt tuning often requires a large num-
ber of learnable tokens to bridge the gap between the pre-
training and downstream tasks, which greatly exacerbates the
already high computational overhead. In this paper, we re-
visit the principle of prompt tuning for Transformer-based
VLP models, and reveal that the impact of soft prompt to-
kens can be actually approximated via independent informa-
tion diffusion steps, thereby avoiding the expensive global at-
tention modeling and reducing the computational complex-
ity to a large extent. Based on this finding, we propose a
novel Approximated Prompt Tuning (APT) approach towards
efficient VL transfer learning. To validate APT, we apply it
to two representative VLP models, namely ViLT and ME-
TER, and conduct extensive experiments on a bunch of down-
stream tasks. Meanwhile, the generalization of APT is also
validated on CLIP for image classification and StableDiffu-
sion for text-to-image generation. The experimental results
not only show the superior performance gains and computa-
tion efficiency of APT against the conventional prompt tun-
ing methods, e.g.,+7 .01% accuracy and −82.30% additional
computation overhead on METER, but also confirm its merits
over other parameter-efficient transfer learning approaches1.
Introduction
Prompt tuning (Li and Liang 2021; Cui et al. 2021; Rad-
ford et al. 2021; Liu et al. 2021; Jia et al. 2022; Zhou et al.
2022a,b) is a parameter-efficient way to adapt large-scale
pre-trained models to downstream tasks. It inserts multi-
ple prompt tokens into the input sequence to unify the pre-
trained and downstream data distributions (Petroni et al.
2019; Radford et al. 2021), thereby avoiding the expensive
full fine-tune of pre-trained models. Recent advances (Li and
Liang 2021; Jia et al. 2022; Zhou et al. 2022a,b) resort to
trainable tokens to replace the hand-craft ones for the adap-
tion on downstream tasks, which is named soft prompt tun-
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
1Our code is given in supplementary materials and will be pub-
licly released after acceptance.
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015
/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000014/uni00000011/uni00000014/uni00000018/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000010/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048/uni00000039/uni00000010/uni0000002f
/uni00000031/uni0000002f/uni00000033
/uni00000026/uni00000039
/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000027/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000026/uni00000058/uni00000055/uni00000059/uni00000048/uni00000024/uni00000033/uni00000037/uni00000003/uni00000057/uni00000052/uni00000003/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000024/uni00000033/uni00000037/uni00000003/uni00000057/uni00000052/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048
/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048Figure 1: (a) The relative performance of soft prompt tuning to full
fine-tune w.r.t. the prompt number on VL, NLP and CV tasks2. VL
tasks often require more prompt tokens than single-modal ones. (b)
The comparison between our APT and soft prompt tuning in terms
of information diffusion efficiency, i.e., the attention weights to the
input sequence. The adaption efficiency of soft prompting is much
less efficient on VL tasks.
ing(Li and Liang 2021; Liu et al. 2021). In terms of the way
of insertion, soft prompt tuning can be further categorized
intodeep prompt tuning (Jia et al. 2022) and shallow prompt
tuning (Li and Liang 2021), respectively. Currently, prompt
tuning has achieved great success in natural language pro-
cessiong (NLP) (Petroni et al. 2019; Li and Liang 2021;
Lester, Al-Rfou, and Constant 2021) and also been recently
applied to shallow-fusion based vision-language pre-trained
(VLP) models like CLIP (Radford et al. 2021) for image
classification (Zhou et al. 2022a,b).
However, it is still more challenging to apply prompt tun-
ing to common VLP models, e.g., ViLT (Kim, Son, and
Kim 2021) and METER (Dou et al. 2022), than the lan-
guage ones. Above all, most VL tasks are greatly different
from the VL pre-training objectives, e.g.,such as visual ques-
tion answering (VQA) (Goyal et al. 2017). Thus, hand-craft
prompts often fail to adapt to these tasks. Soft prompt tun-
ing is applicable to VLP models (Jia et al. 2022). But due
to the large task gap and the increase of modalities, more
learnable tokens are often required to adapt VLP models to
2These results are from VPT (Jia et al. 2022) on VTAB datasets
(CV), Prefix (Li and Liang 2021) on DART (NLP) and deep
prompting on VQA (V-L)arXiv:2306.15706v2  [cs.CV]  21 Aug 2023

--- PAGE 2 ---
…
… ……… …
…
…
… ………
… …… …
…
………Prompt InputPrompt Input… …
(a) Global Attention Matrix……
……
…
… ……
……
…
…InputPrompt Input…
(b) The attention matrix of our APT
Input-Prompt
Input-onlyInput-only
Prompt-only Prompt-InputInput-Prompt
ApproximatedFigure 2: The illustrations of global self-attention matrices with
and without APT. (a) is the attention matrices of common prompt
tuning. In (b), the prompt-input and prompt-only parts are removed,
and the input-prompt attention is approximated by APT.
downstream tasks, as shown in Fig. 1-a. In addition, we also
notice that even with a bunch of tokens, soft prompt tuning
still has limited impacts on the input sequence, i.e., atten-
tion weights, leading to a sub-optimal adaption, as shown in
Fig. 1-b. Considering that these tokens are often involved in
self-attention, of which computation is quadratic to the input
length (Vaswani et al. 2017), this inefficient adaption will
significantly increase the already high computational over-
head of VLP models.
By revisiting the principle of prompt tuning, we find that
there exists a potential solution for efficient VL adaption.
Particularly, prompt tuning aims to use additional tokens
to influence the input sequence, so as to minimize the gap
between pre-training and downstream tasks (Petroni et al.
2019; Cui et al. 2021). In terms of soft prompt tuning, the
tokens are usually inserted into the self-attention layers of
VLP models (Li and Liang 2021; Jia et al. 2022; Zhou et al.
2022a). Via analyzing self-attention, we observe that the ob-
tained attention weight matrix can be actually divided into
four sub-parts as shown in Fig. 2-a. Here, we call them input-
only,input2prompt ,prompt2input andprompt-only attention
matrices, respectively. Under the setting of deep prompt tun-
ing (Jia et al. 2022), i.e., the prompts are layer-wise, the
computations of prompt2input andprompt-only can be in-
deed skipped and will not affect the prompt tuning of next
layer. And the input-only is the default operation of pre-
trained models that cannot be changed. In this case, the key
to improving prompt tuning lies on the input2prompt , which
is essentially an information diffusion step from the prompt
tokens to the input sequence under the perspective of graph
theory (Zhou et al. 2020). However, we find that its func-
tionality can be actually approximated via a more effective
process independent to global attention, thereby improving
the efficiency of VL adaption.
Motivated by this observation, we propose a novel ap-
proximated prompt tuning (APT) approach for VLP mod-
els in this paper. Similar to deep prompt tuning (Jia et al.
2022; Li and Liang 2021), APT inserts a set of learnable
tokens into each self-attention layer of the VLP model. As
shown in Fig. 2-b, a key difference is that we separate thesetokens from the expensive global self-attention and approx-
imate their effects independently by aggregating the prompt
tokens with low-rank transformations. In this way, the pro-
posed APT can effectively diffuse more information from
prompt tokens to the input sequence while avoiding the ex-
pensive global self-attention, as shown in Fig. 1-b.
To validate APT, we apply it to two deep-fusion based
VLP models, namely ViLT (Kim, Son, and Kim 2021) and
METER (Dou et al. 2022), on three VL benchmarks includ-
ing VQA (Antol et al. 2015), NLVR2(Suhr et al. 2019) and
Flickr30K (Plummer et al. 2017). In addition, we also ex-
amine its generalization on CLIP (Radford et al. 2021) for
thebase-to-new classification task (Zhou et al. 2022b) and
on StableDiffusion (Rombach et al. 2022; Ruiz et al. 2022)
for text-to-image generation. The experimental results well
confirm the obvious merits of APT over the conventional
prompt tunning methods (Li and Liang 2021; Jia et al. 2022;
Zhou et al. 2022a,b), e.g.,+8.30% accuracy on VQA2.0
for METER while reducing up to 17.70% additional com-
putation overhead. Our APT also yield better performance
than most parameter-efficient transfer learning (PETL) ap-
proaches (Jia et al. 2022; Hu et al. 2022; Sung, Cho, and
Bansal 2022; He et al. 2022), e.g.70.94% on VQA2.0 for
ViLT and 80.97% on NLVR2for METER.
Overall, our contributions are three-fold:
• We identify the key challenges of prompt tuning on com-
mon VLP models, e.g.ViLT (Kim, Son, and Kim 2021)
and METER (Dou et al. 2022), which are excessive com-
putation overhead and low prompt tuning efficiency.
• We propose a novel approximated prompt tuning (APT)
method for both parameter- and computation-efficient
prompt tuning, which approximates the influence of
prompt tokens via independent aggregation steps.
• The proposed APT not only outperforms existing prompt
tuning methods but also achieves better performance than
other PETL approaches on 2 VLP models and 4 VL
tasks. Its generalization is also validated on CLIP and
StableDiffusion.
Related Work
Vision-Language Pre-training
Similar to NLP pre-training paradigms (Devlin et al. 2019;
Brown et al. 2020; Raffel et al. 2020; Lewis et al. 2020;
Liu et al. 2019; Lan et al. 2020), vision-language (VL)
pre-training also apply generative prediction tasks, e.g.
masked language modeling (MLM) and maksed image mod-
eling (MIM), to achieve self-supervised learning on massive
cross-modal data. A key difference is that VLP models usu-
ally require two encoders to process image and language
information, e.g., BERT (Devlin et al. 2019) and Faster-
RCNN (Ren et al. 2015), and their Transformer-based back-
bone networks are not only used for high-level representa-
tion learning, but also for cross-modal deep fusion and in-
teraction (Li et al. 2019; Su et al. 2020; Chen et al. 2020;
Huang et al. 2020; Liu et al. 2019; Kim, Son, and Kim 2021;
Dou et al. 2022; Huang et al. 2021). METER (Dou et al.
2022), the base model of this paper, is the typcial model us-
ing this paradigm. Meanwhile, we also validate APT on the

--- PAGE 3 ---
other representative model called ViLT (Kim, Son, and Kim
2021), which process the image and text information with
only one end-to-end Transformer network.
Prompt Tuning
Prompt tuning (Brown et al. 2020; Petroni et al. 2019; Li and
Liang 2021; Cui et al. 2021; Radford et al. 2021; Jia et al.
2022; Zhou et al. 2022a,b; Liu et al. 2021) is a parameter-
efficient way to adapt pre-trained models to downstream
tasks. Concretely, for hand-crafted prompts (Petroni et al.
2019; Radford et al. 2021), it often inserts a pre-defined
prompt phrase into the input sequences, thus reminding the
model of pre-trained knowledge, e.g., “This is a picture of
[X]”. However, hard prompt tuning heavily relies on manual
design. To overcome this issue, soft prompt tuning (Li and
Liang 2021; Jia et al. 2022; Zhou et al. 2022b) is proposed to
automatically learn trainable prompts via downstream task
adaption. In terms of the prompt placement, soft prompt tun-
ing can be further divided into two patterns, i.e., the shal-
low (Li and Liang 2021) and the deep (Jia et al. 2022) ones.
Shallow prompt tuning methods (Lester, Al-Rfou, and Con-
stant 2021; Li and Liang 2021) only expand the input se-
quence with trainable vectors at the first layer, while deep
prompt tuning methods (Jia et al. 2022) expand the input
sequence between any two layers with trainable tokens.
Parameter Efficient Transfer Learning
Parameter Efficient Transfer Learning (PETL) (Houlsby
et al. 2019; Mahabadi et al. 2021; Zhang et al. 2020; Guo,
Rush, and Kim 2021; Sung, Nair, and Raffel 2021; Ma-
habadi, Henderson, and Ruder 2021; Sung, Cho, and Bansal
2022; Hu et al. 2022; He et al. 2022; Mao et al. 2022)
aims to update a small number of parameters to approach
the fully-tuned performance on downstream tasks. In addi-
tion to prompt tuning, a common paradigm is the adapter-
based methods (Houlsby et al. 2019; Mahabadi et al. 2021;
Mahabadi, Henderson, and Ruder 2021; Gao et al. 2021;
Zhang et al. 2021; Sung, Cho, and Bansal 2022), called
Adapter for short, which insert lightweight networks into
the pre-trained model to project hidden features onto down-
stream data space. To avoid the additional computation over-
head during inference, Hu et al. propose a low-rank adap-
tion (LoRA) (Hu et al. 2022) method, based on weight
re-parameterization. In the field of vision-language learn-
ing, VL-adapter (Sung, Cho, and Bansal 2022) insert low-
dimensional networks into a pre-trained language model to
adapt to common VL tasks (Chen et al. 2015; Goyal et al.
2017; Suhr et al. 2019). Its key difference to this paper is that
the language model is not VL pre-trained, lacking enough
generalization for common VLP models.
Preliminary
Before introducing our approach, we first recap the principle
of prompt tuning for VLP models. Concretely, given a pre-
trained vision-language (VLP) model, denoted as G(·), and
the image-text example of the downstream task, denoted as
(I, T), the target of prompt tuning is to minimize the adap-tion loss with a set of prompt tokens P∈Rp×d:
argmin
PL 
G(I, T, P |θ+)
, (1)
where θ+is the pre-trained weights of Gand will be fixed
during prompt tuning3.Lis the objective function of the
downstream task.
Considering that the parameters are fixed during adaption,
the features of the input sequence are hard to update for the
downstream task. In this case, prompt tokens Pare often
used in the self-attention of VLP models for diffusing task-
related information to the input sequence X∈Rn×d:
[X′||P′] =SA(X||P), (2)
where SA(·)represents the self-attention module. X′and
P′are the corresponding outputs of XandP, respectively.
In particular, X’ and P’ are obtained by
X′=AIXW v+AIPPW v,
P′=APIXW v+APPW v,(3)
whereAI,AIP,APIandAPare the sub-attention
matrices, corresponding to the input-only ,input2prompt ,
prompt2input andprompt-only parts described in introduc-
tion and shown in Fig. 2. Wq,WkandWvare the weight
matrices of Q,K,Vprojections in SA.
Under the layer-wise setting (Jia et al. 2022), the prompt
tokens are initialized for each layer and will not be used
in the next SA. In this case, the computation of P′can
be indeed removed, which can reduce the complexity by
O(2pd2+ 4npd+ 2p2d), where pis often a large value on
VL tasks.
Eventually, the feature update of VLP models with
prompt tokens can be relaxed to
X′=AIXW v+AIPPW v
=γI
γI+γIPσ(XW q(XW k)T
√
d)XW v
+γIP
γI+γIPσ(XW q(PW k)T
√
d)PW v,
where γI=X
eQKT
i, γIP=X
eQPkT
j(4)
Here, σ(·)is the Softmax function, and γIandγIPare the
attention proportion for the input sequence and prompt to-
kens, respectively. In Eq. 4, the first term is the self-attention
update of input features, which is the compulsory operation
of VLP models. To this end, the effectiveness of prompt tun-
ing lies in the second term, which is essentially a weighted
information diffusion step from PtoX. Since the scale-dot
product is still required, this diffusion step is also expensive.
Approximated Prompt Tuning
Based on the above observation, we propose approximated
prompt tuning (APT) to model the attention impacts of
prompt tokens. In particular, we can get the prompt tuning
process of APT with the following formula:
X′=SA(X) +APT (X,P). (5)
3In most case, the classifier will be trained for a specific task

--- PAGE 4 ---
For simplicity, we regard the information diffusion from P
to X as ∆X:
∆X=APT (X,P)
=γIP
γI+γIPσ(XW q(PW k)T)PW v.(6)
To approximate ∆X, we first focus on the information
aggregation of prompt tokens, denoted as ∆X′:
∆X′=σ(XW qWT
kPT)PW v. (7)
Note that, Wvis fixed in SA, and Pis a trainable matrix.
To this end, we can directly update the projection of prompt
tokens onto the Vsubspace, i.e., putPW vasP′. Similarly,
we can simplify PW kWT
qasK. Then, the Xcan be directly
taken as the Qwithout projection, and the computation from
transforming XandPintoQ,KandVofSAcan be saved.
Next, we show that Vcan be linearly transformed to K:
∆P=PW kWT
q−PW v,
=P(WkWT
q−Wv).(8)
Here, ∆Pdenotes the difference between VandK.Because
Vcan be transformed to Kby a linear transformation, we
approximate Eq. 7 as
∆X′=σ 
X(P′Wp+P′)T
P′, (9)
whereWp∈Rd×daims at transforming prompt tokens
from VtoK. However, calculating PW pis still not cheap
due to the high feature dimension.
As the low intrinsic dimension component (Li et al. 2018;
Aghajanyan, Gupta, and Zettlemoyer 2021) plays a domi-
nant role in model optimization, the rank for Wpis finite
according to the theorem of the rank of matrices:
rank(Wp)≤rank(WkWT
q) +rank(Wv), (10)
where rank(·)is the rank of the matrix. We can approximate
the aggregation of prompt tokens in a low-rank way:
∆X′=σ 
X(P′W1W2+P′)T
P′. (11)
Here,W1∈Rd×randW2∈Rr×dare two low-
dimensional matrix, where r≪d. The rank of projection
matrixW1W2is limited by r. The way we obtain Q,K
andVmatrices for attention modeling is cheaper than the
original global attention.
Afterwards, we consider the way to merge the original
output of the self-attention module SA(X)and the infor-
mation of prompt tokens ∆X. Because the calculation of
attention is still related to the input sequence, it is difficult to
reduce the complexity of the approximation via independent
computation. In this case, to adaptively adopt the impact of
each prompt token, a simple solution is activating the atten-
tion matrix with ReLU instead of Softmax and omitting
the weight item. Then, Eq. 6 can be represented as
∆X=ψ 
X(P′W1W2+P′)T
P′, (12)
where ψ(·)represent ReLU activation. In this manner, the
weights for prompts depend on the norm of prompt tokens
and their relation to input sequence.Furthermore, from the weight calculation in Eq. 6, we ob-
serve that the effect of prompt tokens is not only influenced
by their dependency to the input sequence, but also by the
sum of attentions to the input sequence. With the intrinsic of
the Softmax function that the maximum value has the most
impact, we re-define Eq.6 by
∆X=α·ψ(X(P′W1W2+P′)T)P′,
where α=max{P′W1W2+P′},(13)
where max{·}is the maximum function for the weight of
each token. Thus, APT can globally adjust the information
diffusion from the prompt tokens. Since the activation func-
tion of Eq. 13 no longer relies on the original attention ma-
trix, the APT is easier to deploy for VLP models.
Up to now, we have fully considered the effect of prompt
tokens in diffusing task-related information to the input se-
quence. Then, we also take into account the effect of prompt
tokens on the original attention matrix. As shown in Eq. 4,
the information diffusion also influences the original atten-
tion matrix by increasing the denominator of the weight for
the item from VLP module. To this end, we add a learnable
scale sfor the entire output, and the proposed APT can be
summarised as follow:
X′=AIXW v+AIPPW v
≈es· 
SA(X) +α·ψ(X(P′W1W2+P′)T)P′
,
where α=max{P′W1W2+P′}.
(14)
Here, the learnable value scontrol the total amount of infor-
mation diffused by APT and also make the output of atten-
tion modules fit the following layers.
Eventually, the proposed APT method separates the effect
of prompt tokens from the original attention module. The in-
dependence of APT brings two main benefits: (1) Informa-
tion diffusion can break the limitation of patterns from VLP
model, i.e.not constrained by Softmax-based normalization.
(2) The computational overhead is significantly reduced by
about O(2pd2). In practice, it can save about 82.30% and
62.62% computations for ViLT (Kim, Son, and Kim 2021)
and METER (Dou et al. 2022) compare to conventional
prompt tuning methods.
Experiments
Datasets and Experimental Setup
Dataset and Metric. VQA2.0 (Goyal et al. 2017) is one
of the most popular datasets for visual question answering
(VQA) task. It uses images from MS-COCO (Ren, Kiros,
and Zemel 2015) and has about 443,757,214,254 and
447,793VQA examples for training, validation and testing,
respectively. NLVR2(Suhr et al. 2019) is built for visual rea-
soning. It contains 107,292examples of human-written En-
glish sentences for pairs of photographs. Flickr30k (Plum-
mer et al. 2017) is a widely-used benchmark dataset in this
image-text matching task. The dataset consists of 31,783
images, and each has five corresponding captions. For CLIP,
we validate APT on 11popular image classification datasets,
including ImageNet (Deng et al. 2009), Caltech101 (Fei-Fei,

--- PAGE 5 ---
Table 1: Comparisons of APT and the conventional prompt tuning methods for ViLT and METER on VQA, NLVR2and
Flickr30K. The best performance is bold while the second one is underlined .
Backbone MethodUpdated
ParameterAdditional
FLOPsVQA NLVR2Flickr30KAvg.test-dev test-P IR R@1 TR R@1
ViLTFull Tuning 115.43M 0.0 71.26 76.13 64.40 83.50 73.82
Shallow Prompt 0.15M 19.53G 66.47 66.47 55.92 74.80 65.92
Deep Prompt 1.84M 5.14G 69.30 73.34 58.64 79.50 70.20
APT 1.92M 0.91G 70.94 75.92 63.26 81.60 72.93
METERFull Tuning 323.31M 0.0 77.43 83.05 82.22 94.30 84.25
Deep Prompt 3.68M 13.05G 67.57 65.79 70.90 87.70 72.99
Shallow Prompt 0.30M 28.71G 68.51 65.69 74.20 88.60 74.25
APT 3.83M 2.31G 75.45 80.97 80.88 92.90 82.55
Shallow
Prompt
Deep
Prompt
APT
Figure 3: The visualizations of the attention results of shallow prompt, deep prompt and our APT with ViLT on VQA2.0 dataset. The color
denotes the degree of attention, while the redder the higher and vice versa . Compared with shallow prompt and deep prompt, APT can more
effectively diffuse prompt information to the input sequence from the low layers of ViLT, see the red arrows.
Fergus, and Perona 2007), OxfordPets (Parkhi et al. 2012),
StandfordCars (Krause et al. 2013), Flowers102 (Nils-
back and Zisserman 2008), Food101 (Bossard, Guillau-
min, and Gool 2014), FGVCAircraft (Maji et al. 2013),
SUN397 (Xiao et al. 2010), DTD (Cimpoi et al. 2014), Eu-
roSAT (Helber et al. 2019), UCF101 (Soomro, Zamir, and
Shah 2012)4. This comprehensive benchmark comprises
datasets that cover a diverse set of vision tasks, including
classification on generic objects, scenes, actions, and fine-
grained categories. It also includes specialized tasks like rec-
ognizing textures and satellite imagery.
Implementation details. We validate APT on two deep-
fusion based VLP models, namely ViLT (Kim, Son, and
Kim 2021) and METER (Dou et al. 2022), and one shallow-
fusion based VLP network called CLIP (Radford et al.
2021). In terms of ViLT, we add APT to its each SA layer.
We set the rank value rin Eq. 11 to 4and the number of
prompt tokens p= 200 as the default setting. The prompt
tokens are initialized by a normal distribution with a mean
of0.0and a variance of 0.02. And we only apply a single
attention rather than the multi-head one (Devlin et al. 2019)
for the proposed APT method. During the training, we up-
date the classifier, class tokens and modal-type embeddings,
while the rest parameters of ViLT are kept fixed. For each
task, we follow its default settings and increase the learning
rate by five times. In terms of METER, APT is inserted into
its self-attention and cross-attention layers. The rest settings
are the same as ViLT. For CLIP (Radford et al. 2021), we
insert APT into the self-attention layers of its text encoder,
4The details of these datasets are given in Appendix.and we set the rank r= 2and the number of prompts p= 4.
APT is optimized by SGD with a learning rate of 2×10−4
and weight decay of 0.3for10epochs. Following (Radford
et al. 2021), we also use a hard prompt phrase of “ a photo of
[X]”, which is fed to the text encoder of CLIP.
Experimental results
Comparison with prompt tuning methods. We first com-
pare APT with two common soft prompt tuning methods,
i.e., deep prompt (Jia et al. 2022) and shallow prompt (Li
and Liang 2021), in Tab. 1. For all methods, the number of
prompts is set to 200for a fair comparison. From Tab. 1, the
performance of existing prompt tuning methods is far behind
the full tuning one, i.e.−7.90% to−3.62% on ViLT and
−11.26% to−10.00% on METER. These results are also
worse than their performance on NLP (Li and Liang 2021)
and vision tasks (Jia et al. 2022), showing the challenge
of prompt tuning on VLP models. Among these compared
methods, Deep Prompt shows better results than shallow
prompts at most cases, while its parameter size is larger and
similar to APT. Notably, the average improvements of APT
to these prompt methods are +2.73% to+7.01% on ViLT
and+8.30% to+9.56% on METER, respectively, while
the saved additional computations can be up to 82.30% on
ViLT and 91.95% on METER. Meanwhile, the performance
of APT almost approaches full tuning, e.g.,−0.89% and
−1.70% on average for ViLT and METER, respectively.
Considering the small number of parameters updated, these
results are indeed significant.
To obtain more intuitive comparisons, we also visualize
the attentions of these prompt tuning methods in Fig. 3. In

--- PAGE 6 ---
Table 2: Comparisons of APT and the state-of-the-art PETL methods for ViLT and METER on VQA, NLVR2and Flickr30K.
The best performance is bold and the second best is underlined .
Backbone MethodUpdated
ParameterAdditional
FLOPsVQA NLVR2Flickr30KAvg.test-dev test-P IR R@1 TR R@1
ViLTFull Tuning 115.43M 0.0 71.26 76.13 64.40 83.50 73.82
Classifier Only - 0.0 65.75 66.08 57.42 78.00 66.81
Deep Prompt 1.84M 5.14G 69.30 73.34 58.64 79.50 70.20
LoRA 0.15M 0.0 68.44 72.77 57.44 77.70 69.09
Scaled PA 1.80M 0.44G 70.40 75.13 61.88 79.00 71.60
Adapter 3.56M 0.86G 70.85 75.51 62.68 81.40 72.61
APT 1.92M 0.91G 70.94 75.92 63.26 81.60 72.93
METERFull Tuning 323.31M 0.0 77.43 83.05 82.22 94.30 84.25
Classifier Only - 0.0 69.93 73.23 78.80 89.00 77.74
Deep Prompt 3.68M 13.05G 67.57 65.79 70.90 87.70 72.99
LoRA 0.29M 0.0 74.00 78.82 79.86 92.60 81.32
Adapter 5.34M 1.64G 74.70 79.93 80.38 91.90 81.73
Scaled PA 3.82M 1.12G 75.36 79.86 80.30 91.80 81.83
APT 3.83M 2.31G 75.45 80.97 80.88 92.90 82.55
/uni00000014/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000018 /uni00000015/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000016/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000018 /uni00000017/uni00000011/uni00000013
/uni00000024/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000019/uni0000001c/uni00000011/uni00000015/uni00000018/uni00000019/uni0000001c/uni00000011/uni00000018/uni00000013/uni00000019/uni0000001c/uni00000011/uni0000001a/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000015/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000018/uni00000013/uni0000001a/uni00000013/uni00000011/uni0000001a/uni00000018/uni0000001a/uni00000014/uni00000011/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000024/uni00000033/uni00000037
/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni00000033/uni00000024
/uni00000027/uni00000048/uni00000048/uni00000053/uni00000003/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
Figure 4: The comparison between APT and other PETL methods
in terms of performance and parameter size. APT has a better trade-
off between performance and parameter costs.
this figure, We select the 15 most active tokens of the visual
and text inputs, and the top 30 prompt tokens for visualiza-
tion, 60 tokens in total. The global attention matrices can be
divided into six sub-parts, i,e. Text-Text ,Text-Image ,Text-
Prompt ,Image-Text ,Image-Image ,Image-Prompt . From
these examples, we can first observe that in the lower lay-
ers of the VLP model, the information exchanges mainly
happens among the tokens of the same modality, and the
prompts barely affect the input sequence. As the inference
progress, their impacts of common prompts become slightly
more pronouns. In terms of shallow prompts, the impact of
its tokens is still marginal, while deep prompt will be bet-
ter at the last few layers of the model. The above results are
also consistent with their performance on VL tasks. In stark
contrast, APT can effectively diffuse prompt information to
the input sequence of the VLP models, see the arrows. And
its attention weights become more intensive in the higher
layers, suggesting its effectiveness towards task adaption.
Comparison with existing PETL methods. Next, wecompare APT with a bunch of PETL methods, includ-
ing LoRA (Hu et al. 2022), VL-Adapter (Adapter) (Sung,
Cho, and Bansal 2022) and Scaled Parallel Adapter (Scaled
PA) (He et al. 2022), of which results are given in Tab. 25.
From this table, we can first see that LoRA is most effi-
cient in both parameters and computation due to its low-
rank re-parameterization scheme. Compared to pre-trained
language models (Liu et al. 2019; Brown et al. 2020), its
performance on VLP models is much inferior, especially on
the tasks that are greatly different from pre-training, e.g.,
VQA and NLVR2, suggesting the challenge of VL adap-
tion. We can also find that although the adapter-based meth-
ods show better adaptabilities than LoRA, they still perform
worse than our APT. Compared with VL-Adapter, APT can
achieve obvious gains on ViLT and METER, while saving
about 46.07% and28.28% parameters, respectively. In terms
of the most advanced Scaled PA, APT is slightly inferior
in parameter and computation costs, but its adaption per-
formance is consistently better than Scaled PA on two VLP
models. Overall, these results suggest that our APT is a com-
petitive method in PETL with great potential.
In Fig. 4, we also present the performance comparison of
APT to other PETL methods with different parameter costs.
It can be seen that Deep Prompt are much inferior than other
methods in terms of parameter efficiency and performance,
suggesting its difficulty on VL adaption. Adapter (Sung,
Cho, and Bansal 2022) and Scaled PA (He et al. 2022), as
the advanced PETL methods, are all parameter efficient, and
their adaptions are also plausible on VQA. However, the
overall performance of these two methods is close, which is
beyond expectation. Compared to these adapter-based meth-
ods, the performance of APT can achieve obvious gains at a
scale of about 2M parameters, which becomes stable as the
parameter size grows.
Ablation Study. We first examine the impact of prompt
5These results are reproduced by us because there are no ready-
made literature to refer. Details are given in Appendix.

--- PAGE 7 ---
Table 3: Ablation study on different constructions and the
number of prompt tokens. *the default setting.
Prompt Rank valueAdditional
ParameterVQA NLVR2
test-dev test-P
200 Identity 1.84M 70.42 75.27
200 dense 8.93M 71.07 75.67
100 4 0.99M 70.11 74.81
150 4 1.46M 70.49 74.87
400 4 3.76M 70.64 75.68
400 16 3.98M 71.01 75.95
200* 4* 1.92M 70.94 75.92
Table 4: Comparison of zero-shot CLIP ( CLIP ), CoOp, Co-
CoOp and APT on the base to new classification task.
DatasetMethod
Base New
CLIP CoOp CoCoOp APT CLIP CoOp CoCoOp APT
ImgNet 72.43 76.47 75.98 75.97 68.14 67.88 70.43 71.23
Cal101 96.84 98.00 97.96 97.93 94.00 89.81 93.81 94.13
Pets 91.17 93.67 95.20 94.97 97.26 95.29 97.69 97.60
Cars 63.37 78.12 70.49 76.10 94.89 60.40 73.59 75.13
Flowers 72.08 97.60 94.87 94.47 77.80 59.67 71.75 70.13
Food 90.10 88.33 90.70 90.17 91.22 82.26 91.29 90.70
Aircraft 27.19 40.44 33.41 38.63 36.29 22.30 23.71 33.97
SUN 69.36 80.60 79.74 81.50 75.35 65.89 76.86 78.20
DTD 53.24 79.44 77.01 81.00 59.90 41.18 56.00 48.53
SAT 56.48 92.19 87.49 91.10 64.05 54.74 60.04 62.13
UCF 70.53 84.69 82.33 85.13 77.50 56.05 77.64 76.93
Average 69.34 82.69 80.47 82.72 74.22 63.22 71.69 72.64
number and the rank value in Eq. 11, of which results are
given in Tab. 3. Here, “ identify ” denotes that directly using
prompt tokens as KandVin Eq. 9, while “ dense ” means
that low-rank transformation is not used in Eq. 11. The first
observation from Tab. 3 is that the increase of prompt tokens
is beneficial to VLP models, which can obtain improvements
on both tasks, e.g., 100 to 200. However, when exceeding
200, its gains are marginal in contrast to other prompt meth-
ods as shown in Fig. 4, which also suggests the effectiveness
of APT for VLP models. In terms of the rank value, the per-
formance of “identity” suggests that directly using prompt
tokens for attention is suboptimal. And the low-rank approx-
imation can better trade off performance and parameter cost,
e.g.rank value r= 4, which is even superior than the dense
transformation on NLVR2. Overall, these results well con-
firm the effectiveness of APT towards efficient VL adaption.
More experiments can refer to our Appendix .
Generalization on CLIP. We further examine the gener-
alization ability of APT on the shallow-fusion based VLP
model, i.e., CLIP (Radford et al. 2021), under the base-to-
new classification task (Zhou et al. 2022a), of which re-
sults are given in Tab. 4. In this task, the model needs
to adapt to the base dataset, and will be further evaluated
on unseen data (new dataset). The compared methods in-
clude zero-shot CLIP, CoOp (Zhou et al. 2022b) and Co-
CoOp (Zhou et al. 2022a). The detailed settings are given in
Appendix . From this table, we first observe that zero-shot
Input Images
APT
 LoRAswimming is getting haircut sleeping in the Acropolis in the bucketFigure 5: The comparison between APT and LoRA on StableD-
iffusion under the setting of subject-driven image generation. The
red boxes denote the failure cases. Compared with LoRA, APT can
better customize the generation based on the reference images.
CLIP has a strong transferring learning ability. Due to its
large-scale pre-training, it can obtain superior performance
under the new task evaluations. However, without tuning on
base datasets, its performance is much inferior than PETL
methods. In terms of CoOp, it can achieve satisfactory per-
formance for base task adaption. However, its generalization
is limited to new tasks, only 63.22% on average, suggesting
the over-fitting problem. In stark contrast, APT can obtain
a good performance in adapting base tasks while general-
izing well to new ones. Compared to latest PETL methods
for CLIP, i.e.CoCoOp, its performance is also consistently
better under two settings. These results confirm the general-
ization of APT.
Generalization on StableDiffusion. We also examine the
generalization ability of APT on the StableDiffusion (Rom-
bach et al. 2022) following the setting of DreamBooth (Ruiz
et al. 2022), and the compared method is LoRA, of which re-
sults are given in Fig. 5. The detailed setting is given in Ap-
pendix. From the image, we can find out that the dogs gen-
erated with different prompts can keep the same attribute.
Similar to LoRA, APT binds the attributes of the dog in the
training set to specific vocabulary “sks”. These results sug-
gest that APT is also capable of text-to-image generation.
Conclusion
In this paper, we focus on the issues of high computa-
tion overhead and inefficient adaption of prompt tuning on
vision-language pre-trained (VLP) models. By revisiting the
principle of prompt tuning, we can figure out that the key
to improve prompt tuning lies in its information diffusion to
the input sequence, which can be indeed independent to the
expensive global self-attention via effective approximations.
Motivated by this observation, we propose a novel approxi-
mation prompt tuning (APT) approach towards effective VL
adaption. APT approximates the impacts of prompt tokens to
the input sequence via a low-rank token aggregation design,
reducing the computation cost to a large extent. We validate
APT on 2 VLP models and 3 VL benchmarks, and also gen-
eralize VPT to CLIP for image classification and StableDif-
fusion for subject-driven image generation. The quantitative
and qualitative results not only show the obvious merits of
APT over existing prompt tuning methods in both compu-
tation efficiency and performance, but also outperform the
compared PETL methods on these VL tasks.

--- PAGE 8 ---
References
Aghajanyan, A.; Gupta, S.; and Zettlemoyer, L. 2021. Intrin-
sic Dimensionality Explains the Effectiveness of Language
Model Fine-Tuning. In Zong, C.; Xia, F.; Li, W.; and Nav-
igli, R., eds., ACL, 7319–7328.
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zit-
nick, C. L.; and Parikh, D. 2015. VQA: Visual Question
Answering. In ICCV , 2425–2433.
Bossard, L.; Guillaumin, M.; and Gool, L. V . 2014. Food-
101 - Mining Discriminative Components with Random
Forests. In Fleet, D. J.; Pajdla, T.; Schiele, B.; and Tuyte-
laars, T., eds., ECCV , volume 8694 of Lecture Notes in Com-
puter Science , 446–461.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Mod-
els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;
Hadsell, R.; Balcan, M.; and Lin, H., eds., NeurIPS .
Chen, X.; Fang, H.; Lin, T.; Vedantam, R.; Gupta, S.; Doll ´ar,
P.; and Zitnick, C. L. 2015. Microsoft COCO Captions: Data
Collection and Evaluation Server. CoRR , abs/1504.00325.
Chen, Y .; Li, L.; Yu, L.; Kholy, A. E.; Ahmed, F.; Gan, Z.;
Cheng, Y .; and Liu, J. 2020. UNITER: UNiversal Image-
TExt Representation Learning. In Vedaldi, A.; Bischof, H.;
Brox, T.; and Frahm, J., eds., ECCV , volume 12375 of Lec-
ture Notes in Computer Science , 104–120.
Cimpoi, M.; Maji, S.; Kokkinos, I.; Mohamed, S.; and
Vedaldi, A. 2014. Describing Textures in the Wild. In CVPR ,
3606–3613.
Cui, L.; Wu, Y .; Liu, J.; Yang, S.; and Zhang, Y . 2021.
Template-Based Named Entity Recognition Using BART. In
Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., ACL Find-
ings, volume ACL/IJCNLP 2021 of Findings of ACL , 1835–
1845.
Deng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; and Fei-Fei, L.
2009. ImageNet: A large-scale hierarchical image database.
InCVPR , 248–255.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Burstein, J.; Doran, C.; and
Solorio, T., eds., NAACL-HLT , 4171–4186.
Dou, Z.; Xu, Y .; Gan, Z.; Wang, J.; Wang, S.; Wang, L.; Zhu,
C.; Zhang, P.; Yuan, L.; Peng, N.; Liu, Z.; and Zeng, M.
2022. An Empirical Study of Training End-to-End Vision-
and-Language Transformers. In CVPR , 18145–18155.
Fei-Fei, L.; Fergus, R.; and Perona, P. 2007. Learning gen-
erative visual models from few training examples: An incre-
mental Bayesian approach tested on 101 object categories.
Comput. Vis. Image Underst. , 106(1): 59–70.
Gao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang,
Y .; Li, H.; and Qiao, Y . 2021. CLIP-Adapter: Better
Vision-Language Models with Feature Adapters. CoRR ,
abs/2110.04544.Goyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and
Parikh, D. 2017. Making the V in VQA Matter: Elevating
the Role of Image Understanding in Visual Question An-
swering. In CVPR , 6325–6334.
Guo, D.; Rush, A. M.; and Kim, Y . 2021. Parameter-
Efficient Transfer Learning with Diff Pruning. In Zong, C.;
Xia, F.; Li, W.; and Navigli, R., eds., ACL, 4884–4896.
He, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; and Neubig,
G. 2022. Towards a Unified View of Parameter-Efficient
Transfer Learning. In ICLR .
Helber, P.; Bischke, B.; Dengel, A.; and Borth, D. 2019. Eu-
roSAT: A Novel Dataset and Deep Learning Benchmark for
Land Use and Land Cover Classification. IEEE J. Sel. Top.
Appl. Earth Obs. Remote. Sens. , 12(7): 2217–2226.
Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;
de Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,
S. 2019. Parameter-Efficient Transfer Learning for NLP.
In Chaudhuri, K.; and Salakhutdinov, R., eds., ICML , vol-
ume 97 of Proceedings of Machine Learning Research ,
2790–2799.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-
tation of Large Language Models. In ICLR .
Huang, Z.; Zeng, Z.; Huang, Y .; Liu, B.; Fu, D.; and Fu,
J. 2021. Seeing Out of the Box: End-to-End Pre-Training
for Vision-Language Representation Learning. In CVPR ,
12976–12985.
Huang, Z.; Zeng, Z.; Liu, B.; Fu, D.; and Fu, J. 2020. Pixel-
BERT: Aligning Image Pixels with Text by Deep Multi-
Modal Transformers. CoRR , abs/2004.00849.
Jia, M.; Tang, L.; Chen, B.; Cardie, C.; Belongie, S. J.; Har-
iharan, B.; and Lim, S. 2022. Visual Prompt Tuning. In
ECCV , volume 13693, 709–727.
Kim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-
Language Transformer Without Convolution or Region Su-
pervision. In Meila, M.; and Zhang, T., eds., ICML , volume
139 of Proceedings of Machine Learning Research , 5583–
5594.
Krause, J.; Stark, M.; Deng, J.; and Fei-Fei, L. 2013. 3D
Object Representations for Fine-Grained Categorization. In
ICCV , 554–561.
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;
and Soricut, R. 2020. ALBERT: A Lite BERT for Self-
supervised Learning of Language Representations. In ICLR .
Lester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power
of Scale for Parameter-Efficient Prompt Tuning. In Moens,
M.; Huang, X.; Specia, L.; and Yih, S. W., eds., EMNLP ,
3045–3059.
Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.
2020. BART: Denoising Sequence-to-Sequence Pre-training
for Natural Language Generation, Translation, and Compre-
hension. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault,
J. R., eds., ACL, 7871–7880.
Li, C.; Farkhoor, H.; Liu, R.; and Yosinski, J. 2018. Mea-
suring the Intrinsic Dimension of Objective Landscapes. In
ICLR .

--- PAGE 9 ---
Li, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.; and Chang, K.
2019. VisualBERT: A Simple and Performant Baseline for
Vision and Language. CoRR , abs/1908.03557.
Li, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing
Continuous Prompts for Generation. In Zong, C.; Xia, F.;
Li, W.; and Navigli, R., eds., ACL, 4582–4597.
Liu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang,
Z.; and Tang, J. 2021. GPT Understands, Too. CoRR ,
abs/2103.10385.
Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .
2019. RoBERTa: A Robustly Optimized BERT Pretraining
Approach. CoRR , abs/1907.11692.
Mahabadi, R. K.; Henderson, J.; and Ruder, S. 2021. Com-
pacter: Efficient Low-Rank Hypercomplex Adapter Layers.
In Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.; Liang, P.;
and Vaughan, J. W., eds., NeurIPS , 1022–1035.
Mahabadi, R. K.; Ruder, S.; Dehghani, M.; and Hender-
son, J. 2021. Parameter-efficient Multi-task Fine-tuning for
Transformers via Shared Hypernetworks. In Zong, C.; Xia,
F.; Li, W.; and Navigli, R., eds., ACL, 565–576.
Maji, S.; Rahtu, E.; Kannala, J.; Blaschko, M. B.; and
Vedaldi, A. 2013. Fine-Grained Visual Classification of Air-
craft. CoRR , abs/1306.5151.
Mao, Y .; Mathias, L.; Hou, R.; Almahairi, A.; Ma, H.; Han,
J.; Yih, S.; and Khabsa, M. 2022. UniPELT: A Unified
Framework for Parameter-Efficient Language Model Tun-
ing. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds.,
ACL, 6253–6264.
Nilsback, M.; and Zisserman, A. 2008. Automated Flower
Classification over a Large Number of Classes. In ICVGIP ,
722–729.
Parkhi, O. M.; Vedaldi, A.; Zisserman, A.; and Jawahar,
C. V . 2012. Cats and dogs. In CVPR , 3498–3505.
Petroni, F.; Rockt ¨aschel, T.; Riedel, S.; Lewis, P. S. H.;
Bakhtin, A.; Wu, Y .; and Miller, A. H. 2019. Language Mod-
els as Knowledge Bases? In Inui, K.; Jiang, J.; Ng, V .; and
Wan, X., eds., ACL, 2463–2473.
Plummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo,
J. C.; Hockenmaier, J.; and Lazebnik, S. 2017. Flickr30k
Entities: Collecting Region-to-Phrase Correspondences for
Richer Image-to-Sentence Models. IJCV , 123(1): 74–93.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
Krueger, G.; and Sutskever, I. 2021. Learning Transfer-
able Visual Models From Natural Language Supervision. In
Meila, M.; and Zhang, T., eds., ICML , volume 139 of Pro-
ceedings of Machine Learning Research , 8748–8763.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer. JMLR , 21: 140:1–140:67.
Ren, M.; Kiros, R.; and Zemel, R. 2015. Exploring models
and data for image question answering. NeurIPS , 28.
Ren, S.; He, K.; Girshick, R. B.; and Sun, J. 2015. Faster
R-CNN: Towards Real-Time Object Detection with RegionProposal Networks. In Cortes, C.; Lawrence, N. D.; Lee,
D. D.; Sugiyama, M.; and Garnett, R., eds., NeurIPS , 91–
99.
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-
mer, B. 2022. High-Resolution Image Synthesis with Latent
Diffusion Models. In CVPR , 10674–10685.
Ruiz, N.; Li, Y .; Jampani, V .; Pritch, Y .; Rubinstein, M.;
and Aberman, K. 2022. DreamBooth: Fine Tuning Text-
to-image Diffusion Models for Subject-Driven Generation.
Soomro, K.; Zamir, A. R.; and Shah, M. 2012. UCF101: A
Dataset of 101 Human Actions Classes From Videos in The
Wild. CoRR , abs/1212.0402.
Su, W.; Zhu, X.; Cao, Y .; Li, B.; Lu, L.; Wei, F.; and Dai, J.
2020. VL-BERT: Pre-training of Generic Visual-Linguistic
Representations. In ICLR .
Suhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; and Artzi,
Y . 2019. A Corpus for Reasoning about Natural Language
Grounded in Photographs. In Korhonen, A.; Traum, D. R.;
and M `arquez, L., eds., ACL, 6418–6428.
Sung, Y .; Cho, J.; and Bansal, M. 2022. VL-ADAPTER:
Parameter-Efficient Transfer Learning for Vision-and-
Language Tasks. In CVPR , 5217–5227.
Sung, Y .; Nair, V .; and Raffel, C. 2021. Training Neural Net-
works with Fixed Sparse Masks. In Ranzato, M.; Beygelz-
imer, A.; Dauphin, Y . N.; Liang, P.; and Vaughan, J. W., eds.,
NeurIPS , 24193–24205.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention is All you Need. In Guyon, I.; von Luxburg, U.;
Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.
V . N.; and Garnett, R., eds., NeurIPS , 5998–6008.
Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba,
A. 2010. SUN database: Large-scale scene recognition from
abbey to zoo. In CVPR , 3485–3492.
Zhang, J. O.; Sax, A.; Zamir, A.; Guibas, L. J.; and Malik,
J. 2020. Side-Tuning: A Baseline for Network Adaptation
via Additive Side Networks. In Vedaldi, A.; Bischof, H.;
Brox, T.; and Frahm, J., eds., ECCV , volume 12348 of Lec-
ture Notes in Computer Science , 698–714.
Zhang, R.; Fang, R.; Zhang, W.; Gao, P.; Li, K.; Dai, J.;
Qiao, Y .; and Li, H. 2021. Tip-Adapter: Training-free CLIP-
Adapter for Better Vision-Language Modeling. CoRR ,
abs/2111.03930.
Zhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022a. Con-
ditional Prompt Learning for Vision-Language Models. In
CVPR , 16795–16804.
Zhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022b. Learn-
ing to Prompt for Vision-Language Models. IJCV , 130(9):
2337–2348.
Zhou, Y .; Ji, R.; Sun, X.; Luo, G.; Hong, X.; Su, J.; Ding,
X.; and Shao, L. 2020. K-armed Bandit based Multi-Modal
Network Architecture Search for Visual Question Answer-
ing. In Chen, C. W.; Cucchiara, R.; Hua, X.; Qi, G.; Ricci,
E.; Zhang, Z.; and Zimmermann, R., eds., ACM MM , 1245–
1254.
