# 2309.06526.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.06526.pdf
# File size: 327471 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
EXPLORING THE BENEFITS OF DIFFERENTIALLY PRIV ATE PRE-TRAINING AND
PARAMETER-EFFICIENT FINE-TUNING FOR TABLE TRANSFORMERS
Xilong Wang⋆, Chia-Mu Yu†, and Pin-Yu Chen‡
⋆University of Science and Technology of China, Hefei, China
†National Yang Ming Chiao Tung University, Hsinchu, Taiwan
‡IBM Research, New York, USA
ABSTRACT
For machine learning with tabular data, Table Transformer
(TabTransformer) is a state-of-the-art neural network model,
while Differential Privacy (DP) is an essential component
to ensure data privacy. In this paper, we explore the bene-
fits of combining these two aspects together in the scenario
of transfer learning – differentially private pre-training and
fine-tuning of TabTransformers with a variety of parameter-
efficient fine-tuning (PEFT) methods, including Adapter,
LoRA, and Prompt Tuning. Our extensive experiments on
the ACSIncome dataset show that these PEFT methods out-
perform traditional approaches in terms of the accuracy of
the downstream task and the number of trainable parame-
ters, thus achieving an improved trade-off among parameter
efficiency, privacy, and accuracy. Our code is available at
https://github.com/IBM/DP-TabTransformer .
Index Terms —Table Transformer, Differential Privacy,
Transfer Learning
1. INTRODUCTION
Table transformer (TabTransformer) [1] is a novel deep tab-
ular data modeling for various scenarios, such as supervised
and semi-supervised learning. Its main contribution is to
transform regular categorical embeddings into contextual
ones, thus achieving higher accuracy compared to previous
state-of-the-art methods. On the other hand, differential Pri-
vacy (DP) [5] is a frequently used technique to ensure privacy
for individual data points in a training dataset. DP-SGD [6],
which combines DP with stochastic gradient descent (SGD),
is one of the most frequently used optimization techniques
in machine learning (ML) to train models on sensitive data
while safeguarding individual privacy.
In the literature, DP-SGD techniques either fine-tune a
pre-trained model or train a model from scratch. However,
almost none of them have focused on TabTransformer. In this
paper, we implement various recent parameter-efficient fine-
tuning techniques, such as LoRA [2], Adapter [3], and Prompt
Tuning [4] (both Shallow Tuning and Deep Tuning), so as to
explore the benefits of differentially private pre-training andfine-tuning for TabTransformers. To summarize, our key con-
tributions are as follows: 1) We study an unexplored scenario
for transfer learning in TabTransformers with DP, i.e., im-
plementing various kinds of parameter-efficient techniques in
the fine-tuning stage instead of full tuning. 2) Different from
previous tabular learning methods which mainly exploited
DP at the fine-tuning stage, we study the use of DP-SGD
for both pre-training and fine-tuning, thus ensuring end-to-
end privacy. 3) Our experiments on the ACSIncome dataset
showed that the accuracy outperforms the baselines in most
cases, while the parameter efficiency improves by more than
97.86% . In addition, we report the best advantageous PEFT
setting to inform and inspire the future design of DP-aware
pre-training and fine-tuning for TabTransformers.
2. BACKGROUND AND RELATED WORKS
2.1. Differential Privacy (DP) and DP-SGD
ML is widely known for its ability to analyze large datasets,
identify patterns, and make predictions or decisions based on
that data. However, this also introduces the risk of disclosing
sensitive information from the training dataset. DP [5] and
DP-SGD [6] are introduced to address this issue. A random-
ized algorithm Asatisfies (ϵ, δ)−DP if it holds that:
P[A(D)∈S]≤exp(ϵ)P[A(D′)∈S] +δ, (1)
where P[A(D)∈S]is the probability that the output of A
on dataset Dfalls within a set S, andP[A(D′)∈S]is the
probability that the output of the Aon a neighboring dataset
D′(which differs from Dby one data point) falls within S.
The smaller ϵis, the stronger privacy guarantee Ahas.
Inspired by DP, Differential Privacy Stochastic Gradi-
ent Descent (DP-SGD) [6] is one of the most widely used
privacy-preserving optimization techniques in ML [12–15].
It is a two-stage procedure. Formally, given the SGD gradient
estimator gevaluated on the training dataset, and define its
sensitivity Sgas the maximum of ∥g(D)−g(D′)∥2. In the
first stage, DP-SGD adds a zero-mean Gaussian noise with a
given covariance matrix, i.e., N(0, S2
gσ2I)to the computedarXiv:2309.06526v1  [cs.LG]  12 Sep 2023

--- PAGE 2 ---
gradient estimator as follows:
g(D) +N(0, S2
gσ2I).
In the second stage, DP-SGD passes the gradient estimator
through the Clip operator:
Clip(x) =x
max{1,∥x∥2/C},
so as to fix the sensitivity of the gradient estimator at a hyper-
parameter C. However, regrettably, almost none of DP-SGD
techniques have been applied to the study of TabTransformer.
2.2. Parameter-Efficient Fine-Tuning (PEFT)
PEFT [2–4, 9–11] is an emerging technique in the field of
transfer learning that aims to adapt large pre-trained mod-
els to specific tasks with a smaller number of task-specific
parameters. It fine-tunes the pre-trained model on a target
task while keeping the majority of the original model’s pa-
rameters frozen. Compared to full-tuning which fine-tunes
the entire model, this approach reduces the computational re-
sources and memory requirements needed for task-specific
adaptation. PEFT is particularly valuable in scenarios with
limited computing resources or when deploying models to
resource-constrained environments, without sacrificing task
performance. The most popular PEFT techniques include
LoRA [2], Adapter [3], and (Deep/Shallow) Prompt Tuning
[4]. Nevertheless, similar to standard ML, PEFT also faces
the risk of disclosing sensitive data throughout the fine-tuning
procedure and thus needs a privacy guarantee [7].
3. METHODOLOGY
3.1. TabTransformer
TabTransformer [1] is a deep learning architecture for tabu-
lar data modeling. It uses contextual embeddings to achieve
higher prediction accuracy and better interpretability. It out-
performs state-of-the-art deep learning methods for tabular
data and is highly robust against missing or noisy data fea-
tures. The brief structure of TabTransformer is displayed in
Fig. 1 (a). The TabTransformer architecture consists of a col-
umn embedding layer, a stack of NTransformer blocks, and a
multi-layer perceptron (MLP). Each Transformer layer com-
prises a multi-head self-attention layer followed by a position-
wise feed-forward layer. As shown in Fig. 1 (a), the areas
highlighted in red are where we can perform PEFT. To be
specific, we implemented LoRA [2] and Adapter [3] in Trans-
former Blocks, while Deep Tuning and Shallow Tuning [4]
were exploited in MLP.
3.2. Deep Tuning and Shallow Tuning
Visual Prompt Tuning (VPT) [4] is an efficient alternative
compared to full fine-tuning for large-scale Transformer
Column Embedding LayerNormTransformer
Blocks × NConcatenationMulti-Layer PerceptionLoss
Categorical Features Numerical Features(a) Architecture of TabTransformer.
Input Layer ∈ ℝ⁶
Hidden Layer ∈ ℝ⁶
Hidden Layer ∈ ℝ⁶
Output Layer ∈ ℝ¹ (b) Overview of Deep Tuning
and Shallow Tuning.
Fig. 1 . Genreral framework of parameter-efficient tuning on
TabTransformer.
models. It offers two tuning strategies: VPT-Deep and
VPT-Shallow. VPT-Deep prepends a set of learnable pa-
rameters to each Transformer encoder layer’s input, while
VPT-Shallow only inserts the prompt parameters to the first
layer’s input. Inspired by VPT, we proposed Deep Tuning
and Shallow Tuning, which aims at fine-tuning the MLP of
TabTransformer.
3.3. Adapter
Adapter [3], as shown in Fig. 2 (b), is a transfer learning
approach that allows for efficient parameter sharing and ex-
tensibility in large pre-trained models. It uses small and task-
specific modules that are inserted between the pre-trained lay-
ers of the base model. These modules have a near-identity
initialization and a small number of parameters, which allows
for stable training and slow growth of the total model size
when more tasks are added.
3.4. LoRA
LoRA [2], as shown in Fig. 2 (c), is a low-rank adaptation
technique that reduces the number of trainable parameters for
downstream tasks while maintaining high model quality. It
works by injecting a low-rank adaptation matrix into the pre-
trained model, which can be shared and used to build many
small LoRA modules for different tasks. LoRA makes train-
ing more efficient and allows for quick task-switching.
3.5. Joint PEFT with DP-SGD
We incorporate DP-SGD into PEFT by initially pre-training
a TabTransformer model with DP on a pre-training dataset.
Subsequently, we freeze the backbone of the pre-trained

--- PAGE 3 ---
Multi-Head AttentionAdd & NormFeed ForwardAdd & Norm
Down ProjectNonlinearityUp ProjectAdd
Feed
Forwardr
(a) Transforer block (b) Adapter (c) LoRAFig. 2 . PEFT techniques applied on Transformer block.
model and apply the aforementioned PEFT techniques to
fine-tune the model in conjunction with DP-SGD on the
downstream dataset. This approach serves to safeguard the
privacy of both the pre-training dataset and the downstream
dataset, thus ensuring end-to-end privacy. To be more de-
tailed, as shown in Fig. 2 (a) and Fig. 2 (c), we combine
LoRA with Feed Forward Layer in each Transformer block
of TabTransformers. Moreover, we inject an Adapter between
the Feed Forward Layer and the Add & Norm Layer in each
Transformer block of TabTransformers. For Deep Tuning and
Shallow Tuning, as shown in Fig. 1 (b), Deep Tuning tunes
certain neurons in every layer of MLP, i.e., the red-marked
part in Fig. 1 (b). Meanwhile, Shallow Tuning only tunes a
few neurons in MLP’s input layer, i.e., the green-marked part
in the figure.
4. PERFORMANCE EV ALUATION
In this section, we test the performance of all mentioned
PEFT approaches and identify the most effective one to bene-
fit future research. For comparison, we exploit two baselines,
i.e., full tuning and training from scratch. Furthermore, to
illustrate the impact of PEFT, we also evaluate the pre-trained
model directly on the downstream data without PEFT (i.e.,
Zero-shot Inference). Experimental results clearly show that
PEFT methods ensure high parameter efficiency without the
loss of accuracy, thus outperforming basic approaches in
terms of accuracy, parameter efficiency, and privacy.
4.1. Experiment Setup
ACSIncome dataset . The ACSIncome dataset [8] is derived
from the American Community Survey (ACS) Public Use Mi-
crodata Sample (PUMS) data. It aims to predict whether US
working adults’ yearly income is above $50,000. It covers all
states of the United States and includes multiple years. In-
spired by this feature, we can perform pre-training and fine-
tuning across different states. To be more detailed, we chose
two states, California (CA) and Indiana (IN), which are geo-
graphically distant and have significant differences in popula-
tion size and economic disparities. CA and IN exhibit obvioustraining set distribution shifts, and therefore we utilized them
for the study of pre-training and fine-tuning with TabTrans-
former.
Baselines . (1) Full Tuning: In this scenario, after pre-
training, Full Tuning tunes all parameters of the pre-trained
model. (2) Train from scratch: This baseline simply trains the
entire model on the downstream dataset from scratch without
pre-training. (3) Zero-shot Inference: To emphasize the effect
of PEFT, after obtaining a pre-trained model, this baseline
directly evaluates the performance of the same model on the
downstream dataset.
Parameters . (1) Let epindicate ϵused for pre-training
andefdenote ϵused for fine-tuning. The values for epandef
are chosen from {0.5,1,2,4,8,16,32}. (2) Clipping norm
C= 2. (3) δ= 10−5. (4) For TabTransformer, we set the
hidden (embedding) dimension, the number of Transformer
blocks, and the number of attention heads to be 32, 4, and 8,
respectively. The size of MLP is 5 layers with 72 units for
each layer. (4) Batch Size B= 64 for both pre-training and
fine-tuning. (5) Full tuning tunes 8 units (tokens) in every
MLP layer, and Shallow Tuning tunes 8 units just in the first
layer of MLP.
4.2. Experimental Results
Number of Trainable Parameters. The degree of parame-
ter efficiency in a PEFT technique hinges on the number of
parameters that remain trainable during fine-tuning. Let N
represent the number of trainable parameters, and then the N
of all the techniques are shown in Table 1.
Table 1 . Number of Trainable Parameters of Various Methods
Methods Deep Tuning Full Tuning Shallow Tuning
N 4,408 206,193 2,072
Methods Adapter LoRA Train from Scratch
N 1,424 1,424 206,193
Based on the parameter counts in Table 1, we can arrive
at the following conclusion. When we make a comparison
between the PEFT methods listed in the table and the base-
line methods (Full Tuning and Train from Scratch), it be-
comes evident that all the PEFT approaches have substan-
tially decreased the value of Nby at least206,193−4,408
206,193=
97.86% . To delve into the specifics, it’s worth highlight-
ing that LoRA and Adapter emerge as the most parameter-
efficient alternatives, exhibiting a remarkable reduction in N
by206,193−1,424
206,193=99.3% .
Testing Accuracy. In our endeavor to evaluate the per-
formance of all PEFT methods against the baseline, the Tab-
Transformer model underwent a two-step procedure. Ini-
tially, it was subjected to pretraining on the ACSIncome
dataset sourced from California (195,665 samples in total),
followed by fine-tuning on the ACSIncome dataset from
Indiana (35,022 samples in total). The entire process of
pretraining and finetuning is performed using DP-SGD. For

--- PAGE 4 ---
Table 2 . Testing Accuracy Comparison.
Methods
Full Tuning
ϵpAcc ϵ f0.5 1 2 4 8 16 32
0.5 0.4345 0.6325 0.6985 0.6989 0.7018 0.7168 0.7221
1 0.4468 0.6811 0.7001 0.7028 0.7031 0.7172 0.7243
2 0.4714 0.7024 0.7046 0.7054 0.7141 0.7201 0.7302
4 0.5473 0.7065 0.7075 0.7098 0.7232 0.7313 0.7355
8 0.6397 0.7088 0.7125 0.7228 0.7253 0.7352 0.7445
16 0.6859 0.7136 0.7149 0.7255 0.7263 0.749 0.7507
32 0.6889 0.7189 0.7289 0.7348 0.735 0.7512 0.7543
Deep Tuning [4]
ϵpAcc ϵ f0.5 1 2 4 8 16 32
0.5 0.6664 0.6865 0.7012 0.7048 0.7091 0.7109 0.7259
1 0.6969 0.7099 0.7142 0.7188 0.7195 0.7261 0.7319
2 0.6999 0.7108 0.7201 0.7212 0.7269 0.7336 0.736
4 0.7001 0.7263 0.7271 0.7285 0.7329 0.7362 0.7385
8 0.7132 0.7275 0.7319 0.735 0.7362 0.7429 0.7432
16 0.7182 0.7332 0.7359 0.7375 0.7399 0.7438 0.7445
32 0.7239 0.7362 0.7378 0.7409 0.743 0.746 0.75
Adapter [3]
ϵpAcc ϵ f0.5 1 2 4 8 16 32
0.5 0.6985 0.7086 0.7105 0.7112 0.7131 0.7162 0.7213
1 0.7044 0.7143 0.7172 0.7185 0.7196 0.7252 0.7259
2 0.7158 0.7246 0.7248 0.7261 0.7273 0.7288 0.7335
4 0.7175 0.7261 0.7278 0.7291 0.7298 0.7308 0.7336
8 0.7209 0.7279 0.7292 0.7335 0.7345 0.7345 0.7366
16 0.7288 0.7352 0.7358 0.7359 0.7366 0.7376 0.7436
32 0.7368 0.7386 0.7435 0.7452 0.7453 0.747 0.7475
LoRA [2]
ϵpAcc ϵ f0.5 1 2 4 8 16 32
0.5 0.6754 0.6772 0.6996 0.7008 0.7096 0.7108 0.7116
1 0.6791 0.7048 0.7113 0.7116 0.7155 0.7182 0.7202
2 0.6871 0.7155 0.7171 0.7172 0.7181 0.7209 0.7241
4 0.6966 0.7178 0.7181 0.7196 0.7198 0.7256 0.7256
8 0.7203 0.7215 0.7216 0.7232 0.7258 0.7266 0.7318
16 0.7231 0.7253 0.7269 0.7271 0.7292 0.7295 0.7336
32 0.7272 0.7293 0.7348 0.737 0.7373 0.7463 0.7472
Shallow Tuning [4]
ϵpAcc ϵ f0.5 1 2 4 8 16 32
0.5 0.5019 0.5879 0.588 0.6363 0.6801 0.6922 0.6962
1 0.6348 0.6959 0.7039 0.7094 0.7095 0.7095 0.7143
2 0.6514 0.6984 0.7098 0.7162 0.7173 0.7175 0.7183
4 0.7009 0.7046 0.7173 0.7192 0.7203 0.7248 0.7282
8 0.7142 0.7188 0.7228 0.7275 0.7306 0.7313 0.7333
16 0.7263 0.7269 0.7272 0.7335 0.7342 0.7388 0.7395
32 0.736 0.7382 0.7392 0.7409 0.7445 0.7449 0.7452
Train from Scratch
ϵ 0.5 1 2 4 8 16 32
Acc 0.633 0.6889 0.6998 0.7002 0.7008 0.7011 0.7099
Zero-shot Inference
ϵ 0.5 1 2 4 8 16 32
Acc 0.6471 0.6604 0.6682 0.6711 0.6814 0.7016 0.7098evaluation, we randomly split 20% of ACSIncome-Indiana as
the test set. Furthermore, we opted for the utilization of the
accuracy metric, denoted as Accuracy (Acc), as the primary
evaluation criterion for assessing the TabTransformer’s abil-
ity to predict whether an individual’s annual income exceeds
$50,000. The detailed results are shown in Table 2.
Based on the findings presented in Table 2 and Table 1,
we can infer the following conclusions. Firstly, all the PEFT
methods demonstrate comparable Accwhen compared to the
baselines. For example, when ϵp, ϵfare both set to 32, the
Acc of Deep Tuning, Adapter, LoRA, and Shallow Tuning is
0.75,0.7475,0.7472,and0.7452 , respectively. Meanwhile,
when ϵ= 32 , theAcc of Train from Scratch and Zero-shot
Inference are 0.7099 and0.7098 , respectively. These val-
ues suggest that when compared to Train from Scratch and
Zero-shot Inference, the PEFT techniques increase the Acc
by at least 4.7% . Moreover, the Acc of PEFT is not signifi-
cantly lagging behind that of Full Tuning. Hence, to sum up,
PEFT techniques achieve excellent levels of accuracy ( Acc)
while demonstrating a remarkably high degree of parameter
efficiency. Secondly, PEFT methods exhibit a robust toler-
ance to low values of ϵcompared to Full Tuning. which in-
dicates that PEFT can ensure a higher level of privacy than
Full Tuning. For example, when (ϵp, ϵf) = (32 ,0.5), the
Accof Deep Tuning, Adapter, LoRA, and Shallow Tuning are
0.7239,0.7368,0.7272,0.736, respectively, while the Acc of
Full Tuning is 0.6889 . Hence, when (ϵp, ϵf) = (32 ,0.5), the
Accof PEFT is at least 3.5% higher than Full Tuning. Third,
we find that Deep Tuning and Adapter outperform other meth-
ods in most cases. Considering Adapter is more parameter-
efficient than Deep Tuning as shown in Table 1, we can con-
clude that the Adapter achieves the best trade-off among pri-
vacy, accuracy, and parameter efficiency.
5. CONCLUSION
In this paper, we presented a pilot study exploring the ben-
efits of combining differentially private pre-training and
parameter-efficient fine-tuning (PEFT) for TabTransfomrers
with a variety of fine-tuning methods, including Adapter [3],
LoRA [2], Deep/Shallow Tuning [4]. We conducted extensive
experiments on the ACSIncome dataset with different con-
figurations. The results in Table 1 indicate that the number
of trainable parameters of PEFT techniques reduces at least
97.86% compared to baselines. The results in Table 2 show
that the accuracy of PEFT methods outperforms baselines in
most cases. Hence, compared to three baselines which are
either parameter-consuming or ineffective, PEFT techniques
achieve a significantly improved trade-off among privacy,
accuracy, and parameter efficiency. We also find that Adapter
is the most optimal setting for PEFT in this setting. Our
study uncovers the unexplored benefits and provides new in-
sights into applying PEPT on differentially private pre-trained
TabTransformer for differentially private transfer learning.

--- PAGE 5 ---
6. REFERENCES
[1] Xin Huang, Ashish Khetan, Milan Cvitkovic, and
Zohar Karnin. Tabtransformer: Tabular data mod-
eling using contextual embeddings. arXiv preprint
arXiv:2012.06678 , 2020.
[2] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large lan-
guage models. arXiv preprint arXiv:2106.09685 , 2021.
[3] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. Parameter-
efficient transfer learning for nlp. In International
Conference on Machine Learning , pages 2790–2799.
PMLR, 2019.
[4] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire
Cardie, Serge Belongie, Bharath Hariharan, and Ser-
Nam Lim. Visual prompt tuning. In European Con-
ference on Computer Vision , pages 709–727. Springer,
2022.
[5] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography: Third Theory
of Cryptography Conference, TCC 2006, New York, NY,
USA, March 4-7, 2006. Proceedings 3 , pages 265–284.
Springer, 2006.
[6] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In Proceedings
of the 2016 ACM SIGSAC conference on computer and
communications security , pages 308–318, 2016.
[7] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,
Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni,
Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Dif-
ferentially private fine-tuning of language models. arXiv
preprint arXiv:2110.06500 , 2021.
[8] Frances Ding, Moritz Hardt, John Miller, and Ludwig
Schmidt. Retiring adult: New datasets for fair machine
learning. Advances in neural information processing
systems , 34:6478–6490, 2021.
[9] Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebas-
tian Ruder. Mad-x: An adapter-based framework for
multi-task cross-lingual transfer. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 7654–7673, 2020.
[10] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Scott Yih, and MadianKhabsa. Unipelt: A unified framework for parameter-
efficient language model tuning. In Proceedings of
the 60th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers) , pages
6253–6264, 2022.
[11] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. In Proceedings
of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers) , pages 4582–4597, 2021.
[12] Muah Kim, Onur G ¨unl¨u, and Rafael F Schaefer. Feder-
ated learning with local differential privacy: Trade-offs
between privacy, utility, and communication. In ICASSP
2021-2021 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP) , pages
2650–2654. IEEE, 2021.
[13] Christophe Dupuy, Radhika Arava, Rahul Gupta, and
Anna Rumshisky. An efficient dp-sgd mechanism for
large scale nlu models. In ICASSP 2022-2022 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 4118–4122. IEEE, 2022.
[14] Huzaifa Arif, Alex Gittens, and Pin-Yu Chen.
Reprogrammable-fl: Improving utility-privacy tradeoff
in federated learning via model reprogramming. In 2023
IEEE Conference on Secure and Trustworthy Machine
Learning (SaTML) , pages 197–209. IEEE, 2023.
[15] Yizhe Li, Yu-Lin Tsai, Xuebin Ren, Chia-Mu Yu,
and Pin-Yu Chen. Exploring the benefits of vi-
sual prompting in differential privacy. arXiv preprint
arXiv:2303.12247 , 2023.
