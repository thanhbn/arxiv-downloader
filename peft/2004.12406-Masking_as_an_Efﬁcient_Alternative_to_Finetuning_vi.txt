# Che mặt nạ như một Thay thế Hiệu quả cho Tinh chỉnh
# cho các Mô hình Ngôn ngữ được Huấn luyện trước

Mengjie Zhao†*, Tao Lin‡*, Fei Mi‡, Martin Jaggi‡, Hinrich Schütze†
†LMU Munich, Đức ‡EPFL, Thụy Sĩ

## Tóm tắt
Chúng tôi trình bày một phương pháp hiệu quả để sử dụng các mô hình ngôn ngữ được huấn luyện trước, nơi chúng tôi học các mặt nạ nhị phân có chọn lọc cho các trọng số được huấn luyện trước thay vì sửa đổi chúng thông qua tinh chỉnh. Các đánh giá mở rộng về việc che mặt nạ BERT, RoBERTa, và DistilBERT trên mười một tác vụ NLP đa dạng cho thấy rằng sơ đồ che mặt nạ của chúng tôi mang lại hiệu suất tương đương với tinh chỉnh, nhưng có dung lượng bộ nhớ nhỏ hơn nhiều khi cần suy luận cho nhiều tác vụ. Các đánh giá nội tại cho thấy rằng các biểu diễn được tính toán bởi các mô hình ngôn ngữ che mặt nạ nhị phân của chúng tôi mã hóa thông tin cần thiết để giải quyết các tác vụ hạ nguồn. Phân tích cảnh quan tổn thất, chúng tôi cho thấy rằng che mặt nạ và tinh chỉnh tạo ra các mô hình nằm trong các cực tiểu có thể được kết nối bằng một đoạn thẳng với độ chính xác kiểm tra gần như không đổi. Điều này xác nhận rằng che mặt nạ có thể được sử dụng như một thay thế hiệu quả cho tinh chỉnh.

## 1 Giới thiệu
Tinh chỉnh một mô hình ngôn ngữ được huấn luyện trước lớn như BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), và XLNet (Yang et al., 2019) thường mang lại kết quả cạnh tranh hoặc thậm chí là tốt nhất trên các tiêu chuẩn đánh giá NLP (Wang et al., 2018, 2019). Cho một tác vụ NLP, tinh chỉnh tiêu chuẩn xếp một lớp tuyến tính lên trên mô hình ngôn ngữ được huấn luyện trước và sau đó cập nhật tất cả các tham số bằng mini-batch SGD. Các khía cạnh khác nhau như tính mỏng manh (Dodge et al., 2020) và tính thích ứng (Peters et al., 2019) của mô hình học chuyển giao hai giai đoạn NLP này (Dai và Le, 2015; Howard và Ruder, 2018) đã được nghiên cứu.

Mặc dù có tính đơn giản và hiệu suất ấn tượng của tinh chỉnh, số lượng tham số cần được tinh chỉnh quá lớn, ví dụ, 340 triệu trong BERT-large, là một trở ngại lớn cho việc triển khai rộng rãi các mô hình này. Dung lượng bộ nhớ lớn của các mô hình được tinh chỉnh trở nên nổi bật hơn khi cần giải quyết nhiều tác vụ - phải lưu nhiều bản sao của hàng triệu tham số được tinh chỉnh để suy luận.

Nghiên cứu gần đây (Gaier và Ha, 2019; Zhou et al., 2019) chỉ ra tiềm năng của việc tìm kiếm kiến trúc mạng nơ-ron trong một mô hình cố định, như một thay thế cho việc tối ưu hóa trọng số mô hình cho các tác vụ hạ nguồn. Được truyền cảm hứng từ những kết quả này, chúng tôi trình bày che mặt nạ, một sơ đồ đơn giản nhưng hiệu quả để sử dụng các mô hình ngôn ngữ được huấn luyện trước. Thay vì trực tiếp cập nhật các tham số được huấn luyện trước, chúng tôi đề xuất chọn lọc các trọng số quan trọng cho các tác vụ NLP hạ nguồn trong khi loại bỏ những trọng số không liên quan. Cơ chế chọn lọc bao gồm một tập hợp các mặt nạ nhị phân, một mặt nạ được học cho mỗi tác vụ hạ nguồn thông qua huấn luyện đầu cuối đến đầu cuối.

Chúng tôi cho thấy rằng che mặt nạ, khi được áp dụng cho các mô hình ngôn ngữ được huấn luyện trước như BERT, RoBERTa, và DistilBERT (Sanh et al., 2019), đạt được hiệu suất tương đương với tinh chỉnh trong các tác vụ như gán nhãn từ loại, nhận dạng thực thể có tên, phân loại chuỗi, và đọc hiểu. Điều này đáng ngạc nhiên ở chỗ một cơ chế chọn lọc phụ đơn giản không thay đổi bất kỳ trọng số nào lại cạnh tranh với một chế độ huấn luyện - tinh chỉnh - có thể thay đổi giá trị của từng trọng số đơn lẻ.

Chúng tôi tiến hành phân tích chi tiết tiết lộ các yếu tố quan trọng và lý do có thể cho hiệu suất mong muốn của che mặt nạ.

Che mặt nạ là hiệu quả về tham số: chỉ cần lưu một tập hợp các mặt nạ nhị phân 1-bit cho mỗi tác vụ sau khi huấn luyện, thay vì tất cả các tham số float 32-bit trong tinh chỉnh. Dung lượng bộ nhớ nhỏ này cho phép triển khai các mô hình ngôn ngữ được huấn luyện trước để giải quyết nhiều tác vụ trên các thiết bị biên. Tính nhỏ gọn của che mặt nạ cũng tự nhiên cho phép các tổ hợp hiệu quả về tham số của các mô hình ngôn ngữ được huấn luyện trước.

**Các đóng góp của chúng tôi:** (i) Chúng tôi giới thiệu che mặt nạ, một sơ đồ mới để sử dụng các mô hình ngôn ngữ được huấn luyện trước bằng cách học các mặt nạ chọn lọc cho các trọng số được huấn luyện trước, như một thay thế hiệu quả cho tinh chỉnh. Chúng tôi cho thấy rằng che mặt nạ có thể áp dụng cho các mô hình như BERT/RoBERTa/DistilBERT, và tạo ra hiệu suất ngang bằng với tinh chỉnh. (ii) Chúng tôi tiến hành phân tích thực nghiệm mở rộng về che mặt nạ, làm sáng tỏ các yếu tố quan trọng để đạt được hiệu suất tốt trên mười một tác vụ NLP đa dạng. (iii) Chúng tôi nghiên cứu cảnh quan tổn thất và biểu diễn ngôn ngữ của các mô hình ngôn ngữ che mặt nạ nhị phân, tiết lộ lý do tiềm năng tại sao che mặt nạ có hiệu suất tác vụ tương đương với tinh chỉnh.

## 2 Nghiên cứu Liên quan

**Mô hình NLP hai giai đoạn.** Các mô hình ngôn ngữ được huấn luyện trước (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Radford et al., 2019) thúc đẩy NLP với biểu diễn từ theo ngữ cảnh. Tinh chỉnh một mô hình ngôn ngữ được huấn luyện trước (Dai và Le, 2015; Howard và Ruder, 2018) thường mang lại hiệu suất cạnh tranh một phần vì huấn luyện trước dẫn đến khởi tạo tốt hơn trên các tác vụ hạ nguồn khác nhau so với huấn luyện từ đầu (Hao et al., 2019). Tuy nhiên, tinh chỉnh trên các tác vụ NLP riêng lẻ không hiệu quả về tham số. Mỗi mô hình được tinh chỉnh, thường bao gồm hàng trăm triệu tham số điểm thả nổi, cần được lưu riêng lẻ. Stickland và Murray (2019) sử dụng các lớp chú ý được chiếu với học đa tác vụ để cải thiện hiệu quả của việc tinh chỉnh BERT. Houlsby et al. (2019) chèn các mô-đun adapter vào BERT để cải thiện hiệu quả bộ nhớ. Các mô-đun được chèn thay đổi quá trình truyền tiến của BERT, do đó cần được khởi tạo cẩn thận để gần với hàm đồng nhất.

Chúng tôi đề xuất trực tiếp chọn các tham số phù hợp cho một tác vụ hạ nguồn, bằng cách học các mặt nạ nhị phân chọn lọc thông qua huấn luyện đầu cuối đến đầu cuối. Giữ các tham số được huấn luyện trước không thay đổi, chúng tôi giải quyết nhiều tác vụ NLP hạ nguồn với chi phí tối thiểu.

**Mạng nhị phân và cắt tỉa mạng.** Các mặt nạ nhị phân có thể được huấn luyện bằng "bộ ước lượng thẳng" (Bengio et al., 2013; Hinton, 2012). Hubara et al. (2016), Rastegari et al. (2016), Hubara et al. (2017), và những người khác áp dụng kỹ thuật này để huấn luyện các mạng nơ-ron nhị phân hiệu quả. Chúng tôi sử dụng bộ ước lượng này để huấn luyện các mặt nạ chọn lọc cho các tham số mô hình ngôn ngữ được huấn luyện trước.

Nghiên cứu giả thuyết vé số (Frankle và Carbin, 2018) của cắt tỉa mạng (Han et al., 2015a; He et al., 2018; Liu et al., 2019c; Lee et al., 2019; Lin et al., 2020), Zhou et al. (2019) phát hiện rằng việc áp dụng mặt nạ nhị phân cho một mạng nơ-ron là một dạng huấn luyện mạng. Gaier và Ha (2019) đề xuất tìm kiếm kiến trúc mạng nơ-ron cho học tăng cường và các tác vụ phân loại hình ảnh, mà không cần huấn luyện trọng số rõ ràng. Công trình này truyền cảm hứng cho sơ đồ che mặt nạ của chúng tôi (có thể được diễn giải như tìm kiếm kiến trúc mạng nơ-ron ngầm (Liu et al., 2019c)): việc áp dụng mặt nạ cho một mô hình ngôn ngữ được huấn luyện trước tương tự như tinh chỉnh, nhưng hiệu quả hơn nhiều về tham số.

Có lẽ công trình gần nhất, Mallya et al. (2018) áp dụng mặt nạ nhị phân cho CNN và đạt được hiệu suất tốt trong thị giác máy tính. Chúng tôi học các mặt nạ nhị phân chọn lọc cho các mô hình ngôn ngữ được huấn luyện trước trong NLP và làm sáng tỏ các yếu tố quan trọng để đạt được hiệu suất tốt. Mallya et al. (2018) cập nhật rõ ràng các trọng số trong một lớp phân loại cụ thể cho tác vụ. Ngược lại, chúng tôi cho thấy rằng việc học đầu cuối đến đầu cuối các mặt nạ chọn lọc, nhất quán cho cả mô hình ngôn ngữ được huấn luyện trước và một lớp phân loại được khởi tạo ngẫu nhiên, đạt được hiệu suất tốt. Radiya-Dixit và Wang (2020) nghiên cứu tinh chỉnh BERT bằng cách sử dụng một số kỹ thuật, bao gồm cả những gì họ gọi là sparsification, một phương pháp tương tự như che mặt nạ. Trọng tâm của họ là phân tích tinh chỉnh BERT trong khi mục tiêu của chúng tôi là cung cấp một thay thế hiệu quả cho tinh chỉnh.

## 3 Phương pháp

### 3.1 Bối cảnh về Transformer và tinh chỉnh

Bộ mã hóa của kiến trúc Transformer (Vaswani et al., 2017) được sử dụng phổ biến khi huấn luyện trước các mô hình ngôn ngữ lớn. Chúng tôi tóm tắt ngắn gọn kiến trúc của nó và sau đó trình bày sơ đồ che mặt nạ của chúng tôi. Lấy BERT-base làm ví dụ, mỗi một trong 12 khối transformer bao gồm (i) bốn lớp tuyến tính WK, WQ, WV, và WAO để tính toán và xuất chú ý tự nhiên giữa các wordpiece đầu vào (Wu et al., 2016). (ii) hai lớp tuyến tính WI và WO truyền tiến các biểu diễn từ đến khối transformer tiếp theo.

Cụ thể hơn, xét một câu đầu vào X ∈ RN×d trong đó N là độ dài câu tối đa và d là kích thước chiều ẩn. WK, WQ, và WV được sử dụng để tính toán các phép biến đổi của X:

K = XWK; Q = XWQ; V = XWV;

và chú ý tự nhiên của X được tính như:

Attention(K; Q; V) = softmax(QKT/√d)V.

Chú ý sau đó được biến đổi bởi WAO, và tiếp theo được truyền tiến bởi WI và WO đến khối transformer tiếp theo.

Khi tinh chỉnh trên một tác vụ hạ nguồn như phân loại chuỗi, một lớp phân loại tuyến tính WT, chiếu từ chiều ẩn đến chiều đầu ra, được khởi tạo ngẫu nhiên. Tiếp theo, WT được xếp chồng lên trên một lớp tuyến tính được huấn luyện trước WP (lớp pooler). Tất cả các tham số sau đó được cập nhật để tối thiểu hóa tổn thất tác vụ như cross-entropy.

### 3.2 Học mặt nạ

Cho một mô hình ngôn ngữ được huấn luyện trước, chúng tôi không tinh chỉnh, tức là chúng tôi không cập nhật các tham số được huấn luyện trước. Thay vào đó, chúng tôi chọn một tập con các tham số được huấn luyện trước quan trọng cho một tác vụ hạ nguồn trong khi loại bỏ những tham số không liên quan với mặt nạ nhị phân. Chúng tôi liên kết mỗi lớp tuyến tính Wl ∈ {WlK, WlQ, WlV, WlAO, WlI, WlO} của khối transformer thứ l với một ma trận giá trị thực Ml được khởi tạo ngẫu nhiên từ một phân phối đều và có cùng kích thước với Wl. Sau đó chúng tôi truyền Ml qua một hàm ngưỡng theo từng phần tử (Hubara et al., 2016; Mallya et al., 2018), tức là một bộ nhị phân hóa, để có được một mặt nạ nhị phân Mlbin cho Wl:

(mlbin)i,j = {1 nếu mli,j ≥ τ; 0 ngược lại}

trong đó mli,j ∈ Ml, i, j chỉ tọa độ của lớp tuyến tính 2-D và τ là một siêu tham số ngưỡng toàn cục.

Trong mỗi lần truyền tiến của huấn luyện, mặt nạ nhị phân Mlbin (được tạo từ Ml qua Phương trình 1) chọn trọng số trong một lớp tuyến tính được huấn luyện trước Wl bằng tích Hadamard:

Ŵl := Wl ⊙ Mlbin.

Trong lần truyền ngược tương ứng của huấn luyện, với hàm tổn thất L liên quan, chúng tôi không thể truyền ngược qua bộ nhị phân hóa, vì Phương trình 1 là một phép ngưỡng cứng và gradient theo Ml bằng không gần như ở mọi nơi. Tương tự như cách xử lý trong Bengio et al. (2013); Hubara et al. (2016); Lin et al. (2020), chúng tôi sử dụng ∂L(Ŵl)/∂Mlbin như một bộ ước lượng nhiễu của ∂L(Ŵl)/∂Ml để cập nhật Ml, tức là:

Ml ← Ml - η∂L(Ŵl)/∂Mlbin;

trong đó η đề cập đến kích thước bước. Do đó, toàn bộ cấu trúc có thể được huấn luyện đầu cuối đến đầu cuối.

Chúng tôi học một tập hợp các mặt nạ nhị phân cho một tác vụ NLP như sau. Nhớ lại rằng mỗi lớp tuyến tính Wl được liên kết với một Ml để có được một lớp tuyến tính được che mặt nạ Ŵl thông qua Phương trình 1. Chúng tôi khởi tạo ngẫu nhiên một lớp tuyến tính bổ sung với một Ml liên quan và xếp chồng nó lên trên mô hình ngôn ngữ được huấn luyện trước. Sau đó chúng tôi cập nhật mỗi Ml thông qua Phương trình 2 với mục tiêu tác vụ trong quá trình huấn luyện.

Sau khi huấn luyện, chúng tôi truyền mỗi Ml qua bộ nhị phân hóa để có được Mlbin, sau đó được lưu để suy luận trong tương lai. Vì Mlbin là nhị phân, nó chỉ chiếm 3% bộ nhớ so với việc lưu các tham số float 32-bit trong một mô hình được tinh chỉnh. Ngoài ra, chúng tôi sẽ chỉ ra rằng nhiều lớp - đặc biệt là lớp nhúng - không cần phải được che mặt nạ. Điều này càng làm giảm tiêu thụ bộ nhớ của che mặt nạ.

### 3.3 Cấu hình che mặt nạ

Sơ đồ che mặt nạ của chúng tôi được thúc đẩy bởi quan sát: các trọng số được huấn luyện trước tạo thành một khởi tạo tốt (Hao et al., 2019), nhưng vẫn cần một vài bước thích ứng để tạo ra hiệu suất cạnh tranh cho một tác vụ cụ thể. Tuy nhiên, không phải mọi tham số được huấn luyện trước đều cần thiết để đạt được hiệu suất hợp lý, như được gợi ý bởi lĩnh vực cắt tỉa mạng nơ-ron (LeCun et al., 1990; Hassibi và Stork, 1993; Han et al., 2015b). Bây giờ chúng tôi nghiên cứu hai lựa chọn cấu hình ảnh hưởng đến số lượng tham số "đủ điều kiện" để che mặt nạ.

**Độ thưa thớt ban đầu của Mlbin.** Khi chúng tôi khởi tạo ngẫu nhiên các mặt nạ từ phân phối đều, độ thưa thớt của mặt nạ nhị phân Mlbin trong giai đoạn khởi tạo mặt nạ kiểm soát có bao nhiêu tham số được huấn luyện trước trong một lớp Wl được giả định là không liên quan đến tác vụ hạ nguồn. Các tỷ lệ độ thưa thớt ban đầu khác nhau dẫn đến các hành vi tối ưu hóa khác nhau.

Việc hiểu rõ hơn về cách độ thưa thớt ban đầu của một mặt nạ ảnh hưởng đến động lực học huấn luyện và hiệu suất mô hình cuối cùng là rất quan trọng, để tổng quát hóa sơ đồ che mặt nạ của chúng tôi cho các lĩnh vực và tác vụ rộng hơn. Trong §5.1, chúng tôi nghiên cứu khía cạnh này một cách chi tiết. Trong thực tế, chúng tôi cố định τ trong Phương trình 1 trong khi điều chỉnh phân phối đều để đạt được độ thưa thớt ban đầu mục tiêu.

**Lớp nào để che mặt nạ.** Các lớp khác nhau của mô hình ngôn ngữ được huấn luyện trước nắm bắt các khía cạnh khác biệt của ngôn ngữ trong quá trình huấn luyện trước, ví dụ, Tenney et al. (2019) phát hiện rằng thông tin về gán nhãn từ loại, phân tích cú pháp, nhận dạng thực thể có tên, vai trò ngữ nghĩa, và đồng tham chiếu được mã hóa trên các lớp cao hơn dần của BERT. Khó để biết trước loại tác vụ NLP nào phải được giải quyết trong tương lai, khiến việc quyết định lớp nào để che mặt nạ trở nên không tầm thường. Chúng tôi nghiên cứu yếu tố này trong §5.2.

Chúng tôi không học mặt nạ cho lớp nhúng thấp nhất, tức là các nhúng wordpiece không theo ngữ cảnh được "chọn" hoàn toàn, cho tất cả các tác vụ. Động lực có hai mặt. (i) Trọng số lớp nhúng chiếm một phần lớn, ví dụ, gần 21% (23m/109m) trong BERT-base-uncased, tổng số tham số. Không phải học mặt nạ chọn lọc cho lớp này làm giảm tiêu thụ bộ nhớ. (ii) Huấn luyện trước đã mã hóa hiệu quả các nghĩa chung không phụ thuộc ngữ cảnh của từ trong lớp nhúng (Zhao et al., 2020). Do đó, việc học mặt nạ chọn lọc cho lớp này là không cần thiết. Ngoài ra, chúng tôi không học mặt nạ cho bias và các tham số chuẩn hóa lớp vì chúng tôi không quan sát thấy tác động tích cực đến hiệu suất.

## 4 Tập dữ liệu và Thiết lập

**Tập dữ liệu.** Chúng tôi trình bày kết quả cho việc che mặt nạ BERT, RoBERTa, và DistilBERT trong gán nhãn từ loại, nhận dạng thực thể có tên, phân loại chuỗi, và đọc hiểu.

Chúng tôi thí nghiệm với gán nhãn từ loại (POS) trên Penn Treebank (Marcus et al., 1993), sử dụng phân chia train/dev/test của Collins (2002). Cho nhận dạng thực thể có tên (NER), chúng tôi tiến hành thí nghiệm trên tác vụ chia sẻ NER CoNLL-2003 (Tjong Kim Sang và De Meulder, 2003).

Cho phân loại chuỗi, các tác vụ GLUE sau đây (Wang et al., 2018) được đánh giá: Stanford Sentiment Treebank (SST2) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan và Brockett, 2005), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), và Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016).

Thêm vào đó, chúng tôi thí nghiệm trên các tập dữ liệu phân loại chuỗi có tập kiểm tra công khai: tập dữ liệu phân loại câu hỏi 6 lớp TREC (Voorhees và Tice, 2000), tập dữ liệu phân loại tin tức 4 lớp AG News (AG) (Zhang et al., 2015), và tác vụ phân loại cảm xúc Twitter nhị phân SemEval-2016 4B (SEM) (Nakov et al., 2016).

Chúng tôi thí nghiệm với đọc hiểu trên SWAG (Zellers et al., 2018) sử dụng các phân chia dữ liệu chính thức. Chúng tôi báo cáo hệ số tương quan Matthew (MCC) cho CoLA, micro-F1 cho NER, và độ chính xác cho các tác vụ khác.

**Thiết lập.** Do hạn chế về tài nguyên và tinh thần trách nhiệm môi trường (Strubell et al., 2019; Schwartz et al., 2019), chúng tôi tiến hành thí nghiệm trên các mô hình cơ sở: BERT-base-uncased, RoBERTa-base, và DistilBERT-base-uncased. Do đó, các mô hình BERT/RoBERTa chúng tôi sử dụng có 12 khối transformer (được đánh chỉ số 0–11) tạo ra các vector 768 chiều; mô hình DistilBERT chúng tôi sử dụng có cùng chiều nhưng chứa 6 khối transformer (được đánh chỉ số 0–5). Chúng tôi triển khai các mô hình trong PyTorch (Paszke et al., 2019) với framework HuggingFace (Wolf et al., 2019).

Xuyên suốt tất cả thí nghiệm, chúng tôi giới hạn độ dài tối đa của một câu (cặp) là 128 sau khi token hóa wordpiece. Theo Devlin et al. (2019), chúng tôi sử dụng bộ tối ưu hóa Adam (Kingma và Ba, 2014) trong đó tốc độ học là một siêu tham số trong khi các tham số khác vẫn mặc định. Chúng tôi cẩn thận điều chỉnh tốc độ học cho mỗi thiết lập: quy trình điều chỉnh đảm bảo rằng tốc độ học tốt nhất không nằm ở biên của lưới tìm kiếm, nếu không chúng tôi mở rộng lưới tương ứng. Lưới ban đầu là {1e-5, 3e-5, 5e-5, 7e-5, 9e-5}.

Cho phân loại chuỗi và đọc hiểu, chúng tôi sử dụng [CLS] làm biểu diễn của câu (cặp). Theo Devlin et al. (2019), chúng tôi hình thức hóa NER như một tác vụ gán nhãn và sử dụng một lớp đầu ra tuyến tính, thay vì một lớp trường ngẫu nhiên có điều kiện. Cho thí nghiệm POS và NER, biểu diễn của một từ được token hóa là wordpiece cuối cùng của nó (Liu et al., 2019a; He và Choi, 2020). Lưu ý rằng độ dài tối đa 128 của một câu cho POS và NER có nghĩa là một số chú thích từ-nhãn cần được loại trừ. Phụ lục §A chỉ ra danh sách kiểm tra tái tạo của chúng tôi chứa thêm chi tiết triển khai và tiền xử lý.

## 5 Thí nghiệm

### 5.1 Độ thưa thớt ban đầu của mặt nạ nhị phân

Đầu tiên chúng tôi nghiên cứu cách tỷ lệ độ thưa thớt ban đầu (tức là phần trăm số không) của mặt nạ nhị phân Mlbin ảnh hưởng đến hiệu suất của một mô hình ngôn ngữ che mặt nạ nhị phân trên các tác vụ hạ nguồn. Chúng tôi thí nghiệm trên bốn tác vụ, với độ thưa thớt ban đầu trong {1%, 3%, 5%, 10%, 15%, 20%, ..., 95%}. Tất cả các siêu tham số khác được kiểm soát: tốc độ học được cố định ở 5e-5; kích thước batch là 32 cho các tập dữ liệu tương đối nhỏ (RTE, MRPC, và CoLA) và 128 cho SST2. Mỗi thí nghiệm được lặp lại bốn lần với các hạt giống ngẫu nhiên khác nhau {1, 2, 3, 4}. Trong thí nghiệm này, tất cả các khối transformer, lớp pooler, và lớp phân loại đều được che mặt nạ.

Hình 1 cho thấy che mặt nạ đạt được hiệu suất khá tốt mà không cần tìm kiếm siêu tham số. Cụ thể, (i) một độ thưa thớt ban đầu lớn loại bỏ hầu hết các tham số được huấn luyện trước, ví dụ 95%, dẫn đến hiệu suất kém cho bốn tác vụ. Điều này do thực tế là kiến thức được huấn luyện trước phần lớn bị loại bỏ. (ii) Giảm dần độ thưa thớt ban đầu cải thiện hiệu suất tác vụ. Nói chung, một độ thưa thớt ban đầu trong khoảng 3%–10% mang lại kết quả hợp lý trên các tác vụ. Các tập dữ liệu lớn như SST2 ít nhạy cảm hơn các tập dữ liệu nhỏ như RTE. (iii) Chọn gần như tất cả các tham số được huấn luyện trước, ví dụ độ thưa thớt 1%, làm tổn hại hiệu suất tác vụ. Nhớ lại rằng một mô hình được huấn luyện trước cần được thích ứng với một tác vụ hạ nguồn; che mặt nạ đạt được sự thích ứng bằng cách học các mặt nạ chọn lọc - bảo tồn quá nhiều tham số được huấn luyện trước trong khởi tạo cản trở việc tối ưu hóa.

### 5.2 Hành vi theo lớp

Các lớp mạng nơ-ron thể hiện đặc tính không đồng nhất (Zhang et al., 2019) khi được áp dụng cho các tác vụ. Ví dụ, thông tin cú pháp được biểu diễn tốt hơn ở các lớp thấp hơn trong khi thông tin ngữ nghĩa được nắm bắt ở các lớp cao hơn trong ELMo (Peters et al., 2018). Kết quả là, việc đơn giản che mặt nạ tất cả các khối transformer (như trong §5.1) có thể không lý tưởng.

Chúng tôi nghiên cứu hiệu suất tác vụ khi áp dụng mặt nạ cho các lớp BERT khác nhau. Hình 2 trình bày hiệu suất tác vụ tối ưu khi che mặt nạ chỉ một tập con các khối transformer của BERT trên MRPC, CoLA, và RTE. Các số lượng và chỉ số khác nhau của các khối transformer được che mặt nạ: "bottom-up" và "top-down" chỉ việc che mặt nạ số lượng khối transformer mục tiêu, từ đáy hoặc đỉnh của BERT.

Chúng tôi có thể quan sát rằng (i) trong hầu hết các trường hợp, che mặt nạ top-down vượt trội hơn che mặt nạ bottom-up khi độ thưa thớt ban đầu và số lượng lớp được che mặt nạ được cố định. Do đó, việc chọn tất cả các trọng số được huấn luyện trước trong các lớp thấp hơn là hợp lý, vì chúng nắm bắt thông tin chung hữu ích và có thể chuyển giao cho các tác vụ khác nhau (Liu et al., 2019a; Howard và Ruder, 2018). (ii) Cho che mặt nạ bottom-up, tăng số lượng lớp được che mặt nạ dần cải thiện hiệu suất. Quan sát này minh họa sự phụ thuộc giữa các lớp BERT và động lực học của che mặt nạ: được cung cấp với các trọng số được huấn luyện trước được chọn trong các lớp thấp hơn, các lớp cao hơn cần được trao sự linh hoạt để chọn các trọng số được huấn luyện trước tương ứng để đạt được hiệu suất tác vụ tốt. (iii) Trong che mặt nạ top-down, hiệu suất CoLA tăng khi che mặt nạ một số lượng lớp ngày càng tăng trong khi MRPC và RTE không nhạy cảm. Nhớ lại rằng CoLA kiểm tra tính chấp nhận được về mặt ngôn ngữ thường yêu cầu cả thông tin cú pháp và ngữ nghĩa. Tất cả các lớp BERT đều tham gia vào việc biểu diễn thông tin này, do đó cho phép nhiều lớp thay đổi sẽ cải thiện hiệu suất.

### 5.3 So sánh tinh chỉnh và che mặt nạ

Chúng tôi đã nghiên cứu hai yếu tố - độ thưa thớt ban đầu (§5.1) và hành vi theo lớp (§5.2) - quan trọng trong việc che mặt nạ các mô hình ngôn ngữ được huấn luyện trước. Ở đây, chúng tôi so sánh hiệu suất và tiêu thụ bộ nhớ của che mặt nạ và tinh chỉnh.

Dựa trên các quan sát trong §5.1 và §5.2, chúng tôi sử dụng độ thưa thớt ban đầu 5% khi áp dụng che mặt nạ cho BERT, RoBERTa, và DistilBERT. Chúng tôi che mặt nạ các khối transformer 2–11 trong BERT/RoBERTa và 2–5 trong DistilBERT. WP và WT luôn được che mặt nạ. Lưu ý rằng thiết lập toàn cục này chắc chắn là không tối ưu cho một số kết hợp mô hình-tác vụ, nhưng mục tiêu của chúng tôi là minh họa tính hiệu quả và khả năng tổng quát của che mặt nạ. Do đó, việc tiến hành tìm kiếm siêu tham số mở rộng là không cần thiết.

Cho AG và QNLI, chúng tôi sử dụng kích thước batch 128. Cho các tác vụ khác chúng tôi sử dụng kích thước batch 32. Chúng tôi tìm kiếm tốc độ học tối ưu cho mỗi tác vụ như mô tả trong §4, và chúng được thể hiện trong Phụ lục §A.4.

**So sánh hiệu suất.** Bảng 1 báo cáo hiệu suất của che mặt nạ và tinh chỉnh trên tập dev cho mười một tác vụ NLP. Chúng tôi quan sát rằng việc áp dụng che mặt nạ cho BERT/RoBERTa/DistilBERT mang lại hiệu suất tương đương với tinh chỉnh. Chúng tôi quan sát sự giảm hiệu suất trên RoBERTa-RTE. RTE có kích thước tập dữ liệu nhỏ nhất (train: 2.5k; dev: 0.3k) trong tất cả các tác vụ - điều này có thể góp phần vào kết quả không hoàn hảo và phương sai lớn.

Kết quả BERT-NER của chúng tôi hơi kém hơn Devlin et al. (2019). Điều này có thể do thực tế là "ngữ cảnh tài liệu tối đa" được sử dụng bởi Devlin et al. (2019) trong khi chúng tôi sử dụng ngữ cảnh cấp câu với độ dài chuỗi tối đa 128.

Các hàng "Single" trong Bảng 2 so sánh hiệu suất của che mặt nạ và tinh chỉnh BERT trên tập kiểm tra của SEM, TREC, AG, POS, và NER. Cùng một thiết lập và tìm kiếm siêu tham số như Bảng 1 được sử dụng, các siêu tham số tốt nhất được chọn trên tập dev. Kết quả từ Sun et al. (2019); Palogiannidi et al. (2016) được bao gồm làm tham chiếu. Sun et al. (2019) sử dụng các tối ưu hóa như tốc độ học theo lớp, tạo ra hiệu suất hơi tốt hơn của chúng tôi. Palogiannidi et al. (2016) là hệ thống có hiệu suất tốt nhất trên tác vụ SEM (Nakov et al., 2016). Một lần nữa, che mặt nạ mang lại kết quả tương đương với tinh chỉnh.

**So sánh bộ nhớ.** Sau khi chỉ ra rằng hiệu suất tác vụ của che mặt nạ và tinh chỉnh là tương đương, chúng tôi tiếp theo thể hiện một điểm mạnh chính của che mặt nạ: hiệu quả bộ nhớ. Chúng tôi lấy BERT-base-uncased làm ví dụ. Hình 3 cho thấy số lượng tham số tích lũy tính bằng triệu và bộ nhớ tính bằng megabyte (MB) cần thiết khi một số lượng tác vụ hạ nguồn ngày càng tăng cần được giải quyết bằng tinh chỉnh và che mặt nạ. Che mặt nạ yêu cầu một chi phí nhỏ khi giải quyết một tác vụ duy nhất nhưng hiệu quả hơn nhiều so với tinh chỉnh khi cần suy luận cho nhiều tác vụ. Che mặt nạ lưu một bản sao duy nhất của một mô hình ngôn ngữ được huấn luyện trước chứa các tham số float 32-bit cho tất cả mười một tác vụ và một tập hợp các mặt nạ nhị phân 1-bit cho mỗi tác vụ. Ngược lại, tinh chỉnh lưu mọi mô hình được tinh chỉnh nên việc tiêu thụ bộ nhớ tăng tuyến tính.

Che mặt nạ tự nhiên cho phép các tổ hợp nhẹ của các mô hình. Các hàng "Ensem." trong Bảng 2 so sánh kết quả tổ hợp và kích thước mô hình. Chúng tôi xét tổ hợp của (i) nhãn được dự đoán; (ii) logit; (iii) xác suất. Phương pháp tổ hợp tốt nhất được chọn trên dev và sau đó đánh giá trên test. Che mặt nạ chỉ tiêu thụ 474MB bộ nhớ - nhỏ hơn nhiều so với 1752MB cần thiết cho tinh chỉnh - và đạt được hiệu suất tương đương. Do đó, che mặt nạ cũng hiệu quả hơn nhiều về bộ nhớ so với tinh chỉnh trong một thiết lập tổ hợp.

## 6 Thảo luận

### 6.1 Đánh giá nội tại

§5 chứng minh rằng che mặt nạ là một thay thế hiệu quả cho tinh chỉnh. Bây giờ chúng tôi phân tích các thuộc tính của biểu diễn được tính toán bởi các mô hình ngôn ngữ che mặt nạ nhị phân với đánh giá nội tại.

Một thuộc tính hấp dẫn của tinh chỉnh, tức là xếp chồng một lớp phân loại lên trên một mô hình ngôn ngữ được huấn luyện trước rồi cập nhật tất cả các tham số, là một lớp phân loại tuyến tính đủ để tiến hành phân loại khá chính xác. Quan sát này ngụ ý rằng cấu hình của các điểm dữ liệu, ví dụ các câu có cảm xúc tích cực hoặc tiêu cực trong SST2, nên gần với khả năng phân tách tuyến tính trong không gian ẩn.

Như tinh chỉnh, che mặt nạ cũng sử dụng một lớp phân loại tuyến tính. Do đó, chúng tôi giả thuyết rằng các lớp trên trong các mô hình ngôn ngữ che mặt nạ nhị phân, ngay cả khi không cập nhật trọng số rõ ràng, cũng tạo ra một không gian ẩn trong đó các điểm dữ liệu gần với khả năng phân tách tuyến tính.

Hình 4 sử dụng t-SNE (Maaten và Hinton, 2008) để trực quan hóa biểu diễn của [CLS] được tính toán bởi khối transformer trên cùng trong BERT/RoBERTa được huấn luyện trước, được tinh chỉnh, và được che mặt nạ, sử dụng các ví dụ tập dev của SST2. Biểu diễn của các mô hình được huấn luyện trước (trái) rõ ràng không thể phân tách được vì mô hình cần được thích ứng với các tác vụ hạ nguồn. Biểu diễn câu được tính toán bởi bộ mã hóa được tinh chỉnh (trên phải) và che mặt nạ nhị phân (dưới phải) gần như có thể phân tách tuyến tính và phù hợp với nhãn vàng. Do đó, một bộ phân loại tuyến tính được kỳ vọng mang lại độ chính xác phân loại khá tốt. Đánh giá nội tại này minh họa rằng các mô hình che mặt nạ nhị phân trích xuất biểu diễn tốt từ dữ liệu cho tác vụ NLP hạ nguồn.

### 6.2 Thuộc tính của các mô hình che mặt nạ nhị phân

**Các mô hình che mặt nạ nhị phân có tổng quát hóa không?** Hình 4 cho thấy rằng một mô hình ngôn ngữ che mặt nạ nhị phân tạo ra biểu diễn thích hợp cho lớp phân loại và do đó hoạt động tốt như một mô hình được tinh chỉnh. Ở đây, chúng tôi quan tâm đến việc xác minh rằng mô hình che mặt nạ nhị phân thực sự giải quyết các tác vụ hạ nguồn bằng cách học biểu diễn có ý nghĩa - thay vì khai thác các tương quan giả tạo có khả năng tổng quát hóa kém (Niven và Kao, 2019; McCoy et al., 2019). Để làm điều này, chúng tôi kiểm tra xem mô hình che mặt nạ nhị phân có thể tổng quát hóa cho các tập dữ liệu khác của cùng loại tác vụ hạ nguồn hay không. Chúng tôi sử dụng hai tập dữ liệu phân loại cảm xúc: SST2 và SEM. Chúng tôi đơn giản đánh giá mô hình được che mặt nạ hoặc tinh chỉnh trên SST2 so với tập dev của SEM và ngược lại.

Bảng 3 báo cáo kết quả so với đường cơ sở majority-vote. Các mô hình tinh chỉnh và che mặt nạ nhị phân của SEM tổng quát hóa tốt trên SST2, cho thấy cải thiện 20% so với đường cơ sở majority-vote. Mặt khác, chúng tôi quan sát rằng kiến thức học được trên SST2 không tổng quát hóa cho SEM, cho cả tinh chỉnh và che mặt nạ. Chúng tôi giả thuyết rằng điều này là do lĩnh vực Twitter (SEM) cụ thể hơn nhiều so với đánh giá phim (SST2). Ví dụ, một số Emoji hoặc ký hiệu như ":)" phản ánh cảm xúc mạnh không xuất hiện trong SST2, dẫn đến việc tổng quát hóa không thành công. Để kiểm tra giả thuyết của chúng tôi, chúng tôi lấy một tập dữ liệu đánh giá phim khác IMDB (Maas et al., 2011), và trực tiếp áp dụng các mô hình được tinh chỉnh-SST2 và che mặt nạ nhị phân-SST2 lên nó. Che mặt nạ và tinh chỉnh đạt được độ chính xác 84.79% và 85.25%, tương đương và cả hai đều vượt trội hơn đường cơ sở 50%, thể hiện việc chuyển giao kiến thức thành công.

Do đó, tinh chỉnh và che mặt nạ mang lại các mô hình với khả năng tổng quát hóa tương tự. Các mô hình che mặt nạ nhị phân thực sự tạo ra biểu diễn chứa thông tin hợp lệ cho các tác vụ hạ nguồn.

**Phân tích mặt nạ.** Chúng tôi nghiên cứu sự khác biệt giữa các mặt nạ được học bởi các lớp BERT khác nhau và các tác vụ hạ nguồn. Cho các mặt nạ nhị phân ban đầu và được huấn luyện Mt,init bin và Mt,trained bin của một lớp được huấn luyện trên tác vụ t ∈ {t1, t2}. Chúng tôi tính toán:

s = (||Mt1,trained bin - Mt2,trained bin||1) / (||Mt1,trained bin - Mt1,init bin||1 + ||Mt2,trained bin - Mt2,init bin||1)

trong đó ||W||1 = Σmi=1 Σnj=1 |wi,j|. Lưu ý rằng cho cùng một hạt giống ngẫu nhiên, Mt1,init bin và Mt2,init bin là giống nhau. Sự khác biệt s đo lường sự khác biệt giữa hai mặt nạ như một phần của tất cả các thay đổi được tạo ra bởi huấn luyện. Hình 5 cho thấy rằng, sau khi huấn luyện, sự khác biệt của các mặt nạ của các lớp BERT cao hơn lớn hơn so với các lớp BERT thấp hơn. Các quan sát tương tự được thực hiện cho tinh chỉnh: trọng số lớp trên trong BERT được tinh chỉnh cụ thể hơn cho tác vụ (Kovaleva et al., 2019). Hình cũng cho thấy rằng các mặt nạ được học cho các tác vụ hạ nguồn có xu hướng khác biệt với nhau, ngay cả cho các tác vụ tương tự. Cho một tác vụ nhất định, tồn tại các tập hợp mặt nạ khác nhau (được khởi tạo với các hạt giống ngẫu nhiên khác nhau) mang lại hiệu suất tương tự. Quan sát này tương tự với kết quả đánh giá giả thuyết vé số trên BERT (Prasanna et al., 2020; Chen et al., 2020): một số mạng con tồn tại trong BERT đạt được hiệu suất tác vụ tương tự.

### 6.3 Cảnh quan tổn thất

Huấn luyện mạng nơ-ron phức tạp có thể được xem như tìm kiếm các cực tiểu tốt trong cảnh quan rất không lồi được định nghĩa bởi hàm tổn thất (Li et al., 2018). Các cực tiểu tốt thường được mô tả như các điểm ở đáy của các thung lũng lồi địa phương khác nhau (Keskar et al., 2016; Draxler et al., 2018), đạt được hiệu suất tương tự. Trong phần này, chúng tôi nghiên cứu mối quan hệ giữa hai cực tiểu được thu được bởi che mặt nạ và tinh chỉnh.

Nghiên cứu gần đây phân tích cảnh quan tổn thất cho thấy rằng các cực tiểu địa phương trong cảnh quan tổn thất đạt được bởi các thuật toán huấn luyện tiêu chuẩn có thể được kết nối bằng một đường dẫn đơn giản (Garipov et al., 2018; Gotmare et al., 2018), ví dụ một đường cong Bézier, với tổn thất tác vụ thấp (hoặc độ chính xác tác vụ cao) dọc theo đường dẫn. Chúng tôi quan tâm đến việc kiểm tra xem hai cực tiểu tìm thấy bởi tinh chỉnh và che mặt nạ có thể được kết nối dễ dàng trong cảnh quan tổn thất hay không. Để bắt đầu, chúng tôi xác minh hiệu suất tác vụ của một mô hình nội suy W(α) trên đoạn thẳng giữa một mô hình được tinh chỉnh W0 và một mô hình che mặt nạ nhị phân W1:

W(α) = W0 + α(W1 - W0); 0 ≤ α ≤ 1.

Chúng tôi tiến hành thí nghiệm trên MRPC và SST2 với các mô hình BERT và RoBERTa có hiệu suất tốt nhất thu được trong Bảng 1 (cùng hạt giống và epoch huấn luyện); Hình 6 (trên) cho thấy kết quả của kết nối mode, tức là sự phát triển của độ chính xác tác vụ dọc theo một đường thẳng nối hai cực tiểu ứng viên.

Đáng ngạc nhiên, các mô hình nội suy trên đoạn thẳng nối một mô hình được tinh chỉnh và một mô hình che mặt nạ nhị phân tạo thành một đường dẫn độ chính xác cao, chỉ ra cảnh quan tổn thất được kết nối cực kỳ tốt. Do đó, che mặt nạ tìm thấy các cực tiểu trên cùng một đa tạp tổn thất thấp được kết nối như tinh chỉnh, xác nhận tính hiệu quả của phương pháp chúng tôi. Ngoài ra, chúng tôi cho thấy trong Hình 6 (dưới) cho đoạn thẳng giữa BERT được huấn luyện trước và BERT được tinh chỉnh/che mặt nạ, rằng kết nối mode không chỉ do một mô hình ngôn ngữ được huấn luyện trước quá tham số hóa. Thí nghiệm đường cong Bézier cho thấy kết quả tương tự, xem Phụ lục §B.

## 7 Kết luận

Chúng tôi đã trình bày che mặt nạ, một thay thế hiệu quả cho tinh chỉnh để sử dụng các mô hình ngôn ngữ được huấn luyện trước như BERT/RoBERTa/DistilBERT. Thay vì cập nhật các tham số được huấn luyện trước, chúng tôi chỉ huấn luyện một tập hợp các mặt nạ nhị phân cho mỗi tác vụ để chọn các tham số quan trọng. Các thí nghiệm mở rộng cho thấy che mặt nạ mang lại hiệu suất tương đương với tinh chỉnh trên một loạt các tác vụ NLP. Giữ nguyên các tham số được huấn luyện trước, che mặt nạ hiệu quả hơn nhiều về bộ nhớ khi cần giải quyết nhiều tác vụ. Các đánh giá nội tại cho thấy các mô hình che mặt nạ nhị phân trích xuất biểu diễn hợp lệ và có thể tổng quát hóa cho các tác vụ hạ nguồn. Hơn nữa, chúng tôi chứng minh rằng các cực tiểu thu được bởi tinh chỉnh và che mặt nạ có thể được kết nối dễ dàng bằng một đoạn thẳng, xác nhận tính hiệu quả của việc áp dụng che mặt nạ cho các mô hình ngôn ngữ được huấn luyện trước.

Mã nguồn của chúng tôi có sẵn tại: https://github.com/ptlmasking/maskbert.

Nghiên cứu tương lai có thể khám phá khả năng áp dụng che mặt nạ cho các bộ mã hóa đa ngôn ngữ được huấn luyện trước như mBERT (Devlin et al., 2019) và XLM (Conneau và Lample, 2019). Ngoài ra, các mặt nạ nhị phân được học bởi phương pháp chúng tôi có độ thưa thớt thấp sao cho tốc độ suy luận không được cải thiện. Phát triển các phương pháp cải thiện cả hiệu quả bộ nhớ và suy luận mà không hy sinh hiệu suất tác vụ có thể mở ra khả năng triển khai rộng rãi các mô hình ngôn ngữ được huấn luyện trước mạnh mẽ cho nhiều ứng dụng NLP hơn.
