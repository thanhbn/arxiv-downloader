# 2308.10462.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.10462.pdf
# Kích thước tệp: 1063509 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho
Sinh mã với Mô hình Ngôn ngữ Lớn
MARTIN WEYSSOW∗,DIRO, Đại học Montreal, Canada
XIN ZHOU, Đại học Quản lý Singapore, Singapore
KISUB KIM, Đại học Quản lý Singapore, Singapore
DAVID LO, Đại học Quản lý Singapore, Singapore
HOUARI SAHRAOUI, DIRO, Đại học Montreal, Canada
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng ấn tượng trong việc sinh ra các đoạn mã chính xác khi được đưa ra ý định bằng ngôn ngữ tự nhiên theo cách zero-shot, tức là không cần tinh chỉnh cụ thể. Trong khi các nghiên cứu trước đã nhấn mạnh những ưu điểm của việc tinh chỉnh LLM, quá trình này phát sinh chi phí tính toán cao, làm cho nó không thực tế trong môi trường khan hiếm tài nguyên, đặc biệt đối với các mô hình có hàng tỷ tham số. Để giải quyết những thách thức này, nghiên cứu trước đã khám phá học trong ngữ cảnh (ICL) và sinh tăng cường truy xuất (RAG) như các chiến lược để hướng dẫn quá trình sinh của LLM với các ví dụ prompt cụ thể cho tác vụ. Tuy nhiên, ICL và RAG gây ra những bất tiện, chẳng hạn như nhu cầu thiết kế các prompt có liên quan theo ngữ cảnh và việc thiếu học các tham số cụ thể cho tác vụ, do đó hạn chế hiệu suất tác vụ hạ nguồn. Trong bối cảnh này, chúng tôi dự kiến tinh chỉnh hiệu quả tham số (PEFT) như một cách tiếp cận đầy hứa hẹn để chuyên môn hóa LLM hiệu quả cho dữ liệu cụ thể tác vụ trong khi duy trì mức tiêu thụ tài nguyên hợp lý. Trong bài báo này, chúng tôi thực hiện một nghiên cứu toàn diện về các kỹ thuật PEFT cho LLM trong bối cảnh sinh mã tự động. Cuộc điều tra toàn diện của chúng tôi về các kỹ thuật PEFT cho LLM tiết lộ tính vượt trội và tiềm năng của chúng so với ICL và RAG trên một tập hợp đa dạng các LLM và ba bộ dữ liệu sinh mã Python đại diện: Conala, CodeAlpacaPy, và APPS. Hơn nữa, nghiên cứu của chúng tôi làm nổi bật tiềm năng cho việc tinh chỉnh các LLM lớn hơn và giảm đáng kể việc sử dụng bộ nhớ bằng cách kết hợp PEFT với lượng tử hóa. Do đó, nghiên cứu này mở ra cơ hội cho các ứng dụng rộng hơn của PEFT trong các tình huống kỹ thuật phần mềm.
Khái niệm CCS: •Phần mềm và kỹ thuật của nó →Tạo và quản lý phần mềm; Kỹ thuật phát triển phần mềm.
Từ khóa và Cụm từ bổ sung: sinh mã, mô hình ngôn ngữ lớn, tinh chỉnh hiệu quả tham số, lượng tử hóa, nghiên cứu thực nghiệm
∗Tác giả liên hệ.
Địa chỉ tác giả: Martin Weyssow, martin.weyssow@umontreal.ca, DIRO, Đại học Montreal, Canada; Xin Zhou, xinzhou.2020@phdcs.smu.edu.sg, Đại học Quản lý Singapore, Singapore; Kisub Kim, falconlk00@gmail.com, Đại học Quản lý Singapore, Singapore; David Lo, davidlo@smu.edu.sg, Đại học Quản lý Singapore, Singapore; Houari Sahraoui, sahraouh@iro.umontreal.ca, DIRO, Đại học Montreal, Canada.
Quyền tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này để sử dụng cá nhân hoặc trong lớp học được cấp miễn phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi ích thương mại và các bản sao mang thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của người khác ngoài (các) tác giả phải được tôn trọng. Việc tóm tắt có ghi nguồn được phép. Để sao chép khác, hoặc tái xuất bản, đăng trên máy chủ hoặc phân phối lại cho danh sách, yêu cầu quyền cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
©2024 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM XXXX-XXXX/2024/12-ART
https://doi.org/10.1145/nnnnnnn.nnnnnnn
, Tập 1, Số 1, Bài viết . Ngày xuất bản: Tháng 12 2024.arXiv:2308.10462v3 [cs.SE] 27 Tháng 12 2024

--- TRANG 2 ---
2•M. Weyssow và cộng sự.
Định dạng Tham khảo ACM:
Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, và Houari Sahraoui. 2024. Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn. 1, 1 (Tháng 12 2024), 27 trang. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên kiến trúc Transformer [67], thể hiện tiềm năng đáng kể trong các lĩnh vực đa dạng, bao gồm xử lý ngôn ngữ tự nhiên (NLP) [29,44,76], thị giác máy tính [7,59,85], và kỹ thuật phần mềm [9,66,82]. Những mô hình này xuất sắc trong việc sinh nội dung chất lượng cao khi được đưa ra ý định bằng ngôn ngữ tự nhiên theo cách zero-shot, tức là không cần tinh chỉnh. Khả năng này đã khơi dậy sự quan tâm đáng kể trong lĩnh vực kỹ thuật phần mềm để tự động hóa các tác vụ liên quan đến mã như sửa chữa chương trình [27, 80, 81] và sinh mã [3, 9, 48].

Trong khi khả năng zero-shot của LLM rất ấn tượng, tiềm năng đầy đủ của chúng thường xuất hiện thông qua tinh chỉnh [54,75]. Cụ thể, tinh chỉnh một LLM với dữ liệu cụ thể tác vụ cho phép nó học và mã hóa kiến thức của dữ liệu có thể có ngữ cảnh cao tại tay và do đó sinh nội dung có ý nghĩa hơn. Tuy nhiên, quá trình này đi kèm với chi phí tính toán đáng kể. Tinh chỉnh đầy đủ, nơi tất cả các tham số của LLM được cập nhật trong quá trình huấn luyện, đòi hỏi tài nguyên tính toán đáng kể, đặc biệt khi LLM chứa hàng tỷ tham số [62]. Để giảm thiểu gánh nặng tính toán này, các nghiên cứu trước trong kỹ thuật phần mềm [53,80,91] đã điều tra các kỹ thuật thiết kế prompt như Học trong Ngữ cảnh (ICL) [5,54] và Sinh Tăng cường Truy xuất (RAG) [31]. ICL bao gồm việc cung cấp các ví dụ prompt của tác vụ cho LLM, hướng dẫn nó sinh nội dung phù hợp theo ngữ cảnh mà không có bất kỳ tinh chỉnh nào liên quan. Những ví dụ này có thể được tạo thủ công hoặc chọn ngẫu nhiên từ một bộ dữ liệu huấn luyện có liên quan. Kỹ thuật này đã cho thấy kết quả đầy hứa hẹn cho các tác vụ liên quan đến mã, bao gồm sửa chữa chương trình tự động [80], sửa lỗi [53], và sinh mã [61,73,91].

Mở rộng ICL, RAG cung cấp một lựa chọn thay thế mạnh mẽ và mạnh hơn kết hợp hệ thống truy xuất kiến thức tại thời điểm suy luận. Sử dụng RAG, một mô hình truy xuất lấy thông tin có liên quan từ một kho tài liệu được lập chỉ mục, chẳng hạn như tài liệu mã hoặc các đoạn mã tương tự với vấn đề đầu vào. Thông tin được truy xuất sau đó được thêm vào prompt đầu vào để hướng dẫn sinh. Không giống như ICL, dựa vào các ví dụ được chọn trước có thể không phải lúc nào cũng được điều chỉnh cho đầu vào cụ thể, RAG thích ứng động với từng vấn đề đầu vào riêng lẻ, cung cấp ngữ cảnh có liên quan hơn. Kỹ thuật này đã chứng minh những cải thiện đáng kể trong các tác vụ kỹ thuật phần mềm như sinh mã và tóm tắt [39,52,91], hoàn thiện mã [41], và sửa chữa chương trình [70].

Mặc dù ICL và RAG cung cấp một lựa chọn thay thế khả thi cho tinh chỉnh đầy đủ, nó hoạt động tại thời điểm suy luận và không liên quan đến việc học các tham số cụ thể tác vụ, điều này có thể ngăn LLM nắm bắt thông tin chi tiết về tác vụ và dẫn đến mất hiệu quả. Trong bối cảnh này, các kỹ thuật Tinh chỉnh Hiệu quả Tham số (PEFT) đã nổi lên như những giải pháp đầy hứa hẹn để giảm chi phí tinh chỉnh xuống mức thấp nhất trong khi cho phép mô hình học các tham số cụ thể tác vụ. Các công trình trước [10,57,68,69] trong trí tuệ mã đã chứng minh khả năng của các kỹ thuật PEFT, và thường cho thấy tính vượt trội của chúng so với tinh chỉnh đầy đủ trên một loạt rộng các tác vụ. Tuy nhiên, những nghiên cứu này tập trung vào các mô hình ngôn ngữ nhỏ (SLM) (<0.25B tham số) như CodeBERT [16] và CodeT5 [72] và bỏ qua khả năng áp dụng của các kỹ thuật PEFT cho LLM (≥1B tham số), để lại một khoảng trống nghiên cứu quan trọng. Với sự phổ biến ngày càng tăng của LLM, chúng tôi tin rằng việc giải quyết khoảng trống này là tối quan trọng trong việc thúc đẩy lĩnh vực trí tuệ mã và khai thác toàn bộ tiềm năng của LLM. Hơn nữa, chúng tôi xác định một cơ hội nghiên cứu bổ sung trong việc khám phá việc sử dụng các kỹ thuật PEFT dưới các tình huống tài nguyên hạn chế, nhằm chứng minh việc dân chủ hóa tinh chỉnh LLM thông qua PEFT. Việc giải quyết những khoảng trống này sẽ không chỉ cho thấy cách các kỹ thuật PEFT có thể tăng cường hiệu quả của LLM mà còn cách chúng mở rộng khả năng tiếp cận và tiện ích của LLM trong các cài đặt tính toán khan hiếm và giảm bớt sự phụ thuộc của các nhà thực hành vào cơ sở hạ tầng tính toán lớn.

Trong bài báo này, chúng tôi trình bày một nghiên cứu thực nghiệm về việc sử dụng các kỹ thuật PEFT hiện có với LLM. Chúng tôi tập trung nghiên cứu vào sinh mã, đã là một lĩnh vực nghiên cứu then chốt do tác động biến đổi của nó đối với việc tự động hóa phát triển phần mềm [9,48,50]. Mục tiêu của chúng tôi có hai mặt. Thứ nhất, chúng tôi nhằm đánh giá khả năng sinh mã của LLM sử dụng các kỹ thuật PEFT hiện có như LoRA [24] và QLoRA [13] trên các bộ dữ liệu không có test case, bao gồm Conala [91] và CodeAlpacaPy [8], cũng như bộ dữ liệu APPS [22] có test case. Thứ hai, chúng tôi tìm cách so sánh hiệu quả của LLM được tinh chỉnh với các kỹ thuật PEFT này với SLM, ICL, và RAG. Ngoài ra, chúng tôi tiến hành nghiên cứu so sánh với tính khả dụng hạn chế của tài nguyên tính toán để điều tra tính thực tiễn rộng của việc sử dụng các kỹ thuật PEFT cho LLM. Để đạt được những mục tiêu này, chúng tôi xây dựng bốn câu hỏi nghiên cứu hướng dẫn nghiên cứu của chúng tôi:

– RQ1: LLM và SLM hoạt động như thế nào khi sử dụng ICL trên các bộ dữ liệu Conala và CodeAlpacaPy?
–RQ2: LLM và SLM hoạt động như thế nào khi sử dụng các kỹ thuật PEFT trên các bộ dữ liệu Conala và CodeAlpacaPy?
– RQ3: LoRA so sánh như thế nào với ICL và RAG trên các bộ dữ liệu Conala và CodeAlpacaPy?
–RQ4: Chúng ta có thể tăng cường hiệu quả của LLM cho sinh mã trong bộ dữ liệu APPS bằng LoRA và QLoRA không?

Tổng thể, việc trả lời bốn câu hỏi nghiên cứu này hoàn thành cả hai mục tiêu của nghiên cứu thực nghiệm này. Ba RQ đầu tiên của chúng tôi tập trung vào đánh giá SLM và LLM cho sinh mã trên các bộ dữ liệu Conala và CodeAlpaca. Trong RQ1, chúng tôi minh họa hiệu quả cơ bản của SLM và LLM sử dụng ICL, truy xuất các ví dụ ngẫu nhiên từ tập huấn luyện để hướng dẫn mô hình sinh mã. Bằng cách giải quyết RQ2, chúng tôi có được hiểu biết toàn diện về mức độ hiệu quả của SLM và LLM khi sử dụng các kỹ thuật PEFT khác nhau. Trong RQ3, chúng tôi tiến hành một nghiên cứu so sánh về hiệu quả của LoRA với ICL và RAG, một đường cơ sở mạnh truy xuất động các ví dụ có liên quan bằng cách chọn những ví dụ gần nhất với hướng dẫn kiểm tra từ tập huấn luyện. Cuối cùng, để thể hiện tác động rộng hơn tiềm năng của PEFT, chúng tôi nghiên cứu trong RQ4 liệu việc tinh chỉnh LLM sử dụng LoRA và QLoRA có thể cải thiện hiệu quả của chúng trên APPS, một điểm chuẩn thách thức với test case.

Để giải quyết những RQ này, chúng tôi tiến hành thí nghiệm trên ba bộ dữ liệu, APPS [22], Conala [86], và CodeAlpacaPy được tuyển chọn cụ thể từ CodeAlpaca [8] cho sinh mã Python. Ngược lại với các bộ dữ liệu đánh giá như HumanEval [9], các bộ dữ liệu APPS, Conala và CodeAlpaca, được sử dụng rộng rãi trong các nghiên cứu sinh mã trước [49,71,73,73,88,91], bao gồm đủ ví dụ huấn luyện có thể được sử dụng cho tinh chỉnh. Để phân tích so sánh toàn diện, chúng tôi chọn bốn họ mô hình khác biệt: CodeT5+ [71], CodeGen [48], CodeGen2 [47], và CodeLlama [56], bao gồm tám biến thể lớn và ba biến thể nhỏ. Lưu ý rằng chúng tôi bỏ qua các LLM nguồn đóng như Codex do không thể truy cập các tham số của chúng, điều này làm cho việc nghiên cứu bất kỳ kỹ thuật tinh chỉnh nào trở nên không khả thi. Hơn nữa, nghiên cứu của chúng tôi kết hợp sáu kỹ thuật PEFT: LoRA [24], IA3 [37], Prompt tuning [30], và Prefix tuning [33]. Ngoài ra, chúng tôi khám phá QLoRA [13] với lượng tử hóa 8-bit và 4-bit, kết hợp LoRA và lượng tử hóa mô hình. Không giống như ICL và RAG, những kỹ thuật này đòi hỏi việc học các tham số mới để tinh chỉnh LLM cho tác vụ hạ nguồn cụ thể. Những phát hiện chính của chúng tôi là như sau:

–ICL cải thiện đáng kể hiệu quả của tất cả các mô hình so với prompt zero-shot cho sinh mã trên Conala và CodeAlpacaPy.
–Tăng số lượng ví dụ ICL không phải lúc nào cũng dẫn đến cải thiện hiệu quả. Các mô hình đạt hiệu quả đỉnh với tám và bốn ví dụ cho Conala và CodeAlpacaPy, tương ứng.
–LLM được tinh chỉnh với LoRA, IA3, và Prompt tuning, tức là vài triệu tham số, liên tục vượt trội SLM được tinh chỉnh đầy đủ với hàng trăm triệu tham số.
– Trong số các kỹ thuật PEFT, LoRA đạt hiệu quả cao nhất cho LLM và SLM.
–QLoRA giảm đáng kể việc sử dụng bộ nhớ, đạt được sự giảm lên đến 2 lần so với LoRA trong khi cải thiện hoặc bảo toàn hiệu quả của các mô hình. Hơn nữa, QLoRA cho phép tinh chỉnh LLM lên đến 34B tham số với ít hơn 24GB bộ nhớ GPU.
–LoRA tăng cường đáng kể hiệu suất của tất cả các mô hình so với ICL và RAG cho sinh mã trên Conala và CodeAlpacaPy.
– LoRA và QLoRA cải thiện hiệu quả của CodeLlama-7B-Instruct cho sinh mã trên bộ dữ liệu APPS.

Nghiên cứu của chúng tôi làm sáng tỏ những cơ hội đầy hứa hẹn mà các kỹ thuật PEFT nắm giữ, đảm bảo khám phá sâu hơn cho ứng dụng của chúng trong các tác vụ và tình huống liên quan đến mã khác.

Để tóm tắt, đóng góp của chúng tôi như sau:
–Chúng tôi tiến hành một nghiên cứu thực nghiệm toàn diện về sáu kỹ thuật PEFT, tức là LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA-8bit, và QLoRA-4bit, cho sinh mã Python trên một phạm vi rộng SLM và LLM.
–Một so sánh và phân tích toàn diện về các kỹ thuật PEFT đối với ICL và RAG cho LLM trên sinh mã.
–Chúng tôi chứng minh tính thực tiễn của việc tận dụng các kỹ thuật PEFT để tinh chỉnh hiệu quả LLM của mã và giảm gánh nặng tính toán liên quan đến tinh chỉnh đầy đủ, thể hiện các ứng dụng rộng hơn tiềm năng của chúng trong kỹ thuật phần mềm.

2 KIẾN THỨC NỀN
2.1 Học trong Ngữ cảnh (ICL) và Sinh Tăng cường Truy xuất (RAG)
Như một trong những loại kỹ thuật cụ thể liên quan đến LLM, ICL đã nổi lên như một kỹ thuật hiệu quả [5, 11,35,45,51]. ICL tìm cách cải thiện khả năng của LLM bằng cách tích hợp thông tin cụ thể ngữ cảnh, dưới dạng prompt đầu vào hoặc mẫu hướng dẫn, trong quá trình suy luận và do đó không cần thực hiện huấn luyện dựa trên gradient. Do đó, bằng cách xem xét ngữ cảnh, mô hình trở nên có khả năng sinh ra các đầu ra mạch lạc và có liên quan theo ngữ cảnh hơn. Tính mạch lạc ngữ cảnh này của LLM và không phải thực hiện huấn luyện dựa trên gradient tốn kém tạo thành những ưu điểm chính của việc sử dụng ICL để chuyên môn hóa LLM cho một tác vụ hoặc bộ dữ liệu cụ thể. Tuy nhiên, ICL cũng gây ra một số bất tiện, bao gồm nhu cầu thiết kế các prompt đại diện [37, 74, 90].

RAG là một cách tiếp cận tinh vi hơn để tiêm các ví dụ vào prompt đầu vào tại thời điểm suy luận. Không giống như ICL chọn các ví dụ ngẫu nhiên, RAG dựa vào một mô hình truy xuất truy xuất động các ví dụ từ một bộ dữ liệu gần với một truy vấn. Trong thực tế, truy vấn có thể được hình thành bằng cách sử dụng thông tin từ ví dụ kiểm tra tại thời điểm kiểm tra, chẳng hạn như vấn đề lập trình cho trường hợp sinh mã. Tổng thể, RAG cho phép tiêm thông tin có liên quan hơn trong prompt đầu vào so với ICL và đã được áp dụng thành công cho các tác vụ kỹ thuật phần mềm, như sinh mã [52,91], tóm tắt mã [39,52], và hoàn thiện mã [41]. Tuy nhiên, cả ICL và RAG đều gặp phải một số hạn chế. Một hạn chế liên quan đến việc giới thiệu thêm token đầu vào trong prompt, điều này có thể không khả thi khi thông tin ngữ cảnh quá lớn. Một hạn chế khác là sự phụ thuộc vào chất lượng và sự liên quan của các ví dụ được truy xuất. Trong RAG, mô hình truy xuất phải tìm chính xác các ví dụ thực sự tương tự hoặc hữu ích cho truy vấn kiểm tra. Nếu cơ chế truy xuất không thể xác định các ví dụ phù hợp, nó có thể tiêm thông tin không liên quan hoặc gây hiểu lầm vào prompt, cuối cùng làm giảm hiệu suất.

2.2 Tinh chỉnh Hiệu quả Tham số (PEFT)
PEFT đề cập đến việc sử dụng các kỹ thuật tối ưu hóa quá trình tinh chỉnh LLM bằng cách cập nhật có chọn lọc một tập con tham số thay vì cập nhật toàn bộ tham số của mô hình [14]. Về mặt kỹ thuật, các kỹ thuật PEFT tập trung vào việc học một số lượng nhỏ tham số cho tác vụ hiện tại bằng cách thiết kế các lớp bổ sung [23], thêm các token bổ sung đầu tiên [30,33], phân tách gradient trọng số thành các ma trận cụ thể [24]. Một trong những kỹ thuật PEFT tiên tiến đại diện là Thích ứng Thứ hạng Thấp của LLM (LoRA) [24]. Kỹ thuật này bao gồm việc đóng băng trọng số mô hình và tiêm các ma trận có thể huấn luyện thứ hạng thấp vào các lớp attention của kiến trúc Transformer [67], do đó giảm đáng kể số lượng tham số có thể huấn luyện. Chúng tôi sử dụng LoRA như một trong những kỹ thuật PEFT vì nó đã được sử dụng rộng rãi trong NLP [14,37,65] và cho thấy hiệu suất đầy hứa hẹn. Chúng tôi cũng sử dụng IA3 có ý định cải thiện LoRA và giảm thêm lượng tham số có thể huấn luyện [37]. Ngoài LoRA và IA3, chúng tôi cũng bao gồm Prompt tuning [30] và Prefix tuning [30] trong nghiên cứu của chúng tôi. Prompt tuning liên quan đến quá trình thêm token ảo vào token đầu vào của LLM, trong khi Prefix tuning chèn token ảo trong tất cả các lớp của mô hình mục tiêu và do đó yêu cầu học nhiều tham số hơn. Những token ảo này có thể vi phân, cho phép chúng được học thông qua lan truyền ngược trong quá trình tinh chỉnh, trong khi phần còn lại của LLM vẫn bị đóng băng. Hơn nữa, QLoRA [13] kết hợp LoRA với lượng tử hóa mô hình, cho phép tinh chỉnh LLM với ít bộ nhớ GPU hơn bằng cách giảm độ chính xác của các loại dữ liệu điểm nổi trong mô hình.

3 ÁP DỤNG LLM VỚI TÀI NGUYÊN HẠN CHẾ
Trong thời đại LLM, tính khả dụng của tài nguyên tính toán đáng kể đóng vai trò quan trọng trong việc khai thác khả năng cao của chúng. Thật không may, nhiều nhà nghiên cứu và nhà thực hành thường thấy mình bị hạn chế bởi tính khả dụng hạn chế của cơ sở hạ tầng tính toán cao cấp.

Ví dụ, một kỹ sư phần mềm chỉ có quyền truy cập vào một GPU tiêu dùng duy nhất (ví dụ: 24GB VRAM) có thể thấy tinh chỉnh đầy đủ không thực tế do nhu cầu bộ nhớ đáng kể. Sự gia tăng nhanh chóng về kích thước mô hình và số lượng tham số có thể huấn luyện làm trầm trọng thêm vấn đề này. Mặc dù hiệu quả, tinh chỉnh đầy đủ đi kèm với chi phí tính toán cao [3,15,51], nhấn mạnh sự đánh đổi tính toán-hiệu quả (xem Bảng 1).

Để giải quyết những hạn chế này, các cách tiếp cận thay thế như ICL và RAG đã thu hút sự chú ý. ICL và RAG cung cấp một tùy chọn tính toán thấp bằng cách loại bỏ nhu cầu cập nhật tham số. Tuy nhiên, những kỹ thuật này đi kèm với bộ thách thức riêng, bao gồm việc lựa chọn các ví dụ đại diện và độ nhạy cảm với thiết kế prompt [37,74,90]. Trong thực tế, điều này có thể dẫn đến hiệu quả thấp hơn so với tinh chỉnh, đặc biệt cho các tác vụ có ngữ cảnh cao phổ biến trong kỹ thuật phần mềm.

--- TRANG 6 ---
6•M. Weyssow và cộng sự.
Bảng 1. Sự đánh đổi tính toán-hiệu quả cho mỗi kỹ thuật tinh chỉnh mô hình.
Kỹ thuật Chi phí tính toán Hiệu quả
Tinh chỉnh đầy đủ cao [X] cao [✓]
ICL và RAG thấp [✓] thấp [X]
PEFT thấp [✓] cao [✓]
0 5 10 15 20 24
Tiêu thụ bộ nhớ đỉnh (GB)CodeT5+-220M-ft
CodeGen-350M-mono-ft
CodeT5+-770M-ft
CodeLlama-7B-QLoRA-4bit
CodeGen2-1B-lora
CodeGen2-3.7B-lora
CodeLlama-13B-QLoRA-4bit
CodeLlama-7B-lora
CodeGen2-7B-lora
CodeLlama-34B-QLoRA-4bit3.54
5.84
8.16
9.16
9.8
14.08
15.01
19.06
20.29
23.59
Hình 1. Tiêu thụ bộ nhớ GPU đỉnh trong quá trình tinh chỉnh mô hình sử dụng tinh chỉnh đầy đủ (ft), LoRA, và QLoRA.

Để vượt qua những hạn chế này, chúng tôi dự kiến sự xuất hiện của các kỹ thuật PEFT như những giải pháp đầy hứa hẹn, cung cấp các cách tiếp cận hiệu quả và có thể mở rộng hơn về mặt tính toán để tinh chỉnh LLM. Các phương pháp PEFT, như LoRA và QLoRA, hạn chế số lượng tham số được cập nhật, do đó giảm mức tiêu thụ bộ nhớ trong khi duy trì hiệu quả cạnh tranh với tinh chỉnh đầy đủ. Điều này làm cho PEFT đặc biệt phù hợp cho các nhà thực hành có quyền truy cập hạn chế vào tài nguyên tính toán. Như được minh họa trong Bảng 1, PEFT đạt được sự cân bằng tối ưu giữa chi phí tính toán và hiệu quả. Hơn nữa, Hình 1 cho thấy rằng bằng cách sử dụng các kỹ thuật PEFT như LoRA, các nhà thực hành có thể tinh chỉnh các mô hình như CodeLlama-7B mà không vượt quá 19GB bộ nhớ GPU. Đối với các mô hình thậm chí lớn hơn, như CodeLlama-34B, QLoRA với lượng tử hóa cho phép tinh chỉnh trong các ràng buộc của GPU VRAM 24GB.

Kết luận, PEFT trao quyền cho các kỹ sư phần mềm vượt qua những hạn chế tài nguyên, cho phép tinh chỉnh LLM hiệu quả trong các tác vụ có ngữ cảnh cao mà không dựa vào cơ sở hạ tầng tính toán đắt tiền. Điều này làm cho PEFT không chỉ là một công cụ thực tiễn mà còn thiết yếu để dân chủ hóa quyền truy cập vào khả năng LLM.

4 PHƯƠNG PHÁP
Trong phần này, chúng tôi trình bày thiết lập thí nghiệm của nghiên cứu. Chúng tôi tiến hành tất cả các thí nghiệm dưới một tình huống hạn chế tài nguyên. Cụ thể, tất cả các thủ tục, tức là tinh chỉnh và suy luận, của các mô hình được thực hiện với quyền truy cập vào một GPU 24GB duy nhất. Mục tiêu chính của nghiên cứu là chứng minh liệu việc tinh chỉnh LLM thông qua PEFT có khả thi và mong muốn hơn các cách tiếp cận trước và các mô hình nhỏ hơn trong bối cảnh này.

--- TRANG 7 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •7
4.1 Câu hỏi Nghiên cứu
Trong nghiên cứu này, chúng tôi tập trung vào các câu hỏi nghiên cứu sau:
–RQ1: LLM và SLM hoạt động như thế nào khi sử dụng ICL trên các bộ dữ liệu Conala và CodeAlpacaPy?
Chúng tôi nghiên cứu hiệu quả cơ bản của LLM (≥1B tham số) và SLM (<1B tham số) cho sinh mã sử dụng prompt zero-shot và ICL, nơi n ví dụ được chọn ngẫu nhiên được thêm vào prompt đầu vào. Chúng tôi kiểm tra mỗi mô hình với tối đa 16 ví dụ ICL, do tài nguyên tính toán hạn chế của chúng tôi.

Chúng tôi nghiên cứu hiệu quả của một phổ rộng SLM và LLM cho sinh mã trên hai bộ dữ liệu bao gồm mã có độ dài khác nhau. Chúng tôi chọn một loạt rộng các mô hình với kích thước khác nhau, được tiền huấn luyện trên các codebase đa dạng và với các mục tiêu học khác nhau để nghiên cứu cách những yếu tố này ảnh hưởng đến hiệu quả của chúng.

–RQ2: LLM và SLM hoạt động như thế nào khi sử dụng các kỹ thuật PEFT trên các bộ dữ liệu Conala và CodeAlpacaPy? Trong RQ này, chúng tôi điều tra liệu các kỹ thuật PEFT có liên tục vượt trội ICL cho SLM và LLM không. Chúng tôi so sánh các cấu hình hoạt động tốt nhất của ICL trong RQ1 với các kỹ thuật PEFT, bao gồm LoRA, IA3, Prompt tuning, Prefix tuning. Hơn nữa, chúng tôi cũng điều tra tác động của lượng tử hóa với QLoRA-8bit và QLoRA-4bit trên mô hình hoạt động tốt nhất và các biến thể lớn hơn.

Đối với SLM, chúng tôi cũng bao gồm so sánh với tinh chỉnh tham số đầy đủ, như thường được sử dụng trong các nghiên cứu SE trước [16,72,77,92]. Chúng tôi không bao gồm tinh chỉnh tham số đầy đủ cho LLM, vì nó không khả thi trong ngân sách tính toán của chúng tôi.

–RQ3: LoRA so sánh như thế nào với ICL và RAG trên các bộ dữ liệu Conala và CodeAlpacaPy?
Trong RQ này, chúng tôi so sánh hiệu quả của LLM hoạt động tốt nhất được tinh chỉnh sử dụng LoRA với RAG. Thiết lập RAG của chúng tôi bao gồm việc truy xuất tối đa 16 ví dụ từ tập huấn luyện có liên quan chặt chẽ đến prompt đầu vào, tương tự như các cách tiếp cận khác được đề xuất trước đây cho các tác vụ SE khác nhau [39, 41, 70].

–RQ4: Chúng ta có thể tăng cường hiệu quả của LLM cho sinh mã trong bộ dữ liệu APPS bằng LoRA và QLoRA không? Cuối cùng, chúng tôi khám phá liệu LLM được tinh chỉnh sử dụng LoRA và QLoRA có cho thấy cải thiện về tính chính xác chức năng trong bộ dữ liệu APPS. Chúng tôi tinh chỉnh LLM hoạt động tốt nhất sử dụng LoRA và QLoRA trên tập huấn luyện của APPS, và báo cáo trung bình của các test case đã qua cũng như Pass@k trên tập kiểm tra của APPS cho các vấn đề lập trình cấp độ giới thiệu, phỏng vấn, và thi đấu.

4.2 Bộ dữ liệu và Tác vụ
Trong suốt nghiên cứu, chúng tôi so sánh tất cả các mô hình được nghiên cứu trên tác vụ sinh mã Python. Tác vụ này đã thu hút sự chú ý đáng kể trong những năm gần đây [9,10,48,50,61] với sự xuất hiện của LLM và khả năng của chúng trong việc sinh mã Python theo cách zero-shot, tức là không cần tinh chỉnh thêm. Đặc biệt, các bộ dữ liệu đánh giá như HumanEval [9] đã được sử dụng rộng rãi để đánh giá các cách tiếp cận sinh mã [3,9,82]. Trong khi HumanEval được sử dụng rộng rãi, nó thiếu một kho huấn luyện để đánh giá các cách tiếp cận tinh chỉnh hoặc PEFT. Vì trọng tâm của nghiên cứu là chuyên môn hóa LLM sử dụng các kỹ thuật PEFT, chúng tôi đã chọn không sử dụng HumanEval. Thay vào đó, chúng tôi chọn sử dụng ba bộ dữ liệu sinh mã được sử dụng rộng rãi khác: các bộ dữ liệu Conala [87], CodeAlpaca [8], và APPS [22]. Tất cả các bộ dữ liệu cung cấp một số lượng đủ lớn

--- TRANG 8 ---
8•M. Weyssow và cộng sự.
0 25 50 75 100 125 150 175 200
Độ dài token0100200300400500600700Số mẫuAPPs
CodeAlpacaPy
Conala
Hình 2. Phân phối độ dài token của các bộ dữ liệu Conala, CodeAlpacaPy, và APPS.

các ví dụ có thể được sử dụng cho việc tinh chỉnh một mô hình và đã được sử dụng trong các nghiên cứu sinh mã trước với LLM [71, 73, 88, 91].

Bộ dữ liệu Conala. Chúng tôi sử dụng một phiên bản được tuyển chọn của bộ dữ liệu Conala [91]. Bộ dữ liệu được thu thập từ StackOverflow và chứa các cặp mã và ý định ngôn ngữ tự nhiên được chú thích thủ công. Mỗi ý định ngôn ngữ tự nhiên chứa gợi ý về các biến được thao tác trong mã thực tế, ví dụ, xem ví dụ đầu tiên trong Bảng 2, cung cấp thêm ngữ cảnh cho mô hình để sinh mã có liên quan. Trong Hình 2, chúng tôi báo cáo phân phối độ dài token của ba bộ dữ liệu. Trong Conala, hầu hết các giải pháp mã đều ngắn và một dòng, làm cho nó tương đối dễ dàng cho một LLM để sinh các dự đoán khớp chính xác. Trong phiên bản được tuyển chọn này của bộ dữ liệu, các tác giả đảm bảo rằng mỗi mẫu trong các tập validation và test chứa ít nhất một hàm Python không xuất hiện trong tập huấn luyện. Ngoài ra, họ đảm bảo rằng các ví dụ được thu thập từ cùng một bài đăng StackOverflow xuất hiện trong các tập khác nhau. Do đó, chúng tôi có thể đảm bảo rằng mỗi ý định tự nhiên trong kiểm tra không xuất hiện trong tập huấn luyện. Bộ dữ liệu chứa 2,135/201/543 mẫu như các tập huấn luyện/validation/test, tương ứng.

Bộ dữ liệu CodeAlpacaPy. Chúng tôi xây dựng một phiên bản Python được tuyển chọn của bộ dữ liệu CodeAlpaca [8] bằng cách chọn cụ thể các mẫu dữ liệu Python trong bộ dữ liệu CodeAlpaca. Chúng tôi lọc ra các mẫu mã không thể được phân tích tĩnh để đảm bảo bộ dữ liệu chỉ bao gồm các mã Python hợp lệ về cú pháp. Như được minh họa trong ví dụ dưới cùng của Bảng 2 và trong Hình 2, CodeAlpacaPy chứa các ví dụ dài hơn và phức tạp hơn so với Conala, cho phép đánh giá toàn diện hơn về PEFT cho sinh mã. Bộ dữ liệu chứa 2,192/314/628 mẫu như các tập huấn luyện/validation/test, tương ứng.

Bộ dữ liệu APPS. Bộ dữ liệu APPS bao gồm 10,000 vấn đề sinh mã, mỗi vấn đề được ghép với các giải pháp Python. Những vấn đề này được phân loại thành ba cấp độ khó: giới thiệu, phỏng vấn, và thi đấu, với các giải pháp biến đổi từ một dòng đơn giản đến các thuật toán phức tạp. Chúng ta có thể thấy trong Hình 2 và Bảng 2 rằng APPS bao gồm các ví dụ dài hơn và phức tạp hơn so với hai bộ dữ liệu khác. Trung bình, mỗi vấn đề được kèm theo 21.2 test case, được thiết kế để đánh giá tính chính xác chức năng của mã được sinh. Bộ dữ liệu gốc được chia thành 5,000 mẫu cho huấn luyện và 5,000 cho kiểm tra.

--- TRANG 9 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •9
Bảng 2. Tổng quan về tác vụ sinh mã, với ba ví dụ được lấy từ các bộ dữ liệu Conala, CodeAlpacaPy, và APPS.

Conala
Prompt :### Hướng dẫn:
ánh xạ hai danh sách 'keys' và 'values' thành một từ điển
### Phản hồi:
Thực tế :dict([(k, v) for k, v inzip(keys, values)])

CodeAlpacaPy
Prompt :### Hướng dẫn:
Viết một hàm để tính độ lệch chuẩn của các điểm dữ liệu trong Python.
### Phản hồi:
Thực tế :def stdev(data):
avg = sum(data) / len(data)
total = 0
for xindata:
total += (x - avg) ** 2
return (total / (len(data) - 1)) ** 0.5

APPS
Prompt :### Hướng dẫn:
Bạn được đưa ra một chuỗi s = s1 s2 . . . sn có độ dài n, chỉ chứa các chữ số 1, 2,..., 9. Một chuỗi con s[l...r] của s là một chuỗi slsl+1sl+2 ...sr. Một chuỗi con s[l...r] của s được gọi là chẵn nếu số được biểu diễn bởi nó là chẵn. Tìm số lượng chuỗi con chẵn của s. Lưu ý, ngay cả khi một số chuỗi con bằng nhau như chuỗi, nhưng có l và r khác nhau, chúng được tính như các chuỗi con khác nhau. Dòng đầu tiên chứa một số nguyên n (1≤n≤65000) — độ dài của chuỗi s. Dòng thứ hai chứa một chuỗi s có độ dài n. Chuỗi s chỉ bao gồm các chữ số 1, 2,..., 9. In số lượng chuỗi con chẵn của s.
### Phản hồi:
Thực tế :n = int(input())
ans = 0
for iinrange(n):
for jinrange(i, n):
ifint(s[i:j+1]) \% 2 == 0:
ans += 1
print(ans)

Trong nghiên cứu này, chúng tôi sử dụng 4,500 mẫu cho huấn luyện, 500 cho validation, và 750 cho kiểm tra, đảm bảo phân phối cân bằng 250 mẫu kiểm tra mỗi cấp độ khó.

Thiết kế tác vụ. Trong Bảng 2, chúng tôi minh họa tổng quan về thiết kế tác vụ. Prompt có dạng một mẫu hướng dẫn, nơi "### Hướng dẫn:" và "### Phản hồi:" đóng vai trò phân định hướng dẫn, tức là ý định ngôn ngữ tự nhiên, và câu trả lời, tức là sinh mã. Lưu ý rằng thiết kế prompt này có thể không tối ưu, nhưng loại mẫu hướng dẫn này đã được chứng minh là hiệu quả trong các công trình trước [36,89]. Mã được sinh bởi mô hình được so sánh với thực tế để đánh giá

--- TRANG 10 ---
10 •M. Weyssow và cộng sự.

chất lượng của việc sinh. Trong quá trình tinh chỉnh, chúng tôi tối thiểu hóa một hàm mất mát entropy chéo tự hồi quy tiêu chuẩn:

L=−𝑇+1∑︁
𝑖=1𝑀𝑖·log𝑃(𝑥𝑖|𝑥<𝑖),

trong đó:
𝑀𝑖=(
1,if𝑥𝑖≠−100
0,otherwise .

Mô hình nhận một sự nối của prompt và thực tế như đầu vào và dự đoán mỗi token 𝑥𝑖 theo cách tự hồi quy dựa trên các token trước 𝑥<𝑖. Lưu ý rằng trong tính toán mất mát, chúng tôi bỏ qua các token từ mẫu hướng dẫn để buộc mô hình tập trung vào việc sinh mã. Chúng tôi đặt giá trị của các token hướng dẫn thành −100 và bỏ qua chúng trong tính toán mất mát sử dụng hàm chỉ thị 𝑀𝑖. Tại thời điểm suy luận, mô hình nhận prompt như đầu vào và cố gắng sinh mã thực tế bằng cách sinh tối đa 10 ứng viên mã.

4.3 ICL và RAG
Chúng tôi tiến hành thí nghiệm sử dụng ICL và RAG trên các bộ dữ liệu Conala và CodeAlpacaPy. Đối với cả hai kỹ thuật, chúng tôi chọn số lượng mẫu tối đa có thể vừa với bộ nhớ GPU của chúng tôi. Đối với ICL, chúng tôi sử dụng tối đa 16 ví dụ cho bộ dữ liệu Conala và 8 ví dụ cho CodeAlpacaPy. Những ví dụ này được lấy mẫu ngẫu nhiên từ các bộ dữ liệu huấn luyện tương ứng và được nối với prompt đầu vào trong quá trình suy luận. Đối với RAG, chúng tôi tận dụng GTE-small, một mô hình nhúng mục đích chung, nhẹ vượt trội nhiều mô hình lớn hơn, bao gồm các nhúng độc quyền của OpenAI [34]. Chúng tôi tạo nhúng cho tất cả các hướng dẫn (loại trừ mã) trong các tập huấn luyện. Tại thời điểm suy luận, chúng tôi truy xuất tối đa 16 ví dụ cho Conala và 4 ví dụ cho CodeAlpacaPy, chọn những ví dụ có hướng dẫn tương tự nhất với đầu vào kiểm tra. Như với ICL, các ví dụ được truy xuất được nối với vấn đề đầu vào để hướng dẫn sinh mã.

4.4 Mô hình Ngôn ngữ Nhỏ và Lớn
Để thực hiện phân tích toàn diện, chúng tôi đã chọn SLM và LLM theo một số tiêu chí. Đầu tiên, chúng tôi chỉ xem xét các mô hình nguồn mở. Chúng tôi bỏ qua các LLM nguồn đóng như Codex do không thể truy cập các tham số của chúng, điều này làm cho việc nghiên cứu bất kỳ kỹ thuật tinh chỉnh nào trở nên không khả thi. Tất cả các checkpoint mô hình được nghiên cứu có thể được truy cập tự do, và đã được tiền huấn luyện sử dụng dữ liệu nguồn mở. Thứ hai, chúng tôi chọn các LLM đã được phát hành trong hai năm qua. Cuối cùng, để điều tra tác động của việc mở rộng, chúng tôi chọn các mô hình với một loạt tham số đa dạng. Chúng tôi xem các mô hình có ít hơn 1B tham số là SLM, và những mô hình khác là LLM. Lưu ý rằng chúng tôi chọn các mô hình phù hợp với một GPU 24GB duy nhất để tinh chỉnh và suy luận mà không gây tràn bộ nhớ. Tổng cộng, chúng tôi bao gồm 11 SLM và LLM từ các họ mô hình đa dạng để tiến hành thí nghiệm.

–SLM. Chúng tôi sử dụng CodeGen-350M-mono [48], CodeT5+-220M [71], và CodeT5+-770M [71] như SLM. CodeGen-350M-mono là một mô hình ngôn ngữ tự hồi quy và một phiên bản nhỏ của CodeGen được tiền huấn luyện trên các ngôn ngữ lập trình khác nhau và được tinh chỉnh thêm trên dữ liệu Python. CodeT5+-220M và CodeT5+-770M là các mô hình ngôn ngữ encoder-decoder cải thiện CodeT5 bằng cách tận dụng giai đoạn tiền huấn luyện hai giai đoạn trên dữ liệu ngôn ngữ tự nhiên và mã, và các mục tiêu học mới.

--- TRANG 11 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •11
–CodeGen2 [47] là một họ mô hình ngôn ngữ dựa trên tiền tố kết hợp các lược đồ học của encoder hai chiều và decoder một chiều. CodeGen2 cải thiện CodeGen [48], do đó chúng tôi không bao gồm họ CodeGen trong đánh giá của chúng tôi. Các mô hình CodeGen2 được tiền huấn luyện trên một phiên bản khử trùng của TheStack [28] bao trùm một loạt rộng các ngôn ngữ. Chúng tôi sử dụng CodeGen2-1B, CodeGen2-3.7B và CodeGen2-7B.

–CodeLlama [56] là một họ LLM dựa trên Llama 2 [64]. Mỗi mô hình được khởi tạo với Llama 2 và được tiền huấn luyện thêm trên mã. CodeLlama có ba biến thể khác nhau: CodeLlama chuyên cho mã, CodeLlama-Instruct chuyên cho tinh chỉnh hướng dẫn và CodeLlama-Python chuyên cho Python. Chúng tôi sử dụng CodeLlama-7B, CodeLlama-7B-Instruct và CodeLlama-7B-Python để khởi tạo thí nghiệm. Trong RQ4, chúng tôi tinh chỉnh CodeLlama-13B-Python và CodeLlama-34B-Python sử dụng QLoRA.

4.5 Metrics
Chúng tôi đo hiệu quả của các mô hình thông qua các metrics được sử dụng rộng rãi trong công trình sinh mã trước. Đối với thí nghiệm trên Conala và CodeAlpacaPy, chúng tôi báo cáo các metrics Exact Match (EM) và CodeBLEU [55]. Đưa ra một mã được sinh và một thực tế, EM trả về 1 nếu cả hai mã giống hệt nhau, ngược lại 0. Để đánh giá hiệu quả của các mô hình trên một danh sách k∈[1,10] ứng viên, chúng tôi báo cáo EM@k, tính toán trung bình dự đoán chính xác trong một danh sách k ứng viên. Đối với thí nghiệm trên bộ dữ liệu APPS, chúng tôi báo cáo hai metrics: số lượng test case đã qua trung bình và Pass@k. Số lượng test case đã qua trung bình đánh giá mô hình hoạt động tốt như thế nào bằng cách đo tỷ lệ test case mà mã được sinh của nó vượt qua cho mỗi mẫu. Ngược lại, Pass@k là một metric nghiêm ngặt hơn đo tỷ lệ phần trăm vấn đề mà ít nhất một trong số k mẫu mã được sinh hàng đầu vượt qua tất cả test case, phản ánh khả năng của mô hình để tạo ra các giải pháp hoàn toàn chính xác trong k lần thử.

4.6 Chi tiết Thực hiện
Đối với tất cả thí nghiệm, chúng tôi sử dụng một GPU NVIDIA RTX A5000 24GB duy nhất. Chúng tôi nghiên cứu tổng cộng bảy kỹ thuật tinh chỉnh: Tinh chỉnh đầy đủ, ICL, LoRA [24], IA3 [37], Prompt tuning [30], Prefix tuning [33], và QLoRA [13]. Chúng tôi thực hiện tất cả các kỹ thuật tinh chỉnh sử dụng các thư viện HuggingFace [79] và PEFT [43].

Chúng tôi chỉ sử dụng tinh chỉnh đầy đủ cho SLM, vì việc tinh chỉnh tất cả các tham số của LLM không thể xử lý được về mặt tính toán trong tối đa 24GB bộ nhớ GPU. Chúng tôi đặt tốc độ học thành 5e−5. Đối với LoRA và IA3, chúng tôi áp dụng phân tách ma trận thứ hạng thấp trên các lớp attention của các mô hình và đặt r=16 và α=32. Để thực hiện QLoRA, chúng tôi sử dụng lượng tử hóa 8-bit và 4-bit [12]. Chúng tôi đặt tốc độ học thành 3e−4 cho LoRA, IA3 và QLoRA. Đối với Prompt tuning và Prefix tuning, chúng tôi thêm một tập 20 token ảo liên tục có thể huấn luyện vào mỗi mẫu đầu vào của các mô hình và áp dụng tốc độ học 3e−3 và 3e−2.

Chúng tôi sử dụng optimizer Adafactor [60] với độ chính xác float 16-bit cho tất cả các mô hình. Chúng tôi tinh chỉnh các mô hình tối đa năm epoch và đánh giá chúng mỗi 0.2∗len(train_set) bước tối ưu hóa. Chúng tôi tinh chỉnh tất cả các mô hình với kích thước batch 8. Chúng tôi chọn checkpoint có mất mát đánh giá thấp nhất cho suy luận và thấy rằng tìm kiếm chùm với kích thước chùm 10 mang lại hiệu quả tốt nhất. Đưa ra phân phối độ dài token khác nhau và độ phức tạp của các bộ dữ liệu, chúng tôi sinh mã với tối đa 64,

--- TRANG 12 ---
12 •M. Weyssow và cộng sự.
012345 8 16
Số ví dụ ngẫu nhiên051015202530EM@10
Conala
0 1 2 3 4 5 8
Số ví dụ ngẫu nhiên024681012EM@10
CodeAlpacaPyCodeLlama-7B-Python
CodeLlama-7B-Instruct
CodeLlama-7BCodeGen2-7B
CodeGen2-3.7B
CodeGen2-1BCodeGen-350M-mono
CodeT5+-770M
CodeT5+-220M
Hình 3. [RQ1] – Hiệu quả của các mô hình sử dụng ICL với số lượng ví dụ ngẫu nhiên khác nhau trên các bộ dữ liệu Conala và CodeAlpacaPy.

128, và 1024 token cho Conala, CodeAlpacaPy, và APPS, tương ứng. Chúng tôi công khai mã của chúng tôi: https://github.com/martin-wey/peft-llm-code.

5 KẾT QUẢ THÍ NGHIỆM
5.1 RQ1: Hiệu quả Cơ bản của Mô hình Sử dụng Zero-Shot và ICL
Chúng tôi bắt đầu bằng việc điều tra hiệu quả cơ bản của tất cả SLM và LLM cho sinh mã dựa trên khớp. Cụ thể, chúng tôi sử dụng các cách tiếp cận zero-shot và ICL với tối đa 16 ví dụ ngẫu nhiên được truy xuất cho bộ dữ liệu Conala và tám cho bộ dữ liệu CodeAlpacaPy. Lý do sử dụng ít ví dụ hơn cho CodeAlpacaPy là vì xem xét 16 ví dụ dẫn đến lỗi hết bộ nhớ dưới thiết lập của chúng tôi. Chúng tôi đánh giá hiệu quả của các mô hình sử dụng EM@10 và so sánh chúng trên hai bộ dữ liệu này trong Hình 3. Lưu ý rằng kiến trúc CodeGen2 dẫn đến việc sử dụng bộ nhớ GPU nhiều hơn đáng kể so với các mô hình khác, điều này giải thích tại sao chúng tôi đánh giá ICL với ít ví dụ hơn so với các mô hình khác.

Đầu tiên, chúng tôi quan sát một khoảng cách đáng kể trong EM@10 giữa hai bộ dữ liệu. Sự khác biệt này có thể được giải thích bởi thực tế là bộ dữ liệu CodeAlpacaPy chứa các mẫu thách thức hơn nhiều so với bộ dữ liệu Conala, như được hiển thị trong Bảng 2.

Thứ hai, có một khoảng cách đáng chú ý về hiệu quả giữa SLM và LLM, bất kể số lượng ví dụ được cung cấp. Quan sát này làm nổi bật những ưu điểm của tiền huấn luyện quy mô lớn và việc sử dụng các mô hình lớn hơn trong bối cảnh này.

Đối với bộ dữ liệu Conala, việc tăng số lượng ví dụ dẫn đến điểm EM@10 cao hơn. Tuy nhiên, khi sử dụng nhiều hơn tám ví dụ, hiệu quả của các mô hình bắt đầu giảm. Đối với bộ dữ liệu CodeAlpacaPy, một xu hướng tương tự được quan sát, nhưng số lượng ví dụ tối ưu nhỏ hơn. Hầu hết các mô hình đạt điểm EM@10 tốt nhất khi sử dụng ba hoặc bốn ví dụ. Quan sát này nhấn mạnh hạn chế của ICL, vì việc thêm nhiều ví dụ hơn dẫn đến sự suy giảm hiệu quả của các mô hình.

--- TRANG 13 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •13

Cuối cùng, các mô hình CodeLlama vượt trội tất cả các mô hình trên cả hai bộ dữ liệu, đạt EM@10 đỉnh 29.83 trên Conala (CodeLlama-7B) và 11.94 trên CodeAlpacaPy (CodeLlama-7B-Python). Ngược lại, các mô hình nhỏ hơn, như CodeGen2-3.7B đạt EM@10 là 23.94 và 7.00 trên Conala và CodeAlpacaPy, tương ứng.

Trả lời cho RQ1: ICL cải thiện đáng kể hiệu quả của tất cả các mô hình so với zero-shot. Mô hình tốt nhất của chúng tôi, CodeLlama-7B, đạt điểm EM@10 29.83 (7.73) và 11.62 (7.01) trên Conala và CodeAlpacaPy với ICL (zero-shot), tương ứng.

5.2 RQ2: Hiệu quả của Mô hình sử dụng Kỹ thuật PEFT
Chúng tôi báo cáo kết quả chi tiết về hiệu quả của SLM và LLM trên sinh mã dựa trên khớp cho cả bộ dữ liệu Conala và CodeAlpacaPy trong Bảng 3.

SLM vs. LLM. CodeGen-350M-mono với LoRA thể hiện hiệu quả tốt nhất trung bình trong số các mô hình nhỏ, trong khi CodeLlama-7B-Python với LoRA là LLM tốt nhất trung bình. Dưới cùng một hạn chế bộ nhớ GPU 24GB, LLM tốt nhất vượt trội mô hình nhỏ tốt nhất 39.8%, 41.7%, và 47.1% (72.3%, 48.8%, và 9.1%) trong EM@1, EM@10, và CodeBLEU liên quan đến bộ dữ liệu Conala (CodeAlpacaPy), tương ứng.

SLM. Trong số các SLM, CodeGen-350M-mono cho thấy hiệu quả cao nhất trên tất cả các metrics trên cả hai bộ dữ liệu. Kết quả của chúng tôi phù hợp với các nghiên cứu trước [48,73,91] xác định CodeGen-350M-mono như một SLM mạnh mẽ cho các tác vụ sinh mã Python. Thú vị, mặc dù nó yêu cầu tinh chỉnh khoảng 1% tổng tham số của mô hình, LoRA xuất hiện như kỹ thuật tinh chỉnh tốt nhất, vượt trội tinh chỉnh đầy đủ với một biên độ đáng kể trên gần như tất cả các cấu hình. Ví dụ, điểm EM@10 cho CodeGen-350M-mono trên bộ dữ liệu Conala, với tinh chỉnh đầy đủ, là 18.42, trong khi nó tăng vọt lên 25.60 với LoRA.

LLM. Trong Hình 4, chúng tôi trình bày phân tích so sánh về hiệu quả của các mô hình khi được tinh chỉnh sử dụng LoRA, tập trung vào điểm CodeBLEU và EM@10. Cả hai biểu đồ đều rõ ràng thiết lập các mô hình CodeLlama như LLM hoạt động tốt nhất trong nghiên cứu của chúng tôi. Đáng chú ý, CodeGen2-7B, mặc dù chia sẻ số lượng tham số tương tự, thua kém tất cả các biến thể CodeLlama-7B. Không ngạc nhiên, việc khai thác các mô hình lớn hơn dẫn đến hiệu quả tốt hơn. Đưa ra chi phí tính toán thấp của các kỹ thuật PEFT, việc tận dụng các mô hình nhỏ hơn trong bối cảnh tương tự như của chúng tôi có vẻ phản tác dụng. Sau đó, trong bài báo này, chúng tôi chứng minh rằng ngay cả các mô hình lớn hơn có thể được tinh chỉnh thông qua sự kết hợp của PEFT với lượng tử hóa.

Kỹ thuật PEFT tốt nhất. Nhìn chung, LoRA nổi lên như kỹ thuật PEFT hiệu quả nhất trong số những kỹ thuật được nghiên cứu. Mặc dù được trình bày như một cải thiện tăng dần so với LoRA [37], IA3 thường cho thấy điểm thấp hơn so với LoRA. Prompt tuning xuất hiện như một tùy chọn tinh chỉnh khả thi khác, trong khi giảm thêm số lượng tham số có thể huấn luyện. Tuy nhiên, Prefix tuning thất bại trong việc thích ứng hiệu quả các mô hình lớn hơn với cả hai bộ dữ liệu.

Phân tích của chúng tôi tiết lộ điểm EM cao hơn đáng kể cho bộ dữ liệu Conala, có thể được quy cho sự khác biệt về độ phức tạp tác vụ giữa hai bộ dữ liệu (xem Phần 4.2). Điều quan trọng cần lưu ý là điểm CodeBLEU trên Conala tương đối thấp hơn do sự phụ thuộc của metric vào tính toán đồ thị luồng dữ liệu, có thể không phải lúc nào cũng có sẵn cho các ví dụ mã nhỏ.

--- TRANG 14 ---
14 •M. Weyssow và cộng sự.
Bảng 3. [RQ2] – So sánh SLM và LLM sử dụng các kỹ thuật tinh chỉnh khác nhau (xanh: phương pháp tinh chỉnh hoạt động tốt nhất mỗi mô hình, cam: mô hình hoạt động tốt nhất tổng thể).

Conala CodeAlpacaPy
Mô hình Tinh chỉnh # Params EM@1 EM@10 CodeBLEU EM@1 EM@10 CodeBLEU
SLMs
CodeT5+-220M Tinh chỉnh đầy đủ 220M 3.87 8.84 16.70 3.98 7.64 25.49
LoRA 2.7M 6.08 12.71 18.96 3.34 5.26 24.32
IA3 0.17M 4.42 10.68 17.08 0.64 1.27 22.06
Prompt tuning 0.03M 4.79 9.21 16.30 0.96 2.07 19.01
Prefix tuning 0.18M 3.13 7.55 14.56 0.16 1.27 20.80
CodeT5+-770M Tinh chỉnh đầy đủ 770M 4.05 8.29 15.11 3.19 6.21 27.73
LoRA 7M 8.66 17.13 20.64 3.66 6.85 26.10
IA3 0.4M 8.10 17.50 18.68 2.87 5.26 25.84
Prompt tuning 0.04M 7.37 15.47 16.75 1.91 3.82 20.57
Prefix tuning 0.5M 4.97 11.97 16.77 0.16 1.27 22.91
CodeGen-350M-mono Tinh chỉnh đầy đủ 350M 7.92 18.42 14.68 2.23 5.73 21.78
LoRA 1.3M 12.52 25.60 17.89 4.62 10.70 30.09
IA3 0.16M 11.42 25.78 18.83 4.46 10.70 28.56
Prompt tuning 0.02M 7.92 20.26 16.29 0.0 0.0 25.91
Prefix tuning 0.4M 5.34 12.52 17.53 0.0 0.0 26.89
LLMs
CodeGen2-1B LoRA 2M 9.39 23.02 19.76 3.82 9.08 23.48
IA3 0.2M 10.13 22.84 18.64 3.82 9.87 24.42
Prompt tuning 0.04M 11.97 22.65 18.38 0.80 2.07 18.17
Prefix tuning 0.6M 5.89 15.84 18.46 0.0 0.32 13.68
CodeGen2-3.7B LoRA 4M 11.60 25.97 19.00 5.41 10.70 23.75
IA3 0.5M 10.87 25.23 19.21 5.41 10.99 26.26
Prompt tuning 0.08M 11.05 26.89 19.53 0.0 0.0 23.42
Prefix tuning 1.3M 10.68 24.68 20.23 0.16 0.32 21.73
CodeGen2-7B LoRA 8.3M 11.23 29.83 23.86 5.57 11.94 27.73
IA3 1M 11.42 29.65 21.98 5.73 12.42 28.26
Prompt tuning 0.08M 11.97 27.26 22.37 0.0 0.0 25.40
Prefix tuning 2.6M 9.95 23.94 22.29 0.0 0.32 25.72
CodeLlama-7B LoRA 12.5M 20.07 39.31 25.33 7.33 16.24 32.05
IA3 1M 17.68 37.20 23.19 8.12 15.45 30.47
Prompt tuning 0.08M 19.15 38.12 25.01 0.32 0.48 31.55
Prefix tuning 2.6M 8.47 19.52 23.19 0.16 0.16 28.09
CodeLlama-7B-Instruct LoRA 12.5M 17.68 36.28 24.27 7.01 17.04 31.42
IA3 1M 15.84 36.10 24.71 8.12 16.72 31.01
Prompt tuning 0.08M 18.97 35.54 25.77 1.59 3.50 31.14
Prefix tuning 2.6M 10.13 18.23 23.66 0.64 0.96 31.27
CodeLlama-7B-Python LoRA 12.5M 17.50 36.28 24.27 7.96 15.92 32.84
IA3 1M 14.55 31.12 24.74 8.76 16.56 29.82
Prompt tuning 0.08M 16.76 37.02 26.31 0.96 3.03 33.46
Prefix tuning 2.6M 9.76 22.47 19.47 0.0 0.0 30.71

--- TRANG 15 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •15
15 20 25 30 35
EM@101819202122232425CodeBLEU
CodeT5+-220MCodeT5+-770M
CodeGen2-1B
CodeGen-350M-monoCodeGen2-3.7BCodeGen2-7BCodeLlama-7B-PythonCodeLlama-7B-InstructCodeLlama-7B
(a) Conala
6 8 10 12 14 16
EM@102426283032CodeBLEU
CodeT5+-220MCodeT5+-770M
CodeGen2-1BCodeGen2-3.7BCodeGen-350M-mono
CodeGen2-7BCodeLlama-7BCodeLlama-7B-Python
CodeLlama-7B-Instruct (b) CodeAlpacaPy
Hình 4. [RQ2] – Hiệu quả của các mô hình được tinh chỉnh sử dụng LoRA cho cả hai bộ dữ liệu về EM@10 và CodeBLEU.

Tác động của lượng tử hóa với QLoRA. Chúng tôi khám phá những lợi ích tiềm năng của việc sử dụng QLoRA [13], một kỹ thuật hiệu quả về mặt tính toán kết hợp LoRA với lượng tử hóa 8-bit hoặc 4-bit để tinh chỉnh LLM. Trong Hình 5, chúng tôi hiển thị điểm EM@10 cho ba biến thể mô hình CodeLlama: CodeLlama-7B-Python, CodeLlama-13B-Python, và CodeLlama-34B-Python, cùng với mức tiêu thụ bộ nhớ GPU đỉnh luôn dưới 24GB cho mỗi cấu hình tinh chỉnh. Kết quả nhấn mạnh sự cải thiện đáng kể về hiệu quả của các mô hình lượng tử hóa lớn hơn trên Conala, với tác động vừa phải hơn trên CodeAlpacaPy. Ví dụ, CodeLlama-34B-Python, được tinh chỉnh với QLoRA-4bit, đạt được sự gia tăng đáng kể 12.2% trong điểm EM@10 của Conala (40.70) so với CodeLlama-7B-Python với LoRA (36.28). Đáng ngạc nhiên, QLoRA cũng mang lại những cải thiện đáng chú ý so với LoRA cho CodeLlama-7B-Python trên Conala, trong khi đạt kết quả tương đương trên CodeAlpacaPy. Việc áp dụng lượng tử hóa cho phép sử dụng các mô hình lớn hơn có thể được chứa trong một GPU 24GB duy nhất. Cụ thể, đối với CodeLlama-7B-Python, QLoRA-4bit đạt được sự giảm 2x đáng kể về việc sử dụng bộ nhớ đỉnh trong khi cải thiện đáng kể điểm EM@10.

Trả lời cho RQ2: LLM với PEFT liên tục và đáng kể vượt trội SLM dưới cùng một giới hạn GPU. Cụ thể, LLM hoạt động tốt nhất với PEFT vượt trội mô hình nhỏ tốt nhất 39.8–72.3% về EM@k. Trong số các kỹ thuật PEFT khác nhau, LoRA là hiệu quả nhất. Ngoài ra, việc áp dụng lượng tử hóa với LoRA dẫn đến sự giảm đáng kể về việc sử dụng GPU trong khi duy trì hiệu quả trên cả hai bộ dữ liệu và chứa việc tinh chỉnh các mô hình lớn hơn lên đến 34B tham số.

5.3 RQ3: Phân tích So sánh LoRA, ICL, và RAG
Trong RQ này, chúng tôi nhằm điều tra liệu các kỹ thuật PEFT có liên tục vượt trội ICL và RAG được sử dụng rộng rãi khi áp dụng LLM trong sinh mã dựa trên khớp không.

Trong Hình 6, chúng tôi so sánh hiệu quả của SLM và LLM sử dụng ICL và LoRA về CodeBLEU và EM@10. Trong hình này, chúng tôi báo cáo các metrics cao nhất đạt được qua các cấu hình ICL khác nhau cho mỗi mô hình. Trong Hình 7, chúng tôi khám phá hiệu quả của các mô hình CodeLlama sử dụng RAG, với tối đa 16 và 4 ví dụ được truy xuất cho Conala và CodeAlpacaPy, tương ứng. Tương tự như RQ1, chúng tôi sử dụng ít ví dụ hơn cho CodeAlpacaPy để tránh lỗi hết bộ nhớ. Chúng tôi so sánh hiệu quả của RAG với LoRA và điểm EM@10 tốt nhất đạt được sử dụng ICL.

LoRA vs. ICL. Như được hiển thị trong Hình 6, tất cả các mô hình được tinh chỉnh với LoRA thể hiện điểm EM@10 cao hơn đáng kể so với ICL trên cả hai bộ dữ liệu. Ví dụ, CodeLlama-7B-Python với tinh chỉnh LoRA đạt được sự cải thiện 23.1% về EM@10 trên Conala (36.28 cho LoRA vs. 29.47 cho ICL). Mô hình này

--- TRANG 16 ---
16 •M. Weyssow và cộng sự.
CL-7B CL-13B CL-34B303234363840EM@1036.337.438.5
37.838.140.7Conala
CL-7B CL-13B CL-34B05101515.916.417.4
15.816.417.4CodeAlpacaPy
0
10
20GPU peak (GB)24 GB0
10
2024 GBLoRA QLoRA-8bit QLoRA-4bit
Hình 5. [RQ2] – Hiệu quả và việc sử dụng GPU của các LLM CodeLlama-Python (CL) 7B, 13B, và 34B được tinh chỉnh sử dụng LoRA và QLoRA với lượng tử hóa 8-bit và 4-bit.

0 510 15 20 25 30 35 40
EM@10121518212427CodeBLEUConala
Tinh chỉnh
LoRA
ICL
0 3 6 9 12 15
EM@101820222426283032CodeBLEUCodeAlpacaPyCodeT5+-220M
CodeT5+-770M
CodeGen-350M-monoCodeGen2-1B
CodeGen2-3.7B
CodeGen2-7BCodeLlama-7B
CodeLlama-7B-Instruct
CodeLlama-7B-Python
Hình 6. [RQ3] – So sánh hiệu quả của các mô hình được tinh chỉnh sử dụng LoRA và ICL trên các bộ dữ liệu Conala và CodeAlpacaPy.

mô hình giữ cho CodeAlpacaPy, với những lợi ích tương đối lớn hơn về EM@10. Tuy nhiên, chúng tôi quan sát một số biến thiên trong điểm CodeBLEU cho hầu hết các mô hình trên CodeAlpacaPy. Ví dụ, CodeLlama-7B thấy sự gia tăng CodeBLEU 2.36 với LoRA. Tuy nhiên, trên CoNala, tác động của LoRA đối với CodeBLEU ít rõ ràng hơn so với ICL. Những khác biệt này có thể được giải thích bởi bản chất của các metrics: EM@10 bảo thủ hơn, yêu cầu giải pháp được sinh phải khớp chính xác với thực tế, trong khi CodeBLEU cho điểm cao hơn cho các giải pháp gần nhưng không chính xác. Sự phân biệt này làm nổi bật cách LoRA thích ứng tốt hơn các mô hình với các bộ dữ liệu hạ nguồn, đặc biệt khi độ chính xác là quan trọng.

RAG vs. ICL vs. LoRA. Trong việc so sánh RAG, ICL, và LoRA trên bộ dữ liệu CoNala, RAG thể hiện hiệu quả cao hơn ICL nhưng không đạt được hiệu quả của LoRA trên tất cả ba biến thể mô hình CodeLlama. Đáng chú ý, CodeLlama-7B đạt tối đa 29.83 và 35.17 EM@10 với ICL và RAG, tương ứng, trong khi mô hình được tinh chỉnh với LoRA đạt EM@10 39.31.

Đối với cả bộ dữ liệu Conala và CodeAlpacaPy, những lợi ích về EM@10 trở nên mỏng hơn khi chúng tôi tăng số lượng ví dụ sử dụng RAG. EM@10 bão hòa ở khoảng 8–16 ví dụ cho Conala và 3–4 ví dụ cho CodeAlpacaPy. Hơn nữa, chúng tôi lưu ý rằng đối với bộ dữ liệu CodeAlpacaPy thách thức hơn, RAG mang lại EM@10 thấp hơn so với các ví dụ được chọn ngẫu nhiên sử dụng ICL, làm nổi bật những hạn chế của RAG khi độ phức tạp vấn đề tăng. Tuy nhiên, LoRA liên tục vượt trội cả RAG và ICL trên CodeAlpacaPy, làm nổi bật khả năng vượt trội của nó trong việc thích ứng với các bộ dữ liệu thách thức hơn.

Trả lời cho RQ3: LoRA vượt trội ICL và RAG trên các bộ dữ liệu Conala và CodeAlpacaPy trên ba biến thể CodeLlama-7B.

--- TRANG 17 ---
17 •M. Weyssow và cộng sự.
010203040EM@10LoRA: 39.31
ICL: 29.83LoRA: 36.28
ICL: 27.99LoRA: 36.28
ICL: 29.47
CL-7B CL-7B-Instruct CL-7B-Python051015EM@10LoRA: 16.24
ICL: 11.62LoRA: 17.04
ICL: 10.35LoRA: 15.92
ICL: 11.94# Ví dụ
1
2
3
4
5
8
16
Hình 7. [RQ3] – So sánh hiệu quả của RAG với số lượng ví dụ được truy xuất khác nhau đối với ICL và LoRA trên các bộ dữ liệu Conala (trên) và CodeAlpacaPy (dưới). Điểm ICL mô tả điểm cao nhất đạt được cho mỗi mô hình trong RQ1.

--- TRANG 18 ---
18 •M. Weyssow và cộng sự.
Bảng 4. [RQ4] – Hiệu quả của CodeLlama-7B-Instruct trên bộ dữ liệu APPs trong zero-shot và sử dụng LoRA, QLoRA-8bit, và QLoRA-4bit về số test đã qua trung bình (Avg) và Pass@k (P@k).

Giới thiệu Phỏng vấn Thi đấu
Mô hình Avg P@1 P@2 P@5 Avg P@1 P@2 P@5 Avg P@1 P@2 P@5
CodeLlama-7B-Instruct 13.66 4.16 6.24 8.80 13.44 0.80 1.32 2.40 6.27 0.56 1.00 2.00
+LoRA 19.57 5.60 8.04 11.20 16.96 1.04 1.80 3.20 6.93 0.32 0.60 1.20
+QLoRA-8bit 17.63 3.68 5.40 7.60 15.53 1.04 1.64 2.40 7.59 0.24 0.48 1.20
+QLoRA-4bit 20.84 5.76 8.40 12.40 20.34 1.04 1.76 2.80 6.66 0.48 0.88 1.60

5.4 RQ4: Khám phá LoRA và QLoRA cho Sinh mã trên APPs
Trong RQ cuối cùng này, chúng tôi khám phá khả năng áp dụng rộng hơn của LoRA và QLoRA, để tăng cường hiệu quả của CodeLlama-7B-Instruct cho sinh mã dựa trên thực thi. Lý do chọn biến thể instruct của CodeLlama-7B là vì mô hình này thường cho thấy hiệu quả cao hơn so với các biến thể mô hình khác trên APPs trong bài báo tiên phong của CodeLlama [56]. Chúng tôi không so sánh LoRA và QLoRA với ICL và RAG cho bộ dữ liệu này vì chúng yêu cầu tăng độ dài prompt vượt quá 2,048 token, dẫn đến lỗi hết bộ nhớ. Kết quả của chúng tôi, được tóm tắt trong Bảng 4, tập trung vào số lượng test case đã qua trung bình (Avg) và Pass@k cho các tác vụ cấp độ giới thiệu, phỏng vấn, và thi đấu.

Đối với cả tác vụ sinh mã cấp độ giới thiệu và phỏng vấn, LoRA và QLoRA-8/4bit dẫn đến những cải thiện đáng kể về số lượng test case đã qua trung bình. Cụ thể, QLoRA-4bit dẫn đến sự gia tăng đáng chú ý 52% về số lượng test đã qua trung bình so với mô hình cơ sở. Về các metrics Pass@k, cả LoRA và QLoRA-4bit đều thể hiện lợi ích ở cấp độ giới thiệu, với Pass@5 cải thiện +3.60% so với mô hình cơ sở. Tuy nhiên, những cải thiện này ít đáng kể hơn cho sinh mã cấp độ phỏng vấn và thi đấu, phản ánh độ phức tạp và thách thức lớn hơn do những tác vụ tiên tiến hơn này đặt ra.

Trả lời cho RQ4: LoRA và QLoRA tăng cường hiệu quả của CodeLlama-7B-Instruct trên APPs, đặc biệt ở cấp độ giới thiệu, với QLoRA-4bit tăng số lượng test case đã qua trung bình 52% và Pass@5 40%. Tuy nhiên, những cải thiện ít đáng chú ý hơn cho các tác vụ cấp độ phỏng vấn và thi đấu.

6 THẢO LUẬN
Nghiên cứu của chúng tôi khám phá PEFT áp dụng cho LLM mã, làm sáng tỏ tác động tích cực của những ứng dụng này trong việc tinh chỉnh LLM hiệu quả cho các bộ dữ liệu cụ thể tác vụ sinh mã. Đặc biệt, nghiên cứu của chúng tôi minh họa tính thực tiễn của việc tinh chỉnh LLM sử dụng PEFT, do đó giảm bớt sự phụ thuộc của các nhà thực hành vào cơ sở hạ tầng lớn và đắt tiền. Những phát hiện của chúng tôi cũng chỉ ra một số lĩnh vực đầy hứa hẹn cho khám phá tương lai, bao gồm điều tra các kỹ thuật hiệu quả trên các cài đặt tinh chỉnh đa dạng, trong quá trình suy luận, và cho các tác vụ SE khác.

Kỹ thuật hiệu quả cho LLM của mã. Công trình của chúng tôi nhấn mạnh các kỹ thuật tinh chỉnh hiệu quả, dân chủ hóa việc tinh chỉnh LLM cho đối tượng rộng. Tuy nhiên, nghiên cứu của chúng tôi không bao gồm khám phá các kỹ thuật hiệu quả cho suy luận chi phí thấp. Trong khi các kỹ thuật PEFT yêu cầu thời gian tinh chỉnh bổ sung so với ICL và RAG, đáng chú ý là những kỹ thuật này không áp đặt bất kỳ chi phí thời gian bổ sung nào trong quá trình suy luận. Tuy nhiên, chúng tôi thừa nhận sự cần thiết của các điều tra tương lai về các kỹ thuật để giảm chi phí thời gian liên quan đến LLM trong quá trình suy luận.

PEFT và ICL/RAG là các kỹ thuật không loại trừ có thể được sử dụng cùng nhau. Tuy nhiên, chúng tôi quyết định không bao gồm thí nghiệm về việc áp dụng ICL/RAG cho LLM được tinh chỉnh sử dụng PEFT. Trong thực tế, việc tăng số lượng ví dụ ICL/RAG tại thời điểm suy luận đòi hỏi tăng chi phí tính toán khi độ dài token của prompt mở rộng. Do đó, chúng tôi cho rằng việc sử dụng ICL/RAG trên một LLM được tinh chỉnh có thể phản tác dụng làm tăng nhu cầu tính toán, vượt quá những lợi ích tiềm năng.

Từ một góc độ khác, các nghiên cứu trước [18,78,83] nhấn mạnh nhu cầu xem xét các mô hình ngôn ngữ được tiền huấn luyện và LLM của mã trong các cài đặt học liên tục. Trong mô hình này, mô hình phải thích ứng động với dữ liệu mới theo thời gian trong khi bảo toàn hiệu suất trên dữ liệu đã thấy trước đó. Trong cài đặt cụ thể của LLM phát triển liên tục, các kỹ thuật PEFT có khả năng cung cấp những lợi ích có giá trị. Tuy nhiên, vẫn chưa được xác định liệu các kỹ thuật PEFT có thể thích ứng hiệu quả LLM dưới cài đặt học liên tục cho các tác vụ liên quan đến mã, mà không làm tổn hại đến việc giữ lại kiến thức quá khứ.

Hiệu quả của QLoRA. Trên tất cả các bộ dữ liệu nghiên cứu, chúng tôi quan sát rằng QLoRA-4bit thể hiện hiệu quả cạnh tranh hoặc tương đương với các phương pháp PEFT khác. Đáng chú ý, QLoRA-4bit vượt trội LoRA và QLoRA-8bit trên các bộ dữ liệu Conala và APPs. Chúng tôi giả thuyết rằng sự cải thiện này bắt nguồn từ hiệu ứng điều hòa của việc giảm độ chính xác trọng số xuống 4 bit, giúp ổn định tinh chỉnh và giảm thiểu quá khớp. Những phát hiện này làm nổi bật tiềm năng cho các kỹ thuật PEFT hiệu quả hơn, mặc dù cần khám phá thêm để hiểu đầy đủ khả năng áp dụng rộng hơn của chúng.

Phát hiện mới cho PEFT trong kỹ thuật phần mềm. Những phát hiện của chúng tôi trong RQ1 tiết lộ rằng các phương pháp PEFT vượt trội tinh chỉnh đầy đủ cho SLM trong các tác vụ sinh mã. Điều này trái ngược với các nghiên cứu quy mô lớn trước trong NLP, như Ding và cộng sự [14], đã chứng minh tính vượt trội về hiệu quả của tinh chỉnh đầy đủ so với các kỹ thuật như LoRA, Prompt Tuning, và Prefix Tuning trên một loạt rộng các tác vụ NLP.

Trong bối cảnh kỹ thuật phần mềm, trong khi các nghiên cứu trước [38,40] đã cho thấy rằng các phương pháp PEFT, như LoRA, có thể hoạt động tương đương với tinh chỉnh đầy đủ cho SLM, kết quả của chúng tôi đi xa hơn. Chúng tôi cho thấy rằng tất cả các kỹ thuật PEFT được nghiên cứu trong bài báo này vượt trội đáng kể tinh chỉnh đầy đủ cho SLM như CodeGen-350M-mono và CodeT5+-770M trên các bộ dữ liệu Conala và CodeAlpacaPy (xem Bảng 3), làm nổi bật những ưu điểm rõ ràng của PEFT trong những tình huống này. Tuy nhiên, do hạn chế tài nguyên, chúng tôi không thể đánh giá tinh chỉnh đầy đủ cho LLM, để lại không gian cho các nghiên cứu tương lai khám phá điều này sâu hơn trong lĩnh vực kỹ thuật phần mềm.

Ngoài ra, nghiên cứu của chúng tôi khám phá những hiểu biết mới về lợi ích của QLoRA và hiệu quả so sánh của LoRA so với RAG cho các tác vụ sinh mã. Đầu tiên, trong RQ3 và RQ4, chúng tôi chứng minh rằng QLoRA cung cấp hiệu suất tương đương hoặc thậm chí vượt trội so với LoRA trong khi cắt giảm đáng kể chi phí tính toán. Thứ hai, chúng tôi tiết lộ những hạn chế của ICL và RAG, cho thấy rằng hiệu quả LLM có xu hướng đình trệ khi nhiều ví dụ hơn được truy xuất. Ngược lại, nghiên cứu của chúng tôi làm nổi bật những ưu điểm nhất quán của các kỹ thuật PEFT như LoRA và QLoRA trong việc vượt qua những hạn chế này.

Tác vụ SE và đa tác vụ. Để đảm bảo một nghiên cứu tập trung, chúng tôi tránh thêm các tác vụ và bộ dữ liệu bổ sung, ngăn chặn một tập hợp phân tích quá rộng. Khám phá các kỹ thuật PEFT cho LLM trên các tác vụ và bộ dữ liệu đa dạng là một hướng đầy hứa hẹn cho nghiên cứu tương lai. Đặc biệt, Lorahub [26], một khung được giới thiệu gần đây cho học đa tác vụ, chứng minh rằng một sự kết hợp của các module LoRA được huấn luyện trên các tác vụ khác nhau có thể tổng quát hóa cho các tác vụ mới, chưa thấy trong khi cung cấp sự đánh đổi hiệu suất-hiệu quả mạnh mẽ. Chúng tôi tin rằng việc áp dụng các cách tiếp cận tương tự trong AI cho SE có tiềm năng lớn, đặc biệt khi lĩnh vực nghiên cứu nhằm tự động hóa một loạt rộng các tác vụ liên quan đến mã.

7 MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
Tính hợp lệ bên ngoài. Một mối đe dọa chính liên quan đến việc lựa chọn SLM và LLM của chúng tôi. Chúng tôi giảm thiểu mối đe dọa này bằng cách cẩn thận chọn một tập hợp đa dạng các mô hình, như được giải thích trong Phần 4.4. Những mô hình này bao gồm các họ LLM khác nhau, được huấn luyện trên dữ liệu tiền huấn luyện và mục tiêu học riêng biệt, và có kích thước khác nhau. Hơn nữa, chúng tôi không chọn các biến thể mô hình lớn hơn ngoại trừ khi sử dụng QLoRA, vì các kỹ thuật PEFT khác, ICL, và RAG hạn chế việc sử dụng các mô hình lớn hơn trong các ràng buộc tài nguyên của chúng tôi.

Một mối đe dọa bên ngoài khác đối với tính hợp lệ liên quan đến chất lượng và tính đại diện của các bộ dữ liệu tinh chỉnh. Để giảm bớt mối lo ngại này, chúng tôi chọn bộ dữ liệu Conala, chứa các ví dụ chất lượng cao được khai thác từ các bài đăng StackOverflow. Ngoài ra, bộ dữ liệu này đã được sử dụng đại diện bởi nhiều nghiên cứu trước [49,73,91] về các tác vụ sinh mã. Hơn nữa, các tác giả đã làm phong phú mỗi ý định ngôn ngữ tự nhiên với gợi ý, tăng cường sự phù hợp của prompt đầu vào với các ý định con người có thể. Để làm phong phú nghiên cứu của chúng tôi, chúng tôi bao gồm CodeAlpacaPy như một bộ dữ liệu thứ hai bao gồm các ví dụ dài hơn, mang lại một dòng phân tích khác. Chúng tôi không bao gồm các bộ dữ liệu đánh giá như HumanEval [9] và MBPP [3], vì chúng không bao gồm các ví dụ huấn luyện. Tuy nhiên, để mở rộng thêm nghiên cứu, chúng tôi khám phá hiệu quả của LoRA và QLoRA cho sinh mã dựa trên thực thi trên bộ dữ liệu APPs.

Cuối cùng, khía cạnh đơn ngôn ngữ của các bộ dữ liệu tạo thành một mối đe dọa khác đối với tính hợp lệ bên ngoài. Chúng tôi nghiên cứu tinh chỉnh đầy đủ, PEFT, ICL, và RAG cho sinh mã của các đoạn mã Python. Tuy nhiên, chúng tôi dự đoán rằng PEFT cũng áp dụng được cho các ngôn ngữ lập trình khác, xem xét khả năng sinh ấn tượng của LLM trên một loạt đa dạng các ngôn ngữ lập trình [2, 6].

Tính hợp lệ nội bộ. Các lựa chọn siêu tham số cho các phương pháp PEFT tạo thành mối đe dọa chính đối với tính hợp lệ nội bộ. Đối với mỗi kỹ thuật PEFT, chúng tôi sử dụng các giá trị siêu tham số đã được sử dụng trong công trình trước về PEFT cho các mô hình mã cũng như trong các bài báo tiên phong đóng góp các kỹ thuật PEFT. Ngoài ra, vì LoRA với r=16 và α=32 liên tục vượt trội tất cả các cấu hình của ICL và RAG trên ba mô hình hàng đầu của chúng tôi, việc tiến hành phân tích độ nhạy siêu tham số chi tiết của LoRA có thể củng cố thêm ưu thế của PEFT so với ICL và RAG. Công trình tương lai có thể khám phá độ nhạy của các siêu tham số LoRA chính, như thứ hạng r và hệ số tỷ lệ α, trên một loạt rộng hơn các tác vụ kỹ thuật phần mềm.

Tính hợp lệ cấu trúc. Lựa chọn các metrics đánh giá tạo thành mối đe dọa chính đối với tính hợp lệ cấu trúc. Để giảm thiểu mối đe dọa này, chúng tôi chọn các metrics đánh giá được sử dụng rộng rãi trong các công trình trước [22,32,42, 56,72,84] về sinh mã. Hơn nữa, chúng tôi đánh giá mỗi cách tiếp cận sử dụng EM@k trên Conala và CodeAlpacaPy, làm phong phú phân tích của chúng tôi bằng cách tính toán khớp chính xác trên các phạm vi khác nhau của ứng viên mã. Tương tự, đối với APPs, chúng tôi đánh giá mô hình cơ sở và LoRA/QLoRA trên Pass@k với tối đa 5 ứng viên. Cuối cùng, chúng tôi không sử dụng các metrics Pass@k vì các bộ dữ liệu CoNaLa và CodeAlpacaPy không bao gồm unit test. Làm phong phú các bộ dữ liệu với unit test tạo thành một lĩnh vực thú vị của công trình tương lai.

--- TRANG 21 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •21

8 CÔNG TRÌNH LIÊN QUAN
Trong phần này, chúng tôi tổng quan công trình hiện có về LLM cho sinh mã và tương phản các đóng góp trước về thích ứng mô hình hiệu quả của mã cho các tác vụ hạ nguồn với nghiên cứu của chúng tôi.

Sinh mã Tự động. Một phần đáng kể của các kỹ thuật sinh mã [1,4,21,63,72] dựa vào các cách tiếp cận dựa trên học sâu. Xu hướng mới nhất trong sinh mã tự động xoay quanh việc tận dụng LLM như các mô hình GPT [50] do những đột phá đáng kể của chúng trong lĩnh vực này. Một ví dụ đáng chú ý là Codex, được phát triển bởi Chen và cộng sự [9], là một phiên bản được tinh chỉnh của GPT-3. Các mô hình đáng chú ý khác theo sau thành công của Codex bao gồm CodeGen [48], CodeGen2 [47] và CodeLlama [56]. Những LLM này dân chủ hóa hiệu quả hiệu suất đột phá đạt được bởi Codex và đưa nó đến đối tượng rộng hơn. Tuy nhiên, chi phí tính toán cao liên quan đến tinh chỉnh đầy đủ cho LLM để đạt hiệu suất tối ưu là không thực tế cho hầu hết các nhà nghiên cứu và thực hành. Chúng tôi tin rằng nghiên cứu của chúng tôi có thể làm sáng tỏ các cách tiếp cận hiệu quả và tiết kiệm chi phí hơn để tinh chỉnh những LLM này, giảm thiểu gánh nặng tính toán liên quan đến việc áp dụng chúng.

Thích ứng Hiệu quả của Mô hình Mã. Thích ứng hiệu quả của mô hình mã liên quan đến việc sử dụng các kỹ thuật để thích ứng hiệu quả một mô hình với bộ dữ liệu cụ thể tác vụ (xem Phần 2). Trong bối cảnh này, thuật ngữ "hiệu quả" đề cập đến việc làm cho chi phí tính toán tinh chỉnh thấp, ví dụ, sử dụng LoRA, hoặc sử dụng các kỹ thuật không tham số như prompting và ICL.

Hầu hết nghiên cứu trước đã tập trung vào việc sử dụng ICL và prompting để thích ứng mô hình với các tác vụ liên quan đến mã đa dạng. Gao và cộng sự [17] thể hiện những ưu điểm của ICL trong các tác vụ như sửa lỗi, tóm tắt mã, và tổng hợp chương trình. Họ nhấn mạnh rằng hiệu suất của mô hình trên các tác vụ hạ nguồn bị ảnh hưởng bởi nhiều yếu tố, bao gồm việc lựa chọn, số lượng, và thứ tự của các ví dụ prompt. Các nghiên cứu khác [53,80] cũng chứng minh rằng các mô hình ngôn ngữ được tiền huấn luyện và LLM như Codex có thể xử lý hiệu quả sửa lỗi và sửa chữa chương trình tự động sử dụng ICL. Hơn nữa, Geng và cộng sự [19] chứng minh khả năng của Codex trong việc sinh tạo comment đa ý định để mô tả chức năng của một phương pháp hoặc chi tiết thực hiện của nó, chẳng hạn. Việc lựa chọn các prompt có liên quan cho một tác vụ với ICL là quan trọng để đảm bảo hiệu suất tốt của một LLM. Các công trình trước [46,91] thiết kế các kỹ thuật lựa chọn để truy xuất các ví dụ prompt có liên quan cao được điều chỉnh cho các tác vụ hạ nguồn, vượt trội các phương pháp lựa chọn ngẫu nhiên. Cuối cùng, nghiên cứu gần đây [61] nhấn mạnh những ưu điểm của việc truy xuất các ví dụ prompt ở cấp độ kho lưu trữ, cung cấp LLM với thông tin ngữ cảnh có giá trị trong các prompt. Trong nghiên cứu này, chúng tôi tận dụng ICL mà không có ý định khám phá đầy đủ tiềm năng của nó. Thay vào đó, chúng tôi chọn một thực hiện đơn giản của ICL bằng cách chọn các ví dụ few-shot ngẫu nhiên sử dụng các seed khác nhau. Mở rộng nghiên cứu này để kết hợp nhiều cách tiếp cận ICL hơn sẽ tăng cường so sánh với các kỹ thuật PEFT cho mã.

Liên quan đến các kỹ thuật PEFT, nghiên cứu trước trong trí tuệ mã đã tập trung vào Prompt tuning [30], Prefix-tuning [33] và Adapters [20,23,25,57,58]. Wang và cộng sự [68] khởi xướng việc sử dụng Prompt tuning cho các tác vụ liên quan đến mã và chứng minh tính vượt trội của nó so với tinh chỉnh đầy đủ của CodeT5 và CodeBERT trong dự đoán khuyết tật, tóm tắt mã, và dịch mã. Goel và cộng sự [20] khám phá việc sử dụng adapter cụ thể ngôn ngữ lập trình cho chuyển giao kiến thức trong các mô hình ngôn ngữ được tiền huấn luyện, chứng minh rằng việc tinh chỉnh BERT với những adapter này vượt trội CodeBERT trên kiểm tra cloze và phát hiện clone mã. Choi và cộng sự [10] thiết kế một cách tiếp cận Prefix tuning cụ thể mã trong kiến trúc sequence-to-sequence cho các tác vụ sinh. Nghiên cứu của chúng tôi khác biệt với ba công trình trước này vì chúng tập trung vào SLM, trong khi chúng tôi đề xuất nghiên cứu toàn diện đầu tiên về các kỹ thuật PEFT với LLM cho sinh mã. Hơn nữa, nghiên cứu của chúng tôi bao gồm LoRA, IA3, và QLoRA, mà không có công trình trước nào trong trí tuệ mã xem xét để tinh chỉnh hiệu quả LLM của mã. Wang và cộng sự [69] thể hiện tính vượt trội của việc sử dụng Adapters để tinh chỉnh các mô hình ngôn ngữ được tiền huấn luyện so với tinh chỉnh đầy đủ. Các công trình gần đây đã đóng góp các nghiên cứu thực nghiệm cho các tác vụ kỹ thuật phần mềm khác nhau, bao gồm thay đổi mã [40], tóm tắt mã [38,57], dự đoán khuyết tật [38], và phát hiện clone mã [57], sử dụng Adapter tuning và LoRA cho SLM. Nghiên cứu của chúng tôi khác biệt với những công trình trước này, vì chúng tôi tập trung vào LLM. Mặc dù chúng tôi không kết hợp Adapters trong điều tra của chúng tôi, chúng tôi tin rằng LoRA, IA3, Prompt tuning, Prefix tuning, và QLoRA cung cấp một phân tích đủ kỹ lưỡng về các kỹ thuật PEFT. Chúng tôi nhận ra giá trị của việc khám phá các kỹ thuật PEFT bổ sung cho các tác vụ trí tuệ mã khác nhau trong tương lai.

9 KẾT LUẬN VÀ CÔNG TRÌNH TƯƠNG LAI
Nghiên cứu này thiết lập hiệu quả của các kỹ thuật PEFT trong việc tinh chỉnh LLM cho sinh mã. Phân tích so sánh của chúng tôi trên các kỹ thuật hiệu quả tham số khác nhau, bao gồm LoRA, IA3, Prompt tuning, Prefix tuning, và QLoRA, tiết lộ tính vượt trội của PEFT so với tinh chỉnh đầy đủ cho SLM và ICL và RAG cho LLM. Hơn nữa, nghiên cứu của chúng tôi minh họa tính thực tiễn của PEFT dưới tình huống tài nguyên hạn chế, giảm thiểu hiệu quả sự phụ thuộc vào cơ sở hạ tầng tính toán lớn và đắt tiền. Theo hiểu biết tốt nhất của chúng tôi, nghiên cứu này là trong số những khám phá toàn diện đầu tiên về các kỹ thuật PEFT cho LLM trong kỹ thuật phần mềm, gợi ý một con đường đầy hứa hẹn cho nghiên cứu tương lai. Chúng tôi dự đoán những phát hiện của chúng tôi sẽ truyền cảm hứng cho điều tra sâu hơn về việc áp dụng các kỹ thuật PEFT trong kỹ thuật phần mềm, với tác động có thể xa rộng. Công trình tương lai của chúng tôi sẽ mở rộng nghiên cứu sang các tác vụ kỹ thuật phần mềm thay thế như đánh giá mã tự động và sinh comment. Cuối cùng, chúng tôi nhằm xác thực thêm sự liên quan của các kỹ thuật PEFT dưới các cài đặt đa tác vụ và học liên tục cho kỹ thuật phần mềm tự động.

TÀI LIỆU THAM KHẢO
[1]Uri Alon, Roy Sadaka, Omer Levy, và Eran Yahav. 2020. Structural language models of code. Trong International conference on machine learning. PMLR, 245–256.
[2]Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, và cộng sự. 2022. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868 (2022).
[3]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, và cộng sự. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[4]Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, và Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989 (2016).
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[6]Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, và cộng sự. 2023. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering (2023).
[7]Christel Chappuis, Valérie Zermatten, Sylvain Lobry, Bertrand Le Saux, và Devis Tuia. 2022. Prompt-RSVQA: Prompting visual context to a language model for remote sensing visual question answering. Trong Proceedings of the IEEE/CVF

--- TRANG 23 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •23

Conference on Computer Vision and Pattern Recognition. 1372–1381.
[8]Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.com/ sahil280114/codealpaca.
[9]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[10] YunSeok Choi và Jee-Hyong Lee. 2023. CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation. Trong Findings of the Association for Computational Linguistics: ACL 2023. 5282–5297.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, và cộng sự. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).
[12] Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv preprint arXiv:2208.07339 (2022).
[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 (2023).
[14] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, và cộng sự. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904 (2022).
[15] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, và cộng sự. 2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence 5, 3 (2023), 220–235.
[16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và cộng sự. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).
[17] Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, và Michael R Lyu. 2023. Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study. arXiv preprint arXiv:2304.07575 (2023).
[18] Shuzheng Gao, Hongyu Zhang, Cuiyun Gao, và Chaozheng Wang. 2023. Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models. arXiv preprint arXiv:2302.03482 (2023).
[19] Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, và Xiangke Liao. 2024. Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning. (2024).
[20] Divyam Goel, Ramansh Grover, và Fatemeh H Fard. 2022. On the cross-modal transfer from natural language to code through adapter modules. Trong Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. 71–81.
[21] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, và Graham Neubig. 2018. Retrieval-based neural code generation. arXiv preprint arXiv:1808.10025 (2018).
[22] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, và Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. NeurIPS (2021).
[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. Trong International Conference on Machine Learning. PMLR, 2790–2799.
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).
[25] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, và Soujanya Poria. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint arXiv:2304.01933 (2023).
[26] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, và Min Lin. 2023. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269 (2023).
[27] Harshit Joshi, José Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, và Ivan Radiček. 2023. Repair is nearly generation: Multilingual program repair with llms. Trong Proceedings of the AAAI Conference on Artificial Intelligence,

--- TRANG 24 ---
24 •M. Weyssow và cộng sự.

Tập 37. 5131–5140.
[28] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, và Harm de Vries. 2022. The Stack: 3 TB of permissively licensed source code. Preprint (2022).
[29] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199–22213.
[30] Brian Lester, Rami Al-Rfou, và Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 3045–3059.
[31] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, và cộng sự. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[32] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, và Xing Hu. 2023. Skcoder: A sketch-based approach for automatic code generation. arXiv preprint arXiv:2302.06144 (2023).
[33] Xiang Lisa Li và Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).
[34] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, và Meishan Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive Learning. arXiv:2308.03281 [cs.CL] https://arxiv.org/abs/2308.03281
[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, và cộng sự. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).
[36] W Liang, M Yuksekgonul, Y Mao, E Wu, và J Zou. 2023. GPT detectors are biased against non-native English writers (arXiv: 2304.02819). arXiv.
[37] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems 35 (2022), 1950–1965.
[38] Jiaxing Liu, Chaofeng Sha, và Xin Peng. 2023. An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models. Trong 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 397–408.
[39] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, và Yang Liu. 2020. Retrieval-augmented generation for code summarization via hybrid gnn. arXiv preprint arXiv:2006.05405 (2020).
[40] Shuo Liu, Jacky Keung, Zhen Yang, Fang Liu, Qilin Zhou, và Yihan Liao. 2024. Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study. arXiv preprint arXiv:2402.06247 (2024).
[41] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, và Alexey Svyatkovskiy. 2022. Reacc: A retrieval-augmented code completion framework. arXiv preprint arXiv:2203.07722 (2022).
[42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, và cộng sự. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).
[43] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, và Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https://github.com/huggingface/peft.
[44] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, và Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. Comput. Surveys (2021).
[45] Sewon Min, Mike Lewis, Luke Zettlemoyer, và Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 (2021).
[46] Noor Nashid, Mifta Sintaha, và Ali Mesbah. 2023. Retrieval-based prompt selection for code-related few-shot learning. Trong Proceedings of the 45th International Conference on Software Engineering (ICSE'23).
[47] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, và Yingbo Zhou. 2023. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).
[48] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv:2203.13474 [cs.LG]

--- TRANG 25 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •25

[49] Sajad Norouzi, Keyi Tang, và Yanshuai Cao. 2021. Code generation from natural language with less prior knowledge and more monolingual data. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 776–785.
[50] R OpenAI. 2023. GPT-4 technical report. arXiv (2023), 2303–08774.
[51] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730–27744.
[52] Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 (2021).
[53] Julian Aron Prenner, Hlib Babii, và Romain Robbes. 2022. Can OpenAI's codex fix bugs? an evaluation on QuixBugs. Trong Proceedings of the Third International Workshop on Automated Program Repair. 69–75.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, và cộng sự. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.
[55] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, và Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[56] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, và cộng sự. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).
[57] Iman Saberi, Fatemeh Fard, và Fuxiang Chen. 2024. Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering. Empirical Software Engineering 29, 4 (2024), 94.
[58] Iman Saberi và Fatemeh H Fard. 2023. Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models. arXiv preprint arXiv:2303.06233 (2023).
[59] Zhenwei Shao, Zhou Yu, Meng Wang, và Jun Yu. 2023. Prompting large language models with answer heuristics for knowledge-based visual question answering. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14974–14983.
[60] Noam Shazeer và Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. Trong International Conference on Machine Learning. PMLR, 4596–4604.
[61] Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. 2023. Repository-level prompt generation for large language models of code. Trong International Conference on Machine Learning. PMLR, 31693–31715.
[62] Emma Strubell, Ananya Ganesh, và Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).
[63] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, và Lu Zhang. 2020. Treegen: A tree-based transformer architecture for code generation. Trong Proceedings of the AAAI Conference on Artificial Intelligence, Tập 34. 8984–8991.
[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, và cộng sự. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[65] Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, và cộng sự. 2023. Efficient methods for natural language processing: A survey. Transactions of the Association for Computational Linguistics 11 (2023), 826–860.
[66] Priyan Vaithilingam, Tianyi Zhang, và Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. Trong Chi conference on human factors in computing systems extended abstracts. 1–7.
[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[68] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, và Michael R Lyu. 2022. No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence. Trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 382–394.
[69] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, và Xiangke Liao. 2023. One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. arXiv preprint arXiv:2303.15822 (2023).

--- TRANG 26 ---
26 •M. Weyssow và cộng sự.

[70] Weishi Wang, Yue Wang, Shafiq Joty, và Steven CH Hoi. 2023. Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair. Trong Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 146–158.
[71] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).
[72] Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[73] Zhiruo Wang, Shuyan Zhou, Daniel Fried, và Graham Neubig. 2022. Execution-Based Evaluation for Open-Domain Code Generation. arXiv preprint arXiv:2212.10481 (2022).
[74] Albert Webson và Ellie Pavlick. 2021. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247 (2021).
[75] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[76] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, và cộng sự. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).
[77] Martin Weyssow, Houari Sahraoui, và Bang Liu. 2022. Better modeling the programming world with code concept graphs-augmented multi-modal learning. Trong Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 21–25.
[78] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, và Houari Sahraoui. 2023. On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. arXiv preprint arXiv:2305.04106 (2023).
[79] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, và cộng sự. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).
[80] Chunqiu Steven Xia, Yuxiang Wei, và Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. Trong Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery.
[81] Chunqiu Steven Xia và Lingming Zhang. 2022. Less training, more repairing please: revisiting automated program repair via zero-shot learning. Trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 959–971.
[82] Frank F. Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. 2022. A Systematic Evaluation of Large Language Models of Code (MAPS 2022). Association for Computing Machinery, New York, NY, USA, 1–10. https: //doi.org/10.1145/3520312.3534862
[83] Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, và cộng sự. 2023. Exploring Continual Learning for Code Generation Models. arXiv preprint arXiv:2307.02435 (2023).
[84] Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Tingting Han, và Taolue Chen. 2023. ExploitGen: Template-augmented exploit code generation based on CodeBERT. Journal of Systems and Software 197 (2023), 111577.
[85] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, và Mark Yatskar. 2023. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19187–19197.
[86] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, và Graham Neubig. 2018. Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow. Trong Proceedings of the 15th International Conference on Mining Software Repositories (Gothenburg, Sweden) (MSR '18). Association for Computing Machinery, New York, NY, USA, 476–486. https://doi.org/10.1145/3196398.3196408
[87] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, và Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. Trong 2018 IEEE/ACM 15th international conference on mining software repositories (MSR). IEEE, 476–486.
[88] Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, và Yiling Lou. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv preprint arXiv:2308.01240 (2023).

--- TRANG 27 ---
Khám phá các Kỹ thuật Tinh chỉnh Hiệu quả Tham số cho Sinh mã với Mô hình Ngôn ngữ Lớn •27

[89] Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, và Liwen Jing. 2023. Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media. arXiv preprint arXiv:2304.03087 (2023).
[90] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, và Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. Trong International Conference on Machine Learning. PMLR, 12697–12706.
[91] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, và Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. Trong The Eleventh International Conference on Learning Representations.
[92] Xin Zhou, DongGyun Han, và David Lo. 2021. Assessing generalizability of codebert. Trong 2021 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 425–436.

, Tập 1, Số 1, Bài viết. Ngày xuất bản: Tháng 12 2024.
