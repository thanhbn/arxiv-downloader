# 2303.13635.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.13635.pdf
# File size: 1187172 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Low Rank Optimization for Efﬁcient Deep
Learning: Making A Balance between Compact
Architecture and Fast Training
Xinwei Ou, Zhangxin Chen, Ce Zhu, Fellow, IEEE , and Yipeng Liu, Senior Member, IEEE
Abstract —Deep neural networks have achieved great success in
many data processing applications. However, the high computa-
tional complexity and storage cost makes deep learning hard to be
used on resource-constrained devices, and it is not environmental-
friendly with much power cost. In this paper, we focus on low-
rank optimization for efﬁcient deep learning techniques. In the
space domain, deep neural networks are compressed by low
rank approximation of the network parameters, which directly
reduces the storage requirement with a smaller number of
network parameters. In the time domain, the network parameters
can be trained in a few subspaces, which enables efﬁcient
training for fast convergence. The model compression in the
spatial domain is summarized into three categories as pre-train,
pre-set, and compression-aware methods, respectively. With a
series of integrable techniques discussed, such as sparse pruning,
quantization, and entropy coding, we can ensemble them in an
integration framework with lower computational complexity and
storage. Besides of summary of recent technical advances, we
have two ﬁndings for motivating future works: one is that the
effective rank outperforms other sparse measures for network
compression. The other is a spatial and temporal balance for
tensorized neural networks.
Index Terms —model compression, subspace training, effective
rank, low rank tensor optimization, efﬁcient deep learning.
I. I NTRODUCTION
DEEP neural networks (DNNs) have been widely used in
many data processing applications, such as speech recog-
nition, computer vision [70], [122], [63], natural language
processing [134], [43], etc.As a deeper or wider structure can
lead to better performance, DNNs are gradually characterized
by their over-parameterization. Over-parameterization, on the
other hand, suggests too much redundancy in DNNs, which
leads to overﬁtting [53], [29]. There are mainly two challenges
in deep learning: high complexity and slow convergence. The
high complexity means that there are millions of parameters
in DNNs, and computation between massive parameters and
inputs is cumbersome, which underlines the need for efﬁ-
cient algorithms to compress and accelerate. For example,
the number of parameters in VGG-16 [122] is almost seven
million. For an image in ImageNet dataset [70] with a size
of 224×224×3, the feedforward process requires 30.9 billion
ﬂoat point-operations (FLOPs). The high complexity is unaf-
fordable for resource-limited devices, such as mobile phones
All the authors are with the School of Information and Communication
Engineering, University of Electronic Science and Technology of China
(UESTC), Chengdu 611731 China (e-mails: xinweiou@std.uestc.edu.cn,
fzhangxinchen, eczhu, yipengliu g@uestc.edu.cn).
Manuscript received ×× ××, 2023; revised ×× ××, 2023.[65] and IoT devices [73]. The slow convergence is caused by
the conventional back propagation (BP) algorithm, resulting
in time-consuming training [1]. Also, the convergence speed
is sensitive to the setting of the learning rate and the way to
initialize weights.
There are many works attempting to reduce the high com-
plexity of DNNs with acceptable performance decay. The in-
vestigation of model compression can be summarized into two
folds: one is to reduce the number of parameters, and the other
is to reduce the average bit width of data representation. The
ﬁrst fold includes but is not limited to low rank approximation
[75], [62], [137], [92], pruning [99], [150], weight-sharing
[131], sparsity [57], knowledge distillation [48]. Since these
techniques have their own limitations, it is better to combine
them to exploit the redundancy in DNNs fully. Quantization
[40], [142] and entropy coding [48] belong to the second
category, which is designed to achieve a lower number of bits
per parameter.
Low rank approximation has been widely adopted due to
its strong theoretical basis and ease of implementation on
hardware. In this survey, we comprehensively review this
rapidly developing area by dividing low rank optimization
for model compression into three main categories: pre-train
method, pre-set method, and compression-aware method. The
biggest distinction among them is the way to train. The pre-
train method directly decomposes a pre-trained model to get
warm initialization for the compressed format, followed by
retraining the compressed model to recover the performance.
Without pre-training, the pre-set method trains a network that
is pre-set to a compact format from scratch. Totally different
from the above two methods, the compression-aware method
explicitly accounts for compression in the training process by
gradually enforcing the network to enjoy low-rank structure.
Although the discussion about low rank optimization can also
be found in [136], we further investigated how to integrate it
with other compression techniques to pursue lower complexity
and recommended the effective rank as the most efﬁcient
measure used in low rank optimization.
When the redundancy in DNNs is exploited by subspace
training, DNNs can converge faster without losing accuracy.
In deep learning, it is conventional to train networks with ﬁrst-
order optimization methods, e.g.SGD [118], which is com-
putationally cheap. But there are some inherent drawbacks to
ﬁrst-order optimization methods, such as slow theoretical and
empirical convergence. Second-order methods can deal with
such a problem well, but because of the heavy computationalarXiv:2303.13635v1  [cs.LG]  22 Mar 2023

--- PAGE 2 ---
2
burden of Hessian matrices, second-order methods are not
applicable to DNNs. The idea that projecting parameters onto
a tiny subspace represented by several independent variables
is an effective way to solve this problem. Since only a few
variables need to be optimized, we can apply second-order
optimization methods to achieve the temporal efﬁciency of
deep learning.
In this survey, we ﬁrst present a comprehensive overview
of various tensor decomposition methods applicable to model
compression. Next, the low rank optimization for model
compression is summarized in terms of pre-set, pre-train,
and compression-aware methods. For each method, a detailed
discussion on key points about implementation is given. More
meticulously, we make a comparison among various sparsity
measures used in the compression-aware method, and dig out
the most efﬁcient measure, i.e.effective rank, which is seldom
used as a sparse regularizer before. In addition, while there are
already many works that give a list of joint-way compression
[28], [21], little attention has been paid to the integration
between low rank approximation and other compression tech-
niques. Therefore, we present an overall survey on this kind
of integration here. Then, we introduce low rank optimization
for subspace training. Furthermore, we are the ﬁrst to relate
these two types of low rank optimization, discovering that
redundancy in the temporal domain and spatial domain are
of the same origin. And there is a discussion on how to apply
subspace training on tensorized neural networks to achieve
spatial efﬁciency and temporal efﬁciency simultaneously. The
overall mind map to achieve efﬁcient deep learning through
low rank optimization is shown in Fig. 1.
Different from the previous surveys on tensors for efﬁcient
deep learning [92], [88], [90], the main contributions of this
paper can be summarized as follows.
1) We make a detailed overview of low rank approximation
for model compression, and we ﬁnd that RNNs can be
effectively compressed using Hierarchical Tucker (HT)
decomposition and Kronecker Product Decomposition
(KPD), CNNs can be effectively compressed using Tensor
Train (TT) and Generalized Kronecker Product Decom-
position (GKPD), while Tensor Ring (TR) and Block
Term Decomposition (BTD) can suitably compress both
RNNs and CNNs.
2) A series of integratable neural network compression tech-
niques have been discussed in details, and an integration
framework is summarized to well take advantage of
various methods.
3) We analyse that the redundancy in the space domain and
time domain are of the same origin. In order to accelerate
the training of tensorized neural networks, we should
make the balance between spatial efﬁciency and temporal
efﬁciency.
4) After discussing and testing various sparse measures for
low rank optimization for deep neural network com-
pression, the effective rank outperforms in numerical
experiments.
This survey is organized as follows. In Section II, we give
an overview of low rank optimization for model compression.Low rank approximation integrated with other compression
techniques is reviewed in Section III. Section IV introduces
low rank optimization for subspace training and analyses the
coupling between these two types of low rank optimization.
II. L OWRANK OPTIMIZATION FOR MODEL COMPRESSION
Since DNNs are over-parameterized, there are opportunities
to make deep networks more compact. Compression meth-
ods, like quantization, pruning and low-rank approximation,
can lower complexity of DNNs without signiﬁcant accuracy
degradation. Among them, low rank approximation has been
widely adopted because of the solid theoretical basis of
tensor decomposition. In this section, we will ﬁrst introduce
various tensor decomposition methods applicable for network
compression, and then divide optimization methods for low
rank approximation into three categories: pre-train, pre-set,
and compression-aware methods. In addition, we make a
discussion on efﬁcient sparsity measures.
A. Tensor Decomposition
Low rank approximation can provide an ultra-high compres-
sion ratio for recurrent neural network (RNN) with insigniﬁ-
cant accuracy loss. However, when it comes to convolutional
neural network (CNN), the compression performance isn’t
as satisfying as RNN. In early literatures, 4D convolutional
kernels are reshaped into matrices and singular value decom-
position (SVD) is utilized to decompose matrices into two
factors [151]. However, the reshaping operation leads to dis-
tortion of structure information. Hence, more efﬁcient tensor
decomposition has attracted interests. Canonical-Polyadic (CP)
decomposition [92] is applied to decompose a convolutional
layer into four consecutive convolutional layers, signiﬁcantly
speeding up CNNs [75]. Tucker decomposition [129] can
decompose the 4D kernel into a 4D compact kernel and two
matrices by exploiting the channel-wise redundancy. Based
on these three classic decompositions, there are many other
ﬂexible methods emerged including HT [42], TT [108], TR
[153], BTD [26], GKPD [46], Semi-tensor Product (STP)-
based Tensor Decomposition [152], which dramatically im-
prove the compression performance for DNNs. Table I shows
the performance of widely-used tensor decomposition methods
applied to compress ResNet32 with Cifar10 dataset.
TABLE I
COMPARISON OF COMPRESSION PERFORMANCE OF ADVANCED TENSOR
DECOMPOSITION METHODS ON RESNET32WITH CIFAR 10DATASET
Method Top-1 Accuracy (%) Compression Ratio
Tucker [65] 87.70 5×
TT [37] 88.3 4.8×
TR [137] 90.6 5×
BTD [146] 91.1 5×
GKPD [46] 91.5 5×
HT [141] 89.9 1.6×
STT [152] 91.0 9×
Here, we outline some key notations. For a fully-connected
(FC) layer, we let W2ROIdenote the weight matrix of this
layer, where IandOrepresent the number of input neurons

--- PAGE 3 ---
3
 Spatial Efficiency Low Rank Approximation Pre-train Method
 Pre-set Method
 Compression-aware Method
 Integratable Techniques Parallel Integration Pruning
 Sparsification
 Weight Sharing
 Knowledge Distillation
 Orthogonal Integration Quantization
 Entropy Coding
 Temporal Efficiency  Subspace Training Make A Balance
Fig. 1. Overview of low rank tensor optimization for efﬁcient deep learning.
and output neurons, respectively. And for a convolutional
(Conv) layer, we let K 2RSCHWdenote the weight
of the convolutional kernel, where S,Care the number of
ﬁlters and input channels, H,Ware the height and width of
the kernel. In some cases, we need to reshape a tensor into
a higher-order one. We assume that I1I2Id=I,
O1O2Od=O,C1C2Cd=C, and
S1S2Sd=S. Some necessary mathematical operators
are listed in Table II.
TABLE II
NOTATIONS USED IN THIS PAPER .
Notations Descriptions
diag()generate a diagonal matrix by taking
the input vector as the main diagonal

 Kronecker product
 vector outer product
n n-mode product
n semi-tensor product
Base on these deﬁned notation, we can make a comparison
among various state-of-art tensor decompositions on their
ability to compress and accelerate. When aiming at FC layers,
the comparison is shown in Table III. And Table IV is for
Conv layers.
1) Singular Value Decomposition
For a given matrix X2RMN, its SVD can be written as
X=Udiag(s)VT: (1)
LetRdenote the rank of the matrix, RminfM;Ng. Note
thatU2RMNandV2RNRsatisfy UUT=Iand
VVT=I, respectively. And the elements of s2RRdecrease
from ﬁrst to end, i.e.s1s2::sR.TABLE III
COMPARISON AMONG FULLY -CONNECTED LAYER COMPRESSED BY TT,
TR, HT, BTD, STR, AND KPD ON COMPUTATION COSTS AND STORAGE
CONSUMPTION . NOTE THAT Im= max k2f1;;dgIk,
Om= max k2f1;;dgOk,d= 2 FOR KPD, rIS THE MAXIMAL RANK ,R
IS THE CP RANK OF BTD, ANDtIS THE RATIO BETWEEN CONNECTED
DIMENSIONALITY .
Method Computation Storage
FCO(IO) O(IO)
TTO(dImmax(I; O)r2)O(dImOmr2)
TRO(d(I+O)r3)O(d(Im+Om)r2)
HTO(dmin(I; O)(r3+Imr2))O(dImOmr+dr3)
BTDO(dImmax(I; O)rdR)O((dImOmr+rd)R)
STRO(d(I+O)r3=t)O(d(Im+Om)r2=t2)
KPDO(IOm+OIm)O(ImOm)
Since the format of weights in FC layers is a natural matrix,
SVD can be directly utilized. By using SVD, the FC layer can
be approximated by two consecutive layers. The weight of the
ﬁrst and second layer can be represented by B=diag(ps)VT
andA=Udiag(ps), respectively. For Conv layers, the 4D
kernel should be reshaped into a 2D matrix ﬁrst. By exploiting
different types of redundancy, there are two decomposition
schemes, one reshapes Winto aS-by-CHW matrix, namely
channel-wise decomposition [151], the other called spatial-
wise decomposition [62] gets a SH-by-CW matrix. Then,
compute SVD of the reshaped matrix. Similar to the process
of compressing FC layers, two Conv layers represented by
tensors reshaped from factors BandAcan be used to replace
the original layer.
However, both methods only can exploit one type of re-
dundancy. Moreover, there is also redundancy between input
channels. Exploiting all kinds of redundancy at the same time
can help us achieve a much higher compression ratio, which

--- PAGE 4 ---
4
TABLE IV
COMPARISON AMONG CONVOLUTIONAL LAYER COMPRESSED BY TT, TR, HT, BTD, STR, GKPD ON COMPUTATION COSTS AND STORAGE
CONSUMPTION . NOTE THAT Cm= max k2f1;;dgCk,Sm= max k2f1;;dgSk,d= 2 FOR GKPD, k= max( k1; k2)WITH k1k2=K,rIS THE
MAXIMAL RANK ,RIS THE CP RANK OF BTD, MANDNARE THE HEIGHT AND WIDTH OF FEATURE MAP ,ANDtIS THE RATIO BETWEEN CONNECTED
DIMENSIONALITY .
Method Computation Storage
ConvO(SCK2MN) O(SCK2)
TTO(drmax(rCm; K2) max( C; S)MN)O(dCmSmr2+K2r)
TRO(r3(C+S) + (r3K2+r2(C+S))MN)O((dCmSm+K2)r2)
HTO(log2dCS(r3+r2) +SCK2MN)O(dCmSmr+K2r+dr3)
BTDO((K2r2+ (C+S)r)RMN ) O((K2r2+ (I+O)r)R)
STRO(r3
t3(C+S) + (r3K2+r2
t(C+S))MN)O((dCmSm
t2+K2)r2)
GKPDO(r(CmS+SmC)k2MN) O(rCmSmk2)
can be achieved by tensor decomposition.
2) CP Decomposition
While SVD factorizes a matrix into a sum of rank-one
matrices, CP decomposition factorizes a tensor into a sum of
rank-one tensors. For a Nth order tensor,X2RI1I2IN,
the CP decomposition can be formulated as:
X=h h
;A(1);A(2);;ANi i
=RX
r=1ra(1)
ra(2)
ra(N)
r:
(2)
Eacha(n)
rrepresents the rth column of A(n)and2RR
represents the signiﬁcance of Rcomponents. The rank of the
tensorX, denoted by R, is deﬁned as the smallest number of
rank-one tensors [88], [94].
When using CP to compress FC layers, the weight ma-
trixWshould be ﬁrstly tensorized into a 2dth order ten-
sorW 2RO1O2OdI1I2Id. Meanwhile, the input
vector x2RIshould be presented as a dth order tensor
X 2RI1I2Id. For convolutional kernels, by directly
performing CP on the 4-D kernel tensor, the layer will be
approximated by four consecutive convolutional layers whose
weights are represented by four factor matrices, respectively.
3) Tucker Decomposition
The Tucker decomposition can be considered as a higher-
order generalization of principal component analysis (PCA). It
represents an N-th order tensor with a Nth order core tensor
multiplied by a basis matrix along each mode. Thus, for X2
RI1I2IN, we have
X=G 1A(1)2A(2)3NA(N); (3)
WhereG2RR1R2RNis called core tensor. Here, ” n”
represents the n-mode product, i.e.multiply a tensor by a
matrix in mode n. Elementwise, ” n” can be formulated as:
(G 1A(1))i1;r2;;rN=R1X
r1=1Gr1;r2;;rNA(1)
i1;r1:(4)
Columns of the factor matrix A(n)2RInRncan be consid-
ered as the principal components of the nth mode. The core
tensorGcan be viewed as a compressed version of Xor the
coefﬁcient in the low dimensional subspace. In this case, we
can say thatXis a rank-(R1;R2;;RN) tensor [88], [94].
In the case of compressing FC layers, similar to CP, the
same tensorization for weights and input is needed, followedby directly performing Tucker decomposition on the 2dth order
tensor. For Conv layers, since the spatial size of the kernel is
too small, we can just use Tucker2 [130] to take advantage
of redundancy between ﬁlters and between input channels,
generating 11 convolution, HWconvolution, and 1 1
convolution.
4) Block Term Decomposition
Block Term Decomposition (BTD) was introduced in [26]
as a more powerful tensor decomposition, which combines the
CP decomposition and Tucker decomposition. Consequently,
BTD is more robust than the original CP and Tucker de-
composition. While CP approximates a tensor with a sum
of rank-one tensors, BTD is a sum of tensors in low rank
Tucker format. Or, by concatenating factor matrices in each
mode and arranging all the core tensors of each subtensor
into a block diagonal core tensor, BTD can be considered as
an instance of Tucker. Hence, consider a Nth order tensor,
X2RI1I2Id, its BTD can be written as:
X=NX
n=1Gn1A(1)
n2A(2)
n3dA(d)
n: (5)
In this formula, Ndenotes the CP rank, i.e.the number of
block terms, and Gn2RR1R2Rdis the core tensor
of then-th block term with multilinear ranks that equals
(R1;R2;;Rd).
When BTD is applied to compress an FC layer, the yielded
compact layer is called Block Term Layer (BTL) [146]. In
the BTL, the input tensor X 2RI1I2Idis tensorized
from the original input vector x2RIand the original weight
matrix Wis reshaped asW 2RO1I1O2I2OdId.
Then, we can factorize Wby BTD with factor tensors fA(d)
n2
ROdIdRdgd
n=1. By conducting a tensor contraction operator
between BTD(W) andX, the output tensorY2RO1O2Od
comes out, which can be vectorized as the ﬁnal output vector.
For Conv layers, it is claimed in [146] that by reshaping the
4D kernel into a matrix, W2RSCHW, the layer can be
transformed into BTL. Speciﬁcally speaking, the matrix should
be further reshaped as 1H1WS1C1S2C2
SdCd.
5) Hierarchical Tucker Decomposition
Hierarchical Tucker (HT) decomposition is a hierarchical
variant of the Tucker decomposition, which iteratively rep-
resents a high-order tensor with two lower-order subtensors

--- PAGE 5 ---
5
 
𝑈𝑈12⋯𝑑𝑑 
𝑈𝑈12⋯𝑑𝑑/2 
𝑈𝑈𝑑𝑑/2⋯𝑑𝑑  
⋯ 
𝑈𝑈1 
𝑈𝑈2 
𝑈𝑈3 
𝑈𝑈4 
𝑈𝑈𝑑𝑑−1 
𝑈𝑈𝑑𝑑 
 ⋯ 
Fig. 2. HT decomposition
and a transfer matrix via taking advantage of Tucker decom-
position [42], [95]. For a tensor X 2RI1I2IN, we
can simply divide the index set f1;2;;Nginto two sub-
sets, i.e.T=ft1;t2;;tkg;S=fs1;s2;;sN kg. Let
U12N2RIt1It2ItkIs1Is2IsN k1denotes the matrix re-
shaped fromX, and truncated matrices Ut2RIt1It2ItkRt,
Us2RIs1Is2IsN kRsrepresent the corresponding column
basis matrix of two subspaces. Then, we could have:
U12N= (Ut
Us)B12N; (6)
where B12N2RRtRs1is termed as transfer matrix
and “
” denotes the Kronecker product between two ma-
trices. Subsequently, divide the set Tinto two subsets L=
fl1;l2;;lqgandV=fv1;v2;;vk qg. We can represent
UtwithUl2RIl1Il2IlqRl,Uv2RIv1Iv2Ivk qRv, and
Bt2RRlRvRtas:
Ut= (Ul
Uv)Bt: (7)
The similar factorization procedure applies simultaneously to
Us. By repeating this procedure until the index set cannot be
divided, we can eventually obtain the treelike HT format of
the target tensor. An illustration of a simple version of HT can
be seen in Fig. 2.
Since the Kronecker product in Eq. (7) is computationally
expensive, there are other concise forms of HT, such as the
contracted form introduced in [141]. This form merges index
subsets to the universal set from bottom to top. In this form,
an external input can be contracted with each transfer matrix
and truncated matrix one by one. This way can avoid the
memory and computation-consuming weight reconstruction
procedure and intermediate outputs will not be too large to
out of memory.
For the realization of compressing FC layers by HT,
the weight matrix should be transformed into W 2
R(I1O1)(I2O2) (IdOd), and the input data is tensorized
intoX2RI1I2Id. For reducing computation complex-
ity, the chain computation shown in Fig. 3 is applied. However,
as there is no law associating convolution and contraction, the
kernel of Conv layers must be recovered from the HT format.
By the way, in order to keep balance, the 4D kernel should
be tensorized into W2R(HW)(C1S1)(C2S2) (CdSd).
 
  ℬ12 
𝑈𝑈1 𝑈𝑈2 ℬ1234  
𝑈𝑈3 𝑈𝑈4 ℬ34 
𝒳𝒳 Fig. 3. The chain computation for a 4th order case. Xrepresents a 4th order
input. These arrows represent the order of contraction.
 
  
𝑅𝑅0 
𝑅𝑅1 
 𝑅𝑅2 
𝑅𝑅3 
𝑅𝑅4 
𝐼𝐼2 
 𝐼𝐼3 
𝐼𝐼4 
 𝐼𝐼1 
Fig. 4. A 4th order tensor in TT format.
6) Tensor Train Decomposition
Tensor Train (TT) is a special case of HT, which is a
degenerate HT format [108], [93]. TT factorizes a high-order
tensor into a collection of 3rd or 2nd-order tensors. These core
tensors are connected by the contraction operator. Assume that
we have aNth order tensor,X2RI1I2IN, elementwise,
we can factorize it into TT format as:
Xi1;i2;;iN=X
r1;r2;;rNG1
i1;r1G2
r1;i2;r2GN
rN 1;iN;(8)
where the collection of fGn2RRn 1InRngN
n=1withR0=
1andRN= 1 is called TT-cores [108]. The collection of
ranksfRngN
n=0is called TT-ranks. Fig. 4 gives an illustration
of a 4th order tensor represented in TT format.
The TT was ﬁrst applied to compress FC layers in [105],
where the weight matrix is reshaped into a high order tensor,
W2R(I1O1)(I2O2) (IdOd). After representing Win TT
format, the resulted TT-cores fGn2RRn 1InOnRngN
n=1
can directly be contracted with the tensorized input. It was
suggested in [141] that TT is more efﬁcient for compressing
Conv layers than HT, while HT is more suitable for com-
pressing FC layers whose weight matrix is more prone to be
reshaped into a balanced tensor.
Employing TT on Conv layers is introduced in [37], where
the 4D kernel tensor should be reshaped to size of (HW)
(C1S1)(C2S2) (CdSd)and the input feature maps
are reshaped toX 2RHWC1Cd. In the feedforward
phase, the tensorized input Xwill be contracted with each
TT-core one by one. Although TT can signiﬁcantly save
storage costs, the computational complexity may be higher
than the original Conv layer. Hence, HODEC (High-Order
Decomposed Convolution) was proposed in [149] to enable
simultaneous reductions in computational and storage costs,
which further decomposes each TT-cores into two 3rd-order
tensors.
7) Tensor Ring Decomposition
Due to the disunity of edge TT-cores, there is still an open
issue that how to arrange dimensions of tensors to ﬁnd the
optimal TT format. To conquer this problem, Tensor Ring (TR)

--- PAGE 6 ---
6
 
  
𝐼𝐼3 
𝐼𝐼2 
 𝐼𝐼1 
𝐼𝐼4 
𝑅𝑅1 
𝑅𝑅2 
𝑅𝑅3 
𝑅𝑅4 
Fig. 5. A 4th order tensor in TR format.
decomposition was proposed to perform a circular multilinear
product over cores [153], [56], [87], [97]. Consider a given
tensor,X 2RI1I2IN, elementwise, we can formulate
its TR representation as:
Xi1;i2;;iN=X
r1;r2;;rNG1
r1;i1;r2G2
r2;i2;r3GN
rN;iN;r1
=tr(X
r2;;rNG1
:;i1;r2G2
r2;i2;r3GN
rN;iN;:);(9)
where all coresfGn2RRnInRn+1gN
n=1withRN+1=R1
are called TR-cores. Its tensor diagram for a 4th order tensor
is illustrated in Fig. 5. This form is equivalent to the sum
ofR1TT format. Thanks to the circular multilinear product
gained by employing the trace operation, TR treats all the cores
equivalently and becomes much more powerful and general
than TT.
Moreover, due to the circular strategy, TR amends the
variousness of gradients in TT. Hence, TR is also suitable
for compressing FC layers. In [137], TR was ﬁrst applied to
compress DNNs. Speciﬁcally speaking, the weight matrix of
FC layers should be reshaped into a 2 dth order tensor of size
I1IdO1O2, followed by representing
the tensor into TR format. For the feedforward process,
ﬁrstly, merge the ﬁrst dcores and the last dcores to obtain
F12RR1I1IdRd+1andF22RRd+1O1OdR1,
respectively. Then, we can calculate contraction between input
X 2RI1I2IdandF1, yielding a matrix that can be
contracted withF2. The ﬁnal output tensor will be of size
O1O2Od. For Conv layers, if keeping the kernel
tensor in 4th order and maintaining the spatial information, its
TR-format can be formulated as:
Ks;c;h;w =R1X
r1=1R2X
r2=1R3X
r3=1Ur1;s;r 2Vr2;c;r 3Qr3;h;w;r 1:(10)
Hence, the original layer can be represented by three consecu-
tive layers whose weight tensors are V,QandUrespectively.
If a higher compression ratio is needed, we can further view
UandVas tensors merged from dcore tensors respectively,
with an extra computation burden of merging.
8) Generalized Kronecker Product Decomposition
Kronecker Product Decomposition (KPD) can factorize a
matrix into two smaller factor matrices interconnected by
Kronecker product, which has shown to be very effective when
applied to compress RNNs [128]. To further compress Conv
layers, it was generated to Generalized Kronecker ProductDecomposition (GKPD) [46], which represents a tensor by the
sum of multidimensional Kronecker product between two fac-
tor tensors. Formally, the multidimensional Kronecker product
betweenA 2RJ1J2JNandB 2RK1K2KNis
formulated as:
(A
B )i1;i2;;iN=Aj1;j2;;jNBk1;k2;;kN; (11)
wherejn=bin=Kncandkn=inmodKn. Based on this, for
a givenNth order tensorX2RJ1K1J2K2JNKN, GKPD
can be denoted as:
X=RX
r=1Ar
Br; (12)
whereRis referred to as Kronecker rank. For ﬁnding the best
approximation in GKPD with a given R, we can transform this
optimization problem into ﬁnding a best rank- Rapproximation
for a matrix, which can be solved by SVD conveniently, via
carefully rearranging Xinto a matrix and rearranging Aand
Binto vectors.
For the realization of using GKPD to decompress Conv
layers, the 4D kernel is represented as:
Ws;c;h;w =RX
r=1(Ar)s1;c1;h1;w1
(Br)s2;c2;h2;w2; (13)
whereS1S2=S,C1C2=C,H1H2=HandW1W2=
W. The 2D convolution between each Ar
Brand input
can be transformed into a 3D convolution whose depth equals
C2, followed by multiple 2D convolutions. Furthermore, the
group ofRKronecker products can be viewed as Rparallel
channels that calculate the above two steps separately. And it
was analysed that large S1andC2can help to obtain more
reduction in FLOPs.
9) Semi-tensor Product-based Tensor Decomposition
Semi-tensor product (STP) [18] is a generation of the
conventional matrix product, which extends the calculation
of two dimensionally matching matrices to that of two di-
mensionally arbitrary matrices. Since tensor contraction is
based on the conventional matrix product, we can further
substitute STP into tensor contraction, which will lead to more
general and ﬂexible tensor decomposition methods. Proposed
in [152], Semi-tensor Product-based Tensor Decomposition is
designed to enhance the ﬂexibility of Tucker decomposition,
TT and TR by replacing the conventional matrix product in
tensor contraction by STP, which demonstrates much higher
efﬁciency than original methods. Consider a special case in
which the number of columns X2RMNPis proportional
to that of rows in W2RPQ, the STP can be denoted as:
Y=XnW; (14)
or, elementwise, as:
Ym;g(n;q)=PX
p=1Xm;g(n;p)Wp;q: (15)
Note that Y2RMNQ, ”n” denotes the STP, g(n;q) =
(q 1)N+nandg(n;p) = (p 1)N+nare re-indexing
functions.

--- PAGE 7 ---
7
Hence, take STP-based Tucker decomposition as an exam-
ple, namely STTu, which can be formulated as:
X=Gn1A(1)n2A(2)n3nNA(N); (16)
whereG 2RR1R2RN,A(n)2RIn
tRn
t. Compared
with normal Tucker, the number of parameters is reduced from
(NQ
n=0Rn+PN
n=1InRn) to (NQ
n=0Rn+PN
n=1InRn
t2).
B. Low Rank Optimization Method
We have already introduced various tensor decomposition
methods, but how to apply these methods to DNNs without
signiﬁcant accuracy degradation is an optimization problem,
which remains to be discussed. Since the lower the tensor
rank is, the higher compression ratio we will get, we hope
that each layer of DNNs can be decomposed by very low
rank tensor decomposition. However, as the rank gets lower,
the approximation error increases, leading to a dramatic loss
of accuracy. Hence, there is a tradeoff between accuracy and
compression ratio, which is a widely studied problem. There
are mainly three kinds of low rank optimization methods to
achieve a good tradeoff: pre-train method, pre-set method and
compression-aware method (representative works can be seen
in Table V). For each method, we will give the key points
about the implementation in detail.
1) Pre-train Method
The pre-train method is the earliest used method in the
literature of applying tensor compression to model compres-
sion, which directly decomposes an already trained network
into a compact format, followed by ﬁne-tuning to recover the
accuracy. There are two critical issues for implementation:
tensor rank selection and instability.
Tensor rank selection means how to select the proper tensor
rank of each layer in a network. Since the extent of redundancy
varies from one layer to another, the rank of each layer is
not supposed to be equal. Hence, unlike time-consuming trial-
and-error, an efﬁcient rank selection method should allocate
the limited computation or storage resources to each layer
reasonably via carefully deciding the rank of each layer, while
ensuring the lowest accuracy degradation.
A simple but effective way is to set the rank of each layer
to be proportional to the number of corresponding input or
output channels, which usually performs better than roughly
setting all ranks equal. A probabilistic matrix factorization tool
called variational Bayesian matrix factorization (VBMF) [103]
is used in [65] to estimate tensor ranks of a tensor in Tucker
format. In order to get the mode- nrank, the corresponding
mode-nmatricization of the target tensor is viewed as an
observation with noise. Then, VBMF is employed on the
observation to ﬁlter out the noise and then obtain a low rank
matrix. In [151], the rank selection problem is formulated as
a combinatorial optimization problem [114] with computation
or memory resource constrained. The objective function is
denoted as the product of PCA energy (the sum of singular
values) of each layer, as the authors empirically observe
that the PCA energy is roughly related to the classiﬁcation
accuracy. Similarly, the algorithm in [83] also employs theidea that the approximation error is linked to the accuracy loss.
But more efﬁciently and reasonably, it selects the maximum
approximation error of all the layers as a proxy for model
accuracy. By minimizing this proxy, it is guaranteed that
no layer decomposed will signiﬁcantly reduce the accuracy.
Together with the resource constraint, the ﬁnal problem is a
minimax optimization which can be solved by binary search.
Since the approximation error does not necessarily reﬂect
the loss of accuracy, the above methods can only obtain a
suboptimal rank conﬁguration scheme. To address this chal-
lenge, reinforcement learning is employed to automatically
select ranks [19], [120]. In the established state-action-reward
system, the reward favors a reduction in resource cost and
penalizes loss of accuracy. The state (a possible global rank
conﬁguration of all the layers) that renders the maximum
reward can be chosen as the next state.
Instability means that if a model is approximated by an
unstable decomposed format such us CP format and TR
format, it will lead to difﬁculty in ﬁne-tuning, i.e.converge
slowly and converge to a false local minima. In [100], [50],
[69], it was noted that there is a degeneracy problem that
causes instability in CP decomposition. Speciﬁcally speaking,
when CP represents a relatively high-rank tensor in a low-
rank format, there are at least two rank-one components
whose Frobenius norm goes to inﬁnity and cancels each other
out. Due to the instability, [30], [75] fails to decompose the
whole network by CP decomposition, as it is difﬁcult to
ﬁnd a suitable ﬁne-tuning learning rate. To deal with this
challenge, [5] proposes to use the Tensor Power Method [2] to
calculate CP decomposition and employ iterative ﬁne-tuning,
i.e.decompose one layer at a time and then ﬁne-tune the entire
network iterativelly. The authors of [111] devise a procedure
to minimize the sensitivity (a measure for the degeneracy
degree) of the tensor reconstructed from CP format so that the
decomposed network with low sensitivity can be ﬁne-tuned
faster and obtains a better accuracy. A more direct method
proposed in [135] holds that each column of the factor matrix
should be normalized after each update, as normalization can
improve numerical stability in the outer vector product [67].
A similar instability problem also happens to TR [35]. Hence,
[110] proposes a sensitivity correction procedure to address the
problem via minimizing the sensitivity with an approximation
error bounded constraint.
2) Pre-set Method
The pre-set method has the interpretation that a tensorized
neural network that is preset to a low tensor rank format
will be trained from scratch. As the method requires no pre-
training, it can save a great deal of time to get a compressed
model. However, the method is sensitive to initialization and
difﬁcult to achieve high accuracy due to the limited model
capacity. Moreover, similar to the pre-train method, there
are also problems in conﬁguring ranks. In a nutshell, proper
initialization and tensor rank selection are the main issues with
this method.
Initialization plays an important role in providing a warm
start for training DNNs [39] as well as for the training of low
rank structure networks [137], and can have an impact on the
ﬁnal accuracy to a large extent. An empirically determined

--- PAGE 8 ---
8
TABLE V
THREE TYPES OF LOW RANK OPTIMIZATION METHOD FOR MODEL COMPRESSION
Method Description Representative Works
Pre-trainpretrain the target model, apply tensor decomposition to
trained weight tensors, and then ﬁne-tune to recover accuracy.[65], [83], [151], [75]
Pre-setconstruct tensorized netwoks, set proper
initialization, and then train the whole network.[137], [37], [146]
Compression-awaretrain the original network with normal optimizers
but enforce weight tensors to enjoy low rank structure.[61], [148], [147]
suitable initialization distribution for weights in a layer is
N(0;std =q
2
N), whereNdenotes the total number of
parameters in this layer. For a pre-set model, we should make
sure that weights in each layer approximated by factor tensors
also obey this distribution. For example, when a layer is
compressed by TR and the distribution of each core tensor
isN(0;2), then after merging these dcore tensors, elements
of the merged tensor will have mean 0 and variance Rd2d.
Hence, we need to set 2as(2
N)1
dR 1to obtain a good ini-
tialization. Similarly, for TT, the variance of TT-cores should
be(2
N)1
dR1
d 1. A more systematic analysis of initialization
for any tensor decomposition method is introduced in [109].
It is suggested that by extracting the Backbone structure ( i.e.a
structure only contains contracted dimensions, since only the
contraction operator will change the variance of weights) from
the original tensorized structure, an adjacency matrix can be
obtained from node edges of the Backbone structure, which
can be utilized to adjust the variance of factor tensors.
Tensor rank selection is seldom studied in the works of
training a tensorized neural network and usually set the
ranks to equal in experiments, as it’s difﬁcult to verify the
redundancy in each layer without a pre-training network. At
present, there are only a few methods to solve this problem for
speciﬁc tensor decompositions. Inspired by neural architecture
search (NAS) [157], [79] proposes a progressive searching
tensor ring network (PSTRN), which has the ability to ﬁnd
an appropriate rank conﬁguration for TR efﬁciently. In this
algorithm, an evolutionary phase and a progressive phase
are alternatively performed. While the evolutionary phase is
responsible for deriving good rank choices within the search
space via multi-objective genetic algorithm NSGA-II [27], the
progressive phase is responsible for narrowing the search space
in the vicinity of the optimized rank coming from the previous
evolutionary phase. For rank selection with TT decomposition,
[51] proposes a low-rank Bayesian tensorized neural network.
Bayesian methods are always used to infer tensor ranks in
CP format or Tucker format through low-rank priors in tensor
completion tasks[112], [44], [7]. This paper generates this
approach to TT format and nonlinear neural networks.
A more easily implemented method, modiﬁed Beam-search,
was proposed in [34] to ﬁnd the optimal rank setting, costing
much lower search time than the full search. To verify optimal-
ity, it adopts the validation accuracy on a mini-batch validation
dataset as its metric. This method is applicable to all kinds of
tensor decompositions.3) Compression-aware Method
Compression-aware method is the method that through
standard training and iterative optimization, the weights of
kernels and FC layers can gradually have desired low tensor
rank structures. That is, consider the future compression into
the standard training phase. Upon the end of this one-shot
training, the suitable tensor ranks are automatically learned,
without efforts to design efﬁcient rank selection schemes.
Moreover, since the training process is still on the original
network structure instead of a deeper factorized network, it’s
easy to converge towards high accuracy without being prone to
gradient vanishing or explosion. There are mainly two kinds of
ways to realize this idea, namely using low rank regularization
and solving constrained optimization.
low rank regularization is similar to the sparse regular-
ization which is always used in DNNs to avoid overﬁtting.
The main idea of low rank regularization is to add low
rank regularizer on weights in each layer to the basic loss
function. Hence, with the constraint of such regularizer, weight
tensors will gradually have a desired low rank structure during
training. Then, after low rank approximation, there is no need
to retrain for a long time and no risk of unstable recovery.
For the low rank regularizer, an index to measure the low
rank degree is essential. Since explicitly minimizing the rank
of a matrix is NP-hard, nuclear norm [11] was widely used as
a continuous convex relaxation form of rank. In [4], the sum
of nuclear norms of weight matrices in each layer is added to
cross-entropy loss, yielding a new optimization problem which
can be solved by proximal stochastic gradient descent. Simi-
larly, [144] also uses nuclear norm and the same optimization
problem is solved by stochastic sub-gradient descent [6]. In
addition, this paper embeds the low rank approximation into
the training phase to boost the low rank structure.
However, for the above, SVD will be performed on every
training step, which is inefﬁcient, especially for larger models.
Hence, [145] proposes SVD training which performs training
directly on the decomposed factors. By employing sparsity
regularization on singular values, it can achieve the goal of
boosting low rank. In order to maintain the valid SVD form,
orthogonality regularization on the left and right singular
matrices is necessary. Moreover, Orthogonality also can ef-
ﬁciently prevent the gradient to explode or vanish, therefore
achieving higher accuracy.
Solving constrained optimization is a method that through
solving an optimization problem with explicit or implicit
constraints on tensor ranks of weights, we can get an optimal

--- PAGE 9 ---
9
network not only with low loss but also with low rank
structures. Classically, [61] forms the low rank constrained
problem as minimizing the sum of the loss and a mem-
ory/computation cost function but constraining each rank not
to exceed a maximum rank. It can be solved by a learning-
compression algorithm [12]. More conveniently, [147] directly
uses budget ( e.g. memory/computation cost) as constraints,
with low rank regularizer added on the loss function. However,
since it represents tensor ranks by the sum of nuclear norms
of unfolding matrices in each mode, it cannot be generalized
to certain decomposition methods such as CP and BTD. And
when dealing with high-order tensors, there will be too many
auxiliary variables used in the augmented Lagrangian algo-
rithm, which will affect convergence. Without using nuclear
norm, [148] just set the upper bound of ranks, therefore it is
applicable to various tensor decompositions.
The above methods have an unsatisfactory tradeoff between
accuracy and compression. To address this drawback, the
Frank Wolfe algorithm is utilized in [154] to optimize net-
work weights with the low-rank constraint. This improvement
beneﬁts from the highly structured update directions of Frank
Wolfe.
For compression-aware methods, using different sparsity
measures as low rank regularizers will greatly impact com-
pression performance. For an instance, it was noted in [145]
that the`1measure ( e.g.nuclear norm) is more suitable for
an extremely high compression ratio while Hoyer measure
performs better when aiming for a relatively low compression
ratio. Hence, it’s essential to dig out an efﬁcient sparsity
measure that is attractive for any compression ratio. This is
exactly the point we want to make below.
C. Sparsity Measure
Recently, researches on compression-aware method emerge
in large numbers and plenty of experiments show that with
the premise of using the same tensor decomposition method,
compression-aware method can outperform the other two
methods[147], [148], [145]. Hence, we should pay more
attention to it. One thing that has not been fully studied
is the sparsity measure used. As the most classical convex
relaxation form of rank, nuclear norm ( `1measure) is widely
used. However, there is no evidence that the nuclear norm is a
perfect choice. Consequently, a comparison between common
sparsity measures should be made. Finding a more efﬁcient
measure may greatly improve the compression capability of
existing compression-aware algorithms.
1) Common sparsity measure
For sparse representation problems, the `0norm deﬁned
as the number of non-zeros is the traditional measure of
sparseness. However, Since the `0norm is sensitive to noise
and its derivative contains no information, the `pnorm with
0< p1is introduced to less consider the small elements
[121]. For a vector x2RN, its`pnorm can be formulated
as:
`p(x) = (NX
i=1jxijp)1
p: (17)The`1norm,`pnorm withp= 1, is the most widely used
sparsity measure. Formally, consider a vector x2RN, its`1
norm can be denoted as:
`1(x) =NX
i=1jxij: (18)
The`1norm is introduced in [143] as a more practical
substitute for the `0norm. In addition, in order to better
measure sparsity in noisy data, more ﬂexible forms based on
`1norm are proposed in [9], [59], namely sorted `1norm and
two-level`1norm. The sorted `1norm is formulated as:
`sort
1(x) =NX
i=1ijxij; (19)
where12N0. In this way, the higher the
magnitude of the element, the larger the penalty on it. More
concisely, the two-level `1norm only considers two levels of
penalty, which can be formulated as:
`2level
1(x) =X
i2I1jxij+X
j2I2jxjj: (20)
Here, we havejxijjxjj;8i2I1;8j2I2.
The Gini Index was initially proposed as a measure of the
inequity of wealth [24], [98]. Afterward, the utility of Gini
index as a measure of sparsity has been demonstrated in [115],
[60]. Given a sorted vector x2RNwhose elements increases
by degrees, i.e.x1x2xN, its Gini Index is given
by:
G(x) = 1 2NX
i=1xi
jjxjj1(N i+1
2
N): (21)
Note that if all elements are equal, i.e.no sparsity, the Gini
Index reaches its minimal 0. For the most sparse case, i.e.
onlyxNis non-zero, the Gini Index goes to a maximum of
1 1
N. Graphically, the Gini Index can be represented as twice
the area between the Lorenz curve [98] and the 45-degree
line. Each point on the Lorenz curve ( x=a,y=b) has the
interpretation that top 100apercent of the sorted elements
expresses 100bpercent of the total power. The degree line
represents the least sparse case with Gini Index equal to 0.
Fig. 6 illustrates the Lorenz curve for a vector.
The Hoyer measure was devised in [55] as a new sparsity
measure, which is a normalized version of`2
`1. For a given
vector x2RN, its Hoyer measure can be formulated as:
H(x) =p
N jjxjj1
jjxjj2p
N 1: (22)
This function goes to unity if and only if xcontains only a
single non-zero component, and takes a value of zero if and
only if all components are equal, changing smoothly between
the two extremes.
The above-mentioned sparsity measure can be applied to
the singular value vector as a low rank measure of the corre-
sponding matrix. There are other non-strict measures for the
rank of a matrix. Here, we concentrate on effective rank [116].
Let us consider a matrix Xof sizeMNwhose singular

--- PAGE 10 ---
10
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
Fig. 6. A graphical illustration of Gini Index for a vector [1, 2, 3, 4, 10]. The
dot line (45-degree line) represents the case in which all elements are equal,
and the full line is the Lorenz curve of the vector. Twice the area between
them is equal to the Gini Index of such a vector.
value vector is denoted by 2RKwithK= minfM;Ng,
its effective rank can be given by:
E(X) = exp( KX
i=1ilogi); (23)
wherei=i
jjjj1, the logarithm is to the base e, and
the convention that log 0 = 0 is adopted. This measure
is maximized when all the singular values are equal, and
minimized when the maximum singular value is much larger
than other values.
2) Comparison
In the compression-aware method, it is common to employ
sparsity regularizer on singular value vectors to encourage
weight matrices to lie in a low rank subspace. The nuclear
norm is the most frequently used. However, it simply makes
everything closer to zero, which is unfriendly to keeping the
energy of weight matrices. Hence, we prefer other measures
that encourage the insigniﬁcant singular values (with small
magnitude) to go to zero but keep the signiﬁcant values (with
large magnitude) or make them larger to maintain the energy.
Hence, we choose Gini Index, Hoyer, and effective rank as
potential objects, and make a comparison among them together
with the nuclear norm.
We execute the comparison experiment on ResNet32 trained
on the Cifar10 dataset. We utilize the most simple SVD to
compress the network, and in the compression-aware training
phase, we employ various sparsity measures on singular vector
values of each weight matrix, with a hyperparameter to
make the balance between accuracy and low rank. After this
training, there are many singular values close to zero that can
be set to zero without degrading performance. An appropriate
indicator for identifying singular values retained is introduced
in [17], namely spectral norm based indicator. This indicator
is deﬁned as the ratio of the largest discarded singular value
to the maximal singular value. It is more efﬁcient than the
normal Frobenius norm based indicator [107], as it can get rid
of the interference caused by small and noisy singular values.
 707580859095
0 1 2 3 4 5 6 7 8 9Accuracy(%)
CREffective Rank Hoyer Gini Nuclear NormFig. 7. Accuracy on Cifar10 vs compression ratio of the number of parameters
in ResNet32. Effect of different sparsity measures.
Fig. 7 shows the effect of the four sparsity measures.
The most frequently used Nuclear Norm shows the worst
performance. With the increase in the compression rate, the
accuracy drops sharply. The reason behind this can be that
at the time of pursuing a high compression ratio, the value
ofis increased, with more singular values imposed to zero.
It dramatically destroys the expressive ability of the model.
This ﬁgure also suggests that effective rank surpasses the rest
measures for any compression regime. To be speciﬁc, when
the accuracy is close to 85%, effective rank can achieve a
compression ratio almost 4 greater than the nuclear norm.
And in the case of 90%, it can achieve 2 greater than Hoyer.
For a low compression regime, effective rank also has the
greatest potential to achieve accuracy close to the original.
For the spectral norm based indicator, if we are in the
need of discarding most of singular values to gain a high
compression ratio, there are two choices: increase the value
of maximum singular value or decrease the value of tiny sin-
gular values. However, increasing the value of the maximum
singular value 10 times is much more difﬁcult than decreasing
the value of tiny singular values 10 times. Hence, we prefer
a measure that can strongly encourage tiny singular values
to reach 0. This is also the reason why effective rank can
demonstrate great efﬁciency.
III. I NTEGRATABLE TECHNIQUES
Apart from low rank approximation, there are other com-
pression schemes that can result in a signiﬁcant reduction
of parameters at the expense of only a small drop in output
accuracies, such as pruning [8], weight-sharing [16], sparsiﬁ-
cation [49] and knowledge distillation [48]. Undoubtedly, the
integration of these parameter reduction techniques, namely
parallel integration, can further enhance the efﬁciency of com-
pression. While plenty of surveys suggest integrating various
compression techniques, a detailed discussion on the combi-
nation between low rank approximation and other schemes is
still lacking. In addition, not only the reduction of parameters
but also the reduction of bits for representing parameters can
signiﬁcantly cut down the high complexity, which can be
realized by quantization and entropy coding. Quantization can
represent each parameter with lower bit-width, and entropy
coding can use codewords to encode source symbols. Both
techniques are orthogonal to the above parameter reduction

--- PAGE 11 ---
11
methods. Hence, we can directly employ them on a com-
pact model to gain a more compact representation, namely
orthogonal integration. Table VI lists representative works of
different types of integration, and Table VII lists whether these
techniques can compress or accelerate models.
A. Parallel Integration
In this part, we will give an all-round survey on how to inte-
grate low rank approximation with other parallel compression
techniques, including pruning, weight sharing, sparsiﬁcation,
and knowledge distillation. Through joint-way use, we can
pursue a higher compression capacity.
1) Integration with Pruning
Pruning is used to ﬁnd unimportant connections in a full
structure network and then abandon them, resulting in a
compact structure without signiﬁcant loss of accuracy. Pruning
can be classiﬁed according to various levels of granularity,
including weight-level, ﬁlter-level and layer-level. Weight-
level is the most ﬂexible approach [49] and can gain the
lowest memory costs by storing in sparse matrix format such
as Compressed Sparse Column (CSC) [48]. However, it leads
to difﬁculty in inference due to the need for identifying each
weight kept or abandoned. That is, this approach cannot speed
up inference or save the memory footprint unless supported by
hardware [47]. Layer-level aims at abandoning trivial layers,
which is unsuitable for shallow networks [15]. To overcome
these drawbacks, a more ﬂexible and applicable approach,
namely ﬁlter-level, is proposed [58]. Filter-level considers each
ﬁlter as a unit and discards insigniﬁcant ﬁlters to obtain a
compact model but with regular structures. Note that for two
successive Conv layers, the removal of a ﬁlter in the ﬁrst kernel
leads to the removal of the input channel in the next kernel.
Filter pruning doesn’t deal with the redundancy within a
ﬁlter, while low rank approximation can overcome this by rep-
resenting each ﬁlter in low rank format. Hence, it is promising
to combine them to explore a higher compression ratio. [41]
proposes to perform ﬁlter pruning ﬁrst and then employ Tucker
decomposition on the pruned kernels. Experiments in [41]
show that the joint-way approach can achieve up to 57% higher
compression ratio than either of them. [17] exchanges the
order of ﬁlter pruning and low-rank approximation since the
smaller ﬁlters obtained by low rank approximation can reduce
the probability of discarding essential ﬁlters. In addition,
previous works pointed out that ﬁlter pruning is likely to
prune more ﬁlters in deeper layers, resulting in still high
computation costs of the whole network[101]. But with the
help of low rank approximation, the shallow layers also can
be compressed. Then, both high-level compression of memory
and computation costs can be achieved.
One branch of works can achieve low rank approximation
and ﬁlter pruning simultaneously via regularizers. In [4],
the nuclear norm regularizer and the sparse group Lasso
regularizer [3] are combined to make weight matrices not only
low rank but also group sparse. Then the original layer can
be represented by two smaller layers, followed by discarding
insigniﬁcant input channels of the ﬁrst layer and output
channels of the second layer. Different from this method, [117]uses one type of regularizer to achieve both two motivations. It
represents a weight matrix by a basis matrix and a coefﬁcient
matrix. By imposing `2;1regularization both on the coefﬁcient
matrix and its transpose, the basis matrix can turn to be
low rank and insigniﬁcant output channels are identiﬁed. Or,
there are also some works that employ the two techniques
on different modules of a network. For instance, aiming for
Transformer architecture, [71] compresses the attention blocks
by low rank approximation and applies to prune to feedforward
blocks, which gains great enhancement.
2) Integration with Sparsiﬁcation
Sparsiﬁcation in DNNs focuses on making weight matrices
sparser so that sparse matrix computation can be employed
to reduce high computation costs. Meanwhile, it can provide
storage efﬁciency, as non-zeros and their locations can be
recorded in Compressed Sparse Row (CSR) [48] or Ellpack
Sparse Block (ESB) [89] format. There are two types of
sparsiﬁcation, namely irregular sparsity and structural sparsity.
When the non-zeros are located randomly in the matrix, we
call it irregular sparsity, which is ﬂexible but may result in
poor acceleration due to its irregular data access pattern. On
the contrary, structural sparsity can achieve regular data access
patterns. To be more speciﬁc, structural sparsity normally zeros
out a series of continuous elements in the matrix.
Low rank approximation factors a matrix into smaller
components, but these components still contain tiny elements
which can be zeroed out without leading to a signiﬁcant
increase in approximation error. Hence, it’s promising to
combine low rank approximation and sparsiﬁcation to achieve
better compression. Sparse Principal Component Analysis
(SPCA) [158] is a well-known instance to integrate factor-
ization with sparsity. The main idea of SPCA is to make each
principal component only contain a few features of data, so
that SPCA is more explainable than PCA. There are also sparse
HOSVD and sparse CP proposed in [2].
In [86], it has shown that surprisingly high sparsity can
be achieved after two-stage decomposition. It was claimed
that more than 90% of parameters can be zeroed out with
less than 1% accuracy degradation on ILSVRC2012 dataset.
In this algorithm, sparsity and low rank are achieved by
employing`1norm and`2;1norm respectively on a coefﬁcient
matrix. Finally, it converts the convolution operation in Conv
layers into spare matrix multiplication, which dramatically
reduces computation costs. Sparse SVD, i.e.factor matrices in
SVD are sparsed, was proposed in [124], which outperforms
truncated SVD. According to the view that a portion of the
input and output neurons in a layer may be insigniﬁcant, the
corresponding rows of the left and right singular matrix can
be zeroed out. And considering the importance of entries in a
row of left or right singular matrix decreases from left to right,
the Sparse SVD prefers to abandon entries nearing the right.
The resulting structural sparsity allows BLAS[76] libraries to
be used for higher speed-up.
Aiming for RNNs, [139] proposed Low-Rank Structured
Sparsity. Considering dimensional invariance in time, this
method employs `1regularization on the left and right singular
matrix derived from SVD, resulting in a column-wise and row-
wise sparse matrix without dimension distortion.

--- PAGE 12 ---
12
TABLE VI
INTEGRATABLE TECHNIQUES
Techniques Description Representative Integration Works
Parallel Integrationpruning discard insigniﬁcant connections [17], [4], [117]
sparsiﬁcation zero out insigniﬁcant weights [124], [86], [139]
weight sharing share weights across different connections [106], [82], [123]
knowledge distillation transfer knowledge learned from teacher to student [80], [85], [119]
Orthogonal Integrationquantization reduce precision [78], [72], [104]
entropy coding encode weights into binary codewords [20], [140], [14]
TABLE VII
ABILITY TO COMPRESS AND ACCELERATE FOR VARIOUS TECHNIQUES
Techniques Acceleration Compression
pruning ! !
sparsiﬁcation ! !
weight sharing # !
knowledge distillation ! !
quantization ! !
entropy coding # !
3) Integration with Weight Sharing
Weight sharing is deﬁned as an operation that shares pa-
rameters across different connections in DNNs by exploiting
redundancy. In order to design a more complex network with
a better capacity for feature extraction, it is common to copy
or reform some well-designed modules in a shallow network,
and then add new modules to the end, yielding a deeper
network. One typical network is the well-known ResNet [77].
Due to this similarity, it is promising to explore a more
compact representation by sharing parameters across these
similar subnetworks. For low rank approximation, similarly,
the idea of sharing factor tensors across tensor decompositions
of similar weight tensors can also be adopted.
A simple illustration of integration with weight sharing can
be found in [82], where a set of 3D ﬁlter bases is shared across
several or all convolutional layers. The search for bases is
equivalent to low rank approximation of all the matrix-shaped
kernels with a shared basis matrix.
Some tensor decomposition methods naturally combine
weight sharing. For example, in the previously mentioned
Semi-tensor Product-based Tensor Decomposition, STP can
calculate a multiplication between a vector x2RNand a
weight vector w2RP, resulting an output vector y2RN
P.
TheN
Pentries in each block of xshare one weight parameter
ofW.
Alternatively, one branch of works shares factor tensors
across tensor decompositions of weight tensors in different
layers. [106] proposed T-Basis, which constructs a set of 3rd-
order tensors. For an arbitrary-shaped tensor, each of its TR-
cores can be represented as a linear combination of T-Basis.
Hence, a compact representation of DNNs can be derived.
[123] proposed coupled TT, which contains a common compo-
nent and an independent component. The common component
is represented by shared TT-cores for similar network blocks,
while the independent components in TT format are variousfrom different layers to maintain the characteristics of each
layer.
4) Integration with Knowledge Distillation
Knowledge distillation [52] is a promising solution, which
aims to feed some extra knowledge learned from teacher
networks (one or more complex networks) into a student
network (much simpler network). With the help of a teacher,
the student can achieve comparable accuracy but with much
lower memory and computation costs compared with the
teacher. Let qsandqtdenote the softmax outputs of the
student network and teacher network, respectively. The student
network will be trained via aligning qsandqt. But in the
case that qtis close to the one-hot code of true labels, the
information contained in small values cannot be transferred to
the student. Hence, a trick named temperature [52] is utilized
to soften the distribution of both qsandqt.
Networks compressed by low rank approximation is also
a simpler network that can learn knowledge from the un-
compressed version. In general, the decomposed networks are
recovered by simply ﬁne-tuning to minimize the cross-entropy
function. However, the ﬁne-tuning process always converges
slowly and cannot recover the original accuracy well. Hence,
this underlines the need for training the compressed network
with information from the corresponding pre-training network.
However, it was demonstrated in [39] that it is difﬁcult to
train a student network deeper than the teacher network with
knowledge distillation due to the undesirable phenomenon
of vanishing gradient. Hence, a novel knowledge transfer
(KT) was proposed in [85], which aligns both outputs and
intermediate responses from a teacher (original) network to
its student (compressed) network. Experiments show that it
surpasses the common ﬁne-tuning and knowledge distillation,
particularly with a high compression ratio.
However, the KT method is still time-consuming and has a
demand for a fully annotated large-scale training set, which
may be infeasible in practice. Li et al. [80] proposed a
revised knowledge distillation that only requires a few label-
free samples. It adds a 1×1 Conv layer at the end of each
block of the student network, and aligns block-level outputs
of teacher and student by estimating the 1×1 Conv layer’s
parameters using least-squared regression. Since the number
of parameters in 1×1 Conv layers is relatively small, only a few
samples are necessary. It also enables fast model convergence,
thereby saving much time for recovery of accuracy. After
learning, the 1×1 Conv layer will be merged into the previous
layer, without an increase in the number of parameters.

--- PAGE 13 ---
13
B. Orthogonal Integration
1) Quantization
The operation that maps data from full precision to reduced
precision is referred to as quantization. In the training and
inference phase of DNNs, it is common to represent weights
and activations in 32-bit. However, transferring data in 32-bit
is a burden, and Multiply-Accumulate (MAC) will be oper-
ated between 32-bit ﬂoating-point values. In addition, energy
consumed scales linearly to quadratically with the number of
bits used. Hence, lowering the precision is necessary for the
reduction of memory size, acceleration and energy saving.
There are some special advantages of applying quantization
on neural networks. First, compared with continuous form, the
discrete representations are more robust to noise [13], [36]
and are more similar to the way of storing information in
human brains [132], [127]. Second, both high generalization
power [64], [74] and high efﬁciency under limited resources
[133] of discrete forms are actually what deep learning needs.
Third, common compression methods, like low rank approxi-
mation, weight-sharing, and pruning, focus on either memory
compression or acceleration so that it is deﬁcient to achieve
signiﬁcant acceleration and compression simultaneously for a
whole network, while quantization can conquer this challenge.
In addition, it is shown in [84] that most of the weights and
activations in DNNs are close to zero, which can greatly
promote the compression ability of quantization. A more
detailed survey about implementing quantization on DNNs can
be found in [38], [102].
A straight-forward way to combine low rank approximation
and quantization is to consider the network compressed by
tensor decomposition as a new network, which can be nor-
mally further compressed by various quantization methods.
However, since there is already an approximation error derived
from decomposition, the subsequent quantization will suffer
from serious accuracy degradation. Hence, a novel integration
method that considers low rank decomposition and quantiza-
tion simultaneously instead of successively has the potential
to address the challenge.
This idea can be found in [68], where both factors of
Tucker format and activations are quantized, and with the
help of knowledge distillation, the approximation error is
minimized. In [72], quantization is introduced in principal
component analysis (PCA), where the component matrix and
the coefﬁcient matrix are quantized with different bit-widths.
Together With a sparsity constraint on the coefﬁcient matrix,
the approximation error on the data manifold derived from
low rank decomposition, sparsity and quantization will be
minimized by an iterative projected gradient descent method.
Also, there are some approaches that directly extend basic
tensor decomposition algorithms to tensor decompositions
with quantized factors. For instance, quantized CP-ALS was
proposed in [104], wherein each optimization iteration factors
are quantized, and it is shown that the reconstruction error
under ALS and quantized ALS are almost the same.
The above-mentioned methods are all aiming at approxi-
mating a tensor with quantized factors, which is not suitable
for pre-set method. In [78], a quantized tensor train (QTT)
is utilized for compressing three-dimensional convolutionalneural networks. TT-Cores in tensorizied neural networks
are ﬁrst quantized, and then the quantization of feedforward
process is also made, achieving a 3× faster inference than using
only TT.
2) Entropy Coding
Entropy coding is a lossless compression scheme, which
encodes source symbols with a lower number of bits per
symbol by exploiting the probability distribution of source
[113]. Entropy coding originally adopted for data compression
is introduced to further reduce the memory size of quantized
DNNs by representing quantized weights with binary code-
words [48]. It uses Huffman coding to further save 20% to
30% of network storage with no loss of accuracy.
Huffman coding is a theoretically optimal method to encode
multivariate independence source symbols, but with the pre-
condition that statistical characteristics of source symbols are
already known. There is a problem with DNNs that statistical
characteristics of weights calculated by histogram is a time-
consuming preparation and are different for each network,
even for a network ﬁne-tuned. Hence, an encoding method
without the need for exact statistics is more efﬁcient for
compressing DNNs.
One branch of works called universal coding, such as
the variants of Lempel–ZivWelch [155], [156], [138] and
the Burrows–Wheeler transform [33], can be applied to deal
with this problem. The ’universal’ means that this coding
method has a general probability model which can be slightly
adapted to a broad class of input sources. In application,
Deep Context-based Adaptive Binary Arithmetic Coder (Deep-
CABAC) [140], one type of universal coding, is utilized to
encode weights in DNNs. It is the ﬁrst attempt to apply state-
of-art video coding methods (CABAC) to DNNs. Compared
with Huffman coding, DeepCABAC also has the advantage of
higher efﬁciency in throughput.
However, both Huffman coding and DeepCABAC are
Fixed-to-Variable (F2V) schemes in which the number of bits
for each symbol is variable. Due to the variable length in
codewords, it is inefﬁcient for memory usage when decoding,
and hence leads to high latency for inference. Instead, Tunstall
coding [14], a Variable-to-Fixed (V2F) method, is designed to
ﬁx the length of each codeword so that we can process multiple
bits simultaneously and decode multiple encoded strings in
parallel. It is reported that Tunstall coding can achieve around
6faster decoding than Huffman coding.
IV. L OWRANK OPTIMIZATION FOR SUBSPACE TRAINING
A. Low Rank Function
For a differentiable real-valued function, if its gradient
always lies in a ﬁxed low-dimensional subspace, it can be
called a low rank function [23]. The dimensionality of such
subspaces is much lower than the number of independent
variables, and it is referred to as the rank of the function. Ridge
functions are the most common low rank function, which are
deﬁned as functions that can be converted into a univariate
function by applying an afﬁne transformation to the argument
[96]. Hence, the gradient of such a function can also be
projected into a line. For example, the least-square regression

--- PAGE 14 ---
14
function which is a classic ridge function can be considered
a rank-one function. The low rank property of ridge functions
makes them widely used in classic statistics. They are utilized
as regression functions in projection pursuit regression to deal
with the curse of dimensionality and the noise in data [31].
In scientiﬁc computing, since the variables of functions for
uncertainty quantiﬁcation are always correlated, the concept of
active subspaces can be utilized to reveal a set of independent
variables whose ﬂuctuation can lead to the most signiﬁcant
change [22], [91].
Low rank property has also been found in the training
phase of DNNs. In DNNs, the number of trainable parameters
is always far more than that of training samples. Thus, for
this type of over-parameterized model, it is possible to guess
that there is a large part of the parameters that will remain
unchanged during the whole training phase. More generally,
there is a hypothesis that the training trajectory of parameters
lies in a subspace constructed by a few irrelevant variables.
That is to say, the optimization of millions of parameters can
be equivalent to optimization in a tiny subspace. There is also
evidence that the gradient of various DNNs will gradually
remain in a tiny subspace spanned by a few top eigenvectors
of the Hessian [45].
B. Subspace Training
In deep learning, the challenge that the process of training
converges very slowly is a thorny obstacle. The slow con-
vergence is caused by the dominating ﬁrst-order method, i.e.
gradient descent-based methods. This problem can be relieved
by second-order methods which utilize the information derived
from Hessian matrices. Moreover, the second-order method
is not sensitive to the learning rate, so no speciﬁc learning
rate schedule needs to be designed. However, due to the
massive parameters in DNNs, it is a computational burden to
calculate Hessian matrices. Some approaches such as Adam
[66], RMSprop [25] and AdaGrad [32] utilize part of second-
order information, like momentum and accumulation informa-
tion, have already surpassed the performance of conventional
gradient-based methods.
In order to apply second-order methods such as quasi-
Newton method [10] to network training, the straightforward
way is to reduce the number of parameters that need to be op-
timized. In view of the low rank structure discovered in DNNs,
it is promising to optimize the whole network in a subspace
using quasi-Newton method, without the loss of accuracy.
DLDR-based Quasi-Newton method [81] is introduced to save
35% of training time versus SGD [118]. To be speciﬁc, in this
algorithm, DLDR is devised to identify the low-dimensional
subspace constructed in some important directions which can
contribute signiﬁcantly to the variance of the loss function.
It achieves this by sampling the training trajectory and then
performing Principal Component Analysis (PCA) to analyse
the dominating directions. Then, second-order optimization
can be directly executed in this tiny subspace, resulting in
fast convergence.C. Spatial Redundancy and Temporal Redundancy
While model compression exploits the redundancy in net-
works to reduce memory and computation complexity, sub-
space training exploits the redundancy to reduce training
time. In other words, the objective of model compression and
subspace training is spatial efﬁciency and temporal efﬁciency,
respectively. Since they both exploit redundancy, we are won-
dering whether the redundancy they deal with is of the same
origin or not.
We analyse this by performing subspace training on low
rank approximated networks to determine if subspace training
has a poor performance on compressed networks. If so, it is
evidence that the redundancy decreased by model compression
is insufﬁcient for subspace training, i.e.the low rank property
in time domain disappears.
Here, we perform a simple experiment on LetNet-300-100
with MNIST dataset. LeNet-300-100 contains two hidden fully
connected layers with output dimensions 300 and 100, and
an output layer with dimension 10. We apply SVD on the
ﬁrst two layers and then ﬁne-tune. We record the training
trajectory and establish a 5D subspace by performing PCA.
To see if such a tiny subspace is sufﬁcient, we project weights
onto this subspace and calculate the normalized approximate
error. Fig. 8 shows that as the rank decreases, the normalized
error increases almost linearly. It suggests that the higher the
compression ratio, the less suitable the subspace with such low
ﬁxed dimensionality is. In other words, model compression
decreases the redundancy subspace training can exploit.
 0.0000.0020.0040.0060.0080.0100.0120.0140.0160.0180.020
20 30 40 50 60 baseNormolized Error
Rank
Fig. 8. Normalized Error vs rank when projecting SVD-compressed LeNet-
300-100 on a 5D subspace. The normalized error is the ratio of `2norm of
error between original parameters and projected parameters and `2norm of the
original parameters. The rank is in respect to SVD. ’base’ is the uncompressed
network.
Also, we can ﬁgure that after low rank decomposition, a
higher-dimensional subspace is in need. As shown in Fig. 9,
increasing the dimensionality of subspace has a greater effect
on the highly compressed network. Under all the rank settings,
normalized error goes to zero when the dimensionality is equal
to 12. But there is a sharp descent when the dimensionality is
increased from 11 to 12 for rank=20. That is to say, a slight
drop in dimensionality is serious for a highly compressed
network. When a network is compressed extremely, there is
little redundancy in time domain.

--- PAGE 15 ---
15
6 8 10 12 14
Dimensionality0.00000.00250.00500.00750.01000.01250.01500.0175Normalized Errorrank=20
rank=40
rank=60
Fig. 9. Normalized Error vs dimensionality of subspace under different ranks
(different compression extents). The dimensionality of subspace ranges from
5 to 15.
D. Make A Balance
Since redundancy exploited by model compression and
subspace training are of the same origin, there is a balance
between spatial efﬁciency and temporal efﬁciency. If we assign
most of the redundancy to model compression, we can obtain
a compact network and hence achieve spatial efﬁciency, but
little redundancy is left for subspace training. Conversely, if
we are in need to train a network quickly, we should promise
to assign most of the redundancy to subspace training.
For model compression, the training of a tensorized neural
network (TNN) is much time-consuming than that of the
original network. Hence, there is a need for utilizing subspace
training to accelerate the training of TNN. Intuitively, for a
highly compressed TNN, since there is little redundancy, it
is inefﬁcient to train such a TNN in a tiny subspace. Fig.
10 shows the performance of subspace training when applied
to TT-based TNNs with various compression regimes. The
base network is ResNet32 trained on Cifar10 dataset. All the
experiments run 15 epochs (saving 35% time of SGD method)
with Quasi-Newton method and the subspace is ﬁxed to 40D.
In this ﬁgure, the orange line (the case in which TT-Net is
trained in normal way) is almost a horizontal line, but the
green line (trained in subspace) descends sharply at the time
of high compression ratio. It suggests that subspace training
can be combined with model compression to achieve spatio-
temporal efﬁciency under a moderate compression regime, but
such a tiny space is not suitable for an extremely compressed
network.
Hence, under an extreme compression regime, it is essential
to increase the dimensionality of subspace to relieve the
accuracy degradation. But it is infeasible to increase dimen-
sionality blindly, as the number of sampling epochs will also
increase, i.e.lessen temporal efﬁciency. Fig. 11 shows the
effect of increasing the dimensionality of subspace for a highly
compressed TT-Net. It demonstrates that as the dimensionality
of subspace increases, the accuracy degradation of subspace
training decreases. When the dimensionality is increased to 55,
we can achieve a good accuracy close to the original, but it
2 3 4 5 6
CR7880828486889092Accuracy
TT without subspace training
TT with subspace trainingFig. 10. Compare the accuracy degradation when applying subspace training
to TT-Nets. CR denotes the compression ratio of model size.
35 40 45 50 55
Dimensionality808284868890AccuracyTT with subspace training
Fig. 11. The effect of increasing the dimensionality of subspace on training
TT-Nets in subspace. The dashed line represents the accuracy of training TT-
Nets in a normal way.
is worth noting that the total time (time for subspace training
and for sampling) is near the normal training time. However,
in the case that we want to train a compact TNN quickly and
a small drop in accuracy can be tolerated, it is a good choice
to train such a network in a moderate subspace.
V. C ONCLUSION AND FUTURE DIRECTIONS
In this paper, two types of low rank tensor optimization
for efﬁcient deep learning are discussed, namely low rank
approximation for model compression and subspace training
for fast convergence. For low rank approximation, we list
various efﬁcient tensor decomposition methods and introduce
three types of optimization methods. Since sparsity measure
is applied frequently in low rank approximation, we make a
comparison among common measures, and experiments show
that effective rank can achieve the best accuracy-compression
tradeoff. In addition, we investigate how to integrate low rank
approximation with other compression techniques. Then, we
give a brief introduction to subspace training and analyze
that redundancy exploited by subspace training and low rank

--- PAGE 16 ---
16
approximation is of the same origin. Further, we make a
discussion on how to combine the two to accelerate the
training of tensorized neural networks.
However, up to now, few works focus on integrating more
than three types of parameter reduction compression tech-
niques, which is more promising to take maximum advantage
of redundancy in networks. Further, it is possible to devise
a ﬂexible framework to integrate all kinds of compression
techniques.
In practice, low computation complexity is not equivalent to
low latency [126], and the energy consumed by computation is
only a small part of the total energy for inference [54], [125].
But most works take FLOPs and memory size as benchmarks.
That is to say, an advanced algorithm with very low complexity
may not be applied to battery-powered mobile devices. Hence,
more efforts are needed in decreasing the energy consumption
of DNNs.
For subspace training, the temporal efﬁciency is still limited,
as the Quasi-Newton method is still based on the gradient
of the original millions of parameters. Direct optimization on
several independent variables is still to be studied. In addition,
since the sampling procedure occupies most of the training
time, there is a need to introduce new techniques to construct
subspace with fewer sample epochs. One potential way is
to represent all the parameters in tensor format and apply
tensor decomposition to better analyze principal components,
i.e.higher-order PCA [67].
ACKNOWLEDGMENT
The work is supported by the National Natural
Science Foundation of China (NSFC, No. 62171088),
Medico-Engineering Cooperation Funds from University
of Electronic Science and Technology of China (No.
ZYGX2021YGLH215).
REFERENCES
[1] Norhamreeza Abdul Hamid, Nazri Mohd Nawi, Rozaida Ghazali, and
Mohd Najib Mohd Salleh. Accelerating learning performance of back
propagation algorithm by using adaptive gain together with adaptive
momentum and adaptive learning rate on classiﬁcation problems. In
International Conference on Ubiquitous Computing and Multimedia
Applications , pages 559–570. Springer, 2011.
[2] Genevera Allen. Sparse higher-order principal components analysis. In
Artiﬁcial Intelligence and Statistics , pages 27–36. PMLR, 2012.
[3] Jose M Alvarez and Mathieu Salzmann. Learning the number of
neurons in deep networks. Advances in neural information processing
systems , 29, 2016.
[4] Jose M Alvarez and Mathieu Salzmann. Compression-aware training
of deep networks. Advances in neural information processing systems ,
30, 2017.
[5] Marcella Astrid and Seung-Ik Lee. Cp-decomposition with tensor
power method for convolutional neural networks compression. In 2017
IEEE International Conference on Big Data and Smart Computing
(BigComp) , pages 115–118. IEEE, 2017.
[6] Haim Avron, Satyen Kale, Shiva Kasiviswanathan, and Vikas Sind-
hwani. Efﬁcient and practical stochastic subgradient descent for nuclear
norm regularization. arXiv preprint arXiv:1206.6384 , 2012.
[7] Juan Andr ´es Bazerque, Gonzalo Mateos, and Georgios B Giannakis.
Rank regularization and bayesian inference for tensor completion and
extrapolation. IEEE transactions on signal processing , 61(22):5689–
5703, 2013.
[8] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John
Guttag. What is the state of neural network pruning? Proceedings of
machine learning and systems , 2:129–146, 2020.[9] Malgorzata Bogdan, Ewout van den Berg, Weijie Su, and Emmanuel
Candes. Statistical estimation and testing via the sorted l1 norm. arXiv
preprint arXiv:1310.1969 , 2013.
[10] Richard H Byrd, Jorge Nocedal, and Robert B Schnabel. Repre-
sentations of quasi-newton matrices and their use in limited memory
methods. Mathematical Programming , 63(1):129–156, 1994.
[11] Jian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular
value thresholding algorithm for matrix completion. SIAM Journal on
optimization , 20(4):1956–1982, 2010.
[12] Miguel A Carreira-Perpin ´an and Yerlan Idelbayev. “learning-
compression” algorithms for neural net pruning. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 8532–8541, 2018.
[13] Rishidev Chaudhuri and Ila Fiete. Computational principles of memory.
Nature neuroscience , 19(3):394–403, 2016.
[14] Chunyun Chen, Zhe Wang, Xiaowei Chen, Jie Lin, and Mohamed
M Sabry Aly. Efﬁcient tunstall decoder for deep neural network
compression. In 2021 58th ACM/IEEE Design Automation Conference
(DAC) , pages 1021–1026. IEEE, 2021.
[15] Shi Chen and Qi Zhao. Shallowing deep networks: Layer-wise pruning
based on feature representations. IEEE transactions on pattern analysis
and machine intelligence , 41(12):3048–3056, 2018.
[16] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and
Yixin Chen. Compressing neural networks with the hashing trick.
InInternational conference on machine learning , pages 2285–2294.
PMLR, 2015.
[17] Zhen Chen, Zhibo Chen, Jianxin Lin, Sen Liu, and Weiping Li. Deep
neural network acceleration based on low-rank approximated channel
pruning. IEEE Transactions on Circuits and Systems I: Regular Papers ,
67(4):1232–1244, 2020.
[18] Daizhan Cheng, Hongsheng Qi, and Ancheng Xue. A survey on semi-
tensor product of matrices. Journal of Systems Science and Complexity ,
20(2):304–322, 2007.
[19] Zhiyu Cheng, Baopu Li, Yanwen Fan, and Yingze Bao. A novel rank
selection scheme in tensor ring decomposition based on reinforcement
learning for deep neural networks. In ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 3292–3296. IEEE, 2020.
[20] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Universal deep
neural network compression. IEEE Journal of Selected Topics in Signal
Processing , 14(4):715–726, 2020.
[21] Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagannathan
Sarangapani. A comprehensive survey on model compression and
acceleration. Artiﬁcial Intelligence Review , 53(7):5113–5155, 2020.
[22] Paul G Constantine, Michael Emory, Johan Larsson, and Gianluca
Iaccarino. Exploiting active subspaces to quantify uncertainty in the
numerical simulation of the hyshot ii scramjet. Journal of Computa-
tional Physics , 302:1–20, 2015.
[23] Romain Cosson, Ali Jadbabaie, Anuran Makur, Amirhossein Rei-
sizadeh, and Devavrat Shah. Gradient descent for low-rank functions.
arXiv preprint arXiv:2206.08257 , 2022.
[24] Hugh Dalton. The measurement of the inequality of incomes. The
Economic Journal , 30(119):348–361, 1920.
[25] Yann Dauphin, Harm De Vries, and Yoshua Bengio. Equilibrated
adaptive learning rates for non-convex optimization. Advances in
neural information processing systems , 28, 2015.
[26] Lieven De Lathauwer. Decompositions of a higher-order tensor in
block terms—part ii: Deﬁnitions and uniqueness. SIAM Journal on
Matrix Analysis and Applications , 30(3):1033–1066, 2008.
[27] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyari-
van. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE
transactions on evolutionary computation , 6(2):182–197, 2002.
[28] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model
compression and hardware acceleration for neural networks: A com-
prehensive survey. Proceedings of the IEEE , 108(4):485–532, 2020.
[29] Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and
Nando De Freitas. Predicting parameters in deep learning. Advances
in neural information processing systems , 26, 2013.
[30] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and
Rob Fergus. Exploiting linear structure within convolutional networks
for efﬁcient evaluation. Advances in neural information processing
systems , 27, 2014.
[31] David L Donoho and Iain M Johnstone. Projection-based approxima-
tion and a duality with kernel methods. The Annals of Statistics , pages
58–106, 1989.
[32] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient
methods for online learning and stochastic optimization. Journal of
machine learning research , 12(7), 2011.
[33] Michelle Effros, Karthik Visweswariah, Sanjeev R Kulkarni, and Sergio

--- PAGE 17 ---
17
Verd ´u. Universal lossless source coding with the burrows wheeler
transform. IEEE Transactions on Information Theory , 48(5):1061–
1081, 2002.
[34] Moonjung Eo, Suhyun Kang, and Wonjong Rhee. An effective
low-rank compression with a joint rank selection followed by a
compression-friendly training. Neural Networks , 161:165–177, 2023.
[35] Mike Espig, Wolfgang Hackbusch, Stefan Handschuh, and Reinhold
Schneider. Optimization problems in contracted tensor networks.
Computing and visualization in science , 14(6):271–285, 2011.
[36] A Aldo Faisal, Luc PJ Selen, and Daniel M Wolpert. Noise in the
nervous system. Nature reviews neuroscience , 9(4):292–303, 2008.
[37] Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry
Vetrov. Ultimate tensorization: compressing convolutional and fc layers
alike. arXiv preprint arXiv:1611.03214 , 2016.
[38] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W
Mahoney, and Kurt Keutzer. A survey of quantization methods for
efﬁcient neural network inference. arXiv preprint arXiv:2103.13630 ,
2021.
[39] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings of the thir-
teenth international conference on artiﬁcial intelligence and statistics ,
pages 249–256. JMLR Workshop and Conference Proceedings, 2010.
[40] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Com-
pressing deep convolutional networks using vector quantization. arXiv
preprint arXiv:1412.6115 , 2014.
[41] Saurabh Goyal, Anamitra Roy Choudhury, and Vivek Sharma. Com-
pression of deep neural networks by combining pruning and low rank
decomposition. In 2019 IEEE International Parallel and Distributed
Processing Symposium Workshops (IPDPSW) , pages 952–958. IEEE,
2019.
[42] Lars Grasedyck. Hierarchical singular value decomposition of tensors.
SIAM journal on matrix analysis and applications , 31(4):2029–2054,
2010.
[43] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech
recognition with deep recurrent neural networks. In 2013 IEEE
international conference on acoustics, speech and signal processing ,
pages 6645–6649. Ieee, 2013.
[44] Rajarshi Guhaniyogi, Shaan Qamar, and David B Dunson. Bayesian
tensor regression. The Journal of Machine Learning Research ,
18(1):2733–2763, 2017.
[45] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent
happens in a tiny subspace. arXiv preprint arXiv:1812.04754 , 2018.
[46] Marawan Gamal Abdel Hameed, Marzieh S Tahaei, Ali Mosleh, and
Vahid Partovi Nia. Convolutional neural network compression through
generalized kronecker product decomposition. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , pages 771–779, 2022.
[47] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A
Horowitz, and William J Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. ACM SIGARCH Computer Architecture
News , 44(3):243–254, 2016.
[48] Song Han, Huizi Mao, and William J Dally. Deep compression:
Compressing deep neural networks with pruning, trained quantization
and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
[49] Song Han, Jeff Pool, John Tran, and William Dally. Learning both
weights and connections for efﬁcient neural network. Advances in
neural information processing systems , 28, 2015.
[50] Richard A Harshman. The problem and nature of degenerate solutions
or decompositions of 3-way arrays. In Talk at the Tensor Decompo-
sitions Workshop, Palo Alto, CA, American Institute of Mathematics ,
2004.
[51] Cole Hawkins and Zheng Zhang. Bayesian tensorized neural networks
with automatic rank selection. Neurocomputing , 453:172–180, 2021.
[52] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the
knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2(7),
2015.
[53] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever,
and Ruslan R Salakhutdinov. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 ,
2012.
[54] Mark Horowitz. 1.1 computing’s energy problem (and what we can do
about it). In 2014 IEEE International Solid-State Circuits Conference
Digest of Technical Papers (ISSCC) , pages 10–14. IEEE, 2014.
[55] Patrik O Hoyer. Non-negative matrix factorization with sparseness
constraints. Journal of machine learning research , 5(9), 2004.
[56] Huyan Huang, Yipeng Liu, Zhen Long, and Ce Zhu. Robust low-rank
tensor ring completion. IEEE Transactions on Computational Imaging ,
6:1117–1126, 2020.
[57] Junzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with
structured sparsity. Journal of Machine Learning Research , 12(11),2011.
[58] Qiangui Huang, Kevin Zhou, Suya You, and Ulrich Neumann. Learning
to prune ﬁlters in convolutional neural networks. In 2018 IEEE Winter
Conference on Applications of Computer Vision (WACV) , pages 709–
718. IEEE, 2018.
[59] Xiaolin Huang, Yipeng Liu, Lei Shi, Sabine Van Huffel, and Johan AK
Suykens. Two-level `1minimization for compressed sensing. Signal
Processing , 108:459–475, 2015.
[60] Niall Hurley, Scott Rickard, and Paul Curran. Parameterized lifting for
sparse signal representations using the gini index. Signal Processing
with Adaptative Sparse Structured Representations (SPARS05), Rennes,
France , 2005.
[61] Yerlan Idelbayev and Miguel A Carreira-Perpin ´an. Low-rank compres-
sion of neural nets: Learning the rank of each layer. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , pages 8049–8059, 2020.
[62] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up
convolutional neural networks with low rank expansions. arXiv preprint
arXiv:1405.3866 , 2014.
[63] Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and Shih-Fu
Chang. Exploiting feature and class relationships in video catego-
rization with regularized deep neural networks. IEEE transactions on
pattern analysis and machine intelligence , 40(2):352–364, 2017.
[64] Mel Win Khaw, Luminita Stevens, and Michael Woodford. Discrete
adjustment to a changing environment: Experimental evidence. Journal
of Monetary Economics , 91:88–103, 2017.
[65] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu
Yang, and Dongjun Shin. Compression of deep convolutional neural
networks for fast and low power mobile applications. arXiv preprint
arXiv:1511.06530 , 2015.
[66] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 , 2014.
[67] Tamara G Kolda and Brett W Bader. Tensor decompositions and
applications. SIAM review , 51(3):455–500, 2009.
[68] Nikolay Kozyrskiy and Anh-Huy Phan. Cnn acceleration by
low-rank approximation with quantized factors. arXiv preprint
arXiv:2006.08878 , 2020.
[69] Wim P Krijnen, Theo K Dijkstra, and Alwin Stegeman. On the non-
existence of optimal solutions and the occurrence of “degeneracy” in
the candecomp/parafac model. Psychometrika , 73(3):431–439, 2008.
[70] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. Advances in
neural information processing systems , 25, 2012.
[71] Ankur Kumar. Vision transformer compression with structured pruning
and low rank approximation. arXiv preprint arXiv:2203.13444 , 2022.
[72] Andrey Kuzmin, Mart van Baalen, Markus Nagel, and Arash Behboodi.
Quantized sparse weight decomposition for neural network compres-
sion. arXiv preprint arXiv:2207.11048 , 2022.
[73] Nicholas D Lane, Sourav Bhattacharya, Petko Georgiev, Claudio For-
livesi, and Fahim Kawsar. An early resource characterization of deep
learning on wearables, smartphones and internet-of-things devices. In
Proceedings of the 2015 international workshop on internet of things
towards applications , pages 7–12, 2015.
[74] Kenneth W Latimer, Jacob L Yates, Miriam LR Meister, Alexander C
Huk, and Jonathan W Pillow. Single-trial spike trains in parietal cortex
reveal discrete steps during decision-making. Science , 349(6244):184–
187, 2015.
[75] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and
Victor Lempitsky. Speeding-up convolutional neural networks using
ﬁne-tuned cp-decomposition. arXiv preprint arXiv:1412.6553 , 2014.
[76] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise
brain damage. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 2554–2564, 2016.
[77] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.
nature , 521(7553):436–444, 2015.
[78] Donghyun Lee, Dingheng Wang, Yukuan Yang, Lei Deng, Guangshe
Zhao, and Guoqi Li. Qttnet: Quantized tensor train neural networks
for 3d object and video recognition. Neural Networks , 141:420–432,
2021.
[79] Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dongbin Zhao, and
Zenglin Xu. Heuristic rank selection with progressively searching
tensor ring network. Complex & Intelligent Systems , 8(2):771–785,
2022.
[80] Tianhong Li, Jianguo Li, Zhuang Liu, and Changshui Zhang. Few
sample knowledge distillation for efﬁcient network compression. In
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 14639–14647, 2020.
[81] Tao Li, Lei Tan, Zhehao Huang, Qinghua Tao, Yipeng Liu, and Xiaolin
Huang. Low dimensional trajectory hypothesis is true: Dnns can be

--- PAGE 18 ---
18
trained in tiny subspaces. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2022.
[82] Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning
ﬁlter basis for convolutional neural network compression. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision ,
pages 5623–5632, 2019.
[83] Lucas Liebenwein, Alaa Maalouf, Dan Feldman, and Daniela Rus.
Compressing neural networks: Towards determining the optimal layer-
wise decomposition. Advances in Neural Information Processing
Systems , 34:5328–5344, 2021.
[84] Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy. Fixed point
quantization of deep convolutional networks. In International confer-
ence on machine learning , pages 2849–2858. PMLR, 2016.
[85] Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo
Luo. Holistic cnn compression via low-rank decomposition with
knowledge transfer. IEEE transactions on pattern analysis and machine
intelligence , 41(12):2889–2905, 2018.
[86] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Mar-
ianna Pensky. Sparse convolutional neural networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition ,
pages 806–814, 2015.
[87] Jiani Liu, Ce Zhu, and Yipeng Liu. Smooth compact tensor ring
regression. IEEE Transactions on Knowledge and Data Engineering ,
34(9):4439–4452, 2020.
[88] Jiani Liu, Ce Zhu, Zhen Long, Yipeng Liu, et al. Tensor regression.
Foundations and Trends® in Machine Learning , 14(4):379–565, 2021.
[89] Xing Liu, Mikhail Smelyanskiy, Edmond Chow, and Pradeep Dubey.
Efﬁcient sparse matrix-vector multiplication on x86-based many-core
processors. In Proceedings of the 27th international ACM conference
on International conference on supercomputing , pages 273–282, 2013.
[90] Yipeng Liu. Tensors for Data Processing: Theory, Methods, and
Applications . Academic Press, 2021.
[91] Yipeng Liu, Maarten De V os, Ivan Gligorijevic, Vladimir Matic,
Yuqian Li, and Sabine Van Huffel. Multi-structural signal recovery
for biomedical compressive sensing. IEEE Transactions on Biomedical
Engineering , 60(10):2794–2805, 2013.
[92] Yipeng Liu, Jiani Liu, Zhen Long, and Ce Zhu. Tensor decomposition
in deep networks. In Tensor Computation for Data Analysis , pages
241–263. Springer, 2022.
[93] Yipeng Liu, Jiani Liu, and Ce Zhu. Low-rank tensor train coefﬁcient
array estimation for tensor-on-tensor regression. IEEE transactions on
neural networks and learning systems , 31(12):5402–5411, 2020.
[94] Yipeng Liu, Zhen Long, Huyan Huang, and Ce Zhu. Low cp rank
and tucker rank tensor completion for estimating missing components
in image data. IEEE Transactions on Circuits and Systems for Video
Technology , 30(4):944–954, 2019.
[95] Yipeng Liu, Zhen Long, and Ce Zhu. Image completion using low
tensor tree rank and total variation minimization. IEEE Transactions
on Multimedia , 21(2):338–350, 2018.
[96] Benjamin F Logan and Larry A Shepp. Optimal reconstruction of a
function from its projections. Duke mathematical journal , 42(4):645–
659, 1975.
[97] Zhen Long, Ce Zhu, Jiani Liu, and Yipeng Liu. Bayesian low
rank tensor ring for image recovery. IEEE Transactions on Image
Processing , 30:3568–3580, 2021.
[98] Max O Lorenz. Methods of measuring the concentration of wealth.
Publications of the American statistical association , 9(70):209–219,
1905.
[99] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter level
pruning method for deep neural network compression. In Proceedings
of the IEEE international conference on computer vision , pages 5058–
5066, 2017.
[100] Ben C Mitchell and Donald S Burdick. Slowly converging parafac
sequences: swamps and two-factor degeneracies. Journal of Chemo-
metrics , 8(2):155–168, 1994.
[101] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan
Kautz. Pruning convolutional neural networks for resource efﬁcient
inference. arXiv preprint arXiv:1611.06440 , 2016.
[102] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bon-
darenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on
neural network quantization. arXiv preprint arXiv:2106.08295 , 2021.
[103] Shinichi Nakajima, Masashi Sugiyama, S Derin Babacan, and Ry-
ota Tomioka. Global analytic solution of fully-observed variational
bayesian matrix factorization. The Journal of Machine Learning
Research , 14(1):1–37, 2013.
[104] Amirreza Nekooei and Saeed Safari. Compression of deep neural
networks based on quantized tensor decomposition to implement on
reconﬁgurable hardware platforms. Neural Networks , 150:350–363,
2022.[105] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P
Vetrov. Tensorizing neural networks. Advances in neural information
processing systems , 28, 2015.
[106] Anton Obukhov, Maxim Rakhuba, Stamatios Georgoulis, Menelaos
Kanakis, Dengxin Dai, and Luc Van Gool. T-basis: a compact
representation for neural networks. In International Conference on
Machine Learning , pages 7392–7404. PMLR, 2020.
[107] Kazuki Osawa and Rio Yokota. Evaluating the compression efﬁciency
of the ﬁlters in convolutional neural networks. In International
Conference on Artiﬁcial Neural Networks , pages 459–466. Springer,
2017.
[108] Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on
Scientiﬁc Computing , 33(5):2295–2317, 2011.
[109] Yu Pan, Zeyong Su, Ao Liu, Wang Jingquan, Nannan Li, and Zenglin
Xu. A uniﬁed weight initialization paradigm for tensorial convolutional
neural networks. In International Conference on Machine Learning ,
pages 17238–17257. PMLR, 2022.
[110] Anh-Huy Phan, Konstantin Sobolev, Dmitry Ermilov, Igor V orona,
Nikolay Kozyrskiy, Petr Tichavsky, and Andrzej Cichocki. How to
train unstable looped tensor network. arXiv preprint arXiv:2203.02617 ,
2022.
[111] Anh-Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Er-
milov, Julia Gusak, Petr Tichavsk `y, Valeriy Glukhov, Ivan Oseledets,
and Andrzej Cichocki. Stable low-rank tensor decomposition for
compression of convolutional neural network. In European Conference
on Computer Vision , pages 522–539. Springer, 2020.
[112] Piyush Rai, Yingjian Wang, Shengbo Guo, Gary Chen, David Dunson,
and Lawrence Carin. Scalable bayesian low-rank decomposition of
incomplete multiway tensors. In International Conference on Machine
Learning , pages 1800–1808. PMLR, 2014.
[113] Stefano Recanatesi, Matthew Farrell, Madhu Advani, Timothy Moore,
Guillaume Lajoie, and Eric Shea-Brown. Dimensionality com-
pression and expansion in deep neural networks. arXiv preprint
arXiv:1906.00443 , 2019.
[114] Colin R Reeves. Modern heuristic techniques for combinatorial
problems . John Wiley & Sons, Inc., 1993.
[115] Scott Rickard. Sparse sources are separated sources. In 2006 14th
European signal processing conference , pages 1–5. IEEE, 2006.
[116] Olivier Roy and Martin Vetterli. The effective rank: A measure of
effective dimensionality. In 2007 15th European signal processing
conference , pages 606–610. IEEE, 2007.
[117] Xiaofeng Ruan, Yufan Liu, Chunfeng Yuan, Bing Li, Weiming Hu,
Yangxi Li, and Stephen Maybank. Edp: An efﬁcient decomposition and
pruning scheme for convolutional neural network compression. IEEE
Transactions on Neural Networks and Learning Systems , 32(10):4499–
4513, 2020.
[118] Sebastian Ruder. An overview of gradient descent optimization
algorithms. arXiv preprint arXiv:1609.04747 , 2016.
[119] R. Sadhukhan, A. Saha, J. Mukhopadhyay, and A. Patra. Knowledge
distillation inspired ﬁne-tuning of tucker decomposed cnns and adver-
sarial robustness analysis. In 2020 IEEE International Conference on
Image Processing (ICIP) , 2020.
[120] Mohammad Samragh, Mojan Javaheripi, and Farinaz Koushanfar. Au-
torank: Automated rank selection for effective neural network cus-
tomization. In Proceedings of the ML-for-Systems Workshop at the
46th International Symposium on Computer Architecture (ISCA’19) ,
2019.
[121] Lei Shi, Xiaolin Huang, Yunlong Feng, and Johan Suykens. Sparse
kernel regression with coefﬁcient-based lq-regularization. Journal of
Machine Learning Research , 20, 2019.
[122] Karen Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[123] Weize Sun, Shaowu Chen, Lei Huang, Hing Cheung So, and Min Xie.
Deep convolutional neural network compression via coupled tensor
decomposition. IEEE Journal of Selected Topics in Signal Processing ,
15(3):603–616, 2020.
[124] Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan, and Frederic
Andres. Sparse low rank factorization for deep neural network
compression. Neurocomputing , 398:185–196, 2020.
[125] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efﬁcient
processing of deep neural networks: A tutorial and survey. Proceedings
of the IEEE , 105(12):2295–2329, 2017.
[126] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. How
to evaluate deep neural network processors: Tops/w (alone) considered
harmful. IEEE Solid-State Circuits Magazine , 12(3):28–41, 2020.
[127] James Tee and Desmond P Taylor. Is information in the brain
represented in continuous or discrete form? IEEE Transactions on
Molecular, Biological and Multi-Scale Communications , 6(3):199–209,

--- PAGE 19 ---
19
2020.
[128] Urmish Thakker, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov,
Ganesh Dasika, and Matthew Mattina. Compressing rnns for iot devices
by 15-38x using kronecker products. arXiv preprint arXiv:1906.02876 ,
2019.
[129] Ledyard R Tucker. Implications of factor analysis of three-way matrices
for measurement of change. Problems in Measuring Change , 15:122–
137, 1963.
[130] Ledyard R Tucker. Some mathematical notes on three-mode factor
analysis. Psychometrika , 31(3):279–311, 1966.
[131] Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing
for neural network compression. arXiv preprint arXiv:1702.04008 ,
2017.
[132] Ruﬁn VanRullen and Christof Koch. Is perception discrete or contin-
uous? Trends in cognitive sciences , 7(5):207–213, 2003.
[133] Lav R Varshney, Per Jesper Sj ¨ostr¨om, and Dmitri B Chklovskii. Opti-
mal information storage in noisy synapses under resource constraints.
Neuron , 52(3):409–423, 2006.
[134] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention
is all you need. Advances in neural information processing systems ,
30, 2017.
[135] Lokesh Veeramacheneni, Moritz Wolter, Reinhard Klein, and Jochen
Garcke. Canonical convolutional neural networks. In 2022 Interna-
tional Joint Conference on Neural Networks (IJCNN) , pages 1–8. IEEE,
2022.
[136] Maolin Wang, Yu Pan, Xiangli Yang, Guangxi Li, and Zenglin Xu.
Tensor networks meet neural networks: A survey. arXiv preprint
arXiv:2302.09019 , 2023.
[137] Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, and Vaneet
Aggarwal. Wide compression: Tensor ring nets. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , pages
9329–9338, 2018.
[138] Terry A. Welch. A technique for high-performance data compression.
Computer , 17(06):8–19, 1984.
[139] Weijing Wen, Fan Yang, Yangfeng Su, Dian Zhou, and Xuan Zeng.
Learning low-rank structured sparsity in recurrent neural networks. In
2020 IEEE International Symposium on Circuits and Systems (ISCAS) ,
pages 1–4. IEEE, 2020.
[140] Simon Wiedemann, Heiner Kirchhoffer, Stefan Matlage, Paul Haase,
Arturo Marban, Talmaj Marin ˇc, David Neumann, Tung Nguyen, Heiko
Schwarz, Thomas Wiegand, et al. Deepcabac: A universal compression
algorithm for deep neural networks. IEEE Journal of Selected Topics
in Signal Processing , 14(4):700–714, 2020.
[141] Bijiao Wu, Dingheng Wang, Guangshe Zhao, Lei Deng, and Guoqi Li.
Hybrid tensor decomposition in neural network compression. Neural
Networks , 132:309–320, 2020.
[142] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
Quantized convolutional neural networks for mobile devices. In
Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 4820–4828, 2016.
[143] Peng Xu, Yin Tian, Huafu Chen, and Dezhong Yao. Lp norm iterative
sparse solution for eeg source localization. IEEE Transactions on
Biomedical Engineering , 54(3):400–409, 2007.
[144] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Wenrui
Dai, Yingyong Qi, Yiran Chen, Weiyao Lin, and Hongkai Xiong.
Trained rank pruning for efﬁcient deep neural networks. In 2019
Fifth Workshop on Energy Efﬁcient Machine Learning and Cognitive
Computing-NeurIPS Edition (EMC2-NIPS) , pages 14–17. IEEE, 2019.
[145] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li,
Hai Li, and Yiran Chen. Learning low-rank deep neural networks via
singular vector orthogonality regularization and singular value sparsiﬁ-
cation. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops , pages 678–679, 2020.
[146] Jinmian Ye, Guangxi Li, Di Chen, Haiqin Yang, Shandian Zhe, and
Zenglin Xu. Block-term tensor neural networks. Neural Networks ,
130:11–21, 2020.
[147] Miao Yin, Huy Phan, Xiao Zang, Siyu Liao, and Bo Yuan. Batude:
Budget-aware neural network compression based on tucker decomposi-
tion. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,
2022.
[148] Miao Yin, Yang Sui, Siyu Liao, and Bo Yuan. Towards efﬁcient
tensor decomposition-based dnn model compression with optimization
framework. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10674–10683, 2021.
[149] Miao Yin, Yang Sui, Wanzhao Yang, Xiao Zang, Yu Gong, and Bo
Yuan. Hodec: Towards efﬁcient high-order decomposed convolutional
neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 12299–12308, 2022.[150] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen,
Makan Fardad, and Yanzhi Wang. A systematic dnn weight pruning
framework using alternating direction method of multipliers. In
Proceedings of the European Conference on Computer Vision (ECCV) ,
pages 184–199, 2018.
[151] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Acceler-
ating very deep convolutional networks for classiﬁcation and detec-
tion. IEEE transactions on pattern analysis and machine intelligence ,
38(10):1943–1955, 2015.
[152] Hengling Zhao, Yipeng Liu, Xiaolin Huang, and Ce Zhu. Semi-tensor
product-based tensordecomposition for neural network compression.
arXiv preprint arXiv:2109.15200 , 2021.
[153] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Ci-
chocki. Tensor ring decomposition. arXiv preprint arXiv:1606.05535 ,
2016.
[154] Max Zimmer, Christoph Spiegel, and Sebastian Pokutta. Compression-
aware training of neural networks using frank-wolfe. arXiv preprint
arXiv:2205.11921 , 2022.
[155] Jacob Ziv and Abraham Lempel. A universal algorithm for sequen-
tial data compression. IEEE Transactions on information theory ,
23(3):337–343, 1977.
[156] Jacob Ziv and Abraham Lempel. Compression of individual sequences
via variable-rate coding. IEEE transactions on Information Theory ,
24(5):530–536, 1978.
[157] Barret Zoph and Quoc V Le. Neural architecture search with rein-
forcement learning. arXiv preprint arXiv:1611.01578 , 2016.
[158] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal
component analysis. Journal of computational and graphical statistics ,
15(2):265–286, 2006.
