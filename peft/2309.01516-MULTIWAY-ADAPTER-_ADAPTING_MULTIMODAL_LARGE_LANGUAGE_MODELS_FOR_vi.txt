# 2309.01516.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.01516.pdf
# Kích thước tệp: 1371384 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MULTIWAY-ADAPTER: ĐIỀU CHỈNH CÁC MÔ HÌNH NGÔN NGỮ LỚN ĐA PHƯƠNG THỨC ĐỂ TRUY XUẤT HÌNH ẢNH-VĂN BẢN CÓ THỂ MỞ RỘNG

Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa
Đại học Glasgow, Scotland, Anh

TÓM TẮT
Khi các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) phát triển về kích thước, việc điều chỉnh chúng cho các tác vụ chuyên biệt trở nên ngày càng thách thức do nhu cầu cao về tính toán và bộ nhớ. Thật vậy, các phương pháp tinh chỉnh truyền thống tốn kém do cần đào tạo rộng rãi, đặc thù cho từng tác vụ. Trong khi tồn tại các phương pháp điều chỉnh hiệu quả nhằm giảm thiểu chi phí này, trên thực tế chúng gặp phải vấn đề căn chỉnh liên phương thức nông cạn, điều này làm tổn hại nghiêm trọng đến hiệu quả của mô hình. Để giải quyết những thách thức tính toán này và cải thiện căn chỉnh liên phương thức, chúng tôi giới thiệu MultiWay-Adapter (MWA), một framework mới với 'Alignment Enhancer'. Bộ tăng cường này làm sâu sắc hơn căn chỉnh liên phương thức, cho phép khả năng chuyển giao cao với nỗ lực điều chỉnh tối thiểu. Các thí nghiệm của chúng tôi cho thấy rằng không giống như các phương pháp điều chỉnh hiệu quả trước đây, MWA duy trì hiệu quả của mô hình trong khi giảm thời gian đào tạo lên đến 57%. MWA cũng nhẹ, chỉ tăng kích thước mô hình 2-3% (về mặt tham số) cho các mô hình nền tảng tiên tiến như BEiT-3 Large. Những kết quả này chứng minh rằng MWA cung cấp một phương pháp điều chỉnh hiệu quả và hiệu quả cho MLLM, mở rộng đáng kể khả năng ứng dụng của chúng.

Từ khóa—Mô hình Ngôn ngữ Lớn Đa phương thức, Truy xuất Hình ảnh-Văn bản, Adapter, Transformers, Học chuyển giao

1. GIỚI THIỆU
Những tiến bộ gần đây trong Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM), như BLIP2 [1] và BEiT-3 [2], đã chứng minh hiệu suất tiên tiến trong các tác vụ đa phương thức, được thể hiện qua khả năng của chúng trong Trả lời Câu hỏi Hình ảnh. Tuy nhiên, việc điều chỉnh các MLLM này cho các tác vụ hạ lưu chuyên biệt vẫn là một thách thức lớn, đặc biệt đối với truy xuất hình ảnh-văn bản, một trường hợp sử dụng phổ biến trong học đa phương thức. Tinh chỉnh toàn bộ truyền thống yêu cầu đào tạo lại cô lập, toàn diện cho mỗi tác vụ mới, đòi hỏi tài nguyên tính toán chuyên sâu và do đó hạn chế các ứng dụng thực tế. Ví dụ, đào tạo BLIP2-Giant trên GPU Nvidia A100 mất 144 ngày [1].

Với thách thức của việc tinh chỉnh MLLM, có nhu cầu ngày càng tăng để phát triển các phương pháp điều chỉnh hiệu quả cho MLLM [3, 4]. Trong khi đã có tiến bộ trong các lĩnh vực đơn phương thức sử dụng các mô-đun adapter, những phương pháp này vẫn chưa được khám phá nhiều trong bối cảnh đa phương thức, đặc biệt là cho truy xuất hình ảnh-văn bản. Hơn nữa, các phương pháp điều chỉnh hiện có cho MLLM [4, 5, 6] tập trung vào việc trích xuất thông tin từ các tập dữ liệu hạ lưu nhưng bỏ qua nhu cầu quan trọng về căn chỉnh liên phương thức. Mục tiêu của căn chỉnh liên phương thức là đưa các phương thức khác nhau vào một không gian đặc trưng chung nơi chúng có thể được so sánh, kết hợp hoặc liên kết một cách hiệu quả. Với căn chỉnh nông cạn, mô hình sẽ không thể nắm bắt được các mối quan hệ phức tạp giữa các phương thức khác nhau, từ đó ảnh hưởng đến hiệu quả của nó trong các tác vụ đa phương thức [7, 8, 9].

Để giải quyết vấn đề căn chỉnh liên phương thức nông cạn trong khi vẫn duy trì các lợi thế hiệu quả của phương pháp adapter, chúng tôi giới thiệu MultiWay-Adapter (MWA), một framework nhẹ nhưng hiệu quả được thiết kế đặc biệt cho việc điều chỉnh MLLM. Các thành phần bổ sung của MWA có kích thước nhỏ nhưng mang lại sự cải thiện hiệu suất đáng kể trong học chuyển giao với chi phí tinh chỉnh tối thiểu. Những đóng góp chính của chúng tôi bao gồm:

• Chúng tôi đề xuất MWA kết hợp phương pháp hai thành phần, cụ thể là New Knowledge Extractor và Modality Enhancer. MWA không chỉ trích xuất kiến thức mới từ các tập dữ liệu hạ lưu mà còn đảm bảo căn chỉnh liên phương thức sâu sắc, điều quan trọng cho hiệu suất vượt trội trong các tác vụ thị giác-ngôn ngữ. Theo hiểu biết của chúng tôi, bài báo này là công trình đầu tiên giảm thiểu vấn đề căn chỉnh liên phương thức nông cạn trong các phương pháp adapter cho MLLM.

• Thông qua các thí nghiệm toàn diện, chúng tôi chứng minh rằng MWA đạt được hiệu suất zero-shot vượt trội trên tập dữ liệu Flickr30k bằng cách chỉ điều chỉnh thêm 2.58% tham số cho mô hình BEiT-3 Large, tiết kiệm đến 57% thời gian tinh chỉnh so với tinh chỉnh toàn bộ mô hình. MWA cũng không cho thấy sự giảm hiệu suất có ý nghĩa thống kê trong các cài đặt khác, so với tinh chỉnh toàn bộ, yêu cầu ít tài nguyên hơn đáng kể.

• Kết quả thí nghiệm chứng minh tính mạnh mẽ của MWA khi các tham số được mở rộng, làm cho nó sẵn sàng cho các MLLM đang liên tục tăng kích thước.

• Nghiên cứu ablation của chúng tôi xác nhận hiệu quả của cả hai thành phần MWA, chứng thực cho các lựa chọn thiết kế của chúng tôi.

--- TRANG 2 ---
[Hình 1 mô tả so sánh giữa MultiWay Transformer và fine-tuning MultiWay-Adapter]

Hình 1. So sánh MultiWay Transformer và fine-tuning MultiWay-Adapter của chúng tôi. MultiWay-Adapter sử dụng thiết kế hai thành phần, bao gồm New Knowledge Extractor và Alignment Enhancer. Chúng tôi thay thế FFN gốc bằng New Knowledge Extractor: nhánh đông lạnh (trái) và mô-đun bottleneck có thể huấn luyện (phải). Hơn nữa, chúng tôi thêm Alignment Enhancer lên trên FFN gốc để tăng cường căn chỉnh liên phương thức.

2. CÔNG TRÌNH LIÊN QUAN
Thách thức trong Điều chỉnh Mô hình Đa phương thức Lớn. Gần đây, việc tăng kích thước mô hình đã được chứng minh là một chiến lược hiệu quả để cải thiện hiệu suất. Các mô hình như BEiT-3 [2] và BLIP-2 [1], với tương ứng 1.9 tỷ và 12.1 tỷ tham số, đã thiết lập kết quả tiên tiến mới trong các tác vụ đa phương thức như Trả lời Câu hỏi Hình ảnh. Tuy nhiên, việc ứng dụng chúng vào các tác vụ hạ lưu chuyên biệt thường bị hạn chế bởi các ràng buộc tính toán [10, 11, 12, 13, 14, 15]. Ví dụ, yêu cầu bộ nhớ GPU lớn trong tinh chỉnh toàn bộ hạn chế việc điều chỉnh chúng cho các tác vụ chuyên biệt trên phần cứng thông thường, ví dụ 45GB cho tinh chỉnh toàn bộ mô hình BEiT-3 Large.

Phương pháp Học chuyển giao Hiệu quả. Thách thức về hiệu quả tính toán trong tinh chỉnh MLLM đã dẫn đến các phương pháp Học chuyển giao Hiệu quả Tham số (PETL). Chúng được phân loại rộng rãi thành cập nhật tham số từng phần [16] và bổ sung mô-đun [17, 4]. Cái trước tốn nhiều tài nguyên và đặc thù cho mô hình, trong khi cái sau thêm các mô-đun mới vào kiến trúc, chỉ cập nhật những thành phần này. Tuy nhiên, hầu hết các nghiên cứu chỉ tập trung vào các tác vụ đơn phương thức trong các lĩnh vực như thị giác [18], văn bản [19] hoặc âm thanh [20, 21, 22], bỏ qua các tác vụ đa phương thức. Một số công trình [4, 5, 16, 23] nhắm vào các tác vụ đa phương thức nhưng gặp phải vấn đề căn chỉnh liên phương thức nông cạn. Công trình của chúng tôi giới thiệu MultiWay-Adapter, được thiết kế cho học chuyển giao MLLM hiệu quả và tăng cường căn chỉnh liên phương thức.

3. PHƯƠNG PHÁP
Chúng tôi giới thiệu MultiWay-Adapter (MWA), được thiết kế để chuyển giao hiệu quả các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) sang các tác vụ hạ lưu. Mặc dù trọng tâm chính của bài báo này là các tác vụ truy xuất hình ảnh-văn bản, khả năng ứng dụng của MWA rộng hơn, như truy xuất văn bản video và tạo chú thích hình ảnh.

Kiến thức cơ bản. Framework tổng thể được xây dựng dựa trên kiến trúc phổ biến của MLLM, sử dụng thiết kế MultiWay Transformer [2]. Như được mô tả ở bên trái Hình 1, mỗi khối MultiWay Transformer bao gồm một mô-đun self-attention chung và một tập hợp các mạng feed-forward (tức là các chuyên gia phương thức) được điều chỉnh cho các phương thức khác nhau. Thiết kế này tương tự như kiến trúc hai backbone của các mô hình đa phương thức, ví dụ một encoder cho đầu vào thị giác và một encoder khác cho đầu vào ngôn ngữ, nhưng khác biệt bằng cách chia sẻ các trọng số trong mỗi mô-đun self-attention. Lựa chọn thiết kế này giảm số lượng tham số và tăng cường căn chỉnh liên phương thức—một chất lượng thiết yếu cho các tác vụ đa phương thức hiệu suất cao [2].

3.1. MultiWay-Adapter
Kiến trúc Tổng thể. MWA đề xuất của chúng tôi sử dụng phương pháp hai thành phần: New Knowledge Extractor và Alignment Enhancer, như được minh họa ở bên phải Hình 1.

New Knowledge Extractor. New Knowledge Extractor được thiết kế để trích xuất kiến thức mới từ các tác vụ hạ lưu đích. Trái ngược với tinh chỉnh toàn bộ MultiWay Transformers thông thường, chúng tôi thay thế cả hai mạng feed-forward (FFN) trong khối transformer bằng New Knowledge Extractor. Bộ trích xuất này bao gồm hai nhánh: nhánh trái, giống hệt với mạng gốc, và một nhánh phải bổ sung được giới thiệu cho tinh chỉnh đặc thù tác vụ. Nhánh sau sử dụng cấu trúc bottleneck để hạn chế số lượng tham số và bao gồm một lớp down-projection và một lớp up-projection. Chính thức, đối với một đặc trưng đầu vào cụ thể xi′, nhánh phải của New Knowledge Extractor tạo ra

--- TRANG 3 ---
các đặc trưng đã điều chỉnh, ˜xi, như sau:
˜xi=ReLU (LN(xi′)·Wdown)·Wup (1)

Ở đây, Wdown∈Rd×ˇd và Wup∈Rˇd×d biểu thị các lớp down-projection và up-projection, tương ứng. ˇd là chiều giữa bottleneck và thỏa mãn ˇd≪d. LN biểu thị LayerNorm. Mô-đun bottleneck này được kết nối với FFN gốc (nhánh trái) thông qua một kết nối residual qua một hệ số tỷ lệ α. Sau đó, các đặc trưng này, xi′ và ˜xi, được kết hợp với cái gốc, xi, thông qua một kết nối residual:

xi=FFN (LN(xi′)) +α·˜xi+x′i (2)

Alignment Enhancer. Sau khi trích xuất kiến thức mới từ tác vụ hạ lưu đích, để duy trì và cải thiện căn chỉnh liên phương thức, một mô-đun Alignment Enhancer được thêm vào trên tập hợp các mạng feed-forward. Mô-đun này bắt chước kiến trúc của New Knowledge Extractor nhưng sử dụng chiều giữa lớn hơn để tạo điều kiện cho việc hợp nhất và căn chỉnh đặc trưng tốt hơn.

Trong giai đoạn tinh chỉnh, chỉ các tham số của những mô-đun mới thêm này được tối ưu hóa, trong khi phần còn lại của mô hình bị đông lạnh (như được chỉ ra bởi dấu đông lạnh trong Hình 1). Chiến lược này làm cho MWA trở thành một mô-đun plug-and-play, có thể áp dụng cho các MLLM khác, như CLIP [26], VLMo [27], và ALIGN [25].

4. THÍ NGHIỆM
Thiết lập. Chúng tôi tiến hành thí nghiệm trên hai MLLM tiên tiến, BEiT-3 Base và BEiT-3 Large, trên hai tập dữ liệu truy xuất hình ảnh-văn bản được sử dụng rộng rãi: MSCOCO [28] và Flickr30K [29]. Chúng tôi sử dụng tập test 5k của MSCOCO và tập test 1k của Flickr30k để báo cáo các chỉ số, phù hợp với các nghiên cứu trước [28, 29]. Chúng tôi khởi tạo backbone, loại trừ các mô-đun bổ sung của chúng tôi, bằng các trọng số được huấn luyện trước, được đông lạnh trong quá trình tinh chỉnh khi sử dụng MultiWay-Adapter. Để tinh chỉnh, kích thước batch là 512 cho mô hình Large và 1024 cho mô hình Base, trong 20 epoch với tốc độ học ban đầu là 0.001. Các chiều giữa cho New Knowledge Extractor và Alignment Enhancer được đặt lần lượt là 64 và 128. Tất cả mã được sử dụng trong thí nghiệm của chúng tôi có thể tìm thấy tại https://github.com/longkukuhi/MultiWay-Adapter.

Kết quả Thí nghiệm. Mục tiêu của thí nghiệm này là đánh giá hiệu quả và hiệu suất của framework MWA so với các phương pháp tinh chỉnh toàn bộ truyền thống. Chúng tôi so sánh phương pháp MWA với tinh chỉnh toàn bộ trong hai cài đặt khác nhau: hiệu suất tinh chỉnh và hiệu suất zero-shot.

Hiệu suất Tinh chỉnh: Như được thể hiện trong Bảng 1, phương pháp MWA của chúng tôi chứng minh hiệu quả tính toán vượt trội. Cụ thể, nó chỉ sử dụng 3.21% và 2.58% tham số có thể huấn luyện cho các biến thể Base và Large của BEiT-3, tương ứng, trái ngược với tinh chỉnh toàn bộ thông thường. Điều này dẫn đến sự giảm đáng kể trong tiêu thụ bộ nhớ GPU—7GB và 9GB cho các biến thể Base và Large, tương ứng. Hơn nữa, MWA giảm đáng kể thời gian cần thiết cho tinh chỉnh. Ví dụ, tinh chỉnh MWA với mô hình BEiT-3 Base giảm 57% so với tinh chỉnh toàn bộ.

Về hiệu quả, sự suy giảm hiệu suất khi sử dụng MWA là không có ý nghĩa thống kê cho cả hai biến thể BEiT-3 Base và Large, với các độ lệch nằm trong biên độ ít hơn 1%. Tổng hợp các thuộc tính hiệu quả và hiệu suất này chứng minh rằng MWA, khi áp dụng cho mô hình BEiT-3 Large, chỉ tiêu thụ 86% thời gian cần thiết để tinh chỉnh toàn bộ mô hình BEiT-3 Base, nhưng vượt trội hơn về hiệu suất. Điều này cho thấy MWA cho phép hiệu suất tăng cường với thời gian tính toán giảm, đặc biệt là cho các mô hình lớn hơn. Ngoài ra, khi kích thước mô hình tăng, sự khác biệt hiệu suất giữa MWA và tinh chỉnh toàn bộ giảm, cho thấy mối tương quan tích cực giữa hiệu quả của MWA và kích thước mô hình.

Hiệu suất Zero-Shot: Để đánh giá khả năng chuyển giao của MWA và các phương pháp tinh chỉnh toàn bộ, chúng tôi tiến hành thí nghiệm trong cài đặt zero-shot. Trong cài đặt này, mô hình được đánh giá trên Flickr30k (tập test 1k), mà nó không có kiến thức trước về nó, do đó cần dựa vào kiến thức được học nội tại để mô phỏng việc xử lý các mẫu chưa từng thấy. Những mô hình này ban đầu được tinh chỉnh trên tập dữ liệu MSCOCO. Như được thể hiện trong Bảng 2, MWA vượt trội so với hiệu suất tinh chỉnh toàn bộ khi được sử dụng với mô hình BEiT-3 Large. Chúng tôi giả định rằng sự cải thiện này là do việc bảo tồn kiến thức có thể khái quát hóa trong các trọng số đông lạnh, kiến thức có thể bị mất trong quá trình tinh chỉnh toàn bộ. Kiến thức được giữ lại này tăng cường khả năng của mô hình để xử lý khéo léo các trường hợp chưa thấy. Do đó, MWA không chỉ phù hợp với hiệu suất của phương pháp tinh chỉnh toàn bộ mà còn phân biệt bản thân về mặt hiệu quả tài nguyên và khả năng chuyển giao.

Tóm lại, kết quả thí nghiệm chứng minh rằng MWA phục vụ như một phương pháp tinh chỉnh hiệu quả và tiết kiệm tài nguyên cho MLLM, đặc biệt khi tài nguyên tính toán bị ràng buộc.

5. PHÂN TÍCH
Mở rộng Tham số Có thể Điều chỉnh: Mục tiêu chính của phần này là điều tra tác động của việc thay đổi số lượng tham số có thể điều chỉnh đối với hiệu suất và xác định giá trị tối ưu cho các tham số bổ sung. "Mid-dimension" của New Knowledge Extractor phần lớn kiểm soát số lượng tham số có thể điều chỉnh. Chúng tôi tiến hành đánh giá thực nghiệm trên một phạm vi các chiều giữa {0, 1, 16, 32, 64, 128} trên tập dữ liệu MSCOCO sử dụng mô hình BEiT-3 Base. Kết quả được tóm tắt trong Hình 2. Dữ liệu tiết lộ sự gia tăng đáng chú ý trong hiệu suất khi chiều tăng, đạt đỉnh ở 64. Cụ thể, chúng tôi quan sát được hiệu suất tăng

--- TRANG 4 ---
[Bảng 1 và 2 với các kết quả thí nghiệm]

Bảng 1. Phân tích So sánh Tinh chỉnh Toàn bộ và MultiWay-Adapter: Bảng hiển thị các chỉ số recall Top-1 trên tập dữ liệu COCO và Flickr30k, được trình bày dưới dạng cả giá trị tuyệt đối và khoảng cách tương đối so với Mô hình Tinh chỉnh Toàn bộ BEiT-3 Base. Các chỉ số cho Truy xuất Văn bản-sang-Hình ảnh (IR) và Truy xuất Hình ảnh-sang-Văn bản (TR) được cung cấp. Sử dụng bộ nhớ GPU và thời gian huấn luyện cũng được bao gồm. Thời gian huấn luyện được đo bằng một GPU NVIDIA A6000 với bộ nhớ 48GB cho một epoch.

Bảng 2. Hiệu suất zero-shot trên Flickr30k.

Hình 2. Đánh giá các kích thước khác nhau của mid-dimension New Knowledge Extractor trên MSCOCO.

tối đa 9.45% trong truy xuất văn bản sang hình ảnh khi tăng chiều từ 1 lên 64. Điều này chỉ ra rằng việc tăng số lượng tham số trong adapter không đảm bảo cải thiện hiệu suất. Khi chiều được đặt bằng không, nó đại diện cho hiệu suất zero-shot của mô hình BEiT-3 Base mà không có MWA. Đáng chú ý, MWA mang lại hiệu suất vượt trội so với hiệu suất zero-shot của mô hình BEiT-3 Base, ngay cả khi mid-dimension chỉ là một. Hơn nữa, biến động hiệu suất tương đối nhỏ khi tăng chiều từ 16 đến 64, cho thấy MWA ổn định trong điều chỉnh và không nhạy cảm với thay đổi kích thước.

Ablation về Các Thành phần của MultiWay Adapter: Trong phần này, trọng tâm của chúng tôi là định lượng đóng góp riêng lẻ của hai thành phần mới giới thiệu: New Knowledge Extractor và Alignment Enhancer. Một nghiên cứu ablation

[Bảng 3 hiển thị kết quả ablation study]

Bảng 3. Nghiên cứu ablation của hai mô-đun của MultiWay-Adapter. KE đề cập đến New Knowledge Extractor và AE đề cập đến Alignment Enhancer.

được thực hiện trên tập dữ liệu MSCOCO sử dụng mô hình BEiT-3 Base. Các chỉ số hiệu suất cho mỗi thành phần, cả riêng lẻ hay kết hợp, được chi tiết trong Bảng 3. Phát hiện của chúng tôi chứng minh rằng việc bỏ qua một trong hai thành phần dẫn đến sự suy giảm hiệu suất đáng kể, khoảng 3% cho truy xuất hình ảnh sang văn bản và khoảng 4% cho truy xuất văn bản sang hình ảnh. Quan trọng là, Alignment Enhancer, một yếu tố mới khác biệt với các phương pháp Adapter trước đây, xác nhận vai trò quan trọng của nó trong việc duy trì căn chỉnh sâu giữa các phương thức thông qua các cải thiện hiệu suất được quan sát. Tóm lại, cả hai thành phần không chỉ đóng góp đáng kể vào hiệu suất tổng thể mà còn bổ sung cho nhau một cách hiệu quả.

6. KẾT LUẬN
Chúng tôi giới thiệu MultiWay-Adapter (MWA), một framework hiệu quả được thiết kế để điều chỉnh hiệu quả các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) cho các tác vụ hạ lưu. Giải quyết vấn đề căn chỉnh liên phương thức nông cạn trong các phương pháp hiện có, MWA sử dụng phương pháp hai thành phần, tận dụng cả New Knowledge Extractor và Alignment Enhancer. Chiến lược này cho phép MWA không chỉ trích xuất thông tin mới từ các tập dữ liệu hạ lưu mà còn đảm bảo căn chỉnh liên phương thức sâu sắc. Các phát hiện thực nghiệm của chúng tôi tiết lộ rằng với việc bổ sung chỉ 2.58% tham số thêm, không có sự suy giảm hiệu suất có ý nghĩa thống kê trong tất cả các cài đặt được thử nghiệm trong khi giảm thời gian tinh chỉnh đến 57%. Nghiên cứu của chúng tôi mở đường cho các nghiên cứu tương lai về các phương pháp tinh chỉnh đa phương thức hiệu quả và có tiềm năng mở rộng sang các tác vụ thị giác-ngôn ngữ khác.

--- TRANG 5 ---
7. TÀI LIỆU THAM KHẢO
[1] Junnan Li, Dongxu Li, Silvio Savarese, et al., "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models," trong Proc. ICML, 23-29 July 2023, Honolulu, Hawaii, USA.

[2] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, và et al., "Image as a foreign language: Beit pretraining for all vision and vision-language tasks," trong Proc. CVPR, 2023.

[3] Edward J. Hu, Yelong Shen, et al., "Lora: Low-rank adaptation of large language models," trong Proc. ICLR, 2022.

[4] Yi-Lin Sung, Jaemin Cho, và Mohit Bansal, "VL-ADAPTER: parameter-efficient transfer learning for vision-and-language tasks," trong Proc. CVPR, 2022.

[5] Xiaohua Zhai, Xiao Wang, Basil Mustafa, et al., "Lit: Zero-shot transfer with locked-image text tuning," trong Proc. CVPR.

[6] Shoufa Chen, Chongjian Ge, Zhan Tong, et al., "Adaptformer: Adapting vision transformers for scalable visual recognition," trong Proc. NeurIPS, 2022.

[7] Tadas Baltrušaitis, Chaitanya Ahuja, và Louis-Philippe Morency, "Multimodal machine learning: A survey and taxonomy," TPAMI, vol. 41, no. 2, pp. 423–443, 2018.

[8] Weijie Su, Xizhou Zhu, Yue Cao, et al., "VL-BERT: pre-training of generic visual-linguistic representations," trong Proc. ICLR, 2020.

[9] Zijun Long và Richard Mccreadie, "Is multi-modal data key for crisis content categorization on social media?," trong Proc. ISCRAM. May 2022, Tarbes, France.

[10] Zijun Long, Richard Mccreadie, và Imran Muhammad, "Crisisvit: A robust vision transformer for crisis image classification," trong Proc. ISCRAM, May 2023.

[11] Zijun Long, George Killick, et al., "Robollm: Robotic vision tasks grounded on multimodal large language models," arXiv preprint arXiv:2310.10221, 2023.

[12] Zixuan Yi, Zijun Long, Iadh Ounis, et al., "Large multimodal encoders for recommendation," arXiv preprint arXiv:2310.20343, 2023.

[13] Zijun Long, George Killick, Lipeng Zhuang, et al., "Elucidating and overcoming the challenges of label noise in supervised contrastive learning," arXiv preprint arXiv:2311.16481, 2023.

[14] Zijun Long, Zaiqiao Meng, et al., "Lacvit: A labelaware contrastive training framework for vision transformers," trong Proc. ICASSP, 2024, Seoul.

[15] Zijun Long và Richard Mccreadie, "Automated crisis content categorization for covid-19 tweet streams," trong Proc. ISCRAM. May 2021, Blacksburg, VA, USA.

[16] Zhao Song, Ke Yang, Naiyang Guan, et al., "Vppt: Visual pre-trained prompt tuning framework for few-shot image classification," trong Proc. ICASSP, 2023, pp. 1–5.

[17] Junyi Peng, Themos Stafylakis, Rongzhi Gu, et al., "Parameter-efficient transfer learning of pre-trained transformer models for speaker verification using adapters," trong Proc. ICASSP, 2023, pp. 1–5.

[18] Sylvestre-Alvise Rebuffi, Hakan Bilen, et al., "Learning multiple visual domains with residual adapters," trong Proc. NeurIPS, December 4-9, 2017, USA.

[19] Neil Houlsby, Andrei Giurgiu, et al., "Parameter-efficient transfer learning for NLP," trong Proc. ICML, 9-15 June 2019, Long Beach, California, USA.

[20] Bethan Thomas, Samuel Kessler, et al., "Efficient adapter transfer of self-supervised speech models for automatic speech recognition," trong Proc. ICASSP, 2022.

[21] Steven Vander Eeckt và Hugo Van hamme, "Using adapters to overcome catastrophic forgetting in end-to-end automatic speech recognition," trong Proc. ICASSP. 2023, pp. 1–5, IEEE.

[22] Samuel Kessler, Bethan Thomas, et al., "An adapter based pre-training for efficient and scalable self-supervised speech representation learning," trong Proc. ICASSP. 2022, pp. 3179–3183, IEEE.

[23] Odysseas S. Chlapanis, Georgios Paraskevopoulos, et al., "Adapted multimodal bert with layer-wise fusion for sentiment analysis," trong Proc. ICASSP, 2023, pp. 1–5.

[24] Junnan Li, Ramprasaath Selvaraju, et al., "Align before fuse: Vision and language representation learning with momentum distillation," trong Proc. NeurIPS, 2021.

[25] Chao Jia, Yinfei Yang, et al., "Scaling up visual and vision-language representation learning with noisy text supervision," trong Proc. ICML. PMLR, 2021.

[26] Alec Radford, Jong Wook Kim, et al., "Learning transferable visual models from natural language supervision," trong Proc. ICML. PMLR, 2021.

[27] Hangbo Bao, Wenhui Wang, et al., "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts," trong Proc. NeurIPS, 2022.

[28] Tsung-Yi Lin, Michael Maire, et al., "Microsoft coco: Common objects in context," trong Proc. ECCV, 2014.

[29] Bryan A Plummer, Wang, et al., "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models," trong Proc. ICCV, 2015.
