# 2309.06759.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.06759.pdf
# File size: 885385 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Scaled Prompt-Tuning for Few-Shot Natural Language Generation
Ting Hu
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
ting.hu@hpi.deChristoph Meinel
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
meinel@hpi.deHaojin Yang
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
haojin.yang@hpi.de
Abstract
The increasingly Large Language Models
(LLMs) demonstrate stronger language under-
standing and generation capabilities, while the
memory demand and computation cost of fine-
tuning LLMs on downstream tasks are non-
negligible. Besides, fine-tuning generally re-
quires a certain amount of data from individual
tasks whilst data collection cost is another issue
to consider in real-world applications. In this
work, we focus on Parameter-Efficient Fine-
Tuning (PEFT) methods for few-shot Natu-
ral Language Generation (NLG), which freeze
most parameters in LLMs and tune a small
subset of parameters in few-shot cases so that
memory footprint, training cost, and labeling
cost are reduced while maintaining or even im-
proving the performance. We propose a Scaled
Prompt-Tuning (SPT) method which surpasses
conventional PT with better performance and
generalization ability but without an obvious
increase in training cost. Further study on in-
termediate SPT suggests the superior transfer-
ability of SPT in few-shot scenarios, provid-
ing a recipe for data-deficient and computation-
limited circumstances. Moreover, a comprehen-
sive comparison of existing PEFT methods re-
veals that certain approaches exhibiting decent
performance with modest training cost such as
Prefix-Tuning in prior study could struggle in
few-shot NLG tasks, especially on challenging
datasets.
1 Introduction
With the emergence and development of Large
Language Models (LLMs), fine-tuning already be-
comes the mainstream paradigm in Natural Lan-
guage Processing regardless of the recently emer-
gent powerful foundation models such as GPT-4
and the comcomitant prompt engineering. Though
fine-tuning offers an effective means of transferring
the pre-trained knowledge to downstream tasks and
necessitates a small quantity of textual data in com-
parison to pre-training corpora, it still demands asignificant amount of memory on device and en-
tails substantial training cost. These facilitate the
development of another research topic, Parameter-
Efficient Fine-Tuning (PEFT), which freezes most
of the parameters in LLMs and merely tunes a small
portion of them, resulting in reduced memory foot-
print and computation expense with comparable
performance to conventional fine-tuning. Further-
more, the emergence of in-context learning (Brown
et al., 2020) instills optimism for few-shot scenar-
ios and draw considerable research interest in few-
shot PEFT methods, dedicated to the least cost of
adopting LLMs to data-scarce and resource-limited
scenarios.
The core question of PEFT is which parameters
in LLMs are to be tuned so that we could tune
as fewer parameters as possible with the least per-
formance drop on downstream tasks. Many ap-
proaches, such as Adapter (Houlsby et al., 2019)
and Prompt-Tuning (Lester et al., 2021), varying
in trainable parameters and performance have been
proposed and applied in various fields. Despite
some recent research on few-shot PEFT for Nat-
ural Language Understanding (NLU) tasks, our
comprehension of PEFT methods on Natural Lan-
guage Generation (NLG) tasks, including Mean-
ing Representation (MR)-to-text and Knowledge
Graph (KG)-to-text generation, in few-shot cases
is insufficient, which motivates us to delve into the
details. We conclude our contributions below.
•We put forward Scaled Prompt-Tuning (SPT)
which drastically outcompetes conventional
Prompt-Tuning with negligible extra trainable
parameters.
•SPT demonstrates better transferability than
fine-tuning in few-shot cases, which provides
a recipe in resource-limited environments
without extra labeling cost via intermediate
SPT.
•The comprehensive comparison of existingarXiv:2309.06759v1  [cs.CL]  13 Sep 2023

--- PAGE 2 ---
PEFT methods manifests that approaches that
perform decently when a sufficient number
of data instances are available such as Prefix-
Tuning could face hurdles in few-shot cases,
especially on challenging datasets.
2 Related work
PEFT approaches strive to tune a fraction of param-
eters and freeze most of the parameters in LLMs
which demands less memory footprint and energy
consumption. These methods differ in which pa-
rameters are tuned on downstream tasks. The very
first PEFT work is Adapter (Houlsby et al., 2019),
where small trainable bottleneck modules are in-
serted in BERT (Devlin et al., 2018) layers per
task. More specific, the adapters are inserted at
two positions of each layer: after the projection
following Multi-Head Attention module and after
two Feed-Forward Networks (FFNs). By tuning
task-specific Adapters on GLUE benchmark (Wang
et al., 2018), the performance drop is within 0.4%
of that of fine-tuning, while only 3.6% of the param-
eters are added. Intuitively, the insertion positions
of the Adapters and the possibility of multi-task
adapters are interesting questions. He et al. (2021)
further demonstrate that adding adapters after the
FFNs is sufficient to encapsulate and refine the
task-specific information from the frozen parame-
ters, since FFNs can better utilize modification at
larger capacities. This effectively reduces 50% of
inserted adapters in previous work. On the other
hand, AdapterFusion (Pfeiffer et al., 2020) con-
ducts a two-stage learning algorithm: task-specific
adapters training and combining adapters in the
fusion module. The authors show that combining
the knowledge from different tasks obtained by
their corresponding adapters could be beneficial
for each individual task. Despite their less mem-
ory demands and tuning costs, adapters actually
introduce around 4-6% extra inference time, since
all parameters of BERT and inserted adapters are
involved in inference. AdapterDrop (Rücklé et al.,
2020) further proposes to drop a variable number of
adapters from lower BERT layers. It dynamically
reduces the computational overhead at run-time
when performing inference over multiple tasks and
maintains task performance to a large extent. Con-
sidering the Fully Connected (FC) layers in the
bottleneck adapters still have a relatively large num-
ber of parameters, Compacter (Karimi Mahabadi
et al., 2021) introduces more efficient Parameter-ized Hypercomplex Multiplication (PHM) Layers
to replace the FC layers, where the weight of each
FC layer is computed as the sum of several Kro-
necker products. Karimi Mahabadi et al. (2021)
further reduce the trainable parameters by letting
the adapters across layers share slow weights and
vary in fast rank-one matrices, resulting in Com-
pacter++. Compacter works on par with fine-tuning
when applied to T5-Base (Raffel et al., 2020) on
GLUE benchmark by only training 0.047% of the
parameters.
Another line of work insert extra trainable param-
eters in other formats instead of the bottleneck mod-
ules, including Prompt-Tuning and Prefix-Tuning.
Prompt-Tuning is different from prompting, i.e.,
in-context learning. In-context learning blooms
as the emergence of GPT-3 (Brown et al., 2020),
which could directly adapt to some downstream
tasks by inputting prompts instead of tuning param-
eters. The prompts usually have some descriptions
of the tasks followed by several exemplars and
specific content that we want the model to help
with. Prompt-Tuning (Lester et al., 2021) prepends
a trainable soft prompt to the input embeddings
of the model for specific downstream tasks. Only
the continuous prompts are updated during train-
ing. This methods performs well when applied on
models that are pretrained in a multi-task setting
such as T5. However, the impact of the soft prompt
could become weaker and weaker as a model goes
deeper, then another line of work that more ef-
fectively modify the representations in the mod-
els show up. Prefix-Tuning (Li and Liang, 2021)
prepends a trainable continuous prefix to the in-
put of each layer for an decoder-style model and
two prefixes for the encoder-decoder model, sepa-
rately. By learning only 0.1% of the parameters of
BART (Lewis et al., 2019), Prefix-Tuning obtains
comparable performance to fine-tuning on table-to-
text generation tasks. P-Tuning v2 (Liu et al., 2021)
shares a similar idea to Prefix-Tuning, which inserts
multi-layer prompts in the model while studying
the performance on NLU tasks.
Other methods explore tuning other parameters
in LLMs. Intrinsic SAID (Aghajanyan et al., 2020)
empirically shows that LLMs have very low in-
trinsic dimensions and proposes to tune the pa-
rameters in a lower-dimension subspace, which is
achieved by a random linear projection via Fast-
food transform. FISH Mask (Sung et al., 2021)]
selects a subset of parameters to update based on

--- PAGE 3 ---
Method Base model Tasks Few-shot tasks
Adapter (Houlsby et al., 2019) BERT GLUE -
LoRA (Hu et al., 2021) RoBERTa, DeBERTa,
GPT-2GLUE, WikiSQL,
SAMSumRTE, MNLI
Compacter (Karimi Mahabadi et al., 2021) T5 GLUE, SuperGLUE -
Unified (He et al., 2021) BART, RoBERTa XSum, WMT 2016,
MNLI, SST-2-
IA3 (Liu et al., 2022) T0 T0 tasks RAFT
UniPELT (Mao et al., 2021) BERT GLUE GLUE
Prompt-Tuning (Lester et al., 2021) T5 GLUE, SuperGLUE -
OptiPrompt (Zhong et al., 2021) BERT LAMA -
WARP (Hambardzumyan et al., 2021) RoBERTa GLUE FewGLUE
P-Tuning v2 (Liu et al., 2021) GPT-2, BERT LAMA, SuperGLUE -
Prefix-Tuning (Li and Liang, 2021) GPT-2, BART NLG, XSum FewGLUE
Table 1: Comparison of PEFT methods. SuperGLUE is a stickier benchmark originating from GLUE. FewGLUE
is a benchmark for few-shot SuperGLUE tasks. RAFT is a real-world few-shot text classification benchmark.
SAMSum and XSum are text summrization tasks. LAMA is a probe for analyzing the factual and commonsense
knowledge contained in LLMs. WMT 2016 is a machine translation task.
their estimated Fisher information. BitFit (Zaken
et al., 2021) demonstrates that solely tuning the
bias terms in BERT is competitive with fine-tuning
with small-to-medium training data scale. It raises
the hypothesis that fine-tuning is mainly about ex-
posing knowledge induced by language-modeling
training, rather than learning new task-specific lin-
guistic knowledge. LoRA (Hu et al., 2021) injects
trainable rank decomposition matrices into each
Transformer layer based on the hypothesis that the
change in weights during model adaptation has a
low intrinsic rank. It does not introduce inference
latency and reduce input sequence length while
retaining high performance on RoBERTa, GPT-2
and even GPT-3. (IA)3(Liu et al., 2022) intro-
duces three learned vectors to rescale the Keys
and Values in the attention modules and the inner
activations of the point-wise FFNs, respectively,
via element-wise multiplication. This approach is
conducted on T0 (Sanh et al., 2021) which is pre-
trained in a multi-prompt and multi-task manner.
UniPELT (Mao et al., 2021) further incorporates
different PEFT methods as sub-modules, including
Adapters, LoRA, and Prefix-Tuning, and learns to
activate the ones that suit the current data or task
setup the best via a gating mechanism on GLUE
tasks. Similarly, He et al. (2021) breaks down the
design of various PEFT methods and re-frame them
as modifications to specific hidden states in LLMs.
It establishes a unified framework with Adapters,
Prefix-Tuning, and LoRA in BART, and conducts
experiments on NLU, summarization and machine
translation tasks.
We compare above methods from different as-
pects in Tab. 1. As we can see, there are few ex-isting PEFT approaches centering on NLG tasks,
not to mention few-shot NLG tasks. Therefore, the
understanding and comprehension of the character-
istics of PEFT methods on NLG tasks in few-shot
cases is deficient, which we focus on in this work.
Moreover, there are some related work proposing
scaling related ideas which we compare with below.
(IA)3(Liu et al., 2022) introduces three learned
vectors to rescale the Keys, Values, and inner ac-
tivations. He et al. (2021) propose scaled parallel
adapters and demonstrate the scaling factor is sig-
nificant for parallel adapters. Our proposed Scaled
Prompt-Tuning could be regarded as a simplified
variant of them which does not pose any extra mod-
ification to each Transformer layer and tune much
less parameters than theirs.
3 Method
Conventional Prompt-Tuning freezes the parame-
ters of LLMs and solely tunes the embeddings of
kadditional tokens, i.e., soft prompt, for individ-
ual downstream tasks. Assume the input sequence
withltokens is X={x1, x2, . . . , x l}. After go-
ing through the embedding layer, the sequence is
represented by a matrix Xe∈Rl×ne, where ne
is the hidden dimension of the embeddings. The
trainable soft prompt, denoted as Xp∈Rk×ne,
is prepended to the input sequence embedding
Xe, resulting in the update embedding matrix
Xh= [Xp;Xe]∈R(k+l)×ne, which is fed into
the later blocks of LLMs for further computation,
while only Xpis optimized during training.
We propose Scaled Prompt-Tuning where the
trainable soft prompt Xpwith an additional train-

--- PAGE 4 ---
able scaling vector s∈Rk×1are employed. Con-
sequently, the updated embedding matrix is Xh=
[s⊙Xp;Xe], where ⊙denotes Hadamard product.
That is, each scaling factor in the scaling vector s
is applied to scale the embedding of an individual
soft token, yielding narrowed representation gap
between the soft prompt and input sequence embed-
dings. Other alternatives could be using a single-
value scaling factor or a scaling matrix s∈Rk×ne,
while both of them empirically underperform the
proposed one-dimension scaling vector. Fig. 1 de-
picts the proposed SPT method, and the encoder-
decoder LLM we work on is T5-large (Raffel et al.,
2020) with a total 770M parameters.
Figure 1: The proposed Scaled Prompt-Tuning. The
parameters of the encoder and decoder are frozen, while
the soft prompt and scaling vector are trained for down-
stream tasks.
4 Experiments
4.1 Data and evaluation metrics
Experiments are conducted on three NLG datasets:
WebNLG 20201, E2E2, and DART3. Since they
contain a large number of instances, we sample
subsets of instances from each dataset for few-shot
tuning. The sampling process is implemented three
times for each few-shot scenario. WebNLG 2020
has 16 categories of KGs, and we sample from each
category. E2E dataset has no definition of category,
then we differentiate the instances based on the
number of slot-value pairs in MRs, thereby cate-
gorizing them into 6 distinct groups and sampling
from each group. The samples in DART are from
6 sources and we sample from each source.
The transformation of structured data into the
inputs of LLMs is significant. This is related to
1https://synalp.gitlabpages.inria.fr/
webnlg-challenge/challenge_2020
2https://github.com/tuetschek/
e2e-metrics
3https://github.com/Yale-LILY/dartthe pre-training paradigm of the LLM that PEFT
methods are applied to. In this work, the model
we adopt is T5, pre-trained in a text-to-text manner.
Therefore, we employ the structured data transfor-
mation method in Tab. 2. For WebNLG 2020 and
DART, we linearize the triples in KGs and prepend
token <S>, <P>, and <O> to the subject, predicate,
and object of each triple, respectively. Regarding
E2E, we linearize the slot-value pairs in MRs, and
insert token <S> and <V> before the slot and value,
separately. This transformation process is demon-
strated to be effective according to prior work ( ?)
and our experiments. The token <S>, <P>, <O>,
and <V> are delimiters that maintain the structural
information to some degree in the transformed text
sequences.
For each dataset, we employ the metrics pro-
vided in the benchmark for evaluation. WebNLG
2020 applies BLEU, METEOR, chrF++, TER,
BERTScore, and BLEURT. E2E uses BLEU, NIST,
METEOR, ROUGE-L, and CIDEr. DART employs
BLEU, METEOR, TER, BERTScore, MoverScore,
and BLEURT. Most metrics measures the similarity
between the generated sentences and the references
from varied facets, the higher the better. TER mea-
sures how much entities in given structured data
are correctly conveyed in generated sentences, the
lower the better. Considering the training instability
of few-shot learning, we conduct experiments three
times with distinct random seeds on each sampled
subset. Eventually, we showcase the average evalu-
ation results of nine experiments in each few-shot
case below if not specially claimed.
4.2 Approaches and implementation details
We compare with the proposed SPT with related
work, including Adapter, LoRA, Compacter, Prefix-
Tuning, Prompt-Tuning, and more recent methods,
IA3 and UniPELT. These PEFT approaches are
applied to T5-large with 770M parameters. For
Prompt-Tuning and SPT, the length of the prompt
is 50 on all datasets. Regarding Prefix-Tuning, we
directly train the prefix parameters rather than re-
placing them by bottleneck modules to compare
with our proposed method more closely. More-
over, bottleneck modules do not outperform simple
prefix parameters in our experiments despite their
effectiveness in related work (Li and Liang, 2021).
For other methods, we start from the default set-

--- PAGE 5 ---
E2E
MR: name[Aromi], eatType[coffee shop], food[French], customer rating[low], area[city centre], familyFridenly[no]
Transformed MR: <S> name <V> Aromi <S> eatType <V> coffee shop <S> food <V> French <S> customer rating
<V> low <S> area <V> city centre <S> familyFridenly <V> no
Reference: In the city centre lies Aromi, a French coffee shop for adults with a low customer rating.
DART
KG: (David Davie Shelby, born/died, 1847-1914), (David Davie Shelby, active, 1899-1914), (David Davie Shelby,
state, AL)
Transformed KG: <S> David Davie Shelby <P> born/died <O> 1847-1914 <S> David Davie Shelby <P> active
<O> 1899-1914 <S> David Davie Shelby <P> state <O> AL
Reference: Judge David Davie Shelby from state AL was born in 1847 and died in 1914 , and is active during 1899
and 1914.
Table 2: Examples of structured data transformation on E2E and DART. The transformed sequence and reference
are used as the input of the encoder and ground truth output of the decoder for PEFT.
Method Learning rate Config % trainable params
Fine-Tuning 1e-4 - 100.0
Prompt-Tuning 5e-1 Prompt length: 50 0.007
SPT 5e-1 Prompt length: 50 0.007
Adapter 1e-4 r: 16 0.824
LoRA 1e-4, 5e-4 Rank: 8 0.306
Compacter 3e-3 PHM dim: 8, r: 16 0.053
Prefix-Tuning 5e-2, 1e-1 Prefix length: 5, 10 0.096, 0.192
IA3 3e-3 Rank: 1 0.045
UniPELT 1e-4, 1e-3 Rank: 8, r: 16, Prefix length: 5, 10 1.194, 1.258
Table 3: Hyperparameter settings of PEFT methods. SPT represents the proposed Scaled Prompt-Tuning. The
reduction factor shared by several methods is denoted as r. For methods that have two learning rates or two settings
in the configuration, the former is shared on WebNLG 2020 and E2E dataset, and the latter is for DART dataset.
Fine-Tuning trains all the parameters in the model, resulting in 100% trainable parameters. Prompt-Tuning and SPT
trains the least number of parameters.
tings provided in adapter-transformers library4and
further tune them on different datasets if the de-
faults are not applicable. DART is a challenging
dataset and requires more tuning on learning rates
and configurations. The detailed settings of PEFT
approaches are summarized in Tab. 3. For each
experiment, we save the checkpoint resulting in the
highest BLEU score on dev set and further evaluate
it on test set.
4.3 Comparison of few-shot PEFT methods
4.3.1 WebNLG 2020
We evaluate the PEFT methods in 8-, 16-, 50- and
100-shot cases on WebNLG 2020 dataset in Fig. 2
(a). Among the methods present, Compacter at-
tains the best performance and even outperforms
Fine-Tuning while only tuning 0.053% of the pa-
rameters. Prefix-Tuning obviously falls behind oth-
ers in 8-shot and 16-shot scenarios. With such low
BLEU scores, the generated texts almost merely
copy some of the given structured data and fail to
4https://github.com/adapter-hub/
adapter-transformersform fluent sentences. Additionally, replacing pre-
fix vectors with more complicated bottleneck mod-
ules and inserting prefixes in the decoder do not
see obvious performance boost. These phenomena
are not manifested in related work such as Li and
Liang (2021) since most of them conduct the whole-
dataset tuning, which demonstrates the acceptable
performance of Prefix-Tuning. In fact, Prefix-
Tuning gradually becomes powerful as the number
of available instances increases and even outper-
forms Prompt-Tuning in 100-shot case, showcasing
the effectiveness of Prefix-Tuning. Therefore, our
conclusion is that Prefix-Tuning is very sensitive to
the number of training instances on WebNLG 2020
and performs poorly in extremely few-shot cases.
Another method that attracts our attention in
Fig. 2 (a) is UniPELT, a combination of LoRA,
Prefix-Tuning, and Adapter, which claims to take
advantage of all of them. However, UniPELT dras-
tically lags behind Adapter and LoRA while outper-
forms Prefix-Tuning. It seems that Prefix-Tuning
plays a major role in UniPELT and thus leads to
performance inferior to other methods. We ana-
lyze that three approaches involved in UniPELT

--- PAGE 6 ---
(a) WebNLG 2020
(b) E2E
(c) DART
Figure 2: The performance of PEFT methods in few-
shot cases on three datasets.
have different convergence property and are sen-
sitive to hyperparameters such as learning rate, as
Tab. 3 shows. This disparity is amplified on gen-
eration tasks and an individual learning rate could
not facilitate the convergence of all the trainable
parameters in few-shot scenarios, yielding a subpar
performance.
On the other hand, the proposed SPT obvi-
ously surpasses Prompt-Tuning and is on-par with
Adapter. Tab. 4 further displays the detailed evalu-
ation results of Fine-Tuning, Prompt-Tuning, SPT,
and the top-performing method in each few-shot
case to shed light on the improvement of the pro-
posed method and its performance gap to the best.The results of other metrics such as MRTEOR and
BLEURT almost align with that of BLEU while
the gap of the scores could be narrower. Therefore,
we mainly refer to BLEU scores when discussing
different approaches’ performance below unless
explicitly stated.
As we can see, SPT outcompetes conventional
Prompt-Tuning by around 5.3 points in BLEU
score under 16-, 50- and 100-shot circumstances.
Compacter is the best-performing approach in all
few-shot cases and surpasses SPT by a maximum
of 3.2-point BLEU score. When the whole dataset
is involved in tuning, UniPELT achieves superior
performance on most evaluation metrics and even
outperforms Fine-Tuning. This is consistent with
Mao et al. (2021) in that UniPELT combines sev-
eral PEFT methods and achieves performance im-
provement on downstream datasets.
# Shots Method BLEU MET chrF++ TER BS BLEURT
8FT 43.5 0.37 0.61 0.52 0.95 0.33
PT 38.1 0.32 0.54 0.57 0.94 0.18
SPT 42.0 0.34 0.57 0.52 0.94 0.27
Com 44.4 0.38 0.62 0.51 0.95 0.35
16FT 47.2 0.39 0.65 0.47 0.95 0.41
PT 41.5 0.35 0.58 0.52 0.94 0.29
SPT 46.9 0.38 0.63 0.46 0.95 0.38
Com 48.1 0.39 0.65 0.46 0.95 0.40
50FT 52.2 0.41 0.68 0.42 0.96 0.46
PT 44.9 0.37 0.61 0.49 0.95 0.34
SPT 50.2 0.41 0.68 0.41 0.96 0.48
Com 53.4 0.42 0.69 0.40 0.96 0.49
100FT 55.1 0.42 0.70 0.39 0.96 0.50
PT 47.8 0.39 0.65 0.45 0.96 0.41
SPT 53.1 0.42 0.69 0.39 0.96 0.50
Com 56.1 0.43 0.71 0.38 0.96 0.52
AllFT 64.1 0.46 0.75 0.32 0.97 0.59
PT 57.5 0.43 0.71 0.35 0.97 0.55
SPT 59.2 0.44 0.72 0.34 0.97 0.56
Uni 65.8 0.47 0.76 0.30 0.97 0.61
Table 4: Comprehensive evaluation results of several
PEFT methods on WebNLG 2020 in few-shot cases. FT
denotes Fine-Tuning, PT represents Prompt-Tuning, and
SPT denotes Scaled Prompt-Tuning. Com represents
Compacter and Uni represents UniPELT. All means the
whole dataset is involved in tuning. The top-performing
method in each few-shot case is listed for comparison.
BS represents BERTScore and MET represents ME-
TEOR.
4.3.2 E2E
The tendency of PEFT methods’ performance on
E2E dataset in few-shot cases is different from that
on WebNLG 2020, as Fig. 2 (b) depicts. No sin-
gle method could dominate in all few-shot scenar-
ios. Compacter stands out in 8- and 16-shot cases.
IA3 then prevails over others when more samples

--- PAGE 7 ---
are available for tuning. Both Prefix-Tuning and
UniPELT perform much better on E2E dataset than
on WebNLG 2020. We conjecture the reason is
that E2E is a less challenging dataset with simpler
structured data, and tuning the prefix vectors is
sufficient to refine the hidden representations for
generation. Moreover, LoRA and Prompt-Tuning
become the underachievers in few-shot cases ac-
cording to Fig. 2 (b).
Furthermore, SPT still outperforms Prompt-
Tuning with a large margin, where the details are
listed in Tab. 5. The largest performance gap be-
tween them is 4.8 points in BLEU score in 50-shot
case. The largest discrepancy in performance be-
tween SPT and the top-performing method is 3.5-
point BLEU score in 8-shot scenario. Similarly,
UniPELT prevails over other approaches and is on-
par with Fine-Tuning when all the samples in the
dataset are used for tuning. Importantly, the pro-
posed SPT merely falls behind UniPELT by 1.4
points in BLEU score while UniPELT tunes 170x
as many parameters as SPT.
# Shots Method BLEU NIST MET R-L CID
8FT 56.0 6.36 0.37 0.62 1.53
PT 49.7 5.60 0.34 0.62 1.39
SPT 52.7 5.64 0.34 0.60 1.34
Com 56.2 6.99 0.37 0.62 1.57
16FT 58.5 7.24 0.37 0.63 1.76
PT 51.1 6.78 0.36 0.61 1.60
SPT 54.1 7.46 0.37 0.60 1.72
Com 57.6 7.49 0.39 0.63 1.81
50FT 60.3 7.83 0.40 0.62 1.68
PT 55.2 6.56 0.36 0.61 1.60
SPT 60.0 7.49 0.38 0.62 1.80
IA3 62.0 8.00 0.41 0.65 2.00
100FT 61.4 7.77 0.39 0.65 1.81
PT 58.9 7.44 0.37 0.61 1.79
SPT 61.7 7.89 0.45 0.66 2.03
IA3 63.1 8.09 0.43 0.66 2.08
AllFT 66.3 8.57 0.45 0.69 2.20
PT 64.6 8.29 0.45 0.67 2.27
SPT 65.0 8.39 0.46 0.68 2.31
Uni 66.4 8.44 0.46 0.70 2.23
Table 5: Comprehensive evaluation results of several
PEFT methods on E2E in few-shot cases. FT denotes
Fine-Tuning, PT represents Prompt-Tuning, and SPT
denotes Scaled Prompt-Tuning. Com represents Com-
pacter and Uni represents UniPELT. MET denotes ME-
TEOR, R-L represents ROUGE-L, and CID denotes
CIDEr. All indicates the whole dataset is involved in
tuning. The best-performing method in each few-shot
case is listed for comparison.
4.3.3 DART
This dataset is the most challenging one among
the datasets we work on, as the instances are fromvarious domains and in distinct original formats.
As Fig. 2 (c) displays, LoRA is the best-performing
in 8-shot case, and Compacter surpasses others in
16-, 50-, and 100-shot scenarios. Prefix-tuning and
UniPELT again drastically underperform others
even in 100-shot case, implicitly reflecting that the
instances in DART are more challenging than that
in WebNLG 2020.
Moreover, SPT stably surpasses Prompt-Tuning,
and the largest performance improvement is 1.8
points in BLEU score in 100-shot case, as Tab. 6
elaborates. Meanwhile, SPT performs worse than
Compacter by 3-point BLEU score. When the
whole dataset is employed in tuning, the perfor-
mance gap between SPT and Adapter, the current
top-1 method, is 1.8-point BLEU score. The pro-
posed SPT is again promising considering Adapter
trains more than 100x the number of parameters of
SPT.
# Shots Method BLEU MET TER MS BS BLEURT
8FT 32.1 0.28 0.58 0.62 0.91 0.10
PT 31.1 0.22 0.64 0.60 0.91 -0.10
SPT 32.2 0.28 0.58 0.63 0.92 0.10
LoRA 35.0 0.28 0.59 0.62 0.92 0.09
16FT 33.4 0.29 0.57 0.63 0.92 0.14
PT 33.9 0.29 0.57 0.63 0.92 0.15
SPT 34.1 0.30 0.55 0.64 0.93 0.17
Com 35.9 0.29 0.56 0.63 0.93 0.14
50FT 36.4 0.33 0.57 0.65 0.93 0.24
PT 33.7 0.30 0.55 0.64 0.93 0.17
SPT 35.2 0.31 0.54 0.65 0.93 0.21
Com 37.2 0.34 0.52 0.66 0.93 0.27
100FT 39.2 0.34 0.53 0.66 0.93 0.30
PT 35.0 0.32 0.54 0.65 0.93 0.22
SPT 36.8 0.33 0.53 0.65 0.93 0.26
Com 39.8 0.34 0.52 0.66 0.93 0.29
AllFT 46.6 0.39 0.48 0.68 0.94 0.38
PT 46.0 0.38 0.47 0.68 0.94 0.39
SPT 46.7 0.39 0.47 0.68 0.94 0.40
Adapter 48.5 0.40 0.46 0.69 0.94 0.41
Table 6: Comprehensive evaluation results of several
PEFT methods on DART in few-shot cases. FT denotes
Fine-Tuning, PT represents Prompt-Tuning, and SPT
denotes Scaled Prompt-Tuning. Com represents Com-
pacter. MET denotes METEOR, MS represents Mover-
Score, and BS denotes BERTScore. All means the
whole dataset is used during tuning. The top-performing
PEFT method in each few-shot case is displayed for
comparison.
4.3.4 Summary
According to the comparison above, we draw sev-
eral suggestions regarding the adoption of PEFT
methods below. First, Compacter is always the
candidate when a small number of instances are
available considering its decent performance and

--- PAGE 8 ---
modest tuning cost on all datasets. Second, the pro-
posed SPT is the optimal selection when memory
on device is limited while a large number of train-
ing instances are available. Third, Prefix-tuning
and UniPELT are not good options in few-shot
cases especially on challenging datasets.
4.4 Prompt length
The prompt length has an inevitable impact on the
performance of SPT. We conduct SPT employing
all the instances in each individual dataset to find
the optimal settings, which are then used in few-
shot scenarios. The configurations of varied PEFT
methods in Tab. 3 are all determined from the all-
shot case. Therefore, one argument is that the per-
formance of aforementioned approaches could be
further improved if the hyperparameter settings are
especially designed for each few-shot case. This
could be the case while the comparisons present
above are still fair and reflect the robustness and
generalization capability of the methods.
Fig. 3 depicts prompt length vs. BLEU and
TER on three datasets. BLEU scores raise up as
the prompt becomes lengthier while the rate of
growth gradually declines on three datasets. TER
decreases following the increase in prompt length
on WebNLG 2020 and DART dataset. Thus, we
set the prompt length to be 50, considering there is
no significant performance gap when the prompt
length varies from 50 to 60.
4.5 Tuning stability
According to previous study, fine-tuning on small
downstream tasks is unstable. When it comes to
few-shot PEFT, the stability issue could be severer.
Fig. 4 illustrates the average performance and error
bands of three tuning methods in few-shot cases:
Fine-Tuning, Prompt-Tuning, and SPT. Regarding
datasets, tuning on WebNLG 2020 is more stable
than others mainly because it has more instances
involved in tuning under the same few-shot circum-
stance as others. Fine-Tuning is more stable than
others, which is reasonable given its largest number
of tunable parameters. Meanwhile, SPT is on-par
with Fine-Tuning on E2E and obviously surpasses
Prompt-Tuning on DART in terms of stability. Ad-
ditionally, SPT demonstrates an almost absolute
performance gain in comparison to Prompt-Tuning
on WebNLG 2020 and E2E dataset.
(a) Prompt length vs. BLEU
(b) Prompt length vs. TER
Figure 3: The impact of prompt length on BLEU and
TER for Scaled Prompt-Tuning. The results are obtained
by whole-dataset tuning. The prompt lengths involved
are 10, 30, 50, and 60. E2E dataset does not define any
TER related evaluation metric.
4.6 Multi-task Tuning vs. intermediate
Tuning
We further study how the proposed SPT performs
in the paradigm of few-shot multi-task training and
intermediate training, where WebNLG 2020 and
E2E are involved. With respect to few-shot multi-
task tuning, the same n-shot samples from two
datasets, respectively, are mixed and used in tuning.
According to Fig. 5, multi-task tuning always leads
to the performance drop compared with single-task
tuning, no matter which tuning method is present.
As the number of shots rises, the performance of
multi-task Fine-Tuning on two datasets gradually
improves, while multi-task SPT presents a marginal
improvement. The possible reason is that the two
datasets are discrepant regarding data source and
format, thus a single soft prompt is hard to recon-
cile them.
Fig. 6 applies the paradigm of intermediate train-
ing to Fine-Tuning and Scaled Prompt-Tuning to
reveal their cross-task transferability. Two transfer-
ring directions are from WebNLG 2020 to E2E and
from E2E to WebNLG 2020. According to Fig. 6

--- PAGE 9 ---
(a) WebNLG 2020
 (b) E2E
 (c) DART
Figure 4: The tuning stability of Fine-Tuning, Prompt-Tuning, and Scaled Prompt-Tuning on three datasets.
(a) Fine-Tuning
(b) Scaled Prompt-Tuning
Figure 5: Multi-task Fine-tuning and Scaled Prompt-
Tuning in few-shot cases. M-E2E andM-WebNLG2020
denote evalution results on E2E and WebNLG 2020,
respectively, after multi-task tuning. E2E andWebNLG
2020 represent corresponding single-task tuning.
(a), the model first fine-tuned on WebNLG 2020
and then fine-tuned on E2E outperforms the model
merely fine-tuned on E2E in few-shot cases. How-
ever, the model first fine-tuned on E2E and then
fine-tuned on WebNLG 2020 does not obviously
surpasses the model only fine-tuned on WeNLG
2020. These manifest that Fine-Tuning results in
the limited transferability of the parameters while
the transfer direction is of significance. Intuitively,
WebNLG 2020 is more complicated and challeng-ing than E2E dataset, and the knowledge the pa-
rameters acquire after fine-tuned on WebNLG 2020
could be beneficial to E2E dataset, but not vice
versa. This is the reason that we see the perfor-
mance gain when the intermediate fine-tuning order
is WebNLG 2020 first and E2E second. Addition-
ally, we witness that the model fine-tuned on one
dataset shows some zero-shot ability on the other
dataset.
In terms of intermediate SPT, Fig. 6 (b) elabo-
rates a slightly different story. There are substantial
performance enhancement in both transfer direc-
tions, from WebNLG 2020 to E2E and from E2E
to WebNLG 2020, in few-shot cases. Moreover,
SPT leads to the parameters’ stronger zero-shot ca-
pability on E2E than Fine-Tuning. These provide
the following suggestion in real-world applications.
With a small number of available instances and
limited computation resources in device, we could
directly take the instances from existing datasets
in similar task and conduct intermediate SPT. This
does not necessitate additional labeling cost and
large memory footprint while yielding decent per-
formance.
5 Conclusion
In this work, we study few-shot PEFT methods
on structured data-to-text generation tasks. Scaled
Prompt-Tuning is proposed which almost does not
introduce extra trainable parameters and computa-
tions than conventional Prompt-Tuning while dras-
tically boosts the performance. Moreover, we com-
prehensively evaluate several PEFT methods in
few-shot cases on three NLG datasets. Experiments
demonstrate that no single approach could always
prevail over others under all circumstances with
a relatively small number of trainable parameters.
Meanwhile, the performance of certain methods
such as Prefix-Tuning and UniPELT could dramati-

--- PAGE 10 ---
(a) Intermediate Fine-Tuning
(b) Intermediate Scaled Prompt-Tuning
Figure 6: The performance of intermediate Fine-Tuning and Scaled Prompt-Tuning in few-shot cases. Take the
evaluation on E2E via intermediate Fine-Tuning in (a) as an example. E2E represents using the model fine-tuned
with E2E dataset for evaluation. WebNLG2020 represents that the model fine-tuned on WebNLG 2020 is evaluated
on E2E, indicating the method’s zero-shot capability. WebNLG2020_to_E2E represents firstly fine-tuning on
WebNLG 2020, then fine-tuning on E2E, and finally evaluating on E2E.
cally deteriorate in few-shot cases on challenging
datasets such as DART. We further study multi-
task tuning and intermediate tuning combined with
PEFT methods under few-shot circumstances. The
proposed SPT approach showcases a decent trans-
ferability in few-shot cases. This provides a promis-
ing scheme in scenarios where a limited number
of instances from downstream tasks are available,
which does not introduce any extra labeling cost or
require large memory footprint.
References
Armen Aghajanyan, Luke Zettlemoyer, and Sonal
Gupta. 2020. Intrinsic dimensionality explains the
effectiveness of language model fine-tuning. arXiv
preprint arXiv:2012.13255 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shotlearners. Advances in neural information processing
systems , 33:1877–1901.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.arXiv preprint arXiv:1810.04805 .
Karen Hambardzumyan, Hrant Khachatrian, and
Jonathan May. 2021. Warp: Word-level adversarial
reprogramming. arXiv preprint arXiv:2101.00121 .
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2021. Towards a
unified view of parameter-efficient transfer learning.
arXiv preprint arXiv:2110.04366 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,

--- PAGE 11 ---
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efficient low-rank
hypercomplex adapter layers. Advances in Neural
Information Processing Systems , 34:1022–1035.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 .
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-
fel. 2022. Few-shot parameter-efficient fine-tuning
is better and cheaper than in-context learning. Ad-
vances in Neural Information Processing Systems ,
35:1950–1965.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,
Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-
tuning v2: Prompt tuning can be comparable to fine-
tuning universally across scales and tasks. arXiv
preprint arXiv:2110.07602 .
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian
Khabsa. 2021. Unipelt: A unified framework for
parameter-efficient language model tuning. arXiv
preprint arXiv:2110.07577 .Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2020.
Adapterfusion: Non-destructive task composition for
transfer learning. arXiv preprint arXiv:2005.00247 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2020. Adapterdrop: On the effi-
ciency of adapters in transformers. arXiv preprint
arXiv:2010.11918 .
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207 .
Yi-Lin Sung, Varun Nair, and Colin A Raffel. 2021.
Training neural networks with fixed sparse masks.
Advances in Neural Information Processing Systems ,
34:24193–24205.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. 2021. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-
models. arXiv preprint arXiv:2106.10199 .
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [mask]: Learning vs. learning to
recall. arXiv preprint arXiv:2104.05240 .
