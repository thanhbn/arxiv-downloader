# 2309.14021.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.14021.pdf
# File size: 688263 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint: Work in Progress
LORD:LOWRANK DECOMPOSITION OF
MONOLINGUAL CODE LLM S FOR ONE -SHOT COM -
PRESSION
Ayush Kaushal
Universit ´e de Montr ´eal, Nolano AI
ayush@nolano.aiTejas Vaidhya
Mila, Universit ´e de Montr ´eal, Nolano AI
tejas@nolano.ai
Irina Rish
Mila, Universit ´e de Montr ´eal, Nolano AI
irina@nolano.ai
ABSTRACT
Low Rank Decomposition of matrix - splitting a large matrix into a product of
two smaller matrix offers a means for compression that reduces the parameters
of a model without sparsification, and hence delivering more speedup on mod-
ern hardware. Moreover, unlike quantization, the compressed linear layers remain
fully differentiable and all the parameters trainable, while being able to leverage
the existing highly efficient kernels over floating point matrices. We study the po-
tential to compress Large Language Models (LLMs) for monolingual Code gener-
ation via L Ow Rank Decomposition (LoRD) and observe that ranks for the linear
layers in these models can be reduced by upto 39.58% with less than 1% increase
in perplexity. We then use LoRD to compress StarCoder 16B to 13.2B parameter
with no drop and to 12.3B with minimal drop in HumanEval Pass@1 score, in less
than 10 minutes on a single A100. The compressed models speeds up inference
by up to 22.35% with just a single line of change in code over huggingface’s im-
plementation with pytorch backend. LoRD models remain compatible with state
of the art near-lossless quantization method such as SpQR, which allows leverag-
ing further compression gains of quantization. Lastly, QLoRA over LoRD model
further reduces memory requirements by as much as 21.2% over vanilla QLoRA
while offering similar gains from parameter efficient fine tuning. Our work shows
LOw Rank Decomposition (LoRD) as a promising new paradigm for LLM com-
pression.1
1 I NTRODUCTION
Code LLMs have become an integral component of Copilots that boost developer productivity (Peng
et al., 2023) and in LLM based agents (Wang et al., 2023a). These Code LLMs are as large as 34
Billion parameters for the publicly available models Rozi `ere et al. (2023) and more than 175 Billion
parameter for closed source ones Chen et al. (2021a). There is not only a pressing need for reducing
model size and running models at a lower cost, but also for increasing the inference speed. The latter
is especially significant for Copilot based applications.
Recently, several methods have been proposed to compress and speed up inference of LLMs. Quan-
tization (Frantar et al., 2023; Dettmers et al., 2023b) reduces the number of bits required per weight
parameter of LLM by lowering the precision, and has shown significant model compression as well
as speedups in low-batch decoding phases of LLMs Kim et al. (2023a). Quantization has also been
shown to generalize well to quantized models Shen et al. (2023). Pruning (Sun et al., 2023a; Frantar
& Alistarh, 2023) has offered another means of compression by removing connections from the neu-
ral network and hence sparsifying the weight matrices of the neural networks. Distillation Gu et al.
1We will release LoRDCoder at https://huggingface.co/nolanoAI
1arXiv:2309.14021v1  [cs.CL]  25 Sep 2023

--- PAGE 2 ---
Preprint: Work in Progress
(2023); Agarwal et al. (2023); Jung et al. (2023) method enables one to train a smaller model using
a larger teacher model for supervision. While quantization and pruning methods that do not require
re-training are viable means of compressing the model, distillation involves a significant amount of
compute for retraining a smaller LLM, often from scratch. Here, we consider another compression
paradigm of L Ow Rank Decomposition (LoRD) , that does not require expensive retraining as in the
case of distillation and covers up several deficiencies of the quantization and pruning compression
method.
Low Rank Decomposition factorizes a dense matrix of a neural network as a product of two smaller
dense matrices. The LoRD model can leverage the highly optimized floating-point dense matrix
multiplication kernels (NVIDIA, 2007; Blackford et al., 2002) that have been written over modern
hardware. In contrast, quantized models require specialized kernels to be written, often different
for each hardware backend in order to enable fast inference. Moreover, the neural network remain-
ing fully-differentiable and all the parameters remaining trainable even after compression, unlike
quantization. The LoRA Hu et al. (2022) layers of tuned models are also easier to merge back into
floating point matrices compared to the quantized ones.
Pruned models produce sparse matrix weights in the neural network. Matrix multiplication over
sparse matrices is much slower than the resulting dense matrices in LoRD on most GPUs. Dense
matrices, in addition avoid representation format overhead that sparse matrices incur from parameter
reduction2and often requires specialized kernels for reducing this overhead Dettmers et al. (2023b).
Dense matrix multiplication is also easier to implement than sparse matrix multiplication, especially
over quantized models.
Several previous works have attempted to apply matrix decomposition methods like SVD, Tucker
or Kronecker decomposition for compression (Ben Noach & Goldberg, 2020; Tahaei et al., 2022;
Edalati et al., 2022). However, these have been limited to small language models like Bert (Devlin
et al., 2019) and GPT2 (Radford et al., 2019), and have shown success only on narrow task-specific
use cases or after retraining, often only with teacher-guided distillation supervision. These works
have observed that weight matrices are not low rank and adapt methods like Singular Value Decom-
position for data-aware decomposition of weights (Chen et al., 2021b; Hsu et al., 2022; Yu & Wu,
2023).
We, adapt these approaches for Large Language Models (Billion+ Parameters) over python code, and
show that these models can be low-rank decomposed to compress and speed up inference without
the need for retraining with little to no performance degradation. We study low-rank decomposition
across two families of code LLMs - StarCoder and CodeGen (§2) for varying parameter sizes and
establish the potential for reducing rank of models through decomposition. We then study these
trends across different kinds of linear layers in a transformer block and observe the potential for
upto 39.58% rank reduction with less than 1% change in perplexity.
We propose various considerations for compressing the models and to achieve inference speedup
on GPUs (§3.1). Using these, we achieve compression of the StarCoder 16B model offering 31.67
HumanEval Chen et al. (2021a) Pass@1 score down to 13.2B parameter with similar performance of
31.57 HumanEval and down to 12.3B parameter with 29.22 HumanEval score (§3.2). LoRD models,
offer an inference speedup of as high as 22.35% with just one line of change in huggingface’s (§3.3).
These LoRD models can be further compressed via the near-lossless quantization method of SpQR
Dettmers et al. (2023b) to reduce it’s precision to 8 and 4 bits without any further reduction in
HumanEval performance (§4.1). Finally, these decomposed models also reduce the memory re-
quirements of adapter finetuning by 21.2% over QLoRA (§4.2).
2This overhead in sparse matrix occurs from having to store indices/bitmasks to indicate which values are
present and not. This can be very significant at low levels of sparsity. PyTorch’s sparse formats (CSR, CSC,
COO) all store indices at int64 format, and for moderate levels of sparsity ( <50%), the sparse matrix takes up
more space than a dense matrix with zero-ed out values.
2

--- PAGE 3 ---
Preprint: Work in Progress
2 C ODE LLM S ARE LOWRANK DECOMPOSABLE
2.1 B ACKGROUND
Let an linear layer Lof an LLM Mwith weight W∈Rd1×d2and bias b∈Rd1×1. Let dmin=
minimum (d1, d2)anddmax=maximum (d1, d2)
ALow Rank Decomposition or Low Rank Factorization of a layer Lwould give us a new layer ˜L
with two weight matrices A∈Rr×d2andB∈Rd1×r, and a bias ˜b∈Rd1×1, where r << d min
such that for a nbatch of input vectors X∈Rd2×nthe batch of output vectors Y∈Rd1×nis,
Y=˜L(X) =BAX +˜b≈L(X) =WX +b (1)
Singular Value Decomposition (SVD) offers the best r-rank approximation of matrix W∈Rd1×d2.
FirstWcan be decomposed as W=USVT, where U∈Rd1×d2andV∈Rd2×d2are orthogonal
matrix and S∈Rd1×d2is a diagonal matrix with entries in decreasing order. Then, by taking top-k
rank, we can decompose Was a product of two low ranked matrices W≈BAas follows
W= (U:,:rS:r,:r)| {z }
B∈Rd1×r(V:r,:)|{z}
A∈Rr×d2(2)
where :a,:bdenotes a slice operation over a matrix that gives its first arows and bcolumns.
Eigendecomposition is another decomposition method applicable to symmetric matrices. We can
represent the eigendecomposition of a symmetric matrix W∈Rd1×d1asW=QΛQT. Here
Q∈Rd1×d1is an orthogonal matrix whose columns are the eigenvectors of W, and Λ∈Rd1×d1
is a diagonal matrix whose entries are the eigenvalues of Wsorted in decreasing order. Similar to
SVD, we can decompose Was a product of two low ranked matrices W≈BAby retaining only
the largest reigenvalues (and corresponding eigenvectors) as follows:
W= (Q:,:rΛ:r,:r)| {z }
B∈Rd1×r(QT
:r,:)
|{z}
A∈Rr×d1(3)
Since Qis orthonormal and the eigenvalues Λis sorted in descending order, Q:,:rQT
:,:r≈Iwhere I
is identity matrix of dimension d1.
While SVD gives the optimal low-rank decomposition of matrix, in terms of Frobenius norm, but
does not take input and output data distribution into account. Approaches like weighted SVD (Hsu
et al., 2022) and SVD over both weight and data (Chen et al., 2021b) have been proposed but
are prohibitively expensive to scale to larger models for their requirement of backpropagation over
calibration dataset. SVD over very large weight matrices is also very computationally expensive.
So, we instead leverage the observation that activations in transformers are low-ranked (Feng et al.,
2022) and adapt the more heuristically driven approach of Atomic Feature Mimicking (AFM) (Yu
& Wu, 2023) that creates low rank matrices conditioned on a small amount of calibration data.
Specifically, consider the eigen-decomposition of Covariance over Yas
E[yyT]−E[y]E[y]T=ˆQˆΛˆQT(4)
Here ˆQis a matrix of its eigenvectors, hence ˆQ:,:rˆQT
:,:r≈I. Using this, we can write the output
vector YasY≈ˆQ:,:rˆQT
:,:rY. By writing Yin terms of W,Xandbfrom Equation 1, we have:
Y≈ˆQ:,:rˆQT
:,:rWX +ˆQ:,:rˆQT
:,:rb (5)
Comparing to Equation 1, this gives us B=ˆQ:,:r∈Rd1×r,A=ˆQT
:,:rW∈Rr×d2and˜b=
ˆQ:,:rˆQT
:,:rb≈b. This approach is also straightforward to adapt for LLMs like LLaMa (Touvron
et al., 2023), Falcon (Penedo et al., 2023), CodeLLaMa (Rozi `ere et al., 2023) which do not have a
bias term in the linear layer by setting ˜bto zero vector.
3

--- PAGE 4 ---
Preprint: Work in Progress
2.2 E XPERIMENTAL SETTINGS
We take our python calibration dataset from the stack (Kocetkov et al., 2022) and consider the
corresponding subset of the stack smol (Bigcode, 2022) as validation data. We filter out those
sequences which are less than 1024 tokens or 10240 characters in length. We consider CodeGen and
StarCoder model family of models. CodeGen mono models are present across 350M, 2B, 6B and
16B parameters and are CodeGen models that were further trained on only python code. StarCoder
16B is the StarCoderBase 16B model further trained on only python code from the stack dataset’s
train split. We also consider StarCoderBase at 3B and 7B parameter sizes in StarCoder family due
to the lack of their monolingual counterparts. All our experiments were performed on a single A100
GPU in under an hour for each run.
For studying the trends of increase in perplexity for a reduction in rank across difference model
sizes, we set a fixed low-rank rfor all the layers. Later we discuss how to achieve compression and
inference speedup via low-rank decomposition in §3
2.3 C HANGE IN PERPLEXITY ACROSS REDUCTION IN RANK
Figure 1a and 1b show the trends of increase in perplexity across reduction in rank of the weight
matrix of CodeGen and StarCoder models. For the largest models in both families, we observe only
about a 1% increase in perplexity for 10% reduction in rank, and upto 35% reduction in rank for less
than 10% increase in perplexity. The smallest model, CodeGen Mono 350M, however, can only be
decomposed to 35% rank reduction for a similar drop in perplexity. We observe that the perplexity
changes much slower for larger models as the % rank reduces, and hence can be compressed mode,
similar to observations in quantization and pruning (Li et al., 2020). It should be noted that for most
models, more than 50% leads to significant output quality degradation.
(a) Perplexity vs % Rank Reduction for CodeGen
Models.
(b) Perplexity vs % Rank Reduction for StarCoder
Models.
Figure 1: Perplexity vs %Reduction in Rank for Different Models.
3 C OMPRESSION AND SPEEDUP THROUGH DECOMPOSITION
In this section, we discuss how we adapt the L Ow Rank Decomposition (LoRD) for reducing the
size of model and achieving inference speedup without a significant reduction in the output quality
of the model. Following (Kim et al., 2023a), we assume memory bandwidth is the bottleneck for
inference, and thus speedups for decoding are directly proportional to the size of the transformer
model.
3.1 A CHIEVING COMPRESSION AND INFERENCE SPEEDUP
Threshold for size reduction across rank reduction: Consider a weight matrix W∈Rd1×d2
of a transformer layer with low rank decomposed A∈Rr×d2andB∈Rd1×r. The number of
parameters before and after decomposition respectively are d1d2andr(d1+d2). Therefore, if
4

--- PAGE 5 ---
Preprint: Work in Progress
r >d1d2
(d1+d2), (i.e a decomposition with small rank reduction), then the size of the model after
decomposition can even be higher than the original models. Ideally, we would want the rank r <<
d1d2
(d1+d2)orr << d min.
Matrix Aspect Ratio and Compression: Let the ratio of the smaller dimension to the larger di-
mension of the matrix (i.e. the aspect ratio) be α=dmin
dmax. For square matrix, α= 1and for tall or
fat matrices α << 1. We can rewrite, the percentage change in parameters from decomposition, in
terms of percent change in rank %∆r= 100∗dmin−r
dmin%and aspect ratio as:
100∗r(dmax+dmin)−dmaxdmin
dmaxdmin= 100 α−(1 +α)%∆r (6)
It should be noted that change in parameters from decomposition can either be positive (the number
of parameters increased after decomposition), or negative (the number of parameters decreased after
decomposition). In order to achieve model compression and consequently inference speedups, one
would want a very high negative percentage change in parameters.
Figure 2: Parity Point across various aspect
ratios ( α) of the different linear layers in
transformers.Parity Point for Compression across Rank Reduc-
tion: Using Eq. 6, one can observe that little reduc-
tion in rank, may lead to increase in model parame-
ters instead of decreasing. For instance, square ma-
trices ( α= 1) will have 100% increase (i.e doubling
in size), then %∆r→0+and only after the rank
is reduced by more than 50%, will the Parity Point
of the rank reduction be reached, that offers same or
lesser number of a parameter in the decomposed layer
as the original matrix. This parity point for tall or fat
matrices ( α→0+), can be achieved with a very small
percent reduction in rank and can start giving a reduc-
tion in model size. For compression to be achieved,
we would want to reduce the rank by an amount to
cross this parity point threshold. However, reducing
the rank by a lot can degrade performance signifi-
cantly. So we must take the aspect ratio into account,
in order to achieve compression without much reduc-
tion in rank (and hence no significant degradation in
output quality)
A transformer model had different aspect ratios across its various linear layers, α= 1.00for output
projection after attention, α= 0.96for Multi-query attention (Shazeer, 2019) projections, α= 0.25
for typical MLP projections with intermediate expansion factor of 4 as in the original transformer
and as low as α= 0.12for the embedding and language model head projection of CodeGen 16B
with 51200 vocab size. Figure 2 plots the % change in the size of the model across % reduction
in rank for matrices with different aspect ratios. For square matrices and near square matrices,
a small rank reduction doubles the size of the linear layer after decomposition, and only after its
parity point of 50% reduction is the size after decomposition, the same as original matrix. By this
extent of rank decomposition, the performance starts to significantly degrade, as seen in §2.3. All the
previous works on smaller models, address this by retraining the model (Yu & Wu, 2023; Chen et al.,
2021b; Hsu et al., 2022; Ben Noach & Goldberg, 2020), often via knowledge distillation supervision
(Hinton et al., 2015; Sanh et al., 2019) on specific narrow tasks. However, retraining is infeasible
for larger models. Thus, we skip matrices with very high aspect ratios such as output projection or
multi-query attention for decomposition. In contrast, the weights in MLP achieve parity at only 20%
rank reduction. While embeddings and LM Head can be compressed through decomposition, as they
have been for smaller transformer models (Baevski & Auli, 2019; Lan et al., 2020), they contribute
only a very small portion of the weight of the model. So, we do not consider decomposing these
matrices. In order to reduce the aspect ratio of matrices, we group layers with the same input vector
to have the same bottleneck matrix after decomposition. Doing so, enables re-use of computation,
and sharing of weights, as well as bringing the aspect ratio down to achieve compression as lower
rank reduction. Candidate layers for grouping include the query, key and value projection matrices
5

--- PAGE 6 ---
Preprint: Work in Progress
in multi-headed attention with aspect ratio reduced to α= 0.33and the gating layer in SwiGLU
(Shazeer, 2020) with first linear linear of MLP in models like LLaMa (Touvron et al., 2023) with
α= 0.1875 .
(a) CodeGen 16B.
 (b) StarCoder 16B.
Figure 3: Parameter Reduction vs perplexity for decomposition across various layers.
Trends across different layers in a transformer block: In addition to considering the parity point
into account for deciding which layers to decompose, we also additionally study the sensitivity of
each of these layers to low rank decomposition across the large model in the two model families.
Figure 3 shows the increase in perplexity vs reduction in model parameters for the two models. For
both models, decomposing all the linear layers achieves the parity point much later than any one of
these linear layers with low aspect ratio. For CodeGen, the attention weight matrix (query, key and
values projection) offers least increase in perplexity for the biggest drop in parameter count, make
this layer the most suitable candidate to be decomposed. It shows less than 1% increase in perplexity
even after 39.58% rank reduction. We observe the mlp 2 (downscaling mlp) to be a better candidate
for decomposition than mlp 1 (upscaling mlp) across both models. This makes mlp 2 to be a good
candidate for low-rank decomposition over the StarCoder model.
Hardware Considerations: On modern hardware accelerators like GPU and their corresponding
software stack, matrix multiplication kernels are faster if their dimensions are divisible by a high
factor of 2. So, we consider ranks at a reduction of approximately every 10%, rounded off to the
nearest multiple of 128 in our experiments.
3.2 P ERFORMANCE OF COMPRESSED MODELS
We consider the largest models of StarCoder and CodeGen family (16B) and perform low-rank de-
composition on both with varying ranks. We consider decomposing layers that offers most parameter
reduction (§3.1) with least increase in perplexity - mlp 2 for StarCoder and attention for CodeGen.
We report the Pass@1 and Pass@10 scores over the Human Eval dataset (Chen et al., 2021a) us-
ing the code-eval GitHub repo (Bacaj, 2023) in Table 1. We observe that StarCoder models can be
low rank decomposed to 13.2B parameters (50% rank reduction) with no drop in Pass@1 perfor-
mance and upto 12.3B parameters (62.5% rank reduction) with very little drop. CodeGen models
shows similar trend in drop in Human Eval performance when measured in terms of rank reduction.
However, in terms of parameter reduction count, while showing very little perplexity change with
large reduction in rank (Fig. 3a), shows much more drop in its HumanEval score when measured
in terms of parameter count reduction due to a higher aspect ratio of the matrix being decomposed.
It should be noted that for certain compressed models, the Pass@1 even slightly improves over the
base model. Similar trend of slight improvements from compression across various metrics and
benchmarks has been observed in the case of other compression attempts (Frantar & Alistarh, 2023;
Cerebras, 2022).
6

--- PAGE 7 ---
Preprint: Work in Progress
Starcoder 16B CodeGen 16B Mono
Model Type Rank HumanEval Score Model Type Rank HumanEval Score
Pass @ 1 Pass @ 10 Pass @ 1 Pass @ 10
Base Model 6144 31.67 48.28 Base Model 6144 29.02 46.34
LoRDCoder 14.9B 4480 33.18 48.41 LoRDCoder 15.9B 4480 29.08 46.95
LoRDCoder 14.5B 4096 31.69 45.12 LoRDCoder 15.6B 4096 28.90 46.24
LoRDCoder 13.8B 3584 30.90 47.56 LoRDCoder 15.1B 3584 28.54 45.73
LoRDCoder 13.2B 3072 31.57 45.36 LoRDCoder 14.7B 3072 27.99 43.29
LoRDCoder 12.6B 2560 29.84 42.31 LoRDCoder 14.3B 2560 27.32 45.12
LoRDCoder 12.3B 2304 29.22 40.12 LoRDCoder 14.1B 2304 27.07 41.46
Table 1: Human Eval Score of LoRD across StarCoder and CodeGen.
3.3 S PEEDUP FROM LORD
We next consider accessing the inference speedup (forward pass) of the models over the standard
cuBLAS floating point kernels. We consider the standard Huggingface implementation (Wolf et al.,
2020) of Starcoder with pytorch backend (Paszke et al., 2019) utilizing standard cuBLAS kernels
on A100 GPUs. LoRD decomposed models were implemented by modifying just one line of code
to replace an MLP with an extra linear layer3. We benchmark over 1024 tokens and 512 tokens
sequence, averaged across 10 runs with warm up of 3 runs. We plot relative time taken and model
size across reduction in rank in Figure
Figure 4: Time and Model size of Star-
Coder 16B across ranks.4.
Inference speedups as high as 22.35% are observed
for decomposed models. The lines in the graph are
generally downward sloping, Therefore reduction in
rank beyond 25% generally implies less inference time
and reduction in model size. However, the underlying
hardware (and pertaining software kernels) also signif-
icantly affect the speedup gains. We notice huge gains,
whenever the rank is rounded off to a multiple of a very
high power of 2 (like 4096 and 2560 at 33% and 58%
rank reduction), despite very little reduction in model
size. In contrast, for certain ranks which are multiples
of a lesser power of 2 (like 3584 and 2304 at 41% and
62% rank reduction) are slower than those at slightly
higher ranks. It is worth noting that affect of hardware
inefficient matrix shape is less significant for longer to-
kens sequence of 1024 because the O(n2)attention overhead starts becoming more significant,
especially in the absence of SoTA attention implementation techniques (Rabe & Staats, 2021; Dao
et al., 2022; Dao, 2023) as in the case of Huggingface’s implementations.
4 C OMBINING LORD WITH QUANTIZATION AND LORA
4.1 Q UANTIZATION
While LoRD enables compression at same precision level, we study whether the decomposed mod-
els can be further compressing through quantization. Table 2 shows the HumanEval pass@1 results
for the different LoRDCoder across 8 and 4 bit quantization levels, using the near lossless quan-
tization technique of SpQR (Dettmers et al., 2023b). We observe that the LoRD models can be
combined with quantization for further compression, showing no performance drop for 8-bit and
very little performance drop on 4-bit quantization for most models. Slight increase in HumanEval
after quantization is also observed, similar to Pangu-Coder2 (Shen et al., 2023).
4.2 P ARAMETER EFFICIENT TUNING OF LORD MODELS
3nn.Linear(in, out) -> nn.Sequential(nn.Linear(in, rank), nn.Linear(rank, out))
7

--- PAGE 8 ---
Preprint: Work in Progress
Model Pass@1@FP16 Pass@1@8-bit Pass@1@4-bit
LoRDCoder 14.9B 33.18 33.17 32.01
LoRDCoder 14.5B 31.69 31.58 32.74
LoRDCoder 13.8B 30.90 31.10 30.73
LoRDCoder 13.2B 31.57 31.52 32.01
LoRDCoder 12.6B 29.84 29.87 30.22
LoRDCoder 12.3B 29.22 29.14 29.45
Table 2: Human Eval score of quantized LoRDCoder models.
d x d
d x r’r’ x d
d x r’r’ x d
d x rr x d
dr ’h
xdr ’h
xrLoRD
Weightsa) LoRA b) LoRD + LoRA
Pretrained
Weights
A   RX   RY   R
W   RX   RY   R
B   R
Figure 5: LoRA vs LoRD + LoRA.We next test the potential for using
LoRD to further reduce the mem-
ory usage over existing parameter-
efficient techniques. We consider the
code instruction dataset (Chaudhary,
2023) and filter those examples that
pertains to python programming lan-
guage. We use QLoRA (Dettmers
et al., 2023a), which is an even more
memory efficient version of LoRA
(Hu et al., 2022) storing the weights
in quantized format, for fine-tuning
for 1 epoch. We compare results
from fine-tuning two of the decom-
posed models LoRDCoder 13.2B and
LoRDCoder 12.3B model to the Star-
Coder model. We observe a HumanEval pass@1 of 37.80 and 37.62 across LoRDCoder 13.2B
and LoRDCoder 12.3B fine-tuning, competitive to the performance of 37.74 offered by StarCoder
model.
5 R ELATED WORK
There is a growing interest in compressing pretrained Large Language Models. Several recent at-
tempts have been dedicated to the quantization of weights of LLMs (Frantar et al., 2023; Lin et al.,
2023; Yuan et al., 2023; Park et al., 2022; Kim et al., 2023b; Chee et al., 2023; Li et al., 2023a) with
tricks such as outlier separation (Dettmers et al., 2022; Dettmers & Zettlemoyer, 2022; Dettmers
et al., 2023c; Wei et al., 2022; Kim et al., 2023a; Lee et al., 2023). Some attempts also quantize
the activations (intermediate representations) in addition to weights to speed up computation time
(Shao et al., 2023; Xiao et al., 2023). The works in quantization that are closest to us is the Low-
Rank Compensation (LoRC) Strategy (Yao et al., 2023; Wu et al., 2023), where the difference of
the quantized matrix to the original matrix is approximated by a product of low-rank matrices. Our
work decomposes the entire matrix for compression.
Pruning neural networks Liang et al. (2021), unlike quantization, reduces the number of parameters
in a model by removing unimportant weights or connections. Several techniques have been proposed
to scale pruning methods for LLMs (Sun et al., 2023a; Frantar & Alistarh, 2023; Ma et al., 2023).
However, pruning as a means of compression is yet to become viable due to no speedups over sparse
matrices without significant performance drop at extreme levels of sparsity or structured sparsity
(Zhu et al., 2023). With low-rank decomposition, we propose an alternate method for reducing
model parameters that offer speedup even at a little reduction in parameter count. Certain works
have also attempted to (Ren & Zhu, 2023; Li et al., 2023b) to split a dense matrix as a sum of low-
rank matrices and a sparse matrix. However, these methods require retraining and have been shown
to work only for Language Models of less than a billion parameters.
Low rank decomposition has been proposed for smaller language models like Bert or GPT2 before
using SVD decomposition (Ben Noach & Goldberg, 2020) and Kronecker decompositions (Tahaei
et al., 2022; Edalati et al., 2022). Hsu et al. (2022) modified SVD to be data aware based on ap-
proximate second-order gradient information. A better weighted SVD was proposed by (Hua et al.,
2022). Chen et al. (2021b) proposed a data aware decomposition method with a provably optimal
closed-form solution, utilizing a large amount of data points over specific tasks to decompose. Sev-
8

--- PAGE 9 ---
Preprint: Work in Progress
eral recent works (Yu & Wu, 2023; Feng et al., 2022) have shown that while the weight matrix of
neural networks is not inherently low-rank, the intermediate representations are, thus propose to
decompose based on representations. All these works have focused on small language models and
require re-training. We proposed low-rank decomposition for compressing neural networks without
the need for retraining. The factorization has also been used just for the embedding layers (Baevski
& Auli, 2019; Lan et al., 2020), as they are good candidates due to their very low aspect ratio of
0.015, where a reduction of rank by even 5% would lead to reduction in number of parameters after
decomposition.
There is also a growing interest in fine-tuning large language models Taori et al. (2023); Chiang
et al. (2023); Wang et al. (2023b); Sun et al. (2023b). With the large memory requirements for fine-
tuning full parameters of the LLM, the more parameter-efficient fine-tuning methods like LoRA
(Hu et al., 2022) are getting widely adopted. These methods freeze the original LLM weights, and
attach two low-rank matrices or adapters, in a skip-connection (He et al., 2016) to the linear layers
of the model. These parameter-efficient fine-tuning approaches have seen improvements in lower
activation memory (Zhang et al., 2023) or by keeping non-trainable model weights at 4-bit precision
(Dettmers et al., 2023a). Our work, while focused on compression through low-rank decomposition,
can also enable more efficient fine-tuning, especially in conjunction with existing methods.
6 C ONCLUSION
We studied the compression of monolingual code generation models through a novel one-shot com-
pression paradigm of low-rank decomposition. We analyse the change in perplexity with change
in rank across the model families of StarCoder and CodeGen as well as their individual layers and
observe that the rank of these models can be reduced by upto 39.58% with less than 1% change in
perplexity. We then proposed considerations for one-shot compressing these models through L Ow
Rank Decomposition (LoRD) in under 10 minutes. Consequently, we compress StarCoder 16B
to 13.2B with no drop in HumanEval pass@1 and very little drop in HumanEval pass@1 to 12.3B
parameters. With a minimal change in code over huggingface’s default inference code of just one
line, we gain speedups of up to 22.35%. The LoRD models are also compatible with near lossless
quantization techniques of SpQR, which offers gains of quantization based compression in addition
to ones from decomposition. The LoRD models also reduce memory requirements by as much as
21.2% over vanilla QLoRA fine-tuning.
7 B ROADER IMPACT AND FUTURE WORK
Our work on LoRD, compresses code LLMs which enables them to run on smaller GPUs including
as consumer grade GPUs. This is especially of pressing importance for the next few years when
the shortage of GPU supply is relative to the increasing demand in today’s market. Moreover, faster
inference helps reduce the GPU cycles, enabling lower running costs and lower power consumption
for LLM inference. Our work helps reduce the carbon emissions incurred and moves towards a
greener NLP. Through compression, our work also promotes inference at the edge, and therefore
opening room for applications involving strict privacy requirements. Lower latency will also help
improve the User Experience in applications like CoPilots where lag between suggestions can im-
pact developer’s productivity. Several of these benefits of LoRD such as lower cost and energy
consumption are also applicable for fine-tuning use cases of LLMs.
Our work opens up a new paradigm for compression via Low Rank Decomposition over Large
Language Models in a single shot without the need for retraining. Since, LoRD models can leverage
existing floating point kernels across BLAS and cuBLAS, in contrast to quantization, these are
much easier to implement and reap inference benefits. Our study on hardware considerations for
speedup also opens up the potential for tuning the rank of decomposed models to fit best on the
target hardware and the accompanying GEMM kernels. While our study is limited to monolingual
code LLMs, the low rank decomposition technique is general and not specific to code domain. Thus
exploring its applicability to more general purpose models like LLaMa is a promising direction
for the compression of transformer LLMs beyond quantization. Another interesting unexplored
question is whether the LoRA or QLoRA modules fine-tuned on original models, can be plugged in
as-is for the LoRD models without any performance drop.
9

--- PAGE 10 ---
Preprint: Work in Progress
REFERENCES
Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier
Bachem. Gkd: Generalized knowledge distillation for auto-regressive sequence models, 2023.
Anton Bacaj. code-eval. https://github.com/abacaj/code-eval , July 2023.
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
International Conference on Learning Representations , 2019. URL https://openreview.
net/forum?id=ByxZX20qFQ .
Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix de-
composition. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association
for Computational Linguistics and the 10th International Joint Conference on Natural Language
Processing , pp. 884–889, Suzhou, China, December 2020. Association for Computational Lin-
guistics. URL https://aclanthology.org/2020.aacl-main.88 .
Project Bigcode. The stack smol, 2022. URL https://huggingface.co/datasets/
bigcode/the-stack-smol .
L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Dem-
mel, Jack Dongarra, Iain Duff, Sven Hammarling, Greg Henry, et al. An updated set of basic
linear algebra subprograms (blas). ACM Transactions on Mathematical Software , 28(2):135–151,
2002.
Team Cerebras. Creating sparse gpt-3 models with iterative prun-
ing, 11 2022. URL https://www.cerebras.net/blog/
creating-sparse-gpt-3-models-with-iterative-pruning .
Sahil Chaudhary. Code instructions dataset. https://huggingface.co/datasets/
sahil2801/code_instructions_120k , Jun 2023.
Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of
large language models with guarantees, 2023.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-
Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large
language models trained on code, 2021a.
Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Drone: Data-
aware low-rank compression for large nlp models. In M. Ranzato, A. Beygelz-
imer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural In-
formation Processing Systems , volume 34, pp. 29321–29334. Curran Associates, Inc.,
2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/
file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.
10

--- PAGE 11 ---
Preprint: Work in Progress
Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and
memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022.
URLhttps://openreview.net/forum?id=H4DqfPSibmx .
Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws,
2022.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix
multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=dXiGWqBoxaD .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms, 2023a.
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashk-
boos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized repre-
sentation for near-lossless llm weight compression, 2023b.
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashk-
boos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized repre-
sentation for near-lossless llm weight compression, 2023c.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423 .
Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Nia, James Clark, and Mehdi Rezagholizadeh.
Kronecker decomposition for GPT compression. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers) , pp. 219–226, Dublin,
Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.
24. URL https://aclanthology.org/2022.acl-short.24 .
Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank
diminishing in deep neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=tIqzLFf3kk .
Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot, 2023.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization
for generative pre-trained transformers. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=tcbBPnfwxS .
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models,
2023.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
770–778, 2016. doi: 10.1109/CVPR.2016.90.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model
compression with weighted low-rank factorization. In International Conference on Learning
Representations , 2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5 .
11

--- PAGE 12 ---
Preprint: Work in Progress
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con-
ference on Learning Representations , 2022. URL https://openreview.net/forum?
id=nZeVKeeFYf9 .
Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, and Hongxia Jin. Numeri-
cal optimizations for weighted low-rank estimation on language models. In Proceedings of
the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 1404–1416,
Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-
tics. doi: 10.18653/v1/2022.emnlp-main.91. URL https://aclanthology.org/2022.
emnlp-main.91 .
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen,
and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset & model
for summarization and paraphrasing, 2023.
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W.
Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization, 2023a.
Young Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. Finequant: Unlocking
efficiency with fine-grained weight-only quantization for llms, 2023b.
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu ˜noz Ferrandis,
Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations , 2020. URL https://openreview.net/forum?
id=H1eA7AEtvS .
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned
from activation outliers for weight quantization in large language models, 2023.
Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance low-bit
quantization of large language models, 2023a.
Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao.
Losparse: Structured compression of large language models based on low-rank and sparse ap-
proximation, 2023b.
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gon-
zalez. Train large, then compress: Rethinking model size for efficient training and inference
of transformers. In Proceedings of the 37th International Conference on Machine Learning ,
ICML’20. JMLR.org, 2020.
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quanti-
zation for deep neural network acceleration: A survey. Neurocomputing , 461:370–403, 2021.
ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2021.07.045. URL https://www.
sciencedirect.com/science/article/pii/S0925231221010894 .
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-
aware weight quantization for llm compression and acceleration, 2023.
Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large
language models, 2023.
Corporation NVIDIA. Compute unified device architecture (cuda). Website, 2007. URL https:
//developer.nvidia.com/cuda-toolkit . Accessed: 2023-09-17.
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung
Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized matrix mul-
tiplication based on luts for efficient inference in large-scale generative language models, 2022.
12

--- PAGE 13 ---
Preprint: Work in Progress
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library , chapter ., pp. . Curran Associates Inc., Red Hook, NY , USA, 2019.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb
dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer
productivity: Evidence from github copilot, 2023.
Markus N. Rabe and Charles Staats. Self-attention does not need o(n2)memory, 2021.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog , 1(8), 2019.
Siyu Ren and Kenny Q. Zhu. Low-rank prune-and-factorize for language model compression, 2023.
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,
Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D ´efossez,
Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and
Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter, 2019.
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang,
Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for
large language models, 2023.
Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019.
Noam Shazeer. Glu variants improve transformer, 2020.
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu,
Jichuan Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. Pangu-coder2: Boosting large
language models for code with ranking feedback, 2023.
Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach
for large language models, 2023a.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming
Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with
minimal human supervision, 2023b.
Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. Kronecker-
BERT: Significant compression of pre-trained language models through kronecker decomposi-
tion and knowledge distillation. In Proceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 2116–2127, Seattle, United States, July 2022. Association for Computational Linguis-
tics. doi: 10.18653/v1/2022.naacl-main.154. URL https://aclanthology.org/2022.
naacl-main.154 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model.
CRFM Stanford , March 2023. URL https://crfm.stanford.edu/2023/03/13/
alpaca.html .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023.
13

--- PAGE 14 ---
Preprint: Work in Progress
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai
Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large
language model based autonomous agents, 2023a.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi
Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.
How far can camels go? exploring the state of instruction tuning on open resources, 2023b.
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Feng-
wei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer lan-
guage models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),
Advances in Neural Information Processing Systems , 2022. URL https://openreview.
net/forum?id=yW5zeRSFdZ .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations , pp. 38–45, Online, October 2020. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https:
//aclanthology.org/2020.emnlp-demos.6 .
Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training
w4a8 quantization using floating-point formats, 2023.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant:
Accurate and efficient post-training quantization for large language models. In Andreas Krause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett
(eds.), Proceedings of the 40th International Conference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research , pp. 38087–38099. PMLR, 23–29 Jul 2023. URL
https://proceedings.mlr.press/v202/xiao23c.html .
Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring
post-training quantization in llms from comprehensive study to low rank compensation, 2023.
Hao Yu and Jianxin Wu. Compressing transformers: Features are low-rank, but weights are
not! Proceedings of the AAAI Conference on Artificial Intelligence , 37(9):11007–11015,
Jun. 2023. doi: 10.1609/aaai.v37i9.26304. URL https://ojs.aaai.org/index.php/
AAAI/article/view/26304 .
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun,
Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for
large language models, 2023.
Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient
low-rank adaptation for large language models fine-tuning, 2023.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for
large language models, 2023.
14
