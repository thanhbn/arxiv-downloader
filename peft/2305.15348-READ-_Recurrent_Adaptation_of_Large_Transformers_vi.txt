# 2305.15348.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.15348.pdf
# Kích thước tệp: 545647 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
READ: Thích ứng Tái phát của các Transformer Lớn
John Nguyen∗Sid Wang∗Ke Li Carole-Jean Wu
Meta

Tóm tắt
Trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP), các transformer quy mô lớn đã khẳng định vị thế then chốt của mình, đạt được kết quả vượt trội trên nhiều tác vụ. Cách tiếp cận thông thường bao gồm tiền huấn luyện các mô hình này trên dữ liệu quy mô web rộng lớn, sau đó tinh chỉnh chúng cho các tác vụ cụ thể. Tuy nhiên, kích thước ngày càng tăng của các mô hình này, đã tăng gần hai bậc độ lớn nhanh hơn bộ nhớ GPU kể từ năm 2018, đã khiến việc tinh chỉnh trở nên tốn kém về mặt tài chính và tính toán, giới hạn khả năng này chỉ cho một số ít tổ chức có nguồn tài trợ tốt. Học chuyển giao hiệu quả tham số (PETL) đã nổi lên như một giải pháp tiềm năng, nhằm thích ứng hiệu quả các tham số mô hình tiền huấn luyện cho các tác vụ đích bằng cách sử dụng các mô hình nhỏ hơn, đặc thù cho tác vụ. Tuy nhiên, các phương pháp PETL hiện tại hoặc gây ra độ trễ suy luận bổ sung hoặc chỉ giảm thiểu yêu cầu bộ nhớ trong quá trình huấn luyện, do đó không giải quyết hoàn toàn động lực chính đằng sau PETL. Bài báo này giới thiệu REcurrent ADaptation (READ), một phương pháp tinh chỉnh mới, nhẹ và hiệu quả bộ nhớ, kết hợp một mạng RNN nhỏ cùng với mô hình xương sống. READ không chỉ đạt được chất lượng mô hình tương đương với tinh chỉnh truyền thống, tiết kiệm hơn 84% tiêu thụ năng lượng, mà còn thể hiện tính mở rộng và độc lập với kích thước mô hình xương sống. Thông qua các thí nghiệm rộng rãi trên nhiều điểm chuẩn NLP khác nhau, bao gồm điểm chuẩn GLUE, READ cho thấy hiệu suất mạnh mẽ và hiệu quả cao, giảm tiêu thụ bộ nhớ huấn luyện mô hình 56% và sử dụng năng lượng GPU 84% so với tinh chỉnh đầy đủ, mà không ảnh hưởng đáng kể đến độ trễ suy luận và bộ nhớ. Chúng tôi cung cấp một giải pháp mở rộng được biện minh về mặt lý thuyết cho việc tinh chỉnh các transformer lớn.

1 Giới thiệu
Kiến trúc transformer quy mô lớn đã đạt được kết quả tiên tiến trong một số tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) [2,5,22,23,25,33]. Việc mở rộng quy mô của các mô hình này đã được chứng minh là mang lại nhiều lợi ích khác nhau, chẳng hạn như cải thiện hiệu suất dự đoán mô hình và hiệu quả mẫu [9,14,34]. Mô hình thông thường là tiền huấn luyện các mô hình quy mô lớn trên dữ liệu quy mô web chung và tinh chỉnh các mô hình cho các tác vụ cụ thể. Tuy nhiên, việc tinh chỉnh các mô hình này đã trở nên cực kỳ tốn kém.

Kể từ năm 2018, kích thước mô hình đã tăng gần hai bậc độ lớn nhanh hơn bộ nhớ GPU [20], dẫn đến chi phí cực kỳ cao để phát triển các công nghệ AI [36]. Chỉ có một số ít tổ chức có nguồn tài trợ tốt mới có tài nguyên để tinh chỉnh các mô hình này. Học chuyển giao hiệu quả tham số (PETL) [1,13,15,16,18,19,38] đã nổi lên như một giải pháp đầy hứa hẹn để vượt qua những thách thức của tinh chỉnh đầy đủ. Các kỹ thuật học chuyển giao hiệu quả tham số nhằm giải quyết những thách thức này bằng cách tận dụng các mô hình nhỏ hơn và đặc thù cho tác vụ hơn để thích ứng hiệu quả các tham số của mô hình tiền huấn luyện cho tác vụ đích.

∗Đóng góp bằng nhau. Email liên hệ: yuwang2020@meta.com, ngjhn@meta.com
37th R0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023(NeurIPS 2023).arXiv:2305.15348v2 [cs.LG] 3 Oct 2024

--- TRANG 2 ---
[Hình 1: (Trái) So sánh READ và các phương pháp tinh chỉnh khác trên các tác vụ GLUE về năng lượng huấn luyện. (Giữa) Bộ nhớ huấn luyện đỉnh so với tinh chỉnh đầy đủ. (Phải) Tiêu thụ năng lượng chuẩn hóa so với tinh chỉnh đầy đủ trên các tác vụ GLUE.]

Tuy nhiên, tất cả các phương pháp này hoặc đi kèm với độ trễ suy luận bổ sung [13] hoặc chỉ giảm một lượng nhỏ yêu cầu bộ nhớ trong quá trình huấn luyện - động lực chính của PETL. Hình 1 minh họa rằng các phương pháp hiệu quả tham số, trong khi chỉ điều chỉnh một tỷ lệ nhỏ các tham số tổng thể, vẫn tiêu thụ năng lượng đáng kể để tinh chỉnh. Vì các tham số được cập nhật nằm bên trong các mô hình ngôn ngữ xương sống, để tính toán gradient cho các tham số này cho lan truyền ngược, các phương pháp PETL cần chạy quá trình lan truyền ngược qua các mô hình ngôn ngữ tiền huấn luyện có kích thước lớn. Điều này ngăn cản các phương pháp PETL được áp dụng cho nhiều ứng dụng thực tế với tài nguyên tính toán hạn chế. Các công trình gần đây của Side-Tuning [39] và Ladder-Side Tuning (LST) [29] đề xuất sử dụng mạng phụ lấy kích hoạt trung gian từ các mạng xương sống để giảm nhu cầu lan truyền ngược qua lớp xương sống lớn. Do đó giảm yêu cầu bộ nhớ huấn luyện. Tuy nhiên, cả Side-Tuning và LST đều có những nhược điểm đáng kể. Trong Side-Tuning, mạng phụ chỉ sử dụng các đầu vào gốc, để lại các kết quả trung gian có thông tin từ xương sống không được sử dụng. LST khắc phục vấn đề này bằng cách sử dụng Transformer phụ. Tuy nhiên, Transformer rất khó huấn luyện [21]. Hơn nữa, LST yêu cầu một giai đoạn tiền huấn luyện bổ sung để trích xuất một sub-Transformer từ xương sống và sử dụng nó để khởi tạo mạng phụ, tăng chi phí tinh chỉnh. Ngoài ra, kích thước của side-Transformer và xương sống tăng, khiến cách tiếp cận này khó mở rộng (Hình 4). Trong công trình này, chúng tôi giới thiệu REcurrent ADaptation (READ), một phương pháp tinh chỉnh mới, nhẹ và hiệu quả, kết hợp một Mạng Neural Tái phát (RNN) nhỏ cùng với mô hình xương sống. READ không chỉ đạt được chất lượng mô hình tương đương với tinh chỉnh truyền thống mà còn tiết kiệm hơn 84% năng lượng trong quá trình này.

Đóng góp
Chúng tôi trình bày READ, một phương pháp tinh chỉnh hiệu quả bộ nhớ và tham số:

1. Giới thiệu thiết kế side-tuning không yêu cầu tiền huấn luyện mạng phụ, giải quyết các hạn chế của các phương pháp PETL và side-tuning trước đây.

2. Thể hiện hiệu suất mạnh mẽ và hiệu quả trên nhiều điểm chuẩn NLP khác nhau, giảm tiêu thụ bộ nhớ huấn luyện mô hình 56% và sử dụng năng lượng GPU 84% so với tinh chỉnh đầy đủ.

3. Chứng minh tính mở rộng để tinh chỉnh các transformer lớn, độc lập với kích thước mô hình xương sống.

4. Cung cấp biện minh lý thuyết cho việc sử dụng trạng thái ẩn xương sống cho side-tuning.

2 Phân tích REcurrent ADaptation (READ)

2.1 READ là gì?
Hình 2 minh họa cơ chế tinh chỉnh READ trên một xương sống transformer encoder-decoder, T, được đóng băng trong quá trình huấn luyện. READ, được khởi tạo ở cả encoder và decoder, chủ yếu bao gồm một RNN chuẩn và một mạng Joiner, tạo điều kiện cho việc kết hợp nhiều nguồn thông tin để tạo ra đầu vào RNN. Quá trình truyền tiến qua T độc lập với READ, với các kết quả trung gian được lưu trữ tại mỗi lớp transformer, sau đó là tính toán lặp đi lặp lại các trạng thái ẩn RNN và cộng các đầu ra RNN và T để tạo ra trạng thái cuối cùng.

2

--- TRANG 3 ---
[Hình 2: Cơ chế Tinh chỉnh READ.]
[Hình 3: Biểu đồ giao hoán cho correction.]

Các thuộc tính chính của READ bao gồm:
1. Tách biệt khỏi Xương sống: Quá trình truyền tiến tách biệt khỏi T, ngăn chặn lan truyền ngược qua nó và giảm bộ nhớ huấn luyện [29].
2. Đơn giản và Hiệu quả: Chỉ bao gồm RNN và FFN, tăng cường khả năng sử dụng và hiệu quả huấn luyện mà không yêu cầu tiền huấn luyện.
3. Khả năng Mở rộng Tham số: Bản chất tái phát đảm bảo các tham số có thể huấn luyện không tăng theo số lớp xương sống, thể hiện tăng trưởng dưới tuyến tính với kích thước xương sống.
4. Tiêu thụ Kết quả Trung gian Không Sửa đổi: READ sử dụng mà không thay đổi các kết quả trung gian của xương sống².

2.2 READ hoạt động như thế nào?
Hình 2 cung cấp một biểu diễn trực quan của cơ chế tinh chỉnh READ, được áp dụng cho một xương sống transformer encoder-decoder, được ký hiệu là T. Nói đơn giản, READ học các điều chỉnh cần thiết, hay corrections, cho các trạng thái ẩn đầu ra tại mỗi lớp của T để thích ứng nó cho một tác vụ mới.

Định nghĩa 2.1 (Correction). Nếu T′ là một phiên bản sửa đổi của T và ϕ′ᵢ biểu diễn các trạng thái ẩn tại lớp L′ᵢ của T′, sự khác biệt ϕ′ᵢ - ϕᵢ được gọi là một correction cho ϕᵢ, và được ký hiệu là δϕᵢ.

Về bản chất, một correction (δϕᵢ) biểu diễn sự khác biệt giữa các trạng thái ẩn của các lớp transformer gốc và được sửa đổi, như được minh họa trong Hình 3. Những hiểu biết quan trọng về READ

Tách biệt khỏi Các Phương pháp Khác: Không giống như nhiều phương pháp tinh chỉnh trực tiếp thay đổi ϕᵢ bằng cách cập nhật trọng số xương sống hoặc tiêm các tham số có thể học, READ tập trung vào việc học correction cần thiết cho một tác vụ mới.

Ứng dụng Thực tiễn: Trong thực tiễn, chúng tôi sử dụng một mạng neural, được đặt tên là READ, để mô hình hóa hệ thống phương trình. Điều này bao gồm việc sử dụng một mạng Joiner để tính toán xᵢ, thay thế các thực thể toán học khác nhau trong phương trình bằng Mạng Feed-Forward (FFN) hoặc các lớp tuyến tính và hợp nhất các tham số có thể học trên tất cả các chỉ số i để đạt hiệu quả và tính mở rộng. Ngoài ra, Mạng Neural Tái phát (RNN) được sử dụng để mô hình hóa một phần của hệ thống phương trình.

Cách tiếp cận này không liên quan đến cơ chế attention và chỉ hoạt động trên không gian cột của ϕ, đảm bảo rằng tất cả các hoạt động được thực hiện một cách hiệu quả và có hiệu quả.

3 Thiết lập Thí nghiệm
Baseline và Các Thiết kế Tiên tiến Khác Chúng tôi so sánh READ với tinh chỉnh đầy đủ và các phương pháp PETL thường được sử dụng khác. Tinh chỉnh đầy đủ không phải là phương pháp tinh chỉnh hiệu quả nhưng phục vụ như một baseline mạnh mẽ cho hiệu suất. BitFit [3] chỉ điều chỉnh các số hạng bias của mô hình trong quá trình huấn luyện. Prompt-tuning [17] chèn các vector prompt có thể huấn luyện vào các vector embedding của đầu vào. Adapter [13]

²Đáng chú ý, mặc dù không được nêu chi tiết trong bài báo này, READ cho phép đa nhiệm vụ với nhiều mạng, chỉ cần một lần truyền xương sống duy nhất, do đó giảm chi phí huấn luyện và suy luận.

3

--- TRANG 4 ---
thêm một MLP dư thừa nhỏ sau mỗi lớp attention và feed-forward. Chúng tôi thí nghiệm với phiên bản adapter tuần tự của Houlsby et al. [13]. LoRA [15] chèn các ma trận low-rank có thể huấn luyện vào mỗi lớp của mô hình Transformer xương sống để tham số hóa các thay đổi trọng số. LST [29] thêm nhiều mạng phụ theo cấp bậc, với mỗi mạng phụ chịu trách nhiệm điều chỉnh các kích hoạt của một lớp cụ thể trong mô hình tiền huấn luyện. Đối với tất cả các phương pháp PETL và READ, chúng tôi giữ transformer xương sống đóng băng trong suốt quá trình huấn luyện và chỉ cập nhật các tham số mới.

Tập dữ liệu. Chúng tôi đánh giá READ và các baseline trên các điểm chuẩn GLUE [31]. Các điểm chuẩn này bao gồm nhiều tác vụ NLP khác nhau, bao gồm tính chấp nhận ngôn ngữ học (CoLA [32]), phát hiện paraphrase (MRPC [10], QQP [8], STS-B [6]), suy luận ngôn ngữ tự nhiên (MNLI [35], QNLI [27]), và phân loại cảm xúc (SST-2)³. Trong GLUE, nhãn tập kiểm tra gốc không được công khai. Thay vào đó, chúng tôi làm theo [40] và [16] để tạo một tập kiểm tra cho mỗi tác vụ như sau: nếu tập huấn luyện chứa ít hơn 10k mẫu, chúng tôi chia đều tập validation gốc thành hai tập con và coi chúng là tập validation và kiểm tra mới; nếu không, chúng tôi sử dụng tập validation gốc làm tập kiểm tra, và tách 1k từ tập huấn luyện làm tập validation mới. Đối với MNLI, chúng tôi sử dụng tập mismatched làm tập validation và tập matched làm tập kiểm tra. Chúng tôi báo cáo kích thước tập dữ liệu trong Phụ lục D.1.

Đặc tả Mô hình và Chi tiết Thí nghiệm. Chúng tôi áp dụng mô hình encoder-decoder T5 [24] làm transformer xương sống. Chúng tôi sử dụng T5BASE cho tất cả các thí nghiệm của chúng tôi, và cũng sử dụng T5LARGE cho các thí nghiệm READ, mà chúng tôi ký hiệu là READ-large. Chúng tôi thực hiện tinh chỉnh trên mỗi tập dữ liệu lên đến 30 epoch và dừng sớm khi metric validation ngừng cải thiện. Đối với READ, chúng tôi thí nghiệm với {128,256} làm kích thước ẩn RNN, {RNN, LSTM, GRU} làm kiến trúc RNN. Đối với các baseline PETL, chúng tôi thí nghiệm với {32,64} làm kích thước bottleneck của Adapter, {8,32} làm rank của LoRA, và {10,20,30} làm kích thước prompt của Prompt-tuning. Đối với tất cả các thí nghiệm, chúng tôi thực hiện tìm kiếm lưới cho tốc độ học trong khoảng [1×10⁻⁶,3×10⁻³] trên thang log cho đến 32 vòng. Chúng tôi chọn các siêu tham số đạt điểm validation tốt nhất và báo cáo điểm kiểm tra của chúng. Thiết lập đầy đủ và chi tiết siêu tham số có trong Phụ lục D.2.

4 Kết quả Đánh giá
Chúng tôi huấn luyện và đánh giá mỗi phương pháp trên tất cả các tác vụ GLUE. Chúng tôi tính tổng mức tiêu thụ năng lượng tích lũy và đo đỉnh GPU trong quá trình huấn luyện. Trong phần này, chúng tôi báo cáo và phân tích kết quả trên các Điểm chuẩn GLUE.

READ vượt trội hơn các phương pháp khác trong khi tiêu thụ năng lượng thấp hơn đáng kể: Hình 1 (trái) cho thấy READ có thể giảm tiêu thụ năng lượng GPU lên đến 90% so với tinh chỉnh đầy đủ. READ giảm dấu chân bộ nhớ GPU 56% trong khi duy trì độ chính xác mô hình tương tự khi huấn luyện lại. Trong khi các phương pháp học chuyển giao hiệu quả tham số (PETL) khác như LoRA, BitFit hoặc Adapter giảm số lượng tham số có thể huấn luyện, chúng không giảm chi phí tính toán cần thiết để tinh chỉnh. Chúng tôi tin rằng mục tiêu tối ưu hóa cơ bản cho PETL là giảm chi phí tính toán này. Bảng 1 cho thấy hiệu suất của tất cả các phương pháp trên GLUE với T5BASE. Ngoại trừ Adapter, READ vượt trội hơn tất cả các phương pháp hiệu quả tham số trong khi tiêu thụ ít nhất 68% năng lượng. So với Adapter, READ đạt được độ chính xác mô hình gần như tương tự (thấp hơn dưới 0,1%) trong khi sử dụng 70% năng lượng. Thú vị hơn, READ với T5LARGE (tức là READ-large) đạt hiệu suất tốt hơn tất cả các phương pháp khác và tiêu thụ năng lượng tương tự hoặc ít hơn so với các phương pháp khác. Ví dụ, READ-large vượt trội hơn Full-tuning và Adapter 1,4% và 0,8% với 69% và 5% năng lượng ít hơn, tương ứng. Những kết quả này cho thấy rằng bằng cách sử dụng READ, chúng ta có thể mở rộng kích thước mô hình trong khi giữ nguyên các ràng buộc phần cứng và bộ nhớ.

READ tiêu thụ ít bộ nhớ huấn luyện hơn: Hình 1 (phải) cho thấy sự cân bằng không gian thiết kế giữa hiệu suất chất lượng mô hình và dấu chân bộ nhớ. READ cải thiện yêu cầu bộ nhớ huấn luyện ít nhất 25% so với tất cả các baseline khác trong khi đạt hiệu suất tương tự hoặc tốt hơn. READ với T5LARGE tiêu thụ lượng bộ nhớ tương tự như tinh chỉnh đầy đủ với T5BASE. Khi kích thước xương sống tăng, việc tiết kiệm bộ nhớ đạt được bởi READ trở nên ngày càng đáng kể so với các phương pháp PETL khác, như được mô tả trong Hình 4 (phải). Đáng chú ý, ở mức xương sống T53B, những khoản tiết kiệm này đạt tới 43%. Quan sát này cho thấy rằng READ cực kỳ hiệu quả trong chế độ tinh chỉnh các Transformer lớn.

³Chúng tôi loại trừ RTE khỏi GLUE do kích thước nhỏ so với các tác vụ khác

4

--- TRANG 5 ---
Bảng 1: Kết quả hiệu suất và tiêu thụ năng lượng của tất cả các phương pháp trên các tác vụ GLUE. Chúng tôi báo cáo độ chính xác cho SST-2, MNLI, QNLI, và Matthew's Correlation cho CoLA. Đối với STS-B chúng tôi báo cáo trung bình của tương quan Pearson và tương quan Spearman. Đối với MRPC và QQP, chúng tôi báo cáo trung bình của điểm F1 và độ chính xác. Đối với tất cả các tác vụ, chúng tôi báo cáo điểm trung bình trên 3 seed khác nhau. Phông chữ đậm chỉ ra kết quả tốt nhất của cột đó.

[Bảng dữ liệu hiệu suất chi tiết]

Bảng 2: READ có và không có tính tái phát
[Bảng so sánh READ với và không có tính tái phát]

READ có thể mở rộng: Như được thể hiện trong Hình 4 (trái), số lượng tham số có thể huấn luyện của READ mở rộng chậm hơn so với các phương pháp PETL khác. Số lượng tham số của READ thể hiện mô hình tăng trưởng log-tuyến tính khi kích thước mô hình xương sống T5 tăng. Trên thực tế, bản chất tái phát của READ khiến kích thước có thể điều chỉnh của nó độc lập với số lượng lớp xương sống, khiến READ trở thành lựa chọn phù hợp hơn để tinh chỉnh các Transformer lớn trong thực tế.

Tầm quan trọng của tính tái phát Chúng tôi thực hiện phân tích ablation về tầm quan trọng của tính tái phát trong READ trong Bảng 2. Chúng tôi thấy rằng việc loại bỏ tính tái phát không cải thiện đáng kể chất lượng read và thậm chí còn làm giảm chất lượng đối với xương sống T5 large. Tuy nhiên, không có tính tái phát dẫn đến hơn 12 lần tham số có thể huấn luyện, làm giảm tính mở rộng.

So sánh với Ladder-Side-Tuning (LST) Chúng tôi so sánh các phương pháp của mình với Ladder-Side-Tuning (LST), một cách tiếp cận tinh chỉnh hiệu quả bộ nhớ khác [29]. Chúng tôi làm theo phương pháp pruning được giới thiệu trong [29] để trích xuất một transformer nhỏ hơn từ transformer xương sống và sử dụng nó để khởi tạo side transformer, và tái triển khai LST. Bảng 1 liệt kê kết quả của LST (sử dụng T5BASE) trên các điểm chuẩn GLUE và hiệu quả năng lượng của nó. Kết quả cho thấy read (base) vượt trội hơn LST (base) trên hầu hết các tác vụ (ngoại trừ một tác vụ nhỏ MRPC), sử dụng 80% ít tiêu thụ năng lượng hơn và 60% ít tham số có thể huấn luyện hơn. Trong khi LST tiêu thụ 15% ít bộ nhớ huấn luyện đỉnh hơn so với READ, nó mất 40% bộ nhớ suy luận hơn và 77% thời gian suy luận lâu hơn READ, một hậu quả của kiến trúc side-network dựa trên attention. Cũng đáng chú ý rằng khi so sánh với LST thậm chí READ-large tiết kiệm 38% năng lượng GPU và cho ra độ trễ suy luận tương tự, với 1,4% cải thiện tương đối trên điểm GLUE trung bình. Hơn nữa, "giai đoạn tiền huấn luyện" đề cập đến quá trình được mô tả trong bài báo LST phần 2.2, nơi chưng cất được thực hiện với mục tiêu tiền huấn luyện T5. Điều quan trọng cần lưu ý là việc lưu trữ các đầu ra attention không liên quan đến việc cập nhật bất kỳ tham số mô hình nào và không nên được coi là một hình thức huấn luyện.

5 Công trình Liên quan
Trong phần này, chúng tôi tóm tắt các công trình liên quan chặt chẽ với công trình của chúng tôi và để lại thảo luận chi tiết hơn trong Phụ lục C.

5

--- TRANG 6 ---
[Hình 4: Số lượng tham số có thể huấn luyện khi kích thước mô hình xương sống tăng.]
[Hình 5: Độ trễ suy luận khi kích thước mô hình xương sống tăng.]

Học Chuyển giao Hiệu quả Tham số. Sự bùng nổ của các ứng dụng AI tạo sinh [4,28,30,33] đã bị cản trở bởi chi phí tính toán và bộ nhớ của việc tinh chỉnh các transformer lớn. Học chuyển giao hiệu quả tham số (PETL) [1,13,18–20,29,38] giải quyết vấn đề này bằng cách huấn luyện một tập tham số tối thiểu, với nhiều phương pháp khác nhau như các cách tiếp cận dựa trên Adapter, Low-Rank Adaptation (LoRA), BitFit, và Prompt-tuning. Không giống như những phương pháp này, READ giới thiệu hiệu quả bộ nhớ bằng cách kết hợp một mạng tái phát nhỏ vào xương sống, tập trung vào việc giảm sử dụng bộ nhớ thay vì tối thiểu hóa tham số.

Huấn luyện Hiệu quả Bộ nhớ. Các chiến lược huấn luyện hiệu quả bộ nhớ, như gradient checkpointing [7], các lớp có thể đảo ngược [11], ZeRO [26], và Layer-wise Adaptive Rate Scaling (LARS) [37], nhằm giảm thiểu tiêu thụ bộ nhớ bằng cách tối ưu hóa việc lưu trữ và tính toán các kích hoạt trung gian và trạng thái mô hình, đặc biệt trong các môi trường huấn luyện phân tán.

Sidenet Tuning. Side-tuning [39] và Ladder side-tuning [29] sử dụng các mạng phụ nhẹ để thích ứng các kích hoạt mô hình tiền huấn luyện cho các tác vụ mới mà không sửa đổi mô hình cơ sở. READ, trong khi lấy cảm hứng từ những phương pháp này, phân biệt bản thân bằng cách sử dụng một khối RNN duy nhất xử lý trạng thái ẩn của mạng xương sống một cách tái phát, đảm bảo số lượng tham số tinh chỉnh không tăng theo kích thước xương sống. Không giống như Side-Tuning, READ tính toán lặp đi lặp lại các trạng thái RNN của nó trên tất cả các lớp và chỉ sử dụng các cấu trúc RNN và Feed-Forward Network (FNN), loại bỏ nhu cầu transformer hoặc cơ chế attention và cho phép sử dụng mà không cần tiền huấn luyện.

6 Kết luận và Hạn chế
Hạn chế. Do tài nguyên tính toán hạn chế của chúng tôi, chúng tôi không thể mở rộng xương sống lên quy mô lớn hơn nữa. Một hướng tương lai là tinh chỉnh read trên Llama-7B [30] hoặc thậm chí các biến thể lớn hơn. Một hướng khác có thể được nghiên cứu là liệu read có thể tổng quát hóa tốt trong chế độ dữ liệu thấp hay không. Một nhược điểm của read là xu hướng yêu cầu nhiều epoch hơn để hội tụ trên các tập dữ liệu nhỏ so với các phương pháp PETL khác. Do đó, mặc dù read hiệu quả hơn trong tính toán đơn vị thời gian, nó có thể không mang lại lợi ích tiêu thụ tổng thể đáng kể khi một tác vụ có ít điểm dữ liệu. Chúng tôi để lại việc điều tra read trong chế độ dữ liệu thấp như công việc tương lai.

Kết luận. Trong bài báo này, chúng tôi đề xuất REcurrent ADaption (READ), một phương pháp tinh chỉnh hiệu quả tham số và bộ nhớ nhẹ và hiệu quả, cho các transformer quy mô lớn. Chúng tôi cho thấy read đạt được độ chính xác tương đương với tinh chỉnh đầy đủ trong khi tiết kiệm hơn 84% tiêu thụ năng lượng và giảm tiêu thụ bộ nhớ huấn luyện 56% so với tinh chỉnh đầy đủ. Chúng tôi chứng minh tính mở rộng của read vì read độc lập với kích thước mô hình xương sống. Chúng tôi hy vọng rằng read có thể làm cho việc tinh chỉnh các mô hình lớn trở nên dễ tiếp cận hơn với một loạt rộng hơn các nhà nghiên cứu và ứng dụng.

6

--- TRANG 7 ---
Tài liệu tham khảo
[1] Armen Aghajanyan, Luke Zettlemoyer, và Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.
[2] Hangbo Bao, Li Dong, Songhao Piao, và Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.
[3] Elad Ben-Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. ArXiv, abs/2106.10199, 2021.
[4] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[6] Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio, và Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In International Workshop on Semantic Evaluation, 2017.
[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, và Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.
[8] Zihang Chen, Hongbo Zhang, Xiaoji Zhang, và Leqi Zhao. Quora question pairs. 2017.
[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[10] William B. Dolan và Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In International Joint Conference on Natural Language Processing, 2005.
[11] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, và Roger B. Grosse. The reversible residual network: Backpropagation without storing activations, 2017.
[12] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.
[13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.
[14] Jeremy Howard và Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.
[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[16] Rabeeh Karimi Mahabadi, James Henderson, và Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021.
[17] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. ArXiv, abs/2104.08691, 2021.
[18] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.

7

--- TRANG 8 ---
[19] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.
[20] Vladislav Lialin, Vijeta Deshpande, và Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning, 2023.
[21] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, và Jiawei Han. Understanding the difficulty of training transformers. In Conference on Empirical Methods in Natural Language Processing, 2020.
[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[23] Jiasen Lu, Dhruv Batra, Devi Parikh, và Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.
[24] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2019.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.
[26] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.
[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. Squad: 100,000+ questions for machine comprehension of text. ArXiv, abs/1606.05250, 2016.
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
[29] Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. arXiv preprint arXiv:2206.06522, 2022.
[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[31] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. ArXiv, abs/1804.07461, 2018.
[32] Alex Warstadt, Amanpreet Singh, và Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2018.
[33] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[34] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.
[35] Adina Williams, Nikita Nangia, và Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics, 2017.

8

--- TRANG 9 ---
[36] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, và Kim Hazelwood. Sustainable ai: Environmental implications, challenges and opportunities. In D. Marculescu, Y. Chi, và C. Wu, editors, Proceedings of Machine Learning and Systems, volume 4, pages 795–813, 2022.
[37] Yang You, Igor Gitman, và Boris Ginsburg. Large batch training of convolutional networks, 2017.
[38] Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2022.
[39] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, và Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pages 698–714. Springer, 2020.
[40] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, và Yoav Artzi. Revisiting few-sample bert fine-tuning. ArXiv, abs/2006.05987, 2020.

9

--- TRANG 10 ---
A Phụ lục
A.1 Xem lại Transformer
Trong tiểu mục này, chúng tôi xem lại ngắn gọn việc tính toán của transformer và giới thiệu một số ký hiệu thuận tiện cho tương lai. Gọi T là một transformer có chiều d với N lớp L1,···,LN. Tại mỗi lớp Li, gọi mạng feed-forward là Fi và multihead-attention là Ai. Cho một chuỗi ngữ cảnh của m token, chúng ta có thể biểu diễn mỗi lớp như một ánh xạ từ Rm×d đến Rm×d như sau:

Li = (F*i + I) ◦ A*i + I, (1)

trong đó I biểu diễn ánh xạ đồng nhất, ◦ biểu thị hợp thành ánh xạ, và F*i =: Fi ◦ LN, A*i =: Ai ◦ LN (tức là hợp thành với layer-normalization). Hơn nữa, chúng ta định nghĩa Ri = (F*i + I) ◦ A*i để viết ánh xạ lớp là Li = Ri + I.

A.2 Dẫn xuất của READ
Theo các ký hiệu trong Tiểu mục A.1, chúng tôi dẫn xuất một công thức quy nạp cho các correction:

δϕi = ϕ'i - ϕi
= (R'i + I)(ϕ'i-1) - (Ri + I)(ϕi-1)
= R'i(ϕ'i-1) - Ri(ϕi-1) + (ϕ'i-1 - ϕi-1)
= R'i(ϕ'i-1) - Ri(ϕi-1) + δϕi-1
= (R'i-1 - Ri)(ϕ'i-1) + (Ri(ϕ'i-1) - Ri(ϕi-1)) + δϕi-1
= δRi(ϕ'i-1) + JRiδϕi-1 + δϕi-1. (2)

Ở đây δRi biểu thị sự khác biệt toán tử R'i - Ri, và JRi là ma trận Jacobian của Ri(·) được đánh giá tại một điểm nào đó nằm trên đoạn thẳng từ ϕi-1 đến ϕ'i-1. Để đơn giản hóa lập luận của chúng tôi, chúng tôi (1) giả sử rằng JRi lấy giá trị tại ϕi-1, (2) để T' là hệ quả của tinh chỉnh với Adapter hoặc LoRA (được áp dụng tại các lớp FFN Fi)4. Chúng tôi sử dụng P để biểu thị một module chung được áp dụng bởi Adapter và LoRA bao gồm một ma trận chiếu xuống tới một chiều thấp hơn có thể được theo sau bởi một kích hoạt phi tuyến, và sau đó được hợp thành với một chiếu trên trở lại chiều gốc5. Dưới những giả định này, số hạng đầu tiên của vế phải trong (2) bây giờ trở thành

δRi(ϕ'i-1) = Pi ◦ (Pi + I)^-1 ϕ'i (Adapter)
              Pi ◦ (Pi + Fi)^-1 ϕ'i (LoRA)
              =: Wi(ϕi + δϕi) (3)

Bây giờ thay (3) trở lại (2), chúng ta có

δϕi = Wi(ϕi + δϕi) + JRiδϕi-1 + δϕi-1. (4)

Chú ý rằng cả hai vế của phương trình (4) đều chứa δϕi. Vì tính phi tuyến của Wi, không có cách đơn giản nào để trích xuất một công thức quy nạp của δϕi từ (4).

Tuy nhiên, hãy viết lại phương trình (4) thành

δϕi - Wi(ϕi + δϕi) - (JRiδϕi-1 + δϕi-1)
= F(δϕi, ϕi, JRiδϕi-1 + δϕi-1) = 0, (5)

và tính toán Jacobian để thấy rằng JδϕiF = I - JWi, có thể nghịch đảo khi Pi (và do đó Wi3) có chuẩn nhỏ. Bây giờ bởi Định lý Hàm Ẩn tồn tại G sao cho

δϕi = G(ϕi, JRiδϕi-1 + δϕi-1). (6)

Một lập luận thay thế là sử dụng xấp xỉ bậc nhất của Wi(ϕi + δϕi) giả sử rằng δϕi đủ nhỏ, điều này cho chúng ta công thức quy nạp sau:

δϕi = (I - JWi)^-1 ◦ (Wiϕi + JRiδϕi-1 + δϕi-1) (7)

4Đối với các phương pháp tinh chỉnh sửa đổi attention, chúng tôi mong đợi một kết luận tương tự đòi hỏi một dòng lập luận phức tạp hơn, mà chúng tôi hoãn lại cho nghiên cứu tương lai.
5Chuẩn toán tử của P nhỏ khi hai ma trận của nó có trọng số nhỏ, và do đó việc cộng với P sẽ không thay đổi tính khả nghịch của một toán tử đã khả nghịch.

10

--- TRANG 11 ---
Chúng tôi áp dụng cách tiếp cận thứ hai ở trên và áp dụng công thức (7) khi chúng tôi tiến lên, vì hình thức hàm rõ ràng của nó. Lưu ý rằng mọi hoạt động trong (7) tác động lên không gian cột của ϕ ngoại trừ phép biến đổi Jacobian JRi, vì vậy trước tiên chúng ta hãy tập trung vào việc mở rộng JRiδϕi-1. Thực tế, chúng tôi sẽ tính toán Jacobian cho một ánh xạ attention tổng quát lấy 3 đối số ϕq, ϕk, ϕv (tức là các trạng thái ẩn của query, key, và value), và sau đó áp dụng kết quả cho trường hợp đặc biệt của self-attention (như trong encoder) và cross-attention (như trong decoder) tương ứng. Vì lý do ngắn gọn, chúng tôi giả sử rằng số đầu attention là 1 và bỏ qua chiếu đầu ra, vì cả hai đều không quan trọng đối với kết luận của chúng tôi. Gọi ϕq, ϕk, ϕv là các ma trận trong Rmq×d, Rmk×d, Rmk×d, biểu diễn cho các biểu diễn vector Rd của các chuỗi token query, key, và value với độ dài mq, mk, mk tương ứng. Chúng tôi sử dụng chỉ số trên α để biểu thị vector liên quan đến token thứ α, và bỏ qua chỉ số lớp dưới i khi không có sự nhập nhằng (ví dụ Aα là cột thứ α của đầu ra A). Đầu tiên, chúng ta có

JRiδϕi-1 = (JF* + I) ◦ JA ◦ JLN(δϕi-1) (8)

theo quy tắc chuỗi. Tiếp theo chúng ta mở rộng JA, vì mọi hoạt động khác trong (8) tác động lên không gian cột của ϕ; đặc biệt, cho đến việc hợp thành với một mạng neural feed-forward, hãy để chúng tôi thay thế JLN bằng một đồng nhất để đơn giản hóa các ký hiệu của chúng tôi. Một tính toán đơn giản cho kết quả sau:

JϕqAα(δϕq) = Σβ=1^mq σαβ·(vβ - Aα)·kβT / √d · WQ δϕqα,
JϕkAα(δϕk) = Σβ=1^mk σαβ·(vβ - Aα)·qαT / √d · WK δϕkβ,
JϕvAα(δϕv) = Σβ=1^mk σαβ·WV δϕvβ. (9)

Ở đây WQ, WK, WV biểu thị các ma trận chiếu query, key, và value của A, qα = WQϕqα, kα = WKϕkα, và σαβ = softmax(qαT·kβ/√d).

Trường hợp 1, A là self-attention Khi đặt ϕq, ϕk, ϕv thành ϕ, và δϕq, δϕk, δϕv thành δϕ trong (9), chúng ta có:

JAαδϕ = Σβ=1^mq σαβ·(vβ - Aα)·kβT / √d · WQ δϕα
       + Σβ=1^mq σαβ·(vβ - Aα)·qαT / √d · WK + WV δϕβ. (10)

Lưu ý hai đại lượng trong dấu ngoặc vuông là các hàm tuyến tính có giá trị ma trận Rd×d của các giá trị có thể được tính toán từ các kết quả được lưu trữ tại Li, mà chúng tôi sẽ ký hiệu bằng Φ, Ψ từ bây giờ:

JAαδϕ = Φ·δϕα + Σβ=1^mq σαβΨ·δϕβ. (11)

Bây giờ, khi chèn (11) vào cột thứ α của (8) bằng cách đặt ϕ là ϕi, và sau đó thay (8) trở lại trong (7), chúng ta có công thức lặp cho đầu ra hi:

ψαi = Φi·Fiδϕαi-1 + Σβ=1^m σαβi Ψi·Fiδϕβi-1
xαi = [ϕαiT, ψαiT]T
δϕαi = Gi(Hixαi + δϕαi-1) (12)

trong đó Φi, Ψi được định nghĩa như trong (11), và Fi, Gi, Hi là các FNN mô phỏng JLN, (I - JWi)^-1, và [Wi, JF*i + I] tương ứng; xem (7) và (8). Lưu ý (12) chính xác là (??) khi thay thế δϕ bằng h.

Trường hợp 2, A là cross-attention Vì công thức lặp correction của decoder tuân theo từ một dòng lập luận tương tự như self attention, chúng tôi trình bày kết quả cuối cùng trong khi bỏ qua các chi tiết:

ψαi = Φi·FDi δϕD,αi-1 + Σβ=1^m σαβi Ψi·FEi δϕE,β
xαi = [ϕαiT, ψαiT]T
δϕD,αi = Gi(Hixαi + δϕD,αi-1) (13)

11

--- TRANG 12 ---
Bảng 3: Kết quả hiệu quả của LST, READ, và Full-tuning. Chúng tôi báo cáo việc sử dụng năng lượng GPU huấn luyện tổng cộng trên tất cả các tác vụ, và bộ nhớ huấn luyện đỉnh (mỗi batch) trung bình trên tất cả các tác vụ. Đối với bộ nhớ/thời gian suy luận, chúng tôi sử dụng MNLI và báo cáo trung bình mỗi batch (với kích thước batch kiểm tra 1).

[Bảng kết quả hiệu quả chi tiết]

trong đó chỉ số trên D\E được sử dụng để phân biệt giữa các trạng thái ẩn của decoder và encoder, và δϕE là correction cuối cùng của encoder.

A.3 Thí nghiệm Ablation
A.4 Phân tích Năng lượng GPU
Để cung cấp một hiểu biết toàn diện, chúng tôi bao gồm một phân tích dưới đây để cho thấy trung bình và độ lệch chuẩn cho tổng năng lượng GPU, số epoch để hội tụ, và thời gian huấn luyện trên tất cả các tác vụ GLUE. Mặc dù phân tích này tiết lộ một số biến thiên trong mức năng lượng/thời gian, chúng không đủ đáng kể để thay đổi xu hướng chung, vì READ tiếp tục nổi bật như cách tiếp cận tiết kiệm năng lượng nhất, với hội tụ nhanh hơn hầu hết các baseline ngoại trừ full-tuning.

[Bảng 4: Tiêu thụ Năng lượng GPU và Thời gian Huấn luyện qua 3 thử nghiệm.]

A.5 Hiệu quả Suy luận và Bộ nhớ của READ
Như Hình 5 (trái) và Bảng 5 cho thấy, READ đạt được độ trễ suy luận và yêu cầu bộ nhớ tương đương với các phương pháp PETL khác. Để đánh giá tác động bộ nhớ suy luận của READ một cách toàn diện hơn, chúng tôi sử dụng Hình 5 (phải) để chứng minh rằng, khi kích thước xương sống tăng, sự tăng trưởng bộ nhớ suy luận (so với Full-tuning) của READ trở nên ít đáng chú ý hơn và suy giảm đến mức tương tự như các phương pháp khác ở T5 LARGE.

B Kiến trúc
B.1 Lựa chọn kiến trúc
Các hàm ma trận Ψ, Φ trong phương trình (12) và (13) yêu cầu tính toán tích vô hướng cho m² cặp vector (9) với độ phức tạp thời gian lớn như O(m²d²). Để giảm chi phí độ trễ trong thực tế, chúng tôi thực hiện các giảm thiểu đáng kể đối với phương trình đầu tiên trong cả (12) và (13) cho các thí nghiệm READ trong bài báo này, như được liệt kê dưới đây:

[Bảng 5: Tiêu thụ bộ nhớ suy luận trung bình (GB) cho mỗi phương pháp với các xương sống khác nhau trên điểm chuẩn GLUE.]

12

--- TRANG 13 ---
Bảng 6: Kích thước phân chia, số GPU huấn luyện, và kích thước batch huấn luyện mỗi nút GPU cho tất cả các tác vụ GLUE.

[Bảng chi tiết kích thước dữ liệu]

• Các chỉ số i được loại bỏ và các tham số có thể học được hợp nhất trên tất cả các lớp;
• Trong self-attention, chúng tôi đặt Ψ, Φ luôn bằng không; nói cách khác, chỉ các trạng thái ẩn được lưu trữ và sử dụng cho các correction encoder;
• Trong cross-attention, chúng tôi đặt Φ bằng không và Ψ·FEi hβi-1 =: Lhβi-1, trong đó L là một chiếu tuyến tính có thể học, vì vậy ngoài các trạng thái ẩn decoder chúng tôi cũng cần lưu trữ các điểm cross-attention để tính toán các correction decoder. Hơn nữa, chúng tôi sử dụng một hoạt động cộng đơn giản để kết hợp ϕi và ψi trong (13) thay vì một lớp có thể học.

Lưu ý một số giảm thiểu chúng tôi thực hiện ở trên có thể được đơn giản hóa quá mức nhưng bài báo này không khám phá các tùy chọn phức tạp hơn⁶ trong khi vẫn hiệu quả về mặt tính toán, chẳng hạn như một mạng neural có cổng:

Φ·Fi(hαi-1) = Gate(ϕαi) ⊙ FFN(hαi-1),
Ψ·Fi(hβi-1) = Gate(ϕαi) ⊙ FFN(hβi-1), (14)

trong đó v ⊙ X = diag(v)·X. Chúng tôi để lại các khám phá liên quan cho các công việc tương lai.

B.2 Thuật toán READ
Thuật toán 1 phác thảo một lần truyền tiến trong quá trình tinh chỉnh READ. Gọi T là một transformer với NE lớp encoder và ND lớp decoder, và X\Y là các chuỗi nguồn\đích với độ dài m\n:

C Công trình Liên quan
Học Chuyển giao Hiệu quả Tham số. Đã có một sự bùng nổ của các ứng dụng AI tạo sinh trong những tháng gần đây [4,28,30,33]. Tuy nhiên, khả năng tinh chỉnh các transformer lớn chủ yếu bị hạn chế bởi chi phí tính toán ngày càng tăng cần thiết để thích ứng và phục vụ các mô hình này. Học chuyển giao hiệu quả tham số (PETL) [1,13,18–20,29,38] nhằm giải quyết vấn đề này bằng cách chỉ huấn luyện một tập nhỏ các tham số. Có nhiều phương pháp PETL mà chúng tôi giới thiệu độc giả đến [20] để có cái nhìn tổng quan toàn diện hơn. Trong phần này, chúng tôi sẽ tóm tắt các phương pháp PETL phổ biến nhất mà chúng tôi sử dụng làm baseline. Các cách tiếp cận dựa trên Adapter [12,13] chèn các module có thể học nhỏ giữa các lớp mô hình tiền huấn luyện và chỉ cập nhật các adapter này trong quá trình tinh chỉnh, giảm chi phí tính toán và yêu cầu bộ nhớ. Low-Rank Adaptation (LoRA) [15] tiêm các ma trận phân tích rank có thể huấn luyện vào mỗi lớp của mô hình Transformer. BitFit [38] chỉ tinh chỉnh các bias của mô hình. Prompt-tuning [18] là người kế nhiệm của Prefix-Tuning [19], thêm một prompt liên tục đặc thù cho tác vụ vào đầu vào. Ngược lại, các cách tiếp cận PETL hiện tại nhằm giảm thiểu số lượng tham số được huấn luyện. Các cách tiếp cận này không dẫn đến hiệu quả bộ nhớ, một mục tiêu có ý nghĩa hơn so với hiệu quả tham số. Công trình này đề xuất READ, các phương pháp hiệu quả bộ nhớ đơn giản bằng cách chèn một mạng tái phát nhỏ vào xương sống.

⁶Một lựa chọn phức tạp hơn có thể giới thiệu nhiều phụ thuộc hơn vào các kết quả được lưu trữ và có khả năng cải thiện hiệu suất với sự đánh đổi của số lượng flop tính toán cao hơn.

13

--- TRANG 14 ---
Thuật toán 1 Thuật toán Tinh chỉnh READ
Khởi tạo RNN NE, ND và một chiếu có thể học Ψ.
{ϕE,αi}NE,m i=1,α=1, {ϕD,αj}ND,n j=1,α=1, {σE,αβi}NE,m,m i=1,α=1,β=1, {σD,αβj}ND,n,m j=1,α=1,β=1 ← T(X, Y)
hE,0 ← 0 ▷ Chúng tôi giả sử embedding không cần correction.
for i in 1, ···, NE do ▷ Tính toán lặp đi lặp lại các correction encoder.
    hE,αi = NE(ϕαi, hE,αi-1), ∀α
hD,0 ← 0
for j in 1, ···, ND do ▷ Tính toán lặp đi lặp lại các correction decoder.
    ψαj = Σm β=1 σαβD,j Ψ hE,βNE, ∀α
    xαj = ϕαj + ψαj, ∀α
    hD,αj = ND(xαj, hD,αj-1), ∀α
ϕ'DND ← ϕDND + hDND ▷ Có được đầu ra thích ứng.

Huấn luyện Hiệu quả Bộ nhớ. Huấn luyện hiệu quả bộ nhớ giảm tiêu thụ bộ nhớ bằng cách giảm việc lưu trữ các kích hoạt trung gian [29]. Gradient checkpointing [7] giảm tiêu thụ bộ nhớ trong quá trình lan truyền ngược bằng cách lưu trữ một tập con các kích hoạt trung gian và tính toán lại chúng khi cần, trao đổi thời gian với bộ nhớ. Các lớp có thể đảo ngược [11] tái tạo các kích hoạt của mỗi lớp từ các kích hoạt của lớp tiếp theo. ZeRO [26] phân chia các trạng thái mô hình, gradient, và trạng thái optimizer trên nhiều thiết bị cho huấn luyện phân tán, giảm đáng kể sự dư thừa bộ nhớ. Layer-wise Adaptive Rate Scaling (LARS) [37] tự động điều chỉnh tốc độ học cho các lớp khác nhau, giảm overhead bộ nhớ liên quan đến gradient lớn và cho phép huấn luyện các mô hình lớn với bộ nhớ hạn chế.

Sidenet Tuning. Side-tuning [39] thêm một mạng phụ nhẹ cùng với mô hình tiền huấn luyện. Trong quá trình huấn luyện, mạng phụ và đầu đặc thù cho tác vụ được cập nhật trong khi các tham số của mô hình tiền huấn luyện được giữ cố định. Mạng phụ học cách điều chỉnh các kích hoạt của mô hình tiền huấn luyện, cho phép nó thích ứng với tác vụ mới mà không thay đổi mô hình cơ sở. Ladder side-tuning [29] thêm nhiều mạng phụ theo cấp bậc, với mỗi mạng phụ chịu trách nhiệm điều chỉnh các kích hoạt của một lớp cụ thể trong mô hình tiền huấn luyện. Trong khi READ lấy cảm hứng từ Side-Tuning và LST, chúng tôi muốn nêu bật những khác biệt đáng kể giữa READ và các công việc trước đây. Đầu tiên, READ chỉ chứa một khối RNN duy nhất lấy trạng thái ẩn của mạng xương sống theo cách tái phát. Bằng cách này, số lượng tham số để tinh chỉnh không tăng theo kích thước của xương sống, trong khi LST gắn nhiều khối transformer vào mạng xương sống. Khi xương sống trở nên lớn hơn, kích thước của mạng LST cũng trở nên lớn hơn. Thứ hai, Side-Tuning sử dụng một mạng phụ cộng để cộng biểu diễn của nó với mạng xương sống chỉ trong lớp cuối cùng. READ tiêu thụ trạng thái ẩn của xương sống ở mỗi lớp để tính toán lặp đi lặp lại các trạng thái RNN của nó. Bản chất tái phát của RNN cho phép thông tin chảy từ lớp này sang lớp khác, đó là lý do tại sao READ vượt trội hơn các phương pháp PETL khác. Cuối cùng, tinh chỉnh của chúng tôi không có transformer vì chỉ các cấu trúc RNN và Feed-Forward Network (FNN) được sử dụng trong READ và không yêu cầu transformer hoặc cơ chế attention. Chúng tôi có thể sử dụng một mạng READ được khởi tạo ngẫu nhiên mà không cần qua tiền huấn luyện như trong LST hoặc khai thác bất kỳ thủ thuật tinh tế nào để huấn luyện một transformer.

D Chi tiết Thí nghiệm
Đo lường Tiêu thụ Năng lượng. Hiệu quả huấn luyện cao hơn dẫn đến tiêu thụ năng lượng thấp hơn. Để chứng minh lợi ích hiệu quả huấn luyện của READ, chúng tôi đo lường và báo cáo tiêu thụ năng lượng GPU huấn luyện (tính bằng kWh) cho mỗi thí nghiệm. Chúng tôi áp dụng phương pháp thường được sử dụng sau đây để đo lường và ước tính tiêu thụ năng lượng huấn luyện mô hình. Chúng tôi tính vào

14

--- TRANG 15 ---
Bảng 7: Kiến trúc mô hình cho bốn mô hình T5 có kích thước khác nhau.

[Bảng chi tiết kiến trúc T5]

Bảng 8: Lựa chọn kiến trúc cuối cùng cho tất cả các thí nghiệm PEFT được báo cáo trong Phần 4.

[Bảng lựa chọn siêu tham số]

việc sử dụng tài nguyên GPU khi tính toán tiêu thụ năng lượng tương ứng bằng cách giả sử một mối quan hệ tuyến tính đơn giản giữa việc sử dụng GPU và tiêu thụ năng lượng của nó. Giả sử một thí nghiệm huấn luyện kéo dài H giờ trên GPU, với tiêu thụ năng lượng p0 kW, ở mức sử dụng GPU (tổng cộng trên tất cả các nút GPU) u(t) (tính bằng phần trăm). Thì tổng tiêu thụ năng lượng (tính bằng kWh) được đưa ra bởi

E = ∫₀ᴴ u(t)/100 · p0 dt = H · (1/H ∫₀ᴴ u(t) dt) · p0/100. (15)

Trong thực tế, chúng tôi lấy mẫu u(t) với độ chi tiết phút trong suốt quá trình huấn luyện sử dụng System Management Interface (smi) của NVIDIA. Sau đó chúng tôi tính tổng tích lũy của nó U = Σ⁶⁰ᴴᵢ₌₁ ui, do đó chúng tôi có thể xấp xỉ vế phải của Phương trình (15) bằng

H · (Σ⁶⁰ᴴᵢ₌₁ ui)/(60H) · p0/100 = U · p0/6000. (16)

Khi báo cáo phân tích tiêu thụ năng lượng cho READ và các thiết kế khác (xem Phần 4), chúng tôi sử dụng p0 = 0,25 kW cho một GPU NVIDIA V100 32 GB⁷ cho Phương trình (16).

D.1 Chi tiết tập dữ liệu và mô hình
Tập dữ liệu GLUE Trong Bảng 6, chúng tôi liệt kê kích thước tập dữ liệu, số nút GPU, và kích thước batch huấn luyện mỗi nút GPU cho mỗi tác vụ trong GLUE. Lưu ý tổng kích thước batch (tổng cộng trên tất cả các nút) được cố định là 96 trên tất cả các tác vụ và tất cả các phương pháp.

Mô hình T5 Bảng 7 đưa ra các con số liên quan đến kiến trúc cho bốn kích thước của mô hình T5. Lưu ý đối với tất cả các thí nghiệm T5BASE chúng tôi sử dụng các kiến trúc gốc, trong khi đối với các thí nghiệm READ với T5LARGE, chúng tôi bỏ 4 lớp cuối cùng từ cả encoder và decoder.

D.2 Siêu tham số
Tìm kiếm kiến trúc Đối với các phương pháp tinh chỉnh có siêu tham số kiến trúc có thể điều chỉnh (ví dụ: kích thước ẩn RNN trong READ, rank trong LoRA, v.v.), chúng tôi thực hiện tìm kiếm siêu tham số như sau: đầu tiên, chúng tôi cố định kiến trúc A (ví dụ: trong READ, lấy RNN-dim = 128 và loại side-net là LSTM), và thực hiện tìm kiếm tốc độ học cho mỗi tập dữ liệu D. Trong số mỗi lần quét siêu tham số H(D) có

⁷250W xuất phát từ datasheet trên trang web của NVIDIA

15

--- TRANG 16 ---
Bảng 9: Tốc độ học cuối cùng cho tất cả các phương pháp tinh chỉnh và tập dữ liệu GLUE

[Bảng tốc độ học chi tiết]

một lần chạy R*(D) có điểm validation tốt nhất S(D). Sau đó chúng tôi tính trung bình của S(D) trên tất cả các tập dữ liệu D làm điểm chất lượng của A, được ký hiệu là S(A). Bây giờ chúng tôi chuyển sang kiến trúc tiếp theo (ví dụ: trong READ, lấy RNN-dim = 256 và side-net là GRU) và lặp lại quá trình trên. Sau khi lặp qua tất cả các ứng viên kiến trúc, chúng tôi chọn kiến trúc A* có điểm tốt nhất S(A*), và báo cáo điểm kiểm tra của mỗi lần chạy tốt nhất R*(D) của A*. Do đó, mỗi phương pháp trong Bảng 1 áp dụng các kiến trúc giống nhau trên tất cả các tập dữ liệu. Đối với Full-tuning và BitFit nơi không có siêu tham số kiến trúc, chúng tôi thực hiện tìm kiếm tốc độ học một lần để có được điểm kiểm tra.

Tìm kiếm tốc độ học Đối với mỗi lần quét tốc độ học, chúng tôi thực hiện tìm kiếm tốc độ học trong khoảng [1×10⁻⁶, 3×10⁻³] ở thang log cho đến 32 vòng, nơi chúng tôi sử dụng tối ưu hóa Bayesian để hội tụ nhanh hơn của các lần quét siêu tham số với chi phí tính toán thấp hơn.

Lựa chọn siêu tham số Bảng 8 và 9 tóm tắt các lựa chọn cuối cùng của chúng tôi về siêu tham số kiến trúc và tốc độ học.

16
