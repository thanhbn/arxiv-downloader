# 2311.09179.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2311.09179.pdf
# File size: 301966 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SiRA: Sparse Mixture of Low Rank Adaptation
Yun Zhu∗Nevan Wichers∗Chu-Cheng Lin∗Xinyi Wang∗Tianlong Chen†
Lei Shu∗Han Lu∗Canoee Liu∗Liangchen Luo∗Jindong Chen∗Lei Meng∗
Google∗, CSAIL@MIT†
Abstract
Parameter Efficient Tuning has been an promi-
nent approach to adapt the Large Language
Model to downstream tasks. Most previous
works considers adding the dense trainable
parameters, where all parameters are used to
adapt certain task. We found this less effective
empirically using the example of LoRA that
introducing more trainable parameters does
not help. Motivated by this we investigate
the importance of leveraging “sparse” compu-
tation and propose SiRA: sparse mixture of
low rank adaption. SiRA leverages the Sparse
Mixture of Expert(SMoE) to boost the perfor-
mance of LoRA. Specifically it enforces the
topkexperts routing with a capacity limit re-
stricting the maximum number of tokens each
expert can process. We propose a novel and
simple expert dropout on top of gating network
to reduce the over-fitting issue. Through ex-
tensive experiments, we verify SiRA performs
better than LoRA and other mixture of expert
approaches across different single tasks and
multitask settings.
1 Introduction
Large Language Models (Thoppilan et al., 2022;
Passos et al., 2023; Ouyang et al., 2022; Touvron
et al., 2023) (LLMs) have demonstrated impres-
sive capabilities in a wide range of tasks. Yet
to adapt these general-purpose models to down-
stream low resource tasks is especially important.
To this end parameter efficient tuning (PET) (Hu
et al., 2021; Li and Liang, 2021; Lester et al.,
2021; Houlsby et al., 2019; Zhang et al., 2023;
Zaken et al., 2021; Chen et al., 2022), which in-
troduces task specific weights to the frozen foun-
dation model for gradient descent, avoids catas-
trophic forgetting (Luo et al., 2023) of fine-tuning
and offers better quality and lower cost than in-
context learning (Liu et al., 2022). LoRA (Hu
et al., 2021) is a widely adopted PET method
∗Correspondence to yunzhu@google.com .which achieves high performance leveraging low-
rank matrices.
One question for user of LoRA or other PET
approach is how much trainable parameter should
be used. In the case of LoRA, it is controlled
by the rank of the low-rank matrices. And by
increasing this value more computation could be
provided to fit specific tasks. However it has been
shown higher rank matrices will not introduce bet-
ter quality to the model due to the instability of
training (Chen et al., 2022), which we verify in
Figure 2 in Appendix 8.1. This poses a hidden bot-
tleneck for the quality of the model even when we
have enough computation budget, and it remains a
challenge to improve the quality of LoRA.
On the other hand, leveraging Sparse Mixture
of Experts in neural networks has been exten-
sively studied as a replacement for FeedForward
Networks, with different approaches to find the
optimal assignment between experts and tokens,
including reinforcement learning (Bengio et al.,
2015), linear programs (Lewis et al., 2021), fixed
rules (Roller et al., 2021), top-1 gating (Fedus
et al., 2022), top-2 gating (Lepikhin et al., 2020),
top-k gating (Shazeer et al., 2017), reverse ex-
pert choosing (Zhou et al., 2022), and soft as-
signment (Puigcerver et al., 2023). Several re-
cent works have proposed to utilize mixture-of-
expert models on top of parameter-efficient tun-
ing (Wang et al., 2022; Zadouri et al., 2023).
However, prior works overlooked the potential of
sparse MoE (SMoE), arguably because of issues
like token dropping (Puigcerver et al., 2023) and
overfitting (Elbayad et al., 2022).
To this end, we investigate leveraging “sparse”
computation for PET in this paper and propose
SiRA , the Sparse Mixture of Low Rank Adap-
tation. SiRA enforces the top kexperts rout-
ing, which improves resource and computation
utilization and also facilitates more fine-grained
capacity control of a given input. SiRA consistsarXiv:2311.09179v1  [cs.CL]  15 Nov 2023

--- PAGE 2 ---
of three important ingredients: the capacity con-
straint which embraces token dropping, an auxil-
iary loss to penalize over-utilizing few experts, and
a novel expert dropout mechanism. They work to-
gether to ensure the proper load balancing and ad-
dress the over-fitting issue.
We conducted extensive experiments to verify
the performance of SiRA, which achieves better
performance than LoRA (Hu et al., 2021) and
its MoE variants Adamix (Wang et al., 2022),
MoLoRA (Zadouri et al., 2023) across wide range
of single task and multitask benchmarks. Our
ablation study showed that the number of used
experts and capacity per expert improves perfor-
mance which demonstrates advantage of being
“sparse”. Notably the expert dropout plays an im-
portant role, and it is more effective than SMoE-
dropout (Chen et al., 2023a).
2 Related Works
Parameter Efficient Tuning (PET) Parameter
Efficient Tuning has a variety of flavors such as
Adapters (Houlsby et al., 2019), Prefix Tuning (Li
and Liang, 2021; Liu et al., 2021), Prompt Tun-
ing (Lester et al., 2021), P-tuning (Liu et al.,
2023), attention-injection (Zhang et al., 2023),
LoRA (Hu et al., 2021; Dettmers et al., 2023), and
combinations of PET methods (Mao et al., 2021).
In this paper, our focus on LoRA as it has been
found to achieve better results, although the meth-
ods could be applied to other flavors as well.
MoE for PET methods Along the intersection
of PET and MoE, Adamix (Wang et al., 2022) and
MoLoRA (Zadouri et al., 2023) are most similar
to our work. Adamix randomly chooses an ex-
pert in training and averages all the experts dur-
ing inference. Albeit efficient, this method is more
like checkpoint averaging (Gao et al., 2022) be-
cause the experts are randomly chosen they don’t
learn to specialize. More importantly, the ran-
dom approach has significantly longer training
time, which is multiplied by the number of ex-
perts used. MoLoRA applies the full soft MoE on
the top of LoRA, where all experts are averaged
using a learned gating. Compared to this work,
our method can achieve better efficiency. Firstly,
SiRA does not need longer time to train compared
to standard LoRA, thanks to the quick conver-
gence of SMoE (Fedus et al., 2022). Secondly, the
sparsity is enforced in SiRA which saves the train-
ing resource and inference computation comparedto MoLoRA.
Another track of the MoE work is for multitask-
ing, such as Task-MoE (Kudugunta et al., 2021)
and Skill Selection (Ponti et al., 2023). These ap-
proaches assume the external task-id as an extra
input for training and inference. Although we ex-
periment with MoE in multitask settings, it does
not require the task-id of inputs.
3 Sparse Mixture of Low Rank
Adaptation
To increase the capacity of LoRA (Hu et al.,
2021) using Mixture of Experts (MoE) without
adding too much computational cost, we propose
Sparse Mixture of Experts of Low Rank Adapta-
tion (SiRA), which leverages multiple lightweight
LoRA adaptors as experts while enforcing sparsity
when using the experts.
Figure 1 shows an illustration of SiRA. The
MoE layer for the adapter consists of Eexperts,
each with their own LoRA weights, W1, ..., W E.
Wkis the product of two low rank matrices Wk=
BkAk. We also assume the base foundation model
hasW0as it is frozen weight, which represents ei-
ther query, key, value, or output projection. Next,
we will introduce each component of our frame-
work that enforces the sparsity of LoRA experts.
Expert Gating To reduce the computational
cost, SiRA only actives a subset of all the expert
modules. Formally, during each forward pass, we
select Kout of Eexperts using the output scores
of a gating network θg. The process is mathemat-
ically expressed as Equation (1) and (2), where s
denote the token index of the sequence xandGs,e
is the gating network output at s-th token e-th ex-
perts.
G(xs) = TopK(softmax( θT
gxs)) (1)
ys=EX
e=1Gs,eWe(xs) +W0(xs) (2)
Experts Dropout We propose a practical way to
make the gate more balanced with a simple gate
dropout. Specifically, we introduce dropout to the
gating output Gas shown in Equation 3.
G(xs) = TopK(Dropout(softmax( θT
gxs)))
(3)

--- PAGE 3 ---
Attention Projection Layer 
Topk 
Attention 
Projection 
Weights Weighted Sum Add
 Used Capacity ……
LoRA weights 
……
Overflow Expert 
Dropout 
Gate Figure 1: SiRA: Sparse Gated Mixture of LoRA.
Expert Token Capacity We enforce the capac-
ity constraints for experts following GShard (Lep-
ikhin et al., 2020). Specifically, we restrict that the
number of tokens processed by each expert should
not exceed a predefined threshold. Once the ca-
pacity is reached, one expert simply drop the over-
flow tokens. If all Kexperts reach their token ca-
pacity before all tokens in a training example is
processed, the rest of the tokens will only be en-
coded using the frozen model parameter W0.
Auxiliary Loss We define an auxiliary loss term
to further encourage load balancing among differ-
ent experts (Shazeer et al., 2017). We define the
total number of tokens to be S, and there is E
experts. We denote ceas the number of tokens
routed to expert e. By using the mean gates per
expert me=Mean s(Dropout(softmax( θT
gxs)))
as a differentiable approximation, we define the
aux loss in Equation 4.
laux=1
EEX
e=1ce
S∗me (4)
4 Experiments
4.1 Evaluation Setup
Baselines and Experiment Configs We specif-
ically compare our model with the standard
LoRA (Hu et al., 2021), Adamix (Wang et al.,
2022) and MoLoRA (Zadouri et al., 2023). Note
that other adapter approaches are not commpared
with as the SiRA approach is orthogonal and could
be applied on top of them as well. We choose the
PALM2-FLAN XXS (Passos et al., 2023) as the
foundation model. We follow the default configu-
rations in (Hu et al., 2021) to inject LoRA weights
on the attention projections and set the intrinsicrank as 4. We use 16experts by default across all
baselines. For training config and model selection,
see Appendix 8.2.
Datasets and Metrics We evaluate on the fol-
lowing datasets:1
XTREME-UP The XTREME-UP dataset
(Ruder et al., 2023) is a multilingual multitask
dataset, with a focus on the scarce-data scenarios
of underrepresented languages. In this work, we
choose two of the underrepresented languages—
Swahili(SW) and Bengali(BN)—and evaluated on
several NLP tasks where these two languages have
training and evaluation data. We follow Ruder
et al. (2023) for each task’s splits and evaluation
metrics.
FinQA FinQA (Chen et al., 2021) is a QA
dataset in the financial domain. Complex rea-
soning capabilities are needed to correctly answer
these questions. Note that the answers of the
FinQA dataset are programs of a special arithmetic
DSL. In this work we only evaluate on metrics
based on surface form matching, i.e., exact match
and F1 scores.
ForumSum ForumSum (Khalman et al., 2021)
is a diverse and high quality conversation sum-
marization dataset with human written summaries
where the conversations are collected from a
wide variety of internet forums. We report
BLEURT (Sellam et al., 2020), ROUGEL, and F1
scores.
4.2 Performance of SiRA
We evaluate all the single tasks performance in
Table 1. Note that as FinQA is a hard task with
financial reasoning, thus the exact match and f1
score is relatively low. We can notice that SiRA
is outperforming all other baselines at most of the
tasks, with less than 1%extra parameters. No-
tably when compared to MoLoRA, SiRA achieves
constantly better performance among all the tasks.
This demonstrates that “sparse" MoE is better than
“full" MoE. Adamix shows some small advantage
on the Semantic Parsing task, but overall loses to
SiRA across all other tasks.
We also conducted experiments on two multi-
task settings on language swahili (SW) and ben-
gali(BN), and two multiligual settings for QA in
languages task and QA across languages task. We
1Since our base model (Chung et al., 2022) had been ex-
posed to many public datasets during training, we choose
dataset that are not consumed yet.

--- PAGE 4 ---
Table 1: Performance Comparison For Single Tasks
Approach δParams FinQA (EN) ForumSum (EN) SP (SW) QA-in (SW) NER (SW) SP (BN) QA-in (BN) QA-cross (BN)
em f1 bleurt rougeL f1 accuracy f1 span-f1 accuracy f1 f1
LoRA 0.043% 5.0 5 .6 96.70 33 .97 23 .54 27.63 82.08 88.95 33.52 80.34 76.81
Adamix 0.689% 5.6 6.0 95.95 35.10 23.88 33.22 81.24 89.00 39.03 81.70 76.07
MoLoRA 0.746% 5.6 6.4 97.05 34.37 24.79 32.50 82.33 89.33 36.28 79.06 76.75
SiRA 0.746% 5.8 6.6 97.14 35.67 25.83 32.52 83.00 89.95 38.61 82.10 76.93
Table 2: Performance Comparison For Multi Tasks
Approach δparams SW Multitask BN Multitask
SP(accuracy) QA-in(f1) NER(span-f1) Average SP(accuracy) QA-in(f1) QA-cross(f1) Average
LoRA 0.043% 28.06 77.71 88.28 64.69 32.06 79.27 75.03 62.12
Adamix 0.689% 35.14 76.99 89.01 67.10 38.41 79.49 75.09 64.33
MoLoRA 0.746% 33.44 79.91 88.92 65.66 35.98 78.14 76.37 63.49
SiRA 0.746% 33.98 81.26 89.04 68.10 37.71 82.17 75.50 65.13
Table 3: Performance Comparison For Multilingual
Tasks.
Approach δparams QA-in (9) QA-cross (25)
LoRA 0.043% 85.09 69.41
Adamix 0.689% 84.75 70.42
MoLoRA 0.746% 85.14(WIP) 69.70(WIP)
SiRA 0.746% 86.38 70.86
Table 4: Self ablations on the hyper-parameter topK(K)
and expert capacity(C) on ForumSum.
Configs bleurt rougeL f1
K=2 96.87 34.51 24.73
K=4 96.60 34.66 25.34
K=6 96.75 34.73 24.55
K=8 96.76 35.31 25.64
K=10 97.51 35.10 25.19
K=12 96.96 34.49 24.24
C=2 96.33 34.15 24.13
C=4 96.60 34.66 25.34
C=6 97.14 35.67 25.83
C=8 97.31 34.97 25.24
C=10 97.25 34.75 25.57
C=12 96.50 34.44 23.94
reported numbers in Table 2 and Table 3 respec-
tively. The overall trend is similar to what we
found in the single tasks. SiRA achieves the best
average performance among all baselines.
4.3 Ablation Study
Computation Ablations We share the ablations
on ForumSum in Table 4. We choose a simple con-
fig as base (k=4, C=4) and then change each of
them while keeping the rest. We first range the top
Kfrom 2to12, with capacity C=K. And then
we fix the K= 4, and range the expert capacity
from 2to12. An interesting finding is increasing
the computation or the capacity will not always in-Table 5: Gating ablations on ForumSum.
Approach bleurt rougeL f1
SiRA 97.14 35.67 25.83
- aux loss 96.37 35.09 25.11
- Expert Dropout 97.09 34.73 24.55
+ SMoE-Dropout 96.30 34.24 24.32
crease the scores and there is a “comfortable zone”
which we need to find out with model tuning. This
also justifies why the “full” MoE based approach
is not as good as SiRA. SiRA provides more fine-
grained control on the computation.
Gating ablations We also provide the ablations
on the gating in Table 5. Specifically we compare
SiRA with 3 cases: 1/ removing the aux loss, 2/ re-
moving the gate dropout, and 3/ using a static rout-
ing based dropout SMoE-Dropout (Chen et al.,
2023a) instead. Results suggested that the learned
gating is still better than a fixed one, and both the
gate dropout and aux loss help the performance of
SiRA.
What the gate learns We use the Swahili mul-
titask experiment to study what the gate is learn-
ing. We measure the average entropy of each gate
weight distribution before TopK is applied. The
average entropy for the QA (in language) task de-
creases from 1.6 to 1.13 nats during training. This
indicates that the model learns to give certain gates
more weight as it trains.
We also measure the average correlation coeffi-
cients between each task index and each gate in-
dex similar to (Chen et al., 2023b). We convert the
task index to a one hot encoding for this. At the
end of training, the average correlation was about

--- PAGE 5 ---
.025, which is not significant. The correlation be-
tween gates and languages in the multilingual ex-
periment is not significant either. This suggests
that our gating mechanism does not learn to route
different tasks to different gates.
5 To-Do List
This manuscript is currently under active devel-
opment. Our upcoming endeavors include getting
more results and analysis, and improving the writ-
ings. We warmly welcome suggestions, insights,
and constructive criticism from the research com-
munity. Should you have any feedback or ideas
that could potentially enhance the quality and im-
pact of our work, please do not hesitate to contact
the lead author. Your input is invaluable to us, and
we are eager to integrate diverse perspectives to
refine and advance our research.
6 Conclusion
This paper introduced SiRA, a Sparse Mixture of
Expert variant of LoRA. SiRA enforces the top k
experts routing with capacity constraint for each
experts. We also devise a novel expert dropout
mechanism on top of the auxiliary loss to re-
duce its over-fitting issue. We conducted extensive
experiments to verify the performance of SiRA,
which achieves better performance than the LoRA
and its MoE variants across different single tasks
and multitask settings.
7 Limitation
SiRA is taking extra serving overhead for serving
with extra parameters on experts and the gating,
compared to LoRA or Adamix. How to minimize
the serving overhead is a challenge problem which
we hope to address our future works.
Acknowledgements
We would like to acknowledge Abhanshu Sharma,
Hassan Mansoor, Qifei Wang, Victor C ˘arbune etc
for their valuable inputs.
References
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau,
and Doina Precup. 2015. Conditional computation
in neural networks for faster models. arXiv preprint
arXiv:1511.06297 .Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and
Shangsong Liang. 2022. Revisiting parameter-
efficient tuning: Are we really there yet? arXiv
preprint arXiv:2202.07962 .
Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei
Liu, and Zhangyang Wang. 2023a. Sparse moe as
the new dropout: Scaling dense and self-slimmable
transformers. arXiv preprint arXiv:2303.01610 .
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena
Shah, Iana Borova, Dylan Langdon, Reema Moussa,
Matt Beane, Ting-Hao Huang, Bryan Routledge,
and William Yang Wang. 2021. FinQA: A dataset of
numerical reasoning over financial data. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 3697–3711,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.
Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang
Chen, Hengshuang Zhao, Erik G. Learned-Miller,
and Chuang Gan. 2023b. Mod-squad: Designing
mixtures of experts as modular multi-task learners.
InIEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2023, Vancouver, BC,
Canada, June 17-24, 2023 , pages 11828–11837.
IEEE.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robin-
son, Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Yu, Vincent Zhao, Yanping Huang, Andrew
Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,
Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V .
Le, and Jason Wei. 2022. Scaling instruction-
finetuned language models.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman,
and Luke Zettlemoyer. 2023. Qlora: Efficient
finetuning of quantized llms. arXiv preprint
arXiv:2305.14314 .
Maha Elbayad, Anna Sun, and Shruti Bhosale. 2022.
Fixing moe over-fitting on low-resource languages
in multilingual machine translation. arXiv preprint
arXiv:2212.07571 .
William Fedus, Barret Zoph, and Noam Shazeer. 2022.
Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. The Jour-
nal of Machine Learning Research , 23(1):5232–
5270.
Yingbo Gao, Christian Herold, Zijian Yang, and Her-
mann Ney. 2022. Revisiting checkpoint averag-
ing for neural machine translation. arXiv preprint
arXiv:2210.11803 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea

--- PAGE 6 ---
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efficient transfer learning for nlp.
InInternational Conference on Machine Learning ,
pages 2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Misha Khalman, Yao Zhao, and Mohammad Saleh.
2021. ForumSum: A multi-speaker conversation
summarization dataset. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2021 ,
pages 4592–4599, Punta Cana, Dominican Repub-
lic. Association for Computational Linguistics.
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
Maxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-
ong, and Orhan Firat. 2021. Beyond distillation:
Task-level mixture-of-experts for efficient inference.
arXiv preprint arXiv:2110.03742 .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
Gshard: Scaling giant models with conditional com-
putation and automatic sharding. arXiv preprint
arXiv:2006.16668 .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman
Goyal, and Luke Zettlemoyer. 2021. Base layers:
Simplifying training of large, sparse models. In In-
ternational Conference on Machine Learning , pages
6265–6274. PMLR.
Xiang Lisa Li and Percy Liang. 2021. Prefix-
tuning: Optimizing continuous prompts for gener-
ation. arXiv preprint arXiv:2101.00190 .
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay
Mohta, Tenghao Huang, Mohit Bansal, and Colin A
Raffel. 2022. Few-shot parameter-efficient fine-
tuning is better and cheaper than in-context learn-
ing. Advances in Neural Information Processing
Systems , 35:1950–1965.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,
Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-
tuning v2: Prompt tuning can be comparable to fine-
tuning universally across scales and tasks. arXiv
preprint arXiv:2110.07602 .
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt
understands, too. AI Open .
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie
Zhou, and Yue Zhang. 2023. An empirical study
of catastrophic forgetting in large language mod-
els during continual fine-tuning. arXiv preprint
arXiv:2308.08747 .Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Ma-
dian Khabsa. 2021. Unipelt: A unified frame-
work for parameter-efficient language model tuning.
arXiv preprint arXiv:2110.07577 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural In-
formation Processing Systems , 35:27730–27744.
Alex Passos, Andrew Dai, Bryan Richter, Christopher
Choquette, Daniel Sohn, David So, Dmitry (Dima)
Lepikhin, Emanuel Taropa, Eric Ni, Erica Mor-
eira, Gaurav Mishra, Jiahui Yu, Jon Clark, Kathy
Meier-Hellstern, Kevin Robinson, Kiran V odrahalli,
Mark Omernick, Maxim Krikun, Maysam Mous-
salem, Melvin Johnson, Nan Du, Orhan Firat, Paige
Bailey, Rohan Anil, Sebastian Ruder, Siamak Shak-
eri, Siyuan Qiao, Slav Petrov, Xavier Garcia, Yan-
ping Huang, Yi Tay, Yong Cheng, Yonghui Wu,
Yuanzhong Xu, Yujing Zhang, and Zack Nado.
2023. Palm 2 technical report. Technical report,
Google Research.
Edoardo Maria Ponti, Alessandro Sordoni, Yoshua
Bengio, and Siva Reddy. 2023. Combining
parameter-efficient modules for task-level generali-
sation. In Proceedings of the 17th Conference of the
European Chapter of the Association for Computa-
tional Linguistics , pages 687–702.
Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and
Neil Houlsby. 2023. From sparse to soft mixtures of
experts. arXiv preprint arXiv:2308.00951 .
Stephen Roller, Sainbayar Sukhbaatar, Jason Weston,
et al. 2021. Hash layers for large sparse models. Ad-
vances in Neural Information Processing Systems ,
34:17555–17566.
Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin,
Mihir Kale, Min Ma, Massimo Nicosia, Shruti Ri-
jhwani, Parker Riley, Jean-Michel A. Sarr, Xinyi
Wang, John Wieting, Nitish Gupta, Anna Katanova,
Christo Kirov, Dana L. Dickinson, Brian Roark,
Bidisha Samanta, Connie Tao, David I. Adelani,
Vera Axelrod, Isaac Caswell, Colin Cherry, Dan
Garrette, Reeve Ingle, Melvin Johnson, Dmitry Pan-
teleev, and Partha Talukdar. 2023. Xtreme-up:
A user-centric scarce-data benchmark for under-
represented languages.
Thibault Sellam, Dipanjan Das, and Ankur P Parikh.
2020. Bleurt: Learning robust metrics for text gen-
eration. In Proceedings of ACL .
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538 .

--- PAGE 7 ---
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
InInternational Conference on Machine Learning ,
pages 4596–4604. PMLR.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
et al. 2022. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, et al. 2023. Llama 2: Open foun-
dation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee,
Xiaodong Liu, Jing Gao, Ahmed Hassan Awadal-
lah, and Jianfeng Gao. 2022. Adamix: Mixture-
of-adaptations for parameter-efficient model tuning.
arXiv preprint arXiv:2210.17451 .
Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza
Ermi¸ s, Acyr Locatelli, and Sara Hooker. 2023.
Pushing mixture of experts to the limit: Extremely
parameter efficient moe for instruction tuning. arXiv
preprint arXiv:2309.05444 .
Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. 2021. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-
models. arXiv preprint arXiv:2106.10199 .
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
Yu Qiao. 2023. Llama-adapter: Efficient fine-tuning
of language models with zero-init attention. arXiv
preprint arXiv:2303.16199 .
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping
Huang, Vincent Zhao, Andrew M Dai, Quoc V Le,
James Laudon, et al. 2022. Mixture-of-experts with
expert choice routing. Advances in Neural Informa-
tion Processing Systems , 35:7103–7114.
8 Appendix
8.1 Effect of LoRA rank
We investigate the effect of LoRA rank in Fig-
ure 2.
8.2 Training and Model selection
During supervised finetuning, SFT, we use 8Ten-
sor Processing Units (TPU) V3 chips for fine-
tuning. The batch size is 64, and the maximum
training step is 30000 . We use the Adafactor op-
timizer (Shazeer and Stern, 2018) with a learn-
ing rate of 0.0005 . Both the input and output
sequence lengths are set to match the dataset re-
quirements. The training dropout rate is 0.05. The
Trainable Parameters (%)RougeL
33343536
0.25 0.50 0.75 1.00 1.25SiRA MoLoRA Adamix LoRAFigure 2: SiRA vs LoRA on ForumSum Task. We in-
crease the rank of LoRA (rank=4, 8, 16, 32, 64, 128)
and report the RougeL as a metrics. Notably increas-
ing the rank does does not help the performance. SiRA
(rank=4) can achieve higher quality by leveraging the
sparse mixture of experts.
expert dropout rate is set to 0.5. We did hyper-
parameters search to find the best model configu-
rations. We decode on the validation sets of each
task every 100steps. And we report test results
from the best checkpoints according to the valida-
tion scores. For multitask results, the checkpoint
is picked on the average each tasks metrics. For
the reported numbers in section 4.2, we use topk
K= 4 as default. Yet we found K= 8 is bet-
ter for BN multitask and QA (in-lang) multilingual
setting, and K= 12 better for QA (cross-lang) ex-
periments.
