# LLaMA-Reviewer: Nâng cao Tự động hóa Đánh giá Mã nguồn
với Mô hình Ngôn ngữ Lớn thông qua
Tinh chỉnh Hiệu quả Tham số

Junyi Lu†‡, Lei Yu†‡, Xiaojia Li§, Li Yang∗†, Chun Zuo¶
†Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
‡Đại học Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
§Khoa Phần mềm, Đại học Thanh Hoa, Bắc Kinh, Trung Quốc¶Công ty TNHH Sinosoft, Bắc Kinh, Trung Quốc
{lujunyi21, yulei21 }@mails.ucas.ac.cn, lixj21@mails.tsinghua.edu.cn,
yangli2017@iscas.ac.cn, zuochun@sinosoft.com.cn

Tóm tắt — Việc tự động hóa các hoạt động đánh giá mã nguồn, một mục tiêu lâu dài trong kỹ thuật phần mềm, đã được giải quyết chủ yếu bằng nhiều mô hình được tiền huấn luyện đặc thù cho từng lĩnh vực. Mặc dù thành công, những mô hình này thường đòi hỏi tài nguyên rộng lớn để tiền huấn luyện từ đầu. Ngược lại, các Mô hình Ngôn ngữ Lớn (LLM) cung cấp một lựa chọn thú vị, xét đến khả năng đáng chú ý của chúng khi được bổ sung kiến thức đặc thù cho từng lĩnh vực. Tuy nhiên, tiềm năng của chúng cho việc tự động hóa các tác vụ đánh giá mã nguồn vẫn chưa được khám phá nhiều.

Để đáp ứng khoảng trống nghiên cứu này, chúng tôi trình bày LLaMA-Reviewer, một khung công tác sáng tạo tận dụng khả năng của LLaMA, một LLM phổ biến, trong lĩnh vực đánh giá mã nguồn. Lưu ý đến các ràng buộc tài nguyên, khung công tác này sử dụng các phương pháp tinh chỉnh hiệu quả tham số (PEFT), mang lại hiệu suất cao trong khi sử dụng ít hơn 1% tham số có thể huấn luyện.

Một đánh giá toàn diện về LLaMA-Reviewer được thực hiện trên hai bộ dữ liệu đa dạng, có sẵn công khai. Đáng chú ý, ngay cả với mô hình cơ sở LLaMA nhỏ nhất gồm 6.7B tham số và số lượng epoch tinh chỉnh hạn chế, LLaMA-Reviewer vẫn bằng hiệu suất của các mô hình tập trung vào đánh giá mã nguồn hiện có.

Các thí nghiệm loại bỏ cung cấp hiểu biết về ảnh hưởng của các thành phần khác nhau trong quy trình tinh chỉnh, bao gồm biểu diễn đầu vào, tinh chỉnh hướng dẫn và các phương pháp PEFT khác nhau. Để thúc đẩy tiến bộ liên tục trong lĩnh vực này, mã nguồn và tất cả các plugin trọng số PEFT đã được công khai mã nguồn mở.

Từ khóa chỉ mục — Tự động hóa Đánh giá Mã nguồn, Mô hình Ngôn ngữ Lớn (LLM), Tinh chỉnh Hiệu quả Tham số (PEFT), Học sâu, LLaMA, Đảm bảo Chất lượng Phần mềm

I. GIỚI THIỆU

Kể từ khi được chính thức hóa bởi Fagan vào năm 1976 [1], đánh giá mã nguồn đã là nền tảng của kỹ thuật phần mềm, đóng vai trò quan trọng trong việc xác định lỗi, cải thiện chất lượng và chia sẻ kiến thức [2]. Tuy nhiên, quy trình chủ yếu thủ công này gây ra gánh nặng công việc đáng kể cho các nhà phát triển. Ngay cả với các thực hành đánh giá mã nguồn hiện đại (MCR), vốn được sắp xếp hợp lý hơn so với các thực hành truyền thống, nỗ lực cần thiết vẫn rất lớn [3]–[5].

Để giảm bớt gánh nặng này, một làn sóng nghiên cứu đã tập trung vào việc tự động hóa quy trình đánh giá mã nguồn. Điều này bao gồm các tác vụ như đề xuất người đánh giá [6]–[15], đánh giá chất lượng mã nguồn [12], [16]–[21], tinh chỉnh mã nguồn có vấn đề [20], [22]–[25], và tạo ra các bình luận đánh giá tiềm năng [20], [23], [26]–[31].

Những tiến bộ gần đây trong xử lý ngôn ngữ tự nhiên (NLP) đã tiếp tục cho phép sử dụng các mô hình ngôn ngữ được tiền huấn luyện (PLM) cho những tác vụ này [20], [23]. Tuy nhiên, những mô hình đặc thù cho từng lĩnh vực như vậy thường đòi hỏi tài nguyên đáng kể để tiền huấn luyện từ đầu.

Ngược lại, các mô hình ngôn ngữ lớn thống nhất (LLM) thể hiện hiệu suất đáng chú ý khi được mở rộng đến một kích thước tham số nhất định [12], [13]. Chúng có thể xử lý hiệu quả các tác vụ cụ thể mà không cần tiền huấn luyện đặc thù cho từng lĩnh vực, mở ra một hướng đi đầy hứa hẹn cho việc tự động hóa đánh giá mã nguồn.

Trong nghiên cứu này, chúng tôi trình bày LLaMA-Reviewer, một khung công tác mới tận dụng LLaMA, một LLM chính thống, để tự động hóa đánh giá mã nguồn. Chúng tôi kết hợp các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) để giải quyết thách thức tính toán của việc tinh chỉnh LLM. Cách tiếp cận của chúng tôi xây dựng dựa trên pipeline được đề xuất bởi Li et al. [20], bao gồm 1) dự đoán tính cần thiết của đánh giá, 2) tạo bình luận đánh giá, và 3) các tác vụ tinh chỉnh mã nguồn.

Chúng tôi đánh giá toàn diện LLaMA-Reviewer trên hai bộ dữ liệu công khai cho mỗi tác vụ phụ và điều tra tác động của biểu diễn đầu vào, tinh chỉnh hướng dẫn và các phương pháp PEFT khác nhau. Những đóng góp chính của công trình này bao gồm:

• Giới thiệu việc áp dụng LLM cho các tác vụ tự động hóa đánh giá mã nguồn, cung cấp một lựa chọn ngoại tuyến và bảo vệ quyền riêng tư thay thế cho các giải pháp mã nguồn đóng như OpenAI APIs.

• Đề xuất mô hình "mô hình thống nhất + PEFT" để giảm nhu cầu tính toán trong các tác vụ đánh giá mã nguồn, với mô hình plug-in là một phần của nó để tối ưu hóa yêu cầu không gian lưu trữ, đầu tiên trong lĩnh vực kỹ thuật phần mềm.

• Thực hiện đánh giá toàn diện về hai phương pháp PEFT và các nghiên cứu loại bỏ về các thành phần tinh chỉnh.

• Công khai mã nguồn, mô hình và kết quả của chúng tôi [32].

Đây là cấu trúc của bài báo: Phần II cung cấp nền tảng cần thiết; Phần III chi tiết cách tiếp cận được đề xuất của chúng tôi; Phần IV mô tả thiết kế thí nghiệm; Phần V thảo luận kết quả đánh giá; Phần VI xem xét công trình liên quan; Phần VII xác định các mối đe dọa tính hợp lệ tiềm năng; Phần VIII kết luận các phát hiện của chúng tôi và đề xuất hướng nghiên cứu tương lai.

II. NỀN TẢNG

Phần này cung cấp một giới thiệu ngắn gọn về ba khái niệm chính làm nền tảng cho nghiên cứu của chúng tôi: pipeline đánh giá mã nguồn tự động, các mô hình ngôn ngữ lớn (LLM), và các phương pháp tinh chỉnh hiệu quả tham số (PEFT).

A. Tự động hóa trong Đánh giá Mã nguồn

Đánh giá Mã nguồn Hiện đại (MCR), một kỹ thuật được áp dụng rộng rãi bởi cả các doanh nghiệp lớn và các dự án mã nguồn mở, có một chu kỳ cốt lõi tương đối nhất quán mặc dù có các triển khai khác nhau. Chu kỳ này, từ việc tạo một pull request đến việc hợp nhất cuối cùng vào nhánh chính hoặc từ chối, bao gồm hai người tham gia chính: những người commit (Pc) và những người đánh giá (Pr). Chu kỳ bao gồm ba bước chính (như được hiển thị trong Hình 1): dự đoán tính cần thiết của đánh giá (Pr), bình luận về mã nguồn (Pr), và tinh chỉnh mã nguồn (Pc). Mục tiêu của việc tự động hóa quy trình đánh giá mã nguồn là giảm bớt khối lượng công việc cho cả hai bên.

Ba bước này được chuyển đổi thành ba tác vụ tự động hóa: 1) Dự đoán Tính cần thiết Đánh giá, dự đoán liệu một diff mã nguồn có cần bình luận đánh giá hay không; 2) Tạo Bình luận Đánh giá, tự động tạo bình luận đánh giá cho một diff mã nguồn được cho; và 3) Tinh chỉnh Mã nguồn, tinh chỉnh mã nguồn dựa trên các đoạn mã trước đó và bình luận đánh giá. Nghiên cứu của chúng tôi tập trung vào những tác vụ này, nhằm tự động hóa hoàn toàn quy trình đánh giá mã nguồn.

B. Mô hình Ngôn ngữ Lớn

Sự phát triển của mô hình hóa ngôn ngữ (LM) đã trải qua bốn giai đoạn quan trọng: mô hình ngôn ngữ thống kê (SLM), mô hình ngôn ngữ thần kinh (NLM), mô hình ngôn ngữ được tiền huấn luyện (PLM), và phát triển mới nhất, mô hình ngôn ngữ lớn (LLM) [33]. PLM, được tiền huấn luyện rõ ràng cho những tác vụ nhất định, đã đạt được thành công đáng chú ý trong nhiều tác vụ kỹ thuật phần mềm phụ trợ khác nhau. Điều này được thể hiện bởi các mô hình như CodeT5 [34] và PLBART [35]. Tuy nhiên, tiềm năng của LLM trong những bối cảnh này vẫn chưa được khám phá đầy đủ.

Sự khác biệt chính giữa PLM và LLM là quy mô tham số và kích thước dữ liệu của chúng. LLM là những mô hình có ~10B tham số hoặc nhiều hơn, được tiền huấn luyện với dữ liệu rộng lớn [33]. Nghiên cứu hiện tại cho thấy việc mở rộng những chiều này cải thiện hiệu suất mô hình và tạo ra các khả năng nổi bật [36]. Đáng chú ý, LLM có thể đạt hiệu suất ngang với PLM mà không cần thiết tiền huấn luyện đặc thù cho từng tác vụ, do đó giảm bớt nhu cầu tốn nhiều tài nguyên của tiền huấn luyện.

Để cụ thể hơn, các LLM hiện đang thịnh hành có thể được phân loại thành LLM thống nhất và LLM mã nguồn. Những mô hình trước chủ yếu được tiền huấn luyện trên các corpus ngôn ngữ tự nhiên, được làm giàu với một phần nhỏ hơn của mã nguồn, và đã được xác nhận là hiệu quả trong nhiều tác vụ khác nhau [37], [38]. Những mô hình sau chủ yếu được tiền huấn luyện trên corpus dựa trên mã nguồn và chúng đã đạt được kết quả ấn tượng trong tạo mã nguồn [39]–[42].

Trong nghiên cứu này, chúng tôi sử dụng LLaMA, một LLM thống nhất mã nguồn mở được phát triển bởi Meta. Lựa chọn này xuất phát từ bốn góc độ: 1) Các mô hình hiệu suất hàng đầu (GPT-3.5/GPT-4) cho các tác vụ mã nguồn là mô hình thống nhất, không phải LLM mã nguồn; 2) Xu hướng tăng cường hướng tới các mô hình thống nhất, được minh họa bởi sự chuyển đổi của OpenAI từ Codex sang GPT-3.5 để sử dụng API; 3) Xu hướng tăng cường hướng tới các mô hình thống nhất, được minh họa bởi sự chuyển đổi của OpenAI từ Codex sang GPT-3.5 để sử dụng API; 4) LLM mã nguồn chủ yếu xuất sắc trong các tác vụ tạo mã nguồn, trong khi các tác vụ đánh giá mã nguồn đặt ra những thách thức khác.

C. Tinh chỉnh Hiệu quả Tham số

Mặc dù hiệu quả, tài nguyên tính toán cao cần thiết để tinh chỉnh các mô hình ngôn ngữ lớn (LLM) tạo ra một thách thức đáng kể. Nhiều chiến lược đã được phát triển để tăng hiệu quả của quy trình tinh chỉnh và giảm chi phí huấn luyện. Chúng bao gồm tinh chỉnh adapter [43], [44], tinh chỉnh prefix [45], tinh chỉnh prompt [46], [47], và thích ứng hạng thấp (LoRA) [48]. Những phương pháp này đóng băng các tham số của mô hình cơ sở trong khi huấn luyện một số tham số bổ sung, đạt hiệu suất tương đương với tinh chỉnh tham số đầy đủ.

Trong nghiên cứu này, chúng tôi sử dụng hai phương pháp PEFT—zero-init attention prefix-tuning [49] và LoRA tuning [48]—để tinh chỉnh LLaMA. Những phương pháp này không giới thiệu độ trễ bổ sung cho mô hình và đã chứng minh hiệu quả trong các tác vụ ngôn ngữ tự nhiên. Các chi tiết cụ thể của phương pháp PEFT được elaborated thêm trong phần tiếp theo về các cách tiếp cận được đề xuất của chúng tôi.

III. LLAMA-REVIEWER: CÁCH TIẾP CẬN ĐỀ XUẤT

A. Tổng quan

Khung công tác của chúng tôi, được minh họa trong Hình 2, sử dụng quy trình tinh chỉnh hai giai đoạn. Chúng tôi bắt đầu với tinh chỉnh theo hướng dẫn trên LLaMA sử dụng dữ liệu lĩnh vực tập trung vào mã nguồn, nâng cao thành thạo của mô hình trong việc hiểu các tác vụ đánh giá mã nguồn và tuân theo hướng dẫn tác vụ. Sau đó chúng tôi thực hiện tinh chỉnh có giám sát cho mỗi tác vụ phụ trong quy trình đánh giá mã nguồn sử dụng LLaMA được nâng cao. Để cân bằng hiệu quả tham số và hiệu suất mô hình, chúng tôi kết hợp hai kỹ thuật chính—zero-init attention prefix-tuning và low-rank adaptation (LoRA) tuning—vì chúng đã đạt được sự chấp nhận rộng rãi và kết quả đầy hứa hẹn, đặc biệt với LLaMA [49], [50]. Những phương pháp này, được thành lập trên chiến lược đóng gói plugin của PEFT, cung cấp cho chúng tôi các plugin đặc thù cho từng tác vụ nhẹ. Những plugin này, độc lập với trọng số của mô hình cơ sở, có thể được tích hợp liền mạch trong quá trình suy luận.

Chúng tôi đánh giá hiệu quả của cách tiếp cận của chúng tôi sử dụng hai bộ dữ liệu riêng biệt. Để rõ ràng, chúng tôi sẽ gọi bộ dữ liệu trong CodeReviewer [20] là "bộ dữ liệu CRer", và bộ dữ liệu từ Tufano et al. [23] là "bộ dữ liệu Tuf.". Các chi tiết thêm về quy trình đánh giá có thể được tìm thấy trong các phần tiếp theo.

B. Tinh chỉnh Hướng dẫn trên LLaMA

Nghiên cứu chỉ ra rằng khi LLM được tinh chỉnh trên một loạt các bộ dữ liệu đa tác vụ đa dạng sử dụng mô tả ngôn ngữ tự nhiên, chúng thể hiện hiệu suất nâng cao trên các tác vụ chưa thấy [51], [52]. Tinh chỉnh theo hướng dẫn giúp mô hình diễn giải ý định người dùng tốt hơn và tuân theo hướng dẫn. Để ban đầu thích ứng mô hình được tiền huấn luyện cho các tác vụ đánh giá mã nguồn, chúng tôi sử dụng tinh chỉnh hướng dẫn trên một lĩnh vực tập trung vào mã nguồn.

Chúng tôi tận dụng quy trình chính và mẫu từ Stanford Alpaca [53], sửa đổi tinh chỉnh tham số đầy đủ để sử dụng các phương pháp PEFT như được giải thích trong Phần III-C và Phần III-D. Cho việc liên quan sâu sắc của tác vụ đánh giá mã nguồn với việc viết mã, chúng tôi thay thế dữ liệu gốc bằng tương đương lĩnh vực mã nguồn của nó, Code Alpaca [54]. Việc kết hợp dữ liệu từ Alpaca và Code Alpaca đã được xem xét nhưng không dẫn đến cải thiện hiệu suất, như được thảo luận thêm trong phần thí nghiệm loại bỏ. Cấu trúc dữ liệu tuân theo định dạng {instruction, input (tùy chọn), output}, theo khung công tác từ [55]. Cùng một mẫu prompt được sử dụng cho các tác vụ phụ tiếp theo để tối đa hóa việc sử dụng mô hình được tinh chỉnh giai đoạn đầu. Hình 3 minh họa mẫu prompt và định dạng hướng dẫn, đầu vào và đầu ra của tác vụ phụ.

C. Zero-init Attention Prefix-tuning

Zero-init attention prefix-tuning, một nhánh của prefix-tuning, giữ nguyên trọng số của mô hình cơ sở trong khi tích hợp thêm K token prefix vào các lớp L hàng đầu của transformer LLaMA. Những prompt linh hoạt này được nối với các token gốc, cho phép tính toán attention đa đầu sử dụng các key và value mới được giới thiệu. Trong quá trình tinh chỉnh, chỉ những prompt thích ứng này được huấn luyện.

Phương pháp này khác với prefix-tuning thông thường bằng cách giới thiệu một yếu tố gating có thể học được trong quá trình tính toán attention. Yếu tố này điều chỉnh mức độ liên quan của các token prompt được chèn. Để tạo điều kiện cho quy trình tinh chỉnh, yếu tố gating này ban đầu được đặt về zero, dẫn đến 'zero-init' trong tên của phương pháp. Yếu tố này đặc biệt kiểm soát attention được chia sẻ giữa các token prompt và các token gốc.

Hình 4 minh họa các chi tiết. Chúng tôi lấy lớp l, một trong các lớp L hàng đầu làm ví dụ. Ở đây, Pl∈RK×C đại diện cho K token prompt và Tl∈RM×C biểu thị M token gốc trong lớp l. Chiều đặc trưng được ký hiệu bởi C. Khi token thứ (M+1) tM+1 được tính toán thông qua cơ chế attention, các query, key và value được thu được thông qua một số lớp tuyến tính như sau:

Ql = Linearq(tM+1)
Kl = Lineark([Pl;Tl;tM+1])
Vl = Linearv([Pl;Tl;tM+1])                                                    (1)

Tiếp theo, các điểm attention được suy ra:

Sl = SK l;SM+1 lT
SK l = QlKK lT/√C ∈ R1×K
SM+1 l = QlKM+1 lT/√C ∈ R1×(M+1)                                          (2)

Yếu tố gating được tích hợp sau khi áp dụng softmax:

Sg l = [Softmax(SK l)·gl; Softmax(SM+1 l)T]                                    (3)

Cuối cùng, đầu ra của token tM+1 được tạo ra thông qua một lớp chiếu tuyến tính, và nó được kiểm soát bởi các prefix:

to M+1 = Linearo(Sg lVl) ∈ R1×C.                                              (4)

D. Low-Rank Adaptation

Low-Rank Adaptation (LoRA) cung cấp một góc nhìn khác về Tinh chỉnh Hiệu quả Tham số (PEFT). Không giống như các phương pháp tinh chỉnh tham số đầy đủ đòi hỏi tất cả trọng số phải được cập nhật trong giai đoạn tinh chỉnh, LoRA giữ lại trọng số của mô hình gốc và tích hợp các ma trận hạng thấp có thể huấn luyện vào các lớp transformer để mô phỏng các điều chỉnh trọng số. Xấp xỉ này dựa trên nguyên lý rằng quy trình thích ứng vốn có "hạng nội tại" thấp.

Hình 5 minh họa thành phần cốt lõi của LoRA. Giả sử W0∈Rd×k đại diện cho ma trận được tiền huấn luyện. Xấp xỉ của việc điều chỉnh trọng số từ W0 đến W0+∆W sử dụng LoRA có thể được biểu diễn như:

W0 + ∆W = W0 + WdownWup                                                    (5)

Ở đây, Wdown ∈ Rd×r và Wup ∈ Rr×k, với r ≪ min(d,k) là hạng. Trong quá trình tinh chỉnh, W0 vẫn không thay đổi, trong khi Wdown và Wup trở thành các tham số có thể huấn luyện. Cho một đầu vào x và đầu ra gốc liên quan h, đầu ra đã điều chỉnh h̄ được tính toán như:

h̄ = W0x + ∆Wx = h + WdownWupx                                            (6)

E. Các Tác vụ Tự động hóa Đánh giá Mã nguồn

LLaMA-Reviewer được thiết kế đặc biệt để tự động hóa ba tác vụ cốt lõi tích hợp vào quy trình đánh giá mã nguồn, cụ thể là dự đoán tính cần thiết đánh giá, tạo bình luận đánh giá mã nguồn, và tinh chỉnh mã nguồn. Những tác vụ này tuần tự tương ứng với các giai đoạn trong một quy trình đánh giá mã nguồn điển hình. Đầu vào và đầu ra cho những tác vụ này được phân loại thành ba định dạng:

• Ngôn ngữ Lập trình (PL) cho các đoạn mã nguồn,
• Ngôn ngữ Tự nhiên (NL) cho các bình luận mã nguồn, và
• Nhãn Nhị phân (L) cho các quyết định về yêu cầu đánh giá thêm. Điều này đơn giản hóa quy trình quyết định thành "có" (cần đánh giá) hoặc "không" (không cần đánh giá).

Bảng I minh họa mỗi tác vụ cùng với định dạng đầu vào và đầu ra của nó, và các tham chiếu bộ dữ liệu tương ứng (Crer. cho bộ dữ liệu CRer, và Tuf. cho bộ dữ liệu Tufano). Cách tiếp cận xây dựng prompt được hình dung trong Hình 3.

1) Dự đoán Tính cần thiết Đánh giá: Tác vụ này bao gồm việc kiểm tra liệu các diff hunk có cần đánh giá hay không. Được đề xuất bởi Li et al. [20], một diff hunk đại diện cho một đoạn mã ngắn gọn cho thấy sự khác biệt giữa các đoạn mã cũ và mới. Mặc dù việc bao gồm bối cảnh phương thức bổ sung có thể có lợi, các dòng trong diff hunk gốc thường đủ dài để tạo ra thách thức khi được quản lý như đầu vào.

2) Tạo Bình luận Đánh giá Mã nguồn: Tác vụ này tạo ra các bình luận liên quan cho một đoạn mã được cho. Hai góc nhìn được xem xét: góc nhìn cấp độ dòng tập trung vào nội dung của từng dòng mã riêng lẻ (sử dụng bộ dữ liệu CRer), và góc nhìn cấp độ phương thức cung cấp cái nhìn toàn diện hơn về bối cảnh của mã (sử dụng bộ dữ liệu Tufano).

3) Tinh chỉnh Mã nguồn: Tinh chỉnh mã nguồn đòi hỏi việc thực hiện các điều chỉnh nhỏ hoặc sắp xếp lại mã hiện có để nâng cao chất lượng của nó. Cho bản chất của những sửa đổi nhỏ này, mã đầu vào và đầu ra thường có sự tương đồng mạnh mẽ. Đầu vào được định dạng theo bộ dữ liệu Tufano và CRer. Thông thường, chúng tôi bỏ qua thông tin loại ngôn ngữ vì các mô hình baseline đã làm như vậy và nó chỉ có sẵn trong một tác vụ của một bộ dữ liệu được công bố, để đảm bảo so sánh công bằng. Các ý nghĩa của thông tin như vậy được khám phá thêm trong phần đánh giá của chúng tôi.

IV. THIẾT KẾ THÍ NGHIỆM

Phần này chi tiết thiết kế thí nghiệm của chúng tôi, phác thảo các câu hỏi nghiên cứu cơ bản thúc đẩy cuộc điều tra của chúng tôi, các bộ dữ liệu được sử dụng, các tiêu chí đánh giá được sử dụng, và một tóm tắt kỹ lưỡng về các mô hình baseline và chi tiết triển khai cụ thể của chúng tôi.

A. Câu hỏi Nghiên cứu

Để đánh giá hiệu quả của khung công tác được đề xuất của chúng tôi, chúng tôi đặt ra các câu hỏi nghiên cứu sau:

(RQ1) Một mô hình ngôn ngữ lớn hiệu quả như thế nào trong việc tự động hóa các tác vụ đánh giá mã nguồn, so với các phương pháp tiên tiến nhất?

Động lực. Những tiến bộ nhanh chóng trong Nội dung Do AI Tạo ra (AIGC) và mối tương quan đã biết giữa khả năng mô hình và kích thước của chúng đã dẫn đến việc sử dụng rộng rãi các mô hình ngôn ngữ lớn được tiền huấn luyện và được tinh chỉnh (LLM). Mặc dù dữ liệu ngôn ngữ lập trình đóng vai trò quan trọng trong việc tăng cường khả năng của mô hình, việc áp dụng dữ liệu như vậy vào các tác vụ liên quan đến mã nguồn—đặc biệt là những tác vụ đòi hỏi thành thạo cả ngôn ngữ tự nhiên (NL) và ngôn ngữ lập trình (PL), như đánh giá mã nguồn tự động—vẫn chưa được khám phá nhiều.

Trong nghiên cứu này, chúng tôi sử dụng biến thể nhỏ nhất của LLaMA làm mô hình ngôn ngữ lớn cơ sở để đánh giá hiệu quả của nó trong việc tự động hóa các tác vụ đánh giá mã nguồn so với các phương pháp tiên tiến nhất, đặc biệt là các mô hình được tiền huấn luyện đặc thù cho từng tác vụ như CodeReviewer [20]. Hiệu suất của mô hình được xem xét kỹ lưỡng trên mỗi tác vụ để xác định điểm mạnh và các lĩnh vực cần nâng cao. Cụ thể, chúng tôi đặt ra các câu hỏi sau:

• (RQ1.1) Một mô hình ngôn ngữ lớn hiệu quả như thế nào trong việc kiểm tra tính cần thiết đánh giá (phân loại)?
• (RQ1.2) Một mô hình ngôn ngữ lớn thành thạo như thế nào trong việc tạo bình luận đánh giá mã nguồn (tạo NL)?
• (RQ1.3) Một mô hình ngôn ngữ lớn có khả năng như thế nào trong việc tinh chỉnh mã nguồn dựa trên bình luận (tạo PL)?

(RQ2) Biểu diễn dữ liệu đầu vào ảnh hưởng như thế nào đến hiệu suất của các mô hình ngôn ngữ lớn?

Động lực. Cho định dạng dữ liệu tiền huấn luyện cố định, có thể có sự không nhất quán giữa định dạng này và định dạng cần thiết cho các tác vụ cụ thể. Để đánh giá khả năng của mô hình ngôn ngữ lớn, chúng tôi tìm hiểu sâu về hai yếu tố quan trọng:

1) Định dạng Mã nguồn. Chúng tôi đánh giá hiệu suất của mô hình trên đầu vào mã thô và đầu vào với định dạng đã sửa đổi (bao gồm loại bỏ các khoảng trắng liên tiếp) để xác định cái nào có thể được xử lý hiệu quả hơn.

2) Nhãn Ngôn ngữ Lập trình. Với nhiều ngôn ngữ lập trình khác nhau, chúng tôi kiểm tra tác động của việc chỉ định loại ngôn ngữ trong đầu vào và tầm quan trọng của vị trí nhãn trong mẫu prompt.

Dưới ánh sáng của những cân nhắc này, chúng tôi đặt ra hai câu hỏi phụ:

• (RQ2.1) Định dạng mã nguồn ảnh hưởng như thế nào đến hiệu suất của mô hình?
• (RQ2.2) Việc bao gồm và vị trí của các nhãn ngôn ngữ lập trình ảnh hưởng như thế nào đến hiệu suất của mô hình?

(RQ3) Tinh chỉnh hướng dẫn ảnh hưởng như thế nào đến hiệu suất của các tác vụ phụ tiếp theo?

Động lực. Tinh chỉnh hướng dẫn, việc bao gồm các hướng dẫn liên quan đến mã nguồn trong giai đoạn ban đầu, nhằm truyền đạt kiến thức lĩnh vực và giúp mô hình hiểu các tác vụ phụ tốt hơn. Hiệu quả của cách tiếp cận này và khả năng tương thích của nó với nhiều phương pháp tinh chỉnh hiệu quả tham số (PEFT) khác nhau vẫn là một lĩnh vực khám phá phong phú. Ngoài ra, cho bản chất kép của các tác vụ đánh giá mã nguồn liên quan đến ngôn ngữ tự nhiên (NL) và ngôn ngữ lập trình (PL), một so sánh giữa việc sử dụng độc quyền các hướng dẫn liên quan đến PL và một hỗn hợp các hướng dẫn liên quan đến NL và PL là được bảo đảm. Do đó, chúng tôi hỏi:

• (RQ3.1) Tác động của giai đoạn tinh chỉnh hướng dẫn ban đầu lên zero-init attention prefix-tuning là gì?
• (RQ3.2) Giai đoạn tinh chỉnh hướng dẫn ban đầu ảnh hưởng như thế nào đến low-rank adaptation (LoRA)?
• (RQ3.3) Cách tiếp cận nào mang lại kết quả vượt trội trong các tác vụ đánh giá mã nguồn: sử dụng hỗn hợp các hướng dẫn liên quan đến NL và PL hay dựa vào độc quyền các hướng dẫn liên quan đến PL?

(RQ4) Những ý nghĩa nào phát sinh từ các phương pháp tinh chỉnh hiệu quả tham số (PEFT) khác nhau?

Động lực. Hai phương pháp PEFT được sử dụng để giảm bớt nhu cầu tài nguyên tính toán trong quá trình tinh chỉnh. Tuy nhiên, việc đạt được cân bằng tối ưu giữa hiệu quả và hiệu quả là thách thức. Tương ứng, chúng tôi dự định phân tích các kết quả thu được từ hai phương pháp này. Hơn nữa, trong bối cảnh của low-rank adaptation (LoRA), hạng r xác định số lượng tham số có thể huấn luyện; do đó, chúng tôi thực hiện một nghiên cứu loại bỏ để điều tra tác động của hạng r. Cuối cùng, chúng tôi so sánh hai phương pháp về số lượng tham số và yêu cầu không gian lưu trữ với các phương pháp trước đó. Với mục đích này, chúng tôi khám phá các câu hỏi phụ sau:

• (RQ4.1) Phương pháp PEFT nào hoạt động tốt hơn, và tại sao?
• (RQ4.2) Trong LoRA, hạng r ảnh hưởng như thế nào đến các tham số có thể huấn luyện và hiệu suất tổng thể?
• (RQ4.3) Hai phương pháp PEFT so sánh như thế nào với các cách tiếp cận trước đó về hiệu quả tham số và yêu cầu không gian lưu trữ?

B. Bộ dữ liệu

Chúng tôi sử dụng hai bộ dữ liệu đánh giá mã nguồn nổi bật: bộ dữ liệu từ CodeReviewer của Li et al. [20] (sau đây là bộ dữ liệu CRer), và bộ dữ liệu từ Tufano et al. [23] (sau đây là bộ dữ liệu Tufano). Lý do cho lựa chọn của chúng tôi bao gồm:

• Không giống như các bộ dữ liệu khác trong tài liệu [30], [56] che phủ các tác vụ phụ cụ thể, các bộ dữ liệu được chọn bao gồm toàn bộ quy trình đánh giá mã nguồn.
• Cả bộ dữ liệu CRer và Tufano đều được rút ra từ một loạt kho lưu trữ đa dạng, cung cấp phạm vi bao phủ rộng. Điều này tương phản với các bộ dữ liệu khác [30], [56] rút ra từ một nhóm kho lưu trữ hạn chế, có thể dẫn đến thiên vị do phạm vi hạn chế của chúng.
• Bộ dữ liệu Tufano là một phiên bản nâng cao so với những bộ được sử dụng trong các nghiên cứu trước đó [22], [25], [57], làm cho nó được ưa thích vì tính cập nhật và toàn diện.
• Cả hai bộ dữ liệu đều có tính bán kết trong lĩnh vực, mỗi bộ cung cấp các đặc điểm độc đáo góp phần vào nghiên cứu của chúng tôi.

Bộ dữ liệu CRer, một corpus đa ngôn ngữ, được rút ra từ các kho lưu trữ GitHub và tuân theo định dạng biết diff, cấp độ dòng. Nó bảo tồn các bình luận nội tuyến và docstring trong các đoạn mã và giữ lại các khoảng trắng liên tiếp. Bộ dữ liệu này được chia thành ba bộ dữ liệu phụ, mỗi bộ được dành riêng cho một khía cạnh cụ thể của đánh giá mã nguồn: dự đoán tính cần thiết đánh giá, tạo bình luận đánh giá mã nguồn, và tinh chỉnh mã nguồn.

Bộ dữ liệu Tufano, ngược lại, đặc thù cho ngôn ngữ (Java) và tập hợp dữ liệu từ cả GitHub và Gerrit. Nó sử dụng định dạng cấp độ hàm, loại bỏ bình luận và khoảng trắng liên tiếp, và không phản ánh sự khác biệt giữa commit liên quan và nhánh cơ sở. Đối với các tác vụ tinh chỉnh mã nguồn, nó biểu thị các khu vực tập trung trong các bình luận sử dụng các dấu hiệu "⟨START⟩" và "⟨END⟩". Chúng tôi sử dụng hai tập con của bộ dữ liệu này cho tạo bình luận đánh giá mã nguồn và tinh chỉnh mã nguồn.

Bảng II cung cấp một tóm tắt thống kê chi tiết về các bộ dữ liệu.

C. Tiêu chí Đánh giá

Chúng tôi sử dụng các tiêu chí cụ thể cho từng tác vụ để đo lường hiệu suất của mô hình của chúng tôi trên các tác vụ đánh giá mã nguồn.

Đối với dự đoán tính cần thiết đánh giá, chúng tôi tiếp cận nó như một vấn đề phân loại nhị phân trong đó 'yêu cầu một đánh giá' là lớp tích cực. Do đó, chúng tôi sử dụng precision, recall và F1-score làm tiêu chí đánh giá để định lượng độ chính xác phân loại của mô hình.

Đối với các tác vụ tạo bình luận đánh giá mã nguồn và tinh chỉnh mã nguồn, liên quan đến tạo phản hồi, chúng tôi sử dụng điểm BLEU-4, đo lường sự chồng lấp của n-gram cho n từ 1 đến 4. Điều này tuân theo phương pháp đánh giá được sử dụng trong CodeReviewer [20].

Chúng tôi không sử dụng tiêu chí codeBLEU được đề xuất bởi Tufano et al. [23], do tính không tương thích của nó với bộ dữ liệu CRer. Cấu trúc và sự đa dạng ngôn ngữ của bộ dữ liệu CRer làm cho nó không phù hợp với tiêu chí này.

Đối với tất cả các tác vụ, chúng tôi xem xét kết quả top-1, phù hợp với mục tiêu tự động hóa quy trình đánh giá mã nguồn để giảm nhẹ khối lượng công việc của nhà phát triển bằng cách tập trung vào phản hồi liên quan nhất.

D. Baseline

Chúng tôi chọn baseline của chúng tôi dựa trên nhu cầu độc đáo của các tác vụ và bộ dữ liệu. Bảng III hiển thị các baseline được chọn.

Chúng tôi bỏ qua các nghiên cứu của [25], [56], [57] khỏi lựa chọn baseline của chúng tôi, vì chúng được thiết kế cho các bộ dữ liệu nhỏ hơn hoặc yêu cầu thông tin đầu vào bổ sung.

E. Chi tiết Triển khai

Chúng tôi triển khai cách tiếp cận của chúng tôi sử dụng các khung xturing¹ và Lit LLaMA² tương ứng. Tất cả các thí nghiệm được thực hiện trên các nền tảng GPU NVIDIA A100-SXM4-80GB, với giới hạn độ dài token được đặt là 2048 và kích thước batch là 64. Chúng tôi sử dụng optimizer AdamW và huấn luyện các mô hình trong 5 epoch cho dự đoán tính cần thiết đánh giá và 10 epoch cho cả tác vụ tạo bình luận đánh giá mã nguồn và tinh chỉnh mã nguồn.

Đối với zero-init attention prefix-tuning, chúng tôi sử dụng tốc độ học 0.009, trọng số decay 0.02, độ dài prompt prefix 10, và lớp prefix 30. Trong Low-rank Adaptation (LoRA), chúng tôi đặt tốc độ học là 0.0003, trọng số decay là 0.01, hạng LoRA là 16, và yếu tố tỷ lệ LoRA là 16. Cài đặt thí nghiệm loại bỏ có thể thay đổi dựa trên yêu cầu của mỗi thí nghiệm. Hạng LoRA và cài đặt prefix-tuning dựa trên kinh nghiệm thực nghiệm [49], [50]. Các chi tiết thêm về các siêu tham số cụ thể có sẵn trong tài liệu của chúng tôi.

Triển khai baseline được điều chỉnh cho từng tình huống. Đối với kết quả bộ dữ liệu CRer, chúng tôi sử dụng các phát hiện được báo cáo trong bài báo CodeReviewer [20], vì chúng tôi không thực hiện sửa đổi nào đối với bộ dữ liệu. Chúng tôi tái tạo kết quả CommentFinder [31] trên cả hai bộ dữ liệu sử dụng mã có sẵn công khai của họ. Tất cả các kết quả baseline khác được rút ra từ các mô hình được cung cấp của họ.

V. ĐÁNH GIÁ

Trong phần này, chúng tôi lần lượt giải quyết từng câu hỏi nghiên cứu, trình bày các kết quả thu được từ thí nghiệm của chúng tôi và rút ra kết luận cho mỗi RQ. Thảo luận của chúng tôi bắt đầu với đánh giá hiệu suất của LLaMA-Reviewer, tiếp theo là khám phá ảnh hưởng của biểu diễn đầu vào và giai đoạn ban đầu của tinh chỉnh hướng dẫn. Chúng tôi kết thúc với phân tích các ý nghĩa phát sinh từ các phương pháp tinh chỉnh hiệu quả tham số (PEFT) khác nhau.

A. RQ1: Đánh giá Hiệu suất của LLaMA-Reviewer

Phần phụ này đánh giá hiệu suất của LLaMA-Reviewer trên mỗi tác vụ, giải thích các kết quả quan sát được, và tóm tắt các kết luận làm những phát hiện chính.

1) (RQ1.1) Hiệu suất Dự đoán Tính cần thiết Đánh giá:
Trong các thí nghiệm dự đoán tính cần thiết đánh giá của chúng tôi, chúng tôi tập trung độc quyền vào bộ dữ liệu CRer, vì đây là bộ dữ liệu duy nhất cung cấp dữ liệu cần thiết cho tác vụ này. Kết quả được trình bày trong Bảng IV, với hàng cuối cùng hiển thị kết quả cho LLaMA-Reviewer. Các hàng trước đó hiển thị kết quả rút ra từ [20]. Chúng tôi xem xét lớp yêu cầu đánh giá là tích cực. Quan trọng, kết quả cho LLaMA-Reviewer với prefix-tuning không được bao gồm trong tác vụ này do cấu trúc huấn luyện cứng nhắc của nó, không thuận lợi để thực hiện các tác vụ phân loại.

LLaMA-Reviewer đạt recall vượt trội với điểm F1 tương đương, như kết quả cho thấy, chỉ ra rằng nó có thể xác định một số lượng lớn hơn các đoạn mã có vấn đề có thể thúc đẩy thảo luận trong quy trình đánh giá mã nguồn tiếp theo. Khả năng này rất quan trọng đối với người đánh giá vì mục tiêu chính của đánh giá mã nguồn là khám phá càng nhiều vấn đề tiềm năng càng tốt. Trong tình huống này, LLaMA-Reviewer có thể giảm số lượng đoạn mã cần đánh giá sau khi lọc, mà không bỏ qua một số lượng đáng kể các đoạn mã có vấn đề.

Chúng tôi thu được những kết quả này thông qua điều chỉnh ngưỡng. Hơn nữa, với ngưỡng 0.5, giống hệt với cài đặt tạo gốc, LLaMA-Reviewer đạt precision 88.61%, vượt qua tất cả baseline. Hiệu suất này cũng có ý nghĩa trong các tình huống thực tế, nơi các đoạn mã có vấn đề ít phổ biến hơn những đoạn bình thường, chỉ ra rằng false positive có thể đặt gánh nặng bổ sung lên người đánh giá.

2) (RQ1.2) Hiệu suất Tạo Bình luận Đánh giá: Chúng tôi đánh giá tác vụ tạo bình luận đánh giá mã nguồn sử dụng cả bộ dữ liệu Tufano và CRer. Bảng V minh họa kết quả, với ký hiệu "−" biểu thị các giá trị bị thiếu. Đáng chú ý rằng CommentFinder [31] không bao gồm số lượng tham số vì nó không sử dụng phương pháp học sâu. Chúng tôi đã chọn không báo cáo một số kết quả baseline do sự không đồng nhất về granularity giữa các phương pháp được đề xuất và bộ dữ liệu, điều này làm cho kết quả vô nghĩa.

Kết quả chỉ ra rằng LLaMA-Reviewer vượt qua tất cả baseline trên bộ dữ liệu CRer, đặc biệt khi sử dụng Low-Rank Adaptation (LoRA) để tinh chỉnh. Hiệu suất vượt trội này làm nổi bật tiềm năng của các mô hình ngôn ngữ lớn (LLM). Mặc dù LLaMA không được tiền huấn luyện đặc biệt cho các tác vụ đánh giá mã nguồn như CodeReviewer [20], hiệu suất vượt trội của nó với một lượng tinh chỉnh hạn chế vẫn vượt qua các mô hình nhỏ hơn. Kết quả trên bộ dữ liệu Tufano tương đối kém lý tưởng hơn, mà chúng tôi sẽ thảo luận thêm trong RQ2.1.

Một giải thích hợp lý cho hiệu suất nâng cao này là sự phù hợp giữa tác vụ tạo ngôn ngữ tự nhiên và corpus tiền huấn luyện của LLaMA. Hơn nữa, hiệu suất ấn tượng trên bộ dữ liệu CRer có thể được quy cho việc sử dụng sự khác biệt mã nguồn và định dạng mã thô, phản ánh các điều kiện của giai đoạn tiền huấn luyện. Cho độ phức tạp của tác vụ tạo bình luận đánh giá mã nguồn so với các tác vụ khác [20], kích thước mô hình lớn hơn của LLaMA cung cấp một lợi thế rõ rệt.

3) (RQ1.3) Hiệu suất Tinh chỉnh Mã nguồn: Chúng tôi đánh giá tác vụ tinh chỉnh mã nguồn trên cả bộ dữ liệu Tufano và CRer. Kết quả được hiển thị trong Bảng VI, nơi ký hiệu "−" biểu thị các giá trị bị thiếu.

Trên cả hai bộ dữ liệu, mặc dù không vượt qua tất cả mô hình, LLaMA-Reviewer cạnh tranh sát sao với CodeReviewer [20] hoặc Tufano et al. [23], các mô hình được tiền huấn luyện đặc biệt cho đánh giá mã nguồn và định dạng dữ liệu tương ứng và vượt qua hiệu suất của các baseline khác. Xem xét rằng chúng tôi sử dụng phiên bản nhỏ nhất của LLaMA và số epoch tinh chỉnh hạn chế, kết quả này gợi ý những cải thiện tiềm năng.

Lợi thế của LLaMA-Reviewer so với hầu hết baseline chủ yếu phát sinh từ kích thước mô hình lớn và bản chất của dữ liệu tiền huấn luyện. Khoảng cách giữa LLaMA-Reviewer và CodeReviewer [20] hoặc Tufano et al. [23] là do sự khác biệt giữa các tác vụ mục tiêu của họ và các tác vụ tiền huấn luyện của LLaMA, cũng như các định dạng đầu vào. Tuy nhiên, tiền huấn luyện đặc thù cho từng tác vụ từ đầu, như với CodeReviewer [20], tốn nhiều tài nguyên, tạo ra rào cản cho việc nâng cao thông qua mở rộng kích thước mô hình. Thay vào đó, việc tích hợp kiến thức lĩnh vực vào một mô hình được tiền huấn luyện duy nhất và áp dụng các phương pháp tinh chỉnh hiệu quả tham số có thể hiệu quả về chi phí hơn.

Thú vị, sự đơn giản tương đối của tác vụ tinh chỉnh mã nguồn so với tác vụ tạo bình luận đánh giá có thể đã một cách nghịch lý làm giảm điểm BLEU của LLaMA-Reviewer. Điều này là do mô hình, được huấn luyện để tạo ra các dự đoán đa dạng mô phỏng hành vi con người, có thể tạo ra các tinh chỉnh đa dạng hơn, nhưng hợp lệ, khác biệt với sự thật duy nhất, do đó giảm sự tương đồng văn bản.

Trả lời cho RQ1: LLaMA-Reviewer tương đối xuất sắc hơn trong việc tạo bình luận đánh giá (NL) và xác định nhiều vấn đề hơn trong dự đoán tính cần thiết trong khi duy trì hiệu suất cạnh tranh trong tinh chỉnh mã nguồn.

B. RQ2: Ảnh hưởng của Biểu diễn Đầu vào

Trong phần phụ này, chúng tôi điều tra tác động của biểu diễn đầu vào sử dụng kết quả từ RQ1 và các thí nghiệm loại bỏ bổ sung. Chúng tôi giải quyết từng câu hỏi phụ lần lượt trước khi rút ra kết luận tổng thể.

1) (RQ2.1) Hậu quả của Định dạng Mã nguồn: Để đánh giá tác động của định dạng mã nguồn, chúng tôi kiểm tra kết quả của các tác vụ tạo bình luận mã nguồn và tinh chỉnh mã nguồn sử dụng cả bộ dữ liệu CRer và Tufano.

Kết quả được trình bày trong Phần V-A cho thấy hiệu suất tương đối vượt trội trên bộ dữ liệu CRer so với bộ dữ liệu Tufano. Mặc dù có sự khác biệt trong phân phối dữ liệu, sự khác biệt chính giữa hai bộ dữ liệu này nằm ở định dạng mã nguồn của chúng. Mã nguồn trong bộ dữ liệu CRer thô sơ hơn và tương tự với định dạng được sử dụng trong quá trình tiền huấn luyện LLaMA, trong khi mã nguồn trong bộ dữ liệu Tufano đã trải qua xử lý tinh vi. Những kết quả này gợi ý rằng một biểu diễn mã nguồn tương tự với cái được sử dụng trong quá trình tiền huấn luyện cho phép mô hình tận dụng tốt hơn sự hiểu biết của nó về cấu trúc và ngữ nghĩa mã nguồn.

2) (RQ2.2) Vai trò của Nhãn Ngôn ngữ: Chúng tôi thực hiện thí nghiệm để đánh giá tác động của nhãn ngôn ngữ độc quyền trên bộ dữ liệu CRer, vì nó bao gồm nhiều ngôn ngữ lập trình. Nhãn ngôn ngữ, được xác định dựa trên ngôn ngữ lập trình của mã nguồn, được tích hợp vào hướng dẫn hoặc đầu vào, như được hiển thị trong Hình 3. Các cài đặt còn lại, được mượn từ tác vụ tinh chỉnh mã nguồn với low-rank adaptation làm phương pháp tinh chỉnh, được giữ nguyên.

Trái với kỳ vọng, kết quả được trình bày trong Bảng VII tiết lộ rằng việc thêm nhãn ngôn ngữ không nâng cao hiệu suất mà không có giai đoạn ban đầu của tinh chỉnh hướng dẫn. Điều này có thể do khó khăn của mô hình trong việc liên kết thông tin nhãn với tác vụ mà không có kiến thức lĩnh vực sẵn có. Tuy nhiên, một khi tinh chỉnh hướng dẫn được triển khai, các nhãn thực sự đóng góp tích cực vào hiệu suất của mô hình. Thông qua một bài kiểm tra resampling bootstrap cặp, chúng tôi xác định rằng việc sử dụng nhãn ngôn ngữ cải thiện hiệu suất so với việc không có nó, được chứng minh bởi p-value là 0.0032.

Mặc dù nhãn ngôn ngữ đã chứng minh giá trị của chúng, chúng tôi chọn không bao gồm chúng trong các thí nghiệm khác để duy trì tính nhất quán với nghiên cứu trước đó [20] và để đảm bảo so sánh công bằng.

Trả lời cho RQ2: LLaMA-Reviewer hoạt động tốt hơn khi biểu diễn đầu vào giống với cái được sử dụng trong quá trình tiền huấn luyện. Thông tin ngôn ngữ tự nhiên bổ sung, như nhãn ngôn ngữ, có thể được tận dụng tốt hơn bởi mô hình thông qua tinh chỉnh hướng dẫn.

C. RQ3: Tác động của Tinh chỉnh Hướng dẫn

Để trả lời RQ3, chúng tôi thực hiện thí nghiệm với các mô hình được huấn luyện có và không có giai đoạn sơ bộ của tinh chỉnh hướng dẫn, sử dụng cả zero-init attention prefix-tuning và Low-Rank Adaptation (LoRA). Chúng tôi cũng giới thiệu các thí nghiệm bổ sung với các hướng dẫn ngôn ngữ tự nhiên bổ sung [53] trong quá trình tinh chỉnh hướng dẫn LoRA để xác định dữ liệu tinh chỉnh hướng dẫn tối ưu (như được đặt ra bởi RQ3.3). Các thí nghiệm được thực hiện trên bộ dữ liệu CRer sử dụng LoRA, và kết quả từ các tác vụ đánh giá mã nguồn được ghi lại trong Bảng VIII.

1) (RQ3.1) Hậu quả cho Zero-init Attention Prefix-tuning: Kết quả của chúng tôi gợi ý rằng tinh chỉnh hướng dẫn không thuận lợi cho prefix tuning. Điều này có thể được quy cho cấu trúc của prefix tuning, sử dụng prefix độc quyền để kiểm soát attention và giữ attention qua postfix cố định. Như vậy, khả năng nắm bắt kiến thức lĩnh vực tổng quát của nó bị hạn chế. Hơn nữa, zero-init prefix attention, đóng góp đáng kể vào hiệu quả của prefix tuning, bị làm suy yếu khi tinh chỉnh hướng dẫn được thêm vào.

2) (RQ3.2) Hậu quả cho Low-Rank Adaptation: Không giống như prefix tuning, tinh chỉnh hướng dẫn kết hợp với LoRA cải thiện hiệu suất trên hầu hết các tác vụ. Đối với dự đoán tính cần thiết đánh giá, nó nâng cao precision của dự đoán từ 81.56% đến 83.99% khi sử dụng bộ dữ liệu PL làm bộ tinh chỉnh duy nhất, mặc dù nó không nâng cao f1-score. Đối với tạo bình luận đánh giá, nó tăng điểm BLEU. Đối với tinh chỉnh mã nguồn, mặc dù không phát hiện cải thiện đáng kể, chúng tôi suy luận từ Phần V-B rằng nó tăng cường khả năng của mô hình để kết hợp thông tin nhãn. Tinh chỉnh hướng dẫn với LoRA giúp mô hình cơ sở hiểu ý định hướng dẫn, điều này, đến lượt nó, có lợi cho tinh chỉnh tác vụ tiếp theo, đặc biệt là tạo bình luận đánh giá, cho độ phức tạp và bản chất đa ý định của nó.

3) (RQ3.3) Ảnh hưởng của Các Loại Hướng dẫn: Chúng tôi tập trung vào các hàng "PL+NL" và "PL" sử dụng LoRA, biểu thị việc sử dụng cả dữ liệu Alpaca và Code Alpaca và chỉ dữ liệu Code Alpaca tương ứng. Thú vị, mặc dù các tác vụ đánh giá mã nguồn liên quan chặt chẽ đến ngôn ngữ tự nhiên, việc kết hợp dữ liệu Alpaca cho tinh chỉnh hướng dẫn làm giảm hiệu suất trên tất cả các tác vụ. Xu hướng này có thể liên kết với sự đa dạng rộng lớn của các hướng dẫn ngôn ngữ tự nhiên trong Alpaca. Các hướng dẫn rộng lớn trong bộ dữ liệu Alpaca có các động từ như rewrite và classify, và chúng dường như quá nhiều cho các tác vụ đánh giá mã nguồn do phạm vi rộng lớn của các tác vụ.

Trả lời cho RQ3: Tinh chỉnh hướng dẫn có thể tiềm ẩn nâng cao hiệu suất tác vụ hoặc khả năng xử lý thông tin ngôn ngữ tự nhiên bổ sung. Tuy nhiên, hiệu quả là nhỏ do sự không nhất quán thói quen từ giữa hướng dẫn và bộ dữ liệu downstream.

D. RQ4: Ảnh hưởng của Tinh chỉnh Hiệu quả Tham số

Để khám phá tác động của các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT), chúng tôi thực hiện các thí nghiệm bổ sung điều chỉnh hạng r của LoRA, đặc biệt đặt hạng là 8 và 16. Việc điều tra các siêu tham số cho prefix-tuning được bỏ qua vì nó đã được phân tích đầy đủ trong công trình trước đó [49]. Kết quả được chi tiết trong Bảng IX. Để phân tích so sánh, chúng tôi cũng bao gồm kết quả của prefix-tuning.

1) (RQ4.1) So sánh giữa các Phương pháp PEFT: Kết quả quyết định chỉ ra rằng LoRA vượt qua prefix-tuning trên tất cả các tác vụ. Chúng tôi quy hiệu suất nâng cao này cho hai khía cạnh chính. Đầu tiên, phương pháp prefix-tuning được triển khai trong nghiên cứu của chúng tôi có ít tham số có thể huấn luyện hơn LoRA, cản trở khả năng thích ứng từ mô hình cơ sở. Thứ hai, trái với prefix-tuning dựa vào prefix để kiểm soát mô hình cơ sở, LoRA xấp xỉ tinh chỉnh tham số đầy đủ, một thuộc tính quan trọng khi đầu ra mục tiêu khác biệt đáng kể so với định dạng tiền huấn luyện.

2) (RQ4.2) Tác động của Hạng LoRA r: Tăng hạng LoRA r từ 8 đến 16 cải thiện hiệu suất của LLaMA-Reviewer. Cải thiện này trực quan, vì hạng cao hơn tăng cường số lượng tham số có thể huấn luyện, đưa mô hình gần hơn đến tinh chỉnh tham số đầy đủ. Tuy nhiên, mục tiêu chính của việc sử dụng các phương pháp PEFT là hạn chế tham số có thể huấn luyện và bảo tồn tài nguyên tính toán. Do đó, đạt được cân bằng giữa hiệu suất và hiệu quả là một cân nhắc quan trọng.

3) (RQ4.3) Hiệu quả của các Phương pháp PEFT: Như được hiển thị trong bảng, các phương pháp PEFT giảm số lượng tham số có thể huấn luyện xuống dưới 1% trong khi vẫn đảm bảo hiệu suất chấp nhận được. Cho rằng các phương pháp PEFT giữ trọng số của mô hình cơ sở không đổi, không gian lưu trữ giảm mạnh từ 13GB xuống dưới 20MB. Những trọng số plug-in độc lập này làm cho các phương pháp PEFT phù hợp lý tưởng cho các quy trình đa tác vụ, như tự động hóa các hoạt động đánh giá mã nguồn.

Trả lời cho RQ4: Trong số các phương pháp PEFT, LoRA phù hợp hơn để tự động hóa các tác vụ đánh giá mã nguồn. Bằng cách chọn hạng LoRA thích hợp, LLaMA-Reviewer có thể đạt hiệu suất cạnh tranh với ít hơn 1% tham số có thể huấn luyện và yêu cầu không gian lưu trữ giảm đáng kể.

VI. CÔNG TRÌNH LIÊN QUAN

A. Tinh chỉnh trên LLaMA

Trong khi ChatGPT và dòng GPT sở hữu độc quyền của OpenAI đã thúc đẩy tiến bộ đáng kể trong lĩnh vực AI, bản chất mã nguồn đóng của chúng đã tạo ra sự dè dặt trong giới nghiên cứu. Giải quyết mối quan tâm này, Meta đã giới thiệu Mô hình Ngôn ngữ Lớn (LLaMA) mã nguồn mở của họ [37], nhanh chóng nổi lên như một tài sản then chốt trong bối cảnh AI do khả năng hiệu suất đáng chú ý của nó.

Một loạt các mô hình được tinh chỉnh dựa trên LLaMA đã cho thấy hiệu suất xuất sắc, cạnh tranh với dòng ChatGPT và GPT. Alpaca của Stanford [53] đại diện cho một phát triển quan trọng sớm trong lĩnh vực này, tinh chỉnh LLaMA sử dụng bộ dữ liệu được tạo từ ChatGPT. Các công trình đáng chú ý tiếp theo đã theo đuổi một loạt mục tiêu, bao gồm thích ứng cụ thể theo ngôn ngữ [59], [60], nâng cao chất lượng văn bản [61], [62], accommodating đầu vào đa phương thức [49], [63], [64], và cải thiện khả năng liên quan đến mã nguồn [54].

Do nhu cầu tính toán đáng kể của tinh chỉnh tham số đầy đủ, các nhà nghiên cứu đã hướng tới các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) để tinh chỉnh LLaMA. Một ví dụ như vậy là Alpaca LoRA, đạt hiệu suất tương đương với Alpaca chỉ với 0.13% tham số huấn luyện, dẫn đến tăng tốc khoảng 60 lần [50]. LLaMA-adapter, được giới thiệu bởi Zhang et al. [49], đại diện cho một đóng góp thêm, sử dụng phương pháp zero-init attention prefix-tuning.

Nghiên cứu của chúng tôi tập trung vào đánh giá hiệu suất của LLaMA trên các tác vụ liên quan đến đánh giá mã nguồn và sử dụng các phương pháp PEFT tiên tiến để nâng cao hiệu quả huấn luyện.

B. Tự động hóa các Hoạt động Đánh giá Mã nguồn

Đánh giá mã nguồn là một khía cạnh thiết yếu, mặc dù tốn thời gian, của phát triển phần mềm, khơi dậy sự quan tâm đáng kể đến các chiến lược tự động hóa cho các hoạt động như đề xuất người đánh giá [6]–[15], đánh giá chất lượng mã nguồn [12], [16]–[21], tinh chỉnh mã nguồn có vấn đề [20], [22]–[25], và gợi ý bình luận đánh giá [20], [23], [26]–[31]. Bài báo này tập trung vào pipeline được đề xuất bởi Li et al. [20], bao gồm dự đoán tính cần thiết đánh giá, tạo bình luận đánh giá mã nguồn, và tinh chỉnh mã nguồn.

Các nghiên cứu sớm về dự đoán tính cần thiết đánh giá chủ yếu kiểm tra việc chấp nhận diff hunk, với Shi et al. [16] tiên phong một khung công tác dựa trên CNN và LSTM, DACE, và Hellendoorn et al. [17] sử dụng Transformer để chiếm tỷ lệ quan hệ inter-diff hunk trong một pull request (PR) duy nhất. Tuy nhiên, lĩnh vực này kể từ đó đã phát triển, với Li et al. [20] chuyển trọng tâm hướng tới xác định diff hunk yêu cầu đánh giá và tích hợp điều này vào một pipeline với một mô hình được tiền huấn luyện đặc thú cho đánh giá mã nguồn.

Những nỗ lực ban đầu trong bình luận đánh giá mã nguồn tận dụng các phương pháp dựa trên truy xuất để trích xuất bình luận lịch sử. Gupta et al. [27] giới thiệu một mô hình dựa trên LSTM, DeepMem, để đề xuất bình luận cho các đoạn mã mới dựa trên mối quan hệ của chúng với các thay đổi mã, trong khi Siow et al. [28] nâng cao truy xuất thông qua việc nắm bắt thông tin ngữ nghĩa LSTM dựa trên attention. Lĩnh vực này kể từ đó đã chuyển hướng tới tạo bình luận đánh giá với sự gia tăng của học sâu. Tufano et al. [23] tiên phong cách tiếp cận này, tiền huấn luyện một mô hình trên cả mã nguồn và ngôn ngữ kỹ thuật, với những nỗ lực tiếp theo bởi CodeReviewer [20] và AUGER [30] sử dụng một mô hình được tiền huấn luyện đặc thù cho đánh giá mã nguồn và review tag tương ứng để có kết quả cải thiện. Đồng thời, CommentFinder [31] trình bày một lựa chọn dựa trên truy xuất hiệu quả.

Đối với tinh chỉnh mã nguồn, những nỗ lực sớm thường được sắp xếp với các kỹ thuật sửa lỗi tự động [65]–[67]. Tiên phong việc thích ứng tác vụ này với đánh giá mã nguồn, Tufano et al. [68] tập trung vào học từ các thay đổi mã được triển khai trong PR. Các nhà nghiên cứu sau đó kết hợp bình luận đánh giá mã nguồn vào đầu vào tác vụ để mô phỏng tốt hơn tinh chỉnh mã nguồn [22], [23]. Giải quyết thách thức của token mới, AutoTransforms [25] sử dụng cách tiếp cận byte-pair encoding (BPE), bổ sung sử dụng phương pháp biết diff, D-ACT [56], để tăng hiệu suất trong các trường hợp liên quan chặt chẽ đến sự khác biệt đơn lẻ giữa cơ sở mã và commit ban đầu. Tuy nhiên, họ không tính đến ảnh hưởng của bình luận đánh giá mã nguồn và loại trừ dữ liệu với đầu vào tương tự. CoditT5 [57], một mô hình được tiền huấn luyện được thiết kế rõ ràng để chỉnh sửa, sử dụng một phần bộ dữ liệu của Tufano để xác thực như một tác vụ downstream. Tương tự, CodeReviewer [20] phát triển một mô hình dựa trên mô hình được tiền huấn luyện của họ, được thiết kế đặc biệt cho quy trình đánh giá mã nguồn.

Mặc dù có tiến bộ đáng kể trong việc tự động hóa các tác vụ đánh giá mã nguồn, nghiên cứu trước đó thường bỏ qua tiềm năng của các mô hình ngôn ngữ lớn thống nhất (LLM). Khi kích thước mô hình và dữ liệu huấn luyện tiếp tục phát triển, LLM thống nhất đang cải thiện hiệu suất của chúng với tốc độ nhanh và cho thấy hiệu suất tương đương với những mô hình được tiền huấn luyện đặc thù cho từng tác vụ. Đối với các mô hình được tiền huấn luyện đặc thù cho từng tác vụ, xây dựng từ đầu tốn nhiều tài nguyên và thời gian. Do đó, trong nghiên cứu này, chúng tôi tận dụng LLaMA, một mô hình ngôn ngữ lớn thống nhất chính thống, để điều tra tiềm năng phát triển của LLM và đánh giá sự phù hợp của chúng cho các tác vụ bao gồm cả lập trình và ngôn ngữ tự nhiên, như các tác vụ đánh giá mã nguồn.

VII. MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ

A. Tính hợp lệ Cấu trúc

Đánh giá của chúng tôi dựa chủ yếu vào một biến thể của tiêu chí BLEU-4. Mặc dù được sử dụng rộng rãi trong nghiên cứu trước đó [20], [22], [23], [25], [31], [57], nó không được công nhận phổ biến như tiêu chí xác định để đánh giá bình luận đánh giá mã nguồn và các đoạn mã được tinh chỉnh. Các tiêu chí khác như rouge không được xem xét vì một số mô hình baseline không cung cấp kết quả trực tiếp hoặc mô hình được tinh chỉnh và dự đoán được tạo. Các bài kiểm tra của chúng tôi trên các baseline, bao gồm AUGER [30] và CommentFinder [31], dựa vào mã hoặc mô hình được cung cấp bởi bài báo gốc, có thể đã chệch khỏi kết quả tối ưu do các định dạng và phân phối dữ liệu khác nhau.

B. Tính hợp lệ Nội bộ

Đánh giá của chúng tôi về tinh chỉnh hiệu quả tham số (PEFT) được giới hạn ở hai phương pháp PEFT nổi bật và không bao gồm so sánh tinh chỉnh tham số đầy đủ. Điều này là do các tài nguyên tính toán đáng kể cần thiết cho tinh chỉnh tham số đầy đủ, đòi hỏi 8 A100-SXM4-80G hơn nửa tháng. Tương tự, các thí nghiệm loại bỏ của chúng tôi bao gồm một tập hợp cài đặt hạn chế do ràng buộc tài nguyên. Mặc dù những hạn chế này có thể đã dẫn đến các phát hiện thay thế, chúng không làm suy yếu cơ bản mục tiêu chính của chúng tôi: đánh giá tiềm năng của các mô hình ngôn ngữ lớn thống nhất.

Một mối đe dọa tính hợp lệ nội bộ tiềm năng khác là chúng tôi hạn chế huấn luyện của chúng tôi ở kích thước nhỏ nhất của LLaMA và một số epoch hữu hạn. Cho rằng khả năng và kết quả dữ liệu của LLaMA tiếp tục cải thiện với sự gia tăng kích thước mô hình và thời gian tinh chỉnh, nghiên cứu của chúng tôi có thể đánh giá thấp khả năng tiềm ẩn thực tế của các mô hình ngôn ngữ lớn.

C. Tính hợp lệ Bên ngoài

Các phát hiện của chúng tôi có thể không tổng quát hóa ngoài bối cảnh của hai bộ dữ liệu [20], [23] được sử dụng trong nghiên cứu này, được rút ra độc quyền từ các dự án mã nguồn mở. Như vậy, những phát hiện này có thể không áp dụng đầy đủ cho các bối cảnh công nghiệp và khác. Ngoài ra, mỗi bộ dữ liệu chỉ giữ lại một bình luận duy nhất cho mỗi thay đổi mã, có thể giới thiệu thiên vị trong quá trình lọc.

VIII. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi giới thiệu LLaMA-Reviewer, một khung công tác để tự động hóa quy trình đánh giá mã nguồn sử dụng các mô hình ngôn ngữ lớn (LLM) và các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT). Chúng tôi chứng minh rằng, mặc dù sử dụng phiên bản nhỏ nhất của LLaMA với chỉ 6.7B tham số và ít hơn 1% tham số có thể huấn luyện qua một số epoch tinh chỉnh hạn chế, LLaMA-Reviewer có thể khớp với hiệu suất của các mô hình tập trung vào đánh giá mã nguồn tiên tiến nhất. Ngoài ra, bằng cách áp dụng các mô hình kiểu plug-in, chúng tôi giảm đáng kể yêu cầu không gian lưu trữ.

Các phát hiện của chúng tôi cũng gợi ý rằng việc sắp xếp biểu diễn đầu vào với định dạng được sử dụng trong quá trình tiền huấn luyện có thể tận dụng tốt hơn khả năng của LLM. Ngoài ra, giai đoạn ban đầu của tinh chỉnh hướng dẫn có thể cải thiện hiệu suất tác vụ và tăng khả năng của mô hình để xử lý thông tin ngôn ngữ tự nhiên bổ sung. Kết quả của chúng tôi cũng cho thấy rằng low-rank adaptation với hạng thích hợp được ưa thích cho các tác vụ với định dạng đầu vào và đầu ra cụ thể.

Nhìn về phía trước, chúng tôi nhằm mở rộng khám phá của chúng tôi về các mô hình ngôn ngữ lớn, xem xét các mô hình có kích thước và loại khác nhau, và điều tra thêm các phương pháp luận PEFT. Chúng tôi cũng quan tâm đến việc kiểm tra chặt chẽ hơn mối quan hệ giữa độ dài token của mẫu prompt, đoạn mã, bình luận, và khối chuỗi của các mô hình được tiền huấn luyện.

LỜI CẢM ÔN

Công trình này được hỗ trợ bởi Kế hoạch Mạng lưới Dịch vụ Khoa học và Công nghệ Viện Hàn lâm Khoa học Trung Quốc-Đông Quan (Số 202016002000032), và Liên minh các Tổ chức Khoa học Quốc tế (Số ANSO-CR-KP-2022-03).

TÀI LIỆU THAM KHẢO

[Phần này chứa danh sách đầy đủ các tài liệu tham khảo được đánh số từ [1] đến [68], bao gồm các nghiên cứu về đánh giá mã nguồn, mô hình ngôn ngữ, và các kỹ thuật học máy liên quan]
