# 2110.07560.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2110.07560.pdf
# Kích thước tệp: 2778404 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh thưa thớt có thể kết hợp cho chuyển giao đa ngôn ngữ
Alan Ansell1Edoardo Maria Ponti1;2Anna Korhonen1Ivan Vuli ´c1
1Phòng thí nghiệm Công nghệ Ngôn ngữ, Đại học Cambridge
2Viện AI Mila - Quebec và Đại học McGill
Tóm tắt
Tinh chỉnh toàn bộ tập tham số của một mô hình được tiền huấn luyện lớn đã trở thành phương pháp chủ đạo cho học chuyển giao. Để tăng hiệu quả và ngăn chặn quên thảm khốc cũng như nhiễu, các kỹ thuật như adapter và tinh chỉnh thưa thớt đã được phát triển. Adapter có tính mô-đun, vì chúng có thể được kết hợp để thích ứng mô hình theo các khía cạnh khác nhau của kiến thức (ví dụ: các adapter ngôn ngữ và/hoặc nhiệm vụ chuyên dụng). Tinh chỉnh thưa thớt có tính biểu đạt, vì nó điều khiển hành vi của tất cả các thành phần mô hình. Trong công trình này, chúng tôi giới thiệu một phương pháp tinh chỉnh mới có cả hai tính chất mong muốn này. Cụ thể, chúng tôi học các mặt nạ thưa thớt, có giá trị thực dựa trên một biến thể đơn giản của Giả thuyết Vé số. Các mặt nạ đặc thù nhiệm vụ được thu được từ dữ liệu được chú thích trong ngôn ngữ nguồn, và các mặt nạ đặc thù ngôn ngữ từ mô hình hóa ngôn ngữ có mặt nạ trong ngôn ngữ đích. Cả hai mặt nạ này sau đó có thể được kết hợp với mô hình được tiền huấn luyện. Khác với tinh chỉnh dựa trên adapter, phương pháp này không tăng số lượng tham số tại thời điểm suy luận cũng không thay đổi kiến trúc mô hình gốc. Quan trọng nhất, nó vượt trội so với adapter trong chuyển giao đa ngôn ngữ zero-shot với biên độ lớn trong một loạt các điểm chuẩn đa ngôn ngữ, bao gồm Universal Dependencies, MasakhaNER, và AmericasNLI. Dựa trên phân tích chuyên sâu, chúng tôi bổ sung tìm thấy rằng độ thưa thớt rất quan trọng để ngăn chặn cả 1) nhiễu giữa các tinh chỉnh được kết hợp và 2) quá khớp. Chúng tôi công bố mã và mô hình tại https://github.com/cambridgeltl/composable-sft.

1 Giới thiệu
Tinh chỉnh các mô hình được tiền huấn luyện (Howard và Ruder, 2018; Devlin et al., 2019, inter alia) có thể nói là mô hình chủ đạo trong NLP hiện tại. Ban đầu, "tinh chỉnh" liên quan đến việc học có giám sát tất cả các tham số của một mô hình được tiền huấn luyện trên các văn bản không có nhãn. Tuy nhiên, với kích thước của các kiến trúc dựa trên Transformer, phương pháp này thường không hiệu quả về thời gian và tài nguyên, và có thể dẫn đến quên thảm khốc và nhiễu (Wang et al., 2020) trong quá trình thích ứng nhiều lần. Để vượt qua những hạn chế này, hai lựa chọn chính đã xuất hiện: 1) thông qua adapter, các tham số mới có thể được thêm vào mô hình được tiền huấn luyện dưới dạng các lớp trung gian bổ sung (Rebuffi et al., 2017; Houlsby et al., 2019) và được tinh chỉnh trong khi giữ tất cả các tham số được tiền huấn luyện cố định; 2) tinh chỉnh thưa thớt (SFT) của một tập con nhỏ các tham số mô hình được tiền huấn luyện (Guo et al., 2021; Zaken et al., 2021; Xu et al., 2021b, inter alia).

Adapter đã được chứng minh là đặc biệt hữu ích trong NLP đa ngôn ngữ (Bapna và Firat, 2019; Üstün et al., 2020; Pfeiffer et al., 2020b; Vidoni et al., 2020; Pfeiffer et al., 2021b; Ansell et al., 2021) bởi vì chúng thể hiện mức độ mô-đun đáng ngạc nhiên. Khả năng tách biệt và tái kết hợp các khía cạnh trực giao của kiến thức theo những cách nguyên bản (Ponti et al., 2021; Ponti, 2021) cho phép học riêng biệt một adapter nhiệm vụ từ dữ liệu có nhãn trong ngôn ngữ nguồn và các adapter ngôn ngữ chuyên dụng từ dữ liệu không có nhãn trong ngôn ngữ nguồn và các ngôn ngữ đích. Bằng cách xếp chồng các thành phần này, có thể thực hiện chuyển giao đa ngôn ngữ zero-shot. So với tinh chỉnh tuần tự toàn bộ mô hình trên cả nhiệm vụ và ngôn ngữ đích, điều này mang lại hiệu suất và hiệu quả vượt trội (Pfeiffer et al., 2020b). Đáng chú ý, việc đạt được phủ sóng trên NT nhiệm vụ trong NL ngôn ngữ đích với phương pháp tuần tự yêu cầu NTNL mô hình được huấn luyện, trong khi tính mô-đun của adapter giảm xuống còn NT+NL.

Trong khi đó, lợi thế của SFT so với adapter là tính biểu đạt của chúng: thay vì một phép biến đổi phi tuyến của đầu ra các lớp Transformer (ví dụ: sử dụng MLP nông như với adapter), chúng có thể hoạt động trực tiếp trên các lớp embedding và attention của mô hình được tiền huấn luyện. Do đó, có vẻ tự nhiên khi tìm kiếm một phương pháp tinh chỉnh tiết kiệm tham số vừa có tính mô-đun vừa có tính biểu đạt.

--- TRANG 2 ---
1. Mô hình được tiền huấn luyện 3. Mô hình được tinh chỉnh 2a. Tinh chỉnh thưa thớt ngôn ngữ 2b. Tinh chỉnh thưa thớt nhiệm vụ

Hình 1: Biểu diễn đồ họa của Tinh chỉnh thưa thớt Vé số: từ các tham số của mô hình được tiền huấn luyện (xám, trái), chúng tôi tạo ra các tinh chỉnh thưa thớt cho kiến thức nhiệm vụ và ngôn ngữ (xanh và đỏ, giữa). Cuối cùng, chúng tôi cộng ba thành phần này (phải) để có được mô hình được thích ứng/tinh chỉnh. Xem tốt nhất ở màu sắc.

Để đạt được mục tiêu này, chúng tôi đề xuất Tinh chỉnh thưa thớt Vé số (LT-SFT), một kỹ thuật thích ứng đơn giản và có mục đích chung được lấy cảm hứng từ Giả thuyết Vé số (LTH; Frankle và Carbin, 2019; Malach et al., 2020), ban đầu được hình thành để cắt tỉa các mạng thần kinh lớn. Cụ thể, sau khi tinh chỉnh một mô hình được tiền huấn luyện cho một nhiệm vụ hoặc ngôn ngữ cụ thể, chúng tôi chọn tập con các tham số thay đổi nhiều nhất. Sau đó, chúng tôi đưa mô hình trở về khởi tạo được tiền huấn luyện của nó (không đặt bất kỳ giá trị nào về không, trái ngược với thuật toán LTH gốc). Bằng cách tinh chỉnh lại chỉ tập con các tham số đã chọn, chúng tôi có được một tinh chỉnh thưa thớt dưới dạng một vector các khác biệt so với mô hình được tiền huấn luyện. Nhiều SFT có thể được kết hợp bằng cách đơn giản cộng chúng với mô hình được tiền huấn luyện. Chúng tôi cung cấp một biểu diễn đồ họa của phương pháp trong Hình 1.

Chúng tôi đánh giá LT-SFT trên một loạt các tập dữ liệu đa ngôn ngữ, bao gồm Universal Dependencies (Zeman et al., 2020) cho gắn thẻ từ loại và phân tích cú pháp phụ thuộc, MasakhaNER (Adelani et al., 2021) cho nhận dạng thực thể có tên, và AmericasNLI (Ebrahimi et al., 2021) cho suy luận ngôn ngữ tự nhiên. Chúng tôi đánh giá nó trong môi trường chuyển giao đa ngôn ngữ zero-shot trên 35 ngôn ngữ đa dạng về kiểu học và địa lý bao gồm cả các ngôn ngữ được nhìn thấy và không được nhìn thấy trong quá trình mô hình hóa ngôn ngữ có mặt nạ của mô hình được tiền huấn luyện. Kết quả trong tất cả các nhiệm vụ chuyển giao cho thấy LT-SFT đạt được những cải thiện đáng kể một cách nhất quán so với phương pháp dựa trên adapter hiện tại tiên tiến cho chuyển giao đa ngôn ngữ, MAD-X (Pfeiffer et al., 2020b).

Ngoài hiệu suất vượt trội, tính mô-đun và tính biểu đạt, LT-SFT cung cấp một loạt các lợi thế bổ sung so với adapter: 1) số lượng tham số vẫn không đổi, ngăn chặn việc giảm tốc độ suy luận được quan sát khi các lớp adapter được thêm vào; 2) kiến trúc mạng thần kinh vẫn giống hệt với mô hình được tiền huấn luyện, làm cho việc phát triển mã độc lập với mô hình thay vì yêu cầu các sửa đổi đặc biệt cho từng kiến trúc có thể (Pfeiffer et al., 2020a). Cuối cùng, 3) chúng tôi chứng minh thực nghiệm rằng đỉnh hiệu suất cho LT-SFT được tìm thấy một cách nhất quán với cùng một tỷ lệ phần trăm tham số có thể tinh chỉnh, trong khi hệ số giảm tốt nhất cho MAD-X phụ thuộc vào nhiệm vụ. Điều này làm cho phương pháp của chúng tôi mạnh mẽ hơn đối với việc chọn siêu tham số.

Ngoài ra, chúng tôi tìm thấy rằng mức độ thưa thớt cao trong các tinh chỉnh ngôn ngữ và nhiệm vụ có lợi cho hiệu suất, vì điều này làm cho sự chồng chéo ít có khả năng xảy ra hơn và đặt ra nguy cơ thấp hơn trong việc tạo ra nhiễu giữa kiến thức mà chúng chứa đựng. Hơn nữa, nó làm cho các tinh chỉnh ít dễ bị quá khớp hơn do khả năng bị hạn chế của chúng. Do đó, độ thưa thớt là một thành phần cơ bản để đạt được tính mô-đun và khả năng kết hợp. Những tính chất này lần lượt cho phép khái quát hóa có hệ thống đến các kết hợp mới của nhiệm vụ và ngôn ngữ theo cách zero-shot.

2 Nền tảng
Để thiết lập bối cảnh rộng hơn cho nghiên cứu của chúng tôi, trước tiên chúng tôi cung cấp một cái nhìn tổng quan ngắn gọn về các phương pháp hiện tại cho tinh chỉnh hiệu quả, như adapter và SFT. Sau đó chúng tôi tóm tắt lại Giả thuyết Vé số,

--- TRANG 3 ---
mà phương pháp mới được đề xuất của chúng tôi được xây dựng dựa trên.

Adapter và Kết hợp. Một adapter là một thành phần được chèn vào mô hình Transformer với mục đích chuyên hóa nó cho một ngôn ngữ, nhiệm vụ, miền hoặc phương thức cụ thể (Houlsby et al., 2019). Công trình trước đây trong NLP đa ngôn ngữ chủ yếu đã áp dụng biến thể adapter nhẹ nhưng hiệu quả của Pfeiffer et al. (2021a). Trong thiết lập này, chỉ một mô-đun adapter, bao gồm một phép chiếu xuống và phép chiếu lên liên tiếp, được chèn vào mỗi lớp Transformer, sau lớp con feed-forward. Adapter Ab tại lớp Transformer thứ b thực hiện phép toán sau:

Ab(hb;rb) = Uba(Dbhb) + rb: (1)

hb và rb là trạng thái ẩn Transformer và phần dư tại lớp b, tương ứng. Db∈Rm×h và Ub∈Rh×m là các phép chiếu xuống và lên, tương ứng (h là kích thước lớp ẩn của Transformer, và m là chiều của adapter), và a() là một hàm kích hoạt phi tuyến. Kết nối dư rb là đầu ra của lớp feed-forward của Transformer trong khi hb là đầu ra của lớp chuẩn hóa tiếp theo. Trong quá trình tinh chỉnh mô hình được tiền huấn luyện với adapter, chỉ các tham số adapter U và D được sửa đổi trong khi các tham số của mô hình được tiền huấn luyện được giữ cố định.

Trong khung kết hợp adapter MAD-X cho chuyển giao đa ngôn ngữ (Pfeiffer et al., 2020b), một adapter ngôn ngữ (LA) cho Transformer đa ngôn ngữ khổng lồ (MMT) được học cho mỗi ngôn ngữ nguồn và đích thông qua mô hình hóa ngôn ngữ có mặt nạ (MLM), và một adapter nhiệm vụ (TA) được học cho mỗi nhiệm vụ đích, trong đó LA cho ngôn ngữ nguồn được chèn trong quá trình huấn luyện TA. Tại thời điểm suy luận, adapter nhiệm vụ và adapter ngôn ngữ đích được kết hợp bằng cách xếp chồng cái này lên cái kia. Phương pháp kết hợp adapter này đã được chứng minh là rất hiệu quả cho chuyển giao đa ngôn ngữ (Pfeiffer et al., 2020b, 2021b; Ansell et al., 2021), đặc biệt là cho các ngôn ngữ ít tài nguyên và các ngôn ngữ đích không được nhìn thấy trong quá trình tiền huấn luyện MMT.

Tinh chỉnh thưa thớt. Chúng tôi gọi F'=F(;θ+δ) là một tinh chỉnh thưa thớt (SFT) của mô hình thần kinh được tiền huấn luyện F(;θ) nếu δ thưa thớt. Đôi khi chúng tôi gọi δ chính nó là một SFT, hoặc là vector khác biệt của SFT. Các phương pháp SFT được đề xuất trước đây bao gồm DiffPruning (Guo et al., 2021), BitFit (Zaken et al., 2021) và ChildTuning (Xu et al., 2021b). DiffPruning mô phỏng độ thưa thớt của vector khác biệt trong quá trình huấn luyện bằng cách áp dụng một phép nới lỏng liên tục của mặt nạ nhị phân lên nó. Mặt khác, BitFit chỉ cho phép các khác biệt khác không cho các tham số bias. ChildTuning chọn một tập con các tham số có thể tinh chỉnh bằng cách sử dụng thông tin Fisher để đo lường mức độ liên quan của mỗi tham số đến nhiệm vụ. Những phương pháp này đã được chứng minh là cạnh tranh với tinh chỉnh đầy đủ trên GLUE (Wang et al., 2019), mặc dù vector khác biệt có ít hơn 0.5% giá trị khác không.

Giả thuyết Vé số. (LTH; Frankle và Carbin, 2019; Malach et al., 2020) nêu rằng mỗi mô hình thần kinh chứa một mạng con (một "vé số thắng") mà, nếu được huấn luyện lại một cách riêng biệt, có thể khớp hoặc thậm chí vượt quá hiệu suất của mô hình gốc. Để đạt được điều này, sau một giai đoạn cắt tỉa trong đó một số tham số được che mặt nạ không và đóng băng theo một tiêu chí nào đó (ví dụ: độ lớn trọng số), các tham số còn lại được khôi phục về giá trị ban đầu của chúng và sau đó được tinh chỉnh lại. Quá trình cắt tỉa và huấn luyện lại này có thể được lặp lại nhiều lần. LTH cho đến nay chủ yếu được sử dụng để nén mô hình thông qua cắt tỉa mạng; theo hiểu biết của chúng tôi, chúng tôi là người đầu tiên sử dụng nó để thích ứng mô hình được tiền huấn luyện.

Huấn luyện Nhiệm vụ Đa nguồn. Ansell et al. (2021) cho thấy rằng việc huấn luyện adapter nhiệm vụ sử dụng dữ liệu từ nhiều ngôn ngữ nguồn có thể dẫn đến những cải thiện đáng kể trong hiệu suất chuyển giao zero-shot hạ nguồn ngay cả khi tổng số ví dụ huấn luyện được giữ không đổi. Trong thiết lập huấn luyện của họ, mỗi batch bao gồm các ví dụ từ một ngôn ngữ nguồn duy nhất, được chọn ngẫu nhiên, adapter ngôn ngữ mà được kích hoạt trong suốt thời gian của bước huấn luyện.

3 Phương pháp
3.1 Tinh chỉnh thưa thớt Vé số

Huấn luyện. Trong công trình này, chúng tôi đề xuất Tinh chỉnh thưa thớt Vé số (LT-SFT). Tương tự như thuật toán Vé số của Frankle và Carbin (2019), phương pháp LT-SFT của chúng tôi bao gồm hai giai đoạn:

(Giai đoạn 1) Các tham số mô hình được tiền huấn luyện θ(0) được tinh chỉnh đầy đủ trên dữ liệu ngôn ngữ hoặc nhiệm vụ đích D, tạo ra θ(1). Các tham số được xếp hạng theo một tiêu chí nào đó, trong trường hợp của chúng tôi là khác biệt tuyệt đối lớn nhất |θ(1)i - θ(0)i|, và K tham số hàng đầu được chọn để tinh chỉnh trong giai đoạn tiếp theo: một mặt nạ nhị phân được đặt có 1 ở các vị trí tương ứng với những tham số này, và 0 ở nơi khác.

(Giai đoạn 2) Sau khi đặt lại các tham số về giá trị ban đầu của chúng θ(0), mô hình được tinh chỉnh lại, nhưng lần này chỉ K tham số đã chọn có thể huấn luyện được trong khi những tham số khác được giữ đóng băng. Trong thực tế, chúng tôi thực hiện điều này bằng cách truyền gradient có mặt nạ ∇θL(F(;θ);D) ⊙ m (trong đó ⊙ biểu thị nhân theo từng phần tử và L là hàm mất mát) cho trình tối ưu hóa tại mỗi bước. Từ các tham số tinh chỉnh kết quả θ(2), chúng tôi có thể thu được vector khác biệt thưa thớt δ = θ(2) - θ(0).

Ngoài ra, chúng tôi thử nghiệm với việc áp dụng một thuật ngữ chính quy hóa ngăn cản các tham số khỏi việc lệch khỏi giá trị được tiền huấn luyện của chúng θ(0). Cụ thể, chúng tôi sử dụng chính quy hóa L1 có dạng J(θ) = λ∑Ni=1|θi - θ(0)i|.

Kết hợp. Mặc dù chúng tôi thường sử dụng thuật ngữ "tinh chỉnh thưa thớt" để chỉ chính vector khác biệt, một SFT được khái niệm hóa chính xác nhất như một hàm số lấy một hàm có tham số làm đối số của nó và trả về một hàm mới, trong đó một số vector khác biệt thưa thớt đã được thêm vào vector tham số ban đầu. Giả sử chúng ta có một SFT ngôn ngữ SL và một SFT nhiệm vụ ST được định nghĩa bởi
SL(F(;θ)) = F(;θ+δL)
ST(F(;θ)) = F(;θ+δT):

Khi đó chúng ta có
SL◦ST(F(;θ)) = F(;θ+δT+δL):

3.2 Chuyển giao Zero-Shot với LT-SFT

Chúng tôi áp dụng một thiết lập chuyển giao đa ngôn ngữ tương tự như MAD-X (Pfeiffer et al., 2020b, xem thêm §2). Chúng tôi bắt đầu với một MMT F với các tham số được tiền huấn luyện θ được học thông qua mô hình hóa ngôn ngữ có mặt nạ trên nhiều ngôn ngữ, như mBERT (Devlin et al., 2019) hoặc XLM-R (Conneau et al., 2020).

Đối với mỗi ngôn ngữ quan tâm l, chúng tôi học một SFT ngôn ngữ δ(l)L thông qua LT-SFT (cũng với mục tiêu MLM) trên văn bản từ ngôn ngữ l.

Đối với mỗi nhiệm vụ quan tâm t, chúng tôi học một SFT nhiệm vụ δ(t)T thông qua LT-SFT trên dữ liệu được chú thích từ một ngôn ngữ nguồn s nào đó. Khi học SFT nhiệm vụ, trước tiên chúng tôi thích ứng với ngôn ngữ nguồn bằng cách áp dụng SFT ngôn ngữ cho s.¹ SFT ngôn ngữ được loại bỏ lại sau khi huấn luyện. Tức là, chúng tôi thực hiện LT-SFT trên F(;θ+δ(s)L) để có được vector tham số tinh chỉnh θ'. Sau đó chúng tôi tính δ(t)T = θ' - (θ+δ(s)L). Lưu ý rằng trong quá trình huấn luyện nhiệm vụ, chúng tôi cũng học một đầu phân loại, được tinh chỉnh đầy đủ trong cả hai giai đoạn của việc thích ứng LT-SFT, với cùng một khởi tạo ngẫu nhiên được áp dụng ở đầu mỗi giai đoạn.

Chúng tôi thực hiện thích ứng zero-shot của F với ngôn ngữ đích l cho nhiệm vụ t bằng cách kết hợp các SFT ngôn ngữ và nhiệm vụ để có được Ft,l = F(;θ+δ(t)T+δ(l)L). Trên đỉnh này, chúng tôi xếp chồng đầu phân loại đã học cho t. Để có thuật toán chính thức của LT-SFT và quy trình chuyển giao, chúng tôi tham khảo Phụ lục A.

¹Thích ứng với ngôn ngữ nguồn mang lại những cải thiện đáng kể trong hiệu suất chuyển giao đa ngôn ngữ với cả MAD-X và LT-SFT, với mức tăng 2-3 điểm trong các thí nghiệm sơ bộ của chúng tôi. Một cách nghịch lý, kết quả của chúng tôi (xem Bảng 7) và kết quả từ công trình trước đây (Pfeiffer et al., 2020b; Ansell et al., 2021) cho thấy rằng việc thích ứng với các ngôn ngữ đích có nhiều tài nguyên tại thời điểm suy luận không mang lại những lợi ích lớn tương tự. Chúng tôi nghĩ rằng hiện tượng này đáng được nghiên cứu sâu hơn.

4 Thiết lập Thí nghiệm

Để đánh giá phương pháp mới của chúng tôi một cách toàn diện, chúng tôi đánh giá hiệu suất chuyển giao đa ngôn ngữ zero-shot của nó trên bốn nhiệm vụ riêng biệt: gắn thẻ từ loại (POS), phân tích cú pháp phụ thuộc (DP), nhận dạng thực thể có tên (NER), và suy luận ngôn ngữ tự nhiên (NLI). Bảng 1 tóm tắt thiết lập thí nghiệm của chúng tôi, bao gồm các tập dữ liệu và ngôn ngữ được xem xét trong các thí nghiệm của chúng tôi. Chúng tôi nhấn mạnh các ngôn ngữ ít tài nguyên và các ngôn ngữ không được nhìn thấy trong quá trình tiền huấn luyện MMT, mặc dù chúng tôi cũng đánh giá trên một vài ngôn ngữ có nhiều tài nguyên. Tổng cộng, chúng tôi bao phủ một tập hợp 35 ngôn ngữ đa dạng về kiểu học và địa lý, làm cho chúng đại diện cho sự biến đổi đa ngôn ngữ (Ponti et al., 2019, 2020).

4.1 Baseline và Biến thể Mô hình

Baseline chính là MAD-X, khung dựa trên adapter tiên tiến cho chuyển giao đa ngôn ngữ (Pfeiffer et al., 2020b). Chúng tôi sử dụng biến thể "MAD-X 2.0", trong đó các lớp adapter cuối cùng bị loại bỏ. Pfeiffer et al. (2021b) phát hiện rằng điều này cải thiện hiệu suất, điều mà chúng tôi có thể xác nhận trong các thí nghiệm sơ bộ của chúng tôi. Vì các adapter với cấu hình được sử dụng bởi Pfeiffer et al. (2020b) không có sẵn cho nhiều ngôn ngữ trong đánh giá của chúng tôi, chúng tôi huấn luyện riêng cho tất cả các ngôn ngữ. Trong Phụ lục D, chúng tôi cũng cung cấp một đánh giá với các adapter ngôn ngữ có thể so sánh từ AdapterHub (Pfeiffer et al., 2020a) nếu có.

Chúng tôi cũng thực hiện các thí nghiệm với BITFIT (Zaken et al., 2021) để thiết lập một baseline cho một kỹ thuật SFT hiện có. Ngoài biến thể mô hình LT-SFT chính, trên POS và DP chúng tôi kiểm tra một biến thể RAND-SFT như một phép loại bỏ, trong đó K tham số cần được tinh chỉnh được chọn ngẫu nhiên thay vì dựa trên một tiêu chí có thông tin.

--- TRANG 5 ---
Nhiệm vụ Đích Tập dữ liệu Tập dữ liệu Nguồn MMT Ngôn ngữ Đích
Gắn thẻ Từ loại (POS), Phân tích Cú pháp Phụ thuộc (DP) Universal Dependencies 2.7 (Zeman et al., 2020) Universal Dependencies 2.7 (Zeman et al., 2020) mBERT Tiếng Ả Rập†, Bambara, Buryat, Quảng Đông, Tiếng Trung†, Erzya, Faroese, Tiếng Nhật†, Livvi, Malta, Manx, Bắc Sami, Komi Zyrian, Phạn, Thượng Sorbian, Uyghur
Nhận dạng Thực thể Có tên (NER) MasakhaNER (Adelani et al., 2021) CoNLL 2003 (Tjong Kim Sang và De Meulder, 2003) mBERT Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, Yorùbá
Suy luận Ngôn ngữ Tự nhiên (NLI) AmericasNLI (Ebrahimi et al., 2021) MultiNLI (Williams et al., 2018) XLM-R Aymara, Asháninka, Bribri, Guarani, Náhuatl, Otomí, Quechua, Rarámuri, Shipibo-Konibo, Wixarika

Bảng 1: Chi tiết về các nhiệm vụ, tập dữ liệu, MMT và ngôn ngữ liên quan đến đánh giá chuyển giao đa ngôn ngữ zero-shot của chúng tôi. † biểu thị các ngôn ngữ ít tài nguyên được nhìn thấy trong quá trình tiền huấn luyện MMT; ‡ biểu thị các ngôn ngữ có nhiều tài nguyên được nhìn thấy trong quá trình tiền huấn luyện MMT; tất cả các ngôn ngữ khác đều ít tài nguyên và không được nhìn thấy. Ngôn ngữ nguồn luôn là tiếng Anh. Chi tiết thêm về tất cả các nguồn ngôn ngữ và dữ liệu được sử dụng được cung cấp trong Phụ lục B.

Đối với cả LT-SFT và MAD-X, chúng tôi cũng đánh giá một cấu hình CHỈ thích ứng nhiệm vụ (TA-ONLY), trong đó chỉ SFT/adapter nhiệm vụ được áp dụng, không có SFT/adapter ngôn ngữ đích.

4.2 Thiết lập Huấn luyện SFT/Adapter Ngôn ngữ

Dữ liệu Huấn luyện MLM. Đối với tất cả các ngôn ngữ trong đánh giá POS và DP của chúng tôi, chúng tôi thực hiện huấn luyện SFT/adapter ngôn ngữ MLM trên các kho ngữ liệu Wikipedia. Chúng tôi cũng sử dụng Wikipedia cho tất cả các ngôn ngữ trong đánh giá NER nếu có. Khi không có, chúng tôi sử dụng Tập dữ liệu Tin tức Luo (Adelani et al., 2021) cho Luo và kho ngữ liệu JW300 (Agić và Vulić, 2019) cho Nigerian Pidgin. Các kho ngữ liệu chính cho các ngôn ngữ trong đánh giá NLI của chúng tôi là những kho được sử dụng bởi các tác giả tập dữ liệu để huấn luyện các mô hình baseline của họ (Ebrahimi et al., 2021); tuy nhiên, vì kích thước của những kho ngữ liệu song song này nhỏ, chúng tôi tăng cường thêm chúng với dữ liệu từ Wikipedia và các kho ngữ liệu của các ngôn ngữ bản địa Peru của Bustamante et al. (2020) nếu có. Chi tiết thêm về các nguồn dữ liệu được cung cấp trong Phụ lục B.

Thiết lập Huấn luyện và Siêu tham số. Đối với cả SFT và adapter, chúng tôi huấn luyện trong thời gian ít hơn giữa 100 epoch hoặc 100.000 bước với kích thước batch 8 và độ dài chuỗi tối đa 256, tuân theo tối thiểu tuyệt đối là 30.000 bước vì 100 epoch có vẻ không đủ cho một số ngôn ngữ với kho ngữ liệu rất nhỏ. Các checkpoint mô hình được đánh giá mỗi 1.000 bước (5.000 cho các ngôn ngữ có nhiều tài nguyên) trên một tập giữ lại 5% của kho ngữ liệu (1% cho các ngôn ngữ có nhiều tài nguyên), và checkpoint có mất mát nhỏ nhất được chọn ở cuối quá trình huấn luyện. Chúng tôi sử dụng trình tối ưu hóa AdamW (Loshchilov và Hutter, 2019) với tốc độ học ban đầu 5e-5 được giảm tuyến tính về 0 trong suốt quá trình huấn luyện.

Theo Pfeiffer et al. (2020b), hệ số giảm (tức là tỷ lệ giữa kích thước ẩn mô hình và kích thước adapter) cho adapter baseline được đặt là 2 với tổng cộng ~7.6M tham số có thể huấn luyện. Để có thể so sánh, chúng tôi đặt cùng số lượng tham số có thể huấn luyện K cho các LT-SFT ngôn ngữ của chúng tôi. Điều này dẫn đến các SFT ngôn ngữ với độ thưa thớt 4.3% cho mBERT và 2.8% cho XLM-R. Vì BITFIT chỉ tinh chỉnh các tham số bias, các SFT ngôn ngữ của nó có độ thưa thớt cố định 0.047% cho mBERT và 0.030% cho XLM-R.

Quan trọng là, trong quá trình tinh chỉnh thưa thớt ngôn ngữ, chúng tôi tách biệt các ma trận embedding đầu vào và đầu ra và cố định các tham số của ma trận đầu ra; nếu không, chúng tôi thấy rằng phần lớn các tham số K thay đổi nhiều nhất trong quá trình tinh chỉnh đầy đủ thuộc về ma trận embedding, dường như do vị trí gần với đầu ra mô hình, điều này làm hại hiệu suất hạ nguồn. Chúng tôi cũng cố định các tham số chuẩn hóa lớp; tất cả các tham số khác có thể huấn luyện được. Đối với thích ứng ngôn ngữ, chúng tôi áp dụng chính quy hóa L1 như mô tả trong §3.1 với λ = 0.1. Lưu ý rằng chế độ huấn luyện được chỉ định được áp dụng theo cách tương tự trong cả hai giai đoạn của LT-SFT.

Đối với huấn luyện adapter ngôn ngữ trong baseline MAD-X, chúng tôi sử dụng cấu hình Pfeiffer (Pfeiffer et al., 2021a) với các adapter có thể đảo ngược, các thành phần con bổ sung đặc biệt được thiết kế để thích ứng với từ vựng của ngôn ngữ đích.

4.3 Thiết lập Huấn luyện SFT/Adapter Nhiệm vụ

Đối với gắn thẻ POS, DP, và NER,² chúng tôi huấn luyện SFT/adapter nhiệm vụ trên các tập dữ liệu được chỉ ra trong Bảng 1

²Các tập dữ liệu MasakhaNER và CoNLL 2003 tương ứng sử dụng các thẻ DATE và MISC không được sử dụng bởi cái khác; chúng tôi thay thế chúng bằng thẻ O tại cả thời gian huấn luyện và kiểm tra.

--- TRANG 6 ---
trong 10 epoch với kích thước batch 8, ngoại trừ trong giai đoạn đầu tiên của huấn luyện LT-SFT nơi chúng tôi huấn luyện chỉ 3 epoch.³ Các checkpoint mô hình được đánh giá trên tập validation mỗi 250 bước, và checkpoint tốt nhất được lấy ở cuối quá trình huấn luyện, với metric chọn lựa là độ chính xác cho POS, điểm gắn kết có nhãn cho DP, và điểm F1 cho NER. Tương tự như tinh chỉnh ngôn ngữ, chúng tôi sử dụng tốc độ học ban đầu 5e-5 được giảm tuyến tính về 0 trong suốt quá trình huấn luyện. Đối với POS và NER, chúng tôi sử dụng đầu mô hình đa lớp cấp token tiêu chuẩn một lớp. Đối với DP, chúng tôi sử dụng biến thể nông (Glavaš và Vulić, 2021) của bộ phân tích cú pháp phụ thuộc biaffine của Dozat và Manning (2017).

Đối với NLI, chúng tôi sử dụng cùng siêu tham số tinh chỉnh như Ebrahimi et al. (2021): 5 epoch với kích thước batch 32, với đánh giá checkpoint trên tập validation mỗi 625 bước, và tốc độ học ban đầu 2e-5. Chúng tôi áp dụng một đầu phân loại đa lớp hai lớp trên đầu ra MMT tương ứng với token [CLS].

Chúng tôi thấy rằng số lượng tham số có thể huấn luyện trong quá trình thích ứng nhiệm vụ (được điều chỉnh bởi K cho SFT và hệ số giảm cho adapter) có ảnh hưởng lớn đến hiệu suất: do đó chúng tôi thử nghiệm với một loạt các giá trị. Cụ thể, chúng tôi kiểm tra các hệ số giảm adapter 32, 16, 8, 4, 2, và 1, và các giá trị K tương đương⁴ cho SFT.

Trong quá trình thích ứng nhiệm vụ, chúng tôi luôn áp dụng adapter ngôn ngữ nguồn theo Pfeiffer et al. (2020b), hoặc SFT ngôn ngữ nguồn (xem §3.2).

4.4 Huấn luyện Đa nguồn

Để xác nhận rằng việc huấn luyện LT-SFT nhiệm vụ, giống như huấn luyện adapter nhiệm vụ trong công trình trước đây (Ansell et al., 2021), có lợi từ sự hiện diện của nhiều ngôn ngữ nguồn trong dữ liệu huấn luyện, và để đẩy ranh giới của chuyển giao đa ngôn ngữ zero-shot, chúng tôi thực hiện các thí nghiệm huấn luyện đa nguồn trên DP và NLI. Chúng tôi áp dụng một thiết lập tương tự như Ansell et al. (2021): chúng tôi có được tập huấn luyện bằng cách nối dữ liệu huấn luyện cho tất cả các ngôn ngữ nguồn. Chúng tôi trộn ngẫu nhiên tập huấn luyện và huấn luyện như trong trường hợp nguồn đơn, ngoại trừ việc mỗi batch được cấu thành từ các ví dụ từ một ngôn ngữ nguồn duy nhất, SFT ngôn ngữ mà được áp dụng trong bước huấn luyện.

Chúng tôi ưu tiên tối đa hóa hiệu suất hơn là cung cấp một so sánh công bằng với trường hợp nguồn đơn, vì vậy không giống như Ansell et al. (2021), chúng tôi sử dụng toàn bộ các tập huấn luyện. Trái với nguyên tắc này, chúng tôi đặt tối đa 15K ví dụ mỗi ngôn ngữ cho DP để cân bằng tốt hơn mẫu của chúng tôi.

Đối với DP, chúng tôi huấn luyện các mô hình của chúng tôi trên các treebank UD của 11 ngôn ngữ có nhiều tài nguyên đa dạng. Đối với NLI, chúng tôi huấn luyện trên MultiNLI (Williams et al., 2018) cộng với dữ liệu cho tất cả 14 ngôn ngữ không phải tiếng Anh trong tập dữ liệu XNLI (Conneau et al., 2018).

³Điều này là do tinh chỉnh đầy đủ dễ bị overfitting hơn tinh chỉnh thưa thớt/adapter. Dừng sớm phần nào giải quyết overfitting, nhưng nó không đủ trong môi trường đa ngôn ngữ vì hiệu suất ngôn ngữ đích thường bắt đầu giảm nhanh hơn hiệu suất ngôn ngữ nguồn.

⁴Tương ứng khoảng 442K, 884K, 1.7M, 3.5M, 7.1M, và 14.2M, tương ứng với các mức độ thưa thớt 0.25%, 0.50%, 1.0%, 2.0%, 4.0% và 8.0% cho mBERT và 0.16%, 0.32%, 0.63%, 1.3%, 2.6% và 5.1% cho XLM-R.

Chúng tôi cũng đánh giá việc huấn luyện SFT nhiệm vụ đa nguồn trên trả lời câu hỏi trích xuất (QA), vì một lượng dữ liệu đa ngôn ngữ tương đối hào phóng có sẵn cho nhiệm vụ này. Cụ thể, chúng tôi huấn luyện trên dữ liệu tiếng Anh từ SQuAD phiên bản 1 (Rajpurkar et al., 2016), tất cả các ngôn ngữ từ MLQA (Lewis et al., 2020), và những ngôn ngữ từ XQuAD (Artetxe et al., 2020) cũng xuất hiện trong MLQA. Chúng tôi đánh giá trên các ngôn ngữ có mặt trong XQuAD nhưng không có trong MLQA. Đối với QA, chúng tôi huấn luyện trong 5 epoch với kích thước batch 12 và tốc độ học ban đầu 3e-5. Chi tiết đầy đủ về các ngôn ngữ nguồn có thể tìm thấy trong Phụ lục B.

Chúng tôi sử dụng hệ số giảm tương đương 1 cho tất cả các nhiệm vụ, theo thiết lập mạnh nhất từ các thí nghiệm nguồn đơn của chúng tôi. Ngoại trừ như đã nêu ở trên, cấu hình huấn luyện và siêu tham số giống như đối với huấn luyện nguồn đơn.

5 Kết quả và Thảo luận

Chúng tôi báo cáo hiệu suất kiểm tra trung bình của chuyển giao đa ngôn ngữ zero-shot cho hệ số giảm tốt nhất (hoặc K tương đương) trong Bảng 2. Một số mẫu xuất hiện trên tất cả bốn nhiệm vụ: đầu tiên, LT-SFT luôn luôn vượt trội so với tất cả các baseline. Cụ thể, nó vượt qua MAD-X tiên tiến trên tất cả các nhiệm vụ, với mức tăng 2.5 độ chính xác trong gắn thẻ từ loại, 2.5 UAS và 3.7 LAS trong phân tích cú pháp phụ thuộc, 1.8 điểm F1 trong nhận dạng thực thể có tên, và 1.9 độ chính xác trong suy luận ngôn ngữ tự nhiên. So với RAND-SFT, hiệu suất vượt trội của nó chứng minh tầm quan trọng của việc chọn "vé số thắng" thay vì một tập con tham số ngẫu nhiên. Thứ hai, các kết quả chứng minh tầm quan trọng của các SFT/adapter ngôn ngữ để chuyên hóa các mô hình được tiền huấn luyện cho các ngôn ngữ chưa được nhìn thấy, vì chúng mang lại sự gia tăng lớn về hiệu suất trên 4 nhiệm vụ so với các thiết lập CHỈ TA tương ứng.

--- TRANG 7 ---
POS DP NER NLI
Độ chính xác UAS LAS Điểm F1 Độ chính xác

LT-SFT 71.1 (1) 57.1 (1) 37.8 (1) 71.7 (1) 51.4 (1)
RAND-SFT 69.2 (1) 54.3 (1) 33.9 (1) - -
MAD-X 68.6 (16) 54.6 (2) 34.1 (1) 69.9 (8) 49.5 (2)
BITFIT 58.1 45.7 23.9 54.9 38.3
LT-SFT TA-ONLY 51.3 (32) 39.1 (1) 19.9 (1) 55.3 (8) 39.9 (4)
MAD-X TA-ONLY 52.1 (32) 38.9 (1) 19.5 (1) 52.4 (32) 41.7 (4)

Bảng 2: Kết quả đánh giá chuyển giao đa ngôn ngữ zero-shot được tính trung bình trên tất cả các ngôn ngữ khi hệ số giảm tương đương tốt nhất (được hiển thị trong ngoặc đơn sau mỗi kết quả) được chọn.

[Các biểu đồ từ 32 đến 1 hiển thị hiệu suất của LT-SFT, MAD-X, và RAND-SFT trên các nhiệm vụ khác nhau]

Hình 2: Đánh giá chuyển giao đa ngôn ngữ zero-shot của Tinh chỉnh thưa thớt Vé số (LT-SFT), Tinh chỉnh thưa thớt Ngẫu nhiên (RAND-SFT), và MAD-X dựa trên adapter trên bốn nhiệm vụ với số lượng tham số có thể huấn luyện khác nhau trong quá trình thích ứng nhiệm vụ. Kết quả là trung bình trên tất cả các ngôn ngữ đích.

Chúng tôi nhận xét rằng hiệu suất zero-shot của LT-SFT cũng vượt qua các baseline dựa trên dịch thuật trên nhiệm vụ AmericasNLI, đạt độ chính xác trung bình 51.4%, so với 48.7% của baseline 'translate-train' của Ebrahimi et al. (2021).

Trong Hình 2, chúng tôi cung cấp một cái nhìn tổng quan chi tiết hơn về hiệu suất mô hình đa ngôn ngữ trung bình trên một loạt các hệ số giảm khác nhau. Kết quả cho các phương pháp LT-SFT và RAND-SFT thường cải thiện hoặc ổn định khi số lượng tham số nhiệm vụ có thể huấn luyện tăng lên. Ngược lại, không có xu hướng như vậy đối với MAD-X, vì các hệ số giảm thấp hơn có thể làm giảm kết quả của nó. Điều này làm cho việc chọn một thiết lập tốt cho siêu tham số này dễ dàng hơn khi sử dụng SFT. Hơn nữa, đáng nhấn mạnh lại rằng, trái ngược với MAD-X, siêu tham số này không ảnh hưởng đến thời gian suy luận.

BITFIT hoạt động tệ hơn nhiều so với các phương pháp khác thực hiện thích ứng ngôn ngữ trên tất cả các nhiệm vụ. Ghi nhớ xu hướng mạnh mẽ hướng tới việc tăng hiệu suất với việc tăng K cho các phương pháp SFT khác, có vẻ như BITFIT, với hai bậc độ lớn ít hơn các tham số có thể huấn luyện, thiếu khả năng để học các SFT nhiệm vụ và ngôn ngữ hiệu quả.

Để có kết quả bổ sung ở mức độ các ngôn ngữ cá nhân và phân tích hiệu quả của thích ứng ngôn ngữ cho các ngôn ngữ đích có tài nguyên cao so với thấp, chúng tôi tham khảo độc giả đến Phụ lục C.

--- TRANG 8 ---
el ro ru th tr
XLM-R Base, full FT 71.1/54.3 78.3/63.7 74.1/57.8 67.1/55.7 67.5/51.1
XLM-R Large, full FT (Artetxe et al., 2020) 79.8/61.7 83.6/69.7 80.1/64.3 74.2/62.8 75.9/59.3
XLM-R Base MS, LT-SFT 81.9/65.5 86.3/73.3 81.4/64.6 82.4/75.2 75.2/58.6

Bảng 3: Kết quả đánh giá chuyển giao đa ngôn ngữ zero-shot trên XQuAD (Artetxe et al., 2020), giới hạn ở các ngôn ngữ không xuất hiện trong MLQA (Lewis et al., 2020) (xem §4.4) theo định dạng F1/điểm khớp chính xác. "Full FT" biểu thị tinh chỉnh đầy đủ, MS biểu thị huấn luyện đa nguồn, nơi dữ liệu bổ sung từ MLQA và XQuAD được sử dụng, LT-SFT biểu thị Tinh chỉnh thưa thớt Vé số.

[Biểu đồ nhiệt hiển thị hiệu suất với các mức độ thưa thớt khác nhau]

Hình 3: Hiệu suất của LT-SFT trên DP và NER điều khiển độ thưa thớt của tinh chỉnh nhiệm vụ và ngôn ngữ. Kết quả được tính trung bình trên một số ngôn ngữ được chọn. Các tinh chỉnh dày đặc hơn có thể gây nhiễu lẫn nhau và do đó làm giảm hiệu suất mô hình.

DP UAS DP LAS NLI Độ chính xác
NGUỒN ĐƠN 57.1 37.8 51.4
ĐA NGUỒN 64.3 47.6 53.1

Bảng 4: Kết quả đánh giá chuyển giao đa ngôn ngữ zero-shot của huấn luyện nhiệm vụ LT-SFT nguồn đơn so với đa nguồn được tính trung bình trên tất cả các ngôn ngữ đích.

5.1 Huấn luyện Đa nguồn

Như được hiển thị trong Bảng 4, huấn luyện LT-SFT đa nguồn mang lại sự cải thiện lớn trong hiệu suất chuyển giao đa ngôn ngữ zero-shot trên DP, và một cải thiện khiêm tốn cho NLI. Điều này có thể là kết quả của việc tập huấn luyện cho NLI chứa một số lượng tương đối nhỏ các ví dụ không phải tiếng Anh so với tập huấn luyện DP. Ngoài ra, các ngôn ngữ đích AmericasNLI thường có mức độ liên quan phả hệ thấp hơn với các ngôn ngữ nguồn so với các ngôn ngữ đích DP.

Bảng 3 chứng minh rằng huấn luyện đa nguồn cũng có lợi cho chuyển giao đa ngôn ngữ zero-shot cho QA trên một loạt các ngôn ngữ tương đối có nhiều tài nguyên. Cụ thể, huấn luyện đa nguồn LT-SFT của XLM-R Base vượt trội so với tinh chỉnh đầy đủ nguồn đơn của XLM-R Large (một mô hình lớn hơn) một cách thoải mái, và vượt trội so với tinh chỉnh đầy đủ nguồn đơn XLM-R Base với một biên độ đáng kể. Việc một sự cải thiện như vậy xảy ra mặc dù mỗi trong 6 ngôn ngữ nguồn không phải tiếng Anh có ít hơn một bậc độ lớn dữ liệu huấn luyện so với dữ liệu tiếng Anh từ SQuAD minh họa lợi thế không cân xứng của dữ liệu nguồn đa ngôn ngữ.

5.2 Lợi ích của Độ thưa thớt

Cuối cùng, chúng tôi giải quyết câu hỏi sau: độ thưa thớt có chịu trách nhiệm ngăn chặn nhiễu của các tinh chỉnh riêng biệt khi chúng được kết hợp không? Để hỗ trợ giả thuyết này với bằng chứng thực nghiệm, chúng tôi sử dụng LT-SFT để huấn luyện các tinh chỉnh ngôn ngữ⁵ và nhiệm vụ với các mức độ mật độ khác nhau, tức là tỷ lệ phần trăm các giá trị khác không (từ 5% đến 100%). Sau đó chúng tôi đánh giá tất cả các kết hợp có thể của các mức độ mật độ. Kết quả được hiển thị dưới dạng biểu đồ đường viền trong Hình 3 cho các kết hợp được chọn của nhiệm vụ và ngôn ngữ: Buryat, Quảng Đông, Erzya, Malta, và Thượng Sorbian cho DP, và Hausa, Igbo, Luganda, Swahili và Wolof cho NER.

Từ Hình 3, rõ ràng rằng hiệu suất giảm đáng kể đối với các SFT với mức mật độ lớn hơn ~30% tham số được tinh chỉnh.⁶ Chúng tôi suy đoán rằng điều này là do các tinh chỉnh thưa thớt hơn có nguy cơ thấp hơn trong việc chồng chéo với nhau, do đó tạo ra nhiễu giữa các khía cạnh khác nhau của kiến thức mà chúng đóng gói. Tuy nhiên, phải lưu ý rằng các giả thuyết thay thế có thể giải thích sự suy giảm hiệu suất ngoài sự chồng chéo tham số, chẳng hạn như overfitting do khả năng quá mức. Trong khi chúng tôi để việc tìm kiếm bằng chứng thuyết phục cho công trình tương lai, cả hai giả thuyết này minh họa tại sao việc thực thi độ thưa thớt trong thích ứng, như chúng tôi đề xuất trong phương pháp của mình, rất quan trọng để đạt được tính mô-đun.

⁵Để giảm chi phí tính toán, chúng tôi huấn luyện các tinh chỉnh ngôn ngữ tối đa 30K bước thay vì 100K của các thí nghiệm chính.

⁶Lưu ý, hơn nữa, rằng các mức độ mật độ tinh chỉnh nhiệm vụ lớn hơn ~60% không khác nhau về hiệu suất. Điều này là do các tập con tham số của chúng bao gồm các embedding của các token chưa bao giờ gặp phải trong quá trình huấn luyện nhiệm vụ, do đó

--- TRANG 9 ---
chúng không được cập nhật ngay cả khi có thể huấn luyện được.

6 Công trình Liên quan

Trong khung của Giả thuyết Vé số, một loạt các cải tiến đã được đề xuất để làm cho thuật toán gốc tìm vé số thắng (Frankle và Carbin, 2019) ổn định hơn: sau khi tinh chỉnh, Frankle et al. (2019) đưa các tham số trở về giá trị của chúng sau một vài lần lặp thay vì giá trị của chúng trước khi huấn luyện, trong khi Renda et al. (2020) cũng đưa tốc độ học trở lại. Ngoài ra, Zhou et al. (2019) thấy rằng 1) các tiêu chí khác nhau có thể được sử dụng để chọn trọng số như một thay thế cho độ lớn của sự thay đổi của chúng; 2) các phương pháp đưa về khác nhau cũng hiệu quả, chẳng hạn như khôi phục dấu gốc, nhưng không phải giá trị. Trong công trình tương lai, chúng tôi sẽ nghiên cứu liệu những biến thể này cũng có lợi cho phương pháp của chúng tôi cho chuyển giao đa ngôn ngữ, nơi LTH được sử dụng để thích ứng thay vì cắt tỉa.

Trong khi LTH ban đầu được hình thành trong lĩnh vực thị giác cho các kiến trúc tích chập, nó cũng hiệu quả để cắt tỉa các mô hình được huấn luyện trên các nhiệm vụ NLP (Yu et al., 2020), chẳng hạn như dịch máy thần kinh, và dựa trên các kiến trúc Transformer (Prasanna et al., 2020). Gần đây, Xu et al. (2021a) đã thích ứng LTH cụ thể để cắt tỉa các mô hình được tiền huấn luyện sau khi tinh chỉnh.

Theo hiểu biết tốt nhất của chúng tôi, Wortsman et al. (2020) là trường hợp duy nhất mà các vé số thắng được kết hợp trong công trình trước đây. Trong thí nghiệm của họ, một tập hợp các mặt nạ đặc thù nhiệm vụ được kết hợp tuyến tính tại thời điểm suy luận, để khái quát hóa đến các nhiệm vụ mới trong một môi trường học liên tục.

7 Kết luận và Công trình Tương lai

Chúng tôi đã trình bày một phương pháp mới để tinh chỉnh các mô hình được tiền huấn luyện vừa có tính mô-đun (như adapter) vừa có tính biểu đạt (như tinh chỉnh thưa thớt). Phương pháp này dựa trên một biến thể của thuật toán tìm vé số thắng trong khung của Giả thuyết Vé số. Chúng tôi suy ra một vector khác biệt thưa thớt đối với mô hình gốc cho mỗi ngôn ngữ cá nhân (bằng cách mô hình hóa văn bản không có nhãn) và mỗi nhiệm vụ cá nhân (với học có giám sát). Các thích ứng cho một ngôn ngữ và một nhiệm vụ sau đó có thể được kết hợp với mô hình được tiền huấn luyện để cho phép chuyển giao đa ngôn ngữ zero-shot. So sánh phương pháp của chúng tôi với baseline tiên tiến trong một số nhiệm vụ đa ngôn ngữ, các kết quả đã chỉ ra những cải thiện đáng kể trên toàn bộ trong cả các ngôn ngữ được nhìn thấy và không được nhìn thấy trong quá trình tiền huấn luyện (bao gồm nhiều ngôn ngữ thực sự ít tài nguyên).

Trong công trình tương lai, phương pháp của chúng tôi cung cấp một số khả năng mở rộng. Ngoài các biến thể của thuật toán Vé số được khảo sát trong §6, với tầm quan trọng của độ thưa thớt đối với tính mô-đun (§5.2), chúng tôi dự định thử nghiệm với các thuật toán bổ sung trước đây được áp dụng cho cắt tỉa có thể xác định và tinh chỉnh một tập con các tham số mô hình, chẳng hạn như DiffPruning (Guo et al., 2021) và ChildTuning (Xu et al., 2021b). Cuối cùng, với tính đơn giản và tổng quát của nó, phương pháp của chúng tôi phù hợp cho nhiều ứng dụng khác của học chuyển giao ngoài chuyển giao đa ngôn ngữ, chẳng hạn như học đa phương thức, giảm thiên lệch, và thích ứng miền. Mã và mô hình có sẵn trực tuyến tại https://github.com/cambridgeltl/composable-sft.

Lời cảm ơn

Alan muốn cảm ơn David và Claudia Harding vì sự hỗ trợ hào phóng của họ thông qua Chương trình Học bổng Sau đại học Xuất sắc Harding. Anna và Ivan được hỗ trợ bởi Tài trợ ERC PoC MultiConvAI (số 957356) và một khoản đóng góp nghiên cứu Huawei. Chúng tôi muốn cảm ơn Chiara Ponti về hình minh họa đồ họa. Chúng tôi cũng cảm ơn các nhà bình duyệt ẩn danh vì những gợi ý hữu ích của họ.

Tài liệu tham khảo

David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki,

--- TRANG 10 ---
Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, và Salomey Osei. 2021. MasakhaNER: Nhận dạng Thực thể Có tên cho các Ngôn ngữ Châu Phi. arXiv preprint.

Željko Agić và Ivan Vulić. 2019. JW300: Một kho ngữ liệu song song có phạm vi phủ rộng cho các ngôn ngữ ít tài nguyên. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 3204–3210, Florence, Italy. Association for Computational Linguistics.

Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš, Ivan Vulić, và Anna Korhonen. 2021. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, trang 4762–4781, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Mikel Artetxe, Sebastian Ruder, và Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 4623–4637, Online. Association for Computational Linguistics.

Ankur Bapna và Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 1538–1548, Hong Kong, China. Association for Computational Linguistics.

David Brambila. 1976. Diccionario Raramuri-Castellano: Tarahumar.

Gina Bustamante, Arturo Oncevay, và Roberto Zariquiey. 2020. No data to crawl? monolingual corpus creation from PDF files of truly low-resource languages in Peru. Trong Proceedings of the 12th Language Resources and Evaluation Conference, trang 2914–2923, Marseille, France. European Language Resources Association.

Luis Chiruzzo, Pedro Amarilla, Adolfo Ríos, và Gustavo Giménez Lugo. 2020. Development of a Guarani - Spanish parallel corpus. Trong Proceedings of the 12th Language Resources and Evaluation Conference, trang 2629–2633, Marseille, France. European Language Resources Association.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 8440–8451, Online. Association for Computational Linguistics.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, và Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, trang 2475–2485, Brussels, Belgium. Association for Computational Linguistics.

Rubén Cushimariano Romano và Richer C. Sebastián Q. 2008. Ñaantsipeta asháninkaki birakochaki. diccionario asháninka-castellano. versión preliminar. http://www.lengamer.org/publicaciones/diccionarios/.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Timothy Dozat và Christopher D. Manning. 2017. Deep biaffine attention for neural dependency parsing. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.

Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir, Gustavo A. Giménez-Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer, Rolando A. Coto Solano, Ngoc Thang Vu, và Katharina Kann. 2021. AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages.

Isaac Feldman và Rolando Coto-Solano. 2020. Neural machine translation models with back-translation for the extremely low-resource indigenous language Bribri. Trong Proceedings of the 28th International Conference on Computational Linguistics, trang 3965–3976, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Jonathan Frankle và Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations.

--- TRANG 11 ---
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, và Michael Carbin. 2019. Stabilizing the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611.

Ana-Paula Galarreta, Andrés Melgar, và Arturo Oncevay. 2017. Corpus creation and initial SMT experiments between Spanish and Shipibo-konibo. Trong Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, trang 238–244, Varna, Bulgaria. INCOMA Ltd.

Goran Glavaš và Ivan Vulić. 2021. Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, trang 3090–3104, Online. Association for Computational Linguistics.

Demi Guo, Alexander Rush, và Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 4884–4896, Online. Association for Computational Linguistics.

Ximena Gutierrez-Vasques, Gerardo Sierra, và Isaac Hernandez Pompa. 2016. Axolotl: a web accessible parallel corpus for Spanish-Nahuatl. Trong Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), trang 4210–4214, Portorož, Slovenia. European Language Resources Association (ELRA).

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. Trong Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, trang 2790–2799. PMLR.

Jeremy Howard và Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 328–339, Melbourne, Australia. Association for Computational Linguistics.

Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, và Holger Schwenk. 2020. MLQA: Evaluating cross-lingual extractive question answering. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 7315–7330, Online. Association for Computational Linguistics.

Ilya Loshchilov và Frank Hutter. 2019. Decoupled weight decay regularization. Trong International Conference on Learning Representations.

Manuel Mager, Diónico Carrillo, và Ivan Meza. 2018. Probabilistic finite-state morphological segmenter for wixarika (huichol) language. Journal of Intelligent & Fuzzy Systems, 34(5):3081–3087.

Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, và Ohad Shamir. 2020. Proving the lottery ticket hypothesis: Pruning is all you need. Trong Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, trang 6682–6691. PMLR.

Elena Mihas. 2011. Añaani katonkosatzi parenini, El idioma del alto Perené. Milwaukee, WI: Clarks Graphics.

John E Ortega, Richard Alexander Castro-Mamani, và Jaime Rafael Montoya Samame. 2020. Overcoming resistance: The normalization of an Amazonian tribal language. Trong Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, trang 1–13, Suzhou, China. Association for Computational Linguistics.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. 2021a. AdapterFusion: Non-destructive task composition for transfer learning. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, trang 487–503, Online. Association for Computational Linguistics.

Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, và Iryna Gurevych. 2020a. AdapterHub: A framework for adapting transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 46–54, Online. Association for Computational Linguistics.

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, và Sebastian Ruder. 2020b. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 7654–7673, Online. Association for Computational Linguistics.

Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, và Sebastian Ruder. 2021b. UNKs everywhere: Adapting multilingual language models to new scripts. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP).

Edoardo Ponti, Ivan Vulić, Ryan Cotterell, Marinela Parovic, Roi Reichart, và Anna Korhonen. 2021. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9(0):410–428.

Edoardo Maria Ponti. 2021. Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning. Ph.D. thesis, University of Cambridge.

--- TRANG 12 ---
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, và Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 2362–2376, Online. Association for Computational Linguistics.

Edoardo Maria Ponti, Helen O'Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, và Anna Korhonen. 2019. Modeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics, 45(3):559–601.

Sai Prasanna, Anna Rogers, và Anna Rumshisky. 2020. When BERT Plays the Lottery, All Tickets Are Winning. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 3208–3229, Online. Association for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. Trong Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, trang 2383–2392, Austin, Texas. Association for Computational Linguistics.

Sylvestre-Alvise Rebuffi, Hakan Bilen, và Andrea Vedaldi. 2017. Learning Multiple Visual Domains with Residual Adapters. Trong Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Alex Renda, Jonathan Frankle, và Michael Carbin. 2020. Comparing rewinding and fine-tuning in neural network pruning. Trong International Conference on Learning Representations.

Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. Trong Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), trang 2214–2218, Istanbul, Turkey. European Language Resources Association (ELRA).

Erik F. Tjong Kim Sang và Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Trong Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, trang 142–147.

Ahmet Üstün, Arianna Bisazza, Gosse Bouma, và Gertjan van Noord. 2020. UDapter: Language adaptation for truly Universal Dependency parsing. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 2302–2315.

Marko Vidoni, Ivan Vulić, và Goran Glavaš. 2020. Orthogonal language and task adapters in zero-shot cross-lingual transfer. CoRR, abs/2012.06460.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. Trong International Conference on Learning Representations.

Zirui Wang, Zachary C. Lipton, và Yulia Tsvetkov. 2020. On negative interference in multilingual models: Findings and a meta-learning treatment. Trong Proceedings of EMNLP 2020, trang 4438–4450.

Adina Williams, Nikita Nangia, và Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. Trong Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), trang 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics.

Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, và Ali Farhadi. 2020. Supermasks in superposition. Trong Advances in Neural Information Processing Systems, volume 33, trang 15173–15184. Curran Associates, Inc.

Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, và Zhibin Xiao. 2021a. Rethinking network pruning – under the pre-train and fine-tune paradigm. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 2376–2382, Online. Association for Computational Linguistics.

Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, và Fei Huang. 2021b. Raise a child in large language model: Towards effective and generalizable fine-tuning. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 9514–9528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Haonan Yu, Sergey Edunov, Yuandong Tian, và Ari S. Morcos. 2020. Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp. Trong International Conference on Learning Representations.

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. 2021. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. CoRR, abs/2106.10199.

Daniel Zeman, Joakim Nivre, Mitchell Abrams, Elia Ackermann, Noëmi Aepli, Hamid Aghaei, Željko Agić, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy Ajede, Gabrielė Aleksandravičiūtė, Ika Alfina, Lene Antonsen, Katya Aplonova, Angelina Aquino, Carolina Aragon, Maria Jesus Aranzabe, Hórunn Arnardóttir, Gashaw Arutie, Jessica Naraiswari Arwidarasti, Masayuki Asahara, Luma Ateyah, Furkan Atmaca, Mohammed Attia,

--- TRANG 13 ---
Aitziber Atutxa, Liesbeth Augustinus, Elena Badmaeva, Keerthana Balasubramani, Miguel Ballesteros, Esha Banerjee, Sebastian Bank, Verginica Barbu Mititelu, Victoria Basmov, Colin Batchelor, John Bauer, Seyyit Talha Bedir, Kepa Bengoetxea, Gözde Berk, Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eckhard Bick, Agnė Bielinskienė, Kristín Bjarnadóttir, Rogier Blokland, Victoria Bobicev, Loïc Boizou, Emanuel Borges Völker, Carl Börstell, Cristina Bosco, Gosse Bouma, Sam Bowman, Adriane Boyd, Kristina Brokaitė, Aljoscha Burchardt, Marie Candito, Bernard Caron, Gauthier Caron, Tatiana Cavalcanti, Gülşen Cebiroğlu Eryiğit, Flavio Massimiliano Cecchini, Giuseppe G. A. Celano, Slavomír Čéplö, Savas Cetin, Özlem Çetinoğlu, Fabricio Chalub, Ethan Chi, Yongseok Cho, Jinho Choi, Jayeol Chun, Alessandra T. Cignarella, Silvie Cinková, Aurélie Collomb, Çağrı Çöltekin, Miriam Connor, Marine Courtin, Elizabeth Davidson, Marie-Catherine de Marneffe, Valeria de Paiva, Mehmet Oguz Derin, Elvis de Souza, Arantza Diaz de Ilarraza, Carly Dickerson, Arawinda Dinakaramani, Bamba Dione, Peter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira Droganova, Puneet Dwivedi, Hanne Eckhoff, Marhaba Eli, Ali Elkahky, Binyam Ephrem, Olga Erina, Tomaž Erjavec, Aline Etienne, Wograine Evelyn, Sidney Facundes, Richárd Farkas, Marília Fernanda, Hector Fernandez Alcalde, Jennifer Foster, Cláudia Freitas, Kazunori Fujita, Katarína Gajdošová, Daniel Galbraith, Marcos Garcia, Moa Gärdenfors, Sebastian Garza, Fabrício Ferraz Gerardi, Kim Gerdes, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Memduh Gökırmak, Yoav Goldberg, Xavier Gómez Guinovart, Berta González Saavedra, Bernadeta Griciūtė, Matias Grioni, Loïc Grobol, Normunds Grūzītis, Bruno Guillaume, Céline Guillot-Barbance, Tunga Güngör, Nizar Habash, Hinrik Hafsteinsson, Jan Hajič, Jan Hajič jr., Mika Hämäläinen, Linh Hà Mỹ, Na-Rae Han, Muhammad Yudistira Hanifmuti, Sam Hardwick, Kim Harris, Dag Haug, Johannes Heinecke, Oliver Hellwig, Felix Hennig, Barbora Hladká, Jaroslava Hlaváčová, Florinel Hociung, Petter Hohle, Eva Huber, Jena Hwang, Takumi Ikeda, Anton Karl Ingason, Radu Ion, Elena Irimia, Ọlájídé Ishola, Tomáš Jelínek, Anders Johannsen, Hildur Jónsdóttir, Fredrik Jørgensen, Markus Juutinen, Sarveswaran K, Hüner Kaşıkara, Andre Kaasen, Nadezhda Kabaeva, Sylvain Kahane, Hiroshi Kanayama, Jenna Kanerva, Boris Katz, Tolga Kayadelen, Jessica Kenney, Václava Kettnerová, Jesse Kirchner, Elena Klementieva, Arne Köhn, Abdullatif Köksal, Kamil Kopacewicz, Timo Korkiakangas, Natalia Kotsyba, Jolanta Kovalevskaitė, Simon Krek, Parameswari Krishnamurthy, Sookyoung Kwak, Veronika Laippala, Lucia Lam, Lorenzo Lambertino, Tatiana Lando, Septina Dian Larasati, Alexei Lavrentiev, John Lee, Phương Lê Hồng, Alessandro Lenci, Saran Lertpradit, Herman Leung, Maria Levina, Cheuk Ying Li, Josie Li, Keying Li, Yuan Li, KyungTae Lim, Krister Lindén, Nikola Ljubešić, Olga Loginova, Andry Luthfi, Mikko Luukko, Olga Lyashevskaya, Teresa Lynn, Vivien Macketanz, Aibek Makazhanov, Michael Mandl, Christopher Manning, Ruli Manurung, Cătălina Mărănduc, David Mareček, Katrin Marheinecke, Héctor Martínez Alonso, André Martins, Jan Mašek, Hiroshi Matsuda, Yuji Matsumoto, Ryan McDonald, Sarah McGuinness, Gustavo Mendonça, Niko Miekka, Karina Mischenkova, Margarita Misirpashayeva, Anna Missilä, Cătălin Mititelu, Maria Mitrofan, Yusuke Miyao, AmirHossein Mojiri Foroushani, Amirsaeid Moloodi, Simonetta Montemagni, Amir More, Laura Moreno Romero, Keiko Sophie Mori, Shinsuke Mori, Tomohiko Morioka, Shigeki Moro, Bjartur Mortensen, Bohdan Moskalevskyi, Kadri Muischnek, Robert Munro, Yugo Murawaki, Kaili Müürisep, Pinkey Nainwani, Mariam Nakhlé, Juan Ignacio Navarro Horñiacek, Anna Nedoluzhko, Gunta Nešpore-Bērzkalne, Lượng Nguyễn Thi., Huyền Nguyễn Thi.Minh, Yoshihiro Nikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza Nourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha, Adédayọ̀ Olúòkun, Mai Omura, Emeka Onwuegbuzia, Petya Osenova, Robert Östling, Lilja Øvrelid, Şaziye Betül Özateş, Arzucan Özgür, Balkız Öztürk Başaran, Niko Partanen, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme Paulino-Passos, Angelika Peljak-Łapińska, Siyao Peng, Cenel-Augusto Perez, Natalia Perkova, Guy Perrier, Slav Petrov, Daria Petrova, Jason Phelan, Jussi Piitulainen, Tommi A Pirinen, Emily Pitler, Barbara Plank, Thierry Poibeau, Larisa Ponomareva, Martin Popel, Lauma Pretkalniņa, Sophie Prévost, Prokopis Prokopidis, Adam Przepiórkowski, Tiina Puolakainen, Sampo Pyysalo, Peng Qi, Andriela Rääbis, Alexandre Rademaker, Taraka Rama, Loganathan Ramasamy, Carlos Ramisch, Fam Rashel, Mohammad Sadegh Rasooli, Vinit Ravishankar, Livy Real, Petru Rebeja, Siva Reddy, Georg Rehm, Ivan Riabov, Michael Rießler, Erika Rimkutė, Larissa Rinaldi, Laura Rituma, Luisa Rocha, Eiríkur Rögnvaldsson, Mykhailo Romanenko, Rudolf Rosa, Valentin Roșca, Davide Rovati, Olga Rudina, Jack Rueter, Kristján Rúnarsson, Shoval Sadde, Pegah Safari, Benoît Sagot, Aleksi Sahala, Shadi Saleh, Alessio Salomoni, Tanja Samardžić, Stephanie Samson, Manuela Sanguinetti, Dage Särg, Baiba Saulīte, Yanin Sawanakunanon, Kevin Scannell, Salvatore Scarlata, Nathan Schneider, Sebastian Schuster, Djamé Seddah, Wolfgang Seeker, Mojgan Seraji, Mo Shen, Atsuko Shimada, Hiroyuki Shirasu, Muh Shohibussirri, Dmitry Sichinava, Einar Freyr Sigurðsson, Aline Silveira, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Mária Šimková, Kiril Simov, Maria Skachedubova, Aaron Smith, Isabela Soares-Bastos, Carolyn Spadine, Steinþór Steingrímsson, Antonio Stella, Milan Straka, Emmett Strickland, Jana Strnadová, Alane Suhr, Yogi Lesmana Sulestio, Umut Sulubacak, Shingo Suzuki, Zsolt Szántó, Dima Taji, Yuta Takahashi, Fabio Tamburini, Mary Ann C. Tan, Takaaki Tanaka, Samson Tella, Isabelle Tellier, Guillaume Thomas, Liisi Torga, Marsida Toska, Trond Trosterud, Anna Trukhina, Reut Tsarfaty, Utku Türk, Francis Tyers, Sumire Uematsu, Roman Untilov, Zdeňka Urešová, Larraitz Uria, Hans Uszkoreit, Andrius Utka, Sowmya Vajjala, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Eric Villemonte de la Clergerie, Veronika Vincze, Aya Wakasa, Joel C. Wallenberg, Lars Wallin, Abigail Walsh, Jing Xian Wang, Jonathan North Washington, Maximilan Wendt, Paul Widmer, Seyi Williams, Mats Wirén, Christian Wittern, Tsegay Woldemariam, Tak-sum Wong, Alina Wróblewska, Mary Yako, Kayo Yamashita, Naoki Yamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M. Yavrumyan, Zhuoran Yu, Zdeněk Žabokrtský, Shorouq Zahra, Amir Zeldes, Hanzhi Zhu, và Anna Zhuravleva. 2020. Universal Dependencies 2.7. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University.

Hattie Zhou, Janice Lan, Rosanne Liu, và Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs, and the supermask. Trong Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.

--- TRANG 15 ---
A Thuật toán Chuyển giao Đa ngôn ngữ với LT-SFT

Thuật toán 1 Chuyển giao Đa ngôn ngữ với Tinh chỉnh thưa thớt Vé số
function LTSFT(D,L,θ(0),λ,K)
    θ(1) ← θ(0)
    while not converged do
        θ(1) ← θ(1) - α∇θL(θ(1);D)
    
    for i = 1 to |θ| do
        mi ← {1 nếu θ(1)i ∈ argmax1,…,K|θ(1) - θ(0)|
              0 nếu khác
    
    θ(2) ← θ(0)
    while not converged do
        θ(2) ← θ(2) - α∇θL(θ(2);D) ⊙ m
    
    δ ← θ(2) - θ(0)
    return δ
end function

function CROSS_LINGUAL_TRANSFER(Dsrc,Dtar,Dtask,Ltask,θ(0),λ,K)
    δsrc ← LTSFT(Dsrc,LMLM,θ(0),λ,K)
    δtask ← LTSFT(Dtask,Ltask,θ(0)+δsrc,λ,K)
    δtar ← LTSFT(Dtar,LMLM,θ(0),λ,K)
    return θ(0) + δtask + δtar
end function

--- TRANG 16 ---
B Ngôn ngữ

[THIS IS TABLE: A detailed table showing language information with columns for Task, Language, ISO Code, Family, UD Treebank, and Corpus source(s). The table includes information for source languages and various target languages for POS/DP, NER, NLI, and QA tasks, with their respective language families and data sources.]

Bảng 5: Chi tiết về các ngôn ngữ và dữ liệu được sử dụng để huấn luyện và đánh giá SFT và adapter. Các kho ngữ liệu của Bustamante et al. (2020) có sẵn tại https://github.com/iapucp/multilingual-data-peru; tất cả các kho ngữ liệu NLI khác được đề cập có sẵn tại https://github.com/AmericasNLP/americasnlp2021. † biểu thị các ngôn ngữ nguồn cho huấn luyện DP đa nguồn; ‡ biểu thị các ngôn ngữ nguồn cho huấn luyện NLI đa nguồn; § biểu thị các ngôn ngữ nguồn cho huấn luyện QA đa nguồn. Tiếng Anh là ngôn ngữ nguồn trong tất cả các thí nghiệm huấn luyện nhiệm vụ nguồn đơn.

--- TRANG 17 ---
C Kết quả theo Ngôn ngữ

[THIS IS TABLE: Multiple tables showing results for different language processing tasks (POS, DP, NER, NLI) with various metrics and methods (LT-SFT, RAND-SFT, MAD-X, etc.) across different languages. The tables include accuracy scores, UAS/LAS scores, and F1 scores.]

Bảng 6: Kết quả đạt được bởi các phương pháp chuyển giao đa ngôn ngữ zero-shot khác nhau trên tất cả các nhiệm vụ cho mỗi ngôn ngữ. Đối với mỗi cặp (phương pháp, nhiệm vụ), hệ số giảm tương đương với điểm trung bình tốt nhất được chọn như được hiển thị trong Bảng 2. LT-SFT MS biểu thị LT-SFT với huấn luyện đa nguồn. In đậm biểu thị phương pháp hoạt động tốt nhất mỗi ngôn ngữ, loại trừ LT-SFT MS vì tập dữ liệu lớn hơn, đa dạng hơn của nó mang lại lợi thế không công bằng.

[Additional table showing results for seen languages]

Bảng 7: Kết quả đánh giá chuyển giao đa ngôn ngữ zero-shot của các ngôn ngữ đã được nhìn thấy bao gồm trong đánh giá POS, DP và NER. Đối với mỗi cặp phương pháp/metric, hệ số giảm tương đương tốt nhất từ Bảng 2 được sử dụng.

Tiếng Ả Rập, tiếng Nhật và tiếng Trung, được bao gồm trong đánh giá POS/DP, có thể được coi là các ngôn ngữ có nhiều tài nguyên; mặt khác, tiếng Swahili và tiếng Yorùbá được bao gồm trong đánh giá NER và có thể nói là nghèo tài nguyên. Phù hợp với công trình trước đây, chúng tôi thấy rằng thích ứng ngôn ngữ có lợi cho các ngôn ngữ đã được nhìn thấy ít hơn các ngôn ngữ chưa được nhìn thấy và—trong số những ngôn ngữ đầu tiên—các ngôn ngữ giàu tài nguyên ít hơn các ngôn ngữ nghèo tài nguyên. Điều này phù hợp với trực giác rằng các ngôn ngữ có ít tài nguyên hơn có phạm vi cải thiện lớn hơn thông qua thích ứng ngôn ngữ do họ nhận được ít tín hiệu hơn trong quá trình tiền huấn luyện MMT. Thú vị là, BITFIT hoạt động cạnh tranh hơn nhiều trên các ngôn ngữ có nhiều tài nguyên so với các ngôn ngữ ít tài nguyên và chưa được nhìn thấy, cho thấy rằng việc thiếu khả năng của nó có vấn đề hơn đối với thích ứng ngôn ngữ hơn là đối với tinh chỉnh nhiệm vụ.

--- TRANG 18 ---
D Kết quả MAD-X với Adapter AdapterHub

[THIS IS FIGURE: Four graphs showing performance comparisons between LT-SFT and MAD-X hub across different reduction factors for various NLP tasks]

Hình 4: Đánh giá chuyển giao đa ngôn ngữ zero-shot của Tinh chỉnh thưa thớt Vé số (LT-SFT) và MAD-X khi các adapter ngôn ngữ được tiền huấn luyện từ AdapterHub (Pfeiffer et al., 2020a) được sử dụng trong quá trình huấn luyện và đánh giá nhiệm vụ. Những adapter này được huấn luyện trong 250.000 bước với kích thước batch 64, trái ngược với 100.000 bước kích thước batch 8 được sử dụng trong các thí nghiệm của chúng tôi. Tuy nhiên, LT-SFT vẫn duy trì lợi thế về hiệu suất trên tất cả các nhiệm vụ. Vì các adapter AdapterHub chỉ có sẵn cho một số ngôn ngữ trong đánh giá của chúng tôi, các kết quả được hiển thị chỉ được tính trung bình trên các ngôn ngữ mà chúng có sẵn, được chỉ ra trong chú thích hình phụ.

E Chồng chéo Tham số giữa các Ngôn ngữ

[THIS IS FIGURE: A heatmap showing parameter overlap percentages between different language pairs]

Hình 5: Tỷ lệ phần trăm tham số được chọn cho tinh chỉnh thưa thớt của cả hai ngôn ngữ trong một cặp.

--- TRANG 19 ---
Để hiểu liệu các ngôn ngữ tương tự cũng chia sẻ các mạng con tương tự, chúng tôi vẽ biểu đồ sự chồng chéo theo cặp (tính theo phần trăm) giữa các tập con tham số của các SFT ngôn ngữ trong Hình 5. Ngoại trừ một trường hợp duy nhất (tiếng Trung quan thoại và tiếng Quảng Đông) trong đó sự chồng chéo cao phản ánh việc cả hai ngôn ngữ đều có liên quan về mặt phả hệ, chúng tôi thấy rằng sự chồng chéo nhỏ đối với hầu hết các cặp ngôn ngữ. Chúng tôi tin rằng lời giải thích là hai mặt. Đầu tiên, hầu hết các ngôn ngữ trong các tập dữ liệu đa ngôn ngữ được xem xét trong các thí nghiệm của chúng tôi thuộc về các chi và họ riêng biệt. Do đó, việc thiếu tương quan trong các tập con tham số là điều được mong đợi. Thứ hai, đối với một mô hình được tiền huấn luyện, tồn tại nhiều tập con tham số ("vé số thắng") với hiệu suất có thể so sánh (Prasanna et al., 2020). Thuật toán Vé số chọn ngẫu nhiên trong số những tập con hợp lệ tương đương này. Do đó, việc thiếu chồng chéo không nhất thiết có nghĩa là sự phụ thuộc vào các mạng con rời rạc.
