# 2203.01104.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2203.01104.pdf
# File size: 1585431 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Parameter-EfÔ¨Åcient Mixture-of-Experts Architecture
for Pre-trained Language Models
Ze-Feng Gao1;4;5, Peiyu Liu1;4, Wayne Xin Zhao1;4y, Zhong-Yi Lu2and Ji-Rong Wen1;3;4
1Gaoling School of ArtiÔ¨Åcial Intelligence, Renmin University of China
2Department of Physics, Renmin University of China
3School of Information, Renmin University of China
4Beijing Key Laboratory of Big Data Management and Analysis Methods
5Beijing Academy of ArtiÔ¨Åcial Intelligence, Beijing, 100084, China
{zfgao,liupeiyustu,zlu,jrwen}@ruc.edu.cn, batmanÔ¨Çy@gmail.com
Abstract
Recently, Mixture-of-Experts (short as MoE)
architecture has achieved remarkable success
in increasing the model capacity of large-
scale language models. However, MoE re-
quires incorporating signiÔ¨Åcantly more param-
eters than the base model being extended. In
this paper, we propose building a parameter-
efÔ¨Åcient MoE architecture by sharing informa-
tion among experts. We adopt matrix product
operator (MPO, a tensor decomposition from
quantum many-body physics) to reconstruct
the parameter matrix in the expert layer and in-
crease model capacity for pre-trained language
models by sharing parameters of the cen-
tral tensor (containing the core information)
among different experts while enabling the
speciÔ¨Åcity through the auxiliary tensors (com-
plementing the central tensor) of different ex-
perts. To address the unbalanced optimiza-
tion issue, we further design the gradient mask
strategy for the MPO-based MoE architecture.
Extensive experiments based on T5 and GPT-2
show improved performance and efÔ¨Åciency of
the pre-trained language model (27.2x reduc-
tion in total parameters for the superior model
performance, compared with the Switch Trans-
formers). Our code is publicly available at
https://github.com/RUCAIBox/MPOE .
1 Introduction
Large-scale pre-trained language models (PLMs),
such as BERT (Devlin et al., 2018) and T5 (Raffel
et al., 2020), have become the de facto standard
in natural language processing (NLP). By involv-
ing a huge number of parameters pre-trained on
the general-purpose corpus, PLMs can achieve ex-
cellent performance in many NLP tasks. In order
to increase the model capacity, a promising direc-
tion is to explore the scaling properties with the
mixture-of-experts (MoE) paradigm (Jacobs et al.,
Authors contributed equally.
yCorresponding author.1991; Shazeer et al., 2017) for developing more
powerful PLMs. By incorporating multiple expert
networks, MoE schedules the learning of data sam-
ples through a routing component that is usually
implemented by some gating function, which in-
creases model capacity without a proportional in-
crease in computation costs. Despite the effective-
ness, it has been shown that the MoE architecture
is parameter inefÔ¨Åcient (Zuo et al., 2021), consid-
ering the yielded improvement w.r.t. the involved
costs. Most of the existing studies (Yang et al.,
2021; Roller et al., 2021; Lewis et al., 2021) at-
tribute this issue to the unbalanced load of experts,
focusing on improving the routing strategies.
However, an important question about the MoE
architecture has been neglected in previous studies:
whether the increased parameters from the experts
are all necessary to increase the model capacity. As
different experts from an MoE network are often
trained with correlated data samples ( e.g., sample
correlation from training data), it is likely to lead to
parameter redundancy across experts. Indeed, ex-
pert redundancy has been identiÔ¨Åed in existing stud-
ies, where Fedus et al. (2021) distills sparse MoE
models into dense models and Kim et al. (2021)
prunes experts to compress MoE models. This Ô¨Ånd-
ing motivates us to develop a parameter-efÔ¨Åcient
MoE architecture by reducing its parameter redun-
dancy. Intuitively, a straightforward approach is
to share a certain proportion of parameters among
experts. However, it is difÔ¨Åcult to identify and op-
timize the key parameters that encode the shared
information across experts, since expert networks
typically consist of dense matrices.
To address this issue, we propose a novel param-
eter sharing approach inspired by the matrix prod-
uct operators (MPO) decomposition from quantum
many-body physics (Gao et al., 2020), which de-
composes a matrix into a sequential product of
local tensors (either central orauxiliary tensors).
Unlike other matrix decomposition methods, MPOarXiv:2203.01104v4  [cs.CL]  10 Oct 2022

--- PAGE 2 ---
can effectively reorganize and aggregate important
information of the original matrix into the central
tensor . The auxiliary tensors, on the other hand,
serve to complement the central tensor for recov-
ering the original matrix (Liu et al., 2021). In the
setting of MoE, considering the small parameter
variations among experts, we speculate that the
central tensors of different experts (with MPO de-
composition for each expert) are likely to be very
similar. If the central tensors could be shared for
all expert networks, we would signiÔ¨Åcantly reduce
the parameters of the MoE architecture.
To this end, we propose a novel MPO -
based parameter-efÔ¨Åcient Mo Earchitecture,
called MPOE . Based on classic MoE architec-
ture (Shazeer et al., 2017), our approach introduces
a major extension allowing experts to share a
global central tensor while keeping expert-speciÔ¨Åc
auxiliary tensors. In our setting, the parameter
matrix of in a single expert is formed by the
product of the globally shared central tensor and
the corresponding auxiliary tensors. Since the
central tensor contains most of the parameters
from an MPO decomposition, our MPOE approach
can signiÔ¨Åcantly reduce the parameters of the
original MoE architecture. Another major merit of
MPO is that auxiliary tensors are closely entangled
with the central tensor (Pirvu et al., 2010), and
it is theoretically guaranteed that any change
from auxiliary tensors can be propagated to the
central tensor. That is to say, though a large
proportion of parameters are shared, local auxiliary
tensors still enable the experts to capture speciÔ¨Åc
variations or differences according to routing
data samples. However, directly optimizing
the MPOE architecture is likely to lead to an
unbalanced optimization issue, where the central
tensors are updated more frequently than auxiliary
tensors during Ô¨Åne-tuning. Therefore, we further
propose a gradient mask strategy that masks the
central tensor gradient to effectively alleviate the
unbalanced optimization issue.
To the best of our knowledge, this is the Ô¨Årst
attempt to reduce the parameter redundancy of the
MoE architecture with structural matrix decompo-
sition. We conduct extensive experiments to evalu-
ate the effectiveness of the MPOE architecture on
two representatives PLMs, T5 and GPT. Experi-
ments have demonstrated the effectiveness of our
approach in increasing model capacity (27.2x fewer
parameters for the superior model performance,compared with several competitive MoE-enhanced
PLMs.
2 Preliminary
2.1 Mixture-of-Experts (MoE)
We Ô¨Årst describe the mixture-of-experts architec-
ture (MoE) (Shazeer et al., 2017), which has been
used to enhance the model capacity of Transformer
based models. Let G(x)andEi(x)denote the out-
put vectors of the gating network and the output
of thei-th expert network for a given input x, re-
spectively. The output of MoE architecture ycan
be formally computed as:
y=nX
i=1G(x)Ei(x): (1)
Thesoftmax function is widely adopted as the
gating function G(x). The sparsely-gated MoE
architecture, which uses a noisy top- kgating mech-
anism to reduce the computational cost, has been
proposed in Shazeer et al. (2017). It adds tunable
Gaussian noise with H(), and then keeps only the
top-kvalues with KeepTopK()and sets the rest
 1. This keeps only the top kexperts to be evalu-
ated with:
G(x) =softmax(KeepTopK (H(x);k)):(2)
Furthermore, Switch Transformer designs a
switch routing strategy to simplify this gating func-
tion by routing to a single expert (Fedus et al.,
2021).
2.2 Tensor and Matrix Product Operators
We refer to one-dimensional arrays as vectors (de-
noted by bold lowercase letters, e.g., v), two-
dimensional arrays as matrices (denoted by bold
uppercase letters, e.g.,W), and arrays of higher di-
mensions as tensors (denoted by calligraphic bold
uppercase letters, e.g.,T).
MPO decomposition (Oseledets, 2011) ( a.k.a.
tensor-train decomposition) has been a widely used
matrix decomposition technique from quantum
many-body physics, which decomposes a matrix ( 2-
order tensor) into mlocal tensors (Pirvu et al.,
2010). Given a matrix WIJ2RIJ, the MPO
decomposition is given in the following format:
MPO (W) =mY
k=1T(k)[dk 1;ik;jk;dk];(3)

--- PAGE 3 ---
ùëæùë∞√óùë±	ùë®ùüè	ùë®ùüê	ùë®ùüë	ùë®ùüí	ùë™MPO%(%)%*%+Figure 1: MPO decomposition for matrix WIJwith
Ô¨Åve local tensors. Auxiliary tensors ( fAig4
i=1) and cen-
tral tensor (C) are marked in orange and blue, respec-
tively.
whereI=Qn
k=1ikandJ=Qn
k=1jk,T(k)is a
4-order tensor with size dk 1ikjkdk. The
dkis dimension of bond linking T(k)andT(k+1).
According to Gao et al. (2020), the original ma-
trixWcan be exactly reconstructed by tensor con-
traction of MPO (W)without truncation of the con-
nection bondfdkgm
k=1. Figure 1 presents the illus-
tration of the MPO decomposition procedure for a
matrix (m= 5). More detailed analysis on differ-
ent factorization ways ( i.e.,m= 3;5;7;9) will be
given in Section 4.5. After MPO decomposition,
the central tensor (the tensor right in the middle)
with most of the parameters can encode the core
information of the original matrix, while the aux-
iliary tensors (the rest of these tensors) with only
a small proportion of parameters play the role of
complementing the central tensor.
3 Approach
To reduce the information redundancy across dif-
ferent experts, we design an MPO-based MoE ar-
chitecture for increasing the model capacity in a
parameter-efÔ¨Åcient way. We Ô¨Årstly describe the
MPO-based MoE architecture and then introduce
an improved optimization algorithm for learning
the parameters in this architecture.
3.1 MPO-based Mixture-of-Experts
Previous MoE architecture (Jacobs et al., 1991;
Shazeer et al., 2017) usually treats different ex-
perts as individual components, requiring a com-
pete copy of network parameters for each expert.
Although it has been found (Fedus et al., 2021; Kim
et al., 2021) that there exists redundant information
among different experts in the MoE architecture,
it is not easy to identify the shareable parameters
from the highly coupling network.
Considering this issue, our solution is inspired
by an important merit of MPO decomposition: it
can reorganize and aggregate the core information
in central tensors (Gao et al., 2020) as aforemen-
tioned. Based on this property, the core idea of
our approach is to share the central tensors for allthe expert layers and enable speciÔ¨Åcity via expert-
speciÔ¨Åc auxiliary tensors.
Parameter-EfÔ¨Åcient MoE Architecture . The
Transformer network consists of two major neu-
ral components, namely FFN and multi-head at-
tention. Following previous work on MoE-based
PLMs (Shazeer et al., 2017; Fedus et al., 2021),
we consider FFN layers as experts to be extended,
while our approach is generally applicable to vari-
ous matrix-based model components. A straightfor-
ward method to reducing information redundancy
is to share a proportion of parameters across ex-
perts. However, in Transformer-based networks,
the experts ( i.e.,FFN here) are mainly composed
of large dense matrices, which are difÔ¨Åcult for shar-
ing partial parameters from these matrices. As our
solution, we consider parameter sharing through
the MPO decomposition, so that the derived central
tensors can be Ô¨Çexibly shared across matrices.
Lightweight MoE Design . SpeciÔ¨Åcally, we sim-
plify the discussion by assuming that an expert
corresponds to one parameter matrix at each layer,
and it is similar for the multi-matrix cases. We
consider a MoE architecture of nexperts each
withLlayers, so that there are Lnmatrices
in total, denoted by fW(l;i)gL;n
l=1;i=1. As discussed
in Section 2.2, a matrix can be decomposed into
mtensors, consisting of one central tensor and
m 1auxiliary tensors. In this work, we con-
sider Ô¨Åve decomposed tensors, i.e.,m= 5. At
thel-th layer, the decomposition results can be
denoted byfC(l;i);A(l;i)
1;A(l;i)
2;A(l;i)
3;A(l;i)
4gn
i=1,
whereC(l;i)andA(l;i)
are the central and auxil-
iary tensors of the i-th parameter matrix, respec-
tively, at the l-th layer. To develop the MPO-based
MoE architecture, the core idea is to share the cen-
tral tensors as global parameters and keep expert-
speciÔ¨Åc auxiliary tensors as local parameters, i.e.,
C(l;1)=C(l;2)=C(l;n)(8l= 1L), and we
denote the global central tensor at the l-th layer by
C(l). In this way, we can only keep Lcentral cen-
sors for aL-layer MoE architecture. For MPO, the
decomposition process is transparent to external
modules, so that we can reuse the previous rout-
ing mechanism (Section 2.1) by distributing data
samples to different experts. A slight difference is
that we only need to consider the routing to local
tensors for each matrix since the global tensor is
shared across experts. We call such an MPO-based
MoE architecture as MPOE .

--- PAGE 4 ---
(a) MPO-based mixture-of-experts architecture.
 (b) Gradient mask strategy.
Figure 2: Illustration the proposed MPOE architecture and gradient mask strategy. We decompose the weight
matrix of each expert in the MoE architecture into Ô¨Åve local tensors using MPO, containing four auxiliary tensors
and one central tensor, which are marked in orange and blue, respectively. In our approach, the central tensor of the
nexperts is shared in the MPOE architecture. During optimization, each backward propagation process updates
a set of auxiliary tensors while updating the central tensor with a probability of pb(the mask probability of the
central tensor), which can effectively avoid the unbalanced optimization of the central tensor.
Discussion . Since the central tensor contains most
of the information from original parameter matri-
ces (Gao et al., 2020), a key question is whether
the current architecture enables sufÔ¨Åcient Ô¨Çexibil-
ity and speciÔ¨Åcity for each expert. To answer
this question, we refer to an important property
of MPO decomposition from quantum many-body
physics (Pirvu et al., 2010): it is guaranteed in
principle, that any change on one tensor will be
propagated to the entire local tensor set. In other
words, only tuning the auxiliary tensors (keeping
the central tensor Ô¨Åxed) can lead to the same effect
as tuning the whole matrix. Since the parameters of
the central tensor are shared, our approach can sig-
niÔ¨Åcantly reduce the number of actual parameters
given the MoE architecture with the same number
of experts. Assuming the original model consisting
ofnexperts with Tparameters each, we have a
total number of nTparameters. SpeciÔ¨Åcally, let
denote the parameter ratio of the auxiliary tensor
to the central tensor for expert networks. Given the
total number Tfor an expert network, the central
and auxiliary tensors correspond to the parameters
numbers of
+1Tand1
+1T, respectively. Since
our MPOE approach shares the central tensor, the
Ô¨Ånal number of parameters will be
+1T+n
+1T.
Thus, our MPOE approaches corresponds to a ratio
ofn+
n(+1)of the original parameter scale. In our
experiments, the ratio is about 12, andn+
n(+1)
approximately equals to 0:19whenn= 8. Such a
ratio will be further decreased when we have more
experts. It can be seen that our MPOE approach is
able to effectively reduce the parameter scale.3.2 Alleviate Unbalanced Optimization
As the experts share the central tensor in the MPOE
approach, the corresponding parameters of the cen-
tral tensor will be updated more frequently than
those in the auxiliary tensors during Ô¨Åne-tuning. It
tends to lead to the unbalanced optimization issue
as reported by Xu et al. (2021), due to deviation
from the pre-trained weights. As a result, it is
crucial to develop a more stable optimization tech-
nique that is suited to the MPOE architecture.
Inspired by the solution of gradient dropout strat-
egy (Tseng et al., 2020; Xu et al., 2021), we pro-
pose to mask the gradients for the central tensor
to improve model optimization for the MPO-based
MoE architecture. At each iteration, we take a
certain probability pbto discard the update in the
central tensor. This can effectively alleviate the
unbalanced optimization which is caused by the
frequent updates of the central tensor. SpeciÔ¨Åcally,
we generate a binary mask bdrawn from Bernoulli
distribution with a mask probability pb, which can
be calculated by bBernoulli(pb). We denote
theCas the update of the central tensor at each
iteration:
C=@L(C)
@C(1 b): (4)
The largerpbis, the less frequently the central ten-
sor is updated. In particular, when pbis equal to 1,
it means that the parameters of the central tensor
are frozen for each input of the data. The compu-
tational cost of central tensor update can be also
reduced with this trick.

--- PAGE 5 ---
Note that the gradient mask trick is only applied
to central tensors. For auxiliary tensors, we per-
form the standard gradient update for learning the
parameters. Compared with two alternative ways to
implement the gradient mask technique, i.e.,mask
pre-activation or post-activation in FFN layers, we
Ô¨Ånd that such a sampling-based masking strategy
can effectively improve the model performance in
our experiments.
3.3 The Overall Algorithm Procedure
Our approach can be generally applied to various
MoE-based models for increasing the model ca-
pacity. In this work, we adopt the MoE-extended
PLMs (Radford et al., 2019) for study.
Algorithm 1 presents a complete procedure for
the proposed update procedure, which can be
brieÔ¨Çy summarized as follows. First, we obtain the
PLM and perform MPO decomposition for each
weight matrix of the FFN layers in the Transformer.
For each weight matrix, we decompose it into one
central tensorCand a list of auxiliary tensors A.
In the original MoE architecture, we will have n
sets of such decomposed parameters. Next, the key
point lies in that we share the central tensor Cin
the decomposition process but keep expert-speciÔ¨Åc
auxiliary tensors. In this way, each expert is com-
posed of a set of auxiliary tensors and a shared
central tensor. To recover the original FFN matrix
in some speciÔ¨Åc expert, we can simply multiply the
shared central tensor by expert-speciÔ¨Åc auxiliary
tensors. Then, we apply the gradient mask strat-
egy to update the parameters in these experts, i.e.,
masking the gradient of the central tensor.
Since the parameters of the central tensor are
two orders of magnitude larger than the parameters
of the auxiliary tensors (Liu et al., 2021), the cost
of MoE-based networks will be largely reduced by
sharing the central tensor.
3.4 Discussion
For the parameter inefÔ¨Åciency issue of MoE-based
networks, existing studies mainly focus on alleviat-
ing the unbalanced load of experts, which have pro-
posed different routing methods to balance the rout-
ing probabilities of different experts, such as BASE-
Layer (Lewis et al., 2021), HASHLayer (Roller
et al., 2021), GShard (Lepikhin et al., 2021) and
Switch Transformers (Fedus et al., 2021). As a
comparison, we aim to reduce information redun-
dancy by sharing common parameters among ex-
perts. Actually, the MPOE approach can be furtherAlgorithm 1 The proposed updating procedure.
Require:ffAjg4
j=1;Cg: Initialize experts
Require:: learning rate
Require:pb: mask probability
Require: time stept 0(Initialize timestep)
1:while not converged do
2:t t+ 1
3:gt
C @L(Ct)
@(Ct),gt
A @L(At)
@(At)
(Get gradients at timestep t)
4:b GenerateMask( pb)
(Compute gradient mask)
5:Ct Ct 1 gt
C(1 b)
(Update central tensors)
6:At At 1 gt
A
(Update the routed auxiliary tensors)
7:end while
8:returnffAt
jg4
j=1;Ctg(Resulting parameters)
enhanced with existing improved routing methods.
SpeciÔ¨Åcally, Deepspeed-MoE proposed to use
pyramid residual MoE architecture to reduce the
parameters of the MoE architecture (Rajbhandari
et al., 2022), while our work takes a different per-
spective to improve the original MoE architecture
by sharing parameters among different experts.
4 Experiments
In this section, we Ô¨Årst set up the experiments and
then report the results and analysis. Then, we con-
duct a detailed analysis under different experimen-
tal settings. Here, we use T5 (Raffel et al., 2020)
and GPT-2 (Radford et al., 2019) models as the
base model in our experiments.
4.1 Experimental Setup
Datasets. To evaluate the effectiveness of the pro-
posed MPOE as an efÔ¨Åcient strategy to improve
the model capacity of PLMs, we follow the set-
ting of T5 and GPT-2 to perform experiments on
Natural Language Understanding (NLU) and Nat-
ural Language Generation (NLG) tasks. SpeciÔ¨Å-
cally, we evaluate the NLG tasks in GLUE bench-
mark (Wang et al., 2018), the language modeling
task with WikiText-2 (Merity et al., 2017), the text
generation task with IMDB (Maas et al., 2011) and
EMNLP2017 WMT News (Guo et al., 2018). Fur-
thermore, we follow the setup of Raffel et al. (2020)
on the GLUE benchmark for a direct comparison
with the T5 model.
GLUE benchmark covers multiple datasets

--- PAGE 6 ---
NLU with T5
Experiments MNLI QNLI SST-2 RTE QQP CoLA MRPC STS-B Avg. #To (M)
T5-Large 89.23 94.03 96.20 83.94 91.54 55.10 90.15 91.90 86.51 737
+MoEÔ¨Åcation 87.50 93.20 95.40 86.40 90.20 55.50 87.50 90.60 85.79 737
+MoEÔ¨Åcation ++ 88.70 93.60 96.20 87.50 91.30 59.40 89.30 91.00 87.13 737
+Switch | / / / / / / / / 88.50 26000
+MPOE 87.16 94.12 96.80 88.60 90.63 67.63 93.65 91.97 88.82 956
T5-Base 87.78 93.82 94.72 71.74 91.11 53.49 89.16 91.16 84.12 223
+Switch | / / / / / / / / 86.70 3800
+Switch  87.73 93.85 94.87 77.53 91.59 59.90 91.64 91.16 86.03 1015
+MoEF 86.98 92.82 94.60 69.56 90.02 64.56 87.68 90.89 84.64 1015
+MPOE 87.60 93.30 94.81 77.13 90.81 65.53 93.14 91.17 86.69 294
+MPOE ++ 87.78 93.93 94.83 77.42 91.61 65.90 91.14 91.65 86.78 365
NLG with GPT-2
ExperimentsWikiText-2 EMNLP News IMDB#To (M)PPL (#) BLEU-2 BLEU-4 BLEU-2 Self-BLEU-2 BLEU-2 Self-BLEU-2
GPT-2 21.27 28.69 9.46 62.61 74.67 73.12 83.85 124
+MoEF 21.86 28.27 9.14 65.27 79.79 74.46 90.01 578
+Switch  21.25 28.71 9.44 64.62 81.11 75.35 91.82 578
+MPOE 20.72 28.78 9.51 66.99 83.10 76.30 92.72 157
+MPOE ++ 20.73 28.82 9.57 68.49 83.11 76.82 93.08 171
Table 1: Performance comparison of different models on NLU and NLG tasks (in percent). ‚Äú#To (M)‚Äù denote the
number (in millions) of total parameters. We set the number of experts n= 8in these models, MPOE. Furthermore,
we usen= 16 for a more powerful version of our approach, denoted by MPOE ++. We report the average test
performance of three runs, and the best results are highlighted in bold. : Experimental results by Zhang et al.
(2021b)|: Experimental results by Fedus et al. (2021) : Our re-implementation by Fedus et al. (2021). F:Apply
method by Shazeer et al. (2017).
(MRPC, QQP, SST-2, MNLI, RTE, QNLI, CoLA)1.
The original test sets are not publicly available, and
following Zhang et al. (2021a), for datasets fewer
than 10Ksamples (RTE, MRPC, STS-B, CoLA),
we divide the original validation set in half, using
one half for validation and the others for the test.
Evaluation Metrics. We use perplex-
ity (PPL) (Brown et al., 1992) to measure
how well the probability model predicts a sample
compared with the ground-truth. To evaluate
the ratios of the overlapping n-grams between
generated and real samples, we use BLEU- n
score (Papineni et al., 2002). We also take into
account the Self-BLEU- nscore (Zhu et al.,
2018) to evaluate the diversity of generated
samples especially. For metrics used in the GLUE
benchmark, we follow Mahabadi et al. (2021) and
use Matthew‚Äôs correlation for CoLA, Pearson for
STS-B, and accuracy for the other tasks.
Comparison methods. We adopt the T5 and
GPT-2 as the base architectures for both MoE and
MPOE. Following Shazeer et al. (2017), we ex-
1Following Raffel et al. (2020), as a common practice, due
to the adversarial nature of WNLI with respect to the training
set, we do not experiment with WNLItend the FFN components with the MoE archi-
tecture containing nexperts in each Transformer
block of the T5 and GPT-2 model. We refer to this
method as ‚Äú+MoE‚Äù. The Switch Transformers (Fe-
dus et al., 2021) use a simpliÔ¨Åed strategy that routes
to only a single expert instead of top- 2routing in
MoE. We refer to this method as ‚Äú+Switch‚Äù. To
ensure a fair comparison, we maintain the same
number (n= 8 ) of experts for baselines and
MPOE. We also implement an enhanced version of
MPOE with n= 16 experts, which is referred to as
‚Äú+MPOE ++‚Äù. Based on the released gpt2 model2,
t5-base model3andt5-large model4provided by
Huggingface, we Ô¨Årst initialize the experts, then
Ô¨Åne-tune the models on the downstream tasks. For
the T5 model, we follow the setting in Mahabadi
et al. (2021) and Ô¨Åne-tune all parameters of the
model on all tasks. For different downstream tasks,
we run a hyperparameter sweep and select the best
conÔ¨Åguration according to the accuracy results on
the validation set. The hyperparameters that we
tune include the epochs, batch size and learning
rates.
2https://huggingface.co/gpt2
3https://huggingface.co/t5-base
4https://huggingface.co/t5-large

--- PAGE 7 ---
4.2 Mains Results
In our main experiments, we adopt T5 (Raffel
et al., 2020), GPT-2 (Radford et al., 2019), Switch
Transformers (Fedus et al., 2021) and MoEÔ¨Åca-
tion (Zhang et al., 2021b) as baselines, and report
the comparison results of both NLU and NLG tasks
in Table 1.
Overall, compared to these MoE variants,
our proposed MPOE approach achieves perfor-
mance improvement while being more parameter-
efÔ¨Åcient. For the NLU task, our proposed approach
(‚Äú+MPOE‚Äù) outperforms the best baseline method,
i.e.,‚Äú+Switch‚Äù (88.82 vs. 88.50 for T5-Large) with
up to 27.2x reduction in total parameters in the
GLUE benchmark. By zooming into low-resource
datasets such as CoLA and MRPC, our approach
yields more signiÔ¨Åcant improvements. This sug-
gests that sharing parameters across experts rein-
forces the positive transfer effects5of informa-
tion from other datasets toward the learning of
low-resource datasets. For the NLG task, GPT-
2+MPOE achieves gains in BLEU-2 score (1.72
for GPT-2+MoE and 2.37 for GPT-2+Switch) with
3.7x reduction in total parameters on the EMNLP
News dataset. This indicates that GPT-2 also bene-
Ô¨Åts from sharing central tensors.
Moreover, T5+MPOE ++and GPT-2+MPOE ++
perform better when we add more auxiliary tensors
as additional experts. This demonstrates the neces-
sity of improving model capacity (Shazeer et al.,
2017), as more parameters of experts tend to result
in an improved model capacity.
4.3 Evaluation on Multi-task Learning
To demonstrate the efÔ¨Åciency of MPOE in multi-
task learning, we adopt the T5-Base model for anal-
ysis to be comparable with Hyperformer (Mahabadi
et al., 2021). We conduct experiments on the multi-
task GLUE benchmark. The detailed metrics can
be found in Section 4.1. Note that compared to Hy-
performer, MPOE approach does not incorporate
additional neural network components, thus it is
more Ô¨Çexible to be used with the PLMs.
Table 2 shows the results on GLUE benchmark
for T5-base (Raffel et al., 2020), Hyperformer (Ma-
habadi et al., 2021) and MPOE. As we can see, the
performance of the MPOE approach is consistently
better than the Hypernetwork in all cases, while
5Here, the positive transfer effects can be referred to Ma-
habadi et al. (2021), which means that the transferred knowl-
edge can lead to improved performance for unseen in-domain
tasks.Datasets T5-Base Hyper | +MPOE
MNLI (acc) 87.73 85.74 87.83
QNLI (acc) 93.51 93.02 93.89
SST-2 (acc) 92.50 94.03 94.73
RTE (acc) 75.41 75.36 75.51
QQP (acc) 91.12 90.28 91.17
CoLA (mcc) 54.93 63.73 65.85
MRPC (acc) 89.21 89.66 90.10
STS-B (pearson) 90.75 90.00 90.92
Avg. 84.39 85.23 86.25
#To (M) 223 343 258
Table 2: Performance of multi-task learning on GLUE
benchmark obtained by Ô¨Åne-tuning T5-Base (in per-
cent).|: Experimental results from Hyperformer (Ma-
habadi et al., 2021).
VariantsWikiText-2#To (M)PPL (#) B2 B4
+MoEF 21.86 28.27 9.14 578
w/o PS 21.28 28.67 9.44 153
w/o GM 21.17 28.71 9.47 157
+MPOE 20.72 28.78 9.51 157
Table 3: Ablation study on the WikiText-2 dataset
about the NLG tasks (in percent). ‚ÄúB2‚Äù and ‚ÄúB4‚Äù are
short for BLEU-2 and BLEU-4, respectively. F: The
method from Shazeer et al. (2017)
the MPOE is more parameter-efÔ¨Åcient (258M vs.
343M in total parameters). It further demonstrates
the potential beneÔ¨Åts of the MPOE approach in a
multi-task learning setting, where the central tensor
learns common information across tasks and the
auxiliary tensor learns task-speciÔ¨Åc information.
4.4 Ablation Results
Our approach has incorporated two novel improve-
ments: (1) MoE architecture with parameters shar-
ing (PS) among experts based on MPO decom-
position and (2) gradient mask (GM) to alleviate
unbalanced optimization.
To verify the effectiveness of each component,
we conduct the ablation study on the WikiText-2
dataset to analyze the contribution of each part. We
adopt PPL, BLEU-2 and BLEU-4 as the evalua-
tion metrics, and consider removing the parameters
sharing and gradient mask strategy respectively.
The ablation results of our MPOE approach are
shown in Table 3. We can see that removing any
component would lead to a decrease in the model
performance. It shows the effectiveness of all these
components in our approach. Besides, parameter
sharing seems more important than the gradient

--- PAGE 8 ---
VariantsPPL(#)WikiText-2
B2 B4#To (M)
GPT-2 21.27 28.69 9.46 124.4
MPOE (m=3) 24.01 27.86 8.93 130.3
MPOE (m=5) 20.72 28.77 9.48 157.4
MPOE (m=7) 20.73 28.76 9.47 198.7
MPOE (m=9) 20.78 28.45 9.38 214.6
Table 4: Evaluation with different factorization manner
on the WikiText-2 dataset about the NLG tasks (in per-
cent). ‚ÄúB2‚Äù and ‚ÄúB4‚Äù are short for BLEU-2 and BLEU-
4, respectively.
mask strategy, which yields a larger performance
drop after being removed.
4.5 Detailed Analysis
MPO decomposition has different factorization
manners. However, the MPOE approach requires
a deÔ¨Åned MPO decomposition form to be given
before it can be used. Therefore, different factor-
ization manners may affect the efÔ¨Åciency of the
MPOE approach. To vertify this, we perform a
detailed analysis on different factorization man-
ners of MPO decomposition. We present three
variants of MPOE with different lengths of local
tensors produced by MPO decomposition empiri-
cally. Tabel 4 shows the evaluation results on the
WikiText-2 dataset about NLG tasks. As we can
see, the variants of m > 3are all superior to the
GPT-2 model. Additionally, we can observe that
more local tensors performs similarly but leads to
higher memory cost. Thus we Ô¨Ånally choose to
setm= 5for MPOE architecture considering the
trade-off between the cost and quality.
5 Related Work
We will review the related works in four aspects.
PLMs with MoE. It has been reported that mod-
els with more parameters are usually considered to
have a larger model capacity (Fedus et al., 2021;
Zuo et al., 2021). In order to increase the model ca-
pacity, a promising direction is to explore the scal-
ing properties with MoE architecture which was
introduced by Jacobs et al. (1991). Thus, Shazeer
et al. (2017) Ô¨Årst applied the MoE architecture to
large-scale language models. Then, Switch Trans-
formers (Fedus et al., 2021), GShard (Lepikhin
et al., 2021), BASELayer (Lewis et al., 2021) and
HashLayer (Roller et al., 2021) studied how to
build large-scale Transformer-based model with
MoE as well as improving routing strategy, whichcan better utilize the model capacity. In addi-
tion, Zhang et al. (2021b) proposed a strategy for
sparse activation of MoE architecture. He et al.
(2021) suggested a distributed training system for
fast training of MoE. Zoph et al. (2022) proposed a
sparse expert model with more stable training. Yu
et al. (2022) proposed a sparse expert model based
on all-MLP architecture. In contrast, our approach
aims to reduce information redundancy by sharing
parameters among experts.
Matrix Product Operators Decomposition.
Matrix product operators (MPO) (Pirvu et al.,
2010) decomposition was proposed in quantum
many-body physics, a.k.a. tensor-train (TT) de-
composition (Oseledets, 2011). A major category
of MPO studies relies on model compression (Gao
et al., 2020). They focus on compressing weight
matrix and convolutional layers (Novikov et al.,
2015; Garipov et al., 2016; Sun et al., 2020).
Furthermore, the MPO decomposition was used to
compress the PLMs as well as enable lightweight
Ô¨Åne-tuning in downstream tasks (Liu et al., 2021).
In this work, we utilize such a decomposition
mechanism for parameter sharing to construct a
parameter-efÔ¨Åcient MoE architecture.
Improved Variants of MoE. Despite the
achieved performance performance, MoE architec-
ture has been hindered by the model complexity
and high memory costs (Shazeer et al., 2017;
Fedus et al., 2021). This problem can be alleviated
by using distillation (Fedus et al., 2021) and expert
pruning (Kim et al., 2021). Then, Kudugunta
et al. (2021) and Zuo et al. (2021) indicated
that sub-networks can be employed when using
the model. Indeed, our approach can be further
enhanced by these existing methods for improving
inference time.
Multi-task Learning. The exploitation of MoE
architectures for multi-task learning is a very
promising direction in recent years (Ma et al.,
2018). Houlsby et al. (2019) suggested training
adapters for each task separately while keeping the
model Ô¨Åxed. Further research suggested that model
parameters could be shared across tasks, and task-
speciÔ¨Åc adapter parameters were introduced (Stick-
land and Murray, 2019). Based on this idea, Ma-
habadi et al. (2021) and Pilault et al. (2020)
proposed that parameter-efÔ¨Åcient multi-task Ô¨Åne-
tuning for transformer-based models via shared
hypernetworks. Our approach differs from these

--- PAGE 9 ---
works in that the MPOE approach allows us to re-
duce model size while keeping the same number
of experts, and meanwhile achieve performance
improvement for multi-task learning.
6 Conclusion
In this paper, we proposed a parameter-efÔ¨Åcient
MoE architecture for increasing model capacity
based on the MPO decomposition. First, we shared
the central tensors among different experts based
on MPO decomposition, which largely reduced the
model parameters of MoE architecture. Then, we
designed the gradient mask strategy to alleviate
the unbalanced optimization issues and ensured
that different tensors capture different types of in-
formation efÔ¨Åciently. Extensive experiments have
shown that our approach outperforms several com-
petitive PLM scaling strategies, especially in terms
of improving the parameter efÔ¨Åciency of the MoE
architecture.
In the future, we will enhance the proposed
MPOE approach with recently proposed routing
methods, such as BASELayer (Lewis et al., 2021),
HASHLayer (Roller et al., 2021) and GShard (Lep-
ikhin et al., 2021). We will also consider exploring
additional decomposition methods for developing
parameter-efÔ¨Åcient MoE architecture.
Acknowledgments
This work was partially supported by Beijing Natu-
ral Science Foundation under Grant No. 4222027,
National Natural Science Foundation of China un-
der Grants No. 62206299 and 11934020, Beijing
Outstanding Young Scientist Program under Grant
No. BJJWZYJH012019100020098 and Beijing
Academy of ArtiÔ¨Åcial Intelligence (BAAI). Xin
Zhao is the corresponding author.
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, Jennifer C. Lai, and Robert L. Mercer. 1992.
An estimate of an upper bound for the entropy of
english. Comput. Linguistics , 18(1):31‚Äì40.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efÔ¨Åcient sparsity. arXiv
preprint arXiv:2101.03961 .Ze-Feng Gao, Song Cheng, Rong-Qiang He, ZY Xie,
Hui-Hai Zhao, Zhong-Yi Lu, and Tao Xiang.
2020. Compressing deep neural networks by ma-
trix product operators. Physical Review Research ,
2(2):023300.
Timur Garipov, Dmitry Podoprikhin, Alexander
Novikov, and Dmitry Vetrov. 2016. Ultimate ten-
sorization: compressing convolutional and fc layers
alike. arXiv preprint arXiv:1611.03214 .
Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2018. Long text generation via
adversarial training with leaked information. In Pro-
ceedings of the Thirty-Second AAAI Conference on
ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innova-
tive Applications of ArtiÔ¨Åcial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Ad-
vances in ArtiÔ¨Åcial Intelligence (EAAI-18), New Or-
leans, Louisiana, USA, February 2-7, 2018 , pages
5141‚Äì5148. AAAI Press.
Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Ji-
dong Zhai, and Jie Tang. 2021. Fastmoe: A fast
mixture-of-expert training system. arXiv preprint
arXiv:2103.13262 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efÔ¨Åcient transfer learning for NLP.
InProceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , volume 97 of
Proceedings of Machine Learning Research , pages
2790‚Äì2799. PMLR.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan,
and Geoffrey E Hinton. 1991. Adaptive mixtures of
local experts. Neural computation , 3(1):79‚Äì87.
Young Jin Kim, Ammar Ahmad Awan, Alexandre
Muzio, Andres Felipe Cruz Salinas, Liyang Lu,
Amr Hendy, Samyam Rajbhandari, Yuxiong He, and
Hany Hassan Awadalla. 2021. Scalable and efÔ¨Å-
cient moe training for multitask multilingual models.
arXiv preprint arXiv:2109.10465 .
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
Maxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-
ong, and Orhan Firat. 2021. Beyond distillation:
Task-level mixture-of-experts for efÔ¨Åcient inference.
arXiv preprint arXiv:2110.03742 .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. 2021.
Gshard: Scaling giant models with conditional com-
putation and automatic sharding. In 9th Inter-
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman
Goyal, and Luke Zettlemoyer. 2021. Base layers:

--- PAGE 10 ---
Simplifying training of large, sparse models. arXiv
preprint arXiv:2103.16716 .
Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan
Xie, Zhong-Yi Lu, and Ji-Rong Wen. 2021. En-
abling lightweight Ô¨Åne-tuning for pre-trained lan-
guage model compression based on matrix product
operators. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing, ACL/IJCNLP 2021, (Vol-
ume 1: Long Papers), Virtual Event, August 1-6,
2021 , pages 5388‚Äì5398. Association for Computa-
tional Linguistics.
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan
Hong, and Ed H. Chi. 2018. Modeling task re-
lationships in multi-task learning with multi-gate
mixture-of-experts. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge
Discovery & Data Mining, KDD 2018, London, UK,
August 19-23, 2018 , pages 1930‚Äì1939. ACM.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y . Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 142‚Äì150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
Dehghani, and James Henderson. 2021. Parameter-
efÔ¨Åcient multi-task Ô¨Åne-tuning for transformers via
shared hypernetworks. In Proceedings of the
59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-
tual Event, August 1-6, 2021 , pages 565‚Äì576. Asso-
ciation for Computational Linguistics.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.
Alexander Novikov, Dmitry Podoprikhin, Anton Os-
okin, and Dmitry Vetrov. 2015. Tensorizing neural
networks. arXiv preprint arXiv:1509.06569 .
Ivan V Oseledets. 2011. Tensor-train decomposition.
SIAM Journal on ScientiÔ¨Åc Computing , 33(5):2295‚Äì
2317.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA , pages 311‚Äì318. ACL.Jonathan Pilault, Amine Elhattami, and Christopher
Pal. 2020. Conditionally adaptive multi-task learn-
ing: Improving transfer learning in nlp using
fewer parameters & less data. arXiv preprint
arXiv:2009.09139 .
Bogdan Pirvu, Valentin Murg, J Ignacio Cirac, and
Frank Verstraete. 2010. Matrix product operator rep-
resentations. New Journal of Physics , 12(2):025012.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a uniÔ¨Åed text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1‚Äì140:67.
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Min-
jia Zhang, Reza Yazdani Aminabadi, Ammar Ah-
mad Awan, Jeff Rasley, and Yuxiong He. 2022.
Deepspeed-moe: Advancing mixture-of-experts in-
ference and training to power next-generation ai
scale. arXiv preprint arXiv:2201.05596 .
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,
and Jason Weston. 2021. Hash layers for large
sparse models. arXiv preprint arXiv:2106.04426 .
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538 .
Asa Cooper Stickland and Iain Murray. 2019. BERT
and pals: Projected attention layers for efÔ¨Åcient
adaptation in multi-task learning. In Proceedings
of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , volume 97 of Proceedings of Ma-
chine Learning Research , pages 5986‚Äì5995. PMLR.
Xingwei Sun, Ze-Feng Gao, Zhong-Yi Lu, Junfeng Li,
and Yonghong Yan. 2020. A model compression
method with matrix product operators for speech
enhancement. IEEE/ACM Transactions on Audio,
Speech, and Language Processing , 28:2837‚Äì2847.
Hung-Yu Tseng, Yi-Wen Chen, Yi-Hsuan Tsai, Sifei
Liu, Yen-Yu Lin, and Ming-Hsuan Yang. 2020. Reg-
ularizing meta-learning via gradient dropout. In Pro-
ceedings of the Asian Conference on Computer Vi-
sion.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. EMNLP 2018 ,
page 353.

--- PAGE 11 ---
Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,
Baobao Chang, Songfang Huang, and Fei Huang.
2021. Raise a child in large language model: To-
wards effective and generalizable Ô¨Åne-tuning. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2021, Virtual Event / Punta Cana, Dominican Re-
public, 7-11 November, 2021 , pages 9514‚Äì9528. As-
sociation for Computational Linguistics.
An Yang, Junyang Lin, Rui Men, Chang Zhou,
Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-
mang Wang, Yong Li, Di Zhang, Wei Lin, Lin
Qu, Jingren Zhou, and Hongxia Yang. 2021. Ex-
ploring sparse expert models and beyond. CoRR ,
abs/2105.15082.
Ping Yu, Mikel Artetxe, Myle Ott, Sam Shleifer,
Hongyu Gong, Ves Stoyanov, and Xian Li. 2022.
EfÔ¨Åcient language modeling with sparse all-mlp.
arXiv preprint arXiv:2203.06850 .
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.
Weinberger, and Yoav Artzi. 2021a. Revisiting few-
sample BERT Ô¨Åne-tuning. In 9th International Con-
ference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net.
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2021b. MoeÔ¨Å-
cation: Conditional computation of transformer
models for efÔ¨Åcient inference. arXiv preprint
arXiv:2110.01786 .
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-
gen: A benchmarking platform for text generation
models. In The 41st International ACM SIGIR Con-
ference on Research & Development in Information
Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-
12, 2018 , pages 1097‚Äì1100. ACM.
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du,
Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. 2022. Designing effective sparse ex-
pert models. arXiv preprint arXiv:2202.08906 .
Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin
Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao,
and Jianfeng Gao. 2021. Taming sparsely acti-
vated transformer with stochastic experts. CoRR ,
abs/2110.04260.
