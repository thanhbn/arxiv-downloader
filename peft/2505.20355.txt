# 2505.20355.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2505.20355.pdf
# File size: 3776386 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GraLoRA: Granular Low-Rank Adaptation for
Parameter-Efficient Fine-Tuning
Yeonjoon Jung1,2∗Daehyun Ahn1Hyungjun Kim1Taesu Kim1Eunhyeok Park2†
1SqueezeBits2POSTECH
{yeonjoon.jung, daehyun.ahn, hyungjun.kim, taesu.kim}@squeezebits.com
yeonjoon.jung@postech.ac.kr, eh.park@postech.ac.kr
Abstract
Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-
tuning (PEFT) of generative models, valued for its simplicity and effectiveness.
Despite recent enhancements, LoRA still suffers from a fundamental limitation:
overfitting when the bottleneck is widened. It performs best at ranks 32–64, yet its
accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning
(FFT) performance. We identify the root cause as LoRA’s structural bottleneck,
which introduces gradient entanglement to the unrelated input channels and distorts
gradient propagation. To address this, we introduce a novel structure, Granular
Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks,
each with its own low-rank adapter. With negligible computational or storage cost,
GraLoRA overcomes LoRA’s limitations, effectively increases the representational
capacity, and more closely approximates FFT behavior. Experiments on code gen-
eration and commonsense reasoning benchmarks show that GraLoRA consistently
outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in
Pass@1 on HumanEval+. These improvements hold across model sizes and rank
settings, making GraLoRA a scalable and robust solution for PEFT. Code, data,
and scripts are available at https://github.com/SqueezeBits/GraLoRA.git
1 Introduction
Task-specific fine-tuning enables a wide range of applications and significantly improves the quality
and effectiveness of generative models. However, the massive scale of these models poses substantial
challenges for practical deployment. To address these limitations, Parameter-Efficient Fine-Tuning
(PEFT) methods have emerged as a cost-effective alternative [ 10,26]. Among them, Low-Rank
Adaptation (LoRA) [ 11] has gained particular attention for its simplicity and effectiveness, introducing
trainable low-rank matrices while keeping the pre-trained model weights frozen. Although the
imposed rank- rbottleneck may lead to slight performance degradation compared to full fine-tuning
(FFT), its efficiency has led to widespread adoption in practice.
To maximize the benefits of LoRA, various studies have proposed techniques such as improved
initialization [5, 18, 20, 23] and structural refinements [9, 13, 14, 15] to enhance fine-tuning quality.
While these efforts have advanced performance, a substantial quality gap remains compared to FFT,
largely due to the inherent upper bound on the rank. Although using a higher rank, within hardware
limits, appears to be a natural solution, unfortunately, current implementations of LoRA and its
variants do not support such flexibility. Simply increasing the rank often leads to degraded accuracy
in many scenarios.
In this paper, we present a theoretical analysis identifying the root cause of the rank limitation in
LoRA. Our analysis reveals a fundamental issue in LoRA’s structure, channel dominance in the
gradient, where a small subset of outlier channels disproportionately influences the update direction.
Preprint. Under review.arXiv:2505.20355v1  [cs.LG]  26 May 2025

--- PAGE 2 ---
Figure 1: Illustration of LoRA architecture and GraLoRA architecture. GraLoRA consists of k2small
adapter pairs, where each input and output dimension is ktimes smaller than the original LoRA.
This dominance suppresses contributions from other channels, leading to under-utilization of the
available rank and degraded performance in tasks that require nuanced or distributed representations.
To overcome these expressivity bottlenecks, we propose Granular Low-Rank Adaptation (GraLoRA),
a novel architectural extension of LoRA. As shown in Figure 1, GraLoRA divides the weight matrix
into multiple sub-blocks and applies independent LoRA modules to each, enabling fine-grained
updates. This design enhances the model’s capacity to capture complex, localized, or multi-faceted
patterns, effectively mitigating the channel dominance issue and improving performance—especially
at higher ranks.
Extensive experiments show that GraLoRA consistently outperforms vanilla LoRA across a range of
NLP benchmarks, particularly in scenarios with high input heterogeneity or task complexity. These
results position GraLoRA as a principled and practical advancement in the PEFT landscape.
2 Details and Limitations of LoRA
2.1 Introduction to LoRA
LoRA is one of the most widely adopted strategies for PEFT. Given a pre-trained weight matrix
W0∈RM×N, where MandNrepresent the input and output channel dimension, respectively,
LoRA keeps W0frozen and introduces a trainable low-rank update defined as:
R=sBA⊤, A∈RN×r, B∈RM×r, s =α
r. (1)
Here, rank randαare user-defined hyperparameters. Then, for a given input X∈RN×T, the
output of the LoRA-adapted layer is Y=W0X+RX∈RM×T, where Tdenotes the batch or
token dimension. This low-rank decomposition allows the model to adapt using significantly fewer
trainable parameters and reduced memory overhead.
While FFT updates the entire weight matrix, LoRA only updates the decomposed low-rank matrices
AandB. Note that we assume s = 1 for simplicity, the gradient of the loss with respect to Ris:
∂L
∂R=∂L
∂YX⊤∈RM×N(2)
From this, the gradients with respect to the LoRA parameters BandAare given by:
∂L
∂B=∂L
∂YX⊤A,∂L
∂A⊤=B⊤∂L
∂YX⊤. (3)
2

--- PAGE 3 ---
Figure 2: Gradient dynamics of FFT and LoRA in the presence of an outlier input channel. The
red channel in input Xdenotes the outlier. While FFT localizes the gradient impact, LoRA’s entire
gradient update becomes disproportionately influenced by the single outlier.
These result in the following update in the fused weight space:
∂L
∂R=∂L
∂BA⊤+B∂L
∂A⊤=∂L
∂YX⊤AA⊤+BB⊤∂L
∂YX⊤. (4)
This expression reveals how the structure of LoRA introduces non-trivial interactions between the
gradients and the input, particularly through the rank- rmatrices.
2.2 Why Does LoRA Suffer from a Larger Rank?
When fine-tuning with a large LoRA rank (e.g., r >64), it is often observed that accuracy degrades
compared to using a moderate rank. This counterintuitive behavior arises from the distinct gradient
dynamics of LoRA, which differ significantly from those of FFT.
LoRA’s structural design makes its gradients inherently sensitive to the entire input space, as illustrated
in Figure 2. In particular, we observe that outlier channels , input channels with abnormally high
activations, can disproportionately dominate the gradient signal.
In FFT, the effect of such outliers is typically localized, affecting only a single column of the weight
matrix Wthat directly interacts with the outlier channel. In contrast, LoRA’s low-rank constraint
causes the entire gradient of the adapter matrix B, denoted ∂L/∂B , to be influenced by these
outliers. This results in distorted weight updates in the fused weight space, where the gradient signal
from outlier channels overwhelms the contributions from other inputs. Consequently, LoRA fails to
accurately replicate the gradient dynamics of FFT, limiting its ability to match FFT-level performance.
We observe that in certain layers, most notably the down-projection matrix of Layer 1 in
LLaMA3.1–8B, input activations exhibit severe channel-wise imbalance (Figure 3 (a)). As shown in
Figure 4, these outlier channels disproportionately impact the adapter’s gradient updates. Figure 3
further illustrates that the gap between LoRA and FFT gradient updates widens as the LoRA rank
increases.
These findings reveal a fundamental misalignment between LoRA updates and the gradient landscape
shaped by FFT. The entangled influence of input channels caused by the low-rank projection limits
LoRA’s ability to selectively learn from salient features, particularly under skewed input statistics.
While the negative impact of outliers has been well recognized in the context of quantization-aware
training [ 25] [16], their influence on LoRA’s behavior has not been systematically studied until now.
3

--- PAGE 4 ---
Figure 3: (a) Mean input channel values for the down-projection matrices across layers in
LLaMA3.1–8B. A pronounced outlier exists in Layer 1, channel 198 and 2427. (b) Gradient deviation
between LoRA and FFT increases with rank, showing LoRA’s susceptibility to input outliers. (c)
GraLoRA gradient results at rank 128. GraLoRA noticeably reduces gradient deviation between FFT.
Figure 4: Gradient distribution in Layer 1 down-projection matrix. LoRA gradients show poor
alignment with FFT, outlier channel increases the overall gradient scale, while less emphasizing the
corresponding outlier channel.
3 Method
3.1 GraLoRA: Granular Low-Rank Adaptation
Motivated by observation in previous section, we propose GraLoRA , a fine-grained and modular
extension of LoRA. As illustrated in Figure 1, GraLoRA addresses the limitations of standard LoRA
by partitioning the weight matrix into a grid of k×kindependent blocks, each equipped with its own
local low-rank adapter. Here, kis a hyperparameter that determines the number of splits along the
input and output dimensions. When k= 1, GraLoRA reduces to the vanilla LoRA formulation.
Specifically, the weight update R∈RM×Nis expressed as the concatenation of block-wise updates:
RGraLoRA =
B1,1A⊤
1,1··· B1,kA⊤
1,k
.........
Bk,1A⊤
k,1··· Bk,kA⊤
k,k
, A i,j∈RN
k×r
k, B i,j∈RM
k×r
k (5)
This block-wise reparameterization provides localized control over each spatial subregion of the
parameter space. As detailed in Section 3.4, GraLoRA incurs the same parameter count and com-
putational overhead as standard LoRA when using the same rank. However, it introduces two key
advantages; (1) Enhanced Expressivity and (2) Robustness to Input Outliers . By enabling in-
dependent adaptation across k2subspaces, GraLoRA supports more fine-grained and specialized
feature learning. In addition, Localized gradient updates ensure that only the adapters associated with
4

--- PAGE 5 ---
Figure 5: Regularized form of GraLoRA as multiplication of sparse two matrices, AGraLoRA and
BGraLoRA .
the affected input regions receive large gradients, thereby reducing global gradient distortion and
preserving inter-channel signal balance.
3.2 Expression Power Analysis
While the weight update of GraLoRA was expressed as concatenation of block-wise updates in ( 5), it
can also be regularized as the form of multiplication of two matrices as in the vanilla LoRA. The sparse
matrix AGraLoRA ∈RN×krcan be constructed as Figure 5 (a), where Ai,jfori, j∈ {n∈N|n≤k}
is located in position (i+ (j−1)×k, j)ofAGraLoRA . Other elements are masked out, thus the total
number of parameter becomes N×r.
Then, BGraLoRA ∈RN×kris constructed as Figure 5 (b), where matrix Bi,jfori, j∈ {n∈N|
n≤k}is located in position (i, j+ (i−1)×k)ofBGraLoRA , Similarly, other composition of the
matrix is masked, therefore the total number of parameter becomes M×r. Then the weight update
of GraLoRA can be expressed as W=W0+RGraLoRA =W0+BGraLoRA A⊤
GraLoRA .
Assuming that all columns of [Bi,1,···, Bi,k]are linearly independent, the rank of BGraLoRA becomes
R(BGraLoRA ) =kr. Similarly, if all columns of [A1,j,···, Ak,j]are linearly independent, the rank
ofAGraLoRA isR(AGraLoRA ) =kr. Applying Sylvester’s rank inequality to derive the lower bound
and the matrix product theorem for the upper bound, we obtain:
R(BGraLoRA ) +R(A⊤
GraLoRA )−kr≤R(BGraLoRA A⊤
GraLoRA )≤min(R(BGraLoRA ),R(A⊤
GraLoRA ))(6)
Thus, the effective rank of RGraLoRA becomes kr, which is ktimes higher than that of the vanilla
LoRA method—effectively enhancing the model’s expressive capacity. The rank analysis of fine-
tuned LoRA and GraLoRA, summarized in Table 4 in Appendix, demonstrates that GraLoRA linearly
scales the representational power of the adaptation matrix in practical settings.
3.3 Gradient Dynamics Under Outlier Activation
GraLoRA effectively localizes the influence of outlier channels to a limited subset of adapter blocks.
Because each block processes only a specific slice of the input, only the kadapter pairs intersecting
with the outlier channel are exposed to amplified gradients. In contrast, the remaining k2−kadapters
maintain gradient magnitudes close to baseline levels. This selective gradient propagation resembles
the behavior of FFT, where only weights directly connected to active inputs are significantly updated.
GraLoRA’s impact on gradient dynamics can be observed by comparing gradient distributions of the
down-projection matrix in Layer 1 with standard LoRA. As illustrated in the Figure 3 (c) and Figure 6,
GraLoRA reduces the gradient deviation and limits the influence of outlier channels, overcoming the
limitations of standard LoRA with larger ranks.
3.4 Tradeoff Analysis
As discussed, GraLoRA provides several advantages over standard LoRA. However, these benefits
do not come without cost. In this section, we provide deeper analysis on the overhead introduced by
GraLoRA.
Computation Overhead Analysis: First, we analyze the expected computational cost of LoRA in
terms of FLOPs. To take advantage of the low-rank structure, LoRA computes the projection in two
sequential steps. The first computes A⊤X∈Rr×T, followed by the reconstruction B(A⊤X)∈
5

--- PAGE 6 ---
Figure 6: Comparison of gradient distributions under outlier activation. In GraLoRA, only the blocks
interacting with the outlier exhibit elevated gradients, mitigating global distortion and aligning with
FFT behavior.
RM×T. These steps require 2NrT and2rMT FLOPs, respectively, resulting in a total complexity
ofO(r(M+N)T).
Similarly, GraLoRA divides the computation into two steps involving k2adapter blocks. In the first
step, the projection computes A⊤
i,jXj∈Rr
k×Tfor each of the k2blocks, incurring a total cost of
2·N
k·r
k·T·k2= 2NrT. In the second step, each intermediate output is processed by its corresponding
Bi,j, producing Bi,j(A⊤
i,jXj)∈RM
k×T. This step adds another 2·r
k·M
k·T·k2= 2rMT. FLOPs
to the total cost. Hence, the overall computational cost of GraLoRA remains O(r(M+N)T),
maintaining efficiency comparable to vanilla LoRA while significantly enhancing expressive power.
A detailed analysis of computational overhead is provided in Appendix C.
Table 1: Maximal allocated memory during training LLaMA3.1–8B model with batch size 1. Input
length was set to 1024 and memory allocated for weight was removed for direct comparison.
LoRA GraLoRA (k=2) GraLoRA (k=4) GraLoRA (k=8)
Vanilla Backward (GB) 10.0 10.1 10.2 10.4
Gradient Checkpointing (GB) 2.6 2.6 2.6 2.6
Memory Overhead Analysis: As with classical LoRA, GraLoRA can be merged into the original
weight matrix at inference time. Therefore, our analysis focuses on the memory overhead incurred
during training. Although the number of parameters and FLOPs are identical to those of LoRA,
the intermediate latent representation A⊤
GraLoRA Xbecomes ktimes larger than the corresponding
A⊤Xin standard LoRA. This expanded latent space allows for greater information preservation,
which can be beneficial. However, it also leads to increased memory consumption during training
time. Fortunately, the rank ris typically much smaller than the input and output dimensions, thus
the additional memory required remains marginal—even for large k, as demonstrated in Table 1.
Moreover, by applying recent techniques such as gradient checkpointing, the memory overhead from
the expanded latent space can be effectively hidden, making the impact negligible in practice.
Selection of k While GraLoRA increases the total rank from rtokr, each individual block, rep-
resented as Bi,jA⊤
i,j∈RM
k×N
k, is constrained to a reduced rank ofr
k. As a result, increasing k
beyond a certain threshold can degrade performance due to limited expressiveness within each block.
This effect is especially pronounced when the overall rank ris small. Empirically, we observed that
maintaining a minimum block expressiveness of approximately r/k2≈8yields stable performance
across various configurations. Based on this observation, we adopted k= 2for ranks 16 and 32, and
k= 4for ranks 64 and 128 in our experiments. Detailed results from the k-sweep can be found in
Section 4.4.
3.5 Hybrid GraLoRA
On the other hand, for smaller ranks—typically rank 16 or below—using k= 2 may still lead to
performance degradation or yield only marginal gains. To address this limitation, we introduce
6

--- PAGE 7 ---
Figure 7: Hybrid GraLoRA architecture when GraLoRA k= 2. LoRA parameter becomes shared
across small GraLoRA adapters in the same row or same column.
a hybrid approach that combines the strengths of LoRA and GraLoRA. This method retains the
fine-grained input handling and increased total rank offered by GraLoRA, while preserving the
expressive power of larger block units through LoRA. Since LoRA shares the same parameters across
both rows and columns, it can be naturally integrated with GraLoRA in a concatenated form, which
we refer to as Hybrid GraLoRA (see Figure 7). Empirically, we found that allocating up to1
2of
the total rank to the LoRA component mitigated the limitations of GraLoRA in low-rank scenarios
(γ <= 16 ), while fully allocating the rank to GraLoRA better performed in high-rank circumstances.
4 Experiments
In order to validate the superiority of the proposed idea, we conduct an extensive analysis on large-
scale dataset with the state-of-the art LLMs. We evaluate GraLoRA across two challenging domains:
code generation andcommonsense reasoning . Our experiments are designed to assess whether the
proposed granular adaptation mechanism improves performance across varying model sizes, LoRA
ranks, and tasks that require nuanced reasoning and high representational fidelity.
4.1 Experimental Setup
Code Generation. We fine-tuned each model on the Magicoder-Evol-Instruct-110k [ 24] train
dataset, a curated and decontaminated subset of WizardCoder [ 17], comprising high-quality instruc-
tion–response pairs for programming tasks. Evaluation was conducted on the Humaneval+ test dataset
following He et al. [ 9], which samples 50 completions per problem using a temperature of 0.2. We
report Pass@1, Pass@5, and Pass@10 accuracy following standard protocol via BigCode Evaluation
Harness [1]. We have evaluated the last epoch weight across all methods.
Commonsense Reasoning We fine-tune each model across 8 commonsense tasks: BoolQ [ 6],
PIQA [ 4], SIQA [ 22], HellaSwag [ 28], WinoGrande [ 21], ARC-Challenge, ARC-Easy [ 7], and
OpenBookQA [ 19]. We followed the training pipeline proposed by LLM-Adapters [ 12], using a
merged dataset composed of the training sets from all tasks. Evaluation was conducted on individual
testing dataset for each task on the last epoch weight. Detailed training parameters can be found in
Appendix D.
Training Details We conducted experiments on four open-sourced LLMs with different architecture
and size: LLaMA3.1–8B, LLaMA3.1–70B ( [ 8]), Qwen-2.5-1.5B, and Qwen-2.5-7B ( [ 27]). We used
pre-trained models rather than instruction-tuned models following the common practice ( [ 14,15]).
We applied PEFT methods on all linear modules from attention ( Wq, Wk, Wv, Wo)and feed-forward
networks ( Wup, Wdown, Wgate). We set the hyper-parameters based on the optimal configurations
from Biderman et al. [ 3] and He et al. [ 9], employing decoupled LionW optimizer with batch size of
192 and setting LoRA α= 2r. We applied alpaca-chat template for both tasks. Code generation was
conducted on 4 A100 80G GPUs and commonsense reasoning task was conducted on 2 H100 80G
GPUs for 1.5 8B models. 70B model was conducted on 8 A100 80G GPUs. We compared GraLoRA
to three representative PEFT methods: LoRA, MoRA [14] and RaSA [9].
7

--- PAGE 8 ---
Table 2: Pass@1, Pass@5, and Pass@10 results on LLaMA3.1–8B using LoRA, MoRA, RaSA, and
GraLoRA across different ranks. Best results per group are in bold. * indicates Hybrid GraLoRA.
Rank Method Training Time Relative Time Pass@1 Pass@5 Pass@10
16LoRA 6.2h 1.00 × 56.1% 65.3% 68.1%
MoRA 8.8h 1.42 × 53.6% 62.2% 64.5%
RaSA 6.7h 1.08 × 53.7% 64.4% 66.7%
GraLoRA* 6.7h 1.08 × 58.0% 67.1 % 70.1%
32LoRA 6.5h 1.00 × 58.4% 68.0% 69.9%
MoRA 9.1h 1.40 × 58.3% 66.7% 69.0%
RaSA 6.8h 1.05 × 57.2% 67.9% 70.5%
GraLoRA 6.9h 1.06 × 58.9% 67.0% 69.0%
64LoRA 6.7h 1.00 × 58.1% 66.4% 68.5%
MoRA 9.7h 1.45 × 57.2% 66.4% 69.2%
RaSA 6.9h 1.03 × 56.6% 65.4% 67.9%
GraLoRA 7.2h 1.07 × 60.5% 71.2 % 72.6%
128LoRA 7.0h 1.00 × 55.8% 64.8% 68.6%
MoRA 9.9h 1.41 × 52.8% 62.3% 65.3%
RaSA 7.6h 1.09 × 57.5% 65.5% 67.5%
GraLoRA 7.7h 1.10 × 64.3% 71.7 % 73.7%
Table 3: Commonsense reasoning accuracy across models and tasks. All values are percentages; bold
indicates the best performance per row. HS means HellaSwag, and WG WinoGrande.
Model Method BoolQ PIQA SIQA HS WG ARC-c ARC-e OBQA Avg.
Qwen2.5–1.5BLoRA 66.5% 84.0% 74.9% 83.6% 73.7% 75.2% 88.1% 83.4% 78.7%
MoRA 65.9% 82.2% 74.7% 82.6% 73.4% 72.6% 86.5% 82.8% 77.6%
RaSA 67.5% 83.7% 75.7% 85.3% 72.9% 76.4% 89.8% 83.8% 79.4%
GraLoRA 67.2% 84.2% 75.9% 85.7% 73.8% 77.5% 89.9% 84.4% 79.8%
Qwen2.5–7BLoRA 72.3% 88.2% 79.2% 92.9% 84.7% 84.0% 93.6% 89.6% 85.6%
MoRA 69.9% 85.3% 78.5% 83.7% 81.4% 77.5% 88.6% 85.0% 81.2%
RaSA 72.0% 88.5% 78.9% 93.6% 81.8% 86.1% 94.2% 90.2% 85.7%
GraLoRA 73.4% 89.7% 79.0% 93.0% 84.0% 86.9% 94.5% 90.6% 86.4%
LLaMA3.1–70BLoRA 81.7% 93.4% 82.2% 97.5% 93.1% 90.2% 96.5% 95.6% 91.3%
GraLoRA 83.1% 94.7% 83.6% 97.9% 93.8% 92.3% 97.8% 96.2% 92.4%
4.2 Results on Code Generation
As shown in Table 2, GraLoRA outperformed LoRA, MoRA, and RaSA across all tested ranks for
Pass@1 accuracy. At rank 64, GraLoRA achieved an absolute improvement of +2.4% in Pass@1,
+4.8% in Pass@5, and +4.1% in Pass@10 over LoRA. At rank 128, the gains were even more
pronounced, with increases of +8.5% in Pass@1, +6.9% in Pass@5, and +5.1% in Pass@10. Notably,
while other methods struggled to fully utilize the increasing rank capacity—often reaching perfor-
mance plateaus at lower ranks—GraLoRA maintained a consistent upward trajectory, effectively
overcoming the limitations of LoRA.
Even in low-rank settings (e.g., rank 16), where expressive capacity is typically constrained, the
hybrid variant of GraLoRA demonstrated superior performance. These improvements highlight
GraLoRA’s enhanced capability to preserve diverse gradient signals and resist suppression from
dominant outliers. The strong results on the HumanEval+ benchmark further underscore the benefits
of fine-grained adaptation in tackling complex, high-precision code generation tasks.
4.3 Results on Commonsense Reasoning
As shown in Table 3, GraLoRA outperformed other methods across a wide range of models and tasks.
Notably, GraLoRA demonstrated superior performance across models of varying scales, achieving a
1.1% improvement in average accuracy on both Qwen2.5-1.5B and LLaMA3.1-70B. It also delivered
a 0.9% gain on the widely used mid-sized model, Qwen2.5-7B.
Furthermore, GraLoRA achieved the best results on 20 out of 24 tasks, consistently outperforming
alternatives across benchmarks. These results support our analysis in Section 3.3, showing that
8

--- PAGE 9 ---
Figure 8: (a) GraLoRA ksweep results and (b) Hybrid GraLoRA Ratio sweep results for
LLaMA3.1–8B on code generation task. Ratio 0 implies default GraLoRA and ratio 1 implies
vanilla LoRA in (b).
GraLoRA’s localized updates enhance alignment with FFT and promote robust generalization in
multi-aspect reasoning tasks.
4.4 Ablation Study
GraLoRA kSweep We evaluated the impact of varying kon code generation accuracy. As shown
in Figure 8 (a), k= 2yielded the best performance at rank 32, while k= 4was optimal at rank 128.
These results are consistent with the theoretical prediction that a smaller kis preferable for lower
ranks, as reduced sub-block rank can be particularly detrimental when the overall rank is limited.
Hybrid GraLoRA Ratio Sweep We assessed performance across different LoRA-to-GraLoRA
rank allocation ratios for the Hybrid GraLoRA configuration (Figure 8 (b)). At rank 16, partially
allocating the rank to LoRA led optimal accuracy. However, for larger ranks, allocating rank to
LoRA resulted in degraded performance. This suggests that Hybrid GraLoRA is advantageous in
low-rank regimes, where the sub-block rank of GraLoRA alone may be insufficient. In contrast,
under higher-rank settings where GraLoRA’s sub-blocks are expressive enough, introducing LoRA
components may lead to gradient entanglement, thereby hindering effective learning.
5 Conclusion
In this work, we introduced GraLoRA , a novel PEFT method that extends LoRA with granular, block-
wise decomposition. Motivated by a rigorous analysis of LoRA’s gradient behavior, we identified that
input outliers can dominate the low-rank update, suppressing meaningful contributions from other
input channels and misaligning with the localized gradient propagation observed in FFT.
GraLoRA addresses this limitation by dividing the adaptation space into k2independently trained
low-rank adapters, enabling spatially localized and context-aware updates. Our theoretical analysis
shows that this design increases expressivity by a factor of k, without additional parameters or
computational cost. Moreover, under outlier activations, GraLoRA effectively mitigates the global
gradient distortion seen in vanilla LoRA and better preserves inter-channel balance. Empirically,
GraLoRA consistently outperforms standard LoRA and strong baselines such as RaSA across diverse
tasks and model scales. On the code generation benchmark HumanEval+, it achieves up to +8.5%
absolute gain in Pass1. In commonsense reasoning, GraLoRA delivers improvements across all tasks,
with especially strong results on multi-hop and structured reasoning benchmarks.
Future Work. While GraLoRA improves gradient locality and expressive power, its current design
assumes uniform partitioning. Future extensions may explore adaptive or learned partitioning
schemes, sparsity-aware block activation, or task-driven dynamic rank allocation. Additionally,
applying GraLoRA to vision transformers, multimodal architectures, or continual learning setups
may further highlight its potential for robust and efficient model adaptation.
Overall, GraLoRA represents a principled and practical step forward in the design of PEFT methods,
bridging the gap between global low-rank reparameterization and local, fine-grained adaptation.
9

--- PAGE 10 ---
References
[1]Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro
von Werra. A framework for the evaluation of code generation models. https://github.
com/bigcode-project/bigcode-evaluation-harness , 2022.
[2]Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard,
Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney,
and John P. Cunningham. LoRA learns less and forgets less. Transactions on Machine Learning
Research , 2024.
[3]Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard,
Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns
less and forgets less. arXiv preprint arXiv:2405.09673 , 2024.
[4]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence , volume 34, pages 7432–7439, 2020.
[5]Kerim Büyükakyüz. Olora: Orthonormal low-rank adaptation of large language models. arXiv
preprint arXiv:2406.01775 , 2024.
[6]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In
Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) , pages 2924–2936, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics.
[7]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. arXiv preprint arXiv:1803.05457 , 2018.
[8] Aaron Grattafiori et al. The llama 3 herd of models, 2024.
[9]Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang,
Wenxiang Jiao, Zhuosheng Zhang, and Rui Wang. RaSA: Rank -sharing low -rank adaptation. In
Proceedings of the 2025 International Conference on Learning Representations (ICLR) , 2025.
[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
for nlp, 2019.
[11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
[12] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya
Poria, and Roy Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large
language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing , pages 5254–5276, 2023.
[13] Qiushi Huang, Tom Ko, Zhan Zhuang, Lilian Tang, and Yu Zhang. HiRA: Parameter -efficient
hadamard high -rank adaptation for large language models. In Proceedings of the 2025 Interna-
tional Conference on Learning Representations (ICLR) , 2025.
[14] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei
Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. MoRA: High -rank updating for
parameter-efficient fine-tuning. arXiv preprint arXiv:2405.12130 , 2024.
[15] Dawid J. Kopiczko, Tijmen Blankevoort, and Yuki M. Asano. VeRA: Vector -based random
matrix adaptation. In Proceedings of the 2024 International Conference on Learning Represen-
tations (ICLR) , 2024.
10

--- PAGE 11 ---
[16] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Outlier-
aware weight quantization for efficient fine-tuning and inference of large language models. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 13355–13364,
2024.
[17] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing
Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models
with evol-instruct. In The Twelfth International Conference on Learning Representations , 2024.
[18] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular
vectors adaptation of large language models. Advances in Neural Information Processing
Systems , 37:121038–121072, 2024.
[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering, 2018.
[20] Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth,
and Sepp Hochreiter. One initialization to rule them all: Fine-tuning via explained variance
adaptation. arXiv preprint arXiv:2410.07170 , 2024.
[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,
2021.
[22] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-
monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728 , 2019.
[23] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approxima-
tion. Advances in Neural Information Processing Systems , 37:54905–54931, 2024.
[24] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empower-
ing code generation with OSS-instruct. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller,
Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of
the 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine
Learning Research , pages 52632–52657. PMLR, 21–27 Jul 2024.
[25] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
Smoothquant: Accurate and efficient post-training quantization for large language models,
2024.
[26] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient
fine-tuning methods for pretrained language models: A critical review and assessment, 2023.
[27] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint
arXiv:2412.15115 , 2024.
[28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
11

--- PAGE 12 ---
A Rank Analysis in Real-World Scenarios
Table 4: Average rank size in each projection layer across LoRA and GraLoRA variants. Rank rwas
set to 128 in all methods.
q_proj k_proj v_proj o_proj up_proj down_proj gate_proj
LoRA 128 128 128 128 128 128 128
GraLoRA (k=2) 256 256 256 256 256 256 256
GraLoRA (k=4) 512 512 512 512 512 512 512
GraLoRA (k=8) 1024 1016 1022 1024 1024 1024 1024
As shown in Table 4, GraLoRA denoted linearly increasing ranks as the kincreased. The observation
aligns with our theoretical analysis that increasing GraLoRA kleads to higher expression power by
increasing the latent space from rtokr.
B Gradient Distribution of LoRA and GraLoRA
Figure 9: Comparison of gradient distributions under outlier activation for rank 32, 64, and 128 in
LLaMA3.1-8B Layer 1 down-projection matrix.
Figure 9 displays gradient distributions of LoRA and GraLoRA for varying ranks. In GraLoRA, only
the blocks interacting with the outlier exhibit elevated gradients, structurally solving the gradient
entanglement discovered in vanilla LoRA. This enables to mitigate global distortion and align with
FFT behavior in all ranks.
12

--- PAGE 13 ---
C Precise Analysis on Computation Overhead
Figure 10: Computation workflow in GraLoRA is composed of 3 steps: two sub-block matrix
multiplications and a following matrix addition.
In the previous “Computation Overhead Analysis” section 3.4 we compared the computation of
LoRA and GraLoRA with the big Onotation on the two major matrix multiplication steps. In this
section we further examine the exact computation requirement and compare their efficiency.
LoRA FLOPs LoRA performs the projection in two sequential steps to leverage its low-rank structure.
In the first step, the computation of A⊤X∈Rr×Trequires (2N−1)rTFLOPs. In the second step,
the reconstruction B(A⊤X)∈RM×Tincurs (2r−1)MT FLOPs. Therefore, the total FLOPs for
LoRA is:
LoRA FLOPs = (2N−1)rT+ (2r−1)MT
= 2r(M+N)T−(r+M)T.
GraLoRA FLOPs In practice, GraLoRA computations can be divided into three stages, involving
k2adapter blocks: two matrix multiplications followed by a matrix addition as shown in Figure 10.
In the first stage (projection), each adapter block computes A⊤
i,jXj∈Rr
k×T, which requires 
2n
k−1r
kTFLOPs. Since there are k2such blocks, the total FLOPs for this step is (2n−k)rT.
In the second stage (reconstruction), each adapter block performs Bi,j(A⊤
i,jXj)∈Rm
k×T, which
costs 
2r
k−1m
kTFLOPs. With k2blocks, the total becomes (2r−k)mT.
The final stage involves aggregating the outputs across kprojections for each row:
kX
j=1Bi,j(A⊤
i,jXj)∈Rm
k×T,
which requires m
k×T
(k−1) =mT(k−1)
kFLOPs per row. Across krows, the total cost becomes
(k−1)mT.
Combining all three stages, the total FLOPs for GraLoRA is:
GraLoRA FLOPs = (2n−k)rT+ (2r−k)mT+ (k−1)mT
= 2r(m+n)T−k(r+m)T+ (k−1)mT
= 2r(m+n)T−krT−mT.
This can also be expressed as:
GraLoRA FLOPs =LoRA FLOPs−(k−1)rT,
demonstrating that GraLoRA introduces reduced computation compared to LoRA.
13

--- PAGE 14 ---
D Baselines and Hyperparameters
Baseline Methods . We compared GraLoRA with three baseline methods. Key idea for each method
is as follows:
•LoRA freezes pretrained model weights and injects trainable low-rank matrices into selected
layers, allowing efficient fine-tuning with significantly fewer parameters, approximating
weight updates as a product of two small matrices.
•MoRA employs a single square matrix instead of low-rank matrices to achieve high-rank
updating while maintaining the same number of trainable parameters.
•RaSA enhances LoRA by sharing partial low-rank components across layers while keeping
layer-specific updates.
Table 5: Hyperparameter settings for each method and dataset.
Task Model Method Rank LR Batch size Epochs
Code Generation LLaMA3.1–8BLoRA
{16, 32, 64, 128} 2e-4 192 2MoRA
RaSA
GraLoRA
Commonsense ReasoningQwen-2.5-1.5BLoRA
64 2e-4 192 2MoRA
RaSA
GraLoRA
Qwen-2.5-7BLoRA
64 4e-4 192 2MoRA
RaSA
GraLoRA
LLaMA3.1–70BLoRA64 3e-4 192 1GraLoRA
For hyperparameter, we fixed LoRA α= 2rwhich is known to be generally applicable in different
models with different ranks [ 2]. Detailed hyperparameter settings for our experiments are denoted in
Table 5.
14
