# 2405.15525.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2405.15525.pdf
# File size: 1295782 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparse Matrix in Large Language Model Fine-tuning
Haoze He∗
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
haozeh@cs.cmu.eduJuncheng Billy Li∗
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
junchenl@cs.cmu.edu
Xuan Jiang
College of Engineering
University of California, Berkeley
Berkeley, CA 94720
xuanjiang@berkeley.eduHeather Miller†
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
heather.miller@cs.cmu.edu
Preprint. Under review.
Abstract
LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT)
methods due to their ability to avoid excessive computational costs. However,
an accuracy gap often exists between PEFT methods and full fine-tuning (FT),
and this gap has yet to be systematically studied. In this work, we introduce a
method for selecting sparse sub-matrices that aim to minimize the performance
gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning
computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method
begins by identifying the most significant sub-matrices in the gradient update,
updating only these blocks during the fine-tuning process. In our experiments, we
demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA
and DoRA) in fine-tuning popular large language models such as LLaMA across
a broad spectrum of tasks, while reducing the GPU memory footprint by 67%
compared to FT. We also examine how the performance of LoRA and DoRA tends
to plateau and decline as the number of trainable parameters increases, in contrast,
our SMT method does not suffer from such issue.
1 Introduction
While the inherent generalization capability of Large Language Models (LLMs) is impressive,
enhancing performance on downstream tasks often still necessitates fine-tuning Ding et al. [2022],
Chung et al. [2022]. However, as the size of these LLMs increases, there is a pressing challenge
to optimize the fine-tuning process for better computational efficiency and memory utilization. For
example, fine-tuning a pre-trained LLaMA 7B model without CPU offloading3requires at least 58 GB
of GPU vRAM—13.6 GB for trainable parameters, 40 GB for Adam optimizer states and gradients,
and 2 GB for activation. This requirement makes fine-tuning on consumer-level GPUs such as the
NVIDIA RTX 4090 with 24 GB of memory impractical Zhao et al. [2024].
To address the prohibitive computational challenges of full parameter fine-tuning, many parameter-
efficient fine-tuning (PEFT) methods have emerged over the past two years. LoRA and its variants
∗Equal Contribution
†Corresponding Author
3Although some libraries such as Deepspeed can move the optimizer memory cost to CPU, it will also slow
down the fine-tuning with extra I/O communication time Rajbhandari et al. [2020], Aminabadi et al. [2022].arXiv:2405.15525v3  [cs.CL]  19 May 2025

--- PAGE 2 ---
Hu et al. [2021], Zhao et al. [2024], Dettmers et al. [2024], Liu et al. [2024b,a] utilize the weight
low-rank adaptation method and successfully reduce both the optimizer memory and computational
cost. However, even in state-of-the-art (SoTA) PEFT research, results show a notable performance
gap between low-rank adaptation methods and full parameter tuning across many datasets Liu et al.
[2024a]. Additionally, in this work, we report a less recognized phenomenon: low-rank adaptation
PEFT methods experience a performance plateau and subsequent decline as the parameter count
(rank r) increases. Surprisingly, even with more trainable parameters, performance decreases.
On the other hand, previous studies have extensively analyzed the internal logic of LLMs. Some
knowledge editing methods, such as Constrained fine-tuning Zhu et al. [2020], ROME Meng et al.
[2022a], and MEMIT Meng et al. [2022b], have shown that LLMs have memory sections located
in distinct layers. These memories can be modified via fine-tuning Zhu et al. [2020]. These
works observed that domain-specific knowledge can be distributed separately and sparsely among
layers. Motivated by these observations, and to address the challenges of PEFT mentioned above,
we proposed a Sparse Matrix Tuning(SMT) approach. By applying matrix sparsity, we aim to
identify and fine-tune the most relevant memory sections efficiently. Contrary to Geva et al. [2020,
2022]’s claim that transformer’s MLP layers primarily serve as key–value memories, we empirically
demonstrate that attention mechanisms, particularly the value vector, store the largest number of
memories and are the most influential during fine-tuning.
In our experiments, our Sparse Matrix Tuning (SMT) approach achieves better performance compared
to LoRA and DoRA using same amount of trainable parameters. Additionally, SMT closes the
accuracy gap between full fine-tuning, overcomes the performance plateau of low-rank adaptation
PEFT methods, and significantly outperforms LoRA and DoRA while utilizing less than 5% of
trainable parameters. Our experimental results show that SMT consistently outperforms SoTA
PEFT (including LoRA and DoRA) methods by 2+ points when fine-tuning popular LLMs (e.g.
LLaMA series base model4) on commonsense reasoning and arithmetic reasoning benchmarks. Our
experimental results show that SMT consistently outperforms DoRA, such as commonsense reasoning
(+3.0/+2.8 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +2.0 on LLaMA3-8B) and arithmetic
reasoning (+2.3 on LLaMA-7B). In addition, SMT eliminates the accuracy gap between SMT and full
fine-tuning, overcomes the plateau of low-rank adaption PEFT methods, and significantly exceeds
performance of LoRA and DoRA with a small proportion of trainable parameters(5%<). For layers
without selected sub-matrices, SMT freezes these layers, saving all their backward propagation
computational cost, parameters update computational cost, optimizer memory cost, and activation
memory cost. For layers with selected sub-matrices, SMT reduces the computational costs of
backward propagation and parameter updates, as well as the optimizer and activation memory costs,
to less than 1% of those incurred by standard full tuning (FT).
Below are our key contributions:
•PEFT SOTA Algorithm We propose a novel fine-tuning method (SMT) which achieves
state-of-the-art performance in parameter-efficient fine-tuning, effectively closing the gap
between SMT and full fine-tuning. In contrast, LoRA and DoRA’s performance saturation
is faster than that of SMT as the number of trainable parameters grows.
•Language Model Anatomy: We investigate the distinct impacts of attention mechanisms
versus MLPs (Multi-Layer Perceptrons) in LLMs. Our findings indicate that attention layers
are more critical than MLPs for downstream performance. Among the Q,K,V vectors of
attention layers, we found V to be the most influential for performance.
•Large Language Model System Efficiency: The SMT implementation significantly reduces
the computational cost of backward propagation, parameter updates, optimizer memory, and
activation memory during fine-tuning. Our implementation is open source.
2 Background and Related Works
Many works on parameter-efficient fine-tuning (PEFT) Mangrulkar et al. [2022] have aimed to
improve efficiency and performance by only fine-tuning lower-dimensional parameterization of
model weights. Notable examples include LoRA Hu et al. [2021], DoRA Liu et al. [2024a], QLoRA
Dettmers et al. [2023], and several other variants Liu et al. [2024b], Dettmers et al. [2023]. However,
4Base model is not yet instruction tuned
2

--- PAGE 3 ---
the results of these works still indicate a performance gap between PEFT methods and full fine-tuning
(FT). Concurrent research Biderman et al. [2024] empirically demonstrated that such a gap is difficult
if not impossible to eliminate, they also notice the performance saturation issue of LoRA, as we will
discuss in Section §4.3.
Figure 1: Differences between low-rank adaption method LoRA and SMT. Upper picture dedicates
adaption approach in LoRA and lower picture represents the sub-matrices sparsity approach in SMT.
Beside low-rank adaptation methods, sparsity-inspired approach is a natural alternative to reduce
computational costs and memory footprint. As they have been recently applied to accelerate LLM
inference, H 2O Zhang et al. [2024] leveraged sparsity in KV cache eviction policy; DeepSparse
Kurtic et al. [2023] used L2-based distillation to promote sparsity. In the case of sparsity-inspired
fine-tuning, Song et al. [2023] developed a Sparse Increment Fine-Tuning (SIFT) approach to reduce
GPU memory cost. However, SIFT Song et al. [2023] still requires full backward propagation to
calculate all gradients, having no speed advantages compared to full fine-tuning(FT). Moreover, SIFT
maps discontinuous memory gradients to continuous memory addresses, creating a significant time
bottleneck and resulting in longer fine-tuning time than FT. Our work builds upon existing strategies,
but unlike previous approaches, our matrix sparsity method directly incorporates task-specific gradient
information to dynamically adjust the sparsity level for optimization, and we achieve speedup as we
describe in Section §3.3.
Suppose we are given a pre-trained auto-regressive language model PΦ(y|x)parameterized by
Φ. Each downstream task is represented by a training dataset of context-target pairs: Z=
(xi, yi)i=1,...,N, where both xiandyiare sequences of tokens. Equation (1) dedicates the LoRA Hu
et al. [2021] fine-tuning process to maximize the conditional language modeling objective, which
uses a low-rank representation to encode task-specific parameters. Specifically, LoRA freezes the
pre-trained model weights and injects trainable rank decomposition matrices into each layer of the
Transformer architecture. This is formulated as ∆Φ = ∆Φ(Θ) , where Θrepresents a much smaller-
sized set of parameters with |Θ| ≪ |Φ0|. The resulting increment ∆Φcan be as small as 0.01% of
the pre-trained weights parameter size |Φ0|in gradient updates. This greatly reduces the number of
trainable parameters and the GPU memory requirement while maintaining or even enhancing model
performance.
max
ΘX
(x,y)∈Z|y|X
t=1log(PΦ0+∆Φ(Θ) (yt|x, y<t)) (1)
In our work, our proposed Sparse Matrix Tuning(SMT) uses matrix sparsity as the parameter-efficient
approach. In SMT’s case, reusing Equation( 1), the Θrepresents the sub-matrices within the sparse
weight matrices. SMT only fine-tunes sparse sub-matrices Θinstead of fine-tuning the whole pre-
trained weight. Figure 1 illustrates the differences between weight low-rank adaption method LoRA
and our proposed sparse matrices tuning approach SMT. For a pre-trained weight matrix W0, LoRA
constrains its update by representing the latter with a low-rank decomposition W0+∆W=W0+BA,
where B∈Rd×r,A∈Rd×r, and the rank r≪min(d, k). In SMT, we slice the pre-trained weight
intoNsub-matrices and only fine-tune selected Msub-matrices. The dimension of sub-matrices is
l×l, the total number of sub-matrices Nin a pre-trained weight is N=d×k
l×l. SMT constrains its
update by representing the latter with a sparse gradient matrix ∆WM,W0+ ∆W=W0+ ∆WM,
where the number of fine-tuning sub-matrices m≪N.
Since our proposed SMT method focuses on fine-tuning sub-matrices which are most relevant to
downstream tasks’ performance, identifying these sub-matrices is non-trivial. Previous works Zhu
et al. [2020], MEMIT Meng et al. [2022b], and Geva et al. [2020, 2022] indicated that feed-forward
3

--- PAGE 4 ---
MLP layers of the LLMs are most influential. However, through our experiment analysis§ 5.1, we
found attention layers to be more relevant for performance than MLP layers.
3 Methodology
3.1 Selecting the most influential sparse sub-matrices
Figure 2: (a) A sparse weight matrix W. The green sub-
matrices with significant gradients can be updated. (b)
Backward propagation calculation for partial gradient
for weight matrix w. (c) Computation graph in auto-
differential systems.Table 1: The experiments involved Full
Fine-Tuning, SMT, LoRA, and DoRA
on 4×A100 40GB GPUs using data par-
allel, with a batch size of 16. Communi-
cation between the GPU and CPU was
facilitated via PCIe-G4.
LLaMA-7B
PEFT method #Params% Time/s Speedup
Full Fine-tuning 100 243.84 1 ×
SMT 1.26 16.68 14.6 ×
LoRA 1.26 17.82 13.6 ×
DoRA 1.27 18.04 13.5 ×
Our methodology centers around the application of a matrix sparsity approach during fine-tuning
LLMs. Specifically, we select certain sub-matrices in weight matrices within the model’s weight
matrices that exhibit maximal gradient changes during a 100 iterations warm-up phase (Fig. 2.a) at
the beginning of fine-tuning. Intuitively, our approach aims to select and modify the sub-matrices
most relevant to the fine-tuning sub-task. When fine-tuning LLMs, SMT improves computational
efficiency and reduces memory needs during fine-tuning by not updating a portion of weight matrices
after the warm-up phase and storing sparse weights in compressed form.
First and for most, SMT reduces the backward computation costs with respect to weight to 0.5%
of those associated with Full Fine-tuning (FT). SMT achieves this by decreasing the computation
cost of gradients during backward propagation, as gradients are calculated for only a subset of the
weights. For linear layers in LLMs, where Z=Wx, the gradients with respect to weight matrix W
and input x can be calculated as Equation (2):
∇xf(x) =∂l
∂Z·W;∇Wf(x) =∂l
∂Z·x (2)
where ∂l/∂z is the gradient information from backward propagation in (Fig. 2.b,c). ∇wf(x)is the
gradient matrix and and xis the activation in the (Fig. 2.b). (Fig. 2.b) also illustrates that only partial
backward computations are necessary when we update selected sparse matrices. To calculate the
sub-matrix gradient (highlighted in yellow), it is only necessary to multiply the yellow row in ∂l/∂z
with the yellow column in the activation x. Similarly, to calculate the green sub-matrix gradient, we
only need to multiply the green row in ∂l/∂z with the green column in activation x. Note that in
backward propagation, we can only reduce computation when derivative to gradient matrix wamong
as illustrated by the green arrows in (Fig. 2.c). but not other necessary computation. (black arrows)
Besides, SMT reduces the activation memory cost for the in forward pass to 0.5%. Since SMT only
computes the partial gradient, it only saves the relevant portions of activation Xnecessary for the
gradient calculation as is represented in Equation. 2. In (Fig. 2.b), to calculate the green and yellow
gradients in the gradient matrix, we only need to save the yellow and green columns of the activation
X. This approach reduces the memory cost for the forward pass of the selected linear layer.
In addition, SMT reduces the memory cost of the optimizer gradients to 0.5%. Since SMT only
updates selected sparse sub-matrices, only partial gradients are stored. This approach significantly
cuts the memory cost of the Adam optimizer to 0.5%. This reduction is crucial because the memory
cost of the Adam optimizer is typically twice the size of the model, which often consumes the majority
of GPU RAM.
4

--- PAGE 5 ---
Furthermore, SMT reduces the gradient step computation cost to 0.5%. By updating selected sparse
sub-matrices, SMT performs partial gradient steps and significantly reduce step computation cost.
In SMT, all the layers except selected Q, K, and V vectors are frozen during fine-tuning. By
doing this, SMT avoids all the weight backward propagation computational cost, parameters update
computational cost, optimizer memory cost, and activation memory cost in frozen layers. The
rationale for fine-tuning only the Q, K, and V vector is detailed in Section §5.1.
By applying sparse sub-matrix fine-tuning, SMT can reduce the fine-tuning memory cost of LLaMA-
7B and LLaMA2-7B to less than 20GB and fit the fine-tuning into a 3090 24GB GPU. We also reduce
the computation and achieve faster fine-tuning compared with FT and LoRA/DoRA, Section § 3.3
provides more details.
3.2 Implementation
In SMT, we first sum up gradient from the attention linear layers in every single warm-up iterations.
The sum up gradient information are used to identify task-specific sparse blocks. After the warm-up
steps, we average the absolute values within the sub-matrices, select the sub-matrices with the
largest value, and save the indices for the selected sub-matrices. In all of our experiments, we use
l×l= 256 ×256as sub-matrices block size. During the warm up steps, we can apply offload
Rajbhandari et al. [2020] on memory constraint GPU devices. Since SMT requires fewer than
100 warm-up steps in our experiments, it does not become a bottleneck during fine-tuning epochs.
Additionally, SMT implements a custom sparse linear layer to ensure that unselected gradients are
not calculated, saved, and updated (Code Snippet 6). We replace the selected linear layers with these
customized sparse linear layers.
The custom sparse linear layer applies a specialized sparse linear multiplication function , integrated
into our customized sparse linear layers (Code Snippet 7). This function calculates partial weight
gradients based on the input, weight, and selected weight index. It significantly reduces the computa-
tional cost of backward propagation weight gradients to just 0.5% and minimizes the memory usage
of the returned partial gradients to only 0.5%.
The specialized sparse linear multiplication function rewrites both forward and backward functions.
In the forward pass (Code Snippet 7) of sparse linear multiplication function, we only save selected
activation xusing ctx.save_for_backward() , and in the backward pass (Code Snippet 8), we
customize matrix multiplication to calculate the needed partial gradients given partial input and
gradient index(shown in Figure 2(b)). It is important to note that we do not use Sparse Matrix-Matrix
Multiplication(SPMM)5because we concatenate the selected sparse sub-matrices and formed a
m×l×ldense matrix as illustrated in right part of figure 1. This would not cost additional time since
memory allocations remain continuous within each sub-matrix. Despite employing matrix sparsity,
we still leverage the advantages of dense matrix multiplication.
Furthermore, SMT gathers sparse matrix but still leverages dense matrix. SMT customizes the
function for gathering trainable parameters. This customized function selectively gathers weight
sub-matrices in the Q, K, and V vector layers and passes them to the Adam optimizer. By continuing
to use the well-designed FusedAdam from the deepspeed library Aminabadi et al. [2022], we
maintain the computational speed of dense matrix weight updates. However, our approach reduces
the gradient memory cost in the optimizer to just 0.5%.
3.3 Memory and Computation Saving: SMT vs. Low-rank Adaption Methods
SMT is more computational efficient than weight low-rank adaption method when the number of
trainable parameters are the same, weight low rank adaption methods need to maintain additional
adapters, which require additional forward computation. For instance, since LoRA maintains adaptors
AandB, and the forward propagation is:
h=W0x+ ∆Wx=W0x+BAx (3)
where the term BAx requires additional forward propagation calculation, which is cut off in SMT.
Regarding memory cost, since SMT does not require additional low-rank adapters AandB, SMT can
achieve lower memory cost than LoRA and DoRA under the same amount of trainable parameters
5Sparse Matrix-Matrix Multiplication (SPMM) is significantly slower than General Matrix Multiply (GeMM)
5

--- PAGE 6 ---
setting. We illustrate this in Figure 1, by abandoning additional adaptor weight AandB, SMT can
achieve lower memory cost. Taking the popular LLaMA-13B model as an example, since the model
size is approximately 25 GB, if we fine-tune 1% of parameters, SMT can potentially save 250MB
GPU meory. In Table 1, we provide the fine-tuning time costs for SMT, Full Fine-tuning, LoRA, and
DoRA. SMT achieves an 14.6 ×speedup compared to Full Fine-tuning and outperforms both LoRA
and DoRA. We conducted time profiling by averaging the fine-tuning time every 10 iterations over
1000 iterations, following a 500-iteration warm-up period. Full fine-tuning utilized offload settings to
accommodate the LLaMA model, which employs the Adam optimizer, within the 40GB GPU.
4 Experiments and Results
4.1 Experimental Settings
Model Architecture and Dataset: In our experimental setup, we use open-weight LLaMA-7B,
LLaMA-13B, LLaMA2-7B, and LLaMA3-8B models AI@Meta [2024]. In Subsection§ 4.2§ 4.3, We
perform fine-tuning on the Common Sense Reasoning tasks with 8 sub-tasks, each with a predefined
training and testing set. We follow the setting of Hu et al. [2023], Liu et al. [2024a] and amalgamate
the training datasets from all 8 tasks to create the final training dataset commonsense_170k and
conduct evaluations on the individual testing dataset for each task. We calculate an average score to
encapsulate the overall efficacy. In Subsection§ 4.4, we perform fine-tuning on Math10K Hu et al.
[2023] dataset which includes MultiArith ,GSM_8K Cobbe et al. [2021], AddSub ,AQuA ,SingleEq ,
SVAMP datasets and evaluate the efficiency on their testsets.
Training Framework and SMT Hyper-parameters: We used the DeepSpeed Aminabadi et al.
[2022] library for fine-tuning and accelerate Gugger et al. [2022] library for inference evaluation.
Both training and fine-tuning are using dtype bf16. All experiments are fine-tuned for 3 epochs. In
all our experiments in Section§ 4, sub-matrices are selected in blocks of size l= 256 . We choose this
specific sub-matrix dimension lbecause it is the largest common factor of the column and row sizes of
all linear layers in the LLaMA series models, using this dimension for slicing avoids remainder issues.
We freeze all MLP layers and apply SMT only to the Q, K, and V vectors in the attention mechanism.
In Section§5.1, we explain the rationale why we only apply SMT to attention mechanism instead
of MLP. At the end of the gradient warm-up iteration, SMT ranks the average absolute gradient
values within each sub-matrix and selects those with the highest average values. The rationale of
such selection is explained more in detail in Appdendix B. We apply 100 warm-up iterations to all
SMT experiments on Commonsense dataset and apply 25 warm-up iterations to all SMT experiments
onMath10K dataset.
PEFT Baselines: For state-of-the-art (SOTA) baselines, we choose to include LoRA Hu et al. [2021]
and DoRA Liu et al. [2024a], both of them focus on fine-tuning using low-rank adaptation method.
Computational Resources: We conduct our experiments and SOTA baselines of LoRA Microsoft
[2021] and DoRA Shih-yang [2024] to fine-tune LLaMA-7B and LLaMA2-7B model with 4 NVIDIA
A100_40GB GPUs and fine-tune LLaMA-13B and LLaMA3-8B model with 4 NVIDIA A100_80GB
GPUs. Communication between the CPU and GPU was facilitated via PCIe-G4 and communication
between GPUs was facilitated via Nvlink-3.
Evaluation Metrics: We have evaluated the performance of SMT in terms of computational effi-
ciency (wall-clock time speedup), memory usage (analysis for memory complexity) in methodology
Section§ 3. In this section, we will mainly evaluate SMT in terms of popular NLP tasks to test its
ability to generalize to all downstream tasks. In Subsection§ 4.2§ 4.3, we evaluate the performance
of SMT on 8 tasks in the Commonsense dataset, including BoolQ ,PIQA ,SIQA ,HellaSwag ,ARC-e ,
ARC-c , and OBQA , and we calculate an average score to encapsulate the overall efficacy.In Subsec-
tion§ 4.4, we perform fine-tuning on Math10K Hu et al. [2023] dataset which includes MultiArith ,
GSM_8K ,AddSub ,AQuA ,SingleEq ,SVAMP datasets and evaluate the efficiency of SMT on their
testsets. All of the experiments are evaluated using accuracy.
4.2 Commonsense Reasoning
We evaluate SMT against the state-of-the-art(SoTA) weight low-rank adaptor method includes LoRA
and DoRA. To ensure a fair comparison, we fine-tuned model with SMT following the LoRA and
DoRA configuration. We ensure all the hyper-parameters including batch size, data type, learning
6

--- PAGE 7 ---
Table 2: Accuracy comparison of LLaMA 7B, LLaMA 13B, LLaMA2 7B, and LLaMA3 8B with
various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods
on LLaMA 7B, LLaMA 13B, LLaMA2 7B, LLaMA3 8B are taken from Liu et al. [2024a]. Results
of all SMT are obtained using the hyper-parameters described in Liu et al. [2024a] under the same
settings. Bold texts dedicate the performance of SMT under the same numbers of parameters where
LoRA and DoRA achieve the best performance. Blue texts dedicate the best performance of SMT.
Please note that the performance of LoRA and DoRA under larger numbers of trainable parameters
can be found in Table 3.
Model PEFT method #Params% BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA A VG
ChatGPT(175B) - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0
LLaMA-7BLoRA(Best) 0.83 67.5 80.8 78.2 83.4 80.4 78.0 62.6 79.1 76.3
DoRA(Best) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4
SMT 0.84 68.7 81.7 78.3 91.6 78.8 84.1 68.7 77.4 78.7
SMT(Best) 4.91 72.0 82.9 80.7 93.3 82.4 86.1 70.6 83.0 81.4
Full Fine-tuning 100 69.9 84.2 78.9 92.3 83.3 86.6 72.8 83.4 81.4
LLaMA-13BLoRA(Best) 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5
DoRA(Best) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5
SMT 0.68 71.1 84.4 81.7 93.7 83.2 86.7 73.7 85.2 82.4
SMT(Best) 4.91 72.6 86.1 81.9 95.0 86.1 88.2 77.1 87.4 84.3
LLaMA2-7BLoRA(Best) 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6
DoRA(Best) 0.42 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5
SMT 0.84 72.0 83.8 80.8 93.3 82.8 86.7 74.0 81.0 81.8
SMT(Best) 4.91 72.6 85.2 82.0 94.4 85.7 87.8 74.5 85.0 83.4
Full Fine-tuning 100 72.8 83.4 78.7 92.7 85.5 86.2 74.7 83.4 82.2
LLaMA3-8BLoRA(Best) 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8
DoRA(Best) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2
SMT 0.71 75.7 88.4 81.4 96.2 88.2 92.7 83.2 88.6 86.8
SMT(Best)) 3.01 75.1 89.9 82.4 96.3 88.8 92.6 82.8 89.6 87.2
rate, and sequence length are identical to what was reported in LoRA and DoRA Hu et al. [2021], Liu
et al. [2024a]. We re-implemented LoRA and DoRA and achieved their best performance reported in
Liu et al. [2024a].
Table 2 demonstrates that SMT consistently surpasses baseline methods across LLaMA-7B,
LLaMA13B, LLaMA2-7B, and LLaMA3-8B. Notably, by overcome plateau phenomenon, SMT
further enhances accuracy of DoRA by 3.0%, 2.8%, 2.9%, and 2% on LLaMA-7B, LLaMA-13B,
LLaMA2-7B, and LLaMA3-8B respectively. Notably, LoRA and DoRA will not achieve better
performance with larger trainable parameters and exhibit the plateau phenomenon. In Subsection§ 4.3,
we report and demonstrate the plateau issue in LoRA and DoRA and demonstrate SMT overcomes
this issue. Moreover, by fine-tuning less than 5% of all parameters, SMT achieves similar accuracy
performance of full fine-tuning while speedup 14.6 ×(speedup details in Table 1) and save 99.5% of
optimizer memory(memory bottleneck in fine-tuning, details discussed in Section§ 3).
SMT can also consistently surpass LoRA and DoRA under the same number of trainable parameters
where LoRA and DoRA achieve the best results, SMT can surpass their performance and also outstrip
ChatGPT-3.5-turbo6. For instance, SMT consistently surpasses DoRA on LLaMA2-7B, LLaMA3-
8B, LLaMA-13B, and LLaMA-7B by 1.3%, 1.6%, 0.9%, and 0.3% respectively, under their best
performance trainable parameter number.
4.3 Plateau in Weight low rank adaption methods
In Table 3 and Figure 3, we scale up the model size and presents how the performance of LoRA and
DoRA will be under larger number of trainable parameters. We reimplement all the experiments of
LoRA Microsoft [2021] and DoRA Shih-yang [2024] using their official repository and followed
their recommendation of hyper-parameters to achieve best performance under every single trainable
parameter size. We observe that both DoRA and LoRA models, with some larger ranks, their
performance slightly degrades. However, SMT continues improving its performance when we
scale up the trainable parameter size. When we scale up the trainable parameter size to 4.91%,
SMT significantly surpass DoRA by 3.8% and 4.9% on LLaMA-7B and LLaMA-2-7B fine-tuned
models. We postulate that such plateau phenomenon of LoRA or DoRA is due to their lossy low-rank
approximation of the full weight information (includes lots of noise), whereas our SMT focuses
6Results of ChatGPT-3.5-turbo are reported in DoRA Shih-yang [2024]
7

--- PAGE 8 ---
on most prominent submatrices (contains less noise) and remains full rank gradient updates for the
selected portion, making SMT performs better.
Figure 3: Accuracy comparison of LoRA, DoRA, and SMT under different scaling of trainable
parameters on commonsense reasoning datasets.
Table 3: Accuracy comparison of LoRA, DoRA, and SMT
under different scaling of trainable parameters on common-
sense reasoning datasets. Given certain base model and PEFT
method, we gradually increase the number of trainable pa-
rameters on each line from left to right. On each line, the
best performing model has∗.
Method 0.43 0.84 1.26 2.50 3.73 4.91
LLaMA-7BLoRA 70.9 76.3∗76.4 75.0 75.3 74.7
DoRA 77.5 78.4∗76.0 77.3 77.5 77.6
SMT 77.3 78.6 79.2 80.2 80.8 81.4∗
LLaMA2-7BLoRA 76.5 77.6∗78.4 77.6 77.3 77.0
DoRA 80.5∗79.7 78.8 77.6 76.8 78.5
SMT 81.1 81.8 81.7 82.2 82.8 83.4∗Table 4: Fine-tuned LLaMA-7B
model performance on Commonsense .
A VG dedicates the average test score
of eight subsets among Commonsense .
MLP% and Attention% presents the
percentage of trainable parameters ap-
ply to MLPs and attention mechanisms
respectively.
Model MLP% Attention% A VG
SMT(0.84%)
LLaMA-7B0.84 0 76.7
0.42 0.42 77.3
0.21 0.63 77.8
0 0.84 78.7
4.4 Other Dataset
To ensure our findings above are generalizable, we further examine the performance of SMT under
arithmetic reasoning dataset, Math10K Hu et al. [2023]. Math10K dataset has six subsets including
GSM8k ,SingleEq ,SVAMP ,MultiArith ,AddSub , and AQuA . More details about Math10K dataset
can be found in Appendix C. To ensure a fair comparison, we follow the open source hyper-parameter
instruction in Hu et al. [2023] to achieve best performance for LoRA and Dora, and apply the same
hyper-parameters to SMT while only fine-tune the learning rate. Table 5 reports the performance
of LoRA, DoRA, and SMT on the Math10K dataset. We can observe that SMT surpasses the best
achievable performance of LoRA and DoRA by 1.3% and 1.1% respectively using the same amount
of trainable parameters. In addition, by scaling up the trainable model size to 1.26%, SMT achieves
better performance and surpasses the best performance of LoRA and DoRA by 2.5% and 2.3%
respectively.
Table 5: SMT, LoRA and DoRA reproduction, and experiment results on Math10K dataset.
Model PEFT method #Params% GSM8k SingleEq SV AMP MultiArith AddSub AQuA A VG
LLaMA-7BLoRA(Best) 0.86 35.4 83.2 52.1 92.8 83.4 18.6 60.9
DoRA(Best) 0.86 35.2 83.7 51.8 92.8 82.8 20.2 61.1
SMT 0.86 34.2 84.6 53.6 91.5 85.8 23.6 62.2
SMT(Best) 1.26 35.6 85.3 54.8 93.4 86.8 24.2 63.4
8

--- PAGE 9 ---
Table 6: K SMT, Q SMT, and V SMT assign all trainable parameters to only K, or only Q, or
only V vectors respectively, and fine-tuned 0.86% of the parameters on LLaMA-7B using the
Commonsense dataset. QKV SMT assign all trainable parameters to QKV vectors and select sub-
matrices automatically.
Model Param location #Params% BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA A VG
LLaMA-7BK SMT 0.84 65.5 79.1 76.2 88.3 73.2 80.3 60.8 68.0 73.9
Q SMT 0.84 65.7 79.3 75.5 88.2 72.5 80.1 59.6 72.5 75.3
V SMT 0.84 68.7 82.1 78.1 91.6 78.8 83.0 68.7 77.2 78.5
QKV SMT 0.84 68.7 81.7 78.3 91.6 78.8 84.1 68.7 77.4 78.7
5 Further Dicussion
5.1 Attention versus MLP
In order to study what components are more critical for LLM’s downstream performance, we conduct
ablation studies that compares MLPs vs. attention layers by adjusting the ratio of their trainable
parameters respectively. We apply SMT and fine-tune 0.86% of parameters on LLaMA-7B using
Commonsense dataset. In Table 4, we present four experiments. In the first row, all trainable
parameters are allocated to MLPs. In the second row, both MLPs and Q, K, V vectors from attention
mechanisms receive 0.43% of trainable parameters. In the third row, 0.62% of trainable parameters
are assigned to Q, K, V vectors from attention mechanisms and 0.21% to MLPs. In the fourth row, all
trainable parameters are dedicated to Q, K, V vectors from attention mechanisms. To guarantee a fair
comparison, all the other hyper-parameters and settings are the same among these experiments.
In these experiments, allocating X% of trainable parameters to MLPs or attention mechanisms
means ranking the average absolute gradient values of each sub-matrix within the MLPs or attention
mechanisms and selecting those with the highest average values until the number of parameters
reaches X%. The results reveal a significant performance gap between the first and fourth rows.
The more trainable parameters we allocate to attention mechanisms, the better the fine-tuned model
performs. When all SMT trainable parameters are applied to attention mechanisms, the model
outperforms the one where all parameters are allocated to MLPs by 2.0%. Our empirical findings
challenge previous assumptions Zhu et al. [2020], Meng et al. [2022a], Geva et al. [2020, 2022] that
the memory sections of large language models are primarily located in feed-forward MLP layers.
5.2 V Vector Versus Q, K Vector
Based on our observation that attention is more critical, in all of our SMT experiments in Section§ 4,
we only allocate SMT trainable parameters to Q, K, V vectors from attention mechanisms. We rank
the average absolute gradient values of every single sub-matrix within attention mechanisms and
select those with the highest average values until the parameter ratio limit is reached. Surprisingly, we
observed that the trainable parameters are predominantly assigned to the V vectors. In our ablation
experiments, we experimented with assigning all trainable parameters to only K, or only Q, or only V
vectors, and fine-tuned 0.86% of the parameters on LLaMA-7B using the Commonsense dataset. As
shown in Figure 5, 95.17% of the trainable parameters are automatically assigned to the V vectors by
SMT. Figure 4 indicates that all V vectors have trainable parameters, while 22 Q vectors and 21 K
vectors are completely frozen. This suggest that the V vectors contain most of the memory and are
the most significant among the Q, K, and V .
Figure 4: A visualization of trainable Q, K, V layers when fine-tuning
0.86% trainable parameters on LLaMA-7B. LLaMA-7B has 32 layers of
MLPs, each contains a Q vector, a K vector, and a V vector. White layers
are frozen and green layers contain trainable parameters.
Figure 5: Distribution
of trainable parameters
among Q, K, V .
9

--- PAGE 10 ---
Table 6 presents four additional experiments where we fine-tuned 0.86% of the parameters of LLaMA-
7B using SMT on the Commonsense dataset. In the first three rows, all trainable parameters are
allocated to the K vectors, Q vectors, and V vector , respectively. In the fourth row, the trainable
parameters are assigned to Q, K, V vectors directly and allocated by SMT automatically. The trainable
parameters are distributed among the K, Q, and V vectors, as detailed in Figure 5, with the trainable
states of the QKV layers shown in Figure 4.
The results show a significant performance gap when comparing the allocation of all trainable
parameters to the V vectors versus the Q and K vectors. Assigning all parameters to the V vectors
outperforms the K vectors by 4.6% and the Q vectors by 3.2%. These observations suggest that the
V vectors are the most significant among the Q, K, and V vectors; it also hints that SMT is able to
effectively select sub-matrices containing crucial memory sections.
6 Conclusion
Our proposed Sparse Matrix Tuning (SMT) achieves SoTA performance, narrowing the gap between
SMT and full fine-tuning. SMT can also reduce the computational cost of backward propagation,
parameter updates, optimizer memory, and activation memory during fine-tuning to achieve 14.6 ×
speedup. The empirical evidence presented in our extensive experiments suggests that attention layers
are more critical than MLPs for downstream performance; V vector is the most influential vector for
performance among Q, K, V vectors.
7 Acknowledgements
We sincerely thank Sida Wang for her valuable contributions to this project, including developing and
finalizing the implementation methodology, generating critical experimental results that strengthened
our conference presentation, and supporting the open-source release of our codebase.
We also extend our gratitude to Qianou (Christina) Ma, Chenyang Yang, Chen Liu, and Yu (Ivy)
Yang for the suggestion in paper writing and their valuable feedback.
This research used the Bridges-2 at Pittsburgh Supercomputing Center(PSC) and Delta advanced
computing. Pittsburgh Supercomputing Center is supported by National Science Foundation grants
#2138259, #2138286, #2138307, #2137603, and #2138296. The Delta advanced computing is
supported by the National Science Foundation (award OAC 2005572) and the State of Illinois.
Delta is a joint effort of the University of Illinois Urbana-Champaign and its National Center for
Supercomputing Applications.
Though Heather Miller and Juncheng Billy Li are employees of Two Sigma Investments, this work
was performed independently from Two Sigma Investments.
10

--- PAGE 11 ---
References
AI@Meta. Llama 3 model card. https://github.com/meta-llama/llama3/blob/main/
MODEL_CARD.md , 2024. Accessed: May 20, 2024.
Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton
Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference:
enabling efficient inference of transformer models at unprecedented scale. In SC22: International
Conference for High Performance Computing, Networking, Storage and Analysis , pages 1–15.
IEEE, 2022.
Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings,
Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less.
arXiv preprint arXiv:2405.09673 , 2024.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: efficient finetuning of
quantized llms (2023). arXiv preprint arXiv:2305.14314 , 2023.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms. Advances in Neural Information Processing Systems , 36, 2024.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter
efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904 , 2022.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are
key-value memories. arXiv preprint arXiv:2012.14913 , 2020.
Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers
build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680 ,
2022.
Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Man-
grulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple,
efficient and adaptable. https://github.com/huggingface/accelerate , 2022.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to
solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pages 523–533, 2014.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya
Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning
of large language models. arXiv preprint arXiv:2304.01933 , 2023.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang.
Parsing algebraic word problems into equations. Transactions of the Association for Computational
Linguistics , 3:585–597, 2015.
Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, and Dan Alistarh. Sparse fine-tuning
for inference acceleration of large language models, 2023. URL https://arxiv.org/abs/
2310.06927 .
11

--- PAGE 12 ---
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 ,
2017.
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-
Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint
arXiv:2402.09353 , 2024a.
Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, and Yvette Graham. Alora: Allocating low-rank
adaptation for fine-tuning large language models. arXiv preprint arXiv:2403.16187 , 2024b.
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin
Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/
huggingface/peft , 2022.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associations in gpt. Advances in Neural Information Processing Systems , 35:17359–17372, 2022a.
Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing
memory in a transformer. arXiv preprint arXiv:2210.07229 , 2022b.
Microsoft. Lora. https://githubc.com/microsoft/LoRA , 2021. Accessed: May 20, 2024.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve sim-
ple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,
editors, Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pages 2080–2094, Online, June
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL
https://aclanthology.org/2021.naacl-main.168 .
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations
toward training trillion parameter models. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis , pages 1–16. IEEE, 2020.
Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint
arXiv:1608.01413 , 2016.
L. Shih-yang. Dora. https://github.com/nbasyl/DoRA , 2024. Accessed: May 20, 2024.
Weixi Song, Zuchao Li, Lefei Zhang, Hai Zhao, and Bo Du. Sparse is enough in fine-tuning
pre-trained large language model. arXiv preprint arXiv:2312.11875 , 2023.
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient
generative inference of large language models. Advances in Neural Information Processing
Systems , 36, 2024.
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong
Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint
arXiv:2403.03507 , 2024.
Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv
Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363 , 2020.
12

--- PAGE 13 ---
A Implementation Details with Code Snippets
Figure 6: Implementation of customized sparse linear layer.
Figure 7: Implementation of customized forward in specialized sparse linear multiplication function.
Figure 8: Implementation of customized backward in specialized sparse linear multiplication function.
13

--- PAGE 14 ---
B Sub-Matrices Selection
SMT ranks the average absolute gradient values within each sub-matrix and selects those with the
highest averages. The rationale behind this selection process is to enable SMT to automatically
identify sub-matrices containing memory that is most relevant to downstream tasks. During fine-
tuning, the absolute gradient values can indicate the relevance of a block to these tasks hence it
requires more tuning. By averaging the absolute gradient values within each sub-matrix, we can
determine the importance of the sub-matrix to specific downstream tasks.
C Math10K Dataset
Math10K dataset can evaluate the effectiveness of LLMs on the arithmetic reasoning task. Math10K
incorporate six subsets including GSM8k ,SingleEq ,SVAMP ,MultiArith ,AddSub , and AQuA .(1)
theGSM8K Cobbe et al. [2021] dataset consists of high quality linguistically diverse grade school
math word problems created by human problem writers, (2) the SVAMP Patel et al. [2021] benchmark
consists of one-unknown arithmetic word problems for up-to-4 grade level students by making simple
changes to a set of problems from another existing dataset, (3) the MultiArith Roy and Roth [2016]
dataset of math word problems requiring multiple reasoning steps and operations, (4) the AddSub
Hosseini et al. [2014] dataset of addition and subtraction arithmetic word problems, (5) the AQuA
Ling et al. [2017] dataset of algebraic word problems with natural language rationales, and (6) the
SingleEq Koncel-Kedziorski et al. [2015] dataset of grade-school algebra word problems that map
to single equations with varying length;
14
