# 2304.12410.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2304.12410.pdf
# File size: 719772 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PEFT-Ref: A Modular Reference Architecture and Typology
for Parameter-Efficient Finetuning Techniques
Mohammed Sabry
ADAPT/DCU, Dublin, Ireland
mohammed.sabry@adaptcentre.ieAnya Belz
ADAPT/DCU, Dublin, Ireland
anya.belz@adaptcentre.ie
Abstract
Recent parameter-efficient finetuning (PEFT)
techniques aim to improve over the consider-
able cost of fully finetuning large pretrained
language models (PLM). As different PEFT
techniques proliferate, it is becoming difficult
to compare them, in particular in terms of (i) the
structure and functionality they add to the PLM,
(ii) the different types and degrees of efficiency
improvements achieved, (iii) performance at
different downstream tasks, and (iv) how dif-
ferences in structure and functionality relate to
efficiency and task performance. To facilitate
such comparisons, this paper presents a refer-
ence architecture which standardises aspects
shared by different PEFT techniques, while iso-
lating differences to specific locations and inter-
actions with the standard components. Through
this process of standardising and isolating dif-
ferences, a modular view of PEFT techniques
emerges, supporting not only direct compari-
son of different techniques and their efficiency
and task performance, but also systematic ex-
ploration of reusability and composability of
the different types of finetuned modules. We
demonstrate how the reference architecture can
be applied to understand properties and relative
advantages of PEFT techniques, hence to in-
form selection of techniques for specific tasks,
and design choices for new PEFT techniques.
1 Introduction
Over the past few years, there has been a signif-
icant increase in the size of pretrained language
models (PLMs) such as GPT3 (Brown et al., 2020),
OPT (Zhang et al., 2022a), BLOOM (Workshop
et al., 2022), and PaLM (Chowdhery et al., 2022),
which have billions of parameters. This increase
in size has been accompanied by a commensurate
increase in the cost of training and deploying large
PLMs, with substantial financial and environmen-
tal implications. Reusing PLMs via adaptation to
downstream tasks, rather than training new lan-
guage models for new tasks, mitigates this costsignificantly. However, full finetuning, the default
task adaptation approach, is still very costly as it
retrains, and subsequently stores, the entire model.
Parameter-efficient finetuning (PEFT) tech-
niques improve this cost by (re)training a much
smaller set of parameters. Heuristic approaches
modify a specific subset of the model’s existing
parameters, e.g. Lee et al. (2019) finetune the last
quarter of the layers in BERT and RoBERTa, and
Zaken et al. (2022) finetune just the bias terms
of the model. Other PEFT techniques such as
Adapters (Houlsby et al., 2019), prefix tuning (Li
and Liang, 2021), prompt tuning (Lester et al.,
2021), and LoRA (Hu et al., 2021), instead freeze
all of the PLM parameters, and add and train a
small set of new parameters in conjunction with the
latter. Several studies (Ding et al., 2023; Chen et al.,
2022a; He et al., 2022; Mao et al., 2022) have found
such parameter-adding PEFT techniques highly ef-
fective in real-world tasks. It is this group of PEFT
methods that is our focus in this paper.
As an increasing number of PEFT techniques are
reported, it is becoming harder to compare them in
terms of efficiency improvements and performance
at different tasks, in particular which aspects of
their structure and functionality are linked to bet-
ter efficiency and performance. To address this
we propose the PEFT-Ref framework consisting
of a modular reference architecture and typology
which provide a standardised way of characterising
PEFT techniques in terms of their structural and
functional properties. In this paper, we present the
reference architecture (Section 2), and use it to cre-
ate a typology of seven leading PEFT techniques
(Section 3), and to compare the techniques in terms
of efficiency and performance (Section 4). We illus-
trate how this in turn can be used to inform design
choices and technique selection for specific tasks
(Sections 5), and finish with a review of related
work and some conclusions (Section 6 and 7).
1arXiv:2304.12410v2  [cs.CL]  19 Oct 2023

--- PAGE 2 ---
2 The PEFT-Ref Framework
In this section, we present the PEFT-Ref in dia-
grammatic form (Section 2.1), and in terms of the
typological properties it defines (Section 2.2). In
combination, reference architecture and properties
are intended to fully capture differences and sim-
ilarities between different PEFT techniques, as a
basis for understanding the causes for their relative
strengths and weaknesses, and informing technique
selection and development.
2.1 Modular PEFT reference architecture
Figure 1 displays the PEFT reference architecture
in diagrammatic form, showing how the different
types of modules created and trained by different
PEFT techniques slot into and interact with the
standard Transformer architecture. Most of the
properties defined in the next section are also de-
picted in the diagram (see the Figure 1 legend).
In the diagram, the L×repeated layers are
shown inside the grey box, with the residual flow
to the right. The components of the standard
Transformer PLM are shown in black (non-dashed)
boxes. In the embedding, attention and feed-
forward layers, and immediately following the at-
tention and feed-forward layers, we show where
and how different PEFT techniques insert their
modules, indicated by dashed boxes. Note that
these are not normally combined, i.e. only one type
of PEFT module is normally inserted.
2.2 Modular properties of PEFT modules
The PEFT-Ref typology comprises the following
modular structural and functional properties. We
adapt some of the property names from the general
modular computing literature, some from recent
work on modularity in neural networks, and some
are new, as indicated. The range of property values
is specific to PEFT-Ref in all cases. Table 1 lists
these properties and their specific values for seven
leading PEFT techniques.
1.Intra-connectivity (Clune et al., 2013; Meu-
nier et al., 2010) – dense orsparse : Neuron
connectivity within the PEFT module’s lay-
ers. Denser intra-connectivity indicates higher
modularity. All current PEFT techniques, ex-
cept (IA)3, are densely intra-connected. In
Table 1, we additionally show the specific
type of densely connected component: em-
bedding layer, non-linear MLP , linear MLP ,self-attention ; (IA)3inserts a standalone vec-
tor parameter that is neither dense nor sparse.
2.Inter-connectivity (Béna and Goodman,
2021; Meunier et al., 2010) – fixed:dense,
fixed:sparse ordynamic : How PEFT mod-
ules are connected to the PLM architecture.
Sparser inter-connectivity indicates higher
modularity. All current PEFT techniques ex-
cept tiny-attention adapters (Zhao et al., 2022)
have fixed/dense interconnectivity.
3.Parameters adapted (Ding et al., 2023) – ad-
dition orreparameterisation : All PEFT tech-
niques alter the model parameters, either by
adding them or reparameterising existing com-
ponents in the PLM architecture.
4.Parameter Sharing/Tying –shared ,tied, or
none : In parameter sharing two sets of pa-
rameters are forced to be the same; in tying,
two sets of parameters are kept close to each
other. Parameter sharing/tying has several ad-
vantages in regularisation and inductive bi-
ases (Yeh et al., 2022), including better per-
formance and stability with fewer parameters.
Among current PEFT techniques, only Com-
pacters (Karimi Mahabadi et al., 2021) share
parameters, in their reparameterised layers.
5.Input type (Pfeiffer et al., 2023b; Auda and
Kamel, 1999) – hidden ,data orweights : The
type of input PEFT modules receive: (i) hid-
den representations received from a Trans-
former layer block, (ii) the data before it
goes into the block, or (iii) a newly initialised
weight matrix, in the case of PEFT techniques
that add and optimise a weight matrix (Prompt
Tuning, Prefix Tuning, and (IA)3).
6.Insertion Form (Pfeiffer et al., 2023b; Auda
and Kamel, 1999) – sequential orparallel :
Whether the finetuned module is inserted into
the PLM sequentially or in parallel. Most
techniques that insert modules sequentially
receive the output of the Transformer layer
block they collaborate with.
7.#Insertions –n layers orall layers : How
many instances of a PEFT module are inserted
into the PLM. All current PEFT techniques
except prompt tuning (which inserts only into
the embedding layer), insert modules into all
of the L×repeated Transformer layers. Pre-
fix tuning additionally adds parameters to the
embedding layer.
2

--- PAGE 3 ---
Figure 1: Modular PEFT reference architecture showing PLM components (central box), different types of PEFT
modules (left and right of centre), and insertion slots of PEFT modules (dashed boxes in PLM box) and interactions
between PEFT modules and PLM components (see also legend on left).
8.Integration Form (Auda and Kamel, 1999)
–concatenation, scaled addition ,direct ad-
dition ,gated addition , or rescaling : How
PEFT module outputs are incorporated into
the PLM.
9.Workspace –attention layer ,FFN layer
orembedding layer : In cognitive science,
workspace is a limited-bandwidth communi-
cation channel in which different modules
exchange information (Baars, 1988). In AI,
Goyal et al. (2022) use a shared workspace
model to describe systematic information ex-
change between specialist regions in a neural
network. In our context, most PEFT tech-
niques use attention layers and/or fully con-
nected layers in the PLM as their workspace.
Table 1 additionally indicates, where appro-
priate, the specific locus of interaction within
the workspace – queries/values ,keys/values ,
(FFN) intermediate representation .
3 Characterisation of PEFT Techniques
with PEFT-Ref
In this section, we characterise seven leading PEFT
techniques in terms of PEFT-Ref modular struc-tural properties. We focus on PEFT techniques
that are modular in the sense that they add and
train topologically distinct sets of parameters. Such
techniques have been shown to be highly effective
and are commonly used in downstream and real-
world tasks (Ding et al., 2023; Liu et al., 2022), in
contrast to more heuristic approaches that adapt a
specific fixed subset of PLM parameters (Lee et al.,
2019; Zaken et al., 2022). Table 1 provides an
overview of the seven PEFT techniques covered in
this section, in terms of the typological properties
introduced in Section 2.2.
3.1 Prompt Tuning (PT)
Module-internal topology and functionality:
Prompt Tuning (PT) (Lester et al., 2021) generates
token-like embeddings using an embedding layer
(intra-connectivity), which are then concatenated
(integration form) to the input embeddings of
the PLM (workspace), as shown in Figure 1.
The finetuning process customises the token-like
embeddings to the task objective.
Modular properties and collaboration with PLM:
the PT only inserts parameters at the embedding
layer, concatenating all token-like embeddings with
3

--- PAGE 4 ---
PEFT Intra- Inter- Parameters ParameterInput typeInsertion#InsertionsIntegrationWorkspacetechnique connectivity connectivity adapted Sharing form form
Prompt tuning dense:embedding fixed:dense Addition none Weights Parallel 1 layer concatenation embedding layer
Prefix tuning dense:non-linear MLP fixed:dense Addition none Weights Parallel all layers gated additionembedding layer;
att. layer:keys/values
LoRA dense:linear MLP fixed:dense Reparametr. none Data Parallel all layers scaled addition att. layer:queries/values
Adapters dense:non-linear MLP fixed:dense Addition none Hidden Sequential all layers direct addition FFN layer; attention layer
Tiny-Att. Ad. dense:self-attention dynamic Addition none Hidden Sequential all layers direct addition attention layer
Compacters dense:non-linear MLP fixed:dense Addition shared Hidden Sequential all layers direct addition FFN layer; attention layer
(IA)3none:parameter vector fixed:dense Addition none Weights Sequential all layers rescalingFFN layer:intermed. repres.;
attention layer:keys/values
Table 1: Structural properties of PEFT modules created by seven PEFT techniques (for descriptions of techniques
see Section 3; for definitions of properties see Section 2.2).
the input embeddings, which results in a fixed-
dense inter-connectivity between the PLM and PT.
3.2 Prefix Tuning (PF)1
Module-internal topology and functionality: In con-
trast to Prompt Tuning, which generates token-like
embeddings using only the embedding layer, Li
and Liang (2021) propose using two linear layers
with a Softmax activation in-between (Figure 1).
Modular properties and collaboration with
PLM: Li and Liang (2021) furthermore extend the
workspace to be the input embeddings and the At-
tention’s keys and values in all Transformer layers.
The PF token-like embeddings are concatenated
with these matrices ( i.e. integration form).2PF
connects all its information to the PLM (i.e. its
inter-connectivity is fixed:dense).
3.3 LoRA
Module-internal topology and functionality: LoRA
(Hu et al., 2021) adapts PLMs using low-rank de-
composition matrices. The idea is that the update
of model parameters can be approximated using
low-dimensional decomposition. LoRA reparam-
eterises the Attention queries and values weights
into low-rank matrices. For each, LoRA uses two
small linear projection layers (inter-connectivity)
to reparameterise the weights. LoRA receives the
same input that the reparameterised weights receive
(i.e. the insertion form is parallel).
Modular properties and collaboration with PLM:
LoRA generates queries and values of the input and
collaborates (integration form) via scaled addition
(h+λ∆h)with the Attention’s queries and values
(workspace) of the input in all Transformer layers.
1Prefix tuning was published prior to prompt tuning, but
the two appear to have been developed simultaneously and,
coincidentally, the former is an enhanced variation of the latter.
2He et al. (2022) demonstrate that the concatenation in
the Attention block can be viewed as a form of gated addi-
tion integration; in (1−λ)h+λ∆hthehrepresents PLM
functionality and ∆hrepresents PEFT module functionality.LoRA sends all its information to the workspace
(i.e. inter-connectivity = fixed:dense).
3.4 Adapters
Module-internal topology and functionality:
Adapters (Houlsby et al., 2019) use a feed-
forward layer (intra-connectivity) that bottlenecks
information via two linear layers that project
the information down and then up, with ReLU
activation in-between. Adapters adapt the hidden
representations resulting from Attention and FNN
blocks (insertion form = sequential).
Modular properties and collaboration with
PLM: Adapters integrate their results with their
workspace (Attention and FNN blocks) via direct
addition (h+ ∆h). Although variants of Adapters
exist that change internal connectivity or #inser-
tions, such as AdapterDrop (Rücklé et al., 2021),
Compacters (Karimi Mahabadi et al., 2021), and
Tiny-Attention Adapters (Zhao et al., 2022), they
all use direct addition for integration. Adapters
send all their information to their workspace (inter-
connectivity = fixed:dense).
3.5 Tiny-Attention Adapters
Module-internal topology and functionality: Tiny-
Attention Adapters (Zhao et al., 2022) are a variant
of Adapters that change the intra-connectivity to a
small Attention layer (Figure 1).
Modular properties and collaboration with PLM:
Like Adapters, Tiny-Attention Adapters are in-
serted sequentially, collaborate via direct addition
with their workspace, and receive hidden represen-
tations as inputs. However, they are inserted after
the Attention block (workspace), and send their
information to the workspace selectively based on
the input (inter-connectivity = dynamic).
3.6 Compacters
Module-internal topology and functionality: Com-
pacters (Karimi Mahabadi et al., 2021) are a vari-
4

--- PAGE 5 ---
ant of Adapters with the following difference. In
the vanilla Adapter layer, W∈Rk×d. In con-
trast, Compacters reparameterise layer Was a
sum of Kronecker products, with kandddivisi-
ble by a user-defined hyperparameter n. Specif-
ically, the sum of nKronecker products is W=Pn
i=1Ai⊗Bi, where Ai∈Rn×nandBi∈Rk
n×d
n.
Compacters further improve parameter efficiency
by sharing the weights of Aibetween the layers of
the compacter.
Modular properties and collaboration with PLM:
Compacters have the same properties as Adapters
in terms of collaboration with the PLM, insertion
form, integration form, and workspace.
3.7 (IA)3
Module-internal topology and functionality: An
(IA)3(Liu et al., 2022) module comprises three
vectors that rescale the Attention (keys, values),
and FFN blocks of a Transformer layer (Figure
1). During the tuning process, these vectors are
initialised to one to ensure that the module does not
affect the PLM’s functionality before being guided
by the task’s objective gradient.
Modular properties and collaboration with
PLM: (IA)3applies learned vector rescaling to its
workspace (keys, values, and intermediate FFN)
across all Transformer layers. It is inserted sequen-
tially and sends all its information to its workspace
(inter-connectivity = fixed:dense).
4 Efficiency and Performance
Comparisons with PEFT-Ref
In this section we use PEFT-Ref as the basis for
several different types of comparisons between the
seven techniques characterised in the preceding
section. In Section 4.1 we take a closer look at
exactly what the efficiency improvements achieved
by each PEFT technique are, (i) as compared to
full finetuning involving all PLM parameters, and
(ii) as compared to the other PEFT techniques.
Then in Section 4.2 we review what we know so
far about the performance of the seven techniques
at different benchmark tasks, and link it to their
modular properties.
4.1 Efficiency improvements
4.1.1 Complexity
Table 2 provides an overview of PEFT techniques
in terms of the time complexity per token of the
module(s) they add (column 2), and the numberPEFT Module Number of parameters
complexity per Transformer layer
Prompt Tuning O(1) n dm
Prefix Tuning O(kd) n dm+dm2+ 2dhdm
LoRA O(rd) 2 × (2 dhdm)
Tiny-Attention Adapters O(T) 4 ×dm
Adapters O(kd) 2 × (2 dhdm)
Compacters O(kd
N) 2 × (2 ( dh+dm) )
(IA)3O(1) 6 ×dm
Table 2: Efficiency of the seven PEFT techniques sur-
veyed; d m= model dimension, d h= PEFT module
dimension, n= number of tokens for prompt and prefix
tuning; k, r, d = input/output dimension of PEFT mod-
ule, where for LoRA ris the rank, and for Adapters
kis the bottleneck dimension. d=dm.T= #Input
embeddings. N= Reduction dimension in Kronecker-
products.
of parameters added per Transformer layer (col-
umn 3). Module time complexity (column 2) is
controlled by intra-connectivity and input type.3
Here, we take into account only the time it takes
a PEFT technique to produce the output for col-
laboration with the PLM. In this sense, e.g (IA)3
has constant time complexity O(1), as the output is
obtained directly from the module after weight ini-
tialisation, and is used as a rescaler for the PLM’s
activations. We provide further details of our anyal-
ysis of module complexity in Appendix A.
The number of parameters (column 3) is chiefly
controlled by workspace type, #insertions, and
intra-connectivity. For example, (IA)3utilises three
vectors to rescale the Attention keys, values, and
FFN intermediate representations, giving d m(the
model dimension) for keys and values each, plus
4dmfor the FFN intermediate representation, i.e. a
total of 6d m.
4.1.2 In-training efficiency
Parameter efficiency does not necessarily translate
into learning efficiency. Ding et al. (2023) exam-
ined the convergence of PEFT techniques such as
LoRA, Adapters, Prefix Tuning, and Prompt Tun-
ing against full finetuning. The results showed
that full finetuning converged the fastest, followed
by Adapters/LoRA, and then Prefix Tuning, while
Prompt Tuning had the slowest convergence rate.
As PLMs grow in size, the convergence of PEFT
techniques becomes faster. Ding et al.’s results
3PEFT techniques with O(1)time complexity output from
input in one step. Except for methods that use another network
to produce weights, all PEFT techniques that take weights as
input and produce weights as output have O(1)time complex-
ity in this sense.
5

--- PAGE 6 ---
also indicate that convergence is more sensitive to
structure than to number of parameters.
PEFT-Ref explicitly accounts for the structural
properties that control convergence rate, including
intra/inter-connectivity, #insertions, output produc-
tion (input type and insertion form), and parameter
sharing. For instance, slow convergence in Prompt
Tuning can be attributed to instability caused by the
output (token-like embeddings) being optimised di-
rectly, while Prefix Tuning is sensitive to reparame-
terisation choices (intra-connectivity) that produce
this output. Additionally, some PEFT techniques
can have similar or better convergence rates than
full finetuning depending on task complexity.
Chen et al. (2022a) examined the stability of per-
formance across different random seeds for several
PEFT techniques including Adapters, LoRA, and
Prefix Tuning, following a similar study on the sta-
bility of full finetuning (Dodge et al., 2020). The
authors found that these PEFT techniques, like full
finetuning, are susceptible to performance fluctua-
tions resulting from weight initialisation and data
ordering. Furthermore, the authors investigated the
impact of controlling the number of parameters in
these techniques on their stability. They observed
that reducing the number of parameters in PEFT
techniques can increase their stability. As a result,
they recommend exercising caution when selecting
the reduction factors in Adapters, the rank in LoRA,
and the prompt length in Prefix Tuning, and setting
them in a low range. In Appendix A, we look in
more detail at forward & backward training passes
efficiency within the context of module complexity
as per Section 4.1.1.
4.1.3 Storage and in-application efficiency
The last column in Table 2 shows the number of
parameters added per transformer layer, and the
variables that control it, for each of the seven PEFT
techniques. By saving only the task-specific post-
finetuning PEFT modules4instead of the entire
model as would be required in full finetuning, stor-
age size can be drastically reduced from gigabytes
to a few megabytes. This storage efficiency makes
it possible to serve multiple users and applications
using a single standalone PLM in conjunction with
multiple different task-specific PEFT modules.
The structural properties defined in PEFT-Ref
(e.g., insertions, input type, adapted parameters,
4All PEFT techniques save their tunable parameters, the
exception is Prefix Tuning, which only saves the final token-
like embeddings and discards the network that produced them.workspace, parameter sharing) directly control ef-
ficiency in this sense, thus facilitating insights
for potential improvements. In Appendix A, we
look in more detail at the inference latency for in-
application efficiency within the context of module
complexity as per Section 4.1.1.
4.2 Task performance
In Table 3 in Appendix B, we have documented the
performance of various PEFT techniques across
different tasks based on previous research.
Among the techniques we examined, LoRA
stands out as the top performer in several tasks,
either as the first or the second-best option. LoRA
works in collaboration with the PLM to improve
critical components, particularly the queries matri-
ces. It is the only PEFT technique that collaborates
with this component.
Adapters, and their variants, also exhibit excel-
lent performance scores, and it appears that the
reparameterisation and parameter-sharing proper-
ties in Compacter enhance their effectiveness.5Fi-
nally, we observe that (IA)3performs better in com-
monsense reasoning tasks compared to LoRA and
Adapters which could be attributable to the former
using rescaling as its integration form, and the latter
using addition.
LoRA, Adapters, and Compacter use either just
attention layers, or attention layers and FFN lay-
ers, as their workspace and our analysis indicates
that PEFT techniques that use feed-forward and/or
Attention blocks as their workspace are associated
with higher performance scores.
5 Using PEFT-Ref to Guide Technique
Selection and Development
In this section, we start from (i) the information
that PEFT-Ref provides about PEFT techniques,
and (ii) their performance at different downstream
tasks, to draw broad conclusions about the suit-
ability of each technique for different task types
(Section 5.1).
Then we take this one step further and surmise
how PEFT techniques (Section 5.2) can potentially
be developed further, or even combined, to improve
their stability, convergence speed, and/or task per-
formance.
5Adapters and Compacters differ in parameter sharing and
reparameterisation, with Compacters being more performant
than Adapters. These properties are responsible for their per-
formance, as shown in the performance Table 3.
6

--- PAGE 7 ---
5.1 PEFT technique selection
Prompt Tuning is a suitable technique for task like
Named Entity Recognition, because it works on
the embedding layer, which already has enough
contextual information to solve this task, after prop-
agating through the frozen language model layers.
This means that conditioning the task on the em-
beddings alone is sufficient. Additionally, Prompt
Tuning has a layer complexity of O(1)and a low
number of parameters, making it an efficient op-
tion that can achieve good performance even with
a small computation budget.
LoRA can be a suitable choice for Question
Answering tasks as it operates on the attention
queries and values workspaces which enables the
model to identify relevant relationships between
words and phrases in the question and the answer
(LoRA’s performance in multiple-choice QA tasks
Table 3 supports this). Additionally, the tunable
scaling integration form can assist the model to bet-
ter utilise important information to solve the task.
Tiny-Attention Adapters can provide additional at-
tention and potentially improve upon the hidden
representation output after the Transformer atten-
tion block as they are inserted sequentially after
it.
Data-to-Text and Summarisation tasks can bene-
fit from using either LoRA or Prefix Tuning. Previ-
ous research (Li and Liang, 2021; Liu et al., 2022;
Xu et al., 2022; Ding et al., 2023) has shown that
these techniques provide comparable performance,
but the choice between them depends on the avail-
able computation budget. LoRA has fewer parame-
ters and better layer complexity compared to Prefix
Tuning, which makes it a more efficient option,
and their performance on these tasks can be ex-
plained by their properties in PEFT-Ref. Recent
work (Xu et al., 2022) evaluated Adapters for gen-
eration tasks and found that although they have
good performance, they have worse faithfulness
scores than full finetuning and Prefix Tuning. To
explain these results in light of PEFT-Ref, it can be
noted that Adapters use the feed-forward block in
addition to the attention block as their workspace.
However, Zhang et al. (2022b) found that the feed-
forward block contains a lot of redundancy. Alter-
ing this block further may result in lower faithful-
ness scores for generation tasks.
In conclusion, the selection of a PEFT technique
depends on the complexity of the task at hand. For
instance, if the task requires reasoning over context(Chen et al., 2022b), it is advisable to choose a
method that has attention modules as a workspace.
Alternatively, if the task involves the addition of
new concepts to the language model, feed-forward
modules can be used to store knowledge in the
Transformer (Dai et al., 2022), thus making them
potential workspaces for adaptation. For simple
tasks that do not require any of the above require-
ments, adding task-specific information via the em-
bedding workspace should suffice. All of these
insights can be easily deduced using PEFT-Ref.
5.2 Further development of PEFT techniques
Parameter sharing/tying has several advantages
in regularisation and inductive biases (Yeh et al.,
2022). ALBERT (Lan et al., 2020), a language
model that achieves parameter reduction by shar-
ing and factorising parameters, achieves high per-
formance and stability with fewer parameters than
BERT. Consequently, parameter sharing is an at-
tractive property that can significantly contribute
to the performance and stability of finetuning tech-
niques. Enabling parameter sharing/tying across
layers of different modules, as well as across Trans-
former layers, holds the potential of significantly
enhancing the performance and stability of PEFT
techniques.
Adopting a tunable scaling parameter in
Adapters, as in LoRa (Hu et al., 2021), could dra-
matically improve these methods as they collabo-
rate with all blocks in the Transformer layer. Such
significant collaboration may need to be controlled
via scaled addition. We also note simple but effec-
tive tweaks, like AdapterDrop (Rücklé et al., 2021),
which dynamically removes some of the Adapter
layers that are attached to all Transformer layers in
the vanilla settings. Additionally, stability could be
increased in prompt tuning by introducing proper
layering to produce prompt weights to concatenate
with the embeddings.
Another potential direction for development is
controlling the number of insertions of a PEFT
module by choosing specific layers (rather than
all) for insertion. Heuristic specification finetun-
ing techniques (e.g. Lee et al., 2022, finetune the
last quarter of the layers in BERT and RoBERTa)
that achieve good performance could be used as
indications of which layers to choose.
PEFT modules could potentially use the residual
flow (i.e. contextualised embeddings of the input
sequence) as a workspace, and adapt it by either
7

--- PAGE 8 ---
reparameterising or adding a new set of parameters
such as scaling vectors.
In addition, heuristic specification finetuning
techniques like BitFit (Zaken et al., 2022) and LN-
Tuning (Qi et al., 2022) which finetune the bias
terms and LayerNorm in the model respectively,
represent potential workspaces for designing PEFT
modules to adapt them. The advantage of using
PEFT on these heuristic specifications is that it
preserves the PLM model’s knowledge of parame-
ters like bias and LayerNorm and collaborates with
them rather than changing them.
6 Related Work
He et al. (2022) include a treatment of PEFT meth-
ods addressing internal architecture, modified rep-
resentations, insertion form, and composition func-
tion. However, to fully grasp the potential of PEFT
techniques from a modular perspective, embrac-
ing a diverse range of properties that compensate
for their subtle variations is essential: in the func-
tional form, all four considered PEFT methods
were treated as having the form Project down →
Nonlinear/linear →Project up, but not all PEFT
methods have this form (e.g. Prompt Tuning, (IA)3,
Tiny-Attention Adapters). Moreover, in terms of
modified representations, the treatment confusingly
treats a Transformer module that produces a hidden
representation as a hidden representation in itself
(i.e. it treats a position (Transformer module) as a
hidden representation).
Additionally, not all PEFT methods modify a hid-
den representation. In our work, we make a clear
separation between the position (the Workspace
in PEFT-Ref), and the modified hidden representa-
tions (input type in PEFT-Ref). Also not all PEFT
techniques are typically integrated with the lan-
guage model solely through addition forms (e.g.
Prompt Tuning, (IA)3).
Pfeiffer et al. (2023b) present a unified view of
modular deep learning, focusing on four key di-
mensions: module implementation, routing func-
tions, module aggregation, and module training.
This perspective revealed connections between pre-
viously independent research threads and various
applications of modular networks. While Pfeiffer
et al. briefly discuss some PEFT techniques under
module implementation, they only use composi-
tion type to categorise them (input composition for
prompt and prefix tuning, parameter composition
for LoRA, function composition for Adapters).Other work has surveyed parameter-efficient
techniques and studied their theoretical underpin-
nings and performance on various downstream
tasks. For example, Ding et al. (2023) design a
library on top of the Transformers library (Wolf
et al., 2020) to enable flexible training, compos-
ing, attaching/detaching of PEFT techniques with
PLMs. Mao et al. (2022) propose a mixture of ex-
perts framework for PEFT techniques that learn to
activate a PEFT technique that best suit the task.
7 Conclusion
In the work reported here, we aim to contribute to a
more comprehensive understanding of the rapidly
evolving research area of PEFT techniques. In this
paper, we have introduced the PEFT-Ref frame-
work consisting of a reference architecture and
typology based on an inventory of standardised
structural and functional properties of PEFT meth-
ods. We have shown how PEFT techniques can
be characterised in terms of the framework and
how such characterisation enables direct compar-
isons between PEFT methods in terms of efficiency
improvements and task performance.
We further analysed our PEFT-Ref characterisa-
tions of seven leading PEFT methods, to (i) draw
important conclusions about their suitability for dif-
ferent task types, and (ii) extract clear pointers for
developing improved PEFT methods in the future.
PEFT-Ref provides a simple but general refer-
ence architecture desgined to facilitate (i) easy re-
call of its components, and (ii) comparative under-
standing of different PEFT methods. Moreover, tak-
ing a modular view of PEFT techniques encourages
increased reusability of PLMs for various use cases
and tasks, and aligns with a recent call to build and
maintain large language models like open-source
software.6
Limitations
In this work, our aim is to establish a solid founda-
tion for comprehending PEFT techniques by em-
phasising a modular view of the parameters they
add and/or manipulate. We propose that PEFT tech-
niques can be seen as small modules working in
collaboration with large modules, such as language
models, to address specific tasks. By adopting this
modular perspective, we can capitalise on the struc-
tural and functional benefits of modularity.
6https://colinraffel.com/blog/a-call-to-build-models-like-
we-build-open-source-software.html
8

--- PAGE 9 ---
Our main objective is to unify PEFT techniques,
delving deeper into their inner workings to gain
a comprehensive understanding. Additionally, we
seek to identify areas where these techniques can be
improved and offer guidance on making informed
choices when selecting a technique for a down-
stream task.
Regarding pointers for future development we do
not (yet) provide implementations for the improve-
ments to PEFT methods we suggest. Regarding
relative strengths of different PEFT methods, there
are other factors that play into method selection
which are beyond the scope of the present work.
Finally, while we have included the leading
PEFT methods in our sample characterisations, we
have not included all variants and other methods
that exist. It is therefore conceivable that their inclu-
sion would lead to modification of the framework,
in particular in terms of property value ranges.
References
Gasser Auda and Mohamed Kamel. 1999. MODULAR
NEURAL NETWORKS: A SURVEY. International
Journal of Neural Systems , 09(02):129–151.
Bernard J. Baars. 1988. A Cognitive Theory of Con-
sciousness . Cambridge University Press.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners.
Gabriel Béna and Dan F. M. Goodman. 2021. Extreme
sparsity gives rise to functional specialization.
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and
Shangsong Liang. 2022a. Revisiting parameter-
efficient tuning: Are we really there yet? In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 2612–2626,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
S. Chen, M. Jiang, J. Yang, and Q. Zhao. 2022b. Atten-
tion in reasoning: Dataset, analysis, and modeling.
IEEE Transactions on Pattern Analysis &amp; Ma-
chine Intelligence , 44(11):7310–7326.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Jeff Clune, Jean-Baptiste Mouret, and Hod Lipson. 2013.
The evolutionary origins of modularity. Proceed-
ings of the Royal Society B: Biological Sciences ,
280(1755):20122863.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neurons in
pretrained transformers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 8493–
8502, Dublin, Ireland. Association for Computational
Linguistics.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-
han Yang, Yusheng Su, Shengding Hu, Yulin Chen,
Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,
Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei
Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong
Sun. 2023. Parameter-efficient fine-tuning of large-
scale pre-trained language models. Nature Machine
Intelligence , 5(3):220–235.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali
Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.
2020. Fine-tuning pretrained language models:
Weight initializations, data orders, and early stop-
ping. CoRR , abs/2002.06305.
Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kar-
tikeya Badola, Nan Rosemary Ke, Nasim Rahaman,
Jonathan Binas, Charles Blundell, Michael Cur-
tis Mozer, and Yoshua Bengio. 2022. Coordina-
tion among neural modules through a shared global
workspace. In International Conference on Learning
Representations .
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2022. Towards a
unified view of parameter-efficient transfer learning.
InInternational Conference on Learning Representa-
tions .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
9

--- PAGE 10 ---
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large
language models. CoRR , abs/2106.09685.
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021. Compacter: Efficient low-rank
hypercomplex adapter layers. In Advances in Neural
Information Processing Systems , volume 34, pages
1022–1035. Curran Associates, Inc.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. Albert: A lite bert for self-supervised learning
of language representations. In International Confer-
ence on Learning Representations .
Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What
would elsa do? freezing layers during transformer
fine-tuning.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
2022. Few-shot parameter-efficient fine-tuning is
better and cheaper than in-context learning.
Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-
hairi, Hao Ma, Jiawei Han, Scott Yih, and Madian
Khabsa. 2022. UniPELT: A unified framework for
parameter-efficient language model tuning. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 6253–6264, Dublin, Ireland. Associa-
tion for Computational Linguistics.
David Meunier, Renaud Lambiotte, and Edward Bull-
more. 2010. Modular and hierarchically modular
organization of brain networks. Frontiers in Neuro-
science , 4.
Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and
Edoardo Maria Ponti. 2023a. Modular deep learning.Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and
Edoardo Maria Ponti. 2023b. Modular deep learning.
Wang Qi, Yu-Ping Ruan, Yuan Zuo, and Taihao Li. 2022.
Parameter-efficient tuning on layer normalization for
pre-trained language models.
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2021. AdapterDrop: On the efficiency
of adapters in transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7930–7946, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
BigScience Workshop, :, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, Matthias Gallé, Jonathan
Tow, Alexander M. Rush, Stella Biderman, Albert
Webson, Pawan Sasanka Ammanamanchi, Thomas
Wang, Benoît Sagot, Niklas Muennighoff, Albert Vil-
lanova del Moral, Olatunji Ruwase, Rachel Bawden,
Stas Bekman, Angelina McMillan-Major, Iz Belt-
agy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Laurençon,
Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor
Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,
Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,
Chris Emezue, Christopher Klamm, Colin Leong,
Daniel van Strien, David Ifeoluwa Adelani, Dragomir
Radev, Eduardo González Ponferrada, Efrat Lev-
kovizh, Ethan Kim, Eyal Bar Natan, Francesco
De Toni, Gérard Dupont, Germán Kruszewski, Gi-
ada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu
Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar
Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse
Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,
Joseph Tobing, Joydeep Bhattacharjee, Khalid Al-
mubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,
Leon Weber, Long Phan, Loubna Ben allal, Lu-
dovic Tanguy, Manan Dey, Manuel Romero Muñoz,
Maraim Masoud, María Grandury, Mario Šaško,
Max Huang, Maximin Coavoux, Mayank Singh,
Mike Tian-Jian Jiang, Minh Chien Vu, Moham-
mad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,
Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas, Pe-
ter Henderson, Pierre Colombo, Priscilla Amuok,
10

--- PAGE 11 ---
Quentin Lhoest, Rheza Harliman, Rishi Bommasani,
Roberto Luis López, Rui Ribeiro, Salomey Osei,
Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
Shayne Longpre, Somaieh Nikpoor, Stanislav Silber-
berg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-
rent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-
lat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Davut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.
Mielke, Wilson Y . Lee, Abheesht Sharma, Andrea
Santilli, Antoine Chaffin, Arnaud Stiegler, Debajy-
oti Datta, Eliza Szczechla, Gunjan Chhablani, Han
Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan
Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-
ful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-
hal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon
Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-
mish Thakker, Vikas Raunak, Xiangru Tang, Zheng-
Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,
Hadar Tojarieh, Adam Roberts, Hyung Won Chung,
Jaesung Tae, Jason Phang, Ofir Press, Conglong Li,
Deepak Narayanan, Hatim Bourfoune, Jared Casper,
Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François
Lavallée, Rémi Lacroix, Samyam Rajbhandari, San-
chit Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aurélie Névéol, Charles Lover-
ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,
Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog-
danov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa
Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,
Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani
Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,
Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan
Belinkov, Zachary Bamberger, Zden ˇek Kasner, Al-
ice Rueda, Amanda Pestana, Amir Feizpour, Ammar
Khan, Amy Faranak, Ana Santos, Anthony Hevia,
Antigona Unldreaj, Arash Aghagol, Arezoo Abdol-
lahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh
Behroozi, Benjamin Ajibade, Bharat Saxena, Car-
los Muñoz Ferrandis, Danish Contractor, David Lan-
sky, Davis David, Douwe Kiela, Duong A. Nguyen,
Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fa-
tima Mirza, Frankline Ononiwu, Habib Rezanejad,
Hessie Jones, Indrani Bhattacharya, Irene Solaiman,
Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh
Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Sama-
gaio, Maraim Elbadri, Margot Mieskes, Marissa Ger-
chick, Martha Akinlolu, Michael McKenna, Mike
Qiu, Muhammed Ghauri, Mykola Burynok, Nafis
Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,Olanrewaju Samuel, Ran An, Rasmus Kromann,
Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas
Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi
Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Ab-
hinav Ramesh Kashyap, Alfredo Palasciano, Al-
ison Callahan, Anima Shukla, Antonio Miranda-
Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang,
Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin
Xu, Clémentine Fourrier, Daniel León Periñán,
Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio
Barth, Florian Fuhrimann, Gabriel Altay, Giyased-
din Bayrak, Gully Burns, Helena U. Vrabec, Imane
Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas
Golde, Jose David Posada, Karthik Rangasai Sivara-
man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,
Madeleine Hahn de Bykhovetz, Maiko Takeuchi,
Marc Pàmies, Maria A Castillo, Marianna Nezhurina,
Mario Sänger, Matthias Samwald, Michael Cullan,
Michael Weinberg, Michiel De Wolf, Mina Mihalj-
cic, Minna Liu, Moritz Freidank, Myungsun Kang,
Natasha Seelam, Nathan Dahlberg, Nicholas Michio
Broad, Nikolaus Muellner, Pascale Fung, Patrick
Haller, Ramya Chandrasekhar, Renata Eisenberg,
Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi
Su, Samuel Cahyawijaya, Samuele Garda, Shlok S
Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-
mon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Ste-
fan Schweter, Sushil Bharati, Tanmay Laud, Théo
Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis
Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yi-
fan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zi-
fan Ye, Mathilde Bras, Younes Belkada, and Thomas
Wolf. 2022. Bloom: A 176b-parameter open-access
multilingual language model.
Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Vir-
ginia Adams, Ryan Prenger, Wei Ping, Nayeon Lee,
Mohammad Shoeybi, and Bryan Catanzaro. 2022.
Evaluating parameter efficient learning for genera-
tion. In Proceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 4824–4833, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Raymond A. Yeh, Yuan-Ting Hu, Mark Hasegawa-
Johnson, and Alexander Schwing. 2022. Equivari-
ance discovery by learned parameter-sharing. In Pro-
ceedings of The 25th International Conference on
Artificial Intelligence and Statistics , volume 151 of
Proceedings of Machine Learning Research , pages
1527–1545. PMLR.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.
2022. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022a. Opt: Open
pre-trained transformer language models.
11

--- PAGE 12 ---
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,
Maosong Sun, and Jie Zhou. 2022b. MoEfication:
Transformer feed-forward layers are mixtures of ex-
perts. In Findings of the Association for Compu-
tational Linguistics: ACL 2022 , pages 877–890,
Dublin, Ireland. Association for Computational Lin-
guistics.
Hongyu Zhao, Hao Tan, and Hongyuan Mei. 2022.
Tiny-attention adapter: Contexts are more important
than the number of parameters. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 6626–6638, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
A Module Complexity: Further Analysis
In Table 2 and Section 4.1.1, we initially analysed
the module complexity of PEFT from the perspec-
tive of the time it takes for them to produce their
output (forward pass). We also discussed the im-
pact of PEFT-Ref’s structural properties on this
complexity. In this section, we further extend the
analysis and examine how different modules can
affect also the backward pass during training, as
well as the inference of the language model:
Training (forward pass): As previously men-
tioned, the complexity in Table 2 represents the
number of steps required to generate the PEFT
module’s output per token. Prompt Tuning and
(IA)3only require one step for initialising token-
like embeddings weights and scaling vectors, re-
spectively, without additional layering. Therefore,
these processes can be disregarded. However, for
other techniques, layering is involved from the ini-
tialised input to the output, with complexity per
token as provided in Table 2. Additionally, it is
worth noting that while the attention complexity is
O(Td)per token ( ?), Tiny-Attention Adapters use
vectors for query, keys, and matrices, resulting in a
per-token complexity of O(T)withd= 1.
Training (backward pass): For most PEFT
modules, it’s unnecessary to backpropagate
through the entire language model. Whether back-
propagation is required or not depends on the lo-
cation of the PEFT technique’s workspace within
the language model hierarchy. If the workspace
is situated deeper-from backward pass perspective-
in the hierarchy, such as in the embedding layer,
backpropagation needs to occur at that specific
level. For instance, techniques like Prompt Tuning
and Prefix Tuning treat the embedding layer as a
workspace and necessitate backpropagation to that
level.Inference: When it comes to inference, Prompt
Tuning, Prefix Tuning, and LoRA do not signifi-
cantly impact latency because we can conceal them
behind the inherent latency of the language model.
Prompt Tuning and Prefix Tuning techniques re-
quire allocation from the model’s context window,
which falls within the expected processing latency
for the size of the context window. In the case of
LoRA, as it involves reparameterisation of weights
and parallel insertion, we can explicitly compute
and store the weights along with their reparam-
eterised version ( W=W0+BA) to facilitate
inference as usual (Hu et al., 2021) .
As for (IA)3, this technique introduces minimal
latency since it involves scaling vectors, which are
computationally straightforward.
However, Adapters, along with their variants
Compacters and Tiny-Attention Adapters, inserted
sequentially and processing hidden representations,
contribute more substantial latency to the language
model compared to other PEFT techniques. To
address these implications, Rücklé et al. (2021) dis-
cussed strategies like AdapterDrop and AdapterFu-
sion that can be employed to mitigate the additional
latency.
B Performance Table
12

--- PAGE 13 ---
Work Task Datasets Training Details LoRAPrefix
TuningPrompt
Tuning(IA)3Adapter Compacter
(Ding et al., 2023) Sentiment AnalysisGLUE-SST2
ROTTEN_TOMATOES
FINANCIAL_PHRASEBANK
POEM_SENTIMENT
YELP_POLARITYModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken93.09 92.83 85.48 - 92.06 -
(Ding et al., 2023) Classification/emotionEMO
EMOTION
TWEET_EV AL-HATE
TWEET_EV AL-IRONY
TWEET_EV AL-OFFENSIVE
TWEET_EV AL-SENTIMENT
TWEET_EV AL-STANCE_ABORTION
TWEET_EV AL-STANCE_ATHEISM
TWEET_EV AL-STANCE_CLIMATE
TWEET_EV AL-STANCE_FEMINIST
TWEET_EV AL-STANCE_HILLARYModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken68.70 67.21 52.95 - 68.31 -
(Ding et al., 2023) Natural Language InferenceANLI
GLUE-MNLI
GLUE-QNLI
GLUE-RTE
SCITAIL
SUPERGLUE-RTE
SICK
SUPERGLUE-CBModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken82.73 80.07 51.93 - 83.06 -
(Ding et al., 2023) Multiple-Choice QACOSMOS_QA
DREAM
HELLASWAG
OPENBOOKQA
QASC
QUAREL
QUARTZ-NO_KNOWLEDGE
QUARTZ-WITH_KNOWLEDGE
RACE-HIGH
RACE-MIDDLE
SUPERGLUE-COPA
WINO_GRANDE
COMMONSENSE_QA
SCIQ
WIQAModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken58.67 53.93 46.93 - 56.11 -
(Ding et al., 2023) SummarisationSAMSUM
XSUMModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken35.44 33.61 30.35 - 35.38 -
(Liu et al., 2022) CommonsenseH-Swag
COPA
StoryCloze
WinograndeModel: T0-3b
Training Steps: 1k
except Compacter/Adapter trained with 500 steps
The best Result from the combination of
{8} batch size, learning rate {3e-3} except for Prompt Tuning {1e-3}66.23 58.95 61.05 68.01 63.75 65.65
(Liu et al., 2022) Word Sense Disambiguation WiCModel: T0-3b
Training Steps: 1k
except Compacter/Adapter trained with 500 steps
The best Result from the combination of
{8} batch size, learning rate {3e-3} except for Prompt Tuning {1e-3}54.86 52.51 52.51 54.23 54.70 55.33
(Ding et al., 2023) Classification/hate-speech detectionTHOS-DISABILITY
ETHOS-GENDER
ETHOS-NATIONAL_ORIGIN
ETHOS-RACE
ETHOS-RELIGION
ETHOS-DIRECTED_VS_GENERALIZED
HATE_SPEECH_OFFENSIVE
HATE_SPEECH
HATEXPLAINModel: T5 base
Training Steps: 20k
except Prompt Tuning trained with 100K steps
The best Result from the combination of
{16,32} batch size, learning rate {1e-3,1e-4,5e-4} is taken85.22 84.37 67.69 - 86.02 -
Table 3: Average PEFT techniques accuracies on different tasks across datasets. Tiny-Attention Adapters are
omitted from the Table due to the absence of comparative studies in the published literature.
13
