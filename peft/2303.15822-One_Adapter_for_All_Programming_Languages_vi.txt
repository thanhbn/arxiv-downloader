# 2303.15822.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2303.15822.pdf
# Kích thước tệp: 457204 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Adapter cho Tất cả Ngôn ngữ Lập trình?
Điều chỉnh Adapter cho Tìm kiếm và Tóm tắt Mã nguồn
Deze Wang†, Boxing Chen, Shanshan Li†, Wei Luo, Shaoliang Peng‡, Wei Dong†, Xiangke Liao†
†Trường Đại học Quốc phòng Công nghệ, Trường Sa, Trung Quốc
‡Đại học Hồ Nam, Trường Sa, Trung Quốc
{wangdeze14,shanshanli,wdong,xkliao}@nudt.edu.cn, chenboxing@gmail.com, luowei828@163.com, slpeng@hnu.edu.cn
Tóm tắt—Khi các mô hình được pre-train tự động hóa nhiều tác vụ trí tuệ mã nguồn, một mô hình được sử dụng rộng rãi là fine-tune mô hình trên tập dữ liệu tác vụ cho từng ngôn ngữ lập trình. Một nghiên cứu gần đây báo cáo rằng fine-tuning đa ngôn ngữ có lợi cho một loạt các tác vụ và mô hình. Tuy nhiên, chúng tôi phát hiện rằng fine-tuning đa ngôn ngữ dẫn đến suy giảm hiệu suất trên các mô hình gần đây UniXcoder và CodeT5.
Để giảm thiểu vấn đề quên thảm khốc có thể xảy ra trong các mô hình đa ngôn ngữ, chúng tôi cố định tất cả các tham số mô hình được pre-train, chèn cấu trúc adapter hiệu quả về tham số, và fine-tune nó. Chỉ cập nhật 0,6% tổng số tham số so với fine-tuning toàn bộ mô hình cho từng ngôn ngữ lập trình, điều chỉnh adapter mang lại cải thiện nhất quán trên các tác vụ tìm kiếm và tóm tắt mã nguồn, đạt được kết quả tốt nhất. Ngoài ra, chúng tôi thực nghiệm chứng minh tính hiệu quả của nó trong các tình huống đa ngôn ngữ và tài nguyên thấp. Fine-tuning đa ngôn ngữ với 200 mẫu cho mỗi ngôn ngữ lập trình tiếp cận kết quả được fine-tune với toàn bộ tập dữ liệu trên tóm tắt mã nguồn. Các thí nghiệm của chúng tôi trên ba tác vụ probing cho thấy điều chỉnh adapter vượt trội đáng kể so với fine-tuning toàn bộ mô hình và khắc phục hiệu quả vấn đề quên thảm khốc.
Từ khóa chỉ mục—học chuyển giao, adapter, tác vụ đa ngôn ngữ

I. GIỚI THIỆU
Các mô hình học sâu được áp dụng rộng rãi cho các tác vụ của các ngôn ngữ lập trình khác nhau, chẳng hạn như tìm kiếm và tóm tắt mã nguồn trong các ngôn ngữ lập trình như Java, Python, Ruby, v.v. Mô hình hiện tại để giải quyết các tác vụ này là trước tiên tải các mô hình ngôn ngữ được pre-train và sau đó huấn luyện và đánh giá chúng trên các tập dữ liệu cụ thể cho từng ngôn ngữ. Đối với một tác vụ nhất định, cần phải huấn luyện và duy trì các mô hình riêng biệt cho từng ngôn ngữ.
N ngôn ngữ lập trình cần N mô hình riêng biệt, điều này thử thách để huấn luyện, triển khai và duy trì từng cái một cách riêng lẻ. Ngoài ra, các nhà nghiên cứu trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) đã báo cáo rằng việc tiếp xúc các mô hình với nhiều ngôn ngữ cải thiện hiệu suất trong các ngôn ngữ có tài nguyên thấp [1–3]. Không giống như ngôn ngữ tự nhiên, nhiều ngôn ngữ lập trình có dạng ngữ pháp tương tự, và các mô hình đơn ngôn ngữ khác nhau được fine-tune từ cùng một mô hình được pre-train sẽ chia sẻ một từ vựng chung. Do đó, huấn luyện đa ngôn ngữ có thể có lợi hơn cho việc chuyển giao kiến thức giữa các ngôn ngữ lập trình so với ngôn ngữ tự nhiên.
Công việc liên quan nhất của Ahmed và Devanbu [4] điều tra rằng dựa trên các mô hình mã nguồn được pre-train CodeBERT [5] và GraphCodeBERT [6], fine-tuning đa ngôn ngữ dẫn đến Shanshan Li là tác giả liên hệ. cải thiện gần như nhất quán cho tất cả sáu ngôn ngữ lập trình [7] trên các tác vụ tìm kiếm và tóm tắt mã nguồn. Tuy nhiên, chúng tôi phát hiện thực nghiệm rằng kết luận này không thể được tổng quát hóa cho các mô hình mã nguồn được pre-train mới nhất UniXcoder [8] và CodeT5 [9]. Trên các mô hình này, fine-tuning đa ngôn ngữ dẫn đến suy giảm hiệu suất trong hầu hết các ngôn ngữ lập trình cho các tác vụ tìm kiếm và tóm tắt mã nguồn. Thật thử thách để huấn luyện một mô hình hỗ trợ nhiều ngôn ngữ lập trình đồng thời và duy trì hiệu suất tương đương với một mô hình đơn ngôn ngữ chuyên dụng.
Nhiều nghiên cứu đã chỉ ra rằng các mô hình đa ngôn ngữ có thể gặp phải chuyển giao tiêu cực do quên thảm khốc kiến thức thu được từ pre-training [10]. Chúng tôi đơn giản cố định tất cả các tham số của mô hình được pre-train, chèn adapter [11] vào mô hình và fine-tune nó. Kết quả là, chúng tôi thấy rằng nó có thể giảm thiểu vấn đề trên. Hơn nữa, chúng tôi chứng minh tại sao điều chỉnh adapter hiệu quả thông qua các thí nghiệm phân tích probing [12].
Chúng tôi giới thiệu mô-đun adapter vào các mô hình mã nguồn được pre-train khác nhau. So với việc huấn luyện toàn bộ mô hình cho từng ngôn ngữ lập trình, điều chỉnh adapter chỉ điều chỉnh không quá 0,6% tổng số tham số. Các mô hình mới thu được vượt trội hơn các mô hình được fine-tune với tất cả tham số và đạt được kết quả tốt nhất trên các tác vụ tìm kiếm và tóm tắt mã nguồn. Chúng tôi phát hiện thực nghiệm rằng trên tác vụ tóm tắt mã nguồn, điều chỉnh adapter với một số lượng nhỏ mẫu cho mỗi ngôn ngữ có thể tiếp cận kết quả của fine-tuning với toàn bộ tập dữ liệu, chứng minh rằng có thể thích ứng với một ngôn ngữ lập trình mới một cách nhanh chóng. Hơn nữa, chúng tôi chỉ ra tính hiệu quả của nó trong các tác vụ đa ngôn ngữ và tiến hành các thí nghiệm probing ngôn ngữ học để chỉ ra tại sao nó hoạt động.
Các đóng góp chính của bài báo chúng tôi như sau:
Chúng tôi phát hiện thực nghiệm rằng fine-tuning đa ngôn ngữ, thu được lợi ích hiệu suất đáng kể trên CodeBERT và GraphCodeBERT, dẫn đến suy giảm hiệu suất trên UniXcoder và CodeT5.
Chúng tôi chỉ ra bằng chứng rằng điều chỉnh adapter có thể khắc phục đáng kể vấn đề quên thảm khốc trong fine-tuning đa ngôn ngữ thông qua ba tác vụ probing.
Dựa trên UniXcoder và CodeT5 được pre-train, các mô hình của chúng tôi điều chỉnh không quá 0,6% tổng số tham số trên các tác vụ tìm kiếm và tóm tắt mã nguồn cho sáu ngôn ngữ lập trình, thu được lợi ích hiệu suất nhất quán và đạt được kết quả tốt nhất.arXiv:2303.15822v1 [cs.SE] 28 Mar 2023

--- TRANG 2 ---
AdapterLayer Norm2x Feed-forward layer +
+
Multi-head AttentionFeed-forward layer Layer Norm
AdapterTransformer
Layer
Nonlinearity+
Down-projection 
layer Up-projection 
layer AdapterHình 1. Kiến trúc của mô-đun adapter và sự tích hợp của nó với Transformer. Trái: Adapter được chèn sau multi-head attention và hai feed-forward layer. Phải: Adapter chứa một thắt cổ chai, có ít tham số hơn so với attention và feed-forward layer, và nó cũng chứa một skip-connection. Phần màu xanh của hình cho thấy các tham số cần điều chỉnh.
Chúng tôi chỉ ra rằng điều chỉnh adapter với 200 mẫu cho mỗi ngôn ngữ lập trình (0,02% tập dữ liệu gốc) có thể hoạt động tốt trong tác vụ tóm tắt mã nguồn.

II. KIẾN THỨC CHUẨN BỊ
A. Fine-tuning
Với thành công lớn của các mô hình được pre-train, mô hình pretrain-then-finetune đã trở thành mô hình thống trị trong lĩnh vực NLP. Fine-tuning sử dụng các tham số của mô hình được pre-train làm khởi tạo và nhanh chóng thích ứng với các tác vụ mới mà không cần huấn luyện từ đầu. Nó huấn luyện mô hình theo cách có giám sát. Chính thức, cho một tập hợp các mẫu cụ thể cho tác vụ X và các nhãn tương ứng Y, fine-tuning là tìm một tập hợp tham số thỏa mãn θ = arg minθ P(Y|X; θ).
Trong bài báo này, huấn luyện đa ngôn ngữ mà chúng tôi nghiên cứu ở đây đề cập đến giai đoạn fine-tuning của các mô hình được pre-train. Nó được thực hiện trên tập dữ liệu kết hợp của tất cả các ngôn ngữ lập trình. Các mô hình được pre-train đã được huấn luyện sử dụng dữ liệu đa ngôn ngữ theo cách không giám sát. Không giống như fine-tuning một mô hình sử dụng tập dữ liệu đơn ngôn ngữ và xây dựng một mô hình cho mỗi ngôn ngữ, chúng tôi mong muốn fine-tune một mô hình đa ngôn ngữ có thể xử lý nhiều ngôn ngữ lập trình đồng thời.
B. Adapters
Khi mô hình phổ biến của học chuyển giao là fine-tune tất cả tham số của một mô hình được pre-train, mô-đun adapter cung cấp một sự thay thế nhẹ để chỉ cập nhật một số lượng nhỏ tham số bổ sung trong khi giữ các tham số được pre-train bị đóng băng.
Hình 1 minh họa kiến trúc adapter tiêu chuẩn [11]. Đối với kiến trúc dựa trên Transformer [13], một tập hợp nhỏ các tham số mới được giới thiệu trong mỗi lớp transformer. Nó được chèn sau attention và feed-forward layer của mỗi lớp transformer, tương ứng. Adapter chứa hai lớp projection và một lớp phi tuyến. Một lớp skip-connection được sử dụng trên adapter. Cho một vector đầu vào ẩn h, đầu ra của adapter là:
Z = W_Up(f(W_Down(h))) + h (1)
trong đó f là hàm kích hoạt, W_Up ∈ R^(m×d) và W_Down ∈ R^(d×m) là các tham số của các lớp projection. d là kích thước ẩn của mô hình Transformer, và m là chiều của adapter. Như được thể hiện trong Hình 1, chiều của m thường nhỏ hơn d. Hai lớp projection tạo thành một cấu trúc thắt cổ chai.

III. PHƯƠNG PHÁP NGHIÊN CỨU
Để giải quyết các tác vụ đa ngôn ngữ cho hiểu biết và tạo sinh mã nguồn, chúng tôi điều tra một loạt các câu hỏi nghiên cứu và mô tả thiết kế nghiên cứu của chúng.
A. RQ1: Fine-tuning Đa ngôn ngữ Hoạt động Như thế nào trên Các Mô hình và Tác vụ Downstream Khác nhau?
Ahmed và Devanbu [4] phát hiện rằng fine-tuning đa ngôn ngữ có thể có lợi cho CodeBERT và GraphCodeBERT, và chúng tôi điều tra xem phát hiện này có thể được tổng quát hóa cho các mô hình được pre-train khác không.
Thiết kế RQ1: Chúng tôi fine-tune UniXcoder và CodeT5 với tập dữ liệu đa ngôn ngữ trên các tác vụ tìm kiếm mã nguồn và tóm tắt mã nguồn. Tập dữ liệu đa ngôn ngữ là sự kết hợp của các tập dữ liệu trong sáu ngôn ngữ lập trình [7], bao gồm Python, Java, JavaScript, Ruby, Go, và PHP. Trên tìm kiếm mã nguồn và tóm tắt mã nguồn, chúng tôi đánh giá hiệu suất của các mô hình được fine-tune trên tập dữ liệu đa ngôn ngữ và so sánh chúng với các mô hình được fine-tune trên các tập dữ liệu đơn ngôn ngữ. Để kiểm tra xem fine-tuning đa ngôn ngữ có hoạt động nhất quán trên các mô hình mã nguồn được pre-train khác nhau, chúng tôi cũng so sánh kết quả của CodeBERT, GraphCodeBERT, và PLBART với cùng cài đặt [4].
B. RQ2: Điều chỉnh Adapter Hiệu quả Như thế nào trong Các Tình huống Đa ngôn ngữ?
Một tác vụ đa ngôn ngữ là fine-tune các mô hình được pre-train với tập dữ liệu trong một ngôn ngữ lập trình và kiểm tra trên tập dữ liệu trong ngôn ngữ khác. Bằng cách chèn adapter vào các mô hình được pre-train và sau đó chỉ điều chỉnh số lượng nhỏ tham số bổ sung này, chúng tôi quan tâm xem điều chỉnh adapter có hiệu quả trong việc giải quyết các tác vụ đa ngôn ngữ không.
Thiết kế RQ2: Chúng tôi fine-tune UniXcoder và CodeT5 với sáu tập dữ liệu đơn ngôn ngữ, và cho mỗi mô hình đơn ngôn ngữ, chúng tôi kiểm tra nó trên tất cả các ngôn ngữ lập trình riêng biệt. Đối với điều chỉnh adapter, chúng tôi điều chỉnh một adapter cho mỗi ngôn ngữ và sau đó kiểm tra các adapter này trên tất cả các ngôn ngữ lập trình một cách bình đẳng. Chúng tôi so sánh thêm hiệu suất của điều chỉnh adapter và điều chỉnh toàn bộ mô hình trên mỗi tác vụ đa ngôn ngữ.
C. RQ3: Điều chỉnh Adapter Hiệu quả Như thế nào So với Fine-tuning Toàn bộ Mô hình Đa ngôn ngữ trên Các Mô hình Pre-trained Khác nhau?
Vì các tác vụ đa ngôn ngữ đòi hỏi khả năng chuyển giao kiến thức trong các cặp ngôn ngữ lập trình, các tác vụ đa ngôn ngữ đòi hỏi một mô hình có thể xử lý nhiều ngôn ngữ lập trình

--- TRANG 3 ---
đồng thời. Chúng tôi mong muốn điều tra xem điều chỉnh adapter có giải quyết hiệu quả các tác vụ đa ngôn ngữ không.
Thiết kế RQ3: Chúng tôi chèn adapter vào UniXcoder và CodeT5 và fine-tune adapter với tập dữ liệu đa ngôn ngữ. Hiệu suất của adapter được đánh giá trên các tác vụ tìm kiếm mã nguồn và tóm tắt mã nguồn. Chúng tôi so sánh hiệu suất của các mô hình đa ngôn ngữ được fine-tune bằng adapter với tất cả các mô hình khác, bao gồm các mô hình đơn ngôn ngữ tương ứng và các mô hình đa ngôn ngữ với fine-tuning toàn bộ mô hình.
D. RQ4: Fine-tuning Đa ngôn ngữ Hiệu quả Như thế nào trong Các Tình huống Tài nguyên Thấp?
Đối với một mô hình đa ngôn ngữ, chúng tôi mong muốn nó học và tổng quát hóa cho các ngôn ngữ lập trình mới một cách nhanh chóng. Tuy nhiên, có dữ liệu được gán nhãn hạn chế cho các mô hình để học đối với nhiều ngôn ngữ lập trình. Do đó, chúng tôi lấy mẫu các tập dữ liệu hiện tại và nghiên cứu hiệu suất của học đa ngôn ngữ trong các tình huống tài nguyên thấp.
Thiết kế RQ4: Chúng tôi lấy mẫu ngẫu nhiên các tập dữ liệu trong mỗi ngôn ngữ lập trình, và chọn 100, 200, 500 và 1000 mẫu cho mỗi ngôn ngữ. Sau đó, chúng tôi chèn adapter vào CodeT5 và đánh giá mô hình trên các tổ hợp của dữ liệu này. Chúng tôi thay đổi random seed, lặp lại thí nghiệm nhiều lần, và lấy trung bình kết quả để kiểm tra tính hiệu quả của học đa ngôn ngữ.
E. RQ5: Tại sao Điều chỉnh Adapter Tốt hơn Fine-tuning Toàn bộ Mô hình trong Các Tình huống Trên?
Mặc dù điều chỉnh adapter cho thấy hiệu suất vượt trội trong các tình huống trên, nó không trực tiếp cung cấp thông tin chi tiết về lý do tại sao điều chỉnh adapter có thể vượt qua fine-tuning toàn bộ mô hình với rất ít tham số. Chúng tôi sử dụng các thí nghiệm probing ngôn ngữ học để khám phá điểm này.
Thiết kế RQ5: Chúng tôi sử dụng các thí nghiệm probing để đánh giá các embedding trạng thái ẩn của nhiều mô hình và đo lường khả năng của chúng để nắm bắt các đặc điểm cơ bản liên quan đến mã nguồn. Chúng tôi áp dụng ba tác vụ probing về dự đoán độ dài mã nguồn, độ phức tạp vòng lặp và phát hiện kiểu không hợp lệ [14]. Các tác vụ này tương ứng với probing thông tin bề mặt, cú pháp và ngữ nghĩa của mã nguồn, tương ứng. Chính xác, sau khi fine-tuning các mô hình khác nhau trên các tác vụ downstream, chúng tôi trích xuất các embedding vector được pre-train trên các tác vụ probing để kiểm tra xem các mô hình có phản ánh sự hiểu biết về thông tin mã nguồn không.
Đặc biệt, chúng tôi xác minh xem điều chỉnh adapter cho thấy tính hiệu quả trong các tình huống trên có hoạt động nhất quán trong các thí nghiệm probing không.

IV. THIẾT LẬP THÍ NGHIỆM
Bài báo này áp dụng ba tác vụ downstream: tìm kiếm mã nguồn, tóm tắt mã nguồn, và tóm tắt mã nguồn tài nguyên thấp. Chúng tôi cũng áp dụng ba tác vụ probing để kiểm tra các mô hình: dự đoán độ dài mã nguồn, độ phức tạp vòng lặp và phát hiện kiểu không hợp lệ.
Chúng tôi tiếp theo mô tả chi tiết các mô hình được pre-train, tác vụ, và tập dữ liệu.
A. Các Mô hình Pre-trained
Chúng tôi chọn mô hình tối ưu nhất UniXcoder cho tìm kiếm mã nguồn và mô hình tối ưu nhất CodeT5 cho tác vụ tóm tắt mã nguồn. UniXcoder có thể hỗ trợ cả các tác vụ hiểu biết và tạo sinh mã nguồn. Nó kiểm soát hành vi mô hình thông qua các mặt nạ self-attention, áp dụng kiến trúc encoder-only cho tìm kiếm mã nguồn và kiến trúc encoder-decoder cho tóm tắt mã nguồn. Đối với CodeT5, nó có hai phiên bản CodeT5-small (60M) và CodeT5-base (220M). Chúng tôi sử dụng CodeT5-base hoạt động tốt cho tóm tắt mã nguồn.
Chúng tôi cũng so sánh kết quả với những kết quả trên các mô hình CodeBERT, GraphCodeBERT và PLBART [15]. CodeBERT và GraphCodeBERT áp dụng cùng kiến trúc như RoBERTa [16] với 125M tham số. CodeBERT được pre-train với dữ liệu song phương của mã nguồn và ngôn ngữ tự nhiên. GraphCodeBERT tích hợp thông tin luồng dữ liệu vào quá trình pre-training trên cơ sở CodeBERT. Hai mô hình này là các mô hình encoder-only cho hiểu biết mã nguồn. Chúng tôi tuân theo công việc trước đó và thêm sáu lớp transformer vào chúng làm decoder cho tác vụ tạo sinh mã nguồn. PLBART áp dụng kiến trúc encoder-decoder và áp dụng các mục tiêu denoising để pre-train mô hình với mã nguồn và ngôn ngữ tự nhiên. Nó có 140M tham số cho cả các tác vụ hiểu biết và tạo sinh mã nguồn.
B. Tác vụ
a) Tóm tắt Mã nguồn: Tóm tắt mã nguồn nhằm tạo ra một bản tóm tắt văn bản mô tả mã nguồn. Đầu vào cho mô hình là một đoạn mã nguồn, và đầu ra là một mô tả bằng ngôn ngữ tự nhiên về chức năng của mã nguồn.
b) Tìm kiếm Mã nguồn: Cho một truy vấn ngôn ngữ tự nhiên làm đầu vào, tác vụ tìm kiếm mã nguồn là tìm mã nguồn liên quan nhất về mặt ngữ nghĩa từ một tập hợp các chương trình ứng cử viên. Vì CodeT5 không cung cấp bất kỳ kết quả nào về tìm kiếm mã nguồn, chúng tôi chỉ sử dụng UniXcoder cho tác vụ này.
c) Dự đoán Độ dài Mã nguồn (LEN): Lượng thông tin trong mã nguồn có thể thay đổi theo độ dài. Chúng tôi mong muốn sử dụng một tác vụ trực quan để dự đoán độ dài mã nguồn và kiểm tra xem các mô hình khác nhau có mã hóa thông tin bề mặt như vậy không. Để đơn giản, tác vụ này được chuyển đổi thành một tác vụ phân loại dự đoán chuỗi mã nguồn rơi vào khoảng độ dài nào trong năm khoảng độ dài.
d) Độ phức tạp Vòng lặp (CPX): Độ phức tạp vòng lặp phản ánh thông tin cấu trúc của mã nguồn. Để thực hiện các tác vụ mã nguồn như tóm tắt mã nguồn, cần thiết để các mô hình hiểu cấu trúc cú pháp của các token mã nguồn đầu vào. Tác vụ này nhằm dự đoán độ phức tạp vòng lặp tương ứng với các token mã nguồn và kiểm tra mức độ mà các mô hình khác nhau mã hóa thông tin cấu trúc. Vì số lượng đường dẫn độc lập tuyến tính thông qua một đoạn mã nguồn xác định độ phức tạp của mã nguồn, có thể là một thử thách để dự đoán nó chỉ dựa trên chuỗi token.
e) Phát hiện Kiểu Không hợp lệ (TYP): Tương tự như các tác vụ denoising của BART [15, 17], tác vụ này là phân biệt các đoạn mã nguồn hợp lệ về mặt ngữ nghĩa khỏi các đoạn không hợp lệ. Các mẫu không hợp lệ được

--- TRANG 4 ---
BẢNG I
TẬP DỮ LIỆU CODESEARCH NET
Ngôn ngữ lập trình Huấn luyện Dev Test Mã nguồn ứng cử viên
Ruby 24,927 1,400 1,261 4,360
JavaScript 58,025 3,885 3,291 13,981
Java 164,923 5,183 10,955 40,347
Go 167,288 7,325 8,122 28,120
PHP 241,241 12,982 14,014 52,660
Python 251,820 13,914 14,918 43,827
xây dựng bằng cách ngẫu nhiên can thiệp vào các kiểu dữ liệu gốc trong các đoạn mã nguồn. Mục đích của TYP là kiểm tra xem các mô hình khác nhau có thể xác định các kiểu dữ liệu không hợp lệ bằng ngữ cảnh mã nguồn và xác minh thêm mức độ mà các mô hình này hiểu ngữ nghĩa mã nguồn.
C. Tập dữ liệu Đánh giá
a) Tóm tắt Mã nguồn: Chúng tôi chọn tập dữ liệu từ CodeXGLUE, tích hợp CodeSearchNet [18] và được loại bỏ trùng lặp cẩn thận. Bảng I cho thấy thống kê của tập dữ liệu. Tập dữ liệu này chứa các cặp đoạn mã nguồn và mô tả ngôn ngữ tự nhiên cho sáu ngôn ngữ lập trình, bao gồm Python, Java, JavaScript, Ruby, Go, và PHP. Từ bảng, có thể thấy rằng có sự khác biệt đáng kể về kích thước dữ liệu cho các ngôn ngữ lập trình khác nhau. Đặc biệt, các tập dữ liệu của Ruby và JavaScript nhỏ hơn nhiều so với các tập dữ liệu của các ngôn ngữ lập trình khác.
b) Tìm kiếm Mã nguồn: Tập dữ liệu này chúng tôi sử dụng được điều chỉnh từ cùng tập dữ liệu CodeSearchNet với các mã nguồn ứng cử viên bổ sung bởi Guo et al. [6]. Ngoại trừ các mã nguồn ứng cử viên bổ sung để truy xuất, tập dữ liệu giống với tập dữ liệu chúng tôi sử dụng cho tóm tắt mã nguồn.
c) Tập dữ liệu cho Tác vụ Probing: Chúng tôi áp dụng tập dữ liệu từ Karmakar và Robbes [14] cho các tác vụ probing. Chi tiết, nhãn độ dài được đặt thành 5 class-bin (0-50, 50-100, v.v.) cho tác vụ LEN. Nhãn độ phức tạp của CPX được thu thập với công cụ metrix++, dao động từ 0 đến 9. Các đoạn mã nguồn được chia thành hai lớp cho tác vụ TYP dựa trên việc nó có chứa kiểu không hợp lệ hay không. Tập dữ liệu cho mỗi tác vụ chứa 10k mẫu và được cân bằng lớp.
D. Chỉ số Đánh giá
a) Tóm tắt Mã nguồn: Theo công việc trước đó, chúng tôi sử dụng BLEU-4 làm mịn [19] làm chỉ số đánh giá. Đây là một chỉ số dựa trên độ chính xác và đo lường độ chính xác hình học n-gram giữa văn bản được tạo ra và văn bản sự thật cơ bản. Chúng tôi cũng theo công việc trước đó và lấy trung bình BLEU trên các ngôn ngữ lập trình để báo cáo điểm số tổng thể.
b) Tìm kiếm Mã nguồn: Chúng tôi sử dụng Mean Reciprocal Rank (MRR) làm chỉ số đánh giá. MRR là trung bình của thứ hạng đảo ngược của kết quả của một tập hợp truy vấn. Thứ hạng đảo ngược của một truy vấn là nghịch đảo của thứ hạng của kết quả trúng đầu tiên.
c) Tác vụ Probing: Tất cả các tác vụ probing chúng tôi sử dụng đều là các tác vụ phân loại, và chúng tôi sử dụng độ chính xác phân loại làm chỉ số cho các tác vụ này.
E. Chi tiết Thực hiện
Mã nguồn của chúng tôi đều được thực hiện trong Pytorch¹. Chúng tôi tải các mô hình được pre-train từ Huggingface² trong khi giữ các cài đặt siêu tham số của chúng. Vì điều chỉnh adapter điều chỉnh ít tham số hơn so với fine-tuning toàn bộ mô hình, chúng tôi đặt tỷ lệ học của UniXcoder là 5e-5, và CodeT5 là 1e-4. Chúng tôi tái tạo kết quả của các mô hình này trên các tác vụ downstream và trình bày chúng dưới đây.
Đối với cài đặt adapter, chúng tôi chèn adapter sau self-attention layer và feed-forward layer của encoder và decoder. Chiều của adapter được đặt thành 128. Tất cả các thí nghiệm được tiến hành trên 4 card NVIDIA Tesla V100 và mỗi card có 32GB bộ nhớ đồ họa.

V. KẾT QUẢ THÍ NGHIỆM
A. RQ1: Fine-tuning Đa ngôn ngữ Hoạt động Như thế nào trên Các Mô hình và Tác vụ Downstream Khác nhau?
a) Tóm tắt Mã nguồn: Trong phần này, chúng tôi so sánh kết quả của fine-tuning đa ngôn ngữ và đơn ngôn ngữ trên tác vụ tóm tắt mã nguồn dựa trên các mô hình được pre-train khác nhau. Fine-tuning đơn ngôn ngữ là cách ban đầu để fine-tune một mô hình trên tập dữ liệu trong mỗi ngôn ngữ, và fine-tuning đa ngôn ngữ chỉ điều chỉnh một mô hình có cùng kích thước cho tất cả các ngôn ngữ lập trình. Các mô hình được pre-train bao gồm CodeBERT, GraphCodeBERT, PLBART, UniXcoder, và CodeT5, trong đó CodeT5 là mô hình tối ưu nhất cho tác vụ này.
Kết quả được thể hiện trong Bảng II. Chúng tôi ký hiệu các mô hình được fine-tune đa ngôn ngữ với tiền tố m, như mCodeBERT là một mô hình đa ngôn ngữ được fine-tune dựa trên CodeBERT. Kết quả trên CodeBERT và GraphCodeBERT là từ Ahmed và Devanbu [4]. Có thể thấy rõ rằng kết quả của fine-tuning đa ngôn ngữ dựa trên CodeBERT và GraphCodeBERT tốt hơn đáng kể so với fine-tuning đơn ngôn ngữ, và fine-tuning đa ngôn ngữ cho thấy tính hiệu quả của nó trong tất cả sáu ngôn ngữ lập trình. Nhìn chung, các cải thiện cũng đáng kể, với cải thiện 6,90% trên CodeBERT và 5,64% trên GraphCodeBERT.
Tuy nhiên, trên PLBART, UniXcoder, và CodeT5, kết quả cho thấy xu hướng khác. Điểm số tổng thể của PLBART, UniXcoder, và CodeT5 thay vào đó giảm xuống. Kết quả của PLBART là từ kho lưu trữ mã nguồn mở³ của nó, nơi các tác giả đã tiến hành các thí nghiệm khám phá về các tác vụ tóm tắt và tạo sinh mã nguồn đa ngôn ngữ. Kết quả trên cả hai tác vụ cho thấy rằng fine-tuning đa ngôn ngữ dẫn đến suy giảm hiệu suất trên hầu hết các ngôn ngữ lập trình. Trên UniXcoder, điều chỉnh đa ngôn ngữ gây ra suy giảm hiệu suất ở một nửa số ngôn ngữ lập trình. Trên CodeT5, điều chỉnh đa ngôn ngữ chỉ cải thiện trên Ruby.
b) Tìm kiếm Mã nguồn: Chúng tôi cũng kiểm tra tính hiệu quả của fine-tuning đa ngôn ngữ trên tìm kiếm mã nguồn, và kết quả được thể hiện trong Bảng III. Vì CodeT5 không báo cáo kết quả về tìm kiếm mã nguồn
¹https://pytorch.org/
²https://huggingface.co/models
³https://github.com/wasiahmad/PLBART/tree/main/multilingual/

--- TRANG 5 ---
BẢNG II
TÍNH HIỆU QUẢ CỦA FINE-TUNING ĐA NGÔN NGỮ CHO TÓM TẮT MÃ NGUỒN
Mô hình Ruby JavaScript Java Go PHP Python Tổng thể
CodeBERT 12.53 13.86 18.72 18.15 25.48 18.25 17.83
mCodeBERT 14.75 15.80 20.11 18.77 26.23 18.71 19.06
GraphCodeBERT 12.62 14.79 19.22 18.40 25.45 18.02 18.08
mGraphCodeBERT 14.95 15.79 19.91 18.92 26.15 18.90 19.10
PLBART 13.94 16.36 18.73 17.99 24.21 19.79 18.50
mPLBART 13.99 14.11 18.14 17.82 23.41 17.48 17.49
UniXcoder 15.07 15.69 20.15 19.22 26.36 19.14 19.27
mUniXcoder 14.97 15.78 19.95 19.13 26.41 19.38 19.27
CodeT5 15.18 16.09 20.23 19.70 25.88 20.26 19.56
mCodeT5 15.23 15.61 19.99 19.66 25.78 20.17 19.41

BẢNG III
TÍNH HIỆU QUẢ CỦA FINE-TUNING ĐA NGÔN NGỮ CHO TÌM KIẾM MÃ NGUỒN
Mô hình Ruby JavaScript Java Go PHP Python Tổng thể
CodeBERT 67.7 61.6 67.6 88.5 62.9 67.6 69.3
mCodeBERT 73.2 64.3 69.7 88.5 63.5 67.8 71.2
GraphCodeBERT 70.8 64.4 69.3 89.4 64.8 69.2 71.3
mGraphCodeBERT 73.8 66.0 71.0 89.4 64.6 69.5 72.4
UniXcoder 73.9 68.9 72.9 91.6 67.5 72.2 74.5
mUniXcoder 76.4 68.4 72.5 91.2 66.8 72.0 74.6
nguồn, chúng tôi chỉ so sánh kết quả dựa trên CodeBERT, GraphCodeBERT, và UniXcoder. UniXcoder là mô hình tối ưu nhất cho tác vụ này.
Như có thể thấy từ Bảng III, fine-tuning đa ngôn ngữ cho thấy tính hiệu quả trên CodeBERT và GraphCodeBERT, vượt trội hơn fine-tuning cho từng ngôn ngữ riêng biệt trên tất cả các ngôn ngữ lập trình. Trên UniXcoder, fine-tuning đa ngôn ngữ chỉ cải thiện trên Ruby và giảm sút trên các ngôn ngữ lập trình khác. Fine-tuning đa ngôn ngữ cải thiện đáng kể hiệu suất trong Ruby, điều này nên được quy cho kích thước dữ liệu của nó. Tập dữ liệu của nó có kích thước dữ liệu nhỏ nhất, và từ Bảng I có thể thấy rằng tập dữ liệu của các ngôn ngữ lập trình khác thậm chí lớn hơn hai đến mười lần so với tập dữ liệu của nó.
Trên cả hai tác vụ, các thí nghiệm của chúng tôi phản ánh kết quả tương tự. Fine-tuning đa ngôn ngữ không còn hiệu quả trên UniXcoder và CodeT5 như trên CodeBERT và GraphCodeBERT. Nó chỉ cho thấy tính ưu việt của nó trong các ngôn ngữ tài nguyên thấp, và điều này phù hợp với các phát hiện của các nghiên cứu trước đó [1–3] rằng các ngôn ngữ tài nguyên thấp có thể được hưởng lợi thông qua chuyển giao kiến thức tích cực trong học đa ngôn ngữ. Trong khi đó, trong các ngôn ngữ lập trình khác, kết quả của fine-tuning đa ngôn ngữ tệ hơn so với fine-tuning đơn ngôn ngữ. Lý do có thể là quên thảm khốc do học nhiều ngôn ngữ lập trình trên cùng một mô hình.
Phát hiện 1: Dựa trên CodeBERT và GraphCodeBERT, fine-tuning đa ngôn ngữ đã cho thấy tính hiệu quả của nó trong tất cả các ngôn ngữ lập trình. Trên UniXcoder và CodeT5, fine-tuning đa ngôn ngữ có lợi cho các ngôn ngữ tài nguyên thấp và đồng thời dẫn đến suy giảm hiệu suất trong các ngôn ngữ lập trình khác.
Python PHP Go Java JavaScript Ruby
Ngôn ngữ lập trình đích Python PHP Go Java JavaScript Ruby Ngôn ngữ lập trình nguồn 0.09 0.49 0.06 0.54 0.39 0.17
0.4 0.21 -0.72 1 0.26 0.26
-0.97 -1.3 0.09 -0.37 -0.5 -0.33
0.24 0.71 0.1 0.03 0.25 0.37
0.28 0.45 0.04 0.92 -0.07 0.48
0.32 1.3 -0.07 1.5 0.22 0.33
−1.0−0.50.00.51.0Hình 2. Cải thiện BLEU-4 tương đối của điều chỉnh adapter so với điều chỉnh toàn bộ mô hình trên tác vụ tóm tắt mã nguồn đa ngôn ngữ.
B. RQ2: Điều chỉnh Adapter Hiệu quả Như thế nào trong Các Tình huống Đa ngôn ngữ?
Khi fine-tuning toàn bộ mô hình cho nhiều ngôn ngữ lập trình gây ra suy giảm hiệu suất do quên thảm khốc, chúng tôi chọn điều chỉnh adapter để cố định hầu hết các tham số được pre-train và cập nhật một số lượng nhỏ tham số. Chúng tôi đầu tiên thử nghiệm với việc áp dụng điều chỉnh adapter cho các tình huống đa ngôn ngữ của tóm tắt mã nguồn và tìm kiếm mã nguồn.
a) Tóm tắt Mã nguồn: Dựa trên CodeT5, chúng tôi so sánh hiệu suất của điều chỉnh adapter và fine-tuning toàn bộ mô hình

--- TRANG 6 ---
Python PHP Go Java JavaScript Ruby
Ngôn ngữ lập trình đích Python PHP Go Java JavaScript Ruby Ngôn ngữ lập trình nguồn -0.3 0 0.9 0.6 0.2 0.2
0.4 -1 0.2 0 1.1 0.5
0.4 0 0.1 -0.1 0.8 0.7
-0.1 -0.9 0.9 -0.8 0.7 -0.1
1.1 0.2 0.4 0.7 0.1 1.1
0.9 0.1 1.1 0.7 0.7 1
−1.00−0.75−0.50−0.250.000.250.500.751.00Hình 3. Cải thiện MRR tương đối của điều chỉnh adapter so với điều chỉnh toàn bộ mô hình trên tác vụ tìm kiếm mã nguồn đa ngôn ngữ.
trên tóm tắt mã nguồn trong các tình huống đa ngôn ngữ. Chúng tôi fine-tune các tham số mô hình trên tập dữ liệu trong một ngôn ngữ lập trình và đánh giá mô hình trên tập dữ liệu trong ngôn ngữ khác. Để có ấn tượng trực quan về sự khác biệt hiệu suất, Hình 2 cho thấy cải thiện BLEU-4 tương đối của điều chỉnh adapter so với fine-tuning toàn bộ mô hình. Trục dọc là ngôn ngữ lập trình của tập huấn luyện, và trục ngang là ngôn ngữ lập trình để đánh giá. Hình 2 cho thấy rằng điều chỉnh adapter hoạt động tốt hơn so với fine-tuning toàn bộ mô hình trên hầu hết các tác vụ đa ngôn ngữ. Ngoại lệ là adapter được điều chỉnh trong ngôn ngữ Go hoạt động tệ hơn trong hầu hết các ngôn ngữ lập trình so với fine-tuning toàn bộ mô hình. Có thể thấy rằng điều chỉnh adapter trong các ngôn ngữ lập trình khác cũng không có lợi đáng kể trong ngôn ngữ Go. Đối với adapter, dữ liệu trong ngôn ngữ Go không dễ thích ứng. Có thể cần nhiều dữ liệu hoặc tham số hơn để tham gia.
b) Tìm kiếm Mã nguồn: Trên tác vụ tìm kiếm mã nguồn, chúng tôi cũng kiểm tra hiệu suất tương đối của điều chỉnh adapter so với fine-tuning toàn bộ mô hình dựa trên UniXcoder, như được thể hiện trong Hình 3. Trong hầu hết các tác vụ đa ngôn ngữ, điều chỉnh adapter vượt trội hơn fine-tuning toàn bộ mô hình với ít cập nhật tham số hơn. Các tác vụ mà điều chỉnh adapter hoạt động tệ hơn so với fine-tuning toàn bộ mô hình chủ yếu được phân bố trong phần đường chéo của hình. Các tác vụ này được huấn luyện và đánh giá trong cùng một ngôn ngữ và không phải là các tác vụ đa ngôn ngữ. Trên các tác vụ đơn ngôn ngữ này, fine-tuning toàn bộ mô hình cho phép nhiều tham số hơn được điều chỉnh để phù hợp với tác vụ so với điều chỉnh adapter, mà không có lo ngại về quên thảm khốc.
Phát hiện 2: Điều chỉnh adapter hiệu quả hơn so với fine-tuning toàn bộ mô hình trong hầu hết các tình huống đa ngôn ngữ trên các tác vụ tìm kiếm và tóm tắt mã nguồn.
C. RQ3: Điều chỉnh Adapter Hiệu quả Như thế nào So với Fine-tuning Toàn bộ Mô hình Đa ngôn ngữ trên Các Mô hình Pre-trained Khác nhau?
Khi điều chỉnh adapter chứng minh tính hiệu quả của nó trong các tình huống đa ngôn ngữ, chúng tôi tiếp tục khám phá hiệu suất của nó trong các tác vụ đa ngôn ngữ.
a) Tóm tắt Mã nguồn: Bảng IV cho thấy kết quả so sánh trên tác vụ tóm tắt mã nguồn. Mô hình với tiền tố m là một mô hình đa ngôn ngữ chỉ yêu cầu một tập hợp tham số và các mô hình khác phải huấn luyện các mô hình cho sáu ngôn ngữ lập trình riêng biệt. Dựa trên UniXcoder và CodeT5, điều chỉnh adapter cho thấy tính hiệu quả của nó trong tất cả các ngôn ngữ lập trình so với fine-tuning toàn bộ mô hình đa ngôn ngữ. So với fine-tuning ban đầu, madapter cũng vượt trội hơn các mô hình đơn ngôn ngữ khác nhau trong hầu hết các ngôn ngữ lập trình với ít cập nhật tham số hơn.
Để xác minh xem sự cải thiện của madapter trên các mô hình được pre-train so với fine-tuning toàn bộ mô hình đa ngôn ngữ có đáng kể hay không, chúng tôi áp dụng kiểm định t-test một phía theo cặp để phân tích thống kê. Giả thuyết không được bác bỏ cho tất cả sáu ngôn ngữ trên CodeT5 và hầu hết các ngôn ngữ trên UniXcoder. Rõ ràng là madapter có cải thiện thống kê đáng kể so với huấn luyện đa ngôn ngữ trên tóm tắt mã nguồn.
b) Tìm kiếm Mã nguồn: Trên tìm kiếm mã nguồn, như được thể hiện trong Bảng V, điều chỉnh adapter cũng vượt trội hơn fine-tuning toàn bộ mô hình đơn ngôn ngữ và đa ngôn ngữ trong hầu hết các ngôn ngữ lập trình. Sự cải thiện đặc biệt đáng kể đối với các ngôn ngữ lập trình tài nguyên thấp, chẳng hạn như Ruby và JavaScript. Kết quả phân tích thống kê cho thấy rằng sự cải thiện của madapter so với huấn luyện đa ngôn ngữ có ý nghĩa thống kê (p < 0,001) cho tất cả các ngôn ngữ lập trình ngoại trừ Ruby với p-value là 0,004.
Phát hiện 3: Mặc dù ít tham số hơn được cập nhật, điều chỉnh adapter hiệu quả hơn trong học đa ngôn ngữ so với fine-tuning toàn bộ mô hình. Hơn nữa, nó cũng vượt trội hơn kết quả của fine-tuning riêng biệt cho từng ngôn ngữ lập trình.
D. RQ4: Fine-tuning Đa ngôn ngữ Hiệu quả Như thế nào trong Các Tình huống Tài nguyên Thấp?
Các mô hình đa ngôn ngữ được fine-tune từ dữ liệu của nhiều ngôn ngữ lập trình. Huấn luyện chung và chuyển giao tích cực sau đó có lợi cho việc học các ngôn ngữ lập trình khác nhau. Trong thực tế, nhiều ngôn ngữ lập trình thiếu dữ liệu được gán nhãn chất lượng cao. Do đó, chúng tôi đánh giá xem một mô hình đa ngôn ngữ hoạt động tốt có thể được học với dữ liệu cực kỳ hạn chế cho mỗi ngôn ngữ lập trình hay không.
Chúng tôi lấy mẫu các ví dụ huấn luyện một cách bình đẳng từ tập dữ liệu của sáu ngôn ngữ lập trình, và tập hợp nhiều tập dữ liệu đa ngôn ngữ có kích thước 600, 1200, 3000 và 6000. Chúng tôi fine-tune mAdapter-CodeT5 trên các tập dữ liệu này và so sánh kết quả với điều chỉnh adapter trên toàn bộ tập dữ liệu. Kết quả được thể hiện trong Bảng VI.
Bảng cho thấy rằng khi số lượng mẫu huấn luyện tăng lên, hiệu suất của điều chỉnh adapter tiếp tục cải thiện. Có sự khác biệt đáng kể giữa kết quả huấn luyện sử dụng 100 mẫu cho mỗi ngôn ngữ lập trình và các kết quả khác. Tại điểm này, mô hình không thể hội tụ do

--- TRANG 7 ---
BẢNG IV
SO SÁNH TÓM TẮT MÃ NGUỒN
Mô hình Tham số Ruby JavaScript Java Go PHP Python Tổng thể
CodeBERT 6x 173M 12.16 14.90 17.65 18.07 25.16 19.06 17.83
GraphCodeBERT 6x 173M 12.39 14.81 19.00 18.41 25.59 18.06 18.04
PLBART 6x 140M 14.11 15.56 18.45 18.91 23.58 19.30 18.32
UniXcoder 6x 253M 15.07 15.69 20.15 19.22 26.36 19.14 19.27
mAdapter-UniXcoder 10M 15.43 15.87 20.01 19.49 26.46 19.83 19.52
mUniXcoder 263M 14.97 15.78 19.95 19.13 26.41 19.38 19.27
p-value <0.001 <0.001 0.440 <0.001 0.386 <0.001 <0.001
CodeT5 6x 223M 15.18 16.09 20.23 19.70 25.88 20.26 19.56
mAdapter-CodeT5 9M 15.49 16.06 20.42 19.84 26.08 20.52 19.74
mCodeT5 232M 15.23 15.61 19.99 19.66 25.78 20.17 19.41
p-value <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001

BẢNG V
SO SÁNH TÌM KIẾM MÃ NGUỒN
Mô hình Tham số Ruby JavaScript Java Go PHP Python Tổng thể
CodeBERT 6x 125M 67.9 62.0 67.6 88.2 62.8 67.2 69.3
GraphCodeBERT 6x 125M 70.3 64.4 69.1 89.7 64.9 69.2 71.3
PLBART 6x 140M 67.5 61.6 66.3 88.7 61.1 66.3 68.5
UniXcoder 6x 126M 73.9 68.9 72.9 91.6 67.5 72.2 74.5
mAdapter-UniXcoder 5M 77.3 70.2 73.5 90.9 67.1 72.7 75.3
mUniXcoder 131M 76.4 68.4 72.5 91.2 66.8 72.0 74.6
p-value 0.004 <0.001 <0.001 0.986 0.445 <0.001 <0.001

BẢNG VI
KẾT QUẢ CỦA ĐIỀU CHỈNH ĐA NGÔN NGỮ TRONG TÁC VỤ TÓM TẮT MÃ NGUỒN TÀI NGUYÊN THẤP
Mẫu huấn luyện Ruby JavaScript Java Go PHP Python Tổng thể
6x 100 12.81 13.41 12.44 14.89 19.35 15.28 14.70
6x 200 15.03 15.24 18.92 16.38 23.65 18.85 18.01
6x 500 15.21 15.64 19.28 18.34 24.76 19.02 18.71
6x 1,000 15.32 15.62 19.36 18.79 24.72 19.26 18.85
908,224 15.49 16.06 20.42 19.84 26.08 20.52 19.74
thiếu dữ liệu huấn luyện. Khi huấn luyện với 200 mẫu cho mỗi ngôn ngữ lập trình, có sự khác biệt ít hơn 2 BLEU-4 từ fine-tuning toàn bộ tập dữ liệu. Khi số lượng mẫu được tăng lên 1000 cho mỗi ngôn ngữ, sự khác biệt với đường cơ sở là ít hơn 1 BLEU-4. Rõ ràng là huấn luyện đa ngôn ngữ hiệu quả cho tác vụ tóm tắt mã nguồn tài nguyên thấp.
Có thể thấy rằng sự tăng trưởng của hiệu suất mô hình dần dần chậm lại khi dữ liệu tăng lên. Trong sự so sánh giữa hàng cuối cùng và hàng áp cuối, thậm chí hơn 900.000 mẫu mang lại cải thiện không quá 1 điểm BLEU-4. Điều này cho thấy, một mặt, mô hình được pre-train đủ mạnh và có thể nhanh chóng thích ứng với tác vụ downstream với rất ít dữ liệu. Mặt khác, nó chứng minh tiềm năng của fine-tuning đa ngôn ngữ, có thể tận dụng đầy đủ dữ liệu đa ngôn ngữ cho sự hội tụ nhanh chóng của mô hình trong các tình huống tài nguyên thấp.
Phát hiện 4: Fine-tuning đa ngôn ngữ hiệu quả đến mức trong tác vụ tóm tắt mã nguồn tài nguyên thấp có thể tiếp cận kết quả của fine-tuning với toàn bộ tập dữ liệu sử dụng rất ít mẫu cho mỗi ngôn ngữ lập trình.
E. RQ5: Tại sao Điều chỉnh Adapter Tốt hơn Fine-tuning Toàn bộ Mô hình trong Các Tình huống Trên?
Điều chỉnh adapter chứng minh tính hiệu quả của nó trong các tình huống đa ngôn ngữ, đa ngôn ngữ và tài nguyên thấp bằng cách chỉ cập nhật ít tham số. Để kiểm tra các mô hình ở mức độ chi tiết và kiểm tra xem adapter có hoạt động nhất quán hay không, chúng tôi áp dụng các thí nghiệm probing để kiểm tra xem các mô hình có mã hóa một tập hợp các đặc điểm mã nguồn hay không. Chúng tôi áp dụng các tác vụ LEN, CPX và TYP để probing thông tin bề mặt, cấu trúc và ngữ nghĩa của mã nguồn, tương ứng.
Cụ thể, chúng tôi chèn adapter vào BERT [20], CodeBERT, GraphCodeBERT và UniXcoder, và fine-tune chúng trên tìm kiếm mã nguồn cùng với các mô hình ban đầu. Sau đó chúng tôi trích xuất các vector đặc trưng từ các lớp ẩn của các mô hình này và huấn luyện một phân loại tuyến tính đơn giản để liên kết các vector đặc trưng này với các đặc điểm mã nguồn khác nhau. Vì phân loại tuyến tính không có đơn vị ẩn, hiệu suất của nó trên các tác vụ probing phụ thuộc rất nhiều vào các vector đặc trưng từ các mô hình này.
Chúng tôi trích xuất các vector đặc trưng từ các lớp ẩn cuối cùng của các mô hình này để probing, và kết quả được trình bày trong Bảng VII. Nhìn chung, các mô hình này hoạt động tốt nhất trên dự đoán kiểu không hợp lệ. Adapter-UniXcoder đạt 92,65% độ chính xác trên tác vụ này, chỉ ra rằng mô hình thực sự mã hóa thông tin ngữ nghĩa như vậy

--- TRANG 8 ---
20253035404550556065707580
012345678910Độ chính xác
LớpLEN
1015202530354045
012345678910Độ chính xác
LớpCPX
50556065707580859095100
012345678910Độ chính xác
LớpTYP
BERTAdapter-BERTCodeBERTAdapter-CodeBERTGraphCodeBERTAdapter-GraphCodeBERTHình 4. Độ chính xác của các mô hình khác nhau trên các tác vụ LEN, CPX và TYP. Trục ngang biểu thị chỉ số của lớp ẩn được sử dụng để probing.

BẢNG VII
ĐỘ CHÍNH XÁC TÁC VỤ PROBING
Mô hình LEN CPX TYP
bề mặt cấu trúc ngữ nghĩa
BERT 23.85 15.55 54.10
Adapter-BERT 65.20 37.00 79.70
CodeBERT 24.15 17.70 53.60
Adapter-CodeBERT 57.95 35.65 87.50
GraphCodeBERT 35.25 24.30 61.85
Adapter-GraphCodeBERT 44.95 40.35 82.30
UniXcoder 47.25 31.30 86.70
Adapter-UniXcoder 52.55 36.25 92.65
thông tin. Adapter-BERT hoạt động tốt nhất trên dự đoán độ dài mã nguồn với 65,20% độ chính xác. Tác vụ này về cơ bản dự đoán số lượng token trong chuỗi đầu vào, và tác vụ pre-training bổ sung của các mô hình khác có thể gây tổn hại cho tác vụ này. Phát hiện này cũng phù hợp với kết luận được tìm thấy bởi Karmakar và Robbes [14] trong các thí nghiệm probing của họ. Trên tác vụ độ phức tạp vòng lặp, Adapter-GraphCodeBERT là mô hình hoạt động tốt nhất. Vì GraphCodeBERT được pre-train đặc biệt trên thông tin cấu trúc, kết quả này không ngoài dự đoán. Tuy nhiên, lợi thế của nó so với các mô hình khác không rõ ràng, có lẽ vì bản thân tác vụ này thử thách hơn.
So sánh thêm các mô hình được fine-tune với adapter với các mô hình ban đầu trong Bảng VII, có sự khác biệt khá đáng kể trong hiệu suất của chúng, đặc biệt là trên BERT, CodeBERT và GraphCodeBERT. Trên tất cả các tác vụ probing, các mô hình với điều chỉnh adapter cho thấy sự ưu việt rõ ràng so với các mô hình ban đầu.
Để so sánh rõ ràng hơn điều chỉnh adapter với các mô hình ban đầu của chúng, chúng tôi cũng trích xuất các biểu diễn ẩn của tất cả các lớp trước đó làm vector đặc trưng và đánh giá hiệu suất của chúng trên các tác vụ probing. Hình 4 cho thấy hiệu suất trên ba tác vụ probing thay đổi như thế nào với chỉ số lớp ẩn trên BERT, CodeBERT và GraphCodeBERT.⁴
Các đường đứt nét trong hình biểu thị hiệu suất của các mô hình với điều chỉnh adapter trong các tác vụ probing. Các đường liền nét cùng màu biểu thị hiệu suất của các mô hình ban đầu tương ứng. Nhìn chung, mặc dù mỗi tác vụ probing tương ứng với các đặc điểm mã nguồn khác nhau để kiểm tra, có rất nhiều điểm tương đồng trong sự thay đổi độ chính xác trên các mô hình về các tác vụ này. Trong sáu lớp ẩn đầu tiên, có ít sự khác biệt giữa hiệu suất của điều chỉnh adapter và fine-tuning toàn bộ mô hình. Trong các lớp ẩn tiếp theo, khoảng cách hiệu suất giữa điều chỉnh adapter và fine-tuning ban đầu ngày càng nổi bật.
Cần lưu ý rằng các tác vụ này được fine-tune cho tìm kiếm mã nguồn. Tuy nhiên, các đặc điểm mã nguồn cần thiết cho các tác vụ probing này không phải lúc nào cũng hữu ích cho tác vụ downstream. Do đó, độ chính xác của các mô hình khác nhau trên các tác vụ probing giảm ở các lớp ẩn cuối cùng. Vì điều chỉnh adapter hoạt động tốt hơn đáng kể so với fine-tuning toàn bộ mô hình trong các lớp ẩn cuối cùng, các mô hình với điều chỉnh adapter mã hóa nhiều thông tin hơn so với các mô hình ban đầu. Ngược lại, fine-tuning toàn bộ mô hình gặp phải quên thảm khốc và loại bỏ thông tin này. Các đặc điểm mã nguồn này là thông tin cấp thấp và được tổng quát hóa trên các ngôn ngữ hơn so với thông tin cần thiết cho các tác vụ downstream. Do đó, điều chỉnh adapter có thể hoạt động tốt hơn trên các tác vụ đa ngôn ngữ và đa ngôn ngữ với sự giúp đỡ của thông tin này. Chúng tôi đoán rằng đây là lý do tại sao điều chỉnh adapter có thể giảm thiểu các vấn đề quên thảm khốc.
Từ những thay đổi tổng thể của độ chính xác, có thể thấy rằng độ chính xác trên LEN giảm theo độ sâu của các lớp ẩn. Ngược lại, độ chính xác của CPX tăng trước rồi giảm dần. Ngoại trừ mô hình ngôn ngữ tự nhiên được pre-train BERT, hiệu suất tốt nhất của CodeBERT và GraphCodeBERT cũng được đạt được trong lớp ẩn giữa trên TYP. Vì LEN tương ứng với probing thông tin bề mặt và CPX và TYP tương ứng với thông tin cấu trúc và ngữ nghĩa của mã nguồn, rõ ràng là các mô hình này học thông tin bề mặt trong các lớp thấp hơn và các đặc trưng cấu trúc và ngữ nghĩa trong các lớp sâu hơn. Kết luận này phù hợp với các nghiên cứu trước đó.
Từ góc độ của mỗi tác vụ, Adapter-BERT liên tục duy trì độ chính xác cao trên LEN, trong khi tất cả các mô hình khác loại bỏ một số thông tin bề mặt. Tác vụ probing này
⁴Kết quả chi tiết trên tất cả các mô hình được hiển thị trong kho lưu trữ ẩn danh sau.

--- TRANG 9 ---
BẢNG VIII
SO SÁNH CÁC THIẾT KẾ ADAPTER KHÁC NHAU TRÊN TÓM TẮT MÃ NGUỒN VÀ TÌM KIẾM MÃ NGUỒN
Tác vụ Mô hình Ruby JavaScript Java Go PHP Python Tổng thể
Tóm tắt Mã nguồn mAdapter-CodeT5 15.49 16.06 20.42 19.84 26.08 20.52 19.74
mAdapter-MoE-CodeT5 15.62 16.04 20.03 19.79 25.75 20.36 19.60
Tìm kiếm Mã nguồn mAdapter-UniXcoder 77.3 70.2 73.5 90.9 67.1 72.7 75.3
mAdapter-MoE-UniXcoder 76.6 69.1 72.8 90.7 66.3 72.1 74.6

BẢNG IX
TÁC ĐỘNG CỦA CÁC MINI-BATCH KHÁC NHAU TRÊN TÓM TẮT MÃ NGUỒN VÀ TÌM KIẾM MÃ NGUỒN
Tác vụ Phương pháp Ruby JavaScript Java Go PHP Python Tổng thể
Tóm tắt Mã nguồn mAdapter-CodeT5- Đa ngôn ngữ 15.49 16.06 20.42 19.84 26.08 20.52 19.74
- Đơn ngôn ngữ 15.66 16.03 20.14 19.76 25.63 20.29 19.59
Tìm kiếm Mã nguồn mAdapter-UniXcoder- Đa ngôn ngữ 76.7 69.3 73.1 90.7 66.5 72.2 74.8
- Đơn ngôn ngữ 77.3 70.2 73.5 90.9 67.1 72.7 75.3
- không có thẻ 76.6 69.9 73.1 90.7 66.9 72.6 75.0
cho thấy sự khác biệt giữa mã nguồn và các mô hình được pre-train bằng ngôn ngữ tự nhiên trong xử lý các tác vụ downstream của mã nguồn. Trên tác vụ CPX, GraphCodeBERT duy trì độ chính xác cao trong các lớp ẩn nông, điều này xác nhận rằng GraphCodeBERT mã hóa đầy đủ thông tin cấu trúc đó. Tuy nhiên, hiệu suất của CPX giảm đáng kể trong các lớp ẩn cuối cùng. Tác vụ downstream có thể không yêu cầu sự tham gia của thông tin độ phức tạp vòng lặp. Trên tác vụ TYP, các mô hình khác nhau với điều chỉnh adapter cuối cùng có độ chính xác cao, và thông tin ngữ nghĩa đó có thể được tham gia hiệu quả vào tác vụ downstream. Sự thay đổi độ chính xác luôn tốt nhất trên GraphCodeBERT, tiếp theo là CodeBERT và sau đó là BERT. Điều này cũng trùng hợp với hiệu suất của các mô hình được pre-train trong tác vụ downstream.
Phát hiện 5: Điều chỉnh adapter vượt trội đáng kể so với fine-tuning toàn bộ mô hình trên tất cả các tác vụ probing. Ngoài ra, chúng tôi quan sát thấy rằng các lớp ẩn của các mô hình này dần dần mã hóa thông tin cấp cao hơn từ nông đến sâu, phù hợp với các nghiên cứu trước đó.

VI. PHÂN TÍCH
Trong phần này, chúng tôi xem xét một số yếu tố tác động đáng kể đến kết quả, bao gồm thiết kế adapter, batch dữ liệu, và chiều của adapter.
A. Một hay Nhiều Adapter?
Nghiên cứu này sử dụng một mô hình đa ngôn ngữ để xử lý các tác vụ trong nhiều ngôn ngữ lập trình. Do đó, sự can thiệp lẫn nhau trong mô hình là không thể tránh khỏi khi xử lý các tác vụ trong các ngôn ngữ lập trình khác nhau. Để giảm thiểu vấn đề này, chúng tôi lấy cảm hứng từ lớp Sparsely-Gated Mixture-of-Expert (MoE) [21] và chia adapter 128 chiều ban đầu thành bốn adapter 32 chiều. Mà không tăng đáng kể các tham số, chúng tôi mong muốn mô hình học nhiều adapter như các chuyên gia xử lý dữ liệu trong các ngôn ngữ lập trình khác nhau riêng biệt. Chúng tôi sử dụng một mạng cổng để thực hiện việc lựa chọn adapter và chọn hai adapter hoạt động tốt nhất để tham gia vào tính toán khi xử lý các mẫu khác nhau.
Dựa trên hai mô hình được pre-train, UniXcoder và CodeT5, chúng tôi so sánh hiệu suất của việc triển khai này với adapter ban đầu trên tìm kiếm mã nguồn và tóm tắt mã nguồn. Kết quả thí nghiệm được thể hiện trong Bảng VIII. Phương pháp này hoạt động tệ hơn so với adapter ban đầu trên cả hai tác vụ. Chúng tôi đoán rằng không có đủ thông tin để hướng dẫn mô hình lựa chọn các adapter khác nhau như chuyên gia, và có thể cần nhiều dữ liệu hơn để hỗ trợ điều này.
B. Mini-batch Đơn ngôn ngữ so với Mini-batch Đa ngôn ngữ
Khi học dữ liệu đa ngôn ngữ, một câu hỏi thú vị là liệu mô hình học các mẫu cùng ngôn ngữ mỗi lần hay học các mẫu ngẫu nhiên có tốt hơn không. Cụ thể, khi tối ưu hóa mô hình sử dụng stochastic gradient descent, gradient được tính toán trên mini-batch. Các mẫu của một mini-batch là tất cả dữ liệu mà mô hình tiếp xúc trong một bước huấn luyện. Lựa chọn mini-batch tốt hơn sẽ tạo điều kiện cho sự hội tụ của mô hình, từ đó cải thiện hiệu suất trên tác vụ downstream.
Chúng tôi kiểm tra hai cài đặt mini-batch. Một sử dụng mini-batch đa ngôn ngữ, sắp xếp ngẫu nhiên các tập dữ liệu đa ngôn ngữ và sau đó chia mini-batch. Cài đặt khác áp dụng mini-batch đơn ngôn ngữ bằng cách chia mini-batch trên mỗi tập dữ liệu đơn ngôn ngữ và sau đó sắp xếp ngẫu nhiên các mini-batch này. Chúng tôi đánh giá các cài đặt này trên cả hai tác vụ và mô hình. Kết quả thí nghiệm được thể hiện trong Bảng IX.
Trên tóm tắt mã nguồn, CodeT5 được huấn luyện với mini-batch đa ngôn ngữ vượt trội hơn cùng mô hình với mini-batch đơn ngôn ngữ. Trên tìm kiếm mã nguồn, phát hiện thí nghiệm của UniXcoder ngược lại. Tuy nhiên, Ahmed và Devanbu [4] phát hiện rằng hiệu suất của cài đặt mini-batch đa ngôn ngữ luôn tốt hơn trong fine-tuning toàn bộ mô hình đa ngôn ngữ. Chúng tôi cho rằng hiện tượng này liên quan đến tác vụ. Tác vụ tìm kiếm mã nguồn có thể yêu cầu nhiều thông tin danh mục ngôn ngữ lập trình hơn để khớp mã nguồn và truy vấn. Ngược lại, mục tiêu của tác vụ tóm tắt mã nguồn đa ngôn ngữ là về việc tạo ra ngôn ngữ tự nhiên

--- TRANG 10 ---
BẢNG X
TÁC ĐỘNG CỦA KÍCH THƯỚC CHIỀU THẮT CỔ CHAI ADAPTER TRÊN TÓM TẮT MÃ NGUỒN VÀ TÌM KIẾM MÃ NGUỒN
Tác vụ Phương pháp Chiều Ruby JavaScript Java Go PHP Python Tổng thể
Tóm tắt Mã nguồn mAdapter-CodeT5 24 15.82 15.95 18.76 15.32 24.34 20.35 18.42
64 15.69 16.03 20.42 19.73 25.54 20.02 19.57
128 15.49 16.06 20.42 19.84 26.08 20.52 19.74
Tìm kiếm Mã nguồn mAdapter-UniXcoder 24 73.9 66.5 70.7 87.9 64.7 70.1 72.3
64 76.8 69.6 73.0 90.7 66.5 72.3 74.8
128 77.3 70.2 73.5 90.9 67.1 72.7 75.3
mô tả. Chúng tôi giới thiệu thêm các thẻ cụ thể cho ngôn ngữ như gợi ý vào tác vụ tìm kiếm mã nguồn để bổ sung thông tin danh mục ngôn ngữ lập trình. Về thí nghiệm, chúng tôi thấy rằng việc thêm các thẻ cụ thể cho ngôn ngữ thực sự cải thiện hiệu suất.
C. Chiều của Adapter
Một tham số quan trọng của adapter là chiều thắt cổ chai của nó, xác định dung lượng của adapter. Trong bài báo này, chúng tôi thay đổi chiều thắt cổ chai adapter từ 24 đến 128 và tiến hành thí nghiệm trên tất cả các tác vụ và mô hình. Bảng X cho thấy rằng việc tăng chiều thắt cổ chai của adapter có thể cải thiện đáng kể hiệu suất của hầu hết các tác vụ trên UniXcoder và CodeT5. Mô hình hoạt động tốt hơn trong Ruby cho tóm tắt mã nguồn trên các chiều adapter nhỏ hơn. Chúng tôi đoán rằng ngôn ngữ tài nguyên thấp này chủ yếu được hưởng lợi từ dữ liệu trong các ngôn ngữ lập trình khác và cũng nhường chỗ cho việc học các ngôn ngữ lập trình khác do dữ liệu không đủ. Một cách tiếp cận học cân bằng hơn cho dữ liệu đa ngôn ngữ có thể được yêu cầu. Trong nghiên cứu này, chúng tôi đặt kích thước adapter là 128. Dung lượng adapter lớn hơn có thể mang lại cải thiện hiệu suất nhiều hơn, nhưng chúng tôi để lại cho công việc tương lai vì nguồn tài nguyên tính toán hạn chế.

VII. CÔNG VIỆC LIÊN QUAN
A. Pre-training cho Ngôn ngữ Lập trình
Với thành công lớn của các mô hình được pre-train trong lĩnh vực xử lý ngôn ngữ tự nhiên, một loạt các mô hình được pre-train trong các ngôn ngữ lập trình đã xuất hiện để tạo điều kiện cho các tác vụ hiểu biết và tạo sinh mã nguồn, chẳng hạn như tìm kiếm mã nguồn, tạo sinh mã nguồn, và phát hiện lỗi [22–27]. CuBERT [28] và CodeBERT được đề xuất đầu tiên để học biểu diễn của các ngôn ngữ lập trình sử dụng dữ liệu không nhãn quy mô lớn theo cách tự giám sát. CuBERT sử dụng mục tiêu pre-training mô hình ngôn ngữ có mặt nạ trong BERT, và CodeBERT được pre-train trong cả ngôn ngữ tự nhiên và lập trình. GraphCodeBERT giới thiệu thông tin luồng dữ liệu trên cơ sở CodeBERT để tạo điều kiện cho việc hiểu cấu trúc mã nguồn. Ngoài các mô hình encoder-only nêu trên, các mô hình decoder-only cũng được đề xuất cho các ngôn ngữ lập trình. GPT-C [29] và CodeGPT [7] đều sử dụng mô hình ngôn ngữ một chiều sử dụng tất cả các token trước đó để dự đoán token tiếp theo cho pre-training.
Một số công việc gần đây khám phá các mô hình encoder-decoder để hỗ trợ cả các tác vụ hiểu biết và tạo sinh, bao gồm PLBART, CodeT5 và UniXcoder. PLBART dựa trên BART và được pre-train với các mục tiêu denoising. CodeT5 điều chỉnh T5 [30] để giải quyết các tác vụ liên quan đến mã nguồn và cho phép học đa tác vụ cho các tác vụ downstream khác nhau. UniXcoder dựa trên UniLM [31] và được pre-train trên dữ liệu đa phương thức, bao gồm mã nguồn, bình luận, và AST. Trong bài báo này, chúng tôi tiến hành thí nghiệm trên các mô hình gần đây UniXcoder và CodeT5.
B. Điều chỉnh Adapter
Các mô-đun adapter ban đầu được điều tra trong các tác vụ thị giác máy tính và đã được sử dụng để thích ứng các mô hình cho nhiều miền [32]. Trong NLP, adapter được sử dụng để điều chỉnh hiệu quả tham số của mô hình Transformer được pre-train cơ bản để thích ứng với các tác vụ mới [11, 33]. Santoro et al. áp dụng các mô-đun adapter để tránh quên thảm khốc [34]. Bapna và Firat sử dụng adapter để fine-tune một mô hình dịch đa ngôn ngữ trong các ngôn ngữ khác nhau [35]. MAD-X pre-train các adapter cụ thể cho tác vụ và cụ thể cho ngôn ngữ và sau đó kết hợp các biểu diễn của chúng để khai thác kiến thức đa tác vụ và ngôn ngữ học [36]. Cũng có các công việc nghiên cứu cho thấy rằng fine-tuning dựa trên adapter có thể hoạt động tốt hơn so với fine-tuning bình thường trên các tình huống few-shot và đa ngôn ngữ [37] và mạnh mẽ hơn dưới các cuộc tấn công đối kháng [38]. Trong bài báo này, chúng tôi cố gắng thích ứng adapter cho các tác vụ hiểu biết và tạo sinh mã nguồn trong nhiều ngôn ngữ lập trình.

VIII. MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
Tính hợp lệ Bên ngoài. Trong nghiên cứu này, chúng tôi thí nghiệm trên số lượng hạn chế các tác vụ, tập dữ liệu, và mô hình được pre-train. Tất cả những điều này có thể mang lại thiên lệch cho kết quả. Chúng tôi chọn các mô hình đại diện và tối ưu nhất và các tập dữ liệu được sử dụng rộng rãi nhất để giảm thiểu vấn đề này. Trong kho lưu trữ mã nguồn mở, chúng tôi bổ sung các thí nghiệm về điều chỉnh adapter với CodeBERT và GraphCodeBERT trên tóm tắt mã nguồn và tìm kiếm mã nguồn. Kết quả vẫn hứa hẹn và phù hợp với UniXcoder và CodeT5. Chúng tôi cũng duy trì cùng random seed trên các mô hình để đảm bảo tính nhất quán của các thí nghiệm. Trong tương lai, chúng tôi sẽ mở rộng điều chỉnh adapter cho nhiều tác vụ trí tuệ mã nguồn và tập dữ liệu hơn. Ngoài ra, các thí nghiệm probing nhắm mục tiêu vào một số khía cạnh cục bộ của thông tin mã nguồn, và kết luận cuối cùng được rút ra bằng quy nạp thay vì chứng minh nghiêm ngặt.
Tính hợp lệ Nội bộ. Chúng tôi minh họa tính hiệu quả của điều chỉnh adapter trong các tình huống khác nhau. Tuy nhiên, adapter được áp dụng trong bài báo của chúng tôi là cấu trúc tiêu chuẩn. Trên thực tế, chúng tôi cố gắng sửa đổi cấu trúc của adapter hoặc mở rộng dung lượng của adapter sử dụng mixture-of-experts, nhưng không có nỗ lực nào trong số này đạt được kết quả mong muốn. Adapter có thể đạt được kết quả tốt hơn với

--- TRANG 11 ---
điều chỉnh tham số ít, điều này nên có những khám phá thú vị hơn để khám phá.

IX. KẾT LUẬN
Chúng tôi bắt đầu nghiên cứu này với sự suy giảm hiệu suất của fine-tuning đa ngôn ngữ và khám phá hiệu suất của điều chỉnh adapter trong các tình huống đa ngôn ngữ, đa ngôn ngữ và tài nguyên thấp. Chúng tôi chứng minh thực nghiệm tính hiệu quả của điều chỉnh adapter trong các tình huống này. Nó vượt trội hơn fine-tuning toàn bộ mô hình với ít cập nhật tham số hơn trong tất cả các tình huống. Chúng tôi thấy rằng fine-tuning đa ngôn ngữ với 200 mẫu ngẫu nhiên cho mỗi ngôn ngữ lập trình có thể tiếp cận hiệu suất của huấn luyện trên toàn bộ tập dữ liệu. Khi fine-tuning toàn bộ mô hình đa ngôn ngữ gặp phải quên thảm khốc, chúng tôi chứng minh thông qua các thí nghiệm probing rằng điều chỉnh adapter có thể khắc phục đáng kể vấn đề này.
Khi các mô hình được pre-train ngày càng khổng lồ, adapter như một mô-đun thần kinh nhỏ cung cấp một cách để đơn giản hóa và tăng tốc học chuyển giao. Trong tương lai, chúng tôi muốn khám phá việc tích hợp kiến thức mã nguồn vào adapter và hiệu suất của adapter trong nhiều tác vụ trí tuệ mã nguồn hơn. Mã nguồn và mô hình của chúng tôi được cung cấp công khai tại: https://github.com/wangdeze18/Multilingual-Adapter-for-SE.

LỜI CẢM ƠN
Các tác giả muốn cảm ơn các nhà phản biện ẩn danh về những nhận xét sâu sắc của họ. Công việc này được hỗ trợ đáng kể bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62032019, 61872373, và 62272473). Công việc này cũng được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia Trung Quốc 2022YFC3400404.

TÀI LIỆU THAM KHẢO
[1] T.-L. Ha, J. Niehues, và A. H. Waibel, "Toward multilingual neural machine translation with universal encoder and decoder," ArXiv, vol. abs/1611.04798, 2016.
[2] O. Firat, B. Sankaran, Y. Al-Onaizan, F. T. Yarman-Vural, và K. Cho, "Zero-resource translation with multilingual neural machine translation," ArXiv, vol. abs/1606.04164, 2016.
[3] R. Dabre, C. Chu, và A. Kunchukuttan, "A comprehensive survey of multilingual neural machine translation," ArXiv, vol. abs/2001.01115, 2020.
[4] T. Ahmed và P. Devanbu, "Multilingual training for software engineering," 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), pp. 1443–1455, 2022.
[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, và M. Zhou, "Codebert: A pre-trained model for programming and natural languages," ArXiv, vol. abs/2002.08155, 2020.
[6] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, J. Yin, D. Jiang, và M. Zhou, "Graphcodebert: Pre-training code representations with data flow," ArXiv, vol. abs/2009.08366, 2021.
[7] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, và S. Liu, "Codexglue: A machine learning benchmark dataset for code understanding and generation," ArXiv, vol. abs/2102.04664, 2021.
[8] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, và J. Yin, "Unixcoder: Unified cross-modal pre-training for code representation," in ACL, 2022.
[9] Y. Wang, W. Wang, S. R. Joty, và S. C. H. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," ArXiv, vol. abs/2109.00859, 2021.
[10] R. M. French, "Catastrophic forgetting in connectionist networks," Trends in Cognitive Sciences, vol. 3, pp. 128–135, 1999.
[11] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, và S. Gelly, "Parameter-efficient transfer learning for nlp," in ICML, 2019.
[12] J. Hewitt và C. D. Manning, "A structural probe for finding syntax in word representations," in NAACL, 2019.
[13] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin, "Attention is all you need," ArXiv, vol. abs/1706.03762, 2017.
[14] A. Karmakar và R. Robbes, "What do pre-trained code models know about code?" 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 1332–1336, 2021.
[15] W. U. Ahmad, S. Chakraborty, B. Ray, và K.-W. Chang, "Unified pre-training for program understanding and generation," ArXiv, vol. abs/2103.06333, 2021.
[16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," ArXiv, vol. abs/1907.11692, 2019.
[17] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, và L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in ACL, 2020.
[18] H. Husain, H. Wu, T. Gazit, M. Allamanis, và M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," ArXiv, vol. abs/1909.09436, 2019.
[19] K. Papineni, S. Roukos, T. Ward, và W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in ACL, 2002.
[20] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," ArXiv, vol. abs/1810.04805, 2019.
[21] N. M. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, và J. Dean, "Outrageously large

--- TRANG 12 ---
neural networks: The sparsely-gated mixture-of-experts layer," ArXiv, vol. abs/1701.06538, 2017.
[22] D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, và X. Liao, "Bridging pre-trained models and downstream tasks for source code understanding," 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), pp. 287–298, 2022.
[23] H. Liu, Y. Yu, S. Li, Y. Guo, D. Wang, và X. Mao, "Bugsum: Deep context understanding for bug report summarization," Proceedings of the 28th International Conference on Program Comprehension, 2020.
[24] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, và P. S. Yu, "Improving automatic source code summarization via deep reinforcement learning," 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 397–407, 2018.
[25] W. Wang, G. Li, S. Shen, X. Xia, và Z. Jin, "Modular tree network for source code representation learning," ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 29, pp. 1–23, 2020.
[26] X. Gu, H. Zhang, và S. Kim, "Deep code search," 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE), pp. 933–944, 2018.
[27] A. N. Lam, A. T. Nguyen, H. A. Nguyen, và T. N. Nguyen, "Bug localization with combination of deep learning and information retrieval," 2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC), pp. 218–229, 2017.
[28] A. Kanade, P. Maniatis, G. Balakrishnan, và K. Shi, "Pre-trained contextual embedding of source code," ArXiv, vol. abs/2001.00059, 2020.
[29] A. Svyatkovskiy, S. K. Deng, S. Fu, và N. Sundaresan, "Intellicode compose: code generation using transformer," Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020.
[30] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, và P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," ArXiv, vol. abs/1910.10683, 2020.
[31] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, và H.-W. Hon, "Unified language model pre-training for natural language understanding and generation," ArXiv, vol. abs/1905.03197, 2019.
[32] S.-A. Rebuffi, H. Bilen, và A. Vedaldi, "Learning multiple visual domains with residual adapters," in NIPS, 2017.
[33] A. C. Stickland và I. Murray, "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning," in ICML, 2019.
[34] A. Santoro, S. Bartunov, M. M. Botvinick, D. Wierstra, và T. P. Lillicrap, "One-shot learning with memory-augmented neural networks," ArXiv, vol. abs/1605.06065, 2016.
[35] E. A. Platanios, M. Sachan, G. Neubig, và T. M. Mitchell, "Contextual parameter generation for universal neural machine translation," ArXiv, vol. abs/1808.08493, 2018.
[36] J. Pfeiffer, I. Vulic, I. Gurevych, và S. Ruder, "Mad-x: An adapter-based framework for multi-task cross-lingual transfer," in EMNLP, 2020.
[37] R. He, L. Liu, H. Ye, Q. Tan, B. Ding, L. Cheng, J.-W. Low, L. Bing, và L. Si, "On the effectiveness of adapter-based tuning for pretrained language model adaptation," ArXiv, vol. abs/2106.03164, 2021.
[38] W. Han, B. Pang, và Y. N. Wu, "Robust transfer learning with pretrained language models through adapters," ArXiv, vol. abs/2108.02340, 2021.
