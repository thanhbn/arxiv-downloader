# 2308.03303.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.03303.pdf
# Kích thước tệp: 512499 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo
LORA-FA: THÍCH ỨNG CẤP THẤP HIỆU QUẢ VỀ BỘ NHỚ CHO TINH CHỈNH CÁC MÔ HÌNH NGÔN NGỮ LỚN

Longteng Zhang1*, Lin Zhang2*, Shaohuai Shi4†, Xiaowen Chu3,2†, Bo Li2
1Đại học Baptist Hồng Kông, 2Đại học Khoa học và Công nghệ Hồng Kông,
3Đại học Khoa học và Công nghệ Hồng Kông (Quảng Châu),
4Viện Công nghệ Harbin, Thâm Quyến
ltzhang@comp.hkbu.edu.hk, lzhangbv@connect.ust.hk,
shaohuais@hit.edu.cn, xwchu@ust.hk, bli@cse.ust.hk

TÓM TẮT
Phương pháp thích ứng cấp thấp (LoRA) có thể giảm đáng kể số lượng tham số có thể huấn luyện để tinh chỉnh các mô hình ngôn ngữ lớn (LLM), tuy nhiên, nó vẫn yêu cầu bộ nhớ kích hoạt tốn kém để cập nhật các trọng số cấp thấp. Việc giảm số lượng lớp LoRA hoặc sử dụng tính toán lại kích hoạt có thể làm hại hiệu suất tinh chỉnh hoặc tăng chi phí tính toán. Trong công trình này, chúng tôi trình bày LoRA-FA, một phương pháp tinh chỉnh hiệu quả về bộ nhớ giảm bộ nhớ kích hoạt mà không làm giảm hiệu suất và không có tính toán lại tốn kém. LoRA-FA chọn đóng băng trọng số chiếu xuống của A và cập nhật trọng số chiếu lên của B trong mỗi lớp LoRA. Điều này đảm bảo sự thay đổi của trọng số mô hình nằm trong không gian cấp thấp trong quá trình tinh chỉnh LLM, đồng thời loại bỏ yêu cầu lưu trữ các kích hoạt đầu vào đầy đủ cấp. Chúng tôi thực hiện các thí nghiệm rộng rãi trên nhiều loại mô hình (RoBERTa, T5, LLaMA) và quy mô mô hình. Kết quả của chúng tôi cho thấy LoRA-FA luôn có thể đạt được độ chính xác tinh chỉnh gần như nhau trên các nhiệm vụ khác nhau so với tinh chỉnh tham số đầy đủ và LoRA. Hơn nữa, LoRA-FA có thể giảm tổng chi phí bộ nhớ lên đến 1,4× so với LoRA.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã trở thành nền tảng của xử lý ngôn ngữ tự nhiên (Brown et al., 2020; Touvron et al., 2023a; OpenAI, 2023; Anil et al., 2023), và việc tinh chỉnh các LLM được huấn luyện trước đã được chứng minh là rất hiệu quả để cải thiện hiệu suất của chúng trong các nhiệm vụ hạ nguồn khác nhau (Liu et al., 2019; Wei et al., 2021) và để cho phép chúng phù hợp với ý định của con người (Ouyang et al., 2022; Bai et al., 2022). Tuy nhiên, việc tinh chỉnh LLM với tham số đầy đủ là cực kỳ tốn kém, ví dụ, tinh chỉnh mô hình LLaMA-65B (Touvron et al., 2023a) với AdamW (Loshchilov & Hutter, 2017) yêu cầu hơn 1TB bộ nhớ GPU để lưu trữ tham số mô hình, gradient và trạng thái tối ưu hóa (Rajbhandari et al., 2020).

Để giảm bộ nhớ của tinh chỉnh tham số đầy đủ, các phương pháp tinh chỉnh hiệu quả tham số (PEFT) được đề xuất để chỉ cập nhật một phần nhỏ tham số, chẳng hạn như trọng số bộ điều hợp (Houlsby et al., 2019; Hu et al., 2022) và trọng số prompt (Li & Liang, 2021; Lester et al., 2021). Trong số các phương pháp này, LoRA (Hu et al., 2022) đã cho thấy đạt được hiệu suất tương đương với tinh chỉnh tham số đầy đủ, và nó đã được sử dụng rộng rãi trong nhiều ứng dụng (Dettmers et al., 2023).

Cụ thể, LoRA thêm một bộ điều hợp cấp thấp song song bên cạnh trọng số của một lớp tuyến tính, như được hiển thị trong Hình 1(b), trong đó W là trọng số được huấn luyện trước, A và B là các trọng số cấp thấp. Bởi vì LoRA đóng băng W và chỉ cập nhật các ma trận nhỏ hơn A và B, chi phí bộ nhớ cho tham số có thể huấn luyện và gradient và trạng thái tối ưu hóa tương ứng có thể được giảm đáng kể, so với tinh chỉnh tham số đầy đủ như được hiển thị trong Hình 1(a), có thể được coi là cập nhật W và đóng băng A và B. Hơn nữa, LoRA không tạo ra độ trễ suy luận bổ sung bằng cách hợp nhất giá trị của AB vào W.

*Đóng góp bằng nhau.
†Tác giả liên hệ.
1arXiv:2308.03303v1 [cs.CL] 7 Aug 2023

--- TRANG 2 ---
Bản thảo
B 
A W 
(a) Tinh chỉnh Tham số Đầy đủ (b) LoRA (c) LoRA-FA có thể huấn luyện đóng băng 
X B 
A W 
X B 
A W 
X 

Hình 1: Minh họa về (a) tinh chỉnh tham số đầy đủ (FT), (b) LoRA, và (c) LoRA-FA.

Tuy nhiên, LoRA vẫn có những hạn chế vì nó yêu cầu tiêu thụ bộ nhớ kích hoạt tốn kém trong các lớp LoRA. Điều này là do kích hoạt đầu vào lớn của X cần được lưu trữ trong quá trình truyền xuôi và được sử dụng để xây dựng gradient của A trong quá trình truyền ngược. Điều này có nghĩa là LoRA không thể giảm chi phí bộ nhớ kích hoạt so với tinh chỉnh tham số đầy đủ. Ví dụ, tinh chỉnh LLaMA-65B với độ dài chuỗi đầu vào 2048 và kích thước batch 4 yêu cầu hơn 50GB bộ nhớ kích hoạt (ở định dạng 16-bit) trong tất cả các lớp LoRA. Để giải quyết vấn đề này, các phương pháp hiện tại chọn một phần các lớp tuyến tính để tinh chỉnh LoRA (Hu et al., 2022) hoặc sử dụng tính toán lại kích hoạt (Chen et al., 2016), tuy nhiên điều này có thể ảnh hưởng đến hiệu suất hoặc hiệu quả tinh chỉnh.

Trong công trình này, chúng tôi đề xuất LoRA với Frozen-A (LoRA-FA), có thể giảm đáng kể dấu chân bộ nhớ kích hoạt của LoRA mà không thêm bất kỳ chi phí tính toán nào. Cụ thể, LoRA-FA chọn đóng băng cả trọng số được huấn luyện trước của W và trọng số chiếu xuống của A, và chỉ cập nhật trọng số chiếu lên của B, như được hiển thị trong Hình 1(c). Bằng cách làm như vậy, chúng ta chỉ cần tính toán gradient của B, điều này yêu cầu lưu trữ một đầu vào nhỏ hơn nhiều của AX trong quá trình truyền xuôi.

Giả sử rằng W ∈ R^(d×d), A ∈ R^(d×r), và B ∈ R^(r×d), trọng số chiếu xuống của A đã ánh xạ một đầu vào d-chiều của X thành một đầu vào r-chiều của XA. Vì chúng ta có r ≪ d, yêu cầu bộ nhớ kích hoạt cho LoRA-FA có thể được giảm đáng kể. Ví dụ, LoRA-FA (với kích thước rank r = 4) có thể giảm bộ nhớ kích hoạt trong một lớp tuyến tính của LLaMA-65B (với chiều ẩn d = 8192) đi 2048 lần so với tinh chỉnh tham số đầy đủ. Đồng thời, LoRA-FA giảm số lượng tham số có thể huấn luyện từ d² đến dr đi 2048 lần.

Chúng tôi khởi tạo A một cách ngẫu nhiên từ phân phối chuẩn (thường là ma trận rank-r) và khởi tạo B bằng không. Điều này đảm bảo rằng các mô hình được huấn luyện trước với các mô-đun LoRA-FA sẽ không thay đổi dự đoán mô hình trước khi chúng ta bắt đầu tinh chỉnh. Trong quá trình thích ứng mô hình, LoRA-FA sẽ cố định A và cập nhật B để cải thiện hiệu suất mô hình, và điều này ngụ ý rằng sự thay đổi của trọng số mô hình tức là ∆W = AB đang nằm trong một không gian cấp thấp, được định nghĩa bởi không gian cột của A được khởi tạo.

Về mặt thực nghiệm, kết quả thí nghiệm của chúng tôi cho thấy LoRA-FA đủ để tinh chỉnh LLM. Ngoài ra, LoRA-FA không thay đổi các tính toán truyền xuôi và truyền ngược của LoRA (trừ việc bỏ qua tính toán gradient của A), do đó, nó sẽ không tăng chi phí tính toán trong giai đoạn tinh chỉnh. Trong quá trình suy luận, tương tự như LoRA, nó có thể hợp nhất các trọng số cấp thấp bằng cách thêm AB vào W, không tạo ra độ trễ suy luận bổ sung so với mô hình được tinh chỉnh đầy đủ.

Chúng tôi thực hiện các thí nghiệm rộng rãi trên nhiều loại và quy mô mô hình. Chúng tôi tinh chỉnh RoBERTa (Liu et al., 2019) trên các nhiệm vụ hiểu ngôn ngữ tự nhiên, T5 (Raffel et al., 2020) trên các nhiệm vụ dịch máy, và LLaMA (Touvron et al., 2023a) trên các benchmark MMLU (Hendrycks et al., 2021). Chúng tôi thấy rằng LoRA-FA của chúng tôi có thể đạt được độ chính xác mô hình rất gần trên nhiều nhiệm vụ so với tinh chỉnh tham số đầy đủ (FT) và LoRA. Về chi phí bộ nhớ, LoRA-FA của chúng tôi có thể giảm tổng chi phí bộ nhớ lên đến 2× và 1,4×, so với tinh chỉnh tham số đầy đủ và LoRA, tương ứng. Ví dụ, LoRA-FA đã giảm dấu chân bộ nhớ từ 56GB xuống 27,5GB để tinh chỉnh mô hình LLaMA-7B. Điều này có nghĩa là chúng ta có thể sử dụng ngân sách tài nguyên thấp hơn (ví dụ, dịch vụ GPU rẻ hơn với kích thước bộ nhớ nhỏ hơn) để đạt được hiệu suất tinh chỉnh tương tự. Bên cạnh đó, chúng tôi nghiên cứu ảnh hưởng của các siêu tham số, và kết quả của chúng tôi cho thấy LoRA-FA mạnh mẽ với các siêu tham số.

2

--- TRANG 3 ---
Bản thảo

Tóm lại, LoRA-FA có một số lợi thế chính: 1) nó hiệu quả về bộ nhớ bằng cách giảm số lượng tham số có thể huấn luyện và kích hoạt, 2) nó không tăng chi phí tính toán trong giai đoạn tinh chỉnh và chi phí độ trễ trong giai đoạn suy luận, và 3) nó đạt được hiệu suất mô hình tương tự trên nhiều mô hình và nhiệm vụ so với tinh chỉnh tham số đầy đủ.

2 KIẾN THỨC NỀN TẢNG

2.1 CÁC MÔ HÌNH NGÔN NGỮ LỚN

Chúng tôi tập trung vào các mô hình ngôn ngữ lớn dựa trên transformer (LLM). Mô hình transformer được đề xuất lần đầu trong (Vaswani et al., 2017) cho nhiệm vụ dịch máy. Sau đó, các mô hình transformer khác nhau đã được sử dụng trong mô hình hóa ngôn ngữ (tức là huấn luyện trước), và các mô hình được huấn luyện trước được thích ứng cho nhiều nhiệm vụ hạ nguồn (Kenton & Toutanova, 2019; Raffel et al., 2020; Brown et al., 2020).

Lấy mô hình GPT chỉ giải mã (Brown et al., 2020) làm ví dụ, nó bao gồm L khối transformer xếp chồng, và mỗi khối có hai mô-đun con: attention đa đầu (MHA) và mạng feed-forward (FFN). Trong MHA, ba lớp tuyến tính biến đổi đầu vào thành query, key và value, chúng được đưa vào self-attention để tương tác, và đầu ra attention được gửi đến một lớp tuyến tính khác. Trong FFN, chúng ta có hai lớp tuyến tính và một hàm kích hoạt GeLU giữa chúng. Đối với MHA và FFN, layernorm và kết nối residual được áp dụng để cải thiện hiệu suất mô hình. Trong LLM, các trọng số trong các lớp tuyến tính này thường chiếm phần lớn tham số mô hình và chịu trách nhiệm cho hầu hết các phép tính flops.

2.2 THÍCH ỨNG CẤP THẤP

Vì tinh chỉnh LLM với tham số đầy đủ rất tốn kém, các phương pháp tinh chỉnh hiệu quả tham số đặc biệt là LoRA (Hu et al., 2022) được đề xuất để chỉ cập nhật một phần nhỏ tham số mô hình để giảm bớt chi phí bộ nhớ, đồng thời đạt được hiệu suất tương đương của tinh chỉnh LLM.

Cụ thể, LoRA thêm một bộ điều hợp cấp thấp bên cạnh trọng số của một lớp tuyến tính như sau:

Y = XW + alpha XAB, (1)

trong đó W ∈ R^(din×dout) là trọng số được huấn luyện trước, din là chiều đầu vào, và dout là chiều đầu ra. Chúng tôi bỏ qua số hạng bias vì nó không ảnh hưởng đến phân tích của chúng tôi. X ∈ R^(b×s×din) và Y ∈ R^(b×s×dout) là các tensor đầu vào và đầu ra, tương ứng, b là kích thước batch và s là độ dài chuỗi.

Đối với phần LoRA, A ∈ R^(din×r) là trọng số chiếu xuống, và B ∈ R^(r×dout) là trọng số chiếu lên, r là kích thước rank, và alpha > 0 là một siêu tham số (thường được đặt là 1/r).

Đối với mô hình transformer, chẳng hạn như GPT (Brown et al., 2020), chúng ta thường có din = dout = d cho bốn lớp tuyến tính trong MHA, và din = d, dout = 4d (hoặc din = 4d, dout = d) cho lớp tuyến tính đầu tiên (hoặc thứ hai) trong FFN¹, trong đó d là chiều ẩn. Theo mặc định, chúng tôi thêm các mô-đun LoRA vào tất cả các lớp tuyến tính trong các khối transformer để tăng cường hiệu suất tinh chỉnh (Zhang et al., 2023b).

Độ phức tạp bộ nhớ. Đối với tinh chỉnh tham số đầy đủ, chúng ta cần cập nhật trọng số của W trong một lớp tuyến tính, có din × dout phần tử, và tổng số tham số trọng số cho mô hình kiểu GPT được cho bởi n = 12d²L². Đối với LoRA, chúng ta chỉ cập nhật hai ma trận cấp thấp, có (din + dout)r phần tử, và tổng số tham số LoRA cho GPT là nr = 18drL. Do đó, LoRA có thể giảm đáng kể số lượng tham số có thể huấn luyện nếu kích thước rank r nhỏ hơn nhiều so với d.

Bây giờ xem xét cài đặt huấn luyện độ chính xác hỗn hợp 16-bit, tinh chỉnh tham số đầy đủ mất 2n byte cho trọng số mô hình, và 14n byte cho gradient và trạng thái tối ưu hóa (trạng thái AdamW 32-bit và bản sao tham số) (Rajbhandari et al., 2020), trong khi LoRA mất 2n byte cho trọng số mô hình, và 16nr byte cho trọng số, gradient và trạng thái tối ưu hóa liên quan đến adaptor. Điều này có nghĩa là LoRA có thể giảm phần chi phí bộ nhớ này khoảng 8 lần nếu chúng ta có nr ≪ n (hoặc r ≪ d).

Tuy nhiên, tình hình khá khác khi so sánh chi phí bộ nhớ kích hoạt. Tinh chỉnh tham số đầy đủ cần lưu trữ đầu vào của X để tính toán gradient của W, trong khi LoRA cần lưu trữ đầu vào của X để tính toán gradient của A, cũng như đầu vào cấp thấp của XA để tính toán gradient của B. Cụ thể, LoRA và tinh chỉnh tham số đầy đủ mất 14bsdL + 8bsrL byte và

¹Chiều mở rộng là 8d/3 cho các mô hình LLaMA bằng cách sử dụng hàm SwiGLU (Touvron et al., 2023a).
²Tổng số tham số đầy đủ sẽ lớn hơn vì chúng tôi không bao gồm embeddings, biases, v.v.

3

--- TRANG 4 ---
Bản thảo

14bsdL byte kích hoạt (ở 16-bit), tương ứng. Bên cạnh đó, cả hai đều sẽ tiêu thụ bộ nhớ kích hoạt trong các thành phần khác như attention, GeLU, và layernorm (Korthikanti et al., 2023).

Do đó, LoRA không thể giảm (và thậm chí tăng) chi phí bộ nhớ kích hoạt so với tinh chỉnh tham số đầy đủ, điều này thật không may trở thành một nút cổ chai bộ nhớ mới.

Thách thức của việc giảm bộ nhớ kích hoạt. Có hai cách để giảm chi phí bộ nhớ kích hoạt của LoRA. Thứ nhất, chúng ta có thể thêm các mô-đun LoRA vào một số lượng nhỏ các lớp tuyến tính, chẳng hạn như các phép chiếu query và value trong mô hình Transformer (Hu et al., 2022), do đó, các lớp tuyến tính đóng băng khác không có LoRA không cần lưu trữ các kích hoạt đầu vào của chúng. Tuy nhiên, phương pháp này có thể ảnh hưởng đến hiệu suất nhiệm vụ tinh chỉnh (Dettmers et al., 2023), và nó cũng tạo ra khó khăn trong việc chọn lớp nào để tinh chỉnh với LoRA (Zhang et al., 2023b). Thứ hai, tính toán lại kích hoạt (Chen et al., 2016; Korthikanti et al., 2023) đã được đề xuất để chỉ checkpoint đầu vào của mỗi khối transformer trong quá trình truyền xuôi, và tính toán lại các kích hoạt cần thiết khác bắt đầu từ checkpoint này trong quá trình truyền ngược. Tuy nhiên, tính toán lại kích hoạt có chi phí tính toán lại rất tốn kém, điều này tạo ra khoảng 1/3 tổng flops tính toán.

3 PHƯƠNG PHÁP LORA-FA

Trước tiên, chúng tôi trình bày thiết kế của phương pháp LoRA-FA, giải thích nó từ góc độ thích ứng mô hình cấp thấp, và phân tích lợi ích của nó trong việc giảm chi phí bộ nhớ. Thứ hai, chúng tôi cho thấy LoRA-FA có thể được tích hợp vào các kỹ thuật tối ưu hóa bộ nhớ khác để tăng cường việc sử dụng của nó. Thứ ba, chúng tôi thảo luận về mối quan hệ giữa LoRA-FA và nén gradient.

3.1 LORA VỚI FROZEN-A

Phương pháp LoRA cập nhật hai ma trận cấp thấp A và B, và sử dụng AB làm sự thay đổi của một trọng số được huấn luyện trước và đóng băng W của một lớp tuyến tính, tức là W + alpha∆W = W + alphaAB. Như chúng tôi đã thảo luận trước đây, LoRA không cập nhật W trực tiếp và nó có thể giảm đáng kể số lượng tham số có thể huấn luyện, nhưng nó vẫn yêu cầu bộ nhớ kích hoạt rất tốn kém.

Để giải quyết vấn đề này, chúng tôi đề xuất LoRA với Frozen-A (LoRA-FA), đóng băng cả W và A, và chỉ cập nhật B trong quá trình tinh chỉnh. Cụ thể, chúng tôi khởi tạo A một cách ngẫu nhiên từ phân phối chuẩn, thường cho một ma trận rank-r, và chúng tôi khởi tạo B bằng không, vì vậy ∆W = AB vẫn bằng không và dự đoán mô hình không bị ảnh hưởng trước khi chúng ta bắt đầu tinh chỉnh.

Thích ứng mô hình cấp thấp. Trong quá trình tinh chỉnh, như được hiển thị trong Hình 1(c), chúng tôi đóng băng A được khởi tạo và W được huấn luyện trước, và cập nhật trọng số chiếu lên B. Do đó, sự thay đổi của trọng số trong quá trình thích ứng mô hình sẽ bị ràng buộc trong một không gian cấp thấp như sau:

∆W = AB = QB̄ = Σ(i=1 to r) Q:,i B̄i,:, (2)

trong đó A = QR là phân tích QR của A, và r cột của Q, tức là Q:,i cho i = 1, ···, r, là các vector đơn vị trực giao, khi A là ma trận rank-r. Chúng tôi ký hiệu B̄ = RB, và suy ra rằng ∆W:,j = Σ(i=1 to r) B̄ij Q:,i, vì vậy bất kỳ cột nào của ∆W là một tổ hợp của k vector trực giao. Nói cách khác, sự thay đổi của trọng số nằm trong một không gian cấp thấp, được định nghĩa bởi không gian cột của A.

Độ phức tạp bộ nhớ. Chúng tôi nghiên cứu độ phức tạp bộ nhớ của LoRA-FA chi tiết. Đối với một mô-đun LoRA-FA, nó chỉ tính toán gradient của B, có dout × r phần tử. Trong mô hình kiểu GPT, tổng tham số có thể huấn luyện là nr/2 = 9drL, tức là một nửa số lượng tham số có thể huấn luyện trong LoRA.

Do đó, chi phí bộ nhớ cho trọng số mô hình và các trạng thái liên quan đến adaptor là 2n + 8nr byte trong huấn luyện độ chính xác hỗn hợp 16-bit. Quan trọng hơn, về bộ nhớ kích hoạt, LoRA-FA chỉ lưu trữ đầu vào cấp thấp của XA để tính toán gradient của B, mất 8bsrL byte kích hoạt (ở 16-bit) cho tất cả các mô-đun LoRA-FA. So với tinh chỉnh tham số đầy đủ, LoRA-FA hiệu quả về bộ nhớ bằng cách giảm đáng kể số lượng tham số có thể huấn luyện và kích hoạt đầu vào.

3.2 KẾT HỢP VỚI CÁC TỐI ƯU HÓA BỘ NHỚ

LoRA-FA có thể được kết hợp tự nhiên với các phương pháp tối ưu hóa bộ nhớ tiên tiến, chẳng hạn như lượng tử hóa trọng số (Dettmers et al., 2023), phân mảnh trọng số (Rajbhandari et al., 2020), và tính toán lại kích hoạt có chọn lọc (Korthikanti et al., 2023).

4

--- TRANG 5 ---
Bản thảo

Lượng tử hóa trọng số. Như đã thảo luận trước đây, chi phí bộ nhớ cho trọng số mô hình ở định dạng 16-bit là 2n, trong đó n là số lượng tham số mô hình. Ví dụ, chi phí bộ nhớ trọng số mô hình là 130GB cho mô hình LLaMA-65B, không thể được giữ trong một GPU NVIDIA A100 (80GB). Trong LoRA-FA, vì các trọng số mô hình được đóng băng trong quá trình tinh chỉnh, chúng ta có thể lượng tử hóa chúng thành độ rộng bit thấp hơn để giảm chi phí bộ nhớ trọng số mô hình mà không ảnh hưởng đến hiệu suất tinh chỉnh. Ví dụ, các phương pháp lượng tử hóa 8-bit (Dettmers et al., 2022a) và 4-bit (Dettmers et al., 2023) có thể được kết hợp với LoRA-FA để giảm bộ nhớ trọng số mô hình đi 2 và thậm chí 4 lần.

Phân mảnh trọng số. Khi huấn luyện LLM trên nhiều GPU với song song dữ liệu, kỹ thuật phân mảnh trọng số hoặc ZeRO stage-3 (Rajbhandari et al., 2020) có thể được kết hợp với LoRA-FA để phân mảnh trọng số mô hình thành các GPU khác nhau, sao cho chi phí bộ nhớ trên mỗi GPU được giảm theo số lượng GPU. Khác với việc sử dụng ZeRO stage-3 trong tinh chỉnh tham số đầy đủ, chúng ta chỉ phân mảnh các trọng số mô hình và all-gather chúng để hỗ trợ các tính toán truyền xuôi và truyền ngược, mà không phân mảnh các trọng số liên quan đến adaptor và gradient và trạng thái tối ưu hóa của chúng. Tuy nhiên, phân mảnh trọng số đã tạo ra chi phí giao tiếp thu thập trọng số tốn kém trong LoRA-FA, trong khi song song dữ liệu chỉ giao tiếp một lượng nhỏ gradient cho các tham số có thể huấn luyện.

Tính toán lại kích hoạt có chọn lọc. Chi phí bộ nhớ kích hoạt tồn tại trong các thành phần khác của mô hình transformer, chẳng hạn như attention, layernorm, GeLU, và dropout (Korthikanti et al., 2023). Để giải quyết vấn đề này, chúng ta có thể sử dụng tính toán lại kích hoạt đầy đủ để lưu trữ đầu vào của mỗi khối transformer. Tuy nhiên, điều này sẽ vô hiệu hóa lợi thế bộ nhớ của LoRA-FA so với LoRA, vì không cần lưu trữ các đầu vào của các lớp LoRA với tính toán lại kích hoạt đầy đủ. Để cân bằng chi phí kích hoạt và chi phí tính toán lại, thay vào đó chúng ta sử dụng tính toán lại kích hoạt có chọn lọc để chỉ tính toán lại một phần các thành phần mô hình. Ví dụ, FlashAttention (Dao et al., 2022) có thể loại bỏ chi phí bộ nhớ của đầu ra softmax attention và tăng tốc các tính toán attention với ít truy cập HBM hơn. Bên cạnh đó, chúng ta có thể tính toán lại dropout bằng cách lưu trữ trạng thái bộ sinh ngẫu nhiên để có được mask chính xác.

3.3 MỐI QUAN HỆ VỚI NÉN GRADIENT

Chúng tôi thảo luận về mối quan hệ giữa LoRA-FA và nén gradient cấp thấp (Vogels et al., 2019; Zhang et al., 2023a). Cho một lớp LoRA-FA (chúng tôi bỏ qua alpha để đơn giản), tức là Y = XW + XAB, gradient của B được tính bởi

dB = A^T X^T dY = A^T dW. (3)

Sự thay đổi của B với một bước cập nhật của vanilla SGD là ∆B = -eta dB, trong đó eta là tốc độ học, vì vậy sự thay đổi của W với A đóng băng là ∆W = A∆B = -eta AA^T dW, và dW là gradient của W.

Điều này ngụ ý rằng LoRA-FA tương đương với một phương pháp nén gradient cấp thấp cho tinh chỉnh tham số đầy đủ, trong đó gradient trọng số được tính toán được nén bởi A^T dW (để giảm chi phí giao tiếp gradient trong bối cảnh nén gradient), và sau đó nó được giải nén bởi A(A^T dW). Bởi vì A được khởi tạo từ phân phối chuẩn, chúng ta có E[AA^T dW] = E[AA^T]dW = rdW, điều này (gần như) cho một nén gradient không thiên lệch.

Tuy nhiên, nén gradient không có lợi thế gì so với LoRA-FA để tinh chỉnh LLM, bởi vì nó vẫn cập nhật tham số đầy đủ với chi phí bộ nhớ lớn, trong khi LoRA-FA với một lượng nhỏ trọng số có thể huấn luyện cũng có thể hưởng lợi từ việc giảm giao tiếp gradient trong cài đặt song song dữ liệu. Bên cạnh đó, hai phương pháp này trở nên khác nhau khi áp dụng các phương pháp thích ứng như AdamW.

4 THÍ NGHIỆM

4.1 HIỆU SUẤT TINH CHỈNH

Chúng tôi đánh giá hiệu suất tinh chỉnh của ba phương pháp: tinh chỉnh tham số đầy đủ (FT), LoRA (Hu et al., 2022) và LoRA-FA trên các loại mô hình và quy mô mô hình khác nhau. Các thí nghiệm của chúng tôi bao gồm một loạt các nhiệm vụ, từ hiểu ngôn ngữ tự nhiên (NLU) đến dịch máy (MT) và sinh ngôn ngữ tự nhiên (NLG). Cụ thể, chúng tôi đánh giá trên benchmark GLUE (Wang et al., 2019) cho các mô hình RoBERTa-base và RoBERTa-large (Liu et al., 2019), dịch WMT16 En-Ro cho các mô hình T5-small, T5-base và T5-large (Raffel et al., 2020), và MMLU (Hendrycks et al., 2021) cho các mô hình LLaMA (Touvron et al., 2023a). Chúng tôi tuân theo các thiết lập của các công trình trước đây (Hu et al., 2022; Dettmers et al., 2023), và chúng tôi thực hiện điều chỉnh siêu tham số cho mỗi thí nghiệm, bao gồm tốc độ học eta ∈ {5×10^-5, 6×10^-5, ..., 1×10^-4, 2×10^-4, ..., 5×10^-3}, và rank LoRA r ∈ {1, 2, 4, 8, 16, 32, 64, 128}. Chúng tôi báo cáo hiệu suất tốt nhất để so sánh công bằng, và tác động của các siêu tham số đã được nghiên cứu trong Mục 4.3. Do ngân sách hạn chế, chúng tôi thực hiện các thí nghiệm trên các thiết bị khác nhau: chúng tôi sử dụng NVIDIA Turing RTX2080Ti cho các mô hình RoBERTa kích thước nhỏ, NVIDIA Ada RTX4090 cho các mô hình T5 kích thước trung bình, và NVIDIA Ampere A100 cho các mô hình LLaMA kích thước lớn.

4.1.1 ROBERTA BASE/LARGE

RoBERTa (Liu et al., 2019) là một mô hình chỉ mã hóa được xây dựng trên BERT (Devlin et al., 2019) và nó sử dụng một sơ đồ tinh chỉnh khác, cùng với việc loại bỏ mục tiêu huấn luyện câu tiếp theo. Nó hoạt động khá tốt trên các nhiệm vụ NLU với quy mô mô hình khá nhỏ so với LLM. Do đó, chúng tôi lấy RoBERTa-base được huấn luyện trước với 125 triệu tham số và RoBERTa-large với 355 triệu tham số để đánh giá hiệu suất tinh chỉnh trên GLUE. Chúng tôi chủ yếu sao chép kết quả của Transformers (Wolf et al., 2020) và (Dettmers et al., 2023) theo thiết lập của họ. Trước khi thực hiện tất cả các thí nghiệm, trước tiên chúng tôi thực hiện tìm kiếm siêu tham số trên một nhiệm vụ MRPC duy nhất để có được cài đặt siêu tham số tối ưu, được sử dụng trong các thí nghiệm khác. Chúng tôi sử dụng kích thước batch 64 để tinh chỉnh RoBERTa-base, và kích thước batch 32 để tinh chỉnh RoBERTa-large. Chúng tôi sử dụng độ dài chuỗi 128 để tinh chỉnh cả hai mô hình. Kết quả được trình bày trong Bảng 1.

Bảng 1: Tinh chỉnh các mô hình RoBERTa-base (RoB-B) và RoBERTa-large (RoB-L) trên benchmark GLUE. Chúng tôi báo cáo tương quan Matthews cho COLA, tương quan Pearson cho STS-B, độ chính xác trung bình được khớp và không khớp cho MNLI, và độ chính xác cho các nhiệm vụ khác. Cao hơn là tốt hơn cho tất cả các chỉ số. Chúng tôi cũng báo cáo số lượng tham số có thể huấn luyện (# TPs) cho mỗi phương pháp.

Mô hình & Phương pháp | # TPs | MRPC | COLA | QNLI | RTE | SST-2 | STS-B | MNLI | QQP | Trung bình
---|---|---|---|---|---|---|---|---|---|---
RoB-B (FT) | 118.9M | 90.1 | 60 | 92.5 | 67.1 | 94.8 | 89.4 | 87.4 | 90.5 | 84.0
RoB-B (LoRA) | 2.4M | 89.5 | 63.6 | 90.5 | 67.5 | 94 | 90.1 | 86.8 | 89.8 | 84.5
RoB-B (LoRA-FA) | 1.8M | 90 | 63.6 | 92.5 | 67.9 | 94.8 | 89.6 | 86.8 | 90.1 | 84.4
RoB-L (FT) | 338.9M | 90.1 | 67.8 | 94.2 | 86 | 96.1 | 92 | 90.2 | 91.1 | 88.4
RoB-L (LoRA) | 5.4M | 90.2 | 68 | 94.4 | 86.3 | 96.2 | 91.9 | 90 | 91.1 | 88.5
RoB-L (LoRA-FA) | 3.7M | 90 | 68 | 94.4 | 86.1 | 96 | 92 | 90.1 | 91.1 | 88.5

Bảng 1 cho thấy cả LoRA và LoRA-FA đều có nhóm tham số có thể huấn luyện nhỏ hơn so với tinh chỉnh tham số đầy đủ, với LoRA-FA giảm thêm quy mô. Ví dụ, LoRA-FA chỉ mất 1,5% tham số đầy đủ trong tinh chỉnh RoBERTa-base, trong khi LoRA mất 2%. Với quy mô nhóm tham số nhỏ như vậy, LoRA-FA vẫn có thể đạt được kết quả gần (và thậm chí tốt hơn) so với tinh chỉnh tham số đầy đủ. Cụ thể, LoRA-FA dẫn đầu trên COLA, QNLI, RTE, SST-2 khi tinh chỉnh RoBERTa-base, và trên COLA, RTE, QQP khi tinh chỉnh RoBERTa-large. LoRA-FA đạt được độ chính xác trung bình 84,4% trong tinh chỉnh RoBERTa-base, và 88,5% trong tinh chỉnh RoBERTa-large, điều này chứng minh rằng LoRA-FA có khả năng tinh chỉnh RoBERTa trên nhiều nhiệm vụ.

4.1.2 T5 SMALL/BASE/LARGE

T5 (Raffel et al., 2020) là một mô hình mã hóa-giải mã được huấn luyện trước trên một hỗn hợp đa nhiệm vụ của các nhiệm vụ không giám sát và có giám sát. T5 hoạt động tốt trên nhiều nhiệm vụ khác nhau bằng cách thêm tiền tố vào đầu vào tương ứng với mỗi nhiệm vụ, ví dụ, cho các nhiệm vụ dịch: translate English to Romanian. T5 có 5 kích thước: T5-small (60M), T5-base (220M), T5-large (770M), T5-3B, T5-11B. Chúng tôi sử dụng kích thước small, base và large của các mô hình T5 để đánh giá hiệu suất của tinh chỉnh tham số đầy đủ, LoRA và LoRA-FA trên các nhiệm vụ dịch WMT16 En-Ro. Chỉ số đánh giá bao gồm BLEU và RougeL. Chúng tôi sử dụng kích thước batch 64 để tinh chỉnh T5-small, 32 để tinh chỉnh T5-base, và 16 để tinh chỉnh T5-large. Kết quả được trình bày trong Bảng 2.

Như được chỉ ra trong Bảng 2, khi so sánh với tinh chỉnh tham số đầy đủ, LoRA-FA thể hiện kích thước tham số có thể huấn luyện giảm. Sự giảm này được định lượng là 0,35% cho T5-small, 0,28% cho T5-base, và

6

--- TRANG 7 ---
Bản thảo

Bảng 2: Tinh chỉnh các mô hình T5-small, T5-base và T5-large với ba phương pháp trên tập dữ liệu WMT16 En-Ro. Chúng tôi báo cáo điểm BLEU và ROUGE-L. Cao hơn là tốt hơn cho tất cả các chỉ số.

Mô hình & Phương pháp | # Tham số Có thể Huấn luyện | BLEU | ROUGE-L
---|---|---|---
T5-small (FT) | 57.7M | 28.7 | 40.1
T5-small (LoRA) | 0.4M | 27 | 39.6
T5-small (LoRA-FA) | 0.2M | 27 | 39.7
T5-base (FT) | 212.6M | 33.4 | 42.6
T5-base (LoRA) | 1.3M | 32.8 | 43.2
T5-base (LoRA-FA) | 0.6M | 33.5 | 42.8
T5-large (FT) | 703.5M | 36.9 | 49
T5-large (LoRA) | 4.5M | 37 | 49.1
T5-large (LoRA-FA) | 2.25M | 37 | 49

0,32% cho T5-large, tương ứng. Đáng ngạc nhiên, chúng tôi thấy rằng LoRA-FA dẫn đầu bảng khi tinh chỉnh T5-base và T5-large. Điều này cho thấy LoRA-FA phù hợp để tinh chỉnh các mô hình T5 tương đối lớn như T5-base và T5-large. Bên cạnh đó, LoRA-FA có thể đạt được hiệu suất tương tự trong tinh chỉnh một mô hình nhỏ như T5-small so với LoRA, nhưng cả hai đều hoạt động hơi kém hơn so với tinh chỉnh tham số đầy đủ. Điều này có thể là do nhóm tham số nhỏ của LoRA và LoRA-FA không thể xử lý tập dữ liệu tinh chỉnh khi được áp dụng cho một mô hình cơ sở kích thước nhỏ, ví dụ T5-small chỉ có 57,7M tham số. Có nhiều hứng thú hơn khi áp dụng LoRA-FA vào các mô hình lớn.

4.1.3 LLAMA

Kết quả tinh chỉnh trên RoBERTa và T5 minh họa rằng LoRA-FA có thể là một giải pháp thay thế cạnh tranh cho tinh chỉnh tham số đầy đủ và LoRA trên các nhiệm vụ NLU và MT. Chúng tôi tiếp tục đánh giá xem LoRA-FA có còn vượt trội trên các mô hình NLG dựa trên decoder lớn hơn, chẳng hạn như LLaMA-7B. Các mô hình LLaMA (Touvron et al., 2023a) là các mô hình ngôn ngữ lớn mã nguồn mở được phát triển bởi Meta AI. Chúng có kích thước từ 7B đến 65B tham số và được huấn luyện trên 1T đến 1,4T token, thiết lập nền tảng cho khả năng hiểu và sinh ngôn ngữ. Để làm cho chúng tuân theo hướng dẫn, chúng tôi tinh chỉnh mô hình LLaMA-7B trên các tập dữ liệu hướng dẫn Alpaca (Taori et al., 2023) và FLAN v2 (Wei et al., 2021). Do quy mô khổng lồ của FLAN v2, chúng tôi lấy mẫu ngẫu nhiên một phần chứa 50k dữ liệu huấn luyện theo (Dettmers et al., 2023). Phần này giữ kích thước tương tự với Alpaca (52k mẫu huấn luyện). Theo các công trình trước đây (Touvron et al., 2023a; Dettmers et al., 2023), chúng tôi đánh giá hiệu suất 5-shot trung bình của các mô hình LLaMA được tinh chỉnh trên các benchmark MMLU (Hendrycks et al., 2021), bao gồm 57 nhiệm vụ bao gồm toán học cơ bản, lịch sử Mỹ, khoa học máy tính, luật, v.v. Chúng tôi sử dụng kích thước batch 32 để tinh chỉnh LLaMA-7B. Kết quả được trình bày trong Bảng 3.

Bảng 3: So sánh độ chính xác MMLU 5-shot trung bình cho các mô hình LLaMA-7B được tinh chỉnh với ba phương pháp trên hai tập dữ liệu khác nhau: Alpaca và FLAN v2. Cao hơn là tốt hơn cho chỉ số độ chính xác. Chúng tôi báo cáo cải thiện hiệu suất tuyệt đối so với mô hình LLaMA-7B cơ bản trong ngoặc đơn.

Mô hình & Phương pháp | # Tham số Có thể Huấn luyện | Độ chính xác MMLU 5-shot
---|---|---
LLaMA-7b-Alpaca (FT) | 6426.3M | 37.6 (+2.5)
LLaMA-7b-Alpaca (LoRA) | 152.5M | 37.2 (+2.1)
LLaMA-7b-Alpaca (LoRA-FA) | 83M | 37.4 (+2.3)
LLaMA-7b-FLANv2 (FT) | 6426.3M | 45.2 (+10.1)
LLaMA-7b-FLANv2 (LoRA) | 152.5M | 43.9 (+8.8)
LLaMA-7b-FLANv2 (LoRA-FA) | 83M | 44 (+8.9)

7

--- TRANG 8 ---
Bản thảo

Bảng 3 cho thấy nhóm tham số của LoRA-FA chỉ chiếm 1,3% của tinh chỉnh tham số đầy đủ. Tinh chỉnh tham số đầy đủ dẫn đầu bảng cho cả Alpaca và FLAN v2, do sức mạnh của nhóm tham số lớn nhất. Trong khi đó, LoRA-FA đạt được hiệu suất cạnh tranh chỉ sử dụng 83 triệu tham số có thể huấn luyện, và nó hoạt động tốt hơn LoRA trong các đánh giá của chúng tôi. Điều này cho thấy LoRA-FA có khả năng tinh chỉnh LLaMA với ít tham số có thể huấn luyện hơn nhiều, và có thể nhắm tới hiệu suất tương tự và thậm chí tốt hơn LoRA. Hơn nữa, các cải thiện hiệu suất tuyệt đối (ví dụ, 10,1% với FLAN v2) so với mô hình cơ bản xác nhận tầm quan trọng của điều chỉnh hướng dẫn, và FLAN v2 hữu ích hơn Alpaca để cải thiện khả năng giải quyết vấn đề của LLM.

4.1.4 HIỆU SUẤT HỘI TỤ

Do ít tham số có thể huấn luyện hơn mà LoRA-FA sử dụng so với tinh chỉnh tham số đầy đủ và LoRA, chúng tôi cung cấp một phân tích về hiệu suất hội tụ để xem liệu nó có tác động đến tốc độ hội tụ hay không. Chúng tôi báo cáo kết quả hội tụ của tinh chỉnh RoBERT-base trên các tập dữ liệu COLA và SST2 làm hai ví dụ trong Hình 2, và kết quả trên các nhiệm vụ khác rất tương tự. Các thí nghiệm được thực hiện trên 4x A100 40GB, chúng tôi sử dụng kích thước batch trên mỗi thiết bị là 320, và độ dài chuỗi là 128.

[Hình 2 hiển thị các biểu đồ so sánh hội tụ]

Hình 2: So sánh hội tụ giữa tinh chỉnh tham số đầy đủ (FT), LoRA, và LoRA-FA cho mô hình RoBERTa-base trên các tập dữ liệu COLA và SST-2.

Tinh chỉnh với LoRA-FA không cho thấy bất kỳ sự suy giảm nào về tốc độ hội tụ dưới các cài đặt siêu tham số phù hợp theo Hình 2. Ở giai đoạn đầu (ví dụ, <10 bước) của tinh chỉnh, LoRA và LoRA-FA có thể hội tụ chậm hơn tinh chỉnh tham số đầy đủ, nhưng tất cả đều có thể đạt được mục tiêu sau vài bước ngắn. Nhìn chung, kết quả của chúng tôi cho thấy LoRA-FA có hiệu suất hội tụ tương tự so với tinh chỉnh tham số đầy đủ và LoRA.

4.2 HIỆU QUẢ BỘ NHỚ

LoRA-FA có thể tiết kiệm một lượng đáng kể việc sử dụng bộ nhớ GPU bằng cách giảm số lượng tham số có thể huấn luyện và chi phí bộ nhớ kích hoạt so với tinh chỉnh tham số đầy đủ. Do đó chúng tôi đưa ra một phân tích về chi phí bộ nhớ GPU của 3 phương pháp (tinh chỉnh tham số đầy đủ, LoRA, LoRA-FA) trong tinh chỉnh các mô hình RoBERTa-base/large, T5-small/base/large, LLaMA-7B. Đối với các cài đặt siêu tham số, chúng tôi sử dụng kích thước batch 128 để tinh chỉnh T5-small, 64 cho RoBERTa-base, RoBERTa-large và T5-base, 32 cho T5-large và 1 cho LLaMA-7B. Đối với LoRA và LoRA-FA, chúng tôi so sánh việc sử dụng bộ nhớ dưới kích thước rank với hiệu suất độ chính xác tốt nhất. Thí nghiệm được chạy trên một GPU A100 duy nhất (40GB). Kết quả được trình bày trong Bảng 4.

Bảng 4: Sử dụng bộ nhớ GPU đỉnh (Mem) tính bằng GB của ba phương pháp tinh chỉnh để tinh chỉnh các mô hình RoBERTa, T5 và LLaMA.

Mô hình & Phương pháp | Rank | Mem | Mô hình & Phương pháp | Rank | Mem
---|---|---|---|---|---
RoBERTa-base (FT) | - | 9.6 | RoBERTa-large (FT) | - | 23.1
RoBERTa-base (LoRA) | 8 | 9.2 | RoBERTa-large (LoRA) | 8 | 22.5
RoBERTa-base (LoRA-FA) | 8 | 6.9 | RoBERTa-large (LoRA-FA) | 8 | 15.7
T5-small (FT) | - | 30.7 | T5-base (FT) | - | 35.7
T5-small (LoRA) | 8 | 29.4 | T5-base (LoRA) | 8 | 32.1
T5-small (LoRA-FA) | 8 | 25.5 | T5-base (LoRA-FA) | 8 | 25.3
T5-large (FT) | - | 40.4 | LLaMA-7B (FT) | - | OOM
T5-large (LoRA) | 16 | 34.3 | LLaMA-7B (LoRA) | 64 | 29.4
T5-large (LoRA-FA) | 16 | 27.6 | LLaMA-7B (LoRA-FA) | 64 | 27.5

Bảng 4 cho thấy LoRA-FA có thể giảm đáng kể việc sử dụng bộ nhớ GPU trong quá trình tinh chỉnh. So với LoRA, LoRA-FA có trung bình 3GB tiết kiệm bộ nhớ trong tinh chỉnh RoBERTa-base, 4 đến 7GB tiết kiệm bộ nhớ trong tinh chỉnh T5-base, T5-large, RoBERTa-large, và 2GB tiết kiệm bộ nhớ trong tinh chỉnh LLaMA-7B, trong khi tinh chỉnh tham số đầy đủ đã gây ra hết bộ nhớ trong tinh chỉnh LLaMA-7B. Những kết quả này đã xác nhận phân tích của chúng tôi trong Mục 2.2, rằng chi phí bộ nhớ kích hoạt có thể được giảm đáng kể khi áp dụng LoRA-FA cho LLM. Chúng tôi sẽ nghiên cứu sự kết hợp của LoRA-FA với các tối ưu hóa bộ nhớ khác trong phiên bản tương lai của chúng tôi.

Chúng tôi tiếp tục đưa ra một phân tích về mối quan hệ giữa dấu chân bộ nhớ GPU và rank của LoRA và LoRA-FA trong tinh chỉnh các mô hình RoBERTa-large và LLaMA-7B. Chúng tôi sử dụng kích thước batch 64 và độ dài chuỗi 128 cho RoBERTa-large, kích thước batch 32 và độ dài nguồn/đích tối đa

8

--- TRANG 9 ---
Bản thảo

128 cho LLaMA-7B. Như được hiển thị trong Hình 3, LoRA mất nhiều bộ nhớ GPU hơn LoRA-FA trong tất cả các rank khi tinh chỉnh cả hai mô hình, với khoảng cách trung bình 5GB trong tinh chỉnh RoBERTa-large, và 1GB trong tinh chỉnh LLaMA-7B. Dấu chân bộ nhớ vẫn ổn định theo các rank, điều này cho thấy LoRA-FA có thể tăng rank một cách không đau đớn từ 1 đến 128 để đạt được hiệu suất tốt hơn mà không gây ra OOM.

[Hình 3: Biểu đồ so sánh dấu chân bộ nhớ GPU theo các kích thước rank khác nhau]

4.3 NGHIÊN CỨU SIÊU THAM SỐ

LoRA-FA đã cho thấy sức mạnh của nó trong hiệu suất tinh chỉnh LLM theo kết quả ở trên, vì nó có thể đạt được độ chính xác gần trong thời gian ngắn nhưng có hiệu quả bộ nhớ runtime tốt. Để xác nhận tính mạnh mẽ đối với siêu tham số của LoRA-FA, chúng tôi tiếp tục thực hiện một nghiên cứu siêu tham số về mối tương quan giữa rank r và tốc độ học eta trên LoRA-FA. Chúng tôi so sánh hiệu suất của LoRA và LoRA-FA trong tinh chỉnh RoBERTa-base trên MRPC dưới một phạm vi rộng các rank và tốc độ học. Chúng tôi sử dụng kích thước batch 64, và độ dài chuỗi 128. Kết quả được trình bày trong Hình 4, cho thấy LoRA và LoRA-FA thể hiện không gian siêu tham số tương tự, trong khi LoRA có phạm vi hơi rộng hơn khi rank eta khoảng 2 và 5e-5 đồng thời. Cả hai phương pháp đều cho thấy cùng một mô hình rằng có mối tương quan âm giữa rank r và tốc độ học eta về hiệu suất tinh chỉnh, tức là khi rank tăng cao hơn, tốc độ học nên được giảm một cách thích hợp để duy trì hiệu suất.

[Hình 4: So sánh hiệu suất tinh chỉnh giữa LoRA và LoRA-FA dưới các rank và tốc độ học khác nhau]

5 CÔNG TRÌNH LIÊN QUAN

Tinh chỉnh có giám sát. Một LLM được huấn luyện trước có hiểu biết tốt về corpus của con người. Nó có thể sinh văn bản bằng cách tiếp tục chuỗi đầu vào, hoặc sinh từ đã được che trong chuỗi, do đó nó có thể được sử dụng cho các nhiệm vụ khác nhau sau khi tinh chỉnh. Thêm một đầu dự đoán vào mô hình chỉ mã hóa (Devlin et al., 2019; Liu et al., 2019; He et al., 2020), sau đó tinh chỉnh mô hình trên tập dữ liệu có chú thích của con người là một cách phổ biến cho các nhiệm vụ NLU, chẳng hạn như COLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), MNLI (Kim et al., 2019) từ benchmark GLUE (Wang et al., 2019). LLM cũng có thể được sử dụng cho các nhiệm vụ text-to-text như dịch thuật, và cấu trúc encoder-decoder được cần thiết trong các mô hình như vậy, bao gồm T5 (Raffel et al., 2020), mT5 (Xue et al., 2021), ByT5 (Xue et al., 2022), UMT5 (Chung et al., 2023), v.v. Tuy nhiên, các phương pháp tinh chỉnh này chủ yếu tập trung vào một nhiệm vụ (tức là chuyên môn hóa đơn nhiệm vụ), có nghĩa là chúng không thể xử lý các hướng dẫn khác nhau của con người (tức là tổng quát hóa đa nhiệm vụ). Để làm cho LLM tuân theo hướng dẫn, chúng được tinh chỉnh trên một hoặc nhiều tập dữ liệu hướng dẫn, chẳng hạn như Alpaca (Taori et al., 2023), Self-instruct (Wang et al., 2022a), UnNaturalInstruction (Honovich et al., 2022), SuperNaturalInstructions (Wang et al., 2022b), và FLAN v2 (Wei et al., 2021), bao gồm các cặp giá trị hướng dẫn-đầu ra. Phương pháp tinh chỉnh này được gọi là điều chỉnh hướng dẫn. Nhiều mô hình được điều chỉnh hướng dẫn gần đây bao gồm InstructGPT (Ouyang et al., 2022), Llama 2 (Touvron et al., 2023b), Guanaco (Dettmers et al., 2023), Vicuna (Chiang et al., 2023), Falcon (Almazrouei et al., 2023), FLAN-T5 (Chung et al., 2022), và chúng đã đạt được hiệu suất tuyệt vời trong việc hiểu kiến thức chung trên nhiều lĩnh vực khác nhau từ OpenLLM Leaderboard (Edward et al., 2023; Gao et al., 2021; Clark et al., 2018; Zellers et al., 2019; Hendrycks et al., 2021; Lin et al., 2022). Trong công trình của chúng tôi, chúng tôi cho thấy LoRA-FA có thể tinh chỉnh các loại mô hình khác nhau, bao gồm mô hình chỉ mã hóa, mã hóa-giải mã và chỉ giải mã.

Tinh chỉnh hiệu quả tham số. Với sự phong phú của LLM được phát hành, các mô hình ngày càng lớn hơn, và tinh chỉnh tham số đầy đủ trở nên không khả thi để huấn luyện chúng trên phần cứng tiêu dùng. Các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) được sinh ra để giải quyết vấn đề này nhằm giảm số lượng tham số có thể huấn luyện bằng các phương pháp khác nhau trong khi duy trì hiệu suất. Ví dụ, Prefix tuning (Li & Liang, 2021) thêm các tham số tiền tố vào các trạng thái ẩn trong mỗi lớp của mô hình. Prompt tuning (Lester et al., 2021; Liu et al., 2021; Gu et al., 2022) sử dụng template để tái cấu trúc prompt, và chỉ cập nhật các tham số liên quan đến hiểu prompt. IA3 (Liu et al., 2022) tiêm các vector học được vào các mô-đun attention và feed-forward. BitFit (Ben Zaken et al., 2022) chỉ cập nhật bias của mô hình. LoRA (Hu et al., 2022) thêm các adapter cấp thấp như một đường vòng tới các lớp tuyến tính. Trong tất cả, LoRA được sử dụng thường xuyên hơn để tinh chỉnh LLM cho các nhiệm vụ mới, và nhiều phương pháp gần đây dựa trên LoRA đã được đề xuất. QLoRA (Dettmers et al., 2023) tinh chỉnh một mô hình được lượng tử hóa với LoRA. ReLoRA (Lialin et al., 2023) áp dụng một chiến lược làm nóng với LoRA cho huấn luyện trước. LoraHub (Huang et al., 2023) đề xuất một chiến lược để tự động xây dựng các mô-đun LoRA cho một mô hình trong tinh chỉnh với các nhiệm vụ đa dạng được đưa ra. GLoRA (Chavan et al., 2023) thêm một mô-đun prompt bổ sung vào mô hình, và tiêm các vector để tái scale trọng số và bias. Ngược lại, LoRA-FA đã cho thấy sức mạnh của nó trong việc sử dụng bộ nhớ trong khi bảo tồn hiệu suất so với LoRA khi tinh chỉnh LLM. Chúng tôi sẽ so sánh với nhiều phương pháp PEFT hơn trong công trình tương lai.

Huấn luyện hiệu quả bộ nhớ. Để tải hoặc huấn luyện LLM lên phần cứng hiệu quả và có thể mở rộng hơn, nhiều phương pháp huấn luyện hiệu quả bộ nhớ đã được đề xuất. ZeRO (Rajbhandari et al., 2020)

10

--- TRANG 11 ---
Bản thảo

phân vùng các tham số, gradient và trạng thái tối ưu hóa đều đặn trên tất cả các GPU, và mỗi GPU có một phân vùng duy nhất cũng được gọi là một mảnh. Ở giai đoạn tính toán, mỗi GPU xây dựng trọng số của mỗi lớp bằng cách yêu cầu các GPU tham gia gửi thông tin mà nó thiếu. Tương tự, FSDP (Zhao et al., 2023) phân mảnh tất cả các trạng thái này trên các worker song song dữ liệu, và nó có thể tùy chọn offload các tham số mô hình được phân mảnh vào CPU. Tính toán lại kích hoạt (Korthikanti et al., 2023; Jain et al., 2020; Smith et al., 2022), còn được gọi là gradient checkpointing, được sử dụng để tiết kiệm bộ nhớ trong quá trình forward pass bằng cách tính toán lại các kích hoạt trung gian just-in-time trong quá trình backward pass. Offloading (Ren et al., 2021; Shoeybi et al., 2020) là một kỹ thuật để offload các trọng số hoặc trạng thái vào CPU và chỉ tải chúng vào GPU khi cần thiết. Lượng tử hóa (Dettmers et al., 2023; Jacob et al., 2017; Dettmers et al., 2022b) tập trung vào việc lượng tử hóa các tham số và gradient thành các biểu diễn bit thấp, chẳng hạn như số thực hoặc số nguyên 8-bit, số thực hoặc số nguyên 4-bit, hoặc thậm chí kiểu dữ liệu 1-bit. LoRA-FA cho thấy một lợi thế trong việc giảm tham số có thể huấn luyện và bộ nhớ kích hoạt trong tinh chỉnh LLM, và nó có thể được kết hợp với các phương pháp huấn luyện hiệu quả bộ nhớ ở trên.

6 KẾT LUẬN

Trong công trình này, chúng tôi đã đề xuất một phương pháp PEFT mới LoRA-FA, yêu cầu ít dấu chân bộ nhớ hơn nhiều bằng cách giảm các tham số có thể huấn luyện và chi phí bộ nhớ kích hoạt. Chúng tôi đã thực hiện các thí nghiệm rộng rãi để cho thấy LoRA-FA có thể đạt được hiệu suất tinh chỉnh tương tự so với hai baseline mạnh: tinh chỉnh tham số đầy đủ và LoRA. Trong khi đó, LoRA-FA đã giảm lên đến 13GB dấu chân bộ nhớ GPU so với các phương pháp khác mà không có bất kỳ tính toán lại nào. Chúng tôi hy vọng phương pháp của chúng tôi có thể giúp cộng đồng khám phá tiềm năng của việc thích ứng LLM với tài nguyên thấp.

TÀI LIỆU THAM KHẢO

[Phần tài liệu tham khảo tiếp tục với các trích dẫn đầy đủ...]
