# 2308.08285.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2308.08285.pdf
# Kích thước tệp: 340609 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tiền huấn luyện với Mở rộng Tài liệu dựa trên Mô hình Ngôn ngữ Lớn
cho Truy xuất Đoạn văn Dày đặc
Guangyuan Ma1,2*, Xing Wu1,2*, Peng Wang1,2, Zijia Lin3, Songlin Hu1,2
1Viện Kỹ thuật Thông tin, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
2Trường An ninh Mạng, Đại học Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
3Công nghệ Kuaishou
{maguangyuan,wuxing,wangpeng2022,husonglin }@iie.ac.cn, linzijia07@tsinghua.org.cn
Tóm tắt
Trong bài báo này, chúng tôi nghiên cứu một cách có hệ thống tiềm năng của việc tiền huấn luyện với mở rộng tài liệu dựa trên Mô hình Ngôn ngữ Lớn (LLM) cho truy xuất đoạn văn dày đặc. Cụ thể, chúng tôi tận dụng khả năng của LLM để mở rộng tài liệu, tức là tạo truy vấn, và chuyển giao hiệu quả kiến thức mở rộng cho các bộ truy xuất bằng cách sử dụng các chiến lược tiền huấn luyện được thiết kế riêng cho truy xuất đoạn văn. Những chiến lược này bao gồm học tương phản và tạo truy vấn thắt cổ chai. Hơn nữa, chúng tôi kết hợp một chiến lược học chương trình giảng dạy để giảm sự phụ thuộc vào các suy luận LLM. Kết quả thực nghiệm cho thấy tiền huấn luyện với mở rộng tài liệu dựa trên LLM cải thiện đáng kể hiệu suất truy xuất trên các tác vụ tìm kiếm web quy mô lớn. Công trình của chúng tôi cho thấy khả năng truy xuất zero-shot và ngoài miền mạnh mẽ, làm cho nó có thể áp dụng rộng rãi hơn cho truy xuất khi khởi tạo mà không có dữ liệu được gán nhãn bởi con người.

Giới thiệu
Truy xuất đoạn văn dày đặc (Karpukhin et al. 2020) có nhiều ứng dụng thực tế rộng rãi, như tìm kiếm web (Liu et al. 2021; Zou et al. 2023), tạo sinh có tăng cường truy xuất (Lewis et al. 2020; Cai et al. 2022) và trả lời truy vấn (Sakata et al. 2019). Nó sử dụng các bộ truy xuất dựa trên mô hình ngôn ngữ được huấn luyện tốt để trích xuất biểu diễn câu và truy xuất các đoạn văn liên quan với các truy vấn cho trước. Các nghiên cứu gần đây đã đạt được tiến bộ ấn tượng trong việc cải thiện hiệu quả của các bộ truy xuất dày đặc, chẳng hạn như khai thác âm tính khó (Qu et al. 2021), tương tác muộn (Khattab và Zaharia 2020; Santhanam et al. 2022), chưng cất (Ren et al. 2021; Lu et al. 2022), và tổ hợp (Gao và Callan 2022; Wu et al. 2023b). Hơn nữa, sự phát triển của tiền huấn luyện dành riêng cho tác vụ (Gao và Callan 2021; Wu et al. 2023a; Liu và Shao 2022) đẩy giới hạn của các tác vụ truy xuất đến những ranh giới mới. Cụ thể, những nghiên cứu đó thường sử dụng học tương phản với sự hỏng span (Gao và Callan 2022; Izacard et al. 2021; Ma et al. 2022), hoặc bộ giải mã bổ sung với cấu trúc thắt cổ chai (Gao và Callan 2021; Lu et al. 2021; Liu và Shao 2022; Wu et al. 2023a) để học biểu diễn tốt hơn.

Các mô hình ngôn ngữ lớn (LLM), như ChatGPT (Ouyang et al. 2022), PaLM (Chowdhery et al. 2022), LLaMA (Touvron et al. 2023), và tk-Instruct (Wang et al. 2022b), được tiền huấn luyện trên corpus web quy mô lớn và thể hiện khả năng xuất sắc trong tạo ngữ cảnh và tuân theo hướng dẫn. Có một sự quan tâm ngày càng tăng trong việc tích hợp các LLM mạnh mẽ vào các tác vụ truy xuất. Các nghiên cứu hiện tại (Gao et al. 2023; Wang, Yang, và Wei 2023; Jagerman et al. 2023; Yu et al. 2023) tập trung vào mở rộng truy vấn với LLM để tăng cường khớp từ vựng của các cặp truy vấn-đoạn văn. Họ sử dụng các đoạn văn liên quan do LLM tạo ra như những ngữ cảnh truy vấn được làm phong phú. Những nghiên cứu đó đã cho kết quả hiệu suất truy xuất tốt hơn, đặc biệt là cho các tình huống zero-shot. Tuy nhiên, việc thực hiện mở rộng truy vấn vẫn cần suy luận trực tuyến nặng với LLM, điều này làm chậm tốc độ truy xuất.

Trong khi mở rộng truy vấn mở rộng truy vấn với các đoạn văn được tạo ra, mở rộng tài liệu, tức là tạo truy vấn, cũng là một kỹ thuật phổ biến để tăng hiệu suất truy xuất. Nó khai thác một mô hình được tinh chỉnh hoàn toàn, như T5 (Nogueira et al. 2019) hoặc BART (Cho et al. 2022), để tạo ra các truy vấn liên quan của một đoạn văn cho trước, mà hoặc làm phong phú ngữ cảnh của đoạn văn hoặc phục vụ như corpus tinh chỉnh bổ sung. Do khả năng tạo sinh xuất sắc của LLM, tiềm năng lớn nằm trong việc sử dụng LLM như các mô hình mở rộng tài liệu. Tuy nhiên, chúng tôi cho rằng một số nhược điểm vẫn cản trở việc sử dụng như vậy. Thứ nhất, mở rộng tài liệu dựa vào suy luận trực tuyến của LLM trong truy xuất đoạn văn miền mở, đặc biệt khi xử lý corpus ứng viên từ các miền mới. Để tránh nhu cầu suy luận LLM bổ sung trong quá trình truy xuất, một giải pháp khả thi là tiền huấn luyện hoặc tinh chỉnh một bộ truy xuất đầu cuối. Tuy nhiên, cách tiếp cận này thiếu khám phá và cần các mô hình huấn luyện được thiết kế đặc biệt cho truy xuất. Hơn nữa, mở rộng tài liệu liên quan đến việc đưa một corpus đáng kể vào LLM để tạo truy vấn, dẫn đến chi phí đáng kể liên quan đến suy luận LLM. Không may, thiếu phương pháp để giảm thiểu những chi phí suy luận này.

Để giảm thiểu chi phí suy luận trực tuyến cao của mở rộng tài liệu LLM, như được trình bày trong Hình 1, chúng tôi nhắc LLM tạo truy vấn cho một loạt thí nghiệm tiền huấn luyện được thiết kế riêng cho truy xuất dày đặc. Chúng tôi nhấn mạnh rằng công trình của chúng tôi chỉ liên quan đến suy luận LLM ở giai đoạn tiền huấn luyện của các bộ truy xuất, nhưng không phải giai đoạn suy luận như mở rộng truy vấn truyền thống (Gao et al. 2023; Wang, Yang, và Wei 2023) hoặc tài liệu (Nogueira et al. 2019). Hai mô hình tiền huấn luyện, tức là học tương phản và tạo truy vấn thắt cổ chai, được khám phá chi tiết.

Đối với tiền huấn luyện tương phản, một mất mát tương phản trực tiếp của arXiv:2308.08285v1 [cs.IR] 16 Aug 2023

--- TRANG 2 ---
### Hướng dẫn: Tạo mười truy vấn tìm kiếm cho đoạn văn sau### Đầu vào: <đoạn văn>### Phản hồi:    Nhắc LLaMA 
Định nghĩa: Tạo một truy vấn tìm kiếm ở định dạng câu hỏi hoặc cụm từ. Truy vấn được tạo ra phải rõ ràng và liên quan đến đầu vào.Ví dụ tích cực 1−    Đầu vào: <Ví dụ 1 - Đầu vào>    Đầu ra: <Ví dụ 1 - Đầu ra>Ví dụ tích cực 2−    Đầu vào: <Ví dụ 2 - Đầu vào>    Đầu ra: <Ví dụ 2 - Đầu ra>Bây giờ hoàn thành ví dụ sau−    Đầu vào: <đoạn văn>    Đầu ra:     Nhắc Tk-InstructHình 1: Nhắc tạo truy vấn cho Alpaca-LLaMA và tk-Instruct.

các truy vấn được tạo ra và đoạn văn được sử dụng để kéo gần các embedding của chúng, trong khi đẩy ra khỏi các âm tính trong batch trong không gian tiềm ẩn. Chúng tôi tuân theo kiến trúc tương phản trong (Gao và Callan 2022) để so sánh công bằng, và chúng tôi cho rằng các truy vấn do LLM tạo ra có thể phục vụ như ngữ cảnh tốt hơn cho sự liên kết truy vấn-đoạn văn hiệu quả.

Các kỹ thuật tiền huấn luyện thắt cổ chai phổ biến trong các công trình gần đây (Lu et al. 2021; Liu và Shao 2022; Wu et al. 2023a), kết nối các bộ giải mã truy cập chỉ thông qua biểu diễn của bộ mã hóa. Để tiền huấn luyện với tạo truy vấn thắt cổ chai, tương tự như (Wu, Ma, và Hu 2022), chúng tôi điều chỉnh một bộ giải mã Transformers một lớp và sử dụng tác vụ mô hình ngôn ngữ nhân quả (CLM) để tạo ra các truy vấn mở rộng với sự hỗ trợ của các embedding của bộ mã hóa. Cấu trúc bộ mã hóa-giải mã thắt cổ chai này đầu tiên dẫn xuất một biểu diễn nén thông qua bộ mã hóa và sau đó giải nén thông tin ngữ cảnh như các truy vấn mở rộng LLM thông qua bộ giải mã. Kết quả là, các embedding câu chứa thông tin ngữ cảnh được làm phong phú, cung cấp khởi tạo hiệu quả cho tinh chỉnh và suy luận. Đặc biệt, mở rộng tài liệu dựa trên LLM không yêu cầu corpus được gán nhãn bởi con người như các công trình trước (Wu, Ma, và Hu 2022; Cho et al. 2022) để huấn luyện các mô hình sinh bổ sung dành riêng cho miền như docT5query (Nogueira et al. 2019).

Hơn nữa, để giảm thiểu chi phí suy luận LLM cho mở rộng tài liệu, chúng tôi nội suy một chiến lược học chương trình giảng dạy hai giai đoạn cho cả hai lược đồ tiền huấn luyện. Hỏng span đầu tiên được sử dụng để lấy mẫu ngẫu nhiên các cặp ngữ cảnh từ một tài liệu dài. Sau đó chúng tôi tận dụng khả năng tạo sinh của LLM để sản xuất một lượng tương đối nhỏ các truy vấn cho giai đoạn tiền huấn luyện tiếp theo.

Trong nghiên cứu của chúng tôi, chúng tôi sử dụng Alpaca-LLaMA (Wang et al. 2023) và tk-Instruct (Wang et al. 2022b) với các kích thước tham số khác nhau để tạo truy vấn. Chúng tôi tiến hành các thí nghiệm trên các tập dữ liệu MS-MARCO quy mô lớn (Nguyen et al. 2016) và kiểm tra trên tác vụ truy xuất đoạn văn MS-MARCO trong miền, TREC-DL 2019 & 2020 (Craswell et al. 2020, 2021) và tác vụ BEIR ngoài miền (Thakur et al. 2021). Một số lợi ích được quan sát trong các nghiên cứu của chúng tôi. 1) LLM có thể tạo ra một số lượng lớn các truy vấn chất lượng cao dựa trên kiến thức thế giới của chính LLM, không yêu cầu gán nhãn bổ sung của con người và phù hợp cho các tình huống thiếu dữ liệu được chú thích thủ công. 2) Tiền huấn luyện tương phản với các truy vấn do LLM tạo ra có hiệu suất truy xuất zero-shot trong miền mạnh hơn và hiệu suất ngang bằng với các phương pháp tiên tiến nhất (SOTA) sau khi tinh chỉnh đầy đủ. Nó cũng cho thấy khả năng thích ứng miền tốt hơn trong các tập dữ liệu BEIR ngoài miền. 3) Tạo truy vấn thắt cổ chai cho thấy khả năng khởi tạo tốt hơn sau khi tinh chỉnh đầy đủ. 4) Với chiến lược học chương trình giảng dạy hai giai đoạn của chúng tôi, chúng tôi giảm số lượng corpus MS-MARCO liên quan đến suy luận LLM từ 8.8 triệu xuống 0.4 triệu, trong khi giữ sự suy giảm hiệu suất nhỏ.

Đóng góp của chúng tôi được tóm tắt như sau.
• Chúng tôi nghiên cứu một cách có hệ thống tiềm năng của việc kết hợp LLM vào giai đoạn tiền huấn luyện của truy xuất đoạn văn dày đặc, phù hợp cho sự khan hiếm dữ liệu được chú thích bởi con người.
• Chúng tôi tìm thấy hiệu suất zero-shot và tinh chỉnh mạnh hơn với học tương phản và khả năng khởi tạo tốt với tiền huấn luyện tạo truy vấn thắt cổ chai.
• Chúng tôi thiết kế một chiến lược học chương trình giảng dạy hai giai đoạn giảm đáng kể việc sử dụng các truy vấn mở rộng LLM trong khi giữ sự suy giảm hiệu suất nhỏ.

Phương pháp
Trong phần này, chúng tôi đầu tiên giới thiệu định nghĩa của truy xuất đoạn văn dày đặc. Sau đó chúng tôi giới thiệu phương pháp của chúng tôi cho tạo truy vấn LLM, các thiết kế tiền huấn luyện chi tiết của học tương phản và tạo truy vấn thắt cổ chai, và chiến lược học chương trình giảng dạy hai giai đoạn cho các phân tích mở rộng.

Kiến thức cơ bản
Cho một truy vấn q và một tập các đoạn văn Pn, tác vụ truy xuất đoạn văn nhằm tìm các đoạn văn liên quan dựa trên tìm kiếm tương tự. Truy xuất đoạn văn dày đặc sử dụng một mô hình mã hóa Enc, ví dụ, một mô hình dựa trên Transformers như BERT (Devlin et al. 2019), để tạo ra các biểu diễn câu và đo lường độ tương tự truy vấn-đoạn văn thông qua tích vô hướng hoặc khoảng cách cosine. Một cách chính thức, cho một truy vấn q và một đoạn văn p, chúng ta có thể sử dụng một bộ mã hóa truy vấn Encq và một bộ mã hóa đoạn văn Encp để dẫn xuất các biểu diễn câu tương ứng của chúng, tức là vq và vp từ các trạng thái ẩn của bộ mã hóa của lớp cuối tại vị trí CLS h[CLS]last. Sau đó độ tương tự

--- TRANG 3 ---
Bộ giải mãBộ mã hóaCLS
Đoạn văn1) Đoạn văn được lấy mẫu2) Truy vấn được tạo bởi LLMBộ mã hóaCLS
Đoạn vănBộ mã hóaCLSChia sẻ tham sốMất mát TươngphảnMất mát MLMMất mát MLMMất mát MLMMất mát CEb) Tiền huấn luyện Tạo truy vấn Thắt cổ chaic) Tiền huấn luyện Tươngphản

Mô hình Ngôn ngữ LớnĐoạn văn
Truy vấn được tạo bởi LLMa) Tạo truy vấn LLM

Hình 2: Tiền huấn luyện với mở rộng tài liệu dựa trên LLM cho truy xuất đoạn văn dày đặc. a) Chúng tôi sử dụng các mô hình ngôn ngữ lớn (LLM) để tạo ra các pseudo-truy vấn với nhắc zero-shot hoặc few-shot. b) Tiền huấn luyện tạo truy vấn thắt cổ chai thêm vào một bộ giải mã Transformers phụ trợ cho bộ mã hóa. Bên cạnh mất mát Mô hình Ngôn ngữ Có Mặt nạ (MLM) của bộ mã hóa, chúng tôi kết nối bộ mã hóa-giải mã chỉ với biểu diễn thắt cổ chai, tức là các trạng thái ẩn của token [CLS], và làm cho bộ giải mã tạo ra toàn bộ các truy vấn mở rộng LLM với mất mát Cross-Entropy (CE). c) Tiền huấn luyện tương phản kéo gần các biểu diễn của đoạn văn và các truy vấn mở rộng LLM và đẩy ra các âm tính trong batch. Để giảm thiểu sự phụ thuộc vào mở rộng LLM, chúng tôi triển khai một chiến lược học chương trình giảng dạy hai giai đoạn. Nó đầu tiên sử dụng các đoạn văn được lấy mẫu ngẫu nhiên để khởi tạo đầy đủ các bộ mã hóa. Và sau đó chúng ta có thể sử dụng một lượng tương đối nhỏ các truy vấn mở rộng LLM trong giai đoạn thứ hai.

giữa q và p, tức là Sim(q, p), có thể được tính như tích vô hướng của vq và vp để đơn giản như sau.
Sim(q, p) = Encq(q)·Encp(p) = vTq vp (1)

Chìa khóa để cải thiện hiệu suất truy xuất là tạo ra các biểu diễn mạnh hơn vq, vp với sự liên kết ngữ cảnh tốt hơn. Các biểu diễn có thể được coi là sự nén của toàn bộ ngữ cảnh. Chúng tôi tin rằng việc kết hợp khả năng tạo ngữ cảnh mạnh mẽ của LLM vào giai đoạn tiền huấn luyện với các pre-task được thiết kế cẩn thận có thể là một cách mới để cải thiện sự liên kết như vậy.

Tạo truy vấn LLM
Cho một đoạn văn p, chúng tôi sử dụng một nhắc zero-shot cho Alpaca-LLaMA và một nhắc few-shot cho tk-Instruct để mở rộng truy vấn, như được minh họa trong Hình 1. Chúng tôi thực nghiệm thấy rằng các mô hình Alpaca 7B và 13B hoạt động tốt trên nhắc zero-shot, điều này giúp tiết kiệm ngân sách tính toán. Chúng tôi viết thủ công một vài ví dụ cho tk-Instruct, vì chúng tôi thấy rằng nhắc few-shot làm cho việc tạo truy vấn của nó ổn định hơn.

Mở rộng tài liệu dựa trên LLM làm phong phú corpus tiền huấn luyện với thông tin ngữ cảnh bổ sung. Thay vì trực tiếp thêm các truy vấn mở rộng vào đoạn văn, chúng tôi tìm cách kết hợp chúng vào giai đoạn tiền huấn luyện của chúng tôi để khởi tạo tốt hơn các bộ truy xuất đầu cuối. Công trình của chúng tôi chỉ liên quan đến suy luận LLM ở giai đoạn tiền huấn luyện, nhưng không phải giai đoạn truy xuất như các công trình mở rộng truy vấn hoặc tài liệu truyền thống. Hai mô hình tiền huấn luyện được liên quan để kết hợp các truy vấn do LLM tạo ra vào tiền huấn luyện mô hình dày đặc.

Tiền huấn luyện Tạo truy vấn Thắt cổ chai
Tiền huấn luyện thắt cổ chai huấn luyện một bộ mã hóa đơn nguyên (Enc) với khả năng khởi tạo tốt cho tinh chỉnh tiếp theo. Cho một câu được token hóa t∈T từ corpus huấn luyện, chúng tôi chọn ngẫu nhiên một tỷ lệ nhất định các token, với các chỉ số tương ứng được ký hiệu là M, và thay thế chúng bằng các token mặt nạ [m]:

mask(t) = {[CLS], t1, t2, [m], t4, ..., tn, [SEP]} (2)

Mất mát Cross-Entropy (CE) sau đó được sử dụng để tối ưu hóa như mất mát Mô hình Ngôn ngữ Có Mặt nạ (MLM) cho bộ mã hóa.
Lenc = −∑t∈T ∑i∈M log p(ti|Enc(mask(t))) (3)

trong đó ti là các token thực tế w.r.t các token mặt nạ tương ứng [m].

Một bộ giải mã Transformers một lớp truy cập (Dec) được giới thiệu thêm, nhận đầu vào từ sự kết nối của biểu diễn bộ mã hóa h[CLS]last và các văn bản ngữ cảnh x, ví dụ, các truy vấn do LLM tạo ra.
Tctx = {h[CLS]last, x1, ..., xN, [SEP]} (4)

Sau đó bộ giải mã sử dụng mất mát Mô hình Ngôn ngữ Nhân quả (CLM) để tạo ra toàn bộ ngữ cảnh đầu vào với sự hỗ trợ của biểu diễn bộ mã hóa.

--- TRANG 4 ---
MS-MARCO TREC DL 19 TREC DL 20
Mô hình / Đánh giá Zero-shot MRR@10 Recall@50 Recall@1k nDCG@10 nDCG@10
BM25 18.7 59.2 85.7 51.2 47.7
SimCSE (Gao, Yao, và Chen 2021) † 8.7 33.7 64.6 24.5 17.9
coCondenser (Gao và Callan 2022) † 7.5 31.3 58.1 22.1 20.7
Contriever (Izacard et al. 2021) † 16.8 60.8 89.1 44.5 43.2
Tiền huấn luyện Tương phản
Cơ sở 12.5 49.0 82.3 36.0 38.4
+ tk-inst 3b truy vấn 20.9+8.4 70.2+21.2 92.8+10.5 47.0+11.0 48.6+10.2
+ Alpaca 7b truy vấn 22.6+10.1 70.7+21.7 93.8+11.5 51.0+15.0 48.9+10.5
+ Alpaca 13b truy vấn 22.7+10.2 71.7+22.7 94.3+12.0 53.9+17.9 50.1+11.7

Bảng 1: Đánh giá zero-shot của tiền huấn luyện tương phản với mở rộng tài liệu dựa trên LLM. †biểu thị kết quả được tái tạo của chúng tôi. Điểm tốt nhất được đánh dấu in đậm. Kết quả với sự gia tăng so với cơ sở tương ứng đã được kiểm tra bằng t-test hai đuôi, chứng minh cải thiện có ý nghĩa thống kê (p-value ≤ 0.01).

Ldec = −∑xi∈Tctx log p(xi|Dec(x[:i−1])) (5)

Mất mát cuối cùng L sau đó được công thức hóa như sau.
L = Lenc + Ldec (6)

Thông qua cấu trúc bộ mã hóa-giải mã thắt cổ chai, chúng tôi tìm cách nén tín hiệu ngữ cảnh từ các truy vấn do LLM tạo ra vào các biểu diễn bộ mã hóa và cung cấp khả năng khởi tạo mạnh cho bộ mã hóa.

Tiền huấn luyện Tương phản
Để tái tạo và so sánh công bằng, chúng tôi điều chỉnh kiến trúc tiền huấn luyện tương phản từ coCondenser (Gao và Callan 2022). Đoạn văn p và ngữ cảnh được lấy mẫu hoặc tạo ra pctx được chuyển tiếp trực tiếp qua bộ mã hóa Enc. Bên cạnh mất mát MLM Lenc của bộ mã hóa, một bộ giải mã Transformers bổ sung Decext cũng được giới thiệu cho tiền huấn luyện biểu diễn, nhận sự kết nối của biểu diễn bộ mã hóa h[CLS]last và các trạng thái ẩn bộ mã hóa hil từ lớp thứ l. Sau đó một mất mát cross-entropy được sử dụng cho pre-task của bộ giải mã.

Lext = −∑t∈T ∑i∈M log p(ti|Decext(h[CLS]last, h1l, ..., hil)) (7)

Khác biệt, đối với tiền huấn luyện với các truy vấn mở rộng LLM, giả sử vp và vctx biểu thị các biểu diễn của bộ mã hóa, một mất mát tương phản với các âm tính trong batch được sử dụng như sau.

LCL = −log exp(vp·v+ctx) / (exp(vp·v+ctx) + ∑exp(vp·v−ctx)) (8)

trong đó v+ctx là văn bản ngữ cảnh, ví dụ các truy vấn do LLM tạo ra, tương ứng với p. Và v−ctx là các âm tính trong batch, tức là các văn bản ngữ cảnh của các đoạn văn khác trong batch.

Mục tiêu tối ưu hóa cuối cùng là tổng của các mất mát trên.
L = Lenc + Lext + LCL (9)

Thông qua tiền huấn luyện tương phản, các biểu diễn của đoạn văn và các truy vấn do LLM tạo ra được kéo gần trực tiếp trong cùng không gian tiềm ẩn, điều này cho sự liên kết truy vấn-đoạn văn tốt hơn và khả năng zero-shot cho các bộ mã hóa.

Học Chương trình Giảng dạy
Như đã thảo luận trước đây, mở rộng tài liệu dựa trên LLM đối mặt với thách thức của suy luận tốn kém do số lượng lớn tài liệu hoặc đoạn văn. Vì chúng tôi dự định tiền huấn luyện mô hình của chúng tôi với các ngữ cảnh được làm phong phú, lấy cảm hứng từ trí tuệ của học chương trình giảng dạy (Bengio et al. 2009), chúng tôi xem xét 1) một span đoạn văn được cắt ngẫu nhiên như một ngữ cảnh thô, trong khi 2) các truy vấn mở rộng LLM như ngữ cảnh tinh, như được mô tả trong Hình 2. Theo các chiến lược hỏng span trong seed-encoder (Lu et al. 2021) và coCondenser (Gao và Callan 2022), chúng tôi sử dụng ngữ cảnh thô như chính đoạn văn trong tiền huấn luyện tạo thắt cổ chai, và span đoạn văn được lấy mẫu ngẫu nhiên trong tiền huấn luyện tương phản. Vì chúng tôi tập trung vào mở rộng tài liệu dựa trên LLM, các chiến lược hỏng span khác (Wu et al. 2023a) được để lại cho công trình tương lai của chúng tôi. Sau khi tiền huấn luyện trên một lượng lớn ngữ cảnh được cắt ngẫu nhiên, chúng tôi khởi tạo từ giai đoạn đầu và sau đó sử dụng các truy vấn mở rộng LLM tinh cho tiền huấn luyện giai đoạn thứ hai. Thí nghiệm thấy rằng chiến lược chương trình giảng dạy này giảm đáng kể nhu cầu suy luận LLM trên các đoạn văn MS-MARCO, trong khi vẫn duy trì hiệu suất truy xuất tương tự.

Đánh giá Zero-shot và Tinh chỉnh
Chúng tôi tiến hành đánh giá zero-shot của bộ mã hóa được tiền huấn luyện tương phản mà không tinh chỉnh trên các tập dữ liệu MS-MARCO, TREC-DL, và BEIR. Chúng tôi tiến hành tinh chỉnh trên cả hai lược đồ tiền huấn luyện để xác minh khả năng khởi tạo truy xuất của chúng. Theo DPR (Karpukhin et al. 2020), một mất mát tương phản đơn giản được áp dụng để tối ưu hóa bộ truy xuất.

L = −log exp(q·p+) / (exp(q·p+) + ∑exp(q·p−)) (10)

--- TRANG 5 ---
MS-MARCO TREC DL 19 TREC DL 20
Mô hình / Kết quả Tinh chỉnh MRR@10 Recall@50 Recall@1k nDCG@10 nDCG@10
Contriever (Izacard et al. 2021) † 33.4 85.0 98.4 62.8 63.2
Condenser (Gao và Callan 2021) 36.6 85.4 † 97.4 69.8 66.5†
coCondenser (Gao và Callan 2022) 38.2 86.5 † 98.4 71.7† 68.4†
SimLM (Wang et al. 2022a) 39.1 87.3 † 98.6 68.9† 68.8†
RetroMAE (Liu và Shao 2022) 39.3 87.0 † 98.5 69.1† 70.0†
CoT-MAE (Wu et al. 2023a) 39.4 87.0 98.7 70.9† 70.4
Tiền huấn luyện Tương phản
Cơ sở 38.8 87.8 98.8 71.1 68.4
+ tk-instruct 3b truy vấn 39.6+0.8 88.8+1.0 99.0 72.9+1.8 71.1+2.7
+ Alpaca 7b truy vấn 40.0+1.2 89.0+1.2 99.1 72.9+1.8 71.3+2.9
+ Alpaca 13b truy vấn 39.6+0.8 88.8+1.0 98.9 72.6+1.5 72.3+3.9
Tạo truy vấn Thắt cổ chai
Cơ sở 39.3 87.9 98.6 69.9 67.4
+ tk-instruct 3b truy vấn 40.3+1.0 88.7+0.8 98.9 70.7+0.8 70.0+2.6
+ Alpaca 7b truy vấn 39.9+0.6 88.2 98.7 69.6 70.7+3.3
+ Alpaca 13b truy vấn 39.7 88.3 98.7 70.8+0.9 69.4+2.0

Bảng 2: Kết quả tinh chỉnh của tiền huấn luyện với mở rộng tài liệu dựa trên LLM. †biểu thị kết quả được tái tạo của chúng tôi. Điểm tốt nhất được đánh dấu in đậm. Kết quả với sự gia tăng so với cơ sở tương ứng đã được kiểm tra bằng t-test hai đuôi, chứng minh cải thiện có ý nghĩa thống kê (p-value ≤ 0.01).

trong đó q là một truy vấn cho trước, p+ và p− là đoạn văn tích cực tương ứng và các đoạn văn tiêu cực của chúng.

Thí nghiệm
Phần này giới thiệu các thiết lập thí nghiệm chi tiết cho tiền huấn luyện và tinh chỉnh. Sau đó chúng tôi trình bày các kết quả chính.

Tiền huấn luyện
Theo các thiết lập tiền huấn luyện trong (Gao và Callan 2022), chúng tôi chọn tập dữ liệu MS-MARCO (Nguyen et al. 2016) với 3.2M tài liệu làm corpus tiền huấn luyện của chúng tôi. LLM với các loại và kích thước tham số khác nhau, tức là Alpaca 7B, 13B (Wang et al. 2023), và tk-instruct 3B (Wang et al. 2022b) được sử dụng để tạo ra các truy vấn cho mở rộng tài liệu dựa trên LLM. Lấy mẫu hạt nhân với top p = 0.95, top k = 50, và nhiệt độ = 0.7 được sử dụng cho việc tạo LLM.

Đối với tiền huấn luyện tạo truy vấn thắt cổ chai, bộ mã hóa được khởi tạo từ mô hình BERT-base 12 lớp (Devlin et al. 2019), trong khi bộ giải mã một lớp được khởi tạo ngẫu nhiên từ đầu. Chúng tôi sử dụng bộ tối ưu hóa AdamW với tốc độ học 3e-4, kích thước batch 2048, tổng số bước 80k, và tỷ lệ khởi động 0.1. Tiền huấn luyện sử dụng 8 GPU Tesla A100 và huấn luyện trong 19 giờ. Đối với tiền huấn luyện tương phản, chúng tôi điều chỉnh mã và kiến trúc từ (Gao và Callan 2022) và khởi tạo từ (Gao và Callan 2021) bằng cách theo các thiết lập của họ. Chúng tôi sử dụng tốc độ học 1e-4, kích thước batch 2048, và tổng số bước 120k và giữ các siêu tham số khác giống như trên để huấn luyện 50 giờ.

Đối với học chương trình giảng dạy, 75% tổng số bước được sử dụng cho giai đoạn đầu tiền huấn luyện với các span được lấy mẫu, và 25% số bước còn lại được sử dụng cho giai đoạn thứ hai của tiền huấn luyện với các truy vấn do LLM tạo ra. Chúng tôi sử dụng bộ lập lịch cosine với cùng các thiết lập siêu tham số cho giai đoạn đầu, và tốc độ học không đổi cho giai đoạn thứ hai. Tất cả các seed tiền huấn luyện được đặt thành 42 để có thể tái tạo. Các bộ mã hóa được kiểm tra trực tiếp trên các tác vụ downstream mà không tinh chỉnh cho đánh giá zero-shot.

Tinh chỉnh
Bộ mã hóa được tinh chỉnh và kiểm tra trên tác vụ Xếp hạng Đoạn văn MS-MARCO (Nguyen et al. 2016), TREC Deep Learning (DL) 2019 (Craswell et al. 2020) và 2020 (Craswell et al. 2021). Tập dữ liệu Xếp hạng Đoạn văn MS-MARCO chứa 8.8 triệu đoạn văn và 500k cặp truy vấn-đoạn văn được chú thích bởi con người. Theo (Gao và Callan 2021), chúng tôi báo cáo các thước đo hiệu suất trên MRR@10, Recall@50, Recall@1K, và đánh giá các mô hình trên tập phát triển của nó với 6,980 truy vấn, vì tập kiểm tra của nó không có sẵn công khai. Các tập kiểm tra TREC-DL 2019 và 2020 đều chứa 200 truy vấn được chú thích. Chúng tôi áp dụng pipeline Tevatron (Gao et al. 2022) với bộ tối ưu hóa AdamW cho tốc độ học 2e-5, kích thước batch 8, mẫu âm tính trên mỗi đoạn văn là 15, độ sâu âm tính 200, và huấn luyện trong 3 epoch. Các thước đo hiệu suất của TREC và BEIR được báo cáo trên NDCG@10.

Cơ sở
Chúng tôi so sánh với các cơ sở tự chứa mà không sử dụng các truy vấn mở rộng LLM, nhưng chỉ sử dụng các span được lấy mẫu ngẫu nhiên như ngữ cảnh thô. Tất cả các siêu tham số khác được sử dụng trong tiền huấn luyện vẫn giống như các thí nghiệm chính để so sánh công bằng. Trong các thí nghiệm tinh chỉnh, các cơ sở tiền huấn luyện tương phản chủ yếu từ (Wu, Ma, và Hu

--- TRANG 6 ---
Kết quả / nDCG@10 BM25 coCondenser Contriever SimCSE Cơ sở + tk-Instruct 3b + Alpaca 7b + Alpaca 13b
TREC-COVID 65.6 21.2 27.3 27.5 16.2 36.8+20.6 52.3+36.1 54.7+38.5
NFCorpus 32.5 13.7 31.7 10.5 29.9 33.1+3.2 30.9+1.0 33.5+3.5
NQ 32.9 10.7 25.4 16.3 9.3 34.3+25.0 31.8+22.5 31.9+22.6
HotpotQA 60.3 22.3 48.1 23.8 24.2 56.2+32.0 51.5+27.3 51.8+27.6
FiQA-2018 23.6 7.2 24.5 9.7 19.6 29.8+10.3 27.2+7.6 28.6+9.0
ArguAna 31.5 34.4 37.9 28.0 35.8 44.6+8.8 40.5+4.8 40.6+4.9
Touché-2020 36.7 5.8 16.7 13.4 8.1 16.3+8.2 13.7+5.5 16.9+8.7
CQADupStack 29.9 10.5 28.4 13.5 18.2 30.9+12.8 32.4+14.2 33.3+15.1
Quora 78.9 71.3 83.5 73.7 75.8 83.8+8.0 83.3+7.5 84.3+8.5
DBPedia 31.3 16.3 29.2 16.7 22.5 30.2+7.7 28.8+6.3 29.6+7.1
SCIDOCS 15.8 4.6 14.9 6.1 10.4 13.6+3.2 13.5+3.2 14.4+4.1
FEVER 75.3 16.8 68.2 29.2 43.6 61.9+18.3 67.2+23.6 73.1+29.5
Climate-FEVER 21.3 6.4 15.5 14.2 8.5 18.4+9.8 13.8+5.3 17.2+8.6
SciFact 66.5 43.2 64.9 25.0 52.7 64.4+11.7 60.8+8.1 60.9+8.2
Trung bình 43.0 20.3 36.9 22.0 26.8 39.6+12.8 39.1+12.4 40.8+14.0

Bảng 3: Đánh giá zero-shot ngoài miền của tiền huấn luyện tương phản với mở rộng tài liệu dựa trên LLM trên benchmark BEIR. Tất cả các cơ sở được kiểm tra trên nDCG@10 dựa trên việc tái tạo của chúng tôi. Kết quả với sự gia tăng so với cơ sở tương ứng đã được kiểm tra bằng t-test hai đuôi, chứng minh cải thiện có ý nghĩa thống kê (p-value ≤ 0.01).

2022) bằng cách theo các thiết lập siêu tham số của họ, và các cơ sở khác dựa trên các thiết lập của chúng tôi.

Chúng tôi cũng so sánh với các cơ sở đáng chú ý khác, bao gồm truy xuất thưa truyền thống BM25 (Robertson, Zaragoza et al. 2009), bộ mã hóa tương tự câu không giám sát SimCSE (Gao, Yao, và Chen 2021), phương pháp tiền huấn luyện tương phản không giám sát coCondenser (Gao và Callan 2022) và Contriever (Izacard et al. 2021) cho đánh giá zero-shot. Đối với kết quả tinh chỉnh, chúng tôi cũng so sánh với các phương pháp tiền huấn luyện thắt cổ chai mới nhất, bao gồm Condenser (Gao và Callan 2021), SimLM (Wang et al. 2022a), RetroMAE (Liu và Shao 2022) và CoT-MAE (Wu et al. 2023a). Lưu ý rằng các phương pháp thắt cổ chai gần đây sử dụng tiền huấn luyện đa tác vụ (Zhou et al. 2022) hoặc truy xuất lai (Liu et al. 2023; Wu et al. 2023b) không được so sánh, vì chúng nằm ngoài phạm vi so sánh công bằng.

Đánh giá Zero-shot
Bảng 1 báo cáo đánh giá zero-shot trong miền của tiền huấn luyện tương phản với mở rộng tài liệu dựa trên LLM. Tiền huấn luyện với các truy vấn mở rộng LLM cho thấy cải thiện rõ ràng so với các cơ sở của nó chỉ sử dụng các đoạn văn được lấy mẫu ngẫu nhiên. Điều này chỉ ra rằng phương pháp của chúng tôi đạt được khả năng truy xuất zero-shot mạnh mẽ cho đánh giá trong miền trên các tập dữ liệu MS-MARCO và TREC-DL 19 & 20.

Truy xuất Tinh chỉnh
Kết quả tinh chỉnh của hai phương pháp tiền huấn luyện, tức là tiền huấn luyện tương phản và tiền huấn luyện tạo truy vấn thắt cổ chai, được trình bày trong Bảng 2. Tiền huấn luyện với các truy vấn mở rộng LLM cũng cung cấp một sự thúc đẩy có ý nghĩa thống kê cho các cơ sở và đối tác của chúng. Ngoài ra, chúng tôi nhận thấy rằng 1) Tiền huấn luyện tương phản cho kết quả tốt hơn trên tác vụ đoạn văn MS-MARCO (trong Recall@50 và Recall@1k) và TREC-DL 19 & 20 (trong nDCG@10). 2) Tạo truy vấn thắt cổ chai cho khởi tạo tốt hơn trên MS-MARCO w.r.t thước đo ưa thích chính thức MRR@10, nhưng vẫn đứng sau tiền huấn luyện tương phản trong các thước đo khác.

Đánh giá Ngoài miền
Chúng tôi cũng đánh giá benchmark BEIR zero-shot ngoài miền cho tiền huấn luyện tương phản với mở rộng tài liệu dựa trên LLM và báo cáo thước đo (nDCG@10) trong Bảng 3. BM25 là một cơ sở rất mạnh w.r.t tất cả các phương pháp tiền huấn luyện tương phản khác không trải qua tinh chỉnh được gán nhãn bởi con người. Tuy nhiên, phương pháp của chúng tôi vẫn cho thấy cải thiện mạnh mẽ so với cơ sở tương phản của nó. Cụ thể, so với Contriever (Izacard et al. 2021), là một phương pháp tương phản không giám sát được tiền huấn luyện trên corpus CCNET lớn hơn nhiều (Wenzek et al. 2020), tiền huấn luyện với mở rộng LLM cũng cho thấy hiệu suất truy xuất vượt trội.

Phân tích Mở rộng
Trong phần này, chúng tôi phân tích ảnh hưởng của việc mở rộng LLM và chiến lược học chương trình giảng dạy với các truy vấn mở rộng được tạo bởi Alpaca 13b1.

Ảnh hưởng của Mở rộng LLM
Chúng tôi sử dụng ba LLM với các kích thước tham số khác nhau từ 3b đến 13b, nhắc chúng cho mở rộng tài liệu và tích hợp các truy vấn được tạo ra vào tiền huấn luyện. Như được hiển thị trong Bảng 1, việc mở rộng LLM cho thấy hiệu suất truy xuất tốt hơn trong tiền huấn luyện tương phản zero-shot.

1Alpaca 13b được chọn vì kết quả tốt hơn trong zero-shot và hiệu suất ngang bằng trong truy xuất tinh chỉnh.

--- TRANG 7 ---
63.0 65.0 67.0 69.0 71.0 73.0 75.0
37.0 37.5 38.0 38.5 39.0 39.5 40.0
50k 0.1M 0.4M 0.8M 1M 4M 8.8M TREC-DL 20 / nDCG@10 MS-MARCO / MRR@10
Lượng Corpus Huấn luyện cho Tiền huấn luyện Tinh
Thắt cổ chai (MARCO)
Thắt cổ chai (DL20)

Hình 3: Ảnh hưởng của học chương trình giảng dạy cho tiền huấn luyện thắt cổ chai tinh chỉnh với các truy vấn mở rộng được tạo bởi Alpaca 13b. Các đường gạch ngang là các cơ sở tương ứng từ Bảng 2.

Nhưng quan sát này không có hiệu lực sau khi tinh chỉnh trong Bảng 2. Chúng tôi giả thuyết rằng đối với tinh chỉnh với nhãn con người, những LLM này đều đủ có khả năng để cung cấp một khởi tạo tốt cho truy xuất.

Ảnh hưởng của Học Chương trình Giảng dạy
Để giảm hơn nữa nhu cầu các truy vấn mở rộng LLM trong tiền huấn luyện, chúng tôi cố gắng sử dụng một chiến lược học chương trình giảng dạy như đã chi tiết trước đây. Chúng tôi sử dụng các span được lấy mẫu ngẫu nhiên như ngữ cảnh thô trong giai đoạn đầu của tiền huấn luyện chương trình giảng dạy cho 75% tổng số bước huấn luyện. Sau đó chúng tôi sử dụng một lượng nhỏ các truy vấn mở rộng LLM như ngữ cảnh tinh cho các bước tiền huấn luyện còn lại. Hình 3 và 4 cho thấy rằng cả hai lược đồ tiền huấn luyện đều hưởng lợi từ học chương trình giảng dạy. Tạo truy vấn thắt cổ chai vượt trội hơn cơ sở của nó chỉ với 0.4 triệu truy vấn mở rộng LLM sau khi tinh chỉnh. Tiền huấn luyện tương phản zero-shot vượt qua các cơ sở và tiếp tục chứng minh cải thiện bền vững khi số lượng truy vấn tinh tăng lên.

Công trình Liên quan
Tiền huấn luyện cho Truy xuất Dày đặc
Truy xuất đoạn văn dày đặc đã đạt được cải thiện bền vững với sự phát triển gần đây của các tác vụ tiền huấn luyện. Một số công trình tập trung vào tiền huấn luyện tương phản với mối quan hệ span được xây dựng (Chang et al. 2020), các span được cắt ngẫu nhiên (Gao và Callan 2022) hoặc liên kết đa độ chi tiết (Ma et al. 2022). Và trong khi đó, những người khác tập trung vào tiền huấn luyện với các bộ giải mã thắt cổ chai phụ trợ, như tiền huấn luyện với một bộ giải mã sinh yếu (Lu et al. 2021), tỷ lệ mặt nạ cực độ (Liu và Shao 2022), và lấy mẫu span ngữ cảnh (Wu et al. 2023a). Phương pháp của chúng tôi tương tự như (Gao và Callan 2022) và (Wu et al. 2023a), nhưng đóng góp cốt lõi của chúng tôi là phương pháp kết hợp các truy vấn mở rộng được tạo bởi LLM vào các lược đồ tiền huấn luyện như vậy, mang lại sự liên kết ngữ cảnh tốt hơn và hiệu suất zero-shot và tinh chỉnh mạnh hơn.

30.0 35.0 40.0 45.0 50.0 55.0
0.0 5.0 10.0 15.0 20.0 25.0
50k 0.1M 0.4M 0.8M 1M 4M 8.8M TREC-DL 20 / nDCG@10 MS-MARCO / MRR@10
Lượng Corpus Huấn luyện cho Tiền huấn luyện Tinh
Tương phản (MARCO)
Tương phản (DL20)

Hình 4: Ảnh hưởng của học chương trình giảng dạy cho tiền huấn luyện tương phản zero-shot với các truy vấn mở rộng LLM.

Mở rộng Truy vấn và Tài liệu dựa trên LLM
Mở rộng truy vấn hoặc tài liệu truyền thống tạo ra ngữ cảnh bổ sung thông qua viết lại truy vấn (Lavrenko và Croft 2017), hoặc với các mô hình T5 (Nogueira et al. 2019) hoặc BART (Cho et al. 2022) được tinh chỉnh đặc biệt. Với sự nở rộ của LLM (Ouyang et al. 2022; Touvron et al. 2023; Wang et al. 2022b), các nghiên cứu ngày càng tăng tập trung vào việc sử dụng LLM như các mô hình mở rộng truy vấn (Gao et al. 2023; Wang, Yang, và Wei 2023; Jagerman et al. 2023; Yu et al. 2023), tăng cường khớp từ vựng của các cặp truy vấn-đoạn văn.

Tuy nhiên, như đã thảo luận trước đây, mở rộng tài liệu dựa trên LLM vẫn thiếu khám phá do chi phí suy luận đắt đỏ mang lại bởi số lượng tài liệu lớn và vấn đề suy luận trực tuyến. Chúng tôi đề xuất giải quyết những vấn đề đó bằng các kỹ thuật tiền huấn luyện và chiến lược học chương trình giảng dạy được thiết kế riêng cho truy xuất dày đặc. Phương pháp của chúng tôi cũng trực giao với mở rộng truy vấn và tài liệu truyền thống và có thể kết hợp chúng vào giai đoạn truy xuất.

Kết luận
Bài báo này nghiên cứu một cách có hệ thống tiềm năng của tiền huấn luyện với mở rộng tài liệu dựa trên Mô hình Ngôn ngữ Lớn cho truy xuất đoạn văn dày đặc. Cải thiện mạnh mẽ trong hiệu suất zero-shot và ngoài miền được quan sát trong tiền huấn luyện tương phản với mở rộng tài liệu dựa trên LLM. Hơn nữa, cả tiền huấn luyện tương phản và tiền huấn luyện tạo truy vấn thắt cổ chai đều đạt được khả năng truy xuất tốt sau khi tinh chỉnh. Chúng tôi tiến xa hơn đề xuất một chiến lược học chương trình giảng dạy hai giai đoạn có thể giảm đáng kể nhu cầu các truy vấn mở rộng LLM trong tiền huấn luyện, trong khi giữ sự suy giảm hiệu suất nhỏ. LLM xuất sắc trong việc mở rộng các truy vấn chất lượng cao với thông tin ngữ cảnh được làm phong phú, phù hợp cho các tình huống thiếu chú thích của con người. Do đó các nhà nghiên cứu có thể triển khai khởi tạo nhanh của một hệ thống truy xuất dày đặc không giám sát với tiền huấn luyện mở rộng tài liệu dựa trên LLM, thậm chí KHÔNG có nhãn con người được cung cấp.

Hạn chế
Chúng tôi cũng quan tâm đến việc kiểm tra nhiều loại LLM hơn với các kích thước khác nhau, chẳng hạn như ChatGPT (Ouyang et al. 2022), và

--- TRANG 8 ---
LLaMA 2 (Touvron et al. 2023), hoặc các nhắc khác nhau cho mở rộng tài liệu, nhưng ngân sách thí nghiệm của chúng tôi bị hạn chế để hỗ trợ các điều tra ngay lập tức và chúng tôi để điều đó cho các công trình tương lai của chúng tôi.

Tài liệu tham khảo
Bengio, Y.; Louradour, J.; Collobert, R.; và Weston, J. 2009. Curriculum learning. Trong Danyluk, A. P.; Bottou, L.; và Littman, M. L., eds., Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 của ACM International Conference Proceeding Series, 41–48. ACM.

Cai, D.; Wang, Y.; Liu, L.; và Shi, S. 2022. Recent advances in retrieval-augmented text generation. Trong Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 3417–3419.

Chang, W.; Yu, F. X.; Chang, Y.; Yang, Y.; và Kumar, S. 2020. Pre-training Tasks for Embedding-based Large-scale Retrieval. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Cho, S.; Jeong, S.; Yang, W.; và Park, J. C. 2022. Query Generation with External Knowledge for Dense Retrieval. Trong Agirre, E.; Apidianaki, M.; và Vulic, I., eds., Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 22–32. Association for Computational Linguistics.

Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; và Fiedel, N. 2022. PaLM: Scaling Language Modeling with Pathways. CoRR, abs/2204.02311.

Craswell, N.; Mitra, B.; Yilmaz, E.; và Campos, D. 2021. Overview of the TREC 2020 deep learning track. arXiv:2102.07662.

Craswell, N.; Mitra, B.; Yilmaz, E.; Campos, D.; và Voorhees, E. M. 2020. Overview of the TREC 2019 deep learning track. arXiv:2003.07820.

Devlin, J.; Chang, M.-W.; Lee, K.; và Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. Minneapolis, Minnesota: Association for Computational Linguistics.

Gao, L.; và Callan, J. 2021. Condenser: a Pre-training Architecture for Dense Retrieval. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 981–993. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.

Gao, L.; và Callan, J. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2843–2853. Dublin, Ireland: Association for Computational Linguistics.

Gao, L.; Ma, X.; Lin, J.; và Callan, J. 2022. Tevatron: An efficient and flexible toolkit for dense retrieval. arXiv preprint arXiv:2203.05765.

Gao, L.; Ma, X.; Lin, J.; và Callan, J. 2023. Precise Zero-Shot Dense Retrieval without Relevance Labels. Trong Rogers, A.; Boyd-Graber, J. L.; và Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 1762–1777. Association for Computational Linguistics.

Gao, T.; Yao, X.; và Chen, D. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6894–6910. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.

Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P.; Joulin, A.; và Grave, E. 2021. Towards Unsupervised Dense Information Retrieval with Contrastive Learning. CoRR, abs/2112.09118.

Jagerman, R.; Zhuang, H.; Qin, Z.; Wang, X.; và Bendersky, M. 2023. Query Expansion by Prompting Large Language Models. CoRR, abs/2305.03653.

Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov, S.; Chen, D.; và Yih, W.-t. 2020. Dense Passage Retrieval for Open-Domain Question Answering. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 6769–6781. Online: Association for Computational Linguistics.

Khattab, O.; và Zaharia, M. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Trong Huang, J. X.; Chang, Y.; Cheng, X.; Kamps, J.; Murdock, V.; Wen, J.; và Liu, Y., eds., Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, 39–48. ACM.

Lavrenko, V.; và Croft, W. B. 2017. Relevance-Based Language Models. SIGIR Forum, 51(2): 260–267.

Lewis, P. S. H.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.; Rocktäschel, T.; Riedel, S.; và Kiela, D. 2020. Retrieval-Augmented

--- TRANG 9 ---
Generation for Knowledge-Intensive NLP Tasks. Trong Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; và Lin, H., eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Liu, Y.; Lu, W.; Cheng, S.; Shi, D.; Wang, S.; Cheng, Z.; và Yin, D. 2021. Pre-trained Language Model for Web-scale Retrieval in Baidu Search. Trong Zhu, F.; Ooi, B. C.; và Miao, C., eds., KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, 3365–3375. ACM.

Liu, Z.; và Shao, Y. 2022. RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder. arXiv preprint arXiv:2205.12035.

Liu, Z.; Xiao, S.; Shao, Y.; và Cao, Z. 2023. RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. Trong Rogers, A.; Boyd-Graber, J. L.; và Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 2635–2648. Association for Computational Linguistics.

Lu, S.; He, D.; Xiong, C.; Ke, G.; Malik, W.; Dou, Z.; Bennett, P.; Liu, T.-Y.; và Overwijk, A. 2021. Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2780–2791.

Lu, Y.; Liu, Y.; Liu, J.; Shi, Y.; Huang, Z.; Sun, S. F. Y.; Tian, H.; Wu, H.; Wang, S.; Yin, D.; et al. 2022. Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for dense passage retrieval. arXiv preprint arXiv:2205.09153.

Ma, X.; Guo, J.; Zhang, R.; Fan, Y.; và Cheng, X. 2022. Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction. arXiv preprint arXiv:2204.10641.

Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.; Majumder, R.; và Deng, L. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. Trong Besold, T. R.; Bordes, A.; d'Avila Garcez, A. S.; và Wayne, G., eds., Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 của CEUR Workshop Proceedings. CEUR-WS.org.

Nogueira, R. F.; Yang, W.; Lin, J.; và Cho, K. 2019. Document Expansion by Query Prediction. CoRR, abs/1904.08375.

Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; và Lowe, R. 2022. Training language models to follow instructions with human feedback. Trong NeurIPS.

Qu, Y.; Ding, Y.; Liu, J.; Liu, K.; Ren, R.; Zhao, W. X.; Dong, D.; Wu, H.; và Wang, H. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 5835–5847. Online: Association for Computational Linguistics.

Ren, R.; Qu, Y.; Liu, J.; Zhao, W. X.; She, Q.; Wu, H.; Wang, H.; và Wen, J.-R. 2021. RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2825–2835. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.

Robertson, S.; Zaragoza, H.; et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval, 3(4): 333–389.

Sakata, W.; Shibata, T.; Tanaka, R.; và Kurohashi, S. 2019. FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance. Trong Piwowarski, B.; Chevalier, M.; Gaussier, É.; Maarek, Y.; Nie, J.; và Scholer, F., eds., Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019, 1113–1116. ACM.

Santhanam, K.; Khattab, O.; Saad-Falcon, J.; Potts, C.; và Zaharia, M. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 3715–3734. Seattle, United States: Association for Computational Linguistics.

Thakur, N.; Reimers, N.; Rücklé, A.; Srivastava, A.; và Gurevych, I. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. CoRR, abs/2104.08663.

Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; và Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR, abs/2302.13971.

Wang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang, D.; Majumder, R.; và Wei, F. 2022a. SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. CoRR, abs/2207.02578.

Wang, L.; Yang, N.; và Wei, F. 2023. Query2doc: Query Expansion with Large Language Models. CoRR, abs/2303.07678.

Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.; Khashabi, D.; và Hajishirzi, H. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. Trong Rogers, A.; Boyd-Graber, J. L.; và Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 13484–13508. Association for Computational Linguistics.

--- TRANG 10 ---
Wang, Y.; Mishra, S.; Alipoormolabashi, P.; Kordi, Y.; Mirzaei, A.; Naik, A.; Ashok, A.; Dhanasekaran, A. S.; Arunkumar, A.; Stap, D.; Pathak, E.; Karamanolakis, G.; Lai, H. G.; Purohit, I.; Mondal, I.; Anderson, J.; Kuznia, K.; Doshi, K.; Pal, K. K.; Patel, M.; Moradshahi, M.; Parmar, M.; Purohit, M.; Varshney, N.; Kaza, P. R.; Verma, P.; Puri, R. S.; Karia, R.; Doshi, S.; Sampat, S. K.; Mishra, S.; A, S. R.; Patro, S.; Dixit, T.; và Shen, X. 2022b. SuperNaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Trong Goldberg, Y.; Kozareva, Z.; và Zhang, Y., eds., Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 5085–5109. Association for Computational Linguistics.

Wenzek, G.; Lachaux, M.; Conneau, A.; Chaudhary, V.; Guzmán, F.; Joulin, A.; và Grave, E. 2020. CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. Trong Calzolari, N.; Béchet, F.; Blache, P.; Choukri, K.; Cieri, C.; Declerck, T.; Goggi, S.; Isahara, H.; Maegaard, B.; Mariani, J.; Mazo, H.; Moreno, A.; Odijk, J.; và Piperidis, S., eds., Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020, 4003–4012. European Language Resources Association.

Wu, X.; Ma, G.; và Hu, S. 2022. Query-as-context Pre-training for Dense Passage Retrieval. CoRR, abs/2212.09598.

Wu, X.; Ma, G.; Lin, M.; Lin, Z.; Wang, Z.; và Hu, S. 2023a. ConTextual Masked Auto-Encoder for Dense Passage Retrieval. Trong Williams, B.; Chen, Y.; và Neville, J., eds., Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, 4738–4746. AAAI Press.

Wu, X.; Ma, G.; Wang, P.; Lin, M.; Lin, Z.; Zhang, F.; và Hu, S. 2023b. CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval. arXiv:2304.03158.

Yu, W.; Iter, D.; Wang, S.; Xu, Y.; Ju, M.; Sanyal, S.; Zhu, C.; Zeng, M.; và Jiang, M. 2023. Generate rather than Retrieve: Large Language Models are Strong Context Generators. Trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.

Zhou, K.; Liu, X.; Gong, Y.; Zhao, W. X.; Jiang, D.; Duan, N.; và Wen, J.-R. 2022. MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. arXiv preprint arXiv:2212.07841.

Zou, L.; Lu, W.; Liu, Y.; Cai, H.; Chu, X.; Ma, D.; Shi, D.; Sun, Y.; Cheng, Z.; Gu, S.; Wang, S.; và Yin, D. 2023. Pre-trained Language Model-based Retrieval and Ranking for Web Search. ACM Trans. Web, 17(1): 4:1–4:36.
