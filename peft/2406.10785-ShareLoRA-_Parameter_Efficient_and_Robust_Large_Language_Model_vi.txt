ShareLoRA: Tinh chỉnh Mô hình Ngôn ngữ Lớn hiệu quả về tham số và mạnh mẽ qua Thích ứng Hạng thấp Chia sẻ

Yurun Song
UC Irvine
yuruns@uci.edu

Junchen Zhao
UC Irvine
junchez3@uci.edu

Ian G. Harris
UC Irvine
harris@ics.uci.edu

Sangeetha Abdu Jyothi
UC Irvine, VMware Research
sangeetha.aj@uci.edu

Tóm tắt

Trong bài báo này, chúng tôi giới thiệu Thích ứng Hạng thấp Chia sẻ (ShareLoRA), một kỹ thuật tinh chỉnh Mô hình Ngôn ngữ Lớn (LLM) cân bằng giữa hiệu quả tham số, khả năng thích ứng và độ mạnh mẽ mà không làm giảm hiệu suất. Bằng cách chia sẻ chiến lược các ma trận trọng số hạng thấp qua các lớp khác nhau, ShareLoRA đạt được giảm 44% đến 96% tham số có thể huấn luyện so với LoRA tiêu chuẩn, cùng với việc giảm đáng kể chi phí bộ nhớ. Việc tăng hiệu quả này tỉ lệ với kích thước mô hình, làm cho ShareLoRA đặc biệt có lợi cho các môi trường hạn chế tài nguyên. Quan trọng hơn, ShareLoRA không chỉ duy trì hiệu suất mô hình mà còn thể hiện độ mạnh mẽ trong cả tác vụ phân loại và tạo sinh trên các mô hình đa dạng, bao gồm RoBERTa, GPT-2, và chuỗi LLaMA (1, 2, và 3). Nó liên tục vượt trội hơn LoRA trong các kịch bản zero-shot, few-shot, và tinh chỉnh liên tục, đạt được cải thiện độ chính xác trung bình lên tới 1,2%, và nâng cao khả năng tổng quát hóa qua các miền. Trong các thiết lập học liên tục, ShareLoRA đạt được độ chính xác cao hơn 1,2% trên GSM8K, 0,6% trên HumanEval, và 0,5% trên cả MMLU và MMLU-Pro. Kết quả của chúng tôi chứng minh rằng ShareLoRA hỗ trợ tinh chỉnh chất lượng cao trong khi cung cấp khả năng tổng quát mạnh mẽ và thích ứng liên tục qua các quy mô mô hình và tác vụ đa dạng khác nhau.

1 Giới thiệu

Khi các Mô hình Ngôn ngữ Được huấn luyện trước (PLMs) đã trở nên nổi bật (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019), các nhà nghiên cứu đang tập trung ngày càng nhiều vào việc tối ưu hóa việc sử dụng các trọng số được huấn luyện trước của những mô hình này. Tinh chỉnh truyền thống, liên quan đến việc điều chỉnh tất cả các tham số của PLM cho một tập dữ liệu hoặc tác vụ cụ thể, thường tốn nhiều tài nguyên và thời gian, đặc biệt với quy mô khổng lồ của các mô hình ngôn ngữ lớn (LLMs) (Brown and et.al, 2020; Kaplan et al., 2020; Hoffmann and et.al, 2022; et.al, 2022; Zhang et al., 2022; et.al, 2023b).

Tinh chỉnh Hiệu quả Tham số (PEFT) đã chứng minh là một chiến lược hiệu quả để giảm thiểu các thách thức liên quan đến điều chỉnh tham số rộng lớn. Bằng cách chỉ sửa đổi một tập con được chọn của các tham số mô hình, PEFT cho phép thích ứng tiết kiệm chi phí với các tác vụ cụ thể theo miền trong khi duy trì mức hiệu suất tương đương với những gì đạt được bằng tinh chỉnh đầy đủ (Houlsby et al., 2019; Li và Liang, 2021a; Lin et al., 2020; Lei et al., 2023; He et al., 2022, 2023; Mahabadi et al., 2021). Các kỹ thuật như Thích ứng Hạng thấp (LoRA) (Hu et al., 2021) nổi bật trong PEFT bằng cách chứng minh rằng các mô hình được tinh chỉnh với tập tham số giảm có thể khớp với hiệu suất của những mô hình được tinh chỉnh với tham số đầy đủ, hiệu quả thu hẹp khoảng cách về hiệu quả và hiệu suất.

Với hiệu suất ấn tượng của LoRA, các nghiên cứu tiếp theo đã nhắm đến việc nâng cao hiệu quả của nó, chủ yếu bằng cách giảm số lượng tham số có thể huấn luyện để giảm thiểu dấu chân bộ nhớ trong quá trình tinh chỉnh. Tuy nhiên, việc giảm đáng kể các tham số có thể huấn luyện có thể dẫn đến hội tụ chậm, trong khi việc giảm không đủ có thể khuyến khích mô hình dễ dàng quá khớp. Hơn nữa, các phương pháp PEFT hiện tại thường gặp khó khăn trong việc duy trì độ mạnh mẽ qua các miền khác nhau sau khi tinh chỉnh.

Để giải quyết những thách thức này, chúng tôi giới thiệu ShareLoRA, một phương pháp PEFT hiệu quả và đơn giản hiệu quả cân bằng việc lựa chọn tham số có thể huấn luyện trong khi tối ưu hóa khả năng thích ứng của mô hình, giảm thiểu yêu cầu bộ nhớ, và đảm bảo độ mạnh mẽ qua các miền. Cách tiếp cận của chúng tôi tận dụng quan sát rằng các ma trận trọng số hạng thấp A và B không cần được cấu hình duy nhất qua các lớp để đạt được hiệu suất PEFT tối ưu trong PLMs. Thay vào đó, chúng tôi đề xuất chia sẻ ma trận A hoặc B qua tất cả các lớp trong khi duy trì đối tác của nó như riêng biệt trong mỗi lớp. Chiến lược này đáp ứng một số mục tiêu chính:

• Hiệu quả Tham số: Chia sẻ ma trận hạng thấp qua các lớp giảm các tham số có thể huấn luyện 44% đến 96% so với LoRA tiêu chuẩn, cho các mô hình như LLaMA-7B. Việc giảm bộ nhớ này tỉ lệ với kích thước mô hình điều quan trọng cho việc tinh chỉnh hiệu quả LLMs trên GPUs tiêu dùng và các thiết bị biên.

• Khả năng Thích ứng Mô hình: Giữ ma trận chia sẻ có thể huấn luyện bảo tồn khả năng thích ứng của mô hình, cho phép nó học tập hiệu quả và thích ứng với các tác vụ và miền mới. Ngoài ra, các trọng số cập nhật cho mỗi thành phần mà LoRA áp dụng vẫn duy trì tính duy nhất nhưng chia sẻ một cơ sở chung, thúc đẩy tính nhất quán qua các lớp trong khi cho phép thích ứng cụ thể theo tác vụ.

• Thích ứng Liên tục: ShareLoRA thể hiện độ mạnh mẽ khi tinh chỉnh liên tục với các miền khác với miền nó đã được tinh chỉnh. Khả năng tổng quát hóa này phân biệt nó khỏi LoRA truyền thống và các phương pháp PEFT khác, thường gặp khó khăn trong việc duy trì hiệu suất khi đối mặt với các tác vụ ngoài miền.

Các thí nghiệm rộng rãi của chúng tôi qua nhiều mô hình, bao gồm RoBERTa, GPT-2, và chuỗi LLaMA, chứng minh rằng ShareLoRA không chỉ bảo tồn hiệu suất mô hình mà còn cho thấy độ mạnh mẽ đáng chú ý qua nhiều tác vụ khác nhau trong cả phân loại và tạo sinh.

2 Các Công trình Liên quan

PLMs được huấn luyện trên các tập dữ liệu lớn để phát triển biểu diễn ngôn ngữ rộng (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), nhưng thường thiếu sót trong các tác vụ chuyên biệt do thiếu kiến thức miền. Các cách tiếp cận truyền thống liên quan đến việc tinh chỉnh đầy đủ PLMs để nâng cao hiệu suất cụ thể theo miền (Xu và Wang, 2023; Xie et al., 2020; Dabre et al., 2019). Tuy nhiên, với kích thước ngày càng tăng của PLMs (Workshop et al., 2023; et.al, 2023b,a; Zhang et al., 2022), phương pháp này trở nên quá nặng về tài nguyên. Như một giải pháp thay thế, Tinh chỉnh Hiệu quả Tham số (PEFT) cung cấp một cách hiệu quả để duy trì hiệu suất với chi phí tính toán ít hơn.

Các phương pháp PEFT đã trở nên quan trọng để thích ứng các mô hình được huấn luyện trước quy mô lớn với các tác vụ cụ thể mà không cần đại tu rộng rãi các tham số của chúng. Cách tiếp cận này tiết kiệm tài nguyên tính toán và tăng hiệu quả. Ví dụ, Tinh chỉnh Tiền tố (Li và Liang, 2021a) thêm tham số vào các trạng thái ẩn qua các lớp, ảnh hưởng tinh tế đến hành vi của mô hình mà không thay đổi kiến trúc cơ bản của nó. Tinh chỉnh Gợi ý (Lester et al., 2021) thay đổi gợi ý và chỉ cập nhật các tham số liên quan, tập trung vào các khu vực cụ thể của hiệu suất mô hình. BitFit (Zaken et al., 2022) chỉ cập nhật các độ lệch trong mô hình, dẫn đến các sửa đổi tối thiểu nhưng hiệu quả.

Một kỹ thuật PEFT đáng chú ý là Thích ứng Hạng thấp (LoRA) (Hu et al., 2021), đạt được tinh chỉnh hiệu quả bằng cách kết hợp cơ chế thích ứng ma trận hạng thấp cùng với các trọng số hiện có của các lớp tuyến tính. Cách tiếp cận này giảm chi phí bộ nhớ trong khi bảo tồn hiệu quả của quá trình tinh chỉnh.

Các cải tiến gần đây cho LoRA đã mở rộng đáng kể khả năng của nó. QLoRA (Dettmers et al., 2023) tối ưu hóa LoRA cho việc tinh chỉnh các mô hình được lượng tử hóa, do đó tăng hiệu quả. ReLoRA (Lialin et al., 2023) kết hợp chiến lược khởi động trong quá trình huấn luyện trước để tăng khả năng thích ứng. LoraHub (Huang et al., 2024) đơn giản hóa quá trình bằng cách tự động hóa việc tạo các mô-đun LoRA tùy chỉnh cho các tác vụ cụ thể. Ngoài ra, GLoRA (Chavan et al., 2023) giới thiệu một mô-đun gợi ý tinh chỉnh trọng số và độ lệch, nâng cao hiệu suất qua nhiều ứng dụng khác nhau.

Mặc dù có những tiến bộ này, LoRA vẫn đối mặt với chi phí bộ nhớ đáng kể do việc sử dụng bộ nhớ kích hoạt cao trong các lớp LoRA trong giai đoạn tinh chỉnh. Để giải quyết vấn đề này, LoRA-FA (Zhang et al., 2023) chiến lược đông băng ma trận hạng thấp A và chỉ cập nhật ma trận B. Cách tiếp cận này giảm đáng kể số lượng tham số có thể huấn luyện và bộ nhớ kích hoạt, do đó nâng cao hiệu quả tinh chỉnh các mô hình ngôn ngữ lớn mà không ảnh hưởng đáng kể đến hiệu suất.

Tuy nhiên, LoRA-FA không giảm đầy đủ tổng số tham số cần được lưu trữ, đặt ra một thách thức đáng kể trong bối cảnh mà tài nguyên tính toán và lưu trữ bị hạn chế. Ngoài ra, bằng cách đông băng ma trận A, LoRA-FA hạn chế khả năng thích ứng và học hỏi từ dữ liệu mới của mô hình trong quá trình tinh chỉnh. Tính cứng nhắc này có thể cản trở hiệu suất của mô hình, đặc biệt trong các tác vụ phức tạp hoặc cụ thể theo miền.

Ngược lại, cách tiếp cận được đề xuất của chúng tôi ShareLoRA cung cấp một chiến lược động và linh hoạt hơn bằng cách cho phép ma trận A hoặc B, hoặc cả hai, được chia sẻ qua các lớp khác nhau. Phương pháp này không chỉ bảo tồn khả năng thích ứng của mô hình mà còn giảm thêm yêu cầu bộ nhớ.

3 Phương pháp

Trong phần này, chúng tôi cung cấp mô tả chi tiết về cách tiếp cận PEFT được đề xuất ShareLoRA, như được minh họa trong Hình 1. ShareLoRA tạo điều kiện cho các cấu hình linh hoạt thông qua hai chiều chính: 1. lựa chọn chia sẻ giữa các ma trận A, B, hoặc cả A và B (ShareA, ShareB, và ShareAB), và 2. phạm vi chia sẻ, có thể là qua các lớp khác nhau như các lớp tự chú ý. Khung này cho phép nhiều kết hợp khác nhau, cho phép thích ứng tùy chỉnh các mô hình hạng thấp cho các tác vụ cụ thể.

Cấu hình ShareA Trong cấu hình ShareA, ma trận hạng thấp A được chia sẻ đồng nhất qua tất cả các lớp, với mỗi lớp sử dụng ma trận Bi duy nhất riêng của nó. Công thức cho thích ứng trọng số trong mỗi lớp i có thể được mở rộng để chi tiết ảnh hưởng lên biến đổi mô hình:

∆Wi = αABi = α∑(k=1 to r) A:,k Bk,:,i (1)

trong đó A:,k đại diện cho cột thứ k của A, và Bk,:,i là hàng thứ k của ma trận Bi. Phương trình này cho thấy thay đổi trọng số của mỗi lớp, ∆Wi, là một kết hợp tuyến tính của các cột của A được trọng số bởi các phần tử tương ứng của Bi. Ma trận chiếu xuống chia sẻ A này giảm chiều một cách đồng nhất qua tất cả các lớp, do đó giảm thiểu redundancy trong học tập và sử dụng bộ nhớ trong khi cho phép các biến đổi đầu ra được tùy chỉnh thông qua các ma trận Bi cụ thể theo lớp.

Cấu hình ShareB Trong cấu hình ShareB, ma trận B được chia sẻ đồng nhất qua tất cả các lớp, trong khi mỗi lớp sử dụng ma trận Ai duy nhất riêng của nó. Điều chỉnh trọng số cho mỗi lớp được biểu diễn như:

∆Wi = αAiB = α∑(k=1 to r) Ai,:,k Bk,: (2)

trong đó Ai,:,k biểu thị cột thứ k của ma trận Ai cho lớp i, và Bk,: đại diện cho hàng thứ k của ma trận chia sẻ B. Ở đây, ma trận chiếu lên đồng nhất B đảm bảo mở rộng nhất quán của dữ liệu được biến đổi trở lại chiều đầu ra qua tất cả các lớp, trong khi các ma trận Ai riêng biệt cho phép thích ứng với các đặc tính đầu vào cụ thể của mỗi lớp.

Cấu hình ShareAB Khi cả hai ma trận A và B được chia sẻ qua tất cả các lớp, sự thay đổi trong trọng số được đơn giản hóa, dẫn đến giảm tham số đáng kể:

∆W = αAB = α∑(k=1 to r) A:,k Bk,: (3)

trong đó cả A:,k và Bk,: đều được chia sẻ qua tất cả các lớp. Cấu hình này giảm đáng kể độ phức tạp mô hình bằng cách loại bỏ nhu cầu các ma trận riêng biệt trong mỗi lớp, do đó giảm yêu cầu bộ nhớ và chi phí tính toán. Toàn bộ mô hình hoạt động dưới một sơ đồ biến đổi đồng nhất, đơn giản hóa huấn luyện và lưu trữ nhưng yêu cầu hiệu chuẩn cẩn thận các giá trị ban đầu và điều chỉnh liên tục trong quá trình tinh chỉnh để bảo tồn hiệu quả mô hình qua các tác vụ đa dạng.

Chia sẻ Qua các Lớp Tự Chú ý Trong cấu hình ShareA của ShareLoRA áp dụng cho PLMs qua tất cả các lớp tự chú ý, các ma trận AQ, AK, và AV được chia sẻ. Các ma trận này chịu trách nhiệm giảm chiều của các đầu vào cho Truy vấn (Q), Khóa (K), và Giá trị (V) tương ứng, chúng tôi gọi nó là ShareA qkv trong các đoạn tiếp theo. Quá trình cho mỗi thành phần trong lớp tự chú ý thứ i được hình thức hóa như sau:

Qi = Xi AQ BQi (4)
Ki = Xi AK BKi (5)
Vi = Xi AV BVi (6)

Attention(Qi, Ki, Vi) = softmax(Qi KiT / √dKi) Vi, (7)

trong đó Xi biểu thị đầu vào cho lớp tự chú ý thứ i. Mỗi ma trận AQ, AK, và AV tạo điều kiện cho việc giảm nhất quán trong chiều đầu vào qua tất cả các lớp, đơn giản hóa kiến trúc mô hình bằng cách duy trì cách tiếp cận đồng nhất để xử lý các khía cạnh cơ bản của tự chú ý. Các ma trận duy nhất BQi, BKi, và BVi cho mỗi thành phần cho phép các biến đổi được tùy chỉnh đáp ứng nhu cầu cụ thể của mỗi lớp tự chú ý.

4 Thí nghiệm

Trong nghiên cứu của chúng tôi, chúng tôi tiến hành đánh giá toàn diện về hiệu suất downstream của ShareLoRA qua một số mô hình chuỗi, bao gồm RoBERTa (Liu et al., 2019) và GPT-2 (Radford et al., 2019). Chúng tôi so sánh các kết quả này với các cách tiếp cận đã được thiết lập khác như LoRA (Hu et al., 2021), LoRA-FA (Zhang et al., 2023). Ngoài ra, chúng tôi mở rộng ứng dụng ShareLoRA cho mô hình quy mô lớn trong kiến trúc chuỗi LLaMA (et.al, 2023b, et.al, 2023a, Dubey et al., 2024), đặc biệt trong các kịch bản few-shot, zero-shot. Hơn nữa, các thí nghiệm của chúng tôi bao gồm một loạt kích thước mô hình, từ 7 tỷ đến 13 tỷ tham số, và bao gồm cả các biến thể mô hình được lượng tử hóa và không được lượng tử hóa. Tất cả các thử nghiệm được thực hiện trên GPUs Nvidia A6000 và RTX 3090. Cho các thiết lập siêu tham số thí nghiệm, xem Phụ lục Bảng 8-Bảng 11.

4.1 Tập dữ liệu

Các tập dữ liệu thí nghiệm được chia chủ yếu thành ba danh mục: Hiểu biết Ngôn ngữ Tự nhiên (NLU), Tạo sinh Ngôn ngữ Tự nhiên (NLG) và các tác vụ few-shot, sử dụng cùng cấu hình và tập dữ liệu như LoRA (Hu et al., 2021) và (Dettmers et al., 2023).

Cho NLU, chúng tôi sử dụng benchmark GLUE (Wang et al., 2019), bao gồm các tác vụ MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, và STS-B. Đáng chú ý, cho các tác vụ MRPC, RTE, và STS-B, chúng tôi khởi tạo các mô-đun LoRA với checkpoint MNLI đã được huấn luyện như (Hu et al., 2021) đã chứng minh. Cho NLG, chúng tôi sao chép các thí nghiệm tương tự như của LoRA sử dụng tập dữ liệu thách thức E2E (Novikova et al., 2017), theo cùng thiết lập thí nghiệm.

Ngoài ra, chúng tôi mở rộng các thí nghiệm của mình cho các tác vụ few-shot và zero-shot trên các mô hình lớn hơn, chứng minh khả năng thích ứng của cách tiếp cận của chúng tôi. Theo cấu hình được nêu trong (Dettmers et al., 2023), chúng tôi sử dụng Alpaca (Taori et al., 2023), CodeAlpaca (Chaudhary, 2023) và MATH (Hendrycks et al., 2021b) cho LoRA và ShareLoRA, sử dụng benchmark MMLU (Hendrycks et al., 2021a) để đánh giá. Một số benchmark khác như ARC (Chollet, 2019), Hellaswag (Zellers et al., 2019), MMLU-Pro (Wang et al., 2024), HumanEval (Chen et al., 2021) và GSM8K (Cobbe et al., 2021) được sử dụng để so sánh khả năng thích ứng mô hình. Tất cả các thiết lập thí nghiệm đều nhất quán với các nghiên cứu được mô tả và demo của các repository của chúng, dựa trên kiến thức tốt nhất của chúng tôi.

4.2 Baseline

Tinh chỉnh Đầy đủ (FT) là cách tiếp cận được sử dụng phổ biến cho thích ứng mô hình liên quan đến việc cập nhật tất cả các tham số của mô hình.

LoRA (Hu et al., 2021) là một kỹ thuật giới thiệu một cặp ma trận có thể huấn luyện phân tách hạng cùng với các ma trận trọng số hiện có trong mạng neural.

Bitfit (Zaken et al., 2022) là một kỹ thuật để cập nhật chỉ một tập con nhỏ được chọn của các tham số độ lệch, để cải thiện hiệu suất trên các tác vụ mới trong khi đông băng tất cả các trọng số được huấn luyện trước khác.

PreLayer/Prefix (Li và Liang, 2021b) là một kỹ thuật hiệu quả tham số để tùy chỉnh các mô hình ngôn ngữ lớn bằng cách học các kích hoạt cụ thể sau mỗi lớp Transformer cho các token tiền tố được chỉ định, trong khi các tham số mô hình chính vẫn không thay đổi.

Adapter (Houlsby et al., 2019) liên quan đến việc chèn các lớp adapter giữa các mô-đun neural như các mô-đun tự chú ý và MLP, nâng cao tính linh hoạt của mô hình mà không cần sửa đổi rộng rãi.

AdapterL (Lin et al., 2020) giới thiệu các adapter sau mô-đun MLP theo sau bởi một LayerNorm, trong khi AdapterD (Rücklé et al., 2021) tăng hiệu quả bằng cách bỏ qua một số lớp adapter.

IA3 (Liu et al., 2022) là một cách tiếp cận PEFT nâng cao hiệu suất mô hình bằng cách mở rộng các kích hoạt với các vector đã học.

LoRA-FA (Zhang et al., 2023) là một cách tiếp cận tiết kiệm bộ nhớ để tinh chỉnh các mô hình ngôn ngữ lớn bằng cách giảm bộ nhớ kích hoạt cần thiết.

VERA (Kopiczko et al., 2023) giảm các tham số có thể huấn luyện bằng cách sử dụng các ma trận ngẫu nhiên đông băng và các vector tỷ lệ đã học, khớp với hiệu suất của LoRA một cách hiệu quả hơn.

Tied-LoRA (Renduchintala et al., 2023) cải thiện hiệu quả tham số bằng cách ràng buộc trọng số và huấn luyện ít ma trận hạng thấp hơn, khớp với hiệu suất LoRA với ít tham số hơn đáng kể.

VB-LoRA (Li et al., 2024) đạt được hiệu quả tham số cực đoan bằng cách tạo ra các trọng số thích ứng hạng thấp từ một ngân hàng vector chia sẻ sử dụng lựa chọn top-k có thể phân biệt.

5 Kết quả

Hiệu quả Tham số và Hiệu suất

ShareLoRA chứng minh hiệu quả tham số đáng kể trong khi duy trì hoặc cải thiện hiệu suất qua các kích thước mô hình và tác vụ khác nhau. Cho các mô hình LLaMA quy mô lớn, như được hiển thị trong Bảng 3, ShareA giảm các tham số có thể huấn luyện 44% so với LoRA. Mặc dù có sự giảm đáng kể này, ShareA đạt được điểm MMLU tương đương hoặc cải thiện, với LLaMA 13B cho thấy sự tăng từ 47.60 lên 48.15.

Trên Thách thức E2E NLG trong Bảng 2, ShareA chứng minh hiệu quả lớn hơn đáng kể trên các mô hình GPT-2: nó giảm số lượng tham số của LoRA 43% trên mô hình Medium, nhưng vẫn đạt được các cải thiện hiệu suất. Cụ thể, BLEU của GPT-2 Medium cải thiện từ 69.5 lên 69.7 và ROUGE-L từ 71.51 lên 71.63, trong khi BLEU của GPT-2 Large tăng từ 69.8 lên 70.0.

Đáng chú ý, trong khi ShareA liên tục vượt trội hơn LoRA, các thí nghiệm của chúng tôi cho thấy ShareB và ShareAB thường kém hiệu suất so với ShareA. Ví dụ, trong mô hình GPT-2 Large, Bảng 2 cho thấy ShareB đạt được điểm BLEU 69.7 và điểm ROUGE-L 70.94, thấp hơn cả LoRA và ShareA.

So sánh ShareLoRA với các phương pháp PEFT tiên tiến khác, chúng tôi quan sát hiệu suất cạnh tranh hoặc vượt trội. Ví dụ, trên benchmark GLUE sử dụng RoBERTa-large, Bảng 1 cho thấy ShareA đạt được điểm trung bình 88.6 trên GLUE, so với 84.9 cho Prefix-tuning trong khi sử dụng ít tham số hơn đáng kể. Ngay cả trong cấu hình tích cực nhất, ShareAB, với chỉ 0.03M tham số có thể huấn luyện giảm 96% tham số có thể huấn luyện so với LoRA, vượt trội hơn IA3 sử dụng 0.18M tham số, đạt được điểm trung bình 87.0 trên GLUE so với 86.0 của IA3.

Hơn nữa, dưới kích thước tham số có thể huấn luyện tương tự, ShareA chứng minh hiệu suất tốt hơn LoRA-FA. Ví dụ, ShareA đạt được điểm GLUE trung bình 89.3 với 0.4M tham số trên RoBERTa-large, vượt qua điểm 88.5 của LoRA-FA với cùng số lượng tham số.

Khả năng Thích ứng Mô hình

ShareLoRA chứng minh khả năng thích ứng vượt trội qua một loạt tác vụ và kích thước mô hình đa dạng. Trong các thí nghiệm với mô hình RoBERTa-base trên benchmark GLUE được hiển thị trong Bảng 1, ShareA thể hiện sức mạnh đặc biệt trên các tập dữ liệu nhỏ hơn thường dễ bị quá khớp. Cụ thể, trên các tác vụ như MRPC, CoLA, và RTE, ShareA đạt được các cải thiện hiệu suất từ 0.2% đến 0.5%. Những cải thiện này đặc biệt đáng chú ý vì các tập dữ liệu này thường đã đạt được hội tụ đầy đủ dưới các cấu hình huấn luyện tiêu chuẩn (Hu et al., 2021), gợi ý khả năng của ShareLoRA để trích xuất hiệu suất bổ sung ngay cả trong các kịch bản thách thức.

ShareA tiếp tục thể hiện khả năng học chuyển giao nâng cao. Khi tinh chỉnh trên các tác vụ thích ứng như MRPC, RTE, và STS-B sử dụng checkpoint MNLI tốt nhất, ShareA liên tục hoạt động ngang bằng hoặc vượt trội hơn LoRA. Đáng chú ý, ShareA cũng vượt trội hơn các phương pháp PEFT khác trong kịch bản học chuyển giao này. Ví dụ, trên tác vụ RTE, ShareA, với 0.16M tham số cho RoBERTa-base, đạt được điểm 87.1, vượt trội đáng kể so với 54.51 của Prefix-tuning như được hiển thị trong Bảng 1. ShareA cũng chứng minh hiệu suất vượt trội khi so sánh với các phương pháp có kích thước tham số có thể huấn luyện tương tự, như BitFit với 0.1M tham số và LoRA-FA với 0.15M tham số. Điều này làm nổi bật hiệu quả của ShareA trong việc sử dụng tham số và khả năng trích xuất hiệu suất tốt hơn từ một ngân sách tham số nhất định, đặc biệt trong các kịch bản học chuyển giao.

Độ Mạnh mẽ Qua các Miền

ShareLoRA cho thấy độ mạnh mẽ và khả năng thích ứng mạnh mẽ qua cả các miền tác vụ đa dạng và kích thước mô hình khác nhau. Như được trình bày trong Bảng 3 và 4, ShareLoRA liên tục vượt trội hơn LoRA trong các kịch bản học zero-shot và few-shot qua nhiều benchmark đánh giá.

Trên mô hình LLaMA2-7B, ShareLoRA cải thiện độ chính xác MMLU 0.7%, trong khi trên mô hình LLaMA2-13B, nó đạt được mức tăng 0.5%. Ngoài MMLU, ShareLoRA mang lại cải thiện hiệu suất trung bình 1.8% và 1.3% trên các mô hình LLaMA2-7B và LLaMA2-14B tương ứng, với cải thiện độ chính xác từ 0.5% đến 2.5% qua các tác vụ khác nhau. Những kết quả này tập thể làm nổi bật hiệu quả của ShareLoRA trong việc nâng cao khả năng tổng quát và khả năng chuyển giao mô hình qua cả các mô hình ngôn ngữ quy mô nhỏ và lớn.

Thích ứng Liên tục

Để đánh giá độ mạnh mẽ và khả năng giữ lại kiến thức trong quá trình tinh chỉnh liên tục, chúng tôi triển khai các mô hình LLaMA3 và LLaMA3.1 trên tập dữ liệu MATH. Sau đó chúng tôi đánh giá hiệu suất của chúng trong toán học và qua các miền khác, như MMLU và MMLU-Pro, để so sánh mức độ các mô hình này bảo tồn kiến thức, như được hiển thị trong Bảng 5. Phát hiện của chúng tôi chỉ ra rằng cả ShareLoRA và LoRA đều mang lại hiệu suất khớp nhau cho các miền được tinh chỉnh trực tiếp. Tuy nhiên, khi thích ứng các mô hình được tinh chỉnh này với các benchmark đánh giá khác, ShareLoRA chứng minh độ mạnh mẽ lớn hơn, vượt trội hơn LoRA. Cụ thể, trên MMLU-Pro, ShareLoRA vượt trội hơn LoRA 0.86% trên LLaMA3.1 và 0.75% trên LLaMA3.

Chúng tôi cũng điều tra tinh chỉnh liên tục qua nhiều tác vụ—bắt đầu từ Alpaca, theo sau là GSM8K, sau đó CodeAlpaca, và cuối cùng quay lại Alpaca trong Bảng 6. ShareLoRA liên tục vượt trội hơn LoRA trong thiết lập này, với các cải thiện quan sát được 0.5% trên MMLU và MMLU-Pro, 1.2% trên GSM8K, và 0.6% trên HumanEval, làm nổi bật độ mạnh mẽ của nó trong học liên tục đa tác vụ.

6 Phân tích và Thảo luận

Tầm quan trọng Tương đối của các Thành phần LoRA

Các phát hiện thí nghiệm của chúng tôi chứng minh rằng cả LoRA và ShareA đều liên tục vượt trội hơn ShareB trong nhiều tác vụ phân loại và tạo sinh, qua hầu hết các metric. Trong khung LoRA, ma trận chiếu lên B đóng vai trò quan trọng bằng cách nâng cao đáng kể chiều của biểu diễn hạng thấp. Do đó, việc chia sẻ mô-đun ít quan trọng hơn, LoRA A, trong khi giữ lại tính toàn vẹn của B là cả thực tế và có thể biện minh. Tuy nhiên, chia sẻ đồng thời cả hai ma trận A và B có xu hướng thỏa hiệp quá nhiều thông tin quan trọng. Đặc biệt trong các tác vụ tạo sinh, việc lựa chọn chia sẻ thành phần A thay vì B trong khung ShareLoRA có lợi ích chiến lược, như được thấy trong Bảng 2. Điều này là do việc mở rộng chiều trung gian chứng minh quan trọng và thách thức hơn so với việc nén các đặc trưng chiều cao trong các kịch bản tạo sinh phức tạp.

Chia sẻ QKV Chú ý vs. Chia sẻ Tất cả

Sự khác biệt giữa việc chia sẻ cơ chế tự chú ý và tất cả các mô-đun tuyến tính tồn tại trên các thành phần MLP như gates và các chiếu lên/xuống. Điều này dẫn đến sự khác biệt trong các tham số có thể huấn luyện giữa A và B của LoRA. Lựa chọn chiến lược liên quan đến việc quyết định có nên chia sẻ đồng nhất trọng số qua tất cả các lớp (ShareA) hay chia sẻ có chọn lọc chúng, như chỉ cho chiếu xuống (ShareAB) trong khi duy trì trọng số duy nhất cho các thành phần khác như chiếu lên và gates. Kết quả sơ bộ trong Phụ lục Hình 5 gợi ý rằng việc chia sẻ có chọn lọc, đặc biệt của các ma trận QKV trong Share qkv, cung cấp sự cân bằng hiệu quả bằng cách căn chỉnh chặt chẽ với cả ShareA và LoRA, có thể giảm thiểu rủi ro quá khớp.

Dấu chân Bộ nhớ

Trong bối cảnh các mô hình nhỏ hơn như RoBERTa và GPT-2, ShareA mang lại tiết kiệm tham số tối thiểu, điều này không đáng kể với khả năng GPU hiện đại. Tuy nhiên, với các mô hình lớn hơn như LLaMA, ShareA chứng minh các giảm đáng kể hơn. Cụ thể, các mô hình LLaMA 7B và 13B cắt giảm khoảng 60 triệu và 110 triệu tham số có thể huấn luyện, khi so sánh với LoRA. Điều này dẫn đến các cải thiện hiệu quả đáng kể, giảm cả dấu chân tính toán và nhu cầu lưu trữ đĩa.

Như được mô tả trong Hình 2 và Hình 7, trong mô hình Llama3 70B, thích ứng ShareA đạt được giảm 6.3GB trong dấu chân bộ nhớ dưới cấu hình lượng tử hóa. Trong khi đó, trong mô hình Llama2 13B với cấu hình LoRA, ShareA quản lý để giảm dấu chân bộ nhớ 3.8GB và nâng cao tốc độ huấn luyện khoảng 3%. Các khoảng tin cậy trong Bảng 3 minh họa rằng ShareA không chỉ cải thiện hiệu suất mà còn tăng độ mạnh mẽ so với LoRA tiêu chuẩn, làm nổi bật các lợi thế thực tế của ShareLoRA trong LLMs.

Phân tích SVD của Trọng số LoRA và ShareA

Chúng tôi đã tiến hành phân tích Phân tách Giá trị Đơn lẻ (SVD) trên các trọng số của LLaMA 13B cho cả LoRA và ShareA, như được hiển thị trong Hình 3 trong Phụ lục. Kết quả tiết lộ các mẫu riêng biệt trong phân phối giá trị đơn lẻ của chúng qua các lớp. Trọng số LoRA thể hiện sự giảm mạnh trong các giá trị đơn lẻ, chỉ ra sự tập trung thông tin trong một vài thành phần chi phối. Điều này có thể dẫn đến chuyên biệt hóa nhưng cũng có thể tăng rủi ro quá khớp. Ngược lại, trọng số ShareA cho thấy sự giảm mượt mà, dần dần hơn trong các giá trị đơn lẻ, gợi ý phân phối cân bằng hơn của thông tin giữa các thành phần. Phân phối cân bằng này góp phần vào khả năng thích ứng và tổng quát hóa nâng cao của ShareA qua các tác vụ khác nhau.

Những phát hiện này cung cấp cái nhìn về lý do tại sao ShareA có thể cung cấp độ mạnh mẽ cải thiện và hiệu suất huấn luyện liên tục so với LoRA. Phân phối giá trị đơn lẻ đồng nhất hơn trong ShareA gợi ý rằng nó nắm bắt các đặc trưng phong phú hơn, dẫn đến khả năng tổng quát hóa tốt hơn qua các miền khác nhau.

7 Kết luận

Trong bài báo này, chúng tôi giới thiệu ShareLoRA, một tối ưu hóa của kiến trúc LoRA chia sẻ hoặc chiếu lên hoặc xuống qua các lớp khác nhau. ShareLoRA giảm đáng kể số lượng tham số có thể huấn luyện ít nhất một nửa so với LoRA gốc và cho thấy hiệu suất cải thiện trên các tập dữ liệu đã hội tụ đầy đủ. Thông qua thí nghiệm rộng rãi với các tác vụ NLU, NLG, và zero-shot trên các mô hình với quy mô khác nhau, ShareLoRA chứng minh sự cân bằng mạnh mẽ giữa hiệu quả tính toán và hiệu suất mạnh mẽ. Nó liên tục duy trì khả năng thích ứng cao, độ mạnh mẽ mạnh mẽ, và khả năng học liên tục hiệu quả qua các tác vụ và kiến trúc đa dạng.

8 Hạn chế

Các hạn chế của ShareLoRA chủ yếu trong tốc độ hội tụ và các ứng dụng thực tế. ShareAB và ShareB có xu hướng hội tụ chậm hơn so với LoRA, mặc dù ShareA cho thấy tốc độ hội tụ phần lớn cạnh tranh với LoRA trên các tập dữ liệu nhỏ hơn, chỉ với độ trễ nhẹ trên các tập dữ liệu lớn hơn. Điều này chỉ ra rằng ShareA khá thành thạo với các tập dữ liệu dễ hội tụ và hiệu quả giảm thiểu các kịch bản gần quá khớp.

Liên quan đến ứng dụng thực tế của GPUs, ShareLoRA giới thiệu một số phức tạp trong quá trình huấn luyện song song trên nhiều GPUs. Điều này chủ yếu do nhu cầu đồng bộ hóa nhất quán của Mô-đun Chia sẻ, một khi nó được sao chép qua các GPUs khác nhau tại mọi bước tính toán.
