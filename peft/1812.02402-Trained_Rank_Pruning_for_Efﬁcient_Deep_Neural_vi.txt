# 1812.02402.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/1812.02402.pdf
# Kích thước tệp: 576259 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cắt Tỉa Hạng Được Huấn Luyện cho Mạng Nơ-ron Sâu
Hiệu Quả
Yuhui Xu1, Yuxi Li1, Shuai Zhang2, Wei Wen3, Botao Wang2, Wenrui Dai1, Yingyong Qi2, Yiran
Chen3, Weiyao Lin1và Hongkai Xiong1
1Đại học Jiao Tong Thượng Hải, Email: {yuhuixu, lyxok1, daiwenrui, wylin,
xionghongkai}@sjtu.edu.cn
2Nghiên cứu AI Qualcomm, Email: {shuazhan, botaow, yingyong}@qti.qualcomm.com
3Đại học Duke, Email: {wei.wen, yiran.chen}@duke.edu
Tóm tắt
Để tăng tốc suy luận DNN, xấp xỉ hạng thấp đã được áp dụng rộng rãi nhờ vào cơ sở lý thuyết vững chắc và triển khai hiệu quả. Một số công trình trước đây đã cố gắng trực tiếp xấp xỉ mô hình được huấn luyện trước bằng phân tích hạng thấp; tuy nhiên, các lỗi xấp xỉ nhỏ trong tham số có thể lan truyền thành tổn thất dự đoán lớn. Rõ ràng, việc tách biệt xấp xỉ hạng thấp khỏi quá trình huấn luyện là không tối ưu. Khác với các công trình trước, bài báo này tích hợp xấp xỉ hạng thấp và điều chuẩn hóa vào quá trình huấn luyện. Chúng tôi đề xuất Cắt Tỉa Hạng Được Huấn Luyện (TRP), luân phiên giữa xấp xỉ hạng thấp và huấn luyện. TRP duy trì khả năng của mạng gốc trong khi áp đặt ràng buộc hạng thấp trong quá trình huấn luyện. Một điều chuẩn hóa hạt nhân được tối ưu hóa bằng gradient descent ngẫu nhiên con được sử dụng để thúc đẩy thêm hạng thấp trong TRP. Mạng được huấn luyện với TRP có cấu trúc hạng thấp tự nhiên, và được xấp xỉ với tổn thất hiệu suất không đáng kể, do đó loại bỏ việc tinh chỉnh sau xấp xỉ hạng thấp. Phương pháp đề xuất được đánh giá toàn diện trên CIFAR-10 và ImageNet, vượt trội hơn các phương pháp nén trước đây sử dụng xấp xỉ hạng thấp. Mã nguồn của chúng tôi có sẵn tại: https://github.com/yuhuixu1993/Trained-Rank-Pruning.

1 Giới thiệu
Mạng Nơ-ron Sâu (DNN) đã thể hiện thành công đáng kể trong nhiều tác vụ thị giác máy tính như phân loại hình ảnh [8], phát hiện đối tượng [15] và phân đoạn ngữ nghĩa [3]. Mặc dù có hiệu suất cao trong các DNN lớn được hỗ trợ bởi phần cứng tính toán song song tiên tiến, hầu hết các kiến trúc mạng tiên tiến không phù hợp cho việc sử dụng hạn chế tài nguyên như sử dụng trên thiết bị luôn bật, thiết bị tầm thấp chạy bằng pin, do những hạn chế về khả năng tính toán, bộ nhớ và năng lượng.

Để giải quyết vấn đề này, các phương pháp phân tích hạng thấp [6,10,7,17,1] đã được đề xuất để tối thiểu hóa tính dư thừa theo kênh và không gian bằng cách phân tích mạng gốc thành một mạng compact với các lớp hạng thấp. Khác với các công trình trước, bài báo này đề xuất một phương pháp mới để thiết kế mạng hạng thấp.

Mạng hạng thấp có thể được huấn luyện trực tiếp từ đầu. Tuy nhiên, rất khó để đạt được kết quả thỏa mãn vì một số lý do. (1) Dung lượng thấp: So với mạng hạng đầy đủ gốc, dung lượng của mạng hạng thấp bị hạn chế, điều này gây khó khăn trong việc tối ưu hóa hiệu suất. (2) Cấu trúc sâu: Phân tích hạng thấp thường làm tăng gấp đôi số lượng lớp trong mạng. Các lớp bổ sung làm cho tối ưu hóa số trở nên khó khăn hơn nhiều do gradient bùng nổ và/hoặc biến mất. (3) Lựa chọn hạng: Hạng của mạng được phân tích thường được chọn làm siêu tham số dựa trên các mạng được huấn luyện trước; có thể không phải là hạng tối ưu cho mạng được huấn luyện từ đầu.

Hoặc, một số công trình trước [18,7,10] đã cố gắng phân tích các mô hình được huấn luyện trước để có được mạng hạng thấp ban đầu. Tuy nhiên, hạng thấp được áp đặt một cách heuristic có thể gây ra tổn thất độ chính xác lớn và cần huấn luyện lại mạng để phục hồi hiệu suất của mạng gốc nhiều nhất có thể. Một số nỗ lực đã được thực hiện để sử dụng điều chuẩn hóa thưa thớt [17,4] để ràng buộc mạng vào không gian hạng thấp. Mặc dù điều chuẩn hóa thưa thớt làm giảm lỗi gây ra bởi phân tích đến một mức độ nào đó, hiệu suất vẫn giảm nhanh chóng khi tỷ lệ nén tăng.

Trong bài báo này, chúng tôi đề xuất một phương pháp mới, được gọi là Cắt Tỉa Hạng Được Huấn Luyện (TRP), để huấn luyện mạng hạng thấp. Chúng tôi nhúng phân tích hạng thấp vào quá trình huấn luyện bằng cách từ từ đẩy phân bố trọng số của một mạng hoạt động tốt vào dạng hạng thấp, trong đó tất cả các tham số của mạng gốc được giữ lại và tối ưu hóa để duy trì dung lượng của nó. Chúng tôi cũng đề xuất một điều chuẩn hóa hạt nhân gradient descent ngẫu nhiên con được tối ưu hóa để ràng buộc thêm các trọng số trong không gian hạng thấp nhằm tăng cường TRP. Giải pháp đề xuất được minh họa trong Hình 1.

Nhìn chung, các đóng góp của chúng tôi được tóm tắt dưới đây.
1. Một phương pháp huấn luyện mới gọi là TRP được trình bày bằng cách nhúng rõ ràng phân tích hạng thấp vào quá trình huấn luyện mạng;
2. Một điều chuẩn hóa hạt nhân được tối ưu hóa bằng gradient descent ngẫu nhiên con để tăng cường hiệu suất của TRP;
3. Cải thiện tăng tốc suy luận và giảm tổn thất độ chính xác xấp xỉ trong cả phương pháp phân tích theo kênh và theo không gian.

𝑘𝑤𝑘ℎ𝐶𝑘𝑤𝑘ℎ𝐶
𝑘𝑤𝑘ℎ𝐶Luồng đặc trưng
Luồng gradient
Xấp xỉ hạng thấp
Thay thế
……
…𝑊𝑡
𝑇𝑧
𝑊𝑡
(a) (b)

Hình 1: Quá trình huấn luyện TRP bao gồm hai phần như được minh họa trong (a) và (b). (a) Một vòng lặp bình thường với lan truyền tới-lùi và cập nhật trọng số. (b) Một vòng lặp huấn luyện được chèn bởi cắt tỉa hạng, trong đó xấp xỉ hạng thấp được áp dụng trước trên các bộ lọc hiện tại trước khi tích chập. Trong quá trình lan truyền ngược, các gradient được cộng trực tiếp vào các bộ lọc hạng thấp và các trọng số gốc được thay thế bởi các bộ lọc hạng thấp đã cập nhật. (b) được áp dụng một lần mỗi m vòng lặp (tức là khi vòng lặp cập nhật gradient t=zm; z = 0;1;2; ...), ngược lại (a) được áp dụng.

2 Phương pháp
2.1 Kiến thức cơ bản
Một cách chính thức, các bộ lọc tích chập trong một lớp có thể được ký hiệu bằng một tensor W∈R^(n×c×kw×kh), trong đó n và c là số lượng bộ lọc và kênh đầu vào, kh và kw là chiều cao và chiều rộng của các bộ lọc. Một đầu vào của lớp tích chập Fi∈R^(c×x×y) tạo ra một đầu ra là Fo=W⊛Fi. Tương quan theo kênh [18] và tương quan theo không gian [10] được khám phá để xấp xỉ các bộ lọc tích chập trong không gian hạng thấp. Trong bài báo này, chúng tôi tập trung vào hai sơ đồ phân tích này. Tuy nhiên, khác với các công trình trước, chúng tôi đề xuất một sơ đồ huấn luyện mới TRP để có được mạng hạng thấp mà không cần huấn luyện lại sau khi phân tích.

--- TRANG 2 ---
2.2 Cắt Tỉa Hạng Được Huấn Luyện
Chúng tôi đề xuất một sơ đồ huấn luyện đơn giản nhưng hiệu quả gọi là Cắt Tỉa Hạng Được Huấn Luyện (TRP) theo cách định kỳ:

Wt+1 = {
    Wt - α∇f(Wt) if t%m ≠ 0
    Tz - α∇f(Tz) if t%m = 0
    Tz = D(Wt); z = t/m
}                                                                                                                (1)

trong đó D(·) là một toán tử xấp xỉ tensor hạng thấp, α là tỷ lệ học, t chỉ số vòng lặp và z là vòng lặp của toán tử D, với m là chu kỳ cho xấp xỉ hạng thấp. Chúng tôi áp dụng xấp xỉ hạng thấp mỗi m vòng lặp SGD. Điều này tiết kiệm thời gian huấn luyện đáng kể. Như được minh họa trong Hình 1, mỗi m vòng lặp, chúng tôi thực hiện xấp xỉ hạng thấp trên các bộ lọc gốc, trong khi các gradient được cập nhật trên dạng hạng thấp kết quả. Ngược lại, mạng được cập nhật thông qua SGD bình thường. Sơ đồ huấn luyện của chúng tôi có thể được kết hợp với các toán tử hạng thấp tùy ý. Trong công trình đề xuất, chúng tôi chọn các kỹ thuật hạng thấp được đề xuất trong [10] và [18], cả hai đều biến đổi các bộ lọc 4 chiều thành ma trận 2D và sau đó áp dụng phân tích giá trị đơn cắt cụt (TSVD). SVD của ma trận Wt có thể được viết là:

Wt = Σ(i=1 to rank(Wt)) σiUi(Vi)^T;                                                                                (2)

trong đó σi là giá trị đơn của Wt với σ1 ≥ σ2 ≥ ... ≥ σrank(Wt), và Ui và Vi là các vector đơn. TSVD được tham số hóa (Wt; e) là tìm số nguyên nhỏ nhất k sao cho

Σ(j=k+1 to rank(Wt)) (σj)^2 ≤ e·Σ(i=1 to rank(Wt)) (σi)^2;                                                      (3)

trong đó e ∈ (0,1) là một siêu tham số được định nghĩa trước của tỷ lệ bảo toàn năng lượng. Sau khi cắt cụt n-k giá trị đơn cuối cùng, chúng tôi biến đổi ma trận 2D hạng thấp trở lại tensor 4D.

2.3 Điều chuẩn hóa Chuẩn Hạt nhân
Chuẩn hạt nhân được sử dụng rộng rãi trong các bài toán hoàn thành ma trận. Gần đây, nó được giới thiệu để ràng buộc mạng vào không gian hạng thấp trong quá trình huấn luyện [1].

min(f(x;w) + λΣ(l=1 to L)||Wl||*)                                                                                 (4)

trong đó f(·) là hàm mục tiêu tổn thất, chuẩn hạt nhân ||Wl||* được định nghĩa là ||Wl||* = Σ(i=1 to rank(Wl)) σi^l, với σi^l là các giá trị đơn của Wl. λ là một siêu tham số thiết lập ảnh hưởng của chuẩn hạt nhân.

Trong bài báo này, chúng tôi sử dụng gradient descent ngẫu nhiên con [2] để tối ưu hóa điều chuẩn hóa chuẩn hạt nhân trong quá trình huấn luyện. Cho W = UV^T là SVD của W và cho Utru, Vtru là U, V được cắt cụt đến rank(W) cột hoặc hàng đầu tiên, thì UtruVtru^T là gradient con của ||W||* [16]. Do đó, gradient con của Phương trình (4) trong một lớp là

∇f + λUtruVtru^T.                                                                                                  (5)

Chuẩn hạt nhân và hàm mục tiêu được tối ưu hóa đồng thời trong quá trình huấn luyện mạng và có thể được kết hợp thêm với TRP đề xuất.

3 Thí nghiệm
3.1 Chi tiết Triển khai
Chúng tôi đánh giá hiệu suất của sơ đồ TRP trên hai bộ dữ liệu thông thường, CIFAR-10 [11] và ImageNet [5]. Chúng tôi triển khai sơ đồ TRP với GPU NVIDIA 1080 Ti. Đối với huấn luyện trên CIFAR-10, chúng tôi

--- TRANG 3 ---
bắt đầu với tỷ lệ học cơ bản là 0.1 để huấn luyện 164 epoch và giảm giá trị theo hệ số 10 tại epoch thứ 82 và thứ 122. Đối với ImageNet, chúng tôi trực tiếp tinh chỉnh mô hình với sơ đồ TRP từ baseline được huấn luyện trước với tỷ lệ học 0.0001 cho 10 epoch. Đối với cả hai bộ dữ liệu, chúng tôi áp dụng solver SGD để cập nhật trọng số và đặt giá trị phân rã trọng số là 10^(-4) và giá trị momentum là 0.9.

3.2 Kết quả trên CIFAR-10
Như được thể hiện trong Bảng 2, đối với cả phân tích theo không gian (TRP1) và theo kênh (TRP2), TRP đề xuất vượt trội hơn các phương pháp cơ bản [18,10] trên ResNet-20 và ResNet-56. Kết quả trở nên tốt hơn khi sử dụng điều chuẩn hóa hạt nhân. Ví dụ, trong phân tích theo kênh (TRP2) của ResNet-56, kết quả của TRP kết hợp với điều chuẩn hóa hạt nhân thậm chí có thể đạt được tỷ lệ tăng tốc gấp 2 lần so với [18] với cùng mức giảm độ chính xác. Phương pháp của chúng tôi cũng vượt trội hơn cắt tỉa bộ lọc [12] và cắt tỉa kênh [9]. Ví dụ, ResNet-56 được huấn luyện TRP phân tích kênh có thể đạt được độ chính xác 92.77% với tăng tốc 2.31×, trong khi [9] là 91.80% và [12] là 91.60%. Với sự giúp đỡ của điều chuẩn hóa hạt nhân, các phương pháp của chúng tôi có thể đạt được tỷ lệ tăng tốc gấp 2 lần so với [9] và [12] với độ chính xác cao hơn.

[THIS IS TABLE: Table 2 showing experiment results on CIFAR-10 with Model names, Top 1 accuracy percentages, and Speed up ratios for various ResNet configurations]

[THIS IS TABLE: Table 3 showing results of ResNet-18 on ImageNet with Method names, Top1 accuracy percentages, and Speed up ratios]

[THIS IS TABLE: Table 4 showing results of ResNet-50 on ImageNet with Method names, Top1 accuracy percentages, and Speed up ratios]

3.3 Kết quả trên ImageNet
Kết quả trên ImageNet được thể hiện trong Bảng 3 và Bảng 4. Đối với ResNet-18, phương pháp của chúng tôi vượt trội hơn các phương pháp cơ bản [18,10]. Ví dụ, trong phân tích theo kênh, TRP đạt được tỷ lệ tăng tốc 1.81× với độ chính xác Top5 86.48% trên ImageNet, vượt trội hơn cả phương pháp dựa trên dữ liệu [18] và phương pháp độc lập dữ liệu [18] với biên độ lớn. Điều chuẩn hóa hạt nhân có thể tăng tỷ lệ tăng tốc với cùng độ chính xác.

Đối với ResNet-50, để xác thực tốt hơn hiệu quả của phương pháp chúng tôi, chúng tôi cũng so sánh TRP đề xuất với [9] và [13]. Với tăng tốc 1.80×, ResNet-50 được phân tích của chúng tôi có thể đạt được độ chính xác Top1 73.97% và Top5 91.98%, cao hơn nhiều so với [13]. TRP đạt được tăng tốc 2.23× cao hơn [9] với cùng mức giảm Top5.

4 Kết luận
Trong bài báo này, chúng tôi đề xuất một sơ đồ mới Cắt Tỉa Hạng Được Huấn Luyện (TRP) để huấn luyện mạng hạng thấp. Nó tận dụng dung lượng và cấu trúc của mạng gốc bằng cách nhúng xấp xỉ hạng thấp vào quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất điều chuẩn hóa chuẩn hạt nhân được tối ưu hóa bằng gradient descent ngẫu nhiên con để tăng cường TRP. TRP đề xuất có thể được kết hợp với bất kỳ phương pháp phân tích hạng thấp nào. Trên các bộ dữ liệu CIFAR-10 và ImageNet, chúng tôi đã chứng minh rằng các phương pháp của chúng tôi có thể vượt trội hơn các phương pháp cơ bản trong cả phân tích theo kênh và theo không gian.

--- TRANG 4 ---
Tài liệu tham khảo
[1] J. M. Alvarez và M. Salzmann. Compression-aware training of deep networks. Trong NIPS, 2017.
[2] H. Avron, S. Kale, S. P. Kasiviswanathan, và V. Sindhwani. Efficient and practical stochastic subgradient descent for nuclear norm regularization. Trong ICML, 2012.
[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, và A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 40:834–848, 2018.
[4] W. Chen, J. Wilson, S. Tyree, K. Weinberger, và Y. Chen. Compressing neural networks with the hashing trick. Trong ICML, 2015.
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009.
[6] E. Denton, W. Zaremba, J. Bruna, Y. Lecun, và R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. Trong NIPS, 2014.
[7] J. Guo, Y. Li, W. Lin, Y. Chen, và J. Li. Network decoupling: From regular to depthwise separable convolutions. Trong BMVC, 2018.
[8] K. He, X. Zhang, S. Ren, và J. Sun. Deep residual learning for image recognition. 2016.
[9] Y. He, X. Zhang, và J. Sun. Channel pruning for accelerating very deep neural networks. Trong ICCV, 2017.
[10] M. Jaderberg, A. Vedaldi, và A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
[11] A. Krizhevsky và G. Hinton. Learning multiple layers of features from tiny images. Computer Science, 2009.
[12] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
[13] J.-H. Luo, J. Wu, và W. Lin. Thinet: A filter level pruning method for deep neural network compression. ICCV, 2017.
[14] J.-H. Luo, H. Zhang, H.-Y. Zhou, C.-W. Xie, J. Wu, và W. Lin. Thinet: pruning cnn filters for a thinner net. TPAMI, 2018.
[15] S. Ren, K. He, R. B. Girshick, và J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI, 39:1137–1149, 2015.
[16] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear algebra and its applications, 170:33–45, 1992.
[17] W. Wen, C. Xu, C. Wu, Y. Wang, Y. Chen, và H. Li. Coordinating filters for faster deep neural networks. Trong ICCV, 2017.
[18] X. Zhang, J. Zou, K. He, và J. Sun. Accelerating very deep convolutional networks for classification and detection. TPAMI, 38(10):1943–1955, 2016.
