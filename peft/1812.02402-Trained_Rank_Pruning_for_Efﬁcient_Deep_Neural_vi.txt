# 1812.02402.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/peft/1812.02402.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 576259 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n cho Máº¡ng NÆ¡-ron SÃ¢u
Hiá»‡u Quáº£
Yuhui Xu1, Yuxi Li1, Shuai Zhang2, Wei Wen3, Botao Wang2, Wenrui Dai1, Yingyong Qi2, Yiran
Chen3, Weiyao Lin1vÃ  Hongkai Xiong1
1Äáº¡i há»c Jiao Tong ThÆ°á»£ng Háº£i, Email: {yuhuixu, lyxok1, daiwenrui, wylin,
xionghongkai}@sjtu.edu.cn
2NghiÃªn cá»©u AI Qualcomm, Email: {shuazhan, botaow, yingyong}@qti.qualcomm.com
3Äáº¡i há»c Duke, Email: {wei.wen, yiran.chen}@duke.edu
TÃ³m táº¯t
Äá»ƒ tÄƒng tá»‘c suy luáº­n DNN, xáº¥p xá»‰ háº¡ng tháº¥p Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i nhá» vÃ o cÆ¡ sá»Ÿ lÃ½ thuyáº¿t vá»¯ng cháº¯c vÃ  triá»ƒn khai hiá»‡u quáº£. Má»™t sá»‘ cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ cá»‘ gáº¯ng trá»±c tiáº¿p xáº¥p xá»‰ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c báº±ng phÃ¢n tÃ­ch háº¡ng tháº¥p; tuy nhiÃªn, cÃ¡c lá»—i xáº¥p xá»‰ nhá» trong tham sá»‘ cÃ³ thá»ƒ lan truyá»n thÃ nh tá»•n tháº¥t dá»± Ä‘oÃ¡n lá»›n. RÃµ rÃ ng, viá»‡c tÃ¡ch biá»‡t xáº¥p xá»‰ háº¡ng tháº¥p khá»i quÃ¡ trÃ¬nh huáº¥n luyá»‡n lÃ  khÃ´ng tá»‘i Æ°u. KhÃ¡c vá»›i cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c, bÃ i bÃ¡o nÃ y tÃ­ch há»£p xáº¥p xá»‰ háº¡ng tháº¥p vÃ  Ä‘iá»u chuáº©n hÃ³a vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n. ChÃºng tÃ´i Ä‘á» xuáº¥t Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n (TRP), luÃ¢n phiÃªn giá»¯a xáº¥p xá»‰ háº¡ng tháº¥p vÃ  huáº¥n luyá»‡n. TRP duy trÃ¬ kháº£ nÄƒng cá»§a máº¡ng gá»‘c trong khi Ã¡p Ä‘áº·t rÃ ng buá»™c háº¡ng tháº¥p trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Má»™t Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng gradient descent ngáº«u nhiÃªn con Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÃºc Ä‘áº©y thÃªm háº¡ng tháº¥p trong TRP. Máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i TRP cÃ³ cáº¥u trÃºc háº¡ng tháº¥p tá»± nhiÃªn, vÃ  Ä‘Æ°á»£c xáº¥p xá»‰ vá»›i tá»•n tháº¥t hiá»‡u suáº¥t khÃ´ng Ä‘Ã¡ng ká»ƒ, do Ä‘Ã³ loáº¡i bá» viá»‡c tinh chá»‰nh sau xáº¥p xá»‰ háº¡ng tháº¥p. PhÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n trÃªn CIFAR-10 vÃ  ImageNet, vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ©n trÆ°á»›c Ä‘Ã¢y sá»­ dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p. MÃ£ nguá»“n cá»§a chÃºng tÃ´i cÃ³ sáºµn táº¡i: https://github.com/yuhuixu1993/Trained-Rank-Pruning.

1 Giá»›i thiá»‡u
Máº¡ng NÆ¡-ron SÃ¢u (DNN) Ä‘Ã£ thá»ƒ hiá»‡n thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ trong nhiá»u tÃ¡c vá»¥ thá»‹ giÃ¡c mÃ¡y tÃ­nh nhÆ° phÃ¢n loáº¡i hÃ¬nh áº£nh [8], phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng [15] vÃ  phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a [3]. Máº·c dÃ¹ cÃ³ hiá»‡u suáº¥t cao trong cÃ¡c DNN lá»›n Ä‘Æ°á»£c há»— trá»£ bá»Ÿi pháº§n cá»©ng tÃ­nh toÃ¡n song song tiÃªn tiáº¿n, háº§u háº¿t cÃ¡c kiáº¿n trÃºc máº¡ng tiÃªn tiáº¿n khÃ´ng phÃ¹ há»£p cho viá»‡c sá»­ dá»¥ng háº¡n cháº¿ tÃ i nguyÃªn nhÆ° sá»­ dá»¥ng trÃªn thiáº¿t bá»‹ luÃ´n báº­t, thiáº¿t bá»‹ táº§m tháº¥p cháº¡y báº±ng pin, do nhá»¯ng háº¡n cháº¿ vá» kháº£ nÄƒng tÃ­nh toÃ¡n, bá»™ nhá»› vÃ  nÄƒng lÆ°á»£ng.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch háº¡ng tháº¥p [6,10,7,17,1] Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a tÃ­nh dÆ° thá»«a theo kÃªnh vÃ  khÃ´ng gian báº±ng cÃ¡ch phÃ¢n tÃ­ch máº¡ng gá»‘c thÃ nh má»™t máº¡ng compact vá»›i cÃ¡c lá»›p háº¡ng tháº¥p. KhÃ¡c vá»›i cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c, bÃ i bÃ¡o nÃ y Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i Ä‘á»ƒ thiáº¿t káº¿ máº¡ng háº¡ng tháº¥p.

Máº¡ng háº¡ng tháº¥p cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trá»±c tiáº¿p tá»« Ä‘áº§u. Tuy nhiÃªn, ráº¥t khÃ³ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ thá»a mÃ£n vÃ¬ má»™t sá»‘ lÃ½ do. (1) Dung lÆ°á»£ng tháº¥p: So vá»›i máº¡ng háº¡ng Ä‘áº§y Ä‘á»§ gá»‘c, dung lÆ°á»£ng cá»§a máº¡ng háº¡ng tháº¥p bá»‹ háº¡n cháº¿, Ä‘iá»u nÃ y gÃ¢y khÃ³ khÄƒn trong viá»‡c tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t. (2) Cáº¥u trÃºc sÃ¢u: PhÃ¢n tÃ­ch háº¡ng tháº¥p thÆ°á»ng lÃ m tÄƒng gáº¥p Ä‘Ã´i sá»‘ lÆ°á»£ng lá»›p trong máº¡ng. CÃ¡c lá»›p bá»• sung lÃ m cho tá»‘i Æ°u hÃ³a sá»‘ trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n nhiá»u do gradient bÃ¹ng ná»• vÃ /hoáº·c biáº¿n máº¥t. (3) Lá»±a chá»n háº¡ng: Háº¡ng cá»§a máº¡ng Ä‘Æ°á»£c phÃ¢n tÃ­ch thÆ°á»ng Ä‘Æ°á»£c chá»n lÃ m siÃªu tham sá»‘ dá»±a trÃªn cÃ¡c máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c; cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  háº¡ng tá»‘i Æ°u cho máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u.

Hoáº·c, má»™t sá»‘ cÃ´ng trÃ¬nh trÆ°á»›c [18,7,10] Ä‘Ã£ cá»‘ gáº¯ng phÃ¢n tÃ­ch cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c máº¡ng háº¡ng tháº¥p ban Ä‘áº§u. Tuy nhiÃªn, háº¡ng tháº¥p Ä‘Æ°á»£c Ã¡p Ä‘áº·t má»™t cÃ¡ch heuristic cÃ³ thá»ƒ gÃ¢y ra tá»•n tháº¥t Ä‘á»™ chÃ­nh xÃ¡c lá»›n vÃ  cáº§n huáº¥n luyá»‡n láº¡i máº¡ng Ä‘á»ƒ phá»¥c há»“i hiá»‡u suáº¥t cá»§a máº¡ng gá»‘c nhiá»u nháº¥t cÃ³ thá»ƒ. Má»™t sá»‘ ná»— lá»±c Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»ƒ sá»­ dá»¥ng Ä‘iá»u chuáº©n hÃ³a thÆ°a thá»›t [17,4] Ä‘á»ƒ rÃ ng buá»™c máº¡ng vÃ o khÃ´ng gian háº¡ng tháº¥p. Máº·c dÃ¹ Ä‘iá»u chuáº©n hÃ³a thÆ°a thá»›t lÃ m giáº£m lá»—i gÃ¢y ra bá»Ÿi phÃ¢n tÃ­ch Ä‘áº¿n má»™t má»©c Ä‘á»™ nÃ o Ä‘Ã³, hiá»‡u suáº¥t váº«n giáº£m nhanh chÃ³ng khi tá»· lá»‡ nÃ©n tÄƒng.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i, Ä‘Æ°á»£c gá»i lÃ  Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n (TRP), Ä‘á»ƒ huáº¥n luyá»‡n máº¡ng háº¡ng tháº¥p. ChÃºng tÃ´i nhÃºng phÃ¢n tÃ­ch háº¡ng tháº¥p vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng cÃ¡ch tá»« tá»« Ä‘áº©y phÃ¢n bá»‘ trá»ng sá»‘ cá»§a má»™t máº¡ng hoáº¡t Ä‘á»™ng tá»‘t vÃ o dáº¡ng háº¡ng tháº¥p, trong Ä‘Ã³ táº¥t cáº£ cÃ¡c tham sá»‘ cá»§a máº¡ng gá»‘c Ä‘Æ°á»£c giá»¯ láº¡i vÃ  tá»‘i Æ°u hÃ³a Ä‘á»ƒ duy trÃ¬ dung lÆ°á»£ng cá»§a nÃ³. ChÃºng tÃ´i cÅ©ng Ä‘á» xuáº¥t má»™t Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n gradient descent ngáº«u nhiÃªn con Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ rÃ ng buá»™c thÃªm cÃ¡c trá»ng sá»‘ trong khÃ´ng gian háº¡ng tháº¥p nháº±m tÄƒng cÆ°á»ng TRP. Giáº£i phÃ¡p Ä‘á» xuáº¥t Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1.

NhÃ¬n chung, cÃ¡c Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t dÆ°á»›i Ä‘Ã¢y.
1. Má»™t phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n má»›i gá»i lÃ  TRP Ä‘Æ°á»£c trÃ¬nh bÃ y báº±ng cÃ¡ch nhÃºng rÃµ rÃ ng phÃ¢n tÃ­ch háº¡ng tháº¥p vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng;
2. Má»™t Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng gradient descent ngáº«u nhiÃªn con Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t cá»§a TRP;
3. Cáº£i thiá»‡n tÄƒng tá»‘c suy luáº­n vÃ  giáº£m tá»•n tháº¥t Ä‘á»™ chÃ­nh xÃ¡c xáº¥p xá»‰ trong cáº£ phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch theo kÃªnh vÃ  theo khÃ´ng gian.

ğ‘˜ğ‘¤ğ‘˜â„ğ¶ğ‘˜ğ‘¤ğ‘˜â„ğ¶
ğ‘˜ğ‘¤ğ‘˜â„ğ¶Luá»“ng Ä‘áº·c trÆ°ng
Luá»“ng gradient
Xáº¥p xá»‰ háº¡ng tháº¥p
Thay tháº¿
â€¦â€¦
â€¦ğ‘Šğ‘¡
ğ‘‡ğ‘§
ğ‘Šğ‘¡
(a) (b)

HÃ¬nh 1: QuÃ¡ trÃ¬nh huáº¥n luyá»‡n TRP bao gá»“m hai pháº§n nhÆ° Ä‘Æ°á»£c minh há»a trong (a) vÃ  (b). (a) Má»™t vÃ²ng láº·p bÃ¬nh thÆ°á»ng vá»›i lan truyá»n tá»›i-lÃ¹i vÃ  cáº­p nháº­t trá»ng sá»‘. (b) Má»™t vÃ²ng láº·p huáº¥n luyá»‡n Ä‘Æ°á»£c chÃ¨n bá»Ÿi cáº¯t tá»‰a háº¡ng, trong Ä‘Ã³ xáº¥p xá»‰ háº¡ng tháº¥p Ä‘Æ°á»£c Ã¡p dá»¥ng trÆ°á»›c trÃªn cÃ¡c bá»™ lá»c hiá»‡n táº¡i trÆ°á»›c khi tÃ­ch cháº­p. Trong quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c, cÃ¡c gradient Ä‘Æ°á»£c cá»™ng trá»±c tiáº¿p vÃ o cÃ¡c bá»™ lá»c háº¡ng tháº¥p vÃ  cÃ¡c trá»ng sá»‘ gá»‘c Ä‘Æ°á»£c thay tháº¿ bá»Ÿi cÃ¡c bá»™ lá»c háº¡ng tháº¥p Ä‘Ã£ cáº­p nháº­t. (b) Ä‘Æ°á»£c Ã¡p dá»¥ng má»™t láº§n má»—i m vÃ²ng láº·p (tá»©c lÃ  khi vÃ²ng láº·p cáº­p nháº­t gradient t=zm; z = 0;1;2; ...), ngÆ°á»£c láº¡i (a) Ä‘Æ°á»£c Ã¡p dá»¥ng.

2 PhÆ°Æ¡ng phÃ¡p
2.1 Kiáº¿n thá»©c cÆ¡ báº£n
Má»™t cÃ¡ch chÃ­nh thá»©c, cÃ¡c bá»™ lá»c tÃ­ch cháº­p trong má»™t lá»›p cÃ³ thá»ƒ Ä‘Æ°á»£c kÃ½ hiá»‡u báº±ng má»™t tensor WâˆˆR^(nÃ—cÃ—kwÃ—kh), trong Ä‘Ã³ n vÃ  c lÃ  sá»‘ lÆ°á»£ng bá»™ lá»c vÃ  kÃªnh Ä‘áº§u vÃ o, kh vÃ  kw lÃ  chiá»u cao vÃ  chiá»u rá»™ng cá»§a cÃ¡c bá»™ lá»c. Má»™t Ä‘áº§u vÃ o cá»§a lá»›p tÃ­ch cháº­p FiâˆˆR^(cÃ—xÃ—y) táº¡o ra má»™t Ä‘áº§u ra lÃ  Fo=WâŠ›Fi. TÆ°Æ¡ng quan theo kÃªnh [18] vÃ  tÆ°Æ¡ng quan theo khÃ´ng gian [10] Ä‘Æ°á»£c khÃ¡m phÃ¡ Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c bá»™ lá»c tÃ­ch cháº­p trong khÃ´ng gian háº¡ng tháº¥p. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o hai sÆ¡ Ä‘á»“ phÃ¢n tÃ­ch nÃ y. Tuy nhiÃªn, khÃ¡c vá»›i cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t sÆ¡ Ä‘á»“ huáº¥n luyá»‡n má»›i TRP Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c máº¡ng háº¡ng tháº¥p mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i sau khi phÃ¢n tÃ­ch.

--- TRANG 2 ---
2.2 Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n
ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t sÆ¡ Ä‘á»“ huáº¥n luyá»‡n Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ gá»i lÃ  Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n (TRP) theo cÃ¡ch Ä‘á»‹nh ká»³:

Wt+1 = {
    Wt - Î±âˆ‡f(Wt) if t%m â‰  0
    Tz - Î±âˆ‡f(Tz) if t%m = 0
    Tz = D(Wt); z = t/m
}                                                                                                                (1)

trong Ä‘Ã³ D(Â·) lÃ  má»™t toÃ¡n tá»­ xáº¥p xá»‰ tensor háº¡ng tháº¥p, Î± lÃ  tá»· lá»‡ há»c, t chá»‰ sá»‘ vÃ²ng láº·p vÃ  z lÃ  vÃ²ng láº·p cá»§a toÃ¡n tá»­ D, vá»›i m lÃ  chu ká»³ cho xáº¥p xá»‰ háº¡ng tháº¥p. ChÃºng tÃ´i Ã¡p dá»¥ng xáº¥p xá»‰ háº¡ng tháº¥p má»—i m vÃ²ng láº·p SGD. Äiá»u nÃ y tiáº¿t kiá»‡m thá»i gian huáº¥n luyá»‡n Ä‘Ã¡ng ká»ƒ. NhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1, má»—i m vÃ²ng láº·p, chÃºng tÃ´i thá»±c hiá»‡n xáº¥p xá»‰ háº¡ng tháº¥p trÃªn cÃ¡c bá»™ lá»c gá»‘c, trong khi cÃ¡c gradient Ä‘Æ°á»£c cáº­p nháº­t trÃªn dáº¡ng háº¡ng tháº¥p káº¿t quáº£. NgÆ°á»£c láº¡i, máº¡ng Ä‘Æ°á»£c cáº­p nháº­t thÃ´ng qua SGD bÃ¬nh thÆ°á»ng. SÆ¡ Ä‘á»“ huáº¥n luyá»‡n cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i cÃ¡c toÃ¡n tá»­ háº¡ng tháº¥p tÃ¹y Ã½. Trong cÃ´ng trÃ¬nh Ä‘á» xuáº¥t, chÃºng tÃ´i chá»n cÃ¡c ká»¹ thuáº­t háº¡ng tháº¥p Ä‘Æ°á»£c Ä‘á» xuáº¥t trong [10] vÃ  [18], cáº£ hai Ä‘á»u biáº¿n Ä‘á»•i cÃ¡c bá»™ lá»c 4 chiá»u thÃ nh ma tráº­n 2D vÃ  sau Ä‘Ã³ Ã¡p dá»¥ng phÃ¢n tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n cáº¯t cá»¥t (TSVD). SVD cá»§a ma tráº­n Wt cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t lÃ :

Wt = Î£(i=1 to rank(Wt)) ÏƒiUi(Vi)^T;                                                                                (2)

trong Ä‘Ã³ Ïƒi lÃ  giÃ¡ trá»‹ Ä‘Æ¡n cá»§a Wt vá»›i Ïƒ1 â‰¥ Ïƒ2 â‰¥ ... â‰¥ Ïƒrank(Wt), vÃ  Ui vÃ  Vi lÃ  cÃ¡c vector Ä‘Æ¡n. TSVD Ä‘Æ°á»£c tham sá»‘ hÃ³a (Wt; e) lÃ  tÃ¬m sá»‘ nguyÃªn nhá» nháº¥t k sao cho

Î£(j=k+1 to rank(Wt)) (Ïƒj)^2 â‰¤ eÂ·Î£(i=1 to rank(Wt)) (Ïƒi)^2;                                                      (3)

trong Ä‘Ã³ e âˆˆ (0,1) lÃ  má»™t siÃªu tham sá»‘ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c cá»§a tá»· lá»‡ báº£o toÃ n nÄƒng lÆ°á»£ng. Sau khi cáº¯t cá»¥t n-k giÃ¡ trá»‹ Ä‘Æ¡n cuá»‘i cÃ¹ng, chÃºng tÃ´i biáº¿n Ä‘á»•i ma tráº­n 2D háº¡ng tháº¥p trá»Ÿ láº¡i tensor 4D.

2.3 Äiá»u chuáº©n hÃ³a Chuáº©n Háº¡t nhÃ¢n
Chuáº©n háº¡t nhÃ¢n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c bÃ i toÃ¡n hoÃ n thÃ nh ma tráº­n. Gáº§n Ä‘Ã¢y, nÃ³ Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘á»ƒ rÃ ng buá»™c máº¡ng vÃ o khÃ´ng gian háº¡ng tháº¥p trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n [1].

min(f(x;w) + Î»Î£(l=1 to L)||Wl||*)                                                                                 (4)

trong Ä‘Ã³ f(Â·) lÃ  hÃ m má»¥c tiÃªu tá»•n tháº¥t, chuáº©n háº¡t nhÃ¢n ||Wl||* Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  ||Wl||* = Î£(i=1 to rank(Wl)) Ïƒi^l, vá»›i Ïƒi^l lÃ  cÃ¡c giÃ¡ trá»‹ Ä‘Æ¡n cá»§a Wl. Î» lÃ  má»™t siÃªu tham sá»‘ thiáº¿t láº­p áº£nh hÆ°á»Ÿng cá»§a chuáº©n háº¡t nhÃ¢n.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i sá»­ dá»¥ng gradient descent ngáº«u nhiÃªn con [2] Ä‘á»ƒ tá»‘i Æ°u hÃ³a Ä‘iá»u chuáº©n hÃ³a chuáº©n háº¡t nhÃ¢n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Cho W = UV^T lÃ  SVD cá»§a W vÃ  cho Utru, Vtru lÃ  U, V Ä‘Æ°á»£c cáº¯t cá»¥t Ä‘áº¿n rank(W) cá»™t hoáº·c hÃ ng Ä‘áº§u tiÃªn, thÃ¬ UtruVtru^T lÃ  gradient con cá»§a ||W||* [16]. Do Ä‘Ã³, gradient con cá»§a PhÆ°Æ¡ng trÃ¬nh (4) trong má»™t lá»›p lÃ 

âˆ‡f + Î»UtruVtru^T.                                                                                                  (5)

Chuáº©n háº¡t nhÃ¢n vÃ  hÃ m má»¥c tiÃªu Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»“ng thá»i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p thÃªm vá»›i TRP Ä‘á» xuáº¥t.

3 ThÃ­ nghiá»‡m
3.1 Chi tiáº¿t Triá»ƒn khai
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a sÆ¡ Ä‘á»“ TRP trÃªn hai bá»™ dá»¯ liá»‡u thÃ´ng thÆ°á»ng, CIFAR-10 [11] vÃ  ImageNet [5]. ChÃºng tÃ´i triá»ƒn khai sÆ¡ Ä‘á»“ TRP vá»›i GPU NVIDIA 1080 Ti. Äá»‘i vá»›i huáº¥n luyá»‡n trÃªn CIFAR-10, chÃºng tÃ´i

--- TRANG 3 ---
báº¯t Ä‘áº§u vá»›i tá»· lá»‡ há»c cÆ¡ báº£n lÃ  0.1 Ä‘á»ƒ huáº¥n luyá»‡n 164 epoch vÃ  giáº£m giÃ¡ trá»‹ theo há»‡ sá»‘ 10 táº¡i epoch thá»© 82 vÃ  thá»© 122. Äá»‘i vá»›i ImageNet, chÃºng tÃ´i trá»±c tiáº¿p tinh chá»‰nh mÃ´ hÃ¬nh vá»›i sÆ¡ Ä‘á»“ TRP tá»« baseline Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i tá»· lá»‡ há»c 0.0001 cho 10 epoch. Äá»‘i vá»›i cáº£ hai bá»™ dá»¯ liá»‡u, chÃºng tÃ´i Ã¡p dá»¥ng solver SGD Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ vÃ  Ä‘áº·t giÃ¡ trá»‹ phÃ¢n rÃ£ trá»ng sá»‘ lÃ  10^(-4) vÃ  giÃ¡ trá»‹ momentum lÃ  0.9.

3.2 Káº¿t quáº£ trÃªn CIFAR-10
NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 2, Ä‘á»‘i vá»›i cáº£ phÃ¢n tÃ­ch theo khÃ´ng gian (TRP1) vÃ  theo kÃªnh (TRP2), TRP Ä‘á» xuáº¥t vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n [18,10] trÃªn ResNet-20 vÃ  ResNet-56. Káº¿t quáº£ trá»Ÿ nÃªn tá»‘t hÆ¡n khi sá»­ dá»¥ng Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n. VÃ­ dá»¥, trong phÃ¢n tÃ­ch theo kÃªnh (TRP2) cá»§a ResNet-56, káº¿t quáº£ cá»§a TRP káº¿t há»£p vá»›i Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n tháº­m chÃ­ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ tÄƒng tá»‘c gáº¥p 2 láº§n so vá»›i [18] vá»›i cÃ¹ng má»©c giáº£m Ä‘á»™ chÃ­nh xÃ¡c. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÅ©ng vÆ°á»£t trá»™i hÆ¡n cáº¯t tá»‰a bá»™ lá»c [12] vÃ  cáº¯t tá»‰a kÃªnh [9]. VÃ­ dá»¥, ResNet-56 Ä‘Æ°á»£c huáº¥n luyá»‡n TRP phÃ¢n tÃ­ch kÃªnh cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c 92.77% vá»›i tÄƒng tá»‘c 2.31Ã—, trong khi [9] lÃ  91.80% vÃ  [12] lÃ  91.60%. Vá»›i sá»± giÃºp Ä‘á»¡ cá»§a Ä‘iá»u chuáº©n hÃ³a háº¡t nhÃ¢n, cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ tÄƒng tá»‘c gáº¥p 2 láº§n so vá»›i [9] vÃ  [12] vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n.

[THIS IS TABLE: Table 2 showing experiment results on CIFAR-10 with Model names, Top 1 accuracy percentages, and Speed up ratios for various ResNet configurations]

[THIS IS TABLE: Table 3 showing results of ResNet-18 on ImageNet with Method names, Top1 accuracy percentages, and Speed up ratios]

[THIS IS TABLE: Table 4 showing results of ResNet-50 on ImageNet with Method names, Top1 accuracy percentages, and Speed up ratios]

3.3 Káº¿t quáº£ trÃªn ImageNet
Káº¿t quáº£ trÃªn ImageNet Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 3 vÃ  Báº£ng 4. Äá»‘i vá»›i ResNet-18, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n [18,10]. VÃ­ dá»¥, trong phÃ¢n tÃ­ch theo kÃªnh, TRP Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ tÄƒng tá»‘c 1.81Ã— vá»›i Ä‘á»™ chÃ­nh xÃ¡c Top5 86.48% trÃªn ImageNet, vÆ°á»£t trá»™i hÆ¡n cáº£ phÆ°Æ¡ng phÃ¡p dá»±a trÃªn dá»¯ liá»‡u [18] vÃ  phÆ°Æ¡ng phÃ¡p Ä‘á»™c láº­p dá»¯ liá»‡u [18] vá»›i biÃªn Ä‘á»™ lá»›n. Äiá»u chuáº©n hÃ³a háº¡t nhÃ¢n cÃ³ thá»ƒ tÄƒng tá»· lá»‡ tÄƒng tá»‘c vá»›i cÃ¹ng Ä‘á»™ chÃ­nh xÃ¡c.

Äá»‘i vá»›i ResNet-50, Ä‘á»ƒ xÃ¡c thá»±c tá»‘t hÆ¡n hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i, chÃºng tÃ´i cÅ©ng so sÃ¡nh TRP Ä‘á» xuáº¥t vá»›i [9] vÃ  [13]. Vá»›i tÄƒng tá»‘c 1.80Ã—, ResNet-50 Ä‘Æ°á»£c phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c Top1 73.97% vÃ  Top5 91.98%, cao hÆ¡n nhiá»u so vá»›i [13]. TRP Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 2.23Ã— cao hÆ¡n [9] vá»›i cÃ¹ng má»©c giáº£m Top5.

4 Káº¿t luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t sÆ¡ Ä‘á»“ má»›i Cáº¯t Tá»‰a Háº¡ng ÄÆ°á»£c Huáº¥n Luyá»‡n (TRP) Ä‘á»ƒ huáº¥n luyá»‡n máº¡ng háº¡ng tháº¥p. NÃ³ táº­n dá»¥ng dung lÆ°á»£ng vÃ  cáº¥u trÃºc cá»§a máº¡ng gá»‘c báº±ng cÃ¡ch nhÃºng xáº¥p xá»‰ háº¡ng tháº¥p vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘á» xuáº¥t Ä‘iá»u chuáº©n hÃ³a chuáº©n háº¡t nhÃ¢n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng gradient descent ngáº«u nhiÃªn con Ä‘á»ƒ tÄƒng cÆ°á»ng TRP. TRP Ä‘á» xuáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i báº¥t ká»³ phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch háº¡ng tháº¥p nÃ o. TrÃªn cÃ¡c bá»™ dá»¯ liá»‡u CIFAR-10 vÃ  ImageNet, chÃºng tÃ´i Ä‘Ã£ chá»©ng minh ráº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n trong cáº£ phÃ¢n tÃ­ch theo kÃªnh vÃ  theo khÃ´ng gian.

--- TRANG 4 ---
TÃ i liá»‡u tham kháº£o
[1] J. M. Alvarez vÃ  M. Salzmann. Compression-aware training of deep networks. Trong NIPS, 2017.
[2] H. Avron, S. Kale, S. P. Kasiviswanathan, vÃ  V. Sindhwani. Efficient and practical stochastic subgradient descent for nuclear norm regularization. Trong ICML, 2012.
[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, vÃ  A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 40:834â€“848, 2018.
[4] W. Chen, J. Wilson, S. Tyree, K. Weinberger, vÃ  Y. Chen. Compressing neural networks with the hashing trick. Trong ICML, 2015.
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, vÃ  L. Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009.
[6] E. Denton, W. Zaremba, J. Bruna, Y. Lecun, vÃ  R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. Trong NIPS, 2014.
[7] J. Guo, Y. Li, W. Lin, Y. Chen, vÃ  J. Li. Network decoupling: From regular to depthwise separable convolutions. Trong BMVC, 2018.
[8] K. He, X. Zhang, S. Ren, vÃ  J. Sun. Deep residual learning for image recognition. 2016.
[9] Y. He, X. Zhang, vÃ  J. Sun. Channel pruning for accelerating very deep neural networks. Trong ICCV, 2017.
[10] M. Jaderberg, A. Vedaldi, vÃ  A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
[11] A. Krizhevsky vÃ  G. Hinton. Learning multiple layers of features from tiny images. Computer Science, 2009.
[12] H. Li, A. Kadav, I. Durdanovic, H. Samet, vÃ  H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
[13] J.-H. Luo, J. Wu, vÃ  W. Lin. Thinet: A filter level pruning method for deep neural network compression. ICCV, 2017.
[14] J.-H. Luo, H. Zhang, H.-Y. Zhou, C.-W. Xie, J. Wu, vÃ  W. Lin. Thinet: pruning cnn filters for a thinner net. TPAMI, 2018.
[15] S. Ren, K. He, R. B. Girshick, vÃ  J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI, 39:1137â€“1149, 2015.
[16] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear algebra and its applications, 170:33â€“45, 1992.
[17] W. Wen, C. Xu, C. Wu, Y. Wang, Y. Chen, vÃ  H. Li. Coordinating filters for faster deep neural networks. Trong ICCV, 2017.
[18] X. Zhang, J. Zou, K. He, vÃ  J. Sun. Accelerating very deep convolutional networks for classification and detection. TPAMI, 38(10):1943â€“1955, 2016.
