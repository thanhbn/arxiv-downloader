# 2303.16753.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.16753.pdf
# File size: 734280 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Scaling Pre-trained Language Models to Deeper
via Parameter-efﬁcient Architecture
Peiyu Liu1;3, Ze-Feng Gao1, Yushuo Chen1;3, Wayne Xin Zhao1;3y,and Ji-Rong Wen1;2;3
1Gaoling School of Artiﬁcial Intelligence, Renmin University of China
2School of Information, Renmin University of China
3Beijing Key Laboratory of Big Data Management and Analysis Methods
{liupeiyustu,zfgao,jrwen}@ruc.edu.cn,
batmanfly@gmail.com,chenyushuo1999@foxmail.com
Abstract
In this paper, we propose a highly parameter-
efﬁcient approach to scaling pre-trained lan-
guage models (PLMs) to a deeper model depth.
Unlike prior work that shares all parameters or
uses extra blocks, we design a more capable
parameter-sharing architecture based on ma-
trix product operator (MPO). MPO decompo-
sition can reorganize and factorize the infor-
mation of a parameter matrix into two parts:
the major part that contains the major infor-
mation ( central tensor ) and the supplementary
part that only has a small proportion of pa-
rameters ( auxiliary tensors ). Based on such
a decomposition, our architecture shares the
central tensor across all layers for reducing
the model size and meanwhile keeps layer-
speciﬁc auxiliary tensors (also using adapters)
for enhancing the adaptation ﬂexibility. To
improve the model training, we further pro-
pose a stable initialization algorithm tailored
for the MPO-based architecture. Extensive ex-
periments have demonstrated the effectiveness
of our proposed model in reducing the model
size and achieving highly competitive perfor-
mance.
1 Introduction
Recently, pre-trained language models (PLMs)
have achieved huge success in a variety of NLP
tasks by exploring ever larger model architec-
ture (Raffel et al., 2020; Radford et al., 2019). It
has been shown that there potentially exists a scal-
ing law between the model size and model capacity
for PLMs (Kaplan et al., 2020), attracting many ef-
forts to enhance the performance by scaling model
size (Chowdhery et al., 2022; Wang et al., 2022).
As a straightforward approach, we can directly
increase the layer number of Transformer networks
for improving the model capacity (Wang et al.,
2022; Huang et al., 2020). While, a very deep
Authors contributed equally.
yCorresponding author.
Depth
# Params
Number of Samples
Figure 1: A comparison of our model and representa-
tive PLMs in the three dimensions of model size ,model
depth , and performance (measured on GLUE score).
architecture typically corresponds to a signiﬁcantly
large model size, leading to high costs in both com-
putation and storage (Gong et al., 2019). And, it
is difﬁcult to deploy deep networks in resource-
limited settings, though it usually has a stronger
model capacity. Therefore, there is an urgent need
for developing a parameter-efﬁcient way for scal-
ing the model depth.
To reduce the parameters in deep networks,
weight sharing has proven to be very useful to de-
sign lightweight Transformer architectures (Zhang
et al., 2022; Lan et al., 2019). As a representa-
tive work by across-layer parameter sharing, AL-
BERT (Lan et al., 2019) keeps only ten percent
of the whole parameters of BERT while maintain-
ing comparable performance. Although the idea
of parameter sharing is simple yet (to some extent)
effective, it has been found that identical weights
across different layers are the main cause of perfor-
mance degradation (Zhang et al., 2022). To address
this issue, extra blocks are designed to elevate pa-
rameter diversity in each layer (Nouriborji et al.,
2022). While, they still use the rigid architecture
of shared layer weights, having a limited model ca-arXiv:2303.16753v2  [cs.CL]  11 Apr 2023

--- PAGE 2 ---
pacity. Besides, it is difﬁcult to optimize very deep
models, especially when shared components are in-
volved. Although recent studies (Wang et al., 2022;
Huang et al., 2020) propose improved initialization
methods, they do not consider the case with param-
eter sharing, thus likely leading to a suboptimal
performance on a parameter-sharing architecture.
To address these challenges, in this paper, we
propose a highly parameter-efﬁcient approach to
scaling PLMs to a deeper model architecture. As
the core contribution, we propose a matrix prod-
uct operator (MPO) based parameter-sharing ar-
chitecture for deep Transformer networks. Via
MPO decomposition, a parameter matrix can be
decomposed into central tensors (containing the
major information) and auxiliary tensors (contain-
ing the supplementary information). Our approach
shares the central tensors of the parameter matrices
across all layers for reducing the model size, and
meanwhile keeps layer-speciﬁc auxiliary tensors
for enhancing the adaptation ﬂexibility. In order to
train such a deep architecture, we propose an MPO-
based initialization method by utilizing the MPO
decomposition results of ALBERT. Further, for the
auxiliary tensors of higher layers (more than 24 lay-
ers in ALBERT), we propose to set the parameters
with scaling coefﬁcients derived from theoretical
analysis. We theoretically show it can address the
training instability regardless of the model depth.
Our work provides a novel parameter-sharing
way for scaling model depth, which can be gen-
erally applied to various Transformer based mod-
els. We conduct extensive experiments to evalu-
ate the performance of the proposed MPOBERT
model on the GLUE benchmark in comparison
to PLMs with varied model sizes (tiny, small and
large). Experiments results have demonstrated the
effectiveness of the proposed model in reducing the
model size and achieving competitive performance.
With fewer parameters than BERT BASE , we scale
the model depth by a factor of 4x and achieve 0.1
points higher than BERT LARGE for GLUE score.
2 Related Work
Matrix Product Operators . Matrix product op-
erators ( a.k.a. tensor-train operators (Oseledets,
2011)) were proposed for a more effective rep-
resentation of the linear structure of neural net-
works (Gao et al., 2020a), which was then used to
compress deep neural networks (Novikov et al.,
2015), convolutional neural networks (Garipovet al., 2016; Yu et al., 2017), and LSTM (Gao et al.,
2020b; Sun et al., 2020a). Based on MPO decom-
position, recent studies designed lightweight ﬁne-
tuning and compression methods for PLMs (Liu
et al., 2021), and developed parameter-efﬁcient
MoE architecture (Gao et al., 2022). Different from
these works, our work aims to develop a very deep
PLM with lightweight architecture and stable train-
ing.
Parameter-Efﬁcient PLMs . Existing efforts to
reduce the parameters of PLMs can be broadly
categorized into three major lines: knowledge dis-
tillation, model pruning, and parameter sharing.
For knowledge distillation-based methods (Sanh
et al., 2019; Sun et al., 2020b,b; Liu et al., 2020),
PLMs were distilled into student networks with
much fewer parameters. For pruning-based meth-
ods, they tried to remove less important compo-
nents (Michel et al., 2019; Wang et al., 2020) or
very small weights (Chen et al., 2020). Moreover,
the parameter-sharing method was further proposed
by sharing all parameters (Lan et al., 2019) or incor-
porating speciﬁc auxiliary components (Reid et al.,
2021; Nouriborji et al., 2022). Different from these
works, we design an MPO-based architecture that
can reduce the model size and enable adaptation
ﬂexibility, by decomposing the original matrix.
Optimization for Deep Models . Although it is
simple to increase the number of layers for scal-
ing model size, it is difﬁcult to optimize very deep
networks due to the training instability issue. Sev-
eral studies have proposed different strategies to
overcome this difﬁculty for training deep Trans-
former networks, including Fixup (Zhang et al.,
2019) by properly rescaling standard initialization,
T-Fixup (Huang et al., 2020) by proposing a weight
initialization scheme, and DeepNorm (Wang et al.,
2022) by introducing new normalization function.
As a comparison, we study how to optimize the
deep MPO-based architecture with the parameter
sharing strategy, and explore the use of well-trained
PLMs for initialization, which has a different focus
from existing work.
3 Method
In this section, we describe the proposed
MPOBERT approach for building deep PLMs via
a highly parameter-efﬁcient architecture. Our ap-
proach follows the classic weight sharing paradigm,
while introducing a principled mechanism for shar-

--- PAGE 3 ---
ing informative parameters across layers and also
enabling layer-speciﬁc weight adaptation.
3.1 Overview of Our Approach
Although weight sharing has been widely explored
for building compact PLMs (Lan et al., 2019), ex-
isting studies either share all the parameters across
layers (Lan et al., 2019) or incorporate additional
blocks to facilitate the sharing (Zhang et al., 2022;
Nouriborji et al., 2022). They either have limited
model capacity with a rigid architecture or require
additional efforts for maintenance.
Considering the above issues, we motivate our
approach in two aspects. Firstly, only informative
parameters should be shared across layers, instead
of all the parameters. Second, it should not affect
the capacity to capture layer-speciﬁc variations. To
achieve this, we utilize the MPO decomposition
from multi-body physics (Gao et al., 2020a) to de-
velop a parameter-efﬁcient architecture by sharing
informative components across layers and keep-
ing layer-speciﬁc supplementary components (Sec-
tion 3.2). As another potential issue, it is difﬁ-
cult to optimize deep PLMs due to unstable train-
ing (Wang et al., 2022), especially when weight
sharing (Lan et al., 2019) is involved. We further
propose a simple yet effective method to stabilize
the training of MPOBERT (Section 3.3). Next, we
introduce the technical details of our approach.
3.2 MPO-based Transformer Layer
In this section, we ﬁrst introduce the MPO decom-
position and introduce how to utilize it for building
parameter-efﬁcient deep PLMs.
3.2.1 MPO Decomposition
Given a weight matrix W2RIJ, MPO decompo-
sition (Gao et al., 2020a) can decompose a matrix
into a product of ntensors by reshaping the two
dimension sizes IandJ:
Wi1;:::;i n;j1;:::;j n=T(1)[i1;j1]T(n)[in;jn];(1)
where we have I=Qn
k=1ik,J=Qn
k=1jk,
andT(k)[ik;jk]is a 4-dimensional tensor with size
dk 1ikjkdkin whichdkis a bond dimen-
sion linking T(k)andT(k+1)withd0=dn= 1.
For simplicity, we omit the bond dimensions in
Eq.(1). Whennis odd, the middle tensor con-
tains the most parameters (with the largest bond
dimensions), while the parameter sizes of the rest
decrease with the increasing distance to the middle
tensor. Following (Liu et al., 2021), we further
𝑾𝑨𝒅𝒂𝒑𝒕𝒆𝒓Sharing parameterLayer−specific parameters
𝑨𝟏(𝒍)
𝑨𝟐(𝒍)
𝑨𝟑(𝒍)
𝑨𝟒(𝒍)
"(
!!
")
"*
	𝑪(𝒍)+𝐷(()𝑈(()MPO(𝑾(𝒍))
MPO−based LayerTransformer Layer
MPOBERT"
	𝑪(𝟏)
…𝒍=𝟏𝒍=𝑳
…𝐿	layers𝐺	groupsMPOBERT𝒍=𝟏…𝒍=𝑳/𝑮𝒍=𝑳−𝑳/𝑮…𝒍=𝑳
FFN
MHA𝑾(𝒍)Figure 2: Overview architecture of MPOBERT and
MPOBERT +. We use blocks with dashed borderlines
to represent shared central tensors. Central tensors are
shared across all LLayers in MPOBERT and within
groups in MPOBERT +.
simplify the decomposition results of a matrix as a
central tensorC(the middle tensor) and auxiliary
tensorsfAign 1
i=1(the rest tensor).
As a major merit, such a decomposition can ef-
fectively reorganize and aggregate the information
of the matrix (Gao et al., 2020a): central tensor C
can encode the essential information of the original
matrix, while auxiliary tensors fAign 1
i=1serve as
its complement to exactly reconstruct the matrix.
3.2.2 MPO-based Scaling to Deep Models
Based on MPO decomposition, the essence of
our scaling method is to share the central tensor
across layers ( capturing the essential information )
and keep layer-speciﬁc auxiliary tensors ( model-
ing layer-speciﬁc variations ). Fig. 2 shows the
overview architecture of the proposed MPOBERT.
Cross-layer Parameter Sharing . To introduce
our architecture, we consider a simpliﬁed struc-
ture ofLlayers, each consisting of a single ma-
trix. With the ﬁve-order MPO decomposition
(i.e.,n= 5 ), we can obtain the decomposi-
tion results for a weight matrix ( W(l)), denoted
asfC(l);A(l)
1;A(l)
2;A(l)
3;A(l)
4gL
l=1, whereC(l)and
fA(l)
ig4
i=1are the central tensor and auxiliary ten-
sors of the l-th layer. Our approach is to set a
shared central tensor Cacross layers, which means
thatC(l)=C(8l= 1L). As shown in Gao et al.
(2020a), the central tensor contains the major pro-
portion of parameters (more than 90%), and thus
our method can largely reduce the parameters when
scaling a PLM to very deep architecture. Note that
this strategy can be easily applied to multiple ma-

--- PAGE 4 ---
trices in a Transformer layer, and we omit the dis-
cussion for the multi-matrix extension. Another
extension is to share the central tensor by different
grouping layers. We implement a layer-grouping
MPOBERT, named MPOBERT +, which divides
the layers into multiple parts and sets unique shared
central tensors in each group.
Layer-speciﬁc Weight Adaptation . Unlike AL-
BERT (Lan et al., 2019), our MPO-based architec-
ture enables layer-speciﬁc adaptation by keeping
layer-speciﬁc auxiliary tensors ( fA(l)
ig4
i=1). These
auxiliary tensors are decomposed from the origi-
nal matrix, instead of extra blocks (Zhang et al.,
2022). They only contain a very small propor-
tion of parameters, which does not signiﬁcantly
increase the model size. While, another merit of
MPO decomposition is that these tensors are highly
correlated via bond dimensions, and a small pertur-
bation on an auxiliary tensor can reﬂect the whole
matrix (Liu et al., 2021). If the downstream task
requires more layer speciﬁcity, we can further in-
corporate low-rank adapters (Hu et al., 2021) for
layer-speciﬁc adaptation. Speciﬁcally, we denote
W(l)
Adapteras the low-rank adapter for W(l). In
this way,W(l)can be formulated as a set of ten-
sors:fC(l);A(l)
1;A(l)
2;A(l)
3;A(l)
4;W(l)
Adapterg. The
parameter scale of adapters, Lrdtotal, is deter-
mined by the layer number L, the rankr, and the
shape of the original matrix ( dtotal=din+dout
is the sum of the input and output dimensions of
a Transformer Layer). Since we employ low-rank
adapters, we can effectively control the number of
additional parameters from adapters.
3.3 Stable Training for MPOBERT
With the above MPO-based approach, we can
scale a PLM to a deeper architecture in a highly
parameter-efﬁcient way. However, as shown in
prior studies (Lan et al., 2019; Wang et al., 2022),
it is difﬁcult to optimize very deep PLMs, espe-
cially when shared components are involved. In
this section, we introduce a simple yet stable train-
ing algorithm for MPOBERT and then discuss how
it addresses the training instability issue.
3.3.1 MPO-based Network Initialization
Existing work has found that parameter initializa-
tion is important for training deep models (Huang
et al., 2020; Zhang et al., 2019; Wang et al., 2022),
which can help alleviate the training instability. To
better optimize the scaling MPOBERT, we proposea specially designed initialization method based on
the above MPO-based architecture.
Initialization with MPO Decomposition . Since
MPOBERT shares global components ( i.e.,the cen-
tral tensor) across all layers, our idea is to employ
existing well-trained PLMs based on weight shar-
ing for improving parameter initialization. Here,
we use the released 24-layer ALBERT with all
the parameters shared across layers. The key idea
is to perform MPO decomposition on the param-
eter matrices of ALBERT, and obtain the corre-
sponding central and auxiliary tensors. Next, we
discuss the initialization of MPOBERT in two as-
pects. For central tensors , we directly initialize
them (each for each matrix) by the derived central
tensors from the MPO decomposition results of
ALBERT. Since they are globally shared, one sin-
gle copy is only needed for initialization regardless
of the layer depth. Similarly, for auxiliary tensors ,
we can directly copy the auxiliary tensors from the
MPO decomposition results of ALBERT.
Scaling the Initialization . A potential issue is that
ALBERT only provides a 24-layer architecture, and
such a strategy no longer supports the initialization
for an architecture of more than 24 layers (without
corresponding auxiliary tensors). As our solution,
we borrow the idea in Wang et al. (2022) that avoids
the exploding update by incorporating an additional
scaling coefﬁcient and multiplying the randomly
initialized values for the auxiliary tensors (those in
higher than 24 layers) with a coefﬁcient of (2L) 1
4,
whereLis the layer number. Next, we present a
theoretical analysis of training stability.
3.3.2 Theoretical Analysis
To understand the issue of training instability
from a theoretical perspective, we consider a
Transformer-based model F(x;W)withxand
Was input and parameters, and consider one-step
update4F1. According to Wang et al. (2022), a
large model update ( 4F) at the beginning of train-
ing is likely to cause the training instability of deep
Transformer models. To mitigate the exploding up-
date problem, the update should be bounded by a
constant, i.e.,k4Fk=O(1). Next, we study how
the4Fis bounded with the MPOBERT.
MPO-based Update Bound . Without loss of gen-
erality, we consider a simple case of low-order
14F4=F(x;W @
@WL(F(x) y)) F(x;W):

--- PAGE 5 ---
MPO decomposition: n= 3 in Eq. (1). Follow-
ing the derivation method in Wang et al. (2022),
we simplify the matrices W,A1,CandA2to
scalarsw,u,c,v, which means the parameter wl
at thel-th layer can be decomposed as wl=
ulclvl. Based on these notations, we consider
L-layer Transformer-based model F(x;w)(w=
fw1;w2;:::;wLg), where each sub-layer is normal-
ized with Post-LN: xl+1=LN(xl+Gl(xl;wl)).
Then we can prove k4Fksatisﬁes (see Theo-
rem A.1 in the Appendix):
k4FkLX
l=1(c1vlku
l ulk+c1ulkv
l vlk
+vlulkc
1 c1k); (2)
The above equation bounds the model update in
terms of the central and auxiliary tensors. Since
central tensors ( cl) can be initialized using the pre-
trained weights, we can further simplify the above
bound by reducing them. With some derivations
(See Corollary A.2 in the Appendix), we can ob-
tain(v2
i+u2
i)(uLvL) =O(1
L)in order to guar-
antee thatk4Fk=O(1). For simplicity, we set
ui=vi= (2L) 1
4to bound the magnitude of each
update independent of layer number L. In the im-
plementation, we ﬁrst adopt the Xavier method for
initialization, and then scale the parameter values
with the coefﬁcient of (2L) 1
4.
Comparison . Previous research has shown that
using designed values for random initialization can
improve the training of deep models (Huang et al.,
2020; Zhang et al., 2019; Wang et al., 2022). These
methods aim to improve the initialization of gen-
eral Transformer architectures for training from
scratch. As a comparison, we explore the use of
pre-trained weights and employ the MPO decompo-
sition results for initialization. In particular, Gong
et al. (2019) have demonstrated the effectiveness of
stacking pre-trained shallow layers for deep mod-
els in accelerating convergence, also showing per-
formance superiority of pre-trained weights over
random initialization.
3.3.3 Training and Acceleration
To instantiate our approach, we pre-train a 48-
layer BERT model ( i.e.,MPOBERT 48). For a fair
comparison with BERT BASE and BERT LARGE , we
adopt the same pre-training corpus (BOOKCOR-
PUS (Zhu et al., 2015) and English Wikipedia (De-
vlin et al., 2018)) and pre-training tasks (maskedlanguage modeling, and sentence-order predic-
tion). We ﬁrst perform MPO decomposition on
the weights of ALBERT and employ the initializa-
tion algorithm in Section 3.3.1 to set the parame-
ter weights. During the training, we need to keep
an updated copy of central tensors and auxiliary
tensors: we optimize them according to the pre-
training tasks in an end-to-end way and combine
them to derive the original parameter matrix for
forward computation (taking a relatively small cost
of parallel matrix multiplication).
Typically, the speed of the pre-training process
is affected by three major factors: arithmetic band-
width, memory bandwidth, or latency. We further
utilize a series of efﬁciency optimization ways to
accelerate the pre-training, such as mixed preci-
sion training with FP16 (reducing memory and
arithmetic bandwidth) and fused implementation
of activation and normalization (reducing latency).
Finally, we can train the 48-layer MPOBERT at
a time cost of 3.8 days (compared with a non-
optimized cost of 12.5 days) on our server con-
ﬁguration (8 NVIDIA V100 GPU cards and 32GB
memory). More training details are can be found
in the experimental setup Section 4.1 and Ap-
pendix A.2 (Table 6 and Algorithm 1).
4 Experiments
In this section, we ﬁrst set up the experiments and
then evaluate the efﬁciency of MPOBERT on a
variety of tasks with different model settings.
4.1 Experimental Setup
Pre-training Setup . For the architecture, we
denote the number of layers as L, the hid-
den size as H, and the number of self-
attention heads as A. We report results on
four model sizes: MPOBERT 12(L=12,H=768,
A=12), MPOBERT 24(L=24,H=1024,A=16),
MPOBERT 48(L=48,H=1024,A=16) and
MPOBERT 48+that implement cross-layer param-
eter sharing in three distinct groups as discussed
in subsection 3.2.2. We pre-train all of the models
with a batch size of 4096 for 10 ksteps. Our code
will be released after the review period.
Fine-tuning Datasets . To evaluate the perfor-
mance of our model, we conduct experiments
on the GLUE (Wang et al., 2018) and SQuAD
v1.1 (Rajpurkar et al., 2016) benchmarks. Since
ﬁne-tuning is typically fast, we run an exhaustive

--- PAGE 6 ---
ExperimentsMRPC SST-2 CoLA RTE STS-B QQP MNLI QNLI SQuAD Avg. #To (M)
F1 Acc. Mcc. Acc. Spear. F1/Acc. Acc. Acc. F1
Development set
Tiny Models (#To < 50M)
ALBERT 12 89.0 90.6 53.4 71.1 88.2 -/89.1 84.5 89.4 89.3 82.7 11
ALBERT 24 84.6 93.6 52.5 79.8 90.1 -/88.1 85.0 91.7 90.6 84.0 18
MPOBERT 12 90.3 92.3 55.2 71.8 90.5 -/90.1 84.7 91.2 90.1 84.0 20
MPOBERT 24 90.3 94.4 58.1 75.5 91.1 -/90.2 87.0 92.6 92.3 85.7 46
Small Models (50M < #To < 100M)
T512 89.2 94.7 53.5 71.7 91.2 -/91.1 87.8 93.8 90.0 84.8 60
MPOBERT 48 90.8 94.7 58.3 77.3 91.4 -/89.5 86.3 92.0 92.3 85.8 75
Base Models (#To > 100M)
BERT 12 90.7 91.7 48.9 71.4 91.0 -/90.8 83.7 89.3 88.5 82.9 110
XLNet 12 85.3 94.4 49.3 63.9 85.6 -/90.7 90.9 91.8 90.2 82.5 117
RoBERTa 12 91.9 92.2 59.4 72.2 89.4 -/91.2 88.0 92.7 91.2 85.4 125
BART 12 91.4 93.8 56.3 79.1 89.9 -/90.8 86.4 92.4 91.6 82.8 140
MPOBERT 48+ 89.7 94.4 57.4 79.8 91.1 -/89.3 87.1 92.4 91.4 86.0 102
Test set
Tiny Models (#To < 50M)
ALBERT 12 89.2 93.2 53.6 70.2 87.3 70.3/- 84.6 92.5 89.3 81.1 11
ALBERT 24 88.7 94.0 51.7 73.7 86.9 69.1/- 84.9 91.8 90.6 81.2 18
MobileBERT 24 88.8 92.6 51.1 70.4 84.8 70.5/- 83.3 91.6 90.3 80.4 25
MPOBERT 12 89.2 91.9 52.7 70.6 87.1 69.6/- 85.0 91.0 90.1 80.8 20
MPOBERT 24 89.0 94.5 55.5 73.4 88.2 71.0/- 86.3 93.0 92.3 82.6 46
Small Models (50M < #To < 100M)
T512 89.7 91.8 41.0 69.9 85.6 70.0/- 82.4 90.3 90.0 78.7 60
TinyBERT 6| 87.3 93.1 51.1 70.0 83.7 71.6/- 84.6 90.4 87.5 79.9 67
DistilBERT 6| 86.9 92.5 49.0 58.4 81.3 70.1/- 82.6 88.9 86.2 77.3 67
MPOBERT 48 90.0 94.0 55.0 74.0 88.7 71.0/- 86.5 91.8 92.3 82.6 75
Base Models (#To > 100M)
BERT 12 88.9 93.5 52.1 66.4 85.8 71.2/- 84.6 90.5 88.5 79.1 110
XLNet 12 89.2 94.3 47.3 66.5 85.4 71.9/- 87.1 91.4 90.2 80.4 117
RoBERTa 12 89.9 93.2 57.9 69.9 88.3 72.5/- 87.7 92.5 91.2 82.6 125
BART 12 89.9 93.7 49.6 72.6 86.9 71.7/- 84.9 92.3 91.6 81.5 140
MPOBERT 48+ 89.9 94.5 56.0 74.5 88.4 70.5/- 86.5 92.6 91.4 82.7 102
Table 1: Performance comparison of different models on natural language understanding tasks (in percent). “#
To (M)” denote the number (in millions) of total parameters. We compare MPOBERT with PLMs ( i.e.,BERT
and ALBERT) and Parameter-efﬁcient Transformers ( i.e.,MobileBERT, TinyBERT and DistilBERT), respectively.
The best and the second-best performance in each task are highlighted in bold and underlined. : Experimental
results by Sun et al. (2020b). |: Experimental results by Jiao et al. (2019). : Experimental results by Devlin
et al. (2018).
parameter search and choose the model that per-
forms best on the development set to make predic-
tions on the test set. We include the details in the
Appendix(see Appendix A.3.1 for the datasets and
Appendix A.3.2 for evaluation metrics)
Baseline Models . We compare our proposed
MPOBERT to the existing competitive deep PLMs
and parameter-efﬁcient models. In order to make
fair comparisons, we divide the models into three
major categories based on their model sizes:
Tiny Models (#To < 50M). ALBERT 12(Lan
et al., 2019) is the most representative PLM that
achieves competitive results with only 11M.
Small models (50M< #To <100M). We con-sider PLMs (T5 12) and compressed models (Mo-
bileBERT (Sun et al., 2020b), DistilBERT (Sanh
et al., 2019) and TinyBERT (Jiao et al., 2019)).
Base models (#To > 100M). We compare with
BERT 12, XLNet 12, RoBERTa 12and BART 12for
this category. Note that we only include the base
variants that have similar model sizes in order to
make a fair comparison.
More details about the baseline models are de-
scribed in Appendix A.3.3.
4.2 Main Results
Fully-supervised setting . We present the results
of MPOBERT and other baseline models on GLUE

--- PAGE 7 ---
and Squad for ﬁne-tuning in Table 1.
Firstly, we evaluate MPOBERT’s performance
in comparison to other models with similar num-
bers of parameters. In particular, for tiny mod-
els, MPOBERT 24outperforms ALBERT 24, and
achieves substantial improvements on both the de-
velopment set (85.7 v.s.84.0) and test sets (82.6
v.s.81.2). This highlights the beneﬁts of increased
capacity from layer-speciﬁc parameters ( i.e.,the
auxiliary tensors and layer-speciﬁc adapters) in
MPOBERT. Furthermore, for small and base mod-
els, 48-layer MPOBERT consistently achieves bet-
ter results than T5 12and all parameter-efﬁcient
models, while also achieving comparable results to
other 12-layer PLMs with a reduced number of pa-
rameters. This demonstrates the signiﬁcant beneﬁts
of scaling along the model depth with layer-speciﬁc
parameters in MPOBERT.
Secondly, we assess MPOBERT’s parameter ef-
ﬁciency by comparing it to other PLMs within the
same model depth. For instance, when consider-
ing models with L=12 layers, MPOBERT achieves
comparable results or even outperforms (+1.7 for
BERT 12and +0.4 for XLNet 12) PLMs while hav-
ing fewer parameters. This further highlights the
advantages of MPOBERT’s parameter-efﬁcient ap-
proach in constructing deep models.
Multitask Fine-tuning Setting . To demonstrate
the effectiveness of our proposed parameter-
sharing model in learning shared representations
across multiple tasks, we ﬁne-tune MPOBERT,
BERT and ALBERT on the multitask GLUE bench-
mark and report the results in Table 2. Speciﬁ-
cally, we design two groups of experiments. (1)
Deep vs. shallow models. Comparing with
BERT 12, MPOBERT 48has much deeper Trans-
former layers but still fewer total parameters ( i.e.,
75M vs. 110M). We ﬁnd that MPOBERT 48
achieves 1.4 points higher on average GLUE score
than BERT 12. (2) Central tensors sharing vs. all
weight sharing. Comparing with ALBERT 12,
MPOBERT 12only shares part of weights, i.e.,cen-
tral tensors, while ALBERT 12shares all of the
weights. We ﬁnd that sharing central tensors may
effectively improve the average results than sharing
all weights (82.0 v.s.81.4 for MRPC).
Few-shot Learning Setting . We evaluate the per-
formance of our proposed model, MPOBERT, in
few-shot learning setting (Huang et al., 2022) on
two tasks, SST-2 and MNLI, using a limited num-Datasets B 12 M48 M12 A12
MNLI (Acc.) 83.9 85.4 82.8 82.7
QNLI (Acc.) 90.8 91.1 90.0 89.4
SST-2 (Acc.) 91.7 93.0 90.9 90.6
RTE (Acc.) 81.2 82.0 79.8 79.1
QQP (Acc.) 91.2 87.6 90.4 89.7
CoLA (Mcc.) 53.6 54.9 45.0 35.9
MRPC (F1) 84.2 91.8 89.9 89.2
STS-B (Spear.) 87.4 89.0 86.9 87.5
Avg. 83.0 84.4 82.0 80.5
#To (M) 110 75 20 11
Table 2: Performance of multi-task learning on GLUE
benchmark obtained by ﬁne-tuning BERT 12(B12),
MPOBERT 48 (M48), MPOBERT 12 (M12) and
ALBERT 12(A12) (in percent).
SST-2 MNLI
Shots (K) 10 20 30 10 20 30
BERT 12 54.8 59.7 61.6 37.0 35.6 35.7
ALBERT 12 56.7 59.3 60.0 36.3 35.6 36.5
MPOBERT 12 58.9 65.4 64.6 36.7 36.7 37.1
Table 3: Comparison of few-shot performance.
ber of labeled examples. Results in Table 3 show
that MPOBERT outperforms BERT, which suffers
from over-ﬁtting, and ALBERT, which does not
beneﬁt from its reduced number of parameters.
These results further demonstrate the superiority of
our proposed model in exploiting the potential of
large model capacity under limited data scenarios.
4.3 Detailed Analysis
Analysis of Initialization Methods . This experi-
ment aims to exclude the effect of initialized pre-
trained weights on ﬁne-tuning results. We plot the
performance of the model on SST-2 w.r.t training
steps. In particular, we compare the performance
of MPOBERT using different initialization meth-
ods (Xavier in Fig. 3(a) and decomposed weights
of ALBERT in Fig. 3(b)) for pre-training. The
results demonstrate that pre-training MPOBERT
from scratch requires around 50 ksteps to achieve
performance comparable to BERT BASE , while ini-
tializing with the decomposed weights of ALBERT
signiﬁcantly accelerates convergence and leads to
obvious improvements within the ﬁrst 10 ktraining
steps. In contrast, the gains from continual pre-
training for ALBERT are negligible. These results
provide assurance that the improvements observed
in MPOBERT are not solely attributed to the use

--- PAGE 8 ---
0k10k20k30k40k50k
Pre-training Steps90.091.092.093.0Acc.
BERT MPOBERT(a) Pre-training from scratch
0k10k 20k 30k 40k
Pre-training Steps90.091.092.093.0Acc.
BERT
ALBERTMPOBERT (b) Continual Pre-training
Figure 3: Comparison of the SST-2 accuracy achieved
through pre-training from scratch and pre-training with
the initialization of decomposed ALBERT weights.
of initialized pre-trained weights.
BERTMPOBERTALBERTSurfaceSyntacticSemantic
Low PerformanceHigh Performance
Figure 4: A visualization of layer-wise linguistic pat-
terns. Each column represents a probing task, and each
row represents a Transformer layer. The red dashed box
indicates the layer that performs best.
Experiment SST-2 RTE MRPC #To (M)
MPOBERT 12 92.8 72.9 91.8 20.0
w/o Adapter 92.3 71.8 90.3 19.4
w/o PS 91.4 67.9 85.8 11.9
Table 4: Ablation study on the SST-2, RTE, and MRPC
datasets (in percent).
Ablation Analysis . To assess the individual impact
of the components in our MPOBERT model, we
conduct an ablation study by removing either the
layer-speciﬁc adapter or the cross-layer parameter-
sharing strategy. The results, displayed in Table 4,
indicate that the removal of either component re-
sults in a decrease in the model’s performance,
highlighting the importance of both components
in our proposed strategy. While the results alsoRank SST-2 RTE MRPC #To (M)
4 91.9 69.7 88.2 19.7
8 92.8 72.9 91.8 20.0
64 91.6 69.3 88.1 24.3
Table 5: Comparison of different adapter ranks on three
GLUE tasks (in percent). “Rank” denotes the adapter
rank in MPOBERT.
indicate that cross-layer parameter sharing plays a
more crucial role in the model’s performance.
Performance Comparison w.r.t Adapter Rank .
To compare the impact of the adapter rank in layer-
speciﬁc adapters on MPOBERT’s performance, we
trained MPOBERT with different ranks (4,8 and
64) and evaluate the model on downstream tasks in
Table 5. The results demonstrate that a rank of 8 is
sufﬁcient for MPOBERT, which further shows the
necessity of layer-speciﬁc adapters. However, we
also observe a decrease in the performance of the
variant with adapter rank 64. This illustrates that
further increasing the rank may increase the risk of
over-ﬁtting in ﬁne-tuning process. Therefore, we
set a rank of 8 for MPOBERT in the main results.
Analysis of Linguistic Patterns . To investigate
the linguistic patterns captured by MPOBERT,
BERT, and ALBERT, we conduct a suite of probing
tasks, following the methodology of Tenney et al.
(2019). These tasks are designed to evaluate the
encoding of surface, syntactic, and semantic infor-
mation in the models’ representations. The results,
shown in Fig. 4, reveal that BERT encodes more
local syntax in lower layers and more complex se-
mantics in higher layers, while ALBERT does not
exhibit such a clear trend. However, MPOBERT
exhibits similar layer-wise behavior to BERT in
some tasks ( i.e.,task 0,2,4), and improved results
in lower layers for others ( i.e.,task 3) which is
similar to ALBERT. The result demonstrates that
MPOBERT captures linguistic information differ-
ently than other models, and its layer-wise parame-
ters play an important role in this difference.
5 Conclusion
We develop MPOBERT, a parameter-efﬁcient pre-
trained language model that allows for the efﬁcient
scaling of deep models without the need for addi-
tional parameters or computational resources. We
achieve this by introducing an MPO-based Trans-
former layer and sharing the central tensors across

--- PAGE 9 ---
layers. During training, we propose initialization
methods for the central and auxiliary tensors, which
are based on theoretical analysis to address training
stability issues. The effectiveness of MPOBERT
is demonstrated through various experiments, such
as supervised, multitasking, and few-shot where it
consistently outperforms other competing models.
Limitations
The results presented in our study are limited
by some natural language processing tasks and
datasets that are evaluated, and further research
is needed to fully understand the interpretability
and robustness of our MPOBERT models. Addi-
tionally, there is subjectivity in the selection of
downstream tasks and datasets, despite our use of
widely recognized categorizations from the litera-
ture. Furthermore, the computational constraints
limited our ability to study the scaling performance
of the MPOBERT model at deeper depths such
as 96 layers or more. This is an area for future
research.
Ethics Statement
The use of a large corpus for training large language
models may raise ethical concerns, particularly re-
garding the potential for bias in the data. In our
study, we take precautions to minimize this issue
by utilizing only standard training data sources,
such as BOOKCORPUS and Wikipedia, which are
widely used in language model training (Devlin
et al., 2018; Lan et al., 2019). However, it is im-
portant to note that when applying our method to
other datasets, the potential bias must be carefully
considered and addressed. Further investigation
and attention should be given to this issue in future
studies.
References
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael
Carbin. 2020. The lottery ticket hypothesis for pre-
trained BERT networks. In Advances in Neural
Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fe-
dus, Denny Zhou, Daphne Ippolito, David Luan,
Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,
Ryan Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
Palm: Scaling language modeling with pathways.
CoRR , abs/2204.02311.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Ze-Feng Gao, Song Cheng, Rong-Qiang He, Zhi-Yuan
Xie, Hui-Hai Zhao, Zhong-Yi Lu, and Tao Xiang.
2020a. Compressing deep neural networks by ma-
trix product operators. Physical Review Research ,
2(2):023300.
Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi
Lu, and Ji-Rong Wen. 2022. Parameter-efﬁcient
mixture-of-experts architecture for pre-trained lan-
guage models. In Proceedings of the 29th Inter-
national Conference on Computational Linguistics,
COLING 2022, Gyeongju, Republic of Korea, Oc-
tober 12-17, 2022 , pages 3263–3273. International
Committee on Computational Linguistics.
Ze-Feng Gao, Xingwei Sun, Lan Gao, Junfeng Li,
and Zhong-Yi Lu. 2020b. Compressing lstm net-
works by matrix product operators. arXiv preprint
arXiv:2012.11943 .
Timur Garipov, Dmitry Podoprikhin, Alexander
Novikov, and Dmitry Vetrov. 2016. Ultimate ten-
sorization: compressing convolutional and fc layers
alike. arXiv preprint arXiv:1611.03214 .
Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei
Wang, and Tie-Yan Liu. 2019. Efﬁcient training of
BERT by progressively stacking. In Proceedings
of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , volume 97 of Proceedings of Ma-
chine Learning Research , pages 2337–2346. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .

--- PAGE 10 ---
Xiao Shi Huang, Felipe Pérez, Jimmy Ba, and Mak-
sims V olkovs. 2020. Improving transformer opti-
mization through better initialization. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event , volume 119 of Proceedings of Machine
Learning Research , pages 4475–4483. PMLR.
Zixian Huang, Ao Wu, Jiaying Zhou, Yu Gu, Yue
Zhao, and Gong Cheng. 2022. Clues before answers:
Generation-enhanced multiple-choice QA. In Pro-
ceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3272–3287, Seattle, United States. Association for
Computational Linguistics.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling bert for natural language
understanding. arXiv preprint arXiv:1909.10351 .
Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. Scaling laws for neural language
models. CoRR , abs/2001.08361.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
ing of language representations. arXiv preprint
arXiv:1909.11942 .
Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan
Xie, Zhong-Yi Lu, and Ji-Rong Wen. 2021. En-
abling lightweight ﬁne-tuning for pre-trained lan-
guage model compression based on matrix product
operators. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers) ,
pages 5388–5398.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,
Haotang Deng, and Qi Ju. 2020. Fastbert: a self-
distilling bert with adaptive inference time. arXiv
preprint arXiv:2004.02178 .
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? In Ad-
vances in Neural Information Processing Systems
32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-
14, 2019, Vancouver, BC, Canada , pages 14014–
14024.
Mohammadmahdi Nouriborji, Omid Rohanian,
Samaneh Kouchaki, and David A. Clifton.
2022. Minialbert: Model distillation via
parameter-efﬁcient recursive transformers. CoRR ,
abs/2210.06425.
Alexander Novikov, Dmitry Podoprikhin, Anton Os-
okin, and Dmitry Vetrov. 2015. Tensorizing neural
networks. arXiv preprint arXiv:1509.06569 .Ivan V Oseledets. 2011. Tensor-train decomposition.
SIAM Journal on Scientiﬁc Computing , 33(5):2295–
2317.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. J. Mach. Learn. Res. , 21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Machel Reid, Edison Marrese-Taylor, and Yutaka Mat-
suo. 2021. Subformer: Exploring weight sharing for
parameter efﬁciency in generative transformers. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4081–4090, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .
Xingwei Sun, Ze-Feng Gao, Zhong-Yi Lu, Junfeng Li,
and Yonghong Yan. 2020a. A model compression
method with matrix product operators for speech
enhancement. IEEE/ACM Transactions on Audio,
Speech, and Language Processing , 28:2837–2847.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020b. Mobilebert:
a compact task-agnostic bert for resource-limited de-
vices. arXiv preprint arXiv:2004.02984 .
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R. Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R. Bowman, Dipan-
jan Das, and Ellie Pavlick. 2019. What do you
learn from context? probing for sentence structure
in contextualized word representations. In 7th Inter-
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Hongyu Wang, Shuming Ma, Li Dong, Shaohan
Huang, Dongdong Zhang, and Furu Wei. 2022.
Deepnet: Scaling transformers to 1, 000 layers.
CoRR , abs/2203.00555.

--- PAGE 11 ---
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020.
Structured pruning of large language models. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020 , pages 6151–6162.
Association for Computational Linguistics.
Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu,
Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,
James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
2020. Large batch optimization for deep learn-
ing: Training BERT in 76 minutes. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Rose Yu, Stephan Zheng, Anima Anandkumar, and
Yisong Yue. 2017. Long-term forecasting using
tensor-train rnns. Arxiv .
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma.
2019. Fixup initialization: Residual learning with-
out normalization. In 7th International Conference
on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019 . OpenReview.net.
Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu,
Bin Xiao, Jianlong Fu, and Lu Yuan. 2022. Minivit:
Compressing vision transformers with weight mul-
tiplexing. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , pages 12135–
12144. IEEE.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE inter-
national conference on computer vision , pages 19–
27.

--- PAGE 12 ---
A Appendix
A.1 Proofs
Notations. We denoteL()as the loss function.
LN(x)as the standard layer normalization with
scale= 1and bias= 0. LetO()denote stan-
dard Big-O notation that suppresses multiplicative
constants.=stands for equal bound of magnitude.
We aim to study the magnitude of the model up-
dates. We deﬁne the model update as k4Fk.
DeﬁnitionF(x;)is updated by ()per SGD
step after initialization as !0. That is,
k4F(x)k= ()where4F(x)can be calcu-
lated through F(x; @
@L(F(x) y)) F(x;).
Theorem A.1 Given anN-layer transformer-
based model F(x;)(=f1;2;:::;Ng), where
ldenotes the parameters in l-th layer and each
sub-layer is normalized with Post-LN: xl+1=
LN(xl+Gl(xl;l)). In MPOBERT, lis decom-
posed by MPO to local tensors: l=ulclvl, and
we sharefcigN
i=1acrossNlayers:cl=c1;l=
1;2;;N. Thenk4Fksatisﬁes:
k4FkNX
i=1(c1viku
i uik+c1uikv
i vik
+viuikc
1 c1k) (3)
Proof: We follow (Zhang et al., 2019) and make
the following assumptions to simplify the deriva-
tions:
1. Hidden dimension dequals to 1;
2.var(x+Gl(x))=var(x) +var(Gl(x));
3.All relevant weights are positive with mag-
nitude less than 1.
Given Assumption 1, if Gl(x)is MLP with the
weightl, thenGl(x)=lx. With assumption 2,
we have:
xl+1=fl(xl;l) =x+Gl(x)p
Var(x+Gl(x))(4)
=1 +lq
1 +2
lxl; (5)Then, with Taylor expansion, the model update
k4Fksatisﬁes:
k4Fk=kF(x;) F(x;k
=x
N+1 xN+1
=kf(x
N;
N) f(xN;N)k
=kf(x
N;U
N;C
N;V
N)
 f(xN;UN;CN;VN)k
@f
@x(x
N xN)
+@f
@@
@UN(U
N UN)T
+@f
@@
@CN(C
N CN)T
+@f
@@
@VN(V
N VN)T(6)
With Eq. (5), the magnitude of@fl
@xand@fl
@is
bounded by:
@fl
@x=1 +lq
1 +2
l(7)
@fl
@l=1 l
(1 +2
l)3
2xl (8)
Since we apply MPO decomposition to l, we get:
l=UlClVl (9)
For simplicity, we reduce the matrices U,C,Vto
the scalarsu,c,v. Thus with Assumption 3, Eq. (6)
is reformulated as: Finally, with Assumption 3 we
have:
k4Fk=x
N+1 xN+1 (10)
NX
i=11 uic1vi
(1 +u2
ic2
1v2
i)3
2(c1viku
i uik
+c1uikv
i vik) +viuikc
1 c1k)
NX
i=1(c1viku
i uik+c1uikv
i vik
+viuikc
1 c1k) (11)
2
Corollary A.2 Given that we initialise c1in
MPOBERT with well-trained weights, it is reason-
able to assume that updates of c1are well-bounded.
Then4Fsatisﬁesk4Fk=O(1)when for all
i= 1;;N:
(v2
i+u2
i)(uNvN) =O(1
N) (12)

--- PAGE 13 ---
Proof: For anN-layer MPOBERT, we have:
k4FkNX
i=1(viku
i uik+uikv
i vik)
(13)
NX
i=1(vi@L
@F@F
@i@i
@ui
+ui@L
@F@F
@i@i
@vi)(14)
By assumption@L
@F=O(1)and@F
@i@F
@N=kNk, we achieve:
NX
i=1(vi@L
@F@F
@i@i
@ui
+ui@L
@F@F
@i@i
@vi) (15)
=NX
i=1(v2
iuNvN+u2
iuNvN)
=O(NX
i=1(v2
i+u2
i)(uNvN)) =O(1); (16)
Finally, we achieve:
(v2
i+u2
i)(uNvN) =O(1
N) (17)
Due to symmetry, we set ui=u,vi=v. Thus,
from A.1, we set u=v= (2N) 1
4to achieve to
bound the magnitudes of each update to be inde-
pendent of model depth N,i.e.,k4Fk=O(1).
2
A.2 Training Details
A.2.1 Details of Training
Here we describe the details of the pre-training
process in Algorithm 1. For pre-training, we tune
the learning rate in the range of [ 1:010 5,1:0
10 6] and use the LAMB optimizer (You et al.,
2020). Since ﬁne-tuning is typically fast, we run an
exhaustive parameter search ( i.e.,learning rate in
the range of [ 2:010 4,2:010 6], batch size
in {8,16,32}) and choose the model that performs
best on the development set to make predictions on
the test set.
A.2.2 Details of Training Conﬁgurations
In this part, we list the training conﬁgurations of
MPOBERT and other representative PLMs in Ta-
ble 6.Algorithm 1 The MPOBERT training procedure.
Require: W(l): Weight matrix of l-th layer in MPOBERT.
W(0)
A: Pre-trained weight matrix in ALBERT. U(l)and
D(l): Matrices in low-rank adapter. : Learning rate.L:
Stochastic objection function. L: Model layers number.
(MPO decomposition)
1:fA(l)
1;A(l)
2;C(l);A(l)
3;A(l)
4g MPO (W(l))
2:fA(0)
1;A(0)
2;C(0);A(0)
3;A(0)
4g MPO (W(0)
A)
(Initialization Procedure)
3:for0<l24do
4:C(l) C(0);fA(l)
jg4
j=1 fA(0)
jg4
j=1
5:end for
6:for24<lLdo
7:C(l) C(0);fA(l)
jg4
j=1 f(2L) 1
4A(0)
jg4
j=1
8:end for
9:U(l) 0,D(l) N (0;2)
10:W(l)=A(l)
1A(l)
2C(l)A(l)
3A(l)
4+W(l)
Adapter
(Training procedure with mixed precision and fused im-
plementation techniques.)
11: while not converged do
12:t t+ 1
13:gt @L(W(l)
t)
@(W(l)
t)
14: W(l)
t W(l)
t 1 gt
15: end while
16: return Converged model
A.3 Experimental Details
A.3.1 Details of Fine-tuning Datasets
GLUE benchmark covers multiple datasets (MNLI,
QNLI, QQP, CoLA, RTE, MRPC, SST-2)2. The
SQuAD is a collection of 100 kcrowd-sourced
question/answer pairs. Given a question and a pas-
sage, the task is to predict the answer text span in
the passage.
A.3.2 Details of Evaluation Metrics
Following Gao et al. (2022), we employ Matthew’s
correlation for CoLA, Spearman for STS-B, F1 for
MRPC, and accuracy for the remaining tasks as the
metrics for the GLUE benchmark. We compute and
present the average scores across all test samples
for each of the aforementioned metrics.
A.3.3 Details of Baseline Models
We compare our proposed MPOBERT to the exist-
ing competitive deep PLMs and parameter-efﬁcient
models. In order to make fair comparisons, we di-
vide the models into three major categories based
on their model sizes: Tiny Models (#To < 50M).
ALBERT 12(Lan et al., 2019) is the most represen-
tative PLM that achieves competitive results with
only 11M.
2In line with Raffel et al. (2020), we do not test WNLI due
to its adversarial character with respect to the training set.

--- PAGE 14 ---
Models #To (M) Depth Samples Training time GLUR Dev. GLUE Test
T511B 11000 24 - - - 89.0
T5BASE 220 24 128 524k 16 TPU v3
1 Day (t5-base)84.1 82.5
BERT LARGE 330 24 256 1000k 16 Cloud TPUs
4 Days84.1 81.6
ALBERT XXLARGE 235 1 4096 1.5M TPU v3
16 Days90.0 -
BART LARGE 407 24 8000 500k - 88.8 -
RoBERTa LARGE 355 24 8000 500k 1024 V100 GPUs
1 Day88.9 -
XLNet LARGE 361 24 8192 500k 512 TPU v3
5.5 Days87.4 -
MPOBERT 48+ 102 48 4096 10k 8 V100 GPUs
3.8 Days85.6 81.7
Table 6: Comparison with the strongest variants of popular PLMs. Since T5 11Bhas far more parameters than other
candidates, it’s reasonable to use T5 basefor a fair comparison.
Small models (50M< #To <100M). T5 12is
a small variant of T5 (Raffel et al., 2020) which
has only 6 encoder layers and 6 decoder layers.
In addition, there are three parameter-efﬁcient
Transformer models that have similar parameters,
namely MobileBERT (Sun et al., 2020b), Distil-
BERT (Sanh et al., 2019) and TinyBERT (Jiao et al.,
2019). We compare with these compressed models
to show the beneﬁt of scaling to deeper models
over compressing large models to small variants.
Base models (#To > 100M). We compare with
BERT 12, XLNet 12, RoBERTa 12and BART 12for
this category. Note that we only include the base
variants that have similar model sizes in order
to make a fair comparison. More details about
the comparison with the strongest variants are de-
scribed in Appendix A.3.3.
