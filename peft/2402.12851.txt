# 2402.12851.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2402.12851.pdf
# File size: 815992 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MoELoRA: Contrastive Learning Guided Mixture of Experts on
Parameter-Efficient Fine-Tuning for Large Language Models
Tongxu Luo1∗Jiahe Lei1∗Fangyu Lei1,2Weihao Liu1
Shizhu He1,2Jun Zhao1,2Kang Liu1,2
1Institute of Automation, CAS2University of Chinese Academy of Sciences
tongxuluo@163.com {shizhu.he, kliu}@nlpr.ia.ac.cn
Abstract
Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks.
Nonetheless, the process of updating billions of parameters demands significant computational resources and
training time, which poses a substantial obstacle to the widespread application of large-scale models in various
scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm
in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as
LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different
computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We
consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE,
we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted
experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number
of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average
performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B
GPT -3.5 on several benchmarks.
Keywords: Large Language Models, Mixture of Experts, Parameter Efficient Fine-tuning, Contrastive
Learning
1. Introduction
With the rapid advancement of Large Language
Models (LLMs) such as GPT3 (Brown et al., 2020),
BLOOM (Scao et al., 2022) and LLaMA (Touvron
et al., 2023), the successful application of self-
supervised pretraining on unlabeled text data has
presented unprecedented opportunities for enhanc-
ing downstream tasks. However, to fully harness
the potential of these LLMs in practical applications,
it is also necessary to continuously fine-tuning (Wei
et al., 2021; Chung et al., 2022) the LLMs based on
the training data of specific tasks to meet the per-
formance requirements of downstream tasks. The
substantial number of parameters, often exceeding
one billion, makes fine-tuning these LLMs a costly
endeavor, demanding a significant investment in
computational resources (Figure 1a). Therefore,
in recent years, Parameter-Efficient Fine-Tuning
(PEFT) (Mangrulkar et al., 2022; Zhang et al.,
2023) techniques have emerged with the aim of
reducing the cost of fine-tuning by freezing cer-
tain model weights or introducing smaller trainable
modules.
In the continual exploration within this field, a
series of methods such as LoRA (Hu et al., 2021),
AdaLoRA (Zhang et al., 2023), Adamix (Wang
et al., 2022), QLoRA (Dettmers et al., 2023) and
LoRAHub (Huang et al., 2023) have emerged,
each offering unique perspectives on efficiently
fine-tuning Large Language Models for better ap-
* Equal Contributions.plicability in downstream tasks. LoRA (Figure 1b)
introduces the concept of LoRA rank to reduce the
number of trainable parameters. AdaLoRA builds
upon LoRA’s foundation, achieving a search-free
approach that greatly simplifies the fine-tuning pro-
cess. Adamix combines the MoE with Adapters
to surpass the performance of LoRA. LoRAHub
employs a gradient-free method (Liu et al., 2020)
to perform weighted combinations of multiple LoRA
weights, thereby better adapting to new down-
stream tasks.
However, current PEFT approaches that employ
a limited set of global parameters face challenges
in flexibly combining different computational mod-
ules in downstream tasks. Inspired by methods
such as Mixture of Experts (MoE), Adamix, and
LoRAHub, we propose a novel PEFT approach
named MoELoRA. This method considers LoRA
as a Mixture of Experts, leveraging the modeling
capabilities of multiple experts for complex data
domains, as well as utilizing LoRA’s parameter-
efficient characteristics. As well as Figure 1c, dur-
ing both training and inference, only the LoRA se-
lected by the gating network will be activated and
only these "experts" relevant to specific tasks will
participate in gradient updates or forward inference.
However, applying MoE to LoRA presents chal-
lenges. Firstly, under the MoE architecture, gating
network doesn’t exhibit a preference for a particular
expert, leading to a certain level of routing random-
ness (Zuo et al., 2021). Secondly, guiding experts
to learn distinct features poses a challenging task.arXiv:2402.12851v1  [cs.CL]  20 Feb 2024

--- PAGE 2 ---
Pretrained 
Weights
A1B1
AnBn
A2B2
Gating 
Network
Input 
HiddenGate 
Select+
Output 
HiddenLoad Balance 
LossContrastive 
Loss(C) MoELoRA
Pretrained 
Weights
AB
Input 
Hidden+
Output 
Hidden(b) LoRA
Pretrained 
WeightsΔW
Input 
Hidden+
Output 
Hidden(a) Fine -TuningFigure 1: The Different Architectures for (a)Fine-Tuning, (b)LoRA and (c)proposed method MoELoRA.
∆Wdenotes the gradient increment for the downstream tasks. LoRA decomposes ∆Winto two matrices
AandBand our proposed MoELoRA can select AiandBicorresponding to a specific task for better
adaptation. In order to differentiate the capabilities of different experts, we employed contrastive learning
on the outputs of the experts.
To address these issues, we introduce con-
trastive learning among experts. Through this
contrastive learning approach, we treat the out-
puts of the same expert as positive samples and
the outputs of different experts as negative sam-
ples, encouraging experts to learn distinct features.
In the end, we achieve performance surpassing
LoRA under the same number of parameters. In
math reasoning, MoELoRA averaged 4.2% higher
performance than LoRA, and in common-sense
reasoning, it averaged 1.0% higher than LoRA.
Furthermore, MoELoRA exhibits competitive per-
formance compared to the 175B GPT -3.5 on a few
benchmarks.
In summary, our work makes the following con-
tributions:
(1) We consider LoRA as Mixture of Experts and
propose a novel PEFT method named MoELoRA,
which leverages the MoE architecture to achieve
dynamic combinations of multiple LoRA modules,
better catering to the requirements of downstream
tasks.
(2) In response to the random routing issue in
using Mixture of Experts (MoE) for LoRA fusion,
we propose employing contrastive learning to en-
courage experts to learn distinct features.
(3) We conduct experiments on 11 datasets
for math reasoning and common-sense reason-
ing tasks, demonstrating that our approach outper-
forms LoRA in all tasks. The results of ablation ex-
periments also show improvement in downstream
tasks with contrastive learning. Furthermore, we
perform tracking analysis of MoE routing to under-
stand the impact of our method on the model’s
decision-making process.2. Related Work
2.1. Parameter-Efficient Fine-Tuning
While fine-tuning with task-specific data sets, full-
model fine-tuning not only demands substantial
computational and storage resources but can
also result in catastrophic forgetting. In contrast,
Parameter-Efficient Fine-Tuning (PEFT) (Man-
grulkar et al., 2022) selectively adjusts a limited
number of parameters or introduces additional
trainable parameters rather than the entire back-
bone model, yet it still achieves comparable or
even superior performance compared to full fine-
tuning (Ding et al., 2023). Prefix-tuning (Li and
Liang, 2021) and Prompt-tuning (Lester et al.,
2021) conditions frozen language models via train-
able virtual token embeddings. Adapters (Houlsby
et al., 2019; He et al., 2021; Wang et al., 2022)
insert trainable adapter layers between existing
layers in neural networks and fine-tune only them.
Hu et al. (2021) introduced LoRA, which using two
low-rank matrices and exclusively fine-tuning LLMs.
However, single LoRA cannot flexibly combine dif-
ferent computational modules in downstream tasks.
We set up multiple LoRAs as distinct experts and
dynamically combine them to achieve better PEFT.
2.2. Mixture-of-Experts
The Mixture of Experts (MoE) integrates the out-
puts of specialized sub-models, referred to as ex-
perts , through an token-dependent router mecha-
nism. Assuming the existence of natural subsets
in the dataset, such as originating from different
domains or topics, a gating network is employed

--- PAGE 3 ---
to determine which expert should be trained. This
enables each network to process a subset of the
entire training dataset, addressing the challenge
of generalization for a single model on complex
datasets.
Shazeer et al. (2017) introduced the Sparsely
Gated Mixture of Expert (MoE) models, employ-
ing a top-k routing strategy to maintain sparsity
while scaling the model parameters. This approach
achieved a parameter scale of 137 billion in RNN-
based networks, while ensuring low computational
costs for both training and inference (e.g., FLOPs,
parameters). By designing loss functions to en-
force expert load balancing, this methodology re-
sulted in state-of-the-art performance in language
modeling and machine translation benchmarks.
Additionally, recent studies by GShard (Lepikhin
et al., 2020), Switch-Transformer (Fedus et al.,
2022), BASELayer (Lewis et al., 2021), and Hash
Layer (Roller et al., 2021) have focused on the de-
velopment of large-scale Transformer-based mod-
els incorporating MoE, alongside the exploration
of optimal training strategies to fully harness the
model’s capacity. In contrast to their work, we inte-
grate MoE into PEFT and validate its effectiveness.
2.3. Contrastive Learning
Contrastive Learning (Hadsell et al., 2006) has
emerged as a powerful paradigm in the field of
unsupervised representation learning. It aims
to learn meaningful representations by maximiz-
ing the agreement between differently augmented
views of the same data. Several studies (Zhuang
et al., 2019; Misra and Maaten, 2020; Chen et al.,
2020) have introduced methods to align the repre-
sentations of various augmentations applied to an
image, leading to notable successes in computer
vision.
Contrastive learning has also proven to be a
successful approach in NLP tasks. For instance,
Conneau et al. (2019) introduced a contrastive
learning framework tailored for acquiring multilin-
gual representations, showcasing its efficacy in
cross-lingual tasks. CERT (Fang et al., 2020) uti-
lizes the method of back-translation to generate
augmented versions of original sentences, while
DeCLUTR (Giorgi et al., 2020) posits that different
segments within a document are similar to each
other. CLEAR (Wu et al., 2020), adopts a structure
with only an encoder, and acquire a noise-invariant
sentence representation.
Furthermore, numerous variants and extensions
of contrastive learning have been introduced to en-
hance its effectiveness. For example, Chen et al.
(2020) introduced SimCLR, which employs a set
of data augmentations and a large batch size to
achieve impressive results on various computer
vision tasks. MoCo (He et al., 2020) introduced amemory bank mechanism to enable more efficient
contrastive learning. In this paper, we introduce
the framework of contrastive learning into the MoE
model, aiming to maximize the discrepancy in out-
put distributions among different experts in order to
capture diverse features in downstream tasks, miti-
gating the random routing phenomenon showed in
Zuo et al. (2021).
3. The Proposed Method
3.1. Framework of MoELoRA
MoELoRA combines the concept of MoE with
LoRA, effectively increasing model parameters
while maintaining the same computational cost to
achieve superior performance. Specifically, our
method is detailed as follows:
Firstly, we consider the traditional MoE archi-
tecture. For an input token x∈Rd, we ob-
tain the weight for each expert through a gat-
ing network G:Rd7→Rn, resulting in G(x) =
[G(x)1, G(x)2, ..., G (x)n], where nrepresents the
number of experts, and G(x)∈Rn. Subsequently,
we utilize these weights to linearly combine the
outputs of different experts, yielding the output yof
the MoE layer:
y=nX
i=1G(x)i⊙Ei(x) (1)
The essence of MoE lies in increasing the
model’s capacity while keeping the number of
parameters for prediction and training constant.
The gating network adopts a Top krouting strat-
egy, where only k≪nweights in G(x)are non-
zero. This means that despite adding more experts,
which increases the overall model parameter count,
only a small number of experts are involved in
computations during both forward and backward
passes, achieving sparsity.
Next, we consider the LoRA structure. Initially,
the input xundergoes a LoRA Dropout operation
to enhance its generalization capability. Subse-
quently, it is projected downwards to r(r≪d)
dimensions through A(x), where rrepresents the
LoRA Rank. Following this, it is projected back up
toddimensions through B(x), and this process
can be represented as:
A(x) =xA (2)
B(x) =xB (3)
LoRA (x) =B(A(x)) =xAB (4)
Where A∈Rd×randB∈Rr×dare weight ma-
trices.
We consider different LoRA modules as experts,
forming the architecture of MoELoRA. For an in-
put sample x, we first utilize the gating network to

--- PAGE 4 ---
generate a weight vector G(x). Subsequently, we
apply these weights to different branches within
each LoRA structure, resulting in multiple fine-
tuned branches, denoted as LoRA i(x). Ultimately,
we obtain the final MoELoRA prediction output by
linearly combining these branches as follows:
MoELoRA (x) =nX
i=1G(x)i⊙LoRA i(x)(5)
3.2. Challenge of MoELoRA
3.2.1. Load Imbalance
Without intervention, Top kMoE often assigns a
large number of tokens to a few experts, while the
remaining experts receive little or no tokens as-
signed (Zuo et al., 2021). This can lead to poor
performance. Therefore, previous work (Shazeer
et al., 2017; Fedus et al., 2022) used Load Balanc-
ing Loss to encourage balanced routing.
3.2.2. Random Routing
The MoE model exhibits a phenomenon, where
the gating network shows no preference for any
specific expert, resulting in a routing process that
appears random. In such cases, due to the fact
that each expert receives tokens generated by ran-
dom routing (Zuo et al., 2021), the content learned
by all experts actually does not differ significantly.
This contradicts the original intention of employ-
ing MoE, which is to break down a large problem
into smaller subproblems, train different experts to
address these subproblems effectively, and then
combine the outputs of these experts. Therefore,
addressing random routing presents a major chal-
lenge that must be overcome in the MoE architec-
ture.
3.3. Auxiliary loss
3.3.1. Load Balancing Loss
During the training process, the gating network
tends to converge towards a state wherein it con-
sistently allocates substantial weights to a limited
subset of experts (Zuo et al., 2021), potentially
resulting in an imbalanced distribution of workload
among them. To address this concern, Shazeer
et al. (2017) and Fedus et al. (2022) proposed the
load-balancing loss and this paper, we adopt the
latter.
Consider a training batch BwithTtokens. Let
firepresent the proportion of tokens assigned to
thei-th expert, i.e.,
fi=1
TX
x∈B1{arg max p(x) =i} (6)LetPibe the average of all Tprobabilities gen-
erated by the gating network for the i-th expert. Pi
can be expressed as:
Pi=1
TX
x∈Bpi(x) (7)
Based on the above equations, fis non-
differentiable while Pis differentiable. The Load
Balancing Loss Llis defined as the dot product
between fandP, making it differentiable, and it
can be represented as:
Ll=nnX
i=1fi(x)·Pi (8)
This loss optimizes "load balancing" from two
perspectives: fcharacterizes the distribution of the
number of tokens assigned to each expert, while
Pdescribes the distribution of the output from the
gating network. When the gating network outputs
an average probability distribution of [1/n···1/n]
for tokens in a batch, Llachieves its minimum
value, which is nPn
i=11/n·1/n= 1.
3.3.2. Experts Contrastive Loss
We introduce contrastive learning to encourage
experts to learn different features and mitigate ran-
dom routing. For each input token, we select the
topkexperts using a gating network, ensuring
that each token is assigned to some experts. To
promote different experts in learning distinct con-
tent from the input x∈RT×d(where T represents
the total number of token batches), an intuitive ap-
proach is as follows: For the Titokens assigned to
expert Ei, they should share a common attribute,
for example, if Eispecializes in processing "verb"
type tokens, then the common attribute among
the tokens assigned to this expert is "verbs." For
these "verb" type tokens, after being processed by
Ei, they should be sufficiently close in the seman-
tic space. Conversely, for two experts EiandEj,
since we expect them to learn different features,
the tokens they process should be far apart in the
semantic space. This can be expressed simply as:
d(Ei(xk), Ei(xm))≪d(Ei(xk), Ej(xn)) (9)
Therefore, we can employ a contrastive learning
approach proposed in He et al. (2020), where the
outputs of the same expert are treated as pos-
itive samples, while the outputs of different ex-
perts are considered negative samples. Given
input x∈RT×d, the expert model outputs E(x) =
[E1(x), E2(x),···, En(x)], where Ei(x)∈Rti×h,
andtirepresents the number of tokens activated by

--- PAGE 5 ---
E1 E2 E3 E4 Experts
Gating Network
Input Hidden
Gate Select+Frozen Weight 
Output
Output 
Hidden
Update QueueQ1qin qout
Q4qin qout
✔
✔
✔
✔❌
❌
❌❌
❌❌❌❌
❌❌
❌
❌
Contrastive LossFigure 2: As shown in the figure, it illustrates the process of calculating the Experts Contrastive Loss.
The example uses a sentence input h∈RT×d, where each token selects the top 2 experts. Initially, each
expert updates its respective queue with tokens selected by that expert. Subsequently, the Contrastive
Loss is computed using the samples from these queues.
thei-th expert, satisfying the relationship T·top k =Pti. As per our definition of positive and neg-
ative samples in expert contrastive learning, let
q∈Ei(x)andk+∈Ei(x).
Ultimately, for the i-th expert, the Experts Con-
trastive Loss can be defined as:
LEi=−X
q̸=k+logexp(q·k+/τ)P
k∈E(x)exp(q·k/τ)(10)
Here, τrepresents the temperature coefficient,
controlling the distribution shape of q·k. When
τincreases, it smoothens the distribution of q·k,
reducing the discriminative power of LEover all
negative samples. Conversely, a lower τvalue
makes the model focus more on the negative sam-
ples during training. In Figure 2, we illustrate the
detailed calculation process of the Experts Con-
trastive Loss.
Finally, the Auxiliary Loss we adopt is defined
as:
L=α· Ll+β· LE (11)
where αandβare hyperparameters.
4. Experiments
4.1. Experimental Setup
4.1.1. Dataset
We evaluated LoRA and MoELoRA and other
adapters on math reasoning and common-sense
reasoning tasks. Our math reasoning dataset,as well as all the rationales for the samples, are
taken from Hu et al. (2023). All rationales for
the samples are generated through zero-shot-
CoT (Kojima et al., 2022) on GPT-3.5, but with-
out undergoing any error filtering. The math rea-
soning tasks includes a total of 6 benchmarks:
AddSub (Hosseini et al., 2014), AQuA (Ling et al.,
2017), gsm8k (Cobbe et al., 2021), MultiArith (Roy
and Roth, 2016), SingleEQ (Koncel-Kedziorski
et al., 2015), and SVAMP (Patel et al., 2021).
The Common-sense tasks we selected includes
5 benchmarks: namely ARC-C, ARC-E (Chollet,
2019), BoolQ (Clark et al., 2019), OBQA (Mihaylov
et al., 2018), and PIQA (Bisk et al., 2020).
4.1.2. Implementation Details
We using the LLaMA-7b (Touvron et al., 2023) as
the Large Language Model. We conducted a com-
parison between Series-Adapter, Parallel-Adapter,
LoRA and MoELoRA. We introduce LoRA or
MoELoRA into the ’q_proj’ and ’p_proj’ of LLaMA.
We set LoRA and MoELoRA with the same num-
ber of trainable parameters, demonstrating that
MoELoRA outperforms LoRA significantly under
the same settings. Subsequently, we conducted
ablation experiments to analyze the various design
components of MoELoRA.
In experiments, as AdapterH (Houlsby et al.,
2019) and AdapterP (Pfeiffer et al., 2020)
are Series adapters, and AdapterP outperforms
AdapterH, we use AdapterP with bottleneck size
768 as Series Adapter. For Parallel-Adapter (Pfeif-
fer et al., 2020), the adapter layers have been
placed in multi-head attention modules with a bot-
tleneck size of 256. For LoRA, we set the LoRA

--- PAGE 6 ---
Task Metric Series-Adapter Parallel-Adapter LoRA MoELoRA GPT -3.5
AddSub Acc 69.6 77.2 84.8 88.6 85.3
AQuA Acc 15.6 9.8 17.6 25.5 38.9
gsm8k Acc 18.5 22.7 31.1 32.6 56.4
MultiArith Acc 88.3 83.3 88.3 95.0 83.8
SingleEQ Acc 79.4 81.3 90.2 94.1 88.1
SVAMP Acc 52.0 57.0 65.0 66.0 69.9
Avg. 53.9 55.2 62.8 67.0 70.4
Param. 200M 200M 18.9M 18.9M 175B
Table 1: Results on math reasoning tasks, "Param." represents the number of trainable parameters.
Series-Adapter, Parallel-Adapter and GPT -3.5 results are taken from Hu et al. (2023)
Rank to R= 36 , while for MoELoRA, we set the
LoRA Rank to R= 32, with a total of n= 8experts,
each having a LoRA Rank of r= 4. This configu-
ration ensured that LoRA and MoELoRA had an
equal number of trainable parameters. For loss,
τis set to 0.07. αandβare set to 0.01. All our
experiments were conducted on a single RTX3090.
4.2. Main Results
Table 1 presents the performance on six math
reasoning tasks benchmarks. In the AddSub,
MoELoRA achieved a higher accuracy compared
to LoRA by 3.8, and it also outperformed GPT -3.5
by 3.3 points. In the case of AQuA, MoELoRA
showed an accuracy improvement of 7.9 over
LoRA. For the gsm8k, MoELoRA’s accuracy ex-
ceeded LoRA by 1.5. In the MultiArit, MoELoRA
demonstrated an accuracy increase of 6.7 com-
pared to LoRA, and it also outperformed GPT -3.5
by 11.2. In SingleEQ, MoELoRA’s accuracy was
3.9 higher than LoRA, and it surpassed GPT -3.5 by
6.0. Finally, in the SVAMP , MoELoRA achieved a
1.0 accuracy improvement over LoRA. Our experi-
ments have demonstrated that, with the same num-
ber of parameters, MoELoRA consistently outper-
forms LoRA in all aspects. On average accuracy,
MoELoRA exhibits a 4.2 improvement over LoRA,
surpassing the baseline LoRA comprehensively.
Furthermore, MoELoRA remains highly competi-
tive even when compared to GPT-3.5, which has
nearly 104times more parameters.
Table 2 showcases the performance of LoRA,
MoELoRA, and GPT-3.5 on five common-sense
reasoning benchmarks. In ARC-C, MoELoRA
achieved an accuracy 1.7 higher than LoRA. In
ARC-E, MoELoRA’s accuracy was 0.3 higher than
LoRA. For BoolQ, MoELoRA surpassed LoRA by
1.1 and also outperformed GPT-3.5 by 0.6. On
OBQA, MoELoRA’s accuracy exceeded LoRA by
1.4 and GPT-3.5 by 8.8. In the case of PIQA,
MoELoRA’s accuracy was 0.9 higher than LoRA
and 0.2 higher than GPT-3.5. Our experiments
have demonstrated that, with the same number of
parameters, MoELoRA exhibits a 1.0% improve-ment over LoRA on the common-sense reasoning
tasks, and it remains competitive compared to GPT -
3.5 on a few benchmarks.
Task Contrastive Loss(w/o) Total
AddSub 86.1 88.6
AQuA 21.6 25.5
gsm8k 30.7 32.6
MultiArith 91.7 95.0
SingleEQ 88.2 94.1
SVAMP 65.5 66.0
Avg. 64.0 67.0
Table 3: Results of ablation experiments on the
Experts Contrastive Loss in the math reasoning
tasks.
Task Contrastive Loss(w/o) Total
ARC-C 71.4 72.2
ARC-E 85.3 85.6
BoolQ 73.6 73.7
OBQA 81.8 83.6
PIQA 83.9 85.6
Avg. 79.2 80.1
Table 4: Results of ablation experiments on the
Experts Contrastive Loss in the common-sense
reasoning tasks.
4.3. Ablation Studies
4.3.1. Ablations on Auxiliary Loss
To validate the effectiveness of our Experts Con-
trastive Loss, we conducted ablation experiments.
Tables 3 display the results of ablation experiments
on math reasoning tasks, and Table 4 presents
the results for common-sense reasoning tasks. In
these experiments, we kept LoRA Rank at R= 32,
with a total of n= 8experts, and utilized the setting
where each token is assigned to the top 2 activated
experts. The experimental outcomes indicate that
removing the expert contrastive loss results in an
average decrease of 3.0 in math reasoning tasks
and an average decrease of 0.9 in common-sense

--- PAGE 7 ---
Task Metric Series-Adapter Parallel-Adapter LoRA MoELoRA GPT -3.5
ARC-C Acc 57.1 57.3 70.5 72.2 79.9
ARC-E Acc 74.5 73.7 85.3 85.6 89.8
BoolQ Acc 63.0 67.9 72.6 73.7 73.1
OBQA Acc 72.4 75.2 82.2 83.6 74.8
PIQA Acc 79.2 76.4 84.7 85.6 85.4
Avg. 69.2 70.1 79.1 80.1 80.6
Param. 200M 200M 18.9M 18.9M 175B
Table 2: Results for mixed training on common-sense reasoning tasks, "Param." represents the number
of trainable parameters.
reasoning tasks. These experiments provide ev-
idence of the significant improvement in perfor-
mance attributed to the Experts Contrastive Loss
in MoELoRA.
4.3.2. Ablations on Selecting Top-k per Token
Task Top-1 Top-2 Top-4
AddSub 86.1 88.6 87.3
AQuA 23.5 25.5 19.6
gsm8k 32.2 32.6 27.7
MultiArith 89.2 95.0 93.3
SingleEQ 91.2 94.1 89.2
SVAMP 65.5 66.0 66.0
Avg. 64.6 67.0 63.9
Table 5: Results of ablation experiments on Select-
ing Top-n per Token in the math reasoning tasks.
Task Top-1 Top-2 Top-4
ARC-C 69.1 72.2 71.2
ARC-E 84.1 85.6 85.0
BoolQ 72.6 73.7 74.0
OBQA 82.4 83.6 81.2
PIQA 83.6 85.6 84.2
Avg. 78.4 80.1 79.1
Table 6: Results of ablation experiments on Select-
ing Top-n per Token in the common-sense reason-
ing tasks.
Simultaneously, we conducted experiments involv-
ing the selection of the top-k experts for each token.
We fixed LoRA Rank at R= 32 and employed a to-
tal of n= 8experts. Surprisingly, we found that the
performance exhibited a significant improvement
when using the top-2 experts, as compared to top-
1 and top-4 experts.Table 5 displays the results for
math reasoning tasks, and Table 6 presents the
outcomes for common-sense reasoning tasks.5. Analysis
5.1. Why The Improvement In
Common-sense Tasks Is So
Inconspicuous?
In Appendix A, we displays four different formats
of benchmarks for common-sense tasks, namely
ARC, BoolQ, OBQA, and PIQA. Each of these
benchmarks necessitates that the LLM possesses
corresponding knowledge, which places significant
demands on the LLM’s pretraining efficacy. Fur-
thermore, during fine-tuning, if knowledge cannot
be effectively injected, then fine-tuning on common-
sense tasks becomes futile. Therefore, the perfor-
mance on common-sense tasks relies more on the
reservoir of knowledge that LLMs have accumu-
lated during the pretraining phase. While PEFT
does have an impact on common-sense tasks, it
ultimately cannot address the issue that LLMs may
lack relevant knowledge.
Geva et al. (2020); Dai et al. (2021) have demon-
strated that Feedforward Networks (FFNs) can be
interpreted as memory networks capable of storing
substantial amounts of knowledge. In moefication,
Zhang et al. (2022) analyzed the activation patterns
of FFNs within Transformer models and discovered
a phenomenon wherein only a small fraction of
neurons are activated for a single input. Their find-
ings corroborate that a Transformer model can be
transformed into an equivalent Mixture-of-Experts
(MoE) model.
5.2. Tracing tokens through Experts
In our math reasoning task, we tracked the token
routing within the MoE to analyze whether the phe-
nomenon of random routing has been mitigated.
Firstly, we traced the routing of all numerical to-
kens in certain layers, as shown in Figure 3. We
observed that there are always a few specific ex-
perts who excel at handling numerical tokens.

--- PAGE 8 ---
Figure 3: The figure displays the routing of all nu-
meric tokens, which are often assigned to specific
experts.
We also observed that for specific numerical
tokens, such as ’2’ or ’4’ in figure 4 and 5, they
are routed to specific experts for processing in
the early layers. However, due to the influence of
the attention mechanism, as the layers progress
and tokens assimilate a wealth of information, their
routing becomes more uniform.
Figure 4: The figure displays the routing of numeri-
cal token ’2’ .
Figure 5: The figure displays the routing of numeri-
cal token ’4’ .
Furthermore, to our surprise, we found that the
load is not particularly balanced. However, upon
closer examination, this is expected because the
dataset inherently contains variations in token fre-
quency. Some tokens appear more frequently in
the dataset, while others occur less often. See
table 7. The differing occurrence frequencies of
tokens in the dataset make achieving load balance
a challenging task. But Load balancing Loss is
still needed, otherwise some experts will not be
assigned tokens from beginning to end.
6. Conclusions and Future Work
We have introduced a novel Parameter-Efficient
Fine-Tuning method called MoELoRA and mitigate
the random routing phenomenon observed in MoE
through contrastive learning. Additionally, we con-
ducted extensive experiments on 11 math reason-
ing and common-sense reasoning datasets. Inmath reasoning, MoELoRA averaged 4.2% higher
performance than LoRA, and in common-sense
reasoning, it averaged 1.0% higher than LoRA. The
results demonstrate that MoELoRA consistently
outperforms LoRA across all tasks. Furthermore,
when compared to the GPT -3.5 model, MoELoRA
demonstrates its competitive performance.
Future Work: In Section 5.1, we mentioned
the limited improvements on common-sense tasks.
Therefore, it may be worthwhile to explore
MoELoRA by reframing common-sense tasks as
knowledge editing tasks. In addition, we can po-
tentially adopt LoRA modules trained on different
tasks for each expert, freeze them, and only train
the gating network.
Token Frequency
top-1 _ 748
top-2 . 421
top-3 _the 371
... ... ...
top-51 _her 46
top-68 _game 24
top-100 _scored 14
Table 7: On the development set of the MultiArith
benchmark, we have conducted a statistical anal-
ysis of token frequencies, specifically focusing on
the fast decay of the token frequency.
7. References
Y onatan Bisk, Rowan Zellers, Jianfeng Gao, Y ejin
Choi, et al. 2020. Piqa: Reasoning about physi-
cal commonsense in natural language. In Pro-
ceedings of the AAAI conference on artificial
intelligence , volume 34, pages 7432–7439.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sas-
try, Amanda Askell, et al. 2020. Language mod-
els are few-shot learners. Advances in neural
information processing systems , 33:1877–1901.
Ting Chen, Simon Kornblith, Mohammad Norouzi,
and Geoffrey Hinton. 2020. A simple framework
for contrastive learning of visual representations.
InInternational conference on machine learning ,
pages 1597–1607. PMLR.
François Chollet. 2019. On the measure of intelli-
gence. arXiv preprint arXiv:1911.01547 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma,

--- PAGE 9 ---
et al. 2022. Scaling instruction-finetuned lan-
guage models. arXiv preprint arXiv:2210.11416 .
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surpris-
ing difficulty of natural yes/no questions. arXiv
preprint arXiv:1905.10044 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton,
Reiichiro Nakano, et al. 2021. Training verifiers
to solve math word problems. arXiv preprint
arXiv:2110.14168 .
Alexis Conneau, Kartikay Khandelwal, Naman
Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Unsupervised cross-lingual representation learn-
ing at scale. arXiv preprint arXiv:1911.02116 .
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui,
Baobao Chang, and Furu Wei. 2021. Knowl-
edge neurons in pretrained transformers. arXiv
preprint arXiv:2104.08696 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman,
and Luke Zettlemoyer. 2023. Qlora: Efficient
finetuning of quantized llms. arXiv preprint
arXiv:2305.14314 .
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei,
Zonghan Yang, Yusheng Su, Shengding Hu,
Yulin Chen, Chi-Min Chan, Weize Chen, et al.
2023. Parameter-efficient fine-tuning of large-
scale pre-trained language models. Nature Ma-
chine Intelligence , 5(3):220–235.
Hongchao Fang, Sicheng Wang, Meng Zhou, Ji-
ayuan Ding, and Pengtao Xie. 2020. Cert: Con-
trastive self-supervised learning for language
understanding. arXiv preprint arXiv:2005.12766 .
William Fedus, Barret Zoph, and Noam Shazeer.
2022. Switch transformers: Scaling to trillion
parameter models with simple and efficient spar-
sity.The Journal of Machine Learning Research ,
23(1):5232–5270.
Mor Geva, Roei Schuster, Jonathan Berant, and
Omer Levy. 2020. Transformer feed-forward
layers are key-value memories. arXiv preprint
arXiv:2012.14913 .
John Giorgi, Osvald Nitski, Bo Wang, and Gary
Bader. 2020. Declutr: Deep contrastive learning
for unsupervised textual representations. arXiv
preprint arXiv:2006.03659 .Raia Hadsell, Sumit Chopra, and Yann LeCun.
2006. Dimensionality reduction by learning an
invariant mapping. In 2006 IEEE computer soci-
ety conference on computer vision and pattern
recognition (CVPR’06) , volume 2, pages 1735–
1742. IEEE.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor
Berg-Kirkpatrick, and Graham Neubig. 2021. To-
wards a unified view of parameter-efficient trans-
fer learning. arXiv preprint arXiv:2110.04366 .
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie,
and Ross Girshick. 2020. Momentum contrast
for unsupervised visual representation learning.
InProceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages
9729–9738.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learn-
ing to solve arithmetic word problems with verb
categorization. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 523–533.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efficient transfer learning for
nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR.
Edward J Hu, Y elong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu
Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong
Bing, and Soujanya Poria. 2023. Llm-adapters:
An adapter family for parameter-efficient fine-
tuning of large language models. arXiv preprint
arXiv:2304.01933 .
Chengsong Huang, Qian Liu, Bill Yuchen Lin,
Tianyu Pang, Chao Du, and Min Lin. 2023.
Lorahub: Efficient cross-task generalization
via dynamic lora composition. arXiv preprint
arXiv:2307.13269 .
Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid,
Yutaka Matsuo, and Yusuke Iwasawa. 2022.
Large language models are zero-shot reason-
ers.Advances in neural information processing
systems , 35:22199–22213.

--- PAGE 10 ---
Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
Ashish Sabharwal, Oren Etzioni, and Siena Du-
mas Ang. 2015. Parsing algebraic word prob-
lems into equations. Transactions of the Associ-
ation for Computational Linguistics , 3:585–597.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong
Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng
Chen. 2020. Gshard: Scaling giant models with
conditional computation and automatic sharding.
arXiv preprint arXiv:2006.16668 .
Brian Lester, Rami Al-Rfou, and Noah Constant.
2021. The power of scale for parameter-efficient
prompt tuning. arXiv preprint arXiv:2104.08691 .
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman
Goyal, and Luke Zettlemoyer. 2021. Base layers:
Simplifying training of large, sparse models. In
International Conference on Machine Learning ,
pages 6265–6274. PMLR.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation.
arXiv preprint arXiv:2101.00190 .
Wang Ling, Dani Yogatama, Chris Dyer, and Phil
Blunsom. 2017. Program induction by ratio-
nale generation: Learning to solve and ex-
plain algebraic word problems. arXiv preprint
arXiv:1705.04146 .
Jialin Liu, Antoine Moreau, Mike Preuss, Jeremy
Rapin, Baptiste Roziere, Fabien Teytaud, and
Olivier Teytaud. 2020. Versatile black-box op-
timization. In Proceedings of the 2020 Ge-
netic and Evolutionary Computation Conference ,
pages 620–628.
Sourab Mangrulkar, Sylvain Gugger, Lysandre
Debut, Younes Belkada, and Sayak Paul.
2022. Peft: State-of-the-art parameter-efficient
fine-tuning methods. https://github.com/
huggingface/peft .
Todor Mihaylov, Peter Clark, Tushar Khot, and
Ashish Sabharwal. 2018. Can a suit of ar-
mor conduct electricity? a new dataset for
open book question answering. arXiv preprint
arXiv:1809.02789 .
Ishan Misra and Laurens van der Maaten. 2020.
Self-supervised learning of pretext-invariant rep-
resentations. In Proceedings of the IEEE/CVF
conference on computer vision and pattern
recognition , pages 6707–6717.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are nlp models really able to solve
simple math word problems? arXiv preprint
arXiv:2103.07191 .Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. Mad-x: An adapter-based
framework for multi-task cross-lingual transfer.
arXiv preprint arXiv:2005.00052 .
Stephen Roller, Sainbayar Sukhbaatar, Jason We-
ston, et al. 2021. Hash layers for large sparse
models. Advances in Neural Information Pro-
cessing Systems , 34:17555–17566.
Subhro Roy and Dan Roth. 2016. Solving gen-
eral arithmetic word problems. arXiv preprint
arXiv:1608.01413 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François
Yvon, Matthias Gallé, et al. 2022. Bloom: A
176b-parameter open-access multilingual lan-
guage model. arXiv preprint arXiv:2211.05100 .
Noam Shazeer, Azalia Mirhoseini, Krzysztof
Maziarz, Andy Davis, Quoc Le, Geoffrey Hin-
ton, and Jeff Dean. 2017. Outrageously large
neural networks: The sparsely-gated mixture-of-
experts layer. arXiv preprint arXiv:1701.06538 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al. 2023. Llama: Open
and efficient foundation language models. arXiv
preprint arXiv:2302.13971 .
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is all you need. Advances in neural informa-
tion processing systems , 30.
Yaqing Wang, Subhabrata Mukherjee, Xiaodong
Liu, Jing Gao, Ahmed Hassan Awadallah, and
Jianfeng Gao. 2022. Adamix: Mixture-of-adapter
for parameter-efficient tuning of large language
models. arXiv preprint arXiv:2205.12410 , 1(2):4.
Jason Wei, Maarten Bosma, Vincent Y Zhao,
Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M Dai, and Quoc V Le. 2021. Fine-
tuned language models are zero-shot learners.
arXiv preprint arXiv:2109.01652 .
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian
Khabsa, Fei Sun, and Hao Ma. 2020. Clear:
Contrastive learning for sentence representation.
arXiv preprint arXiv:2012.15466 .
Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and
Tuo Zhao. 2023. Adaptive budget allocation for
parameter-efficient fine-tuning. arXiv preprint
arXiv:2303.10512 .

--- PAGE 11 ---
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2022. Moefi-
cation: Transformer feed-forward layers are mix-
tures of experts. In Findings of the Association
for Computational Linguistics: ACL 2022 , pages
877–890.
Chengxu Zhuang, Alex Lin Zhai, and Daniel
Yamins. 2019. Local aggregation for unsuper-
vised learning of visual embeddings. In Proceed-
ings of the IEEE/CVF International Conference
on Computer Vision , pages 6002–6012.
Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin
Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao,
and Jianfeng Gao. 2021. Taming sparsely acti-
vated transformer with stochastic experts. arXiv
preprint arXiv:2110.04260 .
A. Case Study on Common-sense
Tasks
To investigate why the improvement on common-
sense tasks is relatively small, we conducted a
case study on several benchmarks for this task, as
listed in Table 8.

--- PAGE 12 ---
Benchmark Question Answer
ARC George wants to warm his hands quickly by rubbing
them. Which skin surface will produce the most heat?The correct answer is an-
swer1.
Answer1: dry palms
Answer2: wet palms
Answer3: palms covered with oil
Answer4: palms covered with lotion
BoolQ Do iran and afghanistan speak the same language?
True or False.The correct answer is true.
OBQA The sun is responsible for The correct answer is an-
swer4.
Answer1: puppies earning new tricks
Answer2: children growing up and getting old
Answer3: flowers wilting in a vase
Answer4: plants sprouting, blooming and wilting
PIQA When boiling butter,when it’s ready, you can The correct answer is so-
lution2.
Solution1: Pour it onto a plate
Solution2: Pour it into a jar
Table 8: Case study displays four different formats of benchmarks for the common-Sense tasks, each of
which essentially requires the LLM to possess relevant knowledge.
