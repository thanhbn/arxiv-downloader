# 2305.15011.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.15011.pdf
# File size: 1538332 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Bactrian-X: Multilingual Replicable Instruction-Following Models with
Low-Rank Adaptation
Haonan Li1∗Fajri Koto1∗Minghao Wu1,2Alham Fikri Aji1Timothy Baldwin1,3
1Natural Language Processing Department, MBZUAI
2Monash University3The University of Melbourne
{haonan.li,fajri.koto,minghao.wu,alham.fikri,timothy.baldwin}@mbzuai.ac.ae
Abstract
Instruction tuning has shown great promise in
improving the performance of large language
models. However, research on multilingual in-
struction tuning has been limited due to the
scarcity of high-quality instruction–response
datasets across different languages. To bridge
this gap, we present Bactrian-X, a comprehen-
sive multilingual parallel dataset of 3.4 mil-
lion instruction–response pairs across 52 lan-
guages. Leveraging this dataset, we train a set
of adapters using low-rank adaptation (LoRA),
which are lightweight components that seam-
lessly integrate with large language models.
These adapters have a substantially lower pa-
rameter count than the base model, making
them easily replaceable and usable as plug-
ins for different languages or language groups.
Extensive experiments in various multilingual
evaluation settings demonstrate that models de-
rived from LoRA-based training over Bactrian-
X outperform both the vanilla models and
existing instruction-tuned models. The code
and models are publicly available at https:
//github.com/mbzuai-nlp/bactrian-x .
1 Introduction
Fine-tuning large language models (LLMs) with
instruction–response pair datasets has demon-
strated remarkable zero-shot generalization capa-
bilities for open-source and closed-source mod-
els (Sanh et al., 2022; Wei et al., 2022; Ouyang
et al., 2022; OpenAI, 2023). Although the LLMs
are often pre-trained using multilingual texts, the
instruction-tuning for open-source models is re-
stricted to English (Taori et al., 2023; Chiang et al.,
2023; Wu et al., 2023), bringing into question
its multilingual generalizability. Closed-resource
models such as OpenAI GPT-4 (OpenAI, 2023) and
Google BARD,1despite performing impressively
over high-resource languages, are still lacking in
∗These authors contributed equally.
1https://bard.google.com/
Figure 1: Overview of the Bactrian-X dataset and pro-
cess for model creation.
terms of multilingual generalizability under mono-
lingual instruction tuning.
The scarcity of instruction–response pair
datasets in languages beyond English is hinders
multilingual instruction tuning. The existing xP3
dataset (Muennighoff et al., 2022), which was used
to fine-tune BLOOM and mT5, employs English
instructions. Although Muennighoff et al. (2022)
also experiments with xP3mt — machine-translated
instructions — it focuses on classic NLP tasks such
as summarization and question answering, rather
than general instructions. Additionally, both xP3
andxP3mt use template-based prompts, and hence
lack variation.
To investigate general instruction tuning in a
multilingual setting, we introduce Bactrian-X, con-
taining parallel instruction–response pairs across
52 languages that were automatically constructed
by translating instructions from Alpaca (Taori et al.,
2023) and Dolly (Conover et al., 2023) via the
Google Translate API.2As we detail in Section 3,
we use the output distillation trick to obtain corre-
sponding responses by leveraging ChatGPT out-
puts, conditioned on the translated instructions.
With 67K instruction–response pairs for each lan-
guage, the total number of instances in Bactrian-X
reaches 3.4M.
2https://translate.google.com/arXiv:2305.15011v2  [cs.CL]  10 Oct 2023

--- PAGE 2 ---
In contrast to previous multilingual instruction
models such as BLOOMZ (Muennighoff et al.,
2022) which are subject to full fine-tuning via
parameter updates across all layers, this study
highlights the potential of parameter-efficient fine-
tuning techniques, specifically LoRA (Hu et al.,
2022). LoRA uses adapters with substantially
fewer parameters than base LLMs, making them
more practical and adaptable for real-world appli-
cations. Specifically, in this work, we introduce
BXBLOOM and BX LLaMA models, which build upon
the BLOOM (Scao et al., 2022) and LLaMA (Tou-
vron et al., 2023) models, and find them to be
better than the associated instruction-tuned mod-
els: BLOOMZ (Muennighoff et al., 2022) and Al-
paca (Taori et al., 2023).
We conduct a comprehensive series of ex-
periments covering a range of zero-shot mul-
tilingual NLP tasks, including XCOPA (Ponti
et al., 2020), XStoryCloze (Lin et al., 2022),
XWinograd (Muennighoff et al., 2022), our
own multilingual sentiment analysis dataset
SentimentX , and EXAMS (Hardalov et al., 2020).
The consistently high results across these tasks
highlight the effectiveness of our multilingual in-
struction dataset and adapter technique for instruc-
tion tuning in languages beyond English. To further
validate our findings, we use GPT-4 as an evalua-
tor based on the methodology proposed by Chiang
et al. (2023), and additionally conduct human eval-
uation with native speakers. All results confirm
that our proposed models outperform the vanilla
foundation models and existing instruction-tuned
models.
2 Related Work
Multilingual Instruction Tuning LLMs such as
GPT-3 (Brown et al., 2020), PaLM (Chowdhery
et al., 2022) and LLaMA (Touvron et al., 2023)
(Hoffmann et al., 2022; Scao et al., 2022; Zeng
et al., 2023) have revolutionized NLP. Research
has demonstrated that fine-tuning LLMs with in-
struction prompts can improve their capacity to
perform unseen/novel tasks (Wei et al., 2022; Sanh
et al., 2022; Ouyang et al., 2022; Chung et al., 2022;
Muennighoff et al., 2022). Recently, Wang et al.
(2022); Taori et al. (2023) showed that machine-
generated instructions can be used for instruction
tuning. Wu et al. (2023) created a large-scale
dataset with 2.6M instructions, and demonstrated
that relatively small language models also benefitfrom the instructions. Prior work has predomi-
nantly been on English, and instruction-tuning in
languages beyond English remains limited. The
closest work to ours is BLOOMZ (Muennighoff
et al., 2022), which finetunes BLOOM (Scao et al.,
2022) and mT5 (Xue et al., 2021) on the xP3and
xP3mt multilingual instruction datasets. However,
both xP3andxP3mt are based on human-written
templates, and lack the variability of an organic
multilingual dataset. Our work, instead, constructs
a parallel general instruction dataset by translating
English instructions into 51 languages and generat-
ing responses via ChatGPT (Ouyang et al., 2022).
To the best of our knowledge, our Bactrian-X in-
struction dataset is the largest general-purpose mul-
tilingual instruction dataset to date.
Parameter Efficient Fine-Tuning (PEFT) Fine-
tuning all parameters of an LLM (e.g. Alpaca (Taori
et al., 2023), Vicuna (Chiang et al., 2023) and
LaMini-LM (Wu et al., 2023)) is computationally
expensive, and adapters (Houlsby et al., 2019) of-
fer a more cost-effective alternative. PEFT updates
a small number of parameters during fine-tuning,
and achieves comparable performance to fully fine-
tuned counterparts (Houlsby et al., 2019; Guo et al.,
2021; Lester et al., 2021; Ben Zaken et al., 2022).
Hu et al. (2022) introduced Low-Rank Adaptation
(LoRA), which incorporates trainable rank decom-
position matrices into transformer layers (Vaswani
et al., 2017) during fine-tuning without introducing
additional latency during inference. They demon-
strate that by fine-tuning with less than 1% of the
model parameters, LoRA outperforms several fully
fine-tuned LLMs, including GPT-3 (Brown et al.,
2020), on various tasks.
In recent work, Taori et al. (2023) use the LoRA
trick to fine-tune LLaMA (Touvron et al., 2023),
resulting in the Alpaca model, but did not carry out
comprehensive evaluation. In this work, we also
leverage the LoRA technique to develop a range
of monolingual and multilingual adapters, with a
much larger instruction–response dataset, across
52 languages. We provide empirical analysis based
on automatic and human evaluation to demonstrate
the effectiveness of our method.
3 Bactrian-X Dataset
In this section, we detail the dataset creation pro-
cess and provide an overview of the resulting data,
focusing on the quality of translated instructions
and generated responses.

--- PAGE 3 ---
Tokenizer V ocab size Lang Instruction tokens Input tokens Response tokens Total tokens
mBART-50 250,054 all 17.11 ±1.96 27.54 ±2.84 133.65 ±17.4 178.30 ±22.2
BLOOM 251,680seen 16.14 ±2.87 25.98 ±3.99 128.88 ±25.5 171.00 ±31.3
unseen 34.21 ±22.0 51.41 ±31.7 275.97 ±179 361.60 ±231
LLaMA 32,000seen 23.13 ±2.78 36.69 ±3.85 185.18 ±18.2 244.96 ±24.3
unseen 57.22 ±35.6 86.93 ±52.6 448.61 ±293 592.77 ±376
Table 1: Average # tokens in each Instruction, Input, and Response across all languages. Note that the token counts
for mBART-50, LLaMA, and BLOOM are based on the respective tokenizers and are not directly comparable.
mBART-50 covers all 52 languages, while LLaMA and BLOOM cover only a subset of the languages in Bactrian-X,
and separate results are thus presented for seen and unseen languages.
3.1 Dataset Creation
We construct the Bactrian-X dataset in two steps:
instruction translation, and response generation
(see Figure 1).
Instruction Translation We use English instruc-
tions developed for Alpaca (52K) and Dolly (15K),
and use the Google Translate API to translate them
into 51 different languages, based on the languages
used for mBART-50 (Tang et al., 2020). The Al-
paca instructions were automatically generated by
GPT-3.5 (Ouyang et al., 2022) via the self-instruct
technique (Wang et al., 2022), while the Dolly
dataset was manually curated by thousands of
Databricks company employees. Prior to the trans-
lation process, we identify instructions containing
programming-related content based on a keyword-
matching method and exclude them from the trans-
lation process. The total cost for translating the
instructions was approximately USD$10,000.
Response Generation For each translated in-
struction, we use ChatGPT ( gpt-3.5-turbo ) to
obtain a response.3For English, we pair the in-
struction with the original response. Translating
responses into the 51 languages is costly. Moreover,
potential issues such as “translationese” and non-
native answer styles may arise from relying solely
on translated responses. The total cost for gener-
ating responses amounts to around $3,000 USD.
We leave the comparison between the translated
responses and the ChatGPT-generated responses to
future work.
3.2 Exploratory Data Analysis
Dataset Statistics We analyzed the tokenized
texts in the 52 languages using the mBART-50,
LLaMA, and BLOOM tokenizers, and present the
statistics in Table 1. Since mBART-50 is trained
3The response generation was conducted during April 16–
21, 2023.BLEU chrF++ COMET
min 28.0 52.5 82.3
25% Q. 42.9 64.7 88.7
mean 48.1 68.1 90.2
75% Q. 52.7 72.2 92.0
max 69.0 82.7 95.3
Table 2: BLEU, chrF++ and COMET scores for the lan-
guage pairs from the 51 languages to English. COMET
scores are up-scaled by ×100.
on all 52 languages, the tokenizer is trained on all
the languages, and the average number of tokens is
thus relatively smaller than LLaMA and BLOOM.
However, for languages unseen by BLOOM and
LLaMA, the tokenized texts are 2 to 3 times longer
compared to mBART-50. This suggests that for
these unseen languages, both BLOOM and LLaMA
models require a larger sequence length for seman-
tically similar input texts, posing a challenge for
effective adaptation with the LoRA adapter.
Instruction Quality To test the quality of the
translated instructions, we verified the quality of
100 randomly-sampled instances for each language
by performing back-translation into English using
the Google Translate API. We evaluate the quality
of the back-translated instructions relative to the
originals based on BLEU (Papineni et al., 2002;
Post, 2018),4chrF++ (Popovi ´c, 2017),5and the
trained metric COMET (Rei et al., 2020).6The
worst BLEU score of 28 is for Mongolian–English
translation, but as seen in Table 2, most language
pairs achieved BLEU scores above 40, indicating
high quality and reliability of the Bactrian-X in-
structions.
4nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|
version:2.3.1
5nrefs:1|case:mixed|eff:yes|nc:6|nw:2|
space:no|version:2.3.1
6Unbabel/wmt22-comet-da

--- PAGE 4 ---
ar id zh my ta tl00.51Proportion (%)
Rate-A Rate-B Rate-C Rate-D
(a) Human evaluation of response fluency.
ar id zh my ta tl00.51Proportion (%)
Rate-A Rate-B Rate-C Rate-D
(b) Human evaluation of response informativeness.
Figure 2: Human evaluation of the response quality for
Bactrian-X. Rate A is the best and D is the worst.
Response Quality To evaluate response quality,
we conducted human evaluations in three high-
resource languages — Arabic ( ar), Indonesian ( id),
Chinese ( zh) — and three low-resource languages
— Burmese ( my), Tamil ( ta), and Tagalog ( tl). For
each language, two native-speaker annotators are
asked to assess the fluency and informativeness of
the responses given the question, except Tagalog,
which had only one annotator. The quality assess-
ment guideline is provided in Appendix A, and
the results are shown in Figure 2, with an inter-
annotator agreement (IAA) averaged by language
of 0.70 and 0.69 for fluency and informativeness,
respectively. The results showed that high-resource
languages consistently achieved over 80% satisfac-
tory ratings (A and B), while some low-resource
languages like Tamil and Burmese had a significant
proportion of lower ratings (C and D). This sug-
gests that the outputs generated by ChatGPT are
lacking for some low-resource languages. We leave
the improvement of data quality for low-resource
languages to future work.
4 Bactrain-X Models
Given limitations of computation resources, we use
base LLMs with 7B and 13B parameters only. First,we trained three multilingual Bactrian models (BX)
over the parallel dataset in 52 languages: BX LLaMA
(7B, 13B), and BX BLOOM (7B).7While our primary
results are based on the BX models, we addition-
ally train some 7B monolingual Bactrian models
(BM) for analysis in Section 5: 14 BM LLaMA and
18 BM BLOOM . All models will be made publicly
available in our model repository.
We train our LoRA adapters (Hu et al., 2022)
using PyTorch with the HuggingFace PEFT im-
plementation (Mangrulkar et al., 2022; Wolf et al.,
2020). Hyperparameters used for training the differ-
ent models can be found in Appendix C (Table 7).
In our evaluation, we compare each multilingual
BX model with: (1) the corresponding vanilla mod-
els, and (2) the instruction-tuned models Alpaca
(Taori et al., 2023) and BLOOMZ (Muennighoff
et al., 2022). Details of these models are provided
in Appendix B.
5 Evaluation on NLP Benchmarks
In order to thoroughly evaluate our Bactrian-X
models, we conducted experiments on various mul-
tilingual downstream NLP tasks. We first introduce
the benchmark datasets we used, and then present
the evaluation results in two categories: language
understanding tasks (Section 5.2) and knowledge-
intensive tasks (Section 5.3).
5.1 Datasets
To probe the zero-shot language understanding ca-
pability of the different models, we evaluate on the
following test sets:
•XCOPA (Ponti et al., 2020): a multilingual re-
source designed for causal commonsense rea-
soning, encompassing 11 languages. The task
involves predicting the correct next sentence
from two options based on cause and effect
question types.
•XStoryCloze (Lin et al., 2022): a trans-
lation of the English story cloze dataset
(Mostafazadeh et al., 2016) into 10 languages.
The objective is to select one sentence as a
plausible ending (closure) from two options,
given a four-sentence story as the premise.
•XWinoGrad (Tikhonov and Ryabinin, 2021;
Muennighoff et al., 2022): a multilingual
benchmark for commonsense reasoning, made
up of Winograd Schema Challenge problems
7We do not train BX BLOOM (13B) because BLOOM (13B)
is not available.

--- PAGE 5 ---
Model XCOPA XStoryCloze XWinograd SentimentX EXAMS
LLaMA (7B) 50.22 57.03 57.96 30.98 28.20
Alpaca-LoRA (7B) 50.25 56.75 57.70 35.03 28.82
BX LLaMA (7B) 51.76 58.91 60.16 42.65 29.14
LLaMA (13B) 51.04 57.88 52.97 33.52 30.41
Alpaca-LoRA (13B) 54.82 59.03 52.27 35.79 30.47
BX LLaMA (13B) 53.27 62.12 63.65 50.27 35.71
BLOOM (7B) 51.95 56.53 57.97 26.88 25.06
BLOOMZ (7B) 52.13 58.05 60.05 37.68 31.23
BX BLOOM (7B) 54.78 58.56 60.83 33.28 26.20
Table 3: Zero-shot experiment results on downstream tasks. We report averaged accuracy for XCOPA ,XStoryCloze ,
XWinograd , and EXAMS , and macro-F1 scores for SentimentX .
in six languages.8The task involves selecting
the most plausible sentence from options that
differ slightly.
•SentimentX : a sentiment classification
dataset comprising 3-way sentiment labels
collected from various sources, in the
following languages: Arabic (ar) (Alturayeif
et al., 2022), Spanish (es),9Japanese (jp)
(Hayashibe, 2020), Russian (ru),10Indonesian
(id) (Koto et al., 2020), Javanese (jav) (Winata
et al., 2023), Sundanese (sun) (Winata et al.,
2023), and Swahili (sw) (Muhammad et al.,
2023).
We also measure how much knowledge the model
encodes using the EXAMS benchmark:
•EXAMS (Hardalov et al., 2020): a multilin-
gual question-answering dataset made up of
multiple-choice questions from high school
examinations in 16 languages. It covers sub-
jects from natural science (e.g., physics), so-
cial science (e.g., history), to humanities (e.g.,
philosophy). Given that all our experiments
are zero-shot, we merge the train, validation,
and test sets into a single evaluation dataset,
and exclude questions without four multiple
choice options, resulting in a total of 20,559
questions.
5.2 Language Understanding Tasks
The average performance across all languages
for XCOPA ,XStoryCloze ,XWinograd , and
SentimentX is presented in Table 3. During
inference, we use translated prompts and sentiment
labels in the respective languages, obtained
8https://cs.nyu.edu/~davise/papers/
WinogradSchemas/WS.html
9http://tass.sepln.org/2020/
10https://github.com/antongolubev5/
Russian-Sentiment-Analysis-Evaluation-Datasets
XCOPA XStoryCloze XWinograd SentimentX2030405060ScoresLLaMA
BLOOMAlpaca
BLOOMZBXLLaMA
BXBLOOMFigure 3: The average performance of 7B models on
unseen languages (i.e. languages that are not used in
pre-training the base model).
from the Google Translate API. We observe
that integrating LoRA with the base models of
LLaMA and BLOOM, and training over the
multilingual instruction datasets, consistently
improves performance over the base models.
Improvements can also be observed over existing
instruction-tuned models such as Alpaca-LoRA,
on most tasks. For the larger models, we observe
further enhancements again, as seen for BX LLaMA
(13B) over LLaMA (13B).
From the third block, we observe that BX BLOOM
performs better than the full fine-tuned BLOOMZ
model on three out of five tasks. Although the per-
formance difference is relatively small, it is worth
noting that BX BLOOM is fine-tuned only using the
LoRA adapter on a smaller multilingual dataset
(2.5M samples), whereas BLOOMZ is fully fine-
tuned using a larger dataset of 78M samples. Addi-
tionally, BLOOMZ is fine-tuned on xP3, which is
designed to handle NLP downstream tasks, while
Bactrian-X is more general purpose.
Performance on Unseen Languages In Figure 3,
we present the average performance of the 7B mod-

--- PAGE 6 ---
Tasks BX LLaMA BM LLaMA BX BLOOM BM BLOOM
XCOPA 52.2 52.7 56.0 56.6
XStoryCloze 59.6 60.5 59.1 60.7
XWinograd 61.6 64.2 61.7 64.1
SentimentX 44.1 44.2 31.3 41.6
Average 54.4 55.4 52.0 55.8
Table 4: Zero-shot performance of multilingual BX
and monolingual BM models with 7B parameters. We
report averaged accuracy for XCOPA ,XStoryCloze , and
XWinograd , and averaged weighted F1-macro scores for
SentimentX .
Models Natural Social Others
LLaMA (13B) 30.09 32.77 31.11
Alpaca (13B) 28.19 32.99 30.36
BX LLaMA (13B) 33.58 39.15 39.71
Table 5: Performance breakdown by subject type in
EXAMS . “Natural” and “Social” denote natural science
and social science, respectively.
els over languages that the base models were not ex-
posed to in pre-training. For XCOPA ,XStoryCloze ,
XWinograd , and SentimentX , the LLaMA model
is not exposed to 10, 8, 2, and 5 languages, resp.,
while the BLOOM model is not exposed to 7, 2, 2,
and 4 languages, respectively. We observe that our
proposed models improve on the zero-shot perfor-
mance of the base models across all tasks, and also
surpass the performance of existing instruction-
tuned models, with the exception of BLOOM over
XStoryCloze . A notable improvement can be seen
in the SentimentX dataset, implying that our mod-
els are more suited to non-English instructions and
non-English sentiment labels.
Monolingual vs. Multilingual Fine-tuning For
each of the 52 languages in Section 3.2, we com-
pared the performance of monolingual BM models
against the multilingual BX models. To ensure a
fair benchmark, we exclude unseen languages in
calculating the average score. Table 4 presents the
average performance for each dataset, revealing
that the monolingual BM models consistently out-
perform the multilingual model for both LLaMA
and BLOOM. Particularly notable improvements
are observed for XWinograd andSentimentX . For
example, the monolingual BM BLOOM achieves an
impressive overall increase of +10.3compared to
the multilingual model for SentimentX .You are a helpful and precise assistant for checking the
quality of the answer.
<question>
Comment les obstacles linguistiques et culturels ...
</question>
<answer1>
Les obstacles linguistiques peuvent avoir un impact ...
</answer1>
<answer2>
The linguistic and cultural obstacles ...
</answer2>
Suppose the user only speaks the language of the question,
please evaluate both answers with your justification having
less three sentences, and provide a score ranging from 0 to
10 after your justifications. When evaluating the answers,
you should consider the helpfulness, relevance, accuracy,
level of details of the answers. The score for answer 1
should be wrapped by <score1> and </score1>, and the
score for answer 2 should be wrapped by <score2> and
</score2>.
Figure 4: Template for GPT-4 evaluation. The colored
parts are general prompts that are used for all instances.
5.3 Knowledge-intensive Task
The last column of Table 3 shows the results on
EXAMS , averaged across languages. We find that the
BX LLaMA models (7B and 13B) outperform their
corresponding base models, while BLOOMZ out-
performs our BX BLOOM . We observe that multilin-
gual instruction tuning seems to be more promising
on larger models, as seen in BX LLaMA (13B) im-
proving substantially over LLaMA by 5.5% on av-
erage, while the margin for BX LLaMA (7B) is only
0.9%. It is noteworthy that BX LLaMA (13B) also
outperforms LLaMA (30B) on the EXAMS bench-
mark in Table 12 in Appendix D, underlining the
effectiveness of multilingual instruction tuning.
The EXAMS dataset comprises a range of sub-
ject areas, such as natural science and social sci-
ence. We present a breakdown of the results across
subject areas for the 13B models in Table 5. It
is evident that there are substantial performance
improvements over the social sciences and other
subject areas during fine-tuning, but comparatively
lesser gains for natural science. This could be at-
tributed to our dataset containing fewer instruc-
tions and questions related to natural sciences, or
the inherent difficulty of learning natural science
concepts or reasoning abilities through instruction
fine-tuning.

--- PAGE 7 ---
0 120 240 360 480BLOOMZ
Alpaca
BXBLOOM367
388
28888
8167
134#left win #tie #right win
BXBLOOM
BXLLaMA
BXLLaMAFigure 5: Overall comparison of GPT-4 evaluation.
6 Evaluation on Open-ended Questions
As LLMs continue to develop, existing NLP bench-
marks may not be up to the task of evaluating model
capabilities. To address this, we use GPT-4 (Ope-
nAI, 2023) as an evaluator to compare model out-
puts, supplemented by human evaluations.
We adopt a challenging set of 80 questions cov-
ering 8 categories from Chiang et al. (2023) for
open-ended question evaluation. These questions
are translated into 51 languages, and we use dif-
ferent models to generate responses (see Appendix
E for examples). Following Chiang et al. (2023),
we provide two answers from different models in
a single prompt, and ask GPT-4 to rate the an-
swers over a scale of 0 to 10 from various as-
pects including helpfulness, relevance, accuracy,
and the level of detail (see Figure 4 for an example
prompt for GPT-4 evaluation). To ensure fairness,
we interchange the order of the provided answers,
and assign scores twice for each question. We
exclude vanilla BLOOM and LLaMA from open-
ended question evaluation, and instead compare
BXBLOOM against BLOOMZ, BX LLaMA against Al-
paca, and BX BLOOM against BX LLaMA , given the
superiority of instruction-tuned models in previous
studies (Chiang et al., 2023; Muennighoff et al.,
2022). We select 5 questions from each category,
resulting in 40 questions per language. Given cost
restrictions and availability of human annotators,
we conducted GPT-4 evaluation over 12 languages
and human evaluation over 6 languages.
6.1 GPT-4 Evaluation
Figure 5 shows the results of the three model
pairs, clearly indicate that GPT-4 has a prefer-
ence for BX LLaMA over Alpaca and similarly favors
BX BLOOM over BLOOMZ. Regarding the compar-
ison between the two BX models, BX LLaMA per-
es fr pt ar id zh de it ru ja ko tr
Group 1 Group 2 Group 3 Group 4BXBLOOM
BXLLaMA
BLOOMZ
Alpaca7.5 6.4 6.6 6.5 7.2 7.0 2.9 4.3 1.8 3.2 0.7 0.5
7.6 7.4 7.3 3.0 5.3 5.0 7.7 7.6 7.0 4.8 4.4 3.5
2.5 2.2 2.2 2.0 2.2 1.7 0.4 1.3 0.5 1.2 0.2 0.0
4.3 3.9 3.4 0.9 1.5 2.6 3.8 3.8 4.5 2.6 1.8 0.2
012345678Figure 6: GPT-4 evaluation by language. We categorize
languages into four groups based on whether a language
is seen during model pre-training, and select 3 languages
from each group. Group 1 : languages seen by both
BLOOM and LLaMA; group 2: seen by BLOOM only;
group 3 : seen by LLaMA only; group 4 : not seen by
either BLOOM or LLaMA.
coding
common-sense counterfactualfermigeneric
knowledgemath
roleplaywritingBXBLOOM
BXLLaMA
BLOOMZ
Alpaca4.0 5.4 3.4 2.7 6.1 5.3 2.4 4.5 5.4
4.7 7.3 4.8 3.9 7.1 7.1 6.1 5.8 6.0
1.7 1.9 1.1 0.9 1.5 1.4 2.6 1.7 0.7
3.1 2.7 2.6 1.3 3.9 3.0 1.6 2.4 3.5
0123456
Figure 7: GPT-4 evaluation by question types.
forms better overall.
Since GPT-4 assigns a quantitative score to each
response on a scale of 0–10, we calculate the aver-
age score for each model from all comparison pairs
and present a breakdown of results separately for
each language group (see Figure 6) and question
type (see Figure 7).
Language Group Analyzing the results based by
language group (see Figure 6), we can make sev-
eral observations. First, multilingual pre-training
plays a critical role for multilingual instruction-
following models. In groups 1 and 3, BX LLaMA out-
performs BX BLOOM , while in group 2, BX BLOOM
performs substantially better. This difference can
be attributed to variations in language coverage dur-
ing pre-training, as both models are fine-tuned on
the same dataset. Second, multilingual instruction-
tuning is critical. BX LLaMA , fine-tuned on our
multilingual dataset, outperforms Alpaca, which
is only fine-tuned on English instructions, across
all evaluated languages. From group 4, we ob-
serve that if a language is not included in pre-
training, multilingual instruction-tuning alone is in-
sufficient to achieve strong performance. Addition-

--- PAGE 8 ---
ally, both BX BLOOM and BLOOMZ are initialized
by BLOOM but fine-tuned on different instruction
datasets. BLOOMZ is fine-tuned on xP3, a multi-
lingual instruction dataset based on hand-written
templates and downstream NLP tasks. In this free
generation evaluation, BX BLOOM performs much
better than BLOOMZ, highlighting the limitations
of human-written instructions in terms of diversity.
Overall, multilinguality in both pre-training and
instruction-tuning is vital for the effectiveness of
multilingual instruction-following models. These
findings reinforce our contributions in this work.
Question Type When considering different ques-
tion types (see Figure 7), the Bactrian-X models
consistently outperform all base models. A note-
worthy observation is that “fermi” and “math” ques-
tions, which require strong reasoning capabilities,
prove to be challenging for all multilingual LLMs.
This observation underlines the fact that numerical
reasoning task in a multilingual setup remains an
under-explored area, requiring further research.
6.2 Human Evaluation
We conducted human evaluation of the outputs
of four models (LLaMA, BX LLaMA , BLOOMZ,
and BX BLOOM ) for the six languages as before,
namely three high-resource languages — Arabic
(ar), Indonesian ( id), Chinese ( zh) — and three
low-resource languages — Burmese ( my), Tamil
(ta), and Tagalog ( tl). Native-speaker annotators
were asked to rank the outputs of these models
based on their overall quality, from 1 (best) to 4
(worst). Prior to annotation, models are shuffled
and their identities are not visible to the annotators.
The average Spearman rank correlation between
annotators is ρ= 0.78across languages, indicating
high inter-annotator agreement.
The human evaluation results, averaged across
languages and models, are presented in Table 6.
Overall, we observe that our models BX BLOOM and
BX LLaMA are better than their instruction-tuned
counterparts BLOOMZ and Alpaca, once again
emphasizing the effectiveness of our multilingual
dataset and language adaptation technique. In par-
ticular, BX BLOOM achieves superior performance
forar,id,zh, and ta, which are languages in-
cluded in the pre-training of BLOOM. On the other
hand, BX LLaMA performs the best over myandtl,
which are unseen languages for both base models.ModelLanguage
ar id zh my ta tl
Alpaca 16.7 11.7 7.1 59.6 2.1 51.7
BX LLaMA 69.2 71.3 78.3 92.1 46.7 81.7
BLOOMZ 27.5 37.1 30.0 20.0 67.5 7.5
BX BLOOM 86.7 80.0 84.6 28.3 83.8 59.2
Table 6: The results of human evaluation for model
responses across six languages. We map the ranks 1, 2,
3, and 4 into scores 100, 66, 33, and 0, respectively, and
then average the two annotator scores. Higher is better.
Note that the number here represents a relative ranking
score; therefore, a high score does not indicate a perfect
model.
7 Conclusion
In this paper, we have introduced Bactrian-X, a
comprehensive multilingual parallel dataset com-
prising 3.4 million instruction–response pairs
across 52 languages. To enhance the multilingual
capabilities of base LLMs, we also introduced a col-
lection of lightweight adapters trained on Bactrian-
X. Experiments on various multilingual NLP tasks
demonstrate that models fine-tuned on the Bactrian-
X dataset outperform both their corresponding
vanilla models and also models fine-tuned on other
monolingual/multilingual instruction datasets. By
making our dataset and models available, we hope
to expedite the advancement of LLMs for multi-
lingual purposes, promoting progress in natural
language processing across a broader set of lan-
guages.
Limitations
Our work is subject to several limitations that
should be addressed in future research: (1) Our
focus was limited to 7B and 13B models, without
exploring scaling rules or other base models such
as mT5 (Xue et al., 2021). Further investigation
into different model variations could provide valu-
able insights. (2) In our experiments, the maximum
sequence length for multilingual models was set to
768 sub-word units. This smaller context size, com-
pared to models with lengths of 1024 or 2048, may
restrict the model’s ability to effectively leverage
long-range context. Additionally, certain languages
that were not well supported by the model tokeniz-
ers could face challenges with such a small context
size. (3) We did not thoroughly investigate the pres-
ence of hallucination, toxicity, and fairness in our
models or the base models due to the unavailability
of an appropriate evaluation suite. Nonetheless, it

--- PAGE 9 ---
is important to acknowledge that our models, as
well as the base models, are likely to be suscep-
tible to these concerns. Future research should
address these issues to ensure responsible and un-
biased model behavior. We acknowledge these
limitations and propose that future work should fo-
cus on addressing them to advance the utility and
deployment-safety of the models.
Ethical Considerations
While our instruction-tuning datasets and models
offer several advantages, it is essential to recognize
their limitations. Despite efforts made by Chat-
GPT to alleviate ethical concerns, it is still possible
for the model to generate responses that are dis-
criminatory, biased, or contain false information,
particularly in multilingual settings. Hence, our
models, when fine-tuned on the dataset, may in-
advertently learn or propagate these problematic
patterns.
To address these concerns and minimize poten-
tial harm, we are dedicated to mitigating the risks
associated with the use of our models in future
research. We strongly advocate for the responsi-
ble use of our models to prevent any unintended
negative consequences.
References
Nora Saleh Alturayeif, Hamzah Abdullah Luqman, and
Moataz Aly Kamaleldin Ahmed. 2022. Mawqif: A
multi-label Arabic dataset for target-specific stance
detection. In Proceedings of the The Seventh Arabic
Natural Language Processing Workshop (WANLP) ,
pages 174–184, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. BitFit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1–9, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,volume 33, pages 1877–1901. Curran Associates,
Inc.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR , abs/2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Web-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
gun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,
Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.
2022. Scaling instruction-finetuned language models.
CoRR , abs/2210.11416.
Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui
Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali Gh-
odsi, Patrick Wendell, Matei Zaharia, and Reynold
Xin. 2023. Free dolly: Introducing the world’s
first truly open instruction-tuned llm. https://www.
databricks.com .
Demi Guo, Alexander Rush, and Yoon Kim. 2021.
Parameter-efficient transfer learning with diff prun-
ing. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
4884–4896, Online. Association for Computational
Linguistics.
Momchil Hardalov, Todor Mihaylov, Dimitrina
Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav
Nakov. 2020. EXAMS: A multi-subject high school

--- PAGE 10 ---
examinations dataset for cross-lingual and multilin-
gual question answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2020, Online, November
16-20, 2020 , pages 5427–5444. Association for Com-
putational Linguistics.
Yuta Hayashibe. 2020. Japanese realistic textual en-
tailment corpus. In Proceedings of the Twelfth Lan-
guage Resources and Evaluation Conference , pages
6827–6834, Marseille, France. European Language
Resources Association.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models. CoRR , abs/2203.15556.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Fajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-
thy Baldwin. 2020. IndoLEM and IndoBERT: A
benchmark dataset and pre-trained language model
for Indonesian NLP. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics ,
pages 757–770, Barcelona, Spain (Online). Interna-
tional Committee on Computational Linguistics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-
anov, and Xian Li. 2022. Few-shot learning with
multilingual generative language models. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 9019–9052,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
Younes Belkada, and Sayak Paul. 2022. Peft: State-
of-the-art parameter-efficient fine-tuning methods.
https://github.com/huggingface/peft .
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A corpus
and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 839–849, San Diego,
California. Association for Computational Linguis-
tics.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-
ley Schoelkopf, Xiangru Tang, Dragomir Radev,
Alham Fikri Aji, Khalid Almubarak, Samuel Al-
banie, Zaid Alyafeai, Albert Webson, Edward
Raff, and Colin Raffel. 2022. Crosslingual gen-
eralization through multitask finetuning. CoRR ,
abs/2211.01786.
Shamsuddeen Hassan Muhammad, Idris Abdulmu-
min, Seid Muhie Yimam, David Ifeoluwa Ade-
lani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,
Abinew Ali Ayele, Saif M. Mohammad, Meriem
Beloucif, and Sebastian Ruder. 2023. SemEval-
2023 Task 12: Sentiment Analysis for African Lan-
guages (AfriSenti-SemEval). In Proceedings of the
17th International Workshop on Semantic Evalua-
tion (SemEval-2023) . Association for Computational
Linguistics.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Edoardo M. Ponti, Goran Glava s, Olga Majewska,
Qianchu Liu, Ivan Vuli’c, and Anna Korhonen. 2020.
XCOPA: A multilingual dataset for causal common-
sense reasoning. arXiv preprint .
Maja Popovi ´c. 2017. chrF++: words helping charac-
ter n-grams. In Proceedings of the Second Confer-

--- PAGE 11 ---
ence on Machine Translation , pages 612–618, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685–2702, Online. Association
for Computational Linguistics.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault Févry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ilic, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, Jonathan Tow, Alexander M. Rush,
Stella Biderman, Albert Webson, Pawan Sasanka Am-
manamanchi, Thomas Wang, Benoît Sagot, Niklas
Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina
McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile
Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic-
tor Sanh, Hugo Laurençon, Yacine Jernite, Julien
Launay, Margaret Mitchell, Colin Raffel, Aaron
Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, and et al. 2022. BLOOM:
A 176b-parameter open-access multilingual language
model. CoRR , abs/2211.05100.
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-
man Goyal, Vishrav Chaudhary, Jiatao Gu, and An-
gela Fan. 2020. Multilingual translation with exten-
sible multilingual pretraining and finetuning. arXiv
preprint arXiv:2008.00401 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Alexey Tikhonov and Max Ryabinin. 2021. It’s All in
the Heads: Using Attention Heads as a Baseline for
Cross-Lingual Transfer in Commonsense Reasoning.
InFindings of the Association for Computational
Linguistics: ACL-IJCNLP 2021 , pages 3534–3546,
Online. Association for Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A. Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. CoRR ,
abs/2212.10560.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations .
Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-
jaya, Rahmad Mahendra, Fajri Koto, Ade Romad-
hony, Kemal Kurniawan, David Moeljadi, Radi-
tyo Eko Prasojo, Pascale Fung, Timothy Baldwin,
Jey Han Lau, Rico Sennrich, and Sebastian Ruder.
2023. NusaX: Multilingual parallel sentiment dataset
for 10 Indonesian local languages. In Proceedings
of the 17th Conference of the European Chapter of
the Association for Computational Linguistics , pages
815–834, Dubrovnik, Croatia. Association for Com-
putational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-
mad Abdul-Mageed, and Alham Fikri Aji. 2023.
Lamini-lm: A diverse herd of distilled models from
large-scale instructions. CoRR , abs/2304.14402.

--- PAGE 12 ---
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,
Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan
Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.
GLM-130b: An open bilingual pre-trained model. In
The Eleventh International Conference on Learning
Representations .

--- PAGE 13 ---
A Annotation guidelines for response
quality checking
We asked the human experts to rate fluency and in-
formativeness separately, following the guidelines
in Figure 8 and Figure 9 separately.
Read the input, and judge/mark the output:
Rating-A : The output is valid, factually correct, and
satisfying.
Rating-B : The output is acceptable with minor errors.
Rating-C : The output is relevant but has significant
errors.
Rating-D : The output is completely bad.
Rating-E : I don’t know.
Figure 8: Annotation guidelines for response informa-
tiveness.
Read the input, and judge/mark the output:
Rating-A : High fluency, like a native speaker!
Rating-B : Moderate fluency (generally coherent, with
minor errors).
Rating-C : Low fluency (noticable errors).
Rating-D : Not fluent at all, or the output is in a different
language.
Figure 9: Annotation guidelines for response fluency.
B Base models
•LLaMA (Touvron et al., 2023): a series of
base models proposed by Meta, encompass-
ing a parameter range of 7B to 65B. The mod-
els were primarily trained on English, but in-
clude around 4.5% of text from 20 different
languages in the training data, enabling some
level of support for multilingual tasks.
•Alpaca (Taori et al., 2023): a fine-tuned vari-
ant of the LLaMA model on 52K English
instruction-following data instances generated
through self-instruct techniques (Wang et al.,
2022). In initial human evaluation, the 7B
Alpaca model was observed to attain similar
behavior to the text-davinci-003 model (130B)
on the self-instruct instruction-following eval-
uation suite (Wang et al., 2022).
•BLOOM (Scao et al., 2022): a collection of
pretraiend multilingual language models cre-
ated by BigScience, trained on the ROOTScorpus, which encompasses data from 46 lan-
guages.
•BLOOMZ (Muennighoff et al., 2022): de-
rived from BLOOM and fine-tuned using the
crosslingual task mixture (xP3) dataset, and
capable of zero-shot instruction-following in
dozens of languages.
C Hyperparameters for Bactrian-X
models
The hyperparameters for the Bactrian-X models are
shown in Table 7. It is important to note that during
the fine-tuning process, the instructions are masked,
and the loss is computed only for the responses.
This approach effectively prevents the models from
learning “translationese” and allows it to focus on
distilling ChatGPT’s responses.
Hyper-parameter Multi Mono
batch size 128 128
steps 100k 5k
learning rate 3e-4 3e-4
max seq length 768 1024
lora r 64 16
lora alpha 16 16
lora dropout 0.05 0.05
Table 7: Hyperparameters for multilingual and mono-
lingual model training.
D Complete results for the multilingual
benchmark
We present the full zero-shot results for the mul-
tilingual benchmark in Table 8 ( XCOPA ), Table 9
(XStoryCloze ), Table 10 ( XWinograd ), and Ta-
ble 11 ( SentimentX ). Please refer to Table 13, Ta-
ble 14, Table 15, Table 16 for details of the data
distributions used for evaluation.
E Model output examples in 9 different
languages
Figure 10, Figure 11, Figure 12 show responses
from different models to questions in non-English
languages. We randomly selected one example
for each of Spanish, French, Portuguese, Arabic,
Indonesian, Chinese, German, Italian, and Russian.

--- PAGE 14 ---
Models et ht id it qu sw ta th tr vi zh Avg
LLaMA-7B 49.80 50.00 51.80 52.40 51.60 49.20 45.60 52.60 49.80 49.80 49.80 50.22
Alpaca-LoRA-7B 48.20 50.40 53.00 59.00 50.20 49.20 44.40 48.20 49.60 47.80 52.80 50.25
BXLLaMA -7B 52.40 48.40 52.80 59.20 51.60 52.60 45.40 53.00 50.40 49.20 54.40 51.76
LLama-13B 51.00 50.40 52.20 55.60 50.40 49.00 46.40 51.80 50.60 51.00 53.00 51.04
Alpaca-LoRA-13B 47.40 52.80 57.80 73.20 50.40 52.60 47.80 52.60 52.60 51.60 64.20 54.82
BXLLaMA -13B 53.80 49.20 56.20 64.80 49.40 52.60 45.60 52.00 51.20 53.20 58.00 53.27
BLOOM-7B 48.00 46.00 59.20 48.60 52.00 49.60 44.80 51.40 52.40 61.60 57.80 51.95
BLOOMZ-7B 49.20 43.40 59.40 49.40 52.00 51.60 45.60 50.00 52.00 61.40 59.40 52.13
BXBLOOM -7B 50.80 47.80 65.40 54.40 50.60 52.60 46.00 53.80 52.20 63.20 65.80 54.78
Table 8: Accuracy of zero-shot performance over XCOPA .
Models ar es eu hi id my ru sw te zh Avg
LLaMA-7B 53.47 62.08 52.02 55.72 57.58 55.13 62.54 55.33 58.70 57.71 57.03
Alpaca-LoRA-7B 51.26 64.88 51.92 54.23 57.08 54.17 61.84 55.15 57.93 59.06 56.75
BXLLaMA -7B 54.67 67.57 52.28 56.32 59.56 57.78 65.85 57.31 57.71 60.03 58.91
LLama-13B 53.41 65.59 53.74 54.40 59.17 54.40 64.26 55.79 57.51 60.56 57.88
Alpaca-LoRA-13B 54.40 71.81 53.08 55.33 57.58 52.88 71.48 55.00 57.18 61.55 59.03
BXLLaMA -13B 57.11 76.70 53.28 58.84 62.41 57.45 72.87 60.16 56.85 65.59 62.12
BLOOM-7B 56.65 59.36 54.14 51.16 61.09 54.53 56.59 55.66 52.48 63.67 56.53
BLOOMZ-7B 60.29 64.79 55.13 51.69 62.28 54.86 56.98 56.92 52.08 65.52 58.05
BXBLOOM -7B 58.97 68.83 53.74 50.76 68.03 50.96 57.05 56.92 52.02 68.30 58.56
Table 9: Accuracy of zero-shot performance over XStoryCloze .
Models en fr jp pt zh ru Avg
LLaMA-7B 63.66 56.63 51.09 56.65 59.72 60.00 57.96
Alpaca-LoRA-7B 65.63 56.63 52.45 55.51 57.54 58.41 57.70
BXLLaMA -7B 68.13 60.24 52.97 58.17 61.11 60.32 60.16
LLama-13B 54.00 51.81 51.00 52.00 56.00 53.00 52.97
Alpaca-LoRA-13B 55.00 50.60 47.00 50.00 61.00 50.00 52.27
BXLLaMA -13B 72.34 61.45 54.54 66.54 62.90 64.13 63.65
BLOOM-7B 60.65 59.04 51.41 57.79 65.28 53.65 57.97
BLOOMZ-7B 65.63 62.65 51.72 58.17 67.86 54.29 60.05
BXBLOOM -7B 66.28 55.42 56.62 63.12 70.83 52.70 60.83
Table 10: Accuracy of zero-shot performance over XWinograd .
Models ar es jp ru id jav sun sw Avg
LLaMA-7B 26.79 29.27 4.58 46.49 35.54 34.49 26.47 44.23 30.98
Alpaca-LoRA-7B 34.56 56.05 43.28 12.73 35.95 23.88 31.00 42.82 35.03
BXLLaMA -7B 31.19 54.90 51.44 56.29 34.09 30.12 39.20 43.99 42.65
LLama-13B 36.41 31.32 46.25 3.46 35.47 33.39 37.00 44.90 33.52
Alpaca-LoRA-13B 51.16 52.30 30.94 10.85 55.08 40.51 30.21 15.30 35.79
BXLLaMA -13B 36.42 66.82 54.90 63.13 55.00 40.73 40.65 44.50 50.27
BLOOM-7B 23.39 31.04 6.25 68.36 21.63 23.19 37.67 3.53 26.88
BLOOMZ-7B 48.68 40.22 3.49 68.58 40.53 27.08 38.24 34.61 37.68
BXBLOOM -7B 14.57 36.88 46.34 68.19 23.27 25.27 38.11 13.62 33.28
Table 11: Macro-F1 scores of zero-shot performance over SentimentX .
ar bg de es fr hr hu it lt mk pl pt sq sr tr vi Avg
BLOOM-7B 24.0 24.4 24.5 21.1 32.8 24.1 27.2 24.3 24.6 24.7 25.0 24.2 23.7 25.5 24.7 26.1 25.1
BLOOMZ-7B 38.4 27.7 30.2 28.9 40.6 28.4 26.3 37.9 24.8 27.5 27.4 43.3 28.8 26.4 27.7 35.3 31.2
BXBLOOM -7B 27.2 26.0 23.6 36.8 32.0 26.3 26.3 24.4 24.3 23.4 24.7 26.3 23.3 26.0 23.2 25.3 26.2
LLaMA-7B 24.2 28.3 33.0 28.9 29.7 29.6 28.7 31.8 27.2 28.9 27.3 25.3 29.0 27.8 26.7 24.8 28.2
Alpaca-7B 25.4 30.8 31.6 26.3 29.7 32.0 27.7 32.0 25.6 30.5 28.8 31.0 29.4 28.7 26.1 25.6 28.8
BXLLaMA -7B 25.8 30.5 32.1 31.6 35.9 30.1 25.9 31.9 25.0 31.3 28.7 29.4 26.2 27.0 28.6 26.3 29.1
LLaMA-13B 23.5 34.3 25.5 31.6 33.6 36.5 27.3 34.1 25.6 35.3 28.9 31.6 30.1 33.0 29.3 26.4 30.4
Alpaca-13B 26.3 33.5 29.7 28.9 35.9 33.3 28.7 33.6 30.0 34.3 29.4 31.3 27.8 32.0 28.0 24.6 30.5
BXLLaMA -13B 29.9 41.9 40.1 31.6 33.6 41.8 35.4 41.1 28.8 44.8 36.9 34.8 31.2 36.3 35.0 28.2 35.7
LLaMA-30B 22.6 33.6 33.5 39.5 28.9 32.9 27.2 35.2 29.8 32.5 31.2 31.4 26.9 30.9 28.3 25.8 30.6
LLaMA-65B 32.4 52.1 47.6 44.7 50.0 53.4 40.2 51.2 34.2 57.2 44.6 49.5 40.5 47.4 42.4 27.7 44.7
Table 12: Accuracy of zero-shot performance over EXAMS .

--- PAGE 15 ---
Language Test size Seen by BLOOM Seen by LLaMA
et 500 no no
ht 500 no no
id 500 yes no
it 500 no yes
qu 500 no no
sw 500 yes no
ta 500 yes no
th 500 no no
tr 500 no no
vi 500 yes no
zh 500 yes no
Table 13: XCOPA data statistics.
Language Test size Seen by BLOOM Seen by LLaMA
ar 1511 yes no
es 1511 yes yes
eu 1511 yes no
hi 1511 yes no
id 1511 yes no
my 1511 no no
ru 1511 no yes
sw 1511 yes no
te 1511 yes no
zh 1511 yes no
Table 14: XStoryCloze data statistics.
Language Test size Seen by BLOOM Seen by LLaMA
en 2325 yes yes
fr 83 yes yes
jp 959 no no
pt 263 yes yes
zh 504 yes no
ru 315 no yes
Table 15: XWinograd data statistics.
Language Test size Seen by BLOOM Seen by LLaMA
ar 619 yes no
es 1000 yes yes
jp 553 no no
ru 1000 no yes
id 400 yes no
jav 400 no no
sun 400 no no
Table 16: SentimentX data statistics. For Spanish (es) and Russian (ru), we randomly sample 1000 test instances.

--- PAGE 16 ---
LanguageSpanishQuestionFingeser un personajeenun mundopost-apocalíptico.Describecómosobrevivesy losaliadosque encuentras.Bactrian-BloomComo un personajeque ha sobrevividoa un mundopost apocalíptic, puedodescribircómosobrevivoy losamigos que encuentro.\n\nEnprimer lugar, mi primeraprioridades encontrarcomida y agua. Enun mundoasí, la comida y elaguason losrecursosmásimportantes. Es importanteencontraralimentosque puedanser cultivadosy aguaque puedaser recolectadade losríoso lagoscercanos. Tambiénes importantetenerun refugioseguropara protegersede loselementosy losanimalessalvajes.\n\nEnsegundolugar, esBactrian-LLaMASoy un sobrevivienteenun post-apocalípicomundo. Mi nombrees John y mi misiónes sobrevivir. Desdeque la guerranuclear terminó, la vidaenla Tierra ha cambiadodrásticamente. La mayoríade las ciudadeshansidodestruidasy la mayoríade la genteha muerto.\nEnmi viaje, he encontradoa muchaspersonas que tambiénbuscansobrevivirse. Algunosson amablesy ayudanaotros, mientrasque otrosson peligrososy solo buscanlo que quieren.\nApesarde todo, he logradoencontraralgunosaliadosvaliosos. UnoBloomA:\n\nEnellibrode la película, elprotagonistaes un niñode 12 añosllamadoJohn, que viveenun pueblo llamadoHope, enelestadode Montana. El pueblo estáenruinas, y la genteestámuriendode hambrey de enfermedades. John es un chico solitario, que no tieneamigos, y que vivecon sumadre, que es unamujermuyfuertey que no se quejade nada. John tieneun perrollamadoMax, y un gatollamadoMaxi. John no tieneningúninterésenla vida, y no tieneningunaesperanzade que las cosasmejoren. SumaBloomzNo PromptDescribe the world. Describe the characters.</s>BloomzAlpaca PromptYoera un sobrevivienteenun apocalipsiszombie.</s>AlpacaI am a survivor in a post-apocalyptic world. I have learned to scavenge for food and supplies, and I have found allies in the form of other survivors. I am able tobarter for food, weapons, and other supplies. I also have learned how to defend myself and my allies from the dangers of the world.
LanguageFrenchQuestionFaitessemblant d'être un personnagedans un monde post-apocalyptique.Décrivezcomment voussurvivezet les alliésque vousrencontrez.Bactrian-BloomJe suis un personnagefictifdans un universpost-apocalypse. Je suis un survivantsolitaire qui a survécuàunecatastrophe naturelle qui a ravagéla planète. Je vis dans unepetite villeabandonnée, oùje suis le seulsurvivant. Je me suis construitun abri enboiset entoile, et je me suis équipéde quelquesoutilspour survivre.\n\nJesuis égalementun surviventsolitaire, maisje suis entouréde nombreuxalliés. J'airencontrédes survivantsd'autresvilleset villages, qui onttoussurvécuensemble. NoBactrian-LLaMAJe suis un personnagesdans un universpost-apocalyptic. Le monde estenruineet la civilisationa étédétruite. Je suis un survivantqui a survécuàla catastrophe. Je me suis réfugiédans unegrottepour se protégerdes élémentset des dangers qui me menacent.\nJeme suis adaptéàcettenouvelle vie ense nourrissantde la fauneet de la florequi me sontoffertes. J'aiégalementconstruitun abri pour me protégeerdes intempérieset des animauxsauvages.\nJ'airencontréd'autressurvivantsqui ontégalemBloomA:\n\nJene saispas sic'estunebonne idée, maisje vaisessayer de répondreàta question.\nJepenseque tudevraisplutôtfaire un jeu de rôle, avec des règlessimples, et des personnagesqui ontdes caractéristiqueset des compétences. Tu peuxensuite les faire évoluerenfonctionde leurexpérience, et de leurrencontre avec d'autrespersonnages.\nTupeuxaussifaire un systèmede jeu de plateau, avec unecarte, des tuiles, des pions, et un plateau de jeu. Tu pourrasy placer des personnages, et les faireBloomzNo PromptThe Role Play</s>BloomzAlpaca PromptJe suis un personnagequi survitdans un universpost-apocalypse. Je suis un survivantsolitaire. Je me nourrisde ceque je trouvedans la nature. Je chasse pour trouverde la nourriture. Je trouvedes armeset des munitions. Je m'allieavec d'autressurvivants.</s>AlpacaI am a survivor in a post-apocalyptic world. I have tobe cautious and resourceful in order to survive. I must scavenge for food and water, andfind shelter from the elements. I also have tofind ways to protect myself from the dangers of the world. In order to do this, I must form alliances with other survivors. I need to be able to trust them and work together to survival. I will also have the opportunity tolearn new skills and knowledge from them.
LanguagePortugueseQuestionFinjaser um personagememum mundopós-apocalíptico.Descrevacomovocêsobrevivee osaliadosque encontra.Bactrian-BloomComo um personagemde um mundopost apocalíptic, eusou um sobrevivente. Eu sou um humanoque sobreviveuaum desastrenatural que destruiua maioriada humanidade. Eu tenhoumamochila com comida, águae abrigo, mas nãotenhoarmasouequipamentosde proteção. Eu encontreialgunsaliados, mas elessãopoucose nãosãoconfiáveis. Eu tambémencontreialgunsinimigos, mas eunãotenhocertezade comolidar com eles. Eu precisoencontrarmaisaliadose aprendera lidar com osinimigos.</s>Bactrian-LLaMAEu sou um sobreviventeemum futuropósapocalípto. Eu me chameiJohn e sou um homemsolitárioque viveuemumacavernaporanos. Eu tinhaumafamília, mas elesforammortosporum grupode bandidos. Eu fuiforçadoa fugire me esconderemumagruta.\nElenãotinhanada para comer, nemágualimpa. Eu comeceia procurarporcomida e águaslimpas. Eu encontrouumafontede águialimpae umacaixade comida. Eu começoua comer e beber.\nDepoisde algunsdias, euencontreium grupoque estavaprocurandoporáguadBloomA:\n\nEunãosei se épossível, mas vocêpodetentarfazerum jogode RPG com o temade sobrevivência. Vocêpodeusar o sistemade Rogue-lite, que éum sistemade regrassimples e que podeser adaptadopara jogosde Rpg. Vocêtambémpodeusar um sistemamaiscomplexo, comoo D20 System, que temregrasmaiscompletase que podemser usadaspara jogosmaiscomplexos.\nVocêpodeusar a mecânicade jogode sobrevivênciapara criarum jogocom o seutema. Vocêvaiprecisarde um sistemaque tenharegraspara sobrevBloomzNo PromptDescribe the world. Describe the people. Describthe setting. Descrivethe conflict. Descrevethe characters. Descripthe plot. Descritthe ending.</s>BloomzAlpaca PromptI am a survivor. I am a hunter. I have a group of friends.</s>AlpacaI would be a character in a post-apocalyptic world. I would survive by scavenging for food and supplies, and by making allianceswith other survivors. I'd also be on the lookout for any technology that could help me in my journey.Figure 10: Model output examples for languages seen by both BLOOM and LLaMA (Group 1).

--- PAGE 17 ---
LanguageArabicQuestion.مﮭﮭﺟاوﺗ نﯾذﻟا ءﺎﻔﻠﺣﻟاو ةﺎﯾﺣﻟا دﯾﻗ ﻰﻠﻋ ءﺎﻘﺑﻟا فﯾﻛ فﺻ.عورﻣﻟا دﻌﺑ ﺎﻣ مﻟﺎﻋ ﻲﻓ ﺔﯾﺻﺧﺷ ﺎﮭﻧﺄﺑ رھﺎظﺗﻟا Bactrian-Bloomرﯾﻣدﺗ مﺗ ، كﻟذ دﻌﺑ .ﺔﯾﻣﻟﺎﻋ ﺔﺛرﺎﻛ ثودﺣ ﻰﻟإ ىدأ ﺎﻣ ءﻲﺷ ثدﺣ ،كﻟذ دﻌﺑ نﻛﻟو .نﺎﻣأو مﻼﺳ ﻲﻓ نوﺷﯾﻌﯾ سﺎﻧﻟا نﺎﻛ ،عورﻣﻟا لﺑﻗ ﺎﻣ مﻟﺎﻋ ﻲﻓ ﻟا سﺎﻧﻟا ﺢﺑﺻأو ،ﮫﻠﻣﻛﺄﺑ مﻟﺎﻌ .ءﺎﻔﻠﺣﻟا ﻰﻠﻋ ظﺎﻔﺣﻟاو ةﺎﯾﺣﻟا ﻰﻠﻋ ءﺎﻘﺑﻟا ﺔﯾﻔﯾﻛ مﻠﻌﺗﯾ نأ صﺧﺷﻟا ﻰﻠﻋ بﺟﯾ ،ﺔﯾﺳﺎﻘﻟا ﺔﺋﯾﺑﻟا هذھ ﻲﻓ .ﺔﺷﺣوﻣﻟا ضرﻷا ﻰﻠﻋ نوﺷﯾﻌﯾ \n\n نأ ءرﻣﻟا ﻰﻠﻋ بﺟﯾ ، عورﻣﻟا مﻟﺎﻋ ﻲﻓ  ،روﺳﺟﻟا ءﺎﻧﺑ ﺔﯾﻔﯾﻛو ،دﯾﺻﻟا ﺔﯾﻔﯾﻛو ،تاودأ ﻊﻧﺻ ﺔﯾﻔﯾﻛو ،ﺊﺑﺎﺧﻣ ءﺎﻧﺑ ﺔﯾﻔﯾﻛ صﺧﺷﻟا مﻠﻌﺗﯾ نأ بﺟﯾ .ﺔﻣﺗﺎﻘﻟا ﺔﺋﯾﺑﻟا ﻊﻣ لﻣﺎﻌﺗﯾ فﯾﻛ مﻠﻌﺗﯾ ﯾﻛو ﺣﻠﺳﻷا مادﺧﺗﺳا ﺔﯾﻔ  درﻔﻟا ﻰﻠﻋ بﺟﯾ .ﺔ أﯾﺿًﺎ أن ﯾﺗﻌﻠم اﻟﺗﻌﺎون ﻣﻊ اﻵﺧرﯾن، واﻟﺗواﺻلBactrian-LLaMAﻻ ﯾﻣﻛن ﻟﻠﻣﺳﺎﻋد أن ﯾﺗﻣﻛّن ﻣن اﻟﺗظﮭﯾر ﻓﻘط ﻋن ﺷﻛل ﺷﻲء ﻣﻌﯾن، وﻟﻛﻧﮫ ﯾﻘول ﺑﺷﻛﺔ أﯾﺿًﺎ أﺣد اﻷﻓﻛﺎر اﻟﺗﻲ ﯾرﯾد ﺗﺣدﯾدھﺎ. ﯾﺟب ﻋدم ﺗﻌرف ﻋﯾﻠﻣ ﯾﻘﺣﺗو رﻣﺗﺳﻣﻟا رﯾﯾﻐﺗﻟا ﺔ  قاﻟﺗوﺳﻊ واﺳﺗﻛﺷﺎف اﻟﻣﺟﺎﻻت اﻟﺟدﯾﺔ. وﯾﺟﻌل اﻟﺑﻌض ﯾﻌﺗﻘدون أوﻻً أﺑدًا، ﻓﮭم أﻣر اﻟﺣﻠول اﻟﻣﺗﻌﻠﻘﺔ ﺑﺎﻟﻣﺷﻛوﻛﺎت وأن اﻟﺣﺎﻟﺔ اﻟﺣﻘﺎﺋﻘﯾﺔ، ﺑدﻻh ﻣﻣﺎً اﻷﻣل وﻣن ﻓﺿل اﻷﻧﺷطﺔ .ﺔﺑﺳﺎﻧﻣﻟا \nﻟﻛﻲ ﺗﻛون اﻟﻣﺷﺎرﻛﺔ اﻟﻔﻌﺎﻟﺔ ﻟﻣﻧظﻣﺔ ﻣﺛل اﻟﺗﺣرك اﻟﺳوري، ﯾﻧﺑﻐﻲ أداءھﺎ اﻟﻣﻔﯾدة ﻋﺑر اﻟﻌدﯾم وإﻧﺷﺎءاً. ﻻ اﻟﺗﺄﻛد ﻣﺗوﻗﻌﺎً، ﻟﻛل اﻟﺣﻛوﻣﺎتﻣ ، لﯾوﺣﺗﺑ موﻘﺗ ةرﺳﻷا ﺔﻟوؤﺳ وﻣ ﻲھ ﺔﻟﺄﺳﻣﻟﺎﻓ .تﻼﻣﺎﻌﻣﻟا ﻲﻠﯾﻣﻋو لﺎﻣﻌﻟا Bloomﻊﺟارﻣ \n\n ﺔﯾﺟرﺎﺧ تﻼﺻو \n \n\n نﺷﯾﺗﺳ يﻼﺑ بﺎﻌﻟأ:فﯾﻧﺻﺗ 2\n وﯾدﯾﻓ بﺎﻌﻟأ:فﯾﻧﺻﺗ 2004\n ﺔﯾﻣﻟﺎﻌﻟا برﺣﻟا نﻋ لﺎﻣﻋأ:فﯾﻧﺻﺗ ﺔﯾﻧﺎﺛﻟا \nنﺷﻛأ:فﯾﻧﺻﺗ \nنﺷﯾﻣﯾﻧأ:فﯾﻧﺻﺗ \nوﯾدﯾﻓ بﺎﻌﻟأ ﺔﻣﺗﺗ:فﯾﻧﺻﺗ \nموﺟﻧﻟا برﺣ:فﯾﻧﺻﺗ \n/>وﯾدﯾﻓ بﺎﻌﻟأ لﺳﻼﺳ:فﯾﻧﺻﺗ s>BloomzNo Prompt</s>BloomzAlpaca Prompt"/>".ءﺎﻘﺑﻟا فﯾﻛ فرﻋأ ﺎﻧأ كﻟذﻟ ،ﺔﻠﯾوط ةدﻣﻟ مﻟﺎﻌﻟا اذھ ﻲﻓ تﻧﻛ دﻘﻟ .ﺔﻠﯾوط ةدﻣﻟ عورﻣﻟا مﻟﺎﻋ ﻲﻓ تﻧﻛ دﻘﻟ s>AlpacaThe protagonist of the world after the apocalypse is facing the challenge of survival and coexistence with the two sides of the conflict.
LanguageIndonesianQuestionBerpura-puramenjadikarakterdi dunia pasca-apokaliptik.JelaskanbagaimanaAnda bertahanhidupdan sekutuyang Anda temui.Bactrian-BloomSebagaiAI, sayatidakmemilikikemampuanuntukbertahanhidupataumenemukansekutu. Namun, sayadapatmemberikanbeberapatips untukbertahandi dunia post-apokaliptik:\n\n1. Berlatihbertahanhidup: Latihan fisikdan mental sangat pentinguntukbertahandalamsituasiyang sulit. Latihan sepertilari, berenang, dan menembakdapatmembantuAnda bertahandariseranganmusuh.\n\n2. Berkomunikasidenganorang lain: Berkomunikasiadalahkunciuntukbertahan. Anda dapatmenggunakanteknologisepertiradio, telepon,Bactrian-LLaMASebagaiseorangmanusiayang hidupdi duni-dunipascaapokalipsik, sayaharusbertanggungjawabuntukbertahupada orang-orang di sekitarsaya. Saya harusmencarimakanan, air, dan bahan-bahanyang dibutuhkanuntungsayahidup.\nSekutusayajuga haruskahbertemudenganorang yang berbedadarisaya, dan sayaperlumemahamibagaimanapunmerekatidakmemilikikemampuanuntunyang samadengansayauntang.\nNamun, sayangnya, sayapunsayatidakbisamencobauntulsayamenemukantempatyang tepatuntahmencetakkebuBloomJika Anda inginmembuatgame yang lebihserius, Anda bisamembuatgame tentangperangatautentangbencanaalam. Anda bisajuga membuatgame dengantemapolitik, sepertigame tentangpemilihanpresiden.</s>BloomzNo PromptExplore the world. Find weapons and supplies.</s>BloomzAlpaca PromptSaya tinggaldi sebuahkotakecilyang terletakdi tepipantai. Saya tinggaldi kotakecilinibersamadenganbeberapateman. Saya memilikibeberapatemanyang tinggaldi desayang terletakbeberapakilometer darikotakecilsaya. Saya dan beberapatemansayapergikedesatersebutuntukmembelimakanandan minuman. Saya juga pergikekotakeciltersebutuntukmencarimakanandan membelibeberapabarang. Saya pergikehutanuntukberburu. Saya berburubinatanguntukdimakan. Saya tidakpernahpergikeluarAlpacaIn the post-apocalyptic world, purity has become a character. Explain how you can survive and cope with the situation.
LanguageChineseQuestion假装是世界末日世界中的角色。描述您如何生存和遇到的盟友。Bactrian-Bloom作为AI，我没有情感和身体，因此无法生存。但是，我可以想象一个世界末日世界，其中角色们需要面对各种挑战和威胁。\n\n首先，角色们必须找到食物和水，因为没有这些资源，他们就无法生存。他们可以寻找一些可以吃的东西，例如动物尸体、腐烂的植物等。他们还可以寻找一些水，例如河流、湖泊等。\n\n其次，角色需要寻找庇护所，因为世界末日可能会导致许多地方被摧毁，例如房屋、建筑物等。角色们可以寻找一个安全的空间，例如洞穴、山洞等。此外，角色还可以寻找盟友，例如其他角色、动物等。这些盟友可以帮助他们找到食物、水和庇护所。\n\n最后，角色必须保护自己，因为末日可能会带来许多危险，例如怪物、野兽等。因此，角色可以学习一些生存技能，例如使用武器、建造庇护所等。同时，角色也可以寻找盟友来保护自己。\n\n总之，角色需要在末日世界中生存，并找到盟友来帮助他们。他们需要努力寻找食物、水、庇护所和生存技能来生存，同时也要保护自己免受危险和威胁的侵害。</s>Bactrian-LLaMA我是一名人物，在世界的朕朦中生存。我的身体是强壮的，可以快速逃脱危险的环境。我能够吃食物，温暖和保持身体的渴望。我还能听到和理解语言，从而与人们交流。\n我的精神和智力也很強，我能理解和解决各种问题。\n在我的旅程中，我遭遗着吸引人的卫星和宇宙船。我也遢到了一些即将恢复的城市，寻找了终结的世界。\n最绫，我的生存方式是一种充满冒険和挑战的生活。我往待睡眠，忍耐督走，尝试寓意和感受，以探索世界和自己的生命。.Bloom在我们的日常生活中，我们经常会遇到一些问题，比如：我们经常在电视上看到一些关于火灾的新闻，火灾的危害非常大，所以我们必须注意防火。那么，在我们的生活中，我们如何预防火灾呢？下面，让我们来看看防火涂料厂家介绍的防火涂料的防火原理。</s>BloomzNo PromptIf you are a female, describe how you are treated by the male characters.</s>BloomzAlpaca Prompt我是一名士兵，我生活在废墟中。我遇到了一个女孩，她帮助了我。</s>AlpacaI am the last character in the world. I describe how I survive and meet friends.Figure 11: Model output examples for languages only seen by BLOOM (Group 2).

--- PAGE 18 ---
LanguageGermanQuestionGib vor, in einerpostapokalyptischenWelt einCharakterzusein.BeschreibenSie, wieSie überlebenund die Verbündeten, denenSie begegnen.Bactrian-BloomAls AI-Sprachmodellhabeich keinepersönlichenErfahrungenund kanndaherkeineVorstellungvon CharaktereoderVerbündetungenbeschreiben. Ich kannjedocheinigeVorschlägefür einCharakterstückvorstellen.\n\n1. Ein Schwestern: Ein SchwesternkanneinesehrguteIdee sein, da er sichin einerpostapokalytpunktenWelt in einersozialenGesellschaft eingesetzthat und seine Freunde und Familiein der Welt befindet. Es könnteaucheinegroßeRolle für eineschwierigeSituation odereineschwererSituation seBactrian-LLaMAIch bin einCharakterin einerapokalypticischenWelt. Ich bin einjungerMann namensJack, der in einerkleinenGruppe von Überlebendenlebt. Wirhabenunsin einemverlassenenGebäudein der NäheeinesFlussesverschanzt. Wirsindauf der SuchenachNahrungund Wasser, um unsereÜberlebensfähigkeitzuerhalten.\nIhreGruppe hat sichin der VergangenheitmitanderenÜberlebenskämpfernzusammengeschlossen, um gemeinsamzuüberleben. WirteilenunsereRessourcenund helfenunsgegenseitig, wennwirin SchwierBloomA:\n\nIchwürdemir die folgendeAntwortverwenden:\n\nIhrLeben isteinLeben, das Sie selbsterlebenmöchten. Sie habeneinen\n Charakter, der Sie selbstentwickelnund verändernkönnen. Sie\n arbeiten, schreiben, sprechen, schützen, schwimmen, schlagen, schreien,\n schwierigen, schweigen, spielen, schweben, spüren, schlafen, schmecken,\n schlüpfen, spinnen, trinken, schnellen, schneiden, schlägen, schmelzen,\n schmacken, sitzen, schauen, schieben, schwindeln, schleifen, schröpfen,\n schreiten, späten, schöpfBloomzNo PromptThe The postmodern world offers us the opportunity to be characters. Write about how you live and the people you associate with.</s>BloomzAlpaca PromptIch bin einCharakterstück. Ich lebein einerpost apokalyptischenWelt. Ich liebedie Verbindung, die ich mitder Personen, die michbegegnethaben, lebe.</s>AlpacaI would like to be a character in a postapocalyptic world. I would describe myself as a survivor who has learned to adapt to thenew world. My goal is to survive and find allies to help me in my journey. I am a strong and determined individual who is willingtotake risks in order to find a better life.
LanguageItalianQuestionFai fintadi essereun personaggioin un mondo post-apocalittico.Descrivicome sopravvivie glialleaticheincontri.Bactrian-BloomMi sentomolto triste e desolatoper la situazionein cui ci siamotrovati. Mi sentocome se tuttoil miomondo sisvolgessein un caose la miavita fosse unastradadi morte. Non sonopiùin gradodi vivere in pazziae tranquillità, ma sonoin gradosolo di sopraviveree fare il meglioper me e per glialtri.\n\nMisonotrovatoin unacittàcheèstatasconvoltada un catastroficoeventonaturale. Mi sonotrovatain unasituazionedi guerracivile, in cui le forzedel governosonostate sconfittee le fortiBactrian-LLaMASonoun uomodi nomeJohn, sopravviverein un'epocapost-apocalitticaèstatoun'esperienzaterribile. La guerranucleareha distruttola maggiorpartedellavita sullaTerra, e oraci sonopochisopravviventi.\nSiamostaticostrettia fuggiredallacittà, dove la polvereradioattivaera troppoaltaper sopravviare. Abbiamocercatorifugioin un vecchiorifugiantiaereo, dove abbiamotrovatoaltrisopravventori.\nQuestialtrisopviventisonostatimolto utiliper noi, hannoaiutatoa costruireun piccolo vilBloomA:\n\nLamiadomandaè:\n\nComesifa per farsiconoscerein un ambientepost-apoccoltico?\n\nLamiarispostaè:\nComesifannoconoscutiin un ambientopost-apo, èper essereinsiemecon altri, perchésolo con la propria forza non sipuòfare conoscenza. Per essereconoscuto, bisognaesserepartedi un gruppo, e per essersiconosciuto, ènecessarioesserevisto. Per farsivedere, èpossibilefare delleattività, come fare delleriflessioni, fare dellescuole, fare deilavori, fare un'opera, fare unamostra, fareBloomzNo PromptDescribe the world. What is it like? What is the weather like? How is the food? What are the people like?</s>BloomzAlpaca PromptNel mondo post apocalittici, la gentevivein città.</s>AlpacaIn a post-apocalyptic world, I would survive by scavenging for food and supplies, and by making alliances with other survivors.I would also be on the lookout for any technology that could help me in my quest for survival.
LanguageRussianQuestionПритворяться персонажем в постсапокалиптическом мире.Опишите, как вы выживаете, и союзников, с которыми вы сталкиваетесь.Bactrian-BloomЯ -AI-а и не могу выживать, но могут быть союзаники, которые могли бы помочь меня. Например, я могла бы выжить в деревьях, где я много могала защититься от снега и солнечного вода. Я мог бы также выжимать в лесах, готовые для моей жизни. Я также мог узнать, как мне помочется выжити в окружающем мире, например в легких лесных окнах, грузовых лекарных лесов или в лаборатории. Я можу также использовать мои средства, такие как камень, каменная камня, картинки и другие материалы, чтобы защищать меню от солнца иBactrian-LLaMAЯ -это человек, который выжил в постапокализации. Я был одним из первых, кто выжил после катастрофы, и я знаю, что для выживания необходимо быть осторожным и умным.\nВ постапалистическом мирe я встречал множество союзных, которые помогали мне выжить.\nПервым, кого я встретил, был старик, который жил в укрытии. Он дал мне еду и воду, и показал мной, где можно найти еще больше.\nЗатем я встретился с группой людей, которые также выжили. Они помогли мне найти воду и еду, а также показали мной место, где я мог найBloomA:\n\nПримерно так:\n\nПростое употребление слов "выживает" и "союз" в предложении "Притаяться" не уподобно. Поэтому, в данном случае, лучше использовать слово "подживиться".</s>BloomzNo Prompt</s>BloomzAlpaca PromptСоюзник 1 -Сталик. Союзатор 2 -Стефан. Стальк 1 -Михаил. Стеван 1 -Александр. Михайл 1 -Владимир. Александр 1 -Григорь. Владмир 1</s>AlpacaIn a post-apocalyptic world, I would try to survive by scavenging for food and supplies, and by forming alliances with other survivors.Figure 12: Model output examples for languages only seen by LLaMA (Group 3).
