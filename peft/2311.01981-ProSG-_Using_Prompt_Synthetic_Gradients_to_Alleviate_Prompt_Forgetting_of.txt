# 2311.01981.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2311.01981.pdf
# File size: 697473 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of
RNN-like Language Models
Haotian Luo*Kunming Wu*Cheng Daiâ€ 
Sixian Ding Xinhao Chen
Sichuan University
haotianluo2002@gmail.com
Abstract
RNN-like language models are getting renewed attention
from NLP researchers in recent years and several models have
made significant progress, which demonstrates performance
comparable to traditional transformers. However, due to the
recurrent nature of RNNs, this kind of language model can
only store information in a set of fixed-length state vectors.
As a consequence, they still suffer from forgetfulness though
after a lot of improvements and optimizations, when given
complex instructions or prompts. As the prompted generation
is the main and most concerned function of LMs, solving the
problem of forgetting in the process of generation is no won-
der of vital importance. In this paper, focusing on easing the
prompt forgetting during generation, we proposed an archi-
tecture to teach the model memorizing prompt during gener-
ation by synthetic gradient. To force the model to memorize
the prompt, we derive the states that encode the prompt, then
transform it into model parameter modification using low-
rank gradient approximation, which hard-codes the prompt
into model parameters temporarily. We construct a dataset for
experiments, and the results have demonstrated the effective-
ness of our method in solving the problem of forgetfulness in
the process of prompted generation. We will release all the
code upon acceptance.
Introduction
Transformer (Vaswani et al. 2023) has long dominated the
field of NLP across various domains due to its excellent per-
formance and training parallelism, but the O(N)complex-
ity of per step generation and history key-value pairs mem-
ory overhead makes transformers less efficient when the out-
put sequence growing longer. As a consequence, researchers
make a lot of efforts to explore a more efficient structure,
for example, attention-free transformer(AFT) (Zhai et al.
2021). Lately, researchers have successfully developed pow-
erful models that possess both parallel training and recur-
sive inference capabilities, for example, RWKV (Peng et al.
2023) and RetNet (Sun et al. 2023).
Models like RWKV and RetNet combine the advantages
of both transformers and RNNs, ensuring that the training
complexity remains unchanged while achieving an inference
*These authors contributed equally.
â€ Corresponding Authorprocess with a complexity of O(N). Through extensive ex-
periments, this type of model surprisingly demonstrates per-
formance comparable to traditional transformers.
However, it is also because of the recurrent nature of
such models, which means compressing historical informa-
tion into a set of fixed-length state hidden vectors, making
it impossible for the model to retain all historical informa-
tion. Therefore, it will no doubt lead to a series of forgetting
problems. Take prompted generation as an example, given
a prompt: â€œWrite a story about Tomâ€, the result may sat-
isfy our expectations. But when the prompt is not a simple
instruction but consists of two steps of operations, forget-
fulness may occur. For example, given this prompt: â€œWrite
a story about Tom, this story should have a tragic endingâ€
after the model generates relatively long content, it may for-
get the latter part of the given prompt, and not write a tragic
ending as expected. We call this â€prompt forgettingâ€. We
use this term to define the phenomenon where a language
model, when given a specific prompt for generation, either
forgets the specific details of the prompt or does not generate
according to the requirements given by the prompt. Table 1
shows more cases of prompt forgetting.
As enabling model obedience to the prompt is considered
a paramount objective of the most important function of a
language model (Liu et al. 2021), addressing the issue of
prompt forgetting in the process of language model genera-
tion is undoubtedly imperative and calls for a resolution.
For the convenience of description and problem formula-
tion, we have defined the term â€œmulti-stage promptâ€ to refer
to a prompt that consists of more than one sub-prompt or
process (examples were shown in Table 1). Forgetting oc-
curs because the length of the model state vector is fixed.
During the process of inputting text into the model, there
are situations where the model cannot determine whether to
retain the information obtained at the current moment. As
more text is inputted, important information also runs the
risk of being forgotten. This problem is particularly detri-
mental for prompted generation tasks, which is the main
function of large pre-trained language models.
In this paper, aiming at alleviating the forgetfulness of
prompt during generation, we exploit a structure called
ProSG (Prompt Synthetic Gradient), that encodes prompt in-
formation into the model temporarily during one generation
run to boost generation quality. First, in order to model thearXiv:2311.01981v1  [cs.CL]  3 Nov 2023

--- PAGE 2 ---
Sample Prompt Behaviour Output
A Write a story about student Noah,
and write a summary for the story
in the end.Forget to summarize the story in the
end of output.Noah was a bright student who had
always dreamed ... Eventually, he
now works as a doctor at a hospital
where he feels fulfilled with what
he does every day.
B List 10 character names, and write
a story using these names.Exactly list 10 names but forget to
write the story.1. Sophia - A kind-hearted woman.
2. Jack - A charming man who
loves to explore. ... 10. Max -
A trustworthy man who is always
there for his friends and family.
C Given a sequence of numbers: 1, 0,
1, 0, 0, 6, 2, 0, 0, 7, 2, 6, 0, 1, 2, 9.
Now recite it.Forget the given numbers. The sequence is: 0, 1, 0, 1, 2, 0, 0,
1, 2, 6, 7, 2, 9
Table 1: Examples for prompt forgetting
LLM state LLM
promptoutput
Synthetic
Gradient
Figure 1: Architecture Overview.
problem better, we build up a dataset, which contains 21k
multi-stage prompts and corresponding answers following
the common paradigm of instruction fine-tuning. Second,
an adaptive module is utilized to calculate the gradients of
the prompt and apply the change in the following genera-
tion. This adaptive module is a set of neural networks using
low-rank adaption technique (Hu et al. 2021) that take state
which encode prompt as input and generate synthetic gradi-
ents (Jaderberg et al. 2017) as output. The rough architecture
is shown in Figure 1.
We conduct extensive and complete experiments to
evaluate the effectiveness of our novel structure and dataset.
Experimental results show that our method leads to excellent
generation performance in terms of both automatic metrics
and human evaluation. In summary, our contributions are as
follows:
(1)We evaluate the phenomenon of prompt forgetfulness
and build a multi-stage instruction dataset(MuSI);
(2) We propose a framework that facilitates better col-
laboration with datasets by temporarily encoding promptsinto the model parameters, thus significantly alleviating the
issue of prompt forgetting.
Related Work
RNN-like LM
After the efforts of researchers, there are currently two rel-
atively successful RNN-like models, namely RWKV (Peng
et al. 2023) and RetNet (Sun et al. 2023). This kind of model
combines the advantages of both transformers and RNNs,
ensuring that the training parallelism remains unchanged
while achieving an inference process with linear complex-
ity. Although these two models have decent performance, as
RNNs, they are still relatively limited by fixed memory ca-
pacity, which is a direct consequence of fixed state vectors.
This drawback leads to a noticeable forgetting effect, espe-
cially when giving the model multi-stage instruction for gen-
eration. The model forgets information about a given prompt
as generation proceeds, or even before the generation has be-
gun. Though the two models both claim to have a great per-
formance, researchers of RWKV have released their code
for a relatively long time. So we choose RWKV to conduct
most of our research work.
ICL is Implicitly Fine-tuning
ICL(in-context learning) (Dong et al. 2023) is an emergent
ability of large language models that enables them to adapt
to various tasks when given a set of demonstration examples.
This property has been demonstrated in LLMs like GPT-3
(Brown et al. 2020). (Dai et al. 2023) provide a novel view
of explaining ICL as a process of meta-optimization, which
considers LM as a meta-optimizer producing meta-gradients
according to the demonstration examples through forward
computation. Then gradients will be applied to the original
LM through attention to build an ICL pattern.
Inspired by this view, we come up with the idea that con-
sider instructed generation as a process of implicitly fine-
tuning, which means that model can be seen to be optimized
by the gradient from the prompt. So we can synthesize the

--- PAGE 3 ---
Description of the different 
kinds of promptsGPT -3.5
â€¦ 
â€¦â€¦ 
â€¦GPT -3.5â€¦ 
â€¦
Answers given to each promptGive meprompts that satisfy this 
condition: [consists of twosub-instruction],
Here aresome examples:
(1) write astory that...
(2) write apoem that...
Example Phase 1: Prompt generationDifferent kinds of promptsDetective Johnson was known for his 
sharp instincts and unwavering 
determination. â€¦Write a short story about a detective 
solving a mysterious crime.
Phase 2: Anwser generationFigure 2: Dataset Construction.
gradient brought by the prompt to achieve the goal of miti-
gating forgetting.
Synthetic Gradient
Synthetic Gradients (Jaderberg et al. 2017) is a method that
is used for decoupling neural modules, aimed at accelerating
the training process. In traditional neural network training,
gradient information needs to be propagated from the output
layer to the input layer through backpropagation (Rumel-
hart, Hinton, and Williams 1986). Synthetic gradients, how-
ever, approximate the real gradient information by introduc-
ing an additional model. Usually, synthetic gradient model
takes the intermediate layer outputs of the neural network as
input and generates an approximate gradient, which is then
passed to the previous layer of the network. In our frame-
work, we utilize this technique to approximate gradients pro-
duced by prompt.
Low Rank Adaption
LoRA (Hu et al. 2021) is a technique to reduce the com-
putational complexity and memory requirements when fine-
tuning models. LoRA has been proven to deliver excellent
results in various fine-tuning tasks in the field of NLP.
We make an assumption that the modelâ€™s parameters need
only minor modifications to achieve a certain level of mem-
orization of prompt information which have the capability
to enhance generation. So we use low-rank adaption when
calculating prompt synthetic gradient to reduce the memory
overhead of our framework.Method
Dataset
Common Instruction vs. Multi-stage Instruction Most
common instruction consists of only one operation/step, for
example, â€Write a story about Tomâ€. The dataset used by
Alpaca is a typical dataset containing mostly simple instruc-
tions. While multi-stage instruction consists of more than
one operation/step, for example, â€Write a story about Tom,
and also make a summary at the endâ€. For traditional lan-
guage models using transformer architecture, the two kinds
of instruction has slightly different, as the transformer can
preserve complete information about instruction by storing
K-V pairs. However, it is completely different for RNN-like
models as the recurrent nature leads to an inevitable loss
of information during generation, which disables RNN-like
models from â€looking backâ€ the instruction like transform-
ers. As a consequence, the forgetting phenomenon of the
RNN-like model is particularly evident when dealing with
multi-stage instructions.
Multi-stage Instruction Dataset (MuSI) To deal with the
forgetting process better, we leverage ChatGPT to collect
a dataset named Multi-stage Instruction Dataset (MuSI),
which contains a number of multi-stage instructions and cor-
responding answers, and then we manually eliminate non-
compliant data. Finally, we collect 22k pieces of instruction-
answer pair. It should be emphasized that the prompts se-
lected for our dataset contain very little domain-specific
knowledge, allowing for a fairer comparison with other large
language models in public domains after fine-tuning. The

--- PAGE 4 ---
12n-1n
ğ‘¥012n-1n
ğ‘¥1...
...
...
... 12n-1n
ğ‘¥ğ‘›ğ‘¦0
1(+ğ›¥ğ‘Š1)2(+ğ›¥ğ‘Š2)(n-1)(+ğ›¥ğ‘Šğ‘›âˆ’1)n(+ğ›¥ğ‘Šğ‘›)
ğ‘¦01(+ğ›¥ğ‘Š1)2(+ğ›¥ğ‘Š2)(n-1)(+ğ›¥ğ‘Šğ‘›âˆ’1)n(+ğ›¥ğ‘Šğ‘›)
ğ‘¦1ğ‘¦2 ğ‘¦1
...
...
...
...Gradient
Systhesis
... ... ... ... ... ... ...iRNN -layer i
before gradient systhesisOutput Linear Layer
iRNN -layer i
after gradient systhesis
Hidden states
ğ‘¥ğ‘–
ğ‘¦ğ‘–Prompt token
Output token
ğ›¥ğ‘Šğ‘– Prompt gradientFigure 3: Model architecture.
whole construction process of MuSI is illustrated in Figure
2.
Overview Our architecture consists of two parts, one of
them is the backbone RNN-like LM, which is set to be
RWKV as it has released most resources. Another part is
prompt gradient synthesis module G. For each generation,
we first feed the prompt sequence X, which contains n tokes
{x1,x2, ...,xn}into RNN LM, then the hidden states vec-
torHnwhich containing prompt information will be utilized
to generate synthetic gradient. Briefly, the hidden states will
be used by gradient synthesis module as input, and output
the low-rank approximate gradient âˆ†W , which is expected
to consistently convey prompt information in the process
of generation by adding to the original parameter Was an
increment. Thus in the follow-up generation, each forward
pass that produces the k-th output yktoken will be enhanced
with the parameter increment âˆ†W . The whole process can
be formulated as follows:
{H0,H1...,Hn}=RNN (x1,x2, ...,xn) (1)
âˆ†W =G(Hn),yk=RNN (+âˆ†W )(ykâˆ’1) (2)
RWKV RWKV is a RNN-like model that mainly com-
prised of a stack of residual blocks, each formed by a time-
mixing sub-block and a channel-mixing sub-block with re-
current structure. In this paper, we append our gradient syn-
thesis module to the time-mixing block. The time-mixing
block can be formulated as :
rt=WrÂ·(Âµrxt+ (1âˆ’Âµr)xtâˆ’1) (3)
kt=WkÂ·(Âµkxt+ (1âˆ’Âµk)xtâˆ’1) (4)
vt=WvÂ·(Âµvxt+ (1âˆ’Âµv)xtâˆ’1) (5)
wkv t=Ptâˆ’1
i=1eâˆ’(tâˆ’1âˆ’i)w+kivi+eu+ktvtPtâˆ’1
i=1eâˆ’(tâˆ’1âˆ’i)w+ki+eu+kt(6)
ot=WoÂ·(Ïƒ(rt)âŠ™wkv t) (7)The generation process will be modified by our synthetic
gradient after computing prompt gradient. Thus time-mix
computation will be:
rt= (Wr+ âˆ†Wr)Â·(Âµrxt+ (1âˆ’Âµr)xtâˆ’1) (8)
kt= (Wk+ âˆ†Wk)Â·(Âµkxt+ (1âˆ’Âµk)xtâˆ’1)(9)
vt= (Wv+ âˆ†Wv)Â·(Âµvxt+ (1âˆ’Âµv)xtâˆ’1)(10)
wkv t=Ptâˆ’1
i=1eâˆ’(tâˆ’1âˆ’i)w+kivi+eu+ktvtPtâˆ’1
i=1eâˆ’(tâˆ’1âˆ’i)w+ki+eu+kt(11)
ot= (Wo+ âˆ†Wo)Â·(Ïƒ(rt)âŠ™wkv t) (12)
The computation of other parts in RWKV model leave un-
changed.
State Embedding Due to the distinct feature spaces of the
matrices for the states key, value, receptance, and output, we
used the same gradient synthesis module to approximate the
gradients of the four parameters in the same layer. This ap-
proach helped reduce the parameter count and enable paral-
lel computation. This method speeds up computation, but it
also has a fatal flaw: different parameters should receive dif-
ferent updates when calculating prompt synthetic gradient.
To overcome this issue, we introduced state embedding.
State embedding is a set of learned embedding vectors for
each different state that store in each layer, which will be
added to the input xâ–¡before being fed to the core module
(transforming stack S) of gradient synthesis module G.
State embedding enables the use of the same module to
process inputs in four different modes, allowing a single
module to provide synthetic gradients with distinct proper-
ties for the four kinds of state.
Transforming Stack Transforming stack Sis a set of
identical blocks, which is a sequential model which contains
linear layers, activation function, and layer norm. In detail, a
transforming block consists of a linear layer, a ReLU activa-
tion function, another linear layer, and a layer norm. We also
add a residual connection between the output and input. For

--- PAGE 5 ---
Figure 4: Gradient synthesis module.
a single block. The transforming stack is the core module
used for gradient synthesis.
Output Low-rank Matrix The shape of the state vector is
1Ã—E, and the shape of the state matrix is EÃ—E. It should
be noticed that we augment the state to be a vector of shape
1Ã—nE, where nis chosen from 1, 2 or 3. The state vec-
tor remains unchanged after being processed by the trans-
forming stack and then will be reshaped to nÃ—E. Based on
the assumption that prompt memorization can be achieved
to some extent by low-order changes in the model parame-
ter space, we employed the technical of low-rank adaption.
In order to transform the output shape to EÃ—E, we design
a learned output low-rank matrix Bof shape EÃ—n, which
will multiple with the output of transforming stack S.
Gradient Synthesis Module Our gradient synthesis mod-
ule consists of a transforming stack S, a set of state embed-
dinge, and a set of output low-rank matrix B. This module
is shown in Figure 4. Since the detailed design of hidden
state varies with the model architecture, our formulation is
specified with RWKV in order to show module details more
clearly. The state of each layer will be fed into Gradient Syn-
thesis Module Gand calculate gradients. Every layer per-
forms the same operation, in order to formulate the whole
process in detail, we select the i-th layer as an example. In
our experiments, we choose the input xof each layer, which
encodes prompt to add with 4 different state embeddings:
xk=x+ek (13)
xv=x+ev (14)
xr=x+er (15)
xo=x+eo (16)
(17)
where k, v, r, o stand for key, value, receptance and output re-
spectively. And then each xâ–¡will be processed by the trans-
forming stack and produce Aâ–¡:
Aâ–¡=S(xâ–¡) (18)
(19)
After this, low-rank matrix Aâ–¡will multiply with corre-
sponding Bâ–¡and produces âˆ†Wâ–¡.
âˆ†Wâ–¡=Bâ–¡Ã—Aâ–¡ (20)
(21)Training Strategy
Parallel Training During training, the gradient synthesis
module requires access to the hidden states of the encoded
prompt X, specifically the final state vector. To achieve
this, we initially pad the several prompts {X1,X2, ...,Xk},
which has a length of {l1,l2, ...,lk}and input a batch
into the model, thus obtaining the state vectors sequence
{H1,H2, ...,Hk},HiâˆˆRLÃ—E, where Lis the padding
length and Eis channel dimension. However, due to varying
prompt lengths and the introduction of padding tokens, the
information in the vector at the end of the prompt might be
lost. The required is in fact to be {Hl1
1,Hl2
2, ...,Hlk
k},Hj
iâˆˆ
REinstead of {HL
1,HL
2, ...,HL
k}. Therefore, during pro-
cessing, itâ€™s essential to pass the original length of the
prompt. This allows us to use the vector at the position corre-
sponding to the original length of the prompt as the required
state vector and also ensures consistency between training
and inference.
Two-Phase Training We used a two-stage training ap-
proach, where the first stage involved fine-tuning the back-
bone language model, which a training loss, and the sec-
ond stage focused on training the gradient synthesis module
to further enhance its memory capability. Formally, let the
dataset be Dwith size N. the first stage aims to maximize
the log-likelihood (denoted by JRNN) of the target over all
the training samples of D, that is,
JRNN=NX
nTnX
tlogPÎ¸1(xn,t|xn,<t) (22)
where xn,tstands for the t-th word of the n-th sample. Tn
denotes the word length of the sample yn.Î¸1is the modelâ€™s
parameters. The second stage also aims to maximize the log-
likelihood of all training samples, but the parameter of back-
bone model Î¸1is frozen, which can be formulated as fol-
lows:
JS=NX
nTnX
tlogPÎ¸1,Î¸2(xn,t|xn,<t) (23)
In this formula, JSdenotes the log-likelihood of the second
stage, which optimizes the gradient synthesis module, which
has parameter Î¸2

--- PAGE 6 ---
Models Perplexity AccuracyHuman Evaluation
Fluency Accuracy Quality
Vicuna-7B â€“ 0.760 â€“ â€“ â€“
ChatGLM-6B â€“ 0.938 â€“ â€“ â€“
GPT-2-0.4B (MuSI) â€“ 0.893 â€“ â€“ â€“
RWKV-4-0.4B 5.480 0.534 4.03 3.42 3.45
RWKV-4-0.4B (MuSI) 3.583 0.698 4.10 3.57 3.58
RWKV-4-0.4B (MuSI, ProSG) 3.161 0.761 4.22 3.87 3.78
Table 2: Experiment results. PPL of first three language models is not computed. Because they each has different tokenization,
the results are not comparable.
Experiment
Dataset
Due to the lack of a suitable dataset that can effectively meet
the requirements of multi-stage instructions at present, our
experiments are all conducted on the MuSI dataset.
Competing Methods/Models
We will compare the model with models that has a similar
size, and also widely recognized models with much more pa-
rameters which is capable of generating satisfying outputs.
We describe several chosen models as follows:
GPT-2, Vicuna, ChatGLM GPT-2 (Radford et al. 2019),
Vicuna (Chiang et al. 2023), ChatGML (Zeng et al. 2022)
are all pre-trained decoder-only transformers. They have the
capability to comprehend the context of input text and gen-
erate coherent and logical output text. As transformers, they
retain the entire history token sequence. Therefore, we con-
sider these three models as benchmarks.
RWKV For RWKV , We designed three experiments: (1)
Original fin-tuned version of RWKV-4-World; (2) MuSI
fine-tuned; (3) MuSI fine-tuned plus ProSG architecture.
This experimental setup allows us to analyze whether MuSI
and ProSG each play a role in mitigating prompt forgetting.
Evaluation Metircs
We adopted three analytical metrics: Perplexity, Accuracy,
and Human Evaluation.
Perplexity Perplexity, or PPL, is a quantitative measure
that indicates how well a language model can predict a given
set of words based on its training data. Lower perplexity val-
ues suggest that the model is better at making accurate pre-
dictions.
The perplexity of language models is the exponential of
cross-entropy and is used to indicate the predictive capa-
bility of the model. Assuming we have a set of test data
denoted as D, consisting of Nsamples, each represented
asxi, where xiis a tokenized textual sequence. The model
predicts the probability distribution pi(y)for the next token
given each sequence xi, where yrepresents the next token.For each sample xi, the formula to compute cross-entropy
is:
H(xi) =âˆ’1
MMX
j=1logpi(yj)
Here, Mis the number of tokens in sample xi,yjis the j-th
token in the sample, and pi(yj)is the predicted probabil-
ity of the j-th token by the model. Then, sum up the cross-
entropy for all samples to obtain the cross-entropy for the
entire dataset:
H(D) =NX
i=1H(xi)
Finally, perplexity (PPL) can be calculated using cross-
entropy:
PPL (D) =eH(D)
Accuracy We define accuracy as whether the generated
content follows the given prompt. If an output answers all
the questions or considers all the sub-instructions in the
given prompt, this output will receive one point; otherwise,
it will receive a zero.
This evaluation method has the following two advan-
tages:(1)allows us to focus on analyzing whether the modelâ€™s
generation follows the prompt, thereby determining the de-
gree of forgetting during generation. (2) by not focusing on
the modelâ€™s learning of the datasetâ€™s language style during
fine-tuning, we can compare it with other models that have
not been fine-tuned by MuSI.
However, manually analyzing a large volume of model
outputs for compliance is quite challenging and time-
consuming. Therefore, we leverage ChatGPT with appropri-
ate prompts to achieve the goal of a rough automated analy-
sis. Through our validation, this approach has demonstrated
a relatively reliable accuracy as a rough automatic evalua-
tion.
Human Evaluation As we observe that ChatGPT fails to
make correct judgments on certain questions, such as those
involving mathematical calculations, and also for the pur-
pose of achieving a more comprehensive and granular anal-
ysis for three RWKV variations, we conducted manual eval-
uations. We asked five annotators to analyze and compare

--- PAGE 7 ---
Instruction
What are three common types of exercise routines? Briefly describe each one and recommend one for someone looking to
improve their cardiovascular fitness.
RWKV-4-0.4B (MuSI, ProSG)
Three common types of exercise routines are:
1. Cardiovascular exercise: This type ...
2. Strength training: This type ...
3. Yoga: Yoga is a ...
For someone looking to improve cardiovascular fitness, I would recommend a combination of aerobic exercises and
strength training exercises in order to improve cardiovascular fitness and overall health.
RWKV-4-0.4B (MuSI)
Three common types of exercise routines are cardio, strength training, and flexibility exercises.
1. Cardiovascular Exercise: This involves ...
2. Strength Training: This involves ...
3. Flexibility Exercise: This involves ...
RWKV-4-0.4B
1. Walking: Walking is a low-impact exercise that ...
2. Cycling: Cycling is a low-impact exercise that ...
3. Swimming: Swimming is a low-impact exercise that ...
Table 3: Case study
the quality, fluency, and accuracy of the generated results in
a blind fashion. Scoring ranges of 0 to 5 were established for
all three indicators.
These three indicators have the following definition :
1) Fluency : to measure the fluency of generated sentences
and identify any occurrences of repetitive generation.
2) Accuracy : same as that mentioned in the last subsection.
3) Quality : to assess whether the provided answers are
logically coherent, appropriately detailed, and aligned with
human preferences.
Results
Automatic Evaluation Results Table 2 presents the per-
formance of all the models. From the table, we can ob-
serve that RWKV , after fine-tuning with the MuSI dataset,
achieved a higher accuracy score. This indicates that our
dataset indeed mitigated the modelâ€™s forgetting of prompts.
Furtherly, the use of the ProSG architecture enhanced the
modelâ€™s memory capacity, which can be inferred from the
improvement in indicators. Our methods made the modelâ€™s
performance much closer to three transformers, which have
complete memory capabilities. This exactly aligns with our
expectations.
Human Evaluation Results The results of human evalu-
ation are reported in the right part of Table 2. We can see
that, our dataset and architecture both have contributed to
the improvement of the modelâ€™s performance. The results
are consistent with the automatic metric.
Case Study In order to have an intuitive presentation of
the generated results, we select a few prompts and record
the output of several different models, which is shown in
Table 3. Due to space constraints, we only show 1 sam-ple and display the key parts of the results, omitting many
less significant details. The complete results can be found in
the supplementary materials. We can observe a significant
improvement in the modelâ€™s output after incorporating the
ProSG architecture, achieving the desired effect of prompt
memorization. This once again demonstrates the effective-
ness of our framework.
Conclusion
In this paper, we specifically analyze the prompt forgetting
problem during the process of generation. We construct a
multi-stage instruction dataset (MuSI) and propose a gradi-
ent synthesis-based architecture(ProSG) to alleviate prompt
forgetting. Extensive experiments demonstrate that our ap-
proach successfully enhances the modelâ€™s memory capacity
for prompts, enabling better generation tasks under prompt
control.
Limitation
Although our method has achieved promising results, due
to resource limitations, we were unable to conduct related
experiments on larger language models such as 13B or 33B.
Future research should be undertaken to explore the prompt
forgetting and the corresponding solution of large language
models.
References
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,

--- PAGE 8 ---
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Dai, D.; Sun, Y .; Dong, L.; Hao, Y .; Ma, S.; Sui, Z.; and
Wei, F. 2023. Why Can GPT Learn In-Context? Lan-
guage Models Implicitly Perform Gradient Descent as Meta-
Optimizers. arXiv:2212.10559.
Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun,
X.; Xu, J.; Li, L.; and Sui, Z. 2023. A Survey on In-context
Learning. arXiv:2301.00234.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-
tation of Large Language Models. arXiv:2106.09685.
Jaderberg, M.; Czarnecki, W. M.; Osindero, S.; Vinyals, O.;
Graves, A.; Silver, D.; and Kavukcuoglu, K. 2017. Decou-
pled Neural Interfaces using Synthetic Gradients. In Pre-
cup, D.; and Teh, Y . W., eds., Proceedings of the 34th In-
ternational Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , 1627â€“1635.
PMLR.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,
G. 2021. Pre-train, Prompt, and Predict: A Systematic Sur-
vey of Prompting Methods in Natural Language Processing.
arXiv:2107.13586.
Peng, B.; Alcaide, E.; Anthony, Q.; Albalak, A.; Arcad-
inho, S.; Cao, H.; Cheng, X.; Chung, M.; Grella, M.; GV ,
K. K.; He, X.; Hou, H.; Kazienko, P.; Kocon, J.; Kong,
J.; Koptyra, B.; Lau, H.; Mantri, K. S. I.; Mom, F.; Saito,
A.; Tang, X.; Wang, B.; Wind, J. S.; Wozniak, S.; Zhang,
R.; Zhang, Z.; Zhao, Q.; Zhou, P.; Zhu, J.; and Zhu, R.-J.
2023. RWKV: Reinventing RNNs for the Transformer Era.
arXiv:2305.13048.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language Models are Unsupervised
Multitask Learners.
Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. 1986.
Learning representations by back-propagating errors. na-
ture, 323(6088): 533â€“536.
Sun, Y .; Dong, L.; Huang, S.; Ma, S.; Xia, Y .; Xue,
J.; Wang, J.; and Wei, F. 2023. Retentive Network: A
Successor to Transformer for Large Language Models.
arXiv:2307.08621.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2023. At-
tention Is All You Need. arXiv:1706.03762.
Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;
Yang, Z.; Xu, Y .; Zheng, W.; Xia, X.; et al. 2022. Glm-
130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Zhai, S.; Talbott, W.; Srivastava, N.; Huang, C.; Goh, H.;
Zhang, R.; and Susskind, J. 2021. An Attention Free Trans-
former. arXiv:2105.14103.
