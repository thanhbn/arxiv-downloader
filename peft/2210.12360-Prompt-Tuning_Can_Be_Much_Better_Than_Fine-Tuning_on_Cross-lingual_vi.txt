# 2210.12360.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2210.12360.pdf
# Kích thước file: 2091749 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Prompt-Tuning Có Thể Tốt Hơn Nhiều So Với Fine-Tuning Trong Hiểu Biết Đa Ngôn Ngữ
Với Các Mô Hình Ngôn Ngữ Đa Ngôn Ngữ
Lifu Tu và Caiming Xiong và Yingbo Zhou
Salesforce AI Research
{ltu,cxiong,yingbo.zhou}@salesforce.com
Tóm tắt
Các mô hình ngôn ngữ đa ngôn ngữ đã được huấn luyện trước
cho thấy những cải thiện hiệu suất đáng kể cho việc chuyển giao
mô hình đa ngôn ngữ zero-shot trên nhiều tác vụ hiểu biết
ngôn ngữ tự nhiên (NLU). Trước đây, đối với đánh giá
đa ngôn ngữ zero-shot, các mô hình được huấn luyện trước
chỉ được fine-tune trên dữ liệu tiếng Anh và kiểm thử trên
nhiều ngôn ngữ đích khác nhau. Trong bài báo này, chúng tôi
thực hiện đánh giá đa ngôn ngữ trên các tác vụ NLU khác nhau
(phân loại câu, gán nhãn chuỗi, trả lời câu hỏi) sử dụng
prompt-tuning và so sánh với fine-tuning. Kết quả cho thấy
prompt tuning đạt được khả năng chuyển giao đa ngôn ngữ
tốt hơn nhiều so với fine-tuning trên các tập dữ liệu, với
chỉ 0.1% đến 0.3% tham số được điều chỉnh. Ngoài ra,
chúng tôi chứng minh thông qua phân tích rằng prompt tuning
có thể có khả năng chuyển giao biểu diễn đa ngôn ngữ tốt hơn
trên các tác vụ downstream với các ranh giới quyết định
được căn chỉnh tốt hơn¹.

1 Giới thiệu
Các mô hình ngôn ngữ đa ngôn ngữ lớn (Pires et al.,
2019; Wu and Dredze, 2019; Conneau et al.,
2020) cho thấy khả năng chuyển giao đa ngôn ngữ
zero-shot ấn tượng một cách đáng ngạc nhiên trên
các tác vụ NLP, mặc dù chúng chỉ được huấn luyện
từ các corpus đơn ngữ. Gần đây, các benchmark
quy mô lớn như XTREME (Hu et al., 2020) và
XGLUE (Liang et al., 2020) được giới thiệu cho
đánh giá đa ngôn ngữ.

Trong bối cảnh chuyển giao đa ngôn ngữ, các mô hình
chỉ được fine-tune trên các chú thích đặc thù tác vụ
trong một ngôn ngữ và được đánh giá trên các ngôn ngữ
khác. Trong quá trình fine-tuning, các mô hình ngôn ngữ
được huấn luyện trước được sử dụng để khởi tạo và toàn bộ
tham số mô hình được điều chỉnh trên các tác vụ downstream.
Mặc dù fine-tuning đạt được hiệu suất mạnh, nhưng nó
không hiệu quả. Cũng như được chỉ ra trong (Hu et al., 2020),
khoảng cách chuyển giao đa ngôn ngữ giữa hiệu suất
¹Mã nguồn có sẵn tại https://github.com/
salesforce/MPTtrên tập kiểm thử tiếng Anh và tất cả các ngôn ngữ khác là
lớn ngay cả với baseline tốt nhất XLM-R (Conneau
et al., 2020).

Gần đây, prompt tuning, nơi chỉ một lượng nhỏ tham số
bổ sung (tức là prompts) được thêm vào và điều chỉnh,
nhưng mô hình gốc được giữ cố định. Ít tham số hơn
nhiều hoặc không có tham số nào được điều chỉnh và
do đó việc huấn luyện hiệu quả hơn rất nhiều. Prompt tuning
vẫn hoạt động kém hơn fine-tuning trong nhiều tác vụ
NLP (Brown et al., 2020; Shin et al., 2020; Zhong et al., 2021).
Gần đây hơn, Li and Liang (2021); Lester et al. (2021);
Hambardzumyan et al. (2021) chỉ ra rằng prompt tuning
có thể cạnh tranh với fine tuning trên một số tác vụ NLU.
Dung lượng mô hình ngôn ngữ (ví dụ, 10 tỷ tham số) là
thành phần quan trọng để các phương pháp này thành công.
Gần đây hơn, (Liu et al., 2022) cho thấy prompt tuning
cũng có thể so sánh được trên một số tác vụ gán nhãn chuỗi
đơn ngữ khó như trích xuất câu trả lời.

Trong bài báo này, chúng tôi hướng đến việc khảo sát
tác động của prompt tuning trong các tác vụ đa ngôn ngữ.
Chúng tôi đóng băng toàn bộ mô hình ngôn ngữ đa ngôn ngữ
và điều chỉnh các task prompts trên tập huấn luyện tiếng Anh
cho các tác vụ downstream (phân loại câu, dự đoán cấu trúc,
trả lời câu hỏi). Ngay cả với mô hình ngôn ngữ đa ngôn ngữ
kích thước trung bình (ít hơn 1 tỷ tham số), prompt tuning
đạt được hiệu suất cao hơn nhiều so với fine-tuning trên
các tác vụ NLU khác nhau.

Theo kết quả phân tích, prompt tuning thực hiện ít thay đổi
hơn đối với biểu diễn câu so với fine-tuning và giữ được
biểu diễn câu đa ngôn ngữ tốt. Chúng tôi cũng thấy rằng
các ranh giới quyết định của biểu diễn câu các ngôn ngữ
khác nhau sau prompt tuning trên dữ liệu tiếng Anh gần như
được căn chỉnh tốt. Tuy nhiên, các ranh giới quyết định
của các ngôn ngữ khác nhau sau fine-tuning có sự khác biệt
lớn. Những ranh giới quyết định được căn chỉnh này có thể
dẫn đến khả năng chuyển giao đa ngôn ngữ mạnh hơn.

Công trình này làm sáng tỏ khả năng đa ngôn ngữ mạnh
của prompt tuning. Kết quả của chúng tôi gợi ý rằngarXiv:2210.12360v2  [cs.CL]  13 Dec 2022

--- TRANG 2 ---
prompt tuning tốt hơn fine-tuning trong chuyển giao
đa ngôn ngữ. Những đóng góp của chúng tôi được tóm tắt
như sau: chúng tôi cho thấy rằng prompt tuning có thể
hoạt động tốt hơn nhiều so với fine-tuning đối với
chuyển giao đa ngôn ngữ; chúng tôi cũng cho thấy prompt tuning
hoạt động tốt hơn trong trường hợp chuyển giao đa ngôn ngữ
do những thay đổi nhỏ và bền vững tương đối mà nó mang lại
cho các biểu diễn được học ban đầu.

2 Prompt-Tuning cho Các Tác vụ Đa Ngôn Ngữ
Các Mô hình Ngôn ngữ Đa Ngôn ngữ. Trong những
năm qua, nhiều mô hình ngôn ngữ đa ngôn ngữ được
huấn luyện trước đã ra đời: mBERT, XLM (CONNEAU and
Lample, 2019), XLM-R (Conneau et al., 2020), v.v.
XLM-R (Conneau et al., 2020) vượt trội đáng kể so với
multilingual BERT (mBERT; Devlin et al., 2019) trên
nhiều benchmark đa ngôn ngữ XTREME (Hu et al., 2020).
Trong một số công trình trước đây (Luo et al., 2021;
Zhang et al., 2019), XLM-R cũng được sử dụng để khởi tạo
để thực hiện một vòng huấn luyện trước khác với dữ liệu
song song để có được khả năng đa ngôn ngữ mạnh hơn.
Trước đây, trong đánh giá đa ngôn ngữ, các mô hình được
fine-tune trên dữ liệu huấn luyện tiếng Anh nhưng được
đánh giá trên tất cả các ngôn ngữ đích. Theo hiểu biết
của chúng tôi, chúng tôi là những người đầu tiên khám phá
prompt tuning trên một số tác vụ NLP đa ngôn ngữ khó
bao gồm dự đoán cấu trúc và trả lời câu hỏi.

Hình 1: Hai phương pháp khác nhau cho đánh giá đa ngôn ngữ
khi sử dụng mô hình ngôn ngữ đa ngôn ngữ lớn. Trái: Trong
fine-tuning, tất cả tham số mô hình được điều chỉnh trên dữ liệu
tác vụ tiếng Anh. Cài đặt này được sử dụng trong đánh giá
đa ngôn ngữ trước đây. Phải: Trong prompt tuning, chỉ tỷ lệ
nhỏ tham số được điều chỉnh. Chúng tôi sử dụng prefix prompts
và sử dụng layer prompts trong các thí nghiệm của chúng tôi.

Prompt Tuning. Fine-tuning trên các mô hình ngôn ngữ
được huấn luyện trước lớn dẫn đến hiệu suất mạnh trên
các tác vụ downstream, tuy nhiên, nó tiêu tốn bộ nhớ
và nhiều tham số cần được lưu cho mỗi tác vụ. Trong
prompt tuning, chỉ một phần nhỏ tham số (ví dụ, prompts
hoặc bộ phân loại tác vụ) được điều chỉnh trong quá trình
học. Tuy nhiên, nó thường không hoạt động tốt như
so với fine-tuning. Gần đây, Lester et al. (2021) thấy rằng
prompt tuning có thể tốt hơn fine-tuning khi kích thước
mô hình không cực kỳ lớn (10 tỷ tham số). Prefix-tuning
(Li and Liang, 2021) đạt được hiệu suất tương đương
cho các tác vụ sinh ngôn ngữ tự nhiên. Liu et al. (2022)
cho thấy prompt tuning có thể sánh ngang với fine-tuning
trên các tác vụ hiểu biết ngôn ngữ ngay cả ở các tác vụ
gán thẻ chuỗi khó.

Chúng tôi khảo sát prompt tuning trên hiểu biết đa ngôn ngữ
trên một mô hình ngôn ngữ đa ngôn ngữ được huấn luyện trước.
Khung làm việc được hiển thị trong Hình 1. Cài đặt của chúng tôi
tương tự như Li and Liang (2021); Liu et al. (2022).
Các continuous prompts được thêm vào dưới dạng prefix tokens
và được điều chỉnh trong quá trình học. Trong việc triển khai,
các prompts được vận hành dưới dạng past keys và values
trong mỗi lớp transformer. Mỗi lớp transformer có các prompts
riêng biệt. Những continuous prompts này được tối ưu hóa,
nhưng các tham số mô hình ngôn ngữ đa ngôn ngữ được đóng băng.

3 Thiết lập Thí nghiệm
3.1 Tập dữ liệu.
Chúng tôi thực hiện các thí nghiệm trên bốn tập dữ liệu
được bao gồm trong XTREME: cross-lingual natural language
inference (XNLI; Conneau et al., 2018), cross-lingual
adversarial dataset for paraphrase identification
(PAWS-X; Yang et al., 2019), part-of-speech tagging
trên Universal Dependencies (UD-POS; Nivre et al., 2018),
cross-lingual question answering trên XQuAD (Artetxe et al.,
2020) và TyDiQA-GoldP (Clark et al., 2020). Ba loại
tác vụ downstream được bao gồm: (1) phân loại câu;
(2) dự đoán cấu trúc; (3) trả lời câu hỏi.

3.2 Chi tiết Huấn luyện.
Các mô hình đóng băng của chúng tôi được xây dựng trên
đầu checkpoint XLM-R được huấn luyện trước với kích thước
LARGE với khoảng 560M tham số. Công trình trước đây
(Hu et al., 2020) cho thấy nó đạt được hiệu suất mạnh hơn
mBERT². Tất cả các thí nghiệm của chúng tôi được chạy
với Huggingface (Wolf et al., 2020). Các chi tiết khác
có trong phần phụ lục.

Độ dài Prompt. Độ dài prompt thường đóng vai trò quan trọng
trong prompt tuning. Trong các thí nghiệm của chúng tôi,
chúng tôi coi đây là một siêu tham số. Độ dài prompt dài hơn
²Một số kết quả sơ bộ được thu được với mBERT.

--- TRANG 3 ---
MôHình Phân loại Câu Dự đoán Có cấu trúc Trả lời Câu hỏi
XNLI PAWS-X UD-POS XQuAD TyDiQA
Chỉ số Acc. Acc. F1 F1 / EM F1 / EM
Fine Tuning
MBERT* 65.4 81.9 70.3 64.5 / 49.4 59.7 / 43.9
XLM-R-L ARGE * 79.2 86.4 72.6 76.6 / 60.8 65.1 / 45.0
XLM-R-L ARGE+79.2 - 75.0 77.2 / 61.6 64.3 / 45.8
XLM-R-L ARGE (CỦA CHÚNG TÔI) 78.8 (0.2) 87.9 (0.5) 74.4 (0.7) 77.3 (0.4) / 61.8 (0.5) 70.1 (0.6) / 51.7 (2.7)
Prompt Tuning
XLM-R-L ARGE 79.9 (0.1) 88.4 (0.3) 75.4 (0.2) 79.0 (0.2) / 64.1 (0.4) 71.5 (0.4) / 55.1 (0.6)

Bảng 1: Kết quả đánh giá chuyển giao đa ngôn ngữ zero-shot (với độ lệch chuẩn) trên các tác vụ dự đoán có cấu trúc XTREME, trả lời câu hỏi và phân loại câu. Đối với cả fine tuning và prompt tuning, các mô hình chỉ được fine-tune trên dữ liệu huấn luyện tiếng Anh nhưng được đánh giá trên tất cả các ngôn ngữ đích. Kết quả fine-tuning baseline với "*" và "+" được lấy từ (Hu et al., 2020) và (Ruder et al., 2021) tương ứng. Nhiều kết quả khác được hiển thị trong Phụ lục.

thường dẫn đến hiệu suất cao hơn. Trong các thí nghiệm
của chúng tôi, độ dài prompt được đặt là 16 hoặc 32 và
được điều chỉnh trên tập validation tiếng Anh.

4 Kết quả
So sánh Kích thước Tham số Được Điều chỉnh Đối với
kết quả kiểm thử prompt tuning trong Bảng 1, chúng tôi
thực hiện điều chỉnh hạn chế về độ dài prompt. Độ dài
prompt là 16, ngoại trừ độ dài prompt cho tác vụ XNLI là 32.
Chỉ với 0.1% đến 0.3% tham số prompt bổ sung so với
mô hình gốc, khung làm việc đã thể hiện kết quả đa ngôn ngữ
mạnh mẽ.

Kết quả Tổng thể Bảng 1 cho thấy kết quả đa ngôn ngữ
zero-shot trên bốn tác vụ khác nhau. Prompt tuning hoạt động
tốt hơn nhiều so với fine-tuning, đặc biệt là đối với tác vụ
trả lời câu hỏi chuỗi khó. Và prompt tuning cũng có độ
biến thiên nhỏ hơn.

Trước đây, mặc dù có dữ liệu song song hoặc nhiều dữ liệu
đơn ngữ hơn, kết quả chuyển giao đa ngôn ngữ (Zhang et al.,
2019; Luo et al., 2021; Ruder et al., 2021) trên các tác vụ
trả lời câu hỏi và dự đoán có cấu trúc chỉ cải thiện nhẹ.
Với prompt tuning, có những cải thiện hiệu suất lớn hơn
cho các tác vụ trả lời câu hỏi và dự đoán có cấu trúc. Điều này
gợi ý rằng prompt tuning là một phương pháp điều chỉnh tốt hơn
cho chuyển giao đa ngôn ngữ.

Khoảng cách Chuyển giao Đa ngôn ngữ Theo kết quả trên,
trung bình, prompt tuning đạt hiệu suất tốt hơn fine tuning.
Bảng 2 cho thấy khoảng cách chuyển giao đa ngôn ngữ của
hai phương pháp điều chỉnh khác nhau. Prompt tuning cũng
có thể giảm khoảng cách một cách đáng kể.

Thảo luận Trong các thí nghiệm sơ bộ của chúng tôi, đối với
mô hình kích thước nhỏ hơn (ví dụ, mBERT), prompt tuning

XNLI PAWS-X UD-POS XQuAD
Fine Tuning 10.2 12.4 24.3 16.3
Prompt Tuning 9.7 8.7 20.7 14.5

Bảng 2: Khoảng cách chuyển giao đa ngôn ngữ của hai phương pháp điều chỉnh. Khoảng cách chuyển giao đa ngôn ngữ là sự khác biệt hiệu suất giữa tập kiểm thử tiếng Anh và trung bình của các ngôn ngữ khác. Càng nhỏ càng tốt.

hoạt động kém hơn một chút so với fine tuning trên tiếng Anh,
và đạt hiệu suất tương đương với fine-tuning trên tất cả
các ngôn ngữ. Kích thước mô hình ngôn ngữ vẫn quan trọng.
Vẫn còn một số không gian cho mô hình kích thước nhỏ hơn.
Điều này cũng chỉ ra tiềm năng cho công việc tương lai
với phương pháp prompt tuning tốt hơn.

5 Phân tích
Để thực hiện một số phân tích về prompt tuning và
fine tuning, chúng tôi chọn 1000 mẫu cho mỗi ngôn ngữ
(en, de, es, fr, ja, ko, zh) từ tập dữ liệu PAWS-X
(Yang et al., 2019). Đối với mỗi mẫu ngôn ngữ tiếng Anh
trong lựa chọn của chúng tôi, có một mẫu được dịch bởi
con người từ sáu ngôn ngữ khác.³

Hình 2 cho thấy trực quan hóa t-SNE của các biểu diễn
mẫu từ mô hình ngôn ngữ đa ngôn ngữ đóng băng XLM-R.
Các biểu diễn mẫu được phân cụm tốt theo các ngôn ngữ,
tuy nhiên, có sự tương quan yếu với các nhãn.

5.1 Thay đổi Biểu diễn Ngôn ngữ
Đối với mỗi phương pháp điều chỉnh (fine-tuning và
prompt-tuning), Bảng 3 cho thấy độ tương tự cosine của
biểu diễn từ mô hình ngôn ngữ đóng băng và mô hình được
điều chỉnh. Theo kết quả, cả hai phương pháp điều chỉnh
đều tạo ra thay đổi đáng chú ý trên biểu diễn câu. Tuy nhiên,
độ tương tự cosine trung bình của fine-tuning nhỏ hơn
nhiều. Điều này chỉ ra rằng fine-tuning dẫn đến những
thay đổi lớn hơn nhiều trên biểu diễn câu so với prompt tuning.
Chúng ta cũng có thể thấy thay đổi biểu diễn lớn hơn khi
điều chỉnh trên MNLI, trong khi prompt tuning vẫn có ít
thay đổi hơn trên biểu diễn.

en de es fr ja ko zh
Huấn luyện trên PA WS
FT 25.2 26.5 24.5 25.2 18.9 15.0 22.6
PT 57.6 56.8 57.2 57.7 58.7 59.4 59.5
Huấn luyện trên MNLI
FT -16.9 -19.1 -16.3 -14.5 -16.7 -11.8 -14.9
PT 32.2 32.1 31.2 32.1 33.8 36.0 35.8

Bảng 3: Độ tương tự cosine (%) của biểu diễn sau điều chỉnh cho mỗi ngôn ngữ. FT: fine-tuning; PT: prompt tuning. Các checkpoint này được điều chỉnh trên hai tập dữ liệu tiếng Anh: PAWS và MNLI.⁴

5.2 Căn chỉnh Đa ngôn ngữ Sau Điều chỉnh
Chúng tôi tính toán độ tương tự cosine trung bình của
tất cả 1000 cặp dịch thuật cho mỗi cặp ngôn ngữ <en , xx>,
trong đó xx là de, es, fr, ja, ko hoặc zh. Chúng tôi cũng
tính toán độ tương tự cosine trung bình của tất cả
1000*999/2 không phải bản dịch cho mỗi cặp ngôn ngữ.
Như được hiển thị trong Bảng 3, cả fine tuning và prompt
tuning đều hoạt động tốt. Prompt tuning có lợi thế theo
nghĩa rằng chúng thay đổi biểu diễn một cách nhẹ nhàng hơn,
vẫn có độ tương tự cosine cao trên các cặp dịch thuật.
Điều này dẫn đến khả năng chuyển giao bền vững hơn
và ít overfitting hơn.

5.3 Ranh giới Quyết định
Prompt tuning giữ được căn chỉnh đa ngôn ngữ cao với
ít thay đổi hơn trong các phần trước. Tuy nhiên, mức độ
chung của chất lượng biểu diễn đã học vẫn chưa được biết.
Chất lượng biểu diễn đã học được khảo sát trong phần này.

Hình 2 (a) và (b) cho thấy trực quan hóa t-SNE của biểu diễn
trước hai phương pháp điều chỉnh khác nhau. Mỗi điểm
trong hai hình là một mẫu PAWS-X

--- TRANG 4 ---
(a) Trực quan hóa trước fine tuning (FT).
(b) Trực quan hóa trước prompt tuning (PT).
(c) Ranh giới quyết định sau fine tuning (FT).
(d) Ranh giới quyết định sau prompt tuning (PT).

Hình 2: Trực quan hóa t-SNE của biểu diễn của bốn ngôn ngữ (en: tiếng Anh; de: tiếng Đức; ja: tiếng Nhật; zh: tiếng Trung) trước và sau hai phương pháp điều chỉnh khác nhau trên dữ liệu tác vụ tiếng Anh. Ranh giới quyết định sau prompt tuning được căn chỉnh tốt hơn nhiều.

từ bốn ngôn ngữ: tiếng Đức (de), tiếng Trung (zh), tiếng Anh (en), tiếng Nhật (ja). Mẫu màu xanh là paraphrase, mẫu màu cam là non-paraphrase. Các mẫu cùng một ngôn ngữ được nhóm lại với nhau. Tuy nhiên, thông tin nhãn bị thiếu từ biểu diễn mẫu.

Hình 2 (c) và (d) cho thấy trực quan hóa t-SNE (van der Maaten and Hinton, 2008) sau fine tuning (FT) và prompt tuning (PT). Sau điều chỉnh, cả hai đều có biểu diễn được phân tách hợp lý và đẹp. Đối với mỗi ngôn ngữ, chúng tôi cũng vẽ ranh giới quyết định hồi quy logistic cho các embedding t-SNE này. Ranh giới quyết định cho các ngôn ngữ khác nhau thay đổi đáng kể sau fine tuning. Ranh giới quyết định tiếng Anh không thể phân tách tốt trên các mẫu tiếng Đức. Sau prompt tuning, ranh giới quyết định của bốn ngôn ngữ được căn chỉnh tốt một cách đáng ngạc nhiên. Điều này gợi ý rằng prompt tuning học được bộ phân loại độc lập ngôn ngữ tốt hơn fine tuning, mặc dù việc điều chỉnh chỉ trên tập huấn luyện tiếng Anh.

en-de en-es en-fr en-ja en-ko en-zh
Huấn luyện trên MNLI
FT 81.5 85.4 83.0 71.8 68.2 73.9
FT-neg 52.6 53.1 52.8 51.5 50.6 50.0
rel-diff (%) 54.8 60.8 57.2 39.4 34.8 47.8
PT 96.4 97.3 96.6 94.8 93.8 95.0
PT-neg 91.0 91.1 90.8 90.5 90.1 90.2
rel-diff (%) 5.9 6.8 6.4 4.8 4.1 5.3
Huấn luyện trên PA WS
FT 90.4 92.1 88.8 76.8 75.3 82.0
FT-neg 13.3 13.2 13.4 14.3 14.4 13.6
rel-diff (%) 580 598 563 437 423 503
PT 98.4 98.6 98.3 96.3 96.0 96.7
PT-neg 88.1 88.1 88.3 89.1 89.4 88.9
rel-diff (%) 11.7 11.9 11.3 8.1 7.4 8.8

Bảng 4: Độ tương tự cosine (%) của các cặp dịch thuật sau điều chỉnh trên hai tập dữ liệu tiếng Anh: MNLI và PAWS. "-neg" có nghĩa là độ tương tự cosine trung bình của không phải bản dịch cho mỗi cặp ngôn ngữ. "rel-diff" có nghĩa là sự khác biệt tương đối giữa bản dịch và không phải bản dịch. Hai phương pháp điều chỉnh khác nhau được hiển thị, một là fine-tuning (FT), cái kia là prompt tuning (PT).

6 Công trình Liên quan
Gần đây, một số công trình trước đây cho thấy prompt tuning cho các mô hình ngôn ngữ đa ngôn ngữ. Winata et al. (2021) cho thấy kỹ năng đa ngôn ngữ của các mô hình được huấn luyện trước lớn với ít ví dụ. Zhao and Schütze (2021); Huang et al. (2022); Qi et al. (2022) cho thấy các phương pháp prompt tuning mới được đề xuất. Mục tiêu của công trình chúng tôi khác với của họ. Chúng tôi cho thấy prompt tuning tốt hơn fine-tuning

--- TRANG 5 ---
cho đánh giá đa ngôn ngữ. Chúng tôi có kết luận rằng prompt tuning của chúng tôi đạt hiệu suất cao hơn fine-tuning một cách nhất quán trong cài đặt này. Công trình trước đây (Zhao and Schütze, 2021; Huang et al., 2022; Qi et al., 2022) chỉ thực nghiệm trên tác vụ phân loại câu. Các tác vụ gán thẻ chuỗi khó và trả lời câu hỏi không được khám phá hoặc các cài đặt ở chế độ tài nguyên thấp. Chúng tôi khảo sát khả năng chuyển giao đa ngôn ngữ trên các tác vụ NLU khác nhau từ XTREME (Hu et al., 2020), đây là một trong những benchmark đánh giá chuyển giao đa ngôn ngữ quan trọng. Phân loại câu, gán nhãn chuỗi và trả lời câu hỏi được bao gồm.

7 Kết luận
Trong công trình này, chúng tôi so sánh prompt tuning và fine tuning trên hiểu biết đa ngôn ngữ với các mô hình ngôn ngữ đa ngôn ngữ, thấy rằng prompt tuning đạt hiệu suất tốt hơn. Điều này gợi ý rằng việc sử dụng prompt tuning trên chuyển giao đa ngôn ngữ là hứa hẹn.

Hạn chế
Trong công trình này, chúng tôi khảo sát tác động của prompt tuning trên hiểu biết đa ngôn ngữ và thực nghiệm chứng minh một số kết quả hứa hẹn. Chúng tôi cần rất nhiều tài nguyên GPU để hoàn thành các thí nghiệm của mình. Các thí nghiệm trên các mô hình ngôn ngữ đa ngôn ngữ được huấn luyện trước kích thước lớn được tiến hành trên A100s với bộ nhớ 40G. Việc huấn luyện có thể được tăng tốc bằng cách sử dụng batch lớn.

Đây là một khám phá sơ bộ về prompt tuning trên chuyển giao đa ngôn ngữ. Trong công trình này, các mô hình chỉ encoder được khám phá trên các tác vụ hiểu biết ngôn ngữ tự nhiên trong bài báo. Công việc tương lai cũng có thể bao gồm các mô hình encoder-decoder và các tác vụ khác.

Lời cảm ơn
Chúng tôi muốn cảm ơn nhóm Salesforce AI Research cho những thảo luận hữu ích, và các nhà bình duyệt cho những bình luận sâu sắc.

Tài liệu tham khảo
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Association for Computational Linguistics.

--- TRANG 6 ---
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.

Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454–470.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.

Alexis CONNEAU and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4921–4933, Online. Association for Computational Linguistics.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411–4421. PMLR.

Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang. 2022. Zero-shot cross-lingual transfer of prompt-based tuning with a unified multilingual prompt. ArXiv, abs/2202.11451.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics.

Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008–6018, Online. Association for Computational Linguistics.

Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61–68, Dublin, Ireland. Association for Computational Linguistics.

Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, and Luo Si. 2021. VECO: Variable and flexible cross-lingual pre-training for language understanding and generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3980–3994, Online. Association for Computational Linguistics.

Joakim Nivre, Mitchell Abrams, Željko Agić, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Gashaw Arutie, Masayuki Asahara, Luma Ateyah, Mohammed Attia, et al. 2018. Universal dependencies 2.2.

--- TRANG 7 ---
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001, Florence, Italy. Association for Computational Linguistics.

Kunxun Qi, Hai Wan, Jianfeng Du, and Haolan Chen. 2022. Enhancing cross-lingual natural language inference by prompt-learning from cross-lingual templates. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1910–1923, Dublin, Ireland. Association for Computational Linguistics.

Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215–10245, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Empirical Methods in Natural Language Processing (EMNLP).

Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579–2605.

Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung. 2021. Language models are few-shot multilingual learners. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 1–15, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833–844, Hong Kong, China. Association for Computational Linguistics.

Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687–3692, Hong Kong, China. Association for Computational Linguistics.

Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1441–1451, Florence, Italy. Association for Computational Linguistics.

Mengjie Zhao and Hinrich Schütze. 2021. Discrete and soft prompting for multilingual models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8547–8555, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017–5033, Online. Association for Computational Linguistics.

A Phụ lục
A.1 Chi tiết Huấn luyện Khác
Đối với prompt tuning, chúng tôi huấn luyện với bộ tối ưu hóa Adam (Kingma and Ba, 2015) không có bước warmup. Kích thước batch là 32 cho các tác vụ, và ngoại trừ trả lời câu hỏi, có kích thước batch là 8. Bộ lập lịch tốc độ học tuyến tính được sử dụng. Chúng tôi điều chỉnh tốc độ học trong {5e-2;1e-2;5e-3;1e-3;5e-4;1e-4}. Chúng tôi huấn luyện tất cả các mô hình prompt tuning cho 30 epoch. Cuối cùng, độ dài prompt được điều chỉnh cho MNLI là 32. Đối với các tác vụ khác là 16. Chúng tôi sử dụng A100s với bộ nhớ 40G và tất cả các thí nghiệm có thể được thực hiện trong vài giờ.

--- TRANG 8 ---
Phương pháp en ar bg de el es fr hi ru sw th tr ur vi zh avg
88.2 77.4 82.3 82.6 81.1 83.7 82.0 75.2 79 71.0 76.7 77.5 71.4 79.1 78.6 79.1
88.3 76.9 81.9 81.9 81.4 83.6 81.6 74.3 78.1 70.1 75.8 77.6 70.7 78.8 77.6 78.6
FT 88.1 77.5 82.4 81.8 81.3 83.4 82.6 75.0 78.9 70.3 75.6 78.1 70.8 78.5 78.3 78.8
88.4 77.4 81.7 82.0 81.5 83.3 82.3 75.4 79.0 70.2 75.5 78.1 71.2 79.1 77.9 78.9
88.2 77.6 82.5 81.7 80.9 83.2 81.9 75.1 78.2 69.5 76.5 77.6 71.0 78.8 78.6 78.8
88.5 78.3 82.8 82.2 82.5 84.2 83.0 76.1 80.4 71.0 77.6 79.2 72.5 80.0 78.3 79.8
88.7 78.7 82.9 82.1 82.8 84.3 83.2 76.1 80.4 71.0 77.6 79.2 72.5 80.0 78.3 79.8
PT 88.8 78.1 82.7 81.7 81.9 84.0 83.2 75.9 80.7 71.4 77.5 79.3 72.5 79.4 78.7 79.7
89.1 79.2 83.2 82.1 82.4 84.1 83.0 76.2 80.8 70.7 77.7 79.5 72.5 79.9 78.4 79.9
89.0 78.7 83.2 82.2 82.8 84.3 83.4 76.2 80.8 71.3 77.9 79.2 72.5 80.3 78.2 80.0

Bảng 5: Điểm accuracy XNLI cho mỗi ngôn ngữ với fine-tuning (FT) và prompt tuning (PT).

Phương pháp en de es fr ja ko zh avg
95.6 90.8 81.4 91.3 82.7 81.8 84.5 88.3
95.7 90.5 91.0 91.3 81.7 81.2 84.0 87.9
FT 95.4 89.4 90.8 90.9 80.5 80.6 84.0 87.4
95.4 90.2 90.6 90.5 80.6 80.4 83.4 87.2
94.7 91.0 91.4 92.1 82.4 93.2 84.2 88.6
96.2 92.3 91.4 92.1 81.3 83.2 84.8 88.8
95.3 91.6 91.1 92.0 82.7 83.1 84.2 88.6
PT 95.4 90.9 91.4 91.8 82.1 82.8 84.7 88.4
95.9 90.7 90.7 91.6 81.4 81.6 84.6 88.1
95.6 91.6 90.5 91.7 82.2 81.7 83.0 88.0

Bảng 6: Điểm accuracy PAWS-X cho mỗi ngôn ngữ với fine-tuning (FT) và prompt tuning (PT).

Phương pháp en es de el ru tr ar vi th zh hi avg
75.2 / 87.2 61.2 / 80.7 62.7 / 82.5 60.2 / 78.7 63.5 / 80.1 57.8 / 74.3 58.6 / 75.5 59.9 / 79.4 59.9 / 73.2 58.7 / 68.5 56.6 / 74.7 61.3 / 77.7
75.0 / 86.8 61.6 / 79.8 61.9 / 80.0 59.6 / 78.6 62.7 / 79.6 57.6 / 73.3 56.6 / 74.4 57.8 / 78.6 61.3 / 72.4 60 / 67.5 58.2 / 74.6 61.1 / 76.9
PT 75.5 / 87.0 64.0 / 81.3 64.8 / 80.9 62.4 / 80.0 63.8 / 80.1 57.7 / 73.8 55.9 / 72.8 60.2 / 79.5 62.3 / 73.4 61.4 / 69.6 59.8 / 76.1 62.5 / 77.7
75.8 / 87.0 63.0 / 81.4 62.4 / 79.4 62.1 / 79.9 62.9 / 79.8 56.9 / 73.6 57.4 / 74.6 59.6 / 78.5 62.8 / 74.7 60.5 / 70.5 57.5 / 74.2 61.9 / 77.6
76.0 / 87.4 62.8 / 80.8 65.0 / 80.2 61.2 / 78.3 63.1 / 79.5 56.3 / 72.3 57.3 / 73.9 57.6 / 77.5 62.9 / 71.6 61.0 / 68.7 58.4 / 74.2 62.0 / 76.8
77.2 / 88.4 65.1 / 83.1 64.8 / 81.4 63.7 / 81.2 58.7 / 80.2 58.7 / 74.6 60.3 / 77.0 61.4 / 80.6 66.4 / 74.7 60.3 / 68.6 61.8 / 78.1 63.5 / 78.9
77.4 / 88.5 64.4 / 82.3 64.8 / 81.2 63.5 / 80.8 64.7 / 80.7 58.3 / 74.1 60.3 / 76.8 61.0 / 80.3 66.6 / 75.0 61.7 / 70.2 61.5 / 77.5 64.0 / 78.9
PT 77.4 / 88.6 65.4 / 83.4 64.5 / 80.9 64.0 / 81.2 64.1 / 80.7 58.7 / 74.9 59.8 / 76.5 62.1 / 81.4 66.6 / 75.4 61.3 / 69.9 62.8 / 77.8 64.2 / 79.2
77.1 / 88.5 64.7 / 82.9 63.9 / 80.7 62.7 / 80.5 64.7 / 80.4 59.2 / 74.6 59.7 / 76.3 60.8 / 80.7 66.6 / 74.6 61.0 / 69.1 61.6 / 77.6 64.7 / 78.7
77.9 / 88.7 65.0 / 83.0 64.2 / 81.2 63.4 / 80.2 64.8 / 80.9 58.2 / 75.4 60.2 / 77.0 62.9 / 81.3 67.3 / 75.9 60.7 / 69.8 61.1 / 77.9 64.2 / 79.2

Bảng 7: Kết quả XQuAD (EM / F1) cho mỗi ngôn ngữ với fine-tuning (FT) và prompt tuning (PT).

Phương pháp en ar bn fi id ko ru sw te avg
60.5 / 74.2 51.5 / 71.5 50.4 / 68.6 51.0 / 67.6 62.5 / 78.6 49.6 / 60.9 45.3 / 67.7 44.7 / 65.7 56.7 / 75.7 46.2 / 70.0
57.7 / 71.8 51.5 / 71.0 50.4 / 70.4 53.5 / 70.3 61.4 / 77.1 53.6 / 64.5 47.8 / 68.6 50.3 / 70.3 57.0 / 75.7 53.7 / 71.1
FT 57.7 / 73.2 51.9 / 72.5 48.7 / 66.4 53.6 / 69.7 59.8 / 77.0 50.7 / 59.6 50.7 / 68.0 49.1 / 69.1 58.0 / 77.8 53.4 / 70.4
58.6 / 71.7 53.0 / 71.6 46.0 / 62.5 53.7 / 68.4 59.8 / 75.7 52.5 / 63.0 40.4 / 65.2 48.9 / 69.2 58.4 / 76.3 52.4 / 69.3
59.8 / 72.3 48.3 / 70.6 52.2 / 68.6 49.7 / 67.5 60.4 / 77.7 55.1 / 65.5 38.9 / 65.2 45.4 / 66.6 56.8 / 75.4 51.8 / 69.9
61.8 / 75.0 53.7 / 72.3 48.7 / 67.0 58.2 / 73.0 63.0 / 77.9 52.9 / 63.6 50.2 / 70.0 47.5 / 68.5 57.5 / 75.3 54.8 / 71.5
60.7 / 74.0 53.1/72.2 45.1 / 64.5 55.9 / 71.8 63.5 / 78.3 51.8 / 61.9 52.3 / 71.0 48.9 / 68.9 58.4 / 76.2 54.4 / 71.0
PT 60.2 / 73.6 54.8 / 73.9 52.2 / 70.0 56.6 / 71.4 64.8 / 78.7 52.5 / 62.3 53.1 / 71.4 51.1 / 70.7 61.6 / 79.1 56.3 / 72.3
62.0 / 75.3 53.6 / 73.0 46.0 / 64.9 57.3 / 71.3 63.7 / 78.6 53.3 / 62.0 52.7 / 71.8 48.1 / 69.0 58.7 / 75.5 55.0 / 71.3
61.4 / 74.5 54.9 / 72.8 46.9 / 66.3 56.8 / 71.4 63.2 / 77.6 54.3 / 63.0 53.1 / 71.1 47.9 / 68.4 58.6 / 76.4 55.2 / 71.3

Bảng 8: Kết quả TyDiQA-GoldP (EM / F1) cho mỗi ngôn ngữ với fine-tuning (FT) và prompt tuning (PT).
