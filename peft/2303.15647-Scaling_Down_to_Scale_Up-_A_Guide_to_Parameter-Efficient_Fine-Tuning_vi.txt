# 2303.15647.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2303.15647.pdf
# Kích thước file: 1697737 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Thu nhỏ để mở rộng: Hướng dẫn về tinh chỉnh hiệu quả tham số
Thu nhỏ để mở rộng:
Hướng dẫn về tinh chỉnh hiệu quả tham số
Vladislav Lialin vlialin@cs.uml.edu∗
University of Massachusetts Lowell
Vijeta Deshpande vijeta_deshpande@student.uml.edu
University of Massachusetts Lowell
Xiaowei Yao xiaowei_yao@student.uml.edu
University of Massachusetts Lowell
Anna Rumshisky arum@cs.uml.edu
University of Massachusetts Lowell
Amazon Alexa AI
Tóm tắt
Bài báo này trình bày một cái nhìn tổng quan có hệ thống về các phương pháp tinh chỉnh hiệu quả tham số, bao gồm hơn 50 bài báo được xuất bản từ đầu năm 2019 đến giữa năm 2024. Các phương pháp này nhằm giải quyết những thách thức của việc tinh chỉnh các mô hình ngôn ngữ lớn bằng cách chỉ huấn luyện một tập con nhỏ các tham số. Chúng tôi cung cấp một phân loại bao gồm một loạt rộng các phương pháp và trình bày một so sánh phương pháp chi tiết với trọng tâm cụ thể về hiệu quả thực tế trong việc tinh chỉnh các mô hình ngôn ngữ có quy mô hàng tỷ tham số. Chúng tôi cũng tiến hành một so sánh thực nghiệm trực tiếp mở rộng của 15 phương pháp PEFT đa dạng, đánh giá hiệu suất và hiệu quả của chúng trên các mô hình lên đến 11B tham số. Những phát hiện của chúng tôi cho thấy các phương pháp trước đây đã được chứng minh vượt qua một baseline LoRA mạnh gặp khó khăn trong các thiết lập bị hạn chế tài nguyên, nơi tối ưu hóa siêu tham số bị giới hạn và mạng chỉ được tinh chỉnh trong vài epoch. Cuối cùng, chúng tôi cung cấp một tập hợp các khuyến nghị thực tế để sử dụng các phương pháp PEFT và phác thảo các hướng nghiên cứu tiềm năng trong tương lai.
Từ khóa: Tinh chỉnh hiệu quả tham số, Mô hình ngôn ngữ lớn
1 Giới thiệu
Vào tháng 10 năm 2018, BERT Large (Devlin et al., 2019)
Một điều cần học từ
bài học cay đắng là sức mạnh
tuyệt vời của các phương pháp
mục đích chung, của những phương pháp
tiếp tục mở rộng với tính toán
gia tăng...
Rich Sutton, Bài học cay đắng với 350 triệu tham số là mô hình Transformer lớn nhất (Vaswani et al., 2017) từng được huấn luyện. Vào thời điểm đó, phần cứng đương thời gặp khó khăn trong việc tinh chỉnh mô hình này. Phần "Vấn đề hết bộ nhớ" trên GitHub của BERT¹ chỉ định kích thước batch tối đa cho BERT Large với 12Gb RAM GPU và 512 token là không. Năm năm sau, các mô hình có sẵn công khai đã tăng lên 176 tỷ tham số (Scao et al., 2022; Zhang et al., 2022; Zeng et al., 2022), tức là tăng gấp 500 lần. Tài liệu xuất bản bao gồm các mô hình lên đến 1 nghìn tỷ tham số (Chowdhery et al., 2022; Shoeybi et al., 2019; Fedus et al., 2021). Tuy nhiên, RAM GPU đơn tăng ít hơn 10 lần do chi phí cao của bộ nhớ HBM. Quy mô mô hình tăng gần hai bậc độ lớn nhanh hơn tài nguyên tính toán khiến việc tinh chỉnh các mô hình lớn nhất cho các tác vụ downstream trở nên không khả thi đối với hầu hết và không thực tế đối với tất cả mọi người.
Học trong ngữ cảnh (Radford et al., 2019) do đó trở thành chuẩn mới, cách tiêu chuẩn để truyền dữ liệu huấn luyện tác vụ downstream cho các mô hình ngôn ngữ có quy mô tỷ tham số. Tuy nhiên, độ dài ngữ cảnh hạn chế do kiến trúc transformer áp đặt (Vaswani et al., 2017; Huang et al., 2018), sự vắng mặt của khả năng ICL trong các mô hình ngôn ngữ lớn vừa phải (Lu et al., 2023), sự gia tăng chi phí tính toán bậc hai với sự gia tăng độ dài ngữ cảnh (hoặc các minh chứng trong ICL) (Keles et al., 2023), và độ nhạy cảm của hiệu suất ICL (Bertsch et al., 2024) đưa ra những thách thức trong tính hữu ích, độ tin cậy và hiệu quả của ICL. Trong các trường hợp mà mô hình hoạt động ngang bằng hoặc tốt hơn trong thiết lập ICL so với mô hình tinh chỉnh, tinh chỉnh vẫn là một chiến lược hấp dẫn do chi phí suy luận không thực tế của ICL (Bertsch et al., 2024). Do đó, chúng ta, với tư cách là một cộng đồng các nhà nghiên cứu và kỹ sư, cần những cách hiệu quả để huấn luyện trên dữ liệu tác vụ downstream.
Tinh chỉnh hiệu quả tham số (PEFT) nhằm giải quyết vấn đề này bằng cách chỉ huấn luyện một tập hợp nhỏ các tham số, có thể là một tập con của các tham số mô hình hiện có hoặc một tập hợp các tham số mới được thêm vào. Các phương pháp này khác nhau về hiệu quả tham số và bộ nhớ, tốc độ huấn luyện, chất lượng mô hình cuối cùng và chi phí suy luận bổ sung (nếu có).
Trong vài năm qua, hơn một trăm bài báo PEFT đã được xuất bản, với nhiều nghiên cứu (Ding et al., 2022) cung cấp cái nhìn tổng quan tốt về các phương pháp phổ biến nhất, như Adapters (Houlsby et al., 2019), BitFit (Ben-Zaken et al., 2021), LoRA (Hu et al., 2022), Compacter (Karimi Mahabadi et al., 2021), và Soft Prompts (Liu et al., 2021; Li and Liang, 2021).
Pfeiffer et al. (2023) trình bày một khảo sát về học sâu modular, cung cấp cái nhìn tổng quan về một số phương pháp tương tự từ góc độ tính modular và suy luận đa tác vụ. Trọng tâm của chúng tôi khác bằng cách tập trung vào các phương pháp PEFT, cụ thể để tinh chỉnh các mô hình ngôn ngữ lớn, nơi việc giảm thiểu tiêu thụ RAM và thời gian huấn luyện mà không hy sinh hiệu suất là quan trọng.
Khảo sát này trình bày một cái nhìn tổng quan, so sánh và phân loại có hệ thống của 30 phương pháp tinh chỉnh hiệu quả tham số. Trong năm qua, các nỗ lực nghiên cứu cũng tập trung vào việc sao chép thành công của PEFT trong chế độ tiền huấn luyện. Do đó, chúng tôi cũng thảo luận về một số phương pháp nổi bật nhằm đạt được lợi ích hiệu quả trong quá trình tiền huấn luyện. Chúng tôi thảo luận 30 phương pháp một cách sâu sắc, bao gồm hơn 50 bài báo được xuất bản từ đầu năm 2019 đến giữa năm 2024. Chúng tôi nhấn mạnh các thách thức chưa được giải quyết hiện tại trong PEFT, bao gồm sự hiểu biết lý thuyết hạn chế, khoảng cách hiệu suất giữa PEFT và tinh chỉnh truyền thống, và các vấn đề báo cáo.
Chúng tôi tiến hành so sánh thực nghiệm mở rộng nhất về các phương pháp PEFT, đánh giá 14 phương pháp và các biến thể của chúng trên năm bộ dữ liệu và ba kích thước mô hình (0.7B, 3B và 11B). Nghiên cứu bao gồm so sánh chi tiết về hiệu quả của các phương pháp này về tiêu thụ bộ nhớ GPU và thông lượng. Những phát hiện của chúng tôi cho thấy các phương pháp trước đây được chứng minh vượt trội LoRA gặp khó khăn khi làm như vậy trong các thiết lập bị hạn chế tài nguyên và thể hiện độ nhạy cảm siêu tham số cao trong các phương pháp PEFT kết hợp.
Chúng tôi phát hiện rằng các tái tham số hóa dựa trên Kronecker, mặc dù không tăng cường hiệu quả bộ nhớ so với các đối tác tích ma trận, có thể cải thiện tốc độ huấn luyện và suy luận với triển khai hiệu quả. Đáng ngạc nhiên, việc điều chỉnh Layer Norm hoạt động cực kỳ tốt so với hầu hết các phương pháp PEFT trong nghiên cứu của chúng tôi. Chúng tôi cũng lưu ý một sự khác biệt đáng kể giữa số lượng tham số có thể huấn luyện được báo cáo và thực tế trong các phương pháp PEFT. Điều này dẫn đến những kết quả không lường trước được, như chi phí tính toán cao của Prompt Tuning và các phương pháp Hybrid. Mã của chúng tôi có sẵn trên Github².
Kết luận, chúng tôi đề xuất một số hướng cải thiện, như phát triển các benchmark PEFT chuẩn hóa, tiến hành các nghiên cứu sâu về siêu tham số và khả năng diễn giải, khám phá sự khác biệt trong động lực huấn luyện của các mạng neural được tái tham số hóa, cải thiện thêm hiệu quả huấn luyện và suy luận của các phương pháp PEFT, và tính hữu ích của các phương pháp PEFT với các mô hình backbone được lượng tử hóa.
2 Nền tảng: Transformer
Nhiều kỹ thuật tinh chỉnh hiệu quả tham số được thảo luận trong khảo sát này có thể được áp dụng cho tất cả các mạng neural, mặc dù một số được thiết kế cụ thể cho kiến trúc Transformer (Vaswani et al., 2017). Vì Transformers là những mạng neural lớn nhất từng được huấn luyện, các phương pháp này đặc biệt có giá trị. Do đó, chúng tôi trình bày một cái nhìn tổng quan ngắn gọn về Transformer để cung cấp ngữ cảnh cho các kỹ thuật này.
Khối xây dựng cốt lõi của kiến trúc Transformer bao gồm attention đa đầu (MHA) theo sau bởi một lớp kết nối đầy đủ (FFN), như được minh họa trong Hình 1. Cả lớp attention và lớp kết nối đầy đủ đều kết hợp các kết nối dư (He et al., 2016) và Layer Normalization (Ba et al., 2016) để cải thiện khả năng huấn luyện.
Trái tim của Transformer là phép toán attention (Bahdanau et al., 2014). Nó tính toán trung bình có trọng số chuẩn hóa softmax của các token đầu vào. Trọng số attention được tính theo tích vô hướng cặp đôi giữa mỗi key và query của token. Keys và queries thường được tính như các phép chiếu tuyến tính đơn giản của các token đầu vào. Phương trình 1 mô tả nó trong ký hiệu NamedTensor (Chiang et al., 2021).
Att:Rkey×Rseq×key×Rseq×val→Rval
Att(Q, K, V ) =
softmax
seqQ⊙
keyK
p
|key|
⊙
seqV
(1)
Q(x) =x·WQ+bk,
K(x) =x·WK+bq,
V(x) =x·WV+bv,
WQ, W K∈Rinput×key, WV∈Rinput×val
bQ, bK∈Rkey, bV∈Rval
Hình 1: Khối Transformer cơ bản
2. github.com/guitaricet/peft_comparison
3

--- TRANG 2 ---
Lialin, Deshpande, Yao, và Rumshisky
Hình 2: Phân loại các phương pháp tinh chỉnh hiệu quả tham số. Chúng tôi xác định ba lớp phương pháp chính: dựa trên Bổ sung, dựa trên Lựa chọn và dựa trên Tái tham số hóa. Trong các phương pháp bổ sung, chúng tôi phân biệt hai nhóm lớn được bao gồm: các phương pháp giống Adapter và Soft prompts.
Một số phương pháp hoạt động cụ thể trên các ma trận WK,WQ,WV; chúng cung cấp cơ chế chính để truyền thông tin từ token này sang token khác và kiểm soát thông tin nào (giá trị) đang được truyền.
Mặc dù các triển khai cụ thể của Transformer có thể khác nhau, như kết hợp một lớp cross-attention trong các mạng seq2seq hoặc sử dụng LayerNorm trước các lớp con (Pre-LN), hầu hết các phương pháp tinh chỉnh hiệu quả tham số cho Transformers chỉ dựa vào cấu trúc MHA + FFN cơ bản. Các phương pháp này có thể được điều chỉnh dễ dàng cho các biến thể kiến trúc.
3 Phân loại PEFT: cái nhìn tổng quan
Các phương pháp PEFT có thể được phân loại theo cách tiếp cận hoặc mục tiêu của chúng. Về mặt cách tiếp cận, chúng có thể giới thiệu các tham số mới, tinh chỉnh các tham số hiện có hoặc tái tham số hóa chúng. Về mặt mục tiêu, chúng nhằm giảm thiểu dấu chân bộ nhớ, cải thiện hiệu quả lưu trữ hoặc thêm tính modular. Trong phần này, chúng tôi bắt đầu bằng cách trình bày một phân loại dựa trên cách tiếp cận trước.
Hình 2 và Các phần 3.1-3.4 trình bày một cái nhìn tổng quan về phân loại và minh họa hơn 30 phương pháp PEFT. Trong các phần tiếp theo, chúng tôi mô tả các phương pháp PEFT này một cách chi tiết, kèm theo pseudo-code dễ hiểu, trong Các phần 5 - 10.
3.1 Các phương pháp dựa trên bổ sung
Các phương pháp dựa trên bổ sung tăng cường mô hình được tiền huấn luyện với các tham số hoặc lớp bổ sung và chỉ huấn luyện các yếu tố mới được giới thiệu. Đây là danh mục lớn nhất và được khám phá rộng rãi nhất của các phương pháp PEFT. Trong danh mục này, hai danh mục con lớn đã xuất hiện: các phương pháp giống Adapter và Soft Prompts.
Adapters Adapters (Houlsby et al., 2019) là một loại phương pháp PEFT bổ sung giới thiệu các mạng kết nối đầy đủ nhỏ sau các lớp con Transformer. Ý tưởng đã được áp dụng rộng rãi (Pfeiffer et al., 2020b)³, và nhiều biến thể của Adapters đã được đề xuất. Các biến thể này bao gồm việc sửa đổi vị trí đặt adapters (He et al., 2022a; Zhu et al., 2021), cắt tỉa (He et al., 2022b), và sử dụng tái tham số hóa để giảm số lượng tham số có thể huấn luyện (Karimi Mahabadi et al., 2021). Phần 5 thảo luận các phương pháp dựa trên Adapter một cách chi tiết.
Soft Prompts Prompting mô hình ngôn ngữ (Radford et al., 2019) nhằm kiểm soát hành vi của một mô hình ngôn ngữ bằng cách sửa đổi văn bản đầu vào, thường bao gồm một mô tả tác vụ kèm theo một vài ví dụ trong ngữ cảnh. Tuy nhiên, các phương pháp này khó tối ưu hóa và vốn dĩ bị giới hạn bởi độ dài đầu vào tối đa của mô hình. Để giải quyết những nhược điểm này, khái niệm "soft" prompts đã được giới thiệu (Liu et al., 2021; Lester et al., 2021; Li and Liang, 2021), nơi một phần của embeddings đầu vào của mô hình được tinh chỉnh thông qua gradient descent. Điều này chuyển vấn đề tìm prompts trong không gian rời rạc thành một vấn đề tối ưu hóa liên tục. Soft prompts có thể được huấn luyện chỉ cho lớp đầu vào (Liu et al., 2021; Lester et al., 2021) hoặc cho tất cả các lớp (Li and Liang, 2021). Những tiến bộ gần đây khám phá cách soft prompts có thể được tiền huấn luyện hoặc tái sử dụng một phần để giảm chi phí tinh chỉnh (Vu et al., 2021; Hambardzumyan et al., 2021; Su et al., 2021; Qin et al., 2021). Chúng tôi thảo luận Soft Prompts trong Phần 6.
Các cách tiếp cận bổ sung khác Các phương pháp bổ sung là một danh mục đa dạng của các kỹ thuật tinh chỉnh hiệu quả tham số mở rộng vượt ra ngoài adapters và soft prompts. Chúng tôi thảo luận về chúng trong Phần 7.
Tại sao thêm tham số? Mặc dù các phương pháp này giới thiệu các tham số bổ sung vào mạng, chúng đạt được những cải thiện đáng kể về tốc độ và bộ nhớ. Điều này được đạt được bằng cách giảm kích thước của gradients và trạng thái optimizer. Trong trường hợp của Adam (Kingma and Ba, 2015), cho mỗi byte tham số có thể huấn luyện, cần thêm một byte cho gradient của nó, và hai byte nữa cần thiết để lưu trữ trạng thái optimizer: moment thứ nhất và thứ hai của gradient. Trong thực tế, huấn luyện một mô hình đòi hỏi bộ nhớ GPU nhiều hơn 12-20 lần so với trọng số mô hình. Bằng cách tiết kiệm bộ nhớ trên trạng thái optimizer, gradients, và lượng tử hóa các tham số mô hình bị đông lạnh (Dettmers et al., 2023), các phương pháp PEFT bổ sung cho phép tinh chỉnh các mạng lớn hơn nhiều hoặc sử dụng kích thước microbatch lớn hơn⁴, điều này cải thiện thông lượng huấn luyện trên GPUs. Hơn nữa, bước cập nhật optimizer mất ít thời gian hơn cho các phương pháp PEFT so với tinh chỉnh đầy đủ do số lượng tham số cần cập nhật ít hơn. Hiệu ứng này rất đáng chú ý trong thực tế. Cuối cùng, tối ưu hóa ít tham số hơn trong các thiết lập phân tán giảm đáng kể khối lượng giao tiếp.
3. github.com/adapter-hub/adapter-transformers
4. Kích thước batch = kích thước microbatch × tích lũy gradient × số thiết bị
5. Phụ thuộc vào hỗ trợ phần cứng cho các phép toán thưa thớt.
5

--- TRANG 3 ---
3.2 Các phương pháp dựa trên lựa chọn
Có thể nói ví dụ sớm nhất của PEFT có chọn lọc là tinh chỉnh chỉ một vài lớp trên cùng của một mạng (Donahue et al., 2014). Các cách tiếp cận hiện đại thường dựa trên loại lớp (Gheini et al., 2021; AkbarTajari et al., 2022) hoặc cấu trúc nội bộ, chẳng hạn như chỉ điều chỉnh bias của mô hình (Ben-Zaken et al., 2021) hoặc các hàng cụ thể (Vucetic et al., 2022).
Huấn luyện thưa là một phiên bản khác của PEFT có chọn lọc thường bỏ qua cấu trúc và chọn các tham số để điều chỉnh dựa trên các tiêu chí lựa chọn được thiết kế cẩn thận (Sung et al., 2021; Ansell et al., 2022; Guo et al., 2020). Tuy nhiên, cập nhật tham số thưa thớt đưa ra nhiều thách thức về kỹ thuật và hiệu quả. Một số trong số chúng đã được giải quyết trong nghiên cứu gần đây về tái cấu hình tham số (Vucetic et al., 2022) (Phần 8.3) và độ thưa NxM (Holmes et al., 2021). Tuy nhiên, độ thưa không có cấu trúc không hạn chế vẫn không thực tế trên phần cứng đương thời.
Chúng tôi thảo luận các phương pháp có chọn lọc một cách chi tiết trong Phần 8.
3.3 Các phương pháp dựa trên tái tham số hóa
Các phương pháp tinh chỉnh hiệu quả tham số dựa trên tái tham số hóa tận dụng các biểu diễn low-rank để giảm thiểu số lượng tham số có thể huấn luyện. Khái niệm rằng các mạng neural có các biểu diễn chiều thấp đã được khám phá rộng rãi trong cả phân tích thực nghiệm và lý thuyết của học sâu (Maddox et al., 2020; Li et al., 2018; Arora et al., 2018; Malladi et al., 2022).
Aghajanyan et al. (2020) đã chứng minh rằng tinh chỉnh có thể được thực hiện hiệu quả trong các không gian con low-rank. Hơn nữa, họ cho thấy rằng kích thước của không gian con cần thích ứng nhỏ hơn đối với các mô hình lớn hơn hoặc các mô hình được tiền huấn luyện trong thời gian dài hơn. Cách tiếp cận của họ, được gọi là Intrinsic SAID (Phần 9.1), sử dụng biến đổi Fastfood (Le et al., 2013) để tái tham số hóa việc cập nhật các tham số mạng neural. Điều này đã truyền cảm hứng cho nhiều phương pháp, như IntrinsicSAID, LoRA, và KronA (Các phần 9.1, 9.2, 9.3).
Tuy nhiên, có lẽ phương pháp dựa trên tái tham số hóa nổi tiếng nhất là Low-Rank Adaptation hoặc LoRA (Hu et al., 2022), sử dụng phân tích ma trận low-rank đơn giản để tham số hóa việc cập nhật trọng số δW=WdownWup. Cách tiếp cận này đơn giản để triển khai và đã được đánh giá trên các mô hình có lên đến 175 tỷ tham số. Chúng tôi cung cấp một thảo luận chi tiết về phương pháp này trong Phần 9.2. Các công trình gần đây hơn (Karimi Mahabadi et al., 2021; Edalati et al., 2022) cũng đã khám phá việc sử dụng tái tham số hóa tích Kronecker (δW=A⊗B), mang lại một sự đánh đổi thuận lợi hơn giữa rank và số lượng tham số. Các phương pháp dựa trên tái tham số hóa gần đây đã trở nên phổ biến rộng rãi, chứng minh hiệu quả của chúng trên các mô hình lên đến 175B tham số (Hu et al., 2022). Phần 9 thảo luận chi tiết các phương pháp này.
3.4 Các phương pháp lai
Một số phương pháp kết hợp ý tưởng từ nhiều danh mục PEFT (He et al., 2022a,b; Mao et al., 2021; Karimi Mahabadi et al., 2021). Điều này cho phép sử dụng các đánh đổi thuật toán khác nhau để tối ưu hóa cho một mục tiêu cụ thể, chẳng hạn như số lượng tham số có thể huấn luyện. Ví dụ, MAM Adapter (Phần 10.2) kết hợp cả Adapters và Prompt tuning. UniPELT (Phần 10.3) thêm LoRA vào hỗn hợp. Compacter và KronAB_res tái tham số hóa adapters sử dụng tích Kronecker để giảm số lượng tham số của chúng (Các phần 10.4 và 9.3).
4 Đi sâu vào PEFT
Trong các phần tiếp theo, chúng tôi đi sâu vào chi tiết của các cách tiếp cận tinh chỉnh hiệu quả tham số khác nhau. Chúng tôi mô tả sự phân biệt và đánh đổi giữa chúng về các chiều được nêu trong Phần 11. Chúng tôi in đậm một câu tóm tắt của mỗi phương pháp để đơn giản hóa việc lướt qua.
7

Trong mô tả phương pháp, chúng tôi cũng chỉ ra liệu nó đã được áp dụng cho các mô hình có ít hơn 1 tỷ, 20 tỷ, hoặc hơn 20 tỷ tham số. Chúng tôi tuân thủ việc chỉ ra số lượng tham số khi có thể vì các từ "nhỏ" và "lớn" thay đổi ý nghĩa quá nhanh. Để tóm tắt, tham khảo Bảng 2. Cuối cùng, chúng tôi cung cấp một triển khai pseudo-PyTorch ngắn gọn của phần quan trọng nhất của thuật toán khi khả thi.
5 Các phương pháp bổ sung: Adapters
Chúng tôi bắt đầu khám phá các phương pháp PEFT với một trong những họ con lớn nhất: các phương pháp thêm mạng kết nối đầy đủ giữa các lớp mô hình, được gọi là adapters.
5.1 Adapters
Houlsby et al. (2019) giới thiệu ý tưởng adapters cho NLP. Rebuffi et al. (Rebuffi et al., 2017, 2018) ban đầu đề xuất các khái niệm tương tự cho các tác vụ phân loại hình ảnh, và Houlsby et al. mở rộng khái niệm này cho NLP bằng cách đề xuất việc bổ sung các mạng kết nối đầy đủ sau các lớp attention và FFN trong Transformer. Khác với khối FFN transformer, Adapters thường có chiều ẩn nhỏ hơn so với đầu vào. Adapters đã chứng minh hiệu quả tham số ấn tượng, cho thấy rằng có thể đạt được hiệu suất tinh chỉnh đầy đủ bằng cách điều chỉnh ít hơn 4% tổng số tham số mô hình.

```
def transformer_block_with_adapter (x):
    residual = x
    x = SelfAttention (x)
    x = FFN(x)
    x = Adapter (x) # Adapter sau FFN
    x = LN(x + residual ) 
```

Pfeiffer et al. (2020a) phát hiện rằng việc chèn adapter chỉ sau lớp self-attention (sau normalization) đạt được hiệu suất tương tự như sử dụng hai adapters mỗi khối transformer.
5.2 AdaMix
AdaMix (Wang et al., 2022) cải thiện hiệu suất của adapters bằng cách sử dụng nhiều adapters theo kiểu mixture-of-experts (MoE) (Shazeer et al., 2017). Điều này có nghĩa là mỗi lớp adapter bao gồm một tập hợp các lớp (experts), và cho mỗi lần truyền xuôi, chỉ một tập hợp nhỏ experts được kích hoạt. Trái ngược với MoE thông thường, chọn và cân trọng nhiều experts sử dụng một mạng định tuyến, AdaMix chọn ngẫu nhiên một expert duy nhất cho mỗi lần truyền xuôi để giảm thiểu chi phí tính toán.
Để ổn định huấn luyện, các tác giả đề xuất điều chính consistency, giảm thiểu divergence KL đối xứng giữa hai lần truyền xuôi của mô hình với các tập hợp experts khác nhau được chọn. Một sự khác biệt khác với lớp MoE thông thường là các phép chiếu lên và xuống của adapter được chọn độc lập. Sau huấn luyện, các trọng số adapter được trung bình hóa trên các experts để giảm chi phí suy luận.

```
def transformer_block_with_adamix (x):
    residual = x
    x = SelfAttention (x)
    x = LN(x + residual )
    residual = x
    x = FFN(x)
    # adamix bắt đầu tại đây
    x = random_choice ( experts_up )(x)
    x = nonlinearity (x)
    x = random_choice ( experts_down )(x)
    x = LN(x + residual )
    return x
    
def consistency_regularization (x):
    logits1 = transformer_adamix (x)
    # lần thứ hai sử dụng experts khác
    logits2 = transformer_adamix (x)
    r = symmetrized_KL ( logits1 , logits2 )
    return r 
```

Mặc dù AdaMix đạt được hiệu suất tốt hơn adapters thông thường với cùng chi phí suy luận, nó có thể sử dụng nhiều bộ nhớ hơn trong quá trình huấn luyện. Wang et al. (2022) cho thấy AdaMix có thể sử dụng trạng thái ẩn adapter nhỏ hơn nhiều so với adapters thông thường, điều này phân bổ overhead tham số có thể huấn luyện trên số lượng experts (~4-8). Tuy nhiên, consistency regularization tăng yêu cầu bộ nhớ tính toán, vì nó cần giữ hai phiên bản của trạng thái ẩn và gradients qua hai lần truyền xuôi với các experts khác nhau.
6 Các phương pháp bổ sung: Soft Prompts
Prompting các mô hình ngôn ngữ đã chứng minh hiệu suất đáng chú ý trong các kịch bản zero-shot và few-shot (Brown et al., 2020; Schick and Schütze, 2021). Tuy nhiên, việc tối ưu hóa prompts ngôn ngữ tự nhiên rời rạc hoặc sử dụng học trong ngữ cảnh trở nên không thực tế với nhiều ví dụ huấn luyện. Để vượt qua thách thức này, khái niệm "soft" hoặc "continuous" prompts đã được đề xuất (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021), chuyển đổi vấn đề tối ưu hóa rời rạc của việc tìm "hard" prompt tốt nhất thành một vấn đề liên tục.
6.1 Prompt Tuning
Prompt tuning (Lester et al., 2021) đề xuất việc thêm vào đầu embeddings đầu vào. Các tensor này thường được gọi là "soft prompts," và chúng được tối ưu hóa trực tiếp thông qua gradient descent.

```
def prompt_tuning_attention ( input_ids ):
    q = x @ W_q
    k = cat ([ s_k , x]) @ W_k # thêm một
    v = cat ([ s_v , x]) @ W_v # soft prompt
    return softmax (q @ k.T) @ V 
```

Các nghiên cứu ablation bởi [tác giả] về độ dài prompt (1 đến 150 tokens) và kích thước mô hình (10M đến 11B tham số) cho thấy prompt tuning trở nên hiệu quả tham số hơn khi kích thước mô hình tăng. Ví dụ, prompt tuning của T5-11B đạt được hiệu suất SuperGLUE (Wang et al., 2019) tương tự với 5 hoặc 150 soft prompt tokens.
Hơn nữa, hiệu quả tăng nhanh hơn với kích thước mô hình. Hiệu suất T5-large bão hòa ở độ dài prompt 20 hoặc 20K tham số có thể huấn luyện (0.002%), và hiệu suất T5-XL bão hòa ở độ dài prompt 5, cũng 20K tham số có thể huấn luyện (0.0002%). Tuy nhiên, prompt tuning chỉ trở nên so sánh được với tinh chỉnh đầy đủ ở quy mô mô hình 10B. Ngoài ra, việc tăng độ dài chuỗi 20-100 tokens có thể tăng đáng kể tính toán, với độ phức tạp bậc hai của transformer. Tổng thể, soft prompts cực kỳ hiệu quả tham số nhưng đi kèm với overhead suy luận và áp dụng hơn cho các mô hình lớn hơn.
6.2 Prefix Tuning
Li and Liang (2021) phát triển độc lập ý tưởng soft prompts với một hương vị đặc biệt: các tham số có thể huấn luyện chung được thêm vào đầu trạng thái ẩn của tất cả các lớp. Cùng một prefix Pθ∈Rl×h được thêm vào đầu tất cả trạng thái ẩn. Họ quan sát rằng việc tối ưu hóa trực tiếp soft prompt dẫn đến bất ổn trong quá trình huấn luyện. Thay vào đó, soft prompts được tham số hóa thông qua một mạng feed-forward Pθ= FFN(P̂θ). Trong quá trình huấn luyện, P̂θ và các tham số của FFN được tối ưu hóa. Sau huấn luyện, chỉ Pθ cần thiết cho suy luận, và FFN có thể được loại bỏ.
Pseudocode cho một lớp đơn:

```
def transformer_block_for_prefix_tuning (x):
    soft_prompt = FFN( soft_prompt )
    x = concat ([ soft_prompt , x], dim=seq)
    return transformer_block (x) 
```

Lưu ý rằng cách tiếp cận rất tương tự với Prompt Tuning (Phần 6.1), nhưng soft prompts được thêm vào mỗi lớp.
Trong các thí nghiệm của họ, Li and Liang (2021) áp dụng prefix tuning cho mô hình BART (Lewis et al., 2019) (ít hơn 1B tham số) cho các tác vụ tạo sinh khác nhau và cho thấy hiệu suất gần với tinh chỉnh đầy đủ bằng cách huấn luyện chỉ 0.1% số tham số. Độ dài soft prompt được sử dụng trong nghiên cứu thay đổi từ 10 đến 200 tokens.
6.3 Adaptive Prefix Tuning
Zhang et al. (2023b) mở rộng prefix tuning và đề xuất thay đổi thích ứng độ dài prompt. Các tác giả đạt được điều chỉnh thích ứng thông qua gating và scaling đầu vào cho mỗi lớp. Cụ thể, đầu vào cho mỗi lớp transformer đầu tiên được truyền qua một FFN (W) với kích hoạt sigmoid. Các giá trị được kích hoạt sigmoid hoạt động như gates cho mỗi pseudo token được thêm vào soft prompt, cung cấp cấu trúc để chọn hoặc bỏ chọn một cách thích ứng các soft tokens nhất định trong các lớp nhất định. Hơn nữa, các giá trị được kích hoạt sigmoid được nhân tỷ lệ với các tham số λ riêng biệt. W và λ đều là tham số có thể học và được định nghĩa riêng biệt cho mỗi lớp trong mô hình ngôn ngữ. Do đó, W và λ gây ra overhead đáng kể về tính toán, bộ nhớ và lưu trữ ngoài cách tiếp cận prefix-tuning (Li and Liang, 2021).

```
def adaptive_adjustment (x, layer_index ):
    alpha = sigmoid (W[ layer_index , ...] @ x)
    adaptive_factor = lambda_ [ layer_index ] * alpha
    return adaptive_factor
    
def prefix_tuning (x, layer_index ):
    # adaptive gating và scaling
    adaptive_factor = adaptive_adjustment (x, layer_index )
    # soft prompt
    soft_prompt = FFN( soft_prompt )
    soft_prompt *= adaptive_factor
    # prefix-tuning thông thường
    x = concat ([ soft_prompt , x], dim=seq)
    return transformer_block (x) 
```

Tuy nhiên, overhead được chứng minh là có lợi trong các mô hình BERT và DeBERTa series. Adaptive prefix tuning liên tục vượt trội cách tiếp cận prefix tuning tiêu chuẩn và thậm chí vượt qua hiệu suất tinh chỉnh đầy đủ trong hầu hết các trường hợp.
6.4 Intrinsic Prompt Tuning (IPT)
Prompt tuning chậm hội tụ. Một vài nghiên cứu (Su et al., 2021; Vu et al., 2021) đã đề xuất tiền huấn luyện soft prompts để cải thiện hiệu suất và tốc độ hội tụ. Tuy nhiên, các phương pháp này không cung cấp giải pháp để giảm số lượng tham số mỗi tác vụ.
Qin et al. (2021) giả thuyết rằng không gian chiều cao được sử dụng để định nghĩa một soft prompt chứa một "không gian con tác vụ nội tại" chiều thấp và học nó sử dụng một autoencoder theo kiểu đa tác vụ.
Phương pháp IPT hoạt động trong ba bước. Đầu tiên, với một tập hợp các tác vụ huấn luyện, soft prompts của chúng được học theo cách tiêu chuẩn (Phần 6.1). Sau đó, các prompts này được sử dụng để huấn luyện một autoencoder nén chiều của chúng. Sau này, phần encoder được loại bỏ, và chỉ đầu vào cho decoder autoencoder được huấn luyện trên các tác vụ mới.

```
def autoencoder ( soft_prompt ):
    soft_prompt = soft_prompt . flatten ()
    P = FFN_A ( soft_prompt ) # encoder
    P = FFN_B (P) # decoder
    P = P. reshape ( prompt_len , hidden )
    return P
    
def ipt_model (x):
    P = FFN_B ( intrinsic_subspace_prompt )
    P = P. reshape ( prompt_len , hidden )
    x = concat ([P, x], dim =seq)
    return model (x) 
```

Mặc dù framework IPT giảm số lượng tham số cho các tác vụ chưa thấy, việc giảm này có cái giá của việc huấn luyện autoencoder. Các tác giả tiến hành thí nghiệm với mô hình BART-base và độ dài prompt 100. Autoencoder kết quả, được triển khai⁵ như một mạng kết nối đầy đủ chấp nhận một tensor một chiều kích thước 76800, đạt 78 triệu tham số. Điều này chiếm hơn 56% tổng số tham số trong mô hình BART-base. Do đó, các phương pháp mã hóa prompt hiệu quả hơn đáng kể là cần thiết để làm cho IPT có thể áp dụng thực tế.
7 Các phương pháp bổ sung: Các cách tiếp cận khác
7.1 Ladder-Side Tuning (LST)
Ladder-Side Tuning (Sung et al., 2022) huấn luyện một mạng transformer nhỏ bên cạnh mạng được tiền huấn luyện. Mạng bên này kết hợp trạng thái ẩn của mạng backbone được tiền huấn luyện với trạng thái ẩn của chính nó.
Theo cách này, mạng bên chỉ sử dụng mô hình được tiền huấn luyện như một trích xuất đặc trưng, và backpropagation chỉ phải được tính toán thông qua mạng bên, tiết kiệm cả bộ nhớ và tính toán trong quá trình huấn luyện. Để cải thiện hiệu suất và hiệu quả tham số của LST, mạng bên được khởi tạo từ các tham số mô hình được tiền huấn luyện được cắt tỉa có cấu trúc và sử dụng một nửa số lớp.
Ở đây h_pt là đầu ra của lớp tương ứng của mạng được tiền huấn luyện, và alpha là một gate vô hướng có thể huấn luyện độc lập với đầu vào:

```
def ladder_side_layer (x, h_pt ):
    h_pt = h_pt @ W_down # to x. shape
    gate = sigmoid ( alpha )
    x = gate * x + (1 - gate ) * h_pt
    return transformer_block (x)
    
def ladder_side_network (x):
    with no_grad ():
        H_pt = pretrained_network (x, return_all_hiddens = True )
    for i in range ( layers ):
        layer = ladder_side_layers [i]
        x = layer (x, H_pt [i])
    return x 
```

LST chứng minh giảm RAM gấp ba lần trong việc tinh chỉnh T5-Base so với tinh chỉnh đầy đủ và giảm sử dụng RAM gấp hai lần so với LoRA (Phần 9.2) với sự suy giảm nhỏ về độ chính xác. Hơn nữa, LST vượt trội các phương pháp này khi kiểm soát việc sử dụng RAM.
7.2 (IA)³
Liu et al. (2022) đề xuất một phương pháp hiệu quả tham số mới để tinh chỉnh đa tác vụ T0 (Liu et al., 2022). Phương pháp tinh chỉnh được đề xuất của họ, (IA)³, học các tham số mới lv, lk, và lff, thay đổi tỷ lệ key, value, và kích hoạt FFN ẩn. Cụ thể:

```
def transformer_block_with_ia3 (x):
    residual = x
    x = ia3_self_attention (x)
    x = LN(x + residual )
    residual = x
    x = x @ W_1 # FFN input
    x = l_ff * gelu (x) # (IA)3 scaling
    x = x @ W_2 # FFN output
    x = LN(x + residual )
    return x
    
def ia3_self_attention (x):
    k, q, v = x @ W_k , x @ W_q , x @ W_v
    k = l_k * k
    v = l_v * v
    return softmax (q @ k.T) @ V 
```

Chỉ huấn luyện ba vector này, lv, lk, và lff, cho mỗi khối transformer dẫn đến hiệu quả tham số cao. Đối với T0-3B, nó chỉ cập nhật khoảng 0.02% tham số mô hình và vượt trội các phương pháp khác, bao gồm Compacter (Phần 10.4), có số lượng tham số tương tự, và LoRA (Phần 9.2), có nhiều hơn 16 lần tham số có thể huấn luyện. Không giống như các mô hình được điều chỉnh adapter, các mô hình được điều chỉnh (IA)³ thể hiện overhead tối thiểu. Các vector lv và lk có thể được tích hợp vào các lớp tuyến tính tương ứng, và overhead duy nhất đến từ lff.
8 Các phương pháp có chọn lọc
Các phương pháp có chọn lọc tinh chỉnh một tập con của các tham số hiện có của mô hình. Có thể là lựa chọn dựa trên độ sâu lớp, lựa chọn dựa trên loại lớp, hoặc thậm chí lựa chọn tham số cá nhân.
8.1 BitFit
Ben-Zaken et al. (2021) đề xuất chỉ tinh chỉnh bias của mạng. Tức là, đối với mỗi lớp, W không thay đổi và chỉ b được huấn luyện.
BitFit chỉ cập nhật khoảng 0.05% tham số mô hình. Bài báo gốc chứng minh rằng phương pháp đạt được hiệu suất tương tự hoặc tốt hơn tinh chỉnh đầy đủ trong các kịch bản dữ liệu thấp và vừa cho các mô hình BERT (ít hơn 1B tham số). Nghiên cứu thêm cho thấy đối với các mô hình lớn hơn 1B tham số, BitFit kém hiệu suất đáng kể so với tinh chỉnh đầy đủ và các phương pháp PEFT khác (Sanh et al., 2022; Liu et al., 2022; Hu et al., 2022).

```
params = (p for n, p in model . named_parameters ()
          if " bias " in n)
optimizer = Optimizer ( params ) 
```

Kiến trúc không có bias Trong so sánh thực nghiệm của chúng tôi, chúng tôi nhận thấy rằng một số kiến trúc phổ biến không sử dụng các thuật ngữ bias trong mạng. Ví dụ, T5 chỉ sử dụng các thuật ngữ bias trong trọng số attention tương đối, và LLaMA không sử dụng các thuật ngữ bias trong toàn bộ mạng.
8.2 DiffPruning
DiffPruning (Guo et al., 2020) nhằm đạt được hiệu quả tham số bằng cách học một cập nhật thưa của trọng số mạng neural. Phương pháp giới thiệu một mask nhị phân có thể học trên trọng số, được ký hiệu bởi δ=z◦∆W, trong đó ◦ đại diện cho tích Hadamard. Mask tham số này được học trong quá trình tinh chỉnh mô hình như một phần của mục tiêu điều chính, là một xấp xỉ có thể vi phân với chuẩn L0 của vector cập nhật δ.
DiffPruning đạt được hiệu suất so sánh với tinh chỉnh đầy đủ trong khi chỉ sửa đổi 0.5% tham số mô hình trong các kịch bản <1B. Điều này làm cho nó trở thành một phương pháp hữu ích cho triển khai đa tác vụ trong các kịch bản edge (di động) nơi lưu trữ bị hạn chế. Tuy nhiên, phương pháp này đòi hỏi nhiều bộ nhớ hơn tinh chỉnh truyền thống, vì nó liên quan đến việc tối ưu hóa tất cả các tham số trong quá trình huấn luyện ngoài mask nhị phân có thể học.
8.3 Freeze and Reconfigure (FAR)
FAR (Vucetic et al., 2022) chọn các cột của ma trận tham số để huấn luyện và tái cấu hình các lớp tuyến tính thành các thành phần có thể huấn luyện và bị đông lạnh. Phương pháp hoạt động trong hai giai đoạn. Trong giai đoạn đầu tiên, các hàng quan trọng nhất của ma trận tham số được xác định để cập nhật. Quá trình này tương tự như cắt tỉa có cấu trúc và có thể sử dụng bất kỳ phương pháp cắt tỉa nào. Trong bài báo của họ, các tác giả tinh chỉnh mô hình trên một phần trăm dữ liệu và chọn top-r hàng dựa trên khoảng cách L1 giữa các mô hình tinh chỉnh và gốc.
Trong giai đoạn thứ hai, mạng được tái cấu hình bằng cách chia mỗi ma trận tham số W∈Rin×h thành một thành phần có thể huấn luyện Wt∈Rin×h′ và một thành phần bị đông lạnh Wf∈Rin×(h−h′), trong đó h′ là số lượng tham số có thể huấn luyện mong muốn. Các phép nhân ma trận với Wt và Wf được tính toán độc lập, và kết quả được nối. Một phép toán tương tự được thực hiện trên bias.

```
def far_layer (x):
    h1 = x @ W_t
    h2 = x @ W_f
    return concat ([h1 , h2], dim = -1) 
```

Mặc dù cách tiếp cận này tạo ra overhead tính toán bổ sung trong quá trình huấn luyện, nó cung cấp tính linh hoạt tuyệt vời về lựa chọn tham số trên phần cứng hiện đại sử dụng các framework tiêu chuẩn như PyTorch. Sau huấn luyện, các tham số có thể được tái cấu hình trở lại, loại bỏ bất kỳ overhead suy luận nào.
Bài báo gốc tập trung vào các kịch bản edge và sử dụng DistilBERT (66M tham số) cho các thí nghiệm của họ. Nó chỉ được áp dụng cho các lớp feed-forward, vì chúng chiếm phần lớn các tham số DistilBERT. FAR đạt được hiệu suất tương tự với tinh chỉnh trên GLUE và SQuAD 2.0 (Rajpurkar et al., 2018) trong khi chỉ cập nhật 6% tham số.
8.4 FishMask
FishMask (Sung et al., 2021) là một phương pháp tinh chỉnh thưa chọn top-p tham số của mô hình dựa trên thông tin Fisher của chúng. Thông tin Fisher được ước tính theo cách thông thường thông qua xấp xỉ đường chéo.
ˆFθ=1/N∑(i=1 to N)Ey∼pθ(y|xi)(∇θlogpθ(y|xi))²
Hoặc trong pseudocode:

```
sparsity = 0.99
N = len( data )
for x, y in data :
    loss = loss_fn ( model (x), y)
    loss . backward ()
    for n, p in model . named_params ():
        fisher [n] += p. grad ** 2 / N
threshold = percentile (fisher , sparsity )
masks = {n: f > threshold
         for n, f in fisher . items ()} 
```

FishMask đòi hỏi tính toán gradients cho tất cả các tham số trên một số batch dữ liệu. Tuy nhiên, sau khi các tham số Fisher cao nhất được chọn, chỉ chúng cần được tối ưu hóa. Phương pháp thường hoạt động ngang bằng với adapters nhưng kém hơn LoRA và (IA)³ (Các phần 9.2 và 7.2). Nó đã được đánh giá trên các mô hình BERT (ít hơn 1B tham số) và T0-3B. Tuy nhiên, FishMask có chi phí tính toán cao và không hiệu quả trên phần cứng học sâu đương thời do thiếu hỗ trợ cho các phép toán thưa.
8.5 DiffFit
Xie et al. (2023) kết hợp các cách tiếp cận có chọn lọc khác nhau với scaling kiểu bổ sung và nhắm mục tiêu cụ thể vào việc điều chỉnh mô hình khuếch tán. Trong nghiên cứu của họ, Xie et al. (2023) điều chỉnh bias, layer normalization, và các tham số embedding và thêm các tham số scaling mới cho việc trộn dư. Các tham số scaling được thêm riêng biệt cho các lớp self-attention và FFN. Pseudocode cho scaling như sau:

```
def forward (x, c, t):
    x += gamma_1 * self_attn (x, c, t)
    x += gamma_2 * FFN(x, c, t)
    return x 
```

Các tác giả tiến hành thí nghiệm tập trung vào các mô hình khuếch tán và cho thấy DiffFit vượt trội các cách tiếp cận PEFT khác. Ngoài ra, do overhead nhỏ của các tham số scaling, DiffFit cung cấp tốc độ điều chỉnh nhanh hơn và suy giảm tối thiểu trong hiệu quả suy luận và lưu trữ.
9 Các phương pháp dựa trên tái tham số hóa
Các phương pháp này sử dụng ý tưởng tái tham số hóa trọng số của mạng sử dụng một biến đổi low-rank. Điều này giảm số lượng tham số có thể huấn luyện trong khi vẫn cho phép phương pháp hoạt động với các ma trận chiều cao, chẳng hạn như các tham số được tiền huấn luyện của các mạng.
9.1 Intrinsic SAID
Trong công trình của họ, Aghajanyan et al. (2020) điều tra chiều nội tại của tinh chỉnh và chứng minh rằng quá trình này có thể được thực hiện trong một không gian con low-rank. Cụ thể, họ sử dụng biến đổi Fastfood để tái tham số hóa việc cập nhật trọng số mô hình.
Fastfood là một biến đổi mở rộng chiều hiệu quả tính toán F:Rd→RD có thể được thực hiện trong thời gian O(Dlogd) và bộ nhớ O(D).
Họ cho thấy rằng các mô hình lớn hơn đòi hỏi thay đổi trong không gian con có rank thấp hơn so với các mô hình nhỏ hơn để đạt được hiệu suất tinh chỉnh tương tự. Quan sát này thúc đẩy cả việc mở rộng các mô hình lớn và tinh chỉnh hiệu quả tham số. Điều quan trọng cần lưu ý là, không giống như các phương pháp chọn một tập con cụ thể của tham số để tinh chỉnh, Intrinsic SAID cập nhật tất cả các tham số mô hình theo kiểu low-rank, tức là θ=θ0+F(θd), trong đó θ0∈RD biểu thị các tham số mô hình được tiền huấn luyện và θd∈Rd biểu thị các tham số cần được tối ưu hóa. Do đó, trong khi số lượng tham số có thể tối ưu hóa thấp, độ phức tạp bộ nhớ O(D) của Fastfood và việc cập nhật tất cả các tham số của mô hình làm cho Intrinsic SAID không thực tế để tinh chỉnh các mạng lớn. Để biết thêm chi tiết về Fastfood, chúng tôi giới thiệu độc giả đến bài báo gốc của Le et al. (2013).
9.2 LoRA
LoRA (Hu et al., 2022) lấy cảm hứng từ Intrinsic SAID và đề xuất một cách đơn giản hơn để thực hiện tinh chỉnh low-rank. Cập nhật tham số cho một ma trận trọng số trong LoRA được phân tách thành tích của hai ma trận low-rank:
δW=WAWB, WA∈Rin×r, WB∈Rr×out.
Tất cả các tham số mô hình được tiền huấn luyện được giữ đông lạnh, và chỉ các ma trận WA và WB có thể huấn luyện. Hệ số tỷ lệ là hằng số và thường bằng 1/r. Sau huấn luyện, chúng có thể được tích hợp vào W gốc bằng cách chỉ cần thêm ma trận WAWB vào ma trận gốc W.
Pseudocode rất đơn giản:

```
def lora_linear (x):
    h = x @ W # linear thông thường
    dh = x @ W_A @ W_B # cập nhật low-rank
    h += scale * dh # scaling
    return h 
```

Trong Transformers, LoRA thường được sử dụng cho các ma trận chiếu WK và WV trong các mô-đun multi-head attention. Tuy nhiên, để đạt được hiệu suất tốt nhất có thể, tốt nhất là áp dụng LoRA cho tất cả các ma trận trọng số trong mô hình (Dettmers et al., 2023). Phương pháp vượt trội BitFit và Adapters và đã được đánh giá trên các mô hình có lên đến 175B tham số.
9.3 KronA
KronA (Edalati et al., 2022) thay thế phân tích ma trận δW=WAWB trong LoRA bằng phân tích ma trận thông qua tích Kronecker δW=WA⊗WB.
Điều này mang lại một sự đánh đổi rank-to-parameter tốt hơn vì tích Kronecker bảo tồn rank của các ma trận gốc được nhân. Hoặc, nói cách khác, rank(A⊗B) = rankA·rankB. Ngoài ra, Edalati et al. (2022) sử dụng một phép toán tích Kronecker-vector hiệu quả x(A⊗B), tránh biểu diễn δW một cách rõ ràng và dẫn đến tăng tốc đáng kể. KronAB_res, cũng được trình bày trong Edalati et al. (2022), là một adapter song song sử dụng tham số hóa tích Kronecker của trọng số và bao gồm một kết nối dư.
Pseudocode Krona:

```
def krona_linear (x):
    x = x @ W # linear thông thường
    x += kronecker_vector_prod (x, W_A , W_B)
    return scale * x
    
# tương tự như x @ kronecker_product (A, B)
def kronecker_vector_prod (x, A, B):
    x = x. reshape (A. shape [1] , B. shape [1])
    x = A.T @ x @ B
    return x. reshape ( -1) 
```

Trên GLUE, các phương pháp KronA hoạt động ngang bằng hoặc tốt hơn adapters (Phần 5.1), LoRA (Phần 9.2), và Compacter (Phần 10.4) với cùng số lượng tham số có thể huấn luyện 0.07%, trong khi nhanh hơn đáng kể so với adapters hoặc Compacter trong thời gian suy luận. Phương pháp chỉ được đánh giá trên các mô hình nhỏ (ít hơn 1B tham số).
Nền tảng: Tích Kronecker Tích Kronecker là một phép toán tensor được định nghĩa là
A⊗B:Rn×m×Rk×l→Rnk×ml
A⊗B=[a1,1B··· a1,nB; .......; am,1B···am,nB] (2)
Nó có thể được triển khai dễ dàng⁶ trong PyTorch sử dụng lệnh torch.einsum

```
def batched_kronecker_product (a, b):
    bs , i, j = a. shape
    bs , k, m = b. shape
    res = einsum ("bij ,bkm -> bikjm ", a, b)
    return res. view (bs , i * k, j * m) 
```

9.4 DoRA
Trong nghiên cứu được tiến hành bởi Liu et al. (2024), các tác giả kiểm tra sự khác biệt giữa tinh chỉnh đầy đủ và tinh chỉnh LoRA. Đặc biệt, các tác giả phân tích sự thay đổi về hướng (∆D) và độ lớn (∆M) của các ma trận trọng số, so sánh các checkpoint trước và sau tinh chỉnh đầy đủ. Với các giá trị ∆D và ∆M được tính toán cho các checkpoint khác nhau và nhiều lớp, các tác giả cho thấy sự tồn tại của mối quan hệ nghịch đảo giữa ∆D và ∆M cho tinh chỉnh đầy đủ. Tuy nhiên, đối với điều chỉnh LoRA, ∆D và ∆M tỷ lệ thuận với nhau. Do đó, các tác giả đề xuất DoRA, một phương pháp tách độ lớn khỏi phần còn lại của cập nhật trọng số, tương tự như chuẩn hóa trọng số (Salimans and Kingma, 2016). Triển khai DoRA như sau:

```
def initialize_m (W):
    # hệ số tỷ lệ có thể học (thành phần độ lớn)
    m = Parameter ( l2_norm (W), requires_grad = True )
    return m
    
def dora_linear (x):
    W_cur = W + (W_A @ W_B)
    v = W_cur / l2_norm ( W_cur ) # chuẩn hóa trọng số
    W_cur = m * v # nhân với độ lớn có thể học
    # truyền xuôi thông thường
    return x @ W_cur + b 
```

Các tác giả chứng minh lý thuyết rằng với việc scaling được đề xuất, một mối quan hệ nghịch đảo giữa ∆D và ∆M có thể đạt được với tinh chỉnh kiểu LoRA. Nói cách khác, DoRA có thể đạt được hoặc là thay đổi hướng lớn với điều chỉnh nhỏ về độ lớn, hoặc ngược lại. Các thí nghiệm được tiến hành trên các tác vụ và mô hình khác nhau cho thấy cải thiện nhất quán trong DoRA so với LoRA. Thú vị, DoRA cải thiện hiệu quả mẫu trên các tác vụ instruction tuning và cho thấy cải thiện hiệu suất đáng kể trong chế độ low-rank (4, 8) so với LoRA.
9.5 GLoRA
Trong LoRA, các trọng số được tiền huấn luyện chỉ được biến đổi thông qua phép cộng. Mặc dù hiệu quả, các tác vụ khó khăn hơn có thể hưởng lợi từ việc scaling và kích hoạt thêm. Theo cùng giả thuyết, Chavan et al. (2023) đề xuất một biến thể của LoRA được gọi là GLoRA (viết tắt của Generalized-LoRA). Trong GLoRA, các tác giả cải thiện khả năng của LoRA với chi phí của nhiều tham số có thể học hơn để scale và shift các tham số hoặc kích hoạt hoặc cả hai. Tuy nhiên, tất cả các tham số có thể học của GLoRA được hợp nhất trở lại vào mô hình bị đông lạnh, do đó không phát sinh chi phí suy luận bổ sung. Cập nhật GLoRA có thể được viết như sau:
f(x) = (W0+W0A+B)x+CW0+Db0+E+b0
Trong đó, W0 và b0 là các tham số được tiền huấn luyện và được giữ đông lạnh trong quá trình tinh chỉnh. Các tham số GLoRA, A, B, C, D, và E có thể học và được điều chỉnh trong quá trình tinh chỉnh. Trong biểu thức trên, ma trận B sao chép cập nhật LoRA. Tất cả các tham số có thể học khác tức là A, C, D, và E do đó, thêm một overhead cho tinh chỉnh so với LoRA. Tuy nhiên, để giảm overhead này, các ma trận A, B và C có thể được triển khai với xấp xỉ low-rank tức là A=Ain·Aout sao cho Ain∈Rd1×r và Aout∈Rr×d2, cho A∈Rd1×d2. Các tác giả cũng chỉ định rằng tất cả các tham số mới (A đến E) có thể được chuyển đổi thành vectors hoặc scalars để quản lý ngân sách tính toán.

```
def glora_linear (x):
    # điều chỉnh đầu vào
    W_cur = W_0 + AW_0 + B
    h = x @ W_cur + CW_0
    # thêm bias (chủ yếu không có trong các LLMs transformer gần đây)
    if bias_present :
        h += Db_0 + E + b_0
    return h 
```

Trong cập nhật GLoRA ở trên, tất cả các tham số có thể học (A, B, C, D, và E) có thể hợp nhất trở lại vào mô hình gốc sau tinh chỉnh. Trên các bộ tác vụ thị giác và ngôn ngữ khác nhau, các tác giả cho thấy GLoRA vượt trội LoRA. Khi số lượng tham số có thể huấn luyện được giữ không đổi cho cả LoRA và GLoRA, GLoRA vượt trội LoRA trên benchmark VTAB-1K. Ngoài ra, GLoRA cho thấy hiệu quả mẫu tốt hơn LoRA và các phương pháp PEFT khác với số lượng tham số có thể huấn luyện so sánh.
9.6 AdaLoRA
Trong AdaLoRA, Zhang et al. (Zhang et al., 2023a) giới thiệu một phương pháp thích ứng để giảm rank của các ma trận WA và WB trong LoRA. Họ tái công thức hóa cập nhật LoRA trong định dạng giống SVD:
W=W+WAΛWB
trong đó Λ là một ma trận đường chéo đại diện cho các giá trị singular. Để điều chỉnh rank, các tác giả cắt tỉa Λ, ảnh hưởng đến chiều của WA và WB dựa trên điểm số quan trọng phản ánh tác động lên loss. Các điểm số này được làm mịn theo cấp số nhân để hướng dẫn việc loại bỏ tham số. Phương pháp bao gồm điều chính để bảo tồn tính trực giao giữa WA và WB. Các thí nghiệm của họ với DeBERTa-base và BART-large chứng minh rằng AdaLoRA vượt trội LoRA và các phương pháp tinh chỉnh hiệu quả tham số (PEFT) khác dưới ràng buộc ngân sách tham số. Nghiên cứu nhấn mạnh rằng các lớp transformer ban đầu không cần cập nhật high-rank. Tuy nhiên, nó đòi hỏi theo dõi các biến bổ sung (cụ thể là I, Ī, Ū, và S, tham khảo (Zhang et al., 2023a)) để cắt tỉa các giá trị Λ. Điều này có thể gây ra overhead bộ nhớ đáng kể khi kích thước mô hình ngôn ngữ hoặc rank được nhân tỷ lệ. Tuy nhiên, AdaLoRA trình bày những phát hiện sâu sắc thúc đẩy nhu cầu phát triển một chiến lược ranking có chọn lọc cho LoRA có thể hoạt động mà không có overhead bộ nhớ đáng kể. Chúng tôi trình bày một pseudocode đơn giản hóa của các hoạt động chính trong AdaLoRA như sau,

```
def prune (W_A , ... , dW_A , ... , k):
    # tính toán điểm số quan trọng
    s = importance_score (W_A , ... , dW_A , ...)
    # tìm k giá trị thấp nhất
    indices = find_bottom_k (s, k)
    # cắt tỉa
    W_A [ indices , :] = W_A [ indices , :]. detach ()
    W_B [:, indices ] = W_A [:, indices ]. detach ()
    lambda_ [ indices ] = 0
    lambda_ [ indices ] = lambda_ [ indices ]. detach ()
    return W_A , W_B , lambda_
    
def ada_lora_linear (x):
    h = x @ W # linear thông thường
    h += x @ W_A @ lambda_ @ W_B # cập nhật low-rank
    return h 
```

9.7 GaLore
Thường các phương pháp PEFT cải thiện hiệu quả tính toán bằng cách giảm số lượng tham số có thể huấn luyện với chi phí làm giảm tính biểu cảm. Zhao et al. (2024) đề xuất huấn luyện tất cả các tham số của mô hình, nhưng sử dụng điều chính low-rank của gradients. Về bản chất, GaLore làm cho full-tuning hiệu quả bằng cách giảm tiêu thụ bộ nhớ của trạng thái optimizer (moment thứ nhất và thứ hai). Các tác giả cũng cung cấp lý giải lý thuyết của phương pháp.
Dựa trên phân tích lý thuyết, các tác giả đề xuất GaLore với các tính chất nổi bật sau: 1. Không có xấp xỉ low-rank của ma trận tham số; 2. Phân tách gradients trong không gian chiều thấp hơn; 3. Điều chính gradient trong không gian chiều thấp; 4. Chiếu gradient được điều chính trong không gian gốc và cập nhật tham số. GaLore đạt được giảm lợi ích bộ nhớ bằng cách giảm yêu cầu lưu trữ cho thống kê gradient (moment bậc nhất và bậc hai) và bằng cách làm cho điều chính gradient gọn gàng hơn.

```
def galore_update (w, grad , lr):
    u, s, v = SVD( grad )
    m, n = u. shape [-1], v. shape [0]
    p = u if m < n else v
    grad_ = p.T @ grad
    # cập nhật momentum optimizer
    m, v = update_momentum ( grad_ )
    # điều chính grad
    g = m / ( sqrt (v) + eps)
    # chiếu ngược
    grad = alpha * (p @ g)
    return w - lr * grad # cập nhật param
    
def update_momentum ( grad_ ):
    # bậc nhất
    m = beta_1 * m + (1 - beta_1 ) * grad_
    m \= (1 - beta_1 **t)
    # bậc hai
    v = beta_2 * m + (1 - beta_2 ) * grad_ **2
    v \= (1 - beta_2 **t)
    return m, v 
```

Các tác giả đánh giá phương pháp được đề xuất cho cả tiền huấn luyện và tinh chỉnh. Về mặt tiền huấn luyện, GaLore đạt được perplexity validation tốt hơn trên dữ liệu C4 so với LoRA (Hu et al., 2022) và ReLoRA (Lialin et al., 2023) cho một loạt kích thước mô hình thay đổi từ 60M đến 1B. So với tiền huấn luyện full-rank GaLore đạt được các giá trị perplexity so sánh nhưng hơi tệ hơn trong khi giảm đáng kể việc sử dụng bộ nhớ. Đối với phần tinh chỉnh, các tác giả tinh chỉnh RoBERTa-base trên các tác vụ GLUE và nhấn mạnh lợi ích của GaLore so với LoRA.
9.8 Lượng tử hóa và LoRA
Trong LoRA, mô hình ngôn ngữ được giữ đông lạnh ở độ chính xác đầy đủ. Do kích thước ngày càng tăng của các mô hình ngôn ngữ, mô hình ngôn ngữ bị đông lạnh gây ra dấu chân bộ nhớ lớn đáng kể. Do đó, các nghiên cứu gần đây đã tập trung vào việc giảm tải bộ nhớ của LLMs backbone bằng cách giảm độ chính xác của các tham số LLM backbone.
Một nghiên cứu được tiến hành bởi Dettmers et al. (2023) giải quyết vấn đề này với QLoRA. Các tác giả đạt được lợi ích bộ nhớ theo ba cách. Đầu tiên, các tác giả giới thiệu một phương pháp lượng tử hóa 4-bit NormalFloat giảm bộ nhớ cần thiết để lưu trữ mô hình backbone. Thứ hai, các tác giả giảm dấu chân bộ nhớ của các hằng số lượng tử hóa bằng cách giới thiệu lượng tử hóa kép, tức là lượng tử hóa của các hằng số lượng tử hóa. Cả hai bước đầu tiên đều nhận ra lợi ích bộ nhớ đáng kể trong việc lưu trữ mô hình backbone trên GPU. Cuối cùng, các tác giả triển khai một cơ chế paging giữa CPU và GPU cho các trạng thái optimizer, offloading các trạng thái optimizer lên CPU trong trường hợp đột biến bộ nhớ. Với các thí nghiệm mở rộng, các tác giả nhấn mạnh lợi ích hiệu suất của cách tiếp cận lượng tử hóa được đề xuất của họ.
Tuy nhiên, lượng tử hóa sau huấn luyện của mô hình tinh chỉnh LoRA cuối cùng thường dẫn đến sự khác biệt hiệu suất (Xu et al., 2023). Trong một nghiên cứu khác, Xu et al. (2023) trình bày một chiến lược lượng tử hóa theo nhóm vượt qua nhu cầu lượng tử hóa sau huấn luyện, do đó dẫn đến hiệu suất tốt hơn, ngay cả trong các chế độ độ chính xác thấp hơn. Nhiều nghiên cứu hơn đã đề xuất các phương pháp tinh chỉnh nhận biết lượng tử hóa tốt hơn trong tài liệu gần đây, ví dụ, IR-QLoRA (Qin et al., 2024), LoftQ (Li et al., 2023), L4Q (Jeon et al., 2024), LQ-LoRA (Guo et al., 2023).
10 Các cách tiếp cận lai
Các phương pháp lai tận dụng thế mạnh của các kỹ thuật khác nhau trong khi giảm thiểu điểm yếu của chúng để cải thiện hiệu suất và hiệu quả của PEFT.
10.1 SparseAdapter
He et al. (2022b) đề xuất một chiến lược Large-Sparse để huấn luyện các lớp adapter. Trong chiến lược này, họ sử dụng một chiều ẩn lớn cho mô-đun được thêm và cắt tỉa khoảng 40% các giá trị khi khởi tạo. Large-Sparse liên tục vượt trội đối tác không thưa của nó với cùng số lượng tham số có thể huấn luyện. Tuy nhiên, chi phí huấn luyện và suy luận có thể cao hơn tùy thuộc vào hỗ trợ phần cứng cho tensors và phép toán thưa. Cũng đáng chú ý rằng việc tính toán mask cắt tỉa cho phương pháp này có thể đòi hỏi việc thu được gradients cho tất cả các tham số mới được thêm.
10.2 MAM Adapters
Trong nghiên cứu của họ, He et al. (2022a) điều tra kỹ lưỡng việc đặt adapter và soft prompts. Họ kết luận rằng các adapter song song có tỷ lệ vượt trội các adapter được đặt tuần tự và việc đặt một adapter song song với FFN vượt trội các adapter song song multi-head attention. Họ cũng nhận thấy rằng soft prompts có thể sửa đổi hiệu quả attentions bằng cách chỉ thay đổi 0.1% các tham số và đề xuất 'mix-and-match' (MAM) những ý tưởng này. Mô hình cuối cùng của họ, MAM Adapter, là sự kết hợp của adapter song song có tỷ lệ cho lớp FFN và soft prompt.

```
def transformer_block_mam (x):
    x = concat ([x, soft_prompt ], dim=seq)
    residual = x
    x = SelfAttention (x)
    x = LN(x + residual )
    x_a = FFN(x) # adapter song song
    x_a = scale * x_a
    x = LN(x + x_adapter )
    return x 
```

Phương pháp MAM vượt trội BitFit và Prompt Tuning bằng một biên độ lớn và liên tục vượt trội LoRA (Phần 9.2), Adapters (Phần 5.1), và Prefix Tuning (Phần 6.2) với độ dài soft prompt 200 và 7% tham số thêm. Các thí nghiệm được tiến hành trên các mô hình có ít hơn 1 tỷ tham số.
Đáng chú ý rằng các adapter song song đã được nghiên cứu độc lập bởi Zhu et al. (2021) trong lĩnh vực dịch máy.
10.3 UniPELT
UniPELT (Mao et al., 2021) là sự kết hợp có cổng của LoRA, Prefix-tuning, và Adapters. Tái tham số hóa LoRA được sử dụng cho các ma trận attention WQ và WV, prefix-tuning được áp dụng cho keys và values của mỗi lớp, và adapters được thêm sau lớp feed-forward của khối transformer. Đối với mỗi mô-đun, gating được triển khai như một lớp tuyến tính chiếu đầu vào mô-đun vào một chiều kích thước một, kích hoạt sigmoid, và trung bình hóa vector kết quả qua độ dài chuỗi. Các tham số có thể huấn luyện bao gồm các ma trận LoRA WA, WB, các tham số prompt tuning Pq, Pk, các tham số adapter, và trọng số hàm gating.
Triển khai sơ đồ của UniPELT (bỏ qua attention heads để đơn giản):

```
def transformer_block_with_unipelt (x):
    residual = x
    x = unipelt_self_attention (x)
    x = LN(x + residual )
    residual = x
    x = FFN(x)
    adapter_gate = gate (x)
    x = adapter_gate * FFN (x)
    x = LN(x + residual )
    return x
    
def unipelt_self_attention (x):
    k, q, v = x @ W_k , x @ W_q , x @ W_v
    # lora cho queries và values
    lora_gate = gate (x)
    q += lora_gate * W_qA @ W_aB
    v += lora_gate * W_vA @ W_vB
    # prefix tuning
    pt_gate = gate (x)
    q_prefix = pt_gate * P_q
    k_prefix = pt_gate * P_k
    return softmax (q @ k.T) @ V
    
def gate (x):
    x = Linear (x)
    x = sigmoid (x)
    return mean (x, dim =seq) 
```

UniPELT chứng minh cải thiện đáng kể so với các cách tiếp cận LoRA, Adapters, và Prefix Tuning riêng lẻ trong các kịch bản dữ liệu thấp chỉ với 100 ví dụ. Trong các kịch bản dữ liệu cao hơn, UniPELT hoạt động ngang bằng hoặc tốt hơn các cách tiếp cận này. Mao et al. (2021) báo cáo rằng UniPELT sử dụng 1.3% tham số mô hình có thể huấn luyện trên các mô hình BERT có ít hơn một tỷ tham số.
10.4 Compacter
Compacter (Karimi Mahabadi et al., 2021) sử dụng tích Kronecker, ma trận low-rank, và chia sẻ tham số qua các lớp để tạo ra trọng số adapter. Mỗi tham số W trong một adapter bằng tổng của các tích Kronecker
Ŵ=∑(i=0 to n)Ai⊗Bi
Ŵ∈Rk×d, Ai∈Rn×n, Bi∈Rk/n×d/n. (3)
Một lớp tuyến tính xŴ+b với tham số hóa này được gọi là lớp parametrized hypercomplex multiplication (PHM) (Zhang et al., 2021). Compacter đưa ý tưởng này xa hơn, tham số hóa Bi tương tự như LoRA (Phần 9.2) như Bi=Bdown_iBup_i, trong đó tất cả các ma trận có rank tối đa r. Các ma trận Ai được chia sẻ qua tất cả các lớp adapter để hiệu quả tham số thêm. Lớp tương ứng được gọi là Low-rank PHM (LPHM). Lưu ý rằng tất cả các tensors Ai và Bi đều là tensors 3D với chiều đầu tiên bằng n, số lượng tích Kronecker trong lớp PHM. Pseudocode lớp Compacter:

```
def compacter (x):
    x = LPHM (x) # Về cơ bản là một FFN
    x = gelu (x) # nhưng
    x = LPHM (x) # LPHM thay thế linear
    return x
    
def lphm_forward (x):
    B = B_d @ B_u
    W = batched_kronecker_product (A, B)
    W = sum(W, dim =0)
    return x @ W + b 
```

Compacter có hai hương vị: hai adapters mỗi khối transformer hoặc một adapter đơn sau lớp feedforward (Compacter++). Chỉ với 0.05% tham số bổ sung, Compacter++ hoạt động ngang bằng hoặc tốt hơn adapters với 0.8% tham số bổ sung. Mô hình đã được đánh giá trên T5 Base (ít hơn 1B tham số) và các mô hình T0-3B.
10.5 S4
Chen et al. (2023) tiến hành tìm kiếm các kết hợp khác nhau của các kỹ thuật tinh chỉnh hiệu quả tham số. Không gian tìm kiếm của họ bao gồm việc chia các lớp liên tiếp thành bốn nhóm không đều, phân bổ lượng tham số có thể huấn luyện khác nhau cho mỗi lớp, xác định nhóm nào cần tinh chỉnh, và phương pháp PEFT nào áp dụng cho mỗi nhóm.
Phương pháp được đề xuất của họ, S4, chia các lớp thành bốn nhóm (G1,2,3,4) sử dụng mẫu "spindle": nhiều lớp hơn được phân bổ cho các nhóm giữa và ít hơn cho các nhóm trên và dưới. Tất cả các nhóm đều có thể huấn luyện, với các tham số có thể huấn luyện được phân bổ đồng đều qua các lớp trong mỗi nhóm. Các kết hợp khác nhau của các phương pháp PEFT được áp dụng cho các nhóm khác nhau. Cụ thể:
G1:A, L G3:A, P, B
G2:A, P G4:P, B, L (4)
trong đó A là viết tắt của Adapters (Phần 5.1), P cho Prefix-Tuning (Phần 6.2), B cho BitFit (Phần 8.1), và L cho LoRA (Phần 9.2).
Các thí nghiệm tìm kiếm được tiến hành trên mô hình T5-base và bộ dữ liệu GLUE với 0.5% tham số có thể huấn luyện. Phương pháp S4 sau đó được áp dụng cho T5-3B, RoBERTa, và XL-Net, liên tục vượt trội BitFit, Prefix Tuning, LoRA, và Adapters riêng lẻ qua các kiến trúc, kích thước mô hình, và tác vụ khác nhau.
11 So sánh các phương pháp PEFT
Hiệu quả tham số liên quan đến nhiều khía cạnh: lưu trữ, bộ nhớ, tính toán, và hiệu suất. Tuy nhiên, việc đạt được hiệu quả tham số một mình không nhất thiết dẫn đến giảm sử dụng RAM hoặc huấn luyện nhanh hơn. Khi đánh giá các phương pháp PEFT, điều quan trọng là phải xem xét các khía cạnh khác nhau của hiệu suất và hiệu quả, tuy nhiên nhiều ấn phẩm chỉ tập trung vào hiệu suất downstream và số lượng tham số được sử dụng bởi phương pháp PEFT.
"Số lượng tham số" là một metric thường được báo cáo trong tài liệu PEFT, nhưng nó có thể đề cập đến những thứ khác nhau: số lượng tham số có thể huấn luyện, tham số đã thay đổi, hoặc rank của cập nhật (ví dụ, Aghajanyan et al. (2020)). Trong một số trường hợp, số lượng tham số có thể huấn luyện so với đã thay đổi có thể khác nhau theo bậc độ lớn. Ví dụ bao gồm DiffPruning và LT-SFT (Guo et al., 2020; Ansell et al., 2022), đầu tiên tinh chỉnh mạng đầy đủ và sau đó cắt tỉa cập nhật, hoặc Prefix Tuning (Li and Liang, 2021), sử dụng tái tham số hóa FCN của các tham số prompt.
Chúng tôi cung cấp so sánh chi tiết về các tham số có thể huấn luyện và được cập nhật cho 28 phương pháp PEFT trong Bảng 2 và thảo luận vấn đề này thêm trong Phần ??. Nhìn chung, các phương pháp thưa có xu hướng có nhiều tham số có thể huấn luyện hơn tham số đã thay đổi, trong khi các phương pháp tái tham số hóa thường có ít tham số có thể huấn luyện hơn do bản chất của tái tham số hóa.
Trong nghiên cứu của chúng tôi, chúng tôi xem xét năm chiều chính cần thiết để đánh giá hiệu quả của các phương pháp tinh chỉnh hiệu quả tham số. Các chiều này bao gồm hiệu quả lưu trữ, hiệu quả bộ nhớ, hiệu quả tính toán, overhead suy luận, và metrics hiệu suất downstream (ví dụ độ chính xác). Phân tích của chúng tôi về tài liệu đã xuất bản cho thấy rằng mặc dù các chiều này có liên quan với nhau, cải thiện theo một trục không nhất thiết dịch thành cải thiện theo các trục khác. Ví dụ, tối ưu hóa chỉ cho hiệu quả tham số không đảm bảo giảm sử dụng RAM. Bảng 1 tóm tắt những phát hiện của chúng tôi.
Mặc dù có ý nghĩa của chúng, các metrics hiệu suất PEFT như hiệu quả bộ nhớ, tốc độ huấn luyện, và overhead suy luận (ví dụ, thông lượng) chỉ thỉnh thoảng được định lượng trong các bài báo. Tuy nhiên, việc trình bày các metrics này chỉ giúp phân tích thêm một phương pháp PEFT cụ thể, một cách riêng lẻ. So sánh trực tiếp qua các phương pháp PEFT khác nhau vẫn đầy thách thức, chủ yếu do tác động của thiết lập thực nghiệm lên metrics hiệu suất. Do đó, để giải quyết khoảng trống này, chúng tôi cố định thiết lập thực nghiệm và thực hiện so sánh thực nghiệm quy mô lớn của các phương pháp PEFT như một phần của khảo sát này. Chúng tôi thảo luận chi tiết về thiết lập thực nghiệm và kết quả của chúng tôi trong các phần tiếp theo.
11.1 So sánh thực nghiệm: Thiết lập
So sánh thực nghiệm của chúng tôi được thiết kế để cung cấp đánh giá toàn diện về các phương pháp PEFT, vượt qua phạm vi và độ sâu của các nghiên cứu hiện có như Ding et al. (2022). Chúng tôi đã chọn cẩn thận 14 phương pháp PEFT đại diện cho các danh mục đa dạng trong phân loại của chúng tôi – bao gồm các phương pháp Additive, Selective, dựa trên Reparametrization, và Hybrid – để đảm bảo phân tích rộng và bao gồm. Đáng chú ý, chúng tôi loại trừ các phương pháp selective thưa, thừa nhận tính thực tế hạn chế của chúng trên phần cứng hiện đại và trọng tâm chính của chúng vào hiệu quả lưu trữ. Hơn nữa, nghiên cứu của chúng tôi bỏ qua BitFit trong bối cảnh các mạng T5, không sử dụng bias. Ngoài các phương pháp PEFT này, chúng tôi bao gồm một baseline tinh chỉnh đầy đủ để cung cấp điểm tham chiếu cho cả hiệu suất tác vụ downstream và metrics hiệu quả. Không giống như Ding et al. (2022), giới hạn so sánh thực nghiệm của họ ở bốn phương pháp PEFT và tập trung chủ yếu vào hiệu suất downstream và tiêu thụ bộ nhớ, thiết kế thực nghiệm của chúng tôi bao gồm một loạt phương pháp rộng hơn và đánh giá chúng trong tất cả những điều sau: hiệu quả bộ nhớ, tốc độ huấn luyện, tốc độ suy luận, và hiệu suất downstream.
Bộ dữ liệu Chúng tôi sử dụng cả các tác vụ hiểu ngôn ngữ tự nhiên (NLU) và tạo sinh ngôn ngữ tự nhiên (NLG) để so sánh các phương pháp.
Mặc dù benchmark GLUE (Wang et al., 2018) thường được sử dụng để đánh giá các phương pháp tinh chỉnh hiệu quả tham số trong tài liệu hiện có, nhiều mô hình hiện nay phù hợp hoặc vượt hiệu suất con người trên các tác vụ GLUE, một số thậm chí với không hoặc huấn luyện tối thiểu⁷. Điều này làm cho GLUE ít hiệu quả hơn trong việc đánh giá hiệu suất quy trình tinh chỉnh. Gần đây hơn, các giải pháp thay thế được đề xuất cho GLUE bao gồm MMLU (Hendrycks et al., 2021), HELM (Liang et al., 2023), và BigBench (Srivastava et al., 2022). MMLU nhấn mạnh đánh giá zero- hoặc few-shot, làm cho nó không phù hợp để đánh giá tinh chỉnh. Cả HELM và BigBench đều đưa ra thách thức tính toán do tính đa dạng tác vụ của chúng, đặc biệt khi so sánh một loạt phương pháp và mô hình rộng lên đến 11B tham số.
Ngược lại, các tác vụ SuperGLUE vẫn vừa đòi hỏi (chỉ với một vài mô hình vượt qua baseline con người) và có thể quản lý được về mặt tính toán. Cụ thể, chúng tôi chọn BoolQ, RTE, và COPA cho nghiên cứu này. BoolQ là một bộ dữ liệu trả lời câu hỏi có/không chủ yếu đánh giá kiến thức thế giới của mô hình. COPA tập trung vào lý luận nhân quả thông thường, ví dụ, "Tiền đề: Người đàn ông làm gãy ngón chân. Nguyên nhân của điều này là gì? Lựa chọn 1: Anh ấy có lỗ trên tất. Lựa chọn 2: Anh ấy làm rơi búa lên chân." RTE là một bộ dữ liệu suy luận ngôn ngữ tự nhiên nơi, với một tiền đề, mô hình cần dự đoán liệu giả thuyết có thể được suy ra từ nó, mâu thuẫn với nó, hay không liên quan.
Để so sánh đa dạng hơn, chúng tôi cũng bao gồm một bộ dữ liệu tạo sinh ngôn ngữ tự nhiên: CNN-Dailymail (See et al., 2017), một bộ dữ liệu tóm tắt lớn (300K ví dụ huấn luyện). Từ tài liệu khảo sát, chúng tôi phát hiện rằng tóm tắt thường làm nổi bật sự khác biệt giữa các phương pháp PEFT và tinh chỉnh đầy đủ, làm cho bộ dữ liệu này đặc biệt hữu ích.
Mô hình Để so sánh các phương pháp tinh chỉnh hiệu quả tham số, chúng tôi áp dụng chúng cho ba kích thước T5: Large (0.7B), 3B, và 11B (Raffel et al., 2019). Phạm vi từ 0.7B đến 11B mô hình không chỉ kiểm tra hiệu quả của mỗi phương pháp ở các quy mô khác nhau mà còn đưa ra những thách thức phổ biến liên quan đến huấn luyện mô hình quy mô lớn. Một khía cạnh chính của so sánh này là chứng minh cách các phương pháp PEFT có thể giải quyết các vấn đề thực tế như ràng buộc bộ nhớ. Ví dụ, mô hình 11B cho phép chúng tôi so sánh hiệu suất và hiệu quả của các phương pháp PEFT trong một trong những trường hợp thực tế liên quan nhất khi tinh chỉnh đầy đủ không vừa vào ngay cả 80GB bộ nhớ GPU.
Các phương pháp PEFT Chúng tôi sử dụng các phương pháp tinh chỉnh sau trong so sánh của chúng tôi:
• Full tuning – tinh chỉnh thông thường của tất cả các tham số mô hình
• Houlsby – Adapters được chèn sau các lớp attention và FCN của Transformer như được mô tả trong Phần 5 (Houlsby et al., 2019)
• Pfeiffer – Adapters được chèn chỉ sau các lớp FCN (Pfeiffer et al., 2023)
• Parallel Adapter – adapter song song có tỷ lệ như được mô tả trong (He et al., 2022a)
• IA3 – (IA)³ học các vector tái tỷ lệ cho keys, values, và kích hoạt FFN ẩn (Phần 7.2) ()
• Prefix Tuning – học một prefix được thêm vào keys và values và sử dụng tái tham số hóa FCN cho các tham số này (Phần 6.2) (Li and Liang, 2021)
• Prompt Tuning – học một prefix được thêm trực tiếp vào keys (Phần 6.1) (Lester et al., 2021)
• LN tuning – chỉ tinh chỉnh các tham số layer-norm (AkbarTajari et al., 2022)
• LoRA (q and v) – LoRA được áp dụng chỉ cho các mạng query và value (Phần 9.2) (Hu et al., 2022)
• LoRA (all linear) – LoRA được áp dụng cho tất cả các lớp tuyến tính trong mô hình (Hu et al., 2022)
• KronA – tái tham số hóa dựa trên tích Kronecker giống LoRA của các ma trận trọng số (Phần 9.3) (Edalati et al., 2022)
• MAM – Mix-and-Match Adapters (Phần 10.2) ()
• Compacter – tái tham số hóa dựa trên tích Kronecker của các lớp Adapter như được mô tả trong Phần 10.4 (Karimi Mahabadi et al., 2021)
• Compacter++ – các lớp Compacter chỉ được áp dụng sau FCN trong Transformer, tương tự như ý tưởng của Pfeiffer vs Houlsby Adapters ()
• UniPELT – một phương pháp lai kết hợp LoRA, Prefix Tuning, và Adapters thông qua gating (Phần 10.3) (Mao et al., 2021)
Metrics Trong đánh giá của chúng tôi, chúng tôi tập trung vào việc đánh giá hiệu quả phương pháp PEFT về tiêu thụ bộ nhớ, tốc độ huấn luyện, và tốc độ suy luận, và sau đó so sánh các mô hình trên metrics downstream.
Để định lượng hiệu quả bộ nhớ, chúng tôi theo dõi tiêu thụ RAM tối đa trong quá trình huấn luyện sử dụng torch.cuda.max_memory_allocated(). Tốc độ huấn luyện được định lượng bằng số lượng token đầu vào được xử lý mỗi giây trong quá trình huấn luyện và cho suy luận – trong quá trình đánh giá. Chúng tôi không hợp nhất rõ ràng các phương pháp dựa trên tái tham số hóa vào trọng số mô hình trong quá trình đánh giá để trình bày kết quả cho trường hợp sử dụng điển hình khi các phương pháp như LoRA được sử dụng theo kiểu adapter. Khi được hợp nhất, không nên có sự khác biệt giữa các phương pháp dựa trên tái tham số hóa và huấn luyện thông thường về tốc độ suy luận. Chúng tôi sử dụng độ chính xác cho các bộ dữ liệu SuperGLUE và ROUGE-L cho tóm tắt.
Chi tiết triển khai và siêu tham số Tất cả các mô hình được tinh chỉnh theo kiểu text-to-text theo Raffel et al. (2019). Chúng tôi sử dụng thư viện Adapters và PEFT (Poth et al., 2023; Mangrulkar et al., 2022) cho hầu hết các phương pháp và triển khai một số phương pháp trong repository⁸ của chúng tôi từ đầu. Khi sử dụng các triển khai hiện có, chúng tôi sử dụng siêu tham số kiến trúc mặc định cho phương pháp từ thư viện tương ứng, thường gần với các siêu tham số được báo cáo trong bài báo gốc của phương pháp.
Đối với tất cả các bộ dữ liệu NLU, chúng tôi thực hiện quét tốc độ học qua các giá trị {1e-3, 1e-4, và 1e-5} và chọn tốt nhất, sau đó chúng tôi huấn luyện trên hai seed ngẫu nhiên nữa để ước tính độ lệch chuẩn. Các thí nghiệm sơ bộ của chúng tôi chỉ ra tác động không đáng kể của weight decay trong thiết lập của chúng tôi (<0.01). Do ràng buộc tính toán, các thí nghiệm CNN/Dailymail chỉ sử dụng một seed và tốc độ học 1e-3, mà chúng tôi thấy hoạt động nhất quán tốt qua các phương pháp PEFT. Chúng tôi ước tính độ lệch chuẩn của các lần chạy CNN/Dailymail thông qua một số seed ngẫu nhiên trên các phương pháp PEFT được chọn ngẫu nhiên và thấy nó thấp hơn 0.5 điểm ROUGE-L, mà chúng tôi sử dụng cho ước tính độ lệch chuẩn cho phần còn lại của các phương pháp.
Chúng tôi sử dụng độ dài chuỗi đầu vào tối đa 512 tokens trong tất cả các thí nghiệm của chúng tôi. Trong các thí nghiệm NLU, độ dài chuỗi đầu ra tối đa là 8, và cho tóm tắt, nó là 128 tokens.
Mỗi mô hình NLU trải qua huấn luyện trong 3 epochs hoặc tối thiểu 100 bước cập nhật. Đối với CNN/Dailymail, thời gian huấn luyện được đặt thành một epoch (9 nghìn bước cập nhật). Chúng tôi sử dụng kích thước batch 32 trong tất cả các thí nghiệm của chúng tôi, sử dụng tích lũy gradient để đạt được kích thước batch này khi cần. Mặc dù tất cả các tác vụ SuperGLUE hội tụ vào cuối huấn luyện trong hầu hết các thí nghiệm của chúng tôi, chúng tôi quan sát rằng CNN/Dailymail tiếp tục cải thiện trong suốt quá trình huấn luyện và không đạt plat ô. Thiết lập của chúng tôi do đó ưu tiên các phương pháp thể hiện học nhanh hơn, điều này đặc biệt liên quan cho các kịch bản tài nguyên thấp thường gặp trong các ứng dụng PEFT.
Tổng cộng, chúng tôi huấn luyện ba mô hình kích thước 0.7-11B tham số với 14 phương pháp PEFT, năm lần chạy cho mỗi ba bộ dữ liệu NLU, và một cho bộ dữ liệu tóm tắt. Cùng với baseline tinh chỉnh đầy đủ, điều này mang tổng số thí nghiệm lên khoảng 700. Chúng tôi báo cáo kết quả thô (không tổng hợp) trong Phụ lục B.
Thiết lập phần cứng Chúng tôi ước tính thông lượng sử dụng một GPU A100 40GB duy nhất cho hầu hết các thí nghiệm, với một vài ngoại lệ do vấn đề hết bộ nhớ. UniPELT, MAM, và Prefix Tuning cho T5-11B được huấn luyện với một GPU A100 80GB duy nhất, nên cho số thông lượng so sánh với A100 40GB. Các thí nghiệm tinh chỉnh đầy đủ T5-11B được thực hiện với hai GPU A100 80GB sử dụng huấn luyện phân tán song song mô hình. Ước tính RAM cho huấn luyện mô hình này là tổng tiêu thụ bộ nhớ của cả hai GPU, nên cho ước tính so sánh với phần còn lại của các thí nghiệm, vì trạng thái optimizer không được chia sẻ giữa các GPU trong huấn luyện song song mô hình.
Bảng 3 chỉ định số lượng ví dụ được xử lý đồng thời (kích thước microbatch) trong các thí nghiệm của chúng tôi. Kích thước microbatch được giữ nhất quán qua các phương pháp điều chỉnh khác nhau để cho phép so sánh công bằng về hiệu quả bộ nhớ. Điều này cũng cho phép chúng tôi cô lập các cải thiện thông lượng từ chính phương pháp PEFT, thay vì tăng kích thước batch. Các phương pháp với tiêu thụ bộ nhớ thấp hơn có thể hưởng lợi thêm từ kích thước batch tăng.
11.2 Kết quả so sánh: Hiệu suất Downstream
Bảng 4 cho thấy metrics downstream được trung bình hóa qua các bộ dữ liệu. Điểm số được trung bình hóa, và độ lệch chuẩn được tổng hợp sử dụng trung bình Euclidean của phương sai mỗi bộ dữ liệu. Bảng này so sánh hiệu suất downstream của các phương pháp PEFT qua các quy mô mô hình. Kết quả không tổng hợp cho tất cả các thí nghiệm của chúng tôi có sẵn trong Phụ lục B.
Chúng tôi lưu ý một vài quan sát chính:
Houlsby Adapters và LoRA liên tục hoạt động tốt nhất Houlsby Adapters và LoRA là những phương pháp duy nhất liên tục đạt được hiệu suất tinh chỉnh đầy đủ với ít hoặc không nỗ lực trong điều chỉnh siêu tham số.
Các phương pháp lai đặc biệt nhạy cảm với siêu tham số MAM Adapters và UniPELT liên tục khó huấn luyện. Mặc dù kết quả trong Bảng 4 chỉ bao gồm mô hình tốt nhất từ quét của chúng tôi qua ba tốc độ học, các thí nghiệm bổ sung để cải thiện MAM và UniPELT chỉ cải thiện hiệu suất của chúng một cách nhỏ. Chúng tôi quy điều này cho hiệu suất kém chung của Prompt Tuning khi được huấn luyện trong kịch bản hạn chế tính toán.
Prefix Tuning và Prompt Tuning khác biệt đáng kể về hiệu suất Prefix Tuning (Li and Liang, 2021) và Prompt Tuning (Lester et al., 2021) là hai phương pháp PEFT khác nhau dễ nhầm lẫn về mặt đặt tên và khái niệm. Cả hai phương pháp đều sử dụng ý tưởng tối ưu hóa prompt liên tục, nhưng Prefix Tuning tái tham số hóa prefix có thể huấn luyện thông qua một mạng kết nối đầy đủ (Phần 6.2). Ngược lại, Prompt Tuning tối ưu hóa trực tiếp prefix, mặc dù với chi phí hội tụ chậm hơn và thường độ dài prefix lớn hơn nhiều. Chúng tôi quan sát sự khác biệt đáng kể giữa các phương pháp này trong các thí nghiệm của chúng tôi. Cả hai đều chịu đựng hội tụ chậm, làm tổn hại đáng kể hiệu suất trong thiết lập của chúng tôi. Tuy nhiên, Prompt Tuning không bao giờ vượt trội baseline dự đoán hằng số.⁹ Ngoài ra, Prompt Tuning cực kỳ nhạy cảm với seed ngẫu nhiên (đặc biệt cho các mô hình T5-large và 3B), như được quan sát bởi độ lệch chuẩn cao từ trung bình.
Nhiều phương pháp kém hiệu suất so với giá trị được báo cáo của chúng Nhiều phương pháp đã tuyên bố vượt trội Adapters hoặc LoRA (hầu như tất cả các phương pháp khác) không hoạt động tốt trong thiết lập của chúng tôi. Điều này bao gồm hầu hết các phương pháp ngoại trừ Parallel Adapter, Compacter, và KronA, hoạt động ngang bằng với các phương pháp tốt nhất trong một số trường hợp, đặc biệt cho các mô hình 11B.
Pfeiffer Adapters hoạt động kém đáng kể so với Houlsby Pfeiffer et al. (2020a) quan sát rằng việc chèn adapters chỉ sau FCN trong Transformer đạt được hiệu suất tương tự như chèn adapters sau cả lớp FCN và Attention (MHA). Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi thấy một sự khác biệt đáng kể và nhất quán lên đến 15 điểm tăng với quy mô mô hình. Điều này làm nổi bật tầm quan trọng của việc đánh giá các phương pháp cho cả mô hình nhỏ và lớn.
Layer Norm Tuning bất ngờ cạnh tranh Các tham số Layer Norm hiếm khi được sử dụng cho tinh chỉnh hiệu quả tham số; chúng tôi chỉ tìm thấy một vài đề cập đến phương pháp (AkbarTajari et al., 2022; Liu et al., 2022). Tuy nhiên, nó có thể được triển khai trong một dòng mã và cho thấy hiệu suất cạnh tranh với tinh chỉnh đầy đủ cho T5-Large và T5-11B. Chúng tôi muốn nhấn mạnh kết quả này và khuyến nghị sử dụng LN tuning như một baseline cho công việc PEFT trong tương lai.
11.3 Kết quả so sánh: Hiệu quả
Bảng 5 trình bày so sánh chi tiết về hiệu quả và hiệu suất cho 14 phương pháp PEFT được so sánh trong nghiên cứu của chúng tôi. Chúng tôi cho thấy số lượng thực tế các tham số có thể huấn luyện (trái ngược với tham số đã thay đổi), tiêu thụ bộ nhớ GPU tối đa trong quá trình huấn luyện, và thông lượng trong ktok/s (nghìn tokens mỗi giây) cả trong quá trình huấn luyện và suy luận.
Tất cả các phương pháp PEFT giảm tiêu thụ bộ nhớ Như mong đợi, tất cả các phương pháp từ nghiên cứu của chúng tôi giảm đáng kể tiêu thụ bộ nhớ. Cải thiện nhỏ nhất chúng tôi thấy là 4GB trong kết hợp UniPELT và T5-Large, khá đáng kể vì nó là 10% RAM GPU. Cải thiện lớn nhất là 71.5GB trong kết hợp Compacter++ và T5-11B. Điều này cho phép tinh chỉnh T5-11B trên một GPU 40GB duy nhất thay vì hai GPU 80GB và cải thiện đáng kể tốc độ huấn luyện bằng hệ số hơn hai.
Các mô hình nhỏ hơn (<1B) có thể huấn luyện chậm hơn với PEFT Bất kỳ phương pháp PEFT (Parameter-Efficient Fine-Tuning) nào thêm tham số vào mạng đều liên quan đến overhead truyền xuôi (và có thể truyền ngược) bổ sung. Đối với các mô hình đủ lớn hoặc khi chỉ một vài tham số được thêm, overhead này có thể không đáng kể. Tuy nhiên, nếu phương pháp thêm quá nhiều tham số, nó có thể dẫn đến huấn luyện chậm hơn so với tinh chỉnh thông thường. Chúng tôi quan sát điều này trong các mô hình T5-Large, nhỏ chỉ so với các mô hình quy mô tỷ, vì chúng có 738M tham số¹⁰. Ví dụ, áp dụng LoRA cho tất cả các tham số T5-Large dẫn đến giảm tốc 20% huấn luyện. Các giảm tốc tương tự được ghi nhận cho MAM adapters, Compacter, và UniPELT, với 20%, 5%, và 40% huấn luyện chậm hơn, tương ứng, so với tinh chỉnh đầy đủ. Mặc dù những giảm tốc này, tất cả đều cung cấp cải thiện bộ nhớ.
PEFT ảnh hưởng đáng kể đến tốc độ suy luận Trong tất cả các phương pháp PEFT thêm tham số có thể huấn luyện vào mạng, chúng tôi quan sát giảm tốc đáng kể trong tốc độ suy luận. Giảm tốc dao động từ 33-55% cho T5-Large, 20-60% cho T5-3B, và 20-55% cho T5-11B (điểm tuyệt đối). Trong tập hợp các phương pháp bổ sung, chúng tôi quan sát rằng Pfeiffer adapters và (IA)³ cung cấp tốc độ suy luận tốt nhất. Điều quan trọng cần lưu ý là trong ước tính thông lượng của chúng tôi cho các phương pháp dựa trên tái tham số hóa, chúng tôi không hợp nhất các tham số phương pháp vào mạng. Nếu được hợp nhất, chúng sẽ có cùng tốc độ suy luận như tinh chỉnh thông thường, vì không có tham số bổ sung nào hiện diện. Tuy nhiên, các phương pháp như LoRA ngày càng được sử dụng trong các cách tiếp cận modular, như được tham chiếu trong (Huang et al., 2023), mà không hợp nhất các tham số LoRA. Kết quả từ Bảng 5 liên quan cho các kịch bản này.
Các tái tham số hóa dựa trên Kronecker không cải thiện hiệu quả bộ nhớ, nhưng cải thiện tốc độ Qua các quy mô mô hình khác nhau, chúng tôi quan sát rằng các phương pháp cực kỳ hiệu quả tham số như Compacter và KronA, sử dụng tích Kronecker để tăng cường hiệu quả tham số, không giảm đáng kể việc sử dụng bộ nhớ. Mặc dù huấn luyện với ít hơn hai bậc độ lớn tham số so với LoRA, tiêu thụ bộ nhớ của Compacter và KronA gần như giống hệt với LoRA. Ví dụ, LoRA tối ưu hóa 20 triệu tham số cho T5-11B, trong khi KronA và Compacter mỗi cái tối ưu hóa ít hơn 0.5 triệu. Tuy nhiên, tất cả các phương pháp tiêu thụ khoảng 28.6GB bộ nhớ GPU. Kết quả này trở nên trực quan khi nhìn lại: ngoài một điểm nhất định, bộ nhớ được sử dụng cho trạng thái optimizer và gradients trở nên không đáng kể, bị che khuất bởi các yếu tố khác như trọng số mô hình và trạng thái ẩn. Tuy nhiên, chúng tôi quan sát cải thiện tốc độ huấn luyện và suy luận đáng kể với KronA so với LoRA. Điều này có thể xảy ra do triển khai hiệu quả tích Kronecker-vector trong KronA (Phần 9.3).
Kết luận, so sánh thực nghiệm của chúng tôi cho thấy một số kết quả mong đợi, như cải thiện đáng kể trong tiêu thụ bộ nhớ và tốc độ. Tuy nhiên, chúng tôi cũng quan sát một số kết quả đáng ngạc nhiên. Đáng chú ý, chúng tôi quan sát rằng các phương pháp như Layer Norm Tuning, thường bị bỏ qua, có thể bất ngờ hiệu quả. Ngoài ra, tác động của các phương pháp PEFT khác nhau lên tốc độ suy luận, đặc biệt trong các mô hình lớn hơn, làm nổi bật các đánh đổi phức tạp giữa hiệu quả và hiệu suất. Những hiểu biết này nhấn mạnh nhu cầu đánh giá toàn diện các phương pháp PEFT, tính đến không chỉ bộ nhớ và tốc độ mà còn khả năng mở rộng của chúng qua các kích thước mô hình khác nhau.
Dựa trên các thí nghiệm của chúng tôi, chúng tôi lưu ý một vài điểm chính để lấy đi cho các nhà thực hành.
Những phát hiện chính:
• Houlsby Adapters (Houlsby et al., 2019) và LoRA (Hu et al., 2022) hoạt động ngang bằng hoặc tốt hơn full-tuning, với ít hoặc không nỗ lực trong điều chỉnh siêu tham số.
• Layer Norm (AkbarTajari et al., 2022) tuning cung cấp một phương pháp cực kỳ cạnh tranh và hiệu quả dễ triển khai.
12 Thách thức và hướng dẫn
Các bài báo khảo sát có xu hướng thảo luận về các vấn đề báo cáo, và bài này không ngoại lệ. Chúng tôi xác định một số thách thức và bất nhất quán khiến việc đánh giá các phương pháp PEFT và rút ra so sánh trực tiếp giữa các phương pháp PEFT khác nhau trở nên khó khăn, điều đáng thảo luận.
Báo cáo số lượng tham số Một trong những thách thức chính xuất phát từ sự khác biệt trong cách các nhà nghiên cứu báo cáo số lượng tham số. Những bất nhất quán này phát sinh từ độ phức tạp vốn có của vấn đề. Số lượng tham số có thể được phân loại thành ba loại: số lượng tham số có thể huấn luyện, số lượng tham số đã thay đổi giữa mô hình gốc và tinh chỉnh, và rank của sự khác biệt giữa mô hình gốc và tinh chỉnh. Những số lượng tham số này không tương đương. Ví dụ, IntrinsicSAID (Phần 9.1) học một biến đổi low-rank (~100-1000) của các tham số mô hình. Tuy nhiên, nó thay đổi tất cả (100%) các tham số của mô hình. DiffPruning (Phần 8.2) học một cập nhật của 0.5% các tham số, nhưng nó thực sự huấn luyện 200% các tham số: tinh chỉnh mô hình và học mask nhị phân. Đối với các phương pháp dựa trên tái tham số hóa (Các phần 9.2, 9.3, 10.4), yêu cầu bộ nhớ có thể khác nhau tùy thuộc vào các lựa chọn thiết kế triển khai. Trong ba loại, số lượng tham số có thể huấn luyện là dự báo đáng tin cậy nhất về hiệu quả bộ nhớ. Tuy nhiên, nó vẫn không hoàn hảo: Ladder-side Tuning huấn luyện nhiều tham số hơn LoRA hoặc BitFit, nhưng nó sử dụng ít RAM hơn bằng cách tránh backpropagation đến mạng chính.
Hướng dẫn - 1: Báo cáo rõ ràng về số lượng tham số và loại tức là tham số có thể huấn luyện hoặc tham số đã thay đổi hoặc rank của các thay đổi đối với mô hình đang được điều chỉnh.
Báo cáo hiệu quả Đánh giá hiệu quả của các phương pháp PEFT chỉ dựa trên số lượng tham số là thách thức do mối quan hệ phi tuyến giữa số lượng tham số và hiệu quả. Hiệu quả trong thời gian huấn luyện được đánh giá tốt hơn thông qua tiêu thụ bộ nhớ và tốc độ huấn luyện. Hầu hết các danh mục PEFT, ngoại trừ các phương pháp Sparse-selective, cải thiện đáng kể việc sử dụng RAM. Tuy nhiên, phương pháp Intrinsic SAID (Aghajanyan et al., 2020), là dựa trên Reparametrization, có thể dẫn đến việc sử dụng bộ nhớ cao hơn tinh chỉnh đầy đủ do yêu cầu của biến đổi Fastfood. Các thí nghiệm của chúng tôi cho thấy tính modular trong các phương pháp PEFT lai (ví dụ, MAM adapters (), UniPELT (Mao et al., 2021)) có cái giá của tiêu thụ bộ nhớ cao hơn đáng chú ý. Điều này nhấn mạnh nhu cầu cho các nghiên cứu báo cáo tiêu thụ bộ nhớ để giúp các nhà thực hành đưa ra quyết định thông tin. Chúng tôi cũng nhận thấy biến thiên đáng kể trong tốc độ huấn luyện ngay cả với việc sử dụng RAM tương tự, cho thấy tiêu thụ RAM nên được xem xét cùng với tốc độ huấn luyện. Sau huấn luyện, không gian lưu trữ cần thiết cho các tham số đã thay đổi là quan trọng để đánh giá các phương pháp PEFT. Khác với tinh chỉnh đầy đủ, thay đổi tất cả các tham số mô hình, PEFT chỉ đòi hỏi lưu một tập con, cải thiện đáng kể hiệu quả lưu trữ. Tuy nhiên, các phương pháp như IPT đòi hỏi lưu các tập hợp tham số khác nhau ở các giai đoạn huấn luyện khác nhau, làm cho việc báo cáo rõ ràng về yêu cầu không gian trở nên cần thiết. Độ trễ suy luận là một yếu tố quan trọng khác trong thực tế. Các phương pháp bổ sung thường giới thiệu overhead vì chúng đòi hỏi tính toán trên cả mạng gốc và các tham số được thêm, trong khi các phương pháp có chọn lọc thì không, vì chúng hoạt động trên trọng số mô hình hiện có. Hơn nữa, các phương pháp bổ sung và dựa trên tái tham số hóa (ví dụ, LoRA, KronA) cung cấp lợi thế trong suy luận đa tác vụ bằng cách giảm việc sử dụng bộ nhớ từ O(NM) thành O(M+NA), trong đó A là số lượng trọng số được thêm mỗi tác vụ. Một số phương pháp bổ sung, như LST, cũng có thể tăng cường tốc độ suy luận bằng cách sử dụng mạng gốc chỉ như một trích xuất đặc trưng. Để biết thêm chi tiết về huấn luyện và suy luận đa tác vụ, chúng tôi giới thiệu độc giả đến Modular Deep Learning (Pfeiffer et al., 2023).
Hướng dẫn - 2: Báo cáo rõ ràng về tiêu thụ bộ nhớ (RAM) trong quá trình huấn luyện, thông lượng token trong quá trình huấn luyện và suy luận, và yêu cầu lưu trữ là cần thiết để đánh giá hiệu quả của phương pháp PEFT. Hơn nữa, nếu các phương pháp được đề xuất bao gồm nhiều giai đoạn, tất cả các metrics nên được báo cáo cho mỗi giai đoạn.
Kích thước mô hình Một thách thức khác phát sinh từ sự thay đổi trong kích thước mô hình được sử dụng trong đánh giá các phương pháp PEFT. Điều quan trọng là đánh giá các phương pháp tinh chỉnh các kích thước mô hình khác nhau, đặc biệt >1B và <20B tham số. Với sự gia tăng kích thước mô hình backbone, nhu cầu và tính hữu ích của các phương pháp PEFT tăng nhanh chóng. Một số nghiên cứu (Aghajanyan et al., 2020; Hu et al., 2022) đã chứng minh rằng các mô hình lớn hơn đòi hỏi ít tham số hơn cần được cập nhật trong quá trình tinh chỉnh, cả về phần trăm và khi mô hình đủ lớn, đôi khi thậm chí về số lượng tuyệt đối (Li and Liang, 2021). Chúng tôi muốn nhấn mạnh điều này đặc biệt, xem xét rằng ngay cả các bài báo gần đây thường chỉ tập trung vào BERT. Hơn nữa, trong các thí nghiệm của chúng tôi, điều chỉnh LayerNorm (AkbarTajari et al., 2022) là phương pháp duy nhất liên tục hiệu quả ở các quy mô khác nhau, trong khi duy trì hiệu suất cạnh tranh hiện tại trên các tác vụ downstream. Đối với tất cả các phương pháp khác, hiệu quả và hiệu suất thay đổi đáng kể ở các kích thước mô hình khác nhau. Do đó, kích thước mô hình phải được xem xét khi báo cáo các phương pháp PEFT.
Hướng dẫn - 3: Metrics hiệu quả và hiệu suất downstream nên được báo cáo qua nhiều quy mô mô hình. Tác giả cũng nên xem xét báo cáo kết quả tinh chỉnh cho các kích thước mô hình được sử dụng phổ biến nhất bởi cộng đồng nghiên cứu tại thời điểm thí nghiệm.
Triển khai phương pháp Một vấn đề khác gặp phải là trạng thái của các triển khai đã xuất bản. Nhiều codebase chỉ đơn giản là bản sao của thư viện Transformers (Wolf et al., 2020) hoặc các repository khác chỉ với những sửa đổi nhỏ. Những bản sao này thường không sử dụng git forks, làm cho việc xác định sự khác biệt trở nên khó khăn trừ khi chúng được nêu bật trong file README. Nhưng ngay cả khi sự khác biệt dễ tìm, mã thường không đọc được hoặc tái sử dụng được. Người dùng thường được yêu cầu cài đặt một phiên bản sửa đổi của thư viện Transformers, xung đột với phiên bản mới nhất và thiếu tài liệu hoặc ví dụ về cách tái sử dụng phương pháp bên ngoài codebase hiện có. Mặc dù có những thách thức này, có một số phương pháp với các triển khai tái sử dụng đáng nêu bật, như LoRA¹¹ và Compacter¹². Những triển khai này nổi bật vì tính thân thiện với người dùng và khả năng thích ứng, cung cấp nền tảng vững chắc cho nghiên cứu và phát triển thêm.
Hướng dẫn - 4: Nghiên cứu trình bày một phương pháp PEFT nên được kèm theo một triển khai dễ sử dụng của phương pháp.
So sánh Trực quan, phương pháp PEFT được trình bày nên được so sánh với các cách tiếp cận phổ biến (ví dụ, LoRA, BitFit, Adapters) và các phương pháp chia sẻ sự tương tự khái niệm và kiến trúc với phương pháp được trình bày. Tuy nhiên, sự vắng mặt của các benchmark và metrics tiêu chuẩn làm phức tạp việc so sánh các phương pháp PEFT. Các phương pháp mới thường được đánh giá trên các kết hợp mô hình/bộ dữ liệu khác nhau, làm cho việc rút ra kết luận có ý nghĩa trở nên thách thức. Chúng tôi muốn nêu bật các bài báo báo cáo nhiều metrics trên các bộ dữ liệu tiêu chuẩn, đơn giản hóa việc so sánh với các phương pháp khác. Ví dụ, KronA (Edalati et al., 2022) đánh giá T5-base trên benchmark GLUE và báo cáo độ chính xác, thời gian huấn luyện, và thời gian suy luận trong khi duy trì cùng số lượng tham số có thể huấn luyện. UniPELT (Mao et al., 2021) đánh giá BERT trên benchmark GLUE và báo cáo độ chính xác, thời gian huấn luyện, và độ trễ suy luận, mặc dù nó sử dụng số lượng tham số khác nhau cho các phương pháp khác nhau. LST (Sung et al., 2022) đánh giá các kích thước T5 khác nhau trên benchmark GLUE, báo cáo các metrics như độ chính xác, thời gian huấn luyện, số lượng tham số được cập nhật, và việc sử dụng bộ nhớ. MAM (He et al., 2022a) áp dụng nhiều mô hình cho benchmark XSUM và báo cáo độ chính xác qua một loạt tham số có thể huấn luyện, mặc dù so sánh bộ nhớ không được cung cấp. Tuy nhiên, ngay cả những bài báo này thiếu khả năng so sánh đầy đủ do sự khác biệt trong thiết lập đánh giá của chúng, như số lượng tham số thay đổi hoặc sự vắng mặt của các metrics nhất định như so sánh bộ nhớ. Những bất nhất quán này làm nổi bật nhu cầu cho một benchmark chuẩn hóa và metrics thống nhất để tạo điều kiện cho việc so sánh và đánh giá chính xác hơn các phương pháp PEFT. Dựa trên khảo sát và thí nghiệm của chúng tôi, chúng tôi xác định các chất lượng chính của mỗi danh mục và tóm tắt chúng trong phần này và Bảng 6.
Kêu gọi nỗ lực cộng đồng trong việc phát triển các benchmark chuẩn hóa và cuộc thi để đánh giá các phương pháp PEFT.
13 Thảo luận
Sự dễ tiếp cận ngày càng tăng của các mô hình ngôn ngữ lớn (Zhang et al., 2022; Zeng et al., 2022; Khrushchev et al., 2022; Touvron et al., 2023) và việc dân chủ hóa suy luận của chúng thông qua lượng tử hóa bit thấp (Dettmers et al., 2022; Dettmers and Zettlemoyer, 2022) đã cho phép cộng đồng nghiên cứu nghiên cứu, thí nghiệm và giải quyết các tác vụ mới với ngân sách tính toán tương đối khiêm tốn. Tinh chỉnh hiệu quả tham số là bước tiếp theo cho phép chúng ta không chỉ suy luận, mà còn sửa đổi các mô hình này.
Một số phương pháp, bao gồm Adapters, Prompt Tuning, LoRA, và (IA)³, đã cho thấy tính thực tế của chúng ở quy mô (Bảng 2). Tuy nhiên, trong thực tế, việc phù hợp với hiệu suất của tinh chỉnh đầy đủ vẫn là một thách thức. Một trong những lý do là độ nhạy cảm cao với siêu tham số, với các siêu tham số tối ưu thường khác biệt đáng kể so với những siêu tham số được sử dụng trong tinh chỉnh đầy đủ do số lượng tham số có thể huấn luyện khác nhau. Ví dụ, tốc độ học tối ưu cho tinh chỉnh hiệu quả tham số thường cao hơn nhiều so với tinh chỉnh đầy đủ. Cộng đồng nghiên cứu nên thúc đẩy các điều tra sâu về tác động của siêu tham số lên các phương pháp này và tìm các mặc định hợp lý, vì tinh chỉnh hiệu quả tham số của các mô hình lớn có thể đáng chú ý tốn kém ở quy mô 20-100B. Ngoài ra, nỗ lực nên được hướng đến việc phát triển các phương pháp giảm thiểu độ nhạy cảm siêu tham số, như tiền huấn luyện các tham số mới (Vu et al., 2021; Su et al., 2021).
Kiểm tra phân loại các phương pháp và tiến bộ đã đạt được cho đến nay, rõ ràng rằng tái tham số hóa low-rank đã thành công đáng chú ý trong việc tăng cường hiệu quả tham số. Các tái tham số hóa kiểu LoRA (Phần 9.2) và tích Kronecker (Các phần 10.4 và 9.3) đều giảm số lượng tham số có thể huấn luyện trong khi đòi hỏi tính toán thêm tối thiểu. Một hướng có thể cho việc tìm kiếm các mô hình PEFT mới là khám phá các kỹ thuật tái tham số hóa khác nhau với tỷ lệ số lượng tham số có thể huấn luyện vs. rank thuận lợi.
Một hướng có thể khác để cải thiện là sử dụng những gì chúng ta biết về cách các mô hình transformer xử lý văn bản (Rogers et al., 2020). Hầu hết các phương pháp PEFT hoạt động đồng đều cho mô hình, trong khi chúng ta biết rằng các mô hình xử lý đầu vào khác nhau ở các lớp khác nhau. Sử dụng kiến thức này hoặc xây dựng các hệ thống có số lượng tham số thích ứng mỗi lớp có thể cải thiện thêm hiệu quả tham số và độ chính xác.
Trong nhiều khía cạnh, tinh chỉnh các mô hình ngôn ngữ lớn đối mặt với những thách thức tương tự như những thách thức gặp phải trong học máy edge – chúng ta liên tục đối mặt với ràng buộc về bộ nhớ, tính toán, và thậm chí tiêu thụ năng lượng. Các kỹ thuật như lượng tử hóa và cắt tỉa (Gupta et al., 2015; LeCun et al., 1989) được sử dụng rộng rãi trong học máy edge hiện nay có lợi cho các mô hình ngôn ngữ lớn. Khi chúng ta tiến về phía trước, không chỉ hợp lý mà còn có khả năng rằng nhiều ý tưởng hơn có thể được trao đổi giữa hai lĩnh vực này. Hợp tác liên ngành có thể tạo điều kiện cho việc trao đổi ý tưởng, tăng tốc đổi mới và tiến bộ trong tinh chỉnh hiệu quả tham số.
Lời cảm ơn và tiết lộ nguồn tài trợ
Công trình này được tài trợ một phần bởi giải thưởng nghiên cứu Amazon Alexa AI cho Anna Rumshisky. Chúng tôi muốn cảm ơn Vladimir Kluenkov và Victoria Maltseva vì sự giúp đỡ của họ với Hình 2.
