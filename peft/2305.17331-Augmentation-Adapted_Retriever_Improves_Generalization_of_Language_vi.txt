# Augmentation-Adapted Retriever Cải thiện Khả năng Tổng quát hóa của Mô hình Ngôn ngữ như một Plugin Chung

## Tóm tắt

Tăng cường truy xuất có thể hỗ trợ các mô hình ngôn ngữ (LM) trong các tác vụ đòi hỏi kiến thức cao bằng cách cung cấp cho chúng thông tin bên ngoài. Các nghiên cứu trước đây về tăng cường truy xuất thường tinh chỉnh chung bộ truy xuất và LM, khiến chúng được kết nối chặt chẽ. Trong bài báo này, chúng tôi khám phá sơ đồ plugin truy xuất chung: bộ truy xuất nhằm hỗ trợ các LM đích có thể không được biết trước hoặc không thể được tinh chỉnh cùng nhau. Để truy xuất các tài liệu hữu ích cho các LM đích chưa thấy, chúng tôi đề xuất augmentation-adapted retriever (AAR), học các ưu tiên của LM thu được từ một LM nguồn đã biết. Các thí nghiệm trên bộ dữ liệu MMLU và PopQA chứng minh rằng AAR của chúng tôi được huấn luyện với một LM nguồn nhỏ có thể cải thiện đáng kể khả năng tổng quát hóa zero-shot của các LM đích lớn hơn từ 250M Flan-T5 đến 175B InstructGPT. Phân tích sâu hơn cho thấy các ưu tiên của các LM khác nhau trùng lặp, cho phép AAR được huấn luyện với một LM nguồn duy nhất phục vụ như một plugin chung cho nhiều LM đích khác nhau.

## 1. Giới thiệu

Các mô hình ngôn ngữ lớn (LM) sở hữu hàng tỷ tham số có thể nắm bắt một lượng đáng kể kiến thức của con người, dẫn đến những cải thiện nhất quán trên nhiều tác vụ downstream khác nhau. Tuy nhiên, nhược điểm không thể phủ nhận của các LM lớn nằm ở chi phí tính toán cao, ảnh hưởng tiêu cực đến hiệu quả của chúng. Hơn nữa, kiến thức được ghi nhớ từ quá trình pre-training và quy trình suy luận ngầm của các LM đôi khi có thể không chính xác và khó theo dõi, cản trở ứng dụng của chúng trên các tác vụ đòi hỏi kiến thức cao.

Thay vì tận dụng kiến thức và khả năng suy luận được nhúng trong các tham số của LM, tăng cường truy xuất tăng cường LM với một bộ truy xuất có thể truy xuất kiến thức từ một kho tài liệu bên ngoài. Mặt khác, các phương pháp tăng cường truy xuất trước đây đòi hỏi phải tinh chỉnh LM backbone để điều chỉnh với bộ truy xuất và giải quyết các tác vụ downstream cụ thể. Loại tinh chỉnh này có thể tốn kém khi ngày càng nhiều nhu cầu độc đáo xuất hiện. Quan trọng hơn, nhiều LM hàng đầu chỉ có thể được truy cập thông qua các API black-box. Các API này cho phép người dùng gửi truy vấn và nhận phản hồi nhưng thường không hỗ trợ tinh chỉnh.

Trong bài báo này, chúng tôi giới thiệu Augmentation-Adapted Retriever (AAR) để hỗ trợ các LM black-box với các tác vụ downstream như plugin chung. Để truy xuất các tài liệu có giá trị cho nhiều LM chưa thấy, chúng tôi đề xuất tận dụng một LM nguồn nhỏ để cung cấp các tín hiệu ưa thích của LM cho việc huấn luyện bộ truy xuất. Bộ truy xuất sau khi huấn luyện (tức là AAR) có thể được sử dụng trực tiếp để hỗ trợ một LM đích lớn bằng cách cắm vào các tài liệu được truy xuất.

Cụ thể, chúng tôi chọn một LM encoder-decoder nhỏ làm LM nguồn và sử dụng điểm attention fusion-in-decoder của nó để chú thích các tài liệu được LM ưa thích. Các tài liệu được LM ưa thích sau đó được kết hợp với các tài liệu được con người ưa thích để tạo thành tập tài liệu tích cực. Các tài liệu tiêu cực được khai thác bởi chính bộ truy xuất sử dụng kỹ thuật ANCE. Sau khi tinh chỉnh bộ truy xuất với các ưu tiên của LM, nó có thể trực tiếp hỗ trợ các LM đích chưa thấy trong việc tổng quát hóa tác vụ zero-shot.

Chúng tôi đánh giá AAR trên bộ dữ liệu hiểu ngôn ngữ đa nhiệm vụ MMLU và bộ dữ liệu trả lời câu hỏi tập trung vào thực thể PopQA. Đối với các LM đích, chúng tôi chọn dòng Flan-T5 làm backbone cho các LM encoder-decoder và InstructGPT làm backbone cho các LM decoder-only. Hình 1 cho thấy rằng được hỗ trợ bởi một AAR chung, các LM có kích thước và kiến trúc khác nhau có thể liên tục vượt trội hơn các LM độc lập; hiệu suất của các LM nhỏ hơn đôi khi có thể vượt qua các đối tác độc lập có kích thước lớn hơn đáng kể (ví dụ: Flan-T5 Large w/ AAR vượt trội hơn Flan-T5 XL độc lập 0,6%). AAR cũng thể hiện lợi thế so với các phương pháp tăng cường khác như prompting few-shot và truy xuất thích ứng.

Phân tích sâu hơn cho thấy các ưu tiên thu được từ các LM nguồn có kích thước khác nhau tương tự nhau, và các LM có năng lực gần nhau có xu hướng tạo ra các tập tài liệu ưa thích gần gũi hơn. Kết quả là, mô hình AAR của chúng tôi được huấn luyện từ một LM nguồn nhỏ có thể được coi là một plugin chung để tăng cường khả năng tổng quát hóa zero-shot của một LM đích lớn hơn đáng kể. Chúng tôi cũng phát hiện rằng các tài liệu được LM ưa thích có thể cung cấp hỗ trợ cho mô hình từ các góc độ thay thế, thay vì chỉ dựa vào thông tin đầy đủ được người dùng tìm kiếm ưa chuộng.

## 2. Công trình Liên quan

**Tăng cường Truy xuất.** Tăng cường LM với thông tin được truy xuất từ bộ nhớ bên ngoài đã cho thấy hiệu quả trên nhiều tác vụ đòi hỏi kiến thức cao khác nhau. Các nghiên cứu trước đây khám phá những cách mới để huấn luyện toàn bộ hệ thống retriever-LM theo cách end-to-end, sử dụng log-likelihood chuỗi tăng cường truy xuất, chưng cất attention fusion-in-decoder, hoặc đồ thị kiến thức. Để tách bộ truy xuất khỏi LM, Rubin và cộng sự huấn luyện một bộ truy xuất prompt độc lập cho việc học in-context, và Lin và cộng sự chỉ tinh chỉnh LM thông qua dữ liệu được truy xuất tương tự như các mẫu unsupervised few-shot.

Các nghiên cứu gần đây áp dụng tăng cường truy xuất zero-shot không tinh chỉnh LM trên InstructGPT. Nó có thể có lợi cho việc trả lời câu hỏi tập trung vào thực thể, suy luận chain-of-thought và trả lời câu hỏi multi-hop. Nghiên cứu song song sử dụng likelihood của LM để huấn luyện bộ truy xuất nhằm thỏa mãn các ưu tiên của LM black-box, và họ áp dụng GPT-3 Curie để cung cấp các tín hiệu giám sát. Trong công trình này, chúng tôi thiết kế bộ truy xuất có thể được sử dụng như một plugin chung để hỗ trợ nhiều LM chưa thấy khác nhau.

**Học Zero-shot và Suy luận.** Các LM được pre-train unsupervised quy mô lớn như GPT-3, GPT-4 và PaLM có thể thực hiện học zero-shot trên nhiều tác vụ downstream với mô tả tác vụ được cung cấp tại thời điểm suy luận. Các LM được tinh chỉnh theo hướng dẫn, được pre-train trên nhiều tác vụ có giám sát sử dụng hướng dẫn của con người, cũng thể hiện khả năng học zero-shot mạnh mẽ. Yu và cộng sự đề xuất một sơ đồ mới về suy luận zero-shot, đầu tiên nhắc các LM lớn tạo ra các tài liệu liên quan và sau đó thực hiện đọc hiểu trên nội dung được tạo. Gần đây, có xu hướng ngày càng tăng về việc sử dụng injection kiến thức plug-and-play để tăng cường hiệu suất zero-shot của LM, được thực hiện thông qua mạng ánh xạ hoặc mã hóa tài liệu. Công trình của chúng tôi cải thiện khả năng tổng quát hóa zero-shot của LM bằng cách sử dụng thông tin được truy xuất. Chúng tôi chứng minh rằng việc xác định các ưu tiên của LM để huấn luyện bộ truy xuất có thể mang lại thêm văn bản bằng chứng cho LM.

## 3. Phương pháp

Trong phần này, chúng tôi đầu tiên giới thiệu các kiến thức cơ bản về truy xuất dày đặc và LM tăng cường truy xuất (§ 3.1), sau đó đề xuất augmentation-adapted retriever của chúng tôi (§ 3.2).

### 3.1 Kiến thức Cơ bản

LM tăng cường truy xuất là một loại LM tận dụng thông tin bên ngoài để cải thiện hiệu suất của nó. Nó truy xuất các tài liệu liên quan từ một kho dữ liệu sử dụng bộ truy xuất, và sau đó sử dụng các tài liệu để tăng cường khả năng tạo ngôn ngữ của nó.

Mục tiêu của bộ truy xuất là tìm một tập tài liệu tăng cường Da từ kho dữ liệu C giúp LM xử lý một truy vấn q đã cho. Các nghiên cứu trước đây tập trung chủ yếu vào hệ thống truy xuất dày đặc tìm kiếm trong không gian vector dày đặc vì truy xuất dày đặc thường hoạt động chính xác và hiệu quả hơn truy xuất thưa thớt.

Một mô hình truy xuất dày đặc đầu tiên biểu diễn q và tài liệu d vào một không gian embedding sử dụng encoder được pre-train g, và khớp các embedding của chúng bằng hàm tích vô hướng f, hỗ trợ tìm kiếm nearest neighbor xấp xỉ nhanh (ANN). Sau đó chúng tôi định nghĩa Da chứa top-N tài liệu được truy xuất.

Đối với các backbone LM, mô hình decoder-only và encoder-decoder là hai lựa chọn chính của LM tăng cường truy xuất.

Với LM decoder-only như GPT-3, đầu vào LM có thể là phép nối đơn giản của truy vấn và tất cả các tài liệu tăng cường. Sau đó, LM sẽ tạo ra câu trả lời dựa trên các đầu vào một cách tự hồi quy.

Đối với LM encoder-decoder như T5, việc lấy phép nối đơn giản làm đầu vào encoder vẫn có thể hiệu quả. Tuy nhiên, phương pháp này có thể không mở rộng được với khối lượng lớn tài liệu do tính toán self-attention bậc hai liên quan đến số lượng tài liệu. Để tổng hợp nhiều tài liệu hiệu quả hơn, Izacard và Grave đề xuất cơ chế fusion-in-decoder (FiD), sớm trở thành xu hướng chính trong phát triển LM tăng cường truy xuất encoder-decoder. Nó đầu tiên mã hóa riêng từng phép nối của cặp (dai, q) và sau đó để decoder chú ý đến tất cả các phần.

Theo cách này, encoder tính toán self-attention trên một tài liệu tại một thời điểm để chi phí tính toán có thể tăng tuyến tính với số lượng tài liệu. Hơn nữa, cross-attention FiD được tìm thấy hiệu quả trong việc ước tính tầm quan trọng tương đối của các tài liệu tăng cường từ góc độ của LM. Do đó, chưng cất FiD mềm, giảm thiểu KL-divergence giữa likelihood truy xuất và likelihood LM, thường được sử dụng để huấn luyện bộ truy xuất và LM end-to-end.

### 3.2 Augmentation-adapted Retriever

Do các nhu cầu thực tế nổi lên và những hạn chế của API black-box, việc tinh chỉnh LM tăng cường truy xuất cho từng tác vụ downstream có thể có thể không khả thi. Do đó, chúng tôi giới thiệu Augmentation-Adapted Retriever (AAR) như một plugin chung cho các LM black-box. Như được minh họa trong Hình 2, AAR có thể học các ưu tiên của LM mà không cần tinh chỉnh chúng.

Cụ thể, chúng tôi sử dụng LM encoder-decoder làm LM nguồn (Ls) để cung cấp các tín hiệu ưa thích của LM trên tác vụ nguồn (Ts) để tinh chỉnh bộ truy xuất được pre-train. Sau đó, chúng tôi cắm bộ truy xuất được tinh chỉnh vào LM đích chưa thấy (Lt) trên một tập các tác vụ đích (Tt) không giao với Ts.

Phương pháp huấn luyện của chúng tôi bắt đầu từ tác vụ nguồn Ts, nơi chúng tôi tổng hợp điểm cross-attention FiD trung bình (FiDAtt) của LM nguồn Ls tương ứng với tài liệu dai từ token decoder đầu tiên trên tất cả các lớp, tất cả các head và tất cả các token đầu vào.

Để làm cho quá trình huấn luyện mạnh mẽ hơn, chúng tôi sử dụng điểm FiDAtt để chú thích các tài liệu tích cực được LM ưa thích theo cách rời rạc, trong đó Dh+ là tập tài liệu tích cực được con người ưa thích (tức là ground truth) trên Ts.

Sau đó, chúng tôi lấy mẫu hard negative theo ANCE và xây dựng hàm mất mát L của bộ truy xuất. Sau khi tinh chỉnh bộ truy xuất, chúng tôi trực tiếp sử dụng nó để tăng cường LM đích chưa thấy Lt trên từng tác vụ từ tập tác vụ đích Tt.

## 4. Phương pháp Thí nghiệm

Trong phần này, chúng tôi thảo luận về thiết lập thí nghiệm chính của chúng tôi. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.

### 4.1 Tác vụ Đích

Theo các nghiên cứu trước, chúng tôi chọn MMLU và PopQA làm tác vụ đích Tt.

MMLU là bộ dữ liệu hiểu ngôn ngữ đa nhiệm vụ, bao gồm 57 tác vụ trả lời câu hỏi đa lựa chọn. Các tác vụ con này có thể được phân loại chung thành bốn danh mục: nhân văn, khoa học xã hội, STEM và khác. Chúng tôi tính trung bình độ chính xác của các tác vụ con trong mỗi danh mục để có điểm cuối cùng. Chúng tôi báo cáo độ chính xác của tập đánh giá trong các thí nghiệm chính.

PopQA là bộ dữ liệu trả lời câu hỏi tập trung vào thực thể chủ yếu tập trung vào các câu hỏi long-tail. Chúng tôi báo cáo độ chính xác của tập kiểm tra trong các thí nghiệm chính.

### 4.2 Phương pháp của Chúng tôi

**Bộ truy xuất.** Chúng tôi áp dụng hai bộ truy xuất được sử dụng rộng rãi để khởi tạo AAR: ANCE được khởi tạo từ T5Base và Contriever được khởi tạo từ BERT Base. Cả hai đều đã được tinh chỉnh trên MS MARCO trước đó. Đối với kho truy xuất, chúng tôi chọn MS MARCO cho MMLU và KILT-Wikipedia cho PopQA.

**Mô hình Ngôn ngữ.** Chúng tôi áp dụng dòng Flan-T5 làm backbone cho các LM encoder-decoder và InstructGPT làm backbone cho các LM decoder-only. Các mô hình này đã được tinh chỉnh hướng dẫn đa nhiệm vụ và được sử dụng rộng rãi để đánh giá khả năng tổng quát hóa zero-shot.

**Chi tiết Triển khai.** Chúng tôi sử dụng MS MARCO làm tác vụ nguồn Ts vì nó là lựa chọn phổ biến để huấn luyện bộ truy xuất. Bộ dữ liệu này bao gồm các câu hỏi chất lượng cao đòi hỏi kiến thức thực tế để trả lời, phù hợp mạnh mẽ với các tác vụ đích Tt và không có sự trùng lặp với chúng. Xét đến hiệu quả triển khai, chúng tôi lấy Flan-T5 Base làm LM nguồn Ls và coi mô hình lớn hơn là LM đích Lt. Chúng tôi trực tiếp đặt tổng số tài liệu N = 10, số tài liệu được LM ưa thích K = 2, và độ sâu khai thác negative M = 100 trong huấn luyện augmentation-adapted. Chúng tôi chạy tất cả thí nghiệm trên một GPU A100 duy nhất (40G).

### 4.3 Baseline

**Thiết lập Zero-shot.** Chúng tôi so sánh phương pháp của chúng tôi với các baseline zero-shot tiên tiến. Các LM độc lập, bao gồm Flan-T5, InstructGPT, GAL và OPT-IML-Max, được nhắc bằng hướng dẫn ngôn ngữ tự nhiên mô tả tác vụ và câu hỏi mong muốn. Truy xuất thích ứng có chọn lọc sử dụng bộ nhớ phi tham số (tăng cường truy xuất) và bộ nhớ tham số (kiến thức thu được từ pre-training) dựa trên mức độ phổ biến của câu hỏi. Trong thí nghiệm chính, chúng tôi chọn sự kết hợp tối ưu trong bài báo của họ, bao gồm Contriever làm bộ nhớ phi tham số và GenRead làm bộ nhớ tham số.

**Thiết lập Few-shot.** Chúng tôi cũng bao gồm kết quả của các mô hình few-shot trước đây để tham khảo. Flan-T5, InstructGPT, Chinchilla và OPT-IML-Max áp dụng các minh họa few-shot, cung cấp cho LM một số lượng hạn chế các ví dụ tác vụ. Điều này cho phép các mô hình tổng quát từ những ví dụ này và tạo ra phản hồi chính xác. Atlas là LM tăng cường truy xuất tiên tiến, pre-train chung bộ truy xuất với LM sử dụng dữ liệu unsupervised và tinh chỉnh bộ truy xuất thông qua chưng cất attention trên dữ liệu few-shot.

## 5. Kết quả Đánh giá

Trong phần này, chúng tôi thảo luận về kết quả chính trên bộ dữ liệu MMLU và PopQA (§ 5.1) và tiến hành các nghiên cứu toàn diện về cách (§ 5.2, § 5.3, § 5.4) và khi nào (§ 5.5, § 5.6) AAR giúp ích.

### 5.1 Hiệu suất Tổng thể

Bảng 1 chứng minh rằng, với sự hỗ trợ của AAR chung, các LM đích có kích thước và kiến trúc khác nhau có thể vượt trội đáng kể so với các baseline độc lập trong thiết lập zero-shot. Đáng chú ý, AAR thậm chí cải thiện InstructGPT mạnh mẽ 2% trên MMLU và gần 20% trên PopQA. Chúng tôi giả định rằng bộ dữ liệu PopQA chủ yếu bao gồm các câu hỏi long-tail và do đó đòi hỏi thêm thông tin tăng cường để đạt độ chính xác cao. AAR vượt trội hơn các phương pháp tăng cường khác như prompting few-shot và truy xuất thích ứng, vì chúng có thể không cung cấp văn bản bằng chứng rộng rãi như AAR.

Đồng thời, AAR là một phương pháp tăng cường hiệu quả cao vì nó chỉ dựa vào một LM nguồn nhỏ Flan-T5 Base (250M) để cung cấp tín hiệu huấn luyện và có thể tổng quát tốt cho các LM đích có năng lực lớn hơn. Hình 3 minh họa rằng việc chỉ đặt LM nguồn làm LM đích (được biểu diễn bằng các tam giác ngược) không cải thiện đáng kể độ chính xác MMLU. Tuy nhiên, nó có thể tăng gấp ba lần ngân sách huấn luyện cần thiết. Chỉ sử dụng một LM nguồn nhỏ có thể vượt trội hơn Atlas mạnh mẽ với ít FLOP huấn luyện hơn.

### 5.2 Nghiên cứu Ablation

Trong thí nghiệm này, chúng tôi tiến hành nghiên cứu ablation về huấn luyện augmentation-adapted và phân tích hành vi mô hình trong quá trình huấn luyện.

Hình 4a minh họa rằng huấn luyện augmentation-adapted có thể mang lại những cải thiện bổ sung so với các bộ truy xuất được pre-train. Nhìn chung, ANCE được hưởng lợi nhiều hơn từ huấn luyện augmentation-adapted so với Contriever. Điều này có thể do thực tế là Contriever đã được pre-train chuyên sâu trên dữ liệu tăng cường lớn cũng như MS MARCO trong khi ANCE chỉ được huấn luyện trên MS MARCO.

Trong Hình 4b, chúng tôi so sánh các bộ truy xuất được huấn luyện với các tài liệu tích cực khác nhau, bao gồm các tài liệu được con người ưa thích được chú thích bởi người dùng tìm kiếm (thanh xanh), các tài liệu được LM ưa thích thu được bởi LM nguồn (thanh cam), và sự kết hợp của chúng (thanh xanh lá và thanh đỏ). Vì bộ truy xuất đã được pre-train trên MS MARCO được người dùng chú thích, việc chỉ sử dụng các tài liệu được con người ưa thích để huấn luyện nó có thể vô nghĩa và do đó hoạt động tệ nhất trong tất cả các phương pháp. Chỉ sử dụng các tài liệu được LM ưa thích thể hiện những cải thiện đáng chú ý so với chỉ sử dụng các tài liệu được con người ưa thích, và việc hợp nhất cả tài liệu được con người ưa thích và được LM ưa thích (thiết lập chính của chúng tôi) càng tăng cường hiệu suất của bộ truy xuất. Cuối cùng, việc sử dụng Flan-T5 Base làm LM nguồn mang lại kết quả tốt hơn so với việc sử dụng Flan-T5 Large khi các LM đích tương đối nhỏ. Tuy nhiên, khi kích thước LM đích tăng, cả hai phương pháp đều đạt hiệu suất tương đương. Do đó, lựa chọn sử dụng LM nguồn nhỏ trong huấn luyện augmentation-adapted của chúng tôi là hợp lý và hiệu quả.

Hình 5a và Hình 5b vẽ biểu đồ hiệu suất của bộ truy xuất và LM trong quá trình huấn luyện augmentation-adapted. Ở đầu quá trình huấn luyện, MRR@10 của bộ truy xuất trên MS MARCO giảm mạnh, cho thấy khoảng cách phân phối lớn giữa các tài liệu được con người ưa thích và được LM ưa thích. Khi mất mát train và dev của bộ truy xuất liên tục giảm, LM tăng cường truy xuất dần dần hoạt động tốt hơn trên MSMARCO QA và cuối cùng, trên MMLU. Kết quả này ngụ ý rằng các LM trên các tác vụ khác nhau có thể chia sẻ các ưu tiên chung, khiến AAR tổng quát tốt từ tác vụ nguồn đơn lẻ đến các tác vụ đích không đồng nhất.

### 5.3 Phân tích các Tài liệu được LM Ưa thích

Chúng tôi nhấn mạnh sự cần thiết của việc điều chỉnh các bộ truy xuất hiện có với LM bằng cách so sánh các tài liệu ưa thích giữa người dùng tìm kiếm và LM. Nhìn chung, chúng tôi phát hiện rằng các tài liệu được LM ưa thích có thể hỗ trợ LM từ các góc độ thay thế thay vì thông tin đầy đủ được người dùng tìm kiếm ưa chuộng.

Đầu tiên, chúng tôi định nghĩa sự trùng lặp tập hợp O giữa hai tập tài liệu tích cực D+1 và D+2. Như được minh họa trong Hình 6a, sự trùng lặp tập hợp của các tập tài liệu tích cực được chú thích bởi người dùng con người (Dh+) và LM (Top-KSai,Da) khá thấp (gần 13%), chứng minh xu hướng khác biệt của chúng trong việc chọn tài liệu có giá trị. Ngược lại, sự trùng lặp giữa các LM khác nhau tương đối cao (trên 55%). Bằng chứng này cung cấp lý do mạnh mẽ cho khả năng tổng quát của AAR vì các LM có kích thước khác nhau có xu hướng chú thích các tài liệu tích cực tương tự. Hơn nữa, các LM có kích thước gần nhau thường có sự trùng lặp cao hơn. Điều này ngụ ý khả năng tổng quát tốt hơn của AAR với các LM có năng lực gần với LM nguồn. Các phát hiện này càng xác nhận kết quả được minh họa trong Hình 4b.

Để đưa ra phân tích sâu về cách các tài liệu được con người ưa thích và được LM ưa thích khác nhau, chúng tôi trình bày hai trường hợp đại diện được lấy mẫu từ MSMARCO QA trong Bảng 2. Chúng tôi quan sát thấy tài liệu được con người ưa thích luôn có thể trình bày câu trả lời vàng ở đầu văn bản, trong khi tài liệu được LM ưa thích có thể không chứa câu trả lời chính xác. Tuy nhiên, một tài liệu được LM ưa thích có thể (1) đưa ra góc nhìn mới để trả lời câu hỏi đã cho, ví dụ: "trách nhiệm của hãng tàu du lịch nếu bạn lỡ chuyến tàu du lịch" và (2) đưa ra giải thích cụ thể thay vì định nghĩa trừu tượng, ví dụ: "chuyển giao cưỡng bức lãnh thổ của một quốc gia bởi quốc gia khác". Những đặc điểm này khác với người dùng tìm kiếm muốn thông tin đầy đủ và có thể hỗ trợ thêm cho LM trong suy luận dựa trên kiến thức.

Chúng tôi tiếp tục kiểm tra các đặc điểm độc đáo của tài liệu được LM ưa thích thông qua kiểm tra xóa câu trả lời (tức là xóa span câu trả lời chính xác khỏi các tài liệu được truy xuất). Như được hiển thị trong Hình 6b, bộ truy xuất được huấn luyện bởi tài liệu được con người ưa thích (tức là bộ truy xuất được con người ưa thích) hoặc tài liệu được LM ưa thích (tức là bộ truy xuất được LM ưa thích) đều có thể giúp LM trả lời câu hỏi đã cho. Tuy nhiên, sau khi xóa câu trả lời, hiệu suất của LM với bộ truy xuất được con người ưa thích giảm đáng kể hơn so với bộ truy xuất được LM ưa thích. Mặc dù có ít câu trả lời khớp chính xác hơn (0,6% đối với tài liệu được LM ưa thích so với 13,0% đối với tài liệu được con người ưa thích), tài liệu được LM ưa thích cung cấp thông tin hữu ích từ các góc độ thay thế. Do đó, việc điều chỉnh bộ truy xuất với tài liệu được LM ưa thích có thể làm cho LM tăng cường truy xuất hoạt động tốt hơn.

### 5.4 Huấn luyện Đa nhiệm vụ của AAR

Trong phần này, chúng tôi khám phá liệu huấn luyện đa nhiệm vụ của AAR có thể trao cho bộ truy xuất khả năng tổng quát tốt hơn cho tác vụ đích hay không. Cụ thể, chúng tôi chọn KILT làm nguồn dữ liệu đa nhiệm vụ, bao gồm 5 danh mục (Fact Checking, Entity Linking, Slot Filling, Open Domain QA và Dialogue). Chúng tôi lấy một tác vụ con đại diện cho mỗi danh mục để tạo thành một hỗn hợp của nhiều tác vụ nguồn.

Hình 7 minh họa rằng ANCE được huấn luyện với KILT đa nhiệm vụ có thể liên tục vượt trội hơn MSMARCO QA một nhiệm vụ, chứng minh khả năng tổng quát tốt hơn mang lại bởi huấn luyện augmentation-adapted đa nhiệm vụ. Có thể các LM có thể thay đổi nhẹ trong tài liệu ưa thích cho các tác vụ khác nhau và AAR có thể chuyển đổi mượt mà hơn sang tác vụ đích với sự giúp đỡ của huấn luyện đa nhiệm vụ. Contriever không được hưởng lợi nhiều từ huấn luyện đa nhiệm vụ. Chúng tôi phỏng đoán rằng điều này là do Contriever đã được pre-train với nhiều định dạng tăng cường dữ liệu và do đó tổng quát tốt hơn cho phân phối dữ liệu mới so với ANCE. Thú vị là bộ truy xuất được tinh chỉnh hướng dẫn đa nhiệm vụ TART có hiệu suất tổng thể tệ hơn so với AAR, nhấn mạnh lợi ích của việc có tài liệu được LM ưa thích trong quá trình huấn luyện đa nhiệm vụ.

### 5.5 Tác động của Kho Truy xuất

Bảng 3 chứng minh rằng bất kể kho truy xuất nào, AAR đều dẫn đến những cải thiện hiệu suất nhất quán và đáng kể so với LM độc lập.

Trên MMLU, việc sử dụng MS MARCO làm kho truy xuất cải thiện LM nhiều hơn so với KILT-Wikipedia. Chúng tôi giả định rằng bộ truy xuất đã được huấn luyện với kho MS MARCO và do đó có hiệu suất truy xuất tốt hơn trên đó.

Trên PopQA, hiệu suất mô hình sẽ giảm đáng kể nếu chúng tôi sử dụng MS MARCO làm kho truy xuất thay vì KILT-Wikipedia. Lý do chính là bộ dữ liệu PopQA được lấy mẫu từ Wikidata và được thiết kế cho các câu hỏi long-tail. Kiến thức long-tail một phần chỉ có thể được tìm thấy trong KILT-Wikipedia trong khi MS MARCO thiếu bằng chứng không thể thiếu nên được sử dụng để dự đoán câu trả lời. Ví dụ, với câu hỏi "Ai là mẹ của Melissa Benn?", không có tài liệu nào trong MS MARCO chứa câu trả lời "Caroline Benn". Trong những trường hợp như vậy, việc căn chỉnh kho truy xuất với nguồn dữ liệu có thể cần thiết để tận dụng khả năng của AAR.

### 5.6 Kịch bản Ứng dụng của AAR

Để kiểm tra liệu AAR có hoạt động cho các LM chưa thấy thiếu khả năng tổng quát zero-shot hay không, chúng tôi cũng báo cáo kết quả của OPT và GPT-neo. Những mô hình này có thể có hiệu suất zero-shot kém do thiếu huấn luyện hướng dẫn đa nhiệm vụ.

Từ Bảng 4, chúng tôi phát hiện rằng AAR của chúng tôi cải thiện cả hai LM một cách biên tế trên MMLU trong khi đạt được những cải thiện đáng kể trên PopQA. Chúng tôi phỏng đoán rằng các LM có thể được hưởng lợi dễ dàng hơn từ tăng cường truy xuất trên tác vụ thăm dò kiến thức như PopQA, nơi span câu trả lời có thể được thu thập trực tiếp từ các tài liệu được truy xuất. MMLU đòi hỏi LM không chỉ hiểu các mảnh bằng chứng được truy xuất mà còn thực hiện suy luận dựa trên kiến thức trên chúng. OPT và GPT-neo có thể không sở hữu những khả năng như vậy trong các kịch bản zero-shot.

Tóm lại, mặc dù AAR phù hợp hoàn hảo với các LM được tinh chỉnh hướng dẫn đa nhiệm vụ như dòng Flan-T5 và InstructGPT, nó có thể không mang lại những cải thiện đáng kể cho các LM có hiệu suất zero-shot đôi khi kém, đặc biệt là trên suy luận dựa trên kiến thức. Tuy nhiên, chúng tôi tin rằng các mô hình được tinh chỉnh hướng dẫn đa nhiệm vụ sẽ là nền tảng của công việc tương lai do khả năng tổng quát zero-shot xuất sắc của chúng, đảm bảo các kịch bản ứng dụng rộng rãi của AAR.

## 6. Thảo luận

**Tài liệu được LM Ưa thích.** Việc thu thập tín hiệu phản hồi rời rạc từ LM là thách thức vì nó đòi hỏi khả năng gắn nhãn vượt trội, điều này không phải là mục đích thiết kế của LM. Lấy cảm hứng từ ADist và Atlas, chúng tôi sử dụng điểm FiDAtt để chọn các tài liệu được LM ưa thích cho huấn luyện augmentation-adapted. Tuy nhiên, điểm FiDAtt có thể không phản ánh trung thực đóng góp thực tế của từng tài liệu vì LM có thể thích chú ý đến các tài liệu dễ đọc hơn là tài liệu có thông tin. Hơn nữa, chất lượng của tài liệu được LM ưa thích phụ thuộc nhiều vào hiệu suất ban đầu của LM tăng cường truy xuất. Nghiên cứu song song tính toán KL divergence giữa likelihood truy xuất và likelihood LM để huấn luyện bộ truy xuất. Tuy nhiên, họ yêu cầu LM nguồn lớn hơn, Curie (6.7B), để cung cấp tín hiệu likelihood LM chính xác. Trong tương lai, học tăng cường có thể phục vụ như một phương pháp thay thế để huấn luyện bộ truy xuất, vì nó tối ưu hóa bộ truy xuất bằng cách trực tiếp tận dụng tín hiệu của LM mà không dựa vào quy tắc được thiết kế.

**Plugin Truy xuất Chung.** Chatgpt-retrieval-plugin gần đây đã thu hút sự chú ý trong cộng đồng NLP như một plugin truy xuất chung. Nó truy xuất tài liệu liên quan nhất từ nguồn dữ liệu của người dùng và điều chỉnh phản hồi của ChatGPT để đáp ứng nhu cầu cụ thể của họ. Chúng tôi tin rằng các kỹ thuật như AAR sẽ tăng cường khả năng của ChatGPT black-box tạo ra phản hồi hợp lý hơn dựa trên thông tin được truy xuất, từ đó thúc đẩy phát triển thiết kế LM lấy con người làm trung tâm.

## 7. Kết luận và Công việc Tương lai

Bài báo này giới thiệu plugin truy xuất chung sử dụng bộ truy xuất chung để tăng cường các LM đích có thể không được biết trước hoặc không thể được tinh chỉnh chung. AAR được đề xuất của chúng tôi có thể trực tiếp hỗ trợ các LM black-box mà không yêu cầu bất kỳ tinh chỉnh nào của LM. Điều này được thực hiện bằng cách xây dựng dữ liệu huấn luyện của AAR với các tài liệu ưa thích từ LM nguồn nhỏ cùng với ground truth.

Kết quả thực nghiệm trên MMLU và PopQA chứng minh rằng các LM được hỗ trợ AAR vượt trội đáng kể so với những LM độc lập trong các kịch bản zero-shot, và AAR tổng quát tốt cho các LM có kích thước và cấu trúc khác nhau. Kết quả phân tích cho thấy các tài liệu được LM ưa thích và được con người ưa thích bổ sung cho nhau; các tài liệu được LM ưa thích từ các LM khác nhau trùng lặp đáng kể, và các LM có kích thước tương tự có xu hướng tạo ra các tập tài liệu gần gũi hơn.

Chúng tôi để lại giải thích chi tiết hơn về cách các LM khác nhau tương tác với các tài liệu tăng cường và việc lựa chọn hợp lý hơn các tài liệu được LM ưa thích cho công việc tương lai. Chúng tôi hy vọng công trình của chúng tôi soi sáng con đường đến cách chung để coi các LM lớn như hộp đen và điều chỉnh bộ truy xuất để tăng cường chúng.

## Hạn chế

Do hạn chế về tài nguyên tính toán, chúng tôi chưa đánh giá Flan-T5 XXL có số lượng tham số là 11B, và OPT có số lượng tham số lớn hơn 1.3B.

Vì OPT và GPT-neo hoạt động kém trong thiết lập zero-shot và việc tách điểm attention của từng tài liệu trong đầu vào là tẻ nhạt đối với các mô hình decoder-only, chúng tôi chọn không sử dụng chúng làm LM nguồn. Tuy nhiên, chúng tôi chứng minh rằng việc lấy mô hình encoder-decoder Flan-T5 Base làm LM nguồn cũng mạnh mẽ để tăng cường các mô hình decoder-only. Chúng tôi sẽ khám phá các phương pháp mới để chú thích các tài liệu được LM ưa thích của các mô hình decoder-only dựa trên tín hiệu vốn có của chúng.
