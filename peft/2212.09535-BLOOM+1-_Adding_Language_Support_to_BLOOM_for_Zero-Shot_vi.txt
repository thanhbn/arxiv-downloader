# BLOOM+1: Thêm Hỗ trợ Ngôn ngữ cho BLOOM để Prompting Không cần Ví dụ

Zheng-Xin Yong1∗, Hailey Schoelkopf2,3, Niklas Muennighoff4, Alham Fikri Aji5,
David Ifeoluwa Adelani6,Khalid Almubarak7,M Saiful Bari8,Lintang Sutawika2,9,
Jungo Kasai10,Ahmed Baruwa11,Genta Indra Winata12,Stella Biderman2,13,
Edward Raff13,Dragomir Radev3,Vassilina Nikoulina14

1Đại học Brown2EleutherAI3Đại học Yale4Hugging Face5MBZUAI
6University College London7PSAU8Đại học Công nghệ Nanyang
9Datasaur.ai10Trường Khoa học Máy tính Paul G. Allen, Đại học Washington
11Đại học Oregon12Đại học Khoa học và Công nghệ Hồng Kông
13Booz Allen Hamilton14NA VER LABS Europe

## Tóm tắt

Mô hình BLOOM là một mô hình ngôn ngữ đa ngữ lớn có sẵn công khai, nhưng việc tiền huấn luyện của nó chỉ giới hạn ở 46 ngôn ngữ. Để mở rộng lợi ích của BLOOM cho các ngôn ngữ khác mà không phải chịu chi phí cấm cao, việc thích ứng BLOOM với các ngôn ngữ mới không được thấy trong quá trình tiền huấn luyện là mong muốn. Trong công việc này, chúng tôi áp dụng các chiến lược thích ứng ngôn ngữ hiện có cho BLOOM và đánh giá hiệu suất prompting không cần ví dụ của nó trên tám ngôn ngữ mới trong môi trường hạn chế tài nguyên. Chúng tôi thấy rằng thích ứng ngôn ngữ hiệu quả trong việc cải thiện hiệu suất không cần ví dụ trong các ngôn ngữ mới. Đáng ngạc nhiên, chúng tôi thấy rằng tinh chỉnh dựa trên adapter hiệu quả hơn tiếp tục tiền huấn luyện cho các mô hình lớn. Ngoài ra, chúng tôi phát hiện rằng hiệu suất prompting không bị ảnh hưởng đáng kể bởi các đặc điểm ngôn ngữ cụ thể, chẳng hạn như hệ thống chữ viết. Nó chủ yếu được xác định bởi kích thước của dữ liệu thích ứng ngôn ngữ. Chúng tôi cũng thêm các ngôn ngữ mới vào BLOOMZ, đây là phiên bản của BLOOM được tinh chỉnh đa tác vụ có khả năng thực hiện hướng dẫn tác vụ không cần ví dụ. Chúng tôi thấy rằng bao gồm một ngôn ngữ mới trong hỗn hợp tinh chỉnh đa tác vụ là phương pháp hiệu quả nhất để dạy BLOOMZ một ngôn ngữ mới. Chúng tôi kết luận rằng với đủ dữ liệu huấn luyện, thích ứng ngôn ngữ có thể khái quát hóa tốt cho các ngôn ngữ đa dạng. Mã nguồn của chúng tôi có sẵn tại https://github.com/bigscience-workshop/multilingual-modeling.

## 1 Giới thiệu

Mặc dù khả năng tiếp cận các mô hình ngôn ngữ dựa trên transformer đã mở rộng rất nhiều trong vài năm qua (Black et al., 2021; Wang và Komatsuzaki, 2021; Artetxe et al., 2021; Black et al., 2022; Zhang et al., 2022), những công nghệ này vẫn tập trung áp đảo ở một số ngôn ngữ có tài nguyên cao (Talat et al., 2022). BLOOM (Scao et al., 2022), mô hình ngôn ngữ đa ngữ lớn nhất có sẵn công khai cho đến nay với 176 tỷ tham số, chỉ bao gồm 46 ngôn ngữ tự nhiên và thậm chí loại trừ các ngôn ngữ có tài nguyên cao như tiếng Hàn và tiếng Nga có hàng chục triệu người nói. Hạn chế này được thúc đẩy bởi một số yếu tố, đáng chú ý nhất là chỉ xem xét các ngôn ngữ mà cộng đồng có đủ chuyên môn để xác thực chất lượng dữ liệu theo cách thủ công (Kreutzer et al., 2022), khử trùng lặp và loại bỏ thông tin nhận dạng cá nhân (Laurençon et al., 2022) và có quyền truy cập đầy đủ vào văn bản không nhãn được cấp phép (Joshi et al., 2020). Tất cả những yếu tố này đều là sự thật có điều kiện về nhóm đã huấn luyện mô hình, và để ngỏ ý tưởng rằng các nhà nghiên cứu khác có thể đóng góp thêm ngôn ngữ. Vì việc huấn luyện lại một mô hình như vậy thường xuyên tốn kém cấm đoán, câu hỏi liệu mô hình này có thể được thích ứng hiệu quả để hiểu các ngôn ngữ bổ sung sau khi huấn luyện trở nên cấp bách.

Chúng tôi giả thuyết rằng kịch bản thích ứng ngôn ngữ đặc biệt thú vị đối với các ngôn ngữ có tài nguyên thấp sẽ được hưởng lợi từ việc chuyển giao kiến thức. Do đó, chúng tôi thích ứng các mô hình BLOOM để hỗ trợ tám ngôn ngữ mới (tiếng Đức, tiếng Nga, tiếng Bulgaria, tiếng Thái, tiếng Thổ Nhĩ Kỳ, tiếng Hy Lạp, tiếng Hàn và tiếng Guarani) trong các môi trường hạn chế tài nguyên, nơi chúng tôi chỉ sử dụng một lượng mẫu hạn chế (tối đa 100K mẫu) cho mỗi ngôn ngữ. Chúng tôi đánh giá prompting không cần ví dụ của chúng trên các tác vụ NLU khác nhau sau khi thích ứng. Các ngôn ngữ mới bao gồm cả các script được nhìn thấy và không được nhìn thấy trong dữ liệu tiền huấn luyện, và chúng khác nhau về họ ngôn ngữ và thứ tự từ. Chúng tôi đánh giá các phương pháp thích ứng ngôn ngữ hiện có, chẳng hạn như tiếp tục tiền huấn luyện và MAD-X (Pfeiffer et al., 2020), cũng như một phương pháp học chuyển giao hiệu quả tham số tiên tiến, (IA)3 (Liu et al., 2022).

Công việc hiện tại về việc thích ứng các mô hình đa ngữ lớn chủ yếu khám phá việc tiếp tục tiền huấn luyện (Müller và Laurent, 2022; NovelAI, 2022; De la Rosa và Fernández, 2022) của GPT-J-6B của EleutherAI (Wang và Komatsuzaki, 2021). Hơn nữa, Ebrahimi và Kann (2021) cho thấy rằng tiếp tục tiền huấn luyện vượt trội hơn các chiến lược khác để thích ứng các mô hình ngôn ngữ nhỏ/vừa (tức là các mô hình có ít hơn một tỷ tham số). Tuy nhiên, các thí nghiệm của chúng tôi chứng minh rằng, đối với các mô hình ngôn ngữ lớn như BLOOM với kích thước tương đương GPT-J-6B, tiếp tục tiền huấn luyện kém hiệu quả hơn adapter trong môi trường hạn chế tài nguyên. Ngoài ra, công việc của chúng tôi tập trung vào việc nghiên cứu tác động của thích ứng ngôn ngữ đến prompting, điều này đã được khám phá chưa đầy đủ trong công việc thích ứng ngôn ngữ trước đây (Ebrahimi và Kann, 2021; Ansell et al., 2022; Parovi´c et al., 2022; Pfeiffer et al., 2022). Prompting có thể mang lại lợi ích cho nhiều ngôn ngữ thiếu lượng lớn dữ liệu có nhãn vì nó cho phép các mô hình ngôn ngữ khái quát hóa thành một loạt các tác vụ với chi phí huấn luyện và dữ liệu ít hơn đáng kể so với tinh chỉnh đầy đủ (Liu et al., 2021; Le Scao và Rush, 2021).

### 1.1 Đóng góp của chúng tôi

Công việc của chúng tôi là đầu tiên khám phá các tác động mở rộng của các chiến lược thích ứng ngôn ngữ cho các mô hình ngôn ngữ với hàng tỷ tham số trong môi trường hạn chế tài nguyên. Trái ngược với công việc trước đây về các mô hình ngôn ngữ được che dấu đa ngữ nhỏ/vừa (Ebrahimi và Kann, 2021), chúng tôi khuyến nghị huấn luyện adapter thay vì tiếp tục tiền huấn luyện cho BLOOM với ít nhất 3 tỷ tham số để có hiệu suất prompting tốt hơn. Chúng tôi kết nối thêm khuyến nghị này với cách chất lượng biểu diễn độc lập ngôn ngữ mở rộng với các tham số mô hình.

Chúng tôi cũng chứng minh các tác động tích cực của thích ứng ngôn ngữ đơn ngữ đến hiệu suất prompting của BLOOM trên các bộ dữ liệu khác nhau. BLOOMZ là một biến thể của BLOOM được tạo ra bằng cách tinh chỉnh BLOOM trên hỗn hợp đa tác vụ trong cùng các ngôn ngữ được nhìn thấy trong quá trình tiền huấn luyện. Chúng tôi thấy rằng chỉ cần thêm một ngôn ngữ mới trong tinh chỉnh đa tác vụ là hiệu quả trong việc cải thiện hiệu suất trong ngôn ngữ mới.

Để tóm tắt, các đóng góp của chúng tôi bao gồm:
• Nghiên cứu các tác động của thích ứng ngôn ngữ đến prompting không cần ví dụ và điều chỉnh hướng dẫn.
• Đánh giá các phương pháp hiệu quả tham số để thích ứng các mô hình BLOOM ở nhiều quy mô khác nhau và phân tích sự đánh đổi giữa lượng tính toán cần thiết và hiệu suất prompting không cần ví dụ.
• Định lượng tác động của kích thước dữ liệu thích ứng ngôn ngữ đến thích ứng ngôn ngữ.

## 2 Công việc liên quan

**Thích ứng ngôn ngữ** Thích ứng ngôn ngữ cho phép các mô hình ngôn ngữ được tiền huấn luyện hỗ trợ các ngôn ngữ ngoài dữ liệu tiền huấn luyện của chúng. Hầu hết các công việc nghiên cứu thích ứng ngôn ngữ xem xét các mô hình ngôn ngữ được che dấu như mBERT (Devlin et al., 2019) và XLM-R (Conneau et al., 2020) được tiền huấn luyện trên 100+ ngôn ngữ. Các phương pháp thích ứng ngôn ngữ có thể được phân loại rộng rãi thành ba danh mục: (1) tiếp tục tiền huấn luyện mô hình (chỉ giới hạn ở việc huấn luyện lớp embedding trong một số trường hợp) (Neubig và Hu, 2018; Artetxe et al., 2020; Chau et al., 2020; Muller et al., 2021; Zhang et al., 2020; Wang et al., 2020); (2) huấn luyện adapter cụ thể theo ngôn ngữ (Pfeiffer et al., 2020, 2021a,b; Philip et al., 2020; Üstün et al., 2021; Berard, 2021; Faisal và Anastasopoulos, 2022; Parovi´c et al., 2022) cho ngôn ngữ đích; và (3) huấn luyện một tập hợp con thưa thớt các tham số mô hình (Ansell et al., 2022). Động lực cốt lõi đằng sau các phương pháp này là được hưởng lợi từ việc chuyển giao kiến thức được mã hóa trong các mô hình ngôn ngữ được tiền huấn luyện để xử lý ngôn ngữ mới với chi phí tính toán nhỏ (so với việc huấn luyện lại mô hình đầy đủ từ đầu).

Một vấn đề phổ biến là script của ngôn ngữ mới không phải lúc nào cũng được tokenizer hỗ trợ. Artetxe et al. (2020); Aji et al. (2020); Pfeiffer et al. (2021b) chứng minh rằng có thể thêm một ngôn ngữ mới vào các mô hình này bằng cách huấn luyện một lớp embedding mới. Muller et al. (2021) tiếp tục huấn luyện mBERT được tiền huấn luyện trên dữ liệu ngôn ngữ mới, và thấy rằng việc chuyển tự các ngôn ngữ sử dụng script không phải Latin tăng hiệu suất trên các ngôn ngữ này. Berard (2021) thêm các ngôn ngữ mới vào các mô hình dịch máy đa ngữ được tiền huấn luyện bằng cách huấn luyện các lớp embedding và adapter. Họ cho thấy rằng việc thêm một ngôn ngữ đích mới (ngôn ngữ để dịch sang) khó học hơn so với một ngôn ngữ mới để dịch từ đó.

Công việc gần nhất với nỗ lực đánh giá của chúng tôi là nghiên cứu của Ebrahimi và Kann (2021) về các phương pháp khác nhau (tức là tiếp tục tiền huấn luyện, mở rộng từ vựng và các lớp adapter) để mở rộng mô hình XLM-R sang 30 ngôn ngữ mới trên các tác vụ phân loại cấp độ token. Họ kết luận rằng tiếp tục tiền huấn luyện là hướng hứa hẹn nhất. Tuy nhiên, chi phí của việc tiền huấn luyện như vậy sẽ tăng theo kích thước của mô hình được tiền huấn luyện và có thể cấm đoán đối với nhiều nhà nghiên cứu làm việc với các ngôn ngữ có tài nguyên thấp. Kết quả của chúng tôi cũng cho thấy rằng tiếp tục tiền huấn luyện không nhất thiết mang lại lợi ích hiệu suất prompting cho các mô hình ngôn ngữ lớn hơn.

**Prompting đa ngữ** Prompting tái định dạng các tác vụ NLP thành bài toán mô hình hóa ngôn ngữ được che dấu hoặc tạo sinh, tùy thuộc vào mục tiêu tiền huấn luyện của mô hình. Zhao và Schütze (2021) và Qi et al. (2022) cho thấy rằng tinh chỉnh XLM-R trên các prompt kiểu cloze mang lại hiệu suất tốt hơn tinh chỉnh tiêu chuẩn trong chế độ tài nguyên thấp cho XNLI. Mặt khác, Winata et al. (2022) thấy rằng tinh chỉnh tiêu chuẩn của XLM-R vượt trội hơn học dựa trên prompt để dự đoán cảm xúc trong các phương ngữ Indonesia có tài nguyên thấp.

Một số công việc cho thấy rằng huấn luyện dựa trên prompt đa tác vụ trên nhiều tác vụ khác nhau và các prompt tiếng Anh hoặc đã dịch cải thiện hiệu suất không cần ví dụ đa ngữ và đa tác vụ (Muennighoff et al., 2022; Fu et al., 2022). Học dựa trên prompt đa ngữ cũng có thể đạt được mà không cần thực hiện cập nhật gradient cho các tác vụ hạ nguồn. Chẳng hạn, Lin et al. (2021) chứng minh thành công trong việc prompting các mô hình được tiền huấn luyện giống GPT với học trong ngữ cảnh cho các tác vụ NLU, sử dụng các mẫu prompt tiếng Anh hoặc đã dịch. Shi et al. (2023) thấy rằng khi các mô hình ngôn ngữ mở rộng quy mô, chúng có thể thực hiện suy luận chuỗi tư duy đa ngữ tốt hơn.

## 3 Thiết lập thí nghiệm

### 3.1 Các mô hình BLOOM được tiền huấn luyện

Chúng tôi tập trung vào việc thêm hỗ trợ ngôn ngữ cho mô hình ngôn ngữ BLOOM (Scao et al., 2022) từ 560 triệu đến 7,1 tỷ tham số. BLOOM có kiến trúc Transformer chỉ có decoder sử dụng embedding vị trí AliBi (Press et al., 2022) và chuẩn hóa lớp sau các lớp embedding. Tokenizer của nó được huấn luyện với thuật toán Byte Pair Encoding (BPE) cấp byte (Gage, 1994; Sennrich et al., 2016) với kích thước từ vựng là 250.680.

BLOOM được tiền huấn luyện trong khoảng 350 tỷ token trên corpus ROOTS (Laurençon et al., 2022), bao gồm 46 ngôn ngữ tự nhiên và 13 ngôn ngữ lập trình. Phụ lục M hiển thị phân phối của các ngôn ngữ tự nhiên trong corpus ROOTS.

### 3.2 Các ngôn ngữ mới

Chúng tôi xem xét tất cả sáu ngôn ngữ của XNLI (Conneau et al., 2018) hiện không được BLOOM hỗ trợ: tiếng Đức, tiếng Bulgaria, tiếng Nga, tiếng Hy Lạp, tiếng Thổ Nhĩ Kỳ và tiếng Thái. Chúng tôi cũng bao gồm tiếng Hàn để theo dõi công việc trước đây về việc thích ứng phiên bản trước của BLOOM (Yong và Nikoulina, 2022) và tiếng Guarani, đây là một ngôn ngữ bản địa châu Mỹ thực sự có tài nguyên thấp. Bảng 1 tóm tắt các ngôn ngữ không được nhìn thấy được sử dụng trong các thí nghiệm của chúng tôi. Chúng bao gồm các họ ngôn ngữ khác nhau và một số trong số chúng không chia sẻ script với các ngôn ngữ được BLOOM hỗ trợ.

### 3.3 Các chiến lược thích ứng ngôn ngữ

Chúng tôi thực hiện ba chiến lược thích ứng ngôn ngữ để phân tích tác động của chúng đến prompting không cần ví dụ.

**Tiếp tục tiền huấn luyện** Chiến lược tiếp tục tiền huấn luyện đề cập đến việc tiếp tục huấn luyện mô hình BLOOM với mục tiêu tiền huấn luyện mô hình hóa ngôn ngữ nhân quả của nó trên văn bản đơn ngữ của ngôn ngữ mới (Chau et al., 2020; Ebrahimi và Kann, 2021; Muller et al., 2021).

**MAD-X** Chúng tôi sử dụng adapter ngôn ngữ và adapter khả nghịch của cấu hình MAD-X (Pfeiffer et al., 2020) để thích ứng BLOOM với các ngôn ngữ mới. Adapter ngôn ngữ đề cập đến adapter cổ chai với các lớp feedforward chiếu xuống và lên (Houlsby et al., 2019; Pfeiffer et al., 2021a) được chèn vào mỗi khối Transformer. Adapter khả nghịch được sử dụng trong các lớp embedding để giảm thiểu sự không khớp giữa từ vựng ngôn ngữ gốc và mới.

**(IA)³** (IA)³ là một phương pháp tinh chỉnh hiệu quả tham số thực hiện việc chia tỷ lệ theo từng phần tử của các kích hoạt khối Transformer bên trong thông qua các vector có thể học (Liu et al., 2022). Các vector này có thể được hợp nhất với các trọng số được tiền huấn luyện ban đầu của mô hình tại thời điểm suy luận để giảm độ trễ bằng cách tránh truyền các kích hoạt qua các mô-đun adapter bổ sung.

Chúng tôi thí nghiệm với (IA)³ vì nó vượt trội hơn các adapter cổ chai, được sử dụng trong MAD-X, và các phương pháp tinh chỉnh hiệu quả tham số khác như BitFit (Ben Zaken et al., 2022), LoRA (Hu et al., 2022), và FishMask (Sung et al., 2021) trên các tác vụ NLU tiếng Anh (Liu et al., 2022). Các thí nghiệm sơ bộ của chúng tôi cho thấy rằng (IA)³ hoạt động tốt hơn các phương pháp này (xem Phụ lục G), và do đó chúng tôi chỉ chạy (IA)³ do hạn chế tính toán.

Vì (IA)³ không thích ứng lớp embedding, chúng tôi kết hợp (IA)³ với adapter khả nghịch để so sánh công bằng hơn với adapter ngôn ngữ MAD-X. Các thí nghiệm sơ bộ của chúng tôi (Bảng 4) cho thấy lợi ích hiệu suất khi sử dụng adapter khả nghịch với (IA)³.

### 3.4 Thiết lập thích ứng ngôn ngữ

Chúng tôi lấy mẫu ngẫu nhiên 100K mẫu từ các subcorpora OSCAR đã khử trùng lặp (Ortiz Suárez et al., 2019) của các ngôn ngữ tương ứng để thích ứng ngôn ngữ nhằm mô phỏng các thiết lập tài nguyên thấp. Vì Guarani chỉ có khoảng 100 mẫu trong OSCAR, chúng tôi sử dụng corpora song song Jojajovai (Chiruzzo et al., 2022), chứa 30K câu Guarani. Chúng tôi thực hiện 25K bước huấn luyện thích ứng ngôn ngữ sử dụng kích thước batch là 8 và độ dài chuỗi là 1.024. Xem Phụ lục H để biết thêm chi tiết.

Chúng tôi không huấn luyện lại tokenizer vì BLOOM sử dụng tokenization BPE cấp byte, không bao giờ tạo ra token không xác định; do đó, chúng tôi có thể thực hiện thích ứng ngôn ngữ mà không cần mở rộng từ vựng. Chúng tôi thích ứng lớp embedding theo hai cách khác nhau. Đối với tiếp tục tiền huấn luyện, chúng tôi làm cho lớp embedding có thể huấn luyện. Điều này tuân theo công việc trước đây về thích ứng ngôn ngữ (Pfeiffer et al., 2020; Chau et al., 2020; Ebrahimi và Kann, 2021; Fujinuma et al., 2022). Đối với MAD-X và (IA)³, chúng tôi sử dụng adapter khả nghịch để thích ứng lớp embedding trong khi giữ các embedding bị đóng băng.

### 3.5 Các tác vụ và mẫu prompt

Chúng tôi đánh giá các mô hình trên năm tác vụ NLU đa ngữ, bao gồm suy luận ngôn ngữ tự nhiên (XNLI (Conneau et al., 2018), KLUE-NLI (Park et al., 2021), và AmericasNLI (Ebrahimi et al., 2022)), suy luận thông thường (XCOPA (Ponti et al., 2020) và XStoryCloze (Lin et al., 2021)), giải quyết anaphora (XWinograd (Tikhonov và Ryabinin, 2021)), và diễn giải lại (PAWS-X (Yang et al., 2019)). Chúng tôi thực hiện prompting không cần ví dụ mà không có bất kỳ tinh chỉnh cụ thể tác vụ nào và chỉ đơn giản là tái sử dụng các mẫu được sử dụng để prompt mô hình XGLM Lin et al. (2021) mà không thực hiện bất kỳ kỹ thuật prompt nào. Chúng tôi dịch các mẫu prompt bằng API dịch tự động, và các mẫu đã dịch có thể được tìm thấy trong Phụ lục F.

### 3.6 Các đường cơ sở

Chúng tôi so sánh mô hình BLOOM được thích ứng với các mô hình ngôn ngữ đa ngữ tạo sinh đã báo cáo hiệu suất prompting tiên tiến. Chúng tôi cũng báo cáo hiệu suất prompting của các mô hình BLOOM gốc mà không có bất kỳ thích ứng nào.

**XGLM** Các mô hình XGLM (Lin et al., 2021) bao gồm 30 ngôn ngữ tự nhiên và có năm số lượng tham số khác nhau: 564M, 1.7B, 2.9B, 4.9B, và 7.5B.

**mGPT** mGPT (Shliazhko et al., 2022) là một mô hình GPT được huấn luyện trên 60 ngôn ngữ từ 25 họ ngôn ngữ sử dụng Wikipedia và Colossal Clean Crawled Corpus. Nó chỉ có 1.3B tham số.

**BLOOMZ và mT0** BLOOMZ và mT0 là các mô hình BLOOM và mT5 được tinh chỉnh trên hỗn hợp tác vụ đa ngữ, xP3 (Muennighoff et al., 2022). Ở đây chúng tôi báo cáo hiệu suất trên các prompt tốt nhất, tương ứng với các hướng dẫn bằng tiếng Anh trong khi ngữ cảnh và nhãn thường không phải tiếng Anh. Chúng tôi cũng không báo cáo hiệu suất trên dữ liệu PAWS-X vì nó là một phần của hỗn hợp huấn luyện xP3.

Trong số các đường cơ sở, XGLM, mGPT, và mT0 đã thấy tất cả các ngôn ngữ mới trong Bảng 1 ngoại trừ Guarani trong quá trình tiền huấn luyện mô hình.

## 4 Kết quả và thảo luận

### 4.1 Hiệu suất prompting không cần ví dụ

Hình 1 cho thấy rằng thích ứng ngôn ngữ cải thiện prompting không cần ví dụ của BLOOM gốc cho các ngôn ngữ không được nhìn thấy trong thiết lập hạn chế tài nguyên. Hơn nữa, nhìn chung, thích ứng ngôn ngữ tuân theo quy luật mở rộng quy mô, quy định rằng lợi ích hiệu suất tương quan với kích thước mô hình. Chúng tôi lưu ý rằng khi mô hình transformer BLOOM trở nên rộng hơn (từ 560M đến 1.7B tham số), một số tác vụ như German XNLI và PAWSX trải qua giảm hiệu suất.

Đối với mô hình BLOOM nhỏ nhất với 560 triệu tham số, chúng tôi thấy rằng tiếp tục tiền huấn luyện mang lại hiệu suất prompting tốt nhất. Kết quả của chúng tôi hỗ trợ phát hiện của Ebrahimi và Kann (2021) rằng tiếp tục tiền huấn luyện các mô hình ngôn ngữ được che dấu có kích thước tương tự, như mBERT và XLM-Roberta, mang lại hiệu suất NER và gắn thẻ POS tốt hơn so với adapter. Tuy nhiên, khi kích thước mô hình tăng vượt quá 3 tỷ tham số, các phương pháp thích ứng ngôn ngữ dựa trên adapter vượt trội hơn tiếp tục tiền huấn luyện mặc dù có ít tham số có thể huấn luyện hơn. Hơn nữa, trái ngược với các phát hiện trước đây (Yong và Nikoulina, 2022), BLOOM thích ứng tốt với các ngôn ngữ mới bất kể họ ngôn ngữ, thứ tự từ, và liệu chúng có chia sẻ hệ thống script với các ngôn ngữ trong dữ liệu tiền huấn luyện hay không (Hình 2). Chúng tôi lưu ý rằng có nhiều khác biệt trong thiết lập của Yong và Nikoulina (2022). Yong và Nikoulina (2022) đã sử dụng một mô hình đa ngữ sử dụng embedding vị trí được học thay vì Alibi (Press et al., 2022) và chỉ hỗ trợ 13 ngôn ngữ. Họ cũng tinh chỉnh cả lớp embedding vị trí được học và embedding từ.

Chúng tôi thấy rằng BLOOM được thích ứng khớp với hiệu suất của mGPT trong một số tác vụ XNLI và thậm chí vượt trội hơn XGLM và mT0 trong các tác vụ German PAWS-X và Russian XWinograd. Tuy nhiên, mT0, đã thấy các ngôn ngữ trong quá trình tiền huấn luyện và được huấn luyện trên hỗn hợp prompt tác vụ đa ngữ, thể hiện hiệu suất prompting không cần ví dụ tốt nhất khi các tham số mô hình được tăng lên.

Chúng tôi thấy rằng BLOOM được thích ứng hoạt động kém trên Guarani, đây là một ngôn ngữ thực sự có tài nguyên thấp. Thích ứng ngôn ngữ chỉ tăng hiệu suất khi các mô hình vượt quá 3 tỷ tham số được sử dụng. Chúng tôi tin rằng điều này là do dữ liệu huấn luyện thích ứng Guarani hạn chế (30K so với 100K cho các ngôn ngữ khác) như được hỗ trợ bởi các phát hiện trong Phần 4.4.

**Chiến lược thích ứng ngôn ngữ tốt nhất** Chúng tôi khuyến nghị rằng mô hình BLOOM nhỏ nhất nên được thích ứng bằng tiếp tục tiền huấn luyện, nhưng các mô hình BLOOM lớn hơn nên được thích ứng bằng adapter do hiệu suất tốt hơn (Hình 1) và hiệu quả tính toán (Hình 3). Chúng tôi thấy rằng adapter ngôn ngữ MAD-X mang lại hiệu suất prompting không cần ví dụ trung bình tốt hơn, nhưng adapter (IA)³ có lợi thế nhỏ về hiệu quả huấn luyện do có ít tham số có thể huấn luyện hơn đáng kể và thời gian huấn luyện nhỏ hơn cho các mô hình lớn hơn.

### 4.2 Perplexity

Perplexity có thể được xem như một thước đo của sự không chắc chắn khi dự đoán token tiếp theo trong một chuỗi, và khả năng mô hình hóa ngôn ngữ tốt hơn có nghĩa là perplexity thấp hơn. Hình 4 cho thấy perplexity đánh giá trên văn bản tiếng Nga cho tiếp tục tiền huấn luyện và adapter ngôn ngữ MAD-X. Chúng tôi thấy rằng perplexity trong quá trình huấn luyện thích ứng ngôn ngữ không nhất thiết tương quan với hiệu suất prompting. Mặc dù perplexity trở nên thấp hơn cho các mô hình lớn hơn, có sự giảm hiệu suất XWinograd cho cả hai chiến lược thích ứng ngôn ngữ khi khả năng mô hình tăng từ 1.1 tỷ lên 1.7 tỷ tham số. Hơn nữa, mặc dù tiếp tục tiền huấn luyện có perplexity thấp hơn adapter ngôn ngữ MAD-X, điều này gợi ý rằng các mô hình được tiền huấn luyện liên tục mô hình hóa dữ liệu OSCAR tiếng Nga tốt hơn, BLOOM được tiền huấn luyện liên tục kém hiệu suất hơn các đối tác của chúng cho các kích thước mô hình lớn hơn trong cả tác vụ XWinograd và XNLI. Phát hiện này phù hợp với công việc của Liang et al. (2022) làm nổi bật sự không khớp giữa perplexity và hiệu suất tác vụ hạ nguồn.

### 4.3 Kết nối với biểu diễn độc lập ngôn ngữ

Hình 5 báo cáo độ chính xác truy xuất câu (SR) cho tiếng Nga cho các mô hình không được thích ứng, cũng như các mô hình được thích ứng thông qua adapter MAD-X hoặc tiếp tục tiền huấn luyện. Chúng tôi sử dụng độ chính xác truy xuất câu như một cách để đo lường chất lượng biểu diễn độc lập ngôn ngữ, thêm chi tiết trong Phụ lục B. Lưu ý rằng trong thiết lập này, các biểu diễn của tiếng Nga dựa trên mô hình được thích ứng, trong khi các biểu diễn của tiếng Anh dựa trên mô hình gốc, điều này loại trừ vấn đề quên lãng thảm khốc tiềm ẩn. Chúng tôi thấy rằng trước khi thích ứng, độ chính xác SR rất thấp tổng thể, nhưng mô hình lớn hơn thể hiện kết quả SR tốt hơn. Với thích ứng, độ chính xác SR cải thiện đáng kể.

Đối với BLOOM được thích ứng bằng MAD-X, độ chính xác SR cải thiện khi mô hình tăng về tham số. Lý do là các tham số có thể huấn luyện của adapter tăng về kích thước nên chúng biểu diễn các câu tiếng Nga tốt hơn và mô hình lớn hơn bắt đầu từ biểu diễn tốt hơn của cả hai ngôn ngữ. Thú vị, đối với tiếp tục tiền huấn luyện, kết quả độ chính xác SR tốt nhất đạt được với mô hình BLOOM nhỏ nhất có 560 triệu tham số, trong khi các mô hình lớn hơn đạt được độ chính xác SR thấp hơn nhiều. Hiện tượng này đi ngược lại quy luật mở rộng quy mô và ngược lại với những gì đã được quan sát cho MAD-X.

Một số công việc trước đây (Dufter và Schütze, 2020) gợi ý rằng mô hình nhỏ hơn sẽ xuất hiện biểu diễn độc lập ngôn ngữ tốt hơn vì nó bị buộc phải tái sử dụng cùng các tham số cho các ngôn ngữ khác nhau. Tuy nhiên, khi mô hình lớn lên, nó có nhiều tự do hơn để phân vùng các tham số của nó giữa các ngôn ngữ. Lưu ý rằng quan sát này đã được thực hiện trong các thiết lập tổng hợp và theo hiểu biết của chúng tôi, chưa được xác nhận trong các mô hình đa ngữ thực. Kết quả của chúng tôi trong Hình 5 có thể được xem như một hỗ trợ bổ sung cho giả thuyết ban đầu đó. Khi thực hiện tiếp tục tiền huấn luyện với tập dữ liệu thích ứng ngôn ngữ tương đối nhỏ, có nhiều cách để mô hình tối ưu hóa hiệu suất của nó (cf Giả thuyết vé số may mắn (Frankle và Carbin, 2019)). Nếu mô hình có nhiều tự do hơn để phân vùng các tham số của nó giữa các ngôn ngữ khác nhau, không có gì đảm bảo rằng tiếp tục tiền huấn luyện sẽ tận dụng các tham số liên quan đến tiếng Anh và do đó có thể làm cho không gian biểu diễn của nó phân kỳ xa hơn khỏi tiếng Anh. Chúng tôi giả thuyết rằng điều này có thể là một lời giải thích khả thi cho sự suy giảm độ chính xác truy xuất câu tiếp tục tiền huấn luyện cho các mô hình lớn hơn.

### 4.4 Lượng dữ liệu thích ứng ngôn ngữ

Chúng tôi mô phỏng các thiết lập tài nguyên thấp khác nhau với BLOOM-3B sử dụng các lượng dữ liệu huấn luyện thích ứng khác nhau. Chúng tôi sử dụng 1K, 10K và 100K mẫu để mô phỏng các mức độ thiết lập tài nguyên thấp khác nhau (xem Hình 12). Hình 6 chứng minh tương quan tích cực giữa kích thước dữ liệu huấn luyện thích ứng và hiệu suất prompting không cần ví dụ. Chúng tôi thấy rằng, khi được thích ứng với ít hơn 100K mẫu, BLOOM hoạt động tệ hơn so với đối tác không được thích ứng của nó cho các tác vụ như Russian XNLI và Turkish XCOPA. Nói cách khác, dựa trên Hình 6 và Bảng 6, chúng tôi cần khoảng 100 triệu token của ngôn ngữ mới để thích ứng ngôn ngữ hiệu quả. Tuy nhiên, đáng ngạc nhiên, mức độ ảnh hưởng tiêu cực của thiết lập tài nguyên thấp có thể được giới hạn theo loại tác vụ. Chẳng hạn, đối với cùng một ngôn ngữ là tiếng Nga, chúng tôi quan sát tác động hạn chế của thiết lập tài nguyên thấp đến prompting XWinograd và XStoryCloze.

### 4.5 Khả năng của adapter

Chúng tôi điều tra tác động của kích thước khả năng adapter bằng cách thay đổi hệ số giảm (còn được gọi là tỷ lệ nén (Rücklé et al., 2021)) trong lớp cổ chai của adapter. Một giá trị giảm nhỏ hơn sẽ dẫn đến lượng tham số adapter lớn hơn. Trái ngược với Yong và Nikoulina (2022), chúng tôi quan sát tương quan tích cực giữa lượng tham số adapter và hiệu suất prompting (xem Hình 7).

### 4.6 Thích ứng BLOOMZ

Chúng tôi cũng điều tra các chiến lược thích ứng ngôn ngữ cho BLOOMZ, đây là BLOOM được tinh chỉnh trên nhiều prompt tác vụ khác nhau để đạt được khái quát hóa đa ngữ và đa tác vụ tốt hơn (Muennighoff et al., 2022).

#### 4.6.1 Thêm hỗ trợ ngôn ngữ thông qua dữ liệu không nhãn

Tương tự như việc thích ứng BLOOM, chúng tôi huấn luyện adapter ngôn ngữ MAD-X cho BLOOMZ sử dụng cùng thiết lập thí nghiệm trên dữ liệu OSCAR đơn ngữ. Trong Hình 8, chúng tôi cho thấy rằng BLOOMZ-560m có độ chính xác trung vị khoảng 38.5% cho các tác vụ German XNLI (thanh bên trái), nhưng sau khi thích ứng ngôn ngữ, nó hoạt động tệ nhất với độ chính xác kém như một bộ phân loại ngẫu nhiên ở 33% (thanh bên phải). Tuy nhiên, khi được trang bị adapter ngôn ngữ của BLOOM (điều này có thể vì BLOOM và BLOOMZ chia sẻ cùng kiến trúc), BLOOMZ giữ lại khả năng prompting của nó (thanh giữa). Kết quả gợi ý rằng BLOOMZ mất khả năng prompting thu được từ điều chỉnh hướng dẫn đa tác vụ sau khi thích ứng ngôn ngữ trên văn bản dạng tự do của corpora OSCAR đơn ngữ.

#### 4.6.2 Thêm hỗ trợ ngôn ngữ thông qua điều chỉnh hướng dẫn

Chúng tôi thí nghiệm với việc học một ngôn ngữ mới trong quá trình điều chỉnh hướng dẫn sử dụng cùng công thức như BLOOMZ (Muennighoff et al., 2022). Chúng tôi sử dụng tiếng Nga, mà các mô hình BLOOM chưa có ý định nhìn thấy trong quá trình tiền huấn luyện. Chúng tôi thu thập dữ liệu tác vụ ngôn ngữ tự nhiên có giám sát bằng tiếng Nga và tinh chỉnh mô hình BLOOM tiền huấn luyện 7.1 tỷ tham số để tạo ra hai biến thể: (a) BLOOMZ-7.1B-RU, được tinh chỉnh chỉ trên dữ liệu tác vụ tiếng Nga, và (b) BLOOMZ-7.1B-xP3RU, được tinh chỉnh trên toàn bộ bộ dữ liệu xP3 (Muennighoff et al., 2022) với dữ liệu tiếng Nga được thêm vào đó. Chúng tôi so sánh hai mô hình với BLOOM-7.1B và BLOOMZ-7.1B trong Hình 9. Chúng tôi thấy rằng tinh chỉnh chỉ trên tiếng Nga (BLOOMZ-7.1B-RU) mà không có các ngôn ngữ và tác vụ khác trong hỗn hợp xP3 chỉ cho thấy những cải thiện nhỏ so với đường cơ sở được tiền huấn luyện trên XStoryCloze. Điều này có thể là do thiếu sự đa dạng trong tinh chỉnh của BLOOMZ-7.1B-RU (Chung et al., 2022), vì phần chỉ có tiếng Nga chứa ít tác vụ và prompt hơn so với toàn bộ bộ dữ liệu xP3. Mặt khác, khi thêm tiếng Nga vào hỗn hợp điều chỉnh hướng dẫn (BLOOMZ-7.1B-xP3RU), hiệu suất của prompt tốt nhất cải thiện trên XNLI và XStoryCloze. Điều này có nghĩa là việc thêm các ngôn ngữ mới trong quá trình tinh chỉnh đa tác vụ có thể hiệu quả nhưng yêu cầu các tác vụ đa dạng bổ sung trong các ngôn ngữ khác.

## 5 Kết luận

Chúng tôi so sánh sự đánh đổi giữa tính toán và hiệu suất của các chiến lược thích ứng ngôn ngữ khác nhau để mở rộng BLOOM ở nhiều kích thước khác nhau sang các ngôn ngữ mới. Trái ngược với công việc trước đây, chúng tôi thấy rằng các chiến lược dựa trên adapter thích ứng tốt nhất các mô hình BLOOM lớn hơn để prompting trong các thiết lập tài nguyên thấp. Chúng tôi cũng điều tra các yếu tố thích ứng ngôn ngữ khác nhau như kích thước dữ liệu thích ứng ngôn ngữ và khả năng của adapter. Cuối cùng, chúng tôi điều tra mối quan hệ giữa thích ứng ngôn ngữ và điều chỉnh hướng dẫn sử dụng mô hình BLOOMZ, nơi chúng tôi thấy rằng bao gồm các ngôn ngữ mới trong quá trình điều chỉnh hướng dẫn hiệu quả nhất.

## 6 Hạn chế

### 6.1 Thích ứng từ vựng và embedding

Chúng tôi không khám phá thích ứng từ vựng và embedding. Các mô hình của chúng tôi sử dụng tokenization cấp byte, và do đó có thể xử lý các script không được nhìn thấy. Tuy nhiên, người ta có thể lập luận rằng việc tokenization các script không được nhìn thấy có thể không tối ưu. Chẳng hạn, các ngôn ngữ có script không được nhìn thấy sẽ yêu cầu post-tokenization dài hơn, do đó ảnh hưởng đến hiệu quả hiệu suất. Koto et al. (2021) đã cho thấy rằng khi thích ứng với một miền mới, LM đạt được hiệu suất tốt hơn, mặc dù từ vựng cũ cũng có thể hỗ trợ miền mới. Khám phá tác động chất lượng của thích ứng token cho các ngôn ngữ mới và các script mới sẽ rất thú vị. Song song, khám phá cách tốt nhất để khởi tạo embedding của các token mới được hình thành cũng thú vị.

### 6.2 Các chiến lược tinh chỉnh hiệu quả tham số

Chúng tôi chỉ xem xét một số lượng hạn chế các chiến lược tinh chỉnh hiệu quả tham số (xem Phần 3.3 và Phụ lục G) do hạn chế tính toán. Tuy nhiên, chúng tôi tin rằng các chiến lược khác như điều chỉnh prompt (Lester et al., 2021; Tu et al., 2022) và điều chỉnh bên thang (Sung et al., 2022) có thể thích ứng BLOOM cũng như các chiến lược dựa trên adapter được khám phá trong thiết lập thí nghiệm của chúng tôi. Công việc gần đây cũng cho thấy rằng việc kết hợp các loại phương pháp tinh chỉnh hiệu quả tham số khác nhau, bao gồm adapter, có thể dẫn đến hiệu suất tốt hơn (Mao et al., 2022; He et al., 2022). Vì chúng tôi khuyến nghị thích ứng ngôn ngữ dựa trên adapter cho các mô hình ngôn ngữ lớn hơn, sẽ thú vị khi khám phá các phương pháp kết hợp adapter để có hiệu suất prompting tốt hơn.

### 6.3 Các ngôn ngữ có tài nguyên thấp

Một hạn chế của công việc chúng tôi là tập hợp các ngôn ngữ mới của chúng tôi chỉ bao gồm một ngôn ngữ thực sự có tài nguyên thấp, đó là Guarani. Như công việc của chúng tôi cho thấy rằng 100 triệu token cần thiết để thích ứng hiệu quả để prompt trong một ngôn ngữ mới (xem Phần 4.4), một ngôn ngữ thực sự có tài nguyên thấp thường thiếu dữ liệu không nhãn đầy đủ cho việc thích ứng như vậy (Joshi et al., 2020). Do đó, chúng tôi thúc giục cộng đồng nghiên cứu các phương pháp hiệu quả dữ liệu để thích ứng các mô hình ngôn ngữ lớn để prompt trong thiết lập tài nguyên cực kỳ thấp.

### 6.4 Các tác vụ tạo sinh

Vì chúng tôi chỉ bao gồm các tác vụ hiểu ngôn ngữ tự nhiên trong thiết lập thí nghiệm của chúng tôi, các phát hiện của chúng tôi có thể không khái quát hóa cho các tác vụ tạo sinh như tóm tắt. Hơn nữa, thích ứng ngôn ngữ trên dữ liệu đơn ngữ có thể dẫn đến quên lãng thảm khốc của các ngôn ngữ được nhìn thấy (xem Phụ lục L); do đó, các mô hình được thích ứng không phù hợp cho các tác vụ tạo sinh đa ngữ yêu cầu hiểu biết về nhiều ngôn ngữ như dịch máy. Công việc tương lai cần thiết để nghiên cứu các giải pháp giảm thiểu quên lãng thảm khốc.

### 6.5 Thiết lập thí nghiệm

Chúng tôi đã sử dụng độ dài chuỗi 1024 do nhầm lẫn (thay vì 2048 như được mô tả trong Scao et al. (2022)) vì chúng tôi đã tuân theo công việc trước đây về việc thích ứng các mô hình BLOOM với các ngôn ngữ mới (Yong và Nikoulina, 2022). Tuy nhiên, về nguyên tắc, điều này không nên thay đổi kết luận chúng tôi rút ra từ nghiên cứu của mình vì không có tác vụ đánh giá nào được thực hiện trên các chuỗi dài hơn 1024 token. Kết quả thí nghiệm hậu phẫu của chúng tôi với độ dài chuỗi chính xác là 2048 (xem Phụ lục N) cũng phù hợp với kết quả được thảo luận trong Phần 4.1.

Chúng tôi đã không thực hiện thích ứng cho mô hình BLOOM lớn nhất và mô hình BLOOMZ với 176 tỷ tham số do chi phí tính toán cấm đoán. Chúng tôi để lại chúng cho công việc tương lai để khám phá thích ứng ngôn ngữ cho các mô hình ngôn ngữ với hàng trăm tỷ tham số.
