# 2309.09276.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.09276.pdf
# File size: 4145048 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SUBMIT TO IEEE TRANSACTIONS 1
MVP: Meta Visual Prompt Tuning for Few-Shot
Remote Sensing Image Scene Classification
Junjie Zhu, Yiying Li, Chunping Qiu, Ke Yang*, Naiyang Guan*, and Xiaodong Yi
Abstract —Vision Transformer (ViT) models have recently
emerged as powerful and versatile models for various visual
tasks. Recently, a work called PMF [1] has achieved promising
results in few-shot image classification by utilizing pre-trained
vision transformer models. However, PMF employs full fine-
tuning for learning the downstream tasks, leading to significant
overfitting and storage issues, especially in the remote sensing
domain. In order to tackle these issues, we turn to the recently
proposed parameter-efficient tuning methods, such as VPT [2],
which updates only the newly added prompt parameters while
keeping the pre-trained backbone frozen. Inspired by VPT,
we propose the Meta Visual Prompt Tuning (MVP) method.
Specifically, we integrate the VPT method into the meta-learning
framework and tailor it to the remote sensing domain, result-
ing in an efficient framework for Few-Shot Remote Sensing
Scene Classification (FS-RSSC). Furthermore, we introduce a
novel data augmentation strategy based on patch embedding
recombination to enhance the representation and diversity of
scenes for classification purposes. Experiment results on the FS-
RSSC benchmark demonstrate the superior performance of the
proposed MVP over existing methods in various settings, such
as various-way-various-shot, various-way-one-shot, and cross-
domain adaptation.
Index Terms —Few-shot Learning, Remote Sensing, Prompt
Tuning, Meta-learning, Parameter-efficient Fine-tuning
I. I NTRODUCTION
Few-shot remote sensing scene classification (FS-
RSSC) [3], [4] aims to classify remote sensing images
into different categories using only a few labeled examples
per category. This is a challenging but important machine
learning task for practical applications such as land use
classification and environmental monitoring, where obtaining
large-scale and high-quality labeled datasets is expensive and
time consuming. Transfer learning [5], meta-learning [6], and
metric learning [3] have been employed for FS-RSSC tasks.
These methods primarily use convolutional neural networks
(CNNs) that are typically restricted to smaller models with
fewer parameters, such as Conv4 [7], ResNet12 [8], and
ResNet18 [8].
Recently, Vision Transformers (ViT) have yielded remark-
able achievements in various visual tasks. The prospect of ap-
plying them to the field of few-shot learning is highly attractive
and holds significant appeal. For instance, PMF [1], a ViT-
based method consisting of a three-stage learning pipeline, has
made significant progress in few-shot classification tasks. PMF
pre-trained the ViT model on unsupervised external data, then
Junjie Zhu, Ke Yang, Yiying Li, Chunping Qiu, Naiyang Guan, and
Xiaodong Yi are with the National Innovation Institute of Defense Technology,
Beijing, China (*Corresponding authors: Ke Yang, Naiyang Guan. E-mail:
yangke13@nudt.edu.cn, nyguan@sina.com).
Fig. 1: MVP vs. SoTA FS-RSSC algorithms (i.e., SimC-
NAPs [9], DC-DML [10], PMF [1]): A succinct assessment
of their accuracy performance on AIFS dataset [10].
meta-trained the model on base categories, and finally fine-
tuned the model on a novel task. They showed that this simple
transformer-based pipeline yields surprisingly good perfor-
mance on standard benchmarks such as Mini-ImageNet [7],
CIFAR-FS [11], CDFSL [12], and Meta-Dataset [13].
During the three stages of learning, PMF utilizes full
fine-tuning to update network weights. However, this brings
about two significant issues, especially in remote sensing
applications. Firstly, compared to the few-shot classification
task in natural images, the remote sensing domain faces a
more severe issue of sample scarcity, and thus fully fine-
tuning ViT with a large number of weights can lead to severe
overfitting problems. Second, training a ViT model for each
remote sensing task is unfeasible due to storage limitations
on the satellite or drone platforms where the algorithm is
deployed. Our experiment results also indicate that ViT models
based on a full fine-tuning strategy exhibit lower efficacy in
solving FS-RSSC tasks.
In order to address these issues, we turn to explore a
fine-tuning method for ViT models that is suitable for FS-
RSSC tasks. A potential solution is the Parameter-Efficient
Fine-Tuning (PEFT) [14] methods, which have received con-
siderable attention in natural image recognition recently. In
the PEFT paradigm, only a small number of newly added
parameters are updated during training, while the pre-trained
backbone is kept frozen. For instance, Visual Prompt TuningarXiv:2309.09276v1  [cs.CV]  17 Sep 2023

--- PAGE 2 ---
SUBMIT TO IEEE TRANSACTIONS 2
Fig. 2: Meta Prompt Tuning framework for Remote Sensing Scene Classification. Query in the dashed box: actual image in
meta-training and pseudo-query generated via a novel data augmentation method in meta fine-tuning. ‘E’ represents original
backbone embedding tokens; ‘P’ represents newly added prompt tokens.
(VPT) [2] is a recently proposed visual PEFT method that
adds prompt tokens to the input space and only updates
the newly added parameters. The tuned parameters for each
downstream task are less than 1% of model parameters, thus
it can reduce the storage demand and effectively alleviate the
model overfitting issues of remote sensing applications.
Taking inspiration from VPT, we propose the Meta Vi-
sual Prompt Tuning (MVP) method as an effective approach
to address FS-RSSC tasks. Within the framework of meta-
learning, MVP leverages prompt tuning to adapt a pre-trained
ViT model to new tasks with limited data and computational
resources. Unlike previous methods that fine-tune the entire
ViT model, MVP only updates the newly added prompt pa-
rameters while keeping the pre-trained ViT backbone networks
fixed. Specifically, MVP embeds the prompt parameters into
a novel parameter-efficient meta-learning framework. In the
meta-training phase, MVP learns to learn a good initialization
for the newly added prompt parameters on multiple sets of
FS-RSSC source tasks. We denote the optimal initialization
prompt parameters by θ. In the meta fine-tuning phase, MVP
fine-tunes θwith a few gradient steps on the target task and
then makes predictions for remote sensing scene categories.
Additionally, we design a novel data augmentation method
for meta fine-tuning. Our method is motivated by the obser-
vation that remote sensing scene images of the same category
tend to have high consistency, which may lead to model
overfitting and poor generalization to variations in imaging
conditions. To address this issue, we propose to enhance
the diversity of remote sensing scenes by embedding image
patches of other categories into the current image, inspired by
image patch recombination research [15]. Specifically, a new
image patch recombination method based on the ViT network
is designed, which operates on image patch embeddings after
the linear projection of ViT. Moreover, we randomly select
and swap some patch embeddings of an input image with
those from other images in the same batch. Experiments show
that our data augmentation method can effectively enhance
the generalization performance to new categories in the metafine-tuning stage.
Real-world FS-RSSC tasks exhibit two key characteristics:
the number of categories and samples in new tasks varies, and
the data distribution is unpredictable, thus necessitating cross-
domain adaptation [10]. The AIFS dataset [10] meets both
criteria as the benchmark for evaluation. We thoroughly eval-
uated our proposed MVP model on the AIFS dataset through
comprehensive experiments, under various-way-various-shot,
various-way-one-shot, and cross-domain adaptation settings.
Fig. 1 shows an overview of the comparison of the results
and Fig. 2 depicts the pipeline of the MVP model. Our MVP
demonstrated superior performance on various challenging in-
domain and cross-domain benchmarks, significantly surpass-
ing the existing methods. Our contributions can be summarized
as follows:
•To the best of our knowledge, our proposed MVP is the
first study to explore Parameter-Efficient Tuning (PETun-
ing) on remote sensing applications.
•We integrate PETuning into the meta-learning framework
by employing the meta-learning paradigm to initialize
newly added prompt parameters, facilitating rapid adap-
tation to new FS-RSSC tasks.
•Our proposed MVP empowers large transformer models
to perform well in situations where there is only limited
data available, significantly alleviating the overfitting is-
sue.
•We propose a data augmentation method tailored ex-
plicitly for ViT-based FS-RSSC models to enhance their
adaptability to remote sensing scenes.
•Our MVP demonstrates exceptional performance on the
challenging FS-RSSC dataset.
II. R ELATED WORK
A. Scene Classification With Few-Shot Learning
Few-shot remote sensing scene classification (FS-RSSC) is
a challenging task due to the limited availability of annotated
data for model training. Classical methods can be roughly

--- PAGE 3 ---
SUBMIT TO IEEE TRANSACTIONS 3
divided into two categories: metric-based and meta-learning-
based [16]. Metric-based methods learn a feature space or
distance function to measure the similarity between classes [3],
[17], [18]. For instance, RS-MetaNet [17] introduces a novel
balance loss to provide better linear segmentation planes
for scenes in different categories. Another example is DLA-
MatchNet [18], which proposes an approach to automatically
discover discriminative regions. MCMNet [19] takes this a
step further by proposing a multi-scale covariance network to
optimize the manifold space.
On the other hand, meta-learning-based methods [20], [21]
aim to learn a meta-model that can quickly adapt to new
tasks with few gradient updates. For instance, MetaRS [21]
explores the use of meta-learning to improve the generalization
capability of deep neural networks (DNN) on remote sensing
scene classification with limited training data. Another exam-
ple is PTMeta [22], which applies parameter transfer to fix the
parameters in a DNN to relax the problem of training a large
number of parameters within a meta-learning framework.
A review of these methods reveals that most employ shallow
CNNs as their backbone network. While this approach can mit-
igate overfitting when training data is limited, it also constrains
further improvements in classification performance. Recently,
Vision Transformers (ViT) have demonstrated promising re-
sults in visual tasks [23], [1], and there have been efforts
to apply ViT-based approaches to large-scale remote sensing
classification [24], [25]. However, research on the application
of ViT-based methods to FS-RSSC tasks remains limited.
B. Efficient Tuning for Visual Transformer
Efficient tuning of pre-trained language models (PLMs)
has attracted much attention recently [26], [27], as it aims
to reduce the parameter size and computational cost of fine-
tuning PLMs on downstream tasks. Various methods have been
proposed to achieve efficient tuning, such as adding light-
weight adapter modules [28], [29], inserting task-specific pre-
fix tokens [30], or appending prompt tokens [31] to the PLMs
and only updating these additional parameters while keeping
the PLMs frozen. Prompt tuning has recently been extended
to visual tasks. For instance, Visual Prompt Tuning (VPT) [2]
achieves higher accuracy than full fine-tuning by updating only
1% of the model’s parameters. However, prompt tuning also
faces some challenges in few-shot learning settings, where
each task has only a small amount of labeled data available.
There is still a lack of sufficient research on how to leverage
prompt tuning for few-shot learning scenarios. In this article,
we inherit the VPT technique and propose a novel method that
advances the state-of-the-art performance on FS-RSSC tasks.
C. Data Augmentation
In both general and few-shot image classification, data
augmentation expands the number of available images per
class and generates novel classes and tasks [32]. Techniques
range from simple rotations [33] and crops [34] to more
refined strategies such as cutmix [35] and mixup [36]. Somemethods use GAN networks to emulate the target data dis-
tribution [37]. Recent research has shown that data augmen-
tation has different effects on the meta-training and meta-
testing stages of the meta-learning pipeline [38]. For instance,
increasing the number of query samples and tasks during
meta-testing improves the performance of meta-learners more
than increasing the number of support samples during meta-
training. Compared to general few-shot datasets, FS-RSSC
datasets have a smaller volume [3]. To overcome this data
scarcity problem, several methods have been proposed to
augment the training data for FS-RSSC in different ways.
For example, the quad-patch method [15] generates synthetic
samples by cutting and reassembling patches from existing
images, while the spatial vector enhancement method [39]
simulates the distribution of neighboring classes to enrich the
feature space. However, these methods do not consider the
specific properties of ViT as the backbone network. In this
article, we present a novel data augmentation method that is
customized for the structural features of ViT and the attributes
of remote sensing images and fully unleashes the potential of
the ViT architecture.
III. METHODOLOGY
A. Overview
Problem Definition. Few-shot remote sensing scene classi-
fication (FS-RSSC) is a task that requires a model to quickly
and accurately classify unseen scene images with only a
few annotated samples [3]. This task is motivated by the
challenge of domain adaptation in remote sensing images,
which are often collected from different sensors, regions,
and seasons, resulting in a large domain gap between the
source and target domains. Moreover, the number of annotated
samples varies significantly across different tasks, making it
necessary to train models that can cope with different numbers
of annotated samples. Formally, the annotated dataset, referred
to as the support set S, consists of Ccategories (way) with K
samples (shot) per category, where C∈[5,MAXWAY ]and
K∈[1,MAXSHOT ]. The model is trained on the support set
and then used to predict the categories of the query set Q,
which contains unlabeled images from the same categories as
the support set.
Meta-learning Process. Meta-learning methods have shown
promise for the FS-RSSC task. The basic meta-learning pro-
cess consists of two stages: meta-training and meta fine-tuning
[6], [21]. With the development of ViT, the PMF [1] method
enhances the meta-learning process by adding a pre-training
phase. PMF proposes a new three-stage pipeline: pre-training,
meta-training, and meta fine-tuning. The backbone network
is first pre-trained on a large-scale external dataset such as
ImageNet [40], then meta-trained on multiple source datasets,
and finally, meta fine-tuned on a target dataset with limited
annotated support samples. Note that the source and target
datasets have non-overlapping domains.
Prompt-based Meta-learning. During the three stages of
learning, PMF utilizes full fine-tuning to update the backbone
network. However, this brings about two significant chal-
lenges, especially in remote sensing applications. One chal-
lenge is how to avoid overfitting when fine-tuning the whole

--- PAGE 4 ---
SUBMIT TO IEEE TRANSACTIONS 4
Fig. 3: The network structure of the proposed Meta Visual
Prompt Tuning (MVP) model.
ViT model on limited support samples. Another challenge is
how to reduce the storage space required to store different ViT
models for different tasks. To address these issues, we propose
a meta-visual prompt tuning (MVP) framework. Our research
aims to efficiently fine-tune the pre-trained ViT backbone
models for the FS-RSSC task.
Given a pre-trained ViT backbone network, MVP introduces
a new prompt module into the input space of the pre-trained
ViT model (as shown in Fig. 3). The prompt parameters do not
need to undergo the pre-training stage as in PMF. During the
meta-training and meta fine-tuning stages, MVP only updates
the prompt parameters to fit the FS-RSSC task features, while
the whole ViT network is frozen.
In the meta-training stage of MVP, we optimize the prompt
parameters on multiple source datasets that are domain-disjoint
from the target dataset. We use a meta-learning algorithm
that mimics the FS-RSSC task by sampling episodes from
the source datasets. An episode is a few-shot learning task
that contains a support set and a query set with the same
categories but different images. We use a prototypical loss
function [41] to evaluate the classification performance of the
MVP model on the query set and update the prompt parameters
using gradient descent.
After initializing the prompt parameters in meta-training,
the MVP model is able to adapt to the target dataset of
remote sensing scenes in the meta fine-tuning phase. These
target scenes are completely new and different from the
source dataset. The MVP model performs meta fine-tuning on
auxiliary tasks that are based on support data, using a novel
data augmentation method. After meta fine-tuning, the MVP
model can classify all the remaining unlabeled query data.
B. Model Architecture
ViT Backbone Networks. The standard ViT [23] is em-
ployed as our backbone network to address the FS-RSSC
task. As input to the ViT backbone network, the image
x∈R3×H×Wis initially partitioned into mfixed-size patches
{Ij∈R3×h×w|j∈N,1≤j≤m}. Subsequently, eachpatch is projected into d-dimensional feature embedding with
positional encoding [23]:
ej
0=Embed (Ij), (1)
where ej
0∈Rd. Following this, the collection of image patch
embeddings Ei={ej
i∈Rd|i∈N,1≤i≤N}is employed
as the inputs to the ( i+1)-th Transformer layer Li+1. Formally,
the entire ViT backbone networks can be articulated as:
[CLS i,Ei] =Li([CLS i−1,Ei−1]), (2)
fθ(x) =CLS N, (3)
where CLS i−1∈Rdrefers to the class token in the input
sequence of Li. Furthermore, CLS Nin the output of the final
layer is employed as the feature representation f(x)of the
input image x. Moreover, θrepresents the model parameters
of ViT.
Prompt-based ViT Networks. In line with the VPT [2]
model and given a pre-trained ViT backbone network, a set
of prompt tokens Pi={pt
i∈Rd|t∈N,1≤t≤p}
is concatenated into the input space of the transformer layer.
Formally, the ViT architecture in Eq. 2 can be replaced by:
[CLS i,Pi,Ei] =Li([CLS i−1,Pi−1,Ei−1]), (4)
fθ′(x) =CLS N, (5)
where [xi−1,Pi−1,Ei−1]∈R(1+p+m)×d.θ′denotes the
model parameters of ViT, as well as the additional prompt
parameters θP. The network structure of our proposed model,
Meta Visual Prompt Tuning (MVP), is illustrated in Fig. 3.
Throughout the meta-training and meta fine-tuning phase,
only the newly added prompt parameters θPare updated,
while all other parameters of the ViT backbone network
remain unchanged. Therefore, a classification task involving
the prediction of label ycan be represented as follows:
arg max
θPX
xlogp 
y|fθ′(x);θP
. (6)
C. Meta Fine-Tuning Process
Meta fine-tuning for target few-shot tasks requires effec-
tively using a small amount of labeled support data and
achieving meta fine-tuning in a few steps. A common solution
is to use data augmentation to expand the support set [1].
For a few-shot task T={S, Q}, where Sis a set of labeled
support images, and Qare other unlabeled query images, one
method is to create an auxiliary task T′={S, Q′}, where the
pseudo-query Q′=augment (S)is composed of augmented
support images. This auxiliary task-based meta fine-tuning
process enhances the model’s adaptability to novel tasks by
leveraging augmented data.
In this work, we propose a novel data augmentation method
called Random Patch Recombination (RPR), which is designed
specifically for ViT-based models. As shown in Fig. 4, the RPR
method is applied after the linear projection of the data. Unlike
traditional methods [1], [38] that augment the data before feed-
ing it into the backbone networks, our method can fully utilize

--- PAGE 5 ---
SUBMIT TO IEEE TRANSACTIONS 5
Fig. 4: Random Patch Recombination: a novel data augmen-
tation method proposed in this article.
the structure of ViT. Specifically, for the patch embeddings
E={epos∈Rd|pos∈N,1≤pos≤m}of a given
image in a support set S, we select a subset of Eand denote
their corresponding positions as {pos∈Rm′,1≤m′≤m},
with a recombination rate of α. Then, we replace the selected
patches with patches from other images in the support set S
that have the same positions pos. Algorithm 1 summarizes
the process in detail.
D. Loss Functions
With the prompt-based ViT backbone networks, we discuss
how to define our training objective. We denote the feature rep-
resentation of support and query image as fθ′(xs)andfθ′(xq)
and write them as fs
θ′andfq
θ′, for simplicity. Following the
prototypical networks [41], the prototype vectors of support
data are denoted as Ω={µc∈Rd|c∈N,1≤c≤C}.
Where µc=1
|Sc|P
i:ys
i=cfsi
θ′is the prototype of class cand
|Sc|=P
i:ys
i=c1. Then the probability of a query image xq
is defined as a function of its similarity to the prototypes of
support data:
p(yq=c|xq) =exp 
−d(fq
θ′,µc)
P
c′exp 
−d(fq
θ′,µc′), (7)
where dis cosine distance. Finally, the training objective Lof
our proposed MVP is to minimize the negative log-likelihood
andL=−logp(yq=c|xq), which can be further written as:
L=1
|Sc|"
d(fq
θ′,µc) + logX
c′exp−d(fq
θ′,µc′)#
.(8)
IV. EXPERIMENTS
In this section, we first introduce the evaluation dataset and
the implementation details briefly but comprehensively. Then,
we present ablation experiments that demonstrate the signifi-
cant effectiveness of the MVP design. Finally, we contrast the
proposed MVP with state-of-the-art (SoTA) peer competitors.Algorithm 1 PyTorch pseudo code for data augmentation
# Inputs: Patch embeddings of images in a support set.
# Outputs: Recombination of input patch embeddings.
def random_recombine(emb_x, rate):
# x: [bs, pos, dim]
# rate: selection rate of \alpha
pos = emb_x.size(1)
rate = int(pos *rate)
# iterate over all image patch embeddings
for emb_i, _ in enumerate(emb_x):
select_pos = random.sample(range(0, pos), rate)
emb_x = replace(emb_x, emb_i, select_pos)
def replace(emb_x, emb_i, select_pos):
# iterate over all selected positions
bs = emb_x.size(0)
for pos_i in select_pos:
# note: probability of p(i=j) is approximately 0
emb_j = random.randint(0, bs-1)
emb_x[emb_i, pos_i, :] = emb_x[emb_j, pos_i, :]
return emb_x
A. Experimental Setup
Datasets. We evaluate our methods and SoTA algorithms
using a challenging FS-RSSC benchmark named AIFS-
DATASET [10]. This benchmark is a variant of the META-
DATASET [13] that includes a collection of remote sensing
datasets. The AIFS-DATASET is composed of two subsets:
in-domain and out-of-domain sets. The in-domain set con-
sists of six open-source datasets that do not feature re-
mote sensing scenarios: CUB-200-2011, Describable Textures,
Fungi, VGG Flower, Traffic Signs, and CIFAR100. These
six datasets are partitioned such that approximately 70%,
15%, and 15% of data are assigned to training, validation,
and testing sets, respectively. Out-of-domain set comprises
four remote-sensing datasets, namely NWPU-RESISC45, UC-
Merced, WHU-RS19, and AID, all of which are exclusively
used for testing purposes. Table. I displays the specific dataset
partitioning.
TABLE I: DATA COMPOSITION AND SPLIT OF AIFS-
DATASET [10].
Dataset Domain Total Train Val Test
CUB-200-2011
In200 140 30 30
Describable Textures 47 33 7 7
Fungi 1394 994 200 200
VGG Flower 102 71 15 16
Traffic Signs 43 30 6 7
CIFAR100 100 72 13 15
UCMerced
Out21 0 0 21
WHU-RS19 19 0 0 19
NWPU-RESISC45 45 0 0 45
AID 30 0 0 30
Benchmarking Methods. In this article, we compare our
method with several SoTA methods for FS-RSSC, such as
k-NN [42], Finetune [43], MatchingNet [7], ProtoNet [41],
fo-MAML [20], RelationNet [44], fo-Proto-MAML [20],
CNAPs [42], SimpleCNAPs [9], and DC-DML [10]. These
methods have been evaluated on AIFS-DATASET [10], a
large-scale benchmark for FS-RSSC. In addition, we use
PMF [1] as another baseline model, since it has achieved
SoTA performance on various FSL benchmarks. We follow the
official code and settings of PMF to reproduce its results on the

--- PAGE 6 ---
SUBMIT TO IEEE TRANSACTIONS 6
AIFS dataset. For the training settings, all the above methods
use the full fine-tuning method. In contrast, our proposed MVP
method uses the prompt fine-tuning method.
Implementation Details. In terms of data organization, we
follow the same setup as AIFS-DATASET [10]. Specifically,
the number of classes included in each task was randomly
selected from the range [5, MAXWAY], while the number
of support samples per class was randomly selected from the
range [1, MAXSHOT]. The sampling algorithm employed in
this process is based on uniform sampling. In the subsequent
text, we utilize MW and MS to represent MAXWAY and
MAXSHOT, respectively. To comprehensively evaluate model
performance in our experiments, we set MW to 5/10/20 and
MS to 1/5/10/20. As for the pre-training phase, consistent with
PMF [1], we also use ViT as the backbone network. ViT is
pre-trained on the ImageNet1K dataset using the classical self-
supervised mothod DINO [45] method. In our experiments, we
demonstrated results based on ViT-tiny and ViT-small. During
the meta-training and meta fine-tuning phases, the parameters
of ViT are fixed, and only the newly added prompt parameters
are updated.
B. Ablation Study
In this section, we conduct an ablation study to analyze the
effectiveness of the proposed MVP method. First, we compare
the performance of meta-visual prompt-tuning and fully fine-
tuning and present the results in Table II. Table III shows
the number of learnable parameters that need to be updated
using different fine-tuning methods. Second, we evaluate the
effectiveness and computational efficiency of the RPR data
augmentation method proposed in this article. Finally, we
investigate how the number of prompt tokens affects classi-
fication performance and computational efficiency.
Is PMF Effective in the Field of Remote Sensing? To
investigate the performance of the full fine-tuning method
in the FS-RSSC task, we conducted an ablation study on
AIFS-DATASET using the PMF framework. We compared
four settings: (1) M1, which is the pre-trained model without
any meta-learning process; (2) M2, which is the model after
only the meta-training process; (3) M3, which is the model
after only the meta-fine-tuning process; (4) M4, which is the
complete PMF [1] model. From the results in Table II, we
can draw a conclusion that PMF (full fine-tuning) improves
the performance over the pre-trained model without any meta-
learning process (comparing models M4, M3, M2 with M1).
These suggest that full fine-tuning might not be a good solution
for FS-RSSC tasks.
Effectiveness of MVP for FS-RSSC and Which Stage to
Apply MVP? To evaluate the effectiveness of the meta visual
prompt-tuning (MVP) method for FS-RSSC, we performed
ablation studies under three settings: (1) M5, which only
applied MVP for meta-training; (2) M6, which only applied
MVP for meta fine-tuning; (3) M9, which applied MVP for
both meta-training and meta fine-tuning. The results in Table
II revealed that: (1) the model with MVP that completes
either meta-training or meta fine-tuning process has better
performance than the PMF model with the same process (M5TABLE II: THE EFFECT OF UPDATING BACKBONE OR
PROMPT PARAMETERS ACROSS DIFFERENT META-
LEARNING PHASES ON THE A VERAGE CLASSIFICA-
TION ACCURACY (%) OF AIFS-DATASET UNDER MW5
MS5 AND MW10 MS10 SCENARIOS.”-” INDICATES
THAT THIS STAGE WILL NOT BE CARRIED OUT.
Training Configuration Benchmark Results
Model Meta Train Meta Finetune MW5 MS5 MW10 MS10
M1 − − 75.5 ± 0.1 76.4 ± 0.3
M2 Backbone − 75.6 ± 0.2 76.6 ± 0.2
M3 − Backbone 74.9 ± 0.1 77.2 ± 0.3
M4 Backbone Backbone 76.6 ± 0.2 78.1 ± 0.1
M5 Prompt − 79.8 ± 0.3 80.4 ± 0.2
M6 − Prompt 76.3 ± 0.2 78.6 ± 0.2
M7 Backbone Prompt 75.6 ± 0.1 78.9 ± 0.2
M8 Prompt Backbone+Prompt 78.3 ± 0.2 80.1 ± 0.4
M9 Prompt Prompt 79.6 ± 0.3 81.2 ± 0.2
vs M2, and M6 vs M3); (2) Moreover, the model with MVP
that completes either the meta-training or meta fine-tuning
process even outperforms the PMF model that completes both
processes (M5 vs M4 and M6 vs M4); (3) The complete MVP
model significantly surpasses the complete PMF model (M9
vs M4). These findings indicate that MVP is an effective and
superior method for FS-RSSC, as it can learn from a few
examples more efficiently and accurately than PMF.
TABLE III: THE NUMBER OF TRAINABLE PARAME-
TERS FOR RN18, VIT AND PROMPT VIT WITH 200
PROMPT TOKENS.
Backbone Image size Trainable Params (M)
RN18 224×224 11.28
ViT-tiny 224×224 5.52
ViT-small 224×224 21.66
ViT-base 224×224 85.79
Prompt ViT-tiny 224×224 0.46
Prompt ViT-small 224×224 0.92
Prompt ViT-base 224×224 1.84
Combining MVP and Full Fine-Tuning. To validate
whether MVP and full fine-tuning can work together to achieve
better results, we carried out experiments under three settings:
(1) M7 applies full fine-tuning in the meta-training stage and
MVP tuning in the meta fine-tuning stage; (2) M8 applies
MVP tuning in the meta-training stage and full fine-tuning for
all parameters in the meta fine-tuning stage; (3) M9 applied
MVP tuning for both meta-training and meta fine-tuning. The
results in Table II revealed that: (1) M7 does not show a
significant improvement over M4, indicating that performing
MVP only in the meta fine-tuning stage is effective but not
remarkable; (2) M8 achieves a substantial improvement over
M4, indicating that MVP can help the model obtain a better
initialization point which leads to better generalization to new
tasks; (3) M9 achieves the SoTA result, demonstrating that
employing MVP in both meta-training and meta fine-tuning
stages can attain the maximum benefit improvement.
Effectiveness of RPR Data Augmentation . This study pro-
poses a novel data augmentation method called Random Patch

--- PAGE 7 ---
SUBMIT TO IEEE TRANSACTIONS 7
(a)
 (b)
 (c)
 (d)
Fig. 5: Comparison of different data augmentation methods on various-way-various-shot learning for four out-of-domain
datasets. (a) Results on UCM dataset. (b) Results on WHU dataset. (c) Results on NWPU dataset. (d) Results on AID
dataset. None denotes no data augmentation, RPR-aug denotes Random Patch Recombination, and PMF-aug denotes the PMF
data augmentation method.
TABLE IV: RUNNING TIME ANALYSIS ON FS-RSSC
TASKS WITH VIT BACKBONE NETWORKS.
Backbone Method Avg Time (s)
ViT-tinyPMF-Aug 3.16
RPR-Aug 0.67
ViT-smallPMF-Aug 3.08
RPR-Aug 0.97
ViT-basePMF-Aug 3.03
RPR-Aug 1.61
Recombination (RPR-aug), which is detailed in Sec. III-C.
This section mainly verifies the performance of the RPR
method compared to other data augmentation methods. We use
the validated PMF [1] data augmentation method (PMF-aug)
as the main comparison method. The PMF data augmentation
method includes popular techniques such as mixup, cutmix,
color-jitter, translation, and cutout, and activates one or more
of these methods based on probability. Similarly to PMF, we
apply the proposed RPR to construct pseudo queries based
on support images, used as auxiliary tasks in the meta fine-
tuning phase. However, unlike PMF, these auxiliary tasks
are only used to update prompt parameters while freezing
the backbone network. Furthermore, we also automatically
selected the learning rate lrand the recombination rate α
for each task. We used MVP to choose the optimal lrand
αfrom the ranges lr∈[1e−4,1e−3,1e−2,0.1,0]and
α∈[0.05,0.1,0.2,0.25], and then performed meta fine-tuning
with them.
To evaluate the efficacy of RPR, we conducted a series of
experiments on four datasets of AIFS (UCM, WHU, NWPU,
and AID) for both various-way-various-shot and various-way-
one-shot scenarios and compared the results. The outcomes
of these experiments are presented in Fig. 5. Our findings
revealed that overall, RPR outperformed PMF significantly.
Specifically, when considering the results across all four tasks
(MW5 S1, MW5 MS5, MW10 S1, MW10 MS10), RPR
yielded improvements of 0.62%, 0.7%, 0.98%, and 0.68%
compared to PMF on UCM, WHU, NWPU, and AID, re-
spectively. Notably, in 1-shot learning tasks, RPR achieved
particularly significant enhancements compared to PMF, with
the MW10 S1 task indicating improvements of 1.36%, 0.97%,
1.71%, and 0.74% on UCM, WHU, NWPU, and AID, re-
spectively. In summary, the proposed RPR data augmentationtechnique outperforms the PMF method significantly in few-
shot image classification tasks, especially in 1-shot learning
scenarios.
TABLE V: COMPARISON OF PARAMETERS, A VERAGY
ACCURACY AND RUNNING TIME FOR DIFFERENT
NUMBER OF TOKENS.
Token Params Avg Acc (%) Time/Iteration (s)
10 1,150 81.9 ± 0.1 2.70
20 1,210 80.3 ± 0.3 5.87
50 1,385 81.8 ± 0.2 6.20
200 2,542 76.5 ± 0.4 7.53
Efficiency of RPR Data Augmentation. To compare the
efficiency of our RPR-aug method and the PMF-aug method,
we further conducted experiments on AIFS using different
backbone networks of ViT-tiny and ViT-small. We tested six
10-way k-shot tasks, where k∈[1,2,4,6,8,10], and repeated
1000 data augmentation experiments for each task. To ensure
the fairness of the comparison, we set the recombination
rateαof RPR-aug to a maximum value of 0.25 and used
the same model and same batch. Table IV shows the total
average running time of 1000 experiments for each task. From
Table IV, we can see that RPR-aug is 3 times faster than PMF-
aug on average, indicating that our RPR-aug method is more
efficient and more suitable for FS-RSSC tasks based on ViT.
Prompt Tokens Number. This is an important hyper-
parameter needed to tune for MVP, we carried experiment to
test the effect of the number of prompt tokens on the perfor-
mance and efficiency of the model. To conduct comparative
experiments, we designed our experimental setup following
the principle of controlling variables. We used the NWPU
dataset of AIFS as a benchmark and evaluated the performance
of the MVP model on four RS-FSSC tasks, including 5-
way 1-shot, 5-way 5-shot, 10-way 1-shot, and 10-way 10-
shot. Table V shows a comparison of the performance of the
MVP model with different numbers of prompt tokens loaded
in these four tasks. As can be seen from the figures, when
the number of tokens is set to 10, the MVP model achieves
the highest classification accuracy in most tasks. Moreover,
we observed that as the number of tokens increases, so does
the time consumption of the model. In summary, considering
the trade-off between performance accuracy and computational
efficiency, we fixed the number of prompt tokens for the MVP

--- PAGE 8 ---
SUBMIT TO IEEE TRANSACTIONS 8
TABLE VI: IN-DOMAIN AND OUT-OF-DOMAIN ACCURACY OF DIFFERENT MODELS WITH MAXWAY = 5 AND
MAXSHOT = 5. ‘†’ DENOTES THE RESULTS THAT WE RE-IMPLEMENTED.
Model Backbone TuningIn-Domain Accuracy (%) Out-of-Domain Accuracy (%)Avg
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID
k-NN [42] RN18 Full 64.1±0.6 36.8±0.4 45.9±0.6 73.8±0.5 45.9±0.5 50.1±0.5 51.8±0.6 54.6±0.7 42.1±0.5 45.9±0.5 51.1
Finetune [43] RN18 Full 57.0±1.7 38.3±1.2 45.8±1.8 73.9±1.2 50.1±1.3 54.5±1.4 63.6±1.7 76.1±1.5 55.5±1.6 60.2±1.7 57.5
MatchingNet [7] RN18 Full 49.7±0.5 36.7±0.4 38.2±0.5 66.1±0.4 52.6±0.4 46.9±0.4 54.2±0.5 65.0±0.5 46.9±0.5 51.3±0.5 50.8
ProtoNet [41] RN18 Full 44.9±0.5 33.5±0.3 35.0±0.4 56.2±0.5 35.7±0.4 42.8±0.5 51.1±0.5 54.2±0.5 42.2±0.4 44.6±0.4 44.0
fo-MAML [20] RN18 Full 59.5±0.6 38.5±0.4 44.0±0.5 70.3±0.5 49.0±0.4 49.7±0.5 52.1±0.5 60.8±0.5 44.5±0.5 50.0±0.5 51.8
RelationNet [44] RN18 Full 60.5±1.9 38.8±1.3 46.8±1.9 72.8±1.4 79.9±1.2 50.9±1.6 56.6±1.8 65.4±1.6 49.3±1.5 52.8±1.6 57.4
Proto-MAML [13] RN18 Full 47.0±1.5 33.3±1.1 35.5±1.4 60.1±1.4 36.6±1.2 41.9±1.3 50.9±1.4 52.6±1.4 41.6±1.4 44.2±1.5 44.4
CNAPs [42] RN18 Full 65.6±0.6 41.5±0.5 46.5±0.5 69.7±0.9 43.2±0.7 55.7±0.5 66.9±0.6 61.8±0.5 49.1±0.5 54.6±0.5 55.5
SimpleCNAPs [9] RN18 Full 64.0±0.5 44.9±0.9 49.8±0.5 73.1±0.4 48.5±0.4 64.8±0.5 74.6±0.5 74.5±0.5 57.0±0.5 65.2±0.5 61.6
DC-DML [10] RN18 Full 62.2±0.5 49.5±0.5 50.6±0.6 82.2±0.4 48.7±0.4 59.7±0.5 78.9±0.5 80.9±0.5 60.6±0.5 68.6±0.6 64.2
PMF-tiny † [1] ViT-t Full 83.2±0.3 61.5±0.2 55.6±0.5 83.8±0.2 43.5±0.2 54.6±0.4 83.5±0.2 88.2±0.1 75.3±0.3 77.6±0.2 70.7
PMF-small † [1] ViT-s Full 84.3±0.2 72.3±0.1 61.8±0.5 86.3±0.1 53.7±0.5 61.3±0.3 89.1±0.1 93.0±0.4 80.8±0.2 83.8±0.1 76.6
MVP-tiny (ours) ViT-t Prompt 87.9±0.3 66.1±0.2 65.9±0.4 89.1±0.4 55.9±0.3 63.5±0.2 84.1±0.3 89.6±0.4 77.2±0.2 79.6±0.2 75.9
MVP-small (ours) ViT-s Prompt 87.2±0.2 73.4±0.1 66.9±0.4 91.7±0.3 60.3±0.4 67.3±0.2 89.3±0.3 93.4±0.1 81.5±0.2 84.7±0.2 79.6
TABLE VII: IN-DOMAIN AND OUT-OF-DOMAIN ACCURACY OF DIFFERENT MODELS WITH MAXWAY = 10 AND
MAXSHOT = 10. ‘†’ DENOTES THE RESULTS THAT WE RE-IMPLEMENTED.
Model Backbone TuningIn-Domain Accuracy (%) Out-of-Domain Accuracy (%)Avg
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID
k-NN [42] RN18 Full 59.2±1.6 36.5±1.2 39.3±1.6 68.1±1.3 42.4±1.3 45.0±1.3 49.5±1.5 52.7±1.5 38.2±1.4 41.7±1.3 47.3
Finetune [43] RN18 Full 37.4±1.1 31.9±1.2 33.7±1.3 56.1±1.3 46.8±1.1 37.6±1.2 48.5±1.5 61.1±1.3 41.8±1.3 46.8±1.4 44.2
MatchingNet [7] RN18 Full 42.3±1.5 33.5±1.1 31.5±1.3 58.1±1.4 52.9±1.3 39.7±1.3 48.9±1.5 58.3±1.3 40.0±1.4 46.0±1.3 45.1
ProtoNet [41] RN18 Full 35.4±1.2 29.1±1.1 24.2±1.1 42.1±1.3 28.4±1.0 34.8±1.2 42.3±1.3 45.9±1.3 33.9±1.3 35.1±1.2 35.1
fo-MAML [20] RN18 Full 51.3±1.5 35.6±1.3 37.5±1.4 61.5±1.3 43.2±1.2 40.3±1.3 50.1±1.4 58.2±1.4 39.2±1.4 44.8±1.3 46.2
RelationNet [44] RN18 Full 44.9±1.5 33.1±1.2 35.7±1.7 58.3±1.5 73.4±1.3 36.8±1.4 49.1±1.5 59.8±1.3 40.2±1.4 46.0±1.5 47.7
Proto-MAML [13] RN18 Full 37.5±1.4 28.3±1.0 27.0±1.0 48.3±1.4 28.4±1.0 33.0±1.2 39.3±1.4 41.6±1.5 32.2±1.1 33.6±1.3 34.9
CNAPs [42] RN18 Full 59.5±0.5 44.4±0.4 46.6±0.5 70.1±0.4 55.8±0.5 52.8±0.5 61.7±0.5 60.6±0.5 44.3±0.5 52.0±0.5 54.8
SimpleCNAPs [9] RN18 Full 64.6±0.5 40.4±0.4 47.8±0.6 68.6±0.4 54.3±0.5 51.6±0.5 62.1±0.5 61.5±0.5 43.6±0.5 52.7±0.5 54.7
DC-DML [10] RN18 Full 56.3±0.5 45.7±0.5 43.1±0.6 71.9±0.4 47.5±0.4 53.7±0.5 69.4±0.6 72.2±0.5 49,7±0.6 56.8±0.6 57.4
PMF-tiny † [1] ViT-t Full 85.9±0.1 67.6±0.4 54.4±0.4 89.1±0.1 51.5±0.4 60.2±0.6 86.2±0.2 90.4±0.5 78.2±0.3 80.5±0.3 74.4
PMF-small † [1] ViT-s Full 86.1±0.1 75.5±0.1 60.4±0.4 87.2±0.1 58.3±0.3 62.7±0.4 89.8±0.3 93.8±0.2 81.8±0.1 84.8±0.1 78.1
MVP-tiny (ours) ViT-t Prompt 89.1±0.3 68.9±0.1 66.0±0.3 89.0±0.2 56.9±0.3 64.4±0.4 86.8±0.1 90.0±0.4 78.6±0.3 82.1±0.2 77.3
MVP-small (ours) ViT-s Prompt 88.8±0.2 76.2±0.2 67.4±0.1 92.4±0.3 64.9±0.4 69.3±0.3 89.9±0.2 94.4±0.3 83.2±0.4 85.1±0.1 81.2
TABLE VIII: IN-DOMAIN AND OUT-OF-DOMAIN ACCURACY OF DIFFERENT MODELS WITH MAXWAY = 20 AND
MAXSHOT = 20. ‘†’ DENOTES THE RESULTS THAT WE RE-IMPLEMENTED.
Model Backbone TuningIn-Domain Accuracy (%) Out-of-Domain Accuracy (%)Avg
CUB Textures Fungi Flower Signs CIFAR UCM WHU NWPU AID
k-NN [42] RN18 Full 49.7±0.6 45.4±0.4 32.7±0.5 72.5±0.4 48.5±0.5 44.9±0.5 49.2±0.5 53.8±0.5 35.7±0.5 40.5±0.5 47.3
Finetune [43] RN18 Full 33.3±1.0 40.4±0.7 27.5±0.7 54.6±1.0 47.8±1.0 37.0±1.0 46.9±1.0 56.1±1.0 37.1±1.0 41.4±1.0 42.2
MatchingNet [7] RN18 Full 38.5±0.6 41.8±0.4 27.4±0.4 63.9±0.5 58.8±0.5 40.5±0.5 44.7±0.6 54.6±0.5 35.9±0.5 40.6±0.5 44.7
ProtoNet [41] RN18 Full 30.8±0.6 40.0±0.4 20.1±0.4 45.2±0.6 31.5±0.4 32.8±0.6 38.2±0.6 41.7±0.7 29.4±0.6 30.5±0.6 34.1
fo-MAML [20] RN18 Full 45.2±0.6 42.9±0.4 31.4±0.5 63.7±0.5 48.1±0.5 39.5±0.6 42.4±0.6 48.8±0.6 32.4±0.5 36.3±0.5 43.1
RelationNet [44] RN18 Full 35.3±0.6 38.0±0.4 27.6±0.5 65.5±0.5 85.7±0.3 34.3±0.5 40.9±0.6 50.7±0.6 31.9±0.5 37.5±0.6 44.7
Proto-MAML [13] RN18 Full 35.4±1.1 39.4±0.7 22.7±0.7 49.8±1.1 33.8±1.0 34.0±1.0 36.4±1.1 40.5±1.1 28.4±1.0 30.7±1.0 35.1
CNAPs [42] RN18 Full 54.2±0.6 48.9±0.4 41.5±0.6 77.0±0.4 62.7±0.5 51.2±0.6 57.8±0.5 52.5±0.6 38.8±0.6 44.2±0.6 52.9
SimpleCNAPs [9] RN18 Full 54.9±0.6 47.8±0.4 41.7±0.6 74.2±0.4 71.1±0.4 48.0±0.6 55.6±0.6 50.3±0.7 40.0±0.6 45.5±0.6 52.9
DC-DML [10] RN18 Full 48.6±0.7 54.5±0.4 36.0±0.6 76.9±0.4 51.3±0.4 53.0±0.6 62.8±0.6 63.0±0.6 43.3±0.7 48.6±0.7 53.8
PMF-tiny † [1] ViT-t Full 84.4±0.2 69.1±0.3 49.5±0.6 84.5±0.6 40.8±0.4 52.7±0.5 81.3±0.2 86.2±0.1 70.3±0.4 74.9±0.4 69.4
PMF-small † [1] ViT-s Full 84.1±0.3 80.3±0.4 55.7±0.2 85.6±0.2 74.7±0.3 60.4±1.5 88.2±0.3 93.5±0.1 77.8±0.2 83.0±1.1 78.3
MVP-tiny (ours) ViT-t Prompt 87.2±0.3 72.2±0.3 61.3±0.4 89.9±0.4 64.4±0.5 63.4±0.5 85.6±0.2 90.1±0.1 76.9±0.3 79.9±0.3 77.1
MVP-small (ours) ViT-s Prompt 88.9±0.2 80.9±1.4 67.1±0.3 94.3±0.6 78.7±0.5 67.1±0.4 89.7±0.1 93.7±0.2 81.8±0.4 84.3±0.2 82.1
model at 10 in this article and applied this setting consistently
across all tasks.
C. Comparison With SoTA Methods
Main Results and Comparisons. We present the ex-
perimental results obtained under three distinct evaluation
benchmarks, where MW and MS were set to 5, 10, and
20, respectively, in Tables VI-VIII. Comparing these results,
we can deduce that MVP outperforms the SoTA methods
in terms of average accuracy. Specifically, our MVP methodachieves significant improvements over the strong baseline
PMF method, with average increases of 3.0%, 3.1%, and 3.8%
at MW and MS of 5/10/20. These findings support our view
that prompt-tuning techniques are better suited for few-shot
classification tasks than full-tuning techniques.
In the context of in-domain tasks, MVP exhibits consid-
erable advantages over other SoTA methods, particularly in
fine-grained evaluation benchmarks such as CUB, Fungi, and
Flower datasets. In these datasets, MVP surpasses all other
listed methods in terms of performance. This observation
further highlights the superior capabilities of the MVP tech-

--- PAGE 9 ---
SUBMIT TO IEEE TRANSACTIONS 9
nology in fine-grained few-shot classification tasks. Notably,
when compared with the PMF method, the MVP approach
demonstrates the most prominent advantages in the Fungi
dataset at MW and MS of 5/10/20, with improvements of
5.1%, 7%, and 11.4%, respectively.
Regarding out-of-domain tasks, our analysis indicates that
MVP generally outperforms the second-best PMF method.
This trend is particularly noticeable in the NWPU dataset,
where MVP improves by 0.7%, 1.4%, and 4% at MW and
MS of 5/10/20. Moreover, These results suggest that MVP
has better generalization ability when dealing with new unseen
FS-RSSC tasks.
Analysis of Model Parameters and Performance. By
utilizing Tables VI-VIII and Table III, we can perform a com-
prehensive analysis of how model parameters impact the per-
formance and accuracy of our model. Our study demonstrates
that, with the use of a meta-based prompt-tuning framework,
ViT-tiny achieves a significant improvement in performance
compared to RN18 despite having similar parameter counts.
Specifically, in the out-of-domain FS-RSSC task, our MVP-
tiny method shows an average accuracy increase of 19.2%
over the DC-DML [10] method based on RN18 across all
three evaluation benchmarks. Furthermore, while ViT-small
possesses four times as many parameters as ViT-tiny, it does
not exhibit a significant increase in accuracy for the FS-
RSSC task. Nonetheless, the MVP-small method only shows
an average precision increase of 3.9% compared to MVP-
tiny across all three evaluation benchmarks. Therefore, our
results suggest that deploying applications based on MVP-tiny
can provide a more balanced trade-off between efficiency and
performance.
The Impact of Domain Shift on Classification Accuracy.
The cross-domain results shown in Table VII and Table VIII
indicated that all algorithms suffered from a varying degree of
accuracy degradation when dealing with the MW20 MS20 task
compared to the MW10 MS10 task. This phenomenon was
absent in the in-domain scenarios. This suggested that domain
shift, especially when the number of categories increased, had
a significant effect on classification accuracy. However, the
results also revealed that the MVP model was more robust to
domain shift. In particular, the MVP achieved high accuracy on
out-of-domain datasets while effectively reducing the effect of
domain shift. On the UCM, WHU, NWPU, and AID datasets,
MVP-small and MVP-tiny showed an average accuracy drop
of 1.34% and 1.5%, respectively, while PMF-small and PMF-
tiny showed an average drop of 1.95% and 5.65%, and DC-
DML showed an average drop of 7.6%. Hence, the MVP
model outperformed other models in cross-domain FS-RSSC
tasks, especially when the number of categories was large.
Results for One-Shot Learning. Fig. 6 presents the results
of our model on the one-shot learning task with one support
image per category. Our MVP method outperforms the strong
baseline PMF method in all one-shot Learning tasks. Specif-
ically, when using ViT-tiny as the backbone network, MVP-
tiny achieves better results than PMF-tiny in all ten evaluation
tasks. Averaging all evaluation tasks, MVP-tiny improves by
6.64%, 6.84%, and 3.80% compared to PMF-tiny when MW
and MS are 5/10/20 respectively; MVP-small improves by
(a)
(b)
Fig. 6: Results of the PMF [1] and our MVP model on various-
way-one-shot learning. (a) Results with ViT-tiny backbone
network. (b) Results with ViT-small backbone network.
3.46%, 3.94%, and 3.49% compared to PMF-small when
MW and MS are 5/10/20 respectively. Averaging all out-
of-domain tasks, MVP-tiny improves by 2.44%, 3.07%, and
2.73% compared to PMF-tiny in these three tasks respectively;
MVP-small improves by 0.82%, 0.62%, and 0.12% compared
to PMF-small in these three tasks respectively. These results
demonstrate that our MVP method has better generalization
ability in the challenging 1-shot learning task.
Qualitative Analysis of Few-Shot Classification. The aim
of this section is to examine the disparities between the outputs
generated by the MVP and PMF models. Based on the in-
domain results presented in Tables VI to VIII, it was observed
that the MVP model demonstrated a notable advantage in
processing fine-grained classification tasks. This led us to
hypothesize that the transfer of fine-grained classification
abilities may contribute to the superior performance of the
MVP model in downstream tasks. To evaluate this hypothesis,
an experiment was conducted in which a 10-way 5-shot task
was extracted from the NWPU dataset, distinct from the AIFS-
in-domain dataset utilized for training the models. The outputs
generated by each model were subsequently compared for
every query image. A meticulous examination was performed
on categories accurately predicted by the MVP model but
mispredicted by the PMF model. Notably, when comparing
two analogous categories: “dense residential” and “medium
residential”, it was determined that the MVP model achieved a
significantly higher average accuracy than the PMF model. To
further scrutinize this phenomenon, a 2-way 1-shot experiment
was designed. As depicted in Fig. 7, it is apparent that the
MVP model is better equipped to handle fine-grained clas-
sification problems. In summary, these observations indicate

--- PAGE 10 ---
SUBMIT TO IEEE TRANSACTIONS 10
(a)
 (b)
(c)
 (d)
Fig. 7: Qualitative results of few-shot classification for PMF
and ours MVP.
that the MVP model possesses the ability to learn more
discriminative and transferable features, thereby facilitating
the effective transfer of fine-grained classification abilities to
downstream tasks.
V. C ONCLUSION
In this article, we focused on the challenging and practical
scenario of few-shot remote sensing scene classification (FS-
RSSC), where the goal is to classify remote sensing images
into different categories using only a few labeled examples for
each category. To tackle this problem, we proposed a novel and
efficient method called Meta Visual Prompt Tuning (MVP)
that leverages prompt tuning and meta-learning to adapt a
pre-trained visual transformer (ViT) model to new tasks with
minimal data and resources. Specifically, MVP has three main
components: (1) prompt tuning, a parameter-efficient fine-
tuning technique that updates only the newly added prompt
parameters and freezes the rest of the model, which reduces
the storage demand and mitigates the overfitting risk; (2) meta-
learning, a fast adaptation technique that learns to initialize
the prompt parameters from multiple source tasks and rapidly
adapts them to new tasks with only a few gradient steps, which
facilitates cross-domain adaptation; and (3) data augmentation,
a novel technique that operates on the patch embeddings of
the input tokens of transformer blocks to enhance the scene
representation and diversity. We evaluated MVP on a realistic
cross-domain FS-RSSC benchmark dataset and demonstrated
its superior performance over existing methods. MVP achieved
especially remarkable results on vary-way, vary shot, and one-
shot tasks, which are more challenging and relevant for real-
world applications. Our work paved the way for applying ViTmodels to FS-RSSC tasks and provided a solution that is
suitable for the deployment platform of FS-RSSC algorithms.
In the future, we expect to extend the training data domain
to include more data distributions, which may further improve
the accuracy and robustness of prompt tuning-based methods
for FS-RSSC tasks, especially for enhancing cross-domain
performance.
REFERENCES
[1] Hu Shell Xu, Li Da, St ¨uhmer Jan, Kim Minyoung, Hospedales Timothy
M, Pushing the limits of simple pipelines for few-shot learning: External
data and fine-tuning make a difference, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
9068–9077.
[2] Jia Menglin, Tang Luming, Chen Bor-Chun, Cardie Claire, Belongie
Serge, Hariharan Bharath, Lim Ser-Nam, Visual prompt tuning, in:
Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXXIII, Springer, 2022,
pp. 709–727.
[3] Zhenqi Cui, Wang Yang, Li Chen, Haifeng Li, Mkn: Metakernel net-
works for few shot remote sensing scene classification, IEEE TRANS-
ACTIONS ON GEOSCIENCE AND REMOTE SENSING 60.
[4] Zhu Xiao Xiang, Tuia Devis, Mou Lichao, Xia Gui-Song, Zhang
Liangpei, Xu Feng, Fraundorfer Friedrich, Deep learning in remote
sensing: A comprehensive review and list of resources, IEEE geoscience
and remote sensing magazine 5 (4) (2017) 8–36.
[5] Pires de Lima Rafael, Marfurt Kurt, Convolutional neural network for
remote-sensing scene classification: Transfer learning analysis, Remote
Sensing 12 (1) (2019) 86.
[6] Li Yong, Shao Zhenfeng, Huang Xiao, Cai Bowen, Peng Song, Meta-
fseo: A meta-learning fast adaptation with self-supervised embedding
optimization for few-shot remote sensing scene classification, Remote
Sensing 13 (14) (2021) 2776.
[7] Vinyals Oriol, Blundell Charles, Lillicrap Tim, Wierstra Daan, et al.,
Matching networks for one shot learning, in: nips, 2016, pp. 3630–3638.
[8] He Kaiming, Zhang Xiangyu, Ren Shaoqing, Sun Jian, Deep residual
learning for image recognition, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 770–778.
[9] Bateni Peyman, Goyal Raghav, Masrani Vaden, Wood Frank, Sigal
Leonid, Improved few-shot visual classification, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 14493–14502.
[10] Li Lingjun, Yao Xiwen, Cheng Gong, Han Junwei, Aifs-dataset for few-
shot aerial image scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2022) 1–11.
[11] Bertinetto Luca, Henriques Joao F, Torr Philip HS, Vedaldi Andrea,
Meta-learning with differentiable closed-form solvers, arXiv preprint
arXiv:1805.08136.
[12] Guo Yunhui, Codella Noel C, Karlinsky Leonid, Codella James V, Smith
John R, Saenko Kate, Rosing Tajana, Feris Rogerio, A broader study of
cross-domain few-shot learning, in: European Conference on Computer
Vision, Springer, 2020, p. 124–141.
[13] Triantafillou Eleni, Zhu Tyler, Dumoulin Vincent, Lamblin Pascal,
Evci Utku, Xu Kelvin, Goroshin Ross, Gelada Carles, Swersky Kevin,
Manzagol Pierre-Antoine, et al., Meta-dataset: A dataset of datasets for
learning to learn from few examples, arXiv preprint arXiv:1903.03096.
[14] Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan
Jared D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry
Girish, Askell Amanda, et al., Language models are few-shot learners,
Advances in neural information processing systems 33 (2020) 1877–
1901.
[15] Gong Maoguo, Li Jianzhao, Zhang Yourun, Wu Yue, Zhang Mingyang,
Two-path aggregation attention network with quad-patch data augmenta-
tion for few-shot scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2022) 1–16.
[16] Cheng Gong, Cai Liming, Lang Chunbo, Yao Xiwen, Chen Jinyong, Guo
Lei, Han Junwei, Spnet: Siamese-prototype network for few-shot remote
sensing image scene classification, IEEE Transactions on Geoscience
and Remote Sensing 60 (2021) 1–11.
[17] Li Haifeng, Cui Zhenqi, Zhu Zhiqing, Chen Li, Zhu Jiawei, Huang
Haozhe, Tao Chao, Rs-metanet: Deep meta metric learning for few-shot
remote sensing scene classification, arXiv preprint arXiv:2009.13364.

--- PAGE 11 ---
SUBMIT TO IEEE TRANSACTIONS 11
[18] Li Lingjun, Han Junwei, Yao Xiwen, Cheng Gong, Guo Lei, Dla-
matchnet for few-shot remote sensing image scene classification, IEEE
Transactions on Geoscience and Remote Sensing 59 (9) (2020) 7844–
7853.
[19] Zeng Qingjie, Geng Jie, Task-specific contrastive learning for few-shot
remote sensing image scene classification, ISPRS Journal of Photogram-
metry and Remote Sensing 191 (2022) 143–154.
[20] Finn Chelsea, Abbeel Pieter, Levine Sergey, Model-agnostic meta-
learning for fast adaptation of deep networks, in: Proc. Int. Conf.
Machine Learning, 2017, pp. 1126–1135.
[21] Zhang Pei, Bai Yunpeng, Wang Dong, Bai Bendu, Li Ying, A meta-
learning framework for few-shot classification of remote sensing scene,
in: ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 4590–4594.
[22] Ma Chenhui, Mu Xiaodong, Zhao Peng, Yan Xin, Meta-learning based
on parameter transfer for few-shot classification of remote sensing
scenes, Remote Sensing Letters 12 (6) (2021) 531–541.
[23] Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn
Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer
Matthias, Heigold Georg, Gelly Sylvain, et al., An image is worth
16x16 words: Transformers for image recognition at scale, arXiv preprint
arXiv:2010.11929.
[24] Roy Swalpa Kumar, Deria Ankur, Hong Danfeng, Rasti Behnood, Plaza
Antonio, Chanussot Jocelyn, Multimodal fusion transformer for remote
sensing image classification, arXiv preprint arXiv:2203.16952.
[25] Bi Meiqiao, Wang Minghua, Li Zhi, Hong Danfeng, Vision transformer
with contrastive learning for remote sensing image scene classification,
IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 16 (2022) 738–749.
[26] Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina, Bert:
Pre-training of deep bidirectional transformers for language understand-
ing, arXiv preprint arXiv:1810.04805.
[27] Logan IV Robert L, Bala ˇzevi´c Ivana, Wallace Eric, Petroni Fabio,
Singh Sameer, Riedel Sebastian, Cutting down on prompts and param-
eters: Simple few-shot learning with language models, arXiv preprint
arXiv:2106.13353.
[28] Rebuffi Sylvestre-Alvise, Bilen Hakan, Vedaldi Andrea, Efficient
parametrization of multi-domain deep neural networks, in: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 8119–8127.
[29] Zhang Jeffrey O, Sax Alexander, Zamir Amir, Guibas Leonidas, Malik
Jitendra, Side-tuning: a baseline for network adaptation via additive side
networks, in: Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, Springer,
2020, pp. 698–714.
[30] Li Xiang Lisa, Liang Percy, Prefix-tuning: Optimizing continuous
prompts for generation, arXiv preprint arXiv:2101.00190.
[31] Lester Brian, Al-Rfou Rami, Constant Noah, The power of scale for
parameter-efficient prompt tuning, arXiv preprint arXiv:2104.08691.
[32] Shorten Connor, Khoshgoftaar Taghi M, A survey on image data
augmentation for deep learning, Journal of big data 6 (1) (2019) 1–48.
[33] Gidaris Spyros, Singh Praveer, Komodakis Nikos, Unsupervised repre-
sentation learning by predicting image rotations, ICLR.
[34] Zhang Chi, Cai Yujun, Lin Guosheng, Shen Chunhua, Deepemd: Few-
shot image classification with differentiable earth mover’s distance and
structured classifiers, in: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 2020, pp. 12203–12213.
[35] Yun Sangdoo, Han Dongyoon, Oh Seong Joon, Chun Sanghyuk, Choe
Junsuk, Yoo Youngjoon, Cutmix: Regularization strategy to train strong
classifiers with localizable features, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 6023–6032.
[36] Kim Jang-Hyun, Choo Wonho, Song Hyun Oh, Puzzle mix: Exploiting
saliency and local statistics for optimal mixup, in: International Confer-
ence on Machine Learning, PMLR, 2020, pp. 5275–5285.
[37] Li Kai, Zhang Yulun, Li Kunpeng, Fu Yun, Adversarial feature halluci-
nation networks for few-shot learning, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
13470–13479.
[38] Ni Renkun, Goldblum Micah, Sharaf Amr, Kong Kezhi, Goldstein Tom,
Data augmentation for meta-learning, in: International Conference on
Machine Learning, PMLR, 2021, pp. 8152–8161.
[39] Yang Shuo, Liu Lu, Xu Min, Free lunch for few-shot learning: Distri-
bution calibration, arXiv preprint arXiv:2101.06395.
[40] Deng Jia, Dong Wei, Socher Richard, Li Li-Jia, Li Kai, Fei-Fei Li,
Imagenet: A large-scale hierarchical image database, in: 2009 IEEE
conference on computer vision and pattern recognition, Ieee, 2009, pp.
248–255.[41] Snell Jake, Swersky Kevin, Zemel Richard, Prototypical networks for
few-shot learning, in: nips, 2017, pp. 4077–4087.
[42] Requeima James, Gordon Jonathan, Bronskill John, Nowozin Sebastian,
Turner Richard E, Fast and flexible multi-task classification using
conditional neural adaptive processes, Advances in Neural Information
Processing Systems 32.
[43] Yosinski Jason, Clune Jeff, Bengio Yoshua, Lipson Hod, How trans-
ferable are features in deep neural networks?, Advances in neural
information processing systems 27.
[44] Yang Flood Sung Yongxin, Zhang Li, Xiang Tao, Torr Philip HS,
Hospedales Timothy M, Learning to compare: Relation network for few-
shot learning, in: cvpr, 2018, pp. 1199–1208.
[45] Caron Mathilde, Touvron Hugo, Misra Ishan, J ´egou Herv ´e, Mairal
Julien, Bojanowski Piotr, Joulin Armand, Emerging properties in self-
supervised vision transformers, in: Proceedings of the IEEE/CVF inter-
national conference on computer vision, 2021, pp. 9650–9660.
