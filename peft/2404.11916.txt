# 2404.11916.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2404.11916.pdf
# File size: 669180 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SKELETON : A New Framework for Accelerating Language Models via
Task Neuron Localized Prompt Tuning
Nakyeong Yang1, Jiwon Moon1, Junseok Kim2, Yunah Jang1and Kyomin Jung1
1Seoul National University,2Pohang University of Science and Technology
{yny0506, wldnjs913, vn2209, kjung}@snu.ac.kr
{junseokkim00}@postech.ac.kr
Abstract
Prompt tuning methods have shown compara-
ble performance to general training methods
as parameter-efficient fine-tuning (PEFT) meth-
ods in various natural language understanding
tasks. However, existing prompt tuning meth-
ods still utilize the entire model architecture
even when solving a specific task, which pre-
vents them from accelerating inference speed
during the application procedure. In this pa-
per, we propose a novel prompt tuning frame-
work called SKELETON to efficiently utilize a
language model in terms of memory and time
complexity for solving various tasks, retaining
only task-relevant neurons by using an explain-
ability method. From our framework, we can
efficiently solve various tasks by using only
task-relevant neurons and prepending adequate
task-specific prompt tokens with only a single
language model. Experiments reveal that our
method significantly enhances inference effi-
ciency (at most Ã—1.73speed up) for various
widely used benchmarks, showing compara-
ble performance to the prompt tuning method.
Moreover, our method is applicable across var-
ious transformer-based architectures, confirm-
ing its practicality and scalability.
1 Introduction
With the success of parameter-efficient fine-tuning
(PEFT) methods, pre-trained language models per-
form various tasks by adjusting only small num-
bers of parameters. Among PEFT methods, prompt
tuning methods (Lester et al., 2021; Li and Liang,
2021; Liu et al., 2021) are widely used to solve
multiple tasks since they require only a few tun-
able tokens to be prepended to the input tokens
while freezing the original parameters of a lan-
guage model (Chan et al., 2023; Huang et al., 2024).
This strategy is memory efficient, as it enables a
model to solve various tasks with only a few task-
specific prompt tokens while retaining the original
parameters. However, we can raise one research
ğ’‚ğŸ,ğ’‚ğŸ,â‹¯ ğ‘·ğ‘¨
ğ’ƒğŸ,ğ’ƒğŸ,â‹¯ ğ‘·ğ‘©
ğ’„ğŸ,ğ’„ğŸ,â‹¯ ğ‘·ğ‘ªTask Prompt Task Neurons
ğ‘·ğ‘¨
Language model
Task A Task C
Task -localized Network A
Task A InputTask BAd-hoc Task -localized Network Construction and InferenceSKELETON Framework Output
ğ’‚ğŸğ’‚ğŸğ’‚ğ’
â¨ğ’…ğ’‚ğ’•ğ’‚ğ‘¨ğ’ƒğŸğ’ƒğŸğ’ƒğ’
ğ’„ğŸğ’„ğŸğ’„ğ’ŒTask -localized Network B
ğ‘·ğ‘©Task B Input
â¨ğ’…ğ’‚ğ’•ğ’‚ğ‘©ğ‘·ğ‘ªTask C Input
â¨ğ’…ğ’‚ğ’•ğ’‚ğ‘ªTask -localized Network CFigure 1: SKELETON Framework Overview. Our
framework activates only task-localized paths of lan-
guage models in the prompt tuning procedure. Each
color (i.g., red, blue, and yellow) corresponds to a spe-
cific task; we obtain a task-specific prompt and neuron
indices after executing our framework. Gray-colored
paths in the figure represent task-irrelevant paths that
are never used during training and inference, thereby
improving inference speed.
question regarding this task-specific trait of prompt
tuning:
"Leveraging prompt tuning, could we condense
task knowledge into a few task neurons in a frozen
model to efficiently utilize it?"
Existing prompt tuning frameworks are limited
by the need to use the full original model architec-
ture during inference. In fact, these methods may
require additional inference time, as the prepended
prompt tokens introduce further computational
overhead. Therefore, existing methods (Ma et al.,
2022; Liang et al., 2023) have attempted to accel-
erate the inference speed of prompt-tuned models
by pruning unnecessary prompt tokens. However,
they have only marginally improved inference effi-
ciency, as the number of pruned tokens is a small
fraction ( â‰¤0.1%) of all the parameters of a lan-arXiv:2404.11916v2  [cs.CL]  17 Oct 2024

--- PAGE 2 ---
guage model (Lester et al., 2021).
Prior studies have attempted to investigate and
analyze task-relevant neurons in language models
to explain the language modelsâ€™ behavior (Pani-
grahi et al., 2023; Yang et al., 2024). Specifically,
Panigrahi et al. (2023) has suggested training new
parameters, which mask the original parameters of
language models to determine task neurons. Yang
et al. (2024) has utilized an explainability method
to determine task-relevant neurons and reveal that
some task-relevant neurons significantly affect the
outputs of specific tasks (i.e., natural language un-
derstanding, social bias). However, while they have
introduced a method to identify task-relevant neu-
rons, they fall short of proposing a practical ap-
proach to efficiently utilize those neurons during
training and inference.
To address this challenge, we propose a novel
training framework, SKELETON , designed to en-
hance the efficiency of language models by lever-
aging only task-relevant neurons for solving var-
ious tasks. Specifically, we use attribution (Yang
et al., 2023, 2024), an explainability method, to
identify and utilize only task-relevant neurons in
the prompt tuning procedure. Our framework re-
quires only a few prompt tokens to be updated
and freezes the original parameters of a language
model, as it adopts the prompt tuning method. Fur-
thermore, SKELETON dramatically accelerates the
inference speed of language models since it utilizes
only task-relevant neurons. For example, in order
to solve a specific task (e.g., sentiment analysis,
natural language inference), our method prepends
prompt tokens related to the task and activates only
the task-relevant neurons to solve the task.
We demonstrate our method in various widely
used natural language understanding benchmarks
and reveal that our method significantly outper-
forms existing PEFT methods in inference effi-
ciency without sacrificing the original performance
of each task. Surprisingly, our method improves
the inference speed up to Ã—1.73with 65% of the
parameters pruning rate. We additionally conduct
various analyses to offer a guideline for using our
new prompt tuning framework, such as investigat-
ing which types of layers are crucial for processing
task knowledge in a language model. Furthermore,
our prompt tuning framework is practical and scal-
able since it is applicable to any transformer-based
architecture. In summary, this work makes the fol-
lowing contributions:
â€¢We first define task neuron and propose TaskNeuron Localized Prompt Tuning ( SKELETON ),
a practical training framework for accelerating
the inference speed of a language model for
various tasks.
â€¢We reveal that SKELETON successfully en-
hances the inference speed of each task without
compromising the task performance.
2 Backgrounds
2.1 Parameter-efficient Fine-tuning
Parameter-efficient Fine-tuning (PEFT) methods
have emerged as a standard of efficient training
methods for language models (Lester et al., 2021;
Li and Liang, 2021; Liu et al., 2021; Hu et al.,
2021; Poth et al., 2023). Among them, prompt
tuning methods (Lester et al., 2021; Li and Liang,
2021; Liu et al., 2021) have been adopted for var-
ious NLP tasks (Wang et al., 2023; Bansal et al.,
2023). Prompt tuning methods only update prompt
tokens while maintaining the original parameters
of a language model; thus, this characteristic makes
the training process memory-efficient and effective
in injecting task knowledge into a language model.
However, they still face a limitation as they are
not parameter-efficient during the inference pro-
cess. Therefore, existing methods (Ma et al., 2022;
Liang et al., 2023) propose prompt token selec-
tion methods to enhance the inference speed of
prompt-tuned language models. Nevertheless, they
have only marginally enhanced inference efficiency
since the number of pruned tokens is only a tiny
proportion of the entire number of input tokens.
2.2 Task Knowledge Localization
In spite of the significant performance of language
models, it remains challenging to accurately investi-
gate the role of each parameter in language models
in dealing with a specific task. Therefore, existing
studies have attempted to identify task-specific neu-
rons in executing a specific task (Panigrahi et al.,
2023; Yang et al., 2023, 2024). Panigrahi et al.
(2023); Yang et al. (2023, 2024) have studied to
localize neurons for performing a specific task by
clarifying the role of each parameter in a language
model. Panigrahi et al. (2023) has suggested a
training-based task knowledge localization method,
model grafting , which localizes a task knowledge
by training new parameters to mask original param-
eters. Although it has effectively identified task
neurons, it has required excessive additional stor-
age for newly trained masking parameters, which

--- PAGE 3 ---
is equivalent to the number of model parameters.
Yang et al. (2023, 2024) has illuminated task neu-
rons by utilizing the attribution technique (Shriku-
mar et al., 2016), an explainability method that de-
rives the importance of each feature when solving a
specific task. Yang et al. (2023, 2024) has verified
that the attribution effectively detects task neurons
for solving a specific task and proposed a task neu-
ron detection method applicable to language mod-
eling tasks. However, Yang et al. (2023, 2024) has
a limitation in just pruning the task neurons; if we
prune a large number of neurons, the performance
of a specific task can be significantly degraded.
Furthermore, the prior study has only proposed the
method for pruning task neurons and is far from
proposing a practical method to improve the in-
ference speed of task-localized language models.
The architecture of transformer variants consists
of complex neural computation processes; thus,
satisfying the integrity of dimension dependency
for adjacent networks while pruning the weights
is challenging. We adopt the attribution method
(Yang et al., 2023, 2024) and propose a new practi-
cal framework for accelerating a language model,
where the framework utilizes only task neurons
while training a language model via prompt tuning,
resolving the challenges in pruning networks in
transformer variants.
3 Problem Definition
3.1 Prompt Tuning
We adopt the prompt tuning method to propose a
new framework for accelerating a language model.
Formally, suppose we have a function PÎ¸(y|x) =QK
k=1PÎ¸(yk|x, y 1, ..., y kâˆ’1)that represents a lan-
guage model, where Î¸is parameters of the language
model. Prompting is the method of adding extra
information to the model to condition during its
generation of y. Normally, prompting is achieved
by adding a series of tokens PÏ„âˆˆRlÃ—dto the in-
putxasPÎ¸(y|[PÏ„;x]), where landdare token
length and feature dimension, and PÏ„is trainable
parameters for a specific task Ï„. In the prompt
tuning process, we update only PÏ„and get Pâ€²
Ï„to
maximize the likelihood of yusing gradient de-
scent while freezing the parameters of the language
model Î¸. After the prompt tuning, we get the new
conditional generation function PÎ¸(y|[Pâ€²
Ï„;x]).3.2 S KELETON Framework
In the SKELETON framework, we still update only
prompt tokens, freezing the original parameters.
However, we assume that there is a minimal param-
eter combination Î¸Ï„âŠ‚Î¸for solving a specific task
Ï„; thus, we first define task neuron as follows:
Task Neuron. LetPÎ¸be a language model,
where Î¸={Î¸1, ..., Î¸ N}is a set of parameters of it.
Then, Î¸Ï„âŠ‚Î¸is defined as task neurons for a task
Ï„if the below equation is satisfied:
X
(x,y)âˆˆDÏ„L(y,PÎ¸([Pâ€²
Ï„;x]))â‰ˆX
(x,y)âˆˆDÏ„L(y,PÎ¸Ï„([Pâ€²â€²
Ï„;x]))
(1)
where DÏ„means a dataset for a task Ï„andLis a
score function (e.g., loss function or accuracy). Pâ€²
Ï„
andPâ€²â€²
Ï„are task-specific prompts. Nmeans the
total number of parameters in a language model.
SKELETON framework adopts the prompt tuning
strategy using only task neurons Î¸Ï„, and get the
new task-localized conditional generation function
PÎ¸Ï„(y|[Pâ€²â€²
Ï„;x]). Therefore, we can utilize the lan-
guage model more time-efficiently by using only
those neurons within the SKELETON framework.
In addition to being time efficient, this framework
is also memory-efficient since it requires only a
small fraction of original parameters Î¸Ï„and just a
few task-specific prompt tokens Pâ€²â€²
Ï„.
4 Methods
In this section, we describe the process of SKELE -
TON. We first train a language model using the
prompt tuning method to inject prior knowledge
of a specific task into it. After that, we quantify
the task relevance of each neuron to determine task
neurons in a language model. We finally retrain
the prompt tokens with the prompt tuning method
to condense the task knowledge to only the task
neurons. Note that the original language model
maintains only task neurons via a pruning method
in this process. After applying SKELETON frame-
work, we can utilize the language model efficiently
by using only the task-specific prompt and task neu-
rons, accelerating the inference speed for a specific
task. The entire process of SKELETON is described
in Figure 2 and Algorithm 1.
4.1 Quantifying Task Relevance of Neurons
This section describes the process of computing the
task relevance of each neuron. We adopt an attribu-
tion method (Yang et al., 2023, 2024) to quantify

--- PAGE 4 ---
Task Prompt &NeuronsFFN Layer PruningAttention LayerAdd+NormAdd
Ã—num _layers
Embedding LayerPrediction Head Task A 
Prompt
Task Z
Prompt
Task A Neuron Localizationâ€¦â¨
Task A
Data instance
ğ‘¢âˆˆâ„ğ‘‘
Residual NetworkÃ—ğ…ğ…ğğŸ
à·©ğ‘Š1âˆˆâ„ğ‘‘Ã—ğ‘šâ€²
=â„âˆˆâ„ğ‘šâ€²ğ…ğ…ğğŸ
â„
Ã— =à·©ğ‘Š2âˆˆâ„ğ‘šâ€²Ã—ğ‘‘â€²
ğ‘£âˆˆâ„ğ‘‘â€²
0
0+Zero Recoveryğ‘£â€²âˆˆâ„ğ‘‘ğ‘¢
=ğ‘œâˆˆâ„ğ‘‘ğ‘¢ğ‘§1,..,ğ‘§ğ‘™Task A 
Neurons
Task Z 
Neuronsâ€¦ğ‘1,..,ğ‘ğ‘˜Output Text
Dimension -dependent PruningNorm
Add
ğ‘1ğ‘2ğ‘3ğ‘4 ğ‘5ğ‘6 ğ‘7ğ‘1
ğ‘2
ğ‘3
ğ‘4ğ‘“:â„ğ‘‘â€™â†’â„ğ‘‘Figure 2: Task Neuron Localization Process of SKELETON .After training a model using our framework, a task
prompt and neurons are assigned to each task, and we utilize only those task-specific resources in the inference
procedure. Red-colored values in the prune weights, ËœW1andËœW2, mean task neurons for a specific task (task A). The
weight of FFN 2is pruned depending on FFN 1â€™s dimension, and the output representation of FFN 2is zero-recovered
(Blue-colored values) to match the output dimension of the residual network (described in Section 4.5).
the skill relevance of neurons in the pre-trained
language models. Formally, given a task-specific
prompt tokens PÏ„and text input tokens xprovided
toPÎ¸, the contribution of the i-th neuron to the
prediction of an output yis defined as follows:
A(PÏ„,y,x)
i (h) =|hiÃ—âˆ‚PÎ¸(y|[PÏ„;x])
âˆ‚hi|(2)
where hcorresponds to a hidden representation,
andâˆ‚PÎ¸(y|[PÏ„;x])/âˆ‚himeans the gradient of the
logitPÎ¸(y|[PÏ„;x])with respect to the i-th neuron.
We adopt transformer variants; thus, activations
and gradients are computed for all input token rep-
resentations. Therefore, if an input text xjincludes
aKjnumber of tokens, each neuron has a Kjnum-
ber of attributions. In addition, there are multiple
data instances for each task Ï„; thus, we aggregate
attributions for tokens and instances as follows:
A(PÏ„,DÏ„)
i (h) =NX
jKjX
k1
KjA(PÏ„,yj,xj,tk)
i (h)(3)
where A(PÏ„,yj,xj,tk)
i (h)means the attribution
scores computed for an input text token tkâˆˆxj.
DÏ„andNmean a task-specific dataset and the
number of instances in the dataset, respectively.
Therefore, A(PÏ„,DÏ„)
i (h)is the attribution score com-
puted for a specific task dataset DÏ„. AlthoughA(PÏ„,DÏ„)
i (h)can be computed using the entire
dataset, we report the experimental results of com-
puting it using only a significantly small amount of
data (e.g., only twenty data samples) to ensure the
efficiency of our method. To simplify the notation,
A(PÏ„,DÏ„)
i (h)will be abbreviated as Aifrom this
point forward.
4.2 Identifying Task-relevant Neurons.
After quantifying each neuronâ€™s task relevance,
we determine task-relevant neurons and eliminate
task-irrelevant neurons using a structured pruning
method. We first sort the neurons of the whole
target layers by the task relevance scores. Then,
we determine the optimal neuron ratio using the
Binary search algorithm to retain as few task neu-
rons as possible without compromising the task
knowledge. Specifically, we compute the accuracy
for the validation set Dvalto search for the optimal
neuron ratio that does not degrade the task perfor-
mance. We define a margin Ïˆand regard Î¸Ï„as task
neurons if the accuracy degradation after neuron
localization is smaller than the specified margin Ïˆ.
4.3 Task Neuron Localization.
After identifying task neurons, we eliminate task-
irrelevant neurons from a language model using a
structured pruning method. Suppose that a weight
matrix WâˆˆRdÃ—mis a linear matrix multiplica-
tion parameter, and then the matrix after pruning

--- PAGE 5 ---
is denoted as ËœW= (Wij)1â‰¤iâ‰¤d
jâˆˆM, where Mis the
set of task-relevant neuron indices about the W. If
the bias term bâˆˆRmis added to the operation for
an affine transformation, the bias term can also be
pruned by performing the Ëœb= (bi)iâˆˆMoperation
similarly. The task-localized parameters are used to
compute the new representation by performing the
transformation operation hËœWorhËœW+Ëœb. Notice
that this method is model-agnostic since all neural
network models consist of linear transformation
layers. For example, transformer variants have self-
attention and feed-forward network (FFN) modules,
all of which include linear matrix multiplication
operations. After localizing a language model, we
further intensify task knowledge of a localized lan-
guage model using a prompt tuning method.
4.4 Task Neuron Localized Prompt Tuning
OurSKELETON framework follows the four steps
to train a model: (1) the first training step for in-
jecting prior task knowledge; for injecting prior
task knowledge; (2) the quantification step of com-
puting task relevance for each neuron; (3) the task
neuron localization step; (4) the final prompt tuning
step for condensing task knowledge.
The Prior Knowledge Injection Step. The attri-
bution method quantifies the contribution of each
neuron to the prediction of an output; thus, a lan-
guage model must have the ability to solve a spe-
cific task in order to accurately quantify the task-
specific knowledge of each neuron. Therefore, we
first execute the prior knowledge injection process.
Specifically, we train a language model PÎ¸via
prompt tuning and get a task-specific prompt Pâ€²
Ï„.
The Quantification Step. After injecting task
knowledge into the language model, we quantify
the task relevance Aiof each neuron for PÎ¸andPâ€²
Ï„
using Equation 2 and 3.
The Task Neuron Localization Step. After
quantifying the task relevance Ai, we first sort the
neurons by the task relevance in descending order.
Then, we select and maintain only the top- pneu-
ronsÎ¸Ï„âŠ‚Î¸from the language model by pruning
task-irrelevant neurons (Section 4.3). Note that we
use a Binary search algorithm to find the optimal
ratio of neurons pâˆˆ[0.0,0.05, ...,0.95,1.0]. This
search process requires only a maximum of five
trials since we search pruning ratio in 0.05 units.Algorithm 1 SKELETON Framework
Input: a language model PÎ¸; a prompt PÏ„; a margin Ïˆ
Output: a task neuron localized model Pâˆ—
Î¸Ï„
1: train PÎ¸via prompt tuning using PÏ„and get Pâ€²
Ï„
2: get the validation accuracy sÏ„forPÎ¸using PÏ„
3: compute task relevance Aivia Equation 2 and 3
4: sort target neurons in descending order by Ai
5:
6:lâ†0
7:hâ†20
8:while lâ‰¤rdo // Binary search
9: mâ† âŒŠ (l+h)/2âŒ‹
10: p=mÃ—0.05
11: get top- ptask neuron localized model PÎ¸Ï„
12: train PÎ¸Ï„via prompt tuning using Pâ€²
Ï„and get Pâ€²â€²
Ï„
13: get validation accuracy sâ€²
Ï„forPÎ¸Ï„using Pâ€²â€²
Ï„
14: ifsÏ„âˆ’sâ€²
Ï„â‰¤Ïˆthen
15: Pâˆ—
Î¸Ï„â† P Î¸Ï„ // The optimal model
16: lâ†m+ 1
17: else
18: hâ†mâˆ’1
19: return Pâˆ—
Î¸Ï„
The Knowledge Condensation Step. During the
task neuron localization step, we condense the task
knowledge into only the task neurons. Although
we never update the original parameters of the lan-
guage model, we can condense the task knowledge
into the task neurons by updating only Pâ€²
Ï„. We train
the task neuron localized model PÎ¸Ï„and get a new
knowledge-condensed task prompt Pâ€²â€²
Ï„. The Binary
search algorithm determines the optimal pruning
rate after this step.
4.5 Details of the S KELETON Framework
Satisfying Dimension Integrity. This section de-
scribes the detailed task localization processes; the
entire process is shown in Figure 2. We prune
task-irrelevant neurons from a language model.
Recent language models have been trained using
the transformer architecture, but they consist of
complex neural computation processes. For ex-
ample, sequentially connected networks are de-
pendent on each other in terms of dimensionality.
Suppose there are two sequential feed-forward net-
works (FFNs), W1âˆˆRdÃ—mandW2âˆˆRmÃ—d. If
we prune the first FFN and get a pruned weight
ËœW1âˆˆRdÃ—mâ€², this pruning process affects the in-
consistency of the dimension with the weight in
the second FFN. Specifically, W2is forced to be
pruned as ËœW2âˆˆRmâ€²Ã—d. Therefore, we implement
the task neuron localization algorithm by reflecting
the dimension dependency of two FFNs. In addi-
tion, suppose the representation computed by the
second FFN, vâˆˆRd, is connected to the represen-
tation of a residual connection network, uâˆˆRd.

--- PAGE 6 ---
Model Method SST-2 IMDB AGNews MRPC CB
BERT
(110M)Fine-tuning (Single) 92.39 92.50 94.12 82.35 78.57
Fine-tuning (Multi) 91.36 92.16 93.98 78.02 79.76
Prompt-tuning 89.36 88.56 90.58 67.07 69.63
SKELETON 89.21 88.65 89.94 68.05 69.04
Pruning rates (%) 36.7% (31M) 38.3% (33M) 55.0% (47M) 65.0% (55M) 65.0% (55M)
Speed-up ( Ã—) Ã—1.29 Ã—1.30 Ã—1.43 Ã—1.73 Ã—1.72
RoBERTa
(125M)Fine-tuning (Single) 94.34 94.56 94.61 85.13 86.90
Fine-tuning (Multi) 94.65 94.70 94.64 86.85 72.02
Prompt-tuning 93.84 92.72 91.87 67.23 66.06
SKELETON 92.81 91.50 90.79 67.89 69.04
Pruning rates (%) 30.0% (26M) 36.7% (31M) 61.7% (53M) 58.3% (50M) 63.3% (54M)
Speed-up ( Ã—) Ã—1.18 Ã—1.23 Ã—1.25 Ã—1.39 Ã—1.53
Table 1: Task Neuron Localized Prompt Tuning Results. We report the accuracy of five datasets for SKELE -
TON and other baselines. We train models using each method by three trials and report the averaged accuracy.
Pruning Rate (%) andSpeed up ( Ã—)mean the ratio of neurons deactivated after applying our method and the
inference speed up compared to the Prompt-tuning, respectively. The values in parenthesis are the number of pruned
parameters. The pruning rates are calculated based on the maximum number of prunable parameters.
Then, if we prune the second FFN and get a pruned
weight ËœW2âˆˆRmÃ—dâ€², this process also triggers
the inconsistency of the dimension with the rep-
resentation transferred by the residual connection,
making vâˆˆRdâ€². Thus, we insert zero values into
the pruned dimension of the second FFNâ€™s repre-
sentation, v, to ensure its integrity.
Accelerating Inference Speed. Our framework
does not just convert the pruned value of weights
to zero values but eliminates the entire column
(i.e., neuron) of them. Therefore, our framework
accelerates the language model inference speed.
We do not consider other types of networks (e.g.,
Attention modules, Embedding layer, Language
model head) since they play a crucial role in the
modelâ€™s language understanding.
Furthermore, the parameters of FFNs constitute
most of the prunable parameters (about more than
65%); thus, pruning even only the FFNs is suffi-
cient. The experimental results for other modules
are described in Section 5.4.
5 Experiments
5.1 Experimental Setup
Datasets. We conduct experiments on five
widely-used natural language understanding
datasets1(Maas et al., 2011; Zhang et al., 2015;
Wang et al., 2018). Specifically, we utilize SST-2,
IMDB (sentiment analysis); AGNews (topic
classification); MRPC (semantic textual matching);
CB (natural language inference) to verify the
1https://huggingface.co/docs/datasets/applicability of our method. We split the train set
and use 10% of them as a validation set.
Models and Training Methods. We utilize two
transformer variants2, BERT (Devlin et al., 2018)
and RoBERTa (Liu et al., 2019), for our exper-
iments since they are some of the most popular
basic language models. We select a prompt tun-
ing method (Lester et al., 2021) to compare with
our method, and also include the results of basic
fine-tuning and multi-task fine-tuning to show the
upper-bound performance of each task.
Implementation Details. We evaluate SKELE -
TON and baselines on NVIDIA A5000 GPU. We
train models using l= 20 prompt tokens dur-
ing 100 epochs with early-stop conditions for all
datasets. In addition, we set the margin Ïˆ=
1.0%for the validation accuracy score used in
the Binary search algorithm. We search pâˆˆ
[0.0,0.05, ...,0.95,1.0]to find the best pruning rate
in the Binary search algorithm. We quantify the
task relevance using only n= 20 data instances of
each training dataset, following Yang et al. (2024).
We specify the target modules for task neuron lo-
calization as FFN networks since the experimental
results for FFN networks outperform those of other
modules, shown in Figure 6. We use learning rates
Î³1âˆˆ[1e-5, 5e-5] and Î³2âˆˆ[1e-4, 5e-3] for fine-
tuning and prompt-tuning, respectively. We use a
masked language model probing network includ-
ing an instruction for solving a task to utilize the
pre-trained knowledge of BERT and RoBERTa, as
shown in Appendix A.
2https://huggingface.co/docs/transformers/

--- PAGE 7 ---
5.2 S KELETON is Efficient and Robust
Datasets Experiments. We evaluate and report
the task performance and efficiency of each method
in Table 1 for the five natural language under-
standing benchmarks. We adopt BERT-base and
RoBERTa-base to conduct the experiments. The
results show that our method performs comparably
to the prompt tuning method and even outperforms
it in accuracy for some datasets using only a small
number of task-localized parameters (e.g., at most
65% of pruning rates). Surprisingly, the pruned
networks show at most Ã—1.72speed up; thus, these
results reveal that our method successfully localizes
efficient task-relevant sub-networks from language
models. We describe the detailed elapsed time (sec)
ofSKELETON models and prompt tuned model in
Figure 3. These results reveal that our method sig-
nificantly outperforms the prompt tuning method
in inference speed.
BERT RoBERTa1.21.41.61.82.02.2
Ã—1.29Ã—1.18SST-2
Prompt Tuning
Skeleton
BERT RoBERTa708090100110120130
Ã—1.3Ã—1.23IMDB
Prompt Tuning
Skeleton
BERT RoBERTa0.30.40.50.60.70.80.91.01.1
Ã—1.73Ã—1.39MRPC
Prompt Tuning
Skeleton
BERT RoBERTa0.100.150.200.250.30
Ã—1.72Ã—1.53CB
Prompt Tuning
SkeletonElapsed Time (sec)
Figure 3: Elapsed Time and Inference Speed-up. We
plot the elapsed time and inference speed-up for BERT-
base and RoBERTa-base on the four datasets. The val-
ues above the bar plot are the speed-up ratios.
Hyperparameter Experiments. We conduct ex-
periments to investigate the time efficiency of our
framework for varying hyperparameters, as shown
in Figure 4. Specifically, we search varying batch
sizes and the number of input tokens to demon-
strate efficiency by comparing our framework to
the prompt tuning method. We sample random
texts with ktokens and input them into the BERT-
base model for the experiments. For the batch
size experiments, we evaluate the inference speed
with varying batch sizes Î²âˆˆ[16,32,64,256,512],
fixing the number of tokens at 64. For the num-
ber of tokens experiments, we measure the in-
ference speed with multiple numbers of tokenskâˆˆ[32,64,256,512], using the batch size of 64.
The results reveal that our framework shows robust
efficiency improvement in various hyperparame-
ters, showing better speed in higher batch sizes and
a widely used number of tokens.
16 32 64 256 512
Batch Size1.21.31.41.51.6(a) Batch Size
Pruning 50%
Pruning 30%
32 64 256 512
Num Tokens1.11.21.31.41.51.6(b) Num Tokens
Pruning 50%
Pruning 30%Speed Up (Ã—)
Figure 4: Inference Speed Experiments for Varying
Hyper-parameters (Batch size & # of Tokens). We
report the results for the pruning rates of 30% and 50%
for BERT-base to ensure a fair comparison.
5.3 Task Localization Methods Experiments
We compare our method to other task neuron selec-
tion methods: Activation ,Gradient , and Random ,
to justify the excellence of the attribution method
(Section 5.3). Activation identifies task neurons by
using the activated values of neurons. We compute
the sum of activated values of each neuron when
inputting a dataset. Gradient selects task neurons
by using the gradient values of each neuron. We
compute the sum of the gradient values of each
neuron when predicting the label of the utilized
dataset. Random randomly selects task neurons.
We demonstrate the task localization perfor-
mance of those baselines to justify our selection of
the task localization method, Attribution , as shown
in Figure 5. Specifically, we select the BERT-base
model for the SST-2 and IMDB datasets for the
experiments. We prune the top 50% of task neu-
rons by using those methods for a fair compari-
son. Surprisingly, even Activation andGradient
do not show competitive performance compared to
Random . However, SKELETON (i.e., Attribution )
outperforms other methods in the two datasets, re-
vealing the excellence of our method.
5.4 Module-specific Task-localization
Transformer variants have various types of mod-
ules (i.g., attention modules and FFNs). Therefore,
we examine which module is suitable for maintain-
ing the knowledge of a language model (BERT-
base). Specifically, we categorize layers into four
segments: (1) All layers ( All), (2) Attention lay-
ers (Attn), (3) Dense layers ( Dense ), and (4) Feed-
forward layers ( FFN ).Allincludes all linear lay-

--- PAGE 8 ---
0 10 20 30 40 5076788082848688
(a) SST-2
Skeleton
Activation
Gradient
Random
0 10 20 30 40 50
Training Epoch76788082848688
(b) IMDB
Skeleton
Activation
Gradient
RandomAccuracy(%)Figure 5: Task Localization Methods Experiments.
We plot the mean accuracy ( Â±one standard deviation
for three trials) of BERT-base for SST-2 and IMDB.
ers. Attn consists of Query (Q), Key (K), Value
(V), and Output (O) layers of the attention module.
Dense contains Oof the attention module and two
FFNs. FFN includes two FFNs. We prune the top
30% of task neurons from each module for a fair
comparison. Figure 6 represents the results of our
method for various types of modules. From these
experiments, we reveal that FFNs are suitable for
maintaining the knowledge of a language model in
our prompt tuning setting. Adopting the FFN mod-
ule yields faster performance than other modules
without compromising accuracy.
60 70 80 90 1000.00.30.60.91.21.5(a) SST-2
70 75 80 85 90 950.00.30.60.91.21.5(b) IMDB
All  Dense Attn FFN (ours)Accuracy (%)Speed Up (x)
Figure 6: Module-specific Task-localization Results.
We plot the relationship between the accuracy and speed
up of each module, where each diagram corresponds
to the categorized module. The red-dotted line is the
baseline performance (Prompt-tuning).
5.5 Prior Knowledge Injection is Important
This section describes the importance of injecting
prior task knowledge (described Section 4.4) to ac-
curately quantify the task relevance, as shown in
Figure 7. We prune the top 50% of task neurons by
using two methods (i.e., SKELETON and w/o Prior
Task Knowledge) for a fair comparison. The ex-
perimental results reveal that prior task knowledge
injection has a crucial role in quantifying the task
0 10 20 30 40 506669727578818487
(a) SST-2
Skeleton
w/o Prior T ask Knowledge
0 10 20 30 40 50
Training Epoch69727578818487
(b) IMDB
Skeleton
w/o Prior T ask KnowledgeAccuracy(%)Figure 7: Prior Knowledge Injection is Important.
We plot the mean accuracy ( Â±one standard deviation
for three trials) of BERT-base for SST-2 and IMDB.
relevance. Excluding the Prior Knowledge Injec-
tion step results in slower convergence and lower
task accuracy compared to our method.
5.6 S KELETON is Scalable to a Larger Model
We conduct experiments to verify that our method
is scalable to a larger model, as shown in Table 2.
Specifically, we adopt BERT-large to show the scal-
ability of our framework. These results reveal that
our method is scalable to a larger model.
Method SST-2 IMDB
BERT-base
(110M)Fine-tuning (Single) 92.39 92.50
Fine-tuning (Multi) 91.36 92.16
Prompt-tuning 89.36 88.56
SKELETON 89.21 88.65
Pruning rates (%) 36.7% 38.3%
Speed-up ( Ã—)Ã—1.29 Ã—1.30
BERT-large
(340M)Fine-tuning (Single) 92.77 93.57
Fine-tuning (Multi) 92.73 93.83
Prompt-tuning 91.28 89.96
SKELETON 92.19 89.35
Pruning rates (%) 35% 30%
Speed-up ( Ã—)Ã—1.34 Ã—1.27
Table 2: Experiments on various sizes of BERT.
6 Conclusion
In this study, we propose SKELETON , which ex-
plores the extreme of the prompt tuning method by
localizing task-relevant neurons from the original
language model. SKELETON investigates the op-
timal task-localized sub-network from a language
model and condenses task knowledge by utilizing
only task-relevant parameters during the prompt-
tuning procedure. We demonstrate our method for
various benchmarks and reveal that SKELETON dra-
matically enhances the efficiency of language mod-
els, showing significant inference speed boost-up.

--- PAGE 9 ---
Limitations
Our framework accelerates the inference speed
of language models, identifying the optimal sub-
network for utilizing language models. However,
while our approach exhibited comparable perfor-
mance and occasionally slightly superior results to
the existing prompt tuning method, these improve-
ments were not consistent across overall task per-
formance. In addition, our experiments are limited
to five widely-used natural language understanding
datasets; thus, we should extend to a wide range of
datasets to validate the applicability of our frame-
work. Furthermore, we select feed-forward net-
works to localize task neurons in language models
since they are adequate for inference speed acceler-
ation. However, the entire modules in Transformer
should be considered as a localization target to
achieve the extreme inference speed up; we leave
this point as future research.
Acknowledgements
This work was supported by Institute of Informa-
tion & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea gov-
ernment (MSIT) [No.RS-2022-II220184, Develop-
ment and Study of AI Technologies to Inexpen-
sively Conform to Evolving Policy on Ethics &
No.RS-2021-II211343, Artificial Intelligence Grad-
uate School Program (Seoul National University) &
No.RS-2021-II212068, Artificial Intelligence Inno-
vation Hub (Artificial Intelligence Institute, Seoul
National University)]. K. Jung is with ASRI, Seoul
National University, Korea.
References
Srijan Bansal, Semih Yavuz, Bo Pang, Meghana Bhat,
and Yingbo Zhou. 2023. Few-shot unified ques-
tion answering: Tuning models or prompts? arXiv
preprint arXiv:2305.14569 .
Chunkit Chan, Xin Liu, Jiayang Cheng, Zihan Li,
Yangqiu Song, Ginny Y Wong, and Simon See. 2023.
Discoprompt: Path prediction prompt tuning for im-
plicit discourse relation recognition. arXiv preprint
arXiv:2305.03973 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu
Wang, Yu Zhang, and Lilian Tang. 2024. Selective
prompting tuning for personalized conversations with
llms. arXiv preprint arXiv:2406.18187 .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 .
Zujie Liang, Feng Wei, Yin Jie, Yuxi Qian, Zhenghong
Hao, and Bing Han. 2023. Prompts can play lottery
tickets well: Achieving lifelong information extrac-
tion via lottery prompt tuning.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,
Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-
tuning v2: Prompt tuning can be comparable to fine-
tuning universally across scales and tasks. arXiv
preprint arXiv:2110.07602 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan
Wang, Wei Wu, Xiaojun Quan, and Dawei Song.
2022. Xprompt: Exploring the extreme of prompt
tuning. arXiv preprint arXiv:2210.04457 .
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y . Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
Association for Computational Linguistics.
Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and
Sanjeev Arora. 2023. Task-specific skill localiza-
tion in fine-tuned language models. arXiv preprint
arXiv:2302.06600 .
Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya
Purkayastha, Leon EnglÃ¤nder, Timo Imhof, Ivan
VuliÂ´c, Sebastian Ruder, Iryna Gurevych, and Jonas
Pfeiffer. 2023. Adapters: A unified library for
parameter-efficient and modular transfer learning.
arXiv preprint arXiv:2311.11077 .
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina,
and Anshul Kundaje. 2016. Not just a black box:
Interpretable deep learning by propagating activation
differences. arXiv preprint arXiv:1605.01713 , 4.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .

--- PAGE 10 ---
Shijie Wang, Jianlong Chang, Zhihui Wang, Haojie Li,
Wanli Ouyang, and Qi Tian. 2023. Fine-grained re-
trieval prompt tuning. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 37,
pages 2644â€“2652.
Nakyeong Yang, Yunah Jang, Hwanhee Lee, Seo-
hyeong Jeong, and Kyomin Jung. 2023. Task-specific
compression for multi-task language models using
attribution-based pruning. In Findings of the Asso-
ciation for Computational Linguistics: EACL 2023 ,
pages 582â€“592.
Nakyeong Yang, Taegwan Kang, Stanley Jungkyu Choi,
Honglak Lee, and Kyomin Jung. 2024. Mitigating
biases for instruction-following language models via
bias neurons elimination. In Proceedings of the 62nd
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 9061â€“
9073.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text classi-
fication. Advances in neural information processing
systems , 28.
A Instruction Templates.
We use text templates for input texts of the utilized
datasets. Specifically, the text templates consist of
an instruction and an input text, as shown in Table 3.
For the prediction, we utilize the representation
of the mask token (i.e., "<mask>" ) to utilize the
pre-trained knowledge of encoder-only language
models.
B Module-specific Task-localization
Details
We conduct experiments for various modules in
the transformer architecture, as described in Sec-
tion 5.4. We implement algorithms for pruning
those networks following our SKELETON frame-
work to satisfy the dimension of adjacent networks
after pruning. For example, suppose that we prune
the Key network, WKâˆˆRdÃ—d, and get a pruned
weight, ËœWKâˆˆRdÃ—dâ€², in the Attention module.
Then, we should also prune the Query network,
WKâˆˆRdÃ—d, since they are adjacent networks.
Therefore, after the pruning process, if we get
vQâˆˆRdandvKâˆˆRdâ€²from the Query and Key
networks, we recover the pruned parameters of
vKâˆˆRdâ€²by adding zero values to ensure the iden-
tical dimension. We also implement other layers in
the same way with it.

--- PAGE 11 ---
Dataset Template Label
SST-2 Text: {text}. The sentiment of the text is <mask>. positive / negative
IMDB Text: {text}. The sentiment of the text is <mask>. positive / negative
AGNews Text: {text}. The topic of the text is <mask>. World / Sports / Business / Science
MRPC Text1: {text1}. Text2: {text2}. The two texts are <mask>. different / equivalent
CBPremise: {premise}. Hypothesis: {hypothesis}.
The premise and hypothesis have a relationship of <mask>.implication / contradiction / neutrality
Table 3: Instruction Templates and Labels for Each Dataset. The templates show how text, premise, hypothesis,
and mask tokens are used in each dataset.
