# 2304.13639.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2304.13639.pdf
# File size: 632638 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
PVP: Pre-trained Visual Parameter-
EfÔ¨Åcient Tuning
Zhao Song, Ke Y ang*, Naiyang Guan*, Junjie Zhu, Peng Qiao, and Qingyong Hu
Abstract ‚ÄîLarge-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is
still highly challenging to fully Ô¨Åne-tune these models for downstream tasks due to their high computational and storage costs. Recently,
Parameter-EfÔ¨Åcient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) [1] and Low-Rank Adaptation (LoRA) [2], have
signiÔ¨Åcantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning
these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few
parameters need to be adjusted, most PETuning methods still require a signiÔ¨Åcant amount of downstream task training data to achieve
good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To
this end, we Ô¨Årst empirically identify the poor performance is mainly due to the inappropriate way of initializing prompt modules, which
has also been veriÔ¨Åed in the pre-trained language models. Next, we propose a Pre-trained Visual Parameter-efÔ¨Åcient (PVP) Tuning
framework, which pre-trains the parameter-efÔ¨Åcient tuning modules Ô¨Årst and then leverages the pre-trained modules along with the
pre-trained transformer backbone to perform parameter-efÔ¨Åcient tuning on downstream tasks. Experiment results on Ô¨Åve Fine-Grained
Visual ClassiÔ¨Åcation (FGVC) and VTAB-1k datasets demonstrate that our proposed method signiÔ¨Åcantly outperforms state-of-the-art
PETuning methods. As highlighted below, we show that our PVP framework achieves 16.08 %, 11.52%, 6.36%, 2.94%, and 1.95 %
average accuracy improvement under 1, 2, 4, 8, and 16 shot setting on FGVC, respectively, compared with the previous PETuning
techniques, e.g., VPT, in the task of few-shot image classiÔ¨Åcation. PVP also achieves state-of-the-art results in the VTAB-1k benchmark,
surpassing the average accuracy of very recent PETuning methods by 2.33 %.
Index Terms ‚ÄîParameter-EfÔ¨Åcient Tuning, Prompt Tuning, Vision Transformer, Few-shot Learning, Transfer Learning.
F
1 I NTRODUCTION
INthe past few years, vision transformer models including
ViT [3] and Swin [4], have achieved encouraging results
on a number of mainstream vision tasks. However, training
such large transformer models is usually accompanied by
massive training data and expensive computational costs,
making it highly challenging for individuals to train such
models from scratch. Fortunately, the industry technology
giants including Microsoft and Facebook, have released
models with carefully pre-trained parameters on large-scale
pre-training data [5], enabling individuals to use large trans-
former models by either Ô¨Åne-tuning all the model parame-
ters or just a small proportion of model parameters [6], [7],
[8], [9], [10], [11] while keeping the majority frozen.
Recently, a handful of pioneering works termed
Parameter-EfÔ¨Åcient Tuning (PETuning) methods [1], [12],
[13], [14], [15], [16], attempted to tune several newly in-
serted modules instead of part of the transformer backbone.
For example, Visual Prompt Tuning (VPT) [1] is a PETun-
ing method that adds task-speciÔ¨Åc learnable parameters,
namely prompt tokens, to the input space and only Ô¨Åne-
tunes the prompt tokens on downstream tasks. Notably,
prompt tokens only account for less than 2 %of total pa-
rameters. Intuitively, such a small amount of parameter
adjusting is naturally suitable for the scheme of few-shot
Z. Song, K. Yang, N. Guan, and J. Zhu are with the Defense Innovation
Institute, Beijing, China.
P . Qiao is with the National University of Defense Technology, Changsha,
China.
* denotes the corresponding author.
CIFAR-ÔøΩÔøΩÔøΩ
CaltechÔøΩÔøΩÔøΩ
DTD
FlowersÔøΩÔøΩÔøΩ
Pets
SVHN
SunÔøΩÔøΩÔøΩ
Patch Camelyon
EuroSAT
ResiscÔøΩÔøΩ RetinopathyClevr/countClevr/distanceDMLabKITTI/distancedSprites/locationdSprites/orientationSmallNORB/azimuthSmallNORB/elevationVPT-Deep NOAH SSF FacT PVP (ours)Fig. 1. Our proposed PVP demonstrates strong performance over re-
cent state-of-the-art methods on the VTAB-1k benchmark. The dataset
names are color-coded to indicate the best-performing method for each
dataset clearly.
learning, where only a few data samples are provided for
training. However, we empirically Ô¨Ånd that poor perfor-
mance is achieved by VPT when limit tuning data (as shown
in Sec. 3.3). In particular, the accuracy on the CUB-200-2011
dataset drops to 30.05 %using 1 %tuning data, compared to
88.50 %accuracy using all tuning data. Motivated by this, we
aim to explore the fundamental problems of why PETuning
methods do not perform well on few-shot classiÔ¨ÅcationarXiv:2304.13639v1  [cs.CV]  26 Apr 2023

--- PAGE 2 ---
2
tasks.
We attribute this phenomenon to the inadequate initial-
ization of prompt modules, since current PETuning meth-
ods, e.g., VPT [1], Adapter [12], and LoRA [2], usually use
zero- or random-initialized modules for PETuning, mean-
ing the newly added modules need to learn from scratch
on downstream tasks. Moreover, most PETuning methods
require the insertion of trainable prompt modules at ear-
lier layers, particularly at the beginning of the network,
resulting in the weights of all later layers being scrapped.
These two problems lead to the prompt module requiring
a signiÔ¨Åcant amount of data for training, which can prove
challenging for downstream tasks. However, pre-training
datasets, such as ImageNet [5], offer ample data to meet
the required training needs.
To this end, we propose a Pre-trained Visual Parameter-
efÔ¨Åcient (PVP) Tuning framework. We Ô¨Årst pre-train the
newly added modules of PETuning on a large dataset and
subsequently leverage these pre-trained modules to perform
PETuning on downstream few-shot learning tasks. The ra-
tionale behind our approach is that the pre-trained param-
eters offer an excellent foundation for PETuning, requiring
only a few gradient updates to Ô¨Åne-tune the modules. The
tuned modules can then be applied to tasks such as few-
shot image classiÔ¨Åcation. Importantly, we note that the newly
added tuning modules and the vision transformer backbone are
pre-trained on the same dataset, hence no additional pre-training
data is involved.
In addition to its effectiveness in few-shot scenarios,
our proposed Pre-trained Visual Parameter-efÔ¨Åcient (PVP)
Tuning approach is also applicable when sufÔ¨Åcient tuning
data is available. Our experimental results indicate that the
module pre-training stage signiÔ¨Åcantly improves the adapt-
ability of the transformer backbone to downstream tasks,
outperforming current PETuning methods. Thus, our ap-
proach represents a promising Parameter-EfÔ¨Åcient method
for large-scale pre-trained transformer models.
The proposed PVP framework can be readily applied to
different PETuning methods, provided that these methods
integrate tunable modules into the vision transformer back-
bone and tune the newly added modules while keeping the
transformer backbone frozen during downstream task tun-
ing. SpeciÔ¨Åcally, our framework can be applied to methods
such as VPT [1], Adapter [12], and LoRA [2]. Our experi-
mental results demonstrate that these methods experience
signiÔ¨Åcant performance improvements when augmented
with our PVP . Our contributions can be summarized as
follows.
To the best of our knowledge, this is the Ô¨Årst study to
clarify the limitations of Parameter EfÔ¨Åcient Tuning
(PETuning) techniques on few-shot tasks and tackle
this issue by pre-training the newly added PETuning
modules.
We propose a simple yet efÔ¨Åcient Pre-trained Visual
Parameter-efÔ¨Åcient (PVP) Tuning framework, which
achieves signiÔ¨Åcant performance gains on down-
stream few-shot tasks, particularly in extremely low-
data regimes with only 1 or 2 training samples per
class. Our approach can be easily applied to various
PETuning methods and achieves a great performance
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of Samples per Class020406080100T est Accuracy (%)
VPT
LoRA
AdapterFig. 2. Performance degradation of existing visual PETuing techniques
under few-shot classiÔ¨Åcation setting on the CUB-200-2011 datasets.
improvement.
In addition to the few-shot scenario, our PVP ap-
proach also achieves state-of-the-art results on the Vi-
sual Task Adaptation Benchmark (VTAB-1k), outper-
forming recent PETuning methods by a large margin.
2 R ELATED WORKS
2.1 Transformers In Vision
Transformer is a type of deep neural network mainly
based on self-attention mechanisms, which has been widely
investigated due to its superior performance. Vaswani
et al. [17] proposed transformer architecture with a self-
attention mechanism to capture the contextual relationship
between inputs, and achieved great success in the Natural
Language Processing (NLP) Ô¨Åeld [18], [19], [20], [21]. The re-
markable success of large-scale transformer models in NLP
has sparked a growing interest in adopting these models
in Computer Vision (CV). Dosovitskiy et al. [3] introduced
transformer architecture into the Ô¨Åeld of computer vision
and proposed Vision Transformer (ViT). This is achieved
by dividing an image into patches and then embedding
these patches as tokens for the transformer encoder. Liu
et al. [4] proposed swin transformer and calculated self-
attention in the hierarchical local window while allowing
cross-window interaction, which provided a multi-scale
receptive Ô¨Åeld for the transformer. Subsequently, a vari-
ety of visual transformers [22], [23], [24] are proposed to
leverage knowledge distillation, convolutional embedding,
and depth-wise convolution to improve the performance.
Though vision transformer-based methods have achieved
state-of-the-art performance in various vision benchmarks,
Ô¨Åne-tuning pre-trained transformer models on downstream
tasks is still data-dependent and computationally expensive,
which limits the wider application of vision transformer
models. Given that large-scale pre-trained models are pub-
licly available, how to adapt the pre-trained transformers
to downstream tasks [25], [26] in a parameter and memory
efÔ¨Åcient way remains a crucial open problem.
2.2 Parameter EfÔ¨Åcient Tuning
The past few years have witnessed the huge success of
parameter-efÔ¨Åcient transfer learning in NLP [27], [28], [29],

--- PAGE 3 ---
3
Stage1 Stage2Pre-train Data
......Downstream Data
......
Transformer Block Transformer Block
initialize
Original Transformer Module Parameter Efficient Tuning Module
Fig. 3. Overview of Pre-trained Parameter EfÔ¨Åcient Tuning. There are two stages for our pre-trained parameter efÔ¨Åcient tuning method. (1) Parameter
EfÔ¨Åcient Tuning module pre-train stage and (2) Downstream Parameter EfÔ¨Åcient Tuning stage. Original transformer modules are frozen and
parameter-efÔ¨Åcient tuning modules are tunable in both stages. The learned parameter efÔ¨Åcient tuning modules in stage 1 are used to initialize
these in stage 2. The black and red rows represent forward and backward respectively.
[30], [31], [32]. Recently, parameter-efÔ¨Åcient tuning meth-
ods on the pre-trained vision transformer models have
been widely explored. Jia et al. [1] proposed to add a few
additional tokens, namely prompt tokens, into the input
space as tunable parameters. The prompt tokens are fed
into multi-head attention (MHA) together with the original
tokens. In particular, they only Ô¨Åne-tuned the prompt tokens
while keeping the transformer backbone parameters frozen.
Surprisingly, Ô¨Åne-tuning the prompt tokens achieved com-
parable or even better performance than full Ô¨Åne-tuning.
Houlsby et al. [12] inserted an adapter architecture into the
Feed-Forward Network (FFN) and Ô¨Åne-tuned the adapter
layers, aiming to adapt the pre-trained backbone weight to
downstream tasks. The adapter is typically a bottleneck-like
architecture consisting of a down-sample layer, a non-linear
layer, and an up-sample layer. Hu et al. [2] proposed a low-
rank adaption approach by decomposing the increments
of query transformation and value transformation into a
low-rank manner and achieving higher accuracy and lower
memory consumption. Zhang et al. [13] focused on combin-
ing existing PETL methods without manual design. They
trained a large supernet at Ô¨Årst and then performed a neural
architecture search on hidden dimension hof Adapter, rank
rof LoRA, and prompt length lof VPT to Ô¨Ånd the best
subnet for each task using a one-shot neural architecture
search algorithm [33]. Lian et al. [15] proposed a new
baseline for efÔ¨Åcient model tuning. Taking inspiration from
various normalization methods, they scaled and shifted the
deep features extracted by a pre-trained model with scale
and shift factors. Jie et al. [16] proposed a tensorization-
decomposition framework to store the weight increments,
in which the weights of each ViT were tensorized into a
single 3D tensor, and their increments were then decom-
posed into lightweight factors. In the Ô¨Åne-tuning process,
only the factors need to be updated. Typically, the above
methods insert small learnable modules into large-scale pre-
trained transformer models and Ô¨Åne-tune these moduleswith downstream tasks while freezing the pre-trained trans-
former parameters. These methods are instructive for using
pre-trained transformer backbones on various vision tasks.
PETuning for Few-Shot Learning. In several practical
applications, high-quality labeled data is often scarce due
to expensive annotation costs and potential privacy con-
cerns [34], [35]. Pre-trained transformer models have been
successfully adapted to mitigate this limitation through
techniques such as VPT [1], Adapter [12], and NOAH
[13], which Ô¨Åne-tune only a small proportion of the total
parameters while maintaining competitive performance on
downstream tasks. However, it remains an open question
whether these techniques can be effectively applied to few-
shot learning tasks, where the available training examples
are even more limited. Recent studies in natural language
processing have begun to explore this challenge [36], [37],
[38], [39], [40], [41]. Building on this work, we extend the
investigation to few-shot parameter-efÔ¨Åcient tuning (PETun-
ing) in the computer vision domain. Note that, LORA
[2] aims to maintain the identity of the output for an
inserted layer when training a transformer after adding
a new module, achieved by properly initializing the new
module. However, we observe that LORA encounters dif-
Ô¨Åculties in few-shot settings. By contrast, by incorporating
our proposed PVP Tuning framework, we demonstrate a
signiÔ¨Åcant improvement in LORA‚Äôs few-shot performance,
as demonstrated in the experimental section.
3 P ROPOSED METHOD
3.1 Overview
In this section, we Ô¨Årst revisit existing PETuning techniques
and then conduct exploratory experiments to verify the
performance of existing PETuning techniques including
VPT [1], Adapter [12], and LoRA [2] on few-shot learning
tasks. Next, we propose PVP , which Ô¨Årstly pre-trains the
tunable parameters of PETuning on a large dataset and then
uses the pre-trained parameters for downstream PETuning.

--- PAGE 4 ---
4
We summarize this section by discussing the versatility of
our PVP framework.
3.2 Revisit Parameter-EfÔ¨Åcient Tuning Methods
Here, we brieÔ¨Çy recap the PETuning techniques. The key
idea of PETuning is to inject a few parameters into the
transformer backbone. The transformer backbone parame-
ters are frozen to yield generalized representations learned
from large-scale data and the newly inserted parameters are
tunable to adapt the output distribution to speciÔ¨Åc down-
stream tasks. We use Fto denote the vision transformer
model with parameters . For transformer architecture,
y=F(x); (1)
and the gradient is calculated as
g=@F(D;)
@; (2)
whereDis large-scale training dataset. For PETuning meth-
ods, a few new parameters 0are inserted into F,
y=F;0(x); (3)
where0is usually much less than andis Ô¨Åxed during
Ô¨Åne tuning with only 0learnable. The gradient update for
PETuning methods is formulated as
g0
=@F(D0;;0)
@0; (4)
whereD0is a downstream dataset for a speciÔ¨Åc task and is
usually much smaller than D.
3.3 Exploring Few-shot Parameter-EfÔ¨Åcient Tuning
To study the few-shot PETuning, we take VPT [1], Adapter
[12], and LoRA [2] as examples which are tuned with
limited training examples. SpeciÔ¨Åcally, we tuned the VPT,
Adapter, and LoRA framework with different proportions
of training examples on the CUB-200-2011 dataset, reduced
from 16 training samples to 8 or even 1 sample per class,
validating the performance of these methods under few-
shot learning settings. As shown in Figure 2, it is clear that
the performance of both three methods drops signiÔ¨Åcantly
when tuned with less than 4 training samples per class,
and almost fails when there is only 1 training sample per
class. This indicates that existing parameter-efÔ¨Åcient tuning
techniques may not be able to perform well under the few-
shot learning setting.
Analysis. We attribute this phenomenon to the inap-
propriate initialization of newly added parameters because
PETuning methods are used to randomly initialize these
newly added parameters with a mean value of zero, mean-
ing the newly added parameters need to learn from scratch
on downstream tasks. This leads to the newly added module
requiring a signiÔ¨Åcant amount of data for gradient updating,
which can prove challenging for downstream tasks and
limits its application for few-shot tasks. To further utilize
PETuning on limited data, the newly added parameters also
need pre-training. Therefore, we pre-train the newly added
modules to get better initialization for parameter-efÔ¨ÅcientAlgorithm 1 PVP framework based on VPT, PyTorch-like.
# For downstream prompt tuning
# type: visual prompt tuning type, "Deep" or "Shallow"
# k: visual prompt tuning tokens number
# prompt pre-training stage
# build model for prompt tokens pre-training
# type="Deep", pre-train N prompt tokens where N >= k
net=build_model(vpt_type="Deep",num_prompt_tokens=N)
for x, label in pre-train_dataloader:
loss=net.forward(x,label)
loss.backward()
# Prompt_tokens shape: (num_layer,num_tokens,embed_dim)
torch.save(net.Prompt_tokens,"ckpt")
# pre-trained prompt tuning stage
# build model for downstream prompt tuning
net=build_model(vpt_type=type,num_prompt_tokens=k)
load_prompts(net,ckpt,vpt_type=type,num_token=k, load_type)
for x, label in downstream_dataloader:
loss=net.forward(x,label)
loss.backward()
net.test_loop()
# pre-trained prompt loading stage
def load_prompts(net,ckpt,vpt_type,num_token,load_type):
if load_type=="Average":# Use averaged tokens
checkpoint=torch.mean(ckpt,dim=1)
checkpoint=checkpoint.unsqueeze(1)
checkpoint=checkpoint.expand(-1,num_token,-1)
else:# Use sequential tokens
checkpoint=ckpt
if vpt_type=="Deep":
net.Prompt_tokens=checkpoint[:,:num_token,:]
else:# vpt_type=="Shallow"
net.Prompt_tokens=checkpoint[:1,:num_token,:]
tuning on a speciÔ¨Åc task and propose our pre-trained visual
parameter-efÔ¨Åcient tuning framework. This is intuitive as
the newly added module pre-training stage can provide a
good basis and the newly added parameters only require a
few data to learn well on downstream few-shot tasks.
3.4 Pre-trained Visual Parameter-EfÔ¨Åcient Tuning
There are two stages for our Pre-trained Visual Parameter-
efÔ¨Åcient (PVP) Tuning method. As Fig. 2.1 shows, we con-
duct parameter-efÔ¨Åcient tuning on pre-train data in stage 1
and use the learned parameters to initialize the parameter-
efÔ¨Åcient tuning module for downstream tasks in stage 2.
(1) Parameter efÔ¨Åcient tuning module pre-train stage.
From Equations 1- 4, the goal of various Parameter EfÔ¨Åcient
Tuning methods is to optimize the parameters 0using
datasetD0. However, it is difÔ¨Åcult to directly optimize the
parameters 0when the downstream dataset D0is limited.
Here we use another parameter efÔ¨Åcient tuning module pre-
train datasetD00which is larger than D0to pre-train the
newly added parameters 0, as formulated below,
g0
=@F(D00;;0)
@0; (5)
(2) Downstream parameter efÔ¨Åcient tuning stage. We use
the optimized parameters 0in Equ. 5 to initialize these in
Equ. 4 for our Pre-trained Visual Parameter-efÔ¨Åcient tuning.
3.5 Versatility of PVP Tuning
The key to the PVP framework is to use pre-trained prompts
for downstream few-shot tasks. Given that current PETun-
ing methods [1], [2], [12] mainly insert diverse prompt mod-
ules into vision transformers and tune these newly added

--- PAGE 5 ---
5
CUB
 CIFAR Caltech
Flowers102
 Pets
 Sun397 SVHN
 EuroSAT PatchCamelyon
 Resisc45 Retinopathy
DMLab dSprites/loc
 SmallNORB/azi KITTI Clevr/countNABirds OxfordFlowers StanfordDogs StanfordCars
 DTD
dSprites/ori SmallNORB/ele Clevr/distance
Fig. 4. Examples of all classiÔ¨Åcation tasks evaluated. One representative picture for each dataset in FGVC and VTAB-1k.
Table 1 .Detailed infrmation of FGVC and VTAB-1k datasets.
Dataset Description # Classes Train Val Test
Fine-grained visual recognition tasks (FGVC)
CUB [42] Fine-grained bird species recognition 200
1/2/4/8/16 per class600 5,794
NABirds [43] Fine-grained bird species recognition 555 2,393 24,633
Oxford Flowers [44] Fine-grained Ô¨Çower species recognition 102 1,020 6,149
Stanford Dogs [45] Fine-grained dog species recognition 120 1,200 8,580
Stanford Cars [46] Fine-grained car classiÔ¨Åcation 196 815 8,041
Visual Task Adaptation Benchmark (VTAB) [47]
CIFAR-100 [48]
Natural100
800/1000 20010,000
Caltech101 [49] 102 6,084
DTD [50] 47 1,880
Flowers102 [51] 102 6,149
Pets [52] 37 3,669
SVHN [53] 10 26,032
Sun397 [54] 397 21,750
Patch Camelyon [55]
Specialized2
800/1000 20032,768
EuroSAT [56] 10 5,400
Resisc45 [57] 45 6,300
Retinopathy [58] 5 42,670
Clevr/count [59]
Structured8
800/1000 20015,000
Clevr/distance [59] 6 15,000
DMLab [60] 6 22,735
KITTI/distance [61] 4 711
dSprites/location [62] 16 73,728
dSprites/orientation [62] 16 73,728
SmallNORB/azimuth [63] 18 12,150
SmallNORB/elevation [63] 9 12,150
modules while keeping the transformer backbone frozen.
Hence, PVP is baseline-independent and can therefore apply
to various PETuning methods. In this section, we study
the versatility of the proposed PVP framework on VPT [1],
Adapter [12], and LoRA [2].
PVP Tuning based on VPT. The key to our approach is to
use pre-trained visual prompts for prompt tuning. SpeciÔ¨Å-
cally, we Ô¨Årst add prompt tokens into ViT and follow VPT
to initialize the prompt tokens. Next, we execute prompt
tuning on ImageNet-1k with transformer backbone pre-
trained on ImageNet-21k to get pre-trained prompt tokens.
Finally, we load the pre-trained prompt tokens rather than
tuning from scratch, before prompt tuning on downstream
few-shot tasks. Algorithm 1 shows the overall procedureof the PVP framework, including the prompt pre-training
stage, pre-trained prompt loading stage and pre-trained
prompt tuning stage. Notably, the number of prompt tokens
in VPT varies from 1 to 200 and we directly add 200 prompt
tokens into each ViT layer during the prompt pre-training
stage, therefore we can load any number of pre-trained
prompt tokens out of the 200 prompt tokens on downstream
few-shot tasks rather than perform prompt pre-training
repetitively for different prompt tokens number setting. In
particular, there are two manners to load the pre-trained
prompt tokens, which are listed below:
Sequential Loading . As the name implies, we load
the pre-trained prompt tokens sequentially. For example,
if there are Npre-trained prompt tokens in total and we
need to load Kpre-trained prompt tokens. In this case,

--- PAGE 6 ---
6
Table 2 .Quantitative results on FGVC few-shot learning.
Accuracy (%) CUB-200-2011 NABirds Oxford Flowers Stanford Dogs Stanford Cars Average
FULL16 shot 85.12 79.43 99.20 72.10 76.91 82.55
8 shot 77.36 66.60 96.42 41.85 41.20 64.69
4 shot 60.61 39.10 94.23 19.80 23.57 47.46
2 shot 14.53 9.93 56.43 3.90 5.85 18.13
1 shot 9.44 2.50 38.61 1.75 4.17 11.29
VPT16 shot 84.66 76.71 99.38 80.82 57.33 79.78
8 shot 79.10 64.73 98.75 77.11 36.31 71.20
4 shot 70.61 40.43 96.85 68.22 20.62 59.35
2 shot 53.26 27.94 92.73 49.02 8.64 46.32
1 shot 32.88 14.84 66.01 36.67 5.20 31.12
PVP (ours)16 shot 86.28 ("1.62) 80.05 ("3.34) 99.48 ("0.10) 81.77 ("0.95) 61.09 ( "3.76) 81.73 ( "1.95)
8 shot 81.53 ("2.43) 71.78 ("7.05) 99.02 ("0.27) 77.81 ("0.70) 40.57 ( "4.26) 74.14 ("2.94)
4 shot 74.37 ("3.76) 58.16 ("17.73) 98.49 ("1.64) 71.43 ("3.21) 26.08 ("5.46) 65.71 ("6.36)
2 shot 62.20 ("8.94) 53.82 ("25.88) 96.11 ("3.38) 62.32 ("13.30) 14.73 ("6.09) 57.84 ("11.52)
1 shot 49.24 ("16.36) 39.74 ("24.90) 88.84 ("22.83) 47.45 ("10.78) 10.73 ("5.53) 47.20 ("16.08)
Table 3 .Quantitative results on VTAB-1k transfer learning.
CIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Mean
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Mean
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/location
dSprites/orientation
SmallNORB/azimuth
SmallNORB/elevation
Mean
Overall Mean
Traditional Methods
Full Tune [1] 68.9 87.7 64.3 97.2 86.9 87.4 38.8 75.88 79.7 95.7 84.2 73.9 83.36 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 47.64 68.96
Linear Probe [1] 63.4 85.0 63.2 97.0 86.3 36.6 51.0 68.93 78.5 87.5 68.6 74.0 77.16 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 26.84 57.64
PETuning Methods
VPT-Shallow(ECCV‚Äô22) [1] 77.7 86.9 62.6 97.5 87.3 74.5 51.2 76.81 78.2 92.0 75.6 72.9 79.66 55.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 46.98 67.82
VPT-Deep(ECCV‚Äô22) [1] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.48 81.8 96.1 83.4 68.4 82.43 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 54.98 71.96
NOAH(arXiv‚Äô22) [13] 70.7 91.6 68.2 98.9 90.2 88.4 54.0 80.29 85.9 95.3 84.2 73.6 84.75 81.7 63.1 49.0 78.5 82.3 45.0 31.8 43.5 59.36 74.80
SSF(Neurips‚Äô22) [15] 69.0 92.6 71.5 99.4 91.8 90.2 52.9 81.57 87.4 95.9 87.4 75.5 86.55 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 58.96 75.69
FacT(AAAI‚Äô23) [16] 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.64 84.8 96.2 84.5 75.7 85.30 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.71 75.55
PVP (ours) 76.3 94.4 73.1 99.7 92.3 87.3 58.6 83.09 87.5 95.6 87.4 76.9 86.84 76.4 64.6 54.6 82.0 88.0 58.5 36.2 52.8 64.13 78.02
we directly load the Ô¨Årst Kout of the total Npre-trained
prompt tokens.
Average Loading . Different from the sequential loading
manner, we use average pre-trained prompt tokens to ini-
tialize the prompt tokens. For example, if there are Npre-
trained prompt tokens in total and we need to load Kpre-
trained prompt tokens, we average Npre-trained prompt
tokens and then expand it to Ktokens for loading.
PVP Tuning based on Adapter. Adapter insert adapter
architecture to each transformer block,
X0 X+(XWdown )Wup;
whereX2RNdis the output of Feed-Forward Network
(FFN) blocks in each transformer layer, is a nonlinear
function, Wdown2Rdh,Wup2Rhdandh << d . We
directly conduct adapter tuning on ImageNet-1k to get pre-
trained Wdown andWupfor each transformer block andthen load these pre-trained parameters of each Wdown and
Wupfor downstream adapter tuning.
PVP Tuning based on LoRA. LoRA decomposes the
increments of query transformation Wqand value transfor-
mation Wvinto low-rank Aq=v2RdrandBq=v2Rrd
wherer<<d . The query and value are then computed as
Q=V XWq=v+sXAq=vBq=v;
wheresis a hyper-parameter. Similar to Adapter, we Ô¨Årst
pre-train these Aq=vandBq=von ImageNet-1k and then use
pre-trained Aq=vandBq=vfor downstream LoRA tuning.
We show experimental results about the versatility of our
proposed method in Sec. 4.3.2.

--- PAGE 7 ---
7
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class102030405060708090T est accuracy (%)
Average over 5 datasets
FULL
Average
Sequential
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class5102030405060708090T est accuracy (%)
CUB-200-2011
FULL
Average
Sequential
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class0102030405060708090T est accuracy (%)
NABirds
FULL
Average
Sequential
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class30405060708090100T est accuracy (%)
Oxford Flowers
FULL
Average
Sequential
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class0102030405060708090T est accuracy (%)
Standford Dogs
FULL
Average
Sequential
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class010203040506070T est accuracy (%)
Standford Cars
FULL
Average
Sequential
Fig. 5. Result of two prompt tokens load manners in PVP(VPT). Average and sequential represent average loading and sequential loading manner.
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of prompt tokens203040506070T est accuracy (%)
NABirds(4 shots)
VPT
PVP(VPT)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of prompt tokens50607080T est accuracy (%)
NABirds(8 shots)
VPT
PVP(VPT)
Fig. 6. Test accuracy of VPT and our PVP based on VPT with different
numbers of prompt tokens on NABirds dataset under 4 shots and 8 shots
settings.
4 E XPERIMENT
4.1 Datasets
For our proposed method, we evaluate the few-shot learn-
ing performance on the Fine-Grained Visual Recognition
(FGVC) datasets and the transfer learning performance on
the Visual Task Adaption Benchmark (VTAB-1k).
(1) FGVC contains commonly-used Ô¨Åne-grained visual
classiÔ¨Åcation datasets, which are usually used for few-shot
learning, including CUB-200-2011 [42], NABirds [43], Ox-
ford Flowers [44], Stanford Dogs [45] and Stanford Cars [46].We follow [13], [15] to use X (X=1,2,4,8,16) samples per class
for few-shot image classiÔ¨Åcation on these datasets.
(2) VTAB-1k [47], consisting of 19 visual classiÔ¨Åcation
datasets, cover data in 3 Ô¨Åelds, including natural tasks,
specialized tasks, and structured tasks. The natural task
includes images in daily life. The specialized task includes
images captured by specialized equipment, such as medical
and satellite imagery. The structured task includes images
that require semantic understanding, such as object count-
ing. Each of the 19 datasets contains 1000 images, which
reÔ¨Çects ‚Äù1k‚Äù of the name ‚ÄùVTAB-1k‚Äù. These datasets cover
a wide range of the possible domains where downstream
tasks come from, and thus the effectiveness of PETuning
methods can be measured comprehensively.
Table 1 shows detailed information about these datasets.
Examples of FGVC and VTAB-1k benchmarks are shown
in Figure 4. Note that Clevr/count and Clevr/distance,
dSprites/location and dSprites/orientation as well as Small-
NORB/azimuth and SmallNORB/elevation are actually the
same dataset but for different tasks respectively.
4.2 Augmentation and Hyper-Parameters
We adopt a standard image augmentation strategy during
training: normalize with ImageNet means and standard
deviation, randomly resize crop to 224 224 and random
horizontal Ô¨Çip for Ô¨Åve FGVC datasets and resize to 224 224
for the VTAB-1k suite. Following VPT [1], we conduct a grid
search to Ô¨Ånd the tuning-speciÔ¨Åc hyper-parameters, learning
rate, and weight decay values for each task.

--- PAGE 8 ---
8
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class102030405060708090T est accuracy (%)
Average over 5 datasets
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class5102030405060708090T est accuracy (%)
CUB-200-2011
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class0102030405060708090T est accuracy (%)
NABirds
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class30405060708090100T est accuracy (%)
Oxford Flowers
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class0102030405060708090T est accuracy (%)
Standford Dogs
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
Number of samples per class010203040506070T est accuracy (%)
Standford Cars
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
Fig. 7. Result of PVP framework based on Adapter and LoRA.
4.3 Fine-Grained Few-Shot Learning
For few-shot learning performance, we compare our meth-
ods to various competitive baselines, including VPT [1],
Adapter [12], and LoRA [2]. For all baselines, we use a ViT-
B/16 [3] pre-trained on supervised ImageNet-21K as the
transformer backbone. All our experiments are conducted
on NVIDIA A100-40GB GPUs.
4.3.1 PVP based on VPT
We pre-train 200 prompt tokens on ImageNet-1k for down-
stream prompt tuning where the number is approximately
close to the number of image patch tokens (196) within
each Multi-head Self Attention (MSA) for ViT-B/16 [3] ar-
chitecture. For each downstream dataset, we follow VPT [1]
to grid search the number of prompt tokens for a fair
comparison.
Performance under different few-shot learning set-
tings . As shown in Table 2, we quantitatively compare the
performance achieved by FULL tuning, VPT, and the pro-
posed PVP based on VPT under various few-shot settings on
Ô¨Åve different datasets. It can be seen that when the number
of training samples is sufÔ¨Åcient, such as 16 samples per
class, PVP based on VPT reaches over 99% test accuracy
on Oxford Flowers and over 86% test accuracy on CUB-200-
2011, which is comparable to full Ô¨Åne-tuning using all the
training samples on these two datasets. More importantly,
pre-trained prompt tokens show signiÔ¨Åcant performance
improvement in the few-shot regime, like 1 or 2 shots perclass. These results demonstrate that pre-trained prompt
tokens are essential for applying large transformer models
to few-shot tasks.
Tokens load manners . In our implementation, there are
two manners to load the pre-trained prompt tokens. We
study the effect of these two manners. As Figure 5 shows,
loading pre-trained prompts sequentially outperforms that
of averagely and we use the sequential manner as the
default loading setting in the rest of this paper. We attribute
the reason of low performance to the absence of positional
information when loading pre-trained prompt tokens av-
eragely, where the prompt tokens are averaged Ô¨Årst and
then expanded to the required tokens number, therefore, the
positional information is missing.
Prompt length sensitivity . In VPT [1], the num-
ber of newly added prompt tokens is chosen from
f1,5,10,50,100,200gand they use the validation set of each
dataset to determine the best prompt length. We also con-
duct experiments on prompt tokens number to validate the
sensitivity. As Figure 6 shows, the accuracy of VPT varies
greatly (more than 25% under 4 shots setting) when the
number of prompt tokens is different, while the accuracy
of our PVP framework based on VPT is more consis-
tent and robust (less than 2% under 4 shots setting) on
NABirds dataset. We attribute the reason to the pre-training
of prompt tokens since the pre-training process gives the
prompt tokens better initialization and thus they can learn
on limited data steadily.

--- PAGE 9 ---
9
4.3.2 PVP based on Adapter and LoRA
Figure 7 shows the accuracy of the Adapter and LoRA with
or w/o prompt pre-training under various shots settings.
It can be seen that the proposed PVP framework brings
accuracy gains on both Adapter and LoRA in few-shot
learning settings, which further validates the importance of
prompt pre-training as well as the versatility of the PVP
framework.
4.4 VTAB-1k Transfer Learning
For transfer learning performance, we compare our meth-
ods to various PETuning methods, including VPT-Deep [1],
VPT-Shallow [1], NOAH [13], SSF [15] and FacT [16]. We
use a ViT-B/16 [3] pre-trained on supervised ImageNet-21K
as the transformer backbone and use VPT as our baseline.
Following VPT [1], we search the number of prompt tokens
fromf1,10,50,100,200gfor each dataset. For convenience, we
directly use prompt tokens pre-trained on ImageNet-1k for
both natural tasks, specialized tasks, and structured tasks.
All our experiments are conducted on NVIDIA A100-40GB
GPUs.
Experimental results are shown in Table 3, from which
we can see that:
1) Our PVP(VPT) reaches comparable performance with
respect to previous state-of-the-art PETuning methods. On
VTAB-1k benchmark, PVP(VPT) achieves the highest accu-
racy on 16 datasets out of 19 datasets in total and achieves
1.52%, 0.29 %and 3.42 %average accuracy improvement
on natural tasks, specialized tasks, and structured tasks,
respectively.
2) Though we use prompts pre-trained on ImageNet-1k,
which is mainly about natural images, our PVP framework
based on VPT also performs well on specialized tasks and
structured tasks.
5 C ONCLUSION
In this paper, we study recent Parameter-EfÔ¨Åcient Tuning
(PETuning) methods and Ô¨Årst observe that current PETuning
methods perform poorly in the few-shot scenario. Then, we
propose Pre-trained Visual Parameter-efÔ¨Åcient (PVP) Tun-
ing, a conceptually simple and intuitive framework to lever-
age large-scale pre-trained transformer models for few-shot
tasks. The key to our method is to pre-train the prompt mod-
ules of recent PETuning methods, enabling better initializa-
tion for downstream PETuning. Extensive experiments on
VPT, Adapter, and LoRA show the effectiveness and versa-
tility of the PVP framework in terms of few-shot learning.
Besides the few-shot capability, PVP also shows comparable
transfer learning ability to recent PETuning methods. On
VTAB-1k benchmark, PVP achieves state-of-the-art results
on 16 out of total 19 datasets and improves the average
accuracy of 3.34 %, 2.85 %, and 3.66 %for VTAB-Natural,
VTAB-Specialized and VTAB-Structured, respectively. We
hope our work can inspire future research on more efÔ¨Åcient
and lightweight utilization of large vision models. However,
Pre-trained Visual Parameter-efÔ¨Åcient (PVP) Tuning is an
empirical method with experiments proof currently. Though
state-of-the-art results were achieved on FGVC and VTAB-
1k benchmarks, the theoretical interpretation behind PVP is
still under exploration.Acknowledgement. This work is supported by the Na-
tional Natural Science Foundation of China under Grants
62006241, and 61902415.

--- PAGE 10 ---
10
REFERENCES
[1] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
and S.-N. Lim, ‚ÄúVisual prompt tuning,‚Äù 2022.
[2] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, ‚ÄúLora: Low-rank adaptation of large language mod-
els,‚Äù in The Tenth International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022.
[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:
Transformers for image recognition at scale,‚Äù 2021.
[4] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
‚ÄúSwin transformer: Hierarchical vision transformer using shifted
windows,‚Äù in 2021 IEEE/CVF International Conference on Computer
Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 . IEEE,
2021, pp. 9992‚Äì10 002.
[5] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al. , ‚ÄúImagenet
large scale visual recognition challenge,‚Äù International journal of
computer vision , vol. 115, no. 3, pp. 211‚Äì252, 2015.
[6] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, ‚ÄúHow transferable
are features in deep neural networks?‚Äù MIT Press , 2014.
[7] M. Noroozi and P . Favaro, ‚ÄúUnsupervised learning of visual
representations by solving jigsaw puzzles,‚Äù Springer, Cham , 2016.
[8] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, and J. Malik, ‚ÄúSide-
tuning: A baseline for network adaptation via additive side net-
works,‚Äù 2020.
[9] E. B. Zaken, S. Ravfogel, and Y. Goldberg, ‚ÄúBitÔ¨Åt: Simple
parameter-efÔ¨Åcient Ô¨Åne-tuning for transformer-based masked
language-models,‚Äù 2021.
[10] J. Pfeiffer, A. Kamath, A. R ¬®uckl ¬¥e, K. Cho, and I. Gurevych,
‚ÄúAdapterfusion: Non-destructive task composition for transfer
learning,‚Äù arXiv preprint arXiv:2005.00247 , 2020.
[11] J. Pfeiffer, A. R ¬®uckl ¬¥e, C. Poth, A. Kamath, I. Vuli ¬¥c, S. Ruder,
K. Cho, and I. Gurevych, ‚ÄúAdapterhub: A framework for adapting
transformers,‚Äù arXiv preprint arXiv:2007.07779 , 2020.
[12] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Larous-
silhe, A. Gesmundo, M. Attariyan, and S. Gelly, ‚ÄúParameter-
efÔ¨Åcient transfer learning for NLP,‚Äù in Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , ser. Proceedings of Machine
Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds.,
vol. 97. PMLR, 2019, pp. 2790‚Äì2799.
[13] Y. Zhang, K. Zhou, and Z. Liu, ‚ÄúNeural prompt search,‚Äù CoRR ,
vol. abs/2206.04673, 2022.
[14] S. Jie and Z. Deng, ‚ÄúConvolutional bypasses are better vision
transformer adapters,‚Äù CoRR , vol. abs/2207.07039, 2022.
[15] D. Lian, Z. Daquan, J. Feng, and X. Wang, ‚ÄúScaling & shifting your
features: A new baseline for efÔ¨Åcient model tuning,‚Äù 10 2022.
[16] S. Jie and Z. H. Deng, ‚ÄúFact: Factor-tuning for lightweight adapta-
tion on vision transformer,‚Äù 2022.
[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
inAdvances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , I. Guyon, U. von Luxburg, S. Bengio,
H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, and R. Garnett,
Eds., 2017, pp. 5998‚Äì6008.
[18] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-
training of deep bidirectional transformers for language under-
standing,‚Äù 2018.
[19] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, and D. Amodei,
‚ÄúLanguage models are few-shot learners,‚Äù 2020.
[20] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P . J. Liu, ‚ÄúExploring the limits of transfer
learning with a uniÔ¨Åed text-to-text transformer,‚Äù 2019.
[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P . Mishkin, and J. Clark, ‚ÄúLearning transfer-
able visual models from natural language supervision,‚Äù 2021.
[22] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J ¬¥egou, ‚ÄúTraining data-efÔ¨Åcient image transformers & distil-
lation through attention,‚Äù in Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event , ser. Proceedings of Machine Learning Research, M. Meila
and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 10 347‚Äì10 357.[23] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, ‚ÄúIncorporating
convolution designs into visual transformers,‚Äù in 2021 IEEE/CVF
International Conference on Computer Vision, ICCV 2021, Montreal,
QC, Canada, October 10-17, 2021 . IEEE, 2021, pp. 559‚Äì568.
[24] Y. Li, K. Zhang, J. Cao, R. Timofte, and L. V . Gool, ‚ÄúLocalvit: Bring-
ing locality to vision transformers,‚Äù CoRR , vol. abs/2104.05707,
2021.
[25] L. Hagstr ¬®om and R. Johansson, ‚ÄúHow to adapt pre-trained vision-
and-language models to a text-only input?‚Äù in Proceedings of the
29th International Conference on Computational Linguistics, COLING
2022, Gyeongju, Republic of Korea, October 12-17, 2022 , N. Calzolari,
C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P . Ryu,
H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P . Paggio, N. Xue,
S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na,
Eds. International Committee on Computational Linguistics,
2022, pp. 5582‚Äì5596.
[26] R. Upadhyay, P . C. Chhipa, R. Phlypo, R. Saini, and M. Liwicki,
‚ÄúMulti-task meta learning: learn how to adapt to unseen tasks,‚Äù
CoRR , vol. abs/2210.06989, 2022.
[27] Y. Li, F. Luo, C. Tan, M. Wang, S. Huang, S. Li, and J. Bai,
‚ÄúParameter-efÔ¨Åcient sparsity for large language models Ô¨Åne-
tuning,‚Äù in Proceedings of the Thirty-First International Joint Confer-
ence on ArtiÔ¨Åcial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July
2022 , L. D. Raedt, Ed. ijcai.org, 2022, pp. 4223‚Äì4229.
[28] X. Zhou, R. Ma, Y. Zou, X. Chen, T. Gui, Q. Zhang, X. Huang,
R. Xie, and W. Wu, ‚ÄúMaking parameter-efÔ¨Åcient tuning more efÔ¨Å-
cient: A uniÔ¨Åed framework for classiÔ¨Åcation tasks,‚Äù in Proceedings
of the 29th International Conference on Computational Linguistics,
COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022 ,
N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi,
P . Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P . Paggio, N. Xue,
S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na,
Eds. International Committee on Computational Linguistics,
2022, pp. 7053‚Äì7064.
[29] Z. Yang, M. Ding, Y. Guo, Q. Lv, and J. Tang, ‚ÄúParameter-
efÔ¨Åcient tuning makes a good classiÔ¨Åcation head,‚Äù CoRR , vol.
abs/2210.16771, 2022.
[30] Y. Sung, J. Cho, and M. Bansal, ‚ÄúLST: ladder side-tuning for
parameter and memory efÔ¨Åcient transfer learning,‚Äù CoRR , vol.
abs/2206.06522, 2022.
[31] Y. Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, W. Yih,
and M. Khabsa, ‚ÄúUnipelt: A uniÔ¨Åed framework for parameter-
efÔ¨Åcient language model tuning,‚Äù CoRR , vol. abs/2110.07577, 2021.
[32] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen,
C. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H. Zheng,
J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun, ‚ÄúDelta tuning: A com-
prehensive study of parameter efÔ¨Åcient methods for pre-trained
language models,‚Äù CoRR , vol. abs/2203.06904, 2022.
[33] M. Chen, H. Peng, J. Fu, and H. Ling, ‚ÄúAutoformer: Searching
transformers for visual recognition,‚Äù 2021.
[34] Q. Hu, B. Yang, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, and
A. Markham, ‚ÄúSqn: Weakly-supervised semantic segmentation of
large-scale 3d point clouds with 1000x fewer labels,‚Äù arXiv preprint
arXiv:2104.04891 , 2021.
[35] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni, and A. Markham,
‚ÄúSensaturban: Learning semantics from urban-scale photogram-
metric point clouds,‚Äù International Journal of Computer Vision , vol.
130, no. 2, pp. 316‚Äì343, 2022.
[36] Y. Gu, X. Han, Z. Liu, and M. Huang, ‚ÄúPPT: Pre-trained prompt
tuning for few-shot learning,‚Äù in Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) . Dublin, Ireland: Association for Computational
Linguistics, May 2022, pp. 8410‚Äì8423.
[37] I. V . Robert, I. Balaevi, E. Wallace, F. Petroni, S. Singh, and
S. Riedel, ‚ÄúCutting down on prompts and parameters: Simple few-
shot learning with language models,‚Äù 2021.
[38] K. He, Y. Huang, R. Mao, T. Gong, C. Li, and E. Cambria, ‚ÄúVirtual
prompt pre-training for prototype-based few-shot relation extrac-
tion,‚Äù Expert Syst. Appl. , vol. 213, no. Part, p. 118927, 2023.
[39] T. Bansal, S. Alzubi, T. Wang, J. Lee, and A. McCallum, ‚ÄúMeta-
adapters: Parameter efÔ¨Åcient few-shot Ô¨Åne-tuning through meta-
learning,‚Äù in International Conference on Automated Machine Learn-
ing, AutoML 2022, 25-27 July 2022, Johns Hopkins University, Balti-
more, MD, USA , ser. Proceedings of Machine Learning Research,
I. Guyon, M. Lindauer, M. van der Schaar, F. Hutter, and R. Gar-
nett, Eds., vol. 188. PMLR, 2022, pp. 19/1‚Äì18.

--- PAGE 11 ---
11
[40] J. Zhou, L. Tian, H. Yu, Z. Xiao, H. Su, and J. Zhou, ‚ÄúDual
context-guided continuous prompt tuning for few-shot learning,‚Äù
inFindings of the Association for Computational Linguistics: ACL
2022, Dublin, Ireland, May 22-27, 2022 , S. Muresan, P . Nakov, and
A. Villavicencio, Eds. Association for Computational Linguistics,
2022, pp. 79‚Äì84.
[41] G. Cui, S. Hu, N. Ding, L. Huang, and Z. Liu, ‚ÄúPrototypical
verbalizer for prompt-based few-shot tuning,‚Äù in Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 ,
S. Muresan, P . Nakov, and A. Villavicencio, Eds. Association for
Computational Linguistics, 2022, pp. 7014‚Äì7024.
[42] C. Wah, S. Branson, P . Welinder, P . Perona, and S. Belongie, ‚ÄúThe
caltech-ucsd birds-200-2011 dataset,‚Äù california institute of technol-
ogy, 2011.
[43] G. V . Horn, S. Branson, R. Farrell, S. Haber, and S. Belongie,
‚ÄúBuilding a bird recognition app and large scale dataset with
citizen scientists: The Ô¨Åne print in Ô¨Åne-grained dataset collection,‚Äù
inComputer Vision & Pattern Recognition , 2015.
[44] Nilsback, ME, and Zisserman, ‚ÄúAutomated Ô¨Çower classiÔ¨Åcation
over a large number of classes,‚Äù -, vol. -, no. -, pp. 722‚Äì729, 2008.
[45] A. Khosla, N. Jayadevaprakash, B. Yao, and F. Li, ‚ÄúL.: Novel
dataset for Ô¨Åne-grained image categorization,‚Äù 2013.
[46] T. Gebru, J. Krause, Y. Wang, D. Chen, and F. F. Li, ‚ÄúFine-grained
car detection for visual census estimation,‚Äù 2017.
[47] X. Zhai, J. Puigcerver, A. Kolesnikov, P . Ruyssen, C. Riquelme,
M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, and A. Doso-
vitskiy, ‚ÄúA large-scale study of representation learning with the
visual task adaptation benchmark,‚Äù 2019.
[48] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny
images,‚Äù 2012.
[49] F. F. Li, Member, IEEE, R. Fergus, and S. Member, ‚ÄúOne-shot
learning of object categories,‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence , vol. 28, no. 4, pp. 594‚Äì611, 2006.
[50] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi,
‚ÄúDescribing textures in the wild,‚Äù 2013.
[51] M. E. Nilsback and A. Zisserman, ‚ÄúAutomated Ô¨Çower classiÔ¨Åca-
tion over a large number of classes,‚Äù in Sixth Indian Conference
on Computer Vision, Graphics & Image Processing, ICVGIP 2008,
Bhubaneswar, India, 16-19 December 2008 , 2008.
[52] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar, ‚Äú[ieee
2012 ieee conference on computer vision and pattern recognition
(cvpr) - providence, ri (2012.06.16-2012.06.21)] 2012 ieee conference
on computer vision and pattern recognition - cats and dogs,‚Äù pp.
3498‚Äì3505, 2012.
[53] Y. Netzer, T. Wang, A. Coates, A. Bissacco, and A. Y. Ng, ‚ÄúReading
digits in natural images with unsupervised feature learning,‚Äù
2011.
[54] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, ‚ÄúSun
database: Large-scale scene recognition from abbey to zoo,‚Äù in
Computer Vision & Pattern Recognition , 2010.
[55] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling,
‚ÄúRotation equivariant cnns for digital pathology,‚Äù in International
Conference on Medical image computing and computer-assisted inter-
vention . Springer, 2018, pp. 210‚Äì218.
[56] P . Helber, B. Bischke, A. Dengel, and D. Borth, ‚ÄúEurosat: A novel
dataset and deep learning benchmark for land use and land cover
classiÔ¨Åcation,‚Äù 2017.
[57] C. Gong, J. Han, and X. Lu, ‚ÄúRemote sensing image scene classi-
Ô¨Åcation: Benchmark and state of the art,‚Äù Proceedings of the IEEE ,
vol. 105, no. 10, pp. 1865‚Äì1883, 2017.
[58] R. Akhunzyanov and S. Ovcharenko, ‚ÄúDiabetic retinopathy detec-
tion,‚Äù 2016.
[59] J. Johnson, B. Hariharan, L. Maaten, F. F. Li, and R. Girshick,
‚ÄúClevr: A diagnostic dataset for compositional language and el-
ementary visual reasoning,‚Äù IEEE , 2017.
[60] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright,
H. K ¬®uttler, A. Lefrancq, S. Green, V . Vald ¬¥es, A. Sadik et al. ,
‚ÄúDeepmind lab,‚Äù arXiv preprint arXiv:1612.03801 , 2016.
[61] A. Geiger, P . Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets
robotics: The kitti dataset,‚Äù International Journal of Robotics Research ,
vol. 32, no. 11, pp. 1231‚Äì1237, 2013.
[62] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner, ‚Äúdsprites:
Disentanglement testing sprites dataset,‚Äù 2017.
[63] Y. Lecun, J. H. Fu, and L. Bottou, ‚ÄúLearning methods for generic
object recognition with invariance to pose and lighting,‚Äù in Pro-ceedings of the 2004 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, 2004. CVPR 2004 , 2004.
