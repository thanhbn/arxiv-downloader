# Định tuyến Adapter Đa Đầu 
cho Khái quát hóa Đa Nhiệm vụ

Lucas Caccia Edoardo Ponti Zhan Su Matheus Pereira
Nicolas Le Roux Alessandro Sordoni
Microsoft Research, McGill University, MILA,
University of Edinburgh, Université de Montréal, University of Copenhagen
lucas.page-caccia@mail.mcgill.ca,alsordon@microsoft.com

Tóm tắt

Tinh chỉnh hiệu quả tham số (PEFT) cho khái quát hóa đa nhiệm vụ bao gồm việc tiền huấn luyện các adapter trên tập huấn luyện đa nhiệm vụ trước khi thích ứng few-shot cho các nhiệm vụ kiểm tra. Polytropon [Ponti et al., 2023] (Poly) cùng lúc học một kho tàng các adapter và một hàm định tuyến lựa chọn một tập con (có kích thước biến đổi) các adapter cho mỗi nhiệm vụ trong cả quá trình tiền huấn luyện và thích ứng few-shot. Trong bài báo này, chúng tôi điều tra vai trò mà việc định tuyến adapter đóng trong thành công của nó và thiết kế các biến thể mới dựa trên những phát hiện của chúng tôi. Đầu tiên, chúng tôi xây dựng dựa trên trực giác rằng việc định tuyến chi tiết hơn cung cấp khả năng biểu đạt tốt hơn. Do đó, chúng tôi đề xuất MHR (Multi-Head Routing), kết hợp các khối tham số từ các adapter khác nhau và vượt trội hơn Poly trong cùng một ngân sách tham số tương đương; chỉ bằng cách tinh chỉnh hàm định tuyến mà không tinh chỉnh các adapter (MHR-z), chúng tôi đạt được hiệu suất cạnh tranh với hiệu quả tham số cực kỳ cao. Thứ hai, chúng tôi thấy rằng hiệu suất Poly/MHR là kết quả của việc tối ưu hóa đa nhiệm vụ tốt hơn, thay vì các bias quy nạp modular tạo điều kiện thuận lợi cho việc tái kết hợp adapter và thích ứng cục bộ, như đã được giả thuyết trước đây. Thực tế, chúng tôi thấy rằng MHR thể hiện độ căn chỉnh gradient cao giữa các nhiệm vụ huấn luyện. Chúng tôi thấy rằng việc định tuyến có lợi nhất trong quá trình tiền huấn luyện đa nhiệm vụ hơn là trong quá trình thích ứng few-shot và đề xuất MHR-µ, loại bỏ việc định tuyến và tinh chỉnh trung bình của các adapter đã tiền huấn luyện trên mỗi nhiệm vụ downstream. Điều này thiết lập MHR-µ như một phương pháp hiệu quả cho việc tinh chỉnh adapter đơn. Chúng tôi cũng chỉ ra rằng MHR-µ có thể được sử dụng như một phương pháp chuyển giao zero-shot hiệu quả bằng cách huấn luyện trung bình của các adapter đã tiền huấn luyện thêm vài bước trên tập huấn luyện đa nhiệm vụ: điều này mang lại lợi ích lên đến 3% về độ chính xác tuyệt đối so với các baseline. Mã nguồn có sẵn tại https://github.com/microsoft/mttl.

1 Giới thiệu

Khả năng huấn luyện các mô hình hiệu quả với số lượng tương đối nhỏ dữ liệu huấn luyện là vô cùng quan trọng do sự khan hiếm các ví dụ được chú thích cho hầu hết các nhiệm vụ. Một cách tiếp cận học few-shot hiệu quả là tận dụng các mô hình lớn đã được tiền huấn luyện trên một lượng lớn dữ liệu không nhãn và tinh chỉnh chúng trên một số ít ví dụ có sẵn cho mỗi nhiệm vụ downstream. Để giảm chi phí bộ nhớ của việc nhân đôi toàn bộ mảng tham số cho mỗi nhiệm vụ downstream, các phương pháp gần đây sử dụng các phương pháp tinh chỉnh hiệu quả tham số (PEFT), chẳng hạn như LoRA [Hu et al., 2022], SFT [Ansell et al., 2022], hoặc (IA)3[Liu et al., 2022]. Những phương pháp này chỉ tinh chỉnh các adapter trong khi để mô hình tiền huấn luyện 'bị đóng băng'.

Tuy nhiên, vẫn chưa rõ làm thế nào để tận dụng tốt nhất một tập các nhiệm vụ huấn luyện để khái quát hóa tốt hơn cho một tập các nhiệm vụ kiểm tra chưa được thấy một cách tiết kiệm mẫu, chỉ dựa trên một vài ví dụ. Một giải pháp đơn giản là thực hiện tiền huấn luyện đa nhiệm vụ, tức là trước tiên huấn luyện mô hình lớn trên tập hợp các ví dụ từ các nhiệm vụ huấn luyện, sau đó tinh chỉnh mô hình thu được cho nhiệm vụ kiểm tra [Liu et al., 2022, Ye et al., 2021]. Tuy nhiên, giải pháp này không tính đến việc các nhiệm vụ kiểm tra có thể yêu cầu giải quyết các tổ hợp con khác nhau của các vấn đề con so với các nhiệm vụ huấn luyện [Vu et al., 2020], do đó không thể đạt được khái quát hóa tổng hợp [Rosenbaum et al., 2019, Ponti, 2021]. Hơn nữa, việc chuyên biệt hóa mô hình cho các nhiệm vụ khác nhau trong quá trình huấn luyện có thể dẫn đến chuyển giao tiêu cực, do các gradient tương ứng của chúng không được căn chỉnh [Wang et al., 2021].

Một số phương pháp PEFT đã được đề xuất để cho phép khái quát hóa đa nhiệm vụ tốt hơn bằng cách huấn luyện các adapter (hoặc soft prompt) trên mỗi nhiệm vụ một cách độc lập [Pfeiffer et al., 2021, Vu et al., 2021, Asai et al., 2022, Chronopoulou et al., 2023]. Với một nhiệm vụ kiểm tra mới, các tham số từ các nhiệm vụ huấn luyện tương tự được tổng hợp, điều này cho phép chuyển giao. Mặc dù việc chỉ có các tham số đặc thù cho nhiệm vụ là một chiến lược hiệu quả để giảm thiểu sự can thiệp giữa các nhiệm vụ huấn luyện, nó cũng ngăn cản bất kỳ chuyển giao tích cực nào trong cùng một nhóm nhiệm vụ. Polytropon (Poly) gần đây đã được đề xuất bởi Ponti et al. [2023] để giải quyết những vấn đề này: mô hình giả định rằng các adapter đặc thù cho nhiệm vụ là các tổ hợp đã học của một kho tàng các adapter cơ sở hoặc module có thể tái sử dụng. Trong thực tế, mỗi module được triển khai như một adapter LoRA [Hu et al., 2022], sửa đổi một mô hình tiền huấn luyện lớn, chẳng hạn như T5 [Raffel et al., 2020]. Trong cả quá trình tiền huấn luyện đa nhiệm vụ và thích ứng few-shot, Poly học cả kho tàng các adapter và một ma trận định tuyến nhiệm vụ-module (được thư giãn liên tục) nhị phân, xác định module nào hoạt động cho mỗi nhiệm vụ. Mặc dù Poly cho thấy kết quả đầy hứa hẹn, một số câu hỏi vẫn chưa được trả lời: 1) Khả năng biểu đạt của hàm định tuyến có quan trọng không? 2) Tại sao các phương pháp PEFT dựa trên định tuyến mang lại hiệu suất vượt trội? 3) Việc định tuyến có hữu ích trong cả quá trình tiền huấn luyện đa nhiệm vụ và thích ứng few-shot không?

Để trả lời câu hỏi đầu tiên, chúng tôi đề xuất một hàm định tuyến mới, MHR, kết hợp các adapter ở mức chi tiết hơn. Khác với Poly, nơi các quyết định định tuyến được thực hiện cho mỗi adapter như một toàn thể, trong MHR chúng tôi kết hợp tuyến tính các khối của các chiều adapter (tức là các head), mỗi khối với các hệ số kết hợp khác nhau. Chúng tôi đánh giá MHR và một loạt các baseline cạnh tranh cho thích ứng nhiệm vụ few-shot trên bộ nhiệm vụ T0 [Sanh et al., 2022] và Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Dựa trên kết quả của chúng tôi, chúng tôi báo cáo rằng MHR vượt trội hơn Poly và các baseline adapter đơn. Ngoài ra, chúng tôi chỉ ra rằng, nhờ vào khả năng biểu đạt tăng cường của hàm định tuyến, có thể tinh chỉnh chỉ các tham số của hàm định tuyến (và không phải các adapter) trong quá trình thích ứng few-shot: phương pháp kết quả, MHR-z, mang lại hiệu suất cạnh tranh trong khi yêu cầu ít tham số hơn nhiều bậc.

Về câu hỏi thứ hai và thứ ba, chúng tôi khám phá ra rằng việc tối ưu hóa trong quá trình tiền huấn luyện đa nhiệm vụ đóng vai trò quan trọng trong việc giải thích hiệu suất downstream của các phương pháp PEFT dựa trên định tuyến. Cụ thể, chúng tôi thấy rằng MHR thể hiện độ tương tự cosine cao hơn giữa các gradient từ các nhiệm vụ khác nhau so với Poly và huấn luyện đa nhiệm vụ adapter đơn. Do đó, việc định tuyến cho phép chuyển giao kiến thức nhiều hơn và ít can thiệp hơn giữa các nhiệm vụ trong quá trình tiền huấn luyện đa nhiệm vụ. Phát hiện này dẫn chúng tôi đến việc điều tra xem liệu việc định tuyến có hữu ích cũng trong quá trình thích ứng few-shot hay không. Đã được giả thuyết [Ponti et al., 2023] rằng một trong những lý do đằng sau hiệu suất của Poly nằm ở bias quy nạp của kiến trúc modular, cho phép các nhiệm vụ kiểm tra tái kết hợp và thích ứng cục bộ các module có liên quan nhất. Để kiểm tra giả thuyết này, chúng tôi đề xuất MHR-µ, nơi hàm định tuyến được loại bỏ và tất cả các tham số adapter có sẵn được tính trung bình trước khi thích ứng few-shot. Chúng tôi thấy rằng MHR-µ có thể phục hồi hiệu suất của MHR, gợi ý rằng các lợi ích của Poly/MHR chỉ là kết quả của việc tối ưu hóa đa nhiệm vụ tốt hơn. Cuối cùng, chúng tôi chỉ ra rằng MHR-µ cũng có thể được sử dụng như một phương pháp chuyển giao zero-shot hiệu quả bằng cách huấn luyện trung bình của các adapter đã tiền huấn luyện thêm vài bước trên tập huấn luyện đa nhiệm vụ. Điều này mang lại lợi ích lên đến 3% về độ chính xác tuyệt đối so với các baseline mạnh như T0-11B.

2 Kiến thức nền tảng

Trong khái quát hóa đa nhiệm vụ, chúng ta được cung cấp một tập các nhiệm vụ T={T1, ..,T|T |}, với mỗi nhiệm vụ Ti có tập dữ liệu chứa một tập các mẫu Di={(x1, y1), ...,(xn, yn)}. Tập tất cả các nhiệm vụ được phân chia thành các nhiệm vụ huấn luyện và kiểm tra, T=Ttrain∪ Teval, và mục tiêu là tận dụng dữ liệu trong Ttrain và chuyển giao kiến thức để tạo điều kiện thuận lợi cho việc học các nhiệm vụ kiểm tra Teval. Đối với tất cả các phương pháp được thảo luận, việc học diễn ra trong hai giai đoạn, loại trừ việc tiền huấn luyện không giám sát ban đầu của xương sống mô hình ngôn ngữ trên một corpus riêng biệt. Giai đoạn đầu tiên bao gồm tiền huấn luyện đa nhiệm vụ, trong đó hoặc là một adapter, chẳng hạn như LoRA hoặc (IA)3, hoặc toàn bộ xương sống được huấn luyện trên tập các nhiệm vụ huấn luyện Ttrain. Giai đoạn thứ hai bao gồm thích ứng few-shot, nơi các adapter đã học được tinh chỉnh một cách độc lập trên mỗi nhiệm vụ kiểm tra trong Teval. Chúng tôi tuân theo quy trình từ [Raffel et al., 2020] và diễn đạt mỗi nhiệm vụ như một vấn đề văn bản-thành-văn bản, cho phép huấn luyện maximum-likelihood tiêu chuẩn với teacher forcing [Bengio et al., 2015] và hàm mất mát cross-entropy.

2.1 Adapters: LoRA & (IA)3

LoRA [Hu et al., 2022] và (IA)3[Liu et al., 2022] là hai kiến trúc adapter được đề xuất gần đây đạt được sự cân bằng cạnh tranh giữa hiệu suất và hiệu quả tham số [Karimi Mahabadi et al., 2021, Liu et al., 2022]. Đối với mỗi phép biến đổi tuyến tính tương ứng với query (q), key (k), value (v) và output (o) của các lớp self-attention, LoRA sửa đổi các tham số mô hình cơ sở như sau:

hq,k,v,o=Wq,k,v,o0 x+s·Aq,k,v,o(Bq,k,v,o)⊤x, (LoRA)

trong đó W0 là các trọng số (đóng băng) của mô hình tiền huấn luyện (ví dụ T5 [Raffel et al., 2020]). A,B∈Rd×r là các tham số có thể học với rank thấp và s≥1 là một siêu tham số vô hướng có thể điều chỉnh. (IA)3, mặt khác, sửa đổi các biểu diễn key và value trong self-attention theo từng phần tử, và cũng sửa đổi MLP feed-forward (f):

hk,v=lk,v⊙(Wk,v0x);hf= (lf⊙γ(Wf1x))Wf2, ((IA)3)

trong đó lk,v,f∈Rd là các tham số có thể học, Wf1,2 là các tham số đóng băng của lớp feed-forward trong xương sống, và γ là một phi tuyến tính. Để rõ ràng, chúng tôi sẽ bỏ các chỉ số trên q, k, v, o trong phần còn lại của bài báo.

2.2 Polytropon: Định tuyến Adapter

Các phương pháp adapter điển hình hoặc là chia sẻ hoàn toàn các adapter giữa các nhiệm vụ hoặc huấn luyện các adapter riêng lẻ cho mỗi nhiệm vụ. Poly giải quyết vấn đề đa nhiệm vụ bằng cách chia sẻ một cách mềm mại các tham số adapter giữa các nhiệm vụ. Mỗi lớp Poly chứa 1) một kho tàng các module adapter M={ϕ1, . . . , ϕm} với |M| ≪ |T| ; 2) một hàm định tuyến r(·) chọn tập con nào của các module để kết hợp cho mỗi nhiệm vụ.

Mỗi module tương ứng với một adapter LoRA, trong đó ϕi là các tham số liên quan A(i),B(i)∈Rd×r. r(·) được triển khai như một ma trận định tuyến nhiệm vụ-module Z∈R|T|×|M|. zτ=Zτ,:∈R|M| là một vector định tuyến của nhiệm vụ Tτ, với ô Zτ,j là logits xác suất sử dụng module ϕj cho nhiệm vụ Tτ trong lớp hiện tại. Khác với mixture-of-experts [Fedus et al., 2022], thực hiện định tuyến top-k ở mức token, Z hội tụ thành một ma trận nhị phân, xác định một phân vùng mềm trên các module. Điều này được đạt bằng cách sử dụng phân phối Gumbel-sigmoid [Jang et al., 2017] trong quá trình huấn luyện, với ˆZτ,j∼Gumbel(Zτ,j). Tại mỗi lượt truyền thẳng, Poly có thể được định nghĩa như:

Aτ=Σi αiA(i);Bτ=Σi αiB(i) (Poly)

trong đó αi=ˆZτ,i/Σj ˆZτ,j, và A(i),B(i),Aτ,Bτ∈Rd×r. Chúng tôi chuẩn hóa các hệ số trộn ˆZτ,i cho mỗi nhiệm vụ để đảm bảo rằng số lượng module hoạt động không ảnh hưởng đến norm của Aτ,Bτ. Nhìn chung, cách tiếp cận này cho phép các tập con khác nhau của các module được kích hoạt cho lớp hiện tại và được kết hợp theo cách đặc thù cho nhiệm vụ. Theo LoRA, đầu ra của lớp Poly được thêm vào đầu ra của lớp gốc của xương sống đóng băng: h=W0x+sAτ(Bτ)⊤x.

Trong quá trình tiền huấn luyện đa nhiệm vụ, đối với mỗi phép chiếu query, key, value và output trong các lớp self-attention, các tham số được học bởi Poly là các tham số adapter, {Ai,Bi}|M|i=1, và các ma trận định tuyến Z. Trong quá trình tinh chỉnh, đối với mỗi nhiệm vụ kiểm tra τ, Poly khởi tạo ngẫu nhiên vector định tuyến zτ∈R1×|M| và tinh chỉnh cả zτ và tất cả các tham số module M.

3 Định tuyến Adapter Đa Đầu (MHR)

Trong Poly, việc kết hợp module vẫn thô: chỉ có thể thực hiện các tổ hợp tuyến tính của các module, và do đó adapter tổng hợp kết quả vẫn là một hàm tuyến tính của các module. Chúng tôi đề xuất tăng cường khả năng biểu đạt của việc kết hợp module trong khi giữ số lượng tham số tương tự. MHR (Hình 1) lấy cảm hứng từ multi-head attention [Vaswani et al., 2017]: nó phân vùng các chiều đầu vào thành h khối rời rạc khác nhau, thực hiện một tổ hợp kiểu Poly riêng biệt cho mỗi khối, và cuối cùng nối chúng lại. Điều này tương ứng với việc học một ma trận định tuyến Z khác nhau cho mỗi khối của các đặc trưng đầu vào, do đó cho phép mô hình chọn các adapter khác nhau cho các khối khác nhau của các chiều đầu vào. Cách tiếp cận tổng hợp này là tuyến tính từng đoạn (tức là tuyến tính trong các khoảng rời rạc), cho phép các tổ hợp các module có khả năng biểu đạt hơn.

Trong mỗi lớp MHR, hàm định tuyến là một tensor bậc ba Z∈R|T|×|M|×h, trong đó Z:,:,h∈R|T|×|M| là một lát cắt 2D của tensor Z. Một lát cắt đại diện cho ma trận định tuyến cho mỗi h head. Hãy ký hiệu W[k]∈Rd/h×r là phân vùng thứ k-th dọc theo các hàng của ma trận W∈Rd×r. Các tham số adapter Aτ∈Rd×r cho nhiệm vụ τ, và cho mỗi lớp adapter, được tính như (tương tự cho Bτ):

Aτk=Σj Aj[k]·ˆZτ,j,k/Σj ˆZτ,j,k với Aτk∈Rd/h×r, (MHR)

Aτ=concat(Aτ1, . . . ,Aτh)

trong đó concat nối theo chiều đầu tiên. Tiền huấn luyện đa nhiệm vụ và tinh chỉnh tương tự như Poly. Lưu ý rằng MHR chỉ dẫn đến một sự tăng không đáng kể về tổng số tham số, vì hầu hết các tham số được chứa trong các trọng số LoRA A,B (Bảng 1).

Tinh chỉnh Chỉ-Định tuyến (MHR-z) Nghiên cứu trước đây [Shao et al., 2023, inter alia] đã chỉ ra rằng khái quát hóa tổng hợp có thể đạt được bằng cách học tái kết hợp theo những cách mới các module đã có sẵn. Chúng tôi điều tra xem liệu việc tinh chỉnh các tham số module có thực sự cần thiết cho thích ứng few-shot trong ngữ cảnh của cả Poly và MHR hay không. Do đó, chúng tôi đặt tên Poly-z và MHR-z cho các biến thể mà, trong quá trình thích ứng few-shot, giữ các tham số của các module đã học trong quá trình tiền huấn luyện đa nhiệm vụ cố định và chỉ cập nhật các tham số định tuyến Z. Quan trọng là, điều này cho phép thích ứng với hiệu quả tham số cao: đối với các adapter LoRA, các ma trận A và B chiếm đại đa số tham số. Do đó, bằng cách đóng băng các ma trận A,B và chỉ cập nhật Z, chúng ta có thể giảm đáng kể chi phí tham số khi chuyển giao kiến thức cho một nhiệm vụ mới.

Tinh chỉnh Trung bình Adapter (MHR-µ) Để đánh giá tầm quan trọng của các tham số định tuyến trong quá trình thích ứng few-shot, chúng tôi đề xuất một biến thể bổ sung của MHR, MHR-µ, chia sẻ cùng quy trình tiền huấn luyện đa nhiệm vụ như MHR, nhưng đối với mỗi nhiệm vụ kiểm tra τ, cố định zτ= (1/|M|, . . . , 1/|M|) trong quá trình thích ứng few-shot. Điều này tương đương với việc loại bỏ các tham số định tuyến và tính trung bình các tham số module thành một tham số duy nhất trước khi tinh chỉnh. Cụ thể, adapter được sử dụng trong quá trình tinh chỉnh được khởi tạo với (tương tự cho Bτ):

Aτ=1/|M| Σi A*i;Aτ∈Rd×r (MHR-µ)

trong đó A*i là các tham số của các adapter sau khi tiền huấn luyện đa nhiệm vụ MHR. Lưu ý rằng, khác với MHR, MHR-µ tinh chỉnh cùng số lượng tham số như baseline adapter đơn. Do đó, bất kỳ sự khác biệt nào về hiệu suất giữa baseline adapter đơn và MHR-µ đều xuất phát từ sự khác biệt trong việc khởi tạo adapter và phải do quá trình tối ưu hóa diễn ra trong tiền huấn luyện đa nhiệm vụ, trước khi thích ứng few-shot.

Độ chi tiết Định tuyến Trong Poly gốc, Ponti et al. [2023] đã chỉ ra rằng việc học một ma trận định tuyến Z cho mỗi lớp mô hình cho hiệu suất tốt hơn so với việc chia sẻ một ma trận Z duy nhất trên tất cả các lớp. Do đó, chúng tôi điều tra xem điều này có đúng cũng đối với đối tác đa đầu của nó, MHR hay không. Ngoài ra, chúng tôi khám phá các cách tiếp cận trung gian giữa một Z mỗi lớp và một Z duy nhất được chia sẻ cho toàn bộ mô hình. Cụ thể, chúng tôi xem xét việc chia sẻ Z 1) cho các lớp adapter thuộc cùng một khối Transformer; hoặc 2) cho mỗi khối của l lớp, điều này cho phép chúng tôi dễ dàng cân bằng giữa khả năng biểu đạt và hiệu quả tham số. Như chúng tôi sẽ chứng minh trong mục 5.1, đây là một cơ chế hiệu quả để điều hướng mặt trận Pareto này trong các chế độ ngân sách tham số rất nhỏ mỗi nhiệm vụ.

4 Thí nghiệm

Đánh giá thí nghiệm của chúng tôi nhằm trả lời ba câu hỏi nghiên cứu: 1) Khả năng biểu đạt của hàm định tuyến có quan trọng không? 2) Tại sao các phương pháp PEFT dựa trên định tuyến mang lại hiệu suất vượt trội? 3) Việc định tuyến có hữu ích trong cả quá trình tiền huấn luyện đa nhiệm vụ và thích ứng few-shot không? Chúng tôi trước tiên trình bày các baseline và tập dữ liệu được sử dụng trong đánh giá của chúng tôi và sau đó thảo luận từng câu hỏi một cách lần lượt.

4.1 Baselines

Ngoài Poly, chúng tôi so sánh MHR với các baseline sau đây cho khái quát hóa cấp độ nhiệm vụ.

LoRA/(IA)3 huấn luyện một adapter đơn chung cho tất cả các nhiệm vụ tiền huấn luyện, sau đó được tinh chỉnh trên mỗi nhiệm vụ kiểm tra riêng biệt. Đây có thể nói là cách tiếp cận phổ biến nhất cho khái quát hóa đa nhiệm vụ hiệu quả tham số [Liu et al., 2022, Pfeiffer et al., 2023].

AdapterSoup Chronopoulou et al. [2023] huấn luyện một adapter khác nhau cho mỗi nhiệm vụ. Phương pháp này chỉ tính trung bình các trọng số adapter của các nhiệm vụ huấn luyện tương tự nhất với một nhiệm vụ kiểm tra cho trước, trước khi tiến hành với thích ứng few-shot. Để tính toán mối quan hệ nhiệm vụ, chúng tôi đo độ tương tự cosine của các embedding câu cho mỗi nhiệm vụ được tính trung bình trên tập dữ liệu huấn luyện của chúng. Đáng chú ý, khác với các phương pháp được đề xuất trong bài báo này, không có chia sẻ kiến thức (cũng như can thiệp) trong quá trình tiền huấn luyện đa nhiệm vụ vì các adapter nhiệm vụ được huấn luyện độc lập.

4.2 Tập dữ liệu

Chúng tôi kiểm tra các phương pháp của chúng tôi trên bộ đánh giá T0 Sanh et al. [2022], tuân theo cùng thiết lập như Liu et al. [2022], và SuperNI Wang et al. [2022a], một tập dữ liệu quy mô lớn với hơn 1.600 nhiệm vụ huấn luyện.

Các Nhiệm vụ T0 Chúng tôi tuân theo quy trình tiền huấn luyện và tinh chỉnh được thảo luận trong Liu et al. [2022], sử dụng các siêu tham số và hàm mất mát được đề xuất trong codebase công khai cho T-Few.

Tất cả các phương pháp được kiểm tra với T5-XL Raffel et al. [2020] và T0-3B Sanh et al. [2022] làm mô hình xương sống. Quan trọng là, T5 chỉ đơn giản được tiền huấn luyện trên mô hình hóa ngôn ngữ (có mặt nạ), trong khi T0 được tinh chỉnh hướng dẫn thêm: cụ thể, toàn bộ mô hình được tinh chỉnh trên các ví dụ từ nhiều nhiệm vụ huấn luyện đã được tăng cường với hướng dẫn nhiệm vụ. Để đảm bảo công bằng cho tất cả các phương pháp, chúng tôi báo cáo median và độ lệch chuẩn của độ chính xác validation tốt nhất cho mỗi nhiệm vụ kiểm tra trên 3 seed, khi được đánh giá mỗi 50 epoch huấn luyện. Chúng tôi xử lý mỗi cặp tập con dữ liệu-template như một nhiệm vụ duy nhất, tạo ra tổng cộng 313 nhiệm vụ.

SuperNI Để hạn chế chi phí tính toán, chúng tôi báo cáo kết quả trên 20 trong số 119 nhiệm vụ kiểm tra. Các nhiệm vụ được chọn ngẫu nhiên, với yêu cầu có ít nhất 300 ví dụ có sẵn, và được chia đều thành 100 ví dụ huấn luyện, 100 ví dụ validation và 100 ví dụ kiểm tra. Đối với mỗi phương pháp, chúng tôi thực hiện dừng sớm trên tập validation. Chúng tôi báo cáo kết quả với Rouge-L được tính trung bình trên 3 seed. Tất cả các phương pháp sử dụng T5-XL [Raffel et al., 2020] làm xương sống chứ không phải T0, vì các nhiệm vụ huấn luyện T0 và nhiệm vụ kiểm tra SuperNI có thể trùng lặp.

5 Kết quả và Thảo luận

5.1 Khả năng biểu đạt của hàm định tuyến có quan trọng không?

MHR vượt trội hơn các phương pháp PEFT Chúng tôi bắt đầu phân tích bằng cách đánh giá hiệu quả của kỹ thuật đề xuất của chúng tôi khi được áp dụng trên một xương sống chưa trải qua huấn luyện trước trên dữ liệu tuân theo hướng dẫn (T5-XL). Như được chỉ ra trong kết quả benchmark T0 trong bảng trên của Hình 2, rõ ràng là các kỹ thuật định tuyến đa đầu có lợi thế rõ rệt, vượt trội hơn cả định tuyến đơn đầu Poly 1.1%, và vượt qua các phương pháp LoRA tiêu chuẩn một cách ấn tượng 3.1%. Chúng tôi cũng nghiên cứu tác động của việc thực hiện tinh chỉnh hướng dẫn của toàn bộ xương sống trước khi huấn luyện adapter. Để làm điều này, chúng tôi cũng thí nghiệm với T0-3B làm xương sống. Trong bảng dưới của Hình 2, chúng ta có thể quan sát thấy rằng mặc dù khoảng cách tương đối giữa MHR và các baseline nhỏ hơn, định tuyến đa đầu vẫn thành công mang lại kết quả thuận lợi. Do đó, các lợi ích của MHR tích lũy với các phương pháp đa nhiệm vụ khác như tinh chỉnh hướng dẫn. Cuối cùng, chúng tôi chuyển sự chú ý của chúng tôi tới tập dữ liệu SuperNI (Bảng 2). Ở đây, MHR tiếp tục vượt qua các baseline tương tự.

MHR-z tạo điều kiện thuận lợi cho hiệu quả tham số cực kỳ cao Hình 2 (phải) tiết lộ các phát hiện thú vị về MHR-z. Khi chúng tôi hạn chế huấn luyện chỉ cho các tham số định tuyến Z trong Poly gốc, kết quả đáng tiếc không ngang bằng với phiên bản nơi cả định tuyến và adapter đều được cập nhật. Tuy nhiên, khi chúng tôi áp dụng cùng ràng buộc cho MHR, hiệu suất gần hơn đáng kể với mức tối ưu đạt được trong thiết lập này. Thực tế, MHR-z vượt qua các baseline trước đây đồng thời yêu cầu ít tham số hơn để thích ứng hiệu quả với các nhiệm vụ mới. Hơn nữa, bằng cách kiểm soát số lượng lớp chia sẻ cùng một phân bổ Z (xem mục 3), MHR-z có thể cân bằng giữa hiệu suất và hiệu quả tham số, thậm chí vượt qua Poly-z trong các thiết lập với chỉ 3K tham số có thể huấn luyện mỗi nhiệm vụ kiểm tra (xem thêm §A.2.1 để phân tích sâu hơn). Xu hướng này cũng được quan sát thấy tương tự trong benchmark SuperNI (Bảng 2), nơi các cập nhật bị hạn chế chỉ cho các tham số định tuyến mang lại hiệu suất ngang bằng với tinh chỉnh tiêu chuẩn. Do đó, chúng tôi kết luận rằng MHR-z đại diện cho một phương pháp mạnh mẽ để đạt được hiệu quả tham số cực kỳ cao trong thích ứng.

Các head định tuyến bổ sung có lợi hơn so với các module thêm Trong phương pháp Poly gốc, một sự cân bằng giữa khả năng và hiệu quả tham số có thể đạt được bằng cách thêm các module bổ sung cho mỗi lớp adapter. Tuy nhiên, điều này dẫn đến sự tăng tuyến tính về số lượng tham số đa nhiệm vụ, có thể trở nên không thực tế. Để khám phá một sự cân bằng hiệu quả hơn, chúng tôi điều tra tùy chọn thêm các head định tuyến bổ sung thay vì các module thêm. Hình 3 (phải) trình bày so sánh giữa hai phương pháp. Nó chứng minh rằng việc tăng số lượng head định tuyến dẫn đến hiệu suất tốt hơn so với việc thêm nhiều module hơn. Quan trọng là, lợi ích của định tuyến đa đầu có hai mặt: nó cung cấp khả năng biểu đạt tăng cường cho mô hình, đồng thời duy trì hiệu quả tham số. Phát hiện này làm nổi bật lợi thế của định tuyến đa đầu như một cách tiếp cận hiệu quả hơn để cân bằng khả năng biểu đạt và số lượng tham số trong các tình huống thích ứng few-shot.

Các phương pháp dựa trên định tuyến cũng xuất sắc ở quy mô 11B Chúng tôi tiếp tục đánh giá xem Poly và MHR có vượt qua các phương pháp PEFT đã được thiết lập khi được huấn luyện trên một xương sống mô hình lớn hơn hay không. Để hoàn thành điều này, chúng tôi sử dụng phiên bản 11B của T0. Như được mô tả trong Bảng 3, các phương pháp dựa trên định tuyến một lần nữa vượt trội hơn huấn luyện adapter tiêu chuẩn, vượt qua việc tái sản xuất của chúng tôi về state-of-the-art trước đây trong Liu et al. [2022] hơn 2%. Chúng tôi quan sát thấy rằng Poly và MHR cho thấy hiệu suất tương tự trong tinh chỉnh tiêu chuẩn, nhưng tinh chỉnh MHR z-tuning vẫn có hiệu suất tốt hơn trong tinh chỉnh chỉ-định tuyến. Thực tế, MHR-z (221K tham số) vượt trội hơn Poly-z (3.5K tham số) 2.9%, trong khi vẫn hiệu quả tham số hơn so với Liu et al. [2022] (1.1M tham số).

5.2 Tại sao các phương pháp PEFT dựa trên định tuyến mang lại hiệu suất vượt trội?

Mặc dù các phương pháp đề xuất của chúng tôi đã chứng minh kết quả đầy hứa hẹn trên một phạm vi rộng các tập dữ liệu và ngân sách tham số thích ứng khác nhau, câu hỏi về lý do tại sao PEFT dựa trên định tuyến thể hiện hiệu suất vượt trội vẫn chưa được trả lời. Trong phần này, chúng tôi nhằm khám phá các thành phần chính thúc đẩy hiệu suất vượt trội của MHR.

Việc học Hàm Định tuyến là thiết yếu Cho rằng Poly và MHR có quyền truy cập vào nhiều tham số hơn so với các adapter tiêu chuẩn trong quá trình tiền huấn luyện đa nhiệm vụ, chúng tôi điều tra xem điều này, chứ không phải cơ chế định tuyến, có phải là nguyên nhân cho hiệu suất vượt trội của chúng hay không. Để làm điều này, chúng tôi so sánh chúng với một phương pháp baseline. Thay vì học hàm định tuyến, chúng tôi gán ngẫu nhiên một phân bổ module nhị phân cho mỗi điểm dữ liệu trong một minibatch, bỏ qua thông tin nhiệm vụ. Phương pháp định tuyến ngẫu nhiên này, giống như Wang et al. [2022b], cho phép chúng tôi đánh giá trực tiếp ảnh hưởng của các tham số bổ sung trong quá trình huấn luyện đa nhiệm vụ. Tại thời điểm kiểm tra, các module đã học được tính trung bình thành một module duy nhất trước khi tinh chỉnh; do đó chúng tôi gọi baseline này là Random-µ. Trên benchmark T0 với xương sống T5-XL, Random-µ có hiệu suất tương tự như một adapter LoRA tiêu chuẩn (66.0%), trong khi Poly và MHR vượt trội hơn nó 2% và 3.1% tương ứng. Do đó, chúng tôi kết luận rằng việc học một hàm định tuyến là quan trọng, và chỉ đơn thuần tăng khả năng trong quá trình huấn luyện không trực tiếp dẫn đến cải thiện.

MHR thúc đẩy chuyển giao và giảm thiểu can thiệp giữa các nhiệm vụ tiền huấn luyện Nhận ra vai trò then chốt của bước tiền huấn luyện đa nhiệm vụ trong việc tăng cường hiệu suất của Poly, chúng tôi khám phá mức độ chuyển giao và can thiệp giữa các nhiệm vụ huấn luyện. Bằng cách giám sát độ căn chỉnh gradient trung bình cho mỗi cặp nhiệm vụ (theo độ tương tự cosine) trong suốt quá trình huấn luyện, chúng tôi có thể đánh giá mức độ chuyển giao tích cực. Như Hình 3 (trái) cho thấy, MHR thể hiện mức độ tương tự cosine gradient cao hơn giữa các nhiệm vụ so với các lựa chọn PEFT khác, bao gồm cả Poly. Phát hiện này cho thấy rằng sự linh hoạt tăng cường được cung cấp bởi định tuyến đa đầu có thể phục vụ để giảm thiểu can thiệp giữa các nhiệm vụ ở mức độ lớn hơn so với định tuyến tiêu chuẩn trong khi đồng thời thúc đẩy chuyển giao tích cực.

5.3 Việc định tuyến có quan trọng cho khái quát hóa nhiệm vụ không?

Chúng tôi đã đánh giá tầm quan trọng của việc định tuyến trong quá trình tiền huấn luyện. Bây giờ chúng tôi tiếp tục xác minh xem việc định tuyến có quan trọng trong việc học định tuyến trong quá trình thích ứng few-shot hay không. Poly-µ và MHR-µ liên tục vượt trội hơn LoRA, và khớp với hiệu suất của Poly/MHR (Bảng 4). Điều này chứng minh rằng, đối với thích ứng few-shot, trung bình của các module đã tiền huấn luyện cung cấp một khởi tạo tốt hơn so với việc học một adapter được chia sẻ trên tất cả các nhiệm vụ trong quá trình tiền huấn luyện. Hiệu suất vượt trội liên tục của Poly-µ so với Random-µ và AdapterSoup nhấn mạnh tầm quan trọng của việc định tuyến trong quá trình tiền huấn luyện đa nhiệm vụ (nhưng không phải trong quá trình thích ứng), cung cấp một khởi tạo adapter hiệu quả cho học few-shot. Phát hiện này có thể cung cấp cảm hứng cho nghiên cứu tương lai để cải thiện các phương pháp meta-learning và trung bình trọng số [Izmailov et al., 2018].

MHR-µ xuất sắc trong học zero-shot Đối với nhiều nhiệm vụ downstream quan tâm, dữ liệu được gán nhãn bổ sung có thể không có sẵn. Trong những thiết lập như vậy, không rõ làm thế nào để tận dụng các phương pháp MHR-µ và Poly-µ. Để giải quyết điều này, chúng tôi tinh chỉnh trung bình của các adapter được huấn luyện đa nhiệm vụ trên dữ liệu tiền huấn luyện đa nhiệm vụ (thay vì sử dụng dữ liệu few-shot downstream), trong k bước bổ sung. Kết quả được trình bày trong Bảng 5. Chúng tôi thấy rằng không có bất kỳ tinh chỉnh bổ sung nào (k= 0), việc tính trung bình các adapter không mang lại kết quả tốt. Điều này là do sự không khớp tiềm tàng giữa các adapter đã học thông qua định tuyến đặc thù cho nhiệm vụ, và chiến lược định tuyến đồng nhất. Chúng ta có thể quan sát thấy rằng khi tinh chỉnh trung bình của các adapter trên dữ liệu tiền huấn luyện đa nhiệm vụ trong k bước bổ sung, MHR-µ cho thấy hiệu suất mạnh mẽ khi được đánh giá theo cách zero-shot. Để so sánh công bằng, chúng tôi cũng tinh chỉnh LoRA thêm cho cùng số bước bổ sung. Mô hình tốt nhất của chúng tôi đạt được hiệu suất zero-shot 64.5 trên T0-11B, đạt được lợi ích tuyệt đối 3.5% điểm độ chính xác.

6 Nghiên cứu liên quan

Học đa nhiệm vụ hiệu quả cho các nhiệm vụ ít tài nguyên [Wei et al., 2022, Aribandi et al., 2022, Sanh et al., 2022], vì kiến thức có thể được mượn từ các nhiệm vụ tương tự bằng cách chia sẻ các tham số mô hình. Học đa nhiệm vụ cũng đã được áp dụng trên các ngôn ngữ và phương thức [Ponti et al., 2019, Bugliarello et al., 2022]. Trong bối cảnh NLP, một số họ phương pháp cho phép học các nhiệm vụ mới từ một tập hạn chế các ví dụ được gán nhãn. Học trong ngữ cảnh few-shot [ICL; Brown et al., 2020], nơi các ví dụ của một nhiệm vụ mới được nối vào một prompt đầu vào, cho phép các mô hình khái quát hóa cho các nhiệm vụ chưa thấy mà không cần bất kỳ huấn luyện dựa trên gradient nào. Tuy nhiên, những phương pháp như vậy nhạy cảm với định dạng prompt và thứ tự ví dụ [Zhao et al., 2021]. Quan trọng hơn, các phương pháp ICL gây ra một chi phí tính toán đáng kể, vì đối với mỗi dự đoán, toàn bộ tập các ví dụ phải được xử lý bởi mô hình [Liu et al., 2022]. Để khắc phục điều này, nhiều phương pháp tinh chỉnh hiệu quả tham số (PEFT) đã được đề xuất như một thay thế cho ICL, nơi một số lượng nhỏ tham số mới được thêm vào mạng tiền huấn luyện đóng băng. Để kể tên một số, LoRA [Hu et al., 2022] tiêm các ma trận rank thấp có thể học vào mỗi lớp Transformer. Thay vào đó, ma trận có thể học có thể thưa thớt, chọn các dịch chuyển khác không thông qua giả thuyết Lottery-Ticket [Ansell et al., 2021] hoặc thông qua thông tin Fisher gần đúng của chúng [Sung et al., 2021]. Cuối cùng, các phương pháp prefix-tuning [Li and Liang, 2021] đặt trước các embedding có thể học vào đầu vào hoặc các biểu diễn trung gian để chuyên biệt hóa mô hình cho một nhiệm vụ downstream.

Các mạng modular phân vùng các tham số của chúng thành một số module chuyên gia, mỗi module chuyên biệt để xử lý các nhiệm vụ con cụ thể [Jacobs et al., 1991, Kirsch et al., 2018]. Các mạng modular là một giải pháp hấp dẫn cho vấn đề thích ứng với các nhiệm vụ chưa thấy [Corona et al., 2021], vì mô hình có thể tận dụng các module hiện có của nó và tái kết hợp chúng theo cách mới, do đó đạt được khái quát hóa có hệ thống [Bahdanau et al., 2019]. Chúng cũng đã được kiểm tra trong các tình huống học với dữ liệu được trình bày tuần tự [Ostapenko et al., 2021], và với các môi trường thay đổi Goyal et al. [2021]. Trong NLP, các mô hình mixture-of-experts (MoE) [Shazeer et al., 2017, Fedus et al., 2022], nơi một cơ chế gating đã học định tuyến các biểu diễn token đến các chuyên gia phù hợp (các lớp Feed-Forward), đã cho thấy thành công trong việc mở rộng số lượng tham số trong khi duy trì hiệu quả thời gian. Điều này dẫn đến hiệu suất cao hơn khi so sánh với các đối tác dày đặc của chúng sử dụng ngân sách tính toán tương tự.

7 Kết luận

Trong bài báo này, chúng tôi giải quyết thách thức khái quát hóa cho các nhiệm vụ mới dựa trên một vài ví dụ sau tiền huấn luyện đa nhiệm vụ. Cụ thể, chúng tôi tập trung vào Polytropon [Ponti et al., 2023], một mô hình nơi mỗi nhiệm vụ được liên kết với một tập con của các adapter bởi một hàm định tuyến. Chúng tôi điều tra cách thay đổi mức độ kiểm soát được cung cấp bởi hàm định tuyến tác động đến hiệu suất trên hai benchmark toàn diện cho học đa nhiệm vụ, T0 và Super-Natural Instructions. Đầu tiên, một biến thể mới được đề xuất của hàm định tuyến, nơi nhiều head chịu trách nhiệm cho các khối khác nhau của các chiều đầu vào, cải thiện liên tục so với tất cả các baseline khác, bao gồm các adapter LoRA và (IA)3. Thứ hai, chúng tôi xác định nguyên nhân thành công của việc định tuyến trong khả năng ngăn chặn can thiệp giữa các nhiệm vụ, vì nó mang lại sự căn chỉnh tốt hơn giữa các gradient của chúng. Thứ ba, chúng tôi thấy rằng việc tính trung bình đơn giản tất cả các adapter đã tiền huấn luyện đa nhiệm vụ trước khi thích ứng few-shot cho các nhiệm vụ mới cung cấp hiệu suất tương đương, do đó cung cấp hiệu suất state-of-the-art cho học few-shot adapter đơn. Định tuyến đa đầu chứng minh tầm quan trọng của việc lựa chọn adapter chi tiết cho khái quát hóa tiết kiệm mẫu và hứa hẹn cải thiện các phương pháp modular khác, chẳng hạn như Mixtures of Experts [MoEs; Fedus et al., 2022] trong nghiên cứu tương lai.
