# 2312.14327.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2312.14327.pdf
# File size: 253294 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Parameter Efficient Tuning Allows Scalable Personalization of LLMs for
Text Entry: A Case Study on Abbreviation Expansion
Katrin Tomanek, Shanqing Cai, Subhashini Venugopalan
Google LLC
Abstract
Abbreviation expansion is a strategy used
to speed up communication by limiting the
amount of typing and using a language model
to suggest expansions. Here we look at per-
sonalizing a Large Language Model’s (LLM)
suggestions based on prior conversations to en-
hance the relevance of predictions, particularly
when the user data is small ( ≈1000 samples).
Specifically, we compare fine-tuning, prompt-
tuning, and retrieval augmented generation of
expanded text suggestions for abbreviated in-
puts. Our case study with a deployed 8B pa-
rameter LLM on a real user living with ALS,
and experiments on movie character personal-
ization indicates that (1) customization may be
necessary in some scenarios and prompt-tuning
generalizes well to those, (2) fine-tuning on
in-domain data (with as few as 600 samples)
still shows some gains, however (3) retrieval
augmented few-shot selection also outperforms
fine-tuning. (4) Parameter efficient tuning al-
lows for efficient and scalable personalization.
For prompt-tuning, we also find that initializ-
ing the learned “soft-prompts” to user relevant
concept tokens leads to higher accuracy than
random initialization.
1 Introduction
Language models have long been used to reduce
keystrokes and aid text entry in smart keyboards.
This work looks at models for such keyboard ap-
plications in Augmentative and Alternative Com-
munication (AAC) devices, in particular those for
users with severe motor impairments, e.g., peo-
ple living with amyotrophic lateral sclerosis (ALS)
who communicate through eye-gaze typing. Re-
cent advances in the generative capabilities of large
language models (LLMs) can help significantly ac-
celerate communication for such users. Prior stud-
ies (Adhikary et al., 2021; Cai et al., 2022; Shen
et al., 2022) proposed techniques for abbreviation
expansion, where a user types short keywords orabbreviated phrases consisting of the initial letter
of each word and an LLM is used to generate the
fully-expanded sentence. Including the conversa-
tion context (Wisenburn and Higginbotham, 2008;
Gorman et al., 2021) was shown to further improve
the accuracy of the predictions. In this work we ex-
plore personalization, another dimension that can
improve the relevance of the predictions to fit a
user’s vocabulary and language style.
In many real-world applications personalization
plays an important role in enhancing the relevance
of the suggested options and the quality of the user
experience (Valencia et al., 2023). However, very
little data is available to adapt a model to a given
user, and larger models increase the risk of overfit-
ting. Additionally, it remains unclear how to scale
the approach to multiple users given the high cost
of LLM checkpoint storage and serving.
With these challenges in mind, our work evalu-
ates three approaches to personalizing LLMs for
abbreviation expansion as used by eye-gaze typers.
Specifically we consider a pre-trained decoder-only
LLM tuned for dialog (Roller et al., 2020; Thop-
pilan et al., 2022). We further fine-tune the model
on the abbreviation expansion task on data derived
from dialog datasets. We then compare personaliz-
ing this fine-tuned LLM on user-specific data via
(1) fine-tuning the entire model, (2) augmenting
the LLM’s context by retrieving similar conversa-
tions from the user’s history, and (3) parameter ef-
ficient prompt-tuning (Lester et al., 2021). Overall,
prompt-tuning performed best and retrieval aug-
mented in-context learning (RA-ICL) also outper-
formed fine-tuning.
2 Related Work
2.1 Language models for text-entry.
Using language models to expand abbreviated in-
puts for text-entry has been well studied and dif-
ferent schemes of abbreviation have been proposedarXiv:2312.14327v1  [cs.CL]  21 Dec 2023

--- PAGE 2 ---
such as, using just context words (Demasco and
McCoy, 1992), discarding vowels (Shieber and
Nelken, 2007), and additionally omitting repeated
consonants (Willis et al., 2005), flexible letter sav-
ing schemes (Adhikary et al., 2021; Gorman et al.,
2021), and expanding from a bag of words (Shen
et al., 2022). Our study focuses on abbreviation ex-
pansion used by eye-gaze typers living with severe
motor impairments. Given our goal to significantly
reduce the number of keystrokes, we consider a
form of word-initial abbreviation similar to Cai
et al. (2022) where just the initial characters of the
words are typed and an LLM predicts the full sen-
tence. The current study focuses on personalizing
such a model to a user, which has been less studied.
2.2 LLM prompt engineering.
LLMs have shown remarkable capabilities in
understanding and performing tasks with few-
shot (Brown et al., 2020) examples. However, the
tokenization used in LLMs makes our task of gener-
ating expansions from single characters somewhat
hard for the models. Due to this reason and to
enable personalization, we focus on Parameter Ef-
ficient Fine-Tuning (PEFT) (Lester et al., 2021),
and retrieval augmented generation (RAG) (Mi-
alon et al., 2023). PEFT learns a small set of ad-
ditional parameters while keeping the weights of
the original LLM frozen. Many PEFT methods
have been proposed in recent years. In case of
adapters (Houlsby et al., 2019) and Low-Rank Op-
timization (LoRA) (Hu et al., 2021) these parame-
ters are interspersed at different transformer layers
of the model. Other methods such as, Prompt-
tuning (Lester et al., 2021), Prefix-tuning (Li and
Liang, 2021), and P-tuning (Liu et al., 2021) restrict
the parameters to the input prompt tokens. We use
prompt-tuning (Lester et al., 2021) which append
parameters to the token embeddings. We also com-
pare this to retrieval augmentation for ICL (Rubin
et al., 2022) where a dense retriever is used to se-
lect relevant data point(s) that are then added as
context to a generative answering model. While
most RAG studies (Mialon et al., 2023) train the
retriever or the generator, we keep both of these
pre-trained models frozen. Specifically, we use the
retrieved context to create more relevant few-shot
examples specific to the input query.
3 Tuning and personalization
Our broad approach consists of taking a pre-trained
LLM, performing supervised fine-tuning for theabbreviation expansion task, and then personal-
izing the model on user data by means of fur-
ther fine-tuning, prompt-tuning, or retrieval aug-
mented in-context few-shot generation. For the
pre-trained model, we start with an 8B parameter
decoder-only LLM. This model is pre-trained on
the C4 dataset (Raffel et al., 2019) and tuned for
dialogs (Roller et al., 2020; Thoppilan et al., 2022).
We then fine-tune it further for abbreviation expan-
sion on sentences from conversations and associ-
ated word-initial abbreviated text. We follow prior
works (Cai et al., 2022) and experiment with dif-
ferent learning rates, and use a constant rate during
fine-tuning and select the best based on a validation
set. We refer to this as the base-AE model. We
explore 3 strategies for personalization.
3.1 Fine-tuning on user data.
We follow the same fine-tuning recipe on user data
as with the base-AE model. The tuning itself is
fast since the amount of user data is small, and we
avoided overfitting by monitoring performance on
the validation set. We experimented with learning
rates 1e-5, 1e-6, and 5e-5 and found 5e-5 to work
best (see App. Tab. 6).
3.2 Few-shot and Retrieval Augmented
In-Context Learning (RA-ICL)
Another way to personalize an LLM is to provide
it with few-shot examples to allow for in-context
learning (ICL). Performance with ICL can vary
significantly with few-shot examples (Zhao et al.,
2021). Hence, in addition to typical few-shot ex-
amples, we also investigate a retrieval-augmented
few-shot setup. This is similar to works that re-
trieve from databases to augment LLMs (Mialon
et al., 2023) but we use existing pre-trained mod-
els for retrieving and generating, and keep them
frozen. For the retriever, we use a pre-trained 11B
Sentence-T5 (Ni et al., 2022) and generate em-
beddings of the abbreviated inputs from the user
conversations. Given a new input, we embed it and
use Euclidean distance to retrieve the nearest neigh-
bor queries and the corresponding expansions. We
use this retrieved context to create relevant, query-
specific few-shot examples with which we prompt
the LLM.
3.3 Prompt-tuning
We also investigate prompt-tuning (Lester et al.,
2021) for personalization. The basic idea is to
extend few-shot prompting and use substantially

--- PAGE 3 ---
more in-context examples to learn “soft-prompts”
in the input embedding layer specifically suited for
the task at hand. We choose the length of the soft
prompt and initialize the tokens. For tuning, we
correspondingly add new learnable parameters to
the model’s embedding matrix that are updated us-
ing back propagation, keeping the original LLM
weights frozen. The number of learned parameters
is a product of the length of the soft-prompt and
dimensions of the embedding weights. The learned
soft-prompts are saved and passed along with each
user query to the LLM during inference. This ap-
proach allows a single LLM to be served, and the
soft-prompt to be swapped for different users (see
Sec. 6). The soft-prompts themselves can be tuned
on varying amounts of data, and are effective in low
data settings (Lester et al., 2021). We train with a
warm-up learning rate schedule with 1000 warm up
steps to a peak of 0.1 followed by linear decay. We
use small batch sizes of 16 for training and limit
training to 20k steps. We experiment with differ-
ent prompt lengths and initialization strategies, and
choose the best checkpoints based on validation set
accuracy.
4 Dataset
4.1 Abbreviation Expansion Base Model
To fine-tune the LLM for the abbreviation expan-
sion task, we need pairs of abbreviated phrases and
the full expanded text. We use the data from Cai
et al. (2022) where they prepare paired sentences
and abbreviated inputs from four dialog datasets:
crowd-sourced Turk Dialogues (Vertanen, 2017),
DailyDialog (Li et al., 2017), the Cornell Movie Di-
alogues (Danescu-Niculescu-Mizil and Lee, 2011)
from movie scripts, and Turk AAC dataset (Verta-
nen and Kristensson, 2011) of conversations col-
lected with AAC users in mind. The model fine-
tuning is done with a constant low-learning rate
(0.01) using the AdaFactor optimizer (Shazeer and
Stern, 2018) on over 237,000 examples derived
from the dialog datasets.
4.2 Personalization Dataset
A model trained on generic dialog datasets may
not fit the communication needs of all in terms of
preserving their style, and vocabulary including
proper nouns. Our work is motivated to increase
the autonomy and self-expression of AAC users
with motor and speech impairments and deploy our
abbreviation expansion model for their daily usage.
This is also a case where a generic models’ train-ing data is also lacking in terms of conversations
around caregiving and health. Hence, our personal-
ization dataset was collected from a person living
with ALS with informed consent from the user and
the conversation partners. They use eye-gaze text
entry for everyday communication. They type on
a virtual keyboard into the text editor of a text-to-
speech (TTS) software and activate the audio to
"speak" out the contents. Private and sensitive con-
tent was redacted prior to obtaining the data for
research. The data was partitioned chronologically,
and repetition was removed from the validation and
test portions resulting in 630 (train), 194 (val.) and
224 (test) samples.
4.3 Movie character personalization
Outside of the real deployment scenario, we also
examined other conversation datasets where per-
sonalization can be studied without affecting user
privacy. Characters in movies and TV series tend
to have certain quirks and personalities and make
for a great test bed for evaluating personalization
of spoken dialogues. Thus, to evaluate the need for
customization and scalability of the approach, we
performed additional experiments on conversations
from the Cornell Movie Dialogs dataset (Danescu-
Niculescu-Mizil and Lee, 2011) test set. For our
experiments, we selected 10 movies with very high
ratings (with atleast 5k votes on ImDb). From each
movie, we chose 1 character and all their conver-
sations from the movie for personalization. Each
character had over a hundred conversations in the
movie (range 104 to 344, with a mean of 198.4 and
median of 209 conversations). Similar to our AAC
personalization dataset we did a time-based split of
the data to get train, val., and test splits.
5 Experiments and Results
Experimental setup. For all experiments, we sam-
ple 128 responses from the model with temperature
1.0, sort based on frequency of predictions and se-
lect the top-5. We report results on the average
(and±std. dev.) of 3 runs unless specified other-
wise. The metrics we use are Accuracy to measure
exact match of the full sentence expansion, and
BLEU score (Papineni et al., 2002) to consider par-
tial credit, both measured on the top-5 predictions
(noted as @5).
5.1 Prompt-tuning is best for Personalization
Table 1 compares the performance of the different
personalization approaches on the real user data.

--- PAGE 4 ---
We note that the base-AE model achieved a top-5
accuracy of 68.3% on the abbreviation expansion
test set, however from Tab. 1 we can see that it only
gets an accuracy of 22.5% on the user personaliza-
tion test set highlighting the difference between the
user data distribution and the training distribution,
and making a strong case for personalization for
AAC users. Fine-tuning on user data helps, and
retrieval for ICL does even better, however prompt-
tuning results in the best performance.
Model personalized Accuracy@5 BLEU@5
base-AE × 22.5 31.8
ICL ✓ 22.8 34.9
Fine-tuned ✓ 26.5 34.3
RA-ICL ✓ 30.3 39.1
Prompt-tuned ✓ 38.8 47.5
Table 1: Accuracy (exact-match of full sentence) and BLEU
score of top 5 predictions of the different approaches on the
personalization test set.
5.2 Soft prompt initialization matters
We experimented with different soft-prompt
lengths, learning rates, and soft-prompt initializa-
tion strategies. We tried soft-prompt lengths of 10,
25, and 100 tokens all initialized randomly. Re-
call that increasing the prompt lengths increases
the number of learned parameters. In our case, we
found higher prompt lengths led to more training
instabilities. We found a length of 10 to work best.
Fixing the prompt length as 10, we experimented
with learning rates of 0.1, 0.2, and 0.3 and found
0.1 to work best (in App. Tab. 7).
Soft-prompt Initialization Accuracy@5 BLEU@5
Random 32.7±3.2 43.6±2.3
LLM vocab. sampled 33.9±0.4 43.2±1.8
User vocab. sampled 32.6±1.6 41.0±1.9
User relevant concepts 36.8±1.9 45.9±1.4
User concept antonyms 36.4±0.3 46.2±4.3
Table 2: Initializing soft-prompts with proper nouns and con-
cepts from the user’s data performs best.
The thing that made the biggest difference
though was the choice of initialization for the soft-
prompt token embeddings, which can be seen in
Table 2. We examined 5 strategies, (1) random ini-
tialization, (2) sampling from the top 5k words in
the LLM’s vocabulary, (3) sampling from the top
25 most common English words in the user’s vo-
cabulary, (4) hand-selecting proper names and con-
cepts relevant to the user (e.g. ALS) and (5) select-
ing words that are related but might be considered
antonyms of the user concepts (e.g. Parkinsons).
We found the initialization that relied on the user
concepts to perform significantly better. Analogousto what is suggested in Lester et al. (2021), perhaps
these tokens are the ones the base model is most
uncertain about, and hence boosts their chance of
appearing in the predictions when prompt-tuned.
5.3 Fine-tuning hampers generalization
Fig. 1 slices performance of the models based on
the length of the sentences. The performance of
all models degrade with increasing sentence length.
However, the fine-tuned model generalizes poorly
compared to the base-AE model in some cases
(noticeable at lengths 5 and 6). This also highlights
the difficulty with fine-tuning large models on very
small datasets.
Figure 1: Performance of the different approaches on conver-
sation sentences of different lengths. On longer sentences, the
prompt-tuned (green) and base non-personalized model (in
blue) can outperform fine-tuning highlighting their capacities
to generalize to the long tail of complex sentences.
ICL (4-shot) strategy Accuracy@5 BLEU@5
Random 4-shot 22.0±0.9 30.8±1.4
Hand-crafted 4-shot 21.9±0.5 33.3±0.7
Retrieval augmented (RA-ICL) 30.2±0.3 38.5±0.6
Table 3: Comparing different few-shot selection strategies.
Retrieval augmented ICL works best.
5.4 Retrieval augmented few-shots help
Table 3 presents results for in-context learning
where 4-shot examples are selected using different
strategies: (1) random selection from the training
set, (2) hand-crafted examples containing proper
names of people that the user communicates with,
and (3) retrieval-augmented few shots, where 4
nearest neighbor examples (in the embedding space
of abbreviations) are selected based on each test
query. RA-ICL outperforms other strategies by a
large margin.
5.5 Customization is not always necessary
We also evaluated the prompt-tuning approach on
the movie character personalization dataset and re-
port results in Table 4. We observe that: (1) the
base non-personalized model accuracies do seem to

--- PAGE 5 ---
Movie-id character-id Character-Name Non-personalized base-AE Personalized (prompt-tuned) Personalization
Acc. @5 BLEU @5 Acc. @5 BLEU @5 rel. benefit (%)
m106 u1612 JACOB 62.75 67.03 56.86 65.13 -
m119 u1799 GEORGE 50.00 59.18 56.25 62.46 13%
m126 u1916 ANDERTON 44.12 55.66 38.24 55.74 -
m140 u2157 BABE 60.00 69.52 46.67 62.93 -
m148 u2299 NANCY 41.67 52.67 41.67 51.40 -
m203 u3105 MICHAEL 61.90 59.60 47.62 45.32 -
m274 u4099 ABBY 77.78 77.78 77.78 77.78 -
m324 u4866 SONNY 62.86 71.53 65.71 72.97 5%
m352 u5310 JACK 50.00 59.18 56.25 62.46 13%
m565 u8329 JEANNE 61.54 70.61 64.10 71.24 4%
Table 4: Performance comparison between Non-personalized base-AE and Personalized (prompt-tuned) models on movie
character personalization. LLMs raise the bar on average performance indicating that customization may not always be necessary
on certain conversation categories, though some users benefit from it. This is a contrast to the real AAC deployment scenario.
transfer reasonably well indicating that customiza-
tion may not be necessary for conversation types
similar to the training data distribution. (2) 4 of
the 10 speakers still benefit from personalization.
(3) the proposed prompt-tuning approach offers
a way to serve the same end-point, while op-
tionally choosing to personalize results to some
users .
5.6 Error Analysis
In Table 5 we share some examples of the cate-
gories of errors we observe comparing the fine-
tuned and prompt-tuned results. Our analysis of the
predictions show that the fine-tuned model tends to
overfit to proper nouns in the user’s training data,
and often misses generating expansions for some of
the characters in the acronym. On sessions where
there is not enough user context, it can miss the
user’s style (e.g. the word contraction “yall” in row
4 of Table 5 is less common in general text, but
could be stylistic of a user).
6 Discussion
6.1 LLM Blind-spots.
Abbreviation expansion may seem to be an easy
task for current LLMs. However, our work fo-
cuses on abbreviations motivated to help users with
severe disabilities, and hence pushes the limit of
keystroke savings. Wherein, the task depends on
recognizing individual characters/alphabets. In-
terestingly, it falls into what could be a "blind-
spot" for the LLMs because the input tokenization
schemes - meant to overcome a discrete vocabulary
- may fall short in recognizing individual characters.
This is now addressed practically e.g. for gener-
ating text in JSON format (Lamini, 2023), using
constrained decoding and following Backus-Naur
Form (BNF) grammar.6.2 Data efficiency and scaling.
Another point of discussion is how personaliza-
tion can be performed on a small amount of data.
Our experiments show that prompt-tuning leads
to higher test accuracy than fine-tuning in limited
data settings. Fine-tuning the full LLM for person-
alization not only generalizes poorly, but is also
very expensive in terms of storing the personalized
model weights. Prompt-tuning on the other hand
only involves storing a very small set of weights
(on the order of thousands) which would make it
not only possible but also convenient to store these
on users’ personal devices. This also makes the
approach more scalable since only a single model
can be served, while clients can query it using dif-
ferent personalized soft prompts. Further, querying
a prompt-tuned model incurs little additional infer-
ence latency, as the learned prompts and the user
input are provided simultaneously to the model
during inference.
7 Conclusion
Our work presents a case study on personalizing
LLMs for the task of abbreviation expansion in the
context of aiding eye-gaze typers with severe motor
and speech disabilities to communicate faster. We
fine-tuned an LLM on generic dialog data for the
task and compared approaches to personalization
using limited user data. We examined fine-tuning,
parameter-efficient prompt-tuning, and retrieval
augmented in-context learning, and find prompt-
tuning to be the most elegant method for person-
alization in terms of its performance as well as its
training data efficiency, small storage requirements,
and ability to scale. Further, initializing the soft-
prompts with concepts and terms relevant to the
user resulted in better prompt-tuned personalized
models.

--- PAGE 6 ---
Error Type Abbreviation Gold Expansion Fine-tuned Prompt-tuned
Unmatched Acronym s i l t r sweet i love that robin i love that robin sweet i love that robin
Overfitting to names g q d , r a m great question dude , robin and mommy greg q day, robin and greg good q doc, robin and mommy
Misses user style (oftenw a d , o d y what a dunce , okie dokie yallwhat about daddy , okie dokie what a day , okie dokie
when lacking context) wipe and dry , ok thanks we are done , ok day yall
Table 5: Examples of some observed categories of errors. Words that the model misses are highlighted in blue in the Gold
expansion, and errors in names are marked in red. (Proper names have been changed to preserve anonymity)
Limitations
The effectiveness of personalization on real us-
age is difficult to study, since it deals with private
and sensitive content. This difficulty is more pro-
nounced when working with people with disabili-
ties. This limits our work to a case study on real
user data for personalization. Identifying interest-
ing techniques to collect realistic personalization
datasets, perhaps synthetic, can benefit the commu-
nity significantly.
We also limit the extent of hyperparameter tun-
ing, due to significant computation resource con-
sumption of experiments. Though we are able to
take advantage of settings shared in literature and
open source code. Also, while our abbreviation ex-
pansion study and models are limited to English, it
will likely translate well to languages with similar
morphology, but that remains to be studied. Our
references to related work in this space may be
limited and further suggestions are welcome.
Ethics and Societal Impact
Techniques that improve Augmentative and Alter-
native Communication (AAC) applications can sig-
nificantly enhance quality of life, increase inde-
pendence and social participation (Caligari et al.,
2013) of people living with communication and
motor disabilities.
A risk of abbreviation expansion is that, when
the expansions are not exactly the ones that the
user desires, they may be tempted to choose a near
similar prediction leading to conveying content that
may be less accurate, misinterpreted, or reflecting
biases and stereotypes of the underlying models.
While the goal of personalization is to mitigate
these, some of the risks still remain. Hence there
is still a subtle risk of reducing speaker’s auton-
omy and authentic self-expression which people
e.g. with ALS (Kane et al., 2017) value highly. An-
other risk is that of frequent incorrect predictions if
personalization is poor for some users. This could
increase effort required to edit minor errors, and
inadvertently increase fatigue.References
Jiban Adhikary, Jamie Berger, and Keith Vertanen. 2021.
Accelerating text communication via abbreviated sen-
tence input. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 6574–6588.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Shanqing Cai, Subhashini Venugopalan, Katrin
Tomanek, Ajit Narayanan, Meredith R Morris, and
Michael P Brenner. 2022. Context-aware abbrevia-
tion expansion using large language models. arXiv
preprint arXiv:2205.03767 .
Marco Caligari, Marco Godi, Simone Guglielmetti,
Franco Franchignoni, and Antonio Nardone. 2013.
Eye tracking communication devices in amyotrophic
lateral sclerosis: impact on disability and quality of
life. Amyotroph Lateral Scler Frontotemporal De-
gener , 14(7-8):546–552.
Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011.
Chameleons in imagined conversations: A new ap-
proach to understanding coordination of linguistic
style in dialogs. arXiv preprint arXiv:1106.3077 .
Patrick W Demasco and Kathleen F McCoy. 1992. Gen-
erating text from compressed input: An intelligent
interface for people with severe motor impairments.
Communications of the ACM , 35(5):68–78.
Kyle Gorman, Christo Kirov, Brian Roark, and Richard
Sproat. 2021. Structured abbreviation expansion in
context. arXiv preprint arXiv:2110.01140 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .

--- PAGE 7 ---
Shaun Kane, Meredith Ringel Morris, Ann Paradiso,
and Jon Campbell. 2017. "at times avuncular and can-
tankerous, with the reflexes of a mongoose": Under-
standing self-expression through augmentative and
alternative communication devices. In Proceedings
of CSCW 2017 .
Lamini. 2023. Guarantee valid json output
with lamini. https://www.lamini.ai/blog/
guarantee-valid-json-output-with-lamini .
Accessed: 2023-12-21.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 .
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. Dailydialog: A manually
labelled multi-turn dialogue dataset. arXiv preprint
arXiv:1710.03957 .
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. arXiv preprint arXiv:2103.10385 .
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-
foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, et al. 2023. Augmented language
models: a survey. arXiv preprint arXiv:2302.07842 .
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant,
Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang.
2022. Sentence-t5: Scalable sentence encoders from
pre-trained text-to-text models. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Kurt Shuster, Eric M Smith, et al. 2020. Recipes
for building an open-domain chatbot. arXiv preprint
arXiv:2004.13637 .
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. NAACL .
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
InInternational Conference on Machine Learning ,
pages 4596–4604. PMLR.Junxiao Shen, Boyin Yang, John J Dudley, and Per Ola
Kristensson. 2022. Kwickchat: A multi-turn dia-
logue system for aac using context-aware sentence
generation by bag-of-keywords. In 27th Interna-
tional Conference on Intelligent User Interfaces ,
pages 853–867.
Stuart M Shieber and Rani Nelken. 2007. Abbreviated
text input using language modeling. Natural Lan-
guage Engineering , 13(2):165–183.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Stephanie Valencia, Richard Cave, Krystal Kallarackal,
Katie Seaver, Michael Terry, and Shaun K Kane.
2023. “the less i type, the better”: How ai language
models can enhance or impede communication for
aac users. In Proceedings of the 2023 CHI Confer-
ence on Human Factors in Computing Systems , pages
1–14.
Keith Vertanen. 2017. Towards improving predictive
aac using crowdsourced dialogues and partner con-
text. In Proceedings of the 19th International ACM
SIGACCESS Conference on Computers and Accessi-
bility , pages 347–348.
Keith Vertanen and Per Ola Kristensson. 2011. The
imagination of crowds: conversational aac language
modeling using crowdsourcing and large data sources.
InProceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing , pages 700–
711.
Tim Willis, Helen Pain, and Shari Trewin. 2005. A prob-
abilistic flexible abbreviation expansion system for
users with motor disabilities. In Accessible Design
in the Digital World Conference 2005 , pages 1–9.
Bruce Wisenburn and D Jeffery Higginbotham. 2008.
An aac application using speaking partner speech
recognition to automatically produce contextually
relevant utterances: Objective results. Augmentative
and Alternative Communication , 24(2):100–109.
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. arXiv
preprint arXiv:2102.09690 .

--- PAGE 8 ---
A Parameter selection
A.1 Fine-tuning learning rates
Fine-tuning learning rate Accuracy@5 BLEU@5
5e-5 26.8 34.3
1e-6 25.4 34.7
1e-5 23.7 31.6
Table 6: Comparing different learning rates for fine-tuning
base-AE model on personalization data. (val set).
A.2 Prompt-tuning learning rates
Prompt-tuning learning rate Accuracy@5 BLEU@5
0.1 35.7 45.6
0.2 31.7 41.7
0.3 30.8 39.9
Table 7: Comparing different learning rates for prompt-tuning
base-AE model on personalization data. soft prompt length of
10 and random initialization (val set).
B Personalization Data
Our personalization dataset was collected with in-
formed consent from a person living with ALS
over a period of five months from late 2021 to early
2022. We refer to the person with ALS as "the
user". The user used a Tobii (R) eye-tracker and
gaze-driven keyboard to enter text for daily commu-
nication. The gaze-typed text was output as speech
audio through text-to-speech (TTS) software. The
user had full control over when to start and stop
data collection. Private and sensitive content in the
data was redacted by trained human curators prior
before we ingested the dataset for research.
The relevant data used for this study consists of
text transcripts of the user’s TTS output. We split
the data into three non-overlapping splits along the
time axis in chronological order as train, validation
and test, containing 630, 285, and 284 sentences,
respectively. We filter the validation and test split
to preserve only the sentences with abbreviation
length ≤10, leading to 194 and 224 sentences,
respectively. No filtering is done on the training
split. As a result, the average abbreviation length in
the train, validation, and test splits are 6.91±6.25,
4.72±2.39, and 5.05±2.74, respectively ( ±1SD).
The sentences belong to 122, 69, and 72 sessions,
respectively, each session being a continuous pe-
riod of conversation data collection. The percent-
ages of proper nouns among the words were 6.73%,
5.88%, and 8.61% in the three splits, respectively.
