# 2309.14021.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.14021.pdf
# Kích thước file: 688263 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Bản thảo: Đang thực hiện
LORD: PHÂN RÃ RANK THẤP CỦA
CÁC LLM MÃ ĐƠN NGÔN NGỮ CHO NÉN MỘT LẦN
Ayush Kaushal
Université de Montréal, Nolano AI
ayush@nolano.ai Tejas Vaidhya
Mila, Université de Montréal, Nolano AI
tejas@nolano.ai
Irina Rish
Mila, Université de Montréal, Nolano AI
irina@nolano.ai
TÓM TẮT
Phân rã rank thấp của ma trận - chia một ma trận lớn thành tích của hai ma trận nhỏ hơn cung cấp một phương tiện nén giảm tham số của mô hình mà không cần sparsification, và do đó mang lại nhiều tăng tốc hơn trên phần cứng hiện đại. Hơn nữa, khác với quantization, các lớp tuyến tính nén vẫn hoàn toàn có thể vi phân và tất cả tham số có thể huấn luyện, đồng thời có thể tận dụng các kernel hiệu quả cao hiện có trên ma trận điểm động. Chúng tôi nghiên cứu tiềm năng nén các Mô hình Ngôn ngữ Lớn (LLM) cho sinh code đơn ngôn ngữ thông qua Phân rã Rank Thấp (LoRD) và quan sát thấy rank cho các lớp tuyến tính trong các mô hình này có thể giảm đến 39.58% với mức tăng perplexity nhỏ hơn 1%. Sau đó chúng tôi sử dụng LoRD để nén StarCoder 16B xuống 13.2B tham số mà không bị sụt giảm và xuống 12.3B với sụt giảm tối thiểu trong điểm HumanEval Pass@1, trong vòng chưa đầy 10 phút trên một A100. Các mô hình nén tăng tốc suy luận lên đến 22.35% chỉ với một dòng thay đổi code so với triển khai huggingface với backend pytorch. Các mô hình LoRD vẫn tương thích với phương pháp quantization gần như không tổn thất hiện đại như SpQR, cho phép tận dụng thêm lợi ích nén của quantization. Cuối cùng, QLoRA trên mô hình LoRD giảm thêm yêu cầu bộ nhớ lên đến 21.2% so với QLoRA vanilla trong khi cung cấp lợi ích tương tự từ fine tuning hiệu quả tham số. Công trình của chúng tôi chỉ ra Phân rã Rank Thấp (LoRD) là một mô hình mới đầy hứa hẹn cho nén LLM.1
1 GIỚI THIỆU
Các LLM Code đã trở thành thành phần không thể thiếu của các Copilot giúp tăng năng suất phát triển (Peng et al., 2023) và trong các agent dựa trên LLM (Wang et al., 2023a). Các LLM Code này có quy mô lớn đến 34 Tỷ tham số cho các mô hình công khai Rozière et al. (2023) và hơn 175 Tỷ tham số cho các mô hình nguồn đóng Chen et al. (2021a). Không chỉ có nhu cầu cấp thiết về giảm kích thước mô hình và chạy mô hình với chi phí thấp hơn, mà còn để tăng tốc độ suy luận. Điều sau đặc biệt quan trọng đối với các ứng dụng dựa trên Copilot.
Gần đây, một số phương pháp đã được đề xuất để nén và tăng tốc suy luận của LLM. Quantization (Frantar et al., 2023; Dettmers et al., 2023b) giảm số bit cần thiết cho mỗi tham số trọng số của LLM bằng cách giảm độ chính xác, và đã cho thấy nén mô hình đáng kể cũng như tăng tốc trong các giai đoạn giải mã batch thấp của LLM Kim et al. (2023a). Quantization cũng đã được chứng minh là tổng quát hóa tốt cho các mô hình đã quantize Shen et al. (2023). Pruning (Sun et al., 2023a; Frantar & Alistarh, 2023) đã cung cấp phương tiện nén khác bằng cách loại bỏ các kết nối khỏi mạng neural và do đó làm thưa thớt các ma trận trọng số của mạng neural. Distillation Gu et al.
1Chúng tôi sẽ phát hành LoRDCoder tại https://huggingface.co/nolanoAI
1arXiv:2309.14021v1 [cs.CL] 25 Sep 2023

--- TRANG 2 ---
Bản thảo: Đang thực hiện
(2023); Agarwal et al. (2023); Jung et al. (2023) cho phép huấn luyện một mô hình nhỏ hơn sử dụng mô hình teacher lớn hơn để giám sát. Trong khi các phương pháp quantization và pruning không yêu cầu huấn luyện lại là các phương tiện khả thi để nén mô hình, distillation liên quan đến một lượng tính toán đáng kể để huấn luyện lại một LLM nhỏ hơn, thường từ đầu. Ở đây, chúng tôi xem xét một mô hình nén khác của Phân rã Rank Thấp (LoRD), không yêu cầu huấn luyện lại đắt đỏ như trong trường hợp distillation và bao quát một số khuyết điểm của phương pháp nén quantization và pruning.
Phân rã Rank Thấp phân tích một ma trận dày đặc của mạng neural thành tích của hai ma trận dày đặc nhỏ hơn. Mô hình LoRD có thể tận dụng các kernel nhân ma trận dày đặc điểm động được tối ưu hóa cao (NVIDIA, 2007; Blackford et al., 2002) đã được viết trên phần cứng hiện đại. Ngược lại, các mô hình quantized yêu cầu các kernel chuyên dụng được viết, thường khác nhau cho mỗi backend phần cứng để cho phép suy luận nhanh. Hơn nữa, mạng neural vẫn hoàn toàn có thể vi phân và tất cả tham số vẫn có thể huấn luyện ngay cả sau khi nén, khác với quantization. Các lớp LoRA Hu et al. (2022) của các mô hình đã điều chỉnh cũng dễ dàng hợp nhất lại vào ma trận điểm động hơn so với những mô hình đã quantize.
Các mô hình đã pruned tạo ra trọng số ma trận thưa thớt trong mạng neural. Nhân ma trận trên ma trận thưa thớt chậm hơn nhiều so với ma trận dày đặc kết quả trong LoRD trên hầu hết GPU. Ma trận dày đặc, ngoài ra tránh overhead định dạng biểu diễn mà ma trận thưa thớt phát sinh từ việc giảm tham số2 và thường yêu cầu kernel chuyên dụng để giảm overhead này Dettmers et al. (2023b). Nhân ma trận dày đặc cũng dễ triển khai hơn nhân ma trận thưa thớt, đặc biệt trên các mô hình quantized.
Một số công trình trước đây đã thử áp dụng các phương pháp phân rã ma trận như SVD, Tucker hoặc phân rã Kronecker để nén (Ben Noach & Goldberg, 2020; Tahaei et al., 2022; Edalati et al., 2022). Tuy nhiên, những công trình này bị giới hạn ở các mô hình ngôn ngữ nhỏ như Bert (Devlin et al., 2019) và GPT2 (Radford et al., 2019), và chỉ thành công trong các trường hợp sử dụng cụ thể hẹp hoặc sau khi huấn luyện lại, thường chỉ với giám sát distillation hướng dẫn bởi teacher. Các công trình này đã quan sát thấy rằng các ma trận trọng số không có rank thấp và thích ứng các phương pháp như Singular Value Decomposition cho phân rã trọng số nhận thức dữ liệu (Chen et al., 2021b; Hsu et al., 2022; Yu & Wu, 2023).
Chúng tôi thích ứng các phương pháp này cho Mô hình Ngôn ngữ Lớn (Tỷ+ Tham số) trên code python, và cho thấy rằng các mô hình này có thể được phân rã rank thấp để nén và tăng tốc suy luận mà không cần huấn luyện lại với ít hoặc không có suy giảm hiệu suất. Chúng tôi nghiên cứu phân rã rank thấp trên hai họ LLM code - StarCoder và CodeGen (§2) cho các kích thước tham số khác nhau và thiết lập tiềm năng giảm rank của các mô hình thông qua phân rã. Sau đó chúng tôi nghiên cứu các xu hướng này trên các loại lớp tuyến tính khác nhau trong một khối transformer và quan sát tiềm năng giảm rank lên đến 39.58% với thay đổi perplexity nhỏ hơn 1%.
Chúng tôi đề xuất các cân nhắc khác nhau để nén các mô hình và đạt được tăng tốc suy luận trên GPU (§3.1). Sử dụng những cân nhắc này, chúng tôi đạt được việc nén mô hình StarCoder 16B cung cấp điểm HumanEval Chen et al. (2021a) Pass@1 31.67 xuống 13.2B tham số với hiệu suất tương tự 31.57 HumanEval và xuống 12.3B tham số với điểm HumanEval 29.22 (§3.2). Các mô hình LoRD cung cấp tăng tốc suy luận cao nhất 22.35% chỉ với một dòng thay đổi trong huggingface (§3.3). Các mô hình LoRD này có thể được nén thêm thông qua phương pháp quantization gần như không tổn thất SpQR Dettmers et al. (2023b) để giảm độ chính xác xuống 8 và 4 bit mà không có sự giảm thêm nào trong hiệu suất HumanEval (§4.1). Cuối cùng, các mô hình phân rã này cũng giảm yêu cầu bộ nhớ của adapter finetuning 21.2% so với QLoRA (§4.2).
2Overhead này trong ma trận thưa thớt xảy ra từ việc phải lưu trữ chỉ số/bitmask để chỉ ra giá trị nào có mặt và không có. Điều này có thể rất đáng kể ở mức độ thưa thớt thấp. Các định dạng thưa thớt của PyTorch (CSR, CSC, COO) đều lưu trữ chỉ số ở định dạng int64, và với mức độ thưa thớt trung bình (<50%), ma trận thưa thớt chiếm nhiều không gian hơn ma trận dày đặc với các giá trị được zero hóa.
2

--- TRANG 3 ---
Bản thảo: Đang thực hiện
2 CÁC LLM CODE CÓ THỂ PHÂN RÃ RANK THẤP
2.1 KIẾN THỨC NỀN TẢNG
Cho một lớp tuyến tính L của một LLM M với trọng số W∈Rd1×d2 và bias b∈Rd1×1. Cho dmin= minimum(d1, d2) và dmax= maximum(d1, d2)
Một Phân rã Rank Thấp hoặc Phân tích Rank Thấp của một lớp L sẽ cho chúng ta một lớp mới L̃ với hai ma trận trọng số A∈Rr×d2 và B∈Rd1×r, và một bias b̃∈Rd1×1, trong đó r << dmin sao cho đối với một batch gồm n vector đầu vào X∈Rd2×n, batch vector đầu ra Y∈Rd1×n là,
Y=L̃(X) = BAX + b̃ ≈ L(X) = WX + b (1)
Singular Value Decomposition (SVD) cung cấp xấp xỉ rank-r tốt nhất của ma trận W∈Rd1×d2. Đầu tiên W có thể được phân rã thành W=USVT, trong đó U∈Rd1×d2 và V∈Rd2×d2 là ma trận trực giao và S∈Rd1×d2 là ma trận chéo với các phần tử theo thứ tự giảm dần. Sau đó, bằng cách lấy top-k rank, chúng ta có thể phân rã W thành tích của hai ma trận rank thấp W≈BA như sau
W= (U:,:rS:r,:r)|{z}
B∈Rd1×r(V:r,:)|{z}
A∈Rr×d2(2)
trong đó :a,:b biểu thị một phép cắt trên ma trận cho a hàng và b cột đầu tiên.
Eigendecomposition là một phương pháp phân rã khác áp dụng được cho ma trận đối xứng. Chúng ta có thể biểu diễn eigendecomposition của ma trận đối xứng W∈Rd1×d1 là W=QΛQT. Ở đây Q∈Rd1×d1 là ma trận trực giao với các cột là eigenvector của W, và Λ∈Rd1×d1 là ma trận chéo với các phần tử là eigenvalue của W được sắp xếp theo thứ tự giảm dần. Tương tự SVD, chúng ta có thể phân rã W thành tích của hai ma trận rank thấp W≈BA bằng cách chỉ giữ lại r eigenvalue lớn nhất (và eigenvector tương ứng) như sau:
W= (Q:,:rΛ:r,:r)|{z}
B∈Rd1×r(QT:r,:)|{z}
A∈Rr×d1(3)
Vì Q là trực chuẩn và eigenvalue Λ được sắp xếp theo thứ tự giảm dần, Q:,:rQT:,:r ≈ I trong đó I là ma trận đơn vị với chiều d1.
Trong khi SVD cho phân rã rank thấp tối ưu của ma trận về mặt chuẩn Frobenius, nhưng không tính đến phân phối dữ liệu đầu vào và đầu ra. Các phương pháp như weighted SVD (Hsu et al., 2022) và SVD trên cả trọng số và dữ liệu (Chen et al., 2021b) đã được đề xuất nhưng quá đắt để mở rộng cho các mô hình lớn hơn do yêu cầu backpropagation trên tập dữ liệu hiệu chuẩn. SVD trên các ma trận trọng số rất lớn cũng rất tốn kém về mặt tính toán.
Vì vậy, chúng tôi thay vào đó tận dụng quan sát rằng các activation trong transformer có rank thấp (Feng et al., 2022) và thích ứng phương pháp dựa trên heuristic của Atomic Feature Mimicking (AFM) (Yu & Wu, 2023) tạo ra các ma trận rank thấp có điều kiện trên một lượng nhỏ dữ liệu hiệu chuẩn.
Cụ thể, xem xét eigen-decomposition của Covariance trên Y là
E[yyT]−E[y]E[y]T=Q̂Λ̂Q̂T(4)
Ở đây Q̂ là ma trận eigenvector của nó, do đó Q̂:,:rQ̂T:,:r ≈ I. Sử dụng điều này, chúng ta có thể viết vector đầu ra Y là Y≈Q̂:,:rQ̂T:,:rY. Bằng cách viết Y theo W, X và b từ Phương trình 1, chúng ta có:
Y≈Q̂:,:rQ̂T:,:rWX + Q̂:,:rQ̂T:,:rb (5)
So sánh với Phương trình 1, điều này cho chúng ta B=Q̂:,:r∈Rd1×r, A=Q̂T:,:rW∈Rr×d2 và b̃=Q̂:,:rQ̂T:,:rb≈b. Phương pháp này cũng đơn giản để thích ứng cho các LLM như LLaMa (Touvron et al., 2023), Falcon (Penedo et al., 2023), CodeLLaMa (Rozière et al., 2023) không có bias term trong lớp tuyến tính bằng cách đặt b̃ thành vector zero.
3

--- TRANG 4 ---
Bản thảo: Đang thực hiện
2.2 CÀI ĐẶT THỰC NGHIỆM
Chúng tôi lấy tập dữ liệu hiệu chuẩn python từ the stack (Kocetkov et al., 2022) và xem xét tập con tương ứng của the stack smol (Bigcode, 2022) như dữ liệu validation. Chúng tôi lọc ra những chuỗi có ít hơn 1024 token hoặc 10240 ký tự. Chúng tôi xem xét họ mô hình CodeGen và StarCoder. Các mô hình CodeGen mono có mặt ở 350M, 2B, 6B và 16B tham số và là các mô hình CodeGen được huấn luyện thêm chỉ trên code python. StarCoder 16B là mô hình StarCoderBase 16B được huấn luyện thêm chỉ trên code python từ tập train của tập dữ liệu stack. Chúng tôi cũng xem xét StarCoderBase ở kích thước tham số 3B và 7B trong họ StarCoder do thiếu các đối tác đơn ngôn ngữ của chúng. Tất cả thí nghiệm của chúng tôi được thực hiện trên một GPU A100 trong vòng dưới một giờ cho mỗi lần chạy.
Để nghiên cứu xu hướng tăng perplexity khi giảm rank trên các kích thước mô hình khác nhau, chúng tôi đặt một rank thấp r cố định cho tất cả các lớp. Sau này chúng tôi thảo luận cách đạt được nén và tăng tốc suy luận thông qua phân rã rank thấp trong §3
2.3 THAY ĐỔI PERPLEXITY THEO GIẢM RANK
Hình 1a và 1b cho thấy xu hướng tăng perplexity theo giảm rank của ma trận trọng số của các mô hình CodeGen và StarCoder. Đối với các mô hình lớn nhất trong cả hai họ, chúng tôi quan sát chỉ khoảng 1% tăng perplexity cho 10% giảm rank, và lên đến 35% giảm rank cho ít hơn 10% tăng perplexity. Tuy nhiên, mô hình nhỏ nhất, CodeGen Mono 350M, chỉ có thể được phân rã đến 35% giảm rank cho một sụt giảm perplexity tương tự. Chúng tôi quan sát rằng perplexity thay đổi chậm hơn nhiều đối với các mô hình lớn hơn khi % rank giảm, và do đó có thể được nén hơn, tương tự như quan sát trong quantization và pruning (Li et al., 2020). Cần lưu ý rằng đối với hầu hết mô hình, giảm hơn 50% dẫn đến suy giảm chất lượng đầu ra đáng kể.
(a) Perplexity vs % Giảm Rank cho các Mô hình CodeGen.
(b) Perplexity vs % Giảm Rank cho các Mô hình StarCoder.
Hình 1: Perplexity vs % Giảm Rank cho các Mô hình Khác nhau.
3 NÉN VÀ TĂNG TỐC THÔNG QUA PHÂN RÃ
Trong phần này, chúng tôi thảo luận cách chúng tôi thích ứng Phân rã Rank Thấp (LoRD) để giảm kích thước mô hình và đạt được tăng tốc suy luận mà không giảm đáng kể chất lượng đầu ra của mô hình. Theo (Kim et al., 2023a), chúng tôi giả định băng thông bộ nhớ là nút thắt cho suy luận, và do đó tăng tốc cho decoding tỷ lệ thuận với kích thước của mô hình transformer.
3.1 ĐẠT ĐƯỢC NÉN VÀ TĂNG TỐC SUY LUẬN
Ngưỡng cho giảm kích thước theo giảm rank: Xem xét một ma trận trọng số W∈Rd1×d2 của một lớp transformer với phân rã rank thấp A∈Rr×d2 và B∈Rd1×r. Số tham số trước và sau phân rã lần lượt là d1d2 và r(d1+d2). Do đó, nếu r > d1d2/(d1+d2), (tức là phân rã với giảm rank nhỏ), thì kích thước mô hình sau phân rã thậm chí có thể cao hơn mô hình gốc. Lý tưởng nhất, chúng ta muốn rank r << d1d2/(d1+d2) hoặc r << dmin.
Tỷ lệ khung hình Ma trận và Nén: Cho tỷ lệ của chiều nhỏ hơn với chiều lớn hơn của ma trận (tức là tỷ lệ khung hình) là α=dmin/dmax. Đối với ma trận vuông, α=1 và đối với ma trận cao hoặc rộng α << 1. Chúng ta có thể viết lại phần trăm thay đổi tham số từ phân rã, theo phần trăm thay đổi rank %Δr = 100*(dmin-r)/dmin% và tỷ lệ khung hình là:
100*r(dmax+dmin)−dmaxdmin/dmaxdmin = 100α−(1+α)%Δr (6)
Cần lưu ý rằng thay đổi tham số từ phân rã có thể dương (số tham số tăng sau phân rã), hoặc âm (số tham số giảm sau phân rã). Để đạt được nén mô hình và do đó tăng tốc suy luận, người ta muốn một phần trăm thay đổi âm rất cao trong tham số.
Hình 2: Điểm Parity trên các tỷ lệ khung hình khác nhau (α) của các lớp tuyến tính khác nhau trong transformer.
Điểm Parity cho Nén theo Giảm Rank: Sử dụng Eq. 6, người ta có thể quan sát rằng giảm rank ít có thể dẫn đến tăng tham số mô hình thay vì giảm. Ví dụ, ma trận vuông (α=1) sẽ tăng 100% (tức là tăng gấp đôi kích thước), khi %Δr→0+ và chỉ sau khi rank giảm hơn 50%, Điểm Parity của việc giảm rank mới đạt được, cung cấp cùng hoặc ít hơn số tham số trong lớp phân rã so với ma trận gốc. Điểm parity này cho ma trận cao hoặc rộng (α→0+), có thể đạt được với một phần trăm giảm rank rất nhỏ và có thể bắt đầu giảm kích thước mô hình. Để đạt được nén, chúng ta muốn giảm rank một lượng để vượt qua ngưỡng điểm parity này. Tuy nhiên, giảm rank quá nhiều có thể làm suy giảm hiệu suất đáng kể. Vì vậy chúng ta phải tính đến tỷ lệ khung hình, để đạt được nén mà không giảm rank nhiều (và do đó không có suy giảm đáng kể chất lượng đầu ra).
Một mô hình transformer có các tỷ lệ khung hình khác nhau trên các lớp tuyến tính khác nhau của nó, α=1.00 cho output projection sau attention, α=0.96 cho các projection Multi-query attention (Shazeer, 2019), α=0.25 cho các projection MLP thông thường với hệ số mở rộng trung gian là 4 như trong transformer gốc và thấp đến α=0.12 cho embedding và language model head projection của CodeGen 16B với kích thước vocab 51200. Hình 2 vẽ % thay đổi kích thước mô hình theo % giảm rank cho ma trận với các tỷ lệ khung hình khác nhau. Đối với ma trận vuông và gần vuông, một giảm rank nhỏ tăng gấp đôi kích thước lớp tuyến tính sau phân rã, và chỉ sau điểm parity 50% giảm, kích thước sau phân rã mới bằng ma trận gốc. Với mức độ phân rã rank này, hiệu suất bắt đầu suy giảm đáng kể, như thấy trong §2.3. Tất cả các công trình trước đây trên các mô hình nhỏ hơn, giải quyết điều này bằng cách huấn luyện lại mô hình (Yu & Wu, 2023; Chen et al., 2021b; Hsu et al., 2022; Ben Noach & Goldberg, 2020), thường thông qua giám sát knowledge distillation (Hinton et al., 2015; Sanh et al., 2019) trên các tác vụ hẹp cụ thể. Tuy nhiên, huấn luyện lại là không khả thi cho các mô hình lớn hơn. Do đó, chúng tôi bỏ qua các ma trận có tỷ lệ khung hình rất cao như output projection hoặc multi-query attention cho phân rã. Ngược lại, các trọng số trong MLP đạt được parity chỉ ở 20% giảm rank. Trong khi embedding và LM Head có thể được nén thông qua phân rã, như chúng đã được cho các mô hình transformer nhỏ hơn (Baevski & Auli, 2019; Lan et al., 2020), chúng chỉ đóng góp một phần rất nhỏ trọng số của mô hình. Vì vậy, chúng tôi không xem xét phân rã các ma trận này. Để giảm tỷ lệ khung hình của ma trận, chúng tôi nhóm các lớp có cùng vector đầu vào để có cùng ma trận bottleneck sau phân rã. Làm như vậy, cho phép sử dụng lại tính toán và chia sẻ trọng số, cũng như giảm tỷ lệ khung hình xuống để đạt được nén với giảm rank thấp hơn. Các lớp ứng cử viên để nhóm bao gồm các ma trận projection query, key và value trong multi-headed attention với tỷ lệ khung hình giảm xuống α=0.33 và lớp gating trong SwiGLU (Shazeer, 2020) với tuyến tính đầu tiên của MLP trong các mô hình như LLaMa (Touvron et al., 2023) với α=0.1875.
(a) CodeGen 16B.
(b) StarCoder 16B.
Hình 3: Giảm Tham số vs perplexity cho phân rã trên các lớp khác nhau.
Xu hướng trên các lớp khác nhau trong một khối transformer: Ngoài việc xem xét điểm parity để quyết định lớp nào cần phân rã, chúng tôi cũng nghiên cứu thêm độ nhạy của mỗi lớp này đối với phân rã rank thấp trên mô hình lớn trong hai họ mô hình. Hình 3 cho thấy sự tăng perplexity vs giảm tham số mô hình cho hai mô hình. Đối với cả hai mô hình, phân rã tất cả các lớp tuyến tính đạt điểm parity muộn hơn nhiều so với bất kỳ lớp tuyến tính nào trong số này với tỷ lệ khung hình thấp. Đối với CodeGen, ma trận trọng số attention (projection query, key và value) cung cấp ít tăng perplexity nhất cho sự sụt giảm lớn nhất trong số lượng tham số, khiến lớp này trở thành ứng cử viên phù hợp nhất để phân rã. Nó cho thấy ít hơn 1% tăng perplexity ngay cả sau 39.58% giảm rank. Chúng tôi quan sát mlp_2 (downscaling mlp) là ứng cử viên tốt hơn cho phân rã so với mlp_1 (upscaling mlp) trên cả hai mô hình. Điều này khiến mlp_2 trở thành ứng cử viên tốt cho phân rã rank thấp trên mô hình StarCoder.
Cân nhắc Phần cứng: Trên các accelerator phần cứng hiện đại như GPU và stack phần mềm tương ứng, các kernel nhân ma trận nhanh hơn nếu kích thước của chúng chia hết cho hệ số cao của 2. Vì vậy, chúng tôi xem xét các rank với giảm khoảng 10% mỗi lần, làm tròn đến bội số gần nhất của 128 trong các thí nghiệm của chúng tôi.
3.2 HIỆU SUẤT CỦA CÁC MÔ HÌNH NÉN
Chúng tôi xem xét các mô hình lớn nhất của họ StarCoder và CodeGen (16B) và thực hiện phân rã rank thấp trên cả hai với các rank khác nhau. Chúng tôi xem xét phân rã các lớp cung cấp giảm tham số nhiều nhất (§3.1) với ít tăng perplexity nhất - mlp_2 cho StarCoder và attention cho CodeGen. Chúng tôi báo cáo điểm Pass@1 và Pass@10 trên tập dữ liệu Human Eval (Chen et al., 2021a) sử dụng repo GitHub code-eval (Bacaj, 2023) trong Bảng 1. Chúng tôi quan sát rằng các mô hình StarCoder có thể được phân rã rank thấp xuống 13.2B tham số (50% giảm rank) mà không có sụt giảm hiệu suất Pass@1 và lên đến 12.3B tham số (62.5% giảm rank) với rất ít sụt giảm. Các mô hình CodeGen cho thấy xu hướng tương tự trong sụt giảm hiệu suất Human Eval khi đo bằng giảm rank. Tuy nhiên, về số lượng giảm tham số, trong khi cho thấy rất ít thay đổi perplexity với giảm rank lớn (Hình 3a), cho thấy sụt giảm nhiều hơn trong điểm HumanEval khi đo bằng số lượng giảm tham số do tỷ lệ khung hình cao hơn của ma trận được phân rã. Cần lưu ý rằng đối với một số mô hình nén, Pass@1 thậm chí cải thiện nhẹ so với mô hình cơ sở. Xu hướng cải thiện nhẹ tương tự từ nén trên các metric và benchmark khác nhau đã được quan sát trong trường hợp các nỗ lực nén khác (Frantar & Alistarh, 2023; Cerebras, 2022).
6

--- TRANG 7 ---
Bản thảo: Đang thực hiện
Starcoder 16B CodeGen 16B Mono
Loại Mô hình Rank Điểm HumanEval Loại Mô hình Rank Điểm HumanEval
Pass @ 1 Pass @ 10 Pass @ 1 Pass @ 10
Mô hình Cơ sở 6144 31.67 48.28 Mô hình Cơ sở 6144 29.02 46.34
LoRDCoder 14.9B 4480 33.18 48.41 LoRDCoder 15.9B 4480 29.08 46.95
LoRDCoder 14.5B 4096 31.69 45.12 LoRDCoder 15.6B 4096 28.90 46.24
LoRDCoder 13.8B 3584 30.90 47.56 LoRDCoder 15.1B 3584 28.54 45.73
LoRDCoder 13.2B 3072 31.57 45.36 LoRDCoder 14.7B 3072 27.99 43.29
LoRDCoder 12.6B 2560 29.84 42.31 LoRDCoder 14.3B 2560 27.32 45.12
LoRDCoder 12.3B 2304 29.22 40.12 LoRDCoder 14.1B 2304 27.07 41.46
Bảng 1: Điểm Human Eval của LoRD trên StarCoder và CodeGen.
3.3 TĂNG TỐC TỪ LORD
Tiếp theo chúng tôi xem xét đánh giá tăng tốc suy luận (forward pass) của các mô hình trên các kernel điểm động cuBLAS tiêu chuẩn. Chúng tôi xem xét triển khai Huggingface tiêu chuẩn (Wolf et al., 2020) của Starcoder với backend pytorch (Paszke et al., 2019) sử dụng các kernel cuBLAS tiêu chuẩn trên GPU A100. Các mô hình phân rã LoRD được triển khai bằng cách chỉ sửa đổi một dòng code để thay thế một MLP bằng một lớp tuyến tính bổ sung3. Chúng tôi benchmark trên chuỗi 1024 token và 512 token, trung bình trên 10 lần chạy với khởi động 3 lần chạy. Chúng tôi vẽ thời gian tương đối và kích thước mô hình theo giảm rank trong Hình 4.
Hình 4: Thời gian và Kích thước mô hình của StarCoder 16B theo rank.
Tăng tốc suy luận cao đến 22.35% được quan sát cho các mô hình phân rã. Các đường trong đồ thị thường dốc xuống, Do đó giảm rank hơn 25% thường ngụ ý ít thời gian suy luận hơn và giảm kích thước mô hình. Tuy nhiên, phần cứng cơ bản (và các kernel phần mềm liên quan) cũng ảnh hưởng đáng kể đến lợi ích tăng tốc. Chúng tôi nhận thấy lợi ích lớn, bất cứ khi nào rank được làm tròn đến bội số của lũy thừa 2 rất cao (như 4096 và 2560 ở 33% và 58% giảm rank), mặc dù rất ít giảm kích thước mô hình. Ngược lại, đối với một số rank là bội số của lũy thừa 2 thấp hơn (như 3584 và 2304 ở 41% và 62% giảm rank) chậm hơn so với những rank cao hơn một chút. Đáng chú ý rằng ảnh hưởng của hình dạng ma trận không hiệu quả phần cứng ít đáng kể hơn đối với chuỗi token dài hơn 1024 vì overhead attention O(n2) bắt đầu trở nên đáng kể hơn, đặc biệt là trong trường hợp không có kỹ thuật triển khai attention SoTA (Rabe & Staats, 2021; Dao et al., 2022; Dao, 2023) như trong trường hợp triển khai của Huggingface.
4 KẾT HỢP LORD VỚI QUANTIZATION VÀ LORA
4.1 QUANTIZATION
Trong khi LoRD cho phép nén ở cùng mức độ chính xác, chúng tôi nghiên cứu liệu các mô hình phân rã có thể được nén thêm thông qua quantization. Bảng 2 cho thấy kết quả HumanEval pass@1 cho các LoRDCoder khác nhau trên mức quantization 8 và 4 bit, sử dụng kỹ thuật quantization gần như không tổn thất SpQR (Dettmers et al., 2023b). Chúng tôi quan sát rằng các mô hình LoRD có thể được kết hợp với quantization để nén thêm, cho thấy không có sụt giảm hiệu suất cho 8-bit và rất ít sụt giảm hiệu suất trên quantization 4-bit cho hầu hết mô hình. Tăng nhẹ trong HumanEval sau quantization cũng được quan sát, tương tự như Pangu-Coder2 (Shen et al., 2023).
4.2 ĐIỀU CHỈNH HIỆU QUẢ THAM SỐ CỦA CÁC MÔ HÌNH LORD
3nn.Linear(in, out) -> nn.Sequential(nn.Linear(in, rank), nn.Linear(rank, out))
7

--- TRANG 8 ---
Bản thảo: Đang thực hiện
Mô hình Pass@1@FP16 Pass@1@8-bit Pass@1@4-bit
LoRDCoder 14.9B 33.18 33.17 32.01
LoRDCoder 14.5B 31.69 31.58 32.74
LoRDCoder 13.8B 30.90 31.10 30.73
LoRDCoder 13.2B 31.57 31.52 32.01
LoRDCoder 12.6B 29.84 29.87 30.22
LoRDCoder 12.3B 29.22 29.14 29.45
Bảng 2: Điểm Human Eval của các mô hình LoRDCoder quantized.
d x d
d x r'r' x d
d x r'r' x d
d x rr x d
drh
xdrh
xrLoRD
Trọng số a) LoRA b) LoRD + LoRA
Trọng số
Pretrained
A RX RY R
W RX RY R
B R
Hình 5: LoRA vs LoRD + LoRA.
Tiếp theo chúng tôi kiểm tra tiềm năng sử dụng LoRD để giảm thêm việc sử dụng bộ nhớ trên các kỹ thuật hiệu quả tham số hiện có. Chúng tôi xem xét tập dữ liệu code instruction (Chaudhary, 2023) và lọc những ví dụ liên quan đến lập trình python. Chúng tôi sử dụng QLoRA (Dettmers et al., 2023a), là phiên bản hiệu quả bộ nhớ hơn nữa của LoRA (Hu et al., 2022) lưu trữ trọng số ở định dạng quantized, để fine-tuning trong 1 epoch. Chúng tôi so sánh kết quả từ fine-tuning hai mô hình phân rã LoRDCoder 13.2B và LoRDCoder 12.3B với mô hình StarCoder. Chúng tôi quan sát HumanEval pass@1 là 37.80 và 37.62 trên fine-tuning LoRDCoder 13.2B và LoRDCoder 12.3B, cạnh tranh với hiệu suất 37.74 của mô hình StarCoder.
5 CÔNG TRÌNH LIÊN QUAN
Có sự quan tâm ngày càng tăng đối với việc nén các Mô hình Ngôn ngữ Lớn pretrained. Một số nỗ lực gần đây đã được dành cho quantization trọng số của LLM (Frantar et al., 2023; Lin et al., 2023; Yuan et al., 2023; Park et al., 2022; Kim et al., 2023b; Chee et al., 2023; Li et al., 2023a) với các thủ thuật như tách outlier (Dettmers et al., 2022; Dettmers & Zettlemoyer, 2022; Dettmers et al., 2023c; Wei et al., 2022; Kim et al., 2023a; Lee et al., 2023). Một số nỗ lực cũng quantize các activation (biểu diễn trung gian) ngoài trọng số để tăng tốc thời gian tính toán (Shao et al., 2023; Xiao et al., 2023). Các công trình về quantization gần nhất với chúng tôi là Chiến lược Low-Rank Compensation (LoRC) (Yao et al., 2023; Wu et al., 2023), trong đó sự khác biệt của ma trận quantized so với ma trận gốc được xấp xỉ bằng tích của các ma trận rank thấp. Công trình của chúng tôi phân rã toàn bộ ma trận để nén.
Pruning mạng neural Liang et al. (2021), khác với quantization, giảm số tham số trong mô hình bằng cách loại bỏ các trọng số hoặc kết nối không quan trọng. Một số kỹ thuật đã được đề xuất để mở rộng các phương pháp pruning cho LLM (Sun et al., 2023a; Frantar & Alistarh, 2023; Ma et al., 2023). Tuy nhiên, pruning như một phương tiện nén vẫn chưa trở nên khả thi do không có tăng tốc trên ma trận thưa thớt mà không có sụt giảm hiệu suất đáng kể ở mức độ thưa thớt cực độ hoặc thưa thớt có cấu trúc (Zhu et al., 2023). Với phân rã rank thấp, chúng tôi đề xuất một phương pháp thay thế để giảm tham số mô hình cung cấp tăng tốc ngay cả khi giảm ít số lượng tham số. Một số công trình cũng đã thử (Ren & Zhu, 2023; Li et al., 2023b) chia một ma trận dày đặc thành tổng của ma trận rank thấp và ma trận thưa thớt. Tuy nhiên, các phương pháp này yêu cầu huấn luyện lại và đã được chứng minh chỉ hoạt động cho Mô hình Ngôn ngữ ít hơn một tỷ tham số.
Phân rã rank thấp đã được đề xuất cho các mô hình ngôn ngữ nhỏ hơn như Bert hoặc GPT2 trước đây sử dụng phân rã SVD (Ben Noach & Goldberg, 2020) và phân rã Kronecker (Tahaei et al., 2022; Edalati et al., 2022). Hsu et al. (2022) đã sửa đổi SVD để nhận thức dữ liệu dựa trên thông tin gradient bậc hai xấp xỉ. Một weighted SVD tốt hơn được đề xuất bởi (Hua et al., 2022). Chen et al. (2021b) đề xuất một phương pháp phân rã nhận thức dữ liệu với giải pháp closed-form tối ưu có thể chứng minh, sử dụng một lượng lớn điểm dữ liệu trên các tác vụ cụ thể để phân rã. Một số công trình gần đây (Yu & Wu, 2023; Feng et al., 2022) đã chỉ ra rằng trong khi ma trận trọng số của mạng neural không có rank thấp vốn có, các biểu diễn trung gian thì có, do đó đề xuất phân rã dựa trên biểu diễn. Tất cả các công trình này đã tập trung vào các mô hình ngôn ngữ nhỏ và yêu cầu huấn luyện lại. Chúng tôi đề xuất phân rã rank thấp để nén mạng neural mà không cần huấn luyện lại. Phân tích cũng đã được sử dụng chỉ cho các lớp embedding (Baevski & Auli, 2019; Lan et al., 2020), vì chúng là ứng cử viên tốt do tỷ lệ khung hình rất thấp 0.015, trong đó giảm rank thậm chí 5% cũng dẫn đến giảm số tham số sau phân rã.
Cũng có sự quan tâm ngày càng tăng đối với fine-tuning các mô hình ngôn ngữ lớn Taori et al. (2023); Chiang et al. (2023); Wang et al. (2023b); Sun et al. (2023b). Với yêu cầu bộ nhớ lớn để fine-tuning tất cả tham số của LLM, các phương pháp fine-tuning hiệu quả tham số hơn như LoRA (Hu et al., 2022) đang được áp dụng rộng rãi. Các phương pháp này đóng băng trọng số LLM gốc, và gắn hai ma trận rank thấp hoặc adapter, trong một skip-connection (He et al., 2016) vào các lớp tuyến tính của mô hình. Các phương pháp fine-tuning hiệu quả tham số này đã thấy cải thiện trong bộ nhớ activation thấp hơn (Zhang et al., 2023) hoặc bằng cách giữ trọng số mô hình không thể huấn luyện ở độ chính xác 4-bit (Dettmers et al., 2023a). Công trình của chúng tôi, trong khi tập trung vào nén thông qua phân rã rank thấp, cũng có thể cho phép fine-tuning hiệu quả hơn, đặc biệt là kết hợp với các phương pháp hiện có.
6 KẾT LUẬN
Chúng tôi đã nghiên cứu việc nén các mô hình sinh code đơn ngôn ngữ thông qua một mô hình nén một lần mới của phân rã rank thấp. Chúng tôi phân tích sự thay đổi perplexity với thay đổi rank trên các họ mô hình StarCoder và CodeGen cũng như các lớp riêng lẻ của chúng và quan sát rằng rank của các mô hình này có thể được giảm lên đến 39.58% với ít hơn 1% thay đổi perplexity. Sau đó chúng tôi đề xuất các cân nhắc để nén một lần các mô hình này thông qua Phân rã Rank Thấp (LoRD) trong vòng dưới 10 phút. Do đó, chúng tôi nén StarCoder 16B xuống 13.2B mà không có sụt giảm HumanEval pass@1 và rất ít sụt giảm HumanEval pass@1 xuống 12.3B tham số. Với một thay đổi tối thiểu trong code so với code suy luận mặc định của huggingface chỉ một dòng, chúng tôi đạt được tăng tốc lên đến 22.35%. Các mô hình LoRD cũng tương thích với các kỹ thuật quantization gần như không tổn thất của SpQR, cung cấp lợi ích của nén dựa trên quantization ngoài những lợi ích từ phân rã. Các mô hình LoRD cũng giảm yêu cầu bộ nhớ lên đến 21.2% so với fine-tuning QLoRA vanilla.
7 TÁC ĐỘNG RỘNG HƠN VÀ CÔNG TRÌNH TƯƠNG LAI
Công trình của chúng tôi về LoRD, nén các LLM code cho phép chúng chạy trên các GPU nhỏ hơn bao gồm cả GPU cấp người tiêu dùng. Điều này đặc biệt quan trọng cấp thiết cho vài năm tới khi tình trạng thiếu hụt nguồn cung GPU so với nhu cầu ngày càng tăng trong thị trường hiện tại. Hơn nữa, suy luận nhanh hơn giúp giảm chu kỳ GPU, cho phép chi phí vận hành thấp hơn và tiêu thụ năng lượng thấp hơn cho suy luận LLM. Công trình của chúng tôi giúp giảm lượng khí thải carbon phát sinh và hướng tới một NLP xanh hơn. Thông qua nén, công trình của chúng tôi cũng thúc đẩy suy luận tại edge, và do đó mở ra không gian cho các ứng dụng liên quan đến yêu cầu bảo mật nghiêm ngặt. Độ trễ thấp hơn cũng sẽ giúp cải thiện Trải nghiệm Người dùng trong các ứng dụng như CoPilot nơi độ trễ giữa các gợi ý có thể ảnh hưởng đến năng suất của nhà phát triển. Một số lợi ích này của LoRD như chi phí và tiêu thụ năng lượng thấp hơn cũng áp dụng cho các trường hợp sử dụng fine-tuning của LLM.
Công trình của chúng tôi mở ra một mô hình mới cho nén thông qua Phân rã Rank Thấp trên Mô hình Ngôn ngữ Lớn trong một lần mà không cần huấn luyện lại. Vì các mô hình LoRD có thể tận dụng các kernel điểm động hiện có trên BLAS và cuBLAS, ngược lại với quantization, chúng dễ triển khai và thu được lợi ích suy luận hơn nhiều. Nghiên cứu của chúng tôi về các cân nhắc phần cứng cho tăng tốc cũng mở ra tiềm năng điều chỉnh rank của các mô hình phân rã để phù hợp nhất trên phần cứng mục tiêu và các kernel GEMM đi kèm. Trong khi nghiên cứu của chúng tôi bị giới hạn ở các LLM code đơn ngôn ngữ, kỹ thuật phân rã rank thấp là tổng quát và không cụ thể cho lĩnh vực code. Do đó khám phá khả năng áp dụng của nó cho các mô hình mục đích chung hơn như LLaMa là một hướng đầy hứa hẹn cho việc nén các LLM transformer ngoài quantization. Một câu hỏi thú vị khác chưa được khám phá là liệu các module LoRA hoặc QLoRA được fine-tuned trên các mô hình gốc, có thể được cắm vào nguyên trạng cho các mô hình LoRD mà không có sụt giảm hiệu suất nào.
9

--- TRANG 10 ---
Bản thảo: Đang thực hiện
TÀI LIỆU THAM KHẢO
Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, và Olivier Bachem. Gkd: Generalized knowledge distillation for auto-regressive sequence models, 2023.
Anton Bacaj. code-eval. https://github.com/abacaj/code-eval, tháng 7 năm 2023.
Alexei Baevski và Michael Auli. Adaptive input representations for neural language modeling. Trong International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ByxZX20qFQ.
Matan Ben Noach và Yoav Goldberg. Compressing pre-trained language models by matrix decomposition. Trong Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, trang 884–889, Suzhou, Trung Quốc, tháng 12 năm 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.aacl-main.88.
Project Bigcode. The stack smol, 2022. URL https://huggingface.co/datasets/bigcode/the-stack-smol.
L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Demmel, Jack Dongarra, Iain Duff, Sven Hammarling, Greg Henry, et al. An updated set of basic linear algebra subprograms (blas). ACM Transactions on Mathematical Software, 28(2):135–151, 2002.
Team Cerebras. Creating sparse gpt-3 models with iterative pruning, 11 2022. URL https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning.
Sahil Chaudhary. Code instructions dataset. https://huggingface.co/datasets/sahil2801/code_instructions_120k, tháng 6 năm 2023.
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, và Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees, 2023.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Evaluating large language models trained on code, 2021a.
Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, và Cho-Jui Hsieh. Drone: Data-aware low-rank compression for large nlp models. Trong M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, và J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, trang 29321–29334. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, tháng 3 năm 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.
10

--- TRANG 11 ---
Bản thảo: Đang thực hiện
Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, và Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.
Tim Dettmers và Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws, 2022.
Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023a.
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, và Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023b.
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, và Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023c.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota, tháng 6 năm 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Nia, James Clark, và Mehdi Rezagholizadeh. Kronecker decomposition for GPT compression. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), trang 219–226, Dublin, Ireland, tháng 5 năm 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.24. URL https://aclanthology.org/2022.acl-short.24.
Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, và Zheng-Jun Zha. Rank diminishing in deep neural networks. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=tIqzLFf3kk.
Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. Trong The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.
Yuxian Gu, Li Dong, Furu Wei, và Minlie Huang. Knowledge distillation of large language models, 2023.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 770–778, 2016. doi: 10.1109/CVPR.2016.90.
Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network, 2015.
Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, và Hongxia Jin. Language model compression with weighted low-rank factorization. Trong International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5.
11

--- TRANG 12 ---
Bản thảo: Đang thực hiện
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. LoRA: Low-rank adaptation of large language models. Trong International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, và Hongxia Jin. Numerical optimizations for weighted low-rank estimation on language models. Trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, trang 1404–1416, Abu Dhabi, United Arab Emirates, tháng 12 năm 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.91. URL https://aclanthology.org/2022.emnlp-main.91.
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, và Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing, 2023.
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, và Kurt Keutzer. Squeezellm: Dense-and-sparse quantization, 2023a.
Young Jin Kim, Rawn Henry, Raffy Fahim, và Hany Hassan Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms, 2023b.
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, và Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. Trong International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, và Eunhyeok Park. Owq: Lessons learned from activation outliers for weight quantization in large language models, 2023.
Liang Li, Qingyuan Li, Bo Zhang, và Xiangxiang Chu. Norm tweaking: High-performance low-bit quantization of large language models, 2023a.
Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, và Tuo Zhao. Losparse: Structured compression of large language models based on low-rank and sparse approximation, 2023b.
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, và Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference of transformers. Trong Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020.
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, và Xiaotong Zhang. Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2021.07.045. URL https://www.sciencedirect.com/science/article/pii/S0925231221010894.
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, và Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2023.
Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large language models, 2023.
Corporation NVIDIA. Compute unified device architecture (cuda). Website, 2007. URL https://developer.nvidia.com/cuda-toolkit. Truy cập: 2023-09-17.
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, và Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models, 2022.
12

--- TRANG 13 ---
Bản thảo: Đang thực hiện
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, chương ., trang .. Curran Associates Inc., Red Hook, NY, USA, 2019.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
Sida Peng, Eirini Kalliamvakou, Peter Cihon, và Mert Demirer. The impact of ai on developer productivity: Evidence from github copilot, 2023.
Markus N. Rabe và Charles Staats. Self-attention does not need o(n2) memory, 2021.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.
Siyu Ren và Kenny Q. Zhu. Low-rank prune-and-factorize for language model compression, 2023.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, và Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.
Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019.
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, và Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models, 2023.
Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019.
Noam Shazeer. Glu variants improve transformer, 2020.
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, và Qianxiang Wang. Pangu-coder2: Boosting large language models for code with ranking feedback, 2023.
Mingjie Sun, Zhuang Liu, Anna Bair, và J. Zico Kolter. A simple and effective pruning approach for large language models, 2023a.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, và Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision, 2023b.
Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, và Mehdi Rezagholizadeh. Kronecker-BERT: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 2116–2127, Seattle, United States, tháng 7 năm 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.154. URL https://aclanthology.org/2022.naacl-main.154.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. CRFM Stanford, tháng 3 năm 2023. URL https://crfm.stanford.edu/2023/03/13/alpaca.html.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
13

--- TRANG 14 ---
Bản thảo: Đang thực hiện
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, và Ji-Rong Wen. A survey on large language model based autonomous agents, 2023a.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, và Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b.
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, và Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=yW5zeRSFdZ.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander Rush. Transformers: State-of-the-art natural language processing. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 38–45, Online, tháng 10 năm 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.
Xiaoxia Wu, Zhewei Yao, và Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats, 2023.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, và Song Han. SmoothQuant: Accurate and efficient post-training quantization for large language models. Trong Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, trang 38087–38099. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/xiao23c.html.
Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, và Yuxiong He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation, 2023.
Hao Yu và Jianxin Wu. Compressing transformers: Features are low-rank, but weights are not! Proceedings of the AAAI Conference on Artificial Intelligence, 37(9):11007–11015, Jun. 2023. doi: 10.1609/aaai.v37i9.26304. URL https://ojs.aaai.org/index.php/AAAI/article/view/26304.
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, và Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models, 2023.
Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, và Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, và Weiping Wang. A survey on model compression for large language models, 2023.
14
