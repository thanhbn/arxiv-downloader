# 2401.00243.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2401.00243.pdf
# Kích thước tệp: 635953 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Học Tăng Cường từ Phản Hồi Con Người với Phạt Không Chắc Chắn
với Tập Hợp LoRA Phần Thưởng Đa Dạng
Yuanzhao Zhai1 2Han Zhang3 4Yu Lei4Yue Yu1 2Kele Xu1 2Dawei Feng1 2Bo Ding1 2Huaimin Wang1 2
Tóm tắt
Học tăng cường từ phản hồi con người (RLHF) nổi lên như một mô hình đầy hứa hẹn để căn chỉnh các mô hình ngôn ngữ lớn (LLM). Tuy nhiên, một thách thức đáng chú ý trong RLHF là tối ưu hóa quá mức, trong đó vượt qua một ngưỡng nhất định, việc theo đuổi phần thưởng cao hơn dẫn đến sự suy giảm trong sở thích của con người. Trong bài báo này, chúng tôi quan sát thấy điểm yếu của chính quy hóa KL thường được sử dụng trong các phương pháp RLHF hiện tại để giải quyết tối ưu hóa quá mức. Để giảm thiểu hạn chế này, chúng tôi xem xét kỹ mục tiêu RLHF trong tập dữ liệu ngoại tuyến và đề xuất RLHF phạt không chắc chắn (UP-RLHF), kết hợp chính quy hóa không chắc chắn trong quá trình tinh chỉnh RL. Để tăng cường khả năng định lượng không chắc chắn cho các mô hình phần thưởng, trước tiên chúng tôi đề xuất một tập hợp thích ứng thấp hạng (LoRA) đa dạng bằng cách tối đa hóa chuẩn hạt nhân của các nối ma trận LoRA. Sau đó, chúng tôi tối ưu hóa các mô hình chính sách sử dụng phần thưởng bị phạt, được xác định bởi cả phần thưởng và không chắc chắn được cung cấp bởi các tập hợp LoRA phần thưởng đa dạng. Kết quả thực nghiệm của chúng tôi, dựa trên hai tập dữ liệu sở thích thực của con người, cho thấy hiệu quả của các tập hợp LoRA phần thưởng đa dạng trong việc định lượng không chắc chắn phần thưởng. Ngoài ra, chính quy hóa không chắc chắn trong UP-RLHF tỏ ra quan trọng trong việc giảm thiểu tối ưu hóa quá mức, từ đó góp phần vào hiệu suất tổng thể.

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) sở hữu khả năng phi thường, đặc biệt trong việc tạo nội dung sáng tạo (Brown et al., 2020). Được thúc đẩy bởi kho dữ liệu khổng lồ từ internet, có thể chứa dữ liệu chất lượng thấp và có khả năng thiên vị, LLM có thể tạo ra những sự thật bịa đặt, văn bản thiên vị hoặc độc hại, và thậm chí nội dung có hại cho con người (Perez et al., 2022; Kreps et al., 2022). Trong việc theo đuổi giải quyết những vấn đề này, học tăng cường từ phản hồi con người (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022; Touvron et al., 2023) đã nổi lên như một phương pháp tiếp cận chiếm ưu thế trong lĩnh vực căn chỉnh AI cho LLM.

RLHF bao gồm tinh chỉnh ba bước, như được hiển thị trong Hình 1. Bước 1 chứa tinh chỉnh có giám sát (SFT) trên tập dữ liệu minh họa, và các mô hình phần thưởng được huấn luyện để gần đúng sở thích con người liên quan đến văn bản đầu ra được tạo trong Bước 2. Trong Bước 3, LLM được khái niệm hóa như các mô hình chính sách được tối ưu hóa bởi các thuật toán học tăng cường (RL), như REINFORCE (Williams, 1992), A2C (Mnih et al., 2016) và PPO (Schulman et al., 2017). Với các gợi ý, LLM được tối ưu hóa để đưa ra các câu trả lời tối đa hóa điểm số được cung cấp bởi mô hình phần thưởng (RM).

Mặc dù thành công, một trong những vấn đề thách thức nhất trong RLHF là tối ưu hóa quá mức RM (Gao et al., 2023). Tối ưu hóa quá mức có nghĩa là tối ưu hóa LLM bằng cách tối đa hóa phần thưởng của RM vượt qua một ngưỡng nhất định có thể dẫn đến sự suy giảm sở thích con người, có thể được xấp xỉ bởi mô hình phần thưởng vàng trong thực tế. Các trường hợp bao gồm tạo ra thông tin ảo tưởng để giả vờ có chuyên môn, hoặc thậm chí tạo ra các phản hồi quá dài dòng có thể gây ra các lỗi lặp lại (Beeching et al., 2023). Chúng tôi cho rằng vấn đề chủ yếu được gây ra bởi RM quá tự tin, được huấn luyện trên các tập dữ liệu hạn chế và chỉ là một đại diện không hoàn hảo cho sở thích con người. Nếu một RM sai lầm gán phần thưởng cao cho một số mẫu ngoài phân phối (OOD), LLM có thể bị dẫn lạc vào việc đưa ra nội dung chất lượng thấp.

--- TRANG 2 ---
Học Tăng Cường từ Phản Hồi Con Người với Phạt Không Chắc Chắn với Tập Hợp LoRA Phần Thưởng Đa Dạng

Các nghiên cứu RLHF gần đây đã chứng minh tầm quan trọng của việc đưa ra các hình phạt Kullback-Leibler (KL) như chính quy hóa để giảm thiểu vấn đề tối ưu hóa quá mức (Ouyang et al., 2022; Touvron et al., 2023; Yang et al., 2023). Trực giác là việc thêm chính quy hóa KL có thể điều chỉnh độ lệch đầu ra của các mô hình chính sách từ mô hình SFT. Tuy nhiên, chính quy hóa KL dễ bị overfitting (Azar et al., 2023), gây ra sự giảm hiệu suất vàng (Gao et al., 2023). Các phương pháp khác để giảm thiểu tối ưu hóa quá mức bao gồm mở rộng tham số hoặc kích thước dữ liệu huấn luyện của RM (Gao et al., 2023), RM tổng hợp theo các khía cạnh khác nhau (Moskovitz et al., 2023). Chúng tôi cho rằng những phương pháp này không phải lúc nào cũng khả thi vì chi phí đắt đỏ đáng kể.

Trong bài báo này, chúng tôi xem xét lại mục tiêu tối ưu hóa của RLHF với các tập dữ liệu ngoại tuyến và cho thấy chính quy hóa KL bắt nguồn từ tập dữ liệu minh họa Bước 1 dẫn đến chính quy hóa yếu cho các mẫu OOD chất lượng thấp. Dựa trên quan sát này, chúng tôi đề xuất RLHF phạt không chắc chắn (UP-RLHF), hỗ trợ chính quy hóa không chắc chắn bổ sung. Chúng tôi đầu tiên đề xuất tập hợp LoRA phần thưởng đa dạng thông qua tối đa hóa chuẩn hạt nhân trong bước 2. Cụ thể, chúng tôi nối nhiều ma trận LoRA và tối đa hóa chuẩn hạt nhân để tích cực đa dạng hóa các tập hợp LoRA. Theo cách này, chúng tôi huấn luyện các tập hợp LoRA đa dạng, cho phép các mô hình phần thưởng có khả năng tốt về định lượng không chắc chắn theo cách hiệu quả tham số. Sau đó, chúng tôi phạt phần thưởng bằng không chắc chắn ước tính và áp dụng cả chính quy hóa KL và không chắc chắn để giảm thiểu tối ưu hóa quá mức. UP-RLHF có thể ngăn chặn LLM đưa ra nội dung chất lượng thấp với độ không chắc chắn cao, nơi chính quy hóa KL yếu, từ đó giảm thiểu vấn đề tối ưu hóa quá mức.

Tóm lại, các đóng góp của chúng tôi là: (1) Chúng tôi đề xuất UP-RLHF, tăng cường RLHF với chính quy hóa không chắc chắn bằng cách phạt phần thưởng với không chắc chắn được cung cấp bởi mô hình phần thưởng. (2) Chúng tôi đề xuất huấn luyện các mô hình phần thưởng với tập hợp LoRA đa dạng. Phương pháp hiệu quả tham số này thể hiện hiệu quả của nó trong việc huấn luyện các mô hình phần thưởng nhận thức không chắc chắn. (3) Kết quả thực nghiệm cho thấy hiệu quả của UP-RLHF trong việc loại bỏ tối ưu hóa quá mức và cải thiện hiệu suất theo phần thưởng vàng.

2. Kiến thức cơ bản
2.1. Học tăng cường từ phản hồi con người
Đối với một nhiệm vụ NLP, chúng ta được cung cấp một tập dữ liệu có giám sát D={(x(i),y(i))}i=1,2,···ofN ví dụ, trong đó x∈X là các gợi ý và y∈Y là các câu trả lời mục tiêu. Chúng tôi phác thảo quy trình RLHF, được áp dụng trong các nghiên cứu tiếp theo (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022b).

Bước 1: Tinh chỉnh có giám sát: Giai đoạn ban đầu bắt đầu với một LLM được huấn luyện trước, chịu tinh chỉnh thông qua học có giám sát, thường sử dụng mất mát cross-entropy, với các mẫu (x,y). Kết quả của giai đoạn này được ký hiệu là πSFT.

Bước 2: Mô hình hóa phần thưởng. Trong giai đoạn tiếp theo, tập dữ liệu sở thích có dạng (x,yw,yl) được sử dụng để huấn luyện các mô hình phần thưởng, trong đó yw là cái được người gắn nhãn ưa thích và yl là cái ít được ưa thích hơn. Theo các nguyên tắc của mô hình Bradley-Terry (Bradley & Terry, 1952), mất mát xếp hạng của việc huấn luyện mô hình phần thưởng là:

LRM=∑xlogσ(r(yw|x)−r(yl|x)), (1)

trong đó σ là hàm sigmoid. Mô hình phần thưởng r được khởi tạo với πSFT bằng cách thay thế đầu ngôn ngữ bằng đầu giá trị.

Bước 3: Tinh chỉnh RL. Đối với một gợi ý x được lấy mẫu từ tập dữ liệu D, mô hình ngôn ngữ cần tối ưu hóa được ký hiệu là πθ, tạo ra câu trả lời mục tiêu y. Hàm chuyển đổi một cách xác định thêm câu trả lời y vào cuối gợi ý x. Sau đó, mô hình phần thưởng đã học cung cấp phần thưởng theo quỹ đạo r(y|x). Các nghiên cứu trước đây công thức hóa bài toán tối ưu hóa như:

arg maxπθEx∼D,y∼πθ(·|x)[r(y|x)−βlog(πθ(y|x)/πSFT(y|x)], (2)

trong đó β kiểm soát cường độ của hình phạt KL. Hình phạt KL βlog(πθ(y|x)/πSFT(y|x)) được sử dụng để điều chỉnh độ lệch từ mô hình SFT. Các nghiên cứu hiện tại sử dụng các thuật toán RL (Ouyang et al., 2022; Touvron et al., 2023; Li et al., 2023b), thường là PPO (Schulman et al., 2017), để giải quyết mục tiêu 2.

2.2. Thích ứng thấp hạng
Như một trong những phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) phổ biến nhất, LoRA (Hu et al., 2022) giới thiệu các mô-đun bỏ qua để cập nhật các mô hình được huấn luyện trước thông qua phép chiếu lên-xuống, bao gồm các ma trận chiếu xuống được ký hiệu là A và các ma trận chiếu lên được ký hiệu là B. Trong suốt quá trình tinh chỉnh, mô hình khởi tạo với trọng số được huấn luyện trước cố định W(0) và phát triển thành W=W(0)+ΔW. Đối với mỗi đơn vị LoRA, lượt truyền tới có thể được biểu diễn như:

zout=W(0)zin+ΔWzin=W(0)zin+BAzin, (3)

trong đó zin,zout∈Rn×d là đầu vào và đầu ra của các lớp transformer, W,W(0),ΔW∈Rd×d, A∈Rr×d và B∈Rd×r với r≪d. Trong quá trình khởi tạo huấn luyện, khởi tạo Gaussian ngẫu nhiên được áp dụng cho A, trong khi B được khởi tạo bằng không. LoRA giới thiệu ít tham số có thể huấn luyện hơn đáng kể, thường ít hơn 1% kích thước mô hình gốc.

3. Phương pháp
3.1. Phân tích chính quy hóa trong RLHF
RLHF có thể được công thức hóa như RL ngược với các tập dữ liệu ngoại tuyến D. Chúng tôi phân tích lý thuyết mục tiêu tổng thể của nó không thể giải được và cho thấy cách tối ưu hóa nó một cách gần đúng. Nhắc lại mục tiêu ban đầu của chúng ta là tìm một chính sách tối đa hóa phần thưởng dự kiến theo quỹ đạo:

arg maxπθE(x,y)∼ρπθr(y|x), (4)

trong đó ρπθ là độ đo chiếm dụng phụ thuộc vào chính sách πθ. Tối ưu hóa Phương trình 4 đặt ra thách thức do sự phụ thuộc lẫn nhau của ρπθ và πθ, kết hợp với sự cần thiết thu thập mẫu từ πθ. Với xấp xỉ bậc nhất của mục tiêu (Schulman et al., 2015; Peng et al., 2019), chúng ta có thể công thức hóa bài toán tối ưu hóa chính sách có ràng buộc sau:

arg maxπθ∫xD(x)∫yπθ(y|x)r(y|x)dydx
s.t.∫xD(x)DKL(πθ(y|x)||πD(y|x))dx≤ϵ, (5)

trong đó πD là chính sách hành vi được gợi ý bởi D. Ràng buộc trong Phương trình 5 đảm bảo rằng chính sách mới πθ gần với phân phối dữ liệu của πD, và do đó mục tiêu thay thế vẫn là một xấp xỉ hợp lý.

Hình thành Lagrangian của bài toán tối ưu hóa có ràng buộc được trình bày ở trên, chúng ta có được hàm mất mát:

Lθ=∫xD(x)∫yπθ(y|x)r(y|x)dydx+β∫xD(x)DKL(πθ(y|x)||πD(y|x))dx, (6)

trong đó β là một nhân tử Lagrange. Khi lấy đạo hàm hàm mục tiêu L(π,β) theo πθ(y|x) và sau đó giải cho chính sách tối ưu π⋆, biểu thức kết quả cho chính sách tối ưu như sau:

π⋆(y|x)=1/Z(x)πD(y|x)exp(1/β(r(y|x))), (7)

trong đó
Z(x)=∑yπD(y|x)exp(1/β(r(y|x)))
là hàm phân chia hoặc hằng số chuẩn hóa. Theo (Korbak et al., 2022; Go et al., 2023), chúng tôi sử dụng phân kỳ KL ngược giữa πθ và π⋆ để khớp phân phối:

DKL(πθ,π⋆)=Ex∼DEy∼πθ(y|x)log(πθ(y|x)/π⋆(y|x))
=−1/βEx∼DEy∼πθ(y|x)(r(y|x))−βlogπθ(y|x)/πD(y|x)−βlogZ(x), (8)

Theo phân tích của các nghiên cứu trước (Peng et al., 2019; Zhu et al., 2023), hàm phân chia Z(x)≈1. Theo Phương trình 8, việc tối thiểu hóa DKL(πθ,π⋆) trùng khớp với mục tiêu:

arg maxπθEx∼DEy∼πθ(y|x)[r(y|x)−βlog(πθ(y|x)/πD(y|x))]. (9)

Chúng tôi lưu ý rằng πD không thể giải được, vì việc tạo ra D có thể đa dạng, ví dụ, bởi πSFT, LLM mạnh mẽ như GPT-4, hoặc con người. Do đó, phân phối của chính sách hành vi πD không thể truy cập. Vì πSFT đã được tinh chỉnh trên một phần của D, chúng ta có thể xấp xỉ πD bằng πSFT và sau đó có được mục tiêu như trong Phương trình 2.

Xét một câu trả lời chất lượng thấp y, ngay cả khi xác suất tạo ra của nó nhỏ đối với một mô hình chính sách thỏa mãn 7, chúng ta vẫn có thể lấy mẫu y như vậy trong quá trình huấn luyện RL. Trong trường hợp này, hình phạt KL trong Phương trình 2 trở nên yếu hơn hoặc thậm chí âm, điều này sẽ gây ra tối ưu hóa quá mức. Vấn đề này sẽ trở nên nghiêm trọng hơn khi RM sai lầm gán phần thưởng cao cho các mẫu OOD chất lượng thấp như vậy.

Được huấn luyện trên D, các mô hình phần thưởng nên được hiệu chuẩn tốt và rất không chắc chắn đối với các mẫu OOD (x,y), tương ứng với πD(y|x) nhỏ. Với một câu trả lời y được tạo bởi πθ(y|x), mẫu càng OOD thì hạng phạt càng lớn. Do đó, chúng ta có thể xấp xỉ hạng không thể giải trong 9 bằng ước tính không chắc chắn của các mô hình phần thưởng u(y|x), dẫn đến các mục tiêu sau:

arg maxπθEx∼DEy∼πθ(y|x)[r(y|x)−β1log(πθ(y|x)/πSFT(y|x))−β2u(y|x)], (10)

trong đó β1 và β2 là các hệ số để kiểm soát chính quy hóa KL và không chắc chắn tương ứng.

3.2. Huấn luyện tập hợp LoRA phần thưởng đa dạng
Để ước tính không chắc chắn phần thưởng u(y|x), chúng tôi nghiên cứu phương pháp tập hợp, được áp dụng rộng rãi để tăng cường không chắc chắn của các phương pháp học sâu. Vì các mô hình phần thưởng (RM) cũng được khởi tạo từ LLM, chúng tôi huấn luyện nhiều LoRA thay vì các mô hình phần thưởng cho tập hợp, hiệu quả hơn về tham số. Sau đó, lượt truyền tới có thể được công thức hóa như:

zout=1/N∑n=1N(W(0)x+ΔWnzin)=1/N∑n=1N(W(0)x+BnAnzin), (11)

trong đó ΔWn là các LoRA khác nhau của tập hợp. Mặc dù các thành viên tập hợp LoRA có khởi tạo ngẫu nhiên, chúng tôi quan sát thấy rằng các tập hợp LoRA không thể thể hiện khả năng định lượng không chắc chắn thỏa mãn. Chúng tôi giả thuyết điều này là do thiếu sự đa dạng giữa các tập hợp LoRA. Nhớ lại rằng LoRA chỉ học cập nhật tham số, đầu ra của các thành viên tập hợp khác nhau có thể đồng nhất hơn so với các tập hợp sâu truyền thống. Hiện tượng tương tự cũng được quan sát trong các phương pháp tinh chỉnh khác của tập hợp LLM (Gleave & Irving, 2022; Eisenstein et al., 2023).

Để tích cực đa dạng hóa các tập hợp LoRA phần thưởng, chúng tôi đề xuất một chính quy hóa đa dạng thông qua Tối đa hóa Chuẩn Hạt nhân khi huấn luyện các tập hợp LoRA. Như được hiển thị trong Hình 2, chúng tôi đầu tiên nối nhiều An dọc theo chiều LoRA r và thu được ma trận A∈RNr×d. Nếu các thành viên tập hợp LoRA hoàn toàn đồng nhất, hạng của A bằng hạng của thành viên LoRA An. Ngược lại, các thành viên đa dạng có nghĩa là độc lập tuyến tính dọc theo chiều đầu tiên của A. Do đó, chúng ta có thể đo lường sự đa dạng (hoặc tính đồng nhất) của tập hợp LoRA bằng hạng ma trận của ma trận A. Vì bài toán tối ưu hóa hạng được biết là NP-hard, chúng tôi tận dụng hàm thay thế lồi, chuẩn hạt nhân, như một xấp xỉ hiệu quả tính toán của hạng ma trận, được tính toán thông qua phân tích giá trị đơn (SVD).

Ngoài mất mát hạng trong Phương trình 1, hàm mất mát của việc huấn luyện tập hợp LoRA phần thưởng đa dạng là:

LRM=∑xlogσ(1/N∑n=1Nrn(yw|x)−1/N∑n=1Nrn(yl|x))+λ1/M∑m∥A∥∗/∥A∥F, (12)

trong đó λ là trọng số NNM để kiểm soát mất mát đa dạng, ∥A∥∗ là chuẩn hạt nhân của A, và ∥A∥F là chuẩn Frobenius của A, được sử dụng để kiểm soát giá trị của các trọng số không quá lớn.

Sau khi huấn luyện các mô hình phần thưởng với tập hợp LoRA đa dạng, chúng ta có thể ước tính không chắc chắn phần thưởng sử dụng độ lệch chuẩn:

u(y|x)=√(1/N∑n=1N(rn(y|x)−1/N∑n=1Nrn(y|x))2). (13)

3.3. Mục tiêu tối ưu hóa tổng thể
Trong Phương trình 10, ba số vô hướng bao gồm phần thưởng, hình phạt KL, và hình phạt không chắc chắn cần được tối ưu hóa với mục tiêu RL. Để ngăn chặn ba hạng này can thiệp lẫn nhau, chúng tôi làm cho chính quy hóa KL độc lập với mất mát diễn viên. Cụ thể, chúng tôi chỉ tối ưu hóa phần thưởng được phạt không chắc chắn sử dụng các thuật toán RL:

JRLθ=Ex∼DEy∼πθ(y|x)[r(y|x)−β2(u(y|x)−ū(y|x))], (14)

trong đó ū(y|x) đại diện cho không chắc chắn của các mô hình phần thưởng cho (x,y) do các quy mô khác nhau của các thành viên tập hợp. Trong thực tế, chúng tôi sử dụng không chắc chắn trung bình của tất cả các mẫu được thấy trước đó để xấp xỉ ū(y|x).

Đối với chính quy hóa KL, mục tiêu là:

JKLθ=−β1Ex∼DEy∼πθ(y|x)[(logπθ(y|x)/πSFT(y|x))2], (15)

trong đó chúng tôi sử dụng ước tính KL với phương sai thấp hơn, thiên vị thấp, và đảm bảo tích cực. Vì mục tiêu 15 có thể vi phân, chúng tôi trực tiếp tối ưu hóa nó thông qua giảm gradient.

Nhìn chung, mục tiêu của UP-RLHF là:

JUP-RLHFθ=JRLθ+JKLθ. (16)

Chính quy hóa KL có thể được coi là chính quy hóa từ bước 1 của quy trình RLHF, trong khi hình phạt không chắc chắn có thể được coi là chính quy hóa từ bước 2.

--- TRANG 5 ---
Học Tăng Cường từ Phản Hồi Con Người với Phạt Không Chắc Chắn với Tập Hợp LoRA Phần Thưởng Đa Dạng

4. Kết quả thực nghiệm
Trong phần này, chúng tôi tiến hành các thí nghiệm thực nghiệm để đánh giá sự căn chỉnh của UP-RLHF trên hai nhiệm vụ RLHF được sử dụng rộng rãi, cụ thể là tóm tắt và hỏi đáp. Chúng tôi nhắm mục tiêu nghiên cứu ba câu hỏi nghiên cứu chính (RQ):

•RQ1 (Bước 2: Mô hình hóa phần thưởng): Tập hợp LoRA phần thưởng đa dạng cải thiện định lượng không chắc chắn của các mô hình phần thưởng như thế nào?
•RQ2 (Bước 3: Tinh chỉnh RL): Phạt không chắc chắn giảm thiểu vấn đề tối ưu hóa quá mức như thế nào?
•RQ3 (Hiệu suất): UP-RLHF thực hiện như thế nào so với các phương pháp RLHF hiện có?

Để trả lời các câu hỏi trên, chúng tôi sẽ đầu tiên cung cấp giới thiệu ngắn gọn về các tập dữ liệu và thiết lập huấn luyện. Thảo luận tiếp theo bao gồm đánh giá cả các mô hình phần thưởng và mô hình chính sách.

4.1. Tập dữ liệu và thiết lập huấn luyện
Tập dữ liệu. Đối với nhiệm vụ tóm tắt, chúng tôi sử dụng tập dữ liệu "TL;DR" (Quá dài; Không đọc) được giới thiệu bởi Völske et al. (2017). Trong tập dữ liệu này, x đại diện cho một bài đăng diễn đàn có nguồn gốc từ Reddit, và y tương ứng với bản tóm tắt tương ứng. Đáng chú ý, chúng tôi sử dụng phần thưởng vàng để gắn lại nhãn cho tập dữ liệu theo sở thích, đảm bảo rằng phần thưởng vàng là đại diện hoàn hảo cho tập dữ liệu được gắn lại nhãn.

Trong nhiệm vụ hỏi đáp, theo nghiên cứu trước, chúng tôi sử dụng tập dữ liệu Anthropic Helpful (Bai et al., 2022b) với sở thích con người không có gắn lại nhãn bổ sung. x biểu thị một đoạn của cuộc trò chuyện liên quan đến tương tác giữa con người và trợ lý kỹ thuật số. Mô hình được huấn luyện cụ thể để tạo ra lượt tiếp theo hữu ích của trợ lý, được ký hiệu là y.

Thiết lập huấn luyện. Trong nhiệm vụ tóm tắt, mô hình chính sách được thiết lập sử dụng OPT-1.3B (Zhang et al., 2022), và mô hình phần thưởng được thiết lập sử dụng OPT-350m. Trong nhiệm vụ hỏi đáp, cả mô hình chính sách và mô hình phần thưởng đều được thiết lập sử dụng Llama2-7B (Touvron et al., 2023).

Theo quy luật tỷ lệ của mô hình phần thưởng (Gao et al., 2023), RM với tham số lớn hơn và nhiều dữ liệu huấn luyện hơn mạnh mẽ hơn đối với tối ưu hóa. Do đó, chúng tôi sử dụng GPT-J-6B được tinh chỉnh1 như mô hình phần thưởng vàng trong nhiệm vụ tóm tắt vì kích thước tham số RM lớn hơn và độ chính xác thỏa mãn (75% trên tập kiểm tra). Trong bối cảnh của nhiệm vụ hỏi đáp, 3B SteamSHP-XL2 được chọn như mô hình phần thưởng vàng vì kích thước dữ liệu huấn luyện RM lớn hơn mô hình phần thưởng, được tinh chỉnh trên cả tập dữ liệu HH và SHP (Ethayarajh et al., 2022).

Theo (Yao et al., 2023), đối với cả hai nhiệm vụ, chúng tôi thực hiện phân chia ngẫu nhiên cho các tập dữ liệu thành ba phần: 20% cho bước 1, 40% cho bước 2, và 40% còn lại cho bước 3.

4.2. Đánh giá mô hình phần thưởng
Để nghiên cứu khả năng định lượng không chắc chắn của mô hình phần thưởng, chúng tôi nghiên cứu ECE (Naeini et al., 2015), là một chỉ số được sử dụng để đánh giá việc hiệu chuẩn sai của mô hình. Nó liên quan đến việc phân loại các điểm xác suất được gán và so sánh chúng với độ chính xác trung bình trong các nhóm này. Theo mô hình Bradley-Terry, điểm xác suất của việc ưa thích câu trả lời yw hơn yl có thể được tính như:

P(yw>yl|x)=exp(r(yw|x))/(exp(r(yw|x))+exp(r(yl|x)))=1/(1+exp(r(yw|x)−r(yl|x))) (17)

Sau đó chúng ta có thể định nghĩa Lỗi Hiệu chuẩn Dự kiến (ECE) cho mô hình phần thưởng:

ECE=∑m=1M|Bm|/P∑m|Bm||ACC(Bm)−CONF(Bm)|, (18)

trong đó chúng tôi chia mẫu thành M=15 nhóm, Bm, theo sự khác biệt phần thưởng, và

ACC(Bm)=|Bm|−1∑i∈BmI[r(ywi|x)>r(yli|x)],
CONF(Bm)=|Bm|−1∑i∈BmP(ywi>yli|x), (19)

trong đó I là hàm chỉ thị. Chúng tôi quan sát rằng các mô hình phần thưởng khác nhau có quy mô phần thưởng khác nhau. Để tính ECE, chúng tôi tỷ lệ sự khác biệt phần thưởng để đảm bảo rằng sự khác biệt phần thưởng lớn nhất trong tập dữ liệu kiểm tra tương ứng với độ tin cậy 0.99, dẫn đến ACC được hiệu chuẩn.

Chúng tôi thiết lập các mô hình phần thưởng sử dụng OPT-330M trên TL;DR và sử dụng Llama2-7B trên tập dữ liệu Anthropic Helpful. Bảng 1 chi tiết hiệu suất của các mô hình phần thưởng với các phương pháp huấn luyện khác nhau và có thể quan sát thấy rằng Tập hợp LoRA có lợi cho cả độ chính xác và ECE trên tập dữ liệu kiểm tra. Sử dụng NNM, hiệu suất tổng thể theo hai chỉ số có thể được cải thiện thêm.

Chúng tôi sử dụng hai mô hình phần thưởng, được huấn luyện với tập hợp LoRA và tập hợp LoRA đa dạng để huấn luyện mô hình chính sách tương ứng sử dụng mục tiêu RLHF 2.

Bảng 1. Độ chính xác và ECE của các phương pháp huấn luyện khác nhau cho mô hình hóa phần thưởng trên hai tập dữ liệu. Các giá trị hiệu suất tốt nhất được làm nổi bật. Tất cả các phương pháp tập hợp có 5 thành viên.

| Mô hình cơ sở | Phương pháp huấn luyện | ACC ↑ | ECE ↓ |
|---|---|---|---|
| OPT-330M | Full FT | 0.694 | 0.485 |
| | Tập hợp LoRA | 0.697 | 0.480 |
| | Tập hợp LoRA đa dạng | 0.697 | 0.481 |
| Llama2-7B | Full FT | 0.685 | 0.515 |
| | Tập hợp LoRA | 0.710 | 0.496 |
| | Tập hợp LoRA đa dạng | 0.720 | 0.485 |

Theo (Gao et al., 2023), chúng tôi sử dụng phân kỳ KL giữa mô hình chính sách và mô hình SFT DKL(πθ(y|x)||πSFT(y|x)) để đo lường mức độ tối ưu hóa chính sách. Như được hiển thị trong Hình 3, không chắc chắn được cung cấp bởi tập hợp LoRA phần thưởng tăng nhanh trong phạm vi phân kỳ KL từ 0 đến 50, điều này khiến khó phân biệt giữa các mẫu có phần thưởng vàng cao và các mẫu được tạo bởi các mô hình tối ưu hóa quá mức (phân kỳ KL khoảng từ 50 đến 100). Ngược lại, tập hợp LoRA phần thưởng đa dạng được đề xuất của chúng tôi cung cấp không chắc chắn tăng dần theo quá trình tối ưu hóa, chỉ ra khả năng phát hiện OOD tốt hơn.

4.3. Tác động của hình phạt không chắc chắn
Ngay cả với các tập hợp LoRA phần thưởng đa dạng, chúng tôi quan sát thấy tối ưu hóa quá mức đáng kể trong quá trình tối ưu hóa đến phần thưởng trung bình của các tập hợp, như được hiển thị trong Hình 4(a). Khi kết hợp các hình phạt không chắc chắn vào phần thưởng, không chắc chắn của các mẫu được tạo được kiểm soát tốt trong một phạm vi hợp lý, và vấn đề tối ưu hóa quá mức được loại bỏ. Điều này chứng minh hiệu quả của chính quy hóa không chắc chắn trong việc giảm thiểu tối ưu hóa quá mức.

Thú vị, chúng tôi quan sát thấy rằng mặc dù sử dụng chính quy hóa không chắc chắn có thể cải thiện hiệu suất tổng thể theo RM vàng, điểm RM bị giảm. Điều này có thể là do phần thưởng được phạt không chắc chắn hạn chế việc khám phá đầu ra OOD bởi mô hình chính sách, dù những đầu ra này có chất lượng cao hay thấp. Trong trường hợp này, việc sử dụng chính quy hóa không chắc chắn bổ sung có thể hạn chế việc khám phá của các mô hình chính sách, tương ứng với tiến thoái lưỡng nan khám phá-khai thác trong RL.

4.4. Đánh giá mô hình chính sách
Trong phần này, chúng tôi so sánh UP-RLHF được đề xuất với các phương pháp RLHF hiện có trong cả nhiệm vụ tóm tắt và hỏi đáp. Chúng tôi so sánh điểm RM vàng thay vì điểm RM vì các RM khác nhau có quy mô khác nhau, do đó không có ý nghĩa khi so sánh điểm RM trực tiếp.

Như được hiển thị trong Hình 5, UP-RLHF vượt trội hơn RLHF về hiệu suất vàng với biên độ lớn trong cả hai nhiệm vụ. Đặc biệt trong nhiệm vụ tóm tắt, so với RLHF, UP-RLHF có thể đạt được hiệu suất cao hơn với chi phí phân kỳ KL thấp hơn. Lưu ý rằng phương pháp RLHF sử dụng tinh chỉnh đầy đủ cho mô hình hóa phần thưởng, trong khi tập hợp LoRA phần thưởng đa dạng của chúng tôi trong UP-RLHF chỉ tinh chỉnh 4.53% tham số cho OPT-350M và 1.25% tham số cho Llama2-7B.

5. Các nghiên cứu liên quan
5.1. Học tăng cường từ phản hồi con người
RLHF là một phương pháp quan trọng để tinh chỉnh các mô hình ngôn ngữ để căn chỉnh với sở thích con người. Các nhà nghiên cứu đã áp dụng RLHF cho các nhiệm vụ đa dạng (Ramamurthy et al., 2023) như tóm tắt văn bản (Stiennon et al., 2020) và tăng cường tính vô hại và hữu ích của các mô hình ngôn ngữ (Bai et al., 2022b). Đáng chú ý, InstructGPT giới thiệu quy trình RLHF ba bước sử dụng phương pháp có giám sát và thuật toán PPO (Schulman et al., 2017), chứng minh hiệu quả của nó trên ChatGPT. Mặc dù thành công, RLHF phải đối mặt với nhiều thách thức khác nhau (Casper et al., 2023). Một trong những thách thức cấp bách nhất là tối ưu hóa quá mức, được gây ra bởi RM không hoàn hảo (Gao et al., 2023). Tác giả trong (Gao et al., 2023) cung cấp quy luật tỷ lệ của RM, cho thấy tác động của việc tăng tham số RM và kích thước dữ liệu trong việc giảm thiểu vấn đề.

RLHF dựa vào mô hình hóa phần thưởng để đại diện cho sở thích con người. Một số nghiên cứu gần đây nhằm mục đích bỏ qua bước mô hình hóa phần thưởng (Yuan et al., 2023; Rafailov et al., 2023; Song et al., 2023). Cụ thể, DPO trực tiếp tối ưu hóa chính sách hướng tới mục tiêu 2 bằng cách giải quyết bài toán phân loại trên dữ liệu sở thích con người. Mặc dù việc bỏ qua bước mô hình hóa phần thưởng có lợi từ việc triển khai dễ dàng và ổn định huấn luyện, các nghiên cứu gần đây hơn tiết lộ một số ưu điểm của việc sử dụng các mô hình phần thưởng. (Azar et al., 2023) phân tích độ mạnh mẽ của các phương pháp dựa trên mô hình phần thưởng chống lại overfitting được gây ra bởi điểm yếu của chính quy hóa KL. Bên cạnh đó, so với DPO, RLHF dựa trên mô hình phần thưởng cho thấy ưu điểm lớn trên các mẫu ngoài sở thích (Li et al., 2023b;a).

Có nhiều nghiên cứu để giải quyết thách thức trong RLHF như chi phí tính toán (Li et al., 2023b), hiệu quả mẫu (Snell et al., 2023; Gulcehre et al., 2023), huấn luyện không ổn định (Wu et al., 2023), và tối ưu hóa quá mức (Moskovitz et al., 2023; Coste et al., 2023; Eisenstein et al., 2023). Chúng tôi cũng tập trung vào vấn đề tối ưu hóa quá mức. Trong khi hầu hết các nghiên cứu gần đây chỉ tập trung vào bước tinh chỉnh RL, chúng tôi đầu tiên giới thiệu định lượng không chắc chắn vào bước mô hình hóa phần thưởng và làm cho tinh chỉnh RL nhận thức không chắc chắn.

5.2. Học tăng cường nhận thức không chắc chắn
Không chắc chắn là một yếu tố quan trọng trong lĩnh vực RL. Nguyên tắc Lạc quan Trước Không chắc chắn (OFU) (Abbasi-Yadkori et al., 2011) trong các chiến lược RL trực tuyến được áp dụng rộng rãi để tạo điều kiện khám phá tích cực và hiệu quả môi trường (Lockwood & Si, 2022). Trong RL ngoại tuyến (Levine et al., 2020), không chắc chắn thường được sử dụng để bảo thủ kiểm soát các lỗi dự đoán được gây ra bởi các mô hình động lực học không hoàn hảo. Không chắc chắn thường được ước tính bởi các mạng giá trị trong RL không có mô hình (Pathak et al., 2019; Bai et al., 2022a) và bởi các mô hình động lực học trong RL dựa trên mô hình (Janner et al., 2019; Yu et al., 2020).

RLHF có thể được công thức hóa như RL ngược với các tập dữ liệu ngoại tuyến, nơi các mô hình phần thưởng được huấn luyện trên một tập dữ liệu sở thích ngoại tuyến hạn chế là không hoàn hảo. Lấy cảm hứng từ các phương pháp RL ngoại tuyến dựa trên mô hình gần đây (Yu et al., 2020; Kidambi et al., 2020; Lu et al., 2022), chúng tôi đề xuất phạt phần thưởng bằng không chắc chắn mô hình để tối ưu hóa chính sách bảo thủ, nhằm mục đích giảm thiểu vấn đề tối ưu hóa quá mức. Nghiên cứu đồng thời bởi (Coste et al., 2023; Eisenstein et al., 2023) cũng cho thấy tập hợp mô hình phần thưởng giúp giảm thiểu tối ưu hóa quá mức. Tuy nhiên, việc sử dụng tập hợp mô hình phần thưởng tăng tham số RM nhiều lần, và có thể thiếu sự đa dạng giữa các thành viên tập hợp (Gleave & Irving, 2022). Để đa dạng hóa các tập hợp phần thưởng, (Eisenstein et al., 2023) đề xuất sử dụng các seed khác nhau trong giai đoạn huấn luyện trước. Chúng tôi đề xuất huấn luyện các tập hợp LoRA đa dạng với NNM cho mô hình hóa phần thưởng, rẻ hơn nhiều và hiệu quả tham số. Bên cạnh đó, chúng tôi phân tích mối quan hệ giữa chính quy hóa KL và không chắc chắn và làm cho chúng ảnh hưởng độc lập.

5.3. Không chắc chắn cho LLM
Định lượng không chắc chắn cho các mạng thần kinh sâu đã được nghiên cứu kỹ lưỡng (Gawlikowski et al., 2023). Các phương pháp phổ biến bao gồm tập hợp sâu, MC dropout (Gal & Ghahramani, 2016), v.v. Trong bối cảnh của LLM, một số thách thức mới phát sinh. Sự đa dạng đóng vai trò quan trọng trong các phương pháp dựa trên tập hợp (Breiman, 2001). Tuy nhiên, tinh chỉnh LLM cho các tập hợp (Sun et al., 2022) không chỉ quá đắt đỏ để mở rộng quy mô mà còn thiếu sự đa dạng (Gleave & Irving, 2022). Do đó, chúng tôi áp dụng một công nghệ PEFT phổ biến, LoRA (Hu et al., 2022) để huấn luyện tập hợp các mô hình phần thưởng. Khác với nghiên cứu đồng thời (Wang et al., 2023) cũng đề xuất tập hợp LoRA cho tinh chỉnh LLM và các kỹ thuật chính quy hóa khác nhau cho mỗi LoRA, chúng tôi đề xuất một chính quy hóa đa dạng để khuyến khích sự đa dạng giữa các thành viên tập hợp. Bên cạnh đó, chúng tôi chủ yếu tập trung vào mô hình hóa phần thưởng trong bối cảnh của RLHF.

6. Kết luận và hạn chế
Trong bài báo này, chúng tôi đề xuất UP-RLHF, một khung RLHF nhận thức không chắc chắn góp phần vào sự không chắc chắn của các hệ thống AI dựa trên LLM. Tập hợp LoRA phần thưởng đa dạng được đề xuất của chúng tôi có thể cung cấp định lượng không chắc chắn thỏa mãn cho các mẫu trong RLHF. Tận dụng không chắc chắn phần thưởng, chúng tôi nhấn mạnh vai trò quan trọng của chính quy hóa không chắc chắn trong việc giải quyết hiệu quả thách thức tối ưu hóa quá mức trong việc căn chỉnh LLM.

Nghiên cứu của chúng tôi có hạn chế. Mặc dù tập hợp LoRA phần thưởng đa dạng tỏ ra hiệu quả tham số, việc tính toán chuẩn hạt nhân cho các ma trận LoRA được nối giới thiệu chi phí thời gian bổ sung. Hơn nữa, chính quy hóa không chắc chắn có thể thể hiện tính bảo thủ quá mức, đặc biệt trong các trường hợp liên quan đến các đầu ra chất lượng cao gần phân phối. Như một hướng nghiên cứu tương lai, việc khám phá các phương pháp để tạo ra sự cân bằng giữa chính quy hóa KL và không chắc chắn cho các mẫu cụ thể có thể tinh chỉnh thêm hiệu suất của khung.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch giữ nguyên cấu trúc và thông tin như bản gốc]
