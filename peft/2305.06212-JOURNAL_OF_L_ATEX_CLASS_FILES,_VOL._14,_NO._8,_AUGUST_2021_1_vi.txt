--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 3

BẢNG I
CÁC TOKEN THUẦN TÚY ĐƯỢC SỬ DỤNG TRONG CÁC TÁC VỤ SST, QQP, VÀ TP-UK,
TƯƠNG ỨNG. CÁC TOKEN THUẦN TÚY LÀ CÁC CHUỖI TOKEN ĐƯỢC TẠO NGẪU NHIÊN
KHÔNG CẦN PHẢI CÓ Ý NGHĨA.

SST erect langley bain jazz business refrigerator uci hose
baking beijing edward combatants rotary lush emery
ruiz oilers halves poke reasons prospects oak contend
arthur fia bethlehem neighbourhoods loss accents ideol-
ogy groves quinlan pixel azure issues mum lucivar bihar
northeast rooney

QQP weimar hutchinson lift triumphant breaker pathways
bucket anne absent willow huntington brentford land-
marks ike accumulation powerplant lionel cyber boule-
vard christmas translation entertainment apparel thru de-
bate dissolve partner honeymoon brownish ncaa junction
walker artist carpenter clothes medici forge venues mg
deploy

TP-UK plaques vertebrae restart yates educator wood berman
sneakers points resin liner gotham raider basins al-
bert sinatra somalia tree dusty dimensional bytes strips
rangers zheng jimi performance manga bohemian brest
improves beau zion gillespie visit nagasaki bianca brow
installations airs fairfield

Khôi phục token tư nhân hóa tương tự như mô hình hóa ngôn ngữ bị che.
Tuy nhiên, việc khôi phục một từ trong đầu vào tư nhân hóa
không khả thi vì nó có thể chứa thông tin nhạy cảm. Để giải quyết
vấn đề này, chúng tôi đặt trước một chuỗi token cố định,
được gọi là "token thuần túy", vào mỗi đầu vào trong dữ liệu huấn luyện.
Token thuần túy bao gồm các token được chọn tùy ý và có thể
được gửi an toàn cho nhà cung cấp dịch vụ mà không lo ngại về
rò rỉ quyền riêng tư. Hình 2 cung cấp tổng quan về RAPT
trong quá trình tùy chỉnh LLM. Bảng I trình bày các ví dụ
về token thuần túy được sử dụng trong các tác vụ downstream khác nhau.

PCT2T Tư nhân hóa
LLM
Khôi phục Token
LM Head
Prompt
Máy chủ Dự án
Người dùng

Hình 2. Tổng quan về khung RAPT trong quá trình tùy chỉnh LLM. Quá trình
bao gồm tư nhân hóa PCT2T, PEFT, khôi phục token tư nhân hóa
(Khôi phục Token), và đầu mô hình ngôn ngữ (LM) cụ thể cho tác vụ.

Chính thức, cho k = [k1, ..., km] biểu thị token thuần túy.
Cho một tập hợp các ví dụ huấn luyện D = {⟨xi, yi⟩|i = 1, ..., |D|},
trong đó |D| biểu thị kích thước dữ liệu huấn luyện, trước tiên
người dùng đặt trước k vào mỗi đầu vào xi và sau đó thu được
một phiên bản tư nhân hóa M([k; xi]) = [k̂1, ..., k̂m, ŵ1, ..., ŵn]
của [k; xi] thông qua tư nhân hóa văn bản-đến-văn bản ràng buộc POS.
Cho z = M([k; xi]), LLM tạo ra một chuỗi kích hoạt G
với chuỗi nhúng từ Z của đầu vào z và prompt T:
G = f([T; Z]), (6)
trong đó G ∈ R^((l+m+n)×h).

Sau khi thu được G, chúng tôi sử dụng m vector Gl:l+m =
[gl+1, ..., gl+m] trong G để khôi phục token thuần túy. Để
đạt được điều này, chúng tôi giới thiệu một đầu khôi phục bổ sung
bao gồm hai lớp tuyến tính. Phân phối xác suất
để dự đoán token thứ i ki trong token thuần túy được cho bởi
pi = softmax(WdownWupgl+i), (7)
trong đó Wdown ∈ R^(|Vr|×c) và Wup ∈ R^(c×h) là các tham số
của đầu khôi phục, c là kích thước ẩn của đầu khôi phục,
và |Vr| biểu thị kích thước từ vựng của đầu khôi phục. Hàm mất mát
cho tác vụ khôi phục token tư nhân hóa là
Lrec = -∑(i=1 to m) log pi[ji], (8)
trong đó ji biểu thị chỉ số của ki trong từ vựng đầu khôi phục,
và pi[ji] biểu thị vô hướng thứ ji trong vector pi. Lưu ý rằng
từ vựng của đầu khôi phục không cần phải giống với từ vựng của LLM.

Các kích hoạt n còn lại Gl+m:l+m+n được sử dụng để dự đoán
nhãn phụ thuộc tác vụ. Giả sử tác vụ downstream là tác vụ phân loại,
trong đó yj ∈ {0,1} biểu thị nhãn cho lớp thứ j và |C| là số lượng lớp.
Hàm mục tiêu của tác vụ downstream được mô tả chính thức như sau:
q = softmax(Whead (1/n) ∑(i=1 to n) gl+m+i), (9)
Ltask = -∑(i=1 to |C|) yi log q[i], (10)
trong đó Whead ∈ R^(|C|×h) là hàm của đầu mô hình ngôn ngữ cụ thể cho tác vụ,
và q ∈ R^|C| là phân phối xác suất dự đoán. Kết quả là, hàm mất mát
cho việc huấn luyện RAPT được cho bởi
L = Ltask + Lrec. (11)

Trong giai đoạn suy luận, người dùng cũng áp dụng tư nhân hóa văn bản-đến-văn bản
để bảo vệ thông tin riêng tư và thu được dự đoán từ dịch vụ LLM.
Chúng tôi lưu ý rằng đầu khôi phục không được sử dụng trong giai đoạn suy luận.
Do đó, {Wdown, Wup} có thể được loại bỏ sau khi huấn luyện.

IV. THÍ NGHIỆM QUYỀN RIÊNG TƯ
Trước tiên chúng tôi nghiên cứu hiệu quả của RAPT chống lại kẻ thù
thông qua các cuộc tấn công mô phỏng, theo phương pháp của Song và
Raghunathan [39]. Chúng tôi sử dụng BERT BASE làm LLM xương sống
và thực hiện thí nghiệm trên phần UK của tập dữ liệu cảm tính
Trustpilot (TP-UK) [40]. Đối với ràng buộc POS, chúng tôi cẩn thận
chọn Danh từ, Động từ, Đại từ, và Giới từ làm mục tiêu cho
bảo vệ quyền riêng tư. Sự tập trung của chúng tôi vào Danh từ và Đại từ
là do mối liên kết trực tiếp của chúng với thông tin nhận dạng cá nhân.
Động từ được bao gồm vì chúng mô tả hành động và hành vi cá nhân,
có thể tiết lộ các mẫu nhạy cảm cần được bảo vệ. Giới từ cũng
được xem xét vì chúng cung cấp ngữ cảnh về hướng và vị trí,
có thể tiết lộ chi tiết cá nhân.

A. Cuộc tấn công
Để đánh giá hiệu quả của RAPT trong việc bảo vệ quyền riêng tư,
chúng tôi mô phỏng các loại tấn công sau:

1) Tấn công đảo ngược nhúng: Đây là một cuộc tấn công ở cấp độ token
trong đó các biểu diễn của văn bản đầu vào của người dùng có thể bị
đảo ngược để tiết lộ đầu vào ban đầu, có thể bởi kẻ tấn công nghe lén.
Chúng tôi xem xét các loại tấn công sau trong bối cảnh này [30]:
• Tấn công hộp trắng. Tấn công hộp trắng giả định rằng kẻ tấn công
có quyền truy cập đầy đủ vào trọng số của mô hình, kiến trúc,
và quy trình huấn luyện, cho phép họ thu được tín hiệu gradient.
Tuy nhiên, chúng tôi không giả định rằng kẻ tấn công có quyền truy cập
vào dữ liệu huấn luyện hoàn chỉnh. Theo cài đặt đảo ngược hộp trắng
của Qu et al. [30], đối với bất kỳ nhúng token tư nhân hóa vt nào
trong không gian nhúng Rd, kẻ tấn công nhằm khôi phục
nhúng token ban đầu bằng cách giải:
wt = arg min_wk ||E(wk) - vt||2. (12)
Tìm kiếm láng giềng gần nhất được sử dụng để xác định
nhúng token ban đầu gần nhất cho mỗi nhúng từ bị nhiễu.

• Tấn công hộp đen dựa trên MLP. Tấn công hộp đen giả định
rằng kẻ tấn công chỉ có quyền truy cập vào dịch vụ giống như API
nơi họ cung cấp đầu vào và nhận đầu ra, không có thông tin
thêm về mô hình. Kẻ tấn công hộp đen dựa trên MLP sử dụng
mạng nơ-ron để dự đoán dữ liệu ban đầu từ biểu diễn đã biến đổi.

• Tấn công hộp đen dựa trên autoencoder. Kẻ tấn công sử dụng
autoencoder với bộ mã hóa φ và bộ giải mã ψ. Bộ mã hóa
nén đầu vào x thành biểu diễn tiềm ẩn z = φ(x), và sau đó
bộ giải mã cố gắng khôi phục đầu vào là x̂ = ψ(z). Lỗi
bình phương trung bình (MSE) định lượng lỗi khôi phục:
MSE(x, x̂) = ||x - x̂||2. (13)
Autoencoder được huấn luyện để giải mã các biểu diễn tư nhân hóa
trở lại dữ liệu ban đầu, với MSE thấp cho thấy lỗ hổng tiềm tàng.

2) Tấn công suy luận thuộc tính: Kẻ tấn công có thể suy luận
thuộc tính riêng tư t ∈ {0,1}|C| của người dùng từ các biểu diễn ẩn,
trong đó |C| là số lượng lớp riêng tư [41]. Cụ thể, mục tiêu
của kẻ tấn công được định nghĩa như sau:
p = fθ((1/n) ∑(i=1 to n) zi), (14)
LAIA = -∑(i=1 to |C|) ti log p[i], (15)
trong đó zi là nhúng từ bị nhiễu của LLM, và fθ là MLP 2 lớp
với 768 đơn vị ẩn và hàm kích hoạt ReLU, theo cài đặt của
Plant et al. [41]. Việc lựa chọn tập dữ liệu TP-UK bao gồm
các đặc điểm nhân khẩu học của người dùng như giới tính và tuổi,
làm cho nó trở thành lựa chọn phù hợp cho bối cảnh tấn công
suy luận thuộc tính. Chúng tôi áp dụng khung của Plant et al. [41]
để phân loại thuộc tính tuổi thành sáu nhóm độ tuổi có kích thước
bằng nhau với nhãn được gán duy nhất, trong khi giới tính được
biểu diễn như biến phân loại nhị phân được biểu thị bằng 0 và 1.

3) Tấn công NER: Kẻ tấn công sử dụng công cụ nhận dạng thực thể
có tên (NER) để trích xuất chi tiết nhạy cảm như tên, địa chỉ,
và tổ chức từ văn bản [42]. Chúng tôi giả định kẻ tấn công sử dụng
mô hình BERT BASE để thực hiện nhận dạng thực thể có tên (NER)
trên các chuỗi token đầu vào để trích xuất thông tin nhạy cảm.
Cho một chuỗi token đầu vào x = [x1, x2, ..., xn], mô hình NER
nhằm gán một vector xác suất pi cho mỗi token xi, chỉ ra
khả năng token đó được phân loại là một trong các loại thực thể
được xác định trước. Mục tiêu của kẻ tấn công là tối đa hóa
việc xác định chính xác các thực thể nhạy cảm, được chính thức hóa là:
pi = BERT(xi), (16)
LNER = -∑(i=1 to n) ∑(j=1 to m) tij log pi[j], (17)
trong đó tij là chỉ số nhị phân về việc liệu token xi có thuộc
loại thực thể j hay không, và pi[j] là xác suất được gán bởi
mô hình NER cho loại này. Mô hình NER dựa trên BERT được
huấn luyện để tối thiểu hóa mất mát LNER.

B. Chỉ số
Đối với tất cả các loại tấn công, chúng tôi sử dụng 1 - X làm chỉ số
đánh giá, trong đó X đại diện cho tỷ lệ thành công của cuộc tấn công
được đo bằng độ chính xác hoặc điểm F1. Điều này có nghĩa là
tỷ lệ thành công thấp hơn (X) dẫn đến giá trị chỉ số đánh giá cao hơn,
chỉ ra bảo vệ quyền riêng tư tốt hơn. Tuy nhiên, đối với cuộc tấn công
dựa trên autoencoder, thay vào đó chúng tôi sử dụng lỗi bình phương
trung bình (MSE) làm chỉ số. Chúng tôi định nghĩa chỉ số tổng thể
này là quyền riêng tư thực nghiệm, trong đó giá trị cao hơn biểu thị
bảo vệ quyền riêng tư mạnh hơn.

C. Kết quả
Hình 3 minh họa quyền riêng tư thực nghiệm cho T2T, PCT2T,
và RAPT (PCT2T + Khôi phục) dưới các cuộc tấn công mô phỏng khác nhau.
Từ hình, chúng tôi có ba quan sát sau. Thứ nhất, cả T2T và PCT2T
đều hiệu quả trong việc bảo vệ quyền riêng tư. Khi tham số quyền riêng tư
η giảm, quyền riêng tư thực nghiệm cải thiện đáng kể. Trong trường hợp
không có biện pháp bảo vệ quyền riêng tư, kẻ tấn công dễ dàng khôi phục
đầu vào ban đầu. Thứ hai, T2T và PCT2T có thể so sánh về mặt
bảo vệ quyền riêng tư bất kể sự lựa chọn tham số quyền riêng tư η.
Mặc dù việc thêm ràng buộc POS bổ sung có vẻ làm yếu đi mức độ
bảo vệ quyền riêng tư so với T2T, thí nghiệm cho thấy các hiệu ứng
không đáng kể khi lựa chọn cẩn thận các danh mục POS. Thứ ba,
việc thêm mục tiêu khôi phục token thuần túy cùng với PCT2T
không làm tổn hại đến bảo vệ quyền riêng tư. Điều này phù hợp với
trực giác của chúng tôi vì các câu ban đầu không được khôi phục
trong quá trình huấn luyện. Kết quả là, chúng tôi kết luận rằng
RAPT được đề xuất hiệu quả trong việc bảo vệ quyền riêng tư.

V. THÍ NGHIỆM TIỆN ÍCH
Chúng tôi nghiên cứu hiệu quả của RAPT trong việc cải thiện tiện ích.
Chúng tôi sử dụng lớp nhúng LLM xương sống làm mô hình nhúng
để thực hiện tư nhân hóa trừ khi có quy định khác. Lưu ý rằng
các mô hình nhúng khác cũng có thể áp dụng.

A. Thiết lập
1) Tập dữ liệu, Chỉ số, và LLM: Chúng tôi đánh giá phương pháp
của chúng tôi trên cả tác vụ hiểu ngôn ngữ tự nhiên (NLU)
và tác vụ tạo sinh ngôn ngữ tự nhiên (NLG). Đối với NLU, chúng tôi
sử dụng BERT BASE [1] và T5 BASE [43] làm mô hình ngôn ngữ xương sống.
Đánh giá được thực hiện bằng cách sử dụng Stanford Sentiment Treebank
(SST) [44], Quora Question Pairs (QQP) [45], và tập dữ liệu TP-UK [40].
Hiệu suất trên các tác vụ NLU này được đánh giá bằng điểm độ chính xác.

Đối với tác vụ tạo sinh ngôn ngữ tự nhiên (NLG), chúng tôi sử dụng
T5 BASE [43] làm mô hình xương sống và sử dụng tập dữ liệu WEBNLG [46],
bao gồm 14 lĩnh vực khác biệt. Để đánh giá hiệu quả của RAPT,
chúng tôi sử dụng điểm BLEU thu được từ script đánh giá chính thức.
Cụ thể, trong tập dữ liệu WEBNLG, chúng tôi nhất quán áp dụng
tư nhân hóa cho các thực thể có mặt trong cả bảng đầu vào và văn bản
được tạo. Sau khi tạo sinh, các cặp từ gốc và tư nhân hóa
được khôi phục một cách liền mạch ở phía người dùng.

2) Baseline: Chúng tôi xây dựng RAPT dựa trên ba phương pháp PEFT
đại diện. Chúng tôi cũng so sánh với phương pháp FINE-TUNING
được mô tả trong Qu et al. [30].
• PROMPT TUNING [5]. PROMPT TUNING giới thiệu một prompt
liên tục có thể học để điều hướng LLM đến các tác vụ downstream.
• PREFIX-TUNING [4]. PREFIX-TUNING giới thiệu thêm tham số
bằng cách sử dụng prompt sâu, đặt trước các vector liên tục
vào kích hoạt trong tất cả các lớp LLM.
• ADAPTER [3]. ADAPTER tiêm các ma trận phân rã hạng có thể
huấn luyện vào mỗi lớp của LLM.

3) Chi tiết thực hiện: Tất cả thí nghiệm được thực hiện trên
máy có GPU NVIDIA V100. Đối với RAPT, tất cả thí nghiệm
sử dụng độ dài cố định 40 token thuần túy được lấy mẫu đồng nhất
từ các danh mục POS do người dùng chỉ định trừ khi có quy định khác.

Đối với tác vụ NLU, chúng tôi sử dụng bộ tối ưu Adam [47]
với tỷ lệ học cố định 6e-5 cho tất cả các phương pháp. Kích thước
batch là 128, và chúng tôi huấn luyện mỗi phương pháp trong 4 epoch.
Để phù hợp với dữ liệu đầu vào, chúng tôi đặt độ dài chuỗi
tối đa là 128 token. Đối với PROMPT TUNING, chúng tôi sử dụng
độ dài prompt là 150, trong khi đối với PREFIX-TUNING, độ dài prompt
được đặt là 10. Kích thước ẩn c được đặt là 96, và kích thước
từ vựng |Vr| của đầu khôi phục là 19,369. Trong quá trình suy luận,
chúng tôi báo cáo điểm trung bình trên 5 lần chạy độc lập.

Đối với tác vụ NLG, chúng tôi theo cài đặt của Li và Liang
cho PREFIX-TUNING. Chúng tôi sử dụng bộ tối ưu Adam với tỷ lệ học
5e-5, kích thước batch 5, và độ dài prompt 5. Trong trường hợp
PROMPT TUNING, chúng tôi sử dụng bộ tối ưu Adafactor [48] với
tỷ lệ học 1e-3 và độ dài prompt 50. Trong giai đoạn tạo sinh,
chúng tôi sử dụng thuật toán tìm kiếm chùm với kích thước chùm 5,
độ dài tạo sinh tối đa 100, giá trị top-k là 4, và giá trị
top-p là 0.9.

B. Kết quả trên tác vụ NLU
Bảng III hiển thị kết quả của PROMPT TUNING, PREFIX-TUNING,
ADAPTER, và FINE-TUNING với và không có bảo vệ quyền riêng tư
trên ba tác vụ NLU. Lưu ý rằng phương pháp RAPT được đề xuất
kết hợp cả tư nhân hóa PCT2T và khôi phục token tư nhân hóa.
Bảng II đưa ra xác suất thay thế từ cho các tham số quyền riêng tư η
khác nhau, phục vụ như một thước đo trực quan để so sánh mức độ
bảo vệ quyền riêng tư giữa các η khác nhau. Các cột với η = +∞
đại diện cho kết quả không có bảo vệ quyền riêng tư. Từ bảng III,
chúng tôi có những quan sát sau:

• Với η nhỏ hơn, tư nhân hóa văn bản-đến-văn bản (cả T2T và PCT2T)
cung cấp đảm bảo quyền riêng tư mạnh hơn nhưng không tránh khỏi
làm hại hiệu suất của các tác vụ downstream.
• Khi áp dụng tư nhân hóa T2T, chúng ta có thể thấy rằng cả
PROMPT TUNING và PREFIX-TUNING đều hoạt động kém trên tất cả
các tác vụ và LLM thậm chí với bảo vệ quyền riêng tư yếu nhất
(η lớn nhất), cho thấy rằng các phương pháp tinh chỉnh prompt
nhạy cảm với nhiễu ngẫu nhiên được áp đặt bởi tư nhân hóa
văn bản-đến-văn bản.
• Sử dụng tư nhân hóa PCT2T thay vì tư nhân hóa T2T liên tục
cải thiện hiệu suất trên các lựa chọn η khác nhau, cho thấy
hiệu quả của PCT2T trong việc bảo tồn cú pháp và ngữ nghĩa
của câu ban đầu.
• Bằng cách tiếp tục giới thiệu tác vụ khôi phục token tư nhân hóa,
hiệu suất của tất cả bốn phương pháp được cải thiện đáng kể
trên các tác vụ và LLM. Việc thêm khôi phục token không thể
mang lại cải thiện thêm khi η = +∞. Điều này là do việc
khôi phục token thuần túy trở nên tầm thường mà không cần
thêm bảo vệ quyền riêng tư.

Những kết quả này cho thấy rằng cả PCT2T và khôi phục token
tư nhân hóa đều rất hiệu quả trong việc cải thiện hiệu suất
của các phương pháp tinh chỉnh hiệu quả tham số khi được huấn luyện
trên dữ liệu tư nhân hóa. Kết quả cũng phù hợp với trực giác
của chúng tôi rằng việc sử dụng khôi phục có thể giúp LLM học
được biểu diễn tốt hơn. Để xác thực hiệu quả của phương pháp
trên các LLM gần đây, chúng tôi tiến hành thêm thí nghiệm
trên tác vụ SST-2 sử dụng Llama-8B và Mistral-7B. Chúng tôi
sử dụng nhúng từ mô hình BERT BASE để áp dụng T2T và PCT2T
và tinh chỉnh các LLM bằng phương pháp QLoRA [49]. Bảng IV
hiển thị kết quả. Từ kết quả, chúng tôi đưa ra những quan sát
tương tự như trong các thí nghiệm trước, cho thấy rằng các
phương pháp của chúng tôi có thể áp dụng cho các LLM khác nhau.
Kết quả là, chúng tôi kết luận rằng RAPT hiệu quả trong việc
đạt được tiện ích tốt hơn cho việc bảo vệ quyền riêng tư LLM.

C. Kết quả trên tác vụ NLG
Bảng V trình bày kết quả của PROMPT TUNING và PREFIX TUNING
trên tác vụ WEBNLG sử dụng mô hình T5 BASE. Chúng tôi quan sát
rằng hiệu suất giảm đáng kể khi tư nhân hóa T2T được áp dụng.
Đối với PCT2T, nó đạt được hiệu suất tốt hơn nhiều so với T2T
(+15.5 điểm BLEU trung bình), cho thấy hiệu quả của PCT2T
trong việc bảo tồn cú pháp và ngữ nghĩa của đầu vào. Kết hợp
với khôi phục token tư nhân hóa, chúng tôi thu được cải thiện
trung bình +5.2 điểm BLEU. Những kết quả này tiếp tục xác nhận
hiệu quả của các phương pháp PCT2T và khôi phục token tư nhân hóa
được đề xuất trong việc bảo vệ quyền riêng tư cho các tác vụ NLG.

D. So sánh với các phương pháp khác
Chúng tôi so sánh RAPT với các phương pháp khác về cả quyền riêng tư
và tiện ích. Chúng tôi xem xét các phương pháp sau:
• CAPE [41]. Phương pháp này sử dụng nhiễu nhúng dựa trên Laplace
kết hợp với mục tiêu học đối kháng để bảo vệ quyền riêng tư.
Phương pháp này tư nhân hóa các biểu diễn đầu ra của LLM.
• DPNR [16]. Phương pháp này giới thiệu sự kết hợp của loại bỏ từ
và nhiễu nhúng dựa trên Laplace. Nó cũng tư nhân hóa các biểu diễn
đầu ra của LLM.
• Fine-Tuning [30]. Phương pháp này tư nhân hóa lớp nhúng của LLM.
Nó áp dụng dX-privacy để nhiễu nhúng đầu vào của người dùng.

Để đảm bảo so sánh công bằng giữa các phương pháp này, chúng tôi
sử dụng cuộc tấn công suy luận thuộc tính thường được sử dụng
để nghiên cứu sự đánh đổi quyền riêng tư-tiện ích của các phương pháp
khác nhau. Đối với CAPE, chúng tôi sử dụng mô hình BERT BASE
với tham số quyền riêng tư được đặt là ε = 0.01 và λ = 1.5,
và tỷ lệ học huấn luyện đối kháng được đặt là 1e−3. Đối với DPNR,
chúng tôi cũng sử dụng mô hình BERT BASE. Tỷ lệ loại bỏ từ (μ)
được đặt là 0.1 và tham số quyền riêng tư ε được đặt là 5.
Đối với việc thực hiện RAPT của chúng tôi, chúng tôi đặt tham số
quyền riêng tư η là 100.

Hình 4 minh họa biên Pareto cho các phương pháp khác nhau.
Chúng tôi có thể quan sát rằng RAPT liên tục đạt được tiện ích
tốt hơn so với CAPE, DPNR, và FINE-TUNING dưới cùng mức độ
bảo vệ quyền riêng tư. Khi mức độ quyền riêng tư thực nghiệm
tăng lên, khoảng cách hiệu suất giữa RAPT và các phương pháp
khác trở nên lớn hơn. Kết quả tiếp tục xác nhận rằng phương pháp
RAPT của chúng tôi hiệu quả trong việc bảo vệ quyền riêng tư
của LLM.

E. Đánh giá nội tại
Chúng tôi nghiên cứu các đặc điểm của RAPT bằng cách so sánh
các biến thể khác nhau của RAPT trên tác vụ SST-2 sử dụng
mô hình BERT BASE. Chúng tôi giả định rằng RAPT sử dụng
PREFIX-TUNING để điều hướng BERT BASE hướng tới tác vụ SST-2.

1) Mô hình nhúng: Chúng tôi điều tra hiệu ứng của việc sử dụng
các mô hình nhúng khác nhau trong tư nhân hóa PCT2T. Cụ thể,
phía người dùng sử dụng mô hình nhúng từ GPT-2, RoBERTa,
BioBERT [50], hoặc T5 để ánh xạ văn bản đầu vào thành
văn bản tư nhân hóa. Chúng tôi điều chỉnh tham số quyền riêng tư η
cho các mô hình nhúng khác nhau để khớp với mức xác suất
thay thế token trong văn bản. Bảng VI hiển thị kết quả
thí nghiệm của chúng tôi. Chúng tôi thấy rằng việc sử dụng
các mô hình nhúng khác nhau không ảnh hưởng đáng kể đến
hiệu suất trên tác vụ downstream, thậm chí với các mô hình
nhúng được huấn luyện trên các lĩnh vực khác nhau
(ví dụ, BERT so với BioBERT).

2) Khôi phục token tư nhân hóa:
a) Nội dung của token thuần túy: Trước tiên chúng tôi cho thấy
rằng nội dung của token thuần túy có thể được chọn tùy ý.
Chúng tôi tạo ngẫu nhiên 5 token thuần túy với độ dài 40
và so sánh hiệu suất trên tác vụ SST-2. Bảng VII hiển thị
kết quả. Chúng ta có thể thấy rằng hiệu suất với các token
thuần túy khác nhau có cùng độ dài là tương tự. Kết quả
cho thấy chúng ta có thể chọn tùy ý các token thuần túy
được sử dụng trong tác vụ khôi phục token tư nhân hóa.

b) Số lượng token thuần túy: Chúng tôi nghiên cứu hiệu ứng
của việc sử dụng các token thuần túy đa dạng trong tác vụ
khôi phục token tư nhân hóa. Trước tiên chúng tôi xây dựng
một tập hợp token thuần túy có cùng độ dài. Sau đó, trong
quá trình huấn luyện, đối với mỗi đầu vào huấn luyện, chúng tôi
chọn ngẫu nhiên token thuần túy và đặt trước chúng vào đầu vào.
Từ Hình 5 (a), chúng tôi thấy rằng việc sử dụng nhiều token
thuần túy hơn trong quá trình huấn luyện cải thiện hiệu suất
một chút trên SST-2. Tuy nhiên, việc sử dụng một token thuần túy
trong quá trình khôi phục token tư nhân hóa đã đủ để cải thiện
hiệu suất của LLM trên các tác vụ downstream.

c) Độ dài của token thuần túy: Chúng tôi nghiên cứu hiệu ứng
của việc sử dụng token thuần túy với độ dài khác nhau. Trực giác,
LLM cần học biểu diễn tốt hơn để khôi phục token thuần túy
dài hơn. Do đó, việc sử dụng token thuần túy dài hơn có thể
có lợi cho hiệu suất tương ứng trên các tác vụ downstream.
Như được hiển thị trong Hình 5 (b), chúng tôi thấy rằng
kết quả phù hợp với trực giác của chúng tôi: việc sử dụng
token thuần túy dài hơn hoạt động tốt hơn đáng kể so với
việc sử dụng token thuần túy ngắn hơn.

d) Đầu khôi phục: Chúng tôi nghiên cứu kích thước ẩn và
kích thước từ vựng của đầu khôi phục. Hình 5 (c) hiển thị
kết quả. Việc sử dụng kích thước ẩn lớn hơn thường hoạt động
tốt hơn nhưng giới thiệu thêm tham số trong quá trình huấn luyện.
Đối với kích thước từ vựng, chúng tôi thấy rằng việc sử dụng
từ vựng lớn vừa phải là cần thiết. Từ vựng nhỏ làm cho
việc dự đoán token thuần túy trở nên dễ dàng hơn nhiều,
do đó làm giảm lợi ích của tác vụ khôi phục token tư nhân hóa.

e) Độ dài prompt: Bảng VIII hiển thị hiệu suất của RAPT
sử dụng độ dài prompt khác nhau. Chúng ta có thể thấy rằng
RAPT thường hoạt động tốt hơn khi độ dài prompt tăng lên.
Kết quả cũng phù hợp với các nghiên cứu trước [4, 5].

VI. PHÂN TÍCH

A. Phân phối nhúng và bảo vệ quyền riêng tư
Trước tiên chúng tôi điều tra phân phối nhúng từ trong một
danh mục POS cụ thể và quyền riêng tư thực nghiệm tương ứng
sử dụng PCT2T. Đối với mỗi cặp từ wi và wj trong danh mục POS,
chúng tôi tính khoảng cách Euclidean theo cặp là
dij = |wi - wj|. Sau đó, khoảng cách trung bình được tính là
μ = (2/(n(n-1))) ∑∑ dij.

Phương sai của khoảng cách được tính là
σ = (2/(n(n-1))) ∑∑ (dij - μ)².

Chúng tôi vẽ biểu đồ trung bình và phương sai của khoảng cách
và quyền riêng tư thực nghiệm tương ứng cho một danh mục POS
cho trước trong Hình 7. Từ hình, chúng ta có thể thấy rằng
các danh mục Danh từ và Động từ đạt được bảo vệ quyền riêng tư
tốt hơn so với các danh mục khác. Chúng tôi cũng thấy rằng
khoảng cách trung bình và phương sai cho các danh mục Danh từ,
Động từ, và Tính từ là tương tự. Phát hiện này tiết lộ rằng
cần thiết phải phân biệt giữa các danh mục POS khác nhau
trong tư nhân hóa văn bản-đến-văn bản.

B. Hiệu ứng của danh mục POS
Chúng tôi điều tra hiệu ứng của việc lựa chọn danh mục POS
đối với cả tiện ích và quyền riêng tư cho PCT2T. Chúng tôi
tập trung vào các cuộc tấn công đảo ngược nhúng, tấn công NER,
và tấn công suy luận thuộc tính. Tham số quyền riêng tư η
được đặt là 100. Tất cả thí nghiệm tiện ích được thực hiện
trên tác vụ SST-2. Đối với danh mục POS, chúng tôi điều tra
việc sử dụng Danh từ, Động từ, Đại từ, Giới từ, Tính từ,
Ký hiệu, Hạn định từ, và Liên từ. Hình 6 trình bày kết quả,
từ đó chúng tôi rút ra một số phát hiện chính. Thứ nhất,
các danh mục POS như Liên từ và Hạn định từ cung cấp bảo vệ
quyền riêng tư tương đối yếu hơn, cho thấy rằng không phải
tất cả danh mục POS đều hữu ích bằng nhau cho việc bảo vệ
quyền riêng tư. Tuy nhiên, những danh mục này quan trọng
cho việc duy trì cấu trúc cú pháp của câu đầu vào. Phát hiện
này hỗ trợ động lực của chúng tôi cho phương pháp tư nhân hóa
PCT2T. Thứ hai, các danh mục POS như Danh từ và Động từ
hiệu quả cho việc bảo vệ quyền riêng tư. Những danh mục này
liên quan chặt chẽ đến thông tin nhận dạng cá nhân. Do đó,
chúng tôi bao gồm Danh từ và Động từ trong tập hợp danh mục
POS của chúng tôi cho tất cả các thí nghiệm.

C. Hình học của biểu diễn
Chúng tôi phân tích hình học của biểu diễn LLM để nghiên cứu
tại sao RAPT cải thiện hiệu suất của các tác vụ downstream
khi được huấn luyện trên dữ liệu tư nhân hóa cục bộ. Chúng tôi
sử dụng PREFIX TUNING làm phương pháp tinh chỉnh prompt
cho RAPT và đặt η = 100. Chúng tôi sử dụng BERT BASE
làm LLM xương sống.

Trước tiên chúng tôi tính các biểu diễn Oi ∈ R^(n×h) trong
mỗi lớp BERT i cho PREFIX-TUNING không có tư nhân hóa,
PREFIX-TUNING với tư nhân hóa văn bản-đến-văn bản như
Pi ∈ R^(n×h), và RAPT sử dụng dữ liệu từ SST-2 như
Ri ∈ R^(n×h). Đối với mỗi lớp, chúng tôi chiếu {Oi, Pi, Ri}
vào R^(n×2) sử dụng phân tích thành phần chính (PCA).
Sau đó chúng tôi tính khoảng cách trung bình d của biểu diễn
giữa các phương pháp khác nhau. Hình 8 hiển thị kết quả.
Chúng tôi thấy rằng các biểu diễn sâu đã học tương tự với
những biểu diễn được học mà không sử dụng bảo vệ quyền riêng tư.
Rõ ràng là các biểu diễn cho RAPT trở nên gần hơn với những
biểu diễn không có bảo vệ quyền riêng tư khi số lớp tăng lên.
Do đó, chúng tôi xác nhận rằng RAPT có thể học được biểu diễn
tốt hơn.

D. Biên Pareto
Chúng tôi phân tích biên Pareto của RAPT được đề xuất và
tinh chỉnh prompt với tư nhân hóa T2T trên tác vụ SST-2.
Chúng tôi mô phỏng các cuộc tấn công hộp trắng, tấn công
hộp đen dựa trên MLP, tấn công hộp đen dựa trên autoencoder,
và tấn công NER. LLM xương sống là BERT BASE, và tham số
quyền riêng tư η dao động từ 25 đến 175. Kết quả được
minh họa trong Hình 9. Từ hình, chúng tôi thấy rằng đối với
tất cả các cuộc tấn công, RAPT liên tục vượt trội hơn
tư nhân hóa T2T dưới cùng mức độ bảo vệ quyền riêng tư.
Kết quả là, rõ ràng là RAPT có thể đạt được sự đánh đổi
quyền riêng tư-tiện ích tốt hơn so với việc sử dụng tư nhân hóa T2T.

VII. KẾT LUẬN
Trong công trình này, chúng tôi giới thiệu RAPT, một khung
để bảo vệ quyền riêng tư trong các dịch vụ LLM. RAPT tận dụng
quyền riêng tư khác biệt cục bộ để cung cấp bảo vệ mạnh mẽ
trong khi duy trì hiệu suất cạnh tranh thông qua khôi phục
token tư nhân hóa. Các thí nghiệm của chúng tôi chứng minh
hiệu quả của RAPT trên các tác vụ khác nhau, các LLM khác nhau,
và nhiều kịch bản tấn công. Ngoài ra, kết quả thực nghiệm
tiết lộ rằng các thẻ POS khác nhau cung cấp các mức độ bảo vệ
quyền riêng tư khác nhau, nhưng tất cả đều cần thiết cho
việc duy trì cấu trúc cú pháp của câu đầu vào. Điều này xác thực
động lực đằng sau phương pháp tư nhân hóa văn bản-đến-văn bản
ràng buộc POS của chúng tôi.

TÀI LIỆU THAM KHẢO
[1] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova,
"Bert: Pre-training of deep bidirectional transformers
for language understanding," arXiv preprint
arXiv:1810.04805, 2018.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al., "Language models are few-shot learn-
ers," Advances in neural information processing systems,
vol. 33, pp. 1877–1901, 2020.
[3] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li,
S. Wang, L. Wang, và W. Chen, "Lora: Low-rank
adaptation of large language models," arXiv preprint
arXiv:2106.09685, 2021.
[4] X. L. Li và P. Liang, "Prefix-tuning: Optimizing
continuous prompts for generation," in Proc. of
ACL, 2021, pp. 4582–4597. [Online]. Available: https:
//aclanthology.org/2021.acl-long.353
[5] B. Lester, R. Al-Rfou, và N. Constant, "The power
of scale for parameter-efficient prompt tuning," arXiv
preprint arXiv:2104.08691, 2021.
[6] K. Martin và H. Nissenbaum, "Measuring privacy:
An empirical test using context to expose confounding
variables," Colum. Sci. & Tech. L. Rev., vol. 18, p. 176,
2016.
[7] G. D. P. R. GDPR, "General data protection regulation,"
Regulation (EU) 2016/679 of the European Parliament
and of the Council of 27 April 2016 on the protection
of natural persons with regard to the processing of
personal data and on the free movement of such data,
and repealing Directive 95/46/EC, 2016.
[8] S. L. Pardau, "The california consumer privacy act:
Towards a european-style privacy regime in the united
states," J. Tech. L. & Pol'y, vol. 23, p. 68, 2018.
[9] M. Coavoux, S. Narayan, và S. B. Cohen, "Privacy-
preserving neural representations of text," arXiv preprint
arXiv:1808.09408, 2018.
[10] D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Ka-
math, J. Kulkarni, Y. T. Lee, A. Manoel, L. Wutschitz
et al., "Differentially private fine-tuning of language
models," arXiv preprint arXiv:2110.06500, 2021.
[11] W. Shi, A. Cui, E. Li, R. Jia, và Z. Yu, "Selective dif-
ferential privacy for language modeling," arXiv preprint
arXiv:2108.12944, 2021.
[12] R. Anil, B. Ghazi, V. Gupta, R. Kumar, và P. Ma-
nurangsi, "Large-scale differentially private bert," arXiv
preprint arXiv:2108.01624, 2021.
[13] S. Hoory, A. Feder, A. Tendler, S. Erell, A. Peled-
Cohen, I. Laish, H. Nakhost, U. Stemmer, A. Benjamini,
A. Hassidim et al., "Learning and evaluating a differen-
tially private pre-trained language model," in Findings of
the Association for Computational Linguistics: EMNLP
2021, 2021, pp. 1178–1189.
[14] X. Li, F. Tramer, P. Liang, và T. Hashimoto, "Large
language models can be strong differentially private
learners," arXiv preprint arXiv:2110.05679, 2021.
[15] W. Shi, S. Chen, C. Zhang, R. Jia, và Z. Yu, "Just fine-
tune twice: Selective differential privacy for large lan-
guage models," arXiv preprint arXiv:2204.07667, 2022.
[16] L. Lyu, X. He, và Y. Li, "Differentially private
representation for NLP: Formal guarantee and an
empirical study on privacy and fairness," in Proc. of
EMNLP, 2020, pp. 2355–2365. [Online]. Available:
https://aclanthology.org/2020.findings-emnlp.213
[17] S. Arnold, D. Yesilbas, và S. Weinzierl, "Guiding
text-to-text privatization by syntax," in Proceedings of
the 3rd Workshop on Trustworthy Natural Language
Processing (TrustNLP 2023), A. Ovalle, K.-W. Chang,
N. Mehrabi, Y. Pruksachatkun, A. Galystan, J. Dhamala,
A. Verma, T. Cao, A. Kumar, và R. Gupta, Eds. Toronto,
Canada: Association for Computational Linguistics,
Jul. 2023, pp. 151–162. [Online]. Available:
https://aclanthology.org/2023.trustnlp-1.14
[18] S. Sreekumar và D. Gündüz, "Optimal privacy-utility
trade-off under a rate constraint," in 2019 IEEE Interna-
tional Symposium on Information Theory (ISIT). IEEE,
2019, pp. 2159–2163.
[19] J. Ye, Z. Zhu, F. Liu, R. Shokri, và V. Cevher, "Initial-
ization matters: Privacy-utility analysis of overparameter-
ized neural networks," Advances in Neural Information
Processing Systems, vol. 36, 2024.
[20] A. Zhang, Y. Wang, và S. Guo, "On the utility-
informativeness-security trade-off in discrete task-
oriented semantic communication," IEEE Communica-
tions Letters, 2024.
[21] Z. Yang và Y. Liu, "On robust prefix-tuning for text
classification," arXiv preprint arXiv:2203.10378, 2022.
[22] O. Feyisetan, B. Balle, T. Drake, và T. Diethe, "Privacy-
and utility-preserving textual analysis via calibrated mul-
tivariate perturbations," in Proc. of WSDM, 2020, pp.
178–186.
[23] A. Evfimievski, J. Gehrke, và R. Srikant, "Limiting
privacy breaches in privacy preserving data mining," in
Proc of SIGMOD-SIGACT-SIGART, 2003, pp. 211–222.
[24] E. Voita, R. Sennrich, và I. Titov, "The bottom-up
evolution of representations in the transformer: A
study with machine translation and language modeling
objectives," in Proc. of ACL, 2019, pp. 4396–4406.
[Online]. Available: https://aclanthology.org/D19-1448
[25] J. Mamou, H. Le, M. Del Rio, C. Stephenson, H. Tang,
Y. Kim, và S. Chung, "Emergence of separable man-
ifolds in deep language representations," in 37th Inter-
national Conference on Machine Learning, ICML 2020.
International Machine Learning Society (IMLS), 2020.
[26] A. Wettig, T. Gao, Z. Zhong, và D. Chen, "Should
you mask 15% in masked language modeling?" arXiv
preprint arXiv:2202.08005, 2022.
[27] N. Carlini, F. Tramer, E. Wallace, M. Jagielski,
A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song,
U. Erlingsson et al., "Extracting training data from large
language models," in USENIX Security, 2021, pp. 2633–
2650.
[28] D. Yu, H. Zhang, W. Chen, J. Yin, và T.-Y. Liu, "Large
scale private learning via low-rank reparametrization," in
Proc. of ICML. PMLR, 2021, pp. 12208–12218.
[29] G. Kerrigan, D. Slack, và J. Tuyls, "Differentially pri-
vate language models benefit from public pre-training,"
arXiv preprint arXiv:2009.05886, 2020.
[30] C. Qu, W. Kong, L. Yang, M. Zhang, M. Bendersky,
và M. Najork, Natural Language Understanding with
Privacy-Preserving BERT. Association for Computing
Machinery, 2021, p. 1488–1497. [Online]. Available:
https://doi.org/10.1145/3459637.3482281
[31] Y. Ding, X. Wu, Y. Meng, Y. Luo, H. Wang, và W. Pan,
"Delving into differentially private transformer," arXiv
preprint arXiv:2405.18194, 2024.
[32] Y. Huang, Z. Song, D. Chen, K. Li, và S. Arora, "Tex-
thide: Tackling data privacy in language understanding
tasks," in Findings of the Association for Computational
Linguistics: EMNLP 2020, 2020, pp. 1368–1382.
[33] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, và
S. Zanella-Béguelin, "Analyzing leakage of personally
identifiable information in language models," in 2023
IEEE Symposium on Security and Privacy (SP). IEEE,
2023, pp. 346–363.
[34] X. Shen, Y. Liu, H. Liu, J. Hong, B. Duan, Z. Huang,
Y. Mao, Y. Wu, và D. Wu, "A split-and-privatize
framework for large language model fine-tuning," arXiv
preprint arXiv:2312.15603, 2023.
[35] R. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y. Xu, Y. Du,
Y. Wang, và S. Chen, "Openfedllm: Training large lan-
guage models on decentralized private data via federated
learning," in Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining,
2024, pp. 6137–6147.
[36] K. Chatzikokolakis, M. E. Andrés, N. E. Bordenabe, và
C. Palamidessi, "Broadening the scope of differential
privacy using metrics," in Proc. of PETS. Springer,
2013, pp. 82–102.
[37] V. Bindschaedler, R. Shokri, và C. A. Gunter, "Plausible
deniability for privacy-preserving data synthesis," arXiv
preprint arXiv:1708.07975, 2017.
[38] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, và J. Tang, "P-
tuning v2: Prompt tuning can be comparable to fine-
tuning universally across scales and tasks," arXiv preprint
arXiv:2110.07602, 2021.
[39] C. Song và A. Raghunathan, "Information leakage in
embedding models," in Proc. of ACM SIGSAC, 2020, pp.
377–390.
[40] D. Hovy, A. Johannsen, và A. Søgaard, "User review
sites as a resource for large-scale sociolinguistic studies,"
in Proc. of TheWebConf, 2015, pp. 452–461.
[41] R. Plant, D. Gkatzia, và V. Giuffrida, "CAPE: Context-
aware private embeddings for private language learning,"
in Proc. of EMNLP, 2021, pp. 7970–7978. [Online].
Available: https://aclanthology.org/2021.emnlp-main.628
[42] Y. Yang, H. Wu, và H. Zhao, "Attack named en-
tity recognition by entity boundary interference," arXiv
preprint arXiv:2305.05253, 2023.
[43] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, và P. J. Liu, "Exploring
the limits of transfer learning with a unified text-to-
text transformer," The Journal of Machine Learning
Research, vol. 21, no. 1, pp. 5485–5551, 2020.
[44] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Man-
ning, A. Y. Ng, và C. Potts, "Recursive deep models for
semantic compositionality over a sentiment treebank," in
Proc. of EMNLP, 2013, pp. 1631–1642.
[45] Z. Chen, H. Zhang, X. Zhang, và L. Zhao, "Quora
question pairs," University of Waterloo, pp. 1–7, 2018.
[46] C. Gardent, A. Shimorina, S. Narayan, và L. Perez-
Beltrachini, "The webnlg challenge: Generating text from
rdf data," in Proceedings of the 10th International Con-
ference on Natural Language Generation, 2017, pp. 124–
133.
[47] D. P. Kingma và J. Ba, "Adam: A method for stochastic
optimization," arXiv preprint arXiv:1412.6980, 2014.
[48] N. Shazeer và M. Stern, "Adafactor: Adaptive learn-
ing rates with sublinear memory cost," in International
Conference on Machine Learning. PMLR, 2018, pp.
4596–4604.
[49] T. Dettmers, A. Pagnoni, A. Holtzman, và L. Zettle-
moyer, "Qlora: Efficient finetuning of quantized llms,"
Advances in Neural Information Processing Systems,
vol. 36, 2024.
[50] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So,
và J. Kang, "Biobert: a pre-trained biomedical language
representation model for biomedical text mining," Bioin-
formatics, vol. 36, no. 4, pp. 1234–1240, 2020.
