# 2310.11454.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2310.11454.pdf
# Kích thước tệp: 526144 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
VERA: THÍCH ỨNG MA TRẬN NGẪU NHIÊN DỰA TRÊN VECTOR
Dawid J. Kopiczko∗ †
QUV A Lab
Đại học AmsterdamTijmen Blankevoort
Qualcomm AI Research1Yuki M. Asano
QUV A Lab
Đại học Amsterdam

TÓM TẮT
Thích ứng hạng thấp (LoRA) là một phương pháp phổ biến giúp giảm số lượng tham số có thể huấn luyện khi tinh chỉnh các mô hình ngôn ngữ lớn, nhưng vẫn phải đối mặt với những thách thức lưu trữ cấp tính khi mở rộng sang các mô hình thậm chí lớn hơn hoặc triển khai nhiều mô hình được thích ứng theo từng người dùng hoặc từng tác vụ. Trong công trình này, chúng tôi giới thiệu Thích ứng Ma trận Ngẫu nhiên dựa trên Vector (VeRA), phương pháp giảm đáng kể số lượng tham số có thể huấn luyện so với LoRA, nhưng vẫn duy trì hiệu suất tương tự. Phương pháp này đạt được điều đó bằng cách sử dụng một cặp ma trận hạng thấp duy nhất được chia sẻ giữa tất cả các lớp và học các vector tỷ lệ nhỏ thay thế. Chúng tôi chứng minh hiệu quả của phương pháp trên các benchmark GLUE và E2E, các tác vụ phân loại hình ảnh, và cho thấy ứng dụng của nó trong việc tinh chỉnh theo hướng dẫn cho các mô hình ngôn ngữ 7B và 13B.

1 GIỚI THIỆU
Trong kỷ nguyên của các mô hình ngôn ngữ ngày càng lớn và phức tạp, thách thức thích ứng hiệu quả cho các tác vụ cụ thể đã trở nên quan trọng hơn bao giờ hết. Trong khi các mô hình này cung cấp khả năng mạnh mẽ, yêu cầu bộ nhớ rộng lớn của chúng tạo ra một nút thắt cổ chai đáng kể, đặc biệt khi thích ứng chúng để sử dụng cá nhân hóa. Hãy xem xét, ví dụ, một trợ lý hệ điều hành dựa trên đám mây liên tục học hỏi và thích ứng với hành vi và phản hồi của từng người dùng. Nhu cầu lưu trữ nhiều checkpoint của các mô hình được tinh chỉnh cho mỗi người dùng nhanh chóng làm gia tăng dung lượng lưu trữ cần thiết, thậm chí còn nhiều hơn khi có nhiều tác vụ tham gia.

Tình huống càng trở nên tồi tệ hơn khi chúng ta nhìn vào các mô hình tiên tiến như GPT-4 (OpenAI, 2023). Các kỹ thuật tinh chỉnh như LoRA (Hu et al., 2022), mặc dù hiệu quả, vẫn gây ra chi phí bộ nhớ đáng kể. Như một ví dụ minh họa, áp dụng LoRA với hạng 16 cho các lớp truy vấn và giá trị của GPT-3 (Brown et al., 2020) sẽ đòi hỏi ít nhất 288MB bộ nhớ, nếu lưu trữ ở độ chính xác đơn - với một triệu trọng số được tinh chỉnh, ví dụ một trọng số cho mỗi người dùng, điều đó sẽ tương đương với 275TB.

Với sự phổ biến gần đây của các mô hình ngôn ngữ và việc triển khai chúng trong các trợ lý cá nhân hóa, thiết bị biên và các ứng dụng tương tự, các phương pháp thích ứng hiệu quả là vô cùng quan trọng. Chúng tôi tin rằng vẫn còn tiềm năng chưa được khai thác cho các phương pháp tiếp cận thậm chí hiệu quả hơn. Công trình trước đây (Aghajanyan et al., 2021) đã chỉ ra chiều hướng nội tại thấp của các đặc trưng mô hình được tiền huấn luyện. Các nghiên cứu này báo cáo các con số thấp hơn nhiều so với các tham số có thể huấn luyện được sử dụng trong LoRA, cho thấy có chỗ để cải thiện.

Song song với điều này, nghiên cứu gần đây đã cho thấy hiệu quả đáng ngạc nhiên của các mô hình sử dụng trọng số ngẫu nhiên và phép chiếu (Peng et al., 2021; Ramanujan et al., 2020; Lu et al., 2022; Schrimpf et al., 2021; Frankle et al., 2021). Các mô hình như vậy đóng vai trò là cơ sở cho giải pháp được đề xuất của chúng tôi, Thích ứng Ma trận Ngẫu nhiên dựa trên Vector (VeRA), phương pháp giảm thiểu số lượng tham số có thể huấn luyện được giới thiệu trong quá trình tinh chỉnh bằng cách tái tham số hóa các ma trận trọng số. Cụ thể, chúng tôi sử dụng "vector tỷ lệ" để thích ứng một cặp ma trận ngẫu nhiên cố định được chia sẻ giữa các lớp. Với phương pháp này, nhiều phiên bản của mô hình có thể cư trú trong bộ nhớ hạn chế của một GPU duy nhất.

∗dj.kopiczko@gmail.com ;1Qualcomm AI Research là một sáng kiến của Qualcomm Technologies, Inc.
†Bộ dữ liệu chỉ được tải xuống và đánh giá bởi Đại học Amsterdam.
1arXiv:2310.11454v2 [cs.CL] 16 Jan 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Tóm lại, những đóng góp chính của chúng tôi như sau:
• Chúng tôi giới thiệu một phương pháp tinh chỉnh mới không có chi phí thời gian suy luận bổ sung. Phương pháp của chúng tôi giảm thêm số lượng tham số có thể huấn luyện so với phương pháp LoRA tiên tiến, trong khi mang lại kết quả tương đương.
• Chúng tôi so sánh phương pháp của mình với LoRA và các phương pháp thích ứng hiệu quả tham số khác trên các benchmark hiểu biết ngôn ngữ tự nhiên (GLUE) và tạo sinh ngôn ngữ tự nhiên (E2E), và so sánh với LoRA trên các tác vụ theo hướng dẫn và phân loại hình ảnh.
• Chúng tôi thực hiện một nghiên cứu loại bỏ để hiểu rõ hơn các thành phần riêng lẻ của phương pháp và tác động của chúng đối với hiệu suất.

2 CÔNG TRÌNH LIÊN QUAN
Thích ứng Hạng thấp (LoRA). LoRA cung cấp một giải pháp sáng tạo cho các thách thức tính toán do việc tinh chỉnh các mô hình ngôn ngữ lớn được tiền huấn luyện tạo ra. Được giới thiệu bởi Hu et al. (2022), phương pháp này sử dụng các ma trận hạng thấp để xấp xỉ các thay đổi trọng số trong quá trình tinh chỉnh, hiệu quả giảm số lượng tham số cần được huấn luyện. Trong số các ưu điểm của nó, LoRA giảm đáng kể rào cản phần cứng cho việc tinh chỉnh bằng cách giảm nhu cầu tính toán gradient và duy trì trạng thái tối ưu hóa cho hầu hết các tham số. Nó cũng có thể hoạt động với các trọng số mô hình được lượng tử hóa (Dettmers et al., 2023), giảm yêu cầu hơn nữa. Hơn nữa, các mô-đun LoRA dễ dàng hoán đổi, làm cho việc chuyển đổi tác vụ hiệu quả và ít tốn tài nguyên hơn. Quan trọng, và khác với các phương pháp tinh chỉnh dựa trên bộ điều hợp (Houlsby et al., 2019; Lin et al., 2020; Pfeiffer et al., 2021; Rückle et al., 2021), LoRA không phát sinh chi phí thời gian suy luận bổ sung khi triển khai, vì các ma trận có thể huấn luyện có thể được hợp nhất với các trọng số cố định.

Dựa trên điều này, AdaLoRA (Zhang et al., 2023b) mở rộng phương pháp LoRA, giới thiệu điều chỉnh hạng động cho các ma trận hạng thấp trong quá trình tinh chỉnh. Ý tưởng cốt lõi là phân phối ngân sách tham số một cách tối ưu bằng cách loại bỏ có chọn lọc các thành phần ít quan trọng hơn của các ma trận dựa trên một thước đo tầm quan trọng.

Hiệu quả Tham số trong Các Phương pháp Hiện có Trong khi các phương pháp như LoRA đã cho thấy cải thiện đáng kể trong hiệu suất tinh chỉnh, chúng vẫn yêu cầu một lượng tham số có thể huấn luyện đáng kể. Theo Aghajanyan et al. (2021), giới hạn trên cho các chiều nội tại nhỏ hơn nhiều so với những gì thường được sử dụng trong các phương pháp như vậy. Ví dụ, d90 cho RoBERTa base được báo cáo là 896, trong khi các tác giả của bài báo LoRA báo cáo sử dụng 0,3M tham số có thể huấn luyện cho mô hình này, cho thấy số lượng tham số có thể giảm thêm.

Mặc dù AdaLoRA thực hiện các bước theo hướng này bằng cách phân bổ tham số động cho các lớp quan trọng hơn, chúng tôi cho rằng một phương pháp khác có thể đạt được giảm tham số đáng kể, trong khi chấp nhận sự suy giảm hiệu suất nhỏ. Điều này đặt nền tảng cho phương pháp chúng tôi giới thiệu trong phần tiếp theo.

Mô hình và Phép chiếu Ngẫu nhiên. Khái niệm sử dụng ma trận ngẫu nhiên và phép chiếu cho hiệu quả mô hình được hỗ trợ bởi nhiều dòng nghiên cứu. Frankle & Carbin (2019) xác định rằng các mạng neural được khởi tạo ngẫu nhiên chứa các mạng con có khả năng đạt được hiệu suất cao khi được huấn luyện. Trong khi đó, Ramanujan et al. (2020) tiết lộ rằng tồn tại các mạng con có thể đạt được kết quả ấn tượng ngay cả khi không có huấn luyện. Aghajanyan et al. (2021) cho thấy việc huấn luyện chỉ một số lượng nhỏ tham số, được chiếu ngẫu nhiên trở lại không gian đầy đủ, có thể đạt được 90% hiệu suất mô hình tham số đầy đủ. Ruiz et al. (2023) giới thiệu một phương pháp tinh chỉnh hiệu quả tham số cho việc cá nhân hóa các mô hình văn bản thành hình ảnh, sử dụng các ma trận ngẫu nhiên cố định bên trong LoRA. Các công trình khác (Lu et al., 2022; Schrimpf et al., 2021; Frankle et al., 2021) đã cho thấy các mô hình được khởi tạo ngẫu nhiên cố định, với các phần nhỏ được tinh chỉnh, có thể hoạt động tốt một cách đáng ngạc nhiên.

1Chiều nhỏ nhất d cung cấp một giải pháp thỏa đáng, đó là 90% của thước đo huấn luyện đầy đủ, như được định nghĩa bởi Li et al. (2018).

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình 1: So sánh sơ đồ của LoRA (trái) và VeRA (phải). LoRA cập nhật ma trận trọng số W bằng cách huấn luyện các ma trận hạng thấp A và B, với hạng trung gian r. Trong VeRA, các ma trận này được cố định, chia sẻ giữa tất cả các lớp, và được thích ứng với các vector có thể huấn luyện d và b, giảm đáng kể số lượng tham số có thể huấn luyện. Trong cả hai trường hợp, các ma trận hạng thấp và vector có thể được hợp nhất vào ma trận trọng số gốc W, không tạo ra độ trễ bổ sung.]

Tổng hợp lại, các công trình này tạo ra một trường hợp thuyết phục cho việc sử dụng các ma trận ngẫu nhiên cố định trong các phương pháp tinh chỉnh, cung cấp cả nền tảng lý thuyết và thực nghiệm cho phương pháp được áp dụng trong bài báo này.

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi giới thiệu Thích ứng Ma trận Ngẫu nhiên dựa trên Vector, một phương pháp tinh chỉnh hiệu quả tham số mới được xây dựng dựa trên và mở rộng phương pháp tiên tiến, LoRA. Sự đổi mới trung tâm trong VeRA nằm ở việc tái tham số hóa các ma trận hạng thấp. Cụ thể, chúng tôi cố định một cặp ma trận được khởi tạo ngẫu nhiên, chia sẻ giữa tất cả các lớp được thích ứng, và giới thiệu các vector tỷ lệ có thể huấn luyện cho phép thích ứng theo từng lớp, như được hiển thị trong Hình 1. Tương tự như LoRA, các vector tỷ lệ được huấn luyện cùng với các ma trận hạng thấp có thể được hợp nhất vào các trọng số gốc, loại bỏ độ trễ suy luận bổ sung.

3.1 CÔNG THỨC PHƯƠNG PHÁP
LoRA (Hu et al., 2022) tinh chỉnh một tích ma trận của hai ma trận hạng thấp để thích ứng các mô hình ngôn ngữ lớn cho một tác vụ mới. Chính thức, đối với một ma trận trọng số được tiền huấn luyện W0∈Rm×n, việc cập nhật trọng số ΔW bị ràng buộc bởi một phân rã hạng thấp, như được biểu diễn trong Phương trình 1

h=W0x+ ΔWx=W0x+BAx, (1)

trong đó chúng tôi gạch chân các tham số được cập nhật thông qua gradient descent. Xấp xỉ này cho phép mô hình giữ trọng số gốc W0 cố định trong khi chỉ tối ưu hóa các ma trận hạng thấp mới A và B. Các ma trận này nhỏ hơn nhiều về kích thước so với ma trận gốc do tính chất hạng giảm của chúng. A có hình dạng m×r và B có hình dạng r×n, trong đó r≪min(m, n) đóng vai trò như chiều nút thắt cổ chai. Ngược lại, phương pháp VeRA của chúng tôi được biểu diễn như:

h=W0x+ ΔWx=W0x+ ΛbBΛdAx (2)

Trong phương pháp này, B và A được cố định, ngẫu nhiên, và chia sẻ giữa các lớp, trong khi các vector tỷ lệ b và d có thể huấn luyện, và được ký hiệu chính thức bởi các ma trận chéo Λb và Λd. Phương pháp này có thể hiệu quả tỷ lệ và vô hiệu hóa các hàng và cột của cả A và B, cho phép thích ứng theo từng lớp với số lượng tham số có thể huấn luyện tối thiểu. Lưu ý rằng trong thiết lập này, B∈Rm×r và A∈Rr×n

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

không bắt buộc phải là hạng thấp. Điều này là vì chúng vẫn tĩnh và chúng tôi không cần lưu trữ giá trị của chúng. Thay vào đó, việc thay đổi r dẫn đến sự gia tăng tuyến tính trong số lượng tham số có thể huấn luyện thông qua d∈R1×r.

3.2 SỐ LƯỢNG THAM SỐ

Bảng 1: Bộ nhớ lý thuyết cần thiết để lưu trữ các trọng số VeRA và LoRA được huấn luyện cho các mô hình RoBERTa base, RoBERTa large và GPT-3. Chúng tôi giả định rằng các phương pháp LoRA và VeRA được áp dụng trên các lớp truy vấn và khóa của mỗi khối transformer.

[Bảng thể hiện so sánh số lượng tham số và dung lượng byte cần thiết giữa LoRA và VeRA cho các mô hình khác nhau với hạng khác nhau]

Chúng tôi sử dụng Ltuned để biểu thị số lượng lớp được tinh chỉnh và dmodel để biểu thị chiều của các lớp này. Số lượng tham số có thể huấn luyện trong VeRA sau đó được điều chỉnh bởi |Θ|=Ltuned×(dmodel+r), trái ngược với |Θ|= 2×Ltuned×dmodel×r của LoRA. Cụ thể, đối với hạng thấp nhất (tức là, r= 1), VeRA yêu cầu khoảng một nửa tham số có thể huấn luyện của LoRA. Hơn nữa, khi hạng tăng, số lượng tham số của VeRA tăng Ltuned cho mỗi lần tăng, một khoản tiết kiệm đáng kể so với 2Ltuneddmodel của LoRA. Hiệu quả tham số này trở nên đáng kể trong bối cảnh các mô hình cực kỳ sâu và rộng, chẳng hạn như GPT-3 (Brown et al., 2020), có 96 lớp attention và kích thước ẩn 12288.

Dựa trên hiệu quả này, ưu điểm chính của VeRA là dấu chân bộ nhớ tối thiểu để lưu trữ các điều chỉnh trọng số được huấn luyện. Vì các ma trận ngẫu nhiên cố định có thể được tái tạo từ một hạt giống bộ tạo số ngẫu nhiên (RNG), chúng không cần được lưu trữ trong bộ nhớ. Điều này giảm đáng kể yêu cầu bộ nhớ, hiện tại bị hạn chế bởi các byte cần thiết cho các vector b và d được huấn luyện và một hạt giống RNG duy nhất. Hiệu quả bộ nhớ so với LoRA được hiển thị trong Bảng 1.

3.3 CHIẾN LƯỢC KHỞI TẠO

• Ma trận Chia sẻ: Trong phương pháp của chúng tôi, chúng tôi sử dụng khởi tạo Kaiming (He et al., 2015) cho các ma trận hạng thấp cố định A và B. Bằng cách tỷ lệ các giá trị dựa trên chiều ma trận, nó đảm bảo rằng tích ma trận của A và B duy trì phương sai nhất quán cho tất cả các hạng, loại bỏ nhu cầu tinh chỉnh tốc độ học cho mỗi hạng.

• Vector Tỷ lệ: Vector tỷ lệ b được khởi tạo bằng số không, phù hợp với việc khởi tạo ma trận B trong LoRA và đảm bảo ma trận trọng số không bị ảnh hưởng trong lần truyền tiến đầu tiên. Vector tỷ lệ d được khởi tạo với một giá trị khác không duy nhất trên tất cả các phần tử của nó, do đó giới thiệu một siêu tham số mới có thể được điều chỉnh để có hiệu suất tốt hơn.

Hình 1 minh họa các ví dụ khởi tạo cho các ma trận hạng thấp và vector tỷ lệ trong VeRA. Cụ thể, các ma trận hạng thấp được khởi tạo sử dụng phân phối chuẩn, và vector d được khởi tạo bằng một. Lưu ý rằng các khởi tạo thay thế, chẳng hạn như phân phối đều cho A và B, và các hằng số khác không khác cho d, cũng được khám phá trong các thí nghiệm của chúng tôi.

4 THÍ NGHIỆM

Trong phần này, chúng tôi tiến hành một loạt thí nghiệm để đánh giá phương pháp tinh chỉnh của chúng tôi. Chúng tôi bắt đầu bằng cách so sánh phương pháp của mình với LoRA và các baseline khác trên các benchmark GLUE và E2E. Tiếp theo, chúng tôi chuyển sự chú ý đến việc tinh chỉnh theo hướng dẫn các mô hình Llama, và phân loại hình ảnh với Vision Transformers. Tiếp theo, chúng tôi chọn một tác vụ và thay đổi hạng cho cả hai phương pháp, LoRA và VeRA, để kiểm tra cách hiệu suất tỷ lệ với số lượng tham số có thể huấn luyện. Cuối cùng, một nghiên cứu loại bỏ làm sáng tỏ tầm quan trọng của từng thành phần trong phương pháp của chúng tôi, bao gồm ảnh hưởng của các khởi tạo khác nhau.

Baseline. Chúng tôi so sánh VeRA với các baseline sau:

• Tinh chỉnh đầy đủ - mô hình được khởi tạo với các trọng số được tiền huấn luyện và tất cả các tham số đều được huấn luyện.
• Bitfit - baseline này chỉ liên quan đến việc tinh chỉnh các vector bias, giữ tất cả các tham số khác cố định. Kỹ thuật này đã được nghiên cứu sâu sắc bởi Zaken et al. (2022).
• Điều chỉnh adapter - ban đầu được giới thiệu bởi Houlsby et al. (2019), liên quan đến việc tích hợp các lớp adapter giữa các mô-đun self-attention và MLP, tiếp theo là kết nối dư. Thiết lập này bao gồm hai lớp được kết nối đầy đủ và một phi tuyến tính và được ký hiệu là AdapterH. Một biến thể của Lin et al. (2020), AdapterL, sử dụng lớp adapter chỉ sau mô-đun MLP và tiếp theo sau LayerNorm. Điều này giống với một thiết kế thay thế được đề xuất bởi Pfeiffer et al. (2021), được gọi là AdapterP. Một baseline khác, được gọi là AdapterDrop bởi Rückle et al. (2021), tăng hiệu quả bằng cách bỏ qua một số lớp adapter nhất định và được biểu diễn là AdapterD.
• LoRA (Hu et al., 2022) - như đã giới thiệu trong phần trước.

4.1 BENCHMARK GLUE

Chúng tôi đánh giá phương pháp của mình trên benchmark General Language Understanding Evaluation (GLUE) (Wang et al., 2019), sử dụng các mô hình RoBERTa base và RoBERTa large (Liu et al., 2019). Đối với RoBERTa base, chúng tôi sử dụng hạng 1024, và đối với RoBERTa large, hạng 256. Các ma trận chia sẻ được khởi tạo sử dụng phiên bản đều của khởi tạo Kaiming như được triển khai trong PyTorch (Paszke et al., 2019), với giá trị ban đầu 0.1 cho vector d.

Thiết lập thí nghiệm của chúng tôi nói chung phù hợp với của Hu et al. (2022), áp dụng phương pháp của chúng tôi cho các ma trận chiếu truy vấn và giá trị trong mỗi mô-đun self-attention và huấn luyện đầy đủ đầu phân loại. Không giống như Hu et al. (2022), những người đã sử dụng một siêu tham số α bổ sung để điều chỉnh gradient cho các lớp được thích ứng, chúng tôi giới thiệu các tốc độ học riêng biệt cho đầu phân loại và các lớp được thích ứng. Chúng tôi xác định tốc độ học và số lượng epoch huấn luyện thông qua điều chỉnh siêu tham số; để biết cài đặt chi tiết, tham khảo Bảng 8 trong Phụ lục A. Kích thước batch được đặt là 64 cho RoBERTa base và 32 cho RoBERTa large, với độ dài chuỗi tối đa lần lượt là 512 và 128.

Do hạn chế về thời gian và ngân sách, chúng tôi bỏ qua các tác vụ MNLI và QQP tốn thời gian, do đó không sử dụng thủ thuật MNLI2 cho các tác vụ MRPC, RTE và STS-B. Phù hợp với Hu et al. (2022), chúng tôi báo cáo số lượng tham số có thể huấn luyện được gán cho các lớp được tinh chỉnh, loại trừ rõ ràng đầu phân loại, được huấn luyện theo cách tiêu chuẩn. Chúng tôi thực hiện 5 lần chạy với các hạt giống ngẫu nhiên khác nhau, ghi lại kết quả của epoch tốt nhất cho mỗi lần chạy, và báo cáo trung vị của các kết quả này.

Kết quả. Bảng 2 tiết lộ rằng VeRA hoạt động cạnh tranh với LoRA trên cả hai mô hình, nhưng đạt được các kết quả này với ít tham số hơn một bậc độ lớn.

4.2 BENCHMARK E2E

Đối với benchmark E2E (Novikova et al., 2017), chúng tôi theo thiết lập thí nghiệm từ Hu et al. (2022) và tinh chỉnh các mô hình GPT-2 (Radford et al., 2019) Medium và Large. Đối với LoRA, chúng tôi sử dụng việc triển khai và bộ siêu tham số được cung cấp trong Hu et al. (2022), trong khi đối với VeRA, chúng tôi thay đổi hạng và tốc độ học, cả hai đều được điều chỉnh. Bảng với tất cả các siêu tham số được sử dụng có thể được tìm thấy trong Phụ lục A.

2Đối với mô hình RoBERTa base và các tác vụ MRPC, RTE và STS-B, Hu et al. (2022) đã khởi tạo mô hình với các trọng số tốt nhất được tinh chỉnh trên tác vụ MNLI.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 2: Kết quả cho các phương pháp thích ứng khác nhau trên benchmark GLUE. Chúng tôi báo cáo tương quan Matthew cho CoLA, tương quan Pearson cho STS-B, và độ chính xác cho các tác vụ còn lại. Trong tất cả các trường hợp, giá trị cao hơn cho thấy hiệu suất tốt hơn. Kết quả của tất cả các phương pháp ngoại trừ VeRA được lấy từ công trình trước đây (Hu et al., 2022; Zhang et al., 2023a). VeRA hoạt động ngang bằng với LoRA với ít tham số hơn một bậc độ lớn.

[Bảng kết quả chi tiết cho các phương pháp khác nhau trên benchmark GLUE]

Bảng 3: Kết quả cho các phương pháp thích ứng khác nhau trên benchmark E2E và các mô hình GPT2 Medium và Large. Kết quả với (1,2,3) được lấy từ công trình trước đây:1(Hu et al., 2022),2(Valipour et al., 2022),3(Zi et al., 2023). VeRA vượt trội hơn LoRA với ít tham số có thể huấn luyện gấp 3 và 4 lần, lần lượt cho GPT2 Medium và Large.

[Bảng kết quả chi tiết cho benchmark E2E]

Kết quả. Chúng tôi báo cáo kết quả từ epoch cuối cùng. Bảng 3 cho thấy VeRA vượt trội hơn LoRA với ít tham số có thể huấn luyện gấp 3 và 4 lần, lần lượt cho GPT2 Medium và Large.

4.3 TINH CHỈNH THEO HƯỚNG DẪN

Tinh chỉnh theo hướng dẫn là một quá trình mà các mô hình ngôn ngữ được tinh chỉnh để tuân theo các hướng dẫn cụ thể hiệu quả hơn (Ouyang et al., 2022). Chúng tôi chứng minh hiệu quả của VeRA trong việc cho phép các mô hình Llama (Touvron et al., 2023a) và Llama2 (Touvron et al., 2023b) tuân theo hướng dẫn chỉ sử dụng 1.6M và 2.4M tham số có thể huấn luyện, lần lượt cho các biến thể 7B và 13B, trái ngược với 159.9M và 250.3M tham số có thể huấn luyện khi sử dụng LoRA với hạng 64 như được đề xuất bởi Dettmers et al. (2023).

Chúng tôi thực hiện tinh chỉnh sử dụng cả LoRA và VeRA, bằng cách áp dụng cả hai phương pháp trên tất cả các lớp tuyến tính ngoại trừ lớp trên cùng, tương tự như Dettmers et al. (2023). Ngoài ra, chúng tôi tận dụng các kỹ thuật lượng tử hóa từ Dettmers et al. (2023) để huấn luyện mô hình trên một GPU duy nhất.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Đối với thí nghiệm của chúng tôi, chúng tôi sử dụng bộ dữ liệu Alpaca (Taori et al., 2023), cụ thể là phiên bản đã được làm sạch3. Bộ dữ liệu này bao gồm 51K hướng dẫn và minh họa và phù hợp cho việc tinh chỉnh theo hướng dẫn. Phiên bản đã được làm sạch sửa chữa nhiều vấn đề như ảo giác, hướng dẫn được hợp nhất, và đầu ra trống. Chúng tôi huấn luyện trong một epoch, trước đó là một cuộc quét tốc độ học.

Chúng tôi đánh giá các mô hình được tinh chỉnh trên MT-Bench (Zheng et al., 2023), bằng cách tạo ra các phản hồi của mô hình cho một tập hợp được định trước gồm 80 câu hỏi đa lượt và sau đó đánh giá chúng bằng GPT-4 (OpenAI, 2023). GPT-4 xem xét các câu trả lời và gán một điểm số định lượng trên thang điểm 10 cho mỗi phản hồi. Chúng tôi trình bày điểm số trung bình cùng với số lượng tham số có thể huấn luyện trong Bảng 4.

Bảng 4: Điểm số trung bình trên MT-Bench được gán bởi GPT-4 cho các câu trả lời được tạo ra bởi các mô hình được tinh chỉnh với các phương pháp VeRA và LoRA, và mô hình Llama 13B cơ bản. VeRA khá khớp với hiệu suất của LoRA trên tác vụ tuân theo hướng dẫn, với giảm 100 lần trong tham số có thể huấn luyện.

[Bảng kết quả so sánh VeRA và LoRA trên MT-Bench]

Chúng tôi thấy rằng mặc dù giảm 100 lần số lượng tham số có thể huấn luyện, phương pháp của chúng tôi khá khớp với hiệu suất của việc tinh chỉnh dựa trên LoRA.

4.4 PHÂN LOẠI HÌNH ẢNH

Để đánh giá phương pháp trên tác vụ phân loại hình ảnh, chúng tôi thích ứng Vision Transformer (ViT) (Dosovitskiy et al., 2021), các biến thể Base và Large, trên các bộ dữ liệu - CIFAR100 (Krizhevsky, 2009), Food101 (Bossard et al., 2014), Flowers102 (Nilsback & Zisserman, 2008), và RESISC45 (Cheng et al., 2017). Đối với mỗi bộ dữ liệu, chúng tôi huấn luyện trên một tập con gồm 10 mẫu trên mỗi lớp, và đánh giá trên toàn bộ tập kiểm tra (CIFAR100, Food101, Flowers102) hoặc trên tất cả các mẫu còn lại (RESISC45). Chúng tôi sử dụng trọng số của các mô hình ViT được tiền huấn luyện trên bộ dữ liệu ImageNet-21k (Deng et al., 2009).

Chúng tôi đánh giá các phương pháp LoRA và VeRA được áp dụng trên các lớp truy vấn và giá trị của ViT, cùng với hai baseline - mô hình được tinh chỉnh đầy đủ (được gọi là Full), và chỉ huấn luyện đầu phân loại (được gọi là Head). Tương tự như benchmark GLUE, chúng tôi sử dụng hạng 8 cho LoRA, và hạng 256 cho VeRA. Chúng tôi điều chỉnh tốc độ học cho tất cả các phương pháp và báo cáo kết quả sau 10 epoch trong Bảng 5. Số lượng tham số được báo cáo loại trừ đầu phân loại, phải được huấn luyện trong tất cả các phương pháp.

Chúng tôi thấy rằng VeRA tiếp cận hiệu suất của LoRA trên mô hình Base cho ba bộ dữ liệu và vượt trội hơn nó cho Flowers102, mặc dù sử dụng ít tham số có thể huấn luyện hơn 10 lần. Đối với ViT-Large, nó vượt trội hơn LoRA cho ba bộ dữ liệu: CIFAR100, Flowers102 và RESISC45.

4.5 MỞ RỘNG SỐ LƯỢNG THAM SỐ CÓ THỂ HUẤN LUYỆN

Cuối cùng, chúng tôi điều tra các đánh đổi liên quan đến khả năng mở rộng tham số cho cả LoRA và phương pháp của chúng tôi sử dụng mô hình RoBERTa large trên tác vụ RTE từ benchmark GLUE. Chúng tôi sử dụng một tập hợp các hạng r={1,4,16,64,256,1024} cho VeRA và r={1,2,4,8,16,32,64} cho LoRA, và quan sát sự đánh đổi giữa tham số có thể huấn luyện và độ chính xác. Chúng tôi lặp lại mỗi cấu hình năm lần cho các hạt giống ngẫu nhiên khác nhau, và báo cáo trung vị của kết quả. Đối với LoRA, chúng tôi sử dụng việc triển khai HuggingFace PEFT (Mangrulkar et al., 2022), tuân thủ các siêu tham số được chỉ định trong Hu et al. (2022). Phương pháp của chúng tôi sử dụng các siêu tham số giống như được sử dụng trong

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 5: Các mô hình vision được tinh chỉnh với VeRA và LoRA trên các bộ dữ liệu phân loại hình ảnh khác nhau. VeRA tiếp cận hiệu suất của LoRA cho mô hình nhỏ hơn, và vượt trội hơn nó trong trường hợp mô hình lớn, với ít tham số có thể huấn luyện hơn 10 lần.

[Bảng kết quả so sánh VeRA và LoRA trên các tác vụ phân loại hình ảnh]

[Hình 2: Hiệu suất của các phương pháp LoRA và VeRA cho các hạng khác nhau trên tác vụ RTE.]

[Hình 3: Độ lớn của vector d được thích ứng cho các ma trận truy vấn và giá trị qua các lớp cho RoBERTa-L trên tác vụ RTE.]

các thí nghiệm RTE từ phần con trước. Kết quả, được mô tả trong Hình 2, tiết lộ rằng phương pháp của chúng tôi hiệu quả hơn đáng kể về tham số. Đáng chú ý, khi VeRA hạng cao hơn có cùng số lượng tham số như LoRA tiêu chuẩn, nó vượt trội hơn LoRA 4 điểm phần trăm độ chính xác.

4.6 NGHIÊN CỨU LOẠI BỎ

Trong phần này, chúng tôi tiến hành một nghiên cứu loại bỏ để kiểm tra tác động của các thành phần riêng lẻ trong phương pháp của chúng tôi. Tất cả các thí nghiệm tiếp theo tập trung vào các tác vụ MRPC và RTE và sử dụng mô hình RoBERTa large. Chúng tôi tuân thủ các siêu tham số được sử dụng trong các thí nghiệm trước đây, chỉ sửa đổi thành phần đang được điều tra cho mỗi bài kiểm tra. Mỗi thí nghiệm được chạy với 5 hạt giống ngẫu nhiên, và chúng tôi báo cáo trung bình và độ lệch chuẩn của kết quả.

Vector Tỷ lệ Đơn Trước tiên, chúng tôi điều tra tính cần thiết của cả hai vector tỷ lệ d và b trong phương pháp của chúng tôi. Chúng tôi tạo ra hai thiết lập loại bỏ: một thiết lập loại trừ d (được gọi là onlyb) và một thiết lập khác bỏ qua b (được gọi là onlyd). Trong thiết lập onlyd, d được khởi tạo bằng số không. Như được hiển thị trong Bảng 6, việc bỏ qua một trong hai vector tỷ lệ làm giảm hiệu suất. Cấu hình onlyd hoạt động tốt hơn một chút so với đối tác onlyb của nó. Sự khác biệt trong hiệu suất này nhấn mạnh tính biểu đạt cao hơn của

Bảng 6: Kết quả nghiên cứu loại bỏ cho tác động của các vector tỷ lệ d và b và các chiến lược khởi tạo khác nhau. Cài đặt mặc định của chúng tôi được làm nổi bật bằng màu xanh.

[Bảng kết quả chi tiết các thí nghiệm loại bỏ]

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 7: Kết quả cho các tác vụ GLUE được chọn sử dụng ma trận ngẫu nhiên chia sẻ và duy nhất.

[Bảng so sánh ma trận ngẫu nhiên chia sẻ và duy nhất]

vector tỷ lệ d so với vector b. Cụ thể, d điều chỉnh các hàng của cả hai ma trận hạng thấp, do đó ảnh hưởng đến một khía cạnh rộng hơn của ma trận được xây dựng cuối cùng. Ngược lại, b chỉ tỷ lệ các hàng của ma trận cuối cùng kết quả từ tích của các ma trận hạng thấp.

Khởi tạo Ma trận Chia sẻ Chúng tôi kiểm tra ba sơ đồ khởi tạo khác nhau cho các ma trận chia sẻ: Kaiming normal, Kaiming uniform, và khởi tạo đều trong phạm vi [0,0.1]. Theo kết quả trong Bảng 6, cả hai khởi tạo Kaiming đều vượt trội hơn khởi tạo phạm vi đều, với biến thể đều có kết quả tốt hơn một chút so với biến thể normal.

Khởi tạo Vector Tỷ lệ Chúng tôi khám phá thêm tác động của các giá trị khởi tạo cho vector d. Các thí nghiệm được tiến hành với dinit được đặt ở 1.0, 10−1, và 10−7. Kết quả trong Bảng 6 cho thấy việc chọn dinit ảnh hưởng đáng kể đến hiệu suất của phương pháp; trong các cài đặt chúng tôi kiểm tra, các giá trị 10−1 và 10−7 vượt trội hơn 1.0, có thể cung cấp tính linh hoạt hơn trong quá trình tối ưu hóa thông qua việc thay đổi dấu sớm trong các hàng được chọn của các ma trận cố định.

Độ lớn của Thích ứng Trong Hình 3, chúng tôi cung cấp một hình ảnh hóa độ lớn của những thay đổi của các vector d sau khi tinh chỉnh trên tác vụ RTE. Vì các ma trận hạng thấp cố định vẫn giống nhau cho mỗi lớp, chúng tôi có thể so sánh trực tiếp độ dài của vector d qua các lớp để tính toán thích ứng tương đối của nó. Nhìn chung, chúng tôi thấy rằng thích ứng lớn nhất xảy ra cho các ma trận truy vấn so với các ma trận giá trị, cho thấy nhu cầu lớn hơn hoặc dễ dàng hơn để tinh chỉnh mô hình ở đó. Hơn nữa, tương tự như kết quả của các phương pháp thích ứng hiệu quả trước đây (Zhang et al., 2023b; Liu et al., 2021), chúng tôi cũng quan sát thấy thích ứng cao hơn cho các lớp sau so với các lớp trước.

Chia sẻ Ma trận Ngẫu nhiên Chúng tôi tiến hành các thí nghiệm trên các tác vụ RTE, MRPC, CoLA, và STS-B để đánh giá tác động của việc chia sẻ ma trận ngẫu nhiên đối với hiệu suất. Chúng tôi đánh giá hai thiết lập - một với ma trận ngẫu nhiên được chia sẻ trên tất cả các lớp được thích ứng, và một thiết lập khác với các ma trận được tạo ra độc đáo. Kết quả trong Bảng 7 cho thấy hiệu suất trung bình giống hệt nhau trong trường hợp các tác vụ RTE và STS-B, và có một cải thiện nhẹ cho MRPC và CoLA khi sử dụng ma trận độc đáo.

5 KẾT LUẬN

Trong công trình này, chúng tôi giới thiệu một phương pháp tinh chỉnh giảm đáng kể số lượng tham số có thể huấn luyện so với LoRA, mang lại kết quả tương tự hoặc tốt hơn trên các tác vụ downstream. Cụ thể, nó đạt được giảm mười lần tham số mang lại cùng hiệu suất trên benchmark GLUE cho RoBERTa large, giảm mười lần trên các tác vụ phân loại hình ảnh, và giảm ba lần trên benchmark E2E. Phương pháp này đặc biệt phù hợp cho các tình huống yêu cầu hoán đổi thường xuyên của nhiều mô hình được tinh chỉnh, chẳng hạn như các dịch vụ AI dựa trên đám mây được cá nhân hóa cho từng người dùng.

Do kích thước tối thiểu của các vector tỷ lệ, nhiều phiên bản có thể cư trú trong bộ nhớ hạn chế của một GPU duy nhất, do đó cải thiện đáng kể hiệu quả phục vụ và loại bỏ nút thắt cổ chai của việc tải các mô hình cụ thể vào bộ nhớ.

Trong khi nghiên cứu hiện tại tập trung vào các mô hình ngôn ngữ và thị giác với kiến trúc Transformer, khả năng áp dụng của phương pháp trên các kiến trúc và lĩnh vực khác nhau vẫn là một lĩnh vực cho nghiên cứu tương lai. Hơn nữa, hiệu suất của phương pháp có thể được hưởng lợi từ các cải tiến bổ sung, chẳng hạn như phân bổ ngân sách tham số động, hoặc các kỹ thuật khởi tạo và chính quy hóa khác nhau.

LỜI CẢM ơN

Công trình này được hỗ trợ tài chính bởi Qualcomm Technologies Inc., Đại học Amsterdam và khoản trợ cấp Top consortia for Knowledge and Innovation (TKIs) từ Bộ Kinh tế và Chính sách Khí hậu Hà Lan. Chúng tôi cũng ghi nhận việc sử dụng National Supercomputer Snellius và Distributed ASCI Supercomputer 6 (Bal et al., 2016) cho các nhiệm vụ tính toán thiết yếu.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt với các tên riêng và thuật ngữ chuyên môn được giữ nguyên]

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

A SIÊU THAM SỐ

Bảng 8: Cấu hình siêu tham số cho các kích thước mô hình khác nhau trên benchmark GLUE. Optimizer, Warmup Ratio, và LR Schedule được lấy từ Hu et al. (2022)

[Bảng cấu hình siêu tham số chi tiết]

Trong Bảng 8, chúng tôi cung cấp các siêu tham số được sử dụng cho benchmark GLUE trong bài báo chính. Lưu ý rằng do tài nguyên tính toán học thuật của chúng tôi, chúng tôi không thể chạy tìm kiếm lưới đầy đủ trên bất kỳ siêu tham số nào. Chúng tôi chỉ đánh giá các tốc độ học và số lượng epoch khác nhau và thậm chí dựa vào các cấu hình hiện có của LoRA (Optimizer, tỷ lệ warmup, lịch trình LR).

Bảng 9: Cấu hình siêu tham số cho tinh chỉnh theo hướng dẫn.

[Bảng cấu hình siêu tham số cho tinh chỉnh theo hướng dẫn]

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 10: Cấu hình siêu tham số cho VeRA trên benchmark E2E, cho các mô hình GPT2 Medium và Large.

[Bảng cấu hình siêu tham số cho E2E benchmark]

Bảng 11: Cấu hình siêu tham số cho VeRA và LoRA để tinh chỉnh ViT trên các bộ dữ liệu phân loại hình ảnh. Các phương pháp Full, LoRA và VeRA có hai tốc độ học - một cho đầu phân loại, và một cho phần còn lại.

[Bảng cấu hình siêu tham số cho ViT]

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

B TĂNG HIỆU SUẤT TƯƠNG ĐỐI.

[Hình 4: Tăng hiệu suất trên 1K tham số có thể huấn luyện trên tác vụ RTE cho mô hình RoBERTa large so với baseline. Công thức: (accuracymethod/accuracybaseline)/parametersmethod∗100]

Hình 4 định lượng hiệu quả của mỗi phương pháp về tăng hiệu suất trên 1K tham số có thể huấn luyện. Để so sánh tập trung, chúng tôi chọn tác vụ RTE và mô hình RoBERTa large.

Để thiết lập một baseline, chúng tôi tiến hành các thí nghiệm phụ trợ trong đó chỉ đầu phân loại được huấn luyện trong khi phần còn lại của mô hình được cố định. Baseline này được xây dựng sử dụng các siêu tham số giống như trong phương pháp VeRA của chúng tôi. Sau đó, chúng tôi đánh giá tăng hiệu suất được gán cho mỗi phương pháp, được chuẩn hóa bởi các tham số có thể huấn luyện bổ sung được giới thiệu, so với baseline.

Kết quả cho thấy rõ ràng rằng VeRA mang lại tăng hiệu suất cao nhất trên 1K tham số có thể huấn luyện.

C TÁC ĐỘNG ĐẾN THỜI GIAN HUẤN LUYỆN VÀ SỬ DỤNG BỘ NHỚ

Để đánh giá thời gian huấn luyện và lợi ích bộ nhớ GPU của phương pháp chúng tôi, chúng tôi đã tiến hành so sánh giữa LoRA và VeRA trong khi tinh chỉnh LLaMA 7B với cùng hạng (64) trên bộ dữ liệu tinh chỉnh theo hướng dẫn, được giới thiệu trước đó trong công trình này. Kết quả được tóm tắt trong Bảng 12:

Bảng 12: Tác động đến sử dụng bộ nhớ GPU và thời gian huấn luyện.

[Bảng so sánh thời gian huấn luyện và bộ nhớ GPU]

Trong khi VeRA bao gồm nhiều phép toán hơn LoRA vì các phép nhân vector bổ sung trong lượt truyền tiến, chúng tôi thấy rằng nó chỉ dẫn đến tăng 1.8% khiêm tốn trong thời gian huấn luyện. Đối với bộ nhớ GPU, chúng tôi quan sát giảm 7.4% trong sử dụng bộ nhớ với VeRA, vì nó không yêu cầu lưu trữ trạng thái tối ưu hóa và gradient cho các ma trận ngẫu nhiên chia sẻ.

D TƯƠNG ĐỒNG CỦA TRỌNG SỐ ĐƯỢC HUẤN LUYỆN

Chúng tôi so sánh các trọng số được huấn luyện với LoRA và VeRA ở một hạng duy nhất là 64 trên tất cả các lớp truy vấn. Đối với mỗi phương pháp và lớp được thích ứng, chúng tôi xây dựng một sự khác biệt trọng số. Trong trường hợp của LoRA, điều này bao gồm phép nhân của hai ma trận hạng thấp, trong khi đối với VeRA, nó cũng bao gồm phép nhân bởi các vector tỷ lệ. Sau đó, chúng tôi tính toán tương đồng cosine của các trọng số được làm phẳng này. Ngoài ra, chúng tôi so sánh tương đồng giữa các trọng số LoRA được huấn luyện và các ma trận được khởi tạo ngẫu nhiên như một baseline: Chúng tôi thấy rằng tương đồng của VeRA với LoRA trung bình là 2e-3 trong khi LoRA với ma trận ngẫu nhiên là -8e-5.

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình 5: Tương đồng cosine của trọng số LoRA, VeRA, và ngẫu nhiên qua các lớp.]

Trong Hình 5, chúng tôi có thể thấy sự gia tăng đáng kể trong tương đồng giữa các trọng số được huấn luyện, đặc biệt là trong các lớp sau. Quan sát này phù hợp với kết quả trước đây của chúng tôi (Hình 3) rằng thích ứng cao nhất xảy ra trong các lớp này. Những kết quả này hỗ trợ ý tưởng rằng VeRA có thể xấp xỉ các trọng số được huấn luyện với LoRA.

E TÍNH BIỂU ĐẠT CỦA VERA

Chúng tôi đã tiến hành một thí nghiệm về tính biểu đạt của LoRA và VeRA trên tác vụ khớp các ma trận vuông 10x10 ngẫu nhiên, với kết quả được thấy trong Hình 6. Đối với số lượng tham số có thể huấn luyện nhất định, cả hai phương pháp đều hoạt động tốt như nhau, với VeRA cung cấp tính linh hoạt hơn, ví dụ bằng cách cho phép tham số hóa thấp hơn nhiều - dưới hạng 1 của LoRA.

[Hình 6: Mất mát MSE trên tác vụ khớp ma trận 10x10 ngẫu nhiên. LoRA (PQ) và VeRA (XdYb) hoạt động tốt như nhau đối với số lượng tham số có thể huấn luyện nhất định.]

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

F TINH CHỈNH THEO HƯỚNG DẪN VỚI VICUNA EVAL

Kết quả và mẫu từ đánh giá mô hình Llama 7B được tinh chỉnh theo hướng dẫn với Vicuna Eval (Chiang et al., 2023), tiền thân của MT-Bench. Mô hình đã được tinh chỉnh trên một tập con 10K của bộ dữ liệu Alpaca đã được làm sạch.

Bảng 13: Điểm số trung bình được gán bởi GPT-4 cho các câu trả lời được tạo ra bởi các mô hình được tinh chỉnh với các phương pháp VeRA và LoRA. Số lượng tham số có thể huấn luyện và bộ nhớ lý thuyết cần thiết để lưu trữ chúng ở độ chính xác đơn cũng được hiển thị.

[Bảng kết quả Vicuna Eval]

[Các ví dụ so sánh chi tiết giữa LoRA và VeRA trên các câu hỏi cụ thể]

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục các ví dụ so sánh]

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục các ví dụ so sánh]

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Tiếp tục các ví dụ so sánh]
