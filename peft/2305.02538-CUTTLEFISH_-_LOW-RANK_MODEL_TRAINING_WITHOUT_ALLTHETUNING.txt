# 2305.02538.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.02538.pdf
# File size: 3001830 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CUTTLEFISH : LOW-RANK MODEL TRAINING WITHOUT ALLTHETUNING
Hongyi Wang1Saurabh Agarwal2Pongsakorn U-chupala3Yoshiki Tanaka3
Eric P. Xing4 1 5Dimitris Papailiopoulos6
ABSTRACT
Recent research has shown that training low-rank neural networks can effectively reduce the total number of
trainable parameters without sacriﬁcing predictive accuracy, resulting in end-to-end speedups. However, low-rank
model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the
factorization at each layer. In this paper, we tackle this challenge by introducing CUTTLEFISH , an automated
low-rank training approach that eliminates the need for tuning factorization hyperparameters. CUTTLEFISH
leverages the observation that after a few epochs of full-rank training, the stable rank (i.e.,an approximation of the
true rank) of each layer stabilizes at a constant value. CUTTLEFISH switches from full-rank to low-rank training
once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding
stable rank. Our results show that CUTTLEFISH generates models up to 5.6 smaller than full-rank models,
and attains up to a 1.2 faster end-to-end training process while preserving comparable accuracy. Moreover,
CUTTLEFISH outperforms state-of-the-art low-rank model training methods and other prominent baselines. The
source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish .
1 I NTRODUCTION
As neural network-based models have experienced expo-
nential growth in the number of parameters, ranging from
23 million in ResNet-50 (2015) to 175 billion in GPT-3
(2020) and OPT-175B (2022) (Devlin et al., 2018; Brown
et al., 2020; Fedus et al., 2021; Zhang et al., 2022), training
these models has become increasingly challenging, even
with the assistance of state-of-the-art accelerators like GPUs
and TPUs. This problem is particularly pronounced in
resource-limited settings, such as cross-device federated
learning (Kairouz et al., 2019; Wang et al., 2020b; 2021b).
In response to this challenge, researchers have explored the
reduction of trainable parameters during the early stages of
training (Frankle & Carbin, 2018; Waleffe & Rekatsinas,
2020; Khodak et al., 2020; Wang et al., 2021a) as a strategy
for speeding up the training process.
Previous research efforts have focused on developing several
approaches to reduce the number of trainable parameters
during training. One method involves designing compact
neural architectures, such as MobileNets (Howard et al.,
1Machine Learning Department, Carnegie Mellon Uni-
versity2Department of Computer Sciences, University of
Wisconsin-Madison3Sony Group Corporation4Mohamed bin
Zayed University of Artiﬁcial Intelligence5Petuum, Inc.
6Department of Electrical and Computer Engineering, Univer-
sity of Wisconsin-Madison. Correspondence to: Hongyi Wang
<hongyiwa@andrew.cmu.edu >.
Proceedings of the 6thMLSys Conference , Miami Beach, FL,
USA, 2023. Copyright 2023 by the author(s).2017) and EfﬁcientNets (Tan & Le, 2019), which demand
fewer FLOPs. However, this may potentially compromise
model performance. Another alternative is weight prun-
ing, which reduces the number of parameters in neural net-
works (Han et al., 2015a;b; Frankle & Carbin, 2018; Renda
et al., 2020; Sreenivasan et al., 2022b). While unstruc-
tured sparsity pruning methods can result in low hardware
resource utilization, recent advancements have proposed
structured pruning based on low-rank weight matrices to
tackle this issue (Waleffe & Rekatsinas, 2020; Khodak et al.,
2020; Wang et al., 2021a; Hu et al., 2021; Chen et al., 2021b;
V odrahalli et al., 2022). However, training low-rank models
necessitates tuning additional hyperparameters for factoriza-
tion, such as the width/rank of the factorization per layer, in
order to achieve both compact model sizes, as measured by
the number of parameters, and high accuracy.
Striking the right balance between the size of a low-rank
model and its accuracy is crucial, and depends on accurately
tuning the rank of the factorized layers. As demonstrated
in Figure 1, improper tuning of factorization ranks can lead
to either large models or diminished predictive accuracy.
Training low-rank networks from scratch may cause signiﬁ-
cant accuracy loss (Waleffe & Rekatsinas, 2020; Wang et al.,
2021a). To address this, previous studies have suggested
starting with full-rank model training for a speciﬁc num-
ber of epochs, E, before transitioning to low-rank model
training (Waleffe & Rekatsinas, 2020; Wang et al., 2021a).
However, varying the number of full-rank training epochs
can inﬂuence the ﬁnal model accuracy, as illustrated in Fig-arXiv:2305.02538v2  [cs.LG]  5 May 2023

--- PAGE 2 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
ure 1. Thus, selecting the appropriate number of full-rank
training epochs, E, is essential ( e.g., neitherE= 0 nor
E= 120 yield the optimal model accuracy). Furthermore,
to attain satisfactory accuracy, some earlier work (Wang
et al., 2021a) has proposed excluding the factorization from
the ﬁrstKlayers, resulting in a “hybrid network” that bal-
ances model size and accuracy through the choice of K.
0.2 0.4 0.6 0.8 1.0
Model Parameters 1e7909192939495Validation Accuracy
(E=0, K=1)
(E=40, K=1)
(E=80, K=1)
(E=120, K=1)Pufferfish
Vanilla
Cuttlefish
(ours)
0.2 0.4 0.6 0.8 1.0
Model Parameters 1e792.092.593.093.594.094.595.0Validation Accuracy
(E=80, K=1)
(E=80, K=5)
(E=80, K=7)
(E=80, K=9)Pufferfish
Vanilla
Cuttlefish
(ours)
Figure 1. Comparison between CUTTLEFISH and grid search tun-
ing results: ( top): ﬁxingK= 1(the very ﬁrst convolution layer
is always not factorized) and varying E2f0;40;80;120gand
varying the selection of Rby choosing various ﬁxed rank ratios.
(bottom ): ﬁxing a good choice of E,e.g.,E= 80 and varyingK
and the rank ratio. The rank ratio varies among f1
32;1
16;1
8;1
4;1
2g.
Experiments ran on ResNet-18 trained over CIFAR-10.
In this paper, we introduce a novel method for automati-
cally determining the hyperparameters associated with low-
rank training, ensuring that the resulting factorized model
achieves both a compact size and high ﬁnal accuracy.
Challenges. We would like to emphasize several reasons
why this problem presents considerable challenges. Firstly,
the search spaceSis vast. For a two hidden layer fully
connected (FC) neural network with 100neurons in each
layer (assuming the rank for each layer is 100) and train-
ing with 100epochs, the cardinality of the search space
isjSj= 1001001002 = 2106. Furthermore,
our objective of automatically optimizing low-rank train-
ing factorization hyperparameters while maintaining the
advantages of end-to-end training speedups renders tradi-
tional neural architecture search (NAS) methods impractical.
NAS necessitates concurrent training of both network ar-
chitecture and network weights, resulting in computational
requirements that substantially exceed those of standard
model training.
In this work, we present CUTTLEFISH , an automated low-
rank factorized training method that eliminates the need for
tuning factorization hyperparameters. We observe a key
pattern in which the estimated rank of each layer changesrapidly during the initial stages of training and then sta-
bilizes around a constant value (as depicted in Figure 2).
We exploit this observation to develop a simple heuristic
for selecting the layer ranks Rand the full-rank training
durationE: (i) transition from full-rank model training to
low-rank model training when all the layer’s stable ranks
have converged to a constant, and (ii) use these constants as
the rank of the factorization.
0.0 2.5 5.0 7.5 10.0 12.5
% of the T otal Training Epochs0.00.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
Figure 2. The estimated ranks for various layers in ResNet-18
trained on CIFAR-10, using stable rank (which will be discussed
in detail later), can be found in our results. The results for other
tasks are available in the appendix.
In addition to determining the factorization ranks and full-
rank training duration, CUTTLEFISH also addresses the is-
sue of deciding which layers to factorize. For convolutional
neural networks (CNNs), CUTTLEFISH observes that factor-
izing early layers does not lead to considerable speedups, as
elaborated in Section 3.5. This observation, along with in-
sights from prior research (Wang et al., 2021a), implies that
factorizing early layers may negatively affect the ﬁnal model
accuracy without offering signiﬁcant performance gains. To
tackle this challenge, CUTTLEFISH performs lightweight
proﬁling to identify the layers to factorize, ensuring that fac-
torization occurs only in layers that can effectively enhance
the training speed.
Our contributions. We observe a stabilizing effect in the
stable ranks of neural network (NN) layers during training,
where their stable ranks initially change rapidly and then
converge to a constant. Based on this observation, we de-
vise a heuristic to adaptively select the rank of each layer
and the duration of full-rank warm-up training. We im-
plement our technique, called CUTTLEFISH , and assess it
in large-scale training environments for both language and
computer vision tasks. Our comprehensive experimental
results demonstrate that CUTTLEFISH automatically selects
all factorization hyperparameters during training on-the-ﬂy,
eliminating the need for multiple experimental trials for fac-
torization hyperparameter tuning. CUTTLEFISH strikes a
balance between model size and ﬁnal predictive accuracy,
excelling in at least one dimension of producing smaller,
more accurate models and achieving considerable training
speedups compared to state-of-the-art low-rank training,
structured pruning, sparse training, quantized training, and
learnable factorization methods (Rastegari et al., 2016; Fran-
kle et al., 2019; Wang et al., 2020a; Yu & Huang, 2019; You

--- PAGE 3 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
et al., 2019; Idelbayev & Carreira-Perpin ´an, 2020; Khodak
et al., 2020; Wang et al., 2021a).
1.1 Related work.
Several methods have been developed in the literature to
eliminate redundancy in the parameters of modern NNs.
Model compression strives to eliminate redundancy in the
parameters of trained NNs (Han et al., 2015a). Over time,
numerous methods have been devised to remove redundant
weights in NNs, encompassing pruning (Li et al., 2016; Wen
et al., 2016; Hu et al., 2016; Zhu & Gupta, 2017; He et al.,
2017; Yang et al., 2017; Liu et al., 2018; Yu et al., 2018;
2019; Sreenivasan et al., 2022a), quantization (Rastegari
et al., 2016; Zhu et al., 2016; Hubara et al., 2016; Wu et al.,
2016; Hubara et al., 2017; Zhou et al., 2017), low-rank fac-
torization (Xue et al., 2013; Sainath et al., 2013; Jaderberg
et al., 2014; Wiesler et al., 2014; Kone ˇcn`y et al., 2016), and
knowledge distillation (Hinton et al., 2015; Yu et al., 2019;
Sanh et al., 2019; Jiao et al., 2020; Li et al., 2023).
TheLottery Ticket Hypothesis (LTH) suggests that smaller,
randomly initialized subnetworks can be trained to attain
accuracy levels comparable to those of the full network, al-
though pinpointing these subnetworks can be computation-
ally challenging (Frankle & Carbin, 2018). Iterative Mag-
nitude Pruning (IMP) was devised to stabilize LTH while
reducing computational costs through warm-up steps (Fran-
kle et al., 2019). Other efforts have sought to eliminate
the need for model weight rewinding (Renda et al., 2020)
and to identify winning tickets at initialization (Wang et al.,
2020a; Sreenivasan et al., 2022b). Moreover, researchers
have explored sparsifying NNs during training (Evci et al.,
2020). However, these sparsiﬁcation methods focus on un-
structured sparsity, which does not yield actual speedups on
current hardware. In contrast, low-rank training can lead to
tangible acceleration.
Low-rank factorized training, as well as other structured
pruning methods, aim to achieve NNs with structured spar-
sity during training, allowing for tangible speedups to be
obtained (Waleffe & Rekatsinas, 2020; Khodak et al., 2020;
You et al., 2020; Wang et al., 2021a; Chen et al., 2021b).
Low-rank factorized training has also been employed in
federated learning methods to improve communication efﬁ-
ciency and hardware heterogeneity awareness (Hyeon-Woo
et al., 2022; Yao et al., 2021). Low-rank factorization tech-
niques have been shown to be combinable and used for train-
ing low-rank networks from scratch (Ioannou et al., 2015).
However, this method results in a noticeable loss of accu-
racy, as demonstrated in (Wang et al., 2021a). To tackle this
problem, (Khodak et al., 2020) introduces spectral initial-
ization and Frobenius decay, while (V odrahalli et al., 2022)
proposes the Nonlinear Kernel Projection method as an
alternative to SVD. Low-rank training has also been inves-
tigated for ﬁne-tuning large-scale pre-trained models (Huet al., 2021). These techniques all necessitate additional
hyperparameters, which can be tedious to ﬁne-tune. The
LC compression method attempts to resolve this issue by
explicitly learning Rduring model training through alter-
nating optimization (Idelbayev & Carreira-Perpin ´an, 2020).
However, this approach is computationally demanding. Our
proposed CUTTLEFISH method automatically determines
all factorization hyperparameters during training on-the-ﬂy,
eliminating the heavy computation overhead and the need
for multiple experimental trials for factorization hyperpa-
rameter tuning.
Alternative transformations have also been investigated,
including Butterﬂy matrices (Chen et al., 2022), the fu-
sion of low-rank factorization and sparsiﬁcation (Chen
et al., 2021a), and block-diagonal matrices (Dao et al.,
2022). Furthermore, novel architectures have been devel-
oped for enhanced training or inference efﬁciency, such
as SqueezeNet (Iandola et al., 2016), Eyeriss (Chen et al.,
2016), ShufﬂeNet (Zhang et al., 2018), EfﬁcientNet (Tan
& Le, 2019), MobileNets (Howard et al., 2017), Xcep-
tion (Chollet, 2017), ALBERT (Lan et al., 2019), and Re-
former (Kitaev et al., 2020).
2 P RELIMINARY
In this section, we present an overview of the core con-
cepts of low-rank factorization for various NN layers, along
with a selection of specialized training methods speciﬁcally
designed for low-rank factorized training.
2.1 Low-rank factorization of NN layers
FC/MLP Mixer layer. A 2-layer fully connected
(FC) neural network can be represented as f(x) =
((xW1)W2), where Ws are weight matrices, ()is
an arbitrary activation function, and xis the input data point.
The weight matrix Wcan be factorized as UV>. A simi-
lar approach can be applied to ResMLP/MLP mixer layers,
where each learnable weight can be factorized in the same
manner (Touvron et al., 2021a; Tolstikhin et al., 2021).
Convolution layer. For a convolutional layer with dimen-
sions (m;n;k;k ), wheremandnare the number of input
and output channels and krepresents the size of a convo-
lution ﬁlter, a common approach involves factorizing the
unrolled 2D matrix. We will discuss a popular method for
factorizing a convolutional layer. Initially, the 4D tensor W
is unrolled to obtain a 2D matrix of shape (mk2;n), where
each column represents the weight of a vectorized convo-
lution ﬁlter. The rank of the unrolled matrix is determined
byminfmk2;ng. Factorizing the unrolled matrix results
inU2Rmk2randV>2Rrn. Reshaping the factor-
izedU;V>matrices back to 4D yields U2Rmrkk
andV>2Rrn. Consequently, factorizing a convolutional
layer produces a thinner convolutional layer Uwithrconvo-
lution ﬁlters and a linear projection layer V>. TheV>s can
also be represented by a 11convolutional layer, such as

--- PAGE 4 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
V>2Rrn11, which is more suited for computer vision
tasks since it operates directly in the spatial domain (Lin
et al., 2013; Wang et al., 2021a).
Multi-head attention (MHA) layer. Ap-head attention
layer learns pattention mechanisms on the key, value, and
query ( K;V;Q) of each input token:
MHA(Q;K;V) =Concat (head 1;:::; head p)WO:
Each head performs the computation of:
head i=Attention (QW(i)
Q;KW(i)
K;VW(i)
V)
=softmax 
QW(i)
QW(i)>
KK>
p
d=p!
VW(i)
V:
wheredis the hidden dimension. The trainable weights
W(i)
Q;W(i)
K;W(i)
V;i2f1;2;:::;pgcan be factorized by
simply decomposing all learnable weights Win an atten-
tion layer and obtaining UV>(Vaswani et al., 2017).
2.2 Training methods for low-rank networks
Hybrid NN architecture. It has been noted that factor-
izing the initial layers may negatively impact a model’s
accuracy (Kone ˇcn`y et al., 2016; Waleffe & Rekatsinas,
2020; Wang et al., 2021a). One possible explanation is
that NN layers can be viewed as feature extractors, and poor
features extracted by the early layers can accumulate and
propagate throughout the NN. To address this issue, the
hybrid NN architecture was proposed, which only factor-
izes the lower layers while keeping the initial layers full-
rank (Wang et al., 2021a). The weights of a full-rank L-layer
NN can be represented as W=fWij1iLg. The
corresponding hybrid model’s weights can be represented
asH=fW1;W2;:::;WK;UK+1;V>
K+1;:::;UL 1;
V>
L 1;WLg, whereKis the number of layers that are not
factorized and is treated as a hyperparameter to be tuned
manually (Wang et al., 2021a). It is important to note that
the last classiﬁcation layer, i.e.,WL, is usually not factor-
ized (Khodak et al., 2020; Wang et al., 2021a).
Full-rank to low-rank training. Training low-rank fac-
torized models from scratch often results in a decrease in
accuracy (Waleffe & Rekatsinas, 2020; Khodak et al., 2020;
Wang et al., 2021a). To mitigate this drop, it is common to
train the full-rank model for Eepochs before factorizing
it (Waleffe & Rekatsinas, 2020; Khodak et al., 2020; Wang
et al., 2021a). However, determining the appropriate number
of full-rank training epochs is treated as a hyperparameter
and typically tuned manually in experiments (Wang et al.,
2021a). Observations indicate that ﬁnding the right number
of full-rank training epochs is crucial for achieving optimal
ﬁnal model accuracy in low-rank factorized NNs.
Initialization and weight decay. Factorized low-rank
networks can beneﬁt from tailored initialization meth-
ods (Ioannou et al., 2015; Khodak et al., 2020). One such
method, called spectral initialization , aims to approximate
the behavior of existing initialization methods (Khodaket al., 2020). Spectral initialization represents a special
case of transitioning from full-rank to low-rank training
withE= 0. Additionally, speciﬁc regularization techniques
have been proposed to enhance the accuracy of low-rank net-
works. For example, Frobenius decay applies weight decay
onkUV>k2
Finstead ofkUk2
F+kV>k2
Fduring factorized
low-rank training.
3 C UTTLEFISH : AUTOMATED LOW -RANK
FACTORIZED TRAINING
In this section, we outline the problem formulation of CUT-
TLEFISH , elaborate on each factorization hyperparameter,
and describe CUTTLEFISH ’s heuristics for determining all
of these factorization hyperparameters.
3.1 Problem formulation.
The search space for adaptive factorized tuning is deﬁned by
three sets of hyperparameters, namely S= (E;K;R)(full-
rank training epochs, the number of initial layers that remain
unfactorized, and layer factorization ranks). The objective
ofCUTTLEFISH is to ﬁnd an optimal ^s2S on-the-ﬂy, with
minimal computational overhead during training, such that
the resulting low-rank factorized models are both compact
and maintain high accuracy, comparable to their full-rank
counterparts.
3.2 Components in the search space and the
trade-offs among hyperparameter selections.
Full-rank training epochs E.The value of Ecan range
from 0toT 1. Neither too small ( e.g.,E= 0) nor too
large ( e.g.,E= 120 ) values ofEresult in the best accuracy
(Figure 1), highlighting the necessity of tuning E.
The number of full-rank layers K.As pre-
viously mentioned, the weights of a hybrid
NN architecture can be represented by H =
fW1;:::;WK;UK+1;V>
K+1;:::;UL 1;V>
L 1;WLg,
whereKis a hyperparameter to be tuned. The selection
ofKcan range from 1 to L 1, meaning the very ﬁrst
and very last layers are always not factorized. Factorizing
additional layers results in increased accuracy loss but
also reduces the model size and computational complexity.
Thus, an optimal choice for Kshould balance the trade-off
between accuracy loss and model compression rate.
Rank selections for factorized layers R.Rspeciﬁes the
ranks used when factorizing the (L K 1)layers in a
hybrid NN architecture, i.e.,R=frijK+ 1iL 1g.
For a layer weight Wi2Rmn, the full rank of the layer is
rank(Wi) = minfm;ng. Thus, 1rirank(Wi);8i2
fK+ 1;:::;L 1g. Using a too small rfor factorizing
a layer may result in a decrease in accuracy. However,
employing a relatively large rto factorize the layer could
negatively impact the model compression rate.
In this paper, we develop heuristics that automatically iden-
tify optimal choices for each component within the search

--- PAGE 5 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
space, i.e.,^s2S, in order to strike a balance between the
ﬁnal accuracy and model size.
Why is ﬁnding an appropriate s2 S challenging?
Firstly, the search space’s cardinality, jSj, is vast. Further-
more, the primary goal of low-rank factorized training is to
accelerate model training. Therefore, it is crucial to identify
^swithout introducing signiﬁcant computational overhead.
While Neural Architecture Search (NAS) style methods
could potentially be employed to search for s2S, they
result in high computational overhead. Consequently, adopt-
ing NAS-based algorithms is not suitable for our scenario,
as our objective is to achieve faster training.
3.3 Determining factorization ranks ( R) for NN
layers.
In previous work, ranks of the factorized layers have
typically been treated as a hyperparameter, with a ﬁxed
global rank ratio often employed, for example, R=
frank(Wi)j1iLg(Khodak et al., 2020; Wang
et al., 2021a). However, a crucial question to consider is do
all layers converge to the same during training?
Rank estimation metric. One might wonder why we can-
not simply use the normal rank for estimating the layer
ranks of NNs. The answer is that the normal rank is al-
ways full for layer weights of NNs. However, NN weight
matrices are “nearly” low-rank when they exhibit a rapid
spectral decay. Therefore, we require a metric to estimate
layer ranks. In CUTTLEFISH , we utilize the stable rank ,
which serves as a valuable proxy for the actual rank since
it remains largely unaffected by small singular values, to
estimate the rank of model layer weights W. The def-
inition of stable rank is stable rank () =1>21
2max(W),
where 1,2
max(), and represent the identity column vec-
tor, the maximum squared singular value, and the diagonal
matrix that stores all singular values in descending order,
i.e.,1>=1;:::; rank(W)
, respectively. Another ad-
vantage of using stable rank is that its calculation does not
require specifying any additional hyperparameters.
The scaled stable rank. Stable rank, which disregards
minuscule singular values, often results in very low rank
estimations. This can work well for relatively small tasks,
such as CIFAR-10. However, for larger scale tasks like
ImageNet, using stable rank directly leads to a non-trivial
accuracy drop of 2.3% (details can be found in the appendix).
To address this issue, we propose using scaled stable rank .
Scaled stable rank assumes that the estimated rank of a
randomly initialized matrix, i.e.,W0(model weight at the
0-th epoch), should be close or equal to full rank. Nev-
ertheless, based on our experimental observations, stable
rank estimation of randomly initialized weights tends not
to be full rank. Therefore, we store the ratio of full rank
to initial stable rank (denoted as ,e.g., ifrank(W) = 512
andstable rank (0) = 200 , then= 512=200). We
scale each epoch’s stable rank by:
scaled stable rank (;) =stable rank ();
=rank(W0)
stable rank (0);8t2f1;2;:::;Tg:CUTTLEFISH rank selection. We observe that different
layers tend to converge to varying stable ranks (an example
is shown in Figure 3, with similar trends found in other
tasks, as detailed in the appendix). Middle layers generally
converge to larger s, indicating greater redundancy. As a re-
sult, it is unlikely that a ﬁxed rank ratio is optimal, as it may
either fail to eliminate all redundancy in the layer weights
or be too aggressive in compressing model weights, thereby
compromising ﬁnal accuracy. CUTTLEFISH employs the
scaled stable rank at epoch E(i.e.,the transition point from
full-rank to low-rank) to factorize the full-rank model and
obtain a low-rank factorized model.
1591317212529333741454953576165697377818589
Epochs1
3
5
7
9
11
13
15
17Layer Index
 0.050.100.150.20
Figure 3. The rank ratios ( s) of stable ranks for ResNet-18 trained
on CIFAR-10, where darker colors indicate higher values (results
for other datasets can be found in the appendix).
3.4 Determining full-rank training epochs E
As discussed in Section 1, neither too small nor too large
Evalues result in optimal accuracy. Furthermore, larger E
values also lead to slower training time, as full-rank models
have higher computational complexity. CUTTLEFISH is in-
spired by the observation that the estimated ranks for all NN
layers,R, change rapidly during the early training phase
but stabilize in later training epochs. A reasonable heuristic,
therefore, is to switch from full-rank training to low-rank
training when the estimated ranks no longer vary signiﬁ-
cantly. The question now is, how can we determine if the
curves of the estimated ranks have stabilized? CUTTLEFISH
tracks the sequences of stable ranks for each layer at each
epoch, i.e.,%=fr0;r1;:::;rtg.CUTTLEFISH measures
the derivative of the estimated rank sequences for all layer
weights (d%l
dt) to detect when they cease to change signiﬁ-
cantly, using a condition:d%l
dt;8l2fK+1;:::;L 1g,
whereis a close-to-zero rank stabilization threshold.
3.5 Determining Kfor hybrid architectures
Kbalances the ﬁnal accuracy and model compression rate.
However, discerning the relationship between Kand ﬁ-
nal accuracy without fully training the model to conver-
gence is challenging and impractical for achieving faster
training speeds. For each task, CUTTLEFISH conducts
lightweight proﬁling to measure the runtime of the low-
rank NN when factorizing each layer stack, as layers within
the same stack have identical weights and input sizes,
and assesses whether it results in a signiﬁcant speedup.
CUTTLEFISH only performs factorization (with proﬁling
rank ratio candidates: ) if it leads to meaningful accel-
eration (determined by a threshold ). For example, if

--- PAGE 6 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
full-rank time >1:5factorized time for= 1:5when
=1
4, then CUTTLEFISH proceeds with factorization. Fig-
ure 4 illustrates one example benchmark, where factorizing
the ﬁrst convolution stack ( i.e.,layer 2to layer 5) does not
yield a substantial speedup. Consequently, CUTTLEFISH
does not factorize these layers and returns ^K= 6.
1.1x1.7x1.9x2.6x
Figure 4. The per-iteration forward time in milliseconds, bench-
marked using ResNet-18 on CIFAR-10 with a batch size of 1,024
on an EC2 p3.2xlarge instance.
Why does not factorizing the initial layers result in a sig-
niﬁcant speedup? The reason for this can be attributed to
the concept of arithmetic intensity , which is deﬁned as the
ratio of FLOPS to the bytes of data that must be accessed for
a speciﬁc computation (Jeffers et al., 2016). When a certain
layer has low arithmetic intensity, the GPU cannot operate
at peak performance, and therefore, even if the FLOPs are
substantially reduced, the actual speedup will not be sig-
niﬁcant. For a convolution layer, its arithmetic intensity is
proportional toO(Bmnk2HW
mnk2+BmHW)whereB;H;W stand for
the batch size, height, and width of the input image. In con-
volution networks, it is generally assumed that the initial lay-
ers have small mnbut largeHW , leading toBmHW
mnk2andO(Bmnk2HW
mnk2+BmHW)!O (nk2). For later lay-
ers, whereH;W are small and mnk2are large, and thus
mnk2BmHW ,O(Bmnk2HW
mnk2+BmHW)!O (BHW ). In
the example shown in Figure 4, nk2= 649 = 576
for the ﬁrst layer stack, while for the last layer stack,
BHW = 102488 = 65;536576. Consequently,
the bottom layers exhibit much higher arithmetic intensity,
and as a result, factorization leads to signiﬁcant speed im-
provements. For Transformers, where each layer has identi-
cal weight and input sizes ( i.e.,the same arithmetic inten-
sity), we consistently factorize all Transformer layers except
for the word/image sequence embedding layers.
3.6 Putting things together
The main algorithm of CUTTLEFISH is outlined in Algo-
rithm 1. CUTTLEFISH begins with proﬁling to determine
^K. Following this, the training method commences with
full-rank training until the stable ranks for the layers to be
factorized converge, i.e.,at epoch ^E. Subsequently, CUT-
TLEFISH factorizes the partially trained full-rank network
using the converged scaled stable ranks Rto obtain the
factorized low-rank model. Finally, the low-rank model istrained until it reaches full convergence. In our experiments,
we set= 0:1and= 1:5.
Algorithm 1 CUTTLEFISH
Input: The datasetD, initial full-rank neural network weights
W0=fW0
1;:::;W0
Lg, the training algorithm A(),
such as SGD, Adam, etc., the total number of epochs
T, and a rank stabilization threshold .
Output: The trained low-rank factorized NN.
Initialize the following: ^E=T;H=fg;%K+1;:::;% L 1=
fg;:::;fg;^K=Proﬁling (D;W;;)(Algorithm 2), l=
rank(W0
l)=stable rank (0
l);8l2f1;:::;Lg.
fort2f0;1;2;:::;T 1gdo
ift^Ethen
Wt+1 A(Wt;D)
forWl2Wtdo
ifK+ 1l<L then
~Ull~V>
l=SVD(Wl);
rl=stable rank (l),%l=%l[frlg;
end
ifd%l
dx;8l2fK+ 1;:::;L 1gthen
^E=t+ 1;
else ift=^E+ 1then
forWl2Wtdo
ifK+ 1l<L then
~Ull~V>
l=SVD(Wl);
rl=scaled stable rank (l;l);
Ul=~Ul1
2
l,V>
l=1
2
l~V>
l;
H=H[f Ul[:;1 :rl];V>
l[1 :rl;:]g(with
necessary NN weights reshaping);
else
H=H[fWlg;
end
end
Ht=H;Ht+1 A(Ht;D);
else
Ht+1 A(Ht;D);
end
end
Algorithm 2 Proﬁling
Input: The datasetD, full-rank model weights W, the proﬁling
iterations, and a proﬁling rank ratio candidates: .
Output: Determined ^K.
init timer()
forlayer range ( lbeg,lend)2layer stacks do
H=factorize layer stack (W;;lbeg,lend);
start time =timer.tic() ;
foriter2f1;2;:::;gdo
TrainHfor one iteration
end
endtime =timer.toc() ;
avglow-rank time = (endtime-start time)/;
start time =timer.tic() ;
foriter2f1;2;:::;gdo
TrainWfor one iteration;
end
endtime =timer.toc() ;
avgfullrank time = (endtime-start time)/;
iffullrank time>avglow-rank time then
^K=lend
end

--- PAGE 7 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
4 E XPERIMENTS
We have developed an efﬁcient implementation of CUT-
TLEFISH and conducted extensive experiments to evaluate
its performance across various vision and natural language
processing tasks. Our study focuses on the following as-
pects: (i) the sizes of factorized models CUTTLEFISH dis-
covers and their ﬁnal accuracy; (ii) the end-to-end training
speedups that CUTTLEFISH achieves in comparison to full-
rank training and other baseline methods; (iii) how the ^ss
found by CUTTLEFISH compare to manually tuned and
explicitly learned ones. Our comprehensive experimental
results demonstrate that CUTTLEFISH automatically selects
all factorization hyperparameters during training on-the-
ﬂy, eliminating the need for multiple experimental trials
for factorization hyperparameter tuning. More speciﬁcally,
the experimental results reveal that CUTTLEFISH generates
models up to 5.6smaller than full-rank models, and at-
tains up to a 1.2faster end-to-end training process while
preserving comparable accuracy. Moreover, CUTTLEFISH
outperforms state-of-the-art low-rank model training meth-
ods and other prominent baselines.
4.1 Experimental setup and implementation details
Pre-training ML tasks. We conducted experiments
on various computer vision pre-training tasks, in-
cluding CIFAR-10, CIFAR-100 (Krizhevsky et al.,
2009), SVHN (Netzer et al., 2011), and ImageNet
(ILSVRC2012) (Deng et al., 2009). For CIFAR-10, CIFAR-
100, and SVHN (Netzer et al., 2011), we trained VGG-
19-BN (referred to as VGG-19) (Simonyan & Zisserman,
2014) and ResNet-18 (He et al., 2016). In the case of the
SVHN dataset, we utilized the original training images and
excluded the additional images. For ImageNet, our ex-
periments involved ResNet-50, WideResNet-50-2 (referred
to as WideResNet-50), DeiT-base, and ResMLP-S36 (He
et al., 2016; Zagoruyko & Komodakis, 2016; Touvron et al.,
2021b;a). Further details about the machine learning tasks
can be found in the appendix.
Fine-tuning ML tasks. We experiment on ﬁne-tuning
BERT BASE over the GLUE benchmark (Wang et al., 2018).
Hyperparameters & training schedule. For VGG-19
and ResNet-18 training on CIFAR-10 and CIFAR-100, we
train the NNs for 300 epochs, while on SVHN, we train the
NNs for 200 epochs. We employ a batch size of 1,024 for
CIFAR and SVHN tasks to achieve high arithmetic inten-
sity. The initial learning rate is linearly scaled up from 0.1
to 0.8 within ﬁve epochs and then decayed at milestones
of 50% and 75% of the total training epochs (Goyal et al.,
2017). For WideResNet-50 and ResNet-50 training on the
ImageNet dataset, we follow the hyperparameter settings
in (Goyal et al., 2017), where the models are trained for 90
epochs with an initial learning rate of 0.1, which is decayed
by a factor of 0.1 at epochs 30, 60, and 80 using a batch size
of 256. In addition to (Goyal et al., 2017), we apply label
smoothing as described in (Wang et al., 2021a). For DeiTand ResMLP, we train them from scratch, adhering to the
training schedule proposed in (Touvron et al., 2021b). For
the GLUE ﬁne-tuning benchmark, we follow the default hy-
perparameter setup in (Devlin et al., 2018; Jiao et al., 2020).
Since CUTTLEFISH generalizes spectral initialization (SI)
and is compatible with Frobenius decay (FD), we deploy
FD in conjunction with CUTTLEFISH when it contributes
to better accuracy. Further details on hyperparameters and
training schedules can be found in the appendix.
Experimental environment. We employ the NVIDIA
NGC Docker container for software dependencies. Experi-
ments for CIFAR, SVHN, and GLUE tasks are conducted
on an EC2 p3.2xlarge instance (featuring a single V100
GPU) using FP32 precision. For BERT ﬁne-tuning and
ImageNet training of ResNet-50 and WideResNet-50, the
experiments are carried out on an EC2 g4dn.metal instance
(equipped with eight T4 GPUs) using FP32 precision. For
ImageNet training of DeiT and ResMLP, the experiments
are performed on a single p4d.24xlarge instance (housing
eight A100 GPUs) with mixed-precision training enabled.
Baseline methods. We implement CUTTLEFISH and all
considered baselines in PyTorch (Paszke et al., 2019). The
baseline methods under consideration are: (i) PUFFERFISH ,
which employs manually tuned s(Wang et al., 2021a). To
compare with PUFFERFISH , we use the same factorized
ResNet-18, VGG-19, ResNet-50, and WideResNet-50 as
reported in (Wang et al., 2021a). For DeiT and ResMLP
models, not explored in the original PUFFERFISH paper, we
adopt the same heuristic of using a ﬁxed global rank ratio
=1
4, tuningKto match the factorized model sizes found
byCUTTLEFISH , and setting E= 80 for the entire training
epochsT= 300 (Wang et al., 2021a); (ii) the factorized low-
rank training method with SI and FD proposed by (Khodak
et al., 2020) (referred to as “SI&FD”), with s of SI&FD
tuned to match the sizes of factorized models found by CUT-
TLEFISH ; (iii) training time structured pruning method, or
“early bird ticket ” (EB Train) (You et al., 2020); (iv) the
IMP method where each pruning round follows the train-
ing length and prunes 20% of the remaining model weights
at each level, rewinding to the 6th epoch (Frankle et al.,
2019); (v) Gradient Signal Preservation (GraSP) (Wang
et al., 2020a); (vi) the learnable factorized low-rank training
method, or LC model compression, where layer ranks R
are explicitly optimized jointly with model weights Wvia
an alternating optimization process (Idelbayev & Carreira-
Perpin ´an, 2020). For GLUE ﬁne-tuning, we compare C UT-
TLEFISH against DistillBERT and TinyBERT (Sanh et al.,
2019; Jiao et al., 2020); (vii) XNOR-Nets for training time
quantization method (Rastegari et al., 2016), based on the
public PyTorch implementation available at1.
CUTTLEFISH with FD. To implement FD, i.e.,`() +

2kUV>k2
F(where`()stands for the loss function), one
1https://github.com/jiecaoyu/
XNOR-Net-PyTorch

--- PAGE 8 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Cuttlefish
Pufferfish
Full-rank
(a) VGG-19 on CIFAR-10
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Pufferfish
Full-rank
Cuttlefish (b) VGG-19 on CIFAR-100
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Pufferfish
Full-rank
Cuttlefish (c) VGG-19 on SVHN
Figure 5. Comparisons on the selected ranks Rfound by CUTTLEFISH ,PUFFERFISH , LC compression, and full ranks for VGG-19 trained
on CIFAR-10, CIFAR-100, and SVHN datasets.
has to compute the gradient on the regularization term, i.e.,
rU
2kUV>k2
F=UV>V;rV
2kUV>k2
F=U>UV>
where one can see there is a shared term UV>, which does
not need to be recomputed. We optimize the implementation
to only compute UV>once. For the hybrid NN architec-
tures, normal `2weight decay is conducted over full-rank
layers when FD is enabled for factorized low-rank layers.
Extra BatchNorm layers. In experiments where FD
is not enabled for CUTTLEFISH , we incorporate an ex-
tra BatchNorm (BN) layer following the Ulayer, i.e.,
BNV(BNU(xU)V>), drawing inspiration from the net-
work architecture design of MobileNets (Howard et al.,
2017). We also apply this approach to P UFFERFISH .
4.2 Experimental results and analysis
How does C UTTLEFISH scompare to manually
tuned/learned ones? A crucial question to consider is
the appearance of the sreturned by CUTTLEFISH . We dis-
play theRs discovered by CUTTLEFISH ,PUFFERFISH , and
LC compression for VGG-19 trained on CIFAR-10, CIFAR-
100, and SVHN datasets (results presented in Figure 5).
Here, it is evident that CUTTLEFISH provides a selection of
Rthat closely aligns with explicitly trained rank selections,
i.e.,LC compression, where rank selection and low-rank
model weights are jointly learned during model training.
This demonstrates the effectiveness of the rank selection
heuristic employed by C UTTLEFISH .
Parameter reduction and model accuracy. We thor-
oughly investigate the effectiveness of CUTTLEFISH and
conduct extensive comparisons against the baselines, with
results displayed in Tables 1, 2, 3, and 4. The primary obser-
vation is that CUTTLEFISH successfully reduces the number
of parameters while only causing minimal loss in accuracy.
Notably, for VGG-19 trained on CIFAR-10, CUTTLEFISH
identiﬁes a factorized model that is 10.8 smaller than the
full-rank (vanilla) VGG-19 model, while achieving even
better validation accuracy. In comparison to PUFFERFISH
(shown in Table 1), CUTTLEFISH discovers a factorized low-
rank model that is 4.4 smaller with similar ﬁnal model
accuracy for VGG-19 trained on CIFAR-10. To achieve a
factorized model of the same size, SI&FD does not always
yield comparable accuracy to the full-rank model. For in-
stance, for CIFAR-10 and CIFAR-100 trained on VGG-19,
SI&FD results in a non-trivial accuracy drop of 1.2% and1.8%, respectively, because K= 1is always used in SI&FD,
which negatively affects the ﬁnal model accuracy. On Ima-
geNet, CUTTLEFISH attains smaller factorized ResNet-50
(0.5M fewer parameters) and WideResNet-50 (2.7M fewer
parameters) with higher top-1 and top-5 validation accuracy
compared to PUFFERFISH . For DeiT and ResMLP, we use
a ﬁxed rank ratio =1
4and tune the Ks ofPUFFERFISH
to match the factorized low-rank model sizes of CUTTLE -
FISH for fair comparisons. PUFFERFISH factorized DeiT
and ResMLP consistently result in inferior model accuracy
compared to CUTTLEFISH . This occurs because the model
weights of DeiT and ResMLP are less likely to be low rank,
so using=1
4following the original PUFFERFISH heuristic
leads to overly aggressive rank estimations. CUTTLEFISH ,
in contrast, detects this through a more appropriate rank
estimation heuristic.
End-to-end runtime and computational complexity.
As discussed in Section 3.5, factorized low-rank training
achieves substantial speedups when arithmetic intensity is
high. One way to achieve high arithmetic intensity is by
using a large batch size for training. Consequently, we
use a large batch size of 1,024 and measure the end-to-end
training time for the experiments on CIFAR. The results,
presented in Table 1, demonstrate that CUTTLEFISH con-
sistently leads to faster end-to-end training time (including
full-rank epochs and all other overhead computations, such
as proﬁling and stable rank computing) compared to full-
rank training. For instance, CUTTLEFISH achieves 1.2
end-to-end training speedups on both ResNet-18 and VGG-
19 trained on CIFAR-10. CUTTLEFISH yields comparable
runtime to PUFFERFISH for ResNet-18 and faster runtime
on VGG-19 because it ﬁnds smaller Kfor VGG-19, i.e.,
K= 4, while PUFFERFISH usesK= 9. SI&FD achieves
faster runtime than CUTTLEFISH due to its use of K= 1
(and generally higher computational complexities for the
initial convolution layers). However, employing such an
aggressive value for Kinevitably results in accuracy loss,
as discussed earlier. Both FC compression and IMP re-
quire heavy computation to achieve small models, which
are signiﬁcantly slower than full-rank training. XNOR-Nets
employ binary model weights and activations, which re-
sults in a reduction of ﬁnal accuracy for tasks compared to
dense networks. Ideally, the use of binarized weights and
activations should greatly speed up model training and sub-

--- PAGE 9 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 1. The results, averaged across three independent trials with different random seeds, showcase the performance of CUTTLEFISH and
other baselines on ResNet-18 and VGG-19 trained over CIFAR-10 and CIFAR-100 using a batch size of 1,024. The runtime benchmark is
conducted on a single EC2 p3.2xlarge instance. y: The SI&FD baseline is tuned such that the model size is comparable (albeit slightly
larger) to the models that CUTTLEFISH discovers.?:CUTTLEFISH is tested with both FD enabled and disabled, and the results with
the best accuracy are reported in the table. {: XNOR-Net employs binary weights and activations; although the overall number of
trainable parameters remains the same as the vanilla network, each model weight is quantized from 32-bit to 1-bit. Therefore, we report a
compression rate of 3:125% for XNOR-Nets. A comprehensive ablation study can be found in the appendix.
CIFAR-10 CIFAR-100
Model: ResNet-18 Params. (M)Val. Acc. ( %)Time (hrs.) Params. (M)Val. Acc. ( %)Time (hrs.)
Full-rank 11:2(100%) 94:410:14 0:82(1) 11:2(100%) 75:950:23 0:82(1)
PUFFERFISH 3:3(29.9%) 94:180:15 0:70(1.16)3:4(30.2%) 72:430:18 0:70(1.17)
SI&FDy2:1(18.5%) 94:380:03 0:59(1.39)2:7(24.1%) 75:800:17 0:70(1.16)
IMP 1:9(16.8%) 95:040:03 6:55(0.13)2:4(21.0%) 75:510:09 5:73(0.14)
XNOR-Net{11:2(3.1%) 88:760:14 3:61(0.23)11:2(3.1%) 57:230:40 3:62(0.23)
CUTTLEFISH?2:0(17.9%) 94:730:08 0:70(1.18)2:6(23.4%) 75:570:24 0:69(1.19)
Model: VGG-19 Params. (M)Val. Acc. ( %)Time (hrs.) Params. (M)Val. Acc. ( %)Time (hrs.)
Full-rank 20:0(100%) 93:410:15 0:50(1) 20:1(100%) 72:170:37 0:49(1)
PUFFERFISH 8:1(40.5%) 93:360:09 0:46(1.09)8:2(40.6%) 72:430:18 0:46(1.09)
SI&FDy2:0(10.0%) 92:230:08 0:34(1.44)3:3(16.5%) 70:420:48 0:39(1.26)
LC Compress. 1:7(8.7%) 93:230:15 5:9(0.08) 3:8(19.0%) 71:510:07 15:98(0.03)
IMP 1:7(8.6%) 93:680:28 5:48(0.09)3:4(16.8%) 73:390:32 3:96(0.13)
XNOR-Net{20:0(3.1%) 86:610:10 1:43(0.35)20:1(3.1%) 49:070:28 1:43(0.35)
CUTTLEFISH?1:9(9.3%) 93:540:10 0:42(1.18)3:3(16.3%) 72:230:09 0:44(1.14)
stantially decrease memory consumption during the process.
However, PyTorch lacks an efﬁcient implementation of a
binarized convolution operator. Consequently, our experi-
ments utilized FP32 networks and activations to simulate
binary networks, leading to a notably slower runtime com-
pared to conventional FP32 training. This is because each
layer’s output necessitates binarization, and model weights
must be re-binarized for every iteration. For ImageNet ex-
periments (presented in Table 2), the memory footprints are
high, limiting us to a batch size of 256. CUTTLEFISH identi-
ﬁes factorized ResNet-50 and WideResNet-50 models that
achieve 1.2and 1.3end-to-end speedups for ImageNet
training, respectively. Although the factorized models found
byCUTTLEFISH are comparable to PUFFERFISH , it elimi-
nates the need for extensive hyperparameter tuning for such
large-scale tasks.
4.3 Computation overheads introduced by
CUTTLEFISH .
Computational overheads of proﬁling. The proﬁling
process in CUTTLEFISH is a lightweight operation. For
instance, with ResNet-18 on the CIFAR-10 dataset, we per-
form proﬁling using = 11 iterations and exclude the
running time for the ﬁrst iteration for both full-rank and
low-rank models ( i.e.,running 22 iterations in total). We
then average the running time ﬁgures for the remaining 10
iterations for benchmarking purposes. Averaged from threeindependent runs, the entire proﬁling stage takes 3.98 sec-
onds, which accounts for a mere 0.16% of the total running
time of CUTTLEFISH on ResNet-18 trained on the CIFAR-
10 dataset.
Computational overheads of rank estimation. It is im-
portant to emphasize that CUTTLEFISH needs to compute
the singular values of the entire network weights at the end
of each epoch. It is worth noting that to calculate stable
ranks, only singular values are required, rather than singular
vectors. This process can be accelerated by leveraging APIs,
such as scipy.linalg.svdvals , which only compute
singular values of a given matrix. Taking ResNet-18 trained
on CIFAR-10 as an example, the average time taken for rank
estimation using the stable rank is 0.49 seconds per epoch.
ForCUTTLEFISH , which requires E= 82:3epochs (on
average) for full-rank training, the stable rank estimation
takes a total of 39.97 seconds, accounting for 1.6% of the
entire end-to-end running time.
4.4 Ablation Study
Accuracy and runtime efﬁciency of extra BNs. We con-
duct additional ablation studies to examine the inﬂuence
of incorporating extra BN layers on CUTTLEFISH perfor-
mance. In our primary experiments, we use FD and disable
extra BN layers to ensure accurate FD gradient computation
when FD leads to better accuracy. Our ablation studies in-
volve training ResNet-18 and VGG-19 on CIFAR-10 and

--- PAGE 10 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 2. The results presented include vanilla, PUFFERFISH , and CUTTLEFISH implementations of ResNet-50 and WideResNet-50, trained
on ImageNet. The FLOPs numbers represent model inference latency, measured using simulated single-batch input with dimensions of
(3;224;224) . Runtime benchmarks are conducted on EC2 g4dn.metal instances.
# Params. Val. Acc. Val. Acc. FLOPs Time
(M) Top-1 Top-5 (G) (hrs.)
WideResNet-50 68:9(100%) 78:1 94:0 11:4 147:8(1)
PUFFERFISH 40:0(58.1%) 77:860:05 93:970:05 10:0 112:7(1.31)
CUTTLEFISH 37:4(54.3%) 78:00:06 94:040:09 10:0 112:7(1.31)
ResNet-50 25:6(100%) 77:0 93 :4 4 :1 67:0(1)
PUFFERFISH 15:2(59.5%) 76:360:03 93:210:03 3:6 55:6(1.20)
CUTTLEFISH 14:7(57.4%) 76:440:16 93:210:03 3:6 56:7(1.18)
Table 3. The results for vanilla, PUFFERFISH , and CUTTLEFISH
implementations of DeiT-base and ResMLP-S36, trained on Im-
ageNet, are presented. FLOPs numbers, which measure model
inference latency, are determined using simulated single-batch
input with dimensions of (3;224;224) .
# Params. Val. Acc. Val. Acc. FLOPs
(M) Top-1 Top-5 (G)
DeiT-base 86:6 81:8 95:6 17:6
PUFFERFISH 58:3 81:150:04 95:580:04 12:0
CUTTLEFISH 58:3 81:520:03 95:590:04 12:0
ResMLP-S36 44:7 80:1 95 :0 8 :9
PUFFERFISH 29:3 77:780:20 94:000:06 5:9
CUTTLEFISH 29:4 78:940:04 94:520:05 5:8
CIFAR-100 datasets, as well as ResNet-50 on ImageNet,
and evaluating model sizes, best validation accuracy (top-1
for ImageNet), end-to-end training time, and per-iteration
time on low-rank models. The hyperparameters used in
the ablation studies are consistent with those used for the
main results in the Experiment section. The ablation study
results, shown in Table 5, reveal that adding extra BN layers
generally leads to a marginally larger model size and slower
per-iteration and end-to-end runtimes. For example, when
training ResNet-18 on CIFAR-10 without extra BNs, the
end-to-end training time is 1.4% faster, and the per-iteration
runtime is 2.8% faster. The impact of extra BNs on ﬁnal val-
idation accuracy varies across experiments: enabling extra
BNs slightly improves accuracy for ResNet-18 and VGG-19
on CIFAR-10, but not for CIFAR-100. However, for the Im-
ageNet experiment, adding extra BNs leads to a non-trivial
increase in model accuracy by an average of 0.21% across
three independent runs with different random seeds. This
improvement is signiﬁcant, considering it relates to top-1
accuracy for a 1000-class classiﬁcation problem. There are
two potential explanations for why extra BNs help improve
accuracy for ImageNet experiments more than CIFAR exper-
iments: 1) The model capacity of ResNet-18 and VGG-19
seems sufﬁciently large for CIFAR datasets, allowing high
compression rates ( e.g., , 5-10). In contrast, for ResNet-
50 on ImageNet, the model capacity appears inadequate.CUTTLEFISH , in this case, does not achieve exceptionally
high compression rates ( i.e.,less than 2). Consequently,
the inclusion of extra BNs appears to provide the low-rank
factorized model with additional capacity to enhance its
accuracy. 2) For CIFAR experiments, we used a batch size
of 1024, constrained by GPU memory, while a batch size of
256 was employed for ImageNet experiments. It is possible
that extra BNs offer more substantial beneﬁts in smaller
batch settings. Note that for Transformer model-based ex-
periments, we do not enable extra BNs as LayerNorm is
commonly used instead of BNs, which is beyond the scope
of this ablation study.
Conv
21Conv
22Conv
23Conv
24Conv
25Conv
26Conv
27Conv
28Conv
29Conv
30Conv
31Conv
32Conv
33Conv
34Conv
35Conv
36Conv
37Conv
38Conv
39Conv
40Conv
41Conv
42Conv
43Conv
44Conv
45Conv
46Conv
47Conv
48FC
490.00.51.01.52.02.53.0Per Iter Time (ms.)Full-rank RR:0.25 RR:0.125 RR:0.063
0.00.10.20.30.4
Scaled Stable RR
Block-1-Attn. Block-1-MLP Block-2-Attn. Block-2-MLP Block-3-Attn. Block-3-MLP Block-4-Attn. Block-4-MLP Block-5-Attn. Block-5-MLP Block-6-Attn. Block-6-MLP Block-7-Attn. Block-7-MLP Block-8-Attn. Block-8-MLP Block-9-Attn. Block-9-MLP Block-10-Attn. Block-10-MLP Block-11-Attn. Block-11-MLP Block-12-Attn. Block-12-MLP0246Per Iter Time (ms.)Full-rank RR:0.25 RR:0.125 RR:0.063
0.00.20.40.60.8
Scaled Stable RR
Figure 6. Ablation study on the layer-wise costs of ( Top): ResNet-
50 training on ImageNet (along with the scaled stable rank ratios
selected by CUTTLEFISH ); (Bottom ): DeiT-Small training on
ImageNet, batch size of both experiments are 128, and time for
both experiments are measured on an EC2 p3.2xlarge instance
with a batch size of 128. “RR” stands for rank ratio in the ﬁgure.
Effectiveness of low-rank factorization on various layer
types. In order to compare the efﬁciency of low-rank fac-
torization against convolution, FC, MLP, and multi-head
attention layers, we conducted an ablation study using
ResNet-50 and DeiT-small on the ImageNet dataset. The
per-iteration time of each layer was measured and the results
are depicted in Figure 6 (for ResNet-50, we also illustrated
the scaled stable rank ratios selected by CUTTLEFISH ). We
focused on forward computation time for this study, as it is
well known that there is a constant factor between forward
and backward computing time, and the former serves as a

--- PAGE 11 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 4. Vanilla BERT BASE, Distill BERT , Tiny BERT 6, as well as CUTTLEFISH BERT BASE are evaluated on the GLUE benchmark. F1
scores are used as the metric for QQP and MRPC, while Spearman correlations are reported for STS-B, and accuracy scores are reported
for the remaining tasks.
Model # Params. (M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.
BERT BASE 108:3 83:9=84:490:987:666:8 92:2 88:6 60:1 88:682:5
Distill BERT 65:8 81 :1=82:0 89:1 86:2 57:8 90:6 88:6 47:3 83:4 78:4
Tiny BERT 6 67:0 83:9=83:8 90:6 86:872:9 91:5 90:6 46:2 89:3 81:7
CUTTLEFISH 48:8 83:7=84:4 90:8 86:7 67:092:3 88:4 56:8 87:9 82:0
Table 5. In this ablation study, we evaluate the impact of extra BNs on ResNet-18 and VGG-19 trained on CIFAR-10 and CIFAR-100,
as well as ResNet-50 trained on the ImageNet dataset. The end-to-end and per-iteration running time results are measured on a single
p3.2xlarge EC2 instance for ResNet-18 and VGG-19, and a single g4dn.metal EC2 instance for ResNet-50. The results are averaged from
three independent experiments using different random seeds.
CIFAR-10 CIFAR-100
Model: Params. Val. Acc. Time End2end Time Iter. Params. Val. Acc. Time End2end Time Iter.
ResNet-18 (M) (%) (hrs.) (ms) (M) (%) (hrs.) (ms)
w/ extra BNs 2:02 94:360:07 0:716 163:420:51 2:62 73:640:26 0:719 164:721:63
w/o extra BNs 1:97 94:320:22 0:706 158 :940:53 2:60 73:770:14 0:689 158 :531:32
Model: Params. Val. Acc. Time End2end Time Iter. Params. Val. Acc. Time End2end Time Iter.
VGG-19 (M) (%) (hrs.) (ms) (M) (%) (hrs.) (ms)
w/ extra BNs 1:86 93:540:10 0:422 85:530:59 3:31 71:990:02 0:436 91:760:52
w/o extra BNs 1:86 93:490:08 0:419 84 :550:26 3:31 72:150:24 0:432 89 :900:18
ResNet-50 on ImageNet Params. (M) Top-1 Val. Acc. ( %) Time End2end (hrs.) Time Iter. (sec.)
w/ extra BNs 14:7 76:440:16 56:7 0:430:002
w/o extra BNs 14:7 76 :230:21 55:6 0 :420:003
good proxy for per-iteration time. Due to space constraints,
we only plotted the results for the 21st convolution layer on-
wards in the ResNet-50 experiments, although meaningful
speedups were observed for the ﬁrst 20 layers as well. In
the case of convolution layers, our experiments revealed an
average speedup of 2:1across all 49 layers when using
a rank ratio of1
4. However, we observed that the last FC
layer actually slowed down when factorized, regardless of
the rank ratio used. This could be attributed to the small
size of the FC layer, which incurs a large kernel launching
overhead when split into two smaller layers, thereby nulli-
fying any computation cost savings. Our experiments with
DeiT showed that factorizing both multi-head attention and
MLP layers resulted in signiﬁcant speedups for all 12 Trans-
former encoder blocks. Additionally, factorizing the MLP
layer led to greater speedup gains compared to factorizing
the multi-head attention layer. Speciﬁcally, factorizing the
multi-head attention layer resulted in 1:26speedups on
average, while factorizing the MLP layer resulted in 1:73
speedups on average for all 12 blocks at a rank ratio of1
4.
4.5 Limitations of C UTTLEFISH
A limitation of CUTTLEFISH is that the hyperparameters sit
tunes are inﬂuenced by the randomness of the training algo-
rithm and model initialization. Consequently, different trial
runs may not yield factorized models with identical sizes(although the variance is minimal). This can potentially
impact exact reproducibility.
5 C ONCLUSION
We present CUTTLEFISH , an automated low-rank training
method that eliminates the need for tuning additional factor-
ization hyperparameters, i.e.,S= (E;K;R).CUTTLEFISH
leverages two key insights related to the emergence of stable
ranks during training and the actual speedup gains achieved
by factorizing different NN layers. Utilizing these insights,
it designs heuristics for automatically selecting s2S. Our
extensive experiments demonstrate that C UTTLEFISH iden-
tiﬁes low-rank models that are not only smaller, but also
yield better ﬁnal accuracy in most cases when compared to
state-of-the-art low-rank training methods.
Acknowledgments
We thank our shepherd, Bilge Acun, and the anonymous
MLSys reviewers for their valuable insights and recom-
mendations, which have enhanced our work. This research
has been graciously funded by ONR Grant No. N00014-
21-1-2806, three Sony Faculty Innovation Awards, NSF
IIS1563887, NSF CCF1629559, NSF IIS1617583, NSF
IIS1955532, NSF CNS2008248, NSF IIS2123952, and NSF
BCS2040381, and NGA HM04762010002.

--- PAGE 12 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
REFERENCES
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Advances in Neural
Information Processing Systems , 2020.
Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R ´e,
C. Scatterbrain: Unifying sparse and low-rank attention.
InAdvances in Neural Information Processing Systems ,
2021a.
Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A.,
and Re, C. Pixelated butterﬂy: Simple and efﬁcient sparse
training for neural network models. In International
Conference on Learning Representations , 2022.
Chen, P., Yu, H.-F., Dhillon, I., and Hsieh, C.-J. Drone:
Data-aware low-rank compression for large nlp models.
Advances in neural information processing systems , 34:
29321–29334, 2021b.
Chen, Y .-H., Emer, J., and Sze, V . Eyeriss: A spatial ar-
chitecture for energy-efﬁcient dataﬂow for convolutional
neural networks. ACM SIGARCH Computer Architecture
News , 44(3):367–379, 2016.
Chollet, F. Xception: Deep learning with depthwise separa-
ble convolutions. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pp. 1251–
1258, 2017.
Dao, T., Chen, B., Sohoni, N., Desai, A., Poli, M., Grogan,
J., Liu, A., Rao, A., Rudra, A., and R ´e, C. Monarch:
Expressive structured matrices for efﬁcient and accurate
training. arXiv preprint arXiv:2204.00595 , 2022.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pp. 248–255. Ieee, 2009.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 ,
2018.
Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen,
E. Rigging the lottery: Making all tickets winners. In
International Conference on Machine Learning , pp. 2943–
2952. PMLR, 2020.Fedus, W., Zoph, B., and Shazeer, N. Switch transform-
ers: Scaling to trillion parameter models with simple and
efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
Frankle, J. and Carbin, M. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. arXiv preprint
arXiv:1803.03635 , 2018.
Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M.
Stabilizing the lottery ticket hypothesis. arXiv preprint
arXiv:1903.01611 , 2019.
Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P.,
Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and
He, K. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
Han, S., Mao, H., and Dally, W. J. Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 , 2015a.
Han, S., Pool, J., Tran, J., and Dally, W. Learning both
weights and connections for efﬁcient neural network.
Advances in neural information processing systems , 28,
2015b.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pp. 770–778, 2016.
He, Y ., Zhang, X., and Sun, J. Channel pruning for acceler-
ating very deep neural networks. In Proceedings of the
IEEE International Conference on Computer Vision , pp.
1389–1397, 2017.
Hinton, G., Vinyals, O., and Dean, J. Distilling
the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,
W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:
Efﬁcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861 , 2017.
Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,
S., Wang, L., and Chen, W. Lora: Low-rank adaptation of
large language models. arXiv preprint arXiv:2106.09685 ,
2021.
Hu, H., Peng, R., Tai, Y .-W., and Tang, C.-K. Net-
work trimming: A data-driven neuron pruning approach
towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250 , 2016.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y . Binarized neural networks. In Advances in

--- PAGE 13 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
neural information processing systems , pp. 4107–4115,
2016.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y . Quantized neural networks: Training neural
networks with low precision weights and activations. The
Journal of Machine Learning Research , 18(1):6869–6898,
2017.
Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low-
rank hadamard product for communication-efﬁcient fed-
erated learning. In International Conference on Learning
Representations , 2022.
Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K.,
Dally, W. J., and Keutzer, K. Squeezenet: Alexnet-level
accuracy with 50x fewer parameters and¡ 0.5 mb model
size. arXiv preprint arXiv:1602.07360 , 2016.
Idelbayev, Y . and Carreira-Perpin ´an, M. A. Low-rank com-
pression of neural nets: Learning the rank of each layer.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pp. 8049–8059,
2020.
Ioannou, Y ., Robertson, D., Shotton, J., Cipolla, R., and Cri-
minisi, A. Training cnns with low-rank ﬁlters for efﬁcient
image classiﬁcation. arXiv preprint arXiv:1511.06744 ,
2015.
Izsak, P., Berchansky, M., and Levy, O. How to
train bert with an academic budget. arXiv preprint
arXiv:2104.07705 , 2021.
Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up
convolutional neural networks with low rank expansions.
arXiv preprint arXiv:1405.3866 , 2014.
Jeffers, J., Reinders, J., and Sodani, A. Intel Xeon Phi pro-
cessor high performance programming: knights landing
edition . Morgan Kaufmann, 2016.
Jiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,
Wang, F., and Liu, Q. Tinybert: Distilling bert for natural
language understanding. In EMNLP 2020 , pp. 4163–
4174, 2020.
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis,
M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode,
G., Cummings, R., et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977 ,
2019.
Khodak, M., Tenenholtz, N. A., Mackey, L., and Fusi, N. Ini-
tialization and regularization of factorized neural layers.
InInternational Conference on Learning Representations ,
2020.Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efﬁcient transformer. In International Conference on
Learning Representations , 2020.
Kone ˇcn`y, J., McMahan, H. B., Yu, F. X., Richt ´arik, P.,
Suresh, A. T., and Bacon, D. Federated learning: Strate-
gies for improving communication efﬁciency. arXiv
preprint arXiv:1610.05492 , 2016.
Krizhevsky, A., Hinton, G., et al. Learning multiple layers
of features from tiny images. Master’s thesis, Department
of Computer Science, University of Toronto , 2009.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,
and Soricut, R. Albert: A lite bert for self-supervised
learning of language representations. arXiv preprint
arXiv:1909.11942 , 2019.
Li, D., Wang, H., Shao, R., Guo, H., Xing, E., and Zhang,
H. Mpcformer: fast, performant and private transformer
inference with mpc. In The Eleventh International Con-
ference on Learning Representations , 2023.
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf,
H. P. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710 , 2016.
Lin, M., Chen, Q., and Yan, S. Network in network. arXiv
preprint arXiv:1312.4400 , 2013.
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re-
thinking the value of network pruning. arXiv preprint
arXiv:1810.05270 , 2018.
Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
ularization. In International Conference on Learning
Representations , 2019.
Netzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,
and Ng, A. Y . Reading digits in natural images with
unsupervised feature learning. In NIPS workshop on
deep learning and unsupervised feature learning , volume
2011, pp. 5, 2011.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. In Advances in neural information
processing systems , pp. 8026–8037, 2019.
Rastegari, M., Ordonez, V ., Redmon, J., and Farhadi, A.
Xnor-net: Imagenet classiﬁcation using binary convolu-
tional neural networks. In Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands,
October 11–14, 2016, Proceedings, Part IV , pp. 525–542.
Springer, 2016.

--- PAGE 14 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Renda, A., Frankle, J., and Carbin, M. Comparing rewinding
and ﬁne-tuning in neural network pruning. In Interna-
tional Conference on Learning Representations , 2020.
Sainath, T. N., Kingsbury, B., Sindhwani, V ., Arisoy, E.,
and Ramabhadran, B. Low-rank matrix factorization for
deep neural network training with high-dimensional out-
put targets. In Acoustics, Speech and Signal Processing
(ICASSP), 2013 IEEE International Conference on , pp.
6655–6659. IEEE, 2013.
Sanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
a distilled version of bert: smaller, faster, cheaper and
lighter. arXiv preprint arXiv:1910.01108 , 2019.
Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014.
Sreenivasan, K., Sohn, J.-y., Yang, L., Grinde, M., Nagle, A.,
Wang, H., Xing, E., Lee, K., and Papailiopoulos, D. Rare
gems: Finding lottery tickets at initialization. Advances
in Neural Information Processing Systems , 2022a.
Sreenivasan, K., yong Sohn, J., Yang, L., Grinde, M., Nagle,
A., Wang, H., Xing, E., Lee, K., and Papailiopoulos,
D. Rare gems: Finding lottery tickets at initialization.
InAdvances in Neural Information Processing Systems ,
2022b.
Tan, M. and Le, Q. V . Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. arXiv preprint
arXiv:1905.11946 , 2019.
Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,
Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers,
D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architec-
ture for vision. Advances in Neural Information Process-
ing Systems , 34, 2021.
Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-
Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve,
G., Verbeek, J., et al. Resmlp: Feedforward networks for
image classiﬁcation with data-efﬁcient training. arXiv
preprint arXiv:2105.03404 , 2021a.
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and J ´egou, H. Training data-efﬁcient image transform-
ers & distillation through attention. In International Con-
ference on Machine Learning , pp. 10347–10357. PMLR,
2021b.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems , pp. 5998–6008, 2017.V odrahalli, K., Shivanna, R., Sathiamoorthy, M., Jain, S.,
and Chi, E. Algorithms for efﬁciently learning low-rank
neural networks. arXiv preprint arXiv:2202.00834 , 2022.
Waleffe, R. and Rekatsinas, T. Principal component net-
works: Parameter reduction early in training. arXiv
preprint arXiv:2006.13347 , 2020.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. Glue: A multi-task benchmark and anal-
ysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 , 2018.
Wang, C., Zhang, G., and Grosse, R. Picking winning tickets
before training by preserving gradient ﬂow. International
Conference on Learning Representations , 2020a.
Wang, H., Yurochkin, M., Sun, Y ., Papailiopoulos, D., and
Khazaeni, Y . Federated learning with matched averaging.
InInternational Conference on Learning Representations ,
2020b.
Wang, H., Agarwal, S., and Papailiopoulos, D. Pufferﬁsh:
Communication-efﬁcient models at no extra cost. Pro-
ceedings of Machine Learning and Systems , 3, 2021a.
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H. B.,
Al-Shedivat, M., Andrew, G., Avestimehr, S., Daly, K.,
Data, D., et al. A ﬁeld guide to federated optimization.
arXiv preprint arXiv:2107.06917 , 2021b.
Wen, W., Wu, C., Wang, Y ., Chen, Y ., and Li, H. Learning
structured sparsity in deep neural networks. In Advances
in neural information processing systems , pp. 2074–2082,
2016.
Wiesler, S., Richard, A., Schluter, R., and Ney, H. Mean-
normalized stochastic gradient for large-scale deep learn-
ing. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on , pp.
180–184. IEEE, 2014.
Wu, J., Leng, C., Wang, Y ., Hu, Q., and Cheng, J. Quantized
convolutional neural networks for mobile devices. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 4820–4828, 2016.
Xue, J., Li, J., and Gong, Y . Restructuring of deep neural
network acoustic models with singular value decomposi-
tion. In Interspeech , pp. 2365–2369, 2013.
Yang, T.-J., Chen, Y .-H., and Sze, V . Designing energy-
efﬁcient convolutional neural networks using energy-
aware pruning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pp. 5687–
5695, 2017.

--- PAGE 15 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Yao, D., Pan, W., Wan, Y ., Jin, H., and Sun, L. Fedhm:
Efﬁcient federated learning for heterogeneous models via
low-rank factorization. arXiv preprint arXiv:2111.14655 ,
2021.
You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk,
R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets:
Towards more efﬁcient training of deep networks. arXiv
preprint arXiv:1909.11957 , 2019.
You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Bara-
niuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird
tickets: Toward more efﬁcient training of deep networks.
InInternational Conference on Learning Representations ,
2020.
Yu, J. and Huang, T. S. Universally slimmable networks
and improved training techniques. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pp. 1803–1811, 2019.
Yu, J., Yang, L., Xu, N., Yang, J., and Huang, T. Slimmable
neural networks. In International Conference on Learn-
ing Representations , 2019.
Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V . I., Han,
X., Gao, M., Lin, C.-Y ., and Davis, L. S. Nisp: Pruning
networks using neuron importance score propagation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 9194–9203, 2018.
Zagoruyko, S. and Komodakis, N. Wide residual networks.
arXiv preprint arXiv:1605.07146 , 2016.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022.
Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mo-
bile devices. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pp. 6848–6856,
2018.
Zhou, A., Yao, A., Guo, Y ., Xu, L., and Chen, Y . Incre-
mental network quantization: Towards lossless cnns with
low-precision weights. arXiv preprint arXiv:1702.03044 ,
2017.
Zhu, C., Han, S., Mao, H., and Dally, W. J. Trained ternary
quantization. arXiv preprint arXiv:1612.01064 , 2016.
Zhu, M. and Gupta, S. To prune, or not to prune: exploring
the efﬁcacy of pruning for model compression. arXiv
preprint arXiv:1710.01878 , 2017.

--- PAGE 16 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
A A RTIFACT APPENDIX
A.1 Abstract
We have made available the necessary artifacts to repli-
cate all results presented in the paper. Our experi-
ments utilize Amazon EC2 computing resources, includ-
ingp3.2xlarge (for ResNet-18 and VGG-19 training on
CIFAR-10 and CIFAR-100 datasets, as well as BERT ﬁne-
tuning on the GLUE benchmark), g4dn.metal (for example,
ResNet-50 and WideResNet-50 training on ImageNet), and
p4d.24xlarge (for DeiT-base and ResMLP execution on Im-
ageNet) instances. Additionally, we employ the NVIDIA
driver and Docker to construct the software stack.
To facilitate the replication of all reported experiments,
we provide scripts in our GitHub repository, accessible at
https://github.com/hwang595/Cuttlefish .
Running those provided scripts will set up and launch
experiments to reproduce our experimental results. For ease
of reproducibility, we also offer a public Amazon Machine
Image (AMI) – ami-05c0b3732203032b3 (in the region
ofUS West (Oregon) ) where experimental environments
and the ImageNet dataset, which is time-consuming to
download and set up are prepared.
A.2 Artifact check-list (meta-information)
In this section, we offer meta-information regarding the
conﬁguration, datasets, implementation, and other aspects
of our artifacts.
•Algorithm: Our artifact encompasses the CUTTLEFISH
automatic low-rank training schedule, along with the baseline
methods compared in the main paper, such as PUFFERFISH ,
SI&FD, XNOR-Net, GraSP, and others.
•Program: N/A
•Compilation: All methods and baselines are implemented
in PyTorch, therefore requiring no compilation.
•Transformations: N/A
•Binary: N/A
•Data set: For our main experiments, we employ CIFAR-
10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and
GLUE datasets. As preparing the ImageNet dataset can be
time-consuming, we provide a ready-to-use public AMI -
ami-05c0b3732203032b3 in the US West (Oregon) region for
convenience.
•Run-time environment: N/A
•Hardware: Our experiments were conducted using Ama-
zon EC2 instances, speciﬁcally p3.2xlarge ,g4dn.metal , and
p4d.24xlarge .
•Run-time state: N/A•Execution: We provide scripts to execute and launch the
experiments. Detailed descriptions and instructions can be
found in the README of our GitHub repository.
•Metrics: We collect metrics such as the number of param-
eters, validation accuracy (or similar scores for assessing
model quality), wall-clock time (including end-to-end and
per iteration/epoch durations), and computational complexity
(measured in FLOPS).
•Output: Our existing code writes checkpoints to the local
disk and also prints experimental outputs/logs directly.
•Experiments: N/A
•How much disk space required (approximately)?:
Around 1 Terabyte of disk space.
•How much time is needed to prepare workﬂow (approxi-
mately)?: Setting up the experimental environment should
take less than an hour. Downloading the ImageNet (ILSVRC
2012) dataset can take a few days. If the evaluators have
access to AWS, we have also provided a public AMI - ami-
05c0b3732203032b3 (in the region of US West (Oregon) )
which has the datasets and dependencies pre-installed.
•How much time is needed to complete experiments (ap-
proximately)?: Completing the CIFAR-10 and CIFAR-100
experiments with CUTTLEFISH typically takes less than an
hour for each task (see Table 1 for details). BERT ﬁne-tuning
experiments on all GLUE datasets require approximately a
few hours, while ImageNet experiments may take several
days to a week to reach full convergence for each method.
•Publicly available?: All our code is publicly avail-
able on the GitHub repository: https://github.
com/hwang595/Cuttlefish . For easy setup on
AWS, we also provide a public AMI - with ID ami-
05c0b3732203032b3 (in the region of US West (Oregon) ),
which can be used to launch large-scale experiments.
•Code licenses (if publicly available)?: N/A
•Data licenses (if publicly available)?: We use CIFAR-10,
CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and
the GLUE benchmark which come with their own licenses.
All datasets are publicly available.
•Workﬂow framework used?: N/A
•Archived (provide DOI)?: We use Zenedo to cre-
ate a publicly accessible archival repository for our
GitHub repository, i.e.,https://doi.org/10.5281/
zenodo.7884872 .
A.3 Description
We have made available the code necessary to replicate all the
experiments presented in this paper through a public GitHub repos-
itory that contains comprehensive documentation, allowing users
to seamlessly execute the experiments.
A.3.1 How delivered
Our entire codebase is available on the GitHub repository:
https://github.com/hwang595/Cuttlefish . To fa-
cilitate easy setup on AWS, we offer a public AMI - identiﬁed by
ami-05c0b3732203032b3 (in the region of US West (Oregon) ) -
which can be utilized to launch large-scale experiments.

--- PAGE 17 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
A.3.2 Hardware dependencies
For all our experiments we used p3.2xlarge ,g4dn.metal , and
p4d.24xlarge Amazon EC2 instances. To reproduce our results,
one instance of each type is required.
A.3.3 Software dependencies
We established our experimental environments using Docker,
conﬁguring them through PyTorch Docker containers from
NVIDIA GPU Cloud (NGC). The experiments involving the
CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were
based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3 ,
while those focused on the GLUE benchmark utilized the
nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi-
tional software dependencies not included in the Docker containers,
we have supplied installation scripts, accompanied by instructions
in the README ﬁle of our GitHub repository, to facilitate the
installation of these necessary components.
A.3.4 Data sets
For all our experiments we used CIFAR-10, CIFAR-100, SVHN,
ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR-
10, CIFAR-100, SVHN, and GLUE datasets, our code will
automatically download them. For the ImageNet dataset, we
have it ready and provided via the public AMI - with ID ami-
05c0b3732203032b3 (in the region of US West (Oregon) ).
A.4 Installation
In the GitHub README, we offer comprehensive instructions for
installing dependencies and conﬁguring the Docker environments.
A.5 Experiment workﬂow
We have provided a detailed README along with our GitHub
repository which provides bash scripts to execute and launch the
experiments.
A.6 Evaluation and expected result
During the experiment, logs containing details such as accuracy
and running time will be displayed directly. However, given the
inherent variability in machine learning tasks and the diversity of
hardware and system conﬁgurations, it is important to note that the
exact accuracy and running time ﬁgures reported in the paper may
not be replicated. Nevertheless, by using the artifacts provided,
one can expect to achieve comparable accuracy and running time
outcomes.
A.7 Experiment customization
The experiment can be customized by trying on different hardware
setups. One example of this will be to run these experiments on
slower GPUs (or other hardware, e.g., CPUs). Another option
would be to try to support more model architectures using the
heuristics of CUTTLEFISH (an interesting example will be adopting
CUTTLEFISH for some recently designed large language models).
A.8 Notes
If the evaluator utilizes our provided AMI, the initial disk ini-
tialization will take an extended period of time during the ﬁrstrun.

--- PAGE 18 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
B E XPERIMENTAL SETUP
In this section, we delve into the speciﬁcs of the datasets B.1
and model architectures B.2 employed in our experiments. Ad-
ditionally, we elaborate on the software environment B.3 and the
implementation details of all methods included in our experi-
ments B.4. Our code can be accessed at https://github.
com/hwang595/Cuttlefish .
B.1 Dataset
We carried out experiments across multiple computer vision and
NLP tasks to evaluate the performance of CUTTLEFISH and the
other considered baselines. In this section, we discuss the speciﬁcs
of each task in greater detail.
CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR-
100 comprise 60,000 color images with a resolution of 32 32
pixels, where 50,000 images are used for training and 10,000
for validation (since there is no provided test set for CIFAR-
10 and CIFAR-100, we follow the convention of other papers
by conducting experiments and reporting the highest achievable
accuracy on the validation datasets) (Krizhevsky et al., 2009).
CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas-
siﬁcation tasks, respectively. For data processing, we employ
standard augmentation techniques: channel-wise normalization,
random horizontal ﬂipping, and random cropping. Each color
channel is normalized with the following mean and standard
deviation values: r= 0:485;g= 0:456;b= 0:406;
r= 0:229;g= 0:224;b= 0:225. The normalization of
each channel pixel is achieved by subtracting the corresponding
channel’s mean value and dividing by the color channel’s standard
deviation.
SVHN. The SVHN dataset comprises 73,257 training images
and 26,032 validation images, all of which are colored with a
resolution of 3232 pixels (Netzer et al., 2011). This classiﬁ-
cation dataset consists of 10 classes. As there is no clear test-
validation split for the SVHN dataset, we follow the convention of
other papers by conducting experiments and reporting the highest
achievable accuracy on the validation datasets. There are 531,131
additional images for SVHN, but we do not include them in our
experiments for this paper. For data processing, we employ the
same data augmentation and normalization techniques used for
CIFAR-10 and CIFAR-100, as described above.
ImageNet (ILSVRC 2012). The ImageNet ILSVRC 2012
dataset consists of 1,281,167 colored training images spanning
1,000 classes and 50,000 colored validation images, also cov-
ering 1,000 classes (Deng et al., 2009). Augmentation tech-
niques include normalization, random rotation, and random hor-
izontal ﬂip. The training images are randomly resized and
cropped to a resolution of 224 224 using the torchvision API
torchvision.transforms.RandomResizedCrop . The
validation images are ﬁrst resized to a resolution of 256 256
and then center cropped to a resolution of 224 224. Each
color channel is normalized with the following mean and stan-
dard deviation values: r= 0:485;g= 0:456;b= 0:406;
r= 0:229;g= 0:224;b= 0:225. Each channel pixel is
normalized by subtracting the corresponding channel’s mean value
and then dividing by the color channel’s standard deviation.
GLUE benchmark. For the GLUE benchmark, we utilize
the data preparation and pre-processing pipeline implemented byHugging Face2. In accordance with prior work (Devlin et al.,
2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic
WNLI downstream task.
B.2 Model architectures
In this section, we provide a summary of the network architectures
utilized in our experiments.
ResNet-18, ResNet-50, and WideResNet-50-2. The
ResNet-18 and ResNet-50 architectures are derived from the orig-
inal design with minor modiﬁcations (He et al., 2016). The
WideResNet-50-2 adheres to the original wide residual network
design (Zagoruyko & Komodakis, 2016). As we employed ResNet-
18 for CIFAR-10 classiﬁcation, we adjusted the initial convolution
layer to use a 33convolution with padding at 1and stride at 1.
Our ResNet-18 implementation follows the GitHub repository3.
For all ResNet-18, ResNet-50, and WideResNet-50-2 networks,
the strides used for the four convolution layer stacks are respec-
tively 1;2;2;2. Bias terms for all layers are deactivated (owing to
the BatchNorm layers), except for the ﬁnal FC layer.
VGG-19-BN. In our experiments, we employ the VGG-19-BN
network architecture, which is a modiﬁed version of the original
VGG-19 (Simonyan & Zisserman, 2014). The original VGG-
19 network consists of 16 convolution layers and 3 FC layers,
including the ﬁnal linear classiﬁcation layer. We adopt the VGG-
19 architectures from (Frankle & Carbin, 2018; Khodak et al.,
2020), which remove the ﬁrst two FC layers following the last
convolution layer while retaining the ﬁnal linear classiﬁcation
layer. This results in a 17-layer architecture, but we continue to
refer to it as VGG-19-BN since it stems from the original VGG-19
design. Another modiﬁcation is replacing the max pooling layer
after the last convolution layer ( conv16 ) with an average pooling
layer. The detailed architecture is displayed in Table 7. We follow
the implementation of the pytorch-cifar GitHub repository
mentioned above. Due to the BatchNorm layers, bias terms for all
layers are deactivated, except for the ﬁnal FC layer.
DeiT and ResMLP. Our implementations of DeiT-base and
ResMLP-S36 are sourced directly from the model implementa-
tions provided by the Pytorch Image Models ( i.e.,timm ) library4.
For DeiT-base, we do not use the scaled ImageNet resolution ver-
sion and we deactivate the distillation options. More speciﬁcally,
we initiate the training of a deit base patch16 224 model
from scratch, as provided by the timm library. For training, we
employ the training method and hyperparameters speciﬁed in the
original GitHub repository5. For ResMLP-S36, we adhere to
the same training methodology used for DeiT-base, utilizing the
resmlp 36224 provided by the timm library.
BERT BASE, DistillBERT, and TinyBERT. The imple-
mentations of BERTBASE , DistillBERT, and TinyBERT6are
directly provided by Hugging Face. For BERTBASE , we use
the model named bert-base-cased . For DistillBERT, we
2https://github.com/huggingface/
transformers/tree/main/examples/pytorch/
text-classification
3https://github.com/kuangliu/
pytorch-cifar
4https://github.com/rwightman/
pytorch-image-models
5https://github.com/facebookresearch/deit

--- PAGE 19 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 6. The ResNet-18, ResNet-50, and WideResNet-50-2 network architectures used in our experiments. It should be noted that when
using ResNet-18 for CIFAR-10 training, we make corresponding adjustments to the initial convolution layer. Each convolution layer is
followed by a BatchNorm layer. In the notation used in this table, “ 77;64” signiﬁes that the convolution layer contains 64 convolution
kernels, i.e.,each kernel has a dimension of 77and the output dimension is 64.
Model ResNet-18 ResNet-50 WideResNet-50-2
Conv 133, 64 7 7, 64 7 7, 64
padding 1 padding 3 padding 3
stride 1 stride 2 stride 2
- Max Pool, kernel size 3, stride 2, padding 1
Layer stack 1
33, 64
33, 64
2"11, 64
33, 64
11, 256#
3"11, 128
33, 128
11, 256#
3
Layer stack 2
33, 128
33, 128
2"11, 128
33, 128
11, 512#
4"11, 256
33, 256
11, 512#
4
Layer stack 3
33, 256
33, 256
2"11, 256
33, 256
11, 1024#
6"11, 512
33, 512
11, 1024#
6
Layer stack 4
33, 512
33, 512
2"11, 512
33, 512
11, 2048#
3"11, 1024
33, 1024
11, 2048#
3 -
FCAvg Pool, kernel size 4 Adaptive Avg Pool, output size (1;1)
51210 2048 1000 2048 1000
employ the model named distilbert-base-cased . For
BERT BASE, we use the model named bert-base-cased
again. For TinyBERT6, we utilize the model named
huawei-noah/TinyBERT General 6L768D .
All model names are supplied through the API of
--model name orpath in Hugging Face.
B.3 Software details
For the experiments on CIFAR-10, CIFAR-100, and SVHN,
which include CUTTLEFISH and all considered baseline meth-
ods, our software setup is built on the NVIDIA NGC
Docker container for PyTorch. We use the docker image
nvcr.io/nvidia/pytorch:20.07-py3 to set up the ex-
periment environment on p3.2xlarge EC2 instances. The
CUDA version we used is 11.6. For the BERT BASE ﬁne-tuning
experiment on the GLUE benchmark, we employ the docker im-
age,nvcr.io/nvidia/pytorch:22.01-py3 . We install
Hugging Face with version 4.17.0.dev0 .
B.4 Implementation details
For all our experiments, we set
torch.backends.cudnn.benchmark = True
and
torch.backends.cudnn.deterministic = False
to optimize the running speed of the experiments, as the
cuDNN benchmark searches for the fastest low-level im-
plementations. However, perfect reproducibility cannot be
guaranteed under this setup. To measure runtime, we em-
ploytorch.cuda.Event(enable timing=True) to de-
termine the elapsed time between two CUDA Event records. Weﬁne-tune the numworkers and enable pinmemory for all ex-
periments to achieve faster end-to-end runtimes. For the DeiT and
ResMLP experiments on ImageNet, we enable mixed-precision
training using PyTorch AMP.
Examples of factorized low-rank layers. As discussed, a
full-rank layer Wcan be factorized to obtain UandV>. For
fully connected (or linear) layers in ResMLP, BERT, and DeiT
models, the dimensions of UandV>are straightforward. For
instance, if Wis a(784;784) linear projection implemented using
nn.Linear in PyTorch, then UandV>can be implemented
using (784;r)and(r;784) as input dimensions for nn.Linear
in PyTorch. For convolution layers, we use 11convolution
forV>, following the suggestion of (Wang et al., 2021a). As a
concrete example, when factorizing the 16th convolution layer in
the VGG-19 architecture we used with r= 32 , our factorization
results in Uas a5123233dimensional nn.Conv2d layer
in PyTorch and V>as a3251211dimensional nn.Conv2d
layer in PyTorch. Other factorized low-rank convolution networks
in our implementations follow the same approach.
C D ETAILS ON HYPERPARAMETERS .
In this section, we discuss general-purpose hyperparameters, such
as learning rate and training schedule, used in our experiments for
each task. Additionally, we discuss the hyperparameter setup of
CUTTLEFISH and the details on the ﬁnal ^s2S thatCUTTLEFISH
manages to ﬁnd for each experiment.
C.1 General purpose hyperparameters
CIFAR-10 and CIFAR-100. For the CIFAR-10 and CIFAR-
100 tasks, training on ResNet-18 and VGG-19, we train for T=
300 epochs in total using the SGD optimizer with momentum

--- PAGE 20 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 7. A detailed description of the VGG-19 architecture in our experiments. After each convolution layer, a BatchNorm layer followed
by a ReLU activation is included (though not shown in the table). The shapes for convolution layers are represented as (m;n;k;k ).
Parameter Shape Layer hyperparameter
layer1.conv1.weight 36433 stride: 1;padding: 1
layer2.conv2.weight 646433 stride: 1;padding: 1
pooling.max N/A kernel size: 2;stride: 2
layer3.conv3.weight 6412833 stride: 1;padding: 1
layer4.conv4.weight 12812833 stride: 1;padding: 1
pooling.max N/A kernel size: 2;stride: 2
layer5.conv5.weight 12825633 stride: 1;padding: 1
layer6.conv6.weight 25625633 stride: 1;padding: 1
layer7.conv7.weight 25625633 stride: 1;padding: 1
layer8.conv8.weight 25625633 stride: 1;padding: 1
pooling.max N/A kernel size: 2;stride: 2
layer9.conv9.weight 25651233 stride: 1;padding: 1
layer10.conv10.weight 51251233 stride: 1;padding: 1
layer11.conv11.weight 51251233 stride: 1;padding: 1
layer12.conv12.weight 51251233 stride: 1;padding: 1
pooling.max N/A kernel size: 2;stride: 2
layer13.conv13.weight 51251233 stride: 1;padding: 1
layer14.conv14.weight 51251233 stride: 1;padding: 1
layer15.conv15.weight 51251233 stride: 1;padding: 1
layer16.conv16.weight 51251233 stride: 1;padding: 1
pooling.avg N/A kernel size: 2
classiﬁer.weight 51210 N/A
classiﬁer.bias 10 N/A
(Nesterov momentum disabled). We use a batch size of B=
1;024, scaling an initial learning rate from = 0:1to= 0:8in
5 epochs linearly, following the procedure proposed in (Goyal et al.,
2017). We set momentum and weight decay coefﬁcients at 0:9and
110 4, respectively (weight decay is disabled for BatchNorm
layers). We employ a multi-step learning rate schedule, decaying
the learning rate by a factor of 0:1at the 150-th and 225-th epochs
(to= 0:08and= 0:008respectively). The methodology
we follow for this multi-step learning rate decay is to decay the
learning rate by a factor of 0:1at the points of 50% and75% of
the entire training epochs.
SVHN. For the SVHN task, training on ResNet-18 and VGG-
19, we train for T= 200 epochs in total using the SGD optimizer
with momentum (Nesterov momentum disabled). We train SVHN
for a shorter number of epochs, as it is a relatively easier task
compared to CIFAR-10 and CIFAR-100. We use a batch size
ofB= 1;024, scaling an initial learning rate from = 0:1to
= 0:8in 5 epochs linearly, following the procedure proposed
in (Goyal et al., 2017). We set momentum and weight decay coefﬁ-
cients at 0:9and110 4, respectively (weight decay is disabled
for BatchNorm layers). We employ a multi-step learning rate
schedule, decaying the learning rate by a factor of 0:1at the 100-th
and150-th epochs (to = 0:08and= 0:008respectively).
ImageNet (ILSVRC 2012) on CNNs. For the ImageNet
training task on ResNet-50 and WideResNet-50-2, we train for
T= 90 epochs in total using the SGD optimizer with momentum(Nesterov momentum disabled) and a batch size of 256. We set
momentum and weight decay coefﬁcients at 0:9and110 4,
respectively (weight decay is disabled for BatchNorm layers). We
employ a multi-step learning rate schedule, decaying the learning
rate by a factor of 0:1at the 30-th,60-th, and 80-th epochs (to
= 0:01,= 0:001, and 0:0001 respectively), following (Goyal
et al., 2017). We adopt the label smoothing technique with a
probability of 0:1to achieve better ﬁnal accuracy and to compare
against PUFFERFISH (Wang et al., 2021a). For the comparison
against PUFFERFISH , GraSP, and EBTrain, we disable the learning
rate decay at the 80-th epoch and label smoothing to align the
experiment setup.
ImageNet (ILSVRC 2012) on DeiT and ResMLP. For
ImageNet training on DeiT and ResMLP, we adhere to the training
schedule proposed by (Touvron et al., 2021b), wherein the models
are trained on ImageNet directly from scratch. Our experiments
adopt the model initialization, data augmentation, and Exponential
Moving Average (EMA) methods suggested by (Touvron et al.,
2021b). We do not enable distillation for DeiT and ResMLP. The
AdamW optimizer is used for our experiments (Loshchilov &
Hutter, 2019). More details on the hyperparameter values can
be found at https://github.com/facebookresearch/
deit/blob/main/README_deit.md , where we use the de-
fault hyperparameter setup and set the batch size to 256.
BERT ﬁne-tuning on the GLUE benchmark. We
directly utilize the training script provided by Hugging

--- PAGE 21 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Face at https://github.com/huggingface/
transformers/tree/main/examples/pytorch/
text-classification . We set the maximum sequence
length to 128 and the batch size to 32, using the AdamW
optimizer (Loshchilov & Hutter, 2019) for all downstream tasks
in the GLUE benchmark. For each downstream task in GLUE,
we conduct a small hyperparameter sweep within the range of
f1e 5;2e 5;3e 5;4e 5g, employing early stopping. For
the relatively small downstream task MRPC, we ﬁne-tune for 5
epochs, while for all other downstream tasks, we ﬁne-tune for
3epochs. We disable weight decay and learning rate warm-up
during the ﬁne-tuning process.
C.2 C UTTLEFISH hyperparameters
CIFAR-10, CIFAR-100, and SVHN. For ResNet-18 and
VGG-19 models trained on CIFAR-10, CIFAR-100, and SVHN
datasets, we report the details of the hyperparameters ^s2S dis-
covered by CUTTLEFISH and the manually tuned hyperparameters
used by PUFFERFISH in Table 8. It is evident that CUTTLEFISH
identiﬁes larger Kvalues and smaller Evalues (except for CIFAR-
10) compared to PUFFERFISH for ResNet-18, while for VGG-19,
CUTTLEFISH discovers smaller Evalues with slightly longer E
values than PUFFERFISH . Additionally, the selected ranks, i.e.,
R, for CUTTLEFISH ,PUFFERFISH , and LC compression (only
for VGG-19) methods are illustrated in Figure 7. Notably, CUT-
TLEFISH consistently returns lower estimated ranks for bottom
layers than PUFFERFISH , as these layers contain greater redun-
dancy. The most striking observation from Figure 7 is that the R
values returned by CUTTLEFISH closely align with the explicitly
learnedRvalues of LC compression, demonstrating CUTTLE -
FISH’s effectiveness. Another interesting ﬁnding from Figure 7 is
that more challenging tasks generally require higher ranks for the
factorized low-rank network to achieve satisfactory accuracy. For
example, both CUTTLEFISH and LC compression identify larger
Rvalues for CIFAR-100 and smaller Rvalues for SVHN and
CIFAR-10. This is because CIFAR-100, a 100-class classiﬁcation
task, is more challenging than CIFAR-10 and SVHN for a given
model architecture.
ImageNet (ILSVRC 2012) on CNNs. For ResNet-50 and
WideResNet-50-2 trained on ImageNet, we report the details of the
hyper-parameters ^s2S found by CUTTLEFISH and the manually
tuned hyper-parameters by PUFFERFISH in Table 9. We can ob-
serve that for both ResNet-50 and WideResNet-50-2, CUTTLEFISH
identiﬁes the same Kand longerEcompared to PUFFERFISH . The
ranks (Rs) chosen by CUTTLEFISH andPUFFERFISH can be found
in Figure 8, where it is evident that CUTTLEFISH employs lower
ranks to factorize layers in ResNet-50 and WideResNet50-2 while
using longer full-rank training epochs.
ImageNet (ILSVRC 2012) on DeiT and ResMLP. For
DeiT-base and ResMLP-S36 trained on ImageNet, we report the
details of the hyperparameters ^s2S found by CUTTLEFISH and
the manually tuned hyperparameters by PUFFERFISH in Table 10.
We can observe that for DeiT-base and ResMLP-S36, CUTTLEFISH
identiﬁes the same Kand longerEcompared to PUFFERFISH .
When selecting the ranks Rs, we found that even with scaled stable
rank, the factorized low-rank models still result in a signiﬁcant
ﬁnal accuracy drop. To understand why this occurs, we plot the
cumulative distribution function (CDF) of the singular values of
the Transformer encoder layers in the DeiT-base model at the full-
rank to low-rank switching epoch, i.e.,^E(the results are shown in
Figure 9). If the curves are closer to the reference line, it indicatesthat the model weights are more like full-rank, i.e.,contain less
redundancy.
From Figure 9, we can see that to approximate the major infor-
mation of the weight matrix, e.g., preserving 80% of the singular
value information, relatively higher Rs should be used, e.g.,=1
2.
Additionally, the attention weights, such as Wq,Wk,Wv, as well
as the projection layer after the query, key, and value layers ( i.e.,
Woin our notation) tend to have lower ranks (higher redundancy)
compared to the FFN layers, i.e.,FC1 and FC2 in Figure 9. In
CUTTLEFISH , for DeiT and ResMLP models, we use a global rank
ratio=1
2for all factorized layers. It is worth noting that the
linear projection layer after each Wq,Wk,Wvhas dimensions
of(768;768) . Usingfor this layer, the factorized U;V>layers
will have dimensions of (768;384);(384;768) , which will not
result in any model size reduction or computational complexity
savings. Thus, we opt not to factorize the linear projection layers
in each multi-head attention layer in DeiT-base. For ResMLP-S36,
we factorize all layers except for the embedding layers with a ﬁxed
global rank ratio of =1
2.
A concern arises from the fact that our proposed stable rank se-
lection heuristic for choosing Rmay not generally apply to both
CNN and Transformer models, as Transformer model weights tend
to have higher ranks. To address this issue, future work can adjust
CUTTLEFISH ’s rank selection heuristic to:
maxfscaled stable rank ();
accumulative rank (;p)g
Here, accumulative rank (;p)measures the smallest rank
valuersuch that (where s represent the singular values of a
model weight matrix W, and are also the elements on the diagonal
of matrix ):
rX
i=1iprank(W)X
j=1j:
In the DeiT example mentioned earlier, we know that the
accumulative rank (;80%) for most model layers is gen-
erally greater than1
2rank(W)and the scaled stable rank for
these layers is generally lower than those values. Consequently, the
new metric deﬁned above will consistently return1
2rank(W)for
all factorized layers in the DeiT-base model. Another hyperparam-
eter tuning we performed in our experiments is decaying the base
learning rate by a certain fraction after switching from full-rank
to low-rank training at epoch ^E. For CUTTLEFISH DeiT-base, we
decay the base learning rate by1
3. For CUTTLEFISH ResMLP-S36,
we decay the base learning rate by1
2.
CUTTLEFISH begins factorizing layers after the ﬁrst embedding
layer, which is simply a convolution layer, i.e.,K= 1 for both
DeiT-base and ResMLP-S36. Since we tune KforPUFFER -
FISH such that the end model sizes match CUTTLEFISH DeiT
and ResMLP, PUFFERFISH starts factorizing layers from the 7th
encoder block for DeiT, i.e.,K= 19 (6 blocks, 3 layers in each
block), and the 18th ResMLP block, i.e.,K= 52 (17 blocks, 3
layers in each block).
GLUE Benchmark on BERT BASE.ForBERTBASE ﬁne-
tuning on the GLUE benchmark, the ﬁne-tuning epochs for all
downstream tasks are typically small, such as T= 3;5. Therefore,
we setE= 1 forCUTTLEFISH . Additionally, we free the fully
connected layers following each multi-head attention layer when

--- PAGE 22 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 8. The hyperparameters ^s2S optimized by CUTTLEFISH for ResNet-18 and VGG-19 models trained on CIFAR-10 use a batch
size of 1,024. The runtime benchmark was conducted on a single EC2 p3.2xlarge instance, equipped with one V100 GPU.
CIFAR-10 CIFAR-100 SVHN
Model: ResNet-18 EKEKEK
CUTTLEFISH 82:310:15 55:78:75 61:02:25
PUFFERFISH 80 3 80 3 80 3
SI&FD 0 1 0 1 0 1
Model: VGG-19 EKEKEK
CUTTLEFISH 97:31:2 4 86:05:74 84:00:84
PUFFERFISH 80 9 80 9 80 9
SI&FD 0 1 0 1 0 1
Table 9. The hyperparameters ^s2S obtained by CUTTLEFISH , as well as the manually tuned sfrom PUFFERFISH , for ResNet-50 and
WideResNet-50-2 trained on ImageNet using a batch size of 256.
ImageNet
Model: ResNet-50 EK
CUTTLEFISH 19:30:540
PUFFERFISH 10 40
Model: WideResNet-50-2 EK
CUTTLEFISH 21:30:540
PUFFERFISH 10 40
ﬁne-tuning the factorized BERT BASE. As in LoRA, during the ﬁne-
tuning stage, we do not update the feed-forward network (FFN) in
BERT at all; we freeze the FC1 and FC2 layers contained in the
FFN (Hu et al., 2021). We perform learning rate sweeping over
the learning rates s for all methods during GLUE ﬁne-tuning.
The tuned learning rates are shown in Table 11. For the relatively
challenging CoLA task, we ﬁne-tune all models except for the
vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE
task, we ﬁne-tune Distill BERT for 5 epochs.
C.3 Hyperparameters for other baselines.
SI&FD. We adjust the ﬁxed global rank ratios, denoted as s,
for SI&FD so that the resulting model sizes align with the fac-
torized low-rank models produced by CUTTLEFISH . Detailed
information on the s used in our experiments can be found in
Table 12.
LC compression. The implementation and hyperparameter
conﬁgurations for our experiments are taken directly from the
original GitHub repository6associated with (Idelbayev & Carreira-
Perpin ´an, 2020). We modiﬁed the VGG-19 model implementation
6https://github.com/UCMerced-ML/
LC-model-compressionin the LC compression setup to ensure consistency with the VGG-
19 architecture used in our experiments.
D A DDITIONAL EXPERIMENTAL RESULTS
D.1 Ablation study.
Combining C UTTLEFISH with Frobenius decay. In this
section, we present the results of an ablation study examining the
combination of Frobenius decay (FD) with CUTTLEFISH across
various machine learning tasks. The results can be found in Ta-
ble 13 and Table 14. It is evident that applying FD to CUTTLEFISH
does not consistently lead to better model accuracy. For instance,
combining CUTTLEFISH with FD yields a 1.8% higher accuracy
for ResNet-18 training on CIFAR-100. However, for other tasks,
incorporating FD with ResNet-18 either results in worse ﬁnal
model accuracy or only marginal accuracy improvements. This
observation aligns with the ﬁndings of (V odrahalli et al., 2022),
indicating that FD does not always enhance the accuracy of factor-
ized low-rank models.
The impact of scaled stable rank. As mentioned in the
main paper, the use of stable rank can lead to overly aggressive
low rank estimations, potentially harming the ﬁnal accuracy of
factorized low-rank models. To address this issue, CUTTLEFISH
employs scaled stable rank. The ablation study results are pre-

--- PAGE 23 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
246810121416
Layer Index0100200300400500Rank Selection 
Pufferfish
Full-rank
Cuttlefish
(a) ResNet-18 on CIFAR-10
246810121416
Layer Index0100200300400500Rank Selection 
Pufferfish
Full-rank
Cuttlefish (b) ResNet-18 on CIFAR-100
246810121416
Layer Index0100200300400500Rank Selection 
Pufferfish
Full-rank
Cuttlefish (c) ResNet-18 on SVHN
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Cuttlefish
Pufferfish
Full-rank
(d) VGG-19 on CIFAR-10
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Pufferfish
Full-rank
Cuttlefish (e) VGG-19 on CIFAR-100
246810121416
Layer Index0100200300400500600Rank Selection 
LC Compres.
Pufferfish
Full-rank
Cuttlefish (f) VGG-19 on SVHN
Figure 7. The ranks (R) determined by CUTTLEFISH ,PUFFERFISH , and LC compression (available only for VGG-19 experiments) for
various layers in ResNet-18 ((a), (b), (c)) and VGG-19 ((d), (e), (f)) were trained on CIFAR-10 using a batch size of 1,024.
3638404244464850
Layer Index2004006008001000Rank Selection 
Pufferfish
Full-rank
Cuttlefish
(a) ResNet-50 on ImageNet
3638404244464850
Layer Index2004006008001000Rank Selection 
Pufferfish
Full-rank
Cuttlefish (b) WideResNet-50-20 on ImageNet
Figure 8. The ranksRs obtained by CUTTLEFISH andPUFFERFISH methods for different layers in ResNet-50 (a) and WideResNet-50-2
(b) trained on ImageNet with a batch size of 256.
sented in Table 15. We ﬁnd that for CIFAR-10 and CIFAR-100
datasets, utilizing scaled stable rank is crucial for achieving satis-
factory ﬁnal accuracy in factorized low-rank networks. For SVHN,
which is a comparatively simpler task, even the vanilla stable rank
is sufﬁcient to attain good accuracy. Thus, in our main paper’s
reported experiment, we use vanilla stable rank for SVHN and
scaled stable rank for CIFAR-10 and CIFAR-100. For larger scale
tasks on ImageNet, it is evident that adopting scaled stable rank is
essential; otherwise, the model accuracy will suffer a signiﬁcant
drop, as shown in Table 16.
D.2 Additional experimental results.
BERT pre-training using C UTTLEFISH .We perform
BERT pre-training on the Wikipedia and Bookcorpus datasets,
adhering to the training schedule and codebase of the 24h
BERT LARGE for faster training speed and due to limited computing
resources (Izsak et al., 2021). The results, shown in Table 17, indi-
cate that CUTTLEFISH enables pre-training a BERT model with
only72% of the total model parameters while achieving the same
ﬁnal MLM loss.Rank varying trend of other datasets. In our main paper,
we presented the rank varying trend only for ResNet-18 on CIFAR-
10. In this section, we expand our analysis and report additional
experimental results on rank varying trends. Speciﬁcally, we pro-
vide results for VGG-19 trained on CIFAR-10, as well as VGG-19
and ResNet-18 on CIFAR-100 and SVHN datasets. For ResNet-50
on ImageNet, we also show the same results. Figures 10 and 14
display the results for VGG-19 on CIFAR-10, while Figures 11,
15, 16, 13, and 17 illustrate the results for the remaining datasets.
Overall, our observations indicate that the stable rank of the net-
work layers ﬂuctuates signiﬁcantly in the early stages of training
but eventually converges to a constant value. This trend holds
across all the datasets we analyzed.
Comparison of C UTTLEFISH to EB Train and GraSP.
Furthermore, we compare CUTTLEFISH with two other state-of-
the-art approaches, namely EB Train and GraSP (as shown in
Table 18). Notably, CUTTLEFISH outperforms both EB Train and
GraSP in terms of accuracy while achieving smaller model sizes.
ResNet-18 and VGG-19 Experiments on SVHN. The re-
sults of ResNet-18 and VGG-19 experiments on the SVHN dataset

--- PAGE 24 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 10. The tuned hyperparameters ^s2S, as determined by CUTTLEFISH , and the manually tuned sfrom PUFFERFISH for DeiT-base
and ResMLP-S36, trained on the ImageNet dataset using a batch size of 256.
ImageNet
Model: DeiT-base EK
CUTTLEFISH 59:56:51
PUFFERFISH 80 19
Model: ResMLP-S36 EK
CUTTLEFISH 40:51:51
PUFFERFISH 80 52
0 20 40 60 80 100
Dim Percentage (%)020406080100Sig Val Percentage (%)
layer0-QKV
layer0-Proj
layer0-FC1layer0-FC2
ref. line
(a) Encoder 0 of DeiT-base
0 20 40 60 80 100
Dim Percentage (%)020406080100Sig Val Percentage (%)
layer11-QKV
layer11-Proj
layer11-FC1layer11-FC2
Ref. line (b) Encoder 11 of DeiT-base
Figure 9. The Cumulative Distribution Function (CDF) of singular values for the ﬁrst Transformer encoder ( i.e.,Encoder 0, denoted as
layer0 in the ﬁgure) (a) and the last Transformer encoder ( i.e.,Encoder 11, denoted as layer11 in the ﬁgure) (b) of DeiT-base trained on
the ImageNet dataset using a batch size of 256. Other Transformer encoders exhibit similar trends.
are presented in Table 19.

--- PAGE 25 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 11. Tuned learning rates, denoted as s, for vanilla BERT BASE, Distill BERT , Tiny BERT 6, and CUTTLEFISH BERTBASE on the
GLUE benchmark.
Model # Params. (M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B
BERT BASE 108:3 2e-5 2e-5 2e-5 4e-5 2e-5 2e-5 4e-5 2e-5
Distill BERT 65:8 2e-5 2e-5 2e-5 2e-5 4e-5 2e-5 2e-5 2e-5
Tiny BERT 6 67:0 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5
CUTTLEFISH 48:8 2e-5 2e-5 2e-5 3e-5 3e-5 2e-5 3e-5 2e-5
Table 12. The ﬁxed rank ratios ( s) employed in SI&FD experiments for CIFAR-10, CIFAR-100, and SVHN on ResNet-18 and VGG-19.
Model: ResNet-18 CIFAR-10 CIFAR-100 SVHN
SI&FD 0.08 0.105 0.032
Model: VGG-19 CIFAR-10 CIFAR-100 SVHN
SI&FD 0.1 0.165 0.059
Table 13. The ablation study results, averaged across three independent trials with different random seeds, showcase the performance of
CUTTLEFISH combined with Frobenius decay (FD) on ResNet-18 and VGG-19 trained on CIFAR-10 using a batch size of 1,024.
CIFAR-10 CIFAR-100 SVHN
Model: # Params. Val. Acc. # Params. Val. Acc. # Params. Val. Acc.
ResNet-18 (M) (%) (M) (%) (M) (%)
CUTTLEFISH wo. FD 2:0 94:520:01 2:6 73:750:24 0:96 96:470:02
CUTTLEFISH w. FD 2:0 94:620:09 2:6 75:540:18 0:94 96:340:08
Model: # Params. Val. Acc. # Params. Val. Acc. # Params. Val. Acc.
VGG-19 (M) (%) (M) (%) (M) (%)
CUTTLEFISH wo. FD 1:9 93:490:18 3:3 72:270:25 1:2 96:330:04
CUTTLEFISH w. FD 1:9 93:420:25 3:3 72:150:27 1:2 96:330:02
Table 14. The ablation study results, averaged across three independent trials, demonstrate the performance of CUTTLEFISH combined
with Frobenius decay (FD) on ResNet-50 trained on ImageNet using a batch size of 256.
Model: ResNet-50 # Params. (M) Top-1 Val. Acc. ( %) Top-5 Val. Acc. ( %)
CUTTLEFISH wo. FD 14:7 76 :160:04 92:970:06
CUTTLEFISH w. FD 14:7 76:440:16 93:210:03
Table 15. The ablation study results (averaged over 3independent trials with varying random seeds) for CUTTLEFISH using both scaled
stable rank and vanilla stable rank on ResNet-18 and VGG-19, trained on CIFAR-10, CIFAR-100, and SVHN with a batch size of 1;024.
CIFAR-10 CIFAR-100 SVHN
Model: # Params. Val. Acc. # Params. Val. Acc. # Params. Val. Acc.
ResNet-18 (M) (%) (M) (%) (M) (%)
CUTTLEFISH vanilla stable rank 1:2 94:270:05 1:6 74:210:20 0:94 96:470:02
CUTTLEFISH scaled stable rank 2:0 94:620:09 2:6 75:540:18 1:4 96:530:08
Model: # Params. Val. Acc. # Params. Val. Acc. # Params. Val. Acc.
VGG-19 (M) (%) (M) (%) (M) (%)
CUTTLEFISH vanilla stable rank 1:1 93:070:10 1:9 70:620:23 1:2 96:330:04
CUTTLEFISH scaled stable rank 1:9 93:490:18 3:3 72:270:25 2:0 96:420:07

--- PAGE 26 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 16. The ablation study results (averaged over 3independent trials with distinct random seeds) for CUTTLEFISH using both scaled
stable rank and vanilla stable rank on DeiT-base, ResNet-50, and WideResNet-50, trained on ImageNet with a batch size of 256.
# Params. (M) Top-1 Val. Acc. Top-5 Val. Acc.
CUTTLEFISH DeiT-base vanilla stable rank 12:5 64 :800:82 85:460:60
CUTTLEFISH DeiT-base scaled stable rank 58:3 81:520:03 95:590:04
CUTTLEFISH WideResNet-50 vanilla stable rank 29:1 76 :860:01 93:500:03
CUTTLEFISH WideResNet-50 scaled stable rank 37:4 78:00:06 94:040:09
CUTTLEFISH ResNet-50 vanilla stable rank 11:9 74 :960:01 92:390:07
CUTTLEFISH ResNet-50 scaled stable rank 14:7 76:440:16 93:210:03
Table 17. Vanilla and C UTTLEFISH BERT pre-training on Wikipedia and Bookcorpus datasets.
Model # Params. (M) MLM Loss
Vanilla BERT LARGE 345 1:58
Cuttleﬁsh BERT LARGE 249 1 :6
0.0 2.5 5.0 7.5 10.0 12.5
% of the T otal Training Epochs0.00.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 20 40 60 80 100 120
Epochs0.00.20.40.6
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
Layer-16
0 20 40 60 80 100 120
Epochs0.00.20.40.6
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
0 50 100 150 200 250
Epochs0.00.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 50 100 150 200 250
Epochs0.00.20.4
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
Layer-16
0 50 100 150 200 250
Epochs0.00.20.4
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
Figure 10. The stable ranks for various layers in ResNet-18 ( the top row ) and VGG-19 ( the bottom row ) trained on CIFAR-10 using
stable rank.
0 50 100 150 200 250
Epochs0.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 50 100 150 200 250
Epochs0.00.20.40.6
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
Layer-16
0 50 100 150 200 250
Epochs0.00.20.40.6
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
0 50 100 150 200 250
Epochs0.20.40.6Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 50 100 150 200 250
Epochs0.20.40.6
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
0 50 100 150 200 250
Epochs0.00.20.4
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
Figure 11. The stable ranks for various layers in ResNet-18 ( the top row ) and VGG-19 ( the bottom row ) trained on CIFAR-100 using
stable rank with batch size 1;024.

--- PAGE 27 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
0 50 100 150 200
Epochs0.00.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 25 50 75 100 125 150 175 200
Epochs0.00.20.40.6
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
Layer-16
0 25 50 75 100 125 150 175 200
Epochs0.00.20.40.6
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
0 50 100 150 200
Epochs0.00.20.4Rank Ratio
Layer-0
Layer-3
Layer-6Layer-9
Layer-12
Layer-15
0 25 50 75 100 125 150 175 200
Epochs0.00.20.4
Layer-1
Layer-4
Layer-7Layer-10
Layer-13
0 25 50 75 100 125 150 175 200
Epochs0.00.20.4
Layer-2
Layer-5
Layer-8Layer-11
Layer-14
Figure 12. The stable ranks for various layers in ResNet-18 ( the top row ) and VGG-19 ( the bottom row ) trained on SVHN using stable
rank with batch size 1;024.
0 20 40 60 80
Epochs0.00.10.20.30.4Rank Ratio
Layer-32
Layer-35
Layer-38Layer-41
Layer-44
Layer-47
0 10 20 30 40 50 60 70 80
Epochs0.10.20.30.4
 Layer-33
Layer-36
Layer-39Layer-42
Layer-45
Layer-48
0 20 40 60 80
Epochs0.050.100.150.20
Layer-34
Layer-37
Layer-40Layer-43
Layer-46
Layer-49
Figure 13. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank with batch size 256.
1591317212529333741454953576165697377818589
Epochs1
3
5
7
9
11
13
15
17Layer Index
 0.050.100.150.20
(a) ResNet-18
5 913 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81
Epochs1
3
5
7
9
11
13
15
17Layer Index
0.10.2
(b) VGG-19
Figure 14. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-10 using stable rank.
Table 18. Comparison of CUTTLEFISH and other baseline methods: PUFFERFISH , EB Train ( 30% ,50% ) and GraSP ( 30% ,60% ) over the
task of ResNet-50 on ImageNet.
Model # Params. Val. Acc. Val. Acc.
ResNet-50 (M) Top-1(%) Top-5(%)
Full-rank 25:6 75:99 92:98
PUFFERFISH 15:2 75:62 92:55
EB Train ( 30% ) 16:5 73:86 91:52
EB Train ( 50% ) 15:1 73:35 91:36
GraSP ( 30% ) 17:9 74:64 92:08
GraSP ( 60% ) 10:2 74:02 91:86
CUTTLEFISH 14:7 75:80 92:70

--- PAGE 28 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
1591317212529333741454953576165697377818589
Epochs1
3
5
7
9
11
13
15
17Layer Index
0.10.20.30.4
(a) ResNet-18
5 913 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81
Epochs1
3
5
7
9
11
13
15Layer Index
0.10.2
(b) VGG-19
Figure 15. The stable ranks for various layers in ResNet-18 and VGG-19 trained on CIFAR-100 using stable rank.
1591317212529333741454953576165697377818589
Epochs1
3
5
7
9
11
13
15
17Layer Index
0.050.100.15
(a) ResNet-18
5 913 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77
Epochs1
3
5
7
9
11
13
15Layer Index
 0.10.20.30.4
(b) VGG-19
Figure 16. The stable ranks for various layers in ResNet-18 and VGG-19 trained on SVHN using stable rank.
1 5 913 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77
Epochs32
34
36
38
40
42
44
46
48
50Layer Index
0.050.100.150.20
Figure 17. The stable ranks for various layers in ResNet-50 trained on ImageNet using stable rank.

--- PAGE 29 ---
CUTTLEFISH : Low-rank Model Training without All The Tuning
Table 19. The results, averaged over three independent trials with different random seeds, showcase the performance of CUTTLEFISH and
other baselines on ResNet-18 and VGG-19 trained on the SVHN dataset using a batch size of 1,024. Runtime benchmarks are conducted
on a single EC2 p3.2xlarge instance.
Model: Params. Val. Acc. Time
ResNet-18 (M) (%) (hrs.)
Full-rank 11:2 96:270:08 0:81
PUFFERFISH 3:3 96:540:06 0:71
SI&FD 0:94 96:450:04 0:38
IMP 0:96 96:430:01 9:40
CUTTLEFISH 0:96 96:470:02 0:65
CUTTLEFISH +FD 0:94 96:340:08 0:65
Model: Params. Val. Acc. Time
VGG-19 (M) (%) (hrs.)
Full-rank 20:0 96:310:05 0:49
PUFFERFISH 8:1 96:080:11 0:45
SI&FD 1:2 96:040:16 0:30
FC Compress. 1:1 96:420:07 6:68
CUTTLEFISH 1:2 96:330:04 0:39
CUTTLEFISH +FD 1:2 96:330:02 0:39
