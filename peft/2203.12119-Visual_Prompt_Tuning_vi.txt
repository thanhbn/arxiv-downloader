# Tinh chỉnh Prompt Thị giác
Menglin Jia∗1,2, Luming Tang∗1
Bor-Chun Chen2, Claire Cardie1, Serge Belongie3
Bharath Hariharan1, và Ser-Nam Lim2
1Đại học Cornell2Meta AI3Đại học Copenhagen
Tóm tắt. Phương thức hoạt động hiện tại trong việc thích ứng các mô hình đã được tiền huấn luyện bao gồm việc cập nhật tất cả các tham số xương sống, tức là tinh chỉnh hoàn toàn. Bài báo này giới thiệu Tinh chỉnh Prompt Thị giác (VPT) như một giải pháp thay thế hiệu quả và hiệu suất cho tinh chỉnh hoàn toàn đối với các mô hình Transformer quy mô lớn trong thị giác. Lấy cảm hứng từ những tiến bộ gần đây trong việc tinh chỉnh hiệu quả các mô hình ngôn ngữ lớn, VPT chỉ giới thiệu một lượng nhỏ (ít hơn 1% tham số mô hình) các tham số có thể huấn luyện trong không gian đầu vào trong khi giữ nguyên xương sống mô hình. Thông qua các thí nghiệm rộng rãi trên nhiều nhiệm vụ nhận dạng phụ thuộc khác nhau, chúng tôi chỉ ra rằng VPT đạt được những cải thiện hiệu suất đáng kể so với các giao thức tinh chỉnh hiệu quả tham số khác. Quan trọng nhất, VPT thậm chí còn vượt trội hơn tinh chỉnh hoàn toàn trong nhiều trường hợp qua các quy mô khả năng mô hình và quy mô dữ liệu huấn luyện, đồng thời giảm chi phí lưu trữ cho mỗi nhiệm vụ. Mã nguồn có sẵn tại github.com/kmnp/vpt.

1 Giới thiệu
Đối với nhiều ứng dụng nhận dạng, kết quả chính xác nhất hiện nay được thu được bằng cách thích ứng các mô hình nền tảng lớn được tiền huấn luyện trên dữ liệu thô hoặc được tuyển chọn khổng lồ, một phát hiện phản ánh sự phát triển trong xử lý ngôn ngữ tự nhiên (NLP) [6].¹ Thoạt nhìn, đây là một câu chuyện thành công: người ta có thể tiến bộ nhanh chóng trong nhiều vấn đề nhận dạng đơn giản bằng cách tận dụng mô hình nền tảng mới nhất và tốt nhất. Tuy nhiên, trong thực tế, việc thích ứng các mô hình lớn này với các nhiệm vụ phụ thuộc đưa ra những thách thức riêng. Chiến lược thích ứng rõ ràng nhất (và thường hiệu quả nhất) là tinh chỉnh hoàn toàn mô hình đã được tiền huấn luyện trên nhiệm vụ đang xử lý, từ đầu đến cuối. Tuy nhiên, chiến lược này yêu cầu lưu trữ và triển khai một bản sao riêng biệt của các tham số xương sống cho mỗi nhiệm vụ duy nhất. Đây là một đề xuất đắt đỏ và thường không khả thi, đặc biệt đối với các kiến trúc dựa trên Transformer hiện đại, có quy mô lớn hơn đáng kể so với các đối tác mạng nơ-ron tích chập (ConvNet), ví dụ: ViT-Huge [19] (632 triệu tham số) so với ResNet-50 [31] (25 triệu tham số). Do đó, chúng tôi đặt câu hỏi: cách tốt nhất để thích ứng các Transformer lớn đã được tiền huấn luyện với các nhiệm vụ phụ thuộc về hiệu quả và hiệu suất là gì?

∗Đóng góp bằng nhau.
¹Như được chỉ ra trong [6], tất cả các mô hình tiên tiến trong NLP đương đại hiện nay đều được hỗ trợ bởi một vài mô hình dựa trên Transformer (ví dụ: BERT [17], T5 [66], BART [46], GPT-3 [7]) Điều này cũng áp dụng cho lĩnh vực thị giác-ngôn ngữ gần đây, tức là CLIP [65].

2 M. Jia et al.
Xương sống
(a) Giao thức tinh chỉnh hiện tại (c) Kết quả trên các nhiệm vụ phân loại thị giác (b) Tinh chỉnh Prompt Thị giác (VPT)
Đầu
Đầu
Xương sống
Đầu
Hướng đầu:
Hướng xương sống: Prompt
Tinh chỉnh Đông lạnh
Tuyến tính Từng phần MLP
Sidetune Adapter Bias
Hoàn toàn
Của chúng tôi

Hình 1. Tinh chỉnh Prompt Thị giác (VPT) so với các phương pháp học chuyển giao khác. (a) Các giao thức học chuyển giao hiện tại được nhóm dựa trên phạm vi tinh chỉnh: Tinh chỉnh hoàn toàn, phương pháp hướng đầu và hướng xương sống. (b) VPT thay vào đó thêm các tham số bổ sung trong không gian đầu vào. (c) Hiệu suất của các phương pháp khác nhau trên một loạt các nhiệm vụ phân loại phụ thuộc thích ứng xương sống ViT-B đã được tiền huấn luyện, với trung bình và độ lệch chuẩn được chú thích. VPT vượt trội hơn tinh chỉnh hoàn toàn trong 20 trên 24 trường hợp trong khi chỉ sử dụng ít hơn 1% tổng tham số mô hình

Một phương pháp đơn giản là chuyển sang các chiến lược khác mà chúng ta đã hoàn thiện để thích ứng ConvNet với các nhiệm vụ mới, như trong Hình 1(a). Một phương pháp phổ biến là chỉ tinh chỉnh một tập con của các tham số, chẳng hạn như đầu phân loại [56,36,11] hoặc các số hạng bias [8]. Nghiên cứu trước đây cũng đã xem xét việc thêm các khối dư (hoặc adapter) vào xương sống [68,87]. Người ta có thể triển khai các chiến lược tương tự cho Transformer. Tuy nhiên, nói chung các chiến lược này hoạt động kém hơn tinh chỉnh hoàn toàn về độ chính xác.

Chúng tôi khám phá một con đường khác trong bài báo này. Thay vì thay đổi hoặc tinh chỉnh chính Transformer đã được tiền huấn luyện, chúng tôi sửa đổi đầu vào cho Transformer. Lấy cảm hứng từ những tiến bộ gần đây về Prompting trong NLP [50,48,45,51], chúng tôi đề xuất một phương pháp mới đơn giản và hiệu quả để thích ứng các mô hình transformer cho các nhiệm vụ thị giác phụ thuộc (Hình 1(b)), cụ thể là Tinh chỉnh Prompt Thị giác (VPT). Phương pháp của chúng tôi chỉ giới thiệu một lượng nhỏ các tham số có thể học dành riêng cho nhiệm vụ vào không gian đầu vào trong khi đông lạnh toàn bộ xương sống Transformer đã được tiền huấn luyện trong quá trình huấn luyện phụ thuộc. Trong thực tế, các tham số bổ sung này được thêm vào trước chuỗi đầu vào của mỗi lớp Transformer và được học cùng với một đầu tuyến tính trong quá trình tinh chỉnh.

Trên 24 nhiệm vụ nhận dạng phụ thuộc trải rộng các miền khác nhau sử dụng xương sống ViT đã được tiền huấn luyện, VPT đánh bại tất cả các đường cơ sở học chuyển giao khác, thậm chí vượt trội hơn tinh chỉnh hoàn toàn trong 20 trường hợp, trong khi duy trì ưu thế của việc lưu trữ ít tham số hơn đáng kể (ít hơn 1% tham số xương sống) cho mỗi nhiệm vụ riêng lẻ (Hình 1(c)). Kết quả này chứng minh sức mạnh đặc biệt của prompting thị giác: trong khi ở NLP, tinh chỉnh prompt chỉ có thể đạt hiệu suất tinh chỉnh hoàn toàn trong một số trường hợp nhất định [45]. VPT đặc biệt hiệu quả trong chế độ dữ liệu thấp và duy trì ưu thế qua các quy mô dữ liệu. Cuối cùng, VPT có tính cạnh tranh cho một loạt các quy mô và thiết kế Transformer (ViT-Base/Large/Huge, Swin). Tổng hợp lại, kết quả của chúng tôi gợi ý rằng VPT là một trong những cách hiệu quả nhất để thích ứng các xương sống thị giác ngày càng phát triển.

3 Tinh chỉnh Prompt Thị giác

2 Công trình liên quan
Các mô hình Transformer [73] đã đạt được thành công rất lớn trong NLP [17,66,7]. Thành công của kiến trúc Transformer cũng mở rộng sang các nhiệm vụ thị giác máy tính khác nhau, bao gồm phân loại hình ảnh [19,52], phát hiện đối tượng [9,49], phân đoạn ngữ nghĩa và toàn cảnh [71,89,78], hiểu video [25,79,21] và học few-shot [18], vượt trội hơn các phương pháp tiên tiến trước đây. Transformer cũng đang được sử dụng rộng rãi trong các phương pháp tiền huấn luyện tự giám sát gần đây [11,30,3]. Với hiệu suất vượt trội và quy mô lớn hơn nhiều so với ConvNet, cách thích ứng hiệu quả Transformer với các nhiệm vụ thị giác khác nhau vẫn là một vấn đề mở quan trọng. VPT được đề xuất của chúng tôi cung cấp một con đường hứa hẹn phía trước.

Học chuyển giao đã được nghiên cứu rộng rãi cho các nhiệm vụ thị giác trong bối cảnh ConvNet [92] và nhiều kỹ thuật đã được giới thiệu bao gồm side tuning [87], residual adapter [67], bias tuning [8], v.v. Tương đối ít sự chú ý đã được dành cho việc thích ứng Transformer thị giác và mức độ hoạt động tốt của các phương pháp nói trên trên loại kiến trúc hoàn toàn mới này vẫn chưa được biết. Mặt khác, với sự thống trị của các Mô hình Ngôn ngữ (LM) dựa trên Transformer được tiền huấn luyện quy mô lớn [17,66,7], nhiều phương pháp [29,28,35] đã được đề xuất để tinh chỉnh hiệu quả LM cho các nhiệm vụ NLP phụ thuộc khác nhau [77,76]. Trong số đó, chúng tôi tập trung vào hai phương pháp đại diện sau trong các thí nghiệm của chúng tôi cho mục đích đánh giá chuẩn: Adapter [64] và BitFit [5].

Adapter [34] chèn các mô-đun nhẹ bổ sung bên trong mỗi lớp Transformer. Một mô-đun adapter thường bao gồm một phép chiếu tuyến tính xuống, tiếp theo là một hàm kích hoạt phi tuyến tính và một phép chiếu tuyến tính lên, cùng với một kết nối dư [63,64]. Thay vì chèn các mô-đun mới, [8] đề xuất cập nhật số hạng bias và đông lạnh các tham số xương sống còn lại khi tinh chỉnh ConvNet. BitFit [3] áp dụng kỹ thuật này cho Transformer và xác minh hiệu quả của nó trong việc tinh chỉnh LM. Nghiên cứu của chúng tôi chứng minh rằng VPT, nói chung, cung cấp hiệu suất cải thiện trong việc thích ứng các mô hình Transformer cho các nhiệm vụ thị giác, so với hai phương pháp được thiết lập tốt nói trên trong NLP.

Prompting [50] ban đầu đề cập đến việc thêm hướng dẫn ngôn ngữ vào trước văn bản đầu vào để một LM được tiền huấn luyện có thể "hiểu" nhiệm vụ. Với các prompt được chọn thủ công, GPT-3 cho thấy khả năng tổng quát hóa mạnh mẽ đến các nhiệm vụ học chuyển giao phụ thuộc ngay cả trong các thiết lập few-shot hoặc zero-shot [7]. Ngoài các công trình tiếp theo về cách xây dựng các văn bản prompting tốt hơn [70,37], các công trình gần đây đề xuất coi các prompt như các vectơ liên tục dành riêng cho nhiệm vụ và tối ưu hóa chúng trực tiếp thông qua gradient trong quá trình tinh chỉnh, cụ thể là Tinh chỉnh Prompt [48,45,51]. So với tinh chỉnh hoàn toàn, nó đạt hiệu suất tương đương nhưng với việc lưu trữ tham số ít hơn 1000 lần. Mặc dù prompting cũng đã được áp dụng cho các mô hình thị giác-ngôn ngữ gần đây [65,91,39,84,22], prompting vẫn giới hạn ở đầu vào của các bộ mã hóa văn bản. Do sự khác biệt giữa các phương thức thị giác và ngôn ngữ, trong bài báo này chúng tôi đặt câu hỏi: liệu cùng một phương pháp có thể được áp dụng thành công cho các bộ mã hóa hình ảnh hay không? Chúng tôi là công trình đầu tiên (xem các công trình đồng thời liên quan [69,80,14,2]) giải quyết câu hỏi này và điều tra tính tổng quát và khả thi của prompting thị giác thông qua các thí nghiệm rộng rãi trải rộng nhiều loại nhiệm vụ nhận dạng qua nhiều miền và kiến trúc xương sống.

4 M. Jia et al.

Hình 2. Tổng quan về Tinh chỉnh Prompt Thị giác được đề xuất của chúng tôi. Chúng tôi khám phá hai biến thể: (a) thêm một tập hợp các tham số có thể học vào trước đầu vào của mỗi lớp mã hóa Transformer (VPT-deep); (b) chỉ chèn các tham số prompt vào đầu vào của lớp đầu tiên (VPT-shallow). Trong quá trình huấn luyện trên các nhiệm vụ phụ thuộc, chỉ các tham số của prompt và đầu tuyến tính được cập nhật trong khi toàn bộ bộ mã hóa Transformer được đông lạnh.

3 Phương pháp

Chúng tôi đề xuất Tinh chỉnh Prompt Thị giác (VPT) để thích ứng các mô hình Transformer thị giác lớn đã được tiền huấn luyện. VPT tiêm một số lượng nhỏ các tham số có thể học vào không gian đầu vào của Transformer và giữ xương sống đông lạnh trong giai đoạn huấn luyện phụ thuộc. Khung tổng thể được trình bày trong Hình 2. Chúng tôi đầu tiên định nghĩa các ký hiệu trong Mục 3.1, sau đó mô tả VPT một cách chính thức trong Mục 3.2.

3.1 Kiến thức cơ bản

Đối với một Vision Transformer (ViT) [19] thuần túy với N lớp, một hình ảnh đầu vào được chia thành m patch có kích thước cố định {Ij∈R³×h×w|j∈N,1≤j≤m}. h, w là chiều cao và chiều rộng của các patch hình ảnh. Mỗi patch sau đó được nhúng vào không gian ẩn d chiều với mã hóa vị trí:

ej⁰=Embed(Ij) ej⁰∈Rᵈ, j= 1,2, . . . m . (1)

Chúng tôi ký hiệu tập hợp các embedding patch hình ảnh, Ei={ejⁱ∈Rᵈ|j∈N,1≤j≤m}, như các đầu vào cho lớp Transformer thứ (i+1) (Li+1). Cùng với một token phân loại có thể học bổ sung ([CLS]), toàn bộ ViT được công thức hóa như sau:

[xi,Ei] =Li([xi-1,Ei-1]) i= 1,2, . . . , N (2)
y=Head(xN), (3)

trong đó xi∈Rᵈ biểu thị embedding của [CLS] tại không gian đầu vào của Li+1. [·,·] chỉ việc xếp chồng và nối trên chiều độ dài chuỗi, tức là [xi,Ei]∈R⁽¹⁺ᵐ⁾×ᵈ. Mỗi lớp Li bao gồm Multiheaded Self-Attention (MSA) và Feed-Forward Networks (FFN) cùng với LayerNorm [1] và các kết nối dư [31]. Một đầu phân loại thần kinh được sử dụng để ánh xạ embedding [CLS] của lớp cuối cùng, xN, thành một phân phối xác suất lớp dự đoán y.²

3.2 Tinh chỉnh Prompt Thị giác (VPT)

Với một mô hình Transformer đã được tiền huấn luyện, chúng tôi giới thiệu một tập hợp p embedding liên tục có chiều d, tức là prompt, trong không gian đầu vào sau lớp Embed. Chỉ các prompt dành riêng cho nhiệm vụ được cập nhật trong quá trình tinh chỉnh, trong khi xương sống Transformer được giữ đông lạnh. Tùy thuộc vào số lượng lớp Transformer liên quan, phương pháp của chúng tôi có hai biến thể, VPT-shallow và VPT-deep, như được hiển thị trong Hình 2.

VPT-Shallow. Prompt được chèn vào chỉ lớp Transformer đầu tiên L1. Mỗi token prompt là một vectơ d chiều có thể học. Một tập hợp p prompt được ký hiệu là P={pk∈Rᵈ|k∈N,1≤k≤p}, ViT được prompt nông là:

[x1,Z1,E1] =L1([x0,P,E0]) (4)
[xi,Zi,Ei] =Li([xi-1,Zi-1,Ei-1]) i= 2,3, . . . , N (5)
y=Head(xN), (6)

trong đó Zi∈Rᵖ×ᵈ biểu thị các đặc trưng được tính toán bởi lớp Transformer thứ i, và [xi,Zi,Ei]∈R⁽¹⁺ᵖ⁺ᵐ⁾×ᵈ. Các màu • và • lần lượt chỉ các tham số có thể học và đông lạnh. Đáng chú ý đối với ViT, xN không thay đổi với vị trí của prompt vì chúng được chèn sau mã hóa vị trí, ví dụ: [x0,P,E0] và [x0,E0,P] là tương đương về mặt toán học. Điều này cũng áp dụng cho VPT-Deep.

VPT-Deep. Prompt được giới thiệu tại không gian đầu vào của mỗi lớp Transformer. Đối với lớp thứ (i+1) Li+1, chúng tôi ký hiệu tập hợp các prompt có thể học đầu vào là Pi={pkⁱ∈Rᵈ|k∈N,1≤k≤m}. ViT được prompt sâu được công thức hóa như sau:

[xi,Ei] =Li([xi-1,Pi-1,Ei-1]) i= 1,2, . . . , N (7)
y=Head(xN). (8)

Lưu trữ Prompt Thị giác. VPT có lợi khi có nhiều nhiệm vụ phụ thuộc. Chúng tôi chỉ cần lưu trữ các prompt đã học và đầu phân loại cho mỗi nhiệm vụ và tái sử dụng bản sao gốc của mô hình Transformer đã được tiền huấn luyện, giảm đáng kể chi phí lưu trữ. Ví dụ, với một ViT-Base có 86 triệu (M) tham số và d= 768, 50 prompt nông và sâu tạo ra thêm p×d= 50×768 = 0.038M và N×p×d= 0.46M tham số, tương ứng chỉ 0.04% và 0.53% của tất cả tham số ViT-Base.

²Một số kiến trúc Transformer trong Thị giác như Swin [52] không sử dụng [CLS] và coi EN được gộp toàn cục làm đầu vào cho Head. Chúng tôi theo thiết kế của chúng khi thích ứng VPT với các biến thể Transformer này. Xem Phụ lục A để biết thêm chi tiết.

6 M. Jia et al.

4 Thí nghiệm

Chúng tôi đánh giá VPT cho một loạt rộng các nhiệm vụ nhận dạng phụ thuộc với các xương sống Transformer đã được tiền huấn luyện qua các quy mô. Chúng tôi đầu tiên mô tả thiết lập thí nghiệm trong Mục 4.1, bao gồm xương sống đã được tiền huấn luyện và các nhiệm vụ phụ thuộc, và giới thiệu ngắn gọn về các phương pháp học chuyển giao thay thế. Sau đó chúng tôi chứng minh hiệu quả và tiện ích thực tế của phương pháp của chúng tôi trong Mục 4.2. Chúng tôi cũng nghiên cứu có hệ thống cách các lựa chọn thiết kế khác nhau sẽ ảnh hưởng đến hiệu suất (Mục 4.3), điều này dẫn đến hiểu biết cải thiện về phương pháp của chúng tôi.

4.1 Thiết lập thí nghiệm

Xương sống đã được tiền huấn luyện. Chúng tôi thí nghiệm với hai kiến trúc Transformer trong thị giác, Vision Transformer (ViT) [19] và Swin Transformer (Swin [52]). Tất cả các xương sống trong phần này đều được tiền huấn luyện trên ImageNet-21k [16]. Chúng tôi tuân theo các cấu hình gốc, ví dụ: số lượng patch hình ảnh được chia, sự tồn tại của [CLS], v.v. Thêm chi tiết được bao gồm trong Phụ lục A.

Đường cơ sở. Chúng tôi so sánh cả hai biến thể của VPT với các giao thức tinh chỉnh thường được sử dụng khác:

(a) Full: cập nhật hoàn toàn tất cả tham số xương sống và đầu phân loại.

(b) Phương pháp tập trung vào đầu phân loại. Chúng coi xương sống đã được tiền huấn luyện như một bộ trích xuất đặc trưng, có trọng số cố định trong quá trình tinh chỉnh:
- Linear: chỉ sử dụng một lớp tuyến tính làm đầu phân loại.
- Partial-k: tinh chỉnh k lớp cuối cùng của xương sống trong khi đông lạnh các lớp khác, như được áp dụng trong [85,88,60,30]. Nó định nghĩa lại ranh giới của xương sống và đầu phân loại.
- Mlp-k: sử dụng một perceptron đa lớp (MLP) với k lớp, thay vì một lớp tuyến tính, làm đầu phân loại.

(c) Phương pháp cập nhật một tập con tham số xương sống hoặc thêm tham số có thể huấn luyện mới vào xương sống trong quá trình tinh chỉnh:
- Sidetune [87]: huấn luyện một mạng "bên" và nội suy tuyến tính giữa các đặc trưng được tiền huấn luyện và các đặc trưng được tinh chỉnh bên trước khi được đưa vào đầu.
- Bias [8,5]: chỉ tinh chỉnh các số hạng bias của xương sống đã được tiền huấn luyện.
- Adapter [34,63,64]: chèn các mô-đun MLP mới với kết nối dư bên trong các lớp Transformer.

Nhiệm vụ phụ thuộc. Chúng tôi thí nghiệm trên hai bộ sưu tập dataset sau:

FGVC bao gồm 5 nhiệm vụ Phân loại Thị giác Tinh vi được đánh giá chuẩn bao gồm CUB-200-2011 [75], NABirds [72], Oxford Flowers [59], Stanford Dogs [41] và Stanford Cars [23]. Nếu một dataset nhất định chỉ có tập huấn luyện và kiểm tra có sẵn công khai, chúng tôi chia ngẫu nhiên tập huấn luyện thành huấn luyện (90%) và val (10%), và dựa vào val để chọn siêu tham số.

VTAB-1k [86] là một bộ sưu tập 19 nhiệm vụ phân loại thị giác đa dạng, được tổ chức thành ba nhóm: Natural - các nhiệm vụ chứa hình ảnh tự nhiên được chụp bằng camera tiêu chuẩn; Specialized - các nhiệm vụ chứa hình ảnh được chụp qua thiết bị chuyên dụng, như hình ảnh y tế và vệ tinh; và Structured - các nhiệm vụ yêu cầu hiểu biết hình học như đếm đối tượng. Mỗi nhiệm vụ có 1000 ví dụ huấn luyện. Theo [86], chúng tôi sử dụng phân chia 800-200 được cung cấp của tập huấn luyện để xác định siêu tham số và chạy đánh giá cuối cùng sử dụng dữ liệu huấn luyện đầy đủ. Chúng tôi báo cáo độ chính xác trung bình trên tập kiểm tra trong ba lần chạy.

Chúng tôi báo cáo độ chính xác trung bình trên các dataset FGVC, và độ chính xác trung bình trên từng trong ba nhóm trong VTAB. Các kết quả riêng lẻ trên mỗi nhiệm vụ được trong Phụ lục D, cũng như các ví dụ hình ảnh của các nhiệm vụ nói trên.

4.2 Kết quả chính

Bảng 1 trình bày kết quả tinh chỉnh một ViT-B/16 đã được tiền huấn luyện trung bình trên 4 nhóm nhiệm vụ phụ thuộc đa dạng, so sánh VPT với 7 giao thức tinh chỉnh khác. Chúng ta có thể thấy rằng:

1. VPT-Deep vượt trội hơn Full (Bảng 1 (a)) trên 3 trong 4 lớp vấn đề (20 trong 24 nhiệm vụ), trong khi sử dụng ít tham số tổng mô hình hơn đáng kể (1.18× so với 24.02×). Do đó, ngay cả khi lưu trữ không phải là vấn đề, VPT là một phương pháp hứa hẹn để thích ứng các Transformer lớn hơn trong thị giác. Lưu ý rằng kết quả này trái ngược với các nghiên cứu tương đương trong NLP, nơi tinh chỉnh prompt khớp, nhưng không vượt quá tinh chỉnh hoàn toàn [45].

2. VPT-Deep vượt trội hơn tất cả các giao thức tinh chỉnh hiệu quả tham số khác (Bảng 1 (b,c)) trên tất cả các nhóm nhiệm vụ, cho thấy rằng VPT-deep là chiến lược tinh chỉnh tốt nhất trong môi trường hạn chế lưu trữ.

3. Mặc dù không tối ưu bằng VPT-deep, VPT-shallow vẫn mang lại lợi ích hiệu suất không tầm thường hơn các phương pháp tinh chỉnh hướng đầu trong Bảng 1 (b), cho thấy rằng VPT-shallow là một lựa chọn đáng giá khi triển khai các mô hình tinh chỉnh đa nhiệm vụ nếu ràng buộc lưu trữ nghiêm trọng.

8 M. Jia et al.

[Tiếp tục phần còn lại của bản dịch với cùng cách tiếp cận, duy trì định dạng và cấu trúc gốc]

VPT trên quy mô dữ liệu phụ thuộc khác nhau. Chúng tôi xem xét tác động của kích thước dữ liệu huấn luyện đến độ chính xác trong các nhiệm vụ FGVC (VTAB chỉ có 1k ví dụ huấn luyện). Chúng tôi thay đổi dữ liệu huấn luyện từ 10% đến 80% và so sánh tất cả các phương pháp. Cùng một ViT-B đã được tiền huấn luyện được sử dụng cho huấn luyện phụ thuộc. Kết quả trung bình nhiệm vụ cho mỗi phương pháp trên các quy mô dữ liệu huấn luyện khác nhau được trình bày trong Hình 3.

Hình 3 cho thấy VPT-deep vượt trội hơn tất cả các đường cơ sở khác qua các quy mô dữ liệu. Đi sâu hơn, các phương pháp sử dụng ít tham số có thể huấn luyện hơn, tức là VPT, Linear, Adapter, Bias, chiếm ưu thế so với Full trong các chế độ dữ liệu thấp. Tuy nhiên, xu hướng này được đảo ngược khi có nhiều dữ liệu huấn luyện hơn cho Linear và Adapter. Ngược lại, VPT-deep vẫn liên tục vượt trội hơn Full qua các kích thước dữ liệu huấn luyện. Mặc dù Bias mang lại những lợi thế tương tự, nó vẫn hoạt động kém hơn VPT-deep một cách nhỏ nhặt trên toàn bộ (Hình 3 bên phải).

VPT trên các quy mô xương sống khác nhau. Hình 4 cho thấy hiệu suất VTAB-1k dưới 3 quy mô xương sống khác nhau: ViT-Base/Large/Huge. VPT-deep tốt hơn đáng kể so với Linear và VPT-shallow qua tất cả 3 lựa chọn xương sống và 3 nhóm phụ của VTAB-1k. Quan trọng hơn, những ưu thế của VPT-deep so với Full thực sự vẫn tồn tại khi quy mô mô hình tăng lên, tức là VPT-deep vượt trội hơn Full đáng kể trên các nhóm Natural và Structured, trong khi mang lại hiệu suất gần như tương đương trên Specialized.

VPT trên Transformer phân cấp. Chúng tôi mở rộng VPT cho Swin [52], sử dụng MSA trong các cửa sổ cục bộ đã dịch chuyển và hợp nhất các embedding patch tại các lớp sâu hơn. Để đơn giản và không mất tính tổng quát, chúng tôi triển khai VPT theo cách đơn giản nhất: các prompt được chú ý trong các cửa sổ cục bộ, nhưng bị bỏ qua trong các giai đoạn hợp nhất patch. Các thí nghiệm được thực hiện trên Swin-Base đã được tiền huấn luyện có giám sát ImageNet-21k. VPT tiếp tục vượt trội hơn các phương pháp tinh chỉnh hiệu quả tham số khác (b, c) cho tất cả ba nhóm phụ của VTAB Bảng 2, mặc dù trong trường hợp này Full cho điểm số chính xác cao nhất tổng thể (với chi phí nặng nề về tổng tham số).

Thật ngạc nhiên rằng lợi thế của VPT-deep so với VPT-shallow giảm dần đối với Natural: VPT-shallow cho điểm số chính xác hơi tốt hơn tinh chỉnh hoàn toàn.

4.3 Ablation về các biến thể thiết kế mô hình

Chúng tôi ablate các lựa chọn thiết kế mô hình khác nhau trên ViT-Base đã được tiền huấn luyện có giám sát ImageNet-21k và đánh giá chúng trên VTAB, với cùng thiết lập trong Bảng 1. Xem thêm trong Phụ lục B.

Vị trí prompt. Một sự khác biệt quan trọng giữa VPT và các phương pháp khác là các tham số có thể học bổ sung được giới thiệu làm đầu vào cho các lớp Transformer. Hình 5 ablate các lựa chọn khác nhau về cách và nơi chèn prompt trong không gian đầu vào, và chúng sẽ ảnh hưởng đến hiệu suất cuối cùng như thế nào.

Prepend hay Add? Thay vì thêm prompt vào trước chuỗi của các embedding patch hình ảnh Ei như được mô tả trong Mục 3.2, một lựa chọn khác là trực tiếp cộng prompt theo từng phần tử vào những embedding đó, giữ độ dài chuỗi đầu vào của Transformer giống như trước. Mặc dù biến thể này có tính cạnh tranh với Full trong một số trường hợp (ví dụ: VTAB-Natural), hiệu suất của nó thường tụt hậu so với Prepend mặc định trong cả hai thiết lập sâu và nông. Thêm thảo luận về hiện tượng này trong Phụ lục B.

Không gian ẩn hay pixel? Thay vì chèn prompt dưới dạng vectơ ẩn cho lớp Transformer đầu tiên, người ta có thể giới thiệu prompt ở cấp độ pixel trước lớp Embed trong Phương trình (1), tức là Prepend-pixel và Concat-channel. Hình 5 cho thấy hiệu suất thích ứng giảm cho hai biến thể này. Ví dụ, điểm số chính xác của việc thêm prompt nông trước lớp projection (Prepend-pixel) giảm 6,9%, so với việc thêm mặc định trong không gian embedding (Prepend) trên VTAB-Natural. Hiệu suất càng giảm thêm (thậm chí lớn như 30 điểm chính xác giảm trên VTAB-Natural) nếu thay vào đó chúng ta nối một kênh mới vào hình ảnh đầu vào (Concat-channel). Những quan sát này gợi ý rằng việc prompt học các tín hiệu phụ thuộc nhiệm vụ được cô đặc trong không gian đầu vào ẩn của Transformer dễ dàng hơn.

Độ dài prompt. Đây là siêu tham số bổ sung duy nhất cần tinh chỉnh cho VPT so với tinh chỉnh hoàn toàn. Để tham khảo dễ dàng, chúng tôi cũng ablate hai đường cơ sở khác trên các siêu tham số bổ sung riêng lẻ của chúng, tức là số lượng lớp cho Mlp và tỷ lệ giảm cho Adapter. Như được hiển thị trong Hình 6, độ dài prompt tối ưu thay đổi qua các nhiệm vụ. Đáng chú ý, ngay cả với chỉ một prompt, VPT-deep vẫn vượt trội hơn đáng kể so với 2 đường cơ sở khác, và vẫn có tính cạnh tranh hoặc thậm chí tốt hơn so với tinh chỉnh hoàn toàn trên VTAB-Structured và Natural.

Độ sâu prompt. Hình 7 ablate lớp nào và bao nhiêu lớp để chèn prompt. Mỗi biến thể báo cáo độ dài prompt tốt nhất được chọn với tập val. Hiệu suất của VPT có tương quan tích cực với độ sâu prompt nói chung. Tuy nhiên, độ chính xác giảm nếu chúng ta chèn prompt từ trên xuống dưới, gợi ý rằng prompt ở các lớp Transformer sớm hơn quan trọng hơn những lớp ở sau.

Đầu ra cuối cùng. Theo cấu hình gốc của ViT, chúng tôi sử dụng embedding cuối cùng của [CLS], tức là xN, làm đầu vào đầu phân loại, đây cũng là thiết lập mặc định trong các thí nghiệm ViT của chúng tôi. Như được hiển thị trong Hình 8, nếu chúng ta sử dụng gộp trung bình trên các embedding đầu ra patch hình ảnh EN làm đầu ra cuối cùng (Image-pool), kết quả về cơ bản vẫn giống nhau (ví dụ: 82,4 so với 82,3 cho VTAB-Specialized). Tuy nhiên, nếu việc gộp liên quan đến đầu ra prompt cuối cùng ZN (Prompt-pool và Global-pool), độ chính xác có thể giảm tới 8 điểm.

5 Phân tích và thảo luận

Trực quan hóa. Hình 9 cho thấy các trực quan hóa t-SNE [55] của xN, tức là các embedding của [CLS] sau lớp Transformer cuối cùng và trước đầu phân loại, cho 3 nhiệm vụ trong VTAB (SVNH [58], EuroSAT [32], Clevr/count [38]), một cho mỗi nhóm phụ. Tất cả các biểu đồ cho thấy VPT-deep có thể tạo ra các đặc trưng có thể tách tuyến tính trong khi sử dụng ít tham số hơn Full. Chúng tôi cũng quan sát thấy rằng các tham số có thể tinh chỉnh bổ sung cho mỗi lớp Transformer (VPT-deep) cải thiện hiệu suất, so với VPT-shallow, chỉ chèn prompt cho đầu vào của lớp đầu tiên. Thú vị là trên Clevr/count (Hình 9(c)), VPT-deep và Full khôi phục cấu trúc đa tạp cơ bản của nhiệm vụ (đếm đối tượng trong hình ảnh so với nhận dạng số đường phố hoặc cảnh quan), không giống như VPT-shallow và Linear.

Áp dụng VPT cho nhiều nhiệm vụ thị giác hơn. Chúng tôi khám phá tính khả thi của VPT ngoài phân loại thị giác, bằng cách đánh giá nhiệm vụ phân đoạn ngữ nghĩa ADE20K [90] với một mô hình Transformer, SETR-PUP [89]. Nó thêm một đầu ConvNet tiêu chuẩn vào xương sống ViT để thực hiện phân đoạn. Phương pháp de-facto vẫn là tinh chỉnh hoàn toàn xương sống đã được tiền huấn luyện cùng với đầu ConvNet (Full). Chúng tôi bao gồm hai giao thức khác để so sánh: chỉ cập nhật các lớp đầu (Head Only), cập nhật các lớp đầu và vectơ bias trong xương sống (Bias). Trong Bảng 3, chúng tôi báo cáo kết quả mIoU val với và không có suy luận đa quy mô. Mặc dù các giao thức hiệu quả tham số không thể cạnh tranh với Full, VPT vẫn có tính cạnh tranh với Bias. Đáng chú ý, VPT mang lại kết quả cạnh tranh với một mô hình ConvNet tiên tiến được tinh chỉnh hoàn toàn (DeepLab v3+ [10]), trong khi tinh chỉnh ít tham số hơn đáng kể (15M so với 64M, tương ứng).

Áp dụng VPT cho nhiều phương pháp tiền huấn luyện hơn. Ngoài các xương sống được tiền huấn luyện với dữ liệu có nhãn, chúng tôi thí nghiệm với hai mục tiêu tự giám sát: MAE [30] và MoCo v3 [11]. Bảng 4 báo cáo kết quả trên VTAB-1k với ViT-B. Chúng tôi quan sát thấy rằng cả hai biến thể của VPT đều vượt trội hơn Linear, nhưng các so sánh giữa các kỹ thuật khác ít kết luận hơn. Đối với MAE, các phương pháp hiệu quả tham số khác, ví dụ: Partial-1, vượt trội hơn cả VPT và Linear. Trong trường hợp MoCo v3, VPT không còn giữ được hiệu suất tốt nhất, mặc dù nó vẫn có tính cạnh tranh với những phương pháp khác. Điều này gợi ý rằng hai ViT tự giám sát này về cơ bản khác với những mô hình có giám sát trong các phần trước. Tại sao và làm thế nào những khác biệt này xuất hiện vẫn là những câu hỏi mở.

Áp dụng VPT cho ConvNet. Chúng tôi kiểm tra ý tưởng thêm các tham số có thể huấn luyện trong không gian đầu vào của ConvNet: padding cả chiều cao và chiều rộng bằng p pixel prompt có thể học cho hình ảnh đầu vào. Mặc dù hoạt động này có vẻ không thông thường, chúng tôi triển khai VPT theo cách này vì không có giải pháp rõ ràng để thêm prompt bất biến vị trí tương tự như các đối tác Transformer. Thực tế, phương pháp này đã được khám phá trước đây trong tài liệu tấn công đối nghịch [20]. Giá trị p trong thí nghiệm của chúng tôi nhỏ hơn 2 bậc độ lớn so với công trình trước: ví dụ: 5 so với 263. Quan trọng nhất, chúng tôi đặt ý tưởng này trong lăng kính của học chuyển giao. Xem Phụ lục C để thảo luận thêm.

Bảng 5 trình bày kết quả cho ConvNeXt-B [53] (được tiền huấn luyện trên ImageNet-21k) và ResNet-50 [31] (được tiền huấn luyện trên ImageNet-1k), tương ứng. VPT hoạt động tốt trong một xương sống ConvNet lớn hơn, ConvNeXt-B, mang lại cải thiện độ chính xác so với các giao thức tinh chỉnh thưa thớt khác (b,c), và vượt trội hơn Full trên 8 trong 19 trường hợp. Tuy nhiên, những lợi thế của VPT giảm dần với ConvNet nhỏ hơn (ResNet-50), vì không có người chiến thắng rõ ràng cho tất cả 19 nhiệm vụ VTAB-1k.

6 Kết luận

Chúng tôi trình bày Tinh chỉnh Prompt Thị giác, một phương pháp hiệu quả tham số mới để tận dụng các mô hình Transformer thị giác lớn cho một loạt các nhiệm vụ phụ thuộc. VPT giới thiệu các prompt có thể học dành riêng cho nhiệm vụ trong không gian đầu vào, giữ xương sống đã được tiền huấn luyện cố định. Chúng tôi cho thấy VPT có thể vượt trội hơn các giao thức tinh chỉnh khác (thường bao gồm cả tinh chỉnh hoàn toàn) trong khi giảm đáng kể chi phí lưu trữ. Các thí nghiệm của chúng tôi cũng đặt ra những câu hỏi hấp dẫn về động lực tinh chỉnh của Transformer thị giác với các mục tiêu tiền huấn luyện khác nhau, và cách chuyển giao đến các nhiệm vụ nhận dạng thị giác rộng hơn một cách hiệu quả. Do đó, chúng tôi hy vọng công trình của chúng tôi sẽ truyền cảm hứng cho nghiên cứu tương lai về cách khai thác tốt nhất tiềm năng của các mô hình nền tảng lớn trong thị giác.

Lời cảm ơn. Menglin được hỗ trợ bởi một khoản tài trợ nghiên cứu Meta AI được trao cho Đại học Cornell, Luming và Bharath được hỗ trợ bởi NSF IIS-2144117, Serge được hỗ trợ một phần bởi Trung tâm Tiên phong về AI, số tài trợ DNRF P1. Chúng tôi muốn cảm ơn Alexander Rush, Yin Cui vì những đề xuất và thảo luận quý báu.

A Chi tiết triển khai

Chúng tôi sử dụng PyTorch [62] để triển khai tất cả các thí nghiệm trên GPU NVIDIA A100-40GB.

A.1 Thí nghiệm phân loại

VPT. Chúng tôi sử dụng tập val của mỗi dataset để tìm độ dài prompt tốt nhất p, xem Mục 3.2. Độ dài prompt là siêu tham số duy nhất dành riêng cho VPT mà chúng tôi tinh chỉnh. Đối với các xương sống Transformer, phạm vi của p là {1,5,10,50,100,200} và {1,5,10,50} cho ViT và Swin, tương ứng. Lựa chọn tối đa của p xấp xỉ gần với số lượng token patch hình ảnh trong mỗi MSA cho cả hai kiến trúc (ViT: 196, Swin: 49). Chúng tôi cũng áp dụng dropout 0.1 cho VPT-deep. Đối với ConvNet, phạm vi của p là {1,3,5,7,9,11}. Mỗi prompt được khởi tạo ngẫu nhiên với sơ đồ khởi tạo đồng nhất xavier [26]. Chúng tôi tuân theo các lựa chọn thiết kế của xương sống gốc, chẳng hạn như sự tồn tại của token phân loại [CLS], hoặc có sử dụng embedding [CLS] cuối cùng cho đầu vào đầu phân loại hay không.

Adapter. Adapter [34] chèn các mô-đun nhẹ bổ sung bên trong mỗi lớp Transformer. Một mô-đun adapter thường bao gồm một phép chiếu tuyến tính xuống (với tỷ lệ giảm r), tiếp theo là một hàm kích hoạt phi tuyến tính và một phép chiếu tuyến tính lên, cùng với một kết nối dư. [63,64] tìm kiếm đầy đủ tất cả các cấu hình có thể và phát hiện rằng chỉ chèn adapter sau lớp phụ "Add & LayerNorm" FFN hoạt động tốt nhất. Do đó, chúng tôi cũng sử dụng thiết lập này trong triển khai của riêng chúng tôi. Chúng tôi quét tỷ lệ giảm r trong {8,64,256}.

Tăng cường và các siêu tham số khác. Chúng tôi áp dụng chiến lược tăng cường hình ảnh tiêu chuẩn trong quá trình huấn luyện: chuẩn hóa với trung bình và độ lệch chuẩn ImageNet, cắt thay đổi kích thước ngẫu nhiên thành 224×224 và lật ngang ngẫu nhiên cho năm dataset FGVC, và thay đổi kích thước thành 224×224 cho bộ VTAB-1k.³ Bảng 6 tóm tắt các cấu hình tối ưu hóa mà chúng tôi đã sử dụng. Theo [56], chúng tôi tiến hành tìm kiếm lưới để tìm các siêu tham số dành riêng cho tinh chỉnh, tỷ lệ học và giá trị weight decay sử dụng tập val của mỗi nhiệm vụ. Theo quy tắc tỷ lệ tuyến tính [42,27,11,30], tỷ lệ học được đặt là

Bảng 6. Chi tiết triển khai cho mỗi phương pháp tinh chỉnh được đánh giá. ⋆: chúng tôi quan sát thấy rằng đối với VPT-shallow đôi khi hưởng lợi từ một LR cơ sở lớn hơn cho 6 trong 24 nhiệm vụ được đánh giá, nơi chúng tôi tìm kiếm từ {1000.0,500.0,250.0,100.0}

Full, Partial, Bias, Adapter | Linear, Sidetune, Mlp, VPT
---|---
Optimizer | AdamW [54] | SGD
Optimizer momentum | - | 0.9
base lr range | {0.001, 0.0001, 0.0005, 0.005} | {50., 25., 10., 5., 2.5, 1.,0.5, 0.25, 0.1, 0.05}⋆
Weight decay range | {0.01,0.001,0.0001,0.0} | 
Learning rate schedule | cosine decay | 
Warm up epochs | 10 | 
Total epochs | 100 (ViT-B, Swin-B), 50 (ViT-L/H) | 

³Theo các thiết lập mặc định trong VTAB, chúng tôi không áp dụng các tăng cường khác

base lr×b/256, trong đó b là kích thước batch được sử dụng cho mô hình cụ thể, và base lr được chọn từ phạm vi được chỉ định trong Bảng 6. Các giá trị siêu tham số tối ưu cho mỗi thí nghiệm có thể được tìm thấy trong Phụ lục D.

Thông số kỹ thuật dataset và xương sống đã được tiền huấn luyện. Bảng 7 và 8 tóm tắt thống kê và chi tiết của các dataset phân loại được đánh giá và tất cả các xương sống đã được tiền huấn luyện được sử dụng trong bài báo. Hình 10 bao gồm các ví dụ hình ảnh của tất cả 24 nhiệm vụ phân loại được đánh giá.

A.2 Thí nghiệm phân đoạn ngữ nghĩa

ADE20K [90] là một benchmark phân tích cảnh đầy thách thức với 150 nhãn tinh vi. Các tập huấn luyện và validation chứa 20.210 và 2.000 hình ảnh tương ứng. Chúng tôi sử dụng codebase công khai MMSegmentation [15] trong triển khai của chúng tôi.⁴ Xương sống ViT-L được tiền huấn luyện có giám sát trên ImageNet-21k.⁵

SETR [89] là một khung phân đoạn cạnh tranh sử dụng ViT làm bộ mã hóa. PUP là một chiến lược upsampling tăng tiến bao gồm các lớp tích chập liên tiếp và các phép toán upsampling bilinear. Trong số nhiều lựa chọn decoder, PUP hoạt động tốt nhất theo việc tái tạo của MMSegmentation do đó chúng tôi cũng sử dụng nó như trong triển khai của chúng tôi.⁶

Khi áp dụng VPT cho SETR-PUP, chúng tôi chỉ chèn prompt vào xương sống bộ mã hóa ViT của SETR. Đối với decoder, chỉ các embedding patch hình ảnh được sử dụng làm đầu vào và các embedding prompt bị loại bỏ. Giống như các nhiệm vụ nhận dạng, chỉ đầu decoder PUP và prompt được học trong quá trình huấn luyện và xương sống ViT được đông lạnh.

Đối với tinh chỉnh hoàn toàn, chúng tôi sử dụng cùng siêu tham số như trong MMSegmentation. Đối với HeadOnly, Bias và VPT, chúng tôi sử dụng quét siêu tham số về tỷ lệ học {0.05, 0.005, 0.0005, 0.001}. Tỷ lệ học tối ưu là 0.005 cho tất cả các phương pháp. Chúng tôi quét độ dài prompt p∈{1, 5, 10, 50, 100, 200}. Đối với VPT, chúng tôi cũng thay đổi hệ số nhân tỷ lệ học thành 1.0 thay vì 10.0 mặc định, để đầu decoder và prompt chia sẻ cùng tỷ lệ học. Các siêu tham số khác vẫn giữ nguyên như tinh chỉnh hoàn toàn.

B Phân tích mở rộng

Hiệu ứng của việc mở rộng độ dài chuỗi đầu vào. Như được hiển thị trong Bảng 1, bằng cách mở rộng chuỗi đầu vào với các prompt có thể học, VPT đạt hiệu suất tốt hơn Full trên 20 trong 24 nhiệm vụ được đánh giá. Để điều tra liệu lợi thế của VPT có phải do độ dài chuỗi đầu vào được mở rộng hay không, chúng tôi thí nghiệm trên hai biến thể khác: (1) các prompt được giữ đông lạnh trong giai đoạn tinh chỉnh (Prompt-Fixed). (2) chỉ tinh chỉnh token [CLS] ([CLS]-Learned). Từ Hình 11, chúng ta có thể thấy rằng, cập nhật embedding prompt (Prompt-Learned) mang lại lợi ích đáng kể, trong khi Prompt-Fixed cho kết quả tương đương với Linear. Điều này gợi ý rằng hiệu suất cuối cùng của VPT chủ yếu được đóng góp bởi các embedding prompt đã học thay vì độ dài chuỗi được mở rộng. Cập nhật token [CLS] hoạt động tương tự như cập nhật 1 prompt ([CLS] so với Learned p=1), nhưng vẫn tụt hậu so với thiết lập mặc định nơi chúng tôi chọn thủ công số lượng token prompt tốt nhất dựa trên tập val.

Chia sẻ prompt. Chúng tôi kiểm tra hiệu ứng của việc chia sẻ tham số của prompt trong Hình 12 bằng cách đặt cùng embedding prompt trong các lớp Transformer (Shared-intra), giữa tất cả các lớp (Shared-inter), và cho tất cả prompt được chèn trong Transformer (Shared-all). Chúng ta có thể quan sát thấy rằng: (1) Chia sẻ prompt trong lớp (Shared-intra) hoạt động có tính cạnh tranh hoặc hơi vượt trội so với hiệu suất của việc sử dụng một prompt (Default p=1), tiếp tục chứng minh giá trị của việc mở rộng chuỗi đầu vào. (2) Mặc dù Shared-intra hoạt động kém hơn Default nói chung, đáng ngạc nhiên, Shared-inter hơi vượt trội hơn VPT-deep mặc định của chúng tôi trong khi sử dụng số lượng tham số có thể huấn luyện tương tự (tổng số tham số cho tất cả nhiệm vụ VTAB: 1.14× so với 1.13× cho Shared-inter so với Default, tương ứng). Kiểm tra kỹ hơn tiết lộ rằng độ dài prompt tối ưu p cho Shared-inter nói chung lớn hơn Default, tức là độ dài prompt trung bình trên tất cả nhiệm vụ VTAB: 64.58 so với 60.94, cho Shared-inter so với Default, tương ứng. (3) Chia sẻ cùng embedding prompt cả giữa và trong các lớp (Shared-all) làm giảm hiệu suất, nhưng vẫn vượt trội hơn kết quả linear probing qua ba nhóm phụ VTAB.

Khởi tạo prompt. Trong NLP, tinh chỉnh prompt có thể hưởng lợi từ khởi tạo prompt phức tạp hơn, như được hiển thị trong [45]. Chúng tôi điều tra liệu điều này có đúng với prompting thị giác hay không. Chúng tôi sử dụng các biểu diễn nguyên mẫu cho các lớp mục tiêu phụ thuộc để các prompt được khởi tạo với các embedding liệt kê không gian đầu ra. Vì chúng tôi muốn mô hình tạo ra một embedding đầu ra gần với một trong những biểu diễn nguyên mẫu này với một ví dụ kiểm tra, việc khởi tạo prompt theo cách này có thể cho mô hình một số gợi ý về các danh mục mục tiêu do đó giúp cải thiện quá trình tối ưu hóa.

Cụ thể, chúng tôi sử dụng các embedding [CLS] cuối cùng được lấy trung bình trong mỗi lớp mục tiêu của phần chia huấn luyện dataset phụ thuộc. Với ViT đã được tiền huấn luyện có N lớp, và tập huấn luyện phụ thuộc với c lớp mục tiêu, đối với mỗi ví dụ huấn luyện, chúng tôi tính toán các embedding [CLS] cuối cùng, xN∈Rᵈ. Sau đó chúng tôi lấy trung bình các embedding này trong mỗi lớp mục tiêu để có được {x̂ᵏN∈Rᵈ|k∈N,1≤k≤c}.⁷ Đặt độ dài prompt p=c,⁸ chúng tôi khởi tạo P với {x̂ᵏN}ᵏ⁼ᶜₖ₌₁ cho VPT-shallow, và khởi tạo mỗi Pi với {x̂ᵏN}ᵏ⁼ᶜₖ₌₁, trong đó i= 0,1, . . . , N−1, cho VPT-deep.

Chúng tôi so sánh hiệu suất tinh chỉnh sử dụng chiến lược khởi tạo trên (CLS) với khởi tạo ngẫu nhiên mặc định (Random) trong Hình 13. Chúng tôi cũng báo cáo kết quả khi chúng tôi cố định các prompt trong giai đoạn tinh chỉnh (·-fixed). Như được hiển thị trong Hình 13, thật ngạc nhiên rằng khởi tạo ngẫu nhiên mặc định (Random) của chúng tôi hoạt động tốt nhất nói chung, một cách nhất quán qua các nhóm phụ khác nhau của VTAB mà không cần các bước tiền xử lý bổ sung được mô tả ở trên (CLS). CLS hoạt động tương đương trong các nhóm phụ Natural và Specialized.⁹

Độ sâu prompt so với độ dài prompt. Trong Hình 7, chúng tôi ablate số lượng lớp mà chúng tôi chèn prompt vào. Đối với mỗi biến thể độ sâu prompt, Hình 7 báo cáo kết quả sử dụng độ dài prompt tốt nhất cho mỗi nhiệm vụ ("· → · (tốt nhất)" trong Hình 14). Ở đây chúng tôi áp dụng một thiết lập khác nơi độ dài prompt tốt nhất từ 1 → 12 được sử dụng cho tất cả các biến thể độ sâu prompt khác. So sánh cả "· → · (tốt nhất)" và "· → ·", chúng tôi quan sát thấy rằng có độ nhạy cảm khác nhau đối với độ dài prompt cho các độ sâu khác nhau, đặc biệt nếu chúng tôi chèn prompt chỉ trong chín lớp (3 → 12, 12 → 3).

Kết hợp VPT với Bias Tuning. Các thí nghiệm của chúng tôi trong bài báo chính tiết lộ rằng Bias là một đường cơ sở tinh chỉnh hiệu quả tham số có tính cạnh tranh (ví dụ: Bảng 1 (c)). Dựa trên quan sát này, chúng tôi khám phá một giao thức khác nơi chúng tôi cập nhật cả prompt và các số hạng bias của xương sống đã được tiền huấn luyện, giữ mọi thứ khác trong xương sống đông lạnh (VPT+Bias). Như được hiển thị trong Bảng 9, thật ngạc nhiên, việc kết hợp Bias với VPT không cho kết quả vượt trội nói chung, thậm chí còn làm giảm VPT-deep cho tất cả 3 nhóm phụ nhiệm vụ. Điều này gợi ý rằng hai phương pháp này không nhất thiết bổ sung cho nhau.

Ensemble prompt. [45] chứng minh hiệu quả của prompt trong bối cảnh ensemble mô hình. Đối với một ensemble của k mô hình, chúng tôi chỉ cần lưu trữ các vectơ prompt đã học thay vì k bản sao của toàn bộ tham số mô hình đã được tinh chỉnh (ví dụ: k×2.5GB cho ViT-H). Hơn nữa, với một ví dụ kiểm tra trong thời gian suy luận, chỉ một lần forward pass được thực hiện với một batch được thiết kế đặc biệt với dữ liệu gốc được nhân bản nhưng prompt được thay đổi.

Với những lợi thế như vậy, chúng tôi cũng điều tra hiệu quả của VPT trong ensemble prompt. Chúng tôi huấn luyện 5 prompt khác nhau cho mỗi nhiệm vụ VTAB với các hạt giống ngẫu nhiên khác nhau, sử dụng cùng xương sống ViT-B đã được tiền huấn luyện và siêu tham số như trong Bảng 1. Hình 15 cho thấy rằng VPT-deep được ensemble vượt trội hơn các đối tác prompt đơn trung bình hoặc thậm chí tốt nhất, cũng như các phương pháp tinh chỉnh được ensemble khác bao gồm cả Full.

Kiểm tra ý nghĩa thống kê. Chúng tôi thực hiện kiểm tra t một đuôi không tham số ghép cặp (kiểm tra xếp hạng có dấu Wilcoxon [82]) về liệu hiệu suất của VPT-deep có lớn hơn các phương pháp tinh chỉnh khác qua 19 nhiệm vụ VTAB hay không (giả thuyết null H0 nói rằng trung bình khác biệt hiệu suất VTAB giữa VPT-deep và phương pháp đường cơ sở thay thế bằng không. Giả thuyết thay thế H1 nói rằng VPT-deep vượt trội hơn phương pháp đường cơ sở trên VTAB). Bảng 10 trình bày các giá trị p của mỗi kiểm tra, với số quan sát bằng 19 cho mỗi phương pháp được so sánh (chúng tôi sử dụng điểm số chính xác trung bình trong 5 lần chạy cho 19 nhiệm vụ VTAB và tất cả các phương pháp tinh chỉnh). Đối với tất cả các giao thức tinh chỉnh được so sánh, những cải thiện của VPT-deep có ý nghĩa thống kê (p <0.05).

Chúng tôi cũng thực hiện kiểm tra t một đuôi không ghép cặp với phương sai không bằng nhau (kiểm tra t của Welch [81]), so sánh các lần chạy riêng lẻ (số quan sát = 5) cho mỗi nhiệm vụ VTAB (H0 nói rằng VPT-deep và đường cơ sở khác hoạt động giống nhau cho một nhiệm vụ VTAB cụ thể, trong khi H1 nói rằng VPT-deep vượt trội hơn đường cơ sở khác cho một nhiệm vụ VTAB cụ thể). Hình 16 trình bày các giá trị p cho mỗi cặp <VPT-deep, phương pháp đường cơ sở> trên mỗi nhiệm vụ. Chúng tôi từ chối H0 trên 127 trong 19×8 = 152 trường hợp (p <0.05). So với Full, VPT-deep đạt hiệu suất tốt hơn có ý nghĩa thống kê trên 11 trong 19 nhiệm vụ.

Hiệu ứng của các siêu tham số tinh chỉnh khác nhau. Trong Hình 17, chúng tôi trình bày hiệu suất của các giao thức tinh chỉnh khác nhau trên các siêu tham số tinh chỉnh khác nhau bao gồm tỷ lệ học và weight decay. Đối với VPT-deep được đề xuất của chúng tôi, chúng tôi cũng ablate các lựa chọn khác nhau của độ dài prompt p, đây là siêu tham số duy nhất cần tinh chỉnh thủ công. Tất cả các thí nghiệm được đánh giá trên tập val của nhiệm vụ KITTI/Distance (VTAB-Specialized). Chúng tôi quan sát các hành vi khác nhau giữa Linear và VPT. Cả hai phương pháp đều đông lạnh tham số xương sống trong giai đoạn tinh chỉnh. Linear probing nhạy cảm hơn với giá trị weight decay nói chung, trong khi VPT bị ảnh hưởng bởi cả giá trị tỷ lệ học và weight decay. VPT với độ dài prompt lớn hơn cũng ít nhạy cảm hơn với việc lựa chọn tỷ lệ học.

Hiệu ứng của độ phân giải hình ảnh. Bài báo ViT gốc [19] phát hiện rằng tinh chỉnh với độ phân giải hình ảnh cao hơn (384×384) có lợi cho các nhiệm vụ nhận dạng phụ thuộc. Tất cả các thí nghiệm nhận dạng được trình bày trong bài báo chính đều được tinh chỉnh trên độ phân giải 224×224. Như được hiển thị trong Bảng 11, chúng tôi chạy lại các thí nghiệm VTAB với cùng thiết lập như trong Bảng 1 nhưng ở độ phân giải 384 thay vì 224 mặc định. Chúng ta có thể thấy rằng, VPT-deep vẫn đạt hiệu suất tốt nhất trong số tất cả các giao thức tinh chỉnh hiệu quả tham số, và thậm chí vượt trội hơn tinh chỉnh hoàn toàn trên 15 trong 19 nhiệm vụ. Mặc dù việc tăng độ phân giải hình ảnh không dẫn đến hiệu suất tinh chỉnh hoàn toàn tốt hơn nói chung, nó thực sự cải thiện nhẹ hiệu suất của VPT-deep.

Một quan sát thú vị khác từ Bảng 11 là với độ phân giải tinh chỉnh 224 và giá trị p= 380 lớn hơn, VPT có thể đạt hiệu suất tương tự hoặc tốt hơn so với Full với độ phân giải 384, trong khi sử dụng cùng độ dài chuỗi đầu vào nhưng ít tham số có thể huấn luyện hơn đáng kể.

Chi phí tính toán thực nghiệm. Một hạn chế có thể của VPT là độ dài chuỗi đầu vào bổ sung cho Transformer. Về lý thuyết, độ phức tạp của MSA là bậc hai với độ dài chuỗi đầu vào, nhưng điều này có thể không đúng đối với tốc độ thực tế do các chi tiết phần cứng như chiều rộng làn đường và kích thước bộ nhớ cache [19]. Trong Bảng 12 và Hình 18, chúng tôi nghiên cứu chi phí tính toán thực nghiệm, tức là độ trễ và sử dụng bộ nhớ GPU đỉnh tại cả thời gian huấn luyện và suy luận, cho tất cả các giao thức tinh chỉnh được nghiên cứu. Tất cả các thí nghiệm sử dụng cùng GPU A100 với kích thước batch 64 cho cả huấn luyện và suy luận. Chúng ta có thể thấy rằng việc chia tỷ lệ bậc hai lý thuyết với độ dài chuỗi hầu như không xảy ra với VPT. Ví dụ, việc nhân đôi độ dài (p= 200 so với m= 198) về cơ bản chỉ dẫn đến 2× (thay vì 4×) độ trễ suy luận và bộ nhớ GPU đỉnh so với tinh chỉnh hoàn toàn. Đối với huấn luyện, độ trễ sẽ được giảm đáng kể với số lượng prompt ít hơn.

Một triển khai tương đương của VPT trong thời gian kiểm tra là trực tiếp thêm các tham số vào trước các mảng key và value bên trong mô-đun self-attention của Transformer [48] (VPT-prefix). Trong khi chúng tôi phát hiện rằng việc triển khai như vậy không dẫn đến cải thiện độ chính xác trên các dataset VTAB, nó giảm chi phí tính toán trong suy luận. Hình 19 cho thấy sự so sánh với các giá trị p khác nhau. VPT-prefix giảm độ trễ thời gian kiểm tra và bộ nhớ GPU đỉnh với một biên độ lớn đặc biệt khi p trở nên lớn.

C Thảo luận thêm

VPT so với Adversarial Reprogramming (AR). Những khác biệt là: (1) số lượng tham số đã học được tiêm trong không gian đầu vào trong tài liệu AR [20] gần như lớn hơn 20 lần so với của chúng tôi (264k so với 13k). VPT hiệu quả tham số hơn đáng kể; (2) AR đã cho thấy hiệu quả của nó trong ConvNet, trong khi VPT có thể được áp dụng cho các kiến trúc rộng hơn, bao gồm ViT, Swin. Hơn nữa, VPT tổng quát hơn với tùy chọn đi sâu vào các lớp sâu hơn của xương sống đã được tiền huấn luyện (Hình 2), trong khi AR nghiêm túc áp dụng cho lớp đầu vào đầu tiên của ConvNet. (3) một sự khác biệt khác là thiết lập của chúng tôi cập nhật cả prompt và đầu phân loại, trong khi AR [20] trực tiếp sử dụng đầu phân loại đã được tiền huấn luyện. Thiết lập của chúng tôi tổng quát hơn và có thể được áp dụng cho các mô hình với một loạt các mục tiêu tiền huấn luyện rộng hơn (ví dụ: MAE [30], không bao gồm đầu phân loại đã được tiền huấn luyện) và các nhiệm vụ thị giác rộng hơn (ví dụ: phân đoạn).

Prompt thị giác so với prompt văn bản. Bài báo của chúng tôi cũng khám phá sự khác biệt giữa prompt thị giác và văn bản: chúng tôi cho thấy rằng VPT thậm chí có thể vượt trội hơn tinh chỉnh mô hình đầy đủ trên 20 trong 24 trường hợp, điều này trái ngược với công trình liên quan của NLP [45]. Chúng tôi cũng phát hiện rằng các prompt được khởi tạo ngẫu nhiên hoạt động tốt hơn trong Hình 13, và prompt ở các lớp sớm hơn quan trọng hơn (Hình 7 và 14), cũng khác với quan sát ở phía NLP [45,51]. Những khác biệt này cho thấy rằng prompting thị giác có thể về cơ bản khác với prompt văn bản do đó cần được điều tra thêm.

D Kết quả bổ sung

Kết quả số của Bảng 1. Bảng 13 và 14 trình bày kết quả cho từng nhiệm vụ cho 24 nhiệm vụ phân loại được đánh giá trong Bảng 1.

Kết quả từng nhiệm vụ về ablation dữ liệu huấn luyện. Hình 20 trình bày kết quả từng nhiệm vụ cho năm dataset FGVC. Chúng tôi quan sát một xu hướng tương tự trong Hình 3: trong khi tất cả các phương pháp hiệu quả tham số vượt trội hơn tinh chỉnh hoàn toàn trong chế độ dữ liệu nhỏ đến trung bình, VPT-deep liên tục vượt trội hơn Full qua các quy mô dữ liệu cho năm nhiệm vụ FGVC.

Trực quan hóa t-SNE thêm. Trong Hình 21, chúng tôi trình bày thêm các trực quan hóa t-SNE, tương tự như Hình 9, cho tất cả các dataset VTAB với ít hơn hoặc bằng 20 lớp mục tiêu.

[Tiếp tục với các bảng và hình ảnh còn lại theo cùng cách tiếp cận]
