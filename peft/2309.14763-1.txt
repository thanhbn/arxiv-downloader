# 2309.14763.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2309.14763.pdf
# File size: 505328 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
ConPET: Continual Parameter-Efficient Tuning for
Large Language Models
Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun and Tao Yang
Abstract —Continual learning necessitates the continual adap-
tation of models to newly emerging tasks while minimizing the
catastrophic forgetting of old ones. This is extremely challenging
for large language models (LLMs) with vanilla full-parameter
tuning due to high computation costs, memory consumption,
and forgetting issue. Inspired by the success of parameter-
efficient tuning (PET), we propose Continual Parameter-Efficient
Tuning (ConPET), a generalizable paradigm for continual task
adaptation of LLMs with task-number-independent training
complexity. ConPET includes two versions with different applica-
tion scenarios. First, Static ConPET can adapt former continual
learning methods originally designed for relatively smaller models
to LLMs through PET and a dynamic replay strategy, which
largely reduces the tuning costs and alleviates the over-fitting and
forgetting issue. Furthermore, to maintain scalability, Dynamic
ConPET adopts separate PET modules for different tasks and
a PET module selector for dynamic optimal selection. In our
extensive experiments, the adaptation of Static ConPET helps
multiple former methods reduce the scale of tunable parameters
by over 3,000 times and surpass the PET-only baseline by at least
5 points on five smaller benchmarks, while Dynamic ConPET
gains its advantage on the largest dataset. The codes and datasets
are available at https://github.com/Raincleared-Song/ConPET.
Index Terms —Continual learning, parameter-efficient tuning,
large language models.
I. I NTRODUCTION
RECENTLY, large language models (LLMs) have shown
excellent capabilities from a wide range of aspects [1]–
[3], which equip them with larger potential in handling various
task-specific settings. To adapt LLMs to downstream tasks,
fine-tuning is often the first choice [4]. However, in real-life
applications, the consistent emergence of materials such as the
latest corpus [5], new knowledge [6], [7], and heterogeneous
tools [8] can frequently change the task schemas. This ne-
cessitates continual task-specific adaptation of LLMs, which
is highly expensive and performance-risky when conducted
through traditional fine-tuning due to the huge number of LLM
parameters and the catastrophic forgetting issue [9], namely
the significant performance decrease on old tasks after adapted
to new ones.
Although many continual learning methods have been pro-
posed to handle these problems, specific challenges hinder
their adaptation to LLMs. Dynamic-architecture-based meth-
ods [11], [12], which progressively increase the model scale
Chenyang Song, Xu Han, Zheni Zeng, Zhiyuan Liu, and Maosong Sun
are with the Department of Computer Science and Technology, Tsinghua
University, Beijing 100084, China.
Kuai Li, Chen Chen, and Tao Yang are with the Machine Learning Platform
Department, Tencent, Beijing 100193, China.
E-mail: scy22@mails.tsinghua.edu.cn, liuzy@tsinghua.edu.cn
Zhiyuan Liu is the corresponding author.
T1 T2…Old Data
Full
Memory
hierarchical 
sampling
Old
New
Limited
Train
BatchesTksamplingTk-1
New 
Data
LLM
 PETEMR + Static ConPET Traditional EMR
T1 T2…Old Data
Tk-1
fixed -number
sampling
T'1 T'2 T'k-1
…
Tk
New 
DataLimited
Memory
Train Data
(Step k)
LLMfull-paramater
tuning
Sample Raw Data
Tunable Frozenparameter -efficient 
tuningFig. 1. The comparison between traditional EMR [10] and adapted EMR with
Static ConPET. The latter adopts the dynamic replay strategy for training data
generation and PET for LLM tuning.
with the growth of data, suffer from unacceptable linearly
growing costs due to the unrestricted scaling of the archi-
tecture. Meanwhile, most memory-based methods [9], [13]–
[15] frequently re-tune the model through a fixed replay
strategy, where examples saved in limited episodic memory
are replayed a fixed number of times with new data combined.
This makes them susceptible to low scalability and over-
fitting on memorized examples [15]. Furthermore, since their
backbones are relatively small in scale such as BERT [16]
and RoBERTa [17], they adopt full-parameter tuning in their
original works, which imposes a heavy burden on computation
resources for LLMs in terms of time and GPU memory.
Faced with these challenges, we propose Continual
Parameter- Efficient Tuning ( ConPET ), a generalizable
paradigm for the continual fine-tuning-based task adaptation
of LLMs with task-number-independent training complexity,
including Static ConPET andDynamic ConPET .
As shown in Figure 1, we first use Static ConPET , a gen-
eral approach to adapting traditional memory-based continual
methods such as EMR [10] to LLMs while coping with high
training costs, over-fitting, and catastrophic forgetting. Specif-
ically, it contains two key adaptations: (1) Replacing vanillaarXiv:2309.14763v1  [cs.CL]  26 Sep 2023

--- PAGE 2 ---
2
fine-tuning with parameter-efficient tuning (PET). As PET
only updates tiny-scale tunable modules (generally account
for lower than 1%of the LLM) while keeping the original
LLM frozen, the computation costs for parameter updates and
the GPU memory consumption can be largely reduced [4].
(2) Utilizing the historical data through a dynamic replay
strategy, which conducts robust sampling hierarchically from
full-volume historical data rather than a limited memory to
improve data coverage and alleviate over-fitting and forgetting.
A restriction on the number of sampled batches is also
introduced to control the training complexity.
Adapted from memory-based methods, Static ConPET with
a single PET module cannot avoid the low scalability issue.
Therefore, we further propose Dynamic ConPET shown in
Figure 2, a novel dynamic architecture including a back-
bone LLM, a PET module selector, a set of task-specific
PET modules, and a cache system. Similar to mixture-of-
expert (MoE) architectures, Dynamic ConPET separates the
parameters for tasks with different schemas (e.g., distinct
types of knowledge) in different modules, which naturally
alleviates forgetting and maintains scalability to the increasing
task number. As each task-specific module only contains a
lightweight and pluggable PET module rather than heavy
sub-networks in former dynamic-architecture-based methods,
Dynamic ConPET is more tailored for memory-consuming
LLMs. Meanwhile, the PET module selector ensures a con-
stant forward propagation cost by pre-selecting a fixed quantity
of task-specific modules with the highest scores to participate
in the prediction.
We conduct comprehensive experiments on multiple
datasets of knowledge extraction, a representative continual
learning scenario with consistently emerging new knowledge
types. The results demonstrate that both versions of ConPET
are effective in the continual task-specific adaptation of LLMs
while having different application scenarios. Further analysis
shows the effect of parameter-efficient tuning, PET module
pre-selection, and different task splits.
II. R ELATED WORK
A. Parameter-Efficient Tuning
To fine-tune LLM more efficiently, parameter-efficient tun-
ing is proposed [4], which mainly consists of three types:
(1) Addition-based methods [18]–[20] introduce additional
small-scale tunable parameters while freezing the original
LLM. (2) Specification-based methods [21]–[23] selectively
optimize part of the LLM parameters with the remaining
parameters unchanged. (3) Reparameterization-based meth-
ods [24], [25] convert adaptive parameters of LLMs into
parameter-efficient forms during optimization. In this work,
ConPET can be adapted to addition-based methods and some
reparameterization-based methods. Notably, LoRA [24], which
introduces additional parameters to model the weight differ-
ences, is proven to perform better than most mainstream PET
methods and thus widely adopted [4]. Therefore, our experi-
ments in this work will focus on LoRA as a representative.
PET is demonstrated to save considerable computation costs
and memory consumption. According to previous works [26],given the same instruction data and GPUs, the time consumed
by PET tuning on LLaMA-7B [27] is only around one-fourth
that of full-parameter tuning.
B. Continual Learning for LLMs
Continual learning aims at teaching a model to incre-
mentally handle newly emerging tasks while mitigating the
catastrophic forgetting issue. Despite the success of in-context
learning in zero/few-shot leaning [1], fine-tuning is still a
prevalent paradigm in task adaptation of LLMs [4]. The
existing efforts on tuning-based continual learning can be
generally classified into three categories: (1) Consolidation-
based methods protect the parameters of importance from
shifting considerably, which is often implemented through
regularization [28]–[30] or distillation [31], [32]. However,
they perform poorly for lack of historical data utilization. (2)
Dynamic-architecture-based methods [11], [12], [33] maintain
a model whose scale is progressively increasing with the task
number. By introducing independent parameters for different
tasks, they can effectively overcome catastrophic forgetting but
suffer from the linear growth of training costs. (3) Memory-
based methods [9], [13]–[15], [34] introduce episodic memory
to store and replay examples of old tasks. Although the
old task information is partly retained in memory, they are
susceptible to over-fitting and low scalability caused by the
frequent re-training on a fixed model architecture through the
fixed replay strategy. All the above methods adopt vanilla fine-
tuning due to the small scale of their backbones.
Considering the expensive costs of tuning LLMs, some
works integrate parameter-efficient tuning with continual
learning but still cannot fully handle some shortcomings of for-
mer methods. Both LFPT5 [35] and Progressive Prompts [36]
propose to continuously introduce and train new soft prompts
for a new task to tackle catastrophic forgetting, while LFPT5
additionally generate pseudo examples for experience replay.
However, similar to the aforementioned dynamic-architecture-
based methods, the continual accumulation of new prompts
can cause the scalability issue. AdapterCL [37] learns a
separate Adapter [18] for each task and selects the task-specific
Adapter based on the perplexity. Such a selection method
causes a linear increase in forwarding costs with respect to the
task number. LAE [38] iteratively trains and ensembles two
experts favored by novel/historical tasks respectively, which is
susceptible to over-fitting given a fixed number of experts.
Moreover, there are also works focused on the continual pre-
training problem [5], [39], [40], but they are too expensive to
be applied to task-specific LLM fine-tuning, which is often
conducted frequently with low resources.
C. Mixture-of-Expert for LLMs
Another line of work similar to Dynamic ConPET is the
MoE architecture composed of multiple separate networks,
which learn to handle different subsets of input examples
or take distinct responsibilities. MoE is first demonstrated to
be effective in deep learning by introducing an MoE layer
stacked between LSTM modules [41]. Later, GShard [42],
BASELayer [43], HashLayer [44], Switch Transformers [45],

--- PAGE 3 ---
3
MoEfication [46], and DEM IX[47] attempt to explore the
implementation and training strategies of MoE in Transformer-
based models. However, these works have to modify specific
structures (e.g., the FFN layer) in Transformers, which cannot
be easily adapted to the continual fine-tuning of an already pre-
trained LLM. Some recent works have proved that LLMs with
MoE are able to obtain supreme performances when combined
with instruction tuning [48], [49], illustrating the potential of
such kinds of architecture.
To improve efficiency while alleviating over-fitting and
catastrophic forgetting in the continual task-specific adaptation
of LLMs, Static ConPET adapts memory-based continual
learning methods using PET and dynamic replay strategy. Fur-
thermore, Dynamic ConPET handles low scalability through
a dynamic structure with task-specific PET modules and a
selector. Unlike former MoE architectures, Dynamic ConPET
is better suited for LLM tuning as each expert is a lightweight
and highly pluggable PET module, which can be tuned without
altering the original LLM structure or parameters. Finally,
both ConPET versions offer task-number-independent training
complexity.
III. P ROPOSED METHODS
A. Task Definition
Continual fine-tuning of LLMs aims at teaching the LLM
to simultaneously handle a sequence of tasks in a specific
application scenario with consistently emerging materials. We
denote the set of schemas of the materials handled in the k-
th task as Sk, with a corresponding training set Tkand an
evaluation set Qk. The schemas of different tasks are disjoint.
At the k-th step, given the seen training data ˜Tk=∪k
i=1Ti,
the model is required to obtain satisfactory results on the
evaluation set of the new task as well as all the k−1
historical tasks, namely ˜Qk=∪k
i=1Qiwith the schema set
˜Sk=∪k
i=1Si.
Taking knowledge extraction as a representative, Skis
a specific subset of knowledge types (e.g., entity types or
relation types). Each example in the dataset consists of an
input sentence and a groud-truth label, indicating the type of
knowledge expressed in the input. The LLM is then required
to predict the knowledge type label with steadily satisfying
accuracy along the task sequence.
B. Static ConPET
Static ConPET is a generalizable approach to adapting
former memory-based continual learning methods to the con-
tinual task-specific adaptation of LLMs, which mainly consists
of two parts: the PET adaptation and the dynamic replay
strategy.
1) PET Adaptation and Example Encoding: As most for-
mer memory-based methods mainly concern relatively small-
scale models [9], [10], [14], we first replace the vanilla fine-
tuning with PET when applying them to LLMs. Specifically,
instead of tuning all the parameters in LLMs, we only optimize
one tiny PET module, while the LLM and remaining modules
stay frozen. Considering the small size of tunable parameters,TABLE I
THE SETTINGS OF THE TWO CONTINUAL KNOWLEDGE EXTRACTION
TASKS INVOLVED IN OUR EXPERIMENTS .
Entity Typing
Target : Determine the type of entity contained in the input example.
Entity Markers :[E1] entity [/E1]
Prompt Template : In this sentence, entity is a[MASK] .
Relation Extraction
Target : Determine the relation between head entity andtail entity
expressed in the input example.
Entity Markers :[E1] head entity [/E1] ,[E2] tail entity [/E2]
Prompt Template : In this sentence, tail entity is the [MASK] of
head entity .
Entity markers and prompt templates appended at the end of an input
example are also displayed.
the computation costs for parameter updates and the GPU
memory consumption will be significantly reduced.
With the aid of PET adaptation, we can conduct more effi-
cient example encoding, which is aimed at generating informa-
tive representations for downstream tasks. We take knowledge
extraction, including entity typing and relation extraction, as a
representative. To improve the quality of representations, we
adopt an LLM as our backbone encoder and enhance the inputs
through entity markers and prompt templates. The LLM has
obtained large amounts of knowledge through unsupervised
training and can convert inputs into informative hidden state
vectors. Following the previous work [50], we apply entity
markers surrounding each entity in the input example to insert
entity positional information. Besides, inspired by the success
of prompt tuning [51], [52], we append prompt templates at
the end of inputs and take the hidden state of [MASK] as
the example representation. The entity markers and prompt
templates used in the two knowledge extraction tasks involved
are shown in Table I.
Formally, given an input example x, the LLM integrated
with a PET module Mfirst encodes xinto its example
representation, which is then projected to the corresponding
logits by the linear head contained in M. We hereinafter denote
this process as follows,
s=f(M,x) (1)
where fandsstand for the encoding function and the logit
vector respectively.
2) Dynamic Sampling Strategy: Rather than conducting
replays on limited memorized examples as existing memory-
based methods, ConPET utilizes historical data through a
dynamic replay strategy to avoid over-fitting and control the
overall training steps. Specifically, we remove the limits to
storage space to improve data coverage. Instead, the replay
examples are dynamically selected from the full-volume data,
under a restriction on the maximum batch number at each step,
which ensures task-number-independent complexity.
This strategy may challenge a common assumption in
continual learning that the memory is limited and thus it
is unrealistic to save full-volume data [14]. However, we
consider it more reasonable to focus on the restriction of

--- PAGE 4 ---
4
LLMInput 
Example
ID: 34
PET Module
Selector
selection
scoresMM1
M3
Mk
(teacher forcing)
…... …...…...low logit α
logits on S2
Task -Specific
PET Moduleslogits on Sk
I. PET Module Pre -Selection II. Prediction with Active PET ModulesModule ID Example ID Logits When to Save
After Stage I 34 M (selector)
After M 2 is tuned 34 M2 (old)
After Stage II 34 Mk (new)
...... ...... ......
Cache 
System......
LLM
 M1
 M
 Mk
Frozen Modules Tunable Modules
Module Selected LLM-PET Integration
M2
prediction
logitsone-hot
labellossDynamic ConPET
pre-select top -k 
PET modules
Fig. 2. The architecture of ConPET when the number of active PET modules is 2. The working process can be split into two procedures: PET module
pre-selection and prediction with active PET modules. All logits generated by a specific PET module will be saved instantly by the cache system after this
module completes tuning.
training costs (complexity) rather than memory in terms of
price and consumption. Generally, current training corpora do
not exceed the TB-level even for GPT-3 175B [1], one of
the largest existing language models. The cost of storing such
scale data is much less than a single V100 GPU, let alone
that training modern LLMs often requires hundreds or even
thousands of GPUs [1], [27]. More supporting facts for this
claim are provided in Appendix A.
Another problem of full-data storage is statistically in-
efficient sampling [14]. To overcome this issue, we adopt
hierarchical sampling when utilizing stored historical data to
ensure equal coverage for the examples of each old task.
Specifically, rather than direct random sampling, we first
generate an old task ID and then select an example from
the sub-dataset of that task. Besides, a fixed ratio is kept
between old and new examples in each training batch. In this
way, ConPET is more robust to the data imbalance issue and
statistically more efficient in terms of equal coverage for each
historical task.
C. Dynamic ConPET
Despite the efficiency of Static ConPET, there still exists a
potential problem of low scalability. Specifically, under down-
stream tasks with extremely abundant emerging materials, the
volume of knowledge to be acquired may exceed the capacity
of tunable parameters and thus the performance will decrease.
This issue can be further exacerbated by PET due to the tiny
scale of PET modules.
Therefore, we introduce Dynamic ConPET to address this
issue, which is composed of a backbone LLM, a set of task-
specific PET modules, a PET module selector, and a cachesystem. We denote the PET module for the k-th task as Mk.
The working process at the k-th step can be summarized in two
procedures: (1) PET module pre-selection (Section III-C1):
We train the PET module selector to classify input examples
into the kseen schema sets {S1,S2, ...,Sk}. Then, tPET
modules corresponding to schema sets with top- tselection
scores are reserved as active ones. (2) Prediction with active
PET modules (Section III-C2): Active PET modules produce
logits on their own schema set respectively, which are then
concatenated to make the final prediction. We analyze the
training complexity in Section III-C3. As an auxiliary module
to address duplicate logit computations, the cache system is
introduced in Section III-C4. Dynamic ConPET also adopts
the same dynamic replay strategy.
1) PET Module Pre-Selection: To avoid the uncontrol-
lable linear growth of training costs as in former dynamic-
architecture-based methods, we employ PET module pre-
selection to select a fixed quantity of the most important PET
modules. Specifically, at the k-th step, we train a PET module
selector (also a PET module) to distinguish a fixed number tof
task schema sets that each example most probably belongs to
among {S1,S2, ...,Sk}. Then the PET modules specific for the
selected tschema sets are reserved as active ones to participate
in the subsequent inference.
Formally, the PET module selector converts each example
xinto a selection score vector sselof size kfollowing
Formula 1, whose j-th element stands for the confidence
thatxbelongs to Sj. The top- telements of ssel(with
indexes {i1, i2, ..., i t}) determine the selection of tactive PET
modules {Mi1,Mi2, ...,Mit}. Besides, we adopt the teacher-
forcing policy that the correct PET module corresponding to

--- PAGE 5 ---
5
the example is always selected for training. Through pre-
selection, the forward propagation cost is kept unaffected by
the number of PET modules. Experiments in Section IV-G also
demonstrate its positive effect on performance.
2) Prediction with Active PET Modules: To obtain the final
prediction, we integrate the learned information from active
modules {Mi1,Mi2, ...,Mit}. Formally, each active module
obtains a logit vector on its corresponding schema set follow-
ing Formula 1, while the logit vectors of inactive modules
(i.e., those unselected modules) are assigned as vectors of
an identical low enough value α. We then concatenate the
logits of all PET modules and figure out the prediction. This
procedure can be expressed as the following formulas,
sj=f(Mj,x), j∈ {i1, i2, ..., i t},
sj= [α, α, ..., α ], j /∈ {i1, i2, ..., i t},
sj∈R|Sj|, j= 1,2, ..., k,
pred = arg max [ s1,s2, ...,sk],(2)
where [·]denotes concatenation, sjrefers to the logit vector
produced by the j-th PET module, and pred means the
predicted label.
3) Detailed Algorithm and Complexity Analysis: Finally,
we conduct an analysis of the detailed algorithm and its
training complexity. The process of training Dynamic ConPET
for the k-th task is presented in Algorithm 1. Both the PET
module selector and task-specific modules adopt a standard
cross-entropy loss for multi-classification as the training ob-
jective. The PET module selector is iteratively updated. At
thek-th step, the selector is initialized by the (k−1)-th step
selector, except that its linear head should be expanded in
dimension to handle more schemas, which is the function of
Dimension Expand .
The above training process has a complexity independent
of the task number. Specifically, we limit the training batch
number of the PET module selector and the k-th task-specific
module to iter 1anditer 2respectively, and the batch size is
fixed as b. Meanwhile, tactive modules are reserved for each
example. Therefore, the training complexity for the k-th task
isO(b·(iter 1+iter 2·(t+ 1))) , which does not increase with
the task number k.
4) Cache System: To address duplicate logit computations,
we introduce an auxiliary cache system to store the logits
generated by already-tuned PET modules. Specifically, each
cache entry comprises the example ID, the PET module ID
(the index of a task-specific module or the selector), and the
logit vector. When a tuned PET module encounters an input
example, ConPET checks the database using the example ID
and PET module ID. If a match exists, the time-consuming
calculation in Equation 1 can be avoided. It should be noted
that the cache system is available for a specific module only
after the module completes its tuning process and remains
fixed thereafter. For instance, the PET Module selector can
only access the cache after its own training process in the
pre-selection stage is finished.Algorithm 1 Dynamic ConPET for the k-th task
Require: The training set Tkof the k-th task
Require: All the historical training data ˜Tk−1
Require: All the seen task schema sets ˜Sk
Require: The last PET module selector M(k−1)
sel
Require: The maximum training batch number iter 1and
iter 2
1:M(k)
sel←Dimension Expand(M(k−1)
sel)
2:fori←1toiter 1do
3: Sample Bnewrandomly from Tk
4: Sample Boldhierarchically from ˜Tk−1
5:Btot← B new∪ Bold
6: Update M(k)
selwithk-category classification loss on Btot
7:end for
8:Initialize thek-th task-specific module Mk
9:fori←1toiter 2do
10: Sample Bnewrandomly from Tk
11: Sample Boldhierarchically from ˜Tk−1
12:Btot← B new∪ Bold
13: forxinBtotdo
14: Select the most possible tactive PET modules for x
15: Predict forxwith active PET modules
16: end for
17: Update Mkwith|˜Sk|-category classification loss on
Btot
18:end for
IV. E XPERIMENTS
A. Datasets
For experiments, we focus on continual knowledge extrac-
tion as a representative continual adaptation scenario of LLMs,
including entity typing and knowledge extraction. Specifically,
we introduce the following 3 datasets as the benchmark for
continual entity typing:
(1)FewNERD . FewNERD [53] is a large manually-labeled
dataset with hierarchical 66 fine-grained entity types. Specif-
ically, we use FewNERD (SUP) to construct the benchmark,
including 486,044 examples from Wikipedia.
(2)OntoNotes . OntoNotes 5.0 [54] is also a manually
labeled dataset with 86 entity types, 264,404 sentences, and
multiple sources of data.
(3)BBN . Compared with the above two datasets, BBN [55]
is relatively smaller, with 111,728 examples and 46 entity
types. It is mainly employed to test the generalizability of
ConPET on small-scale tasks.
In addition, we test our method on 3 continual relation
extraction tasks:
(1)FewRel . FewRel [56] is a large-scale dataset for relation
extraction with 80 relation types and 56,000 examples from
Wikipedia.
(2)TACRED . TACRED [57] is a sentence-level relation
extraction dataset obtained through crowdsourcing. For sim-
plicity, we remove those examples with the label “n/a”, and
finally it consists of 21,773 sentences and 41 relation types.

--- PAGE 6 ---
6
TABLE II
THE STATISTICAL INFORMATION OF 6BENCHMARKS INVOLVED IN THIS WORK .
FewNERD OntoNotes BBN FewRel TACRED ACE 2005
#train 340,383 211,523 89,365 44,800 13,012 5,648
#valid 48,759 26,440 11,150 5,600 5,436 700
#test 96,902 26,441 11,213 5,600 3,325 722
#total 486,044 264,404 111,728 56,000 21,773 7,070
#type 66 86 46 80 41 18
#cluster 10 10 10 10 10 5
“#train”, “ #valid”, “ #test”, and “ #total” refer to the number of examples in the training set, the validation
set, the test set, and the whole dataset respectively. “ #type” represents the number of knowledge types
contained in a dataset. “ #cluster” denotes the total task number Lof the corresponding benchmark.
TABLE III
THE HYPER -PARAMETERS USED BY CONPET ON6DATASETS .
FewNERD OntoNotes BBN FewRel TACRED ACE 2005
learning rate St-ConPET 2e−5 1e−4 2e−4 2e−4 5e−4 5e−4
learning rate Dy-ConPET 1e−4 1e−4 2e−4 2e−4 5e−4 5e−4
weight decay 0.01 0.01 0.01 0.01 0.01 0.01
batch number limit 25,000 12,500 5,000 4,000 2,000 2,000
training batch size 8 8 8 8 8 8
maximum epoch number 10 10 10 10 20 20
maximum input length 256 256 256 128 128 128
example number 100 100 50 50 20 20
“St-ConPET” and “Dy-ConPET” refer to the settings with Static ConPET and Dynamic ConPET respectively.
(3)ACE 2005 . We adopt the English part of ACE 2005
Multilingual Training Corpus [58], which includes 7,070 sen-
tences and 18 relation types. Considering its small scale, it is
also aimed at testing the small-scale-task generalizability of
our method.
For each dataset, we construct the corresponding task se-
quence for continual learning by randomly splitting its entity
or relation types into clusters (5 clusters for ACE 2005 and
10 clusters for the others). Then the schema set of each task
is specified to one of these clusters.
The train-validation-test split methods of FewNERD and
TACRED are the same as the original works [53], [57]. For the
remaining 4 datasets, we randomly split them into the training,
validation, and test set by a ratio of 8 : 1 : 1 . More key
statistics of these 6 datasets are provided in Table II.
B. Dataset Source and License
All the textual materials included in the 6 datasets are in
English. The examples of FewNERD and FewRel are collected
from Wikipedia. The sentences in BBN are selected from
the Penn Treebank Corpus of Wall Street Journal. The data
sources of the remaining three datasets are mixed. TACRED
is collected from the newswire and web collection. ACE
2005 contains textual materials from broadcast conversations,
broadcast news, newsgroups, telephone conversations, and
weblogs. OntoNotes 5.0 is the most complicated, with a corpus
from a combination of telephone conversations, newswire,
newsgroups, broadcast news, broadcast conversation, weblogs,
and religious texts.
As for the licenses and data usage policy, FewNERD and
FewRel are released under the CC BY-SA 4.0 license, while
OntoNotes, BBN, TACRED, and ACE 2005 are used under the
Linguistic Data Consortium (LDC) data license as a member.All the datasets are used in a way consistent with their intended
use, and we limit public access to them under the requirements
of licenses. Through manual sampling, we do not find any
offensive content or identifiers in these datasets.
C. Experimental Settings
Two evaluation metrics are adopted: whole accuracy and
average accuracy . The former is a standard classification
accuracy on all the evaluation data ˜QL, where Lis the task
number. The latter averages the independent accuracies of
each seen task, which can better assess the ability to address
catastrophic forgetting [15].
We adopt LLaMA-7B [27] as the backbone LLM, which
has 32 layers and a hidden size of 4,096. LoRA [24] is used
as a representative PET method in our experiments. Including
LoRA matrices and a linear head, a PET module contains
around 2M parameters, which only accounts for 0.03% of the
LLM (about 6.74B parameters). For efficiency, the rank of
LoRA matrices and the number tof active PET modules are
set to 4 and 1 respectively under all settings. The ratio between
old and new examples in each batch is fixed as 1 : 1.
For Static ConPET, we mainly adapt it to the following three
memory-based methods:
(1)EMR . EMR [10] is a basic memory-based continual
learning method, which saves a fixed number of examples for
each task and then replays them with new data combined when
training a new model.
(2)EA-EMR . EA-EMR [14] is an extension of EMR. In
addition to memory replay, it conducts embedding alignment at
the end of each step to alleviate the distortion of the embedding
space of old knowledge types.

--- PAGE 7 ---
7
TABLE IV
THE MAJOR EXPERIMENT RESULTS ON SIX DATASETS . “W” AND “A” STAND FOR THE WHOLE ACCURACY (%) AND AVERAGE ACCURACY (%)
RESPECTIVELY . EMR*, EA-EMR*, AND CRL* REFER TO THE STATIC CONPET ADAPTED VERSION OF EMR, EA-EMR, AND CRL
RESPECTIVELY . “D Y-CONPET” MEANS DYNAMIC CONPET.
FewNERD OntoNotes BBN FewRel TACRED ACE 2005
W A W A W A W A W A W A
Baseline: Memory-based Methods +PET Adaptation alone
EMR 57.77 66.03 64.46 68.26 53.30 65.56 84.75 84.75 65.43 62.99 41.27 49.66
EA-EMR 48.61 60.15 67.63 71.28 46.72 64.43 83.12 83.12 59.20 55.70 41.14 53.81
CRL 72.73 75.48 61.39 62.65 55.98 64.02 84.38 84.37 75.91 72.61 54.29 61.96
Memory-based Methods +Static ConPET
EMR* 74.41 77.13 85.12 86.11 80.05 83.13 89.30 89.30 80.44 75.75 79.85 80.53
EA-EMR* 73.32 75.97 84.61 85.89 80.33 84.50 88.61 88.61 84.56 80.89 83.31 82.78
CRL* 75.72 77.31 67.94 68.22 71.03 72.00 89.48 89.48 85.29 81.03 82.55 82.53
Dynamic ConPET
Dy-ConPET 76.15 78.22 84.47 85.83 76.15 81.16 88.62 88.62 84.47 80.27 80.27 80.76
Upper Bound
Limitless 79.49 77.80 88.19 87.73 89.61 85.97 90.34 90.34 87.40 82.58 84.07 83.22
Bold entities represent the best results. Each result in this table except the time-consuming “Limitless” setting is averaged from a
two-time replication.
TABLE V
THE STANDARD DEVIATIONS OF THE MAJOR EXPERIMENTAL RESULTS FROM TWO -TIME REPLICATION ON 6DATASETS .
FewNERD OntoNotes BBN FewRel TACRED ACE 2005
W A W A W A W A W A W A
EMR 2.87 1.66 0.67 0.50 2.41 1.28 0.70 0.70 0.49 0.51 0.55 0.17
EA-EMR 3.32 3.33 1.90 1.73 2.12 0.34 2.17 2.17 0.07 0.41 1.11 1.22
CRL 0.14 0.41 1.95 1.84 1.20 0.69 0.17 0.17 3.13 3.21 2.54 3.25
EMR* 0.68 0.44 0.29 0.19 0.12 0.13 0.16 0.16 2.30 2.27 2.98 2.52
EA-EMR* 1.61 0.12 0.73 0.81 0.53 0.10 0.52 0.52 0.91 0.52 1.18 2.10
CRL* 0.01 0.06 0.53 0.51 0.34 0.05 0.07 0.07 0.33 0.58 0.97 1.27
Dy-ConPET 0.12 0.01 0.02 0.02 1.66 0.34 0.57 0.57 0.34 0.66 1.17 1.02
“W” and “A” stands for the whole accuracy (%) and average accuracy (%) respectively.
(3)CRL . CRL [9], also an EMR extension, further applies
contrastive learning and knowledge distillation when replaying
memorized examples to retain the old relation knowledge.
As the vanilla full-parameter fine-tuning occupies abundant
computation resources with more than 3,000 times tunable
parameters, we introduce the above three methods with PET
adaptation alone as baselines, which adopt limited memory
and select a fixed number of memorized examples for each
knowledge type through K-Means clustering.
In addition, we introduce an upper-bound setting Limitless
for reference. Specifically, it shares the same working process
as Dynamic ConPET, except that its limit on the training batch
number is removed. Therefore, at the k-th step, it is trained
with full-volume historical data ˜Tkand suffers from a linearly
increasing complexity of O(|˜Tk| ·(t+ 2)) .
D. Hyper-Parameters and Implementation Details
The experimental hyper-parameters are tuned through grid
search for each dataset respectively. Besides the most impor-
tant parameters introduced in Section IV-C, other parameters
for ConPET are listed in Table III. Specifically, for Dynamic
ConPET, the learning rate on FewNERD is adjusted to 5e−4
for the first two continual learning steps, and the learning rateon OntoNotes is modified to 5e−4for the first three steps
in all settings. The batch number limit refers to the restriction
to the total training and validation batch number at each step
if the maximum epoch number is reached, as mentioned in
Section III-B2. The ratio between the training and validation
batch number of ConPET is set to 4 : 1 throughout our
experiments. While the table provides the maximum epoch
number, the best epoch is chosen according to the average
accuracies on the validation data. Therefore, the actual training
steps of the best-epoch model may be lower than the provided
batch number limit. The “example number” refers to the
memorized example number for each knowledge type at each
step of the PET-only baseline version of three memory-based
methods EMR, EA-EMR, and CRL, whose replay frequency
is dynamically adjusted to obtain a total training step not lower
than ConPET with dynamic sampling strategy.
The implementation of ConPET is based on PyTorch and
each ConPET instance involved in our experiments is trained
on a single A100 GPU for 1 to 48 hours, depending on the
corresponding dataset scale. Tools and packages that we used
in the experiments include: PyTorch, transformers, numpy,
scikit-learn, tqdm, and loralib.

--- PAGE 8 ---
8
1 2 3 4 5 6 7 8 9 10
task number6065707580859095average accuracy (%)
FewNERD
Dy-ConPET
EMR*
EA-EMR*
CRL*
EMR
EA-EMR
CRL
12345678910708090100
OntoNotes
12345678910708090100
BBN
12345678910859095100
FewRel
1234567891060708090
TACRED
1 2 3 4 5405060708090
ACE 2005
Fig. 3. The average accuracies (%) of different settings at each step throughout the learning process.
E. Overall Results
The main experimental results are shown in Table IV. The
average accuracies of different settings at each step are shown
in Figure 3. From the table and figure, we can come to the
following conclusions.
(1) Overall tendency: All the settings decline in average
accuracies with the increase of task number, which reveals
the inevitable impacts and challenges brought by catastrophic
forgetting.
(2) Effectiveness: Both Static ConPET settings and Dy-
namic ConPET surpass the baselines by a large margin,
which reveals their effectiveness. As all three memory-based
methods adapted with Static ConPET significantly exceed
corresponding PET-only versions, we can also demonstrate
the importance of dynamic sampling strategy. Of course, gaps
still exist between ConPET and the reference upper bound
with linearly growing complexity, indicating that a large room
remains for exploration in the continual task-specific fine-
tuning of LLMs.
(3) Application Scenarios: While both Dynamic ConPET
and Static ConPET yield satisfactory outcomes, the Static
version excels on five benchmarks. A possible reason is the
small scale of these datasets, which fits within the capacity of
a single PET module, thus minimizing the negative impact of
low scalability. Notably, Static ConPET demonstrates signifi-cant superiority in BBN and ACE 2005, which have the least
knowledge schemas and examples among entity typing and
relation extraction datasets respectively. In contrast, Dynamic
ConPET outperforms Static ConPET on FewNERD with a
large schema set and the most examples. Therefore, we can
conclude that Static ConPET is more suitable for scenar-
ios with relatively small-scale emerging data and knowledge
schemas. Conversely, we need Dynamic ConPET to handle
larger schema sets and more extensive data, which demand
higher scalability of the continual fine-tuning architecture.
All the scores displayed in Table IV except the “Limitless”
setting are the average value of the results from a two-
time replication with different random seeds. The standard
deviations of these results are shown in Table V. Due to the
extremely high complexity and training costs of “Limitless”,
we only take its result from a single run.
F . Effect of Parameter-Efficient Tuning
Despite the fact that PET can significantly reduce the costs
of parameter updates and GPU memory, it may lead to a
decrease in overall performance [4]. Due to the huge costs
of tuning the full-parameter LLaMA-7B, we only involve the
ablation study on the smaller 335M BERT-Large [16] as an
alternative to demonstrating the rationality of PET adaptation.
Concretely, we introduce the parallel settings on FewNERD

--- PAGE 9 ---
9
TABLE VI
THE ABLATION RESULTS CONCERNING THE PET ADAPTATION
INCONPET BASED ON BERT-L ARGE AND FEWNERD
BENCHMARK .
PET w/o PET
W A W A
EMR* 74.03 76.52 76.19 77.50
EA-EMR* 74.59 77.96 77.64 75.90
CRL* 73.80 75.87 77.29 77.70
Dy-ConPET 74.25 76.60 76.97 76.87
#param 0.4M 335M
“PET” means the original ConPET setting and “w/o PET” refers
to the parallel setting without PET. “ #param” is the number of
tunable parameters.
for EMR*, EA-EMR*, CRL*, and Dynamic ConPET (Dy-
ConPET) without PET adaptation. The results are shown in
Table VI. As can be observed, although ConPET has only
0.12% tunable parameters and thus saves considerable time
and computation resources, the drop in accuracy is not sig-
nificant, which demonstrates the reasonableness of employing
PET. The satisfactory results on BERT-Large also illustrate the
generalizability of ConPET to relatively smaller pre-trained
language models.
G. Effect of PET Module Pre-Selection
TABLE VII
THE ABLATION RESULTS CONCERNING THE PET- MODULE
PRE-SELECTION IN DYNAMIC CONPET ONFEWNERD AND
ONTONOTES .
FewNERD OntoNotes
W A W A
Dy-ConPET 76.15 78.22 84.47 85.83
w/o Sel 70.75 76.22 81.85 83.37
“w/o Sel” refers to the parallel setting without this technique.
In Section III-C1, we discuss the effect of PET module pre-
selection on efficiency, which is to select a fixed number of ac-
tive PET modules in Dynamic ConPET and ensure a constant
forward propagation cost. Here we further analyze its effect
from the aspect of performance. We conduct experiments on a
parallel setting without this technique, which is corresponding
to the situation with t=kat the k-th step. The results are
shown in Table VII.
As can be observed, the performance significantly drops
without PET module pre-selection, although the setting “w/o
Sel” makes all PET modules active and has a linearly increas-
ing complexity of O(b·(iter 1+iter 2·(k+ 1))) . This may be
attributed to the mutual interference between the logit vectors
produced by kPET modules. Even if the dynamic architecture
can retain the classification capability on each independent
task schema set, it is nontrivial to distinguish between the
schemas of distinct tasks. Therefore, a PET module selector
explicitly taking this responsibility can largely boost the over-
all performance as well as reduce the complexity.
H. Effect of Different Task SplitsTABLE VIII
THE PERFORMANCE OF DYNAMIC CONPET ON
FEWNERD GIVEN DISTINCT TASK SPLITS .
W A Accsel
Correlated 76.15 78.22 80.16
Independent 77.62 76.18 88.91
“Correlated” refers to the random split of length 10,
while “Independent” is based on coarse types with
length 8. Accseldenotes the top-1 accuracy of the PET
module pre-selection.
In our experiments, task splits are generated by randomly
clustering knowledge schemas. However, these schemas may
exhibit correlation, which leads to non-independent tasks.
Such correlations have the potential to impact the performance
of continual learning, especially Dynamic ConPET, whose
architecture heavily relies on the task split. Therefore, this
section will focus on the effect of different task splits on
Dynamic ConPET.
Based on FewNERD, which has a hierarchical entity type
schema with 8 coarse types, we reconstruct a new mutually
independent task sequence of length 8 by assigning each
coarse type to a task, while the original random split is inter-
correlated. The results are shown in Table VIII. While the
overall accuracy does not change substantially, the independent
split increases the pre-selection accuracy by a large margin,
which implies a decrease in the accuracies of task-specific
PET modules. This highlights a trade-off between the PET
module pre-selection and downstream inner-task classification,
as the upstream selector prefers an independent task split
but the downstream PET modules favor a correlated one. To
achieve optimal results, we should align the capacity of each
PET module (including the selector) to its task difficulty. For
instance, further splitting a task-specific PET module may be
beneficial if the task exceeds its capacity.
Moreover, it may be reasonable to explore a wiser strategy
for maintaining the task splits. A possible improvement is to
adopt the knowledge-aware hierarchical organization of PET
modules. Concretely, instead of limiting the layer number
of PET modules to 2 (i.e., one layer of the PET selector
and one layer of task-specific modules), we can introduce
a PET module tree with multiple layers, where non-leaf
PET modules are responsible for conducting pre-selection
on its child nodes, and leaf PET modules make the final
prediction. Meanwhile, considering the hierarchical nature of
some knowledge schemas (e.g., the entity types in FewNERD,
OntoNotes, and BBN), we can assign the responsibility of PET
modules according to their positions in the knowledge hierar-
chy structure rather than the chronological order. Therefore,
hierarchical knowledge can be incorporated explicitly and the
capacity of each PET module can easily controlled given a
well-designed knowledge schema. We leave this improvement
for future work.
V. C ONCLUSION AND FUTURE WORK
In this paper, we mainly discuss the efficient and effective
adaptation of LLMs to continual downstream task sequences.

--- PAGE 10 ---
10
To achieve this goal, we propose the paradigm of ConPET,
including two versions with training complexity independent
of the task number. Static ConPET can adapt former memory-
based methods to LLMs through the cost-saving PET and
a dynamic sampling strategy more robust to over-fitting and
forgetting. In contrast, Dynamic ConPET is more scalable to
scenarios with large-scale data and task schemas owing to its
dynamic MoE-style architecture. The experiments demonstrate
the effectiveness and rationality of key techniques used in
ConPET, with a considerable reduction in tuning costs. In
the future, we will extend ConPET to more diverse continual
learning scenarios (e.g., continual learning of heterogeneous
tools [8]) and further improve our paradigm by exploring wiser
task split strategies.
APPENDIX
MORE SUPPORTING FACTS FOR THE RATIONALITY OF
DYNAMIC SAMPLING STRATEGY
Despite the common assumption of limited memory in the
field of continual learning [14], we still consider it more
reasonable to control the training costs rather than the memory
during the process of fine-tuning LLMs, which is the fun-
damental philosophy of our dynamic sampling strategy. The
rationality of this approach mainly lies in the overwhelming
price of time and computation resources when compared to
storage.
Take GPT-3 175B [1] as an example, whose training corpus
contains about 300B tokens. As a single English token contains
about 4 characters on average1, the overall storage for its
training corpus is around 1∼2TB. Such a scale of memory
is quite acceptable for the majority of modern servers for
AI research. Although the training materials for more recent
LLMs are believed to take up more space, the price of storage
has already been made moderate as no more than a few dozen
dollars per TB in general thanks to the advance in storage
hardware technology2. Besides, since the datasets of most
downstream tasks should be filtered and annotated to ensure
high quality and provide supervision for training, their scales
can hardly reach the TB level and typically need much less
storage than the LLM training corpus.
On the other hand, the computation resources required for
GPT-3 175B (pre-trained on V100 GPUs) are at a more
tremendous scale, which is around 3.64E+03 petaflop/s-days.
Even if PET can save many computation resources, a single
V100 GPU still costs thousands of dollars in a month3, let
alone the fact that LLMs with more than tens of billions of
parameters may need more than one devices for fine-tuning
and inference. In summary, the storage issue is just of little
significance compared to the financial pressure posed by time
and computation resources.
REFERENCES
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell
1https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-
count-them
2https://diskprices.com/
3https://cloud.google.com/compute/gpus-pricinget al. , “Language models are few-shot learners,” Advances in
neural information processing systems , vol. 33, pp. 1877–1901,
2020. [Online]. Available: https://proceedings.neurips.cc/paper files/
paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[2] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
A. M. Dai, and Q. V . Le, “Finetuned language models are zero-shot
learners,” arXiv preprint arXiv:2109.01652 , 2021. [Online]. Available:
https://arxiv.org/pdf/2109.01652
[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language
models to follow instructions with human feedback,” Advances in Neural
Information Processing Systems , vol. 35, pp. 27 730–27 744, 2022.
[Online]. Available: https://proceedings.neurips.cc/paper files/paper/
2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf
[4] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu,
Y . Chen, C.-M. Chan, W. Chen et al. , “Delta tuning: A comprehensive
study of parameter efficient methods for pre-trained language
models,” arXiv preprint arXiv:2203.06904 , 2022. [Online]. Available:
https://arxiv.org/pdf/2203.06904.pdf
[5] X. Jin, D. Zhang, H. Zhu, W. Xiao, S.-W. Li, X. Wei, A. Arnold,
and X. Ren, “Lifelong pretraining: Continually adapting language
models to emerging corpora,” in Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , 2022, pp. 4764–4780.
[Online]. Available: https://aclanthology.org/2022.naacl-main.351.pdf
[6] A. Daruna, M. Gupta, M. Sridharan, and S. Chernova, “Continual
learning of knowledge graph embeddings,” IEEE Robotics and
Automation Letters , vol. 6, no. 2, pp. 1128–1135, 2021. [Online].
Available: https://ieeexplore.ieee.org/abstract/document/9343669
[7] N. Monaikul, G. Castellucci, S. Filice, and O. Rokhlenko, “Continual
learning for named entity recognition,” in Proceedings of the
AAAI Conference on Artificial Intelligence , 2021, pp. 13 570–
13 577. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/17600/17407
[8] Y . Qin, S. Hu, Y . Lin, W. Chen, N. Ding, G. Cui, Z. Zeng,
Y . Huang, C. Xiao, C. Han et al. , “Tool learning with foundation
models,” arXiv preprint arXiv:2304.08354 , 2023. [Online]. Available:
https://arxiv.org/pdf/2304.08354
[9] K. Zhao, H. Xu, J. Yang, and K. Gao, “Consistent representation learning
for continual relation extraction,” in Findings of the Association for
Computational Linguistics: ACL 2022 , 2022, pp. 3402–3411. [Online].
Available: https://aclanthology.org/2022.findings-acl.268.pdf
[10] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter,
“Continual lifelong learning with neural networks: A review,”
Neural Networks , vol. 113, pp. 54–71, 2019. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0893608019300231
[11] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural
networks,” arXiv preprint arXiv:1606.04671 , 2016. [Online]. Available:
https://arxiv.org/pdf/1606.04671.pdf
[12] X. Gu, L. Liu, H. Yu, J. Li, C. Chen, and J. Han, “On the transformer
growth for progressive BERT training,” in Proceedings of the 2021
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , 2021, pp.
5174–5180. [Online]. Available: https://aclanthology.org/2021.naacl-
main.406.pdf
[13] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and
G. Wayne, “Experience replay for continual learning,” Advances
in Neural Information Processing Systems , vol. 32, 2019.
[Online]. Available: https://proceedings.neurips.cc/paper/2019/file/
fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf
[14] H. Wang, W. Xiong, M. Yu, X. Guo, S. Chang, and W. Y . Wang,
“Sentence embedding alignment for lifelong relation extraction,” in
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , 2019, pp. 796–806.
[Online]. Available: https://aclanthology.org/N19-1086.pdf
[15] X. Han, Y . Dai, T. Gao, Y . Lin, Z. Liu, P. Li, M. Sun, and
J. Zhou, “Continual relation learning via episodic memory activation
and reconsolidation,” in Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , 2020, pp. 6429–6440.
[Online]. Available: https://aclanthology.org/2020.acl-main.573.pdf
[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language understanding,”
arXiv preprint arXiv:1810.04805 , 2018. [Online]. Available: https:
//arxiv.org/pdf/1810.04805.pdf

--- PAGE 11 ---
11
[17] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V . Stoyanov, “RoBERTa: A robustly
optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692 ,
2019. [Online]. Available: https://arxiv.org/pdf/1907.11692.pdf
[18] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient
transfer learning for NLP,” in International Conference on Machine
Learning . PMLR, 2019, pp. 2790–2799. [Online]. Available: http:
//proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf
[19] X. L. Li and P. Liang, “Prefix-Tuning: Optimizing continuous
prompts for generation,” in Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , 2021, pp. 4582–4597. [Online]. Available:
https://aclanthology.org/2021.acl-long.353.pdf
[20] T. Gao, A. Fisch, and D. Chen, “Making pre-trained language
models better few-shot learners,” in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , 2021, pp. 3816–3830. [Online]. Available:
https://aclanthology.org/2021.acl-long.295.pdf
[21] J. Lee, R. Tang, and J. Lin, “What would elsa do? freezing layers
during transformer fine-tuning,” arXiv preprint arXiv:1911.03090 , 2019.
[Online]. Available: https://arxiv.org/pdf/1911.03090.pdf
[22] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch ¨utze, “Masking as an
efficient alternative to finetuning for pretrained language models,” in
Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing , 2020, pp. 2226–2241. [Online]. Available:
https://aclanthology.org/2020.emnlp-main.174.pdf
[23] E. B. Zaken, Y . Goldberg, and S. Ravfogel, “Bitfit: Simple parameter-
efficient fine-tuning for transformer-based masked language-models,”
inProceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , 2022, pp. 1–9.
[Online]. Available: https://aclanthology.org/2022.acl-short.1.pdf
[24] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang,
L. Wang, and W. Chen, “LoRA: Low-rank adaptation of large language
models,” arXiv preprint arXiv:2106.09685 , 2021. [Online]. Available:
https://arxiv.org/pdf/2106.09685.pdf
[25] Y . Qin, X. Wang, Y . Su, Y . Lin, N. Ding, Z. Liu, J. Li, L. Hou, P. Li,
M. Sun et al. , “Exploring low-dimensional intrinsic task subspace
via prompt tuning,” arXiv preprint arXiv:2110.07867 , 2021. [Online].
Available: https://arxiv.org/pdf/2110.07867.pdf
[26] X. Sun, Y . Ji, B. Ma, and X. Li, “A comparative study between
full-parameter and lora-based fine-tuning on chinese instruction data
for instruction following large language model,” arXiv preprint
arXiv:2304.08109 , 2023. [Online]. Available: https://arxiv.org/pdf/2304.
08109.pdf
[27] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. ,
“LLaMA: Open and efficient foundation language models,” arXiv
preprint arXiv:2302.13971 , 2023. [Online]. Available: https://arxiv.org/
pdf/2302.13971.pdf
[28] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-
Barwinska et al. , “Overcoming catastrophic forgetting in neural
networks,” Proceedings of the national academy of sciences , vol.
114, no. 13, pp. 3521–3526, 2017. [Online]. Available: https:
//www.pnas.org/doi/epdf/10.1073/pnas.1611835114
[29] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang,
“Overcoming catastrophic forgetting by incremental moment
matching,” in Proceedings of the 31st International Conference
on Neural Information Processing Systems , 2017, pp. 4655–
4665. [Online]. Available: https://proceedings.neurips.cc/paper/2017/
file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf
[30] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr,
“Riemannian walk for incremental learning: Understanding forgetting
and intransigence,” in Proceedings of the European Conference
on Computer Vision (ECCV) , 2018, pp. 532–547. [Online].
Available: https://openaccess.thecvf.com/content ECCV 2018/papers/
Arslan Chaudhry Riemannian Walk ECCV 2018 paper.pdf
[31] Z. Li and D. Hoiem, “Learning without forgetting,” IEEE transactions
on pattern analysis and machine intelligence , vol. 40, no. 12,
pp. 2935–2947, 2017. [Online]. Available: https://ieeexplore.ieee.org/
abstract/document/8107520
[32] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang,
and C.-C. J. Kuo, “Class-incremental learning via deep model
consolidation,” in Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision , 2020, pp. 1131–1140. [Online].
Available: https://openaccess.thecvf.com/content WACV 2020/papers/
Zhang Class-incremental Learning viaDeep Model Consolidation
WACV 2020 paper.pdf
[33] T. Chen, I. Goodfellow, and J. Shlens, “Net2Net: Accelerating learning
via knowledge transfer,” arXiv preprint arXiv:1511.05641 , 2015.
[Online]. Available: https://arxiv.org/pdf/1511.05641.pdf
[34] D. Isele and A. Cosgun, “Selective experience replay for lifelong
learning,” in Proceedings of the AAAI Conference on Artificial
Intelligence , 2018. [Online]. Available: https://ojs.aaai.org/index.php/
AAAI/article/view/11595/11454
[35] C. Qin and S. Joty, “LFPT5: A unified framework for lifelong few-shot
language learning based on prompt tuning of T5,” in International
Conference on Learning Representations , 2021. [Online]. Available:
https://openreview.net/pdf?id=HCRVf71PMF
[36] A. Razdaibiedina, Y . Mao, R. Hou, M. Khabsa, M. Lewis, and
A. Almahairi, “Progressive prompts: Continual learning for language
models,” in The Eleventh International Conference on Learning
Representations , 2022. [Online]. Available: https://openreview.net/pdf?
id=UJTgQBc91
[37] A. Madotto, Z. Lin, Z. Zhou, S. Moon, P. A. Crook, B. Liu, Z. Yu,
E. Cho, P. Fung, and Z. Wang, “Continual learning in task-oriented
dialogue systems,” in Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , 2021, pp. 7452–7467.
[Online]. Available: https://aclanthology.org/2021.emnlp-main.590.pdf
[38] Q. Gao, C. Zhao, Y . Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, “A
unified continual learning framework with general parameter-efficient
tuning,” arXiv preprint arXiv:2303.10070 , 2023. [Online]. Available:
https://arxiv.org/pdf/2303.10070.pdf
[39] Y . Qin, J. Zhang, Y . Lin, Z. Liu, P. Li, M. Sun, and J. Zhou,
“Elle: Efficient lifelong pre-training for emerging data,” in Findings of
the Association for Computational Linguistics: ACL 2022 , 2022, pp.
2789–2810. [Online]. Available: https://aclanthology.org/2022.findings-
acl.220.pdf
[40] Z. Ke, Y . Shao, H. Lin, T. Konishi, G. Kim, and B. Liu, “Continual
pre-training of language models,” in The Eleventh International
Conference on Learning Representations , 2023. [Online]. Available:
https://openreview.net/pdf?id=m GDIItaI3o
[41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,
and J. Dean, “Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer,” in International Conference on Learning
Representations , 2017. [Online]. Available: https://openreview.net/pdf?
id=B1ckMDqlg
[42] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang,
M. Krikun, N. Shazeer, and Z. Chen, “GShard: Scaling giant
models with conditional computation and automatic sharding,”
arXiv preprint arXiv:2006.16668 , 2020. [Online]. Available: https:
//arxiv.org/pdf/2006.16668
[43] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer,
“BASE Layers: Simplifying training of large, sparse models,” in
International Conference on Machine Learning . PMLR, 2021,
pp. 6265–6274. [Online]. Available: http://proceedings.mlr.press/v139/
lewis21a/lewis21a.pdf
[44] S. Roller, S. Sukhbaatar, J. Weston et al. , “Hash layers
for large sparse models,” Advances in Neural Information
Processing Systems , vol. 34, pp. 17 555–17 566, 2021.
[Online]. Available: https://proceedings.neurips.cc/paper files/paper/
2021/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf
[45] W. Fedus, B. Zoph, and N. Shazeer, “Switch Transformers: Scaling
to trillion parameter models with simple and efficient sparsity,” The
Journal of Machine Learning Research , vol. 23, no. 1, pp. 5232–5270,
2022. [Online]. Available: https://jmlr.org/papers/volume23/21-0998/21-
0998.pdf
[46] Z. Zhang, Y . Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, “MoEfication:
Transformer feed-forward layers are mixtures of experts,” in Findings
of the Association for Computational Linguistics: ACL 2022 , 2022, pp.
877–890. [Online]. Available: https://aclanthology.org/2022.findings-
acl.71.pdf
[47] S. Gururangan, M. Lewis, A. Holtzman, N. A. Smith, and
L. Zettlemoyer, “DEMix layers: Disentangling domains for modular
language modeling,” in Proceedings of the 2022 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , 2022, pp. 5557–5576.
[Online]. Available: https://aclanthology.org/2022.naacl-main.407.pdf
[48] S. Shen, L. Hou, Y . Zhou, N. Du, S. Longpre, J. Wei, H. W.
Chung, B. Zoph, W. Fedus, X. Chen, T. Vu, Y . Wu, W. Chen,
A. Webson, Y . Li, V . Zhao, H. Yu, K. Keutzer, T. Darrell, and D. Zhou,

--- PAGE 12 ---
12
“Mixture-of-experts meets instruction tuning: A winning combination
for large language models,” arXiv preprint arXiv:2305.14705 , 2023.
[Online]. Available: https://arxiv.org/pdf/2305.14705.pdf
[49] T. Zadouri, A. ¨Ust¨un, A. Ahmadian, B. Ermis ¸, A. Locatelli, and
S. Hooker, “Pushing mixture of experts to the limit: Extremely parameter
efficient moe for instruction tuning,” arXiv preprint arXiv:2309.05444 ,
2023. [Online]. Available: https://arxiv.org/pdf/2309.05444.pdf
[50] L. B. Soares, N. Fitzgerald, J. Ling, and T. Kwiatkowski, “Matching
the blanks: Distributional similarity for relation learning,” in
Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics , 2019, pp. 2895–2905. [Online]. Available:
https://aclanthology.org/P19-1279.pdf
[51] N. Ding, Y . Chen, X. Han, G. Xu, P. Xie, H.-T. Zheng, Z. Liu,
J. Li, and H.-G. Kim, “Prompt-learning for fine-grained entity
typing,” arXiv preprint arXiv:2108.10604 , 2021. [Online]. Available:
https://arxiv.org/pdf/2108.10604.pdf
[52] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale
for parameter-efficient prompt tuning,” in Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing ,
2021, pp. 3045–3059. [Online]. Available: https://aclanthology.org/
2021.emnlp-main.243.pdf
[53] N. Ding, G. Xu, Y . Chen, X. Wang, X. Han, P. Xie, H. Zheng, and
Z. Liu, “Few-NERD: A few-shot named entity recognition dataset,”
inProceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers) , 2021,
pp. 3198–3213. [Online]. Available: https://aclanthology.org/2021.acl-
long.248.pdf
[54] R. Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Pradhan,
L. Ramshaw, N. Xue, A. Taylor, J. Kaufman, M. Franchini et al. ,
“OntoNotes release 5.0 ldc2013t19,” Linguistic Data Consortium,
Philadelphia, PA , vol. 23, 2013. [Online]. Available: https://catalog.ldc.
upenn.edu/LDC2013T19
[55] R. Weischedel and A. Brunstein, “BBN pronoun coreference and entity
type corpus,” Linguistic Data Consortium, Philadelphia , vol. 112,
2005. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2005T33
[56] X. Han, H. Zhu, P. Yu, Z. Wang, Y . Yao, Z. Liu, and M. Sun, “FewRel:
A large-scale supervised few-shot relation classification dataset with
state-of-the-art evaluation,” in Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing , 2018, pp.
4803–4809. [Online]. Available: https://aclanthology.org/D18-1514.pdf
[57] Y . Zhang, V . Zhong, D. Chen, G. Angeli, and C. D. Manning,
“Position-aware attention and supervised data improve slot filling,”
inProceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing , 2017, pp. 35–45. [Online]. Available:
https://nlp.stanford.edu/pubs/zhang2017tacred.pdf
[58] C. Walker, S. Strassel, J. Medero, and K. Maeda, “ACE 2005
multilingual training corpus,” Linguistic Data Consortium, Philadelphia ,
vol. 57, p. 45, 2006. [Online]. Available: https://catalog.ldc.upenn.edu/
LDC2006T06
