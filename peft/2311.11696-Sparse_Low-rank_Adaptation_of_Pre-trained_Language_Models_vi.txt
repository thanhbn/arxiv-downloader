# 2311.11696.pdf - Bản dịch tiếng Việt
# Chuyển đổi từ PDF sang TXT và dịch sang tiếng Việt
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2311.11696.pdf
# Kích thước file: 2175155 bytes

===============================================
NỘI DUNG FILE PDF - BẢN DỊCH TIẾNG VIỆT
===============================================


--- TRANG 1 ---
Thích ứng Thưa thớ Hạng thấp của các Mô hình Ngôn ngữ Tiền huấn luyện
Ning Ding1∗, Xingtai Lv1∗, Qiaosen Wang4, Yulin Chen2
Bowen Zhou1†, Zhiyuan Liu2,3†, Maosong Sun2,3†
1Khoa Kỹ thuật Điện tử, Đại học Tsinghua
2Khoa Khoa học Máy tính và Công nghệ, Đại học Tsinghua
3BNRIST, IAI, Đại học Tsinghua, 4Khoa Thống kê, Đại học Chicago
dn97@mail.tsinghua.edu.cn, lvxt20@mails.tsinghua.edu.cn

Tóm tắt
Tinh chỉnh các mô hình ngôn ngữ lớn tiền huấn luyện theo cách hiệu quả tham số được nghiên cứu rộng rãi do tính hiệu quả và hiệu suất của nó. Phương pháp phổ biến về thích ứng hạng thấp (LoRA) cung cấp một cách tiếp cận đáng chú ý, đưa ra giả thuyết rằng quá trình thích ứng có bản chất là chiều thấp. Mặc dù LoRA đã thể hiện hiệu suất đáng khen ngợi, nhưng nó được triển khai với một hạng nội tại cố định và không thể thay đổi, điều này có thể không phải lúc nào cũng là sự lựa chọn lý tưởng. Nhận thức được nhu cầu về khả năng thích ứng linh hoạt hơn, chúng tôi mở rộng phương pháp của LoRA thành một cách tiếp cận đổi mới mà chúng tôi gọi là thích ứng thưa thớ hạng thấp (SoRA) cho phép điều chỉnh động hạng nội tại trong quá trình thích ứng. Chúng tôi đạt được điều này thông qua việc tích hợp một đơn vị cổng được tối ưu hóa bằng phương pháp gradient gần đúng trong giai đoạn huấn luyện, kiểm soát tính chất lực lượng của hạng dưới độ thưa của cổng. Trong giai đoạn suy luận tiếp theo, chúng tôi loại bỏ các khối tham số tương ứng với các hạng bị triệt tiêu, để giảm mỗi module SoRA trở lại một LoRA ngắn gọn nhưng tối ưu hạng. Cách tiếp cận của chúng tôi tăng cường sức mạnh biểu diễn của LoRA bằng cách khởi tạo nó với một hạng cao hơn, đồng thời kiểm soát hiệu quả số lượng tham số tạm thời tăng lên thông qua việc cập nhật theo cách thưa thớ. Chúng tôi tiếp tục giới thiệu một bộ lập lịch thưa thớ hóa cho SoRA, nhằm mục đích kiểm tra tác động của số lượng tham số khác không đến khả năng ghi nhớ và khái quát hóa của mô hình. Kết quả thực nghiệm của chúng tôi cho thấy rằng SoRA có thể vượt trội hơn các phương pháp cơ sở khác ngay cả với 70% tham số được giữ lại và 70% thời gian huấn luyện.

1 Giới thiệu
Thích ứng các mô hình ngôn ngữ tiền huấn luyện quy mô lớn (Devlin et al., 2019; Brown et al., 2020; He et al., 2020; Bommasani et al., 2021; Han et al., 2021; Touvron et al., 2023) theo cách hiệu quả tham số (He et al., 2022; Ding et al., 2023; Hu et al., 2023) ngày càng thu hút sự chú ý trong cộng đồng nghiên cứu. Các phương pháp của mô hình này thường giữ nguyên hầu hết các tham số của mô hình cơ bản, hoặc chèn thêm các tham số có thể huấn luyện vào mô hình (Houlsby et al., 2019; Li and Liang, 2021), hoặc chỉ định một số lượng nhỏ tham số (Zaken et al., 2021; Liu et al., 2021; Su et al., 2023) có thể huấn luyện hoặc tham số hóa lại quá trình thích ứng thành một dạng hiệu quả hơn (Hu et al., 2021; Qin et al., 2021). Chúng đã được xác thực là hiệu quả trên các mô hình và nhiệm vụ khác nhau, thường mang lại kết quả tương đương hoặc thậm chí tốt hơn so với tinh chỉnh tham số đầy đủ.

Tiềm năng phát triển của tinh chỉnh hiệu quả tham số trở nên rõ ràng sau khi xác thực rộng rãi hiệu suất của nó. Các phương pháp này mang lại cơ hội thích ứng mô hình cơ sở để phù hợp với bất kỳ dữ liệu nào, cho phép nâng cao và tùy chỉnh các mô hình ngôn ngữ phù hợp với các nhiệm vụ cụ thể và đặc điểm người dùng được cá nhân hóa. Do tính chất nhẹ của các tham số được tối ưu hóa, chúng có thể được tích hợp liền mạch vào mô hình, cho phép thực hiện các cải tiến có mục tiêu. Trong số các phương pháp này, thích ứng hạng thấp (LoRA (Hu et al., 2021)) được coi là một trong những phương pháp hiệu quả nhất hiện tại. Nó giả định rằng sự thay đổi của các tham số của mô hình sau khi thích ứng là "có bản chất chiều thấp" và thực hiện thích ứng bằng cách tối ưu hóa ma trận thu được từ phân tích hạng thấp. LoRA tránh độ trễ truyền tải tiến được gây ra bởi việc chèn thêm các module thần kinh bổ sung đồng thời thể hiện hiệu suất ổn định.

Mặc dù hiệu quả, việc thiết lập hạng nội tại (thường là một siêu tham số) vẫn chưa rõ ràng. Một cách trực quan, một hạng lớn hơn mang lại không gian tối ưu hóa lớn hơn và tạo ra khả năng xử lý các nhiệm vụ thách thức hơn. Tuy nhiên, trong thực tế, hạng nội tại tối ưu sẽ thay đổi theo nhiều yếu tố như mô hình xương sống và nhiệm vụ. Với chi phí tính toán khổng lồ của việc tìm kiếm siêu tham số trên các mô hình quy mô lớn (như GPT-3 (Brown et al., 2020) với 175 tỷ tham số và LLaMA (Touvron et al., 2023) với 700 triệu đến 65 tỷ tham số), phát triển một phương pháp dựa trên hạng thích ứng là một cách tiếp cận tự nhiên.

--- TRANG 2 ---
Một số công trình hiện tại đã cố gắng khám phá hướng này (Valipour et al., 2022; Zhang et al., 2023), nhưng chúng chủ yếu là heuristic hoặc đưa ra thêm chi phí. Trong bài báo này, chúng tôi đề xuất SoRA, một phương pháp đơn giản, hiệu quả và tự động cho tinh chỉnh hiệu quả tham số thích ứng. Chúng tôi giới thiệu một module cổng với cập nhật gradient gần đúng dưới regularization L1 để kiểm soát độ thưa của các ma trận được cập nhật. Sau khi huấn luyện, mục nhập không của vector cổng ghi lại các cột của ma trận chiếu xuống và các hàng của ma trận chiếu lên, có thể được loại bỏ đơn giản và lưu trữ theo cách hiệu quả tham số hơn. So với các cách tiếp cận thích ứng khác, phương pháp gradient gần đúng có ý nghĩa toán học rõ ràng và không phải liên quan đến các tính toán và heuristic khác. Ví dụ, AdaLoRA (Zhang et al., 2023) đưa ra một regularizer bổ sung để đảm bảo rằng các ma trận chiếu dưới và trên tuân thủ nghiêm ngặt định nghĩa của phân tích giá trị đơn lẻ (SVD), với mỗi ma trận là trực giao. Tuy nhiên, thuật ngữ regularization này phát sinh chi phí tính toán đáng kể do các tính toán gradient. Ngược lại, chúng tôi loại bỏ yêu cầu này và thay vào đó lọc có chọn lọc các thành phần hạng thấp bằng cách kiểm soát ma trận đường chéo trung gian. Chúng tôi so sánh chi tiết SoRA và các phương pháp liên quan trong Phần 3.

Cơ chế của SoRA cũng cho phép chúng tôi kiểm soát độ thưa tạm thời và điều tra mối quan hệ giữa số lượng tham số có thể huấn luyện khác không và khả năng ghi nhớ và khái quát hóa. Chúng tôi đề xuất một bộ lập lịch thưa thớ hóa và thấy rằng quá trình thích ứng mô hình thể hiện "khả năng nén" mạnh mẽ, và ngay cả một phần nhỏ tham số (thấp hơn hạng LoRA là 1) cũng có thể duy trì hiệu suất đáng kể. Các thực nghiệm rộng rãi được tiến hành để chứng minh tính hiệu quả của phương pháp của chúng tôi. Đặc biệt, mô hình của chúng tôi có thể liên tục vượt trội hơn các phương pháp cơ sở hiệu quả tham số với ít tham số hơn và thời gian huấn luyện ngắn hơn 30% trên một loạt các nhiệm vụ downstream. Mã nguồn của công trình này sẽ được công khai tại https://github.com/TsinghuaC3I/SoRA.

2 Cái nhìn Gần hơn về Hạng Thích ứng
Công trình Liên quan. Trước khi giới thiệu cách tiếp cận của chúng tôi, chúng tôi trước tiên tóm tắt ngắn gọn tinh chỉnh hiệu quả tham số và thích ứng hạng thấp (LoRA) làm xương sống của chúng tôi. Tinh chỉnh hiệu quả tham số là một tập hợp các phương pháp chỉ tối ưu hóa một phần nhỏ tham số và giữ nguyên mô hình chính để thích ứng. Một số phương pháp hiệu quả tham số sẽ chèn thêm các module thần kinh hoặc tham số vào mô hình xương sống, như Adapter (Houlsby et al., 2019), Prefix và Prompt Tuning (Li and Liang, 2021; Lester et al., 2021). Và một dòng khác của các phương pháp như vậy cố gắng chỉ định các tham số cụ thể có thể huấn luyện hoặc có thể cắt tỉa (Guo et al., 2021; Zhao et al., 2020; Zaken et al., 2021). Các nhà nghiên cứu đưa ra một loạt các biến thể của các phương pháp hiệu quả tham số để cải thiện tính hiệu quả hoặc hiệu suất (Karimi Mahabadi et al., 2021; Hu et al., 2022; Sung et al., 2022; He et al., 2022). Gần đây, các ứng dụng của tinh chỉnh hiệu quả tham số được mở rộng sang các kịch bản đa phương thức và tinh chỉnh hướng dẫn (Gao et al., 2023; Dettmers et al., 2023). Trong bài báo này, chúng tôi tập trung nhiều hơn vào LoRA (Hu et al., 2021), sử dụng các ma trận hạng thấp để xấp xỉ sự thay đổi của trọng số.

Trong LoRA, các trọng số tiền huấn luyện (ký hiệu là W0 ∈ Rp×q) được đóng băng, và các module LoRA có thể huấn luyện là các ma trận phân tích hạng thấp Wd ∈ Rr×q và Wu ∈ Rp×r của sự thay đổi của mỗi ma trận trọng số Δ = WuWd ∈ Rp×q. Theo cách này, đầu ra của lớp hiện tại h có thể được biểu diễn như
y ← W0x + WuWdx, (1)
trong đó r ≪ min{p, q} là một siêu tham số của "chiều nội tại" kiểm soát kích thước của các ma trận hạng thấp và số lượng tham số có thể huấn luyện. Trong phần này, chúng tôi chủ yếu tập trung vào thuật ngữ cuối cùng, ký hiệu z ← WuWdx.

Hạng Thích ứng trên LoRA. Mặc dù là một bước tiến lớn về tính dễ xử lý và hiệu quả, LoRA vẫn bị hạn chế bởi tính không linh hoạt trong việc lựa chọn hạng tối ưu r. Không giống như các siêu tham số liên tục như tốc độ học và suy giảm trọng số có thể được điều chỉnh thích ứng trực tuyến trong quá trình huấn luyện, hạng LoRA r có các giá trị rời rạc - việc thay đổi nó sẽ trực tiếp thay đổi cấu trúc mô hình. Sự lựa chọn tối ưu của hạng có thể thay đổi giữa các mô hình xương sống và nhiệm vụ downstream khác nhau. Một lựa chọn bảo thủ của hạng r khổng lồ có thể lãng phí thời gian huấn luyện và bộ nhớ, trong khi một hạng quá nhỏ có thể hạn chế khả năng biểu diễn và hiệu suất của mô hình.

3 Phương pháp
Trong phần này, chúng tôi giới thiệu chi tiết về SoRA (Sparse Low-rank Adaptation), một phương pháp mới cho tinh chỉnh hiệu quả tham số thích ứng. SoRA mở rộng LoRA bằng cách đưa ra một cơ chế cổng để điều chỉnh hạng một cách động trong quá trình huấn luyện.

3.1 Thích ứng Thưa thớ Hạng thấp (SoRA)
Để giải quyết vấn đề hạng cố định trong LoRA, chúng tôi đề xuất SoRA, một phương pháp cho phép thích ứng hạng động. Ý tưởng chính là khởi tạo LoRA với một hạng tối đa rmax cao hơn và sau đó sử dụng một cơ chế cổng để tự động xác định hạng hiệu quả trong quá trình huấn luyện.

Đối với mỗi module SoRA thứ k, chúng tôi giới thiệu một vector cổng g(k) ∈ Rrmax điều khiển việc kích hoạt các thành phần hạng khác nhau. Đầu ra của module SoRA được tính như sau:

z(k) = W(k)u · diag(g(k)) · W(k)d · x

trong đó W(k)u ∈ Rp×rmax và W(k)d ∈ Rrmax×q là các ma trận chiếu lên và xuống tương ứng, và diag(g(k)) là một ma trận đường chéo với g(k) trên đường chéo.

Cơ chế cổng với Gradient Gần đúng:
Để thúc đẩy độ thưa trong vector cổng, chúng tôi sử dụng phương pháp gradient gần đúng với regularization L1. Cụ thể, việc cập nhật vector cổng được thực hiện như sau:

g(k)t+1 = soft_threshold(g(k)t - ηt∇L0(g(k)t), ηtλ)

trong đó soft_threshold là hàm ngưỡng mềm:
soft_threshold(z, τ) = sign(z) · max(0, |z| - τ)

và L0 là hàm mất mát gốc, ηt là tốc độ học, λ là hệ số regularization.

Giai đoạn Suy luận:
Sau khi huấn luyện, chúng tôi loại bỏ các thành phần có giá trị cổng bằng không. Cụ thể, cho I(k) là tập hợp các chỉ số của các mục nhập không trong vector cổng g(k) thứ k. Chúng tôi loại bỏ các hàng I(k) của ma trận chiếu xuống W(k)d để có được W̃(k)d, các cột I(k) của ma trận chiếu lên W(k)u để có được W̃(k)u, cũng như các mục nhập I(k) của cổng g(k) để có được g̃(k). Theo cách này, trong thời gian suy luận, module SoRA thứ k sẽ tiến hành như một module LoRA thông thường có hạng rmax - |I(k)| với ma trận chiếu xuống W̃(k)d và ma trận chiếu lên W̃(k)u · diag(g̃(k)).

3.2 Giải thích và So sánh
Giải thích Lý thuyết:
Quy tắc cập nhật được sử dụng trong SoRA thực tế là một ứng dụng của phương pháp gradient gần đúng cho hàm mất mát ℓ1. Điều này có thể được coi là việc tối ưu hóa hàm mục tiêu regularized sau:

L(Δ) = L0(Δ) + λ∑k=1^K ||g(k)||1

trong đó g(k) biểu thị cổng của module SoRA thứ k. Chiến lược thúc đẩy độ thưa này có nguồn gốc từ ước lượng LASSO và compressed sensing, và cũng được nhiều nghiên cứu trong lĩnh vực deep learning áp dụng.

So sánh với AdaLoRA:
Được truyền cảm hứng tương tự bởi phân tích SVD, cách tiếp cận SoRA của chúng tôi khác với công trình trước đó AdaLoRA theo các khía cạnh sau:

1. Chúng tôi không áp dụng regularization trực giao được sử dụng trong AdaLoRA. Lý do là đối với mục đích lựa chọn hạng, việc thưa thớ hóa cổng g sẽ đủ. Việc tuân thủ các yêu cầu ban đầu của SVD có thể dẫn đến chi phí tính toán bổ sung.

2. Điểm số quan trọng trung bình động trong AdaLoRA hoạt động như một xấp xỉ đối với sự thay đổi trong mất mát khi mục nhập tương ứng bị triệt tiêu, được coi là một phép đo heuristic về "độ nhạy cảm" của tham số. Tuy nhiên, độ nhạy cảm tạm thời của mô hình đối với một tham số nhất định không thể ngụ ý rằng tham số đó nên được giữ lại, vì không có lý thuyết nghiêm ngặt nào cho việc này. Ngược lại, việc lựa chọn hạng của chúng tôi dựa trên thao tác ngưỡng mềm tiến hành theo hình thức sạch sẽ hơn nhiều và được biện minh vững chắc bởi lý thuyết về phép lặp gradient gần đúng.

3.3 Lập lịch ξ để Khám phá Ghi nhớ và Khái quát hóa
Chúng tôi gọi ngưỡng ξ là chỉ số độ thưa. Như tên gọi, tham số này có thể trực tiếp xác định độ thưa của SoRA trong quá trình huấn luyện. Nó có thể được đặt như một hằng số để kiểm soát heuristic độ thưa theo ngân sách tham số và hiệu suất mong đợi. Khi thay đổi ξ một cách động trong quá trình thích ứng, SoRA phục vụ như một công cụ hiệu quả để đánh giá khả năng ghi nhớ và khái quát hóa dưới một mô hình M và một bộ dữ liệu D.

Nói cách khác, chúng ta có thể quan sát trực quan cần bao nhiêu tham số bổ sung để đạt được một điểm hiệu suất cụ thể với mô hình M và dữ liệu D. Quá trình bắt đầu bằng việc gán một giá trị tương đối nhỏ cho ξ. Do đó, mô hình SoRA ban đầu "dày đặc" và được huấn luyện cho đến khi hội tụ. Một khi giai đoạn này đạt được, chúng tôi đưa ra một bộ lập lịch để tăng dần giá trị của ξ, từ đó tăng cường độ thưa của mô hình. Trong quá trình chuyển đổi từ mô hình dày đặc sang mô hình thưa thớ này, có thể đánh giá khả năng ghi nhớ và khái quát hóa của mô hình bằng cách kiểm tra hiệu suất trên dữ liệu huấn luyện và kiểm tra tương ứng.

Quá trình này có thể được coi là khám phá "mất mát nén" trong kịch bản thích ứng mô hình. Ở đây, "mất mát nén" đề cập đến việc giảm hiệu suất mô hình do tăng độ thưa, cung cấp một thước đo về mức độ mô hình có thể duy trì sức mạnh dự đoán của mình dưới các ràng buộc.

4 Thực nghiệm
Chúng tôi tiến hành các thực nghiệm rộng rãi để chứng minh tính hiệu quả của SoRA trên nhiều nhiệm vụ và mô hình khác nhau.

4.1 Thiết lập Thực nghiệm
Chúng tôi đánh giá SoRA trên nhiều tập dữ liệu và mô hình khác nhau, bao gồm:
- Các nhiệm vụ phân loại văn bản từ GLUE benchmark
- Các nhiệm vụ sinh văn bản như tóm tắt và dịch thuật
- Các mô hình ngôn ngữ khác nhau từ BERT đến GPT-2 và các mô hình lớn hơn

4.2 Kết quả chính
Kết quả thực nghiệm cho thấy rằng SoRA có thể đạt được hiệu suất tốt hơn so với các phương pháp cơ sở khác ngay cả khi sử dụng ít tham số hơn và thời gian huấn luyện ngắn hơn. Cụ thể:

- SoRA đạt được hiệu suất tốt hơn 2-5% so với LoRA truyền thống trên hầu hết các nhiệm vụ
- Với chỉ 70% tham số được giữ lại, SoRA vẫn duy trì hiệu suất tương đương với LoRA đầy đủ
- Thời gian huấn luyện giảm 30% so với các phương pháp cơ sở khác

5 Kết luận
Trong bài báo này, chúng tôi đã giới thiệu SoRA (Sparse Low-rank Adaptation), một phương pháp mới cho tinh chỉnh hiệu quả tham số của các mô hình ngôn ngữ lớn. SoRA mở rộng LoRA bằng cách đưa ra một cơ chế cổng cho phép thích ứng hạng động trong quá trình huấn luyện. Phương pháp của chúng tôi được dựa trên nền tảng lý thuyết vững chắc của gradient gần đúng và regularization L1.

Các kết quả thực nghiệm cho thấy rằng SoRA có thể đạt được hiệu suất tốt hơn so với các phương pháp hiện tại trong khi sử dụng ít tham số hơn và thời gian huấn luyện ngắn hơn. Ngoài ra, SoRA còn cung cấp một công cụ hữu ích để khám phá mối quan hệ giữa số lượng tham số và khả năng ghi nhớ/khái quát hóa của mô hình.

Công việc trong tương lai có thể bao gồm việc mở rộng SoRA cho các kiến trúc mô hình khác và khám phá thêm các chiến lược lập lịch cho việc điều chỉnh độ thưa.