# 2308.07282.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2308.07282.pdf
# File size: 356424 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2308.07282v2  [cs.CL]  8 Apr 2024Comparison between parameter-efficient techniques and fu ll
fine-tuning: A case study on multilingual news article
classification
Olesya Razuvayevskaya1/YinYang*, Ben Wu1/YinYang, Jo˜ ao A. Leite1/YinYang, Freddy Heppell1/YinYang, Ivan Srba2,
Carolina Scarton1, Kalina Bontcheva1, Xingyi Song1
1Department of Computer Science, The University of Sheffield, She ffield, UK
2Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia
/YinYangThese authors contributed equally to this work.
* o.razuvayevskaya@sheffield.ac.uk
Abstract
Adapters and Low-Rank Adaptation (LoRA) are parameter-effic ient fine-tuning
techniques designed to make the training of language models more ef ficient. Previous
results demonstrated that these methods can even improve perf ormance on some
classification tasks. This paper complements existing research by in vestigating how
these techniques influence classification performance and comput ation costs compared
to full fine-tuning. We focus specifically on multilingual text classifica tion tasks (genre,
framing, and persuasion techniques detection; with different inpu t lengths, number of
predicted classes and classification difficulty), some of which have lim ited training
data. In addition, we conduct in-depth analyses of their efficacy a cross different
training scenarios (training on the original multilingual data; on the t ranslations into
English; and on a subset of English-only data) and different languag es. Our findings
provide valuable insights into the applicability of parameter-efficient fine-tuning
techniques, particularly for multilabel classification and non-paralle l multilingual tasks
which are aimed at analysing input texts of varying length.
April 9, 2024 1/45

--- PAGE 2 ---
Introduction
The development of language models has led to a significant increase in the number of
trainable parameters needed to fine-tune such models, with state -of-the-art models
comprising of millions or even billions of parameters [1,2]. This poses a se rious
constraint on the process of fine-tuning such models, often relyin g on significant
computational resources. Many of the recent research effort s are therefore focused on
the development of more efficient training techniques [3–5]. Method s that can decrease
the computational costs make language models more accessible to r esearchers and
practitioners with limited computational resources, and reduces t he carbon footprint
of their training.
In this study, we investigate the effectiveness of parameter-ef ficient fine-tuning
techniques (PEFTs) in multilingual, monolingual and cross-lingual tex t classification
scenarios. We have included PEFT techniques that have been evalua ted on large
volumes of data ( >20B) in prior research [6] – namely, LoRA, adapter, BitFit, prefix
tuning and prompt tuning. Since the majority of these techniques w ere previously
evaluated on transformer-based models (BERT and RoBERTa) for text classification
tasks, we select the subset of those that can be applied to such mo dels – namely
LoRA, adapter and BitFit. We exclude prefix tuning [7] and prompt t uning [8] as
these techniques are applied to generative Large Language Models (LLMs) models for
text-to-text generation tasks and cannot be directly compared to the above-mentioned
three methods.
In more detail, adapter-based fine-tuning represents a family of e fficiency
techniques that work by freezing a pre-trained language model an d adding a small
number of trainable parameters in the layers of the language model [9–11]. This
significantly reduces the training time at the cost of a small or no per formance penalty.
Another method of reducing the number of trainable parameters is based on
performing Low-Rank Adaptation (LoRA) [12]. The main idea behind th e LoRA
approach is to freeze the weights of pre-trained language models a nd insert low-rank
decomposition matrices into the transformer layers. BIas-Term F Ine-Tuning (BitFit)
is a PEFT technique that uses only the bias term and the task-specif ic linear
classification layer during training, omitting most of the parameters in the
April 9, 2024 2/45

--- PAGE 3 ---
encoder-decode layers [3]. This results in a significant reduction in co mputational
costs as bias terms represent only a small fraction (up to 0.1%) of a ll the parameters
of the models.
Prior studies report that, in addition to reducing computational co sts,
adapter-based methods can outperform full fine-tuning (FFT) in zero-shot
cross-lingual settings [11,13,14]. However, the tasks addresse d in these studies focus
on parallel multilingual texts [13], meaning that inference is not perf ormed in a
realistic zero-shot manner [14], on short inputs [11] or in highly specif ic domains [14].
Given that adapter methods are additionally known to have limited cap abilities in
processing long text inputs due to reserving a part of the sequenc e length for
adaptation [12], there is a motivation for evaluating this method in non -parallel
cross-lingual and multilingual settings and on different input length s. LoRA, on the
other hand, does not reduce the input sequence length, and prior comparisons between
FFT and LoRA suggest that, in addition to being parameter-efficien t, LoRA can, for
certain models, outperform FFT [12]. However, there is currently a research gap in
terms of evaluating LoRA in similar multilingual training scenarios, which creates a
motivation for including this PEFT in our study. There is no prior eviden ce regarding
the cross-lingual capabilities of BitFit, however, this PEFT techniqu e was shown to
have comparable performance to adapters and to also be compara ble to FFT on the
GLUE benchmark. This motivated the inclusion of the BitFit method in t his research.
This study is therefore motivated by the lack of consistent evaluat ion of PEFT
techniques in multilingual and cross-lingual zero-shot classification tasks performed on
long non-parallel texts. This gap in current research is addressed through a systematic
comparative investigation of how adapter, LoRA and FitBit techniqu es perform on
multilingual multilabel classification tasks, both in terms of classificat ion performance
and computation costs. We first perform an ablation study on one c lassification task
to identify the PEFT techniques that demonstrate the best perfo rmance in
multilingual and cross-lingual scenarios. Those best performing PE FT methods are
then investigated further in three training scenarios (multilingual a nd cross-lingual) on
three diverse classification tasks.
In particular, we study the behaviour of these PEFTs on three mult ilingual
multilabel news article classification tasks introduced as separate s ub-tasks of the
April 9, 2024 3/45

--- PAGE 4 ---
recent SemEval 2023 Shared Task 3 [15]: news genre, framing and p ersuasion
technique detection. All three tasks are based on multilingual data and contain
‘unseen’ languages, i.e. languages not available in the training set, bu t present in the
test set. Further motivation for this research came from the suc cess of our three
original best performing approaches in each of these three differ ent sub-tasks [16,17].
Our best method for sub-task 1 was based on an ensemble of FFT an d adapter-based
models, as well as language-specific checkpoint selection. Our best sub-task 2
approach was based on mono- and multilingual ensembles, one of whic h combined
FFT and adapter methods with task adaptive pre-training [18]. Finally , our models
for sub-task 3 included language-specific classification threshold s election and the
incorporation of unlabelled data into the training corpus. Overall, we found that
adapters improved performance in certain monolingual scenarios in sub-task 1 and for
multilingual ones in sub-task 2.
The main contributions of this paper are the following:
•Provide an evaluation of PEFTs on diverse text classificati on tasks as
compared to fully fine-tuned models.
•Compare the computational costs for training FFT and PEFT mo dels
on the three sub-tasks as such an evaluation has not been perf ormed
before on these datasets and tasks. We show that PEFTs significantly
reduce the number of trainable parameters (between 140 and 280 times less
parameters) and achieve shorter training times (between 32% and 44%). Unlike
previous analyses between bottleneck adaptors, LoRA and FFT, o ur comparison
is novel is providing more fine-grained statistics, such as the peak V RAM usage
and the relative train time duration. The latter statistic is particular ly important
because the training process typically more time-consuming than inf erence.
•Carry out an in-depth comparison of the performance of PEFTs in
different training scenarios , investigating both joint multilingual training
and two types of single source language training scenarios. We evalu ate how
each method performs on seen languages and generalises to unsee n ones.
•Improve on the original highest SemEval 2023 results. For sub-task 3,
April 9, 2024 4/45

--- PAGE 5 ---
we achieved a better performance on eight out of the nine language s compared to
the top results in the official leaderboard. For sub-tasks 1 and 2 t he results
reported here are mostly comparable to our original SemEval 2023 submissions
despite the fact that in this study we use significantly less complex mo dels (the
original solutions utilised multiple sub-task tailored steps and complex
ensembles).
Related work
Parameter-efficient fine-tuning techniques
As already mentioned above, PEFTs are computationally efficient du e to limiting the
number of trainable parameters. All such techniques freeze the p re-trained model, but
differ in the location of the inserted trainable parameters. We focu s specifically on
Bottleneck adapters and on Low-Rank Adaptation (LoRA).
Bottleneck adapters [10] have a structure similar to autoencoders. The
transformer hidden state his first down-projected to some smaller dimensionality
dbottleneck with matrix Wdown, passes through non-linearity function f, and is then
up-projected to the original dimensionality with matrix Wup, and a residual
connection r. This is defined formally in Eq 1.
h←Wup·f(Wdown·h)+r (1)
The location of the adapter layer depends on the adapter type. Ho ulsby
adapters [10] place adapter layers after the multi-head attention and feed-forward
block, whereas Pfeiffer adapters [19] place the adapter layer on ly after the
feed-forward block. Although adding the extra layer reduces the number of trainable
parameters and thus increases the speed of fine-tuning, it also inc reases the number of
overall parameters permanently, slowing down inference.
LoRA[12] adds low-rank decomposition matrices to the query ( Q), key(K), value
(V) and pre-trained W0matrices of the self-attention sub-layers of the transformer.
Given a layer expressed as the matrix multiplication h←W0x(whereW0∈Rd×k),
during the fine-tuning process, the value of W0is modified by some ∆ W. LoRA
April 9, 2024 5/45

--- PAGE 6 ---
represents this delta as the low-rank decomposition ∆ W=BA(where
B∈Rd×r,A∈Rr×k) of rank r≪min(d,k). Here, W0is frozen, while BandAare
initialised randomly and updated during fine-tuning. The decompositio n is scaled by
hyperparameter αand rank r, thus giving the new expression in Equation 2.
h←W0x+α
rBAx (2)
Once the fine-tuning stage has been completed, the additional mat rices can be
removed by simplifying W0,AandBto a single matrix W′
0, thus giving the same
number of parameters as the original pre-trained model. This solve s the problem of
increased inference time. When performing hyperparameter sear ch, theα
hyperparameter can be fixed since it is proportional to the learning rate [20].
Transformer models consist of six weight matrices, W0,WK,WV,WQ, and two
matrices in multilayer perceptron (MLP) layer. In principle, it is possib le to adapt any
number of weight matrices, however, the authors claim that adapt ing only
self-attention matrices ( W0,WK,WV,WQ) produces results comparable to adapting
all layers [12].
Evaluating using LoRA adaptation in other parts of the model (all at tention layers,
all feed-forward layers, all layers, and attention and feed-forw ard output layers)
revealed that inserting LoRA adaptation in all layers results in the hig hest
performance, and that in this configuration the hyperparameter rhas no effect [20].
BitFit[3] uses only a small percentage of model parameters called bias terms .
This allows a significant reduction in the number of trainable paramete rs.
Each self-attention head mat layerlconsists of the key (K), query(Q) and value
(V) linear layers. Unlike in the FFT fine-tuning method, only the bias te rmsblare
considered during fine-tuning:
Qm,l(x) =Wm,l
qx+bm,l
q
Km,l(x) =Wm,l
kx+bm,l
k
Vm,l(x) =Wm,l
vx+bm,l
v(3)
Multiple attention heads are then combined using an attention mecha nism and fed
into the MLP with layer-norm (LN). The biases from the query, key a nd value layers,
April 9, 2024 6/45

--- PAGE 7 ---
attention and normalisation layers are the only fine-tuned paramet ers of the network.
TheW(l),·
·matrices and other MLP parameters are kept frozen. This reduce s the
number of trainable parameters to 0.08%–0.09%.
The authors additionally show how the number of trainable paramete rs can be
further reduced by focusing only on the bias terms from the query layer and the
second MLP layer.
Related work on the comparison of adapters, LoRA and BitFit
to FFT
Despite significantly reducing the number of trainable parameters, bottleneck adapters
were previously found to have minimal negative impact on the perfor mance of
fine-tuned models for simple sentence classification tasks. In part icular, the evaluation
on the GLUE benchmark dataset [21] (a collection of sentence and s entence-pair
classification tasks) showed the bottleneck adapter performanc e to be within 0.8% of
the performance of FFT, whilst only training 3.6% of parameters [10 ]. Additionally, in
the field of machine translation, Bapna and Firat [9] found that adap ters produce
equivalent or even better results compared to FFT.
Adapters received special attention in the context of multilingual t asks, with
various studies reporting a consistent advantage of adapter mod els in comparison to
FFT. In particular, Pfeiffer et al. [19] proposed a modular adapter -based architecture,
which combines task-specific adapters in the source language with ‘la nguage’ adapters
trained on unlabelled data in the target language using the masked lan guage
modelling (MLM) objective. The authors report that this framewor k is able to
outperform the traditional fully fine-tuned language models in cros s-lingual transfer
inference for the majority of language pairs. Different from Pfeif fer et al. [19], He et
al. [11] investigated the zero-shot cross-lingual capabilities of ada pters without
additionally training them on unlabelled target language data. The aut hors found that
adapters still outperform FFT on named entity recognition (NER), part of speech
tagging (POS) tagging and cross-lingual Natural Language Infer ence (XNLI) tasks.
They report that the former method is particularly beneficial in low- resource and
cross-lingual tasks, since it mitigates forgetting effects by minimis ing the differences
April 9, 2024 7/45

--- PAGE 8 ---
between the representations of the fine-tuned and the pretrain ed model.
Unlike previous studies that mainly focused on the analysis of short t exts,
Chalkidis et al. [13] investigated the performance of adapter models on long legal
documents. Consistently with previous works, the authors found that bottleneck
adapters outperform FFT, and provide better zero-shot cross -lingual capability. Their
findings are based on the MultiEURLEX dataset, which consists of 65 ,000 EU law
texts in 23 languages, categorised at multiple levels of detail (betwe en 21 and 567
categories). In some aspects, this dataset is comparable to the d ataset for sub-task 3
that we explore in this work, as both datasets are multilabel and mult ilingual, and
have a comparable number of labels at MultiEURLEX’s lowest level. Howe ver the
style of EU law is naturally much more rigid and very different to news a rticles.
Additionally, due to the specific data collection methodology used for MultiEURLEX,
this dataset is not likely to contain irrelevant text. Finally, the MultiEU RLEX dataset
contains parallel multilingual data, meaning that the model making a c ross-lingual
zero-shot prediction on the target language has already ‘seen’ th is text at training
time in another language.
The latter limitation of the MultiEURLEX dataset became the focus of the study
by Xenouleas et al. [14] who questioned whether the findings of Chalk idis et al. would
generalise to non-parallel datasets. When the dataset is modified t o include only
non-parallel documents, the authors found that translation-ba sed methods
(translate-test and translate-train) outperform multilingual mo dels. Consistently with
previous research, however, the authors observed that adapt ers still outperform the
FFT in each of the settings, cross-lingual and translation-based. We note that this is
likely dependent on domain and on whether the relevant properties a re significantly
affected by translation: legal documents are more likely to be prop erly represented in
the target language than, for example, the language-specific lingu istic properties
signalling certain persuasion techniques. Additionally, the joint multilin gual
experiments conducted by Xenouleas et al. [14] do not leave out any languages to
perform the zero-shot cross-lingual inference, which makes this approach not
comparable to the monolingual ones due to the difference in the size of the training set.
Taking into account the limitations of prior research in terms of the in put length,
high specificity of the domains, and parallelism of multilingual data, we a im to
April 9, 2024 8/45

--- PAGE 9 ---
perform a wider comparison of adapter models with FFT on a variety o f tasks and on
non-parallel input texts that are not limited to a specific domain and v ary in length.
Another important limitation of prior work is that all the studies ment ioned above
evaluate cross-lingual transfer capabilities in a one-to-many mann er, while the joint
multilingual training setting does not perform zero-shot cross-ling ual inference,
making it not comparable with the monolingual zero-shot cross-lingu al scenarios. To
fill this gap and bring understanding into how the joint multilingual tra ining data can
affect cross-lingual capabilities of the methods, we aim to introduc e a many-to-many
joint inference into our comparison scenarios while keeping certain la nguages as
’unseen’ for all the training scenarios.
To our knowledge, no comparison of LoRA and BitFit to the FFT metho d in a
similar multilingual scenario exists. However, given the prior evidence that LoRA and
BitFit can, for certain tasks, outperform FFT [3,12], there exist s a clear motivation of
performing a pioneering evaluation of these PEFT techniques in multilin gual scenarios.
LoRA technique can be particularly promising for our objective due t o its ability to
perform better on longer texts than adapter methods due to the fact that LoRA does
not reduce the input sequence length [12].
Dataset and Task description
To select the tasks and datasets suitable for our objective, we an alysed the corpora
from the most recent survey on multilingual datasets [22] that kee ps real-time track of
the multilingual data. We selected the datasets for the similar tasks of classification
and sentiment analysis, and filtered out the datasets that contain multilingual data for
short inputs - lemmas, word-pairs, sentences, tweets and short statements. We then
manually analysed each dataset based on how the multilingual data wa s collected, and
filtered out parallel multilingual datasets. Finally, we filtered out the datasets that
focus on specific narrow domains. This method narrowed down the s cope of our
interest to the dataset that was created recently as part of SemEval-2023 Task 3:
“Detecting the genre, the framing, and the persuasion techn iques in online news in a
multi-lingual setup” [15].
Prior to SemEval 2023, a number of other related challenging multiling ual
April 9, 2024 9/45

--- PAGE 10 ---
misinformation and propaganda detection tasks were addressed in SemEval
(https://semeval.github.io ) shared tasks, including detection of hyperpartisan
content [23], sarcasm [24], and a smaller set of persuasion technique s in textual [25]
and multimodal [26] data. The Shared Task 3 within SemEval 2023 cha llenge
extended this prior work on persuasion techniques by introducing n ew kinds of
persuasion techniques, as well as addressing two other related su b-tasks, namely news
genre categorisation and framing detection.
Sub-task 1: News Genre Categorisation. Given a news article, determine
whether it is objective news reporting, an opinion piece, or satire.
Sub-task 2: Framing Detection. Given a news article, identify one or more of
fourteen framing dimensions used: Economic ,Capacity and resources ,Morality,
Fairness and equality ,Legality, constitutionality and jurisprudence ,Policy prescription
and evaluation ,Crime and punishment ,Security and defense ,Health and safety ,
Quality of life ,Cultural identity ,Public opinion ,Political,External regulation and
reputation . The set of framing techniques used in this shared task was defined
following a pre-existing taxonomy [27].
Sub-task 3: Persuasion Techniques Detection. Given a paragraph of a news
article, identify zero or more out of 23 persuasion techniques used (see S1 Appendix
for a detailed list of the techniques). The set of techniques repres ents an extension of
the taxonomy used in previous SemEval datasets [26,28]. The task additionally
provides 6 high-level categories that subsume similar persuasion te chniques. Although
the task is paragraph level, each of the articles has at least one labe lled paragraph.
It should be noted that three of the systems that participated in t he original
SemEval-2023 Task 3 [15] evaluation exercise used adapters. Team s HHU [29] and
NAP [30] entered only sub-task 3, in which they used adapters, whe reas
SheffieldVeraAI [16] applied adapters to sub-tasks 1 and 2. Initial performance
analysis in these sub-tasks showed the effect of adapters to be in consistent across the
different sub-tasks. Namely, adapters achieved higher average performance for
monolingual models in sub-task 1, while hindering performance of mon olingual models
in sub-task 2 but achieving better results there for multilingual mod els. This evidence
April 9, 2024 10/45

--- PAGE 11 ---
provides an even stronger motivation gaining better understandin g of the effectiveness
of adapter methods across a range of classification tasks ranging in difficulty.
These three sub-tasks use broadly overlapping data, i.e. same inpu t articles,
however, differ in properties and the summary statistics for the d atasets, which are
summarised in Table 1.
Table 1. Properties of genre, framing and persuasion techni que detection tasks
Comparison criteria Sub-task 1 Sub-task 2 Sub-task 3
Task type Genre Detection Framing Detection Persuasion Techniques
Input Type Whole document Whole document Paragraphs
Granularity Multiclass Multilabel Multilabel
Official scoring metric F1macro F1micro F1micro
Number of classes 3 14 23
Avg number of tokens 1,157 1,157 74
Train set size 1,234 1,238 10,927
Number of source languages 6 6 6
Number of target languages 9 9 9
Task subjectivity High Low Medium
Data imbalance 12.7 4.6 44
We estimate the data imbalance as a ratio of the class samples for the most
frequent class in the training set to that for the most rare one. Be low, we discuss some
of the individual properties in more detail.
Class Imbalance: In addition to posing multilingual and multiclass classification
challenges, the data for these three sub-tasks is highly imbalanced , which adds further
difficulty. In particular, the class distribution for sub-task 1 is high ly skewed, with
76% falling into the opinionclass and satireaccounting for only less than 6% of the
data. For sub-task 2, the distribution of classes is also uneven, bu t is less skewed
compared to sub-task 1. The most common frame is Political which appears in 49.4%
of the training articles. The least common frame is Cultural Identity which appears in
just 10.8% of the articles. Finally, in sub-task 3 loaded language ,doubtandname
callingare the most common persuasion techniques, accounting for 22%, 15.6% and
April 9, 2024 11/45

--- PAGE 12 ---
12.8% of the training paragraphs, respectively. The remaining 20 cla sses, on average,
account for 2.5% of the training set, totalling 49.6% together. Part icularly, appeal to
time,whataboutism andred herring are the least frequent persuasion techniques,
representing 0.5%, 0.5% and 0.7% of the training paragraphs, respe ctively.
Dataset statistics: multilinguality, size and input lengt h:Three sets of data
are provided for each language and task: labelled training anddevelopment (except for
unseen languages), and unlabelled testing.
The task organisers provided test data in nine languages: English (E N), French
(FR), German (DE), Georgian (KA), Greek (EL), Italian (IT), Polis h (PL), Russian
(RU), and Spanish (ES). Three of the languages (Georgian, Greek and Spanish) are
‘surprise’ languages, meaning that no corresponding labelled trainin g data exists in the
dataset. Therefore, in order to make predictions for these langu ages, their test set
must either be translated to a ‘seen’ language, or a multi-lingual app roach capable of
supporting zero-shot evaluation must be applied. For the remaining 6 languages,
labelled training and validation data is included in the dataset.
It must be noted that the task organisers have not yet released t he gold labels for
the test set in order to prevent researchers from overfitting th eir systems. This means
that detailed error analysis can only be carried out on the six languag es for which the
development sets are available.
Tables 2, 3 and 4 show the detailed analysis of the training, developme nt and test
data used in sub-tasks 1, 2 and 3 respectively. The average length , calculated in the
number of tokens, was estimated using the tokenizer for RoBERTa -large model [31],
since this is the model we use in our experiments. For the training and development
sets in sub-task 3, the average length was calculated for the para graphs that have at
least one persuasion technique assigned. For the test sets in sub- task 3, the average
length includes every paragraph due to the lack of gold-standard la bels for the test
data. This is why the number of examples in the test-sets for sub-t ask 3 is
significantly higher than that for the training and development sets . However, not all
of these examples are expected to contain at least one persuasion technique.
April 9, 2024 12/45

--- PAGE 13 ---
Table 2. Data statistics per language for sub-task 1: Genre D etection.
LanguageNumber of examples Average number of tokens
Training Development TestTraining Development Test
EN 433 83 541,307 1,066 978
FR 158 54 501,241 1,025 927
DE 132 45 50 995 913 1,203
IT 226 77 61 975 856 958
PL 144 50 471,354 1,438 1,935
RU 142 49 721020 797 547
ES 0 0 30N/A N/A 838
EL 0 0 64N/A N/A 1,071
KA 0 0 29N/A N/A 429
Table 3. Data statistics per language for sub-task 2: Framin g Detection.
LanguageNumber of examples Average number of tokens
Training Development TestTraining Development Test
EN 433 83 541,307 1,066 978
FR 158 53 501,196 1,059 927
DE 132 45 501,008 875 1,203
IT 227 76 61 965 885 958
PL 145 49 471,369 1,397 1,935
RU 143 48 721,009 827 547
ES 0 0 30N/A N/A 838
EL 0 0 64N/A N/A 1,071
KA 0 0 29N/A N/A 429
April 9, 2024 13/45

--- PAGE 14 ---
Table 4. Data statistics per language for sub-task 3: Persua sion Techniques.
LanguageNumber of examples Average number of tokens
Training Development Test Training Development Test
EN 3,610 1,103 11,466 88 37 65
FR 1,693 437 7,140 97 108 91
DE 1,251 405 11,060 95 87 76
IT 1,742 594 8,302 99 91 99
PL 1,228 415 14,084 109 122 90
RU 1,232 310 8,414 85 80 66
ES 0 0 1,320 N/A N/A 76
EL 0 0 3,792 N/A N/A 72
KA 0 0 640 N/A N/A 78
Sub-tasks 1 and 2 use the same set of articles in the test set, while t he
accumulative set of articles used in the development and training set s is also identical
for these two sub-tasks, their assignment to a certain set varies slightly. This is why,
as we can see from Tables 2 and 3, the data statistics for these sub -tasks are quite
similar. As can be seen, the distribution of the training examples acro ss the languages
is not even, with EN accounting for almost 4 times as many articles as D E, RU and
PL. We can also observe that the average length of the articles is hig hly dependent on
the language, with articles in the test set for Georgian (KA) being mo re than 4.5
times shorter than that for the articles in the test set for Polish (P L). This aspect is
important for our experiments since it suggests that the models ar e more likely to
omit important information for certain languages compared to othe rs, due the the
limitation of transformer models in terms of the input length. Anothe r observation is
that the training set is not always representative of the test set. For example, the
articles in the test set for Russian (RU) are twice shorter, on aver age, than the articles
in the training set for this language. The difference in terms of the in put length
between training and test data is less significant for sub-task 3, wh ich uses paragraphs
as an input, and for which all the inputs are within the limit of transfor mer models.
We analysed the languages in the training and test sets in terms of th e amount of
resources. For each language, we considered the number of train ing examples in that
April 9, 2024 14/45

--- PAGE 15 ---
language or in a language from the same language family and the amoun t of
pre-training data in that language for the model that we use in our e xperiments. Our
analysis places Georgian as a clear outlier and a low-resource languag e, having
significantly less pre-training data than the other 8 languages and n o training data in
Georgian or a related language. English, on the other hand, is the hig hest-resource
language, benefiting from most training and pre-training data and a dditional training
data in a related language. The details of this analysis are provided in S 4 Appendix.
Task subjectivity: As specified in the annotation instructions [32], the subjectivity
differs across the sub-tasks. Sub-task 1 relies on a very nuance d analysis of the whole
article and on commonsense knowledge, since, as mentioned by the o rganisers, the
satirical articles often “tend to mimic true articles” and to mention “ real-world
individuals, organisations, and events”, while the distinction betwee n opinionated and
objective reporting can lie in the certain ways the reporters tend t o balance out the
reported opinions. The authors also highlight that “the borders be tween opinion and
reporting might be sometimes blurred” and “a news article which cont ains some small
text fragment, e.g., a sentence that appears satirical” does not n ormally trigger a
satire genre. The task of framing detection tends to be more relian t on a certain
linguistic information as the annotation instructions ask the annota tors to specify
exact text spans corresponding to a certain frame. The authors additionally provide
examples of the discussion topics that trigger certain frames. For example, “costs,
benefits, or other financial implications” usually are indicative of the “economic”
frame. Finally, sub-task 3 is the most fine-grained task as it provide s span-level
annotations of the propaganda techniques. The detection of per suasion techniques
usually relies on a certain argumentative structure and linguistic trig gers, such as the
mention of an entity that is considered an authority (“appeal to au thority” technique),
associating an opponent with a group, event or concept that has n egative connotations
(“guilt by association” technique), “the noun phrase, the adject ive that constitutes the
label and/or the name” (“name calling or labeling” technique) or “tex t fragments that
repeat the same message or information that was introduced earlie r” (“repetition”
technique). More objective assessment of the subjectivity is not possible due to the
lack of the inter-annotator agreement scores [15].
April 9, 2024 15/45

--- PAGE 16 ---
Dataset collection: The data is extracted from both mainstream and alternative
media sources, collected through news aggregators (e.g. Google N ews, Europe Media
Monitor) and fact-checking organisations (e.g. MediaBiasFactChe ck, NewsGuard),
respectively. All of the news articles were published between 2020 t o mid 2022. The
text of each article was extracted automatically from the HTML sou rce of each web
page by using either the text-gathering tool Trafilatura [33] or a site-specific
procedure. Notably, this process is error prone as it sometimes inc ludes textual
content which is not a part of the news article itself, such as web polls , newsletter
sign-up forms, and author information. For English, a pre-existing dataset was also
utilised [28], but the organisers of the shared task did not make it suf ficiently clear as
to what other English data was included in the new dataset.
Methods
Training scenarios
While our primary focus is on the multilingual fine-tuning scenario, we in troduce two
additional settings where models are trained on English-only data in o rder to
investigate whether the effectiveness of each training method dif fers depending on the
composition and the size of the training set.
These three different training scenarios are summarised below:
•Multilingual Joint (many-to-many) : models are fine-tuned using all
training data in the original 6 languages.
•English + Translations (one-to-many) : models are fine-tuned on all the
original English training data and English translations of the training d ata in
the other 5 languages. It is important to mention that the test dat a in this
scenario was kept in its original languages, meaning that the predict ions on all
the languages except for English were made in a zero-shot cross-lin gual way.
•English Only (one-to-many) : models are fine-tuned on only the original
English data in the training set. Similarly to the ‘English + Translations’
scenario, the test set was not translated into English.
April 9, 2024 16/45

--- PAGE 17 ---
The choice of the three training scenarios above is motivated by the fact that we
want to evaluate the effect of two different factors on each of t he training techniques:
1. The ‘English + Translations’ training scenario enables the evaluatio n of the
effect of multilinguality. In particular, we want to compare the effe ct of having
multilingual training data against the scenario where training data is a vailable in
only one language. By translating other languages into English, we co mpose a
dataset consisting of the same number of training examples but with out having
the language diversity. This eliminates the possibility that difference s in
performance across the three methods could be due to the size of the training
data for each language. At the same time, machine translation, as a specific
transfer paradigm for cross-lingual learning [34], may introduce so me level of
noise and thus break the required correspondence between the o riginal and
translated sample.
2. The ‘English Only’ training scenario enables the analysis of the effe ct of training
data size on each method. This can be achieved by comparing perfor mance on
‘English Only’ training data against performance on ‘English + Translat ions’
data, where the only difference between the two is in the number of training
examples. This training scenario, however, is not directly comparab le against the
multilingual training scenario, since it does not eliminate the possibility t hat
differences in the method’s performance could be due to the differ ent linguistic
characteristics of the multilingual training data.
Training techniques
In selecting the PEFT methods for our further analysis, we perfor med a detailed
ablation study on sub-task 1, where we compare the adapter, LoR A and BitFit
methods in multilingual and cross-lingual classification scenarios (se e S2 Appendix).
As can be seen, the BitFit method does not reach a performance co mparable to that
of LoRA and adaptor methods for any of languages in any of the clas sification
scenarios. We therefore exclude this PEFT technique from our fur ther experiments.
We experiment with XLM-RoBERTa Large [35], using the following train ing
techniques:
April 9, 2024 17/45

--- PAGE 18 ---
•Full fine-tuning (FFT) : All parameters of the model are updated during
fine-tuning.
•Low-Rank Adaptation (LoRA) : The model’s parameters are frozen and
LoRA matrices (key, query, value) are added to both the MLP and a ttention
layers.
•Bottleneck Adapter (Adapter) : The model’s parameters are frozen and
bottleneck adapters in the Pfeiffer configuration [19] are added to all the layers.
As we discuss further, the choice of the adapter configuration wa s suggested by
our ablation experiments that demonstrated the Pfeiffer configu ration to
outperform the Houlsby version on average (S3 Appendix).
The next section describes the methodology behind each training te chnique and
training scenario.
Experimental setup
Model and hyperparameter selection: When selecting the model size, we took
into account prior comparison of the LoRA and adapter-based PEF T methods for
different sizes of RoBERTa model across a wide range of tasks and found a clear
preference for a larger model size [12,19]. For both FFT and PEFT m ethods, we
additionally conducted the comparison of XLM-RoBERTa Base with XL M-RoBERTa
Large on our sub-tasks in the default training scenario which include s all the data for
each sub-task in the original form (the ‘Multilingual Joint’ scenario) . The results of
this comparison are presented in S3 Appendix. As can be observed, a large model
demonstrates consistently better average performance acros s all three sub-tasks.
Therefore, based on both prior evidence for PEFT techniques and our ablation studies
for FFT, we resolved to using XLM-RoBERTa Large in our main experim ental setup.
Choosing between the Pfeiffer [36] and Houlsby [10] configuration of the adapter
method, we performed the comparison of both adapters for the ‘M ultilingual Joint’
training scenario. The results of this comparison are presented in S 5 Appendix. As can
be seen, Pfeiffer adapter shows slight average advantage over H oulsby one in all three
sub-tasks. We therefore use the Pfeiffer configuration in the re st of our experiments.
April 9, 2024 18/45

--- PAGE 19 ---
For the best hyperparameters, we first perform a search for ea ch training scenario
and training technique within each sub-task. The search is perform ed on the original
development set as provided by the organisers of SemEval 2023 Ta sk 3. The best
configuration obtained for each method can be found in Table 5. One needs to bear in
mind that our objective is to maximise model performance per trainin g scenario for
each sub-task rather than to minimise the computational costs. I n other words,
greater parameter efficiency could be possible, but at the cost of model performance.
Table 5. Hyperparameters.
Full fine-tuning (FFT)
Hyperparameter Sub-task 1 Sub-task 2 Sub-task 3
Learning Rate 1.00E-05 3.00E-05 3.40E-04
Batch size 16 8 32
Low-Rank Adaptation (LoRA)
Hyperparameter Sub-task 1 Sub-task 2 Sub-task 3
Learning rate 7.00E-06 3.00E-4 1.59E-03
Batch size 16 8 32
Rank 8 8 2
Layers all all all
Dropout 5.00E-01 5.00E-01 5.00E-01
Attention matrix k,q,v k,q,v k,q,v
Bottleneck Adapter
Hyperparameter Sub-task 1 Sub-task 2 Sub-task 3
Learning rate 3.16E-05 2.00E-4 4.30E-04
Batch size 16 8 32
Reduction factor 4 8 8
For the bottleneck adapter, we use the default Pfeiffer configur ation [36] by adding
the adapter after each ‘ffn’ sub-layer. Despite prior ablation stu dies showing that the
lowest 4 layers have little contribution to the performance [20], our objective is to
maximise our performance and to make the setting comparable with L oRA where we
use all the layers.
April 9, 2024 19/45

--- PAGE 20 ---
Text preprocessing: As shown in Table 1, sub-tasks 1 and 2 have an average
number of tokens per article of 1,157. Thus in those sub-tasks, 80 .0% of articles are
truncated to a maximum of 512 tokens. In contrast, sub-task 3 p resents an average
number of tokens per sequence of 74, thus all training sentences are fully encoded
without loss of information. For sub-task 1, the articles that are lo nger than 512
tokens are separated into sentences, which are then sampled seq uentially from the
beginning and the end of the article, preserving the original order, until the maximum
of 512 tokens is reached. Such a truncation approach is motivated by our experiments
on sub-task 1 data during the competition stage of SemEval 2023 T ask 3 [16]. This
approach yielded a significant improvement in the F1 macroscore over the setting that
simply truncates texts to the first 512 tokens. This improvement c an potentially be
explained by the fact that the instructions for human annotators in sub-task 1
highlighted the importance of opinionated sentences which tend to b e found towards
the end of the articles.
We perform text preprocessing for sub-tasks 1 and 2 by applying t he following
steps for all languages:
•a full stop is added at the end of each title;
•duplicate sentences directly following each other are removed;
•the @ symbol is removed from any Twitter handles;
•hyperlinks to websites and images are also removed.
English articles were further preprocessed as follows:
•text promoting sharing on different social media platforms was rem oved from the
bottom of the articles;
•sentences encouraging user participation in online polls, comments, or
advertisements were also removed;
•sentences stipulating the site’s terms of use were removed;
•removal of sentences indicating licensing and containing phrases su ch as
‘reprinted with permission’, ‘posted with permission’ and ‘all rights res erved’;
April 9, 2024 20/45

--- PAGE 21 ---
•sentences detailing author biographies were also removed.
For sub-task 3, preliminary experiments found no performance ga ins when text
preprocessing was applied, thus our experiments for this sub-tas k use directly the
original text. Importantly, for sub-task 3 experiments, we includ e sentences that do
not have assigned labels into the training data by assigning them a vec tor of zeros to
indicate that they do not belong to any class. This approach was sho wn to
significantly improve classification performance on this sub-task in o ur initial
experiments [16,17]. The size of the training set displayed in Table 1 (1 0,927
examples) is based on the number of labelled examples. When unlabelled sentences
are added, training data for sub-task 3 grows to 20,704 instances .
The multilabel sub-tasks 2 and 3 use confidence thresholds of 50% a nd 30%,
respectively, after applying a sigmoid activation function to the logit s. The confidence
threshold for sub-task 3 is purposefully lower and was selected acc ording to our
previous experiments [17], which revealed that its careful calibratio n can significantly
influence the performance of the model.
Training scenarios: We experiment with the three training scenarios described
previously. All models are trained on the original training split provide d by the task
organisers, using either data in all 6 seen languages; all EN data and the translations
into English of the data in the other 5 languages; or only using the EN p art of the
training data. Each model is then evaluated on the task organiser’s test split (6 seen
and 3 surprise languages), without translation.
Aligned with previous research [11,14,37], we define the ‘unseen’ lan guage is a
‘target’ language on which a cross-lingual zero-shot prediction is d one and which is
different from the ‘source’ language on which the task-specific fin e-tuning of the
transformer model was performed. The three unseen test set la nguages - Greek,
Georgian, and Spanish - allow us to evaluate the zero-shot cross-lin gual transfer
learning capabilities of the training techniques trained in the first, fu lly multilingual,
setting. In the other two training scenarios (‘English + Translations ’ and ‘English
Only’), the remaining 8 languages (FR, DE, IT, PL, RU, ES, EL and KA) provide an
insight into the models’ performance in the cross-lingual zero-sho t setting.
April 9, 2024 21/45

--- PAGE 22 ---
Evaluation metrics: The performance of the different training techniques is then
compared using two sets of criteria: (1) computational resource efficiency; (2)
classification performance. For the latter, both F1 microand F1 macroare reported as
performance metrics for all three sub-tasks. However, it must b e noted that the
SemEval 2023 Task 3 organisers used only F1 macroas the official scoring metric for
sub-task 1, whereas sub-tasks 2 and 3 used only F1 micro. Therefore, where a more
detailed language-specific analysis is carried out in this paper, only th e respective
official metric for each sub-task is provided.
Mean and standard deviation are computed over three different r andom seed
initialisations.
Resource efficiency is measured through four metrics: (i) the pea k amount of
VRAM used during training; (ii) speedup relative to the fully fine-tune d method,
which is the number of training steps per second Nm/tmof the respective method
(LoRA or Adapter) divided by the number of training steps per seco nd of the fully
fine-tuned method NFFT/tFFT(Equation 4); (iii) the number of trainable parameters;
and (iv) the number of non-trainable parameters.
S∗=Nm
tm
NFFT
tFFT(4)
Implementation details: All experiments were performed with the AdapterHub
framework [36].
In order to obtain the ‘English + Translations’ data, we translate all available
training and development data into English using Google Cloud Translat ion API. The
choice of the Machine Translation (MT) system was motivated by the recent extensive
report on the evaluation of MT systems [38]. The results across all 11 language pairs
and 9 domains analysed by the authors show that Google Translate is the
state-of-the-art system based on the COMET score. We believe t hat this choice helps
to reduce the potential noise caused by the translation, however , it is difficult to
quantify the effect of the noise for the general case of news art icles, since, as the report
shows, the performance of the systems varies highly depending on the domain.
Moreover, while we have the performance estimates for EN-DE, EN -FR, EN-ES and
EN-IT pairs (which are comparable across all domains), we could not find the relevant
April 9, 2024 22/45

--- PAGE 23 ---
results for the remaining 4 language pairs (EN-PL, EN-KA, EN-RU, E N-EL).
Our code is available on GitHub
(https://github.com/GateNLP/PEFT_FFT_multilingual ) and the Zenodo repository
(https://doi.org/10.5281/zenodo.10066649 ).
Results
The analysis of our results is structured around the following three main research
questions:
RQ1:How does the classification performance and computational costs of each
training technique differ for each sub-task?
RQ2:How do the training scenarios (determining the diversity of the langu ages in
the training set and its size) affect the performance of each train ing technique?
RQ3:How do the training techniques compare with each other for each tr aining
scenario and language?
The importance of the research question posed in this study is motiv ated by the
limitations of prior research on the performance of PEFT technique s in the
multilingual article-level classification tasks.
RQ1 provides a high-level analysis of each training technique under t he best
training scenario. This comparison is important because it provides t he first to our
knowledge examination of the PEFT and FFT techniques on non-para llel texts that
are not restricted to a narrow domain and range in length from para graphs to long
articles. This RQ is particularly novel for the LoRA method since, as w e highlighted
above, no comparison of LoRA with adapter methods or FFT was pre viously
performed for the multilingual multilabel classification task. We addit ionally provide
the comparison of the computational costs as such a comparison w as not previously
performed for any of the sub-tasks in this study. It is not possible to assume that
previous results would hold for our tasks given that the computatio nal efficiency of
PEFT methods can be sensitive to the input length [12]. Unlike previous analyses
between bottleneck adaptors, LoRA and FFT that either perform a task-agnostic
inference latency analysis or provide the number of trainable param eters for specific
April 9, 2024 23/45

--- PAGE 24 ---
tasks [12], our comparison focuses on more fine-grained statistics , such as the peak
VRAM usage and the relative train time duration. The latter statistic is particularly
important because the training process is typically more time-consu ming than
inference, and the relation between the number of trainable param eters and the
training time is not linearly proportional.
RQ2 provides better insights into how the performance of each tra ining technique
changes in different training scenarios. As mentioned in the ‘Related work’ section,
previous experiments [14] conducted on non-parallel multilingual te sts conclude that
the zero-shot translation-based (translate-train and translat e-test) approaches
outperform cross-lingual ones for both FFT and adapter training techniques. However,
their cross-lingual zero-shot approaches are limited to one-to-m any scenarios, making
it difficult to fairly compare the translation-based training settings to the joint
multilingual scenarios due to the differences in the sizes of the train ing sets. We aim
to address this gap by sequentially changing one aspect of the train ing process, by first
eliminating the multilinguality of the training set whilst keeping the same s ize, and
then reducing the size and eliminating potentially noisy translated tex ts from the
training set. Additionally, previous joint multilingual experiments [14] do not leave
any languages out to perform the zero-shot cross-lingual infere nce, which also makes it
impossible to compare this approach to the monolingual ones. Finally, we add LoRA
to our set of PEFT methods and provide novel insights on the perfo rmance of this
technique under different scenarios in multilingual tasks.
RQ3 focuses on a different dimension of the problem and tries to ans wer which
method is preferable depending on the amount and type of the train ing data. This
analysis is motivated by the prior work by He et al. [11] on short texts that concludes
that adapter methods are particularly beneficial for low-resourc e and cross-lingual
tasks. We therefore investigate how PEFT methods and FFT compa re to each other
as we reduce the number of training resources in certain languages and reduce the
overall amount of training data.
April 9, 2024 24/45

--- PAGE 25 ---
Comparison of the computational and performance propertie s
of training techniques
To answer the first research question (RQ1), we select the best t raining scenario for
each method and sub-task and compare the performance of the F FT model against
the performance of the LoRA and adapter methods for each sub- task. The results of
this comparison are reported in Table 6. We report mean scores aft er three runs with
different random seeds along with standard deviations. The stand ard deviation is the
square root of the average of the squared deviations from the me an.
April 9, 2024 25/45

--- PAGE 26 ---
Table 6. Performance and computational costs for each sub-t ask and
training technique.
Sub-task 1: Genre Detection
FFT LoRA Adapter
F1macro* 59.9±3.1 57.9±6.3 58.0±2.0
F1micro 61.7±7.5 60.2±3.9 62.8±5.7
Best training scenario Multilingual Joint Multilingual Joint Multilingual Joint
Peak VRAM usage ∼39GB ∼24GB ∼28GB
Train Time relative to FFT 1 0.59 0.67
Trainable parameters ∼560M ∼3.2M ∼26M
Non-trainable parameters 0 ∼560M ∼560M
Sub-task 2: Framing Detection
FFT LoRA Adapter
F1macro 49.2±7.4 45.3±7.1 47.1±7.9
F1micro* 56.7±6.1 53.4±6.0 54.8±7.1
Best training scenario Multilingual Joint Multilingual Joint Multilingual Joint
Peak VRAM usage ∼23GB ∼18GB ∼14GB
Train Time relative to FFT 1 0.68 0.56
Trainable parameters ∼560M ∼4M ∼7M
Non-trainable parameters 0 ∼560M ∼560M
Sub-task 3: Persuasion Techniques
FFT LoRA Adapter
F1macro 23.7±5.0 23.7±6.9 20.6±6.3
F1micro* 41.8±8.6 42.9±9.5 42.2±9.5
Best training scenario Multilingual Joint Multilingual Joint Multilingual Joint
Peak VRAM usage ∼20GB ∼13GB ∼16GB
Train Time relative to FFT 1 0.56 0.71
Trainable parameters ∼560M ∼2M ∼7M
Non-trainable parameters 0 ∼560M ∼560M
The best scores and performance metrics appear in bold. The main metric for a
certain sub-task is marked with an asterisk ( ∗).
April 9, 2024 26/45

--- PAGE 27 ---
The results demonstrate that:
(1)FFT and adapters perform better in sub-tasks 1 and 2, while Lo RA
performs better for sub-task 3 . We observe that for longer texts, such as the
articles analysed in sub-tasks 1 and 2, FFT and adapter-based clas sification
demonstrates better results on average than LoRA. At the same time, LoRA on
average outperforms FFT and adapters for sub-task 3, which is t rained on shorter
texts.
(2)‘Multilingual Joint’ training scenario performs best, reg ardless of the
sub-task and training technique . We observe a pattern of ‘Multilingual Joint‘
training scenario achieving the best results for all three sub-task s as well as all three
training techniques. This implies that, in general, training models on lar ger datasets
with a variety of languages, can be beneficial for both FFT and PEFT methods applied
to the tasks with various properties. This effect has not been, ho wever, consistently
observed for all combinations of training scenarios and training tec hniques (the more
detailed analysis per individual training scenarios is provided in the follo wing section).
(3)LoRA and adapters can save computational costs significant ly. By
design, the PEFTs reduce the number of trainable parameters sign ificantly (between
140 and 280 times less parameters). As a result, for sub-task 1 an d 3, the utilisation of
LoRA led to a significant decrease in the memory consumption — from 3 9GB to
24GB (38%), and from 20GB to 13GB (35%) respectively. For sub-t ask 2, the adapter
achieved the best memory efficiency, while decreasing the peak VRA M usage from
23GB to 14GB (39%). The similar pattern can be observed for a tota l training time,
which decreased to 56-71% of the FFT training time.
(4)Saving computational costs results in lower performance, s ome
exceptions, however, exist . Saving the VRAM usage and shortening training time
is naturally reflected in the lower performance compared to that of FFT. Adapters are
consistently outperformed by FFT in all three sub-tasks. Howeve r, in the case of
sub-task 3, LoRA not only achieved highly comparable results, but a lso outperformed
FFT for the most of languages (the difference is, however, not st atistically significant
— the more detailed analysis is provided in the next sections). For sub -tasks 2 and 3,
we also observe a higher standard deviation of the results, implying a higher instability
of fine-tuning when PEFTs are applied.
April 9, 2024 27/45

--- PAGE 28 ---
Comparison of the effect of a training scenario on each train ing
technique
To answer the second question (RQ2), we compare FFT, LoRA and a dapter training
techniques across the three training scenarios introduced above , namely ‘Multilingual
Joint’, ‘English + Translations’ and ‘English Only’. It should be noted tha t in the
latter two scenarios all languages in the test set are unseen (exce pt English) as the
model did not have access to training data in those languages. The r esults are
measured in the official sub-task metrics (F1 macrofor sub-task 1 and F1 microfor
sub-tasks 2 and 3) and are shown in Tables 7, 8 and 9.
Table 7. Sub-task 1: Genre Detection - Mean ±1 STD F1 macroscores.
LanguageMultilingual Joint English + Translations English Only
FFT LoRA Adapter FFT LoRA Adapter FFT LoRA Adapter
EN 52.7±0.5 49.4±0.452.8±0.2 53.1±1.152.4±0.6*53.5±0.940.9±1.339.1±1.242.2±0.6
FR *69.7±1.267.4±2.3 67.5±0.9 68.0±1.969.2±1.568.4±0.765.0±2.266.3±0.566.7±3.7
DE 66.3±0.5 64.8±1.2*67.2±0.865.4±2.563.9±3.065.8±1.262.1±4.264.2±2.863.6±0.7
IT 52.2±1.4*53.4±1.852.0±3.1 52.1±1.752.0±1.552.9±0.845.1±3.647.3±1.844.2±1.1
PL *69.2±1.166.8±0.4 65.2±1.5 61.3±0.764.0±0.761.8±2.458.9±2.260.6±1.459.6±3.0
RU *57.4±0.655.7±1.7 52.8±0.9 52.4±1.354.2±0.854.9±2.549.7±0.949.4±2.448.7±0.6
Average *61.3±2.759.6±1.9 59.6±3.1 58.7±1.859.3±5.259.6±2.753.6±3.554.5±6.154.2±2.9
ES *47.1±1.441.8±0.5 44.2±0.7 44.5±2.746.0±1.3 46.2±542.3±2.941.7±0.840.9±1.1
EL 40.8±2.441.4±2.7 40.9±1.7*43.5±2.142.9±1.742.2±3.638.9±2.738.6±1.737.5±0.8
KA *83.3±2.180.8±5.0 79.2±1.881.0±3.279.6±4.177.5±2.276.7±3.372.9±1.574.8±2.4
Average *57.1±2.354.7±2.4 54.8±1.856.3±2.456.2±3.355.3±1.952.6±2.951.1±4.051.1±3.3
All *59.9±3.157.9±6.3 58.0±2.0 57.9±3.458.2±3.858.1±4.153.3±5.253.3±2.653.1±3.6
Best scores by language are marked with an asterisk ( ∗). Best scores by training method for each training scenario are inbold.
April 9, 2024 28/45

--- PAGE 29 ---
Table 8. Sub-task 2: Framing Detection - Mean ±1 STD F1 microscores.
LanguageMultilingual Joint English + Translations English Only
FFT LoRA Adapter FFT LoRA Adapter FFT LoRA Adapter
EN *55.8±0.252.2±1.755.7±2.0 54.9±1.954.3±1.755.1±1.546.5±1.346.2±1.347.8±0.7
FR 53.3±3.347.3±1.550.8±3.6*54.5±2.552.9±2.550.3±2.544.2±1.641.0±0.841.7±0.4
DE 63.1±1.962.3±2.2*64.2±1.058.9±1.655.8±1.859.1±3.346.6±3.345.7±1.149.1±2.9
IT *59.9±1.956.8±1.658.2±1.0 57.0±1.955.7±0.459.5±1.054.9±1.955.3±2.055.3±0.6
PL *65.2±0.861.0±0.864.1±1.7 55.3±3.854.1±1.556.3±1.945.5±0.546.3±0.146.1±2.7
RU 45.3±3.043.6±0.641.7±2.0 40.3±2.038.2±1.241.0±2.735.1±1.131.1±1.631.7±1.3
Average *57.1±7.353.9±7.555.8±8.6 53.5±6.751.8±6.853.6±7.045.5±6.344.3±7.945.3±8.0
ES *52.7±2.151.7±3.049.1±2.0 49.7±0.749.9±1.046.6±2.541.8±1.935.9±3.836.4±2.1
EL *54.9±1.751.4±1.254.1±2.9 53.2±0.150.7±0.755.5±1.147.2±1.243.6±0.247.1±1.5
KA *60.1±4.253.9±4.855.3±1.6 55.3±4.952.8±4.057.0±4.651.4±0.945.3±1.750.1±2.1
Average *55.9±3.852.3±1.452.8±3.3 52.7±2.851.1±1.553.1±5.646.8±4.841.6±5.044.5±7.2
All *56.7±6.153.4±6.054.8±7.1 53.2±5.551.6±5.453.4±6.245.9±5.643.4±6.945.0±7.3
Best scores by language are marked with an asterisk ( ∗). Best scores by training method for each training scenario are inbold.
Table 9. Sub-task 3: Persuasion Techniques - Mean ±1 STD F1 microscores.
LanguageMultilingual Joint English + Translations English Only
FFT LoRA Adapter FFT LoRA Adapter FFT LoRA Adapter
EN 34.9±1.737.7±0.9 37.5±2.942.2±0.842.9±0.4*44.0±1.034.0±1.634.2±5.533.3±3.4
FR 45.9±1.2*48.6±0.845.7±1.942.5±0.541.9±1.842.9±1.128.9±2.528.3±6.024.2±4.3
DE 52.1±1.9 52.3±0.9*53.0±0.743.2±1.545.6±2.044.0±1.929.1±2.527.4±4.226.7±6.9
IT 55.1±2.5*58.7±0.558.1±1.653.4±2.552.9±0.954.0±1.533.6±3.033.0±7.532.6±3.0
PL 40.4±3.2*42.1±0.641.9±2.737.8±1.736.9±1.637.5±1.223.4±2.222.3±1.919.6±5.2
RU 40.9±1.4*42.3±0.339.2±2.636.2±1.335.4±0.837.1±0.422.9±2.525.5±4.517.9±3.7
Average 44.9±7.7*46.9±7.745.9±8.142.5±6.042.6±6.343.5±6.428.6±4.828.4±4.525.7±6.4
ES 37.7±2.3*39.0±1.636.7±1.338.3±0.935.3±1.537.8±0.826.1±1.226.0±4.723.0±5.1
EL 26.7±0.9*25.5±0.625.4±1.522.6±0.121.8±0.921.9±0.916.2±1.916.7±0.912.5±1.2
KA *42.6±1.040.4±3.1 42.2±2.436.8±3.135.4±1.636.7±1.829.4±4.224.2±6.122.4±8.7
Average *35.7±8.134.9±8.2 34.7±8.532.6±8.730.8±7.832.1±8.923.9±6.922.3±4.919.3±5.9
All 41.8±8.6*42.9±9.542.2±9.539.2±8.138.7±8.739.7±8.827.1±5.626.4±5.323.6±6.7
Best scores by language are marked with an asterisk ( ∗). Best scores by training method for each training scenario are inbold.
April 9, 2024 29/45

--- PAGE 30 ---
(1)The diversity of languages in the training set improves the a verage
performance of the FFT technique. For all three sub-tasks, we observe a
significantly better average classification performance when the t raining data is
provided in the original 6 different languages as opposed to providin g the same
amount of data in English only. When looking at the individual languages , this effect
holds for all languages in sub-task 1 except for English (the only see n language in the
‘English + Translations’ training scenario) and Spanish (one of the un seen languages
in the joint multilingual setting). The decreased performance on th e English test set
in the ‘Multilingual Joint’ scenario for this task can potentially be expla ined with the
‘negative inference’ effect highlighted in previous studies [39]. How ever, we do not
observe this effect consistently across the sub-tasks. In part icular, for sub-task 2, the
only exception is French, which benefits from a monolingual training s etting. Notably,
the performance on the English test set decreases in the ‘English + T ranslations’
scenario for sub-task 2, despite being trained on much more data in this language.
Finally, for sub-task 3, French is the only language in the test set th at benefits from
being trained on monolingual English data, which is consistent with sub -task 2.
(2)The diversity of languages in the training set has an inconsi stent
effect on the performance of the LoRA training technique acr oss the three
sub-tasks. While we observe an average decrease in classification performance in the
‘English + Translations’ training scenario for sub-tasks 2 and 3, this setting improves
the average results for sub-task 1. For sub-task 3, this effect holds for every language
in the test set, while for sub-task 2, LoRA improves performance o n EN and FR when
trained using monolingual English data. For sub-task 1, LoRA benef its from the
multilingual training data when making predictions on 5 out of the 9 lang uages (DE,
IT, PL, RU, and KA) and favours the monolingual English training set ting for the 4
remaining languages (EN, FR, EL and ES). The difference is particula rly high for
Spanish, resulting in a slightly better average performance in a mono lingual training
scenario for sub-task 1.
(3)The diversity of languages in the training set has an inconsi stent
effect on adapter classification performance across all ta sks.While the
‘English + Translations’ training scenario insignificantly improves aver age performance
in sub-task 1, it decreases the average performance in sub-task s 2 and 3. Adapters
April 9, 2024 30/45

--- PAGE 31 ---
benefit from a monolingual training scenario when making predictions on 6 out of the
9 languages (EN, FR, IT, RU, ES and EL) in sub-task 1. For sub-tas ks 2 and 3, this is
the case only for 3 languages (IT, EL and KA) and 2 languages (EN an d ES)
respectively.
(4)Decrease in the size of the training set decreases the perfor mance of
the FFT, LoRA as well as adapter method on all seen and zero-sh ot
languages . We observe a significant decrease in FFT’s performance and both P EFTs
in the ‘English Only’ training scenario compared to the ‘English + Transla tion’ setting
across all the sub-tasks and for every language within each sub-t ask. This result is
particularly noteworthy for the English test set as it indicates that even potentially
noisy translated data is able to improve the performance on a given la nguage as
compared to using a smaller but better quality dataset.
In summary, the effect of removing language diversity from the tr aining set is not
consistent across SemEval 2023 sub-tasks and training techniqu es. Sub-task 1, in
particular, demonstrates slight improvement in performance for L oRA and adapter
methods when trained on ‘English + Translations’ data. All training te chniques show
a significantly decreased performance when trained on the original English-only data,
demonstrating the importance of the size of the training data.
In the next section, we compare the results of the three training t echniques within
each of the three training scenarios.
Comparison of training techniques for each training scenar io
and language
To answer the third research question (RQ3), we analyse whether the performance of
FFT, LoRA and adapter methods is consistent across training scen arios or are certain
scenarios more prone to the lack of training data and or language div ersity within the
training data. This analysis, on the top of results presented in Table s 7, 8 and 9 for
sub-tasks 1, 2 and 3 respectively, complement the previous RQ2.
(1)FFT outperforms LoRA and adapter methods for sub-tasks 1 and 2
in a ‘Multilingual Joint‘ training scenario, while for sub- task 3, only
zero-shot predictions on unseen languages benefit from the FFT method .
April 9, 2024 31/45

--- PAGE 32 ---
We observe that FFT yields best performance in a multilingual setting for most of the
seen and unseen languages in sub-tasks 1 and 2. While the differenc es between FFT
and LoRA are often less than 1% for sub-task 1, sub-task 2 demon strates a clear
preference for the FFT classification approach.
(2)In an ‘English + Translations’ training scenario, adapters outperform
FFT across all sub-tasks and show better or on-par performan ce compared
to LoRA. A particularly clear preference for adapters can be observed for sub-task 2,
where the majority of seen and unseen languages benefit from this training technique.
For sub-tasks 1 and 2, most of the zero-shot predictions on unse en languages favour
FFT. The performance of the adapter method is particularly consis tent on English,
the only seen language in this training scenario, with all sub-tasks fa vouring the
adapter classifier.
(3)In an ‘English Only’ training scenario with less data, diffe rences
between FFT, LoRA and adapters become less obvious across al l sub-tasks.
In this setting, the differences in the average classification perfo rmances between FFT,
adapters and LoRA across all sub-tasks is often insignificant, with less than 1% of one
method over the other. For example, the difference between the adapter method and
LoRA for sub-task 1 is 0.2%, and the average performance of FFT a nd LoRA is the
same and differs only in the confidence intervals. Similarly, for sub-t ask 2, FFT is
comparable to the adapter method, and for sub-task 3, FFT is ver y close in the
performance to LoRA. We observe that for sub-tasks 1 and 2, ad apters perform better
for EN (the only seen language), while for sub-task 3, LoRA yields be tter performance
for the seen language (EN).
(4)The overall best performance across all training scenarios and
training techniques is achieved in a ‘Multilingual Joint’ t raining scenario.
While the FFT method works better for sub-tasks 1 and 2 in this sett ing, sub-task 3
shows a clear improvement when trained using the LoRA method. While sub-tasks 1
and 2 are consistent in favouring FFT for both seen and unseen (ES , EL, KA)
languages, sub-task 3 favours LoRA when making the predictions o n seen languages
only. For unseen languages, sub-task 3 agrees with sub-tasks 1 a nd 2 in favouring the
FFT classification approach.
Some of the languages demonstrate strong preferences toward s certain training
April 9, 2024 32/45

--- PAGE 33 ---
techniques and training scenarios:
(1)English consistently favours adapter classification appr oach across all
the training scenarios in sub-task 1 .
(2)FFT method yields best performance on Georgian zero-shot
predictions across all settings in sub-tasks 1 and 3 .
(3)In a ‘Multilingual Joint’ training scenario, German demons trates a
consistent preference for the adapter training technique a cross all three
sub-tasks . In particular, German is the only language in sub-task 2 where FFT is not
producing the best performance for the ‘Multilingual Joint’ training scenario. It is also
the only case where adapter models demonstrate better perform ance than LoRA for
seen languages for sub-task 3 in the joint training scenario.
We also observe that LoRA, in a ‘Multilingual Joint’ training scenario, s hows the
best overall performance for sub-task 3. Since this method was n ot used by any of the
teams participating in the shared task, we want to look into how this a pproach
compares to the official leaderboard results after the competitio n. Table 10
demonstrates the scores of the winning system for sub-task 3 alo ng with the scores
achieved in sub-task 3 in these experiments. As can be seen, the Lo RA method for
sub-task 3 outperforms most of the results of the winning system s. We achieve an
increase in the performance of up to 19.63% for all the languages ex cept for Georgian
(KA). 6 out of 9 languages (FR, IT, PL, RU, ES and EL) achieve the b est result in the
‘Multilingual Joint’ training scenario when applying LoRA. Not surprisin gly, the best
score for English is achieved in one-to-one ‘English only’ scenario. Th is increase also
results in the first placings in 8 out of 9 languages.
April 9, 2024 33/45

--- PAGE 34 ---
Table 10. Sub-task 3 official test set leaderboard comparison.
Language 1st Place TeamF1micro
(1st Place)F1micro
(Ours)F1micro
IncreaseFinal Placing
(Ours)
EN APatt 0.37562 0.44937 19.63% 1
FR NAP 0.46869 0.49238 5.05% 1
DE KInITVeraAI 0.51304 0.54174 5.30% 1
IT KInITVeraAI 0.55019 0.59919 8.91% 1
PL KInITVeraAI 0.43037 0.44964 4.48% 1
RU KInITVeraAI 0.38682 0.42635 10.22% 1
ES TeamAmpa 0.38106 0.40674 6.74% 1
EL KInITVeraAI 0.26733 0.27668 3.38% 1
KA KInITVeraAI 0.45714 0.448 -2.00% 2
To summarise this analysis, we observe that adapter models and LoR A produce
comparable or even better results for sub-task 3 or in the scenar ios when the training
data is scarce or is only available in one language. This is a promising resu lt given
that these approaches require less memory and training time, as dis cussed in the
previous section.
Limitations and discussion
The results reported in this study are limited to using one dataset an notated with
different classification tasks. Further analysis would be beneficial to test these findings
on a diverse set of corpora and tasks. Additionally, we examined the effect of one
adapter technique, which prevents us from generalising our finding s to all types of
adapter methods. Finally, since sub-tasks 1 and 2 are performed a t article-level where
the average length of the article is more than 512 tokens (Table 1), many of the signals
present in the rest of the articles could potentially be ignored by all o f the methods
compared in this study.
Our findings provide novel insights into the effectiveness of PEFT t echniques in
multilingual classification tasks. In particular, we found that the mu ltilingual training
scenario on average improves the performance on all sub-tasks. However, the best
April 9, 2024 34/45

--- PAGE 35 ---
performing methods within this setting differed depending on the su b-task.
Our results on the LoRA method are novel as this PEFT technique wa s not
previously investigated in multilingual tasks. While we observe that su b-tasks 1 and 2
benefit from FFT method in a ‘Multilingual Joint’ training scenario, our results
demonstrate a particularly interesting behaviour of the LoRA meth od in this training
scenario for sub-task 3, where it consistently outperforms the F FT approach. While a
thorough analysis is needed to conclude why this is the case, the diff erences in the
properties of these tasks that we presented in Table 1 could poten tially bring light to
this question. Firstly, sub-task 3 is trained on and applied to much sh orter texts than
sub-tasks 1 and 2, while having substantially more training examples. Additionally,
sub-task 3 is characterised by a high number of classes (23 compar ed to 14 in sub-task
2 and 3 in sub-task 1) and a much more severe data imbalance, as sho wn in Table 1.
The nature of the tasks is also quite different. The task of framing detection relies less
on commonsense knowledge and pragmatics, while sub-tasks 1 and 3 are more
subjective for human experts. All of these factors can potentia lly influence the
performance of each method in the respective sub-tasks, and fu rther research is needed
to make conclusions regarding the types of classification tasks tha t benefit from from
LoRA method.
We observe the adapter method to outperform FFT only in the zero -shot
cross-lingual scenario when a sufficient amount of data is provided in the source
language (‘English + Translations’ scenario) while being much less effic ient when the
training data contains a variety of languages or is limited and monolingu al. This is an
important insight that complements previous studies which report t he adapter
approach to always outperform FFT in a cross-lingual zero-shot in ference [11,13]. Our
results also challenge a prior conclusion by Xenouleas et al. [14] which suggests that
adapter methods are universally beneficial for low-resource and c ross-lingual tasks.
Additionally, our results disagree with the prior reports that the FF T approach is
always outperformed by the adapter method in a zero-shot cross -lingual
inference [11,13]. Contrary to this, we found that for the ‘unseen ’ languages consistent
across all training scenarios (‘ES’, ‘EL’ and ‘KA’), FFT outperforms a dapter method
in all training scenarios for sub-task 1 and sub-task 3, and for two our three scenarios
in sub-task 2, ‘Multilingual Joint’ and ‘English Only’. Finally, our study pr ovides the
April 9, 2024 35/45

--- PAGE 36 ---
first to our knowledge comparison of the ‘Multilingual Joint’ scenario with the
translation-based monolingual one in terms of the zero-shot cros s-lingual inference.
While sub-task 3 performs better for the LoRA approach in a multiling ual setting
on average, this is not consistent across seen and zero-shot lang uages, with FFT
performing better in a zero-shot cross-lingual setting. This pref erence is particularly
strong in the cases when the amount of training data is limited. This ca n indicate the
fact that the adapter method is potentially not good at cross-lingu al generalisation,
however, more analysis is needed on a wider variety of tasks.
In the tasks with highly skewed data, adapter method can potentia lly be more
prone than FFT to favoring the most frequent class, since as can b e observed from
Table 6, F1 microscore for sub-task 1 is significantly higher for adapter method
compared to FFT, while FFT results in a higher F1 macro. However, a detailed error
analysis to confirm or refute this assumption is currently not possib le since the
gold-standard labels for the test set are not released.
Low-resource languages may have a preference for FFT in all train ing scenarios
when predictions are made in a cross-lingual zero-shot way. This as sumption is
suggested by the fact that Georgian is the only language that cons istently prefers the
FFT approach across all training scenarios and for all sub-tasks.
Interestingly, the performance on Georgian in sub-tasks 1 and 2 is higher than that
on seen languages, despite being low-resource and zero-shot. On e potential reason for
this observation could be the fact that, as was previously shown in T ables 2 and 3, the
texts in the test set for Georgian are, on average, within the limit of XLM-R and are
much shorter than the input lengths for other 8 languages. This co uld explain why the
same effect is not observed for sub-task 3, where all the inputs a re within the
transformer token limit and are relatively short. However, with the lack of the gold
standard labels for surprise languages, it is not possible to eliminate o ther reasons for
this phenomenon, as it could also be explained with the lack of particula rly difficult to
predict classes in the test set for Georgian for both sub-tasks.
April 9, 2024 36/45

--- PAGE 37 ---
Conclusion
In this work, we performed the first (to our knowledge) analysis of the performance of
Low-Rank Adaptation (LoRA) technique and its comparison with the adapter and full
fine-tuning (FFT) methods in a multilingual multiclass scenario.
We found that parameter-efficient fine-tuning techniques (PEFT s), LoRA and
bottleneck adapter, provide significant computation efficiency co mpared to FFT in
terms of the training time, the number of trainable parameters and the amount of
VRAM memory required. In particular, they reduce the number of t rainable
parameters between 140 and 280 times and achieve between 32% an d 44% shorter
training time.
The comparison between LoRA and adapter methods in terms of the parameter
efficiency suggests that their performance depends on a certain sub-task and
hyperparameters used. This observation is aligned with the results of the previous
study by He et al. [11], who found the benefit of the adapter approa ch to be
task-dependent. While we observe LoRA to be more efficient than t he adapter method
for tasks of news articles’ genre and framing detection, the adap ter method takes less
average time and uses less training parameters for the latter one.
Moreover, we found the performance of the methods to be highly d ependent on the
training scenario. Adapter method performs better than LoRA an d FFT in the
scenario where there is a lack of language diversity in the training set across the
sub-tasks.
The differences between all three methods become insignificant, o ften less than 1%
on average, as the size of the training data decreases. This indicat es that it is possible
to achieve high computational efficiency by using PEFT methods with out losing much
in terms of the classification performance in this setting. More expe riments on this
result involving a gradual decrease in the size of the training set wou ld be beneficial in
future to find the threshold when the performances across the m ethods match or when
PEFT methods become more classification-efficient.
The performance on the unseen languages is often highly dependen t on the training
scenario. We found that FFT performs better than PEFT methods in zero-shot
cross-lingual predictions when trained on a joint multilingual datase t, which is
April 9, 2024 37/45

--- PAGE 38 ---
different from the results reported by Chalkidis et al. [13]. However , we observe the
effect reported by the authors in a monolingual training scenario, where adapter
method performs better on zero-shot languages.
Finally, the multilingual joint LoRA setting allowed us to significantly impr ove our
official results on the SemEval 2023 sub-task 3 (persuasion tech niques detection) and
to outperform most of the official leaderboard-best results, pla cing first in all
languages except Georgian, where we are in the second place compa red to the official
leaderboard results.
Supporting information
S1 Appendix. Complete list of categories and their hierarch y for sub-task
3.
S2 Appendix. Ablation study for PEFT techniques.
S3 Appendix. Model size selection for XLM-RoBERTa.
S4 Appendix Amount of resources per-language.
S5 Appendix Comparison of Houlsby and Pfeiffer adapters acr oss three
sub-tasks.
Acknowledgements
We thank Joanna Wright for providing her comments and suggestion s on the draft of
this work.
References
1. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of De ep
Bidirectional Transformers for Language Understanding. In: No rth American
Chapter of the Association for Computational Linguistics; 2019.Av ailable from:
https://api.semanticscholar.org/CorpusID:52967399 .
April 9, 2024 38/45

--- PAGE 39 ---
2. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring
the limits of transfer learning with a unified text-to-text transfor mer. The
Journal of Machine Learning Research. 2020;21(1):5485–5551.
3. Ben Zaken E, Goldberg Y, Ravfogel S. BitFit: Simple Parameter-e fficient
Fine-tuning for Transformer-based Masked Language-models. I n: Muresan S,
Nakov P, Villavicencio A, editors. Proceedings of the 60th Annual Me eting of
the Association for Computational Linguistics (Volume 2: Short Pap ers).
Dublin, Ireland: Association for Computational Linguistics; 2022. p . 1–9.
Available from: https://aclanthology.org/2022.acl-short.1 .
4. Jiang H, He P, Chen W, Liu X, Gao J, Zhao T. SMART: Robust and Eff icient
Fine-Tuning for Pre-trained Natural Language Models through Pr incipled
Regularized Optimization. In: Jurafsky D, Chai J, Schluter N, Tetr eault J,
editors. Proceedings of the 58th Annual Meeting of the Associatio n for
Computational Linguistics. Online: Association for Computational L inguistics;
2020. p. 2177–2190. Available from:
https://aclanthology.org/2020.acl-main.197 .
5. Xu R, Luo F, Zhang Z, Tan C, Chang B, Huang S, et al. Raise a Child in L arge
Language Model: Towards Effective and Generalizable Fine-tuning. In: Moens
MF, Huang X, Specia L, Yih SWt, editors. Proceedings of the 2021 Co nference
on Empirical Methods in Natural Language Processing. Online and Pu nta Cana,
Dominican Republic: Association for Computational Linguistics; 2021 . p.
9514–9528. Available from:
https://aclanthology.org/2021.emnlp-main.749 .
6. Lialin V, Deshpande V, Rumshisky A. Scaling down to scale up: A guide to
parameter-efficient fine-tuning. arXiv preprint arXiv:230315647 . 2023;.
7. Li XL, Liang P. Prefix-Tuning: Optimizing Continuous Prompts for Generation.
In: Zong C, Xia F, Li W, Navigli R, editors. Proceedings of the 59th A nnual
Meeting of the Association for Computational Linguistics and the 11 th
International Joint Conference on Natural Language Processin g (Volume 1:
April 9, 2024 39/45

--- PAGE 40 ---
Long Papers). Online: Association for Computational Linguistics; 2 021. p.
4582–4597. Available from: https://aclanthology.org/2021.acl-long.353 .
8. Lester B, Al-Rfou R, Constant N. The Power of Scale for Parame ter-Efficient
Prompt Tuning. In: Moens MF, Huang X, Specia L, Yih SWt, editors.
Proceedings of the 2021 Conference on Empirical Methods in Natur al Language
Processing. Online and Punta Cana, Dominican Republic: Association f or
Computational Linguistics; 2021. p. 3045–3059. Available from:
https://aclanthology.org/2021.emnlp-main.243 .
9. Bapna A, Firat O. Simple, Scalable Adaptation for Neural Machine T ranslation.
In: Inui K, Jiang J, Ng V, Wan X, editors. Proceedings of the 2019 C onference
on Empirical Methods in Natural Language Processing and the 9th I nternational
Joint Conference on Natural Language Processing (EMNLP-IJCN LP). Hong
Kong, China: Association for Computational Linguistics; 2019. p. 1 538–1548.
Available from: https://aclanthology.org/D19-1165 .
10. Houlsby N, Giurgiu A, Jastrzebski S, Morrone B, De Laroussilhe Q, Gesmundo
A, et al. Parameter-Efficient Transfer Learning for NLP. In: Cha udhuri K,
Salakhutdinov R, editors. Proceedings of the 36th International Conference on
Machine Learning. vol. 97 of Proceedings of Machine Learning Resea rch. PMLR;
2019. p. 2790–2799. Available from:
https://proceedings.mlr.press/v97/houlsby19a.html .
11. He R, Liu L, Ye H, Tan Q, Ding B, Cheng L, et al. On the Effectivene ss of
Adapter-based Tuning for Pretrained Language Model Adaptatio n. In:
Proceedings of the 59th Annual Meeting of the Association for Com putational
Linguistics and the 11th International Joint Conference on Natur al Language
Processing (Volume 1: Long Papers). Online: Association for Compu tational
Linguistics; 2021. p. 2208–2222. Available from:
https://aclanthology.org/2021.acl-long.172 .
12. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al. LoRA: Low- Rank
Adaptation of Large Language Models. In: International Confer ence on
April 9, 2024 40/45

--- PAGE 41 ---
Learning Representations; 2022.Available from:
https://openreview.net/forum?id=nZeVKeeFYf9 .
13. Chalkidis I, Fergadiotis M, Androutsopoulos I. MultiEURLEX - A mu lti-lingual
and multi-label legal document classification dataset for zero-sho t cross-lingual
transfer. In: Proceedings of the 2021 Conference on Empirical M ethods in
Natural Language Processing. Online and Punta Cana, Dominican Re public:
Association for Computational Linguistics; 2021. p. 6974–6996. A vailable from:
https://aclanthology.org/2021.emnlp-main.559 .
14. Xenouleas S, Tsoukara A, Panagiotakis G, Chalkidis I, Androuts opoulos I.
Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classific ation. In:
Proceedings of the 12th Hellenic Conference on Artificial Intelligenc e. SETN ’22.
New York, NY, USA: Association for Computing Machinery; 2022.Ava ilable
from:https://doi.org/10.1145/3549737.3549760 .
15. Piskorski J, Stefanovitch N, Da San Martino G, Nakov P. SemEv al-2023 Task 3:
Detecting the Category, the Framing, and the Persuasion Techniq ues in Online
News in a Multi-lingual Setup. In: Proceedings of the The 17th Inter national
Workshop on Semantic Evaluation (SemEval-2023). Toronto, Cana da:
Association for Computational Linguistics; 2023. p. 2343–2361. A vailable from:
https://aclanthology.org/2023.semeval-1.317 .
16. Wu B, Razuvayevskaya O, Heppell F, Leite JA, Scarton C, Bont cheva K, et al.
SheffieldVeraAI at SemEval-2023 Task 3: Mono and Multilingual Appr oaches
for News Genre, Topic and Persuasion Technique Classification. In: Proceedings
of the The 17th International Workshop on Semantic Evaluation
(SemEval-2023). Toronto, Canada: Association for Computation al Linguistics;
2023. p. 1995–2008. Available from:
https://aclanthology.org/2023.semeval-1.275 .
17. Hromadka T, Smolen T, Remis T, Pecher B, Srba I. KInITVeraAI at
SemEval-2023 Task 3: Simple yet Powerful Multilingual Fine-Tuning fo r
Persuasion Techniques Detection. In: Proceedings of the The 17t h International
Workshop on Semantic Evaluation (SemEval-2023). Toronto, Cana da:
April 9, 2024 41/45

--- PAGE 42 ---
Association for Computational Linguistics; 2023. p. 629–637. Ava ilable from:
https://aclanthology.org/2023.semeval-1.86 .
18. Gururangan S, Marasovi´ c A, Swayamdipta S, Lo K, Beltagy I, Downey D, et al.
Don’t Stop Pretraining: Adapt Language Models to Domains and Task s. In:
Jurafsky D, Chai J, Schluter N, Tetreault J, editors. Proceeding s of the 58th
Annual Meeting of the Association for Computational Linguistics. O nline:
Association for Computational Linguistics; 2020. p. 8342–8360. A vailable from:
https://aclanthology.org/2020.acl-main.740 .
19. Pfeiffer J, Vuli´ c I, Gurevych I, Ruder S. MAD-X: An Adapter -Based
Framework for Multi-Task Cross-Lingual Transfer. In: Proceed ings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP).
Online: Association for Computational Linguistics; 2020. p. 7654–7 673.
Available from: https://aclanthology.org/2020.emnlp-main.617 .
20. Dettmers T, Pagnoni A, Holtzman A, Zettlemoyer L. QLoRA: Ef ficient
finetuning of quantized llms. arXiv preprint arXiv:230514314. 2023;.
21. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: A Multi- Task
Benchmark and Analysis Platform for Natural Language Understa nding. In:
Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing a nd
Interpreting Neural Networks for NLP. Brussels, Belgium: Assoc iation for
Computational Linguistics; 2018. p. 353–355. Available from:
https://aclanthology.org/W18-5446 .
22. Yu X, Chatterjee T, Asai A, Hu J, Choi E. Beyond Counting Dat asets: A
Survey of Multilingual Dataset Construction and Necessary Resou rces. In:
Goldberg Y, Kozareva Z, Zhang Y, editors. Findings of the Associat ion for
Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab Emir ates:
Association for Computational Linguistics; 2022. p. 3725–3743. A vailable from:
https://aclanthology.org/2022.findings-emnlp.273 .
23. Kiesel J, Mestre M, Shukla R, Vincent E, Adineh P, Corney D, et a l.
SemEval-2019 Task 4: Hyperpartisan News Detection. In: Procee dings of the
13th International Workshop on Semantic Evaluation. Minneapolis, Minnesota,
April 9, 2024 42/45

--- PAGE 43 ---
USA: Association for Computational Linguistics; 2019. p. 829–839 . Available
from:https://aclanthology.org/S19-2145 .
24. Abu Farha I, Oprea SV, Wilson S, Magdy W. SemEval-2022 Task 6:
iSarcasmEval, Intended Sarcasm Detection in English and Arabic. In :
Proceedings of the 16th International Workshop on Semantic Eva luation
(SemEval-2022). Seattle, United States: Association for Comput ational
Linguistics; 2022. p. 802–814. Available from:
https://aclanthology.org/2022.semeval-1.111 .
25. Da San Martino G, Barr´ on-Cede˜ no A, Wachsmuth H, Petrov R , Nakov P.
SemEval-2020 Task 11: Detection of Propaganda Techniques in New s Articles.
In: Proceedings of the Fourteenth Workshop on Semantic Evaluat ion. Barcelona
(online): International Committee for Computational Linguistics; 2020. p.
1377–1414. Available from: https://aclanthology.org/2020.semeval-1.186 .
26. Dimitrov D, Bin Ali B, Shaar S, Alam F, Silvestri F, Firooz H, et al.
SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images.
In: Proceedings of the 15th International Workshop on Semantic Evaluation
(SemEval-2021). Online: Association for Computational Linguistics ; 2021. p.
70–98. Available from: https://aclanthology.org/2021.semeval-1.7 .
27. Card D, Boydstun AE, Gross JH, Resnik P, Smith NA. The Media Fr ames
Corpus: Annotations of Frames Across Issues. In: Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics an d the 7th
International Joint Conference on Natural Language Processin g (Volume 2:
Short Papers). Beijing, China: Association for Computational Ling uistics; 2015.
p. 438–444. Available from: https://aclanthology.org/P15-2072 .
28. Da San Martino G, Yu S, Barr´ on-Cede˜ no A, Petrov R, Nakov P . Fine-Grained
Analysis of Propaganda in News Article. In: Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th I nternational
Joint Conference on Natural Language Processing (EMNLP-IJCN LP). Hong
Kong, China: Association for Computational Linguistics; 2019. p. 5 636–5646.
Available from: https://aclanthology.org/D19-1565 .
April 9, 2024 43/45

--- PAGE 44 ---
29. Billert F, Conrad S. HHU at SemEval-2023 Task 3: An Adapter-ba sed
Approach for News Genre Classification. In: Proceedings of the Th e 17th
International Workshop on Semantic Evaluation (SemEval-2023). Toronto,
Canada: Association for Computational Linguistics; 2023. p. 1166 –1171.
Available from: https://aclanthology.org/2023.semeval-1.162 .
30. Falk N, Eichel A, Piccirilli P. NAP at SemEval-2023 Task 3: Is Less R eally
More? (Back-)Translation as Data Augmentation Strategies for D etecting
Persuasion Techniques. In: Proceedings of the The 17th Interna tional Workshop
on Semantic Evaluation (SemEval-2023). Toronto, Canada: Assoc iation for
Computational Linguistics; 2023. p. 1433–1446. Available from:
https://aclanthology.org/2023.semeval-1.198 .
31. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: A Ro bustly
Optimized BERT Pretraining Approach. Computing Research Reposit ory.
2019;arXiv:1907.11692. doi:10.48550/ARXIV.1907.11692.
32. Piskorski J, Stefanovitch N, Bausier VA, Faggiani N, Linge J, K harazi S, et al.
News Categorization, Framing and Persuasion Techniques: Annota tion
Guidelines. European Commission, Ispra, JRC132862; 2023.
33. Barbaresi A. Trafilatura: A Web Scraping Library and Command -Line Tool for
Text Discovery and Extraction. In: Proceedings of the Joint Conf erence of the
59th Annual Meeting of the Association for Computational Linguist ics and the
11th International Joint Conference on Natural Language Proc essing: System
Demonstrations. Association for Computational Linguistics; 2021 . p. 122–131.
Available from: https://aclanthology.org/2021.acl-demo.15 .
34. Pikuliak M, ˇSimko M, Bielikov´ a M. Cross-lingual learning for text processing: A
survey. Expert Systems with Applications. 2021;165:113765.
doi:https://doi.org/10.1016/j.eswa.2020.113765.
35. Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Gu zm´ an F,
et al. Unsupervised Cross-lingual Representation Learning at Sca le. In:
Proceedings of the 58th Annual Meeting of the Association for Com putational
April 9, 2024 44/45

--- PAGE 45 ---
Linguistics. Online: Association for Computational Linguistics; 2020 . p.
8440–8451. Available from: https://aclanthology.org/2020.acl-main.747 .
36. Pfeiffer J, R¨ uckl´ e A, Poth C, Kamath A, Vuli´ c I, Ruder S, e t al. AdapterHub:
A Framework for Adapting Transformers. In: Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing : System
Demonstrations. Online: Association for Computational Linguistics ; 2020. p.
46–54. Available from: https://aclanthology.org/2020.emnlp-demos.7 .
37. Pires T, Schlinger E, Garrette D. How Multilingual is Multilingual BER T? In:
Proceedings of the 57th Annual Meeting of the Association for Com putational
Linguistics. Florence, Italy: Association for Computational Linguis tics; 2019. p.
4996–5001. Available from: https://aclanthology.org/P19-1493 .
38. Savenkov K, Lopez M. The State of the Machine Translation 202 2. In:
Proceedings of the 15th Biennial Conference of the Association fo r Machine
Translation in the Americas (Volume 2: Users and Providers Track an d
Government Track). Orlando, USA: Association for Machine Trans lation in the
Americas; 2022. p. 32–49. Available from:
https://aclanthology.org/2022.amta-upg.4 .
39. Wang Z, Lipton ZC, Tsvetkov Y. On Negative Interference in Mu ltilingual
Models: Findings and A Meta-Learning Treatment. In: Webber B, Co hn T, He
Y, Liu Y, editors. Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Online: Association for Co mputational
Linguistics; 2020. p. 4438–4450. Available from:
https://aclanthology.org/2020.emnlp-main.359 .
April 9, 2024 45/45

--- PAGE 46 ---
S1 Appendix - Sub-task 3 Category Taxonomy
Descriptions of each category are provided in the original task pap er [15].
•Attack on Reputation
–Name Calling or Labelling
–Guilt by Association
–Casting Doubt
–Appeal to Hypocrisy
–Questioning the Reputation
•Justification
–Flag Waving
–Appeal to Authority
–Appeal to Popularity
–Appeal to Values
–Appeal to Fear, Prejudice
•Distraction
–Strawman
–Red Herring
–Whataboutism
•Simplification
–Causal Oversimplification
–False Dilemma or No Choice
–Consequential Oversimplification
•Call
–Slogans
–Conversation Killer
April 9, 2024 a/g

--- PAGE 47 ---
–Appeal to Time
•Manipulative Wording
–Loaded Language
–Obfuscation, Intentional Vagueness, Confusion
–Exaggeration or Minimisation
–Repetition
April 9, 2024 b/g

--- PAGE 48 ---
S2 Appendix - ablation study for PEFT techniques
In this section, we perform the ablation study to find how each of th ree PEFT
techniques performs in a multilingual, cross-lingual and monolingual s cenarios on
sub-task 1. As can be seen, while LoRA and Adapter often demonst rate comparable
behaviour in all training scenarios, the BitFit method achieves signific antly lower
results. Importantly, while the results in the multilingual setting are comparable, the
difference between BitFit and two other PEFTs is particularly notice able for the two
cross-lingual settings (‘English + Translations’ and ‘English Only’) sce narios.
Table S2. Sub-task 1: Genre Detection - Mean ±1 STD F1 macroscores for three PEFT techniques in 3
training scenarios.
LanguageMultilingual Joint English + Translations English Only
BitFit LoRA Adapter BitFit LoRA Adapter BitFit LoRA Adapter
EN 51.6±0.849.4±0.452.8±0.250.2±2.152.4±0.6*53.5±0.940.3±1.739.1±1.242.2±0.6
FR 65.5±0.967.4±2.367.5±0.968.0±1.2*69.2±1.568.4±0.765.4±1.566.3±0.566.7±3.7
DE 64.2±0.764.8±1.2*67.2±0.864.1±1.563.9±3.065.8±1.260.3±5.264.2±2.863.6±0.7
IT 51.0±1.6*53.4±1.852.0±3.149.5±1.352.0±1.552.9±0.842.1±2.447.3±1.844.2±1.1
PL 63.6±1.7*66.8±0.465.2±1.558.1±0.564.0±0.7 61.8±2.458.2±0.760.6±1.459.6±3.0
RU 53.1±0.9*55.7±1.752.8±0.951.4±1.254.2±0.854.9±2.543.7±1.849.4±2.448.7±0.6
Average 58.13±1.6*59.6±1.9*59.6±3.156.9±2.659.3±5.2*59.6±2.751.3±3.854.5±6.154.2±2.9
ES 42.4±1.641.8±0.544.2±0.744.0±1.846.0±1.3 *46.2±540.3±0.741.7±0.840.9±1.1
EL 40.1±1.341.4±2.7 40.9±1.739.1±1.1*42.9±1.742.2±3.636.7±1.338.6±1.737.5±0.8
KA 78.5±2.2*80.8±5.079.2±1.877.7±2.479.6±4.1 77.5±2.271.3±2.972.9±1.574.8±2.4
Average 53.6±3.154.7±2.454.8±1.853.2±2.4*56.2±3.355.3±1.949.2±2.751.1±4.051.1±3.3
All 56.3±4.157.9±6.358.0±2.055.2±3.7*58.2±3.858.1±4.150.6±4.453.3±2.653.1±3.6
Best scores by language are marked with an asterisk ( ∗). Best scores by training method for each training scenario are inbold.
April 9, 2024 c/g

--- PAGE 49 ---
S3 Appendix - Model size selection for
XLM-RoBERTa
The results of our comparison between different sizes of RoBERTa model are provided
in S3.1, S3.2, S3.3 Tables. As can be seen, for all training techniques, the average
results are consistently significantly higher for the large size of the model for all three
sub-tasks.
Table S3.1. Comparison of the Base and Large sizes of the mode l in the ‘Multilingual Joint’ scenario for
the FFT method
LanguageSub-task 1 Sub-task 2 Sub-task 3
XLMR-Base XLMR-Large XLMR-Base XLMR-Large XLMR-Base XLMR-Large
EN 35.2±1.8 52.7±0.5 50.9±1.1 55.8±0.2 26.4±0.8 34.9±1.7
FR 69.7±0.6 69.7±1.2 44.4±2.5 53.3±3.3 34.2±1.4 45.9±1.2
DE 67.2±2.2 66.3±0.5 60.7±1.0 63.1±1.9 41.4±1.3 52.1±1.9
IT 44.5±2.1 52.2±1.4 53.4±2.5 59.9±1.9 46.2±0.7 55.1±2.5
PL 68.7±3.0 69.2±1.1 60.1±3.5 65.2±0.8 27.6±1.2 40.4±3.2
RU 55.2±1.4 57.4±0.6 42.2±3.0 45.3±3.0 29.6±2.1 40.9±1.4
ES 40.8±2.4 47.1±1.4 51.9±1.7 52.7±2.1 28.3±1.4 37.7±2.3
EL 43.3±3.9 40.8±2.4 48.9±1.1 54.9±1.7 21.1±2.7 26.7±0.9
KA 77.5±1.8 83.3±2.1 49.7±2.6 60.1±4.2 33.6±2.7 42.6±1.0
all 55.2±6.1 59.9±3.1 51.4±6.2 56.7±6.1 32.1±7.8 41.8±8.6
April 9, 2024 d/g

--- PAGE 50 ---
Table S3.2. Comparison of the Base and Large sizes of the mode l in the ‘Multilingual Joint’ scenario for
LoRA method
LanguageSub-task 1 Sub-task 2 Sub-task 3
XLMR-Base XLMR-Large XLMR-Base XLMR-Large XLMR-Base XLMR-Large
EN 45.2±2.6 49.4±0.4 44.9±3.4 52.2±1.7 31.4±0.8 37.7±0.9
FR 62.1±0.9 67.4±2.3 43.8±0.5 47.3±1.5 42.6±1.1 48.6±0.8
DE 60.2±1.6 64.8±1.2 50.8±1.4 62.3±2.2 48.2±1.3 52.3±0.9
IT 49.4±1.5 53.4±1.8 54.1±1.2 56.8±1.6 50.4±1.9 58.7±0.5
PL 60.2±2.0 66.8±0.4 56.7±1.8 61.0±0.8 40.3±0.7 42.1±0.6
RU 52.1±1.5 55.7±1.7 35.7±0.7 43.6±0.6 33.4±1.4 42.3±0.3
ES 33.5±1.7 41.8±0.5 46.5±2.1 51.7±3.0 44.2±1.2 39.0±1.6
EL 37.8±0.7 41.4±2.7 44.9±2.1 51.4±1.2 23.6±2.2 25.5±0.6
KA 75.1±3.3 80.8±5.0 48.1±2.5 53.9±4.8 37.9±2.4 40.4±3.1
all 52.8±4.5 57.9±6.3 47.5±2.8 53.4±6.0 39.7±4.7 42.9±9.5
Table S3.3. Comparison of the Base and Large sizes of the mode l in the ‘Multilingual Joint’ scenario for
the adapter method
LanguageSub-task 1 Sub-task 2 Sub-task 3
XLMR-Base XLMR-Large XLMR-Base XLMR-Large XLMR-Base XLMR-Large
EN 44.5±1.8 52.8±0.2 53.4±2.5 55.7±2.0 30.6±3.1 37.5±2.9
FR 58.8±2.4 67.5±0.9 46.4±1.2 50.8±3.6 37.2±4.1 45.7±1.9
DE 61.7±2.1 67.2±0.8 55.0±1.1 64.2±1.0 38.3±1.9 53.0±0.7
IT 43.9±3.5 52.0±3.1 56.6±1.7 58.2±1.0 50.4±2.2 58.1±1.6
PL 59.2±1.5 65.2±1.5 61.1±3.0 64.1±1.7 35.2±2.7 41.9±2.7
RU 42.2±1.8 52.8±0.9 37.5±1.2 41.7±2.0 28.9±3.1 39.2±2.6
ES 39.8±2.7 44.2±0.7 40.6±3.3 49.1±2.0 28.3±1.5 36.7±1.3
EL 35.7±2.1 40.9±1.7 44.2±1.4 54.1±2.9 21.6±1.7 25.4±1.5
KA 71.5±2.2 79.2±1.8 47.3±2.4 55.3±1.6 37.1±1.6 42.2±2.4
all 51.5±3.1 58.0±2.0 49.4±5.2 54.8±7.1 34.8±6.6 42.2±9.5
April 9, 2024 e/g

--- PAGE 51 ---
S4 Appendix - Amount of resources per-language
S3 Table shows the amount of resources we considered when identif ying low-resource
languages for our task. For the amount of pre-training data, we s ummarised the
statistics from [35]. We also considered the existence of the data fo r fine-tuning in a
certain language or whether training data is available in a language fro m the same
group (column named ”Language family”). Based on our criteria, Ge orgian language
is a clear outlier with substantially less pre-training data and no trainin g data in that
language.
Table S4. Comparison of the amount of resources per each lang uage in sub-tasks 1, 2, and 3
LanguagePre-training data
(number of tokens)Training data
(number of examples) Language family
sub-task1 sub-task2 sub-task3
EN 55,608 433 433 3,610 West-Germanic
FR 9,780 158 158 1,693 Romance
DE 10,297 132 132 1,251 West-Germanic
IT 4,983 226 227 1,742 Romance
PL 6,490 144 145 1,228 Slavic
RU 23,408 142 143 1,232 Slavic
ES 9,374 0 0 0Romance
EL 4,285 0 0 0Hellenic
KA 469 0 0 0Kartvelian
April 9, 2024 f/g

--- PAGE 52 ---
S5 Appendix - Comparison of Houlsby and Pfeiffer
adapters across three sub-tasks
S4 Table shows the comparison of Housby and Pfeiffer configuratio n of the adapter for
each sub-task in our experiments in a ’Multilingual Joint’ training scen ario for the
FFT method. As can be seen, the differences are not consistent a cross all the
languages, but on average, each sub-task benefits from the Pfe iffer adapter setting.
Table S5. Comparison of the Houlsby and Pfeiffer adapters fo r XLM-RoBERTa Large in the ‘Multilingual
Joint’ scenario
LanguageSub-task 1 Sub-task 2 Sub-task 3
Housby Pfeiffer Houlsby Pfeiffer Houlsby Pfeiffer
EN 43.2±5.252.8±0.254.3±2.355.7±2.036.2±2.337.5±2.9
FR 66.7±1.367.5±0.947.4±4.850.8±3.647.7±1.045.7±1.9
DE 68.1±0.567.2±0.863.5±2.164.2±1.052.6±0.453.0±0.7
IT 50.7±2.452.0±3.159.0±1.158.2±1.055.1±1.358.1±1.6
PL 64.0±3.165.2±1.562.6±0.864.1±1.742.5±0.741.9±2.7
RU 59.7±1.952.8±0.940.0±2.241.7±2.040.5±0.639.2±2.6
ES 47.2±1.744.2±0.750.4±2.149.1±2.038.5±2.236.7±1.3
EL 44.5±3.640.9±1.752.0±2.754.1±2.927.6±0.825.4±1.5
KA 76.1±0.479.2±1.856.5±3.155.3±1.643.0±2.742.2±2.4
all 57.9±5.558.0±2.054.0±8.054.8±7.142.1±8.642.2±9.5
April 9, 2024 g/g
