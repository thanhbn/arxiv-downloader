# 2307.08303.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2307.08303.pdf
# File size: 856809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Soft Prompt Tuning for Augmenting Dense Retrieval with Large
Language Models
Zhiyuan Peng∗
Santa Clara University
Santa Clara, USA
zpeng@scu.eduXuyang Wu∗
Santa Clara University
Santa Clara, USA
xwu5@scu.edu
Qifan Wang
Meta AI
Menlo Park, USA
wqfcr@meta.comYi Fang†
Santa Clara University
Santa Clara, USA
yfang@scu.edu
ABSTRACT
Dense retrieval (DR) converts queries and documents into dense
embeddings and measures the similarity between queries and docu-
ments in vector space. One of the major challenges in DR is the lack
of domain-specific training data. While DR models can learn from
large-scale public datasets like MS MARCO through transfer learn-
ing, evidence shows that not all DR models and domains can benefit
from transfer learning. Recently, researchers have resorted to large
language models (LLMs) to improve the zero-shot and few-shot DR
models. However, the hard prompts or human-written prompts uti-
lized in these works are suboptimal and the generated weak queries
are often sensitive to the prompts. To tackle this, we propose soft
prompt tuning for augmenting DR (SPTAR1): for each task, we
leverage soft prompt-tuning to optimize a task-specific soft prompt
on limited ground truth data and then prompt the LLMs to tag
unlabeled documents with weak queries, yielding weak document-
query pairs to train task-specific dense retrievers. We design a filter
to select high-quality example document-query pairs in the prompt
to further improve the quality of weak tagged queries. To the best
of our knowledge, there is no prior work utilizing soft prompt
tuning to augment DR models. Moreover, unlike much of the ex-
isting work, ours is based on popular open-source LLMs to ensure
reproducible and deterministic results. Our experimental results
demonstrate that SPTAR outperforms both unsupervised baselines
and the recently proposed LLMs-based augmentation method for
DR.
CCS CONCEPTS
•Information systems →Information retrieval ;•Computing
methodologies→Natural language generation .
KEYWORDS
Large Language Models, Dense Retrieval, Prompt Tuning, Data
Augmentation
1 INTRODUCTION
Information retrieval (IR) plays a pivotal role in a wide array of
applications, ranging from prominent web search engines such as
∗Both authors contributed equally to this research.
†Yi Fang is the corresponding author.
1https://github.com/zhiyuanpeng/SPTAR.gitGoogle and Bing to personalized recommendation systems like
Walmart’s product recommendations and Apple Music’s song sug-
gestions. Traditional IR methods, like TF-IDF and BM25 [ 39], are
built on token-level similarity matching, which can sometimes fall
short due to a lexical gap [ 1]. This gap occurs when semantically
similar terms, such as synonyms, are overlooked because of their
lexical differences. This oversight can potentially impact the quality
of search results and the user experience.
Given these constraints, researchers have turned to advance-
ments in deep learning to tackle the lexical gap in conventional
IR. One notable approach is Dense Retrieval (DR), which aims to
capture the overarching semantic essence of content rather than fix-
ating on individual tokens. DR models like dense passage retrieval
(DPR) [ 17] and ColBERT [ 18,41] encode each query or document
into a dense vector, with the dimensionality determined by the
neural networks. In practice, dense retrievers pre-compute docu-
ment embeddings and construct an approximate nearest neighbor
(ANN) index for rapid search. When a new query is introduced,
only its embedding is computed and subsequently processed by the
ANN search system. Unlike TF-IDF and BM25, DR places greater
emphasis on assessing the similarity of the overall semantic context.
While DR methods have made strides in bridging the lexical
gap, they are still constrained by the limited availability of domain-
specific training data, hindering their performance in specialized
domains. Although some researchers have proposed to leverage
transfer learning to mitigate this challenge, studies [ 8,48] indi-
cate that not all DR models and domains can benefit from transfer
learning equally. Recently, LLMs like CPT-3 [ 4], LLaMA [ 49], and Vi-
cuna [ 5] have demonstrated potent zero-shot and few-shot learning.
Rather than fine-tuning the LLMs on task-specific data, prompting
integrates task instructions (e.g., TL;DR translate to English) and a
few relevant examples as input and extracts the answers from the
output of large language model (LLM). The terms “hard promp” and
“soft prompt” refer to different approaches to guiding the LLM’s
behavior during text generation or other tasks. A hard prompt [ 55]
involves using explicitly defined and unchangeable text inputs to
instruct the model. The prompt does not involve additional training
or fine-tuning of the model. On the other hand, a soft prompt in-
volves using trainable vectors or learnable embeddings to guide the
model’s behavior. Unlike hard prompts, soft prompts are not explicit
text instructions but rather embeddings that influence the model’s
output. These embeddings are typically learned through a process
1arXiv:2307.08303v5  [cs.IR]  17 Jun 2024

--- PAGE 2 ---
known as prompt tuning [ 20,21,26,57]. The existing work [ 43,44]
suggested that prompts provide a method for injecting task-specific
guidance, which is beneficial in low-data regimes. Recent research
[42] further quantified this benefit through comprehensive test-
ing of prompts. The results showed that well-crafted prompts can
significantly reduce the dependency on large volumes of training
data across downstream tasks. Both InPars [ 2] and PROMPTAGA-
TOR [ 8] employ hard prompts to guide LLMs in tagging unlabeled
documents with weak queries, subsequently training task-specific
retrievers. Nonetheless, hard prompts come with limitations: a)
Crafting effective hard prompts is challenging and often requires it-
erative human effort, intuition, and sometimes a bit of luck; b) Even
with hand-crafted prompts, the downstream tasks still underper-
form tuned models. For instance, compared with the performance
of fine-tuned T5-XXL [ 37] on SuperGLUE [ 51], GPT-3 175B few-
shot gets a 17.5 points smaller score despite using 16 times more
parameters [ 20]. These limitations of hard prompts underscore
their effectiveness and addressing these challenges draws academic
interest as well as generating industrial value.
Given the limitations of hard prompts, we investigate an alterna-
tive. Rather than utilizing humanly-readable words as hard prompts
[33], the soft prompt [ 20,21,26,57] comprises a set of embeddings
which are unrecognizable to humans and are prepended at the be-
ginning of the neural network input. During the soft prompt tuning,
the parameters of the LLM are frozen, and only the parameters as-
sociated with the soft prompt are updated. While both [ 20] and [ 21]
demonstrate that soft prompts surpass the hard prompts, there is
no work utilizing soft prompt tuning to augment DR. In this paper,
we propose soft prompt tuning for augmenting DR (SPTAR). Specif-
ically, for each task, we leverage soft prompt tuning to optimize
the parameters associated with the soft prompt on limited ground
truth data and then prompt the LLMs to tag unlabeled documents
with weak queries, yielding enough weak document-query pairs
to train task-specific retrievers. Moreover, we find that even with
the optimized soft prompt, the quality of generated weak queries is
sometimes sensitive to the example document-query pairs in the
prompt. Thus, we designed a filter to select high-quality example
document-query pairs in the prompt to further improve the qual-
ity of weakly tagged queries as well as the DR tasks. In addition,
most of the existing work has been built on proprietary models
hidden behind opaque API endpoints, which may produce non-
reproducible or non-deterministic experimental results. Instead,
our work is based on widely used open source LLMs [ 34]. Our main
contributions can be summarized as follows:
•To the best of our knowledge, our work stands as one of
the early attempts of LLMs in combination with soft prompt
tuning for enhancing DR tasks.
•We introduce a soft prompt filter designed to curate document-
query pairs within the prompt, thus enhancing the overall
quality of the generated weak data. Additionally, we design
a BM25 filter to reduce noise in the generated data, further
improving performance.
•We conduct a comprehensive set of experiments involving
four datasets and seven retrievers and re-rankers, demon-
strating the generality and superior performance of our ap-
proach over several state-of-the-art baselines.•Experiments are based on the recent open-source LLMs to
ensure reproducible and deterministic experimental results.
All code and data are publicly available2.
2 RELATED WORK
2.1 Dense Retrieval
DR converts the queries and documents into dense vectors on which
the ANN index can be built for fast search. DPR [ 17] employs a
two-tower structure: one BERT model for queries and another for
documents. For each query with one positive document and several
negative documents, DPR measures the similarity between query
embedding and document embeddings and then maximizes the
log-likelihood of the positive passage. A variant of DPR is to utilize
one BERT by concatenating query and document as input and ex-
tracting the query embedding and document embedding after the
encoding. The query encoder and document encoder of ColBERT
[18] [41] share the same BERT but utilize a different special token
following the “[CLS]” to distinguish query and document. Unlike
DPR directly measures the similarity between query embedding
and document embeddings, ColBERT introduces a late interaction
mechanism. Specifically, for each token in the query, ColBERT com-
putes its similarity with all the tokens in the document and applies
a maximum pooling on these similarity scores. The similarity score
of a pair of query and document is the summarization of all the
scores after the maximum pooling. Given a query with one positive
document and one negative document, ColBERT is optimized by
the pairwise softmax cross-entropy loss over the computed scores
of the positive and negative documents. ANCE [ 56] is a bi-encoder
trained on (query, positive document, negative document) tuples
where the negative document is retrieved from an ANN built on
the checkpoint of the last step. TAS-B [ 11] groups queries by their
embedding similarities and employs a training data sampling tech-
nique coupled with dual-teacher supervision distillation. Contriever
[13] trains a bi-encoder model through contrastive learning. Instead
of training the model on the labeled dataset, Contriever generates
positive query-document pairs from unlabeled corpus by two strate-
gies “inverse cloze tasks” and “independent cropping”. ReContriever
[19] adopts the same method as Contriever to generate the weak
query-document pairs, but ReContriever scores the weak query-
document pairs by itself during the training and the loss is weighted
by the weights. BM25CE [ 52] is a re-ranking-based DR. BM25CE
first applies BM25 to retrieve documents and then employs the
trained crossed-encoder to re-rank the retrieved documents. Our
contribution is not to propose new dense retrievers but to propose
a novel method to augment the existing dense retrievers.
2.2 Data Augmentation for Dense Retrieval
For DR datasets, usually, only a fraction of documents are labeled
with queries, for instance, MS MARCO [ 31], a widely used dataset
in DR, has a corpus of 8841823 documents but only has 532761 train-
ing document-query pairs. Given DR demands substantial train-
ing data to achieve quality dense embeddings, some researchers
have turned to data augmentation to generate more document-
query pairs to train better dense embeddings. InPars [ 2] feeds
2https://github.com/zhiyuanpeng/SPTAR.git
2

--- PAGE 3 ---
a task-specific human-written prompt and 3 example document-
query pairs to a 6B GPT-3 [ 4] model Curie to generate 100K weak
document-query pairs and selects the top 10K queries with respect
to the probability of query 𝑞to augment the training data. InPars
[2] employs the same dense retrieval model proposed in [ 32], which
treats the retrieval as a sequence-to-sequence task by concatenating
a query and a document as input to T5 mode and outputs the rele-
vance score. Improved variations of InPars [ 2], such as InPars-v2
[15] and InPars-Light [ 3], have been introduced to enhance the
original methodology. Like InPars [ 2], PROMPTAGATOR [ 8] also
feeds a task-specific human-written prompt and at most 8 example
document-query pairs to LLM to generate weak data. Instead of
selecting the top weak queries by their probabilities, PROMPTA-
GATOR first trains a filter on uncleaned document-query pairs to
filter the weak queries by dropping the weak queries that cannot
retrieve their paired documents in the Top- 𝑘retrieved documents.
By repeating this process multiple times, the filter significantly
improves the performance of a dual-encoder DPR retriever. Besides,
PROMPTAGATOR [ 8] utilizes a much bigger LLM: a 175B model
Flan [ 54] which cannot be accessed by most researchers. DAR [ 14]
argues that the method that generates queries from unlabeled docu-
ments is costly as well as does not add variations to the documents.
To do data augmentation efficiently, DAR [ 14] not only interpo-
lates two different document representations associated with the
labeled query but also stochastically perturbs the representations
of labeled documents in embedding space. RocketQA [ 36] applies a
pre-trained cross-encoder retriever to retrieve positive and negative
documents for a new collection of queries with high confidence
scores. RocketQAv2 [ 38] augments the DR by jointly optimizing
the bi-encoder structure DR and cross-encoder structure reranking
model to have similar output distributions. DRAGON [ 25] fuses
multiple teacher models by progressively training the base DR
model.
2.3 LLMs in Dense Retrieval
Most of the current literature in this domain explores the poten-
tial of LLMs to improve DR tasks through various data generation
techniques, including query generation [ 2,3,8,9,15,40], relevance
generation [ 22], and permutation generation [ 27,35,45]. PROMP-
TAGATOR [ 8] and InPars [ 2] with its variations InPars-v2 [ 15] and
InPars-Light [ 3] are illustrated in section 2.2. UPR [ 40] utilizes LLM
as a zero-shot reranker to re-rank the passages retrieved by retriev-
ers like BM25 and DPR. Given a query, for each retrieved passage,
UPR utilizes a prompt “ Please write a question based on this passage ”
to prompt a LLM and computes the average log-likelihood of the
question tokens conditioned on the input document as the relevance
score. Due to the intensive computational resources required to
train LLMs, all these works utilize LLMs as query generators instead
of fine-tuning them. HyDE [ 9] leverages LLMs to augment queries
by generating hypothetical documents, effectively capturing rele-
vance patterns for unsupervised retrieval. LRL [ 27] trains a listwise
zero-shot re-ranker that leverages LLMs without task-specific su-
pervised training. Unlike pointwise re-rankers, LRL considers all
candidate documents to determine their relative ranking positions.
Another approach involves instructional permutation generation
[45], where the focus is on instructing LLMs to directly outputpermutations of passages. Permutation distillation techniques are
employed to transfer the passage ranking capabilities of ChatGPT
into a smaller, specialized ranking model. While these works uti-
lize LLMs as query generators without fine-tuning, our SPTAR
approach takes a different approach. We first perform soft prompt
tuning to optimize task-specific soft prompts and then employ data
filtering to enhance the quality of the generated weak data.
2.4 Prompt Tuning
Prompt tuning offers a promising avenue for adapting pre-trained
LLMs to specific tasks by focusing on tuning the prompt module
instead of fine-tuning the entire model [ 46]. Prefix-Tuning [ 21] in-
troduces a prompt module with learnable parameters 𝜃outputting
embeddings which are prepended to the embeddings of other in-
putted tokens. This approach preserves the original training ob-
jective intact while updating only the prefix parameters 𝜃through
gradient descent for each task. Another similar technique, referred
to as “ gisting ” [30], compresses arbitrary prompts into a condensed
set of virtual “ gist” tokens using a meta-learning approach. Building
upon T5 [37], Lester et al. [20] propose a method where the learn-
able embeddings of a task-specific prompt are prepended to the
encoder’s output. The concatenated embeddings are then passed
through the decoder to compute the training objective. This ap-
proach enables the model to incorporate task-specific information
into the decoding process. Zhou et al . [59] introduce Dual Context-
guided Continuous Prompt (DCCP), which employs soft prompt
tuning using dual inputs: context-aware prompt and label-aware
context representations. This approach leverages both prompt in-
formation and contextual understanding to enhance the model’s
performance. Prompt tuning can benefit from multi-task learning.
For instance, ATTEMPT proposed by Wang et al . [53] introduces a
multi-task tuning method that transfers knowledge across different
tasks through a mixture of soft prompts. In the context of Multilin-
gual Information Retrieval, Huang et al . [12] explore a soft prompt
decoding approach that treats retrieval in each language as a sepa-
rate task while jointly modeling them to capture shared underlying
structures. They use decomposable prompts in KD-SPD to model
languages, highlighting that languages share common features
and concepts despite their unique properties. Regarding IR tasks,
DPTDR by Tang et al. [47] employs a dual-encoder, two RoBERTa
models, for retrieval. It initializes the dual-encoder through con-
trastive learning and appends learnable soft prompts for query and
document. Both the dual-encoder and the learnable prompts are
updated during the training process.
In contrast, unlike the current DR data augmentation works that
only prompt LLMs to generate weak queries for unlabeled docu-
ments with a few labeled document-query pairs as examples, we
propose to learn task-specific soft prompts on a small proportion of
the labeled data and a novel soft prompt filter method to select high-
quality example document-query pairs in the prompt to improve
the DR tasks further. The whole augmentation pipeline makes our
approach different from the current works.
3

--- PAGE 4 ---
Notation Definition
𝑞,𝑑 query, document
𝐷𝑡𝑟𝑎𝑖𝑛 collection of query-document pairs for training
𝐷𝑡𝑒𝑠𝑡 collection of query-document pairs for testing
𝐷𝑒𝑣𝑎𝑙 collection of query-document pairs for evaluation
𝐶 collection of all the documents
𝐶𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 unlabeled documents in 𝐶
Φ freezed original parameters of large language model
𝑆𝑋
𝑡𝑟𝑎𝑖𝑛collection of 𝑋sampled queries associated with corresponding documents from 𝐷𝑡𝑟𝑎𝑖𝑛
𝑓𝜃(·) prompt’s embedding layer parametrized by 𝜃
𝑓𝜃(𝑠) soft prompt initialized based on a hard prompt 𝑠with a length of 𝑙𝑠
(𝑑𝑚,𝑞𝑚)𝑀
𝑚=1collection of 𝑀sampled document-query pairs from 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛, as examples in prompt
𝑐𝑗 concatenation of(𝑑𝑗,𝑞𝑗)∈(𝑑𝑗,𝑞𝑗)𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)−𝑀
𝑗=1and all the example pairs (𝑑𝑚,𝑞𝑚)𝑀
𝑚=1
(𝑑𝑗,𝑞𝑗)𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)−𝑀
𝑗=1for each epoch, loss is computed on 𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)−𝑀document-query pairs from 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛
𝑝𝜃,Φ(𝑞𝑗|𝑡𝑗) the probability of 𝑞𝑗conditioned on 𝑡𝑗given parameters 𝜃andΦ
ℎ𝑗,𝑖 output hidden vector of 𝑖𝑡ℎtime step for 𝑗𝑡ℎinstance
𝑧𝑗,𝑖 ID of𝑖𝑡ℎtoken of𝑗𝑡ℎinstance
𝐿 loss function
𝑆𝑌
𝑒𝑣𝑎𝑙collection of 𝑌queries associated with corresponding documents from 𝐷𝑒𝑣𝑎𝑙
𝑊𝑙𝑎𝑟𝑔𝑒 collection of 100K sampled documents from 𝐶𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 and each document has a generated weak
query
𝑊𝑠𝑚𝑎𝑙𝑙 collection of 5000 sampled documents 𝑊𝑙𝑎𝑟𝑔𝑒
𝐹𝑘(·) top𝑘weak data filter function
𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒) 𝑊𝑙𝑎𝑟𝑔𝑒 filtered by weak data filter 𝐹𝑘
Table 1: Summary of notation.
Data
PreparationSoft Prompt
TuningSoft Prompt
Filter
Soft Prompt
AugmentorWeak Data
FilterDense
Retrieval
Figure 1: The pipeline of the proposed Soft Prompt Tuning
for Augmenting dense Retrieval (SPTAR).
3SOFT PROMPT TUNING FOR AUGMENTING
DENSE RETRIEVAL
As shown in Figure 1, SPTAR comprises six modules: a) data prepa-
ration; b) soft prompt tuning; c) soft prompt filter; d) soft prompt
augmentor; e) weak data filter; f) DR. In Section 3.1, we elaborate on
how to generate the training and evaluation datasets of soft prompt
tuning. With the training and evaluation datasets, we conduct soft
prompt tuning (Section 3.2) to learn a task-specific soft prompt. To
further improve the quality of the weak generated queries, we in-
troduce the soft prompt filter (Section 3.3) which identifies optimal
example document-query pairs to optimize the task-specific prompt.
We then prompt LLMs to generate weak queries for unlabeled doc-
uments (Section 3.5), yielding enough training data to train DR.
Finally, we train the DR (Section 3.6) models on filtered weak data
(Section 3.4). The notations used in this paper are provided in Table
1.3.1 Data Preparation
We study the augmentation of DR using limited data. The initial
step involves sampling a small dataset on which we fine-tune a
task-specific soft prompt. We define dataset 𝐷as𝐷={(𝑞𝑛,𝑑𝑛)}𝑁
𝑛=1
where for each query 𝑞𝑛, there is a relevant document 𝑑𝑛. There may
exist duplicated queries as one query may have multiple relevant
documents. This domain-specific dataset 𝐷is categorized into train,
test, and evaluation subsets, denoted as 𝐷𝑡𝑟𝑎𝑖𝑛 ,𝐷𝑡𝑒𝑠𝑡, and𝐷𝑒𝑣𝑎𝑙,
respectively. Apart from dataset 𝐷, there is a much bigger document
collection𝐶which contains all the documents in 𝐷but has more
unlabeled documents denoted as 𝐶𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 . After training, DR
encodes all the documents in 𝐶into vectors. When a new query
comes in, DR encodes the query into a vector and searches the
top-𝑘similar documents in vector space.
We randomly sample document-query pairs from the original
training dataset 𝐷𝑡𝑟𝑎𝑖𝑛 to construct the training and evaluation
datasets for the soft prompt module, namely 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛and𝑆𝑌
𝑒𝑣𝑎𝑙where
indices X and Y signify the number of distinct queries within
the training and evaluation datasets respectively. The function
𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑥)designates the quantity of document-query pairs given
𝑥distinct queries in the dataset. Since each query may have more
than one positive document, 𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑥)may be bigger than |𝑥|.
Hence,𝑆𝑋
𝑡𝑟𝑎𝑖𝑛contains𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)document-query pairs, simi-
larly,𝑆𝑌
𝑒𝑣𝑎𝑙comprises𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑌)document-query pairs. For il-
lustration, in our experiment, we draw 50 unique queries and their
corresponding documents from the training dataset 𝑆𝑡𝑟𝑎𝑖𝑛 to form
4

--- PAGE 5 ---
Tokenizer
initialized Prompt  with length D-Q PairsPrompt's Embedding Layer LLM's Embedding LayerLearnable Parameters 
Freeze Parameters 
...Document () : The function of
a moto r neuron is to carry an
electrical signal to a muscle,
triggering it to either contract or
relax.
Relevant Query () : what is the
structure and function of a motor
neuron?
Document (): China likely
biggest inves tor in Indonesia.
Investment Coordinating Board
(BKPM) has recorded Singapore
as the largest contributor of
investment in Indonesia with
US$5.9 billion, a 20 percent share
of total foreign investment in 2015.Weak Relevant Query  () : what
country invests the most in
indonesia
Weak Query Generation 
Doc to be LabeledTrue Query  () : which country invests the most in indonesia
Initialized Prompt
D-Q Pairs...
Doc to be Labeled
Initialized Prompt
Filtered D-Q Pairs...
Doc to be Labeled(c) Soft Prompt Filter (d) Soft Prompt Augmentor(b) Soft Prompt Tuning (a) Shared LLMData (Section 3.1)
LLM
please generate query for this documentFigure 2: The main architecture of the proposed SPTAR: a) The same LLM is shared by soft prompt tuning module, soft prompt
filter module and soft prompt augmentor module; b) soft prompt tuning module fixs the LLM’s original parameters Φand
only fine-tune the parameters of soft prompt’s embedding layer 𝜃on the sampled small dataset (Section 3.1); c) soft prompt
filter module fixs the learned parameters 𝜃∗, and for each group of sampled example document-query pairs, computes the loss
on evaluation dataset. The group of example document-query pairs with the smallest loss will be utilized in the soft prompt
augmentor module; d) with the learned parameters 𝜃∗and a group of filtered example document-query pairs, the soft prompt
augmentor module iterates over the unlabeled document dataset 𝐷𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 to generate weak queries.
𝑆50
𝑡𝑟𝑎𝑖𝑛(𝑋=50). From the remaining data in 𝑆𝑡𝑟𝑎𝑖𝑛 , we randomly se-
lect 100 unique queries and their associated documents to compose
𝑆100
𝑒𝑣𝑎𝑙(𝑌=100).𝑆50
𝑡𝑟𝑎𝑖𝑛serves for optimizing the soft prompt, while
𝑆100
𝑒𝑣𝑎𝑙is employed to assess the model’s convergence, enabling us to
terminate the training process in advance and mitigate overfitting
risks. We also tried other values of 𝑋, and the influence of 𝑋is
studied in Section 5.2.5.
3.2 Soft Prompt Tuning
Soft prompts [ 10,58] introduce a novel technique to steer a model’s
behavior without the need for extensive fine-tuning. Unlike hard
prompts, which are human-readable instructions, soft prompts com-
prise trained embeddings optimized for specific tasks. The soft
prompt tuning module learns a task-specific soft prompt on a small
proportion of labeled data. Figure 2 (b) illustrates the structure of
the soft prompt tuning module, where the red boxes represent the
parameters𝜃to be optimized during model training and the green
boxes represent LLM’s original parameters Φthat are retained dur-
ing the training. 𝑠represents the initialized hard prompt with size
𝑙𝑠, like repeating "please generate query for document" until the
length of𝑠equals𝑙𝑠. Let𝑓𝜃(·)denote the prompt’s embedding layer
implemented by an embedding matrix initialized as the embeddings
of𝑠encoded by LLM’s original embedding layer. 𝑓𝜃(𝑠)represents
the soft prompt.
For each training epoch, we first randomly sample 𝑀document-
query pairs from training dataset 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛as example document-
query pairs(𝑑𝑚,𝑞𝑚)𝑀
𝑚=1, then iterate over the left document-query
pairs(𝑑𝑗,𝑞𝑗)𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)−𝑀
𝑗=1to compute loss. Example pairs (𝑑𝑚,𝑞𝑚)𝑀
𝑚=1
are concatenated with each pair (𝑑𝑗,𝑞𝑗)by keywords like “ docu-
ment ” and “ query ” as𝑐𝑗. Finally, we concatenate 𝑠with𝑐𝑗as onetraining instance 𝑡𝑗=[𝑠;𝑐𝑗]and there are 𝑁𝑢𝑚𝑃𝑎𝑖𝑟(𝑋)−𝑀in-
stances in each epoch. When 𝑡𝑗is inputted into the soft prompt
tuning module, it is first tokenized into a list of IDs 𝑧𝑗indexed by𝑖
then the embeddings of IDs are extracted and fed into the following
layers to compute the hidden vectors. 𝑓𝜃(·)takes the IDs of 𝑠as
inputs and outputs its embeddings while the embeddings of 𝑐𝑗are
generated by LLM’s original embedding layer. For simplicity, we
postulate that each token in 𝑡𝑗has one corresponding ID in 𝑧𝑗. For
training instance 𝑡𝑗, the hidden vector of 𝑖𝑡ℎtime step is defined
asℎ𝑗,𝑖∈R𝑑whereℎ𝑗,𝑖=h
ℎ(1)
𝑗,𝑖;···;ℎ(𝑘)
𝑗,𝑖i
and𝑘is the number of
layers in LLM. The objective function for training is given by:
max
𝜃log𝑝𝜃,𝜙(𝑞𝑗|𝑡𝑗)=max
𝜃∑︁
𝑖∈𝑖𝑑𝑥𝑞𝑗log𝑝𝜃,𝜙 𝑧𝑗,𝑖|ℎ𝑗,<𝑖(1)
where𝑖𝑑𝑥𝑞𝑗denotes the indexes corresponding to the IDs of 𝑑𝑗.
Additionally, 𝑝𝜃,𝜙 𝑧𝑗,𝑖|ℎ𝑗,<𝑖signifies the probability of the sub-
sequent token with ID 𝑧𝑗,𝑖. For loss function 𝐿, we employ the
negative log-likelihood defined as:
𝐿=−log𝑝𝜃,𝜙(𝑞𝑗|𝑡𝑗) (2)
We implemented our soft prompt tuning module based on a public
prompt tuning package PEFT [29].
3.3 Soft Prompt Filter
During the development of the soft prompt module, we observe
that the choice of example document-query pairs (𝑑𝑚,𝑞𝑚)𝑀
𝑚=1pro-
foundly affects the quality of text generation. Therefore, upon com-
pleting the soft prompt training, with the learned parameters 𝜃∗,
we try to select the best group of document-query pairs from 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛
as example document-query pairs in soft prompt augmentor. For
𝑀=2, there are 1225 ( 50∗49/2) groups of example pairs, which
5

--- PAGE 6 ---
makes it impractical to evaluate all. To reduce the computation
complexity, we randomly sample 𝑋groups of example pairs from
𝑆𝑋
𝑡𝑟𝑎𝑖𝑛to evaluate them on the evaluation dataset 𝑆𝑌
𝑒𝑣𝑎𝑙and the
group of example pairs with the best evaluation metric will be cho-
sen as the example pairs in soft prompt augmentor. As shown in
Figure 2 (c), the only difference between soft prompt tuning and
soft prompt filter is the dataset where the 𝑑𝑗comes from. Suppose
we sampled 𝑋groups of document-query pairs each of which has
𝑀document-query pairs (𝑑𝑚,𝑞𝑚)𝑀
𝑚=1. Evaluation dataset 𝑆𝑌
𝑒𝑣𝑎𝑙has
𝑁𝑢𝑚(𝑌)document-query pairs and example pairs (𝑑𝑚,𝑞𝑚)𝑀
𝑚=1are
concatenated with each pair (𝑑𝑗,𝑞𝑗)by keywords like “ document ”
and “ query ” as𝑐𝑗. Then,𝑐𝑗is concatenated with the initialized
prompt𝑠as𝑡𝑗=[𝑠,𝑐𝑗]. The evaluation metric is the same as the
loss function 𝐿(Equation 2). We study the effectiveness of soft
prompt filter in Section 5.2.3 and the filtered example document-
query pairs are documented in Appendix A and B.
3.4 Weak Data Filter
Both InPars [ 2] and PROPAGATE [ 8] emphasize the importance of
filtering weak document-query pairs as the generated weak queries
are not guaranteed to be always relevant to the input documents.
Employing the methodology from InPars [ 2], we clean the weak
data. Upon acquiring these generated weak pairs (Section 3.5), we
apply a BM25-based filtering: For each weak query, BM25 retrieves
the top𝑘documents from the corpus 𝐶. If the document linked
to a weak query isn’t among the top 𝑘results, the pair gets dis-
carded. This filtering approach is denoted as 𝐹𝑘(·). For datasets
MS MARCO and FiQA-2018, we experimented with different top
𝑘values from the set {10,30,50,70}and reported the best results.
The effectiveness of this weak data filter module is discussed in
Section 5.2.4.
3.5 Soft Prompt Augmentor
Generating high-quality queries for unlabeled documents remains
a formidable challenge. Our soft prompt augmentor module har-
nesses both the potency of the learned soft prompts and the context
offered by the best example document-query pairs, providing a
synergistic effect that ensures not just relevance, but also the su-
perior quality of the generated queries. As shown in Figure 2 (d),
with the learned parameters 𝜃∗and the filtered group of example
document-query pairs, soft prompt augmentor generates a weak
query for an unlabeled document 𝑑𝑗sampled from 𝐷𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 . In
this paper, for each dataset, we first created two weak datasets: a)
𝑊𝑙𝑎𝑟𝑔𝑒 .100𝐾unlabled documents are sampled from 𝐷𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 to
generate their weak queries. If the number of unlabeled documents
in𝐷𝑢𝑛𝑙𝑎𝑏𝑒𝑙𝑒𝑑 is smaller than 100𝐾, all the unlabeled documents are
utilized to generate weak queries; b) 𝑊𝑠𝑚𝑎𝑙𝑙 . 5000 document-query
pairs are sampled from 𝑊𝑙𝑎𝑟𝑔𝑒 . Then, we filtered 𝑊𝑙𝑎𝑟𝑔𝑒 and𝑊𝑠𝑚𝑎𝑙𝑙
by weak data filter, described in Section 3.4, to get 𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)and
𝐹𝑘(𝑊𝑠𝑚𝑎𝑙𝑙). During the weak query generation process, LLM not
only utilizes the soft prompt embeddings to capture domain-specific
information but also benefits from the supplementary context pro-
vided by the best example document-query pairs.3.6 Dense Retrieval
DR serves as the concluding step in our methodology, where we
harness the capabilities of neural networks to retrieve relevant
documents. We conducted the experiments on five popular dense
retrievers: DPR, ColBERT, TAS-B, Contriever, and ReContriever.
The descriptions of the models can be found in Section 2.1. For TAS-
B, we used pre-trained bi-encoder model3for clustering queries
and cross-encoder model4and ColBERTv25for teacher mod-
els. For Contriever, we refer to the officially released checkpoint
6as𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 and used it for initial evaluations. Addition-
ally, we fine-tuned 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 as a bi-encoder dense retrieval
model, denoted 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒. Similary, for ReContriever, we have
𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒7and𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒. Further, we incorporated
the cross-encoder model BM25CE, as established in previous litera-
ture [ 52], which re-ranks the top 1000 items retrieved by BM25. Like
BM25CE, we employed the DPR model as a bi-encoder to re-rank
the top 1000 items retrieved by BM25, referring to this method as
BM25BE.
4 EXPERIMENTAL SETUP
4.1 Datasets
Experiments were performed on four datasets MS MARCO [ 31] and
FiQA-2018 [ 28], sourced from BEIR [ 48] and DL2019 [ 7], DL2020
[6]. The description of the four datasets can be found in Table 2.
We follow BEIR [ 48] to report the metrics on the evaluation dataset
instead of test data for MS MARCO, so, for MS MARCO, 𝐷𝑡𝑒𝑠𝑡is
the same as 𝐷𝑒𝑣𝑎𝑙.
As shown in Table 3: a) BM25, 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 and𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒
are evaluated on the original testing split 𝐷𝑡𝑒𝑠𝑡; b) W/O Aug mod-
els are trained on datasets 𝑆50
𝑡𝑟𝑎𝑖𝑛and𝑆100
𝑒𝑣𝑎𝑙utilized to fine-tune
the soft prompt; c) InPars [ 2] models are trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛and
𝑆100
𝑒𝑣𝑎𝑙plus𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)(𝑊𝑙𝑎𝑟𝑔𝑒 filtered by𝐹𝑘, Section 3.4) generated
by human-written prompts. d) SPTAR’s soft prompt tuning mod-
ule (SPTAR-Tuning) is trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛and evaluated on 𝑆100
𝑒𝑣𝑎𝑙;
SPTAR’s DR models (SPTAR-DR) are trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛and𝑆100
𝑒𝑣𝑎𝑙
plus𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)(𝑊𝑙𝑎𝑟𝑔𝑒 filtered by𝐹𝑘, Section 3.4) generated by
soft prompt augmentor (Section 3.5); e) W/O Aug, InPars [ 2] and
SPTAR are all evaluated and tested on the same splits for a fair
comparison; f) For 𝐹𝑘(·), we tried𝑘∈(10,30,50,70)and selected
the𝑘with the best NDCG@10 score on evaluation dataset.
4.2 Training Details
To train the soft prompt module, we performed fine-tuning using
two open-source LLMs: LLaMA-7B and Vicuna-7B. The specific
training hyper-parameters are documented in Table 4.
3https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5
4https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2
5https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz
6https://huggingface.co/facebook/contriever
7https://huggingface.co/Yibin-Lei/ReContriever
6

--- PAGE 7 ---
Task Domain DatasetTrain Eval Test Avg. Word Lengths
#Pairs #Query #Query #Corpus Avg. D/Q Query Document
Passage Retrieval Misc. MS MARCO [31] 532,761 N/A 6,980 8,841,823 1.1 5.96 55.98
Passage Retrieval Misc. DL2019 [7] 532,761 N/A 43 8,841,823 215.3 5.96 55.98
Passage Retrieval Misc. DL2020 [6] 532,761 N/A 54 8,841,823 210.9 5.96 55.98
Question Answering Finance FiQA-2018 [28] 14,166 500 648 57,638 2.6 10.77 132.32
Table 2: Statistics of datasets in BEIR benchmark. Avg. D/Q indicates the average number of relevant documents per query.
Model Train Eval Test
BM25 N/A N/A 𝐷𝑡𝑒𝑠𝑡
𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 N/A N/A 𝐷𝑡𝑒𝑠𝑡
𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 N/A N/A 𝐷𝑡𝑒𝑠𝑡
W/O Aug 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙𝐷𝑒𝑣𝑎𝑙𝐷𝑡𝑒𝑠𝑡
InPars 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)𝐷𝑒𝑣𝑎𝑙𝐷𝑡𝑒𝑠𝑡
SPTAR-Tuning 𝑆50
𝑡𝑟𝑎𝑖𝑛𝑆100
𝑒𝑣𝑎𝑙N/A
SPTAR-DR 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)𝐷𝑒𝑣𝑎𝑙𝐷𝑡𝑒𝑠𝑡
Table 3: Dataset partition for different methods.
Hyperparameters LLaMA-7B Vicuna-7B
Batch Size 4 2
Max Length 1024 1024
Learning Rate 3𝑒−2 3 𝑒−2
Optimizer AdamW AdamW
Early Stop 5 5
Max epochs 100 100
GPU 1 A100 (80G) 1 A100 (80G)
Table 4: Hyperparameters of soft prompt tuning
The training hyper-parameters of dense retrievers are in Table
5. For ColBERT, there is no early stop in the official code and we
saved a checkpoint after each epoch. After training, we manually
evaluated some checkpoints (3, 5, 10, 15, 18, 20) and reported the
testing results of the checkpoint with the highest NDCG@10 score.
4.3 Evaluation Metrics
In the context of text generation models, Perplexity is a commonly
employed metric that quantifies the level of uncertainty exhibited
by a language model when generating new tokens. This metric is
defined as the exponentiated average negative log-likelihood of a
sequence, and a lower perplexity value indicates a higher-quality
language model. Perplexity is used to evaluate the soft prompt
tuning and soft prompt filter modules.
In our evaluation of DR models, we adhere to the metrics estab-
lished in previous studies [ 16,24]. For the MS MARCO and FiQA-
2018 datasets, we utilize Mean Reciprocal Rank at 10 (MRR@10)
and Recall@100. For the DL2019 and DL2020 datasets, we employ
Mean Average Precision (MAP), Normalized Discounted Cumula-
tive Gain at 10 (nDCG@10), and Recall@100. For the BM25CE andBM25BE models, which both re-rank the top 1000 items retrieved by
BM25, Recall@100 is used to specifically evaluate their re-ranking
effectiveness. These metrics together provide a comprehensive as-
sessment of how augmented queries influence the performance of
DR models.
4.4 Baseline Methods
Our study incorporates five baseline methods: BM25, 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒,
𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒, Without Augmentation (W/O Aug), and InPars
[2] (Section 2.3). The training, evaluation, and testing datasets are
documented in Section 4.1. For BM25 [ 39], we use Anserini [ 23]
with the default Lucene parameters ( 𝑘=0.9and𝑏=0.4). TAS-
B,𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 , and𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 are described in Section 3.6. The
differences between InPars [ 2] and SPTAR are twofold: a) InPars
[2] utilizes the human-written prompt while SPTAR utilizes an
optimized soft prompt; b) SPTAR has a soft prompt filter module to
select example document-query pairs. To make it a fair comparison
with InPars [ 2], we choose the same example document-query pairs
in the prompt of SPTAR for InPars [ 2] and utilize InPars’ original
human-written prompt to prompt the LLaMA and Vicuna to obtain
weak document-query pairs. We find for InPars’ human-written
prompt, the quality of generated weak document-query pairs of
Vicuna is much better than that of LLaMA, so, for InPars [ 2], we
choose Vicuna as the weak data generator. For SPTAR, LLaMA is
better than Vicuna and we choose LLaMA for SPTAR.
4.5 Research Questions
An extensive set of experiments was designed to address the fol-
lowing research questions:
RQ1 : Can the proposed SPTAR framework achieve improved
performance on DR tasks over the baseline models? (Section 5.1)
RQ2 : During the soft prompt tuning process, does the soft
prompt tuning module indeed distill the knowledge from the dataset
to the learned soft prompt? What factors contribute to the learned
soft prompts? (Section 5.2.1)
RQ3 : What are the costs of the soft prompt tuning module? Does
the soft prompt tuning module greatly increase the training time
and computational resources? (Section 5.2.2)
RQ4 : What specific role does the soft prompt filter play in SP-
TAR? (Section 5.2.3)
RQ5 : Can the weak data filter further improve the performances
of DR models? (Section 5.2.4)
RQ6 : For SPTAR’s soft prompt tuning module, what is the in-
fluence of the size of training data 𝑋? Is a larger 𝑋better than a
smaller one? (Section 5.2.5)
7

--- PAGE 8 ---
Hyperparameters DPR ColBERT 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒 BM25CE TAS-B
Batch Size 32 32 32 32 96 40
Max Length 350 350 350 350 512 300
Learning Rate 2𝑒−5 2 𝑒−5 2 𝑒−5 2 𝑒−5 2 𝑒−5 2 𝑒−5
DDP No Yes No No No No
Optimizer AdamW AdamW AdamW AdamW AdamW AdamW
Early Stop 10 None 10 10 10 10
Max epochs 20 20 20 20 20 20
GPU 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G) 4 A100s (40G)
Table 5: Hyperparameters of DR Models
RQ7 : For SPTAR’s soft prompt augmentor module, what is the
influence of the number of example document-query pairs 𝑀? Is a
larger𝑀better than a smaller one? (Section 5.2.6)
5 EXPERIMENTAL RESULTS
5.1 SPTAR vs Baseline Models (RQ1)
We conducted evaluations of our SPTAR method on MS MARCO
and its related datasets, including DL2019, DL2020, and FiQA 2018.
As presented in Table 6, the SPTAR data augmentation method
consistently outperforms established baselines, including W/O Aug
and InPars, across a spectrum of widely used datasets and key
retrieval metrics only except the R@100 of SPTAR’s ColBERT which
is slightly outdone by InPars. TAS-B method gets the most best
results (4 out of 10, there are 10 metrics in each row of Table 6) as it
is distilled from two teacher dense retrieval models (cross-encoder
and ColBERT) that are both trained on the full MS MARCO dataset.
For all 7 retrievers, 55 out of 70 improvements are statistically
significant, with p-values < 0.05. ColBERT takes 8 out of 15 un-
significant improvements.
The token-level matching complexity of ColBERT between queries
and documents could explain why SPTAR’s enhancements on Col-
BERT did not significantly surpass the InPars baseline on MS MARCO
related datasets. DPR’s simple architecture makes it have better
generalization ability than ColBERT, which is evidenced by the
fact that on MS MARCO related datasets, DPR’ W/O Aug is better
than ColBERT’s W/O Aug. Also, there exists noise in the gener-
ated queries, especially when the soft prompt is learned from a
small dataset (same as the dataset of W/O Aug), and ColBERT’s
token-level interaction mechanism is more sensitive than DPR.
Regarding the re-ranking models, BM25CE and BM25BE, both
re-rank the top 1000 items initially retrieved by BM25. BM25CE
employs a cross-encoder for re-ranking, whereas BM25BE inte-
grates a DPR model. Without data augmentation, BM25BE beats
BM25CE on 7 out of 10 metrics when trained on a small labeled
dataset (W/O Aug). The cross-encoder model of BM25CE, requir-
ing more extensive data to effectively learn complex interactions
and mitigate overfitting risks, ultimately demonstrated superior
performance under SPTAR over BM25BE on 6 out of 10 metrics.
The officially released checkpoint of 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 is loaded
to be evaluated, and we further improved its performance by fine-
tuning it as a bi-encoder model denoted as 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒in Table 6.We found performance improvement even fine-tuning 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒
on a small dataset as Contriever’s W/O Aug is better than 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒
on all four datasets. Our method, 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒’s SPTAR, can further
improve𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 over the second-best value significantly.
𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒’s InPars only beats W/O Aug on 4 out of 10 metrics.
Similarly, we evaluated the Re 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 and𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒
and the same patterns as Contriever are observed as well.
By harnessing the benefits of soft prompt tuning and LLMs, our
model generates high-quality weak queries that greatly enhance DR
tasks. Moreover, the consistent improvements observed across DPR,
TAS-B, BM25BE, BM25CE, Contriever, and ReContriever substan-
tiate the general applicability of our approach, extending beyond
specific dense retrievers. It is worth noting that in the absence of
augmentation data, all dense retrievers except for the Contriever
and ReContriever perform worse than the unsupervised model
BM25. This underscores the significant reliance of DR on domain-
specific labeled data and highlights the limitations of directly train-
ing dense retrievers in scenarios with limited ground-truth data,
where the expected performance may not be attainable.
5.2 Ablation Study
In this section, our primary objective is to evaluate the distinct
contributions of each module to the overall efficacy of the SPTAR
framework. Our experiments focus on evaluating the perplexity
and NDCG@10 metrics. The perplexity metric, derived from the
𝑆100
𝑒𝑣𝑎𝑙dataset, provided insights into the model’s text generation
quality. The default NDCG@10 scores in this section are obtained
by evaluating the SPTAR-DPR model trained, evaluated, and tested
on𝑆50
𝑡𝑟𝑎𝑖𝑙+𝑆100𝑒𝑣𝑎+𝑊𝑠𝑚𝑎𝑙𝑙 ,𝐷𝑒𝑣𝑎𝑙and𝐷𝑡𝑒𝑠𝑡respectively. We didn’t
filter𝑊𝑠𝑚𝑎𝑙𝑙 so that the NDCG@10 score can genuinely represent
the quality of the weak data.
5.2.1 The Impact of Soft Prompt Tuning Module (RQ2). To gain
deeper insights into the optimized parameters 𝜃∗, we employed the
t-SNE algorithm [ 50] to visualize the virtual token vectors of the
learned soft prompt 𝑓𝜃∗(𝑠)when𝜃∗are converged with different
datasets and LLMs.
Figure 3a illustrates the distribution of virtual token vectors in a
two-dimensional space. We utilized the LLaMA-7B language model
with a virtual token length 𝑙𝑠=50for this experiment. The red
and blue points indicate the MS MARCO and FiQA datasets, re-
spectively. The visual analysis clearly reveals that the virtual token
8

--- PAGE 9 ---
Retriever MS MARCO FiQA-2018 DL2019 DL2020
MRR@10 Recall@100 MRR@10 Recall@100 MAP nDCG@10 Recall@100 MAP nDCG@10 Recall@100
BM25 0.1840 0.6578 0.2956 0.5395 0.3013 0.5058 0.4910 0.2856 0.4796 0.5599
𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 0.1501 0.6412 0.1726 0.3634 0.2187 0.4243 0.4330 0.2324 0.4028 0.4957
𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒 0.1606 0.6729 0.2199 0.4871 0.2489 0.4559 0.4806 0.2449 0.4159 0.5177
DPRW/O Aug 0.1303 0.5141 0.1433 0.3738 0.1796 0.3484 0.3289 0.2196 0.3806 0.4218
InPars 0.1519 0.6092 0.2720 0.5289 0.2474 0.4460 0.3919 0.2407 0.3913 0.4646
SPTAR†0.2114†0.7118 0.2885†0.5747†0.3091†0.5253†0.4928†0.3042†0.5219†0.5585
ColBERTW/O Aug 0.0665 0.3157 0.1498 0.3811 0.0679 0.2257 0.1998 0.0654 0.1612 0.2408
InPars 0.1965 0.5232 0.3031 0.4715 0.1908 0.4618 0.3402 0.2278 0.4645 0.4209
SPTAR 0.2000 0.5312†0.3472†0.5107 0.1923 0.4703 0.3332 0.2338 0.4752 0.4259
BM25BEW/O Aug 0.1456 0.6225 0.1659 0.4779 0.2330 0.4211 0.4155 0.2623 0.4236 0.5239
InPars 0.1660 0.6799 0.2891 0.5700 0.2939 0.4905 0.4720 0.2765 0.4286 0.5552
SPTAR†0.2159†0.7222 0.3000†0.6046†0.3379†0.5596†0.4999†0.3194†0.5395†0.5823
BM25CEW/O Aug 0.0876 0.5978 0.2319 0.6013 0.2137 0.3108 0.4337 0.1510 0.2263 0.5055
InPars 0.1889 0.6230 0.3646 0.6155 0.1549 0.2188 0.3732 0.1425 0.1700 0.4780
SPTAR†0.2009†0.7484 0.3705 0.6193†0.3440★†0.5488†0.5459★†0.2921†0.4405†0.6395★
𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒W/O Aug 0.1872 0.7228 0.2963 0.5920 0.2876 0.4735 0.4959 0.2944 0.4618 0.5851
InPars 0.1752 0.7253 0.3541 0.6196 0.2704 0.4782 0.4910 0.2748 0.4443 0.5736
SPTAR†0.2148†0.7717★†0.3836★†0.6567†0.3284†0.5272†0.5402†0.3325†0.5241†0.6267
𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑒W/O Aug 0.1938 0.7393 0.3402 0.6212 0.2979 0.4801 0.4913 0.2965 0.4661 0.5902
InPars 0.1743 0.7367 0.3535 0.6500 0.2914 0.4554 0.5323 0.2849 0.4270 0.5927
SPTAR†0.2121†0.7676†0.3757†0.6715★†0.3343†0.5207 0.5451†0.3258†0.5206†0.6231
𝑇𝐴𝑆−𝐵W/O Aug 0.0297 0.2046 0.1625 0.3903 0.0525 0.1306 0.1312 0.0388 0.0902 0.1115
InPars 0.2089 0.6911 0.2582 0.5221 0.2828 0.5257 0.4470 0.2982 0.4812 0.5065
SPTAR†0.2541★†0.7353†0.2943†0.5625 0.3049†0.5972★0.4737†0.3488★†0.5612★†0.5690
Table 6: SPTAR vs baseline models: a) 𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒is the original Contriever model pre-trained on CC-net and English
Wikipedia; b) 𝑅𝑒𝐶𝑜𝑛𝑡𝑟𝑖𝑒𝑣𝑒𝑟 𝑏𝑎𝑠𝑒is the original ReContriever model pre-trained on the same datasets as Contriever; c) W/O Aug
doesn’t use any augmented data and only applies the training data specified in Table 3; d) InPars [ 2] utilizes human-written
prompts and it has no soft prompt filter mechanism; e) Within each method, the best results are underscored and the symbols
†denote statistically significant enhancements over the second best result, with p-values < 0.05, as determined by a t-test; f)
The best results cross different methods are denoted with symbol ★. g) Table 3 documents the data splits for each method, and
the filtered example document-query pairs of SPTAR are documented in Appendix A and B.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2MSMARCO
FiQA2018
(a) Different datasets.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2GPT2
LLaMA7B
Vicuna7B
 (b) Different LLMs.
0.2
 0.0 0.2 0.4 0.6 0.8 1.0 1.2
t-SNE Dim 10.2
0.00.20.40.60.81.01.2t-SNE Dim 2Softpromptlength=40
Softpromptlength=50
Softpromptlength=80
 (c) Different lengths.
Figure 3: T-SNE embedding visualization of soft prompt’s virtual tokens: a) soft prompt’s virtual tokens with different datasets;
b) soft prompt’s virtual tokens with different LLMs; c) virtual tokens of soft prompt with different lengths.
vectors from the two datasets exhibit distinct distributions in the
two-dimensional space, with minimal overlap. Notably, at the model
initialization phase, both datasets share the same prompt 𝑠, mak-
ing the observed changes in vector distribution after convergenceparticularly significant. These findings highlight the remarkable ca-
pability of prompt tuning to distill domain-specific knowledge from
datasets to the learned prompt token vectors. This accomplishment
is particularly noteworthy in the scenario where ground-truth data
9

--- PAGE 10 ---
are too limited that human-written prompts struggle to capture
domain-specific information and incorporate it effectively into the
prompt design.
In Figure 3b, various colors distinguish distinct LLMs: GPT-2,
LLaMA-7B, and Vicuna-7B. We kept all the hyperparameters the
same except for the language model to evaluate the influence of
different language models on the parameters 𝜃. The dispersion
of points with the same color indicates the extent of parameter
updated during training. Figure 3b clearly illustrates that the red
point cloud representing the GPT-2 model has less dispersion, with
points tightly clustered together. In contrast, the blue point cloud
representing LLaMA-7B and the green point cloud representing
Vicuna-7B exhibit greater dispersion of virtual token vectors. This
observation suggests that, when trained on the same dataset, the
LLaMA-7B and Vicuna-7B models enable the soft prompt module
to absorb more domain-specific knowledge, leading to an enhance-
ment in the generation of synthesized queries. Moreover, similar
findings were obtained when decoding the virtual tokens into cor-
responding words. For instance, after training the GPT-2 model,
we observed that the resulting soft prompt merely replicates the
prompt tokens used during initialization, essentially duplicating the
manual prompt without additional learning. In contrast, when de-
coding the virtual token vectors into words utilizing the LLaMA-7B
and Vicuna-7B, we discovered that these models not only retain the
initial prompt tokens but also acquire additional symbols and repre-
sentations associated with relevant text, such as “ query ”, “rewrite ”,
“argument ”, “enhance ” and “ adding ”, indicating parameters 𝜃does
learn task-specific knowledge.
In Figure 3c, we aim to understand the effects of different soft
prompt lengths on the tuning module by examining the virtual to-
ken vector distribution of the learned soft prompt. This experiment
was conducted on LLaMA-7B and dataset MS MARCO and all the
hyperparameters are the same except for the soft prompt length.
The three lengths 40, 50, and 80 are represented by the colors red,
blue, and green, respectively. From the point distribution in Figure
3c, we observe partial overlap between the red and blue points, as
well as some distinct points. As the virtual token length increases,
the embedding distribution area of the longer soft prompt encom-
passes the regions corresponding to the shorter ones: 40 and 50.
This result aligns with our expectation: with different lengths of
soft prompts, the embedding distributions of soft prompts’ virtual
tokens are different. Nevertheless, regardless of their lengths, the
distributions of these prompts tend to have significant overlap and
shared regions.
For RQ2, we have conclusions: a) datasets can be distinguished
from the learned soft prompts, demonstrating that soft prompt
tuning does learn task-specific soft prompts; b) both the LLMs and
the length of soft prompts influence the learned soft prompts.
5.2.2 The Efficiency of Soft-Prompt Tuning (RQ3). Table 7 presents
the number of learnable parameters and convergence efficiency of
soft prompt tuning for different LLMs on the MS MARCO dataset.
For the soft prompt tuning module in our proposed SPTAR, despite
the vast number of LLM’s original parameters Φ,Φremains frozen
and does not require fine-tuning. The trainable parameters 𝜃as-
sociated with the fine-tuning of the soft prompt are substantially
fewer. The percentages in the second column highlight that the softLLM𝑐𝑜𝑢𝑛𝑡(𝜃)/𝑐𝑜𝑢𝑛𝑡(Φ)Best Epoch #
GPT-2 0.0308% 17
LLaMA-7B 0.0030% 5
Vicuna-7B 0.0030% 4
Table 7: Efficiency evaluation of SPTAR’s soft prompt tuning
module on MS MARCO 𝑆50
𝑡𝑟𝑎𝑖𝑛and𝑆100
𝑒𝑣𝑎𝑙(Section 3.1).
prompt module’s fine-tuning involves an exceptionally small set of
parameters𝜃, roughly equating to 0.003% of the size of Φ. Notably,
the size of𝜃stays constant, irrespective of the growth of Φ. This
characteristic significantly enhances the practicality and training
efficiency of SPTAR, as we can fine-tune task-specific soft prompts
with a minimal fraction of parameters for optimization.
Furthermore, for a new task or dataset, SPTAR can efficiently
complete the fine-tuning process of the soft prompt tuning module
within a few epochs. As highlighted in the third column of the ta-
ble, we examined the convergence speed of the soft prompt tuning
model on the evaluation dataset 𝑆100
𝑒𝑣𝑎𝑙(Section 3.1) by the best epoch
number and the lower this number is, the faster it converges. It
becomes apparent that employing a more advanced language model
expedites the convergence of the soft prompt tuning module, requir-
ing a mere four or five epochs for convergence. Considering both
the count of 𝜃and the convergence speed, we can confidently con-
clude that the soft prompt tuning module leverages the advantages
offered by LLMs while effectively mitigating the computational
resource consumption associated with fine-tuning the whole LLMs.
In conclusion, the soft prompt tuning model only fine-tunes a
small part of the parameters 𝜃, and the training converges quickly
on LLMs.
Dataset Filter Perplexity (Dec%) NDCG@10 (Imp%)
MS MARCOWorst 4.1934 0.2132
Best 3.6649 (+12.60%) 0.2376 (+11.44%)
FiQA-2018Worst 410.9207 0.1855
Best 5.7898 (+98.59%) 0.1923 (+3.67%)
Table 8: Evaluation of SPTAR-DPR with the best and worst
example document-query pairs in soft prompt augmentor
module. SPTAR-DPR is trained on 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝑊𝑠𝑚𝑎𝑙𝑙 and
tested on𝐷𝑡𝑒𝑠𝑡. Results are obtained on LLaMA-7B. For MS
MARCO and FiQA-2018, 𝑀=2and𝑀=1respectively.
5.2.3 The Impact of Soft Prompt Filter Module (RQ4). With the
learned parameters 𝜃∗in SPTAR’s soft prompt tuning module, we
observe the example document-query pairs in SPTAR’s soft prompt
augmentor module do influence the quality of the generated weak
data, so it is necessary to select certain 𝑀document-query pairs
from𝑆𝑋
𝑡𝑟𝑎𝑖𝑛. In this section, we study the impact of SPTAR’s soft
prompt filter module. In Table 8, we report the best results of SPTAR-
DPR (Section 5.2.6): a) for MS MARCO, we report the results of
SPTAR-DPR with LLaMA-7B and 𝑀=2; b) for FiQA-2018, we
report the results of SPTAR-DPR with LLaMA-7B and 𝑀=1. The
SPTAR-DPR is trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝑊𝑠𝑚𝑎𝑙𝑙 and tested on
10

--- PAGE 11 ---
W/O 10 30 50 700.220.240.26
Top-𝑘of Weak Data FilterNDCG@10MSMARCO
FiQA-2018
Figure 4: SPTAR-DPR NDCG@10 scores with different top- 𝑘
of weak data filter. SPTAR-DPR is trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+
𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒)(Section 4.1). Results are obtained on LLaMA-7B.
For MS MARCO and FiQA-2018, 𝑀=2and𝑀=1respectively.
𝐷𝑡𝑒𝑠𝑡. The best and worst 𝑀example pairs in Table 8 are filtered
by the method proposed in Section 3.3.
As shown in Table 8, the results unequivocally demonstrate that
the soft prompt filter significantly enhances performance across
all comparisons. Specifically, we observe a noteworthy 12.60% to
98.59% decrease in perplexity and a substantial 3.67% to11.44% im-
provement on NDCG@10 in the downstream DPR model. Further-
more, our experimental findings indicate that while the utilization
of in-context learning theory, complemented by limited examples,
greatly enhances the quality of generated weak queries, the choice
of example document-query pairs also exerts a considerable influ-
ence on text generation quality.
5.2.4 The Impact of Weak Data Filter Module (RQ5). To assess the
enhancements achieved by filtering weak data, we applied vari-
ous top-𝑘values to filter the generated weak data 𝑊𝑙𝑎𝑟𝑔𝑒 , yielding
𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒). We then assessed the performance of the SPTAR-DPR
model, trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝐹𝑘(𝑊𝑙𝑎𝑟𝑔𝑒), on𝐷𝑡𝑒𝑠𝑡. This com-
parison served to quantify the gains over the approach devoid of
a weak data filter. Optimal parameters, namely LLM and 𝑀, were
identified and held constant in this section to exclusively assess the
influence of top- 𝑘.
As shown in Figure 4, on MS MARCO, the SPTAR-DPR model
without the data filter gets an NDCG@10 score of 0.2319 while
the score rises to 0.2580 with data filter top- 𝑘=30. On FiQA-2018,
SPTAR-DPR with filter top- 𝑘=70 gets the highest NDCG@10 score
of0.2404 , while it gets an NDCG@10 score of 0.2242 without a data
filter. These consistent gains across different datasets underscore
the effectiveness of the weak data filter module (Section 3.4). We
did not discern any correlation between top- 𝑘and the NDCG@10
metric; thus, in real-world scenarios, top- 𝑘acts as a hyperparameter
requiring tuning per dataset.
5.2.5 The Impact of Training Size 𝑋(RQ6). In this section, we ana-
lyze the impact of different training sizes 𝑋in SPTAR’s soft prompt
tuning module. To evaluate the impact of 𝑋, we first conducted
soft prompt tuning on 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛and evaluated the perplexity on 𝑆100
𝑒𝑣𝑎𝑙.
Notably, perplexity serves as an intrinsic metric to measure the im-
pact of𝑋on the quality of generated weak queries. Subsequently,
we generated 𝑊𝑠𝑚𝑎𝑙𝑙 and tested the SPTAR-DPR model trained
on𝑆𝑋
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝑊𝑠𝑚𝑎𝑙𝑙 on𝐷𝑡𝑒𝑠𝑡. NDCG@10 score is applied10 30 5099.699.7
𝑋PercentagePerplexity(Dec%)
10 30 5010203040
𝑋PercentageNDCG@10(Imp%)
Figure 5: Evaluation of SPTAR-DPR with different 𝑋com-
pared with W/O (Section 4.1). SPTAR-DPR is trained on
𝑆𝑋
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝑊𝑠𝑚𝑎𝑙𝑙 and tested on 𝐷𝑡𝑒𝑠𝑡. Results are obtained
on LLaMA-7B and MS MARCO.
to measure the impact of 𝑋on downstream DR models, like DPR.
As shown in Figure 5, the findings conclusively demonstrate sub-
stantial improvements when employing soft prompt tuning with
varying training sizes 𝑋compared with the results obtained without
soft prompt tuning (W/O in Section 4.1). Specifically, when 𝑋=50,
perplexity is decreased by 99.78% , and an impressive 37.66% en-
hancement is observed. Interestingly, it’s apparent that enhancing
perplexity is more straightforward than improving NDCG@10,
suggesting a disparity between these metrics.
Different from InPars [ 2] and Promptagator [ 8], which only
utilizes several example document-query pairs in human-written
prompts, our findings underscore the benefits of a marginally larger
training size 𝑋in soft prompt tuning, leading to better performance.
This superiority is manifest in the reduced perplexity and the en-
hanced NDCG@10 scores in downstream tasks with the increment
of training size 𝑋.
5.2.6 The Impact of Number of Example Pairs 𝑀(RQ7). In SPTAR’s
soft prompt agumentor module, when tagging the unlabeled docu-
ments with weak queries, 𝑀filtered example document-query pairs
are utilized to instruct the LLM. In this section, we explore the im-
pact of different 𝑀. Initially, we selected LLaMA-7B as the LLM and
did soft prompt tuning on 𝑆50
𝑡𝑟𝑎𝑖𝑛, computing perplexity on 𝑆100
𝑒𝑣𝑎𝑙.
Subsequently, with the filtered 𝑀example document-query pairs
from SPTAR’s soft prompt filter module (Section 3.3), we generated
𝑊𝑠𝑚𝑎𝑙𝑙 . Ultimately, SPTAR-DPR trained on 𝑆50
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+𝑊𝑠𝑚𝑎𝑙𝑙
is tested on 𝐷𝑡𝑒𝑠𝑡to compute NDCG@10. We also did the same ex-
periments on Vicuna, and we found LLaMA-7B model consistently
delivers better results than the Vicuna-7B model, no matter whether
𝑀=1or𝑀=2. Thus, we only report the results on LLaMA-7B in
Figure 6.
As depicted in Figure 6, for dataset MS MARCO, 𝑀=2achieves
the best performance in terms of perplexity and NDCG@10. In
contrast, for dataset FiQA-2008, 𝑀=1demonstrates superior per-
formance. These results contradict our initial assumption that the
bigger𝑀is the better the perplexity and NDCG@10 are. We at-
tribute this inconsistency to varying dataset distributions. Con-
sidering that many QA datasets contain documents with multiple
relevant queries, where each query is constructed from a subset
of the document, it implies a heightened level of uncertainty and
11

--- PAGE 12 ---
MS MARCO FiQA-2018456Perplexity𝑀=1
𝑀=2
MS MARCO FiQA-20180.160.180.20.220.24NDCG@10𝑀=1
𝑀=2
Figure 6: Evaluation of SPTAR-DPR with different numbers
of example pairs 𝑀. SPTAR-DPR is trained on 𝑆𝑋
𝑡𝑟𝑎𝑖𝑛+𝑆100
𝑒𝑣𝑎𝑙+
𝑊𝑠𝑚𝑎𝑙𝑙 and tested on 𝐷𝑡𝑒𝑠𝑡. Results are obtained on LLaMA-7B
and MS MARCO.
complexity for the learning model. These intricacies, in turn, pro-
duce varying performances across different datasets. Therefore, we
acknowledge the importance of delving deeper into this topic in
subsequent research.
6 CONCLUSION AND FUTURE WORK
In this paper, we introduce the soft prompt tuning for augmenting
DR (SPTAR) framework to tackle the challenge of limited domain-
specific training data in DR tasks. Our approach harnesses soft
prompt tuning to optimize soft prompts on limited ground truth
data. By prompting LLMs with these optimized soft prompts as
well as example document-query pairs, we generate weak queries
for unlabeled documents, resulting in an abundant collection of
weak document-query pairs for training domain-specific dense
retrievers. To further enhance the quality of the generated weak
tagged queries, we incorporate a soft prompt filter that selects
high-quality example document-query pairs in the prompt as well
as a weak data filter module to clean the generated weak data.
The effectiveness of our proposed approach is validated through
comprehensive experiments. This work represents an initial step
toward a promising research direction. In future work, we aim to
scrutinize SPTAR’s broad applicability by testing it across diverse
datasets. It’s noteworthy that the loss function employed herein
is a pointwise loss, implying a suboptimal utilization of negative
instances. Future studies might benefit from delving into pairwise
and listwise losses. Moreover, there lies potential in probing multi-
task soft prompt tuning methods to bolster both efficiency and
outcome.
A FILTERED EXAMPLE DOCUMENT-QUERY
PAIRS FOR MS MARCO AND LLAMA
For MS MARCO, 𝑀=2is better than 𝑀=1when LLaMA is
employed.
A.1𝑀=1
Document : According to price comparison website Gocompare.com,
the cost of becoming a new driver has soared by almost a fifth in
the past five years. A survey of 2,000 parents found the average
cost of a young driver’s first car is Â£3,825, with insurance pricedat Â£2,232.Scroll down for video. Expensive: A survey of 2,000 par-
ents found the average cost of a young driver’s first car is Â£3,825.
The typical learner also needs Â£480 of driving lessons. survey of
2,000 parents found the average cost of a young driver’s first car is
Â£3,825, with insurance priced at Â£2,232. Scroll down for video.
Expensive: A survey of 2,000 parents found the average cost of a
young driver’s first car is Â£3,825.
Query : average insurance cost for new drivers
A.2𝑀=2
Document : Oakland weather forecast from AccuWeather.com. Ex-
tended forecast in Oakland, MD 21550 for up to 25 days includes
high temperature, RealFeel and chance of precipitation Oakland
weather forecast from AccuWeather.com. Extended forecast in Oak-
land, MD 21550 for up to 25 days includes high temperature, Re-
alFeel and chance of precipitation my recent locations Â °f Oakland,
MD 41Â °
Query : weather in oakland md
Document : As their name suggests, the triglycerides are composed
of one molecule of glycerol and joined via ester bonds with three
molecules of fatty acids. As is shown in Figure 12, fatty acids are
long chains of carbon and hydrogen usually between 14-24 carbons
long (and they always have an even number of carbons).. Phos-
pholipids: This class of lipids are really derivatives of triglycerides.
The are composed of a glycerol molecule with two fatty acids (a
diglyceride). The third carbon contains a phosphate group and usu-
ally some added polar molecule (such as ethanolamine, serine or
choline).
Query : what are triglycerides composed of
B FILTERED EXAMPLE DOCUMENT-QUERY
PAIRS FOR FIQA-2018 AND LLAMA
For FiQA-2018, 𝑀=1is better than 𝑀=2when LLaMA is em-
ployed.
B.1𝑀=1
Document : As your is a very specific case, please get an advice
of CA. It should not cost you much and make it easier. The sale of
agriculture land is taxable in certain conditions and exempt from
tax in other cases. Sale of agricultural land is subject to capital gains
tax. But there are certain exemptions under Section 54B, subject
to conditions, which are as follows: If deemed taxable, you can
avail indexation, ie the price at which you grandfather got [the
date when he inherited it as per indexation] and pay 10% on the
difference. If the price is not known, you can take the govt pre-
scribed rate. As there is a large deposit in your fathers account,
there can be tax queries and need to be answered. Technically there
is no tax liable even if your grandfather gifts the money to your
father. More details at http://www.telegraphindia.com/1130401/jsp/
business/story_16733007.jsp and http://www.incometaxindia.gov.
in/publications/4_compute_your_capital_gains/chapter2.asp
Query : Is the amount taxable if my grandfather sells agricultural
12

--- PAGE 13 ---
land
B.2𝑀=2
Document : Others have already commented on the impact of any-
thing which dissuades merchants from raising possible breaches, so
I won ´t dwell on that. Maybe we need stronger legislation, maybe we
don´t, but it doesn ´t change todayś answer. Often it works the other
way around to what you might expect - rather than the merchant
noticing and notifying Visa/MC/others, Visa/MC/others spot pat-
terns of suspicious activity (example 1). I don ´t have any data on the
relative numbers of who is being notified/notifying between mer-
chants and payment processors, but at the point when your card is
identified as compromised thereś no reason to suppose that an indi-
vidual merchant in the traditional sense has been compromised, let
alone identified. In fact because thereś a fast moving investigation
it could even be a false alarm that led to your card getting cancelled.
Conversely it could be a hugely complex multinational investiga-
tion which would be jeopardised. Itś simply not safe to assume that
simply ""brand X"" has been compromised, therefore everything
""brand X"" knows about you is also compromised: Furthermore
thereś no reason to assume the merchant has even admitted to, or
discovered the root cause. MC/Visa/Banks, at the point at which
theyŕe cancelling cards simply can ´t say (at least not in a way that
might expensively backfire involving lots of lawyers) because the
standard of proof needed to go on record blaming someone is sim-
ply not yet met. So: yes itś common that you aren ´t told anything
for all of the above reasons. And of course if you really want to
find out more you may have some success with your local data
protection legislation and formally make a subject access request
(or local equivalent) to see what that brings back. Be sure to do it in
writing, to the official address of both mastercard and your bank.
Query : MasterCard won’t disclose who leaked my credit card de-
tails
Document : Surprised nobody has mentioned Freshbooks yet. It’s
lightweight, easy to use, and free for low-end use (scaling up price-
wise as you scale up).
Query : What’s the best application, software or tool that can be
used to track time?
REFERENCES
[1]Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mittal.
2000. Bridging the lexical chasm: statistical approaches to answer-finding. In
SIGIR . ACM, 192–199.
[2]Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Fras-
setto Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information
Retrieval. In SIGIR . ACM, 2387–2392.
[3]Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,
Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-
pervised Training of Efficient Rankers. CoRR abs/2301.02998 (2023).
[4]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
(2020).
[5]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with
90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/
[6]Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview
of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text
REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA],
November 16-20, 2020 (NIST Special Publication, Vol. 1266) , Ellen M. Voorhees
and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST).
https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf
[7]Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.
Voorhees. 2020. Overview of the TREC 2019 deep learning track. CoRR
abs/2003.07820 (2020). arXiv:2003.07820 https://arxiv.org/abs/2003.07820
[8]Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot
dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[9]Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot
Dense Retrieval without Relevance Labels. CoRR abs/2212.10496 (2022).
[10] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi,
and Dongfang Liu. 2023. Eˆ2VPT: An Effective and Efficient Approach for Visual
Prompt Tuning. CoRR abs/2307.13770 (2023). https://doi.org/10.48550/arXiv.
2307.13770 arXiv:2307.13770
[11] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan
Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced
Topic Aware Sampling. In SIGIR ’21: The 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval, Virtual Event, Canada, July
11-15, 2021 , Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones,
and Tetsuya Sakai (Eds.). ACM, 113–122. https://doi.org/10.1145/3404835.3462891
[12] Zhiqi Huang, Hansi Zeng, Hamed Zamani, and James Allan. 2023. Soft Prompt
Decoding for Multilingual Dense Retrieval. CoRR abs/2305.09025 (2023).
[13] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-
mation Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022 (2022).
https://openreview.net/forum?id=jKN1pXi7b0
[14] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park.
2022. Augmenting Document Representations for Dense Retrieval with In-
terpolation and Perturbation. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022 , Smaranda Muresan, Preslav Nakov, and
Aline Villavicencio (Eds.). Association for Computational Linguistics, 442–452.
https://doi.org/10.18653/v1/2022.acl-short.48
[15] Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee,
Roberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023.
InPars-v2: Large Language Models as Efficient Dataset Generators for Informa-
tion Retrieval. CoRR abs/2301.01820 (2023).
[16] Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, Jheng-Hong
Yang, and Jimmy Lin. 2024. Resources for Brewing BEIR: Reproducible Reference
Models and Statistical Analyses. (2024).
[17] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. arXiv preprint arXiv:2004.04906 (2020).
[18] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In SIGIR . ACM, 39–48.
[19] Yibin Lei, Liang Ding, Yu Cao, Changtong Zan, Andrew Yates, and Dacheng
Tao. 2023. Unsupervised Dense Retrieval with Relevance-Aware Contrastive
Pre-Training. In Findings of the Association for Computational Linguistics: ACL
2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 10932–10940.
https://doi.org/10.18653/V1/2023.FINDINGS-ACL.695
[20] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for
Parameter-Efficient Prompt Tuning. In EMNLP . Association for Computational
Linguistics, 3045–3059.
[21] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous
Prompts for Generation. In ACL/IJCNLP . Association for Computational Linguis-
tics, 4582–4597.
[22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-
hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson,
Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu
Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül,
Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya
Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,
William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022.
Holistic Evaluation of Language Models. CoRR abs/2211.09110 (2022).
[23] Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya,
John Foley, Grant Ingersoll, Craig MacDonald, and Sebastiano Vigna. 2016. To-
ward Reproducible Baselines: The Open-Source IR Reproducibility Challenge. In
ECIR (Lecture Notes in Computer Science, Vol. 9626) . Springer, 408–420.
13

--- PAGE 14 ---
[24] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,
and Rodrigo Frassetto Nogueira. 2021. Pyserini: A Python Toolkit for Repro-
ducible Information Retrieval Research with Sparse and Dense Representations.
InSIGIR ’21: The 44th International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 , Fernando
Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai
(Eds.). ACM, 2356–2362. https://doi.org/10.1145/3404835.3463238
[25] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar
Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your
DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval.
CoRR abs/2302.07452 (2023). https://doi.org/10.48550/arXiv.2302.07452
arXiv:2302.07452
[26] Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun
Quan, and Dawei Song. 2022. XPrompt: Exploring the Extreme of Prompt Tuning.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates . Association for Com-
putational Linguistics, 11033–11047. https://preview.aclanthology.org/emnlp-
22-ingestion/2022.emnlp-main.758.pdf
[27] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot
Listwise Document Reranking with a Large Language Model. arXiv preprint
arXiv:2305.02156 (2023).
[28] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDer-
mott, Manel Zarrouk, and Alexandra Balahur. 2018. WWW’18 Open Challenge:
Financial Opinion Mining and Question Answering. In WWW . ACM, 1941–1942.
[29] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
Paul. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods.
https://github.com/huggingface/peft.
[30] Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. 2023. Learning to Compress
Prompts with Gist Tokens. CoRR abs/2304.08467 (2023).
[31] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading
comprehension dataset. choice 2640 (2016), 660.
[32] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a
pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).
[33] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-Shot Learning
with Language Models. In NeurIPS . 11054–11070.
[34] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna:
Zero-Shot Listwise Document Reranking with Open-Source Large Language
Models. CoRR abs/2309.15088 (2023). https://doi.org/10.48550/arXiv.2309.15088
arXiv:2309.15088
[35] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen,
Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky.
2023. Large Language Models are Effective Text Rankers with Pairwise Ranking
Prompting. CoRR abs/2306.17563 (2023).
[36] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxi-
ang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training
Approach to Dense Passage Retrieval for Open-Domain Question Answering. In
Proceedings of the 2021 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2021, Online, June 6-11, 2021 , Kristina Toutanova, Anna Rumshisky, Luke Zettle-
moyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy
Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics,
5835–5847. https://doi.org/10.18653/v1/2021.naacl-main.466
[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of
transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research 21, 1 (2020), 5485–5551.
[38] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu,
Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A Joint Training Method
for Dense Passage Retrieval and Passage Re-ranking. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , Marie-
Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.).
Association for Computational Linguistics, 2825–2835. https://doi.org/10.18653/
v1/2021.emnlp-main.224
[39] Stephen Robertson, Hugo Zaragoza, et al .2009. The probabilistic relevance
framework: BM25 and beyond. Foundations and Trends ®in Information Retrieval
3, 4 (2009), 333–389.
[40] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau
Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with
zero-shot question generation. arXiv preprint arXiv:2204.07496 (2022).
[41] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight lateinteraction. arXiv preprint arXiv:2112.01488 (2021).
[42] Teven Le Scao and Alexander M Rush. 2021. How many data points is a prompt
worth? arXiv preprint arXiv:2103.08493 (2021).
[43] Timo Schick and Hinrich Schütze. 2021. Exploiting Cloze-Questions for Few-
Shot Text Classification and Natural Language Inference. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , Paola Merlo, Jörg
Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics,
255–269. https://doi.org/10.18653/V1/2021.EACL-MAIN.20
[44] Timo Schick and Hinrich Schütze. 2021. It’s Not Just Size That Matters:
Small Language Models Are Also Few-Shot Learners. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021 , Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (Eds.). Association for Computational Linguistics, 2339–2352.
https://doi.org/10.18653/V1/2021.NAACL-MAIN.185
[45] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun
Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as
Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).
[46] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Ji-
ahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes
generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087
(2022).
[47] Zhengyang Tang, Benyou Wang, and Ting Yao. 2022. DPTDR: Deep Prompt
Tuning for Dense Passage Retrieval. In COLING . International Committee on
Computational Linguistics, 1193–1202.
[48] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna
Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of
information retrieval models. arXiv preprint arXiv:2104.08663 (2021).
[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[50] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[51] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier
Benchmark for General-Purpose Language Understanding Systems. (2019), 3261–
3275.
[52] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
of Pre-Trained Transformers. (2020).
[53] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogério Feris, Huan Sun, and
Yoon Kim. 2023. Multitask Prompt Tuning Enables Parameter-Efficient Transfer
Learning. In ICLR . OpenReview.net.
[54] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models
are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[55] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping,
and Tom Goldstein. 2023. Hard Prompts Made Easy: Gradient-Based Discrete
Optimization for Prompt Tuning and Discovery. In Advances in Neural Infor-
mation Processing Systems 36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/
a00548031e4647b13042c97c922fadf1-Abstract-Conference.html
[56] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-
tive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808
(2020).
[57] Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. 2023. Prompt
Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration
For Video Captioning. In Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao,
SAR, China . ijcai.org, 1622–1630. https://doi.org/10.24963/ijcai.2023/180
[58] Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian
Khabsa, Sinong Wang, Zenglin Xu, and Dongfang Liu. 2023. MixPAVE: Mix-
Prompt Tuning for Few-shot Product Attribute Value Extraction. In Proceedings
of the 61th Annual Meeting of the Association for Computational Linguistics, ACL
2023. Association for Computational Linguistics.
[59] Jie Zhou, Le Tian, Houjin Yu, Zhou Xiao, Hui Su, and Jie Zhou. 2022. Dual
Context-Guided Continuous Prompt Tuning for Few-Shot Learning. In ACL.
Association for Computational Linguistics, 79–84.
14
