# 2301.12132.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2301.12132.pdf
# File size: 1102466 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AUTOPEFT: Automatic Configuration Search for
Parameter-Efficient Fine-Tuning
Han Zhou1,*Xingchen Wan2,*Ivan Vuli ´c1Anna Korhonen1
1Language Technology Lab, University of Cambridge
2Machine Learning Research Group, University of Oxford
{hz416, iv250, alk23}@cam.ac.uk
xwan@robots.ox.ac.uk
Abstract
Large pretrained language models are widely
used in downstream NLP tasks via task-
specific fine-tuning, but such procedures
can be costly. Recently, Parameter-Efficient
Fine-Tuning (PEFT) methods have achieved
strong task performance while updating
much fewer parameters than full model fine-
tuning (FFT). However, it is non-trivial to
make informed design choices on the PEFT
configurations , such as their architecture,
the number of tunable parameters, and even
the layers in which the PEFT modules are
inserted. Consequently, it is highly likely
that the current, manually designed config-
urations are suboptimal in terms of their
performance-efficiency trade-off. Inspired
by advances in neural architecture search,
we propose AUTOPEFT for automatic PEFT
configuration selection: we first design an ex-
pressive configuration search space with mul-
tiple representative PEFT modules as build-
ing blocks. Using multi-objective Bayesian
optimisation in a low-cost setup, we then
discover a Pareto-optimal setof configura-
tions with strong performance-cost trade-offs
across different numbers of parameters that
are also highly transferable across differ-
ent tasks. Empirically, on GLUE and Su-
perGLUE tasks, we show that AUTOPEFT -
discovered configurations significantly out-
perform existing PEFT methods and are on
par or better than FFT without incurring sub-
stantial training efficiency costs.
1 Introduction and Motivation
Pretrained language models (PLMs) are used in
downstream tasks via the standard transfer learning
paradigm, where they get fine-tuned for particu-
lar tasks (Devlin et al., 2019; Liu et al., 2019b).
This achieves state-of-the-art results in a wide spec-
trum of NLP tasks, becoming a prevalent modelling
*Equal contribution.
100101
Fine-tuned Parameters (%)8081828384Average ScoreSerial UniPELT
MAMAdaMix Prefix
LoRAParallelAutoPEFT(per-task)
AutoPEFTFull Model FTFigure 1: Performance of AUTOPEFT -discovered con-
figurations ( AutoPEFT &AutoPEFT(per-task) ; see de-
tails in Table 1) compared to other baseline PEFT meth-
ods (markers) and full model FT that updates 100% of
parameters (dashed horizontal bar), averaged across 8
GLUE tasks. Our approach achieves the best trade-off
between task performance and parameter efficiency.
paradigm in NLP (Raffel et al., 2020a). Fine-tuning
the PLMs typically requires a full update of their
original parameters (i.e. the so-called full-model
fine-tuning (FFT) ); however, this is (i) computation-
ally expensive and also (ii) storage-wise expensive
as it requires saving a separate full model copy
for each task-tuned model. With the ever-growing
size of the PLMs (Brown et al., 2020; Sanh et al.,
2022), the cost of full-model FT becomes a major
bottleneck, due to its increasing demands as well
as computational (time and space) non-efficiency.
Parameter-efficient fine-tuning (PEFT) delivers
a solution for alleviating the issues with full-model
FT (Houlsby et al., 2019). By freezing the majority
of pretrained weights of PLMs, PEFT approaches
only update a small portion of parameters for ef-
ficiently adapting the PLM to a new downstream
task. Recent studies have shown that PEFT can
achieve competitive task performance while being
modular, adaptable, and preventing catastrophic
forgetting in comparison to traditional FFT (Wang
et al., 2022; Pfeiffer et al., 2023).arXiv:2301.12132v3  [cs.CL]  29 Jan 2024

--- PAGE 2 ---
Recent developments have created diverse PEFT
modules with distinctive characteristics (Pfeiffer
et al., 2020b; Li and Liang, 2021), with one of
the two main aims in focus: 1)improve task perfor-
mance over other PEFT approaches while maintain-
ing the same parameter budget as the competitor
PEFT methods; or 2)maintain task performance
while reducing the parameter budget needed. Exist-
ing PEFT modules, optimising for one of the two
aims, have been successfully applied to transfer
learning tasks (Chen et al., 2022b; Pfeiffer et al.,
2022). However, different tasks, with different
complexity, show distinct sensitivity to the allo-
cated parameter budget and even to the chosen
PEFT approach (He et al., 2022). At the same
time, most PEFT applications are limited to a sin-
gle PEFT architecture (e.g. serial adapters, prefix-
tuning) with fixed decisions on its components (e.g.
hidden size dimensionality, insertion layers) result-
ing in potentially suboptimal PEFT configurations
across many tasks. Therefore, in this work, we
propose a new, versatile and unified framework
that automatically searches for improved and task-
adapted PEFT configurations, aiming to effectively
balance between the two (often colliding goals)
of (i) improving performance and (ii) keeping the
desired low parameter budget for PEFT.
While recent research has started exploring more
dynamic PEFT configurations, the prior studies
remain limited across several dimensions, includ-
ing how they define the configuration search space.
Namely, they typically focus only on a single PEFT
architecture (e.g. adapters) or their simple combi-
nations, or a single property (e.g. insertion layers –
where to insert the module); see a short overview
later in §3. Here, we propose a unified and more
comprehensive framework for improved configu-
ration search. It covers multiple standard PEFT
modules (serial adapters, parallel adapters, and
prefix-tuning) as building blocks, combined with
the critical parameter budget-related decisions: the
size of each constituent module and the insertion
layers for the modules.
Our defined comprehensive search space is huge;
consequently, traversing it effectively and effi-
ciently is extremely challenging. To enable search
over the large configuration space, we thus propose
the novel AUTOPEFT framework. It automatically
configures multiple PEFT modules along with their
efficiency-oriented design decisions, relying on a
high-dimensional Bayesian optimisation (BO) ap-proach. Crucially, within the search space, we pro-
pose a multi-objective optimisation which learns
to balance simultaneously between maximising the
searched configurations’ task performance andpa-
rameter efficiency.
We conduct extensive experiments on the stan-
dard GLUE and SuperGLUE benchmarks (Wang
et al., 2018, 2019), with encoder-only and encoder-
decoder models. We first study the transferabil-
ity of the AUTOPEFT -searched architecture by
running AUTOPEFT on a single task with a low-
fidelity proxy (aiming to reduce computational
cost), followed by transferring the found archi-
tecture to other tasks. Experimental results show
that this architecture can outperform existing PEFT
baselines while achieving on-par performance with
the standard FFT. Further slight gains can be
achieved with a larger computation budget for train-
ing, where we run AUTOPEFT per task to find a
task-adapted PEFT configuration. As revealed in
Figure 1, AUTOPEFT can find configurations that
offer a solid trade-off between task performance
and parameter efficiency, even outperforming FFT.
We also provide ablation studies over the search
space, validating that the AUTOPEFT framework
is versatile and portable to different search spaces.
Contributions. 1) We propose the AUTOPEFT
search space containing diverse and expressive
combinations of PEFT configurations from three
representative PEFT modules as foundational build-
ing blocks and the binary decisions concerning
Transformer layers for inserting these modules as
searchable dimensions. 2)To navigate the vast AU-
TOPEFT search space and to discover a setof trans-
ferable PEFT configurations that optimally trade
performance against cost across various parameter
ranges in a single run , we further propose an ef-
fective search method based on multi-dimensional
Bayesian optimisation. 3)We demonstrate that the
one-time search cost of AUTOPEFT is low, and
AUTOPEFT yields task-shareable configurations,
outperforming existing PEFT modules while being
transferable across tasks. The AUTOPEFT frame-
work can also be easily extended to other and new
PEFT modules. The code is available at https:
//github.com/cambridgeltl/autopeft .
2 A UTOPEFT Framework
2.1 Designing the A UTOPEFT Search Space
Inspired by the success of neural architecture
search (NAS) methodology (Ru et al., 2020), we

--- PAGE 3 ---
Feed Forward
LayerNorm
Multi-Head AttentionLayerNorm
PrefixSerial ParallelPEFT  Layer    
PEFT  Layer    
Layer    
PEFT  Layer    
Layer    
PEFT  Layer    Layers
PrefixSerial Parallel Layers
Inactive
PEFT  layers
Largest
possible sizesActual  sizes
in this config
PrefixSerial Parallel LayersConfig 1
Config 2Search Space and Connections
Active
PEFT  layersFigure 2: Illustration of the AUTOPEFT search space which combines both layer-level ( Layers ) and within-layer
(Serial, Parallel, Prefix ) search, and the connections within a layer ( Left). We further show two possible
configurations in the search space ( Right ): note that some PEFT layers can be inactive altogether and the searchable
module sizes (shaded in green), i.e. the bottleneck sizes in Serial andParallel (DSAandDPArespectively) and
sizes of PK, PVinPrefix (LPT), are dynamic.
similarly start by designing a large and expressive
configuration space. We additionally provide the
motivation behind each decision to include a par-
ticular module and its components in the configura-
tion space, along with a mathematical formulation.
The search space is known to be one of the most
important factors in the performance of the config-
urations to be discovered subsequently (Ru et al.,
2020; Xie et al., 2019; Li and Talwalkar, 2019;
Dong and Yang, 2020; Yang et al., 2020). In or-
der to simultaneously maximise task performance
along with parameter efficiency, it is necessary to
first define a ‘parameter-reducible’ search space,
where each dimension within the space potentially
contributes to reducing the parameter budget. Simi-
larly, each dimension potentially impacts the perfor-
mance positively without introducing redundancy
in the space (Wan et al., 2022). Therefore, we
propose the following search space with represen-
tative PEFT modules spanning a plethora of (non-
redundant) configurations as illustrated in Figure 2:
PEFT Modules. Inspired by common practices in
NAS of using known well-performing modules as
building blocks, we include three distinctive PEFT
designs to efficiently adapt different forwarding
stages of hidden states in the PLM layers. We com-
bine Serial Adapters (SA), Parallel Adapters (PA),
and Prefix-Tuning (PT) as the three representative
modules in the search space as the building blocks,
where the PT module adapts the multi-head atten-
tion layer, and SA and PA interact with the FFN
layer (Figure 2). Each configuration makes a deci-
sion on the PEFT modules in the insertion layer: allof them can be ‘turned’ on or off. We combine this
binary decision with the actual non-binary decision
on the module size (see next) so that the value of 0,
in fact, denotes the absence of the modules in the
layer(s). We note that other PEFT modules such as
LoRA (Hu et al., 2022a) are scaled variants of PA
with the same insertion form (He et al., 2022). As
we empirically validate later, the resultant search
space spanned by the selected building blocks is
extremely expressive and flexible and enables the
discovery of configurations that outscore any of the
individual building blocks and other PEFT mod-
ules.
Size. Previous studies show that PEFT methods
are highly sensitive to the number of tunable pa-
rameters: adaptively setting their capacity in accor-
dance with the target task is, therefore, essential for
achieving good performance (Chen et al., 2022a).
The number of tunable parameters depends on each
particular module. The additional parameters in-
troduced by both SA and PA are dominated by
their bottleneck dimension D. Similarly, the size
of the PT module is defined by its prefix length
LPT. Thus, we define a binary logarithmic search
scale for the respective discrete sets DSA,DPA,
andLPT, spanning the values from 0 (absence of
the module) to Dhwhere Dhis the dimensionality
of the output embedding of the PLM (e.g. Dh=768
for BERT base).
Insertion Layers. Prior work has also shown that
different layers in the PLMs store different se-
mantic information (Vuli ´c et al., 2020), where the

--- PAGE 4 ---
PrefixSerial Parallel Layers
Fine-tune on target task(s)
Input : vector
representation
of configOutput : next config
to be evaluated by
maximising the
acquisition functionEvaluate
Target: Performance
(e.g. accuracy) and cost
(e.g. #params)A suggested config in the AutoPEFT  config space  (§2.1)
Pareto-optimal config search with
Bayesian optimisation  (§2.2)
Figure 3: Illustration of the Pareto-optimal search with
multi-objective Bayesian optimisation (BO; §2.2): The
BO agent trains on the vector representations of the eval-
uated configurations as inputs and their performance
under a low-fidelity setup (e.g. accuracy – obtained by
fine-tuning the language model with the PEFT config-
uration for a small number of iterations) and cost (e.g.
number of parameters) as targets. The BO agent then it-
eratively suggests new configurations until convergence.
higher layers produce more task-specific and con-
textualized representations (Tenney et al., 2019).
Therefore, as another configuration dimension, we
aim to search for the minimal number and the ac-
tual position of layers in which to insert the PEFT
modules. We define a binary ‘insertion’ decision at
each layer li.
Combining PEFT Modules. The SA module and
the PA module share a bottleneck architecture. The
SA receives hidden states from the FFN output as
its inputs, adapting it with a down-projection ma-
trixWdown
SA∈RDh×DSA, followed by a non-linear
activation function, and then an up-projection ma-
trixWup
SA∈RDSA×Dh:
fSA(h) = ReLU( hWdown
SA)Wup
SA. (1)
PA, on the other hand, receives its inputs from
hidden states before the FFN layer with the same
formulation:
fPA(x) = ReLU( xWdown
PA)Wup
PA. (2)
Therefore, it is able to act in parallel with the SA
without interference. Note that the FFN hidden
states h=F(x)contain the task-specific bias
learned in its pretrained weights. Therefore, by
combining SA with PA, the following compositionof functions is achieved:
fSAPA (x) =ReLU( F(x)Wdown
SA)Wup
SA
+ReLU( xWdown
PA)Wup
PA.(3)
The final composition should adapt effectively
to both bias-influence hidden states and the original
inputs before the pretrained FFN layer.*
Further, applying PEFT modules to interact with
FFNs and multi-head attention should positively
impact task performance (Mao et al., 2022; He
et al., 2022). PT learns two prefix vectors, Pk
andPv∈RLPT×Dh, that are concatenated with
the original multi-head attention’s key and value
vectors, which efficiently adapts the multi-head
attention layer to fit the target task. Thus, we finally
combine the SA and the PA (i.e., SAPA from above)
with PT.
In sum, the overview of the dimensions span-
ning the final configuration space is provided in
Figure 2. The combination of the different ‘con-
figuration dimensions’ outlined above gives rise to
a total of, e.g. 5,451,776 possible configurations
with BERT baseand∼3×1010configurations with
RoBERTa large(i.e. the number of configurations is
2|l|×|DSA|×|DPA|×|LPT|). While a large search
space is crucial for expressiveness and to ensure
that good-performing configurations are contained,
it also increases the difficulty for search strategies
to navigate the search space well while remaining
sample- and thus computationally efficient. Fur-
thermore, in the PEFT setting, we are also often
interested in discovering a family of configurations
that trade-off between performance and efficiency
for general application in various scenarios with
different resource constraints, thus giving rise to a
multi-objective optimisation problem where we si-
multaneously aim to maximise performance while
minimising costs. In what follows, we propose a
search framework that satisfies all those criteria.
2.2 Pareto-Optimal Configuration Search
Multi-objective Optimisation Formulation. The
ultimate goal of AUTOPEFT is to discover promis-
ing PEFT configuration(s) from the expressive
search space designed in §2.1, which is itself chal-
lenging. In this paper, we focus on an even more
challenging but practical goal: instead of aiming
*The PA module also acts as the low-rank reparameteri-
sation of the learned SA and the frozen FFN layer to further
match the intrinsic dimensionality of the target task.

--- PAGE 5 ---
Algorithm 1 Overall AUTOPEFT search pipeline.
1:Input: number of randomly initialising points N0,
maximum number of config evaluations N > N 0,
AUTOPEFT search space A.
2:Output: asetof Pareto-optimal configs A∗.
3:Initialise by sampling randomly at N0configu-
rations a∼ A and fine-tune the PLM to ob-
tainf(·)of the corresponding configs. Initialise
D0← {(ai,f(ai))}N0
i=1and fit a SAAS-GP model
onD0.
4:forn=N0, . . . , N do
5: Select the next configuration(s) to evaluate an
by maximising the NEHVI acquisition function
an= argmaxa∈Aα(a|Dn−1).
6: Fine-tune the PLM with candidate configura-
tion(s) a(possibly with low-fidelity estimates) to
obtain f(a)//Inner-loop optimisation in Eq. 4 .
7: Augment the observation data Dn← D n−1∪
(at,f(at))and update the SAAS-GP model.
8:end for
9:Return the set of non-dominated configurations
A∗⊆ {ai}N
i=1.
to find a single, best-performing PEFT configura-
tion, we aim to discover a family of Pareto-optimal
PEFT configurations that trade performance against
parameter-efficiency (or parameter cost) optimally:
one of the most impactful use cases of PEFT is
its ability to allow fine-tuning of massive language
models even with modest computational resources,
and thus we argue that searching Pareto-optimal
configurations is key as it allows tailored user- and
scenario-specific PEFT deployment depending on
the computational budget.
Formally, denoting the full AUTOPEFT search
space as Aand a single configuration a∈ A with
trainable weights W, without loss of generality,
assuming our objective is to maximise (i) a perfor-
mance metric f(a, W )(e.g. the accuracy on the
dev set) and to (ii) minimise a cost metric g(a)(e.g.
the number of parameters in a), a search method
aims to solve the bi-level, bi-objective optimisation
problem:
max
a∈A
f(a, W∗(a)),−g(a)
;
s.t.W∗(a) = arg min
WLtrain(a, W ),(4)
where the inner loop optimisation problem is the
optimisation of the configuration weights achieved
by fine-tuning the configuration aitself over the
training loss Ltrain. Given the bi-objective na-
ture of the problem, there is, in general, no sin-
gle maximiser of Eq. (4)but a setof Pareto-optimal configurations A∗={a∗
1, ..., a∗
|A∗|}. that
arenon-dominated : we say that a configuration
adominates another a′(denoted f(a′)≺f(a))
ifLval(a, W∗)≤ L val(a′, W∗)andg(a)≤g(a′)
and either Lval(a, W∗)<Lval(a′, W∗)org(a)<
g(a′). Denoting f(a) := [ Lval(a, W∗), g(a)]⊤,
the set of Pareto-optimal architectures A∗are those
that are mutually non-dominated: A∗={a∗
i∈
A |∄a′∈ A s.t.f(a′)≺f(a∗
i)}. The Pareto
front (PF) P∗is the image of the Pareto set of ar-
chitectures: P∗={f(a)|a∈A∗}.
Bayesian Optimisation (BO). To solve Eq. (4),
we adopt a BO approach, illustrated in Figure 3.
On a high level, BO consists of a surrogate model
that sequentially approximates the objective func-
tion based on the observations so far and an ac-
quisition function that is optimised at each iter-
ation to actively select the next configuration to
evaluate. Typically, the surrogate model is a Gaus-
sian process (GP), a flexible and non-parametric
model with well-principled and closed-form uncer-
tainty estimates: given an observed set of ncon-
figurations and their evaluated performance: Dn=
{ 
ai,f(ai)
}n
i=1, the GP surrogate model gives a
closed form posterior distribution P(f(a)|Dn)over
the true, unobserved function values fpotentially
over configurations that have notbeen evaluated
before. The acquisition function α:A →R, on
the other hand, uses the posterior distribution of the
surrogate model to assign a utility value to possible
configuration candidates in A, typically balancing
exploitation (i.e., querying near configurations in
{ai}n
i=1that were previously observed to be strong)
and exploration (i.e., the configurations far from
{ai}n
i=1and are those we do not have knowledge
on and can potentially be even better configura-
tions). At each step of BO, the acquisition function
is optimised (note that while evaluating f(a)is ex-
pensive, evaluating α(a|D), which only uses the
posterior distribution from the surrogate model, is
not) to select the next configuration (or batch of
configurations) an+1= arg max a∈Aα(a|Dn)to
evaluate. For a detailed overview of BO, we refer
the readers to Garnett (2023) and Frazier (2018).
Rationales for Using BO. We argue that BO is
well-suited to the task in principle and has various
advantages over alternative, viable approaches such
as those based on differentiable NAS (DARTS)
(Liu et al., 2019a), which typically utilise a con-
tinuous relaxation of the discrete configurations,
thereby allowing ato be jointly optimised with the

--- PAGE 6 ---
model weights Win Eq. 4 with a supernet:
First, unlike the DARTS-based approach, by
treating the optimisation problem defined in Eq. 4
as ablack box , BO decouples the optimisation of
the weights Wand the optimisation of architec-
turea, and solves the latter problem with no gra-
dient information at all (White et al., 2021; Ru
et al., 2021). This makes a BO-based solution
more parallelisable and more amenable to a dis-
tributed setup, which modern large PLMs often rely
on, as multiple configuration evaluations may take
place simultaneously in different client machines
as long as they can relay the evaluation results f
back to a central server running the BO. This fur-
ther contributes to memory efficiency, as unlike the
DARTS-based method that optimises a supernet
(a heavily over-parameterised network that can be
deemed as a weighted superposition of all configu-
rations in A), each parallel evaluation in BO trains
a single configuration only; we argue that this point
is particularly important for PEFT given its main
promise on parameter efficiency .
Second, as discussed, it is often desirable to
discover a family of configurations with different
trade-offs between performance and parameters in
different application scenarios. As we will show,
while BO generalises elegantly to handle vector-
valued objective functions and may generate a PF
of configurations in a single run , competing meth-
ods, such as supernet-based NAS methods, typi-
cally require a scalar objective function and thus
are limited to discovering a single best-performing
configuration (Eriksson et al., 2021; Izquierdo et al.,
2021); this means that one typically needs to run
the NAS pipeline multiple times for different cost
budgets in these methods.
Lastly, while one of the main arguments favour-
ing differentiable techniques is its lighter computa-
tional expense as one only needs to train the super-
net once rather than repeatedly training different
candidate configurations, as we will later show, the
sample-efficient nature of BO and strong transfer-
ability of the discovered configurations also ensure
that the computational cost of our proposed method
remains tractable. As we will show in §4, while
DARTS-based NAS is indeed a plausible approach
for PEFT configuration search, we show that our
approach performs competitively to S3PET (Hu
et al., 2022b), a DARTS-based method.
Adapting BO to the AUTO PEFT Task. Adapt-
ing BO to the high-dimensional and combinato-rialAUTOPEFT search space is non-trivial. To
address the challenges, we customise both compo-
nents of BO, and the overall pipeline is shown in
Algorithm 1. Instead of a standard GP, we propose
to use a Gaussian process with sparse axis-aligned
subspaces (SAAS-GP) (Eriksson and Jankowiak,
2021) as the surrogate model: As an intuitive expla-
nation, SAAS-GP places strong, sparsity-inducing
priors on the GP hyperparameters to alleviate the
difficulty in modelling high-dimensional data by
assuming that despite the high nominal dimen-
sionality, some search dimensions contribute much
more significantly to the variation of the objective
function than others – this assumption is shown to
hold in related problems of NAS in computer vision
(Wan et al., 2022) and discrete prompt search in
PLMs (Zhou et al., 2023), and we expect similar
findings in our particular case.
For the acquisition function, we use the
noisy expected hypervolume improvement (NE-
HVI) (Daulton et al., 2021) to handle the multi-
objective setting: unlike the commonly used scalar-
isation approach that transforms the vector-valued
objective function to a scalar weighted sum (which
corresponds to a single point on the PF), NEHVI
is capable of automatically exploring all parts of
the PF in a single run. Lastly, we additionally use
low-fidelity approximations, a popular low-cost per-
formance estimation strategy in NAS (Elsken et al.,
2019), to manage the search cost: at search-time,
instead of fine-tuning each candidate PEFT con-
figuration in full, we only fine-tune with a much
smaller number of iterations (5% of full) – this is
possible as we are only interested in the relative
ranking (rather than the performance itself) of the
different configurations during search. Consistent
with NAS literature, we also find the low-fidelity
estimate to provide a reliable ranking, with the
best-performing configurations in low fidelity also
performing the best under fine-tuning with the full
number of iterations. As we will show in §5, us-
ing the low-fidelity search pipeline, in combination
with the strong transferability of the discovered
configurations, AUTOPEFT only incurs an addi-
tional one-off ,1.9% of the total GLUE fine-tuning
cost, but delivers significant performance gains.
3 Related Work
PEFT Methods in NLP. Standard PEFT methods
can be divided into two main groups (Pfeiffer et al.,
2023). 1)Some methods fine-tune a small por-

--- PAGE 7 ---
tion of pretrained parameters (Zhao et al., 2020;
Guo et al., 2021). For instance, Ben Zaken et al.
(2022) propose to fine-tune the PLM’s bias terms,
while Sung et al. (2021) and Ansell et al. (2022)
fine-tune sparse subnetworks withing the original
PLM for a particular task. 2)Other methods fine-
tune an additional set of parameters (Liu et al.,
2022). Since there is no interference with the pre-
trained parameters, this class of PEFT modules, be-
sides offering strong task performance, is arguably
more modular; we thus focus on this class of PEFT
methods in this work. The original adapter mod-
ules (Houlsby et al., 2019; Pfeiffer et al., 2020b)
have a bottleneck serial architecture which can be
inserted into every Transformer layer, see Figure 2.
LoRA (Hu et al., 2022a) assumes the low-rank
intrinsic dimensionality of the target task and per-
forms low-rank updates (Mahabadi et al., 2021).
Li and Liang (2021) propose the Prefix-Tuning
method that appends a learnable vector to the at-
tention heads at each Transformer layer. Similarly,
prompt-tuning (Lester et al., 2021) only appends
this vector to the input embedding. UniPELT (Mao
et al., 2022) integrates multiple PEFT modules with
a dynamic gating mechanism. He et al. (2022)
provide a unified formulation of existing PEFT
modules and propose a parallel adapter module,
along with a combined ‘Mix-and-Match Adapter
(MAM)’ architecture that blends parallel adapters
and prefix-tuning. Wang et al. (2022) propose the
mixture-of-adaptations (AdaMix) architecture with
weight averaging for a mixture of adapters.
Optimising Parameter Efficiency in PEFT. Re-
cent work further aims to optimise the parame-
ter efficiency of existing PEFT modules while
maintaining task performance. The standard ap-
proach is to insert (typically serial) adapters into
all Transformer layers, which still requires a size-
able parameter budget. Rücklé et al. (2021) ad-
dress this question by randomly dropping adapters
from lower-level layers, displaying only a small
decrease in task performance. Adaptable Adapters
(AA) (Moosavi et al., 2022) generalise this idea
by learning gates that switch on or off adapters
in particular Transformer layers. Neural Architec-
ture Search (NAS) methods aim to automate the
design of neural net architectures themselves, and
NAS has seen great advances recently, with per-
formance often surpassing human expert-designed
architectures in various tasks (Zoph and Le, 2017;
Ren et al., 2021; Elsken et al., 2019). ConcerningNLP tasks and PEFT, Hu et al. (2022b) propose
S3PET, which adapts Differentiable Architecture
Search (DARTS) (Liu et al., 2019a) to learn the
positions for inserting the PEFT modules. This
work is closest in spirit to ours and is empirically
compared to in §4. Conceptually, however, as dis-
cussed in detail in §2, we argue that our method
offers a spectrum of advantages over S3PET and
other related PEFT work, including but not limited
to the ability to automatically discover a family of
PEFT configurations across parameter budgets in
a single run, better parallelisability and memory
efficiency. Other concurrent work (Valipour et al.,
2023; Zhang et al., 2023) also approaches the same
problem by dynamic budget allocation mechanisms
on a single PEFT module within a limited search
space. Nonetheless, this field still lacks a compact
solution for automatically configuring a complex
space of PEFT modules (Chen et al., 2023).
4 Experimental Setup
Evaluation Data. We follow prior PEFT research
and base our evaluation on the standard and estab-
lished GLUE and SuperGLUE benchmarks. For
GLUE, we include 4 types of text classification
tasks, including linguistic acceptability: CoLA;
similarity and paraphrase: STS-B, MRPC, QQP;
sentiment analysis: SST-2; natural language infer-
ence: RTE, QNLI, MNLI. We exclude WNLI fol-
lowing previous work (Houlsby et al., 2019; Mao
et al., 2022). We also include CB, COPA, WiC,
and BoolQ from SuperGLUE to further validate
the transferability of AUTOPEFT -found configura-
tion across different tasks and datasets.
Baselines. We compare the performance of the
AUTOPEFT -found configurations to the standard
full model FT and each individual PEFT module
(SA, PA, PT) from the search space used in their
default setup from their respective original work.
We also compare with the LoRA module to provide
a comparison to low-rank decomposition methods.
To compare with recent methods that also integrate
multiple PEFT modules (see §3), we further in-
clude the UniPELT and the MAM adapter in their
default settings. We reproduce AdaMix for a com-
parison to a mixture of homogeneous adaptations.
In ablations on insertion layers, we also include
the Adaptable Adapter (AA) as a baseline that pro-
poses a differentiable gate learning method to select
the insertion layer for PEFT modules (i.e. serial
adapters originally). On T5 (Raffel et al., 2020b)

--- PAGE 8 ---
Method #Param. RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI Avg.
FFT 100% 71.12 1.46 85.74 1.75 89.00 0.45 59.32 0.62 92.57 0.24 91.50 0.08 91.52 0.04 84.43 0.22 83.15
Prefix 0.17% 70.54 0.49 85.93 0.89 88.76 0.15 58.88 1.15 91.93 0.45 90.76 0.14 89.12 0.07 82.78 0.16 82.33
LoRA 0.27% 65.85 1.49 84.46 1.04 88.73 0.08 57.58 0.78 92.06 0.38 90.62 0.22 89.41 0.04 83.00 0.07 81.46
Serial 0.81% 68.01 1.34 84.75 0.45 88.61 0.11 59.73 0.62 91.93 0.33 91.06 0.12 90.52 0.05 84.18 0.22 82.35
AdaMix 0.81% 70.11 0.62 86.86 1.12 89.12 0.11 59.11 1.00 92.06 0.22 91.52 0.15 90.22 0.04 84.25 0.14 82.91
UniPELT 1.25% 67.07 1.82 84.22 0.78 88.84 0.11 60.13 0.46 92.52 0.24 91.09 0.13 90.69 0.11 84.28 0.18 82.35
Parallel 6.46% 68.52 3.44 86.52 0.96 88.90 0.28 58.72 1.69 92.13 0.35 90.83 0.22 90.74 0.08 73.93 19.24 81.29
MAM 6.97% 69.10 1.76 87.16 0.74 89.01 0.48 47.87 23.97 83.94 16.52 90.85 0.22 90.76 0.05 83.31 0.17 80.25
AUTOPEFTRTE0.76% 72.20 0.72 87.16 0.83 88.77 0.07 60.30 1.24 92.22 0.30 90.90 0.10 90.37 0.06 83.46 0.21 83.17
AUTOPEFTtask
Avg. 1.40% 72.35 0.94 87.45 0.87 89.17 0.24 60.92 1.47 92.22 0.30 91.12 0.13 90.64 0.05 84.01 0.10 83.49
Table 1: Results on the GLUE benchmark with BERT base(tasks are ranked in ascending order of training resources
required from left to right). For AUTOPEFTRTE, we search on RTE with a low-fidelity proxy, training for 1 epoch
per iteration, only at a search cost of 1.9% (in terms of additional fine-tuning steps required) over the full GLUE
experiment . We report the average fine-tuned parameters of per-task AUTOPEFT , where we conduct additional
per-task searches on RTE, MRPC, STS-B, and CoLA, and take best-found configurations for the remaining tasks.
We report Spearman’s Correlation for STS-B, Matthew’s Correlation for CoLA, and accuracy for all other tasks
(matched accuracy for MNLI). The percentage of parameters is the ratio of the number of additional parameters to
the pretrained parameters. We reproduce all baselines and report the mean and standard deviation of all results for 5
random seeds. The best,second-best , and third-best results are marked in bold fonts and ranked by colour.
Task %Param. Active PEFT Submodule Value
Layers li
RTE 0.76%3, 4,
8, 9, 10DSA(Serial) 12
DPA(Parallel) 96
LPT(Prefix) 1
Table 2: Specification of the discovered configuration
reported in Table 1 (A UTOPEFTRTE) using BERT base.
models, we also compare against S3PET (Hu et al.,
2022b), one of the most similar works to us that
use differentiable NAS for configuration search.
Implementation Details. Following previous work
on the GLUE benchmark, we report the best GLUE
dev set performance (Ben Zaken et al., 2022) and
use 20 training epochs with an early stopping
scheme of 10 epochs for all per-task experiments.
We use AdapterHub (Pfeiffer et al., 2020a) as the
codebase and conduct extensive experiments with
the uncased BERT base(Devlin et al., 2019) as the
main backbone model. We report main experiments
with the mean and standard deviation over 5 differ-
ent random seeds. Following Pfeiffer et al. (2020b),
we use a recommended learning rate of 10−4for
all PEFT experiments. We use the learning rate
of2×10−5for full model FT according to Mao
et al. (2022). We use batch sizes 32 and 16 for
all BERT and RoBERTa experiments, respectively.
The optimiser settings for each PEFT module fol-
low the default settings in AdapterHub (Pfeiffer
et al., 2020a). We implement the BO search al-
gorithm in BoTorch (Balandat et al., 2020) and
use the recommended settings from Eriksson andJankowiak (2021) for the surrogate. For acquisi-
tion function optimisation, we use a local search
method similar to previous literature with a simi-
lar setup (Wan et al., 2021; Eriksson et al., 2021):
at each search iteration (after the initial randomly
sampled points), we collect the Pareto-optimal ar-
chitectures up to this point. From this collection
of Pareto-optimal architectures, we perform a local
search by evaluating the acquisition function val-
ues of their neighbours and move the current point
to a neighbour with a higher acquisition function
value, and this process is repeated until conver-
gence. Due to the relatively noisy nature of the
problem, we use 100 random initialisation points
for all experiments, followed by 100 BO iterations.
We further show results using RoBERTa large(Liu
et al., 2019b) in Table 5, which shows findings that
are consistent with the BERT base. In experiments
with RoBERTa largeas the underlying PLM, we re-
port the RTE results with a learning rate of 2×10−5
forAUTOPEFTMRPCandAUTOPEFTCoLA;10−4
forAUTOPEFTRTE. We use batch size 16 and a
learning rate of 3×10−4for T5 baseexperiments by
AUTOPEFT with the SAPA space; 10−5for STS-
B. We reproduce S3PET results with batch size 8
in the same experimental setup as A UTOPEFT.
5 Results and Discussion
Discussion of Main Results. The main results on
BERT are summarised in Table 1 where we evalu-
ate the AUTOPEFT -found configurations searched
from RTE, the most low-resource and challenging

--- PAGE 9 ---
102
101
100
Fine-tuned Parameters (%)62.565.067.570.072.575.0T ask Score
RTE
102
101
100
Fine-tuned Parameters (%)758085 
MRPC
Serial Parallel Prefix LoRA AutoPEFT102
101
100
Fine-tuned Parameters (%)87.087.588.088.589.089.5 
STS-B
102
101
100
Fine-tuned Parameters (%)50.052.555.057.560.062.5 
CoLAFigure 4: Pareto Fronts of AUTOPEFT on four tasks compared to baselines on BERT base, over varying parameter
budgets. We report the single-seed task score but otherwise follow the settings in Table 1.
Method CB COPA WiC BoolQ Avg.
FFT 71.43 1.13 51.80 3.76 68.62 1.93 72.17 0.86 66.01
LoRA 67.14 2.42 55.80 1.47 68.56 1.11 69.09 0.42 65.15
Serial 67.86 1.13 54.20 7.68 67.34 0.61 70.00 0.85 64.86
OursRTE71.07 2.86 56.40 6.83 68.87 1.06 70.86 0.89 66.80
Table 3: Results on SuperGLUE tasks with AU-
TOPEFT -discovered configurations searched on RTE
with BERT baseas the underlying PLM. We split 10% of
the training set as the new validation set and report the
AUTOPEFTRTE-found configuration transfer results on
the evaluation set over five random seeds.
task, on the full GLUE suite. We further report
selected GLUE tasks on T5 in Table 4 (where we
also compare against S3PET) and RoBERTa largein
Table 5. For simplicity, we report a single config-
uration that leads to the highest task performance
in a predefined, user-specified parameter budget
from the discovered Pareto-optimal set in Table 1,
whereas the full Pareto-optimal set is evaluated in
Figure 4. On BERT (Table 1, we find that using
only 0.76% of parameters, AUTOPEFTRTEoutper-
forms all the PEFT baselines (more than 2% on
RTE). The AUTOPEFT -found configuration also
outperforms the full-model FT baseline on the RTE
task by more than 1%. These results indicate the
effectiveness of the AUTOPEFT framework in op-
timising both task performance and parameter effi-
ciency. Transferring the RTE-based configurations
to other tasks, we find that strong performance is
maintained across the target tasks, with more bene-
fits on the medium-resource tasks (MRPC, STS-B,
CoLA), but the configuration remains competitive
also for higher-resource tasks (e.g. QQP, MNLI).
Finally, we find the strength of AUTOPEFT to
persist in RoBERTa and T5 as a representative of
the encoder-decoder model families. It is particu-
larly noteworthy that in addition to outperforming
the baseline PEFT methods without configuration
RTE MRPC STS-B CoLA (Avg.)OursRTE
OursMRPC
OursSTSB
OursCoLA72.35 86.13 89.06 60.23 76.94
71.75 87.45 89.02 59.08 76.82
71.57 85.54 89.17 60.80 76.77
70.85 86.70 88.93 60.92 76.85Figure 5: Pairwise transferability study of AUTOPEFT -
discovered configurations: each row (Ours[task]) de-
notes the performances of the AUTOPEFT configura-
tion searched from [task] (e.g. RTE) to the task itself
and 3 other GLUE tasks. The results suggest that AU-
TOPEFT performance is largely robust to the choice of
which task to search on.
search, AUTOPEFT also performs competitively
compared to S3PET with configuration search un-
der a comparable parameter count, even though
S3PET was exclusively developed and tested on
the T5 search space and that the S3PET search
space was designed with meticulous hand-tuning ,
where the authors manually excluded several build-
ing blocks that did not lead to empirical gain; this
provides further empirical support to the strength
of a BO-based search strategy described in §2.2.
Table 2 specifies the composition of the found
configuration, indicating the exact task-active lay-
ers while allocating more parameter budget to the
efficient and effective PA module. On average,
theAUTOPEFTRTEconfiguration shows a com-
parable fine-tuning performance (83.17) to FFT
(83.15) by only updating 0.76% of parameters.
With strong transferability across similar tasks, AU-
TOPEFT provides distinct advantages in parameter
efficiency; the search algorithm itself, coupled with
the transfer, becomes more sample-efficient within
limited training resources.

--- PAGE 10 ---
Method #Param. RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI Avg.
LoRA 0.40% 80.1 89.5 89.2 59.9 94.4 93.6 91.0 86.5 85.5
Serial 0.79% 78.0 88.2 89.1 60.6 94.6 93.1 90.7 86.4 85.1
S3PETRTE0.30% 79.8 89.0 90.2 58.6 94.2 93.3 90.6 86.5 85.3
AUTOPEFTRTE0.33% 82.7 89.0 89.6 61.7 94.6 93.3 90.8 86.7 86.1
Table 4: Experimental results on GLUE with T5 base. We report comparisons of in-task search performance and
transfer performance between the architectures found by AUTOPEFT and the state-of-the-art baseline S3PET in a
constrained parameter budget. Consistent with Table 1, we report AUTOPEFT and S3PET results searched on RTE
in full-resource settings that are then transferred to all other included GLUE tasks.
Method #Param. RTE MRPC STS-B CoLA SST-2 QNLI Avg.
FFT†100% 86.6 90.9 92.4 68.0 96.4 94.7 88.2
LoRA‡0.22% 85.2 90.2 92.3 68.2 96.2 94.8 87.8
Serial 0.89% 84.8 90.2 92.0 66.8 96.3 94.7 87.5
AUTOPEFTRTE0.03% 88.1 89.5 92.3 67.0 96.0 94.6 87.9
AUTOPEFTtask
Avg. 0.88% 88.1 92.2 92.4 70.6 96.8 94.6 89.1
Table 5: Experimental results on GLUE with RoBERTa large. We report the full model fine-tuning†results from Liu
et al. (2019b) with Pearson correlation for STS-B. We include the LoRA‡module performance from Hu et al.
(2022a). We exclude QQP and MNLI tasks due to the high computation cost of RoBERTa large. Consistent with
Table 1, we again report AUTOPEFT results searched on RTE in full-resource settings that are then transferred all
included GLUE tasks (A UTOPEFTRTE) and per-task A UTOPEFT (A UTOPEFTtask
Avg.) but on RoBERTa large.
Extending AUTOPEFT to More Tasks. We next
‘stress-test’ the ability of AUTOPEFT -found con-
figuration in a more challenging scenario, exper-
imenting on a completely new set of dissimilar
tasks. Table 3 reports the results of transferring
AUTOPEFTRTEfrom Table 1 to four SuperGLUE
tasks. In terms of parameter efficiency , we observe
consistent patterns as in Table 1 before, where our
plug-and-play PEFT configuration outperforms ex-
isting PEFT baselines by a substantial margin (2%)
on average while being comparable to the costly
full-model FT.*In terms of search cost , we re-
call that through the use of low-fidelity proxy and
the strong transferability, AUTOPEFTRTEin Ta-
ble 1 only requires an additional, one-off 1.9% in
terms of training time (or equivalently the number
of fine-tuning steps) of that of single-seed train-
ing of the GLUE training sets. Furthermore, Fig-
ure 5 demonstrates the robustness of our frame-
work to the choice of the source task to search
on. Therefore, our framework is task-agnostic with
a cheap one-time cost but yields ‘permanent’ im-
provement towards all efficiency metrics for PEFT:
space, time, and memory.
Per-Task Search. We further conduct full-resource
*With the AUTOPEFT -found off-the-shelf configuration,
this requires no additional search cost and enables a more
efficient and effective tuning approach for new tasks.per-task AUTOPEFT searches. While naturally
more expensive, we argue this setup is useful if,
for example, one is interested in finding absolutely
the best configurations for that particular task and
where search cost is less of a concern. Due to
computational constraints, we search per-task on
RTE, MPRC, STS-B, and CoLA, then port the
small set of best configurations to the remaining
higher-resource tasks (SST-2, QNLI, QQP, MNLI).
We observe consistent gain in all tasks we search
on over the best-performing PEFT baselines, e.g.
MRPC (87.16% ( best baseline ) to 87.45% ( ours))
and CoLA (60.13% to 60.92%), and also the trans-
ferred configuration AUTOPEFTRTEin Table 1.
One interpretation is that while configurations are
highly transferable, the optimal configurations may
nonetheless differ slightly across tasks such that
while transferred AUTOPEFT configurations (e.g.
the one reported in Table 1) perform well, search-
ing per-task performs the best. Crucially, we also
find per-task AUTOPEFT in this setup to even out-
perform FFT, despite only using 1.4% of all param-
eters , except for the high-resources task where we
mostly perform on par; this is consistent with our
observations that similar to the baselines, due to
the richness of training resources, the performance
may be mostly saturated and PEFT methods often
achieve on-par performance to FFT at most.

--- PAGE 11 ---
layer_1
layer_2
layer_3
layer_4
layer_5
layer_6
layer_7
layer_8
layer_9
layer_10
layer_11
layer_12
d_pa
l_pt
d_sa Increasing #Param
CoLA
layer_1
layer_2
layer_3
layer_4
layer_5
layer_6
layer_7
layer_8
layer_9
layer_10
layer_11
layer_12
d_pa
l_pt
d_saMRPC
layer_1
layer_2
layer_3
layer_4
layer_5
layer_6
layer_7
layer_8
layer_9
layer_10
layer_11
layer_12
d_pa
l_pt
d_saRTE
layer_1
layer_2
layer_3
layer_4
layer_5
layer_6
layer_7
layer_8
layer_9
layer_10
layer_11
layer_12
d_pa
l_pt
d_saSTS-B
MinMaxFigure 6: Visualisation of the BO discovered Pareto-optimal sets of configurations A∗in different tasks (i.e., the
configurations on the PFs in Figure 4) in ascending order of parameter budget. layer_i denotes the binary choice
of whether the PEFT module is active in the i-th layer of the PLM. The final 3 columns denote DSA, DPAandLPT
respectively, and feature a range of possible values from 0 to 768.
102
101
100101
Fine-tuned Parameters (%)60657075Accuracy (%)Initialisation
Random Search
AutoPEFT
Figure 7: The distribution of the discovered configura-
tions via BO (orange), described in §2.2 and random
search (grey) using the same total number of evaluations
(200). Both searches use the same 100 random initial-
ising points (blue) on RTE. Note that BO-generated
configurations typically have much better parameter ef-
ficiency for configurations with similar accuracy.
Analysing the ‘Behaviour’ of BO and the Discov-
ered Configurations. Figure 7 shows the distribu-
tion of AUTOPEFT -found configurations when we
conduct its search experiment on RTE. Recalling
that the search strategy (§2.2) starts with random
initialisation, we compare the behaviours of the ran-
dom explorations and the BO-suggested configura-
tions: whereas the random search baseline is purely
exploratory and discovers less parameter-efficient
configurations, BO succeeds in discovering con-
figurations towards the regions with improved pa-
rameter efficiency. The superiority of BO over
the random search baseline is further demonstrated
quantitatively by Figure 8 where we compare the
evolution of the hypervolume , which measures the
size of the space enclosed by the Pareto front over
a reference point (set to the nadir point of the op-
timisation trajectory) (Zitzler and Thiele, 1998),
discovered by BO and random search as a func-
tion of the number of configurations evaluated; it
0 20 40 60 80 100
# Function Evaluations380382384386Hypervolume
Random Search
AutoPEFTFigure 8: The hypervolumes of the Pareto-optimal con-
figurations discovered by BO (orange) and random
search (grey) as a function of the number of config-
urations evaluated.
is clear that as optimisation proceeds, BO finds
a better Pareto set with a better trade-off between
performance and cost in the end. BO eventually dis-
covers a rich family of PEFT configurations across
a wide range of parameters, whereas previous ap-
proaches typically fail to explore the entire PF. This
is a critical strength motivating our BO search strat-
egy. Figure 6, on the other hand, visualises the
discovered sets in different tasks: we observe that
within the Pareto-optimal configuration set of the
same task, some layers are consistently enabled
(e.g., Layer 2 in CoLA) whereas some are consis-
tently disabled (e.g., Layer 1 across all tasks) even
under very different cost budgets; this suggests
PEFT modules in different layers are not equally
important, and by selectively enabling them, AU-
TOPEFT is capable of making better use of the
parameter budgets by allocating them to the more
beneficial Transformer layers only. We observe the
unanimity of preference or disinclination towards
certain layers extends even across tasks that are
unlikely to stem from randomness only: for exam-
ple, we found Layers 2 and 10 are enabled in 71.2%

--- PAGE 12 ---
0 2 4 6
Fine-tuned Parameters (%)65.067.570.072.575.0Accuracy (%)
SA
SA-Layer
SA-PA-Layer
PA-PT-Layer
Scaling
AutoPEFTFigure 9: The performance of AUTOPEFT with ablation
of search space on RTE on BERT base. The SA results re-
fer to the Pfeiffer adapter (Pfeiffer et al., 2020b) with an
enumeration of its bottleneck size. The Scaling results
refer to the PF where smaller configurations are obtained
by simply scaling the largest configuration in Aover all
search dimensions. We report the PF of AUTOPEFT -
found configurations, where SA-PA-PT-Layer forms
the search space of A UTOPEFT.
and 69.2% in all Pareto-optimal configurations over
all tasks, whereas Layers 1 and 12 are enabled in
only 7.7% and 13.4% of the time, respectively. We
also observe that across all tasks, a common trend
is that sequential and prefix adapters are univer-
sally preferred in low-budget ranges, and parallel
adapters are only enabled when we have a more
lenient budget allowance; these commonalities in
high-performing configurations may, to some ex-
tent, account for the strong transferability of the
discovered configurations, as shown in Figure 5.
Ablation of the Configuration Space. To pro-
vide a finer-grained analysis of factors that bring
positive impact to AUTOPEFT , we ablate the AU-
TOPEFT search space from the full configuration
space: 1) to the basic enumeration of the bottle-
neck size DSAof the SA only (the SAspace); 2)
a naïve baseline where instead of searching for
each search dimension independently, we vary a
single, common coefficient that generates a family
of configurations of different sizes by scaling from
the largest PEFT configuration in our search space
(SA-PA-PT) over DSA, DPAandLPT. We then in-
clude the Transformer layer and the SA size into
the search space (the SA-Layer space) to validate
the usefulness of layer selection as one configura-
tion dimension. We can then also expand the search
space by adding another module (e.g. PA yields
theSA-PA-Layer space). Figure 9 plots the perfor-
mance over the ablated configuration spaces and
different parameter budgets. Several key findings
emerge. First, combining multiple single PEFTMethod #Layers Size DSA RTE
Serial 24 64 72.56 0.76
Adaptable Adapter 13 128 73.36 0.80
AdapterDrop 13 128 73.50 1.40
AUTOPEFTSA
Layer 10 128 73.86 0.94
Table 6: Comparing AUTOPEFT to layer selection base-
lines with the same parameter budget on BERT large. We
report the Pfeiffer adapter for all 24 layers ( Serial ),
specialised AdapterDrop (Rücklé et al., 2021) that in-
serts SA for the last 13 layers, and AAuni(Moosavi
et al., 2022) without its rational activation function with
13 selected layers ( Adaptable Adapter ). We run our
AUTOPEFT under the comparable search space of 24
layers and approximately match the size of Serial .
modules has a positive impact on AUTOPEFT in
general (c.f. full AUTOPEFT vs.SA-PA-Layer
vsSA-Layer ). Second, simply scaling all search
dimensions by a common scaling factor is sub-
optimal. This is likely because not all parameters
are equally important, necessitating a configura-
tion search. Relying on layer selection also brings
benefits (c.f. SAvs.SA-Layer ). The comparison
indicates that leaving out Transformer layers while
increasing the capacity of the PEFT module is a
straightforward method to improve the parameter
efficiency and task performance of the PEFT mod-
ule within a fixed parameter budget. The ablation
results also demonstrate that AUTOPEFT is search
space-agnostic, capable of effectively operating
over configuration spaces of different granularity.
Layer Selection. The ability to disable some
PEFT layers altogether is a key novelty of the
AUTOPEFT search space, and to further compare
different layer selection approaches, we conduct
a controlled experiment with the SA module on
BERT large (24 Transformer layers) under a pre-
defined parameter budget. In Table 6, we com-
pare against AdapterDrop, which simply drops the
adapters for the first 11 layers while doubling their
bottleneck sizes, and, within the same architec-
ture, we also include the Adaptable Adapter with
selected layers from switch learning (3 and 10
layers from the first 12 and the other 12 layers,
respectively). We show that AUTOPEFT outper-
forms existing layer selection baselines activating
fewer PEFT layers, leading to better parameter effi-
ciency (12.5% fewer parameters in relative terms)
yet achieving better performance. It indicates that
selecting the best insertion layer is non-trivial, and
AUTOPEFT can efficiently learn the correlation
between layers.

--- PAGE 13 ---
6 Conclusion
We proposed AUTOPEFT , a novel search frame-
work for automatically configuring parameter-
efficient fine-tuning (PEFT) modules of pretrained
language models. AUTOPEFT features both a
large and expressive, newly designed configura-
tionsearch space and an effective search method
featuring Bayesian optimisation that discovers a
Pareto-optimal set of novel PEFT configurations
with promising performance-efficiency trade-offs.
Empirically, we demonstrated that AUTOPEFT -
discovered configurations transfer strongly across
different GLUE and SuperGLUE tasks, outper-
forming various strong PEFT baselines and being
competitive to full model fine-tuning.
Limitations and Future Work
AUTOPEFT search inevitably incurs a search cost
since it requires iterative optimisation at search
time. However, we mitigate this by (i) using a low-
fidelity proxy of 1-epoch training and (ii) lever-
aging strong transferability by generalising from
low-resource and, thus, quick-to-train tasks. While
the search itself can be seen as a one-time cost
yielding a permanent well-performing and share-
able configuration for particular tasks, we plan to
delve deeper into further optimising the search cost
in future work.
Furthermore, while we conduct extensive experi-
ments on the search space that contains three exist-
ing PEFT modules as building blocks, novel PEFT
modules may emerge. However, AUTOPEFT
framework is general, so we may easily integrate
these forthcoming new modules. We defer thor-
ough investigations to future work.
Acknowledgements
Han Zhou is supported by the UK Research
and Innovation (UKRI) Frontier Research Grant
EP/Y031350/1 (the UK government’s funding
guarantee for ERC Advanced Grants) awarded to
Anna Korhonen at the University of Cambridge.
Xingchen Wan is supported by the Clarendon
Scholarship at the University of Oxford. The work
has been supported in part by a Royal Society Uni-
versity Research Fellowship (no 221137; 2022-)
awarded to Ivan Vuli ´c, and by the UK EPSRC grant
EP/T02450X/1. We thank TACL editors and anony-
mous reviewers for their constructive feedback that
enabled us to strengthen our work.References
Alan Ansell, Edoardo Ponti, Anna Korhonen, and
Ivan Vuli ´c. 2022. Composable sparse fine-
tuning for cross-lingual transfer. In Proceedings
of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers) , pages 1778–1796, Dublin, Ireland. As-
sociation for Computational Linguistics.
Maximilian Balandat, Brian Karrer, Daniel Jiang,
Samuel Daulton, Ben Letham, Andrew G Wil-
son, and Eytan Bakshy. 2020. Botorch: A frame-
work for efficient monte-carlo bayesian optimiza-
tion. Advances in neural information processing
systems , 33:21524–21538.
Elad Ben Zaken, Yoav Goldberg, and Shauli
Ravfogel. 2022. BitFit: Simple parameter-
efficient fine-tuning for transformer-based
masked language-models. In Proceedings of
the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Pa-
pers) , pages 1–9, Dublin, Ireland. Association
for Computational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Infor-
mation Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual .
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and
Shangsong Liang. 2022a. Revisiting parameter-
efficient tuning: Are we really there yet? In Pro-
ceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
2612–2626, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li,
Alex Smola, and Diyi Yang. 2023. Parameter-
efficient fine-tuning design spaces. In The

--- PAGE 14 ---
Eleventh International Conference on Learning
Representations .
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu
Wang, Yibing Song, Jue Wang, and Ping Luo.
2022b. Adaptformer: Adapting vision trans-
formers for scalable visual recognition. In Ad-
vances in Neural Information Processing Sys-
tems 35: Annual Conference on Neural Infor-
mation Processing Systems 2022, NeurIPS 2022,
New Orleans, LA, USA, November 28 - Decem-
ber 9, 2022 .
Samuel Daulton, Maximilian Balandat, and Eytan
Bakshy. 2021. Parallel bayesian optimization
of multiple noisy objectives with expected hy-
pervolume improvement. In Advances in Neu-
ral Information Processing Systems 34: Annual
Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , pages 2187–2200.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long
and Short Papers) , pages 4171–4186, Minneapo-
lis, Minnesota. Association for Computational
Linguistics.
Xuanyi Dong and Yi Yang. 2020. Nas-bench-201:
Extending the scope of reproducible neural archi-
tecture search. In 8th International Conference
on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 .
Thomas Elsken, Jan Hendrik Metzen, and Frank
Hutter. 2019. Neural architecture search: A sur-
vey. The Journal of Machine Learning Research ,
20(1):1997–2017.
David Eriksson, Pierce I-Jen Chuang, Samuel
Daulton, Peng Xia, Akshat Shrivastava, Arun
Babu, Shicong Zhao, Ahmed A Aly, Ganesh
Venkatesh, and Maximilian Balandat. 2021.
Latency-aware neural architecture search with
multi-objective bayesian optimization. In 8th
ICML Workshop on Automated Machine Learn-
ing (AutoML) .
David Eriksson and Martin Jankowiak. 2021. High-
dimensional bayesian optimization with sparseaxis-aligned subspaces. In Uncertainty in Artifi-
cial Intelligence , pages 493–503. PMLR.
Peter I. Frazier. 2018. A tutorial on bayesian opti-
mization. CoRR , abs/1807.02811v1.
Roman Garnett. 2023. Bayesian Optimization .
Cambridge University Press.
Demi Guo, Alexander Rush, and Yoon Kim. 2021.
Parameter-efficient transfer learning with diff
pruning. In Proceedings of the 59th Annual
Meeting of the Association for Computational
Linguistics and the 11th International Joint Con-
ference on Natural Language Processing (Vol-
ume 1: Long Papers) , pages 4884–4896, Online.
Association for Computational Linguistics.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor
Berg-Kirkpatrick, and Graham Neubig. 2022.
Towards a unified view of parameter-efficient
transfer learning. In The Tenth International
Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzeb-
ski, Bruna Morrone, Quentin de Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain
Gelly. 2019. Parameter-efficient transfer learn-
ing for NLP. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California,
USA, pages 2790–2799.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2022a. LoRA: Low-rank
adaptation of large language models. In Interna-
tional Conference on Learning Representations .
Shengding Hu, Zhen Zhang, Ning Ding, Yadao
Wang, Yasheng Wang, Zhiyuan Liu, and
Maosong Sun. 2022b. Sparse structure search
for delta tuning. In Advances in Neural Informa-
tion Processing Systems .
Sergio Izquierdo, Julia Guerrero-Viu, Sven Hauns,
Guilherme Miotto, Simon Schrodi, André
Biedenkapp, Thomas Elsken, Difan Deng, Mar-
ius Lindauer, and Frank Hutter. 2021. Bag of
baselines for multi-objective joint neural archi-
tecture search and hyperparameter optimization.
In8th ICML Workshop on Automated Machine
Learning (AutoML) .

--- PAGE 15 ---
Brian Lester, Rami Al-Rfou, and Noah Constant.
2021. The power of scale for parameter-efficient
prompt tuning. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Lan-
guage Processing , pages 3045–3059, Online and
Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Liam Li and Ameet Talwalkar. 2019. Random
search and reproducibility for neural architec-
ture search. In Proceedings of the Thirty-Fifth
Conference on Uncertainty in Artificial Intelli-
gence, UAI 2019, Tel Aviv, Israel, July 22-25,
2019 , pages 367–377.
Xiang Lisa Li and Percy Liang. 2021. Prefix-
tuning: Optimizing continuous prompts for gen-
eration. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Lin-
guistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers) , pages 4582–4597, Online. As-
sociation for Computational Linguistics.
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
2019a. DARTS: differentiable architecture
search. In 7th International Conference on
Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019 .
Haokun Liu, Derek Tam, Muqeeth Mohammed,
Jay Mohta, Tenghao Huang, Mohit Bansal,
and Colin Raffel. 2022. Few-shot parameter-
efficient fine-tuning is better and cheaper than
in-context learning. In Advances in Neural In-
formation Processing Systems .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
2019b. Roberta: A robustly optimized BERT
pretraining approach. CoRR , abs/1907.11692v1.
Rabeeh Karimi Mahabadi, James Henderson, and
Sebastian Ruder. 2021. Compacter: Efficient
low-rank hypercomplex adapter layers. In Ad-
vances in Neural Information Processing Sys-
tems 34: Annual Conference on Neural Infor-
mation Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual , pages 1022–1035.
Yuning Mao, Lambert Mathias, Rui Hou, Am-
jad Almahairi, Hao Ma, Jiawei Han, Scott Yih,and Madian Khabsa. 2022. UniPELT: A uni-
fied framework for parameter-efficient language
model tuning. In Proceedings of the 60th An-
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) ,
pages 6253–6264, Dublin, Ireland. Association
for Computational Linguistics.
Nafise Moosavi, Quentin Delfosse, Kristian Ker-
sting, and Iryna Gurevych. 2022. Adaptable
adapters. In Proceedings of the 2022 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies , pages 3742–3753, Seattle,
United States. Association for Computational
Linguistics.
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li,
James Cross, Sebastian Riedel, and Mikel
Artetxe. 2022. Lifting the curse of multilin-
guality by pre-training modular transformers.
InProceedings of the 2022 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies , pages 3479–3495, Seattle, United
States. Association for Computational Linguis-
tics.
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-
warya Kamath, Ivan Vuli ´c, Sebastian Ruder,
Kyunghyun Cho, and Iryna Gurevych. 2020a.
AdapterHub: A framework for adapting trans-
formers. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Pro-
cessing: System Demonstrations , pages 46–54,
Online. Association for Computational Linguis-
tics.
Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and
Edoardo Ponti. 2023. Modular deep learning.
Transactions on Machine Learning Research .
Survey Certification.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and
Sebastian Ruder. 2020b. MAD-X: An Adapter-
Based Framework for Multi-Task Cross-Lingual
Transfer. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7654–7673, Online.
Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. 2020a.

--- PAGE 16 ---
Exploring the limits of transfer learning with a
unified text-to-text transformer. J. Mach. Learn.
Res., 21:140:1–140:67.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. 2020b.
Exploring the limits of transfer learning with a
unified text-to-text transformer. J. Mach. Learn.
Res., 21:140:1–140:67.
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao
Huang, Zhihui Li, Xiaojiang Chen, and Xin
Wang. 2021. A comprehensive survey of neural
architecture search: Challenges and solutions.
ACM Computing Surveys (CSUR) , 54(4):1–34.
Bin Xin Ru, Xingchen Wan, Xiaowen Dong, and
Michael A. Osborne. 2021. Interpretable neu-
ral architecture search via bayesian optimisation
with weisfeiler-lehman kernels. In 9th Inter-
national Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May
3-7, 2021 .
Robin Ru, Pedro M. Esperança, and Fabio Maria
Carlucci. 2020. Neural architecture generator
optimization. In Advances in Neural Informa-
tion Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
Andreas Rücklé, Gregor Geigle, Max Glockner,
Tilman Beck, Jonas Pfeiffer, Nils Reimers, and
Iryna Gurevych. 2021. AdapterDrop: On the
efficiency of adapters in transformers. In Pro-
ceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pages
7930–7946, Online and Punta Cana, Dominican
Republic. Association for Computational Lin-
guistics.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, An-
toine Chaffin, Arnaud Stiegler, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Ur-
mish Thakker, Shanya Sharma Sharma, Eliza
Szczechla, Taewoon Kim, Gunjan Chhablani, Ni-
hal V . Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Man-
ica, Sheng Shen, Zheng Xin Yong, Harshit
Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault Févry, Jason Alan Fries,Ryan Teehan, Teven Le Scao, Stella Biderman,
Leo Gao, Thomas Wolf, and Alexander M. Rush.
2022. Multitask prompted training enables zero-
shot task generalization. In The Tenth Interna-
tional Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 .
Yi-Lin Sung, Varun Nair, and Colin Raffel. 2021.
Training neural networks with fixed sparse
masks. In Advances in Neural Information
Processing Systems 34: Annual Conference on
Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual ,
pages 24193–24205.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
BERT rediscovers the classical NLP pipeline.
InProceedings of the 57th Annual Meeting of
the Association for Computational Linguistics ,
pages 4593–4601, Florence, Italy. Association
for Computational Linguistics.
Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan
Kobyzev, and Ali Ghodsi. 2023. DyLoRA:
Parameter-efficient tuning of pre-trained mod-
els using dynamic search-free low-rank adap-
tation. In Proceedings of the 17th Conference
of the European Chapter of the Association for
Computational Linguistics , pages 3274–3287,
Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
Ivan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,
Goran Glavaš, and Anna Korhonen. 2020. Prob-
ing pretrained language models for lexical se-
mantics. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7222–7240, Online.
Association for Computational Linguistics.
Xingchen Wan, Vu Nguyen, Huong Ha, Binxin Ru,
Cong Lu, and Michael A Osborne. 2021. Think
global and act local: Bayesian optimisation over
high-dimensional categorical and mixed search
spaces. In International Conference on Machine
Learning , pages 10663–10674. PMLR.
Xingchen Wan, Binxin Ru, Pedro M. Esperança,
and Zhenguo Li. 2022. On redundancy and di-
versity in cell-based neural architecture search.
InThe Tenth International Conference on Learn-
ing Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.

--- PAGE 17 ---
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. 2019.
Superglue: A stickier benchmark for general-
purpose language understanding systems. In
Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada ,
pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.
2018. GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding.
InProceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neu-
ral Networks for NLP , pages 353–355, Brussels,
Belgium. Association for Computational Lin-
guistics.
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukher-
jee, Xiaodong Liu, Jing Gao, Ahmed Hassan
Awadallah, and Jianfeng Gao. 2022. AdaMix:
Mixture-of-adaptations for parameter-efficient
model tuning. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Lan-
guage Processing , pages 5744–5760, Abu Dhabi,
United Arab Emirates. Association for Compu-
tational Linguistics.
Colin White, Willie Neiswanger, and Yash Sa-
vani. 2021. BANANAS: bayesian optimization
with neural architectures for neural architecture
search. In Thirty-Fifth AAAI Conference on Arti-
ficial Intelligence, AAAI 2021, Thirty-Third Con-
ference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Sympo-
sium on Educational Advances in Artificial In-
telligence, EAAI 2021, Virtual Event, February
2-9, 2021 , pages 10293–10301.
Saining Xie, Alexander Kirillov, Ross B. Girshick,
and Kaiming He. 2019. Exploring randomly
wired neural networks for image recognition.
In2019 IEEE/CVF International Conference
on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019 , pages
1284–1293. IEEE.
Antoine Yang, Pedro M. Esperança, and
Fabio Maria Carlucci. 2020. NAS evaluation
is frustratingly hard. In 8th InternationalConference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020 .
Qingru Zhang, Minshuo Chen, Alexander
Bukharin, Pengcheng He, Yu Cheng, Weizhu
Chen, and Tuo Zhao. 2023. Adaptive budget
allocation for parameter-efficient fine-tuning.
InThe Eleventh International Conference on
Learning Representations .
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and
Hinrich Schütze. 2020. Masking as an efficient
alternative to finetuning for pretrained language
models. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 2226–2241, Online.
Association for Computational Linguistics.
Han Zhou, Xingchen Wan, Ivan Vuli ´c, and Anna
Korhonen. 2023. Survival of the most influential
prompts: Efficient black-box prompt search via
clustering and pruning. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP
2023 , pages 13064–13077, Singapore. Associa-
tion for Computational Linguistics.
Eckart Zitzler and Lothar Thiele. 1998. Multiobjec-
tive optimization using evolutionary algorithms
- A comparative case study. In Parallel Prob-
lem Solving from Nature - PPSN V , 5th Inter-
national Conference, Amsterdam, The Nether-
lands, September 27-30, 1998, Proceedings , vol-
ume 1498 of Lecture Notes in Computer Science ,
pages 292–304. Springer.
Barret Zoph and Quoc V . Le. 2017. Neural archi-
tecture search with reinforcement learning. In
5th International Conference on Learning Rep-
resentations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings .
