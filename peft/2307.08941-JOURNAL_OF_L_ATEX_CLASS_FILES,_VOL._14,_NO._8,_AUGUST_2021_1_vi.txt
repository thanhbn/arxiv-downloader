# 2307.08941.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2307.08941.pdf
# Kích thước tệp: 1290209 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP 14, SỐ 8, THÁNG 8 NĂM 2021 1
MLP Fusion: Hướng tới Tinh chỉnh Hiệu quả của Các Mô hình Ngôn ngữ Dày đặc và Mixture-of-Experts
Mengting Ai∗, Tianxin Wei∗, Yifan Chen∗,Thành viên, IEEE , Zeming Guo, Jingrui He, Thành viên cấp cao, IEEE
Tóm tắt—Tinh chỉnh một mô hình ngôn ngữ được huấn luyện trước (PLM) nổi lên như chiến lược chủ đạo trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên. Tuy nhiên, quá trình này được biết là tốn kém, đặc biệt trên các thiết bị edge có khả năng tính toán thấp. Trong khi các phương pháp tổng quát (ví dụ: lượng tử hóa và chưng cất) đã được nghiên cứu rộng rãi để giảm tính toán/bộ nhớ của việc tinh chỉnh PLM, các kỹ thuật nén một lần được thiết kế đặc biệt cho tinh chỉnh vẫn chưa được khám phá nhiều. Trong bài báo này, chúng tôi nghiên cứu kernel tiếp tuyến neural (NTK)–tiết lộ động lực học gradient descent của mạng neural–của các module perceptron đa lớp (MLP) trong PLM và đề xuất tạo ra một PLM nhẹ thông qua MLP fusion xấp xỉ NTK. Bằng cách kết hợp NTK vào quá trình nén, MLP Fusion không chỉ bảo tồn đầu ra của mô hình gốc mà còn duy trì động lực học huấn luyện của nó. Để đạt được điều này, chúng tôi xem xét lại MLP như một nhóm các sub-MLP và phân cụm chúng thành một số centroid cho trước, sau đó có thể được khôi phục thành một MLP nén và xấp xỉ một cách đáng ngạc nhiên tốt NTK của PLM gốc. Phương pháp của chúng tôi áp dụng được cho cả module MLP tiêu chuẩn và module Mixture-of-Experts (MoE) trong PLM, thể hiện tính mở rộng và linh hoạt của nó. Ngoài ra, chúng tôi cung cấp các dẫn xuất lý thuyết để chứng minh cách nén được đề xuất bảo tồn NTK. Các thí nghiệm mở rộng của tinh chỉnh PLM trên cả tác vụ hiểu và sinh ngôn ngữ tự nhiên được cung cấp để xác minh hiệu quả của MLP fusion. Mã của chúng tôi có sẵn tại https://github.com/weitianxin/MLP_Fusion.
Thuật ngữ chỉ mục—Kernel tiếp tuyến neural, tinh chỉnh mô hình ngôn ngữ được huấn luyện trước, học máy hiệu quả, Mixture-of-Experts.

I. GIỚI THIỆU
TINH chỉnh có giám sát (SFT) của các mô hình ngôn ngữ được huấn luyện trước (PLM) đã là phương pháp phổ biến nhất để giải quyết các tác vụ xử lý ngôn ngữ tự nhiên (NLP) downstream [1], [2]. Tuy nhiên, bất chấp hiệu suất cao của SFT trên các tác vụ downstream [3], người dùng phải đối mặt với chi phí tính toán đáng kể về cả thời gian và không gian do kích thước lớn của PLM. Kích thước của các PLM phổ biến gần đây đã tăng từ hàng trăm triệu [4] đến hàng nghìn tỷ [5] tham số, được thúc đẩy bởi các quy luật tỷ lệ [6]. Ngay cả mô hình BERT nhỏ nhất [7] cũng có hơn 110M tham số, chưa kể đến các mô hình Llama-series mới hơn [8], có từ 7B đến 405B tham số. Mixture-of-Experts (MoE) [9], như một sản phẩm khác của quy luật tỷ lệ, mở rộng ra ngoài lớp mạng neural feedforward (FFN) truyền thống bằng cách thay thế một perceptron đa lớp (MLP) đơn lẻ bằng nhiều MLP, được gọi là "experts". Thiết kế MoE thưa thớt cải thiện hiệu suất trong khi giữ chi phí tính toán inference (FLOPs) tương đương với mô hình dày đặc gốc, vì chỉ một số ít expert được chọn được kích hoạt trong quá trình inference. Tuy nhiên, trong quá trình tinh chỉnh, chi phí tính toán tăng đáng kể vì mỗi expert cần được điều chỉnh. Kích thước expert cho Mixtral [10] đạt 176.2M, và sự hiện diện của 8 hoặc thậm chí nhiều expert hơn trong mỗi lớp làm trầm trọng thêm các yêu cầu.

Các nỗ lực khác nhau đã được thực hiện trong nhiều lĩnh vực để nén và khai thác các PLM quy mô lớn. Một kỹ thuật phổ biến trong nén mô hình là chưng cất kiến thức [11]–[14, KD], nhằm chuyển giao kiến thức từ các mô hình ngôn ngữ lớn (LLM) được huấn luyện trước sang các mô hình nhỏ hơn. Tuy nhiên, phương pháp này đòi hỏi huấn luyện lại rộng rãi, bao gồm cả LLM gốc và mô hình compact. Đã có một số nỗ lực trước đó để thiết lập các phương pháp nén mô hình một lần. Các phương pháp pruning một lần [15]–[18, sparsification] xác định các sub-network tại thời điểm khởi tạo liên quan đến các tiêu chí nhất định về độ lớn trọng số hoặc luồng gradient. Tuy nhiên, hầu hết các công trình về pruning (entry-wise) tập trung vào việc giảm FLOPs huấn luyện hoặc inference về mặt khái niệm, trong khi phép nhân ma trận thưa thớt không được hỗ trợ tốt trên phần cứng hiện đại (ví dụ: GPU) và thậm chí làm chậm việc huấn luyện về thời gian thực tế [19].

Trong khi đó, phân tích giá trị đơn bị cắt cụt (SVD) trên các ma trận trọng số [20], [21] đã được áp dụng để tăng tốc các CNN lớn bằng cách tận dụng cấu trúc tuyến tính trong mạng để loại bỏ redundancy. Tuy nhiên, SVD bị cắt cụt có khả năng biểu diễn hạn chế và có thể dẫn đến hiệu suất không tối ưu, vì nó giảm đáng kể chiều của các phép biến đổi tuyến tính trong mạng. LoRA [22], mặc dù giảm thiểu vấn đề này bằng cách kết hợp thông tin từ các ma trận trọng số gốc, không giảm chi phí inference của mô hình được tinh chỉnh. Cụ thể cho các mô hình MoE, nhiều nghiên cứu khác nhau đã giới thiệu khái niệm merging expert [23]–[27] và pruning expert [28], như một phương pháp để giảm số lượng expert trong mỗi lớp của mô hình MoE.

Về một khía cạnh riêng biệt, mặc dù các cơ chế attention hiệu quả [29]–[31] đã trở thành phương pháp chính để tăng tốc việc pre-training của các mô hình ngôn ngữ, chúng tôi thay vào đó tập trung vào các sub-layer FFN của một transformer được huấn luyện trước. Với kích thước đầu vào ẩn ngày càng tăng của PLM, hiện đã vượt quá mười nghìn [8], [10], ý nghĩa của các sub-layer FFN về mặt chi phí tính toán đã tăng lên đáng kể. Chúng tôi quan sát thấy đối với các tác vụ NLP thông thường nơi độ dài chuỗi token không quá 512, chi phí tính toán của các module MLP nặng hơn module attention mặc dù module attention có độ phức tạp bậc hai (so sánh chi tiết tính toán và so sánh chi phí tính toán trong hai module được cung cấp trong tài liệu bổ sung Phụ lục B-A). Sự chênh lệch này thậm chí còn rõ rệt hơn trong các mô hình MoE.

Các hạn chế hiện tại của các phương pháp nén mô hình tổng quát và nhu cầu thực tế để giảm tính toán trong các module MLP thúc đẩy chúng tôi phát triển một kỹ thuật nén MLP cho SFT mô hình ngôn ngữ hiệu quả. Để đạt được hiệu suất SFT cạnh tranh, chúng tôi đề xuất một góc nhìn mới về nén PLM, rằng mô hình nén được cho là xấp xỉ không chỉ đầu ra mô hình, mà còn động lực học huấn luyện của SFT gốc. Chúng tôi chuyển sang kernel tiếp tuyến neural [32], [33, NTK] như một proxy của động lực học SFT và quản lý để cho phép mô hình nén xấp xỉ NTK gốc; cụ thể, chúng tôi phân tích cấu trúc MLP hoặc MoE trong PLM, kết nối nó với fusion mô hình [34, fusion layer-wise nhiều MLP thành một], và đề xuất một phương pháp nén mới, MLP fusion, cụ thể cho tinh chỉnh PLM. Như được hiển thị trong Hình 1, phương pháp "fusion" của chúng tôi có thể chứng minh đạt được lỗi xấp xỉ ma trận NTK nhỏ nhất trên dataset thực tế SST2 [35].

Tóm lại, các đóng góp của công trình này gồm bốn phần:
• Chúng tôi giới thiệu khái niệm xấp xỉ NTK cho nén PLM, để đảm bảo rằng mô hình nén có thể bảo tồn động lực học huấn luyện của mô hình gốc.
• Chúng tôi phân tích các module MLP trong PLM và đề xuất một kỹ thuật độc lập với dữ liệu mới, MLP fusion, tận dụng đặc điểm phân cụm để xấp xỉ NTK. Các dẫn xuất lý thuyết được cung cấp để chứng minh cách nén được đề xuất bảo tồn NTK.
• Chúng tôi chứng minh rằng MLP Fusion có thể được áp dụng cho cả module MLP truyền thống và module MoE trong PLM, chứng minh tính đa dạng của nó.
• Chúng tôi cung cấp kết quả thí nghiệm mở rộng về SFT PLM cho cả tác vụ hiểu ngôn ngữ tự nhiên (NLU) và sinh (NLG), xác thực hiệu quả và tính đúng đắn của chúng.

II. CÔNG TRÌNH LIÊN QUAN
Có nhiều phương pháp nén mô hình để giảm kích thước của MLP trong PLM. Hướng nghiên cứu đầu tiên, chưng cất kiến thức [36]–[38], nén mô hình được huấn luyện trước và sau đó tinh chỉnh mô hình nén trên các tác vụ downstream. Các kỹ thuật như mean squared error [11], optimal transport [39], và maximum mean discrepancy (MMD) [40] thường được sử dụng như các thành phần loss chưng cất. Tuy nhiên, phương pháp này đòi hỏi loading và thực thi PLM teacher lớn, đòi hỏi tài nguyên tính toán đáng kể trước khi tinh chỉnh. Một hướng khác bao gồm các phương pháp được áp dụng sau giai đoạn SFT để đạt được inference nhanh hơn. Ví dụ, FastBert [41] sử dụng cơ chế adaptive theo mẫu để điều chỉnh thời gian inference, và DeeBERT [42] tăng tốc inference bằng cách cho phép các mẫu thoát sớm hơn. Moefication [43] chia các module MLP thành các sub-network với một router để chọn sub-network phù hợp cho mỗi đầu vào. Tuy nhiên, các phương pháp này vẫn dựa vào SFT thông thường và không tận dụng đầy đủ kiến thức PLM.

Ngoài các hướng trên, một paradigm tinh chỉnh hiệu quả nhẹ hơn là nén mô hình một lần. Như một đại diện, các phương pháp pruning một lần [15]–[18], [44] xác định các sub-network tại thời điểm khởi tạo liên quan đến các tiêu chí do người dùng chỉ định (ví dụ: độ lớn trọng số hoặc luồng gradient) và đạt được độ thưa thớt trong trọng số mô hình. Hypothesis Lottery Ticket (LTH) [45], [46] chứng minh sự tồn tại của các sub-network thưa thớt trong DNN và đã được áp dụng thành công cho PLM. Những tiến bộ gần đây như NTK-SAP [47] và PX [48] tích hợp lý thuyết Neural Tangent Kernel (NTK) vào pruning, đạt được hiệu suất mạnh trên các kiến trúc như ResNet [49]. Tuy nhiên, pruning chủ yếu giảm FLOPs lý thuyết, trong khi các phép toán ma trận thưa thớt vẫn không hiệu quả trên phần cứng hiện đại (ví dụ: GPU), dẫn đến thời gian huấn luyện thực tế chậm hơn.

Các kỹ thuật tính toán cổ điển như SVD bị cắt cụt [20] và sketching ngẫu nhiên [50], [51] cũng trực quan cho nén PLM một lần. LoRA [22] giới thiệu các lớp low-rank trên các lớp gốc, giảm chi phí SFT bằng cách chỉ cập nhật các tham số được thêm. Tuy nhiên, LoRA không giảm chi phí inference vì đầu ra cuối cùng kết hợp các lớp gốc và low-rank. Các trade-off chi tiết cho LoRA được thảo luận trong Phần V-F.

Cụ thể cho các mô hình MoE, nhiều nghiên cứu khác nhau đã giới thiệu khái niệm merging expert [23]–[27] và pruning expert [28], như một phương pháp để giảm số lượng expert trong mỗi lớp của mô hình MoE. Tuy nhiên, chúng tôi lưu ý rằng các phương pháp này có thể không tận dụng đầy đủ cấu trúc MoE, vì việc giảm số lượng expert có thể dẫn đến mất thông tin đáng kể. Trong Phần V chúng tôi triển khai các phương pháp nói trên như baseline để so sánh toàn diện.

III. KIẾN THỨC NỀN TẢNG VÀ KÝ HIỆU
Các ký hiệu cho các lớp MoE được giới thiệu trong Phần III-A. Chúng tôi cũng cung cấp kiến thức nền tảng ngắn gọn về NTK trong Phần III-B.

A. Module Multilayer Perceptron và Mixture-of-Experts
Xuyên suốt bài báo này, chúng tôi ký hiệu chuỗi đầu vào như một ma trận đặc trưng X∈Rs×p, trong đó s là độ dài chuỗi và p là chiều của đầu vào/đầu ra MLP (các chiều của đầu vào và đầu ra MLP đồng ý với chiều embedding trong hầu hết PLM). Kiến trúc neural quan tâm (một transformer được huấn luyện trước) được ký hiệu là f, khác với hàm loss scalar ℓ. Để đơn giản ký hiệu, đầu ra của toàn mạng f(x) được giả định là một scalar trong bài báo này, đây là trường hợp trong các tác vụ regression và phân loại nhị phân. Tuy nhiên, dẫn xuất của chúng tôi vẫn giữ cho đầu ra vector/ma trận nếu chúng ta phân tích từng phần tử đầu ra.

Cụ thể, một MLP trong sub-layer FFN của một transformer có thể được biểu diễn như:
H=σ(XW 1+1b⊤1)W2+1b⊤2, (1)
trong đó W1∈Rp×pI,b1∈RpI (tương ứng W2∈RpI×p,b2∈Rp) là ma trận trọng số và bias term của phép biến đổi tuyến tính thứ nhất (tương ứng thứ hai) trong sub-layer FFN, và σ(·) là hàm kích hoạt theo phần tử. Một số ký hiệu khác được sử dụng thường xuyên bao gồm chiều trung gian MLP pI (chỉ số I viết tắt của "intermediate").

Như một cải tiến so với module MLP trên, chúng tôi cũng xem xét các module mixture-of-experts (MoE) cổ điển, trong đó mỗi expert có dạng của một MLP như trên. Chúng tôi cung cấp minh họa framework của một lớp MoE trong Hình 2. Chi tiết hơn, mỗi lớp MoE bao gồm N expert. Expert thứ k Ek (một hàm để biến đổi vector đầu vào x thành một đặc trưng mới) trong một sub-layer FFN là một MLP và được ký hiệu là:
Ek(X) =σ(XW(k)1+1(b(k)1)⊤)W(k)2+1b(k)2.
Đầu ra của lớp MoE do đó được cho bởi (⊙ biểu diễn tích Hadamard):
Hm=∑Nk=1[G(X)](·,k)1⊤⊙Ek(X), (2)
trong đó G(X) = Softmax (TopK ( XW g))∈Rs×N trả về vector gating thưa thớt được chuẩn hóa của tất cả expert cho mỗi token: TopK( g) xuất ra gi khi gi nằm trong top-k giá trị của g∈RN, ngược lại nó trả về −∞. (Chúng tôi hơi lạm dụng ký hiệu trong TopK ( XW g), trong đó TopK( ·) được áp dụng theo hàng cho ma trận chuỗi XW g.) Ở đây, Wg∈Rp×N biểu diễn phép biến đổi tuyến tính, biến token đầu vào xi thành router logit cho mỗi expert. Ví dụ, cho một vector token đầu vào đơn xi, vector gate G(x) = [0.7,0,0.3,0] kích hoạt expert 1 và 3 với điểm số 0.7 và 0.3 (giả sử số lượng expert N= 4 và Top2 được lấy.) Các gradient (cũng như NTK) của các trọng số trong một module MLP hoặc MoE được phân tích trong Phần IV-A.

B. Neural Tangent Kernel
NTK là một kỹ thuật lý thuyết mạnh mẽ để nghiên cứu động lực học gradient descent của mạng neural [32]. Nó bắt nguồn từ nghiên cứu về mạng neural có chiều rộng vô hạn hoặc cực rộng. Trong việc áp dụng NTK cho mạng neural tích chập cho các tác vụ computer vision, người ta lưu ý rằng NTK có thể được mở rộng cho kiến trúc neural tùy ý f và khởi tạo θ0, điều này tạo ra NTK gradient descent ngẫu nhiên (SGD) như [33]:
⟨∇θ0f(x;θ0),∇θ0f(z;θ0)⟩. (3)
Chúng tôi lưu ý rằng NTK trên cụ thể cho optimizer SGD [52]. Đối với Adam [53], optimizer phổ biến nhất cho tinh chỉnh mô hình ngôn ngữ, NTK tương ứng của nó trong giai đoạn đầu của huấn luyện (được tin là phù hợp với bản chất của tinh chỉnh thời gian ngắn) có thể được xấp xỉ bởi cái gọi là Asymmetric SignGD Kernel [54]:
K(AS)(x, z):=⟨∇θ0f(x;θ0),sign (∇θ0f(z;θ0))⟩,(4)
chúng tôi sẽ gọi kernel này là Adam NTK và tập trung vào phân tích của nó trong bài báo này, xem xét Adam là optimizer chủ đạo trong tinh chỉnh mô hình ngôn ngữ.

Các công trình gần đây cho thấy việc sử dụng trực tiếp NTK Phương trình (3) được trích xuất từ một mô hình được huấn luyện trước f(·) có thể đạt được hiệu suất tốt trên các tác vụ computer vision [55], và trong một số trường hợp có thể nắm bắt động lực học huấn luyện của tinh chỉnh mô hình ngôn ngữ [54]. Đã có một số thảo luận về việc nén một mạng được huấn luyện (tinh chỉnh) với NTK được bảo tồn thông qua pruning và quantization [47], [48], [56]. Chúng tôi sẽ sớm tận dụng công cụ hữu ích để hướng dẫn thiết kế các mô hình được đề xuất và phục vụ như một kiểm tra hợp lý.

IV. MLP FUSION VỚI APPROXIMATE NTK
Để thuận tiện cho người đọc, trước tiên chúng tôi dẫn xuất dạng cụ thể của NTK cho các module MLP và MoE trong Phần IV-A. Tiếp theo, Phần IV-B phác thảo dạng chính xác của MLP Fusion dựa trên clustering được đề xuất, tiếp theo là xác minh trong Phần IV-C để đảm bảo phương pháp đáp ứng các kỳ vọng đã nêu trước đó. Cuối cùng, Phần IV-D trình bày layer-wise tuning, kết hợp ý tưởng từ chưng cất thêm.

A. Chuẩn bị: NTK cho MLP & MoE
Vì các gradient w.r.t. trọng số mô hình là các khối xây dựng của NTK, chúng tôi cung cấp biểu thức của các gradient cho MLP và MoE như sau, việc tính toán dựa trên ∇Hf/∇Hmfm∈Rs×p và quy tắc chuỗi (tương tự f, dọc theo bài báo này fm được đặt như một mô hình MoE scalar).

Gradient cho các module MLP. Chúng tôi bắt đầu với các gradient cho một module MLP cổ điển trong Phương trình (1). Cụ thể,
∇W2f=σ⊤∇Hf,∇b2f= (∇Hf)⊤1
∇W1f=X⊤[∇HfW⊤2⊙σ′]
∇b1f=[∇HfW⊤2⊙σ′]⊤1,(5)
trong đó theo quy ước chúng tôi lạm dụng ký hiệu in đậm σ, σ′ như viết tắt cho σ(XW 1+1b⊤1) và σ′(XW 1+1b⊤1) tương ứng. Việc tính toán NTK sau đó sẽ được đưa về tích trong phù hợp của các gradient term đã nêu. Đáng chú ý rằng kỹ thuật nén mô hình cổ điển, pruning, được kỳ vọng xấp xỉ tốt NTK. Giả định rằng đầu ra của một MLP được pruning gần với cái gốc, các gradient term sẽ được xấp xỉ khoảng bởi tích Hadamard của ma trận mask và các gradient term gốc trong Phương trình (5).

Gradient cho các module MoE. Trước tiên chúng tôi đưa ra dạng ma trận của một module MoE để dễ dẫn xuất gradient sau đây; theo hiểu biết tốt nhất của chúng tôi, chúng tôi là người đầu tiên cung cấp dạng cơ bản này cho các module MoE.

Cho một token đơn xi∈Rp (cũng là hàng thứ i trong ma trận chuỗi X), ma trận router được ký hiệu như:
Ri:= diag( G(xi))⊗IpI=[G(xi)]1·I...[G(xi)]N·I∈RN·pI×N·pI,
trong đó ⊗ là tích Kronecker, I là ma trận đơn vị. Chúng tôi nhớ lại G(xi) là một vector thưa với độ dài N chứa giá trị gate. (Vì G(xi) phụ thuộc vào xi, Ri khác nhau cho mỗi token và chúng tôi thêm chỉ số i tương ứng.) Các ma trận trọng số trong lớp MoE được ký hiệu tập thể như:
W1=[W(1)1···W(N)1]∈Rp×N·pI,
W2=[W(1)2⊤···W(N)2⊤]⊤∈RN·pI×p.
Hàng thứ i của đầu ra MoE do đó có thể được biểu diễn như (để đơn giản, các bias term được bỏ qua; cũng vậy, trong thực tế, hầu hết các mô hình MoE không chứa chúng):
(Hm)⊤i=σ(x⊤iW1)RiW2, (6)
trong đó Ri bao gồm kiến thức expert quan trọng và thể hiện độ thưa thớt cao, vì thường chỉ một số lượng được chọn của expert được kích hoạt trong mỗi lớp MoE.

Các gradient của lớp MoE dựa trên Phương trình (6) có thể được thu được như:
∇W2fm=∑iR⊤i·σi·(∇Hmfm)⊤i,
∇W1fm=∑ixi[(RiW2(∇Hmfm)i)⊙σ′i]⊤,(7)
trong đó σi, σ′i là hàng thứ i của σ, σ′ tương ứng.

Như một nhận xét cuối, chúng tôi cố ý bỏ qua gradient cho ma trận trọng số Wg trong router, vì trong thực tế nó được đóng băng trong giai đoạn SFT; thực hành này xuất phát từ quan sát rằng việc bảo tồn thông tin thế giới phổ quát của PLM gốc có thể tăng cường hiệu suất của chúng [57]–[59]. Các phát hiện của chúng tôi trong Phần V-E hỗ trợ thực nghiệm quan sát này, chứng minh việc đóng băng Wg có thể cải thiện hiệu suất mô hình.

B. Phương pháp luận: MLP Fusion với Clustering
Trong phần con này, chúng tôi chủ yếu phát triển phương pháp được đề xuất với mục tiêu đảm bảo rằng đầu ra mới H̃C có thể xấp xỉ hiệu quả đầu ra gốc H (một phân tích xấp xỉ khoảng được cung cấp trong Phụ lục C trong bổ sung), và trì hoãn thảo luận về xấp xỉ NTK đến Phần IV-A.

Chúng tôi đề xuất MLP Fusion theo mục đích trực quan này, thông qua một quan điểm rằng một MLP có thể được coi như ensemble của nhiều bottleneck-1 sub-MLP [60]–[62]. Chúng tôi viết lại đầu ra MLP ở dạng sau:
H=σ(XW 1+1b⊤1)W2+1b⊤2
=∑pIi=1[σ(XW 1,·,i+b1,i1)W2,i,·] +1b⊤2,
trong đó theo quy ước chúng tôi biểu diễn cột thứ i (tương ứng hàng) trong ma trận trọng số W1 (tương ứng W2) như W1,·,i (tương ứng W2,i,·). Phép tổng ngụ ý rằng việc xấp xỉ MLP thông qua một vài bottleneck-1 sub-MLP (các số hạng ở vế phải trên) là khả thi, theo cách tương tự như các phương pháp số như importance sampling [63] và sketching [50], [51]. Xem xét sự tồn tại của hàm kích hoạt phi tuyến σ, chúng tôi chuyển sang clustering và sẽ chứng minh dưới đây cách kỹ thuật học máy cổ điển này có thể được sử dụng để xấp xỉ phép tổng sub-MLP trên.

Để có được "embedding" của các sub-MLP cho clustering, chúng tôi xem xét MLP gốc như pI điểm hỗ trợ, được biểu diễn bởi một ma trận thiết kế W=[W⊤1,b1,W2]∈RpI×(2p+1). Một ý tưởng trực quan để nén một MLP do đó là biểu diễn phân phối thực nghiệm (MLP) bởi các centroid đầu ra của c cluster. Chúng tôi đề xuất sử dụng wi= [W⊤1,·,i,b1,i,W2,i,·]⊤ như vector embedding cho sub-MLP thứ i, vì wi có thể quyết định duy nhất sub-MLP thứ i. Dựa trên các embedding, thuật toán Lloyd [64] có thể được áp dụng trực tiếp để giải quyết vấn đề k-means clustering, thu được c cluster {Pj}cj=1, và trả về một ma trận clustering one-hot C∈Rc×pI với các phần tử Cji= 1{wi∈Pj}. Chuẩn hóa C để các hàng tổng bằng 1, chúng ta có thể xây dựng một ma trận averaging C̄ với các phần tử C̄ji=1|Pj|{wi∈Pj}, trong đó |Pj| là số phần tử trong cluster j. để C̄W sẽ trả về ma trận centroid mong muốn W̃=[W̃⊤1,b̃1,W̃2]∈Rc×(2p+1). Nói chung, việc thực hiện clustering là tối thiểu hóa khoảng cách từ một điểm đến centroid gần nhất của nó, điều này một phần giải thích trực giác của chúng tôi rằng việc thay thế các sub-MLP gốc bằng các centroid tương ứng của chúng có thể có lợi cho việc nén MLP.

Mối quan hệ với Model Fusion [34]. Về nguyên tắc, chúng tôi xem xét clustering của sub-MLP chia sẻ cùng tinh thần với model fusion, lấy một lớp đơn của MLP như một phân phối thực nghiệm của các trọng số tương ứng (either W1,·,i's hoặc W2,i,·'s trong ngữ cảnh của chúng tôi) và sau đó hợp nhất nhiều MLP thành một cái mới thông qua việc giải quyết vấn đề Wasserstein barycenter [65]. Quy trình clustering có liên quan chặt chẽ đến vấn đề trên, vì các centroid đầu ra phục vụ như các barycenter tối ưu khi số điểm w được gán cho mỗi cluster được cố định. Do mối kết nối, chúng tôi gọi quy trình clustering là MLP fusion trong bài báo này.

Dẫn xuất cho nén MLP. Chúng tôi thay thế mỗi vector tham số sub-MLP wi bằng centroid tương ứng (tương đương, chúng tôi thay thế W bằng C⊤W̃). Đầu ra mới có thể được đơn giản hóa tự nhiên như:
σ(XW̃1+1(b̃1)⊤)CC⊤W̃2+1b⊤2
=σ(XW̃1+1(b̃1)⊤)CC⊤W̃2+1b⊤2,
trong đó phương trình trên giữ vì C đơn giản "sao chép" các centroid và do đó có thể được đưa ra khỏi hàm kích hoạt.

Chúng tôi sẽ sớm chỉ ra trong Phần IV-C rằng các tính chất tính toán của ma trận clustering one-hot C thực sự quan trọng cho xấp xỉ Adam NTK.

Sau khi được đưa ra khỏi hàm kích hoạt, C sau đó được phép kết hợp với C⊤ để tạo thành ma trận scaling được gọi là P=CC⊤, là một ma trận đường chéo cố định c×c giảm đáng kể tính toán so với phương trình gốc. Lưu ý kiến trúc của MLP cuối cùng chưa được chỉ định, vì có nhiều cách khác nhau để giải quyết ma trận scaling P: nó có thể
• hoặc được kết hợp vào W̃2, hoặc
• đứng riêng như một ma trận scaling hằng số.

Đáng chú ý rằng cả hai chiến lược đều hoạt động giống hệt nhau trong quá trình lan truyền xuôi; tuy nhiên, trong quá trình lan truyền ngược, gradient của phương pháp thứ hai được nhân với ma trận scaling P. Trong phiên bản cuối cùng của MLP Fusion, chúng tôi áp dụng dạng sử dụng ma trận scaling đứng riêng P, và gọi biến thể kết hợp P vào W̃2 là "clustering".

Dẫn xuất cho nén module MoE. Vì một module MoE được cấu thành từ nhiều MLP, MLP Fusion được đề xuất có thể được áp dụng tương ứng cho các expert trong đó, và dẫn xuất sẽ tương tự; chúng tôi trì hoãn trình bày đến phần con tiếp theo.

C. Xấp xỉ NTK
Ngoài mục tiêu xấp xỉ đầu ra đã nêu, chúng tôi tiếp tục đề xuất một phương pháp nén cho tinh chỉnh được cho là bảo tồn NTK của mô hình gốc để động lực học huấn luyện của nó cũng có thể được bảo tồn.

Để đạt được điều này, chúng tôi xem xét lại ma trận scaling P và làm cho nó "đứng riêng" (lựa chọn thứ hai trong phần con trước); với thiết kế cụ thể này, MLP Fusion có thể xấp xỉ NTK và dạng của nó được đưa ra chính thức như:
H̃C:=σ(XW̃1+1b̃⊤1)PW̃2+1b⊤2 (8)
trong đó chúng tôi chọn W̃1:=W1C̄⊤,b̃1:=C̄b1,W̃2:=C̄W 2 như các tham số mới cho MLP nén, và P được thiết kế để đứng riêng như một ma trận scaling hằng số. Chúng tôi lưu ý rằng Pii= (CC⊤)ii=∑qC2iq=∑qCiq (Pij= (CC⊤)ij= 0 cho i̸=j) biểu diễn số điểm trong cluster i. Từ góc độ trực quan, quá trình ngược của việc nhân gradient với ma trận scaling P có thể được xem như gán learning rate khác nhau cho các cluster khác nhau. Điều này có nghĩa là các cluster lớn hơn được cấp learning rate lớn hơn.

Đối với các module MoE, việc áp dụng trực tiếp MLP Fusion cho mỗi expert sẽ cho:
(H̃m)⊤i=σ(x⊤iW̃1)P(m)iW̃2, (9)
trong đó P(m)i=C(m)Ri(C(m))⊤ và ma trận clustering được nối C(m) được ký hiệu như:
C(m)=[C1...CN]∈RN·c×N·pI,
và Ck là ma trận clustering tương ứng cụ thể cho expert k; để dễ ký hiệu, chúng tôi cũng tải lại các ma trận trọng số nén W̃1,W̃2 như
W̃1=[W̃(1)1···W̃(N)1],
W̃2=[W̃(1)2⊤···W̃(N)2⊤]⊤,
trong đó W̃(k)1=W(k)1C̄⊤k,W̃(k)2=C̄kW(k)2,∀k∈[N], và tương tự C̄, C̄k là phiên bản chuẩn hóa của Ck. Một nhận xét cuối là phiên bản nén trong Phương trình (9) có cùng dạng với đầu ra MoE gốc trong Phương trình (6), và do đó chúng ta có thể dẫn xuất tương tự gradient cho W̃1,W̃2 trong các module MoE như trong Phương trình (7).

Chúng tôi sau đó sẽ chỉ ra cách MLP cụ thể (8) có thể phục vụ để xấp xỉ NTK của MLP gốc (1) (dẫn xuất cho module MoE (2) có thể được tìm thấy trong tài liệu bổ sung). Kỳ vọng này ngụ ý các yêu cầu sau: đầu tiên, đầu ra mới H̃C (tương ứng H̃m) được cho là xấp xỉ đầu ra gốc H (tương ứng Hm), mà chúng tôi đã chỉ ra heuristically trong thảo luận trước; thứ hai, biểu diễn ẩn σ và thành phần composition liên quan ∇HfW⊤2⊙σ′ cũng nên được bảo tồn.

Phần con này do đó sẽ được dành để xác minh rằng phương pháp được đề xuất có thể tạo ra một Adam NTK gần với cái gốc. Bước chính là chỉ ra tích trong của bốn gradient term trong Phương trình (5) sẽ xấp xỉ giữ nguyên. Chúng tôi chuẩn bị một số ký hiệu bổ sung để dễ dàng thảo luận sau và để mô hình neural nén được trang bị module MLP nén như fc. Chuỗi token đầu vào được ký hiệu như X hoặc Z, tương ứng.

Trước tiên chúng tôi đưa ra một giả định rằng clustering có thể nắm bắt phân phối thực nghiệm MLP, sao cho, theo một nghĩa nào đó, CW̃≈W và H̃C≈H như được dẫn xuất trong Phần IV-B.
Giả định này ngụ ý rằng ∇Hf có thể được bảo tồn bởi ∇H̃Cfc, vì chúng phụ thuộc vào H/H̃C theo cùng một cách.
Chúng ta có thể tự động thu được ⟨∇b2f(X),sign (∇b2f(Z))⟩ ≈ ⟨∇b2fc(X),sign (∇b2fc(Z))⟩.

Sau đó chúng tôi phân tích thành phần ⟨∇W2f(X),sign (∇W2f(Z))⟩, trong đó ký hiệu ⟨·,·⟩ được tải lại như tích trong ma trận ⟨X,Z⟩:= Tr(X⊤Z). Thành phần bằng1
Tr[(∇Hf(X))⊤σx·sign(σ⊤z∇Hf(Z))],
và có thể được chỉ ra để tiếp cận
⟨∇W̃2fc(X),sign(∇W̃2fc(Z))⟩.
Cụ thể, chúng tôi tái sử dụng suy luận ∇Hf≈ ∇H̃Cfc để làm cho việc nghiên cứu liệu (σ̃xP)·sign(Pσ̃⊤z) có thể xấp xỉ đối tác của nó σx·sign(σ⊤z) là đủ.

Các phân tích tương tự của tích ma trận [∇Hf(X)W⊤2⊙σ′x]·[∇Hf(Z)W⊤2⊙σ′z]⊤ cũng có thể được thực hiện cho hai thành phần khác ⟨∇W1f(X),sign (∇W1f(Z))⟩ và ⟨∇b1f(X),sign (∇b1f(Z))⟩.
Do hạn chế về không gian, các dẫn xuất cho hai thành phần và các module MoE được cung cấp trong Phụ lục bổ sung B-C & B-D. Đáng chú ý, hàm sign theo phần tử trong optimizer Adam đóng vai trò chính trong việc bảo tồn NTK. Đối với SGD thông thường, các điều kiện và sửa đổi bổ sung được yêu cầu. Người đọc có thể tìm thêm chi tiết trong Phụ lục B-E.

D. Layer-wise Task-specific Tuning
Trong phần trước, MLP Fusion quản lý để khai thác tiềm năng của các mô hình được huấn luyện trước theo cách một lần và độc lập với tác vụ, nơi chúng tôi giữ lại động lực học huấn luyện của mạng neural thông qua bảo tồn NTK. Để có hiệu quả hơn trong việc tiếp thu kiến thức trong mỗi tác vụ, chúng ta có thể tận dụng ý tưởng từ chưng cất và thiết kế trực quan một module tuning cụ thể tác vụ layer-wise (và do đó nhẹ), tiếp tục điều chỉnh MLP được hợp nhất với dữ liệu huấn luyện không giám sát cụ thể tác vụ. So với chưng cất cổ điển, layer-wise tuning kéo dài thời gian ngắn hơn (chỉ 1 epoch trong các thí nghiệm của chúng tôi trong Phần V,) và chúng tôi chỉ cập nhật các trọng số trong MLP được hợp nhất.

Cụ thể, chúng tôi đặt tuning loss như mean squared error (MSE) giữa đầu ra lớp Hlt trong mô hình teacher và đầu ra lớp Hl trong mô hình student cho lớp l của PLM. Tuning loss sau đó được tính như:
ℓtune=∑Ll=1MSE(Hlt,Hl) (10)
trong đó L là số lượng lớp trong PLM và MSE ký hiệu mean squared error.

V. KẾT QUẢ SỐ HỌC
Trong phần này, chúng tôi trình bày kết quả số học của MLP Fusion so với các baseline đại diện trên cả tác vụ NLU và NLG. Chúng tôi bắt đầu với thiết lập thí nghiệm và một đánh giá ban đầu về lỗi xấp xỉ, tiếp theo là nghiên cứu ablation và đánh giá hiệu quả. Thông tin thí nghiệm chi tiết được bao gồm trong tài liệu bổ sung.

A. Thiết lập Thí nghiệm
1) Mô hình Backbone: Vì các thí nghiệm của chúng tôi bao gồm cả tác vụ NLU và NLG, chúng tôi tận dụng các loại mô hình khác nhau được điều chỉnh cho từng loại. Cụ thể, chúng tôi sử dụng RoBERTa [66], một kiến trúc chỉ encoder, cho các tác vụ NLU, và GPT-2 [67], một kiến trúc chỉ decoder, cho các tác vụ NLG. Đối với các mô hình MoE, chúng tôi sử dụng Switch Transformer [5], một mô hình encoder-decoder với 16 expert trong mỗi lớp MoE.

2) Phương pháp Baseline: Chúng tôi chủ yếu so sánh phương pháp của chúng tôi với các phương pháp nén một lần: tinh chỉnh thông thường [1], SVD bị cắt cụt [20], Pruning (pruning không có cấu trúc một lần) [15]–[18], LTH (Lottery Ticket Hypothesis) [45], [46], GEM-MINER [68], Moefication [43]. Chúng tôi cũng so sánh phương pháp pruning có cấu trúc FLOP (Factorized Low-rank Pruning) [69]. Ngoài ra, chúng tôi liệt kê hiệu suất của DistilRoBERTa [70] như tham chiếu cho các tác vụ NLU.

Theo mặc định, tất cả các phương pháp baseline được đặt để giảm kích thước trung gian MLP từ 3076 xuống 768 hoặc số tham số tương đương. Cụ thể, (i) SVD bị cắt cụt chỉ giữ t giá trị đơn lớn nhất và các vector đơn liên kết. (ii) Pruning không có cấu trúc loại bỏ toàn cục một tỷ lệ nhất định của kết nối bằng cách khám phá độ lớn trọng số và gradient. Cụ thể, chúng tôi mask 75% kết nối để phù hợp với tỷ lệ nén (25%). Như một trường hợp đặc biệt của pruning, (iii) Lottery tickets hypothesis (LTH) [45], [46], chứng minh sự tồn tại của các subnetwork thưa thớt trong DNN, thực hiện sparsification lặp trong quá trình tuning để tìm matching network. Chúng tôi lặp lại prune MLP trong một epoch để làm cho nó tương đương về mặt tính toán với module tuning cụ thể tác vụ layer-wise (được giới thiệu trong Phần IV-D). Tỷ lệ mask mạng cũng là 75% như pruning. (iv) Moefication chia các module MLP của một PLM thành nhiều sub-network và thiết kế cơ chế route bổ sung để quyết định sub-network tương ứng cho mỗi đầu vào. Ở đây chúng tôi chia MLP gốc thành 4 sub-network để phù hợp với tỷ lệ nén mạng.

Hơn nữa, chúng tôi giới thiệu ba kỹ thuật học máy, randomized sketching, MMD approximation, và regular clustering, như các baseline mạnh bổ sung. Chúng tôi chứng minh như sau cách áp dụng chúng cho nén MLP.

Sketching các Ma trận Trọng số. Ý tưởng của Sketching là giảm kích thước của W1,W2 bằng cách nhân với một ma trận S:
H̃S=σ(XW 1S+1b⊤1S)S⊤W2+1b⊤2,
trong đó S có thể là một Ma trận Sketching Gaussian, áp dụng phép biến đổi Johnson–Lindenstrauss [71] cho các ma trận trọng số W1,W2. Chúng tôi mong đợi Sketching có thể ít nhiều bảo tồn thông tin trong PLM.

Nén thông qua Tối thiểu hóa MMD. Như được mô tả trong Phần IV-B, chúng tôi đề xuất xem MLP như một phân phối thực nghiệm của sub-MLP. Trực quan, MLP có thể được nén bằng cách tối thiểu hóa khoảng cách MMD giữa phân phối MLP gốc và nén. Phương pháp này tạo ra một mô hình nén với ít điểm hỗ trợ hơn trong khi bảo tồn các tính chất chính (chi tiết được cung cấp trong Phụ lục B-B trong bổ sung).

Clustering không có xấp xỉ NTK. Hơn nữa, để ablate rõ ràng hiệu ứng của xấp xỉ NTK, chúng tôi triển khai một phương pháp dựa trên clustering trong đó các trọng số hợp nhất W̃1,b̃1, và W̃2 được thay thế như sau:
W̃1=W1C̄⊤P1/2,b̃1=C̄b1P1/2,W̃2=P1/2C̄W 2.
Ở đây, P=CC⊤ là một ma trận đường chéo. MLP tương ứng sau đó được định nghĩa như:
σ(XW̃1+1b̃⊤1)W̃2+1b⊤2,
có cùng kiến trúc như "Sketching" và "MMD".

Đối với layer-wise task-specific tuning, chúng tôi áp dụng RoBERTa [66]/Switch Transformer [5] gốc (GPT-2 [67] cho sinh ngôn ngữ) như teacher và mô hình ngôn ngữ với MLP hợp nhất như student, trên hai tác vụ NLU. Tuning chỉ kéo dài 1 epoch để phù hợp với chi phí tính toán của preprocessing trong LTH.

B. Đánh giá Sơ bộ về Lỗi Xấp xỉ
Như một kiểm tra hợp lý, trước tiên chúng tôi thực hiện đánh giá sơ bộ về lỗi xấp xỉ NTK cho mỗi phương pháp áp dụng được. Chúng tôi kiểm tra đầu ra và NTK được tạo ra của MLP lớp đầu tiên trên tập validation của SST2 với RoBERTa-base [66] được nén bởi các phương pháp baseline khác nhau. Kết quả, được trung bình hóa qua ba lần chạy, được tóm tắt trong Bảng I. Lưu ý rằng DistilRoBERTa và Moefication đã bị loại trừ khỏi phân tích này do tập trung vào các mục tiêu khác nhau. Cụ thể, SVD là một phương pháp deterministic và do đó cho độ lệch chuẩn bằng 0. Chúng tôi đưa ra các quan sát sau: (i) Hầu hết các phương pháp được liệt kê có thể xấp xỉ tốt đầu ra MLP với khoảng cách đầu ra nhỏ giữa RoBERTa gốc và mô hình nén. (ii) MLP Fusion đạt được lỗi xấp xỉ NTK nhỏ nhất trong tất cả phương pháp, được định nghĩa như sự khác biệt l2 giữa ma trận kernel NTK được tính trên các mẫu đánh giá trước và sau nén. Thí nghiệm này xác minh đề xuất của chúng tôi rằng MLP Fusion bảo tồn tốt nhất động lực học huấn luyện của PLM gốc.

C. Thí nghiệm về Hiểu Ngôn ngữ Tự nhiên
Chúng tôi cung cấp so sánh thí nghiệm mở rộng dựa trên RoBERTa như PLM với một tập các baseline đại diện trên các benchmark hiểu ngôn ngữ tự nhiên SST2, MNLI, có thể được tìm thấy trong Bảng II, trong khi kết quả test bổ sung được cung cấp trong tài liệu bổ sung. Hơn nữa, chúng tôi trình bày so sánh hiệu suất giữa các phương pháp khác nhau sau task-specific fine-tuning và hai baseline bổ sung cố gắng duy trì đầu ra MLP và NTK trong Phụ lục.

DistilRoBERTa, một phiên bản nhẹ của RoBERTa với cùng quy trình huấn luyện, phục vụ như baseline để so sánh. Đối với tất cả phương pháp nén, chúng tôi giảm kích thước trung gian xuống 25% (3072→768), phù hợp với kích thước đầu vào MLP. Pruning/LTH bao gồm masking 75% kết nối MLP, trong khi Moefication chia MLP thành bốn expert để giảm kích thước mô hình. Tất cả việc giảm được áp dụng cho 8 lớp cuối của PLM để đảm bảo công bằng.

Từ bảng, chúng tôi có những phát hiện sau: (i) MLP Fusion vượt trội so với tất cả baseline, điều này chứng minh hiệu quả của phương pháp được đề xuất của chúng tôi. (ii) Không có xấp xỉ NTK, có sự giảm rõ ràng trong hiệu suất của Clustering, điều này xác minh sự cần thiết của nó. (iii) Layer-wise task-specific tuning tiếp tục tăng cường hiệu suất của MLP Fusion bằng cách kết hợp kiến thức cụ thể tác vụ. Đáng chú ý rằng LTH và GEM-MINER cũng đòi hỏi preprocessing khi masking kết nối; sau khi làm cho chúng tương đương về mặt tính toán với MLP Fusion, hiệu suất vượt trội của phương pháp chúng tôi xác thực rõ ràng hơn các ưu điểm của nó.

Đối với mô hình MoE Switch Transformer, các thí nghiệm được thực hiện dưới điều kiện tương tự (c.f. kết quả trong Bảng III). Không giống như RoBERTa, một phiên bản chưng cất của Switch Transformer không có sẵn. Ngoài việc chọn baseline hoạt động tốt nhất từ Bảng II, chúng tôi bao gồm M-SMoE [24], một phương pháp SFT cụ thể MoE, và NTK-SAP [47], kết hợp NTK với pruning. Kết luận tương tự có thể được rút ra rằng (i) Hiệu suất của Clustering tụt lại phía sau so với MLP Fusion, thường vượt trội so với các phương pháp baseline. Hơn nữa, sau chưng cất bổ sung, hiệu suất cải thiện thậm chí nhiều hơn. (ii) Đáng chú ý rằng mặc dù NTK-SAP tạo ra kết quả đáng khen ngợi, tốc độ tăng tốc thực tế bị hạn chế, như được thảo luận thêm trong Phần V-F.

D. Thí nghiệm về Sinh Ngôn ngữ Tự nhiên
Trong phần này, chúng tôi điều tra hiệu quả của MLP Fusion được đề xuất bằng cách đánh giá trên benchmark sinh ngôn ngữ tự nhiên WebNLG với một tập các baseline một lần tương đương. Kết quả được báo cáo trong Bảng IV. Thiết lập nén/pruning giống với đánh giá NLU được hiển thị trong Phần V-C. Phương pháp được đề xuất của chúng tôi nói chung bảo tồn hiệu suất của phương pháp tinh chỉnh naive một cách hiệu quả nhất. MLP Fusion đạt được cải thiện độ chính xác trung bình khoảng 1% so với các baseline. Trong số các baseline, phương pháp pruning nổi bật do việc bảo tồn kích thước ma trận trọng số MLP gốc. Tuy nhiên, việc thiếu hỗ trợ mạnh mẽ cho phép nhân ma trận thưa thớt trên phần cứng GPU hiện đại hạn chế các lợi ích hiệu quả thực tế, như được chi tiết trong Phần V-F.

E. Nghiên cứu Ablation
Tác động của Sketch Layer. Để phân tích hiệu ứng của sketching layer trong PLM, chúng tôi mở rộng thí nghiệm của chúng tôi ngoài việc sketching 8 lớp cuối trên SST-2. Hình 4 cung cấp hiểu biết, với các đường nét đứt biểu diễn hiệu suất của RoBERTa và DistilRoBERTa. Giá trị trục hoành "2" tương ứng với sketching 2 lớp cuối. MLP Fusion liên tục đạt được hiệu suất tương đương hoặc vượt trội so với các baseline, thường vượt trội so với RoBERTa thô khi ít hơn 6 lớp được sketched. Điều này chứng minh tiềm năng của MLP Fusion để giảm redundancy trong mạng neural, hiệu quả cung cấp "bữa trưa miễn phí" trong tinh chỉnh PLM. Hơn nữa, MLP Fusion liên tục vượt trội so với DistilRoBERTa miễn là ít hơn 10 lớp được sketched. Tuy nhiên, sketching tất cả 12 lớp giảm đáng kể hiệu suất, cho thấy rằng các lớp dưới của PLM giữ lại thông tin semantic quan trọng. Quan sát này phù hợp với các phát hiện trong [72].

Lựa chọn Chiều Trung gian trong MLP Fusion. Trong các thí nghiệm ban đầu của chúng tôi, chúng tôi đặt chiều trung gian thành 768, phù hợp với kích thước đầu vào MLP. Để khám phá tác động của nó, chúng tôi phân tích động lực học huấn luyện và hiệu suất test của MLP Fusion qua các kích thước trung gian khác nhau. Như được hiển thị trong Hình 5, training loss giữ ổn định khi chiều trung gian là 768 hoặc lớn hơn. Tuy nhiên, việc giảm chiều dưới 768 dẫn đến sự tăng mạnh trong loss do MLP bị rank-deficient, điều này ảnh hưởng xấu đến hiệu suất. Kết quả test, được trình bày trong Bảng V, thể hiện xu hướng tương tự, nhấn mạnh tầm quan trọng của việc duy trì chiều trung gian đủ.

Ảnh hưởng của Đóng băng Router Matrix. Dựa trên quan sát rằng thông tin thế giới phổ quát của LLM sẽ tác động đến hiệu suất của chúng [57]–[59], chúng tôi theo [24] để đóng băng router matrix khi tinh chỉnh mô hình MoE. Ở đây, chúng tôi so sánh kết quả thực nghiệm từ tinh chỉnh Switch Transformer trên dataset MRPC trong Bảng VI.

F. Đánh giá Hiệu quả
Trong phần con này, chúng tôi so sánh hiệu quả của phương pháp chúng tôi với LoRA, trên mô hình MoE Switch Transformer (trong đó MLP chiếm tỷ lệ tham số cao hơn các mô hình dày đặc). Chúng tôi cung cấp FLOPs và tổng thời gian trong SFT trong Bảng VII, và những cái trong giai đoạn inference trong Bảng VIII. Đối với các thí nghiệm trong giai đoạn SFT, chúng tôi sử dụng dataset huấn luyện SST2 trong 5 vòng, với batch size 32 và độ dài chuỗi 100. Đối với các thí nghiệm trong giai đoạn inference, chúng tôi test mô hình trên tập validation SST2 với 10 epoch. Chúng tôi đặt config cho LoRA trên tất cả lớp MLP (khác với 8 lớp MoE hàng đầu của chúng tôi) với rank r= 8, hệ số α= 32 (các hyperparameter trong LoRA).

Một số kết luận có thể được thu được dựa trên kết quả: (i) NTK-SAP và Hiệu quả Phần cứng. Như một phương pháp pruning không có cấu trúc, NTK-SAP không chuyển đổi thành cải thiện hiệu quả phần cứng. Bất chấp việc giảm số lượng tham số, bản chất của pruning không có cấu trúc không phù hợp với khả năng tăng tốc phần cứng, được tối ưu hóa nhiều hơn cho việc giảm có cấu trúc. (ii) Trade-off của LoRA. Trong khi LoRA hiệu quả giảm sử dụng bộ nhớ trong giai đoạn SFT bằng cách chỉ sửa đổi một vài tham số, nó không cải thiện hiệu quả trong giai đoạn inference. Ngay cả khi overhead từ các ma trận low-rank bổ sung có thể được loại bỏ bằng cách tích hợp chúng trở lại vào các ma trận đầy đủ, lợi ích hiệu quả vẫn giới hạn ở giai đoạn huấn luyện thay vì inference. (iii) Structured Pruning và SVD. Các phương pháp này cung cấp trade-off cân bằng, với structured pruning và SVD giảm đáng kể bộ nhớ và kích thước tham số. Chúng cũng hoạt động tốt trong inference, làm cho chúng thuận lợi cho các kịch bản triển khai nơi bộ nhớ và tốc độ là quan trọng. (iv) Tính nhất quán của MLP Fusion. Phương pháp của chúng tôi chứng minh lợi ích hiệu quả tương tự như structured pruning và SVD. Nó giảm sử dụng bộ nhớ và duy trì hiệu suất runtime cạnh tranh trong cả huấn luyện và inference, cho thấy tính mạnh mẽ và phù hợp cho triển khai.

VI. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất MLP fusion, một phương pháp nén mô hình một lần mới sử dụng clustering để xấp xỉ NTK của PLM gốc. Chúng tôi chứng minh rằng MLP hợp nhất có thể vừa xấp xỉ tốt đầu ra và đạt được NTK gần nhất với cái gốc so với các phương pháp nén một lần khác. Cùng lúc, MLP fusion có thể được áp dụng cho các mô hình Mixture-of-Experts và thu được lợi ích lớn hơn về hiệu quả không gian, nhấn mạnh hiệu quả phổ quát và khả năng mở rộng của nó trong PLM. Một mở rộng trực tiếp của công trình chúng tôi là sử dụng MLP fusion như một phương pháp khởi tạo cho chưng cất. So với việc huấn luyện lại từ đầu, chúng tôi mong đợi thông tin được bảo tồn trong MLP hợp nhất có thể dễ dàng chưng cất tiếp theo và tăng tốc hội tụ mô hình. Chúng tôi tin rằng MLP fusion làm sáng tỏ paradigm mới cho tinh chỉnh mô hình ngôn ngữ hiệu quả.

ACKNOWLEDGEMENTS
Công trình này được hỗ trợ bởi National Science Foundation dưới Giải thưởng Số IIS-1947203, IIS-2117902, IIS-2137468, và Agriculture and Food Research Initiative (AFRI) grant số 2020-67021-32799/project accession số 1024178 từ USDA National Institute of Food and Agriculture. Các quan điểm và kết luận là của các tác giả và không nên được hiểu như đại diện cho các chính sách chính thức của các cơ quan tài trợ hoặc chính phủ.

REFERENCES
[1] J. Howard và S. Ruder, "Universal language model fine-tuning for text classification," trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, tr. 328–339.
[2] M. Kale và A. Rastogi, "Text-to-text pre-training for data-to-text tasks," trong Proceedings of the 13th International Conference on Natural Language Generation. Dublin, Ireland: Association for Computational Linguistics, Dec. 2020, tr. 97–102.
[3] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, và L. Zettlemoyer, "Deep contextualized word representations," trong Proceedings of NAACL-HLT, 2018, tr. 2227–2237.
[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.
[5] W. Fedus, B. Zoph, và N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," The Journal of Machine Learning Research, tập 23, số 1, tr. 5232–5270, 2022.
[6] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và D. Amodei, "Scaling laws for neural language models," 2020.
[7] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[8] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, và G. Lample, "Llama: Open and efficient foundation language models," 2023.
[9] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, và J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.
[10] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, và W. E. Sayed, "Mixtral of experts," 2024.
[11] G. Hinton, O. Vinyals, và J. Dean, "Distilling the knowledge in a neural network," 2015, cite arxiv:1503.02531Comment: NIPS 2014 Deep Learning Workshop.
[12] J. Gou, B. Yu, S. J. Maybank, và D. Tao, "Knowledge distillation: A survey," International Journal of Computer Vision, tập 129, tr. 1789–1819, 2021.
[13] H. He, J. Wang, Z. Zhang, và F. Wu, "Compressing deep graph neural networks via adversarial knowledge distillation," trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ser. KDD '22. New York, NY, USA: Association for Computing Machinery, 2022, tr. 534–544. [Online]. Available: https://doi.org/10.1145/3534678.3539315
[14] S. Kang, J. Hwang, W. Kweon, và H. Yu, "De-rrd: A knowledge distillation framework for recommender system," trong Proceedings of the 29th ACM International Conference on Information & Knowledge Management, ser. CIKM '20. New York, NY, USA: Association for Computing Machinery, 2020, tr. 605–614. [Online]. Available: https://doi.org/10.1145/3340531.3412005
[15] S. Han, J. Pool, J. Tran, và W. Dally, "Learning both weights and connections for efficient neural network," Advances in neural information processing systems, tập 28, 2015.
[16] N. Lee, T. Ajanthan, và P. Torr, "SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY," trong International Conference on Learning Representations, 2019.
[17] C. Wang, G. Zhang, và R. Grosse, "Picking winning tickets before training by preserving gradient flow," arXiv preprint arXiv:2002.07376, 2020.
[18] H. Tanaka, D. Kunin, D. L. Yamins, và S. Ganguli, "Pruning neural networks without any data by iteratively conserving synaptic flow," Advances in Neural Information Processing Systems, tập 33, tr. 6377–6389, 2020.
[19] T. Dao, B. Chen, N. S. Sohoni, A. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra, và C. Ré, "Monarch: Expressive structured matrices for efficient and accurate training," trong International Conference on Machine Learning. PMLR, 2022, tr. 4690–4721.
[20] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, và R. Fergus, "Exploiting linear structure within convolutional networks for efficient evaluation," Advances in neural information processing systems, tập 27, 2014.
[21] X. Wang, Y. Zheng, Z. Wan, và M. Zhang, "Svd-llm: Truncation-aware singular value decomposition for large language model compression," 2024. [Online]. Available: https://arxiv.org/abs/2403.07378
[22] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, và W. Chen, "Lora: Low-rank adaptation of large language models," 2021.

--- TRANG 11 ---
[23] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, và D. Tao, "Merging experts into one: Improving computational efficiency of mixture of experts," trong Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, tr. 14 685–14 691.
[24] P. Li, Z. Zhang, P. Yadav, Y.-L. Sung, Y. Cheng, M. Bansal, và T. Chen, "Merge, then compress: Demystify efficient smoe with hints from its routing policy," arXiv preprint arXiv:2310.01334, 2023.
[25] F. Xue, X. He, X. Ren, Y. Lou, và Y. You, "One student knows all experts know: From sparse to dense," 2022.
[26] C. Liu, C. Lou, R. Wang, A. Y. Xi, L. Shen, và J. Yan, "Deep neural network fusion via graph matching with applications to model ensemble and federated learning," trong Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, và S. Sabato, Eds., tập 162. PMLR, 17–23 Jul 2022, tr. 13 857–13 869.
[27] G. Stoica, D. Bolya, J. Bjorner, P. Ramesh, T. Hearn, và J. Hoffman, "Zipit! merging models from different tasks without training," 2024.
[28] X. Lu, Q. Liu, Y. Xu, A. Zhou, S. Huang, B. Zhang, J. Yan, và H. Li, "Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models," 2024. [Online]. Available: https://arxiv.org/abs/2402.14800
[29] N. Kitaev, L. Kaiser, và A. Levskaya, "Reformer: The efficient transformer," trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[30] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlós, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. Colwell, và A. Weller, "Rethinking attention with performers," CoRR, tập abs/2009.14794, 2020.
[31] Y. Chen, Q. Zeng, H. Ji, và Y. Yang, "Skyformer: Remodel self-attention with gaussian kernel and nyström method," Advances in Neural Information Processing Systems, tập 34, tr. 2122–2135, 2021.
[32] A. Jacot, F. Gabriel, và C. Hongler, "Neural tangent kernel: Convergence and generalization in neural networks," Advances in neural information processing systems, tập 31, 2018.
[33] S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, và R. Wang, "On exact computation with an infinitely wide neural net," Advances in Neural Information Processing Systems, tập 32, 2019.
[34] S. P. Singh và M. Jaggi, "Model fusion via optimal transport," Advances in Neural Information Processing Systems, tập 33, tr. 22 045–22 055, 2020.
[35] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, và C. Potts, "Recursive deep models for semantic compositionality over a sentiment treebank," trong Proceedings of the 2013 conference on empirical methods in natural language processing, 2013, tr. 1631–1642.
[36] V. Sanh, L. Debut, J. Chaumond, và T. Wolf, "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," arXiv preprint arXiv:1910.01108, 2019.
[37] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, và Q. Liu, "Tinybert: Distilling bert for natural language understanding," arXiv preprint arXiv:1909.10351, 2019.
[38] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, và M. Zhou, "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers," Advances in Neural Information Processing Systems, tập 33, tr. 5776–5788, 2020.
[39] S. Lohit và M. Jones, "Model compression using optimal transport," trong Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, tr. 2764–2773.
[40] Z. Huang và N. Wang, "Like what you like: Knowledge distill via neuron selectivity transfer," arXiv preprint arXiv:1707.01219, 2017.
[41] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, và Q. Ju, "Fastbert: a self-distilling bert with adaptive inference time," arXiv preprint arXiv:2004.02178, 2020.
[42] J. Xin, R. Tang, J. Lee, Y. Yu, và J. Lin, "Deebert: Dynamic early exiting for accelerating bert inference," trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, tr. 2246–2251.
[43] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, và J. Zhou, "Moefication: Conditional computation of transformer models for efficient inference," arXiv preprint arXiv:2110.01786, 2021.
[44] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, và C. Zhang, "Learning efficient convolutional networks through network slimming," trong Proceedings of the IEEE international conference on computer vision, 2017, tr. 2736–2744.
[45] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," arXiv preprint arXiv:1803.03635, 2018.
[46] T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, và M. Carbin, "The lottery ticket hypothesis for pre-trained bert networks," Advances in neural information processing systems, tập 33, tr. 15 834–15 846, 2020.
[47] Y. Wang, D. Li, và R. Sun, "NTK-SAP: Improving neural network pruning by aligning training dynamics," trong The Eleventh International Conference on Learning Representations, 2023.
[48] L. Iurada, M. Ciccone, và T. Tommasi, "Finding lottery tickets in vision models via data-driven spectral foresight pruning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, tr. 16 142–16 151.
[49] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," 2015.
[50] D. P. Woodruff et al., "Sketching as a tool for numerical linear algebra," Foundations and Trends ®in Theoretical Computer Science, tập 10, số 1–2, tr. 1–157, 2014.
[51] Y. Chen, Q. Zeng, D. Hakkani-Tur, D. Jin, H. Ji, và Y. Yang, "Sketching as a tool for understanding and accelerating self-attention for long sequences," trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Seattle, United States: Association for Computational Linguistics, Jul. 2022, tr. 5187–5199.
[52] H. Robbins và S. Monro, "A stochastic approximation method," The annals of mathematical statistics, tr. 400–407, 1951.
[53] D. P. Kingma và J. Ba, "Adam: A method for stochastic optimization," trong 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio và Y. LeCun, Eds., 2015.
[54] S. Malladi, A. Wettig, D. Yu, D. Chen, và S. Arora, "A kernel-based view of language model fine-tuning," arXiv preprint arXiv:2210.05643, 2022.
[55] A. Wei, W. Hu, và J. Steinhardt, "More than a toy: Random matrix models predict how real-world neural representations generalize," arXiv preprint arXiv:2203.06176, 2022.
[56] L. Gu, Y. Du, Y. Zhang, D. Xie, S. Pu, R. C. Qiu, và Z. Liao, ""lossless" compression of deep neural networks: A high-dimensional neural tangent kernel approach," trong Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, và K. Cho, Eds., 2022.
[57] G. He, J. Chen, và J. Zhu, "Preserving pre-trained features helps calibrate fine-tuned language models," trong The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=NI7StoWHJPT
[58] J. Mukhoti, Y. Gal, P. H. S. Torr, và P. K. Dokania, "Fine-tuning can cripple your foundation model; preserving features may be the solution," 2023.
[59] S. Dou, E. Zhou, Y. Liu, S. Gao, J. Zhao, W. Shen, Y. Zhou, Z. Xi, X. Wang, X. Fan et al., "Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment," arXiv preprint arXiv:2312.09979, 2023.
[60] Y. Chen, D. Hazarika, M. Namazifar, Y. Liu, D. Jin, và D. Hakkani-Tur, "Inducer-tuning: Connecting prefix-tuning and adapter-tuning," trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022.
[61] B. Wang, Y. Ren, L. Shang, X. Jiang, và Q. Liu, "Exploring extreme parameter compression for pre-trained language models," trong International Conference on Learning Representations, 2022.
[62] B. Yuan, C. R. Wolfe, C. Dun, Y. Tang, A. Kyrillidis, và C. Jermaine, "Distributed learning of fully connected neural networks using independent subnet training," Proceedings of the VLDB Endowment, tập 15, số 8, tr. 1581–1590, 2022.
[63] J. M. Hammersley và K. W. Morton, "Poor man's monte carlo," Journal of the Royal Statistical Society: Series B (Methodological), tập 16, số 1, tr. 23–38, 1954.
[64] S. Lloyd, "Least squares quantization in pcm," IEEE transactions on information theory, tập 28, số 2, tr. 129–137, 1982.
[65] G. Peyré, M. Cuturi et al., "Computational optimal transport: With applications to data science," Foundations and Trends ®in Machine Learning, tập 11, số 5-6, tr. 355–607, 2019.
[66] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[67] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, tập 1, số 8, tr. 9, 2019.
[68] K. Sreenivasan, J. yong Sohn, L. Yang, M. Grinde, A. Nagle, H. Wang, E. Xing, K. Lee, và D. Papailiopoulos, "Rare gems: Finding lottery tickets at initialization," trong Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, và K. Cho, Eds., 2022.

--- TRANG 12 ---
[69] Z. Wang, J. Wohlwend, và T. Lei, "Structured pruning of large language models," trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online: Association for Computational Linguistics, Nov. 2020, tr. 6151–6162.
[70] V. Sanh, L. Debut, J. Chaumond, và T. Wolf, "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," ArXiv, tập abs/1910.01108, 2019.
[71] N. Ailon và B. Chazelle, "The fast johnson–lindenstrauss transform and approximate nearest neighbors," SIAM Journal on computing, tập 39, số 1, tr. 302–322, 2009.
[72] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, và Y. Artzi, "Revisiting few-sample bert fine-tuning," arXiv preprint arXiv:2006.05987, 2020.
[73] Y. Chen, D. Hazarika, M. Namazifar, Y. Liu, D. Jin, và D. Hakkani-Tur, "Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention," trong Findings of the Association for Computational Linguistics: NAACL 2022. Association for Computational Linguistics, 2022.
[74] Y. Zhao, J. Huang, J. Hu, X. Wang, Y. Mao, D. Zhang, Z. Jiang, Z. Wu, B. Ai, A. Wang, W. Zhou, và Y. Chen, "Swift:a scalable lightweight infrastructure for fine-tuning," 2024. [Online]. Available: https://arxiv.org/abs/2408.05517
[75] I. Loshchilov và F. Hutter, "Decoupled weight decay regularization," trong International Conference on Learning Representations, 2018.
[76] W. B. Dolan và C. Brockett, "Automatically constructing a corpus of sentential paraphrases," trong Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. [Online]. Available: https://aclanthology.org/I05-5002
[77] A. Warstadt, A. Singh, và S. R. Bowman, "Neural network acceptability judgments," Transactions of the Association for Computational Linguistics, tập 7, tr. 625–641, 2019. [Online]. Available: https://aclanthology.org/Q19-1040
[78] C. Gardent, A. Shimorina, S. Narayan, và L. Perez-Beltrachini, "Creating training corpora for nlg micro-planning," trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics (ACL), Aug. 2017, tr. 179–188, 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017 ; Conference date: 30-07-2017 Through 04-08-2017.
[79] K. Papineni, S. Roukos, T. Ward, và W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, tr. 311–318.
[80] S. Banerjee và A. Lavie, "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments," trong Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, tr. 65–72.
[81] M. Snover, B. Dorr, R. Schwartz, L. Micciulla, và J. Makhoul, "A study of translation edit rate with targeted human annotation," trong Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, 2006, tr. 223–231.
[82] K. Fukushima, "Cognitron: A self-organizing multilayered neural network," Biological cybernetics, tập 20, số 3, tr. 121–136, 1975.
[83] L. Balles và P. Hennig, "Dissecting adam: The sign, magnitude and variance of stochastic gradients," trong International Conference on Machine Learning. PMLR, 2018, tr. 404–413.
[84] L. Dong, S. Xu, và B. Xu, "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition," trong 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, tr. 5884–5888.
[85] S. Geng, S. Liu, Z. Fu, Y. Ge, và Y. Zhang, "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)," trong Proceedings of the 16th ACM Conference on Recommender Systems, 2022, tr. 299–315.
[86] T. Wei và J. He, "Comprehensive fair meta-learned recommender system," trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, tr. 1989–1999.
[87] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, và T.-Y. Liu, "Do transformers really perform badly for graph representation?" Advances in Neural Information Processing Systems, tập 34, tr. 28 877–28 888, 2021.
[88] T. Wei, Y. You, T. Chen, Y. Shen, J. He, và Z. Wang, "Augmentations in hypergraph contrastive learning: Fabricated and generative," trong Advances in Neural Information Processing Systems, 2022.

Mengting Ai là sinh viên tiến sĩ năm đầu tại Trường Khoa học Thông tin, Đại học Illinois tại Urbana-Champaign. Cô nhận bằng Thạc sĩ Khoa học Máy tính từ Đại học Illinois tại Urbana-Champaign năm 2023 và bằng Cử nhân Khoa học Máy tính từ Đại học Giao thông Tây An năm 2022, tốt nghiệp với danh hiệu sinh viên xuất sắc. Nghiên cứu của cô tập trung vào học máy hiệu quả, đặc biệt là nén mô hình ngôn ngữ lớn. Mengting đã xuất bản tại các hội nghị hàng đầu như KDD và tích cực đóng góp cho cộng đồng nghiên cứu, bao gồm phục vụ trong ban chương trình AAAI 2024. Chuyên môn của cô trong NLP và hệ thống AI khả mở rộng xuất phát từ cả nghiên cứu học thuật và các dự án hợp tác, phản ánh cam kết thúc đẩy các công nghệ AI hiệu quả và khả mở rộng.

Tianxin Wei là sinh viên tiến sĩ năm thứ tư tại Đại học Illinois tại Urbana-Champaign. Anh nhận bằng Cử nhân Khoa học Máy tính từ Trường Thanh niên Tài năng tại Đại học Khoa học và Công nghệ Trung Quốc năm 2020. Nghiên cứu của anh tập trung vào học máy đáng tin cậy, với ứng dụng trong mô hình hóa ngôn ngữ, truy xuất thông tin và nông nghiệp. Anh là người nhận đề cử Đại học cho Apple Scholar trong AI/ML 2024, học bổng thực tập Amazon năm 2024, giải thưởng Trình bày Hội nghị tại UIUC năm 2023, giải thưởng NeurIPS Scholar năm 2022 và 2023, và giải thưởng ICML Grant năm 2023. Anh đã tác giả hơn 20 xuất bản tại các hội nghị lớn (ICML, NeurIPS, ICLR, KDD), và công trình của anh đã nhận giải thưởng SIGIR Best Paper Honorable Mention năm 2021. Anh đã phục vụ như thành viên ban chương trình cho các địa điểm hàng đầu (ICML, NeurIPS, ICLR, KDD, AAAI, WSDM, v.v.). Sự cống hiến của anh trong việc thúc đẩy nghiên cứu có tác động được công nhận với giải thưởng Outstanding Reviewer cho KDD 2025.

Yifan Chen (Thành viên, IEEE) nhận bằng Cử nhân từ Đại học Fudan, Thượng Hải, Trung Quốc, năm 2018, và bằng Tiến sĩ về Thống kê từ Đại học Illinois Urbana-Champaign năm 2023. Hiện anh là giáo sư trợ lý về khoa học máy tính và toán học tại Đại học Baptist Hong Kong. Anh có quan tâm rộng rãi trong việc phát triển các thuật toán học máy hiệu quả, bao gồm cả mô hình thống kê và học sâu. Anh đã xuất bản nhiều bài báo trong các lĩnh vực này, bao gồm ICML, Neurips, KDD, v.v.

Zeming Guo hiện đang theo học bằng Thạc sĩ Khoa học tại Đại học Cornell, nơi nghiên cứu của anh tập trung vào việc thúc đẩy mô hình hóa ngôn ngữ. Anh nhận bằng Cử nhân Khoa học Thông tin từ Đại học Illinois tại Urbana-Champaign năm 2023. Anh đã làm việc tại Amazon Web Services (AWS), nơi anh đóng góp vào việc phát triển các hệ thống phân tán quy mô lớn. Sở thích nghiên cứu của anh bao gồm lý luận mô hình ngôn ngữ lớn, chưng cất kiến thức, và tăng cường hiệu quả của các mô hình học sâu. Anh đã xuất bản các bài báo tác giả đầu tại hội nghị ICML danh tiếng.

Jingrui He (Thành viên cấp cao, IEEE) là Giáo sư tại Trường Khoa học Thông tin, Đại học Illinois tại Urbana-Champaign. Cô nhận bằng Tiến sĩ từ Đại học Carnegie Mellon năm 2010. Nghiên cứu của cô tập trung vào học máy không đồng nhất, học tích cực, neural bandit, và học tự giám sát, với ứng dụng trong an ninh, nông nghiệp, phân tích mạng xã hội, chăm sóc sức khỏe, và tài chính. Tiến sĩ He là người nhận giải thưởng NSF CAREER 2016, giải thưởng OAT 2020, ba lần nhận giải thưởng IBM Faculty Award năm 2018, 2015 và 2014 tương ứng, và được chọn như IJCAI 2017 Early Career Spotlight. Tiến sĩ He có hơn 180 xuất bản tại các hội nghị lớn (ví dụ: ICML, NeurIPS, ICLR, KDD) và tạp chí (ví dụ: TMLR, TKDD, JMLR), và là tác giả của hai cuốn sách. Các bài báo của cô đã nhận giải thưởng Distinguished Paper Award tại FAccT 2022, cũng như Best of the Conference tại ICDM 2016, ICDM 2010, và SDM 2010. Tiến sĩ He là Thành viên Danh dự của ACM, Thành viên cấp cao của AAAI và IEEE. Cô cũng là đồng Chủ tịch Chương trình của IEEE BigData 2023.

--- TRANG 13 ---
Tài liệu Bổ sung cho "MLP Fusion: Hướng tới Tinh chỉnh Hiệu quả của Các Mô hình Ngôn ngữ Dày đặc và Mixture-of-Experts"

PHỤ LỤC A
CHI TIẾT CỦA CÁC THÍ NGHIỆM

A. Thiết lập Thí nghiệm
Chúng tôi đánh giá MLP fusion được đề xuất trên các tác vụ NLP downstream khác nhau và cung cấp một bản phác thảo về các tác vụ này trong phần này. Ngoài ra, chúng tôi giới thiệu ngắn gọn hai phương pháp baseline trực quan nhưng không tầm thường, "Sketching" và "MMD", trong Phần II. Một phần của việc triển khai thí nghiệm được mượn từ [24], [60], [73], [74]. Mã cho các thuật toán của chúng tôi có sẵn tại https://github.com/weitianxin/MLP_Fusion.

Tất cả các mô hình trong công trình này được triển khai bằng PyTorch. Các thí nghiệm đều được thực hiện trên một GPU Tesla V100 32 GB. Đối với các tác vụ NLU, chúng tôi tinh chỉnh RoBERTa [66] với optimizer AdamW [75] và sử dụng polynomial learning rate scheduler để làm cho learning rate giảm tuyến tính; cụ thể, learning rate được khởi động tuyến tính từ 0 trong 0.06 epoch đầu tiên. Learning rate được tìm kiếm trong phạm vi {1e-5, 2e-5,4e-5, 6e-5, 8e-5}, và batch size được cố định là 32. Đối với các tác vụ NLG, chúng tôi tiếp tục sử dụng optimizer AdamW để tinh chỉnh GPT-2 [67], và một linear learning rate scheduler với thời gian warmup 500-step được sử dụng. Learning rate được điều chỉnh trong cùng phạm vi như trên trong khi batch size được cố định là 8. Theo mặc định, tất cả các phương pháp so sánh giảm kích thước trung gian MLP xuống 768 hoặc số tham số tương đương từ 3076. Việc giảm/sketching được thực hiện trên 8 lớp cuối của PLM theo mặc định. Đối với Clustering, chúng tôi áp dụng thuật toán K-Means do tính đơn giản và hiệu quả của nó. Để giảm biến thiên ngẫu nhiên trong kết quả, các thí nghiệm đều được trung bình hóa qua ba lần chạy.

Đối với Switch Transformer, learning rate được tìm kiếm trong phạm vi {1e-4,2e-4,3e-4,5e-4,1e-3}, batch size trong phạm vi {16,32,64}, và training epoch trong phạm vi {3,5,10,15,20}. Chi tiết của optimizer AdamW được cố định cho tất cả dataset được đưa ra trong bảng IX.

Để đảm bảo khả năng so sánh giữa các phương pháp, chúng tôi chuẩn hóa việc giảm số lượng tham số cho các expert khoảng 75%, có nghĩa là 25% tham số sẽ được giữ lại. Tất cả các phương pháp được thực hiện tại 8 lớp MoE hàng đầu của Switch Transformer.

BẢNG IX
THIẾT LẬP SIÊU THAM SỐ TINH CHỈNH CHO SWITCH TRANSFORMER.

Giá trị
Optimizer AdamW  
Adam ϵ 1e-08
Adam β (0.9, 0.98)
warm-up steps 8
weight decay 0.01

B. Runtime của tinh chỉnh sau nén PLM
Vì MLP fusion được đề xuất của chúng tôi chỉ khác với baseline sketching và mmd trong khởi tạo, chúng tôi tập trung vào đánh giá runtime của MLP fusion cùng với hai phương pháp đại diện, tinh chỉnh thông thường và pruning.

Để so sánh công bằng, chúng tôi cố ý chạy hai tác vụ NLU trên một server cluster (để không có quy trình nào khác sẽ cạnh tranh với việc tinh chỉnh mô hình) với một core của server CPU (Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz) trên Ubuntu 18.04. Trong thiết lập này, chúng tôi huấn luyện mô hình RoBERTa trong 100 step với batch size 32.

--- TRANG 14 ---
Cụ thể, trên SST2, sẽ mất khoảng 6746, 18066, 9342 giây cho mô hình với MLP fusion, pruning, và tinh chỉnh thông thường tương ứng để hoàn thành huấn luyện; trên MNLI, chi phí thời gian khoảng 6956, 17060, 18966 giây cho huấn luyện. Chúng tôi nhận xét rằng kiến trúc của MLP fusion có thể tăng tốc tinh chỉnh thông thường 30% trên SST2, và thậm chí nhanh hơn 2.7 lần trong MNLI, có độ dài chuỗi trung bình dài hơn. Đối với pruning, mặc dù nó có hiệu suất dự đoán tương đương trong hai tác vụ, chi phí thời gian của nó không ít hơn tinh chỉnh thông thường và cao hơn nhiều trong tác vụ nhẹ hơn SST2, do một số chi phí overhead từ việc triển khai của nó.

C. Số lượng tham số của SVD
Đối với SVD, để làm cho các tham số được giữ lại cho mỗi ma trận expert bằng nhau, chúng tôi có:
pI×k+k+k×p≈k×(pI+p)
s×pI×p=sppI,
trong đó s là tỷ lệ tham số chúng tôi giữ lại (25% ở đây), và k là số lượng giá trị đơn top-k trong SVD. Đối với Switch Transformer, chúng tôi có pI= 4p, vậy k=4/5sp. Đối với Qwen chúng tôi có pI=11/16p, vậy k=11/27sp.

D. Chi tiết của Dataset
Chúng tôi cung cấp chi tiết của các dataset chúng tôi sử dụng trong thí nghiệm cùng với giấy phép của chúng ở đây. Thống kê có thể được tìm thấy trong Bảng X và XI.

BẢNG X
THỐNG KÊ DATASET CỦA CÁC TÁC VỤ PHÂN LOẠI ĐƯỢC TINH CHỈNH.

Dataset Danh mục Kích thước Train Kích thước Test Số lớp
SST2 Phân tích Tình cảm 67,349 872 2
MRPC Nhận dạng Paraphrase 3,668 408 2
CoLA Đánh giá Chấp nhận Ngôn ngữ học 8,551 1,043 2
MNLI Suy luận Văn bản 392,702 9,815 3

BẢNG XI
THỐNG KÊ DATASET CỦA CÁC TÁC VỤ SINH ĐƯỢC TINH CHỈNH.

Dataset Danh mục Kích thước Train Kích thước Test Độ dài Văn bản Trung bình
WebNLG Sinh Văn bản 35,426 5,150 24.36

• SST2 [35]: SST2, Stanford Sentiment Treebank phiên bản 2, là một dataset phổ biến cho phân tích tình cảm. Nó chứa các câu đánh giá phim được gắn nhãn tích cực hoặc tiêu cực, loại trừ các câu trung tính, cung cấp một tác vụ phân loại nhị phân. Dataset này đáng chú ý vì chú thích chi tiết, vì nó bao gồm nhãn tình cảm cho mọi subphase trong cây phân tích câu. SST2 được sử dụng rộng rãi để huấn luyện và đánh giá các mô hình về phân tích tình cảm, kiểm tra khả năng hiểu các gam màu cảm xúc tinh tế trong văn bản, với giấy phép CC0: Public Domain.

• MRPC [76]: Microsoft Research Paraphrase Corpus (MRPC) đánh giá các mô hình về nhận dạng paraphrase bằng cách sử dụng các cặp câu từ các nguồn tin tức trực tuyến. MRPC là một phần của benchmark GLUE và có giá trị để đánh giá khả năng hiểu và so sánh nội dung semantic trong câu của mô hình, đặc biệt trong các tác vụ phân tích semantic. Giấy phép của MRPC không rõ.

--- TRANG 15 ---
• CoLA [77]: Corpus of Linguistic Acceptability (CoLA) đánh giá khả năng đánh giá chấp nhận ngôn ngữ học của các mô hình. Nó phân biệt giữa các câu chấp nhận được và không chấp nhận được về mặt ngữ pháp, nhấn mạnh tầm quan trọng của việc hiểu ngữ pháp trong hiểu ngôn ngữ và đánh giá mô hình. Giấy phép cho CoLA không được chỉ định.

• MNLI [77]: Dataset Multi-Genre Natural Language Inference (MNLI) là một corpus đa dạng cho các tác vụ hiểu ngôn ngữ tự nhiên, tập trung vào suy luận văn bản. Nó bao gồm các cặp câu và thách thức các mô hình xác định liệu câu thứ hai có kéo theo, mâu thuẫn, hoặc trung tính với câu đầu tiên. Phạm vi thể loại rộng và nội dung đa dạng của MNLI làm cho nó trở thành một benchmark mạnh mẽ để đánh giá các mô hình trong các tác vụ suy luận ngôn ngữ tự nhiên. Phần lớn dữ liệu thuộc giấy phép của OANC, với phần còn lại thuộc một số giấy phép cho phép, Creative Commons Share-Alike 3.0 Unported License, và Creative Commons Attribution 3.0 Unported Licenses.

• WebNLG [78]: Dataset này được cấu thành từ các cặp dữ liệu/văn bản, trong đó "dữ liệu" ở định dạng của triple (chủ thể, thuộc tính, đối tượng). Đối với tập train và validation, có chín danh mục được trích xuất từ DBpedia; trong khi trong tập test, có năm danh mục chưa thấy bổ sung, có thể phần nào phản ánh khả năng tổng quát hóa của các phương pháp. Các chuỗi đầu vào trong tập huấn luyện chứa 1 đến 7 triple, và độ dài của hầu hết chuỗi được giới hạn bởi 50 (vì mỗi triple chỉ bao gồm ba cụm từ ngắn). Script đánh giá chính thức được sử dụng trong các thí nghiệm của chúng tôi, và chúng tôi báo cáo BLEU [79], METEOR [80] và TER [81] như các metric.

PHỤ LỤC B
CÁC DẪN XUẤT BỊ BỎ QUA TRONG VĂN BẢN CHÍNH

A. Chi phí tính toán của các module attention và MLP
Việc nén các sub-layer FFN là quan trọng để có được một mô hình được huấn luyện trước nhẹ. Bên cạnh các sub-layer self-attention, các sub-layer FFN cũng chiếm nhiều thời gian tính toán và thậm chí trở thành bottleneck thực tế khi độ dài chuỗi đầu vào ngắn. Chúng tôi sẽ xác minh tuyên bố này thông qua dẫn xuất sau.

Trước tiên chúng tôi nhớ lại thiết lập phổ biến nhất của một MLP trong PLM. Lấy RoBERTa-base làm ví dụ, chiều ẩn là p= 768 và có h= 12 head trong mỗi module self-attention; chiều trung gian trong MLP là pI= 4p= 3072. Trong sub-layer self-attention, cho đầu vào độ dài n X chúng ta cần trước tiên tính ma trận query, key, và value Q,K,V, cần 3·np2 phép toán để thực hiện phép biến đổi tuyến tính (bỏ qua bias). Đối với module self-attention cốt lõi, chúng ta sẽ cần ít nhất h·2n2(p/h) phép toán nhân; phép biến đổi tuyến tính cuối cùng sẽ lại cần chi phí np2. Tổng FLOPs của một sub-layer self-attention khoảng 4np2+ 2n2p.

Đối với sub-layer FFN, chi phí tính toán rõ ràng: 2nppI= 8np2. Chúng ta có thể kiểm tra cho các tác vụ nlp thông thường trong đó độ dài đầu vào n được giới hạn bởi 512, 8np2 chứng minh lớn hơn 4np2+ 2n2p, khi p= 768. Cụ thể hơn, khi độ dài đầu vào n < 2p, chi phí tính toán của các lớp FFN trở thành bottleneck chính. Điều kiện này đặc biệt áp dụng cho các mô hình ngôn ngữ nền tảng hiện đại [8], thường có kích thước ẩn khổng lồ thậm chí vượt quá mười nghìn.

B. Maximum mean discrepancies (MMD)
Chúng tôi bắt đầu với một giới thiệu ngắn gọn về MMD. Biểu thức của MMD giữa hai phân phối P và Q được đưa ra như
MMD (P, Q) = sup∥f∥H≤1EX∼P[f(X)]−EY∼Q[f(Y)]
=∥EX∼P[φ(X)]−EY∼Q[φ(Y)]∥H, (11)

--- TRANG 16 ---
trong đó φ(·) :X → H là feature map tạo ra hàm kernel k(x, y) =⟨φ(x), φ(y)⟩H liên kết với một reproducing kernel Hilbert space (RKHS) H. Thông qua tính chất reproducing mạnh mẽ của map φ, chúng ta có thể viết lại MMD bình phương như
MMD2(P, Q) =∥EX∼Pφ(X)−EY∼Qφ(Y)∥2H
=⟨EX∼Pφ(X),EX′∼Pφ(X′)⟩H+⟨EY∼Qφ(Y),EY′∼Qφ(Y′)⟩H−2⟨EX∼Pφ(X),EY∼Qφ(Y)⟩H
=EX,X′∼Pk(X, X′) +EY,Y′∼Qk(Y, Y′)−2EX∼P,Y∼Qk(X, Y), (12)
dễ tối ưu hóa hơn bằng back-propagation.

Theo quan điểm phân phối thực nghiệm của MLP, chúng tôi ký hiệu MLP gốc như μw, một phân phối rời rạc đều trên các hàng của ma trận embedding W, và MLP nén tương tự như μ̂, một phân phối thực nghiệm được phân phối đều trên các hàng trong ma trận Ŵ= [Ŵ1,b̂1,Ŵ2]∈Rc×(2p+1)). Sau đó chúng ta có thể tối ưu hóa vấn đề sau
minŴMMD2(μw,μ̂Ŵ), (13)
từ đó chúng ta có thể thu được Ŵ. Như trong Phần IV-B, chúng ta có thể xây dựng MLP rút gọn với Ŵ như
H̃M=pI/c∑kh[σ(X(W(m)1)⊤+1(b(m))⊤)]W(m)2i+1b⊤2, (14)
cũng giới thiệu một hệ số pI/c vì các kỳ vọng thay vì tổng có liên quan trong MMD.

C. Bảo tồn NTK
Trong văn bản chính, chúng tôi đã đưa ra giả định rằng CW̃≈W và H̃C≈H. Giả định này ngụ ý rằng ∇Hf có thể được bảo tồn bởi ∇H̃Cfc, giúp thu được ⟨∇b2f(X),sign (∇b2f(Z))⟩ ≈ ⟨∇b̃2fc(X),sign(∇b̃2fc(Z))⟩.

Đối với ba thành phần còn lại, trước tiên chúng tôi giải quyết 1:=⟨∇W̃2fc(X),sign(∇W̃2fc(Z))⟩:
1= Tr[∇H̃Cfc(X)⊤σ̃xP·sign(Pσ̃⊤z∇H̃Cfc(Z))]
= Tr[∇H̃Cfc(X)⊤σ(XW̃1+1b̃⊤1)P·sign(P⊤σ(W̃⊤1Z⊤+b̃11⊤)∇H̃Cfc(Z))]
(i)= Tr[∇H̃Cfc(X)⊤σ(XW̃1+1b̃⊤1)CC⊤sign(σ(W̃⊤1Z⊤+b̃11⊤)∇H̃Cfc(Z))]
(ii)= Tr[∇H̃Cfc(X)⊤σ(XW̃1C+1b̃⊤1C)sign(σ(C⊤W̃⊤1Z⊤+C⊤b̃11⊤)∇H̃Cfc(Z))]
≈Tr[(∇Hf(X))⊤σ(XW 1+1b⊤1)sign(σ(W⊤1Z⊤+b11⊤)∇Hf(Z))]
=⟨∇W2f(X),sign (∇W2f(Z))⟩,
trong đó phương trình (i) ở trên giữ vì P=CC⊤ và ma trận đường chéo dương P sẽ không tác động đến dấu của các phần tử ma trận; đối với phương trình (ii), ma trận "copy" C, như đã thảo luận trong Phần IV-B, tự do được đưa vào bên trong cả hàm sign và hàm kích hoạt.

Đối với ⟨∇W1f(X),sign (∇W1f(Z))⟩, chúng ta cần xác minh tích
X⊤[∇Hf(X)W⊤2⊙σ′x]·sign[∇Hf(Z)W⊤2⊙σ′z]⊤Z

--- TRANG 17 ---
có thể được xấp xỉ bởi 2:=X⊤[∇H̃Cfc(X)W̃⊤2P⊙σ̃′x]·sign[∇H̃Cfc(Z)W̃⊤2P⊙σ̃′z]⊤Z, trong đó σ̃′x:=σ′(XW̃1+1b̃⊤1),σ̃z:=σ′(ZW̃1+1b̃⊤1), và σ′(·) là đạo hàm của hàm kích hoạt σ(·)2. Chúng tôi trình bày dẫn xuất như sau:
2(i)=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ̃′x]P·sign{P[∇H̃Cfc(Z)W̃⊤2⊙σ̃′z]⊤}Z
=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ̃′x]CC⊤·sign{[∇H̃Cfc(Z)W̃⊤2⊙σ̃′z]⊤}Z
=X⊤[∇H̃Cfc(X)W̃⊤2C⊙(σ̃′xC)]·sign{[∇H̃Cfc(Z)W̃⊤2C⊙(σ̃′zC)]⊤}Z
≈X⊤[∇Hf(X)W⊤2⊙σ′x]·[∇Hf(Z)W⊤2⊙σ′z]⊤Z,
trong đó chúng tôi thu được phương trình (i) vì P như một ma trận đường chéo có cùng hiệu ứng scaling trên tích Hadamard [∇H̃Cfc(X)W̃⊤2⊙σ̃′x] như trên một trong các thành phần của nó ∇H̃Cfc(X)W̃⊤2; các phương trình còn lại đơn giản theo các dẫn xuất trước đó.

Đối với thành phần cuối cùng ⟨∇b1f(X),sign (∇b1f(Z))⟩, chúng ta chỉ cần thay thế ma trận đầu vào X,Z ở trên bằng 1⊤, và tất cả các bước dẫn xuất sẽ theo.

D. Bảo tồn NTK cho các module MoE
Xem xét không có bias term trong các module MoE phổ biến, chúng tôi chỉ nghiên cứu bảo tồn NTK cho hai ma trận trọng số W1,W2 trong phần con này. Trước khi dẫn xuất, trước tiên chúng tôi nhắc lại các giả định trong Phần IV-C rằng W̃1C(m)≈W1, (C(m))⊤W̃2≈W2, và H̃m≈Hm; chúng tôi cũng chú ý kỹ đến ma trận standalone P(m)i cho các module MoE:
P(m)i=C(m)Ri(C(m))⊤
=[C1[G(xi)]1IpIC⊤1...CN[G(xi)]NIpIC⊤N]
=[[G(xi)]1·C1C⊤1...[G(xi)]N·CNC⊤N]∈RN·c×N·c.

Chúng tôi nhận xét rằng P(m)i tương tự cũng là một ma trận đường chéo dương; trực quan, nó biểu diễn kích thước cluster trong mỗi expert, được trọng số thêm bởi điểm số gating trong router MoE. Hơn nữa, do tính chất đặc biệt của ma trận đường chéo, chúng ta tiếp tục có
P(m)i=C(m)(C(m))⊤·R̃i,trong đó R̃i:= diag( G(xi))⊗Ic.

Ký hiệu mô hình MoE nén là f̃m, chúng tôi bắt đầu với việc giải quyết 1:=⟨∇W̃2f̃m(X),sign(∇W̃2f̃m(Z))⟩ như:
1= Tr[∑i(∇H̃mf̃m(X))iσ̃⊤x,iP(m)i!·sign(∑iP(m)iσ̃z,i(∇H̃mf̃m(Z))⊤i)!]
= Tr[∑i(∇H̃mf̃m(X))iσ(x⊤iW̃1)P(m)i!·sign(∑iP(m)iσ(W̃⊤1zi)(∇H̃mf̃m(Z))⊤i)!]
(i)= Tr[∑i(∇H̃mf̃m(X))iσ(x⊤iW̃1)C(m)Ri(C(m))⊤!·sign(C(m)(C(m))⊤∑iR̃iσ(W̃⊤1zi)(∇H̃mf̃m(Z))⊤i)!].

Phương trình cuối cùng (i) ở trên giữ vì P(m)i=C(m)(C(m))⊤·R̃i. Xem xét ma trận đường chéo dương C(m)(C(m))⊤ sẽ không tác động đến dấu của các phần tử ma trận, chúng ta có
1= Tr[∑i(∇H̃mf̃m(X))iσ(x⊤iW̃1)C(m)Ri(C(m))⊤!·sign(∑iR̃iσ(W̃⊤1zi)(∇H̃mf̃m(Z))⊤i)!]
= Tr[∑i(∇H̃mf̃m(X))iσ(x⊤iW̃1C(m))Ri!·sign(∑i(C(m))⊤R̃iσ(W̃⊤1zi)(∇H̃mf̃m(Z))⊤i)!]
(ii)= Tr[∑i(∇H̃mf̃m(X))iσ(x⊤iW̃1C(m))Ri!·sign(∑iRiσ((C(m))⊤W̃⊤1zi)(∇H̃mf̃m(Z))⊤i)!]
≈Tr[∑i(∇Hmfm(X))iσ(x⊤iW1)Ri!·sign(∑iRiσ(W⊤1zi)(∇Hmfm(Z))⊤i)!]
=⟨∇W2fm(X),sign(∇W2fm(Z))⟩,
trong đó phương trình (ii) giữ vì
(C(m))⊤R̃i=Ri(C(m))⊤,
và ma trận "copy" C(m), như đã thảo luận trong Phần IV-B, tự do được đưa vào bên trong cả hàm sign và hàm kích hoạt.

Đối với thành phần còn lại ⟨∇W̃1f̃m(X),sign(∇W̃1f̃m(Z))⟩, chúng ta cần chỉ ra
(∑ixi[RiW2(∇Hmfm(X))i⊙σ′x,i]⊤)!·sign(∑i[RiW2(∇Hmfm(Z))i⊙σ′z,i]z⊤i)!
có thể được xấp xỉ bởi
2:= (∑ixi[P(m)iW̃2(∇H̃mf̃m(X))i⊙σ̃′x,i]⊤)!·sign(∑i[P(m)iW̃2(∇H̃mf̃m(Z))i⊙σ̃′z,i]z⊤i)!,
trong đó σ̃′x,i:=σ′(W̃⊤1xi),σ̃z,i:=σ′(W̃⊤1zi), và σ′(·) là đạo hàm của hàm kích hoạt σ(·):
2(i)= (∑ixi[W̃2(∇H̃mf̃m(X))i⊙σ̃′x,i]⊤P(m)i)!·sign(∑iP(m)i[W̃2(∇H̃mf̃m(Z))i⊙σ̃′z,i]z⊤i)!
= (∑ixi[W̃2(∇H̃mf̃m(X))i⊙σ̃′x,i]⊤C(m)Ri(C(m))⊤)!·
sign(C(m)(C(m))⊤∑iR̃i[W̃2(∇H̃mf̃m(Z))i⊙σ̃′z,i]z⊤i)!
= (∑ixi[W̃2(∇H̃mf̃m(X))i⊙σ̃′x,i]⊤C(m)Ri(C(m))⊤)!·sign(∑iR̃i[W̃2(∇H̃mf̃m(Z))i⊙σ̃′z,i]z⊤i)!
= (∑ixi[(C(m))⊤W̃2(∇H̃mf̃m(X))i⊙(C(m))⊤σ̃′x,i]⊤Ri)!·
sign(∑i(C(m))⊤R̃i[W̃2(∇H̃mf̃m(Z))i⊙σ̃′z,i]z⊤i)!,
trong đó chúng tôi thu được phương trình (i) vì P(m) như một ma trận đường chéo có cùng hiệu ứng scaling trên tích Hadamard [W̃2(∇H̃mf̃m(X))i⊙σ̃′x,i] như trên một trong các thành phần của nó W̃2(∇H̃mf̃m(X))i. Tiếp theo, chúng tôi lại sử dụng mối quan hệ
(C(m))⊤R̃i=Ri(C(m))⊤

--- TRANG 18 ---
và có
2= (∑ixi[Ri(C(m))⊤W̃2(∇H̃mf̃m(X))i⊙(C(m))⊤σ′(W̃⊤1xi)]⊤)!·
sign(∑iRi[(C(m))⊤W̃2(∇H̃mf̃m(Z))i⊙(C(m))⊤σ′(W̃⊤1zi)]z⊤i)!
= (∑ixi[Ri(C(m))⊤W̃2(∇H̃mf̃m(X))i⊙σ′((C(m))⊤W̃⊤1xi)]⊤)!·
sign(∑i[Ri(C(m))⊤W̃2(∇H̃mf̃m(Z))i⊙σ′((C(m))⊤W̃⊤1zi)]z⊤i)!
≈ (∑ixi[RiW2(∇Hmfm(X))i⊙σ′x,i]⊤)!·sign(∑i[RiW2(∇Hmfm(Z))i⊙σ′z,i]z⊤i)!,
và phương trình cuối cùng lại giữ do tính chất đặc biệt trước đó của Ri và C(m).

E. Yêu cầu mô hình cho SGD NTK
Để bảo tồn SGD NTK thông thường, quy mô của các tham số trọng số cần được điều chỉnh. Chúng tôi định nghĩa lại mô hình MLP hiệu quả như (fc, σx, σz, σ̃x, σ̃z cũng sẽ được định nghĩa lại tương ứng):
H̃C:=σ(XW(c)1+1(b(c)1)⊤)W(c)2, (15)
trong đó W(c)1:=W̃1P1/2,b(c)1:=P1/2b̃1 và W(c)2:=P1/2W̃1 kết hợp ma trận scaling đường chéo P trong Phương trình (8).

Chúng tôi cũng yêu cầu hàm kích hoạt có tính chất sau:
σ(AP) =σ(A)P,
cho ma trận đường chéo không âm tùy ý P, điều này ngụ ý σ(0) = 0, σ(·) là piece-wise linear trên R+,R−, và σ′(x) là piece-wise constant (σ(1) trên R+ và σ(−1) trên R−); như một ví dụ, hàm Rectified Linear Units (ReLU) [82] thường sử dụng σr(x) = max {0, x} có thể thỏa mãn yêu cầu này.

Phương trình (15) có thể tiếp tục duy trì xấp xỉ cho H và vẫn ⟨∇b2f(X),∇b2f(Z)⟩ ≈ ⟨∇b(c)2fc(X),∇b(c)2fc(Z)⟩, vì chúng ta chỉ sửa đổi quy mô của các ma trận trọng số. Sau đó chúng tôi theo dẫn xuất trong phần con trước và kết quả tương tự được thu được

Đối với ba thành phần còn lại, lại chúng tôi trước tiên giải quyết 1:=⟨∇W(c)2fc(X),∇W(c)2fc(Z)⟩:
1= Tr[∇H̃Cfc(X)⊤σ̃x·σ̃⊤z∇H̃Cfc(Z)]
= Tr[∇H̃Cfc(X)⊤σ(XW(c)1+1(b(c)1)⊤)·σ(W(c)1⊤Z⊤+b(c)11⊤)∇H̃Cfc(Z)]
(i)= Tr[∇H̃Cfc(X)⊤σ(XW̃1+1b̃⊤1)P1/2P1/2σ(W̃⊤1Z⊤+b̃11⊤)∇H̃Cfc(Z)]
= Tr[∇H̃Cfc(X)⊤σ(XW̃1+1b̃⊤1)CC⊤σ(W̃⊤1Z⊤+b̃11⊤)∇H̃Cfc(Z)]
≈⟨∇W2f(X),∇W2f(Z)⟩,
trong đó phương trình (i) giữ vì P1/2, như chúng tôi yêu cầu, tự do được đưa ra ngoài hàm kích hoạt; phần dẫn xuất còn lại đơn giản theo đối tác trong Phụ lục B-C.

--- TRANG 19 ---
Đối với ⟨∇W1f(X),∇W1f(Z)⟩, chúng ta tương tự cần xác minh tích X⊤[∇Hf(X)W⊤2⊙σ′]·[∇Hf(Z)W⊤2⊙σ′]⊤X có thể được xấp xỉ bởi 2:=X⊤[∇H̃Cfc(X)(W(c)2)⊤⊙σ̃′x]·[∇H̃Cfc(Z)(W(c)2)⊤⊙σ̃′z]⊤X. σ̃′x:=σ′(XW̃1+1b̃⊤1),σ̃z:=σ′(ZW̃1+1b̃⊤1)

Sau đó chúng tôi trình bày dẫn xuất như sau:
2=X⊤[∇H̃Cfc(X)(W(c)2)⊤⊙σ̃′x]·[∇H̃Cfc(Z)(W(c)2)⊤⊙σ̃′z]⊤X
(i)=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ̃′x]P1/2P1/2·[∇H̃Cfc(Z)W̃⊤2⊙σ̃′z]⊤X
=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ̃′x]CC⊤·[∇H̃Cfc(Z)W̃⊤2⊙σ̃′z]⊤X
=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ′(XW(c)1+1(b(c)1)⊤)]CC⊤
·[∇H̃Cfc(X)W̃⊤2⊙σ′(XW(c)1+1(b(c)1)⊤)]⊤X
(ii)=X⊤[∇H̃Cfc(X)W̃⊤2⊙σ′(XW̃1+1b̃⊤1)]CC⊤
·[∇H̃Cfc(X)W̃⊤2⊙σ′(XW̃1+1b̃⊤1)]⊤X,
trong đó chúng tôi thu được phương trình (i) vì P1/2 như một ma trận đường chéo có cùng hiệu ứng scaling trên tích Hadamard ∇H̃Cfc(X)(W(c)2)⊤⊙σ̃′x như trên một trong các thành phần của nó ∇H̃Cfc(X)(W(c)2)⊤; phương trình (ii) giữ vì σ′ là piece-wise constant và ma trận scaling P1/2 sẽ không thay đổi dấu của các phần tử bên trong. Sau đó, theo cùng dẫn xuất như trong các dẫn xuất trước, chúng ta có 2≈X⊤[∇Hf(X)W⊤2⊙σ′]·[∇Hf(Z)W⊤2⊙σ′]⊤X

Đối với thành phần cuối cùng ⟨∇b1f(X),∇b1f(Z)⟩, chúng ta có thể lại thay thế ma trận đầu vào X ở trên bằng 1⊤, và tất cả các bước dẫn xuất sẽ theo.

PHỤ LỤC C
GIỚI HẠN LỖI CỦA ĐẦU RA MLP

Nhớ lại mục tiêu của clustering là:
minC∥W−C⊤W̃∥2F

Giả định do đó có thể được viết lại theo cách toán học, là ∥W−C⊤W̃∥F≤ε với ε nhỏ. Giả định f(W,C⊤W̃) =∥W−C⊤W̃∥F≤ε, chúng ta có thể cung cấp một phân tích tiêu chuẩn của đầu ra MLP (bỏ qua b1 để đơn giản) như sau.

Ký hiệu Δ:=σ(XW̃1C)−σ(XW 1) và theo các giả định kỹ thuật trong [4] rằng ∥W1∥2≤C1,∥W2∥2≤C2,∥X∥F≤CX và hàm kích hoạt σ(·) là L-Lipschitz continuous. Tiếp tục giả định σ(0) = 0 (các giả định giữ cho các hàm kích hoạt thường sử dụng trong PLM, ví dụ: ReLU và GELU), trước tiên chúng ta có
∥Δ∥F≤L∥XW̃1C−XW 1∥F≤L∥X∥F∥W̃1C−W1∥F≤LCXε.

--- TRANG 20 ---
Sau đó chúng ta có thể giới hạn ∥H−H̃C∥F như
∥σ(XW̃1C)C⊤W̃2−σ(XW 1)W2∥F≤∥σ(XW̃1C)(C⊤W̃2−W2)∥F+∥(σ(XW̃1C)−σ(XW 1))W2∥F
≤∥σ(XW̃1C)∥F∥C⊤W̃2−W2∥F+∥σ(XW̃1C)−σ(XW 1)∥F∥W2∥2
=∥Δ +σ(XW 1)∥F∥C⊤W̃2−W2∥F+∥Δ∥F∥W2∥2
≤(∥Δ∥F+∥σ(XW 1)∥F)·ε+∥Δ∥F·C2
≤L(C2CX+C1CX)·ε+LCX·ε2,
trong đó chúng tôi sử dụng
∥W̃1C−W1∥F,∥C⊤W̃2−W2∥ ≤ ∥W−C⊤W̃∥F≤ε
∥σ(XW 1)∥F=∥σ(XW 1)−σ(0)∥F≤L∥XW 1∥F≤L∥X∥F∥W1∥2=LC1CX,
cho dẫn xuất. Từ giới hạn, chúng ta có thể quan sát với C⊤W̃ được học tốt từ clustering (ε nhỏ), lỗi đầu ra (∥H−H̃C∥F) cũng sẽ nhỏ.

Hơn nữa, một phân tích lỗi tương tự cũng có thể được áp dụng cho Adam NTK. Phân tích lỗi ∥K −K̃C∥F của NTK kernel K có thể được đơn giản hóa như phân tích Tr[A⊤sign(B)−Ã⊤sign(B̃)] trong đó A,B biểu diễn các ma trận tùy ý. Dẫn xuất chính xác của giới hạn lỗi xấp xỉ trên Adam NTK, xem xét giả định bổ sung về ∥sign(B)−sign(B̃)∥F như được mô tả trong [83], được để lại như công việc tương lai.

PHỤ LỤC D
THẢO LUẬN

Chúng tôi không biết về bất kỳ tác động tiêu cực nào đến xã hội liên quan đến công trình của chúng tôi theo hiểu biết tốt nhất của chúng tôi. Đối với tất cả các tập dữ liệu được sử dụng, không có thông tin nhận dạng cá nhân riêng tư hoặc nội dung phản cảm.

Về công việc tương lai, ngoài việc kết hợp với chưng cất, chúng tôi cũng dự định khám phá các phương pháp nén thực tế trong nhiều lĩnh vực khác nhau, bao gồm xử lý giọng nói [84], hệ thống đề xuất [85], [86], và khai thác đồ thị [87], [88]. Dẫn xuất một phân tích lỗi chính xác hơn liên quan đến mô hình được huấn luyện trước cũng là một hướng thách thức và đầy hứa hẹn.

PHỤ LỤC E
SO SÁNH HIỆU SUẤT CỦA CÁC PHƯƠNG PHÁP ĐẠI DIỆN SAU TASK-SPECIFIC FINE-TUNING

BẢNG XII
ĐỘ CHÍNH XÁC CỦA CÁC PHƯƠNG PHÁP ĐẠI DIỆN SAU TASK-SPECIFIC FINE-TUNING TRÊN TẬP VALIDATION SST2

Phương pháp+Task-specific Fine-tuning Độ chính xác
Sketch 91.86
Clustering 93.35
MMD 92.43
SVD 93.01
LTH 93.42
MLP Fusion(Ours) 93.79

Từ Bảng XII và Bảng II trong bài báo, chúng ta có thể quan sát rằng không phải tất cả phương pháp đều có thể được hưởng lợi từ module tuning cụ thể tác vụ layer-wise. Ví dụ, độ chính xác của phương pháp Sketch giảm từ 91.90 xuống 91.86. Trong khi đó, MLP Fusion được đề xuất của chúng tôi cung cấp một điểm khởi đầu đầy hứa hẹn cho tối ưu hóa tiếp theo thông qua xấp xỉ NTK. Bằng cách kết hợp module tuning cụ thể tác vụ layer-wise, chúng ta có thể tiếp tục tăng cường hiệu suất của nó và vẫn đạt được kết quả tốt nhất so với tất cả các phương pháp baseline khác.

--- TRANG 21 ---
PHỤ LỤC F
KẾT QUẢ THÍ NGHIỆM TRÊN NHIỀU DATASET BENCHMARK HƠN

BẢNG XIII
ĐỘ CHÍNH XÁC CỦA MỖI PHƯƠNG PHÁP BASELINE TRÊN CÁC TẬP VALIDATION STS-B VÀ QNLI VỚI ROBERT A LÀM PLM

Phương pháp STS-B(7k) QNLI(105k)
RoBERTa 91.20 92.80
DistilRoBERTa 88.30 90.80
Sketch 86.99 89.84
Clustering 88.12 90.63
LTH 87.37 90.87
MLP Fusion(Ours) 89.37 91.03

Bảng XIII hiển thị kết quả thí nghiệm trên hai tập dữ liệu bổ sung QNLI và STS-B trong benchmark GLUE. Chúng ta có thể thấy phương pháp được đề xuất vẫn có thể đạt được hiệu suất tốt nhất so với các baseline mạnh.

PHỤ LỤC G
SO SÁNH HIỆU SUẤT GIỮA PHƯƠNG PHÁP ĐỀ XUẤT VÀ DUY TRÌ OUTPUT/NTK CỦA MÔ HÌNH MLP

BẢNG XIV
HIỆU SUẤT CỦA PHƯƠNG PHÁP ĐỀ XUẤT VÀ DUY TRÌ OUTPUT/NTK CỦA MÔ HÌNH MLP

Phương pháp Độ chính xác
Maintain NTK Gradient 91.74
Maintain Output 92.35
MLP Fusion (Ours) 93.23

Trong Bảng XIV, chúng tôi so sánh hai baseline bổ sung cố gắng duy trì đầu ra MLP và NTK của mô hình ngôn ngữ được huấn luyện trước. Phương pháp xấp xỉ NTK MLP Fusion của chúng tôi vẫn đạt được hiệu suất tốt nhất. Loss cố gắng duy trì đầu ra với dữ liệu không giám sát xếp thứ hai. Phương pháp cố gắng duy trì NTK của mô hình MLP với gradient có độ chính xác thấp nhất. Điều này chủ yếu vì sự khác biệt gradient trong loss khó tối thiểu hóa vì nó đòi hỏi vận hành đạo hàm bậc hai, cũng có thể tốn thời gian. Ngoài ra, thu thập dữ liệu có nhãn cho tính toán loss cũng có thể là gánh nặng.
