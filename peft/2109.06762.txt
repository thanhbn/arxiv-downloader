# 2109.06762.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2109.06762.pdf
# File size: 538654 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Greenformer: Factorization Toolkit for Efﬁcient Deep Neural Networks
Samuel Cahyawijaya*, Genta Indra Winata, Holy Lovenia, Bryan Wilie,
Wenliang Dai, Etsuko Ishii, Elham J. Barezi, Pascale Fung
Center for Artiﬁcial Intelligence Research (CAiRE)
The Hong Kong University of Science and Technology
scahyawijaya@connect.ust.hk
Abstract
While the recent advances in deep neural networks (DNN)
bring remarkable success, the computational cost also in-
creases considerably. In this paper, we introduce Green-
former, a toolkit to accelerate the computation of neural net-
works through matrix factorization while maintaining per-
formance. Greenformer can be easily applied with a sin-
gle line of code to any DNN model. Our experimental re-
sults show that Greenformer is effective for a wide range
of scenarios. We provide the showcase of Greenformer at
https://samuelcahyawijaya.github.io/greenformer-demo/.
Introduction
With the signiﬁcant computational growth of DNN mod-
els (Hernandez and Brown 2020), AI researchers all around
the globe have started to promote and adopt the concept
of ‘Green AI’ (Schwartz et al. 2020). Many recent works
(Strubell, Ganesh, and McCallum 2019; Lacoste et al. 2019;
Patterson et al. 2021; Dai et al. 2021; Menghani 2021) ad-
dress the environmental challenges such as energy usage and
carbon emission level of DNN models and develop more ef-
ﬁcient deep learning solutions. In response to this problem,
we introduce a robust and easy-to-use low-rank matrix fac-
torization toolkit which reduces not only the computational
cost but also the model size, with minimal performance loss.
Low-rank matrix factorization is done by decomposing
a large matrix into two or more smaller matrices, reduc-
ing computation and memory costs. Post-training factor-
ization methods with singular-value decomposition (SVD)
(Golub and Reinsch 1970) and non-negative matrix factor-
ization (NMF) (Lee and Seung 2001) have been applied to
approximate the weight matrix of a trained model (Winata
et al. 2019; Ben Noach and Goldberg 2020). On the other
line of work, factorization-by-design applies matrix factor-
ization is directly to the model structure prior to the train-
ing. This method produces impressive results with the com-
pressed model is not only smaller and faster but also able to
outperform the uncompressed model (Winata et al. 2020;
Cahyawijaya 2021; Kuchaiev and Ginsburg 2017).
Despite the fact that many works have been published on
low-rank matrix factorization, all the solutions are model-
*The authors contributed equally to this work.
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: Model factorization with Greenformer for an efﬁ-
cient compute time. Greenformer provides efﬁciency boost
with a minimum tweak on the code base.
dependent, making applicability to different model architec-
ture difﬁcult and cumbersome. To improve the generaliza-
tion and applicability of the low-rank matrix factorization
method, we introduce Greenformer, an eloquent low-rank
matrix factorization toolkit that supports multiple use cases
of matrix factorization and is currently implemented for the
PyTorch framework (Paszke et al. 2019). As shown in Fig-
ure 1, with Greenformer, we can easily factorize any deep
neural networks to perform both factorization-by-design and
post-training factorization. We further demonstrate the ef-
fectiveness of our Greenformer toolkit for three different use
cases: 1) factorization-by-design, 2) post-training factoriza-
tion, and 3) few-shot via in-context learning factorization.
Design and Consideration
Greenformer performs decomposition to the weight matrices
of linear and convolution layers. Namely, a weight matrix
W2Rmnis decomposed into two low-rank matrices A2
RmrandB2Rrn, where rminfm; ng.
Greenformer decomposes a matrix by utilizing a factor-arXiv:2109.06762v3  [cs.LG]  9 Oct 2021

--- PAGE 2 ---
64 128 192 256 320
Factorization rank0.850.900.951.00Performance ratio
Performance
1.01.21.41.6
CPU speed up
64 128 192 256 320
Factorization rank0.400.600.801.00
Performance
1.11.21.31.4
CPU speed up
128 256 384 512
Factorization rank0.600.750.901.05
Performance1.11.21.31.4
CPU speed up ratio
CPU speed upFigure 2: Performance and efﬁciency trade-off of utilizing Greenformer on (left) factorization-by-design, (center) post-training
factorization, and (right) in-context learning factorization use cases. Purple andgreen lines denote the relative performance
and speed up ratio against the uncompressed model averaged across all tasks.
Figure 3: Automatic factorization ﬂow with LED. (a)Linear
layer is factorized creating an LED layer. (b)The LED layer
is used to replace the linear layer in the model producing (c)
which requires more efﬁcient than the original linear layer.
ization solver. There are three different factorization solvers
implemented in Greenformer: Random, SVD (Golub and
Reinsch 1970), and Semi-Nonnegative Matrix Factorization
(SNMF) (Lee and Seung 2001). Random solver replaces the
original matrix with two random matrices by referring the
original size and the speciﬁed target rank. Note that random
solver is not suitable for post-training factorization, since it
may break what the model learnt in the main training as it
does not approximate the original matrix. SVD solver com-
putes W=AV=AB where is a diagonal and has
singular values. SNMF is an extension of NMF which alle-
viates the non-negative constraint on W. SNMF solver per-
forms decomposition of W=AB, where Bis strictly non-
negative yet Ahas no restriction on signs.
As the three solvers mentioned above cannot handle ten-
sors, Greenformer rearranges weight tensors to matrices for
decomposition of convolutional layers. For instance, a 1D
convolution layer consists of a weight W2RCinCoutS,
where CinandCoutdenote the number of input channel
and output channel, and Sdenotes the size of the convo-
lution kernel. Greenformer rearranges the weight into a 2-
dimensional matrix W02RCinSCout. The matrix is then
decomposed and converted back into the original dimension
producing tensors A2RCinrSandB2RrCout1. The
same trick is also applied for 2D and 3D convolution layers.The decomposed matrices and/or tensors are then
wrapped into a compatible low-rank module which is then
used to replace the original linear an/or convolution layers
of the model. Speciﬁcally, we replace a linear layer into a
Linear Encoder-Decoder (LED) layer and replace a convolu-
tion layer into a Convolution Encoder-Decoder (CED) layer.
The depiction of LED and/or CED layers work is shown in
Figure 3. Both LED, and CED have the same input and out-
put with the linear and convolution layers; hence, they can
maintain compatibility with the model.
To maximize the outcome of automatic factorization,
Greenformer only performs factorization when the low-rank
ris less than the maximum low-rank rmaxto ensure reduc-
tion of the theoretical computational cost. For a given weight
matrix W2Rmnthe maximum low-rank is deﬁned as:
rmax=(mn)
(m+n)(1)
To improve its ﬂexibility, Greenformer supports factor-
ization with a dynamic rank across all layers by computing
the rank based on a ratio to the maximum rank rmaxof the
corresponding layer. Additionally, we also observe that ap-
plying factorization to all layers of large pretrained models
leads to signiﬁcant performance loss. To address this prob-
lem, Greenformer is equipped with a ﬁltering feature that
enables factorization only on a speciﬁc set of modules.
We test our toolkit on three use cases: 1) Factorization-
by-design, where we train models prior to the training; 2)
post-training factorization, where we factorize models prior
to evaluation phase; and in-context learning factorization,
where we apply factorization to large pretrained language
models and perform in-context learning following Brown
et al. (2020). We test our toolkit on 3 text classiﬁcation tasks
and 2 image classiﬁcation tasks. We show the effectiveness
of our Greenformer toolkit in all use cases in Figure 2.
Conclusion
We present Greenformer, an automatic factorization toolkit
that provides signiﬁcant efﬁciency improvement while
maintaining the model performance. In addition, Green-
former is ﬂexible, easy-to-use, and applicable for multiple
scenarios. For future work, it is interesting to extend Green-
former for more energy-intensive use cases, such as on large
models pretraining and network architecture search.

--- PAGE 3 ---
References
Ben Noach, M.; and Goldberg, Y . 2020. Compressing Pre-
trained Language Models by Matrix Decomposition. In Pro-
ceedings of the 1st Conference of the Asia-Paciﬁc Chapter of
the Association for Computational Linguistics and the 10th
International Joint Conference on Natural Language Pro-
cessing , 884–889. Suzhou, China: Association for Compu-
tational Linguistics.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Cahyawijaya, S. 2021. Greenformers: Improving Compu-
tation and Memory Efﬁciency in Transformer Models via
Low-Rank Approximation. arXiv:2108.10808.
Dai, W.; Cahyawijaya, S.; Liu, Z.; and Fung, P. 2021. Multi-
modal End-to-End Sparse Model for Emotion Recognition.
InNAACL .
Golub, G. H.; and Reinsch, C. 1970. Singular Value Decom-
position and Least Squares Solutions. Numer. Math. , 14(5):
403–420.
Hernandez, D.; and Brown, T. B. 2020. Measuring the Algo-
rithmic Efﬁciency of Neural Networks. arXiv:2005.04305.
Kuchaiev, O.; and Ginsburg, B. 2017. Factorization tricks
for LSTM networks. ICLR Workshop .
Lacoste, A.; Luccioni, A.; Schmidt, V .; and Dandres, T.
2019. Quantifying the Carbon Emissions of Machine Learn-
ing. Workshop on Tackling Climate Change with Machine
Learning at NeurIPS 2019 .
Lee, D.; and Seung, H. S. 2001. Algorithms for Non-
negative Matrix Factorization. In Leen, T.; Dietterich, T.;
and Tresp, V ., eds., Advances in Neural Information Pro-
cessing Systems , volume 13. MIT Press.
Menghani, G. 2021. Efﬁcient Deep Learning: A Survey on
Making Deep Learning Models Smaller, Faster, and Better.
arXiv:2106.08962.
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;
Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,
L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,
M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,
J.; and Chintala, S. 2019. PyTorch: An Imperative Style,
High-Performance Deep Learning Library. In Wallach, H.;
Larochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox, E.;
and Garnett, R., eds., Advances in Neural Information Pro-
cessing Systems 32 , 8024–8035. Curran Associates, Inc.
Patterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia,
L.-M.; Rothchild, D.; So, D.; Texier, M.; and Dean, J.
2021. Carbon Emissions and Large Neural Network Train-
ing. arXiv:2104.10350.
Schwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2020.
Green AI. Commun. ACM , 63(12): 54–63.Strubell, E.; Ganesh, A.; and McCallum, A. 2019. Energy
and Policy Considerations for Deep Learning in NLP. In
Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , 3645–3650. Florence, Italy:
Association for Computational Linguistics.
Winata, G. I.; Cahyawijaya, S.; Lin, Z.; Liu, Z.; and Fung, P.
2020. Lightweight and Efﬁcient End-To-End Speech Recog-
nition Using Low-Rank Transformer. In 2020 IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020 ,
6144–6148. IEEE.
Winata, G. I.; Madotto, A.; Shin, J.; Barezi, E. J.; and Fung,
P. 2019. On the Effectiveness of Low-Rank Matrix Factor-
ization for LSTM Model Compression. In Proceedings of
the 33rd Paciﬁc Asia Conference on Language, Information
and Computation , 253–262. Waseda Institute for the Study
of Language and Information.
