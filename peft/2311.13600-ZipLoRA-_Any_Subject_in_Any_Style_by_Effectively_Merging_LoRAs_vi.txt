# ZipLoRA: Bất kỳ Chủ đề nào trong Bất kỳ Phong cách nào bằng cách Hợp nhất LoRA Hiệu quả

Viraj Shah∗,1,2Nataniel Ruiz1Forrester Cole1Erika Lu1
Svetlana Lazebnik2Yuanzhen Li1Varun Jampani†,1
1Google Research2UIUC

Hình ảnh Nội dung Hình ảnh Phong cách 
Lc Ls
"Một con [V] chó" "Hoa trong phong cách [S]" 
"Con [V] chó trong phong cách [S]" 
"Con [V] chó chơi với quả bóng trong phong cách [S]" "Con [V] chó đang ngủ trong phong cách [S]" 
"Con [V] chó đội vương miện trong phong cách [S]" 

Chủ đề: 
[Vi]Phong cách: [Si]
"Chủ đề [Vi] trong phong cách [Si]" Phong cách: 
[S1], [S2], [S3]
Đối tượng: 
[V1], [V2], [V3]
ZipLoRA LoRA Phong cách 
LoRA Nội dung 
Ls Lc 

Hình 1. Bằng cách hợp nhất hiệu quả các LoRA phong cách và nội dung được huấn luyện độc lập, phương pháp ZipLoRA được đề xuất của chúng tôi có thể tạo ra bất kỳ chủ đề nào do người dùng cung cấp trong bất kỳ phong cách nào do người dùng cung cấp, mang lại khả năng kiểm soát chưa từng có đối với các sáng tạo cá nhân hóa sử dụng mô hình khuếch tán.

Tóm tắt
Các phương pháp tinh chỉnh mô hình sinh để cá nhân hóa theo khái niệm thường đạt được kết quả mạnh mẽ cho việc tạo ra hình ảnh theo chủ đề hoặc theo phong cách. Gần đây, các phép thích ứng thứ hạng thấp (LoRA) đã được đề xuất như một cách tiết kiệm tham số để đạt được cá nhân hóa theo khái niệm. Trong khi các nghiên cứu gần đây khám phá sự kết hợp của các LoRA riêng biệt để đạt được việc tạo ra phong cách và chủ đề học được cùng lúc, các kỹ thuật hiện tại không giải quyết vấn đề một cách đáng tin cậy; chúng thường làm giảm độ trung thực của chủ đề hoặc độ trung thực của phong cách. Chúng tôi đề xuất ZipLoRA, một phương pháp hợp nhất các LoRA phong cách và chủ đề được huấn luyện độc lập một cách rẻ và hiệu quả để đạt được việc tạo ra bất kỳ chủ đề nào do người dùng cung cấp trong bất kỳ phong cách nào do người dùng cung cấp. Các thí nghiệm trên một loạt rộng các kết hợp chủ đề và phong cách cho thấy ZipLoRA có thể tạo ra kết quả hấp dẫn với những cải thiện có ý nghĩa so với các phương pháp cơ sở trong độ trung thực của chủ đề và phong cách trong khi vẫn duy trì khả năng tái bối cảnh hóa.

1. Giới thiệu
Gần đây, các mô hình khuếch tán [13, 28, 34] đã cho phép chất lượng tạo hình ảnh ấn tượng với sự hiểu biết xuất sắc về các khái niệm nghệ thuật đa dạng và khả năng kiểm soát nâng cao do hỗ trợ điều kiện đa phương thức (với văn bản là phương thức phổ biến nhất). Khả năng sử dụng và tính linh hoạt của các mô hình sinh đã tiến triển hơn nữa với nhiều phương pháp cá nhân hóa khác nhau, chẳng hạn như DreamBooth [29] và StyleDrop [33]. Những phương pháp này tinh chỉnh mô hình khuếch tán cơ sở trên các hình ảnh của một khái niệm cụ thể để tạo ra những phiên bản mới trong các bối cảnh khác nhau. Những khái niệm như vậy có thể là một đối tượng hoặc người cụ thể, hoặc một phong cách nghệ thuật.

Trong khi các phương pháp cá nhân hóa đã được sử dụng cho các chủ đề và phong cách một cách độc lập, một vấn đề quan trọng chưa được giải quyết là tạo ra một chủ đề cụ thể do người dùng cung cấp trong một phong cách cụ thể do người dùng cung cấp. Ví dụ, một nghệ sĩ có thể muốn vẽ một người cụ thể theo phong cách cá nhân của họ, học được thông qua các ví dụ về tác phẩm của riêng họ. Một người dùng có thể muốn tạo ra hình ảnh của đồ chơi nhồi bông yêu thích của con họ, theo phong cách của các bức tranh màu nước của đứa trẻ. Hơn nữa, nếu điều này được thực hiện thì hai vấn đề được giải quyết đồng thời: (1) nhiệm vụ biểu diễn bất kỳ chủ đề nào trong bất kỳ phong cách nào, và (2) vấn đề kiểm soát các mô hình khuếch tán thông qua hình ảnh thay vì văn bản, có thể không chính xác và không phù hợp cho một số nhiệm vụ tạo sinh nhất định. Cuối cùng, chúng ta có thể tưởng tượng một ứng dụng quy mô lớn của công cụ như vậy, trong đó một ngân hàng các phong cách và chủ đề học được độc lập được chia sẻ và lưu trữ trực tuyến. Nhiệm vụ hiển thị tùy ý bất kỳ chủ đề nào trong bất kỳ phong cách nào là một vấn đề nghiên cứu mở mà chúng tôi tìm cách giải quyết.

Một nhược điểm của các phương pháp cá nhân hóa gần đây là nhiều phương pháp tinh chỉnh tất cả các tham số của mô hình cơ sở lớn, điều này có thể tốn kém. Các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) cho phép tinh chỉnh mô hình cho cá nhân hóa theo khái niệm với ngân sách bộ nhớ và lưu trữ thấp hơn nhiều. Trong số các phương pháp PEFT khác nhau, Thích ứng Thứ hạng Thấp (LoRA) [14] đã nổi lên như một phương pháp được ưa chuộng bởi các nhà nghiên cứu và thực hành do tính linh hoạt của nó. LoRA học các ma trận trọng số phân tích thứ hạng thấp cho các lớp attention (những trọng số học được này thường được gọi là "LoRA"). Bằng cách kết hợp LoRA và các thuật toán như DreamBooth [29], các trọng số LoRA cụ thể cho chủ đề học được cho phép mô hình tạo ra chủ đề với các biến thể ngữ nghĩa.

Với sự phổ biến ngày càng tăng của cá nhân hóa LoRA, đã có những nỗ lực hợp nhất các trọng số LoRA, cụ thể bằng cách thực hiện kết hợp tuyến tính của các LoRA chủ đề và phong cách, với các hệ số biến thiên [30]. Điều này cho phép kiểm soát "cường độ" của mỗi LoRA, và người dùng đôi khi có thể, thông qua tìm kiếm lưới cẩn thận và đánh giá chủ quan của con người, tìm ra một sự kết hợp cho phép miêu tả chính xác chủ đề dưới phong cách cụ thể. Phương pháp này thiếu tính mạnh mẽ trên các kết hợp phong cách và chủ đề, và cũng cực kỳ tốn thời gian.

Trong công trình của chúng tôi, chúng tôi đề xuất ZipLoRA, một phương pháp đơn giản nhưng hiệu quả để tạo ra bất kỳ chủ đề nào trong bất kỳ phong cách nào bằng cách hợp nhất một cách rẻ các LoRA được huấn luyện độc lập cho chủ đề và phong cách. Phương pháp của chúng tôi hoạt động nhất quán trên nhiều LoRA chủ đề và phong cách khác nhau mà không áp đặt bất kỳ hạn chế nào về cách thức huấn luyện chúng. Điều này cho phép người dùng và nghệ sĩ dễ dàng kết hợp các LoRA chủ đề và phong cách có sẵn công khai theo lựa chọn của họ. ZipLoRA không có siêu tham số, tức là nó không yêu cầu điều chỉnh thủ công bất kỳ siêu tham số hoặc trọng số hợp nhất nào.

Phương pháp của chúng tôi tận dụng mô hình Stable Diffusion XL (SDXL) được phát hành gần đây [27] và dựa trên ba quan sát quan trọng. (1) SDXL thể hiện đặc tính học phong cách mạnh mẽ, có thể so sánh với kết quả được StyleDrop [33] trên Muse [3] hiển thị. Cụ thể, không giống như các phiên bản trước của Stable Diffusion, SDXL có thể học các phong cách chỉ bằng một hình ảnh mẫu duy nhất bằng cách tuân theo giao thức DreamBooth [29] mà không cần phản hồi từ con người. (2) Các trọng số LoRA cho các lớp khác nhau ∆Wi (trong đó i biểu thị lớp) là thưa thớt. tức là, hầu hết các phần tử trong ∆Wi có độ lớn rất nhỏ và có ít ảnh hưởng đến chất lượng tạo sinh và độ trung thực. (3) Các cột của ma trận trọng số của hai LoRA được huấn luyện độc lập có thể có các mức độ "căn chỉnh" khác nhau với nhau, được đo bằng độ tương tự cosine, chẳng hạn. Chúng tôi nhận thấy rằng việc cộng trực tiếp các cột có độ căn chỉnh cao làm giảm hiệu suất của mô hình hợp nhất.

Dựa trên những quan sát này, chúng tôi giả thuyết rằng một phương pháp hoạt động giống như một chiếc khóa kéo, nhằm giảm số lượng tổng cùng hướng trong khi duy trì các thuộc tính tạo sinh nội dung và phong cách của các LoRA gốc sẽ mang lại kết quả hợp nhất mạnh mẽ hơn, chất lượng cao hơn. Giống như một chiếc khóa kéo liền mạch kết hợp hai bên của một mảnh vải, phương pháp dựa trên tối ưu hóa được đề xuất của chúng tôi tìm ra một tập hợp các hệ số hợp nhất rời rạc để pha trộn hai LoRA. Điều này đảm bảo rằng LoRA hợp nhất khéo léo nắm bắt cả chủ đề và phong cách. Quá trình tối ưu hóa của chúng tôi nhẹ và cải thiện đáng kể hiệu suất hợp nhất trên các kết hợp nội dung-phong cách thách thức, nơi hai LoRA có độ căn chỉnh cao.

Chúng tôi tóm tắt các đóng góp của mình như sau:
• Chúng tôi trình bày một số quan sát chính về các mô hình khuếch tán văn bản-thành-hình ảnh hiện tại và các phương pháp cá nhân hóa, đặc biệt liên quan đến cá nhân hóa phong cách. Chúng tôi tiếp tục kiểm tra tính thưa thớt của các hệ số ma trận trọng số LoRA được cá nhân hóa khái niệm và sự phổ biến cũng như tác động có hại của các cột có độ căn chỉnh cao đối với ma trận LoRA.

• Sử dụng những hiểu biết này, chúng tôi đề xuất ZipLoRA, một phương pháp tối ưu hóa đơn giản cho phép hợp nhất hiệu quả các LoRA phong cách và chủ đề được huấn luyện độc lập để cho phép tạo ra bất kỳ chủ đề nào trong bất kỳ phong cách nào. ZipLoRA là một khám phá đầu tiên vào thế giới các kỹ thuật hợp nhất LoRA để đạt được khả năng tạo sinh mới.

• Chúng tôi chứng minh hiệu quả của phương pháp trên nhiều nhiệm vụ tạo phong cách hình ảnh khác nhau, bao gồm chuyển đổi nội dung-phong cách và tái bối cảnh hóa. Chúng tôi cũng chứng minh rằng ZipLoRA vượt trội hơn các phương pháp hiện tại để hợp nhất LoRA cũng như các phương pháp cơ sở khác.

2. Công trình liên quan
Tinh chỉnh Mô hình Khuếch tán cho Tạo sinh Tùy chỉnh.
Trong lĩnh vực đang phát triển của cá nhân hóa mô hình văn bản-thành-hình ảnh (T2I), các nghiên cứu gần đây đã giới thiệu các phương pháp khác nhau để tinh chỉnh các mô hình khuếch tán T2I quy mô lớn để mô tả các chủ đề cụ thể dựa trên mô tả văn bản. Các kỹ thuật như Textual Inversion [8] tập trung vào việc học embedding văn bản, trong khi DreamBooth [29] tinh chỉnh toàn bộ mô hình T2I để biểu diễn chủ đề tốt hơn. Các phương pháp sau này nhằm tối ưu hóa các phần cụ thể của mạng [11, 20]. Ngoài ra, các kỹ thuật như LoRA [14] và StyleDrop [33] tập trung vào việc tối ưu hóa các phép xấp xỉ thứ hạng thấp và một tập hợp nhỏ các trọng số, tương ứng, để cá nhân hóa phong cách. DreamArtist [5] giới thiệu một phương pháp cá nhân hóa một lần mới sử dụng chiến lược điều chỉnh prompt tích cực-tiêu cực. Trong khi các phương pháp tinh chỉnh này mang lại kết quả chất lượng cao, chúng thường bị giới hạn trong việc học chỉ một khái niệm (chủ đề hoặc phong cách). Một ngoại lệ là Custom Diffusion [20], cố gắng học nhiều khái niệm đồng thời. Tuy nhiên, Custom Diffusion đòi hỏi huấn luyện chung đắt đỏ từ đầu và vẫn mang lại kết quả kém hơn khi được sử dụng cho tạo phong cách vì không thể tách biệt phong cách khỏi chủ đề.

Kết hợp LoRA. Việc kết hợp các LoRA khác nhau vẫn chưa được khám phá đầy đủ trong văn học đặc biệt từ quan điểm hợp nhất các khái niệm phong cách và chủ đề. Ryu [30] trình bày một phương pháp kết hợp các LoRA được huấn luyện độc lập bằng tổng số học có trọng số. Trong [10], các tác giả thảo luận về việc hợp nhất nhiều LoRA khái niệm, tuy nhiên, đây là một phương pháp đắt đỏ đòi hỏi huấn luyện lại vì nó không hợp nhất LoRA mà thay vào đó huấn luyện lại toàn bộ mô hình. Một công trình đồng thời thảo luận về một chiến lược để có được Hỗn hợp Chuyên gia bằng cách kết hợp nhiều LoRA sử dụng một hàm cổng [1].

Tạo Phong cách Hình ảnh. Chuyển đổi phong cách dựa trên hình ảnh là một lĩnh vực nghiên cứu có từ ít nhất 20 năm trước [6, 12]. Những tiến bộ lớn trong chuyển đổi phong cách tùy ý đã đạt được bởi các phương pháp dựa trên mạng nơ-ron tích chập [9, 15, 16, 22, 26]. Các mô hình sinh như GAN [17–19] cũng có thể được sử dụng như một prior cho các nhiệm vụ tạo phong cách hình ảnh [2, 24, 35]. Nhiều phương pháp dựa trên GAN gần đây đạt được tạo phong cách một lần thành công [4, 7, 21, 23, 25, 32, 36–39] bằng cách tinh chỉnh GAN được huấn luyện trước cho một phong cách tham chiếu nhất định. Tuy nhiên, những phương pháp này bị giới hạn trong các hình ảnh chỉ từ một miền duy nhất (chẳng hạn như khuôn mặt). Hơn nữa, hầu hết các GAN hiện tại không cung cấp bất kỳ kiểm soát trực tiếp, dựa trên văn bản nào đối với ngữ nghĩa của đầu ra, do đó chúng không thể tạo ra chủ đề tham chiếu trong các bối cảnh mới. So với các mô hình sinh cũ hơn, các mô hình khuếch tán [13, 28, 34] cung cấp chất lượng tạo sinh vượt trội và kiểm soát dựa trên văn bản; tuy nhiên, cho đến nay, việc sử dụng chúng cho tạo phong cách một lần được điều khiển bởi các ví dụ hình ảnh là khó khăn. Của chúng tôi là một trong những nghiên cứu đầu tiên chứng minh việc sử dụng mô hình khuếch tán cho tạo phong cách chất lượng cao dựa trên ví dụ kết hợp với khả năng tái bối cảnh hóa đến các tình huống đa dạng.

3. Phương pháp

3.1. Bối cảnh
Mô hình Khuếch tán [13, 28, 34] là các mô hình sinh tiên tiến được biết đến với khả năng tổng hợp hình ảnh chất lượng cao, chân thực. Quá trình huấn luyện của chúng bao gồm hai giai đoạn: một quá trình tiến, trong đó một hình ảnh chuyển đổi thành nhiễu Gaussian thông qua việc thêm nhiễu Gaussian tăng dần, và một quá trình ngược, tái tạo dữ liệu gốc từ nhiễu. Quá trình ngược thường được học bằng cách sử dụng U-net với hỗ trợ điều kiện văn bản cho phép tạo sinh văn bản-thành-hình ảnh tại thời điểm suy luận. Trong công trình của chúng tôi, chúng tôi tập trung vào mô hình khuếch tán tiềm ẩn được sử dụng rộng rãi [28] học quá trình khuếch tán trong không gian tiềm ẩn thay vì không gian hình ảnh. Cụ thể, chúng tôi sử dụng Stable Diffusion XL v1 [27] cho tất cả các thí nghiệm của mình.

Tinh chỉnh LoRA. LoRA (Thích ứng Thứ hạng Thấp) là một phương pháp thích ứng hiệu quả của các Mô hình Ngôn ngữ và Thị giác Lớn cho một nhiệm vụ downstream mới [14, 30]. Khái niệm chính của LoRA là các cập nhật trọng số ∆W cho trọng số mô hình cơ sở W0∈Rm×n trong quá trình tinh chỉnh có "thứ hạng nội tại thấp", do đó cập nhật ∆W có thể được phân tách thành hai ma trận thứ hạng thấp B∈Rm×r và A∈Rr×n để tham số hóa hiệu quả với ∆W=BA. Ở đây, r đại diện cho thứ hạng nội tại của ∆W với r << min(m, n). Trong quá trình huấn luyện, chỉ A và B được cập nhật để tìm ∆W=BA phù hợp, trong khi giữ W0 không đổi. Để suy luận, ma trận trọng số cập nhật W có thể được thu được dưới dạng W=W0+BA. Do tính hiệu quả của nó, LoRA được sử dụng rộng rãi để tinh chỉnh các mô hình khuếch tán mã nguồn mở.

Thiết lập Vấn đề. Trong công trình này, chúng tôi nhằm tạo ra những phiên bản chính xác của một đối tượng tùy chỉnh trong một phong cách tham chiếu nhất định bằng cách hợp nhất các trọng số LoRA thu được bằng cách tinh chỉnh riêng biệt một mô hình khuếch tán văn bản-thành-hình ảnh nhất định trên một vài hình ảnh tham chiếu của đối tượng/phong cách.

Chúng tôi bắt đầu với mô hình khuếch tán cơ sở được biểu diễn là D với trọng số được huấn luyện trước W(i)_0 với i là chỉ số lớp. Người ta có thể thích ứng mô hình cơ sở D với bất kỳ khái niệm nào bằng cách đơn giản thêm tập hợp trọng số LoRA tương ứng Lx{∆W(i)_x} vào trọng số mô hình. Chúng tôi biểu diễn nó là: DLx=D⊕Lx=W0+∆Wx. Chúng tôi bỏ chỉ số trên (i) để đơn giản vì các phép toán của chúng tôi được áp dụng trên tất cả các ma trận trọng số có hỗ trợ LoRA của mô hình cơ sở D.

Chúng tôi được cho hai tập hợp LoRA được huấn luyện độc lập Lc={∆W(i)_c} và Ls={∆W(i)_s} cho mô hình cơ sở D, và chúng tôi nhằm tìm một LoRA hợp nhất Lm={∆W(i)_m}=Merge(Lc, Ls) có thể kết hợp tác động của cả hai LoRA riêng lẻ để tạo phong cách cho đối tượng nhất định trong phong cách tham chiếu mong muốn.

Hợp nhất Trực tiếp. LoRA được sử dụng phổ biến như một mô-đun cắm-và-chạy trên mô hình cơ sở, do đó cách phổ biến nhất để kết hợp nhiều LoRA là một kết hợp tuyến tính đơn giản:

Lm=Lc+Ls⇒∆Wm=wc·∆Wc+ws·∆Ws, (1)

trong đó wc và ws là các hệ số của LoRA nội dung và phong cách, tương ứng, cho phép kiểm soát "cường độ" của mỗi LoRA. Đối với một LoRA chủ đề và phong cách nhất định, người ta có thể tìm ra một sự kết hợp cụ thể của wc và ws cho phép tạo phong cách chính xác thông qua tìm kiếm lưới cẩn thận và đánh giá chủ quan của con người, nhưng phương pháp này không mạnh mẽ và rất tốn thời gian. Để đạt được điều này, chúng tôi đề xuất một phương pháp không có siêu tham số không yêu cầu quá trình phiền phức này.

3.2. ZipLoRA
Phương pháp của chúng tôi xây dựng trên hai hiểu biết thú vị:

(1) Các ma trận cập nhật LoRA là thưa thớt. Chúng tôi quan sát thấy rằng các ma trận cập nhật ∆W cho các lớp LoRA khác nhau là thưa thớt, tức là, hầu hết các phần tử trong ∆W có độ lớn rất gần với không, và do đó có ít tác động đến đầu ra của mô hình được tinh chỉnh. Đối với mỗi lớp, chúng ta có thể sắp xếp tất cả các phần tử theo độ lớn của chúng và đặt bằng không những phần tử thấp nhất đến một phần trăm nhất định. Chúng tôi mô tả sự phân bố của các phần tử của ∆W^(m×n)_i trong Hình 2, cùng với các mẫu được tạo ra sau khi đặt bằng không 80% và 90% các phần tử có độ lớn thấp nhất của ma trận cập nhật trọng số ∆W cho tất cả các lớp. Như có thể thấy, hiệu suất mô hình không bị ảnh hưởng ngay cả khi 90% các phần tử bị loại bỏ. Quan sát này xuất phát từ thực tế rằng thứ hạng của ∆W rất nhỏ theo thiết kế, do đó thông tin chứa trong hầu hết các cột của ∆W là dư thừa.

(2) Các trọng số LoRA có độ căn chỉnh cao hợp nhất kém. Các cột của ma trận trọng số của hai LoRA được huấn luyện độc lập có thể chứa thông tin không được tách biệt, tức là, độ tương tự cosine giữa chúng có thể khác không. Chúng tôi quan sát thấy rằng mức độ căn chỉnh giữa các cột của trọng số LoRA đóng vai trò quan trọng trong việc xác định chất lượng của kết quả hợp nhất: nếu chúng ta cộng trực tiếp các cột có độ tương tự cosine khác không với nhau, nó dẫn đến sự chồng chéo thông tin về các khái niệm riêng lẻ, dẫn đến mất khả năng của mô hình hợp nhất trong việc tổng hợp các khái niệm đầu vào một cách chính xác. Chúng tôi tiếp tục quan sát thấy rằng việc mất thông tin như vậy được tránh khi các cột trực giao với nhau với độ tương tự cosine bằng không.

Lưu ý rằng mỗi ma trận trọng số đại diện cho một biến đổi tuyến tính được định nghĩa bởi các cột của nó, vì vậy trực quan rằng việc hợp nhất sẽ giữ lại thông tin có sẵn trong những cột này chỉ khi những cột đang được cộng lại trực giao với nhau. Đối với hầu hết các cặp LoRA nội dung-phong cách, độ tương tự cosine khác không, dẫn đến can thiệp tín hiệu khi chúng được cộng trực tiếp. Trong Hình 3, chúng tôi hiển thị các giá trị độ tương tự cosine trung bình cho mỗi lớp của khối U-net cuối cùng cho một cặp nội dung-phong cách cụ thể trước và sau khi áp dụng ZipLoRA. Người ta có thể thấy các giá trị độ tương tự cosine khác không cao cho việc hợp nhất trực tiếp dẫn đến chất lượng tạo phong cách kém. Mặt khác, ZipLoRA giảm các giá trị tương tự đáng kể để đạt được kết quả vượt trội.

Để ngăn chặn can thiệp tín hiệu trong quá trình hợp nhất, chúng tôi nhân mỗi cột với một hệ số có thể học được sao cho có thể đạt được tính trực giao giữa các cột. Thực tế rằng các cập nhật LoRA là thưa thớt cho phép chúng tôi bỏ qua một số cột từ mỗi LoRA, do đó tạo thuận lợi cho nhiệm vụ giảm thiểu can thiệp. Như thể hiện trong Hình 4, chúng tôi giới thiệu một tập hợp các vectơ hệ số hợp nhất mc và ms cho mỗi lớp LoRA của LoRA nội dung và phong cách, tương ứng:

Lm=Merge(Lc, Ls, mc, ms)
⇒∆Wm=mc⊗∆Wc+ms⊗Ws, (2)

trong đó ⊗ đại diện cho phép nhân element-wise giữa ∆W và vectơ hệ số hợp nhất được broadcast m sao cho cột thứ j của ∆W được nhân với phần tử thứ j của m. Các chiều của mc và ms bằng với số lượng cột trong ∆W tương ứng, do đó mỗi phần tử của vectơ hệ số hợp nhất đại diện cho đóng góp của cột tương ứng của ma trận LoRA ∆W vào việc hợp nhất cuối cùng.

Phương pháp ZipLoRA của chúng tôi có hai mục tiêu: (1) giảm thiểu can thiệp giữa các LoRA nội dung và phong cách, được định nghĩa bởi độ tương tự cosine giữa các cột của LoRA nội dung và phong cách trong khi (2) bảo tồn khả năng của LoRA hợp nhất để tạo ra chủ đề tham chiếu và phong cách một cách độc lập bằng cách giảm thiểu sự khác biệt giữa hình ảnh chủ đề/phong cách được tạo ra bởi LoRA hỗn hợp và LoRA chủ đề/phong cách gốc. Để đảm bảo rằng các cột được hợp nhất với nhau giảm thiểu can thiệp tín hiệu, hàm loss được đề xuất của chúng tôi tìm cách giảm thiểu độ tương tự cosine giữa các vectơ hợp nhất mc và ms của mỗi lớp. Trong khi đó, chúng tôi muốn đảm bảo rằng hành vi gốc của cả LoRA phong cách và nội dung được giữ nguyên trong mô hình hợp nhất. Do đó, như được mô tả trong Hình 4, chúng tôi xây dựng một bài toán tối ưu hóa với hàm loss sau:

Lmerge=∥(D⊕Lm)(xc, pc)−(D⊕Lc)(xc, pc)∥2
+∥(D⊕Lm)(xs, ps)−(D⊕Ls)(xs, ps)∥2
+λ∑i|m(i)_c·m(i)_s|, (3)

trong đó mô hình hợp nhất Lm được tính toán bằng cách sử dụng mc và ms theo Phương trình 2; pc, ps là các prompt điều kiện văn bản cho tham chiếu nội dung và phong cách tương ứng, và λ là một hệ số nhân phù hợp cho số hạng loss độ tương tự cosine. Lưu ý rằng hai số hạng đầu tiên đảm bảo rằng mô hình hợp nhất giữ lại khả năng tạo ra phong cách và nội dung riêng lẻ, trong khi số hạng thứ ba áp đặt một ràng buộc trực giao giữa các cột của các trọng số LoRA riêng lẻ. Quan trọng là, chúng tôi giữ trọng số của mô hình cơ sở và các LoRA riêng lẻ được đóng băng, và chỉ cập nhật các vectơ hệ số hợp nhất. Như được thấy trong phần tiếp theo, một phương pháp tối ưu hóa đơn giản như vậy hiệu quả trong việc tạo ra tạo phong cách mạnh mẽ cho các chủ đề tùy chỉnh. Hơn nữa, ZipLoRA chỉ yêu cầu 100 cập nhật gradient nhanh hơn 10 lần so với các phương pháp huấn luyện chung.

4. Thí nghiệm
Bộ dữ liệu. Chúng tôi chọn một tập hợp đa dạng các hình ảnh nội dung từ bộ dữ liệu DreamBooth [29], cung cấp 30 tập hình ảnh mỗi tập chứa 4-5 hình ảnh của một chủ đề nhất định. Tương tự, một tập hợp đa dạng các hình ảnh tham chiếu phong cách được chọn từ dữ liệu được cung cấp bởi các tác giả của StyleDrop [33]. Chúng tôi chỉ sử dụng một hình ảnh duy nhất cho mỗi phong cách. Thông tin ghi công và giấy phép cho tất cả các hình ảnh nội dung và phong cách được sử dụng có sẵn trong các bản thảo/trang web của DreamBooth và StyleDrop.

Thiết lập Thí nghiệm. Chúng tôi thực hiện tất cả các thí nghiệm của mình bằng cách sử dụng mô hình cơ sở SDXL v1.0 [27]. Chúng tôi sử dụng tinh chỉnh DreamBooth với LoRA thứ hạng 64 để có được tất cả các LoRA phong cách và nội dung. Chúng tôi cập nhật các trọng số LoRA bằng cách sử dụng tối ưu hóa Adam trong 1000 bước với kích thước batch là 1 và tốc độ học là 0.00005. Chúng tôi giữ các bộ mã hóa văn bản của SDXL được đóng băng trong quá trình tinh chỉnh LoRA. Đối với ZipLoRA, chúng tôi sử dụng λ=0.01 trong Phương trình 3 cho tất cả các thí nghiệm của mình, và chạy tối ưu hóa cho đến khi độ tương tự cosine giảm xuống không với số lượng cập nhật gradient tối đa được đặt là 100.

4.1. Hành vi điều chỉnh phong cách của mô hình SDXL
Như đã thảo luận trong Phần 3, chúng tôi quan sát, đáng ngạc nhiên, rằng mô hình SDXL được huấn luyện trước thể hiện việc học phong cách mạnh mẽ khi được tinh chỉnh chỉ trên một hình ảnh phong cách tham chiếu duy nhất. Chúng tôi hiển thị kết quả điều chỉnh phong cách trên mô hình SDXL trong Hình 5. Đối với mỗi hình ảnh tham chiếu, chúng tôi áp dụng tinh chỉnh LoRA của mô hình SDXL bằng cách sử dụng mục tiêu DreamBooth với thứ hạng LoRA = 64. Để tinh chỉnh, chúng tôi tuân theo một cách tạo prompt tương tự như được cung cấp trong StyleDrop: "một <object> trong phong cách <style description>". Sau khi được tinh chỉnh, SDXL có thể biểu diễn tập hợp đa dạng các khái niệm trong phong cách tham chiếu bằng cách nắm bắt các sắc thái của phong cách vẽ, ánh sáng, màu sắc và hình học một cách chính xác. Câu hỏi về lý do tại sao mô hình này thể hiện hiệu suất học phong cách mạnh mẽ này, trái ngược với hiệu suất kém hơn của các phiên bản SD trước đó [28] (hoặc Imagen [31]) vẫn còn mở và có thể có nhiều câu trả lời bao gồm dữ liệu huấn luyện, kiến trúc mô hình và lược đồ huấn luyện.

Chúng tôi cũng cung cấp so sánh với StyleDrop trên Muse [3], DreamBooth trên Imagen, và DreamBooth trên Stable Diffusion trong Hình 5. Chúng tôi quan sát thấy rằng điều chỉnh phong cách SDXL hoạt động tốt hơn đáng kể so với các phương pháp cạnh tranh. Lưu ý rằng StyleDrop yêu cầu huấn luyện lặp lại với phản hồi từ con người trong khi điều chỉnh phong cách SDXL thì không. Hành vi này của SDXL làm cho nó trở thành ứng cử viên hoàn hảo để điều tra việc hợp nhất các LoRA phong cách với LoRA chủ đề để đạt được các tạo phong cách cá nhân hóa. Do đó, chúng tôi chọn sử dụng nó làm mô hình cơ sở cho tất cả các thí nghiệm của mình.

4.2. Tạo phong cách cá nhân hóa
Để bắt đầu, chúng tôi có được các LoRA phong cách theo điều chỉnh phong cách trên SDXL như được mô tả trong Phần 4.1, và có được các LoRA đối tượng bằng cách áp dụng tinh chỉnh DreamBooth trên các tham chiếu chủ đề. Hình 1 và Hình 6 hiển thị kết quả của phương pháp chúng tôi để kết hợp các LoRA phong cách và nội dung khác nhau. Phương pháp của chúng tôi thành công trong việc bảo tồn danh tính của chủ đề tham chiếu và nắm bắt các đặc điểm độc đáo của phong cách tham chiếu.

Chúng tôi cũng trình bày so sánh định tính với các phương pháp khác trong Hình 6. Như một baseline, chúng tôi so sánh với việc hợp nhất số học trực tiếp thu được thông qua Phương trình 1 với wc và ws được đặt là 1. Việc cộng trực tiếp như vậy dẫn đến mất thông tin được nắm bắt trong mỗi LoRA và tạo ra kết quả kém hơn với đối tượng và/hoặc phong cách bị biến dạng.

Chúng tôi cũng so sánh phương pháp của mình với huấn luyện chung của chủ đề và phong cách bằng cách sử dụng một biến thể đa chủ đề của DreamBooth với nhiều định danh duy nhất hiếm. Như được hiển thị, huấn luyện chung không thể học được việc tách biệt giữa đối tượng và phong cách và tạo ra kết quả kém. Nó cũng là phương pháp ít linh hoạt nhất vì không cho phép sử dụng các LoRA được huấn luyện trước, cũng không thể được sử dụng như một LoRA chỉ phong cách hoặc chỉ nội dung. Hơn nữa, nó yêu cầu gấp 10 lần số bước huấn luyện so với ZipLoRA.

StyleDrop [33] đề xuất phương pháp DreamBooth+StyleDrop để đạt được các tạo phong cách cá nhân hóa, trong đó phương pháp StyleDrop được áp dụng trên mô hình DreamBooth được tinh chỉnh trên đối tượng tham chiếu. Các so sánh của chúng tôi cho thấy rằng hiệu suất của nó không lý tưởng, xem xét chi phí tính toán cao và yêu cầu phản hồi từ con người. Nó cũng yêu cầu điều chỉnh trọng số mô hình đối tượng và phong cách wc và ws tương tự như việc hợp nhất trực tiếp để tạo ra đầu ra hợp lý, trong khi phương pháp của chúng tôi không cần bất kỳ điều chỉnh siêu tham số nào như vậy.

Kết quả định lượng. Chúng tôi tiến hành nghiên cứu người dùng để so sánh định lượng phương pháp của chúng tôi với các phương pháp hiện tại. Trong nghiên cứu của chúng tôi, mỗi người tham gia được hiển thị một chủ đề tham chiếu và một phong cách tham chiếu cùng với đầu ra của hai phương pháp được so sánh, theo thứ tự ngẫu nhiên, và được hỏi đầu ra nào mô tả tốt nhất phong cách tham chiếu trong khi duy trì độ trung thực của chủ đề tham chiếu. Chúng tôi tiến hành các nghiên cứu người dùng riêng biệt cho ZipLoRA so với từng phương pháp cạnh tranh trong ba trường hợp, và nhận được 360 phản hồi từ 45 người dùng cho mỗi trường hợp. Chúng tôi hiển thị kết quả trong Bảng 1. Như chúng ta có thể thấy, ZipLoRA nhận được sự ưa thích của người dùng cao hơn trong cả ba trường hợp do tạo phong cách chất lượng cao trong khi duy trì tính toàn vẹn của chủ đề.

Theo DreamBooth [29], chúng tôi cũng cung cấp so sánh bằng cách sử dụng điểm căn chỉnh hình ảnh và căn chỉnh văn bản trong Bảng 2. Chúng tôi sử dụng ba chỉ số: để căn chỉnh phong cách, chúng tôi sử dụng điểm CLIP-I của embedding hình ảnh của đầu ra và tham chiếu phong cách; để căn chỉnh chủ đề, chúng tôi sử dụng các tính năng DINO cho đầu ra và chủ đề tham chiếu; và để căn chỉnh văn bản, chúng tôi sử dụng embedding CLIP-T của đầu ra và prompt văn bản. Trong cả ba trường hợp, chúng tôi sử dụng độ tương tự cosine làm chỉ số và tính toán trung bình trên 4 chủ đề trong 8 phong cách mỗi chủ đề. ZipLoRA dẫn đến điểm căn chỉnh phong cách cạnh tranh so với huấn luyện chung và hợp nhất trực tiếp, trong khi đạt được điểm cao hơn đáng kể cho căn chỉnh chủ đề. Điều này làm nổi bật sự vượt trội của ZipLoRA trong việc duy trì độ trung thực của chủ đề. ZipLoRA cũng vượt trội hơn hai phương pháp kia trong căn chỉnh văn bản, ngụ ý rằng nó bảo tồn khả năng tạo sinh văn bản-thành-hình ảnh, và cũng biểu đạt phong cách và chủ đề được chỉ định tốt hơn (vì những điều này cũng là một phần của prompt văn bản). Cần lưu ý rằng những chỉ số này không hoàn hảo, đặc biệt khi đo lường căn chỉnh phong cách, vì chúng thiếu khả năng nắm bắt các chi tiết phong cách tinh tế, và bị entangle với các thuộc tính ngữ nghĩa của hình ảnh, chẳng hạn như nội dung tổng thể.

Khả năng tái bối cảnh hóa. Mô hình ZipLoRA hợp nhất có thể tái bối cảnh hóa các đối tượng tham chiếu trong các bối cảnh đa dạng và với các sửa đổi ngữ nghĩa trong khi duy trì chất lượng tạo phong cách. Như được hiển thị trong Hình 7, phương pháp của chúng tôi bảo tồn khả năng tạo sinh văn bản-thành-hình ảnh của mô hình cơ sở trong khi tạo phong cách chính xác cho toàn bộ hình ảnh trong phong cách tham chiếu. Khả năng như vậy rất có giá trị trong các trường hợp sử dụng nghệ thuật khác nhau đòi hỏi kiểm soát bối cảnh, danh tính chủ đề và phong cách.

Kiểm soát mức độ tạo phong cách. Phương pháp dựa trên tối ưu hóa của chúng tôi trực tiếp cung cấp giá trị trọng số scalar cho mỗi cột của cập nhật LoRA, do đó loại bỏ nhu cầu điều chỉnh và điều chỉnh để có được kết quả hợp lý. Tuy nhiên, chúng tôi vẫn có thể cho phép cường độ của nội dung đối tượng và phong cách được thay đổi để có thêm khả năng kiểm soát. Người ta có thể giảm trọng số lớp phong cách bằng cách nhân chúng với một hệ số nhân scalar bổ sung ws để hạn chế đóng góp của phong cách trong đầu ra cuối cùng. Như được hiển thị trong Hình 9, điều này cho phép kiểm soát mượt mà mức độ tạo phong cách khi ws thay đổi từ 0 đến 1.

Khả năng tạo ra đối tượng tham chiếu và phong cách. Ngoài việc tạo ra các tạo phong cách chính xác, một việc hợp nhất LoRA lý tưởng cũng nên bảo tồn khả năng tạo ra đối tượng và phong cách riêng lẻ một cách chính xác. Bằng cách này, một mô hình LoRA hợp nhất cũng có thể được sử dụng như một sự thay thế cho cả hai LoRA riêng lẻ, hoặc như một mô hình Hỗn hợp-Chuyên gia. Như được hiển thị trong Hình 8, phương pháp của chúng tôi giữ lại hành vi gốc của cả hai mô hình và có thể tạo ra chính xác các yếu tố cấu trúc và phong cách cụ thể của mỗi LoRA cấu thành, trong khi hợp nhất trực tiếp thất bại.

5. Kết luận
Trong bài báo này, chúng tôi đã giới thiệu ZipLoRA, một phương pháp mới để hợp nhất liền mạch các LoRA phong cách và chủ đề được huấn luyện độc lập. Phương pháp của chúng tôi mở ra khả năng tạo ra bất kỳ chủ đề nào trong bất kỳ phong cách nào bằng cách sử dụng các mô hình khuếch tán đủ mạnh như SDXL. Bằng cách tận dụng những hiểu biết quan trọng về các trọng số LoRA được huấn luyện trước, chúng tôi vượt qua các phương pháp hiện tại cho nhiệm vụ này. ZipLoRA cung cấp một giải pháp hợp lý, rẻ và không có siêu tham số cho cá nhân hóa chủ đề và phong cách đồng thời, mở ra một mức độ khả năng kiểm soát sáng tạo mới cho các mô hình khuếch tán.
