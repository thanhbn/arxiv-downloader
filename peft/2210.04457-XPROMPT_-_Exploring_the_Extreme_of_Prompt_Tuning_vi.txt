# 2210.04457.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2210.04457.pdf
# Kích thước tệp: 1392346 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
XPROMPT: Khám phá cực hạn của Prompt Tuning
Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang,
Wei Wu, Xiaojun Quan, Dawei Song
Beijing Institute of Technology {mfang,czhang,dwsong}@bit.edu.cn
Meituan NLP {wangjingang02,wuwei30}@meituan.com, renlei_work@163.com
Meta AI wqfcr@fb.com
Sun Yat-Sen University quanxj3@mail.sysu.edu.cn

Tóm tắt
Prompt tuning học các prompt mềm để điều khiển các Pre-trained Language Models (PLMs) đã đóng băng để thực hiện các nhiệm vụ downstream theo cách hiệu quả về tham số. Trong khi prompt tuning đã dần đạt được mức hiệu suất của fine-tuning khi quy mô mô hình tăng lên, vẫn còn một khoảng cách hiệu suất lớn giữa prompt tuning và fine-tuning đối với các mô hình có quy mô vừa và nhỏ (thường ít hơn 11B tham số). Trong bài báo này, chúng tôi chứng minh thực nghiệm rằng các token prompt đã huấn luyện có thể có tác động tiêu cực đến một nhiệm vụ downstream và do đó làm giảm hiệu suất của nó. Để thu hẹp khoảng cách này, chúng tôi đề xuất một mô hình P ROMPT tuning mới với quy mô cực kỳ nhỏ (XP ROMPT) theo chế độ giả thuyết vé số may mắn. Cụ thể, XP ROMPT loại bỏ các token prompt tiêu cực ở các mức độ chi tiết khác nhau thông qua một pruning có cấu trúc phân cấp, tạo ra một prompt hiệu quả hơn về tham số nhưng vẫn có hiệu suất cạnh tranh. Các thí nghiệm toàn diện được thực hiện trên các nhiệm vụ SuperGLUE, và kết quả mở rộng chỉ ra rằng XP ROMPT có thể thu hẹp khoảng cách hiệu suất ở các quy mô mô hình nhỏ hơn.

1 Giới thiệu
Pre-trained Language Models (PLMs) đã được áp dụng rộng rãi và đạt được thành công đáng kể trong các nhiệm vụ NLP khác nhau (Devlin et al., 2019; Raffel et al., 2020; Zhou et al., 2020) theo mô hình pre-train-then-fine-tune (Liu et al., 2019). Mặc dù có hiệu suất hấp dẫn, fine-tuning không hiệu quả về tham số đối với PLMs quy mô lớn do thực tế là dung lượng bộ nhớ tỷ lệ thuận với số lượng tham số có thể huấn luyện mà gradient và trạng thái optimizer cần được lưu trữ (Guo et al., 2021).

Dawei Song và Jingang Wang là các tác giả liên hệ.

770M(T5-Large) 3B(T5-XL) 11B(T5-XXL)
Quy mô mô hình 75 80 85 90 95 Điểm số SuperGLUE
Fine-Tuning
Prompt-Tuning
XPrompt(Của chúng tôi)

Hình 1: XP ROMPT vượt trội hơn Prompt-Tuning vanilla (Lester et al., 2021) và có thể cải thiện đáng kể so với Prompt-Tuning trên các nhiệm vụ và quy mô mô hình. Điều đáng chú ý là có một khoảng cách hiệu suất nhỏ giữa prompt tuning và fine-tuning trên T5-XXL (11B) do các cài đặt hyperparameter và khởi tạo khác nhau. Những quan sát tương tự đã được tìm thấy trong Hình 3-a và Hình 3-b của Lester et al. (2021).

Gần đây, Prompt-Tuning (Lester et al., 2021; Liu et al., 2021b) đã được đề xuất để giải quyết vấn đề này bằng cách thêm một soft prompt vào đầu input và chỉ cập nhật các tham số của prompt tokens trong quá trình tuning. Prompt-Tuning cung cấp một giải pháp thay thế hiệu quả về tham số cho fine-tuning, vì quy mô của soft prompt nhỏ hơn hàng chục nghìn lần. Nó cũng đơn giản hơn về mặt khái niệm và linh hoạt hơn các phương pháp tuning hiệu quả về tham số khác như Adapters cần các sửa đổi xâm nhập vào các lớp transformer (Houlsby et al., 2019; Guo et al., 2021). Sử dụng ít tham số có thể điều chỉnh hơn, prompt tuning đạt được hiệu suất cạnh tranh với fine-tuning khi quy mô mô hình tăng lên. Tuy nhiên, vẫn còn một khoảng cách hiệu suất lớn giữa prompt tuning và fine-tuning đối với các mô hình có quy mô nhỏ hơn (như được thể hiện trong Hình 1).

Bài báo này nhằm mục đích lấp đầy khoảng cách này, từ góc độ giả thuyết vé số may mắn (LTH) (Frankle và Carbin, 2019). Chúng tôi được thúc đẩy bởi một quan sát rằng, trên một nhiệm vụ cụ thể, không phải tất cả các token prompt đều đóng góp như nhau cho hiệu suất nhiệm vụ, trong khi một số token prompt nhất định thậm chí có thể mang lại tác động tiêu cực.

--- TRANG 2 ---
WiC WSC RTE 70 75 80 85 90 95 Độ chính xác
74.29 86.53 89.16
75.7 89.42 91.33
74.82 85.61 89.53 Prompt-Tuning
Negative Prompt Masking
Random Prompt Masking

Hình 2: So sánh hiệu suất của Prompt-Tuning, Negative Prompt Masking và Random Prompt Masking với T5-XL(3B) trên ba nhiệm vụ SuperGLUE. Prompt-Tuning sử dụng tất cả prompt tokens. Negative Prompt Masking che các token prompt đã chọn (tiêu cực) có điểm số quan trọng thấp. Random Prompt Masking che ngẫu nhiên cùng số lượng token như trong Negative Prompt Masking.

Hình 2 cung cấp kết quả sơ bộ về quan sát này. Những token prompt tiêu cực này có thể được tránh theo chế độ LTH. Về bản chất, LTH nói rằng một mạng quá tham số chứa một mạng con mà, khi được khởi tạo và huấn luyện riêng biệt, có thể khớp hoặc vượt quá độ chính xác thử nghiệm của mạng gốc sau khi huấn luyện trong tối đa cùng số lần lặp. Mạng con được gọi là vé số may mắn, và tập hợp các vé được gọi là vé chiến thắng trong PLMs (Liang et al., 2021). Trong vấn đề prompt-tuning, các vé chiến thắng là tập hợp các token prompt tích cực có thể đạt được hiệu suất tương tự như sử dụng toàn bộ tập hợp prompt, trong khi các vé thua cuộc là tập hợp các token prompt tiêu cực.

Do đó, điều quan trọng là xác định các vé chiến thắng và loại bỏ những vé thua cuộc, trong tập hợp các token prompt đã huấn luyện. Cụ thể, chúng tôi đề xuất loại bỏ các vé thua cuộc thông qua một pruning có cấu trúc phân cấp, đầu tiên loại bỏ các token tiêu cực ở mức token và sau đó prune những token còn lại ở mức chi tiết hơn, tức là mức piece, để có sự cân bằng tốt hơn giữa hiệu quả và hiệu suất. Phù hợp với LTH, weight rewinding (Renda et al., 2020) được áp dụng để huấn luyện lại các soft prompt tích cực đã xác định. Với việc loại bỏ các token prompt tiêu cực, một PROMPT hiệu quả hơn về tham số với quy mô cực kỳ nhỏ (XP ROMPT) được thu được.

Để xác minh hiệu quả của XPROMPT, chúng tôi thực hiện một tập hợp thí nghiệm mở rộng trên SuperGLUE (Wang et al., 2019) trong cả kịch bản tài nguyên cao và tài nguyên thấp. Như được thể hiện trong Hình 1 và Bảng 1, kết quả cho thấy XPROMPT cải thiện đáng kể các phương pháp prompt-tuning trên các nhiệm vụ và quy mô mô hình. Đối với các mô hình có quy mô vừa phải, XPROMPT thu hẹp khoảng cách và đạt được hiệu suất tương đương với fine-tuning. Đối với các mô hình quy mô lớn, XPROMPT cũng dẫn đến những cải thiện hiệu suất lớn so với Prompt-Tuning, và thậm chí vượt qua fine-tuning cho hầu hết các nhiệm vụ.

2 Công trình liên quan
2.1 Pre-trained Language Models
Pre-trained Language Models (PLMs) đã đạt được thành công đáng kể trong các nhiệm vụ NLP khác nhau (Zhou et al., 2020; Raffel et al., 2020; Brown et al., 2020). BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019) là hai tiên phong học biểu diễn ngữ cảnh với các nhiệm vụ pre-training mô hình ngôn ngữ có mặt nạ (MLM) và dự đoán câu tiếp theo. Gần đây, một loạt PLMs quy mô lớn đã xuất hiện với các thiết kế pre-training khác nhau, như GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), ELECTRA (Clark et al., 2020), XLNet (Yang et al., 2019), BART (Lewis et al., 2020) và T5 (Raffel et al., 2020). Tuy nhiên, với số lượng tham số bùng nổ, fine-tuning các mô hình trở nên không hiệu quả về tham số và tốn kém về mặt tính toán do việc duy trì tất cả các tham số trong PLMs. Hơn nữa, người ta phải fine-tune các mô hình khác nhau cho các nhiệm vụ khác nhau và lưu trữ chúng riêng biệt, điều này tiêu tốn tài nguyên.

2.2 Prompt Learning trong NLP
Với sự phát triển của GPT-3 (Brown et al., 2020), prompt learning đã thu hút nhiều sự chú ý trong cộng đồng NLP (Liu et al., 2021a; Ding et al., 2022), cho phép học tập hiệu quả bằng cách thêm một số prompt tokens vào input. Prompt learning đã được chứng minh là hiệu quả trong các nhiệm vụ downstream khác nhau (Davison et al., 2019; Gong và Eldardiry, 2021; Radford et al., 2019; Wang et al., 2021; Khashabi et al., 2020). Gần đây, prompt đã được mở rộng từ các token rời rạc (token trong từ vựng) sang các token liên tục (embedding có thể huấn luyện), tức là soft prompt (Li và Liang, 2021; Zhong et al., 2021; Qin và Eisner, 2021). Ví dụ, (Lester et al., 2021) đề xuất một phương pháp prompt tuning hiệu quả về tham số bằng cách chỉ điều chỉnh soft prompt và cố định toàn bộ tham số trong PLM. Prompt tuning đạt được thành công lớn và cho thấy nó có thể đạt được hiệu suất của fine-tuning với PLM lớn. Tuy nhiên, vẫn còn một khoảng cách hiệu suất lớn giữa prompt tuning và fine-tuning đối với các mô hình có quy mô vừa phải.

--- TRANG 3 ---
Prompt-Tuning Hierarchical Structured Pruning Rewinding
Chuỗi đầu vào.………… Chuỗi đầu vào.… T5 (Encoder Decoder, Cố định) Đầu ra ……… Chuỗi đầu vào.………… Chuỗi đầu vào.………… T5 (Encoder Decoder, Cố định) Đầu ra T5 (Encoder Decoder, Cố định) Đầu ra T5 (Encoder Decoder, Cố định) Đầu ra
Token Prompt đã huấn luyện … Mặt nạ mức Token … Mặt nạ mức Piece … Token Prompt đã rewinding … Soft Prompt Token Piece

Hình 3: Minh họa phương pháp XP ROMPT được đề xuất của chúng tôi. XP ROMPT bao gồm ba giai đoạn, cụ thể là Prompt-Tuning, Hierarchical Structured Pruning và Rewinding. Trong tất cả các giai đoạn, các tham số của T5 được đóng băng - chỉ các tham số của prompt được điều chỉnh. Các prompt được huấn luyện ở giai đoạn trước được đưa vào giai đoạn tiếp theo làm prompt khởi tạo. Sự thay đổi màu sắc thể hiện quá trình mà các tham số của prompt được điều chỉnh hoặc pruned.

Gần đây hơn, (Vu et al., 2021) đề xuất một phương pháp học transfer dựa trên prompt, SPOT, để cải thiện hiệu suất của prompt tuning, học một prompt trên các nhiệm vụ nguồn và sau đó áp dụng để khởi tạo prompt của nhiệm vụ mục tiêu. Gần đây nhất, (He et al., 2022) đề xuất HyperPrompt sử dụng hypernetwork để tạo ra hyper-prompt và đạt được hiệu suất vượt trội. Tuy nhiên, nó cần điều chỉnh tất cả tham số và cho thấy rằng chỉ điều chỉnh các tham số có điều kiện tác vụ là không đủ để đạt được kết quả cạnh tranh như fine-tuning mô hình đầy đủ cho multi-task learning.

2.3 Giả thuyết Vé số may mắn
Giả thuyết vé số may mắn (Frankle và Carbin, 2019) phát hiện rằng một mạng quá tham số chứa một mạng con được khởi tạo sao cho - khi được huấn luyện riêng biệt - nó có thể khớp với độ chính xác thử nghiệm của mạng gốc sau khi huấn luyện trong tối đa cùng số lần lặp. Mạng con được gọi là vé số may mắn. Trong NLP, tập hợp các vé số may mắn được gọi là vé chiến thắng trong các mô hình có tham số hóa cao, ví dụ như PLMs (Liang et al., 2021). Những vé chiến thắng như vậy đã chứng minh khả năng của chúng trong việc chuyển giao qua các nhiệm vụ và tập dữ liệu (Morcos et al., 2019; Yu et al., 2020; Desai et al., 2019). Gần đây, Chen et al. (2021) đã chỉ ra sự tồn tại của các vé chiến thắng trong PLMs. Liang et al. (2021) quan sát rằng hiệu suất khái quát hóa của các vé chiến thắng thậm chí có thể vượt quá hiệu suất của mô hình đầy đủ.

3 Sơ bộ
Được xây dựng trên phương pháp text-to-text của T5 (Raffel et al., 2020), prompt tuning xây dựng tất cả các nhiệm vụ dưới dạng tạo văn bản bằng cách thêm các soft prompt tokens có thể điều chỉnh vào đầu input và chỉ cập nhật các tham số của các soft prompt tokens được chèn vào. Cụ thể, cho một chuỗi n input tokens X = {x1; x2; ... ; xn}, T5 đầu tiên tạo ra token embeddings Xe ∈ Rn×e, trong đó e là chiều của không gian embedding. Nó cũng tạo ra soft prompt embeddings Pe = {p1; p2; ...; pm} ∈ Rm×e, trong đó m là độ dài của soft prompt. Sau đó, soft prompt được thêm vào đầu chuỗi input là [Pe; Xe] ∈ R(m+n)×e. Mục tiêu của prompt tuning là tối đa hóa likelihood của nhãn Y bằng cách chỉ tối ưu hóa Pe:

arg max Pe log p(Y|[Pe; Xe]) (1)

Prompt tuning trở nên hiệu quả hơn khi quy mô mô hình tăng lên. Tuy nhiên, vẫn còn một khoảng cách hiệu suất đáng kể giữa prompt tuning và fine-tuning đặc biệt đối với các mô hình có quy mô nhỏ và vừa phải. Giả thuyết của chúng tôi là không phải tất cả các soft prompt tokens đều đóng góp như nhau cho hiệu suất sau khi huấn luyện trên nhiệm vụ mục tiêu. Tồn tại một số soft prompt tokens nhất định có thể có tác động tiêu cực đến nhiệm vụ. Do đó, kết hợp ý tưởng của giả thuyết vé số may mắn, chúng tôi đề xuất XPROMPT với hierarchical structured pruning để xác định các soft prompt tối ưu và thu hẹp khoảng cách hiệu suất.

4 XP ROMPT
Quá trình tổng thể của XPROMPT được minh họa trong Hình 3, bao gồm ba giai đoạn chính: Prompt-Tuning, Hierarchical Structured Pruning và Rewinding. Cụ thể, prompt tuning học một tập hợp giá trị ban đầu cho tất cả các soft prompt tokens trên nhiệm vụ mục tiêu. Trong quá trình hierarchical structured pruning, các quá trình pruning ở mức token và mức piece được thực hiện lặp đi lặp lại để xác định các soft token và piece tối ưu ở các tỷ lệ nén khác nhau. Cuối cùng, một kỹ thuật weight rewinding được áp dụng để huấn luyện lại các soft prompt.

4.1 Prompt Tuning
Phương pháp prompt tuning thêm một số soft prompt tokens vào đầu input và chỉ điều chỉnh soft prompt bằng cách cố định toàn bộ tham số trong PLM. Prompt tuning đã được chứng minh là hiệu quả trong các nhiệm vụ downstream khác nhau. Trong giai đoạn prompt tuning của chúng tôi, theo các công trình trước đây (Liang et al., 2021), chúng tôi thực hiện một quá trình tuning hoàn chỉnh trên nhiệm vụ mục tiêu để có được embeddings cho tất cả các soft prompt tokens. Những soft prompt đã huấn luyện này được sử dụng làm khởi tạo trong hierarchical structured pruning.

4.2 Hierarchical Structured Pruning
Hierarchical structured pruning được thiết kế để tách các token prompt tiêu cực khỏi các token prompt đã huấn luyện và xác định một tập hợp tối ưu các soft prompt. Phương pháp được minh họa trong Hình 4. Pruning ở mức token được sử dụng đầu tiên để xác định các token prompt tiêu cực, tuy nhiên, các token prompt còn lại vẫn có thể chứa các piece tiêu cực. Do đó, pruning ở mức piece sau đó được áp dụng để xác định các piece prompt tiêu cực chi tiết hơn trong mỗi token prompt. Pruning ở mức token và mức piece cùng nhau tạo ra sự cân bằng tốt hơn giữa hiệu quả và hiệu suất.

4.2.1 Pruning ở mức Token
Để xác định các token prompt tiêu cực trong các token prompt đã huấn luyện, chúng tôi liên kết biến mask αi với mỗi vector soft prompt token pi:

P̂e = α ⊙ Pe (2)

trong đó α = {α1; α2; ... ; αm}; αi ∈ {0, 1}, và giá trị 0 cho biết rằng soft prompt token tương ứng bị pruned.

--- TRANG 4 ---
Token-level Pruning Piece-level Pruning
……………… ……………… ……………… p1 p2 pm p1 p2 pm p1 p2 pm q1 q2 qk

Hình 4: Minh họa Hierarchical Structured Pruning. Trong đó, độ đậm nhạt của màu sắc chỉ ra mức độ của điểm quan trọng, và màu càng đậm, điểm quan trọng của cấu trúc tương ứng (token hoặc piece) càng cao.

Sau đó chúng tôi tính toán điểm quan trọng (Michel et al., 2019) của mỗi token để phân biệt các token prompt tiêu cực với các token khác. Điểm quan trọng được định nghĩa là độ nhạy cảm kỳ vọng của đầu ra mô hình đối với các biến mask. Chính thức, điểm quan trọng Ipi của mỗi soft prompt token pi được tính như sau:

Ipi = E x~Dx |∂L(x)/∂αi| (3)

trong đó L là hàm mất mát và Dx là phân phối dữ liệu huấn luyện.

Về cơ bản, điểm quan trọng của mỗi soft prompt token chỉ ra đóng góp cá nhân của nó cho hiệu suất mô hình. Một điểm quan trọng thấp có nghĩa là soft prompt token tương ứng có đóng góp nhỏ hoặc thậm chí tiêu cực cho mô hình. Nói cách khác, soft prompt token như vậy chứa thông tin prompt không đáng kể để tạo ra đầu ra. Ngược lại, một điểm quan trọng lớn ngụ ý một đóng góp lớn với thông tin prompt có ý nghĩa hơn. Do đó, các token prompt có điểm quan trọng thấp rất có thể là các token prompt tiêu cực, được pruned trong giai đoạn pruning ở mức token.

4.2.2 Pruning ở mức Piece
Pruning ở mức token tìm thấy các soft prompt token quan trọng nhất. Tuy nhiên, nó có thể không đủ vì vẫn còn các piece prompt tiêu cực chi tiết hơn trong embedding của mỗi soft prompt token. Các piece khác nhau của embedding có thể dẫn đến các hiệu ứng khác nhau trên các nhiệm vụ downstream. Do đó, chúng tôi tiến hành pruning ở mức piece để loại bỏ các piece prompt tiêu cực trong mỗi token. Cụ thể, chúng tôi chia vector embedding của mỗi soft prompt token pie thành k piece có quy mô bằng nhau, qe = {q1e; q2e; ... ; qke}, và xem mỗi piece như một đơn vị độc lập có thể được tối ưu hóa với các cập nhật gradient. Biến mask βi được liên kết với mỗi piece trong soft prompt token để xác định các piece prompt tiêu cực:

q̂e = β ⊙ qe (4)

trong đó β = {β1; β2; ... ; βk}; βi ∈ {0, 1}, và giá trị 0 cho biết rằng piece tương ứng bị pruned.

Sau đó chúng tôi tính điểm quan trọng Iqi của mỗi piece cho mỗi embedding token prompt để prune các piece có tầm quan trọng thấp:

Iqi = E x~Dx |∂L(x)/∂βi| (5)

--- TRANG 5 ---
Tương tự như điểm quan trọng ở mức token, một điểm quan trọng ở mức piece thấp cho thấy rằng piece đó có đóng góp nhỏ hoặc thậm chí tiêu cực đối với hiệu suất mô hình. Những piece có tầm quan trọng thấp như vậy chứa thông tin hạn chế để tạo ra đầu ra. Chúng tôi thực hiện lặp đi lặp lại cả pruning ở mức token và mức piece để có được các sub-prompt token và piece ở các tỷ lệ nén khác nhau.

4.3 Rewinding
Giả thuyết vé số may mắn (LTH) (Frankle và Carbin, 2019) nói rằng các mạng con thưa thớt (các prompt chưa bị pruned) có thể được huấn luyện riêng biệt đến cùng độ chính xác như mạng gốc (tất cả prompt), và đề xuất huấn luyện đến pruning và sau đó rewinding các trọng số chưa bị pruned. Theo ý tưởng trong LTH, chúng tôi áp dụng kỹ thuật weight rewinding (Renda et al., 2020) để huấn luyện lại các soft prompt sau pruning có cấu trúc phân cấp hai mức. Cụ thể, chúng tôi đặt lại các tham số của các soft prompt tối ưu đã chọn bằng cách sử dụng giá trị của chúng sau giai đoạn prompt tuning. Các soft prompt khác được pruned bằng cách đặt các biến mask tương ứng thành 0. Cuối cùng, chúng tôi huấn luyện lại các soft prompt bằng cách sử dụng các chiến lược học tập ban đầu trong prompt tuning.

5 Thí nghiệm
5.1 Tập dữ liệu
Để bao phủ các nhiệm vụ NLP rộng rãi và đa dạng trong thí nghiệm của chúng tôi, chúng tôi đánh giá phương pháp của mình trên các tập dữ liệu khác nhau của benchmark SuperGLUE (Wang et al., 2019) trong cả kịch bản tài nguyên cao và tài nguyên thấp. Do quyền truy cập thử nghiệm hạn chế cho SuperGLUE, theo các công trình trước đây (Lester et al., 2021; Ding et al., 2021), chúng tôi điều chỉnh mô hình prompt trên tập huấn luyện trong một số bước cố định và báo cáo kết quả trên tập validation bằng cách sử dụng checkpoint tốt nhất. Mô tả chi tiết, thống kê và số liệu của các nhiệm vụ SuperGLUE được cung cấp trong Bảng 9 của Phụ lục E. Các mẫu soft prompt và generation verbalizers được cung cấp trong Bảng 10 của Phụ lục E.

5.2 Baseline
Fine-Tuning Chúng tôi so sánh với phương pháp fine-tuning tiêu chuẩn (Raffel et al., 2020; Aribandi et al., 2021) của T5, trong đó tất cả các tham số pre-trained được fine-tuned trên mỗi nhiệm vụ mục tiêu riêng biệt.

Prompt-Tuning Phương pháp prompt tuning vanilla của (Lester et al., 2021) cho thấy rằng prompt tuning là một kỹ thuật cạnh tranh để thích ứng các PLM đóng băng với các nhiệm vụ downstream.

P-Tuning (Liu et al., 2021c) là một phương pháp dựa trên prompt sử dụng PLM có mask để chuyển đổi nhiệm vụ mục tiêu thành một vấn đề cloze. Nó sử dụng các kỹ thuật soft-prompting để tối ưu hóa prompt trong không gian liên tục. Chúng tôi cũng so sánh với phiên bản thứ hai của nó P-TuningV2 (Liu et al., 2021b).

Prefix-Tuning (Li và Liang, 2021) là một giải pháp thay thế nhẹ cho fine-tuning cho các nhiệm vụ tạo ngôn ngữ tự nhiên, chỉ tối ưu hóa một vector liên tục nhỏ cụ thể cho nhiệm vụ (gọi là prefix). Prefix-Tuning thêm prefix vào đầu input của mỗi lớp transformer độc lập.

5.3 Triển khai
Phương pháp của chúng tôi được triển khai với thư viện OpenPrompt (Ding et al., 2021), một bộ công cụ thống nhất và mở rộng cho prompt learning. Chúng tôi dịch mỗi tập dữ liệu SuperGLUE thành định dạng text-to-text theo (Raffel et al., 2020), ngoại trừ việc chúng tôi bỏ qua các tên nhiệm vụ thêm vào đầu input chỉ ra tập dữ liệu SuperGLUE nào mà một ví dụ thuộc về.

XPROMPT của chúng tôi được xây dựng dựa trên các checkpoint T5 pre-trained của ba quy mô: Large, XL, XXL với 770M, 3B và 11B tham số tương ứng. Theo các nghiên cứu trước đây (Lester et al., 2021; Ding et al., 2021), chúng tôi huấn luyện prompt của mình trong 100 epoch với tốc độ học không đổi 0.3 và batch size 16. (Lester et al., 2021) cho thấy rằng việc tăng vượt quá 20 token chỉ mang lại lợi ích biên, vì vậy trong suốt thí nghiệm của chúng tôi, chúng tôi đặt số lượng token prompt mặc định là 20 để kiểm soát số lượng tham số có thể huấn luyện và sử dụng từ vựng được lấy mẫu để khởi tạo các tham số prompt. Số lượng piece trong mỗi token được đặt thành 16. Tần suất pruning được tìm kiếm tuyến tính từ {10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%}. Weight rewinding được áp dụng chỉ một lần để huấn luyện lại các soft prompt đã pruned. Các checkpoint tốt nhất được chọn thông qua early stopping trên tập development. Các mô hình được huấn luyện bằng optimizer Adafactor (Shazeer và Stern, 2018) với weight decay 1e-5.

6 Kết quả
6.1 Kết quả trong Kịch bản Tài nguyên Cao
XPROMPT cải thiện đáng kể hiệu suất của prompt tuning và giúp thu hẹp khoảng cách với fine-tuning trên tất cả các quy mô mô hình.

--- TRANG 6 ---
[Bảng 1 với kết quả thí nghiệm chính trên bảy nhiệm vụ SuperGLUE được dịch sang tiếng Việt]

--- TRANG 7 ---
[Tiếp tục dịch nội dung còn lại của trang 7 về kết quả trong kịch bản tài nguyên thấp]

--- TRANG 8 ---
[Tiếp tục dịch phần phân tích và thảo luận]

--- TRANG 9 ---
[Tiếp tục dịch phần kết luận và hạn chế]

--- TRANG 10 ---
[Tiếp tục dịch phần tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục dịch phần tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục dịch phần tài liệu tham khảo]

--- TRANG 13 ---
[Tiếp tục dịch phần phụ lục A]

--- TRANG 14 ---
[Tiếp tục dịch phần phụ lục B và C]

--- TRANG 15 ---
[Tiếp tục dịch phần phụ lục D và E]
