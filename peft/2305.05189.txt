# 2305.05189.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2305.05189.pdf
# File size: 11980707 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion
Models with Large Language Models
Shanshan Zhong∗
zhongshsh5@mail2.sysu.edu.cn
Sun Yat-sen University
Guangzhou, ChinaZhongzhan Huang∗
huangzhzh23@mail2.sysu.edu.cn
Sun Yat-sen University
Guangzhou, ChinaWushao Wen
wenwsh@mail.sysu.edu.cn
Sun Yat-sen University
Guangzhou, China
Jinghui Qin†
scape1989@gmail.com
Guangdong University of Technology
Guangzhou, ChinaLiang Lin
linliang@ieee.org
Sun Yat-sen University
Guangzhou, China
A collection of seven vintage glass bottles in dif ferent shapes
and sizes, arranged on a windowsill
A vibrant butterfly with iridescent wings in shades of blue,
green, and purple, perched on a bright pink flower
An aristocratic girl in medieval finery and a headdress of
bright feathers drinking afternoon tea
Three fluf fy white kittens playing with a ball of yarn on a
bright green carpet
Figure 1: 512 ×512 samples with various types of prompts (Counting, Color, Action, etc.), showing that SUR-adapter has powerful
capabilities of fine-grained semantic control.
ABSTRACT
Diffusion models, which have emerged to become popular text-to-
image generation models, can produce high-quality and content-
rich images guided by textual prompts. However, there are limita-
tions to semantic understanding and commonsense reasoning in
existing models when the input prompts are concise narrative, re-
sulting in low-quality image generation. To improve the capacities
for narrative prompts, we propose a simple-yet-effective parameter-
efficient fine-tuning approach called the Semantic Understanding
and Reasoning adapter (SUR-adapter) for pre-trained diffusion mod-
els. To reach this goal, we first collect and annotate a new dataset
SURD which consists of more than 57,000 semantically corrected
multi-modal samples. Each sample contains a simple narrative
prompt, a complex keyword-based prompt, and a high-quality
image. Then, we align the semantic representation of narrative
prompts to the complex prompts and transfer knowledge of large
language models (LLMs) to our SUR-adapter via knowledge distilla-
tion so that it can acquire the powerful semantic understanding and
reasoning capabilities to build a high-quality textual semantic rep-
resentation for text-to-image generation. We conduct experiments
by integrating multiple LLMs and popular pre-trained diffusion
∗Both authors contributed equally to this research.
†Corresponding author.models to show the effectiveness of our approach in enabling dif-
fusion models to understand and reason concise natural language
without image quality degradation. Our approach can make text-
to-image diffusion models easier to use with better user experience,
which demonstrates our approach has the potential for further
advancing the development of user-friendly text-to-image genera-
tion models by bridging the semantic gap between simple narrative
prompts and complex keyword-based prompts. The code is released
at https://github.com/Qrange-group/SUR-adapter.
CCS CONCEPTS
•Computing methodologies →Natural language processing ;
Computer vision ;Machine learning algorithms .
KEYWORDS
diffusion model, large language model, multimodal image genera-
tion, adapter, knowledge distillation
1 INTRODUCTION
In recent years, diffusion model based multimodal text-to-image
generation techniques have made impressive strides [ 50]. With
these models [ 38,46] trained on massive amounts of data and model
parameters, people are able to generate text-relevant and visually
appealing images end-to-end through text prompts and other in-
formation, without requiring complex painting skills. However, thearXiv:2305.05189v4  [cs.CL]  29 Nov 2023

--- PAGE 2 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
Four freshly baked pies Seven vintage glass bottles
A green for est with tall trees
and a small waterfallA vibrant red sports car  speeding
down a winding roadCounting Color
Simple pr ompt Complex pr ompt
 Simple pr ompt + LLM
(a) Sementic understanding (b) Commonsense r easoning
Figure 2: The semantic understanding and commonsense reasoning capability of text encoder in diffusion models.
quality of image generation in these existing diffusion models heav-
ily relies on the complex and elaborate design of keyword-based
text prompts or other forms of text prompts. Furthermore, if the text
prompts are concise narratives or short phrases that are daily ex-
pressions, the fidelity and text relevance of the generated images are
often significantly compromised. This limitation makes diffusion
models difficult to be controlled intuitively by concise narratives
with excellent user experience. The most important reason for this
issue is that the text encoders of these diffusion models, which are
often the text encoder of pre-trained CLIP [ 34] trained with image-
text contrastive learning, are unaligned to text-to-image generation
task, leading to poor semantic understanding and reasoning (SUR)
for image generation.
To be specific, CLIP is a multi-modal neural model trained on
about 400M image-text pairs with contrastive learning and its image
encoder and text encoder have been widely applied in various multi-
modal tasks or models, such as diffusion models, since it can bridge
the association between images and text successfully. Although the
learning objective of CLIP is to establish image-text correspondence
by only pulling the matched image and text pair closer in feature
space, the text describing the corresponding image is brief and
may only match partial semantics in the image, resulting in the
incomplete feature generated by the text encoder. However, the
text-to-image generation task asks a text encoder can not only
understand the semantics of a concise narrative but also reason out
and complete the implicit commonsense or knowledge grounded in
the narrative so that a model can generate an accurate image that is
highly consistent with the narrative. Therefore, embedding the text
encoder of CLIP into diffusion models for conditional text-to-image
generation results in low-quality image generation when the input
text is natural language due to a lack of the capability of semantic
understanding and commonsense reasoning in the text encoder.
To show these deficiencies, we first evaluate the semantic under-
standing capability of the text encoder in diffusion models using
three common types of text prompts in multi-modal visual ques-
tion answering [ 1,7,8,31]: "counting", "color", and "action". As
shown in Table 1, we designed three different prompts for eachTable 1: Evaluation of semantic accuracy (Acc.) in images gen-
erated by simple prompts using diffusion models. The sim-
ple prompts consisted of three types of sentences, including
"Counting", "Color", and "Action". Each prompt generated 130
images, and the images were manually checked for semantic
accuracy. The results showed that the semantic accuracy of
most prompts is below 50%, and even two types of prompts
have the semantic accuracy rate of 0%.
Type Prompt Acc. <50%?
CountingFour freshly baked pies. 63.08%
Six colorful balloons floating over a picturesque landscape. 8.46% ✓
Seven vintage glass bottles. 0.00% ✓
ColorA vibrant red sports car speeding down a winding road. 86.15%
The blue glass containing red juice. 17.69% ✓
A couple wearing blue and yellow solid color clothes. 0.00% ✓
ActionSomeone shooting a basketball on the sports field. 41.54% ✓
Giraffes eating trees. 25.38% ✓
A chef tossing a pizza dough in the air in a kitchen. 15.38% ✓
type, and for each text prompt, we generated 130 images using a
text-to-image diffusion model [ 38] and manually evaluated whether
the generated images fulfilled the semantic meaning of the given
text prompts. Through statistical analysis of the results, we found
that the accuracy of semantic understanding for most of the text
prompts does not exceed 50%. Surprisingly, even seemingly sim-
ple narrative prompts such as "Seven vintage glass bottles" and "A
couple wearing blue and yellow solid color clothes respectively"
have 0% accuracy, indicating that the text encoder in the diffusion
model completely fails to understand the semantics of these sim-
ple texts for image generation and resulted in severe information
bias. Fig.2(a) further illustrates examples of semantic error due to
inadequate semantic understanding capability.
Next, we consider the commonsense reasoning ability of the
text encoder. If we hope the stable diffusion model to generate a
beautiful cat, according to widely verified generation techniques,
we need some complex and elaborate keyword-based prompts to
obtain high-quality generated images, such as the following prompt:

--- PAGE 3 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models Preprint, Technical Report,
(Complex prompt example ) 8k uhd, a RAW photo, a beauti-
ful cat, (realistic:1.1), masterwork, RAW photo, real cat, RAW
photograph, ultra high res, photorealistic, best quality, (high
detailed skin,skin details), visible pores, shiny skin, an ex-
tremely delicate and beautiful, extremely detailed 8K wallpa-
per, 8k high quality, film grain, DSLR, beautiful cat with beau-
tiful details, (looking at viewer), professional photography
lighting, extremely detailed eyes and face, eyes with beauti-
ful details, analog style, cute and playful, adorable, (splendid
and colorful:1.1), portrait picture of cat, <lora:mikeneko:0.7>,
from side, full body, (brown black white fur)
We can observe that images generated using complex prompts,
as shown in Fig.2 (b), have better details, more accurate outlines,
and precise common sense (such as the cat’s body is natural and
non-distorted) compared to images generated using simple prompts
like "a beautiful cat".
(Simple prompt example ) a beautiful cat
Inputting complex prompts is equivalent to directly injecting
the details and understanding between "beautiful" and "cat" into
the text encoder, allowing diffusion models to generate a pleasing
"beautiful cat". This indicates that diffusion models have the poten-
tial to generate semantically meaningful images, but are limited by
the text encoder’s commonsense reasoning ability. Simple prompts
do not allow the text encoder to directly understand the meaning
of "beautiful cat" well, nor can it deduce the meaning of "beautiful"
from the encoder’s own knowledge. Facing such a problem, recent
advances in large language models (LLMs) such as ChatGPT and
LLaMA [ 45] have shown astonishing conversational capabilities,
with improved SUR abilities, creating new heights in the field of nat-
ural language processing (NLP). Therefore, we made an attempt to
describe "a beautiful cat" using ChatGPT and obtained the following
text:
(Commonsense reasoning of LLM ) Cats are known for
their captivating beauty, with their soft fur, expressive eyes,
and graceful movements. A beautiful cat might have distinc-
tive features such as a sleek coat with unique patterns, pierc-
ing eyes, and an elegant posture. Each cat is unique in its
own way, and their beauty is subjective to the beholder’s
perspective.
This text demonstrates the LLMs understanding of "beautiful" and
"cat", as well as its commonsense reasoning on what kind of "cat" is
considered "beautiful". The image produced by this text is similar
in quality to images generated using complex prompts, as shown
in Fig.2 (b) bottom right.
All of the case studies above inspire us to consider whether
we can transfer the SUR abilities of LLMs to pre-trained diffusion
models so that diffusion models can produce semantically correct
and high-quality images even with simple narrative prompts.
To achieve this goal, in this paper, we first collect and annotate
a new dataset named SURD, which consists of more than 57,000 se-
mantically corrected image-text pairs. Each image-text pair contains
a simple narrative prompt, a complex keyword-based prompt, and a
high-quality image. Leveraging SURD, we propose the SUR-adapter
to transfer the SUR abilities of LLMs to pre-trained diffusion modelsand align the representations of simple and complex prompts. Ex-
tensive experiments and statistical tests confirm that our proposed
SUR-adapter significantly enhances the text encoder of pre-trained
diffusion models and generates high-quality images that alleviate
the mismatch between concise narrative prompts and generated
images. In summary, our contributions are threefold:
•We collect and annotate a dataset SURD, which includes over
57,000 semantically corrected image-text pairs. Each image-
text pair contains a simple prompt, a complex prompt, and a
high-quality corresponding image.
•Based on SURD, we propose SUR-adapter to effectively trans-
fer the semantic understanding and reasoning abilities of
LLMs to pre-trained diffusion models, alleviating the issue of
semantic mismatch and low-quality images generated with
simple prompts.
•We conduct extensive statistical tests and discussions on
the generated images using the proposed SUR-adapter to
analyze its effectiveness and further discuss its limitations.
2 RELATED WORKS
2.1 Text-to-Image Diffusion
Diffusion models have been extensively utilized in text-to-image
generation [ 2,11,23,25,38,41,46]. Text-to-image diffusion uti-
lizes textual input as a conditioning signal for diffusion models,
generating text-related images via a process of noise addition and
removal [ 38]. The text encoder of text-to-image diffusion is often
accomplished by leveraging pre-trained language models such as
CLIP [ 34] to encode textual inputs into latent vectors. Text-to-image
diffusion is widely used in various fields, such as image super-
resolution [ 27,42], inpainting [ 32], manipulation [ 5,54], semantic
segmentation [4, 12], video generation [51, 56], etc.
2.2 Large Language Models
Recently, the NLP field has witnessed a proliferation of LLMs [ 17].
Jozefowicz et al. [ 21] achieved state-of-the-art results on the Billion
Word benchmark by scaling LSTMs to 1 billion parameters. Subse-
quently, scaling transformers led to improvements on many NLP
tasks, with notable models including BERT [ 10], GPT-2 [ 35], Mega-
tronLM [ 43], and T5 [ 37]. The introduction of GPT-3 [ 6], a model
with 175 billion parameters, marked a significant breakthrough in
this area and led to the development of numerous LLMs, such as
Jurassic-1 [ 29], Megatron-Turing NLG [ 44], Gopher [ 36], Chinchilla
[17], PaLM [ 9], OPT [ 57], GLM [ 52] and LLAMA [ 45]. Furthermore,
several studies [ 15,17,22,40,49] have investigated the impact of
scaling on LLM performance to enhance their ease of use.
3 SEMANTIC UNDERSTANDING AND
REASONING DATASET
SURD is a multi-modal dataset comprising 57,603 triplets of simple
narrative prompts, complex keyword-based prompts, and semanti-
cally correct images, as shown in Fig. 3. To our knowledge, SURD
is the first dataset that records both simple and complex prompts
and focuses on providing semantically correct image-text pairs to
aid in solving the SUR problem of text-to-image diffusion models,

--- PAGE 4 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
Complex pr ompt : close up full body portrait of
cutecreature style T iny cute isometric alebrije emoji,
soft smooth lighting, with bold flat colors, 3d icon
clay render , 100mm lens, 3d blender render ,natural
environment scenery , Mexican inspired background,
trending on polycount, modular constructivism,
physically based rendering, geometric, centered,
dynamic pose. The alebrije should be inspired by
Mexican folk art and should be a combination of
several dif ferent animals, such as a lion, eagle, or
serpent., soloSimple pr ompt : a colorful animal with big eyes on
a blue background ImageBLIP
Diffusion models
Figure 3: An example of SURD. We collect a diverse set of
complex prompts and corresponding images generated by
diffusion models from publicly available websites and lever-
age pre-trained BLIP to generate simple prompts.
which allows diffusion models to generate high-quality images that
are semantically consistent based on simple prompts alone.
3.1 Data Collection
Raw Data. To construct a content-rich and semantically reliable
dataset, we extensively investigate various open-source image gen-
eration websites with reliable prompts and high-quality images.
Among them, we select three websites: Lexica1, civitai2, and Stable
Diffusion Online3. On these websites, publicly available images
are often semantically correct and of high quality with complex
prompts. Therefore, we collect the prompts from websites as com-
plex prompts. In total, we collect 114,148 image-text pairs.
Data Cleaning . In order to ensure the correct semantic match of
each sample in the SURD, we perform data cleaning in two steps. In
the first step, to ensure the semantic accuracy of the simple prompts
generated by BLIP [ 13], we use the publicly available pre-trained
model CLIP [ 34] for semantic cleaning since the text encoder in
most of diffusion models is the text encoder of the CLIP model,
which will be explained in Section 4.1. If the CLIP model judges
the semantic of a simple prompt that matches the semantic of
the corresponding image, diffusion models are likely to be able
to generate similar images according to the simple prompt. For
each image, we ask CLIP to classify between its simple prompt
and its complex prompt for selecting a prompt matching the image
best semantically. In general, a complex prompt often contains
other semantically irrelevant information, such as image quality
descriptions, so a semantically correct simple prompt generally has
a higher CLIP score than the complex prompt. Therefore, if the
CLIP score of a simple prompt is not lower than the corresponding
complex prompt, we retain the sample. After the automatic semantic
cleaning based on the CLIP score, we retain 66,408 samples. In
the second step, we further filter the samples retained in the first
step manually to ensure that all image-text pairs are semantically
matched. Finally, SURD contains 57,603 image-text pairs where
each image-text pair contains an image, a simple prompt, and a
complex prompt.
1https://lexica.art,2https://civitai.com,3https://stablediffusionweb.comKnowledge from LLM. Since we hope to distill knowledge from
LLM to improve the semantic understanding and reasoning capaci-
ties of a text encoder, we also save the knowledge of simple prompts
from LLM in vectors. Specifically, we use the recently open-sourced
large language model LLaMA [ 45] with three different parameter
sizes: 7B (32 layers, dimension is 4096), 13B (40 layers, dimension
is 5120), and 33B (60 layers, dimension is 6656). For each simple
prompt, we compute the mean value of each token embeddings
generated by the LLM as the knowledge representation so that we
can handle different samples with different lengths uniformly.
In addition, we resize all images to 512 ×512 uniformly. Further
details regarding the usage of BLIP, CLIP, and LLM can be found in
the appendix.
3.2 Data Analysis
Prompt Length. Fig. 4 shows the distribution of sentence length for
prompts, with (a) representing the distribution for complex prompts
and (b) representing the distribution for simple prompts. In order
to enhance visual clarity, prompts longer than 300 words have
been incorporated into 300 words. The length distribution of simple
prompts is relatively concentrated, with sentence lengths centered
around 10, which is consistent with human language patterns. In
contrast, complex prompts, with a long tail distribution, not only
contain semantics but also include definitions and image quality
information, resulting in sentence lengths that are significantly
longer than simple prompts.
Prompt Content. A prompt for text-to-image generation usually
contains a significant number of nouns which could influence the
quality and semantic coherence of the generated image greatly
since an image consists of different objects. Therefore, we con-
duct a statistical analysis of the frequency distribution of nouns
occurred in the SURD to demonstrate the diversity of both text
and visual content. Fig. 4 (c) displays the frequency-proportional
distribution of selected entities from SURD. These entities cover a
diverse range of ordinary objects, such as people, animals, plants,
and scenes, indicating the content diversity of SURD. Besides, the
diversity of these entities can make pre-trained diffusion models
have strong high-level understanding capacities of text and visual
content in more complex scenes. Furthermore, we also present a
word cloud of the text as shown in Fig. 4 (d) by filtering out stop
words to illustrate the overall distribution of text vocabulary in
SURD. The most frequently occurred phrases, such as "best quality",
"masterpiece best", and "extremely detailed", primarily constrain
the image quality and originate from complex prompts, indicating
that these consistent text constraints are important for high-quality
image generation. Therefore, the semantic representation of com-
plex prompts will play a crucial role in enhancing the diffusion
models with SUR-adapter through finetuning.
4 METHOD
In this section, we introduce how SUR-adapter transfers the se-
mantic understanding and reasoning capabilities of large language
models and achieves the representation alignment between complex
prompts and simple prompts.

--- PAGE 5 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models Preprint, Technical Report,
(a)
(b)(c)
(d)
Figure 4: (Left) Prompt length distributions and (Right) prompt content distributions.
Simple Prompt
Image Noise+
Pre-trained 
Diffusion Model
Text Encoder
PredictorComplex PromptLarge Language Model
(LLM)
Adapter
Noise+
Frozen
LearnableForward
DistillationTransformer
FCNLLM
Text Encoder
×
Attention++Adapter
Figure 5: Illustration of SUR-adapter. FCN is a fully-
connected network. (Left) The fine-tuning pipeline for pre-
trained diffusion models. Given a pre-trained diffusion
model, the adapter is used to transfer the semantic under-
standing and reasoning capabilities of large language mod-
els and align the representation between complex and sim-
ple prompts. The weight coefficient, 𝜂, is used to adjust the
adapter’s effect. (Right) The network structure of the adapter.
4.1 Preliminary
Diffusion models are excellent methods for multi-modal image
generation, which typically involve two stages: (1) Forward noise
process. Assuming that the training data x0comes from a given
distribution 𝑝(x0), the diffusion model first obtains a sequence
x1,x2,...,x𝑇by adding𝑇rounds of noise to x0, as follows:
𝑞(x𝑡|x0)=𝑁(x𝑡;𝛼𝑡x0,𝜎2
𝑡I), (1)
where𝜖is sampled from the standard normal distribution 𝑁(0,I),
𝜎2
𝑡is a given noise strength that depends on 𝑡, and𝛼𝑡is generally set
to𝛼𝑡=√︃
1−𝜎2
𝑡. At this point, we have x𝑡=𝛼𝑡x0+𝜎𝑡𝜖.(2) Reverse
denoising process. After obtaining the sequence x1,x2,...,x𝑇from
the forward noise process, the denoising process from x𝑡tox𝑡−1
can be modeled by
𝑝𝜃(x𝑡−1|x𝑡)=𝑁(x𝑡−1;ˆ𝜇𝜃(x𝑡),ˆΣ𝜃(x𝑡)), (2)
where ˆ𝜇𝜃(x𝑡)andˆΣ𝜃(x𝑡))are the predicted statistics, and 𝜃is the
learnable parameter. Many recent works [ 16,38,53] have shownthat Eq.(2) can be efficiently optimized via the following loss func-
tion:
ℓ𝑡
simple(𝜃)=E∥𝜖−ˆ𝜖𝜃(𝛼𝑡x0+𝜎𝑡𝜖,𝑡)∥2
2, (3)
where ˆ𝜖𝜃(·)is a learnable neural network that predicts the added
noise𝜖in the input x𝑡. When this neural network is well-trained,
we can use x𝑡=𝛼𝑡x0+𝜎𝑡𝜖and some certain sampling methods
to infer x0. Note that as 𝑇→∞ or becomes sufficiently large,
x𝑇can be viewed as an approximation of a normally distributed
noise. Therefore, we can randomly sample noise 𝜖0from a normal
distribution and use the neural network ˆ𝜖𝜃(·), also known as the
predictor (as shown in Fig. 5), to generate an image ˆx0. To achieve
controllable generation, a condition 𝑐can be added to the predictor,
i.e., rewriting the predictor as ˆ𝜖𝜃(·,𝑐). For text-to-image generation
tasks, the condition 𝑐is usually generated from a text prompt by a
text encoder, such as the text encoder of CLIP.
Algorithm 1 The Algorithm of Fine-tuning Pre-trained Diffusion
Model with SUR-adapter.
1:Input: The dataset SURD (𝑝𝑖𝑐,𝑝𝑖𝑠,𝐼𝑖)𝑁
𝑖=1, a learnable transformation
𝑔(·;𝜙1)and Adapter 𝑔Ada(·;𝜙2); Large language model 𝑓LLMand the
text encoder 𝑓Enwith fixed parameters. Training step 𝑇0.
2:while The training step 𝑇0≥0do
3: // Knowledge distillation from LLM
4: Calculate the knowledge distillation loss ℓLLMby Eq.(5)
5: Measure the semantic information 𝑐′
LLMby Eq.(7)
6: // Performance maintenance
7: Add noise to 𝐼𝑖to obtain𝛼𝑡𝐼𝑖+𝜎𝑡𝜖by Eq.(1)
8: Use𝑐′
LLMto measureℓ𝑡
simple(𝜙)by Eq.(8)
9: // Representation alignment
10: Measure𝑓𝐸𝑛(𝑝𝑖𝑐)by complex prompt 𝑝𝑖𝑐
11: Use𝑐′
LLMand𝑓𝐸𝑛(𝑝𝑖𝑐)to measureℓCP(𝜙)by Eq.(9)
12: // Update the parameters
13: Calculate the total loss ℓtotal(𝜙)by Eq.(10)
14: Update the learnable parameters 𝜙=[𝜙1,𝜙2]byℓtotal(𝜙)
15:𝑇0←𝑇0−1
16:end while
17:return𝜙
4.2 The Fine-tuning Pipeline of SUR-adapter
In this section, we introduce our simple yet effective fine-tuning
approach called the semantic understanding and reasoning adapter

--- PAGE 6 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
Table 2: Evaluation results of the diverse pre-trained models and controlled methods described in Section 5.1 in terms of various
semantic metrics.
Pre-trained
ModelControlled MethodCLIP Score Action (%) Color (%) Counting (%)
Baseline Ours Baseline Ours Baseline Ours Baseline Ours
DM (1.5),
LLM (13B)- 0.498 0.517 ↑ 75.33 80.67↑ 81.33 87.33↑ 14.67 36.67↑
ControlNet (canny) 0.508 0.492 ↓ 76.67 84.67↑ 68.67 69.33↑ 96.00 94.00↓
ControlNet (seg) 0.481 0.472 ↓ 7.33 9.33↑ 10.00 10.67↑ 40.67 62.00↑
Prompt Weighting 0.486 0.514 ↑ 78.00 85.33↑ 91.33 88.00↓ 43.33 58.00↑
MultiDiffusion 0.470 0.516 ↑ 74.67 88.67↑ 87.33 81.33↓ 23.33 62.67↑
Self-attention Guidance 0.474 0.526 ↑ 73.33 86.00↑ 86.00 86.67↑ 12.67 14.00↑
DM (cartoon),
LLM (13B)- 0.467 0.490 ↑ 58.00 68.67↑ 82.00 88.00↑ 21.33 38.00↑
ControlNet (canny) 0.514 0.486 ↓ 83.33 81.33↓ 47.33 67.33↑ 74.00 86.67↑
ControlNet (seg) 0.509 0.491 ↓ 38.67 51.33↑ 28.00 30.67↑ 45.33 62.00↑
Prompt Weighting 0.554 0.546 ↓ 84.00 79.33↓ 88.67 91.33↑ 41.33 50.00↑
MultiDiffusion 0.413 0.587 ↑ 63.33 80.67↑ 88.00 87.33↓ 18.67 36.67↑
Self-attention Guidance 0.440 0.560 ↑ 65.33 73.33↑ 72.67 86.00↑ 16.67 39.33↑
(SUR-adapter) for the controllable text-to-image diffusion model.
Let us consider the image-text pairs (𝑝𝑖𝑐,𝑝𝑖𝑠,𝐼𝑖)𝑁
𝑖=1in the SURD
dataset, where 𝑝𝑖𝑐and𝑝𝑖𝑠are the complex and simple prompts, re-
spectively, for the 𝑖-th high-quality and semantically correct image
𝐼𝑖. As shown in Fig.5 (Left), we first freeze all learnable parameters
of the large language model 𝑓LLM, the text encoder 𝑓𝐸𝑛, and the
predictor𝑓𝑝𝑟𝑒in the pre-trained diffusion model, and then we add
two trainable neural networks, a fully-connected network (FCN)
𝑔(·;𝜙1)and an adapter 𝑔Ada(·;𝜙2), with learnable parameters 𝜙1
and𝜙2.
4.2.1 Knowledge Distillation by LLM. The structure of the adapter
𝑔Ada(·;𝜙2)is shown in Fig. 5 (Right), and it consists of three learn-
able transformations, ℎ𝑗(·)for𝑗=1,2,3, which are implemented
using fully connected neural networks or Transformer [ 47]. For the
output𝑓𝐸𝑛(𝑝𝑖𝑠)of the text encoder, we construct 𝑄𝑖=ℎ3[𝑓𝐸𝑛(𝑝𝑖𝑠)]
and𝐾𝑖=ℎ2[𝑓𝐸𝑛(𝑝𝑖𝑠)], and calculate an attention value as [10, 47]
att𝑖=softmax(𝑄𝑖𝐾𝑇
𝑖√
𝑑), (4)
where𝑑is the feature dimension of 𝑄𝑖and𝐾𝑖. To ensure that the
semantic information of the simple prompt is not directly interfered
with, we directly set 𝑉𝑖=𝑓𝐸𝑛(𝑝𝑖𝑠)without any transformation. In
particular, to embed the powerful semantic understanding and
reasoning capabilities of the LLM in att𝑖, we distill knowledge from
LLM by the following loss function:
ℓLLM(𝜙)=KL[W0𝑓LLM(𝑝𝑖
𝑠)/𝜏,𝑄𝑖/𝜏], (5)
Here,𝜏is the temperature, which is typically set to 1, and KLis the
KL divergence. W0is a randomly initialized matrix using Kaiming
initialization and is unlearnable, which ensures that semantic in-
formation of LLM is reserved as much as possible while aligning
the dimensions between 𝑓LLM(𝑝𝑖𝑠)and𝑄. Moreover, we obtain the
calibrated semantic information as 𝑉′
𝑖=𝑉𝑖⊗att𝑖. Finally, the out-
put of the Adapter is transformed by the learnable transformation
𝑔(·;𝜙1)to obtain the output 𝑐LLMwith LLM semantic capabilities
as prior works [14, 20, 58]:
𝑔
𝑔Ada(𝑓𝐸𝑛(𝑝𝑖
𝑠);𝜙2);𝜙1	
=𝑔
𝑉′
𝑖+𝑉𝑖+ℎ1[𝑉′
𝑖+𝑉𝑖];𝜙1	
,(6)
and the semantic information input to the predictor is as follows:
𝑐′
LLM=𝜂·𝑐LLM+(1−𝜂)·𝑓𝐸𝑛(𝑝𝑖
𝑠). (7)where𝜂is a constant.
4.2.2 Performance Maintenance of DMs During Fine-tuning. To
maintain the performance of the diffusion model during fine-tuning,
we add varying levels of noise to the image 𝐼𝑖by Eq.(1), and feed
the semantic information feature 𝑐′
LLMobtained from Eq.(7) to the
predictor, guided by the simple prompt 𝑝𝑖𝑠. To ensure that the pre-
trained diffusion model maintains sufficient denoising ability for
new images 𝐼𝑖during fine-tuning, we minimize the following loss
function:
ℓ𝑡
simple(𝜙)=E∥𝜖−ˆ𝜖(𝛼𝑡𝐼𝑖+𝜎𝑡𝜖,𝑡,𝑐′
LLM)∥2
2, (8)
Furthermore, to ensure stable training of the added adapter and
reduce its adverse impact on the pre-trained diffusion model dur-
ing the early stage of training, we follow the setting of previous
works [ 19,54] by initializing all elements of the matrices in param-
eter𝜙1to 0.
4.2.3 Aligning the Representation between Complex Prompts and
Simple Prompts. From the description in Section 3, we know that
image𝐼𝑖is a semantically correct and high-quality image gener-
ated by𝑝𝑖𝑐. In order to generate images of sufficient similarity and
quality as𝐼𝑖by a simple prompt. we need to align the semantic
representation of feature between 𝑐′
LLMand𝑓𝐸𝑛(𝑝𝑖𝑐). Specifically,
we consider minimizing the following loss function:
ℓCP(𝜙)=KL(𝑐′
LLM/𝜏,𝑓𝐸𝑛(𝑝𝑖
𝑐)/𝜏), (9)
where𝜏is set as in Eq.(5) and KLdenotes the KL divergence [26].
In summary, the final loss function for SUR-adapter training is
as follows:
ℓtotal(𝜙)=𝜆1·ℓLLM(𝜙)+𝜆2·ℓCP(𝜙)+ℓ𝑡
simple(𝜙), (10)
where𝜆𝑖≤1,𝑖=1,2are loss coefficients. We present the training
process of SUR-adapter in Algorithm 1. After training, the fine-
tuned diffusion model can generate images using the same sampling
method as before.

--- PAGE 7 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models Preprint, Technical Report,
Table 3: Evaluation results of the diverse pre-trained models and controlled methods described in Section 5.1 in terms of various
quality metrics. We calculate the T-test for the means of two independent samples of scores, and if the resulting P-value is
greater than 0.05, it implies that there is no significant difference between the NR scores of the baselines and SUR-adapter,
indicating that their generation quality is comparable.
Pre-trained
ModelControlled MethodBRISQUE CLIP-IQA MUSIQ User Preference (%)
Baseline Ours (P > 0.05?) Baseline Ours (P > 0.05?) Baseline Ours (P > 0.05?) Baseline Ours
DM (1.5),
LLM (13B)- 13.85 14.78 ( ✓) 0.686 0.688 ( ✓) 67.38 67.04 ( ✓) 48.31 51.69
ControlNet (canny) 22.68 25.15 ( ×) 0.673 0.668 ( ✓) 67.41 67.14 ( ✓) 49.81 50.19
ControlNet (seg) 39.86 42.12 ( ✓) 0.662 0.668 ( ✓) 64.12 65.71 ( ×) 53.56 46.44
Prompt Weighting 13.29 13.74 ( ✓) 0.681 0.691 ( ✓) 66.97 67.02 ( ✓) 47.94 52.06
MultiDiffusion 10.84 11.83 ( ✓) 0.696 0.691 ( ✓) 66.60 67.95 ( ✓) 52.06 47.94
Self-attention Guidance 15.08 17.06 ( ✓) 0.694 0.706 ( ✓) 67.51 68.97 ( ✓) 48.31 51.69
DM (cartoon),
LLM (13B)- 15.74 19.53 ( ×) 0.699 0.707 ( ✓) 66.07 67.03 ( ✓) 50.94 49.06
ControlNet (canny) 18.68 18.49 ( ✓) 0.697 0.696 ( ✓) 67.98 67.95 ( ✓) 50.56 49.44
ControlNet (seg) 35.84 31.96 ( ×) 0.710 0.701 ( ✓) 67.51 67.68 ( ✓) 51.69 48.31
Prompt Weighting 17.62 19.12 ( ✓) 0.714 0.698 ( ✓) 67.38 66.46 ( ✓) 51.31 48.69
MultiDiffusion 14.88 15.96 ( ✓) 0.709 0.711 ( ✓) 68.05 67.26 ( ✓) 47.94 52.06
Self-attention Guidance 20.44 20.98 ( ✓) 0.705 0.706 ( ✓) 67.74 66.90 ( ✓) 52.43 47.57
Table 4: The performance of diffusion models under various LLM settings. Bold and underline indicate the optimal and
suboptimal performance, respectively.
Pre-trained
ModelLLM Layer or
Controlled MethodCLIP Score Action (%) Color (%) Counting (%) BRISQUE CLIP-IQA MUSIQ
DM (1.5),
LLM (13B)1 0.414 68.00 82.00 32.67 13.89 0.688 67.04
10 0.502 74.00 84.67 34.00 15.28 0.694 68.04
20 0.496 78.00 81.33 30.00 15.77 0.690 67.27
30 0.482 72.67 90.00 31.33 17.85 0.691 67.25
40 0.517 80.67 87.33 36.67 14.78 0.684 67.47
DM (cartoon),
LLM (13B)1 0.387 70.00 79.33 26.67 15.94 0.707 67.04
10 0.434 72.67 82.67 34.67 17.31 0.703 66.02
20 0.493 76.00 87.33 31.33 16.20 0.707 67.03
30 0.533 78.00 91.33 38.00 17.58 0.707 66.50
40 0.490 68.67 88.00 38.00 19.53 0.695 66.04
DM (1.5),
LLM (7B)- 0.494 80.67 85.33 35.33 12.96 0.688 67.33
ControlNet (canny) 0.476 82.67 68.67 88.00 22.80 0.675 67.30
ControlNet (seg) 0.519 8.00 8.67 60.67 39.11 0.670 65.54
Prompt Weighting 0.601 84.00 83.33 53.33 14.53 0.688 67.09
MultiDiffusion 0.399 92.00 88.00 63.33 14.70 0.691 67.85
Self-attention Guidance 0.514 80.67 85.33 18.00 17.76 0.694 67.15
DM (1.5),
LLM (33B)- 0.523 82.00 88.67 38.67 14.38 0.690 67.66
ControlNet (canny) 0.482 84.67 70.00 94.67 26.94 0.671 67.74
ControlNet (seg) 0.505 7.33 8.00 64.00 39.39 0.673 65.54
Prompt Weighting 0.530 84.67 92.67 58.67 13.96 0.702 67.38
MultiDiffusion 0.496 87.33 88.67 61.33 13.84 0.705 67.92
Self-attention Guidance 0.517 86.00 89.33 20.67 15.48 0.706 67.95
5 EXPERIMENTS
5.1 Implementation Details
We utilize two pre-trained diffusion models (DMs) and three LLMs
[45] with different parameters. DM (1.5) [ 38] specialized in high-
resolution image synthesis and DM (cartoon)2trained on modern
anime feature film images. LLM ( 𝑠) means the LLaMa model with
the parameter size of 𝑠. In addition, we validate the universality of
SUR-adapter with various controlled methods. ControlNet [ 54] is
an auxiliary network that introduces an additional condition. Our
experiments include 2 canonical pre-trained ControlNets, namely
edge detection with ControlNet (canny) and semantic segmenta-
tions with ControlNet (seg). Prompt weighting3is a straightforward
technique that assigns higher attention weights to specific parts of
the text input. MultiDiffusion [ 3] defines a novel generation process
2https://huggingface.co/nitrosocke/Ghibli-Diffusion
3https://github.com/damian0815/compelon top of a pre-trained diffusion model, which merges multiple dif-
fusion generation methods. Self-attention Guidance [ 18] provides
direction from predictions that are not reliant on high-frequency
details to fully conditioned images. The high-frequency details are
extracted from the UNet self-attention maps.
We use the SURD dataset to evaluate models by two types of
metrics: semantic and quality. It is worth noting that all metrics
are positively oriented. For semantic evaluation , we design three
types of prompts [ 1,7,8,31], namely Action, Color, and Counting,
each with fifteen prompts. These prompts are used to evaluate
the semantic capabilities of the baselines and SUR-adapter. Action,
Color, and Counting are all percentage metrics that indicate the
proportion of images that meet the different types of semantics.
During testing, we generate ten images for each prompt. To further
evaluate the semantic quality, we also use the CLIP Score [ 34]. We
use CLIP to construct the binary classification problem for both

--- PAGE 8 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
the baselines and SUR-adapter and select the most appropriate
images based on the prompts. After applying Softmax to avoid the
effects of extreme values, we record the scores of the baselines and
SUR-adapter, and use the mean value on the test set as the final
CLIP score of the diffusion models. For quality evaluation , we use
BRISQUE [ 33], CLIP-IQA [ 48], MUSIQ [ 24], and user preference
study. The user preference study consists of single-choice questions
where users choose the image with the best quality from a pair of
images generated by the baselines and SUR-adapter. We collected 89
valid questionnaires from the user preference study. In the appendix,
we provide detailed training recipes.
5.2 Experiment Analysis
Table 2 shows the semantic capabilities of both baselines and
SUR-adapter. Notably, the results demonstrate that SUR-adapter
can effectively enhance the SUR performance of the baselines in
most cases. Furthermore, we can draw the following conclusions:
(a) the use of Softmax to obtain a relative score in CLIP can render
the CLIP Score unreliable, particularly when both the baselines and
SUR-adapter yield equally poor results. For instance, ControlNet
(seg) attains a relatively high score despite its subpar generation
effects on Action and Color. (b) ControlNet performs well in Count-
ing scores since it utilizes image outlines with the correct amount of
information as a reference. (c) Inaccurate image segmentation can
cause diffusion models with ControlNet (seg) to disregard semantic
information and generate entirely blurry images, thus resulting in
unsatisfactory generation effects on Action and Color. Nonethe-
less, the negative impact of ControlNet (seg) can be alleviated by
SUR-adapter. (d) The SUR capability of pre-trained diffusion models
can be improved by employing Prompt Weighting and MultiDif-
fusion, with further enhancement achievable through the use of
SUR-adapter.
As shown in Fig. 5, while the extra added adapter helps enhance
the semantic understanding and reasoning abilities of diffusion
models, adding additional parameters does not guarantee the preser-
vation of the original image generation quality of the pre-trained
diffusion models, as the adapter and pre-trained models are not
trained simultaneously. However, our proposed SURD can be used
to mitigate this issue by supplying high-quality images. In Table
3, we demonstrate through multiple image quality metrics with
T-tests and user preference study that SUR-adapter can maintain
image generation quality, meaning there is no significant differ-
ence between the image quality of SUR-adapter and the original
pre-trained diffusion model (P-value ≥0.05). Moreover, since these
high-quality images of SURD also come from diffusion models, they
do not lead to the generation of images of higher quality than those
generated by the pre-trained diffusion models in our method.
6 ABLATION STUDY
The Analysis of LLMs. As introduced in Section 3.2, LLM (13B)
has 40 layers. The performance of LLM vectors with different layers
is shown in the first two rows of Table 4. We find that in most
cases, LLM vectors corresponding to the later layers are better.
This suggests that the high-level semantic features in the deeper
layers are more conducive to semantic distillation. Additionally,
we show in the last two rows of Table 4 the performance of LLMs
with different parameter sizes. Combining the analysis of Table 4,2, and 3, we find that there is no significant difference in diffusion
model performance among LLMs with different parameter sizes.
Although existing work suggests that models with larger parameter
sizes have stronger SUR abilities, existing SUR-adapter may only
be able to transfer limited semantic knowledge from LLMs.
The Knowledge Distillation of SUR-adapter. As shown in Table
5, we conduct ablation studies on the knowledge distillation of LLM
represented by the green line and complex prompts represented by
the purple line in Fig. 5. Distilling the knowledge of LLM or complex
prompts alone improves the SUR capability of SUR-adapter, and the
effect of knowledge distillation based on LLM is stronger than that
based on complex prompts. Furthermore, distilling the knowledge
of both can further enhance the performance of SUR-adapter.
Table 5: Ablation study on the knowledge distillation of SUR-
adapter.
LLMComplex
PromptsBRISQUE Action (%) Color (%) Number (%)
13.85 75.33 81.33 14.67
✓ 13.97 78.67 84.00 34.67
✓ 12.31 74.00 86.67 32.00
✓ ✓ 14.78 80.67 87.33 36.67
7 LIMITATIONS
As shown in Table 2, SUR-adapter has limited capacity to improve
diffusion models and cannot completely address the SUR issue. For
instance, after improvement, the Counting of DM (1.5), LLM (13B)
is only increased by 36.67%. However, addressing the deficiency of
SUR may require a large-scale multimodal dataset to optimize the
text encoder of diffusion models, which is a costly and challenging
task. Moreover, as highlighted in Section 6, there is no significant
difference in performance among LLMs of different parameter sizes
after distilling, indicating that SUR-adapter can only transfer limited
semantic knowledge from LLMs due to factors such as parameter
limitations. Hence, further enhancements are necessary for SUR-
adapter to more effectively distill semantic information from LLMs.
8 CONCLUSION
In this paper, we uncover the limitations of existing pre-trained
diffusion models in terms of their ability to comprehend seman-
tics and engage in commonsense reasoning when presented with
simple narrative prompts as inputs, leading to suboptimal image
generation. To mitigate this issue, we introduce a new dataset called
SURD, which comprises over 57,000 semantically corrected image-
text pairs, and the SUR-adapter module that can distill semantic
understanding and reasoning knowledge from complex keyword-
based prompts and large language models. Extensive experiments
and rigorous evaluations conducted on SURD demonstrate that
SUR-adapter can enhance the semantic understanding of diffusion
models without compromising image generation quality.
ACKNOWLEDGMENTS
This work was supported in part by National Natural Science
Foundation of China (NSFC) under Grant No.62206314 and Grant
No.U1711264, GuangDong Basic and Applied Basic Research Foun-
dation under Grant No.2022A1515011835, China Postdoctoral Sci-
ence Foundation funded project under Grant No.2021M703687.

--- PAGE 9 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models Preprint, Technical Report,
REFERENCES
[1]Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence
Zitnick, Devi Parikh, and Dhruv Batra. 2015. VQA: Visual Question Answering.
International Journal of Computer Vision 123 (2015), 4–31.
[2]Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shiliang Pu, Yaole Wang, Gang
Yue, Yue Cao, Hang Su, and Jun Zhu. 2023. One Transformer Fits All Distributions
in Multi-Modal Diffusion at Scale. ArXiv abs/2303.06555 (2023).
[3]Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. MultiDiffu-
sion: Fusing Diffusion Paths for Controlled Image Generation. arXiv preprint
arXiv:2302.08113 2 (2023).
[4]Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and
Artem Babenko. 2021. Label-Efficient Semantic Segmentation with Diffusion
Models. ArXiv abs/2112.03126 (2021).
[5]Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, and Christian Etmann.
2021. Conditional Image Generation with Score-Based Diffusion Models.
[6]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language Models are Few-Shot Learners. ArXiv
abs/2005.14165 (2020).
[7]Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.
2020. Counterfactual Samples Synthesizing for Robust Visual Question Answer-
ing.2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(2020), 10797–10806.
[8]Long Chen, Yuhang Zheng, and Jun Xiao. 2022. Rethinking Data Augmentation
for Robust Visual Question Answering. ArXiv abs/2207.08739 (2022).
[9]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar
Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ip-
polito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-
malayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
PaLM: Scaling Language Modeling with Pathways. ArXiv abs/2204.02311 (2022).
[10] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. (2018).
[11] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-
Chiang Frank Wang. 2022. Frido: Feature Pyramid Diffusion for Complex Scene
Image Synthesis. ArXiv abs/2208.13753 (2022).
[12] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. 2022.
Diffusion models as plug-and-play priors. ArXiv abs/2206.09012 (2022).
[13] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li,
Dacheng Tao, and Steven CH Hoi. 2022. From Images to Textual Prompts: Zero-
shot VQA with Frozen Large Language Models. arXiv preprint arXiv:2212.10846
(2022).
[14] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning
for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2015), 770–778.
[15] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos,
Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and
Yanqi Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. ArXiv
abs/1712.00409 (2017).
[16] Jonathan Ho, Ajay Jain, and P. Abbeel. 2020. Denoising Diffusion Probabilistic
Models. ArXiv abs/2006.11239 (2020).
[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. 2022. Training Compute-Optimal
Large Language Models. ArXiv abs/2203.15556 (2022).
[18] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. 2022. Improv-
ing Sample Quality of Diffusion Models Using Self-Attention Guidance. arXiv
preprint arXiv:2210.00939 (2022).
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large
language models. arXiv preprint arXiv:2106.09685 (2021).
[20] Zhongzhan Huang, Senwei Liang, Mingfu Liang, and Haizhao Yang. 2019. DI-
ANet: Dense-and-Implicit Attention Network. In AAAI Conference on ArtificialIntelligence .
[21] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. 2016. Exploring
the Limits of Language Modeling.
[22] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020. Scaling
Laws for Neural Language Models. ArXiv abs/2001.08361 (2020).
[23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-Tang Chang, Tali Dekel,
Inbar Mosseri, and Michal Irani. 2022. Imagic: Text-Based Real Image Editing
with Diffusion Models. ArXiv abs/2210.09276 (2022).
[24] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. Musiq:
Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International
Conference on Computer Vision . 5148–5157.
[25] Gwanghyun Kim, Taesung Kwon, and Jong-Chul Ye. 2021. DiffusionCLIP: Text-
Guided Diffusion Models for Robust Image Manipulation. 2022 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) (2021), 2416–2425.
[26] Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency.
The annals of mathematical statistics 22, 1 (1951), 79–86.
[27] Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhi hai Xu, Qi Li, and Yue ting
Chen. 2021. SRDiff: Single Image Super-Resolution with Diffusion Probabilistic
Models. Neurocomputing 479 (2021), 47–59.
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping
language-image pre-training for unified vision-language understanding and
generation. In International Conference on Machine Learning . PMLR, 12888–12900.
[29] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical
details and evaluation. White Paper. AI21 Labs 1 (2021).
[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common
Objects in Context. In European Conference on Computer Vision .
[31] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Fang Yang, and Xiao-Ming Wu. 2021.
Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual
Question Answering. 2021 IEEE 18th International Symposium on Biomedical
Imaging (ISBI) (2021), 1650–1654.
[32] Andreas Lugmayr, Martin Danelljan, Andrés Romero, Fisher Yu, Radu Timo-
fte, and Luc Van Gool. 2022. RePaint: Inpainting using Denoising Diffusion
Probabilistic Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2022), 11451–11461.
[33] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012. No-
reference image quality assessment in the spatial domain. IEEE Transactions on
image processing 21, 12 (2012), 4695–4708.
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning . PMLR, 8748–8763.
[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
[36] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy
Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme
Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-
raine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic
Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias
Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tay-
fun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hecht-
man, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon
Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stan-
way, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
2021. Scaling Language Models: Methods, Analysis & Insights from Training
Gopher. ArXiv abs/2112.11446 (2021).
[37] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of
Transfer Learning with a Unified Text-to-Text Transformer. ArXiv abs/1910.10683
(2019).
[38] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
2021. High-Resolution Image Synthesis with Latent Diffusion Models. 2022
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021),
10674–10685.
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) . 10684–10695.
[40] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019.
A Constructive Prediction of the Generalization Error Across Scales. ArXiv

--- PAGE 10 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
abs/1909.12673 (2019).
[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
Kfir Aberman. 2022. DreamBooth: Fine Tuning Text-to-Image Diffusion Models
for Subject-Driven Generation. ArXiv abs/2208.12242 (2022).
[42] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and
Mohammad Norouzi. 2021. Image Super-Resolution via Iterative Refinement.
IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (2021), 4713–
4726.
[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter
Language Models Using Model Parallelism. ArXiv abs/1909.08053 (2019).
[44] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi,
Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston,
Saurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to
Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.
ArXiv abs/2201.11990 (2022).
[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[46] Dani Valevski, Matan Kalman, Y. Matias, and Yaniv Leviathan. 2022. UniTune:
Text-Driven Image Editing by Fine Tuning an Image Generation Model on a
Single Image. ArXiv abs/2210.09477 (2022).
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[48] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2022. Exploring CLIP for
Assessing the Look and Feel of Images. arXiv preprint arXiv:2207.12396 (2022).
[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huaihsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
Fedus. 2022. Emergent Abilities of Large Language Models. ArXiv abs/2206.07682
(2022).
[50] Ling Yang, Zhilong Zhang, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia
Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. 2022. Diffusion Models:
A Comprehensive Survey of Methods and Applications. ArXiv abs/2209.00796
(2022).
[51] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. 2022. Diffusion Probabilis-
tic Modeling for Video Generation. ArXiv abs/2203.09481 (2022).
[52] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,
Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,
Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang.
2022. GLM-130B: An Open Bilingual Pre-trained Model. ArXiv abs/2210.02414
(2022).
[53] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In-So Kweon. 2023.
Text-to-image Diffusion Models in Generative AI: A Survey. ArXiv abs/2303.07909
(2023).
[54] Lvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to
Text-to-Image Diffusion Models. ArXiv abs/2302.05543 (2023).
[55] Lvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to
Text-to-Image Diffusion Models. arXiv:2302.05543 [cs.CV]
[56] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo,
Lei Yang, and Ziwei Liu. 2022. MotionDiffuse: Text-Driven Human Motion
Generation with Diffusion Model. ArXiv abs/2208.15001 (2022).
[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained
Transformer Language Models. ArXiv abs/2205.01068 (2022).
[58] Shan Zhong, Wushao Wen, and Jinghui Qin. 2023. SPEM: Self-adaptive Pooling
Enhanced Attention Module for Image Recognition. In Conference on Multimedia
Modeling .

--- PAGE 11 ---
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models Preprint, Technical Report,
A SUPPLEMENTAL DATASET INFORMATION
A.1 Pre-trained Models
BLIP. We utilize the BLIP (Bootstrapping Language-Image Pre-
training) [ 13,28] model to generate simple narrative prompts of
images for SURD. Specifically, we employ the BLIP caption base
model, which has been fine-tuned on the MSCOCO [ 30] captioning
dataset, using the load function provided in the official documenta-
tion4.
CLIP. We utilize CLIP to ensure the correctness of both simple
narrative prompts and complex keyword-based prompts. Specif-
ically, we designed a data cleaning process, which is briefly de-
scribed in Section 3.1 of the main text. We leverage the semantic
similarity between images and prompts by asking CLIP to classify
between simple and complex prompts, where the goal is to select
the prompts that best match the semantics of the images. Typically,
complex prompts contain semantically irrelevant information, such
as image quality descriptions, and therefore, semantically correct
simple prompts generally achieve higher CLIP scores than complex
prompts. We retain a sample if the CLIP score of the corresponding
simple prompt is not lower than that of the complex prompt. We
use the publicly available pre-trained CLIP model, which has a
ViT-B/32 architecture, and load it using the function provided in
the official documentation5.
LLMs. In this paper, we utilize LLaMA [ 45], a collection of foun-
dation language models ranging from 7B to 65B parameters, as
knowledge distillation for large language models (LLMs). Specifi-
cally, we save the vector representations of simple prompts in LLMs,
which serve as the text understanding to finetune diffusion models.
The details of the LLMs used in our experiments, including the
number of parameters, vector dimensions, and model structures,
are shown in Table 6.
Table 6: Model sizes and architectures of LLMs used in the
main text.
LLM params dimension n heads n layers
7B 6.7B 4096 32 32
13B 13.0B 5120 40 40
33B 32.5B 6656 52 60
A.2 Impact and Ethics
Impact and Usage. Improving the SUR ability of diffusion mod-
els is an important issue that has received limited attention in the
research community. In this paper, we approach this problem from
a novel perspective by constructing a semantically correct dataset,
SURD, and using knowledge distillation to transfer semantic knowl-
edge from complex prompts and LLM. SURD can not only be used
to finetune diffusion models for solving SUR problems but can also
be directly used as a training dataset for diffusion models due to its
ensured semantic correctness.
Social Ethics. Unlike many multimodal datasets in the natural
domain, SURD is entirely built on data generated by DNNs. As
a result, it is less likely to be used in surveillance systems that
could potentially violate people’s privacy. Moreover, during the
4https://github.com/salesforce/LAVIS
5https://github.com/openai/CLIP
data cleaning, a manual inspection stage ensures that SURD doesnot contain any sensitive personal information, such as gender
and race, nor does it include data that could exacerbate biases
towards underrepresented communities. Therefore, upon careful
examination of our dataset, we believe that it is unlikely to be used
to directly harm individuals.
Figure 6: Loss value during the training of SUR-adapter with
different initializations. The mathematical symbols corre-
spond to Eq.(10).
B SUPPLEMENTAL EXPERIMENTS
B.1 Supplemental Implementation details
In our study, we validate the universality of SUR-adapter with two
pre-trained diffusion models, three LLMs with different parameters,
and various controlled methods. Unless otherwise specified, we
follow the settings of [ 3,18,39,45,55]. Specifically, all models are
trained on one Nvidia RTX 3090 GPU, with step set to 5000, batch
size set to 16, and resolution set to 512. During training, we apply
mixed precision and standard data augmentation techniques such
as normalization, center cropping, and horizontal flipping. The
learning rate and hyper-parameters in Eq.(7) and Eq.(10) are set to
1e-5.
All control methods utilize the default settings of diffusers6. Be-
sides, we manually curated a set of images that satisfy the semantic
requirements. These images serve as conditional inputs for Con-
trolNet (canny) and ControlNet (seg). The setting of MultiDiffusion
is that the pretrained model for DM (1.5) uses the schedulers of DM
(cartoon), and vice versa, the pretrained model for DM (cartoon)
uses the schedulers of DM (1.5).
B.2 The Initiation of SUR-adapter
As shown in Fig. 5, we use a fully connected network to connect the
adapter and the backbone. To ensure stable training of the adapter,
we initialize the FCN with 0, following some well-known adapter-
related works [ 19,54]. Additionally, as shown in Fig. 6, we also
demonstrate the impact of different initialization methods on the
loss of SUR-adapter. We observe that different initializations have
6https://github.com/huggingface/diffusers

--- PAGE 12 ---
Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin
Table 7: Evaluation of semantic accuracy (Acc.) in images generated by simple prompts using diffusion models. The simple
prompts consisted of three types of sentences, including "counting", "color", and "action". Each prompt generated 130 images,
and the images were manually checked for semantic accuracy.
Type Prompt Accuracy Accuracy (Ours)
CountingFour freshly baked pies. 63.08% 73.85%
Six colorful hot air balloons floating over a picturesque landscape. 8.46% 41.54%
Seven vintage glass bottles. 0.00% 36.92%
ColorA vibrant red sports car speeding down a winding road. 86.15% 93.85%
The blue glass containing red juice. 17.69% 20.00%
A couple wearing blue and yellow solid color clothes respectively. 0.00% 6.92%
ActionSomeone shooting a basketball on the sports field. 41.54% 56.92%
Giraffes eating trees. 25.38% 50.77%
A chef tossing a pizza dough in the air in a kitchen. 15.38% 32.31%
Table 8: Examples of testing prompts.
Type Prompt
ActionA gymnast performing a balance beam routine with graceful flips and twists.
A skateboarder doing a kickflip over a set of stairs.
A diver swimming underwater with colorful fish and coral all around him.
ColorA golden sun setting over a calm ocean, with orange and pink hues appearing in the sky.
A tranquil scene of a meadow filled with wildflowers in shades of purple, pink, and yellow.
A funky and retro diner with a color scheme of bright pink, teal, and silver.
CountingA set of four antique teacups and saucers with intricate floral designs.
Five different types of fresh fruit cut into slices and arranged on a platter.
Seven colorful beach umbrellas on a sandy beach.
Figure 7: Title, description, and some questions of the user
preference study.
little impact on ℓLLM(𝜙)in Eq.(10), but have a significant effect onthe training of ℓCP(𝜙)and the diffusion model, which is consistent
with existing works [19, 54].
B.3 Accuracy of SUR-adapter in Table 1
We have provided additional information on the semantic accuracy
of SUR-adapter prompts in Table 7, which supplements the prompt
examples shown in Table 1 of the Introduction.
B.4 User Preference Study
In this paper, there are two metrics that require manual judgment.
One is the semantic accuracy of the generated images (action, color,
counting), which is an objective metric. Therefore, it can be easily
assessed and counted by the authors. The other metric that requires
manual judgment is user preference, as shown in Table 3. This
metric is subjective. To gather data for this metric, we collected a
total of 89 valid questionnaires (an example of the questionnaire
is provided in Fig. 7). We randomly presented images generated
by our method and baselines to the participants and asked them
to select a picture that they deemed of better quality based on the
question, "Which of the following pictures do you think is of better
quality?" Finally, based on the 89 questionnaires, we compiled and
analyzed the data.
B.5 Testing Prompts
To evaluate Semantic Understanding and Reasoning (SUR), we have
divided the semantics into three main types, namely Action, Color,
and Counting, with each type having fifteen prompts whose exam-
ples are shown in Table 8. For each prompt, we generate ten images
during testing.
