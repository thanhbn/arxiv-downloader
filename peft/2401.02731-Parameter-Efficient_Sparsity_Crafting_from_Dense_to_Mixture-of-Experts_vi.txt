# 2401.02731.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2401.02731.pdf
# Kích thước file: 663414 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Tạo lập Sparsity Hiệu quả Tham số từ Dense đến Mixture-of-Experts
cho Instruction Tuning trên Các Tác vụ Tổng quát
Haoyuan Wu♠, Haisheng Zheng♡, Zhuolun He♠,♣, Bei Yu♠,
♠Đại học Trung văn Hồng Kông, Hồng Kông SAR
♡Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải, Trung Quốc
♣ChatEDA Tech, Trung Quốc
{hywu24,byu}@cse.cuhk.edu.hk

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã chứng minh được khả năng đáng kể trong các tác vụ xử lý ngôn ngữ tự nhiên (NLP) tổng quát. Instruction tuning, một paradigm thành công, nâng cao khả năng của LLM để tuân theo các hướng dẫn ngôn ngữ tự nhiên và thể hiện khả năng tổng quát hóa mạnh mẽ trên các tác vụ tổng quát. Tuy nhiên, những mô hình này thường gặp phải các hạn chế về hiệu suất trên nhiều tác vụ do dung lượng mô hình bị hạn chế. Mở rộng dung lượng này trong giai đoạn instruction tuning đặt ra những thách thức đáng kể. Để giải quyết vấn đề này, chúng tôi giới thiệu parameter-efficient sparsity crafting (PESC), phương pháp tạo lập các mô hình sparse từ các mô hình dense bằng cách sử dụng kiến trúc mixture-of-experts (MoE). PESC tích hợp các adapter vào các lớp MoE của mô hình sparse, phân biệt các expert mà không thay đổi các trọng số cá nhân trong những lớp này. Phương pháp này giảm đáng kể chi phí tính toán và yêu cầu bộ nhớ GPU, tạo điều kiện mở rộng dung lượng mô hình thông qua việc tăng tham số tối thiểu khi đảm bảo chất lượng xấp xỉ trong không gian hàm so với sparse upcycling gốc. Đánh giá thực nghiệm của chúng tôi chứng minh hiệu quả của phương pháp PESC. Sử dụng PESC trong instruction tuning, mô hình sparse tốt nhất của chúng tôi vượt trội hơn các mô hình sparse và dense khác và thể hiện khả năng tổng quát vượt trội so với GPT-3.5. Mã nguồn có sẵn tại https://github.com/wuhy68/Parameter-Efficient-MoE.

1 Giới thiệu
Những tiến bộ gần đây trong NLP đã được thúc đẩy đáng kể bởi sự ra đời của các LLM như GPT (Brown et al., 2020; OpenAI, 2023), Llama (Touvron et al., 2023a,b), Mistral (Mistral AI, 2023; Jiang et al., 2024), v.v. Quy mô ngày càng tăng của LLM đã thiết lập chúng là những chuyên gia cho các tác vụ NLP do khả năng đặc biệt trong việc nhận diện các mẫu ngôn ngữ phức tạp (Wei et al., 2022).

MBPPNaturalQuestionsAverage
MMLU
MATH
GSM8K
HellaSwagHumanEval
Camelidae-8x34B-pro
Yi-34B-ChatMixtral-8x7B-Instruct
LLAMA2-70B-ChatDeepSeekMoE-16B-Chat
Qwen-72B-ChatHình 1: Camelidae-8×34B-pro đạt hiệu suất xuất sắc trên các tác vụ tổng quát.

Một phương pháp nổi bật để huấn luyện LLM là instruction tuning (Wei et al., 2021). Cách tiếp cận này sử dụng dữ liệu hướng dẫn quy mô lớn, được định dạng tốt, cho phép LLM tinh chỉnh các biểu diễn đã được pre-train để tuân thủ các hướng dẫn của con người (Taori et al., 2023; Xu et al., 2024; Dettmers et al., 2024; Mukherjee et al., 2023). Những LLM được instruction-tuned như vậy thể hiện khả năng tổng quát hóa đáng chú ý trong các tác vụ NLP (Longpre et al., 2023). Việc tổng quát hóa này đòi hỏi huấn luyện trên một phạm vi rộng các tác vụ theo hướng dẫn từ nhiều lĩnh vực như toán học, lập trình, sinh học, v.v. (Chung et al., 2022; Sanh et al., 2021). Tuy nhiên, tính phức tạp vốn có của những tác vụ này có thể cản trở việc fine-tuning mô hình (Zhang và Yang, 2021). Cụ thể, các mô hình có kích thước nhất định có thể gặp khó khăn trong việc tối ưu hóa loss từ các tác vụ xung đột, dẫn đến hiệu suất kém cho các tác vụ tổng quát.

Định luật scaling (Chung et al., 2022) gợi ý rằng việc tăng quy mô mô hình là quan trọng để có hiệu suất tốt hơn. Mở rộng dung lượng mô hình cũng có thể cải thiện hiệu quả instruction tuning cho các tác vụ tổng quát (Kaplan et al., 2020). Tuy nhiên,

--- TRANG 2 ---
hầu hết các LLM đều là các mô hình dense đã được pre-train được thiết kế dựa trên kiến trúc transformer, điều này hạn chế khả năng mở rộng trong quá trình instruction tuning. Komatsuzaki et al. (2023) trình bày một phương pháp để upcycling các mô hình dense thành các mô hình MoE được kích hoạt sparse, có dung lượng lớn hơn (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Puigcerver et al., 2023). Đáng chú ý, Shen et al. (2023) đề xuất rằng các mô hình MoE phản ứng hiệu quả hơn với instruction tuning so với các mô hình dense. Do đó, việc chuyển đổi các mô hình dense thành mô hình MoE trong quá trình instruction tuning có tiềm năng đạt được hiệu suất tuyệt vời trên các tác vụ tổng quát. Việc chuyển đổi này bao gồm khởi tạo mỗi expert trong mô hình MoE như một bản sao của các lớp feedforward neural network (FFN) (Chen et al., 2015; Rae et al., 2021). Với quy mô tham số của các LLM hiện tại, việc huấn luyện những mô hình khổng lồ như vậy đòi hỏi cập nhật trọng số của các expert trong lớp MoE, điều này bị hạn chế bởi tài nguyên bộ nhớ GPU và chi phí tính toán.

Để giảm thiểu những thách thức này, chúng tôi giới thiệu parameter-efficient sparsity crafting (PESC), một cách tiếp cận mở rộng dung lượng mô hình hiệu quả trong khi phối hợp với các kỹ thuật parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019; Dettmers et al., 2024). PESC bao gồm việc chèn các adapter (Houlsby et al., 2019) vào các lớp MoE của mô hình sparse, cho phép phân biệt giữa các expert mà không thay đổi trọng số của mỗi expert trong các lớp MoE khi đảm bảo chất lượng xấp xỉ trong không gian hàm so với sparse upcycling gốc (Komatsuzaki et al., 2023). Xem xét rằng cấu trúc phức tạp hơn có thể cải thiện xấp xỉ (Ding et al., 2022), chúng tôi cũng áp dụng kỹ thuật QLoRA (Dettmers et al., 2024) để cập nhật các trọng số khác trong mô hình sparse. Như được thể hiện trong Hình 1, Camelidae-8×34B-pro của chúng tôi, được instruction fine-tuned sử dụng PESC, đạt được hiệu suất tốt nhất trong số các mô hình sparse và dense mã nguồn mở khác nhau. Những đóng góp của chúng tôi được mô tả như sau:

• Chúng tôi đề xuất một cách tiếp cận, parameter-efficient sparsity crafting (PESC), để mở rộng dung lượng mô hình một cách hiệu quả.

• Chúng tôi triển khai phương pháp PESC cho instruction tuning trên các tác vụ tổng quát, đạt được những cải thiện hiệu suất đáng kể trên nhiều benchmark khác nhau.

• Chúng tôi phát triển các mô hình Camelidae, mô hình sparse được huấn luyện bằng phương pháp PESC, đạt được hiệu suất tốt nhất trên các mô hình sparse mã nguồn mở và thể hiện khả năng tổng quát vượt trội so với GPT-3.5.

2 Phương pháp luận

2.1 Kiến thức cơ bản

Adapters. Houlsby et al. (2019) đề xuất việc tích hợp các adapter vào các mô hình dựa trên transformer đã được pre-train để nâng cao hiệu quả tham số. Cách tiếp cận này bao gồm chỉ điều chỉnh các tham số được thêm bởi các adapter. Một adapter bao gồm hai ma trận, Wdown∈Rd1×d2 và Wup∈Rd2×d1, kết hợp với một hàm phi tuyến σ(·). Ở đây, d1 và d2 biểu thị các chiều đặc trưng trong các mô hình đã được pre-train và chiều ẩn của adapter, tương ứng, với d2< d1 thông thường. Cho một đặc trưng U∈RN×d1 trong mô hình đã được pre-train, đầu ra của module Adapter được biểu diễn như:

U′=σ(UWdown)Wup+U. (1)

Mixture-of-Experts. Như được mô tả trong Hình 2, một lớp MoE bao gồm n experts, {Ei}ni=1, và một router R. Đầu ra y cho một đầu vào x trong lớp MoE được tính như:

y=∑ni=1R(x)iEi(x), (2)

trong đó R(x)i biểu thị đầu ra của mạng gating cho expert thứ i, và Ei(x) là đầu ra của expert thứ i.

Sparsity Crafting. Dựa trên khái niệm sparsity upcycling (Komatsuzaki et al., 2023), sparsity crafting tận dụng các trọng số của mô hình dense. Như được mô tả trong Hình 2, sparsity crafting bao gồm một quá trình biến đổi: thay thế

--- TRANG 3 ---
lớp FFN F trong mỗi block của mô hình transformer dense bằng một lớp MoE. Việc thay thế này tạo ra một block transformer sparse sáng tạo. Trong giai đoạn khởi tạo của sparsity crafting, mỗi expert Ei trong lớp MoE được khởi tạo với lớp FFN F. Để đảm bảo tính nhất quán về cấu trúc, các thành phần khác, như các lớp normalization và attention, được sao chép trực tiếp từ block transformer dense.

Để rõ ràng, chúng ta hãy định nghĩa Fi(θi) như là hàm mục tiêu cho expert thứ i trong lớp MoE, trong đó θi biểu thị các tham số cho Ei. θi được khởi tạo từ θo, là các tham số của lớp FFN F từ mô hình dense gốc. Bản chất của quá trình huấn luyện sparsity crafting nằm ở việc tối ưu hóa Fi(θi). Mục tiêu là rút ra θ+i, các tham số tối ưu cho mỗi expert. Điều này được biểu diễn một cách chính thức như:

θ+i = arg minθi Fi(θi). (3)

Sau quá trình instruction tuning sử dụng kỹ thuật sparsity crafting, các tập tham số tối ưu {θ+i}ni=1 được thu được cho các expert {Ei}ni=1 trong lớp MoE.

2.2 Parameter-Efficient Sparsity Crafting

Như được thể hiện trong Phương trình (3), sparsity crafting truyền thống đòi hỏi tối ưu hóa các tham số {θi}ni=1 cho mỗi expert Ei trong lớp MoE, dẫn đến tiêu thụ tài nguyên đáng kể, bao gồm thời gian huấn luyện và chi phí bộ nhớ do số lượng tham số lớn của các lớp FFN trong LLM. Do đó, như được minh họa trong Hình 2, chúng tôi giới thiệu PESC, một cách tiếp cận giải quyết chi phí thời gian huấn luyện và bộ nhớ cao liên quan đến sparsity crafting trong LLM. Cụ thể, PESC, tận dụng paradigm parameter-efficient fine-tuning (PEFT), tập trung vào việc điều chỉnh một tập con nhỏ hơn của các tham số để đạt được hiệu quả.

Cốt lõi của PESC nằm ở hàm mục tiêu F̃i(θi, ωi), trong đó ωi biểu thị các tham số được chọn để điều chỉnh. Đáng chú ý, các tham số của ωi ít hơn đáng kể so với θi, như được chỉ ra bởi |ωi| ≪ |θi|, trong đó |·| biểu thị số lượng tham số liên quan. Mỗi expert Ei bắt đầu quá trình với trạng thái khởi tạo (θo, ωo), trong đó ωo được khởi tạo bằng không để tạo điều kiện identity mapping, dẫn đến F̃i(θo, ωo) = Fi(θo). Quy trình huấn luyện cho PESC do đó là việc tối ưu hóa F̃i(θo, ωi), dẫn đến một giải pháp ω+i được định nghĩa như:

ω+i = arg minωi F̃i(θo, ωi). (4)

Xem xét rằng |ωi| ≪ |θi|, chúng ta có

∑ni=1|ω+i|+|θo|=n×|ωo|+|θo|
≪n×|θo|=∑ni=1|θ+i|. (5)

Do đó, tập giải pháp {ω+i}ni=1 này hiệu quả hơn so với các tham số sparsity crafting gốc {θ+i}ni=1 cho tập {Ei}ni=1.

Để đảm bảo hiệu quả của PESC so với sparsity crafting truyền thống, điều quan trọng là duy trì lỗi xấp xỉ nhỏ, được định nghĩa bởi:

|F̃i(θ+i, ωo)−F̃i(θo, ω+i)|< ξ, (6)

trong đó ξ là lỗi xấp xỉ. Điều này có thể đạt được bằng cách thiết kế một hàm xấp xỉ F̃i(θo, ω+i) khớp chặt chẽ với F̃i(θ+i, ωo) (Houlsby et al., 2019; Ding et al., 2022). Xem xét rằng quỹ đạo tối ưu hóa θi xấp xỉ theo một đa tạp, có thể được chiếu vào một không gian chiều thấp hơn như adapter trong Phương trình (1). Lỗi xấp xỉ phụ thuộc vào khả năng biểu diễn của các adapter được chèn. Với tính chất xấp xỉ phổ quát của các lớp MLP với các hàm kích hoạt tổng quát, module Adapter là một bộ xấp xỉ phổ quát (Funahashi, 1989; Leshno et al., 1993; Kidger và Lyons, 2020). Kết quả là, việc sử dụng các adapter như ωi có thể đảm bảo hiệu quả chất lượng xấp xỉ của F̃i(θ+i, ωo).

2.3 Thiết kế Mô hình

Parameter-Efficient Experts. Theo phân tích trong Phần 2.2, các adapter có thể đảm bảo một cận dưới tốt ξ trong Phương trình (6). Do đó, chúng ta có thể giới thiệu các lớp MoE parameter-efficient bằng cách tích hợp các adapter, từ đó đạt được sparsity theo cách parameter-efficient hơn.

Trong việc huấn luyện các block transformer sparse, gradient được lan truyền ngược đến mỗi expert, đòi hỏi cập nhật tham số. Đối với một tập hợp n expert, sparsity crafting gốc đòi hỏi chi phí tính toán n lần so với một lớp FFN đơn. Như được mô tả trong Hình 3, PESC của chúng tôi sử dụng các adapter để tránh việc cập nhật dư thừa trọng số expert θi. Cụ thể, chúng tôi cập nhật ωi của n adapter được chèn để phân biệt giữa các expert mà không thay đổi trọng số gốc θo của mỗi expert được sao chép từ lớp FFN gốc. Do đó, cho một đầu vào x, Phương trình (2) có thể được công thức hóa lại như:

y=∑ni=0R(x)iAi(E(x)), (7)

trong đó Ai(x) xây dựng expert parameter-efficient như sau:

Ai(x) = σ(xWidown)Wiup+x. (8)

Xem xét rằng cấu trúc phức tạp hơn có thể cải thiện xấp xỉ, chúng ta cũng có thể cập nhật trọng số được chia sẻ θo của {Ei}ni=1. Như được minh họa trong Phương trình (7), cách tiếp cận này cho phép mở rộng hiệu quả dung lượng mô hình bằng cách giới thiệu số lượng tham số tối thiểu trên n adapter được chèn.

Top-K Gate Router. Trong block transformer sparse, lớp MoE bao gồm một số lượng expert được chỉ định. Một router, sử dụng hàm kích hoạt softmax, mô hình hóa phân phối xác suất trên những expert này, phản ánh khả năng của mỗi expert để xử lý các token đầu vào. Trọng số của router, ký hiệu là Wr, được tích hợp vào block transformer sparse, ban đầu được khởi tạo ngẫu nhiên. Như được mô tả trong Hình 3, chúng tôi sử dụng top-k gate router trong block transformer sparse (Lepikhin et al., 2020; Du et al., 2022). Router này kích hoạt hai expert phù hợp nhất trong số n expert {Ei}ni=1 cho mỗi token x trong một chuỗi đầu vào. Sau khi nhận token đầu vào x, router tạo ra router logits R(x) = Wr·x. Trước khi được chuẩn hóa thông qua phân phối softmax trên n expert có sẵn, chúng tôi thực hiện hàm KeepTopK. Hàm KeepTopK được áp dụng để chỉ giữ lại k giá trị hàng đầu của router logits, gán −∞ cho phần còn lại, hiệu quả đưa chúng về không sau chuẩn hóa softmax. Do đó, cho một token x, đầu ra logit của router được biểu diễn như:

R(x) = Softmax(KeepTopK(Wr·x)). (9)

Giá trị gate của mỗi expert Ei cho token đầu vào x là R(x)i. Mặc dù có sự tăng tham số, các expert của lớp MoE được kích hoạt sparse, ngụ ý rằng chỉ một tập con hạn chế của expert được sử dụng cho mỗi token đầu vào. Cách tiếp cận này nâng cao dung lượng của mô hình trong khi duy trì hiệu quả tính toán. Top-k gate router chọn hai expert tốt nhất cho mỗi token trong quá trình suy luận. Trong một lớp MoE với n expert, điều này cho phép tối đa (n choose k) tổ hợp khác nhau của expert, trái ngược với một tổ hợp duy nhất trong kiến trúc transformer truyền thống, cung cấp khả năng thích ứng tính toán được nâng cao.

Experts Loading Balance. Top-k gate router, thông qua cơ chế gating của nó, có xu hướng ưu tiên không cân đối một vài expert, dẫn đến mất cân bằng khi những expert này được huấn luyện thường xuyên hơn và do đó được router chọn. Để chống lại sự mất cân bằng này và thúc đẩy sử dụng expert đồng đều, một auxiliary loss như được đề xuất bởi Fedus et al. (2022) được tích hợp trong quá trình huấn luyện cho mỗi block transformer sparse. Với n expert và một batch B chứa T token, auxiliary loss L này cho cân bằng tải expert được tính như tích vô hướng có tỷ lệ của các vector f và p,

L=α·n·∑ni=1fi·pi, (10)

trong đó fi biểu thị phần token được gửi đến expert i và pi biểu thị phần xác suất router được phân bổ cho expert i. α là hệ số nhân cho auxiliary loss. Chúng tôi sử dụng α= 10−2 đủ lớn để đảm bảo cân bằng tải trong khi đủ nhỏ để không áp đảo mục tiêu cross-entropy chính. Vì tình huống lý tưởng đòi hỏi định tuyến đồng đều trên n expert, cả hai vector lý tưởng nên có giá trị 1/n. Auxiliary loss của Phương trình (10) thúc đẩy phân phối đồng đều này, đạt được mức tối thiểu trong những điều kiện như vậy.

3 Thí nghiệm

3.1 Cài đặt

Training Data. Để chứng minh khả năng học tập của mô hình sparse với các lớp MoE, chúng tôi đồng thời huấn luyện mô hình trên một tập hợp kỹ năng đa dạng, bao gồm lập trình, toán học và các khả năng tổng quát khác từ nhiều chủ đề khác nhau. Việc huấn luyện này bao gồm tích hợp ba bộ dữ liệu riêng biệt từ các lĩnh vực khác nhau trong giai đoạn instruction tuning: SlimOrca (Lian et al., 2023; Mukherjee et al., 2023; Longpre et al., 2023), Magicoder (Wei et al., 2023), và MetaMathQA (Yu et al., 2023). Sau khi lọc và lấy mẫu, chúng tôi có thể thu được hai bộ dữ liệu instruction bao gồm IDAE-500K và IDAE-720K cuối cùng. Chúng tôi cung cấp thêm chi tiết về bộ dữ liệu IDAE trong Phụ lục A.

Evaluation Benchmarks. Đánh giá của chúng tôi so sánh hiệu suất của các mô hình dense và sparse trên các benchmark học thuật. Các mô hình dense bao gồm Llama2 (Touvron et al., 2023b), Vicuna (Zheng et al., 2023), Yi (01 AI, 2023), SUSChat (SUSTech IDEA, 2023), Qwen (Bai et al., 2023), GPT3.5 (Brown et al., 2020), và các mô hình Camel của chúng tôi, trong khi các mô hình sparse bao gồm Mixtral (Jiang et al., 2024), DeepSeekMoE (Dai et al., 2024), và các mô hình Camelidae của chúng tôi. Đánh giá được thực hiện bằng cách sử dụng OpenCompass (OpenCompass, 2023), LM-Eval-Harness (Gao et al., 2023), và các thư viện đánh giá nội bộ của chúng tôi, tóm tắt hiệu suất trên các benchmark nổi tiếng. Những benchmark này được minh họa như sau:

• Code: Đánh giá bao gồm điểm pass@1 cho HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021).

• Math: Điểm độ chính xác cho các benchmark GSM8K (Cobbe et al., 2021) (5-shot) và MATH (Hendrycks et al., 2021) (4-shot).

• Commonsense Reasoning (CR): Điểm độ chính xác cho PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy, và ARC-challenge (Clark et al., 2018).

• Word Knowledge (WK): Đánh giá hiệu suất 0-shot trên NaturalQuestions (Kwiatkowski et al., 2019) và TriviaQA (Joshi et al., 2017) sử dụng metric exact match (EM).

• Aggregated Benchmarks: Kết quả tổng thể cho MMLU (Hendrycks et al., 2020) (5-shot) sử dụng metric điểm độ chính xác.

Đáng chú ý, để có kết quả thí nghiệm chi tiết hơn, vui lòng tham khảo Phụ lục C.

Camel và Camelidae Models. Chúng tôi fine-tuned các mô hình Camel và Camelidae sử dụng bộ dữ liệu giống hệt nhau, IDAE-500K, để đảm bảo so sánh công bằng giữa các mô hình dense và sparse. Cụ thể, các mô hình Camel là mô hình dense trong khi các mô hình Camelidae là mô hình sparse với kiến trúc MoE. Đáng chú ý, để nâng cao hơn nữa khả năng của các mô hình sparse, chúng tôi cũng sử dụng IDAE-720K cho instruction-tuning của mô hình Camelidae-pro. Tất cả các mô hình Camelidae sử dụng top-2 gate router.

Implementation Details. Chúng tôi sử dụng kỹ thuật QLoRA (Dettmers et al., 2024) để fine-tuning hiệu quả cả mô hình Camel và Camelidae được xuất phát từ Llama2-7B (Touvron et al., 2023b), Llama2-13B (Touvron et al., 2023b), và Yi-34B (01 AI, 2023). Đối với cấu hình QLoRA, chúng tôi sử dụng lược đồ lượng tử hóa 4-bit cho thí nghiệm của mình, giảm đáng kể việc sử dụng bộ nhớ trong khi bảo toàn hiệu suất mô hình. Quá trình này đòi hỏi sử dụng lịch trình learning rate không đổi với tỷ lệ warm-up 0.03, và optimizer paged AdamW (Dettmers et al., 2024; Loshchilov và Hutter, 2017) với learning rate 2×10−4, không có weight decay, batch size 128, và độ dài chuỗi 2048 token. Các mô hình trải qua instruction tuning trong một epoch trên 16 GPU A100, mỗi GPU được trang bị bộ nhớ 80G. Vui lòng tham khảo Phụ lục B để biết thêm chi tiết.

3.2 So sánh với Chat LLM

Chúng tôi trình bày hiệu suất của nhiều chat LLM khác nhau trên một tập hợp các benchmark tiêu chuẩn. Các mô hình chat được đánh giá là Camelidae-8×34B-pro, Mixtral-8×7B-Instruct (Jiang et al., 2024), DeepSeekMoE-16B-Chat (Dai et al., 2024), Yi-34B-Chat (01 AI, 2023), Llama2-70B-Chat (Touvron et al., 2023b), Qwen-72B-Chat (Bai et al., 2023), và GPT-3.5 (Brown et al., 2020). Các benchmark bao gồm một loạt các lĩnh vực, bao gồm câu hỏi trắc nghiệm trên 57 chủ đề (MMLU), toán học tiểu học (GSM8K), bài toán toán học trên nhiều mức độ khó khác nhau (MATH), tác vụ lập trình Python (HumanEval), tạo mã Python (MBPP), lý luận thường thức (HellaSwag), và trả lời câu hỏi kiến thức thế giới (NaturalQuestions).

Như được thể hiện trong Phần 3.1, Camelidae-8×34B-pro chứng minh điểm mạnh của nó trong phạm vi rộng lớn về kiến thức, khả năng toán học, lập trình và lý luận thường thức trên nhiều mô hình sparse và dense khác nhau.

Knowledge và Reasoning Abilities. Camelidae-8×34B-pro thể hiện hiệu suất ấn tượng trên MMLU với tỷ lệ thành công cao 75.7%, cho thấy kiến thức chuyên môn và học thuật rộng lớn. Trong khi đó, Camelidae-8×34B-pro đạt 31.2% trên NaturalQuestions, chứng minh cơ sở kiến thức thế giới toàn diện. Mặc dù Camelidae-8×34B-pro yếu hơn một số mô hình trong benchmark HellaSwag, độ chính xác 85.2% vẫn khá tốt cho lý luận thường thức.

Mathematical Proficiency. Camelidae-8×34B-pro xuất sắc trên benchmark GSM8K với 79.4%

--- TRANG 4 ---
độ chính xác, cao nhất trong số các mô hình. Tuy nhiên, điểm 24.0% trên benchmark MATH tụt hậu so với GPT-3.5, cho thấy điểm yếu tương đối trong việc giải quyết các bài toán toán học phức tạp hơn.

Coding Skills. Camelidae-8×34B-pro thể hiện khả năng lập trình mạnh mẽ với độ chính xác 48.8% trên benchmark HumanEval, tương đương với GPT-3.5, và tỷ lệ pass 43.2% trên benchmark tạo mã Python MBPP, thể hiện khả năng hiểu và tạo mã của nó.

3.3 Nghiên cứu Ablation

Dense models vs. Sparse Models. Chúng tôi đánh giá hiệu quả của phương pháp huấn luyện mới thông qua phân tích so sánh các mô hình Camelidae, bao gồm cả cấu hình dense và sparse trên nhiều kích thước tham số khác nhau, như được nêu trong Bảng 2 và Bảng 3. Các mô hình Camelidae thể hiện lợi thế đáng kể so với các đối thủ trên nhiều kích thước mô hình khác nhau. Sự vượt trội này đặc biệt rõ ràng trong các tác vụ đòi hỏi hiểu biết sâu hơn, bao gồm các benchmark lập trình và toán học, làm nổi bật hiệu quả của cách tiếp cận huấn luyện của chúng tôi trong việc tăng cường khả năng mô hình. Để đảm bảo so sánh công bằng, các mô hình Camel và Camelidae được fine-tuned sử dụng cùng bộ dữ liệu, IDAE-500K. Như được chỉ ra trong Bảng 2, các mô hình Camelidae, như các mô hình sparse, liên tục thể hiện hiệu suất vượt trội so với các mô hình Camel dense có kích thước tương đương. Hơn nữa, Camelidae-8x34B-pro, được huấn luyện sử dụng bộ dữ liệu IDAE-720K, vượt trội hơn Camelidae-8x34B, điều này cho thấy rằng hiệu quả của phương pháp chúng tôi được duy trì ngay cả với việc tăng khối lượng dữ liệu huấn luyện.

Numbers of Experts. Kết quả từ nghiên cứu, như được thể hiện trong Bảng 4, rõ ràng chứng minh rằng việc tăng số lượng expert trong các lớp MoE cải thiện đáng kể hiệu suất của mô hình. Xu hướng này rõ ràng trong việc cải thiện điểm số dần dần trên nhiều benchmark học thuật khác nhau khi số lượng expert tăng từ 4 đến 16 trong các mô hình Camelidae. Đáng chú ý, mô hình Camelidae-16×7B thể hiện hiệu suất đặc biệt trên tất cả các benchmark. Mối tương quan tích cực này giữa số lượng expert và hiệu suất mô hình cho thấy tiềm năng chưa được khai thác của cách tiếp cận của chúng tôi. Cụ thể, việc tăng thêm số lượng expert có thể mang lại những tiến bộ đáng kể hơn nữa trong hiệu suất mô hình.

3.4 Phân tích Routing

Nghiên cứu của chúng tôi kiểm tra kỹ lưỡng quá trình lựa chọn expert bởi router, với sự tập trung mạnh mẽ vào việc xác định liệu các expert cụ thể có thể hiện chuyên môn hóa trong các lĩnh vực riêng biệt như lập trình và toán học.

Cuộc điều tra này bao gồm phân tích kỹ lưỡng các mẫu phân phối của các expert được chọn trên nhiều tập con dữ liệu khác nhau. Những tập con này bao gồm SlimOrca (Lian et al., 2023; Mukherjee et al., 2023; Longpre et al., 2023), Magicoder (Wei et al., 2023), và MetaMathQA (Yu et al., 2023). Kết quả của phân tích này được mô tả trong Hình 4, với sự nhấn mạnh đặc biệt vào lớp thứ 15 của mô hình Camelidae-8×7B.

Các phát hiện của chúng tôi làm nổi bật những biến thể có thể nhận biết trong phân phối expert giữa ba bộ dữ liệu. Ví dụ, Expert 1 thể hiện mức kích hoạt cao đáng chú ý trong bộ dữ liệu Magicoder, trong khi Expert 6 chứng minh tỷ lệ kích hoạt đáng kể trong bộ dữ liệu MetaMathQA so với các expert khác. Những quan sát này gợi ý rằng router hoạt động với cách tiếp cận có cấu trúc về mặt cú pháp. Quan trọng, mặc dù có sự biến thể trong lựa chọn expert trên các bộ dữ liệu khác nhau, một số expert (cụ thể là Expert 1, 2, 5, và 6) liên tục thể hiện tỷ lệ kích hoạt cao.

4 Công trình Liên quan

4.1 Dense và Sparse Models

Các mô hình dense truyền thống kích hoạt tất cả tham số trong quá trình huấn luyện và suy luận, dẫn đến yêu cầu tính toán và bộ nhớ cao khi kích thước mô hình tăng. Ngược lại, các mô hình sparse, sử dụng kiến trúc MoE (Shazeer et al., 2017), chỉ kích hoạt một tập con của tổng số tham số có sẵn cho mỗi token đầu vào. Trong các mô hình sparse, lớp FFN được thay thế bằng lớp MoE, hướng dẫn mỗi token đầu vào đến một nhóm mạng expert được chọn để xử lý. Biểu diễn token cuối cùng là sự kết hợp đầu ra từ những expert được chọn này. Mặc dù có sự tăng tham số, việc kích hoạt sparse của expert đảm bảo hiệu quả tính toán trong khi nâng cao khả năng mô hình. Các mô hình sparse với kiến trúc MoE đã được khám phá rộng rãi trong lĩnh vực NLP (Lepikhin et al., 2020; Du et al., 2022; Fedus et al., 2022), đặc biệt với việc tích hợp vào block transformer. Cách tiếp cận của chúng tôi áp dụng chiến lược routing từ (Lepikhin et al., 2020; Du et al., 2022), với kích hoạt tham số có chọn lọc để đạt được hiệu quả tính toán.

4.2 Tái sử dụng Trained Weights

Các nghiên cứu gần đây tập trung vào việc cải thiện hiệu quả huấn luyện bằng cách tận dụng trọng số mô hình có sẵn để khởi đầu ấm, từ đó giảm thiểu chi phí huấn luyện (Chen et al., 2015; Rae et al., 2021; Yang et al., 2021; Lin et al., 2021; Lan et al., 2019). Sparse Upcycling (Komatsuzaki et al., 2023) giới thiệu một phương pháp để khởi tạo các mô hình MoE sparse sử dụng trọng số từ mô hình dense đã được pre-train. Cách tiếp cận này giảm đáng kể tài nguyên tính toán cần thiết so với việc huấn luyện mô hình dense gốc. Sparse Upcycling bao gồm việc chuyển giao trực tiếp các tham số layer normalization, attention và embedding từ mô hình dense sang mô hình sparse mới. Hơn nữa, nó thay thế một số lớp Multilayer Perceptron (MLP) bằng các lớp MoE, khởi tạo các expert trong những lớp này với trọng số từ MLP của mô hình dense. Quá trình này hiệu quả chuyển giao các biểu diễn học được có giá trị từ giai đoạn pre-training của mô hình dense vào mô hình sparse. Trong nghiên cứu của chúng tôi, chúng tôi áp dụng phương pháp này, tái sử dụng trọng số từ mô hình dense đã được pre-train cho phương pháp PESC của chúng tôi.

4.3 Parameter-Efficient Fine-Tuning

Truyền thống, full fine-tuning đã là chuẩn mực để thích ứng các mô hình pre-trained, bao gồm LLM. Tuy nhiên, do kích thước khổng lồ của LLM, cách tiếp cận này đòi hỏi tài nguyên tính toán đáng kể. Để giảm thiểu điều này, nhiều phương pháp PEFT đã xuất hiện (Houlsby et al., 2019; Hu et al., 2021; Li và Liang, 2021; Liu et al., 2022; Wu et al., 2024a). PEFT tập trung vào việc huấn luyện một tập con hạn chế của tham số, từ mô hình hiện có hoặc những tham số mới được thêm vào. Các phương pháp dựa trên adapter (Houlsby et al., 2019; Hu et al., 2021; Liu et al., 2022; Wu et al., 2024a) tích hợp các module nhỏ, có thể học được gọi là adapter vào các mô hình pre-trained, chỉ fine-tuning những tham số mới được chèn này. Trong số này, QLoRA (Dettmers et al., 2024) đã trở nên phổ biến do hiệu quả trong việc fine-tuning LLM, mang lại kết quả tương đương với full fine-tuning. Một xu hướng mới nổi khác trong PEFT là prefix-/prompt-tuning (Lester et al., 2021; Li và Liang, 2021), bao gồm việc thêm các vector token có thể học được vào keys và values trong các module attention hoặc trực tiếp vào chuỗi đầu vào. Trong nghiên cứu này, chúng tôi chèn các adapter sau các lớp FFN được sao chép để xây dựng các lớp MoE và sử dụng QLoRA để cập nhật các metric trọng số khác của LLM.

4.4 Mixture of LoRA Experts

Các công trình khác cũng khám phá sự kết hợp của MoE với các kỹ thuật PEFT (Diao et al., 2023; Gou et al., 2023; Wu et al., 2024b; Liu et al., 2023; Luo et al., 2024; Dou et al., 2024). Ví dụ, LoRAMoE (Dou et al., 2024) tập trung vào việc giữ lại kiến thức thế giới, và MoELoRA (Luo et al., 2024) tập trung vào khả năng Math và CommonSense Reasoning sử dụng framework PEFT kết hợp MOE và LoRA. Tuy nhiên, framework mixture of LoRA phát sinh chi phí tính toán bổ sung bao gồm sử dụng bộ nhớ cao hơn và tốc độ chậm hơn mà không có song song hóa trong quá trình huấn luyện và suy luận. Phương pháp PESC của chúng tôi, ngược lại, không đối mặt với những thách thức này. PESC xây dựng trên framework mô hình dựa trên adapter, fine-tuning nhiều adapter được chèn sau các lớp FFN được sao chép thay vì tất cả các lớp FFN được sao chép trong các expert tương ứng. Trong thiết kế MoE của PESC, mỗi expert sử dụng một module adapter duy nhất, giảm đáng kể tổng dung lượng bộ nhớ so với module LoRA, cái mà sẽ yêu cầu nhiều module cho mỗi expert do vị trí của nó trong các lớp FFN và attention. Sự khác biệt này đặc biệt quan trọng khi xử lý số lượng lớn expert, vì hạn chế bộ nhớ trở nên ngày càng thách thức. Hơn nữa, các expert dựa trên adapter của chúng tôi cho phép tính toán song song trên các expert do tính độc lập của chúng với đầu ra của nhau, không giống như LoRA, nơi các phụ thuộc giữa các lớp có thể hạn chế song song hóa. Thiết kế này tăng tốc thời gian huấn luyện, đặc biệt trong các tình huống mà số lượng expert tăng lớn, đảm bảo khả năng mở rộng và hiệu quả. Cũng đáng chú ý rằng LoRA có thể yêu cầu merge trọng số vào mô hình chính để suy luận, dẫn đến tăng sử dụng bộ nhớ và các vấn đề độ trễ tiềm ẩn, đặc biệt vì nhiều token kích hoạt các expert khác nhau. Ngược lại, MoE parameter-efficient dựa trên adapter không áp đặt overhead như vậy trong quá trình suy luận, duy trì gánh nặng tính toán thấp tương tự như mô hình dense gốc.

5 Kết luận

Trong bài báo này, chúng tôi giới thiệu Parameter-Efficient Sparsity Crafting (PESC) là phương pháp upcycle các mô hình dense thành mô hình sparse sử dụng kiến trúc MoE. PESC kết hợp các adapter (Houlsby et al., 2019) trong các lớp MoE của mô hình sparse, cho phép phân biệt các expert mà không sửa đổi trọng số cá nhân của mỗi expert, và đảm bảo chất lượng xấp xỉ so với sparsity upcycling truyền thống (Komatsuzaki et al., 2023) trong không gian hàm (Phần 2.2). Kỹ thuật này giảm đáng kể chi phí tính toán và yêu cầu bộ nhớ GPU so với sparse upcycling. Nó tạo điều kiện mở rộng dung lượng mô hình với sự tăng tham số tối thiểu do việc tích hợp các adapter. Chúng tôi áp dụng phương pháp PESC cho instruction tuning trên nhiều tác vụ tổng quát khác nhau, dẫn đến những cải thiện hiệu suất đáng chú ý trên nhiều benchmark khác nhau (Phần 3). Ngoài ra, chúng tôi phát triển các mô hình sparse, Camelidae, sử dụng cách tiếp cận PESC và đạt được hiệu suất vượt trội trên nhiều mô hình sparse mã nguồn mở khác nhau và thể hiện khả năng tổng quát vượt trội so với GPT-3.5.

Hạn chế

Phương pháp PESC giới thiệu nhiều tham số hơn một chút so với một số kỹ thuật PEFT (LoRA, v.v.). Quá trình instruction tuning của các mô hình sparse sử dụng phương pháp PESC sẽ yêu cầu nhiều bộ nhớ GPU và thời gian tính toán hơn so với các mô hình dense. Mặc dù PESC nâng cao hiệu suất của instruction tuning cho các tác vụ tổng quát, nó vẫn có thể không sánh được với hiệu suất của sparse upcycling với full fine-tuning, vì PESC là một xấp xỉ toán học của sparse upcycling như được minh họa trong Phương trình (6).

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi Hội đồng Tài trợ Nghiên cứu của Hồng Kông SAR (Số CUHK14210723 và Số CUHK14211824), và dự án MIND (MINDXZ202404).

Tài liệu tham khảo

01 AI. 2023. Yi. https://github.com/01-ai/Yi.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. PiQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in neural information processing systems.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Tianqi Chen, Ian Goodfellow, và Jonathon Shlens. 2015. Net2Net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. 2024. DeepSeek-Moe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2024. QLoRA: Efficient finetuning of quantized LLMs. In Advances in Neural Information Processing Systems.

--- TRANG 5 ---
Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, và Tong Zhang. 2023. Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, trang 5113–5129.

Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2022. Delta Tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904.

Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei Shen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhiheng Xi, Xiaoran Fan, et al. 2024. LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, trang 1932–1945.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning.

William Fedus, Barret Zoph, và Noam Shazeer. 2022. Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research.

Ken-Ichi Funahashi. 1989. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183–192.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. 2023. A framework for few-shot language model evaluation.

Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James T Kwok, và Yu Zhang. 2023. Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning. arXiv preprint arXiv:2312.12379.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, và Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning.

Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of Experts. arXiv preprint arXiv:2401.04088.

Mandar Joshi, Eunsol Choi, Daniel S Weld, và Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Patrick Kidger và Terry Lyons. 2020. Universal approximation with deep narrow networks. In Conference on learning theory, trang 2306–2327. PMLR.

Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, và Neil Houlsby. 2023. Sparse Upcycling: Training mixture-of-experts from dense checkpoints. In International Conference on Learning Representations.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural Questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. 2019. AlBert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. 2020. GShard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.

Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, và Shimon Schocken. 1993. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861–867.

Brian Lester, Rami Al-Rfou, và Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Conference on Empirical Methods in Natural Language Processing.

Xiang Lisa Li và Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In The Association for Computational Linguistics.

Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, và "Teknium". 2023. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification.

Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, et al. 2021. M6-10T: A sharing-delinking paradigm for efficient multi-trillion parameter pre-training. arXiv preprint arXiv:2110.03888.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems.

Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, và Yefeng Zheng. 2023. MoELoRA: An MoE-based parameter efficient fine-tuning method for multi-task medical applications. arXiv preprint arXiv:2310.18339.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.

Ilya Loshchilov và Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.

Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, và Kang Liu. 2024. MoELoRA: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2402.12851.

Mistral AI. 2023. Mistral. https://mistral.ai/news/announcing-mistral-7b//.

Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, và Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707.

OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.

OpenCompass. 2023. OpenCompass: A Universal Evaluation Platform for Foundation Models. https://github.com/open-compass/opencompass.

Joan Puigcerver, Carlos Riquelme, Basil Mustafa, và Neil Houlsby. 2023. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.

Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. 2023. Mixture-of-experts meets instruction tuning: A winning combination for large language models. arXiv preprint arXiv:2305.14705.

SUSTech IDEA. 2023. SUSChat. https://github.com/SUSTech-IDEA/SUS-Chat.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, và William Fedus. 2022. Emergent Abilities of Large Language Models. Journal of Machine Learning Research.

Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, và Lingming Zhang. 2023. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120.

Haoyuan Wu, Xinyun Zhang, Peng Xu, Peiyu Liao, Xufeng Yao, và Bei Yu. 2024a. p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, tập 38, trang 6003–6011.

Xu Wu, Shaohan Huang, và Furu Wei. 2024b. MoLE: Mixture of loRA experts. In International Conference on Learning Representations.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. 2024. WizardLM: Empowering large language models to follow complex instructions. In International Conference on Learning Representations.

Shuo Yang, Le Hou, Xiaodan Song, Qiang Liu, và Denny Zhou. 2021. Speeding up deep model training by sharing weights and then unsharing. arXiv preprint arXiv:2110.03848.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, và Weiyang Liu. 2023. MetaMath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.

Yu Zhang và Qiang Yang. 2021. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 34(12):5586–5609.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.

--- TRANG 6 ---
[Trang này chứa các bảng kết quả thí nghiệm từ bài báo gốc - tôi sẽ dịch tiêu đề và nội dung của các bảng này]

Bảng 1: Hiệu suất của Camelidae-8×34B-pro trên các benchmark học thuật. Chúng tôi trình bày so sánh chi tiết của mô hình Camelidae-8×34B-pro với nhiều mô hình chat sparse và dense mã nguồn mở khác nhau. Chúng tôi in đậm điểm số cao nhất trong tất cả các mô hình.

Bảng 2: Hiệu suất tổng thể trên tất cả các benchmark đánh giá của mô hình dense (Camel) và sparse (Camelidae) trên các kích thước mô hình khác nhau. Chúng tôi in đậm điểm số cao nhất riêng biệt cho các kích thước mô hình khác nhau.

--- TRANG 7 ---
[Tiếp tục các bảng và hình vẽ phân tích kết quả]

Hình 4: Tỷ lệ token được phân bổ cho mỗi expert trên các tập con dữ liệu khác nhau.

Bảng 3: Hiệu suất tổng thể trên các benchmark được nhóm của nhiều mô hình dense khác nhau (Llama2-Chat (Touvron et al., 2023b), Vicuna (Zheng et al., 2023), Yi-Chat (01 AI, 2023), SUSChat (SUSTech IDEA, 2023)) trên các kích thước mô hình khác nhau. Chúng tôi in đậm điểm số cao nhất riêng biệt cho các kích thước mô hình khác nhau.

Bảng 4: Đánh giá trên số lượng expert khác nhau trong các lớp MoE. Chúng tôi in đậm điểm số cao nhất cho mỗi benchmark được nhóm.

--- TRANG 8 ---
[Tiếp tục phần kết luận và các phần phụ lục]

--- TRANG 9 ---
[Tiếp tục các tài liệu tham khảo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
A Chi tiết về Bộ dữ liệu IDAE

Chúng tôi thể hiện tỷ lệ của các bộ dữ liệu SlimORCA (Lian et al., 2023; Mukherjee et al., 2023; Longpre et al., 2023), Magicoder (Wei et al., 2023), và MetaMathQA (Yu et al., 2023) trong các bộ dữ liệu IDAE-500K và IDAE-720K trong Bảng 5.

Bảng 5: Tỷ lệ của các bộ dữ liệu SlimORCA, Magicoder, và MetaMathQA trong các bộ dữ liệu IDAE.

B Chi tiết Triển khai

Chúng tôi thể hiện các hyperparameter mà chúng tôi sử dụng cho instruction tuning trong Bảng 6.

Bảng 6: Hyperparameter của instruction tuning.

C Kết quả Đánh giá Chi tiết trên Các Benchmark được Nhóm

Chúng tôi thể hiện kết quả đánh giá chi tiết của mỗi benchmark học thuật được nhóm như sau:

• Trong Bảng 7, chúng tôi báo cáo chi tiết đánh giá của benchmark MMLU.
• Trong Bảng 8, chúng tôi báo cáo kết quả trên các benchmark GSM8K và MATH.
• Trong Bảng 9, chúng tôi so sánh kết quả trên các benchmark HumanEval và MBPP.
• Trong Bảng 10, chúng tôi thể hiện kết quả trên một số benchmark lý luận thường thức.
• Trong Bảng 11, chúng tôi đánh giá hiệu suất trên các benchmark NaturalQuestions và TriviaQA.

[Tiếp theo là các bảng kết quả chi tiết từ Bảng 7 đến Bảng 11]
