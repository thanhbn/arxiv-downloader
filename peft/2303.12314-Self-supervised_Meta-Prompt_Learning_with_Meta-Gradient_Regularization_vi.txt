# Học meta-prompt tự giám sát với điều chuẩn meta-gradient cho khái quát hóa few-shot

Kaihang Pan1,2∗, Juncheng Li1†, Hongye Song2, Jun Lin2, Xiaozhong Liu3, Siliang Tang1
1Đại học Zhejiang,2Viện nghiên cứu DAMO, Tập đoàn Alibaba,
3Viện Công nghệ Worcester
{kaihangpan, junchengli, siliang}@zju.edu.cn
{hongye.shy, linjun.lj}@alibaba-inc.com, xliu14@wpi.edu

## Tóm tắt

Điều chỉnh prompt là một phương pháp hiệu quả về tham số, học các prompt mềm và điều kiện các mô hình ngôn ngữ đóng băng để thực hiện các tác vụ downstream cụ thể. Mặc dù hiệu quả, điều chỉnh prompt trong thiết lập few-shot một mặt phụ thuộc nhiều vào khởi tạo tốt các prompt mềm. Mặt khác, nó có thể dễ dàng overfit với các mẫu huấn luyện few-shot, do đó làm suy yếu khả năng khái quát hóa. Các nghiên cứu hiện tại tận dụng pre-training hoặc meta-learning có giám sát để khởi tạo prompt mềm nhưng chúng thất bại trong việc khái quát hóa hiệu quả dữ liệu đến các tác vụ downstream chưa thấy. Để giải quyết các vấn đề trên, bài báo này đề xuất một khung học meta-prompt tự giám sát mới với điều chuẩn meta-gradient cho khái quát hóa few-shot (SUPMER). SUPMER tận dụng meta-learning tự giám sát với một tập hợp đa dạng các tác vụ meta-training được thiết kế tốt để học khởi tạo prompt universal cho việc thích ứng hiệu quả chỉ sử dụng dữ liệu không nhãn. Ngoài ra, nó cùng meta-học một hàm điều chuẩn gradient để biến đổi gradient thô thành hướng khái quát hóa domain, do đó giảm thiểu vấn đề overfitting. Các thí nghiệm rộng rãi cho thấy SUPMER đạt hiệu suất tốt hơn cho các tác vụ downstream few-shot khác nhau, và cũng thể hiện khả năng khái quát hóa domain mạnh hơn. Mã nguồn cho SUPMER sẽ có sẵn tại https://github.com/beepkh/SUPMER.

## 1 Giới thiệu

Những thành tựu NLP gần đây đã chứng kiến sự phát triển nhanh chóng của các mô hình ngôn ngữ pre-trained (PLM) (ví dụ, BERT Devlin et al., 2019; T5 Raffel et al., 2020; GPT3 Brown et al., 2020). Fine-tuning, điều chỉnh toàn bộ tham số PLM, đã đạt được hiệu suất xuất sắc trong các tác vụ NLP khác nhau. Tuy nhiên, khi quy mô mô hình pre-trained tăng lên, việc điều chỉnh toàn bộ tập tham số đôi khi sẽ không thể chi trả được. Gần đây hơn, các phương pháp dựa trên prompt, chỉ đơn giản chèn một đoạn văn bản được thiết kế cẩn thận vào đầu vào (ví dụ, "It was ⟨X⟩.") và dự đoán các từ mục tiêu (ví dụ, "great" hoặc "terrible") tại vị trí mask với PLM đóng băng, đã thể hiện hiệu quả đáng kể. Nhưng người ta đã quan sát thấy rằng hiệu suất của các phương pháp dựa trên prompt bị ảnh hưởng rất nhiều bởi thiết kế prompt. Dựa trên điều này, điều chỉnh prompt (PT Lester et al., 2021), như một phương pháp điều chỉnh hiệu quả tham số, được đề xuất để chỉ thêm vào trước một số token có thể học được gọi là prompt mềm vào văn bản đầu vào, với tất cả tham số PLM đóng băng.

Mặc dù điều chỉnh prompt là một paradigm hiệu quả và hiệu suất, Gu et al. (2022) cho thấy nó hoạt động kém hơn nhiều so với fine-tuning trong thiết lập few-shot. Chúng tôi cho rằng hiệu suất không thỏa mãn chủ yếu do hai hạn chế: 1) Hiệu suất của PT rất nhạy cảm với khởi tạo prompt mềm, đặc biệt cho các tác vụ few-shot. Như được thể hiện trong Hình 1 (a), khởi tạo prompt mềm khác nhau dẫn đến biến đổi hiệu suất đáng kể. 2) PT few-shot có nguy cơ overfitting với một số tương quan giả mạo vì prompt mềm được điều chỉnh trên các mẫu huấn luyện hạn chế, do đó làm suy yếu khả năng khái quát hóa của PLM. Như được thể hiện trong Hình 1 (b), hiệu suất của PT few-shot vanilla suy giảm đáng kể trong các bước huấn luyện cuối.

Nghiên cứu gần đây chủ yếu tập trung vào hạn chế đầu tiên, tận dụng pre-training hoặc meta-learning có giám sát cho khởi tạo prompt mềm. Một phương pháp điều chỉnh prompt pre-trained (PPT) (Gu et al., 2022) được đề xuất từ đầu, sử dụng các tác vụ tự giám sát để pre-train prompt mềm và sau đó áp dụng chúng trong tình huống few-shot. Tuy nhiên, không tối ưu hóa rõ ràng khả năng thích ứng nhanh của mô hình, PPT gặp phải sự không khớp train-test giữa dữ liệu pre-training và dữ liệu downstream. Vì vậy nó hạn chế khái quát hóa đến các tác vụ few-shot chưa thấy, đặc biệt khi có sự chênh lệch đáng kể về domain hoặc format tác vụ. MetaPrompting (Hou et al., 2022), như một nỗ lực khác, tìm sự hỗ trợ từ meta-learning model-agnostic (MAML Finn et al., 2017) cho thích ứng nhanh trong thiết lập few-shot. Tuy nhiên, trong mỗi tác vụ, MetaPrompting yêu cầu nhiều dữ liệu có nhãn trong các lớp nhất định để thực hiện meta-learning có giám sát cho khởi tạo prompt, điều này thường không thể tiếp cận trong các tình huống few-shot thực tế. Và khởi tạo đã học chỉ có thể khái quát hóa đến các lớp còn lại của cùng tác vụ theo cách few-shot, thể hiện khả năng chuyển giao tác vụ yếu. Hơn nữa, tất cả các nghiên cứu hiện tại này đều bỏ qua hạn chế thứ hai, tức là xu hướng điều chỉnh prompt few-shot dẫn đến overfitting.

Để giải quyết các thiếu sót của các nghiên cứu hiện tại, chúng tôi đề xuất SUPMER, một khung học meta-prompt tự giám sát với điều chuẩn meta-gradient. Nó tận dụng meta-learning tự giám sát để học universal một khởi tạo prompt mềm hiệu quả, cũng với một hàm điều chuẩn meta-gradient để giảm thiểu overfitting. Quá trình toàn diện này chỉ yêu cầu thực hiện một lần và cho phép thích ứng liền mạch với các tác vụ downstream few-shot khác nhau, đồng thời cũng tạo điều kiện hội tụ nhanh hơn cho điều chỉnh prompt downstream.

Cụ thể, để giải quyết hạn chế đầu tiên, chúng tôi thiết kế một phương pháp meta-learning tự giám sát mới cho khởi tạo prompt, tự động tạo ra một tập hợp đa dạng các tác vụ meta-training từ corpus không nhãn quy mô lớn và học rõ ràng để thích ứng nhanh qua các tác vụ này. Để đảm bảo tính đa dạng tác vụ, chúng tôi ban đầu thiết kế một bộ sưu tập các tác vụ meta-training tự giám sát anchor với các format khác nhau. Và sau đó một phương pháp tăng cường tác vụ dựa trên curriculum được đề xuất thêm để làm giàu phân phối tác vụ một cách động theo khả năng mô hình hiện tại.

Đối với vấn đề thứ hai, chúng tôi tích hợp một hàm điều chuẩn meta-gradient vào học meta-prompt. Khi chúng tôi mô phỏng sự chuyển đổi phân phối thông qua tăng cường tác vụ, các tham số điều chuẩn meta-gradient được tối ưu hóa cùng để căn chỉnh hướng gradient qua các phân phối khác nhau trong paradigm học meta-prompt được đề xuất của chúng tôi. Do đó, trong các tác vụ downstream, các tham số được tối ưu hóa này có thể được sử dụng trực tiếp để biến đổi gradient thô trên các mẫu few-shot thành hướng khái quát hóa domain, ngăn chặn điều chỉnh prompt overfitting với một số tương quan domain-specific.

Nhìn chung, những đóng góp của chúng tôi chủ yếu là ba khía cạnh:

(1) Chúng tôi đề xuất một khung học meta-prompt tự giám sát mới để khởi tạo prompt mềm tốt hơn, trong đó chỉ sử dụng dữ liệu pre-training không nhãn để xây dựng các tác vụ meta-training khác nhau với tăng cường tác vụ dựa trên curriculum để làm giàu tác vụ thêm.

(2) Chúng tôi kết hợp một hàm điều chuẩn meta-gradient mới vào khung học meta-prompt của chúng tôi, meta-học để biến đổi gradient thô trong quá trình học few-shot thành hướng khái quát hóa domain, do đó ngăn chặn điều chỉnh prompt overfitting với các tương quan domain-specific.

(3) Các thí nghiệm toàn diện về học few-shot và khái quát hóa domain xác nhận tính ưu việt của phương pháp chúng tôi, thậm chí vượt trội hơn điều chỉnh toàn bộ mô hình trong học few-shot. Nó cũng thể hiện khả năng khái quát hóa domain mạnh hơn.

## 2 Nghiên cứu liên quan

**Điều chỉnh Prompt Mềm.** Điều chỉnh prompt mềm là một trong những phương pháp điều chỉnh hiệu quả tham số được sử dụng rộng rãi nhất trong NLP (Liu et al., 2023) và các tác vụ vision-language (Zhou et al., 2022; Li et al., 2023a), chỉ điều chỉnh một số lượng nhỏ tham số (thêm) để đạt được hiệu suất mạnh. Cụ thể, nó đóng băng các tham số PLM và thêm vào trước một số embedding liên tục có thể huấn luyện (tức là prompt mềm) vào chuỗi đầu vào (Lester et al., 2021) hoặc mọi lớp của mô hình pre-trained (Li and Liang, 2021; Liu et al., 2022).

Để huấn luyện hiệu quả các prompt mềm thích ứng tác vụ trong các tình huống few-shot, một số nghiên cứu (Vu et al., 2022; Asai et al., 2022; Sun et al., 2022) sử dụng các kỹ thuật thích ứng tác vụ, thu được prompt nguồn từ các tác vụ nguồn theo cách có giám sát và nội suy chúng vào prompt mục tiêu. Các nghiên cứu khác tập trung vào huấn luyện các khởi tạo prompt được cải thiện. PPT (Gu et al., 2022) pre-train các prompt mềm với một số tác vụ tự giám sát trên corpus không nhãn, nhưng nó không tối ưu hóa rõ ràng khả năng thích ứng nhanh của mô hình. MetaPrompting (Hou et al., 2022) sử dụng meta-learning có giám sát cho khởi tạo prompt mềm, chia mỗi dataset thành hai tập với các lớp dữ liệu rời rạc. Một phần được sử dụng để khởi tạo prompt mềm trong khi phần khác phục vụ như tác vụ downstream. So sánh, SUPMER khác với MetaPrompting ở những điểm sau: 1) đối với mỗi tác vụ downstream MetaPrompting tập trung vào một dataset có giám sát cố định để khởi tạo lại prompt mềm, trong khi SUPMER có thể khái quát hóa universal đến các tác vụ chưa thấy khác nhau với corpus không nhãn quy mô lớn để khởi tạo; 2) MetaPrompting không đóng băng tham số PLM, trong khi SUPMER chỉ điều chỉnh prompt mềm như các phương pháp điều chỉnh prompt mềm chung làm.

**Meta-Learning.** Meta-learning, còn được biết đến là học để học, tối ưu hóa khả năng học các tác vụ mới một cách nhanh chóng và hiệu quả, sử dụng kinh nghiệm từ các tác vụ đã thấy trước đó. Nó có thể được phân loại thành ba loại: các phương pháp dựa trên metric (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017), các phương pháp dựa trên mô hình (Graves et al., 2014; Mishra et al., 2018; Qiao et al., 2018), và các phương pháp dựa trên gradient (Hochreiter et al., 2001; Ravi and Larochelle, 2017; Nichol et al., 2018; Li et al., 2020). Trong nghiên cứu này, chúng tôi tập trung vào thuật toán meta-learning dựa trên gradient (tức là MAML Finn et al., 2017). So với các phương pháp meta-learning điển hình dựa vào các tác vụ meta-training được chú thích bởi con người, chúng tôi tự động tạo ra các tác vụ phong phú theo cách tự giám sát, cũng tích hợp một hàm điều chuẩn meta-gradient vào MAML để hướng gradient theo hướng khái quát hóa domain.

## 3 Phương pháp

Trong phần này, chúng tôi mô tả toàn bộ khung của SUPMER (được thể hiện trong Hình 2). Với các điều kiện tiên quyết được xác định trước, chúng tôi đầu tiên giới thiệu cách xây dựng các tác vụ meta tự giám sát anchor và nền tảng của tăng cường tác vụ để làm dày đặc phân phối tác vụ. Sau đó chúng tôi trình bày chi tiết về mô hình SUPMER, bao gồm hàm điều chuẩn meta-gradient. Cuối cùng, chúng tôi nâng cấp phương pháp tăng cường tác vụ ban đầu thành phương pháp dựa trên curriculum. Bên cạnh đó, chúng tôi chính thức hóa tất cả các tác vụ theo định dạng text-to-text theo kiểu T5 (Raffel et al., 2020).

### 3.1 Điều kiện tiên quyết

**Điều chỉnh Prompt.** Trong điều chỉnh prompt (Lester et al., 2021), cho một mẫu huấn luyện (xi, yi) từ tác vụ Dτ, chúng tôi áp dụng một template prompt P chuyển đổi xi thành chuỗi mới P(xi) và sau đó nối một tập hợp prompt mềm θ vào đầu P(xi). Và verbalizer V đóng vai trò ánh xạ yi thành một số token nhãn tương ứng V(yi) trong từ vựng của PLM. Vì vậy mục tiêu của điều chỉnh prompt có thể được công thức hóa như sau:

arg min θ LDτ(θ) = arg max θ Σ(xi,yi)∈Dτ log p(⟨X⟩=V(yi)|[θ;P(xi)];θ) (1)

trong đó θ biểu thị embedding prompt mềm (tham số duy nhất có thể điều chỉnh trong điều chỉnh prompt). ⟨X⟩ cho phép PLM dự đoán token mục tiêu tại các vị trí được mask và [·;·] là phép nối.

**Model-Agnostic Meta-Learning.** Giả sử có quyền truy cập vào phân phối tác vụ p(T), mục tiêu của meta-learning là sử dụng các tác vụ τi∼p(T), được gọi là các tác vụ meta-training hoặc meta tasks, để huấn luyện một thủ tục học khái quát hóa đến các tác vụ chưa thấy từ phân phối. Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) là một phương pháp meta-learning tối ưu hóa hai cấp dựa trên gradient, bao gồm một vòng lặp trong học cụ thể tác vụ và vòng lặp ngoài thích ứng nhanh qua các tác vụ.

Cụ thể, một tác vụ τ được cấu thành từ tập support Dsτ và tập query Dqτ. Trong vòng lặp trong của MAML, một mô hình học để thích ứng với tác vụ mới τi sử dụng tập support của nó theo cách sau:

θ'i = θ - α1∇θLDsτi(θ) (2)

trong đó α1 là tỷ lệ học vòng lặp trong và θ là tham số của mô hình. Và tham số được tối ưu hóa θ'i sau đó được đánh giá trên tập query của tác vụ τi với hàm loss LDqτi. Trong vòng lặp ngoài, loss này qua các tác vụ meta-training được coi là loss huấn luyện cuối cùng để cập nhật θ:

θ ← θ - β1∇θ Σ τi∼p(T) LDqτi(θ'i) (3)

trong đó β1 là tỷ lệ học vòng lặp ngoài.

### 3.2 Xây dựng các Meta Task Anchor

Các dataset có giám sát với lượng lớn dữ liệu có nhãn thường không có sẵn trong nhiều tác vụ NLP. Trong khi dữ liệu không nhãn dễ tiếp cận hơn và thường bao phủ các khái niệm ngữ nghĩa rộng hơn. Vì vậy chúng tôi sử dụng dữ liệu không nhãn từ một corpus lớn để tạo ra các tác vụ meta-training tự giám sát anchor.

Dữ liệu không nhãn đầu tiên được nhóm thành các cluster khác nhau. Chúng tôi sử dụng PLM để thu được các embedding có ý nghĩa ngữ nghĩa cho các câu trong corpus, và sau đó áp dụng K-means không giám sát để cluster các câu không nhãn này. Dựa trên kết quả của K-means, chúng tôi thiết kế ba định dạng khác nhau của các tác vụ meta-training tự giám sát: phân loại cặp câu, phân loại đa lựa chọn, và phân loại câu đơn.

Cụ thể, phân loại cặp câu liên quan đến việc dự đoán xem hai câu có liền kề trong cùng một tài liệu hay từ cùng một cluster sau khi clustering K-means. Phân loại đa lựa chọn xác định câu đúng trong số nhiều ứng viên, câu này hoặc liền kề với câu truy vấn hoặc từ cùng cluster của nó. Và phân loại câu đơn nhằm liên kết mỗi câu với nhãn cluster đúng của nó, như được xác định bởi K-means. Trên cơ sở này, đối với mỗi định dạng tác vụ, chúng tôi phân phối dữ liệu meta-training thành các tác vụ khác nhau để xây dựng các tác vụ meta-training anchor với phân phối tác vụ cân bằng tốt. Chúng tôi nhóm các mẫu có embedding tương tự vào cùng tác vụ dựa trên kết quả của K-means. Và chúng tôi đưa ra mô tả chi tiết hơn về xây dựng tác vụ meta-training anchor trong Phụ lục A.2.

### 3.3 Tăng cường Tác vụ Vanilla

Với một tập hợp các tác vụ meta-training anchor, trong phần này chúng tôi đầu tiên giới thiệu tăng cường tác vụ vanilla để làm dày đặc phân phối tác vụ. Mở rộng ý tưởng của mixup (Zhang et al., 2018), chúng tôi tăng cường tập tác vụ thông qua nội suy tác vụ, kết hợp tuyến tính các đặc trưng và nhãn tương ứng của các mẫu từ tập query trong các tác vụ khác nhau. Trong §3.5 chúng tôi nâng cấp thêm phương pháp tăng cường tác vụ vanilla thành phương pháp dựa trên curriculum, điều khiển động việc nội suy tác vụ theo khả năng mô hình hiện tại.

Cụ thể, đối với một tác vụ bao gồm tập support và tập query, chúng tôi ký hiệu các biểu diễn ẩn của các mẫu tập query trong tác vụ τk là Hq. Cho một tác vụ anchor τi, đầu tiên chúng tôi chọn ngẫu nhiên một tác vụ khác τj. Trong khi giữ nguyên tập support của τi, chúng tôi tái cấu trúc tập query của nó bằng cách nội suy trên các biểu diễn ẩn (Hqi, Hqj) và nhãn tương ứng (Yqi, Yqj) từ các tập query trong τi và τj, có thể được thực hiện bằng mixup:

H̃qi = (1-λ)Hqi + λHqj, Ỹqi = (1-λ)Yqi + λYqj (4)

trong đó tỷ lệ trộn λ∈[0,1] được lấy từ phân phối Beta Beta(α, α), và α là một siêu tham số. Quá trình tăng cường tác vụ không chỉ làm giàu phân phối tác vụ, mà còn mô phỏng sự chuyển đổi phân phối giữa tập support và tập query trong một tác vụ, vì chúng tôi chỉ tận dụng nội suy giữa các tập query của các tác vụ meta-training anchor khác nhau. Và trong §3.4 chúng tôi sẽ thể hiện hiệu ứng của sự chệch lệch phân phối này.

### 3.4 Học Meta-Prompt với Điều chuẩn Meta-Gradient

Trong phần này chúng tôi giới thiệu thuật toán của khung học meta-prompt của chúng tôi, đây là một paradigm meta-learning hai cấp học một khởi tạo prompt mềm universal tác vụ θ cho thích ứng hiệu quả. Và nó cùng meta-học một hàm điều chuẩn meta-gradient ψϕ biến đổi gradient thô thành hướng khái quát hóa domain để ngăn chặn điều chỉnh prompt khỏi overfitting.

Cụ thể, xem xét rằng việc cập nhật vòng lặp trong của MAML (tức là Eq. (2)) trên các mẫu hạn chế có thể overfit với một số tương quan domain-specific, chúng tôi đề xuất học một hàm điều chuẩn gradient ψϕ(·), thực hiện biến đổi trực tiếp đến gradient thô thu được từ tập support Dsτi. Hàm này đầu tiên thực hiện biến đổi affine h(·) (ví dụ, xoay) để điều chỉnh gradient thô g, và sau đó một vector cổng cập nhật z được sử dụng để kết hợp g và h(g) thành gradient cuối cùng:

ψϕ(g) = z·h(g) + (1-z)·g (5)

Rõ ràng, giá trị của z có thể được sử dụng để điều khiển mức độ gradient được biến đổi h(g) đóng góp vào đầu ra của ψϕ(g). Chúng tôi hy vọng xác định trọng số này dựa trên chính các mẫu đầu vào, đặt z=σ(WH+b), trong đó H là các biểu diễn ẩn của mẫu đầu vào. Chính thức, bây giờ chúng tôi biến đổi Eq. (2) thành:

θ'i = θ - α1ψϕ(∇θLDsτi(θ)) (6)

Sau khi thích ứng embedding prompt mềm với tập support Dsτi, trong vòng lặp ngoài chúng tôi tối ưu hóa khởi tạo prompt θ dựa trên các embedding được thích ứng này θ' thông qua Eq. (3). Bên cạnh đó, các tham số điều chuẩn meta-gradient ϕ cũng được tối ưu hóa sử dụng cùng loss để học biến đổi gradient tốt hơn, với β2 là tỷ lệ học:

ϕ ← ϕ - β2∇ϕ Σ τi∼p(T) LDqτi(θ'i) (7)

Nhìn chung, mục tiêu học meta-prompt tổng thể có thể được công thức hóa như sau:

arg min θ,ϕ Σ τi∼p(T) LDqτi(θ - α1ψϕ(∇θLDsτi(θ))) (8)

**Điều chỉnh Prompt Downstream.** Khung học meta-prompt ở trên chỉ yêu cầu thực hiện một lần. Khởi tạo prompt được tối ưu hóa θ* và tham số điều chuẩn meta-gradient ϕ* sau đó universal cho các tác vụ downstream khác nhau. Trong quá trình điều chỉnh prompt downstream, chúng tôi cố định ϕ* và thích ứng thêm θ* với các tác vụ kiểm tra như Eq. (6).

**Phân tích SUPMER.** Ở đây chúng tôi đưa ra một số phân tích về cách SUPMER có thể tăng cường khả năng khái quát hóa, với chứng minh hoàn chỉnh hơn trong Phụ lục A.1. Cho rằng x=θ-α1ψϕ(∇θLDs(θ)) và x0=θ, tập trung vào một tác vụ meta-training duy nhất, chúng tôi có thể áp dụng khai triển Taylor bậc nhất quanh điểm x0 để tái công thức hóa Eq. (8) như:

∵ LDq(x) = LDq(x0) + L'Dq(x0)(x-x0)
∴ arg min θ,ϕ LDq(θ - α1ψϕ(∇θLDs(θ)))
= arg min θ,ϕ LDq(θ) - α1∇θLDq(θ)·ψϕ(∇θLDs(θ)) (9)

Dựa trên thảo luận trên, chúng tôi có thể đạt được những kết luận sau: (1) Việc cập nhật θ giảm thiểu loss mong đợi trên tập query. (2) Tối ưu hóa cả θ và ϕ tối đa hóa tích vô hướng giữa gradient được điều chỉnh từ tập support và gradient từ tập query. Tích vô hướng của hai vector lớn hơn nếu chúng theo hướng tương tự. Nhớ lại rằng chúng tôi mô phỏng sự chuyển đổi phân phối giữa tập support và tập query, việc tối ưu hóa θ và ϕ cố gắng căn chỉnh hướng gradient qua các phân phối khác nhau. Để cải thiện việc căn chỉnh giữa các gradient domain-specific, các tham số điều chuẩn gradient ϕ được tối ưu hóa để giữ lại một số thông tin domain-invariant của dữ liệu meta-training và sau đó có thể được sử dụng để điều chỉnh gradient thô thu được từ các mẫu few-shot thành hướng khái quát hóa domain trong điều chỉnh prompt downstream, do đó tránh overfitting với một số tương quan giả mạo.

### 3.5 Tăng cường Tác vụ dựa trên Curriculum

Trong §3.4 chúng tôi cho thấy SUPMER có thể giúp căn chỉnh hướng tối ưu hóa qua hai phân phối có độ lệch, được mô phỏng bằng cách thực hiện tăng cường tác vụ chỉ trên các tập support. Từ Eq. (4) rõ ràng rằng tỷ lệ trộn λ của mixup điều khiển mức độ độ lệch phân phối, với λ lớn hơn dẫn đến độ lệch đáng kể hơn. Tuy nhiên, trong phương pháp được thảo luận trước đó, λ được lấy mẫu từ một phân phối Beta cố định. Trong phần này, chúng tôi đề xuất một cách tiếp cận lấy mẫu linh hoạt hơn, nâng cấp phương pháp tăng cường tác vụ ban đầu thành phương pháp dựa trên curriculum, tăng dần độ khó tác vụ và đạt được sự chuyển đổi phân phối hợp lý hơn.

Tăng cường tác vụ dựa trên curriculum điều chỉnh động các tham số của phân phối Beta, từ đó chúng tôi lấy mẫu tỷ lệ trộn λ. Cụ thể, một lô các meta task được lấy mẫu trong mỗi epoch huấn luyện. Đối với mỗi tác vụ, chúng tôi có thể thu được gradient trên tập support gsi và gradient trên tập query gqi, cùng với độ tương tự cosine của chúng. Chúng tôi tận dụng độ tương tự cosine trung bình sk-1 của tất cả các tác vụ trong một lô trong epoch cuối cùng để thu được tỷ lệ trộn λk cho epoch hiện tại k:

λk = Beta(α, bkα)
bk = (m(1+sk-1)/2 - 1)/(m-1),
trong đó sk-1 = (1/|B|)·Σ|B|i=1 (gsi·gqi)/(||gsi||·||gqi||) (10)

trong đó m là tham số đường cong. Theo cách này, khi mô hình của chúng tôi không có khả năng căn chỉnh hướng tối ưu hóa qua các phân phối khác nhau ở đầu, một λ nhỏ hơn là tốt hơn để tạo ra độ lệch phân phối nhỏ hơn. Sau đó λ có xu hướng tăng dần khi khả năng của mô hình cải thiện, dẫn đến độ lệch phân phối lớn hơn và tăng độ khó tác vụ tương ứng.

Chúng tôi trình bày mã giả của SUPMER trong Phụ lục A.4.

## 4 Thí nghiệm

### 4.1 Thiết lập Thí nghiệm

Chúng tôi đánh giá phương pháp của chúng tôi trong hai thiết lập vấn đề: 1) Học few-shot với các tác vụ downstream NLP khác nhau; 2) Khái quát hóa domain.

**Học Few-shot.** Chúng tôi xem xét 6 tác vụ downstream với 12 dataset: 1) các dataset phân tích tình cảm SST-2, SST-5 (Socher et al., 2013), MR (Pang and Lee, 2005) và CR (Hu and Liu, 2004); 2) dataset phân loại chủ quan SUBJ (Pang and Lee, 2004); 3) dataset phân loại câu hỏi TREC (Voorhees and Tice, 2000); 4) các dataset suy luận ngôn ngữ tự nhiên CB (De Marneffe et al., 2019) và RTE (Wang et al., 2019); 5) dataset trả lời câu hỏi QNLI (Rajpurkar et al., 2016); 6) dataset phân biệt nghĩa từ WiC (Pilehvar and Camacho-Collados, 2019); 7) các dataset phát hiện paraphrase MRPC (Dolan and Brockett, 2005) và QQP. Theo Karimi Mahabadi et al. (2022), đối với mỗi dataset chúng tôi lấy mẫu 16 instance trên mỗi nhãn từ tập huấn luyện ban đầu để tạo thành tập huấn luyện và validation cho học few-shot.

**Khái quát hóa Domain.** Sau đó chúng tôi thiết kế một vấn đề thách thức hơn về khái quát hóa domain zero-shot trong tác vụ phân tích tình cảm. Các thí nghiệm của chúng tôi bao gồm 6 domain qua 3 dataset: 1) dataset đánh giá Amazon (Blitzer et al., 2007) chứa đánh giá về Sách (B), DVD (D), Điện tử (E) và Thiết bị nhà bếp (K); 2) dataset đánh giá hãng hàng không (A) (Nguyen, 2015; Ziser and Reichart, 2018); 3) domain nhà hàng (R) thu được từ dataset Yelp (Zhang et al., 2015). Chúng tôi chọn A làm domain nguồn và năm domain khác (B, D, E, K, R) cấu thành các domain mục tiêu. Trên cơ sở này, chúng tôi lấy mẫu 16 instance trên mỗi nhãn từ tập huấn luyện của domain nguồn để điều chỉnh prompt mềm. Và sau đó chúng tôi trực tiếp sử dụng prompt mềm đã học từ domain nguồn để đánh giá hiệu suất trên tập test của mỗi domain.

### 4.2 Chi tiết Thí nghiệm

**Baseline.** Các thí nghiệm của chúng tôi được xây dựng trên một mô hình quy mô nhỏ hơn, T5-base (Raffel et al., 2020), và sau đó trên một mô hình điều chỉnh instruction quy mô lớn hơn, Flan-T5-XL (Chung et al., 2022). Đối với cả hai mô hình backbone, chúng tôi sử dụng các baseline sau: (1) các phương pháp điều chỉnh prompt với cùng số lượng tham số có thể điều chỉnh như SUPMER: điều chỉnh prompt vanilla (PT Lester et al., 2021), PPT (Gu et al., 2022), Unified-PPT (Gu et al., 2022), và MetaPT (Huang et al., 2022). (2) các phương pháp với nhiều tham số có thể điều chỉnh hơn: Prefix-Tuning (Li and Liang, 2021), P-tuning-v2 (Liu et al., 2022), điều chỉnh toàn bộ mô hình (FT). Hơn nữa, cho rằng FLAN-T5-XL cũng được thiết kế với suy luận few-shot, chúng tôi so sánh thêm với hai phương pháp baseline trên FLAN-T5-XL, tức là suy luận zero-shot và suy luận few-shot, trực tiếp sử dụng Flan-T5-XL cho đánh giá downstream. Chúng tôi liệt kê chi tiết các baseline trong Phụ lục B.

**Chi tiết Triển khai.** Chúng tôi giải quyết tất cả các tác vụ downstream theo định dạng text-to-text và chạy mỗi thí nghiệm với 5 random seed khác nhau. Đối với tất cả các phương pháp điều chỉnh prompt, chúng tôi theo Lester et al. (2021) để thiết kế prompt mềm bao gồm 100 token mềm, với tham số có thể điều chỉnh ít hơn nhiều so với điều chỉnh toàn bộ mô hình. Đối với SUPMER của chúng tôi, theo PPT (Gu et al., 2022) chúng tôi lấy mẫu 10GB dữ liệu từ OpenWebText (Gokaslan et al., 2019), một corpus không nhãn quy mô lớn, để xây dựng các tác vụ meta-training tự giám sát. Giai đoạn meta-training chỉ yêu cầu thực hiện một lần. Trong điều chỉnh prompt downstream, chúng tôi đóng băng các tham số điều chuẩn meta-gradient và prompt mềm là tham số duy nhất có thể điều chỉnh. Chúng tôi đưa ra chi tiết hơn về siêu tham số huấn luyện trong Phụ lục C.

### 4.3 Kết quả Chính

Bảng 1 và Bảng 2 cho thấy kết quả chính của học few-shot và khái quát hóa domain. Từ kết quả, chúng tôi có các quan sát sau.

Thứ nhất, trong học few-shot, SUPMER đạt được hiệu suất tốt hơn tất cả baseline trên 10 trong 12 dataset, dù sử dụng T5-base hay Flan-T5-XL làm backbone. Và độ chính xác trung bình của SUPMER trên tất cả dataset đạt 71.3% trên T5-base, vượt trội đáng kể so với các baseline khác (ví dụ, cải thiện hiệu suất +1.3 điểm so với FT). Đáng chú ý, khi sử dụng Flan-T5-XL lớn hơn làm backbone, SUPMER thể hiện cải thiện hiệu suất thậm chí đáng kể hơn (ví dụ, cải thiện hiệu suất trung bình +2.5 điểm so với FT), điều này chỉ ra rằng phương pháp của chúng tôi mở khóa khả năng lớn hơn cho các mô hình mạnh hơn đã trải qua điều chỉnh instruction với số lượng tham số cao hơn.

Cụ thể, SUPMER vượt trội nhất quán so với tất cả các phương pháp điều chỉnh prompt khác có cùng số lượng tham số có thể điều chỉnh trên tất cả dataset. Điều này cho thấy phương pháp của chúng tôi cung cấp prompt mềm với khả năng khái quát hóa few-shot tốt hơn. Và đáng chú ý là SUPMER sử dụng chính xác cùng dữ liệu không nhãn như PPT và Unified-PPT cho khởi tạo prompt mềm. Tuy nhiên nó vượt trội đáng kể so với hai baseline này, chứng tỏ rằng cải thiện hiệu suất chủ yếu do phương pháp luận của chúng tôi hơn là chính dữ liệu meta-training. Ngoài ra, SUPMER vượt trội so với các phương pháp baseline có nhiều tham số có thể điều chỉnh hơn (ví dụ, điều chỉnh toàn bộ mô hình) trên đa số dataset, đạt được hiệu suất vượt trội với ít tham số hơn.

Thứ hai, SUPMER vượt trội so với tất cả baseline trong hầu hết các thiết lập khái quát hóa domain. Ví dụ, so với MetaPT meta-train prompt mềm với dataset phân tích tình cảm có giám sát, SUPMER thể hiện cải thiện trung bình 1.1% trên T5-base và 1.4% trên Flan-T5-XL. Vì vậy có thể suy ra rằng SUPMER thể hiện độ bền vững mạnh hơn đối với sự chuyển đổi domain, thể hiện khả năng khái quát hóa tốt hơn đến các tác vụ hoặc domain chưa thấy.

Thứ ba, đối với cả học few-shot và khái quát hóa domain trên Flan-T5-XL, SUPMER thể hiện hiệu suất vượt trội trên hầu hết tất cả dataset và domain so với suy luận few-shot. Nó cung cấp bằng chứng thêm rằng đối với các LM như Flan-T5-XL có khả năng suy luận few-shot vốn có, phương pháp của chúng tôi có thể tăng cường đáng kể khả năng của chúng trong một chiến lược điều chỉnh hiệu quả tham số, mà không cung cấp bất kỳ ví dụ in-context nào trong quá trình suy luận.

Thứ tư, SUPMER cũng dẫn đến phương sai thấp hơn trên hầu hết dataset. Học few-shot thường khét tiếng về tính bất ổn. Và trong phương pháp của chúng tôi, chúng tôi giữ điều chỉnh prompt few-shot ổn định hơn.

### 4.4 Nghiên cứu Loại bỏ

**Phân tích Khái quát hóa.** Hình 3 cho thấy xu hướng hiệu suất cho mỗi phương pháp sau các bước huấn luyện khác nhau trên dataset CB và MRPC với mô hình T5-base. Nó minh họa rằng điều chỉnh prompt few-shot hội tụ chậm với hiệu suất thường thể hiện sự suy giảm tổng thể trong các bước huấn luyện cuối cùng vì chúng có thể dễ dàng dẫn đến overfitting. So sánh, SUPMER đạt được khái quát hóa few-shot nhanh hơn, mạnh hơn và lâu dài hơn. Nó không chỉ tăng tốc sự hội tụ đến hiệu suất tối ưu thực hiện thích ứng nhanh, mà còn duy trì nhất quán hiệu suất tối ưu của nó qua các thời kỳ huấn luyện kéo dài.

**Hiệu ứng Kích thước Mẫu.** Chúng tôi cũng thảo luận về cách hiệu suất của SUPMER và các baseline khác thay đổi khi số lượng mẫu huấn luyện tăng lên trên SST-5 và SUBJ. Như được thể hiện trong Hình 4, với T5-base làm PLM cơ bản, khi số lượng mẫu huấn luyện trên mỗi nhãn tăng từ 4 đến 64, SUPMER nhất quán tốt hơn các phương pháp điều chỉnh prompt khác. Và khoảng cách hiệu suất giữa các phương pháp này được giảm dần khi số lượng dữ liệu huấn luyện tăng.

**Tự giám sát so với Có giám sát.** Để minh họa rằng meta-learning tự giám sát có thể khái quát hóa tốt hơn đến các tác vụ chưa thấy so với meta-learning có giám sát, chúng tôi cũng thu thập một tập hợp dataset có nhãn (đảm bảo không chồng chéo với các dataset kiểm tra downstream) để công thức hóa các tác vụ meta-training cho khởi tạo prompt mềm và tiến hành các thí nghiệm học few-shot trên T5-base. Kết quả được hiển thị trong Bảng 3 (hàng 1 và 2). Vì dữ liệu có nhãn thu thập của chúng tôi chứa nhiều dataset phân tích tình cảm (ví dụ, Yelp5), SUPMER (chỉ có nhãn) và SUPMER (chỉ không nhãn) thể hiện gần gũi trong hiệu suất của chúng trên các tác vụ phân tích tình cảm (tức là SST-2, SST-5, MR, CR). Nhưng trong các tác vụ khác, sử dụng dữ liệu không nhãn nhất quán đạt được kết quả tốt hơn so với chỉ sử dụng dữ liệu có nhãn, cũng với độ chính xác trung bình cao hơn trên tất cả dataset, xác nhận tính ưu việt của meta-learning tự giám sát.

**Hiệu ứng Tích hợp Dữ liệu Có nhãn.** Để khám phá thêm tác động của việc tích hợp dữ liệu có nhãn và chứng minh hiệu quả của SUPMER sau khi tích hợp này, chúng tôi kết hợp dữ liệu meta-training không nhãn ban đầu với dữ liệu có nhãn thu thập của chúng tôi đã đề cập ở trên, với tỷ lệ trộn có nhãn với không nhãn là 1:2. Dữ liệu kết hợp được sử dụng để xây dựng các tác vụ meta-training để meta-train SUPMER. Hơn nữa, theo PPT (Gu et al., 2022) và MetaPT (Huang et al., 2022), chúng tôi cũng tận dụng pre-training và MAML vanilla để khởi tạo prompt mềm sử dụng cùng dữ liệu kết hợp. Kết quả thí nghiệm của học few-shot trên T5-base được thể hiện trong Bảng 3 (hàng 3-5). Thứ nhất, chúng tôi có thể thấy rằng SUPMER (có nhãn+không nhãn) vượt trội so với SUPMER (không nhãn) và SUPMER (có nhãn) vì nó cho phép chúng tôi khai thác ưu điểm chất lượng cao của dữ liệu có nhãn trong khi cũng khai thác các khái niệm ngữ nghĩa rộng hơn được đóng gói bởi dữ liệu không nhãn. Thứ hai, sau khi tích hợp dữ liệu có nhãn, SUPMER vẫn nhất quán thể hiện hiệu suất vượt trội đáng kể so với các phương pháp baseline sử dụng cùng dữ liệu cho khởi tạo prompt, điều này nhấn mạnh thêm hiệu quả của SUPMER.

**Hiệu ứng Các Thành phần Riêng lẻ.** Chúng tôi huấn luyện các mô hình loại bỏ sau. 1) chỉ sp / mc / ss: chúng tôi giữ lại phân loại cặp câu / phân loại đa lựa chọn / phân loại câu đơn như định dạng tác vụ meta-training anchor duy nhất. 2) w/o ta: chúng tôi hoàn toàn loại bỏ phương pháp tăng cường tác vụ. 3) w/o curriculum: chúng tôi chỉ giữ lại tăng cường tác vụ vanilla mà không có ý tưởng dựa trên curriculum. 4) w/o mgr: chúng tôi loại bỏ hàm điều chuẩn meta-gradient. Tất cả thí nghiệm theo thiết lập trong §4.1 và được tiến hành trên T5-base. Chúng tôi báo cáo độ chính xác trung bình của học few-shot và khái quát hóa domain trong Bảng 4. Kết quả chi tiết hơn trong Phụ lục D.

Kết quả của Hàng 1-3 cho thấy xem xét các định dạng tác vụ đa dạng trong meta-training giúp khái quát hóa hiệu quả đến các tác vụ khác nhau vì các tác vụ downstream thường chứa các định dạng tác vụ khác nhau. Hàng 4 và Hàng 5 nhấn mạnh rằng tăng cường tác vụ đóng vai trò thiết yếu trong khung của chúng tôi, với tăng cường dựa trên curriculum làm giàu thêm phân phối tác vụ và mô phỏng thực tế sự chuyển đổi phân phối. Hơn nữa, Hàng 6 xác nhận tính ưu việt của điều chuẩn meta-gradient trong việc tránh overfitting với một số tương quan domain-specific, do đó đạt được hiệu suất tốt hơn.

## 5 Kết luận

Trong bài báo này, chúng tôi trình bày SUPMER, một khung học meta-prompt tự giám sát với điều chuẩn meta-gradient. Với một tập hợp đa dạng các tác vụ meta-training tự giám sát được thiết kế tốt, SUPMER cùng meta-học một khởi tạo prompt universal và một hàm điều chuẩn gradient hiệu quả cho khái quát hóa few-shot hiệu quả. Các thí nghiệm rộng rãi về học few-shot và khái quát hóa domain cho thấy SUPMER vượt trội so với các phương pháp prompt khác và điều chỉnh toàn bộ mô hình, đạt được hiệu suất tốt nhất.

## Hạn chế

Mặc dù SUPMER hoạt động xuất sắc trong nhiều tình huống vấn đề khác nhau, vẫn tồn tại một số hạn chế trong nghiên cứu của chúng tôi: 1) Chúng tôi không tiến hành bất kỳ hoạt động lọc hoặc làm sạch dữ liệu nào đối với dữ liệu meta-training, có thể dẫn đến việc bao gồm một số nội dung thiên vị. 2) Các thí nghiệm của chúng tôi chỉ được tiến hành trên các tác vụ tiếng Anh, và cũng không bao gồm một số loại tác vụ NLP (ví dụ, tạo ngôn ngữ Li et al., 2022c) hoặc các tác vụ vision-language (Zhang et al., 2022b; Li et al., 2022b; Zhang et al., 2019; Li et al., 2021).

Để giải quyết những hạn chế này, trong tương lai chúng tôi dự định tiến hành làm sạch và lọc thêm trên dữ liệu meta-training hiện tại. Bên cạnh đó, chúng tôi dự định đánh giá hiệu suất few-shot của khung chúng tôi trong thiết lập đa ngôn ngữ và cũng mở rộng phạm vi các tác vụ, bao gồm truy xuất (Pan et al., 2023), tạo ngôn ngữ (Li et al., 2022c) và các tác vụ vision-language (Li et al., 2023b; Chen et al., 2023; Li et al., 2022a; Zhang et al., 2022a). Hơn nữa, chúng tôi hy vọng nghiên cứu của chúng tôi có thể mở đường cho nghiên cứu tương lai về việc tận dụng tốt hơn các phương pháp hiệu quả tham số trong thiết lập few-shot.

## Lời cảm ơn

Nghiên cứu này đã được hỗ trợ một phần bởi Zhejiang NSF (LR21F020004), Các dự án Nghiên cứu và Phát triển Trọng điểm tỉnh Chiết Giang (Số 2023C01030, 2023C01032), NSFC (Số 62272411), Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (2018AAA0101900), Ant Group và Viện Nghiên cứu Công nghệ Tiên tiến Liên kết Alibaba-Đại học Zhejiang.
