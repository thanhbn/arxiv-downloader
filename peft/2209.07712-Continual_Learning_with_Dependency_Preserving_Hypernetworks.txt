# 2209.07712.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2209.07712.pdf
# File size: 446819 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Continual Learning with Dependency Preserving Hypernetworks
Dupati Srikar Chandra1Sakshi Varshney1P.K. Srijith1Sunil Gupta2
1Indian Institute of Technology, India2Deakin University, Australia
{ai20resch11004,cs16resch01002 }@iith.ac.in srijith@cse.iith.ac.in sunil.gupta@deakin.edu.au
Abstract
Humans learn continually throughout their lifespan by
accumulating diverse knowledge and fine-tuning it for fu-
ture tasks. When presented with a similar goal, neural
networks suffer from catastrophic forgetting if data distri-
butions across sequential tasks are not stationary over the
course of learning. An effective approach to address such
continual learning (CL) problems is to use hypernetworks
which generate task dependent weights for a target net-
work. However, the continual learning performance of ex-
isting hypernetwork based approaches are affected by the
assumption of independence of the weights across the lay-
ers in order to maintain parameter efficiency. To address
this limitation, we propose a novel approach that uses a de-
pendency preserving hypernetwork to generate weights for
the target network while also maintaining the parameter ef-
ficiency. We propose to use recurrent neural network (RNN)
based hypernetwork that can generate layer weights effi-
ciently while allowing for dependencies across them. In ad-
dition, we propose novel regularisation and network growth
techniques for the RNN based hypernetwork to further im-
prove the continual learning performance. To demonstrate
the effectiveness of the proposed methods, we conducted ex-
periments on several image classification continual learn-
ing tasks and settings. We found that the proposed methods
based on the RNN hypernetworks outperformed the base-
lines in all these CL settings and tasks.
1. Introduction
There are various applications where a computational
system has to learn from a stream of data continually and
adapt to the environment by using knowledge gained from
past experiences. For example, Autonomous agents in the
real world have to learn over continuous streams of data
and need to remember the information from various non-
stationary distributions without forgetting [20]. Deep neural
networks have achieved high performance on many images
classification benchmarks, comparable or even better than
humans. However, while learning a new task, these net-works forget the knowledge gained from previous ones. The
process of forgetting knowledge or information gained from
previous tasks due to drastic weight alteration while learn-
ing new tasks is known as catastrophic forgetting [18]. If
data distributions across the sequential tasks is not station-
ary, the weights of the model alter drastically to classify the
new task, leading to forgetting of knowledge gained from
previous tasks. Ideally, performance of a newly learnt task
should not have an impact on previous one (or vice versa).
To overcome forgetting, computational systems or agents,
on one hand, should be plastic to acquire new information
and refine old information based on continuous input and,
on the other hand, it should be stable to prevent the novel in-
put from interfering with old information. This is referred to
asplasticity-stability dilemma [7, 19]. Continual learning
aims to develop machine learning and deep learning models
which are stable enough to retain information learnt from
old tasks but also has the required plasticity to learn new
tasks [20].
Continual learning techniques for neural networks has
gained significant attention recently [20]. Several contin-
ual learning methods have been proposed to avoid forget-
ting in neural networks. These can be broadly categorised
into three approaches, Regularisation techniques [9, 12],
dynamic architecture methods [2, 24] and replay based
methods [5, 21]. Recently, Continual learning with hy-
pernetworks has shown very promising results in dealing
with forgetting at a meta level by generating task specific
weights [4, 8, 28]. Hypernetwork is a meta neural network
which generates parameters for the main network associated
with the task (for instance, classification or regression net-
work) by considering some task related information. Dur-
ing training, instead of directly trying to update parameters
of the main network, hypernetwork parameters which gen-
erates them are updated. But generating the entire main net-
work parameters will require a larger hypernetwork as it re-
quires generating a very high dimensional output. Instead,
chunked hypernetworks [8, 28] generate them in smaller
chunks (chunk referred to subset of main network weights)
multiple times iteratively using smaller hypernetwork that is
reusable and also help in model compression significantly.arXiv:2209.07712v1  [cs.LG]  16 Sep 2022

--- PAGE 2 ---
A key limitation of chunked hypernetworks is that it as-
sumes the weight matrices associated with the chunks to
be independent, which affects the continual learning per-
formance significantly. Chunked hypernetwork [28] uses
a feed forward neural network to generate weights without
considering dependencies across chunks. We propose to use
recurrent neural networks (RNNs) , in order to capture de-
pendencies in weight matrix generation across the chunks.
But standard RNNs suffer from vanishing gradient problems
and may not be able to remember dependencies for a long
time. So, a variant of RNN ,LSTM [9] has been used to re-
member the weight matrix dependencies over a longer du-
ration. Thus, we propose LSTM based hypernetwork that
can efficiently generate weights of the main network while
also maintaining the dependencies across the chunks.
While learning multiple tasks in sequential manner, the
hypernetwork should remember the knowledge gained from
previous tasks and should also be able to forward that
knowledge to the upcoming tasks. To achieve this, we pro-
pose a novel hypernetwork regularisation technique, Im-
portance Weighted Regularisation (IWR) that can further
improve the performance of hypernetwork based continual
learning by enabling forward transfer while also retaining
previous information. IWR considers the importance of the
main network parameters and allows more flexibility to the
hypernetwork to adapt to the new task by considering this
importance. We also propose a network growth technique
for the LSTM based hypernetworks for continual learning.
The approach is based on the idea that the dependence
among the main network parameters across tasks remains
the same while their exact values could be different. This is
achieved by sharing the weights associated with the hidden
states in the LSTM across the tasks. We still learn the input
specific weights for each task separately. This approach im-
proves the performance of continual learning with no extra
regularisation and accelerates the model training. Our ex-
perimental results on real world data show that the proposed
approaches along with LSTM based hypernetwork can ef-
fectively mitigate catastrophic forgetting and significantly
improve the continual learning performance.
Our main contributions can be summarised as follows.
• We propose a novel dependency preserving LSTM hy-
pernetwork for continual learning.
• We propose a novel regularisation technique for hy-
pernetwork based continual learning and a network
growth technique specifically for the LSTM based hy-
pernetwork which does not require regularisation.
• We demonstrate the improvement in continual learn-
ing performance of the proposed approaches through
experiments on several image classification data sets
and for different CL settings.2. Related Work
Several approaches were proposed recently for continual
learning and to deal with catastrophic forgetting. Conceptu-
ally, these approaches are classified based on replaying the
stored examples, methods that expand the model on seeing
the new task and methods that regularise the parameter shift
by retaining the network [20, 13]. Regularisation based ap-
proaches [1, 11, 16, 30] avoid forgetting by imposing con-
straints on the update of parameters. With the advantage
of no extra memory requirement, regularisation approaches
are used for a broader variety of applications that have con-
straints on memory, computational resources, and data pri-
vacy. Elastic weight consolidation(EWC) [11] and Synaptic
intelligence(SI) [30] are most well-known approaches, pro-
posed to mitigate forgetting by constraining the update of
important parameters of the task. It imposes a quadratic
penalty on the difference between old and new parameters
which helps to slow down the learning of new tasks by up-
dating old parameters. But experiments in [10] showed that
EWC is not effective in learning new classes incrementally.
Replay based methods [2, 17, 21] alleviate catastrophic
forgetting by replaying old examples while learning new
tasks. These methods either store examples from previ-
ous tasks or, generate synthetic examples from trained gen-
erative models from learnt feature space. Variational au-
toencoders (VAEs) and Generative adversarial networks
(GANs) are used to generate samples from feature space.
iCaRL [21], stores a subset of samples per class in fixed
memory and selected examples should best approximate
the class means in feature space. But as the number of
tasks grows, samples per class stored will become too
few to ensure required performance. Various other ap-
proaches [6, 22, 3] have been proposed which generate sam-
ples for old tasks instead of storing them and these are re-
played while learning a new task.
Dynamic architecture based methods provide a solu-
tion towards continual learning by growing or updating its
model structure for each task [15, 29]. Progressive neural
network (PNNs) [23] grow their architecture by expanding
the network statically with new modules. Forgetting can be
circumvented by adding lateral connections from previous
modules. Instead of growing the network structure stati-
cally, Dynamically expandable network (DEN) grows net-
work architecture for each task with only a limited number
of units and identifies neurons that are important for the new
task and train them selectively [29].
Recently, hypernetwork based approaches have been
proposed [8, 10, 26, 28] which has the advantage of having
a constrained search space compared to the main network.
Hypernetwork based techniques use a secondary neural net-
work to generate the parameters of the main network and
deal with forgetting at the hypernetwork level. To keep the
size of the hypernetwork to be small compared to the main

--- PAGE 3 ---
network, they generate the weights of the main network
in small fixed sized chunks [8]. However, we notice that
the process of independent generation of chunk weights ig-
nores the dependence among main network parameters and
thus affects the continual learning performance. To over-
come this, we propose a LSTM based hypernetwork that
can generate weights in smaller chunks while also maintain-
ing dependency across them. Due to their ability to capture
dependencies, LSTMs were used for learning from sequen-
tial data. Recently, continual learning methods for recurrent
neural networks based on existing regularisation technique
and hypernetworks were proposed in [4]. In contrast to the
work in [4], where the aim is to model continual learning for
tasks involving sequence data using RNNs with existing CL
techniques, the goal of this paper is to develop novel con-
tinual learning methodology based on RNNs (specifically
LSTMs ) by treating them as a hypernetwork. In addition, we
also introduce novel continual learning techniques for such
LSTM based hypernetworks such as importance weighted
regularisation and network growth.
3. LSTM Hypernetwork and Regularization
Techniques for Continual Learning
In many realistic real world learning scenarios, the tasks
arrive in sequential manner. Continual learning aims to
learn from a sequence of tasks where the data of all the tasks
are not available at once and we have a fixed memory size.
We assume that we are given a sequence of Ktasks, where
each task t∈ T={1, . . . , K }contains input Xt={xt
j}nt
j=1
and target label Yt={yt
j}nt
j=1where ntbeing the number of
samples in task t. The goal of main network mis to learn a
function ft
m(·,Θt
m) :Xt→Ytwith parameters Θt
masso-
ciated with task t. While learning a task twe have access to
only observations of current task tbut no access to data of
previous tasks.
We can learn Θt
mseparately for each task, but it results
in a linear growth in number of parameters and the fixed
sized memory will not be sufficient to store them. If we
maintain the main network parameter to be same across all
the tasks, the parameter values will get overwritten by the
new task data which will results in catastrophic forgetting.
To learn continuously over the tasks without requiring a lin-
ear growth in parameters, hypernetworks are proposed to
generate the main network parameters for each task. The
hypernetwork hlearns a function fh(·,Θh) :et→Θt
mto
generate the task specific parameter Θt
mgiven a task em-
bedding etusing trainable parameters Θh.
Generating the high dimensional main network param-
eters all at once requires a very large hypernetwork with a
large number of outputs and is computationally costly to
train them. With the motive of reducing the number of
trainable parameters in hypernetworks, chunked hypernet-works [8, 28] are proposed to generate weights in smaller
chunks (subsets of the weight matrix of main network) by
reusing the same hypernetwork fhmultiple times with dif-
ferent chunk embeddings.
Thus, hypernetwork fhwith parameters Θh
takes task embedding etand chunk embeddings
c={c1, . . . ,cnc}as input to generate set of
main network weights, Θt
m =fh(et,c,Θh)=
{fh(et,c1,Θh), fh(et,c2,Θh), . . . , f h(et,cnc,Θh)},
where ncbeing the number of chunks. hypernetworks can
still suffer from catastrophic forgetting when hypernetwork
parameters are updated to generate main network parame-
ters for the new task. In order to overcome this forgetting,
an additional regularisation term is used while learning the
parameters of the hypernetwork for a new task along with
task specific loss [28].
Chunked hypernetworks generate weights of the main
network in smaller chunks using a hypernetwork consid-
ering the task embedding and chunk embedding. We
observe that they do not consider sequential nature and
inter-dependence of weights between the chunks. We
note that the chunked hypernetworks make conditional
independence assumptions on the weights of the main
network. Consequently, if we consider a probabil-
ity distribution over weights, it gets decomposed over
chunks as P(Θt,1
m,Θt,2
m, . . .,Θt,ncm|et,c) =P(Θt,1
m|et,c1)×
P(Θt,2
m|et,c2). . . P(Θt,ncm|et,cnc).Assumption of indepen-
dence across the chunks does not usually hold and can af-
fect the main network parameter generation and the perfor-
mance of the continual learning.
3.1. LSTM Hypernetworks
To capture the interdependence in the chunk weights
and to be parameter efficient at the same time, we propose
a recurrent neural network ( RNN ) and in particular long
short term memory ( LSTM ) [9] based hypernetwork called
LSTM NET.LSTM NET is capable of generating weights
for main network in smaller chunks while also maintain-
ing the dependencies across the chunks. LSTMs are se-
quence models capable of capturing long range dependen-
cies and will be able to generate a chunk weight depend-
ing on the weights associated with the preceding chunks.
Consequently, it models the joint probability over the main
network parameters as
P(Θt,1
m,Θt,2
m, . . .,Θt,nc
m|et,c) =P(Θt,1
m|et,c1)×
P(Θt,2
m|Θt,1
m,et,c2). . . P(Θt,nc
m|Θt,1
m. . .Θt,nc−1
m ,et,cnc)(1)
The proposed LSTM hypernetwork uses hidden state
ht
j−1and cell state st
j−1of preceding chunk along with cur-
rent chunk embedding to generate the chunk weights. The
following LSTM operations are used to generate the chunk

--- PAGE 4 ---
Figure 1: LSTM based Hypernetwork can generate main
network weights Θt
mfor task tin smaller chunks by using
same hypernetwork iteratively but also by maintaining the
dependencies across them using LSTM .
weights assuming a single layer network where it takes task
embedding etand chunk embedding cjas input.
it
j=σ(wi× ⟨et,cj⟩+ui×ht
j−1)
ft
j=σ(wf× ⟨et,cj⟩+uf×ht
j−1)
ot
j=σ(wo× ⟨et,cj⟩+uo×ht
j−1)
gt
j=Tanh (wg× ⟨et,cj⟩+ug×ht
j−1)
st
j=ft
j⊙st
j−1+it
j⊙gt
j
ht
j=ot
j⊙tanh(st
j)
Θ(t,j)
m=ht
jW
where f,i,o,gare forget gate, input gate, output gate,
and cell gate respectively. σis the sigmoid function, and ⊙
is the Hadamard product. w, u are the weights associated
with the input and hidden states respectively with subscript
denoting the corresponding gate. Θ(t,j)
mis the jthchunk of
main network weights generated by LSTM hypernetwork.
Here, W∈Rd1×d2are the weights of the feed forward
layer producing the chunk weights.
We learn the LSTM parameters Θhwhen presented with
data from task Tby minimising the following loss consist-
ing of task specific loss and regularization loss [28].
arg min
ΘhLtotal=Ltask(Θh,eT,c, XT, YT)
+β
T−1T−1X
t=1||fh(et,c,Θ∗
h)−fh(et,c,Θh+ ∆Θ h)||2(2)
where fh(et,c,Θh) = Θt
mdenotes all the parameters of
the main network for task t,Θ∗
hare parameters of the hy-pernetwork before learning task T,βis a regularisation
constant which balances task specific loss and regulariza-
tion loss, and ∆Θhis change in direction of the weights of
the hypernetwork evaluated on the task-specific loss. The
task-specific loss Ltask is the loss associated with the task
(for e.g., cross-entropy loss for classification). The regu-
larization loss constraints the hypernetwork learnt on the
new task to generate main network parameters similar to the
ones generated by previously learnt hypernetwork. Hyper-
network parameters Θhand chunk embeddings are learnt
by minimising the total loss Ltotal, and the task embeddings
are learnt using Ltaskalone using backpropagation.
3.2. Importance Weighted Regularisation
We propose a novel regularisation technique for contin-
ual learning in hypernetworks which provides more flexi-
bility to the hypernetwork to adapt to the new task com-
pared to the regularisation in (2). The proposed importance
weighted regularisation (IWR) updates the hypernetwork
parameters based on the importance of the parameters as-
sociated with the main network for each task. IWR requires
the hypernetwork to generate only important main network
parameters associated with old tasks and not all the main
network parameters. We achieve this by considering the
fisher information score of the main network parameters in
the regularisation term in (2). This will enforce hypernet-
work to give importance to significant main network param-
eters during generation of main network weights. Mean-
while, it provides flexibility to the hypernetwork to adapt
its parameters more freely to the new task as it is not con-
strained to generate all the main network parameters with
equal importance. The objective function considering the
IWR regularisation is defined as
arg min
ΘhLtotal=Ltask(Θh,eT,c, XT, YT)
+β
T−1T−1X
t=1X
iFIt
i(fhi(et,c,Θ∗
h)−fhi(et,c,Θh+ ∆Θ h))2
(3)
where the first term Ltaskis task specific loss and the sec-
ond term is the IWR term that regularises the hypernetwork
parameters to avoid forgetting. The IWR term uses FIt
which is the fisher information matrix (defined below) over
the main network parameters associated with task t.FIt
provides the importance of the main network parameters for
taskt, and the index iiterates over all the main network pa-
rameters. fhi(et,c,Θh)denotes the ithmain network pa-
rameter generated by the hypernetwork. We can observe
that if the Fisher information associated with the ithmain
network parameter is high (implying that this parameter is
important), then hypernetwork is required to generate it ex-
actly while it does not have to do the same for the unimpor-

--- PAGE 5 ---
tant parameters. Thus, IWR provides more flexibility to the
hypernetwork to learn and adapt to the new tasks.
The Fisher information matrix (FI) provides the infor-
mation on the importance of each weight in the network.
For the IWR in (3), the FI matrix is defined as
FIt=1
NtNtX
i=1
∇ΘtmLtask(Θh,et, yi,xi)×
∇ΘtmLtask(Θh,et, yi,xi)T
(4)
We note that the derivatives are computed with respect to
the main network parameters to assess the importance of
those parameters and not with respect to the hypernetwork
parameters that we are learning unlike the standard regular-
ization techniques. Thus, using Fisher information matrix
we can find out the main network parameters which are im-
portant in learning the task. The existing regularisation for
hypernetworks in (2) treats all the main network parameters
equally. In practice, not all main network parameters con-
tribute equally to solving a particular task. Hence, it is not
required to exactly generate all the main network parame-
ters by the hypernetwork but only important ones and can
be achieved using the IWR regularisation. The IWR regu-
larization is a generic technique that can be used with any
hypernetwork and not only the LSTM hypernetwork to im-
prove the continual learning performance.
3.3. Network Growth Technique
One potential problem with the regularisation ap-
proaches is that training time grows with number of tasks
as can be seen from Eq (2) and Eq (3). Moreover, the
same hypernetwork parameters are used to generate all the
task specific main parameters. This can become a bottle-
neck and affects continual learning performance in situa-
tions with a large number of tasks. We propose LSTM hy-
pernetworks (LSTM NET GROW) based on network growth
for continual learning. It provides more flexibility in adapt-
ing to the new task by maintaining task specific parame-
ters and accelerates model training by requiring no regu-
larisation. In order to transfer knowledge across the tasks,
LSTM NET GROW also maintains a shared set of hypernet-
work parameters.
We hypothesize that, though the actual main network
parameters differ across the tasks, the dependencies exist-
ing among the main network parameters remain the same
across the tasks. Based on this intuition, we define the
shared and task specific parameters in the LSTM hyper-
network. In LSTM , the dependencies are captured by the
weights associated with the hidden state. Hence, we as-
sume them to be the same across the tasks in the proposed
LSTM NET GROW model. The variability in parametergeneration across the tasks is captured by having task spe-
cific weights associated with inputs. More specifically, the
weights ( uf,ui,uo,ug) of the LSTM are shared across the
tasks and we maintain input weights ( wt
f,wt
i,wt
o,wt
g) to be
task specific. LSTM NET GROW uses the following LSTM
operations to generate chunk weights of the main network
parameters associated with task t.
it
j=σ(wt
i× ⟨et,cj⟩+ui×ht
j−1)
ft
j=σ(wt
f× ⟨et,cj⟩+uf×ht
j−1)
ot
j=σ(wt
o× ⟨et,cj⟩+uo×ht
j−1)
gt
j=Tanh (wt
g× ⟨et,cj⟩+ug×ht
j−1)
st
j=ft
j⊙st
j−1+it
j⊙gt
j
ht
j=ot
j⊙tanh(st
j)
Θ(t,j)
m=ht
jWt
The LSTM NET GROW model freezes the hidden
weights ( uf,ui,uo,ug) of the LSTM after learning first
task and is shared across all the tasks. It keeps learning new
task specific input weights ( wt
f,wt
i,wt
o,wt
g) upon training
on the new task and those are stored for inference at a later
stage. This approach does not require the additional regular-
isation term and can be learnt just based on task specific loss
(Ltask). In addition, the task specific weights provide ex-
tra flexibility to the LSTM hypernetwork in generating task
specific main network weights.
4. Experiments
We perform extensive experiments on various continual
learning setups and real world benchmarks datasets to show
the effectiveness of our approach. We present our results on
Split MNIST, permuted MNIST, CIFAR-10, and CIFAR-
100 datasets. Through experiments we aim to demonstrate:
• impact of maintaining dependencies across the chunks
using the LSTM based hypernetwork (LSTM NET) ,
• impact of the proposed regularisation IWR on
LSTM NET (LSTM NET IWR) and on HNET
(HNET IWR) in mitigating catastrophic forgetting,
• improvement in performance using proposed dy-
namically growing LSTM based hypernetwork
LSTM NET GROW .
• knowledge transfer and mitigating forgetting across
the tasks using the challenging Cifar datasets.
4.1. Experimental Setup
Continual learning models are tested on three different
continual learning scenarios [27].
CL1 (Task incremental learning) : It provides the task iden-
tity information to the model both at training and testing

--- PAGE 6 ---
Table 1: Comparison of average test accuracy (%) of Split MNIST and Permuted MNIST for all three scenarios of continual
learning without generative replay
Split MNIST Permuted MNIST
CL1 CL2 CL3 CL1 CL2 CL3
EWC [11] 98.64±0.22 63.95 ±1.90 20.10 ±0.06 94.74±0.05 94.31 ±0.11 25.04 ±0.50
Online EWC[11] 99.12±0.11 64.32 ±1.90 19.96 ±0.07 95.96±0.06 94.42 ±0.13 33.88 ±0.49
SI[30] 99.09±0.15 65.36 ±1.57 19.99 ±0.06 94.75±0.14 95.33 ±0.11 29.31 ±0.62
HNET[28] 99.79±0.01 87.01 ±0.47 69.48 ±0.80 97.57±0.02 92.80 ±0.15 91.75 ±0.21
HNET IWR 99.79±0.01 88.51 ±0.18 71.90 ±0.11 97.60±0.04 93.90 ±0.11 92.15 ±0.19
LSTM NET 99.82±0.01 89.50 ±0.19 71.31 ±0.07 97.65±0.01 93.11 ±0.13 92.10 ±0.20
LSTM NET IWR 99.85±0.02 90.17±0.25 71.54 ±0.04 97.74±0.03 94.26 ±0.10 92.21 ±0.23
LSTM NET GROW 99.85±0.02 97.11 ±0.16 83.21 ±0.02 97.88±0.02 95.46 ±0.09 92.23 ±0.19
Table 2: Comparison of average test accuracy (%) of Split MNIST and Permuted MNIST for all three scenarios of continual
learning with generative replay
Split MNIST Permuted MNIST
CL1 CL2 CL3 CL1 CL2 CL3
LWF[14] 99.57±0.02 71.50 ±1.63 23.85 ±0.44 69.84±0.46 72.64 ±0.52 22.64 ±0.23
DGR[25] 99.50±0.03 95.72 ±0.25 90.79 ±0.41 92.52±0.08 95.09 ±0.04 92.19 ±0.09
DGR+distill[25] 99.61±0.02 96.83 ±0.20 91.79 ±0.32 97.51±0.01 97.35 ±0.02 96.38 ±0.03
HNET+R[28] 99.83±0.01 98.00±0.03 95.30 ±0.13 97.87±0.01 97.60 ±0.01 97.76 ±0.01
HNET IWR+R 99.83±0.01 97.94±0.05 95.38 ±0.16 97.85±0.01 97.66 ±0.02 97.76 ±0.02
LSTM NET+R 99.83±0.01 98.17±0.02 95.46 ±0.11 97.87±0.01 97.60 ±0.01 97.77 ±0.01
LSTM NET IWR+R 99.83±0.01 98.39±0.05 96.50 ±0.19 97.87±0.01 97.66 ±0.01 97.80±0.02
LSTM NET GROW+R 99.83±0.01 98.43 ±0.05 97.01 ±0.13 97.90±0.01 97.70 ±0.02 97.80 ±0.01
time. Since task identity is available, dedicated components
can be assigned to the tasks in the sequence. Multi-headed
model is one such kind of model architecture used in con-
tinual learning.
CL2 (Domain incremental learning) : It does not provide
task identity information at test time, nor is it required to
infer the task identity. Here, each task treats data to be from
different domains but with the same classes. It considers the
same number of classes across all the tasks in the sequence
and utilises the same output heads for all the tasks.
CL3 (Class incremental learning) : In this continual learn-
ing scenario, the model is not provided with the task iden-
tity. It not only requires learning the tasks incrementally but
also requires to infer the task identity. Here, the task identity
is inferred through the predictive distribution entropy. This
scenario resembles most with the real time setting with new
class objects appearing incrementally.
The effectiveness of the hypernetwork based CLtech-
niques for parameter generation are also tested on two con-
tinual learning setups, replay based and non-replay based
setups. In the non-replay-based setup, hypernetwork is
trained to generate parameters of the classifier used for solv-
ing the image classification problems. In the non-replaybased setup, we compare the proposed approach with reg-
ularisation baselines Elastic Weight Consolidation (EWC)
[11], Synaptic Intelligence (SI) [30] and the baseline hyper-
network HNET [28]. In the replay based setup, we augment
our system with a generative model, for e.g. variational
auto-encoder ( VAE) to generate synthetic examples from
the previous tasks that can be replayed to aid the classi-
fier to remember previous tasks. In this case, hypernetwork
will generate weights for the replay network i.e. VAE but
not the target classifier. In the replay based setup, we com-
pare the proposed approach with baselines deep generative
replay with distillation (DGR) [25], learning without forget-
ting (LWF) [14] and the baseline hypernetwork HNET [28].
We conduct experiments on the standard continual learn-
ing task of image classification on publicly available real
world data sets such as split MNIST, permuted MNIST,
CIFAR-10 and CIFAR-100. In these experiments, we use
a single layer LSTM that takes a task and chunk embedding
each of size 96. For MNIST, we consider an LSTM with
hidden state size 64 and batch size 128. For CIFAR, hid-
den size and batch size are 128 and 32 respectively. For
Split MNIST, the classifier is a fully connected network
(FCN) with 2 layers each with size 400 [28]. For Permuted

--- PAGE 7 ---
Table 3: Test accuracy comparison of various methods on CIFAR-10 (C-10) and subsequent five splits ( S1...5) each with ten
classes of CIFAR-100 (C-100).
C-10
(%)C-100
S1(%)C-100
S2(%)C-100
S3(%)C-100
S4(%)C-100
S5(%)Average-
accuracy(%)
Finetuning 15.3 13.1 12.2 10.2 20.3 87.0 26.35
Training-from-scratch 88.6 79.3 77.0 83.0 74.4 81.1 80.60
HNET[28] 88 83 79 82 81 82 82.50
HNET IWR 86.2 86.1 80.2 85.0 83.6 85.01 84.35
LSTM NET during 88.78 89.3 85.2 84.4 83.5 82.7 85.64
LSTM NET 88.74 89.1 84.9 84.3 83.4 82.7 85.52
LSTM NET IWR 88.44 88.9 85.2 88.5 86.3 86.8 87.35
LSTM NET GROW 88.98 87.7 86.3 88.2 89.2 89.1 88.25
MNIST, layer size is taken to be 1000 as done in [28]. For
CIFAR datasets, Resnet-32 is used as the classifier. In re-
play based setup, we use VAE which uses an FCN with two
layers each of size 400 both as the encoder and the decoder
and uses a latent space of dimension 100.
4.2. Results
We demonstrate the results of our experiments on various
image classification datasets used for the continual learning
setup and several baselines. For fair comparison, we main-
tained the number of trainable parameters in the hypernet-
work to be equal or lesser than the baseline methods.
4.2.1 Split MNIST:
Split MNIST is a popular continual learning benchmark
for image classification. The dataset consists of im-
ages of ten digits (0-9) and form five binary classification
tasks by pairing them sequentially i.e. {(0,1),(2,3),(4,5),
(6,7),(8,9)}. The results are presented in Table 1 for non-
replay based setup, and in Table 2 for the replay based setup.
The results show the efficacy of our approach in achieving
better continual learning performance in all the three CL
scenarios and for each of the setups.
The proposed hypernetwork LSTM NET outperforms
the baselines EWC, SI and HNET in the non-replay based
setup, and outperforms the baselines LWF, DGR and HNET
in the replay based setup. The performance of HNET IWR
andLSTM NET IWR demonstrates that the proposed regu-
larisation technique IWR further improves the performance
ofHNET andLSTM NET in all the continual learning se-
tups. While the methods provide comparable results for
the easier CL1 setting, the improvement in performance us-
ing the proposed techniques are more evident in the more
complex and realistic CL2 and CL3 settings. This is even
more evident in the non-replay setup where the standard
techniques struggle. We can observe that the proposed ap-
proach LSTM NET GROW has significantly improved thecontinual learning performance over the other models for
these CL scenarios and setups. One of the major reasons
for large improvement in accuracy of CL2 and CL3 with
LSTM NET GROW is because of dynamically expanding
network with new tasks, this helps each task to have task-
specific parameters which won’t get updated while learning
new tasks and contribute significantly for improvement in
performance.
4.2.2 Permuted MNIST
This CL benchmark is a variant of MNIST, it consists of
tasks which are created by performing random permutation
over MNIST images. The sequence of T= 10 tasks are
obtained by repeating this procedure. We consider a dataset
with a sufficiently long sequence of tasks ( T= 10 ) to inves-
tigate the remembering capacity of our continual learning
model. The results presented in Table 1 and Table 2 for non-
replay and replay based setups respectively demonstrates
the effectiveness of the proposed approaches for continual
learning on Permuted MNIST. The results follows a sim-
ilar trend as in the split MNIST, with LSTM NET GROW
giving the best results, followed by LSTM NET IWR and
LSTM NET, beating the baseline approaches.
4.2.3 CIFAR-10/100 dataset
We further evaluate the effectiveness of the proposed ap-
proaches on a more challenging image classification data
CIFAR-10 and CIFAR-100. The model is first trained on
10 classes of CIFAR-10, and subsequently on five sets of
ten classes from cifar-100 following the experimental setup
in [28]. Thus, the model needs to learn T= 6 tasks. We
use ResNet-32 for classification of CIFAR-10/100 datasets
and the hypernetworks are trained to generate parameters
of ResNet-32 architecture. The experiments are conducted
on the CL1 scenario and non-replay based setup following
[28]. In addition to the baseline hypernetwork HNET , we

--- PAGE 8 ---
also consider baselines which allows us to demonstrate the
knowledge transfer across the tasks. Training-from-scratch
baseline independently and separately learns the main net-
work parameters for each task and test the performance
on the corresponding task. The baseline Finetuning adapts
the main network to the new tasks without taking into ac-
count catastrophic forgetting. The model adapted to the fi-
nal task is then used for predicting performance on all the
tasks. To demonstrate effectiveness of LSTM NET in deal-
ing with catastrophic forgetting, we also considered a base-
lineLSTM NET during , where we test LSTM NET on each
task immediately after training on that task instead of test-
ing after training on all the tasks as in LSTM NET.
We provide the results comparing all teh approaches in
Table 3. We can clearly observe from the results that our
approach LSTM NET outperforms HNET by a great mar-
gin on a challenging dataset like CIFAR-10/100. In Ta-
ble 3, comparing results of LSTM NET,LSTM NET IWR
andLSTM NET GROW with Training-from-scratch , we can
see that the knowledge transfer across the tasks helps the
proposed approaches in getting a better performance. We
can also observe that the LSTM NET during matches with
LSTM NET which indicates that LSTM based hypernet-
work is very effective in dealing with catastrophic forget-
ting. We also perform experiments using different reg-
ularisation techniques. The proposed IWR regularisation
achieves a better result than the baseline regularisation pro-
posed in [28]. In fact, it improves the HNET performance
as well, an improvement in the overall test accuracy by al-
most 2%. Hence, IWR is an effective regularisation tech-
nique for any hypernetwork based continual learning ap-
proaches. The performance improves further by using the
LSTM NET GROW approach for continual learning in this
data similar to MNIST.
4.2.4 Ablation Study
We conduct further ablation study on CIFAR-10/CIFAR-
100 to understand the impact of compression ratio (Figure
2a) and regularisation constant (Figure 2b) on the proposed
models. From Figure 2a, we can see that as the number
of trainable parameters grows in hypernetwork compared
to the main network, LSTM NET performance further im-
proves over the HNET . In Figure 2b, we study the effect of
varying the regularisation constant ( β) in the IWR regular-
ization term in LSTM NET IWR on Cifar datasets. The per-
formance is poor when regularization term is neglected (low
value of β) as expected and is high and stable for higher val-
ues of β.
From the experiments on CIFAR-10/CIFAR-100 tasks,
we observe that training time using LSTM NET with regu-
larisations in Eq.2 and Eq.3 is approximately 28 hours but
with LSTM NET GROW it is approximately 20 hours. The
(a) Compression ratio effect
 (b) Regularization effect
Figure 2: In Figure 2a, plotted results of accuracies of
LSTM NET vs HNET with increase in compression ratio
(ratio of trainable parameters in hypernetwork to main net-
work) on Cifar datasets. In Figure 2b, plotted graph with
accuracy values using LSTM NET IWR with varying regu-
larisation constant ( β) in the IWR on Cifar datasets.
above results are calculated with a batch size of 32 and 200
epochs for each batch. As the number of tasks increases,
training time of regularisation approaches grows linearly
but that of LSTM NET GROW remains constant. On the
other hand, the memory requirements of the regularisation
approaches remains constant but LSTM NET GROW grows
linearly with tasks due to some task specific parameters
but is much lower than maintaining separate parameters for
each task.
5. Conclusion
We propose a novel LSTM based hypernetwork for con-
tinual learning which could capture dependencies across the
main network parameters while also maintaining the pa-
rameter efficiency. To improve continual learning perfor-
mance using hypernetworks, we propose a novel regular-
isation Importance Weighted Regularisation (IWR) which
is well-suited for hypernetwork based CL approaches.
To further improve the continual learning performance
of the proposed LSTM hypernetwork, we propose a net-
work growth technique for LSTMs . Through experiments
on several image classification tasks and datasets, we
demonstrate the effectiveness of our proposed approaches,
LSTM based hypernetwork, IWR regularisation for hyper-
networks, and network growth on LSTMs . The proposed
approaches improved the continual learning performance
on all the CL tasks, settings, and datasets. As a future
work, we would like to improve the parameter growth in
LSTM NET GROW , and develop hybrid models combining
the network growth and regularisation to further improve
CL performance.

--- PAGE 9 ---
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Vittorio Fer-
rari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss,
editors, Computer Vision - ECCV 2018 - 15th European Con-
ference, Munich, Germany, September 8-14, 2018, Proceed-
ings, Part III , volume 11207 of Lecture Notes in Computer
Science , pages 144–161. Springer, 2018.
[2] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with A-
GEM. In 7th International Conference on Learning Rep-
resentations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019 . OpenReview.net, 2019.
[3] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc’Aurelio Ranzato. On tiny episodic memo-
ries in continual learning. arXiv preprint arXiv:1902.10486 ,
2019.
[4] Benjamin Ehret, Christian Henning, Maria R. Cervera,
Alexander Meulemans, Johannes von Oswald, and Ben-
jamin F. Grewe. Continual learning in recurrent neural net-
works with hypernetworks. CoRR , abs/2006.12109, 2020.
[5] Sibo Gai, Zhengyu Chen, and Donglin Wang. Multi-modal
meta continual learning. In International Joint Conference
on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-
22, 2021 , pages 1–8. IEEE, 2021.
[6] Chandan Gautam, Sethupathy Parameswaran, Ashish
Mishra, and Suresh Sundaram. Generalized continual zero-
shot learning. CoRR , abs/2011.08508, 2020.
[7] Stephen Grossberg. Consciousness CLEARS the mind. Neu-
ral Networks , 20(9):1040–1053, 2007.
[8] David Ha, Andrew M. Dai, and Quoc V . Le. Hypernet-
works. In 5th International Conference on Learning Rep-
resentations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net, 2017.
[9] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term
memory. Neural computation , 9(8):1735–1780, 1997.
[10] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L.
Hayes, and Christopher Kanan. Measuring catastrophic for-
getting in neural networks. In Sheila A. McIlraith and Kil-
ian Q. Weinberger, editors, Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, (AAAI-18), the
30th innovative Applications of Artificial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018 , pages 3390–3398. AAAI Press,
2018.
[11] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521–3526, 2017.
[12] Jan Koutn ´ık, Faustino J. Gomez, and J ¨urgen Schmidhuber.
Evolving neural networks in compressed weight space. InMartin Pelikan and J ¨urgen Branke, editors, Genetic and Evo-
lutionary Computation Conference, GECCO 2010, Proceed-
ings, Portland, Oregon, USA, July 7-11, 2010 , pages 619–
626. ACM, 2010.
[13] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and
Tinne Tuytelaars. Continual learning: A comparative study
on how to defy forgetting in classification tasks. CoRR ,
abs/1909.08383, 2019.
[14] Zhizhong Li and Derek Hoiem. Learning without forgetting.
In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling,
editors, Computer Vision - ECCV 2016 - 14th European Con-
ference, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part IV , volume 9908 of Lecture Notes in Com-
puter Science , pages 614–629. Springer, 2016.
[15] Xialei Liu, Marc Masana, Luis Herranz, Joost van de Wei-
jer, Antonio M. L ´opez, and Andrew D. Bagdanov. Rotate
your networks: Better weight consolidation and less catas-
trophic forgetting. In 24th International Conference on Pat-
tern Recognition, ICPR 2018, Beijing, China, August 20-24,
2018 , pages 2262–2268. IEEE Computer Society, 2018.
[16] Noel Loo, Siddharth Swaroop, and Richard E. Turner. Gen-
eralized variational continual learning. In 9th International
Conference on Learning Representations, ICLR 2021, Vir-
tual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.
[17] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. In Isabelle Guyon,
Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob
Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors,
Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems
2017, December 4-9, 2017, Long Beach, CA, USA , pages
6467–6476, 2017.
[18] Michael McCloskey and Neal J Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. In Psychology of learning and motivation , vol-
ume 24, pages 109–165. Elsevier, 1989.
[19] Martial Mermillod, Aur ´elia Bugaiska, and Patrick Bonin.
The stability-plasticity dilemma: Investigating the contin-
uum from catastrophic forgetting to age-limited learning ef-
fects, 2013.
[20] German Ignacio Parisi, Ronald Kemker, Jose L. Part,
Christopher Kanan, and Stefan Wermter. Continual lifelong
learning with neural networks: A review. Neural Networks ,
113:54–71, 2019.
[21] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H. Lampert. icarl: Incremental clas-
sifier and representation learning. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017 , pages 5533–5542.
IEEE Computer Society, 2017.
[22] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,
Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn
without forgetting by maximizing transfer and minimizing
interference. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net, 2019.

--- PAGE 10 ---
[23] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
CoRR , abs/1606.04671, 2016.
[24] J ¨urgen Schmidhuber. Learning to control fast-weight mem-
ories: An alternative to dynamic recurrent networks. Neural
Comput. , 4(1):131–139, 1992.
[25] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In Isabelle
Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wal-
lach, Rob Fergus, S. V . N. Vishwanathan, and Roman Gar-
nett, editors, Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Pro-
cessing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA , pages 2990–2999, 2017.
[26] Kenneth O. Stanley, David B. D’Ambrosio, and Jason Gauci.
A hypercube-based encoding for evolving large-scale neural
networks. Artif. Life , 15(2):185–212, 2009.
[27] Gido M. van de Ven and Andreas S. Tolias. Three scenarios
for continual learning. CoRR , abs/1904.07734, 2019.
[28] Johannes von Oswald, Christian Henning, Jo ˜ao Sacramento,
and Benjamin F. Grewe. Continual learning with hypernet-
works. In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net, 2020.
[29] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable net-
works. In 6th International Conference on Learning Rep-
resentations, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings . OpenRe-
view.net, 2018.
[30] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In Doina Precup
and Yee Whye Teh, editors, Proceedings of the 34th Interna-
tional Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017 , volume 70 of Pro-
ceedings of Machine Learning Research , pages 3987–3995.
PMLR, 2017.
