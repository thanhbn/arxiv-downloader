# 2309.05444.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.05444.pdf
# Kích thước tệp: 878876 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Đẩy Mixture of Experts đến giới hạn:
MoE cực kỳ hiệu quả tham số cho
Instruction Tuning
Ted Zadouri
Cohere for AI
ted@cohere.comAhmet Üstün
Cohere for AI
ahmet@cohere.comArash Ahmadian†
Cohere for AI
arash@cohere.com
Beyza Ermiş
Cohere For AI
beyza@cohere.comAcyr Locatelli
Cohere
acyr@cohere.comSara Hooker
Cohere for AI
sarahooker@cohere.com
Tóm tắt
Mixture of Experts (MoE) là một kiến trúc mạng neural được biết đến rộng rãi nơi một tập hợp các
mô hình phụ chuyên biệt tối ưu hóa hiệu suất tổng thể với chi phí tính toán không đổi. Tuy nhiên, các
MoE thông thường gây ra thách thức ở quy mô lớn do cần lưu trữ tất cả các expert trong bộ nhớ. Trong
bài báo này, chúng tôi đẩy MoE đến giới hạn. Chúng tôi đề xuất MoE cực kỳ hiệu quả tham số bằng
cách kết hợp độc đáo kiến trúc MoE với các expert nhẹ. Kiến trúc MoE của chúng tôi vượt trội hơn các
phương pháp fine-tuning hiệu quả tham số (PEFT) tiêu chuẩn và tương đương với full fine-tuning chỉ
bằng cách cập nhật các expert nhẹ – ít hơn 1% của một mô hình 11B tham số. Hơn nữa, phương pháp
của chúng tôi tổng quát hóa cho các tác vụ chưa thấy vì nó không phụ thuộc vào bất kỳ kiến thức tác vụ
trước đó nào. Nghiên cứu của chúng tôi nhấn mạnh tính linh hoạt của kiến trúc mixture of experts, thể
hiện khả năng cung cấp hiệu suất mạnh mẽ ngay cả khi chịu các ràng buộc tham số nghiêm ngặt. Mã
nguồn được sử dụng trong tất cả các thí nghiệm đều công khai tại: https://github.com/for-ai/parameter-efficient-moe .

1 Giới thiệu
Một paradigm huấn luyện thông thường là áp dụng các trọng số của mô hình cho mỗi đầu vào. Có thể
nói, điều này không hiệu quả vì một đầu vào cho trước có thể không cần tất cả dung lượng của mô hình.
Ngược lại, các MoE xây dựng trên tiền đề rằng các thành phần mô-đun phụ – được gọi là các expert –
có thể chuyên biệt hóa cho các loại đầu vào khác nhau. Sự nhấn mạnh vào tính toán có điều kiện này có
các tác động hiệu quả quan trọng như chi phí suy luận không đổi. Điều này đã khiến MoE trở thành một
lĩnh vực nghiên cứu và áp dụng rộng rãi trong kỷ nguyên của các Transformer quy mô lớn nơi việc
mở rộng quy mô đã tăng chi phí triển khai và độ trễ (Shazeer et al., 2018; Riquelme et al., 2021; Du et
al., 2022; Fedus et al., 2022).

Trong khi phần lớn công việc cho đến nay đã tập trung vào MoE như một chiến lược pretraining, động
lực cố có của MoE không chỉ giới hạn trong pretraining. Thực tế, những ưu điểm của MoE có thể nói
là rất phù hợp với một setting instruction fine-tuning nơi dữ liệu thường được cấu trúc có chủ ý để
†Cũng liên kết với Đại học Toronto & Vector Institute for Artificial Intelligence.
Phát hành như một preprint vào ngày 12 tháng 9 năm 2023 1arXiv:2309.05444v1  [cs.CL]  11 Sep 2023

--- TRANG 2 ---
0.01% 0.6% 3% 5%
% Tham số được cập nhật54.056.058.060.062.0Độ chính xác trung vị trung bình(IA)3MoV-10MoV-30
LoRAMoLoRA-10MoLoRA-15
Full fine-tuningHiệu suất vs Ngân sách tham số
770M 3B 11B
Tham số mô hình gốc50.052.054.056.058.060.062.064.066.0Độ chính xác trung vị trung bình
Hiệu suất vs Kích thước mô hình gốc
Full Fine-Tuning
MoV-60
IA3Hình 1: Trái: Mixture of PEFT experts của chúng tôi vượt trội hơn các phương pháp PEFT đơn
lẻ SOTA sử dụng lượng tham số tương đương được minh họa cho T5-XL (3B). Phải: Phương pháp
Mixture of PEFT mở rộng quy mô lên đến 11B; với các cập nhật tham số nhỏ, nó xấp xỉ hoặc khớp
với hiệu suất full fine-tuning.

đại diện cho một tập hợp đa dạng các tác vụ, thường được gọi là multi-task finetuning (Chung et al.,
2022; Wei et al., 2022; Sanh et al., 2022; Longpre et al., 2023; Muennighoff et al., 2023).

Trong công việc này, chúng tôi đặt ra câu hỏi liệu chúng ta có thể tận dụng MoE cho instruction fine-tuning không?
Một trong những nhược điểm chính của paradigm MoE là nó giới thiệu một lượng tham số tổng cực lớn
(Fedus et al., 2022). Mặc dù có tính toán có điều kiện, việc fine-tuning hoàn toàn kiến trúc MoE là
cực kỳ đòi hỏi về mặt tính toán do cần cập nhật tất cả các tham số. Đối với hầu hết các nhà thực hành,
với quy mô của các LLM hiện đại (Brown et al., 2020; Touvron et al., 2023; Kaplan et al., 2020; Anil
et al., 2023), đây là một chi phí tính toán không khả thi.

Do đó, chúng tôi tập trung vào một setting thực tế hơn cho các nhà thực hành hàng ngày – liệu chúng ta
có thể áp dụng thành công MoE cho các phương pháp parameter-efficient fine-tuning (PEFT) như (IA)3
(Liu et al., 2022) hoặc LORA (Hu et al., 2021) chỉ fine-tune một số lượng tham số nhỏ hơn nhiều không.
Đây là một thách thức đáng kể không chỉ vì mục tiêu của chúng tôi là chỉ cập nhật một tỷ lệ nhỏ
tất cả tham số mà còn khi chúng tôi cũng điều hướng các thách thức tối ưu hóa cố có của MoE đã được
công việc trước đây ghi nhận (Chen et al., 2022) trong một môi trường bị ràng buộc hơn.

Trong công việc này, chúng tôi đề xuất một framework mới tận dụng lợi ích của MoE trong một môi
trường tính toán bị ràng buộc nghiêm trọng. Chúng tôi giới thiệu Mixture of Vectors (MoV) và Mixture
of LORA (MoLORA), một sự thích ứng parameter-efficient của phương pháp Mixture of Experts. Không
giống như MoE tiêu chuẩn, framework của chúng tôi có thể được sử dụng trong một setting giới hạn
tham số do tính chất nhẹ của nó. Đáng chú ý, phương pháp của chúng tôi đạt được hiệu suất tương đương
với full fine-tuning trên các tác vụ chưa thấy bằng cách cập nhật ít hơn 1% tham số. Nó cũng dễ dàng
vượt trội hơn các kỹ thuật parameter-efficient cơ bản như (IA)3 hoặc LORA.

Chúng tôi đạt được kết quả nhất quán trên các mô hình T5 (Raffel et al., 2020) từ 770M đến 11B trên
12 tác vụ khác nhau từ 55 bộ dữ liệu P3 (Sanh et al., 2022). Tóm lại, các đóng góp của chúng tôi như sau:

(i) Chúng tôi trình bày MoE cực kỳ parameter-efficient. Kiến trúc này tận dụng MoE trong một
2

--- TRANG 3 ---
setting thực tế hơn sử dụng các expert mô-đun và nhẹ. MoE của chúng tôi có thể được sử dụng để
fine-tune một mô hình dense bằng cách cập nhật ít hơn 1% tham số của nó.

(ii) Instruction fine-tuning với các phương pháp đề xuất của chúng tôi liên tục vượt trội hơn các
phương pháp parameter efficient truyền thống trên các tác vụ chưa thấy, trong khi duy trì hiệu
quả tham số cao qua các quy mô khác nhau. Mixture of (IA)3 vectors (MoV) đạt được cải thiện
lên đến 14.57% và 8.39% so với (IA)3 tiêu chuẩn ở kích thước mô hình 3B và 11B tương ứng.
Sự ưu việt này giữ vững qua các kích thước mô hình khác nhau, loại expert và ngân sách tham
số có thể huấn luyện.

(iii) Chúng tôi chỉ ra rằng công thức của chúng tôi có thể khớp với hiệu suất của full fine-tuning ở
quy mô lớn trong khi cập nhật một phần nhỏ tham số mô hình. Kết quả của chúng tôi trên 8 tác
vụ chưa thấy cho thấy MoV chỉ cập nhật 0.32% và 0.86% tham số trong các mô hình 3B và 11B
đạt hiệu suất cực kỳ cạnh tranh với full fine-tuning với chi phí tính toán giảm đáng kể.

(iv) Cuối cùng, chúng tôi trình bày một tập hợp mở rộng các nghiên cứu ablation đánh giá một cách
hệ thống hiệu quả của các kiến trúc MoE khác nhau và chiến lược PEFT ở các kích thước mô hình
khác nhau, các loại adapter khác nhau, số lượng expert, cơ chế routing, và tầm quan trọng của
việc tối ưu hóa hyper-parameter, đặc biệt là với độ nhạy cảm của MoE.

2 Phương pháp
Thiết lập instruction tuning được hình thành như vậy nơi có tập hợp các tác vụ được chia thành các tác
vụ huấn luyện và đánh giá giữ lại, T=Ttrain∪Teval. Mô hình pretrained cơ bản đầu tiên được fine-tuned
trên Ttrain và sau đó được đánh giá theo cách zero-shot trên mỗi tác vụ chưa thấy từ Teval. Phương
pháp tiêu chuẩn là fine-tuning tất cả tham số mô hình gây ra chi phí tính toán và bộ nhớ cao. Phương
pháp của chúng tôi cung cấp một giải pháp thay thế hiệu quả sử dụng parameter-efficient mixture of
experts. Trong phần này, chúng tôi mô tả framework của chúng tôi một cách chi tiết.

2.1 Parameter-efficient Fine-tuning với (IA)3 và LORA Adapters
Trong công việc này, chúng tôi đẩy kiến trúc mixture of expert (MoE) đến mức độ hiệu quả tham số
cực độ sử dụng các phương pháp parameter-efficient fine-tuning (PEFT). Các phương pháp PEFT
giải quyết các thách thức liên quan đến việc cập nhật một số lượng lớn tham số – đặc biệt nổi lên ở
quy mô khi fully fine-tuning một LLM – bằng cách hạn chế cập nhật trọng số cho một số lượng hạn chế
tham số.

Để chỉ ra cách phương pháp của chúng tôi mở rộng quy mô với các kỹ thuật PEFT khác nhau, chúng tôi
thử nghiệm với cả (IA)3 và LORA. Những phương pháp này thêm một số lượng nhỏ tham số vào mô hình
pretrained hiện có. Chúng tôi giới thiệu ngắn gọn từng phương pháp PEFT dưới đây:

(IA)3 giới thiệu ba vector mới, lk∈Rdk,lv∈Rdv,lff∈Rdff điều chỉnh lại các kích hoạt key và value
trong self-attention, và các kích hoạt trung gian trong các lớp feed-forward theo vị trí:
softmaxQ(lk⊙KT)√dk
(lv⊙V); (lff⊙γ(W1x))W2 ((IA)3)

nơi Q,K,V là các ma trận chiếu query, key, và value cho self-attention, và W1,W2 là các trọng số
đông lạnh của các lớp feed-forward trong mô hình pretrained. Vì (IA)3 chỉ cập nhật lk,lv,lff
3

--- TRANG 4 ---
Q K VW 1
RouterVector 1
Vector 2
Vector 3
Vector 4Mixture of Vectors
(MOV)
x inputW 2
x hidden1class MOV_Layer (nn. module ):
2 n_experts : int # số lượng expert
3
4 def call (self , inputs ):
5 # inputs shape : [batch , seq , h_dim ]
6 batch , seq , h_dim = inputs . shape
7
8 # MOV scaling vectors : [ n_experts , h_dim ]
9 mov_vectors = self . param (' mov_scalers ',
10 nn. init . ones () , ( self . n_experts , h_dim ))
11
12 # router probs : [batch , seq , n_experts ]
13 router_probs = self . router (inputs ,
14 self . n_experts , dtype ='float32 ')
15
16 # combined vector : [batch , seq , h_dim ]
17 mov_combined = jnp. einsum ('...e,ed - >...d',
18 router_probs ,
19 mov_vectors )
20
21 return inputs * mov_combined
Hình 2: Trái: Tổng quan về kiến trúc MoV nhấn mạnh soft-merging nơi chỉ các vector và router
được cập nhật cho mỗi khối multi-head attention, được biểu thị bằng màu sắc. Phải: Mã giả kiểu
JAX minh họa việc triển khai cốt lõi của một lớp MoV.

rescaling vectors cho mỗi lớp Transformer∗, nó là cực kỳ parameter-efficient. Đối với mô hình T5
3 tỷ tham số (Raffel et al., 2020), nó chỉ cập nhật 0.018% tổng tham số.

Lưu ý rằng, không giống như adapter (Houlsby et al., 2019) hay prompt-tuning (Lester et al., 2021),
số lượng tham số mới được chèn bởi (IA)3 được xác định bởi kiến trúc vì các scaling vector cần
có cùng kích thước với các chiều kích hoạt tương ứng.

Low-Rank adaptation (LORA; Hu et al., 2021) tối ưu hóa phân rã low-rank của các lớp dense trong
LLM. Đối với một ma trận trọng số pretrained W0∈Rdm×dp và kích hoạt đầu vào x∈Rdm, LORA
phân rã W0 thành hai ma trận low-rank:
h=W0+ ∆Wx=W0+BAx (LORA)

nơi B∈Rdp×rA∈Rr×dm, và rank r=min(dm, dp). Trong quá trình fine-tuning, tất cả trọng số
pretrained được đông lạnh, và chỉ các ma trận A và B được cập nhật.

LORA adaptation có thể được sử dụng cho tất cả các lớp linear trong mỗi khối Transformer bao gồm
query Q, key K, value V, và output O của self-attention và các lớp feed-forward W1 và W2. Không
giống như (IA)3, LORA adaptation cung cấp nhiều tính linh hoạt hơn về mặt tham số được sử dụng.
Chúng ta có thể điều chỉnh dung lượng bằng cách tăng rank r của phân rã ma trận cho đến khi nó
đạt tối đa, được xác định bởi r=min(dm, dp). Để minh họa hiệu quả tham số của nó, đối với mô hình
T5 3B, LORA với rank 4, cập nhật 0.3% tham số mô hình.

∗Đối với một mô hình encoder-decoder với L số lớp ở cả hai bên, (IA)3 chỉ giới thiệu L(dk+dv+dff) tham
số mới cho encoder và L(2dk+ 2dv+dff) cho decoder, do khối encoder-decoder attention bổ sung.
4

--- TRANG 5 ---
2.2 Mixture of Experts cực kỳ Parameter Efficient
Chúng tôi đề xuất một framework Mixture of Experts (MoE) cực kỳ parameter-efficient tận dụng các
"adapter" nhẹ như expert trên một mô hình dense pretrained. Cụ thể, MoE là một họ kiến trúc mạng
neural cho phép tính toán có điều kiện thông qua nhiều expert được kích hoạt dựa trên một cơ chế
gating (router). Một lớp MoE bao gồm một mạng router R và một tập hợp n expert E1, ..., En nơi
mỗi expert Ei là một hàm được tham số hóa. Theo Fedus et al. (2022), mạng router của chúng tôi
thường bao gồm một lớp dense với trọng số có thể huấn luyện Wg∈Rdm×n theo sau bởi một hàm
softmax lấy một biểu diễn token trung gian x làm đầu vào và kết hợp đầu ra của mỗi expert dựa trên
các điểm gating s1, ..., sn:

si=R(x)i=softmax (WTgx) (Router)
y=n∑i=1si·Ei(x) (MoE)

Đối với các mô hình Transformer (Vaswani et al., 2023), các lớp feed-forward dense được thay thế
bởi các lớp MoE nơi mỗi expert Ei tương ứng với một mạng feed-forward dense độc lập. Điều này
nhân lên tổng số tham số mô hình khi kích thước mỗi expert và số lượng expert tăng. Tuy nhiên, trong
kiến trúc MoE parameter-efficient của chúng tôi, chúng tôi thay thế mỗi expert bằng một PEFT adapter
nhẹ như vector (IA)3 hay LORA adapter. Trong quá trình fine-tuning, các trọng số pretrained của
các lớp dense vẫn cố định, trong khi các expert và lớp router được huấn luyện từ đầu. Không giống
như MoE tiêu chuẩn, các expert nhẹ của chúng tôi học cách thích ứng các lớp Transformer pretrained
trong thời gian fine-tuning. Theo cách này, framework MoE của chúng tôi yêu cầu một số lượng hạn
chế cập nhật tham số và không giới thiệu kích thước mô hình lớn tổng cộng.

Ngoài hiệu quả tham số, việc lựa chọn PEFT adapter của chúng tôi cho phép tính toán routing với
soft merging. Cụ thể, vì cả vector (IA)3 và LORA adapter đều là các hàm tuyến tính, chúng tôi tính
toán trung bình có trọng số của các expert trước và sau đó áp dụng một biến đổi PEFT sử dụng expert
kết hợp Emix tương tự như Muqeeth et al. (2023):

Emix=n∑i=1si·Ei;y=Emix(x) (Soft Merging)

Chúng tôi gọi các biến thể của phương pháp này là Mixture of Vectors (MoV) và Mixture of LORA
(MoLORA) tận dụng vector (IA)3 hay LORA adapter như expert tương ứng, cả hai đều thể hiện những
cải thiện nhất quán so với phương pháp PEFT tương ứng. Hình 2 hiển thị kiến trúc của một lớp MoV
cùng với mã giả tương ứng. Chỉ cập nhật một phần nhỏ tham số thông qua MoV và MoLORA có nhiều
lợi ích thực tế không chỉ đối với huấn luyện mà còn cho thời gian suy luận, với cái sau là độc nhất
đối với kiến trúc MoE. Chúng tôi cung cấp một tổng quan ngắn gọn về những lợi ích này dưới đây:

Hiệu quả trong huấn luyện Công thức MoE cực kỳ parameter-efficient của chúng tôi dẫn đến việc
giảm đáng kể bộ nhớ. Việc đóng băng hầu hết tham số trong quá trình huấn luyện giảm overhead tính
toán của việc tính gradient cho các tham số mô hình mà còn giảm yêu cầu bộ nhớ lưu trữ trạng thái
optimizer cho mô hình. Cái sau có thể khá đáng kể tùy thuộc vào lựa chọn optimizer, ví dụ, các biến
thể của Adam (Kingma & Ba, 2017) bao gồm AdamW (Loshchilov & Hutter, 2019), yêu cầu gấp đôi
bộ nhớ cần thiết cho mỗi tham số, để lưu trữ trạng thái optimizer (ước tính cho moment thứ nhất và
thứ hai) trong khi Adafactor (Shazeer & Stern, 2018) giảm overhead này khoảng một nửa thông qua
ước tính nhân tử của moment tham số bậc hai.

Hiệu quả tại suy luận Tính mô-đun cấu trúc cố có của các phương pháp MoV và MoLORA của chúng
tôi cho phép những cải thiện bộ nhớ đáng kể tại thời gian suy luận. Đối với các mô hình MoE truyền
thống, nhiều bản sao của các khối feed-forward đầy đủ (hoặc thậm chí các bản sao hoàn chỉnh của
mô hình dựa trên kiến trúc cụ thể) cần được lưu trữ trong bộ nhớ tại thời gian suy luận là một công
việc tốn kém. Với các phương pháp của chúng tôi, bất kể loại chính xác nào, chỉ một bản sao duy nhất
của backbone mô hình cần được lưu trữ trong bộ nhớ ngoài các expert parameter-efficient nhẹ. Điều
này dẫn đến việc giảm đáng kể yêu cầu bộ nhớ tại thời gian suy luận.

3 Thí nghiệm
Bộ dữ liệu Chúng tôi tiến hành các thí nghiệm instruction-tuning sử dụng một tập hợp toàn diện
các hướng dẫn prompt từ bộ dữ liệu Public Pool of Prompts (P3) Sanh et al. (2022). Chúng tôi tuân
theo cùng thủ tục như Raffel et al. (2020) nơi mỗi tác vụ được chuyển đổi thành định dạng được cung
cấp template trong (Sanh et al., 2022). P3 là một tập hợp 62 bộ dữ liệu bao gồm nhiều tác vụ đa dạng.

Thiết lập thí nghiệm Đối với các mô hình pretrained cơ bản, chúng tôi sử dụng T5 v1.1+LM
adaptation (Lester et al., 2021) bao gồm các mô hình T5 có kích thước khác nhau từ 770M đến 11B
tham số. Đối với tất cả thí nghiệm, chúng tôi fine-tune sử dụng optimizer Adafactor (Shazeer &
Stern, 2018) với tỷ lệ học 3e−4. Chúng tôi đặt độ dài chuỗi thành 1024 cho đầu vào và 256 cho
target theo Sanh et al. (2022). Đối với tất cả biến thể MoE parameter-efficient, chúng tôi fine-tune
các mô hình T5 sử dụng batch size 32 qua 500K bước.

Baseline Chúng tôi so sánh mixture of parameter-efficient expert của chúng tôi với cả T0 baseline
như mô hình fully fine-tuned, và các phương pháp parameter-efficient fine-tuning tiêu chuẩn (IA)3
và LORA. Đối với baseline T0, dựa trên các thí nghiệm của chúng tôi với các hyperparameter khác
nhau, chúng tôi thấy rằng batch size và learning rate lớn hơn dẫn đến hiệu suất tốt hơn, do đó,
chúng tôi sao chép T0 bằng cách fine-tuning trong 10k bước với batch size 256, và learning rate
1e−3, theo Phang et al. (2023) – những hyperparameter này đạt kết quả cao hơn đáng kể như thể
hiện trong Bảng 1. Đối với (IA)3 và LORA với rank=4, chúng tôi sử dụng cùng hyper-parameter
huấn luyện như learning rate 3e−4 và batch 32 qua 500k bước.

Metrics Theo đánh giá zero-shot được trình bày trong T0 Sanh et al. (2022), chúng tôi kiểm tra
phương pháp và các baseline của chúng tôi trên 8 bộ dữ liệu giữ lại (chưa thấy trong quá trình huấn
luyện) – ANLI (Nie et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al.,
2019), và 5 bộ dữ liệu Super Glue Wang et al. (2020). Những bộ dữ liệu này bao gồm các tác vụ
khác nhau từ giải quyết coreference, suy luận ngôn ngữ tự nhiên, câu hỏi trắc nghiệm, hoàn thành
câu chuyện, và phân biệt nghĩa từ. Chúng tôi tính toán độ chính xác trung vị cho mỗi bộ dữ liệu
đánh giá qua các template prompt khác nhau và sau đó báo cáo kết quả theo bộ dữ liệu cùng với
trung bình qua tất cả bộ dữ liệu. Chúng tôi cũng bao gồm độ chính xác trung bình cho tất cả bộ
dữ liệu đánh giá trong Phụ lục.

6

--- TRANG 6 ---
Hạ tầng Tất cả thí nghiệm được tiến hành trên máy TPU v4 lên đến 256 pod slice. Để huấn luyện,
đánh giá, và suy luận của tất cả các mô hình được thí nghiệm, chúng tôi sử dụng framework SeqIO
và T5X (Roberts et al., 2022) cho phép song song hóa dữ liệu và mô hình qua các lõi TPU với xử lý
dữ liệu tuần tự tích hợp.

3.1 Ablation
Với việc không có công việc nào cho đến nay nghiên cứu MoE trong setting cực kỳ parameter-efficient,
chúng tôi cũng tìm hiểu các đặc điểm chính của phương pháp đề xuất bằng cách chạy các ablation
nghiêm ngặt. Chúng tôi mô tả cả hai một cách ngắn gọn, cùng với thiết lập thí nghiệm dưới đây:

Routing Input: Token vs Sentence Embeddings Làm thế nào một inductive bias rõ ràng cho
task representation dưới dạng instruction embedding ảnh hưởng đến routing và tổng quát hóa
downstream? Trong các phương pháp MoV và MoLORA chính của chúng tôi, các lớp router lấy
embedding trung gian của token đầu vào làm đầu vào tương tự như các kiến trúc MoE khác (Shazeer
et al., 2017; Fedus et al., 2022). Tuy nhiên, như một sự thay thế, một sentence embedding có thể
được tính toán cho mỗi instruction (prompt với đầu vào tương ứng) và được sử dụng làm đầu vào
cho router (Ye et al., 2022). Để so sánh cả hai – sentence embedding cho mỗi instruction được
dẫn xuất sử dụng bộ encoder Sentence-T5 (Ni et al., 2022), được huấn luyện với mô hình retrieval
T5-XL (Ni et al., 2021). Encoder này được khởi tạo từ T5 pretrained và được huấn luyện trên các
nguồn dữ liệu đa dạng như được nêu trong Ni et al. (2022). Không có fine-tuning bổ sung, mỗi
chuỗi instruction bao gồm một template prompt và câu đầu vào, được truyền để truy xuất các embedding
với chiều 768.

Routing Strategy: Soft vs Discrete Chiến lược routing tốt nhất trong parameter-efficient MoE
là gì? Trong framework MoE của chúng tôi, chúng tôi sử dụng soft merging của expert như chiến
lược routing. Soft merging đề cập đến trung bình có trọng số của tất cả expert được tính toán trong
một khối routing được chỉ định. Như một thay thế, chiến lược routing top-k rời rạc như được sử dụng
trong kiến trúc MoE tiêu chuẩn giới thiệu độ thưa thớt và giảm lượng tính toán (Shazeer et al.,
2018; Fedus et al., 2022). Trong phương pháp routing top-k, thay vì xem xét tất cả expert cho một
quyết định, chỉ 'k' expert hàng đầu, được xác định bởi router, được chọn cho việc tính toán. Lưu ý
rằng, mặc dù tính toán có điều kiện đối với top-k expert, bộ nhớ yêu cầu phụ thuộc vào tổng số
expert.

Chúng tôi đánh giá lựa chọn top-k với k={1,2} như chúng được đề xuất bởi công việc trước đây
(Shazeer et al., 2017; Fedus et al., 2022). Kết quả cho những chiến lược này được trình bày chi tiết
trong Phần 4.4. Ngoài ra, chúng tôi đánh giá routing rời rạc với top-k sử dụng load balancing theo
Fedus et al. (2022) thúc đẩy lựa chọn top-k cân bằng thông qua một auxiliary loss, nhắm đến phân
bố khối lượng công việc công bằng giữa các expert.

4 Kết quả và Thảo luận
Parameter efficient MoE vs PEFT Phương pháp MoE của chúng tôi so sánh như thế nào với
một PEFT expert đơn lẻ? Bảng 1 so sánh hiệu suất zero-shot của các phương pháp PEFT ((IA)3
và LORA), và các biến thể của parameter-efficient MoE của chúng tôi (MoV và MoLORA), sử dụng
T5-3B làm mô hình cơ bản. Chúng tôi quan sát thấy rằng các biến thể MoE của chúng tôi (MoV và
MoLORA) mang lại một sự cải thiện hiệu suất đáng kể so với vector (IA)3 và LORA adapter tiêu chuẩn.

MoV sử dụng 30 expert đạt được cải thiện hiệu suất 14.57% so với bản dense tương ứng
7

--- TRANG 7 ---
Kết quả Zero-shot ở Quy mô 3B
Mô hình % Tham số. ANLI CB RTE WSC WIC Copa WNG HS Trung bình
Full-FTT0-3B (Sanh et al., 2022) 100% 33.46 50.0 64.08 64.42 50.39 74.92 50.51 27.51 51.91
T0-3B (sao chép của chúng tôi) 100% 41.08 80.36 76.17 53.37 53.92 88.94 57.46 29.19 60.06
PEFT(IA)30.018% 34.08 50.0 66.43 56.25 55.41 79.08 52.09 29.91 52.90
LORA 0.3% 37.5 75.57 73.53 61.02 51.25 83.6 54.33 25.32 57.51
Phương pháp của chúng tôiMoV-10 0.32% 38.92 75.0 78.88 62.5 52.19 85.77 55.96 30.24 59.93
MoV-30 0.68% 38.7 78.57 80.87 63.46 51.1 87.25 56.27 28.63 60.61
MoV-60 1.22% 38.83 76.79 74.55 60.1 52.66 89.79 55.49 30.47 59.83
MoLORA-10 3.18% 38.5 78.57 78.16 63.46 50.86 86.5 55.41 26.72 59.77
MoLORA-15 4.69% 40.0 80.36 80.51 62.98 50.86 89.0 55.33 27.3 60.79
Bảng 1: Kết quả trung vị trung bình trên các tác vụ chưa thấy cho full model fine-tuning (T0), các
phương pháp parameter-efficient fine-tune ((IA)3 và LORA) và mixture of parameter-efficient expert
của chúng tôi (MoV và MoLORA), sử dụng mô hình cơ bản T5-3B (Raffel et al., 2020). Lưu ý rằng
sao chép T0 của chúng tôi hoạt động cao hơn đáng kể so với T0 gốc xác nhận công việc trước đây
(Phang et al., 2023; Ivison et al., 2023).

(IA)3. Sự cải thiện này nhất quán qua tất cả các tác vụ chưa thấy và được đạt được với sự gia tăng
biên trong số lượng tham số được cập nhật – chỉ thêm 0.018% tham số cho mỗi expert. Trong bối
cảnh LORA, MoLORA của chúng tôi được trang bị 15 expert, đạt được sự gia tăng điểm trung vị
trung bình 5.70%. Sự cải thiện này ít đáng kể hơn đáng kể khi so sánh với MoV. Chúng tôi cho rằng
sự khác biệt này do sự khác biệt trong số lượng tham số cập nhật trong LORA adapter và vector (IA)3
(0.3% vs 0.018%). Nhìn chung, việc học một mixture cho cả MoV và MoLORA thay vì một mô hình
dense đơn lẻ dẫn đến những cải thiện đáng chú ý trong hiệu suất zero-shot.

MoV vượt trội hơn MoLORA với cùng ngân sách tham số Giữa các phương pháp của chúng tôi,
MoV đạt được sự cân bằng hiệu suất-chi phí tham số tốt hơn ở mô hình cơ bản 3B tham số. Như
thể hiện trong biểu đồ bên trái trong hình 1, MoV với 30 expert, chỉ cập nhật 0.68% tất cả tham số,
đạt được hiệu suất gần như tương đương với MoLORA với 15 expert cập nhật 4.69% tham số. Điều
này cho thấy hiệu quả của các phương pháp MoE của chúng tôi ngay cả với các expert nhỏ ở quy mô
mô hình cơ bản lớn.

Parameter efficient MoE vs full fine-tuning MoE so sánh như thế nào với việc cập nhật tất cả
tham số trong quá trình finetuning? Như thể hiện trong Bảng 1, khi so sánh với T0-3B fully fine-tuned,
các phương pháp đề xuất của chúng tôi, MoV và MoLORA cả hai với 10 expert, đều tương đương với
full fine-tuning. Điều này ấn tượng vì MoV-10 chỉ cập nhật 0.32% tất cả tham số mô hình. Hơn nữa,
khi tăng số lượng expert từ 10 lên 15 và 30 cho MoV và MoLORA tương ứng, cả hai phương pháp
của chúng tôi đều vượt trội hơn full fine-tuning với một biên nhỏ.

4.1 Parameter-efficient MoE mở rộng quy mô như thế nào với kích thước mô hình cơ bản?
Hình 1 (phải) hiển thị đặc tính mở rộng quy mô của MoV với 60 expert so sánh với (IA)3 và full
fine-tuning cho các mô hình cơ bản 770M, 3B và 11B tham số. Chúng tôi thấy rằng qua tất cả kích
thước mô hình chúng tôi đánh giá, parameter-efficient MoE của chúng tôi liên tục duy trì hiệu suất
cao hơn so với PEFT tiêu chuẩn và đạt được kết quả tương đương với full fine-tuning.

MoV hưởng lợi từ việc mở rộng quy mô Ở tất cả kích thước mô hình, MoV-60 vượt trội đáng kể
so với (IA)3 tiêu chuẩn. Nó cũng gần hơn nhiều trong hiệu suất với full fine-tuning hơn một expert
đơn lẻ. Ví dụ, ở 770M
8

--- TRANG 8 ---
(IA)3 MoV-60 LoRA MoLoRA-1050525456586062Độ chính xác trung vị trung bìnhFull Fine-TuningKích thước mô hình 770M
(IA)3 MoV-30 LoRA MoLoRA-1550525456586062Độ chính xác trung vị trung bìnhFull Fine-TuningKích thước mô hình 3BPhương pháp của chúng tôi vs PEFT tiêu chuẩnHình 3: So sánh các biến thể hoạt động tốt nhất từ mixture of PEFT expert đề xuất của chúng tôi
với các bản dense tương ứng qua T5-Large (Trái) và T5-XL (Phải).

tham số, có khoảng cách hiệu suất 12.34% giữa (IA)3 và full fine-tuning vs 5.56% cho MoV-60.
Khi mô hình cơ bản mở rộng quy mô, MoV trở nên cạnh tranh hơn với full fine-tuning. Đối với các
mô hình 3B và 11B tham số, MoV-60 đạt hiệu suất xấp xỉ tương đương với full fine-tuning, mặc dù
cập nhật ít hơn 1.3% tổng tham số.

MoLORA vượt trội hơn MoV ở chế độ kích thước mô hình nhỏ hơn Như đã thảo luận trong kết
quả chính, ở kích thước mô hình lớn hơn, MoV đạt được sự cân bằng hiệu suất-hiệu quả tham số
tốt hơn so với MoLORA. Ngược lại, ở quy mô 770M, MoLORA với 10 expert cập nhật 3.18% tổng
tham số, hoạt động tốt hơn so với MoV-60 và gần như khớp với hiệu suất của full fine-tuning (Hình
3). Cuối cùng, tương tự như MoV, MoLORA đạt hiệu suất cao hơn LORA ở cả quy mô 770M và 3B.

4.2 Số lượng expert ảnh hưởng như thế nào đến hiệu suất downstream?
Biểu đồ trung tâm của Hình 4 hiển thị hiệu suất của MoV với số lượng expert khác nhau ở tất cả
kích thước mô hình. Chúng tôi thấy rằng việc tăng số lượng expert thường cải thiện hiệu suất tác vụ
chưa thấy. Tuy nhiên, sự cải thiện này phụ thuộc vào số lượng expert cụ thể và kích thước mô hình
cơ bản. Đối với cả mô hình cơ bản 770M và 11B tham số, phương pháp MoV của chúng tôi đạt hiệu
suất tốt nhất bằng cách sử dụng 60 expert. Để minh họa, khi số lượng expert tăng từ 10 lên 60, độ
chính xác trung vị trung bình cải thiện từ 52.47 lên 53.63 cho mô hình 770M và từ 62.3 lên 64.08
cho mô hình 11B. Tuy nhiên, đối với mô hình 3B, chỉ sử dụng 30 expert, cập nhật 0.68% tham số,
đạt độ chính xác đỉnh với điểm số 60.61 ở quy mô này, vì hiệu suất trì trệ khi sử dụng 60 expert.

Xu hướng cải thiện hiệu suất này bằng cách mở rộng quy mô nhiều expert hơn được củng cố thêm
trong bối cảnh MoLORA; khi mở rộng quy mô expert từ tập (5, 10, 15), có sự gia tăng tương ứng
trong điểm trung vị trung bình, ghi nhận ở 58.6, 59.77, và 60.79, tương ứng.

4.3 Chiến lược routing tốt nhất trong parameter-efficient MoE là gì?
Trong Hình 4, biểu đồ bên phải nhất hiển thị hiệu suất tác vụ chưa thấy tổng thể khi sử dụng các
chiến lược routing khác nhau cho MoV. Cụ thể, chúng tôi so sánh soft merging của 10 expert (đường
nét đứt) với
9

--- TRANG 9 ---
770M 3B 11B
Thành phần mô hình cơ bản525456586062646668Độ chính xác trung vị trung bìnhToken vs Sentence Embedding cho Routing
token
sentence embedding
1 10 30 60
Số lượng expert5052545658606264Độ chính xác trung vị trung bình
Hiệu quả của số lượng expert vs kích thước mô hình
11B
3B
770M
Chiến lược routing52535455565758596061Độ chính xác trung vịMoV-10Lựa chọn Top-K
MoV-10 (top-2)
MoV-10 (top-1)
MoV-2
MOV-1 = (IA)3Hình 4: Trái: Hiệu suất zero-shot của việc truyền embedding của chuỗi token cho router
vs truyền token cho router. Giữa: Hiệu suất zero-shot qua các kích thước mô hình T5 (Large, XL,
XXL) khi số lượng expert tăng. Phải: Hiệu quả của việc kích hoạt top-k expert.

routing rời rạc top-2 và top-1. Chúng tôi quan sát thấy rằng soft merging vượt trội đáng kể so với
routing rời rạc trong setting MoV-10. Cụ thể, đối với routing rời rạc với top-k expert, nơi k là 1
và 2, MoE đạt độ chính xác trung vị trung bình là 54.92 và 57.45 tương ứng. Ngược lại, sử dụng
phương pháp soft merging, nơi tất cả expert được kích hoạt, chúng tôi quan sát độ chính xác là 59.93.

Hơn nữa, để hiểu liệu chúng tôi có thể phục hồi mất mát hiệu suất của routing top-k bằng cách sử
dụng load balancing không, chúng tôi đã tích hợp loss balancing theo Fedus et al. (2022). Tuy nhiên,
chúng tôi thấy rằng lựa chọn top-k của k=2 với load balancing loss dẫn đến giảm hiệu suất thêm
1.5 điểm trung vị trung bình.

Cùng nhau, những kết quả này cho thấy rằng trong setting MoE cực kỳ parameter-efficient, soft
merging cho phép hiệu suất vượt trội. Lưu ý rằng các chiến lược routing top-2 và top-1 (trong số
10 expert) hoạt động tốt hơn MoV chỉ với 2 expert và một expert đơn lẻ (IA)3 tương ứng, cho thấy
rằng soft merging hoạt động tốt hơn khi sử dụng số lượng expert lớn hơn.

4.4 Thông tin tác vụ rõ ràng trong routing có dẫn đến hiệu suất cao hơn không?
Để hiểu tác động của một inductive bias rõ ràng đối với task representation trong framework MoE
của chúng tôi, chúng tôi so sánh việc sử dụng sentence embedding của instruction với token embedding
cho đầu vào routing. Những sentence embedding này được thu thập offline sử dụng một mô hình
sentence embedding bên ngoài. Ở đây, chúng tôi nhắm đến đánh giá cách thông tin tác vụ rõ ràng
ảnh hưởng đến quyết định của router và khả năng tổng quát hóa tiếp theo của mô hình trong các tác
vụ downstream. Biểu đồ bên trái nhất của Hình 4 hiển thị hiệu suất của token routing và sentence
routing ở tất cả kích thước mô hình. Chúng tôi thấy rằng token routing thể hiện hiệu suất vượt trội
với cải thiện 3.03%, 8.86%, và 0.94% cho kích thước mô hình cơ bản 770M, 3B, và 11B tương ứng.
Những kết quả này cho thấy rằng mức độ inductive bias cao hơn cho bộ dữ liệu tác vụ không nhất
thiết có lợi vì các phương pháp của chúng tôi có thể thu được một tập hợp đa dạng kiến thức tác vụ
trực tiếp từ biểu diễn ẩn của token. Hơn nữa, token routing cho phép sử dụng các expert đã học và
lớp routing mà không cần bất kỳ thông tin tác vụ trước đó nào cho các tác vụ chưa thấy.

4.5 Các expert có chuyên biệt hóa trong kiến thức đa dạng qua các tác vụ khác nhau không?
Để hiểu cách routing expert khác nhau cho các tác vụ khác nhau, chúng tôi xem xét kỹ hơn cách
các expert được kích hoạt cho nhiều tác vụ đa dạng. Hình 5 hiển thị xác suất expert trung bình
cho MoV với 5 expert nằm trong các lớp feed-forward ở khối decoder cuối cùng ở T5 tham số 770M
10

--- TRANG 10 ---
Expert 1 Expert 2 Expert 3 Expert 4 Expert 50.00.10.20.30.40.50.6Xác suất routing trung bìnhCác tác vụ huấn luyện
quail
rotten_tomatoes
common_gen
Expert 1 Expert 2 Expert 3 Expert 4 Expert 50.00.10.20.30.40.50.6Các tác vụ đánh giá
super_glue_cb
super_glue_wic
winogrande

Hình 5: Xác suất routing expert trung bình cho các kích hoạt trung gian ở lớp feedforward cuối cùng.
Giá trị được tính trung bình qua token và batch. Các expert được trọng số khác nhau trong soft merging
tùy thuộc vào tác vụ. Trái: Đo lường trên các tác vụ thấy trong quá trình huấn luyện. Phải: Đo lường
trên các tác vụ đánh giá chưa thấy.

mô hình. Chúng tôi chọn khối decoder cuối cùng vì nó đã được chỉ ra rằng các lớp sâu hơn học thông
tin cụ thể tác vụ hơn (Rogers et al., 2020). Chúng tôi vẽ biểu đồ xác suất routing trung bình cho
cả tác vụ huấn luyện và tác vụ đánh giá chưa thấy trong quá trình huấn luyện, để hiểu tổng quát
hóa cross-task thông qua lăng kính của các expert nếu các kỹ năng học được tại thời gian huấn luyện
tổng quát hóa cho các tác vụ chưa thấy tại thời gian đánh giá.

Trực quan, nếu các expert thực sự đã học các kỹ năng khác nhau, chúng tôi mong đợi rằng chúng
đóng góp ở các mức độ khác nhau cho các tác vụ có bản chất khác nhau. Lượng đóng góp được phản
ánh trực tiếp trong xác suất routing của mỗi expert vì chúng tôi sử dụng soft merging tức là tổng
của các vector expert được trọng số bởi xác suất routing như mô tả trong Hình 2. Như vậy, các xác
suất routing trung bình được vẽ trong Hình 5 cung cấp một bức tranh tổng thể về đóng góp của mỗi
expert, tùy thuộc vào tác vụ downstream.

Chuyên biệt hóa qua các tác vụ chưa thấy vs đã thấy Như mô tả trong Hình 5, cả tác vụ đánh giá
và huấn luyện đều dẫn đến việc kích hoạt các expert ở các mức độ khác nhau. Ví dụ, cả quail và
super_glue_cb đều kích hoạt Expert 3 nhiều nhất trong số 5 expert, theo sau là Expert 4 nhưng
khác nhau cả về mặt đóng góp tương đối của mỗi expert và thứ tự của 3 expert còn lại dựa trên
xác suất routing. Một mẫu tương tự có thể được quan sát cho common_gen & winogrande vì cả hai
đều kích hoạt Expert 2 nhiều nhất nhưng khác nhau về mặt khác. Nhìn chung, việc chuyên biệt hóa
routing dường như xảy ra bất kể tác vụ downstream có được huấn luyện hay không, cho thấy rằng
chuyên biệt hóa expert là cố có và có thể chuyển giao từ các tác vụ đã thấy sang các tác vụ chưa thấy.

4.6 Độ nhạy cảm Hyper-parameter
Với tính nhạy cảm được ghi nhận rộng rãi của kiến trúc kiểu MoE đối với hyperparameter (Fedus et
al., 2022; Shazeer et al., 2017), chúng tôi đã chạy các nghiên cứu ablation mở rộng để khám phá
tính đặc thù của các phương pháp PEFT trong bối cảnh MoE. Chúng tôi thử nghiệm với các batch
size 32, 128, 256, và 2048 và chúng tôi thấy rằng batch size càng lớn, MoE của chúng tôi càng có
khả năng sụp đổ thành một expert đơn lẻ. Phát hiện thực nghiệm của chúng tôi cộng hưởng với Shen
et al. (2023) cũng thấy rằng một batch nhỏ là cần thiết cho huấn luyện ổn định. Ví dụ, bằng cách
thử nghiệm với batch size 2048 và đánh giá mỗi 5K bước lên đến 20K, chúng tôi quan sát thấy rằng
hiệu suất của parameter-efficient MoE của chúng tôi xấu đi sau 5K bước, hội tụ về mức hiệu suất
tương tự như các bản dense tương ứng. Ngoài ra, chúng tôi thử nghiệm với các learning rate khác
nhau từ 3e−3 đến 6e−4 nơi chúng tôi khám phá ra đối với các phương pháp của chúng tôi, một
learning rate nhỏ hơn là 3e−4 dẫn đến hiệu suất cao hơn so với bản PEFT dense tương ứng và full
fine-tuning. Learning rate nhỏ hơn ổn định hóa huấn luyện trong các expert parameter-efficient
bằng cách ngăn chặn các cập nhật nhanh, mất cân bằng có thể triệt tiêu tính đa dạng và dẫn đến
các giải pháp tối ưu dưới mức.

5 Công việc liên quan
Mixture-of-Experts Mixture-of-Experts (MoE) đã được nghiên cứu kỹ lưỡng trong Natural Language
Processing (Lou et al., 2022; Mustafa et al., 2022; Shazeer et al., 2017; Lepikhin et al., 2020;
Fedus et al., 2022; Du et al., 2022; Zoph et al., 2022; Clark et al., 2022; Zhou et al., 2022; Komatsuzaki
et al., 2023; Kudugunta et al., 2021; Zuo et al., 2022) như một cách hiệu quả tăng khả năng của
mô hình về kích thước tham số nơi các phần nhất định của mô hình được kích hoạt trong khi tính
toán được giữ nguyên hoặc gần với bản dense tương ứng. Trong bối cảnh MoE, có một nhóm công
việc tập trung vào cải thiện routing (Hazimeh et al., 2021; Lewis et al., 2021; Roller et al., 2021;
Zhou et al., 2022) bao gồm random routing (Zuo et al., 2022) kích hoạt tất cả expert thông qua
trung bình có trọng số (Eigen et al., 2014) đến lựa chọn thưa thớt một hoặc k expert (Fedus et al.,
2022; Du et al., 2022). MoE cũng đã được đầu tư trong setting multi-task bao gồm multilingual
neural machine translation (Hazimeh et al., 2021; Kudugunta et al., 2021). Không giống như những
nghiên cứu này, nghiên cứu của chúng tôi giải quyết MoE bằng cách mở rộng quy mô cả khối lượng
dữ liệu và số lượng tác vụ, nhằm giảm thiểu tính bất ổn cố có trong việc huấn luyện các mô hình
MoE. Nhưng trọng tâm chính của chúng tôi vẫn là đạt được fine-tuning hiệu quả.

Gần đây, Shen et al. (2023) đã làm nổi bật cách instruction fine-tuning với các tác vụ được mở
rộng quy mô có thể chống lại các thách thức tổng quát hóa gắn liền với các mô hình MoE. Khác với
điều này, nghiên cứu của chúng tôi xem xét kỹ hiệu quả của instruction fine-tuning trong lĩnh vực
MoE, đặc biệt tập trung vào một tập hợp độc nhất của các thành phần PEFT, xem xét chi phí bộ nhớ
của MoE truyền thống có thể cấm đoán đối với nhiều nhà thực hành. Tương tự như công việc nói
trên, Ye et al. (2022) đã sử dụng MoE trong bối cảnh multi-task, sử dụng BART Lewis et al. (2019)
làm mô hình pretrained của họ. Tuy nhiên, họ giới hạn phạm vi thí nghiệm của mình ở quy mô nhỏ
hơn và sử dụng các bản sao của mỗi lớp transformer làm expert, đơn giản là nhân mô hình với số
lượng expert. Công việc của chúng tôi, mặt khác, trình bày hiệu quả tham số cực độ với các expert
nhỏ ở quy mô lớn lên đến mô hình cơ bản 11B tham số.

Instruction Tuning Instruction tuning, như được làm sáng tỏ trong (Sanh et al., 2022; Wei et al.,
2022; Mishra et al., 2022), là một kỹ thuật nơi một mô hình ngôn ngữ được fine-tuned trên một
tập hợp các tác vụ sử dụng các prompt và response được ghép đôi. Mục tiêu chính của kỹ thuật
này là cho phép mô hình dự đoán response chính xác dựa trên các prompt được cung cấp, từ đó tăng
cường khả năng hiểu và thực hiện hướng dẫn hiệu quả. Phương pháp này đã nhận được sự chú ý
đáng kể do thành công rõ rệt của nó trong việc tăng cường hiệu suất zero-shot trên các tác vụ mà
mô hình chưa từng tiếp xúc trước đây. Ngoài ra, instruction tuning đã dẫn đến những đột phá như
Chain of Thought Prompting (Wei et al., 2023) nơi việc chia nhỏ các vấn đề phức tạp thành các
bước nhỏ hơn để tạo ra lý luận trung gian cùng với giải pháp cuối cùng, PaLM (Chowdhery et al.,
2022), FLAN (Wei et al., 2022). Trong công việc của chúng tôi, chúng tôi khám phá việc sử dụng
instruction fine-tuning với ý định khai thác lợi ích của nó cho phép mô hình học từ một tập hợp
đa dạng đầu vào nơi các mô hình kiểu mixture of expert phù hợp tốt, để tăng cường hiệu suất đánh
giá trên các tác vụ chưa thấy. Mục tiêu của chúng tôi vẫn là tối ưu hóa hiệu quả tính toán mà không
làm tổn hại đến hiệu suất zero-shot.

Parameter-Efficient Fine-tuning. Houlsby et al. (2019) đã thiết lập "adapter" trong lĩnh vực NLP
để fine-tune BERT. Có nhiều biến thể của adapter với các lựa chọn thiết kế khác nhau (Bapna
12

--- TRANG 11 ---
et al., 2019; Pfeiffer et al., 2021). Li & Liang (2021) đề xuất cập nhật các soft prompt được nối
vào embedding hoặc đầu ra lớp thay vì adapter. Zaken et al. (2022) cho thấy rằng chỉ cập nhật
một tập con nhỏ tham số trong quá trình fine-tuning (ví dụ chỉ bias) là rất hiệu quả. Hu et al.
(2021) đề xuất LORA dựa trên các ma trận phân rã low-rank của các lớp transformer. Họ cho thấy
hiệu suất vượt trội với ngân sách tham số nhỏ hơn và không có chi phí suy luận vì các tham số LORA
có thể được áp dụng offline cho mô hình baseline. Liu et al. (2022) đề xuất (IA)3, các vector cụ
thể tác vụ để sửa đổi attention activation. Thay vì sử dụng các lớp feedforward được chèn vào
các lớp transformer làm adapter, họ học các vector để cập nhật (bằng broadcast multiplication)
các ma trận trọng số key, value, và linear layer. Không giống như các phương pháp PEFT khác,
(IA)3 không gây ra chi phí suy luận bổ sung nào và cho phép mix-batches (từ các bộ dữ liệu khác
nhau). Tính chất nhân của (IA)3 tạo ra một cơ hội thú vị cho loại mô hình mixture-of-expert mà
không có overhead song song hóa.

Chen et al. (2023) thử nghiệm với các không gian thiết kế khác nhau (về cơ bản là một tìm kiếm
hyperparameter) cho PEFT. Họ đề xuất bốn giai đoạn: 1) nhóm các lớp thành các tập hợp khác
nhau; 2) thêm tham số có thể huấn luyện cho mỗi nhóm; 3) quyết định nhóm nào nên được huấn
luyện; 4) gán các nhóm với các chiến lược huấn luyện khác nhau. Phát hiện của họ là các kiến trúc
khác nhau có các setting tốt nhất khác nhau. Chúng tôi đã chọn (IA)3 và LORA làm các thành phần
PEFT của chúng tôi vì chúng cung cấp sự cân bằng tối ưu giữa hiệu suất và hiệu quả tham số
(Mahabadi et al., 2021; Liu et al., 2022).

Một số nghiên cứu đã khám phá PEFT trong bối cảnh MoE hoặc theo cách tương tự, mặc dù có một
số khác biệt nhất định. Ví dụ, Wang et al. (2022) tập trung vào single-task fine-tuning sử dụng
mixture of adapter cho BERT base với 110M tham số (Devlin et al., 2019) và RoBERTa large với
355M tham số (Liu et al., 2019), kết hợp random routing, và áp dụng đánh giá few-shot. Khác với
điều này, công việc của chúng tôi tập trung vào instruction-tuning với nhiều tác vụ có mặt trong
quá trình fine-tuning. Chúng tôi nhấn mạnh hiệu quả của phương pháp này bằng cách kiểm tra
nghiêm ngặt lên đến mô hình text-to-text 11B tham số Raffel et al. (2020), triển khai token routing,
và nhấn mạnh nghiêm ngặt đánh giá trên một tập hợp các tác vụ chưa thấy (giữ lại) để nhấn mạnh
tiềm năng của instruction tuning. Trong một công việc khác, Ponti et al. (2022) giới thiệu Polytropon,
bao gồm việc học các adapter (được gọi là 'skill') cụ thể cho mỗi tác vụ và sử dụng một ma trận
nhị phân task-skill để xác định tập hợp skill liên quan đến mỗi tác vụ. Trong phương pháp của họ,
các ví dụ đầu vào quyết định việc lựa chọn adapter. Những adapter này sau đó được tổng hợp, và
adapter đơn lẻ kết quả được tích hợp vào kiến trúc tổng thể. Mở rộng framework Polytropon, Caccia
et al. (2023) triển khai một tập hợp skill riêng biệt cho mỗi lớp trong biến thể của họ có tên
Polytropon-S. Họ giới thiệu một hàm routing xác định, đi sâu vào các inductive bias bổ sung, cho
thấy hiệu quả lên đến các mô hình 3B, và họ không sử dụng kiến trúc kiểu MoE. Nghiên cứu của
chúng tôi trình bày sự khác biệt với hai nghiên cứu này. Cụ thể, thiết lập thí nghiệm chính của
chúng tôi sử dụng MoE không yêu cầu bất kỳ task identifier cụ thể nào trong quá trình fine-tuning
bằng việc sử dụng chiến lược token routing của chúng. Theo cách này, chúng tôi có thể đánh giá
các MoE instruction-tuned của chúng tôi trên các tác vụ chưa thấy mà không cần bất kỳ task-specific
few-shot fine-tuning nào thêm. Chúng tôi đã chỉ ra tính chất mở rộng quy mô của MoE trong setting
này bằng cách fine-tuning các mô hình lên đến 11B tham số.

6 Kết luận
Công việc này giới thiệu MoE trong một môi trường tính toán cực kỳ hạn chế. Chúng tôi đề xuất
giới thiệu Mixture of Vectors (MoV) và Mixture of LoRA (MoLORA) để giảm thiểu các thách thức
liên quan đến việc mở rộng quy mô LLM instruction-tuned ở quy mô. Phương pháp của chúng tôi
vượt trội hơn các kỹ thuật parameter-efficient và đạt được hiệu suất tương đương với full fine-tuning
trên các tác vụ chưa thấy bằng cách cập nhật ít hơn 1% tham số mô hình 3B và 11B. Tỷ lệ phần
trăm này có thể thay đổi tùy thuộc vào kích thước mô hình cơ bản và số lượng expert liên quan.
Các thí nghiệm mở rộng của chúng tôi, bao gồm các ablation nghiêm ngặt qua các kích thước mô
hình, biểu diễn của token vs embedding, soft vs top-k routing, xác nhận hiệu quả của phương pháp
của chúng tôi qua các tác vụ chưa thấy đa dạng, làm nổi bật độ chính xác vượt trội và hiệu quả
tính toán. Hơn nữa, tính linh hoạt của framework của chúng tôi tích hợp liền mạch với các chiến
lược parameter-efficient khác và vẫn tương thích với các kỹ thuật tăng cường hiệu quả như quantization.

Hạn chế Một ràng buộc chính của framework thí nghiệm của chúng tôi là tập trung vào các mô hình
text-to-text, như T5, mà không mở rộng đánh giá cho decoder-only như các mô hình kiểu GPT. Chúng
tôi để lại điều này như chủ đề của công việc tương lai. Ngoài ra, đánh giá của chúng tôi chỉ trong
bối cảnh fine-tuning. Khám phá hiệu quả của nó trong giai đoạn pre-training vẫn là một hướng cho
nghiên cứu tương lai.

Tài liệu tham khảo
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James
Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,
Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy
Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello
Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado,
John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,
Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,
Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,
Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny
Zhou, Slav Petrov, và Yonghui Wu. Palm 2 technical report, 2023.

Ankur Bapna, Naveen Arivazhagan, và Orhan Firat. Simple, scalable adaptation for neural machine
translation, 2019.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, và Dario Amodei. Language models are few-shot learners, 2020.

Lucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, và Alessandro Sordoni.
14

--- TRANG 12 ---
Multi-head adapter routing for cross-task generalization, 2023.

Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, và Diyi Yang. Parameter-efficient
fine-tuning design spaces, 2023.

Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, và Yuanzhi Li. Towards understanding the
mixture-of-experts layer in deep learning. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=MaYzugDmQV.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra,
Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan
Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan
Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat,
Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping
Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
Denny Zhou, Quoc V. Le, và Jason Wei. Scaling instruction-finetuned language models, 2022.

Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,
Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche,
Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones,
Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich
Elsen, Koray Kavukcuoglu, và Karen Simonyan. Unified scaling laws for routed language models,
2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019.

Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,
Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,
Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng
Chen, và Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.

David Eigen, Marc'Aurelio Ranzato, và Ilya Sutskever. Learning factored representations in a deep
mixture of experts, 2014.

William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2022.
15

--- TRANG 13 ---
Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,
Rahul Mazumder, Lichan Hong, và Ed H. Chi. Dselect-k: Differentiable selection in the mixture
of experts with applications to multi-task learning, 2021.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
và Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.

Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, và Matthew E Peters. Hint:
Hypernetwork instruction tuning for efficient zero-and few-shot generalisation. Trong Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 11272–11288, 2023.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language models,
2020.

Diederik P. Kingma và Jimmy Ba. Adam: A method for stochastic optimization, 2017.

Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua
Ainslie, Yi Tay, Mostafa Dehghani, và Neil Houlsby. Sparse upcycling: Training mixture-of-
experts from dense checkpoints, 2023.

Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang
Luong, và Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference,
2021.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding, 2020.

Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt
tuning, 2021.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, và Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension, 2019.

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. Base layers:
Simplifying training of large, sparse models. Trong Marina Meila và Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pp. 6265–6274. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.pr
ess/v139/lewis21a.html.

Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.

Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, và Colin
Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning,
2022.
16

--- TRANG 14 ---
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach, 2019.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.
Le, Barret Zoph, Jason Wei, và Adam Roberts. The flan collection: Designing data and methods
for effective instruction tuning, 2023.

Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization, 2019.

Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, và Yang You. Cross-token modeling with conditional
computation, 2022.

Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, và James Henderson. Parameter-
efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, và Hannaneh Hajishirzi. Cross-task generalization
via natural language crowdsourcing instructions, 2022.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le
Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir
Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,
Edward Raff, và Colin Raffel. Crosslingual generalization through multitask finetuning, 2023.

Mohammed Muqeeth, Haokun Liu, và Colin Raffel. Soft merging of experts with adaptive routing,
2023.

Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, và Neil Houlsby. Multimodal
contrastive learning with limoe: the language-image mixture of experts, 2022.

Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,
Yi Luan, Keith B. Hall, Ming-Wei Chang, và Yinfei Yang. Large dual encoders are generalizable
retrievers, 2021.

Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, và Yinfei
Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Trong Findings
of the Association for Computational Linguistics: ACL 2022, pp. 1864–1874, 2022.

Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, và Douwe Kiela. Adversarial
nli: A new benchmark for natural language understanding, 2020.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. Adapter-
fusion: Non-destructive task composition for transfer learning, 2021.

Jason Phang, Yi Mao, Pengcheng He, và Weizhu Chen. Hypertuning: Toward adapting large
language models without back-propagation. Trong International Conference on Machine Learning,
pp. 27854–27875. PMLR, 2023.

Edoardo M. Ponti, Alessandro Sordoni, Yoshua Bengio, và Siva Reddy. Combining modular skills
in multitask learning, 2022.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer, 2020.
17

--- TRANG 15 ---
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André
Susano Pinto, Daniel Keysers, và Neil Houlsby. Scaling vision with sparse mixture of experts.
Advances in Neural Information Processing Systems, 34:8583–8595, 2021.

Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel
Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor
Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares,
Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian,
Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan
Garrette, JamesLee-Thorp, ColinRaffel, NoamShazeer, MarvinRitter, MaartenBosma, Alexandre
Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi,
AlexanderSpiridonov, JoshuaNewlan, vàAndreaGesmundo. Scalingupmodelsanddatawith t5x
andseqio.arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/abs/2203.17189.

Anna Rogers, Olga Kovaleva, và Anna Rumshisky. A primer in bertology: What we know about
how bert works, 2020.

Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, và Jason Weston. Hash layers for large
sparse models, 2021.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale, 2019.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen
Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,
Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,
Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,
Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, và Alexander M. Rush. Multitask prompted
training enables zero-shot task generalization, 2022.

Noam Shazeer và Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
Trong International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.

Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
PeterHawkins, HyoukJoongLee, MingshengHong, CliffYoung, RyanSepassi, vàBlakeHechtman.
Mesh-tensorflow: Deep learning for supercomputers, 2018.

Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret
Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan
Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, và Denny Zhou. Mixture-of-experts
meets instruction tuning:a winning combination for large language models, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language
models, 2023.
18

--- TRANG 16 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. Attention is all you need, 2023.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, và Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language
understanding systems, 2020.

Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan
Awadallah, và Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model
tuning, 2022.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, và Quoc V. Le. Finetuned language models are zero-shot learners, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc
Le, và Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.

Qinyuan Ye, Juan Zha, và Xiang Ren. Eliciting and understanding cross-task skills with task-level
mixture-of-experts, 2022.

Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models, 2022.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Can a machine
really finish your sentence?, 2019.

Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng
Chen, Quoc Le, và James Laudon. Mixture-of-experts with expert choice routing, 2022.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và
William Fedus. St-moe: Designing stable and transferable sparse expert models, 2022.

Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, và
Jianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2022.
19

--- TRANG 17 ---
A Kết quả thí nghiệm đầy đủ
A.1 Đánh giá Zero-Shot cho bộ dữ liệu P3
Trong nghiên cứu của chúng tôi, chúng tôi đã tiến hành đánh giá toàn diện các biến thể của các
phương pháp đề xuất so sánh với các baseline đã thiết lập. Đánh giá này bao gồm các kích thước
khác nhau của mô hình T5, cụ thể là 770M, 3B, và 11B. Cả điểm trung bình và trung vị đều được
báo cáo cho mọi tập hợp đánh giá được dẫn xuất từ bộ dữ liệu P3, bao gồm một loạt các tác vụ.
Để biết thêm chi tiết và khám phá sâu hơn, vui lòng tham khảo URL sau: https://huggingface.co/datasets/bigscience/P3.

T5-Large (770M)
Mô hình % Tham số. Metric ANLI CB RTE WSC WIC Copa WNG HS Trung bình
Full-FT T0-770M (của chúng tôi)100% trung vị 35.6 71.43 75.63 57.21 51.41 77.0 53.04 26.78 56.01
trung bình 35.57 57.74 75.88 52.31 52.52 74.6 52.93 26.74 53.54
PEFT(IA)30.036% trung vị 33.5 42.86 67.87 62.02 52.35 67.0 51.22 26.33 50.39
trung bình 33.27 45.12 67.08 58.17 52.74 66.63 51.35 26.32 50.09
LoRA 0.497% trung vị 35.0 55.36 57.4 63.46 50.24 77.0 53.28 26.67 52.3
trung bình 35.26 51.67 59.35 62.98 50.66 76.5 52.41 27.24 52.0
Phương pháp của chúng tôiMOV-5 0.27% trung vị 33.6 41.07 71.48 61.54 50.86 76.5 51.46 26.02 51.57
trung bình 33.51 42.62 71.26 60.96 51.14 73.8 51.55 26.01 51.36
MoV-10 0.55% trung vị 33.9 42.86 74.19 62.5 50.31 77.0 52.64 26.34 52.47
trung bình 33.68 42.38 74.51 59.23 50.74 74.82 52.2 26.72 51.78
MoV-20 1.10% trung vị 33.7 41.07 73.83 63.46 50.94 75.46 51.14 25.48 51.89
trung bình 33.98 45.12 73.36 59.13 51.33 73.47 51.3 25.45 51.64
MoV-30 1.66% trung vị 33.75 41.07 72.92 55.77 51.25 77.0 51.46 26.55 51.22
trung bình 33.81 44.88 72.56 56.15 51.29 77.43 51.81 26.52 51.81
MoV-60 3.32% trung vị 34.0 53.57 75.81 57.69 50.55 77.96 53.12 26.33 53.63
trung bình 34.24 52.26 75.02 58.37 50.78 77.06 52.87 26.74 53.42
MoLoRA-10 5.60% trung vị 33.2 67.86 68.41 64.9 50.39 80.0 52.64 52.64 55.52
trung bình 33.37 56.31 68.88 63.37 51.55 79.35 52.31 52.31 53.99
Bảng 2: Đánh giá zero-shot của mô hình tham số 770M qua tất cả các tác vụ chưa thấy, so sánh
số lượng expert khác nhau cho cả MoV và MoLoRA.
20

--- TRANG 18 ---
T5-XL (3B)
Mô hình % Tham số. Metric ANLI CB RTE WSC WIC Copa WNG HS Trung bình
Full-FTT0-3B (Sanh et al., 2022)100% trung vị 33.46 50.0 64.08 64.42 50.39 74.92 50.51 27.51 51.91
trung bình 33.42 45.36 64.55 65.10 50.69 72.40 50.97 27.29 51.22
T0-3B (sao chép của chúng tôi)100% trung vị 41.08 80.36 76.17 53.37 53.92 88.94 57.46 29.19 60.06
trung bình 40.73 74.52 76.82 52.21 53.84 88.99 56.83 29.2 59.14
PEFT(IA)30.018% trung vị 34.08 50.0 66.43 56.25 55.41 79.08 52.09 29.91 52.90
trung bình 34.56 51.07 68.38 54.9 55.61 78.23 52.14 28.97 52.98
LoRA 0.3% trung vị 37.5 75.57 73.53 61.02 51.25 83.6 54.33 25.32 57.51
trung bình 37.85 66.9 77.04 56.73 52.29 82.83 55.64 26.79 57.01
Phương pháp của chúng tôiMoV-2 0.18% trung vị 34.7 46.43 66.06 56.25 54.86 85.42 53.75 29.25 53.34
trung bình 35.14 50.36 69.31 56.15 54.4 83.79 53.69 28.47 53.91
MoV-5 0.23% trung vị 37.1 76.79 78.16 57.69 52.27 86.77 53.99 29.31 59.01
trung bình 37.66 62.14 78.3 58.46 53.54 86.52 54.54 28.3 57.43
MoV-10 0.32% trung vị 38.92 75.0 78.88 62.5 52.19 85.77 55.96 30.24 59.93
trung bình 38.83 63.45 79.49 60.19 53.04 86.41 56.27 29.11 58.35
MoV-20 0.50% trung vị 39.2 75.0 76.71 57.69 53.45 89.0 55.64 30.89 59.7
trung bình 39.25 64.05 76.53 56.63 53.45 86.93 56.24 29.36 57.81
MoV-30 0.68% trung vị 38.7 78.57 80.87 63.46 51.1 87.25 56.27 28.63 60.61
trung bình 38.9 67.5 81.23 59.9 52.43 86.28 56.39 27.57 58.77
MoV-60 1.22% trung vị 38.83 76.79 74.55 60.1 52.66 89.79 55.49 30.47 59.83
trung bình 38.97 63.93 75.38 57.79 53.5 86.04 55.88 29.28 57.59
MoV-10 (top-1) 0.32% trung vị 33.9 75.0 71.12 61.06 50.71 70.0 51.7 25.89 54.92
trung bình 34.31 60.6 71.41 58.94 51.24 68.39 51.79 25.98 52.82
MoV-10 (top-2) 0.32% trung vị 38.7 82.14 75.63 48.08 53.68 79.88 54.14 27.37 57.45
trung bình 38.89 69.76 74.95 47.69 53.51 79.89 53.83 26.91 55.67
MoLORA-2 0.75% trung vị 39.2 82.14 80.32 62.5 50.39 80.58 57.38 28.47 60.12
trung bình 38.86 65.71 80.0 60.0 50.8 82.17 56.51 28.03 57.76
MoLORA-5 1.66% trung vị 36.75 71.43 79.96 56.25 55.17 85.81 55.8 27.63 58.6
trung bình 37.52 62.14 80.22 52.6 55.34 84.05 56.04 26.62 56.82
MoLORA-10 3.18% trung vị 38.5 78.57 78.16 63.46 50.86 86.5 55.41 26.72 59.77
trung bình 38.49 66.43 77.44 59.9 51.63 84.96 56.1 26.7 57.71
MoLORA-15 4.69% trung vị 40.0 80.36 80.51 62.98 50.86 89.0 55.33 27.3 60.79
trung bình 39.73 69.52 80.97 60.67 51.54 86.5 55.03 27.25 58.9
Bảng 3: Trong thiết lập thí nghiệm toàn diện nhất của chúng tôi, chúng tôi đã tiến hành đánh giá
zero-shot qua tất cả các tác vụ chưa thấy sử dụng mô hình tham số 3B. Chúng tôi đã so sánh số
lượng expert khác nhau cho cả MoV và MoLORA và thử nghiệm với chiến lược routing lựa chọn top-k

T5-XXL (11B)
Mô hình % Tham số. Metric ANLI CB RTE WSC WIC Copa WNG HS Trung bình
Full-FTT0-11B (Sanh et al., 2022) 100% trung vị 42.17 78.57 81.23 64.42 57.21 90.79 60.46 33.65 63.56
trung bình 41.16 70.12 80.83 61.45 56.58 90.02 59.94 33.58 61.70
T0-11B (sao chép của chúng tôi) 100% trung vị 47.1 80.36 81.41 60.1 56.27 96.08 67.32 31.61 65.03
trung bình 45.83 72.62 81.52 58.17 56.66 96.0 66.77 30.95 63.57
PEFT(IA)30.0098% trung vị 42.3 73.21 75.99 58.65 52.04 86.27 54.3 30.27 59.12
trung bình 42.1 63.27 75.31 55.49 52.27 85.74 55.06 30.09 57.41
Phương pháp của chúng tôiMoV-10 0.143% trung vị 45.83 76.79 78.52 53.85 51.88 94.23 63.77 33.5 62.3
trung bình 44.73 70.12 78.88 54.23 53.26 93.64 63.57 33.59 61.5
MoV-20 0.287% trung vị 44.58 76.79 73.83 55.77 52.98 95.0 62.27 32.92 61.77
trung bình 43.54 69.17 74.4 52.88 54.5 93.93 62.95 32.85 60.53
MoV-30 0.431% trung vị 43.6 76.79 77.62 56.73 53.84 93.62 64.25 31.34 62.22
trung bình 43.32 69.29 77.22 53.56 56.03 93.65 63.52 31.32 60.99
MoV-60 0.862% trung vị 45.17 75.0 83.03 60.1 53.68 95.42 65.82 34.38 64.08
trung bình 43.9 69.88 83.07 56.54 54.51 94.01 64.56 34.17 62.58
Bảng 4: Chúng tôi đã đánh giá kích thước mô hình lớn nhất có sẵn từ checkpoint T5 pretrained
gốc, T5-XXL với 11B tham số, để chứng minh hiệu quả của mixture of PEFT expert đề xuất
của chúng tôi ở quy mô này.
21

--- TRANG 19 ---
A.2 Token vs. Sentence Embeddings cho Routing
Chúng tôi trình bày kết quả trung bình và trung vị cho các chiến lược routing của chúng tôi. Cụ thể,
chúng tôi đã đánh giá hiệu suất bằng cách truyền token trực tiếp cho router hoặc bằng cách truyền
sentence embedding. Phát hiện của chúng tôi chỉ ra rằng, đặc biệt đối với mô hình T5-XL(3B),
token routing liên tục mang lại hiệu suất tốt hơn về cả giá trị trung bình và trung vị. Bộ dữ liệu
Anli bị loại trừ khỏi bộ dữ liệu embedding của chúng tôi.

MoV – Token vs. Sentence Embedding
Mô hình Metric CB RTE WSC WIC Copa WNG HS Trung bình
MoV-10 (Token) - 770M trung vị 42.86 74.19 62.5 52.64 52.64 77.0 26.34 55.12
trung bình 42.38 74.51 59.23 52.2 52.2 74.82 26.72 54.37
MoV-10 (Embedding) - 770M trung vị 48.21 67.15 62.98 51.8 50.99 67.0 26.38 53.5
trung bình 51.67 67.29 58.37 51.79 50.99 65.8 26.57 53.21
MoV-10 (Token) - 3B trung vị 75.0 78.8 62.5 52.19 55.96 85.77 30.24 62.94
trung bình 63.45 79.49 60.19 53.04 56.27 86.41 29.11 61.14
MoV-10 (Embedding) - 3B trung vị 57.14 67.15 61.06 55.33 52.49 82.5 29.08 57.82
trung bình 51.07 68.81 58.65 55.28 52.57 80.53 28.51 56.49
MoV-10 (Token) - 11B trung vị 76.79 78.52 53.85 51.88 63.77 94.23 33.5 64.65
trung bình 70.12 78.88 54.23 53.26 63.57 93.64 33.59 63.9
MoV-10 (Embedding) - 11B trung vị 75.0 78.7 57.69 54.0 57.85 92.0 33.08 64.05
trung bình 66.19 79.1 58.37 54.83 58.78 91.17 32.7 63.02
Bảng 5: Kết quả trên chứng minh hiệu quả của token routing so với việc áp đặt một inductive bias
mạnh, như sentence embedding qua các tham số mô hình khác nhau.
22
