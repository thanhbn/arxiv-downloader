Q-Probe: Một Phương Pháp Nhẹ để
Tối Đa Hóa Phần Thưởng cho Các Mô Hình Ngôn Ngữ

Kenneth Li1 2Samy Jelassi3Hugh Zhang1 2Sham Kakade1 2Martin Wattenberg1David Brandfonbrener2

Tóm tắt
Chúng tôi trình bày một phương pháp gọi là Q-probing để điều chỉnh một mô hình ngôn ngữ đã được tiền huấn luyện nhằm tối đa hóa một hàm phần thưởng cụ thể cho tác vụ. Ở mức độ cao, Q-probing nằm giữa các phương pháp nặng hơn như tinh chỉnh và các phương pháp nhẹ hơn như prompting few shot, nhưng cũng có thể được kết hợp với cả hai. Ý tưởng là học một hàm tuyến tính đơn giản trên không gian embedding của mô hình có thể được sử dụng để cân nhắc lại các hoàn thành ứng viên. Chúng tôi chứng minh về mặt lý thuyết rằng quy trình lấy mẫu này tương đương với việc tối đa hóa Q-probe có ràng buộc KL khi số lượng mẫu tăng lên. Để huấn luyện các Q-probe, chúng tôi xem xét việc mô hình hóa phần thưởng hoặc một lớp các mục tiêu học chính sách trực tiếp mới dựa trên gradient chính sách có trọng số quan trọng. Với kỹ thuật này, chúng tôi thấy có cải thiện trong các lĩnh vực có phần thưởng thực tế (sinh mã) cũng như các phần thưởng ngầm được định nghĩa bởi dữ liệu ưa thích, thậm chí vượt trội hơn tinh chỉnh trong các chế độ dữ liệu hạn chế. Hơn nữa, một Q-probe có thể được huấn luyện trên một API vì nó chỉ giả định quyền truy cập vào việc lấy mẫu và embedding. Mã: https://github.com/likenneth/q_probe .

1. Giới thiệu
Tiền huấn luyện trên dữ liệu đa dạng trao cho các mô hình ngôn ngữ lớn (LLM) khả năng ngôn ngữ chung mạnh mẽ. Tuy nhiên, các tác vụ hạ nguồn hướng mục tiêu như lập trình, lý luận toán học và hệ thống đối thoại đòi hỏi phải điều chỉnh LLM cho tác vụ cụ thể. Vì các mục tiêu trong những tác vụ này có thể được đóng khung như phần thưởng, việc điều chỉnh này có thể có dạng tối đa hóa phần thưởng.

Một cách tiếp cận để thực hiện điều này là tinh chỉnh, trong đó các trọng số của mô hình được điều chỉnh để cải thiện phần thưởng. Các kỹ thuật tiêu biểu bao gồm học tăng cường từ phản hồi của con người (RLHF, Ouyang et al., 2022; Rafailov et al., 2023) và tinh chỉnh có giám sát trên các ví dụ thành công (Singh et al., 2023; Dong et al., 2023; Yuan et al., 2023).

Mặt khác, có bằng chứng cho thấy các khả năng cần thiết cho những tác vụ hạ nguồn này đã được học trong quá trình tiền huấn luyện, và nhiệm vụ của việc điều chỉnh chỉ đơn thuần là trích xuất chúng từ phổ rộng các khả năng đã được tiền huấn luyện. Ví dụ, Zaken et al. (2021) đề xuất rằng việc tinh chỉnh cực kỳ hiệu quả về tham số là bằng chứng cho thấy quá trình tinh chỉnh chủ yếu là về "việc phơi bày kiến thức được tạo ra bởi huấn luyện mô hình ngôn ngữ", trong khi Saunders et al. (2022) phát hiện rằng các mô hình ngôn ngữ đã được tiền huấn luyện thường tốt hơn trong việc phân biệt hơn là sinh ra câu trả lời.

Được thúc đẩy bởi dòng suy nghĩ này, chúng tôi trình bày một phương pháp nhẹ để tối đa hóa phần thưởng. Đối với mỗi tác vụ hạ nguồn, chúng tôi giữ toàn bộ mô hình đã được tiền huấn luyện bị đóng băng và chỉ huấn luyện một probe nhỏ có cùng chiều với dòng dư (Alain và Bengio, 2016). Chúng tôi gọi phương pháp của mình là Q-probe vì nó "thăm dò" tiện ích mong đợi của một hoàn thành (hành động) cho trước một lời nhắc nhất định (trạng thái).

Để tận dụng Q-probe tại thời điểm suy luận để sinh mẫu, chúng tôi thực hiện một dạng lấy mẫu từ chối. Cụ thể, trước tiên chúng tôi rút k hoàn thành được lấy mẫu từ LLM cho trước lời nhắc đầu vào và cũng lưu trữ embedding của mỗi cặp lời nhắc-hoàn thành. Sau đó Q-probe dự đoán một giá trị cho mỗi embedding, điều này xác định các logit cho phân phối softmax k-chiều mà chúng tôi sử dụng để lấy mẫu hoàn thành được chọn. Về mặt lý thuyết, chúng tôi chỉ ra rằng quy trình này tối đa hóa giá trị có ràng buộc KL của probe khi k tiến đến vô cùng.

Đầu tiên, chúng tôi đánh giá các Q-probe với quyền truy cập vào phần thưởng thực tế trên các benchmark lập trình - Q-probe tốt nhất của chúng tôi đạt độ chính xác cao hơn 17% trên MBPP (Austin et al., 2021) so với Code-LLaMA-7B cơ bản (Roziere et al., 2023) và vượt trội hơn tinh chỉnh trên các thành công với LORA (Hu et al., 2021) và prompting few shot. Mặc dù một lần nữa, chúng tôi nhấn mạnh rằng các Q-probe không loại trừ lẫn nhau với những kỹ thuật khác này và có thể được kết hợp để có kết quả thậm chí tốt hơn. Một thành phần chính của kết quả là một mục tiêu mới để huấn luyện các Q-probe thông qua học chính sách trực tiếp.

Chúng tôi thấy rằng thay vì huấn luyện Q-probe để mô hình hóa phần thưởng, việc sử dụng một mục tiêu gradient chính sách có trọng số quan trọng hiệu quả hơn. Vì chúng tôi chỉ cần quyền truy cập vào các mẫu và embedding, chúng tôi có thể huấn luyện Q-probe trên các mô hình dựa trên API nơi mà các cải thiện khiêm tốn hơn (cải thiện 3% so với mô hình cơ bản) do mô hình cơ bản mạnh hơn và thiếu quyền truy cập vào embedding mô hình nội bộ.

Tiếp theo, chúng tôi đánh giá Q-probe về việc học từ ưa thích của con người. Chúng tôi tiến hành một so sánh tiêu chuẩn hóa (Ethayarajh et al., 2022) và thấy rằng Q-probe vượt trội hơn PPO offline và DPO 6% về tỷ lệ thắng như được đánh giá bởi GPT4. Hơn nữa, chúng tôi chỉ ra rằng một Q-probe có thể được huấn luyện trên một mô hình đã được tinh chỉnh KTO và vượt trội hơn mỗi phương pháp riêng lẻ thêm 4%. Điều này chứng minh cách Q-probe có thể được kết hợp hiệu quả với các chiến lược điều chỉnh khác.

Cuối cùng, về mặt chi phí tính toán, chúng ta nên lưu ý rằng việc sử dụng Q-probe đòi hỏi ít hơn đáng kể về tính toán huấn luyện, nhưng nhiều tính toán thời gian suy luận hơn khi so sánh với tinh chỉnh. Trong các thí nghiệm của chúng tôi, chúng tôi có thể huấn luyện một Q-probe trong vài giây (vì nó chỉ là một mô hình tuyến tính 4096 chiều) trong khi ngay cả tinh chỉnh hiệu quả tham số (Hu et al., 2021) cũng mất vài giờ. Nhưng, tại thời điểm suy luận, chúng tôi rút k mẫu từ mô hình cơ bản thay vì 1 từ mô hình đã được tinh chỉnh, mặc dù các cải thiện trong giải mã song song và suy đoán đang làm cho việc giải mã theo lô dễ dàng hơn (Fang et al., 2021; Yu et al., 2022; Shen et al., 2024).

2. Công trình liên quan
Probing. Q-probe tận dụng ý tưởng probing để giải quyết các vấn đề tối đa hóa phần thưởng. Ý tưởng này xây dựng trên công trình trước đây sử dụng probe để hiểu nội bộ của mạng neural (Alain và Bengio, 2016; Belinkov, 2016; Li et al., 2022). Một probe là một bộ phân loại hoặc hồi quy lấy các kích hoạt nội bộ của mạng làm đầu vào và được huấn luyện để dự đoán một đặc trung quan tâm, ví dụ: từ loại, độ sâu cây phân tích cú pháp, hoặc phần thưởng mong đợi trong trường hợp của chúng tôi.

Lấy mẫu từ chối. Lấy mẫu từ chối để tối đa hóa phần thưởng không phải là ý tưởng mới. Thực tế, Gao et al. (2023); Ganguli et al. (2022); Rafailov et al. (2023) cũng đánh giá lấy mẫu từ chối như một trong những đường cơ sở của họ. Tuy nhiên, mô hình lựa chọn của họ được khởi tạo bởi mô hình ngôn ngữ ưa thích được huấn luyện theo cách tương tự với giai đoạn đầu của RLHF bởi Ouyang et al. (2022). Phiên bản lấy mẫu từ chối này không chỉ liên quan đến chi phí huấn luyện cao hơn mà còn gấp đôi chi phí suy luận để chạy mô hình phần thưởng trong khi đánh giá Q-probe về cơ bản là miễn phí so với mô hình cơ bản.

Lấy mẫu từ chối + tinh chỉnh. Một dòng công trình khác tinh chỉnh hoặc chưng cất các mô hình trên dữ liệu được thu thập bởi lấy mẫu từ chối (Singh et al., 2023; Dong et al., 2023; Yuan et al., 2023; Rafailov et al., 2023). Trong công trình này, chúng tôi chỉ tập trung vào một cách nhẹ để thực hiện lấy mẫu từ chối, nhưng thêm một bước chưng cất nào đó lên trên để giảm chi phí suy luận có thể là một hướng tương lai thú vị.

Tinh chỉnh lặp. Trong khi chúng tôi tập trung các thí nghiệm chủ yếu vào các thiết lập ngoại tuyến để đơn giản, cũng có một hướng thú vị là tinh chỉnh lặp để tối đa hóa phần thưởng. (Anthony et al., 2017; Gulcehre et al., 2023; Singh et al., 2023; Zelikman et al., 2022; Dong et al., 2023). Ý tưởng Q-probe có thể được áp dụng bên trong các thuật toán lặp như thế này và đó là một hướng thú vị cho công việc tương lai.

Prompting. Một dòng quan trọng của các phương pháp điều chỉnh không cần huấn luyện tập trung xung quanh prompting (Salewski et al., 2023) bao gồm học trong ngữ cảnh (ICL, Min et al., 2022) và Chuỗi-suy-nghĩ (CoT, Wei et al., 2022). Mặc dù nó có tính linh hoạt tuyệt vời, Mosbach et al. (2023) tiết lộ bằng cách kiểm tra kỹ hơn rằng tinh chỉnh vẫn vượt trội hơn các phương pháp prompting. Prompting cũng có thể nhạy cảm với kỹ thuật prompt (Lu et al., 2021) và chiếm một cửa sổ ngữ cảnh có giá trị, hạn chế lượng dữ liệu chúng ta có thể đưa vào để so sánh công bằng với Q-probe và tinh chỉnh.

Prompting với quyền truy cập phần thưởng. Cũng có một loạt các kỹ thuật thời gian suy luận khác được thiết kế cho các thiết lập lập trình và lý luận (Zhou et al., 2023a; Shinn et al., 2023; Yao et al., 2023). Tuy nhiên, chúng đòi hỏi quyền truy cập vào phản hồi từ môi trường tại thời điểm kiểm tra điều này khác với thiết lập một lần đi qua được xem xét bởi chúng tôi.

3. Thiết lập
Chúng tôi xem xét một khung tổng quát kiểm tra các tác vụ ngôn ngữ hạ nguồn như vấn đề tối đa hóa phần thưởng. Trong thiết lập này, các chuỗi lời nhắc x được lấy mẫu i.i.d. từ một phân phối nào đó Pprompt. Sau đó, mô hình của chúng tôi sinh ra các chuỗi hoàn thành mà chúng tôi sẽ ký hiệu bằng a ("hành động" trong thuật ngữ học tăng cường). Mục tiêu là sinh ra các hoàn thành để tối đa hóa một hàm phần thưởng nào đó r(x, a).

Trong thiết lập này, chúng tôi sẽ xem xét nhiều loại phản hồi (phần thưởng oracle hoặc ưa thích) cũng như các mức tương tác (dữ liệu ngoại tuyến hoặc quyền truy cập phần thưởng trực tuyến) mà Q-probe có thể giải quyết. Chúng tôi cũng chỉ cần quyền truy cập hộp đen hạn chế vào mô hình cơ bản. Phần này chính thức hóa tất cả những giả định này về thiết lập.

3.1. Phản hồi: phần thưởng oracle và ưa thích
Phản hồi hàm phần thưởng Oracle. Trong thiết lập này, chúng tôi giả định quyền truy cập vào một tập huấn luyện các lời nhắc x trong Dtrain và quyền truy cập vào hàm phần thưởng "oracle" hoặc thực tế trên các lời nhắc huấn luyện r(x, a) cho x trong Dtrain và bất kỳ a nào. Ví dụ, trong các vấn đề lập trình điều này là giả định rằng chúng ta có các trường hợp kiểm tra cho các lời nhắc huấn luyện. Để đánh giá, chúng tôi giả định quyền truy cập vào một tập kiểm tra các lời nhắc x trong Dtest và cũng là hàm phần thưởng trên các lời nhắc kiểm tra.

Mục tiêu khi được cung cấp phản hồi phần thưởng oracle là học một chính sách pi để tối đa hóa lợi nhuận mong đợi:
J(pi) = E_x E_{a∼pi|x}[r(x, a)] (1)

Lưu ý, có một văn học lớn về công trình trước đây về việc sử dụng học tăng cường trực tiếp để tinh chỉnh các mô hình ngôn ngữ khi được cung cấp quyền truy cập vào các hàm phần thưởng oracle, ví dụ: cho các tác vụ ngôn ngữ một lượt (Schulman et al., 2017; Snell et al., 2022; Ramamurthy et al., 2022; Chang et al., 2023) hoặc trong các thiết lập nhiều lượt (Zhou et al., 2023b; Abdulhai et al., 2023). Ngược lại, chúng tôi tập trung vào một phương pháp trọng lượng nhẹ hơn chỉ đòi hỏi huấn luyện probe, nhưng chỉ ra cách huấn luyện probe có thể xấp xỉ các mục tiêu RL truyền thống.

Phản hồi ưa thích. Điều này giống như trên, ngoại trừ việc chúng ta có quyền truy cập vào các so sánh theo cặp. Cho một x trong Dtrain cho bất kỳ cặp hành động (a0, a1) nào chúng ta có thể nhận được một nhãn l trong {0,1} chỉ ra hành động nào được ưa thích (Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2023).

Mục tiêu khi được cung cấp phản hồi ưa thích là học một chính sách pi sinh ra các hành động để tối đa hóa hàm phần thưởng ẩn tạo ra các ưa thích (nếu chúng ta giả định ví dụ một mô hình Bradley-Terry của ưa thích (Bradley và Terry, 1952)).

3.2. Quyền truy cập trực tuyến vs ngoại tuyến vào phản hồi
Chúng tôi luôn giả định một tập dữ liệu cố định của các ngữ cảnh (tức là lời nhắc) xi trong Dtrain. Ví dụ, những cái này có thể là các vấn đề lập trình, câu hỏi toán học, hoặc truy vấn người dùng. Từ những lời nhắc này, chúng tôi xem xét hai mức quyền truy cập có thể vào nguồn phản hồi:

1. Trực tuyến. Với quyền truy cập trực tuyến, chúng ta có thể truy vấn phần thưởng hoặc ưa thích của bất kỳ hành động a hoặc cặp hành động (a0, a1) nào từ bất kỳ ngữ cảnh xi nào trong tập huấn luyện để nhận r(xi, a). Thiết lập này hợp lý nếu chúng ta có kiểm tra đơn vị cho lập trình hoặc một con người trong vòng lặp cho học ưa thích.

2. Ngoại tuyến. Trong thiết lập ngoại tuyến, chúng tôi giả định rằng tập dữ liệu cũng chứa các hành động hoặc bộ hành động và các nhãn phần thưởng hoặc ưa thích. Vậy dữ liệu có các bộ (xi, ai, ri) hoặc (xi, (a0)i, (a1)i, yi). Chúng ta chỉ có thể truy cập phần thưởng hoặc ưa thích thông qua những nhãn này và không thể thực hiện các truy vấn tùy ý.

Phương pháp của chúng tôi có thể hoạt động trong thiết lập ngoại tuyến nơi tập dữ liệu được lấy mẫu từ mô hình cơ bản hoặc trong thiết lập trực tuyến khi chỉ được cung cấp quyền truy cập lấy mẫu vào mô hình cơ bản. Trong suốt bài báo chúng tôi sẽ mặc định thiết lập ngoại tuyến để chúng ta có thể học từ các tập dữ liệu cố định.

Thiết lập của chúng tôi khác với các thiết lập trực tuyến khác trong đó người học có thể truy vấn r(x, a) tại bất kỳ x nào cũng như bất kỳ a nào. Cho mục đích của chúng tôi, chúng tôi giả định rằng mức độ truy cập này quá mạnh vì nó cho phép tìm kiếm chống lại hàm phần thưởng trên tập kiểm tra. Ví dụ về các phương pháp trong thiết lập này là Reflexion (Shinn et al., 2023) hoặc LATS (Zhou et al., 2023a). Đây là một thiết lập thú vị, nhưng nằm ngoài phạm vi của bài báo này và không thể so sánh trực tiếp với kết quả của chúng tôi.

3.3. Quyền truy cập vào LLM
Chúng tôi giả định quyền truy cập vào một mô hình ngôn ngữ đã được tiền huấn luyện cung cấp cho chúng tôi hai thứ:

1. Lấy mẫu từ phân phối LM p0. Cho một ngữ cảnh x chúng ta có thể lấy mẫu một hoàn thành a từ p0(·|x).

2. Quyền truy cập vào embedding. Chúng ta có thể trích xuất một embedding φ(x, a) của chuỗi lời nhắc-hoàn thành kết hợp.

Nói chung chúng tôi không giả định quyền truy cập vào mô hình cơ bản để cho phép tinh chỉnh, và phương pháp của chúng tôi sẽ không đòi hỏi quyền truy cập như vậy, nhưng chúng tôi xem xét các phương pháp đó để so sánh. Chúng tôi cũng không giả định quyền truy cập vào mật độ hoặc logit từ mô hình cơ bản. Với những giả định này, phương pháp của chúng tôi có thể áp dụng trên các mô hình dựa trên API.

Chúng tôi không biết về công trình trước đây về các thuật toán học sử dụng mô hình truy cập này. Vì vậy, chúng tôi sẽ so sánh với một vài đường cơ sở có được nhiều quyền truy cập hơn (tinh chỉnh đầy đủ của các mô hình nguồn mở) hoặc ít quyền truy cập hơn (chỉ lấy mẫu với các lời nhắc khác nhau).

4. Suy luận sử dụng Q-probe
4.1. Định nghĩa chính sách Q-probe
Để định nghĩa chính sách Q-probe chúng tôi cân nhắc lại các mẫu từ mô hình cơ bản bằng cách sử dụng một hàm giá trị. Cho Qtheta: X × A → R, thì chính sách pitheta,k của chúng tôi được định nghĩa bởi quy trình sau:

1. Lấy mẫu ai∼p0|x, 1<=i<=k.
2. Lấy mẫu a∼softmax[Qtheta(x,a1)/beta, . . . ,Qtheta(x,ak)/beta].

Lưu ý rằng Qtheta không nhất thiết phải đại diện cho một hàm Q trong thuật ngữ của RL, và có thể là bất kỳ hàm có giá trị thực nào, đây chỉ là một cách để định nghĩa một chính sách.

4.2. Động lực lý thuyết cho chính sách Q-probe
Để thúc đẩy chính sách Q-probe, việc xem xét giới hạn khi chúng ta lấy k→ ∞ là hữu ích. Cụ thể, chúng tôi sẽ chỉ ra rằng trong giới hạn này, chính sách hội tụ đến chính sách tối ưu có ràng buộc KL tối đa hóa giá trị mong đợi của probe Qtheta.

Định lý 4.1. Chính sách của chúng tôi tiếp cận giới hạn sau
lim_{k→∞} pitheta,k(a|x) = p0(a|x)exp(Qtheta(x, a)/beta) / E_{b∼p0|x}[exp(Qtheta(x, b)/beta)].

Hệ quả 4.2. Chính sách giới hạn là chính sách điều hòa KL tối ưu hóa các giá trị Q:
lim_{k→∞} pitheta,k = arg max_pi E_{a∼pi|x}[Qtheta(x, a)] − beta·KL(pi∥p0)

Xem các chứng minh trong Phụ lục A.

Kết nối với lấy mẫu từ chối. Thuật toán lấy mẫu softmax của chúng tôi có sự tương tự rõ ràng với lấy mẫu từ chối tiêu chuẩn hơn. Để định nghĩa tương tự lấy mẫu từ chối, giả sử chúng ta biết một giá trị M sao cho M >= exp(Qtheta(x, a)/beta) cho tất cả a. Bây giờ thuật toán là:

1. Lấy mẫu a từ p0(·|x)
2. Chấp nhận a với xác suất exp(Qtheta(x,a)/beta)/M, nếu không thì quay lại bước 1.

Thời gian chạy để có một mẫu được chấp nhận là M lần lặp kỳ vọng. Chúng ta có thể xem phiên bản softmax như một xấp xỉ của lấy mẫu từ chối với k thay cho M. Điều này cho chúng ta thời gian chạy nhất quán và song song hóa, nhưng có nghĩa là đối với k hữu hạn, chúng ta chỉ xấp xỉ lấy mẫu từ phân phối mục tiêu.

Điều này cũng làm rõ rằng để gửi beta→0 chúng ta cần gửi M→∞ (và ngầm k→∞).

5. Thuật toán huấn luyện cho Q-probe
Cho đến nay chúng tôi đã định nghĩa quy trình lấy mẫu từ chính sách Q-probe và chỉ ra rằng đây là một định nghĩa chính sách hợp lý. Bây giờ chúng tôi chuyển sang chứng minh sự đa dạng của các thuật toán học có thể được sử dụng để huấn luyện các Q-probe. Về cơ bản, chúng ta có thể cố gắng học các hàm phần thưởng/giá trị hoặc học chính sách trực tiếp. Hơn nữa, chúng ta có thể áp dụng ý tưởng này cho phản hồi phần thưởng hoặc phản hồi ưa thích.

5.1. Học từ phản hồi phần thưởng oracle
Học phần thưởng. Cách tiếp cận đơn giản nhất là chỉ đơn giản sử dụng lỗi bình phương trung bình để học một Qprobe xấp xỉ hàm phần thưởng oracle trực tiếp.

LQ(theta) = E_x E_{a∼p0|x}[(Qtheta(x, a) − r(x, a))^2] (2)

Qtheta đã học này sau đó tạo ra một chính sách pitheta,k. Lưu ý rằng trong các vấn đề chúng tôi xem xét, chỉ có một bước tương tác với môi trường nên hàm phần thưởng bằng hàm Q theo nghĩa RL, đây là lý do chúng tôi gọi nó là Q-probe.

Trong nhiều vấn đề chúng tôi xem xét, phần thưởng là 0 hoặc 1. Trong trường hợp này chúng ta cũng có thể ước lượng phần thưởng với mất mát phân loại như cross entropy (CE). Sau đó mất mát là:

LCE(theta) = E_x E_{a∼p0|x}[r(x, a) log sigma(Qtheta(x, a)) + (1−r(x, a)) log(1 − sigma(Qtheta(x, a)))] (3)

Qtheta đã học này cũng tạo ra một chính sách pitheta,k theo cách tương tự.

Học chính sách trực tiếp. Một lợi ích của Q-probe là chúng ta có thể suy ra một mất mát trực tiếp hơn cố gắng tối ưu hóa lợi nhuận mong đợi của chính sách. Để thuận tiện ký hiệu, định nghĩa f(a) = exp(Qtheta(x, a)/beta). Sau đó chúng ta có thể định nghĩa xác suất softmax là:

rhotheta(a,{ai}_{i=2}^k) = f(a) / (f(a) + sum_{i=2}^k f(ai)). (4)

Rhotheta này là xác suất lấy mẫu a có điều kiện trên k mẫu từ bước 1 của quy trình lấy mẫu là a, a2, . . . , ak. Điều tốt về rhotheta là nó xấp xỉ tỷ lệ mật độ giữa pitheta,k và p0. Điều này cho phép chúng ta định nghĩa mất mát gradient chính sách có trọng số quan trọng sau:

LPG(theta) = E_x E_{a∼p0|x} [−r(x, a) * pi^k_theta(a|x) / p0(a|x)] (5)

≈ E_x E_{a∼p0|x} E_{a2,...,ak∼p0|x} [−r(x, a) * rhotheta(a,{ai}_{i=1}^k)]

Trong đó bởi Định lý 4.1 chúng ta có rằng xấp xỉ này chính xác khi k → ∞.

Như tiêu chuẩn trong văn học gradient chính sách, chúng ta cũng có thể giới thiệu một đường cơ sở b(x) và thay thế −r(x, a) trong mất mát bằng −(r(x, a) − b(x)) (Greensmith et al., 2004; Schulman et al., 2015). Trong thực tế, chúng tôi sử dụng phần thưởng trung bình độc lập ngữ cảnh trong tập dữ liệu làm đường cơ sở của chúng tôi.

Nhận xét 5.1. Mất mát PG này kết thúc trông giống như một mất mát đối比, đã được sử dụng truyền thống cho học biểu diễn (Wu et al., 2018; Oord et al., 2018). Ở đây, mất mát đối tỷ phát sinh tự nhiên vì quy trình thời gian suy luận của việc chọn một mẫu từ nhiều đòi hỏi chúng ta so sánh và đối tỷ một tập mẫu. Bằng cách liên kết trực tiếp mất mát với quy trình suy luận chúng ta có thể buộc mô hình phân bổ lỗi của nó theo cách thực hiện tốt hơn khi chọn một mẫu bằng softmax.

5.2. Học từ phản hồi ưa thích
Học phần thưởng. Cách tiếp cận đơn giản nhất để sử dụng Q-probe để học từ ưa thích là sử dụng probe để học mô hình phần thưởng bằng mô hình Bradley-Terry. Mất mát mỗi mẫu là:

ℓ(x, aw, al, theta) = sigma(Qtheta(x, aw) − Qtheta(x, al)) (6)

Và hàm mất mát ưa thích Q đầy đủ trở thành:

LQP(theta) = E_x E_{aw,al∼p0}[−log ℓ(x, aw, al, theta)] (7)

Qtheta đã học này sau đó tạo ra một chính sách pitheta,k.

Nhận xét 5.2. Mục tiêu phần thưởng học ưa thích có một hương vị đối tỷ nào đó. Vì chúng ta ghép các mẫu tích cực và tiêu cực và khuyến khích cho chúng các giá trị khác nhau, mất mát này phù hợp tốt hơn với quy trình suy luận hạ nguồn của việc lấy mẫu nhiều hoàn thành và chọn một.

Cuối cùng, trong khi chúng tôi không thấy nó hữu ích trong thực tế, cũng có thể tham số hóa các mục tiêu học chính sách trực tiếp từ phản hồi ưa thích với Q-probe như trong DPO (Rafailov et al., 2023). Một suy luận đầy đủ có thể được tìm thấy trong Phụ lục B.

6. Thí nghiệm phần thưởng Oracle
Đối với thí nghiệm đầu tiên của chúng tôi, chúng tôi đánh giá khả năng của Q-probe trong việc tối đa hóa phần thưởng oracle thực tế. Cụ thể, chúng tôi tập trung vào tổng hợp chương trình như một tác vụ với phần thưởng oracle được cung cấp bởi việc đánh giá các trường hợp kiểm tra. Chúng tôi huấn luyện probe sử dụng tập huấn luyện từ MBPP (Austin et al., 2021) và kiểm tra trên tập kiểm tra MBPP cũng như đánh giá khái quát hóa đến HumanEval (Chen et al., 2021). Để xem liệu phương pháp có khái quát hóa đến khả năng toán học hay không, chúng tôi thực hiện thí nghiệm trên GSM-8K trong Phần 6.4.

Thay vì sử dụng LLM thô làm mô hình cơ bản, chúng tôi bắt đầu từ một mô hình đã được tinh chỉnh trên dữ liệu lập trình (Chen et al., 2021; Roziere et al., 2023; Li et al., 2023; Azerbayev et al., 2023). Tinh chỉnh có giám sát này tạo điều kiện cho Q-probing hiệu quả hơn cho phần thưởng cụ thể tác vụ. Cụ thể, chúng tôi trình bày hai bộ kết quả, đầu tiên xây dựng trên Code-LLaMA-7B (Roziere et al., 2023) và thứ hai xây dựng trên OpenAI API để chứng minh cách Q-probe có thể được áp dụng cho các mô hình API.

6.1. Thiết lập
Chúng tôi huấn luyện các mô hình trên tập huấn luyện MBPP bao gồm 464 lời nhắc lập trình với các trường hợp kiểm tra. Chúng tôi xem xét phần thưởng là 1 nếu tất cả kiểm tra được thông qua và 0 nếu không. Đối với mỗi lời nhắc huấn luyện, chúng ta có thể sinh ra nhiều hoàn thành tùy ý từ mô hình cơ bản để tự động gắn nhãn với những phần thưởng này. Chúng tôi lấy mẫu từ mô hình cơ bản với nhiệt độ 0.8 và top-p 0.95, theo (Roziere et al., 2023), trừ khi được nêu khác. Đối với các thí nghiệm trên Code-LLaMA-7B, chúng tôi lấy tầng ẩn thứ 26 của cùng mô hình cho embedding. Đối với thí nghiệm OpenAI API, chúng tôi thí nghiệm với cả gọi API embedding cũng như Code-LLaMA-70B. Trừ khi được nêu khác, Q-probe là một probe 1 tầng (tuyến tính), bộ tối ưu là Adam (Kingma và Ba, 2014), tốc độ học là 5e−5, kích thước lô là 1000, và chúng tôi huấn luyện trong 150 epoch. Đối với mất mát PG, chúng ta cần nhiều mẫu từ một lời nhắc để tính mất mát. Để làm điều này, chúng tôi nhóm các mẫu theo lời nhắc và định hình lại lô để nó chứa 100 vấn đề với 10 mẫu từ mỗi vấn đề.

Chúng tôi đánh giá các mô hình trên tập kiểm tra MBPP gồm 500 lời nhắc lập trình với các trường hợp kiểm tra và cũng kiểm tra khái quát hóa đến tập dữ liệu HumanEval có 164 lời nhắc với các trường hợp kiểm tra. Tập dữ liệu HumanEval có định dạng hơi khác nhau, nhưng chứa các vấn đề có mức độ khó tương tự để kiểm tra khả năng khái quát hóa của các probe.

Chúng tôi xem xét nhiều đường cơ sở khác nhau. Đầu tiên, chúng tôi báo cáo tỷ lệ thành công trung bình của mô hình cơ bản với lấy mẫu nhiệt độ mặc định (BASELINE (PASS@1)). Chúng tôi cũng báo cáo lấy mẫu tham lam từ mô hình cơ bản (BASE-GREEDY). Chúng tôi bao gồm một đường cơ sở few-shot nơi chúng tôi lấy mẫu 5 hoàn thành thành công từ tập dữ liệu huấn luyện và đặt chúng vào ngữ cảnh và sau đó lấy mẫu với nhiệt độ 0 (5-SHOT ON SUCCESSES). Chúng tôi cũng bao gồm một skyline của pass@48 có quyền truy cập oracle vào phần thưởng thực tế.

Đối với mô hình Code-LLaMA, chúng tôi có quyền truy cập hộp trắng vào mô hình nên chúng tôi cũng thêm các đường cơ sở sử dụng tinh chỉnh LORA (Hu et al., 2021). Chúng tôi xem xét tinh chỉnh có giám sát trên các hoàn thành thành công từ dữ liệu huấn luyện theo sau bởi giải mã tham lam (SFT ON SUCCESSES) (Singh et al., 2023; Dong et al., 2023). Chúng tôi cũng xem xét hai loại thay thế lấy mẫu từ chối: một sử dụng hướng dẫn để nhắc mô hình đánh giá sinh ra của chính nó (PROMPT RM) và cái khác sử dụng mô hình phần thưởng tinh chỉnh LORA thay vì một probe nhẹ (FINETUNE RM). Đối với cái sau, chúng tôi thêm probe tuyến tính vào mô hình chính sách cơ bản tại dòng dư cuối cùng; nhưng khác với Q-probe, toàn bộ mô hình được điều chỉnh để đánh giá phần thưởng với Lora (Hu et al., 2021). Tại thời gian suy luận, cả hai đường cơ sở lấy mẫu từ chối đều áp dụng hardmax trên 48 thế hệ.

6.2. Kết quả Code-LLaMA
Chúng tôi trình bày kết quả cho việc huấn luyện Q-probe trên Code-LLaMA-7B trong Bảng 1. Phát hiện chính là Q-probe với mất mát gradient chính sách LPG là mô hình tốt nhất. Điều này xác nhận ý tưởng rằng việc tìm một mất mát là proxy trực tiếp hơn cho tác vụ hạ nguồn dẫn đến kết quả tốt hơn. Mất mát gradient chính sách đối tỷ nhiều mẫu cho cùng lời nhắc, điều này phản ánh quy trình suy luận và dẫn đến hiệu suất tốt hơn tại thời gian kiểm tra.

Ngoài ra, hãy nhớ rằng Q-probe dễ dàng được kết hợp với các phương pháp khác. Để minh họa điều này, chúng tôi kết hợp prompting few shot với Q-probe. Điều này dẫn đến hiệu suất thậm chí tốt hơn, cho thấy cách các quy trình suy luận khác nhau thực sự dẫn đến các cải thiện hiệu suất bổ sung mà cả hai cách tiếp cận đều không đạt được một mình.

Ở mức độ cao hơn, cũng quan trọng để lưu ý lợi ích của việc huấn luyện các probe nhỏ và nhẹ như vậy. Bởi vì probe rất nhỏ, chúng ta có thể trích xuất một bộ phân biệt hữu ích từ mô hình sinh với chỉ một lượng nhỏ huấn luyện và sử dụng probe này để cải thiện hiệu suất.

Hình 2 cho thấy cách các Q-probe mở rộng khi chúng ta thay đổi số lượng lời nhắc trong tập dữ liệu huấn luyện. Trong thí nghiệm này chúng tôi lấy 10 mẫu ngẫu nhiên khác nhau của n lời nhắc và huấn luyện Q-probe trên tập dữ liệu hoàn thành của những lời nhắc này từ mô hình cơ bản. Chúng tôi thấy rằng mất mát PG nhất quán đánh bại các mất mát Q và CE và hiệu quả dữ liệu có thể khá tốt, đạt phần thưởng kiểm tra 0.4 từ chỉ 50 lời nhắc.

Hình 3 cho thấy cách các Q-probe mở rộng khi chúng ta thay đổi k, số lượng mẫu được rút tại thời gian suy luận. Chúng ta thấy rằng mô hình được huấn luyện với mất mát PG thấy cải thiện nhất quán với k, mặc dù nó bắt đầu bão hòa. Ngược lại, LQ và LCE thực sự thấy hiệu suất hơi giảm khi chúng ta tăng k. Điều này một lần nữa khẳng định cách khớp mất mát huấn luyện với quy trình suy luận là có lợi.

Hình 4 cố gắng cung cấp một số trực giác về cách LPG khác với LQ và LCE theo cách có lợi. Đầu tiên và quan trọng nhất LPG cố gắng tối ưu hóa một proxy của metric kiểm tra, phần thưởng mong đợi. Thí nghiệm này cố gắng nhìn ở mức thấp hơn để xem điều này thay đổi các mô hình đã học như thế nào. Trực giác, LQ và LCE xử lý các mẫu a từ cùng x độc lập (vì chúng chỉ tính tổng trên tất cả mẫu) và kết thúc phân bổ một lượng khả năng tốt để phân loại lời nhắc nào khó (gây ra độ dốc cao hơn trong hình). Nhưng mất mát LPG buộc mô hình học hoàn thành nào tốt khi so sánh với nhau cho cùng lời nhắc. Bản chất đối tỷ của mất mát này giúp mô hình phân bổ khả năng hiệu quả hơn cho phần của vấn đề quan trọng: so sánh các hoàn thành khác nhau của cùng lời nhắc.

6.3. Kết quả OpenAI API
Cuối cùng, chúng tôi tiến hành thí nghiệm tương tự trên các thế hệ của OpenAI API. Kết quả được báo cáo trong Bảng 2. Chúng tôi sử dụng embedding từ CodeLlama-70b-Python vì embedding không có sẵn từ mô hình sinh API. Chúng tôi thấy cải thiện so với các đường cơ sở trên cả hai tập dữ liệu.

Mặc dù đây là một bằng chứng khái niệm tốt rằng Q-probe có thể được áp dụng trên các mô hình dựa trên API, kết quả không mạnh như chúng đối với Code-LLaMA. Chúng tôi giả thuyết rằng điều này chủ yếu vì hai lý do: (1) mô hình cơ bản mạnh hơn nhiều trên tác vụ và có khả năng đã được tinh chỉnh để thực hiện đặc biệt tốt tại những tác vụ lập trình này nên chỉ đơn giản là có ít chỗ cho việc cân nhắc lại để giúp đỡ, và (2) chúng tôi không có quyền truy cập vào embedding từ chính mô hình và các embedding nguồn mở từ Code-LLaMA có khả năng kém hiệu suất hơn.

Chúng tôi cũng thí nghiệm với embedding từ OpenAI API, và thấy chúng hoạt động kém hiệu quả hơn embedding Code-LlaMa. Kết quả đầy đủ và thảo luận về những thí nghiệm này có trong Phụ lục C.

6.4. Thí nghiệm bổ sung trên GSM-8K
Chúng tôi cũng tiến hành thí nghiệm trên GSM-8K với Code-Llama-7B, k = 48 và beta = 0.1, theo việc thực hiện của (Gao et al., 2022; Cobbe et al., 2021), sử dụng đánh giá 8-shot với mã được áp dụng từ dự án Code Generation LM Evaluation Harness (Ben Allal et al., 2022). Kết quả trong Bảng 3 cho thấy xu hướng tương tự như các thí nghiệm về lập trình.

7. Thí nghiệm phản hồi ưa thích
Chúng tôi cũng thí nghiệm với Q-probe về việc học từ dữ liệu ưa thích của con người. Chúng tôi tuân theo thiết lập và thực hiện của Ethayarajh et al. (2023) một cách nghiêm ngặt trừ khi được chỉ định khác. Chúng tôi sử dụng sự kết hợp của ba tập dữ liệu ưa thích nguồn mở - Anthropic Helpfulness and Harmlessness (HH) (Ganguli et al., 2022), OpenAssistant (Köpf et al., 2023), và Stanford Human Preferences Dataset (SHP) (Ethayarajh et al., 2022). Thí nghiệm được thực hiện trên LLaMA-7B (Touvron et al., 2023).

7.1. Thiết lập
Đầu tiên chúng tôi trích xuất các đặc trưng cho huấn luyện probe. Kết hợp các tập huấn luyện của ba tập dữ liệu với nhau, chúng tôi có được một tập dữ liệu với 200,336 cặp huấn luyện, mỗi cặp chứa một hoàn thành thắng và một hoàn thành thua. Chúng tôi nối lời nhắc với cả hai hoàn thành và chạy một lượt truyền tới của mô hình để trích xuất embedding. Lưu ý rằng Q-probing của chúng tôi được áp dụng trên mô hình tinh chỉnh có giám sát, đây cũng là điểm bắt đầu cho các phương pháp so sánh (Ouyang et al., 2022; Rafailov et al., 2023; Ethayarajh et al., 2023). PPO ngoại tuyến, DPO, và KTO sử dụng các hàm mất mát khác nhau để tinh chỉnh trọng số mô hình từ mô hình tinh chỉnh có giám sát này.

Sau khi hoàn thành huấn luyện, chúng tôi lấy mẫu 48 mẫu cho mỗi lời nhắc trong tập kiểm tra và nhúng chúng với mô hình. Q-probe sau đó trả về điểm số cho mỗi hoàn thành. Ở đây chúng tôi sử dụng beta = 0 và chọn argmax của điểm số. Trong quá trình đánh giá, hoàn thành của mô hình được so sánh với hoàn thành thắng trong dữ liệu cho lời nhắc đó bởi GPT-4 làm thẩm phán để tính "tỷ lệ thắng".

Chi tiết thí nghiệm Chúng tôi thực hiện Q-probe với probe 1 tầng, được huấn luyện với tốc độ học 5e−5 với kích thước lô 1024 trong 150 epoch sử dụng 20% toàn bộ tập huấn luyện được sử dụng bởi các phương pháp khác, là 40,067 cặp thế hệ thắng và thua. Tất cả phương pháp sử dụng lấy mẫu nucleus (p = 0.95) và nhiệt độ 1.0 tại suy luận (Holtzman et al., 2019).

Hình 5 cho thấy cách tỷ lệ thắng trên các benchmark học ưa thích của con người mở rộng với phần trăm dữ liệu được sử dụng để huấn luyện ba loại probe khác nhau, từ 5% đến 100% với khoảng 5%. Có tổng cộng 200,336 cặp huấn luyện.

7.2. Kết quả thí nghiệm
Bảng 4 trình bày kết quả của chúng tôi về dữ liệu ưa thích của con người. Bắt đầu từ cùng mô hình tinh chỉnh có giám sát, Q-probe vượt trội hơn các phương pháp hiện có mạnh như PPO (ngoại tuyến) và DPO, trong khi thực hiện ngang bằng với KTO. Chúng tôi cũng thí nghiệm với việc hoán đổi mô hình cơ bản với mô hình tinh chỉnh KTO, và chỉ ra rằng Q-probe trên mô hình tinh chỉnh KTO vượt trội hơn KTO đơn lẻ hoặc Q-probing trên mô hình cơ bản. Điều này cho thấy cách thuật toán thời gian suy luận đề xuất của chúng tôi trực giao với các phương pháp tinh chỉnh hiện có và chúng có thể được áp dụng cùng nhau.

Trong Hình 5, chúng tôi thay đổi lượng tính toán thời gian suy luận bằng cách thay đổi k, số lượng mẫu chúng ta sinh ra. Cải thiện bắt đầu bình ổn khoảng k = 5 nhưng mở rộng thêm tiếp tục tăng tỷ lệ thắng từ từ.

Trong Hình 6, chúng tôi kiểm tra bao nhiêu dữ liệu cần thiết cho Q-probe để hoạt động tốt. Đối với probe tuyến tính 1 tầng, nhờ tính đơn giản của nó, chỉ 20% dữ liệu cần thiết để đạt hiệu suất bình ổn, làm cho Q-probe trở thành phương pháp ứng viên đáng xem xét khi dữ liệu ưa thích có sẵn nhỏ. Chúng tôi cũng thí nghiệm với các kiến trúc probe mạnh hơn, ví dụ MLP 2 hoặc 3 tầng, khám phá điều này thực sự gây hại hiệu suất bằng overfitting (lưu ý rằng các tập dữ liệu lớn hơn cũng dẫn đến nhiều huấn luyện hơn vì chúng tôi cố định số epoch và kích thước lô). Trong một cách giải thích, Q-probe khám phá một hướng ưa thích tuyến tính trong không gian ẩn của LLM, có thể liên quan đến sự hình thành các cấu trúc tuyến tính trong các mạng neural khác nhau (Radford et al., 2017; Voynov và Babenko, 2020; Rogers et al., 2021).

8. Thảo luận
Chúng tôi đã đề xuất Q-probe, một phương pháp nhẹ để tối đa hóa phần thưởng trên các tác vụ hạ nguồn cho trước một mô hình ngôn ngữ đã được tiền huấn luyện. Q-probe có thể được sử dụng hiệu quả như một bổ sung hoặc thay thế cho các kỹ thuật khác như tinh chỉnh hoặc prompting. Trên hai thiết lập với quyền truy cập vào phần thưởng oracle và các cặp ưa thích của con người tương ứng, Q-probe vượt trội hơn các đường cơ sở mạnh. Đối với bất kỳ ai không có tài nguyên hoặc quyền truy cập để tinh chỉnh các mô hình ngôn ngữ lớn nhưng muốn điều chỉnh chúng cho các tác vụ hạ nguồn của riêng họ, Q-probe có thể phục vụ như một sự thay thế vững chắc, và thậm chí cho trước một mô hình đã được tinh chỉnh, Q-probe có thể được thêm lên trên để tận dụng nhiều tính toán thời gian suy luận hơn để vắt ra hiệu suất tốt hơn.

Một hướng thú vị cho công việc tương lai là nghiên cứu sâu hơn loại probe nào được học bởi Q-probe trên các tác vụ khác nhau. Các probe có thể tương tự qua các tác vụ? Cũng có thể có các kết nối thú vị với "vector tác vụ" (Ilharco et al., 2022).

Cuối cùng, Q-probe được truyền cảm hứng bởi, và xác nhận, các phát hiện trước đây về khoảng cách thế hệ-phân biệt (GD) trong các mô hình ngôn ngữ lớn (Saunders et al., 2022). Công trình này về cơ bản chứng minh khả năng kỹ thuật của việc đóng khoảng cách GD bằng lấy mẫu từ chối - sử dụng khả năng phân biệt mạnh hơn để giúp khả năng thế hệ yếu hơn. Một hướng thú vị cho công việc tương lai là điều tra liệu tinh chỉnh với chính sách cải thiện có thể, lần lượt, nâng cao khả năng phân biệt, và nếu vậy, vòng tự cải thiện này có thể kéo dài bao lâu.

Tuyên bố tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hậu quả xã hội tiềm tàng của công trình chúng tôi, không có gì chúng tôi cảm thấy phải được làm nổi bật cụ thể ở đây. Một lợi ích của phương pháp nhẹ được đề xuất là nó giảm lượng khí thải carbon.

Lời cảm ơn
Kenneth Li và Hugh Zhang được hỗ trợ bởi một học bổng từ Viện Kempner để Nghiên cứu Trí tuệ Tự nhiên và Nhân tạo tại Đại học Harvard. Tài nguyên tính toán Viện Kempner đã kích hoạt công trình này. Hugh được hỗ trợ thêm bởi Học bổng Nghiên cứu Sinh viên Tốt nghiệp từ Quỹ Khoa học Quốc gia. Samy Jelassi thừa nhận tài trợ được hỗ trợ bởi Trung tâm Khoa học Toán học và Ứng dụng. Công trình này đã được thực hiện một phần bởi một món quà từ Quỹ Chan Zuckerberg Initiative để thành lập Viện Kempner để Nghiên cứu Trí tuệ Tự nhiên và Nhân tạo. Sham Kakade thừa nhận tài trợ từ Văn phòng Nghiên cứu Hải quân dưới giải thưởng N00014-22-1-2377.
