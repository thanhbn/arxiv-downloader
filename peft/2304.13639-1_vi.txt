# 2304.13639.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2304.13639.pdf
# Kích thước tệp: 632638 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1
PVP: Điều chỉnh Tham số Hiệu quả
có Tiền huấn luyện Thị giác
Zhao Song, Ke Yang*, Naiyang Guan*, Junjie Zhu, Peng Qiao, và Qingyong Hu
Tóm tắt —Các transformer tiền huấn luyện quy mô lớn đã thể hiện thành công đáng kể trong nhiều nhiệm vụ thị giác máy tính. Tuy nhiên, việc tinh chỉnh đầy đủ các mô hình này cho các nhiệm vụ hạ nguồn vẫn là thách thức lớn do chi phí tính toán và lưu trữ cao. Gần đây, các kỹ thuật Điều chỉnh Tham số Hiệu quả (PETuning), ví dụ như Visual Prompt Tuning (VPT) [1] và Low-Rank Adaptation (LoRA) [2], đã giảm đáng kể chi phí tính toán và lưu trữ bằng cách chèn các mô-đun prompt nhẹ vào các mô hình tiền huấn luyện và điều chỉnh các mô-đun prompt này với số lượng nhỏ tham số có thể huấn luyện, trong khi giữ nguyên backbone transformer đông lạnh. mặc dù chỉ cần điều chỉnh một vài tham số, hầu hết các phương pháp PETuning vẫn yêu cầu lượng dữ liệu huấn luyện nhiệm vụ hạ nguồn đáng kể để đạt kết quả tốt. Hiệu suất không đủ trong các chế độ dữ liệu thấp, đặc biệt khi chỉ có một hoặc hai ví dụ mỗi lớp. Để giải quyết vấn đề này, trước tiên chúng tôi xác định thực nghiệm rằng hiệu suất kém chủ yếu do cách khởi tạo các mô-đun prompt không phù hợp, điều này cũng đã được xác minh trong các mô hình ngôn ngữ tiền huấn luyện. Tiếp theo, chúng tôi đề xuất khung Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP), tiền huấn luyện các mô-đun điều chỉnh tham số hiệu quả trước và sau đó tận dụng các mô-đun tiền huấn luyện cùng với backbone transformer tiền huấn luyện để thực hiện điều chỉnh tham số hiệu quả trên các nhiệm vụ hạ nguồn. Kết quả thí nghiệm trên năm bộ dữ liệu Phân loại Thị giác Tinh vi (FGVC) và VTAB-1k cho thấy phương pháp đề xuất của chúng tôi vượt trội đáng kể so với các phương pháp PETuning tiên tiến. Như được nhấn mạnh dưới đây, chúng tôi cho thấy khung PVP của chúng tôi đạt được cải thiện độ chính xác trung bình 16.08%, 11.52%, 6.36%, 2.94%, và 1.95% dưới thiết lập 1, 2, 4, 8, và 16 shot trên FGVC, tương ứng, so với các kỹ thuật PETuning trước đây, ví dụ như VPT, trong nhiệm vụ phân loại ảnh few-shot. PVP cũng đạt kết quả tiên tiến trên benchmark VTAB-1k, vượt qua độ chính xác trung bình của các phương pháp PETuning rất gần đây 2.33%.
Thuật ngữ chỉ mục —Điều chỉnh Tham số Hiệu quả, Prompt Tuning, Vision Transformer, Few-shot Learning, Transfer Learning.
F
1 GIỚI THIỆU
TRONG vài năm qua, các mô hình vision transformer bao gồm
ViT [3] và Swin [4], đã đạt được kết quả khuyến khích
trên một số nhiệm vụ thị giác chính thống. Tuy nhiên, huấn luyện
các mô hình transformer lớn như vậy thường đi kèm với
dữ liệu huấn luyện khổng lồ và chi phí tính toán đắt đỏ,
khiến việc huấn luyện các mô hình như vậy từ đầu trở nên rất thách thức đối với cá nhân. May mắn thay, các gã khổng lồ công nghệ trong ngành bao gồm Microsoft và Facebook, đã phát hành
các mô hình với các tham số được tiền huấn luyện cẩn thận trên dữ liệu tiền huấn luyện quy mô lớn [5], cho phép cá nhân sử dụng các mô hình transformer lớn bằng cách tinh chỉnh tất cả tham số mô hình
hoặc chỉ một tỷ lệ nhỏ tham số mô hình [6], [7],
[8], [9], [10], [11] trong khi giữ phần lớn đông lạnh.
Gần đây, một số công trình tiên phong được gọi là
các phương pháp Điều chỉnh Tham số Hiệu quả (PETuning) [1], [12],
[13], [14], [15], [16], đã cố gắng điều chỉnh một số mô-đun mới được chèn thay vì một phần của backbone transformer.
Ví dụ, Visual Prompt Tuning (VPT) [1] là một phương pháp PETuning thêm các tham số có thể học được dành riêng cho nhiệm vụ,
cụ thể là các token prompt, vào không gian đầu vào và chỉ tinh chỉnh các token prompt trên các nhiệm vụ hạ nguồn. Đáng chú ý,
các token prompt chỉ chiếm dưới 2% tổng số tham số. Trực quan, việc điều chỉnh một lượng tham số nhỏ như vậy
tự nhiên phù hợp với sơ đồ few-shot
Z. Song, K. Yang, N. Guan, và J. Zhu làm việc tại Viện Đổi mới Quốc phòng, Bắc Kinh, Trung Quốc.
P. Qiao làm việc tại Đại học Công nghệ Quốc phòng, Trường Sa,
Trung Quốc.
* biểu thị tác giả liên hệ.
CIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Patch Camelyon
EuroSAT
Resisc45 RetinopathyClevr/countClevr/distanceDMLabKITTI/distancedSprites/locationdSprites/orientationSmallNORB/azimuthSmallNORB/elevationVPT-Deep NOAH SSF FacT PVP (của chúng tôi)Hình 1. PVP được đề xuất của chúng tôi thể hiện hiệu suất mạnh mẽ so với các phương pháp tiên tiến gần đây trên benchmark VTAB-1k. Tên các bộ dữ liệu được mã hóa màu để chỉ rõ phương pháp có hiệu suất tốt nhất cho từng bộ dữ liệu.
learning, nơi chỉ có một vài mẫu dữ liệu được cung cấp cho
huấn luyện. Tuy nhiên, chúng tôi thấy rằng hiệu suất kém
được đạt bởi VPT khi hạn chế dữ liệu điều chỉnh (như thể hiện
trong Mục 3.3). Cụ thể, độ chính xác trên bộ dữ liệu CUB-200-2011
giảm xuống 30.05% khi sử dụng 1% dữ liệu điều chỉnh, so với
88.50% độ chính xác khi sử dụng tất cả dữ liệu điều chỉnh. Được thúc đẩy bởi điều này, chúng tôi
nhằm khám phá các vấn đề cơ bản tại sao các phương pháp PETuning
không hoạt động tốt trên các nhiệm vụ phân loại few-shot

--- TRANG 2 ---
2
các nhiệm vụ.
Chúng tôi cho rằng hiện tượng này do việc khởi tạo không đầy đủ các mô-đun prompt, vì các phương pháp PETuning hiện tại, ví dụ VPT [1], Adapter [12], và LoRA [2], thường sử dụng các mô-đun được khởi tạo bằng không hoặc ngẫu nhiên cho PETuning, có nghĩa là các mô-đun mới được thêm vào cần phải học từ đầu trên các nhiệm vụ hạ nguồn. Hơn nữa, hầu hết các phương pháp PETuning
yêu cầu việc chèn các mô-đun prompt có thể huấn luyện ở các lớp sớm hơn, đặc biệt là ở đầu mạng,
dẫn đến trọng số của tất cả các lớp sau đó bị loại bỏ.
Hai vấn đề này dẫn đến mô-đun prompt yêu cầu
lượng dữ liệu đáng kể để huấn luyện, điều này có thể chứng minh
là thách thức cho các nhiệm vụ hạ nguồn. Tuy nhiên, các bộ dữ liệu tiền huấn luyện, chẳng hạn như ImageNet [5], cung cấp dữ liệu dồi dào để đáp ứng
nhu cầu huấn luyện cần thiết.
Để giải quyết vấn đề này, chúng tôi đề xuất khung Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP). Chúng tôi đầu tiên tiền huấn luyện các mô-đun mới được thêm vào của PETuning trên một bộ dữ liệu lớn và
sau đó tận dụng các mô-đun tiền huấn luyện này để thực hiện
PETuning trên các nhiệm vụ học few-shot hạ nguồn. Lý luận đằng sau cách tiếp cận của chúng tôi là các tham số tiền huấn luyện cung cấp một nền tảng tuyệt vời cho PETuning, chỉ cần
một vài cập nhật gradient để tinh chỉnh các mô-đun. Các mô-đun được điều chỉnh sau đó có thể được áp dụng cho các nhiệm vụ như phân loại ảnh few-shot. Quan trọng là, chúng tôi lưu ý rằng các mô-đun điều chỉnh mới được thêm vào và backbone vision transformer được tiền huấn luyện trên cùng một bộ dữ liệu, do đó không có dữ liệu tiền huấn luyện bổ sung nào được tham gia.
Ngoài hiệu quả trong các tình huống few-shot,
cách tiếp cận Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP)
được đề xuất của chúng tôi cũng có thể áp dụng khi có đủ dữ liệu điều chỉnh. Kết quả thí nghiệm của chúng tôi chỉ ra rằng giai đoạn tiền huấn luyện mô-đun
cải thiện đáng kể khả năng thích ứng của backbone transformer với các nhiệm vụ hạ nguồn,
vượt trội hơn các phương pháp PETuning hiện tại. Do đó, cách tiếp cận của chúng tôi
đại diện cho một phương pháp Hiệu quả Tham số đầy hứa hẹn
cho các mô hình transformer tiền huấn luyện quy mô lớn.
Khung PVP được đề xuất có thể được áp dụng dễ dàng cho
các phương pháp PETuning khác nhau, với điều kiện là các phương pháp này
tích hợp các mô-đun có thể điều chỉnh vào backbone vision transformer và điều chỉnh các mô-đun mới được thêm vào trong khi giữ backbone transformer đông lạnh trong quá trình điều chỉnh nhiệm vụ hạ nguồn. Cụ thể, khung của chúng tôi có thể được áp dụng cho các phương pháp
như VPT [1], Adapter [12], và LoRA [2]. Kết quả thí nghiệm của chúng tôi
chứng minh rằng các phương pháp này trải qua
cải thiện hiệu suất đáng kể khi được tăng cường
với PVP của chúng tôi. Đóng góp của chúng tôi có thể được tóm tắt như
sau.
Theo hiểu biết tốt nhất của chúng tôi, đây là nghiên cứu đầu tiên
làm rõ các hạn chế của các kỹ thuật Điều chỉnh Hiệu quả Tham số
(PETuning) trên các nhiệm vụ few-shot và giải quyết
vấn đề này bằng cách tiền huấn luyện các mô-đun PETuning mới được thêm vào.
Chúng tôi đề xuất một khung Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP) đơn giản nhưng hiệu quả, đạt được
cải thiện hiệu suất đáng kể trên các nhiệm vụ few-shot hạ nguồn, đặc biệt là trong các chế độ dữ liệu cực thấp với chỉ 1 hoặc 2 mẫu huấn luyện mỗi
lớp. Cách tiếp cận của chúng tôi có thể được áp dụng dễ dàng cho các phương pháp PETuning khác nhau và đạt được sự cải thiện hiệu suất tuyệt vời.
Ngoài tình huống few-shot, cách tiếp cận PVP của chúng tôi
cũng đạt kết quả tiên tiến trên Benchmark Thích ứng Nhiệm vụ Thị giác (VTAB-1k), vượt trội hơn các phương pháp PETuning gần đây với biên độ lớn.
2 CÁC CÔNG TRÌNH LIÊN QUAN
2.1 Transformers Trong Thị giác
Transformer là một loại mạng neural sâu chủ yếu
dựa trên cơ chế self-attention, đã được nghiên cứu rộng rãi do hiệu suất vượt trội của nó. Vaswani
et al. [17] đề xuất kiến trúc transformer với cơ chế self-attention để nắm bắt mối quan hệ ngữ cảnh giữa các đầu vào, và đạt được thành công lớn trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) [18], [19], [20], [21]. Thành công đáng kể của các mô hình transformer quy mô lớn trong NLP
đã khơi dậy sự quan tâm ngày càng tăng trong việc áp dụng các mô hình này
trong Thị giác Máy tính (CV). Dosovitskiy et al. [3] giới thiệu
kiến trúc transformer vào lĩnh vực thị giác máy tính
và đề xuất Vision Transformer (ViT). Điều này được thực hiện
bằng cách chia một hình ảnh thành các patch và sau đó nhúng
các patch này thành token cho bộ mã hóa transformer. Liu
et al. [4] đề xuất swin transformer và tính toán self-attention trong cửa sổ cục bộ phân cấp trong khi cho phép
tương tác cross-window, cung cấp trường tiếp nhận đa tỷ lệ
cho transformer. Sau đó, nhiều
visual transformers [22], [23], [24] được đề xuất để
tận dụng knowledge distillation, convolutional embedding,
và depth-wise convolution để cải thiện hiệu suất.
Mặc dù các phương pháp dựa trên vision transformer đã đạt được
hiệu suất tiên tiến trong nhiều benchmark thị giác,
việc tinh chỉnh các mô hình transformer tiền huấn luyện trên các nhiệm vụ hạ nguồn
vẫn phụ thuộc vào dữ liệu và tốn kém về mặt tính toán,
điều này hạn chế ứng dụng rộng rãi hơn của các mô hình vision transformer. Với việc các mô hình tiền huấn luyện quy mô lớn được
công khai, cách thích ứng các transformer tiền huấn luyện
với các nhiệm vụ hạ nguồn [25], [26] một cách hiệu quả về tham số và bộ nhớ
vẫn là một vấn đề mở quan trọng.
2.2 Điều chỉnh Tham số Hiệu quả
Những năm qua đã chứng kiến thành công lớn của
transfer learning hiệu quả tham số trong NLP [27], [28], [29],

--- TRANG 3 ---
3
Giai đoạn 1 Giai đoạn 2Dữ liệu Tiền huấn luyện
......Dữ liệu Hạ nguồn
......
Khối Transformer Khối Transformer
khởi tạo
Mô-đun Transformer Gốc Mô-đun Điều chỉnh Tham số Hiệu quả
Hình 3. Tổng quan về Điều chỉnh Tham số Hiệu quả có Tiền huấn luyện. Có hai giai đoạn cho phương pháp điều chỉnh tham số hiệu quả có tiền huấn luyện của chúng tôi. (1) Giai đoạn tiền huấn luyện mô-đun Điều chỉnh Tham số Hiệu quả và (2) Giai đoạn Điều chỉnh Tham số Hiệu quả hạ nguồn. Các mô-đun transformer gốc được đông lạnh và các mô-đun điều chỉnh tham số hiệu quả có thể điều chỉnh trong cả hai giai đoạn. Các mô-đun điều chỉnh tham số hiệu quả được học trong giai đoạn 1 được sử dụng để khởi tạo các mô-đun trong giai đoạn 2. Các hàng đen và đỏ biểu thị lan truyền thuận và ngược tương ứng.
[30], [31], [32]. Gần đây, các phương pháp điều chỉnh tham số hiệu quả trên các mô hình vision transformer tiền huấn luyện đã
được khám phá rộng rãi. Jia et al. [1] đề xuất thêm một vài
token bổ sung, cụ thể là các token prompt, vào không gian đầu vào
như các tham số có thể điều chỉnh. Các token prompt được đưa
vào multi-head attention (MHA) cùng với các
token gốc. Cụ thể, họ chỉ tinh chỉnh các token prompt
trong khi giữ các tham số backbone transformer đông lạnh.
Đáng ngạc nhiên, việc tinh chỉnh các token prompt đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với tinh chỉnh đầy đủ.
Houlsby et al. [12] chèn kiến trúc adapter vào
Feed-Forward Network (FFN) và tinh chỉnh các lớp adapter, nhằm thích ứng trọng số backbone tiền huấn luyện với
các nhiệm vụ hạ nguồn. Adapter thường là kiến trúc giống bottleneck
bao gồm lớp down-sample, lớp phi tuyến, và lớp up-sample. Hu et al. [2] đề xuất phương pháp thích ứng low-rank bằng cách phân tách các gia số
của biến đổi query và biến đổi value thành
cách thức low-rank và đạt được độ chính xác cao hơn và tiêu thụ bộ nhớ thấp hơn. Zhang et al. [13] tập trung vào việc kết hợp các phương pháp PETL hiện có mà không cần thiết kế thủ công. Họ
huấn luyện một supernet lớn đầu tiên và sau đó thực hiện tìm kiếm kiến trúc neural trên chiều ẩn h của Adapter, rank r của LoRA, và độ dài prompt l của VPT để
tìm subnet tốt nhất cho mỗi nhiệm vụ bằng thuật toán tìm kiếm kiến trúc neural one-shot [33]. Lian et al. [15] đề xuất một baseline mới cho việc điều chỉnh mô hình hiệu quả. Lấy cảm hứng từ
các phương pháp chuẩn hóa khác nhau, họ đã scale và shift các đặc trưng sâu được trích xuất bởi một mô hình tiền huấn luyện với các yếu tố scale và shift. Jie et al. [16] đề xuất khung tensorization-decomposition để lưu trữ các gia số trọng số, trong đó các trọng số của mỗi ViT được tensorized thành một tensor 3D duy nhất, và các gia số của chúng sau đó được
phân tách thành các yếu tố nhẹ. Trong quá trình tinh chỉnh, chỉ cần cập nhật các yếu tố. Thông thường, các phương pháp trên chèn các mô-đun có thể học nhỏ vào các mô hình transformer tiền huấn luyện quy mô lớn và tinh chỉnh các mô-đun này với các nhiệm vụ hạ nguồn trong khi đông lạnh các tham số transformer tiền huấn luyện. Các phương pháp này có tính hướng dẫn cho việc sử dụng các backbone transformer tiền huấn luyện trên nhiều nhiệm vụ thị giác.
PETuning cho Few-Shot Learning. Trong một số ứng dụng thực tế, dữ liệu được gán nhãn chất lượng cao thường khan hiếm do
chi phí chú thích đắt đỏ và các mối quan tâm về quyền riêng tư tiềm ẩn [34], [35]. Các mô hình transformer tiền huấn luyện đã được
thích ứng thành công để giảm thiểu hạn chế này thông qua
các kỹ thuật như VPT [1], Adapter [12], và NOAH
[13], tinh chỉnh chỉ một tỷ lệ nhỏ tổng số
tham số trong khi duy trì hiệu suất cạnh tranh trên
các nhiệm vụ hạ nguồn. Tuy nhiên, vẫn còn là câu hỏi mở
liệu các kỹ thuật này có thể được áp dụng hiệu quả cho các nhiệm vụ few-shot learning, nơi các ví dụ huấn luyện có sẵn
thậm chí còn hạn chế hơn. Các nghiên cứu gần đây trong xử lý ngôn ngữ tự nhiên đã bắt đầu khám phá thách thức này [36], [37],
[38], [39], [40], [41]. Dựa trên công trình này, chúng tôi mở rộng nghiên cứu sang điều chỉnh tham số hiệu quả few-shot (PETuning) trong lĩnh vực thị giác máy tính. Lưu ý rằng, LORA
[2] nhằm duy trì danh tính của đầu ra cho một
lớp được chèn khi huấn luyện transformer sau khi thêm
một mô-đun mới, đạt được bằng cách khởi tạo đúng cách mô-đun mới. Tuy nhiên, chúng tôi quan sát thấy rằng LORA gặp khó khăn trong các thiết lập few-shot. Ngược lại, bằng cách kết hợp
khung PVP Tuning được đề xuất của chúng tôi, chúng tôi chứng minh
sự cải thiện đáng kể trong hiệu suất few-shot của LORA,
như được chứng minh trong phần thí nghiệm.
3 PHƯƠNG PHÁP ĐỀ XUẤT
3.1 Tổng quan
Trong phần này, trước tiên chúng tôi xem xét lại các kỹ thuật PETuning hiện có
và sau đó tiến hành các thí nghiệm khám phá để xác minh
hiệu suất của các kỹ thuật PETuning hiện có bao gồm
VPT [1], Adapter [12], và LoRA [2] trên các nhiệm vụ few-shot learning. Tiếp theo, chúng tôi đề xuất PVP, đầu tiên tiền huấn luyện các tham số có thể điều chỉnh của PETuning trên một bộ dữ liệu lớn và sau đó
sử dụng các tham số tiền huấn luyện cho PETuning hạ nguồn.

--- TRANG 4 ---
4
Chúng tôi tóm tắt phần này bằng cách thảo luận về tính linh hoạt của
khung PVP của chúng tôi.
3.2 Xem lại các Phương pháp Điều chỉnh Tham số Hiệu quả
Ở đây, chúng tôi tóm tắt ngắn gọn các kỹ thuật PETuning. Ý tưởng chính
của PETuning là chèn một vài tham số vào
backbone transformer. Các tham số backbone transformer
được đông lạnh để tạo ra các biểu diễn tổng quát được học
từ dữ liệu quy mô lớn và các tham số mới được chèn là
có thể điều chỉnh để thích ứng phân phối đầu ra với các nhiệm vụ hạ nguồn cụ thể. Chúng tôi sử dụng F để biểu thị mô hình vision transformer
với tham số θ. Đối với kiến trúc transformer,
y = F(x; θ); (1)
và gradient được tính như
g = ∂F(D; θ)/∂θ; (2)
trong đó D là bộ dữ liệu huấn luyện quy mô lớn. Đối với các phương pháp PETuning, một vài tham số mới θ' được chèn vào F,
y = F(x; θ, θ'); (3)
trong đó θ' thường nhỏ hơn nhiều so với θ và θ được cố định trong quá trình
tinh chỉnh với chỉ θ' có thể học. Cập nhật gradient cho
các phương pháp PETuning được công thức hóa như
g' = ∂F(D'; θ, θ')/∂θ'; (4)
trong đó D' là bộ dữ liệu hạ nguồn cho một nhiệm vụ cụ thể và
thường nhỏ hơn nhiều so với D.
3.3 Khám phá Điều chỉnh Tham số Hiệu quả Few-shot
Để nghiên cứu PETuning few-shot, chúng tôi lấy VPT [1], Adapter
[12], và LoRA [2] làm ví dụ được điều chỉnh với
các ví dụ huấn luyện hạn chế. Cụ thể, chúng tôi điều chỉnh khung VPT,
Adapter, và LoRA với các tỷ lệ khác nhau của
các ví dụ huấn luyện trên bộ dữ liệu CUB-200-2011, giảm từ 16 mẫu huấn luyện xuống 8 hoặc thậm chí 1 mẫu mỗi lớp,
xác thực hiệu suất của các phương pháp này dưới thiết lập few-shot learning. Như thể hiện trong Hình 2, rõ ràng là
hiệu suất của cả ba phương pháp đều giảm đáng kể
khi được điều chỉnh với ít hơn 4 mẫu huấn luyện mỗi lớp,
và gần như thất bại khi chỉ có 1 mẫu huấn luyện mỗi
lớp. Điều này chỉ ra rằng các kỹ thuật điều chỉnh tham số hiệu quả hiện có có thể không thể hoạt động tốt dưới thiết lập few-shot learning.
Phân tích. Chúng tôi cho rằng hiện tượng này do việc khởi tạo không phù hợp của các tham số mới được thêm vào vì
các phương pháp PETuning được sử dụng để khởi tạo ngẫu nhiên các tham số mới được thêm vào này với giá trị trung bình bằng không, có nghĩa là các tham số mới được thêm vào cần phải học từ đầu
trên các nhiệm vụ hạ nguồn. Điều này dẫn đến mô-đun mới được thêm vào
yêu cầu lượng dữ liệu đáng kể để cập nhật gradient,
điều này có thể chứng minh là thách thức cho các nhiệm vụ hạ nguồn và
hạn chế ứng dụng của nó cho các nhiệm vụ few-shot. Để tận dụng hơn nữa
PETuning trên dữ liệu hạn chế, các tham số mới được thêm vào cũng cần tiền huấn luyện. Do đó, chúng tôi tiền huấn luyện các mô-đun mới được thêm vào để có được khởi tạo tốt hơn cho điều chỉnh tham số hiệu quả

Thuật toán 1 Khung PVP dựa trên VPT, giống PyTorch.
# Để điều chỉnh prompt hạ nguồn
# type: loại visual prompt tuning, "Deep" hoặc "Shallow"
# k: số lượng token visual prompt tuning
# giai đoạn tiền huấn luyện prompt
# xây dựng mô hình để tiền huấn luyện token prompt
# type="Deep", tiền huấn luyện N token prompt với N >= k
net=build_model(vpt_type="Deep",num_prompt_tokens=N)
for x, label in pre-train_dataloader:
    loss=net.forward(x,label)
    loss.backward()
# Prompt_tokens shape: (num_layer,num_tokens,embed_dim)
torch.save(net.Prompt_tokens,"ckpt")
# giai đoạn prompt tuning có tiền huấn luyện
# xây dựng mô hình cho prompt tuning hạ nguồn
net=build_model(vpt_type=type,num_prompt_tokens=k)
load_prompts(net,ckpt,vpt_type=type,num_token=k, load_type)
for x, label in downstream_dataloader:
    loss=net.forward(x,label)
    loss.backward()
net.test_loop()
# giai đoạn tải prompt có tiền huấn luyện
def load_prompts(net,ckpt,vpt_type,num_token,load_type):
    if load_type=="Average":# Sử dụng token trung bình
        checkpoint=torch.mean(ckpt,dim=1)
        checkpoint=checkpoint.unsqueeze(1)
        checkpoint=checkpoint.expand(-1,num_token,-1)
    else:# Sử dụng token tuần tự
        checkpoint=ckpt
    if vpt_type=="Deep":
        net.Prompt_tokens=checkpoint[:,:num_token,:]
    else:# vpt_type=="Shallow"
        net.Prompt_tokens=checkpoint[:1,:num_token,:]
điều chỉnh trên một nhiệm vụ cụ thể và đề xuất khung điều chỉnh tham số hiệu quả thị giác có tiền huấn luyện của chúng tôi. Điều này là trực quan vì
giai đoạn tiền huấn luyện mô-đun mới được thêm vào có thể cung cấp một cơ sở tốt và các tham số mới được thêm vào chỉ yêu cầu
một vài dữ liệu để học tốt trên các nhiệm vụ few-shot hạ nguồn.
3.4 Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện
Có hai giai đoạn cho phương pháp Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP) của chúng tôi. Như Hình 2.1 cho thấy, chúng tôi thực hiện điều chỉnh tham số hiệu quả trên dữ liệu tiền huấn luyện trong giai đoạn 1
và sử dụng các tham số đã học để khởi tạo mô-đun điều chỉnh tham số hiệu quả cho các nhiệm vụ hạ nguồn trong giai đoạn 2.
(1) Giai đoạn tiền huấn luyện mô-đun điều chỉnh tham số hiệu quả.
Từ Phương trình 1-4, mục tiêu của các phương pháp Điều chỉnh Tham số Hiệu quả khác nhau là tối ưu hóa các tham số θ' sử dụng
bộ dữ liệu D'. Tuy nhiên, rất khó để tối ưu hóa trực tiếp các tham số θ' khi bộ dữ liệu hạ nguồn D' bị hạn chế.
Ở đây chúng tôi sử dụng một bộ dữ liệu tiền huấn luyện mô-đun điều chỉnh tham số hiệu quả khác D'' lớn hơn D' để tiền huấn luyện các tham số mới được thêm vào θ', như được công thức hóa dưới đây,
g' = ∂F(D''; θ, θ')/∂θ'; (5)
(2) Giai đoạn điều chỉnh tham số hiệu quả hạ nguồn. Chúng tôi sử dụng
các tham số tối ưu hóa θ' trong Phương trình 5 để khởi tạo các tham số trong Phương trình 4 cho Điều chỉnh tham số hiệu quả thị giác có tiền huấn luyện của chúng tôi.
3.5 Tính linh hoạt của PVP Tuning
Chìa khóa của khung PVP là sử dụng các prompt tiền huấn luyện
cho các nhiệm vụ few-shot hạ nguồn. Với việc các phương pháp PETuning hiện tại [1], [2], [12] chủ yếu chèn các mô-đun prompt đa dạng vào vision transformers và điều chỉnh các mô-đun mới được thêm vào

--- TRANG 5 ---
5
CUB
 CIFAR Caltech
Flowers102
 Pets
 Sun397 SVHN
 EuroSAT PatchCamelyon
 Resisc45 Retinopathy
DMLab dSprites/loc
 SmallNORB/azi KITTI Clevr/countNABirds OxfordFlowers StanfordDogs StanfordCars
 DTD
dSprites/ori SmallNORB/ele Clevr/distance
Hình 4. Ví dụ về tất cả các nhiệm vụ phân loại được đánh giá. Một hình ảnh đại diện cho mỗi bộ dữ liệu trong FGVC và VTAB-1k.
Bảng 1. Thông tin chi tiết về các bộ dữ liệu FGVC và VTAB-1k.
Bộ dữ liệu Mô tả # Lớp Huấn luyện Kiểm tra Thử nghiệm
Nhiệm vụ nhận dạng thị giác tinh vi (FGVC)
CUB [42] Nhận dạng loài chim tinh vi 200
1/2/4/8/16 mỗi lớp600 5,794
NABirds [43] Nhận dạng loài chim tinh vi 555 2,393 24,633
Oxford Flowers [44] Nhận dạng loài hoa tinh vi 102 1,020 6,149
Stanford Dogs [45] Nhận dạng loài chó tinh vi 120 1,200 8,580
Stanford Cars [46] Phân loại xe hơi tinh vi 196 815 8,041
Benchmark Thích ứng Nhiệm vụ Thị giác (VTAB) [47]
CIFAR-100 [48]
Tự nhiên100
800/1000 20010,000
Caltech101 [49] 102 6,084
DTD [50] 47 1,880
Flowers102 [51] 102 6,149
Pets [52] 37 3,669
SVHN [53] 10 26,032
Sun397 [54] 397 21,750
Patch Camelyon [55]
Chuyên biệt2
800/1000 20032,768
EuroSAT [56] 10 5,400
Resisc45 [57] 45 6,300
Retinopathy [58] 5 42,670
Clevr/count [59]
Có cấu trúc8
800/1000 20015,000
Clevr/distance [59] 6 15,000
DMLab [60] 6 22,735
KITTI/distance [61] 4 711
dSprites/location [62] 16 73,728
dSprites/orientation [62] 16 73,728
SmallNORB/azimuth [63] 18 12,150
SmallNORB/elevation [63] 9 12,150
các mô-đun trong khi giữ backbone transformer đông lạnh.
Do đó, PVP không phụ thuộc vào baseline và có thể áp dụng
cho các phương pháp PETuning khác nhau. Trong phần này, chúng tôi nghiên cứu
tính linh hoạt của khung PVP được đề xuất trên VPT [1],
Adapter [12], và LoRA [2].
PVP Tuning dựa trên VPT. Chìa khóa của cách tiếp cận của chúng tôi là
sử dụng các visual prompt tiền huấn luyện cho prompt tuning. Cụ thể, trước tiên chúng tôi thêm các token prompt vào ViT và theo
VPT để khởi tạo các token prompt. Tiếp theo, chúng tôi thực hiện prompt tuning trên ImageNet-1k với backbone transformer được tiền huấn luyện trên ImageNet-21k để có được các token prompt tiền huấn luyện.
Cuối cùng, chúng tôi tải các token prompt tiền huấn luyện thay vì
điều chỉnh từ đầu, trước khi prompt tuning trên các nhiệm vụ few-shot hạ nguồn. Thuật toán 1 hiển thị quy trình tổng thể của khung PVP, bao gồm giai đoạn tiền huấn luyện prompt, giai đoạn tải prompt tiền huấn luyện và giai đoạn prompt tuning có tiền huấn luyện. Đáng chú ý, số lượng token prompt
trong VPT dao động từ 1 đến 200 và chúng tôi trực tiếp thêm 200 token prompt
vào mỗi lớp ViT trong giai đoạn tiền huấn luyện prompt, do đó chúng tôi có thể tải bất kỳ số lượng token prompt tiền huấn luyện nào
trong số 200 token prompt trên các nhiệm vụ few-shot hạ nguồn thay vì thực hiện tiền huấn luyện prompt
lặp đi lặp lại cho các thiết lập số lượng token prompt khác nhau. Cụ thể, có hai cách để tải các token prompt tiền huấn luyện, được liệt kê dưới đây:
Tải Tuần tự. Như tên gọi, chúng tôi tải
các token prompt tiền huấn luyện một cách tuần tự. Ví dụ,
nếu có tổng cộng N token prompt tiền huấn luyện và chúng tôi
cần tải K token prompt tiền huấn luyện. Trong trường hợp này,

--- TRANG 6 ---
6
Bảng 2. Kết quả định lượng về few-shot learning FGVC.
Độ chính xác (%) CUB-200-2011 NABirds Oxford Flowers Stanford Dogs Stanford Cars Trung bình
FULL16 shot 85.12 79.43 99.20 72.10 76.91 82.55
8 shot 77.36 66.60 96.42 41.85 41.20 64.69
4 shot 60.61 39.10 94.23 19.80 23.57 47.46
2 shot 14.53 9.93 56.43 3.90 5.85 18.13
1 shot 9.44 2.50 38.61 1.75 4.17 11.29
VPT16 shot 84.66 76.71 99.38 80.82 57.33 79.78
8 shot 79.10 64.73 98.75 77.11 36.31 71.20
4 shot 70.61 40.43 96.85 68.22 20.62 59.35
2 shot 53.26 27.94 92.73 49.02 8.64 46.32
1 shot 32.88 14.84 66.01 36.67 5.20 31.12
PVP (của chúng tôi)16 shot 86.28 (+1.62) 80.05 (+3.34) 99.48 (+0.10) 81.77 (+0.95) 61.09 (+3.76) 81.73 (+1.95)
8 shot 81.53 (+2.43) 71.78 (+7.05) 99.02 (+0.27) 77.81 (+0.70) 40.57 (+4.26) 74.14 (+2.94)
4 shot 74.37 (+3.76) 58.16 (+17.73) 98.49 (+1.64) 71.43 (+3.21) 26.08 (+5.46) 65.71 (+6.36)
2 shot 62.20 (+8.94) 53.82 (+25.88) 96.11 (+3.38) 62.32 (+13.30) 14.73 (+6.09) 57.84 (+11.52)
1 shot 49.24 (+16.36) 39.74 (+24.90) 88.84 (+22.83) 47.45 (+10.78) 10.73 (+5.53) 47.20 (+16.08)
Bảng 3. Kết quả định lượng về transfer learning VTAB-1k.
CIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Trung bình
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Trung bình
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/location
dSprites/orientation
SmallNORB/azimuth
SmallNORB/elevation
Trung bình
Trung bình tổng thể
Phương pháp truyền thống
Full Tune [1] 68.9 87.7 64.3 97.2 86.9 87.4 38.8 75.88 79.7 95.7 84.2 73.9 83.36 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 47.64 68.96
Linear Probe [1] 63.4 85.0 63.2 97.0 86.3 36.6 51.0 68.93 78.5 87.5 68.6 74.0 77.16 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 26.84 57.64
Phương pháp PETuning
VPT-Shallow(ECCV'22) [1] 77.7 86.9 62.6 97.5 87.3 74.5 51.2 76.81 78.2 92.0 75.6 72.9 79.66 55.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 46.98 67.82
VPT-Deep(ECCV'22) [1] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.48 81.8 96.1 83.4 68.4 82.43 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 54.98 71.96
NOAH(arXiv'22) [13] 70.7 91.6 68.2 98.9 90.2 88.4 54.0 80.29 85.9 95.3 84.2 73.6 84.75 81.7 63.1 49.0 78.5 82.3 45.0 31.8 43.5 59.36 74.80
SSF(Neurips'22) [15] 69.0 92.6 71.5 99.4 91.8 90.2 52.9 81.57 87.4 95.9 87.4 75.5 86.55 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 58.96 75.69
FacT(AAAI'23) [16] 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.64 84.8 96.2 84.5 75.7 85.30 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.71 75.55
PVP (của chúng tôi) 76.3 94.4 73.1 99.7 92.3 87.3 58.6 83.09 87.5 95.6 87.4 76.9 86.84 76.4 64.6 54.6 82.0 88.0 58.5 36.2 52.8 64.13 78.02
chúng tôi trực tiếp tải K đầu tiên trong tổng số N token prompt tiền huấn luyện.
Tải Trung bình. Khác với cách tải tuần tự, chúng tôi sử dụng token prompt tiền huấn luyện trung bình để khởi tạo các token prompt. Ví dụ, nếu có tổng cộng N token prompt tiền huấn luyện và chúng tôi cần tải K token prompt tiền huấn luyện, chúng tôi lấy trung bình N token prompt tiền huấn luyện và sau đó mở rộng nó thành K token để tải.
PVP Tuning dựa trên Adapter. Adapter chèn kiến trúc adapter vào mỗi khối transformer,
X' ← X + σ(XWdown)Wup;
trong đó X ∈ R^(N×d) là đầu ra của các khối Feed-Forward Network
(FFN) trong mỗi lớp transformer, σ là hàm phi tuyến, Wdown ∈ R^(d×h), Wup ∈ R^(h×d) và h << d. Chúng tôi trực tiếp thực hiện adapter tuning trên ImageNet-1k để có được Wdown và Wup tiền huấn luyện cho mỗi khối transformer và sau đó tải các tham số tiền huấn luyện này của mỗi Wdown và Wup cho adapter tuning hạ nguồn.
PVP Tuning dựa trên LoRA. LoRA phân tách các gia số của biến đổi query Wq và biến đổi value Wv thành low-rank Aq/v ∈ R^(d×r) và Bq/v ∈ R^(r×d) trong đó r << d. Query và value sau đó được tính như
Q/V = X·Wq/v + s·X·Aq/v·Bq/v;
trong đó s là một siêu tham số. Tương tự như Adapter, trước tiên chúng tôi tiền huấn luyện các Aq/v và Bq/v này trên ImageNet-1k và sau đó sử dụng Aq/v và Bq/v tiền huấn luyện cho LoRA tuning hạ nguồn.
Chúng tôi hiển thị kết quả thí nghiệm về tính linh hoạt của phương pháp đề xuất của chúng tôi trong Mục 4.3.2.

--- TRANG 7 ---
7
1 2 4 8 16
Số lượng mẫu mỗi lớp10203040506070809T est accuracy (%)
Trung bình trên 5 bộ dữ liệu
FULL
Average
Sequential
1 2 4 8 16
Số lượng mẫu mỗi lớp510203040506070809T est accuracy (%)
CUB-200-2011
FULL
Average
Sequential
1 2 4 8 16
Số lượng mẫu mỗi lớp010203040506070809T est accuracy (%)
NABirds
FULL
Average
Sequential
1 2 4 8 16
Số lượng mẫu mỗi lớp3040506070809100T est accuracy (%)
Oxford Flowers
FULL
Average
Sequential
1 2 4 8 16
Số lượng mẫu mỗi lớp010203040506070809T est accuracy (%)
Stanford Dogs
FULL
Average
Sequential
1 2 4 8 16
Số lượng mẫu mỗi lớp01020304050607T est accuracy (%)
Stanford Cars
FULL
Average
Sequential
Hình 5. Kết quả của hai cách tải token prompt trong PVP(VPT). Average và sequential biểu thị cách tải trung bình và cách tải tuần tự.
1 5 10 50 100
Số lượng token prompt20304050607T est accuracy (%)
NABirds(4 shots)
VPT
PVP(VPT)
1 5 10 50 100
Số lượng token prompt50607080T est accuracy (%)
NABirds(8 shots)
VPT
PVP(VPT)
Hình 6. Độ chính xác kiểm tra của VPT và PVP dựa trên VPT của chúng tôi với số lượng token prompt khác nhau trên bộ dữ liệu NABirds dưới thiết lập 4 shots và 8 shots.
4 THÍ NGHIỆM
4.1 Bộ dữ liệu
Đối với phương pháp đề xuất của chúng tôi, chúng tôi đánh giá hiệu suất few-shot learning trên các bộ dữ liệu Nhận dạng Thị giác Tinh vi (FGVC) và hiệu suất transfer learning trên Benchmark Thích ứng Nhiệm vụ Thị giác (VTAB-1k).
(1) FGVC chứa các bộ dữ liệu phân loại thị giác tinh vi thường được sử dụng, thường được sử dụng cho few-shot learning, bao gồm CUB-200-2011 [42], NABirds [43], Oxford Flowers [44], Stanford Dogs [45] và Stanford Cars [46]. Chúng tôi theo [13], [15] để sử dụng X (X=1,2,4,8,16) mẫu mỗi lớp
cho phân loại ảnh few-shot trên các bộ dữ liệu này.
(2) VTAB-1k [47], bao gồm 19 bộ dữ liệu phân loại thị giác, bao quát dữ liệu trong 3 lĩnh vực, bao gồm nhiệm vụ tự nhiên,
nhiệm vụ chuyên biệt, và nhiệm vụ có cấu trúc. Nhiệm vụ tự nhiên bao gồm hình ảnh trong cuộc sống hàng ngày. Nhiệm vụ chuyên biệt bao gồm
hình ảnh được chụp bằng thiết bị chuyên dụng, chẳng hạn như hình ảnh y tế và vệ tinh. Nhiệm vụ có cấu trúc bao gồm hình ảnh
yêu cầu hiểu biết ngữ nghĩa, chẳng hạn như đếm đối tượng. Mỗi trong số 19 bộ dữ liệu chứa 1000 hình ảnh, phản ánh "1k" trong tên "VTAB-1k". Các bộ dữ liệu này bao quát
một phạm vi rộng các miền có thể có mà các nhiệm vụ hạ nguồn
đến từ, và do đó hiệu quả của các phương pháp PETuning có thể được đo lường một cách toàn diện.
Bảng 1 hiển thị thông tin chi tiết về các bộ dữ liệu này.
Ví dụ về các benchmark FGVC và VTAB-1k được hiển thị
trong Hình 4. Lưu ý rằng Clevr/count và Clevr/distance,
dSprites/location và dSprites/orientation cũng như SmallNORB/azimuth và SmallNORB/elevation thực tế là
cùng một bộ dữ liệu nhưng cho các nhiệm vụ khác nhau tương ứng.
4.2 Augmentation và Siêu tham số
Chúng tôi áp dụng chiến lược augmentation hình ảnh tiêu chuẩn trong quá trình huấn luyện: chuẩn hóa với giá trị trung bình và độ lệch chuẩn ImageNet, cắt thay đổi kích thước ngẫu nhiên thành 224×224 và lật ngang ngẫu nhiên cho năm bộ dữ liệu FGVC và thay đổi kích thước thành 224×224
cho bộ VTAB-1k. Theo VPT [1], chúng tôi thực hiện tìm kiếm lưới
để tìm các siêu tham số dành riêng cho điều chỉnh, tốc độ học,
và giá trị weight decay cho mỗi nhiệm vụ.

--- TRANG 8 ---
8
1 2 4 8 16
Số lượng mẫu mỗi lớp10203040506070809T est accuracy (%)
Trung bình trên 5 bộ dữ liệu
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
1 2 4 8 16
Số lượng mẫu mỗi lớp510203040506070809T est accuracy (%)
CUB-200-2011
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
1 2 4 8 16
Số lượng mẫu mỗi lớp010203040506070809T est accuracy (%)
NABirds
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
1 2 4 8 16
Số lượng mẫu mỗi lớp3040506070809100T est accuracy (%)
Oxford Flowers
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
1 2 4 8 16
Số lượng mẫu mỗi lớp010203040506070809T est accuracy (%)
Stanford Dogs
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
1 2 4 8 16
Số lượng mẫu mỗi lớp01020304050607T est accuracy (%)
Stanford Cars
LoRA
PVP(LoRA)
Adapter
PVP(Adapter)
Hình 7. Kết quả của khung PVP dựa trên Adapter và LoRA.
4.3 Few-Shot Learning Tinh vi
Đối với hiệu suất few-shot learning, chúng tôi so sánh các phương pháp của chúng tôi với nhiều baseline cạnh tranh, bao gồm VPT [1],
Adapter [12], và LoRA [2]. Đối với tất cả baseline, chúng tôi sử dụng ViT-B/16 [3] được tiền huấn luyện trên ImageNet-21K có giám sát làm backbone transformer. Tất cả các thí nghiệm của chúng tôi được thực hiện trên GPU NVIDIA A100-40GB.
4.3.1 PVP dựa trên VPT
Chúng tôi tiền huấn luyện 200 token prompt trên ImageNet-1k cho prompt tuning hạ nguồn trong đó số lượng gần bằng với số lượng token patch hình ảnh (196) trong mỗi Multi-head Self Attention (MSA) cho kiến trúc ViT-B/16 [3]. Đối với mỗi bộ dữ liệu hạ nguồn, chúng tôi theo VPT [1]
để tìm kiếm lưới số lượng token prompt cho so sánh công bằng.
Hiệu suất dưới các thiết lập few-shot learning khác nhau. Như thể hiện trong Bảng 2, chúng tôi so sánh định lượng hiệu suất đạt được bởi điều chỉnh FULL, VPT, và PVP dựa trên VPT được đề xuất dưới các thiết lập few-shot khác nhau trên
năm bộ dữ liệu khác nhau. Có thể thấy rằng khi số lượng mẫu huấn luyện đủ, chẳng hạn như 16 mẫu mỗi
lớp, PVP dựa trên VPT đạt độ chính xác kiểm tra hơn 99% trên Oxford Flowers và hơn 86% độ chính xác kiểm tra trên CUB-200-2011, tương đương với tinh chỉnh đầy đủ sử dụng tất cả các mẫu huấn luyện trên hai bộ dữ liệu này. Quan trọng hơn,
các token prompt tiền huấn luyện cho thấy cải thiện hiệu suất đáng kể trong chế độ few-shot, như 1 hoặc 2 shots mỗi lớp. Những kết quả này chứng minh rằng các token prompt tiền huấn luyện
là cần thiết để áp dụng các mô hình transformer lớn
cho các nhiệm vụ few-shot.
Cách tải token. Trong triển khai của chúng tôi, có hai cách để tải các token prompt tiền huấn luyện. Chúng tôi
nghiên cứu tác động của hai cách này. Như Hình 5 cho thấy,
việc tải các prompt tiền huấn luyện một cách tuần tự vượt trội hơn
việc tải trung bình và chúng tôi sử dụng cách tuần tự làm
thiết lập tải mặc định trong phần còn lại của bài báo này. Chúng tôi cho rằng
lý do hiệu suất thấp là do thiếu thông tin vị trí khi tải các token prompt tiền huấn luyện một cách trung bình, trong đó các token prompt được lấy trung bình trước và sau đó được mở rộng thành số lượng token yêu cầu, do đó,
thông tin vị trí bị thiếu.
Độ nhạy độ dài prompt. Trong VPT [1], số lượng token prompt mới được thêm vào được chọn từ
{1,5,10,50,100,200} và họ sử dụng tập kiểm tra của mỗi
bộ dữ liệu để xác định độ dài prompt tốt nhất. Chúng tôi cũng thực hiện thí nghiệm về số lượng token prompt để xác thực
độ nhạy. Như Hình 6 cho thấy, độ chính xác của VPT thay đổi
rất nhiều (hơn 25% dưới thiết lập 4 shots) khi số lượng token prompt khác nhau, trong khi độ chính xác
của khung PVP dựa trên VPT của chúng tôi nhất quán hơn và mạnh mẽ hơn (ít hơn 2% dưới thiết lập 4 shots) trên
bộ dữ liệu NABirds. Chúng tôi cho rằng lý do là việc tiền huấn luyện
các token prompt vì quá trình tiền huấn luyện cung cấp cho các token prompt khởi tạo tốt hơn và do đó chúng có thể học
trên dữ liệu hạn chế một cách ổn định.

--- TRANG 9 ---
9
4.3.2 PVP dựa trên Adapter và LoRA
Hình 7 hiển thị độ chính xác của Adapter và LoRA với
hoặc không có tiền huấn luyện prompt dưới các thiết lập shots khác nhau.
Có thể thấy rằng khung PVP được đề xuất mang lại
cải thiện độ chính xác trên cả Adapter và LoRA trong các thiết lập few-shot learning, điều này càng xác thực tầm quan trọng của
tiền huấn luyện prompt cũng như tính linh hoạt của khung PVP.
4.4 Transfer Learning VTAB-1k
Đối với hiệu suất transfer learning, chúng tôi so sánh các phương pháp của chúng tôi với nhiều phương pháp PETuning, bao gồm VPT-Deep [1],
VPT-Shallow [1], NOAH [13], SSF [15] và FacT [16]. Chúng tôi
sử dụng ViT-B/16 [3] được tiền huấn luyện trên ImageNet-21K có giám sát làm backbone transformer và sử dụng VPT làm baseline của chúng tôi.
Theo VPT [1], chúng tôi tìm kiếm số lượng token prompt
từ {1,10,50,100,200} cho mỗi bộ dữ liệu. Để thuận tiện, chúng tôi
trực tiếp sử dụng các token prompt được tiền huấn luyện trên ImageNet-1k cho
cả nhiệm vụ tự nhiên, nhiệm vụ chuyên biệt, và nhiệm vụ có cấu trúc.
Tất cả các thí nghiệm của chúng tôi được thực hiện trên GPU NVIDIA A100-40GB.
Kết quả thí nghiệm được hiển thị trong Bảng 3, từ đó
chúng ta có thể thấy rằng:
1) PVP(VPT) của chúng tôi đạt hiệu suất tương đương với
các phương pháp PETuning tiên tiến trước đây. Trên
benchmark VTAB-1k, PVP(VPT) đạt độ chính xác cao nhất
trên 16 bộ dữ liệu trong tổng số 19 bộ dữ liệu và đạt được
cải thiện độ chính xác trung bình 1.52%, 0.29% và 3.42%
trên nhiệm vụ tự nhiên, nhiệm vụ chuyên biệt, và nhiệm vụ có cấu trúc,
tương ứng.
2) Mặc dù chúng tôi sử dụng các prompt được tiền huấn luyện trên ImageNet-1k,
chủ yếu về hình ảnh tự nhiên, khung PVP dựa trên VPT của chúng tôi
cũng hoạt động tốt trên các nhiệm vụ chuyên biệt và
nhiệm vụ có cấu trúc.
5 KẾT LUẬN
Trong bài báo này, chúng tôi nghiên cứu các phương pháp Điều chỉnh Tham số Hiệu quả (PETuning) gần đây và đầu tiên quan sát thấy rằng các phương pháp PETuning hiện tại
hoạt động kém trong tình huống few-shot. Sau đó, chúng tôi
đề xuất Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP), một khung đơn giản về mặt khái niệm và trực quan để tận dụng các mô hình transformer tiền huấn luyện quy mô lớn cho các nhiệm vụ few-shot. Chìa khóa của phương pháp chúng tôi là tiền huấn luyện các mô-đun prompt của các phương pháp PETuning gần đây, cho phép khởi tạo tốt hơn cho PETuning hạ nguồn. Các thí nghiệm rộng rãi trên
VPT, Adapter, và LoRA cho thấy hiệu quả và tính linh hoạt của khung PVP về few-shot learning.
Bên cạnh khả năng few-shot, PVP cũng cho thấy khả năng transfer learning tương đương với các phương pháp PETuning gần đây. Trên
benchmark VTAB-1k, PVP đạt kết quả tiên tiến
trên 16 trong tổng số 19 bộ dữ liệu và cải thiện độ chính xác trung bình
3.34%, 2.85%, và 3.66% cho VTAB-Natural,
VTAB-Specialized và VTAB-Structured, tương ứng. Chúng tôi
hy vọng công trình của chúng tôi có thể truyền cảm hứng cho nghiên cứu tương lai về việc sử dụng hiệu quả và nhẹ hơn các mô hình thị giác lớn. Tuy nhiên,
Điều chỉnh Tham số hiệu quả Thị giác có Tiền huấn luyện (PVP) là một
phương pháp thực nghiệm với bằng chứng thí nghiệm hiện tại. Mặc dù
đạt được kết quả tiên tiến trên các benchmark FGVC và VTAB-1k, việc giải thích lý thuyết đằng sau PVP
vẫn đang được khám phá.
Lời cảm ơn. Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc dưới Grants
62006241, và 61902415.

--- TRANG 10 ---
10
TÀI LIỆU THAM KHẢO
[1] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
và S.-N. Lim, "Visual prompt tuning," 2022.
[2] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
và W. Chen, "Lora: Low-rank adaptation of large language models," trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,
2022.
[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, và N. Houlsby, "An image is worth 16x16 words:
Transformers for image recognition at scale," 2021.
[4] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, và B. Guo,
"Swin transformer: Hierarchical vision transformer using shifted
windows," trong 2021 IEEE/CVF International Conference on Computer
Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE,
2021, pp. 9992–10 002.
[5] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., "Imagenet
large scale visual recognition challenge," International journal of
computer vision, vol. 115, no. 3, pp. 211–252, 2015.
[6] J. Yosinski, J. Clune, Y. Bengio, và H. Lipson, "How transferable
are features in deep neural networks?" MIT Press, 2014.
[7] M. Noroozi và P. Favaro, "Unsupervised learning of visual
representations by solving jigsaw puzzles," Springer, Cham, 2016.
[8] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, và J. Malik, "Sidetuning: A baseline for network adaptation via additive side networks," 2020.
[9] E. B. Zaken, S. Ravfogel, và Y. Goldberg, "Bitfit: Simple
parameter-efficient fine-tuning for transformer-based masked
language-models," 2021.
[10] J. Pfeiffer, A. Kamath, A. Rückle, K. Cho, và I. Gurevych,
"Adapterfusion: Non-destructive task composition for transfer
learning," arXiv preprint arXiv:2005.00247, 2020.
[11] J. Pfeiffer, A. Rückle, C. Poth, A. Kamath, I. Vulić, S. Ruder,
K. Cho, và I. Gurevych, "Adapterhub: A framework for adapting
transformers," arXiv preprint arXiv:2007.07779, 2020.
[12] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, và S. Gelly, "Parameterefficient transfer learning for NLP," trong Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, ser. Proceedings of Machine
Learning Research, K. Chaudhuri và R. Salakhutdinov, Eds.,
vol. 97. PMLR, 2019, pp. 2790–2799.
[13] Y. Zhang, K. Zhou, và Z. Liu, "Neural prompt search," CoRR,
vol. abs/2206.04673, 2022.
[14] S. Jie và Z. Deng, "Convolutional bypasses are better vision
transformer adapters," CoRR, vol. abs/2207.07039, 2022.
[15] D. Lian, Z. Daquan, J. Feng, và X. Wang, "Scaling & shifting your
features: A new baseline for efficient model tuning," 10 2022.
[16] S. Jie và Z. H. Deng, "Fact: Factor-tuning for lightweight adaptation on vision transformer," 2022.
[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, và I. Polosukhin, "Attention is all you need,"
trong Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio,
H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, và R. Garnett,
Eds., 2017, pp. 5998–6008.
[18] J. Devlin, M. W. Chang, K. Lee, và K. Toutanova, "Bert: Pretraining of deep bidirectional transformers for language understanding," 2018.
[19] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, và D. Amodei,
"Language models are few-shot learners," 2020.
[20] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, và P. J. Liu, "Exploring the limits of transfer
learning with a unified text-to-text transformer," 2019.
[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, và J. Clark, "Learning transferable visual models from natural language supervision," 2021.
[22] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, và
H. Jégou, "Training data-efficient image transformers & distillation through attention," trong Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, ser. Proceedings of Machine Learning Research, M. Meila
và T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 10 347–10 357.
[23] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, và W. Wu, "Incorporating
convolution designs into visual transformers," trong 2021 IEEE/CVF
International Conference on Computer Vision, ICCV 2021, Montreal,
QC, Canada, October 10-17, 2021. IEEE, 2021, pp. 559–568.
[24] Y. Li, K. Zhang, J. Cao, R. Timofte, và L. V. Gool, "Localvit: Bringing locality to vision transformers," CoRR, vol. abs/2104.05707,
2021.
[25] L. Hagström và R. Johansson, "How to adapt pre-trained visionand-language models to a text-only input?" trong Proceedings of the
29th International Conference on Computational Linguistics, COLING
2022, Gyeongju, Republic of Korea, October 12-17, 2022, N. Calzolari,
C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu,
H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue,
S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, và S. Na,
Eds. International Committee on Computational Linguistics,
2022, pp. 5582–5596.
[26] R. Upadhyay, P. C. Chhipa, R. Phlypo, R. Saini, và M. Liwicki,
"Multi-task meta learning: learn how to adapt to unseen tasks,"
CoRR, vol. abs/2210.06989, 2022.
[27] Y. Li, F. Luo, C. Tan, M. Wang, S. Huang, S. Li, và J. Bai,
"Parameter-efficient sparsity for large language models finetuning," trong Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July
2022, L. D. Raedt, Ed. ijcai.org, 2022, pp. 4223–4229.
[28] X. Zhou, R. Ma, Y. Zou, X. Chen, T. Gui, Q. Zhang, X. Huang,
R. Xie, và W. Wu, "Making parameter-efficient tuning more efficient: A unified framework for classification tasks," trong Proceedings
of the 29th International Conference on Computational Linguistics,
COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022,
N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi,
P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue,
S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, và S. Na,
Eds. International Committee on Computational Linguistics,
2022, pp. 7053–7064.
[29] Z. Yang, M. Ding, Y. Guo, Q. Lv, và J. Tang, "Parameterefficient tuning makes a good classification head," CoRR, vol.
abs/2210.16771, 2022.
[30] Y. Sung, J. Cho, và M. Bansal, "LST: ladder side-tuning for
parameter and memory efficient transfer learning," CoRR, vol.
abs/2206.06522, 2022.
[31] Y. Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, W. Yih,
và M. Khabsa, "Unipelt: A unified framework for parameterefficient language model tuning," CoRR, vol. abs/2110.07577, 2021.
[32] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen,
C. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H. Zheng,
J. Chen, Y. Liu, J. Tang, J. Li, và M. Sun, "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained
language models," CoRR, vol. abs/2203.06904, 2022.
[33] M. Chen, H. Peng, J. Fu, và H. Ling, "Autoformer: Searching
transformers for visual recognition," 2021.
[34] Q. Hu, B. Yang, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, và
A. Markham, "Sqn: Weakly-supervised semantic segmentation of
large-scale 3d point clouds with 1000x fewer labels," arXiv preprint
arXiv:2104.04891, 2021.
[35] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni, và A. Markham,
"Sensaturban: Learning semantics from urban-scale photogrammetric point clouds," International Journal of Computer Vision, vol.
130, no. 2, pp. 316–343, 2022.
[36] Y. Gu, X. Han, Z. Liu, và M. Huang, "PPT: Pre-trained prompt
tuning for few-shot learning," trong Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Dublin, Ireland: Association for Computational
Linguistics, May 2022, pp. 8410–8423.
[37] I. V. Robert, I. Balaevi, E. Wallace, F. Petroni, S. Singh, và
S. Riedel, "Cutting down on prompts and parameters: Simple fewshot learning with language models," 2021.
[38] K. He, Y. Huang, R. Mao, T. Gong, C. Li, và E. Cambria, "Virtual
prompt pre-training for prototype-based few-shot relation extraction," Expert Syst. Appl., vol. 213, no. Part, p. 118927, 2023.
[39] T. Bansal, S. Alzubi, T. Wang, J. Lee, và A. McCallum, "Metaadapters: Parameter efficient few-shot fine-tuning through metalearning," trong International Conference on Automated Machine Learning, AutoML 2022, 25-27 July 2022, Johns Hopkins University, Baltimore, MD, USA, ser. Proceedings of Machine Learning Research,
I. Guyon, M. Lindauer, M. van der Schaar, F. Hutter, và R. Garnett, Eds., vol. 188. PMLR, 2022, pp. 19/1–18.

--- TRANG 11 ---
11
[40] J. Zhou, L. Tian, H. Yu, Z. Xiao, H. Su, và J. Zhou, "Dual
context-guided continuous prompt tuning for few-shot learning,"
trong Findings of the Association for Computational Linguistics: ACL
2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, và
A. Villavicencio, Eds. Association for Computational Linguistics,
2022, pp. 79–84.
[41] G. Cui, S. Hu, N. Ding, L. Huang, và Z. Liu, "Prototypical
verbalizer for prompt-based few-shot tuning," trong Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022,
S. Muresan, P. Nakov, và A. Villavicencio, Eds. Association for
Computational Linguistics, 2022, pp. 7014–7024.
[42] C. Wah, S. Branson, P. Welinder, P. Perona, và S. Belongie, "The
caltech-ucsd birds-200-2011 dataset," california institute of technology, 2011.
[43] G. V. Horn, S. Branson, R. Farrell, S. Haber, và S. Belongie,
"Building a bird recognition app and large scale dataset with
citizen scientists: The fine print in fine-grained dataset collection,"
trong Computer Vision & Pattern Recognition, 2015.
[44] Nilsback, ME, và Zisserman, "Automated flower classification
over a large number of classes," -, vol. -, no. -, pp. 722–729, 2008.
[45] A. Khosla, N. Jayadevaprakash, B. Yao, và F. Li, "L.: Novel
dataset for fine-grained image categorization," 2013.
[46] T. Gebru, J. Krause, Y. Wang, D. Chen, và F. F. Li, "Fine-grained
car detection for visual census estimation," 2017.
[47] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme,
M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, và A. Dosovitskiy, "A large-scale study of representation learning with the
visual task adaptation benchmark," 2019.
[48] A. Krizhevsky, "Learning multiple layers of features from tiny
images," 2012.
[49] F. F. Li, Member, IEEE, R. Fergus, và S. Member, "One-shot
learning of object categories," IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 28, no. 4, pp. 594–611, 2006.
[50] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, và A. Vedaldi,
"Describing textures in the wild," 2013.
[51] M. E. Nilsback và A. Zisserman, "Automated flower classification over a large number of classes," trong Sixth Indian Conference
on Computer Vision, Graphics & Image Processing, ICVGIP 2008,
Bhubaneswar, India, 16-19 December 2008, 2008.
[52] O. M. Parkhi, A. Vedaldi, A. Zisserman, và C. V. Jawahar, "[ieee
2012 ieee conference on computer vision and pattern recognition
(cvpr) - providence, ri (2012.06.16-2012.06.21)] 2012 ieee conference
on computer vision and pattern recognition - cats and dogs," pp.
3498–3505, 2012.
[53] Y. Netzer, T. Wang, A. Coates, A. Bissacco, và A. Y. Ng, "Reading
digits in natural images with unsupervised feature learning,"
2011.
[54] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, và A. Torralba, "Sun
database: Large-scale scene recognition from abbey to zoo," trong
Computer Vision & Pattern Recognition, 2010.
[55] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, và M. Welling,
"Rotation equivariant cnns for digital pathology," trong International
Conference on Medical image computing and computer-assisted intervention. Springer, 2018, pp. 210–218.
[56] P. Helber, B. Bischke, A. Dengel, và D. Borth, "Eurosat: A novel
dataset and deep learning benchmark for land use and land cover
classification," 2017.
[57] C. Gong, J. Han, và X. Lu, "Remote sensing image scene classification: Benchmark and state of the art," Proceedings of the IEEE,
vol. 105, no. 10, pp. 1865–1883, 2017.
[58] R. Akhunzyanov và S. Ovcharenko, "Diabetic retinopathy detection," 2016.
[59] J. Johnson, B. Hariharan, L. Maaten, F. F. Li, và R. Girshick,
"Clevr: A diagnostic dataset for compositional language and elementary visual reasoning," IEEE, 2017.
[60] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright,
H. Küttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik et al.,
"Deepmind lab," arXiv preprint arXiv:1612.03801, 2016.
[61] A. Geiger, P. Lenz, C. Stiller, và R. Urtasun, "Vision meets
robotics: The kitti dataset," International Journal of Robotics Research,
vol. 32, no. 11, pp. 1231–1237, 2013.
[62] L. Matthey, I. Higgins, D. Hassabis, và A. Lerchner, "dsprites:
Disentanglement testing sprites dataset," 2017.
[63] Y. Lecun, J. H. Fu, và L. Bottou, "Learning methods for generic
object recognition with invariance to pose and lighting," trong Proceedings of the 2004 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, 2004. CVPR 2004, 2004.
