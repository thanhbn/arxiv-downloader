# 2306.01485.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2306.01485.pdf
# File size: 1009722 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ROBUST LOW -RANK TRAINING VIA APPROXIMATE
ORTHONORMAL CONSTRAINTS
Dayana Savostianova
Gran Sasso Science Institute
67100 L’Aquila (Italy)
dayana.savostianova@gssi.itEmanuele Zangrando
Gran Sasso Science Institute
67100 L’Aquila (Italy)
emanuele.zangrando@gssi.it
Gianluca Ceruti
EPF Lausanne
1015 Lausanne (Switzerland)
gianluca.ceruti@epfl.chFrancesco Tudisco
Gran Sasso Science Institute
67100 L’Aquila (Italy)
francesco.tudisco@gssi.it
ABSTRACT
With the growth of model and data sizes, a broad effort has been made to design pruning
techniques that reduce the resource demand of deep learning pipelines, while retaining
model performance. In order to reduce both inference and training costs, a prominent line
of work uses low-rank matrix factorizations to represent the network weights. Although
able to retain accuracy, we observe that low-rank methods tend to compromise model
robustness against adversarial perturbations. By modeling robustness in terms of the
condition number of the neural network, we argue that this loss of robustness is due to the
exploding singular values of the low-rank weight matrices. Thus, we introduce a robust
low-rank training algorithm that maintains the network’s weights on the low-rank matrix
manifold while simultaneously enforcing approximate orthonormal constraints. The re-
sulting model reduces both training and inference costs while ensuring well-conditioning
and thus better adversarial robustness, without compromising model accuracy. This is
shown by extensive numerical evidence and by our main approximation theorem that
shows the computed robust low-rank network well-approximates the ideal full model,
provided a highly performing low-rank sub-network exists.
1 Introduction
Deep learning and neural networks have achieved great success in a variety of applications in computer
vision, signal processing, and scientific computing, to name a few. However, their robustness with respect
to perturbations of the input data may considerably impact security and trustworthiness and poses a major
drawback to their real-world application. Moreover, the memory and computational requirements for both
training and inferring phases render them impractical in application settings with limited resources. While
a broad literature on pruning methods and adversarial robustness has been developed to address these
two issues in isolation, much less has been done to design neural networks that are both energy-saving
and robust. Actually, in many approaches the two problems seem to compete against each other as
most adversarial robustness improving-techniques require even larger networks [ 43,73,32,37,42] or
computationally more demanding loss functions, and thus more expensive training phases [ 12,21,40,62].
The limited work available so far on robust pruned networks is mostly focused on reducing memory and
computational costs of the inference phase, while retaining adversarial robustness [ 52,26,71,38,18].
However, the inference phase amounts to only a very limited fraction of the cost of the whole deep
learning pipeline, which is instead largely dominated by the training phase. Reducing both inferencearXiv:2306.01485v1  [cs.LG]  2 Jun 2023

--- PAGE 2 ---
and training costs is a challenging but desirable goal, especially in view of a more accessible AI and its
effective use on limited-resource and limited-connectivity devices such as drones or satellites.
Some of the most effective techniques for the reduction of training costs so far have been based on
low-rank weights parametrizations [ 51,67,27]. These methods exploit the intrinsic low-rank structure
of parameter matrices and large data matrices in general [ 55,65,45,16]. Thus, assuming a low-rank
structure for the neural network’s weights W=USV⊤, the resulting training procedures only use the
small individual factors U, S, V . This results in a training cost that scales linearly with the number of
neurons, as opposed to a quadratic scaling required by training full-rank weights. Despite significantly
reducing training parameters, these methods achieve accuracy comparable with the original full networks.
However, their robustness with respect to adversarial perturbations has been largely unexplored so far.
In this paper, we observe that the adversarial robustness of low-rank networks may actually deteriorate with
respect to the full baseline. By modeling the robustness of the network in terms of the neural network’s
condition number, we argue that this loss of robustness is due to the exploding condition number of the
low-rank weight matrices, whose singular values grow very large in order to match the baseline accuracy
and to compensate for the lack of parameters. Thus, to mitigate this growing instability, we design an
algorithm that trains the network using only the low-rank factors U, S, V while simultaneously ensuring
the condition number of the network remains small. To this end, we interpret the loss optimization problem
as a continuous-time gradient flow and use techniques from geometric integration theory on manifolds
[66,29,51,11] to derive three separate projected gradient flows for U, S, V , individually, which ensure
the condition number of the network remains bounded to a desired tolerance 1 +τ, throughout the epochs.
For a fixed small constant ε >0, this is done by bounding the singular values of the small rank matrices
within a narrow band [s−ε, s+ε]around a value s, chosen to best approximate the original singular
values.
We provide several experimental evaluations on different architectures and datasets, where the robust
low-rank networks are compared against a variety of baselines. The results show that the proposed
technique allows us to compute from scratch low-rank weights with bounded singular values, significantly
reducing the memory demand and computational cost of training while at the same time retaining or
improving both the accuracy and the robust accuracy of the original model. On top of the experimental
evidence, we provide a key approximation theorem that shows that if a high-performing low-rank network
with bounded singular values exists, then our algorithm computes it up to a first-order approximation
error.
This paper focuses on feed-forward neural networks. However, our techniques and analysis apply
straightforwardly to convolutional filters reshaped in matrix form, as done in e.g. [ 51,67,27]. Other
ways exist to promote orthogonality of convolutional filters, e.g. [ 56,61,72], which we do not consider
in this work.
2 Related work
Neural networks’ robustness against adversarial perturbations has been extensively studied in the machine
learning community. It is well-known that the adversarial robustness of a neural network is closely related
to its Lipschitz continuity [ 58,12,64,21], see also Section 3. Accordingly, training neural networks with
bounded Lipschitz constant is a widely employed strategy to address the problem. A variety of works
studied Lipschitz architectures [ 64,61,33,56], and a number of certified robustness guarantees have been
proposed [ 21,57,47]. While scaling each layer to impose 1-Lipschitz constraints is a possibility, this
approach may lead to vanishing gradients and it is known that a more effective way to reduce the Lipschitz
constant and increase robustness is obtained by promoting orthogonality on each layer [ 5,12]. On top of
robustness, small Lipschitz constants and orthogonal layers are known to lead to improved generalization
bounds [ 10,41] and more interpretable gradients [ 63]. Orthogonality was also shown to improve signal
propagation in (very) deep networks [69, 48].
A variety of methods to integrate orthogonal constraints in deep neural networks have been developed
over the years. Notable example approaches include methods based on regularization and landing [ 1,12],
cheap parametrizations of the orthogonal group [ 35,6,34,44,46], Riemannian and projected gradient
descent schemes [9, 3, 2].
2

--- PAGE 3 ---
In parallel to the development of methods to promote orthogonality, an active line of research has grown
to develop effective training strategies to enforce low-rank weights. Unlike sparsity-promoting pruning
strategies that primarily aim at reducing the parameters required for inference [ 20,8,19,39,28], low-
rank neural network models are designed to train directly on the low-parametric manifold of low-rank
matrices and are particularly effective to reduce the number of parameters required by both inference and
training phases. Similar to orthogonal training, methods for low-rank training include methods based on
regularization [ 24,27], as well as methods based on efficient parametrizations of the low-rank manifold
using the SVD or the polar decomposition [ 67,70], and Riemannian optimization-based training models
[51, 53].
By combining low-rank training with approximate orthogonal constraints, in this work we propose
a strategy that simultaneously enforces robustness while only requiring a reduced percentage of the
network’s parameters during training. The method is based on a gradient flow differential formulation of
the training problem, and the use of geometric integration theory to derive the governing equations of
the low-rank factors. With this formulation, we are able to reduce the sensitivity of the network during
training at almost no cost, yielding well-conditioned low-rank neural networks. Our experimental findings
are supported by an approximation theorem that shows that, if the ideal full network can be approximated
by a low-rank one, then our method computes a good approximation. This is well-aligned with recent
work that shows the existence of high-performing low-rank nets in e.g. deep linear models [ 45,16,7,23].
Moreover, as orthogonality helps in training really deep networks, low-rank orthogonal models may be
used to mitigate the effect of increased effective depth when training low-rank networks [50].
3 The condition number of a neural network
The adversarial robustness of a neural network model fcan be measured by the worst-case sensitivity
offwith respect to small perturbations of the input data x. In an absolute sense, this boils down to
measuring the best global and local Lipschitz constant of fwith respect to suitable distances, as discussed
in a variety of papers [ 58,21,14,12]. However, as the model and the data may assume arbitrary large and
arbitrary small values in general, a relative measure of the sensitivity of fmay be more informative. In
other words, if we assume a perturbation δof small size as compared to x, we would like to quantify the
largest relative change in f(x+δ), as compared to f(x). This is a well-known problem of conditioning,
as we review next, and naturally leads to the concept of condition number of a neural network.
In the linear setting, the condition number of a matrix is a widely adopted relative measure of the worst-
case sensitivity of linear problems with respect to noise in the data. For a matrix Aand the matrix operator
norm∥A∥= supx̸=0∥Ax∥/∥x∥, the condition number of Ais defined as cond( A) =∥A∥∥A+∥, where
A+denotes the pseudo-inverse of A. Note that it is immediate to verify that cond( A)≥1. Now, if for
example uanduεare the solutions to the linear system Au=b, when Aandbare exact data or when they
are perturbed with noise δA,δbof relative norm ∥δA∥/∥A∥ ≤εand∥δb∥/∥b∥ ≤ε, respectively, then the
following relative error bound holds
∥u−uε∥
∥u∥≲cond( A)ε .
Thus, small perturbations in the data A, bimply small alterations in the solution if and only if Ais well
conditioned, i.e. cond( A)is close to one.
As in the linear case, it is possible to define the concept of condition number for general functions f,
[22, 49]. Let us start by defining the relative error ratio of a function f:Rd→Rmin the point x:
R(f, x;δ) =∥f(x+δ)−f(x)∥
∥f(x)∥.∥δ∥
∥x∥(1)
In order to take into account the worst-case scenario, the local condition number of fatxis defined by
taking the sup of (1)over all perturbations of relative size ε, i.e. such that ∥δ∥ ≤ε∥x∥, in the limit of
small ε. Namely,
cond( f;x) = lim
ε↓0sup
δ̸=0:∥δ∥≤ε∥x∥R(f, x;δ).
3

--- PAGE 4 ---
This quantity is a local measure of the “infinitesimal” conditioning of faround the point x. In fact, a
direct computation reveals that
∥f(x+δ)−f(x)∥
∥f(x)∥≲cond( f;x)ε , (2)
as long as ∥δ∥ ≤ε∥x∥. Thus, cond( f;x)provides a form of relative local Lipschitz constant for fwhich
in particular shows that, if ∥δ∥/∥x∥is smaller than cond( f;x)−1, we expect limited change in fwhen x
is perturbed with δ. A similar conclusion is obtained using an absolute local Lipschitz constant in e.g.
[21]. Similarly to the absolute case, a global relative Lipschitz constant can be obtained by looking at the
worst-case over x, setting
cond( f) = sup
x∈Xcond( f;x).
Clearly, the same bound (2)holds for cond( f). Note that this effectively generalizes the linear case, as
when f(x) =Axwe have cond( f) = cond( f, x) = cond( A).
When fis a neural network, cond( f)is a function of the network’s weights and robustness may be
enforced by reducing cond( f)while training. In fact, cond( f)is the relative equivalent of the network’s
Lipschitz constant and thus standard Lipschitz-based robustness certificates [ 36,64,21] can be recast
in terms of cond( f). However, for general functions fand general norms ∥ · ∥,cond( f)may be (very)
expensive to compute, it may be non-differentiable, and cond( f)>1can hold [ 22]. Fortunately, for feed-
forward neural networks, it holds (proof and additional details moved to Appendix B in the supplementary
material)
Proposition 1. LetXbe the feature space and let f(x) =zL+1be a network with Llinear layers
zi+1=σi(Wizi),i= 1, . . . , L . Then,
cond( f) = sup
x∈X\{ 0}cond( f;x)≤LY
i=1sup
x∈X\{ 0}cond( σi;x) LY
i=1cond( Wi)
.
In particular, for typical Xand typical choices of σi, including σi∈ {leakyReLU ,sigmoid ,tanh ,
hardtanh ,softplus ,siLU}, we have
supx∈X\{ 0}cond( σi;x)≤C <+∞
for a positive constant C >0that depends only on the activation function σi.
Note that for entrywise nonlinearities σ, the condition number cond( σ;x)can be computed straightfor-
wardly. In fact, when σis Lipschitz, the problem can be reduced to a one-dimensional function, and it
follows directly from its definition that (see also [60])
cond( f;x) = sup
νx∈∂σ(x)|νx||x||σ(x)|−1, x ∈R
where ∂σ(x)denotes Clarke’s generalized gradient [ 13] ofσat the point x. Thus, for example, if σis
LeakyRelu with slope α, we have cond( σ) = 1 ; ifσis the logistic sigmoid (1 +e−x)−1and the feature
spaceXcontains only nonnegative points, then cond( σ)≤supx≥0|x|e−x(1 +e−x)−1≤1/e.
From Proposition 1 we see that when fis a feed-forward network, to reduce the condition number of
fit is enough to reduce the conditioning of all its weights. When ∥ · ∥ =∥ · ∥ 2is the Euclidean L2
norm, we have cond 2(W) =smax(W)/smin(W), the ratio between the largest and the smallest singular
value of W. This implies that orthogonal weight matrices, for example, are optimally conditioned with
respect to the L2metric. Thus, a notable and well-known consequence of Proposition 1 is that imposing
orthogonality constraints on Wimproves the robustness of the network [12, 34, 25, 46].
While orthogonal constraints are widely studied in the literature, orthogonal matrices are not the only
optimally conditioned ones. In fact, cond 2(W) = 1 for any Wwith constant singular values. In
the next section, we will use this observation to design a low-rank and low-cost algorithm that trains
well-conditioned networks by ensuring cond 2(W)≤1+τ, for all layers Wand a desired tolerance τ >0.
4

--- PAGE 5 ---
02000 4000 6000 8000
# optimization step101layers condition numbersBaseline
02000 4000 6000 8000
# optimization step100101102103layers condition numbersLow-rank gradient flow
02000 4000 6000 8000
# optimization step100101102103104105106107layers condition numbersDLRT
0 200 400 600
# optimization step101102103104105106107layers condition numbersLayer decompositionFigure 1: Evolution of layers’ condition numbers during training for LeNet5 on MNIST. From left to
right: standard full-rank baseline model; [ 67] vanilla low-rank training; [ 51] dynamical low-rank training
based on gradient flow; [ 70] low-rank training through regularization. All low-rank training strategies are
set to 80% compression ratio (percentage of removed parameters with respect to the full baseline model).
4 Robust low-rank training
4.1 Instability of low-rank networks
Low-rank methods are popular strategies to reduce the memory storage and the computational cost of
both training and inference phases of deep learning models [ 51,67,27]. Leveraging the intrinsic low-rank
structure of parameter matrices [ 7,55,45,16], these methods train a subnetwork with weight matrices
parametrized as W=USV⊤, for “tall and skinny” matrices U, V withrcolumns, and a small r×r
matrix S. Training low-rank weight matrices has proven to effectively reduce training parameters while
retaining performance comparable to those of the full model. However, while a variety of contributions
have analyzed and refined low-rank methods to match the full model’s accuracy, the robust accuracy of
low-rank models has been largely overlooked in the literature.
Here we observe that reducing the rank of the layer may actually deteriorate the network’s robustness. We
argue that this phenomenon is imputable to the exploding condition number of the network. In Figure 1
we plot the evolution of the condition number cond 2for the four internal layers of LeNet5 during training
using different low-rank training strategies and compare them with the full model. While the condition
number of the full model grows moderately with the iteration count, the condition number of low-rank
layers blows up drastically. This singular value instability leads to poor robustness performance of the
methods, as observed in the experimental evaluation of Section 5.
In the following, we design a low-rank training model that allows imposing simple yet effective training
constraints, bounding the condition number of the trained network to a desired tolerance 1 +τ, and
improving the network robustness without affecting training nor inference costs.
4.2 Low-rank gradient flow with bounded singular values
LetW∈Rn×mbe the weight matrix of a linear layer within f. For an integer r≤min{m, n}let
Mr={W: rank( W) =r}be the manifold of rank- rmatrices which we parametrize as
Mr=
USV⊤:U∈Rn×r, V∈Rm×rwith orthonormal columns , S∈Rr×rinvertible	
.
Obviously, the singular values of W=USV⊤∈ M rcoincide the singular values of S. For s, εsuch
that0< ε < s , define Σs(ε)as the set of matrices with singular values in the interval [s−ε, s+ε].
Note that Σs(0)is a Riemannian manifold obtained essentially by an sscaling of the standard Stiefel
manifold and any A∈Σs(0)is optimally conditioned, i.e. cond 2(A) = 1 . Thus, εcan be interpreted
as an approximation parameter that controls how close Σs(ε)is to the “optimal” manifold Σs(0). To
enhance the network robustness, in the following we will constrain the parameter weight matrix Sto
Σs(ε). With this constraint, we get cond 2(W)≤(s−ε)−1(s+ε) = 1 + τ, with τ= 2(s−ε)−1ε, so
that the tolerance τon the network’s conditioning can be tuned by suitably choosing the approximation
parameter ε.
Given the loss function L, we are interested in the constrained optimization problem
minL s.t. W=USV⊤∈ M randS∈Σs(ε),for all layers W . (3)
5

--- PAGE 6 ---
To approach (3), we use standard arguments from geometric integration theory [ 66,29] to design a training
scheme that updates only the factors U, S, V and the gradient of Lwith respect to U, S, V , without ever
forming the full weights nor the full gradients. To this end, following [ 51], we first recast the optimization
ofLwith respect to each layer Was a continuous-time gradient flow
˙W(t) =−∇WL(W(t)), (4)
where “dot” denotes the time derivative and where we write Las a function of Wonly, for brevity. Along
the solution of the differential equation above, the loss decreases and a stationary point is approached
ast→ ∞ . Now, if we assume W∈ M r, then ˙W∈TWMr, the tangent space of Mrat the point
W. Thus, to ensure the whole trajectory W(t)∈ M r, we can consider the projected gradient flow
˙W(t) =−PW(t)∇WL(W(t)), where PWdenotes the orthogonal projection (in the ambient space
of matrices) onto TWMr. Next, we notice that the projection PW∇WLcan be defined by imposing
orthogonality with respect to any point Y∈TWMr, namely
⟨PW∇WL − ∇ WL, Y⟩= 0 for all Y∈TWMr
where ⟨·,·⟩is the Frobenius inner product. As discussed in e.g. [ 29,51], the above equations combined
with the well-known representation of TWMryield a system of three gradient flow equations for the
individual factors 

˙U=−G1(U), G 1(U) =P⊥
U∇UL(USV⊤)(SS⊤)−1
˙V=−G2(V), G 2(V) =P⊥
V∇VL(USV⊤)(S⊤S)−⊤
˙S=−G3(S), G 3(S) =∇SL(USV⊤)(5)
where P⊥
U= (I−UU⊤)andP⊥
V= (I−V V⊤)are the projection operators onto the space orthogonal
to the span of UandV, respectively.
Based on the system of gradient flows above, we propose a training scheme that at each iteration and for
each layer parametrized by the tuple {U, S, V }proceeds as follows:
1. update UandVby numerically integrating the gradient flows ˙U=−G1(U)and˙V=−G2(V)
2. project the resulting U, V onto the Stiefel manifold of matrices with rorthonormal columns
3. update the r×rweight Sby integrating ˙S=−G3(S)
4. for a fixed robustness tolerance τ, project the computed SontoΣs(ε), choosing sandεso that
•sis the best constant approximation to S⊤S, i.e.s= argminα∥S⊤S−α2I∥F
•εis such that the cond 2of the projection of Sdoes not exceed 1 +τ
Note that the coefficients s,εat point 4can be obtained explicitly by setting s=qPr
j=1sj(S)2/r, the
second moment of the singular values sj(S)ofS, and ε=τs/(2 +τ). Note also that, in the differential
equations for U, S, V in(5), the four steps above can be implemented in parallel for each of the three
variables. The detailed pseudocode of the training scheme is presented in Algorithm 1. We conclude with
several remarks about its implementation.
Remarks, implementation details, and limitations
Each step of Algorithm 1 requires three optimization steps at lines 2, 4, 6. These steps can be implemented
using standard first-order optimizers such as SGD with momentum or ADAM. Standard techniques can be
used to project onto the Stiefel manifold at lines 3 and 5 of Algorithm 1, see e.g. [ 4,66]. Here, we use the
QR decomposition. As for the projection onto Σs(ε)at line 9, we compute the SVD of the small factor S
and set to s+εors−εthe singular values that fall outside the interval [s−ε, s+ε]. Note that, when
τ= 0, i.e. when we require perfect conditioning for the layer weight W=USV⊤, then the SVD of S
can be replaced by a QR step or any other Stiefel manifold projection. Indeed, we can equivalently set
s=p
trace( S⊤S)/r, and then project onto Σs(0)by rescaling by a factor sthe projection of Sonto the
Stiefel manifold. In this case, the system (5)further simplifies, as we can replace (SS⊤)−1and(S⊤S)−⊤
with the scalar 1/s2.
Overall, the compressed low-rank network has r(n+m+r)parameters per each layer, where nand
mare the number of input and output neurons. Thus, choosing rso that 1−r(n+m+r)/(nm) =α
6

--- PAGE 7 ---
Algorithm 1: Pseudocode of robust well-Conditioned Low-Rank ( CondLR ) training scheme
Input: Chosen compression rate, i.e. for each layer Wchoose a rank r;
Initial layers’ weights parametrized as W=USV⊤, with S∼r×r;
Second singular value moment of S,s=pP
ksk(S)2/r
Conditioning tolerance τ >0
1foreach iteration and each layer do(each block in parallel)
2 U←one optimization step with gradient G1and initial point U
3 U←project Uon Stiefel manifold with rorthonormal columns
4 V←one optimization step with gradient G2and initial point V
5 V←project Von Stiefel manifold with rorthonormal columns
6 S←one optimization step with gradient G3and initial point S
7 s←pP
ksk(S)2/r, squareroot of second moment of the singular values of S
8 ε←τs/(2 +τ)
9 S←project SontoΣs(ε)
can yield a desired compression rate 0< α < 1on the number of network parameters, i.e. the number
of parameters one eliminates with respect to the full baseline. For example, in our experiments we will
choose rso that α= 0.5orα= 0.8.
Limitations. As the rank parameter rhas to be chosen a-priori for each layer of the network, a limitation
of the proposed approach is the potential need for fine-tuning such parameter, even though the proposed
analysis in Table 1 shows competitive performance for both 50% and80% compression rates. Also, if the
layer size n×mis not large enough, the compression ratio 1−r(n+m+r)/(nm)might be limited.
Thus the method works well only for wide-enough networks. Finally, a standard way to obtain better
adversarial performance would be to combine the proposed conditioning-based robustness with adversarial
training strategies [ 15,68,59]. However, the cost of producing adversarial examples during training is
not negligible, especially when based on multi-step attacks, and thus the way to incorporate adversarial
training without affecting the benefits obtained with low-rank compression is not straightforward.
4.3 Approximation guarantees
Optimization methods over the manifold of low-rank matrices are well-known to be affected by the
stiff intrinsic geometry of the constraint manifold which has very high curvature around points where
W∈ M ris almost singular [ 4,66,29]. This implies that even very small changes in Wmay yield very
different tangent spaces, and thus different training trajectories, as shown by the result below:
Lemma 1 (Curvature bound, Lemma 4.2 [ 29]).ForW∈ M rletsmin(W)>0be its smallest singular
value. For any W′∈ M rarbitrarily close to Wand any matrix B, it holds
∥PWB−PW′B∥F≤C smin(W)−1∥W−W′∥F,
where C >0depends only on B.
In our gradient flow terminology, this phenomenon is shown by the presence of the matrix inversion in (5).
While this is often an issue that may dramatically affect the performance of low-rank optimizers (see also
Section 5), the proposed regularization step that enforces bounded singular values allows us to move along
paths that avoid stiffness points. Using this observation, here we provide a bound on the quality of the
low-rank neural network computed via Algorithm 1, provided there exists an optimal trajectory leading to
an approximately low-rank network. We emphasize that this assumption is well-aligned with recent work
showing the existence of high-performing low-rank nets in e.g. deep linear models [45, 16, 7, 23].
Assume the training is performed via gradient descent with learning rate λ >0, and let W(t)be the full
gradient flow (4). Further, assume that for t∈[0, λ]and a given ε >0, for each layer there exists E(t)
andfW(t)∈ M r∩Σs(ε)such that
W(t) =fW(t) +E(t),
7

--- PAGE 8 ---
where sis the second moment of the singular values of W(t)andE(t)is a perturbation that has bounded
variation in time, namely ∥˙E(t)∥ ≤η. In other words, we assume there exists a training trajectory that
leads to an approximately low-rank weight matrix W(t)with almost constant singular values. Because
the value sis bounded by construction, the parameter-dependent matrix fW(t)possesses singular values
exhibiting moderate lower bound. Thus, W(t)is far from the stiffness region of (5)and we obtain the
following bound, based on [29] (proof moved to Appendix C in the supplementary material)
Theorem 1. LetUkSkV⊤
kbe a solution to (5)computed with ksteps of Algorithm 1. Assume that
•The low-rank initialization U0S0V⊤
0coincides with the low-rank approximation fW(0).
•The norm of the full gradient is bounded, i.e., ∥∇WL(W(t))∥ ≤µ.
•The learning rate is bounded as λ≤s−ε
4√2µη.
Then, assuming no numerical errors, the following error bound holds
∥UkSkV⊤
k−W(λk)∥ ≤3λη .
5 Experiments
We illustrate the performance of Algorithm 1 on a variety of test cases. All the experiments can be
reproduced with the code available in the supplementary material. In order to assess the combined
compression and robustness performance of the proposed method, we compare it against both full and
low-rank baselines.
For all models, we compute natural accuracy and robust accuracy. Let {(xi, yi)}i=1,...,nbe the set of test
images and the corresponding labels and let fbe the neural network model, with output f(x)on the input
x. We quantify the test set robust accuracy as:
robust _acc(δ) =1
nPn
i=1 1{yi}(f(xi+δi))
where δ= (δi)i=1,...,n are the adversarial perturbation associated to each sample. Notice that, in the
unperturbed case with ∥δi∥= 0, the definition of robust accuracy exactly coincides with the definition of
test accuracy. In our experiments, adversarial perturbations are produced by the fast gradient sign method
[17], thus they are of the form δi=ϵsign(∇xL(f(xi), yi)), where ϵcontrols perturbation strength, since
∥δi∥∞=ϵ. As images in our set-up have input entries in [0,1], the perturbed input is then clamped to
that interval. Note that, for the same reason, the value of ϵcontrols in our case the relative size of the
perturbation.
Datasets. We consider MNIST and CIFAR10 [ 30] datasets for evaluation purposes. The first contains
60,000 training images and the last one contains 50,000 training images. All the datasets have 10,000 test
images and 10 classes. No data-augmentation is performed.
Models. We use LeNet5 [ 31] for MNIST dataset and VGG16 [ 54] for the CIFAR10. The general
architecture for all the used networks is preserved across the models, while the weight-storing structures
and optimization frameworks differ.
Methods. Our baseline network is the one done with the standard implementation. Cayley SGD [ 35]
and Projected SGD [ 3] are Riemannian optimization-based methods that train the network weights over
the Stiefel manifold of matrices with orthonormal columns. Thus, both methods ensure cond 2(W) = 1
for all layers. The former uses an iterative estimation of the Cayley transform, while the latter uses
QR-based projection to retract the Riemannian gradient onto the Stiefel manifold. Both methods have
no compression and use full weight matrices. DLRT, SVD prune, and Vanilla are low-rank methods
that ensure compression of the model parameters during training. DLRT [ 51] is based on a low-rank
gradient flow model similar to the proposed Algorithm 1. SVD prune [ 70] is based on a regularized
loss with a low-rank-promoting penalty term. This approach was designed to compress the ranks of the
network after training, but we impose a fixed compression rate from the beginning for a fair comparison.
“Vanilla” denotes the obvious low-rank approach, that parametrizes the layers as W=UV⊤and performs
alternate descent steps over UandV, as done in e.g. [ 67,27]. All models are implemented with fixed
training compression ratios α= 0.5andα= 0.8, i.e. we only use 50% and20% of the parameters the
full model would use during training, respectively. Finally, we implement CondLR as in Algorithm 1 for
three choices of the conditioning tolerance τ∈ {0,0.1,0.5}. We also implement a modified version of
8

--- PAGE 9 ---
Table 1: Method comparison results
LeNet5 MNIST VGG16 Cifar10 c.r.
(%)Rel. perturbation ϵ 0.0 0.02 0.04 0.06 0.0 0.002 0.004 0.006
Baseline 0.9908 0.9815 0.9647 0.9328 0.9087 0.7515 0.5847 0.4640 0
Cayley SGD 0.9872 0.9812 0.9695 0.9497 0.8965 0.7435 0.5802 0.4508 0
Projected SGD 0.9665 0.9047 0.7908 0.6260 0.8584 0.6967 0.5402 0.4124 0CondLRτ= 0 0.9884 0.9819 0.9717 0.9582 0.9131 0.7457 0.5611 0.4157 50
τ= 0.1 0.9873 0.9825 0.9759 0.9668 0.9117 0.7539 0.6201 0.5212 50
τ= 0.5 0.9865 0.9791 0.9726 0.9612 0.8999 0.7262 0.5940 0.487 50
mean 0.9606 0.9414 0.9156 0.8773 0.8793 0.6846 0.4861 0.3459 50
DLRT 0.9907 0.981 0.9608 0.9245 0.8367 0.6079 0.4342 0.3233 50
Vanilla 0.987 0.9748 0.9488 0.9094 0.8995 0.6801 0.4917 0.3856 50
SVD prune 0.9876 0.9718 0.9458 0.8966 0.8981 0.6750 0.4753 0.3737 50CondLRτ= 0 0.9881 0.9802 0.9694 0.9505 0.9054 0.7476 0.5718 0.4237 80
τ= 0.1 0.9877 0.9794 0.9673 0.9477 0.9063 0.7079 0.5115 0.3760 80
τ= 0.5 0.9858 0.9754 0.9585 0.9288 0.8884 0.6936 0.5082 0.3787 80
mean 0.9816 0.9730 0.9597 0.9395 0.8698 0.6627 0.4659 0.3213 80
DLRT 0.9888 0.9747 0.9503 0.9067 0.8438 0.6178 0.4184 0.2883 80
Vanilla 0.9839 0.9660 0.9314 0.8687 0.8806 0.6540 0.4505 0.3234 80
SVD prune 0.9847 0.9650 0.9261 0.8513 0.8845 0.6355 0.4195 0.3068 80
Algorithm 1 in which Sis directly projected onto the Stiefel manifold Σ1(0), i.e. the parameter sis fixed
to one.
Training. Each method and model was trained for 120 epochs of stochastic gradient descent with a
minibatch size of 128. We used a learning rate of 0.1 for LeNet5 and 0.05 for VGG16 with momentum
0.3 and 0.45, respectively, and a learning rate scheduler with factor = 0.4 at 70 and 100 epochs.
Results. For comparison, we measure robust accuracy for all chosen combinations of datasets, models,
and methods with relative perturbation budget ϵ∈ {0,0.01,0.02,0.03,0.04,0.05,0.06}for MNIST and
ϵ∈ {0,0.001,0.002,0.003,0.004,0.005,0.006}for CIFAR10. The results are summarized in Table 1
for MNIST and CIFAR10 and a subset of the perturbation budget. The complete set of results is shown
in Tables 2–3 in the supplementary material. In the tables, we highlight (in gray) the best-performing
method for each range of compression rates α∈ {0%,50%,80%}, and the best method overall (in bold).
The experiments show that the proposed method meaningfully conserves the accuracy as compared to
baseline for all datasets; moreover, the robustness is improved from baseline for 50% compression rate
and additionally in case of MNIST for 80% compression rate. Compared to orthogonal non-compressed
methods, CondLR with 50% compression rate also outperforms Cayley SGD and Projected SGD.
At the same time CondLR is compressed by design, so we reduce memory costs alongside gaining
robustness. As anticipated in Section 4.1, low-rank methods without regularization deteriorates the
condition number and thus the robustness of the model. This is consistent with the results of Table 1,
where we observe that compression without regularization does not work well in terms of robust accuracy,
in particular DLRT, Vanilla and SVD prune exhibit a drop in the performance in comparison with baseline
and, consequently, CondLR . Specifically, per each fixed compression ratio αour method exhibits the
highest robustness; moreover, the robustness of CondLR with α= 0.8is higher than the robustness of
any other method with an even lower compression ratio, α= 0.5.
As a further experimental evaluation, we analyze the robustness of low-rank training models with respect
to small singular values. As shown by Theorem 1, CondLR is not affected by the steep curvature of
the low-rank manifold Mr, i.e. accuracy and convergence rate of CondLR do not deteriorate when the
conditioning of the weight matrices explodes. In contrast, small singular values may significantly affect
9

--- PAGE 10 ---
0200 400 600 800 1000 1200 1400
Optimization step0.00000.00250.00500.00750.01000.01250.01500.01750.0200Loss
0200 400 600 800 1000 1200 1400
Optimization step0.00.20.40.60.81.0Accuracy
0200 400 600 800 1000 1200 1400
Optimization step103108101310181023102810331038Upper bound cond(f)CondLR,=0
DLRT
SVD Prune
VanillaFigure 2: Evolution of loss, accuracy, andQ
icond( Wi)for Lenet5 on MNIST dataset, for ill-conditioned
initial layers whose singular values are forced to decay exponentially with powers of two.
alternative low-rank training models. In Figure 2 we show the behavior of loss, accuracy, and condition
number as functions of the iteration steps, when the network is initialized with ill-conditioned layer
matrices. Precisely, similar to what is done in [ 27], we randomly sample Gaussian initial weights, we
compute their SVD to define the initial UandVfactors, and then force the singular values of the initial S
factor to decay exponentially. We observe that CondLR and DLRT (which is also based on a low-rank
gradient flow formulation) are the most robust, while the performance of the other methods deteriorates
dramatically. This confirms that, as also observed in [ 27,51], most low-rank approaches require specific
fine-tuned choices of the initialization, while the proposed robust low-rank training model ensures solid
performance independent of the initialization.
A Additional results
In this section, we report the results on natural and robust accuracy for the broader range of
perturbation budgets ϵ∈ {0,0.01,0.02,0.03,0.04,0.05,0.06}for LeNet5 on MNIST, and ϵ∈
{0,0.001,0.002,0.003,0.004,0.005,0.006}for VGG16 on CIFAR10. See Table 2 and Table 3. The
results confirm the same findings as the ones reported in the previous sections.
Table 2: LeNet5 MNIST
Rel. perturbation ϵ0.0 0.01 0.02 0.03 0.04 0.05 0.06 cr (%)
Baseline 0.9908 0.9866 0.9815 0.9751 0.9647 0.9512 0.9328 0
Cayley SGD 0.9872 0.9844 0.9812 0.9767 0.9695 0.9600 0.9497 0
Projected SGD 0.9665 0.9409 0.9047 0.8520 0.7908 0.7111 0.6260 0CondLRτ= 0.0 0.9884 0.9850 0.9819 0.9772 0.9717 0.9665 0.9582 50
τ= 0.1 0.9873 0.9848 0.9825 0.9795 0.9759 0.9719 0.9668 50
τ= 0.5 0.9865 0.9827 0.9791 0.9764 0.9726 0.9682 0.9612 50
mean 0.9868 0.9837 0.9809 0.9781 0.9728 0.9681 0.9614 50
DLRT 0.9907 0.9863 0.9810 0.9716 0.9608 0.9454 0.9245 50
vanilla 0.9870 0.9816 0.9748 0.9621 0.9488 0.9307 0.9094 50
SVD prune 0.9876 0.9803 0.9718 0.9625 0.9458 0.9235 0.8966 50CondLRτ= 0.0 0.9881 0.9842 0.9802 0.9752 0.9694 0.9615 0.9505 80
τ= 0.1 0.9877 0.9839 0.9794 0.9739 0.9673 0.9581 0.9477 80
τ= 0.5 0.9858 0.9812 0.9754 0.9683 0.9585 0.9459 0.9288 80
mean 0.9816 0.9770 0.9730 0.9676 0.9597 0.9491 0.9395 80
DLRT 0.9888 0.9836 0.9747 0.9646 0.9503 0.9317 0.9067 80
vanilla 0.9839 0.9756 0.9660 0.9506 0.9314 0.9033 0.8687 80
SVD prune 0.9847 0.9760 0.9650 0.9477 0.9261 0.8937 0.8513 80
10

--- PAGE 11 ---
B Proof of Proposition 1
Lemma 2. Letϕ: (Y,∥ · ∥ Y)→(Z,∥ · ∥ Z)andψ: (X,∥ · ∥ X)→(Y,∥ · ∥ Y)be continuous mappings
between finite-dimensional Banach spaces, and assume cond( ψ)andcond( ϕ)are well defined. Then the
following inequality holds:
cond( ϕ◦ψ)≤cond( ϕ) cond( ψ)
Proof. To prove this lemma, we will pass through the definition of R(ϕ◦ψ, x, δ ). Let us recall that it is
defined as
R(ϕ◦ψ, x, δ ) =∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥x∥X
∥δ∥X∥ϕ◦ψ(x)∥Z
Now, if ψ(x+δ)−ψ(x) = 0 forδ̸= 0, then R(ϕ◦ψ, x, δ ) = 0 . Since we’re interested in the supremum,
we can restrict to δsuch that ψ(x+δ)−ψ(x)̸= 0. Moreover, by multiplying and dividing by ∥ψ(x)∥Y
we obtain:
R(ϕ◦ψ, x, δ ) =∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥ψ(x)∥Y
∥ψ(x+δ)−ψ(x)∥Y∥ϕ◦ψ(x)∥Z∥ψ(x+δ)−ψ(x)∥Y∥x∥X
∥δ∥X∥ψ(x)∥Y
Now, if we define cond( f, x, ε ) = sup
δ̸=0:∥δ∥X≤εR(f, x, δ ), we take the supremum both sides on the set
{δ∈X|∥δ∥X≤ε}and we can upper bound it with the product of the suprema of the two blocks
sup
δ̸=0:∥δ∥X≤εR(ϕ◦ψ, x, δ ) = cond( ψ, x, ε ) sup
δ̸=0:∥δ∥X≤ε∥ϕ◦ψ(x+δ)−ϕ◦ψ(x)∥Z∥ψ(x)∥Y
∥ψ(x+δ)−ψ(x)∥Y∥ϕ◦ψ(x)∥Z
=⋆
Now, letting η=ψ(x+δ)−ψ(δ), we can rewrite the second part of the last equation as a sort of
restriction of cond( ϕ, ψ(x), ε)to the particular set of perturbation directions of the form η. Thus, we can
lower-bound it as:
⋆≤cond( ψ, x, ε ) sup
η̸=0:∥η∥Y≤ε∥ϕ(ψ(x) +η)−ϕ(ψ(x))∥Z∥ψ(x)∥Y
∥η∥Y∥ϕ(ψ(x))∥Z
=
= cond( ψ, x, ε ) cond( ϕ, ψ(x), ε)
Table 3: VGG16 Cifar10
Rel. perturbation ϵ0.0 0.001 0.002 0.003 0.004 0.005 0.006 cr (%)
Baseline 0.9087 0.8375 0.7515 0.6643 0.5847 0.5194 0.4640 0
Cayley SGD 0.8965 0.8278 0.7435 0.6612 0.5802 0.5087 0.4508 0
Projected SGD 0.8584 0.7810 0.6967 0.6169 0.5402 0.4743 0.4124 0CondLRτ= 0.0 0.9131 0.8371 0.7457 0.6515 0.5611 0.4848 0.4157 50
τ= 0.1 0.9117 0.8331 0.7539 0.6883 0.6201 0.5656 0.5212 50
τ= 0.5 0.8999 0.8180 0.7262 0.6550 0.5940 0.5397 0.4877 50
mean 0.8793 0.7851 0.6846 0.5729 0.4861 0.4116 0.3459 50
DLRT 0.8367 0.7163 0.6079 0.5130 0.4342 0.3702 0.3233 50
vanilla 0.8995 0.7991 0.6801 0.5721 0.4917 0.4299 0.3856 50
SVD prune 0.8981 0.7923 0.6750 0.5662 0.4753 0.4161 0.3737 50CondLRτ= 0.0 0.9054 0.8314 0.7476 0.6574 0.5718 0.4912 0.4237 80
τ= 0.1 0.9063 0.8134 0.7079 0.6070 0.5115 0.4330 0.3760 80
τ= 0.5 0.8884 0.7992 0.6936 0.5897 0.5082 0.4377 0.3787 80
mean 0.8698 0.7719 0.6627 0.5558 0.4659 0.3854 0.3213 80
DLRT 0.8438 0.7269 0.6178 0.5103 0.4184 0.3489 0.2883 80
vanilla 0.8806 0.7721 0.6540 0.5395 0.4505 0.3767 0.3234 80
SVD prune 0.8845 0.7646 0.6355 0.5143 0.4195 0.3552 0.3068 80
11

--- PAGE 12 ---
By hypothesis, cond( ϕ)andcond( ψ)are finite, so we can take the limit ε↓0to get:
cond( ϕ◦ψ, x)≤cond( ϕ, ψ(x)) cond( ψ, x)
Finally, taking the supremum over xon both members of the last equation, we can upper bound it with the
original condition number without constraint on the directions:
cond( ϕ◦ψ)≤sup
xcond( ϕ, ψ(x)) cond( ψ, x)≤cond( ϕ) cond( ψ)
and thus conclude.
Now, the proof of Proposition 1 follows directly by applying the Lemma above recursively.
Proof of Proposition1. By defining Ti(z) =Wiz, we can write the neural network fas
f(x) = (σL◦TL◦σL−1◦ ··· ◦ T1)(x)
for nonlinear activation functions σi. Thus, for L= 1, the thesis follows directly from Lemma2, as long as
cond( σ1)≤C <∞. ForL >1, one can unwrap the compositional structure of ffrom the left, defining
ϕ(z) = (σL◦TL)(z)andψ(x) = (σL−1◦ ··· ◦ T1)(x). Then by using Lemma2 we have that
cond( f)≤cond( ϕ) cond( ψ)≤cond( σL) cond( TL) cond( ψ).
Now, since ψis a network of depth L−1, we can proceed inductively to obtain that cond( f)≤
CQ
icond( Ti), with C= ΠL
i=1cond( σi), and we conclude.
Note that the result above is of interest only if cond( σ) = supx∈Xcond( σ;x)<∞. When σis Lipschitz,
using the formula
cond( f;x) = sup
νx∈∂σ(x)∥νx∥∥x∥∥σ(x)∥−1,
with∂being the Clarke’s generalized gradient operator [13], we observe below that this is the case for a
broad list of activation functions σand feature spaces X.
•LeakyReLU. Forx∈R,α >0, letσ(x) = max {0, x}+αmin{0, x}. Then any νx∈∂σ(x)is such
thatνx= 1ifx >0;νx=αifx <0;νx= [min( α,1),max( α,1)]otherwise. Thus
cond( σ) = sup
x̸=0cond( σ;x) = sup
x̸=0sup
β∈∂σ(x)|x||1x>0+α1x<0+β1x=0|
|max{0, x}+αmin{0, x}|= max( α,1)
•Tanh. Forx∈R, letσ(x) = tanh( x). Then σ′(x) =1
cosh2(x)and thus
cond( σ) = sup
x|x|
|tanh( x)||cosh2(x)|= sup
x|4x|
|ex−e−x||ex+e−x|= 1
Since the maximum of cond( σ, x)is reached at zero, where the function can be extended by continuity.
•Hardtanh. Forx∈[−a, a]anda >0, letσ(x) =a1x>a−a1x<−a+x1x∈[−a,a]. Then, we have
that∂σ(x)coincides with the derivative values in all points but x=±a. In those two points, we have
∂σ(±a) = [0 ,1]. Thus, for any νx∈∂σ(x), we have
cond( σ) = sup
x∈[−a,a]|νx||x|
|σ(x)|≤sup
x∈[−a,a]|x|
|σ(x)|=a
•Logistic sigmoid. Forx∈Rletσ(x) = (1 + e−x)−1. Then σ′(x) =σ(x)(1−σ(x))and thus
cond( σ;x) =|x|(1−σ(x)) =|x|e−x(1 +e−x)−1.
Therefore, when x≥0, we have |x|e−x≤1/eand(1 +e−x)≥1, thus cond( σ;x)≤1/e.
•Softplus. Forx∈R, letσ(x) = ln(1 + ex). Then σ′(x) =S(x) = (1 + e−x)−1andcond( σ;x) =
|x|S(x)σ(x)−1. Thus, for x≥0, we have cond( σ;x)≤1.
•SiLU. Forx∈Rletσ(x) =x(1 +e−x)−1=xS(x). Then, σ′(x) =S(x) +xS(x)(1−S(x))and
thus for any x≥0we have
cond( σ;x) =|1 +x(1−S(x))| ≤1 +1
e
12

--- PAGE 13 ---
C Proof of Theorem 1
In the following, the proof of the main approximation result is presented. We underline that the core
part of the proof relies on [ 29, Theorem 5.2]. For completeness, we repropose here main elements of the
argument. We refer the interested reader to [29] and references therein for further details.
Proof. LetY(t)be the solution of (5)at time t∈[0, λ]. First, we observe that the projected subflows of
W(t) =fW(t) +E(t)andfW(t)satisfy the differential equations


˙Y=P(Y)˙fW+P(Y)˙E ,
˙fW=P
fW˙fW .
where P(·)denotes the orthogonal projection into the tangent space of the low-rank manifold Mr. Next,
we observe that the following identities hold
(P(Y)−P(fW))˙fW=−(P⊥(Y)−P⊥(fW))˙fW=−P⊥(Y)˙fW=−P⊥(Y)2˙fW .
where P⊥(·) =I−P(·)represents the complementary orthogonal projection. The latter implies that
⟨Y−fW,(P(Y)−P(fW))˙fW⟩=⟨P⊥(Y)(Y−fW),(P(Y)−P(fW))˙fW⟩.
Letγ= 32µ(s−ε)−2. It follows from [29, Lemma 4.2] that
⟨Y−fW,˙Y−˙fW⟩=⟨P⊥(Y)(Y−fW),(P(Y)−P(fW))˙fW⟩+⟨Y−fW, P (Y)˙E⟩
≤γ∥Y−fW∥3+η∥Y−fW∥.
Further, we remind that
⟨Y−fW,˙Y−˙fW⟩=1
2d
dt∥Y−fW∥2=∥Y−fW∥d
dt∥Y−fW∥.
Hence, the error e(t) =∥Y(t)−fW(t)∥satisfies the differential inequality
˙e≤γe2+η, e (0) = 0 .
The error e(t)fort∈[0, λ]admits an upper bound given by the solution of
˙z=γz2+η, z (0) = 0 .
The last differential initial-value problem admits a closed solution given by
z(t) =p
η/γtan (tp
ηγ),
where the last term is bounded by 2tηfort√γη≤1. The proof thus concludes as follows
∥Y(t)−W(t)∥ ≤ ∥ Y(t)−fW(t)∥+∥E(t)∥ ≤2tη+tη= 3tη ,
where the last estimate arise by the integral identity E(t) =Rt
0˙E(s)ds.
References
[1]P. Ablin and G. Peyré. Fast and accurate optimization on the orthogonal manifold without retraction.
InInternational Conference on Artificial Intelligence and Statistics , pages 5636–5657. PMLR, 2022.
[2]P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds . Princeton
University Press, 2008.
[3]P.-A. Absil and J. Malick. Projection-like retractions on matrix manifolds. SIAM Journal on
Optimization , 22(1):135–158, 2012.
[4]P.-A. Absil and I. V . Oseledets. Low-rank retractions: a survey and new results. Computational
Optimization and Applications , 62(1):5–29, 2015.
13

--- PAGE 14 ---
[5]C. Anil, J. Lucas, and R. Grosse. Sorting out lipschitz function approximation. In International
Conference on Machine Learning , pages 291–301. PMLR, 2019.
[6]M. Arjovsky, A. Shah, and Y . Bengio. Unitary evolution recurrent neural networks. In International
conference on machine learning , pages 1120–1128. PMLR, 2016.
[7]S. Arora, N. Cohen, W. Hu, and Y . Luo. Implicit regularization in deep matrix factorization. Advances
in Neural Information Processing Systems , 32, 2019.
[8]A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2n learning: Network to network compression
via policy gradient reinforcement learning. In International Conference on Learning Representations ,
2018.
[9]N. Bansal, X. Chen, and Z. Wang. Can we gain more from orthogonality regularizations in training
deep networks? Advances in Neural Information Processing Systems , 31, 2018.
[10] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. Advances in neural information processing systems , 30, 2017.
[11] G. Ceruti and C. Lubich. An unconventional robust integrator for dynamical low-rank approximation.
BIT Numerical Mathematics , 62(1):23–44, 2022.
[12] M. Cisse, P. Bojanowski, E. Grave, Y . Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In International Conference on Machine Learning , pages
854–863. PMLR, 2017.
[13] F. H. Clarke. Optimization and nonsmooth analysis . SIAM, 1990.
[14] J. Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smoothing.
Ininternational conference on machine learning , pages 1310–1320. PMLR, 2019.
[15] J. Ding, T. Bu, Z. Yu, T. Huang, and J. Liu. Snn-rat: Robustness-enhanced spiking neural network
through regularized adversarial training. Advances in Neural Information Processing Systems ,
35:24780–24793, 2022.
[16] R. Feng, K. Zheng, Y . Huang, D. Zhao, M. Jordan, and Z.-J. Zha. Rank diminishing in deep neural
networks. arXiv:2206.06072 , 2022.
[17] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples, 2015.
[18] S. Gui, H. Wang, H. Yang, C. Yu, Z. Wang, and J. Liu. Model Compression with Adversarial
Robustness: A Unified Optimization Framework . 2019.
[19] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for model compression and
acceleration on mobile devices. In Proceedings of the European conference on computer vision ,
pages 784–800, 2018.
[20] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In IEEE
International Conference on Computer Vision , pages 1389–1397, 2017.
[21] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. Advances in neural information processing systems , 30, 2017.
[22] D. J. Higham. Condition numbers and their condition numbers. Linear Algebra and its Applications ,
214:193–213, 1995.
[23] M. Huh, H. Mobahi, R. Zhang, B. Cheung, P. Agrawal, and P. Isola. The low-rank simplicity bias in
deep networks. Transactions on Machine Learning Research , 2023.
[24] Y . Idelbayev and M. A. Carreira-Perpinán. Low-rank compression of neural nets: Learning the
rank of each layer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8049–8059, 2020.
[25] K. Jia, D. Tao, S. Gao, and X. Xu. Improving training of deep neural networks via singular value
bounding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 4344–4352, 2017.
[26] A. Jordao and H. Pedrini. On the effect of pruning on adversarial robustness. In 2021 IEEE/CVF
International Conference on Computer Vision Workshops (ICCVW) . IEEE Computer Society, 2021.
14

--- PAGE 15 ---
[27] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized
neural layers. In International Conference on Learning Representations , 2021.
[28] H. Kim, M. U. K. Khan, and C.-M. Kyung. Efficient neural network compression. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition , pages 12569–12577, 2019.
[29] O. Koch and C. Lubich. Dynamical low-rank approximation. SIAM Journal on Matrix Analysis and
Applications , 29(2):434–454, 2007.
[30] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[31] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[32] H. Lee, S. Han, and J. Lee. Generative adversarial trainer: Defense to adversarial perturbations with
gan, 2017.
[33] K. Leino, Z. Wang, and M. Fredrikson. Globally-robust neural networks. In International Conference
on Machine Learning , pages 6212–6222. PMLR, 2021.
[34] M. Lezcano-Casado and D. Martınez-Rubio. Cheap orthogonal constraints in neural networks: A
simple parametrization of the orthogonal and unitary group. In International Conference on Machine
Learning , pages 3794–3803. PMLR, 2019.
[35] J. Li, L. Fuxin, and S. Todorovic. Efficient Riemannian optimization on the Stiefel manifold via the
Cayley transform. 2019.
[36] Q. Li, S. Haque, C. Anil, J. Lucas, R. B. Grosse, and J.-H. Jacobsen. Preventing gradient attenuation
in lipschitz constrained convolutional networks. Advances in neural information processing systems ,
32, 2019.
[37] Y . Li, Z. Yang, Y . Wang, and C. Xu. Neural architecture dilation for adversarial robustness. In
Advances in Neural Information Processing Systems , 2021.
[38] N. Liao, S. Wang, L. Xiang, N. Ye, S. Shao, and P. Chu. Achieving adversarial robustness via
sparsity. Machine Learning , pages 1–27, 2022.
[39] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representations
for efficient architecture search. In International Conference on Learning Representations , 2018.
[40] X. Liu, Y . Li, C. Wu, and C.-J. Hsieh. Adv-BNN: Improved adversarial defense through robust
bayesian neural network. In International Conference on Learning Representations , 2019.
[41] P. M. Long and H. Sedghi. Generalization bounds for deep convolutional neural networks. In
International Conference on Learning Representations , 2020.
[42] D. Madaan, J. Shin, and S. J. Hwang. Adversarial neural pruning with latent vulnerability suppression.
InICML , 2020.
[43] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations , 2018.
[44] K. D. Maduranga, K. E. Helfrich, and Q. Ye. Complex unitary recurrent neural networks using scaled
cayley transform. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33,
pages 4528–4535, 2019.
[45] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning Research ,
22(165):1–73, 2021.
[46] E. Massart. Orthogonal regularizers in deep learning: how to handle rectangular matrices? In 2022
26th International Conference on Pattern Recognition (ICPR) , pages 1294–1299. IEEE, 2022.
[47] L. Meunier, B. J. Delattre, A. Araujo, and A. Allauzen. A dynamical system perspective for lipschitz
neural networks. In International Conference on Machine Learning , pages 15484–15500. PMLR,
2022.
[48] J. Pennington, S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning through
dynamical isometry: theory and practice. Advances in neural information processing systems , 30,
2017.
15

--- PAGE 16 ---
[49] J. R. Rice. A theory of condition. SIAM Journal on Numerical Analysis , 3(2):287–310, 1966.
[50] D. A. Roberts, S. Yaida, and B. Hanin. The Principles of Deep Learning Theory . Cambridge
University Press, may 2022.
[51] S. Schotthöfer, E. Zangrando, J. Kusch, G. Ceruti, and F. Tudisco. Low-rank lottery tickets:
finding efficient low-rank neural networks via matrix differential equations. In Advances in Neural
Information Processing Systems , 2022.
[52] V . Sehwag, S. Wang, P. Mittal, and S. Jana. Hydra: Pruning adversarially robust neural networks.
NeurIPS , 2020.
[53] U. Shalit, D. Weinshall, and G. Chechik. Online learning in the manifold of low-rank matrices.
Advances in neural information processing systems , 23, 2010.
[54] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
[55] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural
network Hessian maps. In Advances in Neural Information Processing Systems , volume 34, 2021.
[56] S. Singla and S. Feizi. Skew orthogonal convolutions. In International Conference on Machine
Learning , pages 9756–9766. PMLR, 2021.
[57] S. Singla, S. Singla, and S. Feizi. Improved deterministic l2 robustness on cifar-10 and cifar-100. In
International Conference on Learning Representations , 2021.
[58] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR) ,
2014.
[59] D. Terjék. Adversarial lipschitz regularization. In International Conference on Learning Representa-
tions , 2020.
[60] L. N. Trefethen and D. Bau. Numerical Linear Algebra . SIAM, 1997.
[61] A. Trockman and J. Z. Kolter. Orthogonalizing convolutional layers with the cayley transform. In
International Conference on Learning Representations , 2021.
[62] T. Tsiligkaridis and J. Roberts. On frank-wolfe adversarial training. In ICML 2021 Workshop on
Adversarial Machine Learning , 2021.
[63] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with
accuracy. In International Conference on Learning Representations , 2018.
[64] Y . Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of perturba-
tion invariance for deep neural networks. Advances in neural information processing systems , 31,
2018.
[65] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on
Mathematics of Data Science , 1(1):144–160, 2019.
[66] A. Uschmajew and B. Vandereycken. Geometric methods on low-rank matrix and tensor manifolds.
Handbook of variational methods for nonlinear geometric data , pages 261–313, 2020.
[67] H. Wang, S. Agarwal, and D. Papailiopoulos. Pufferfish: communication-efficient models at no extra
cost. Proceedings of Machine Learning and Systems , 3:365–386, 2021.
[68] Y .-L. Wu, H.-H. Shuai, Z.-R. Tam, and H.-Y . Chiu. Gradient normalization for generative adversarial
networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
6373–6382, 2021.
[69] L. Xiao, Y . Bahri, J. Sohl-Dickstein, S. Schoenholz, and J. Pennington. Dynamical isometry and a
mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In
International Conference on Machine Learning , pages 5393–5402. PMLR, 2018.
[70] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y . Chen. Learning low-rank deep neural
networks via singular vector orthogonality regularization and singular value sparsification. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages
2899–2908, 2020.
16

--- PAGE 17 ---
[71] S. Ye, K. Xu, S. Liu, H. Cheng, J.-H. Lambrechts, H. Zhang, A. Zhou, K. Ma, Y . Wang, and
X. Lin. Adversarial robustness vs. model compression, or both? In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , October 2019.
[72] T. Yu, J. Li, Y . Cai, and P. Li. Constructing orthogonal convolutions in an explicit manner. In
International Conference on Learning Representations , 2022.
[73] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled trade-
off between robustness and accuracy. In International conference on machine learning , pages
7472–7482. PMLR, 2019.
17
