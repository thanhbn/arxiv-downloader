# 2308.08285.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2308.08285.pdf
# File size: 340609 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pre-training with Large Language Model-based Document Expansion
for Dense Passage Retrieval
Guangyuan Ma1,2*, Xing Wu1,2*, Peng Wang1,2, Zijia Lin3, Songlin Hu1,2
1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
3Kuaishou Technology
{maguangyuan,wuxing,wangpeng2022,husonglin }@iie.ac.cn, linzijia07@tsinghua.org.cn
Abstract
In this paper, we systematically study the potential of pre-
training with Large Language Model(LLM)-based document
expansion for dense passage retrieval. Concretely, we lever-
age the capabilities of LLMs for document expansion, i.e.
query generation, and effectively transfer expanded knowl-
edge to retrievers using pre-training strategies tailored for
passage retrieval. These strategies include contrastive learn-
ing and bottlenecked query generation. Furthermore, we in-
corporate a curriculum learning strategy to reduce the re-
liance on LLM inferences. Experimental results demonstrate
that pre-training with LLM-based document expansion sig-
nificantly boosts the retrieval performance on large-scale
web-search tasks. Our work shows strong zero-shot and out-
of-domain retrieval abilities, making it more widely applica-
ble for retrieval when initializing with no human-labeled data.
Introduction
Dense passage retrieval (Karpukhin et al. 2020) has broad
real-world applications, like web search (Liu et al. 2021;
Zou et al. 2023), retrieval-augmented generation (Lewis
et al. 2020; Cai et al. 2022) and query answering (Sakata
et al. 2019). It utilizes well-trained language-model-based
retrievers to extract sentence representations and retrieve rel-
evant passages with given queries. Recent studies have made
impressive progress in improving the effectiveness of dense
retrievers, such as hard negative mining (Qu et al. 2021),
late interaction (Khattab and Zaharia 2020; Santhanam et al.
2022), distillation (Ren et al. 2021; Lu et al. 2022), and en-
sembling (Gao and Callan 2022; Wu et al. 2023b). More-
over, the development of task-specific pre-training (Gao and
Callan 2021; Wu et al. 2023a; Liu and Shao 2022) pushes
the limits of retrieval tasks to new boundaries. Specifically,
those studies usually employ contrastive learning with span
corruption (Gao and Callan 2022; Izacard et al. 2021; Ma
et al. 2022), or additional decoder with bottlenecked struc-
tures (Gao and Callan 2021; Lu et al. 2021; Liu and Shao
2022; Wu et al. 2023a) for better representation learning.
Large language models (LLMs), like ChatGPT (Ouyang
et al. 2022), PaLM (Chowdhery et al. 2022), LLaMA (Tou-
vron et al. 2023), and tk-Instruct (Wang et al. 2022b), are
pre-trained on large-scale web corpus and exhibit excel-
*These authors contributed equally.lent abilities in context generation and instruction follow-
ing. There has been growing interest in incorporating pow-
erful LLMs into retrieval tasks. Existing studies (Gao et al.
2023; Wang, Yang, and Wei 2023; Jagerman et al. 2023; Yu
et al. 2023) focus on query expansion with LLMs for en-
hancing the lexical match of query-passage pairs. They uti-
lize the LLM-generated relevant passages as enriched query
contexts. Those studies have yielded better retrieval per-
formances, especially for zero-shot scenarios. Nevertheless,
conducting query expansion still needs heavy online infer-
ences with LLMs, which slows down the retrieval speed.
While query expansion expands the query with gener-
ated passages, document expansion , i.e., query generation,
is also a popular technique to boost retrieval performances.
It exploits a fully fine-tuned model, like T5 (Nogueira et al.
2019) or BART (Cho et al. 2022), to generate relevant
queries of a given passage, which either enrich the context
of the passage or serve as additional fine-tuning corpus. Due
to the excellent generation ability of LLMs, huge poten-
tial lies in the utilization of LLMs as document expansion
models. However, we argue that several drawbacks still hin-
der such usage. Firstly, document expansion relies on the
online inference of LLM in open-domain passage retrieval,
particularly when dealing with candidate corpora from new
domains. To avoid the need for additional LLM inferences
during retrieval, a feasible solution is to pre-train or fine-
tune an end-to-end retriever. However, this approach lacks
exploration and necessitates training paradigms specifically
designed for retrieval. Furthermore, document expansion in-
volves feeding a substantial corpus into LLMs to generate
queries, resulting in significant costs associated with LLM
inferences. Unfortunately, there is a shortage of methods to
mitigate these inference costs.
To mitigate the high online inference costs of LLM doc-
ument expansion, as is presented in Figure 1, we prompt
the LLM query generation for a series of pre-training ex-
periments tailored for dense retrieval. We emphasize that
our work only involves LLM inferences at the pre-training
stage of retrievers, but not the inference stage as traditional
query (Gao et al. 2023; Wang, Yang, and Wei 2023) or doc-
ument expansion (Nogueira et al. 2019). Two pre-training
paradigms, i.e., contrastive learning and bottlenecked query
generation, are explored in detail.
For contrastive pre-training, a direct contrastive loss of thearXiv:2308.08285v1  [cs.IR]  16 Aug 2023

--- PAGE 2 ---
### Instruction: Generate ten search queries for the following passage### Input: <passage>### Response:    LLaMA Prompts
Deﬁnition: Generate one search query in question or phrase format. The generated query should be unambiguous and related to the input.Positive Example 1−    Input: <Example 1 - Input>    Output: <Example 1 - Output>Positive Example 2−    Input: <Example 2 - Input>    Output: <Example 2 - Output>Now complete the following example−    Input: <passage>    Output:     Tk-Instruct PromptsFigure 1: Query Generation prompts for Alpaca-LLaMA
and tk-Instruct.
generated queries and passages is used to pull together their
embeddings, while pushing away in-batch negatives in the
latent space. We follow the contrastive architecture in (Gao
and Callan 2022) for fair comparision, and we argue that
LLM-generated queries can serve as the better context for
effective query-passage alignment.
Bottlenecked pre-training techniques are popular in re-
cent works (Lu et al. 2021; Liu and Shao 2022; Wu et al.
2023a), which connect accessional decoders solely through
the encoder’s representation. To pre-train with bottlenecked
query generation, similar to (Wu, Ma, and Hu 2022), we
adapt a single-layer Transformers decoder and use the casual
language model (CLM) task to generate expanded queries
with the assistance of the encoder’s embeddings. This bottle-
necked encoder-decoder structure first derives a compressed
representation through the encoder and then decompresses
the context information as LLM-expanded queries via the
decoder. As a result, the sentence embeddings contain en-
riched context information, providing effective initialization
for fine-tuning and inference. Especially, LLM-based doc-
ument expansion requires no human-labeled corpus as pre-
vious works (Wu, Ma, and Hu 2022; Cho et al. 2022) for
training additional domain-specific generative models like
docT5query (Nogueira et al. 2019).
Furthermore, to mitigate the LLM inference costs for
document expansion, we interpolate a two-stage curriculumlearning strategy for both pre-training schemas. Span cor-
ruption is firstly used to randomly sample contextual pairs
from a long document. Then we leverage the generation
abilities of LLMs to produce a relatively small amount of
queries for the next stage of pre-training.
In our study, we use Alpaca-LLaMA (Wang et al. 2023)
and tk-Instruct (Wang et al. 2022b) with different parameter
sizes for query generation. We conduct the experiments on
the large-scale MS-MARCO (Nguyen et al. 2016) datasets
and test on the in-domain MS-MARCO passage retrieval
task, TREC-DL 2019 & 2020 (Craswell et al. 2020, 2021)
and the out-of-domain BEIR (Thakur et al. 2021) task. Sev-
eral benefits are observed in our studies. 1) LLMs can gen-
erate a large number of high-quality queries based on the
world knowledge of LLM itself, which requires no addi-
tional human labeling and is suitable for scenarios lack-
ing in manually annotated data. 2) Contrastive pre-training
with LLM-generated queries has stronger in-domain zero-
shot retrieval performance and on-par performance with the
state-of-the-art (SOTA) methods after full fine-tuning. It
also shows better domain adaption abilities in out-of-domain
BEIR datasets. 3) Bottlenecked query generation shows bet-
ter initialization abilities after full fine-tuning. 4) With our
two-stage curriculum learning strategy, we reduce the num-
ber of MS-MARCO corpus involved in LLM inferences
from 8.8 million to 0.4 million, while keeping the minor per-
formance degeneration.
Our contributions are summarized as follows.
• We systematically study the potential of incorporating
LLMs into the pre-training stage of dense passage re-
trieval, suitable for the scarcity of human-annotated data.
• We find stronger zero-shot and fine-tuned performances
with contrastive learning and good initialization abilities
with bottlenecked query generation pre-training.
• We design a two-stage curriculum learning strategy that
greatly reduces the usage of LLM-expanded queries
while keeping the minor performance degeneration.
Methodology
In this section, we first introduce the definition of dense pas-
sage retrieval. Then we introduce our method for LLM query
generation, the detailed pre-training designs of contrastive
learning and bottlenecked query generation, and the two-
stage curriculum learning strategy for extended analyses.
Preliminaries
Given a query qand a set of passages Pn, the passage re-
trieval task aims to find the relevant passages based on the
similarity search. Dense passage retrieval utilizes an encoder
model Enc, e.g., a Transformers-based model like BERT
(Devlin et al. 2019), to yield the sentence representations
and measure query-passage similarities through inner prod-
uct or cosine distance. Formally, given a query qand a pas-
sageq, we can use a query encoder Encqand a passage
encoder Encpto derive their corresponding sentence repre-
sentations, i.e., vqandvpfrom the encoder hidden states of
the last layer at CLS position h[CLS]
last. Then the similarity

--- PAGE 3 ---
DecoderEncoderCLS
Passage1) Sampled Passage2) LLM Generated QueryEncoderCLS
PassageEncoderCLSShareParamContrastive LossMLM LossMLM LossMLM LossCE Lossb) Bottlenecked Query Generation Pre-trainingc) Contrastive Pre-trainingLarge Language ModelPassage
LLM Generated Querya) LLM Query GenerationFigure 2: Pre-training with LLM-based document expansion for dense passage retrieval. a)We utilize large language models
(LLMs) to generate pseudo-queries with zero-shot or few-shot prompts. b)Bottlenecked query generation pre-training appends
an auxiliary Transformers decoder to the encoder. Besides the Masked Language Modelling (MLM) loss of the encoder, we
connect the encoder-decoder with merely the bottlenecked representation, i.e., the hidden states of [CLS] token, and make the
decoder generate whole LLM-expanded queries with the Cross-Entropy (CE) loss. c)Contrastive pre-training pulls together
the representations of the passage and LLM-expanded queries and pushes away in-batch negatives. To minimize reliance on
LLM expansions, we implement a two-stage curriculum learning strategy. It first utilizes randomly sampled passages to fully
initialize the encoders. And then we can use a relatively small amount of LLM-expanded queries in the second phase.
between qandp, i.e., Sim(q, p), can be calculated as the
inner product of vqandvpfor simplicity as follows.
Sim(q, p) =Encq(q)·Encp(p) =vT
qvp (1)
The key to improving retrieval performances is to yield
stronger representations vq,vpwith better context align-
ment. The representations can be regarded as the compres-
sion of full contexts. We believe that incorporating the strong
context-generation abilities of LLMs into the pre-training
stage with carefully designed pre-tasks can be a new way
for improving such alignment.
LLM Query Generation
Given a passage p, we use a zero-shot prompt for Alpaca-
LLaMA and a few-shot prompt for tk-Instruct to expand
queries, as illustrated in Figure 1. We empirically find that
Alpaca 7B and 13B models work well on the zero-shot
prompt, which helps save computation budgets. We man-
ually write a few examples for tk-Instruct, as we find that
few-shot prompts make its query generation more stable.
LLM-based document expansion enriches the pre-training
corpus with additional contextual information. Instead of di-
rectly appending the expanded queries onto the passage, we
seek to incorporate them into our pre-training stage for bet-
ter initialization of end-to-end retrievers. Our work only in-
volves LLM inference at the pre-training stage, but not the
retrieval stage like traditional query or document expansion
works. Two pre-training paradigms are involved to incorpo-rate the LLM-generated queries into the dense model pre-
training.
Bottlenecked Query Generation Pre-training
Bottlenecked pre-training trains a monomeric encoder
(Enc) with good initialization abilities for subsequent fine-
tuning. Given a tokenized sentence t∈Tfrom the training
corpus, we randomly select a certain ratio of tokens, with the
corresponding indices denoted as M, and replace them with
mask tokens [m]:
mask (t) ={[CLS] , t1, t2,[m], t4, ..., t n,[SEP] }(2)
Cross-Entropy (CE) loss is then used to optimize as
Masked Language Model (MLM) loss for the encoder.
Lenc=−X
t∈TX
i∈Mlogp(ti|Enc(mask (t))) (3)
where tiis groundtruth tokens w.r.t corresponding mask to-
kens[m].
A single-layer accessional Transformers decoder ( Dec) is
further introduced, which receives the input from the con-
catenation of the encoder representation h[CLS]
lastand con-
textual texts x, e.g., LLM-generated queries.
Tctx={h[CLS]
last, x1, ..., x N,[SEP] } (4)
Then the decoder uses the Casual Language Model
(CLM) loss to generate the whole input context with the as-
sistance of encoder representation.

--- PAGE 4 ---
MS-MARCO TREC DL 19 TREC DL 20
Model / Zero-shot Evaluation MRR@10 Recall@50 Recall@1k nDCG@10 nDCG@10
BM25 18.7 59.2 85.7 51.2 47.7
SimCSE (Gao, Yao, and Chen 2021) † 8.7 33.7 64.6 24.5 17.9
coCondenser (Gao and Callan 2022) † 7.5 31.3 58.1 22.1 20.7
Contriever (Izacard et al. 2021) † 16.8 60.8 89.1 44.5 43.2
Contrastive Pre-training
Baseline 12.5 49.0 82.3 36.0 38.4
+ tk-inst 3b queries 20.9+8.470.2+21.292.8+10.547.0+11.048.6+10.2
+ Alpaca 7b queries 22.6+10.170.7+21.793.8+11.551.0+15.048.9+10.5
+ Alpaca 13b queries 22.7+10.271.7+22.794.3+12.053.9+17.950.1+11.7
Table 1: Zero-shot evaluation of contrastive pre-training with LLM-based document expansion. †denotes our reproduced re-
sults. The best scores are marked in bold . Results with the increment over the corresponding baseline have been tested with
two-tailed t-tests, demonstrating statistically significant improvements ( p-value ≤0.01 ).
Ldec=−X
xi∈Tctxlogp(xi|Dec(x[:i−1])) (5)
The final loss Lis then formulated as follows.
L=Lenc+Ldec (6)
Through the bottlenecked encoder-decoder structure, we
seek to compress the context signal from LLM-generated
queries into the encoder representations and give strong ini-
tialization ability to the encoder.
Contrastive Pre-training
For reproduction and fair comparison, we adapt the con-
trastive pre-training architecture from coCondenser (Gao
and Callan 2022). The passage pand its sampled or gener-
ated context pctxare directly forwarded through the encoder
Enc. Besides the MLM loss Lencof the encoder, an extra
Transformers decoder Decextis also introduced for repre-
sentation pre-training, which takes the concatenation of the
encoder representation h[CLS]
lastand encoder hidden states
hi
lfroml-th layer. Then a cross-entropy loss is used for the
decoder’s pre-task.
Lext=−X
t∈TX
i∈Mlogp(ti|Decext(h[CLS]
last, h1
l, ..., hi
l))
(7)
Differently, for pre-training with LLM-expanded queries,
assuming vpandvctxdenote encoders’ representations, a
contrastive loss with in-batch negatives is used as follows.
LCL=−logexp(vp·v+
ctx)
exp(vp·v+
ctx) +Pexp(vp·v−
ctx)(8)
where v+
ctxis the context text, e.g. LLM-generated queries,
corresponding to p. And v−
ctxis the in-batch negatives, i.e.,
the context texts of the other passages in the batch.
The final optimization objective is the sum of the above
losses.L=Lenc+Lext+LCL (9)
Through contrastive pre-training, the representations of
passage and LLM-generated queries are directly pulled to-
gether in the same latent space, which gives better query-
passage alignment and zero-shot ability to encoders.
Curriculum Learning
As discussed before, LLM-based document expansion faces
the challenge of costly inference due to large numbers of
documents or passages. Since we intend to pre-train our
model with enriched contexts, inspired by the wisdom of
curriculum learning (Bengio et al. 2009), we consider 1) a
randomly cropped passage span as a coarse-grained context,
while 2) the LLM-expanded queries as fine-grained con-
text, as depicted in Figure 2. Following the span corruption
strategies in the seed-encoder (Lu et al. 2021) and coCon-
denser (Gao and Callan 2022), we use the coarse-grained
context as the passage itself in the bottlenecked generation
pre-training, and the randomly sampled passage span in con-
trastive pre-training. As we focus on LLM-based document
expansion, other span corruption strategies (Wu et al. 2023a)
are left to our future work. After pre-training on a large
amount of randomly cropped contexts, we initialize from
the first stage and then use the fine-grained LLM-expanded
queries for the second-phrase pre-training. Experiments find
that this curriculum strategy greatly reduces the need for
LLM inferences on MS-MARCO passages, while still main-
taining similar retrieval performances.
Zero-shot evaluation and Fine-tuning
We conduct the zero-shot evaluation of the contrastive
pre-trained encoder without fine-tuning on MS-MARCO,
TREC-DL, and BEIR datasets. We conduct fine-tuning on
both pre-training schemas to verify their retrieval initializa-
tion ability. Following DPR (Karpukhin et al. 2020), a sim-
ple contrastive loss is applied to optimize the retriever.
L=−logexp(q·p+)
exp(q·p+) +Pexp(q·p−)(10)

--- PAGE 5 ---
MS-MARCO TREC DL 19 TREC DL 20
Model / Fine-tuned Results MRR@10 Recall@50 Recall@1k nDCG@10 nDCG@10
Contriever (Izacard et al. 2021) † 33.4 85.0 98.4 62.8 63.2
Condenser (Gao and Callan 2021) 36.6 85.4 † 97.4 69.8 66.5†
coCondenser (Gao and Callan 2022) 38.2 86.5 † 98.4 71.7† 68.4†
SimLM (Wang et al. 2022a) 39.1 87.3 † 98.6 68.9† 68.8†
RetroMAE (Liu and Shao 2022) 39.3 87.0 † 98.5 69.1† 70.0†
CoT-MAE (Wu et al. 2023a) 39.4 87.0 98.7 70.9† 70.4
Contrastive Pre-training
Baseline 38.8 87.8 98.8 71.1 68.4
+ tk-instruct 3b queries 39.6+0.888.8+1.099.0 72.9+1.871.1+2.7
+ Alpaca 7b queries 40.0+1.289.0+1.299.1 72.9+1.871.3+2.9
+ Alpaca 13b queries 39.6+0.888.8+1.098.9 72.6+1.572.3+3.9
Bottlenecked Query Generation
Baseline 39.3 87.9 98.6 69.9 67.4
+ tk-instruct 3b queries 40.3+1.088.7+0.898.9 70.7+0.870.0+2.6
+ Alpaca 7b queries 39.9+0.688.2 98.7 69.6 70.7+3.3
+ Alpaca 13b queries 39.7 88.3 98.7 70.8+0.969.4+2.0
Table 2: Fine-tuned results of pre-training with LLM-based document expansion. †denotes our reproduced results. The best
scores are marked in bold . Results with the increment over the corresponding baseline have been tested with two-tailed t-tests,
demonstrating statistically significant improvements ( p-value ≤0.01 ).
where qis a given query, p+andp−are their corresponding
positive passage and negative passages respectively.
Experiments
This section introduces detailed experiment settings for pre-
training and fine-tuning. Then we present the main results.
Pre-training
Following the pretraining settings in (Gao and Callan 2022),
we choose the MS-MARCO dataset (Nguyen et al. 2016)
with 3.2M documents as our pre-training corpus. LLMs with
different types and parameter sizes, i.e. Alpaca 7B, 13B
(Wang et al. 2023), and tk-instruct 3B (Wang et al. 2022b)
are used to generate the queries for LLM-based document
expansion. Nucleus sampling with topp= 0.95,topk= 50 ,
andtemperature = 0.7is used for LLM generation.
For bottlenecked query generation pre-training, the en-
coder is initialized from the 12-layer BERT- base model (De-
vlin et al. 2019), while the single-layer decoder is randomly
initialized from scratch. We use the AdamW optimizer with
a learning rate of 3e-4, batch size of 2048, total steps of
80k, and a warmup ratio of 0.1. The pre-training uses 8
Tesla A100 GPUs and trains for 19 hours. For contrastive
pre-training, we adapt the codes and architecture from (Gao
and Callan 2022) and initialize from (Gao and Callan 2021)
by following their settings. We use a learning rate of 1e-4,
batch size of 2048, and total steps of 120k and keep other
hyper-parameters the same as above for training 50 hours.
For curriculum learning, 75% of the total steps are used for
the first stage of pre-training with sampled spans, and the
remaining 25% of the steps are used for the second stageof pre-training with LLM-generated queries. We use the co-
sine scheduler with the same hyper-parameter settings for
the first stage, and a constant learning rate for the second
stage. All pre-training seeds are set to 42 for reproducibility.
The encoders are directly tested on downstream tasks with-
out fine-tuning for zero-shot evaluation.
Fine-tuning
The encoder is fine-tuned and tested on MS-MARCO Pas-
sage Ranking task (Nguyen et al. 2016), TREC Deep Learn-
ing (DL) 2019 (Craswell et al. 2020) and 2020 (Craswell
et al. 2021). MS-MARCO Passage Ranking dataset con-
tains 8.8 million passages and 500k human annotated query-
passage pairs. Following (Gao and Callan 2021), we re-
port the performance metrics on MRR@10, Recall@50, Re-
call@1K, and evaluate the models on its development set
with 6,980 queries, because its test set is not publicly avail-
able. TREC-DL 2019 and 2020 test sets both contain 200 an-
notated queries. We adopt the Tevatron pipeline (Gao et al.
2022) with the AdamW optimizer for a learning rate of 2e-5,
a batch size of 8, negative samples per passage of 15, a neg-
ative depth of 200, and trains for 3 epochs. The performance
metrics of TREC and BEIR are reported on NDCG@10.
Baselines
We compare to self-contained baselines without using LLM-
expanded queries, but only use randomly sampled spans as
coarse-grained contexts. All other hyper-parameters used in
the pre-training remain the same as the main experiments for
fair comparison. In fine-tuned experiments, the contrastive
pre-training baselines are mainly from (Wu, Ma, and Hu

--- PAGE 6 ---
Results / nDCG@10 BM25 coCondenser Contriever SimCSE Baseline + tk-Instruct 3b + Alpaca 7b + Alpaca 13b
TREC-COVID 65.6 21.2 27.3 27.5 16.2 36.8+20.652.3+36.154.7+38.5
NFCorpus 32.5 13.7 31.7 10.5 29.9 33.1+3.230.9+1.033.5+3.5
NQ 32.9 10.7 25.4 16.3 9.3 34.3+25.031.8+22.531.9+22.6
HotpotQA 60.3 22.3 48.1 23.8 24.2 56.2+32.051.5+27.351.8+27.6
FiQA-2018 23.6 7.2 24.5 9.7 19.6 29.8+10.327.2+7.628.6+9.0
ArguAna 31.5 34.4 37.9 28.0 35.8 44.6+8.840.5+4.840.6+4.9
Touch ´e-2020 36.7 5.8 16.7 13.4 8.1 16.3+8.213.7+5.516.9+8.7
CQADupStack 29.9 10.5 28.4 13.5 18.2 30.9+12.832.4+14.233.3+15.1
Quora 78.9 71.3 83.5 73.7 75.8 83.8+8.083.3+7.584.3+8.5
DBPedia 31.3 16.3 29.2 16.7 22.5 30.2+7.728.8+6.329.6+7.1
SCIDOCS 15.8 4.6 14.9 6.1 10.4 13.6+3.213.5+3.214.4+4.1
FEVER 75.3 16.8 68.2 29.2 43.6 61.9+18.367.2+23.673.1+29.5
Climate-FEVER 21.3 6.4 15.5 14.2 8.5 18.4+9.813.8+5.317.2+8.6
SciFact 66.5 43.2 64.9 25.0 52.7 64.4+11.760.8+8.160.9+8.2
Average 43.0 20.3 36.9 22.0 26.8 39.6+12.839.1+12.440.8+14.0
Table 3: Out-of-domain zero-shot evaluation of contrastive pre-training with LLM-based document expansion on BEIR bench-
mark. All baselines tested on nDCG@10 are based on our reproduction. Results with the increment over the corresponding
baseline have been tested with two-tailed t-tests, demonstrating statistically significant improvements ( p-value ≤0.01 ).
2022) by following their hyper-parameter settings, and other
baselines are based on our settings.
We also compare with other remarkable baselines, in-
cluding the traditional sparse retrieval BM25 (Robertson,
Zaragoza et al. 2009), unsupervised sentence similarity
encoder SimCSE (Gao, Yao, and Chen 2021), unsuper-
vised contrastive pre-training method coCondenser (Gao
and Callan 2022) and Contriever (Izacard et al. 2021) for
zero-shot evaluation. For fine-tuned results, we also com-
pare with the latest bottlenecked pre-training methods, in-
cluding Condenser (Gao and Callan 2021), SimLM (Wang
et al. 2022a), RetroMAE (Liu and Shao 2022) and CoT-
MAE (Wu et al. 2023a). Note that the recent bottlenecked
methods using multi-task pre-training (Zhou et al. 2022) or
hybrid retrieval (Liu et al. 2023; Wu et al. 2023b) are not
compared, as they are beyond the scope of fair comparison.
Zero-shot Evaluation
Table 1 reports the in-domain zero-shot evaluation of con-
trastive pre-training with LLM-based document expansion.
Pre-training with LLM-expanded queries shows clear im-
provements over its baselines that merely use randomly
sampled passages. This indicates that our method achieves
strong zero-shot retrieval abilities for in-domain evaluation
on the MS-MARCO and TREC-DL 19 & 20 datasets.
Fine-tuned Retrieval
The fine-tuned results of the two pre-training methods, i.e.,
contrastive pretraining and bottlenecked query generation
pretraining, are presented in Table 2. Pre-training with LLM-
expanded queries also gives a statistically significant boost
to their baselines and counterparts. In addition, we notice
that 1) Contrastive pre-training gives better results on theMS-MARCO passage task (in Recall@50 and Recall@1k)
and TREC-DL 19 & 20 (in nDCG@10). 2) Bottlenecked
query generation gives better initialization on MS-MARCO
w.r.t the official preferred metric MRR@10, but still lies be-
hind contrastive pre-training in other metrics.
Out-of-domain Evaluation
We also evaluate the out-of-domain zero-shot BEIR bench-
mark for contrastive pre-training with LLM-based document
expansion and report the metric (nDCG@10) in Table 3.
BM25 is a very strong baseline w.r.t all the other contrastive
pre-training methods that do not go through human-labeled
fine-tuning. Nevertheless, our method still shows strong im-
provements over its contrastive baseline. Specifically, com-
pared with Contriever (Izacard et al. 2021), which is an un-
supervised contrastive method pre-trained on a much larger
corpus CCNET (Wenzek et al. 2020), pre-training with LLM
expansion also shows superior retrieval performances.
Extended Analyses
In this section, we analyze the effect of scaling up LLMs
and the curriculum learning strategy with expanded queries
generated by Alpaca 13b1.
Effects of Scaling up LLMs
We use three LLMs with different parameter sizes ranging
from 3b to 13b, prompting them for document expansion
and integrating the generated queries into pre-training. As
shown in Table 1, scaling up the LLMs shows better re-
trieval performances in zero-shot contrastive pre-training.
1Alpaca 13b is chosen because of better results in zero-shot and
on-par performances in fine-tuned retrieval.

--- PAGE 7 ---
63.065.067.069.071.073.075.0
37.037.538.038.539.039.540.0
50k0.1M0.4M0.8M1M4M8.8MTREC-DL 20 / nDCG@10MS-MARCO / MRR@10Amount of Training Corpus for Fine-grained Pre-trainingBottleneck (MARCO)Bottleneck (DL20)Figure 3: Effects of curriculum learning for fine-tuned bot-
tlenecked pre-training with expanded queries generated by
Alpaca 13b. The dashed lines are the corresponding base-
lines from Table 2.
But this observation is not valid after fine-tuning in Table 2.
We hypothesize that for fine-tuning with human labels, these
LLMs are all capable enough for giving a good initialization
for retrieval.
Effects of Curriculum Learning
To further reduce the need for LLM-expanded queries in
pre-training, we attempt to use a curriculum learning strat-
egy as detailed before. We use randomly sampled spans as
the coarse-grained context in the first stage of curriculum
pre-training for 75% of the total training steps. Then we
use a small amount of LLM-expanded queries as the fine-
grained context for the remaining pre-training steps. Fig-
ure 3 and 4 show that both pre-training schemas benefit
from curriculum learning. Bottleneck query generation out-
performs its baseline with just 0.4 million LLM-expanded
queries after fine-tuning. Zero-shot contrastive pre-training
surpasses the baselines and continues to demonstrate sus-
tainable improvements as the number of fine-grained queries
increases.
Related Works
Pre-training for Dense Retrieval
Dense passage retrieval has gained sustainable improve-
ments with the recent development of pre-training tasks.
Some works focus on contrastive pre-training with con-
structed span relationship (Chang et al. 2020), randomly
cropped spans (Gao and Callan 2022) or multiple granular-
ity alignments (Ma et al. 2022). And meanwhile, the others
focus on pre-training with auxiliary bottlenecked decoders,
like pre-training with a weak generative decoder (Lu et al.
2021), extreme masked ratio (Liu and Shao 2022), and con-
textual span sampling (Wu et al. 2023a). Our method is sim-
ilar to (Gao and Callan 2022) and (Wu et al. 2023a), but
our core contribution is the methodology of incorporating
expanded queries generated by LLMs into such pre-training
schemas, which brings better context alignment and stronger
zero-shot and fine-tuned performances.
30.035.040.045.050.055.0
0.05.010.015.020.025.0
50k0.1M0.4M0.8M1M4M8.8MTREC-DL 20 / nDCG@10MS-MARCO / MRR@10Amount of Training Corpus for Fine-grained Pre-trainingContrast (MARCO)Contrast (DL20)Figure 4: Effects of curriculum learning for zero-shot con-
trastive pre-training with LLM-expanded queries.
LLM-based Query and Document Expansion
Traditional query or document expansions generate addi-
tional context via query rewriting (Lavrenko and Croft
2017), or with specially fine-tuned T5 (Nogueira et al. 2019)
or BART models (Cho et al. 2022). With the bloom of
LLMs (Ouyang et al. 2022; Touvron et al. 2023; Wang et al.
2022b), growing researches focus on using LLMs as query
expansion models (Gao et al. 2023; Wang, Yang, and Wei
2023; Jagerman et al. 2023; Yu et al. 2023), which enhance
the lexical match of query-passage pairs.
However, as discussed before, LLM-based document ex-
pansion is yet lacking exploration due to expensive infer-
ence costs brought by the huge amount of documents and
the online inference issue. We propose to tackle those issues
with pre-training techniques and curriculum learning strate-
gies tailored for dense retrieval. Our method is also orthog-
onal to traditional query and document expansion and can
incorporate them into the retrieval stage.
Conclusion
This paper systematically studies the potential of pre-
training with Large Language Model-based document ex-
pansion for dense passage retrieval. Strong improvements
in zero-shot and out-of-domain performances are observed
in contrastive pre-training with LLM-based document ex-
pansion. Moreover, both contrastive pretraining and bottle-
necked query generation pretraining achieve good retrieval
abilities after fine-tuning. We further propose a two-stage
curriculum learning strategy that can greatly reduce the need
for LLM-expanded queries in pre-training, while keeping
the minor performance degeneration. LLMs excel in ex-
panding high-quality queries with enriched context informa-
tion, which is suitable for scenarios lacking in human anno-
tations. Researchers can thus deploy quick initialization of
an unsupervised dense retrieval system with the pre-training
of LLM-based document expansion, with even NOhuman
labels provided.
Limitation
We are also interested in testing more types of LLMs with
different sizes, such as ChatGPT (Ouyang et al. 2022), and

--- PAGE 8 ---
LLaMA 2 (Touvron et al. 2023), or different prompts for
document expansion, but our experiment budget is limited
to support immediate investigations and we leave that to our
future works.
References
Bengio, Y .; Louradour, J.; Collobert, R.; and Weston, J.
2009. Curriculum learning. In Danyluk, A. P.; Bottou, L.;
and Littman, M. L., eds., Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, ICML 2009,
Montreal, Quebec, Canada, June 14-18, 2009 , volume 382
ofACM International Conference Proceeding Series , 41–48.
ACM.
Cai, D.; Wang, Y .; Liu, L.; and Shi, S. 2022. Recent ad-
vances in retrieval-augmented text generation. In Proceed-
ings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval , 3417–
3419.
Chang, W.; Yu, F. X.; Chang, Y .; Yang, Y .; and Kumar, S.
2020. Pre-training Tasks for Embedding-based Large-scale
Retrieval. In 8th International Conference on Learning Rep-
resentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-
30, 2020 . OpenReview.net.
Cho, S.; Jeong, S.; Yang, W.; and Park, J. C. 2022. Query
Generation with External Knowledge for Dense Retrieval. In
Agirre, E.; Apidianaki, M.; and Vulic, I., eds., Proceedings
of Deep Learning Inside Out: The 3rd Workshop on Knowl-
edge Extraction and Integration for Deep Learning Archi-
tectures, DeeLIO@ACL 2022, Dublin, Ireland and Online,
May 27, 2022 , 22–32. Association for Computational Lin-
guistics.
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton,
C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko,
S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y .; Shazeer,
N.; Prabhakaran, V .; Reif, E.; Du, N.; Hutchinson, B.;
Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.;
Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;
Michalewski, H.; Garcia, X.; Misra, V .; Robinson, K.; Fe-
dus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;
Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omer-
nick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,
A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;
Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei,
J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and
Fiedel, N. 2022. PaLM: Scaling Language Modeling with
Pathways. CoRR , abs/2204.02311.
Craswell, N.; Mitra, B.; Yilmaz, E.; and Campos, D.
2021. Overview of the TREC 2020 deep learning track.
arXiv:2102.07662.
Craswell, N.; Mitra, B.; Yilmaz, E.; Campos, D.; and
V oorhees, E. M. 2020. Overview of the TREC 2019 deep
learning track. arXiv:2003.07820.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , 4171–4186. Min-
neapolis, Minnesota: Association for Computational Lin-
guistics.
Gao, L.; and Callan, J. 2021. Condenser: a Pre-training
Architecture for Dense Retrieval. In Proceedings of the
2021 Conference on Empirical Methods in Natural Lan-
guage Processing , 981–993. Online and Punta Cana, Do-
minican Republic: Association for Computational Linguis-
tics.
Gao, L.; and Callan, J. 2022. Unsupervised Corpus Aware
Language Model Pre-training for Dense Passage Retrieval.
InProceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers) ,
2843–2853. Dublin, Ireland: Association for Computational
Linguistics.
Gao, L.; Ma, X.; Lin, J.; and Callan, J. 2022. Tevatron:
An efficient and flexible toolkit for dense retrieval. arXiv
preprint arXiv:2203.05765 .
Gao, L.; Ma, X.; Lin, J.; and Callan, J. 2023. Precise Zero-
Shot Dense Retrieval without Relevance Labels. In Rogers,
A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Proceedings
of the 61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL 2023,
Toronto, Canada, July 9-14, 2023 , 1762–1777. Association
for Computational Linguistics.
Gao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-
trastive Learning of Sentence Embeddings. In Proceedings
of the 2021 Conference on Empirical Methods in Natural
Language Processing , 6894–6910. Online and Punta Cana,
Dominican Republic: Association for Computational Lin-
guistics.
Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski,
P.; Joulin, A.; and Grave, E. 2021. Towards Unsuper-
vised Dense Information Retrieval with Contrastive Learn-
ing. CoRR , abs/2112.09118.
Jagerman, R.; Zhuang, H.; Qin, Z.; Wang, X.; and Bender-
sky, M. 2023. Query Expansion by Prompting Large Lan-
guage Models. CoRR , abs/2305.03653.
Karpukhin, V .; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,
S.; Chen, D.; and Yih, W.-t. 2020. Dense Passage Retrieval
for Open-Domain Question Answering. In Proceedings of
the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , 6769–6781. Online: Associa-
tion for Computational Linguistics.
Khattab, O.; and Zaharia, M. 2020. ColBERT: Efficient and
Effective Passage Search via Contextualized Late Interac-
tion over BERT. In Huang, J. X.; Chang, Y .; Cheng, X.;
Kamps, J.; Murdock, V .; Wen, J.; and Liu, Y ., eds., Proceed-
ings of the 43rd International ACM SIGIR conference on
research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020 , 39–48. ACM.
Lavrenko, V .; and Croft, W. B. 2017. Relevance-Based Lan-
guage Models. SIGIR Forum , 51(2): 260–267.
Lewis, P. S. H.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
V .; Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.; Rockt ¨aschel,
T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented

--- PAGE 9 ---
Generation for Knowledge-Intensive NLP Tasks. In
Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and
Lin, H., eds., Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual .
Liu, Y .; Lu, W.; Cheng, S.; Shi, D.; Wang, S.; Cheng, Z.; and
Yin, D. 2021. Pre-trained Language Model for Web-scale
Retrieval in Baidu Search. In Zhu, F.; Ooi, B. C.; and Miao,
C., eds., KDD ’21: The 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, Virtual Event, Sin-
gapore, August 14-18, 2021 , 3365–3375. ACM.
Liu, Z.; and Shao, Y . 2022. RetroMAE: Pre-training
Retrieval-oriented Transformers via Masked Auto-Encoder.
arXiv preprint arXiv:2205.12035 .
Liu, Z.; Xiao, S.; Shao, Y .; and Cao, Z. 2023. RetroMAE-2:
Duplex Masked Auto-Encoder For Pre-Training Retrieval-
Oriented Language Models. In Rogers, A.; Boyd-Graber,
J. L.; and Okazaki, N., eds., Proceedings of the 61st An-
nual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , 2635–2648. Association for Computational
Linguistics.
Lu, S.; He, D.; Xiong, C.; Ke, G.; Malik, W.; Dou, Z.; Ben-
nett, P.; Liu, T.-Y .; and Overwijk, A. 2021. Less is More:
Pretrain a Strong Siamese Encoder for Dense Text Retrieval
Using a Weak Decoder. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Process-
ing, 2780–2791.
Lu, Y .; Liu, Y .; Liu, J.; Shi, Y .; Huang, Z.; Sun, S. F. Y .; Tian,
H.; Wu, H.; Wang, S.; Yin, D.; et al. 2022. Ernie-search:
Bridging cross-encoder with dual-encoder via self on-the-
fly distillation for dense passage retrieval. arXiv preprint
arXiv:2205.09153 .
Ma, X.; Guo, J.; Zhang, R.; Fan, Y .; and Cheng, X. 2022.
Pre-train a Discriminative Text Encoder for Dense Re-
trieval via Contrastive Span Prediction. arXiv preprint
arXiv:2204.10641 .
Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.;
Majumder, R.; and Deng, L. 2016. MS MARCO: A Hu-
man Generated MAchine Reading COmprehension Dataset.
In Besold, T. R.; Bordes, A.; d’Avila Garcez, A. S.; and
Wayne, G., eds., Proceedings of the Workshop on Cognitive
Computation: Integrating neural and symbolic approaches
2016 co-located with the 30th Annual Conference on Neural
Information Processing Systems (NIPS 2016), Barcelona,
Spain, December 9, 2016 , volume 1773 of CEUR Workshop
Proceedings . CEUR-WS.org.
Nogueira, R. F.; Yang, W.; Lin, J.; and Cho, K. 2019.
Document Expansion by Query Prediction. CoRR ,
abs/1904.08375.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,
C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,
A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,
M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and
Lowe, R. 2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .Qu, Y .; Ding, Y .; Liu, J.; Liu, K.; Ren, R.; Zhao, W. X.;
Dong, D.; Wu, H.; and Wang, H. 2021. RocketQA: An Op-
timized Training Approach to Dense Passage Retrieval for
Open-Domain Question Answering. In Proceedings of the
2021 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies , 5835–5847. Online: Association for Compu-
tational Linguistics.
Ren, R.; Qu, Y .; Liu, J.; Zhao, W. X.; She, Q.; Wu, H.;
Wang, H.; and Wen, J.-R. 2021. RocketQAv2: A Joint Train-
ing Method for Dense Passage Retrieval and Passage Re-
ranking. In Proceedings of the 2021 Conference on Empir-
ical Methods in Natural Language Processing , 2825–2835.
Online and Punta Cana, Dominican Republic: Association
for Computational Linguistics.
Robertson, S.; Zaragoza, H.; et al. 2009. The probabilistic
relevance framework: BM25 and beyond. Foundations and
Trends® in Information Retrieval , 3(4): 333–389.
Sakata, W.; Shibata, T.; Tanaka, R.; and Kurohashi, S. 2019.
FAQ Retrieval using Query-Question Similarity and BERT-
Based Query-Answer Relevance. In Piwowarski, B.; Cheva-
lier, M.; Gaussier, ´E.; Maarek, Y .; Nie, J.; and Scholer,
F., eds., Proceedings of the 42nd International ACM SI-
GIR Conference on Research and Development in Informa-
tion Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 ,
1113–1116. ACM.
Santhanam, K.; Khattab, O.; Saad-Falcon, J.; Potts, C.; and
Zaharia, M. 2022. ColBERTv2: Effective and Efficient Re-
trieval via Lightweight Late Interaction. In Proceedings
of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Lan-
guage Technologies , 3715–3734. Seattle, United States: As-
sociation for Computational Linguistics.
Thakur, N.; Reimers, N.; R ¨uckl´e, A.; Srivastava, A.; and
Gurevych, I. 2021. BEIR: A Heterogenous Benchmark
for Zero-shot Evaluation of Information Retrieval Models.
CoRR , abs/2104.08663.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.; Azhar,
F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.
2023. LLaMA: Open and Efficient Foundation Language
Models. CoRR , abs/2302.13971.
Wang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang,
D.; Majumder, R.; and Wei, F. 2022a. SimLM: Pre-training
with Representation Bottleneck for Dense Passage Retrieval.
CoRR , abs/2207.02578.
Wang, L.; Yang, N.; and Wei, F. 2023. Query2doc:
Query Expansion with Large Language Models. CoRR ,
abs/2303.07678.
Wang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Align-
ing Language Models with Self-Generated Instructions. In
Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Pro-
ceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023 , 13484–13508. As-
sociation for Computational Linguistics.

--- PAGE 10 ---
Wang, Y .; Mishra, S.; Alipoormolabashi, P.; Kordi, Y .;
Mirzaei, A.; Naik, A.; Ashok, A.; Dhanasekaran, A. S.;
Arunkumar, A.; Stap, D.; Pathak, E.; Karamanolakis, G.;
Lai, H. G.; Purohit, I.; Mondal, I.; Anderson, J.; Kuznia,
K.; Doshi, K.; Pal, K. K.; Patel, M.; Moradshahi, M.; Par-
mar, M.; Purohit, M.; Varshney, N.; Kaza, P. R.; Verma, P.;
Puri, R. S.; Karia, R.; Doshi, S.; Sampat, S. K.; Mishra, S.;
A, S. R.; Patro, S.; Dixit, T.; and Shen, X. 2022b. Super-
NaturalInstructions: Generalization via Declarative Instruc-
tions on 1600+ NLP Tasks. In Goldberg, Y .; Kozareva,
Z.; and Zhang, Y ., eds., Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language Process-
ing, EMNLP 2022, Abu Dhabi, United Arab Emirates, De-
cember 7-11, 2022 , 5085–5109. Association for Computa-
tional Linguistics.
Wenzek, G.; Lachaux, M.; Conneau, A.; Chaudhary, V .;
Guzm ´an, F.; Joulin, A.; and Grave, E. 2020. CCNet: Extract-
ing High Quality Monolingual Datasets from Web Crawl
Data. In Calzolari, N.; B ´echet, F.; Blache, P.; Choukri,
K.; Cieri, C.; Declerck, T.; Goggi, S.; Isahara, H.; Mae-
gaard, B.; Mariani, J.; Mazo, H.; Moreno, A.; Odijk, J.; and
Piperidis, S., eds., Proceedings of The 12th Language Re-
sources and Evaluation Conference, LREC 2020, Marseille,
France, May 11-16, 2020 , 4003–4012. European Language
Resources Association.
Wu, X.; Ma, G.; and Hu, S. 2022. Query-as-
context Pre-training for Dense Passage Retrieval. CoRR ,
abs/2212.09598.
Wu, X.; Ma, G.; Lin, M.; Lin, Z.; Wang, Z.; and Hu, S.
2023a. ConTextual Masked Auto-Encoder for Dense Pas-
sage Retrieval. In Williams, B.; Chen, Y .; and Neville, J.,
eds., Thirty-Seventh AAAI Conference on Artificial Intelli-
gence, AAAI 2023, Thirty-Fifth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2023, Thirteenth
Symposium on Educational Advances in Artificial Intelli-
gence, EAAI 2023, Washington, DC, USA, February 7-14,
2023 , 4738–4746. AAAI Press.
Wu, X.; Ma, G.; Wang, P.; Lin, M.; Lin, Z.; Zhang, F.; and
Hu, S. 2023b. CoT-MAE v2: Contextual Masked Auto-
Encoder with Multi-view Modeling for Passage Retrieval.
arXiv:2304.03158.
Yu, W.; Iter, D.; Wang, S.; Xu, Y .; Ju, M.; Sanyal, S.; Zhu,
C.; Zeng, M.; and Jiang, M. 2023. Generate rather than Re-
trieve: Large Language Models are Strong Context Gener-
ators. In The Eleventh International Conference on Learn-
ing Representations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023 . OpenReview.net.
Zhou, K.; Liu, X.; Gong, Y .; Zhao, W. X.; Jiang, D.; Duan,
N.; and Wen, J.-R. 2022. MASTER: Multi-task Pre-trained
Bottlenecked Masked Autoencoders are Better Dense Re-
trievers. arXiv preprint arXiv:2212.07841 .
Zou, L.; Lu, W.; Liu, Y .; Cai, H.; Chu, X.; Ma, D.; Shi, D.;
Sun, Y .; Cheng, Z.; Gu, S.; Wang, S.; and Yin, D. 2023. Pre-
trained Language Model-based Retrieval and Ranking for
Web Search. ACM Trans. Web , 17(1): 4:1–4:36.
