# 2303.06233.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2303.06233.pdf
# Kích thước tệp: 3519968 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thông tin Cú pháp Bất khả tri Mô hình cho 
Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện
Iman Saberi, Fatemeh H. Fardy
Khoa Khoa học Máy tính, Toán học, Vật lý và Thống kê
Đại học British Columbia
Kelowna, Canada
Email: iman.saberi@ubc.ca, yfatemeh.fard@ubc.ca

Tóm tắt — Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện (PPLMs) đã đạt được nhiều kết quả hiện đại cho nhiều tác vụ kỹ thuật phần mềm liên quan đến code. Mặc dù một số nghiên cứu sử dụng dòng dữ liệu hoặc đề xuất các mô hình dựa trên cây sử dụng Cây Cú pháp Trừu tượng (AST), hầu hết PPLMs không sử dụng đầy đủ thông tin cú pháp phong phú trong mã nguồn. Vẫn còn, đầu vào được xem xét như một chuỗi token. Có hai vấn đề; thứ nhất là tính không hiệu quả tính toán do mối quan hệ bậc hai giữa độ dài đầu vào và độ phức tạp attention. Thứ hai, bất kỳ thông tin cú pháp nào, khi cần thiết như đầu vào bổ sung cho PPLMs hiện tại, đòi hỏi mô hình phải được tiền huấn luyện từ đầu, lãng phí tất cả tài nguyên tính toán đã được sử dụng để tiền huấn luyện các mô hình hiện tại. Trong công trình này, chúng tôi đề xuất bộ adapter Nhận dạng Thực thể Có tên (NER), các module nhẹ có thể được chèn vào các khối Transformer để học thông tin loại được trích xuất từ AST. Những adapter này có thể được sử dụng với PPLMs hiện tại như CodeBERT, GraphCodeBERT, và CodeT5. Chúng tôi huấn luyện các adapter NER sử dụng hàm mục tiêu Phân loại Loại Token mới (TTC). Chúng tôi chèn công trình đề xuất vào CodeBERT, xây dựng CodeBERTER, và đánh giá hiệu suất trên hai tác vụ tinh chỉnh code và tóm tắt code. CodeBERTER cải thiện độ chính xác của tinh chỉnh code từ 16.4 lên 17.8 trong khi sử dụng 20% ngân sách tham số huấn luyện so với phương pháp tinh chỉnh đầy đủ, và điểm BLEU của tóm tắt code từ 14.75 lên 15.90 trong khi giảm 77% tham số huấn luyện so với phương pháp tinh chỉnh đầy đủ.

Từ khóa chỉ mục — Adapters, Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện

I. GIỚI THIỆU
Sự thành công của Mô hình Ngôn ngữ Được Tiền huấn luyện (PLM) trong Xử lý Ngôn ngữ Tự nhiên (NLP) đã dẫn đến sự xuất hiện của các mô hình được tiền huấn luyện trong kỹ thuật phần mềm, chẳng hạn như CodeBERT [1], CuBERT [2], và CodeT5 [3]. Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện (PPLMs) –PLMs được tiền huấn luyện trên ngôn ngữ lập trình– học một đại diện chung của code sử dụng hàm mục tiêu trừu tượng như Mô hình Ngôn ngữ Mặt nạ (MLM) trong giai đoạn tiền huấn luyện. Những đại diện này sau đó được khai thác cho các tác vụ downstream định hướng ngôn ngữ lập trình như tóm tắt code [4], [5], [6], phát hiện bản sao code [7], [8] và tinh chỉnh code [9], [10] trong giai đoạn tinh chỉnh.

Mặc dù có sự tiến bộ của PPLMs, hầu hết chúng [2], [11], [12] khai thác hoặc encoder hoặc decoder Transformer blocks [13]. Những mô hình này xử lý một đoạn code như một chuỗi token giống như các kỹ thuật thông thường cho mô hình hóa đầu vào trong lĩnh vực ngôn ngữ tự nhiên trong khi bỏ qua thông tin cú pháp phong phú của mã nguồn. Gần đây, một số nghiên cứu đã đề xuất các đại diện với thông tin cấu trúc của mã nguồn trong tâm trí [9], [3], [14].

Jiang et al. đề xuất một PLM dựa trên cây cho các tác vụ sinh ngôn ngữ lập trình [14], sử dụng Cây Cú pháp Trừu tượng (AST) để cung cấp thông tin cấu trúc cho việc tiền huấn luyện kiến trúc dựa trên Transformer [13]. So với các phương pháp dựa trên chuỗi xem xét token code như đầu vào của mô hình, trong phương pháp của họ, mỗi token code được đại diện như một đường dẫn từ nút gốc đến nút terminal tương ứng với token code đó trong AST. Mặc dù phương pháp của họ cung cấp thông tin cấu trúc phong phú cho mô hình, độ dài đầu vào của mô hình được nhân với độ dài trung bình của tất cả đường dẫn trong AST. Việc mở rộng độ dài đầu vào này không hiệu quả về mặt tính toán, theo Ainslie et al., người đề cập rằng độ phức tạp tính toán và bộ nhớ của hoạt động attention tăng theo bậc hai khi độ dài đầu vào tăng [15].

Một số PPLMs khác cung cấp thông tin loại cho các mô hình hiện có. GraphCodeBERT [9] xem xét cấu trúc vốn có của code và sử dụng dataflow được trích xuất từ AST tương ứng với mỗi đầu vào để cung cấp thông tin cấp độ ngữ nghĩa như đầu vào bổ sung cho mô hình; sau đó tiền huấn luyện CodeBERT [1] sử dụng thông tin mới này. Hơn nữa, tác giả của CodeT5 [3] đề cập rằng các định danh được gán bởi nhà phát triển bao gồm ngữ nghĩa code phong phú, vì vậy họ đề xuất một tác vụ tiền huấn luyện nhận biết định danh mới để thông báo cho mô hình liệu một token code có phải là định danh hay không. Cả hai phương pháp này đều yêu cầu mô hình phải được tiền huấn luyện từ đầu. Ngay cả thông tin mới được cung cấp bởi GraphCodeBERT, mặc dù mô hình này được tiền huấn luyện trên CodeBERT, cũng yêu cầu tiền huấn luyện từ đầu, điều này không hiệu quả về thời gian và tính toán.

Do đó, chúng ta không thể trực tiếp tận dụng PPLMs hiện có như CodeBERT [1], GraphCodeBERT [9], và CodeT5 [3] để đề xuất thông tin hoặc định dạng đầu vào mới. Vì những mô hình này được tiền huấn luyện dựa trên chuỗi token code được làm phẳng [16], việc đề xuất một hình thức đại diện đầu vào mới đòi hỏi tất cả những mô hình này phải được tiền huấn luyện từ đầu. Hai vấn đề này, cụ thể là tính không hiệu quả tính toán của việc sử dụng thông tin cú pháp của mã nguồn (do tăng độ dài) và yêu cầu tiền huấn luyện PPLMs từ đầu khi giới thiệu đầu vào mới, đã thúc đẩy chúng tôi đề xuất một phương pháp giải quyết chúng. Do đó, công trình này nhằm mục đích cung cấp embedding cú pháp của mã nguồn cho các mô hình ngôn ngữ lập trình được tiền huấn luyện hiện có.

Để đề xuất một phương pháp hiệu quả tham số và bất khả tri mô hình để áp đặt thông tin cú pháp cho PPLMs hiện tại, chúng tôi sử dụng adapters. Trong NLP, adapter là một thành phần nhẹ được đặt bên trong mỗi khối Transformer để sửa đổi hành vi của nó [17]. Trong quá trình huấn luyện adapter, trọng số của mô hình được tiền huấn luyện được cố định, và trọng số adapter mới được giới thiệu được huấn luyện trên một hàm mục tiêu. Adapters thường được sử dụng để điều chỉnh các mô hình NLP hiện có để hoạt động trong một bối cảnh cụ thể (tức là, transfer learning) [18], [19] hoặc để thực hiện một tác vụ cụ thể (tức là, tinh chỉnh nhanh) [20], [21]. Tuy nhiên, adapters chưa bao giờ được sử dụng để nắm bắt thông tin cú pháp của mã nguồn cũng như cung cấp thông tin mới cho PLMs hoặc PPLMs hiện có.

Trong công trình này, chúng tôi đề xuất Adapters Nhận dạng Thực thể Có tên (NER), các module nhẹ được chèn bên trong các khối Transformer có mục đích học thông tin loại được trích xuất từ AST. Những module có thể cắm này có thể được chèn vào PPLMs hiện tại như CodeBERT [1], GraphCodeBERT [9], và CodeT5 [3]. Để huấn luyện adapters NER, chúng tôi đặt vấn đề như một tác vụ phân loại token trong đó mỗi token đầu vào được gán một loại được trích xuất từ AST, và adapter NER nhằm mục đích phát hiện loại của mỗi token một cách chính xác.

Chúng tôi chèn adapters NER vào CodeBERT [1], do đó, phát triển một mô hình mà chúng tôi gọi là CodeBERTER. Chúng tôi tiến hành thí nghiệm trên dữ liệu sửa lỗi Java được giới thiệu trong [10] cho tinh chỉnh code, và bộ dữ liệu CodeSearchNet [22] cho tác vụ tóm tắt code. Câu hỏi nghiên cứu chính mà chúng tôi điều tra ở đây là: Liệu adapters NER có thể cải thiện hiệu suất của các tác vụ mục tiêu kỹ thuật phần mềm, cụ thể là tinh chỉnh code và tóm tắt code, trong khi sử dụng ít tham số có thể huấn luyện hơn không?

Kết quả cho thấy rằng đối với tinh chỉnh code, chúng tôi đã cải thiện độ chính xác baseline CodeBERT từ 16.4 lên 17.8 với 23 triệu tham số có thể huấn luyện, ít hơn 80% so với tổng số tham số có thể huấn luyện của baseline. Chúng tôi cũng áp dụng adapters NER trên tóm tắt code, điều này cải thiện điểm BLEU-4 của hai ngôn ngữ, bao gồm Ruby và Go, lần lượt 30% và 29%, với ít hơn 77% tham số so với tinh chỉnh đầy đủ.

Những đóng góp chính của chúng tôi như sau:
Giới thiệu adapters NER với hàm mất mát mới, Token Type Classification Loss (TTC), để áp đặt thông tin cú pháp cho PPLMs hiện có. Chúng tôi cũng công bố mã nguồn¹.
Đánh giá hiệu suất của adapters NER trên tinh chỉnh code và tóm tắt code.
Cải thiện kết quả của những tác vụ này trong khi huấn luyện ít tham số hơn và hiệu quả tính toán hơn.

Phần còn lại của bài báo được tổ chức như sau. Trong Phần II, chúng tôi cung cấp tổng quan về thông tin nền quan trọng và sau đó giới thiệu adapters NER trong Phần III. Chúng tôi cung cấp thiết lập thí nghiệm và chi tiết của nghiên cứu trong Phần IV. Kết quả và thảo luận được giải thích trong Phần V. Phần VI và VII được dành cho các công trình liên quan và mối đe dọa đối với tính hợp lệ. Cuối cùng, chúng tôi kết luận bài báo trong Phần VIII.

--- TRANG 2 ---
II. KIẾN THỨC NỀN TẢNG

A. Cây Cú pháp Trừu tượng
AST là một đại diện giống cây của cú pháp của một ngôn ngữ lập trình. Mỗi nút trong cây đại diện cho một cấu trúc cú pháp trong mã nguồn, như từ khóa, toán tử, biến, hoặc hàm. Cấu trúc của cây phản ánh cấu trúc của chương trình, với nút gốc đại diện cho cấu trúc cấp cao nhất và các nút lá đại diện cho các cấu trúc nhỏ nhất, cơ bản nhất [23].

ASTs có thể được sử dụng cho nhiều mục đích khác nhau, bao gồm phân tích cấu trúc của chương trình để kiểm tra lỗi cú pháp hoặc thực thi tiêu chuẩn mã hóa [24], và trích xuất thông tin về chương trình, như định nghĩa biến và hàm, để tạo tài liệu hoặc sinh báo cáo coverage code [25].

B. Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện
Mô hình Ngôn ngữ Lập trình Được Tiền huấn luyện (PPLMs) là các mô hình ngôn ngữ neural sâu được huấn luyện trên bộ dữ liệu lớn mã nguồn và học dự đoán từ hoặc token tiếp theo trong code dựa trên ngữ cảnh của các từ hoặc token trước đó. Những mô hình này sau đó có thể được sử dụng để thực hiện nhiều tác vụ mục tiêu trong giai đoạn tinh chỉnh, như tóm tắt code [3], [26], hoàn thành code [27], sinh code [3], và tinh chỉnh code [9].

C. Adapters
Trong xử lý ngôn ngữ tự nhiên, adapters là các thành phần nhẹ có thể được sử dụng để điều chỉnh mô hình ngôn ngữ cho một tác vụ hoặc bộ dữ liệu cụ thể. Huấn luyện dựa trên adapter là một phương pháp tinh chỉnh hiệu quả và nhanh chóng đòi hỏi ít tham số hơn so với tinh chỉnh đầy đủ truyền thống. So với việc đơn giản thêm một head mới lên trên mô hình ngôn ngữ được tiền huấn luyện (PLM), adapters cung cấp hiệu suất vượt trội do khả năng tích hợp vào cấu trúc nội bộ của PLM và ảnh hưởng đến embeddings nội bộ của mạng.

Adapters được sử dụng trong nhiều lĩnh vực, bao gồm (1) tinh chỉnh, là quá trình tiếp tục huấn luyện mô hình machine learning trên bộ dữ liệu mới, sử dụng tham số và trọng số được học từ bộ dữ liệu huấn luyện gốc như điểm khởi đầu và (2) thích ứng domain là một lĩnh vực khác mà adapters được sử dụng, là quá trình điều chỉnh mô hình machine learning cho một domain mới, hoặc một lĩnh vực chủ đề hoặc ngữ cảnh cụ thể. Điều này có thể được thực hiện bằng cách tiếp tục huấn luyện mô hình trên bộ dữ liệu đại diện cho domain mới hoặc bằng cách thêm thông tin cụ thể domain vào mô hình, như embeddings từ hoặc đặc trưng cụ thể domain.

D. Language Adapters
Mục đích của language adapters là học các biến đổi cụ thể ngôn ngữ [19]. Chúng được huấn luyện trên dữ liệu không nhãn sử dụng hàm mục tiêu trừu tượng như mask language modeling (MLM). Chúng bao gồm down-projection và up-projection tại mỗi layer, với kết nối residual giữa chúng. Down-projection, D, là ma trận với kích thước h x d, trong đó h là kích thước ẩn của mô hình transformer và d là kích thước của adapter. Up-projection, U, là ma trận với kích thước d x h. Language adapter tại mỗi layer nhận trạng thái ẩn Transformer, hl, và residual, rl, và áp dụng down-projection và up-projection cho chúng, với hàm kích hoạt ReLU. Đầu ra của language adapter sau đó được thêm vào kết nối residual.

LanguageAdapter l(hl;rl) = Ul(ReLU (Dl(hl))) + rl (1)

E. AdapterFusion
AdapterFusion là phương pháp kết hợp kiến thức từ các language adapters khác nhau để cải thiện hiệu suất trên các tác vụ downstream như tóm tắt code [28]. Với một tập N language adapters, adapter fusion bao gồm việc lấy tổng có trọng số của đầu ra của những adapters này trong khi trọng số của mô hình được tiền huấn luyện và language adapters được cố định. AdapterFusion bao gồm các ma trận key, value, và query tại mỗi layer. Mục tiêu của AdapterFusion là tìm sự kết hợp tối ưu của language adapters bằng cách tối thiểu hóa hàm mất mát sử dụng tác vụ mục tiêu. Điều này cho phép trích xuất và cấu thành kiến thức từ các language adapters khác nhau để cải thiện hiệu suất trên các tác vụ downstream.

 = argminL(D; ;1;:::; N) (2)

trong đó  bao gồm các metrics Key l, Value l và Query l tại mỗi layer l, như được hiển thị trong Hình 1. Tại mỗi khối transformer, đầu ra của sub-layer feed-forward được lấy làm Query, và đầu ra của mỗi language adapter được sử dụng cho cả vector Key và Value.

III. NER ADAPTERS

NER adapter nhằm mục đích kết hợp thông tin token-type vào mạng, vì các loại token có thể cung cấp thông tin cú pháp có giá trị cho mô hình. Theo nghiên cứu trong [29], các loại token khác nhau có thể có mức độ quan trọng khác nhau đối với mô hình ngôn ngữ được tiền huấn luyện; ví dụ, định danh quan trọng hơn các loại khác về lượng attention từ mô hình và các đại diện đã học của chúng. Để huấn luyện mô hình trên các loại token code, trước tiên chúng ta cần xác định mỗi loại token trong bộ dữ liệu của mình. Sau đó chúng tôi giới thiệu một biến thể mới của adapters, adapters NER, mà chúng tôi cắm vào PPLM và huấn luyện chúng trên tác vụ phân loại token-type. Adapter NER được kỳ vọng sẽ dự đoán loại của mỗi token khi giai đoạn huấn luyện adapter hoàn thành. Chúng tôi thảo luận chi tiết về adapters NER trong ba phần phụ dưới đây: cấu trúc, hàm mục tiêu, và giai đoạn huấn luyện. Trong phần phụ cuối cùng, chúng tôi giải thích cách adapter NER được đặt trong các khối transformer của PPLM.

--- TRANG 3 ---
Multi-Head Attention
Add & Norm
Feed Forward
Adapter Fusion
Add & Norm
Add & Norm

Language Adapter    NER Adapter
Key Value Query

Hình 1: Kiến trúc đề xuất để tích hợp thông tin cú pháp vào các khối transformer bao gồm hai thành phần chính: language adapter và named entity recognition (NER) adapter. Những adapter này đã được huấn luyện riêng biệt trước khi được chèn vào một stack song song. Một module AdapterFusion được đặt trên đỉnh stack, được huấn luyện để thực hiện một tác vụ mục tiêu cụ thể, như tinh chỉnh code hoặc tóm tắt code. Điều này cho phép tích hợp và kết hợp kiến thức thu được từ các adapter bên dưới, dẫn đến hiệu suất chính xác và hiệu quả hơn.

A. Cấu trúc
Chúng tôi sử dụng cùng kiến trúc của language adapters cho adapters NER, vì vậy chúng bao gồm các khối feedforward down- và up-sampling kết hợp với kết nối residual:

NERAdapter l(hl;rl) = Ul(ReLU (Dl(hl))) + rl (3)

trong đó hl và rl lần lượt là trạng thái ẩn và residual tại layer l.

B. Token Type Classification Loss (TTC)
Với một loại token được gán cho mỗi token trong mẫu code, chúng tôi đặt vấn đề như một vấn đề phân loại token, trong đó adapter chịu trách nhiệm dự đoán loại của mỗi token. Để huấn luyện adapters NER, chúng tôi sử dụng hàm mất mát cross-entropy.

Gọi Xi = x1;:::;xN đại diện cho một chuỗi token ids cho mẫu i, T = t1;:::;tN chỉ ra chuỗi type ids tương ứng, và Y = y1;:::;yN là đại diện one-hot của những type ids này, với mỗi phần tử có kích thước của tổng số loại được trình bày trong bộ dữ liệu. Cross-entropy cho mẫu i được tính như sau:

LNER = ∑(t=1 to N) YT log(Pt) (4)

trong đó LNER chỉ ra hàm mất mát và Pt là phân phối xác suất cho các loại chúng ta nhận được từ mạng cho token t. Hàm mất mát này cho phép chúng ta đo lường sự khác biệt giữa các loại token được dự đoán và thực tế, cung cấp phương tiện để điều chỉnh tham số của mô hình tương ứng và cải thiện hiệu suất của nó.

C. Huấn luyện Adapters NER
Phần này giải thích các bước chúng ta cần để huấn luyện adapters NER.

1) Trích xuất NERs Tương ứng với Các Nút Lá:
Adapter NER yêu cầu được huấn luyện trên dữ liệu có nhãn của các loại token. Để trích xuất chính xác loại của mỗi token trong một mẫu code, chúng tôi sử dụng tree-sitter parser². Parser này được thiết kế đặc biệt cho phân tích ngôn ngữ và có thể xác định loại của mỗi từ trong nhiều ngôn ngữ lập trình. Tree-sitter được sử dụng trong các nghiên cứu trước đây, như trong GraphCodeBERT [9] để trích xuất dataflow từ AST và trong các thí nghiệm phân tích tĩnh trên Github [30]. Một ví dụ về AST được tạo cho một đoạn code nhất định được hiển thị trong Hình 3. Để có được các loại token, chúng tôi cung cấp mỗi đoạn code cho tree-sitter và sau đó duyệt AST để trích xuất loại của mỗi token code.

2) Mã hóa Token bằng Tokenizer và Gán Mỗi Sub-Token Cùng Loại Token:
Sử dụng các bước đã đề cập trước đó, chúng tôi trích xuất các loại token code. Tuy nhiên, do bản chất của tokenization, trong đó một từ duy nhất có thể được chia thành nhiều token, mối quan hệ một-một giữa từ và token không thể được giả định. Do đó, chúng ta phải thiết lập ánh xạ giữa các từ và các token tương ứng của chúng và sau đó gán cùng loại cho mỗi token. Ví dụ, tên hàm find\_bad\_files sẽ được đánh dấu là loại identifier bởi tree-sitter. Khi được tokenized, tên này có thể được chia thành ba token: "find", "bad," và "files." Trong trường hợp này, chúng ta sẽ đánh dấu tất cả chúng là loại identifier. Sau quá trình này, chúng ta có một danh sách các token và các loại tương ứng của chúng cho mỗi mẫu, cung cấp nền tảng vững chắc cho phân tích và xử lý tiếp theo.

3) Tinh chỉnh Adapters NER bằng Hàm Mất mát Phân loại Loại Token:
Bước cuối cùng là tích hợp adapters NER và tinh chỉnh trọng số của chúng sử dụng hàm mất mát phân loại loại token. Điều này được thực hiện trong khi giữ trọng số của mô hình được tiền huấn luyện cố định, cho phép adapters NER chuyên môn hóa trong việc nhận dạng các thực thể có tên cụ thể trong khi tận dụng hiểu biết ngôn ngữ chung của mô hình được tiền huấn luyện. Phương pháp này cho phép cải thiện hiệu suất trong việc xác định các thực thể có tên trong văn bản trong khi duy trì độ chính xác tổng thể của mô hình.

²https://tree-sitter.github.io/tree-sitter/

Hình 2: Một ví dụ về đoạn code java được gửi đến tree-sitter để trích xuất AST tương ứng của nó.

D. Tổng quan Kiến trúc
Kiến trúc đề xuất để tích hợp thông tin cú pháp vào các khối transformer được minh họa trong Hình 1. Kiến trúc này bao gồm hai thành phần chính: language adapter và named entity recognition (NER) adapter. Chúng tôi huấn luyện những adapter này riêng biệt trước khi chèn chúng vào một stack song song, như được hiển thị trong hình. Language adapter được huấn luyện trên hàm mất mát mask language modeling. Language adapter được sử dụng để học kiến thức ngôn ngữ chung nằm dưới bộ dữ liệu. Cuối cùng, chúng tôi sử dụng AdapterFusion để kết hợp kiến thức thu được từ language và NER adapters để cung cấp một đại diện đầu ra mạnh mẽ hơn tại mỗi khối transformer. Module AdapterFusion được đặt trên đỉnh stack và được huấn luyện để thực hiện một tác vụ mục tiêu cụ thể. Những tác vụ này trong thí nghiệm của chúng tôi là tinh chỉnh code và tóm tắt code. Sử dụng AdapterFusion cho phép tích hợp và kết hợp kiến thức thu được từ các adapter bên dưới, dẫn đến hiệu suất chính xác và hiệu quả hơn.

Chi tiết của dữ liệu đầu vào và cách embeddings của sub-token và adapter được cấu thành trong AdapterFusion được hiển thị thông qua một ví dụ trong Hình 4. Mẫu code đầu vào được trình bày trong Hình 2, và Hình 4 minh họa chi tiết. Mẫu code được cung cấp vào một khối transformer được trang bị NER và language adapters. Trước embeddings đầu vào, các từ được chia thành sub-token khi cần thiết. Những embeddings sub-token này, ký hiệu bởi Etokens, sau đó được truyền qua cả NER và language adapters song song. Điều này dẫn đến hai embeddings cho mỗi sub-token, ký hiệu bởi Ttoken và Ltoken, tương ứng với NER và language adapters. Cuối cùng, những embeddings này được cấu thành bởi AdapterFusion để chọn thông tin hữu ích từ các embeddings trước đó cho một tác vụ mục tiêu cụ thể, như tinh chỉnh code. Embeddings cuối cùng được ký hiệu bởi Ftoken.

IV. THIẾT LẬP THÍ NGHIỆM
Chúng tôi tiến hành thí nghiệm sử dụng CodeBERT làm mô hình backbone cho tinh chỉnh code và tóm tắt code. Tác vụ trước nhằm mục đích xác định và sửa lỗi tự động, và tác vụ sau đề cập đến việc tự động tạo mô tả về chức năng của một đoạn code nhất định bằng ngôn ngữ tự nhiên. CodeBERT được chọn vì nó đã được nghiên cứu và đánh giá trong một số công trình kỹ thuật phần mềm trước đây [1], [31], [11]. Tinh chỉnh code được chọn vì nó phụ thuộc rất nhiều vào thông tin cú pháp để xác định và sửa lỗi và tối ưu hóa cấu trúc code, và tóm tắt code được chọn vì nó cho phép chúng ta đánh giá khả năng sinh của các mô hình và đánh giá kiến thức ngữ nghĩa nằm dưới các khía cạnh lập trình và ngôn ngữ tự nhiên của PPLM. Sau đây, chúng tôi giải thích

--- TRANG 4 ---
Left:
Identifier: i
Right:
call
Identifier: range
Argument List
Integer: 5
Block
Expression Statement
Call
Identifier: print
Argument List
Identifier: i
For Statement

Hình 3: Một ví dụ về AST được tạo bởi tree-sitter parser cho code đầu vào của Hình 2. Các nút lá được đại diện bởi các vòng tròn màu xanh, trong khi các nút không phải lá được đại diện bởi các vòng tròn màu xanh lá. AST này được tạo ra khi đoạn code được cung cấp vào parser.

các tác vụ downstream và bộ dữ liệu chúng tôi sử dụng để thực hiện thí nghiệm, mô hình baseline, và các metric đánh giá cho mỗi tác vụ downstream.

A. Tác vụ Downstream và Bộ dữ liệu
Tinh chỉnh Code là một khía cạnh thiết yếu của phát triển phần mềm và một bước quan trọng trong việc đảm bảo tính mạnh mẽ và độ tin cậy của hệ thống phần mềm [9]. Bằng cách tận dụng các kỹ thuật và công cụ khác nhau, nó nhằm mục đích tự động xác định và sửa lỗi trong code, do đó giảm đáng kể chi phí và nỗ lực liên quan đến việc giải quyết chúng thủ công. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng bộ dữ liệu được phát hành bởi Tufano et al. [10] để đánh giá hiệu quả của kiến trúc đề xuất và adapters NER trên tinh chỉnh code. Chúng tôi thực hiện thí nghiệm trên bộ dữ liệu BFP small. Tổng số điểm dữ liệu trong tập huấn luyện, test, và validation lần lượt là 46680, 5835 và 5835.

Tóm tắt Mã nguồn là một kỹ thuật mạnh mẽ trong kỹ thuật phần mềm nhằm mục đích cung cấp mô tả ngôn ngữ tự nhiên về chức năng của mã nguồn, do đó tạo điều kiện cho việc hiểu và bảo trì code [26]. Mục tiêu của tóm tắt mã nguồn là làm cho việc hiểu cấu trúc và chức năng tổng thể của code trở nên dễ dàng hơn. Tác vụ này được nghiên cứu rộng rãi, và các phương pháp khác nhau được đề xuất, đặc biệt sử dụng các mô hình ngôn ngữ lập trình như CodeBERT [1], CodeT5 [3], và Multilingual CodeBERT [32] để tóm tắt mã nguồn tự động.

Chúng tôi chọn tinh chỉnh code vì đây là tác vụ trừu tượng và phức tạp hơn so với suy luận loại khi chúng ta có adapters NER. Cụ thể hơn, chúng tôi cung cấp thông tin AST để đánh giá mô hình trên các tác vụ phức tạp hơn thay vì suy luận loại. Chúng tôi chọn tóm tắt code vì đây là tác vụ sinh, và mục tiêu là đánh giá mức độ mà thông tin cấu trúc đề xuất của chúng tôi có thể hữu ích cho các tác vụ mục tiêu sinh NL-PL.

Nghiên cứu của chúng tôi sử dụng CodeSearchNet [22] làm bộ dữ liệu để huấn luyện language và NER adapters. Bộ dữ liệu bao gồm sáu ngôn ngữ lập trình. Kích thước của mỗi ngôn ngữ được thể hiện trong Bảng I. Vì tree-sitter xử lý toàn bộ đoạn code PHP như một phần tử văn bản duy nhất và không cung cấp thông tin cú pháp, chúng tôi loại trừ PHP khỏi thí nghiệm. Thay vào đó, chúng tôi sẽ tập trung vào việc sử dụng tree-sitter cho các ngôn ngữ sau: Go, Java, JavaScript, Python, và Ruby. Cụ thể hơn, đối với tinh chỉnh code, language và NER adapters được huấn luyện trên split huấn luyện của bộ dữ liệu Java unimodal trong CodeSearchNet, và AdapterFusion được huấn luyện trên bộ dữ liệu sửa lỗi Java được giới thiệu trong [10]. Đối với tóm tắt code, language và NER adapters được huấn luyện trên split huấn luyện của dữ liệu unimodal cho tất cả ngôn ngữ (kết hợp) có sẵn trong CodeSearchNet. AdapterFusion sau đó được huấn luyện trên các split huấn luyện tổng hợp của dữ liệu bimodal (tức là, code và comment) cho tất cả ngôn ngữ được bao gồm trong CodeSearchNet, cho phép phương pháp toàn diện và đa ngôn ngữ để tóm tắt code.

NER và language adapters được huấn luyện trong 50,000 bước huấn luyện với batch size 48 và learning rate 2e-5. Đối với tinh chỉnh code, AdapterFusion được huấn luyện trong 100,000 bước huấn luyện với batch size 16 và learning rate 5e-5. Đối với tóm tắt code, chúng tôi huấn luyện AdapterFusion trong 50,000 bước huấn luyện với batch size 32.

B. Mô hình Baseline
Phương pháp của chúng tôi được thiết kế để mô-đun và linh hoạt, cho phép tích hợp dễ dàng các mô hình ngôn ngữ lập trình được tiền huấn luyện khác nhau. Điều này cho phép thí nghiệm và so sánh dễ dàng các mô hình khác nhau và hiệu suất của chúng trên các tác vụ khác nhau.

Trong nghiên cứu của chúng tôi, chúng tôi chọn CodeBERT [1] làm mô hình backbone và baseline. Mô hình này được tiền huấn luyện trên bộ dữ liệu CodeSearchNet [22] và được nghiên cứu rộng rãi trong kỹ thuật phần mềm. CodeBERT là mô hình đa ngôn ngữ được tinh chỉnh trên các bộ dữ liệu đơn ngôn ngữ.

Để đánh giá hiệu quả của phương pháp của chúng tôi, chúng tôi so sánh kết quả của việc tinh chỉnh kiến trúc đề xuất khi nó được chèn vào CodeBERT với kết quả của việc tinh chỉnh CodeBERT trên hai tác vụ. Điều này cho phép chúng ta đo lường sự cải thiện tiềm năng trong hiệu suất và hiệu quả của nó khi được áp dụng cho các tác vụ tinh chỉnh và tóm tắt code. Ngoài ra, so sánh này cung cấp hiểu biết về điểm mạnh và hạn chế của phương pháp đề xuất và cách nó so sánh với mô hình baseline. Đối với mỗi tác vụ, các phương pháp/mô hình khác được sử dụng để so sánh, được giải thích trong phần Kết quả để tiết kiệm không gian và giảm dư thừa.

C. Metric Đánh giá
Tác vụ tinh chỉnh code được đánh giá sử dụng sự kết hợp của điểm BLEU-4 và đo lường độ chính xác. Điểm BLEU là một metric đánh giá được chấp nhận rộng rãi trong xử lý ngôn ngữ tự nhiên, được sử dụng để đo lường sự tương tự giữa đầu ra được tạo và câu trả lời đúng. Ngoài ra, chúng tôi đo lường độ chính xác, đại diện cho tỷ lệ mẫu mà mô hình có thể sửa đúng. Đối với tinh chỉnh code, chúng tôi tuân theo CodeXGLUE và báo cáo cả điểm BLEU và độ chính xác để phản ánh hiệu suất của mô hình, vì chỉ xem xét một trong những điểm này có thể gây hiểu lầm. Ví dụ, điểm BLEU của "naive copy" nhiều hơn điểm BLEU của tất cả baseline. Mục tiêu cuối cùng của quá trình đánh giá của chúng tôi là tối ưu hóa đồng thời cả hai metric này, phấn đấu cho mức độ cao của cả sự tương tự và độ chính xác sửa chữa.

Tác vụ tóm tắt code được đánh giá sử dụng điểm BLEU (tức là, Bilingual Evaluation Understudy), một đo lường dựa trên precision để đánh giá hiệu suất của các mô hình NLP, đặc biệt là hệ thống dịch máy. Nó hoạt động bằng cách so sánh đầu ra của các mô hình ngôn ngữ (ví dụ, tóm tắt được tạo) với tóm tắt tham chiếu được tạo bởi chuyên gia con người và tính toán mức độ chồng chéo giữa hai bên. Trong công trình này, chúng tôi tính điểm BLEU-4 [33] trong đó thuật toán trước tiên xác định unigrams đến 4-grams trong cả đầu ra và bản dịch tham chiếu và sau đó tính precision giữa tóm tắt được tạo (tức là, n-gram hit) và tóm tắt ground truth (tức là, tổng số n-gram count). Điểm BLEU cao hơn chỉ ra rằng đầu ra của mô hình tương tự hơn với bản dịch tham chiếu và do đó được coi là chất lượng cao hơn. BLEU thường được sử dụng trong tóm tắt code để đánh giá và so sánh chất lượng và tính mạch lạc của các tóm tắt được tạo từ các PPLMs khác nhau [26], [3], [9], [5].

--- TRANG 5 ---
for ( int i = 0 ; i < 0 ; i ++ )    Input Words
for ( int i = 0 ; i < 0 ; i ++ )    Sub-Tokens    f System .out. println
f Sys .out. print tem ln

Transformer Block
Efor E( Eint Ei E= E0 E; Ei E< E0 E; Ei E++ E) Ef ESys E. Eout E. Eprint Etem Eln

Lfor L( Lint Li L= L0 L; Li L< L0 L; Li L++ L) Lf LSys L. Lout L. Lprint Ltem Lln

Tfor T( Tint Ti T= T0 T; Ti T< T0 T; Ti T++ T) Tf TSys T. Tout T. Tprint Ttem Tln

...

Language Adapter    NER Adapter

AdapterFusion
Embeddings

Language &
Type
Embeddings

Ffor F( Fint Fi F= F0 F; Fi F< F0 F; Fi F++ F) Ff FSys F. Fout F. Fprint Ftem Fln

Fusion
Embeddings

Hình 4: Luồng dữ liệu đầu vào cho mẫu được hiển thị trong Hình 2 tiến hành như sau khi được cung cấp vào một khối transformer được trang bị NER và language adapters. Trước embeddings đầu vào, các từ được chia thành sub-token khi cần thiết. Những embeddings sub-token này, ký hiệu bởi Etokens, sau đó được truyền qua cả NER và language adapters song song. Điều này dẫn đến hai embeddings cho mỗi sub-token, ký hiệu bởi Ttoken và Ltoken, tương ứng với NER và language adapters. Cuối cùng, những embeddings này được cấu thành bởi AdapterFusion để chọn thông tin hữu ích từ các embeddings trước đó cho một tác vụ mục tiêu cụ thể, như tinh chỉnh code. Embeddings cuối cùng được ký hiệu bởi Ftoken.

BẢNG I: Thống kê của bộ dữ liệu CodeSearchNet cho năm ngôn ngữ trong thí nghiệm của chúng tôi [22]

Language    Bimodal Data    Unimodal Data
Go          317,832         726,768
Java        500,754         1,569,889
JavaScript  143,252         1,857,835
Python      458,219         1,156,085
Ruby        52,905          164,048

điểm tương tự giữa đầu ra được tạo và câu trả lời đúng. Ngoài ra, chúng tôi đo lường độ chính xác, đại diện cho tỷ lệ mẫu mà mô hình có thể sửa đúng. Đối với tinh chỉnh code, chúng tôi tuân theo CodeXGLUE và báo cáo cả điểm BLEU và độ chính xác để phản ánh hiệu suất của mô hình, vì chỉ xem xét một trong những điểm này có thể gây hiểu lầm. Ví dụ, điểm BLEU của "naive copy" nhiều hơn điểm BLEU của tất cả baseline. Mục tiêu cuối cùng của quá trình đánh giá của chúng tôi là tối ưu hóa đồng thời cả hai metric này, phấn đấu cho mức độ cao của cả sự tương tự và độ chính xác sửa chữa.

Tác vụ tóm tắt code được đánh giá sử dụng điểm BLEU (tức là, Bilingual Evaluation Understudy), một đo lường dựa trên precision để đánh giá hiệu suất của các mô hình NLP, đặc biệt là hệ thống dịch máy. Nó hoạt động bằng cách so sánh đầu ra của các mô hình ngôn ngữ (ví dụ, tóm tắt được tạo) với tóm tắt tham chiếu được tạo bởi chuyên gia con người và tính toán mức độ chồng chéo giữa hai bên. Trong công trình này, chúng tôi tính điểm BLEU-4 [33] trong đó thuật toán trước tiên xác định unigrams đến 4-grams trong cả đầu ra và bản dịch tham chiếu và sau đó tính precision giữa tóm tắt được tạo (tức là, n-gram hit) và tóm tắt ground truth (tức là, tổng số n-gram count). Điểm BLEU cao hơn chỉ ra rằng đầu ra của mô hình tương tự hơn với bản dịch tham chiếu và do đó được coi là chất lượng cao hơn. BLEU thường được sử dụng trong tóm tắt code để đánh giá và so sánh chất lượng và tính mạch lạc của các tóm tắt được tạo từ các PPLMs khác nhau [26], [3], [9], [5].

V. KẾT QUẢ VÀ THẢO LUẬN

A. Hiệu suất của Adapters NER
Bảng II trình bày các metric đánh giá của hàm mất mát phân loại loại token, cụ thể là Precision, Recall, F1 score, và Accuracy, trong giai đoạn huấn luyện adapter NER. Precision là metric đo lường tỷ lệ dự đoán đúng dương tính trong tất cả dự đoán dương tính. Recall đo lường tỷ lệ dự đoán đúng dương tính từ tất cả quan sát dương tính thực tế. F1 Score là trung bình điều hòa của Precision và Recall, và Accuracy là metric đo lường tỷ lệ dự đoán đúng trong tất cả quan sát. Những adapters NER này được huấn luyện trên các loại thu được từ tree-sitter parser trên bộ dữ liệu CodeSearchNet.

Các ngôn ngữ lập trình trong bảng được sắp xếp theo thứ tự kích thước bộ dữ liệu, với các ngôn ngữ ít tài nguyên nhất được liệt kê trước và các ngôn ngữ nhiều tài nguyên nhất được liệt kê sau. Python đạt hiệu suất tốt nhất cho tất cả metric so với các ngôn ngữ khác, và Ruby có hiệu suất thấp nhất vì nó gặp khó khăn từ bộ dữ liệu ít tài nguyên. Đáng chú ý rằng precision, recall, và F1 scores phụ thuộc rất nhiều vào kích thước của bộ dữ liệu, chỉ ra rằng các bộ dữ liệu lớn hơn thường

--- TRANG 6 ---
BẢNG II: Bảng trình bày các metric đánh giá, bao gồm Precision, Recall, F1 Score, và Accuracy, cho giai đoạn huấn luyện của adapters NER. Các ngôn ngữ lập trình được bao gồm trong bảng được tổ chức theo thứ tự tài nguyên có sẵn, với những ngôn ngữ có ít tài nguyên nhất có sẵn (tức là, kích thước của bộ dữ liệu) được liệt kê trước và những ngôn ngữ có nhiều tài nguyên nhất được liệt kê sau. Những metric này cung cấp đánh giá toàn diện về hiệu suất của adapters NER.

Languages    Precision    Recall    F1 Score    Accuracy
Ruby         0.68         0.65      0.66        0.92
JavaScript   0.78         0.79      0.78        0.91
Go           0.78         0.82      0.80        0.94
Python       0.95         0.94      0.94        0.98
Java         0.78         0.79      0.78        0.89

mang lại kết quả tốt hơn cho huấn luyện adapter NER. Tuy nhiên, kết quả cho Java tệ hơn so với các ngôn ngữ khác, mặc dù có bộ dữ liệu tương đối phong phú. Sự khác biệt này có thể được quy cho độ phức tạp cú pháp có mặt trong Java so với Python, góp phần vào hiệu suất thấp hơn của nó. Mặt khác, Python thể hiện hiệu suất xuất sắc trên tất cả các metric được đánh giá. Mức độ chính xác cao được ghi nhận cho tất cả ngôn ngữ lập trình tiếp tục chứng minh rằng adapters NER được huấn luyện hiệu quả để trích xuất thông tin cú pháp từ embeddings nội bộ của mạng. Điều này làm nổi bật hiệu quả và tính mạnh mẽ của việc triển khai adapter NER của chúng tôi.

B. Kết quả của CodeBERTER cho Tinh chỉnh Code
Chúng tôi đánh giá hiệu suất của adapters NER trên tác vụ tinh chỉnh code để kiểm tra mức độ chúng có thể hiệu quả trên một tác vụ lập trình mục tiêu. Kết quả được trình bày trong Bảng III. Naive copy áp dụng phương pháp đơn giản bằng cách sao chép trực tiếp code có lỗi như kết quả tinh chỉnh. LSTM sử dụng kiến trúc Long Short Term Memory, và mô hình Transformer sử dụng 12 khối encoder transformer (tức là, cùng số lượng layer và kích thước ẩn với các mô hình được tiền huấn luyện). Đối với Transformer, encoder được khởi tạo với các mô hình được tiền huấn luyện, trong khi tham số của decoder được khởi tạo ngẫu nhiên. Kết quả mô tả trong bảng chứng minh rằng Transformer vượt trội đáng kể so với mô hình LSTM.

Chúng tôi tách kết quả của những mô hình này trong Bảng III khỏi nhóm mô hình thứ hai tận dụng tiền huấn luyện trên ngôn ngữ lập trình. Điểm BLEU và Accuracy cao hơn đối với các mô hình có tiền huấn luyện. RoBERTa (code) chỉ được tiền huấn luyện trên code. NSEdit [34] đề xuất mạng pointer mới cho các khối transformer để dự đoán vị trí chỉnh sửa. CoTexT [35] là mô hình encoder-decoder lập trình bimodal được tiền huấn luyện. Trong số các mô hình được tiền huấn luyện lập trình, phương pháp của chúng tôi, ký hiệu bởi CodeBERTER, nổi bật bằng cách đạt hiệu suất tốt nhất trên điểm BLEU. Lưu ý rằng CodeBERTER cải thiện kết quả của CodeBERT mà không tận dụng tinh chỉnh tất cả tham số của nó. Điều này minh họa tác động có lợi của việc tích hợp thông tin cấu trúc code trong tinh chỉnh code.

C. Kết quả của CodeBERTER cho Tóm tắt Code
Tóm tắt code đánh giá các khía cạnh ngữ nghĩa và cú pháp của mô hình ngôn ngữ lập trình. Bảng IV hiển thị

BẢNG III: Kết quả điểm BLEU và Accuracy của adapter NER trên tác vụ tinh chỉnh code. Các baseline được chia bởi đường ngang, trong đó nhóm dưới hiển thị các mô hình được tiền huấn luyện trên ngôn ngữ lập trình. Font đậm chỉ ra kết quả tốt nhất, thuộc về CodeBERTER.

Method/Model         BLEU     Accuracy
Naive copy           78.06    0.0
LSTM                 76.76    10.0
Transformer          77.21    14.7
RoBERTa (code)       77.30    15.9
CodeBERT             77.42    16.4
CodeBERTER           78.2     17.8
CoTexT               77.91    22.64
NSEdit               71.06    24.04

kết quả của phương pháp chúng tôi so với các mô hình khác. Đối với tác vụ mục tiêu này, chúng tôi chọn mô hình tinh chỉnh đa ngôn ngữ dựa trên các phát hiện và khuyến nghị gần đây trong [32], nơi các tác giả đề cập rằng tóm tắt code có thể hưởng lợi từ tinh chỉnh đa ngôn ngữ. Có nghĩa là, trong phương pháp của chúng tôi, đầu tiên, chúng tôi huấn luyện language và NER adapters trên bộ dữ liệu đa ngôn ngữ, như đã đề cập trước đó. Chúng tôi cũng huấn luyện AdapterFusion trên bộ dữ liệu tóm tắt code đa ngôn ngữ và kiểm tra nó trên từng ngôn ngữ riêng biệt. Lưu ý rằng đối với tinh chỉnh code, vì chỉ tồn tại bộ dữ liệu Java, theo hiểu biết tốt nhất của chúng tôi, chúng tôi chỉ có thể đánh giá hiệu suất của adapters NER trên tinh chỉnh code trong môi trường đơn ngôn ngữ.

Bảng IV hiển thị điểm BLUE-4 của các phương pháp hiện đại và phương pháp của chúng tôi, ký hiệu bởi CodeBERTER, được tách bởi đường ngang. Điểm tốt nhất được hiển thị bằng font đậm. Điểm thu được từ CodeBERT cũng được gạch chân để dễ đọc. CodeBERT là baseline chính chúng ta cần so sánh kết quả với vì mô hình backbone của chúng tôi là CodeBERT. CodeBERTER cải thiện điểm cho bốn ngôn ngữ, Ruby, JavaScript, Go, và Java, và có kết quả ngang bằng với các mô hình tốt nhất cho Python. Nhìn vào thống kê của dữ liệu được cung cấp trong Bảng I chỉ ra rằng ba ngôn ngữ đầu tiên có số lượng dữ liệu thấp hơn để huấn luyện so với hai ngôn ngữ còn lại. Đối với những ngôn ngữ ít tài nguyên này, mô hình của chúng tôi cải thiện đáng kể kết quả so với baseline của nó, CodeBERT.

polyglot CodeBERT và polyglot GraphCodeBERT, như được mô tả trong [32] bởi Ahmed et al. (2021), là các mô hình đã được tinh chỉnh đầy đủ trong môi trường đa ngôn ngữ. Ví dụ, để đánh giá hiệu suất của chúng trên Ruby, các mô hình được tinh chỉnh trên tất cả ngôn ngữ lập trình trong bộ dữ liệu CodeSearchNet [22] và sau đó được đánh giá trên một bộ dữ liệu test Ruby. Phương pháp này đã cải thiện hiệu suất của các mô hình polyglot, như được chứng minh bởi sự khác biệt trong hiệu suất giữa CodeBERT và polyglot CodeBERT. Tuy nhiên, CodeBERTER bổ sung cải thiện điểm của polyglot CodeBERT, được quy cho việc tích hợp thông tin cú pháp đa ngôn ngữ vào mô hình sử dụng adapters NER.

Đối với các ngôn ngữ có nhiều tài nguyên, Python và Java, CodeBERTER hoạt động ngang bằng với mô hình baseline (cải thiện kết quả Java từ CodeBERT lên 2 điểm BLEU). Chúng tôi liên kết điều này với thực tế rằng đối với các ngôn ngữ có nhiều tài nguyên, mô hình ngôn ngữ có thể học kiến thức chung nằm dưới một ngôn ngữ tốt hơn so với ngôn ngữ ít tài nguyên, mà có ít dữ liệu huấn luyện tồn tại [39]. Do đó, adapter NER cho phép mô hình cung cấp thêm thông tin loại cho các ngôn ngữ ít tài nguyên.

Mặc dù CodeBERTER có điểm tốt nhất cho một số ngôn ngữ, lưu ý rằng CodeBERTER sử dụng CodeBERT làm backbone, vì vậy sẽ công bằng hơn khi so sánh hiệu quả của mô hình chúng tôi với CodeBERT chứ không phải các mô hình lớn hơn như CodeT5. Những mô hình lớn hơn này có gấp đôi tham số so với CodeBERT. Tuy nhiên, chúng tôi xem xét các mô hình lớn hơn trong Bảng IV để chứng minh rằng phương pháp của chúng tôi thậm chí có thể có kết quả ngang bằng với các mô hình lớn hơn cho từng ngôn ngữ.

Theo bảng xếp hạng trên CodeXGLUE, chúng tôi cũng báo cáo điểm trung bình trong số các mô hình trong Bảng IV. Nếu chúng ta tính trung bình trên các ngôn ngữ được nghiên cứu cho tóm tắt code, hiệu suất CodeBERTER là 18.738, cao hơn tất cả các mô hình khác trên bảng xếp hạng cho tác vụ này.

D. Thảo luận
Hiệu quả tính toán của phương pháp chúng tôi. Đối với tất cả ngôn ngữ, chúng tôi huấn luyện adapters NER hiệu quả hơn với chi phí tính toán thấp hơn. Mỗi language và NER adapters có 0.9 triệu và AdapterFusion có 21 tham số có thể huấn luyện. Chúng tôi huấn luyện ba adapters này cho tinh chỉnh code tổng cộng, dẫn đến 23 triệu tham số có thể huấn luyện. Con số này vẫn ít hơn tổng tham số có thể huấn luyện của mô hình CodeBERT, là 110 triệu. Lưu ý rằng tất cả 110 triệu tham số trong CodeBERT phải được huấn luyện lại trong giai đoạn tinh chỉnh đầy đủ tiêu chuẩn. Tuy nhiên, điều này không cần thiết trong phương pháp của chúng tôi. Trong CodeBERTER, chúng tôi huấn luyện 23 triệu tham số cho tất cả adapters, bao gồm language, NER, và AdapterFusion. Về hiệu quả thời gian, mỗi adapter NER mất khoảng 10 giờ để được huấn luyện, cho 40,000 bước huấn luyện. Điều này trong khi nếu chúng ta tinh chỉnh đầy đủ CodeBERT cho tác vụ NER với cùng số lượng bước huấn luyện, sẽ mất khoảng 17 giờ.

Hiệu ứng của adapter NER. Hình 5 minh họa các mẫu attention của các token khác nhau trong một mẫu code trên nhau trong layer cuối của CodeBERT, được tạo bởi bertviz³. Độ mạnh của các đường giữa các token thể hiện lượng attention; đường càng dày, attention được đặt trên token đó càng nhiều. Chúng tôi chỉ hiển thị một attention head (trong số 12 heads) của layer cuối để tránh đại diện lộn xộn trong hình này. Hình 5 so sánh các mẫu attention cho một mẫu Java, cho CodeBERT bên trái, và cho CodeBERTER (tức là, CodeBERT với adapters NER) bên phải. Như được hiển thị, việc sử dụng adapters NER dẫn đến sự thay đổi trong mẫu attention, có nghĩa là nó được phân bố nhiều hơn giữa các token. Cụ thể, trong CodeBERTER, adapter NER đặt trọng tâm lớn hơn vào các token định danh như sum, a, và b, so với mô hình baseline. Điều này cho phép mô hình tận dụng attention bổ sung được dành cho những token này cho các tác vụ downstream, dẫn đến hiệu suất được cải thiện.

Lưu ý rằng attention CodeBERT hiển thị nhiều cross attention yếu giữa các token, và hầu hết attention nằm trên token <s>. Điều này phù hợp với các phát hiện trước đây của Sharma et al. [29], nơi các tác giả phát hiện rằng các token đặc biệt (ví dụ, <s>) nhận được lượng attention nhiều nhất đặc biệt trong layer cuối. Vì vậy, như đã được hiển thị trước đây trong [29], các kỹ thuật mới được yêu cầu để hướng attention của các mô hình dựa trên BERT về phía các token khác cho ngôn ngữ lập trình. Điều này được đạt được trong công trình của chúng tôi sử dụng adapters NER.

³https://github.com/jessevig/bertviz

Hình 5: Hình minh họa các mẫu attention của layer cuối của CodeBERT cho một mẫu Java, có (hình bên phải) và không có (hình bên trái) việc chèn adapters NER vào mô hình được tiền huấn luyện. So sánh này làm nổi bật tác động của adapters NER đối với khả năng xác định và trích xuất các thực thể liên quan trong văn bản mẫu của mô hình.

--- TRANG 7 ---
VI. CÔNG TRÌNH LIÊN QUAN
Trong những năm gần đây, đã có nhiều nghiên cứu tập trung vào học đại diện code cho các tác vụ kỹ thuật phần mềm khác nhau như sinh code [40], [41], [42], tóm tắt code [4], [5], [43], tổng hợp chương trình [44], [45], [46], [47], tìm kiếm code [48], và sửa lỗi [49], [50]. Với sự ra đời của các mô hình ngôn ngữ được tiền huấn luyện trong NLP, các nhà nghiên cứu trong kỹ thuật phần mềm đã tận dụng những tiến bộ trong các mô hình ngôn ngữ được tiền huấn luyện trong NLP để đề xuất các mô hình ngôn ngữ được tiền huấn luyện lập trình được tiền huấn luyện trên bộ dữ liệu unimodal (tức là, code) hoặc bimodal (tức là, code và comment).

CodeBERT [1], GraphCodeBERT [9] và CodeT5 [3] là ví dụ của phương pháp này. Mặc dù mã nguồn chứa thông tin cú pháp và ngữ nghĩa phong phú, hầu hết PPLMs xử lý code như một chuỗi token. GraphCodeBERT [9] mở rộng đầu vào với dataflow được trích xuất từ AST. Điều này cho phép mô hình hiểu rõ hơn các mối quan hệ và phụ thuộc giữa các phần khác nhau của code. TreeBERT [14] đại diện đầu vào như một đường dẫn nối tiếp của các lá AST tương ứng với mỗi mẫu code. Điều này cho phép mô hình nắm bắt tốt hơn cấu trúc và thứ bậc của code. CodeT5 [3] là mô hình học đại diện code dựa trên kiến trúc T5. Mô hình được tiền huấn luyện trên corpus code lớn để dự đoán token tiếp theo trong chuỗi code. Để cải thiện hiểu biết của mô hình về code, một hàm mục tiêu mới đã được đề xuất trong CodeT5 để thông báo cho mô hình về sự hiện diện của token định danh trong giai đoạn tiền huấn luyện. Mô hình được đánh giá trên một số tác vụ liên quan đến code như sinh code, tóm tắt code, và tìm kiếm code và cho thấy hiệu suất được cải thiện so với các mô hình hiện có khác.

Adapters đã được sử dụng rộng rãi trong NLP [17], [19], [28] như một phương pháp tinh chỉnh nhanh và hiệu quả tham số. Adapters trước đây được sử dụng trong kỹ thuật phần mềm để cho thấy khả năng chuyển giao của PLMs ngôn ngữ tự nhiên sang ngôn ngữ lập trình với chi phí huấn luyện thấp hơn. Các tác giả huấn luyện language adapters (tức là, adapters được huấn luyện trên mask language modeling trên dữ liệu không nhãn) và task adapters (tức là, adapters được huấn luyện cho một tác vụ mục tiêu trên dữ liệu có nhãn) cho phát hiện bản sao code [18].

Sự khác biệt của các nghiên cứu hiện tại với công trình của chúng tôi. Mặc dù thông tin cú pháp được sử dụng trong một số PPLMs hiện tại, không có mô hình nào có thể tích hợp thông tin này vào mô hình hiện có mà không yêu cầu tiền huấn luyện từ đầu. Mặt khác, adapters NER cho phép áp đặt thông tin cú pháp trong các mô hình hiện tại mà không cần tiền huấn luyện. Hơn nữa, mặc dù adapters được sử dụng rộng rãi trong NLP và được nghiên cứu gần đây trong kỹ thuật phần mềm, không có công trình nào sử dụng adapters trong bối cảnh như chúng tôi làm trong công trình của mình. Phương pháp của chúng tôi mới không chỉ cho hàm mất mát TTC mà chúng tôi giới thiệu cho adapters NER mà còn cho cách chúng được sử dụng (tức là, kiến trúc đề xuất trong Hình 1).

VII. MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
Tính hợp lệ bên ngoài Trong nghiên cứu này, chúng tôi đánh giá kết quả của việc áp đặt thông tin cú pháp cho tóm tắt code trên bộ dữ liệu CodeSearchNet và tinh chỉnh code cho ngôn ngữ Java. Tác vụ và ngôn ngữ lập trình bị hạn chế, và kết quả có thể không khái quát hóa được cho tất cả tác vụ downstream và bộ dữ liệu khác. Tuy nhiên, dựa trên quan sát của chúng tôi và thực tế rằng chúng tôi cung cấp thông tin ngôn ngữ và cấu trúc thông qua adapters, chúng tôi giả thuyết rằng kết quả cho các tác vụ khác sẽ ít nhất ngang bằng với các mô hình hiện tại, nhấn mạnh rằng điều này có thể đạt được với ít tham số có thể huấn luyện hơn. Mặc dù vẫn cần thí nghiệm để xác nhận điều này. Lưu ý rằng phương pháp của chúng tôi không bị hạn chế với CodeBERT và adapters NER có thể được sử dụng trong các PPLMs khác.

Đối với tinh chỉnh code, tồn tại các phương pháp khác nhau đã được đề xuất trong tài liệu. Một đóng góp đáng chú ý là việc sử dụng beam search, đã được nghiên cứu thực nghiệm bởi Tufano et al. [10]. Tuy nhiên, vì nghiên cứu của chúng tôi tập trung vào một khía cạnh khác của tinh chỉnh code, chúng tôi không khám phá phương pháp cụ thể này một cách sâu sắc.

Tính hợp lệ nội bộ Siêu tham số có thể ảnh hưởng đến giai đoạn tinh chỉnh của mô hình được tiền huấn luyện, và không có quy tắc cứng nào để chọn giá trị tốt nhất cho những tham số này. Do đó, mô hình có thể bị mắc kẹt trong các giải pháp tối ưu cục bộ. Vì Pfeiffer et al. [28] đã thực hiện tìm kiếm siêu tham số mở rộng trên adapters, chúng tôi xem xét cài đặt mặc định cho các siêu tham số của adapters. Tuy nhiên, nó được thực hiện trong lĩnh vực NLP và có thể không dẫn đến trạng thái tối ưu trong lĩnh vực kỹ thuật phần mềm.

--- TRANG 8 ---
VIII. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong nghiên cứu này, chúng tôi giới thiệu adapters NER, một phương pháp mới để tăng cường các mô hình được tiền huấn luyện hiện có bằng cách áp đặt thông tin cú pháp. Để đánh giá hiệu suất của chúng, chúng tôi tiến hành thí nghiệm trên hai tác vụ liên quan đến lập trình: tinh chỉnh code sử dụng bộ dữ liệu Java trong môi trường đơn ngôn ngữ và tóm tắt code sử dụng phương pháp đa ngôn ngữ. Kết quả của chúng tôi cho thấy rằng CodeBERTER –CodeBERT với adapters NER– vượt trội hơn các mô hình baseline trong cả hai tác vụ. Adapters NER bất khả tri mô hình và yêu cầu ít tham số hơn để được huấn luyện so với việc tiền huấn luyện hoặc tinh chỉnh đầy đủ một mô hình. Chúng tôi dự định áp dụng CodeBERTER cho các tác vụ downstream khác và adapters NER cho các mô hình được tiền huấn luyện khác.

LỜI CẢM ƠN
Nghiên cứu này được hỗ trợ bởi một khoản tài trợ từ Hội đồng Khoa học Tự nhiên và Kỹ thuật Canada RGPIN-2019-05175.

TÀI LIỆU THAM KHẢO
[1] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., "Codebert: A pre-trained model for programming and natural languages," arXiv preprint arXiv:2002.08155, 2020.
[2] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, "Pre-trained contextual embedding of source code," 2019.
[3] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 8696–8708.
[4] J. Gu, P. Salza, and H. C. Gall, "Assemble foundation models for automatic code summarization," arXiv preprint arXiv:2201.05222, 2022.
[5] T. Ahmed and P. Devanbu, "Learning code summarization from a small and local dataset," arXiv preprint arXiv:2206.00804, 2022.
[6] C. Zhang, J. Wang, Q. Zhou, T. Xu, K. Tang, H. Gui, and F. Liu, "A survey of automatic source code summarization," Symmetry, vol. 14, no. 3, p. 471, 2022.
[7] L. Büch and A. Andrzejak, "Learning-based recursive aggregation of abstract syntax trees for code clone detection," in 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2019, pp. 95–104.
[8] K. W. Nafi, T. S. Kar, B. Roy, C. K. Roy, and K. A. Schneider, "Clcdsa: cross language code clone detection using syntactical features and api documentation," in 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1026–1037.
[9] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., "Graphcodebert: Pre-training code representations with data flow," arXiv preprint arXiv:2009.08366, 2020.
[10] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and D. Poshyvanyk, "An empirical study on learning bug-fixing patches in the wild via neural machine translation," ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 28, no. 4, pp. 1–29, 2019.
[11] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," arXiv preprint arXiv:2102.04664, 2021.
[12] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, "Unified pre-training for program understanding and generation," arXiv preprint arXiv:2103.06333, 2021.
[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[14] X. Jiang, Z. Zheng, C. Lyu, L. Li, and L. Lyu, "Treebert: A tree-based pre-trained model for programming language," in Uncertainty in Artificial Intelligence. PMLR, 2021, pp. 54–63.
[15] J. Ainslie, S. Ontanon, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A. Ravula, S. Sanghai, Q. Wang, and L. Yang, "Etc: Encoding long and structured inputs in transformers," arXiv preprint arXiv:2004.08483, 2020.
[16] R. Wu, Y. Zhang, Q. Peng, L. Chen, and Z. Zheng, "A survey of deep learning models for structural code understanding," arXiv preprint arXiv:2205.01293, 2022.
[17] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for nlp," in International Conference on Machine Learning. PMLR, 2019, pp. 2790–2799.
[18] D. Goel, R. Grover, and F. H. Fard, "On the cross-modal transfer from natural language to code through adapter modules," arXiv preprint arXiv:2204.08653, 2022.
[19] J. Pfeiffer, I. Vulić, I. Gurevych, and S. Ruder, "Mad-x: An adapter-based framework for multi-task cross-lingual transfer," arXiv preprint arXiv:2005.00052, 2020.
[20] R. He, L. Liu, H. Ye, Q. Tan, B. Ding, L. Cheng, J.-W. Low, L. Bing, and L. Si, "On the effectiveness of adapter-based tuning for pretrained language model adaptation," arXiv preprint arXiv:2106.03164, 2021.
[21] H. Le, J. Pino, C. Wang, J. Gu, D. Schwab, and L. Besacier, "Lightweight adapter tuning for multilingual speech translation," arXiv preprint arXiv:2106.01463, 2021.
[22] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," arXiv preprint arXiv:1909.09436, 2019.
[23] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, "A novel neural source code representation based on abstract syntax tree," in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 783–794.
[24] D. Wendel and P. Medlock-Walton, "Thinking in blocks: Implications of using abstract syntax trees as the underlying program model," in 2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond). IEEE, 2015, pp. 63–66.
[25] C. Lin, Z. Ouyang, J. Zhuang, J. Chen, H. Li, and R. Wu, "Improving code summarization with block-wise abstract syntax tree splitting," in 2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC). IEEE, 2021, pp. 184–195.
[26] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, "Summarizing source code with transferred api knowledge," 2018.
[27] W. Takerngsaksiri, C. Tantithamthavorn, and Y.-F. Li, "Syntax-aware on-the-fly code completion," arXiv preprint arXiv:2211.04673, 2022.
[28] J. Pfeiffer, A. Kamath, A. Rückle, K. Cho, and I. Gurevych, "Adapterfusion: Non-destructive task composition for transfer learning," arXiv preprint arXiv:2005.00247, 2020.
[29] R. Sharma, F. Chen, F. Fard, and D. Lo, "An exploratory study on code attention in bert," arXiv preprint arXiv:2204.10200, 2022.
[30] T. Clem and P. Thomson, "Static analysis at github: An experience report," Queue, vol. 19, no. 4, pp. 42–67, 2021.
[31] Y. Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal, V. Chaudhary, J. Gu, and A. Fan, "Multilingual translation with extensible multilingual pretraining and finetuning," arXiv preprint arXiv:2008.00401, 2020.
[32] T. Ahmed and P. Devanbu, "Multilingual training for software engineering," arXiv preprint arXiv:2112.02043, 2021.
[33] C.-Y. Lin and F. J. Och, "Orange: a method for evaluating automatic evaluation metrics for machine translation," in COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004, pp. 501–507.
[34] Y. Hu, X. Shi, Q. Zhou, and L. Pike, "Fix bugs with transformer through a neural-symbolic edit grammar," arXiv preprint arXiv:2204.06643, 2022.
[35] L. Phan, H. Tran, D. Le, H. Nguyen, J. Annibal, A. Peltekian, and Y. Ye, "Cotext: Multi-task learning with code-text transformer," in Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), 2021.
[36] W. Qi, Y. Gong, Y. Yan, C. Xu, B. Yao, B. Zhou, B. Cheng, D. Jiang, J. Chen, R. Zhang et al., "Prophetnet-x: large-scale pre-training models for english, chinese, multi-lingual, dialog, and code generation," arXiv preprint arXiv:2104.08006, 2021.
[37] L. Zhuang, L. Wayne, S. Ya, and Z. Jun, "A robustly optimized bert pre-training approach with post-training," in Proceedings of the 20th Chinese National Conference on Computational Linguistics, 2021, pp. 1218–1227.
[38] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," Advances in neural information processing systems, vol. 27, 2014.

--- TRANG 9 ---
[39] F. Chen, F. H. Fard, D. Lo, and T. Bryksin, "On the transferability of pre-trained language models for low-resource programming languages," in 2022 IEEE/ACM 30th International Conference on Program Comprehension (ICPC). IEEE, 2022, pp. 401–412.
[40] Z. Zeng, H. Tan, H. Zhang, J. Li, Y. Zhang, and L. Zhang, "An extensive study on pre-trained models for program understanding and generation," 2022.
[41] S. Zhou, U. Alon, F. F. Xu, Z. JIang, and G. Neubig, "Doccoder: Generating code by retrieving and reading docs," arXiv preprint arXiv:2207.05987, 2022.
[42] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis, "Incoder: A generative model for code infilling and synthesis," arXiv preprint arXiv:2204.05999, 2022.
[43] P. Nie, J. Zhang, J. J. Li, R. Mooney, and M. Gligoric, "Impact of evaluation methodologies on code summarization," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 4936–4960.
[44] P. Vaithilingam, T. Zhang, and E. L. Glassman, "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models," in CHI Conference on Human Factors in Computing Systems Extended Abstracts, 2022, pp. 1–7.
[45] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "A conversational paradigm for program synthesis," arXiv preprint arXiv:2203.13474, 2022.
[46] K. Ellis, C. Wong, M. Nye, M. Sablé-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B. Tenenbaum, "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning," in Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, 2021, pp. 835–850.
[47] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., "Program synthesis with large language models," arXiv preprint arXiv:2108.07732, 2021.
[48] U. Nadeem, N. Ziems, and S. Wu, "Codedsi: Differentiable code search," arXiv preprint arXiv:2210.00328, 2022.
[49] C. Richter and H. Wehrheim, "Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes," arXiv e-prints, p. arXiv:2207.00301, Jul. 2022.
[50] J. Zhang, S. Panthaplackel, P. Nie, J. Jessy Li, and M. Gligoric, "CoditT5: Pretraining for Source Code and Natural Language Editing," arXiv e-prints, p. arXiv:2208.05446, Aug. 2022.
