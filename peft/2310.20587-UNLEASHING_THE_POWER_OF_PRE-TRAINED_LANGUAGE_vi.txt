# 2310.20587.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2310.20587.pdf
# Kích thước tệp: 2134645 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
GIẢI PHÓNG SỨC MẠNH CỦA CÁC MÔ HÌNH NGÔN NGỮ ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC CHO HỌC TĂNG CƯỜNG NGOẠI TUYẾN

Ruizhe Shi1*Yuyao Liu1∗Yanjie Ze23Simon S. Du4Huazhe Xu125
1Đại học Thanh Hoa, IIIS2Viện Qi Zhi Thượng Hải3Đại học Giao Thông Thượng Hải
4Đại học Washington5Phòng thí nghiệm AI Thượng Hải

TÓM TẮT
Học tăng cường ngoại tuyến (RL) nhằm tìm một chính sách gần tối ưu bằng cách sử dụng các bộ dữ liệu đã được thu thập trước đó. Trong các tình huống thực tế, việc thu thập dữ liệu có thể tốn kém và rủi ro; do đó, RL ngoại tuyến trở nên đặc biệt thách thức khi dữ liệu trong miền bị hạn chế. Với những tiến bộ gần đây trong các Mô hình Ngôn ngữ Lớn (LLM) và khả năng học few-shot của chúng, bài báo này giới thiệu Language Models for Motion Control (LaMo), một khung tổng quát dựa trên Decision Transformers để sử dụng hiệu quả các Mô hình Ngôn ngữ (LM) đã được huấn luyện trước cho RL ngoại tuyến. Khung của chúng tôi nồi bật bốn thành phần quan trọng: (1) Khởi tạo Decision Transformers với các LM được huấn luyện trước một cách tuần tự, (2) sử dụng phương pháp tinh chỉnh LoRA, trái ngược với việc tinh chỉnh toàn bộ trọng số, để kết hợp hiệu quả kiến thức đã được huấn luyện trước từ LM và kiến thức trong miền, (3) sử dụng biến đổi MLP phi tuyến thay vì phép chiếu tuyến tính, để tạo ra các embedding, và (4) tích hợp một mất mát dự đoán ngôn ngữ phụ trợ trong quá trình tinh chỉnh để ổn định LM và giữ lại khả năng ban đầu của chúng về ngôn ngữ. Kết quả thực nghiệm cho thấy LaMo đạt được hiệu suất xuất sắc trong các nhiệm vụ thưởng thưa và thu hẹp khoảng cách giữa các phương pháp RL ngoại tuyến dựa trên giá trị và decision transformers trong các nhiệm vụ thưởng dày đặc. Đặc biệt, phương pháp của chúng tôi thể hiện hiệu suất vượt trội trong các tình huống với số lượng mẫu dữ liệu hạn chế. Trang web dự án của chúng tôi là lamo2023.github.io.

[Biểu đồ 1: Điểm số chuẩn hóa trên bộ dữ liệu D4RL (Fu et al., 2020) của Language Models for Motion Control (LaMo), Decision Transformer (DT, Chen et al., 2021), Wiki-RL (Reid et al., 2022), Conservative Q-Learning (CQL, Kumar et al., 2020) và Behavior Cloning (BC). Chúng tôi tính trung bình điểm số qua các nhiệm vụ và tỷ lệ mẫu dữ liệu cho mỗi miền. (Medium cho Mujoco và Atari, Complete và Partial cho Kitchen, với các tỷ lệ mẫu khác nhau, được mô tả trong Phụ lục B.)]

1 GIỚI THIỆU
Học tăng cường ngoại tuyến (RL) đã thu hút sự chú ý đáng kể trong những năm gần đây do tiềm năng của nó trong việc sử dụng các bộ dữ liệu đã được thu thập trước để cải thiện hiệu suất của tác nhân (Lange et al., 2012; Prudencio et al., 2023; Levine et al., 2020). Trong số các thuật toán nổi bật trong RL ngoại tuyến, Decision Transformer (DT) (Chen et al., 2021) đóng khung RL như một bài toán mô hình hóa chuỗi có điều kiện và sử dụng kiến trúc Transformer (Vaswani et al., 2017), cho thấy tiềm năng của các mô hình chuỗi cho việc ra quyết định (Xu et al., 2022; Hu et al., 2023a;b; Xie et al., 2023; Laskin et al., 2023).

∗Đóng góp bằng nhau. Thứ tự được quyết định bằng cách tung đồng xu.

--- TRANG 2 ---
Tuy nhiên, Transformers được biết đến là cần rất nhiều dữ liệu (Khan et al., 2022; Brown et al., 2020; OpenAI, 2023), có nghĩa là việc huấn luyện trước trên lượng dữ liệu khổng lồ thường được yêu cầu để đạt được khả năng mô hình thỏa đáng (Touvron et al., 2021). Một trong những ứng dụng nổi bật nhất của Transformers — các mô hình ngôn ngữ lớn (LLM) — đã đạt được tiến bộ đáng kể trong hiểu ngôn ngữ gần đây, chẳng hạn như GPT (Radford & Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; OpenAI, 2023), ChatGPT (Ouyang et al., 2022), và LLaMA (Touvron et al., 2023a). Được huấn luyện trước trên dữ liệu ngôn ngữ phong phú và đa dạng, LLM có được khả năng học few-shot và zero-shot tuyệt vời (Brown et al., 2020; Kojima et al., 2022).

Một ý tưởng tự nhiên để tăng cường các phương pháp ra quyết định tuần tự dựa trên Transformer là giới thiệu sức mạnh của các Mô hình Ngôn ngữ (LM) đã được huấn luyện trước vào chúng, ban đầu được khám phá bởi nhiều công trình gần đây (Ichter et al., 2022; Huang et al., 2022; Driess et al., 2023; Wu et al., 2023; Li et al., 2022; Reed et al., 2022; Lin et al., 2023; Brohan et al., 2023b;a; Tang et al., 2023; Wang et al., 2023b). Trong số đó, Li et al. (2022) đề xuất mã hóa các trạng thái môi trường với LLM và học một chính sách dựa trên các trạng thái đã được giải mã, trong khi các trạng thái môi trường của họ chỉ bị hạn chế trong các mô tả ngôn ngữ, làm cho việc điều khiển chuyển động trở nên khó khăn. Reid et al. (2022) giải quyết điểm yếu này bằng cách trực tiếp sử dụng LM đã được huấn luyện trước làm khởi tạo của DT và xử lý trực tiếp các trạng thái và hành động tác nhân mức thấp, thay vì xử lý các mô tả ngôn ngữ. Kiến trúc của họ do đó thành công sử dụng LM đã được huấn luyện trước trong các nhiệm vụ điều khiển chuyển động như di chuyển (Fu et al., 2020). Tuy nhiên, bất chấp tính mới lạ của phương pháp đề xuất trong Reid et al. (2022), họ vẫn không hoàn toàn giải phóng sức mạnh của LM: hiệu suất thực nghiệm của họ ngang bằng với các phương pháp DT thuần túy và thua kém CQL (Kumar et al., 2020). Do đó chúng tôi đặt câu hỏi,

Liệu chúng ta có thể giải phóng sức mạnh của LM đã được huấn luyện trước để giải quyết các bài toán ra quyết định tuần tự?

Trong công trình này, chúng tôi đề xuất Language Models for Motion Control (LaMo), một khung để sử dụng hiệu quả LM đã được huấn luyện trước cho RL ngoại tuyến. Mặc dù động cơ là đơn giản, cần bốn thiết kế quan trọng để trao quyền cho LaMo: 1) mô hình ngôn ngữ đã được huấn luyện trước được sử dụng làm trọng số ban đầu của DT; 2) các trọng số đã được huấn luyện trước được đóng băng và mô hình được tinh chỉnh với phương pháp tinh chỉnh hiệu quả tham số LoRA (Hu et al., 2022) trên 0.7% tham số; 3) chúng tôi thay thế các embedding đầu vào và các phép chiếu tuyến tính đầu ra bằng Multi-Layer Perceptrons (MLP); 4) một hàm mất mát dự đoán ngôn ngữ như một mục tiêu phụ trợ. Do đó, chúng tôi thấy rằng bốn thành phần kết hợp có thể giúp LaMo bảo tồn kiến thức tiền nghiệm và khả năng tổng quát hóa có được từ việc huấn luyện trước trong khi thích ứng hiệu quả với miền mới của RL ngoại tuyến.

Chúng tôi tiến hành các thí nghiệm toàn diện trên ba môi trường riêng biệt: Kitchen (Gupta et al., 2019), MuJoCo (Todorov et al., 2012), và Atari (Bellemare et al., 2013), bao gồm tổng cộng 8 nhiệm vụ. Các nhiệm vụ này dao động từ thưởng thưa đến thưởng dày đặc, và từ đầu vào trạng thái và đầu vào hình ảnh. Đối với mỗi nhiệm vụ, chúng tôi đánh giá hiệu suất dưới các tỷ lệ dữ liệu khác nhau để kiểm tra ảnh hưởng của số lượng mẫu đến kết quả. Chúng tôi quan sát thấy như được hiển thị trong Hình 1, LaMo vượt trội hơn cả DT và các baseline dựa trên giá trị trong các nhiệm vụ thưởng thưa; và trong các nhiệm vụ thưởng dày đặc, phương pháp của chúng tôi vượt trội đáng kể so với DT và thu hẹp khoảng cách giữa các phương pháp dựa trên giá trị và các phương pháp dựa trên DT. Đặc biệt, chúng tôi thấy rằng khi quy mô dữ liệu bị hạn chế (ví dụ, 1% của toàn bộ bộ dữ liệu), LaMo thể hiện khả năng học mạnh mẽ hơn nhiều, có thể được ghi nhận cho thiên hướng quy nạp trong LM đã được huấn luyện trước.

Đóng góp của chúng tôi gồm ba phần:
• Chúng tôi đề xuất LaMo, một khung RL ngoại tuyến mới giải phóng sức mạnh của các mô hình ngôn ngữ đã được huấn luyện trước.
• Để sử dụng tốt hơn kiến thức xuyên miền từ mô hình hóa ngôn ngữ, chúng tôi đề xuất 3 kỹ thuật bổ sung bao gồm tinh chỉnh LoRA, phép chiếu MLP phi tuyến, và một mất mát ngôn ngữ phụ trợ. Mỗi mô-đun được chứng minh đóng góp tích cực vào kết quả cuối cùng của LaMo.
• Thông qua các thí nghiệm mở rộng trong 8 nhiệm vụ trên các miền đa dạng, quy mô bộ dữ liệu, và mật độ thưởng, chúng tôi chứng minh sự vượt trội của LaMo so với các thuật toán RL ngoại tuyến dựa trên DT và dựa trên giá trị. Cụ thể, chúng tôi thấy rằng LaMo có thể xử lý thành công chế độ dữ liệu thấp đầy thách thức trong khi DT thì không. Điều này làm nổi bật tiềm năng lớn của việc huấn luyện trước xuyên miền của chúng tôi cho mô hình hóa tuần tự.

--- TRANG 3 ---
2 CÔNG TRÌNH LIÊN QUAN

Transformers cho ra quyết định. Transformers đã thống trị các nhiệm vụ ngôn ngữ trong cộng đồng NLP (Radford & Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Devlin et al., 2019) và cũng bắt đầu cho thấy tiềm năng trong các miền khác, chẳng hạn như ra quyết định. Như một thử nghiệm ban đầu để giới thiệu Transformers vào học tăng cường (RL), Decision Transformer (DT, Chen et al., 2021) mô hình hóa các yếu tố như trạng thái và hành động thành một chuỗi, do đó đóng khung bài toán RL thành một bài toán dự đoán chuỗi. Có nhiều công trình tiếp theo thực hiện cải tiến dưới khung của DT (Xu et al., 2022; Hu et al., 2023b; Xie et al., 2023; Yamagata et al., 2023; Liu & Abbeel, 2023). Ví dụ, Prompt DT (Xu et al., 2022) nối thêm các minh chứng vào chuỗi để đạt được khái quát hóa trong các nhiệm vụ mới; Xie et al. (2023) huấn luyện trước DT bằng cách tận dụng thông tin quỹ đạo tương lai; Q-learning DT (Yamagata et al., 2023) tinh chỉnh return-to-go trong dữ liệu huấn luyện bằng cách sử dụng Q-values, do đó trang bị cho DT thành thạo của Q-learning trong việc xử lý dữ liệu không tối ưu. Agentic Transformer (Liu & Abbeel, 2023) giải quyết các vấn đề về tính không tối ưu bằng cách sử dụng chuỗi hồi tưởng để gắn nhãn lại các return mục tiêu, đạt được hiệu suất cạnh tranh so với các phương pháp dựa trên giá trị. Trajectory Transformer (Janner et al., 2021) huấn luyện trên các chuỗi trạng thái, hành động và thưởng đã được rời rạc hóa, chỉ ra một giải pháp trực tiếp hơn. Công trình của chúng tôi tập trung vào việc sử dụng kiến thức xuyên miền, tức là huấn luyện trước ngôn ngữ, như thông tin đặc quyền để tăng cường các phương pháp dựa trên DT, do đó là trực giao với các công trình này.

Các Mô hình Ngôn ngữ Lớn (LLM) đã là ứng dụng nổi bật nhất của kiến trúc Transformer trong những năm gần đây (Radford & Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; OpenAI, 2023; Devlin et al., 2019; Touvron et al., 2023a;b). Được huấn luyện trước trên lượng lớn kho ngữ liệu, LLM đã cho thấy khả năng few-shot và thậm chí zero-shot đáng ngạc nhiên trong các nhiệm vụ ngôn ngữ, chẳng hạn như dòng GPT (Radford & Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; OpenAI, 2023). Để cá nhân hóa LLM cho các ứng dụng downstream khác nhau của người dùng với hiệu quả tính toán, các nhà nghiên cứu thường sử dụng các kỹ thuật tinh chỉnh hiệu quả tham số (Hu et al., 2022; Zhang et al., 2023a; Li & Liang, 2021; Lester et al., 2021; Liu et al., 2022; Wang et al., 2023a) để tinh chỉnh LLM. Trong công trình này, chúng tôi sử dụng kiến trúc GPT-2 (Radford et al., 2019) làm xương sống do tính khả thi và sử dụng LoRA (Hu et al., 2022) cho việc tinh chỉnh downstream.

LM cho ra quyết định. Thành công lớn của LM trong các nhiệm vụ ngôn ngữ cũng thúc đẩy các nhà nghiên cứu khám phá tiềm năng của LM cho các bài toán ra quyết định (Ichter et al., 2022; Huang et al., 2022; Driess et al., 2023; Wu et al., 2023). Một dòng công trình (Ichter et al., 2022; Huang et al., 2022; Driess et al., 2023; Wu et al., 2023) sử dụng LM cho phân tách nhiệm vụ và lập kế hoạch nhiệm vụ mức cao, trong khi chính sách thực thi mức thấp của họ được học hoặc thiết kế riêng biệt. Một dòng công trình khác (Li et al., 2022; Reed et al., 2022; Lin et al., 2023; Brohan et al., 2023a; Tang et al., 2023; Wang et al., 2023b) khai thác sức mạnh biểu diễn và khái quát hóa của LM đã được huấn luyện trước. Li et al. (2022) thích ứng LM đã được huấn luyện trước để tạo ra chính sách cho các nhiệm vụ mà đầu vào có thể được chuyển đổi thành chuỗi từ và chỉ ra tầm quan trọng của cấu trúc tuần tự của đầu vào; Lin et al. (2023) sử dụng một bộ lập kế hoạch khả thi hình học để khuyến khích LM tạo ra cả kế hoạch mức trung và mức thấp được đưa ra hướng dẫn ngôn ngữ; và Tang et al. (2023) thiết kế prompts cho LM để mã hóa hướng dẫn ngôn ngữ. Khi đầu vào đa phương thức được liên quan, một giải pháp là chuyển đổi chúng thành một không gian embedding chung (Brohan et al., 2023a; Reed et al., 2022). Ví dụ, RT-2 (Brohan et al., 2023a) sử dụng một Mô hình Thị giác-Ngôn ngữ được huấn luyện trước trên dữ liệu ngôn ngữ và thị giác-ngôn ngữ khổng lồ, và cũng biểu diễn các hành động như tokens văn bản trong giai đoạn Fine-tuning Robot-Action; GATO (Reed et al., 2022) sử dụng một Vision Transformer để mã hóa đầu vào hình ảnh, và học từ một bộ dữ liệu đa phương thức, đa nhiệm vụ lớn để thực hiện các nhiệm vụ khác nhau tất cả trong một mô hình.

Công trình liên quan nhất với chúng tôi là Wiki-RL (Reid et al., 2022), cũng sử dụng một mô hình ngôn ngữ đã được huấn luyện trước làm khởi tạo của DT cho RL ngoại tuyến. Tuy nhiên, kết quả thực nghiệm của họ được hiển thị chỉ gần với DT và không thể vượt qua CQL (Kumar et al., 2020). Do đó, công trình của chúng tôi cố gắng giải phóng tốt hơn sức mạnh của LM đã được huấn luyện trước cho RL ngoại tuyến.

3 KIẾN THỨC CƠ BẢN

3.1 HỌC TĂNG CƯỜNG NGOẠI TUYẾN

Chúng tôi công thức hóa học tăng cường (RL) như một Quá trình Quyết định Markov (MDP) tiêu chuẩn với một tuple (S,A, T, d₀,R, γ), trong đó S là tập hợp các trạng thái s∈S, A là tập hợp các hành động a∈A, T

--- TRANG 4 ---
là phân phối chuyển tiếp có dạng T(st+1|st, at), d0(s0) mô tả phân phối của các trạng thái s0, R:S × A → R là hàm thưởng, rt=R(st, at) là thưởng tại timestep t, và γ∈(0,1) là hệ số chiết khấu. Tác nhân trong MDP này tuân theo một chính sách π(a|s), và mục tiêu là:

J(π) = Es0∼d0(·), at∼π(·|st), st+1∼T(·|st,at)[∑∞t=0 γtR(st, at)]. (1)

Trong RL ngoại tuyến, việc truy cập tương tác với môi trường bị loại bỏ trong khi mục tiêu vẫn là J(π). Các tác nhân chỉ có thể học trên các quỹ đạo đã được thu thập trước D={(s(i)t, a(i)t, s(i)t+1, r(i)t)}, được tạo ra bởi một chính sách hành vi πB không xác định. Ở đây chúng tôi giới thiệu các tính chất phổ biến của bộ dữ liệu D: 1) Tính không tối ưu. Trong nhiều bối cảnh, πB không phải là một chính sách tối ứu, tức là D sẽ không chứa các hành vi tối ưu, và do đó việc bắt chước đơn giản có thể thể hiện hiệu suất không tối ưu; 2) Thưởng dày đặc hoặc thưởng thưa. Trong môi trường thưởng dày đặc, tác nhân nhận được tín hiệu thưởng tương ứng với việc liệu hành vi của tác nhân có tốt cho mỗi timestep hay không, trong khi trong thiết lập thưởng thưa, tín hiệu thưởng tích cực từ môi trường chỉ có thể được đưa ra khi thành công được đạt được, và nếu không thì bằng không. Thiết lập thưởng thưa do đó khó khăn hơn nhiều nhưng gần gũi hơn với các tình huống thế giới thực.

3.2 DECISION TRANSFORMER

Theo Decision Transformer (DT), chúng tôi đóng khung bài toán RL như một bài toán mô hình hóa tuần tự. Chúng tôi xem xét mỗi quỹ đạo τ như một chuỗi return-to-go ˆR, hành động a, và trạng thái s được sắp xếp theo thứ tự, được định nghĩa như sau,

τ = (ˆRt0, st0, at0, ˆRt0+1, st0+1, at0+1, . . . , ˆRt0+K−1, st0+K−1, at0+K−1). (2)

trong đó return-to-go ˆR được định nghĩa là tổng thưởng từ timestep hiện tại đến tương lai: ˆRk=∑Ti=k+1 ri, T là độ dài episode, và K là độ dài ngữ cảnh. Mục tiêu học của mô hình là dự đoán hành động tương lai a′t cho trước chuỗi lịch sử và trạng thái hiện tại st, trong khi sự thật cơ bản là at, được viết như một thuật ngữ lỗi bình phương đơn giản:

Ldecision = ∑t0+K−1t=t0 ∥at−a′t∥22. (3)

4 PHƯƠNG PHÁP

Chúng tôi đề xuất Language Models for Motion Control (LaMo), một khung hiệu quả kết hợp các Mô hình Ngôn ngữ (LM) đã được huấn luyện trước vào Học Tăng cường Ngoại tuyến, để tận dụng khả năng lý luận và few-shot của LM và giải quyết các tình huống thách thức như dữ liệu hạn chế và thưởng thưa. Một minh họa của LaMo được đưa ra trong Hình 2. LaMo bao gồm một số thiết kế quan trọng:

1) Chúng tôi áp dụng một LM đã được huấn luyện trước (tức là, GPT-2 (Radford et al., 2019)) làm khởi tạo của một Decision Transformer (DT) (Chen et al., 2021); 2) Chúng tôi thay thế các phép chiếu embedding tuyến tính bằng MLP để tăng cường khả năng học biểu diễn cho các nhiệm vụ phức tạp; 3) Trong quá trình huấn luyện các tác nhân RL ngoại tuyến, chúng tôi đóng băng các phần đã được huấn luyện trước và sử dụng kỹ thuật tinh chỉnh hiệu quả tham số LoRA (Hu et al., 2022), trong đó các tham số có thể huấn luyện chỉ chiếm 0.7% của toàn bộ mô hình; 4) Chúng tôi giới thiệu dự đoán ngôn ngữ như một mục tiêu phụ trợ trong khi tinh chỉnh, để ổn định hiệu suất và duy trì khả năng ngôn ngữ.

4.1 HUẤN LUYỆN TRƯỚC TRÊN CÁC NHIỆM VỤ NGÔN NGỮ

Bước đầu tiên liên quan đến việc thu thập các mô hình ngôn ngữ đã được huấn luyện trước (LM). Xem xét sự công nhận rộng rãi và khả năng chi trả tính toán của kiến trúc GPT-2 (Radford et al., 2019), chúng tôi sử dụng trọng số đã được huấn luyện trước có sẵn phổ biến của GPT-2 từ Hugging Face¹. Để khám phá thêm về ảnh hưởng của chất lượng của các mô hình đã được huấn luyện trước khác nhau đối với các nhiệm vụ RL ngoại tuyến downstream, chúng tôi cũng

¹https://huggingface.co/gpt2

--- TRANG 5 ---
huấn luyện trước GPT-2 bằng chính chúng tôi trong nghiên cứu ablation, sử dụng bộ dữ liệu corpus WikiText (Merity et al., 2017) và mục tiêu dự đoán token tiếp theo phổ biến

Llanguage = ∑s−1i=1 −logT(wi+1|w1, . . . , wi), (4)

trong đó wi là token ngôn ngữ thứ i trong một câu, và T là phân phối xác suất của token tiếp theo được dự đoán bởi mô hình. Chúng tôi đã khám phá ba biến thể của mô hình: 1) một mô hình được huấn luyện trước với ít bước hơn; 2) một mô hình được huấn luyện trước trên corpus văn bản được xáo trộn ngẫu nhiên; 3) một mô hình với trọng số được khởi tạo ngẫu nhiên. Kết quả của chúng tôi trong Phần 5.5 và Phụ lục G cho thấy chất lượng huấn luyện trước ngôn ngữ cao có ích cho các nhiệm vụ RL downstream, nhấn mạnh tầm quan trọng và sự cần thiết của việc huấn luyện trước.

4.2 TINH CHỈNH CHO HỌC TĂNG CƯỜNG NGOẠI TUYẾN

Multi-layer perceptrons cho embeddings. Các LM đã được huấn luyện trước xử lý đầu vào thành các vector tiềm ẩn và giải mã các vector tiềm ẩn thành đầu ra thông qua các phép chiếu tuyến tính đơn giản. Chúng tôi thấy rằng để sử dụng hiệu quả mô hình ngôn ngữ đã được huấn luyện trước trong RL ngoại tuyến, việc thay thế các phép chiếu tuyến tính bằng MLP là cần thiết để bắc cầu khoảng cách miền. Các ablation mở rộng được cung cấp trong Phần 5.5 để hỗ trợ tầm quan trọng của mô-đun phi tuyến này.

Trọng số đóng băng và thích ứng rank thấp. Chúng tôi áp dụng kỹ thuật huấn luyện hiệu quả tham số LoRA (Hu et al., 2022), ràng buộc quá trình cập nhật gradient trong một không gian chiều thấp bằng cách viết lại ma trận trọng số W∈Rd×k như W0+ ∆W=W0+BA, trong đó B∈Rd×r, A∈Rr×k, và r≪min(d, k). Chúng tôi tiêm các ma trận rank thấp vào các trọng số attention Q, K, V và đóng băng tất cả các trọng số khác của Transformer.

Trong khi đó, mô hình được mong muốn duy trì kiến thức của các LM. Số lượng tham số có thể huấn luyện chỉ chiếm 0.7% của toàn bộ Transformer. Chúng tôi giả thuyết rằng cơ chế như vậy sẽ để mô hình đã được huấn luyện trước xử lý các đầu vào như ngôn ngữ ở mức độ tối đa trong khi vẫn duy trì tính thích ứng. Về mặt thực nghiệm, chúng tôi thấy rằng tinh chỉnh toàn bộ trọng số hoặc các lớp Transformer đóng băng sẽ làm hại hiệu suất, như được hiển thị trong Hình 5. Thêm thảo luận được cung cấp trong Phần 5.5.

Dự đoán ngôn ngữ như một mục tiêu phụ trợ. Để ổn định thêm quá trình huấn luyện và duy trì kiến thức học được từ ngôn ngữ, chúng tôi đồng thời huấn luyện mô hình trên các nhiệm vụ dự đoán ngôn ngữ. Corpus chúng tôi huấn luyện trên là WikiText (Merity et al., 2017), giống như giai đoạn huấn luyện trước. Để thực hiện dự đoán ngôn ngữ, chúng tôi sẽ tạm thời thay thế các phép chiếu đầu vào và đầu ra bằng các phép chiếu của LM đã được huấn luyện trước. Mục tiêu phụ trợ này được sử dụng trong Reid et al. (2022). Về mặt thực nghiệm, chúng tôi thấy rằng thuật ngữ này có thể ngăn chặn đáng kể việc mô hình overfitting. Thú vị là, đối với các nhiệm vụ thưởng thưa như Kitchen, hiệu suất của LaMo được tăng cường quan trọng để vượt qua các baseline mạnh gần đây, như được hiển thị trong Hình 6b. Bên cạnh đó, mục tiêu này có thể giúp bảo tồn khả năng hiểu ngôn ngữ, có nghĩa là chúng ta có thể có được một mô hình thành thạo cả hiểu ngôn ngữ và điều khiển chuyển động như một tác dụng phụ. Thảo luận chi tiết hơn trong Phần 5.5. Mục tiêu tổng thể trong khi huấn luyện các tác nhân RL ngoại tuyến sau đó là

L = Ldecision + λ · Llanguage (5)

trong đó λ là một tham số có thể điều chỉnh được đặt trong {0,0.1,1}.

5 THÍ NGHIỆM

Trong công trình này, chúng tôi nghiên cứu giải quyết các bài toán ra quyết định tuần tự trong khi chỉ có các bộ dữ liệu tương tác ngoại tuyến có sẵn trong quá trình huấn luyện, được biết đến là bài toán RL Ngoại tuyến. Chúng tôi đánh giá hiệu suất của LaMo trên benchmark tiêu chuẩn D4RL (Fu et al., 2020) và cũng đánh giá khả năng học của LaMo dưới chế độ dữ liệu thấp. Để cho thấy hiệu quả của mỗi thành phần trong LaMo, các ablation mở rộng cũng được tiến hành.

5.1 THIẾT LẬP THÍ NGHIỆM

Chúng tôi tiến hành các thí nghiệm của mình trên 8 nhiệm vụ từ 3 miền MuJoCo, Atari, và Kitchen. Mô tả nhiệm vụ chi tiết được cung cấp trong Phụ lục C. Chúng tôi sử dụng các bộ dữ liệu từ D4RL (Fu et al., 2020) và d4rl-atari (chi tiết hơn được cung cấp trong Phụ lục B). Do hạn chế về tài nguyên tính toán, chúng tôi chạy mỗi thí nghiệm cho 3 seed với số 0,1,2 để đảm bảo tính tái tạo.

Chúng tôi so sánh hiệu suất của LaMo với các baseline mạnh khác nhau trong học tăng cường ngoại tuyến: CQL (Kumar et al., 2020), IQL (Kostrikov et al., 2022), TD3+BC (Fujimoto & Gu, 2021), BCQ (Fujimoto et al., 2019), NFQ (Riedmiller, 2005), Behavior Cloning (BC), và DT (Chen et al., 2021). Bên cạnh đó, chúng tôi so sánh với Wiki-RL (Reid et al., 2022), cũng sử dụng mô hình ngôn ngữ đã được huấn luyện trước trong học tăng cường ngoại tuyến. Để báo cáo một cách có hệ thống hiệu suất của tất cả các phương pháp này, chúng tôi tính toán hiệu suất trung bình trong 20K bước huấn luyện cuối cùng trên tổng số 100K bước huấn luyện với các đánh giá được tiến hành mỗi 2500 bước huấn luyện. Điểm số chúng tôi báo cáo là điểm số chuẩn hóa sao cho 100 đại diện cho một chính sách chuyên gia và 0 đại diện cho một chính sách ngẫu nhiên, theo quy ước của Fu et al. (2020) và Hafner et al. (2020).

5.2 CÁC NHIỆM VỤ THƯỞNG THƯA

[Bảng 1: Điểm số chuẩn hóa cho các nhiệm vụ thưởng thưa. Chúng tôi so sánh LaMo với DT, Wiki-RL, CQL, IQL, TD3+BC, và BC. Trung bình của 3 seed với số 0,1,2. Tô sáng màu xanh chỉ ra điểm số cao nhất, tô sáng màu cam chỉ ra điểm số cao thứ hai, và số màu đỏ đại diện cho sự cải thiện của LaMo so với DT.]

Kết quả cho các nhiệm vụ thưởng thưa bao gồm Kitchen và Reacher2d được đưa ra trong Bảng 1. Chúng tôi chọn các baseline mạnh bao gồm CQL, IQL, TD3+BC, BC, DT và Wiki-RL. Chúng tôi quan sát thấy LaMo cho thấy một lợi thế áp đảo so với Decision Transformer và Wiki-RL trên tất cả các nhiệm vụ và bộ dữ liệu, điều này chỉ ra rằng phương pháp của chúng tôi hiệu quả khai thác sức mạnh của mô hình đã được huấn luyện trước. Nhìn chung, LaMo đã cải thiện hiệu suất của DT lên đến 50%. So với các phương pháp dựa trên giá trị, phương pháp của chúng tôi cũng thể hiện lợi thế đáng kể về hiệu suất trung bình. Chúng tôi đã đạt được hiệu suất tốt nhất trong số tất cả các baseline mạnh trong 7 nhiệm vụ và kết quả vị trí thứ hai trong 2 nhiệm vụ Kitchen Partial với 1% dữ liệu và Reacher2d Medium với 10% dữ liệu.

Đáng kể, trong các nhiệm vụ Kitchen, CQL ban đầu hoạt động khá tốt, nhưng khi quá trình huấn luyện tiến triển, nó gặp phải vấn đề overfitting, gây ra sự giảm đáng kể trong hiệu suất của nó, điều này được hiển thị trong Phụ

--- TRANG 6 ---
lục F. Trong khi đối với LaMo, hiện tượng như vậy không xảy ra, phản ánh thành công của LaMo trong việc ngăn chặn overfitting.

5.3 CÁC NHIỆM VỤ THƯỞNG DÀY ĐẶC

[Bảng 2: Điểm số chuẩn hóa cho 3 nhiệm vụ thưởng dày đặc trong Atari. Chúng tôi so sánh LaMo với DT, Wiki-RL, CQL, BCQ, NFQ và BC. Trung bình của 3 seed với số 0,1,2. Tô sáng màu xanh chỉ ra điểm số cao nhất, tô sáng màu cam chỉ ra điểm số cao thứ hai, và số màu đỏ đại diện cho sự cải thiện của LaMo so với DT.]

[Bảng 3: Điểm số chuẩn hóa cho 3 nhiệm vụ thưởng dày đặc trong MuJoCo. Chúng tôi so sánh LaMo với DT, Wiki-RL, CQL, IQL, TD3+BC, và BC.]

Kết quả cho các nhiệm vụ thưởng dày đặc được đưa ra trong Bảng 2 và Bảng 3. Đối với Atari, Vì IQL và TD3+BC không hỗ trợ điều khiển rời rạc (Seno & Imai, 2022), chúng tôi chọn CQL, BCQ, và NFQ làm baseline. Chúng tôi quan sát thấy LaMo đạt được điểm số trung bình cao nhất trong Atari và MuJoCo dưới chế độ dữ liệu thấp. Tuy nhiên, chúng tôi cũng nhận thấy rằng trong miền MuJoCo, khi quy mô dữ liệu tương đối lớn (10%, 100%), LaMo chỉ gần với DT và thua kém CQL trong Halfcheetah và Walker2d. Trong Qbert Medium (100%) và Pong Medium (10%), LaMo cũng không vượt qua CQL. Chúng tôi quy cho các lý do sau: không giống như các nhiệm vụ thưởng thưa, nơi các backups Bellman sẽ từ từ truyền bá thông tin của thưởng (Chen et al., 2021), hạn chế hiệu suất của các thuật toán dựa trên giá trị, các nhiệm vụ thưởng dày đặc cực kỳ phù hợp cho các phương pháp dựa trên giá trị như CQL trong khi DT ít được ưa thích hơn, điều này được kiểm tra thực nghiệm bởi Bhargava et al. (2023). Các thí nghiệm của chúng tôi xác minh quan điểm và chỉ ra rằng LaMo có thể tăng cường thêm tiềm năng của DT, thu hẹp khoảng cách hiệu suất giữa DT và CQL trong các nhiệm vụ thưởng dày đặc.

5.4 KHẢ NĂNG TRONG CHỂ ĐỘ DỮ LIỆU THẤP

[Hình 3: Điểm số chuẩn hóa thu được bởi LaMo, CQL, và DT trên các tỷ lệ mẫu dữ liệu khác nhau. Trung bình của 3 seed với số 0,1,2. Vùng tô là khoảng [µ−0.5σ, µ+ 0.5σ], trong đó µ là trung bình và σ là độ lệch chuẩn.]

Chúng tôi tìm hiểu mối quan hệ giữa hiệu suất của các thuật toán khác nhau và quy mô dữ liệu. Như được mô tả trong Hình 3, LaMo có khả năng đạt được hiệu suất xuất sắc ngay cả với các bộ dữ liệu tương đối nhỏ. Ví dụ, trong Hopper, LaMo vượt qua hiệu suất của CQL và DT khi tỷ lệ mẫu dữ liệu là 0.5% và duy trì lợi thế này một cách nhất quán khi tỷ lệ mẫu tăng lên.

5.5 CÁC ABLATION

Để cho thấy đóng góp của các thiết kế khác nhau của chúng tôi trong LaMo, chúng tôi tiến hành các thí nghiệm ablation mở rộng.

Phép chiếu tuyến tính so với MLP. Trong LaMo, chúng tôi thấy rằng các phép chiếu tuyến tính đơn giản không thể khai thác đầy đủ kiến thức xuyên miền từ việc huấn luyện trước ngôn ngữ, và do đó thiết kế của chúng tôi để thay thế các phép chiếu tuyến tính bằng MLP là quan trọng. Như được hiển thị trong Hình 4, thiết kế như vậy thể hiện sự cải thiện rõ ràng so với các phép chiếu tuyến tính (được gọi là LaMo w/o. MLP). Cũng được quan sát thấy rằng trong nhiệm vụ Walker2d, LaMo với các phép chiếu tuyến tính đạt được điểm số tốt sau một vài bước huấn luyện nhưng gặp phải overfitting sau nhiều bước huấn luyện hơn, dẫn đến hội tụ không tối ưu.

[Hình 4: Ablation về hiệu quả của embedding MLP. Chúng tôi thay thế các MLP trong LaMo làm embedding bằng các phép chiếu tuyến tính, được ký hiệu là LaMo w/o. MLP. Chúng tôi so sánh LaMo với LaMo w/o. MLP và DT trên tất cả các nhiệm vụ. Trung bình của 3 seed với số 0,1,2. Vùng tô là khoảng [µ−0.5σ, µ+ 0.5σ], trong đó µ là trung bình và σ là độ lệch chuẩn.]

So sánh LoRA với tinh chỉnh đầy đủ và tham số đóng băng. Kết quả được đưa ra trong Hình 5. Mặc dù Hansen et al. (2022); Ze et al. (2023a) cho thấy việc tinh chỉnh đầy đủ các biểu diễn cho các nhiệm vụ RL thị giác tốt hơn việc áp dụng các mô hình đã được huấn luyện trước đóng băng, có các công trình (Ze et al., 2023b) cho thấy việc tinh chỉnh chỉ một phần nhỏ tham số có thể vượt trội so với các mô hình đóng băng và được tinh chỉnh đầy đủ, và chúng tôi quan sát thấy rằng trong các thiết lập của chúng tôi, việc đóng băng các tham số đã được huấn luyện trước và thích ứng với LoRA không chỉ có thể cải thiện hiệu quả huấn luyện mà còn giải quyết vấn đề overfitting xảy ra trong tinh chỉnh đầy đủ. Chúng tôi quy cho điều này là kiến thức có thể tổng quát hóa nội tại trong LM từ việc huấn luyện trước quy mô lớn và chúng tôi chuyển giao nó sang miền điều khiển chuyển động. Chúng tôi cũng tiến hành thí nghiệm về việc loại bỏ LoRA và chỉ sử dụng LM đã được huấn luyện trước đóng băng, điều này cũng kém hiệu quả hơn LaMo áp dụng LoRA cho việc học nhiệm vụ trong miền.

[Hình 5: Ablation về hiệu quả của LoRA. (1) Chúng tôi liên quan tất cả các tham số vào tinh chỉnh, được ký hiệu là Full Finetuning. (2) Chúng tôi đóng băng tất cả tham số trong các lớp Transformer và bỏ ra LoRA, được ký hiệu là Freezing. Chúng tôi so sánh LaMo với Full Finetuning, Freezing, và DT.]

--- TRANG 7 ---
Huấn luyện trước ngôn ngữ so với huấn luyện trước thị giác. Hơn nữa, xem xét các quan sát trong Atari là ở định dạng pixel, chúng tôi điều tra liệu việc huấn luyện trước thị giác cũng có thể hữu ích cho điều khiển chuyển động hay không. Chúng tôi thay thế mô hình đã được huấn luyện trước bằng ImageGPT (Chen et al., 2020), một Transformer được huấn luyện trước trên bộ dữ liệu ImageNet (Russakovsky et al., 2015). Trong quá trình huấn luyện trước, ImageGPT định hình lại hình ảnh hai chiều thành vectors một chiều sau khi downsampling, và được huấn luyện theo cách tự hồi quy. Kết quả được trình bày trong Bảng 4. Được quan sát thấy trên các nhiệm vụ Atari rằng việc huấn luyện trước thị giác có thể là một khởi tạo tích cực cho DT, trong khi vì LM mô hình hóa tốt hơn cấu trúc chuỗi, tồn tại một khoảng cách đáng kể giữa LaMo và ImageGPT. Bằng chứng thực nghiệm này củng cố thêm giả thuyết của chúng tôi rằng thành thạo trong mô hình hóa tuần tự là chìa khóa để giải phóng tiềm năng của các mô hình đã được huấn luyện trước xuyên miền.

[Bảng 4: Ablation về hiệu quả của việc huấn luyện trước ngôn ngữ tuần tự. Chúng tôi thay thế mô hình đã được huấn luyện trước trong LaMo bằng ImageGPT (Chen et al., 2020), được ký hiệu là LaMo (ImageGPT Pre-training). Chúng tôi so sánh LaMo với LaMo (ImageGPT Pre-training) và DT trên 3 nhiệm vụ Atari. Tô sáng màu xanh chỉ ra điểm số cao nhất.]

Mối quan hệ giữa khả năng ngôn ngữ và khả năng điều khiển chuyển động. Chúng tôi thấy rằng việc huấn luyện trên các nhiệm vụ ngôn ngữ cùng nhau có thể ngăn chặn overfitting và cải thiện hiệu suất tổng thể. Đối với nhiệm vụ thách thức nhất trong số 8 nhiệm vụ, Kitchen, như Hình 6b cho thấy, chúng tôi nhận thấy rằng bằng cách thêm một mất mát có trọng số đơn giản trong quá trình huấn luyện, hiệu suất không còn giảm đáng kể trong giai đoạn huấn luyện RL, và nó liên tục vượt trội so với các baseline. Điều này gợi ý rằng việc huấn luyện với một mất mát dự đoán ngôn ngữ như một regularization cùng nhau có thể giữ lại các lợi thế của mô hình đã được huấn luyện trước trong khi học từ một bộ dữ liệu ra quyết định hạn chế. Như được trình bày trong Hình 6a, chúng tôi hiển thị đường cong mất mát cross-entropy để minh họa gần đúng sự thay đổi khả năng ngôn ngữ trong quá trình huấn luyện, vẫn nhất quán trên tất cả các nhiệm vụ. Điều này xác thực thực nghiệm khả năng của các mô hình ngôn ngữ để đồng thời học hai nhiệm vụ mô hình hóa tuần tự khác nhau. Tuy nhiên, liệu thuật ngữ này có thể tăng cường hiệu suất trong tất cả các trường hợp hay không vẫn cần điều tra thêm.

[Hình 6: Ablations để cho thấy ảnh hưởng của mất mát ngôn ngữ đối với điều khiển chuyển động.]

--- TRANG 8 ---
[Hình 7: Ablation về ảnh hưởng của Chất lượng của Các Mô hình Đã được Huấn luyện Trước và Corpus. Chúng tôi huấn luyện các mô hình với cùng kiến trúc như GPT-2 từ đầu, cả trên WikiText và WikiText đã được xáo trộn. So với hai mô hình này và DT, LaMo cho thấy lợi thế một cách nhất quán.]

Ảnh hưởng của chất lượng huấn luyện trước của LM. Chúng tôi tiến hành một nghiên cứu có hệ thống về cách chất lượng huấn luyện trước của LM sẽ ảnh hưởng đến hiệu suất của các tác nhân RL ngoại tuyến downstream. Chúng tôi huấn luyện trước một số mô hình GPT-2 như sau: 1) huấn luyện trước dừng sớm, được huấn luyện trước trên WikiText trong 100K bước huấn luyện. 2) corpus ngẫu nhiên, được huấn luyện trước trên WikiText được xáo trộn ngẫu nhiên, để việc dự đoán token bị xáo trộn hoàn toàn. Bằng cách này, chúng tôi nhằm điều tra liệu sự cải thiện hiệu suất do việc huấn luyện trước có liên quan chặt chẽ đến bản chất của corpus hay chỉ được quy cho việc khởi động mạng. Sau đó chúng tôi thay thế GPT-2 trong LaMo bằng các mô hình này và so sánh hiệu suất trong các nhiệm vụ RL downstream. Như Hình 7 cho thấy, trong khi hai mô hình đã được huấn luyện trước này đạt được kết quả cạnh tranh so với DT, chúng vẫn thua kém so với LaMo trong một số nhiệm vụ nhất định. Quan sát ban đầu này xác minh giả thuyết của chúng tôi rằng một mô hình với khả năng ngôn ngữ mạnh hơn có thể hoạt động hiệu quả hơn khi chuyển giao sang lĩnh vực điều khiển chuyển động.

6 KẾT LUẬN

Chúng tôi đề xuất LaMo, một khung RL ngoại tuyến tận dụng các Mô hình Ngôn ngữ (LM) đã được huấn luyện trước cho điều khiển Chuyển động mức thấp. Trên các nhiệm vụ thưởng thưa, LaMo đạt được kết quả mạnh và vượt qua các thuật toán mạnh gần đây CQL, IQL, TD3+BC, và DT; Trên các nhiệm vụ thưởng dày đặc, LaMo cải thiện đáng kể Decision Transformer và thu hẹp khoảng cách giữa các phương pháp dựa trên giá trị và các phương pháp dựa trên DT. Đáng chú ý, trong các tình huống dữ liệu thấp, phương pháp của chúng tôi thể hiện khả năng học few-shot mạnh mẽ, có thể được quy cho thiên hướng quy nạp từ LM đã được huấn luyện trước.

Cũng quan trọng là thừa nhận những hạn chế của công trình chúng tôi. Trên các nhiệm vụ MuJoCo thưởng dày đặc, chúng tôi thấy rằng CQL rất cạnh tranh với LaMo, cho thấy các phương pháp dựa trên giá trị vẫn rất mạnh trong RL ngoại tuyến. Bên cạnh đó, mất mát dự đoán ngôn ngữ phụ trợ trong LaMo chỉ cho thấy lợi thế của nó trong các nhiệm vụ horizon rất thấp, ví dụ, Kitchen, trong khi trong các nhiệm vụ khác, nó phục vụ mục đích bảo tồn khả năng ngôn ngữ nhưng không tăng hiệu suất đáng kể. Cách tận dụng tốt hơn khả năng lý luận ngôn ngữ để giúp thêm RL ngoại tuyến do đó là một hướng tương lai. Cuối cùng, bị hạn chế bởi tài nguyên tính toán, chúng tôi đã không xem xét việc sử dụng các mô hình ngôn ngữ lớn hơn (Touvron et al., 2023a;b; Chung et al., 2022), và chúng tôi hy vọng công trình của chúng tôi có thể thúc đẩy cộng đồng khám phá thêm các ứng dụng của LLM trong RL ngoại tuyến.

--- TRANG 9 ---
TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch từ tiếng Anh sang tiếng Việt, bao gồm các tác giả, tiêu đề bài báo, tên hội nghị/tạp chí, năm xuất bản và các thông tin liên quan khác]

--- TRANG 10 đến TRANG 24 ---
[Nội dung các trang tiếp theo bao gồm các phần phụ lục với thông tin chi tiết về cài đặt thực nghiệm, mô tả dữ liệu, thông số siêu tham số, kết quả bổ sung và thảo luận mở rộng]
