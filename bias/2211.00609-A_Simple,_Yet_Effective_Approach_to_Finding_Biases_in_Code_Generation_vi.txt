# 2211.00609.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/bias/2211.00609.pdf
# Kích thước tệp: 2247033 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Phương Pháp Đơn Giản Nhưng Hiệu Quả để Tìm Ra Thiên Kiến trong Sinh Mã

Spyridon Mouselinos
Đại học Warsaw
s.mouselinos@uw.edu.pl

Mateusz Malinowski
DeepMind
mateuszm@deepmind.com

Henryk Michalewski
Google, Đại học Warsaw
henrykm@google.com

Tóm tắt

Gần đây, các hệ thống sinh mã hiệu suất cao dựa trên mô hình ngôn ngữ lớn đã xuất hiện. Chúng được huấn luyện trên các tập dữ liệu khổng lồ chứa nhiều văn bản tự nhiên hơn mã máy tính thực thi được. Nghiên cứu này cho thấy rằng các hệ thống sinh mã hiện tại thể hiện những thiên kiến không mong muốn được thừa hưởng từ các mô hình ngôn ngữ lớn nền tảng của chúng, điều này có thể làm giảm chất lượng mã được sinh ra trong những tình huống cụ thể.

Để nghiên cứu hiệu ứng này, chúng tôi đề xuất khái niệm "khối ảnh hưởng", cho phép phân tách và phân tích mô-đun hóa các thách thức lập trình. Chúng tôi giới thiệu một cơ chế can thiệp tự động gợi nhớ đến việc kiểm thử đối kháng, phơi bày những thiên kiến không mong muốn thông qua các chế độ thất bại của các mô hình được kiểm thử. Cuối cùng, chúng tôi chứng minh cách framework của chúng tôi có thể được sử dụng như một kỹ thuật biến đổi dữ liệu trong quá trình tinh chỉnh, đóng vai trò như một chiến lược giảm thiểu những thiên kiến này.

1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) gần đây đã chứng minh khả năng sinh mã (Li et al., 2022; Brown et al., 2020; Wang et al., 2021) hoặc giải quyết các tác vụ lập trình/toán học thách thức ngang tầm với các lập trình viên con người (Li et al., 2022; Lewkowycz et al., 2022b; Chowdhery et al., 2022a); những mô hình này được huấn luyện bằng mô hình dựa trên dữ liệu. Mặt khác, một số nghiên cứu ngày càng tăng cũng đặt câu hỏi liệu phương pháp dựa trên dữ liệu có dẫn đến việc thu được kỹ năng lý luận hay không (Piekos et al., 2021; Zhang et al., 2022; Mouselinos et al., 2022), cho thấy rằng nếu được để riêng, nó có thể không đủ để đạt được hiệu suất thực sự ở mức con người trên các tác vụ như lý luận logic hoặc thị giác. Trong nhiều trường hợp được nghiên cứu, các mô hình vẫn dựa vào các gợi ý khác nhau trong quá trình lý luận của chúng. Nghiên cứu này mở rộng các kết quả trên, tức là thiếu khả năng lý luận, đến lĩnh vực sinh mã. Cụ thể hơn, chúng tôi thiết kế một framework tự động xác định các tín hiệu tinh tế mà một mô hình sinh mã có thể khai thác. Việc thay đổi hoặc loại bỏ những tín hiệu đó đóng vai trò như một bài kiểm tra lý luận đối với khả năng sinh ra của mô hình hiện tại.

Chúng tôi cho rằng quá trình lý luận của các mô hình sinh mã nên vẫn bất biến dưới những thay đổi vẫn cung cấp đủ ngữ cảnh hoặc đặt ra ít, nếu có, thách thức bổ sung cho một lập trình viên con người. Để đạt được mục tiêu này, chúng tôi đề xuất một framework tự động và bất khả tri mô hình sửa đổi những thứ sau: (1) tên hàm, (2) từ khóa trong đặc tả vấn đề, và (3) ví dụ được cung cấp trong prompt vấn đề. Chúng tôi gọi những phần này là Khối-Ảnh-Hưởng; xem Hình 1. Mỗi khối đóng góp một phần vào ngữ cảnh cần thiết để hoàn thành chính xác. Chúng tôi cho thấy rằng những sửa đổi nhỏ của những khối này là đủ để "đánh lừa" các phương pháp sinh mã dựa trên LLM.

Kết quả của chúng tôi tiết lộ những thiên kiến như ưu tiên từ khóa và hiệu ứng ghi nhớ, có thể được xác định qua nhiều mô hình. Trong các thí nghiệm của chúng tôi, chúng tôi đảm bảo rằng bất kỳ sửa đổi nào cũng duy trì ngữ nghĩa toàn cục của thách thức lập trình. Điều này được đạt được thông qua một cơ chế lọc nhận thức ngữ cảnh đảm bảo bất kỳ thông tin nào bị thay đổi hoặc loại bỏ vẫn tồn tại và/hoặc có thể được suy ra từ phần còn lại không bị thay đổi.

Đóng góp. Những đóng góp chính của nghiên cứu chúng tôi có thể được tóm tắt thành ba điểm.

Thứ nhất, chúng tôi đề xuất một framework tự động mới xác định những thiên kiến có thể có trong các mô hình sinh mã. Framework của chúng tôi loại bỏ những gợi ý tinh tế, giới thiệu những thay đổi tối thiểu như thay thế từ khóa hoặc bỏ qua một phần khối mã, cuối cùng hoạt động như một bài kiểm tra đối kháng. Vì framework hoạt động ở cấp độ dữ liệu, nó bất khả tri với cấu trúc và hoạt động nội bộ của mô hình. Framework có thể dễ dàng điều chỉnh cho bất kỳ định dạng đầu vào hoặc ngôn ngữ lập trình nào.

Thứ hai, chúng tôi giới thiệu khái niệm "Khối Ảnh Hưởng". Chúng tôi đề xuất rằng mỗi trường hợp của một thách thức lập trình điển hình có thể được phân tích thành ba phần (khối). Mỗi phần có tương quan với một phương pháp gợi ý khác nhau và được sử dụng như một mục tiêu của các biến đổi của chúng tôi. Quá trình lý luận của một mô hình được thông báo bởi cả ba khối, khiến chúng trở thành công cụ phân tích hoàn hảo cho các trường hợp sinh mã thất bại.

Thứ ba, chúng tôi khám phá những cách mới để giảm thiểu thiên kiến trong quá trình sinh mã. Trong Mục 6, chúng tôi nghiên cứu hiệu ứng của huấn luyện đối kháng chống lại các nhiễu loạn được đề xuất của chúng tôi, và lợi ích của việc bao gồm các ví dụ với mô tả dài hơn trong quá trình tinh chỉnh. Kết quả của chúng tôi cho thấy rằng việc kết hợp những kỹ thuật này dẫn đến việc hoàn thành mã chính xác hơn.

2 Nghiên cứu liên quan

Phương pháp của chúng tôi được lấy cảm hứng từ các nghiên cứu của nhiều hướng nghiên cứu khác nhau, mà chúng tôi mô tả ngắn gọn ở đây.

Giải quyết các thách thức lập trình và toán học. Khả năng mới nổi của các mô hình ngôn ngữ lớn trong việc sinh, tóm tắt và dịch thông tin văn bản, gần đây đã khơi dậy sự quan tâm đến năng khiếu của chúng trong các thách thức toán học, logic và lập trình. Các tác vụ như hoàn thành mã (Chen et al., 2021; Shin et al., 2019; Hendrycks et al., 2021a; Li et al., 2022), tóm tắt mã và dịch mã (Lu et al., 2021) đã được đề xuất, với các mô hình liên tục tiến bộ hướng tới hiệu suất gần như con người. Tương tự, (Hendrycks et al., 2021b; Saxton et al., 2019; Ling et al., 2017; Amini et al., 2019) đã đề xuất các bài kiểm tra đo lường khả năng của mô hình trong việc thực hiện toán học và logic, từ các bài toán trường học đến các thách thức cấp độ cuộc thi. Kết quả ấn tượng trong nhiều ngôn ngữ lập trình cũng đã được đạt được bởi các nghiên cứu chỉ sử dụng decoder (Brown et al., 2020; Chen et al., 2021). Fried et al. (2022) đã tạo ra mô hình sinh ra đầu tiên có thể thực hiện điền vào chỗ trống sử dụng một mục tiêu che mặt nạ mới. Cuối cùng, các mô hình quy mô lớn như (Chowdhery et al., 2022b; Lewkowycz et al., 2022a) đã chứng minh khả năng đột phá trong các tác vụ ngôn ngữ, lý luận và mã đạt được hiệu suất tối tân trong nhiều lĩnh vực cùng lúc.

Thiên kiến xã hội trong các mô hình ngôn ngữ lớn. Được huấn luyện trên lượng dữ liệu công khai ngày càng tăng, các mô hình ngôn ngữ lớn đã được nghiên cứu về việc áp dụng các thiên kiến xã hội thường thấy trong con người. Wallace et al. (2019) cho thấy rằng các mô hình sinh có thể được điều kiện hóa để tạo ra nội dung độc hại, với việc sử dụng các tiền tố đối kháng vô nghĩa. Tương tự, Liang et al. (2021) đề xuất rằng các mô hình có thể áp dụng những thiên kiến và định kiến xã hội được tìm thấy trong dữ liệu huấn luyện của chúng và cung cấp các cách để áp dụng tính công bằng trong quá trình sinh. Các biện pháp đối phó đã được đề xuất bởi (Zhao et al., 2021; Liu et al., 2022), khẳng định rằng các ví dụ zero-shot được khử trùng góp phần giảm thiểu thiên kiến trong quá trình sinh.

Thăm dò lý luận thông qua thiên kiến nhận thức. Đã có những nỗ lực đáng chú ý để hệ thống hóa trí tuệ và lý luận như các khái niệm (Legg, 2008; Chollet, 2019), tuy nhiên một số nghiên cứu gần đây cố gắng tiếp cận lý luận, thông qua việc phân tích các chế độ thất bại, do thiên kiến trong các mô hình học sâu. Glockner et al. (2018) đề xuất rằng các hệ thống suy luận ngôn ngữ tự nhiên có thể dễ dàng bị đánh lừa bằng một lần hoán đổi từ cấp trên/cấp dưới duy nhất, thể hiện thiên kiến đối với lựa chọn từ cụ thể. Tương tự, Lin et al. (2020) chứng minh rằng lý luận thường thức số học trong LLM bị thiên kiến nặng nề bởi các tính từ mô tả đối tượng quan tâm. Mối quan ngại đối với các phương pháp dựa trên dữ liệu hiện tại đã được bày tỏ bởi Razeghi et al. (2022), chỉ ra rằng LLM chính xác hơn trên các thách thức toán học liên quan đến các thuật ngữ xuất hiện thường xuyên hơn đáng kể trong tập dữ liệu tiền huấn luyện của chúng. Piekos et al. (2021) khẳng định rằng LLM có thể trả lời các câu hỏi toán học và logic mà không hiểu lý do đằng sau chúng, dựa mù quáng vào sự tồn tại của các từ khóa cụ thể. Chúng tôi đặt nghiên cứu của mình vào dòng nghiên cứu này, khiêu khích và nghiên cứu những thất bại của LLM dưới các tác vụ lập trình nặng về lý luận. Mục tiêu chính của chúng tôi bao gồm việc xác định các nguồn thiên kiến nhận thức, tức là từ, cấu trúc, hoặc mô hình đồng xuất hiện, tồn tại trong LLM hiện tại, và dẫn đến những thất bại có hệ thống của lý do.

Phương pháp đối kháng và Xử lý ngôn ngữ. Cộng đồng NLP đã phát triển các phương pháp xuất sắc để chuẩn bị các tác vụ đối kháng, bao gồm framework TextAttack (Morris et al., 2020) và các kỹ thuật tinh vi để khơi gợi các ví dụ đối kháng từ con người, như trong Talmor et al. (2022), mặc dù nghiên cứu của chúng tôi dường như là nghiên cứu đầu tiên tập trung vào việc xây dựng có kỷ luật các ví dụ đối kháng cho mã.

3 Bộ dữ liệu đánh giá

Trong mục này, chúng tôi mô tả các tập dữ liệu được sử dụng trong các thí nghiệm của chúng tôi. Chúng tôi sử dụng các thách thức lập trình được sử dụng rộng rãi HumanEval (HE) và MBPP và một tập dữ liệu phức tạp hơn với mô tả dài về các vấn đề (DMCC). Thông tin thêm về các tập dữ liệu có thể được tìm thấy trong Phụ lục 10.2.

HumanEval (HE). Đây là một tập dữ liệu giải quyết vấn đề được tuyển chọn bởi con người được mô tả trong Chen et al. (2021). Nó bao gồm 164 thách thức lập trình nguyên bản đánh giá hiểu biết ngôn ngữ, thuật toán và toán học đơn giản. Mỗi vấn đề được trình bày như một hàm chưa hoàn chỉnh, kèm theo một docstring. Docstring chứa tác vụ và một vài trường hợp ví dụ. Đối với mỗi tác vụ, chúng tôi được cung cấp một tập hợp các bài kiểm tra đơn vị. Một tác vụ được coi là đã giải quyết khi tất cả các bài kiểm tra đơn vị đều được thông qua.

Mostly Basic Python Problems (MBPP). Được giới thiệu trong Austin et al. (2021), nó chứa 974 hàm Python ngắn được thiết kế để được giải quyết bởi các lập trình viên cấp độ nhập môn. Trái ngược với HumanEval, mỗi tác vụ được đưa ra thông qua mô tả văn bản thay vì docstring. Vì không có ví dụ đầu vào-đầu ra trong prompt, chúng tôi tạo ra 3 cặp hợp lệ sử dụng các giải pháp mã được cung cấp. MBPP thách thức các mô hình thực hiện các tác vụ của luồng điều khiển bắt buộc, yêu cầu vòng lặp và điều kiện.

DeepMind Code Contests (DMCC). Là tập dữ liệu rất thách thức được đề xuất bởi Li et al. (2022). Tập dữ liệu bao gồm các vấn đề từ nền tảng Codeforces (Mirzayanov, 2020), Description2Code (Caballero, 2016), và CodeNet (Puri et al., 2021). Chúng tôi sử dụng các thách thức được viết bằng ngôn ngữ Python3 của phần huấn luyện cho các thí nghiệm của chúng tôi. DMCC chứa mô tả dài về các vấn đề và ví dụ đầu vào-đầu ra của các hàm cần hoàn thành.

Trong nghiên cứu này, DMCC được sử dụng cho các đặc tính ngữ cảnh dài của nó trong các thí nghiệm tinh chỉnh tăng cường (Bảng 5). Các mô hình được trình bày trong nghiên cứu của chúng tôi đạt được điểm số không hoặc gần không trên nó; do đó nó được loại trừ khỏi phân tích nhiễu loạn của chúng tôi, với HumanEval và MBPP là những mục tiêu phù hợp hơn.

4 Đánh giá

Mô hình. Trong thiết lập thí nghiệm của chúng tôi, chúng tôi kiểm thử năm mô hình đại diện cho các phương pháp khác nhau đối với sinh mã. CodeParrot (Tunstall et al., 2022a) đi kèm với một tập dữ liệu mã nguồn mở và có thể dễ dàng được sử dụng cho các thí nghiệm tinh chỉnh do kích thước của nó. Biến thể nhỏ hơn của nó (110M) đạt được kết quả cạnh tranh với các LLM mã nguồn mở khác ở ngân sách tham số lớn hơn. Bằng cách khám phá tập dữ liệu của nó, chúng tôi đã kiểm thử giả thuyết của mình rằng tên hàm hoạt động như thiên kiến trong quá trình sinh mã. Các mô hình có thể được truyền cảm hứng mạnh mẽ từ các đoạn mã có tên tương tự trong tập huấn luyện của chúng và dùng đến việc sao chép toàn bộ hoặc các phần của giải pháp thay vì thực hiện lý luận. (Xem Phụ lục A.9) Chúng tôi cũng kiểm thử mô hình Incoder (Fried et al., 2022), được huấn luyện dưới một mục tiêu nhân quả hai chiều mới, có thể xử lý ngữ cảnh hiệu quả hơn so với các đối tác nhân quả của nó. Trái với giả thuyết ban đầu của chúng tôi, các phương pháp của chúng tôi gây ra sụt giảm hiệu suất đáng kể mặc dù khả năng hiểu ngữ cảnh được nâng cao của mô hình (Bảng 3). Mô hình Bloom (Mitchell et al., 2022) thể hiện khả năng mới nổi trong nhiều lĩnh vực bằng cách huấn luyện trên nội dung đa ngôn ngữ và đa mục đích quy mô lớn. Mặc dù không phải là mô hình sinh mã, nó hoạt động tương đương tốt với các mô hình chuyên biệt về mã trong cùng ngân sách tham số. Về mặt lý thuyết, hiệu ứng thiên kiến có thể được giảm bớt khi một mô hình được tiếp xúc với các ví dụ huấn luyện đa dạng. Các thí nghiệm của chúng tôi tiết lộ rằng điều này vẫn không phải là trường hợp dưới thiết lập của chúng tôi, và các giải pháp sau huấn luyện được khám phá. CodeGen (Nijkamp et al., 2022) là một mô hình hiệu suất cao được huấn luyện trong hiểu biết ngôn ngữ tự nhiên và mã. Chúng tôi kiểm thử biến thể Mono của nó, được tinh chỉnh thêm trên ngôn ngữ Python. Cuối cùng, chúng tôi có mô hình Codex mạnh mẽ, có thể giải quyết hầu hết các thách thức lập trình được đề xuất trong các tập dữ liệu HumanEval và MBPP. Danh sách các mô hình được kiểm thử, cũng như KeyBert (Grootendorst, 2020) được sử dụng trong framework của chúng tôi, có thể được tìm thấy trong Bảng 1.

Tên Mô hình | Kích thước Sử dụng
KeyBert (Grootendorst, 2020) | 2M
Codeparrot (Tunstall et al., 2022a) | 110M / 350M*/ 1.5B
InCoder (Fried et al., 2022) | 1.6B / 6B
CodeGen (Nijkamp et al., 2022) | 350M / 6B
Bloom (Mitchell et al., 2022) | 560M* / 1.7B / 176B†
Codex (v1 / v2) (Chen et al., 2021) | 175B†(Ước tính)
Bảng 1: Các mô hình được sử dụng: (*) đề cập đến tinh chỉnh và (†) đến API.

Chỉ số hiệu suất. Chúng tôi đánh giá tính chính xác chức năng của các chương trình được sinh ra bằng chỉ số pass@k, được giới thiệu trong Kulal et al. (2019). Chỉ số này phục vụ như một ước lượng khả năng sinh ra của mô hình dưới một ngân sách cụ thể. Trong Chen et al. (2021), các tác giả đề xuất một phiên bản cập nhật không thiên kiến mà chúng tôi áp dụng trong suốt phần còn lại của nghiên cứu này. Để tránh bất kỳ nhầm lẫn nào, chúng tôi tính pass@k ở chính xác k lần thử. Trung bình của mười lần chạy với các seed khác nhau được trình bày cho tất cả các thí nghiệm trong Bảng 3. Chúng tôi sử dụng nhiệt độ lấy mẫu 0.2 / 0.8 cho pass@1 / pass@100, đây là các giá trị tối ưu qua các mô hình được kiểm thử.

5 Phương pháp

5.1 Khối Ảnh Hưởng

Phương pháp của chúng tôi coi mỗi thách thức lập trình như một sự kết hợp của ba khối riêng biệt nhưng bổ sung thay vì một đầu vào đồng nhất duy nhất. Chúng tôi gọi chúng là Khối Ảnh Hưởng và liên kết mỗi khối với một nguồn thiên kiến khác nhau trong quá trình sinh mã. Lấy ví dụ Hình 1, chúng tôi thách thức mô hình hoàn thành một hàm đảo ngược một danh sách và sau đó trả về phần tử thứ hai của nó.

Khối Tên. Khối ảnh hưởng đầu tiên, được đánh dấu màu đỏ, thông báo cho mô hình về tên hàm và tên cũng như kiểu của các đối số đầu vào. Giả sử rằng ban đầu, một mô hình tạo ra các giải pháp chính xác cho một vấn đề. Tuy nhiên, mô hình thất bại khi chúng tôi đổi tên hàm thành thứ gì đó không liên quan đến tác vụ, ví dụ: "fun". Chế độ thất bại này cho thấy rằng mô tả vấn đề không được hiểu và mô hình cũng không thể trích xuất một mô hình lý luận từ các ví dụ sử dụng được đưa ra. Chúng tôi liên kết những trường hợp như vậy với hiệu ứng ghi nhớ, nơi mô hình dựa rất nhiều vào tên hàm, sao chép các đoạn mã từ tập dữ liệu huấn luyện của nó với cùng tên hoặc tên tương tự.

Khối Mô tả. Mô tả vấn đề đóng vai trò là khối thứ hai, được đánh dấu màu xanh lá cây. Ở đây mô hình được mong đợi tạo ra một giải pháp bằng cách sử dụng khả năng hiểu ngôn ngữ tự nhiên của nó. Chúng tôi quan sát thấy rằng việc loại bỏ các từ khóa cụ thể khỏi mô tả vấn đề có thể dẫn đến kết quả thảm khốc trong hiệu suất của mô hình. Điều quan trọng là việc loại bỏ những từ khóa này không được làm suy giảm ngữ nghĩa mô tả, và bất kỳ thông tin nào bị mất nên có thể phục hồi từ phần còn lại của ngữ cảnh. Ví dụ, trong Hình 1, việc loại bỏ cặp từ "the list" tạo ra một mô tả vẫn có thể hiểu được bởi một lập trình viên con người. Chúng tôi thách thức mô hình suy ra ngữ cảnh bị thiếu từ từ "list" trong tên hàm và kiểu danh sách đầu vào trong ví dụ được đưa ra. Sự không thể phục hồi ngữ cảnh bị thiếu được liên kết với thiên kiến ưu tiên vốn có, nơi mô hình dựa vào các manh mối từ vựng bề mặt hoặc các thuật ngữ đồng xuất hiện thường xuyên được thấy trong quá trình huấn luyện thay vì ngữ cảnh được đưa ra để "điền vào" bất kỳ khoảng trống nào về tinh thần.

Khối Ví dụ. Như khối cuối cùng, chúng tôi xem xét các ví dụ sau mô tả vấn đề. Chúng hoạt động như các minh họa, hướng dẫn mô hình đến các mô hình lý luận cụ thể. Hãy xem xét một kịch bản nơi các mô hình không thể tạo ra mã chính xác khi thiếu ví dụ. Có thể nói, nhiều hơn tác vụ và các đầu vào được đưa ra một mình là cần thiết cho mô hình để tạo thành sự hiểu biết vấn đề phù hợp. Trong chế độ thất bại này, các ví dụ được cung cấp hoạt động như một "người phá vỡ sự cân bằng lý luận" giữa các giải pháp được đề xuất mà mô hình có thể tạo ra. Các giải pháp được tạo ra không hoàn toàn không liên quan nhưng là một cách giải thích tương đối kém của vấn đề. Ví dụ, trong Hình 2, khi bị tước bỏ các ví dụ của nó, mô hình vẫn thể hiện các dấu hiệu của việc hiểu tác vụ (tức là so sánh sự khác biệt của phần tử với một ngưỡng, lặp qua các phần tử). Tuy nhiên, việc kết hợp những phần logic này theo cách có ý nghĩa đủ phức tạp để mô hình yêu cầu các ví dụ bổ sung để lọc ra các chiến lược sai lầm. Chúng tôi liên kết những hiệu ứng như vậy với khả năng lý luận kém.

5.2 Framework

Bước đầu tiên liên quan đến việc chia một thách thức lập trình thành ba Khối Ảnh Hưởng. Cho mục đích này, chúng tôi sử dụng một mô-đun biểu thức chính quy tìm kiếm các mô hình phổ biến của phần bắt đầu hoặc kết thúc của mỗi khối. (ví dụ: Khối Tên: "def (...):", Khối Mô tả: " hoặc """, Khối Ví dụ: "Examples:" hoặc >= theo sau là việc sử dụng tên hàm).

Như bước tiếp theo, Khối Mô tả được phân tích thêm để xác định các từ khóa gợi ý có thể có. Lý tưởng nhất, chúng tôi quan tâm đến các unigram hoặc bigram cung cấp thông tin dư thừa hướng tới việc hoàn thành tác vụ lập trình. Để xác định từ khóa, chúng tôi sử dụng KeyBert (Grootendorst, 2020), một LLM được giao nhiệm vụ thực hiện trích xuất từ khóa và độ tương tự từ. Chúng tôi tiến hành tinh chỉnh KeyBert trên tập dữ liệu CodeParrot mã nguồn mở (Tunstall et al., 2022a) để các đề xuất chuyên biệt về mã hơn được cung cấp. Đối với mỗi từ khóa ứng viên, chúng tôi tính toán độ tương tự embedding của nó với tập hợp các từ: [Python, Programming, Code, Variable], một lần nữa thông qua KeyBert. Các từ có điểm tương tự cosine dưới 0.7 cho tất cả các mục của tập hợp không liên quan đến lập trình và do đó được lọc ra. Tuy nhiên, việc loại bỏ từ khóa một cách bất cẩn có thể dẫn đến sự sụt giảm hiệu suất không thú vị liên quan đến việc loại bỏ thông tin quan trọng thay vì hiệu ứng gợi ý. Do đó, một giai đoạn lọc nhận thức ngữ cảnh bổ sung được sử dụng để xác thực rằng bất kỳ thông tin nào bị mất có thể được truy xuất từ thách thức lập trình còn lại.

Trong giai đoạn này, chúng tôi tính toán độ tương tự embedding của mỗi từ khóa ứng viên với mọi token từ khóa không tiềm năng. Từ khóa được đánh dấu là hợp lệ để loại bỏ nếu ít nhất một từ "gần" được xác định. Một lần nữa, chúng tôi coi các từ khóa "gần" có điểm tương tự lớn hơn 0.7. Nếu một từ khóa tồn tại ở nhiều vị trí, trường hợp đầu tiên không được đánh dấu là hợp lệ để loại bỏ, trong khi phần còn lại thì có. Khi một từ khóa tình cờ là một kiểu đối số (tức là list, integer, tuple), chúng tôi cũng tìm kiếm các trường hợp của kiểu đó trong các ví dụ hoặc khối tên. Trong trường hợp khớp, từ khóa an toàn để loại bỏ. Thông tin tương đương đã tồn tại trong ngữ cảnh. Như bước cuối cùng, chúng tôi chọn giữa các biến đổi sau:

Drop one. Loại bỏ một trong các từ khóa được cung cấp khỏi Khối Mô tả. Biến đổi được lặp lại N lần trong đó N là số lượng từ khóa được xác định.

Drop all. Loại bỏ tất cả các từ khóa được cung cấp đồng thời khỏi Khối Mô tả.

Drop examples. Loại bỏ tất cả các ví dụ được cung cấp khỏi Khối Ví dụ.

Anonymize. Thay thế tên hàm bằng một token tùy ý. Chúng tôi sử dụng "func" trong các thí nghiệm của chúng tôi. Lưu ý rằng tên hàm cũng được thay thế trong các ví dụ được cung cấp, vì vậy không có rò rỉ thông tin nào xảy ra. Chúng tôi cũng đã kiểm thử xem liệu việc chọn "func" có thể mang theo một số hiệu ứng đối kháng nội tại liên quan đến dữ liệu huấn luyện hay không. Chúng tôi đã thí nghiệm với các lựa chọn thay thế từ khác ("action", "do stuff", "XYZ") và nhận được kết quả tương tự. Hơn nữa, chúng tôi đã xác định các trường hợp nơi tên hàm, mặc dù có tương quan chặt chẽ với tác vụ hiện tại, nếu nó được coi là nguồn thông tin duy nhất, thay vào đó có thể gây hiểu lầm, biểu thị nhu cầu hiểu ngữ cảnh phù hợp của các mô hình được kiểm thử (Xem Phụ lục 10.8).

Ví dụ, hãy sử dụng framework của chúng tôi trên thách thức được trình bày trong Hình 1. Ở giai đoạn đầu tiên, KeyBert sẽ xác định các từ khóa sau: [Reverse, list, return, second]. Trong số này, từ second không vượt qua giai đoạn lọc đầu tiên với điểm tương tự trên 0.7 đối với tập hợp của chúng tôi. Trong giai đoạn thứ hai, mỗi từ sẽ được so sánh với tất cả các token hiện có. Reverse và return sẽ không được liên kết với các token khác. List sẽ được xác định trong tên hàm và kiểu đối số đầu vào. Ngoài ra, vì list cũng là một từ khóa python, nó sẽ được khớp với kiểu list của đầu vào được đưa ra trong các ví dụ. Điều này để lại list như từ khóa duy nhất có sẵn để loại bỏ. Nếu việc loại bỏ từ khóa được kết hợp với việc ẩn danh, việc loại bỏ vẫn sẽ hợp lệ vì thông tin vẫn sẽ có sẵn trong các ví dụ và kiểu đầu vào.

Những biến đổi này kiểm thử các giả thuyết mà chúng tôi liên kết với mỗi khối, như được trình bày trong Mục 5.1. Việc loại bỏ các gợi ý có thể dẫn đến sự sụt giảm hiệu suất giữa các thách thức gốc và đã sửa đổi, tiết lộ những thiên kiến cơ bản trong logic của các mô hình. Có thể nói, bất kỳ biến đổi nào được đề xuất của chúng tôi đều có thể phá hủy ngữ nghĩa cục bộ. Tuy nhiên, chúng tôi thực hiện các biện pháp đáng kể để đảm bảo rằng ngữ nghĩa toàn cục được bảo tồn và đủ thông tin tồn tại hướng tới giải pháp của nó. Đây cũng là lý do tại sao chúng tôi tránh thực hiện các biến đổi đồng thời trong Khối Ví dụ và Khối Mô tả, hoặc tất cả các Khối Ảnh Hưởng cùng nhau; một mô hình bị tước bỏ tất cả thông tin cần thiết không thể tạo ra một giải pháp phù hợp. Để định lượng mức độ mơ hồ có thể mà các biến đổi của chúng tôi giới thiệu, chúng tôi sử dụng bài kiểm tra phê bình LM, được lấy cảm hứng từ công trình của (Yasunaga et al., 2021; Yasunaga and Liang, 2021): Chúng tôi thu thập một mẫu ngẫu nhiên 200 thách thức lập trình từ HumanEval và MBPP. Mỗi thách thức sau đó được biến đổi theo các phương pháp được trình bày trong Bảng 2. Sau đó, đối với cả phiên bản gốc và mọi phiên bản đã sửa đổi của một thách thức, chúng tôi tính toán điểm xác suất log của chúng bằng cách sử dụng một mô hình ngôn ngữ lớn. Ý tưởng cốt lõi là mô hình sẽ hoạt động như một nhà phê bình mềm, xếp hạng các đầu vào mô hình theo tính hợp lý tổng thể của chúng. Các đầu vào đã sửa đổi có vẻ "lạ" đối với nhà phê bình và được hiểu một phần sẽ được gán điểm xác suất log thấp hơn nhiều so với những đầu vào chưa sửa đổi. Vì tiêu chí này dựa trên tính tối ưu lân cận cục bộ, chỉ những thay đổi vừa phải được phép giữa các thách thức đang được so sánh. Ví dụ, hai đoạn văn bản hoàn toàn khác nhau nhưng đúng về mặt cú pháp và ngữ nghĩa có thể có điểm xác suất log tương tự. Trong quá trình so sánh của chúng, tuy nhiên, chúng tôi sẽ vi phạm giả định về tính cục bộ, và không có kết luận nào có thể được rút ra về nội dung của chúng. Như nhà phê bình của chúng tôi, chúng tôi sử dụng mô hình Codex-v2 (Chen et al., 2021). Chúng tôi tính toán độ tương tự xác suất log như:

Sim = 100 × (LogP_Method - LogP_Original) / LogP_Original.

Bảng 2 cho thấy rằng các biến đổi của chúng tôi không giới thiệu những thay đổi mạnh mẽ đối với thách thức lập trình. Ngay cả trong biến đổi tích cực nhất là Anonymization + Drop All, nhà phê bình gán trên 94% độ tương tự giữa các thách thức mã bị ảnh hưởng bởi nó so với dạng gốc của chúng. Để so sánh, việc loại bỏ giai đoạn lọc nhận thức ngữ cảnh, dẫn đến chỉ 78% độ tương tự trong trường hợp biến đổi Anonymization + Drop All. Chúng tôi tin rằng đây là một chỉ số công bằng rằng các mô hình được kiểm thử quan sát các đầu vào có chất lượng và khả năng hiểu tương tự trong các thí nghiệm của chúng tôi. Lưu ý rằng chúng tôi bỏ qua kết quả cho phương pháp Drop Examples. Trong trường hợp này, các xác suất log sẽ thay đổi đáng kể vì chúng tôi loại bỏ nhiều token, điều này vi phạm điều kiện tiên quyết về tính cục bộ của phương pháp.

Độ tương tự (%)
Phương pháp | w/ CAF | w/o CAF
Original | 100.0 (±0.0) | 100.0 (±0.0)
Anonymization | 98.5 (±1.2) | 98.5 (±1.2)
Drop One | 97.3 (±1.5) | 84.2 (±2.2)
Drop All | 95.3 (±1.9) | 80.3 (±2.8)
Anonymization + Drop One | 95.8 (±1.4) | 80.9 (±2.3)
Anonymization + Drop All | 94.6 (±2.3) | 78.4 (±3.1)

Bảng 2: Điểm tương tự cho các phương pháp khác nhau của framework chúng tôi với (w/) và không có (w/o) cơ chế lọc nhận thức ngữ cảnh được đề xuất (CAF). Kết quả của 200 mẫu được trình bày.

6 Thí nghiệm

6.1 Kết quả về Biến đổi Khối

Kết quả chính của các thí nghiệm của chúng tôi được trình bày trong Bảng 3. Mặc dù đơn giản, các biến đổi của chúng tôi gây ra sự sụt giảm hiệu suất nhất quán qua các kích thước mô hình khác nhau trên cả hai tập dữ liệu. Việc ẩn danh đơn thuần gây ra sự sụt giảm 19% trung bình trong cả chỉ số Pass@1 và Pass@100, xác thực các tuyên bố của chúng tôi về hiệu ứng ghi nhớ. Việc loại bỏ từ khóa đơn lẻ (Drop One) và loại bỏ từ khóa hoàn toàn (Drop All) làm giảm hiệu suất của các mô hình lần lượt 15% và 22% trung bình, gợi ý sự không thể suy ra ngữ cảnh bị thiếu từ Khối Tên và Khối Ví dụ. Thay vào đó, các mô hình dựa vào việc tạo ra các đoạn mã tùy ý, thường được sử dụng mà mơ hồ phù hợp với tác vụ. Đặc biệt thú vị là các trường hợp Drop Examples và Anonymize + Drop Examples, với sự sụt giảm trung bình lần lượt là 15% và 25%. Cả hai biến đổi đều loại bỏ thông tin được cung cấp bởi các ví dụ docstring, với biến đổi sau có ràng buộc bổ sung của một hàm được ẩn danh. Với Khối Mô tả không được sửa đổi trong cả hai trường hợp, những biến đổi này nhắm vào khả năng của các mô hình để tạo ra các giải pháp dựa trên sự hiểu biết ngôn ngữ tự nhiên của chúng. Sự kết hợp của việc ẩn danh với việc loại bỏ tất cả các từ khóa (Anonymize + Drop All) dường như là biến đổi thách thức nhất tổng thể, với sự sụt giảm khoảng 40%. Mục đích chính của nó là đánh giá khả năng của mô hình trong việc suy ra ngữ cảnh bị thiếu của Khối Mô tả chỉ bằng cách quan sát các mô hình trong các ví dụ. Những quan sát này gợi ý một sự ưu tiên mô hình rõ ràng đối với các nguồn thông tin của nó, với mô tả tác vụ là nguồn chính. Do đó, khi một mô hình cạn kiệt khả năng hiểu tác vụ, nó khai thác sự tương tự của tên hàm với các giải pháp mã đã thấy trước đó. Đồng thời, lý luận của mô hình dựa vào các minh họa ví dụ, mà như đã thấy từ (Anonymize + Drop All), không phải lúc nào cũng có thể cung cấp các chỉ dẫn rõ ràng.

6.2 Hướng tới Giảm thiểu Thiên kiến

Được lấy cảm hứng từ lĩnh vực huấn luyện đối kháng, chúng tôi quyết định nghiên cứu hiệu ứng của việc sử dụng các biến đổi framework của chúng tôi như các tăng cường huấn luyện. Để đạt được mục tiêu này, chúng tôi áp dụng framework của chúng tôi vào các ví dụ của thách thức MBPP và sử dụng chúng như một tập dữ liệu tinh chỉnh cho ba mô hình Codeparrot khác nhau. Chúng tôi sử dụng HumanEval làm tập dữ liệu kiểm thử của chúng tôi, không có sự chồng chéo nào với MBPP. Bằng cách này, các mô hình của chúng tôi đã không thấy các ví dụ của tập kiểm thử trong quá trình huấn luyện hoặc các bước tinh chỉnh của chúng. Trong Bảng 4, chúng tôi so sánh kết quả của các mô hình trước và sau tinh chỉnh. Các mô hình hưởng lợi từ việc giới thiệu các ví dụ tăng cường và một phần phục hồi từ các chế độ thất bại do nhu cầu dựa vào gợi ý. Mô hình càng lớn, khả năng của nó càng được hưởng lợi nhiều hơn. Chúng tôi tin rằng hiệu ứng này có liên quan chặt chẽ đến khả năng lý luận mở rộng của các mô hình ngôn ngữ lớn và kích thước tham số của chúng. Nhu cầu dựa vào gợi ý có thể được quy cho chất lượng dữ liệu thấp hoặc thiếu các thiên kiến quy nạp cụ thể cho tác vụ. Tuy nhiên, khả năng hiểu đúng các tác vụ lập trình chắc chắn có ở đó. Để cải thiện khả năng sinh mã của các mô hình, chúng tôi do đó đề xuất việc tiếp xúc chúng với các thách thức đẩy khả năng suy luận và lý luận của chúng. Chúng tôi quyết định lặp lại các thí nghiệm, nhưng không bao gồm bất kỳ kỹ thuật tăng cường dữ liệu nào của chúng tôi trong quá trình tinh chỉnh. Chúng tôi quan sát thấy rằng dưới thiết lập này, các mô hình không thể hiện bất kỳ cải thiện đáng kể nào chống lại các nhiễu loạn của phương pháp chúng tôi. Các tăng cường dữ liệu được đề xuất của chúng tôi đẩy giới hạn lý luận của các mô hình do đó là một lựa chọn thay thế hợp lệ cho việc tinh chỉnh đơn giản.

6.3 Hiệu ứng của Ngữ cảnh Dài hơn

Khi huấn luyện nhân quả trên các tập dữ liệu lập trình, các mô hình điều kiện trên nhiều hàm và khai báo trong cùng một tệp. Đầu vào là một tập hợp của các ngữ cảnh thay đổi nhanh chóng, với mỗi hàm hoặc lớp là một thực thể tự chứa. Sau đó, một mô hình quen với việc địa phương hóa sự tập trung của nó khi được huấn luyện trên dữ liệu như vậy. Như một phần mở rộng cho thí nghiệm trước của chúng tôi, chúng tôi đo lường hiệu ứng của việc sử dụng một tập dữ liệu mô tả dài, DMCC, như một mục tiêu tinh chỉnh. Bằng cách huấn luyện trên các mô tả dài của ngôn ngữ tự nhiên, chúng tôi thúc đẩy các kỹ năng suy luận ngữ cảnh của mô hình đang được kiểm thử. Một mô hình có thể mở rộng sự tập trung của nó có thể tránh được những phiền nhiễu do từ khóa bị thiếu. Sự hiểu biết ngữ cảnh hiệu quả sẽ thay thế việc không dựa rất nhiều vào các thiên kiến nội bộ. Chúng tôi chọn Bloom làm mô hình đang được kiểm thử vì nó không được tinh chỉnh rõ ràng cho sinh mã mà thay vào đó là hiểu biết ngôn ngữ tổng quát. Trong Bảng 5, chúng tôi trình bày kết quả của việc tinh chỉnh trên MBPP, được sửa đổi bởi framework của chúng tôi. Chúng tôi quan sát thấy những cải thiện hiệu suất tương tự như trong Bảng 4. Chúng tôi thí nghiệm lại, lần này kết hợp cả MBPP và các ví dụ DMCC. Chúng tôi cho thấy rằng việc kết hợp các ví dụ của ngữ cảnh mở rộng dẫn đến hiệu suất thậm chí tốt hơn chống lại các biến đổi nhắm vào Khối Mô tả và hiểu biết ngôn ngữ. Các thí nghiệm tương tự đã được tiến hành với các biến thể CodeParrot nhưng không thành công. Chúng tôi quy điều này cho sự tập trung hạn chế liên quan đến dữ liệu huấn luyện (chỉ mã Python3) và sự khác biệt kiến trúc giữa các mô hình. Chúng tôi tin rằng việc hợp nhất lợi ích của hai thiết lập được đề xuất của chúng tôi có thể phục vụ như một hướng thú vị hướng tới khả năng phục hồi của mô hình trong các kịch bản sinh mã.

7 Kết luận

Chúng tôi trình bày một phương pháp đơn giản để cô lập các tín hiệu và đánh giá lý luận của các mô hình sinh mã thông qua các biến đổi ở cấp độ đầu vào. Phương pháp của chúng tôi coi các ví dụ mã như một sự kết hợp của ba khối, mỗi khối cung cấp các tín hiệu khác nhau cho mô hình. Chúng tôi cho thấy rằng các biến đổi nhỏ có thể dẫn các mô hình đến thất bại, biểu thị sự tồn tại của những thiên kiến. Framework của chúng tôi có thể tự động xác định và loại bỏ các từ khóa chịu trách nhiệm cho việc gợi ý gián tiếp. Chúng tôi cho thấy rằng các mô hình phổ biến với kết quả vững chắc trên các thách thức lập trình khó có tính dễ bị tổn thương đối với các bài kiểm tra của chúng tôi, với hiệu suất của chúng suy giảm đáng chú ý. Hơn nữa, chúng tôi nghiên cứu hiệu ứng của việc sử dụng các biến đổi được đề xuất của chúng tôi trong quá trình tinh chỉnh của một mô hình. Các mô hình có thể hưởng lợi từ những thay đổi được đề xuất của chúng tôi, với hiệu ứng tỷ lệ thuận với kích thước tham số của chúng. Chúng tôi tin rằng, mặc dù thành công, các hệ thống sinh mã với LLM như xương sống thừa hưởng một số thiên kiến và chế độ thất bại của chúng. Huấn luyện trên mã có cấu trúc và được tài liệu hóa tốt, kết hợp với các kỹ thuật được đề xuất của chúng tôi, là một hướng đầy hứa hẹn hướng tới sinh mã đáng tin cậy. Mặc dù phù hợp lý tưởng cho các thách thức kiểu cuộc thi, phương pháp của chúng tôi có thể được mở rộng để hỗ trợ các cơ sở mã chất lượng cao ít được định dạng hơn (ví dụ: kho lưu trữ GitHub). Để phân tích ngắn xem Mục 10.1 của Phụ lục.

8 Hạn chế

Một số hạn chế và hướng nghiên cứu có thể tồn tại trong nghiên cứu của chúng tôi. Nghiên cứu của chúng tôi tập trung vào ngôn ngữ lập trình Python3, với nhiều thách thức lập trình tồn tại trong các lựa chọn phổ biến khác (ví dụ: C, C++, Java, Scala). Mặc dù cơ chế xác định Khối Ảnh Hưởng có thể dễ dàng được điều chỉnh cho từng trường hợp, việc áp dụng trực tiếp framework của chúng tôi trong một ngôn ngữ khác sẽ dẫn đến lỗi.

Tương tự, framework giả định rằng mỗi thách thức lập trình sẽ ở định dạng "kiểu cuộc thi", có nghĩa là một mô tả vấn đề phù hợp, các ví dụ trong docstring, và mỗi kiểu đầu vào được trình bày cho mỗi ví dụ. Trong Mục Phụ lục 10.1, chúng tôi trình bày cách thích ứng với các cơ sở mã ít được định dạng hơn sẽ có thể, nhưng hiện tại, chúng tôi để nó như một nghiên cứu trong tương lai.

Cuối cùng, không có đảm bảo rằng hiệu suất được cải thiện chống lại các nhiễu loạn được đề xuất phản ánh sự gia tăng hiệu suất tương đương trong các ứng dụng trợ lý mã thế giới thực. Các đề xuất và hoàn thành mã thời gian thực phù hợp với người dùng hơn nằm ngoài phạm vi của nghiên cứu này.

9 Rủi ro và Cân nhắc Đạo đức

Nghiên cứu của chúng tôi nhằm khám phá và loại bỏ những thiên kiến trong các kịch bản sinh mã thông qua can thiệp đối kháng. Tuy nhiên, chúng tôi thừa nhận rằng mã không an toàn hoặc độc hại vẫn có thể được tạo ra sau khi tinh chỉnh với các tăng cường được đề xuất của chúng tôi. Hơn nữa, nghiên cứu của chúng tôi chỉ tập trung vào các thiên kiến nhận thức ảnh hưởng đến lý luận và logic đằng sau quá trình lập trình của các mô hình ngôn ngữ lớn. Các thiên kiến xã hội và định kiến vẫn có thể xuất hiện khi các LLM đa mục đích như Codex hoặc Bloom được sử dụng trong các kịch bản sinh văn bản điển hình. Các dấu hiệu của sự mạnh mẽ chống lại các phương pháp của chúng tôi không được nhầm lẫn với các chỉ số của các hình thức thiên kiến khác không tồn tại.

Lời cảm ơn

Tất cả các thí nghiệm được thực hiện sử dụng cụm Entropy được tài trợ bởi NVIDIA, Intel, tài trợ của Trung tâm Khoa học Quốc gia Ba Lan UMO-2017/26/E/ST6/00622 và ERC Starting Grant TOTAL. Công trình của Spyridon Mouselinos và Henryk Michalewski được hỗ trợ bởi tài trợ của Trung tâm Khoa học Quốc gia Ba Lan UMO-2018/29/B/ST6/02959.

Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên như trong bản gốc]

10 Phụ lục

[Phần phụ lục được giữ nguyên như trong bản gốc]
