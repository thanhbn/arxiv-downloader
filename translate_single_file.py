#!/usr/bin/env python3
"""
Script d·ªãch m·ªôt file c·ª• th·ªÉ t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát
"""

import os
import time
import re

def translate_text_chunks(text, max_chunk_size=6000):
    """Chia text th√†nh chunks nh·ªè ƒë·ªÉ d·ªãch v·ªõi chi·∫øn l∆∞·ª£c th√¥ng minh"""
    chunks = []
    current_chunk = ""
    
    paragraphs = text.split('\n\n')
    threshold_size = int(max_chunk_size * 1.5)  # 150% of max chunk size
    
    i = 0
    while i < len(paragraphs):
        paragraph = paragraphs[i]
        
        # Calculate remaining text size from current paragraph onwards
        remaining_text = '\n\n'.join(paragraphs[i:])
        current_plus_paragraph = current_chunk + paragraph
        
        # Check if adding this paragraph exceeds max_chunk_size
        if len(current_plus_paragraph) > max_chunk_size:
            if current_chunk:
                # Smart decision: if remaining text (including current paragraph) is < 150% of max_chunk_size,
                # include everything in current chunk instead of creating a small leftover chunk
                if len(current_chunk + remaining_text) <= threshold_size:
                    current_chunk += remaining_text
                    chunks.append(current_chunk.strip())
                    break  # All remaining text processed
                else:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph + "\n\n"
            else:
                # Paragraph qu√° d√†i, chia nh·ªè h∆°n
                sentences = paragraph.split('. ')
                j = 0
                while j < len(sentences):
                    sentence = sentences[j]
                    remaining_sentences = '. '.join(sentences[j:])
                    current_plus_sentence = current_chunk + sentence
                    
                    if len(current_plus_sentence) > max_chunk_size:
                        if current_chunk:
                            # Smart decision for sentences too
                            if len(current_chunk + remaining_sentences) <= threshold_size:
                                current_chunk += remaining_sentences
                                j = len(sentences)  # Exit sentence loop
                            else:
                                chunks.append(current_chunk.strip())
                                current_chunk = sentence + ". "
                        else:
                            chunks.append(sentence)
                            current_chunk = ""
                    else:
                        current_chunk += sentence + ". "
                    j += 1
        else:
            current_chunk += paragraph + "\n\n"
        
        i += 1
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def simulate_translation(chunk):
    """M√¥ ph·ªèng vi·ªác d·ªãch - thay b·∫±ng d·ªãch th·∫≠t"""
    print(f"ƒêang d·ªãch chunk ({len(chunk)} k√Ω t·ª±)...")
    
    # Prompt d·ªãch thu·∫≠t chuy√™n nghi·ªáp
    translation_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω d·ªãch thu·∫≠t chuy√™n nghi·ªáp, c√≥ kh·∫£ nƒÉng d·ªãch vƒÉn b·∫£n h·ªçc thu·∫≠t v√† k·ªπ thu·∫≠t m·ªôt c√°ch ch√≠nh x√°c.

Nhi·ªám v·ª• c·ªßa b·∫°n l√† d·ªãch to√†n b·ªô n·ªôi dung sau sang ti·∫øng Vi·ªát.

T·∫≠p trung v√†o vi·ªác:
1. D·ªãch thu·∫≠t ch√≠nh x√°c c√°c thu·∫≠t ng·ªØ AI/ML v√† khoa h·ªçc m√°y t√≠nh.
2. Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng c·ªßa vƒÉn b·∫£n g·ªëc (URLs arXiv, DOI, v.v.).
3. S·ª≠ d·ª•ng vƒÉn phong h·ªçc thu·∫≠t, trang tr·ªçng.
4. ƒê·∫£m b·∫£o b·∫£n d·ªãch m∆∞·ª£t m√† v√† d·ªÖ hi·ªÉu trong ng·ªØ c·∫£nh ti·∫øng Vi·ªát.
5. Gi·ªØ nguy√™n c√°c t·ª´ ti·∫øng Anh trong ngo·∫∑c ƒë∆°n sau thu·∫≠t ng·ªØ ti·∫øng Vi·ªát n·∫øu c·∫ßn thi·∫øt.
6. ƒê·∫∑c bi·ªát ch√∫ √Ω d·ªãch ƒë√∫ng c√°c ch·ªß ƒë·ªÅ nghi√™n c·ª©u AI nh∆∞ RAG, Chain-of-Thought, Multimodal, v.v.

VƒÉn b·∫£n c·∫ßn d·ªãch:
"""
    
    # M√¥ ph·ªèng d·ªãch - trong th·ª±c t·∫ø s·∫Ω g·ªçi API OpenAI ho·∫∑c Claude
    # ·ªû ƒë√¢y t√¥i ch·ªâ d·ªãch m·ªôt v√†i ph·∫ßn quan tr·ªçng
    translated_chunk = chunk
    
    # D·ªãch m·ªôt s·ªë t·ª´ kh√≥a ch√≠nh
    translations = {
        'COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models': 'COMCAT: H∆∞·ªõng t·ªõi N√©n v√† T√πy ch·ªânh Hi·ªáu qu·∫£ cho C√°c M√¥ h√¨nh Th·ªã gi√°c D·ª±a tr√™n Attention',
        'Abstract': 'T√≥m t·∫Øt',
        'Introduction': 'Gi·ªõi thi·ªáu',
        'Related Works': 'C√°c C√¥ng tr√¨nh Li√™n quan',
        'Method': 'Ph∆∞∆°ng ph√°p',
        'Experiments': 'Th√≠ nghi·ªám',
        'Conclusion': 'K·∫øt lu·∫≠n',
        'Acknowledgements': 'L·ªùi c·∫£m ∆°n',
        'References': 'T√†i li·ªáu tham kh·∫£o',
        'Attention-based vision models': 'C√°c m√¥ h√¨nh th·ªã gi√°c d·ª±a tr√™n attention',
        'Vision Transformer': 'Vision Transformer',
        'multi-head attention': 'multi-head attention',
        'low-rank compression': 'n√©n low-rank',
        'model compression': 'n√©n m√¥ h√¨nh',
        'pruning': 'c·∫Øt t·ªâa',
        'knowledge distillation': 'ch∆∞ng c·∫•t tri th·ª©c',
        'diffusion models': 'm√¥ h√¨nh diffusion',
        'text-to-image': 'text-to-image',
        'customization': 't√πy ch·ªânh',
        'feedforward network': 'm·∫°ng feedforward',
        'singular value decomposition': 'ph√¢n t√≠ch gi√° tr·ªã ƒë∆°n l·∫ª',
        'ImageNet': 'ImageNet',
        'DeiT-small': 'DeiT-small',
        'DeiT-base': 'DeiT-base',
        'top-1 accuracy': 'ƒë·ªô ch√≠nh x√°c top-1',
        'parameters': 'tham s·ªë',
        'FLOPs': 'FLOP',
        'rank selection': 'l·ª±a ch·ªçn rank',
        'neural architecture search': 't√¨m ki·∫øm ki·∫øn tr√∫c m·∫°ng n∆°-ron',
        'weight matrices': 'ma tr·∫≠n tr·ªçng s·ªë',
        'embedding': 'embedding',
        'query': 'query',
        'key': 'key',
        'value': 'value',
        'softmax': 'softmax',
        'cross-attention': 'cross-attention',
        'fine-tuning': 'tinh ch·ªânh',
        'pre-trained': 'ƒë∆∞·ª£c ti·ªÅn hu·∫•n luy·ªán',
        'training': 'hu·∫•n luy·ªán',
        'validation': 'validation',
        'GPU': 'GPU',
        'throughput': 'throughput',
        'speedup': 'tƒÉng t·ªëc',
        'storage': 'l∆∞u tr·ªØ',
        'memory': 'b·ªô nh·ªõ',
        'inference': 'suy lu·∫≠n',
        'deployment': 'tri·ªÉn khai'
    }
    
    for english, vietnamese in translations.items():
        translated_chunk = translated_chunk.replace(english, vietnamese)
    
    return translated_chunk

def translate_file(input_file, output_file):
    """D·ªãch m·ªôt file"""
    print(f"B·∫Øt ƒë·∫ßu d·ªãch file: {input_file}")
    
    # ƒê·ªçc n·ªôi dung file
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Chia th√†nh chunks
    chunks = translate_text_chunks(content)
    print(f"Chia th√†nh {len(chunks)} chunks")
    
    # D·ªãch t·ª´ng chunk
    translated_chunks = []
    for i, chunk in enumerate(chunks):
        print(f"D·ªãch chunk {i+1}/{len(chunks)}")
        translated_chunk = simulate_translation(chunk)
        translated_chunks.append(translated_chunk)
        
        # T·∫°m d·ª´ng ƒë·ªÉ tr√°nh rate limit
        time.sleep(0.1)
    
    # Gh√©p l·∫°i th√†nh vƒÉn b·∫£n ho√†n ch·ªânh
    translated_content = "\n\n".join(translated_chunks)
    
    # L∆∞u file d·ªãch
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(translated_content)
    
    print(f"ƒê√£ l∆∞u b·∫£n d·ªãch: {output_file}")
    print(f"S·ªë chunk ƒë√£ d·ªãch: {len(chunks)}")
    return output_file

def main():
    input_file = "multimodal/2305.17235-COMCAT-_Towards_Efficient_Compression_and_Customization_of.txt"
    output_file = "multimodal/2305.17235-COMCAT-_Towards_Efficient_Compression_and_Customization_of_vi.txt"
    
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªãch...")
    translate_file(input_file, output_file)
    print("‚úÖ Ho√†n th√†nh!")

if __name__ == "__main__":
    main()