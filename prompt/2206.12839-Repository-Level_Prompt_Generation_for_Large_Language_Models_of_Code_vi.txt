# 2206.12839.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2206.12839.pdf
# Kích thước tệp: 814092 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code
Disha Shrivastava1 2Hugo Larochelle1 2 3 4Daniel Tarlow1 5 3
Tóm tắt
Với sự thành công của các mô hình ngôn ngữ lớn
(LLM) của code và việc sử dụng chúng như trợ lý
lập trình (ví dụ Codex (Chen et al., 2021) được sử
dụng trong GitHub Copilot), các kỹ thuật để đưa
kiến thức đặc thù miền vào quá trình thiết kế
prompt trở nên quan trọng. Trong nghiên cứu này,
chúng tôi đề xuất một framework được gọi là Repo-
Level Prompt Generator học cách tạo ra prompt
đặc thù ví dụ sử dụng các đề xuất prompt. Các đề
xuất prompt lấy ngữ cảnh từ toàn bộ repository,
do đó kết hợp cả cấu trúc của repository và ngữ
cảnh từ các tệp liên quan khác (ví dụ import, tệp
lớp cha). Kỹ thuật của chúng tôi không yêu cầu
truy cập vào trọng số của LLM, làm cho nó có thể
áp dụng trong các trường hợp chúng ta chỉ có
quyền truy cập black-box vào LLM. Chúng tôi tiến
hành thí nghiệm trên nhiệm vụ tự động hoàn thành
code một dòng sử dụng các repository code được
lấy từ kho lưu trữ Google Code. Chúng tôi chứng
minh rằng một oracle được xây dựng từ các đề xuất
prompt của chúng tôi cho cải thiện tương đối 36%
so với Codex, cho thấy chất lượng của các đề xuất
này. Hơn nữa, chúng tôi cho thấy rằng khi chúng tôi
huấn luyện một mô hình để dự đoán một đề xuất
prompt, chúng ta có thể đạt được cải thiện hiệu suất
đáng kể so với Codex và các baseline khác. Chúng
tôi phát hành code, dữ liệu và các checkpoint đã
huấn luyện tại https://github.com/shrivastavadisha/
repo_level_prompt_generation .

1. Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện hiệu suất
đáng chú ý trong các nhiệm vụ xử lý ngôn ngữ tự nhiên
(Brown et al., 2020; Chowdhery et al., 2022), tạo ảnh từ
văn bản (Ramesh et al., 2022; Rombach et al., 2022) và
thậm chí như một agent tổng quát (Reed et al., 2022).

1Mila2Université de Montréal3Google4CIFAR Associate
Fellow5McGill University. Liên hệ: Disha Shrivastava
<dishu.905@gmail.com>.

Kỷ yếu Hội nghị Quốc tế lần thứ 40 về Học máy, Honolulu,
Hawaii, USA. PMLR 202, 2023. Bản quyền 2023 thuộc về
(các) tác giả.

Trái ngược với mô hình pretrain-finetune, việc prompting
các LLM này đã được phát hiện mang lại hiệu suất tốt
ngay cả với few-examples (Liu et al., 2023). Bên cạnh việc
cung cấp cơ chế để điều khiển và đánh giá LM, prompt đã
được chứng minh là có thể kích thích hành vi emergent.
Ví dụ về hành vi này bao gồm GPT-3 (Brown et al., 2020)
hoạt động tốt hơn trong các nhiệm vụ nó chưa bao giờ
thấy trong quá trình huấn luyện và khả năng lý luận được
cải thiện với prompt few-shot (Wei et al., 2022) và zero-
shot (Kojima et al., 2022) khuyến khích một chuỗi suy nghĩ.
Những yếu tố này làm nổi bật tầm quan trọng của việc thiết
kế prompt đặc thù nhiệm vụ hiệu quả. Tuy nhiên, hiện tại
chúng ta có hiểu biết hạn chế về cách thực hiện điều này
(Reynolds & McDonell, 2021). LLM cũng đã được sử dụng
để mô hình hóa source code với kết quả ấn tượng (Austin
et al., 2021; Fried et al., 2022; Xu et al., 2022a). Đặc biệt,
một trong những LLM hoạt động tốt nhất, Codex (Chen
et al., 2021), đã được triển khai như một phần của GitHub
Copilot1, một trợ lý code trong IDE tiên tiến. Mặc dù LLM
của code ngày càng phổ biến, không có nghiên cứu nào
giải quyết một cách có hệ thống các khía cạnh khác nhau
của việc tạo prompt liên quan đến source code. Một khía
cạnh như vậy là khi nói đến code, ngữ cảnh liên quan để
đưa vào prompt có thể đến không chỉ từ tệp hiện tại, mà
còn từ bên ngoài, chẳng hạn như import, lớp cha, tệp trong
cùng thư mục và tài liệu API. Ngoài ra, tùy thuộc vào tình
huống, ngữ cảnh liên quan có thể được phân tán qua nhiều
vị trí. Vì LLM có độ dài ngữ cảnh hạn chế có sẵn cho prompt,
việc hiểu biết đặc thù miền của chúng ta hướng dẫn việc
lựa chọn ngữ cảnh liên quan trở nên ngày càng quan trọng.
Hiện tại, không rõ làm thế nào để tích hợp kiến thức miền
này về những gì tạo nên ngữ cảnh liên quan vào việc tạo
prompt. Giải quyết câu hỏi này có lợi ích tiềm năng trong
các miền khác như trả lời câu hỏi (Liu et al., 2022) và tóm
tắt đa tài liệu (Xiao et al., 2022), nơi việc truy xuất ngữ
cảnh có cấu trúc đặc thù miền có thể hữu ích.

Trong nghiên cứu này, chúng tôi giải quyết vấn đề này bằng
cách đề xuất Repo-Level Prompt Generator (RLPG), một
framework trong khi tạo prompt, kết hợp cả cấu trúc của
repository cũng như ngữ cảnh liên quan trong các tệp trong
repository. Trong RLPG, việc lựa chọn từ đâu và lấy gì từ
repository được chỉ định bởi một tập các đề xuất prompt.
Ví dụ, một trong các đề xuất prompt có thể là

1https://copilot.github.com/
1arXiv:2206.12839v3 [cs.LG] 5 Jun 2023

--- TRANG 2 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Hình 1. Repo-Level Prompt Generator: Cho một danh sách các đề xuất prompt và vị trí hole đích cùng với repository liên quan làm đầu vào, bộ phân loại đề xuất prompt dự đoán một đề xuất prompt. Ngữ cảnh từ đề xuất prompt được dự đoán p= 14, tức là tên phương thức và nội dung từ tệp được import (được tô màu tím) sau đó được kết hợp với ngữ cảnh Codex mặc định hoặc ngữ cảnh trước vị trí hole trong tệp hiện tại (được tô màu xám) để tạo thành một prompt. Prompting Codex với prompt được tạo sẽ tạo ra dự đoán cho hole đích (được tô màu đỏ đậm).

lấy tất cả các identifier được sử dụng trong tệp import đầu tiên. Những đề xuất prompt này cho phép các kỹ sư prompt đưa chuyên môn miền của họ vào quá trình thiết kế prompt. Với việc sử dụng LLM ngày càng tăng như các agent hỗ trợ con người, nhu cầu về tính minh bạch và mong muốn của các kỹ sư phần mềm muốn tùy chỉnh prompt để phù hợp với yêu cầu của họ (Jiang et al., 2022; Sun et al., 2022), khả năng này trở nên quan trọng. Tương tự như một số nghiên cứu trước đây trong NLP (Shin et al., 2020; Schick & Schütze, 2021), các đề xuất prompt của chúng tôi là rời rạc. Tuy nhiên, thay vì cố định một đề xuất prompt cụ thể cho mỗi ví dụ, chúng tôi thay vào đó dự đoán đề xuất prompt tốt nhất có điều kiện trên ví dụ. Chúng tôi thực hiện điều này bằng cách đưa ra một mạng nơ-ron được gọi là Prompt Proposal Classifier (PPC) học cách lựa chọn một đề xuất prompt sao cho prompt kết quả có khả năng tạo ra đầu ra mong muốn. Do đó, RLPG cho phép đưa vào chuyên môn miền và đồng thời tạo điều kiện cho việc tạo prompt đặc thù ví dụ tự động thông qua một mạng nơ-ron đã học. Lưu ý rằng có một số kỹ thuật tạo prompt tự động trong NLP (Li & Liang, 2021; Shin et al., 2020; Lester et al., 2021) yêu cầu cập nhật một số hoặc tất cả trọng số của LLM. Tuy nhiên, những LLM mạnh nhất không có sẵn công khai (ví dụ OpenAI chỉ cung cấp quyền truy cập vào đầu ra được tạo từ Codex thông qua API https://openai.com/blog/openai-codex/ và không cung cấp quyền truy cập vào trọng số mô hình và dữ liệu), làm cho những kỹ thuật này ít hữu ích hơn trong tình huống này. RLPG giải quyết hạn chế này bằng cách tạo prompt giả định chỉ có quyền truy cập black-box vào LLM. Ngay cả đối với các trường hợp chúng ta có quyền truy cập vào trọng số mô hình, RLPG cung cấp một cách để thích ứng với ngữ cảnh cấp repository mà không cần phải fine-tune mô hình lặp đi lặp lại. Điều này có thể đặc biệt hữu ích khi thích ứng với một repository chứa phần mềm độc quyền hoặc ngách, mà mô hình có cơ hội hạn chế nhìn thấy trong quá trình huấn luyện.

Chúng tôi tập trung vào nhiệm vụ tự động hoàn thành code một dòng trong IDE, nơi mục tiêu là dự đoán phần bị che (hoặc hole đích) bắt đầu từ vị trí của con trỏ tưởng tượng đến cuối dòng (được tô màu xanh trong Hình 1). Chúng tôi hoạt động dưới thiết lập bảo trì cấp dòng (Shrivastava et al., 2020; Hellendoorn & Devanbu, 2017) phản ánh tình huống mà người dùng đang chỉnh sửa một tệp hiện có. Điều này có nghĩa là có thể có code sau dòng đó. Hình 1 cung cấp minh họa về phương pháp của chúng tôi. Bộ phân loại đề xuất prompt nhận vị trí hole (vị trí con trỏ) trong tệp hiện tại, repository mà tệp hiện tại thuộc về và một tập các đề xuất prompt cấp repo làm đầu vào, và dự đoán một đề xuất prompt.

--- TRANG 3 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

posal. Trong ví dụ minh họa của chúng tôi, đề xuất prompt được dự đoán tương ứng với việc lấy tên phương thức và nội dung từ MaximizingGibbsSampler.java (mg.before vị trí hole cho thấy rằng một phương thức từ tệp được import có khả năng được gọi). Prompt Composer sử dụng ngữ cảnh từ đề xuất prompt được dự đoán và kết hợp nó với ngữ cảnh Codex mặc định, tức là code trước vị trí hole trong tệp hiện tại. Prompt kết quả bao gồm tên phương thức InitializeToAssignment (từ ngữ cảnh đề xuất prompt) và phương thức CurrentAssignments() (từ ngữ cảnh Codex mặc định), dẫn đến dự đoán thành công (hộp màu nâu ở trên) của hole đích. Những đóng góp chính của chúng tôi như sau:

•Chúng tôi đề xuất một framework được gọi là Repo-Level Prompt Generator (RLPG) học cách tạo prompt có điều kiện trên ví dụ, mà không yêu cầu quyền truy cập vào trọng số của LLM.

•RLPG cho phép chúng ta sử dụng cả cấu trúc của repository cũng như ngữ cảnh liên quan từ tất cả các tệp trong repository, do đó cung cấp cơ chế để kết hợp kiến thức miền trong quá trình tạo prompt.

•Trong nhiệm vụ tự động hoàn thành code một dòng, chúng tôi cho thấy rằng một oracle được xây dựng từ các đề xuất prompt được đề xuất của chúng tôi cải thiện tương đối lên đến 36% so với Codex. Cải thiện này khá bất ngờ vì Codex chưa bao giờ thấy prompt được tạo từ những đề xuất prompt này trong quá trình huấn luyện. Hơn nữa, chúng tôi cho thấy rằng khi chúng tôi sử dụng bộ phân loại đề xuất prompt để dự đoán đề xuất prompt tốt nhất, chúng ta có thể đạt được cải thiện tương đối lên đến 17% so với Codex, cũng như cải thiện so với các baseline khác.

2. Repo-Level Prompt Generator (RLPG)

Trong phần này, chúng tôi cung cấp chi tiết về framework của chúng tôi. Chúng tôi bắt đầu bằng việc mô tả các đề xuất prompt và sau đó thảo luận về bộ phân loại đề xuất prompt, tiếp theo là mô tả về prompt composer.

2.1. Đề xuất Prompt Cấp Repo

Ý tưởng cốt lõi của RLPG bao gồm việc thay thế một phần ngữ cảnh mặc định được Codex sử dụng bằng ngữ cảnh đến từ đâu đó khác trong repository. Quyết định lấy gì và từ đâu trong repository được điều chỉnh bởi một tập các đề xuất prompt. Những đề xuất prompt này được quyết định dựa trên việc kiểm tra thủ công dữ liệu huấn luyện của chúng tôi và có ý định nắm bắt các mẫu mã hóa phổ biến (nhưng tổng quát hơn cũng có thể bao gồm các thực hành mã hóa đặc thù dự án/tổ chức). Một đề xuất prompt có thể được coi như một hàm nhận vị trí hole đích và repository mà hole là một phần làm đầu vào, và trả về ngữ cảnh đề xuất prompt (một chuỗi được tạo thành bởi ngữ cảnh từ đề xuất prompt). Một đề xuất prompt được chỉ định bởi một nguồn prompt và một loại ngữ cảnh prompt. Chúng tôi đề cập đến từng cái cùng với động lực của chúng dưới đây.

Nguồn Prompt: Đối với một vị trí hole đích, nguồn prompt xác định từ đâu chúng ta nên lấy code sẽ là một phần của ngữ cảnh đề xuất prompt. Chúng tôi đề xuất mười nguồn prompt khác nhau:

1.Current: lấy code từ tệp hiện tại loại trừ nội dung của hole đích. Tệp hiện tại là tệp chứa hole đích. Code trong tệp hiện tại (ví dụ các dòng sau vị trí hole) có thể rất hữu ích trong việc dự đoán hole đích.

2.Parent Class: lấy code từ tệp chứa lớp cha của lớp mà hole đích thuộc về. Trực giác đằng sau điều này là để tính đến các trường hợp mà một phương thức có mặt trong lớp cha được gọi trong tệp hiện tại (tức là lớp con).

3.Import: lấy code từ các tệp import được sử dụng trong tệp hiện tại. Các phụ thuộc được chỉ định thông qua import có thể cung cấp các manh mối hữu ích để dự đoán hole đích.

4.Sibling: lấy code từ các tệp trong cùng thư mục với tệp hiện tại. Các tệp trong cùng thư mục có xu hướng chia sẻ biến code (ví dụ identifier).

5.Similar Name: lấy code từ các tệp có tên tương tự như tệp hiện tại. Tên tương tự được xác định bằng cách chia tên tệp dựa trên dấu gạch dưới hoặc định dạng camelCase và sau đó khớp các phần của tên tệp. Nếu một hoặc nhiều phần khớp, hai tệp được coi là có tên tương tự. Trực giác đằng sau điều này là các nhà phát triển phần mềm có xu hướng đặt tên tệp dựa trên chức năng của code được viết trong tệp đó. Do đó, một tệp có tên tương tự có thể chứa một số phần code chung với tệp hiện tại và do đó có thể hữu ích để dự đoán hole đích.

6.Child Class: lấy code từ các tệp có tệp hiện tại là tệp lớp cha của chúng.

7.Import of Parent Class: lấy code từ các tệp import được sử dụng trong các tệp lớp cha.

8.Import of Sibling: lấy code từ các tệp import được sử dụng trong các tệp sibling.

9.Import of Similar Name: lấy code từ các tệp import được sử dụng trong các tệp có tên tương tự.

10.Import of Child Class: lấy code từ các tệp import được sử dụng trong các tệp lớp con.

Bốn nguồn prompt cuối cùng hữu ích khi hole đích xảy ra ở đầu tệp hiện tại. Trong những trường hợp này, sẽ có ít ngữ cảnh hơn đến từ các nguồn prompt khác. Đối với mỗi nguồn prompt, chúng ta có thể nhận được một tệp duy nhất hoặc một danh sách tệp được xếp hạng (xem Phụ lục B.1). Trong trường hợp sau, chúng tôi sẽ lấy ngữ cảnh từ những tệp này cho đến khi chúng tôi cạn kiệt độ dài ngữ cảnh tối đa được phân bổ cho đề xuất prompt.

--- TRANG 4 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Loại Ngữ cảnh Prompt: Loại ngữ cảnh prompt xác định code nào để lấy từ nguồn prompt. Chúng tôi đề xuất bảy loại ngữ cảnh prompt khác nhau (Phụ lục B.2 có ví dụ về mỗi loại):

1.Post Lines (PL): Lấy tất cả các dòng sau dòng hole đích cho đến cuối tệp hiện tại2.

2.Identifiers (I): Lấy tất cả các identifier được sử dụng trong nguồn prompt.

3.Type Identifiers (TI): Lấy tất cả các type identifier được sử dụng trong nguồn prompt.

4.Field Declarations (FD): Lấy tất cả các khai báo trường được sử dụng trong nguồn prompt.

5.String Literals (SL): Lấy tất cả các chuỗi literal được sử dụng trong nguồn prompt.

6.Method Names (MN): Lấy tất cả các tên phương thức cùng với chữ ký của chúng được sử dụng trong nguồn prompt.

7.Method Names and Bodies (MNB): Lấy tất cả các tên phương thức cùng với chữ ký và nội dung tương ứng được sử dụng trong nguồn prompt.

Bằng cách kết hợp nguồn prompt với loại ngữ cảnh prompt, chúng ta có tổng cộng 63 đề xuất prompt (xem Phụ lục B.4 để biết chi tiết). Lưu ý rằng tùy thuộc vào hole đích, không phải tất cả đề xuất prompt đều có thể áp dụng (ví dụ nếu không có lớp cha trong tệp hiện tại, các đề xuất prompt với nguồn prompt là tệp lớp cha sẽ không áp dụng được). Trong Hình 1, đề xuất prompt được dự đoán tương ứng với việc lấy nguồn prompt Import và loại ngữ cảnh prompt MNB. Chúng tôi nhắm đến một tập các đề xuất prompt cung cấp nhiều sự đa dạng hơn là một tập các đề xuất prompt đều tốt. Điều này đảm bảo rằng đối với bất kỳ vị trí hole nào, một số lượng đáng kể các đề xuất prompt đều có thể áp dụng.

2.2. Bộ phân loại Đề xuất Prompt (PPC)

Cho một vị trí hole, mục tiêu của bộ phân loại đề xuất prompt là dự đoán đề xuất prompt p sẽ dẫn đến thành công, nơi thành công xảy ra khi hole được dự đoán ĥ khớp chính xác với hole đích h. Nhiệm vụ này được công thức hóa như một bài toán phân loại nhị phân đa nhãn vì đối với một hole đích cho trước, nhiều hơn một đề xuất prompt có thể dẫn đến thành công. Trong công thức này, chúng tôi coi ngữ cảnh Codex mặc định như một trong các đề xuất prompt. Tiếp theo, chúng tôi mô tả quy trình huấn luyện cho PPC.

Huấn luyện: Đối với mỗi hole đích h, chúng tôi tạo một vector ground-truth Yh= [yh_p]M_{p=1} là một vector multi-hot có kích thước M, trong đó M là tổng số đề xuất prompt. Vector này được thu thập bằng cách đưa prompt được tạo từ đề xuất prompt p vào Codex và sau đó xem liệu ĥ=h. Nếu có sự khớp, chúng tôi nói rằng đề xuất prompt p thành công. Đối với hole h, nếu đề xuất prompt p có thể áp dụng và dẫn đến thành công, yh_p= 1 và sẽ bằng không trong trường hợp ngược lại. Đối với mỗi hole h, chúng tôi thu được một mask Th trong đó Th_p= 1 khi p có thể áp dụng hoặc bằng không trong trường hợp ngược lại. Loss huấn luyện tổng thể L có thể được biểu diễn như tổng của các loss hole cá nhân Lh:

L=1/N∑^N_{h=1}Lh=1/N∑^N_{h=1}1/Mh∑^Mh_{p=1}BCE(ŷh_p, yh_p)*Th_p

Trong phương trình trên, Mh=∑_p Th_p biểu thị tổng số đề xuất prompt có thể áp dụng cho h, N là tổng số hole gặp phải trong khi huấn luyện và BCE tương ứng với binary cross entropy loss. Masking đảm bảo rằng chúng tôi chỉ xem xét các đề xuất prompt có thể áp dụng. Tiếp theo, chúng tôi mô tả hai biến thể PPC của chúng tôi có thể được sử dụng để thu được dự đoán ŷh_p.

RLPG-H: Gọi Hh là cửa sổ hole bao gồm code có mặt xung quanh hole h loại trừ chính hole đó. Trong nghiên cứu của chúng tôi, chúng tôi lấy hai dòng trước vị trí hole, code cho đến vị trí hole và hai dòng sau vị trí hole. Chúng tôi sử dụng một mô hình pretrained Fφ để thu được vector biểu diễn ngữ cảnh có kích thước Z, trong đó Z là chiều của hidden state của mô hình. Cụ thể, chúng tôi lấy hidden state tại vị trí đầu tiên, tức là biểu diễn của token [CLS]. Để làm cho việc huấn luyện PPC hiệu quả về mặt tính toán, các tham số φ được đóng băng trong quá trình huấn luyện. Mô hình RLPG-H lấy biểu diễn ngữ cảnh của cửa sổ hole và chiếu nó vào không gian đề xuất prompt có kích thước M thông qua hai lớp dense với một tính phi tuyến ở giữa (xem Phương trình 1). Lấy sigmoid của đầu ra này cho dự đoán của đề xuất prompt.

ŷh_p=P(yh_p= 1|Hh)
= sigmoid( W2(relu( W1(Fφ(Hh)) +b1)) +b2)(1)

RLPG-R: Động lực đằng sau biến thể này là sử dụng sự tương tự của cửa sổ hole và ngữ cảnh đề xuất prompt để xác định đề xuất prompt nào có thể hữu ích. Cho một hole cụ thể h, gọi Ch_p biểu thị ngữ cảnh đề xuất prompt từ đề xuất prompt p. Theo trực giác, nếu cửa sổ hole chứa các biến (ví dụ identifier) tương tự như các biến trong ngữ cảnh đề xuất prompt, thì có khả năng h có thể xảy ra ở đâu đó trong Ch_p. Sự tương tự được mô hình hóa bằng cách sử dụng cơ chế attention đa đầu (Vaswani et al., 2017), bằng cách coi biểu diễn cửa sổ hole được chiếu như một query Qh và biểu diễn ngữ cảnh đề xuất prompt được chiếu Kh_p như một key. Value Vh_p giống với key.

Qh=Fφ(Hh), Kh_p=Fφ(Ch_p), Vh_p=Fφ(Ch_p)

Đầu ra từ module multi-headed attention, MultiHead (Qh, Kh_p, Vh_p) được đưa vào module G bao gồm hai lớp của mạng feedforward với kích hoạt relu ở giữa (xem Phụ lục C để biết thêm chi tiết). Đầu ra kết quả sau đó được chiếu tuyến tính và áp dụng sigmoid để có được đề xuất prompt được dự đoán.

ŷh_p=P(yh_p= 1|Hh, Ch_p)
=sigmoid(WpG(MultiHead (Qh, Kh_p, Vh_p)) +bp)

2.3. Prompt Composer

Prompt composer kết hợp ngữ cảnh từ đề xuất prompt được chọn (được cung cấp bởi PPC) với ngữ cảnh thường được Codex sử dụng (ngữ cảnh Codex mặc định) để tạo prompt. Vì tổng độ dài có thể được sử dụng cho prompt là cố định, chúng tôi áp dụng chiến lược phân bổ ngữ cảnh động trong đó nếu ngữ cảnh đề xuất prompt ngắn hơn độ dài được phân bổ, chúng tôi gán phần còn lại từ ngữ cảnh đề xuất prompt cho ngữ cảnh Codex mặc định. Ngữ cảnh đề xuất prompt luôn được thêm trước ngữ cảnh Codex mặc định. Đối với tất cả đề xuất prompt, chúng tôi gán một nửa tổng độ dài ngữ cảnh cho ngữ cảnh đề xuất prompt và phần còn lại cho ngữ cảnh Codex mặc định. Đối với post lines, ngoài ra, chúng tôi cũng gán một phần tư và ba phần tư tổng độ dài ngữ cảnh cho ngữ cảnh đề xuất prompt. Nếu ngữ cảnh đề xuất prompt hoặc ngữ cảnh Codex mặc định lớn hơn độ dài ngữ cảnh được phân bổ cho nó, chúng tôi cắt bớt nó (xem Phụ lục B.3 cho các chiến lược cắt bớt của chúng tôi).

3. Thí nghiệm và Kết quả

Trong phần này, chúng tôi mô tả cách chúng tôi tạo dataset, chi tiết thí nghiệm cùng với các phương pháp khác nhau và kết quả của chúng, và các nghiên cứu ablation thú vị.

3.1. Tạo Dataset

Để giảm thiểu các hiệu ứng gây ra bởi việc ghi nhớ tiềm năng của code có trong dataset được sử dụng để huấn luyện Codex, chúng tôi tránh các repository code từ GitHub (Chen et al., 2021). Thay vào đó, chúng tôi scraped Google Code https://code.google.com/archive/ cho các repository bằng Java (loại bỏ những cái khớp với repository trên GitHub có cùng tên). Chúng tôi chọn các repository có giấy phép cho phép, cho chúng tôi tổng cộng 47 repository. Chúng tôi chia các repository thành các split train, validation và test, trong đó mỗi repository trong toàn bộ là một phần của một split. Trong mỗi tệp trong repository, chúng tôi loại bỏ các dòng trống hoặc là một phần của comment và đặt vị trí hole là ký tự giữa trong dòng. Tất cả các ký tự từ vị trí giữa đến cuối dòng tạo thành hole đích.

Vì việc trùng lặp code đã được chỉ ra có tác động bất lợi (Allamanis, 2019), trong một repository, chúng tôi tìm các tệp là bản sao chính xác của nhau nhưng được đặt trong thư mục khác. Chúng tôi đánh dấu tất cả các bản sao như vậy là duplicate

Bảng 1. Thống kê của dataset của chúng tôi.
Tính năng Train Val Test Tổng
# Repository 19 14 14 47
# Tệp 2655 1060 1308 4757
# Hole 92721 48548 48288 189557

và bỏ qua tất cả chúng khi tạo hole đích cho dataset của chúng tôi. Hơn nữa, chúng tôi thấy rằng các repository khá không đều về kích thước. Để tránh các repository lớn thống trị việc huấn luyện PPC, chúng tôi giới hạn đóng góp tối đa của hole từ một repository là 10000, tức là nếu tổng số hole trong repository vượt quá 10000, chúng tôi chọn ngẫu nhiên 10000 hole từ tổng số hole. Vui lòng xem Bảng 1 cho thống kê dataset của chúng tôi. #Holes đại diện cho các hole sau deduplication và capping. Đối với một số đề xuất prompt của chúng tôi, chúng tôi yêu cầu thông tin ngữ nghĩa có thể thu được bằng parse tree. Chúng tôi sử dụng API tree-sitter cho Java3 cho phép chúng tôi có AST của một tệp và truy vấn nó. Vì các đề xuất prompt của chúng tôi cần thông tin ở cấp repository, chúng tôi lưu trữ một số thông tin bổ sung cho phép chúng tôi đối chiếu thông tin từ các tệp riêng lẻ theo cấu trúc thư mục bên trong repository (xem Phụ lục 3.1 để biết thêm chi tiết).

3.2. Chi tiết Thí nghiệm

Tạo Prompt: Chúng tôi sử dụng OpenAI Codex Completions API để tạo hole được dự đoán từ mô hình Codex. Đặc biệt, chúng tôi sử dụng engine code-davinci-001 với temperature được đặt thành 0.0 và tiêu chí stop là xuống dòng. Độ dài completion là 24 và độ dài prompt tối đa là 4072. Để cho phép tính toán nhanh, chúng tôi sử dụng các mô hình đơn giản như CodeBERT (Feng et al., 2020) và GraphCodeBERT (Guo et al., 2020) làm mô hình pretrained. Một trong những hạn chế của những mô hình pretrained này là độ dài ngữ cảnh tối đa có thể được lấy làm đầu vào bởi những mô hình này nhỏ hơn nhiều so với độ dài ngữ cảnh tối đa được Codex cho phép. Do đó, trong PPC khi chúng tôi thu được biểu diễn của ngữ cảnh đề xuất prompt, chúng tôi cần cắt bớt ngữ cảnh. Điều này có thể dẫn đến việc bỏ qua các phần quan trọng của ngữ cảnh đề xuất prompt trong một số trường hợp nhất định. Sử dụng các mô hình pretrained cho phép độ dài ngữ cảnh lớn hơn hoặc các mô hình tăng cường ngữ cảnh (Wu et al., 2022) cung cấp hướng cho nghiên cứu tương lai. Xem Phụ lục D.5 cho kết quả khi chúng tôi sử dụng độ dài ngữ cảnh nhỏ hơn với Codex.

Độ phức tạp Tính toán và Khả năng Mở rộng của RLPG: Để thu thập dữ liệu ground-truth cho việc huấn luyện bộ phân loại đề xuất prompt của chúng tôi, chúng tôi truy vấn API Codex cho từng đề xuất prompt có thể áp dụng trên mỗi hole (với batching 20 truy vấn, chúng tôi có giới hạn tốc độ tối đa 400 hole mỗi phút). Điều này tương đương với ~150k truy vấn để có nhãn cho dữ liệu huấn luyện, ~80k truy vấn để có nhãn cho dữ liệu validation, tổng cộng ~230k truy vấn cho huấn luyện, tức là 1.63 truy vấn trên mỗi hole đích. Độ phức tạp tính toán của việc huấn luyện biến thể RLPG-R lớn hơn của chúng tôi (3.6M tham số, 141269 hole, và 9.19 phút mỗi epoch trên một GPU Tesla V100 duy nhất) nhỏ hơn nhiều so với finetuning tất cả hoặc một phần của Codex (175B tham số). Trong quá trình inference, chúng tôi cần tính toán thống kê cấp repo chỉ một lần và tất cả các completion hole tiếp theo trong repo có thể sử dụng thông tin cached này, không phát sinh thêm độ phức tạp tính toán. Bên cạnh việc huấn luyện PPC, tất cả thí nghiệm của chúng tôi được thực hiện trên CPU với 8GB RAM. Các đề xuất prompt của chúng tôi dựa trên các khái niệm như post lines, import, tệp có tên tương tự, tên phương thức và identifier khá tổng quát và có thể áp dụng cho các ngôn ngữ lập trình khác. Ngoài các đề xuất prompt hiện có, framework của chúng tôi cung cấp tính linh hoạt để kết hợp các đề xuất prompt mới. Vì chi phí huấn luyện lại RLPG với các đề xuất prompt mở rộng cực kỳ thấp (thấp hơn nhiều so với finetuning Codex với các đề xuất prompt mới), framework của chúng tôi có thể được sử dụng để can thiệp vào LLM để giải quyết các điểm yếu quan sát được miễn là can thiệp có thể được biểu diễn như một đề xuất prompt thêm ngữ cảnh thiếu vào LLM. Trái ngược với các kỹ thuật thực hiện prompt engineering trong không gian tiềm ẩn và yêu cầu quyền truy cập vào trọng số của LLM như Li & Liang (2021), RLPG tạo điều kiện biểu đạt ý định dưới dạng các đề xuất prompt trực quan cho con người, dễ hiểu và không yêu cầu quyền truy cập vào trọng số của LLM.

Phương pháp: Chúng tôi thí nghiệm với các phương pháp sau để tạo prompt:

1.Codex: Sử dụng ngữ cảnh mặc định từ Codex làm toàn bộ prompt.

2.Oracle: Sử dụng vector ground-truth Yh (được đề cập trong Phần 2.2). Prompt được tạo tương ứng với việc sử dụng bất kỳ đề xuất prompt thành công nào (tức là yh_p= 1). Vì thông tin này không có sẵn tại inference, oracle đại diện cho giới hạn trên.

3.Fixed Prompt Proposal: Sử dụng đề xuất prompt thành công nhất cho tất cả hole đích. Điều này được chọn dựa trên hiệu suất trên validation set và tương ứng với việc lấy 75% tổng độ dài ngữ cảnh từ post lines trong tệp hiện tại.

4.RLPG-H và RLPG-R: Sử dụng đề xuất prompt được dự đoán bởi các biến thể RLPG-H và RLPG-H của PPC. Đề xuất prompt được chọn tương ứng với việc lấy argmax của xác suất được dự đoán trên các đề xuất prompt khác nhau.

5.RLPG-BM25: Thay vì sử dụng PPC để xếp hạng đề xuất prompt, sử dụng điểm số thu được bởi BM25 (Jones et al., 2000) để chọn đề xuất prompt tốt nhất. Điểm số được tính với cửa sổ hole là truy vấn và ngữ cảnh đề xuất prompt là search

Bảng 2. Hiệu suất của oracle so với Codex.
Data
Split Success Rate
Codex(%) Success Rate
Oracle(%) Rel.↑
over Codex(%)
Train 59.78 80.29 34.31
Val 62.10 79.05 27.28
Test 58.73 79.63 35.58

documents. Điều này phục vụ như một phương pháp retrieval không học sử dụng các đề xuất prompt của chúng tôi.

6.File-level BM25: Giống như trên, ngoại trừ thay vì sử dụng ngữ cảnh đề xuất prompt của chúng tôi, search documents bao gồm ngữ cảnh đầy đủ từ các tệp khác trong repository.

7.Random: Đối với mỗi hole đích, chọn ngữ cảnh ngẫu nhiên từ bất kỳ đâu trong repository.

8.Random NN: Giống như Random, ngoại trừ trong số các ngữ cảnh được chọn ngẫu nhiên, chúng tôi lấy các nearest neighbour của cửa sổ hole trong không gian biểu diễn của mô hình pretrained. Điều này tương tự như kỹ thuật được sử dụng trong Liu et al. (2022).

9.Identifier Usage: Đối với mỗi hole đích, chúng tôi lấy identifier gần nhất và lấy usage window của identifier đó từ khắp nơi trong repository. Usage window bao gồm hai dòng trên và hai dòng dưới usage line, bao gồm usage line. Chúng tôi có thể xếp hạng usage window hoặc ngẫu nhiên (random) hoặc dựa trên khoảng cách nearest neighbour đến cửa sổ hole trong không gian biểu diễn (NN).

Bốn phương pháp cuối cùng giúp chúng tôi hiểu hiệu suất khi ngữ cảnh khác với ngữ cảnh đề xuất prompt được sử dụng. Để tạo prompt bằng các phương pháp này, chúng tôi lấy 50% ngữ cảnh từ những cái này tiếp theo là ngữ cảnh Codex mặc định chiếm độ dài ngữ cảnh còn lại. Đối với các baseline NN, chúng tôi sử dụng CodeBERT (Feng et al., 2020) làm mô hình pretrained. Các ngữ cảnh được lấy theo thứ tự tăng dần của khoảng cách nearest neighbour cho đến khi chúng tôi cạn kiệt độ dài ngữ cảnh được phân bổ. RLPG-BM25 giúp chúng tôi hiểu vai trò của PPC. Xem Phụ lục C.3 để biết thêm chi tiết về việc triển khai các phương pháp này.

Metric Đánh giá: Như đã đề cập trong Phần 2.2, để đo lường thành công, chúng tôi sử dụng exact match giữa chuỗi hole được dự đoán được tạo bởi Codex và chuỗi hole đích. Trong thí nghiệm của chúng tôi, chúng tôi báo cáo phần trăm hole thành công chia cho tổng số hole cho mỗi split. Chúng tôi sẽ gọi đây là success rate (SR) từ nay.

3.3. Kết quả

Trong phần này, chúng tôi trình bày kết quả của hai câu hỏi nghiên cứu sau được khám phá trong bài báo:

--- TRANG 7 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Bảng 3. Success Rate (SR) của các phương pháp khác nhau trên dữ liệu test khi được tính trung bình trên tất cả hole.

Phương pháp Success Rate(%) Rel. ↑(%)
Codex (Chen et al., 2021) 58.73 -
Oracle 79.63 35.58
Random 58.13 -1.02
Random NN 58.98 0.43
File-level BM25 63.14 7.51
Identifier Usage (Random) 64.93 10.55
Identifier Usage (NN) 64.91 10.52
Fixed Prompt Proposal 65.78 12.00
RLPG-BM25 66.41 13.07
RLPG-H 68.51 16.65
RLPG-R 67.80 15.44

•[RQ1]- Có hữu ích khi tạo một prompt được tạo thành từ ngữ cảnh code khác với ngữ cảnh Codex mặc định không? Nếu có, ngữ cảnh nào có thể hữu ích?

•[RQ2]- Đối với mỗi hole đích, có cách nào tự động chọn prompt không? Nếu có, hệ thống này hoạt động như thế nào so với Codex?

RQ1 - Hiệu suất của Đề xuất Prompt: Chúng tôi thấy rằng việc kết hợp ngữ cảnh đề xuất prompt (ngữ cảnh từ các tệp khác trong repository) với ngữ cảnh Codex mặc định dẫn đến cải thiện hiệu suất đáng kể. Bảng 2 cho thấy hiệu suất của oracle được xây dựng từ các đề xuất prompt của chúng tôi. Chúng ta thấy rằng trên tất cả data split, các đề xuất prompt đóng góp vào cải thiện đáng kể so với Codex (lên đến 36% cho test split). Những kết quả này có thể tưởng chừng đáng ngạc nhiên vì Codex chưa được huấn luyện trên prompt bao gồm ngữ cảnh khác với ngữ cảnh Codex mặc định. Điều làm cho kết quả này đáng ngạc nhiên hơn là trong hầu hết các trường hợp, prompt bao gồm ngữ cảnh được trộn lẫn mà không có thứ tự logic có thể thậm chí không trông giống như một đoạn code có ý nghĩa ngữ nghĩa (ví dụ danh sách string literal từ tệp sibling tiếp theo là ngữ cảnh Codex mặc định hoặc post lines được đặt trước ngữ cảnh Codex mặc định thay vì sau). Những kết quả này có thể gợi ý rằng miễn là ngữ cảnh liên quan (trong trường hợp của chúng tôi là kiến thức cấp repo dưới dạng đề xuất prompt) có mặt dưới bất kỳ hình thức nào trong prompt, nó có thể khá hiệu quả.

RQ2 - Hiệu suất của PPC: Sau khi thấy hứa hẹn trong các đề xuất prompt của chúng tôi, tiếp theo, chúng tôi trình bày kết quả của RLPG. Bảng 3 trình bày success rate cùng với phần trăm cải thiện tương đối cho dữ liệu test. Success rate được tính bằng cách lấy trung bình trên tất cả hole trong dữ liệu test (hole-wise). Như có thể thấy từ bảng, tất cả các biến thể RLPG cũng như fixed prompt proposal cải thiện hiệu suất đáng kể so với Codex. Các baseline random hoặc tệ hơn hoặc ngang bằng với Codex. Identifier usage là một

Hình 2. (Trên) Biến thiên của RLPG và Fixed Prompt Proposal với #attempts (k), khi được tính trung bình trên các repository riêng lẻ (repo-wise) và tất cả hole (hole-wise); (Dưới) Success rate trung bình của các nguồn prompt khác nhau khi chúng có thể áp dụng.

baseline tốt nhưng vẫn hoạt động tệ hơn so với fixed prompt proposal hoặc RLPG. Chúng ta thấy rằng File-level BM25 cho thấy rằng mặc dù tốt hơn Codex, nó hoạt động kém hơn so với các phương pháp sử dụng một số khái niệm ngữ cảnh có ý nghĩa ngữ nghĩa (ví dụ method body hoặc field declaration). Tuy nhiên, khi chúng tôi kết hợp BM25 với ngữ cảnh đề xuất prompt (RLPG-BM25), hiệu suất cải thiện rất nhiều. Tất cả các phương pháp dựa trên RLPG đều tốt hơn fixed prompt proposal, cho thấy giá trị của việc tạo prompt đặc thù ví dụ bằng RLPG. Tuy nhiên, cả hai biến thể học của RLPG, tức là RLPG-H và RLPG-R đều vượt trội hơn RLPG-BM25, nhấn mạnh tầm quan trọng của việc học PPC. Xem Phụ lục D.1 và Phụ lục D.7 cho hiệu suất của tất cả phương pháp trên các repository riêng lẻ. Lưu ý rằng mặc dù chúng tôi coi identifier usage như một baseline riêng biệt, người ta có thể coi nó như một trong các đề xuất prompt dẫn đến hiệu suất RLPG cải thiện hơn nữa.

Mặc dù nỗ lực tránh chồng chéo của chúng tôi, vì dữ liệu huấn luyện cho Codex không được biết chính xác, có thể có khả năng một phần dữ liệu Google Code của chúng tôi là một phần của dữ liệu huấn luyện cho Codex. Ngay cả khi có chồng chéo, chúng tôi muốn chỉ ra rằng vì Codex đã được huấn luyện với ngữ cảnh Codex mặc định, trong quá trình inference, sẽ có lợi hơn cho nó khi sử dụng ngữ cảnh Codex mặc định trong prompt (thay vì ngữ cảnh từ các đề xuất prompt hoặc bất kỳ ngữ cảnh nào khác từ các phương pháp khác). Điều này có nghĩa là dưới tình huống này, đánh giá của chúng tôi sẽ hào phóng hơn với baseline Codex, dẫn đến kết quả có lợi hơn cho baseline Codex so với các phương pháp khác chúng tôi đã sử dụng.

Biến thiên với #attempts: Hãy tưởng tượng một tình huống mà chúng ta có human-in-the-loop đã được cho k attempts để prompt LLM và sau đó chọn một trong k dự đoán. Chúng tôi muốn xem hiệu suất của framework chúng tôi biến thiên như thế nào với #attempts dưới thiết lập này. Điều này tương ứng với việc sử dụng k prompt được tạo với top-k đề xuất prompt (một prompt trên mỗi proposal) và đánh dấu thành công nếu bất kỳ k prompt nào dẫn đến thành công. Phần trên của Hình 2 cho thấy biến thiên của SR trên dữ liệu validation với giá trị k. Đối với RLPG, top-k đề xuất prompt được chọn dựa trên thứ tự giảm dần của xác suất được cung cấp bởi PPC. Đối với fixed prompt proposal, top-k đề xuất prompt được quyết định dựa trên thứ tự giảm dần của success rate của các đề xuất prompt riêng lẻ trên validation dataset. Từ hình, chúng ta nhận thấy rằng khi chúng ta tăng giá trị k, hiệu suất tăng dần lúc đầu và sau đó bão hòa về phía hiệu suất oracle (79.05% cho val data). Hành vi này được quan sát thấy cho cả fixed prompt proposal cũng như RLPG. Tuy nhiên, chúng ta thấy rằng đối với cùng một giá trị k, success rate cho RLPG cao hơn cho thấy rằng PPC học được việc xếp hạng hữu ích của ngữ cảnh đề xuất prompt có thể mở rộng tốt với #attempts.

Hiệu suất dựa trên Đề xuất Prompt: Phần dưới của Hình 2 cho thấy success rate trung bình của các nguồn prompt, trong đó thành công được tính chỉ khi nguồn prompt tương ứng có thể áp dụng. Từ hình, chúng ta thấy rằng tệp hiện tại là nguồn prompt quan trọng nhất. Theo sát là các tệp sibling và tệp có tên tương tự. Chúng ta thấy rằng tất cả nguồn prompt đều có cơ hội thành công khác không, nhấn mạnh tính hữu ích của mỗi nguồn prompt. Xem Phụ lục D.2 cho phân tích tương tự dựa trên loại ngữ cảnh prompt và Phụ lục E cho phân tích các trường hợp mẫu dẫn đến thành công và thất bại cho RLPG.

Bảng 4. Đánh giá hiệu suất dựa trên edit distance.
Phương pháp Normalized Edit Distance(%) Rel. ↑(%)
Codex 30.73 -
RLPG-H 22.55 26.62
RLPG-R 23.00 25.14

Edit Distance như một Metric: Ngoài việc đo exact match chuỗi, chúng tôi cũng đánh giá hiệu suất của RLPG bằng edit distance4 cấp ký tự như một metric. Bảng 4 báo cáo edit distance cấp ký tự trung bình được chuẩn hóa bởi tổng số ký tự trong hole đích (thấp hơn là tốt hơn). Chúng ta thấy rằng cả hai biến thể RLPG đều cho thấy cải thiện tương đối đáng kể so với Codex với RLPG-H đạt được cao nhất

4https://pypi.org/project/editdistance/

26.62% cải thiện tương đối.

Thí nghiệm với code-cushman-001: Để điều tra xem các cải thiện đạt được với RLPG có áp dụng được cho một mô hình code khác không, chúng tôi tiến hành thí nghiệm trên mô hình code-cushman-001 từ OpenAI5. Mô hình này hỗ trợ độ dài ngữ cảnh lên đến 2048 token, bằng một nửa độ dài ngữ cảnh của code-davinci-001 và dự kiến sẽ tương đối nhỏ hơn (xem Phụ lục A.2 của Rajkumar et al., 2022).

Để đánh giá RLPG, chúng tôi chọn ngữ cảnh đề xuất prompt dựa trên dự đoán từ các mô hình RLPG đã huấn luyện của chúng tôi (được huấn luyện trên nhãn thu được từ code-davinci-001). Những ngữ cảnh này sau đó được sử dụng làm prompt cho code-cushman-001 để có completion. Như được hiển thị trong Bảng 5, trên test set, RLPG-H đạt được cải thiện tương đối 10.87% và RLPG-R đạt được cải thiện tương đối 10.95% so với việc sử dụng ngữ cảnh trước trong tệp. Những kết quả này gợi ý rằng RLPG có tiềm năng cho thấy cải thiện trên các mô hình code completion khác nhau. Chúng tôi mong đợi rằng việc có các mô hình RLPG được huấn luyện trên nhãn từ code-cushman-001 sẽ cải thiện kết quả hơn nữa. Tuy nhiên, theo ý kiến của chúng tôi, việc chúng ta có thể sử dụng một mô hình RLPG duy nhất (được huấn luyện trên code-davinci-001) để có cải thiện cho hai mô hình code completion khác nhau (code-cushman-001 và code-davinci-001) khá thú vị.

Bảng 5. Success Rate (SR) với code-cushman-001.
Phương pháp Success Rate(%) Rel. ↑(%)
code-cushman-001 58.40 -
RLPG-H 64.74 10.87
RLPG-R 64.79 10.95

4. Nghiên cứu Liên quan

LLM cho Code: Gần đây, đã có nhiều nghiên cứu xung quanh các mô hình ngôn ngữ lớn của code. Các mô hình chỉ decoder tương ứng với việc tạo code từ trái sang phải (Chen et al., 2021; Austin et al., 2021; Wang & Komatsuzaki, 2021; Black et al., 2022; Xu et al., 2022a; Fried et al., 2022). Các mô hình chỉ encoder sử dụng mục tiêu masked language modeling (Feng et al., 2020; Guo et al., 2020; Kanade et al., 2020). Chúng ta cũng có các mô hình encoder-decoder thường sử dụng encoding hai chiều của ngữ cảnh để decode một chuỗi các token bị mask (Wang et al., 2021b; Li et al., 2022).

Thông tin Cấp Repo: Hellendoorn & Devanbu (2017) đề xuất một mô hình n-gram lồng nhau sử dụng cache dựa trên locality trong đó locality bao gồm tất cả thư mục từ root của dự án (bao gồm tệp hiện tại). Zhang et al. (2021)

5https://platform.openai.com/docs/models/codex

--- TRANG 9 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

sử dụng lớp cha để tạo comment cho lớp con. Pashakhanloo et al. (2022b;a) chuyển đổi repository thành cơ sở dữ liệu quan hệ và đề xuất cơ chế dựa trên graph-walk để loại bỏ ngữ cảnh không liên quan trong khi Wang et al. (2021a) đề xuất mạng nơ-ron đồ thị đa quan hệ sử dụng ngữ cảnh inter-class và intra-class để thu được tóm tắt code. Lyu et al. (2021) kết hợp đồ thị phụ thuộc API trong mô hình LSTM-based Seq2Seq để hỗ trợ việc tạo code trong khi Zhou et al. (2023) huấn luyện một mô hình để tăng cường tài liệu code cho một ý định ngôn ngữ tự nhiên. Xu et al. (2022b) kết hợp ba loại tính năng locality cấu trúc trong khi huấn luyện kNN-LM (Khandelwal et al., 2020). Những tính năng này là các biến nhị phân tương ứng với sự hiện diện hoặc vắng mặt của hierarchy tương tự. Ba cấp hierarchy là (a) tệp sibling, (b) tệp trong cùng repo (c) không có hierarchy. Ngược lại, chúng tôi có một tập các đề xuất prompt phong phú hơn nhiều kết hợp ngữ nghĩa và cấu trúc của repository. Ngoài ra, chúng tôi giả định quyền truy cập black-box vào mô hình và tạo prompt cho LLM mà không thực hiện bất kỳ finetuning nào của LLM.

Tạo Prompt: Đã có những nghiên cứu hứa hẹn xung quanh các kỹ thuật tạo prompt trong NLP. Nói chung, có hai loại kỹ thuật tạo prompt tự động. Loại đầu tiên tương ứng với việc tạo ra prompt liên tục/mềm trong đó prompt được mô tả trong không gian tiềm ẩn của mô hình ngôn ngữ (Li & Liang, 2021; Qin & Eisner, 2021; Bragg et al., 2021; Lester et al., 2021; Liu et al., 2021). Ví dụ, Prefix-Tuning (Li & Liang, 2021) thêm một prefix vào LM có thể được học bằng finetuning trên các ví dụ từ nhiệm vụ downstream. Loại thứ hai tạo ra prompt rời rạc trong đó prompt là một chuỗi văn bản có thể được con người diễn giải (Shin et al., 2020; Gao et al., 2021; Schick & Schütze, 2021). Ví dụ, Autoprompt (Shin et al., 2020) tạo prompt sử dụng template cố định bao gồm các trigger token. Các trigger token được chia sẻ qua tất cả đầu vào và được xác định bằng tìm kiếm hướng dẫn gradient liên quan đến LM. Nghiên cứu của chúng tôi thuộc loại kỹ thuật tạo prompt rời rạc vì chúng tôi tạo ra prompt bao gồm các token code có thể dễ dàng được con người diễn giải. Tuy nhiên, trái ngược với các nghiên cứu trước đây sử dụng một tập template cố định cho tất cả ví dụ, chúng tôi học cách tạo ra prompt có điều kiện trên mỗi ví dụ. Một khác biệt quan trọng khác là chúng tôi không yêu cầu quyền truy cập vào trọng số của LM. Một nghiên cứu đồng thời với chúng tôi, (Wang et al., 2022) nghiên cứu vai trò của prompt-tuning khi so sánh với fine-tuning cho dịch code, định vị lỗi và tóm tắt code. Tuy nhiên, kỹ thuật của họ yêu cầu quyền truy cập vào trọng số của LLM và họ thực hiện thí nghiệm trên các mô hình có quy mô nhỏ hơn nhiều so với Codex. Theo hiểu biết tốt nhất của chúng tôi, nghiên cứu của chúng tôi là nghiên cứu đầu tiên khám phá việc tạo prompt tự động trong thiết lập quyền truy cập black-box trong lĩnh vực source code.

5. Thảo luận

Chúng tôi lưu ý rằng các hệ thống code-completion được sử dụng kết hợp với LLM nên được triển khai một cách thận trọng (Chen et al., 2021). Sự tin tưởng mù quáng vào những hệ thống này có thể dẫn đến tác động tiêu cực tiềm năng, vì có thể có những trường hợp code được tạo ra không an toàn (Perry et al., 2022) hoặc chứa thông tin nhạy cảm. Sau khi nộp bài báo này, các LLM với độ dài ngữ cảnh đầu vào lớn hơn đã được giới thiệu, chẳng hạn như GPT-46 hỗ trợ 32k token. Với độ dài ngữ cảnh mở rộng này, người ta có thể cân nhắc bao gồm toàn bộ nội dung của repository trong prompt. Tuy nhiên, trong thực tế, các repository phần mềm thường dài hơn nhiều. Trong dataset của chúng tôi (sau deduplication), chúng tôi quan sát thấy rằng 70.22% repository chứa hơn 32k token. Đáng chú ý rằng ngoài repository hiện tại, có những nguồn ngữ cảnh liên quan khác, chẳng hạn như tài liệu API, hướng dẫn hoặc các repository liên quan, có thể hỗ trợ trong việc tự động hoàn thành code. RLPG cung cấp cơ chế để kết hợp những nguồn ngữ cảnh bổ sung này thông qua các đề xuất prompt mới. Do đó, bất kể độ dài ngữ cảnh của mô hình tạo code, RLPG cung cấp phương pháp có giá trị để xác định ngữ cảnh nào liên quan để đưa vào prompt. Với độ dài ngữ cảnh tăng trong GPT-4, chúng tôi dự đoán ít việc cắt bớt ngữ cảnh đề xuất prompt hơn, có khả năng dẫn đến cải thiện thậm chí lớn hơn với RLPG.

Kết luận, chúng tôi trình bày RLPG, một framework học cách tự động tạo prompt có điều kiện trên ví dụ, mà không yêu cầu quyền truy cập vào trọng số của LLM. RLPG sử dụng cấu trúc của repository cũng như ngữ cảnh từ các tệp khác trong repository sử dụng một tập các đề xuất prompt dễ hiểu. Trong nghiên cứu này, chúng tôi đang lấy ngữ cảnh chỉ từ một đề xuất prompt. Đối với nghiên cứu tương lai, chúng tôi muốn học một mô hình có thể tự động tạo prompt từ nhiều đề xuất prompt (xem Phụ lục D.4 cho kết quả ban đầu hứa hẹn). Các hướng thú vị khác bao gồm kết hợp phản hồi của người dùng trong RLPG và mở rộng RLPG cho tự động hoàn thành code nhiều dòng.

Lời cảm ơn

Hugo Larochelle muốn ghi nhận sự hỗ trợ của Canada CIFAR AI Chairs cho tài trợ nghiên cứu. Các tác giả muốn cảm ơn Google Cloud vì đã cung cấp tài nguyên tính toán cần thiết cho dự án này. Chúng tôi cũng muốn gửi lời cảm ơn đến Breandan Considine vì đã giúp crawl dữ liệu kho lưu trữ Google Code, đến Justine Gehring, Avinash Bhat và Breandan Considine vì đã giúp đỡ với tài nguyên để chạy thí nghiệm; và David Bieber vì phản hồi và nhận xét về bản thảo đã giúp chúng tôi cải thiện bài viết. Cuối cùng, chúng tôi muốn ghi nhận OpenAI vì đã cung cấp quyền truy cập vào API Codex.

6https://openai.com/product/gpt-4

--- TRANG 10 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Tài liệu tham khảo

Allamanis, M. Các tác động bất lợi của việc trùng lặp code trong các mô hình học máy của code. Trong Kỷ yếu Hội nghị Quốc tế ACM SIGPLAN 2019 về Ý tưởng Mới, Mô hình Mới và Suy nghĩ về Lập trình và Phần mềm. Association for Computing Machinery, 2019.

Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Tổng hợp chương trình với các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2108.07732, 2021.

Ba, J. L., Kiros, J. R., và Hinton, G. E. Chuẩn hóa lớp. arXiv preprint arXiv:1607.06450, 2016.

Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., và Weinbach, S. GPT-NeoX-20B: Một mô hình ngôn ngữ tự hồi quy mã nguồn mở. Trong Kỷ yếu Workshop ACL về Thách thức & Quan điểm trong Tạo Mô hình Ngôn ngữ Lớn, 2022.

Bragg, J., Cohan, A., Lo, K., và Beltagy, I. FLEX: Thống nhất đánh giá cho NLP few-shot. Trong Advances in Neural Information Processing Systems, 2021.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., và Amodei, D. Các mô hình ngôn ngữ là những người học few-shot. Trong Advances in Neural Information Processing Systems, 2020.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên code. arXiv preprint arXiv:2107.03374, 2021.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Mở rộng mô hình ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311, 2022.

Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al. CodeBERT: Một mô hình được huấn luyện trước cho lập trình và ngôn ngữ tự nhiên. arXiv preprint arXiv:2002.08155, 2020.

Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., và Lewis, M. Incoder: Một mô hình tạo sinh cho code infilling và synthesis. arXiv preprint arXiv:2204.05999, 2022.

Gao, T., Fisch, A., và Chen, D. Làm cho các mô hình ngôn ngữ được huấn luyện trước trở thành những người học few-shot tốt hơn. Trong Kỷ yếu Hội nghị Thường niên lần thứ 59 của Association for Computational Linguistics và Hội nghị Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên (Tập 1: Bài báo Dài), 2021.

Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., et al. GraphCodeBERT: Huấn luyện trước biểu diễn code với data flow. arXiv preprint arXiv:2009.08366, 2020.

He, K., Zhang, X., Ren, S., và Sun, J. Học deep residual cho nhận dạng hình ảnh. Trong Kỷ yếu hội nghị IEEE về computer vision và pattern recognition, 2016.

Hellendoorn, V. J. và Devanbu, P. Các mạng nơ-ron sâu có phải là lựa chọn tốt nhất để mô hình hóa source code không? Trong Kỷ yếu Cuộc họp Chung lần thứ 11 năm 2017 về Nền tảng Kỹ thuật Phần mềm, 2017.

Jiang, E., Toh, E., Molina, A., Olson, K., Kayacik, C., Donsbach, A., Cai, C. J., và Terry, M. Khám phá cú pháp và chiến lược của lập trình ngôn ngữ tự nhiên với các mô hình ngôn ngữ tạo sinh. Trong Kỷ yếu Hội nghị CHI 2022 về Yếu tố Con người trong Hệ thống Máy tính, 2022.

Jones, K. S., Walker, S., và Robertson, S. E. Một mô hình xác suất của truy xuất thông tin: phát triển và thí nghiệm so sánh - phần 1. Inf. Process. Manag., 2000.

Kanade, A., Maniatis, P., Balakrishnan, G., và Shi, K. Học và đánh giá embedding ngữ cảnh của source code. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 37 về Học máy, 2020.

Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., và Lewis, M. Tổng quát hóa thông qua ghi nhớ: Các mô hình ngôn ngữ nearest neighbor. Trong International Conference on Learning Representations, 2020.

Kingma, D. P. và Ba, J. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. Trong International Conference on Learning Representations, 2015.

Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., và Iwasawa, Y. Các mô hình ngôn ngữ lớn là những người lý luận zero-shot. arXiv preprint arXiv:2205.11916, 2022.

Lester, B., Al-Rfou, R., và Constant, N. Sức mạnh của quy mô cho prompt tuning hiệu quả tham số. Trong Kỷ yếu Hội nghị 2021 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, 2021.

--- TRANG 11 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Li, X. L. và Liang, P. Prefix-tuning: Tối ưu hóa prompt liên tục cho generation. Trong Kỷ yếu Hội nghị Thường niên lần thứ 59 của Association for Computational Linguistics và Hội nghị Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên (Tập 1: Bài báo Dài), 2021.

Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Tạo code cấp độ thi đấu với AlphaCode. Science, 2022.

Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., và Chen, W. Điều gì làm cho các ví dụ in-context tốt cho GPT-3? Trong Kỷ yếu Deep Learning Inside Out (DeeLIO 2022): Workshop lần thứ 3 về Trích xuất Kiến thức và Tích hợp cho Kiến trúc Deep Learning. Association for Computational Linguistics, 2022.

Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., và Neubig, G. Pre-train, prompt, và predict: Một khảo sát có hệ thống về các phương pháp prompting trong xử lý ngôn ngữ tự nhiên. ACM Computing Surveys, 2023.

Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., và Tang, J. GPT hiểu, cũng vậy. arXiv:2103.10385, 2021.

Lyu, C., Wang, R., Zhang, H., Zhang, H., và Hu, S. Embedding API dependency graph cho neural code generation. Empirical Softw. Engg., 2021.

Pashakhanloo, P., Naik, A., Dai, H., Maniatis, P., và Naik, M. Học cách đi bộ trên đồ thị quan hệ của source code. Trong Deep Learning for Code Workshop, 2022a.

Pashakhanloo, P., Naik, A., Wang, Y., Dai, H., Maniatis, P., và Naik, M. Codetrek: Mô hình hóa linh hoạt code sử dụng biểu diễn quan hệ có thể mở rộng. Trong International Conference on Learning Representations, 2022b.

Perry, N., Srivastava, M., Kumar, D., và Boneh, D. Người dùng có viết code không an toàn hơn với các trợ lý AI không? arXiv preprint arXiv:2211.03622, 2022.

Qin, G. và Eisner, J. Học cách hỏi: Truy vấn LM với hỗn hợp các prompt mềm. Trong Kỷ yếu Hội nghị 2021 của North American Chapter của Association for Computational Linguistics: Human Language Technologies, 2021.

Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., và Chen, M. Tạo hình ảnh có điều kiện văn bản phân cấp với clip latents. arXiv preprint arXiv:2204.06125, 2022.

Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. Một agent tổng quát. arXiv preprint arXiv:2205.06175, 2022.

Reynolds, L. và McDonell, K. Lập trình prompt cho các mô hình ngôn ngữ lớn: Vượt ra ngoài mô hình few-shot. Trong Extended Abstracts của Hội nghị CHI 2021 về Yếu tố Con người trong Hệ thống Máy tính, 2021.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., và Ommer, B. Tổng hợp hình ảnh độ phân giải cao với các mô hình diffusion tiềm ẩn. Trong Kỷ yếu Hội nghị IEEE/CVF về Computer Vision và Pattern Recognition, 2022.

Schick, T. và Schütze, H. Khai thác câu hỏi cloze cho phân loại văn bản few-shot và suy luận ngôn ngữ tự nhiên. Trong Kỷ yếu Hội nghị lần thứ 16 của European Chapter của Association for Computational Linguistics: Tập Chính, 2021.

Shin, T., Razeghi, Y., IV, R. L. L., Wallace, E., và Singh, S. AutoPrompt: Khai thác kiến thức từ các mô hình ngôn ngữ với prompt được tạo tự động. Trong Empirical Methods in Natural Language Processing (EMNLP), 2020.

Shrivastava, D., Larochelle, H., và Tarlow, D. Thích ứng on-the-fly của các mô hình source code. Trong NeurIPS 2020 Workshop on Computer-Assisted Programming, 2020.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., và Salakhutdinov, R. Dropout: một cách đơn giản để ngăn chặn overfitting của mạng nơ-ron. The journal of machine learning research, 2014.

Sun, J., Liao, Q. V., Muller, M., Agarwal, M., Houde, S., Talamadupula, K., và Weisz, J. D. Điều tra khả năng giải thích của AI tạo sinh cho code thông qua thiết kế dựa trên tình huống. Trong Hội nghị Quốc tế lần thứ 27 về Giao diện Người dùng Thông minh, 2022.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, u., và Polosukhin, I. Attention is all you need. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 31 về Neural Information Processing Systems, 2017.

Wang, B. và Komatsuzaki, A. GPT-J-6B: Một Mô hình Ngôn ngữ Tự hồi quy 6 Tỷ Tham số. https://github.com/kingoflolz/mesh-transformer-jax, 2021.

Wang, C., Yang, Y., Gao, C., Peng, Y., Zhang, H., và Lyu, M. R. Không còn fine-tuning nữa? một đánh giá thực nghiệm về prompt tuning trong code intelligence. Trong Kỷ yếu Hội nghị Chung Châu Âu lần thứ 30 về Kỹ thuật Phần mềm ACM và Hội nghị chuyên đề về Nền tảng Kỹ thuật Phần mềm, 2022.

Wang, Y., Shi, E., Du, L., Yang, X., Hu, Y., Han, S., Zhang, H., và Zhang, D. Cocosum: Tóm tắt code ngữ cảnh với mạng nơ-ron đồ thị đa quan hệ. arXiv preprint arXiv:2107.01933, 2021a.

--- TRANG 12 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Wang, Y., Wang, W., Joty, S., và Hoi, S. C. CodeT5: Các mô hình encoder-decoder được huấn luyện trước thống nhất nhận biết identifier cho hiểu và tạo code. Trong Kỷ yếu Hội nghị 2021 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, 2021b.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., và Zhou, D. Chain of thought prompting kích thích lý luận trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2201.11903, 2022.

Wu, Y., Rabe, M. N., Hutchins, D., và Szegedy, C. Memorizing transformers. Trong International Conference on Learning Representations, 2022.

Xiao, W., Beltagy, I., Carenini, G., và Cohan, A. PRIMERA: Huấn luyện trước câu bị mask dựa trên pyramid cho tóm tắt đa tài liệu. Trong Kỷ yếu Hội nghị Thường niên lần thứ 60 của Association for Computational Linguistics (Tập 1: Bài báo Dài), 2022.

Xu, F. F., Alon, U., Neubig, G., và Hellendoorn, V. J. Một đánh giá có hệ thống về các mô hình ngôn ngữ lớn của code. arXiv preprint arXiv:2202.13169, 2022a.

Xu, F. F., He, J., Neubig, G., và Hellendoorn, V. J. Nắm bắt locality cấu trúc trong các mô hình ngôn ngữ phi tham số. Trong International Conference on Learning Representations, 2022b.

Zhang, J., Panthaplackel, S., Nie, P., Mooney, R. J., Li, J. J., và Gligoric, M. Học cách tạo comment code từ hierarchy lớp. arXiv preprint arXiv:2103.13426, 2021.

Zhou, S., Alon, U., Xu, F. F., Jiang, Z., và Neubig, G. Docprompting: Tạo code bằng cách truy xuất docs. Trong International Conference on Learning Representations, 2023.

--- TRANG 13 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

A. Chi tiết Tạo Dataset

A.1. Tạo Dữ liệu Hoàn thành Hole

Để thu thập dữ liệu hoàn thành hole, chúng tôi scraped Google Code7 cho các repository được gắn thẻ với ngôn ngữ "Java". Sau đó chúng tôi deduplicate các repository bằng cách tìm kiếm repository khớp có cùng tên trên GitHub. Đối với những repository có zero tên khớp trên GitHub, chúng tôi tải xuống archive và trích xuất source code (bảo toàn cấu trúc thư mục). Tiếp theo, chúng tôi cố gắng xác định giấy phép của tất cả repository bằng cách tìm kiếm tệp LICENSE hoặc khớp với từ khóa "license", "copyright", "mit", v.v. Đối với các repo mà quy trình của chúng tôi có thể đưa ra giấy phép đã biết, chúng tôi chọn những cái có giấy phép cho phép, tức là MIT, ApacheV2 và BSD. Điều này được theo sau bởi việc loại bỏ các tệp là bản sao chính xác của nhau trong một repo. Một trong những lý do chúng tôi thấy việc trùng lặp inter-repository này có thể là vì đôi khi các nhà phát triển áp dụng thực hành tồi tệ trong đó thay vì khai báo package và import function, họ chỉ đơn giản copy-paste tệp mong muốn vào thư mục hiện tại. Các hole đích đến từ bất kỳ tệp duplicate nào không tạo thành một phần của dataset hoàn thành hole. Tuy nhiên, những tệp này có thể được sử dụng để đóng góp vào ngữ cảnh đề xuất prompt để hoàn thành hole đích trong tệp không duplicate. Chúng tôi cảm thấy thoải mái với lựa chọn này vì chúng tôi không muốn dự đoán hole đích trong tệp duplicate, nhưng chúng tôi vẫn có thể sử dụng ngữ cảnh từ tệp duplicate để dự đoán hole trong tệp không phải là duplicate của nó (ví dụ trong tệp sibling). Đối với các tệp còn lại, chúng tôi lấy mỗi dòng không phải là dòng trống hoặc comment và chọn ký tự giữa làm vị trí hole, tức là tất cả các ký tự từ giữa dòng đến cuối dòng tạo thành hole đích. Để tránh các repo lớn có bias mạnh trên bộ phân loại đề xuất prompt của chúng tôi, chúng tôi giới hạn đóng góp từ mỗi repo tối đa 10000 hole. Nếu số lượng hole trong repo vượt quá 10000, chúng tôi chọn ngẫu nhiên 10000 hole. Tokenization được thực hiện bằng tokenizer được đề xuất từ OpenAI8.

A.2. Tạo Dữ liệu cho Đề xuất Prompt Cấp Repo

Chúng tôi sử dụng API tree-sitter cho Java9 để có parse-tree của một tệp riêng lẻ trong repo. Để có thông tin ở cấp repo, đối với mỗi tệp trong repo, chúng tôi lưu trữ thông tin sau:

1. danh sách tất cả tên lớp trong tệp. Điều này giúp chúng tôi có tệp lớp cha hoặc con tương ứng với lớp cha hoặc con cho trước.
2. tệp tương ứng với mỗi câu lệnh import.
3. đối với mỗi câu lệnh import trong tệp, vị trí trong tệp nơi import được sử dụng. Điều này được sử dụng để xếp hạng các tệp dựa trên heuristics được đề cập trong Bảng 6.
4. danh sách các tệp sibling
5. danh sách các tệp có tên tương tự. Điều này được thực hiện bằng cách chia tên tệp dựa trên camel-case hoặc dấu gạch dưới. Nếu các phần con của hai tệp khớp, thì chúng được cho là có tên tương tự.

Meta-data trên được tính chỉ một lần cho mỗi repo. Các completion hole tiếp theo có thể sử dụng cùng thông tin cached. Trong thực tế, chúng ta có thể sử dụng hash để lưu trữ và truy xuất thông tin này một cách hiệu quả. Đối với một đề xuất prompt, cho nguồn prompt, trước tiên chúng tôi thu được một tệp duy nhất hoặc danh sách tệp được xếp hạng (xem Bảng 6) sử dụng thông tin trong parse tree kết hợp với meta-data cấp repo trên. Tất cả thông tin loại ngữ cảnh đề xuất prompt (MN, MNB, SL, I, TI, FD) sau đó có thể được thu thập bằng cách truy vấn parse tree của tệp được chọn.

B. Chi tiết Đề xuất Prompt

B.1. Xếp hạng tệp dựa trên nguồn prompt

Trong Bảng 6, chúng tôi cung cấp chi tiết về cách chúng tôi chọn tệp cho một nguồn prompt cho trước. Tùy thuộc vào đề xuất prompt, chúng tôi nhận được một tệp duy nhất hoặc danh sách tệp được xếp hạng dựa trên một số tiêu chí. Ví dụ, nếu nguồn prompt là Import, chúng tôi lấy tất cả câu lệnh import được sử dụng trong tệp hiện tại và xác định vị trí trong tệp hiện tại nơi các import tương ứng đã được sử dụng. Theo heuristic của chúng tôi, việc sử dụng import càng gần với vị trí hole, thì ngữ cảnh đề xuất prompt đến từ tệp import tương ứng càng có khả năng liên quan hơn (để dự đoán hole đích). Chúng tôi có một danh sách tệp import được xếp hạng được sắp xếp dựa trên thứ tự tăng dần của khoảng cách (tức là số dòng) giữa việc sử dụng import và vị trí hole. Chúng tôi bắt đầu bằng cách lấy tất cả ngữ cảnh đề xuất prompt từ tệp đầu tiên trong danh sách được xếp hạng và sau đó tiếp tục lặp qua danh sách được xếp hạng cho đến khi tổng độ dài ngữ cảnh được phân bổ cho đề xuất prompt cạn kiệt hoặc chúng tôi đến cuối danh sách được xếp hạng.

7https://code.google.com/archive/
8https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast
9https://github.com/tree-sitter/tree-sitter-java

--- TRANG 14 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Bảng 6. Chọn tệp cho một nguồn prompt

Nguồn Prompt Xếp hạng Tệp
Current tệp với hole đích. Trả về một tệp duy nhất.
Parent Class tệp chứa lớp cha xuất hiện gần nhất với hole đích. Trả về một tệp duy nhất.
Import tệp với việc sử dụng import tương ứng được xếp hạng dựa trên độ gần với hole. Trả về danh sách tệp được xếp hạng.
Sibling tệp với việc sử dụng import chung với tệp hiện tại và tệp sibling, được xếp hạng dựa trên độ gần với hole. Tổng số import chung giữa tệp hiện tại và tệp sibling được sử dụng làm tie-breaker. Trả về danh sách tệp được xếp hạng.
Similar Name tệp với việc sử dụng import chung với tệp hiện tại và tệp có tên tương tự, được xếp hạng dựa trên độ gần với hole. Tổng số import chung giữa tệp hiện tại và tệp có tên tương tự được sử dụng làm tie-breaker. Trả về danh sách tệp được xếp hạng.
Child Class tệp với việc sử dụng import chung với tệp hiện tại và tệp con, được xếp hạng dựa trên độ gần với hole. Tổng số import chung giữa tệp hiện tại và tệp lớp con được sử dụng làm tie-breaker. Trả về danh sách tệp được xếp hạng.
Import of Sibling tệp import được xếp hạng dựa trên tần suất sử dụng trong tất cả tệp sibling. Trả về danh sách tệp được xếp hạng.
Import of Similar Name tệp import được xếp hạng dựa trên tần suất sử dụng trong tất cả tệp có tên tương tự. Trả về danh sách tệp được xếp hạng.
Import of Parent Class tệp import được xếp hạng dựa trên tần suất sử dụng trong tất cả tệp lớp cha. Trả về danh sách tệp được xếp hạng.
Import of Child Class tệp import được xếp hạng dựa trên tần suất sử dụng trong tất cả tệp lớp con. Trả về danh sách tệp được xếp hạng.

B.2. Ví dụ về Loại Ngữ cảnh Prompt

Chúng tôi cung cấp ví dụ về mỗi loại ngữ cảnh prompt của chúng tôi dưới đây:

1. Post Lines (PL): Đối với ví dụ được hiển thị trong Hình 1 của bài báo chính, post lines sẽ lấy tất cả các dòng sau dòng mg.InitializeToAssignment(CurrentAssignments()) cho đến khi chúng ta đến cuối tệp (AffinityPropagation.java).

2. Identifiers (I): Identifier là tên của các biến được sử dụng trong code. Ví dụ, đối với ngữ cảnh đề xuất prompt được lấy từ tệp được import được hiển thị trong Hình 1 trong bài báo chính (được tô màu tím), identifier là InitializeToAssignment (dòng 1), a (dòng 1), currentAssignment_ (dòng 2), a (dòng 2), clone (dòng 2), alreadyInitialized_ (dòng 3), justOneRound_ (dòng 4).

3. Type Identifiers (TI): Type Identifier xác định loại của một identifier. Ví dụ, trong đoạn code class DPAffinityPropagation extends AffinityPropagation, AffinityPropagation được gắn nhãn như một type identifier. Tương tự trong đoạn DPAPParameters parameters_;, DPAPParameters là một type identifier.

4. Field Declarations (FD): Các biến của loại lớp được giới thiệu bởi field declaration. Ví dụ, double[][] mHijMujT_; và MessageValuePair[][] sortedMHijMujTs_; là ví dụ về field declaration.

5. String Literals (SL): String literal là chuỗi ký tự được bao quanh bởi dấu ngoặc kép. Ví dụ, trong đoạn code System.err.println("DPAP load Warning: unknown parameter " + entries[0] + ", value = " + entries[1]);, chúng ta có hai string literal: (a)"DPAP load Warning: unknown parameter "; (b)", value = ".

6. Method Names (MN): Đối với ví dụ được hiển thị trong Hình 1 của bài báo chính,

--- TRANG 15 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

public void InitializeToAssignment(int[] a) là loại ngữ cảnh prompt method name.

7. Method Names and Bodies (MNB): Đối với ví dụ được hiển thị trong Hình 1 của bài báo chính, phần được tô màu tím đại diện cho method names và bodies.

B.3. Chiến lược Cắt bớt cho Ngữ cảnh Đề xuất Prompt

Nếu ngữ cảnh đề xuất prompt lớn hơn độ dài ngữ cảnh được phân bổ cho nó, thì chúng tôi cần cắt bớt ngữ cảnh đề xuất prompt. Chúng tôi theo hai phương án dưới đây để cắt bớt ngữ cảnh:

• front: Chúng tôi cắt bớt ngữ cảnh từ phía trước. Điều này được sử dụng cho tất cả nguồn prompt ngoại trừ Parent Class và khi chúng tôi lấy PL từ Current.

• back: Chúng tôi cắt bớt ngữ cảnh từ phía sau. Điều này được sử dụng khi nguồn prompt là Parent Class và khi chúng tôi lấy các loại ngữ cảnh prompt khác PL từ Current.

Các chiến lược cắt bớt cho mỗi trường hợp được chọn dựa trên kết quả trên một validation set nhỏ. Đối với nguồn prompt Current, ngoại trừ khi loại ngữ cảnh prompt là PL, chúng tôi luôn bắt đầu bằng cách lấy code của loại ngữ cảnh prompt từ sau vị trí hole. Điều này có ý nghĩa vì ngữ cảnh Codex mặc định sẽ luôn chứa code trước hole. Chỉ khi điều này trống, chúng tôi mới sử dụng code của loại ngữ cảnh từ trước hole.

B.4. Danh sách Đề xuất Prompt

Bảng 7. Danh sách các đề xuất prompt cấp repo được đề xuất của chúng tôi

ID Đề xuất Prompt Nguồn Prompt Loại Ngữ cảnh Prompt
0,1,2,3,4 Current MN, I, TI, SL, FD
5,6,7 Current PL (lấy 25%, 50% và 75% đóng góp vào tổng độ dài ngữ cảnh)
8,9,10,11,12,13 Parent Class MNB, MN, I, TI, SL, FD
14,15,16,17,18,19 Import MNB, MN, I, TI, SL, FD
20,21,22,23,24,25 Sibling MNB, MN, I, TI, SL, FD
26,27,28,29,30,31 Similar Name MNB, MN, I, TI, SL, FD
32,33,34,35,36,37 Child Class MNB, MN, I, TI, SL, FD
38,39,40,41,42,43 Import of Sibling MNB, MN, I, TI, SL, FD
44,45,46,47,48,49 Import of Similar Name MNB, MN, I, TI, SL, FD
50,51,52,53,54,55 Import of Parent Class MNB, MN, I, TI, SL, FD
56,57,58,59,60,61 Import of Child Class MNB, MN, I, TI, SL, FD
62 Codex -

B.5. Các Biến thể Đề xuất Prompt Khác

Chúng tôi thí nghiệm với các biến thể khác bao gồm: (a) thêm tên lớp vào đầu ngữ cảnh đề xuất prompt, (b) sử dụng newline hoặc space để nối ngữ cảnh đề xuất prompt và ngữ cảnh Codex mặc định, (c) lấy tất cả hoặc top-k của các loại ngữ cảnh prompt, (d) thứ tự của top-k.

• Context Separator: Điều này xác định cách chúng tôi nối chuỗi ngữ cảnh đề xuất prompt với chuỗi ngữ cảnh Codex mặc định. Chúng tôi thí nghiệm với space và newline làm context separator.

• Prompt Proposal Context Formatting: Chúng tôi có thể định dạng ngữ cảnh đề xuất prompt trước khi đưa nó cho Prompt Composer. Chúng tôi thí nghiệm với các tùy chọn sau:

1. class_name: thêm [tên lớp của tệp] vào đầu ngữ cảnh đề xuất prompt được lấy từ mỗi tệp là một phần của nguồn prompt. Ví dụ, nếu chúng tôi lấy ngữ cảnh đề xuất prompt từ hai tệp import f1 và f2, ngữ cảnh đề xuất prompt sẽ được định dạng như: [tên lớp của f1] ngữ cảnh đề xuất prompt từ f1+ space + [tên lớp của f2] ngữ cảnh đề xuất prompt từ f2. Chúng tôi sử dụng điều này khi các loại ngữ cảnh đề xuất prompt là MN, I, TI, FD và SL.

--- TRANG 16 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

2. class_method_name: chúng tôi áp dụng điều này chỉ khi loại ngữ cảnh đề xuất prompt là MNB. Chúng tôi thêm tên phương thức vào đầu mỗi nội dung phương thức tương ứng. Chúng tôi cũng thêm ngữ cảnh đề xuất prompt từ một tệp với tên của lớp như được mô tả trong mục trước.

3. comment: Thêm ngữ cảnh đề xuất prompt dưới dạng comment, tức là định dạng nó như: /** ngữ cảnh đề xuất prompt */. Điều này không được thấy là hữu ích lắm.

4. none: truyền ngữ cảnh đề xuất prompt như chính nó. Chúng tôi sử dụng điều này khi loại ngữ cảnh đề xuất prompt là PL.

• Top-k Type: Đối với mỗi loại ngữ cảnh đề xuất prompt, ngoại trừ PL, chúng tôi thí nghiệm với việc lấy (a) first (b) last và (c) all của các loại ngữ cảnh đề xuất prompt, tức là chúng ta có thể lấy first-10 identifier. Chúng tôi thấy 'all' là tốt nhất trong tất cả.

• Top-k: Chúng tôi thí nghiệm với các giá trị k là (a) 10 (b) 20 và (c) all. Chúng tôi thấy 'all' hoạt động tốt nhất cho tất cả loại ngữ cảnh prompt.

C. Chi tiết Triển khai

C.1. RLPG-H

Chúng tôi sử dụng optimizer Adam (Kingma & Ba, 2015) với learning rate 3e-4 và batch size 64. Chúng tôi sử dụng CodeBERT (Feng et al., 2020) làm mô hình pretrained Fφ để thu được biểu diễn của hole window. Kích thước của biểu diễn (tương ứng với chiều ẩn của token [CLS]) là 768. W1∈R512×768, b1= 512, W2∈R63×512, b2= 63.

C.2. RLPG-R

Chúng tôi sử dụng optimizer Adam (Kingma & Ba, 2015) với learning rate 3e-4 và batch size 64. Chúng tôi sử dụng CodeBERT (Feng et al., 2020) làm mô hình pretrained Fφ để thu được biểu diễn của hole window và ngữ cảnh đề xuất prompt. Kích thước của biểu diễn (tương ứng với chiều ẩn của token [CLS]) là 768. Multiheaded attention (Vaswani et al., 2017) được mô hình hóa như sau:

Qh=Fφ(Hh), Kh_p=Fφ(Ch_p), Vh_p=Fφ(Ch_p)

Att(Qh, Kh_p, Vh_p) =Vh_p*softmax(Qh⊤Kh_p/√dk)

MultiHead (Qh, Kh_p, Vh_p) =WO*concat (head 1, . . . head τ)

trong đó head i=Att(WQ_i*Qh, WK_i*Kh_p, WV_i*Vh_p)

Trong các phương trình trên, dk là chiều của key, WQ_i, WK_i, WV_i là các ma trận chiếu query, key và value, τ là số head và WO là chiếu tuyến tính kết hợp các head. Các ma trận chiếu WQ_i∈Rdq×dmodel, WK_i∈Rdk×dmodel, WV_i∈Rdv×dmodel, WO∈Rdmodel×τdv. Đối với multihead attention, chúng tôi sử dụng dk=dq=dv= 32, τ= 4 và dmodel = 768, Wr∈R63×768 và bp= 63. Đối với mỗi head, chúng tôi thực hiện scaled dot-product attention. Module G bao gồm dropout layer (Srivastava et al., 2014), residual connection (He et al., 2016), layernorm (Ba et al., 2016), tiếp theo là chuỗi (a) dense layer với trọng số= 2048×768, bias= 768, (b) relu activation, (c) dense layer với trọng số= 768×2048, bias= 2048, (d) dropout layer, (e) residual connection, (f) layernorm. Giá trị dropout 0.25 được sử dụng trong khi huấn luyện. Mô hình của chúng tôi giống một lớp của transformer encoder block (Vaswani et al., 2017).

C.3. Baseline

Baseline Random đầu tiên chọn một tệp ngẫu nhiên từ repository hiện tại tiếp theo là chọn một dòng ngẫu nhiên trong tệp đó. Chúng tôi chọn tất cả các dòng bắt đầu từ dòng đó đến dòng cuối của tệp được chọn làm ngữ cảnh (loại trừ hole window nếu tệp được chọn là tệp hiện tại). Sự tương tự nearest neighbour dựa trên dot product giữa biểu diễn của hole window và biểu diễn của ngữ cảnh, trong đó chúng tôi sử dụng mô hình CodeBERT pretrained (Feng et al., 2020) để thu được các biểu diễn. Đối với baseline Identifier Usage, nếu identifier gần nhất với hole không trả về bất kỳ usage window nào, chúng tôi tiến hành identifier gần nhất tiếp theo. Để tính toán nhanh hơn và tránh vấn đề bộ nhớ khi chạy trên phần cứng của chúng tôi, đối với các baseline NN, chúng tôi thu thập 64 neighbour ngẫu nhiên và sau đó xếp hạng dựa trên khoảng cách nearest neighbour. Các baseline dựa trên BM25 sử dụng triển khai Okapi BM25 với tham số mặc định được cung cấp bởi pip package rank-bm25 0.2.210. Đối với file-level BM25, nếu ngữ cảnh tệp vượt quá độ dài ngữ cảnh được phân bổ, chúng tôi cắt bớt từ phía sau.

10https://pypi.org/project/rank-bm25/

--- TRANG 17 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

D. Kết quả Bổ sung

D.1. Kết quả Hole-wise và Repo-wise

Bảng 3 cho thấy hiệu suất của tất cả phương pháp khi được tính trung bình trên tất cả hole (hole-wise) và trên các repository riêng lẻ (repo-wise). Lưu ý rằng metric sau độc lập với kích thước của repository.

Bảng 8. Success Rate (SR) Hole-wise và Repo-wise của các phương pháp khác nhau trên dữ liệu test.

Phương pháp Success Rate(%) (hole-wise) Rel.↑(%) (hole-wise) Success Rate(%) (repo-wise) Rel.↑(%) (repo-wise)
Codex (Chen et al., 2021) 58.73 - 60.64 -
Oracle 79.63 35.58 80.24 32.31
Random 58.13 -1.02 58.95 -2.79
Random NN 58.98 0.43 60.04 -0.99
File-level BM25 63.14 7.51 64.28 6.00
Identifier Usage (Random) 64.93 10.55 67.83 11.85
Identifier Usage (NN) 64.91 10.52 67.94 12.03
Fixed Prompt Proposal 65.78 12.00 68.01 12.15
RLPG-BM25 66.41 13.07 68.15 12.39
RLPG-H 68.51 16.65 69.26 14.21
RLPG-R 67.80 15.44 69.28 14.26

D.2. Ablation về Hiệu suất dựa trên Đề xuất Prompt

Hình 3 cho thấy success rate trung bình của các loại ngữ cảnh prompt khi success được tính chỉ cho các trường hợp khi những ngữ cảnh prompt này có thể áp dụng. Như có thể thấy từ hình, post lines là loại ngữ cảnh prompt hữu ích nhất trung bình. Đóng góp từ các loại ngữ cảnh prompt khác tuy nhỏ hơn post lines nhưng vẫn đáng kể, nhấn mạnh tầm quan trọng của mỗi loại ngữ cảnh prompt.

--- TRANG 18 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Hình 3. Success rate trung bình trên dữ liệu validation dựa trên loại ngữ cảnh prompt khi chúng có thể áp dụng.

Hình 4. (Trái) Success rate chuẩn hóa của các nguồn prompt khi có thể áp dụng, (Phải) Success rate chuẩn hóa của các loại ngữ cảnh prompt khi có thể áp dụng

Hình 4 cho thấy success rate chuẩn hóa trong đó việc chuẩn hóa được thực hiện trên các đề xuất prompt. Điều này giúp chúng ta hiểu hiệu suất tương đối của các nguồn đề xuất prompt và loại ngữ cảnh. Phần bên trái của hình phân tích hiệu suất dựa trên nguồn prompt và phần bên phải phân tích dựa trên loại ngữ cảnh prompt. Một điều cần lưu ý từ biểu đồ của các loại ngữ cảnh prompt là khi chúng ta xem xét hiệu suất tương đối, post lines không còn là loại ngữ cảnh chiếm ưu thế nhất. Điều này là do post lines chỉ gắn với khi nguồn prompt tương ứng với tệp hiện tại, do đó đóng góp vào số lượng thấp hơn khi so sánh với hầu hết các loại ngữ cảnh khác được gắn với tất cả nguồn prompt.

D.3. Hiệu suất trên Post Lines không ngay lập tức

Bảng 9 cho thấy hiệu suất của post lines khi bắt đầu từ dòng thứ tư sau dòng hole đích (tức là bỏ qua ba dòng sau hole đích) thay vì bắt đầu từ dòng ngay sau hole đích. Thí nghiệm này giúp chúng ta hiểu hiệu suất khi chúng ta quan tâm đến việc thực hiện nhiệm vụ khó hơn nhiều là tự động hoàn thành code nhiều dòng, trong đó mục tiêu là dự đoán không chỉ phần bị che trong dòng hiện tại mà còn ba dòng tiếp theo có thể tương ứng với việc hoàn thành một khối code như nội dung hàm. Từ bảng, chúng ta thấy rằng khi bắt đầu từ dòng thứ tư, một sự suy giảm hiệu suất nhẹ xảy ra. Điều này được mong đợi vì càng di chuyển xa khỏi hole đích, ngữ cảnh post lines càng ít liên quan. Tuy nhiên, sự sụt giảm hiệu suất không đáng kể, gợi ý rằng post lines vẫn là loại ngữ cảnh prompt rất hữu ích có thể được sử dụng dưới thiết lập tự động hoàn thành code nhiều dòng. Tương đương, chúng ta có thể bao gồm điều này như một trong các đề xuất prompt trong framework của chúng tôi cùng với phiên bản hiện tại của post lines.

Bảng 9. Success Rate (SR) khi lấy các phiên bản khác nhau của post lines.

Phương pháp Success Rate(%) (hole-wise) Rel.↑(%) (hole-wise) Success Rate(%) (repo-wise) Rel.↑(%) (repo-wise)
Codex (Chen et al., 2021) 58.73 - 60.64 -
Post Lines (dòng ngay sau hole) 65.78 12.00 68.01 12.15
Post Lines (bỏ qua ba dòng sau hole) 65.11 10.86 66.42 9.53

D.4. Thành phần của các đề xuất prompt

Bảng 10 cho thấy hiệu suất của hai phiên bản RLPG khi chúng tôi tạo ngữ cảnh đề xuất prompt từ l đề xuất prompt. Chúng tôi lấy top-l đề xuất prompt được cung cấp bởi RLPG dựa trên thứ tự giảm dần của xác suất. Để quyết định nên sử dụng bao nhiêu ngữ cảnh cho mỗi đề xuất prompt, chúng tôi chia tổng độ dài ngữ cảnh theo tỷ lệ với xác suất chuẩn hóa của top-l đề xuất prompt. Như có thể thấy từ bảng, mặc dù PPC không được huấn luyện một cách rõ ràng để thực hiện thành phần (cả vector ground-truth và biểu diễn của ngữ cảnh đề xuất prompt đều liên quan đến một đề xuất prompt duy nhất), tất cả các thành phần đều dẫn đến cải thiện đáng kể so với Codex. Tuy nhiên, như mong đợi, kết quả tốt nhất tương ứng với việc lấy ngữ cảnh từ một đề xuất prompt duy nhất (tức là thiết lập huấn luyện). Sự sụt giảm success rate với l= 2 và l= 5 không đáng kể, điều này gợi ý rằng việc huấn luyện RLPG một cách rõ ràng để học cách tạo ngữ cảnh từ các đề xuất prompt khác nhau có thể dẫn đến kết quả hứa hẹn và do đó cung cấp hướng nghiên cứu tương lai thú vị.

Bảng 10. Success Rate (SR) của các thành phần khác nhau của đề xuất prompt trên test set.

Phương pháp Success Rate(%) (hole-wise) Rel.↑(%) (hole-wise) Success Rate(%) (repo-wise) Rel.↑(%) (repo-wise)
Codex (Chen et al., 2021) 58.73 - 60.64 -
RLPG-H ( l= 1) 68.51 16.65 69.26 14.21
RLPG-R ( l= 1) 67.80 15.44 69.28 14.26
RLPG-H ( l= 2) 67.07 14.20 67.87 11.91
RLPG-R ( l= 2) 66.57 13.35 67.88 11.94
RLPG-H ( l= 5) 66.60 13.40 67.91 11.98
RLPG-R ( l= 5) 65.78 12.01 67.69 11.62
RLPG-H ( l= 10 ) 65.53 11.58 67.24 10.88
RLPG-R ( l= 10 ) 63.59 8.27 65.98 8.79

D.5. Tác động của Độ dài Ngữ cảnh

Để hiểu tác động của độ dài ngữ cảnh đối với hiệu suất của các đề xuất prompt của chúng tôi, chúng tôi lấy một nửa độ dài ngữ cảnh có sẵn cho prompt trong Codex và quan sát hiệu suất của oracle và fixed prompt proposal. Như trước đây, chúng tôi thấy rằng oracle được xây dựng từ các đề xuất prompt của chúng tôi cho thấy cải thiện đáng chú ý so với Codex, nhấn mạnh giá trị của các đề xuất prompt của chúng tôi. Tuy nhiên, khi so sánh với độ dài ngữ cảnh lớn hơn, lợi ích tương đối nhỏ hơn. Điều này được mong đợi vì độ dài ngữ cảnh nhỏ hơn có nghĩa là ngữ cảnh liên quan đến từ đề xuất prompt cần được cắt bớt để phù hợp bên trong prompt, do đó dẫn đến mất thông tin.

--- TRANG 20 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Bảng 11. Success Rate (SR) của Codex và oracle trên test set khi tổng độ dài ngữ cảnh = 2048.

Phương pháp Success Rate(%) (hole-wise) Rel.↑(%) (hole-wise) Success Rate(%) (repo-wise) Rel.↑(%) (repo-wise)
Codex (Chen et al., 2021) 57.77 - 58.90 -
Oracle 61.90 7.15 67.18 14.07

D.6. Tác động của Cắt bớt

Chúng tôi tính phần trăm lần ngữ cảnh được bao gồm trong prompt bị cắt bớt. Trong Bảng 12, cột thứ hai, thứ ba, thứ tư và thứ năm đại diện cho phần trăm cắt bớt với ngữ cảnh codex mặc định, ngữ cảnh đề xuất prompt, với một trong hai và với cả hai, tương ứng. Các số liệu cắt bớt gợi ý rằng mô hình code completion cho phép độ dài ngữ cảnh dài hơn có thể hữu ích. Chúng tôi không khuyến khích một cách rõ ràng tính đúng đắn cú pháp của đoạn code được bao gồm. Vì độ dài ngữ cảnh bị hạn chế, hoàn toàn có thể ngữ cảnh được bao gồm trong prompt không đúng cú pháp tổng thể. Tuy nhiên, khi vẽ biểu đồ các số liệu repo-wise, chúng tôi không quan sát bất kỳ mối tương quan cụ thể nào giữa lượng cắt bớt và hiệu suất. Điều này làm cho chúng tôi tin rằng nội dung của ngữ cảnh được bao gồm (liệu nó có đến từ đề xuất prompt được chọn hay không) là điều quan trọng hơn thay vì liệu nó có đúng cú pháp hay không.

Bảng 12. Phần trăm cắt bớt khi sử dụng các ngữ cảnh khác nhau trong prompt.

Phương pháp Ngữ cảnh Codex Mặc định Ngữ cảnh Đề xuất Prompt Một trong hai Cả hai Success Rate
RLPG-H 26.95 30.95 39.22 18.68 68.51
RLPG-R 26.82 31.31 38.88 19.24 67.80

D.7. Hiệu suất trên các repository riêng lẻ

Bảng 13. Success Rate của các phương pháp khác nhau trên dữ liệu training

Tên Repo #Tổng Hole Oracle Codex Fixed prompt proposal RLPG-H RLPG-R
largemail 1653 75.38 55.11 62.73 63.94 63.28
ftpserverremoteadmin 7323 86.44 66.11 76.09 76.21 76.76
myt5lib 838 91.65 53.58 61.34 73.51 74.46
seamlets 4890 92.74 62.25 62.72 71.55 74.27
gloodb 10000 91.07 57.50 57.50 70.32 72.31
jjskit 9043 80.36 65.61 72.18 72.00 72.44
mobileexpensetracker 2298 75.94 57.88 67.28 66.84 66.97
gfsfa 10000 80.55 57.33 57.33 59.28 65.24
swe574-group3 2029 76.79 54.46 66.19 65.16 64.91
strudem-sicsa 6131 77.83 64.96 72.55 73.25 73.32
soap-dtc 1370 81.24 64.82 70.73 71.61 72.70
openprocesslogger 7191 81.06 62.19 71.77 72.22 72.62
tapestry-sesame 397 72.54 45.84 61.21 60.71 63.98
exogdx 735 84.76 63.81 75.51 75.92 76.60
designpatternjavapedro 1069 78.30 54.82 64.36 63.99 68.57
quidsee 3020 81.66 60.79 69.50 70.36 70.26
healpix-rangeset 4734 63.54 48.71 54.67 54.94 55.07
sol-agent-platform 10000 73.76 58.22 65.72 65.65 65.94
rsbotownversion 10000 75.23 57.89 65.58 66.22 66.31

--- TRANG 21 ---
Tạo Prompt Cấp Repository cho Mô hình Ngôn ngữ Lớn của Code

Bảng 14. Success Rate của các phương pháp khác nhau trên dữ liệu validation

Tên Repo #Tổng Hole Oracle Codex Fixed prompt proposal RLPG-H RLPG-R
tyrond 721 83.91 60.33 71.15 71.57 72.68
math-mech-eshop 2225 83.46 62.20 72.76 73.53 73.17
infinispan-storage-service 373 82.31 71.85 78.55 76.94 77.75
teammates-shakthi 7665 82.02 63.74 72.38 72.47 72.46
javasummerframework 10000 79.27 55.92 65.30 65.74 65.55
tinwiki 10000 73.67 69.27 69.27 69.12 69.58
jloogle 3145 84.55 73.16 77.87 77.17 77.36
jcontenedor 5464 81.26 58.99 67.77 67.95 68.32
sohocms 772 76.68 57.90 67.10 67.49 67.62
affinity_propagation_java 1466 79.54 59.14 70.33 70.26 70.26
jata4test 1921 71.06 44.09 54.92 55.91 57.47
swinagile 2595 79.69 63.01 72.29 72.49 72.68
navigablep2p 1322 75.72 59.76 65.43 65.13 65.28
springlime 879 83.50 62.34 74.18 74.86 74.40

Bảng 15. Success Rate của các phương pháp khác nhau trên dữ liệu test

Tên Repo #Tổng Hole Oracle Codex Fixed PP RLPG-H RLPG-R Random Random NN Iden Usage (Random) Iden Usage (NN) File-Level BM25 RLPG-BM25
dovetaildb 10000 76.89 57.12 66.45 66.06 66.25 57.45 57.58 61.39 60.77 59.39 66.09
project-pt-diaoc 10000 82.01 52.67 52.81 65.08 61.25 51.58 52.93 55.54 56.21 57.04 58.29
realtimegc 2513 77.64 57.58 67.01 67.85 68.48 57.78 58.89 63.51 63.99 61.84 66.69
fswuniceubtemplates 2070 77.44 55.7 58.89 66.81 65.8 55.22 55.89 65.7 66.43 59.28 66.71
qwikioffice-java 1138 76.45 70.21 70.21 69.86 70.56 46.13 48.15 60.37 62.92 64.41 58.17
glperaudsimon 1766 78.65 53.57 62.51 62.4 61.66 55.66 57.76 69.42 68.4 69.14 61.55
xiaonei-java-ap 839 73.42 57.57 62.1 62.69 63.29 57.09 57.21 71.28 72.35 63.77 63.29
ircrpgbot 6591 83.67 69.67 77.24 76.71 76.65 69.55 70.54 74.68 74.43 69.32 75.75
robotsimulator2009w 7514 75.63 56.28 67.55 67.53 67.55 56.4 56.18 64.61 64.71 62.96 66.12
gwt-plugindetect 73 84.93 60.27 68.49 65.75 68.49 58.9 57.53 63.01 63.01 50.68 75.34
apiitfriends 1385 85.05 65.05 74.8 75.67 75.31 65.7 68.59 70.25 70.11 66.93 73.57
wicketbits 754 83.02 59.81 72.94 72.81 73.08 60.21 61.94 81.96 79.31 84.48 73.47
hucourses 590 84.41 70.68 77.46 77.63 77.97 70 72.2 70.68 72.54 53.39 75.08
xfuze 3055 84.09 62.82 73.62 72.73 73.62 63.67 65.17 77.25 75.97 77.32 74.01

Bảng 13, Bảng 14 và Bảng 15 trình bày success rate của các phương pháp khác nhau trên các repository riêng lẻ trong các split training, validation và test, tương ứng. Trung bình repo-wise trong Bảng 2 trong bài báo chính được tính bằng cách lấy trung bình của các số tương ứng với mỗi cột. Trung bình hole-wise tương ứng với việc nhân các số repo-wise của mỗi phương pháp với tổng số hole trong repo để có tổng số hole thành công bởi phương pháp đó cho repo đó. Sau đó chúng tôi cộng tổng số hole thành công trên các repo và chia cho tổng số hole trong toàn bộ data split để có trung bình hole-wise.

E. Phân tích Các Trường hợp Mẫu

Trong Hình 1, RLPG chọn đề xuất prompt tương ứng với việc lấy method names và bodies từ tệp được import (tức là MaximizingGibbsSampler.java). Lưu ý rằng mg. trước vị trí hole cho thấy rằng một phương thức được sử dụng trong tệp được import có khả năng được gọi. Trong trường hợp này, ngữ cảnh đề xuất prompt (được tô màu tím) chứa tên phương thức InitializeToAssignment (một phần của hole đích). Điều này kết hợp với ngữ cảnh Codex mặc định chứa phương thức CurrentAssignments() (một phần của hole đích) dẫn đến việc tạo ra prompt thành công. Mặt khác, prompt được tạo từ ngữ cảnh Codex mặc định thất bại trong việc dự đoán hole đích trong trường hợp này. Nói chung, chúng tôi quan sát thấy rằng trong sự vắng mặt của tín hiệu mạnh, Codex có xu hướng ưu tiên comment ngôn ngữ tự nhiên xuất hiện trước vị trí hole, ví dụ đặt tên phương thức dựa trên comment. Điều này trong một số trường hợp có thể có hại. Chúng tôi cung cấp các trường hợp mẫu tích cực và tiêu cực cho RLPG dưới đây:

E.1. Trường hợp Tích cực

Chúng tôi cung cấp một số ví dụ về các trường hợp mà RLPG dẫn đến dự đoán đúng và Codex thất bại.

1. Các trường hợp mà một phần của hole đích được tìm thấy chính xác trong ngữ cảnh đề xuất prompt.
• RLPG = Propagation(int numVars) vs Codex = Propagation()
• RLPG = tersFromFile(String filename) { vs Codex = ters(String filename) {
• RLPG = als("dampingFactor")) { vs Codex = als("numVars")) {
• RLPG = ] + ", value = " + entries[1]); vs Codex = ]);
• RLPG = stem.exit(1); vs Codex = stem.err.println("DPAP load error: " + ex.get

2. Các trường hợp mà Codex lấy gợi ý mạnh từ comment ngôn ngữ tự nhiên trước đó, do đó tạo ra dự đoán sai.
• RLPG = d PassMessages() vs Codex = d DoOneRoundOfMessagePassing()
• RLPG = teger> CurrentExemplars() { vs Codex = teger> ChooseExemplars() {
• RLPG = ring FileName() { vs Codex = ring GetAlgorithmFilename() {

E.2. Trường hợp Tiêu cực

Trong một số trường hợp, thông tin bổ sung từ ngữ cảnh đề xuất prompt có thể dẫn đến nhầm lẫn và tạo ra dự đoán sai.
• RLPG = an hasConverged_; vs Codex = an converged_;
• RLPG = _[i][j] = -Double.MAX_VALUE; vs Codex = _[i][j] = 0;
