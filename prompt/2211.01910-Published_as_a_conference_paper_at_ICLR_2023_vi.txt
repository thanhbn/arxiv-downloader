# 2211.01910.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2211.01910.pdf
# Kích thước tệp: 4085477 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
CÁC MÔ HÌNH NGÔN NGỮ LỚN LÀ NHỮNG KỸ SƯ PROMPT Ở MỨC CON NGƯỜI
Yongchao Zhou1;2;, Andrei Ioan Muresanu2;3;, Ziwen Han1;2;, Keiran Paster1;2,
Silviu Pitis1;2, Harris Chan1;2, Jimmy Ba1;2
1University of Toronto2Vector Institute3University of WaterlooĐóng góp ngang nhau
{yczhou,hanziwen,keirp,spitis,hchan,jba}@cs.toronto.edu
{andrei.muresanu}@uwaterloo.ca
TÓM TẮT
Bằng cách điều kiện hóa trên các hướng dẫn bằng ngôn ngữ tự nhiên, các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng ấn tượng như những máy tính đa năng. Tuy nhiên, hiệu suất tác vụ phụ thuộc đáng kể vào chất lượng của prompt được sử dụng để điều khiển mô hình, và hầu hết các prompt hiệu quả đều được con người tạo ra thủ công. Lấy cảm hứng từ tổng hợp chương trình cổ điển và phương pháp tiếp cận của con người đối với kỹ thuật prompt, chúng tôi đề xuất Automatic Prompt Engineer1 (APE) để tự động sinh và lựa chọn hướng dẫn. Trong phương pháp của chúng tôi, chúng tôi coi hướng dẫn như "chương trình", được tối ưu hóa bằng cách tìm kiếm trên một nhóm các ứng cử viên hướng dẫn được đề xuất bởi một LLM để tối đa hóa một hàm điểm số đã chọn. Để đánh giá chất lượng của hướng dẫn được chọn, chúng tôi đánh giá hiệu suất zero-shot của một LLM khác khi tuân theo hướng dẫn được chọn. Các thí nghiệm rộng rãi cho thấy rằng các hướng dẫn được tạo tự động của chúng tôi vượt trội hơn so với baseline LLM trước đó với một biên độ lớn và đạt được hiệu suất tốt hơn hoặc tương đương với các hướng dẫn được tạo bởi các chú thích viên con người trên 24/24 tác vụ Instruction Induction và 17/21 tác vụ BIG-Bench được tuyển chọn. Chúng tôi tiến hành các phân tích định tính và định lượng rộng rãi để khám phá hiệu suất của APE. Chúng tôi cho thấy rằng các prompt được thiết kế bởi APE có thể cải thiện hiệu suất học few-shot (bằng cách đơn giản là thêm chúng vào đầu các prompt học trong ngữ cảnh tiêu chuẩn), tìm ra các prompt chain-of-thought zero-shot tốt hơn, cũng như điều hướng các mô hình về phía tính trung thực và/hoặc tính thông tin.2

1 GIỚI THIỆU
Sự kết hợp của quy mô và kiến trúc dựa trên attention đã dẫn đến các mô hình ngôn ngữ sở hữu mức độ tổng quát chưa từng có (Kaplan et al., 2020; Vaswani et al., 2017). Những "mô hình ngôn ngữ lớn" (LLM) này đã cho thấy khả năng đáng chú ý, thường vượt trội hơn con người, trên một loạt các tác vụ đa dạng, bao gồm cả thiết lập zero-shot và few-shot (Brown et al., 2020; Srivastava et al., 2022). Tuy nhiên, với tính tổng quát, xuất hiện một câu hỏi về kiểm soát: làm thế nào chúng ta có thể khiến LLM làm những gì chúng ta muốn chúng làm?

Để trả lời câu hỏi này và điều hướng LLM về phía các hành vi mong muốn, các nghiên cứu gần đây đã xem xét fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), học trong ngữ cảnh (Brown et al., 2020), và một số hình thức sinh prompt (Gao, 2021), bao gồm cả việc điều chỉnh khả vi của soft prompt (Qin & Eisner, 2021; Lester et al., 2021) và kỹ thuật prompt ngôn ngữ tự nhiên (Reynolds & McDonell, 2021). Cái sau đặc biệt thú vị, vì nó cung cấp một giao diện tự nhiên để con người giao tiếp với máy móc và có thể có liên quan lớn không chỉ đối với LLM mà còn đối với các mô hình tổng quát khác như bộ tổng hợp hình ảnh được prompt (Rombach et al., 2022; Ramesh et al., 2022), mà sự quan tâm của công chúng về thiết kế và sinh prompt cũng đã xuất hiện (xem Phụ lục A để biết ví dụ).

Đằng sau sự quan tâm này là thực tế rằng các prompt ngôn ngữ thuần túy không phải lúc nào cũng tạo ra kết quả mong muốn, ngay cả khi những kết quả đó có thể được tạo ra bằng các hướng dẫn thay thế. Do đó, người dùng phải thử nghiệm với một loạt các prompt để gợi ra các hành vi mong muốn, vì họ có ít kiến thức về mức độ tương thích của các hướng dẫn với một mô hình cụ thể. Chúng ta có thể hiểu điều này bằng cách xem LLM như những máy tính hộp đen thực thi các chương trình được chỉ định bởi các hướng dẫn ngôn ngữ tự nhiên: trong khi chúng có thể thực thi một loạt rộng các chương trình ngôn ngữ tự nhiên, cách thức xử lý các chương trình này có thể không trực quan đối với con người, và chất lượng của hướng dẫn chỉ có thể được đo lường khi thực thi các hướng dẫn này trên một tác vụ downstream (Sanh et al., 2022; Wei et al., 2021).

Để giảm nỗ lực của con người trong việc tạo ra và xác thực các hướng dẫn hiệu quả, chúng tôi đề xuất một thuật toán mới sử dụng LLM để tự động sinh và lựa chọn hướng dẫn. Chúng tôi gọi vấn đề này là tổng hợp chương trình ngôn ngữ tự nhiên và đề xuất giải quyết nó như một vấn đề tối ưu hóa hộp đen sử dụng LLM để sinh và tìm kiếm các giải pháp ứng cử viên có thể thực hiện được theo kinh nghiệm. Khi làm như vậy, chúng tôi tận dụng khả năng tổng quát của LLM theo ba cách. Đầu tiên, chúng tôi sử dụng một LLM như một mô hình suy luận (Ellis et al., 2021; Honovich et al., 2022) để sinh các ứng cử viên hướng dẫn dựa trên một tập nhỏ các minh chứng dưới dạng các cặp đầu vào-đầu ra. Tiếp theo, chúng tôi hướng dẫn quá trình tìm kiếm bằng cách tính toán một điểm số cho mỗi hướng dẫn dưới LLM mà chúng tôi tìm cách kiểm soát. Cuối cùng, chúng tôi đề xuất một phương pháp tìm kiếm Monte Carlo lặp đi lặp lại nơi LLM cải thiện các ứng cử viên tốt nhất bằng cách đề xuất các biến thể hướng dẫn tương tự về mặt ngữ nghĩa. Một cách trực quan, thuật toán của chúng tôi yêu cầu LLM tạo ra một tập hợp các ứng cử viên hướng dẫn dựa trên các minh chứng và sau đó yêu cầu chúng đánh giá hướng dẫn nào có triển vọng hơn. Chúng tôi gọi thuật toán của mình là Automatic Prompt Engineer (APE). Các đóng góp chính của chúng tôi là:

• Chúng tôi khung hoá việc sinh hướng dẫn như tổng hợp chương trình ngôn ngữ tự nhiên, công thức hóa nó như một vấn đề tối ưu hóa hộp đen được hướng dẫn bởi LLM, và đề xuất cả phương pháp tìm kiếm Monte Carlo ngây thơ và lặp lại để xấp xỉ giải pháp.

• Phương pháp đề xuất của chúng tôi, APE, đạt được hiệu suất cấp độ con người trong học zero-shot với các hướng dẫn được tạo bởi mô hình trên 24/24 Instruction Induction và 17/21 tác vụ Big-Bench.

• Chúng tôi cung cấp các phân tích định tính và định lượng rộng rãi khám phá các khía cạnh khác nhau của APE, và chứng minh các ứng dụng của APE để cải thiện học few-shot, tìm ra các prompt chain of thought zero-shot tốt hơn, và điều hướng LLM về phía các hành vi mong muốn như tính trung thực và/hoặc tính thông tin.

2 NGHIÊN CỨU LIÊN QUAN

Các Mô hình Ngôn ngữ Lớn Việc mở rộng quy mô các mô hình ngôn ngữ dựa trên transformer về kích thước mô hình, dữ liệu huấn luyện và tính toán huấn luyện đã được chứng minh là cải thiện hiệu suất một cách có thể dự đoán trên một loạt rộng các tác vụ NLP downstream (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). Nhiều khả năng nổi lên (Wei et al., 2022a) của LLM đã được khám phá như là kết quả của việc mở rộng quy mô này, bao gồm học few-shot trong ngữ cảnh, giải quyết vấn đề zero-shot, lý luận chain of thought, tuân theo hướng dẫn, và cảm ứng hướng dẫn (Cobbe et al., 2021; Wei et al., 2022b; Kojima et al.,

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Giáo sư Smith được đưa ra những hướng dẫn sau: <INSERT> 
Đây là những phản hồi của Giáo sư: 
# Bắt đầu Minh chứng  
Đầu vào : chứng minh    Đầu ra : bác bỏ  
Đầu vào : bật    Đầu ra : tắt 
... 
# Kết thúc Minh chứng   LLMs như Mô hình Suy luận
Hướng dẫn : viết từ trái nghĩa của từ.  
Đầu vào : trực tiếp  Đầu ra : gián tiếp

viết từ ngược lại của từ đã cho. đưa ra từ trái nghĩa của từ được cung cấp.
...
đảo ngược đầu vào.
để đảo ngược thứ tự của các chữ cái
-0.16-0.28
...
-0.86
-1.08Tạo ra một biến thể của hướng dẫn sau đây trong khi giữ nguyên ý nghĩa ngữ nghĩa.
Đầu vào : viết từ trái nghĩa của từ .
Đầu ra : <COMPLETE>LLMs như Mô hình Lấy mẫu lạiLLMs như Mô hình Chấm điểm

viết từ trái nghĩa của từ. -0.26
... ...
liệt kê các từ trái nghĩa cho từ đã cho. -0.39Xác suất 
Log  
Ứng cử viên
Điểm cao<LIKELIHOOD>
[Tùy chọn]Đề xuấtChấm điểm
Ứng cử viên
TươngTựGiữ các ứng cử viên có điểm cao Loại bỏ các ứng cử viên có điểm thấp Prompt cuối cùng được chọn với điểm cao nhất
① ② ③
④
⑤
(a) Quy trình Automatic Prompt Engineer (APE)

Tham lam (GPT-3) Tham lam (InstructGPT) APE (GPT-3) APE (InstructGPT)00.20.40.60.750.8
0.01
350M0.02
350M0.61
350M0.59
350M0.01
1.3B0.01
1.3B0.63
1.3B0.73
1.3B0.03
6.7B0.03
6.7B0.65
6.7B0.57
6.7B0.03
175B0.40
175B0.71
175B0.81
175BHiệu suất Zero-Shot Trung vị Khoảng tứ phânKỹ sư Prompt Con người (b) Trung vị khoảng tứ phân trên 24 tác vụ

Hình 1: (a) Phương pháp của chúng tôi, Automatic Prompt Engineer (APE), tự động sinh hướng dẫn cho một tác vụ được chỉ định thông qua các minh chứng đầu ra: nó sinh ra một số ứng cử viên hướng dẫn, hoặc thông qua suy luận trực tiếp hoặc một quá trình đệ quy dựa trên sự tương tự ngữ nghĩa, thực thi chúng bằng mô hình mục tiêu, và chọn hướng dẫn phù hợp nhất dựa trên điểm số đánh giá được tính toán. (b) Như được đo bởi trung vị khoảng tứ phân trên 24 tác vụ NLP được giới thiệu bởi Honovich et al. (2022), APE có thể vượt qua hiệu suất con người khi sử dụng mô hình InstructGPT (Ouyang et al., 2022).

có thể thực thi một loạt rộng các chương trình ngôn ngữ tự nhiên, cách thức xử lý các chương trình này có thể không trực quan đối với con người, và chất lượng của hướng dẫn chỉ có thể được đo lường khi thực thi những hướng dẫn này trên một tác vụ downstream (Sanh et al., 2022; Wei et al., 2021).

Để giảm nỗ lực của con người trong việc tạo ra và xác thực các hướng dẫn hiệu quả, chúng tôi đề xuất một thuật toán mới sử dụng LLM để tự động sinh và lựa chọn hướng dẫn. Chúng tôi gọi vấn đề này là tổng hợp chương trình ngôn ngữ tự nhiên và đề xuất giải quyết nó như một vấn đề tối ưu hóa hộp đen sử dụng LLM để sinh và tìm kiếm các giải pháp ứng cử viên có khả năng thực hiện được theo kinh nghiệm. Khi làm như vậy, chúng tôi tận dụng khả năng tổng quát của LLM theo ba cách. Đầu tiên, chúng tôi sử dụng một LLM như một mô hình suy luận (Ellis et al., 2021; Honovich et al., 2022) để sinh các ứng cử viên hướng dẫn dựa trên một tập nhỏ các minh chứng dưới dạng các cặp đầu vào-đầu ra. Tiếp theo, chúng tôi hướng dẫn quá trình tìm kiếm bằng cách tính toán một điểm số cho mỗi hướng dẫn dưới LLM mà chúng tôi tìm cách kiểm soát. Cuối cùng, chúng tôi đề xuất một phương pháp tìm kiếm Monte Carlo lặp đi lặp lại nơi LLM cải thiện các ứng cử viên tốt nhất bằng cách đề xuất các biến thể hướng dẫn tương tự về mặt ngữ nghĩa. Một cách trực quan, thuật toán của chúng tôi yêu cầu LLM tạo ra một tập hợp các ứng cử viên hướng dẫn dựa trên các minh chứng và sau đó yêu cầu chúng đánh giá hướng dẫn nào có triển vọng hơn. Chúng tôi gọi thuật toán của mình là Automatic Prompt Engineer (APE). Các đóng góp chính của chúng tôi là:

•Chúng tôi khung hoá việc sinh hướng dẫn như tổng hợp chương trình ngôn ngữ tự nhiên, công thức hóa nó như một vấn đề tối ưu hóa hộp đen được hướng dẫn bởi LLM, và đề xuất cả phương pháp tìm kiếm Monte Carlo ngây thơ và lặp lại để xấp xỉ giải pháp.

•Phương pháp đề xuất của chúng tôi, APE, đạt được hiệu suất cấp độ con người trong học zero-shot với các hướng dẫn được tạo bởi mô hình trên 24/24 Instruction Induction và 17/21 tác vụ Big-Bench.

•Chúng tôi cung cấp các phân tích định tính và định lượng rộng rãi khám phá các khía cạnh khác nhau của APE, và chứng minh các ứng dụng của APE để cải thiện học few-shot, tìm ra các prompt chain of thought zero-shot tốt hơn, và điều hướng LLM về phía các hành vi mong muốn như tính trung thực và/hoặc tính thông tin.

2 NGHIÊN CỨU LIÊN QUAN

Các Mô hình Ngôn ngữ Lớn Việc mở rộng quy mô các mô hình ngôn ngữ dựa trên transformer về kích thước mô hình, dữ liệu huấn luyện và tính toán huấn luyện đã được chứng minh là cải thiện hiệu suất một cách có thể dự đoán trên một loạt rộng các tác vụ NLP downstream (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). Nhiều khả năng nổi lên (Wei et al., 2022a) của LLM đã được khám phá như là kết quả của việc mở rộng quy mô này, bao gồm học few-shot trong ngữ cảnh, giải quyết vấn đề zero-shot, lý luận chain of thought, tuân theo hướng dẫn, và cảm ứng hướng dẫn (Cobbe et al., 2021; Wei et al., 2022b; Kojima et al.,

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

2022; Sanh et al., 2022; Wei et al., 2021; Ouyang et al., 2022; Honovich et al., 2022). Trong bài báo này, chúng tôi xem LLM như những máy tính hộp đen thực thi các chương trình được chỉ định bởi các hướng dẫn ngôn ngữ tự nhiên và điều tra cách kiểm soát hành vi của LLM bằng các hướng dẫn được tạo bởi mô hình.

Kỹ thuật Prompt Prompting cung cấp một giao diện tự nhiên và trực quan để con người tương tác và sử dụng các mô hình tổng quát như LLM. Do tính linh hoạt của nó, prompting đã được sử dụng rộng rãi như một phương pháp chung cho các tác vụ NLP (Schick & Schütze, 2021; Brown et al., 2020; Sanh et al., 2022). Tuy nhiên, LLM yêu cầu kỹ thuật prompt cẩn thận, hoặc thủ công (Reynolds & McDonell, 2021) hoặc tự động (Gao et al., 2021; Shin et al., 2020), vì các mô hình dường như không hiểu các prompt theo cách mà con người sẽ hiểu (Webson & Pavlick, 2021; Lu et al., 2021). Mặc dù nhiều phương pháp điều chỉnh prompt thành công thực hiện tối ưu hóa trên một không gian liên tục sử dụng các phương pháp dựa trên gradient (Liu et al., 2021; Qin & Eisner, 2021; Lester et al., 2021), điều này trở nên kém thực tế hơn với quy mô, vì việc tính toán gradient trở nên ngày càng đắt đỏ và việc truy cập vào các mô hình chuyển sang API có thể không cung cấp quyền truy cập gradient. Trong bài báo của chúng tôi, chúng tôi mượn các thành phần từ các phương pháp tìm kiếm prompt rời rạc, như sinh prompt (Gao et al., 2021; Ben-David et al., 2021), chấm điểm prompt (Davison et al., 2019) và diễn giải lại prompt (Jiang et al., 2020; Yuan et al., 2021) để tối ưu hóa các hướng dẫn bằng cách tìm kiếm trực tiếp trong không gian giả thuyết ngôn ngữ tự nhiên. So với nghiên cứu trước đây, sử dụng các mô hình chuyên biệt cho mỗi thành phần và dựa nhiều vào các mẫu của con người, chúng tôi cho thấy rằng toàn bộ quá trình tìm kiếm có thể được thực hiện bởi một LLM duy nhất.

Tổng hợp Chương trình Tổng hợp chương trình bao gồm việc tìm kiếm tự động trong "không gian chương trình" để tìm một chương trình thỏa mãn một đặc tả cụ thể (Gulwani et al., 2017). Tổng hợp chương trình hiện đại chấp nhận nhiều loại đặc tả đa dạng, bao gồm các ví dụ đầu vào-đầu ra (Ellis et al., 2021; Wong et al., 2021) và ngôn ngữ tự nhiên (Jain et al., 2022). Phạm vi các không gian chương trình khả thi để tìm kiếm cũng đã phát triển, từ các ngôn ngữ chuyên dụng hạn chế về mặt lịch sử đến các ngôn ngữ lập trình đa năng (Austin et al., 2021). Trái ngược với các phương pháp trước đây yêu cầu một không gian giả thuyết có cấu trúc phù hợp và thư viện các thành phần (Liang et al., 2010; Ellis et al., 2018), chúng tôi tận dụng cấu trúc được cung cấp bởi LLM để tìm kiếm trong không gian các chương trình ngôn ngữ tự nhiên. Sử dụng các mô hình suy luận là một thực hành tiêu chuẩn để tăng tốc tìm kiếm bằng cách hạn chế không gian tìm kiếm vào một không gian hạn chế của các biểu thức có thể (Menon et al., 2013; Lee et al., 2018; Devlin et al., 2017; Ellis et al., 2021). Lấy cảm hứng từ điều này, chúng tôi sử dụng LLM như các mô hình suy luận xấp xỉ để sinh các ứng cử viên chương trình dựa trên một tập nhỏ các minh chứng. Không như tổng hợp chương trình cổ điển, các mô hình suy luận của chúng tôi không yêu cầu bất kỳ huấn luyện nào và tổng quát hóa tốt cho các tác vụ khác nhau.

3 TỔNG HỢP CHƯƠNG TRÌNH NGÔN NGỮ TỰ NHIÊN SỬ DỤNG LLM

Chúng tôi xem xét một tác vụ được chỉ định bởi một tập dữ liệu Dtrain=f(Q;A)g các minh chứng đầu vào/đầu ra được lấy mẫu từ tổng thể X, và một mô hình được prompt M. Mục tiêu của tổng hợp chương trình ngôn ngữ tự nhiên là tìm một hướng dẫn đơn lẻ sao cho, khi M được prompt với việc nối [;Q] của hướng dẫn và một đầu vào cho trước, M tạo ra đầu ra tương ứng A. Chính thức hơn, chúng tôi khung hoá điều này như một vấn đề tối ưu hóa, nơi chúng tôi tìm kiếm hướng dẫn  tối đa hóa kỳ vọng của một số điểm số mỗi mẫu f(;Q;A) trên các (Q;A) có thể:

?= arg max
f() = arg max
E(Q;A)[f(;Q;A)] (1)

Lưu ý rằng nói chung, Q có thể là chuỗi rỗng, sao cho chúng tôi đang tối ưu hóa  như một prompt trực tiếp tạo ra các đầu ra fAg. Mặc dù tác vụ này đã được con người thử rộng rãi, chúng tôi có ít kiến thức về mức độ tương thích của bất kỳ hướng dẫn cụ thể nào với mô hình M. Do đó, chúng tôi đề xuất xử lý câu hỏi khó giải quyết của con người này như một quá trình tối ưu hóa hộp đen được hướng dẫn bởi LLM. Thuật toán của chúng tôi, APE, sử dụng LLM trong mỗi hai thành phần chính, đề xuất và chấm điểm. Như được hiển thị trong Hình 1 và được tóm tắt trong Thuật toán 1, APE đầu tiên đề xuất một vài ứng cử viên prompt, và sau đó lọc/tinh chỉnh tập ứng cử viên theo một hàm điểm số đã chọn, cuối cùng chọn hướng dẫn có điểm số cao nhất. Chúng tôi thảo luận các tùy chọn cho đề xuất và chấm điểm tiếp theo.

3.1 PHÂN PHỐI ĐỀ XUẤT BAN ĐẦU

Do không gian tìm kiếm vô hạn lớn, việc tìm hướng dẫn đúng có thể cực kỳ khó khăn, điều này đã làm cho tổng hợp chương trình ngôn ngữ tự nhiên trở nên không thể giải quyết về mặt lịch sử. Tiến bộ gần đây trong NLP đã cho thấy các mô hình ngôn ngữ rất giỏi trong việc sinh ra văn bản ngôn ngữ tự nhiên đa dạng. Do đó, chúng tôi

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Thuật toán 1 Automatic Prompt Engineer (APE)
Yêu cầu: Dtrain f(Q;A)gn: ví dụ huấn luyện, f:D7! R: hàm điểm số
1: Sử dụng LLM để lấy mẫu các đề xuất hướng dẫn U f1;:::;mg. (Xem Mục 3.1)
2: while chưa hội tụ do
3: Chọn một tập con huấn luyện ngẫu nhiên eDtrainDtrain.
4: for tất cả trong U do
5: Đánh giá điểm số trên tập con es f(;eDtrain)(Xem Mục 3.2 )
6: end for
7: Lọc k% hướng dẫn hàng đầu với điểm cao UkU sử dụng fes1;:::;esmg
8: Cập nhật hướng dẫn U Uk hoặc sử dụng LLM để lấy mẫu lại U resample (Uk)(Xem Mục 3.3)
9: end while
Trả về hướng dẫn với điểm cao nhất ? arg max2Ukf(;Dtrain)

xem xét việc tận dụng một LLM được huấn luyện trước để đề xuất một tập U tốt của các giải pháp ứng cử viên sẽ hướng dẫn thủ tục tìm kiếm của chúng tôi. Mặc dù các mẫu ngẫu nhiên từ LLM không chắc sẽ tạo ra các cặp (Q;A) mong muốn, thay vào đó chúng tôi có thể yêu cầu LLM xấp xỉ suy luận các hướng dẫn có khả năng nhất với điểm số cao, cho các minh chứng đầu vào/đầu ra; tức là, xấp xỉ lấy mẫu từ P(j Dtrain; f() cao).

Tôi đã đưa cho một người bạn một hướng dẫn và năm đầu vào. Người bạn đã đọc hướng dẫn và viết một đầu ra cho mỗi đầu vào. Đây là các cặp đầu vào-đầu ra:  
Đầu vào: [] Đầu ra: [] 
Đầu vào: [] Đầu ra: [] 
... 
Hướng dẫn là <COMPLETE>  Mẫu Sinh Thuận

Tôi đã hướng dẫn bạn tôi <INSERT>. 
Người bạn đã đọc hướng dẫn và viết một đầu ra cho mỗi đầu vào. Đây là các cặp đầu vào-đầu ra:  
Đầu vào: [] Đầu ra: [] 
Đầu vào: [] Đầu ra: [] 
... Mẫu Sinh Ngược

Giáo sư Smith được đưa ra những hướng dẫn sau: <INSERT> 
Đây là những phản hồi của Giáo sư: 
Đầu vào: [] Đầu ra: [] 
Đầu vào: [] Đầu ra: [] 
... Mẫu cho TruthfulQA

Hình 2: Prompt cho LLM

Sinh Chế độ Thuận Chúng tôi xem xét hai phương pháp để sinh các ứng cử viên chất lượng cao từ P(j Dtrain; f() cao). Đầu tiên, chúng tôi áp dụng một phương pháp dựa trên sinh "thuận" bằng cách dịch phân phối P(j Dtrain; f() cao) này thành từ ngữ. Ví dụ, trong các thí nghiệm cảm ứng hướng dẫn của chúng tôi (Mục con 4.1), chúng tôi theo Honovich et al. (2022) và prompt LLM sử dụng Hình 2 (Trên).

Sinh Chế độ Ngược Mặc dù mô hình "thuận" hoạt động ngay từ đầu cho hầu hết các LLM được huấn luyện trước, việc dịch P(j Dtrain; f() cao) thành từ ngữ yêu cầu kỹ thuật tùy chỉnh qua các tác vụ khác nhau. Điều này là do trong khi các hướng dẫn thường được tìm thấy ở đầu các đoạn văn, mô hình "thuận" chỉ sinh văn bản từ trái sang phải, yêu cầu hướng dẫn được dự đoán ở cuối prompt. Do đó, chúng tôi mong muốn một phương pháp linh hoạt hơn sao cho hướng dẫn có thể ở bất cứ đâu trong văn bản. Để giải quyết điều này, chúng tôi xem xét sinh "ngược", sử dụng một LLM với khả năng điền vào chỗ trống—ví dụ, T5 (Raffel et al., 2020), GLM (Du et al., 2022), và InsertGPT (Bavarian et al., 2022)—để suy luận các hướng dẫn bị thiếu. Mô hình "ngược" của chúng tôi trực tiếp lấy mẫu từ P(j Dtrain; f() cao) bằng cách điền vào chỗ trống. Chúng tôi hiển thị một ví dụ về mẫu như vậy trong Hình 2 (Giữa).

Prompt Tùy chỉnh Lưu ý rằng tùy thuộc vào hàm điểm số được sử dụng, có thể tồn tại các prompt phù hợp hơn so với các mẫu ở trên. Ví dụ, trong các thí nghiệm TruthfulQA của chúng tôi, chúng tôi bắt đầu với các hướng dẫn được thiết kế bởi con người từ tập dữ liệu gốc (Lin et al., 2022) và yêu cầu mô hình "ngược" đề xuất các mẫu hướng dẫn ban đầu phù hợp với ngữ cảnh bị thiếu (Hình 2 (Dưới)).

3.2 HÀM ĐIỂM SỐ

Để đưa vấn đề của chúng tôi thành tối ưu hóa hộp đen, chúng tôi chọn một hàm điểm số đo lường chính xác sự liên kết giữa tập dữ liệu và dữ liệu mà mô hình sinh ra. Trong các thí nghiệm cảm ứng hướng dẫn của chúng tôi, chúng tôi xem xét hai hàm điểm số tiềm năng, được mô tả dưới đây. Trong các thí nghiệm TruthfulQA, chúng tôi tập trung chủ yếu vào các metric tự động được đề xuất trong Lin et al. (2022), tương tự như độ chính xác thực thi. Trong mỗi trường hợp, chúng tôi đánh giá chất lượng của một hướng dẫn được sinh ra bằng Phương trình (1), và lấy kỳ vọng trên một tập dữ liệu kiểm tra được giữ lại Dtest.

Độ chính xác thực thi Đầu tiên, chúng tôi xem xét đánh giá chất lượng của một hướng dẫn bằng metric độ chính xác thực thi được đề xuất bởi Honovich et al. (2022), mà chúng tôi ký hiệu là fexec. Trong hầu hết các trường hợp,

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

độ chính xác thực thi chỉ đơn giản được định nghĩa là mất mát 0-1, f(;Q;A) =1[M([;Q]) =A]. Đối với một số tác vụ, độ chính xác thực thi tính đến các bất biến; ví dụ, nó có thể là một mất mát ghép cặp tập hợp bất biến thứ tự, như được mô tả trong Phụ lục A của Honovich et al. (2022).

Xác suất Log Chúng tôi tiếp tục xem xét một hàm điểm số xác suất mềm hơn, mà chúng tôi giả thuyết có thể cải thiện tối ưu hóa bằng cách cung cấp một tín hiệu chi tiết hơn khi tìm kiếm các ứng cử viên hướng dẫn chất lượng thấp. Cụ thể, chúng tôi xem xét xác suất log của câu trả lời mong muốn cho hướng dẫn và câu hỏi dưới mô hình mục tiêu M, mà trên cơ sở mỗi mẫu, là logP(Aj[;Q]).

Ước lượng điểm số hiệu quả Ước lượng điểm số bằng cách tính toán điểm số trên toàn bộ tập dữ liệu huấn luyện cho tất cả ứng cử viên hướng dẫn có thể tốn kém. Để giảm chi phí tính toán, chúng tôi áp dụng một sơ đồ lọc nơi một ứng cử viên triển vọng nhận được nhiều tài nguyên tính toán hơn trong khi một ứng cử viên chất lượng thấp nhận được ít hơn. Nó có thể đạt được bằng cách sử dụng một chiến lược tính toán đa giai đoạn trên các dòng 2-9 Thuật toán 1. Chúng tôi đầu tiên đánh giá tất cả ứng cử viên với một tập con nhỏ của tập dữ liệu huấn luyện. Đối với các ứng cử viên có điểm số lớn hơn một ngưỡng nhất định, chúng tôi lấy mẫu và đánh giá một tập con mới không chồng lấp từ tập dữ liệu huấn luyện để cập nhật trung bình di động của điểm số. Sau đó, chúng tôi lặp lại quá trình này cho đến khi còn lại một tập nhỏ các ứng cử viên, được đánh giá trên toàn bộ tập dữ liệu huấn luyện. Sơ đồ lọc thích ứng này cải thiện đáng kể hiệu quả tính toán bằng cách giữ chi phí tính toán chính xác cho các mẫu chất lượng cao và giảm drastically chi phí tính toán cho các ứng cử viên chất lượng thấp. Chúng tôi lưu ý rằng một sơ đồ ước lượng điểm số tương tự đã được sử dụng trong các nghiên cứu trước đây (Li et al., 2022; Maclaurin & Adams, 2015).

3.3 PHÂN PHỐI ĐỀ XUẤT LẶP LẠI

Mặc dù nỗ lực trực tiếp lấy mẫu các ứng cử viên hướng dẫn chất lượng cao ban đầu, có thể trường hợp phương pháp được mô tả trong Mục con 3.1 không thể tạo ra một tập đề xuất U tốt, hoặc vì thiếu sự đa dạng hoặc không chứa bất kỳ ứng cử viên nào có điểm số đủ cao. Trong trường hợp những thách thức như vậy, chúng tôi khám phá một quá trình lặp lại để lấy mẫu lại U.

Tạo ra một biến thể của hướng dẫn sau đây trong khi giữ nguyên ý nghĩa ngữ nghĩa.
Đầu vào: [HƯỚNG DẪN]
Đầu ra: <COMPLETE> Prompt cho Lấy mẫu lại

Hình 3: Lấy mẫu lại

Tìm kiếm Monte Carlo Lặp lại Thay vì chỉ lấy mẫu từ đề xuất ban đầu, chúng tôi xem xét khám phá không gian tìm kiếm cục bộ xung quanh các ứng cử viên tốt nhất hiện tại. Điều này cho phép chúng tôi sinh ra các hướng dẫn mới có khả năng thành công cao hơn. Chúng tôi gọi biến thể này là APE lặp lại. Ở mỗi giai đoạn, chúng tôi đánh giá một tập hướng dẫn và lọc ra các ứng cử viên có điểm thấp. Sau đó, một LLM được yêu cầu sinh ra các hướng dẫn mới tương tự với những hướng dẫn có điểm cao. Chúng tôi cung cấp prompt được sử dụng cho lấy mẫu lại trong Hình 3. Hình 6 (Phải) cho thấy rằng mặc dù phương pháp này cải thiện chất lượng tổng thể của tập đề xuất U, hướng dẫn có điểm cao nhất có xu hướng giữ nguyên với nhiều giai đoạn hơn. Chúng tôi kết luận rằng việc sinh lặp lại cung cấp cải thiện biên so với tính đơn giản tương đối và hiệu quả của quá trình sinh được mô tả trong Mục con 3.1. Do đó, chúng tôi sử dụng APE không có tìm kiếm lặp lại làm mặc định trừ khi có quy định khác.

4 CÁC MÔ HÌNH NGÔN NGỮ LỚN LÀ NHỮNG KỸ SƯ PROMPT Ở MỨC CON NGƯỜI

Mục này xem xét cách APE có thể hướng dẫn LLM đến các hành vi mong muốn. Chúng tôi điều tra từ bốn góc độ: hiệu suất zero-shot, hiệu suất học trong ngữ cảnh few-shot, lý luận chain-of-thought zero-shot, và tính trung thực. Các thí nghiệm của chúng tôi cho thấy rằng APE có thể tìm các prompt cải thiện hiệu suất tác vụ, thực hiện ngang bằng hoặc thậm chí tốt hơn những prompt được tác giả bởi con người. APE cũng thường tạo ra những thủ thuật sâu sắc về cách prompt các mô hình ngôn ngữ tốt nhất có thể được chuyển giao thành công cho các tác vụ mới (xem Mục 4.3).

4.1 CẢM ỨNG HƯỚNG DẪN

Chúng tôi đánh giá hiệu quả của học zero-shot và few-shot trong ngữ cảnh trên 24 tác vụ cảm ứng hướng dẫn được đề xuất trong Honovich et al. (2022). Các tác vụ bao phủ nhiều khía cạnh của sự hiểu biết ngôn ngữ, từ cấu trúc cụm từ đơn giản đến nhận dạng sự tương tự và quan hệ nhân quả. Chúng tôi cung cấp mô tả chi tiết của mỗi tác vụ trong Phụ lục B. Đối với mỗi tác vụ, chúng tôi lấy mẫu năm cặp đầu vào-đầu ra từ dữ liệu huấn luyện và chọn hướng dẫn tốt nhất bằng thuật toán 1. Sau đó, chúng tôi đánh giá chất lượng của hướng dẫn

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

bằng cách thực thi hướng dẫn trên InstructGPT3. Chúng tôi lặp lại thí nghiệm năm lần với các seed ngẫu nhiên khác nhau để báo cáo trung bình và độ lệch chuẩn. Các mẫu chính xác cho thí nghiệm của chúng tôi có thể được tìm thấy trong Phụ lục (Bảng 5).

Học Zero-shot Chúng tôi so sánh phương pháp của mình với hai baseline: kỹ sư prompt con người (Con người)4 và thuật toán hướng dẫn được tạo bởi mô hình được đề xuất bởi Honovich et al. (2022). Thuật toán này có thể được coi như một phiên bản tham lam của APE, không có quá trình tìm kiếm và lựa chọn; do đó, chúng tôi gọi nó là "Tham lam". Hình 4 cho thấy hiệu suất zero-shot của InstructGPT sử dụng hướng dẫn con người và hướng dẫn được tạo bởi mô hình. Thuật toán của chúng tôi vượt trội hơn "Tham lam" trên mọi tác vụ và đạt được ngang bằng hoặc tốt hơn hiệu suất con người trên 24 trong 24 tác vụ. Hơn nữa, Trung vị Khoảng tứ phân (IQM) (Agarwal et al., 2021) trên tất cả 24 tác vụ trong Hình 1 gợi ý rằng APE với InstructGPT vượt trội hơn các prompt được thiết kế bởi con người, đạt được IQM 0.810 so với 0.749 của con người. Chúng tôi tóm tắt hướng dẫn được chọn bởi APE cho mỗi tác vụ trong Phụ lục (Bảng 12).

Học Few-shot Trong ngữ cảnh Chúng tôi đánh giá các hướng dẫn được tạo bởi APE trong học few-shot trong ngữ cảnh, nơi chúng tôi chèn hướng dẫn trước các minh chứng trong ngữ cảnh. Những hướng dẫn đó được chọn dựa trên độ chính xác thực thi zero-shot, và chúng tôi ký hiệu thiết lập này là "Hướng dẫn + Trong ngữ cảnh" trong Hình 8. Như được hiển thị trong Hình 8, việc thêm một hướng dẫn đạt được hiệu suất kiểm tra tương đương hoặc tốt hơn so với hiệu suất học trong ngữ cảnh tiêu chuẩn trên 21 trong 24 tác vụ. Trái với trực giác, việc thêm các ví dụ trong ngữ cảnh cho Rhymes, Large Animal, và Second Letters làm hại hiệu suất mô hình. Chúng tôi đoán rằng có thể vì các hướng dẫn được chọn overfit kịch bản học zero-shot và do đó không hoạt động tốt trong trường hợp few-shot. Do đó, chúng tôi thí nghiệm sử dụng độ chính xác thực thi few-shot làm metric lựa chọn. Hình 14 cho thấy rằng metric few-shot đạt được tương đương hoặc hơi tốt hơn so với metric zero-shot ngoại trừ Rhymes. Để có sự hiểu biết trực quan về những gì đang xảy ra, chúng tôi cung cấp một phân tích định tính trong Phụ lục C.1.

4.2 BIGBENCH

Để xem liệu APE có thể được áp dụng cho các tác vụ thách thức hơn không, chúng tôi đề xuất và tuyển chọn BIG-Bench Instruction Induction (BBII), một tập con sạch và có thể xử lý được của 21 tác vụ có hướng dẫn rõ ràng, được viết bởi con người có thể được áp dụng cho tất cả các ví dụ trong tập dữ liệu. Các tác vụ được chọn bao phủ nhiều khía cạnh của sự hiểu biết ngôn ngữ và bao gồm tất cả chín vấn đề như vậy từ Tập con BigBench-Hard (Suzgun et al., 2022). Cụ thể, nó bao gồm sự hiểu biết cảm xúc, trả lời câu hỏi không có ngữ cảnh, đọc hiểu, tóm tắt, thuật toán, và các tác vụ lý luận khác nhau (ví dụ, số học, thường thức, tượng trưng, và các tác vụ lý luận logic khác). Chúng tôi cung cấp mô tả chi tiết của tác vụ và tiêu chí lựa chọn của chúng tôi trong Phụ lục B.

Antonyms Cause Selection Common Concept Diff First Letter Formality Large Animal List Letters01
Membership Negation Number to Word Passivization Pluralization Rhymes Second Letter Sentence Similarity01
Sentiment Starting With Sum Synonyms Translation en-de Translation en-es Translation en-fr Word in Context01Độ Chính xác Thực thi Tham lam Con người APE

Hình 4: Độ chính xác kiểm tra zero-shot trên 24 tác vụ Instruction Induction. APE đạt được hiệu suất cấp độ con người hoặc tốt hơn trên tất cả 24 trong 24 tác vụ.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Đối với mỗi tác vụ, chúng tôi sử dụng sinh chế độ ngược của InstructGPT để tạo ra một tập các ứng cử viên hướng dẫn và xếp hạng các hướng dẫn dựa trên độ chính xác thực thi của chúng. Sau đó, chúng tôi thực thi hướng dẫn được chọn trên InstructGPT để tính hiệu suất zero-shot trên tập kiểm tra và so sánh nó với prompt con người mặc định. Như được hiển thị trong Bảng 6 Phụ lục, APE đạt được hiệu suất tương đương hoặc tốt hơn so với prompt con người mặc định trên 17 trong 21 tác vụ.

4.3 CHAIN OF THOUGHT ZERO-SHOT

Lý luận Chain-of-thought đã được chứng minh là cải thiện đáng kể khả năng của LLM để hoàn thành các tác vụ lý luận phức tạp, như giải các bài toán có nhiều bước. Các nghiên cứu đầu (Nye et al., 2021; Betz et al., 2021; Wei et al., 2022b) về chain-of-thought đã sử dụng fine-tuning hoặc học trong ngữ cảnh để khiến LLM thể hiện công việc của chúng cho các vấn đề như vậy. Một trong những nghiên cứu kỹ thuật prompt có ảnh hưởng gần đây nhất là khám phá (Kojima et al., 2022) rằng LLM có thể được khiến đưa ra chain-of-thoughts chỉ đơn giản bằng cách thêm "Let's think step by step." vào đầu phản hồi của LLM. Được biết đến như Zero-Shot-CoT, chiến lược prompting này cải thiện hiệu suất zero-shot của InstructGPT trên MultiArith (Roy & Roth, 2016) từ 17.7 lên 78.7 và cải thiện hiệu suất trên GSM8K (Cobbe et al., 2021) từ 10.4 lên 40.7. Như được hiển thị trong Bảng 7, Kojima et al. (2022) thấy rằng prompt của họ là prompt hoạt động tốt nhất trong ít nhất chín prompt được thiết kế bởi con người.

Chúng tôi sử dụng APE để tự động tìm kiếm tiền tố câu trả lời tốt nhất trên bộ tác vụ được sử dụng trong Kojima et al. (2022). Phương pháp của chúng tôi để tối ưu hóa prompt này được lấy cảm hứng từ Zelikman et al. (2022). Đầu tiên, chúng tôi tạo ra một tập dữ liệu về câu hỏi và các bước lý luận được tạo bằng InstructGPT với "Let's think step by step." Sau đó, chúng tôi loại bỏ bất kỳ điểm dữ liệu nào có câu trả lời không chính xác. Cuối cùng, chúng tôi sử dụng APE để tìm một prompt bắt đầu với "Let's" tối đa hóa khả năng của những bước lý luận chính xác này. Xem Bảng 5 cho mẫu được sử dụng cho sinh prompt và đánh giá. APE tạo ra prompt "Let's work this out in a step by step way to be sure we have the right answer." Prompt được tạo này tiếp tục cải thiện hiệu suất từ 78.7 lên 82.0 trên MultiArith và từ 40.7 lên 43.0 trên GSM8K. Chúng tôi tin rằng quy trình chung này đại diện cho một trường hợp sử dụng phổ biến cho APE nơi các kỹ sư prompt sử dụng APE để tối ưu hóa các phần của các mẫu hiện có của họ để cải thiện hiệu suất. Xem Hình 10 để biết chi tiết về hiệu suất của prompt này trên các tác vụ lý luận khác.

4.4 TRUTHFUL QA

Chúng tôi áp dụng phương pháp của mình trên TruthfulQA (Lin et al., 2022) để xem cách các hướng dẫn được tạo bởi APE có thể điều hướng một LLM để tạo ra câu trả lời với các phong cách khác nhau, và nghiên cứu sự đánh đổi giữa tính trung thực và tính thông tin. Mượn các metric từ bài báo gốc, chúng tôi sử dụng APE để học các hướng dẫn tối đa hóa ba metric: tính trung thực (% True), tính thông tin (% Info), và sự kết hợp của cả hai (%True + %Info). Lin et al. (2022) đã sử dụng đánh giá của con người để đánh giá hiệu suất mô hình, nhưng họ thấy rằng các metric tự động của họ phù hợp với dự đoán của con người hơn 90% thời gian. Trong các thí nghiệm của chúng tôi, chúng tôi dựa vào GPT-judge và GPT-info được fine-tuned của họ để đánh giá các điểm số.

Kỹ thuật Prompt trong TruthfulQA Chúng tôi muốn nhấn mạnh rằng tập dữ liệu TruthfulQA được thiết kế để kiểm tra các mô hình được huấn luyện trước trong thiết lập zero-shot. Kết quả của chúng tôi không tương thích theo bất kỳ cách nào với các benchmark gốc. Bởi vì chúng tôi đã tối ưu hóa các hướng dẫn sử dụng một phần nhỏ của các cặp câu hỏi và câu trả lời làm minh chứng huấn luyện, kết quả của chúng tôi không phải là "học few-shot thực sự" (Perez et al., 2021). Chúng tôi lấy mẫu ngẫu nhiên 100 trong 817 câu hỏi cho các thí nghiệm thực tế để tạo thành các minh chứng huấn luyện Dtrain. Để lấy mẫu tập đề xuất U, chúng tôi yêu cầu một mô hình "ngược" tạo ra các hướng dẫn dựa trên sáu cặp minh chứng được chọn ngẫu nhiên, tương tự như các thí nghiệm trước đây của chúng tôi. Không như trong Instruction Induction, trong TruthfulQA, chúng tôi nhằm tìm một hướng dẫn prompt tốt nhất duy nhất hoạt động tốt trên tất cả 38 danh mục câu hỏi bao gồm sức khỏe, luật pháp, chính trị, và tiểu thuyết. Đáng chú ý rằng tất cả các hướng dẫn được tạo của chúng tôi đều rất tổng quát, ví dụ, "You will be asked a series of questions. For each question, you must either answer the question or decline to answer, in which case you must state that you have no comment", và không chứa bất kỳ ví dụ nào từ tập dữ liệu.

Đánh đổi Tính trung thực vs Tính thông tin Chúng tôi thấy rằng APE vượt trội hơn prompt được thiết kế bởi con người chỉ với 200 ứng cử viên được đề xuất bởi InstructGPT (175B), như được thấy trong Hình 5. Chúng tôi so sánh prompt được tạo của mình với prompt "help" từ Lin et al. (2022). Hiệu suất huấn luyện và kiểm tra được hiển thị trong Hình 5(a)-(b). Chúng tôi thấy rằng việc chọn top 10 trong 200 ứng cử viên trên tập huấn luyện tổng quát hóa tốt cho tập kiểm tra. Chúng tôi báo cáo hiệu suất trung bình trên top 10 hướng dẫn cho ba metric. Kết quả này bản thân nó không đáng ngạc nhiên vì baseline con người

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

% True (GPT-judge) % Info (GPT-info) % True + % Info0.00.20.40.60.8Giá trị Metric (%)Con người
APE
(a) Hiệu suất trung bình
Huấn luyện

% True (GPT-judge) % Info (GPT-info) % True + % Info0.00.20.40.60.8Giá trị Metric (%)Con người
APE(b) Hiệu suất trung bình
Kiểm tra

0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95
% True (GPT-judge)0.20.30.40.50.60.70.8% Informative (GPT-info)Trung thực
Thông tin
Trung thực+Thông tin
Con người(c) Đánh đổi %True-%Info
Huấn luyện

0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95
% True (GPT-judge)0.20.30.40.50.60.70.8% Informative (GPT-info)Trung thực
Thông tin
Trung thực+Thông tin
Con người(d) Đánh đổi %True-%Info
Kiểm tra

Hình 5: So sánh APE và prompt "help" (con người) trên tác vụ TruthfulQA. (a) Phần trăm câu trả lời đúng (% True), thông tin (% Info), hoặc cả hai (% True + % Info) trên 100 ví dụ huấn luyện. (b) Cùng dữ liệu trên 717 ví dụ kiểm tra. (c) Biên %True-%Info được tính trên dữ liệu huấn luyện với top 10 hướng dẫn từ mỗi metric. (d) Biên %True-%Info trên dữ liệu kiểm tra.

không được chọn cẩn thận, như được chỉ ra bởi Askell et al. (2021). Tuy nhiên, chúng tôi thấy rằng các hướng dẫn được khám phá bởi APE có thể đạt được tính trung thực rất cao với các câu trả lời như "No comment," nhưng những câu trả lời này cung cấp ít thông tin. Chúng tôi sử dụng các ứng cử viên hàng đầu của mình để điều tra thêm sự đánh đổi giữa tính trung thực và tính thông tin. Chúng tôi hình dung hóa top 10 mẫu được đề xuất trên ba metric trên các biểu đồ tính trung thực-thông tin được hiển thị trong Hình 5(c) và Hình 5(d). Trong khi APE đạt được hơn 40% độ chính xác trong việc cung cấp câu trả lời vừa đúng vừa thông tin (so với 30% bởi prompt "help" từ con người), các hướng dẫn được khám phá có xu hướng nhắm vào hai đầu của biên Pareto %true-%info này.

5 PHÂN TÍCH ĐỊNH LƯỢNG

Trong mục này, chúng tôi tiến hành phân tích định lượng để hiểu rõ hơn ba thành phần chính của phương pháp chúng tôi: phân phối đề xuất, hàm điểm số, và tìm kiếm lặp lại. Hơn nữa, chúng tôi tiến hành phân tích chi phí trong Phụ lục D để hiểu cách hiệu quả nhất về chi phí để tìm prompt tốt nhất. Chúng tôi quan sát rằng các mô hình ngôn ngữ lớn hơn và mạnh mẽ hơn hiệu quả hơn về chi phí để tạo ra prompt tốt nhất mặc dù chi phí mỗi token cao hơn.

5.1 LLM CHO PHÂN PHỐI ĐỀ XUẤT

Chất lượng đề xuất thay đổi như thế nào khi chúng ta tăng kích thước mô hình? Để hiểu cách kích thước mô hình ảnh hưởng đến chất lượng của phân phối đề xuất ban đầu, chúng tôi xem xét tám mô hình khác nhau5 có sẵn qua OpenAI API. Để đánh giá chất lượng của phân phối đề xuất, chúng tôi tạo ra 250 hướng dẫn trên mỗi mô hình và tính độ chính xác thực thi trên 50 điểm dữ liệu kiểm tra. Chúng tôi hình dung hóa hàm survival (phần trăm hướng dẫn có độ chính xác kiểm tra lớn hơn một ngưỡng nhất định) và histogram của độ chính xác kiểm tra cho một tác vụ đơn giản (tức là, Pluralization) trong Hình 6 (a) và bao gồm một biểu đồ tương tự cho một tác vụ thách thức hơn (Start With) trong Phụ lục (Hình 28). Như được hiển thị trong cả hai hình (và không đáng ngạc nhiên), các mô hình lớn hơn có xu hướng tạo ra phân phối đề xuất tốt hơn so với các mô hình nhỏ hơn, cũng như các mô hình được fine-tuned để tuân theo hướng dẫn con người. Trên tác vụ đơn giản, tất cả các hướng dẫn được tạo bởi mô hình tốt nhất, InstructGPT (175B), có độ chính xác kiểm tra hợp lý. Ngược lại, một nửa số hướng dẫn ngoài chủ đề và hoạt động kém trên tác vụ thách thức hơn.

5.2 LLM CHO LỰA CHỌN

Chất lượng đề xuất có quan trọng dưới việc lựa chọn không? Nếu chúng ta lấy mẫu nhiều hướng dẫn hơn từ LLM, thì chúng ta có khả năng tìm thấy các hướng dẫn tốt hơn. Để xác minh giả thuyết này, chúng tôi tăng kích thước mẫu từ 4 lên 128 và đánh giá thay đổi độ chính xác kiểm tra. Hình 7 (Trái) cho thấy một xu hướng tăng đơn điệu với lợi nhuận giảm dần, vì hiệu suất cấp độ con người đạt được với 64 mẫu hướng dẫn. Do đó, chúng tôi chọn 50 làm kích thước mẫu mặc định của chúng tôi. Dưới cấu hình này, chúng tôi điều tra cách phân phối đề xuất ảnh hưởng đến độ chính xác kiểm tra của hướng dẫn tốt nhất được chọn bởi thuật toán của chúng tôi. Hình 1(b) cho thấy rằng mặc dù các mô hình nhỏ có thể ít khả năng tạo ra hướng dẫn tốt, chúng vẫn tạo ra một số hướng dẫn tốt nếu chúng ta lấy mẫu đủ ứng cử viên. Do đó,

5Chúng tôi sử dụng ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001, text-davanci-002

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

chúng tôi vẫn tìm thấy các hướng dẫn triển vọng với một mô hình nhỏ bằng cách chạy thuật toán lựa chọn của chúng tôi, giải thích tại sao phương pháp của chúng tôi vượt trội hơn phương pháp tham lam Honovich et al. (2022) trên tất cả tám mô hình.

Hàm chấm điểm nào tốt hơn? Chúng tôi tính tương quan giữa độ chính xác kiểm tra và hai metric trên 24 tác vụ cảm ứng hướng dẫn để nghiên cứu các metric đề xuất của chúng tôi tốt như thế nào. Chúng tôi tạo ra 250 hướng dẫn trên mỗi tác vụ sử dụng InstructGPT (175B) trong chế độ "thuận" và tính điểm số metric và độ chính xác kiểm tra trên 10 điểm dữ liệu kiểm tra. Chúng tôi hình dung hóa tương quan Spearman giữa độ chính xác kiểm tra và hai metric. Hình 7 (Giữa) cho thấy rằng độ chính xác thực thi phù hợp tốt hơn với hiệu suất kiểm tra trên các tác vụ. Do đó, chúng tôi chọn nó làm metric mặc định trừ khi có quy định khác.

5.3 TÌM KIẾM MONTE CARLO LẶP LẠI

Tìm kiếm lặp lại có cải thiện chất lượng hướng dẫn không? Chúng tôi hình dung hóa hàm survival và histogram của độ chính xác kiểm tra trên tác vụ "Passivization" trong Hình 6 (Phải) và bao gồm năm tác vụ khác trong Phụ lục. Biểu đồ survival cho thấy rằng các đường cong tăng lên khi số vòng tăng lên, gợi ý rằng tìm kiếm lặp lại thực sự dẫn đến một tập đề xuất chất lượng cao hơn. Tuy nhiên, chúng tôi quan sát lợi nhuận giảm dần đối với các vòng lựa chọn tiếp theo vì chất lượng dường như ổn định sau ba vòng.

Chúng ta có cần Tìm kiếm lặp lại không? Chúng tôi so sánh APE và APE lặp lại trên sáu tác vụ6. Như được hiển thị trong Hình 7, tìm kiếm lặp lại cải thiện hiệu suất một cách biên trên các tác vụ nơi APE hoạt động kém hơn con người nhưng đạt được hiệu suất tương tự trên các tác vụ khác. Điều này phù hợp với giả thuyết của chúng tôi rằng tìm kiếm lặp lại sẽ hữu ích nhất trên các tác vụ nơi việc tạo ra một U ban đầu tốt là thách thức.

6Sáu tác vụ này được chọn sao cho hai trong số chúng tệ hơn con người, và bốn cái khác ở cấp độ con người. Chúng bao gồm sáu danh mục (chính tả, hình thái cú pháp, ngữ nghĩa từ vựng, ngữ nghĩa, đa ngôn ngữ, và GLUE).

0.0 0.2 0.4 0.6 0.8 1.0
Độ chính xác kiểm tra ()
0.00.20.40.60.81.0% hướng dẫn có độ chính xác > 

0.0 0.2 0.4 0.6 0.8 1.0
Độ chính xác kiểm tra ()
100101102
ĐếmGPT-3 (350M)
GPT-3 (1.3B)GPT-3 (6.7B)
GPT-3 (175B)InstructGPT (350M)
InstructGPT (1.3B)InstructGPT (6.7B)
InstructGPT (175B)

0.0 0.2 0.4 0.6 0.8 1.0
Độ chính xác huấn luyện ()
0.00.20.40.60.81.0% hướng dẫn có độ chính xác > 

0.0 0.2 0.4 0.6 0.8 1.0
Độ chính xác huấn luyện ()
100101102
ĐếmBắt đầu 1 2 3 4 5

Hình 6: (Trái) Chất lượng của phân phối đề xuất của các mô hình với kích thước khác nhau được đánh giá bởi độ chính xác thực thi kiểm tra. (Phải) Tìm kiếm Monte Carlo lặp lại cải thiện chất lượng của các ứng cử viên hướng dẫn ở mỗi vòng.

4 8 16 32 64 128
Kích thước Mẫu Posterior0.40.50.60.70.8Độ Chính xác Thực thiAPE (Huấn luyện)
APE (Kiểm tra)
Con người

0 5 10 15 20 25
Chỉ số Tác vụ Được sắp xếp0.2
0.00.20.40.60.81.0Tương quan SpearmanLogP
Độ chính xác Thực thi

Second Letter Passivization Translation en-fr01
Sentiment Antonyms Cause Selection01Độ chính xác Thực thiCon người APE APE (IT)

Hình 7: (Trái) Thực thi kiểm tra của hướng dẫn tốt nhất khi chúng ta tăng số lượng ứng cử viên hướng dẫn. Chúng tôi báo cáo trung bình và độ lệch chuẩn trên 6 tác vụ khác nhau. (Giữa) Tương quan Spearman giữa độ chính xác kiểm tra và hai metric trên 24 tác vụ. (Phải) Độ chính xác thực thi kiểm tra của hướng dẫn tốt nhất được chọn bằng APE và APE lặp lại (APE (IT)).

6 KẾT LUẬN

Các mô hình ngôn ngữ lớn có thể được xem như những máy tính đa năng thực thi các chương trình được chỉ định bởi các prompt ngôn ngữ tự nhiên. Chúng tôi tự động hóa quá trình kỹ thuật prompt bằng cách công thức hóa nó như một vấn đề tối ưu hóa hộp đen, mà chúng tôi đề xuất giải quyết bằng các thuật toán tìm kiếm hiệu quả được hướng dẫn bởi LLM. Phương pháp của chúng tôi đạt được hiệu suất cấp độ con người trên các tác vụ khác nhau với đầu vào con người tối thiểu. Khi các LLM gần đây chứng minh khả năng ấn tượng để tuân theo hướng dẫn con người, chúng tôi kỳ vọng nhiều mô hình tương lai, bao gồm những mô hình cho tổng hợp chương trình chính thức, sẽ có giao diện ngôn ngữ tự nhiên. Nghiên cứu này xây dựng nền tảng để kiểm soát và điều hướng trí tuệ nhân tạo sinh tạo.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

LỜI CẢM ƠN

Chúng tôi muốn cảm ơn Or Honovich và Michael Zhang vì sự giúp đỡ và phản hồi quý báu của họ. JB được hỗ trợ bởi NSERC Grant [2020-06904], chương trình CIFAR AI Chairs, Google Research Scholar Program và Amazon Research Award. KP được hỗ trợ bởi NSERC PGS-D. SP được hỗ trợ bởi NSERC CGS-D. HC được hỗ trợ bởi NSERC CGS-D và RBC Graduate Fellowship. Tài nguyên được sử dụng trong việc chuẩn bị nghiên cứu này được cung cấp, một phần, bởi Tỉnh Ontario, Chính phủ Canada thông qua CIFAR, và các công ty tài trợ Vector Institute for Artificial Intelligence.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch với định dạng tương tự như bản gốc...]
