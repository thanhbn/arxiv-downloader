# 2310.02107.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2310.02107.pdf
# Kích thước tệp: 1047566 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Các Instance Cần Được Chú Ý Hơn: Viết Lại Prompts cho Instances với LLMs trong Vòng Lặp Mang Lại Hiệu Suất Zero-Shot Tốt Hơn
CẢNH BÁO: Bài báo này hiển thị các mẫu dữ liệu và đầu ra mô hình có tính chất độc hại.
Saurabh Srivastava⋆∗, Chengyue Huang#, Weiguo Fan#, Ziyu Yao⋆*
⋆George Mason University,#University of Iowa
{ssrivas6, ziyuyao}@gmu.edu ,
{chengyue-huang, weiguo-fan}@uiowa.edu

Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa hiệu suất tác vụ zero-shot, giảm thiểu nhu cầu chú thích cụ thể cho từng tác vụ trong khi tăng cường khả năng tổng quát hóa tác vụ. Mặc dù có những tiến bộ, các phương pháp hiện tại sử dụng các cụm từ kích hoạt như "Let's think step by step" vẫn còn hạn chế. Nghiên cứu này giới thiệu PROMPTED, một phương pháp tối ưu hóa các prompts zero-shot cho từng instance tác vụ cá biệt theo cách thức sáng tạo "LLMs in the loop". Đánh giá toàn diện của chúng tôi trên 13 bộ dữ liệu và 10 loại tác vụ dựa trên GPT-4 tiết lộ rằng PROMPTED vượt trội đáng kể so với cả các phương pháp zero-shot thông thường và một baseline mạnh (tức là "Output Refinement") - phương pháp tinh chỉnh đầu ra tác vụ thay vì prompt đầu vào. Kết quả thực nghiệm của chúng tôi cũng xác nhận sự tổng quát hóa của lợi thế này đối với GPT-3.5 tương đối yếu hơn. Thậm chí thú vị hơn, chúng tôi phát hiện rằng việc tận dụng GPT-3.5 để viết lại prompts cho GPT-4 mạnh hơn không chỉ ngang bằng mà thỉnh thoảng còn vượt qua hiệu quả của việc sử dụng GPT-4 làm trình viết lại prompt. Nghiên cứu của chúng tôi do đó mang lại giá trị lớn không chỉ trong việc tăng cường hiệu suất LLM zero-shot mà còn có khả năng cho phép giám sát LLMs bằng các đối tác yếu hơn của chúng, một khả năng đang thu hút nhiều sự quan tâm gần đây. Cuối cùng, các thí nghiệm bổ sung của chúng tôi xác nhận sự tổng quát hóa của các lợi thế đối với các LLMs mã nguồn mở như Mistral 7B và Mixtral 8x7B.

1 Giới thiệu
Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa bối cảnh xử lý ngôn ngữ tự nhiên. Các mô hình này thực hiện các tác vụ downstream chủ yếu thông qua prompting, có thể được phân loại thành hai loại, tức là zero-shot prompting và few-shot in-context learning. Trong zero-shot

*Tác giả liên hệ
1Mã nguồn và dữ liệu được phát hành tại https://github.com/salokr/PRoPMTed .

Zero Shot Zero-Shot CoTOutput
RefinementPROMPTED
Prompt
Output
Q: Prompt  
Output   + Let's Think Step by Step
OutputPrompt
Prompt  
Output
Avg. Accuracy
4050607080
GPT-3.5 GPT-4Zero-Shot Zero-Shot CoT Output Refinement PROMPTED

Bảng 1: So sánh giữa PROMPTED và các baseline khác. PROMPTED sử dụng một meta LLM để tinh chỉnh lặp đi lặp lại prompt ở cấp độ instance, đạt được hiệu suất trung bình tốt hơn so với naive zero-shot và zero-shot CoT prompting. Nó cũng vượt trội hơn "Output Refinement", một phương pháp được tổng quát hóa từ "self refinement" (Madaan et al., 2023), phương pháp tinh chỉnh đầu ra tác vụ thay vì prompt đầu vào.

prompting (Kojima et al., 2022), LLMs chỉ được cung cấp một hướng dẫn chung cho tác vụ hiện tại, trong khi trong few-shot learning (Brown et al., 2020) chúng được bổ sung thêm một số cặp input-output làm minh họa tác vụ, theo sau là test input. Mặc dù nghiên cứu trước đây tập trung đáng kể vào loại sau, zero-shot prompting đang trở thành paradigm linh hoạt hơn (ví dụ, cách người dùng bình thường gửi các truy vấn tức thời đến ChatGPT (Liu et al., 2023b)), nhờ vào khả năng tổng quát hóa tác vụ tốt hơn mà chúng mang lại bằng cách tránh nhu cầu chú thích cụ thể cho từng tác vụ.

Tuy nhiên, hiệu suất của LLMs trong zero-shot prompting, đặc biệt đối với các tác vụ phức tạp như lý luận toán học và trích xuất thông tin, vẫn thua kém so với những gì đạt được bằng few-shot prompting (Wei et al., 2022a). Nó cũng cho thấy nhạy cảm với thiết kế của hướng dẫn prompt (Lu et al., 2021; Pryzant et al., 2023). Để cải thiện zero-shot prompting, Kojima et al. (2022) đã đề xuất việc

--- TRANG 2 ---
sử dụng hướng dẫn "Let's think step by step" để kích thích lý luận từ LLMs. Điều này được theo sau bởi Yang et al. (2024) tương tự đề xuất các hướng dẫn tốt hơn để tăng cường các tác vụ lý luận toán học và logic zero-shot. Tuy nhiên, như chúng tôi sẽ chỉ ra trong Phần 3.2, các hướng dẫn cấp độ tác vụ chung như vậy thiếu tính cụ thể và rõ ràng cần thiết, vì gợi ý của chúng rất tổng quát và có thể không dễ dàng để một LLM áp dụng vào instance test cụ thể. Hơn nữa, công việc gần đây cũng cho thấy rằng, khi áp dụng cho các LLMs tương đối yếu hơn như GPT-3.5, các hướng dẫn này có thể kích hoạt các phản hồi không đạo đức (Shaikh et al., 2023). Cách tối ưu hóa hướng dẫn hoặc zero-shot prompt, do đó trở thành một vấn đề quan trọng. Theo hiểu biết tốt nhất của chúng tôi, nó vẫn là một lĩnh vực nghiên cứu khá ít được khám phá.

Thừa nhận các yêu cầu đa dạng của mỗi test instance, chúng tôi ủng hộ tối ưu hóa prompt cấp độ instance, tức là viết lại prompt cho mỗi test input theo cách mà prompt được viết lại có thể kích thích tốt hơn khả năng của LLM trong việc giải quyết test instance cụ thể. Để minh họa tiềm năng của nó, chúng tôi trình bày PROMPTED (Bảng 1), bao gồm một "task LLM" thực thi test prompts trong setting zero-shot mục tiêu, và một "meta LLM", học cách viết lại lặp đi lặp lại các test prompts để có hiệu suất tốt hơn của task LLM. Đáng chú ý, việc tối ưu hóa prompt trong PROMPTED tuân theo ý tưởng mới về "(task) LLM in the loop". Nghĩa là, trong quá trình viết lại prompt, meta LLM được trình bày không chỉ với test prompt hiện tại, mà còn với đầu ra thực thi từ task LLM. Theo trực giác, điều này cho phép meta LLM đánh giá hiệu suất của task LLM và tùy chỉnh prompt được viết lại của nó để phù hợp với khả năng của nó.

PROMPTED cũng mang một sự khác biệt độc đáo từ paradigm được áp dụng rộng rãi của "Output Refinement", paradigm này tinh chỉnh lặp đi lặp lại đầu ra của task LLM (trái ngược với prompt đầu vào của nó) dựa trên phản hồi được cung cấp bởi meta LLM (Hình 1). Một thể hiện của paradigm này là "self refinement" (Madaan et al., 2023; Chen et al., 2023b), trong đó cùng một LLM được prompted để đưa ra phản hồi cho chính nó và sau đó tinh chỉnh lặp đi lặp lại đầu ra của nó. Chiến lược này, mặc dù hữu ích trong việc sửa chữa các vấn đề cục bộ (ví dụ, tính không chính xác về toán học hoặc code patches) trong đầu ra thực thi, không đưa ra các đường dẫn lý luận mới và do đó không thể giải quyết các vấn đề đáng kể hơn (ví dụ, sai lầm logic cơ bản).

Để xác thực hiệu quả của PROMPTED, chúng tôi đánh giá nó trong 13 bộ dữ liệu benchmark, chủ yếu sử dụng GPT-4 (OpenAI et al., 2024) làm cả meta và task LLMs. Kết quả của chúng tôi cho thấy PROMPTED có thể cải thiện đáng kể hiệu suất zero-shot của GPT-4 so với các baseline, bao gồm baseline mạnh của "Output Refinement", chứng minh lợi thế của việc viết lại prompt đầu vào so với tinh chỉnh đầu ra LLM. Phân tích sâu hơn của chúng tôi tiết lộ rằng PROMPTED hỗ trợ task LLM trong việc nhớ lại các sự kiện liên quan cho các tác vụ intensive knowledge, bao gồm các tác vụ cụ thể theo lĩnh vực (ví dụ, trả lời câu hỏi y khoa). Nó cũng dẫn đến các phản hồi đạo đức hơn bằng cách bao gồm các hướng dẫn phù hợp trong prompt được viết lại.

Đặc biệt đáng chú ý là khả năng của PROMPTED trong việc duy trì mức độ chính xác cao khi áp dụng cho GPT-3.5 tương đối yếu hơn. Một quan sát thú vị là, khi sử dụng GPT-3.5 làm meta LLM để viết lại prompts cho GPT-4 làm task LLM, PROMPTED mang lại hiệu suất ngang bằng hoặc thậm chí tốt hơn so với việc sử dụng GPT-4 làm meta LLM. Kết quả này chỉ ra tiềm năng của việc giám sát một LLM mạnh hơn bằng cách sử dụng một LLM yếu hơn, và do đó chúng tôi kỳ vọng công việc của chúng tôi sẽ mở đường cho nghiên cứu tương lai hướng tới việc tăng cường AI cho các tác vụ vượt quá khả năng của con người (Burns et al., 2023). Cuối cùng, các thí nghiệm của chúng tôi với Mistral (Jiang et al., 2023) và Mixtral (Jiang et al., 2024) xác nhận rằng lợi thế của PROMPTED tổng quát hóa tốt cho các LLMs mã nguồn mở và thậm chí có thể hoạt động trong setting LLM "cross-family" (ví dụ, một LLM mã nguồn mở viết lại prompts cho một LLM mã nguồn đóng).

2 PROMPTED: Cải thiện Hiệu suất Zero-Shot của LLMs với Viết lại Prompt Cấp độ Instance

2.1 Tổng quan
PROMPTED tăng cường hiệu suất LLM zero-shot bằng cách viết lại prompt của mỗi test instance theo cách thức "LLM in the loop" (Hình 1). Chúng tôi gọi LLM thực hiện tác vụ zero-shot là "task LLM" và ký hiệu chính thức là Mtask. Trong setting zero-shot (Bước 1), Mtask tạo ra đầu ra ytask cho test input x bằng cách lấy mẫu từ PMtask(ytask|e||x), trong đó e là một câu ngôn ngữ tự nhiên mô tả yêu cầu tác vụ (được gọi là "task instruction"), và e||x biểu thị sự nối tiếp của task instruction và test input. Trong tài liệu, sự nối tiếp này cũng được gọi là "prompt" cho zero-shot LLM, và chúng tôi ký hiệu nó là ρ.

Tác vụ viết lại prompt nhắm đến việc học một hàm viết lại F: ρ → ρ*, sao cho prompt được viết lại ρ* có thể mang lại hiệu suất zero-shot tốt hơn

--- TRANG 3 ---
Lý do: Đầu ra không chính xác. Tuyên bố (...) phụ thuộc vào cách "productivity" được định nghĩa trong bối cảnh này.
Loại Tác vụ: QUESTION ANSWERING
Prompt Tốt hơn: Xem xét các yếu tố khác nhau góp phần vào productivity của telescope, chẳng hạn như lượng dữ liệu được thu thập, số lượng bài báo khoa học được sản xuất bằng cách sử dụng dữ liệu của nó (...)

Prompt Tốt hơnCâu trả lời là (B) No, và lý do là (...)Prompt
 Đầu ra Tác vụ
(B) No
Câu hỏi: Very Large Telescope có phải là telescope productive nhất thế giới không?
Chọn tùy chọn đúng hoặc tốt nhất có thể:
(A) Yes
(B) No
(A) Yes. Very Large Telescope là telescope productive nhất thế giới.
No. Telescope productivity được đo lường dựa trên số lượng bài báo khoa học mà một telescope tạo ra. Hubble Space Telescope là telescope productive nhất thế giới. Câu trả lời là (A) Yes. Very Large Telescope (...)

Bước 1.
Tạo Đầu ra Tác vụ Ban đầu

Bước 2.
Tạo Prompt Tốt hơn Lặp đi lặp lại

Bước 3.
Trích xuất Câu trả lời Cuối cùng

Prompts Outputs Ground TruthXem xét các yếu tố khác nhau góp phần vào productivity của telescope, chẳng hạn như lượng dữ liệu được thu thập, số lượng bài báo khoa học được sản xuất bằng cách sử dụng dữ liệu của nó (...)
Cung cấp câu trả lời của bạn theo định dạng sau: "The answer is [YOUR_ANSWER]" (Prompt Tác vụ Ban đầu)

Đầu ra Tác vụTrong thế giới thiên văn học, productivity của telescope là một khái niệm đa diện. Nó bao gồm các yếu tố khác nhau như khối lượng dữ liệu được thu thập bởi telescope, số lượng bài báo khoa học được sản xuất bằng cách sử dụng dữ liệu của nó, và đóng góp của nó cho các khám phá thiên văn quan trọng. Xem xét các tham số này, (...) Sau khi chọn câu trả lời của bạn, vui lòng cung cấp giải thích chi tiết (...) Cung cấp câu trả lời của bạn theo định dạng sau: "The answer is [YOUR_ANSWER], and the reason is [YOUR_REASONING]"

Hình 1: Tổng quan về PROMPTED, prompting lặp đi lặp lại zero-shot task LLM để tạo ra đầu ra và sau đó tận dụng meta LLM riêng biệt để viết lại prompt đầu vào dựa trên đầu ra tác vụ hiện tại. Câu trả lời cuối cùng được trích xuất từ đầu ra tác vụ mới nhất khi meta LLM coi prompt hiện tại đủ tốt.

với Mtask. Để đạt được điều này, PROMPTED giới thiệu một LLM khác, được gọi là "meta LLM" và ký hiệu là Mmeta, tinh chỉnh test prompt dựa trên đầu ra hiện tại của Mtask (Bước 2). Quá trình này có thể lặp lại cho đến khi Mmeta coi prompt mới nhất là tốt (Bước 3). Ở mức độ cao, PROMPTED tương phản với các phương pháp hiện có như Output Refinement, tinh chỉnh đầu ra tác vụ của Mtask thay vì cải thiện prompt đầu vào cho Mtask. Như chúng tôi sẽ chỉ ra trong các thí nghiệm, công thức độc đáo này cho phép chúng tôi dễ dàng tích hợp kiến thức lĩnh vực và gợi ý cụ thể cho instance để tăng cường hiệu suất của Mtask.

2.2 PROMPTED
Dưới đây, chúng tôi mô tả chính thức từng bước trong PROMPTED.

Bước 1: Tạo Đầu ra Tác vụ Ban đầu. Cho một prompt ban đầu ρ0, Mtask đầu tiên tạo ra đầu ra ban đầu y0task như sau:
y0task = Mtask(ρ0)

Điều này trình bày zero-shot prompting điển hình. Tổng quát hơn, chúng tôi ký hiệu việc tạo đầu ra của Mtask tại iteration i là yitask = Mtask(ρi), trong đó ρi là prompt tại iteration viết lại thứ i.

Bước 2: Tạo Prompt Tốt hơn Lặp đi lặp lại. Cho một prompt đầu vào ρi và đầu ra tương ứng yitask, PROMPTED sử dụng Mmeta để cải thiện prompt ρi thành một prompt tốt hơn, ρi+1. Chính thức, chúng tôi mô tả quá trình này như sau:
yimeta = Mmeta(ρmeta∥ρi∥yitask)

Đáng chú ý, trong khi task LLM Mtask hoạt động trong zero-shot, meta LLM Mmeta được hướng dẫn với các exemplar few-shot chứng minh cách cải thiện prompt dựa trên đầu ra tác vụ hiện tại. Chúng tôi ký hiệu tập hợp các demonstration few-shot viết lại là ρmeta và sẽ giới thiệu việc hình thành và thu thập của nó trong Phần 2.3. Tuy nhiên, chúng tôi cũng lưu ý rằng tập hợp demonstration few-shot này của Mtask là task-agnostic, tức là chúng tôi đã thiết kế ρmeta để đủ chung để có thể viết lại prompts cho bất kỳ tác vụ nào.

Đầu ra của Mmeta, ký hiệu là yimeta, bao gồm ba thành phần: một câu mô tả lý do tại sao ρi có thể được cải thiện (ký hiệu là ri), một cụm từ ngắn chỉ ra loại tác vụ (ký hiệu là ti), và prompt được viết lại ρi+1. Phù hợp với nghiên cứu trước đây (ví dụ, chain-of-thought (Wei et al., 2022b)), chúng tôi phát hiện rằng việc hướng dẫn Mmeta elaborated về quá trình viết lại prompt của nó, dẫn đến chất lượng prompt tốt hơn. Cụ thể, trường lý do ri kích thích Mmeta xác minh yitask với prompt tác vụ hiện tại ρi và thảo luận về bất kỳ vấn đề tiềm ẩn nào trong ρi có thể dẫn đến đầu ra tác vụ không chính xác. Mặt khác, loại tác vụ ti ngầm hướng dẫn Mmeta phân loại test instance thành một loại tác vụ nhất định, có thể truyền cảm hứng cho Mmeta bao gồm các gợi ý cụ thể cho tác vụ được nhắm mục tiêu trong prompt tốt hơn (chẳng hạn như tạo nội dung có thể hưởng lợi từ hướng dẫn role-playing hơn là gợi ý về tính toán toán học). Cùng nhau, elaboration lý do và phân loại loại tác vụ thúc đẩy Mmeta cung cấp prompt ρi+1 có thể giải quyết các vấn đề đã xác định và kích thích các khả năng yêu cầu tác vụ từ Mtask.

PROMPTED xen kẽ giữa việc tạo đầu ra tác vụ bằng Mtask (như trong Bước 1) và viết lại prompt bằng Mmeta (Bước 2), cho đến khi Mmeta coi đầu ra tác vụ mới nhất là chính xác (được đánh giá bằng cách tìm kiếm cụm từ template "output is correct"; xem Phần 2.3 để biết chi tiết), khi nó không sửa đổi prompt nữa, hoặc khi iteration tăng lên đến một lượng tối đa được chỉ định. Việc tinh chỉnh lặp đi lặp lại này cho phép PROMPTED học từ và sửa chữa lỗi trong quá khứ, tăng cường hiệu quả của prompt một cách tiến bộ.

Chúng tôi coi prompt cuối cùng (ρi*) là prompt tối ưu ρ*. Do bản chất của "(task) LLM in the loop", đầu vào mới nhất cho Mmeta đã bao gồm đầu ra tác vụ cuối cùng (yi*task), sẽ được chuyển đến Bước 3 để trích xuất câu trả lời.

Bước 3: Trích xuất Câu trả lời Cuối cùng. Để trích xuất câu trả lời cuối cùng từ yi*task, chúng tôi theo Kojima et al. (2022) để trích xuất đầu ra zero-shot khi thuật toán kết thúc tại i = 0. Nếu không, chúng tôi hard match và trích xuất các phản hồi theo định dạng "The answer is [YOUR_ANSWER]" cụ thể cho đầu ra có cấu trúc của PROMPTED.

2.3 Dataset của Few-Shot Demonstrations để Viết lại Prompt
Như đã elaborated, Mmeta tuân theo công thức few-shot in-context learning, sao cho nó học từ các demonstration few-shot về những gì được coi là prompt tốt hơn và có thể tổng quát hóa insight cho test instances cho bất kỳ tác vụ nào. Để đạt được điều này, chúng tôi chuẩn bị meta prompt ρmeta như một concatenation của các tuples ⟨ρ, ytask, r, t, ρ*⟩. Một nguyên tắc chính nằm trong việc thiết kế lý do r đủ cụ thể (tức là xác định các vấn đề cụ thể trong prompt ban đầu ρ và đầu ra tác vụ ytask), hoàn chỉnh (tức là xác định một tập hợp đầy đủ các vấn đề có thể), và rõ ràng (tức là sử dụng ngôn ngữ rõ ràng để kích thích giải thích ổn định từ task LLM). Một ví dụ được trình bày trong Bảng 3, nơi cụm từ "hiding a body" được đánh dấu (specific) cùng với bốn lý do khác nhau (complete) có thể dẫn đến một nỗ lực jail-breaking.

Chúng tôi đề xuất tận dụng sức mạnh tạo sinh của GPT-4 để chuẩn bị các demonstration viết lại prompt này. Do thiết kế của "(task) LLM in the loop", chúng tôi chuẩn bị một tập hợp demonstration cho mỗi Mtask. Cụ thể, đối với prompt ban đầu ρ (được xác nhận mang lại đầu ra tác vụ không chính xác bằng Mtask), chúng tôi trình bày nhãn đầu ra ground-truth cho ChatGPT và prompt nó tạo ra r cho đầu ra không chính xác và một prompt mới ρ* giải quyết các vấn đề có thể được đề cập trong r. Chúng tôi xác minh thủ công đầu ra cho ρ* mới và lặp lại quá trình cho đến khi đầu ra chính xác có thể được thu được bởi Mtask.

--- TRANG 4 ---
Khi việc viết lại prompt kéo dài qua nhiều lượt, chúng tôi yêu cầu ChatGPT tóm tắt tất cả các lý do có thể vào cuối. Chúng tôi cũng cố ý bao gồm một template "output is correct" trong ρ* để báo hiệu việc dừng viết lại prompt, và một hướng dẫn "The answer is [YOUR_ANSWER]" yêu cầu Mtask định dạng câu trả lời của nó theo cách có cấu trúc để trích xuất câu trả lời dễ dàng hơn. Chi tiết thêm với phân tích chi phí được bao gồm trong Phụ lục A.1.

Vì chúng tôi nhắm đến một Mmeta chung có thể viết lại prompts cho bất kỳ tác vụ nào, việc bao gồm các tác vụ đại diện nhất trong demonstration set ρmeta là rất quan trọng. Trong implementation của chúng tôi, chúng tôi đã chọn tổng cộng 16 ví dụ từ 10 bộ dữ liệu, bao gồm các loại tác vụ từ lý luận toán học đến trích xuất thông tin cụ thể theo lĩnh vực. Để thừa nhận các khía cạnh đạo đức của đầu ra LLM và để kích thích các phản hồi phù hợp với các nguyên tắc honesty và harmlessness (Askell et al., 2021), các exemplar cho question answering, fact verification, và content generation tasks trong meta prompt được điều phối để kích thích các phản hồi honest và safe.

3 Thí nghiệm

3.1 Cài đặt Thí nghiệm
Chúng tôi tiến hành thí nghiệm trên một tập hợp đa dạng gồm 10 loại tác vụ được tóm tắt trong Bảng 2. Mỗi loại tác vụ bao gồm một hoặc hai bộ dữ liệu. Đáng chú ý, một số loại tác vụ và bộ dữ liệu đã được sử dụng trong các demonstration few-shot của Mmeta, và chúng tôi bao gồm các bộ dữ liệu chưa thấy và các loại tác vụ chưa thấy để đánh giá xem PROMPTED có thể tổng quát hóa vượt ra ngoài các loại tác vụ và bộ dữ liệu được tiếp xúc với Mmeta hay không. Đối với mỗi bộ dữ liệu, chúng tôi ngẫu nhiên chọn 250 mẫu² để đánh giá. Mỗi tác vụ được đánh giá bằng cách sử dụng metric tiêu chuẩn riêng của nó. Các thí nghiệm chính của chúng tôi được thực hiện bằng GPT-4 (phiên bản "gpt-4" cho Mtask và "gpt-4-32k-0613" cho Mmeta). Chúng tôi chạy tối đa 3 iterations cho PROMPTED, mặc dù trong thực tế nó chỉ cần 2.07 iterations trung bình. Các tham số temperature và top_k được đặt thành 0.7. Trong Phần 3.5-3.6, chúng tôi cũng đánh giá PROMPTED trên GPT-3.5 (phiên bản "gpt-35-turbo-1106"), Mistral-7B (phiên bản "Mistral-7B-Instruct-v0.2") và Mixtral 8x7B (phiên bản "Mixtral-8x7B-Instruct-v0.1").

²Ngoại trừ MATH, ToxicChats, và Penguins. Đối với MATH chúng tôi theo Lightman et al. (2023) và ngẫu nhiên lấy mẫu mười instances từ năm danh mục khó khăn qua 7 tiểu danh mục dẫn đến 350 mẫu; Penguins có 167 mẫu tổng cộng. Đối với ToxicChats chúng tôi lấy mẫu 50 instances do không có sẵn automated metrics.

--- TRANG 5 ---
[Bảng 2 được duy trì với cấu trúc gốc nhưng dịch sang tiếng Việt]

Loại Tác vụ | Dataset | Zero-Shot | Zero-Shot CoT | Output Refinement | PROMPTED
---|---|---|---|---|---
**Các Loại Tác vụ Đã thấy và Datasets Đã thấy/Chưa thấy**
Lý luận Toán học | GSM8K (Cobbe et al., 2021) ✓ | 92.400 | 93.600 | 94.000 | 94.400
 | MATH (Hendrycks et al., 2021) ✗ | 48.857 | 56.571 | 57.143 | 61.143
Tạo Code | HumanEval (Chen et al., 2021) ✗ | 67.000 | 73.460 | 74.585 | 78.659
Lý luận Logic | Logical Deductions (Suzgun et al., 2022) ✓ | 34.500 | 58.900 | 66.400 | 75.600
 | Penguins (Suzgun et al., 2022) ✗ | 59.286 | 62.143 | 72.734 | 69.434
Tác vụ Thông tin Cụ thể theo Lĩnh vực | MedQA (Jin et al., 2020) ✓ | 86.800 | 88.800 | 90.400 | 92.800
 | CyNER (Alam et al., 2022) ✗ | 38.910 | 39.690 | 63.770 | 73.070
Xác minh Sự thật | FEVER (Aly et al., 2021) ✓ | 78.800 | 86.800 | 87.600 | 89.200
Trả lời Câu hỏi Miền mở | StrategyQA (Geva et al., 2021) ✗ | 72.000 | 71.600 | 68.000 | 74.000
Tạo Nội dung + Harmlessness | ToxicChats (Lin et al., 2023) ✗ | 24.000 | 48.000 | 68.000 | 80.000
**Các Loại Tác vụ Chưa thấy**
Đọc hiểu Cụ thể theo Lĩnh vực | MMLU (PM) (Hendrycks et al., 2021) ✗ | 87.200 | 88.800 | 68.800 | 91.200
Lý luận Hình ảnh | Geometric Shapes (Suzgun et al., 2022) ✗ | 54.400 | 54.400 | 52.800 | 55.200
Lý luận Symbolic | LastLetterConcat (Kojima et al., 2022) ✗ | 3.200 | 90.400 | 50.800 | 58.200
**Trung bình** | | **57.489** | **70.243** | **70.849** | **76.424**

Bảng 2: Hiệu suất prompting trên tất cả 10 loại tác vụ. PROMPTED vượt trội hơn các baseline trong 11 trong số 13 bộ dữ liệu, chỉ có Zero-Shot CoT và Output Refinement vượt qua trong LastLetterConcat và Penguins, tương ứng. Trung bình, độ chính xác của PROMPTED vượt qua các phương pháp khác ít nhất 6%. Các bộ dữ liệu được tích hợp vào meta prompts được chỉ ra với biểu tượng ✓, trong khi những bộ không được bao gồm được đánh dấu với ✗ để rõ ràng.

Chúng tôi so sánh phương pháp của chúng tôi với hai baseline, vanilla Zero-Shot và Zero-Shot CoT tiên tiến hơn. Ngoài ra, chúng tôi cũng so sánh với Output Refinement, một phương pháp được tổng quát hóa từ "self refinement" (Madaan et al., 2023) tinh chỉnh đầu ra của task LLM thay vì prompt đầu vào của nó. Chúng tôi mô tả chi tiết trong Phụ lục B.

3.2 Kết quả Thí nghiệm Chính
Bảng 2 minh họa hiệu suất. Chúng tôi đưa ra các quan sát sau:

**Hiệu quả của PROMPTED trong Hiệu suất Zero-Shot.** PROMPTED tăng cường đáng kể hiệu suất LLM zero-shot. Đáng chú ý, trong các tác vụ lý luận logic và symbolic, nó đạt được cải thiện tuyệt đối hơn 11-50% và ~20% trung bình. Điều này có thể được quy cho các prompt được viết lại của PROMPTED, được làm phong phú với kiến thức lĩnh vực hoặc thực tế. Sự tăng cường như vậy chứng minh vô giá trong các tác vụ mà GPT-4 ban đầu gặp khó khăn, phản ánh việc tích hợp chiến lược các gợi ý tinh tế và hướng dẫn giải pháp bởi PROMPTED.

**Tối ưu hóa prompts ở cấp độ tác vụ có thể không luôn giúp ích và có thể khuyến khích các phản hồi có hại.** So với tối ưu hóa cấp độ tác vụ như zero-shot CoT, PROMPTED thể hiện hiệu suất vượt trội với cải thiện trung bình 6%. Mặc dù zero-shot CoT có thể tăng cường hiệu suất trong các tác vụ lý luận, nó vẫn có thể tạo ra các phản hồi bịa đặt (ví dụ, khi giải quyết StrategyQA, và trên ToxicChats, mặc dù tốt hơn nhiều so với vanilla zero-shot), phù hợp với phát hiện của Shaikh et al. (2023). Hướng dẫn contextual của PROMPTED đảm bảo đầu ra an toàn và có căn cứ hơn với các phản hồi hợp lý và có thể hiểu được, đặc biệt trong lý luận logic nơi nó mang lại một chuỗi suy nghĩ mạch lạc và toàn diện hơn.

**Tinh chỉnh đầu ra có thể không dẫn đến cải thiện hiệu suất.** PROMPTED vượt trội hơn Output Refinement trên hầu như tất cả các bộ dữ liệu, cho thấy lợi thế của việc viết lại prompts so với tinh chỉnh đầu ra tác vụ. Cụ thể, Output Refinement hoạt động kém trên StrategyQA (một tác vụ QA đa lựa chọn). Chúng tôi phát hiện rằng nó thường dẫn đến các phản hồi được bịa đặt với lựa chọn "C" không tồn tại trong bộ dữ liệu. Trên một số bộ dữ liệu khác, nó cũng cho thấy hiểu biết tác vụ kém (ví dụ, tạo ra câu phản hồi "There is no error in the code" cho các tác vụ không phải tạo code). Những hiện tượng này, cùng với hiệu suất tệ hơn so với PROMPTED, được gây ra bởi baseline này theo thiết kế không khuyến khích chuỗi suy nghĩ chính và chỉ tập trung vào việc tinh chỉnh đầu ra cục bộ. Ngoại lệ duy nhất

--- TRANG 6 ---
xảy ra với Penguins, một bộ dữ liệu lý luận logic chứa các truy vấn về chi tiết động vật từ một bảng hoặc tập hợp các bảng đã cho. PROMPTED thất bại trong các trường hợp khi Mtask không thể tuân theo các prompt tốt hơn được tạo ra bởi Mmeta, hoặc khi Mmeta đơn giản hóa quá mức problem statement.

3.3 PROMPTED Tổng quát hóa qua các Lĩnh vực và Loại Tác vụ
Chúng tôi đánh giá hai loại khả năng tổng quát hóa của PROMPTED: (1) tổng quát hóa lĩnh vực, trong đó chúng tôi đánh giá xem PROMPTED có thể hoạt động tốt trên các tác vụ cụ thể theo lĩnh vực hay không, bao gồm các lĩnh vực nó đã hoặc chưa thấy trong meta prompt ρmeta, và (2) tổng quát hóa loại tác vụ, tức là tổng quát hóa đến các loại tác vụ chưa thấy bởi PROMPTED (hoặc Mmeta của nó).

Đối với tổng quát hóa lĩnh vực, chúng tôi phân tích hiệu suất của PROMPTED trên MedQA như một lĩnh vực đã thấy (Biomedical), và trên CyNER và MMLU (PM) như các lĩnh vực chưa thấy (Cybersecurity và Medicine). Kết quả trong Bảng 2 chứng minh sự vượt trội của PROMPTED so với tất cả baseline. Đặc biệt trên CyNER, một tác vụ nhận dạng thực thể có tên lĩnh vực cybersecurity, PROMPTED vượt trội hơn baseline 10-35% tuyệt đối. Như được hiển thị trong Bảng 3, điều này là do khả năng của PROMPTED trong việc thêm các chi tiết cụ thể theo lĩnh vực phong phú hơn (chẳng hạn như định nghĩa của các khái niệm cybersecurity) và hướng dẫn có cấu trúc vào các prompt. Mặc dù Output Refinement cũng cố gắng tiêm kiến thức lĩnh vực, như chúng tôi đã thảo luận, nó có thể đưa ra các phản hồi được bịa đặt.

Đối với tổng quát hóa loại tác vụ, chúng tôi đánh giá PROMPTED trên LastLetterConcat (lý luận symbolic), MMLU (PM) (đọc hiểu cụ thể theo lĩnh vực), và Geometric Shapes (lý luận hình ảnh). PROMPTED thể hiện tổng quát hóa mạnh mẽ trên Geometric Shapes và MMLU(PM), vượt trội hơn baseline trong các loại tác vụ mới này 1-23%. Tuy nhiên, nó gặp khó khăn với LastLetterConcat, một tác vụ lý luận symbolic của việc nối các chữ cái cuối của một chuỗi từ. Thú vị là, Zero-Shot CoT đạt được hiệu suất tốt nhất trên tác vụ này, trong khi cả PROMPTED và baseline Output Refinement đều thất bại với biên độ lớn. Chúng tôi quan sát rằng các meta LLM cho cả hai phương pháp đều không hiệu quả trong việc đánh giá tính chính xác của đầu ra được tạo ra bởi Mtask. Ví dụ, trong khi nối các ký tự cuối trong "Ulises Derek Adrianna Eugene", cả hai phương pháp đều coi đầu ra "skeene" là chính xác. Điều này ngụ ý một điểm yếu nội tại của LLMs trong việc hiểu các hoạt động symbolic, mà chúng tôi để lại như một chủ đề nghiên cứu tương lai.

3.4 PROMPTED Khuyến khích Phản hồi Harmless và Honesty
Khi đánh giá PROMPTED trên ToxicChats, chúng tôi quan sát rằng PROMPTED có thể xử lý tốt hơn các truy vấn có hại (vượt trội hơn baseline 12-56%), bao gồm những truy vấn được che giấu bằng các kỹ thuật như Jail-Breaking, Prompt Injection, hoặc Role Playing. Điều này có thể được quy cho nguyên tắc thiết kế của PROMPTED là "Mtask in the loop", tức là bằng cách nhìn vào đầu ra của Mtask, nó đánh giá tính có hại của prompt ban đầu và viết lại nó để chặn bất kỳ phản hồi không đạo đức nào (Bảng 3). Mặt khác, PROMPTED cũng viết lại các truy vấn có vẻ có hại với nhiều hướng dẫn và gợi ý hơn. Những truy vấn này trong prompt ban đầu của chúng thường bị từ chối bởi Mtask do sự thận trọng quá mức. Prompt được viết lại bởi PROMPTED vượt qua điều này và cuối cùng có thể thu thập các phản hồi có ý nghĩa từ Mtask. Tuy nhiên, chúng tôi phát hiện rằng PROMPTED vẫn gặp khó khăn với các truy vấn có hại được hình thành như Role Playing. Đối với những prompt khéo léo như vậy, Mmeta có thể coi hiệu suất Mtask mạch lạc với hướng dẫn tác vụ và do đó tạo ra lý do "Output is correct. The AI model correctly adhered to the given character's traits and (...)". Chúng tôi để lại việc khám phá này như công việc tương lai.

Ngoài ra, mặc dù chỉ bao gồm hai ví dụ điều phối các phản hồi honest (tức là thừa nhận thiếu kiến thức hoặc khả năng (Shen et al., 2023)) trong các tác vụ fact verification và question answering, PROMPTED viết lại các prompt một cách rõ ràng khuyến khích honesty trong các tác vụ khác nhau, bao gồm lý luận toán học. Ví dụ, trong Bảng 3 chúng tôi trình bày một prompt được viết lại sử dụng ngôn ngữ "Please avoid stating the answer with absolute certainty unless you are drawing from a verified and definitive source". Nghiên cứu tương lai có thể thực hiện một cuộc điều tra có hệ thống hơn về khía cạnh honesty của PROMPTED.

3.5 PROMPTED với GPT-3.5 như Meta LLM
Chúng tôi tiếp tục tiến hành thí nghiệm PROMPTED sử dụng các LLMs tương đối yếu hơn (GPT-3.5; kết quả bổ sung với Mistral và Mixtral trong Phần 3.6). Do hạn chế về tài nguyên, các thí nghiệm được thực hiện trên bốn bộ dữ liệu (tức là StrategyQA, ToxicChat, MATH, và MMLU (PM)). Những bộ dữ liệu này được chọn vì tính đa dạng trong các loại tác vụ và độ khó (Phụ lục A.2). Chúng tôi khám phá các câu hỏi nghiên cứu sau:

**(RQ1) PROMPTED sẽ hoạt động như thế nào với các LLMs yếu hơn như GPT-3.5?** Chúng tôi trình bày kết quả khi sử dụng GPT-3.5 làm cả Mtask và Mmeta trong Hình 2. Chúng tôi quan sát rằng PROMPTED với backbone LLM yếu hơn vượt trội hơn baseline với cùng cấu hình 5% trung bình. Điều này gợi ý rằng GPT-3.5 yếu hơn cũng có thể kích thích gợi ý và insights cụ thể theo lĩnh vực để tăng cường hiệu suất. PROMPTED đạt được lợi ích hiệu suất lớn nhất trên ToxicChats nơi, như được chứng minh bởi kết quả, các LLMs yếu hơn có thể dễ dàng bị lừa bởi các prompt khéo léo và độc hại.

**(RQ2) Một LLM yếu hơn có thể đóng vai trò Mmeta để giám sát một Mtask mạnh hơn không?** Nghiên cứu gần đây đã giả thuyết rằng "evaluation" thường là một tác vụ dễ dàng hơn "generation" (Leike, 2022). Cho rằng khả năng quan trọng của Mmeta của chúng tôi là có thể đánh giá đầu ra tác vụ hiện tại so với prompt, chúng tôi tự hỏi: liệu có khả thi để sử dụng một LLM tương đối yếu hơn (ví dụ, GPT-3.5) làm Mmeta để viết lại prompts cho GPT-4 làm Mtask không? Kết quả của chúng tôi được trình bày trong Hình 2. Thú vị là, chúng tôi quan sát rằng GPT-3.5 làm Mmeta vượt trội đáng kể so với vanilla zero-shot và zero-shot CoT dựa trên GPT-

--- TRANG 7 ---
4. Thậm chí thú vị hơn, hiệu suất của nó thậm chí còn tốt hơn so với việc sử dụng GPT-4 làm Mmeta trên ba trong số bốn tác vụ được thí nghiệm. Đặc biệt trên ToxicChats, GPT-3.5 tiết lộ hành vi thận trọng hơn so với GPT-4 làm Mmeta và do đó có thể từ chối nhiều prompt độc hại hơn (bao gồm những prompt Role Playing mà GPT-4 không thể xử lý tốt). Tuy nhiên, GPT-3.5 kém trong việc đánh giá kết quả cho các tác vụ lý luận toán học phức tạp, dẫn đến hiệu suất hơi tệ hơn so với GPT-4 làm Mmeta trên MATH. Chúng tôi bao gồm một so sánh các prompt được viết lại bởi GPT-3.5 và GPT-4 trong Phụ lục E.

3.6 PROMPTED với Open-Source LLMs
Để hiểu xem các insights chúng tôi thu thập từ các thí nghiệm với GPT-4 và GPT-3.5 có tổng quát hóa đến các LLMs mã nguồn mở hay không, chúng tôi đã tiến hành một tập hợp thí nghiệm mới sử dụng Mistral 7B và Mixtral 8x7B. Kết quả được trình bày trong Bảng 4.

**(RQ3) PROMPTED có hoạt động nhất quán với các LLMs mã nguồn mở không?** Trước tiên chúng tôi nhìn vào hiệu suất của PROMPTED trong setting so sánh với thí nghiệm chính của chúng tôi, tức là khi cùng một LLM được sử dụng để viết lại prompts cho chính nó như một task LLM. Trong thí nghiệm của chúng tôi, chúng tôi đã thử nghiệm PROMPTED khi Mmeta và Mtask đều là Mixtral. Như được diễn tả trong Hàng 1 và Hàng 3 của Bảng 4, PROMPTED tăng cường hiệu suất của Mixtral một cách nhất quán trên tất cả các bộ dữ liệu so với zero-shot Mixtral trung bình 19%, chỉ ra rằng quan sát của chúng tôi về PROMPTED trong các LLMs mã nguồn đóng tổng quát hóa đến các LLMs mã nguồn mở.

Chúng tôi cũng tiến hành thí nghiệm so sánh với Phần 3.5, đánh giá xem một LLM mã nguồn mở yếu hơn (Mistral) có thể viết lại prompts thành công cho một LLM mã nguồn mở mạnh hơn (Mixtral) hay không. Kết quả của chúng tôi trên Hàng 4 cho thấy Mistral-7B có thể giám sát Mixtral-8X7B mạnh hơn vượt trội hơn zero-shot Mixtral-8X7B 15%.

Thú vị là, đối với ToxicChats, chúng tôi quan sát rằng PROMPTED với Mixtral vượt trội hơn PROMPTED khi sử dụng GPT-3.5 hoặc GPT-4 làm backbone Mtask với biên độ lớn. Qua tất cả các thí nghiệm, chúng tôi quan sát rằng cả Mistral và Mixtral đều tốt hơn các LLMs dựa trên GPT trong việc xác định các cuộc tấn công role-playing, điều mà như chúng tôi đã thảo luận trong Phần 3.4 là một điểm yếu chính của các meta LLMs dựa trên GPT.

**(RQ4) Các LLMs cross-family có thể giám sát với PROMPTED không?** Một câu hỏi thú vị ở đây là, liệu một LLM mã nguồn mở có thể viết lại prompts cho một LLM mã nguồn đóng không? Nếu có, điều này có thể mang lại nhiều lợi ích như tiết kiệm chi phí tiền tệ của việc gọi API (Chen et al., 2023a; Yue et al., 2024). Để trả lời câu hỏi này, chúng tôi đã tiến hành thí nghiệm với Mixtral hoặc Mistral mã nguồn mở làm Mmeta và GPT-3.5 làm Mtask. Kết quả trong Bảng 4 chỉ ra rằng các meta LLMs cross-family cải thiện so với zero-shot GPT-3.5 ít nhất 16%.

Cụ thể, trong Hàng 5 của Bảng 4 khi sử dụng Mixtral làm Mmeta để giám sát GPT-3.5, PROMPTED dẫn đến hiệu suất so sánh được trên bộ dữ liệu MATH và hiệu suất được cải thiện trên tất cả các bộ dữ liệu khác so với zero-shot GPT-3.5. Đáng chú ý, đối với ToxicChats, chúng tôi quan sát Mixtral thành công trong việc giải quyết các prompt role-playing mà không được xử lý bởi cả GPT-3.5 và GPT-4 làm Mmeta. Hơn nữa, giống với các quan sát của chúng tôi trong Phần 3.2, Mixtral làm Mmeta giúp giảm việc bịa đặt các phản hồi cho StrategyQA và thêm các gợi ý cụ thể theo lĩnh vực. Tổng thể, chúng tôi quan sát một cải thiện hơn 18% khi so sánh với GPT-3.5 trong setting này.

Tương tự, việc sử dụng Mistral, một LLM yếu hơn nhiều so với Mixtral, dẫn đến lợi ích hiệu suất tương tự hơn 16% trung bình so với zero-shot GPT-3.5. Cụ thể, trên ToxicChats và MMLU (PM) chúng tôi được ít nhất 6%, và hiệu suất so sánh được trên bộ dữ liệu MATH. Tuy nhiên, nó không thể cải thiện hiệu suất của GPT-3.5 trên StrategyQA. Khi kiểm tra, chúng tôi phát hiện rằng Mistral đã thay đổi ý định của người dùng bằng cách sửa đổi các tùy chọn Yes/No mặc định thành các tùy chọn True/False trong tổng cộng 19 instances trong số 250 mẫu test. Do lý do này, việc đánh giá độ chính xác tự động dựa trên việc đánh giá chỉ Yes/No đã coi những trường hợp này là dự đoán sai, mặc dù GPT-3.5 đã chọn đúng từ True/False cho tất cả những trường hợp này.

3.7 Nghiên cứu Ablation
Cuối cùng, chúng tôi tiến hành một nghiên cứu ablation để xác thực sự cần thiết của "Mtask in the loop" trong quá trình viết lại prompt. Để đạt được điều này, chúng tôi đã chuẩn bị một tập hợp meta prompts ρmeta mới không bao gồm đầu ra tác vụ hiện tại yitask. Chúng tôi bao gồm chi tiết của setting này trong Phụ lục C. Kết quả của chúng tôi trong Hình 3 cho thấy rằng việc bao gồm Mtask là cần thiết cho việc viết lại prompt tốt hơn (lợi ích hiệu suất 3-4%). Thú vị là, khi sử dụng GPT-3.5 làm Mmeta, lợi thế của việc viết lại prompt chỉ được kích hoạt khi Mtask được bao gồm trong vòng lặp.

4 Công trình Liên quan
**LLMs trong Zero-Shot** Để giảm nỗ lực thủ công trong việc thiết kế các demonstration cụ thể cho tác vụ, các công trình gần đây đã được thúc đẩy để điều tra zero-shot LLM prompting và cho thấy hiệu quả của nó trong lý luận (Wei et al., 2022b; Kojima et al., 2022; Wang et al., 2022), question-answering (Kannan et al., 2023), phân loại văn bản (Wang et al., 2023c), tạo kế hoạch hành động hướng mục tiêu (Wang et al., 2023a), tạo ngôn ngữ tự nhiên (Axelsson và Skantze, 2023), trích xuất thông tin (Wei et al., 2023), v.v. Zhang et al. (2023) chứng minh rằng các LLMs như GPT-3 (Brown et al., 2020), mặc dù được hiển thị để thực hiện few-shot learning khá tốt, không rất thành công trong zero-shot in-context learning. Để cải thiện khả năng lý luận zero-shot của LLMs, Kojima et al. (2022) đề xuất Zero-Shot Chain-of-Thought (Wei et al., 2022b, CoT). Tuy nhiên, việc sử dụng cụm từ trigger như vậy có thể khuyến khích các phản hồi có hại (Shaikh et al., 2023). Công việc của chúng tôi đóng góp cho lĩnh vực này bằng cách nghiên cứu các phương pháp tối ưu hóa task prompts trong setting zero-shot. Nó khác với công việc trước đây trong việc tối ưu hóa prompts cho từng instances cá biệt với task LLM trong vòng lặp. Phương pháp PROMPTED của chúng tôi được chỉ ra để vượt trội hơn vanilla zero-shot hoặc zero-shot CoT.

**Viết lại và Tối ưu hóa Prompt** Các công trình trước đây đã nhắm đến tối ưu hóa prompts cho LLMs thông qua viết lại thủ công (Reynolds và McDonell, 2021) hoặc tuning dựa trên gradient (Liu et al., 2023a). Gần đây, Bsharat et al. (2024) đề xuất 26 nguyên tắc hướng dẫn được thiết kế để hợp lý hóa quá trình truy vấn và prompting các mô hình ngôn ngữ lớn. Tuy nhiên, việc áp dụng những nguyên tắc này trong cuộc sống thực vẫn có thể đòi hỏi thử và sai và không thân thiện với người dùng không có đủ chuyên môn. Tương tự như công việc của chúng tôi, Gao et al. (2021); Jiang et al. (2020); Yuan et al. (2021); Prasad et al. (2022); Jiang et al. (2020); Honovich et al. (2022); Zhou et al. (2022); Wang et al. (2023b); Yang et al. (2024) cũng đã nghiên cứu tối ưu hóa prompt; tuy nhiên, các phương pháp của họ giả định setting few-shot, trong khi chúng tôi tập trung vào zero-shot. Cuối cùng, Madaan et al. (2023); Chen et al. (2023b) đề xuất một phương pháp thay thế, tối ưu hóa đầu ra tác vụ thay vì prompt đầu vào. Chúng tôi cho thấy rằng phương pháp này kém hiệu quả hơn việc viết lại prompt, vì cái sau có thể dễ dàng hướng dẫn các đường dẫn lý luận của một LLM hơn.

5 Kết luận
Trong bài báo này, chúng tôi đã đề xuất một tác vụ mới về viết lại prompt với (task) LLM-in-the-loop ở cấp độ instance để cải thiện khả năng zero-shot của LLMs. Chúng tôi cho thấy rằng tối ưu hóa ở cấp độ instance hỗ trợ trong việc tạo ra các gợi ý cụ thể cho tác vụ, tạo ra kiến thức lĩnh vực, và khuyến khích các phản hồi harmless và honest. Thú vị là, chúng tôi cũng cho thấy rằng GPT-3.5 yếu hơn có thể viết lại prompts cho GPT-4 mạnh hơn, điều này cho thấy tiềm năng lớn cho PROMPTED được sử dụng để giám sát. Cuối cùng, các thí nghiệm của chúng tôi sử dụng các LLMs mã nguồn mở, bao gồm Mistral và Mixtral, xác nhận khả năng tổng quát hóa của các lợi thế của PROMPTED.

--- TRANG 10 ---
Hạn chế
Chúng tôi trình bày PROMPTED, một phương pháp tối ưu hóa prompt tăng cường hiệu suất LLM zero-shot. Chúng tôi cho thấy rằng tối ưu hóa ở cấp độ instance có thể hỗ trợ trong việc tạo ra các gợi ý cụ thể cho tác vụ và kiến thức lĩnh vực. Tuy nhiên, chúng tôi quan sát một số hạn chế của phương pháp chúng tôi như không thể giải quyết các tác vụ lý luận symbolic, hiểu các prompt lý luận hình ảnh, và từ chối các yêu cầu cho các prompt role-playing có hại. Hơn nữa, mặc dù hiếm, chúng tôi vẫn quan sát các lỗi hallucination, và mất thông tin do đơn giản hóa quá mức hoặc bỏ qua chi tiết từ các prompt dài. Để cung cấp hiểu biết hoàn chỉnh hơn về phương pháp của chúng tôi, chúng tôi đã bao gồm phân tích lỗi và ví dụ trong Phụ lục D-E. Công việc tương lai nên tìm hiểu các cơ chế có thể ngăn chặn hallucination và mất thông tin tốt hơn và một cơ chế mạnh để xác minh đầu ra của LLMs cho các tác vụ như lý luận symbolic.

Tuyên bố Đạo đức
Chúng tôi không dự đoán bất kỳ vấn đề đạo đức nghiêm trọng nào trong quá trình phát triển và từ việc sử dụng phương pháp được đề xuất. Chúng tôi sử dụng các bộ dữ liệu hoàn toàn mã nguồn mở và sẽ mã nguồn mở kết quả và bộ dữ liệu của chúng tôi. Ngoài ra, do khả năng phù hợp với các giá trị con người, PROMPTED được chỉ ra có tiềm năng bảo vệ chống lại các prompt có hại, điều này chỉ ra tác động xã hội tích cực độc đáo của nó.

Lời cảm ơn
Dự án này được tài trợ bởi NSF SHF 2311468, GMU College of Computing and Engineering, và GMU Department of Computer Science. Dự án này cũng được hỗ trợ bởi các tài nguyên được cung cấp bởi Office of Research Computing tại GMU (URL https://orc.gmu.edu) và được tài trợ một phần bởi các khoản tài trợ từ National Science Foundation (Số Giải thưởng 1625039 và 2018631).

Tài liệu tham khảo
[Phần tài liệu tham khảo được duy trì nguyên văn như trong bản gốc nhưng tôi sẽ không dịch từng mục tài liệu tham khảo vì đây là thông tin thư mục học chuẩn]

--- TRANG 11 ---
[Tiếp tục phần tài liệu tham khảo...]

--- TRANG 12 ---
[Tiếp tục phần tài liệu tham khảo...]

--- TRANG 13 ---
[Tiếp tục phần tài liệu tham khảo...]

A Chi tiết Dataset và Phân tích Chi phí

A.1 Xây dựng Meta-Prompts cho PROMPTED
Trong phần này, chúng tôi chi tiết quy trình và các prompt được sử dụng trong việc xây dựng meta prompts.

Đầu tiên, chúng tôi thủ công chọn một tập hợp prompts từ các nguồn được chi tiết trong Bảng 5. Chúng tôi đảm bảo rằng các prompt thực sự tạo ra đầu ra không chính xác bằng Mtask và prompt ChatGPT với prompt, ground truth, và đầu ra được tạo ra với hướng dẫn tác vụ "For the following problem statement [ρ] [$-taskLLM] generated an incorrect response [yi] while the correct solution is [y*i].

[Phần còn lại của tài liệu tiếp tục với các phụ lục A.2, A.3, B, C, D, E chứa các chi tiết kỹ thuật, phân tích chi phí, ví dụ implementation, và các bảng dữ liệu mở rộng...]
