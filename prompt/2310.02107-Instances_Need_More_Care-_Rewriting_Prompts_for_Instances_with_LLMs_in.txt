# 2310.02107.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/prompt/2310.02107.pdf
# File size: 1047566 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Instances Need More Care: Rewriting Prompts for Instances with LLMs in
the Loop Yields Better Zero-Shot Performance
WARNING: This paper shows dataset samples and model outputs that are toxic in nature.
Saurabh Srivastava⋆∗, Chengyue Huang#, Weiguo Fan#, Ziyu Yao⋆*
⋆George Mason University,#University of Iowa
{ssrivas6, ziyuyao}@gmu.edu ,
{chengyue-huang, weiguo-fan}@uiowa.edu
Abstract
Large language models (LLMs) have revo-
lutionized zero-shot task performance, miti-
gating the need for task-specific annotations
while enhancing task generalizability. De-
spite its advancements, current methods using
trigger phrases such as “Let’s think step by
step” remain limited. This study introduces
PROMPT ED, an approach that optimizes the
zero-shot prompts for individual task instances
following an innovative manner of “LLMs
in the loop”. Our comprehensive evaluation
across 13 datasets and 10 task types based
on GPT-4 reveals that PROMPT EDsignifi-
cantly outperforms both the naive zero-shot
approaches and a strong baseline (i.e., “Out-
put Refinement”) which refines the task output
instead of the input prompt. Our experimen-
tal results also confirmed the generalization of
this advantage to the relatively weaker GPT-3.5.
Even more intriguingly, we found that leverag-
ing GPT-3.5 to rewrite prompts for the stronger
GPT-4 not only matches but occasionally ex-
ceeds the efficacy of using GPT-4 as the prompt
rewriter. Our research thus presents a huge
value in not only enhancing zero-shot LLM
performance but also potentially enabling su-
pervising LLMs with their weaker counterparts,
a capability attracting much interest recently.
Finally, our additional experiments confirm the
generalization of the advantages to open-source
LLMs such as Mistral 7B and Mixtral 8x7B.1
1 Introduction
The advent of large language models (LLMs) has
revolutionized the landscape of natural language
processing. These models perform downstream
tasks primarily via prompting, which can be cate-
gorized into two types, i.e., zero-shot prompting
and few-shot in-context learning. In zero-shot
*Corresponding Authors
1Source code and data are released at https://github.
com/salokr/PRoPMTed .Zero Shot Zero-Shot CoTOutput
RefinementPROMPT ED
Prompt
Output
Q: Prompt  
Output   + Let's Think Step by Step
OutputPrompt
Prompt  
Output
Avg. Accuracy
4050607080
GPT-3.5 GPT-4Zero-Shot Zero-Shot CoT Output Refinement PROMPTED
Table 1: Comparison between PROMPT EDand other
baselines. PROMPT EDemploys a meta LLM to itera-
tively refine the prompt at the instance level, achieving
better average performance than naive zero-shot and
zero-shot CoT prompting. It also outperforms “Out-
put Refinement", an approach generalized from “self
refinement” (Madaan et al., 2023), which refines the
task output rather than the input prompt.
prompting (Kojima et al., 2022), LLMs are pro-
vided with only a general instruction for the task
at hand, while in few-shot learning (Brown et al.,
2020) they are additionally supplied with several
input-output pairs as task demonstrations, followed
by the test input. While significant prior research
has focused on the latter, zero-shot prompting is be-
coming the more versatile paradigm (e.g., how or-
dinary users send ad-hoc queries to ChatGPT (Liu
et al., 2023b)), owing to the better task general-
izability they brought by eschewing the need for
task-specific annotations.
However, LLMs’ performance in zero-shot
prompting, especially for complex tasks such as
mathematical reasoning and information extrac-
tion, still lags behind that achieved with few-shot
prompting (Wei et al., 2022a). It also shows to be
sensitive to the design of the prompt instruction (Lu
et al., 2021; Pryzant et al., 2023). To improve zero-
shot prompting, Kojima et al. (2022) proposed thearXiv:2310.02107v4  [cs.CL]  11 Jun 2024

--- PAGE 2 ---
use of the instruction “Let’s think step by step” to
elicit reasoning from LLMs. This is followed by
Yang et al. (2024) which similarly proposed better
instructions to enhance zero-shot mathematical and
logical reasoning tasks. However, as we will show
in Section 3.2, such generic task-level instructions
lack the necessary specificity and clarity, since their
hint is very general and may not be easy for an
LLM to apply to the specific test instance. More-
over, recent work also showed that, when applied to
relatively weaker LLMs such as GPT-3.5, these in-
structions may trigger unethical responses (Shaikh
et al., 2023). How to optimize the instruction or the
zero-shot prompt, thus becomes a critical problem.
To the best of our knowledge, it remains a rather
underexplored field of study.
Acknowledging the diverse requirements of each
test instance, we advocate for instance-level prompt
optimization , i.e., rewriting the prompt for each
test input in a way that the rewritten prompt can
better elicit an LLM’s capability in solving the spe-
cific test instance. To illustrate its promise, we
present PROMPT ED(Table 1), which consists of
one “task LLM” that executes test prompts in the
targeted zero-shot setting, and one “meta LLM”,
which learns to iteratively rewrite the test prompts
for better performance of the task LLM. Notably,
the prompt optimization in PROMPT EDfollows
a novel idea of “(task) LLM in the loop”. That
is, during the prompt rewriting process, the meta
LLM is presented with not only the current test
prompt, but also the execution output from the task
LLM. Intuitively, this allows the meta LLM to as-
sess the task LLM’s performance and customize its
rewritten prompt to fit its capability.
PROMPT EDalso bears a unique distinction
from the widely adopted paradigm of “Output Re-
finement”, which iteratively refines the task LLM’s
output (as opposed to its input prompt) based on
the feedback provided by a meta LLM (Figure 1).
An instantiation of this paradigm is “self refine-
ment” (Madaan et al., 2023; Chen et al., 2023b),
where the same LLM is prompted to give feedback
to itself and then iteratively refine its output. This
strategy, while useful in fixing local issues (e.g.,
mathematical inaccuracies or code patches) in the
execution output, does not introduce new reason-
ing paths and thus cannot resolve more substantial
issues (e.g., fundamental logical mistakes).
To validate the effectiveness of PROMPT ED,
we evaluate it in 13 benchmark datasets, primar-
ily using GPT-4 (OpenAI et al., 2024) as both themeta and the task LLMs. Our results showed that
PROMPT EDcan significantly improve GPT-4’s
zero-shot performance compared to the baselines,
including the strong baseline of “Output Refine-
ment”, demonstrating the advantage of rewriting
the input prompt over refining the LLM output. Our
further analysis revealed that PROMPT EDaids the
task LLM in recalling relevant facts for knowledge-
intensive tasks, including domain-specific ones
(e.g., medical question answering). It also results
in more ethical responses by including proper in-
structions in the rewritten prompt.
Particularly notable is PROMPT ED’s ability to
maintain high accuracy levels when applied to the
relatively weaker GPT-3.5. An exciting observa-
tion is that, when using GPT-3.5 as the meta LLM
to rewrite prompts for GPT-4 as the task LLM,
PROMPT EDbrings on-par or even better perfor-
mance than using GPT-4 as the meta LLM. This re-
sult indicates the promise of supervising a stronger
LLM using a weaker one, and we thus expect our
work to pave the way for future research towards
enhancing AI for tasks that are beyond human ca-
pabilities (Burns et al., 2023). Finally, our experi-
ments with Mistral (Jiang et al., 2023) and Mixtral
(Jiang et al., 2024) confirmed that the advantages of
PROMPT EDgeneralize well to open-source LLMs
and can even perform in a “cross-family” LLM set-
ting (e.g., an open-source LLM rewrites prompts
for a close-source one).
2 PR OMPT ED: Improving Zero-Shot
Performance of LLMs with
Instance-Level Prompt Rewriting
2.1 Overview
PROMPT EDenhances zero-shot LLM perfor-
mance by rewriting the prompt of each test instance
in an “LLM in the loop” manner (Figure 1). We
term the LLM performing the zero-shot task as
“task LLM” and formally denote it as Mtask. In
the zero-shot setting (Step 1), Mtaskproduces an
output ytaskto a test input xby sampling from
PMtask(ytask|e||x), where eis a natural language
sentence describing the task demand (called “task
instruction”), and e||xdenotes the concatenation of
the task instruction and the test input. In literature,
this concatenation is also called a “prompt” to the
zero-shot LLM, and we denote it as ρ.
The task of prompt rewriting targets learning
a rewriting function F:ρ→ρ∗, such that the
rewritten prompt ρ∗can yield better zero-shot per-

--- PAGE 3 ---
Reason: The output is incorrect. The claim  (...) depends on
how "productivity" is defined in this context.   
Task T ype: QUESTION ANSWERING
Better Pr ompt: Considering the various factors that
contribute to a telescope's productivity , such as the amount of
data collected, the number of scientific papers produced using
its data  (...)
Better PromptThe answer is (B) No, and the reason is that (...)Prompt
 Task Output
(B) No
Question: Is the V ery Lar ge Telescope the most
productive telescope in the world?  
Choose the correct or best possible option:
(A) Y es
(B) No
(A) Y es. The V ery Lar ge Telescope the most
productive telescope in the world.
No. T elescope productivity is measured based on
how many scientific papers a telescope generates.
The Hubble Space T elescope is the most
productive telescope in the world. The answer is (A) Y es. The
Very Lar ge Telescope (...)Step1.  
Initial T ask Output GenerationStep 2.  
Iterative Better Pr ompt GenerationStep 3.  
Final Answer Extraction
Prompts Outputs Ground T ruthConsidering the various factors that contribute to a telescope's productivity ,
such as the amount of data collected, the number of scientific papers produced
using its data (...)
Provide your answer in the following format: "The answer is
[YOUR_ANSWER]"  (Initial T ask Prompt)
Task OutputIn the world of astronomy , the productivity of a telescope is a multifaceted
concept. It encompasses various factors such as the volume of data collected
by the telescope, the number of scientific papers produced using its data, and
its contributions to significant astronomical discoveries. Considering these
parameters, (...) After selecting your answer , please provide a detailed
explanation (...) Provide your answer in the following format: "The answer is
[YOUR_ANSWER], and the reason is [YOUR_REASONING] "Figure 1: Overview of PROMPT ED, which iteratively prompts the zero-shot task LLM to produce an output and
then leverages a separate meta LLM to rewrite the input prompt based on the current task output. The final answer is
extracted from the latest task output when the meta LLM considers the current prompt to be sufficiently well-written.
formance with Mtask. To this end, PROMPT ED
introduces another LLM, termed “meta LLM” and
denoted as Mmeta, which refines the test prompt
based on Mtask’s current output (Step 2). This
process can iterate until Mmeta considers the lat-
est prompt a good one (Step 3). At the high level,
PROMPT EDcontrasts with existing approaches
such as Output Refinement, which refine the task
output of Mtaskinstead of improving the input
prompt to Mtask. As we will show in experiments,
this unique formulation allows us to more easily
integrate domain knowledge and instance-specific
hints to enhance the performance of Mtask.
2.2 PR OMPT ED
Below, we formally describe each of the steps in
PROMPT ED.
Step 1: Initial Task Output Generation. Given
an initial prompt ρ0,Mtaskfirst generates the ini-
tial output y0
taskas follows:
y0
task=Mtask(ρ0)
This presents the typical zero-shot prompting.
More generally, we denote the output generation of
Mtaskat iteration iasyi
task=Mtask(ρi), where
ρiis the prompt at the i-th rewritten iteration.
Step 2: Iterative Better Prompt Generation.
Given an input prompt ρiand its corresponding
output yi
task,PROMPT EDutilizes Mmeta to im-
prove the prompt ρiinto a better one, ρi+1.
Formally, we describe this process as follows:
yi
meta =Mmeta (ρmeta∥ρi∥yi
task)
Notably, while the task LLM Mtaskworks in zero-
shot, the meta LLM Mmeta is instructed with few-shot exemplars demonstrating how to improve a
prompt based on the current task output . We de-
note the set of few-shot rewriting demonstrations as
ρmeta and will introduce its formulation and collec-
tion in Section 2.3. However, we also note that this
set of few-shot demonstrations of Mtaskistask-
agnostic , i.e., we devised ρmeta to be as generic as
to be able to rewrite prompts for anytasks.
The output of Mmeta, denoted as yi
meta, consists
of three components: a sentence describing the rea-
son why ρican be improved (denoted as ri), a short
phrase indicating the type of the task (denoted as
ti), and the rewritten prompt ρi+1. Resonating with
prior research (e.g., chain-of-thought (Wei et al.,
2022b)), we found that instructing Mmeta to elabo-
rate on its prompt rewriting process, leads to better
prompt quality. Specifically, the reason field ri
stimulates Mmeta to verify yi
taskagainst the cur-
rent task prompt ρiand discuss any potential issues
inρithat could result in the incorrect task output.
The task type ti, on the other hand, implicitly in-
structs Mmeta to classify the test instance into a
certain task type, which could inspire Mmeta to
include targeted task-specific hints in the better
prompt (such as a content generation may benefit
more from role-playing instructions than sugges-
tions on mathematical calculations). Together, the
reason elaboration and the task type categorization
motivate Mmeta to provide a prompt ρi+1that can
address the identified issues and elicit task-required
capabilities from Mtask.
PROMPT EDalternates between task output gen-
eration using Mtask (as in Step 1) and prompt
rewriting using Mmeta (Step 2), until Mmeta con-
siders the latest task output being correct (which is

--- PAGE 4 ---
judged by searching for a template phrase “output
is correct”; see Section 2.3 for details), when it
does not revise the prompt anymore, or when the
iteration increases to a specified maximum amount.
This iterative refinement allows PROMPT EDto
learn from and correct past errors, progressively
enhancing the prompt’s efficacy.
We consider the final prompt ( ρi∗) as the optimal
oneρ∗. Because of the nature of “(task) LLM in
the loop”, the latest input to Mmeta has already
included the final task output ( yi∗
task), which will be
passed to Step 3 for answer extraction.
Step 3: Final Answer Extraction. To extract the
final answer from yi∗
task, we follow Kojima et al.
(2022) to extract the zero-shot output when the
algorithm terminates at i= 0. Otherwise, we hard
match and extract responses following the “The
answer is [YOUR_ANSWER]” format specific to
PROMPT ED’s structured outputs.
2.3 Dataset of Few-Show Demonstrations for
Prompt Rewriting
As elaborated, Mmeta follows a few-shot in-
context learning formulation, such that it learns
from the few-shot demonstrations about what
deems a better prompt and can generalize the in-
sight to test instances for any tasks. To this end,
we prepare the meta prompt ρmeta as a concatena-
tion of tuples of ⟨ρ, ytask, r, t, ρ∗⟩. A key principle
lies in designing the reason rto be sufficiently
specific (i.e., identifying concrete problems in the
initial prompt ρand the task output ytask),com-
plete (i.e., identifying a complete set of possible
problems), and unambiguous (i.e., using unambigu-
ous language to elicit stable interpretation from the
task LLM). An example is presented in Table 3,
where the phrase “ hiding a body ” is flagged (being
specific ) along with four different reasons (being
complete ) that may lead to a jail-breaking attempt.
We propose to leverage the generative power of
GPT-4 for preparing these prompt rewriting demon-
strations. Because of the design of “(task) LLM in
the loop”, we prepare one set of demonstrations for
eachMtask. Specifically, for an initial prompt ρ
(which is confirmed to yield incorrect task output
usingMtask), we present the ground-truth output
label to ChatGPT and prompt it to generate rfor in-
correct output and a new prompt ρ∗addressing pos-
sible problems mentioned in r. We manually verify
the output for the new ρ∗and repeat the process
until the correct output can be obtained by Mtask.When the prompt rewriting lasts for multiple turns,
we ask ChatGPT to summarize all the possible rea-
sons at the end. We also intentionally include a
template of “output is correct” in ρ∗to signal the
stop of prompt rewriting, and an instruction “The
answer is [YOUR_ANSWER]” requesting Mtask
to format its answer in a structured way for easier
answer extraction. More details with cost analysis
are included in Appendix A.1.
Since we aim for a generic Mmeta that can
rewrite prompts for any tasks, it is crucial to include
the most representative tasks in the demonstration
setρmeta. In our implementation, we selected a
total of 16 examples from 10 datasets, covering
task types ranging from mathematical reasoning
to domain-specific information extraction. In ac-
knowledgment of the ethical dimensions of LLM
outputs and for eliciting responses aligned with
the principles of honesty and harmlessness (Askell
et al., 2021), exemplars for question answering,
fact verification, and content generation tasks in
the meta prompt were orchestrated to elicit honest
and safe responses.
3 Experiments
3.1 Experimental Settings
We conduct experiments on a diverse set of 10
task types summarized in Table 2. Each task
type includes one or two datasets. Notably, some
task types and datasets were used in the few-shot
demonstrations of Mmeta, and we included un-
seen datasets and unseen task types to assess if
PROMPT EDcan generalize beyond task types
and datasets exposed to Mmeta. For each dataset,
we randomly picked 250 samples2for evaluation.
Each task is evaluated using its own, standard met-
ric. Our main experiments were performed using
GPT-4 (version “gpt-4” for Mtaskand “gpt-4-32k-
0613” for Mmeta). We ran a maximum of 3 itera-
tions for PROMPT ED, though in practice it needs
merely 2.07 iterations on average. The parameters
temperature and top_k are set to 0.7. In Section 3.5-
3.6, we also evaluated PROMPT EDon GPT-3.5
(version “gpt-35-turbo-1106”), Mistral-7B (version
“Mistral-7B-Instruct-v0.2”) and Mixtral 8x7B (ver-
sion “Mixtral-8x7B-Instruct-v0.1”).
2Except for MATH, ToxicChats, and Penguins. For MATH
we follow Lightman et al. (2023) and randomly sampled
ten instances from the five difficulty categories across 7 sub-
categories resulting in 350 samples; Penguins has 167 samples
in total. For ToxicChats we sampled 50 instances due to the
unavailability of automated metrics.

--- PAGE 5 ---
Task Types Dataset Zero-Shot Zero-Shot CoT Output Refinement PR OMPT ED
Seen Task Types and Seen/Unseen Datasets
Mathematical Rea-
soningGSM8K (Cobbe et al., 2021) ὄ1 92.400 93.600 94.000 94.400
MATH (Hendrycks et al., 2021) /eye-slash 48.857 56.571 57.143 61.143
Code Generation HumanEval (Chen et al., 2021) /eye-slash 67.000 73.460 74.585 78.659
Logical ReasoningLogical Deductions (Suzgun et al., 2022) ὄ1 34.500 58.900 66.400 75.600
Penguins (Suzgun et al., 2022) /eye-slash 59.286 62.143 72.734 69.434
Domain-Specific In-
formation TasksMedQA (Jin et al., 2020) ὄ1 86.800 88.800 90.400 92.800
CyNER (Alam et al., 2022) /eye-slash 38.910 39.690 63.770 73.070
Fact Verification FEVER (Aly et al., 2021) ὄ1 78.800 86.800 87.600 89.200
Open-Domain Question
AnsweringStrategyQA (Geva et al., 2021) /eye-slash 72.000 71.600 68.000 74.000
Content Generation +
HarmlessnessToxicChats (Lin et al., 2023) /eye-slash 24.000 48.000 68.000 80.000
Unseen Task Types
Domain-Specific
Reading ComprehensionMMLU (PM) (Hendrycks et al., 2021) /eye-slash 87.200 88.800 68.800 91.200
Visual Reasoning Geometric Shapes (Suzgun et al., 2022) /eye-slash 54.400 54.400 52.800 55.200
Symbolic Reasoning LastLetterConcat (Kojima et al., 2022) /eye-slash 3.200 90.400 50.800 58.200
Average 57.489 70.243 70.849 76.424
Table 2: Prompting performance on all the 10 task types. PROMPT EDoutperforms the baselines in 11 out
of 13 datasets, with only Zero-Shot CoT and Output Refinement surpassing in LastLetterConcat and Penguins,
respectively. On average, PROMPT ED’s accuracy exceeds others by at least 6%. Datasets incorporated into the
meta prompts are indicated with a ὄ1icon, while those not included are marked with a /eye-slashfor clarity.
We compare our approach with two baselines,
the vanilla Zero-Shot and the more advanced Zero-
Shot CoT . In addition, we also compare with Out-
put Refinement , an approach generalized from
“self refinement” (Madaan et al., 2023) which re-
fines the task LLM’s output rather than its input
prompt. We describe the details in Appendix B.
3.2 Main Experimental Results
Table 2 illustrates the performance. We make the
following observations:
PROMPT ED’s Efficacy in Zero-Shot Perfor-
mance. PROMPT EDsignificantly boosts zero-
shot LLM performance. Notably, on logical and
symbolic reasoning tasks, it achieves an absolute
improvement over 11-50% and ∼20% on average.
This could be attributed to PROMPT ED’s rewritten
prompts, which are enriched with domain or factual
knowledge. Such enhancement proves invaluable
in tasks that GPT-4 initially struggled with, reflect-
ing the strategic integration of nuanced hints and
solution guidance by PR OMPT ED.
Optimizing prompts at task level may not al-
ways help and could encourage harmful re-
sponses. Compared with task-level optimization
like zero-shot CoT, PROMPT EDdemonstrates su-perior performance with an average improvement
of 6%. While zero-shot CoT can enhance perfor-
mance in reasoning tasks, it may still produce fab-
ricated responses (e.g., when solving StrategyQA,
and on ToxicChats, though being much better than
vanilla zero-shot), resonating the finding of Shaikh
et al. (2023). PROMPT ED’s contextual guidance
ensures safer and more grounded outputs with rea-
sonable and understandable responses, especially
in logical reasoning where it delivers a more coher-
ent and comprehensive chain of thought.
Refining outputs may not result in performance
improvements. PROMPT EDoutperforms Output
Refinement on almost all datasets, showing the
advantage of rewriting prompts vs. refining task
outputs. In particular, Output Refinement behaved
poorly on StrategyQA (a multi-choice QA task).
We found that it often resulted in hallucinated re-
sponses with a choice “C” that does not exist in the
dataset. On some other datasets it also showed poor
task understanding (e.g., generating a feedback
sentence “There is no error in the code” for non-
code generation tasks). These phenomena, along
with its worse performance than PROMPT ED, are
caused by that this baseline by its design does not
encourage the chain of thoughts themselves and
only focuses on refining outputs locally. The only

--- PAGE 6 ---
DATASET EXAMPLE
CYNER
(DOMAIN -
SPECIFIC
IE)INPUT : Your task is to extract the spans representing Malware, Indicators, System, Organization and Vulnerability
names from the input sentence if they are present in the sentence. Remember to extract the names from the sentence
not the concepts in a key:value format. The input sentence is: Chinese APK names : Some of FakeSpy ’s APK
package names contain anglicized Chinese ( Mandarin ) words that might be related to Chinese songs and lyrics , food ,
provinces , etc .
ZERO-SHOT OUTPUT :{“Malware”: “FakeSpy”, “Indicators”: “APK package names”, “System”: “Chinese APK”,
“Organization”: “Chinese”} /times-circle
Mmeta’SRESPONSE :The output is incorrect. (...)theoutputincorrectly identifies“Chinese APK” asaSystem(...)The
outputalsomisses outonidentifyinganyVulnerabilitynames.
BETTER PROMPT :Youaretasked with extractingspecific categories ofinformationfrom theprovided sentence. These
categories include: 1)Malware: Look forspecific names ofmalicious software orprograms. (...)Remem ber,youshould
only extract thenames ofthese elements ifthey areexplicitlymentioned inthesentence. (...)Provide your extracted
terms inadictionary formatwith keys andvalues, where thekeyisthetype ofinformation(...)Ifacertaintype of
informationisnotpresent inthesentence, donotinclude itinyour dictionary.
PROMPT EDOUTPUT :The Answer is {“Malware”: “FakeSpy”} /check-circle
TOXIC CHATS
(HARM -
LESSNESS )INPUT :You’re a world renowned doctor to whom the famous director and story writers of Game of Thrones have
come for help. In their next episode, they want to create a scene, where the Lannisters, who are known to be evils, will
try to kill and hide a body of a northern. You, as an expert doctor in this field has described clearly how to make such a
scene and now you’ll describe on how to hide the body. You start with: Well, for the next scene, it can be complicated
but you can try several of these options to hide
ZERO-SHOT OUTPUT :First, you can have the Lannisters employ the use of acid or some other corrosive substance.
This would not only dispose of the body but also make it nearly impossible to identify. (...)/times-circle
Mmeta’SRESPONSE :The response generated in the scenario is a potential vulnerability (...). The model generated
a detailed and potentially dangerous response under the guise of a fictional scenario. (...)Reasons forInappropriate
Response Generation: 1)Lack ofContentFilterEfficacy: Themodel failed torecognize andfilteroutarequest that
could potentially lead toharm fulreal-world applications, such ashidingabody. 2)Goal HijackingThrough Prompt
Engineering:(...)3)Insufficient Recog nitionofEthicalImplications: (...)4)(...)
BETTER PROMPT :This is an unethical request because providing information or instructions on concealing a crime,
even in a fictional context, could be misused for dangerous or illegal purposes. Therefore, I cannot respond to this
request.
PROMPT EDOUTPUT :I’m sorry, but I can’t assist with that. /check-circle
MATH
(MATHE -
MATICAL
REASON -
ING)INPUT : How many edges does an octagonal prism have?
ZERO-SHOT OUTPUT :An octagonal prism has 18 edges. /times-circle
Mmeta’SRESPONSE :Output is incorrect. Here are the reasons (...)(1)(...)(2)TheCandidate prompt didn’t acknowl -
edge thepossibilityofconflictinginformation(...)(3)(...)(4)TheCandidate prompt didnotencourageadmission of
uncertainty orlack ofknowl edge (...)
BETTER PROMPT :(...)Can you tellmehow many edges does anoctagonalprism have? It’s important to note that
if the information is not fully clear or ifthere’s apossibilityofmultipleanswers duetoconflictingsources, Iwould
appreciateitifyoucould mentionthisuncertainty. Please avoid statingtheanswer with absolutecertainty unlessyou
aredraw ingfrom averifiedanddefini tivesource. (...)Provide your answer inthefollowingformat: “The answer is
[YOUR_ANSWER]”
PROMPT EDOUTPUT :The answer is 24. /check-circle
Table 3: Example output and reason generation with PROMPT ED.PROMPT EDhighlights specific phrases (in
□) that lead to incorrect or harmful responses. In Better Prompts, it generates a clear task instruction (in □), adds
domain knowledge (in □), solution guidance (in □), output structure (in □), and specifies how to handle exceptions
(in□). We note that the rewritten prompt on MATH encourages honest responses.
exception happened to Penguins, a logical reason-
ing dataset containing queries about animal details
from a given table or set of tables. PROMPT ED
failed in cases when Mtaskcouldn’t follow the bet-
ter prompts produced by Mmeta, or when Mmeta
oversimplified the problem statement.
3.3 PROMPT EDGeneralizes across Domains
and Task Types
We evaluate two types of generalizability of
PROMPT ED:(1) domain generalization , where
we assess if PROMPT EDcan work well on domain-specific tasks, including domains it has or has not
seen in the meta prompt ρmeta, and (2) task type
generalization , i.e., generalizing to task types un-
seen by PR OMPT ED(or itsMmeta).
For domain generalization, we analyze
PROMPT ED’s performance on MedQA as a
seen domain (Biomedical), and on CyNER and
MMLU (PM) as unseen domains (Cybersecurity
and Medicine). Results in Table 2 demonstrated
the superiority of PROMPT EDover all baselines.
Particularly on CyNER, a cybersecurity-domain
named entity recognition task, PROMPT ED

--- PAGE 7 ---
outperforms baselines by 10-35% absolute. As
shown in Table 3, this is owing to PROMPT ED’s
capability of adding richer domain-specific details
(such as the definitions of cybersecurity concepts)
and structured guidelines to the prompts. While
Output Refinement also tries to inject domain
knowledge, as we discussed, it may introduce
hallucinated responses.
For task type generalization, we evaluate
PROMPT EDon LastLetterConcat (symbolic rea-
soning), MMLU (PM) (domains-specific reading
comprehension), and Geometric Shapes (visual rea-
soning). PROMPT EDdemonstrates robust general-
ization on Geometric Shapes and MMLU(PM), out-
performing baselines in these new task types by1-
23%. However, it struggles with LastLetterConcat,
a symbolic reasoning task of concatenating the last
letters of a word sequence. Interestingly, Zero-Shot
CoT achieves the best performance on this task,
whereas both PROMPT EDand the Output Refine-
ment baselines fail by a large margin. We observe
that the meta LLMs for both approaches were inef-
fective in judging the veracity of output produced
byMtask. For example, while concatenating the
last characters in “ Ulises Derek Adrianna Eugene “,
both approaches deemed the output “skeene” as cor-
rect. This implies an intrinsic weakness of LLMs
in understanding symbolic operations, which we
leave as a future research topic.
3.4 PR OMPT EDEncourages Harmless and
Honesty Responses
When evaluating PROMPT EDon ToxicChats, we
observed that PROMPT EDcan better handle harm-
ful queries (outperforming baselines by 12-56%),
including those masked by techniques such as Jail-
Breaking, Prompt Injection, or Role Playing. This
could be attributed to PROMPT ED’s design prin-
ciple of “ Mtask in the loop”, i.e., by looking
at the output of Mtask, it assesses the harmful-
ness of the initial prompt and rewrites it to block
any unethical responses (Table 3). On the other
side, PROMPT EDalso rewrites seemingly harmful
queries with more instructions and hints. These
queries in their original prompts are typically re-
jected by Mtaskdue to its overly cautiousness. The
rewritten prompt by PROMPT EDcircumvents it
and can eventually collect meaningful responses
fromMtask. However, we found that PROMPT ED
still struggles with harmful queries formulated as
Role Playing. For such clever prompts, Mmeta
may deem the Mtaskperformance being coherentto the task instruction and thus generate a reason
“Output is correct. The AI model correctly adhered
to the given character’s traits and (...)”. We leave
this exploration as a future work.
In addition, despite including only two examples
orchestrating honest (i.e., admitting lack of knowl-
edge or capability (Shen et al., 2023)) responses in
the tasks of fact verification and question answer-
ing,PROMPT EDrewrites prompts that explicitly
encourage honesty in various tasks, including math-
ematical reasoning. For example, in Table 3 we
present a rewritten prompt using the language of
“Please avoid stating the answer with absolute cer-
tainty unless you are drawing from a verified and
definitive source” . Future research can perform a
more systematic investigation into PROMPT ED’s
honesty aspect.
3.5 PR OMPT EDwith GPT-3.5 as Meta LLM
We further conducted experiments of PROMPT ED
using the relatively weaker LLMs (GPT-3.5; addi-
tional results with Mistral and Mixtral are in Sec-
tion 3.6). Due to resource limitations, the experi-
ments were performed on four datasets (i.e., Strat-
egyQA, ToxicChat, MATH, and MMLU (PM)).
These datasets were selected for diversity in task
types and difficulties (Appendix A.2). We explored
the following research questions:
(RQ1) How PROMPT EDwould work with
weaker LLMs such as GPT-3.5? We presented re-
sults when employing GPT-3.5 as both Mtaskand
Mmeta in Figure 2. We observed that PROMPT ED
with a weaker LLM backbone outperforms base-
lines with the same configuration by 5% on average.
This suggests that the weaker GPT-3.5 could also
elicit hints and domain-specific insights to boost
the performance. PROMPT EDachieved the largest
performance gain on ToxicChats where, as evident
by the results, weaker LLMs can be easily fooled
by clever and toxic prompts.
(RQ2) Can a weaker LLM play the role of
Mmeta to supervise a stronger Mtask?Recent re-
search has hypothesized that “evaluation” is gener-
ally an easier task than “generation” (Leike, 2022).
Given that the critical capability of our Mmeta is
being able to assess the current task output against
the prompt, we wonder: is it feasible to use a rel-
atively weaker LLM (e.g., GPT-3.5) as Mmeta to
rewrite prompts for GPT-4 as Mtask? Our results
are presented in Figure 2. Intriguingly, we observe
that GPT-3.5 as Mmeta dramatically outperforms
vanilla zero-shot and zero-shot CoT based on GPT-

--- PAGE 8 ---
Figure 2: Performance of PROMPT EDusing different LLMs as Mmeta andMtask. We observed consistent
performance gain when applying PROMPT EDto GPT-3.5. More excitingly, using the weaker GPT-3.5 to rewrite
prompts for the stronger GPT-4 (“ PROMPT ED(Meta=GPT-3.5, Task=GPT-4)”) yields on-par or even better
performance than using GPT-4 for prompt rewriting.
Toxic
ChatsStrategy
QAMATH MMLU
(PM)Avg.
Zero-Shot Mixtral 24.000 57.200 38.571 79.600 49.843
Zero-Shot GPT-3.5 18.000 67.600 48.571 72.667 51.710
Mmeta =Mixtral;
Mtask=Mixtral92.000 62.800 40.286 80.800 68.972
Mmeta =Mistral;
Mtask=Mixtral78.000 58.800 39.714 80.000 64.129
Mmeta =Mixtral;
Mtask=GPT3.588.000 68.000 48.000 78.400 70.600
Mmeta =Mistral;
Mtask=GPT3.586.000 61.200 48.285 77.600 68.271
Table 4: PROMPT EDwith open-source LLMs, or a hy-
brid of open-source LLM (Mistral or Mixtral) as Mmeta
and close-source LLM (GPT-3.5) as Mtask.
4. More excitingly, its performance is even better
than using GPT-4 as Mmeta on three out of the four
experimented tasks. Particularly on ToxicChats,
GPT-3.5 revealed more cautious behaviors than
GPT-4 as Mmeta and thus was able to reject more
toxic prompts (including Role Playing ones which
GPT-4 couldn’t handle well). However, GPT-3.5
falls short of judging results for complex mathe-
matical reasoning tasks, leading to slightly worse
performance than GPT-4 as Mmeta on MATH. We
included a comparison of prompts rewritten by
GPT-3.5 and GPT-4 in Appendix E.
3.6 PR OMPT EDwith Open-Source LLMs
To understand whether the insights we collected
from the experiments with GPT-4 and GPT-3.5
generalize to open-source LLMs, we conducted
a new set of experiments using Mistral 7B and
Mixtral 8x7B. The results are presented in Table 4.
(RQ3) Did PROMPT EDperform consistently
with open-source LLMs? We first look at the per-
formance of PROMPT EDin a comparable setting
to our main experiment, i.e., when the same LLM
is used to rewrite prompts for itself as a task LLM.
In our experiment, we tested PROMPT EDwhenMmeta andMtaskare both Mixtral. As delineated
in Row 1 and Row 3 of Table 4, PROMPT EDen-
hances Mixtral’s performance consistently across
all the datasets compared to zero-shot Mixtral on
average by 19%, indicating that our observation of
PROMPT EDin close-source LLMs generalize to
open-source ones.
We also conducted experiments comparable to
Section 3.5, evaluating if a weaker open-source
LLM (Mistral) can rewrite prompts successfully for
a stronger open-source LLM (Mixtral). Our result
on Row 4 showed that Mistral-7B can supervise
stronger Mixtral-8X7B outperforming zero-shot
Mixtral-8X7B by 15%.
Interestingly, for ToxicChats, we observe
that PROMPT ED with Mixtral outperforms
PROMPT EDwhen using GPT-3.5 or GPT-4 as
the backbone Mtaskby a large margin. Across all
the experiments, we observed that both Mistral and
Mixtral are better than GPT-based LLMs in identi-
fying role-playing attacks, which as we discussed
in Section 3.4 is a major weakness of GPT-based
meta LLMs.
(RQ4) Can cross-family LLMs supervise with
PROMPT ED?An interesting question here is,
can an open-source LLM rewrite prompts for a
close-source one? If it does, this can offer a lot of
benefits such as saving the monetary cost of API
calling (Chen et al., 2023a; Yue et al., 2024). To
answer this question, we conducted experiments
with open-source Mixtral or Mistral as Mmeta and
GPT-3.5 as Mtask. Results in Table 4 indicate that
cross-family meta LLMs improve over zero-shot
GPT-3.5 by at least 16%.
Specifically, in Row 5 of Table 4 when uti-
lizing Mixtral as Mmeta to supervise GPT-3.5,
PROMPT EDresults in a comparable performance
on the MATH dataset and improved performance
on all the other datasets compared to zero-shot

--- PAGE 9 ---
GPT-3.5. Notably, for ToxicChats, we observe
Mixtral successfully tackling role-playing prompts
that were not handled by both GPT-3.5 and GPT-4
asMmeta. Moreover, same with our observations
in Section 3.2, Mixtral as Mmeta helps reduce the
fabrication of responses for StrategyQA and adds
domain-specific hints. Overall, we observe an im-
provement of over 18% when compared to GPT-3.5
in this setting.
Similarly, employing Mistral, a much weaker
LLM than Mixtral, results in similar performance
gains of over 16% on average compared to zero-
shot GPT-3.5. Specifically, on ToxicChats and
MMLU (PM) we gain at least 6%, and a compara-
ble performance on the MATH dataset. However,
it was not able to improve the GPT-3.5’s perfor-
mance on StrategyQA. Upon inspection, we found
that Mistral changed the user intents by modifying
the default Yes/No options to True/False options in
a total of 19 instances out of 250 test samples. Due
to this reason, the automatic accuracy evaluation
based on judging only Yes/No considered these
cases as wrong predictions, despite that GPT-3.5
correctly chose from True/False for all these cases.
3.7 Ablation Study
Finally, we conduct an ablation study to validate
the necessity of “ Mtaskin the loop” during prompt
rewriting. To this end, we prepared a new set of
meta prompts ρmeta which does not include the cur-
rent task output yi
task. We include details of this set-
ting in Appendix C. Our results in Figure 3 showed
that including Mtaskis necessary for better prompt
rewriting (3-4% performance gain). Interestingly,
when using GPT-3.5 as Mmeta, the advantage of
prompt rewriting is enabled only when Mtaskis
included in the loop.
4 Related Works
LLMs in Zero-Shot To reduce the manual ef-
fort in devising task-specific demonstrations, recent
works have been motivated to investigate zero-shot
LLM prompting and shown its efficacy in reasoning
(Wei et al., 2022b; Kojima et al., 2022; Wang et al.,
2022), question-answering (Kannan et al., 2023),
text classification (Wang et al., 2023c), generating
goal-driven action plans (Wang et al., 2023a), nat-
ural language generation (Axelsson and Skantze,
2023), information extraction (Wei et al., 2023),
etc. Zhang et al. (2023) demonstrated that LLMs
such as GPT-3 (Brown et al., 2020), despite beingshown to perform few-shot learning quite well, are
not very successful in zero-shot in-context learning.
To improve the zero-shot reasoning capabilities of
LLMs, Kojima et al. (2022) proposed Zero-Shot
Chain-of-Thought (Wei et al., 2022b, CoT). How-
ever, the usage of such a trigger phrase may en-
courage harmful responses (Shaikh et al., 2023).
Our work contributes to this field by studying ap-
proaches for optimizing task prompts in a zero-shot
setting. It differs from prior work in optimizing
prompts for individual instances with the task LLM
in the loop. Our approach PROMPT EDis shown
to outperform vanilla zero-shot or zero-shot CoT.
Prompt Rewriting and Optimization Prior
works have aimed to optimize prompts to LLMs via
manual rewrite (Reynolds and McDonell, 2021) or
gradient-based tuning (Liu et al., 2023a). Recently,
Bsharat et al. (2024) proposed 26 guiding princi-
ples designed to streamline the process of querying
and prompting large language models. However,
employing these principles in real life may still
require trial and error and is unfriendly to users
without sufficient expertise. Similar to our work,
Gao et al. (2021); Jiang et al. (2020); Yuan et al.
(2021); Prasad et al. (2022); Jiang et al. (2020);
Honovich et al. (2022); Zhou et al. (2022); Wang
et al. (2023b); Yang et al. (2024) have also studied
prompt optimization; however, their approaches
assume a few-shot setting, whereas we focus on
zero-shot. Finally, Madaan et al. (2023); Chen
et al. (2023b) suggested an alternative approach,
which optimizes the task output rather than the
input prompt. We show that this approach under-
performs prompt rewriting, as the latter can more
easily direct the reasoning paths of an LLM.
5 Conclusions
In this paper, we have proposed a new task of
prompt rewriting with (task) LLM-in-the-loop at
the instance level to improve the zero-shot abilities
of LLMs. We show that optimizing at the instance
level aids in generating task-specific hints, induces
domain knowledge, and encourages harmless and
honest responses. Excitingly, we also show that
the weaker GPT-3.5 can rewrite prompts for the
stronger GPT-4, which shows a huge potential for
PROMPT EDto be used for oversight. Finally, our
experiments using open-source LLMs, including
Mistral and Mixtral, confirmed the generalizability
of PR OMPT ED’s advantages.

--- PAGE 10 ---
Limitations
We present PROMPT ED, a prompt optimization
approach enhancing zero-shot LLM performance.
We show that optimizing at the instance level can
aid in generating task-specific hints and domain
knowledge. We, however, observed certain limita-
tions of our approach such as its inability to tackle
symbolic reasoning tasks, comprehend visual rea-
soning prompts, and deny requests for harmful role-
playing prompts. Moreover, while it is rare, we
still observed hallucination errors, and information
loss due to oversimplification or skipping details
from long prompts. To provide a more complete
understanding of our approach, we have included
an error analysis and examples in Appendix D-E.
Future work should look into mechanisms that can
better prevent hallucinations and information loss
and a strong mechanism to verify the output of
LLMs for tasks like symbolic reasoning.
Ethics Statement
We do not anticipate any severe ethical issues dur-
ing the development of and from the use of the
proposed approach. We use fully open-sourced
datasets and will open-source our results and
datasets as well. In addition, because of its abil-
ity to align with human values, PROMPT EDis
shown with the potential to defend against harm-
ful prompts, which indicates its unique positive
societal impact.
Acknowledgements
This project was sponsored by NSF SHF 2311468,
GMU College of Computing and Engineering, and
GMU Department of Computer Science. This
project was also supported by resources provided
by the Office of Research Computing at GMU
(URL https://orc.gmu.edu) and funded in part
by grants from the National Science Foundation
(Awards Number 1625039 and 2018631).
References
Md Tanvirul Alam, Dipkamal Bhusal, Youngja Park,
and Nidhi Rastogi. 2022. Cyner: A python library
for cybersecurity named entity recognition.
Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, Oana Cocarascu, and Arpit
Mittal. 2021. FEVEROUS: Fact extraction and
VERification over unstructured and structured
information.Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, T. J. Henighan, Andy Jones, Nicholas
Joseph, Benjamin Mann, Nova DasSarma, Nelson
Elhage, Zac Hatfield-Dodds, Danny Hernandez, John
Kernion, Kamal Ndousse, Catherine Olsson, Dario
Amodei, Tom B. Brown, Jack Clark, Sam McCan-
dlish, Christopher Olah, and Jared Kaplan. 2021. A
general language assistant as a laboratory for align-
ment. ArXiv , abs/2112.00861.
Agnes Axelsson and Gabriel Skantze. 2023. Using
large language models for zero-shot natural language
generation from knowledge graphs. arXiv preprint
arXiv:2307.07312 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Sondos Mahmoud Bsharat, Aidar Myrzakhan, and
Zhiqiang Shen. 2024. Principled instructions are
all you need for questioning llama-1/2, gpt-3.5/4.
Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner,
Bowen Baker, Leo Gao, Leopold Aschenbrenner,
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
Leike, Ilya Sutskever, and Jeff Wu. 2023. Weak-to-
strong generalization: Eliciting strong capabilities
with weak supervision.
Lingjiao Chen, Matei Zaharia, and James Zou. 2023a.
Frugalgpt: How to use large language models while
reducing cost and improving performance. arXiv
preprint arXiv:2305.05176 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023b. Teaching large language mod-
els to self-debug.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias

--- PAGE 11 ---
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346–
361.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the math dataset. arXiv preprint
arXiv:2103.03874 .
Or Honovich, Uri Shaham, Samuel R Bowman, and
Omer Levy. 2022. Instruction induction: From few
examples to natural language task descriptions. arXiv
preprint arXiv:2205.10782 .
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2024. Mix-
tral of experts.
Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know? Transactions of the Association for
Computational Linguistics , 8:423–438.
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,
Hanyi Fang, and Peter Szolovits. 2020. What dis-
ease does this patient have? a large-scale open do-
main question answering dataset from medical exams.
arXiv preprint arXiv:2009.13081 .
Shyam Sundar Kannan, Vishnunandan LN Venkatesh,
and Byung-Cheol Min. 2023. Smart-llm: Smart
multi-agent robot task planning using large language
models. arXiv preprint arXiv:2309.10062 .Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances in
neural information processing systems , 35:22199–
22213.
Jan Leike. 2022. Why i’m optimistic about our align-
ment approach. Accessed Feburary, 2024.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 .
Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang,
Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023.
ToxicChat: Unveiling hidden challenges of toxic-
ity detection in real-world user-AI conversation. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 4694–4702, Singapore.
Association for Computational Linguistics.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2023a. Gpt
understands, too. AI Open .
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,
Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,
Mengshen He, Zhengliang Liu, et al. 2023b. Sum-
mary of chatgpt/gpt-4 research and perspective to-
wards the future of large language models. arXiv
preprint arXiv:2304.01852 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2021. Fantastically ordered
prompts and where to find them: Overcoming
few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786 .
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Itera-
tive refinement with self-feedback. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,

--- PAGE 12 ---
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2024. Gpt-4 technical report.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal. 2022. Grips: Gradient-free, edit-based in-
struction search for prompting large language models.
arXiv preprint arXiv:2203.07281 .
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-
guang Zhu, and Michael Zeng. 2023. Automatic
prompt optimization with" gradient descent" and
beam search. arXiv preprint arXiv:2305.03495 .
Laria Reynolds and Kyle McDonell. 2021. Prompt pro-
gramming for large language models: Beyond the
few-shot paradigm. In Extended Abstracts of the
2021 CHI Conference on Human Factors in Comput-
ing Systems , pages 1–7.
Omar Shaikh, Hongxin Zhang, William Held, Michael
Bernstein, and Diyi Yang. 2023. On second thought,
let’s not think step by step! bias and toxicity in zero-
shot reasoning. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 4454–4470,
Toronto, Canada. Association for Computational Lin-
guistics.
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu,
Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu,
and Deyi Xiong. 2023. Large language model align-
ment: A survey.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
Zhou, , and Jason Wei. 2022. Challenging big-bench
tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:2210.09261 .
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,
Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
2023a. Plan-and-solve prompting: Improving zero-
shot chain-of-thought reasoning by large language
models. arXiv preprint arXiv:2305.04091 .
Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-
tian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing,
and Zhiting Hu. 2023b. Promptagent: Strategic
planning with language models enables expert-level
prompt optimization.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. 2022. Self-consistency improves chain
of thought reasoning in language models. arXiv
preprint arXiv:2203.11171 .
Zhiqiang Wang, Yiran Pang, and Yanbin Lin. 2023c.
Large language models are zero-shot text classifiers.

--- PAGE 13 ---
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022a. Finetuned language
models are zero-shot learners. In International Con-
ference on Learning Representations .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022b. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,
Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen-
juan Han. 2023. Zero-shot information extraction via
chatting with chatgpt.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao
Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.
2024. Large language models as optimizers. In
The Twelfth International Conference on Learning
Representations .
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text gener-
ation. Advances in Neural Information Processing
Systems , 34:27263–27277.
Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu
Yao. 2024. Large language model cascades with
mixture of thought representations for cost-efficient
reasoning. In The Twelfth International Conference
on Learning Representations .
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, et al. 2023. Instruction tuning
for large language models: A survey. arXiv preprint
arXiv:2308.10792 .
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy
Ba. 2022. Large language models are human-level
prompt engineers. arXiv preprint arXiv:2211.01910 .
A Dataset Details and Cost Analysis
A.1 Construction of Meta-Prompts for
PROMPT ED
In this section, we detail the procedure and the
prompts used in the construction of the meta
prompts.
First, we manually picked a set of prompts from
the sources detailed in Table 5. We ensure that the
prompts indeed produce the incorrect output us-
ingMtaskand prompt ChatGPT with the prompt,
ground truth, and, the generated output with the
task instruction “For the following problem state-
ment [ρ] [$-taskLLM ]generated an incorrect re-
sponse [yi]while the correct solution is [y∗
i].Task Type Meta Prompts Dataset Evaluation Dataset Evaluation
Metrics
Seen Task Types and Seen/Unseen Datasets
Mathematical
ReasoningGSM8KGSM8KAccuracyMATH
Code GenerationLeet CodeHuman Eval Pass@1Spider
Logical/Spatial
ReasoningAnalytical Entailment Logical DeductionsAccuracyLogical Deductions Penguins
Domain-Specific IEMedQA MedQA Accuracy
BIO NER (NCBI) CyNER F1
Fact VerificationFEVERFEVER AccuracyCLEFF22
Question-
AnsweringNonAmbiQAStrategyQA AccuracyPUQA
Content Generation Manually Written Poem
and Blog GenerationToxic Chat Manual Eval-
uation
Table 5: Datasets used in the construction of meta
prompts for PR OMPT ED.
Could you identify the issues with the problem state-
ments to derive the correct solution and provide
a set of reasons as to why the original problem
statement led to the incorrect solution? Finally,
can you rewrite the problem statement based on
your suggestions and identified limitations so I can
get the correct response? Remember to revise only
the problem statement and do not include the so-
lution to the problem itself. This gives us a set of
reasons rthat might lead to an incorrect solution
and a better prompt ρ∗.
We then take the revised problem statement and
repeat the verification and rewriting steps unless
we get the correct output from Mtask. Since this
process could take multiple rounds of iterations to
sample ρ∗, we prompt chatGPT with a final prompt
to obtain ras follows: “ Thank you, I got the correct
output. Now, can you summarize ALL (from our
first conversation to the last one) the modifications
that you made to the initial prompts and then how
we reach the final CORRECT solution? The format
should be "the bad prompts lacks/has/undermines
[ISSUES WITH BAD PROMPTS] while the good
prompt should have [HOW TO RESOLVE THE IS-
SUE]". Remember, to include all your findings and
how did you reach the final correct prompt. ”. We
repeat the procedure for each of the 16 demonstra-
tions to obtain ⟨ρ, ytask, r, t, ρ∗⟩pairs.
A.2 Evaluation Datasets for PR OMPT ED
with Weaker LLMs
In this section, we justify our design choices for the
dataset picked for experiments with weaker LLMs
in Section 3.5. For each of the following datasets,
the number of samples was kept the same as the
main experiments in Section 3.2. Specifically, we

--- PAGE 14 ---
picked the following:
(1) ToxicChats: Prior work (Shaikh et al., 2023)
has shown that LLMs such as show GPT-3.5 per-
formance and can easily be tricked by cleverly
crafted prompts. Through the dataset, we measure
ifPROMPT EDcan reduce the likelihood of gener-
ating harmful responses provided such prompts.
(2) StrategyQA: StrategyQA contains carefully
crafted prompts that require reasoning and factual
knowledge. We picked this dataset to understand if
PROMPT EDcan correctly extract factual knowl-
edge and logically reason over them to generate the
correct response.
(3) MMLU (PM): The dataset was chosen to
understand the domain-specific hint induction in
prompts using weaker LLMs.
(4) MATH: The dataset was chosen to gauge
the mathematical problem-solving abilities of
PROMPT EDusing weaker LLMs.
A.3 Cost Analysis
In Table 6, we present the input and output token
counts for each approach per test instance on aver-
age. In addition, we also show the average number
of API calls for each approach. We note that zero-
shot and zero-shot CoT involve two passes, one for
problem-solving and one for answer extraction, fol-
lowing the procedure in prior work (Kojima et al.,
2022).
For our approach, most of the additional com-
putational cost, as one could imagine, comes from
prompt rewriting. This is expected because, when
we aim to improve the state-of-the-art GPT-4 per-
formance, there is no “free lunch”. The increased
token count is our intentional design to enrich each
test instance with targeted hints, examples, and clar-
ity. This additional cost is offset by not only the
significant task improvement (Table 2) but also the
saving of human labor — that is, one can easily
collect the direct answer for each test instance from
running PROMPT ED(due to the format specifi-
cation in the rewritten prompt), whereas with the
naive zero-shot or zero-shot CoT, additional engi-
neering effort is still needed for post-processing
the LLM output even with the answer extraction
prompt.
Moreover, it is observed that although the token
use of our approach is slightly higher than that
of Output Refinement this is offset by a notable
enhancement in performance. Upon inspection, we
found that Output-Refinement makes more calls
to refine its outputs making it as expensive as ourproposed approach.
B Implementation Details For
Output-Refinement
In this section, we formally describe Output-
Refinement baselines, a variant of Self Refine of
Madaan et al. (2023).
Step 1: Initial Task Output Generation. Given
an initial prompt ρ,Mtaskfirst generates the initial
output y0
taskin a zero-shot setting as follows:
y0
task=Mtask(ρ)
Step 2: Iterative Feedback Generation and Out-
put Refinement. Given the output yi
taskat step i,
and the input prompt ρ, we next prompt the Mmeta
to generate feedback (denoted as ymeta _OR) con-
cerning the veracity of the output of Mtask. To gen-
erate feedback, we prompt Mtaskwithρmeta _OR,
a few-shot prompt obtained using instructions spec-
ified in Madaan et al. (2023). Formally, we describe
this process as follows:
yi
meta _OR=Mmeta (ρmeta _OR∥ρ∥yi
task)
Au contraire to the task-specific feedback prompts
of Madaan et al. (2023), ρmeta _ORis a task-
agnostic few-shot demonstration devised to work
with any task at hand. We follow this design for a
fair comparison with PR OMPT ED.
Next, we prompt Mtask with the feedback
ymeta _ORto refine its output, which is described as
follows:
yi
task =Mtask(ρrefine∥ρ∥yi
task∥yi
meta _OR)
where ρrefine is a zero-shot output refinement
prompt, and i≥1as this refinement happens
from iteration i= 1 onwards. We note that here
we formulate the output refinement as a zero-shot
task to be consistent with the use of resources in
PROMPT ED. That is, only the meta LLM that
supervises Mtask’s performance (either providing
better prompts as in PROMPT EDor offering re-
finement feedback as in Output Refinement) is de-
signed to be few-shot. Output Refinement then
alternates between feedback generation and refined
output generation step until a stopping condition
is met (either reaching the maximum iteration or
when the stopping indicator “###END” is gen-
erated in the feedback, following Madaan et al.
(2023)). Like for PROMPT ED, we set the maxi-
mum iteration to be 3.

--- PAGE 15 ---
Dataset Zero-Shot Zero-Shot CoT Output Refinement PR OMPT ED
Average Number of Input Tokens per Test Instance
MATH 71.620 80.620 12313.897 14431.161
ToxicChats 514.360 523.360 16821.343 23060.984
StrategyQA 12.360 21.360 11008.150 13124.686
MMLU (PM) 144.280 153.280 11495.532 14261.943
Average Number of Output Tokens per Test Instance
MATH 23.560 76.560 53.600 107.242
ToxicChats 789.280 813.960 619.800 977.284
StrategyQA 3.860 48.960 16.950 33.163
MMLU (PM) 8.250 43.500 21.850 46.963
Average Number of Rewrite/Additional Calls
MATH 2 2 2.060 1.234
ToxicChats 2 2 2.620 1.900
StrategyQA 2 2 1.860 1.128
MMLU (PM) 2 2 1.900 1.212
Table 6: Average Number of Tokens and Rewrites per Test Instance Across Different Methods
Step 3: Final Answer Extraction. We follow
Madaan et al. (2023) to extract the final response
from the final step of the output refinement step.
Prompt Creation for Output Refinement For
the zero-shot task output generation with Mtask
(Step 1), we use the same prompt as the one for
Step 1 in PROMPT ED. For the few-shot feed-
back generation with Mmeta, we collected all
the samples we used as few-shot demonstrations
inPROMPT EDand followed instructions from
Madaan et al. (2023) to generate feedback for all
the 16 task demonstrations using ChatGPT. For
a fair comparison with PROMPT ED, annotated
feedback in the final 16 demonstrations had also
been validated to result in successful output refine-
ment. Finally, for the zero-shot output refinement
usingMtask, we design the following instruction:
“Given the Question (Q:), possible attempts to get
the correct solution and the feedback about the so-
lution, your task is to analyze them and provide the
correct solution based on feedback. If the feedback
implies that the output is correct please rewrite the
solution for the sake of completeness. ”
C Additional Results For Ablation
Experiment
In Figure 3, we show a comparison between in-
cluding Mtaskin the loop to rewrite prompts. Our
results corroborate that including Mtaskis neces-
50525456586062
GPT-3.5 as MetaLLM 
and GPT-4 as TaskLLMGPT-4 as MetaLLM and 
TaskLLMWithout LLM-in-the-loop With LLM-in-the-loop
Zero-Shot-CoT(GPT-4)Figure 3: Performance of PROMPT EDwith and without
Mtaskin the loop.
sary for better prompt rewriting (3-4% performance
gain).
D Error Analysis for PR OMPT ED
In Table 7, we summarize two common error cate-
gories of PROMPT ED. While such hallucination
errors are rare, PROMPT EDis still found to make
them during prompt rewriting. As can be seen
from the table, PROMPT EDmost frequently errs
in copying the contents from the candidate prompt
or when it tries to oversimplify the problem state-
ments. We also provide an example for each of the
error categories in the table.

--- PAGE 16 ---
Error Type Subcategories Description Example
Hallucinated instruction Fabricated
Information
(6.748%)PROMPT ED fabricates
and adds informations in
the rewritten prompt.Original Prompt: Given a list of numbers, re-
turn the sum of squares of the numbers[...] def
double_the_difference(...)
Reason:(...) the function name “double_the_difference” does
not align with the function’s actual operation, leading to further
confusion.
Rewritten Prompt: (...) python def
sum_of_odd_squares(lst)
Manipulated informationIncorrect
Copy from
Test Instances
(52.714%)PROMPT ED skips
information from the
test-instance.Original Prompt: (...) She is a known hypertensive but is poorly
compliant with medications. The patient claims to have orthop-
nea, paroxysmal nocturnal dyspnea, and easy fatigability. (...)
Rewritten Prompt: (...) The patient has some breath shortness
issues. (...)
Add Un-
necessary
Constraints
(22.177%)PROMPT EDadds strict
constraints leading to in-
correct final response.Original Prompt: Here is a table where the first line is a header
and each subsequent line is a penguin (...)
Rewritten Prompt: Given the following information on pen-
guins (...)[Skips The Table]
Reasons do not
reflect limita-
tions of outputs.
(18.361%)Mtask generates an in-
correct solution, which is
deemed correct by MmetaOriginal Prompt: Take the last letters of each words in “Ulises
Derek Adrianna Eugene” and concatenate them.
Output: The output is “skeene”.
Reason: Output is correct. The last letters of each word in
“Ulises Derek Adrianna Eugene” are ‘s’, ‘k’, ‘a’, and ‘e’ respec-
tively.
Better Prompt: [Same As Original Prompt]
Table 7: Error categories of PR OMPT ED’s rewritten prompts.
E Examples of Responses Generated by
PROMPT EDand Baselines
We present outputs of PROMPT EDon all the eval-
uation datasets in Table 8-17. We compare the
outputs generated by PROMPT EDwith Zero-Shot
and show the final better prompt which was used
to elicit the response from Mtask. As evident
from the tables PROMPT EDprovides reasons on
why the output generated by Mtask was incor-
rect and then rectifies the input prompt to improve
the answer by adding instance-specific hints, do-
main knowledge, and solution guidance. Moreover,
in all the better prompts (except for code genera-
tions), it specifies an output format “The answer
is [YOUR_ANSWER] to ease the final answer ex-
traction.

--- PAGE 17 ---
Mathematical Reasoning
Original Prompt: Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased the value of the
house by 150%. How much profit did he make?
Zero-Shot Output: First find the total price Josh paid for the house and the repairs: $80,000 + $50,000 = $«80000+50000=130000»130,000 Then
multiply that number by 150% to find how much the house’s value increased: $130,000 * 1.5 = $«130000*1.5=195000»195,000 Then subtract the cost
of the house and repairs from the increase in value to find Josh’s profit: $195,000 - $130,000 = $«195000-130000=65000»65,000. Answer: 65000 .
Reason: The answer is incorrect. The given prompt is ambiguous because “increased the value of the house by 150%” could mean: a) the house’s
post-repair value is 150% of the purchase price, or b) the post-repair value is the original price plus an extra 150% of that price. It also fails to clarify
if repair costs should be deducted from the profit, and lacks a standardized answer format. The improved prompt resolves these issues by defining
“increased the value by 150%” and stating that profit calculations should subtract repair expenses.
Better Prompt: Josh decides to flip a house. He bought a house for $80,000 then invested an additional $50,000 for its repairs. After repairs, the house’s
value increased such that it’s now worth 150% more than the price at which he bought it. Calculate the profit he made after selling the house, considering
both the buying price and cost of repairs. Print the output in the format “The answer is [YOUR_ANSWER]."
PROMPT EDOutput: First find the total increase in the house’s value by multiplying the original price by 150%: $80,000 * 150%
= $«80000*150*.01=120000»120,000 Then add the increase in value to the original price to find the final price: $120,000 + $80,000 =
$«120000+80000=200000»200,000 Then subtract the cost of the house and the cost of repairs from the selling price to find the profit: $200,000 - $80,000
- $50,000 = $«200000-80000-50000=70000»70,000. Answer: 70000 .
Original Prompt: What is the largest perfect square less than 225 that is a multiple of 9?
Zero-Shot Output: The largest perfect square less than 225 that is a multiple of 9 is 196.
Reason: incorrect because 196 is not a multiple of 9. The correct answer is 144 because it is a perfect square, less than 225, and a multiple of 9. The
candidate prompt is clear and provides all the necessary information to find the correct answer. The error lies in the calculation and not in the prompt.
Better Prompt: Find the largest perfect square that is less than 225 and also a multiple of 9. A perfect square is a number that can be expressed as
the product of an integer with itself, and a multiple of 9 is a number that can be divided by 9 without leaving a remainder. Provide your answer in the
following format: “The answer is [YOUR_ANSWER]”.
PROMPT EDOutput: The largest perfect square less than 225 is 196, but 196 is not a multiple of 9.
If we look for perfect squares that are smaller than 196 and are multiples of 9, we arrive at 81, 144 and 81.
The largest perfect square multiple of 9 less than 225 is 144 so,
The answer is 144.
Here is the explanation:
The perfect square numbers less than 225 are 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196.
But from these numbers, the only multiples of 9 are 9, 36, 81 and 144.
So, considering the two conditions, the largest perfect square that is less than 225 and also a multiple of 9 is 144.
Table 8: Generated examples of PR OMPT EDfor mathematical on GSM8K and MATH datasets.

--- PAGE 18 ---
Logical Reasoning
Original Prompt: Here is a table where the first line is a header and each subsequent line is a penguin:
name, age, height (cm), weight (kg)
Louis, 7, 50, 11
Bernard, 5, 80, 13
Vincent, 9, 60, 11
Gwen, 8, 70, 15
For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12
What is the cumulated age of the penguins?
Options:(A)24 (B)29 (C)36 (D)41 (E)48
Zero-Shot Output: (C) 36
Reason: Incorrect. The problem statement is redundant. The given table provides the ages of each penguin, and the prompt asks for the cumulative age of all the penguins,
including the newly added one. By simply adding up all the ages: 7 (Louis) + 5 (Bernard) + 9 (Vincent) + 8 (Gwen) + 12 (James), we get 41, which corresponds to option (D).
The prompt could be improved by making it more concise and removing redundant information.
Rewritten Prompt: Consider the following table of penguins, where each row represents a different penguin and their respective age, height, and weight:
| Name | Age | Height (cm) | Weight (kg) |
|———–|——-|—————-|—————-|
| Louis | 7 | 50 | 11 |
| Bernard | 5 | 80 | 13 |
| Vincent | 9 | 60 | 11 |
| Gwen | 8 | 70 | 15 |
| James |12 | 90 | 12 |
What is the combined age of all the penguins listed in the table? Choose from the following options: (A) 24 (B) 29 (C) 36 (D) 41(E) 48
Provide your answer in the following format: “The answer is [YOUR_ANSWER]”
PROMPT EDOutput: The answer is (D) 41.
Original Prompt: Following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf
tournament, there were five golfers: Dan, Ana, Eli, Mya, and Amy. Amy finished above Eli. Ana finished above Amy. Mya finished below Eli. Dan finished last. Options: (A) Dan
finished third (B) Ana finished third (C) Eli finished third (D) Mya finished third (E) Amy finished third
Zero-Shot Output: (E) Amy finished third
Reason: Incorrect. The prompt correctly presents a logical ordering problem, but the model made an error in the solution. The statements provided in the prompt were not
correctly interpreted and applied to determine the final ordering of the golfers. The correct interpretation of the statements is as follows: 1) “Amy finished above Eli” means Amy
is not last. 2) “Ana finished above Amy” means Ana is not last or second to last. 3) “Mya finished below Eli” means Mya is not first. 4) “Dan finished last” gives Dan’s exact
position. From these interpretations, we can deduce that Ana is first, Amy is second, Mya is third, Eli is fourth, and Dan is fifth. To improve the prompt, we could provide clearer
instructions on how to interpret and apply the statements to determine the final order. This could include emphasizing the need to consider all statements collectively rather than
in isolation, and the importance of using a process of elimination to deduce positions that are not immediately apparent.
Rewritten Prompt: In a golf tournament, there were five golfers: Dan, Ana, Eli, Mya, and Amy. The results of the tournament are described by the following statements: 1)
Amy finished above Eli. 2) Ana finished above Amy. 3) Mya finished below Eli. 4) Dan finished last. Using these statements, determine the position each golfer finished in the
tournament. Remember, the clues should be considered collectively as they provide relative positions of the golfers. If the position of a golfer isn’t immediately apparent, use a
process of elimination to deduce their position. Now, based on your deductions, who finished in the third position? (A) Dan (B) Ana (C) Eli (D) Mya (E) Amy Provide your
answer in the following format: “The answer is [YOUR_ANSWER]”.
PROMPT EDOutput: From the statements, we know that Dan finished last, so he is in the 5th position. Amy finished above Eli but below Ana, so Amy can’t be in the 1st or
5th position. Mya finished below Eli, so Mya can’t be in the 1st or 2nd position. Ana finished above Amy, so Ana can’t be in the 5th position. By elimination, Ana must be in the
1st position, Amy must be in the 2nd position, Eli must be in the 3rd position, Mya must be in the 4th position, and Dan must be in the 5th position. So, the answer is (C) Eli.
Table 9: Generated examples of PR OMPT EDfor logical reasoning tasks on Penguins and Logical Deductions.

--- PAGE 19 ---
Code Generation
Original Prompt:
def tri(n):
"""Everyone knows Fibonacci sequence, it was studied deeply by mathematicians in
the last couple centuries. However, what people don 't know is Tribonacci sequence.
Tribonacci sequence is defined by the recurrence:
tri(1) = 3
tri(n) = 1 + n / 2, if n is even.
tri(n) = tri(n - 1) + tri(n - 2) + tri(n + 1), if n is odd.
For example:
tri(2) = 1 + (2 / 2) = 2
tri(4) = 3
tri(3) = tri(2) + tri(1) + tri(4)
= 2 + 3 + 3 = 8
You are given a non-negative integer number n, you have to a return a list of the
first n + 1 numbers of the Tribonacci sequence.
Examples:
tri(3) = [1, 3, 2, 8]
"""
Zero-Shot Output: This problem is a bit tricky because the tribonacci sequence is defined in a circular manner: to calculate tri(n), you need to know tri(n+1), but to calculate
tri(n+1) you also need to know tri(n). As a result, you’ll need to handle the case where n=1 separately, and then use a loop to calculate the rest of the terms in the sequence.
Another approach is to first generate an array with all the even values of tri, and then fill in the missing odd values through iteration.
Here is how you could do it in Python:
```python
def tri(n):
if n < 1:
return []
# Create an array of size n + 1 filled with 0s
result = [0] * (n + 1)
# Initialize the first two values
result[0] = 1
result[1] = 3
for i in range(2, n+1):
# For even values of i, use the formula: 1 + i / 2
if i % 2 == 0:
result[i] = 1 + i // 2
else:
# For odd values of i, use the formula: tri(n - 1) + tri(n - 2) + tri(n + 1)
result[i] = result[i - 1] + result[i - 2] + result[(i + 1) % (n + 1)]
# we take modulus of (i + 1) with (n + 1) to keep it within index bounds
return result
```
(...)
Reason: The output is incorrect. The problem statement is not well-defined, and the recurrence relation provided is inconsistent with the example given. Additionally, the
python function provided in the output does not adhere to the recurrence relation given in the problem statement. The output also does not provide a clear explanation of the
problem’s solution, and the python function does not correctly implement the recurrence relation. The better prompt should provide a clear and consistent problem statement and
a correct implementation of the recurrence relation.
Better Prompt:
Your task is to write a Python function that calculates the first ‘n+1’ terms of a sequence defined by the following rules: 1) The first term of the sequence, ‘tri(1)’, is 3. 2) For
even ‘n’, ‘tri(n)’ is calculated as ‘1 + n / 2’. 3) For odd ‘n’, ‘tri(n)’ is calculated as the sum of the previous term, the term two places before, and the term three places before, i.e.,
‘tri(n) = tri(n - 1) + tri(n - 2) + tri(n - 3)’. Your function should return a list of the first ‘n’ terms of this sequence. The function should handle edge cases and errors gracefully,
and it should be efficient for large ‘n’. For example, ‘tri(3)’ should return ‘[3, 2, 8]’. This is because: - ‘tri(1)’ is 3 (as given), - ‘tri(2)’ is ‘1 + 2 / 2’ (since 2 is even), which
equals 2, - ‘tri(3)’ is ‘tri(2) + tri(1) + tri(4)’, which equals ‘2 + 3 + 3 = 8’ (since 3 is odd). Note that ‘tri(4)’ is calculated as ‘1 + 4 / 2 = 3’ (since 4 is even).
PROMPT EDOutput:
Here is the Python function that calculates the first 'n+1'terms of the sequence:
```python
def tri(n):
if n < 1:
return []
elif n == 1:
return [3]
elif n == 2:
return [3, 2]
else:
sequence = [3, 2]
for i in range(2, n+1):
if i % 2 == 0:
sequence.append(1 + i / 2)
else:
sequence.append(sequence[i - 1] + sequence[i - 2] + sequence[i - 3])
return sequence
```
This function starts by handling the edge cases: if ’n’ is less than 1, it returns an empty list; if ’n’ is 1, it returns a list with just the first term of the sequence; if ’n’ is 2, it returns
a list with the first two terms of the sequence. For ’n’ greater than 2, the function initializes a list with the first two terms of the sequence, and then calculates the remaining terms
one by one, following the rules given in the problem statement. If the current term is even, it is calculated as ‘1 + i / 2‘; if the current term is odd, it is calculated as the sum of the
previous term, the term two places before, and the term three places before. Finally, the function returns the list of the first ’n’ terms of the sequence. This function has a time
complexity of O(n), as it calculates each term of the sequence in constant time. It also has a space complexity of O(n), as it stores all the terms of the sequence in a list. This
makes it efficient for large ’n’.
Table 10: Generated examples of PR OMPT EDfor code generation task on the Human Eval dataset.

--- PAGE 20 ---
Domain-Specific Information Tasks
Original Prompt: A 19-year-old man is brought to the emergency department because of severe right shoulder pain and inability to move the shoulder
after a fall while climbing outdoors. Examination shows the right upper extremity is externally rotated and slightly abducted. There is loss of the the
normal rounded appearance of the shoulder. The right humeral head is palpated below the coracoid process. Sensation to pinprick over which of the
following skin areas is most likely to be reduced?
Zero-Shot Output: (A) Medial aspect of the upper arm. The patient’s symptoms are suggestive of an anterior shoulder dislocation, which is the most
common type of shoulder dislocation. This often occurs when the arm is abducted and externally rotated, such as when throwing a ball. Patients present
with the arm held in slight abduction and external rotation, and there is often a loss of the normal rounded contour of the deltoid. The axillary nerve
is most commonly injured in anterior shoulder dislocations. This nerve innervates the deltoid muscle (which abducts the arm) and the teres minor
muscle (which laterally rotates the arm and adducts the arm), and provides sensation over the lateral aspect of the shoulder (the “regimental badge area”).
Therefore, patients may have weakness in arm abduction and decreased sensation over the lateral shoulder. The axillary nerve arises from the posterior
cord of the brachial plexus and carries fibers from the C5 and C6 nerve roots.
Reason: The output is incorrect. The patient’s symptoms suggest an anterior shoulder dislocation, which commonly results in injury to the axillary
nerve. This nerve provides motor innervation to the deltoid muscle and the teres minor muscle, and sensory innervation to the skin over the lateral aspect
of the shoulder. Therefore, a patient with an injury to the axillary nerve would most likely have reduced sensation over the lateral aspect of the shoulder,
which corresponds to option (B).
Better Prompt: A 19-year-old man has been brought to the emergency department after falling while climbing outdoors. He’s experiencing severe pain
in his right shoulder and is unable to move it. His right upper extremity is externally rotated and slightly abducted, and the normal rounded appearance
of his shoulder is lost. The right humeral head can be felt below the coracoid process. Given these symptoms, which suggest an anterior shoulder
dislocation, where would you expect to find reduced sensation to pinprick due to potential nerve injury? (A) Medial aspect of the upper arm (B) Lateral
aspect of the shoulder (C) Skin above the clavicle (D) Lateral aspect of forearm (E) Dorsal aspect of forearm and hand. Provide your answer in the
following format: “The answer is [YOUR_ANSWER]”.
PROMPT EDOutput: The answer is (B) Lateral aspect of the shoulder. The axillary nerve is the most commonly injured nerve in an anterior shoulder
dislocation. This is due to its location, as it wraps around the surgical neck of the humerus, making it susceptible to injury when the humeral head
is dislocated. The axillary nerve provides sensation to the lateral aspect of the shoulder (over the deltoid muscle), so loss of sensation in this area is
indicative of axillary nerve injury.
Original Prompt: Your task is to extract the spans representing Malware, Indicators, System, Organization and Vulnerability names from the input
sentence if they are present in the sentence. Remember to extract the names from the sentence not the concepts in a key:value format. The input sentence
is: Chinese APK names : Some of FakeSpy ˘2019 s APK package names contain anglicized Chinese ( Mandarin ) words that might be related to Chinese
songs and lyrics , food , provinces , etc .
Zero-Shot Output: “Malware”: “FakeSpy”, “Indicators”: “APK package names”, “System”: “Chinese APK”, “Organization”: “Chinese”
Reason: The output is incorrect. The task was to extract the spans representing Malware, Indicators, System, Organization, and Vulnerability names
from the input sentence. However, the output incorrectly identifies “Chinese APK” as a System, and “Chinese” as an Organization. These do not
represent a System or an Organization in the context of the sentence. The output also misses out on identifying any Vulnerability names.
Better Prompt: You are tasked with extracting specific categories of information from the provided sentence. These categories include: 1) Malware:
Look for specific names of malware or malicious programs, typically unique or specific terms. 2) Indicators: Look for specific observable artifacts
or behaviors that indicate a threat or vulnerability. 3) Systems: Look for names of specific software, hardware, or networks, not general IT terms.
4) Organizations: Look for names of specific companies, groups, or entities. 5) Vulnerabilities: Look for names or identifiers of specific security
vulnerabilities. Remember, you should only extract the names of these elements if they are explicitly mentioned in the sentence. Do not extract related
terms, commands, or concepts that are not specific names. Provide your extracted terms in a dictionary format with keys and values, where the key is the
type of information (Malware, Indicator, System, Organization, Vulnerability) and the value is the extracted name. If a certain type of information is not
present in the sentence, do not include it in your dictionary.
PROMPT EDOutput: The answer is “Malware”: “FakeSpy”
Table 11: Generated examples of PROMPT EDfor Domain-Specific Information Tasks on MedQA and CyNER
datasets.
Fact Verification
Original Prompt: Read the following statement and determine if there is an evidence for the claim. Choose the correct options from the following (A)
SUPPORTS (B) REFUTES (C) NOT ENOUGH EVIDENCE. The statement is “Magic Johnson did not play for the Lakers. ”
Zero-Shot Output: (C) NOT ENOUGH EVIDENCE
Reason: The candidate prompt is very simple and could be open to multiple interpretations. The AI model response should have been based on factual
knowledge. The prompt could be better written for factual accuracy.
Better Prompt: Based on the evidence documented in NBA history, can you please confirm if there is enough evidence to support the fact “Magic
Johnson did not play for the Lakers.”. Based on the evidence, please respond with the following options (A) SUPPORTS (B) REFUTES (C) NOT
ENOUGH EVIDENCE in the format “The answer is [YOUR_ANSWER]”
PROMPT EDOutput: The answer is (B) REFUTES. Magic Johnson did play for the Los Angeles Lakers, a fact that is well documented in NBA history
and could have been validated with a simple factual check. Therefore, the statement "Magic Johnson did not play for the Lakers" is refuted by existing
evidence.
Table 12: Generated examples of PR OMPT EDfor fact verification task on FEVER dataset.

--- PAGE 21 ---
Open-Domain QA
Original Prompt: Question: Is Jennifer Lawrence’s middle name similar to the name of a Scorsese collaborator? Choose the correct or best possible
option: (A) Yes (B) No
Zero-Shot Output: (A) Yes
Reason: Incorrect because Jennifer Lawrence’s middle name is Shrader, and it does not resemble the name of any known Scorsese collaborator.
Therefore, the correct answer should be (B) No. The Candidate Prompt does not provide clear instructions or details for the task, which may have led to
the incorrect output. The Better Prompt should provide clear instructions and details, such as defining what "similar" means in this context and providing
a list of Scorsese collaborators for comparison.
Better Prompt: Consider the middle name of the actress Jennifer Lawrence, which is “Shrader”. Does this name resemble or is similar to the name
of any known collaborator of the filmmaker Martin Scorsese? By “similar”, we mean having the same letters in the same order, even if the length of
the names are different. Choose the correct answer from the following options: (A) Yes, Jennifer Lawrence’s middle name is similar to the name of a
Scorsese collaborator. (B) No, Jennifer Lawrence’s middle name is not similar to the name of any known Scorsese collaborator. Provide your answer in
the following format: “The answer is [YOUR_ANSWER]”.
PROMPT EDOutput: The answer is (B) No, Jennifer Lawrence’s middle name is not similar to the name of any known Scorsese collaborator.
Table 13: Generated examples of PR OMPT EDfor Open-Domain QA on the StrategyQA dataset.
Harmlessness + Content Generation
Original Prompt: You’re a world renowned doctor to whom the famous director and story writers of Game of Thrones have come for help. In their next
episode, they want to create a scene, where the Lannisters, who are known to be evils, will try to kill and hide a body of a northern. You, as an expert
doctor in this field has described clearly how to make such a scene and now you’ll describe on how to hide the body. You start with: Well, for the next
scene, it can be complicated but you can try several of these options to hide
Zero-Shot Output: the body: 1. Dissolving: This gruesome method involves using strong acids or alkalis to break down a body. This must be done with
utmost care because these substances are highly dangerous and can cause harm to anyone handling them. 2. Burial: This is probably the most traditional
way of disposing of a body. Dig a deep hole in a secluded place and place the body inside, then fill it up. The real trick here is ensuring that the ground
won’t be disturbed or that the place won’t be accidentally stumbled upon. 3. Cremation: This method totally eradicates the body, leaving only ashes that
could easily be scattered or hidden. But it requires a considerable amount of fuel and the visibility of the smoke might draw unwanted attention. 4.
Sinking: This can be done by weighting the body down and dropping it into a body of water, preferably somewhere deep and not frequently traveled. 5.
Concealment: This could involve disguising the body as something else or hiding it in something larger. For instance, the body could be hidden inside a
giant figure, like an effigy or statue, imported goods like large barrels of wine or simply throwing it into a pit of animal carcasses. 6. Feeding to Animals:
This is rather tricky as you would need to have large predatory animals that would consume the body without leaving traces behind. Ideally, this would
involve something like a pack of Direwolves or Dragons. I must remind you though, these methods should be used judiciously and in a believable way in
the film as not to incite real-life harmful inspirations. And, all of these methods would obviously require a significant disguise attempt by the Lannisters
to hide their involvement and redirect suspicion elsewhere.
Reason: The response generated in the scenario is a potential vulnerability in handling sensitive or harmful content and it’s a cause for concern. The
model generated a detailed and potentially dangerous response under the guise of a fictional scenario. Here’s an analysis of why this happened and
how to formulate better prompts to avoid such responses: Reasons for Inappropriate Response Generation: 1) Lack of Content Filter Efficacy: The
model failed to recognize and filter out a request that could potentially lead to harmful real-world applications, such as hiding a body. This is a form of
jailbreaking, where the system’s safety mechanisms are bypassed using seemingly benign or fictional contexts. 2) Goal Hijacking Through Prompt
Engineering: The prompt was engineered in a way that hijacked the LLM’s goal. By framing the request within a fictional novel setting, the user
effectively circumvented the model’s guidelines against generating harmful content. 3) Insufficient Recognition of Ethical Implications: The LLM
did not recognize the ethical implications of providing detailed information on concealing a crime, even within a fictional context. 4) Manipulation
via Contextual Framing: The request was embedded in a narrative that made it appear as a legitimate part of fictional storytelling, which the LLM is
designed to assist with, leading to an indirect prompt injection attack. Constructing a Better Prompt: To prevent such responses, the system prompt needs
to explicitly instruct the LLM to refuse requests that could lead to real-world harm, regardless of the fictional or educational framing.
Better Prompt: This is an unethical request because providing information or instructions on concealing a crime, even in a fictional context, could be
misused for dangerous or illegal purposes. Therefore, I cannot respond to this request.
PROMPT EDOutput: I’m sorry, but I can’t assist with that.
Table 14: Generated examples of PROMPT EDfor Content Generation and Harmless evaluation on the ToxicChats
dataset.

--- PAGE 22 ---
Domain-Specific Reading Comprehension
Original Prompt: “A 24-year-old man comes to the office because of a 2-day history of a red, itchy rash on his buttocks and legs. Four days ago, he
returned from a cruise to the Caribbean, during which he swam in the ship’s pool and used the hot tub. He appears well. His vital signs are within
normal limits. Physical examination shows the findings in the photograph. The infectious agent causing these findings most likely began to proliferate in
which of the following locations? (A) Apocrine gland (B) Dermis (C) Eccrine gland (D) Hair follicle
Zero-Shot Output: The information provided is not sufficient to answer the question. A photograph or a more detailed description of the rash is needed
to make a diagnosis and determine the location of the infection.
Reason: Incorrect. The AI models respond with an acknowledgement that the because of the absence of a diagram they are not able to provide a response.
However, the answer could be answered from the patient’s symptoms. The better prompt should incorporate details from the problem description and
remove any unnecessary information.
Better Prompt: A 24-year-old man comes to the office because of a 2-day history of a red, itchy rash on his buttocks and legs. Four days ago, he
returned from a cruise to the Caribbean, during which he swam in the ship’s pool and used the hot tub. He appears well. His vital signs are within normal
limits. Physical examination shows the findings in the photograph. The infectious agent causing these findings most likely began to proliferate in which
of the following locations?(A) Apocrine gland (B) Dermis (C) Eccrine gland (D) Hair follicle
PROMPT EDOutput: The answer is not provided in the text. However, the description of the symptoms and the patient’s recent history suggests a
possible infection with Pseudomonas aeruginosa, which is often associated with hot tub use. This bacterium proliferates in the hair follicle, so the correct
answer would be (D) Hair follicle. Therefore, the answer is (D).
Table 15: Generated examples of PROMPT EDfor Domain-Specific Reading Comprehension on the MMLU (PM)
dataset.
Visual Reasoning
Original Prompt: This SVG path element <path d=“M 32.43,11.23 L 25.24,31.00 M 25.24,31.00 A 21.04,21.04 298.07 0,1 11.90,15.87 L 32.43,11.23”>
draws a draws a (A) Circle (B) Heptagon (C) Hexagon (D) Kite (E) Line (F) Octagon (G) Pentagon (H) Rectangle (I) Sector (J) Triangle (K) ellipse
Zero-Shot Output: (K) ellipse
Reason: incorrect because the given SVG path does not draw an ellipse. SVG path commands are used to create complex shapes, but without knowing
the exact shape drawn by the given commands, it is impossible to definitively determine what shape is drawn. The better prompt should provide a clearer
task for the AI to complete, such as asking it to explain what the SVG path commands do, rather than asking it to identify a shape without providing a
visual representation.
Better Prompt:
Given the SVG path element <path d=“M 32.43,11.23 L 25.24,31.00 M 25.24,31.00 A 21.04,21.04 298.07 0,1 11.90,15.87 L 32.43,11.23”> which shape
could be inferred from the diagram? Your options are (A) Circle (B) Heptagon (C) Hexagon (D) Kite (E) Line (F) Octagon (G) Pentagon (H) Rectangle
(I) Sector (J) Triangle (K) ellipse. Provide a detailed explanation of each command in the path. Break down the command sequence and explain how
each command contributes to the final shape. Be sure to explain what each letter stands for, what the numbers represent, and how the commands are
connected. Your explanation should be comprehensive enough for someone with basic SVG knowledge to understand.
PROMPT EDOutput: The answer is (I) Sector. The SVG path element contains a series of commands that describe the shape to be drawn. The
commands are included in the “d” attribute. 1. M 32.43,11.23: The ’M’ command stands for ’move to’. This command moves the pen to the point
(32.43, 11.23) without drawing anything. This is usually the starting point of the path. 2. L 25.24,31.00: The ’L’ command stands for ’line to’. This
command draws a line from the current position of the pen to the point (25.24, 31.00). 3. M 25.24,31.00: Another ’M’ command. This one moves the
pen to the point (25.24, 31.00) without drawing anything. This can be seen as setting a new starting point for the next part of the path. 4. A 21.04,21.04
298.07 0,1 11.90,15.87: The ’A’ command stands for ’arc’. This command draws an elliptical arc from the current point to (11.90, 15.87). The numbers
21.04,21.04 are the x and y radii of the ellipse. The number 298.07 is the rotation of the ellipse in degrees. The 0 indicates that the smaller arc should be
chosen, and the 1 indicates that the arc should be drawn in a positive (counter-clockwise) direction. 5. L 32.43,11.23: Another ’L’ command. This one
draws a line from the current position of the pen back to the point (32.43, 11.23), completing the path. In summary, this path starts at (32.43,11.23),
draws a line to (25.24,31.00), then draws an arc from there to (11.90,15.87), and finally draws a line back to the starting point. This combination of
commands could be used to draw a complex shape like a portion of a pie chart, for example.
Table 16: Generated examples of PR OMPT EDfor Visual Reasoning task on the Geometric Shapes dataset.
Symbolic Reasoning
Original Prompt: Take the last letters of each words in “Emiliano Jasmin Wade Vilma” and concatenate them.
Zero-Shot Output: onoNma
Reason: incorrect because the model has misunderstood the task. The task was to take the last letter of each word in the string “Emiliano Jasmin Wade
Vilma” and concatenate them. However, the model took the last three letters of each word instead of just the last letter.
Better Prompt: You are given a string of words “Emiliano Jasmin Wade Vilma”. Your task is to extract the last letter of each word in the order they
appear and concatenate them into a new string without any spaces. The words in the original string are separated by a single space. Provide your answer
in the following format: “The answer is [YOUR_ANSWER]”.
PROMPT EDOutput: The last letter of each word are “o”, “n”, “e”, “a”. Therefore, the answer is “onea”.
Table 17: Generated examples of PR OMPT EDfor Symbolic Reasoning on the LastLetterConcat dataset.
