# 2304.07840.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/prompt/2304.07840.pdf
# File size: 1122821 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Enhancing Automated Program Repair through
Fine-tuning and Prompt Engineering
Rishov Paul∗, Md. Mohib Hossain∗, Mohammed Latif Siddiq†
Masum Hasan‡, Anindya Iqbal∗, and Joanna C. S. Santos†
∗Department of Computer Science and Engineering, BUET, Dhaka, Bangladesh
†Department of Computer Science and Engineering, University of Notre Dame, USA
‡Department of Computer Science, University of Rochester, USA
{rishov.paul, mdmohib.hossain }@iqvia.com, msiddiq3@nd.edu
m.hasan@rochester.edu, anindya@cse.buet.ac.bd, and joannacss@nd.edu
Abstract —Sequence-to-sequence models have been used to trans-
form erroneous programs into correct ones when trained with
a large enough dataset. Some recent studies also demonstrated
strong empirical evidence that code review could improve the
program repair further. Large language models, trained with
Natural Language (NL) and Programming Language (PL), can
contain inherent knowledge of both. In this study, we investigate
if this inherent knowledge of PL and NL can be utilized to
improve automated program repair. We applied PLBART and
CodeT5, two state-of-the-art language models that are pre-
trained with both PL and NL, on two such natural language-
based program repair datasets and found that the pre-trained
language models fine-tuned with datasets containing both code
review and subsequent code changes notably outperformed each
of the previous models. With the advent of code generative models
like Codex and GPT-3.5-Turbo, we also performed zero-shot
and few-shots learning-based prompt engineering to assess their
performance on these datasets. However, the practical application
of using LLMs in the context of automated program repair is
still a long way off based on our manual analysis of the generated
repaired codes by the learning models.
Index Terms —automated program repair, pre-trained trans-
former model, code review, prompt engineering, GPT3
I. I NTRODUCTION
Code review is the process of analyzing source code that
has been written by a collaborator in order to determine
whether or not it is of sufficient quality to be merged into
the main source code repository [1]. Code review provides
several advantages, including improving the overall quality of
the code and decreasing the likelihood of introducing errors
into the system [2], [3].
The defects identified by code reviewers, testers, or static
analysis tools need to be fixed within a short deadline before
the release of the software. However, repairing defects in a
program is time-consuming and expensive. In fact, this process
accounts for nearly half of the total cost and time of software
development [4]. Hence, automation of code repair can be
highly beneficial for the software development sector.
Traditional automatic program repair approaches fix a program
using test suites [5]–[8]. However, it still takes extra work
to construct these test suites. Alternative approaches, such asstatic analysis-based [9], [10] and learning-based automated
code repair techniques [11]–[13], have yet to achieve accept-
able results. This has motivated researchers to develop solu-
tions using code review suggestions to achieve a better quality
of bug fix suggestions [14], [15] . They have established that
when a defective (“buggy”) code is given to repair, there is
a performance boost if review comments are given alongside
it. Since code review is a common practice [16], using them
requires no additional resources. Despite considerable and
promising improvement, the learning-based models presented
in [14], [15] could not achieve sufficient ability to be used
in industry-level code repair. For example, the accuracy of
baseline models for the corresponding datasets is around 12%
to 20%. Therefore, this research direction needs much further
exploration to advance the state-of-the-art with the current
techniques.
Recent works have successfully used transformer-based [17]
pre-trained models for different relevant software engineering
tasks such as code summarization, code search, code docu-
mentation, code refinement etc.[18]–[21]. These models are
trained on large corpora to acquire universal language repre-
sentations. They may then be used for downstream NLP (Nat-
ural Language Processing) tasks without having to train new
models from scratch, for which, nowadays, the transformer
has become now the standard pre-trained model architecture.
Hence, it is important to study whether these models can
improve the results of program repair by effectively utilizing
code review along with the associated code context.
Additionally, recent progress on large language models
(LLMs) [22], like GPT-3.5, has demonstrated excellence in
producing code from well-formulated prompts. With the in-
creasing popularity of LLMs, prior works have investigated
the correctness of the generated code [23], their quality (in
terms of code smells) [24], security [25] as well as whether
it can be used for API learning tasks [26], code complexity
prediction [27]. Since they have exhibited strong zero-shot [28]
and few-shot [29] learning on many tasks [30], [31], this opens
up a new path to explore automated code repair using prompt
engineering, where researchers develop methods to craft clear
1arXiv:2304.07840v2  [cs.LG]  21 Jul 2023

--- PAGE 2 ---
and concise prompts and use such models to obtain coherent
and relevant responses.
In light of this, it is clear that there is a need for different
techniques for advancing the state-of-the-art in repairing buggy
code identified during the code review process. Specifically,
our motivation is to explore the program repair capability of
pre-trained models where the prompt will be crafted using both
the buggy code and its code review. Thus, in this paper, we
investigate the following research questions:
RQ1 How do pre-trained models perform in repairing bugs
identified in the code review process?
RQ2 How effective is automated program repair using zero-
shot and few-shot learning-based prompt engineering
on Large Language Models?
RQ3 How effective are language models in repairing bugs
identified in the code review process from a developer’s
perspective?
Our work focuses on repairing buggy Java code identified dur-
ing the code review process. In the first research question, we
compare two pre-trained transformer models (PLBART [18]
and CodeT5 [19]) on the datasets of previous studies [14],
[15] by fine-tuning them with the buggy codes, their fixes,
and corresponding code reviews. For the second research
question, we investigated how two LLMs (GPT-3.5-Turbo [32]
and Code-DaVinci-Edit-001 [33]) perform on these datasets
using zero-shot and few-shots prompting. In the last research
question, we manually investigated the output from the fine-
tuned models and prompted LLMs to see how the repaired
program aligned them with the code review.
The contributions of our work are:
•Validation of the significant improvement of code repair
using large language models pre-trained with NL and PL
from the Tufano et al. [15] dataset and the Review4Repair
dataset
•Discussion on how the architecture and pre-trained weights
contribute towards the code repair performance boost.
•Comparison of the performance of two pre-trained models,
PLBART andCodeT5 , in terms of accuracy.
•A comprehensive investigation of two LLMs ( GPT-3.5-
Turbo , and Code-DaVinci-Edit-001 ) for zero-shot and few-
shot code repair with the help of prompt engineering.
•Manual analysis of the repaired codes to understand the
actual capabilities of the learning models.
•A replication package with all the scripts used to gather the
data and results1
1https://doi.org/10.5281/zenodo.8122636II. B ACKGROUND
This section explains concepts that are relevant to understand
this paper.
A. Code Reviews and Automated Program Repair
Code review [1] is a software quality assurance activity in
which one or more developers analyze a peer developer’s
source code by viewing or reading the code parts after im-
plementing a feature or fixing a defect. During this activity, a
reviewer may identify bugs in the code. For instance, Listing 1
has a source code under review. This example is taken from a
dataset from a prior study [15], which includes the <START >
and<END>tags to indicate where a reviewer made a comment
to repair a bug. The reviewer states that the ifcondition in
line 2 “ could be simplified ”. Thus, the developer fixes the code
as shown in the second snippet in Listing 1.
Code during review
1public boolean accept (Issue issue ) {
2 <START> if ( issueShouldNotBeReported (issue ,excludedLinesByRule ())) { <END>
3 return false;
4 }
5 return true;
6}
Fixed code based on the review
1public boolean accept (Issue issue ) {
2 return ! issueShouldNotBeReported (issue ,excludedLinesByRule ());
3}
Listing 1: Example of a buggy code snippet for review.“could be simplified”
While code review relies on human expertise to identify
and repair issues, automated program repair (APR ) [34]
techniques aim to automatically fix software bugs without
the developer’s intervention [35], [36]. APR is also referred
to as automatic patch generation ,automatic bug repair ,
and automated code repair . Henceforth, we will use the
terms automated code repair andautomated program repair
interchangeably.
By combining both code reviews andAPR techniques, devel-
opers can leverage the strengths of each to enhance the overall
quality of the code. In this work, we focus on studying how
language models can automate the repair of bugs that were
identified during code review.
B. LLMs, Zero Shot and Few Shot Prompting
ALarge Language Model (LLM) [22] refers to a sophis-
ticated artificial intelligent model which consists of a neu-
ral network with tens of millions to billions of parameters.
LLMs are trained on vast amounts of unlabeled text using
self-supervised learning or semi-supervised learning [37]. As
opposed to being trained for a single task (such as senti-
ment analysis or mathematical reasoning), LLMs are general-
purpose models that excel in a variety of natural language pro-
cessing tasks, including language translation, text generation,
question-answering, summarization, and much more. GPT-
3 [37], BERT [38], T5 [39], CodeBERT [21] are examples
of well-known LLMs.
2

--- PAGE 3 ---
To direct a model’s answer generation, one must carefully
craft input instructions. The act of constructing and enhancing
prompts to produce desired outputs is known as prompt
engineering [40]. When engineering a prompt, one may in-
clude a few input-output example pairs ( few-shot prompting )
or simply have a high-level description about the desired
task ( zero-shot prompting ). A model is able to successfully
complete a task due to zero-shot and few-shot learning, which
are learning techniques that address the challenge of training
models with limited training data.
Zero-shot learning is the capacity of a machine learning
model to carry out a task without any explicit examples
or labeled data for that specific task during training [28],
[41]. Large-scale code repositories specific to the target pro-
gramming language are often used to train traditional code
generation models. In this process, the models learn the
specific syntax and semantics of that particular programming
language. With the help of zero-shot learning, the models can
be designed to generalize their understanding across various
programming languages. It uses the shared concepts and
patterns among many programming languages for a target
language it has not seen during the training process.
Few-shot learning [29] is a method where the model is trained
with a limited dataset. Unlike the common practice of ML
models, where the models are fed as much data as possible,
few-shot learning aims to generate a model’s prediction with
less training data. Few-shot learning allows the model to
generalize and make accurate predictions on new classes with
only a few examples available for each class.
To clarify the differences between zero-shot and few-shot
prompting, consider the prompt in Listing 2. On one hand,
lines 1–24 are a case of few-shot prompting ; it includes three
examples of a buggy code, its corresponding review, and fix as
well as an explicit instruction that tells the model to refactor
(fix) the code based on the provided review (line 24). On
the other hand, if the prompt only has the lines 13–24 (high-
lighted) then it is an example of zero-shot prompting .
Prompt_example.java
1Buggy Code: <Buggy Code 1>
2Review: just return this
3Fixed Code: <Fixed Code 1>
4
5Buggy Code: <Buggy Code 2>
6Review: Just return rule.
7Fixed Code: <Fixed Code 2>
8
9Buggy Code: <Buggy Code 3>
10Review: Can't we just rely on @Rule?
11Fixed Code: <Fixed Code 2>
12
13Buggy Code:
14private FirewallRule findById (List <FirewallRule >collection ,String id ) {
15FirewallRule result = null;
16 for (FirewallRule rule :collection ) {
17 if (rule .id().equals (id)) {
18 <START> result =rule ;<END>
19 }
20 }
21 returnresult ;
22}
23Review: Just return rule.
24Refactor the Buggy Code using the Review without comments.
Listing 2: Zero-shot and few shots prompt example.III. M ETHODOLOGY
Figure 1 provides an overview of our study. To answer RQ1 ,
we collected buggy code and their code reviews from two
datasets (Tufano et al. [15] and Review4Repair [14]) to fine-
tune two pre-trained models (PLBART and CodeT5). For
RQ2 , we used the same datasets in RQ1 and prompt engineer-
ing with two LLMs (GPT-3.5-Turbo and Code-DaVinci-Edit-
001). Finally, two developers conducted a manual analysis of
the output of the models to check the alignment in addressing
the code review in the repaired program ( RQ3 ). The next
subsections explain each of these steps in detail.
A. RQ1: Fine-tuning Pre-trained Models for APR
1) Dataset Collection and Preprocessing: We used two
datasets for repairing codes using code reviews. An overview
of each dataset is given in Table I. Both datasets are from
recent prior works [14], [15] and consist of real examples of
code reviews collected from Gerrit and GitHub. We prepro-
cessed each dataset as follows:
•Tufano et al. [15] dataset : It contains 17,194 samples
of buggy code, their corresponding fixes, and code reviews
collected from Gerrit and GitHub. Additionally, each buggy
code has two special tokens ( <START >and<END>) to
encapsulate the erroneous code block. Similar to another
study [14], during the dataset preprocessing, we concate-
nated the buggy code and its respective code review into
a single line, with the code review encapsulated using the
tags<|startcomment| >and<|endcomment| >. These
concatenated snippets were the models’ input, and their
respective fixed codes were the target for the PLBART and
CodeT5 models. We also classified the entire dataset into
three fix categories: Insert ,Delete , and Update . These cat-
egories indicate whether the fixes only added new changes
(insert ), removed code blocks ( delete ), or both ( update ).
•Review4Repair dataset [14] : It contains a total of 56,2112
and2,961 samples that were used for training and testing in
their study, respectively. These samples were collected from
Gerrit. Since the maximum input length was not more than
512 tokens for both pre-trained models, we had to remove
57 samples from the training dataset and 6 samples from
the test dataset as these samples had more than 512 tokens.
Hence, the initial training dataset contained 56,154 samples,
and the test dataset contained 2,955 samples. Since fine-
tuning pre-trained models also require a validation dataset,
which was not present in this dataset, we reorganized the
initial training dataset to ensure that 90% of samples are
in the training dataset, 5% of the samples are in the test
dataset, and 5% of the samples are in the validation dataset.
Thus, we had 53,198 samples in the training dataset, 2,956
samples in the validation dataset, and 2,955 samples in the
test dataset in our modified dataset. We also categorized the
samples into three categories ( Insert ,Update , and Delete ).
2The paper mentioned 55,060 training samples [14], but the replication
package contains 56,211 samples.
3

--- PAGE 4 ---
Reviewer
Commits
buggy
code
DeveloperBuggy
Code
Code
ReviewWrites review
based on the
codeDatasetPre-processing
Prompt
EngineeringLLMs
GPT-3.5-T urbo
Code-DaV inci-Edit-001
Apply
Heuristics
Developer
Analysis
Fixes the
code based
on the reviewFixed
Code
Fixed
CodeRQ1
Fine-tuned Models
(PLBART , CodeT5)
Fixed
Code
RQ2RQ3Fig. 1: Overview of the Methodology.
To fit each sample on a single line, extra spaces and newlines
were removed from each sample’s code and comments in
both the training dataset and test dataset samples. Similar
to the Tufano et al. [15] dataset, the buggy code had two
special tokens, <|startfocus| >and<|endfocus| >, to
encapsulate the erroneous code block. In contrast to the
whole fixed code for the target in the Tufano et al. [15]
dataset, the target for each buggy code was merely the
repair code snippet between the <|startfocus| >and
<|endfocus| >special tokens. The target for the delete
class samples was an empty whitespace, which we replaced
with a special token, <|del|> .
TABLE I: Overview of the Datasets.
Dataset Type Insert Delete Update Total
Review4Repair [14]Train 8,718 4,060 40,420 53,198
Validation 247 481 2,228 2,956
Test 222 425 2,308 2,955
Tufano et al. [15]Train 161 4,385 9,210 13,756
Validation 20 540 1,159 1,719
Test 18 559 1142 1,719
2) Experimental Setup for Fine-tuning the Models: After
dataset preprocessing, we fine-tuned both the PLBART [18]
and the CodeT5 [19] models to automatically repair buggy
code in a code review. For the PLBART model, we set the
input length to 512 and the target length to 200 for both
of the datasets. The settings for all other hyperparameters
were identical to the standard PLBART configuration [18].
Moreover, we used three beam sizes ( 1,5, and10) to generate
theTop-1 ,Top-5 , and Top-10 predictions, respectively, as
we experimented with various numbers of epochs to see the
optimal results. For the Review4Repair dataset [14], we used
11epochs because we found that the model’s performance
remains unchanged after 11 epochs. The epoch was set to
12for the Tufano et al. dataset [15] as well because the
model’s performance did not increase after epoch 12. We set
the hyper-parameter, patience value to 10 epochs to observe
this. We ran these experiments in a local environment using
anNVIDIA GeForce RTX 2070-8GB GPU.
The batch size in the CodeT5 model was set to 4, and the
accumulated gradient steps was set to 8. Furthermore, thedefault batch size of 32 is ensured by the combination of
batch size and accumulated gradient steps. Moreover, the
model was fine-tuned for 45epochs for both datasets based
on observing validation losses. We varied the hyperparameter,
number return sequences to1,5, and10to generate Top-1 ,
Top-5 , and Top-10 predictions, respectively.
After tokenizing the Tufano et al. [15] dataset, we observed
that the maximum length of the source sequences was 590
tokens, and the maximum length of the target sequences was
194 tokens. Because the maximum input sequence length for
the CodeT5 model was 512, we set the model input length to
512 and set the model output length to 200.
Similarly, after tokenizing the Review4Repair dataset [14], we
observed that the maximum length of the source sequences
was 561 tokens, and the maximum length of the target
sequences was 116 tokens. As previously stated, we set the
model input length to 512 here as well, and because the
maximum target length of the sequences was 116, we set the
model output length to 200.
B. RQ2: Prompt Engineering for APR
In this section, we describe how we applied prompt engi-
neering for both of the datasets. Next, we describe how we
performed zero shot [28] and few shot [29] prompt engineering
with GPT-3.5-Turbo and zero-shot prompting with Code-
DaVinci-Edit-001. We also detail on the heuristics used for
modifying the response to fix common errors in the response
from the models.
1) Models: We used two models available via OpenAI API
for zero-shot prompt engineering. On the one hand, the GPT-
3.5-Turbo is the most effective and affordable model in the
GPT-3.5 family [37]. Although GPT-3.5-Turbo is optimized
for chat, it also performs well for code completion tasks. On
the other hand, Code-DaVinci-Edit-001 [33] is another variant
of Codex [42] GPT-3 model with editing capabilities that
are specifically designed to assist with various programming-
related tasks by giving instructions, including fixing code
errors, completing code snippets, suggesting edits in a code
snippet etc.Given a code and an instruction in natural lan-
4

--- PAGE 5 ---
guage, the model edits the code to comply with the instruction
as close as possible.
For the few-shot prompting, we only used the GPT-3.5-Turbo
model. Since few-shots prompting gives model information
about the input-output structure, it has no relation with the
targeted downstream tasks. As GPT-3.5-Turbo is a generalized
model for branches of tasks, giving input-output structure
with few-shots prompting can be helpful [43]. However, for
the Code-DaVinci-Edit-001, there is already a fixed structure
i.e., input code, instruction, and output code. Hence, there is
no need for examples to make the model understand the IO
structure.
2) Zero-shot Prompt Creation: Proper and well-crafted
prompts are crucial for getting the desired response from
generative models like GPT-3.5-Turbo. For zero-shot prompt
creation, we used the buggy code and the review from the
respective datasets. We clearly mentioned each portion by
identifying “Buggy Code” and “Review” in the prompt to
make sure the model can discriminate between the buggy code
and the associated review. Then we added an explicit command
to fix the buggy code “Refactor the Buggy Code using the
Review without comments” . The “without comments” clause
was added to the prompt in order to ensure the response
from the model does not contain any redundant or explanatory
comments that were not present in the input buggy code,
thus further guiding the model to produce a desired outcome.
Listing 2 shows the layout of the prompt for this scenario,
with lines 13 through 22 designating the buggy code, line 23
designating the review associated with the buggy code, and
line 24 designating the explicit command for the model to
generate the fixed code respectively.
For the Code-DaVinci-Edit-001 model, we needed to pass
the buggy source code as the input of the model, and for
instruction parameter, we passed a natural language instruction
i.e., “Refactor the code using the Review: <specific code
review >.”
3) Few-shot Prompt creation: For few-shot prompt creation,
we needed to do some extra tasks. Firstly, we vectorized both
the train and test dataset reviews using TF-IDF [44]. Then we
calculated the cosine similarity [45] score for each test sample
with respect to every training sample. Next, we selected the
three highest-ranked reviews from the training dataset and
their respective buggy code, fixed code, to create the prompt
for each test sample for the few-shot procedure. We aimed
to feed the model three most relevant examples containing
Buggy Code ,Review , and Fixed Code so that it could have
some background knowledge while predicting the fixed code
for each test sample. The later part of the prompt was the
same as the zero-shot prompt. The structure of this few-shot
prompt is given in Listing 2.
4) Repaired Code Generation: Both GPT-3.5-Turbo and
Code-DaVinci-Edit-001 models are available via the OpenAI
API. By following recent studies [24], [46]–[48] for both
models, we set the temperature parameter to zero because
lower temperatures cause the output to be more concentratedand deterministic. In contrast, higher temperatures cause the
output to be more random. Other parameters such as topp,
frequency penalty ,presence penalty were set to their default
settings, and they were 1, 0, 0, respectively.
The GPT-3.5-Turbo model has three distinct roles: assistant ,
system , and user. Following the guidelines outlined in prior
works [31], we set the content for the system role as “You
are a coding assistant. You generate only the source code. ”
The content for the system role helps the model to shape the
personality of the assistant or how it should behave for output
generation. The prompt mentioned in the previous section was
set as the content for the user role. Finally, the assistant
role provides the fixed code as the response. For the Code-
DaVinci-Edit-001 , we have the repaired code in the output
directly.
5) Heuristic-based Analysis of the Generated Repairs: We
generated all the fixed codes with their respective buggy
code alongside reviews with a fixed user prompt for all the
datasets. We observed for the GPT-3.5-Turbo model that, at
first, the accuracy of generating fixed code was low. However,
the predicted code was somewhat similar to the target code.
We also observed that LLMs (i)generate repairs that had
trivial syntax problems; (ii)add an explanation of the code
at the end; (iii) generate the buggy code and fixed code
together; (iv)add a prefix java at the first of the code; (v)
add a title before generating fixed code such as “Refactored
Code” ,“Fixed Code” etc. (vi) added extra spaces that were
not needed; (vii) enclosed the fixed code within backticks
```. However, we could easily extract the fixed code from
the response through heuristics. Hence, similar to a recent
study [31], we developed five heuristics to automatically fix
the aforementioned issues:
H1 Adjust space : Following the structure of the target code
of Tufano et al. [15] dataset, we needed to modify the
response of the LLMs by removing the newlines and
remove the extra spaces
H2 Code explanation removal :GPT-3.5-Turbo sometimes
explains the whole code after the fixed code generation
using some keywords such as Explanation ,Reasoning ,
and Changes Made . Hence, the heuristic removes the
code explanation automatically at the end alongside the
keywords.
H3 Remove starts with java :GPT-3.5-Turbo often mentions
the language of the code in its response, Since our datasets
only had java codes, we applied a heuristic to remove the
first part of the response that starts with java.
H4 Remove redundant keywords : It removes the keywords
such as Refactored code ,Corrected code ,Updated code
etc. at the beginning of the response. Also, as we had
<START >and<END>in our buggy code to specify the
code block to fix, GPT-3.5-Turbo sometimes predicts it
also in the response, which was removed as they were
redundant.
5

--- PAGE 6 ---
TABLE II: Comparison of the fine-tuned PLBART and CodeT5 models on each dataset with the respective baseline models.
Dataset Model Name Top-1 Accuracy (%) Top-5 Accuracy (%) Top-10 Accuracy (%) BLEU-4 (%) CodeBLEU (%)
Review4Repair [14]R4R CC 19.59 baseline 27.73 baseline 31.51 baseline 24.66 baseline 39.30 baseline
Fine-tuned PLBART 25.28 +5.69 37.29 +9.56 41.42 +9.91 40.97 +16.31 49.60 +10.3
Fine-tuned CodeT5 29.82 +10.23 37.73 +10.0 39.96 +8.45 45.98 +21.32 53.19 +13.89
Tufano et al. [15]Tufano 2-encoder 12.16 baseline 24.55 baseline 30.72 baseline 81.80 baseline 80.52 baseline
Fine-tuned PLBART 32.98 +20.82 47.12 +22.57 51.13 +20.41 87.55 +5.75 85.46 +4.94
Fine-tuned CodeT5 33.28 +21.12 50.20 +25.65 55.44 +24.72 86.96 +4.84 86.80 +6.28
H5 Removing backticks :GPT-3.5-Turbo often responds to the
code snippet in markdown format where the code snippet is
enclosed with backticks ( ```). Hence, we applied heuristics
to remove such backticks from the response.
We applied the above-mentioned heuristics to adjust the re-
sponse to the desired behavior as much as possible. However,
even after applying the above-mentioned heuristics, there were
still some discrepancies in the responses, which needed careful
human inspection to be removed. For example, the model
explained the fixed code without any keyword preceding it
in one scenario. Hence, we need to delete the line from
the response manually. Or sometimes, the model uses unique
inconsistent patterns of text like Here’s the updated code or
The updated code is below . Therefore, we manually inspected
each model’s response such that these erroneous patterns are
manually removed.
C. RQ3: Developer Analysis of Generated Repairs
In the previous sections, we described how we fine-tuned
PLBART and CodeT5 as well as prompted two LLMs to
get the repaired code by considering the code review and
evaluation based on the ground truth. However, this ground
truth may not be the only possible solution, or the repaired
code may not fully capture the intention in the review. For
this reason, we randomly collected 314 test samples from
Tufano et al. [15] and 340 test samples from Review4Repair
[14] datasets in order to achieve a 95% confidence interval
and 5% error of margin. We considered five models: top-1
solution for PLBART and CodeT5, zero-shot and few-shots
prompting for GPT-3.5-Turbo after applying heuristics, and
Code-DaVinci-Edit-001 . We asked two software developers
to score the generated repaired code from the five models
based on fulfilling the intention of the code review. They have
one year of industry experience in a Fortune 500 company
and significant involvement in the code review process in
software development ( i.e., as a developer, they submit their
code for code review, and they review other developers’ code).
They individually gave zero if the generated repaired code
did not fulfill the review and gave one if the repaired code
was fully aligned with the intention of the review. We then
calculated Cohen’s Kappa score for inter-rater agreement [49]
and presented the result based on the given score.
D. Evaluation Metrics
To evaluate a model’s performance for code synthesis, there
are various evaluation metrics such as BLEU (Bilingual Eval-uation Understudy) [50], and CodeBLEU [51], Exact Match
(EM), etc.The BLEU score denotes the quality of a machine-
translated output. The CodeBLEU score utilizes the n-gram
match from the BLEU score and further considers a code’s im-
portant syntactic and semantic features. An exact match (EM)
denotes a complete sequence-to-sequence match between the
model prediction and the target code snippet.
As the n-gram match from the BLEU score emphasizes the
similarity between the target and the predictions generated
from the models, a naive copy can achieve higher BLEU
and CodeBLEU scores with zero exact matches. However,
in a code refinement task, getting the exact correct fix is
of utmost importance, as only the correct fix can ensure the
successful compilation of the code. Thus, we considered the
exact match between the predicted output from our model and
the target code snippet as the primary evaluation metric. We
further generated multiple predictions using different beam
sizes and evaluated the predictions against the baseline models.
We measure the Top-1 Accuracy as the percentage of fixes
when the topmost prediction of the model exactly matches the
target code snippet. Similarly, for Top-5 or Top-10 Accuracy,
we measure the percentage of fixes when any of the first
5 or 10 model predictions exactly matches the target code
snippet.
We used all the above-mentioned metrics to fine-tune the
models PLBART and CodeT5. We used three of them for
zero-shot and few-shot prompting: BLEU, CodeBLEU, and
Top-1 Accuracy.
IV. R ESULTS
In this section, we answer our research questions.
A. RQ1: How do pre-trained models perform in repairing
bugs identified in the code review process?
From Table II, we can see that both of the fine-tuned models
outperform each of the previous baseline models by a signifi-
cant margin. Both baseline models [14], [15] were trained with
both a buggy code and its respective code review.
On the Review4Repair dataset, the fine-tuned PLBART model
achieves 9.91% improvement, and the fine-tuned CodeT5
model achieves 8.45% improvement in terms of Top-10 Accu-
racy over the baseline model R4R CC, which is the baseline
model named as model ccin the Review4Repair paper [14]. In
terms of relative performance, the fine-tuned PLBART model
achieves 5.69%, 9.56%, and 9.91% higher accuracy in Top-1,
6

--- PAGE 7 ---
Top-5, and Top-10 predictions, and on the other hand, the fine-
tuned CodeT5 model achieves 10.23%, 10.00%, and 8.45%
higher accuracy in Top-1, Top-5, and Top-10 predictions,
respectively, than the baseline model.
On the Tufano et al. [15] dataset, the fine-tuned PLBART
model achieves 20.41% improvement, and the fine-tuned
CodeT5 model achieves 24.72% improvement in terms of Top-
10 Accuracy over the baseline model named Tufano 2-encoder
that is the baseline model from the paper of Tufano et al.
[15]. The fine-tuned CodeT5 was the best-performing model;
the accuracy increases ranges from 21.12% to 25.65%.
We also report the BLEU-4 and CodeBLEU scores for each
fine-tuned model in Table II. We can see that both the fine-
tuned models improved the BLEU-4 and the CodeBLEU
scores over the baseline models. This also suggests that both of
the fine-tuned models can generate codes with better syntactic
flow than the previous models.
We
To assess each model’s strengths and limitations in predicting
the correct repair in all three fix categories ( i.e.,insert ,
delete andupdate ), we compared the Top-1, Top-5, and Top-
10 predictions generated by the fine-tuned models on both
datasets for all the three classes. Their performance is shown
in Figure 2. From Figure 2(a) and 2(d), we can see that
the fine-tuned CodeT5 model achieved better accuracy in
all predictions for the Insert class than both the baseline
models and the fine-tuned PLBART model for both of the
datasets. This demonstrates the CodeT5 model’s effectiveness
in inserting additional lines of code by following the code
review comments when compared to the other models.
From Figure 2(c) and 2(f), on the Review4Repair dataset, the
baseline model performed well in the Delete class but poorly in
the other classes. Among the two fine-tuned models, the fine-
tuned PLBART model performed better than the fine-tuned
CodeT5 model in the Delete class for both datasets. This indi-
cates that the PLBART model can perform better in removing
buggy lines from code than the CodeT5 model.
From Figure 2(b) and 2(e), we can see that in the Update
class for both of the datasets, the fine-tuned CodeT5 model
outperforms both the baseline models and the fine-tuned
PLBART model, similar to the performance in the Insert Class.
The Update class requires both the insertion and deletion of
specific code snippets for a correct fix. Also, for both the
datasets, the Update samples cover the larger portion. Hence,
the higher performance of CodeT5 in the Update samples leads
to overall higher accuracy. Also, despite the update operation
being a complicated one, the observation of the fine-tuned
CodeT5 model outperforming the fine-tuned PLBART model
in the Update class suggests that the CodeT5 model can utilize
the code review associated with the buggy code much better
than the PLBART model.RQ1 Findings: Fine-tuned models can perform significantly
better in generating repaired code using code review. In
most cases, CodeT5 has slightly better performance than
PLBART fine-tuned model. It also has comparatively better
natural language and programming languages comprehen-
sion capability, and hence it can achieve better accuracy in
predicting correct fixes with the help of code review than
the fine-tuned PLBART model. It can be seen that predicting
the correct fix for the Insert class and the Update class is
much more difficult than for the Delete class.
B. RQ2: How effective is automated program repair using
zero-shot and few-shot learning-based prompt engineering on
Large Language Models?
We used zero-shot prompting with the code generative LLMs,
GPT-3.5-Turbo and Code-DaVinci-Edit-001, and for few-
shot, we utilized GPT-3.5-Turbo on both datasets. A concise
overview of these findings is presented in Table III
We observed in zero-shot prompting that the GPT-3.5-Turbo
model achieved 6.9% and 17.86% accuracy on the Re-
view4Repair Dataset [14] and the Tufano et al. [15] dataset
respectively before applying the heuristics described in the
Methodology section (Section III-B5). Comparing this perfor-
mance to the fine-tuned models described in RQ1 and RQ2,
it is noticeably less than ideal.
After using the heuristics, we can see a substantial improve-
ment in accuracy. We observed that exact match improved
by 15.6% (22.06%-6.9%) and 12.27% (30.13%-17.86%) on
the Review4Repair Dataset [14] and the Tufano et al. [15]
dataset respectively. This implies that with proper heuristics,
the model’s response can be more concise and fitting for target
purposes. The improvement of BLEU and CodeBLEU score
over using heuristics also implies the same.
For the case of few-shot prompting with the GPT-3.5-Turbo,
this technique can provide better performance in case of accu-
racy and before applying heuristics. However, after applying
the heuristic, this technique performs better for Tufano et al.
[15], but not on the Review4Repair dataset [14].
For the instruct model, Code-DaVinci-Edit-001, we did zero-
shot prompting and it performs significantly better in some
cases. For instance, it achieved state-of-the-art performance
regarding the CodeBLEU score for the Review4Repair dataset
[14] and in terms of accuracy for Tufano et al. dataset
[15].
RQ2 Findings: Zero-shot and few-shot prompting can be
helpful when fine-tuning is not feasible. However chat-style
model like the GPT-3.5-Turbo needs attention to removing
an unnecessary portion in the response, whereas the instruct
model like Code-DaVinci-Edit-001 has a better performance
which does not need fine-tuning and heuristics to clear the
output.
7

--- PAGE 8 ---
(a) Insert class for Review4Repair Dataset [14]
 (b) Update class for Review4Repair Dataset [14]
 (c) Delete class for Review4Repair Dataset [14]
(d) Insert class for Tufano et al. Dataset [15]
 (e) Update class for Tufano et al. Dataset [15]
 (f) Delete class for Tufano et al. Dataset [15]
Fig. 2: Performance comparison on all classes on both datasets.
TABLE III: Comparison of zero and few shot prompting on each dataset with the new baselines.
Dataset Model Type Model Name Accuracy (%) BLEU (%) CodeBLEU (%)
Review4Repair [14]Pre-trainedFine-tuned PLBART 25.28 −4.54 40.97 −35.41 49.60 −38.82
Fine-tuned CodeT5 29.82 +0.00 45.98 −30.40 53.19 −35.23
Chat StyleZero-shot GPT-3.5-turbo without heuristics 6.90 −22.92 75.42 −0.96 74.94 −13.48
Zero-shot GPT-3.5-turbo with heuristics 22.06 −7.76 76.38 +0.00 75.92 −12.50
Few-shot GPT-3.5-turbo without heuristics 9.54 −20.28 71.55 −4.83 75.23 −13.19
Few-shot GPT-3.5-turbo with heuristics 21.18 −8.64 71.60 −4.78 75.28 −13.14
Instruct Code-DaVinci-Edit-001 25.05 −4.77 75.29 −1.09 88.42 +0.00
Tufano et al. [15]Pre-trainedFine-tuned PLBART 32.98 −7.72 87.55 +0.00 85.46 −3.17
Fine-tuned CodeT5 33.28 −7.42 86.96 −0.59 86.80 −1.83
Chat StyleZero-shot GPT-3.5-turbo without heuristics 17.86 −22.84 70.88 −16.67 80.96 −7.67
Zero-shot GPT-3.5-turbo with heuristics 31.70 −9.00 77.95 −9.60 83.38 −5.25
Few-shot GPT-3.5-turbo without heuristics 27.69 −13.01 67.91 −19.64 81.03 −7.60
Few-shot GPT-3.5-turbo with heuristics 28.21 −12.49 68.23 −19.32 81.29 −7.34
Instruct Code-DaVinci-Edit-001 40.70 +0.00 85.10 −2.45 88.63 +0.00
C. RQ3: How effective are language models in repairing
bugs identified in the code review process from a developer’s
perspective?
To answer this research question, we have collected a statically
significant amount of samples from the test of the two datasets
and top results from five models to manually score them
based on the fulfillment of the review in the repaired code.
We presented the result in Table IV. The last two columns
contain the count of the score in percentages from both raters.
We have 314 test samples from the Tufano et al. [15] and340 test samples from the Review4Repair dataset [14]. For
both datasets, we can see that the raters have moderate to
substantial agreement [52].
We found that, for the dataset from Tufano et al. [15],
zero-shot GPT-3.5-Turbo and Code-DaVinci-Edit-001 have
more capabilities in fulfilling the review in the repaired code.
However, for the Review4Repair dataset [14], the models
are comparatively less capable of addressing the reviewer’s
comment in the repaired code. In this case, fine-tuned CodeT5
8

--- PAGE 9 ---
TABLE IV: Result of the Developers’ Analysis on the Repaired Code.
Dataset Model Type Model Name Cohen’s Kappa Not Fulfilling Fulfilling
Review4Repair [14]Pre-trainedFine-tuned PLBART 0.66 57.94% 60.29% 42.06% 39.71%
Fine-tuned CodeT5 0.68 47.35% 51.47% 52.65% 48.53%
Chat StyleZero-shot GPT-3.5-turbo 0.51 61.76% 60.88% 38.24% 39.12%
Few-shot GPT-3.5-turbo 0.61 62.94% 66.18% 37.06% 33.82%
Instruct Code-DaVinci-Edit-001 0.61 54.12% 58.53% 45.88% 41.47%
Tufano et al. [15]Pre-trainedFine-tuned PLBART 0.62 45.86% 50.00% 54.14% 50.00%
Fine-tuned CodeT5 0.59 42.99% 51.27% 57.01% 48.73%
Chat StyleZero-shot GPT-3.5-turbo 0.62 42.36% 46.50% 57.64% 53.50%
Few-shot GPT-3.5-turbo 0.60 45.22% 51.27% 54.78% 48.73%
Instruct Code-DaVinci-Edit-001 0.55 41.08% 45.22% 58.92% 54.78%
and Code-DaVinci-Edit-001 perform significantly better than
other models.
RQ3 Findings: Language learning models face difficulties
in aligning the code review in the repaired program. For
Review4Repair [14], the fine-tuned CodeT5 can fulfill the
highest 52.65%, and for the dataset from Tufano et al. [15],
Code-DaVinci-Edit-001 model can fulfill the highest 58.92%
reviews in their repaired programs.
V. D ISCUSSION
In this section, we further investigate the pre-trained large
language models from three different perspectives.
A. Observation from the developers’ analysis
Both raters observed that the quality of reviews in the Re-
view4Repair [14] dataset was not good enough to make
changes as the reviews were often very vague like ”nice”
or required additional context like ”check my previous com-
ment” or”revert the change from previous commit” Also,
in some other cases, the ground truth was not aligned with
the action described in the comment, like required changes
were made outside the focus scope of <|startfocus| >
and<|endfocus| >. Such scenarios could possibly lead to
a difference in agreement between the two raters. We can
notice for the Tufano et al. [15] dataset, the two developers
had the most disagreement on fulfilling the CodeT5 model
(57.01% vs.48.73%), whereas, for the Review4Repair [14],
they had most aligned agreement on fulfilling of the GPT-3.5-
turbo model ( 38.24% vs.39.12%). We can also notice both
the developer had the highest agreement on the CodeT5 model
(κ= 0.68), and both the PLBART and the GPT-3.5-turbo
model ( κ= 0.62) for the Review4Repair [14] dataset and the
Tufano et al. [15] dataset respectively. Also notable that both
reviewers independently agreed that the CodeT5 model had
the highest fulfillment for the Review4Repair [14] dataset and
the Tufano et al. [15] dataset, the Code-DaVinci-Edit-001
achieved the highest fulfilling.
B. Implication for the developers and code reviewers
Using Large Language Models with fine-tuning and prompt
engineering shows promise in the task of automating coderepair. With precise and clear reviews, the models can properly
interpret the intentions and be able to make the required
modification. According to our observations ( i.e.,RQ1 ), the
models struggle with more complicated code changes, such as
insert and update operations, while doing significantly better
for simple code changes, such as delete operations. However,
as we can see, performance improves as the number of predic-
tions increases; thus, this can be partly addressed by having
the models generate several fixes and suggestions. Both the
developer and the code reviewer can benefit from having the
ability to select the most suitable fix recommendation.
As the model’s suggestions offer a starting point for making
essential adjustments, this opens up ground for discussion
among the developers and the code reviewers. It is also notable
that overall the performance still is not satisfactory, as shown
in RQ2 and RQ3. The LLMs may make incorrect or sub-
optimal suggestions. Hence, while the developers can rely on
APR tools to make simpler modifications for complex code
changes, both the developers and the code reviewers need to
validate the model’s recommendations carefully.
VI. T HREATS TO VALIDITY
Threats to internal validity are related to how the experi-
ments might be impacted by the model architectural settings
and hyperparameter tuning. We confined our hyperparameter
adjustment to modifications in batch size, source length, and
target length while following the default configuration of the
models for other hyperparameters. However, considering the
size of the transformer architecture’s search space, locating an
ideal hyperparameter setting can be highly expensive. As a
result, we relied heavily on the best architecture presented
in both papers [18], [19] since the objective of our work
was to fairly compare our approach’s accuracy to the baseline
methodologies now in use, not to determine the ideal hyper-
parameter configuration. We realize that there is a scope for
tuning hyperparameters which is anticipated to result in more
improvements.
Threats to external validity are related to how generalizable
our results are to and across various datasets of different
programming languages. We experimented and evaluated the
9

--- PAGE 10 ---
performance of the models using the datasets from the paper
of Tufano et al. [15] and Review4Repair [14]. However,
the datasets consisted of only Java codes and respective code
reviews in the English language; hence, our focus was confined
to a single programming language. As a result, the coverage
of our findings is limited. Nonetheless, by using a similar
methodology, other datasets of various programming lan-
guages might be investigated in future research. Additionally,
we saw that GPT-3.5-Turbo, particularly the Code-DaVinci-
edit model, worked remarkably well without any fine-tuning
from the zero-shot or few-shot prompt engineering results. One
possible reason might be that these LLMs were also trained
with our aforementioned datasets. As a result, there might be a
data leakage [53]. The knowledge cut-off of these two models
is September 2021, where the dataset from Tufano et al. [15]
was published before this date, and the Review4Repair dataset
[14] was published after that. As these models are black-box,
there is no way we can verify if there is data leakage for these
datasets.
VII. R ELATED WORKS
Numerous studies have been done in the past on how to
automate the code repair process. To begin with, various
studies have attempted to automate code repair without em-
ploying code review. Techniques like fault isolation, statement-
level specification inference, and program synthesis were used
in the works of SemFix [9] to generate the fixed code.
Getafix [11] employed a novel clustering algorithm to identify
code changes at the AST level and utilized the context of a
code change to select the most appropriate fix for a given
bug. They can be used to repair SQL Injection [54]. In
SequenceR [10], copy mechanism efficiency was demon-
strated and provided single-line fixes for the Java dataset.
DeepFix [12] employed a neural network with an attention
mechanism to predict fixes for common errors for programs
written in the C language. CoCoNut [13] introduces the first
application of the FConv [55] architecture for automatic code
repair, which removed the drawbacks of former NMT methods.
Our work used different pre-trained models and LLMs to
repair code based on code reviews.
Some recent works explored the importance of utilizing code
review in the task of automating program repair. Tufano
et al. [15] demonstrated this by employing two transformer
models(1-encoder and 2-encoder) where the first model used
only the source buggy code as input and the second model
used both the source buggy code and the code review as
input. Review4Repair [14] also followed similar approach
using a pointer generator network [56] which is a sequence-
to-sequence [57] architecture for text summarization. They
also employed two models( model candmodel cc) following
similar standards like Tufano et al. [15]. Both studies showed
how utilizing the code review boosted the performance of their
second model by a significant margin, thus establishing that
learning-based models can improve their performance with the
help of code review rather than using just the source code topredict proper fixes. However, in our work, we extended the
study by fine-tuning models, prompting LLMs, and manually
analyzing the result.
Moreover, recent development of large language models like
PLBART [18], CodeT5 [19] demonstrated a strong capability
of understanding both NL and PL since they are trained with
many datasets. PLBART [18], based on the same architecture
as BART [58], showed promising results in a variety of down-
stream tasks, including code summarization, code creation,
and code translation as it picks up on important program
properties, including syntax, identifier naming standards, and
data flow during the pre-training process mentioned in their
paper. Also, on understanding tasks like code defect and clone
detection, as well as generation tasks in a variety of directions
including PL-NL, NL-PL, and PL-PL, CodeT5 [19] performs
noticeably better than previous techniques as they used two
novel techniques named identifier-aware pre-training and bi-
modal dual generation. Our work demonstrated their usability
in generating repaired code based on code review.
Furthermore, recently various large languages models like
CodeGen [59], Codex [42], and GPT-3 [37] showed im-
pressive performance on code generation tasks based on NL
prompts. CodeGen [59], trained on a large corpus of NL and
PL, proposed a conversational program synthesis approach
where specifications can be provided in natural language over
multiple turns and the model responses with the generated
code. GPT-3 [37], a large language model developed by Ope-
nAI, showed spectacular performance in understanding natural
language and generating proper code snippets from natural
language descriptions. A fine-tuned model of GPT-3 named
Codex [42] was the base model for Github’s CoPilot. A sub-
class of GPT-3 models, GPT-3.5, included models like GPT-
3.5-Turbo, the base model for OpenAI’s ChatGPT. In a recent
work [31], they demonstrated an encouraging performance of
zero-shot unit test generation using the GPT-3.5-Turbo model
given proper instructions as a prompt. Our study focused on
how such models can be used to automate code repair in zero-
shot and few-shots learning-based prompt engineering.
VIII. C ONCLUSION
By leveraging code review comments and the higher Program-
ming Language (PL) and Natural Language (NL) compre-
hension capabilities inherited from the learned parameters, a
pre-trained model can perform much better in the context of
automated program repair. Furthermore, this boost in accuracy
is due to mostly the learned parameters of the model rather
than the architecture itself. Both the PLBART and the CodeT5
models effectively understood both PL and NL. Consequently,
fine-tuning the models enables them to understand the specific
semantics of codes and the correlations with the code re-
views. Thus, both outperform the prior baseline models trained
on the aforementioned datasets. In addition to that, GPT-3
[37] based GPT-3.5-Turbo and Code-DaVinci-Edit-001 show
great promise with the prompting techniques for repairing
source code based on review. However, our manual analysis
10

--- PAGE 11 ---
demonstrated that language learning models still may not be
capable of fulfilling the intention of the review in the repaired
code.
REFERENCES
[1] C. Bird and A. Bacchelli, “Expectations, outcomes, and
challenges of modern code review,” in Proc. of the Intl.
Conf. on Software Engineering . IEEE, May 2013. [On-
line]. Available: https://www.microsoft.com/en-us/research/publication/
expectations-outcomes-and-challenges-of-modern-code-review/
[2] T. Baum, O. Liskin, K. Niklas, and K. Schneider, “Factors
influencing code review processes in industry,” in Proc. of the 2016
24th ACM SIGSOFT Intl. Symposium on Foundations of Software
Engineering , ser. FSE 2016. New York, NY , USA: Association
for Computing Machinery, 2016, p. 85–96. [Online]. Available:
https://doi.org/10.1145/2950290.2950323
[3] S. McIntosh, Y . Kamei, B. Adams, and A. E. Hassan, “An empirical
study of the impact of modern code review practices on software
quality,” Empirical Software Engineering , vol. 21, pp. 2146–2189, 2016.
[4] T. Britton, L. Jeng, G. Carver, and P. Cheak, “Reversible debugging
software “quantify the time and cost saved using reversible debuggers”,”
2013.
[5] F. DeMarco, J. Xuan, D. Le Berre, and M. Monperrus, “Automatic
repair of buggy if conditions and missing preconditions with smt,” in
Proc. of the 6th Intl. Workshop on Constraints in Software Testing,
Verification, and Analysis , ser. CSTV A 2014. New York, NY , USA:
Association for Computing Machinery, 2014, p. 30–39. [Online].
Available: https://doi.org/10.1145/2593735.2593740
[6] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semfix:
Program repair via semantic analysis,” ser. ICSE ’13. IEEE Press,
2013, p. 772–781.
[7] D. Kim, J. Nam, J. Song, and S. Kim, “Automatic patch generation
learned from human-written patches,” 05 2013, pp. 802–811.
[8] F. Long and M. Rinard, “Staged program repair with condition
synthesis,” ser. ESEC/FSE 2015. New York, NY , USA: Association
for Computing Machinery, 2015, p. 166–178. [Online]. Available:
https://doi.org/10.1145/2786805.2786811
[9] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semfix:
Program repair via semantic analysis,” in Proc. of the 2013 Intl. Conf.
on Software Engineering , ser. ICSE ’13. IEEE Press, 2013, p. 772–781.
[10] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, and
M. Monperrus, “Sequencer: Sequence-to-sequence learning for end-
to-end program repair,” IEEE Transactions on Software Engineering ,
vol. 47, no. 09, pp. 1943–1959, sep 2021.
[11] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getafix: Learning to fix
bugs automatically,” Proc. ACM Program. Lang. , vol. 3, no. OOPSLA,
oct 2019. [Online]. Available: https://doi.org/10.1145/3360585
[12] R. Gupta, S. Pal, A. Kanade, and S. Shevade, “Deepfix: Fixing
common c language errors by deep learning,” Proc. of the AAAI Conf.
on Artificial Intelligence , vol. 31, no. 1, Feb. 2017. [Online]. Available:
https://ojs.aaai.org/index.php/AAAI/article/view/10742
[13] T. Lutellier, V . H. Pham, L. Pang, Y . Li, M. Wei, and L. Tan, “Coconut:
Combining context-aware neural translation models using ensemble for
program repair,” 2020.
[14] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal,
and T. Ahmed, “Review4repair: Code review aided automatic
program repairing,” Information and Software Technology , vol. 143,
p. 106765, 2022. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0950584921002111[15] R. Tufano, L. Pascarella, M. Tufano, D. Poshyvanyk, and G. Bavota,
“Towards automating code review activities,” in Proc. of the 43rd Intl.
Conf. on Software Engineering , ser. ICSE ’21. IEEE Press, 2021, p.
163–174. [Online]. Available: https://doi.org/10.1109/ICSE43902.2021.
00027
[16] S. McIntosh, Y . Kamei, B. Adams, and A. E. Hassan, “The impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects,” in Proceedings of the 11th
Working Conference on Mining Software Repositories , ser. MSR 2014.
New York, NY , USA: Association for Computing Machinery, 2014, p.
192–201. [Online]. Available: https://doi.org/10.1145/2597073.2597076
[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. of the
31st Intl. Conf. on Neural Information Processing Systems , ser. NIPS’17.
Red Hook, NY , USA: Curran Associates Inc., 2017, p. 6000–6010.
[18] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified
pre-training for program understanding and generation,” in Proc. of
the 2021 Conf. of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies . Online:
Association for Computational Linguistics, Jun. 2021, pp. 2655–2668.
[Online]. Available: https://www.aclweb.org/anthology/2021.naacl-main.
211
[19] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “CodeT5: Identifier-aware
unified pre-trained encoder-decoder models for code understanding and
generation,” in Proc. of the 2021 Conf. on Empirical Methods in Natural
Language Processing . Online and Punta Cana, Dominican Republic:
Association for Computational Linguistics, Nov. 2021, pp. 8696–8708.
[Online]. Available: https://aclanthology.org/2021.emnlp-main.685
[20] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. LIU, L. Zhou,
N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng,
C. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou,
“Graphcode {bert}: Pre-training code representations with data flow,”
inIntl. Conf. on Learning Representations , 2021. [Online]. Available:
https://openreview.net/forum?id=jLoC4ez43PZ
[21] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,
B. Qin, T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained
model for programming and natural languages,” in Findings of the
Association for Computational Linguistics: EMNLP 2020 . Online:
Association for Computational Linguistics, Nov. 2020, pp. 1536–1547.
[Online]. Available: https://aclanthology.org/2020.findings-emnlp.139
[22] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,
D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al. , “Emergent abilities
of large language models,” arXiv preprint arXiv:2206.07682 , 2022.
[23] A. Moradi Dakhel, V . Majdinasab, A. Nikanjam, F. Khomh,
M. C. Desmarais, and Z. M. J. Jiang, “Github copilot ai
pair programmer: Asset or liability?” Journal of Systems and
Software , vol. 203, p. 111734, 2023. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S0164121223001292
[24] M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, and J. C. S.
Santos, “An empirical study of code smells in transformer-based code
generation techniques,” in 2022 IEEE 22nd Int’l Working Conf. on
Source Code Analysis and Manipulation (SCAM) , 2022, pp. 71–82.
[25] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, “Asleep
at the keyboard? assessing the security of github copilot’s code
contributions,” in 2022 2022 IEEE Symposium on Security and Privacy
(SP) (SP) . Los Alamitos, CA, USA: IEEE Computer Society, may
2022, pp. 980–994. [Online]. Available: https://doi.ieeecomputersociety.
org/10.1109/SP46214.2022.00057
[26] M. A. Hadi, I. N. B. Yusuf, F. Thung, K. G. Luong, J. Lingxiao,
F. H. Fard, and D. Lo, “On the effectiveness of pretrained models for
api learning,” in Proc. of the 30th IEEE/ACM Int’l Conf. on Program
Comprehension , ser. ICPC ’22. New York, NY , USA: ACM, 2022, p.
309–320. [Online]. Available: https://doi.org/10.1145/3524610.3527886
11

--- PAGE 12 ---
[27] M. L. Siddiq, A. Samee, S. R. Azgor, M. A. Haider, S. I. Sawraz, and
J. C. Santos, “Zero-shot prompting for code complexity prediction using
github copilot,” in 2023 The 2nd Intl. Workshop on NL-based Software
Engineering , 2023.
[28] Y . Xian, C. H. Lampert, B. Schiele, and Z. Akata, “Zero-shot learning—a
comprehensive evaluation of the good, the bad and the ugly,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 41,
no. 9, pp. 2251–2265, 2019.
[29] R. Logan IV , I. Balazevic, E. Wallace, F. Petroni, S. Singh, and
S. Riedel, “Cutting down on prompts and parameters: Simple few-shot
learning with language models,” in Findings of the Association for
Computational Linguistics: ACL 2022 . Dublin, Ireland: Association
for Computational Linguistics, May 2022, pp. 2824–2835. [Online].
Available: https://aclanthology.org/2022.findings-acl.222
[30] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , “Improving
language understanding by generative pre-training,” 2018.
[31] M. L. Siddiq, J. C. S. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and
V . C. Lopes, “Exploring the effectiveness of large language models in
generating unit tests,” 2023.
[32] “Gpt-3.5 documentation,” Accessed June 27, 2023. [Online]. Available:
https://platform.openai.com/docs/models/gpt-3-5
[33] “New gpt-3 capabilities: Edit & insert,” Accessed July 03, 2023.
[Online]. Available: https://openai.com/blog/gpt-3-edit-insert
[34] M. Harman, “Automated patching techniques: The fix is in: Technical
perspective,” Commun. ACM , vol. 53, no. 5, p. 108, may 2010.
[Online]. Available: https://doi.org/10.1145/1735223.1735248
[35] M. C. Rinard, “Technical perspectivepatching program errors,”
Commun. ACM , vol. 51, no. 12, p. 86, dec 2008. [Online]. Available:
https://doi.org/10.1145/1409360.1409381
[36] M. Harman, “Automated patching techniques: The fix is in: Technical
perspective,” Commun. ACM , vol. 53, no. 5, p. 108, may 2010.
[Online]. Available: https://doi.org/10.1145/1735223.1735248
[37] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language models
are few-shot learners,” arXiv preprint arXiv:2005.14165 , 2020.
[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” in Proc.
of the 2019 Conf. of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) . Minneapolis, Minnesota: Association
for Computational Linguistics, Jun. 2019, pp. 4171–4186. [Online].
Available: https://aclanthology.org/N19-1423
[39] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a unified text-to-text transformer,” J. Mach. Learn. Res. , vol. 21,
no. 1, jan 2020.
[40] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train,
prompt, and predict: A systematic survey of prompting methods in
natural language processing,” ACM Comput. Surv. , vol. 55, no. 9, jan
2023. [Online]. Available: https://doi.org/10.1145/3560815
[41] Y . Xian, C. Lampert, B. Schiele, and Z. Akata, “Zero-shot learning -
a comprehensive evaluation of the good, the bad and the ugly,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. PP, 07
2017.
[42] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large
language models trained on code,” arXiv preprint arXiv:2107.03374 ,
2021.
[43] S. Fakhoury, S. Chakraborty, M. Musuvathi, and S. K. Lahiri, “Towards
generating functionally correct code edits from natural language issue
descriptions,” 2023.[44] A. Aizawa, “The feature quantity: An information theoretic perspective
of tfidf-like measures,” ser. SIGIR ’00. New York, NY , USA:
Association for Computing Machinery, 2000, p. 104–111. [Online].
Available: https://doi.org/10.1145/345508.345556
[45] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,”
inThe 7th Intl. student Conf. on advanced science and technology
ICAST , vol. 4, no. 1, 2012, p. 1.
[46] J.-B. D ¨oderlein, M. Acher, D. E. Khelladi, and B. Combemale, “Piloting
copilot and codex: Hot temperature, cold prompts, or black magic?”
2023.
[47] J. A. Prenner, H. Babii, and R. Robbes, “Can openai’s codex fix bugs?:
An evaluation on quixbugs,” in 2022 IEEE/ACM Intl. Workshop on
Automated Program Repair (APR) , 2022, pp. 69–75.
[48] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, “Asleep
at the keyboard? assessing the security of github copilot’s code contri-
butions,” in 2022 IEEE Symposium on Security and Privacy (SP) , 2022,
pp. 754–768.
[49] M. L. McHugh, “Interrater reliability: the kappa statistic,” Biochemia
medica , vol. 22, no. 3, pp. 276–282, 2012.
[50] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” 10 2002.
[51] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,
M. Zhou, A. Blanco, and S. Ma, “Codebleu: a method for automatic
evaluation of code synthesis,” CoRR , vol. abs/2009.10297, 2020.
[Online]. Available: https://arxiv.org/abs/2009.10297
[52] S. Sun, “Meta-analysis of cohen’s kappa,” Health Services and Outcomes
Research Methodology , vol. 11, pp. 145–163, 2011.
[53] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee,
A. Roberts, T. B. Brown, D. X. Song, ´U. Erlingsson, A. Oprea, and
C. Raffel, “Extracting training data from large language models,” in
USENIX Security Symposium , 2020.
[54] M. L. Siddiq, M. R. R. Jahin, M. R. U. Islam, R. Shahriyar, and A. Iqbal,
“Sqlifix: Learning based approach to fix sql injection vulnerabilities
in source code,” in 2021 IEEE International Conference on Software
Analysis, Evolution and Reengineering (SANER) . IEEE, 2021, pp. 354–
364.
[55] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,
“Convolutional sequence to sequence learning,” in Proc. of the 34th Intl.
Conf. on Machine Learning - Volume 70 , ser. ICML’17. JMLR.org,
2017, p. 1243–1252.
[56] S. Gehrmann, Y . Deng, and A. Rush, “Bottom-up abstractive
summarization,” in Proc. of the 2018 Conf. on Empirical Methods
in Natural Language Processing . Brussels, Belgium: Association for
Computational Linguistics, Oct.-Nov. 2018, pp. 4098–4109. [Online].
Available: https://aclanthology.org/D18-1443
[57] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning
with neural networks,” in Proc. of the 27th Intl. Conf. on Neural
Information Processing Systems - Volume 2 , ser. NIPS’14. Cambridge,
MA, USA: MIT Press, 2014, p. 3104–3112.
[58] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehen-
sion,” 01 2020, pp. 7871–7880.
[59] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, “Codegen: An open large language model for code with
multi-turn program synthesis,” ICLR , 2023.
12
