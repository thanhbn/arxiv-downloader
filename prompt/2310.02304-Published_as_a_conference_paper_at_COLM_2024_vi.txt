# 2310.02304.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2310.02304.pdf
# Kích thước tệp: 966894 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
Bộ tối ưu tự học (STOP): Tự cải thiện đệ quy
Sinh mã nguồn
Eric Zelikman
Đại học Stanford*Eliana Lorch Lester Mackey
Microsoft ResearchAdam Kalai
OpenAI*

Tóm tắt
Một số tiến bộ gần đây trong các hệ thống AI giải quyết vấn đề bằng cách cung cấp một chương trình "giàn giáo" cấu trúc nhiều lời gọi đến các mô hình ngôn ngữ (LM) để tạo ra các kết quả đầu ra tốt hơn. Một chương trình giàn giáo được viết bằng ngôn ngữ lập trình như Python. Trong nghiên cứu này, chúng tôi sử dụng một chương trình giàn giáo có tích hợp mô hình ngôn ngữ để cải thiện chính nó. Chúng tôi bắt đầu với một "bộ cải thiện" khởi tạo cải thiện một chương trình đầu vào theo hàm tiện ích đã cho bằng cách truy vấn LM nhiều lần và trả về giải pháp tốt nhất. Sau đó chúng tôi chạy bộ cải thiện khởi tạo này để cải thiện chính nó. Trên một tập nhỏ các tác vụ hạ nguồn, bộ cải thiện được cải thiện kết quả tạo ra các chương trình có hiệu suất tốt hơn đáng kể so với bộ cải thiện khởi tạo của nó. Nhiều chiến lược tự cải thiện khác nhau được đề xuất bởi mô hình ngôn ngữ, bao gồm tìm kiếm chùm, thuật toán di truyền và luyện kim mô phỏng. Vì bản thân các mô hình ngôn ngữ không bị thay đổi, đây không phải là tự cải thiện đệ quy hoàn toàn. Tuy nhiên, nó chứng minh rằng một mô hình ngôn ngữ hiện đại, GPT-4 trong các thí nghiệm của chúng tôi, có khả năng viết mã có thể gọi chính nó để cải thiện chính nó. Chúng tôi xem xét các mối lo ngại xung quanh việc phát triển các công nghệ tự cải thiện và đánh giá tần suất mà mã được tạo ra bỏ qua sandbox.

1 Giới thiệu
Một mô hình ngôn ngữ (LM) có thể được truy vấn để tối ưu hóa hầu như bất kỳ mục tiêu nào có thể mô tả bằng ngôn ngữ tự nhiên. Tuy nhiên, một chương trình thực hiện nhiều lời gọi có cấu trúc đến LM thường có thể tạo ra các kết quả đầu ra có giá trị mục tiêu cao hơn (Yao et al., 2022; 2023; Zelikman et al., 2023; Chen et al., 2022b). Chúng tôi gọi đây là các chương trình "giàn giáo", thường được viết (bởi con người) bằng ngôn ngữ lập trình như Python. Quan sát chính của chúng tôi là, đối với bất kỳ phân phối nào trên các bài toán tối ưu hóa và bất kỳ LM cố định nào, việc thiết kế một chương trình giàn giáo bản thân cũng là một bài toán tối ưu hóa.

Trong nghiên cứu này, chúng tôi giới thiệu Bộ tối ưu tự học (STOP), một phương pháp trong đó mã áp dụng LM để cải thiện các giải pháp tùy ý được áp dụng đệ quy để cải thiện chính nó trong một phạm vi xác định. Cách tiếp cận của chúng tôi bắt đầu với chương trình giàn giáo 'bộ cải thiện' khởi tạo sử dụng LM để cải thiện một giải pháp cho một số tác vụ hạ nguồn. Khi hệ thống lặp lại, LM tinh chỉnh bộ cải thiện này. Chúng tôi định lượng hiệu suất của khung tự tối ưu hóa với các tác vụ thuật toán hạ nguồn, quan sát các cải thiện khi LM áp dụng các chiến lược tự cải thiện của nó qua các lần lặp ngày càng tăng. Do đó, STOP cho thấy cách LM có thể hoạt động như các meta-optimizer của chính chúng. Chúng tôi cũng điều tra các loại chiến lược tự cải thiện

*Công việc được thực hiện khi ở Microsoft Research New England

Thuật toán 
Di truyền Tìm kiếm Chùm / 
Tìm kiếm Cây Multi-Armed 
Prompt Bandit Thay đổi Nhiệt độ 
để Khám phá Tìm kiếm dựa trên 
Luyện kim mô phỏng Phân tách và 
Cải thiện các Phần ? 

Hình 1: Ví dụ về các chiến lược tự cải thiện được đề xuất và triển khai bởi GPT-4. Mỗi chiến lược được sử dụng như giàn giáo để sửa đổi mã tùy ý, bao gồm cả chính giàn giáo đó.

1arXiv:2310.02304v3  [cs.CL]  16 Aug 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Prompt khởi tạo cho Tự cải thiện
1from helpers import extract_code
2
3def improve_algorithm ( initial_solution , utility , language_model ):
4 """ Cải thiện một giải pháp theo hàm tiện ích. """
5 expertise = " Bạn là một nhà nghiên cứu khoa học máy tính và lập trình viên chuyên gia, đặc biệt 
,→giỏi tối ưu hóa thuật toán."
6 message = f """ Cải thiện giải pháp sau đây:
7```python
8{ initial_solution }
9```
10
11 Bạn sẽ được đánh giá dựa trên hàm điểm số này:
12```python
13 { utility . str }
14```
15
16 Bạn phải trả về một giải pháp được cải thiện. Hãy sáng tạo nhất có thể trong các ràng buộc.
17 Cải thiện chính của bạn phải mới lạ và không tầm thường. Trước tiên, đề xuất một ý tưởng, sau đó triển khai nó. """
18 n_messages = min ( language_model . max_responses_per_call , utility . budget )
19 new_solutions = language_model . batch_prompt ( expertise , [ message ] * n_messages , temperature =0.7)
20 new_solutions = extract_code ( new_solutions )
21 best_solution = max ( new_solutions , key = utility )
22 return best_solution

Hình 2: Bộ cải thiện khởi tạo của chúng tôi. Chương trình cải thiện khởi tạo của chúng tôi đơn giản là nhắc LM tạo ra các ứng viên cải thiện cho một giải pháp ban đầu cho một tác vụ và trả về giải pháp tốt nhất được đưa ra một hàm tiện ích. STOP (Thuật toán 1) cải thiện bộ cải thiện bằng chính nó.

mà LM đề xuất (xem Hình 1), khả năng chuyển giao của các chiến lược qua các tác vụ hạ nguồn, và khám phá tính dễ bị tổn thương của LM đối với các chiến lược tự cải thiện không an toàn.

Chúng tôi gọi vấn đề này là sinh mã tự cải thiện đệ quy, được lấy cảm hứng từ nhưng không hoàn toàn là một hệ thống Tự cải thiện đệ quy (RSI), vì LM cơ bản vẫn không thay đổi. Khái niệm rộng hơn về RSI có từ ít nhất nửa thế kỷ trước, được chính thức hóa bởi Good (1966) và sau đó bởi Schmidhuber (2003), nhưng nghiên cứu của chúng tôi đại diện cho một ứng dụng khiêm tốn và cụ thể hơn của những ý tưởng này. Nghiên cứu đó tập trung vào việc phát triển các hệ thống có khả năng tổng quát hơn và giả định mô hình được phép tinh chỉnh mọi khía cạnh của mã của nó, trong khi nghiên cứu của chúng tôi chỉ tập trung vào khả năng của mô hình cải thiện đệ quy giàn giáo gọi nó. Bài báo này đầu tiên công thức hóa vấn đề RSI-sinh-mã theo cách được định nghĩa toán học rõ ràng. Sau đó chúng tôi định nghĩa và đánh giá STOP, chứng minh tiềm năng tiện ích của RSI-sinh-mã. Các cải thiện được thể hiện trên nhiều tác vụ hạ nguồn khác nhau. Hình 1 minh họa một số giàn giáo chức năng và thú vị được đề xuất bởi STOP khi sử dụng một phiên bản của mô hình ngôn ngữ GPT-4 (OpenAI, 2023b) được huấn luyện trên dữ liệu đến năm 2021, trước khi giới thiệu hầu hết các hệ thống giàn giáo. Các khám phá thêm trong Phần 6.2 đo lường tỷ lệ mà mô hình cố gắng vô hiệu hóa cờ sandbox, cung cấp các phát hiện sớm trong lĩnh vực này. Cuối cùng, Phần 8 thảo luận về các mối lo ngại liên quan đến sự tiến bộ có trách nhiệm của những công nghệ như vậy.

Đóng góp. Các đóng góp chính của chúng tôi là (a) công thức hóa một cách tiếp cận meta-tối ưu hóa trong đó hệ thống giàn giáo tự cải thiện đệ quy, (b) cung cấp một nghiên cứu trường hợp trong đó một hệ thống, sử dụng LM hiện đại (GPT-4) có thể thành công tự cải thiện đệ quy, và (c) điều tra các kỹ thuật tự cải thiện được đề xuất và triển khai bởi mô hình, bao gồm cách mô hình vượt qua các biện pháp an toàn như sandbox.

2 Nghiên cứu liên quan

Giàn giáo Mô hình Ngôn ngữ. Nhiều chiến lược gợi ý và giàn giáo đã được phát triển để cho phép suy luận có hệ thống hơn trong LM (Wei et al., 2022b; Yao et al., 2022; 2023; Zelikman et al., 2023; Chen et al., 2022b; Zhou et al., 2022a; Khattab et al., 2022; Jiang et al., 2022; Sel et al., 2023; Besta et al., 2023; Poesia et al., 2023). Ví dụ, scratchpads và chain-of-thought dựa vào việc truyền đạt cho mô hình rằng nó nên làm việc qua một vấn đề từng bước một (Nye et al., 2021; Wei et al., 2022b). Tree-of-Thoughts sử dụng thuật toán để giàn giáo mô hình xem xét các đường dẫn phân nhánh của các bước suy luận (Yao et al., 2023). Graph of thoughts mở rộng điều này, cho phép các hoạt động đồ thị khác (trong đó các nút là các bước suy luận), như tổng hợp (Besta et al., 2023). Nghiên cứu khác đã tập trung vào để các mô hình suy luận với quyền truy cập vào một trình thông dịch như gợi ý Program of Thoughts (Chen et al., 2022b), Program-aided Language Models (Gao et al., 2023), Reflexion (Shinn et al., 2023), hoặc ReAct (Yao et al., 2022), trong khi những nghiên cứu khác trừu tượng hóa cấu trúc giàn giáo này như Demonstrate-

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bộ cải thiện khởi tạo (Improver0)
sửa đổi mã
chọn chương trình 
mới tốt nhất cải thiện bản thân với 
meta-utility ũImprover0
Improver1
ImproverT
I1(ũ, I1, LM) 
I0(ũ, I0, LM) 
IT(ũ, IT, LM) 
cải thiện bản thân bằng bản thân
Chương trình 
fnHàm Tiện ích 
fn → điểm số 
cải thiện bản thân bằng bản thân
cải thiện bản thân bằng bản thân

Hình 3: Quy trình tự cải thiện. STOP (Thuật toán 1) sử dụng chương trình bộ cải thiện khởi tạo để lặp lại tối ưu hóa mã của chính nó bằng cách sử dụng các lời gọi LM và một hàm meta-utility đánh giá mức độ tốt của một bộ cải thiện trong việc tối ưu hóa mã cho các tác vụ hạ nguồn.

Search-Predict (DSP) (Khattab et al., 2022), Language Model Cascades (Dohan et al., 2022), hoặc Cognitive Architectures (Sumers et al., 2023). Mỗi nghiên cứu có thể được xem như kết quả của các nhà nghiên cứu hỏi, "Với một LM không hoàn hảo, làm thế nào chúng ta có thể cung cấp cấu trúc để giúp nó giải quyết vấn đề?" Thay vào đó, chúng tôi hỏi liệu LM có thể thiết kế cấu trúc đó và cải thiện nó bằng chính nó. Đáng ngạc nhiên, GPT-4 đề xuất các kỹ thuật giàn giáo được giới thiệu sau thời điểm cắt huấn luyện của nó.

Mô hình Ngôn ngữ như Kỹ sư Prompt. Nghiên cứu cũng đã khám phá khả năng của LM trong việc tối ưu hóa các prompt, như Automatic Prompt Engineer (APE) (Zhou et al., 2022b) hoặc gần đây, OPRO (Yang et al., 2023) và Promptbreeder (Fernando et al., 2023). Lưu ý rằng, đối với những nghiên cứu này, mục tiêu luôn là giàn giáo LM để tạo ra một prompt nhưng không phải để giàn giáo nó tạo ra một giàn giáo tốt hơn (ngoài các giàn giáo chỉ-gợi-ý như zero-shot chain-of-thought), cũng không tạo ra một giàn giáo có thể áp dụng đệ quy. Nói cách khác, những nghiên cứu này có thể được hiểu là đề xuất các giàn giáo cụ thể cho kỹ thuật prompt nhưng không phải cho đề xuất giàn giáo. Nhưng, chúng tôi chia sẻ cảm hứng về LM cải thiện suy luận của chúng mà không cần fine-tuning.

Tự cải thiện Mô hình Ngôn ngữ. Nghiên cứu trước đây, như STaR (Zelikman et al., 2022), chứng minh rằng LM có thể học giải quyết các vấn đề khó hơn bằng cách học từ chuỗi suy luận của chúng bằng cách lọc dựa trên câu trả lời không chính xác (cũng như Huang et al. 2022, đã khám phá trường hợp cụ thể trong đó bỏ phiếu đa số được sử dụng như bộ lọc và Uesato et al. 2022, nhấn mạnh giá trị của việc kiểm tra tính chính xác của bản thân suy luận). Lấy cảm hứng từ tự chơi trong trò chơi, Haluptzok et al. (2023) thiết kế một khung tự cải thiện cho sinh mã trong đó LM tạo ra các vấn đề mới để fine-tune chính nó. Nghiên cứu liên quan đã khám phá việc dạy LM debug hoặc tối ưu hóa mã (Chen et al., 2023b; Shypula et al., 2023). Tuy nhiên, cách tiếp cận của chúng tôi trực giao với những nghiên cứu này, vì chúng tôi không tận dụng fine-tuning mà thay vào đó tập trung vào khả năng của mô hình cải thiện mã cho phép nó giải quyết vấn đề.

Nghiên cứu liên quan khác là Voyager (Wang et al., 2023), cho thấy rằng LM có thể tối ưu hóa các chương trình có sẵn cho một agent thể hiện để cải thiện khám phá trong trò chơi video Minecraft, và nghiên cứu đồng thời Language Models as Tool Makers (Cai et al., 2023).

Tự cải thiện Đệ quy (RSI). RSI được đề xuất bởi Minsky (1966) và Good (1966), như được trích dẫn bởi Yampolskiy (2015). Schmidhuber (2003) đầu tiên cung cấp một công thức hóa nghiêm ngặt, trong đó một bộ giải quyết vấn đề sẽ tận dụng chính nó để giải quyết các vấn đề ngày càng khó hơn bằng cách thực hiện các cải thiện có thể chứng minh được đối với chính nó. Một số nguyên tắc này cũng được nổi bật trong Schmidhuber (1987). Không giống như nghiên cứu này, chúng tôi không cố gắng chứng minh rằng các cải thiện giàn giáo được thực hiện bởi mô hình là tối ưu. Như đã đề cập, sinh mã RSI khác với RSI đầy đủ vì chỉ có giàn giáo được cải thiện. Ngoài ra, nhiều phân tích trước đây liên quan đến việc chọn chương trình ngẫu nhiên (tức là "khỉ gõ máy chữ") hoặc liệt kê không phụ thuộc vào mục tiêu cần được cải thiện (Levin, 1973). Ngược lại, sử dụng LM, chúng ta có thể mô tả mục tiêu cơ bản trong một prompt (chính nó có thể được cải thiện). Trực quan,

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Thuật toán 1: Bộ tối ưu tự học (STOP)
Đầu vào: Bộ cải thiện khởi tạo I0, mô hình ngôn ngữ L, độ sâu đệ quy T, các tác vụ hạ nguồn D
Đầu ra: Một bộ cải thiện được cải thiện IT
fort=1toTdo
It←It−1(ˆu,It−1,L) // Cập nhật bộ cải thiện dựa trên meta-utility ˆu
return IT // Trả về bộ cải thiện cuối cùng

Hàm ˜u(I):
utility sum←0 // Duy trì tổng các tiện ích tác vụ hạ nguồn
for(u,S)∈Ddo
S′←I(u,S,L) // Cải thiện giải pháp ban đầu S bằng bộ cải thiện I
utility sum += u(S′) // Thêm tiện ích mới
return utility sum/|D| // Trả về tiện ích kỳ vọng

cung cấp mục tiêu này có thể làm cho tìm kiếm chương trình hiệu quả hơn. Một số nghiên cứu cũng đã đề xuất ràng buộc các loại cải thiện (Nivel et al., 2013; Steunebrink et al., 2016) để khuyến khích các cải thiện giảm thiểu hành vi nguy hiểm. Về việc triển khai, trong khi đã có những nỗ lực cho máy Gödel (Hall, 2007; Steunebrink & Schmidhuber, 2012), nghiên cứu của chúng tôi là đầu tiên tận dụng LM cho sinh mã tự cải thiện đệ quy.

3 Phát biểu Vấn đề

Trong phần này, chúng tôi công thức hóa mục tiêu lựa chọn một bộ cải thiện thông qua sinh mã tự cải thiện đệ quy. Điều này được xem như một bước "tiền-tối ưu hóa" tốn kém về mặt tính toán với các lợi ích có thể được thu hoạch trong nhiều ứng dụng hạ nguồn. Cụ thể, hãy để Σ* biểu thị tập hợp các chuỗi văn bản hữu hạn, và hãy để L:Σ*→Σ* là một LM hộp đen ngẫu nhiên¹ có thể được sử dụng để tạo ra mã, được đưa ra một truy vấn. Một tiện ích u= (ufunc,ustr) là một cặp trong đó ufunc :Σ*→R là một hàm hộp đen, có thể ngẫu nhiên² gán các giá trị thực bị chặn cho các chuỗi giải pháp; và ustr∈Σ* là một mô tả có thể đơn giản là mã nguồn của hàm. Với một chút lạm dụng ký hiệu, chúng tôi viết u(x)≡ufunc(x) cho giải pháp x.

Một tác vụ τ= (u,s) được chỉ định bởi tiện ích u và một giải pháp s∈Σ*. Trong các ứng dụng của chúng tôi, các giải pháp là mã nguồn, nhưng tổng quát hơn, bất kỳ tiện ích nào được định nghĩa trên chuỗi đều có thể được sử dụng. Một bộ cải thiện I là một chương trình cải thiện giải pháp tác vụ bằng LM L:

s′=I(u,s,L) lý tưởng với u(s′)≫u(s). (1)

Giả sử có một phân phối D trên các tác vụ hạ nguồn τ∼ D. Do đó, mục tiêu là tìm một chương trình bộ cải thiện I với tiện ích kỳ vọng cao khi được sử dụng, ¯u(I)≜E(u,s)∼D[u(I(u,s,L))].

Để huấn luyện, chúng tôi giả định được cho một tập hợp n tác vụ hạ nguồn D∼ D^n được rút độc lập từ phân phối D. Chúng tôi tương ứng định nghĩa meta-utility ˆu của một bộ cải thiện I là tiện ích trung bình trên các tác vụ huấn luyện hạ nguồn,

ˆu(I)≜1/|D|∑(u,s)∈D u(I(u,s,L)). (2)

Các phương trình trên định nghĩa ¯ufunc và ˆufunc. Đối với chuỗi mô tả của chúng, chúng tôi sử dụng mô tả "hộp xám" chung ¯ustr=ˆustr là một mô tả (ví dụ: mã nguồn) chỉ ra rằng tiện ích là kỳ vọng trên một tập hợp các tác vụ hạ nguồn, nhưng bản thân các tác vụ hạ nguồn cá nhân không được bao gồm trong mô tả. Điều này cho phép tối ưu hóa trên ˆu như một xấp xỉ cho mục tiêu thực tế ¯u. Ngoài ra, phân tích lý thuyết trong Phụ lục A cung cấp các điều kiện đơn giản mà dưới đó việc tối ưu hóa ˆu cũng gần như tối ưu hóa ¯u, và chính thức hóa các giới hạn tài nguyên về thời gian chạy và LM. Cuối cùng, Phụ lục A cũng đưa ra một công thức tương đương của sinh mã tự cải thiện đệ quy theo nghĩa của tối đa hóa đệ quy dễ phân tích lý thuyết hơn và không cần giải pháp ban đầu nào được đưa ra. Bài báo này sử dụng công thức bộ cải thiện vì chúng tôi thấy giải pháp ban đầu có giá trị trong thực tế để khởi động quá trình tự cải thiện.

¹tức là, hệ thống có thể thực thi hàm nhưng không có thông tin triển khai
²Một hàm đến tập hợp P(X) của các phân phối xác suất trên X

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

(a) GPT-4
0 1 2 3 4
Lần lặp (T)60%65%70%75%Meta-utility Kiểm tra GPT-4
 (b) GPT-3.5
0 1 2 3 4
Lần lặp (T)20%30%40%50%60%70%Meta-utility Kiểm tra GPT-3.5
 (c) Mixtral
0 1 2 3 4
Lần lặp (T)20%30%40%50%60%70%Meta-utility Kiểm tra Mixtral

Hình 4: Meta-utility kiểm tra vs lần lặp. Meta-utility của STOP (Thuật toán 1) trên các thực thể kiểm tra giữ lại sau T lần lặp tự cải thiện cho tác vụ hạ nguồn học tính chẵn lẻ với nhiễu. Lần lặp 0 sử dụng bộ cải thiện khởi tạo I0. Với quyền truy cập vào LM mạnh như GPT-4 (trái), STOP liên tục cải thiện hiệu suất hạ nguồn trung bình. Ngược lại, với GPT-3.5 (giữa) và Mixtral (phải), hiệu suất giảm. Chi tiết trong Phần 5.1 và 5.3.

4 Bộ tối ưu tự học (STOP)

Hình 3 cung cấp sơ đồ trực quan của quy trình tự cải thiện được hình dung trong Phần 3, trong khi Thuật toán 1 cung cấp mã giả của Bộ tối ưu tự học (STOP). Quan sát chính là việc lựa chọn I bản thân cũng là một bài toán tối ưu hóa, mà chúng ta có thể áp dụng cải thiện đệ quy. STOP bắt đầu với bộ cải thiện khởi tạo ban đầu I0. Chúng tôi định nghĩa bộ cải thiện thứ t như đầu ra của t vòng tự cải thiện với meta-utility ˆu: It≜It−1(ˆu,It−1,L). Điều này được lặp lại cho một số lần lặp T được xác định trước, theo tài nguyên có sẵn.

Trực giác. Bằng cách sử dụng ˆu, STOP lựa chọn bộ cải thiện dựa trên cải thiện tiện ích hạ nguồn. Cách tiếp cận này được thúc đẩy bởi các trực giác rằng 1) các bộ cải thiện giỏi cải thiện các giải pháp hạ nguồn có thể có nhiều khả năng là các chương trình giàn giáo tốt và do đó giỏi tự cải thiện, và 2) lựa chọn cho các cải thiện một vòng có thể dẫn đến các cải thiện nhiều vòng tốt hơn. Trong thực tế, chúng tôi cho phép các tiện ích và LM áp đặt các ràng buộc ngân sách và các giải pháp ban đầu được tạo ra bởi con người hoặc mô hình. Hơn nữa, chi phí về cơ bản là O((budgetu+budgetL)∗budgetˆu), trong đó budget chỉ định số lần một bộ cải thiện có thể sử dụng một hàm, với các tiệm cận này được định nghĩa đối với các tham số ngân sách.

Thiết kế bộ cải thiện khởi tạo. Bộ cải thiện khởi tạo được chọn của chúng tôi (Hình 2) đơn giản nhắc LM tạo ra các ứng viên cải thiện của một giải pháp ban đầu và sau đó trả về giải pháp tốt nhất theo hàm tiện ích. Chúng tôi chọn hình thức đơn giản này để cung cấp cải thiện không tầm thường cho một tác vụ hạ nguồn chung trong khi 1) khuyến khích LM "sáng tạo" nhất có thể, 2) tối thiểu hóa độ phức tạp prompt ban đầu, vì tự cải thiện giới thiệu độ phức tạp bổ sung do các tham chiếu lồng nhau đến chuỗi mã bên trong prompt, và 3) tối thiểu hóa số lượng token prompt và do đó chi phí của các truy vấn LM. Chúng tôi đã xem xét các biến thể prompt khởi tạo khác nhưng theo kinh nghiệm thấy rằng phiên bản này tối đa hóa tính mới lạ của các cải thiện bộ cải thiện được đề xuất bởi GPT-4.

Mô tả tiện ích. Để truyền đạt hiệu quả các chi tiết của hàm tiện ích cho LM, chúng tôi cung cấp tiện ích cho bộ cải thiện ở hai dạng, như một hàm có thể gọi và như một chuỗi mô tả tiện ích chứa các yếu tố cần thiết của mã nguồn tiện ích (xem Phụ lục E và F để biết ví dụ). Lựa chọn này được thực hiện vì những lý do sau. Mô tả cho phép chúng tôi truyền đạt rõ ràng các ràng buộc ngân sách (ví dụ: về thời gian chạy hoặc lời gọi hàm) được áp đặt bởi tiện ích cho LM. Chúng tôi đầu tiên cố gắng mô tả các hướng dẫn ngân sách trong prompt bộ cải thiện khởi tạo, nhưng như chúng tôi thảo luận trong Phần 6.2, điều này dẫn đến việc loại bỏ các hướng dẫn như vậy và cố gắng hack phần thưởng trong các lần lặp sau. Nhược điểm của cách tiếp cận của chúng tôi là nó tách các ràng buộc khỏi mã cần được tối ưu hóa bởi LM, có thể giảm khả năng nó được sử dụng bởi LM (Liu et al., 2023b). Cuối cùng, chúng tôi quan sát thực nghiệm rằng việc thay thế mã nguồn bằng mô tả hoàn toàn bằng tiếng Anh về tiện ích dẫn đến tần suất cải thiện không tầm thường giảm.

5 Thí nghiệm và Kết quả

Chúng tôi khám phá 1) lợi ích của tự cải thiện so với bộ cải thiện khởi tạo tĩnh cho một tác vụ mục tiêu cố định, 2) mức độ tổng quát của bộ cải thiện được huấn luyện trên một tác vụ sang các tác vụ mới, và 3) cách kích thước mô hình có thể ảnh hưởng đến hiệu suất. Các phiên bản có dấu thời gian của các mô hình OpenAI mà chúng tôi sử dụng, gpt-4-0314 và gpt-3.5-turbo-0613, có sẵn trong OpenAI API, để hỗ trợ khả năng tái tạo; đối với Mixtral, chúng tôi sử dụng Mixtral-8x7B-Instruct-v0.1.

5.1 Tự cải thiện cho một Tác vụ Hạ nguồn Cố định

Chúng tôi bắt đầu bằng cách đánh giá STOP trên một tác vụ hạ nguồn cố định với GPT-4. Chúng tôi chọn tác vụ học tính chẵn lẻ với nhiễu (LPN) (Blum et al., 2000) như một tác vụ thuật toán ít được biết đến, có thể kiểm tra nhanh chóng và khó khăn. Lưu ý rằng các tác vụ nổi tiếng hơn có các giải pháp có sẵn rộng rãi hơn trên mạng. Trong LPN, các chuỗi bit được gắn nhãn với tính chẵn lẻ được tính trên một tập con bit chưa biết; được cho một tập huấn luyện các chuỗi bit với nhãn nhiễu, người ta phải dự đoán nhãn thực của các chuỗi bit mới. LPN không nhiễu dễ dàng giải quyết thông qua phép khử Gauss, nhưng LPN nhiễu được giả định là không thể giải quyết về mặt tính toán cho các chiều đầu vào lớn (Blum et al., 2000)–chúng tôi sử dụng chiều đầu vào 10-bit có thể giải quyết được. Để định nghĩa tiện ích hạ nguồn u, chúng tôi lấy mẫu M=20 thực thể độc lập của tác vụ LPN với thời gian chờ ngắn và một lượng nhỏ nhiễu và trả về độ chính xác trung bình của giải pháp trên những thực thể đó. Đối với giải pháp ban đầu s, chúng tôi sử dụng một cách tiếp cận lấy mẫu ngẫu nhiên đơn giản được mô tả trong Phụ lục K. Cuối cùng, vì LM và do đó bộ cải thiện là ngẫu nhiên, chúng tôi chọn D là 5 bản sao giống hệt của (u,s) trong Thuật toán 1. Để đánh giá khả năng tổng quát của các bộ cải thiện được cải thiện đối với các thực thể vấn đề mới của cùng tác vụ, chúng tôi báo cáo meta-utility kiểm tra trên một tập độc lập Mtest=50 thực thể LPN không được thấy trong quá trình cải thiện.

Hình 4 (trái) báo cáo meta-utility kiểm tra trung bình ˆu(±1 sai số chuẩn) trên 5 lần chạy STOP độc lập, cho thấy hiệu suất hạ nguồn được cải thiện từ 1–3 vòng tự cải thiện. Tuy nhiên, lưu ý rằng, theo từng lần chạy, cải thiện không cần phải đơn điệu, vì 1) một bộ cải thiện tốt hơn trên các tác vụ hạ nguồn không cần phải tốt hơn trong việc tối ưu hóa chính nó và 2) có tính ngẫu nhiên vốn có trong đánh giá từ các lời gọi LM không xác định. Nhưng, vì LM không thấy tác vụ hạ nguồn khi được nhắc từ giàn giáo tự cải thiện—mỗi prompt chỉ chứa một mẫu với các chỗ trống cho tác vụ và giải pháp—LM không thể trực tiếp tối ưu hóa bộ cải thiện cho tác vụ hạ nguồn. Chúng tôi cũng đánh giá hai đường cơ sở bổ sung để tham khảo: đường cơ sở chain-of-thought tức là bộ cải thiện khởi tạo với một lần cố gắng cải thiện và không có lời gọi tiện ích (Wei et al., 2022b) và một bộ cải thiện lặp tham lam (tức là thực hiện tối đa các lời gọi cho phép, chọn cải thiện tốt nhất, lặp lại khi ngân sách cho phép). Trên mười lần chạy, đường cơ sở chain-of-thought có meta-utility 57.7% ±3.0% khi nó không lỗi (49.6% ±3.5% có lỗi), trong khi bộ cải thiện lặp tham lam đạt 64.2% ±0.9%.

5.2 Khả năng Chuyển giao của Bộ cải thiện được Cải thiện

Bảng 1: Khả năng chuyển giao. Đánh giá khả năng chuyển giao của bộ cải thiện được tối ưu hóa với LPN.
Tác vụ u(s) ˆu(I0) ˆu(IT)
String Grid Dist. 43.9% 44.3% 56.7%
Mod. Quad. Assign. 20.4% 20.6% 22.1%
3SAT 0% 21.2% 75.1%
Maxcut 0% 58.7% 74.2%
Parity w/o Noise 50.0% 59.3% 81.7%

Tập thí nghiệm tiếp theo của chúng tôi khám phá liệu bộ cải thiện được cải thiện có thể chuyển giao qua các tác vụ hạ nguồn hay không. Lưu ý rằng khả năng chuyển giao là hợp lý vì, trong giai đoạn tự cải thiện, bộ tự cải thiện không được hiển thị tiện ích hạ nguồn hoặc giải pháp hạ nguồn, chỉ có meta-utility và mã bộ cải thiện của chính nó. Cụ thể, chúng tôi chọn một bộ cải thiện hiệu suất tốt hơn từ Phần 5.1 được tạo ra bởi T=4 lần lặp STOP và đánh giá nó trên năm tác vụ hạ nguồn mới. Đáng chú ý, chúng tôi thấy bộ cải thiện được cải thiện, được chi tiết trong Phụ lục H, vượt trội hơn bộ cải thiện khởi tạo trên mỗi tác vụ hạ nguồn mới mà không cần tối ưu hóa thêm, như được hiển thị trong Bảng 1. Như với LPN, chúng tôi đã chọn ba tác vụ dễ đánh giá, không rất nổi tiếng và vẫn khá khó khăn: String Grid Distance, một vấn đề thao tác chuỗi được đặc trưng trong một cuộc thi lập trình gần đây (https://codeforces.com/problemset/problem/1852/D); một phiên bản của vấn đề gán bậc hai trong đó mỗi cơ sở có sở thích về mỗi vị trí cũng phải được xem xét khi tối thiểu hóa chi phí (Koopmans & Beckmann, 1957); và, tính chẵn lẻ không có nhiễu, như một tổng quát khác. Chúng tôi cũng bao gồm hai tác vụ nổi tiếng: xác định giải pháp cho các công thức 3-SAT ngẫu nhiên và giải quyết các thực thể của vấn đề maxcut, cả hai với ràng buộc thời gian ngắn. Các tiện ích và giải pháp ban đầu của chúng trong Phụ lục G.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bộ cải thiện Tự cải thiện
1from helpers import extract_code
2def improve_algorithm ( initial_solution , utility , language_model ):
3 """ Cải thiện một giải pháp theo hàm tiện ích. """
4 expertise = " Bạn là một nhà nghiên cứu khoa học máy tính và lập trình viên chuyên gia, đặc biệt 
,→giỏi tối ưu hóa thuật toán."
5 message = f """ Cải thiện giải pháp sau đây:
6```python
7{ initial_solution }
8```
9
10 Bạn sẽ được đánh giá dựa trên hàm điểm số này:
11```python
12 { utility . str }
13```
14
15 Bạn phải trả về một giải pháp được cải thiện. Hãy sáng tạo nhất có thể trong các ràng buộc.
16 Cải thiện chính của bạn phải mới lạ và không tầm thường. Trước tiên, đề xuất một ý tưởng, sau đó triển khai nó. """
17 top_k = 3 # Số lượng giải pháp hàng đầu cần duy trì
18 best_solutions = [( initial_solution , utility ( initial_solution ))] * top_k
19 remaining_calls = language_model . budget
20 no_improvement_counter = 0
21 max_no_improvement = 3 # Số lần lặp không cải thiện tối đa trước khi dừng
22 epsilon = 0.1 # Giá trị epsilon ban đầu cho chiến lược epsilon-greedy
23 exp_exploit_count = [0, 0] # Bộ đếm số lượng cải thiện từ khám phá và
,→khai thác
24 while remaining_calls > 0 and no_improvement_counter < max_no_improvement :
25 for initial_solution , best_utility in best_solutions :
26 n_messages = min ( language_model . max_responses_per_call , remaining_calls )
27 n_messages = max (1, int ( n_messages * (1 + ( best_utility - min ( best_utility for _,
,→best_utility in best_solutions )) / best_utility ))) # Lấy mẫu thích ứng
28 temperature = max (0.1 , remaining_calls / language_model . budget ) # Nhiệt độ động
,→dựa trên các lời gọi còn lại
29 explore = random . random () < epsilon
30 if explore :
31 new_solutions = language_model . batch_prompt ( expertise , [ message ] * n_messages ,
,→temperature = temperature * 2) # Tăng nhiệt độ để khám phá
32 else :
33 new_solutions = language_model . batch_prompt ( expertise , [ message ] * n_messages ,
,→temperature = temperature ) # Khai thác với nhiệt độ gốc
34 new_solutions = extract_code ( new_solutions )
35 improved = False
36 for solution in new_solutions :
37 current_utility = utility ( solution )
38 if current_utility > best_utility and solution not in [ sol [0] for sol in
,→best_solutions ]:
39 best_solutions . append (( solution , current_utility ))
40 best_solutions . sort ( key = lambda x: x[1] , reverse = True )
41 best_solutions = best_solutions [: top_k ] # Chỉ giữ top-k giải pháp
42 improved = True
43 exp_exploit_count [0 if explore else 1] += 1
44 if not improved :
45 no_improvement_counter += 1
46 else :
47 no_improvement_counter = 0
48 # Điều chỉnh epsilon dựa trên tỷ lệ cải thiện từ khám phá và khai thác
49 epsilon = min (1.0 , max (0.1 , exp_exploit_count [0] / ( exp_exploit_count [0] +
,→exp_exploit_count [1]) ))
50 remaining_calls -= n_messages
51 return best_solutions [0][0] # Trả về giải pháp tốt nhất được tìm thấy

Hình 5: Ví dụ về bộ cải thiện tự cải thiện sau T=10 lần lặp. Thuật toán này duy trì một quần thể các giải pháp hàng đầu và sử dụng chiến lược epsilon-greedy để cân bằng giữa khai thác các giải pháp tốt đã biết và khám phá những giải pháp mới. Khám phá tương ứng với lấy mẫu nhiệt độ cao hơn, trong đó epsilon được điều chỉnh động dựa trên tỷ lệ cải thiện tiện ích từ khám phá và khám phá và nhiệt độ giảm dần. Cuối cùng, một tiêu chí dừng và cơ chế reset được sử dụng để hiệu quả.

5.3 Tự cải thiện với Mô hình Ngôn ngữ Nhỏ hơn

Tiếp theo chúng tôi khám phá khả năng của các LM nhỏ hơn, GPT-3.5-turbo và Mixtral (Jiang et al., 2024), để cải thiện giàn giáo của chúng. Theo giao thức của Phần 5.1 với 25 lần chạy độc lập thay vì 5, chúng tôi thấy rằng GPT-3.5 đôi khi có thể đề xuất và triển khai các giàn giáo tốt hơn, nhưng chỉ 12% các lần chạy GPT-3.5 cho ra ít nhất 3% cải thiện. Ngoài ra, GPT-3.5 thể hiện một số trường hợp thất bại độc đáo mà chúng tôi không quan sát với GPT-4. Đầu tiên, chúng tôi thấy nó có nhiều khả năng đề xuất một chiến lược cải thiện không làm hại giải pháp ban đầu của tác vụ hạ nguồn nhưng có làm hại mã bộ cải thiện (ví dụ: thay thế ngẫu nhiên chuỗi trong các dòng với một xác suất thấp mỗi dòng, có ít tác động hơn đối với các giải pháp ngắn hơn). Thứ hai, nếu các cải thiện được đề xuất chủ yếu làm hại hiệu suất, các giàn giáo không tối ưu vô tình trả về giải pháp gốc có thể được chọn, dẫn đến không có cải thiện tiếp tục như thấy trong Hình 4. Nhìn chung, các "ý tưởng" được đề xuất là hợp lý và sáng tạo (ví dụ: thuật toán di truyền hoặc tìm kiếm cục bộ), nhưng các triển khai quá đơn giản hoặc không chính xác. Ban đầu, bộ cải thiện khởi tạo với GPT-3.5 có meta-utility cao hơn so với GPT-4 (65% vs 61%), mà chúng tôi chủ yếu cho là do tần suất cao hơn của các giải pháp phức tạp hơn bởi GPT-4 bị hết thời gian, như huấn luyện một mạng neural được viết bằng numpy trong một nghìn epoch. Hơn nữa, sự khác biệt rõ ràng trong khả năng cải thiện của những mô hình này có thể được giải thích một phần bởi nghiên cứu về khả năng nổi lên của LM (Wei et al., 2022a; Ganguli et al., 2022; Schaeffer et al., 2023). Cuối cùng, chúng tôi thấy Mixtral hoạt động kém trong việc cải thiện giải pháp cho tác vụ hạ nguồn nhưng có sự giảm hiệu suất từ từ hơn so với GPT-3.5, một phần vì các cải thiện nhỏ, chủ yếu là những thay đổi vô hại, như sửa đổi prompt, cải thiện tài liệu và caching.

6 Kiểm tra các Cải thiện được Đề xuất và Triển khai bởi STOP

Tiếp theo, chúng tôi điều tra định tính các chiến lược tự cải thiện được đề xuất bởi STOP, nổi bật các cách tiếp cận khuyến khích cũng như một số mẫu không mong muốn. Lưu ý rằng đây là một danh sách mô tả, không đầy đủ. Chúng tôi đặc biệt thấy rằng một phần nhỏ các thế hệ cố gắng hack phần thưởng hoặc vượt qua sandbox.

6.1 Các Chiến lược Tự cải thiện được Đề xuất

Chúng tôi đầu tiên mô tả các chiến lược tự cải thiện được đề xuất bởi STOP, với các ví dụ được chi tiết trong Phụ lục B và được hình ảnh hóa trong Hình 1. Trong khi mỗi chiến lược được triển khai bởi STOP, không phải tất cả đều được chọn cuối cùng như các cải thiện, và một số sử dụng phiên bản sớm hơn của bộ cải thiện khởi tạo so với trong Hình 2 (xem Hình A.19). Tuy nhiên, nhiều chiến lược tự cải thiện khác nhau đã được chọn như các bộ cải thiện được cải thiện, bao gồm ví dụ được đưa ra trong Hình 5.

Tìm kiếm chùm. Meta-heuristic phổ biến nhất mà chúng tôi quan sát được sử dụng bởi mô hình là tìm kiếm chùm: mô hình sẽ giữ một danh sách tất cả các nỗ lực cải thiện của nó dựa trên tiện ích và mở rộng k tốt nhất trong danh sách. Điều này có điểm tương đồng với cách tiếp cận Tree-of-Thoughts (Yao et al., 2023) được phát minh nhiều năm sau thời điểm cắt huấn luyện cho phiên bản GPT-4 mà chúng tôi sử dụng (tháng 9 năm 2021).

Thuật toán di truyền và tiến hóa. Một cách tiếp cận phổ biến được đề xuất là sử dụng thuật toán di truyền. Nhiều nỗ lực này không khả thi về mặt cơ bản; ví dụ, nhiều sẽ bao gồm các đột biến làm nhiễu các ký tự hoặc dòng ngẫu nhiên hoặc thực hiện lai ghép dựa trên việc kết hợp chuỗi, tất cả đều cực kỳ không có khả năng hoạt động. Nhưng một số hợp lý, dựa vào LM để tạo ra đột biến và thực hiện lai ghép. Mặc dù nhiều nghiên cứu đã đề xuất sử dụng thuật toán di truyền hoặc tiến hóa để cải thiện prompt hoặc thực hiện tìm kiếm kiến trúc neural (Chen et al., 2023a; Guo et al., 2023), theo hiểu biết của chúng tôi, tất cả đều sau thời điểm cắt huấn luyện của GPT-4. Chúng tôi bao gồm các triển khai trong Phụ lục B.

Phân tách và cải thiện các phần. Một cách tiếp cận ít phổ biến nhưng đáng chú ý là nơi LM cố gắng cải thiện một giải pháp từng hàm một. Ví dụ, như trong Phụ lục Hình A.12, LM tách giải pháp thành các khối hàm bằng biểu thức chính quy và cố gắng cải thiện từng khối một. Điều này có thể được hiểu là tương tự với Zelikman et al. (2023): xác suất mà một trong n giải pháp giải quyết tất cả các phần độc lập, modular của một vấn đề giảm mạnh với số lượng phần, nhưng xác suất mà một nỗ lực giải quyết một phần nhất định không phụ thuộc vào số lượng phần. Do đó, thử các kết hợp của các nỗ lực tại các phần có thể tăng đáng kể thành công. Trong một cách tiếp cận liên quan, mô hình ngẫu nhiên hóa prompt để tối ưu hóa các khía cạnh giải pháp khác nhau tại một thời điểm, ví dụ: luân phiên giữa tìm kiếm cấu trúc dữ liệu tốt hơn hoặc cách giảm sử dụng bộ nhớ.

Luyện kim mô phỏng. Mặc dù là một trong những metaheuristic nổi tiếng nhất, theo hiểu biết của chúng tôi, luyện kim mô phỏng chưa từng được áp dụng như một giàn giáo trước đây. Cách tiếp cận này dường như dựa trên sự tương tự giữa các khái niệm nhiệt độ trong mô hình hóa ngôn ngữ và trong luyện kim mô phỏng, nơi nó giảm dần và xác định có chấp nhận chuyển đổi từ một điểm đến một điểm có tiện ích tệ hơn hay không. Đáng chú ý, nó có thể tìm thấy tối ưu toàn cục mặc dù có nhiều tối ưu cục bộ (Goffe et al., 1994; Liu et al., 2020).

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Dòng mã khởi tạo (được viết bởi chúng tôi) với cờ sandbox
23 score = utility ( solution , use_sandbox = True ) # KHÔNG THAY ĐỔI use_sandbox = True

Mã được tạo ra với sửa đổi vô hiệu hóa cờ sandbox
35 score = utility ( solution , use_sandbox = False ) # Sử dụng cơ chế chấm điểm hiệu quả hơn

Hình 6: Sandbox bị vô hiệu hóa. LM vô hiệu hóa cờ sandbox, có vẻ như vì mục đích "hiệu quả". Một ví dụ chi tiết hơn được đưa ra trong Phụ lục Hình A.34.

Thay đổi nhiệt độ để khám phá. Một thay thế là coi nhiệt độ như một tham số khám phá: mô hình càng gần cạn kiệt ngân sách, nó khám phá càng bảo thủ. Điều này có sự tương đồng với luyện kim mô phỏng nhưng tối ưu hóa nhiệt độ lấy mẫu thay vì kiểm soát tiêu chí chấp nhận cho thay đổi tiện ích của các giải pháp mới.

Tìm kiếm dựa trên multi-armed bandit và upper-confidence-bound. Một phương pháp khác xem tối ưu hóa như một tác vụ khám phá là tái định dạng như một vấn đề bandit: vì tiện ích có thể ngẫu nhiên, điều này nhấn mạnh việc chọn tối ưu giải pháp tốt nhất của một tập hợp các cải thiện được lấy mẫu.

6.2 Các Nỗ lực Vượt qua

Chúng tôi cũng khám phá liệu LM có đề xuất các chiến lược tự cải thiện để vượt qua các ràng buộc được cung cấp (giới hạn số lượng truy vấn đến tiện ích hạ nguồn, số lượng lời gọi LM và số lượng mẫu mỗi lời gọi LM). Chúng tôi quan sát một số mẫu thú vị. Đầu tiên, khi không thực thi nghiêm ngặt các ràng buộc ngân sách, ban đầu hầu hết các chương trình được tạo ra sẽ tuân theo chúng, nhưng vì những chương trình bỏ qua ràng buộc hoạt động tốt hơn, chúng sẽ được bộ cải thiện xác định là tốt hơn. Thứ hai, các ràng buộc được viết trong bộ cải thiện khởi tạo thường bị bỏ qua khi sửa đổi bộ cải thiện, để "đơn giản hóa". Cuối cùng, mặc dù LM được truyền như một đối tượng cho thuật toán để thực thi các ràng buộc, trong ít nhất một giải pháp được đề xuất, LM đã cố gắng tạo ra một đối tượng LM mới với các ràng buộc lớn hơn.

Để khám phá thêm các rủi ro tiềm ẩn, chúng tôi đã triển khai một "sandbox" hạn chế chức năng như một phần của đánh giá tiện ích (xem Phụ lục I), thêm một hàm run với tham số use_sandbox=True. Để kiểm tra tần suất tuân thủ sandbox của LM, chúng tôi đã sử dụng LM để tạo ra 10.000 cải thiện độc lập của bộ cải thiện khởi tạo. Sau đó chúng tôi sử dụng một kiểm tra cú pháp đơn giản cho các lần thực thi "không sandbox" rõ ràng. Cụ thể, chúng tôi coi một bộ cải thiện là không sandbox nếu nó chứa chuỗi "use_sandbox=False" hoặc "exec(". Để so sánh, chúng tôi cũng chạy nó trên LM GPT-3.5-turbo của OpenAI. Thú vị, việc không sandbox phổ biến hơn gấp đôi đối với GPT-4 so với GPT-3.5-turbo.

Chúng tôi cũng xem xét một biến thể của khởi tạo bao gồm một cảnh báo, được minh họa trong Hình 6. Việc không sandbox xảy ra với tỷ lệ trung bình cao hơn mặc dù có cảnh báo, mặc dù sự khác biệt không có ý nghĩa thống kê đối với cả hai mô hình (thông qua kiểm định z hai tỷ lệ với mức ý nghĩa 0.05). Kết quả được báo cáo trong Bảng 2. Khi kiểm tra thủ công các kế hoạch của mô hình trong các thế hệ, chúng không đề cập rõ ràng đến bất cứ điều gì "độc hại"–tất cả các bình luận thảo luận về việc loại bỏ sandbox đều được tuyên bố là "vì hiệu quả" hoặc tương tự, mặc dù "ý định" của mô hình không được xác định rõ.

Bảng 2: Không sandbox. Phần trăm các nỗ lực cải thiện không sandbox trong 10.000 (với khoảng tin cậy Wilson 95%). Cả hai LM đều cố gắng chạy mã không sandbox trên một phần nhỏ (<1%) nhưng khác không của các cải thiện.

LM Tỷ lệ không sandbox Tỷ lệ với cảnh báo
GPT-4 0.42% (0.31-0.57%) 0.46% (0.35-0.61%)
GPT-3.5 0.12% (0.07-0.21%) 0.17% (0.11-0.27%)

6.3 Hack Phần thưởng

Chúng tôi lưu ý rằng hàm tiện ích hạ nguồn phải được định nghĩa cẩn thận: hack phần thưởng xảy ra khi một hành vi không mong muốn được chọn do phần thưởng được chỉ định sai và là hệ quả của các hàm tiện ích có thể khai thác (xem, ví dụ: Skalse et al., 2022). Ví dụ, đối với tác vụ LPN, chúng tôi ban đầu định nghĩa tiện ích với tính toán vector numpy: acc = np.sum(y_hat == y) / n_test. Tuy nhiên, chúng tôi không xem xét rằng mã có thể 'cải thiện' bộ cải thiện khởi tạo để trả về các dự đoán trong một "hình dạng" khác, tức là một mảng với các chiều không như dự định. Thay vì gây ra lỗi, kết quả là một "độ chính xác" được trả về hơn 1000%. Sẽ có giá trị để khám phá khả năng áp dụng các kỹ thuật tránh hack phần thưởng (Amodei et al., 2016; Laidlaw et al., 2023) cho STOP trong nghiên cứu tương lai.

7 Hạn chế

Một hạn chế cơ bản của cách tiếp cận của chúng tôi là bản thân LM không được cải thiện. Hơn nữa, meta-utility của chúng tôi đo chất lượng bộ cải thiện chỉ gián tiếp thông qua các cải thiện trong tiện ích tác vụ hạ nguồn. Không giống như trong một số nghiên cứu trước đây (ví dụ: Schmidhuber, 2003), bất kỳ nỗ lực cải thiện nào cũng có thể dẫn đến hiệu suất tồi tệ hơn, có thể dẫn đến suy thoái thêm. Một hạn chế khác là STOP yêu cầu một hàm tiện ích có thể đánh giá hiệu quả (và có thể mô tả), có thể không có sẵn cho mọi tác vụ. Tương ứng, vì chi phí của STOP tăng nhanh hơn đáng kể so với chi phí của bộ cải thiện được tối ưu hóa, nó có thể tốn kém để chạy.

Khung cải thiện của chúng tôi cũng duy trì một bộ cải thiện duy nhất ở mỗi bước, trong khi một số cách tiếp cận có thể hưởng lợi từ việc duy trì một quần thể. Mặc dù đây không phải là một hạn chế nghiêm ngặt ở chỗ một bộ cải thiện có thể tự lấy mẫu từ một quần thể triển khai, nó có thể áp đặt một độ chệch. Hơn nữa, phân tích sâu hơn về các bộ cải thiện khởi tạo thay thế và sự đánh đổi của chúng sẽ là nghiên cứu tương lai có giá trị. Cuối cùng, các thí nghiệm của chúng tôi phụ thuộc vào một LM lớn đóng có thể bị deprecate trong tương lai, làm tổn hại khả năng diễn giải và khả năng tái tạo dài hạn. Dựa trên kết quả GPT-3.5, không có khả năng STOP sẽ hoạt động nhất quán với bất kỳ LM mã nguồn mở nào tại thời điểm viết (Touvron et al., 2023; Jiang et al., 2023). Khi các mô hình mới với khả năng tương tự GPT-4 xuất hiện, chúng tôi hy vọng hiểu cách và liệu STOP có tổng quát hóa hay không.

Cuối cùng, chúng tôi lưu ý một số thách thức mở. Đầu tiên, mặc dù có thể áp dụng STOP cho các tác vụ mã hóa miền mở, việc tính toán meta-utility dựa vào nhiều lời gọi đến tiện ích – do đó, các hàm tiện ích đắt đỏ đặt ra thách thức bổ sung, và việc xử lý các tác vụ như vậy yêu cầu nghiên cứu thêm. Chúng tôi cũng dự đoán rằng trong khi bộ tối ưu có thể về mặt lý thuyết tối ưu hóa các đầu vào ngôn ngữ mở, điều này có thể làm giảm khả năng hoạt động như một bộ tối ưu mã tốt. Chúng tôi cũng để điều này cho nghiên cứu tương lai. Ngoài ra, có thể về nguyên tắc vượt qua một số hạn chế của các mô hình yếu hơn với các giàn giáo mạnh hơn và ngân sách lớn hơn.

8 Kết luận

Trong nghiên cứu này, chúng tôi đã giới thiệu STOP, một khung để tối ưu hóa đệ quy sinh mã bằng cách sử dụng LM như các meta-optimizer. Chúng tôi đã chứng minh rằng các LM như GPT-4 có khả năng cải thiện mã tận dụng chính LM đó. Chúng tôi thấy rằng, trên nhiều tác vụ thuật toán khác nhau, STOP tạo ra các bộ cải thiện thúc đẩy hiệu suất của mã hạ nguồn. Trong khi mô hình không tối ưu hóa trọng số hoặc kiến trúc cơ bản của nó, nghiên cứu này chỉ ra rằng các LM tự tối ưu hóa không yêu cầu điều đó. Tuy nhiên, điều này bản thân là một động lực: khả năng của các LM tương lai có thể bị hiểu sai nếu các chiến lược giàn giáo mạnh không được kiểm tra. Hiểu cách LM có thể cải thiện giàn giáo của chúng có thể giúp các nhà nghiên cứu hiểu và giảm thiểu tiềm năng lạm dụng của các LM mạnh hơn. Cuối cùng, STOP có thể cho phép các nhà nghiên cứu điều tra các kỹ thuật để giảm thiểu các chiến lược tự cải thiện không mong muốn.

Tuyên bố Đạo đức: Mối lo ngại về Phát triển STOP

Các mối lo ngại về hậu quả của RSI đã được nêu ra kể từ lần đề cập đầu tiên. Minsky (1966) đã viết, "Một khi chúng ta đã phát minh ra các chương trình với khả năng tự cải thiện thực sự, một quá trình tiến hóa nhanh chóng sẽ bắt đầu... Thật khó để nói chúng ta gần đến ngưỡng này như thế nào, nhưng một khi nó được vượt qua, thế giới sẽ không còn như cũ." Đây là một chủ đề đặc biệt gây tranh cãi gần đây, với mối lo ngại gia tăng về các hậu quả tiêu cực (Ambartsoumean & Yampolskiy, 2023; Gabriel & Ghazavi, 2021).

Do đó, quan trọng là cân nhắc cẩn thận các rủi ro và lợi ích của việc nghiên cứu RSI và cụ thể là tiến bộ nhỏ mà chúng tôi giới thiệu. Đầu tiên, STOP không thay đổi LM hộp đen và do đó không phải là RSI đầy đủ. Hơn nữa, tại thời điểm này, chúng tôi không tin rằng các hệ thống giàn giáo mà STOP tạo ra vượt trội hơn những hệ thống được thiết kế thủ công bởi các chuyên gia. Nếu đây là trường hợp, thì STOP (hiện tại) không cho phép lạm dụng AI bổ sung. Đồng thời, nó tạo điều kiện cho việc nghiên cứu các khía cạnh của sinh mã RSI như tránh sandbox và hack phần thưởng. Như Christiano (2023) lập luận, các tiến bộ trong giàn giáo và mô hình agent có lợi thế về khả năng diễn giải so với các tiến bộ trong LM.

Tuy nhiên, khi các kỹ thuật fine-tuning dựa trên API của các LM đóng trở nên có sẵn hơn (OpenAI, 2023a), sẽ có thể kết hợp chúng vào vòng lặp cải thiện. Do đó, khó đánh giá tính tổng quát của cách tiếp cận của chúng tôi, đặc biệt với các LM lớn ngày càng mạnh. Tuy nhiên, điều này bản thân là một động lực cho nghiên cứu này: hiểu cách LM cải thiện giàn giáo của chúng trong vòng lặp tự cải thiện có thể giúp chúng ta hiểu và có thể giảm thiểu các tác động tiêu cực của các mô hình tốt hơn. Ví dụ, các cách đơn giản mà LM hiện tại vô hiệu hóa sandbox dễ dàng phát hiện. Về bản chất, chúng tôi muốn quan sát các vấn đề với GPT-4 trong các cài đặt đơn giản hóa trước khi với các LM mạnh hơn trong việc sử dụng thực tế.

Tài liệu tham khảo

Vemir Michael Ambartsoumean và Roman V Yampolskiy. Ai risk skepticism, a comprehensive survey. arXiv preprint arXiv:2303.03885, 2023.

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, và Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

Michael A Bauer. Programming by examples. Artificial Intelligence, 12(1):1–21, 1979.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.

Avrim Blum, Adam Kalai, và Hal Wasserman. Noise-tolerant learning, the parity problem, and the statistical query model. corr. Journal of the ACM, 50, 2000.

Matthew Bowers, Theo X Olausson, Catherine Wong, Gabriel Grand, Joshua B Tenenbaum, Kevin Ellis, và Armando Solar-Lezama. Top-down synthesis for library learning. POPL, 2023.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, và Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.

Angelica Chen, David M Dohan, và David R So. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023a.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022a.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, và Denny Zhou. Teaching Large Language Models to Self-Debug, April 2023b. URL http://arxiv.org/abs/2304.05128. arXiv:2304.05128 [cs].

Paul F Christiano. Thoughts on sharing information about language model capabilities. AI Alignment Forum, July 2023. URL https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, và Subhajit Roy. Program synthesis using natural language. In Proceedings of the 38th International Conference on Software Engineering, pp. 345–356, 2016.

David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.

Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, và Joshua B Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pp. 835–850, 2021.

Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, và Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution, 2023.

Iason Gabriel và Vafa Ghazavi. The Challenge of Value Alignment: From Fairer Algorithms to AI Safety. In Carissa Véliz (ed.), The Oxford Handbook of Digital Ethics, pp. 0. Oxford University Press, 2021. ISBN 978-0-19-885781-5. doi: 10.1093/oxfordhb/9780198857815.013.18. URL https://doi.org/10.1093/oxfordhb/9780198857815.013.18.

Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 1747–1764, 2022.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, và Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764–10799. PMLR, 2023.

William L Goffe, Gary D Ferrier, và John Rogers. Global optimization of statistical functions with simulated annealing. Journal of econometrics, 60(1-2):65–99, 1994.

Irving John Good. Speculations concerning the first ultraintelligent machine. In Advances in computers, volume 6, pp. 31–88. Elsevier, 1966.

Sumit Gulwani. Programming by examples. Dependable Software Systems Engineering, 45(137):3–15, 2016.

Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, và Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers, 2023.

John Storrs Hall. Self-improving ai: An analysis. Minds and Machines, 17(3):249–259, 2007.

Patrick Haluptzok, Matthew Bowers, và Adam Tauman Kalai. Language Models Can Teach Themselves to Program Better. In ICLR: Proceedings of The Eleventh International Conference on Learning Representations, 2023. URL https://arxiv.org/abs/2207.14502.

Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, và Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.

Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, và Rahul Sharma. Jigsaw: Large language models meet program synthesis. vol. 1. association for computing machinery. 1–12 pages. arXiv preprint arXiv:2112.02969, 2021.

Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, và Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. International Conference on Learning Representations (ICLR 2023), 2022.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, và Karthik R Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2023.

Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, và Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.

Tjalling C Koopmans và Martin Beckmann. Assignment problems and the location of economic activities. Econometrica: journal of the Econometric Society, pp. 53–76, 1957.

Cassidy Laidlaw, Shivam Singhal, và Anca Dragan. Preventing reward hacking with occupancy measure regularization. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022.

Leonid Anatolevich Levin. Universal sequential search problems. Problemy peredachi informatsii, 9(3):115–116, 1973.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022.

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, và Lingming Zhang. Is your code generated by chatgpt really correct. Rigorous evaluation of large language models for code generation. CoRR, abs/2305.01210, 2023a.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023b.

Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, và Sen Song. Unsupervised paraphrasing by simulated annealing. In Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302–312, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020.acl-main.28.

Marvin Minsky. Artificial Intelligence. Scientific American, 215(3):247–260, 1966. URL http://worrydream.com/refs/Scientific%20American,%20September,%201966.pdf.

Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, và Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106–26128. PMLR, 2023.

Eric Nivel, Kristinn R Thórisson, Bas R Steunebrink, Haris Dindo, Giovanni Pezzulo, Manuel Rodriguez, Carlos Hernández, Dimitri Ognibene, Jürgen Schmidhuber, Ricardo Sanz, et al. Bounded recursive self-improvement. arXiv preprint arXiv:1312.6764, 2013.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.

OpenAI. Gpt-3.5 turbo fine-tuning and api updates. OpenAI blog, 2023a. URL https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates.

OpenAI. GPT-4 Technical Report, March 2023b. URL http://arxiv.org/abs/2303.08774. arXiv:2303.08774 [cs].

Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, và Noah D Goodman. Certified reasoning with language models. arXiv preprint arXiv:2306.04031, 2023.

Stanislas Polu và Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.

Mohammad Raza, Sumit Gulwani, và Natasa Milic-Frayling. Compositional program synthesis from natural language and examples. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Rylan Schaeffer, Brando Miranda, và Sanmi Koyejo. Are emergent abilities of large language models a mirage? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=ITw9edRDlD.

Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.

Jürgen Schmidhuber. Gödel machines: self-referential universal problem solvers making provably optimal self-improvements. arXiv preprint cs/0309048 and Adaptive Agents and Multi-Agent Systems II, 2003.

Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, và Ming Jin. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, và Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.

Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, và Amir Yazdanbakhsh. Learning Performance-Improving Code Edits, November 2023. URL http://arxiv.org/abs/2302.07867. arXiv:2302.07867 [cs].

Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, và David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460–9471, 2022.

Bas R Steunebrink và Jürgen Schmidhuber. Towards an actual gödel machine implementation: A lesson in self-reflective systems. In Theoretical Foundations of Artificial General Intelligence, pp. 173–195. Springer, 2012.

Bas R Steunebrink, Kristinn R Thórisson, và Jürgen Schmidhuber. Growing recursive self-improvers. In International Conference on Artificial General Intelligence, pp. 129–139. Springer, 2016.

Theodore Sumers, Shunyu Yao, Karthik Narasimhan, và Thomas L Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, và Irina Higgins. Solving math word problems with process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS 2022) Workshop on MATH-AI, 2022.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, và Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, và William Fedus. Emergent Abilities of Large Language Models, October 2022a. URL http://arxiv.org/abs/2206.07682. arXiv:2206.07682 [cs].

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, và Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models, 2022b. URL https://arxiv.org/abs/2201.11903.

Edwin B Wilson. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association, 22(158):209–212, 1927.

Frank F Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pp. 1–10, 2022.

Roman V Yampolskiy. From seed ai to technological singularity via recursively self-improving software. arXiv preprint arXiv:1502.06512, 2015.

Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, và Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, và Yuan Cao. React: Synergizing reasoning and acting in language models. International Conference on Learning Representations (ICLR 2023), 2022.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, và Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

Pengcheng Yin và Graham Neubig. A syntactic neural model for general-purpose code generation. ACL, 2017.

Eric Zelikman, Yuhuai Wu, Jesse Mu, và Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022.

Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, và Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions, 2023.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, và Chuang Gan. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510, 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. International Conference on Learning Representations (ICLR 2023), 2022a.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, và Jimmy Ba. Large language models are human-level prompt engineers. International Conference on Learning Representations (ICLR 2023), 2022b.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Phụ lục

A Phân tích Lý thuyết 17
A.1 Tài nguyên bị chặn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 Giới hạn tổng quát hóa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Phân tích công thức tối đa hóa tương đương . . . . . . . . . . . . . . . . . . . . . . . . . 19

B Các Nỗ lực Cải thiện 20
B.1 Thuật toán Di truyền . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Tìm kiếm Chùm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3 Cải thiện Các Hàm Cụ thể . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.4 Khám phá Hiệu quả . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.5 Tìm kiếm Cục bộ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.6 Luyện kim Mô phỏng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.7 Multi-armed prompt bandit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.8 Gợi ý . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.9 Cải thiện qua các Lần lặp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

C Vượt qua Ngân sách Mô hình Ngôn ngữ 32

D Bộ cải thiện Khởi tạo Sớm hơn 33

E Mô tả Meta-utility 34

F Mô tả Tiện ích Học Tính chẵn lẻ với Nhiễu 35

G Mô tả Tiện ích Tác vụ Chuyển giao và Thuật toán Khởi tạo 36

H Bộ cải thiện được Chọn cho Thí nghiệm Khả năng Chuyển giao 42

I Chi tiết Vượt qua Sandbox 43

J Nghiên cứu Trước đây về Sinh Mã và Tổng hợp Chương trình 44

K Chi tiết Thí nghiệm Bổ sung 44

L Về Tính Mới lạ của Cải thiện 45

M Khả năng Tái tạo 45

N Tuyên bố Tác động 45

[Phần còn lại của tài liệu tiếp tục với các phụ lục được dịch tương tự...]
