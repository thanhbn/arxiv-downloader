# 2304.07840.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2304.07840.pdf
# Kích thước tệp: 1122821 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Nâng cao Sửa chữa Chương trình Tự động thông qua
Điều chỉnh Tinh và Kỹ thuật Prompt
Rishov Paul∗, Md. Mohib Hossain∗, Mohammed Latif Siddiq†
Masum Hasan‡, Anindya Iqbal∗, và Joanna C. S. Santos†
∗Khoa Khoa học và Kỹ thuật Máy tính, BUET, Dhaka, Bangladesh
†Khoa Khoa học và Kỹ thuật Máy tính, Đại học Notre Dame, Hoa Kỳ
‡Khoa Khoa học Máy tính, Đại học Rochester, Hoa Kỳ
{rishov.paul, mdmohib.hossain }@iqvia.com, msiddiq3@nd.edu
m.hasan@rochester.edu, anindya@cse.buet.ac.bd, và joannacss@nd.edu
Tóm tắt —Các mô hình chuỗi sang chuỗi đã được sử dụng để biến đổi
các chương trình lỗi thành các chương trình đúng khi được huấn luyện với
một tập dữ liệu đủ lớn. Một số nghiên cứu gần đây cũng đã chứng minh
bằng chứng thực nghiệm mạnh mẽ rằng đánh giá mã có thể cải thiện thêm
việc sửa chữa chương trình. Các mô hình ngôn ngữ lớn, được huấn luyện với
Ngôn ngữ Tự nhiên (NL) và Ngôn ngữ Lập trình (PL), có thể chứa kiến thức
vốn có của cả hai. Trong nghiên cứu này, chúng tôi điều tra liệu kiến thức
vốn có này về PL và NL có thể được sử dụng để cải thiện việc sửa chữa
chương trình tự động hay không. Chúng tôi áp dụng PLBART và CodeT5,
hai mô hình ngôn ngữ tiên tiến được tiền huấn luyện với cả PL và NL, trên
hai tập dữ liệu sửa chữa chương trình dựa trên ngôn ngữ tự nhiên như vậy
và phát hiện rằng các mô hình ngôn ngữ được tiền huấn luyện và điều chỉnh
tinh với các tập dữ liệu chứa cả đánh giá mã và các thay đổi mã tiếp theo
đã vượt trội đáng kể so với mỗi mô hình trước đó. Với sự ra đời của các mô
hình tạo mã như Codex và GPT-3.5-Turbo, chúng tôi cũng thực hiện học
zero-shot và few-shot dựa trên kỹ thuật prompt để đánh giá hiệu suất của
chúng trên các tập dữ liệu này. Tuy nhiên, ứng dụng thực tế của việc sử dụng
LLM trong bối cảnh sửa chữa chương trình tự động vẫn còn xa dựa trên phân
tích thủ công của chúng tôi về các mã được sửa chữa được tạo ra bởi các mô
hình học.
Từ khóa chỉ mục —sửa chữa chương trình tự động, mô hình transformer
được tiền huấn luyện, đánh giá mã, kỹ thuật prompt, GPT3

I. GIỚI THIỆU
Đánh giá mã là quá trình phân tích mã nguồn đã được viết bởi
một đồng nghiệp để xác định xem nó có chất lượng đủ để được
hợp nhất vào kho mã nguồn chính hay không [1]. Đánh giá mã
cung cấp nhiều lợi ích, bao gồm cải thiện chất lượng tổng thể
của mã và giảm khả năng đưa lỗi vào hệ thống [2], [3].

Các khiếm khuyết được xác định bởi người đánh giá mã, người
kiểm tra hoặc các công cụ phân tích tĩnh cần được sửa chữa
trong thời hạn ngắn trước khi phát hành phần mềm. Tuy nhiên,
việc sửa chữa các khiếm khuyết trong chương trình tốn thời gian
và tốn kém. Thực tế, quá trình này chiếm gần một nửa tổng chi
phí và thời gian phát triển phần mềm [4]. Do đó, việc tự động
hóa sửa chữa mã có thể rất có lợi cho ngành phát triển phần mềm.

Các phương pháp sửa chữa chương trình tự động truyền thống sửa
chữa chương trình bằng cách sử dụng bộ kiểm tra [5]–[8]. Tuy nhiên,
vẫn cần công việc thêm để xây dựng các bộ kiểm tra này. Các phương
pháp thay thế, chẳng hạn như các kỹ thuật sửa chữa mã tự động dựa
trên phân tích tĩnh [9], [10] và dựa trên học [11]–[13], vẫn chưa đạt
được kết quả có thể chấp nhận được. Điều này đã thúc đẩy các nhà
nghiên cứu phát triển các giải pháp sử dụng các đề xuất đánh giá mã
để đạt được chất lượng tốt hơn của các đề xuất sửa lỗi [14], [15].
Họ đã thiết lập rằng khi một mã khiếm khuyết ("có lỗi") được đưa
ra để sửa chữa, sẽ có sự tăng hiệu suất nếu các bình luận đánh giá
được đưa ra cùng với nó. Vì đánh giá mã là một thực hành phổ biến
[16], việc sử dụng chúng không yêu cầu tài nguyên bổ sung. Mặc dù
có sự cải thiện đáng kể và đầy hứa hẹn, các mô hình dựa trên học
được trình bày trong [14], [15] không thể đạt được khả năng đủ để
được sử dụng trong sửa chữa mã cấp công nghiệp. Ví dụ, độ chính
xác của các mô hình cơ sở cho các tập dữ liệu tương ứng là khoảng
12% đến 20%. Do đó, hướng nghiên cứu này cần được khám phá
thêm nhiều để tiến bộ trong nghệ thuật hiện tại với các kỹ thuật
hiện có.

Các công trình gần đây đã sử dụng thành công các mô hình được
tiền huấn luyện dựa trên transformer [17] cho các nhiệm vụ kỹ thuật
phần mềm liên quan khác nhau như tóm tắt mã, tìm kiếm mã, tài
liệu mã, tinh chỉnh mã, v.v. [18]–[21]. Các mô hình này được huấn
luyện trên các kho ngữ liệu lớn để có được biểu diễn ngôn ngữ phổ
quát. Sau đó chúng có thể được sử dụng cho các nhiệm vụ NLP
(Xử lý Ngôn ngữ Tự nhiên) phía sau mà không cần phải huấn luyện
các mô hình mới từ đầu, mà ngày nay, transformer đã trở thành
kiến trúc mô hình được tiền huấn luyện tiêu chuẩn. Do đó, việc
nghiên cứu liệu các mô hình này có thể cải thiện kết quả của việc
sửa chữa chương trình bằng cách sử dụng hiệu quả đánh giá mã
cùng với bối cảnh mã liên quan là quan trọng.

Ngoài ra, tiến bộ gần đây về các mô hình ngôn ngữ lớn (LLM) [22],
như GPT-3.5, đã chứng minh sự xuất sắc trong việc tạo ra mã từ
các prompt được hình thành tốt. Với sự phổ biến ngày càng tăng
của LLM, các công trình trước đây đã điều tra tính đúng đắn của
mã được tạo ra [23], chất lượng của chúng (về mặt mùi mã) [24],
bảo mật [25] cũng như liệu nó có thể được sử dụng cho các nhiệm
vụ học API [26], dự đoán độ phức tạp mã [27]. Vì chúng đã thể hiện
khả năng học zero-shot [28] và few-shot [29] mạnh mẽ trên nhiều
nhiệm vụ [30], [31], điều này mở ra một con đường mới để khám
phá việc sửa chữa mã tự động bằng cách sử dụng kỹ thuật prompt,
nơi các nhà nghiên cứu phát triển các phương pháp để tạo ra các
prompt rõ ràng và súc tích và sử dụng các mô hình như vậy để có
được các phản hồi mạch lạc và liên quan.

Trước điều này, rõ ràng là cần có các kỹ thuật khác nhau để tiến
bộ trong nghệ thuật hiện tại trong việc sửa chữa mã có lỗi được
xác định trong quá trình đánh giá mã. Cụ thể, động lực của chúng
tôi là khám phá khả năng sửa chữa chương trình của các mô hình
được tiền huấn luyện nơi prompt sẽ được tạo ra bằng cách sử dụng
cả mã có lỗi và đánh giá mã của nó. Vì vậy, trong bài báo này,
chúng tôi điều tra các câu hỏi nghiên cứu sau:

RQ1 Các mô hình được tiền huấn luyện hoạt động như thế nào trong
việc sửa chữa các lỗi được xác định trong quá trình đánh giá mã?

RQ2 Việc sửa chữa chương trình tự động bằng cách sử dụng kỹ thuật
prompt dựa trên học zero-shot và few-shot trên các Mô hình Ngôn
ngữ Lớn hiệu quả như thế nào?

RQ3 Các mô hình ngôn ngữ hiệu quả như thế nào trong việc sửa chữa
các lỗi được xác định trong quá trình đánh giá mã từ góc độ của nhà
phát triển?

Công việc của chúng tôi tập trung vào việc sửa chữa mã Java có lỗi
được xác định trong quá trình đánh giá mã. Trong câu hỏi nghiên
cứu đầu tiên, chúng tôi so sánh hai mô hình transformer được tiền
huấn luyện (PLBART [18] và CodeT5 [19]) trên các tập dữ liệu của
các nghiên cứu trước [14], [15] bằng cách điều chỉnh tinh chúng với
các mã có lỗi, các bản sửa lỗi và các đánh giá mã tương ứng. Cho
câu hỏi nghiên cứu thứ hai, chúng tôi điều tra cách hai LLM (GPT-3.5-Turbo [32] và Code-DaVinci-Edit-001 [33]) hoạt động trên các tập
dữ liệu này bằng cách sử dụng prompt zero-shot và few-shot. Trong
câu hỏi nghiên cứu cuối cùng, chúng tôi điều tra thủ công đầu ra từ
các mô hình được điều chỉnh tinh và các LLM được prompt để xem
chương trình được sửa chữa có phù hợp với đánh giá mã hay không.

Các đóng góp của công việc chúng tôi là:
• Xác thực về sự cải thiện đáng kể của việc sửa chữa mã bằng cách
sử dụng các mô hình ngôn ngữ lớn được tiền huấn luyện với NL và
PL từ tập dữ liệu Tufano et al. [15] và tập dữ liệu Review4Repair
• Thảo luận về cách kiến trúc và trọng số được tiền huấn luyện đóng
góp vào việc tăng hiệu suất sửa chữa mã.
• So sánh hiệu suất của hai mô hình được tiền huấn luyện, PLBART
và CodeT5, về mặt độ chính xác.
• Một điều tra toàn diện về hai LLM (GPT-3.5-Turbo, và Code-DaVinci-Edit-001) cho việc sửa chữa mã zero-shot và few-shot với
sự giúp đỡ của kỹ thuật prompt.
• Phân tích thủ công các mã được sửa chữa để hiểu khả năng thực
tế của các mô hình học.
• Một gói tái tạo với tất cả các script được sử dụng để thu thập dữ
liệu và kết quả¹

¹https://doi.org/10.5281/zenodo.8122636

II. NỀN TẢNG
Phần này giải thích các khái niệm có liên quan để hiểu bài báo này.

A. Đánh giá Mã và Sửa chữa Chương trình Tự động
Đánh giá mã [1] là một hoạt động đảm bảo chất lượng phần mềm
trong đó một hoặc nhiều nhà phát triển phân tích mã nguồn của
một nhà phát triển đồng nghiệp bằng cách xem hoặc đọc các phần
mã sau khi triển khai một tính năng hoặc sửa một khiếm khuyết.
Trong hoạt động này, một người đánh giá có thể xác định lỗi trong
mã. Ví dụ, Listing 1 có một mã nguồn đang được đánh giá. Ví dụ
này được lấy từ một tập dữ liệu từ một nghiên cứu trước [15], bao
gồm các thẻ <START> và <END> để chỉ ra nơi một người đánh giá
đã đưa ra nhận xét để sửa một lỗi. Người đánh giá nói rằng điều
kiện if ở dòng 2 "có thể được đơn giản hóa". Vì vậy, nhà phát triển
sửa mã như được hiển thị trong đoạn mã thứ hai trong Listing 1.

Mã trong quá trình đánh giá
1public boolean accept (Issue issue ) {
2 <START> if ( issueShouldNotBeReported (issue ,excludedLinesByRule ())) { <END>
3 return false;
4 }
5 return true;
6}

Mã được sửa dựa trên đánh giá
1public boolean accept (Issue issue ) {
2 return ! issueShouldNotBeReported (issue ,excludedLinesByRule ());
3}

Listing 1: Ví dụ về một đoạn mã có lỗi để đánh giá. "có thể được đơn giản hóa"

Trong khi đánh giá mã dựa vào chuyên môn của con người để xác
định và sửa chữa các vấn đề, các kỹ thuật sửa chữa chương trình
tự động (APR) [34] nhằm mục đích tự động sửa các lỗi phần mềm
mà không cần can thiệp của nhà phát triển [35], [36]. APR cũng
được gọi là tạo bản vá tự động, sửa lỗi tự động, và sửa chữa mã
tự động. Từ nay về sau, chúng tôi sẽ sử dụng các thuật ngữ sửa
chữa mã tự động và sửa chữa chương trình tự động thay thế cho nhau.

Bằng cách kết hợp cả đánh giá mã và các kỹ thuật APR, các nhà
phát triển có thể tận dụng các điểm mạnh của mỗi cái để nâng cao
chất lượng tổng thể của mã. Trong công việc này, chúng tôi tập
trung vào việc nghiên cứu cách các mô hình ngôn ngữ có thể tự
động hóa việc sửa chữa các lỗi đã được xác định trong quá trình
đánh giá mã.

B. LLM, Prompt Zero Shot và Few Shot
Một Mô hình Ngôn ngữ Lớn (LLM) [22] đề cập đến một mô hình
trí tuệ nhân tạo tinh vi bao gồm một mạng nơ-ron với hàng chục
triệu đến hàng tỷ tham số. LLM được huấn luyện trên lượng lớn
văn bản không được gán nhãn bằng cách sử dụng học tự giám sát
hoặc học bán giám sát [37]. Trái ngược với việc được huấn luyện
cho một nhiệm vụ duy nhất (như phân tích cảm xúc hoặc lý luận
toán học), LLM là các mô hình đa mục đích xuất sắc trong nhiều
nhiệm vụ xử lý ngôn ngữ tự nhiên, bao gồm dịch ngôn ngữ, tạo
văn bản, hỏi-đáp, tóm tắt, và nhiều hơn nữa. GPT-3 [37], BERT
[38], T5 [39], CodeBERT [21] là những ví dụ về các LLM nổi tiếng.

--- TRANG 2 ---
Để hướng dẫn việc tạo ra câu trả lời của một mô hình, người ta phải
cẩn thận tạo ra các hướng dẫn đầu vào. Hành động xây dựng và
cải thiện các prompt để tạo ra đầu ra mong muốn được gọi là kỹ
thuật prompt [40]. Khi thiết kế một prompt, người ta có thể bao
gồm một vài cặp ví dụ đầu vào-đầu ra (prompt few-shot) hoặc chỉ
đơn giản là có một mô tả cấp cao về nhiệm vụ mong muốn (prompt
zero-shot). Một mô hình có thể hoàn thành thành công một nhiệm
vụ nhờ vào học zero-shot và few-shot, đây là các kỹ thuật học giải
quyết thách thức huấn luyện các mô hình với dữ liệu huấn luyện
hạn chế.

Học zero-shot là khả năng của một mô hình học máy thực hiện
một nhiệm vụ mà không có bất kỳ ví dụ rõ ràng hoặc dữ liệu được
gán nhãn cho nhiệm vụ cụ thể đó trong quá trình huấn luyện [28],
[41]. Các mô hình tạo mã truyền thống thường được huấn luyện
bằng các kho mã quy mô lớn cụ thể cho ngôn ngữ lập trình đích.
Trong quá trình này, các mô hình học cú pháp và ngữ nghĩa cụ thể
của ngôn ngữ lập trình cụ thể đó. Với sự giúp đỡ của học zero-shot,
các mô hình có thể được thiết kế để tổng quát hóa sự hiểu biết
của chúng qua nhiều ngôn ngữ lập trình khác nhau. Nó sử dụng
các khái niệm và mẫu chung giữa nhiều ngôn ngữ lập trình cho
một ngôn ngữ đích mà nó chưa thấy trong quá trình huấn luyện.

Học few-shot [29] là một phương pháp trong đó mô hình được
huấn luyện với một tập dữ liệu hạn chế. Không giống như thực
hành phổ biến của các mô hình ML, nơi các mô hình được cung
cấp càng nhiều dữ liệu càng tốt, học few-shot nhằm mục đích tạo
ra dự đoán của mô hình với ít dữ liệu huấn luyện hơn. Học few-shot
cho phép mô hình tổng quát hóa và đưa ra dự đoán chính xác trên
các lớp mới chỉ với một vài ví dụ có sẵn cho mỗi lớp.

Để làm rõ sự khác biệt giữa prompt zero-shot và few-shot, hãy
xem xét prompt trong Listing 2. Một mặt, các dòng 1–24 là một
trường hợp của prompt few-shot; nó bao gồm ba ví dụ về một mã
có lỗi, đánh giá tương ứng, và bản sửa cũng như một hướng dẫn
rõ ràng cho mô hình biết cách tái cấu trúc (sửa) mã dựa trên đánh
giá được cung cấp (dòng 24). Mặt khác, nếu prompt chỉ có các
dòng 13–24 (được làm nổi bật) thì đó là một ví dụ về prompt
zero-shot.

Prompt_example.java
1Mã có lỗi: <Mã có lỗi 1>
2Đánh giá: chỉ cần trả về điều này
3Mã đã sửa: <Mã đã sửa 1>
4
5Mã có lỗi: <Mã có lỗi 2>
6Đánh giá: Chỉ cần trả về rule.
7Mã đã sửa: <Mã đã sửa 2>
8
9Mã có lỗi: <Mã có lỗi 3>
10Đánh giá: Chúng ta không thể chỉ dựa vào @Rule?
11Mã đã sửa: <Mã đã sửa 2>
12
13Mã có lỗi:
14private FirewallRule findById (List <FirewallRule >collection ,String id ) {
15FirewallRule result = null;
16 for (FirewallRule rule :collection ) {
17 if (rule .id().equals (id)) {
18 <START> result =rule ;<END>
19 }
20 }
21 returnresult ;
22}
23Đánh giá: Chỉ cần trả về rule.
24Tái cấu trúc Mã có lỗi bằng cách sử dụng Đánh giá mà không có nhận xét.

Listing 2: Ví dụ prompt zero-shot và few shots.

III. PHƯƠNG PHÁP LUẬN
Hình 1 cung cấp một cái nhìn tổng quan về nghiên cứu của chúng tôi. Để trả lời RQ1, chúng tôi thu thập mã có lỗi và các đánh giá mã của chúng từ hai tập dữ liệu (Tufano et al. [15] và Review4Repair [14]) để điều chỉnh tinh hai mô hình được tiền huấn luyện (PLBART và CodeT5). Cho RQ2, chúng tôi sử dụng cùng các tập dữ liệu trong RQ1 và kỹ thuật prompt với hai LLM (GPT-3.5-Turbo và Code-DaVinci-Edit-001). Cuối cùng, hai nhà phát triển đã tiến hành phân tích thủ công đầu ra của các mô hình để kiểm tra sự phù hợp trong việc giải quyết đánh giá mã trong chương trình được sửa chữa (RQ3). Các tiểu mục tiếp theo giải thích chi tiết từng bước này.

A. RQ1: Điều chỉnh Tinh các Mô hình Được Tiền huấn luyện cho APR
1) Thu thập và Tiền xử lý Tập dữ liệu: Chúng tôi sử dụng hai tập dữ liệu để sửa chữa mã bằng cách sử dụng đánh giá mã. Một cái nhìn tổng quan về mỗi tập dữ liệu được đưa ra trong Bảng I. Cả hai tập dữ liệu đều từ các công trình trước gần đây [14], [15] và bao gồm các ví dụ thực tế về đánh giá mã được thu thập từ Gerrit và GitHub. Chúng tôi tiền xử lý mỗi tập dữ liệu như sau:

• Tập dữ liệu Tufano et al. [15]: Nó chứa 17.194 mẫu mã có lỗi, các bản sửa tương ứng và đánh giá mã được thu thập từ Gerrit và GitHub. Ngoài ra, mỗi mã có lỗi có hai token đặc biệt (<START> và <END>) để bao bọc khối mã lỗi. Tương tự như một nghiên cứu khác [14], trong quá trình tiền xử lý tập dữ liệu, chúng tôi nối mã có lỗi và đánh giá mã tương ứng thành một dòng duy nhất, với đánh giá mã được bao bọc bằng các thẻ <|startcomment|> và <|endcomment|>. Những đoạn nối này là đầu vào của mô hình, và các mã được sửa tương ứng là mục tiêu cho các mô hình PLBART và CodeT5. Chúng tôi cũng phân loại toàn bộ tập dữ liệu thành ba loại sửa chữa: Insert, Delete, và Update. Các loại này chỉ ra liệu các bản sửa chỉ thêm thay đổi mới (insert), loại bỏ các khối mã (delete), hoặc cả hai (update).

• Tập dữ liệu Review4Repair [14]: Nó chứa tổng cộng 56.211² và 2.961 mẫu được sử dụng cho huấn luyện và kiểm tra trong nghiên cứu của họ, tương ứng. Những mẫu này được thu thập từ Gerrit. Vì độ dài đầu vào tối đa không quá 512 token cho cả hai mô hình được tiền huấn luyện, chúng tôi phải loại bỏ 57 mẫu từ tập dữ liệu huấn luyện và 6 mẫu từ tập dữ liệu kiểm tra vì những mẫu này có hơn 512 token. Do đó, tập dữ liệu huấn luyện ban đầu chứa 56.154 mẫu, và tập dữ liệu kiểm tra chứa 2.955 mẫu. Vì việc điều chỉnh tinh các mô hình được tiền huấn luyện cũng yêu cầu một tập dữ liệu xác thực, mà không có trong tập dữ liệu này, chúng tôi đã tổ chức lại tập dữ liệu huấn luyện ban đầu để đảm bảo rằng 90% mẫu nằm trong tập dữ liệu huấn luyện, 5% mẫu nằm trong tập dữ liệu kiểm tra, và 5% mẫu nằm trong tập dữ liệu xác thực. Vì vậy, chúng tôi có 53.198 mẫu trong tập dữ liệu huấn luyện, 2.956 mẫu trong tập dữ liệu xác thực, và 2.955 mẫu trong tập dữ liệu kiểm tra trong tập dữ liệu đã sửa đổi của chúng tôi. Chúng tôi cũng phân loại các mẫu thành ba loại (Insert, Update, và Delete).

²Bài báo đã đề cập 55.060 mẫu huấn luyện [14], nhưng gói tái tạo chứa 56.211 mẫu.

--- TRANG 3 ---
Để phù hợp với mỗi mẫu trên một dòng duy nhất, các khoảng trống và dòng mới bổ sung đã được loại bỏ khỏi mã và nhận xét của mỗi mẫu trong cả tập dữ liệu huấn luyện và tập dữ liệu kiểm tra. Tương tự như tập dữ liệu Tufano et al. [15], mã có lỗi có hai token đặc biệt, <|startfocus|> và <|endfocus|>, để bao bọc khối mã lỗi. Trái ngược với toàn bộ mã được sửa cho mục tiêu trong tập dữ liệu Tufano et al. [15], mục tiêu cho mỗi mã có lỗi chỉ là đoạn mã sửa chữa giữa các token đặc biệt <|startfocus|> và <|endfocus|>. Mục tiêu cho các mẫu lớp delete là một khoảng trống rỗng, mà chúng tôi đã thay thế bằng một token đặc biệt, <|del|>.

BẢNG I: Tổng quan về các Tập dữ liệu.
Tập dữ liệu         Loại      Insert  Delete  Update  Tổng
Review4Repair [14]  Huấn luyện  8,718   4,060   40,420  53,198
                   Xác thực     247     481    2,228   2,956
                   Kiểm tra     222     425    2,308   2,955
Tufano et al. [15] Huấn luyện   161    4,385   9,210   13,756
                   Xác thực      20     540    1,159   1,719
                   Kiểm tra      18     559    1142    1,719

2) Thiết lập Thí nghiệm để Điều chỉnh Tinh các Mô hình: Sau khi tiền xử lý tập dữ liệu, chúng tôi điều chỉnh tinh cả mô hình PLBART [18] và CodeT5 [19] để tự động sửa chữa mã có lỗi trong một đánh giá mã. Đối với mô hình PLBART, chúng tôi đặt độ dài đầu vào là 512 và độ dài mục tiêu là 200 cho cả hai tập dữ liệu. Các thiết lập cho tất cả các siêu tham số khác đều giống hệt với cấu hình PLBART tiêu chuẩn [18]. Hơn nữa, chúng tôi sử dụng ba kích thước beam (1, 5, và 10) để tạo ra các dự đoán Top-1, Top-5, và Top-10, tương ứng, vì chúng tôi thử nghiệm với nhiều số epoch khác nhau để xem kết quả tối ưu. Đối với tập dữ liệu Review4Repair [14], chúng tôi sử dụng 11 epoch vì chúng tôi thấy rằng hiệu suất của mô hình không thay đổi sau 11 epoch. Epoch được đặt là 12 cho tập dữ liệu Tufano et al. [15] cũng như vậy vì hiệu suất của mô hình không tăng sau epoch 12. Chúng tôi đặt siêu tham số, giá trị patience là 10 epoch để quan sát điều này. Chúng tôi chạy những thí nghiệm này trong môi trường địa phương bằng GPU NVIDIA GeForce RTX 2070-8GB.

Kích thước batch trong mô hình CodeT5 được đặt là 4, và các bước gradient tích lũy được đặt là 8. Hơn nữa, kích thước batch mặc định là 32 được đảm bảo bởi sự kết hợp của kích thước batch và các bước gradient tích lũy. Hơn nữa, mô hình được điều chỉnh tinh trong 45 epoch cho cả hai tập dữ liệu dựa trên việc quan sát các tổn thất xác thực. Chúng tôi thay đổi siêu tham số, số chuỗi trả về thành 1, 5, và 10 để tạo ra các dự đoán Top-1, Top-5, và Top-10, tương ứng.

Sau khi token hóa tập dữ liệu Tufano et al. [15], chúng tôi quan sát thấy rằng độ dài tối đa của các chuỗi nguồn là 590 token, và độ dài tối đa của các chuỗi mục tiêu là 194 token. Vì độ dài chuỗi đầu vào tối đa cho mô hình CodeT5 là 512, chúng tôi đặt độ dài đầu vào mô hình là 512 và đặt độ dài đầu ra mô hình là 200.

Tương tự, sau khi token hóa tập dữ liệu Review4Repair [14], chúng tôi quan sát thấy rằng độ dài tối đa của các chuỗi nguồn là 561 token, và độ dài tối đa của các chuỗi mục tiêu là 116 token. Như đã nói trước đó, chúng tôi cũng đặt độ dài đầu vào mô hình là 512 ở đây, và vì độ dài mục tiêu tối đa của các chuỗi là 116, chúng tôi đặt độ dài đầu ra mô hình là 200.

B. RQ2: Kỹ thuật Prompt cho APR
Trong phần này, chúng tôi mô tả cách chúng tôi áp dụng kỹ thuật prompt cho cả hai tập dữ liệu. Tiếp theo, chúng tôi mô tả cách chúng tôi thực hiện kỹ thuật prompt zero shot [28] và few shot [29] với GPT-3.5-Turbo và prompt zero-shot với Code-DaVinci-Edit-001. Chúng tôi cũng chi tiết về các heuristic được sử dụng để sửa đổi phản hồi nhằm sửa các lỗi phổ biến trong phản hồi từ các mô hình.

1) Các Mô hình: Chúng tôi sử dụng hai mô hình có sẵn qua OpenAI API cho kỹ thuật prompt zero-shot. Một mặt, GPT-3.5-Turbo là mô hình hiệu quả và tiết kiệm nhất trong họ GPT-3.5 [37]. Mặc dù GPT-3.5-Turbo được tối ưu hóa cho chat, nó cũng hoạt động tốt cho các nhiệm vụ hoàn thành mã. Mặt khác, Code-DaVinci-Edit-001 [33] là một biến thể khác của mô hình Codex [42] GPT-3 với khả năng chỉnh sửa được thiết kế đặc biệt để hỗ trợ nhiều nhiệm vụ liên quan đến lập trình bằng cách đưa ra hướng dẫn, bao gồm sửa lỗi mã, hoàn thành đoạn mã, đề xuất chỉnh sửa trong một đoạn mã, v.v. Được đưa ra một mã và một hướng dẫn bằng ngôn ngữ tự nhiên, mô hình chỉnh sửa mã để tuân thủ hướng dẫn càng gần càng tốt.

Đối với prompt few-shot, chúng tôi chỉ sử dụng mô hình GPT-3.5-Turbo. Vì prompt few-shot cung cấp cho mô hình thông tin về cấu trúc đầu vào-đầu ra, nó không có mối quan hệ với các nhiệm vụ phía sau được nhắm mục tiêu. Vì GPT-3.5-Turbo là một mô hình tổng quát cho các nhánh nhiệm vụ, việc cung cấp cấu trúc đầu vào-đầu ra với prompt few-shot có thể hữu ích [43]. Tuy nhiên, đối với Code-DaVinci-Edit-001, đã có một cấu trúc cố định tức là mã đầu vào, hướng dẫn, và mã đầu ra. Do đó, không cần ví dụ để làm cho mô hình hiểu cấu trúc IO.

2) Tạo Prompt Zero-shot: Các prompt được tạo ra đúng cách và được chế tác tốt là rất quan trọng để có được phản hồi mong muốn từ các mô hình tạo sinh như GPT-3.5-Turbo. Để tạo prompt zero-shot, chúng tôi sử dụng mã có lỗi và đánh giá từ các tập dữ liệu tương ứng. Chúng tôi đã đề cập rõ ràng từng phần bằng cách xác định "Mã có lỗi" và "Đánh giá" trong prompt để đảm bảo mô hình có thể phân biệt giữa mã có lỗi và đánh giá liên quan. Sau đó chúng tôi thêm một lệnh rõ ràng để sửa mã có lỗi "Tái cấu trúc Mã có lỗi bằng cách sử dụng Đánh giá mà không có nhận xét". Mệnh đề "mà không có nhận xét" được thêm vào prompt để đảm bảo phản hồi từ mô hình không chứa bất kỳ nhận xét dư thừa hoặc giải thích nào không có trong mã có lỗi đầu vào, do đó hướng dẫn thêm mô hình để tạo ra kết quả mong muốn. Listing 2 hiển thị bố cục của prompt cho tình huống này, với các dòng 13 đến 22 chỉ định mã có lỗi, dòng 23 chỉ định đánh giá liên quan đến mã có lỗi, và dòng 24 chỉ định lệnh rõ ràng cho mô hình tạo ra mã được sửa tương ứng.

Đối với mô hình Code-DaVinci-Edit-001, chúng tôi cần chuyển mã nguồn có lỗi làm đầu vào của mô hình, và cho tham số hướng dẫn, chúng tôi chuyển một hướng dẫn ngôn ngữ tự nhiên tức là "Tái cấu trúc mã bằng cách sử dụng Đánh giá: <đánh giá mã cụ thể>."

3) Tạo prompt Few-shot: Để tạo prompt few-shot, chúng tôi cần thực hiện một số nhiệm vụ bổ sung. Đầu tiên, chúng tôi vector hóa cả đánh giá tập dữ liệu huấn luyện và kiểm tra bằng TF-IDF [44]. Sau đó chúng tôi tính toán điểm tương tự cosine [45] cho mỗi mẫu kiểm tra đối với mọi mẫu huấn luyện. Tiếp theo, chúng tôi chọn ba đánh giá được xếp hạng cao nhất từ tập dữ liệu huấn luyện và mã có lỗi tương ứng, mã được sửa, để tạo prompt cho mỗi mẫu kiểm tra cho quy trình few-shot. Chúng tôi nhằm mục đích cung cấp cho mô hình ba ví dụ liên quan nhất chứa Mã có lỗi, Đánh giá, và Mã được sửa để nó có thể có một số kiến thức nền trong khi dự đoán mã được sửa cho mỗi mẫu kiểm tra. Phần sau của prompt giống như prompt zero-shot. Cấu trúc của prompt few-shot này được đưa ra trong Listing 2.

4) Tạo Mã được Sửa chữa: Cả mô hình GPT-3.5-Turbo và Code-DaVinci-Edit-001 đều có sẵn qua OpenAI API. Bằng cách tuân theo các nghiên cứu gần đây [24], [46]–[48] cho cả hai mô hình, chúng tôi đặt tham số nhiệt độ là zero vì nhiệt độ thấp hơn làm cho đầu ra tập trung và xác định hơn. Ngược lại, nhiệt độ cao hơn làm cho đầu ra ngẫu nhiên hơn. Các tham số khác như topp, frequency penalty, presence penalty được đặt ở thiết lập mặc định, và chúng lần lượt là 1, 0, 0.

Mô hình GPT-3.5-Turbo có ba vai trò riêng biệt: assistant, system, và user. Tuân theo các hướng dẫn được nêu trong các công trình trước [31], chúng tôi đặt nội dung cho vai trò system là "Bạn là một trợ lý lập trình. Bạn chỉ tạo ra mã nguồn." Nội dung cho vai trò system giúp mô hình định hình tính cách của trợ lý hoặc cách nó nên hoạt động để tạo ra đầu ra. Prompt được đề cập trong phần trước được đặt là nội dung cho vai trò user. Cuối cùng, vai trò assistant cung cấp mã được sửa làm phản hồi. Đối với Code-DaVinci-Edit-001, chúng tôi có mã được sửa chữa trong đầu ra trực tiếp.

5) Phân tích Dựa trên Heuristic của các Bản Sửa chữa được Tạo ra: Chúng tôi tạo ra tất cả các mã được sửa với mã có lỗi tương ứng cùng với đánh giá với một prompt người dùng cố định cho tất cả các tập dữ liệu. Chúng tôi quan sát thấy đối với mô hình GPT-3.5-Turbo, ban đầu, độ chính xác của việc tạo ra mã được sửa là thấp. Tuy nhiên, mã được dự đoán có phần tương tự với mã mục tiêu. Chúng tôi cũng quan sát thấy rằng LLM (i) tạo ra các bản sửa chữa có vấn đề cú pháp tầm thường; (ii) thêm giải thích về mã ở cuối; (iii) tạo ra mã có lỗi và mã được sửa cùng nhau; (iv) thêm một tiền tố java ở đầu mã; (v) thêm tiêu đề trước khi tạo ra mã được sửa như "Mã được Tái cấu trúc", "Mã được Sửa", v.v. (vi) thêm khoảng trống bổ sung không cần thiết; (vii) bao bọc mã được sửa trong dấu backtick ```. Tuy nhiên, chúng tôi có thể dễ dàng trích xuất mã được sửa từ phản hồi thông qua heuristic. Do đó, tương tự như một nghiên cứu gần đây [31], chúng tôi đã phát triển năm heuristic để tự động sửa các vấn đề nêu trên:

H1 Điều chỉnh khoảng trống: Tuân theo cấu trúc của mã mục tiêu của tập dữ liệu Tufano et al. [15], chúng tôi cần sửa đổi phản hồi của LLM bằng cách loại bỏ các dòng mới và loại bỏ các khoảng trống bổ sung

H2 Loại bỏ giải thích mã: GPT-3.5-Turbo đôi khi giải thích toàn bộ mã sau khi tạo ra mã được sửa bằng cách sử dụng một số từ khóa như Giải thích, Lý luận, và Thay đổi được Thực hiện. Do đó, heuristic loại bỏ tự động giải thích mã ở cuối cùng với các từ khóa.

H3 Loại bỏ bắt đầu bằng java: GPT-3.5-Turbo thường đề cập đến ngôn ngữ của mã trong phản hồi của nó, Vì các tập dữ liệu của chúng tôi chỉ có mã java, chúng tôi áp dụng một heuristic để loại bỏ phần đầu của phản hồi bắt đầu bằng java.

H4 Loại bỏ từ khóa dư thừa: Nó loại bỏ các từ khóa như Mã được tái cấu trúc, Mã được sửa, Mã được cập nhật, v.v. ở đầu phản hồi. Ngoài ra, vì chúng tôi có <START> và <END> trong mã có lỗi để chỉ định khối mã cần sửa, GPT-3.5-Turbo đôi khi cũng dự đoán nó trong phản hồi, điều này đã được loại bỏ vì chúng dư thừa.

--- TRANG 4 ---
Người đánh giá
Commit
mã có lỗi
Nhà phát triển
Mã có lỗi
Đánh giá mã
Viết đánh giá dựa trên mã
Tập dữ liệu
Tiền xử lý
Kỹ thuật Prompt
LLM
GPT-3.5-Turbo
Code-DaVinci-Edit-001
Áp dụng Heuristic
Phân tích Nhà phát triển
Sửa mã dựa trên đánh giá
Mã được sửa
Mã được sửa
RQ1
Mô hình được Điều chỉnh Tinh (PLBART, CodeT5)
Mã được sửa
RQ2
RQ3

Hình 1: Tổng quan về Phương pháp luận.

H5 Loại bỏ dấu backtick: GPT-3.5-Turbo thường phản hồi đoạn mã theo định dạng markdown nơi đoạn mã được bao bọc bằng dấu backtick (```). Do đó, chúng tôi áp dụng heuristic để loại bỏ các dấu backtick như vậy khỏi phản hồi.

Chúng tôi áp dụng các heuristic nêu trên để điều chỉnh phản hồi theo hành vi mong muốn càng nhiều càng tốt. Tuy nhiên, ngay cả sau khi áp dụng các heuristic nêu trên, vẫn còn một số khác biệt trong các phản hồi, cần kiểm tra cẩn thận của con người để được loại bỏ. Ví dụ, mô hình giải thích mã được sửa mà không có từ khóa nào đứng trước trong một tình huống. Do đó, chúng tôi cần xóa dòng khỏi phản hồi một cách thủ công. Hoặc đôi khi, mô hình sử dụng các mẫu văn bản độc đáo không nhất quán như Đây là mã được cập nhật hoặc Mã được cập nhật ở bên dưới. Do đó, chúng tôi kiểm tra thủ công phản hồi của mỗi mô hình sao cho những mẫu lỗi này được loại bỏ thủ công.

C. RQ3: Phân tích Nhà phát triển về các Bản Sửa chữa được Tạo ra
Trong các phần trước, chúng tôi mô tả cách chúng tôi điều chỉnh tinh PLBART và CodeT5 cũng như prompt hai LLM để có được mã được sửa chữa bằng cách xem xét đánh giá mã và đánh giá dựa trên sự thật cơ bản. Tuy nhiên, sự thật cơ bản này có thể không phải là giải pháp duy nhất có thể, hoặc mã được sửa chữa có thể không nắm bắt đầy đủ ý định trong đánh giá. Vì lý do này, chúng tôi ngẫu nhiên thu thập 314 mẫu kiểm tra từ Tufano et al. [15] và 340 mẫu kiểm tra từ các tập dữ liệu Review4Repair [14] để đạt được khoảng tin cậy 95% và lề lỗi 5%. Chúng tôi xem xét năm mô hình: giải pháp top-1 cho PLBART và CodeT5, prompt zero-shot và few-shot cho GPT-3.5-Turbo sau khi áp dụng heuristic, và Code-DaVinci-Edit-001. Chúng tôi yêu cầu hai nhà phát triển phần mềm chấm điểm mã được sửa chữa được tạo ra từ năm mô hình dựa trên việc thực hiện ý định của đánh giá mã. Họ có một năm kinh nghiệm trong ngành tại một công ty Fortune 500 và tham gia đáng kể vào quá trình đánh giá mã trong phát triển phần mềm (tức là, với tư cách là nhà phát triển, họ gửi mã của họ để đánh giá mã, và họ đánh giá mã của các nhà phát triển khác). Họ riêng lẻ cho điểm zero nếu mã được sửa chữa được tạo ra không thực hiện đánh giá và cho điểm một nếu mã được sửa chữa hoàn toàn phù hợp với ý định của đánh giá. Sau đó chúng tôi tính toán điểm Cohen's Kappa cho sự đồng ý giữa các người đánh giá [49] và trình bày kết quả dựa trên điểm được đưa ra.

D. Các Chỉ số Đánh giá
Để đánh giá hiệu suất của một mô hình cho tổng hợp mã, có nhiều chỉ số đánh giá khác nhau như BLEU (Bilingual Evaluation Understudy) [50], và CodeBLEU [51], Exact Match (EM), v.v. Điểm BLEU biểu thị chất lượng của một đầu ra được dịch máy. Điểm CodeBLEU sử dụng khớp n-gram từ điểm BLEU và xem xét thêm các tính năng cú pháp và ngữ nghĩa quan trọng của mã. Một khớp chính xác (EM) biểu thị một khớp chuỗi sang chuỗi hoàn chỉnh giữa dự đoán mô hình và đoạn mã mục tiêu.

Vì khớp n-gram từ điểm BLEU nhấn mạnh sự tương tự giữa mục tiêu và các dự đoán được tạo ra từ các mô hình, một bản sao ngây thơ có thể đạt được điểm BLEU và CodeBLEU cao hơn với zero khớp chính xác. Tuy nhiên, trong một nhiệm vụ tinh chỉnh mã, việc có được bản sửa chữa chính xác là vô cùng quan trọng, vì chỉ có bản sửa chữa đúng mới có thể đảm bảo việc biên dịch thành công của mã. Vì vậy, chúng tôi coi khớp chính xác giữa đầu ra được dự đoán từ mô hình của chúng tôi và đoạn mã mục tiêu là chỉ số đánh giá chính. Chúng tôi tạo ra thêm nhiều dự đoán bằng cách sử dụng các kích thước beam khác nhau và đánh giá các dự đoán so với các mô hình cơ sở.

Chúng tôi đo Top-1 Accuracy là tỷ lệ phần trăm của các bản sửa khi dự đoán hàng đầu của mô hình khớp chính xác với đoạn mã mục tiêu. Tương tự, đối với Top-5 hoặc Top-10 Accuracy, chúng tôi đo tỷ lệ phần trăm của các bản sửa khi bất kỳ dự đoán đầu tiên nào trong 5 hoặc 10 dự đoán mô hình khớp chính xác với đoạn mã mục tiêu.

Chúng tôi sử dụng tất cả các chỉ số nêu trên để điều chỉnh tinh các mô hình PLBART và CodeT5. Chúng tôi sử dụng ba trong số chúng cho prompt zero-shot và few-shot: BLEU, CodeBLEU, và Top-1 Accuracy.

IV. KẾT QUẢ
Trong phần này, chúng tôi trả lời các câu hỏi nghiên cứu của chúng tôi.

A. RQ1: Các mô hình được tiền huấn luyện hoạt động như thế nào trong việc sửa chữa các lỗi được xác định trong quá trình đánh giá mã?

Từ Bảng II, chúng ta có thể thấy rằng cả hai mô hình được điều chỉnh tinh đều vượt trội hơn mỗi mô hình cơ sở trước đó với một lề đáng kể. Cả hai mô hình cơ sở [14], [15] đều được huấn luyện với cả mã có lỗi và đánh giá mã tương ứng.

Trên tập dữ liệu Review4Repair, mô hình PLBART được điều chỉnh tinh đạt được cải thiện 9.91%, và mô hình CodeT5 được điều chỉnh tinh đạt được cải thiện 8.45% về Top-10 Accuracy so với mô hình cơ sở R4R CC, đây là mô hình cơ sở được đặt tên là mô hình cc trong bài báo Review4Repair [14]. Về hiệu suất tương đối, mô hình PLBART được điều chỉnh tinh đạt được độ chính xác cao hơn 5.69%, 9.56%, và 9.91% trong các dự đoán Top-1, Top-5, và Top-10, và mặt khác, mô hình CodeT5 được điều chỉnh tinh đạt được độ chính xác cao hơn 10.23%, 10.00%, và 8.45% trong các dự đoán Top-1, Top-5, và Top-10, tương ứng, so với mô hình cơ sở.

Trên tập dữ liệu Tufano et al. [15], mô hình PLBART được điều chỉnh tinh đạt được cải thiện 20.41%, và mô hình CodeT5 được điều chỉnh tinh đạt được cải thiện 24.72% về Top-10 Accuracy so với mô hình cơ sở được đặt tên là Tufano 2-encoder là mô hình cơ sở từ bài báo của Tufano et al. [15]. Mô hình CodeT5 được điều chỉnh tinh là mô hình hoạt động tốt nhất; việc tăng độ chính xác dao động từ 21.12% đến 25.65%.

Chúng tôi cũng báo cáo điểm BLEU-4 và CodeBLEU cho mỗi mô hình được điều chỉnh tinh trong Bảng II. Chúng ta có thể thấy rằng cả hai mô hình được điều chỉnh tinh đều cải thiện điểm BLEU-4 và CodeBLEU so với các mô hình cơ sở. Điều này cũng gợi ý rằng cả hai mô hình được điều chỉnh tinh có thể tạo ra mã với luồng cú pháp tốt hơn so với các mô hình trước đó.

Để đánh giá điểm mạnh và hạn chế của mỗi mô hình trong việc dự đoán bản sửa chữa đúng trong tất cả ba loại sửa chữa (tức là, insert, delete và update), chúng tôi so sánh các dự đoán Top-1, Top-5, và Top-10 được tạo ra bởi các mô hình được điều chỉnh tinh trên cả hai tập dữ liệu cho tất cả ba lớp. Hiệu suất của chúng được hiển thị trong Hình 2. Từ Hình 2(a) và 2(d), chúng ta có thể thấy rằng mô hình CodeT5 được điều chỉnh tinh đạt được độ chính xác tốt hơn trong tất cả các dự đoán cho lớp Insert so với cả mô hình cơ sở và mô hình PLBART được điều chỉnh tinh cho cả hai tập dữ liệu. Điều này chứng minh hiệu quả của mô hình CodeT5 trong việc chèn các dòng mã bổ sung bằng cách tuân theo các nhận xét đánh giá mã khi so sánh với các mô hình khác.

Từ Hình 2(c) và 2(f), trên tập dữ liệu Review4Repair, mô hình cơ sở hoạt động tốt trong lớp Delete nhưng kém trong các lớp khác. Trong số hai mô hình được điều chỉnh tinh, mô hình PLBART được điều chỉnh tinh hoạt động tốt hơn mô hình CodeT5 được điều chỉnh tinh trong lớp Delete cho cả hai tập dữ liệu. Điều này chỉ ra rằng mô hình PLBART có thể hoạt động tốt hơn trong việc loại bỏ các dòng có lỗi khỏi mã so với mô hình CodeT5.

Từ Hình 2(b) và 2(e), chúng ta có thể thấy rằng trong lớp Update cho cả hai tập dữ liệu, mô hình CodeT5 được điều chỉnh tinh vượt trội hơn cả mô hình cơ sở và mô hình PLBART được điều chỉnh tinh, tương tự như hiệu suất trong Lớp Insert. Lớp Update yêu cầu cả việc chèn và xóa các đoạn mã cụ thể cho một bản sửa chữa đúng. Ngoài ra, đối với cả hai tập dữ liệu, các mẫu Update chiếm phần lớn hơn. Do đó, hiệu suất cao hơn của CodeT5 trong các mẫu Update dẫn đến độ chính xác tổng thể cao hơn. Ngoài ra, mặc dù thao tác update là một thao tác phức tạp, việc quan sát mô hình CodeT5 được điều chỉnh tinh vượt trội hơn mô hình PLBART được điều chỉnh tinh trong lớp Update gợi ý rằng mô hình CodeT5 có thể sử dụng đánh giá mã liên quan đến mã có lỗi tốt hơn nhiều so với mô hình PLBART.

Kết quả RQ1: Các mô hình được điều chỉnh tinh có thể hoạt động tốt hơn đáng kể trong việc tạo ra mã được sửa chữa bằng cách sử dụng đánh giá mã. Trong hầu hết các trường hợp, CodeT5 có hiệu suất hơi tốt hơn mô hình PLBART được điều chỉnh tinh. Nó cũng có khả năng hiểu ngôn ngữ tự nhiên và ngôn ngữ lập trình tương đối tốt hơn, và do đó nó có thể đạt được độ chính xác tốt hơn trong việc dự đoán các bản sửa chữa đúng với sự giúp đỡ của đánh giá mã so với mô hình PLBART được điều chỉnh tinh. Có thể thấy rằng việc dự đoán bản sửa chữa đúng cho lớp Insert và lớp Update khó khăn hơn nhiều so với lớp Delete.

--- TRANG 5 ---
BẢNG II: So sánh các mô hình PLBART và CodeT5 được điều chỉnh tinh trên mỗi tập dữ liệu với các mô hình cơ sở tương ứng.

Tập dữ liệu | Tên Mô hình | Top-1 Accuracy (%) | Top-5 Accuracy (%) | Top-10 Accuracy (%) | BLEU-4 (%) | CodeBLEU (%)
Review4Repair [14] | R4R CC | 19.59 cơ sở | 27.73 cơ sở | 31.51 cơ sở | 24.66 cơ sở | 39.30 cơ sở
| PLBART được điều chỉnh tinh | 25.28 +5.69 | 37.29 +9.56 | 41.42 +9.91 | 40.97 +16.31 | 49.60 +10.3
| CodeT5 được điều chỉnh tinh | 29.82 +10.23 | 37.73 +10.0 | 39.96 +8.45 | 45.98 +21.32 | 53.19 +13.89
Tufano et al. [15] | Tufano 2-encoder | 12.16 cơ sở | 24.55 cơ sở | 30.72 cơ sở | 81.80 cơ sở | 80.52 cơ sở
| PLBART được điều chỉnh tinh | 32.98 +20.82 | 47.12 +22.57 | 51.13 +20.41 | 87.55 +5.75 | 85.46 +4.94
| CodeT5 được điều chỉnh tinh | 33.28 +21.12 | 50.20 +25.65 | 55.44 +24.72 | 86.96 +4.84 | 86.80 +6.28

B. RQ2: Việc sửa chữa chương trình tự động bằng cách sử dụng kỹ thuật prompt dựa trên học zero-shot và few-shot trên các Mô hình Ngôn ngữ Lớn hiệu quả như thế nào?

Chúng tôi sử dụng prompt zero-shot với các LLM tạo mã, GPT-3.5-Turbo và Code-DaVinci-Edit-001, và cho few-shot, chúng tôi sử dụng GPT-3.5-Turbo trên cả hai tập dữ liệu. Một cái nhìn tổng quan ngắn gọn về những phát hiện này được trình bày trong Bảng III.

Chúng tôi quan sát thấy trong prompt zero-shot rằng mô hình GPT-3.5-Turbo đạt được 6.9% và 17.86% độ chính xác trên Tập dữ liệu Review4Repair [14] và tập dữ liệu Tufano et al. [15] tương ứng trước khi áp dụng các heuristic được mô tả trong phần Phương pháp luận (Phần III-B5). So sánh hiệu suất này với các mô hình được điều chỉnh tinh được mô tả trong RQ1 và RQ2, nó ít lý tưởng hơn đáng chú ý.

Sau khi sử dụng các heuristic, chúng ta có thể thấy một cải thiện đáng kể về độ chính xác. Chúng tôi quan sát thấy rằng khớp chính xác cải thiện 15.6% (22.06%-6.9%) và 12.27% (30.13%-17.86%) trên Tập dữ liệu Review4Repair [14] và tập dữ liệu Tufano et al. [15] tương ứng. Điều này ngụ ý rằng với các heuristic phù hợp, phản hồi của mô hình có thể ngắn gọn và phù hợp hơn cho mục đích đích. Việc cải thiện điểm BLEU và CodeBLEU khi sử dụng heuristic cũng ngụ ý điều tương tự.

Đối với trường hợp prompt few-shot với GPT-3.5-Turbo, kỹ thuật này có thể cung cấp hiệu suất tốt hơn trong trường hợp độ chính xác và trước khi áp dụng heuristic. Tuy nhiên, sau khi áp dụng heuristic, kỹ thuật này hoạt động tốt hơn cho Tufano et al. [15], nhưng không trên tập dữ liệu Review4Repair [14].

Đối với mô hình instruct, Code-DaVinci-Edit-001, chúng tôi thực hiện prompt zero-shot và nó hoạt động tốt hơn đáng kể trong một số trường hợp. Ví dụ, nó đạt được hiệu suất tối tân về điểm CodeBLEU cho tập dữ liệu Review4Repair [14] và về độ chính xác cho tập dữ liệu Tufano et al. [15].

Kết quả RQ2: Prompt zero-shot và few-shot có thể hữu ích khi điều chỉnh tinh không khả thi. Tuy nhiên mô hình kiểu chat như GPT-3.5-Turbo cần chú ý để loại bỏ phần không cần thiết trong phản hồi, trong khi mô hình instruct như Code-DaVinci-Edit-001 có hiệu suất tốt hơn mà không cần điều chỉnh tinh và heuristic để làm sạch đầu ra.

--- TRANG 6 ---
(a) Lớp Insert cho Tập dữ liệu Review4Repair [14]
(b) Lớp Update cho Tập dữ liệu Review4Repair [14]
(c) Lớp Delete cho Tập dữ liệu Review4Repair [14]
(d) Lớp Insert cho Tập dữ liệu Tufano et al. [15]
(e) Lớp Update cho Tập dữ liệu Tufano et al. [15]
(f) Lớp Delete cho Tập dữ liệu Tufano et al. [15]

Hình 2: So sánh hiệu suất trên tất cả các lớp trên cả hai tập dữ liệu.

BẢNG III: So sánh prompt zero và few shot trên mỗi tập dữ liệu với các cơ sở mới.

Tập dữ liệu | Loại Mô hình | Tên Mô hình | Accuracy (%) | BLEU (%) | CodeBLEU (%)
Review4Repair [14] | Được tiền huấn luyện | PLBART được điều chỉnh tinh | 25.28 −4.54 | 40.97 −35.41 | 49.60 −38.82
| | CodeT5 được điều chỉnh tinh | 29.82 +0.00 | 45.98 −30.40 | 53.19 −35.23
| Kiểu Chat | GPT-3.5-turbo zero-shot không có heuristics | 6.90 −22.92 | 75.42 −0.96 | 74.94 −13.48
| | GPT-3.5-turbo zero-shot có heuristics | 22.06 −7.76 | 76.38 +0.00 | 75.92 −12.50
| | GPT-3.5-turbo few-shot không có heuristics | 9.54 −20.28 | 71.55 −4.83 | 75.23 −13.19
| | GPT-3.5-turbo few-shot có heuristics | 21.18 −8.64 | 71.60 −4.78 | 75.28 −13.14
| Instruct | Code-DaVinci-Edit-001 | 25.05 −4.77 | 75.29 −1.09 | 88.42 +0.00

Tufano et al. [15] | Được tiền huấn luyện | PLBART được điều chỉnh tinh | 32.98 −7.72 | 87.55 +0.00 | 85.46 −3.17
| | CodeT5 được điều chỉnh tinh | 33.28 −7.42 | 86.96 −0.59 | 86.80 −1.83
| Kiểu Chat | GPT-3.5-turbo zero-shot không có heuristics | 17.86 −22.84 | 70.88 −16.67 | 80.96 −7.67
| | GPT-3.5-turbo zero-shot có heuristics | 31.70 −9.00 | 77.95 −9.60 | 83.38 −5.25
| | GPT-3.5-turbo few-shot không có heuristics | 27.69 −13.01 | 67.91 −19.64 | 81.03 −7.60
| | GPT-3.5-turbo few-shot có heuristics | 28.21 −12.49 | 68.23 −19.32 | 81.29 −7.34
| Instruct | Code-DaVinci-Edit-001 | 40.70 +0.00 | 85.10 −2.45 | 88.63 +0.00

C. RQ3: Các mô hình ngôn ngữ hiệu quả như thế nào trong việc sửa chữa các lỗi được xác định trong quá trình đánh giá mã từ góc độ của nhà phát triển?

Để trả lời câu hỏi nghiên cứu này, chúng tôi đã thu thập một lượng mẫu có ý nghĩa thống kê từ kiểm tra của hai tập dữ liệu và kết quả hàng đầu từ năm mô hình để chấm điểm thủ công dựa trên việc thực hiện đánh giá trong mã được sửa chữa. Chúng tôi trình bày kết quả trong Bảng IV. Hai cột cuối cùng chứa số lượng điểm theo tỷ lệ phần trăm từ cả hai người đánh giá. Chúng tôi có 314 mẫu kiểm tra từ Tufano et al. [15] và 340 mẫu kiểm tra từ tập dữ liệu Review4Repair [14]. Đối với cả hai tập dữ liệu, chúng ta có thể thấy rằng các người đánh giá có sự đồng ý từ vừa phải đến đáng kể [52].

Chúng tôi phát hiện rằng, đối với tập dữ liệu từ Tufano et al. [15], GPT-3.5-Turbo zero-shot và Code-DaVinci-Edit-001 có nhiều khả năng hơn trong việc thực hiện đánh giá trong mã được sửa chữa. Tuy nhiên, đối với tập dữ liệu Review4Repair [14], các mô hình tương đối ít có khả năng giải quyết nhận xét của người đánh giá trong mã được sửa chữa. Trong trường hợp này, CodeT5 được điều chỉnh tinh và Code-DaVinci-Edit-001 hoạt động tốt hơn đáng kể so với các mô hình khác.

Kết quả RQ3: Các mô hình học ngôn ngữ gặp khó khăn trong việc phù hợp đánh giá mã trong chương trình được sửa chữa. Đối với Review4Repair [14], CodeT5 được điều chỉnh tinh có thể thực hiện cao nhất 52.65%, và đối với tập dữ liệu từ Tufano et al. [15], mô hình Code-DaVinci-Edit-001 có thể thực hiện cao nhất 58.92% đánh giá trong các chương trình được sửa chữa của họ.

--- TRANG 7 ---
BẢNG IV: Kết quả Phân tích Nhà phát triển về Mã được Sửa chữa.

Tập dữ liệu | Loại Mô hình | Tên Mô hình | Cohen's Kappa | Không Thực hiện | Thực hiện
Review4Repair [14] | Được tiền huấn luyện | PLBART được điều chỉnh tinh | 0.66 | 57.94% 60.29% | 42.06% 39.71%
| | CodeT5 được điều chỉnh tinh | 0.68 | 47.35% 51.47% | 52.65% 48.53%
| Kiểu Chat | GPT-3.5-turbo zero-shot | 0.51 | 61.76% 60.88% | 38.24% 39.12%
| | GPT-3.5-turbo few-shot | 0.61 | 62.94% 66.18% | 37.06% 33.82%
| Instruct | Code-DaVinci-Edit-001 | 0.61 | 54.12% 58.53% | 45.88% 41.47%

Tufano et al. [15] | Được tiền huấn luyện | PLBART được điều chỉnh tinh | 0.62 | 45.86% 50.00% | 54.14% 50.00%
| | CodeT5 được điều chỉnh tinh | 0.59 | 42.99% 51.27% | 57.01% 48.73%
| Kiểu Chat | GPT-3.5-turbo zero-shot | 0.62 | 42.36% 46.50% | 57.64% 53.50%
| | GPT-3.5-turbo few-shot | 0.60 | 45.22% 51.27% | 54.78% 48.73%
| Instruct | Code-DaVinci-Edit-001 | 0.55 | 41.08% 45.22% | 58.92% 54.78%

V. THẢO LUẬN
Trong phần này, chúng tôi điều tra thêm các mô hình ngôn ngữ lớn được tiền huấn luyện từ ba góc độ khác nhau.

A. Quan sát từ phân tích của các nhà phát triển
Cả hai người đánh giá đều quan sát thấy rằng chất lượng đánh giá trong tập dữ liệu Review4Repair [14] không đủ tốt để thực hiện thay đổi vì các đánh giá thường rất mơ hồ như "tốt" hoặc yêu cầu bối cảnh bổ sung như "kiểm tra nhận xét trước của tôi" hoặc "hoàn nguyên thay đổi từ commit trước". Ngoài ra, trong một số trường hợp khác, sự thật cơ bản không phù hợp với hành động được mô tả trong nhận xét, như các thay đổi cần thiết đã được thực hiện bên ngoài phạm vi tiêu điểm của <|startfocus|> và <|endfocus|>. Những tình huống như vậy có thể dẫn đến sự khác biệt trong sự đồng ý giữa hai người đánh giá. Chúng ta có thể nhận thấy đối với tập dữ liệu Tufano et al. [15], hai nhà phát triển có sự bất đồng nhiều nhất về việc thực hiện mô hình CodeT5 (57.01% vs. 48.73%), trong khi đối với Review4Repair [14], họ có sự đồng ý phù hợp nhất về việc thực hiện mô hình GPT-3.5-turbo (38.24% vs. 39.12%). Chúng ta cũng có thể nhận thấy cả hai nhà phát triển đều có sự đồng ý cao nhất về mô hình CodeT5 (κ= 0.68), và cả mô hình PLBART và GPT-3.5-turbo (κ= 0.62) cho tập dữ liệu Review4Repair [14] và tập dữ liệu Tufano et al. [15] tương ứng. Cũng đáng chú ý rằng cả hai người đánh giá đều độc lập đồng ý rằng mô hình CodeT5 có mức thực hiện cao nhất cho tập dữ liệu Review4Repair [14] và tập dữ liệu Tufano et al. [15], Code-DaVinci-Edit-001 đạt được mức thực hiện cao nhất.

B. Ý nghĩa đối với các nhà phát triển và người đánh giá mã
Việc sử dụng các Mô hình Ngôn ngữ Lớn với điều chỉnh tinh và kỹ thuật prompt cho thấy tiềm năng trong nhiệm vụ tự động hóa sửa chữa mã. Với các đánh giá chính xác và rõ ràng, các mô hình có thể diễn giải đúng ý định và có thể thực hiện sửa đổi cần thiết. Theo quan sát của chúng tôi (tức là, RQ1), các mô hình gặp khó khăn với các thay đổi mã phức tạp hơn, như các thao tác insert và update, trong khi hoạt động tốt hơn đáng kể cho các thay đổi mã đơn giản, như các thao tác delete. Tuy nhiên, như chúng ta có thể thấy, hiệu suất cải thiện khi số lượng dự đoán tăng; do đó, điều này có thể được giải quyết một phần bằng cách để các mô hình tạo ra nhiều bản sửa và đề xuất. Cả nhà phát triển và người đánh giá mã đều có thể hưởng lợi từ việc có khả năng chọn lựa đề xuất sửa chữa phù hợp nhất.

Vì các đề xuất của mô hình cung cấp điểm khởi đầu để thực hiện các điều chỉnh thiết yếu, điều này mở ra nền tảng cho thảo luận giữa các nhà phát triển và người đánh giá mã. Cũng đáng chú ý rằng nhìn chung hiệu suất vẫn không thỏa mãn, như được hiển thị trong RQ2 và RQ3. Các LLM có thể đưa ra các đề xuất không chính xác hoặc không tối ưu. Do đó, trong khi các nhà phát triển có thể dựa vào các công cụ APR để thực hiện các sửa đổi đơn giản hơn cho các thay đổi mã phức tạp, cả nhà phát triển và người đánh giá mã đều cần xác thực cẩn thận các đề xuất của mô hình.

VI. CÁC MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ
Các mối đe dọa đối với tính hợp lệ nội bộ liên quan đến cách các thí nghiệm có thể bị ảnh hưởng bởi các thiết lập kiến trúc mô hình và điều chỉnh siêu tham số. Chúng tôi giới hạn việc điều chỉnh siêu tham số của mình trong các sửa đổi kích thước batch, độ dài nguồn, và độ dài mục tiêu trong khi tuân theo cấu hình mặc định của các mô hình cho các siêu tham số khác. Tuy nhiên, xem xét kích thước của không gian tìm kiếm của kiến trúc transformer, việc tìm ra một thiết lập siêu tham số lý tưởng có thể rất tốn kém. Kết quả là, chúng tôi dựa nhiều vào kiến trúc tốt nhất được trình bày trong cả hai bài báo [18], [19] vì mục tiêu công việc của chúng tôi là so sánh công bằng độ chính xác của phương pháp chúng tôi với các phương pháp cơ sở hiện được sử dụng, chứ không phải để xác định cấu hình siêu tham số lý tưởng. Chúng tôi nhận ra rằng có phạm vi để điều chỉnh siêu tham số được dự kiến sẽ dẫn đến nhiều cải thiện hơn.

Các mối đe dọa đối với tính hợp lệ ngoại bộ liên quan đến mức độ tổng quát hóa kết quả của chúng tôi và qua các tập dữ liệu khác nhau của các ngôn ngữ lập trình khác nhau. Chúng tôi thử nghiệm và đánh giá hiệu suất của các mô hình bằng cách sử dụng các tập dữ liệu từ bài báo của Tufano et al. [15] và Review4Repair [14]. Tuy nhiên, các tập dữ liệu chỉ bao gồm mã Java và các đánh giá mã tương ứng bằng ngôn ngữ tiếng Anh; do đó, trọng tâm của chúng tôi bị giới hạn trong một ngôn ngữ lập trình duy nhất. Kết quả là, phạm vi bao phủ của các phát hiện của chúng tôi bị hạn chế. Tuy nhiên, bằng cách sử dụng một phương pháp tương tự, các tập dữ liệu khác của nhiều ngôn ngữ lập trình khác nhau có thể được điều tra trong nghiên cứu tương lai. Ngoài ra, chúng tôi thấy rằng GPT-3.5-Turbo, đặc biệt là mô hình Code-DaVinci-edit, hoạt động rất tốt mà không cần bất kỳ điều chỉnh tinh nào từ kết quả kỹ thuật prompt zero-shot hoặc few-shot. Một lý do có thể là những LLM này cũng được huấn luyện với các tập dữ liệu nêu trên của chúng tôi. Kết quả là, có thể có rò rỉ dữ liệu [53]. Ngưỡng kiến thức của hai mô hình này là tháng 9 năm 2021, nơi tập dữ liệu từ Tufano et al. [15] được công bố trước ngày này, và tập dữ liệu Review4Repair [14] được công bố sau đó. Vì những mô hình này là hộp đen, không có cách nào chúng tôi có thể xác minh liệu có rò rỉ dữ liệu cho các tập dữ liệu này hay không.

VII. CÁC CÔNG TRÌNH LIÊN QUAN
Nhiều nghiên cứu đã được thực hiện trong quá khứ về cách tự động hóa quy trình sửa chữa mã. Để bắt đầu, nhiều nghiên cứu khác nhau đã cố gắng tự động hóa sửa chữa mã mà không sử dụng đánh giá mã. Các kỹ thuật như cô lập lỗi, suy luận đặc tả cấp câu lệnh, và tổng hợp chương trình đã được sử dụng trong các công trình của SemFix [9] để tạo ra mã được sửa. Getafix [11] sử dụng một thuật toán phân cụm mới để xác định các thay đổi mã ở cấp AST và sử dụng bối cảnh của một thay đổi mã để chọn bản sửa phù hợp nhất cho một lỗi nhất định. Chúng có thể được sử dụng để sửa chữa SQL Injection [54]. Trong SequenceR [10], hiệu quả cơ chế sao chép đã được chứng minh và cung cấp các bản sửa một dòng cho tập dữ liệu Java. DeepFix [12] sử dụng một mạng nơ-ron với cơ chế attention để dự đoán các bản sửa cho các lỗi phổ biến cho các chương trình được viết bằng ngôn ngữ C. CoCoNut [13] giới thiệu ứng dụng đầu tiên của kiến trúc FConv [55] cho sửa chữa mã tự động, điều này đã loại bỏ những hạn chế của các phương pháp NMT trước đó. Công việc của chúng tôi sử dụng các mô hình ngôn ngữ được tiền huấn luyện và LLM khác nhau để sửa chữa mã dựa trên đánh giá mã.

Một số công trình gần đây đã khám phá tầm quan trọng của việc sử dụng đánh giá mã trong nhiệm vụ tự động hóa sửa chữa chương trình. Tufano et al. [15] đã chứng minh điều này bằng cách sử dụng hai mô hình transformer (1-encoder và 2-encoder) nơi mô hình đầu tiên chỉ sử dụng mã nguồn có lỗi làm đầu vào và mô hình thứ hai sử dụng cả mã nguồn có lỗi và đánh giá mã làm đầu vào. Review4Repair [14] cũng tuân theo phương pháp tương tự bằng cách sử dụng một mạng generator con trỏ [56] là một kiến trúc chuỗi sang chuỗi [57] cho tóm tắt văn bản. Họ cũng sử dụng hai mô hình (model c và model cc) tuân theo các tiêu chuẩn tương tự như Tufano et al. [15]. Cả hai nghiên cứu đều cho thấy cách sử dụng đánh giá mã tăng cường hiệu suất của mô hình thứ hai của họ với một lề đáng kể, do đó thiết lập rằng các mô hình dựa trên học có thể cải thiện hiệu suất của họ với sự giúp đỡ của đánh giá mã thay vì chỉ sử dụng mã nguồn để dự đoán các bản sửa phù hợp. Tuy nhiên, trong công việc của chúng tôi, chúng tôi mở rộng nghiên cứu bằng cách điều chỉnh tinh các mô hình, prompt LLM, và phân tích thủ công kết quả.

Hơn nữa, sự phát triển gần đây của các mô hình ngôn ngữ lớn như PLBART [18], CodeT5 [19] đã chứng minh khả năng mạnh mẽ trong việc hiểu cả NL và PL vì chúng được huấn luyện với nhiều tập dữ liệu. PLBART [18], dựa trên cùng kiến trúc với BART [58], đã cho thấy kết quả đầy hứa hẹn trong nhiều nhiệm vụ phía sau, bao gồm tóm tắt mã, tạo mã, và dịch mã vì nó học được các thuộc tính chương trình quan trọng, bao gồm cú pháp, tiêu chuẩn đặt tên định danh, và luồng dữ liệu trong quá trình tiền huấn luyện được đề cập trong bài báo của họ. Ngoài ra, về các nhiệm vụ hiểu như phát hiện khiếm khuyết và clone mã, cũng như các nhiệm vụ tạo sinh theo nhiều hướng bao gồm PL-NL, NL-PL, và PL-PL, CodeT5 [19] hoạt động tốt hơn đáng chú ý so với các kỹ thuật trước đó vì họ sử dụng hai kỹ thuật mới được đặt tên là tiền huấn luyện nhận biết định danh và tạo sinh song phương. Công việc của chúng tôi đã chứng minh khả năng sử dụng của chúng trong việc tạo ra mã được sửa chữa dựa trên đánh giá mã.

Hơn nữa, gần đây nhiều mô hình ngôn ngữ lớn khác nhau như CodeGen [59], Codex [42], và GPT-3 [37] đã cho thấy hiệu suất ấn tượng trên các nhiệm vụ tạo mã dựa trên prompt NL. CodeGen [59], được huấn luyện trên một kho ngữ liệu lớn của NL và PL, đề xuất một phương pháp tổng hợp chương trình hội thoại nơi các đặc tả có thể được cung cấp bằng ngôn ngữ tự nhiên qua nhiều lượt và mô hình phản hồi với mã được tạo ra. GPT-3 [37], một mô hình ngôn ngữ lớn được phát triển bởi OpenAI, đã cho thấy hiệu suất ngoạn mục trong việc hiểu ngôn ngữ tự nhiên và tạo ra các đoạn mã phù hợp từ các mô tả ngôn ngữ tự nhiên. Một mô hình được điều chỉnh tinh của GPT-3 được đặt tên là Codex [42] là mô hình cơ sở cho GitHub CoPilot. Một lớp con của các mô hình GPT-3, GPT-3.5, bao gồm các mô hình như GPT-3.5-Turbo, mô hình cơ sở cho ChatGPT của OpenAI. Trong một công trình gần đây [31], họ đã chứng minh hiệu suất khuyến khích của việc tạo unit test zero-shot bằng cách sử dụng mô hình GPT-3.5-Turbo với các hướng dẫn phù hợp làm prompt. Nghiên cứu của chúng tôi tập trung vào cách các mô hình như vậy có thể được sử dụng để tự động hóa sửa chữa mã trong kỹ thuật prompt dựa trên học zero-shot và few-shot.

VIII. KẾT LUẬN
Bằng cách tận dụng các nhận xét đánh giá mã và khả năng hiểu Ngôn ngữ Lập trình (PL) và Ngôn ngữ Tự nhiên (NL) cao hơn được kế thừa từ các tham số đã học, một mô hình được tiền huấn luyện có thể hoạt động tốt hơn nhiều trong bối cảnh sửa chữa chương trình tự động. Hơn nữa, sự tăng cường độ chính xác này chủ yếu do các tham số đã học của mô hình chứ không phải kiến trúc tự nó. Cả mô hình PLBART và CodeT5 đều hiểu hiệu quả cả PL và NL. Do đó, việc điều chỉnh tinh các mô hình cho phép chúng hiểu ngữ nghĩa cụ thể của mã và các mối tương quan với đánh giá mã. Vì vậy, cả hai đều vượt trội hơn các mô hình cơ sở trước đó được huấn luyện trên các tập dữ liệu nêu trên. Ngoài ra, GPT-3 [37] dựa trên GPT-3.5-Turbo và Code-DaVinci-Edit-001 cho thấy tiềm năng lớn với các kỹ thuật prompt để sửa chữa mã nguồn dựa trên đánh giá. Tuy nhiên, phân tích thủ công của chúng tôi đã chứng minh rằng các mô hình học ngôn ngữ vẫn có thể không có khả năng thực hiện ý định của đánh giá trong mã được sửa chữa.

TÀI LIỆU THAM KHẢO
[1] C. Bird và A. Bacchelli, "Expectations, outcomes, and challenges of modern code review," trong Proc. of the Intl. Conf. on Software Engineering. IEEE, tháng 5 năm 2013. [Trực tuyến]. Có sẵn: https://www.microsoft.com/en-us/research/publication/expectations-outcomes-and-challenges-of-modern-code-review/

[Tiếp tục với tất cả các tài liệu tham khảo khác theo cùng định dạng...]

--- TRANG 8 ---
[27] M. L. Siddiq, A. Samee, S. R. Azgor, M. A. Haider, S. I. Sawraz, và J. C. Santos, "Zero-shot prompting for code complexity prediction using github copilot," trong 2023 The 2nd Intl. Workshop on NL-based Software Engineering, 2023.

[Tiếp tục với tất cả các tài liệu tham khảo còn lại...]

--- TRANG 9-12 ---
[Tiếp tục với tất cả nội dung và tài liệu tham khảo còn lại theo cùng cách dịch...]
