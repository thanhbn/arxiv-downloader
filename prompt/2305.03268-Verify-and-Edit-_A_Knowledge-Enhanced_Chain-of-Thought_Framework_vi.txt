# Khung công tác Verify-and-Edit: Một khung suy luận dây chuyền được tăng cường bằng kiến thức

Ruochen Zhao1 Xingxuan Li1;2y Shafiq Joty1;3z Chengwei Qin1 Lidong Bing2
1Đại học Công nghệ Nanyang, Singapore
2Viện DAMO, Tập đoàn Alibaba
3Salesforce AI
{ruochen002, chengwei003}@e.ntu.edu.sg
{xingxuan.li, l.bing}@alibaba-inc.com
srjoty@ntu.edu.sg

## Tóm tắt

Khi các mô hình ngôn ngữ lớn (LLM) đã trở thành chuẩn mực trong NLP, thể hiện hiệu suất tốt trong các tác vụ sinh và suy luận, một trong những nhược điểm nghiêm trọng nhất của chúng là thiếu tính chính xác về mặt sự thật. Việc sinh ra các văn bản không chính xác không chỉ dẫn đến hiệu suất thấp hơn mà còn làm giảm niềm tin và tính hợp lệ của các ứng dụng. Prompting Dây chuyền Tư duy (CoT) cải thiện niềm tin và hiệu suất mô hình trên các tác vụ suy luận phức tạp bằng cách sinh ra các chuỗi suy luận có thể diễn giải được, nhưng vẫn gặp phải các vấn đề về tính thực tế trong các tác vụ đòi hỏi kiến thức chuyên sâu. Trong bài báo này, chúng tôi đề xuất khung công tác Verify-and-Edit cho prompting CoT, nhằm tăng tính thực tế của dự đoán bằng cách chỉnh sửa hậu kỳ các chuỗi suy luận theo kiến thức bên ngoài. Xây dựng trên nền tảng GPT-3, khung công tác của chúng tôi dẫn đến cải thiện độ chính xác trong nhiều tác vụ trả lời câu hỏi miền mở. Để tái tạo kết quả của chúng tôi và mở rộng khung công tác hơn nữa, chúng tôi cung cấp codebase tại https://github.com/RuochenZhao/Verify-and-Edit

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) đã trở thành chuẩn mực mới trong nhiều tác vụ NLP downstream. Trong việc sử dụng các LLM này, prompting Dây chuyền Tư duy (CoT) (Wei et al., 2022) được tìm thấy là cải thiện hiệu suất cho các tác vụ đòi hỏi suy luận phức tạp, chẳng hạn như các bài toán từ toán học, suy luận thông thường và thao tác ký hiệu. Đồng thời, nó có thể sinh ra các chuỗi suy luận có thể diễn giải được. Các nghiên cứu gần đây đã khám phá thêm cách sử dụng các chuỗi suy luận này để lựa chọn các dự đoán tốt hơn.

Tuy nhiên, trọng tâm chính của các phương pháp này là cải thiện hiệu suất tác vụ cuối bằng cách sử dụng các CoT được sinh ra như chúng vốn có. Ví dụ, Ye và Durrett (2022) huấn luyện một bộ hiệu chỉnh điều chỉnh xác suất dự đoán dựa trên điểm số lý luận; Wang et al. (2022) lấy mẫu nhiều đường suy luận để tìm dự đoán phổ biến (nhất quán) nhất. Chỉ một số ít, như Creswell et al. (2022) và Zhou et al. (2022), đã khám phá các cách để cải thiện chất lượng của bản thân các CoT.

Thực tế, việc cải thiện chất lượng CoT có thể có lợi trong việc tăng cường cả khả năng diễn giải và hiệu suất tác vụ cuối. Ye và Durrett (2022) chỉ ra rằng các giải thích được con người đánh giá là tốt thường cho thấy các dự đoán chính xác hơn. Theo trực giác, một tập hợp các prompt CoT tốt hơn có thể cung cấp nền tảng tốt hơn và các quá trình tư duy nhất quán về mặt logic, do đó dẫn đến các dự đoán chính xác hơn.

Để cải thiện chất lượng sinh, một khía cạnh quan trọng là tính chính xác về mặt sự thật, hiện đang là một trong những nhược điểm nghiêm trọng nhất của LLM (OpenAI-Blog, 2022; Zhao et al., 2023). Trong việc trả lời các truy vấn của người dùng, các LLM như GPT-3 (Brown et al., 2020) có xu hướng bịa ra các sự thật và chi tiết, điều này hiện được đánh dấu là cảnh báo chính trong việc sử dụng API của chúng. Vì một trường hợp sử dụng chính của LLM là triển vọng thay thế các công cụ tìm kiếm truyền thống và sử dụng để truy cập thông tin trực tiếp hơn thông qua hỏi-đáp, các vấn đề về tính thực tế có thể làm suy giảm đáng kể tính hợp lệ của chúng và làm giảm mức độ tin tưởng của người dùng (Marcus, 2022). Việc khắc phục vấn đề này là thách thức và các mối quan ngại vẫn tồn tại ngay cả sau khi các mô hình được điều chỉnh hướng dẫn với phản hồi của con người (Ouyang et al., 2022). Điều này là do nguồn sự thật có thể không có sẵn trong quá trình tinh chỉnh (OpenAI-Blog, 2022).

Do đó, việc kiểm soát tốt hơn việc sinh và tăng tính chính xác thực tế của các dự đoán là mối quan ngại cấp thiết. Vì LLM có thể thất bại trong việc nhớ lại các chi tiết chính xác khi hoạt động như một cơ sở kiến thức (Ye và Durrett, 2022; Creswell et al., 2022), nếu có thể, kiến thức từ các nguồn bên ngoài có thể được giới thiệu như sự hỗ trợ. Quá trình tư duy được hỗ trợ cũng phổ biến trong suy luận của con người: khi con người trả lời câu hỏi, họ thường tìm kiếm (hoặc xem lại) các nguồn kiến thức bên ngoài để tìm các sự thật hỗ trợ nhằm làm mới bộ nhớ (nội bộ) của họ.

Được truyền cảm hứng từ điều này, trong công trình này chúng tôi đề xuất khung công tác Verify-and-Edit (VE) để chỉnh sửa hậu kỳ các chuỗi suy luận cho các dự đoán được căn chỉnh thực tế hơn. Như được hiển thị trong Hình 1, đầu tiên chúng tôi chọn các trường hợp không chắc chắn để chỉnh sửa, có tính nhất quán ít-hơn-đa-số-đồng-ý. Các trường hợp này, như được ngụ ý bởi Wang et al. (2022), thường bao gồm các câu phát biểu có vẻ hợp lý, chẳng hạn như câu "John Nyskohus đã chơi cho đội bóng đá Na Uy Odd Greenland" trong Hình 1. Khi chỉnh sửa, đầu tiên chúng tôi sinh ra một câu hỏi để xác minh chi tiết này, chẳng hạn như "John Nyskohus đã chơi cho đội nào?" Sau đó, để trả lời truy vấn này, chúng tôi giới thiệu kiến thức bên ngoài thông qua các hệ thống truy xuất miền mở. Ví dụ, sự thật "John Nyskohus ... đã chơi cho Adelaide City.." được truy xuất trong trường hợp này. Sau đó, các lý luận được chỉnh sửa bằng cách cung cấp các sự thật được truy xuất trong các prompt như việc làm mới bộ nhớ. Do đó, các lý luận được chỉnh sửa có thể được cập nhật tương ứng với các sự thật được truy xuất (Hình 1). Với các lý luận được chỉnh sửa, dự đoán mới được sinh ra, xem xét các dấu vết suy luận được căn chỉnh thực tế hơn.

Theo hiểu biết của chúng tôi, công trình của chúng tôi là đầu tiên chỉnh sửa hậu kỳ các chuỗi suy luận kiểu CoT để tăng cường hiệu suất dự đoán. Chúng tôi thực hiện thí nghiệm trên hai tác vụ Trả lời Câu hỏi (QA) miền mở đòi hỏi suy luận: Adversarial HotpotQA (Yang et al., 2018) và 2WikiMultihop (Ho et al., 2020). Chúng tôi cũng kiểm tra hiệu suất của nó trên tác vụ Xác minh Sự thật sử dụng Fever (Thorne et al., 2018). Chúng tôi nhận thấy rằng mô hình có thể hưởng lợi từ các chuỗi suy luận thực tế hơn, do đó sinh ra các dự đoán chính xác hơn. Ví dụ, đối với QA miền mở, mô hình của chúng tôi thể hiện cải thiện độ chính xác 3.8x so với các mô hình tăng cường truy xuất tương tự trên AdvHotpot. Trên 2WikiMultihop, Verify-and-Edit đạt 33.6% độ chính xác với tìm kiếm miền mở, trong khi CoT Self-Consistency đứng ở 27.7%.

## 2 Công trình liên quan

Dây chuyền Tư duy hay CoT (Wei et al., 2022) là một phương pháp prompting để cải thiện khả năng suy luận của LLM, cho phép LLM phân tách các vấn đề phức tạp thành nhiều bước trung gian. CoT cung cấp khả năng diễn giải và đã được chứng minh là có khả năng giải quyết các vấn đề phức tạp tốt hơn so với các phương pháp prompting tiêu chuẩn.

Tuy nhiên, ảo giác là một vấn đề lâu dài trong NLP, đặc biệt đối với LLM, đã thu hút sự chú ý đáng kể từ cộng đồng nghiên cứu. Quá trình giải mã của LLM là tự hồi quy, điều này không thể tránh khỏi việc tạo ra nội dung không thực tế mà không có việc sinh được kiểm soát (Ye và Durrett, 2022; Wiegreffe et al., 2022). Do đó, việc thiếu các sự thật hỗ trợ trong quá trình sinh CoT có thể làm suy giảm đáng kể tính hợp lệ của câu trả lời cuối cùng (Golovneva et al., 2022). Ye và Durrett (2022) chứng minh rằng độ chính xác của các câu trả lời cuối cùng có mối tương quan lớn với tính thực tế và nhất quán của các giải thích suy luận. Các phương pháp thường được đề xuất để cải thiện tính thực tế của quá trình suy luận CoT có thể được nhóm thành hai loại: kỹ thuật prompt và hiệu chỉnh kết quả.

Các phương pháp kỹ thuật prompt thường được áp dụng để hướng dẫn LLM sinh ra các giải thích suy luận trung gian tốt hơn. ReAct (Yao et al., 2022), có thể so sánh nhất với công trình của chúng tôi, tạo ra sự tương tác giữa suy luận và hành động trong LLM, nơi các bước suy luận giúp mô hình suy ra và cập nhật các hành động, trong khi các bước hành động cho phép mô hình tham khảo thông tin bổ sung từ Wikipedia để kiểm tra tính thực tế. So với ReAct, chúng tôi sinh ra các CoT tự nhiên và đàm thoại hơn để có khả năng diễn giải tốt hơn và học dễ dàng hơn. Do đó, khung công tác của chúng tôi yêu cầu một prompt ngắn hơn nhiều để học. Press et al. (2022) đề xuất self-ask bằng cách hướng dẫn LLM tự hỏi mình (và sau đó trả lời) các câu hỏi tiếp theo trước khi trả lời câu hỏi ban đầu. Một cách tự nhiên để giải quyết một vấn đề phức tạp là phân tách vấn đề thành các vấn đề con và giải quyết chúng một cách tuần tự. Zhou et al. (2022) áp dụng ý tưởng này và đề xuất prompting từ-ít-đến-nhiều. Tuy nhiên, cả self-ask và prompting từ-ít-đến-nhiều vẫn dựa vào việc truy xuất lặp đi lặp lại kiến thức nội bộ được học bởi LLM thay vì kết nối với kiến thức bên ngoài. Do đó, khả năng cải thiện tính thực tế của chúng bị hạn chế.

Hiệu chỉnh kết quả hoạt động trên đầu ra của LLM. Ye và Durrett (2022) huấn luyện một bộ hiệu chỉnh để hiệu chỉnh trọng số của các câu trả lời cuối cùng dựa trên tính thực tế và nhất quán của các giải thích được sinh ra, điều này cải thiện hiệu quả các kết quả. Phương pháp giải mã trong CoT là tham lam ngây thơ, chỉ đơn giản xuất ra token tiếp theo với xác suất cao nhất. Wang et al. (2022) đề xuất một phương pháp giải mã tự nhất quán, lấy mẫu một tập hợp đa dạng các đường suy luận và sau đó chọn câu trả lời nhất quán nhất bằng cách biên hóa các đường suy luận được lấy mẫu. Khung công tác Selection-Inference (SI) (Creswell et al., 2022) là một phương pháp tiên tiến khác khai thác LLM như các module xử lý tổng quát. Trong tất cả các phương pháp, nó cũng là đầu tiên cải thiện có hệ thống tính chính xác thực tế của CoT để dự đoán chính xác hơn. Nó xen kẽ giữa lựa chọn và suy luận để sinh ra một loạt các bước suy luận có thể diễn giải, nhân quả dẫn đến câu trả lời cuối cùng, được chứng minh là hiệu quả. Tuy nhiên, nó không được thiết kế cho việc trả lời câu hỏi miền mở hoặc thông thường.

Hơn nữa, một hướng nghiên cứu có thể so sánh khác đã khám phá việc tiền huấn luyện mô hình ngôn ngữ tăng cường truy xuất (REALM) (Guu et al., 2020), đầu tiên truy xuất tài liệu từ một nguồn kiến thức bên ngoài và sau đó sử dụng các tài liệu được truy xuất để xử lý các tác vụ trả lời câu hỏi. Lazaridou et al. (2022) đề xuất bao gồm kết quả tìm kiếm Google của câu hỏi trong prompt để cải thiện tính thực tế của câu trả lời được sinh ra. Tuy nhiên, các phương pháp như vậy có thể thất bại trong các câu hỏi phức tạp vì nó không sử dụng khả năng suy luận của LLM. Do đó, chúng tôi coi các đường suy luận tăng cường truy xuất như một cách tự nhiên để tăng sự căn chỉnh thực tế.

## 3 Khung công tác Verify-and-Edit

Mục tiêu của chúng tôi là làm cho LLM sinh ra các chuỗi suy luận thực tế hơn với prompting CoT được hỗ trợ bởi kiến thức bên ngoài, từ đó cũng cải thiện độ chính xác dự đoán của câu trả lời cuối cùng. Chúng tôi giả thuyết rằng điều này có thể tăng cường khả năng của LLM trong việc giải quyết các tác vụ phức tạp đòi hỏi kiến thức chuyên sâu mà cần nhiều bước suy luận để đến được câu trả lời.

Nhìn chung, chúng tôi hy vọng tuân theo quá trình suy luận của con người: khi một người trả lời một câu hỏi, nếu anh/cô ấy không chắc chắn, anh/cô ấy sẽ tìm kiếm một sự thật hỗ trợ và xem xét nó trước khi đưa ra câu trả lời cuối cùng. Do đó, chúng tôi có thể tách khung công tác Verify-and-Edit (VE) thành 3 giai đoạn khác nhau: tìm các dự đoán không chắc chắn, chỉnh sửa lý luận của chúng bằng cách tìm kiếm các sự thật hỗ trợ, và sử dụng các lý luận được chỉnh sửa để sinh ra câu trả lời cuối cùng (Hình 1).

Trong việc thiết kế các giai đoạn, chúng tôi hy vọng bảo tồn tối đa lợi thế lớn nhất của LLM: khả năng sinh mở và suy luận của chúng. Và chúng tôi nhắm đến thiết kế các tác vụ và thiết lập càng tự nhiên và đàm thoại càng tốt, do đó làm cho nó dễ hiểu cho con người và LLM được huấn luyện với các văn bản tự nhiên.

### 3.1 Quyết định khi nào chỉnh sửa

Làm thế nào chúng ta có thể xác định khi nào một mô hình không chắc chắn về dự đoán của nó? Phương pháp tự nhất quán (Wang et al., 2022) cung cấp một giải pháp. Trong việc lấy mẫu các đường suy luận và câu trả lời đa dạng, tự nhất quán được tìm thấy có mối tương quan cao với độ chính xác, cho thấy rằng nó có thể cung cấp một ước lượng không chắc chắn và trao khả năng cho mô hình để "biết khi nào nó không biết". Do đó, chúng tôi bắt đầu khung công tác VE bằng cách sử dụng phương pháp nhất quán để lấy mẫu n đường suy luận đa dạng cho một tác vụ dự đoán. Các dự đoán có tính nhất quán cao được để nguyên. Khi tính nhất quán thấp hơn ⌈n/2⌉, tức là đa số không thể đồng ý về cùng một câu trả lời, chúng tôi gắn nhãn nó là "không chắc chắn".

### 3.2 Cách chỉnh sửa một lý luận cụ thể

Lý luận, tức là quá trình tư duy (CoT), có thể được xem trong hai phần: sự thật và suy luận kết hợp các sự thật để rút ra một tuyên bố mới. Do đó, chúng tôi xem xét việc cải thiện CoT từ cả hai khía cạnh.

**Sự thật** Để làm cho quá trình tư duy chính xác hơn về mặt thực tế, chúng tôi tìm kiếm các sự thật hỗ trợ trong các nguồn kiến thức bên ngoài (ví dụ: Wikipedia, Google).

Đầu tiên, để bắt chước truy vấn của con người khi tìm kiếm các sự thật xác nhận, một câu hỏi tự nhiên được sinh ra để xác minh lý luận. Đối với điều này, chúng tôi sử dụng khả năng học trong ngữ cảnh của cùng một LLM. Câu hỏi gốc và lý luận đều được cung cấp trong prompt để sinh câu hỏi xác minh nhằm đảm bảo rằng nó hỏi về thông tin liên quan nhất cần thiết để trả lời câu hỏi gốc, thay vì các thực thể khác trong lý luận. Ví dụ, nếu lý luận (sai) là "tổng thống Mỹ sinh ngày 4 tháng 8 năm 1961 là John Kennedy." và câu hỏi gốc là "ai là vợ/chồng của tổng thống Mỹ sinh ngày 4 tháng 8 năm 1961", chúng tôi mong đợi câu hỏi xác minh được sinh ra sẽ là: "Ai là tổng thống Mỹ sinh ngày 4 tháng 8 năm 1961?" thay vì "Khi nào là sinh nhật của John Kennedy?" Bằng cách sinh ra một câu hỏi có liên quan thay vì truy vấn trực tiếp với lý luận được sinh ra, chúng tôi loại bỏ tiếng ồn tiềm ẩn do sinh sự thật không chính xác mang lại. Trong ví dụ trên, nếu người ta truy xuất sử dụng tuyên bố sai "tổng thống Mỹ sinh ngày 4 tháng 8 năm 1961 là John Kennedy", thực thể không chính xác "John Kennedy" có thể làm rối quá trình tìm kiếm.

Trong bài báo này, chúng tôi sử dụng các ngữ cảnh có liên quan được truy xuất từ 3 hệ thống: (i) DrQA (Chen et al., 2017), một hệ thống trả lời câu hỏi miền mở; (ii) tìm kiếm Wikipedia các trang liên quan; và (iii) tìm kiếm Google, thể hiện khả năng kết hợp LLM và công cụ tìm kiếm.

Vì các ngữ cảnh được truy xuất từ một hệ thống truy xuất có thể dài hơn mong muốn, chúng tôi sử dụng một LM được tiền huấn luyện để xếp hạng và chọn top-k câu tương tự nhất với truy vấn câu hỏi xác minh.

**Suy luận** Trong khi các phương pháp như Selection-Inference (Creswell et al., 2022) trực tiếp sử dụng các sự thật được truy xuất làm lý luận, chúng thường quá dài dòng, dài hơn mong muốn, hoặc chứa các chi tiết không liên quan. Ye và Durrett (2022) đã có những quan sát tương tự: việc sử dụng trực tiếp các câu hỗ trợ thường quá dài dòng và không đủ.

Để có được các lý luận liên quan và logic hơn, chúng tôi lại sử dụng một cách tiếp cận tự nhiên và sinh, vì khả năng suy luận được tin là đã được xây dựng vào LLM (Wei et al., 2022). Cụ thể, bằng cách đưa vào các prompt theo định dạng "câu hỏi, lý luận, câu trả lời", LLM học cách suy luận trong vài bước trước khi sinh câu trả lời. Khi điều tra các lý luận gốc, chúng tôi quan sát thấy rằng, ngay cả khi chúng chứa các sự thật không chính xác, thành phần suy luận logic dường như nhìn chung vẫn nguyên vẹn. Do đó, chúng tôi sử dụng các câu hỏi xác minh (như logic) và các sự thật được truy xuất (như thông tin) để sinh ra các câu trả lời có thông tin. Các câu trả lời có thông tin sau đó được soạn thành một lý luận mới, cung cấp một CoT có khả năng thực tế hơn.

### 3.3 Trả lời lại

Cuối cùng, với CoT được chỉnh sửa hậu kỳ, các câu trả lời mới được sinh ra bằng cách prompting LLM. Một mã giả của toàn bộ quy trình được đưa ra trong Thuật toán 1, và được minh họa bằng một ví dụ trong Hình 1. Chúng ta có thể thấy rằng, bằng cách cho phép LLM kết hợp kiến thức bên ngoài, phương pháp của chúng tôi có thể dẫn đến các lý luận có nền tảng thực tế hơn. Khi được prompting vào LLM như một CoT, nó có thể mang lại thông tin cần thiết để đưa ra một dự đoán mới, điều mà ban đầu không được mô hình nhớ chính xác.

So với các prompt được thiết kế cụ thể như ReAct (Yao et al., 2022), khung công tác Verify-and-Edit đơn giản và có thể nói là tự nhiên hơn. Bản chất đàm thoại của nó có thể cho phép con người hiểu rõ hơn các quá trình tư duy của mô hình và có tiềm năng cho người dùng can thiệp và sửa đổi một cách tự nhiên ở bất kỳ giai đoạn nào của suy luận. Trong các thí nghiệm được trình bày tiếp theo, chúng tôi cũng quan sát thấy rằng một thiết lập như vậy có hiệu quả trong việc giảm thiểu các mối quan ngại về tính thực tế và thúc đẩy hiệu suất tác vụ cuối.

## 4 Thiết lập Thí nghiệm

### 4.1 Các tác vụ suy luận

Vì khung công tác Verify-and-Edit cung cấp các bước suy luận có nền tảng kiến thức hơn, nó nên có lợi cho các tác vụ đáp ứng hai tính chất sau: (i) phụ thuộc vào suy luận đa bước để đến dự đoán sau đó, do đó phụ thuộc vào việc sinh lý luận, và (ii) miền mở, do đó cần tương tác với một nguồn kiến thức bên ngoài.

Do đó, chúng tôi xác thực cách tiếp cận trên ba bộ dữ liệu: (i) Adversarial HotpotQA (Yang et al., 2018), một bộ dữ liệu trả lời câu hỏi đa bước. Chúng tôi sử dụng tập con thách thức được đề xuất bởi Ye và Durrett (2022), nơi các dự đoán đúng và sai được cân bằng sử dụng mô hình của họ. (ii) 2Wiki-Multihop (Ho et al., 2020) một bộ dữ liệu trả lời câu hỏi đa bước khai thác định dạng có cấu trúc trong Wikidata và sử dụng các quy tắc logic.¹ (iii) Fever (Thorne et al., 2018), một bộ dữ liệu xác minh sự thật gắn nhãn các tuyên bố là "SUPPORTS", "REFUTES", hoặc "NOT ENOUGH INFO" dựa trên các đoạn bằng chứng từ Wikipedia. Tương tự như thiết lập HotpotQA, chúng tôi lấy mẫu một tập thách thức bằng cách cân bằng các mẫu nơi GPT3 CoT đưa ra dự đoán đúng và sai. Chi tiết về việc xử lý và sử dụng các bộ dữ liệu có thể được tìm thấy trong Phụ lục A.

### 4.2 Các phương pháp so sánh

Để cung cấp các ước lượng hiệu suất tiên tiến nhất, chúng tôi sử dụng API series instruct GPT-3 text-davinci-003 (Ouyang et al., 2022), mô hình mạnh nhất và cập nhật nhất tại thời điểm thí nghiệm, làm backbone. Chi phí thí nghiệm được nêu trong Phụ lục B.

Các thí nghiệm Adversarial HotpotQA và 2WikiMultihop sử dụng 6-shot và Fever sử dụng 3-shot trong học ngữ cảnh, vì các câu hỏi Fever ngắn hơn và dễ học hơn. Chúng tôi sử dụng các chú thích thủ công được cung cấp cho HotpotQA bởi Ye và Durrett (2022) và chú thích thủ công các ví dụ few-shot cho 2WikiMultihop và Fever theo định dạng tương tự. Các prompt đầy đủ cho baseline và phương pháp của chúng tôi được cung cấp trong Phụ lục C.

**Baseline** Để cung cấp một cái nhìn tổng quát toàn diện hơn về vị trí của khung công tác chúng tôi, chúng tôi sử dụng các baseline sau:

1. **Dự đoán Tiêu chuẩn (Standard)**: Dự đoán trực tiếp nhãn dựa trên đầu vào, với cùng số lượng ví dụ học trong ngữ cảnh.

2. **CoT Gốc (Wei et al., 2022)**: Dự đoán nhãn sau khi sinh giải thích.

3. **CoT với Self-Consistency (CoT-SC) (Wang et al., 2022)**: Lấy mẫu 5 quỹ đạo CoT với nhiệt độ giải mã 0.7, được khuyến nghị bởi bài báo.

4. **Calibrator (Calib.) (Ye và Durrett, 2022)**: Một bộ hiệu chỉnh điều chỉnh xác suất của một dự đoán dựa trên điểm số dự đoán của nó.

5. **ReAct (Yao et al., 2022)**: Một khung suy luận-và-hành động sử dụng API Wikipedia bên ngoài. Đối với baseline này, chúng tôi sử dụng kết quả được báo cáo trong bài báo gốc, sử dụng mô hình PaLM (Chowdhery et al., 2022), có hiệu suất tương tự GPT-3.² Để thêm một góc nhìn hợp lý hơn, chúng tôi báo cáo cải thiện hiệu suất mà nó đạt được trên baseline CoT-SC.³

**Verify-and-Edit (VE)** Trong việc triển khai khung công tác VE, cùng một baseline nhất quán được sử dụng để ước lượng khi nào mô hình không chắc chắn. Như đã nêu trong §3.1, chúng tôi chỉnh sửa tất cả các trường hợp có điểm số tự nhất quán dưới ⌈n/2⌉, trong đó n là số đường được lấy mẫu. Sau đó, các câu hỏi xác minh được tạo ra bằng cách sử dụng thiết lập 2-shot⁴ với học trong ngữ cảnh. Các câu trả lời xác minh được tạo ra sử dụng cùng số lượng ví dụ trong việc sinh câu trả lời gốc và giải mã tham lam.

Để nghiên cứu tác động của các hệ thống truy xuất kiến thức lên kết quả, chúng tôi sử dụng bốn hệ thống:

1. **Wikipedia-API (wiki)**: Tìm kiếm các thực thể truy vấn và chọn các câu hàng đầu từ các trang Wikipedia của chúng.

2. **DrQA (Chen et al., 2017)**: Một mô hình QA miền mở được tiền huấn luyện kết hợp băm bigram, khớp TF-IDF, và một mô hình mạng nơ-ron hồi quy đa lớp. Chúng tôi chỉ sử dụng các ngữ cảnh được truy xuất từ nó.⁵

3. **Google**: Sử dụng kết quả tìm kiếm top-k được tạo ra bởi Google như các ngữ cảnh hỗ trợ. Kết quả này thú vị trong việc cung cấp khả năng kết hợp công cụ tìm kiếm và LLM.

4. **Dataset**: Chọn từ tập hợp các đoạn văn được cung cấp trong Adversarial HotpotQA và 2Wiki-MultihopQA, bao gồm các ngữ cảnh hỗ trợ thực tế và các đoạn văn gây nhiễu. Điều này tương tự như một thiết lập oracle, cung cấp một giới hạn trên của việc tăng hiệu suất, giả sử chúng ta có một hệ thống truy xuất tốt.

Đối với 1, 2, và 4, sau khi truy xuất, chúng tôi chọn 3 câu hàng đầu tương tự nhất với truy vấn được xếp hạng bởi mô hình Sentence BERT được tiền huấn luyện (Reimers và Gurevych, 2019) làm ngữ cảnh.

## 5 Kết quả và Phân tích

### 5.1 Sử dụng Self-Consistency: biết khi nào nó không biết

Đối với bước đầu tiên trong khung công tác Verify-and-Edit, tính nhất quán được sử dụng để đo lường độ tin cậy của mô hình trong một dự đoán. Phù hợp với các phát hiện từ Wang et al. (2022), chúng tôi giả thuyết rằng khi tính nhất quán thấp, mô hình không chắc chắn hơn và do đó có khả năng sinh ra các dự đoán không chính xác hơn. Để kiểm tra xem giả thuyết này có đúng không, chúng tôi vẽ các biểu đồ ước lượng mật độ kernel cho phân phối tính nhất quán trên bộ dữ liệu Adversarial HotpotQA.

Như được hiển thị trong Hình 2, các mẫu không chính xác cho thấy một phân phối tính nhất quán lệch trái, nơi hầu hết các dự đoán không chính xác có tính nhất quán thấp. Mặt khác, phân phối của các dự đoán chính xác cho thấy xu hướng lệch phải, nơi có rất ít mẫu không chính xác với tính nhất quán cao hơn. Điều này hiệu quả xác thực giả thuyết của chúng tôi.

Trong các thí nghiệm chính, chúng tôi sử dụng ⌈n/2⌉ như một ngưỡng đa số và chỉnh sửa tất cả các mẫu dưới ngưỡng đó, là 3. Để hiển thị các tác động của các ngưỡng khác nhau lên hiệu suất của khung công tác, chúng tôi cũng cung cấp một nghiên cứu ablation sau này.

### 5.2 Kết quả trên HotpotQA

Được báo cáo trong Bảng 1, chúng tôi quan sát thấy rằng CoT cải thiện trên thiết lập Standard few-shot. Mặt khác, CoT-SC không thể hiện sự cải thiện tốt trên baseline. Sử dụng bộ hiệu chỉnh từ Ye và Durrett (2022), AUC được cải thiện vì nó học cách hiệu chỉnh trọng số câu trả lời dựa trên các ngữ cảnh thực tế được cung cấp trong bộ dữ liệu.

Do đó, nó nên được so sánh với thiết lập cuối cùng của VE, nơi chúng tôi sử dụng kiến thức bộ dữ liệu. So sánh, bộ hiệu chỉnh dẫn đến AUC thấp hơn và không thể cải thiện độ chính xác vì nó không sinh ra các câu trả lời thay thế trong các thiết lập miền mở.

Sử dụng khung công tác Verify-and-Edit, các hệ thống truy xuất Wikipedia và DrQA có thể tạo ra sự cải thiện 4.5% và 4.8% tương ứng trên baseline, là 2x cải thiện EM cao nhất cho ReAct (1.7%). Khi chúng tôi kết hợp kết quả công cụ tìm kiếm từ Google vào khung công tác, EM tăng 6.5%, là 3.8x kết quả ReAct. Điều này cho thấy một phương pháp đầy hứa hẹn để kết hợp công cụ tìm kiếm và LLM, đây là một hướng phổ biến hiện nay. Công cụ tìm kiếm trả về kết quả thực tế, nhưng kém mạnh mẽ hơn trong các truy vấn đòi hỏi suy luận. Mặt khác, LLM mạnh mẽ trong suy luận và trừu tượng hóa nhưng có xu hướng sinh ra các câu phát biểu có vẻ hợp lý nhưng không chính xác (OpenAI-Blog, 2022; Zhao et al., 2023). Để kết hợp điều tốt nhất của cả hai thế giới, chúng tôi có thể sử dụng bộ nhớ dài của LLM, vì nhiều người dùng đã báo cáo rằng GPT có thể nhớ các đầu vào được đề cập trước đó trong cuộc đối thoại. Bằng cách cung cấp kết quả thực tế từ các công cụ tìm kiếm như việc làm mới bộ nhớ, GPT có thể sinh ra các dự đoán tốt hơn và thực tế hơn.

Sau đó, khi chúng tôi sử dụng các đoạn văn được tăng cường đối kháng được cung cấp trong bộ dữ liệu, mô hình có thể thể hiện EM rất cao (56.8%) và AUC (60.94) đồng thời. Thiết lập này cho thấy rằng, nếu chúng ta có một tập hợp các ngữ cảnh được nén cao và một hệ thống truy xuất gần như lý tưởng, khung công tác Verify-and-Edit có thể dẫn đến hiệu suất rất mạnh.

### 5.3 Kết quả trên 2WikiMultiHop

Như được hiển thị trong Bảng 2, phương pháp của chúng tôi thể hiện hiệu suất thậm chí mạnh hơn trên 2WikiMultiHop so với HotpotQA. Khung công tác Verify-and-Edit với truy xuất miền mở có thể tạo ra sự cải thiện độ chính xác cao, từ 3.4% đến 5.9%. Chọn từ các đoạn văn được cung cấp trong bộ dữ liệu, bao gồm các bằng chứng hỗ trợ và các đoạn văn không liên quan, sự cải thiện độ chính xác được tăng thêm lên 9.5%. Mặt khác, bộ hiệu chỉnh sử dụng các đoạn văn được cung cấp bộ dữ liệu nhưng vẫn tụt hậu so với tất cả các biến thể của khung công tác Verify-and-Edit của chúng tôi.

### 5.4 Kết quả trên xác minh sự thật

Kết quả trên bộ dữ liệu Fever được hiển thị trong Bảng 3. Vì việc suy luận được yêu cầu bởi bộ dữ liệu Fever ít đa bước hơn so với HotpotQA và 2Wiki-MultiHop, chúng tôi dự đoán rằng nó nên thể hiện sự cải thiện thấp hơn so với hai cái kia.

Trong bộ dữ liệu Fever, phương pháp hiệu chỉnh hoàn toàn thất bại, giảm xuống 33.7%: nó hiệu chỉnh điểm số dự đoán dựa trên ước lượng tính thực tế, được tạo ra bằng cách kiểm tra sự chồng chéo giữa đường suy luận và ngữ cảnh được cung cấp. Tuy nhiên, trong các bộ dữ liệu Xác minh Sự thật như vậy, không có ngữ cảnh được cung cấp. Do đó, chúng tôi hiệu chỉnh sử dụng tuyên bố gốc, dẫn đến hiệu suất kém. Nó cho thấy ở đây rằng một hạn chế của phương pháp hiệu chỉnh là nó chỉ áp dụng cho các trường hợp có ngữ cảnh liên quan được cung cấp.

Mặc dù tác vụ này không đòi hỏi nhiều suy luận, sử dụng khung công tác Verify-and-Edit, chúng tôi có thể quan sát các cải thiện nhất quán so với phương pháp baseline. Tương tự như trước, truy xuất Wikipedia có thể dẫn đến sự cải thiện lớn hơn so với DrQA, và tìm kiếm Google cải thiện thêm ở 1.9%.

So với phương pháp của chúng tôi, ReAct có thể thể hiện sự cải thiện lớn hơn trên Fever. Trước hết, đã được đề cập trước đó rằng Fever ít phù hợp cho khung công tác Verify-and-Edit vì nó đòi hỏi ít suy luận hơn để giải quyết tác vụ. Thứ hai, các prompt ReAct dài hơn nhiều so với các prompt của chúng tôi, đòi hỏi chi phí tính toán cao hơn.

### 5.5 Cân nhắc chi phí

Vì việc giảm chi phí là mối quan ngại chính khi tương tác với LLM, phương pháp của chúng tôi xem xét điều này và cố gắng giảm chi phí tính toán từ hai khía cạnh: Thứ nhất, Verify-and-Edit chỉ thực hiện chỉnh sửa cho các trường hợp được chọn, trong khi những cái khác chỉnh sửa mọi lúc. Cụ thể, chúng tôi chỉ sửa đổi khi mô hình không chắc chắn (được đánh giá bằng tính nhất quán), điều này xảy ra 40% thời gian. Như một so sánh, các phương pháp khác, như ReAct, truy xuất thông tin liên quan và chỉnh sửa cho từng trường hợp đơn lẻ, dẫn đến chi phí cao hơn. Thứ hai, Verify-and-Edit thiết kế các tác vụ tự nhiên và đàm thoại, chỉ yêu cầu một vài minh chứng và prompt ngắn để học. Ví dụ, các phương pháp khác thường học các lời gọi không tự nhiên, như các thẻ [thought] và [action] trong ReAct và các lời gọi API trong Toolformer (Schick et al., 2023). Do đó, LLM yêu cầu các prompt dài hơn, nhiều minh chứng hơn, hoặc thậm chí tinh chỉnh để học định dạng. Mặt khác, chúng tôi thiết kế các tác vụ Verify-and-Edit càng tự nhiên càng tốt, yêu cầu nỗ lực tối thiểu để học. Các tác vụ của chúng tôi chỉ bao gồm việc hỏi và trả lời câu hỏi, không có thẻ tổng hợp hoặc tác vụ nào cần được học. Như một so sánh, với API GPT-3, để chỉnh sửa một trường hợp Fever, Verify-and-Edit tốn $0.014, trong khi ReAct tốn $0.017.

### 5.6 Đánh giá các chuỗi suy luận bằng nghiên cứu con người

Để kiểm tra chặt chẽ tính trung thực của các chuỗi suy luận được sinh ra, chúng tôi cũng tiến hành một thí nghiệm nghiên cứu con người quy mô nhỏ. Trong thí nghiệm, hai tình nguyện viên con người được hiển thị 50 câu hỏi được chọn ngẫu nhiên với các chuỗi suy luận được sinh ra từ CoT-SC và Verify-and-Edit trên bộ dữ liệu HotpotQA. Sau đó họ được yêu cầu chọn cái nhất quán hơn về mặt thực tế. Các tình nguyện viên được khuyến khích sử dụng công cụ tìm kiếm như hỗ trợ. Một mô tả chi tiết về thiết lập được mô tả trong Phụ lục D.

Như được hiển thị trong Bảng 4, con người chọn các chuỗi suy luận được tạo ra bởi Verify-and-Edit là nhất quán hơn về mặt thực tế 53% thời gian, so với 17% cho baseline CoT-SC. Cohen κ ở 0.25, cho thấy sự đồng ý công bằng giữa hai người chú thích (McHugh, 2012). Các người chú thích sử dụng tìm kiếm Google như một công cụ hỗ trợ 100% thời gian, điều này cho thấy sự cần thiết của việc giới thiệu kiến thức bên ngoài.

Hơn nữa, chú thích con người trong trường hợp này đòi hỏi rất nhiều nỗ lực. Các người chú thích báo cáo trung bình 1.5 phút để xác thực một điểm dữ liệu. Do đó, việc tự động hóa quá trình Verify-and-Edit có lợi ích như một công cụ hỗ trợ để giảm lao động con người.

Để quan sát các tác động định tính của khung công tác Verify-and-Edit một cách chi tiết, chúng tôi cũng bao gồm một số ví dụ thú vị trong Phụ lục E, cho thấy hiệu quả của khung công tác trong việc sửa chữa các tuyên bố gốc.

### 5.7 Nghiên cứu Ablation: chỉnh sửa ở các ngưỡng nhất quán khác nhau

Trong khung công tác Verify-and-Edit, tham số siêu duy nhất cần chọn là ngưỡng nhất quán. Các ngưỡng tương tự cũng tồn tại trong ReAct (Yao et al., 2022), nơi phương pháp CoT→ReAct là sử dụng prompting kiểu ReAct khi "câu trả lời đa số trong n mẫu CoT-SC xảy ra ít hơn n/2 lần". Tuy nhiên, việc sử dụng đếm đa số ít chi tiết hơn so với việc sử dụng tính nhất quán gốc được công thức hóa với xác suất log. Do đó, chúng tôi sử dụng điểm số gốc được đề xuất bởi Wang et al. (2022), là xác suất câu trả lời không chuẩn hóa được biên hóa trên xác suất log của các lý luận.

Để bắt chước một ngưỡng bỏ phiếu đa số, chúng tôi chọn ⌈n/2⌉, trong đó n là số đường được lấy mẫu.

Để nghiên cứu tác động của việc điều chỉnh ngưỡng nhất quán lên khung công tác của chúng tôi, chúng tôi hiển thị kết quả ablation của Adversarial HotpotQA trong Hình 3. Khi ngưỡng tăng, độ chính xác đầu tiên tăng, đạt đỉnh gần ⌈n/2⌉, là 3, trước khi giảm. Điểm số AUC thể hiện xu hướng tương tự.

Như được hiển thị trong Hình 2, khi tính nhất quán lớn hơn đa số (⌈n/2⌉), thường có nhiều dự đoán chính xác hơn dự đoán không chính xác, và ngược lại. Do đó, khi chúng tôi tăng ngưỡng nhất quán từ 0 đến ⌈n/2⌉, nhiều mẫu không chắc chắn và có thể không chính xác đang được chỉnh sửa bằng cách giới thiệu kiến thức bên ngoài. Khi chúng tôi vượt quá ngưỡng lý tưởng ⌈n/2⌉, chúng tôi chủ yếu đang chỉnh sửa lại các mẫu chính xác, và tiếng ồn được giới thiệu có thể làm gián đoạn các chuỗi suy luận gốc.

Do đó, chúng tôi khuyến nghị một ngưỡng nhất quán ở ⌈n/2⌉ như một mức lý tưởng.

## 6 Kết luận

Trong bài báo này, chúng tôi giới thiệu khung công tác Verify-and-Edit cho việc trả lời câu hỏi miền mở. Đây là nỗ lực đầu tiên chỉnh sửa hậu kỳ các chuỗi suy luận kiểu CoT để có hiệu suất tác vụ cuối tốt hơn. Bằng cách kết hợp truy xuất kiến thức với suy luận, khung công tác chỉnh sửa CoT theo cách tự nhiên và đàm thoại, tăng cường tính thực tế của dự đoán. Kết hợp với tìm kiếm Google, khung công tác cũng cho thấy một hướng đầy hứa hẹn kết hợp khả năng sinh mở của LLM tiên tiến với các sự thật được cập nhật do công cụ tìm kiếm cung cấp.

## Hạn chế

Có một vài hạn chế đối với khung công tác hiện tại. Thứ nhất, Verify-and-Edit hoạt động tốt nhất cho các tác vụ trả lời câu hỏi miền mở đòi hỏi suy luận phức tạp. Các bộ dữ liệu ít phức tạp hơn hoặc các bộ dữ liệu thông thường không đòi hỏi truy xuất kiến thức có thể không dẫn đến sự cải thiện cao. Thứ hai, lý tưởng nhất là chỉnh sửa một nhóm chủ yếu các mẫu không chính xác, mà chúng tôi cố gắng chọn bằng cách sử dụng tính nhất quán. Do đó, phương pháp của chúng tôi phụ thuộc vào hiệu suất của phương pháp nhất quán và khả năng của nó để tách các dự đoán đúng và sai. Thường thì, nó có thể thể hiện sự cải thiện lớn hơn với một tập hợp ví dụ thách thức hơn.

Để giải quyết những hạn chế này, chúng tôi dự định làm việc để giảm tiếng ồn được mang vào trong giai đoạn chỉnh sửa lý luận và sử dụng nhiều tài nguyên kiến thức hơn, như cơ sở kiến thức, như một sự tiếp nối.

## Tuyên bố Đạo đức

Khung công tác Verify-and-Edit có thể giảm thiểu các mối quan ngại đạo đức tiềm ẩn của việc sinh LLM xung quanh ảo giác và chi tiết không thực tế. Một số mối quan ngại tồn tại bao gồm: (1) Vì khung công tác sử dụng google như một trong các phương pháp truy xuất, nó có thể truy xuất thông tin độc hại tiềm ẩn tồn tại trong kết quả tìm kiếm google. (2) Vì khung công tác sử dụng GPT3 làm backbone, nó có thể gặp phải các mối quan ngại đạo đức hiện có của GPT3, như phản hồi các truy vấn độc hại hoặc thể hiện hành vi thiên vị.

Đối với truy xuất kiến thức, chúng tôi đã sử dụng corpus Wikipedia và kết quả tìm kiếm google. Quyền được cấp để sao chép, phân phối và/hoặc sửa đổi văn bản của Wikipedia theo các điều khoản của Giấy phép Creative Commons Attribution-ShareAlike 3.0 Unported. Đối với kết quả tìm kiếm google, việc scraping dữ liệu có thể truy cập công khai được coi là hợp pháp bởi tòa án phúc thẩm Hoa Kỳ.

## 7 Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Quỹ Nghiên cứu Quốc gia, Singapore dưới Chương trình AI Singapore (Giải thưởng AISG Số: AISG-PhD/2021-01-001[T]).
