# 2311.03033.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2311.03033.pdf
# Kích thước tệp: 242705 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2311.03033v1  [cs.LG]  6 Nov 2023Vượt qua từ ngữ: Một khung toán học để diễn giải
các mô hình ngôn ngữ lớn
Javier González và Aditya V. Nori
Microsoft Research
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) là những công cụ AI mạnh mẽ có thể tạo ra và hiểu
văn bản ngôn ngữ tự nhiên cũng như các thông tin phức tạp khác. Tuy nhiên, lĩnh vực này thiếu một khung
toán học để mô tả, so sánh và cải thiện các LLM một cách có hệ thống. Chúng tôi đề xuất
khung Hexa làm rõ các thuật ngữ và khái niệm chính trong nghiên cứu LLM, chẳng hạn như ảo giác, căn chỉnh,
tự xác minh và suy luận chuỗi suy nghĩ. Khung Hex cung cấp một cách chính xác và
nhất quán để đặc trưng hóa các LLM, xác định điểm mạnh và điểm yếu của chúng, và tích hợp
các phát hiện mới. Sử dụng Hex, chúng tôi phân biệt suy luận chuỗi suy nghĩ với việc nhắc nhở chuỗi suy nghĩ
và thiết lập các điều kiện mà theo đó chúng tương đương. Sự phân biệt này làm rõ
các giả định cơ bản đằng sau việc nhắc nhở chuỗi suy nghĩ và ý nghĩa của nó đối với các phương pháp
sử dụng nó, chẳng hạn như tự xác minh và lập trình nhắc nhở.
Mục tiêu của chúng tôi là cung cấp một khung chính thức cho các LLM có thể giúp cả nhà nghiên cứu và
người thực hành khám phá những khả năng mới cho AI tạo sinh. Chúng tôi không khẳng định có một
giải pháp dứt khoát, mà đúng hơn là một công cụ để mở ra những con đường nghiên cứu mới. Chúng tôi lập luận rằng các định nghĩa chính thức và kết quả của chúng tôi là rất quan trọng để thúc đẩy cuộc thảo luận
về cách xây dựng các hệ thống AI tạo sinh an toàn, đáng tin cậy, công bằng và mạnh mẽ, đặc biệt
trong các lĩnh vực như chăm sóc sức khỏe và kỹ thuật phần mềm.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) là những công cụ mạnh mẽ có thể tạo ra phản hồi văn bản cho nhiều loại
nhắc nhở đầu vào khác nhau, chẳng hạn như truy vấn hoặc lệnh. Để tương tác với một LLM, người dùng cung cấp một nhắc nhở
và nhận được một phản hồi từ mô hình, dựa trên các thuật toán nội bộ của nó. Người dùng sau đó
đánh giá phản hồi và quyết định liệu nó có trả lời nhắc nhở một cách thỏa đáng hay không. Quá trình này có thể
được lặp lại nhiều lần cho đến khi người dùng đạt được kết quả mong muốn (hoặc từ bỏ).
Các LLM hiện đại như PaLM [CND+22], GPT-4 [Ope23] hoặc LLaMA [TLI+23], có thể
thể hiện cả những hành vi đáng chú ý và bối rối, tùy thuộc vào nhiệm vụ. Peter Lee nắm bắt được
nghịch lý này bằng cách nói rằng chúng vừa "thông minh hơn và ngu ngốc hơn bạn" trong toán học, thống kê và
logic [Sch23]. Điều gì gây ra sự khác biệt này? Nghiên cứu chủ yếu tập trung vào việc kiểm tra các LLM trên
các nhiệm vụ khác nhau để người dùng có thể hiểu được về hành vi của chúng. Đối với GPT-4, một bộ sưu tập lớn các
hành vi đã được nghiên cứu trên nhiều nhiệm vụ [BCE+23]. Các nỗ lực đánh giá chuẩn đã được tiến hành
trong nhiều lĩnh vực khác nhau như khám phá nhân quả [KNST23], tóm tắt [ZLD+23] và suy luận
[SPH+23]. Những hướng nghiên cứu này là hợp lý, do hầu hết các LLM là những hộp đen mà chúng ta
không thể dễ dàng kiểm tra, diễn giải hoặc giải thích.

--- TRANG 2 ---
Trong bài báo này, chúng tôi đưa ra một góc nhìn mới về việc diễn giải các LLM và trình bày một
khung toán học chính thức hóa và tổng quát hóa những gì đã được biết về các LLM. Để thúc đẩy nghiên cứu LLM,
chúng tôi theo đuổi hai mục tiêu: Thứ nhất, chúng tôi định nghĩa các khái niệm LLM chính như căn chỉnh [SJH+23],
ảo giác [JLF+23] và suy luận chuỗi suy nghĩ [WWS+23] bằng khung toán học của chúng tôi.
Thứ hai, chúng tôi cung cấp một công cụ suy luận tạo điều kiện hiểu biết chung về nghiên cứu LLM
hiện có và tạo cơ sở để khám phá những câu hỏi và thách thức mới. Khung của chúng tôi, được gọi là Hex,
dựa trên các biểu đồ giao hoán liên kết các mức độ trừu tượng và tính toán khác nhau trong
các LLM. Bản thân khung này không nhằm mục đích cung cấp tất cả các câu trả lời mà nó đại diện cho một khung
có thể được sử dụng bởi các nhà nghiên cứu LLM để chia sẻ, định vị và suy luận về các phát hiện của họ.
Chúng tôi cho thấy cách Hex có thể nắm bắt bản chất của các LLM như các máy thực thi trừu tượng sử dụng
ngôn ngữ tự nhiên làm giao diện, và cách nó có thể tiết lộ các giả định và ý nghĩa của nhiều
phương pháp sử dụng LLM, chẳng hạn như tự xác minh và lập trình nhắc nhở. Chúng tôi cũng cho thấy cách Hex
có thể chính thức hóa và đánh giá đầu ra của các LLM bằng cách sử dụng các số liệu khoảng cách và quan hệ tương thích.
Mục tiêu của chúng tôi là cung cấp một khung chính thức cho các LLM có thể giúp cả nhà nghiên cứu và người
thực hành khám phá những khả năng mới cho AI tạo sinh. Chúng tôi không khẳng định có một giải pháp dứt khoát,
mà đúng hơn là một công cụ để mở ra những con đường nghiên cứu mới. Chúng tôi lập luận rằng các định nghĩa chính thức và
kết quả của chúng tôi là rất quan trọng để thúc đẩy cuộc thảo luận về cách xây dựng các hệ thống AI tạo sinh an toàn,
đáng tin cậy, công bằng và mạnh mẽ, đặc biệt trong các lĩnh vực như chăm sóc sức khỏe và kỹ thuật phần mềm.
Phần còn lại của bài báo được tổ chức như sau. Mục 2 giới thiệu khung Hex và các
định nghĩa cơ bản của nó. Mục 3 cho thấy cách khung Hex có thể chính thức hóa các khái niệm LLM thông thường,
chẳng hạn như ảo giác, căn chỉnh và suy luận chuỗi suy nghĩ. Mục 4 thảo luận về các điều kiện
khi việc nhắc nhở chuỗi suy nghĩ gợi ra suy luận chuỗi suy nghĩ, và ý nghĩa rộng lớn hơn của chúng
đối với các phương pháp LLM và cách chúng được sử dụng trong thực tế. Mục 5 kết luận bài báo và
phác thảo công việc tương lai.

2 Khung toán học
Các LLM là máy thực thi trừu tượng sử dụng ngôn ngữ tự nhiên làm giao diện. Với một
nhắc nhở đầu vào nắm bắt ý định của người dùng, một LLM tạo ra một câu trả lời đầu ra dưới dạng
văn bản được trình bày cho người dùng. Nhắc nhở nhằm giải quyết một vấn đề, chẳng hạn như một phép toán
học, một bản tóm tắt, hoặc việc tạo ra một loại văn bản nhất định. Nhắc nhở luôn thể hiện
một ý định thực hiện một số thao tác dựa trên một số thông tin được cung cấp. LLM,
hoạt động như một hộp đen, sử dụng bộ nhớ nội bộ có được tại thời điểm đào tạo để cung cấp một câu trả lời
bằng cách lấy mẫu từ một số phân bố xác suất có điều kiện trên nhắc nhở đầu vào1.
Quá trình này đã trở nên cực kỳ mạnh mẽ trong thế hệ LLM mới nhất, tạo ra một bộ
thuật ngữ mới cho nghiên cứu và thực hành AI. Căn chỉnh, ảo giác, suy luận chuỗi suy nghĩ, tự
xác minh và lập trình nhắc nhở là một số khái niệm mà các nhà nghiên cứu và người thực hành AI
sử dụng thường xuyên – công việc của chúng tôi cho phép chúng tôi định nghĩa chính thức các khái niệm này bằng cách sử dụng một khung toán học
thống nhất (Mục 3). Chúng tôi giới thiệu các định nghĩa cần thiết cho khung của chúng tôi tiếp theo.

Trạng thái cụ thể: Một trạng thái cụ thể là một hàm gán giá trị từ một miền cụ thể cho
một tập hợp (hữu hạn) các biến. Chúng tôi sử dụng V để biểu thị tập hợp các biến, và C để biểu thị
miền cụ thể. Chúng tôi viết σ:V → C để chỉ ra rằng σ là một trạng thái cụ thể. Ví dụ,
giả sử V={x,y} và C=Z, tập hợp các số nguyên. Thì, một trạng thái cụ thể có thể là
1Điều này được tính toán bằng cách chọn từ tiếp theo phù hợp nhất với văn bản trước đó, và lặp lại quá trình cho mỗi từ.

--- TRANG 3 ---
σ={x↦1,y↦2}, có nghĩa là σ(x) = 1 và σ(y) = 2. Tập hợp tất cả các trạng thái cụ thể
trên V và C được ký hiệu là 2V→C.

Vấn đề: Cho q: 2V→C→2V→C là một truy vấn, và cho σ là một trạng thái cụ thể. Một cách không chính thức, truy vấn
q thể hiện những gì người dùng muốn tính toán trên một trạng thái σ. Ví dụ, q có thể chọn một số
biến, áp dụng một số thao tác, hoặc lọc một số điều kiện. Một vấn đề được định nghĩa bởi
cặp truy vấn-trạng thái (q,σ).

Nhắc nhở: Một nhắc nhở là một biểu hiện ngôn ngữ tự nhiên của một vấn đề (q,σ) mà một LLM có thể giải quyết2.
Một nhắc nhở (q,σ) gồm hai chuỗi: một chuỗi truy vấn q đại diện cho truy vấn q bằng lời,
và một chuỗi trạng thái σ mô tả trạng thái cụ thể σ bằng lời. Chúng tôi ký hiệu T là tập hợp
tất cả các chuỗi.

LLM: Với một nhắc nhở (q,σ) làm đầu vào, một LLM M diễn giải điều này để tạo ra một đầu ra
ngôn ngữ tự nhiên hoặc chuỗi.

Trạng thái trừu tượng: Một chuỗi trạng thái σ có ý nghĩa có thể được nắm bắt bởi một vectơ, đó là một
trạng thái trừu tượng ˆσ hoặc một embedding. Một LLM làm việc với những vectơ này để thao tác
chuỗi trạng thái. Chúng tôi ký hiệu A là tập hợp tất cả các trạng thái trừu tượng.

Bản đồ trừu tượng hóa: Một bản đồ trừu tượng hóa α là một hàm biến đổi một trạng thái cụ thể σ thành
một trạng thái trừu tượng ˆσ. Hàm α gồm hai hàm con: (i) αc, chuyển đổi
trạng thái cụ thể σ thành một chuỗi trạng thái ¯σ (điều này sẽ là một phần của quá trình thiết kế nhắc nhở) và
(ii) αa, mã hóa chuỗi trạng thái ¯σ thành một trạng thái trừu tượng ˆσ (đây là bộ mã hóa LLM). Nói cách khác,
α(σ) = (αa◦αc)(σ).

Bản đồ cụ thể hóa: một bản đồ cụ thể hóa γ cụ thể hóa một trạng thái trừu tượng ˆσ bằng cách tạo ra một
trạng thái cụ thể σ. Hàm γ gồm hai bước: (i) γa, giải mã trạng thái trừu tượng
ˆσ thành một chuỗi trạng thái ¯σ bằng cách sử dụng một LLM (bộ giải mã LLM), và (ii) γc, diễn giải
chuỗi trạng thái ¯σ như một mô tả của trạng thái đầu ra cụ thể (diễn giải của con người về
đầu ra LLM). Nói cách khác, γ(ˆσ) = (γc◦γa)(ˆσ).

Hình 1 cho thấy biểu đồ Hex3 cho một LLM M và một vấn đề (q,σ), trong đó q là một truy vấn và
σ là một trạng thái cụ thể. Các góc của biểu đồ là các trạng thái và các cạnh dọc và ngang
kết nối các mức độ trừu tượng khác nhau. Các cạnh dọc dựa trên các bản đồ trừu tượng hóa αc
và αa, và các bản đồ cụ thể hóa γa và γc. Các cạnh ngang dựa trên các hàm số
Λq và Λˆq, tính toán truy vấn q hoặc đối tác trừu tượng ˆq của nó trên các trạng thái tương ứng.
Những hàm số này được định nghĩa như sau:

Λq(σ)def=q(σ) áp dụng truy vấn q lên trạng thái cụ thể σ.
Λˆq(ˆσ)def= ˆq(ˆσ) tính toán truy vấn trừu tượng ˆq (đây là diễn giải của LLM M về chuỗi truy vấn
q) trên trạng thái trừu tượng ˆσ=α(σ).

Một biểu đồ Hex đại diện cho các cách hoặc đường dẫn khác nhau để giải quyết một vấn đề (q,σi). Mỗi đường dẫn bắt đầu
từ một trạng thái cụ thể ban đầu σi và kết thúc tại một trạng thái cụ thể cuối cùng σi+1. Biểu đồ có tính giao hoán
khi và chỉ khi mọi đường dẫn đều dẫn đến một σi+1 tương thích. Nói cách khác, khi và chỉ khi Λq(σi)≡

2Cả hai yếu tố có thể được trộn lẫn trong một số nhắc nhở. Mặc dù lý thuyết của chúng tôi là tổng quát, chúng tôi tách biệt chúng để đơn giản hóa
trình bày của chúng tôi.
3Một biểu đồ giao hoán với sáu cạnh.

--- TRANG 4 ---
ˆσi∈ A ˆσi+1∈ A
σi∈ T σi+1∈ T
σi∈2V→Cσi+1∈2V→Cαcαa γa
γc
ΛqΛˆq

Hình 1: Biểu đồ Hex cho một vấn đề (q,σi).

(γc◦γa◦Λˆq◦αa◦αc)(σi), trong đó ≡ là một quan hệ tương thích phù hợp. Mục 3.1 đưa ra một số
ví dụ về quan hệ tương thích, chẳng hạn như đẳng thức.
Đường dẫn bắt đầu với chuỗi trạng thái σi và kết thúc ở chuỗi trạng thái σi+1 theo dõi việc thực thi
của LLM M. Các trạng thái trừu tượng ˆσi và ˆσi+1 và hàm Λˆq không thể quan sát được khi
LLM là một hộp đen.

2.1 Ví dụ Hex
Trong mục này, chúng tôi cho thấy cách các yếu tố Hex hoạt động cùng nhau thông qua hai ví dụ. Một ví dụ là
một bài toán số học cần tư duy logic, và ví dụ kia là một bài toán tạo mã cần
cả logic và một chút sáng tạo.

2.1.1 Bài toán số học đơn giản
Vấn đề: Giải z=x+y với x= 12, y= 13.
Thể hiện Hex cho vấn đề này được định nghĩa như sau.

• Trạng thái cụ thể: Tập hợp các biến V={x,y,z}, trong đó x, y và z là các biến của
vấn đề. Miền cụ thể C là tập hợp các số nguyên Z. Trạng thái ban đầu cụ thể
σi={x↦12,y↦13,z↦⊥,}, trong đó ⊥ đại diện cho một giá trị không xác định.

• Vấn đề: Truy vấn q=λσ . σ(x)+σ(y), và vấn đề là cặp (q,σi).

• Hàm số Λq(σi) tính toán trạng thái đầu ra cụ thể σi+1.

• σi+1={x↦12,y↦13,z↦25}. Đây là câu trả lời đúng mà người dùng mong đợi từ
LLM, nói cách khác, là sự thật cơ bản.

• Bản đồ trừu tượng hóa: Hàm αc ánh xạ trạng thái đầu vào cụ thể σi thành một chuỗi trạng thái σi
được mã hóa trong nhắc nhở văn bản (hiển thị bên dưới). LLM sau đó diễn giải chuỗi trạng thái
σi (được đại diện bởi chuỗi con "x = 12, y = 13") và tạo ra ˆσi=αc(σi) tương ứng
với một trạng thái LLM nội bộ không thể quan sát được.

--- TRANG 5 ---
• Hàm số Λˆq là một hàm số chức năng không xác định, tính toán truy vấn trừu tượng ˆq, được chỉ định bởi
chuỗi truy vấn q="Giải z = x+y" của nhắc nhở hiển thị bên dưới.

Bài toán số học đơn giản
Nhắc nhở:
final_answer = GPT4("Giải z = x+y, với x = 12, y = 13.")
Câu trả lời:
Giá trị của z là 25.

• Đầu ra LLM cho Nhắc nhở là "Giá trị của z là 25.", và điều này được đại diện bởi
chuỗi trạng thái σi+1.

• Bản đồ cụ thể hóa: Hàm cụ thể hóa γc diễn giải chuỗi trạng thái σi+1 như một trạng thái cụ thể
{x↦12,y↦13,z↦25} bằng với trạng thái cụ thể sự thật cơ bản σi+1. Điều này
dựa vào việc LLM tính toán câu trả lời đúng, có nghĩa là biểu đồ Hex
tương ứng có tính giao hoán (và do đó, có sự căn chỉnh).

Ví dụ này có một giải pháp duy nhất và chính xác, vì vậy chúng ta có thể dễ dàng kiểm tra rằng biểu đồ Hex
có tính giao hoán. Tuy nhiên, như chúng ta sẽ thấy sau này, điều này không phải lúc nào cũng đúng.

2.1.2 Vẽ phác một con kỳ lân
Vấn đề: Viết mã TikZ để vẽ một con kỳ lân.
Thể hiện Hex cho vấn đề này được định nghĩa như sau.

• Trạng thái cụ thể: Chúng tôi sử dụng một tập hợp các biến V={x,body,legs,head,tail,horn,m} để đại diện
cho thông tin cần thiết để vẽ một con vật bằng TikZ, đó là một ngôn ngữ để tạo đồ họa
trong LaTeX. Biến x chứa tên của con vật chúng ta muốn vẽ (giá trị chuỗi), chẳng hạn
như kỳ lân, và các biến body, legs, head, tail, horn chứa mã TikZ để vẽ từng
phần của con vật, chẳng hạn như một hình tròn, một đường thẳng, hoặc một đường cong. Biến m là một biến slack
có thể được sử dụng để lưu trữ bất kỳ thông tin hoặc mã bổ sung nào không được nắm bắt bởi các
biến khác. Chúng ta bắt đầu với một trạng thái ban đầu σi, trong đó x được gán giá trị "kỳ lân" và tất cả
các biến khác được gán giá trị không xác định ⊥, có nghĩa là chúng ta chưa có bất kỳ mã nào
để vẽ kỳ lân.

• Vấn đề: Truy vấn q là một trình tạo mã TikZ nhận σi làm đầu vào vẽ một biểu đồ của
σi(x), và vấn đề là cặp (q,σi).

• Hàm số Λq(σi) tạo ra trạng thái đầu ra cụ thể σi+1. Cụ thể, σi+1={x↦
"kỳ lân", body↦mã cho thân, legs↦mã cho chân, head↦mã cho đầu,
tail↦mã cho đuôi, horn↦mã cho sừng, m↦⊥}. Đây là câu trả lời mong đợi cho
người dùng, hoặc sự thật cơ bản, mà chúng tôi định nghĩa là bất kỳ mã nào phác họa kỳ lân. Tuy nhiên,
có nhiều cách có thể để viết mã như vậy. Trong Mục 3.2, chúng tôi giải thích cách chúng tôi đo lường
chất lượng của các câu trả lời của LLM.

--- TRANG 6 ---
• Bản đồ trừu tượng hóa: Chúng tôi sử dụng hàm αc để biến trạng thái đầu vào cụ thể σi thành một chuỗi trạng thái
σi mà chúng tôi có thể bao gồm trong nhắc nhở văn bản (hiển thị bên dưới). Chuỗi trạng thái là σi="kỳ lân"
mà LLM phải diễn giải. LLM sau đó tạo ra một trạng thái nội bộ ˆσi=αc(σi) mà chúng ta
không thể quan sát.

• Chúng tôi cũng có một hàm số chức năng không xác định Λˆq nhận chuỗi q="vẽ trong Tikz" từ
nhắc nhở và tính toán truy vấn trừu tượng ˆq.

Nhắc nhở:
answer = GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo biểu đồ:
vẽ một con kỳ lân.")

Câu trả lời:
\begin{tikzpicture}
% Thân
\draw[fill=gray!50] (0, 0) circle (1cm);
\draw[fill=gray!50] (1.5, 0) circle (1cm);
\draw (1, 0) ellipse (0.7cm and 1.5cm);
% Chân
\draw (0.4, -1) -- (0.2, -2.5);
\draw (0.6, -1) -- (0.8, -2.5);
\draw (1.9, -1) -- (1.7, -2.5);
\draw (2.1, -1) -- (2.3, -2.5);
% Đầu
\draw[fill=gray!50] (2.7, 1.4) circle (0.7cm);
\draw (2.3, 1.5) -- (2.1, 1.2);
% Đuôi
\draw (0.0, 0.7) to[out=120, in=30] (-1, 1.2);
% Sừng
\draw[thick] (2.7, 2.05) -- (2.9, 3);
\end{tikzpicture}

Kết quả sau khi biên dịch mã được tạo:

• Đầu ra LLM cho Nhắc nhở là "begin{tikzpicture}... end{tikzpicture}", và đây
là chuỗi trạng thái σi+1.

--- TRANG 7 ---
• Bản đồ cụ thể hóa: Hàm cụ thể hóa γc diễn giải chuỗi trạng thái σi+1 như một trạng thái cụ thể.
Trong ví dụ này, chuỗi trạng thái là một đoạn mã, vì vậy chúng ta có thể biên dịch nó để xem nó
làm gì. Mã được biên dịch thực sự vẽ một hình dạng giống một con kỳ lân.

Không giống như ví dụ số học, vấn đề này có nhiều giải pháp và quan hệ làm cho
biểu đồ Hex có tính giao hoán phụ thuộc vào tình huống. Ví dụ, các đánh giá của con người khác nhau
có thể có các tiêu chuẩn khác nhau về mức độ hình dạng giống một con kỳ lân. Những người khác có thể quan tâm nhiều hơn
về độ rõ ràng của mã.

3 Khái niệm LLM với Hex
Mục này cho thấy cách khung Hex có thể chính thức hóa các khái niệm LLM thông thường.

3.1 Ảo giác và căn chỉnh sai
Văn học LLM thường sử dụng các thuật ngữ ảo giác và căn chỉnh sai để mô tả hai vấn đề
thường gặp với các mô hình này. Ảo giác có nghĩa là các LLM tạo ra các phản hồi sai hoặc không liên quan
cho các nhắc nhở, nhưng làm cho chúng nghe có vẻ hợp lý hoặc mạch lạc [JLF+23]. Điều này có thể nguy hiểm khi
các LLM được sử dụng trong các lĩnh vực quan trọng nơi độ chính xác và an toàn là quan trọng [MLC+23]. Căn chỉnh
có nghĩa là các LLM hành động phù hợp với ý định của người dùng [SJH+23]. Các LLM
căn chỉnh sai hành động khác với những gì người dùng muốn. Điều này cũng có thể gây hại, chẳng hạn như đưa ra
câu trả lời sai, tạo ra đầu ra thiên vị hoặc kết quả phân biệt đối xử [WWLS23]. Căn chỉnh bao gồm
việc điều chỉnh các LLM để khuyến khích các hành vi mong muốn và ngăn cản những hành vi không mong muốn.

Chúng tôi sử dụng biểu đồ Hex để thảo luận về các thuật ngữ ảo giác và căn chỉnh sai, đó là những
thuật ngữ chung trong văn học. Với một vấn đề (q,σ), chúng ta cũng cần một quan hệ tương thích ≡ để so sánh
sự tương đương của hai trạng thái cụ thể. Ảo giác và căn chỉnh sai xảy ra khi biểu đồ Hex
cho (q,σ) không có tính giao hoán, có nghĩa là các đường dẫn từ σ đến trạng thái cụ thể đầu ra
không tạo ra các trạng thái tương đương dưới ≡. Một cách chính thức, điều này có nghĩa là quan hệ tương thích
(γ◦Λˆq◦α)(σ)≡Λq(σ), trong đó (γ◦Λˆq◦α)(σ) là câu trả lời được tạo bởi LLM, không thỏa mãn.

Chúng ta muốn phát hiện ảo giác hoặc căn chỉnh sai bằng cách so sánh các trạng thái, vì vậy chúng ta cần một
khái niệm tương đương phù hợp (≡) nắm bắt loại lỗi mà chúng ta quan tâm. Ví dụ, xem xét
bài toán số học đơn giản trong Mục 2.1.1, trong đó câu trả lời đúng là {x↦12,y↦13,z↦
25,w↦⊥}. Nếu LLM đầu ra "Tổng của x và y là 29", điều này tương ứng với trạng thái
{x↦12,y↦13,z↦29,w↦⊥} dưới hàm cụ thể hóa γ. Trạng thái này là một ảo giác,
vì nó không khớp với câu trả lời đúng. Một cách có thể để định nghĩa quan hệ tương đương
là so sánh các giá trị của các biến trong hai trạng thái cụ thể σ1 và σ2. Chúng ta nói rằng σ1 và σ2 là
tương đương, nếu chúng có cùng giá trị cho mọi biến trong V. Một cách chính thức, (σ1≡σ2)⇐⇒ ∀x∈
V. σ1(x) = σ2(x). Theo định nghĩa này, trạng thái duy nhất tương đương với {x↦12,y↦13,z↦
25,w↦⊥} là chính nó.

Tuy nhiên, giả sử LLM đầu ra "Tổng của x và y là một con hổ". Câu trả lời này sai
và cũng căn chỉnh sai, vì nó chứa một đầu ra vô nghĩa không phải là một số nguyên. Trong trường hợp này,
cách duy nhất để hai trạng thái được căn chỉnh là nếu mọi biến trong phạm vi có giá trị số nguyên.
Tức là, (σ1≡σ2)⇐⇒ ∀x∈ V.typeof(σ1(x)) = typeof(σ2(x)) = int.

Để đánh giá một LLM thực hiện một nhiệm vụ tốt như thế nào, chúng ta cần đo lường mức độ ảo giác
hoặc căn chỉnh sai của nó. Chúng ta có thể làm điều đó như thế nào? Chúng tôi thảo luận về điều này trong mục tiếp theo.

--- TRANG 8 ---
3.2 Đánh giá đầu ra của một LLM
Với một vấn đề (q,σ), đầu ra của một LLM M được định nghĩa bởi chuỗi trạng thái σ= (γa◦Λˆq◦α)(σ).
Áp dụng hàm γc cho σ tạo ra trạng thái cụ thể γc(σ). Quan hệ tương thích
được giới thiệu trong Mục 3.1 là rất quan trọng để xác định liệu biểu đồ Hex cho (q,σ) có
tính giao hoán hay không. Nếu không, một câu hỏi tự nhiên để hỏi là câu trả lời của LLM đó lệch bao nhiều so với câu trả lời
sẽ làm cho biểu đồ Hex có tính giao hoán.

Chúng ta có thể đánh giá một LLM giải quyết một vấn đề tốt như thế nào bằng cách so sánh trạng thái đầu ra của nó, σ, với một
trạng thái tham chiếu đại diện cho câu trả lời đúng, Λq(σ). Để làm điều này, chúng ta cần một số liệu khoảng cách
∆ : 2V→C×2V→C→R+ định lượng mức độ khác biệt của hai trạng thái. Việc chọn ∆ phụ thuộc vào
tình huống vấn đề, nhưng nó phải thỏa mãn một điều kiện nhất quán: biểu đồ Hex phải có tính giao hoán,
có nghĩa là trạng thái cụ thể có được từ đầu ra của LLM, γc(σ), phải có khoảng cách bằng không
từ trạng thái sự thật cơ bản tham chiếu, Λq(σ).

Ví dụ, giả sử chúng ta đang giải quyết vấn đề cộng 12 + 13. Nếu đầu ra của LLM là σ
và chúng ta áp dụng γc cho nó, chúng ta có γc(σ) = (12,13,19)T (đây là trạng thái vector hóa). Đây là trạng thái cụ thể
đại diện cho câu trả lời của LLM. Tuy nhiên, câu trả lời đúng là 25, mà chúng ta có thể có được bằng cách
áp dụng Λq cho đầu vào σ. Điều này cho chúng ta trạng thái sự thật cơ bản tham chiếu Λq(σ) = (12,13,25)T.
Để đo lường câu trả lời của LLM cách xa câu trả lời đúng bao nhiều, nếu chúng ta sử dụng chuẩn L1 làm
hàm khoảng cách, chúng ta có ∆(γc(σ),Λq(σ)) = |19−25| = 6. Điều này có nghĩa là câu trả lời của LLM lệch
6 đơn vị. Biểu đồ Hex không có tính giao hoán trong trường hợp này, vì γc(σ) ≠ Λq(σ).

Một số vấn đề cần các phương pháp khoảng cách phức tạp để đo lường biểu đồ gần như có tính giao hoán như thế nào. Điều này khó khăn khi có nhiều giải pháp hợp lệ cho sự thật cơ bản Λq(σ), chẳng hạn như
trong tạo mã nơi các giải pháp khác nhau đều có thể chấp nhận được. Người dùng LLM cần định nghĩa
những số liệu này để nắm bắt các thuộc tính phù hợp cho những vấn đề này.

4 Chuỗi suy nghĩ
Chuỗi suy nghĩ (CoT) [WWS+23] là một phương pháp để cải thiện khả năng của các LLM thực hiện
nhiều nhiệm vụ suy luận khác nhau, chẳng hạn như trả lời câu hỏi, giải quyết bài toán, hoặc tạo giải thích.
Ý tưởng là một số vấn đề quá phức tạp hoặc trừu tượng để LLM giải quyết trực tiếp,
và nó có thể được hưởng lợi từ việc chia chúng thành các vấn đề con đơn giản hơn hoặc cụ thể hơn. Để làm
điều này, phương pháp bao gồm việc nhắc nhở LLM tạo ra các bước trung gian dẫn đến câu trả lời
cuối cùng, và sử dụng những bước này làm ngữ cảnh hoặc phản hồi bổ sung cho bước tiếp theo.

Ví dụ, hãy tưởng tượng vẽ một hình lục giác với một hình tròn bên trong nó, và một hình vuông bên trong hình tròn.
Các bước (suy luận CoT) là:
1. Vẽ một hình lục giác.
2. Vẽ hình tròn bên trong hình lục giác.
3. Vẽ một hình vuông bên trong hình tròn.

Phương pháp suy luận CoT có thể giúp mô hình tránh lỗi, khoảng trống, hoặc không nhất quán trong
suy luận của nó, và tạo ra đầu ra mạch lạc và minh bạch hơn.

Để chính thức hóa suy luận CoT, chúng tôi sử dụng khung Hex. Xem xét một vấn đề (q,σi), trong đó q là
một truy vấn và σi là một trạng thái đầu vào cụ thể. Chúng ta có thể chia vấn đề này thành hai vấn đề con (q1,σ1)
và (q2,σ2), sao cho phép biến đổi truy vấn Λq giống như áp dụng Λq1 trước rồi Λq2,
với σi=σ1 và σ2= Λq1(σ1), nói cách khác, Λq(σi) = (Λq2◦Λq1)(σi).

--- TRANG 9 ---
ˆσ1 ˆσ2 ˆσ3 ˆσ4
σ1 σ2 σ3 σ4
σi σ2 σ2 σi+1Λq1Λˆq1
Λq2Λˆq2

Hình 2: Suy luận chuỗi suy nghĩ.

ˆσ1 ˆσ2
σ1 σ2
σi σ2 σi+1Λq1Λˆq1.q2
Λq2

Hình 3: Nhắc nhở chuỗi suy nghĩ.

Hình 2 minh họa biểu đồ Hex cho suy luận CoT, kết hợp các biểu đồ Hex cho
hai vấn đề con: (q1,σ1) và (q2,σ2), trong đó σi=σ1.

Bổ đề 1. Giả sử chúng ta có một vấn đề (q,σi) có thể được giải quyết bằng cách chia nó thành hai vấn đề con
(q1,σ1) và (q2,σ2), sao cho Λ(σi) = (Λq2◦Λq1)(σi). Giả sử cũng rằng Hình 2 cho thấy cách
sử dụng phương pháp suy luận CoT để giải quyết (q,σi). Thì suy luận CoT hợp lệ khi và chỉ khi mọi
cách có thể biến đổi σi thành Λq(σi) trong biểu đồ đều cho kết quả giống nhau, tức là, biểu đồ
có tính giao hoán.

Chứng minh. Suy ra từ thực tế rằng tính giao hoán ngụ ý không có ảo giác hoặc căn chỉnh sai.

Để gợi ra suy luận CoT, nhắc nhở CoT nhúng nó vào nhắc nhở của LLM [WWS+23]. Phương pháp này
thuận tiện vì nó tránh các cuộc gọi thêm cho LLM, có thể tốn kém. Hình 3
minh họa biểu đồ Hex cho nhắc nhở CoT – cụ thể, chúng ta chia vấn đề (q,σi) thành
hai vấn đề con (q1,σ1) và (q2,σ2), sao cho Λq= Λq1◦Λq2– nói cách khác, suy luận CoT
đúng cho vấn đề cụ thể. Cho (q1,_) và (q2,_) là các nhắc nhở cho vấn đề (q1,σ1) và
(q2,σ2) tương ứng. Thì chúng ta nối các chuỗi truy vấn của các nhắc nhở vấn đề con để tạo thành
nhắc nhở CoT cho vấn đề gốc: (q1.q2,σi). Chúng ta ký hiệu truy vấn trừu tượng tương ứng với
nhắc nhở CoT bằng ˆq1.q2, và hàm LLM nhận nhắc nhở CoT làm đầu vào và tạo ra
chuỗi trạng thái σ2 làm đầu ra bằng Λˆq1.q2. Thì điều sau đây đúng.

--- TRANG 10 ---
Bổ đề 2. Giả sử chúng ta có một vấn đề (q,σi) có thể được giải quyết bằng cách chia nó thành hai vấn đề con
(q1,σ1) và (q2,σ2), sao cho Λ(σi) = (Λq2◦Λq1)(σi). Giả sử cũng rằng Hình 3 cho thấy cách
sử dụng phương pháp nhắc nhở CoT để giải quyết (q,σi). Thì nhắc nhở CoT hợp lệ khi và chỉ khi mọi
cách có thể biến đổi σi thành Λq(σi) trong biểu đồ đều cho kết quả giống nhau, tức là, biểu đồ
có tính giao hoán.

Chứng minh. Suy ra từ thực tế rằng tính giao hoán ngụ ý không có ảo giác hoặc căn chỉnh sai.

Hệ quả 3. Nhắc nhở CoT và suy luận CoT tạo ra cùng đầu ra cho một vấn đề (q,σ) khi và chỉ khi
cả hai điều kiện này đều đúng:
• Suy luận CoT đúng cho vấn đề (q,σ). Tức là, Λq= Λq1◦Λq2.
• Các biểu đồ Hex cho nhắc nhở CoT và suy luận CoT đều có tính giao hoán.

Nhắc nhở CoT và suy luận CoT chỉ tương đương dưới các điều kiện được đưa ra bởi Hệ quả 3.
Tuy nhiên, nhiều hệ thống (bao gồm API kỹ thuật) dựa vào nhắc nhở CoT mà không xác minh
điều này. Điều này có thể dẫn đến kết quả không chính xác, như chúng tôi sẽ minh họa. Nói chung, nhắc nhở CoT không đủ
để kích hoạt suy luận CoT!

Ví dụ sau đây cho thấy một nhiệm vụ nhắc nhở zero-shot cho GPT-4: tạo mã TikZ để vẽ
một hình tròn bên trong một hình lục giác, và một hình vuông bên trong hình tròn. Tuy nhiên, giải pháp của GPT-4 không chính xác. Nó
vẽ một hình tròn bên trong một hình lục giác và một hình vuông bên trong hình tròn, như hiển thị bên dưới.

Nhắc nhở zero-shot với GPT-4
Vấn đề: Vẽ một hình tròn bên trong một hình lục giác và một hình vuông bên trong hình tròn bằng TikZ.
Nhắc nhở:
final_answer = GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo biểu đồ.
Vẽ một hình tròn bên trong một hình lục giác và một hình vuông bên trong hình tròn.")

Kết quả sau khi biên dịch mã được tạo:

Với nhắc nhở CoT, hoặc cố gắng giải quyết vấn đề từng bước trong cùng một nhắc nhở, GPT-4 vẽ
một hình vuông bên trong một hình tròn và hình vuông bên trong một hình tròn khác, như hiển thị bên dưới. Thật không may, điều này
vẫn không chính xác và thực tế, tệ hơn trường hợp zero-shot, không vẽ một hình dạng không chính xác
(hình vuông thay vì hình lục giác).

--- TRANG 11 ---
Nhắc nhở CoT với GPT-4
Nhắc nhở:
final_answer = GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo biểu đồ.
Đầu tiên, vẽ một hình lục giác. Sau đó, vẽ một hình tròn bên trong hình lục giác. Sau đó, vẽ
một hình vuông bên trong hình tròn.")

Kết quả sau khi biên dịch mã được tạo:

Cuối cùng, với suy luận CoT, GPT-4 vẽ đúng hình.

Suy luận CoT với GPT-4:
Nhắc nhở1:
answer1 = GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo biểu đồ.
Vẽ một hình lục giác.")

Nhắc nhở2:
answer2 =GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo biểu đồ.
Một hình lục giác có thể được vẽ như {answer1}. Vẽ một hình tròn bên trong hình lục giác.")

Nhắc nhở3
final_answer =GPT4("Bạn là một lập trình viên sử dụng TikZ để tạo
biểu đồ. Một hình lục giác có thể được vẽ như {answer1}. Một hình tròn bên trong
hình lục giác có thể được vẽ như {answer2}. Vẽ một hình vuông bên trong hình tròn.")

Kết quả sau khi biên dịch mã được tạo:

Hệ quả 3 có những hậu quả quan trọng đối với các khái niệm quen thuộc trong không gian LLM. Những điều này bao gồm
tự xác minh và kỹ thuật nhắc nhở, mà chúng tôi sẽ thảo luận tiếp theo.

--- TRANG 12 ---
ˆσ1 ˆσ2 ˆσ3 ˆσ4
σ1 σ2 σ3 σ4
σi σ2 σ2 σi+1Λq1Λˆq1
Λq2Λˆq2

Hình 4: Tự xác minh.

4.1 Tự xác minh
Tự xác minh [GSC+23,WZX+23] là khả năng của các LLM đánh giá tính hợp lệ của đầu ra của chính chúng.
Trong các vấn đề được giải quyết bằng tự xác minh, cùng một LLM đóng hai vai trò khác nhau: 1) nó là một trình tạo
giải pháp, và 2) một bộ phân biệt các câu trả lời có thể sai. Nhưng làm thế nào một LLM đã
tạo ra một câu trả lời sai có thể tự kiểm tra? Chúng tôi giải thích điều này bằng khung Hex.

Tự xác minh bao gồm hai vấn đề con trong một. Đầu tiên, một vấn đề (q1,σ1) tạo ra một
trạng thái đầu ra σ2= Λq1(σ1). Thứ hai, một vấn đề xác minh (q2,σ2) nhận σ2 làm đầu vào và
tạo ra một trạng thái σ3 chỉ ra liệu σ2 có đúng hay không. Quá trình này có thể được đại diện
bằng cách kết hợp các biểu đồ Hex cho hai vấn đề con (q1,σ1) và (q2,σ2). Điều này tương tự
với suy luận CoT. Thông qua Hex, có một biện minh lý thuyết cho bằng chứng thực nghiệm
trong văn học rằng tự xác minh cải thiện độ chính xác: một biểu đồ có tính giao hoán, như biểu đồ trong
Hình 4, trong đó hai biểu đồ con là các bước tạo và xác minh.

Tự xác minh với tạo và xác minh trong cùng một nhắc nhở
Nhắc nhở:
answer = GPT4("Bạn là một chuyên gia về lịch sử chính trị. Cho tôi một danh sách
tên của 20 chính trị gia sinh ra ở thành phố Chicago. Sau khi
tạo danh sách tên, tạo một danh sách khác với các thành phố nơi họ
được sinh ra.")

Câu trả lời:
Danh sách các chính trị gia sinh ở Chicago:
1. Hillary Clinton 2. Nancy Pelosi 3. Carol Moseley Braun 4. Rahm Emanuel
5. Dan Rostenkowski 6. Harold Washington 7. Bobby Rush 8. Rod Blagojevich
9. Timuel Black 10. John W. E. Thomas

Các thành phố sinh tương ứng của họ (tất cả ở Chicago):
1. Chicago, Illinois 2. Chicago, Illinois 3. Chicago, Illinois 4. Chicago,
Illinois 5. Chicago, Illinois 6. Chicago, Illinois 7. Chicago, Illinois
8. Chicago, Illinois 9. Chicago, Illinois 10. Chicago, Illinois

--- TRANG 13 ---
Ví dụ đơn giản ở trên minh họa điều này. Chúng tôi kết hợp các bước tạo và xác minh
trong cùng một nhắc nhở. Trong bước tạo, mô hình tạo ra một danh sách 10 tên, nhưng nó không có
cách nào để khẳng định liệu tất cả họ có được sinh ở Chicago hay không. Thực tế, Nancy Pelosi, Bobby Rush và Timuel
Black được sinh ở nơi khác. Tuy nhiên, khi chúng ta chia nhiệm vụ thành hai bước – tạo danh sách
và xác minh nó – chúng ta thấy rằng mô hình có thể xác định lỗi của chính nó.

Mặt khác, như thấy bên dưới, việc chia vấn đề thành nhiều cuộc gọi LLM, thay vì
một nhắc nhở thống nhất, cũng giúp ích trong ví dụ này. Đây chỉ là một minh họa, và văn học tự xác minh
thực sự sử dụng các nhắc nhở riêng biệt để tạo và xác minh câu trả lời.

Tự xác minh với xác minh trong một nhắc nhở riêng biệt
Nhắc nhở:
answer = GPT4("Bạn là một chuyên gia về lịch sử chính trị. Cho tôi một danh sách với
các thành phố nơi những chính trị gia này được sinh ra: 1. Hillary Clinton 2. Nancy
Pelosi 3. Carol Moseley Braun 4. Rahm Emanuel 5. Dan Rostenkowski
6. Harold Washington 7. Bobby Rush 8. Rod Blagojevich 9. Timuel Black
10. John W. E. Thomas.")

Câu trả lời:
1. Hillary Clinton - Sinh ở Chicago, Illinois, Hoa Kỳ
2. Nancy Pelosi - Sinh ở Baltimore, Maryland, Hoa Kỳ
3. Carol Moseley Braun - Sinh ở Chicago, Illinois, Hoa Kỳ
4. Rahm Emanuel - Sinh ở Chicago, Illinois, Hoa Kỳ
5. Dan Rostenkowski - Sinh ở Chicago, Illinois, Hoa Kỳ
6. Harold Washington - Sinh ở Chicago, Illinois, Hoa Kỳ
7. Bobby Rush - Sinh ở Albany, Georgia, Hoa Kỳ
8. Rod Blagojevich - Sinh ở Chicago, Illinois, Hoa Kỳ
9. Timuel Black - Sinh ở Birmingham, Alabama, Hoa Kỳ
10. John W. E. Thomas - Sinh ở Springfield, Illinois, Hoa Kỳ.

Tạo và xác minh câu trả lời có thể bao gồm nhiều vấn đề con, mỗi vấn đề có thể được hưởng lợi
từ suy luận CoT. Ví dụ, [WZX+23] đã sử dụng nhắc nhở CoT để tạo câu hỏi cho
xác minh, nhưng chúng tôi lập luận rằng suy luận CoT cũng có thể giúp phân tách quá trình tạo và
xác minh hơn nữa, khi chúng đòi hỏi nhiều bước. Một số bằng chứng thực nghiệm hỗ trợ
ý tưởng này. Công việc chain-of-verification gần đây [DKX+23] đã so sánh hiệu suất của các LLM
khi chúng sử dụng cùng một nhắc nhở cho cả việc lập kế hoạch và thực hiện xác minh, hoặc khi chúng
sử dụng các nhắc nhở riêng biệt cho mỗi bước. Kết quả cho thấy cách tiếp cận sau, tuân theo
các nguyên tắc suy luận CoT, vượt trội hơn cách tiếp cận trước, tuân theo các nguyên tắc nhắc nhở CoT.

4.2 Lập trình nhắc nhở
Cấu trúc văn bản của một nhắc nhở có thể ảnh hưởng đến cách các mô hình ngôn ngữ lớn (LLM) xử lý và phản hồi
với nó. Chúng ta cần một phương pháp để tạo và đánh giá các nhắc nhở phù hợp với mục tiêu và kỳ vọng của chúng ta đối với
LLM, và phù hợp với khả năng xử lý văn bản của chúng. Đây là những gì lập trình nhắc nhở hướng tới:
một cách có hệ thống để thiết kế và kiểm tra các nhắc nhở cho LLM [RM21].

Một nhắc nhở rõ ràng và súc tích có hai thành phần cơ bản: một truy vấn và một trạng thái. Truy vấn nói với
mô hình những gì người dùng muốn làm, chẳng hạn như tóm tắt một văn bản hoặc chú thích một hình ảnh. Trạng thái
cung cấp cho mô hình dữ liệu hoặc thông tin cần thiết để làm điều đó, chẳng hạn như văn bản hoặc hình ảnh.
Sự phân tách này làm cho việc thiết kế các nhắc nhở có thể thích ứng với các tình huống khác nhau dễ dàng hơn.

OpenAI API trong Azure [azu23] (và các nền tảng khác nơi GPT-4 có sẵn) cung cấp một
cách tự nhiên để tách truy vấn và trạng thái trong nhắc nhở. API cho phép người dùng cấu trúc
nhắc nhở thành ba phần: "system", "user", và "assistant". Những phần này tương ứng với các
yếu tố chính của biểu đồ Hex trong Hình 1:

• Phần "system" định nghĩa vai trò của LLM. Đó là thành phần truy vấn của nhắc nhở,
¯q, thể hiện câu hỏi q mà chúng ta muốn trả lời.

• Phần "user" cung cấp các chi tiết cụ thể của câu hỏi. Đó là thành phần trạng thái của
nhắc nhở, ¯σi, đại diện cho phiên bản văn bản của trạng thái ban đầu σi.

• Phần "assistant" cho phép người dùng viết trạng thái ¯σi+1 một cách rõ ràng. Điều này hữu ích cho
các cách tiếp cận học few-shot, nơi nhiều cặp nhắc nhở và phản hồi được đưa cho
LLM trước khi câu hỏi cuối cùng được hỏi.

Sau đây là một ví dụ về việc sử dụng Python API để viết một nhắc nhở zero-shot cho việc cộng
hai số. Đây là cùng ví dụ như trong Mục 2.1.

Học zero-shot
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=[
{"role": "system", "content": "bạn là máy tính có thể tính
tổng của x và y."},
{"role": "user", "content": " x=12 và y =13."}])

Đầu ra LLM: "Tổng của x và y là 12 + 13 = 25."

Cùng API cũng hỗ trợ học few-shot, yêu cầu một số ví dụ về câu hỏi và
câu trả lời liên quan đến truy vấn như hiển thị bên dưới.

--- TRANG 15 ---
Học few-shot
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=[
{"role": "system", "content": "Assistant là một chatbot thông minh
được thiết kế để giúp người dùng trả lời các câu hỏi lịch sử và chính trị."},
{"role": "user", "content": "Ai là tổng thống đầu tiên của
Hoa Kỳ?"},
{"role": "assistant", "content": "George Washington."},
{"role": "user", "content": "Thủ đô của Pháp là gì?"},
{"role": "assistant", "content": "Thủ đô của Pháp là Paris."},
{"role": "user", "content": "Ai đã viết tiểu thuyết Pride and Prejudice?"},
{"role": "assistant", "content": "Jane Austen."},
{"role": "user", "content": "Ai là Thủ tướng hiện tại của
Vương quốc Anh?"}])

Đầu ra LLM: "Thủ tướng hiện tại của Vương quốc Anh là
Rishi Sunak."

API này cho phép chúng ta tạo các nhắc nhở CoT bằng cách cung cấp cho assistant các giải pháp từng bước của
các ví dụ làm đầu vào của nó.

Nhắc nhở chuỗi suy nghĩa
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=
{"role": "system", "content": "Assistant là một người lập kế hoạch du lịch"},
{"role": "user", "content": "Cách nào nhanh hơn để về nhà?
Lựa chọn 1: Đi xe buýt 10 phút, sau đó xe buýt 40 phút, và
cuối cùng tàu hỏa 10 phút. Lựa chọn 2: Đi tàu hỏa 90 phút,
sau đó đi xe đạp 45 phút, và cuối cùng xe buýt 10 phút."},
{"role": "assistant", "content": "Lựa chọn 1 sẽ mất 10+40+10 = 60
phút. Lựa chọn 2 sẽ mất 90+45+10=145 phút. Vì
Lựa chọn 1 mất 60 phút và Lựa chọn 2 mất 145 phút,
Lựa chọn 1 nhanh hơn."},
{"role": "user", "content": "Cách nào nhanh hơn để đến chỗ làm?
Lựa chọn 1: Đi xe buýt 1000 phút, sau đó tàu hỏa nửa giờ, và
cuối cùng đi xe đạp 10 phút. Lựa chọn 2: Đi xe buýt 800 phút,
sau đó tàu hỏa một giờ, và cuối cùng đi xe đạp 30 phút."}])

aVí dụ được chuyển thể từ https://learnprompting.org/docs/intermediate/chain_of_thought

--- TRANG 16 ---
Đầu ra LLM: "Lựa chọn 1 sẽ mất 1000+30+10 = 1040 phút. Lựa chọn 2
sẽ mất 800+60+30 = 890 phút. Vì Lựa chọn 2 mất 890 phút
và Lựa chọn 1 mất 1040 phút, Lựa chọn 2 nhanh hơn."

API này cho người dùng nhiều linh hoạt để cấu trúc vấn đề và gán các vai trò khác nhau cho
các phần của nhắc nhở. Tuy nhiên, nó không cho thấy cách dịch vụ kết hợp những thành phần này
một cách nội bộ. Như Mục 2 giải thích, suy luận CoT bao gồm các giả định mạnh và nhiều cuộc gọi đến
LLM (một cho mỗi bước), không giống như nhắc nhở CoT sử dụng một nhắc nhở duy nhất. Điều này cho phép nhiều
cách đa dạng và hiệu quả hơn để sử dụng và triển khai các mô hình như GPT-4 với ý tưởng suy luận CoT. Điều này
đặc biệt hữu ích trong các lĩnh vực như chăm sóc sức khỏe, nơi một chuyên gia lĩnh vực cần định nghĩa và thực hiện
các nhiệm vụ con của một vấn đề. Chúng tôi hy vọng rằng công việc này đóng góp vào việc thúc đẩy những hướng này.

5 Thảo luận
Các LLM được sử dụng rộng rãi trong AI cho nhiều nhiệm vụ, ứng dụng và dịch vụ khác nhau, nhưng chúng cũng đặt ra những thách thức
và rủi ro, chẳng hạn như tạo ra đầu ra không chính xác hoặc gây hiểu lầm, hoặc không căn chỉnh với giá trị và kỳ vọng của con người
. Chúng tôi đã trình bày một khung toán học để phân tích, so sánh và cải thiện các LLM
một cách có hệ thống. Khung của chúng tôi định nghĩa ảo giác và căn chỉnh, và tiết lộ các yếu tố
ảnh hưởng đến tương tác của chúng ta với LLM. Chúng tôi cho thấy rằng một phương pháp nhắc nhở LLM phổ biến,
chia một truy vấn thành các truy vấn con, không đảm bảo suy luận hợp lý. Phát hiện này có ý nghĩa quan trọng
đối với việc xây dựng các hệ thống AI đáng tin cậy và mạnh mẽ với LLM. Ý tưởng chính là làm thế nào để
đảm bảo tính tự nhất quán của mô hình, hoặc làm thế nào để cải thiện cả mô hình và nhắc nhở để đảm bảo
câu trả lời chính xác về mặt thực tế dẫn đến các biểu đồ có tính giao hoán. Tùy thuộc vào vấn đề, điều này có thể
yêu cầu hướng dẫn chi tiết hơn theo kiểu nhắc nhở CoT, hoặc nhiều cuộc gọi đến mô hình. Chúng tôi
không cung cấp một công thức cho mỗi trường hợp, nhưng chúng tôi đặc trưng hóa những vấn đề này và làm nổi bật sự khác biệt.

Khung toán học của chúng tôi tiết lộ một số hướng nghiên cứu hứa hẹn cho LLM. Một
thách thức chính trong AI là đảm bảo rằng các LLM học các biểu diễn trừu tượng từ dữ liệu tôn trọng một số
bất biến hoặc nguyên tắc chung. Về điểm đó, biểu đồ Hex cho thấy cách các thành phần của nó liên quan
đến các nguyên tắc hoặc lựa chọn thiết kế khác nhau cho LLM. Ví dụ, kỹ thuật nhắc nhở nhằm tìm các bản đồ trừu tượng hóa tốt αc làm cho đầu vào và đầu ra của mô hình nhất quán hơn. Mặt khác,
γc xác định cách mô hình tạo ra câu trả lời. Trong việc sử dụng LLM đối thoại hiện tại,
điều này bao gồm việc học từ phản hồi của con người (RLHF). Chúng ta cũng có thể tinh chỉnh các biểu diễn cho
trạng thái nội bộ ˆσi và bản đồ Λˆq cho các ứng dụng cụ thể. Chúng tôi hy vọng rằng công việc của chúng tôi giúp
giải quyết những thách thức này và đẩy nhanh tiến bộ trong AI.

Lời cảm ơn
Chúng tôi cảm ơn Javier Alvaraz-Valle, Shruthi Bannur và Ted Meeds cho phản hồi quý báu của họ, đã
giúp chúng tôi cải thiện công việc. Chúng tôi cũng cảm ơn Peter Lee, Sriram Rajamani và Desney Tan cho
những cuộc trò chuyện truyền cảm hứng đã khơi dậy một số ý tưởng của chúng tôi.

Tài liệu tham khảo
[azu23] Azure OpenAI API. https://learn.microsoft.com/en-us/azure/ai-services/openai,
2023.

--- TRANG 17 ---
[BCE+23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz,
Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, và Yi Zhang. Sparks of Artificial General Intel-
ligence: Early experiments with GPT-4. ArXiv, abs/2303.12712, 2023.

[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,
Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra,
Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek
Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,
Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pel-
lat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah
Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.

[DKX+23] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli
Celikyilmaz, và Jason Weston. Chain-of-verification reduces hallucination in large
language models, 2023.

[GSC+23] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jian-
feng Gao, và Hoifung Poon. Self-verification improves few-shot clinical information
extraction, 2023.

[JLF+23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin
Bang, Andrea Madotto, và Pascale Fung. Survey of hallucination in natural language
generation. ACM Computing Surveys, 55(12):1–38, mar 2023.

[KNST23] Emre Kiciman, Robert Ness, Amit Sharma, và Chenhao Tan. Causal reasoning and
large language models: Opening a new frontier for causality. CoRR, abs/2305.00050,
2023.

[MLC+23] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson,
và Mark Steedman. Sources of hallucination by large language models on inference
tasks, 2023.

[Ope23] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

[RM21] Laria Reynolds và Kyle McDonell. Prompt programming for large language models:
Beyond the few-shot paradigm. CoRR, abs/2102.07350, 2021.

[Sch23] Charlotte Schubert. Gpt-4 and medicine: Microsoft research head peter lee on the ai
chatbot's potential and limitations. GeekWire, October 2023.

[SJH+23] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zisha Guo, Xinwei
Wu, Yan Liu, và Deyi Xiong. Large language model alignment: A survey. arXiv
preprint arXiv:2309.15025, 2023.

--- TRANG 18 ---
[SPH+23] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas,
Alexander Kranias, John J. Nay, Kshitij Gupta, và Aran Komatsuzaki. Arb: Advanced
reasoning benchmark for large language models, 2023.

[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-
lien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open
and efficient foundation language models, 02 2023.

[WWLS23] Yotam Wolf, Noam Wies, Yoav Levine, và Amnon Shashua. Fundamental limitations
of alignment in large language models. CoRR, abs/2304.11082, 2023.

[WWS+23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed Chi, Quoc Le, và Denny Zhou. Chain-of-thought prompting elicits reasoning in
large language models, 2023.

[WZX+23] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, và Jun Zhao. Large
language models are better reasoners with self-verification, 2023.

[ZLD+23] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, và
Tatsunori Hashimoto. Benchmarking large language models for news summarization.
ArXiv, abs/2301.13848, 2023.
