# 2309.16120.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2309.16120.pdf
# Kích thước tập tin: 1553658 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================

--- TRANG 1 ---
Khắc phục Hiểu nhầm Đặc tả của Mô hình Ngôn ngữ Lớn
để Tạo mã tốt hơn

Zhao Tian
Trường Trí tuệ và
Điện toán, Đại học Thiên Tân
Trung Quốc
tianzhao@tju.edu.cn

Junjie Chen*
Trường Trí tuệ và
Điện toán, Đại học Thiên Tân
Trung Quốc
junjiechen@tju.edu.cn

Xiangyu Zhang
Khoa Khoa học Máy tính,
Đại học Purdue
Hoa Kỳ
xyzhang@cs.purdue.edu

Tóm tắt —Tạo mã là việc tự động tạo ra mã nguồn tuân theo một đặc tả lập trình đã cho, điều này đã nhận được sự chú ý rộng rãi đặc biệt với sự phát triển của các mô hình ngôn ngữ lớn (LLM). Do khó khăn cố hữu của việc tạo mã, mã được tạo bởi LLM có thể không phù hợp với đặc tả. Mặc dù các kỹ thuật prompting gợi ý tư duy đã được đề xuất để nâng cao hiệu suất tạo mã của LLM, việc tạo ra hiểu biết chính xác cho các bài toán lập trình phức tạp vẫn là thách thức, dẫn đến hiệu suất không đạt yêu cầu. Ngoài ra, một số kỹ thuật prompting dựa trên phản hồi đã được đề xuất để sửa mã không chính xác bằng cách sử dụng thông báo lỗi được tạo ra từ việc thực thi test. Tuy nhiên, khi mã được tạo ra lệch khá nhiều so với ground truth, chúng gặp khó khăn trong việc cải thiện hiệu suất dựa trên thông tin thô như vậy.

Trong công trình này, chúng tôi đề xuất một kỹ thuật prompting mới gọi là µFiX, để cải thiện hiệu suất tạo mã của LLM bằng cách thiết kế cả prompting gợi ý tư duy tinh vi và prompting dựa trên phản hồi và thực hiện khám phá đầu tiên về sự tương tác của chúng. Đầu tiên, nó khai thác phân tích test case để thu được hiểu biết đặc tả và cho phép quy trình tự cải thiện để xác định và tinh chỉnh hiểu nhầm trong giai đoạn prompting gợi ý tư duy. µFiX tiếp tục sửa chữa hiểu biết đặc tả theo hướng giảm khoảng cách giữa hiểu biết được cung cấp (từ giai đoạn đầu) và hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã trong giai đoạn prompting dựa trên phản hồi. Bằng cách cải thiện hiểu biết với µFiX, hiệu suất tạo mã của LLM có thể được cải thiện đáng kể. Đánh giá của chúng tôi trên hai LLM tiên tiến (ChatGPT và DeepSeek-Coder) với sáu benchmark được sử dụng rộng rãi bằng cách so sánh với 15 baseline, chứng minh hiệu quả của µFiX. Ví dụ, µFiX vượt trội so với baseline hiệu quả nhất với cải thiện trung bình 35.62% về Pass@1 trên tất cả các chủ đề.

Từ khóa chỉ mục —Tạo mã, Mô hình Ngôn ngữ Lớn, Kỹ thuật Prompting

I. GIỚI THIỆU

Tạo mã nhằm tự động tạo ra mã nguồn tuân theo một đặc tả lập trình đã cho (thường là mô tả bằng ngôn ngữ tự nhiên). Nó có thể giúp giảm các nỗ lực lập trình lặp đi lặp lại và cải thiện năng suất phát triển phần mềm. Trong những năm gần đây, việc tạo mã đã nhận được sự chú ý rộng rãi từ cả học thuật và công nghiệp. Đặc biệt, với việc LLM được phát triển nhanh chóng, tiến bộ đáng kể đã được thực hiện trong việc tạo mã, chẳng hạn như ChatGPT [1] và DeepSeek-Coder [2]. Các LLM lấy đặc tả lập trình (tức là prompt) làm đầu vào và xuất ra giải pháp mã tương ứng, thể hiện những tiến bộ đáng chú ý trong việc tạo mã.

Mặc dù phổ biến, LLM vẫn gặp một số vấn đề về hiệu suất. Đó là, mã được tạo ra có thể không phù hợp với đặc tả do con người cung cấp, đặc biệt khi logic lập trình phức tạp [3]. Ví dụ, một LLM tiên tiến, tức là ChatGPT, tạo ra mã vượt qua tất cả test case chỉ cho 16.33% bài toán lập trình trong một benchmark thực tế [4]. Các vấn đề hiệu suất có thể ảnh hưởng tiêu cực đến việc sử dụng thực tế của LLM, thậm chí làm chậm quá trình phát triển phần mềm và gây hại cho chất lượng phần mềm. Do đó, việc nâng cao khả năng tạo mã của LLM là quan trọng.

Mặc dù các chiến lược fine-tuning đã được áp dụng rộng rãi để cải thiện hiệu suất tạo mã của LLM, chúng tốn thời gian và cần một lượng lớn tài nguyên tính toán [5]–[8]. Trong những năm gần đây, các kỹ thuật prompting đã được đề xuất để đạt được mục tiêu này theo cách plug-and-play [9]–[13]. Trong số đó, prompting gợi ý tư duy là loại phổ biến nhất. Nó nhằm thúc đẩy LLM tạo ra các bước lý luận trung gian như hiểu biết đặc tả để tạo mã chính xác hơn. Các kỹ thuật prompting gợi ý tư duy điển hình bao gồm CoT [9] (thúc đẩy LLM tạo ra các bước lý luận bằng ngôn ngữ tự nhiên trung gian), Self-planning [13] (hướng dẫn LLM phân tách đặc tả thành một tập hợp các bài toán con dễ giải và tạo ra kế hoạch mã để tạo điều kiện cho việc tạo mã), SCoT [14] (nâng cao CoT bằng cách sử dụng cấu trúc chương trình để xây dựng các bước lý luận trung gian, từ đó thúc đẩy LLM tạo mã), v.v. Prompting dựa trên phản hồi là một loại kỹ thuật prompting khác (như Self-Debugging [15], Self-Edit [16], và Self-repair [17]), tận dụng thông báo lỗi được tạo ra từ việc thực thi test để cho phép LLM sửa mã được tạo ra không chính xác.

Mặc dù những kỹ thuật này đã được nghiên cứu rộng rãi, hiệu suất của chúng vẫn cần được cải thiện. Ví dụ, các kỹ thuật prompting gợi ý tư duy hiện tại gặp khó khăn trong việc tạo ra hiểu biết chính xác theo đặc tả (ngắn gọn), khi đối mặt với các bài toán lập trình phức tạp [13], [18]. Hơn nữa, chúng không thể xác định hoặc sửa chữa hiểu biết đặc tả không chính xác xảy ra trước khi tạo mã, dẫn đến kết quả không chính xác. Đối với prompting dựa trên phản hồi, các kỹ thuật hiện tại chỉ sử dụng thông báo lỗi được tạo ra từ việc thực thi test để hiểu tại sao mã được tạo ra không chính xác, điều này quá thô để xác định nguyên nhân gốc rễ và do đó dẫn đến hiệu suất không tối ưu. Nếu mã được tạo ra lệch nhiều so với ground truth, rất khó để chúng cải thiện hiệu suất dựa trên thông báo lỗi từ mã được tạo ra chất lượng thấp như vậy [17]. Đặc biệt, hai loại này đóng vai trò ở các giai đoạn khác nhau (loại đầu hoạt động trước khi thực thi test trong khi loại sau hoạt động sau) nhưng không có công trình nào khám phá sự tương tác của chúng.

Để cải thiện hiệu suất tạo mã của LLM, chúng tôi đề xuất một kỹ thuật prompting mới gọi là µFiX (Misunderstanding FiXing), để khắc phục những hạn chế nêu trên. Đặc biệt, µFiX là kỹ thuật đầu tiên khám phá sự tương tác của hai loại trên, bằng cách thiết kế cả prompting gợi ý tư duy và prompting dựa trên phản hồi. Insight chính là, bằng cách cải thiện hiểu biết đặc tả trong giai đoạn prompting gợi ý tư duy thông qua phân tích test case, hiệu quả của prompting dựa trên phản hồi tiếp theo (và việc tạo mã) có thể được nâng cao. Cụ thể, mặc dù LLM có thể không tạo ra mã chính xác dựa trên hiểu biết đặc tả, nó có thể làm cho mã được tạo ra càng gần với ground truth càng tốt, điều này có thể cung cấp cơ hội lớn hơn cho prompting dựa trên phản hồi tiếp theo để cải thiện thêm hiệu suất tạo mã.

Trong giai đoạn prompting gợi ý tư duy, điều quan trọng nằm ở việc cải thiện hiểu biết của LLM về đặc tả. Chúng tôi đề xuất đạt được điều này bằng cách tận dụng các test case vốn có được bao gồm như một phần của đặc tả. Lưu ý rằng các test case như vậy rất phổ biến trong thực tế, bao gồm những dataset được nghiên cứu rộng rãi trong việc tạo mã (ví dụ, HumanEval [19] và APPS [4]). Do khả năng lý luận cố hữu của LLM, nếu hiểu biết của nó đúng, nó sẽ có thể giải quyết một yêu cầu bằng ngôn ngữ tự nhiên được rút ra từ các test case (nhúng) và tạo ra đầu ra mong đợi. Ngược lại, khi hiểu biết không chính xác, bằng cách prompting LLM để sửa đầu ra test, chúng ta có thể cải thiện hiểu biết, từ đó nâng cao hiệu suất tạo mã sau này. Điều này tương tự như cách các nhà phát triển phần mềm sử dụng ví dụ test case để hiểu logic lập trình phức tạp trong thực tế [20], [21].

Mặc dù hiểu biết có thể được tinh chỉnh để suy ra đầu ra test chính xác, nó không có nghĩa là mã được tạo ra tương ứng thực sự có thể vượt qua test case trong việc thực thi thực tế do khoảng cách giữa hiểu biết đặc tả và việc tạo mã. Cụ thể, hiểu biết đặc tả và việc tạo mã nhấn mạnh các khía cạnh khác nhau của khả năng trong LLM, trong đó khía cạnh đầu dựa vào khả năng lý luận của LLM trong khi khía cạnh sau nhấn mạnh khả năng dịch mô tả ngôn ngữ tự nhiên thành mã nguồn [22], [23]. Do đó, khi gọi LLM để tạo mã, hiểu biết thực tế được LLM sử dụng ngầm định có thể không nhất quán với hiểu biết được cung cấp. µFiX tiếp tục thiết kế prompting dựa trên phản hồi để cải thiện hiệu suất tạo mã (nếu việc thực thi test thất bại). Thay vì prompting LLM trực tiếp sửa mã được tạo ra với thông báo lỗi thô, µFiX prompting LLM để hiểu nguyên nhân gốc rễ (tức là khoảng cách được đề cập ở trên) bằng cách phân tích so sánh hiểu biết được cung cấp và hiểu biết thực tế (thu được thông qua tóm tắt mã [24]), và sau đó điều chỉnh mô tả ngôn ngữ tự nhiên cho hiểu biết đặc tả để nâng cao khả năng tiếp cận của nó đối với khả năng tạo mã của LLM. Đây cũng là một loại sửa chữa hiểu biết đặc tả, nhằm làm cho hiểu biết được sử dụng tốt hơn bởi khả năng tạo mã của LLM.

Chúng tôi đã tiến hành các thí nghiệm rộng rãi để đánh giá µFiX trên hai LLM tiên tiến (tức là ChatGPT [1] và DeepSeek-Coder [2]) dựa trên sáu benchmark được sử dụng rộng rãi. Kết quả của chúng tôi cho thấy µFiX vượt trội đáng kể so với tất cả 15 kỹ thuật prompting được so sánh trên cả hai LLM trên tất cả sáu benchmark, chứng minh ý tưởng của chúng tôi về việc nâng cao hiệu suất tạo mã bằng cách cải thiện hiểu biết đặc tả của LLM. Ví dụ, cải thiện trung bình của µFiX so với tất cả 15 kỹ thuật được so sánh là 35.62% ∼80.11% về Pass@1 (đo tỷ lệ các bài toán lập trình mà mã được tạo ra vượt qua tất cả test case đánh giá) trên tất cả các chủ đề. Hơn nữa, chúng tôi đã xây dựng bốn biến thể của µFiX cho nghiên cứu ablation. Kết quả xác nhận đóng góp của các chiến lược prompting gợi ý tư duy và prompting dựa trên phản hồi trong µFiX.

Chúng tôi tóm tắt các đóng góp trong công trình này như sau:
• Chúng tôi đề xuất một kỹ thuật prompting mới gọi là µFiX, để nâng cao hiệu suất tạo mã của LLM bằng cách cải thiện hiểu biết đặc tả của chúng với cả prompting gợi ý tư duy tinh vi và prompting dựa trên phản hồi.
• Chúng tôi thiết kế một chiến lược prompting gợi ý tư duy mới, khai thác phân tích test case để tạo ra hiểu biết đặc tả chính xác hơn và cho phép quy trình tự cải thiện để xác định và tinh chỉnh hiểu nhầm trước khi tạo mã.
• Chúng tôi thiết kế một chiến lược prompting dựa trên phản hồi mới điều chỉnh hiểu biết theo hướng giảm khoảng cách giữa hiểu biết được cung cấp và hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã.
• Chúng tôi đã tiến hành các thí nghiệm rộng rãi trên hai LLM (tức là ChatGPT và DeepSeek-Coder) trên sáu benchmark được sử dụng rộng rãi bằng cách so sánh với 15 baseline, chứng minh hiệu quả của µFiX trong việc cải thiện hiệu suất tạo mã của LLM.

II. VÍ DỤ MINH HỌA

Chúng tôi sử dụng một ví dụ để minh họa tại sao các kỹ thuật prompting tiên tiến không hoạt động tốt, thúc đẩy ý tưởng của chúng tôi. Hình 1 cho thấy ví dụ (đơn giản hóa) được lấy từ benchmark HumanEval thực tế [19]. Trong hình này, đặc tả lập trình cung cấp một mô tả ngôn ngữ tự nhiên (bao gồm một test case).

Nghiên cứu gần đây đã chỉ ra rằng các kỹ thuật prompting có thể thu được hiểu biết về đặc tả bằng cách khai thác khả năng lý luận logic của LLM, từ đó cải thiện hiệu suất của chúng trong việc tạo mã [13], [18]. Ở đây, chúng tôi sử dụng hai kỹ thuật prompting gợi ý tư duy tiên tiến (tức là CoT [9] và SCoT [14]) để prompt LLM tiên tiến (tức là ChatGPT [1]) tạo mã liên quan đến đặc tả này. Như được hiển thị trong Hình 1, cả CoT và SCoT đều thu được hiểu biết về đặc tả bằng cách chia nó thành một số bước lý luận trung gian (tức là chuỗi tư duy và chuỗi tư duy có cấu trúc). Sau đó, chúng tích hợp những bước lý luận chi tiết này với đặc tả ban đầu làm prompt, tạo điều kiện cho ChatGPT tạo mã.

Tuy nhiên, chúng tôi thấy rằng cả hai hiểu biết thu được đều không chính xác do logic lập trình phức tạp được ngụ ý trong đặc tả, dẫn đến việc tạo ra mã không chính xác. Hơn nữa, cả CoT và SCoT đều không có khả năng kiểm tra tính chính xác của hiểu biết đặc tả trước khi tạo mã, càng hạn chế hiệu suất của chúng. Điều này thúc đẩy tầm quan trọng của việc cải thiện hiểu biết đặc tả trước khi tạo mã cho một kỹ thuật prompting gợi ý tư duy.

Tiếp theo, chúng tôi sử dụng kỹ thuật prompting dựa trên phản hồi tiên tiến, Self-repair [17], để sửa mã không chính xác được tạo ra bởi CoT và SCoT. Nó tận dụng thông báo lỗi được tạo ra từ việc thực thi test trên mã không chính xác để hiểu tại sao nó không chính xác và sau đó prompt ChatGPT sửa mã tương ứng. Thật không may, cả mã đã sửa vẫn không chính xác, do sự lệch lạc đáng kể của mã ban đầu được tạo ra so với ground truth. Rất khó để cải thiện hiệu suất tạo mã chỉ dựa trên thông báo lỗi từ mã chất lượng thấp như vậy. Điều này thúc đẩy rằng một kỹ thuật prompting dựa trên phản hồi không chỉ cần có khả năng cải thiện mã được tạo ra không chính xác mà còn bắt đầu với mã chất lượng cao.

Hai loại kỹ thuật hoạt động ở các giai đoạn khác nhau (loại đầu hoạt động trước khi thực thi test trong khi loại sau hoạt động sau). Đầu ra của prompting gợi ý tư duy (tức là mã được tạo ra bởi LLM được nâng cao bởi hiểu biết đặc tả) phục vụ như đầu vào của prompting dựa trên phản hồi. Đó là, hiệu quả của loại đầu có thể ảnh hưởng đến hiệu quả của loại sau, từ đó ảnh hưởng đến hiệu suất tạo mã, nhưng không có công trình nào khám phá sự tương tác này. Điều này thúc đẩy chúng tôi khám phá sự tương tác của hai loại này để tạo mã tốt hơn. Cụ thể, chúng ta có thể đầu tiên cải thiện hiểu biết đặc tả để tạo ra mã càng gần với ground truth càng tốt trong giai đoạn prompting gợi ý tư duy, và sau đó thiết kế một chiến lược prompting dựa trên phản hồi hiệu quả hơn để tạo ra mã chính xác hơn trên cơ sở mã chất lượng cao từ giai đoạn đầu. Với insight này, chúng tôi đề xuất một kỹ thuật prompting mới gọi là µFiX, tận dụng các test case vốn có trong đặc tả để đạt được mục tiêu này. Với µFiX, hiểu biết đặc tả được cải thiện thực sự được tạo ra, và mã chính xác cũng được tạo ra cho ví dụ này trong Hình 1.

III. PHƯƠNG PHÁP TIẾP CẬN

A. Tổng quan

Chúng tôi đề xuất một kỹ thuật prompting mới gọi là µFiX, để cải thiện hiệu suất tạo mã của LLM. µFiX thiết kế cả prompting gợi ý tư duy tinh vi và prompting dựa trên phản hồi, là khám phá đầu tiên về sự tương tác của cả hai loại kỹ thuật prompting. Insight chính là sử dụng test case (vốn có được bao gồm trong đặc tả) để cải thiện hiểu biết đặc tả, lấy cảm hứng từ thực tế mà các nhà phát triển phần mềm thường sử dụng test case để hiểu logic lập trình phức tạp. Nó hữu ích để tạo ra mã chất lượng cao (gần với ground truth) như điểm khởi đầu cho giai đoạn prompting dựa trên phản hồi để cải thiện thêm hiệu suất LLM. Trong thực tế, test case thường được bao gồm trong đặc tả lập trình [4], [19], và chúng tôi cũng đã đánh giá ảnh hưởng của số lượng test case đến hiệu quả của µFiX trong Phần VI-B. Nếu một đặc tả thiếu test case, công trình hiện tại đã chứng minh hiệu quả của LLM trong việc tạo ra chúng [25], [26], từ đó đảm bảo tính khả thi của µFiX, như được chi tiết trong Phần VI-B.

Hình 2 cho thấy tổng quan về µFiX. Nó bao gồm hai giai đoạn: (1) giai đoạn prompting gợi ý tư duy (Phần III-B), nhấn mạnh phân tích test case để tạo ra hiểu biết đặc tả và cho phép quy trình tự cải thiện để tinh chỉnh hiểu nhầm với sự hỗ trợ của test case, và (2) giai đoạn prompting dựa trên phản hồi (Phần III-C), phân tích so sánh hiểu biết được cung cấp và hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã và tối thiểu hóa khoảng cách giữa hai hiểu biết để tạo mã tốt hơn. Giai đoạn sau chỉ được kích hoạt nếu mã được tạo ra không vượt qua test case (được sử dụng để thu được hiểu biết đặc tả) trong việc thực thi thực tế.

Để dễ hiểu về µFiX của chúng tôi, chúng tôi tái sử dụng ví dụ (tức là HumanEval #84) được giới thiệu trong Phần II để minh họa chi tiết của µFiX.

B. Giai đoạn Prompting Gợi ý Tư duy

Trong giai đoạn prompting gợi ý tư duy, µFiX lấy đặc tả lập trình làm đầu vào, và nhằm xuất ra hiểu biết đặc tả chính xác hơn bằng cách tận dụng các test case vốn có được bao gồm trong đặc tả. Một mặt, việc nhấn mạnh phân tích test case giúp LLM cải thiện hiểu biết đặc tả vì test case chứa các chi tiết cụ thể hơn giúp hiểu logic lập trình phức tạp. Mặt khác, test case cho phép quy trình tự cải thiện để tinh chỉnh hiểu nhầm của LLM trước khi tạo mã. Trong quy trình tự cải thiện, µFiX đầu tiên kiểm tra tính chính xác của hiểu biết đặc tả và sau đó cải thiện nó (nếu đó là một hiểu nhầm). Lưu ý rằng các kỹ thuật prompting gợi ý tư duy hiện tại không thể xác định và tinh chỉnh hiểu nhầm trước khi tạo mã do bỏ qua tầm quan trọng của các test case như vậy. Nhìn chung, giai đoạn prompting gợi ý tư duy trong µFiX bao gồm ba bước: tạo hiểu biết ban đầu, kiểm tra tính chính xác của hiểu biết, và sửa chữa hiểu nhầm. Các bước này sẽ được chi tiết trong phần sau.

Tạo hiểu biết ban đầu: Như được hiển thị trong Hình 1, các kỹ thuật prompting gợi ý tư duy hiện tại cũng lấy đặc tả lập trình (bao gồm test case) làm đầu vào, nhưng chúng không chú ý đặc biệt đến những test case này, từ đó hạn chế hiệu quả của chúng. Thay vào đó, µFiX nhấn mạnh phân tích test case bằng cách cung cấp hướng dẫn có cấu trúc cho LLM, có thể giúp chúng sử dụng test case cụ thể để hiểu logic lập trình phức tạp. <Initial Prompt> trong Hình 3 cho thấy hướng dẫn có cấu trúc kích hoạt phân tích test case, giúp giải thích logic lập trình xử lý đầu vào được cung cấp trong test case để có được đầu ra mong đợi được chỉ định trong test case. Sau đó, µFiX cho phép LLM hoàn thành <?> trong hướng dẫn, tạo ra hiểu biết đặc tả ban đầu. Ví dụ, Hình 3 (<Initial Understanding>) cho thấy hiểu biết ban đầu được tạo ra bởi ChatGPT với phân tích test case liên quan đến đặc tả lập trình trong Hình 1.

Kiểm tra tính chính xác của hiểu biết: Từ Hình 3, chúng tôi thấy rằng hiểu biết ban đầu không chính xác mặc dù đầu ra mong đợi được hiển thị trong hiểu biết là chính xác. Thực tế, đầu ra mong đợi được chỉ định chính xác do sự rò rỉ của nó trong test case như được hiển thị trong đặc tả lập trình của Hình 1. Với hiểu nhầm đặc tả này, LLM khó có thể tạo ra mã chính xác. Do đó, với sự hỗ trợ của test case, µFiX bao gồm một cơ chế để kiểm tra tính chính xác của hiểu biết trước, để có cơ hội cải thiện hiểu biết trước khi tạo mã. Cụ thể, µFiX che đi đầu ra mong đợi trong hiểu biết và cho phép LLM suy ra nó theo logic được phân tích trong hiểu biết. Nếu đầu ra được suy ra khác với đầu ra mong đợi, nó chỉ ra một hiểu biết đặc tả không chính xác. Như được hiển thị trong Hình 4, cơ chế này hiệu quả trong việc xác định hiểu nhầm (được tạo ra trong Hình 3).

Sửa chữa hiểu nhầm: Một khi µFiX xác định được hiểu nhầm đặc tả, nó prompt LLM để tinh chỉnh hiểu nhầm này bằng cách cung cấp hướng dẫn tương ứng (ví dụ: "Hiểu biết trên không chính xác. Vui lòng cung cấp phân tích chính xác..."). Quá trình này kết thúc cho đến khi hiểu biết được tinh chỉnh vượt qua cơ chế kiểm tra hoặc số lần tinh chỉnh đạt đến ngưỡng được định trước (ký hiệu là N). Lưu ý rằng việc vượt qua cơ chế kiểm tra không thể đảm bảo rằng LLM hoàn toàn hiểu đặc tả lập trình vì số lượng test case được trang bị trong đặc tả có xu hướng hạn chế. Tuy nhiên, ít nhất, hiểu biết được tinh chỉnh (như được hiển thị trong Refined Specification Understanding của Hình 1) vượt qua cơ chế kiểm tra chính xác hơn so với hiểu biết không vượt qua nó. Với hiểu biết được tinh chỉnh chất lượng cao như vậy, LLM có thể trực tiếp tạo ra mã chính xác. Nếu không, µFiX vẫn cung cấp một điểm khởi đầu chất lượng cao cho giai đoạn prompting dựa trên phản hồi tiếp theo để cải thiện thêm hiệu suất tạo mã.

C. Giai đoạn Prompting dựa trên Phản hồi

Bằng cách tích hợp hiểu biết đặc tả chính xác hơn vào đặc tả lập trình cùng với hướng dẫn bổ sung "Vui lòng thực hiện hàm trong khối mã theo kiểu markdown (chú ý đến hiểu biết đặc tả)", µFiX prompt LLM tạo mã.

Mặc dù hiểu biết được tạo ra trong giai đoạn prompting gợi ý tư duy chính xác hơn liên quan đến test case, nó không đảm bảo rằng mã được tạo ra tương ứng sẽ vượt qua test case trong việc thực thi thực tế. Điều này là do khoảng cách giữa hiểu biết đặc tả và việc tạo mã, nhấn mạnh các khía cạnh khác nhau của khả năng trong LLM. Đó là, hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã có thể không nhất quán với hiểu biết được cung cấp. Ví dụ, khi prompt LLM tạo mã, chúng có thể bỏ lỡ một số nội dung quan trọng trong hiểu biết được cung cấp do kiểu mô tả bằng ngôn ngữ tự nhiên không quen thuộc với khả năng này của LLM.

Các kỹ thuật prompting dựa trên phản hồi hiện tại tận dụng thông báo lỗi được tạo ra từ việc thực thi test để prompt LLM nâng cao việc tạo mã [17]. Loại thông tin này quá thô để xác định nguyên nhân gốc rễ của mã không chính xác (đặc biệt khi mã được tạo ra lệch đáng kể so với ground truth), dẫn đến hiệu suất không đạt yêu cầu. Khác với chúng, µFiX prompt LLM hiểu nguyên nhân gốc rễ (tức là khoảng cách được đề cập ở trên) bằng cách phân tích so sánh hiểu biết được tinh chỉnh được cung cấp và hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã. Ở đây, dựa trên tính đối xứng giữa việc tạo mã (từ mô tả ngôn ngữ tự nhiên sang mã) và tóm tắt mã [27], [28] (từ mã sang mô tả ngôn ngữ tự nhiên), µFiX ước tính hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã thông qua tóm tắt mã (cũng được gọi là hiểu biết được tóm tắt cho dễ trình bày). Như được hiển thị trong <Summarization Prompt> của Hình 5, µFiX prompt LLM tạo ra hiểu biết được tóm tắt (<Summarized Understanding> của Hình 5).

µFiX sau đó điều chỉnh mô tả ngôn ngữ tự nhiên của hiểu biết đặc tả được tinh chỉnh theo hướng giảm khoảng cách, bằng cách khai thác khả năng lý luận logic của LLM. Ví dụ, một số nội dung quan trọng bị bỏ lỡ bởi hiểu biết được tóm tắt có thể được nổi bật hoặc kiểu mô tả bằng ngôn ngữ tự nhiên không quen thuộc với LLM có thể được cải thiện tương ứng thông qua phân tích so sánh. Ở đây, chúng tôi thiết kế một prompt điều chỉnh (<Adjustment Prompt> của Hình 5) để cho phép LLM đạt được mục tiêu trên, bao gồm mã được tạo ra, kết quả thực thi test, hiểu biết được tinh chỉnh được tạo ra từ giai đoạn prompting gợi ý tư duy, hiểu biết được tóm tắt, và hướng dẫn bổ sung hướng dẫn LLM điều chỉnh hiểu biết.

Lấy Refined Specification Understanding của Hình 1 (hiểu biết được tinh chỉnh được cung cấp) và <Summarized Understanding> của Hình 5 (hiểu biết được tóm tắt) làm ví dụ để minh họa, phân tích so sánh giữa chúng ngụ ý rằng LLM chỉ nắm bắt một số từ khóa (ví dụ: digits, binary, và sum) nhưng không hiểu được logic giữa các từ khóa trong hiểu biết được tóm tắt, cung cấp hướng để điều chỉnh hiểu biết. Thực tế, hiểu biết được điều chỉnh nhấn mạnh logic tính toán chính xác với (1+0+0+0), như được hiển thị trong Adjusted Specification Understanding của Hình 1. Với hiểu biết được điều chỉnh bởi µFiX, ChatGPT thành công tạo ra mã chính xác cho đặc tả lập trình, như được hiển thị trong Hình 1. Giai đoạn này là một quá trình lặp lại và kết thúc cho đến khi mã được tạo ra vượt qua test case tương ứng hoặc số lần điều chỉnh đạt đến ngưỡng (ký hiệu là M).

Thực tế, quá trình điều chỉnh cũng có thể được coi là một loại sửa chữa hiểu biết đặc tả, nhằm đảm bảo rằng hiểu biết được sử dụng chính xác trong quá trình tạo mã. Đó là, hai giai đoạn trong µFiX liên quan đến các khía cạnh bổ sung của việc sửa chữa hiểu biết đặc tả, và tăng cường hiệu suất tạo mã một cách hiệp đồng. Lưu ý rằng cả hai thành phần sửa chữa đều không phải là tái tạo đơn giản. Tương tự như các quy trình cải thiện lặp lại lấy con người làm trung tâm (ví dụ: gỡ lỗi), chúng về cơ bản cung cấp bằng chứng phản thực tế (ví dụ: các trường hợp không chính xác trong lịch sử và giải thích của chúng) làm phản hồi cho LLM. Tương tự như các quy trình của con người, chúng có thể không thành công có thể do khả năng hạn chế của LLM hoặc phản hồi không đủ cho LLM. Trong tương lai, chúng tôi sẽ khám phá các LLM hiệu quả hơn trong khía cạnh này và phản hồi thông tin hơn để nâng cao thêm cả hai thành phần sửa chữa.

IV. THIẾT KẾ ĐÁNH GIÁ

Nghiên cứu của chúng tôi giải quyết các câu hỏi nghiên cứu sau (RQ):
• RQ1: µFiX hoạt động như thế nào trong việc cải thiện hiệu suất tạo mã của LLM so với các kỹ thuật prompting tiên tiến?
• RQ2: Mỗi thành phần chính trong µFiX có đóng góp vào hiệu quả tổng thể không?
• RQ3: µFiX hoạt động như thế nào dưới các cấu hình siêu tham số khác nhau?

A. LLM được Nghiên cứu

Theo tin tức của OpenAI [29], quyền truy cập Codex đã bị ngừng và GPT-3.5 được khuyến nghị thay thế, và do đó chúng tôi đã chọn GPT-3.5 (ChatGPT [1]) làm LLM thương mại đại diện trong nghiên cứu của chúng tôi theo các nghiên cứu hiện tại [13], [17], [30]. Ở đây, chúng tôi không chọn GPT-4 do chi phí cao. Theo các nghiên cứu hiện tại [31]–[33], chúng tôi chọn DeepSeek-Coder [23] làm LLM mã nguồn mở đại diện, vì nó đã thể hiện hiệu quả tiên tiến trong số các LLM mã nguồn mở trên nhiều ngôn ngữ lập trình và các benchmark khác nhau về khả năng mã hóa [34]–[36]. Dựa trên hai LLM tiên tiến trong việc tạo mã, tính tổng quát của µFiX có thể được điều tra ở một mức độ nào đó. Cụ thể, chúng tôi sử dụng ChatGPT (phiên bản gpt-3.5-turbo-0613) thông qua API của OpenAI, và DeepSeek-Coder (phiên bản DeepSeek-Coder-6.7B-Instruct) từ Huggingface [37]. Lưu ý rằng mã được tạo ra bởi LLM thường bao gồm các đoạn văn bản ngôn ngữ tự nhiên, dẫn đến lỗi biên dịch. Do đó, theo công trình hiện tại [26], chúng tôi sử dụng một công cụ khử trùng mã [38] để làm sạch mã được tạo ra bởi LLM.

B. Benchmark

Để đánh giá µFiX đầy đủ, chúng tôi đã áp dụng sáu dataset được sử dụng rộng rãi trong nghiên cứu của mình, tức là HumanEval [19], HumanEval+ [26], APPS [4], HumanEval-ET [39], APPS-ET [39], và MBPP-ET [39]. Các dataset này đã được sử dụng rộng rãi trong nhiều nghiên cứu hiện tại để đo lường hiệu suất của các kỹ thuật prompting trong việc tạo mã [13], [14], [40].

HumanEval có 164 bài toán lập trình được viết bởi con người được đề xuất bởi OpenAI. Đặc tả cho mỗi bài toán bao gồm chữ ký hàm, mô tả bài toán bằng ngôn ngữ tự nhiên, và một số test case. Mỗi bài toán có một tập hợp test case khác để đánh giá tính chính xác của mã được tạo ra, được gọi là test case đánh giá ở đây để phân biệt với test case trong đặc tả.

HumanEval+ mở rộng HumanEval bằng cách tự động tạo ra nhiều test case đánh giá hơn cho mỗi bài toán thông qua framework EvalPlus [26]. Nhiều test case đánh giá hơn có thể xác định tính chính xác của mã được tạo ra một cách chính xác hơn.

APPS chứa 10.000 bài toán lập trình được thu thập từ các nền tảng thi lập trình công cộng (ví dụ: LeetCode [41]), bao gồm 5.000 dữ liệu huấn luyện và 5.000 dữ liệu test. Đặc tả cho mỗi bài toán chứa mô tả bài toán bằng ngôn ngữ tự nhiên và một số test case. Theo công trình hiện tại [17], chúng tôi ngẫu nhiên lấy mẫu 300 bài toán từ dữ liệu test theo phân phối độ khó, để cân bằng chi phí đánh giá và tính tổng quát của kết luận.

Công trình hiện tại [39] đã mở rộng các benchmark HumanEval và APPS thành HumanEval-ET và APPS-ET bằng cách xây dựng hơn 100 test case đánh giá bổ sung cho mỗi bài toán, tương ứng. Những test case đánh giá bổ sung này bao gồm nhiều trường hợp góc hơn để nâng cao tính đầy đủ của đánh giá trên mã được tạo ra. Liên quan đến APPS-ET, chúng tôi sử dụng 300 bài toán giống như APPS. Bên cạnh đó, chúng tôi sử dụng MBPP-ET, một phiên bản mở rộng của MBPP (không có test case đánh giá bổ sung), bằng cách bổ sung test case đánh giá cho mỗi bài toán lập trình theo cách tương tự. Dataset này chứa 974 bài toán lập trình, mỗi bài toán có đặc tả bao gồm mô tả bài toán bằng ngôn ngữ tự nhiên và ba test case.

Lưu ý rằng µFiX chỉ sử dụng test case trong đặc tả cho mỗi bài toán. Đối với các dataset này, 81.57% bài toán có ít nhất ba test case trong đặc tả tương ứng của chúng. Test case đánh giá chỉ được sử dụng để đánh giá mã được tạo ra theo thực tế trong việc tạo mã [39].

C. Chỉ số

Theo công trình hiện tại [13], [39], chúng tôi thực thi test case đánh giá để kiểm tra tính chính xác của mã được tạo ra cho mỗi bài toán lập trình, và tính toán Pass@k và AvgPassRatio để đo lường hiệu suất của LLM trong việc tạo mã.

Pass@k đo lường tính chính xác chức năng của mã được tạo ra trên test case đánh giá. Với một bài toán lập trình, LLM tạo ra k instance mã. Bài toán được coi là được giải quyết nếu bất kỳ instance nào vượt qua tất cả test case đánh giá. Pass@k là tỷ lệ phần trăm của các bài toán được giải quyết so với tổng số bài toán. Như được chứng minh bởi công trình hiện tại [10], các nhà phát triển có xu hướng cân nhắc và đánh giá một instance mã được tạo ra bởi LLM được sử dụng, và do đó chúng tôi đặt k = 1 theo các nghiên cứu hiện tại [10], [13], [18], [39], [42]. Lưu ý rằng Pass@1 là một thiết lập nghiêm ngặt hơn và do đó việc cải thiện nó là thách thức. Giá trị Pass@k lớn hơn có nghĩa là hiệu suất tạo mã tốt hơn.

AvgPassRatio đo lường mức độ chính xác của mã được tạo ra trên test case đánh giá, khác với Pass@k coi liệu mã được tạo ra có hoàn toàn chính xác trên test case đánh giá hay không [4]. Cả hai chỉ số đều bổ sung cho nhau ở mức độ lớn. AvgPassRatio tính toán tỷ lệ test case đánh giá vượt qua so với tất cả test case đánh giá cho mỗi bài toán, và sau đó đo lường tỷ lệ trung bình trên tất cả bài toán. Giá trị AvgPassRatio lớn hơn có nghĩa là hiệu suất tạo mã tốt hơn. Để dễ trình bày trong bảng, chúng tôi viết tắt AvgPassRatio thành APR. Các chỉ số khác (tức là Pass@2, Pass@3, và CodeBLEU [43]) sẽ được thảo luận trong Phần VII.

D. Các Kỹ thuật So sánh

Để đánh giá µFiX đầy đủ, chúng tôi đã xem xét chín kỹ thuật prompting điển hình hoặc tiên tiến để so sánh:

• Zero-shot prompting [19]: trực tiếp sử dụng đặc tả ban đầu để prompt LLM tạo mã.
• Few-shot prompting [19]: cho phép LLM học mối quan hệ giữa đặc tả và mã dựa trên các ví dụ <đặc tả, mã> được chọn ngẫu nhiên. Nó nối các ví dụ này với đặc tả ban đầu để tạo thành một prompt, sau đó được đưa vào LLM để tạo mã.
• CoT prompting [9]: thúc đẩy LLM tạo ra các bước lý luận trung gian như hiểu biết đặc tả. Nó kết hợp hiểu biết đặc tả vào đặc tả ban đầu để tạo thành một prompt, sau đó được đưa vào LLM để tạo mã. Theo công trình hiện tại [14], [40], chúng tôi áp dụng CoT prompting trong cả thiết lập zero-shot [44] và few-shot [9]. Để dễ trình bày, chúng tôi gọi chúng là Zero-shot-CoT prompting và Few-shot-CoT prompting.
• Self-planning prompting [13]: hướng dẫn LLM phân tách đặc tả thành một tập hợp các bài toán con dễ giải và tạo ra kế hoạch mã bằng cách cung cấp các ví dụ ý định-tới-kế hoạch few-shot. Nó kết hợp kế hoạch mã vào đặc tả ban đầu để tạo thành một prompt, sau đó được đưa vào LLM để tạo mã.
• SCoT prompting [14]: nâng cao CoT prompting bằng cách sử dụng cấu trúc chương trình (tức là cấu trúc tuần tự, nhánh và vòng lặp) để tạo ra các bước lý luận trung gian.
• Self-Debugging prompting [15]: sử dụng lỗi thời gian chạy và kết quả thực thi test để hướng dẫn LLM sửa mã được tạo ra không chính xác.
• Self-Edit prompting [16]: bao bọc thông báo lỗi (được tạo ra bởi việc thực thi test) như các bình luận bổ sung, bao gồm đầu vào test, đầu ra mong đợi, đầu ra thực tế, và lỗi thời gian chạy. Sau đó, các bình luận bổ sung phục vụ như phản hồi để hướng dẫn LLM sửa mã được tạo ra không chính xác.
• Self-repair prompting [17]: tận dụng thông báo lỗi được tạo ra từ việc thực thi test để cho phép LLM tạo ra giải thích ngắn gọn về lý do mã thất bại. Sau đó, giải thích hướng dẫn LLM sửa mã được tạo ra không chính xác.

Tóm lại, prompting Zero-shot và Few-shot là các kỹ thuật prompting điển hình được sử dụng rộng rãi như baseline trong các nghiên cứu tạo mã [14], [45]. Zero-shot-CoT và Few-shot-CoT là các kỹ thuật prompting gợi ý tư duy điển hình, trong khi Self-planning và SCoT là các kỹ thuật tiên tiến trong loại này. Self-Debugging, Self-Edit, và Self-repair là các kỹ thuật prompting dựa trên phản hồi tiên tiến.

Lưu ý rằng µFiX là kỹ thuật đầu tiên tích hợp cả kỹ thuật prompting gợi ý tư duy và dựa trên phản hồi. Để đánh giá kỹ lưỡng, chúng tôi cũng kết hợp mỗi kỹ thuật prompting gợi ý tư duy tiên tiến (Self-planning và SCoT) với mỗi kỹ thuật prompting dựa trên phản hồi tiên tiến (Self-Debugging, Self-Edit, và Self-repair). Ví dụ, SCoT kết hợp với Self-repair (được gọi là SCoT + Self-repair) liên quan đến việc áp dụng Self-repair cho mã được tạo ra bởi SCoT. Tổng cộng, chúng tôi đã thực hiện 15 kỹ thuật so sánh như baseline. Với giới hạn độ dài đầu vào của LLM, chúng tôi sử dụng thiết lập 4-shot cho tất cả baseline few-shot theo công trình hiện tại [13], [46], [47].

V. KẾT QUẢ VÀ PHÂN TÍCH

A. RQ1: Hiệu quả Tổng thể của µFiX

1) Quá trình: Để trả lời RQ1, chúng tôi đã áp dụng µFiX và 15 kỹ thuật so sánh cho ChatGPT và DeepSeek-Coder. Sau đó chúng tôi đo lường hiệu quả của mỗi kỹ thuật trên 6 benchmark được sử dụng rộng rãi về Pass@1 và AvgPassRatio.

2) Kết quả: Bảng I và II cho thấy kết quả so sánh hiệu quả trên ChatGPT và DeepSeek-Coder, tương ứng. Chúng tôi thấy rằng SCoT vượt trội so với tất cả baseline prompting gợi ý tư duy, trong khi Self-repair xuất sắc so với tất cả baseline prompting dựa trên phản hồi, xác nhận hiệu quả của chúng như baseline tiên tiến trong loại tương ứng. Ngoài ra, mỗi sự kết hợp của kỹ thuật prompting gợi ý tư duy và dựa trên phản hồi vượt trội so với các kỹ thuật riêng lẻ tương ứng, xác nhận sự tương tác giữa cả hai loại, thúc đẩy kỹ thuật µFiX của chúng tôi.

Đặc biệt, µFiX đạt được hiệu quả tốt nhất trong tất cả các kỹ thuật được nghiên cứu, chứng minh tính ưu việt ổn định của nó trong cả hai chỉ số trên tất cả các chủ đề (hai LLM với sáu benchmark). Dựa trên ChatGPT, µFiX cải thiện đáng kể tất cả các kỹ thuật so sánh từ 16.02% ∼45.30% và 11.84% ∼40.03% về Pass@1 và AvgPassRatio trung bình trên tất cả sáu benchmark, tương ứng. Dựa trên DeepSeek-Coder, µFiX cải thiện đáng kể chúng từ 55.23% ∼114.92% và 16.83% ∼102.37% về Pass@1 và AvgPassRatio, tương ứng. Hơn nữa, Kiểm định Wilcoxon Signed-Rank [48] ở mức ý nghĩa 0.05 xác nhận rằng tất cả p-value đều nhỏ hơn 5.14e-6, chứng minh tính ưu việt có ý nghĩa thống kê của µFiX so với tất cả các kỹ thuật so sánh.

B. RQ2: Đóng góp của Mỗi Thành phần Chính trong µFiX

1) Biến thể: µFiX bao gồm hai giai đoạn: prompting gợi ý tư duy và dựa trên phản hồi. Để điều tra đóng góp của mỗi giai đoạn (bao gồm một số thành phần chính trong mỗi giai đoạn), chúng tôi đã tạo ra bốn biến thể của µFiX:

• µFiX woF: chúng tôi loại bỏ giai đoạn prompting dựa trên phản hồi khỏi µFiX, tức là nó chỉ sử dụng hiểu biết đặc tả được tạo ra trong giai đoạn prompting gợi ý tư duy để tạo mã. Nó có thể đo lường hiệu quả của giai đoạn prompting gợi ý tư duy riêng lẻ trong µFiX và cũng phản ánh đóng góp của giai đoạn prompting dựa trên phản hồi.

• µFiX SR: chúng tôi thay thế chiến lược prompting dựa trên phản hồi trong µFiX bằng chiến lược prompting Self-repair tiên tiến. Nó có thể điều tra hiệu quả của chiến lược prompting dựa trên phản hồi được thiết kế của chúng tôi.

• µFiX SCoT: chúng tôi thay thế chiến lược prompting gợi ý tư duy trong µFiX bằng chiến lược SCoT tiên tiến. Nó có thể điều tra thêm hiệu quả của chiến lược prompting gợi ý tư duy được thiết kế của chúng tôi bằng cách bổ sung cho µFiX woF.

• µFiX woS: chúng tôi loại bỏ thành phần tự cải thiện trong giai đoạn prompting gợi ý tư duy khỏi µFiX. Nó có thể điều tra đóng góp của việc xác định và tinh chỉnh hiểu nhầm trước khi tạo mã trong µFiX.

2) Quá trình: Do tài nguyên tính toán hạn chế và chi phí thời gian đánh giá, chúng tôi đã chọn ChatGPT làm LLM đại diện cho nghiên cứu ablation, vì nó đạt được hiệu quả tốt nhất với µFiX (trong Bảng I và II). Tương tự, chúng tôi sử dụng ChatGPT cho các thí nghiệm trong Phần VI. Cụ thể, chúng tôi áp dụng µFiX và bốn biến thể của nó cho ChatGPT tương ứng, và đo lường hiệu quả của mỗi kỹ thuật trên 6 benchmark được sử dụng rộng rãi về Pass@1 và AvgPassRatio.

3) Kết quả: Bảng III cho thấy kết quả so sánh giữa µFiX và bốn biến thể của nó về Pass@1 và AvgPassRatio. Đầu tiên, µFiX woF đạt được hiệu quả ưu việt so với tất cả 15 baseline về cả hai chỉ số (ngoại trừ SCoT+Self-repair trên APPS-ET về AvgPassRatio). Nó chứng minh hiệu quả của giai đoạn prompting gợi ý tư duy của chúng tôi. µFiX vượt trội so với µFiX SCoT với cải thiện trung bình 12.03% và 10.74% về Pass@1 và AvgPassRatio, tiếp tục xác nhận hiệu quả của chiến lược prompting gợi ý tư duy của µFiX, khi kết hợp với cùng chiến lược prompting dựa trên phản hồi (được thiết kế trong µFiX).

Thứ hai, µFiX vượt trội so với µFiX woF với cải thiện trung bình 10.25% về Pass@1 và 9.65% về AvgPassRatio, chứng minh đóng góp của giai đoạn prompting dựa trên phản hồi của chúng tôi. Hơn nữa, µFiX thể hiện hiệu quả ưu việt so với µFiX SR với cải thiện trung bình 5.26% về Pass@1 và 5.40% về AvgPassRatio, tiếp tục xác nhận tính ưu việt của chiến lược prompting dựa trên phản hồi của chúng tôi so với chiến lược Self-repair tiên tiến, khi kết hợp với cùng chiến lược prompting gợi ý tư duy (được thiết kế trong µFiX).

Thứ ba, µFiX thể hiện hiệu quả ưu việt so với µFiX woS với cải thiện trung bình 17.15% và 10.72% về Pass@1 và AvgPassRatio. Ngoài quá trình sửa chữa trong giai đoạn dựa trên phản hồi, µFiX cũng thực hiện một sửa chữa hiểu nhầm khác thông qua quá trình tự cải thiện trong giai đoạn prompting gợi ý tư duy. Kết quả xác nhận sự cần thiết của quá trình tự cải thiện trong µFiX.

Hơn nữa, Kiểm định Wilcoxon Signed-Rank [48] ở mức ý nghĩa 0.05 xác nhận rằng tất cả p-value đều nhỏ hơn 1.39e-4, chứng minh tính ưu việt có ý nghĩa thống kê của µFiX so với tất cả biến thể. Nhìn chung, mỗi thành phần chính đều có đóng góp vào hiệu quả tổng thể của µFiX.

C. RQ3: Ảnh hưởng của Siêu tham số

1) Thiết lập: µFiX liên quan đến ba siêu tham số chính: N (số lần tinh chỉnh), M (số lần điều chỉnh), và nhiệt độ giải mã của LLM (ký hiệu là T). Theo mặc định, chúng tôi đặt N=1 và M=1 để cân bằng hiệu quả và hiệu suất. Theo công trình hiện tại [49], [50], chúng tôi đặt T=0.7 cho cả hai LLM. Trong RQ này, chúng tôi điều tra hiệu suất của µFiX dưới các thiết lập khác nhau. Do chi phí đánh giá, chúng tôi xem xét N={1,2}, M={1,2}, và T={0.6,0.7,0.8,0.9,1.0}.

2) Kết quả: Bảng IV cho thấy việc tăng N và M nâng cao hiệu quả của µFiX. Cụ thể, µFiXN=2,M=2 vượt trội so với µFiXN=1,M=1 với cải thiện trung bình 4.46% và 4.03% về Pass@1 và AvgPassRatio, tương ứng, trên tất cả sáu benchmark và cả hai LLM. Tuy nhiên, N và M lớn hơn cũng phát sinh chi phí thời gian và token nhiều hơn. µFiXN=2,M=2 mất 44.54% thời gian hơn và 40.94% token hơn so với µFiXN=1,M=1. Chúng tôi sẽ thảo luận về hiệu suất của các kỹ thuật được nghiên cứu của chúng tôi chi tiết trong Phần VI-A. Do đó, chúng tôi sử dụng N=1 và M=1 làm thiết lập mặc định, vì µFiX dưới thiết lập này luôn vượt trội so với tất cả baseline trên tất cả benchmark và cả hai LLM và có chi phí thời gian và token thấp hơn so với các thiết lập khác với N và M lớn hơn, chứng minh hiệu quả chi phí của µFiX dưới thiết lập này.

Sau đó chúng tôi điều tra ảnh hưởng của nhiệt độ giải mã. Do giới hạn không gian, chúng tôi đặt kết quả chi tiết trên trang chủ của chúng tôi [51]. Ví dụ, µFiX dưới tất cả các thiết lập được nghiên cứu cho nhiệt độ giải mã, luôn vượt trội so với SCoT + Self-repair tiên tiến, bằng cách đạt được 16.02% ∼22.60% Pass@1 cao hơn và 11.84% ∼24.36% AvgPassRatio cao hơn trung bình trên tất cả sáu benchmark trên ChatGPT. Điều này chứng minh tính ưu việt ổn định của µFiX dưới các thiết lập nhiệt độ giải mã khác nhau.

VI. THẢO LUẬN

A. Hiệu suất

Chúng tôi đo lường chi phí thời gian và token về việc tạo mã. Trung bình, baseline hiệu quả nhất (prompting Zero-shot) mất 5.78s cho một nhiệm vụ lập trình, trong khi baseline hiệu quả nhất (SCoT + Self-repair) và µFiX mất 20.61s và 22.75s, tương ứng. Chi phí token trung bình cho Zero-shot, SCoT + Self-repair, và µFiX là 0.29K, 3.31K, và 3.44K, tương ứng. Chi phí thời gian và token của µFiX cao hơn một chút so với SCoT + Self-repair. Điều này là do µFiX liên quan đến nhiều lần gọi LLM hơn một chút so với các kỹ thuật prompting hiện tại. Ví dụ, kỹ thuật prompting gợi ý tư duy tiên tiến (SCoT) liên quan đến hai lần gọi LLM cho một nhiệm vụ lập trình, trong khi chiến lược prompting gợi ý tư duy của chúng tôi liên quan đến bốn lần gọi. Xét đến hiệu quả đáng kể của µFiX, một số chi phí bổ sung là chấp nhận được, minh họa sự cân bằng xuất sắc giữa chi phí và hiệu quả.

Các nghiên cứu gần đây [14], [25], [52] gợi ý rằng việc tăng lần gọi LLM có thể nâng cao hiệu suất tạo mã. Do đó, chúng tôi tiến hành thêm một thí nghiệm so sánh dưới cùng số lần gọi LLM để chứng minh hiệu quả của µFiX rõ ràng hơn. CodeT [25] là phương pháp tiên tiến để cải thiện việc tạo mã bằng cách tăng lần gọi LLM. Nó tạo ra nhiều instance mã ứng cử viên và xếp hạng chúng bằng việc thực thi test. Các chiến lược xếp hạng như vậy có thể được sử dụng để nâng cao prompting gợi ý tư duy do hiệu ứng trực giao của chúng [14], [45]. Do đó, chúng tôi kết hợp CodeT với kỹ thuật prompting gợi ý tư duy tiên tiến (SCoT) để so sánh với µFiX woF dưới cùng số lần gọi LLM. Cả hai kỹ thuật đều không liên quan đến prompting dựa trên phản hồi để so sánh công bằng. Chúng tôi sử dụng ChatGPT làm LLM đại diện.

Do giới hạn không gian, chúng tôi đặt kết quả chi tiết trên trang chủ của chúng tôi [51] và tóm tắt kết luận ở đây. µFiX woF đạt được 11.45% Pass@1 cao hơn và 6.93% AvgPassRatio cao hơn so với SCoT trung bình trên tất cả sáu benchmark. SCoT + CodeT cũng có thể vượt trội so với SCoT, nhưng chỉ đạt được 3.65% Pass@1 cao hơn và 3.55% AvgPassRatio cao hơn. Kết quả rõ ràng chứng minh hiệu quả của µFiX của chúng tôi.

B. Ảnh hưởng của Test Case

Đầu tiên, chúng tôi tiến hành thí nghiệm để điều tra ảnh hưởng của số lượng test case được sử dụng trong µFiX. Chúng tôi đặt số lượng test case thành {1, 2, 3+}, với ký hiệu 3+ chỉ việc sử dụng tất cả test case có sẵn trong đặc tả lập trình (thiết lập mặc định trong µFiX). Lưu ý rằng chúng tôi ngẫu nhiên chọn số lượng test case tương ứng từ toàn bộ tập hợp test case trong mỗi đặc tả lập trình. Nếu có ít test case hơn trong đặc tả so với mong muốn, chúng tôi sử dụng tất cả test case có sẵn. Chúng tôi sử dụng ChatGPT làm LLM đại diện trên ba benchmark (HumanEval, HumanEval+, và HumanEval-ET). Bảng V cho thấy hiệu quả của µFiX với các số lượng test case khác nhau về Pass@1 và AvgPassRatio. µFiX luôn thể hiện hiệu suất tốt hơn so với baseline hiệu quả nhất (SCoT + Self-repair), thậm chí chỉ với một test case. Khi số lượng test case tăng, hiệu suất tạo mã của µFiX cải thiện, chứng minh tầm quan trọng của việc sửa chữa hiểu nhầm đặc tả dựa trên test case cho việc tạo mã.

Thứ hai, mặc dù khá phổ biến khi một vài test case được bao gồm như một phần của đặc tả, chúng ta cần xem xét kịch bản khi test case vắng mặt, tiếp tục nâng cao tính tổng quát của µFiX. Để chứng minh khái niệm, chúng tôi sử dụng LLM để tự động tạo ra test case cho mỗi đặc tả lập trình (không có test case) theo công trình hiện tại [25], [26]. Chúng tôi gọi biến thể này là µFiX woT. Chúng tôi điều tra hiệu quả của µFiX woT bằng cách lấy ChatGPT làm mô hình đại diện trên cùng ba benchmark như trên. Chúng tôi đặt số lượng test case được tạo ra thành 3 vì hầu hết đặc tả lập trình bao gồm ba test case như đã đề cập trong Phần IV-B. Từ Bảng V, µFiX woT vượt trội đáng kể so với baseline hiệu quả nhất (SCoT + Self-repair) bằng 5.61% và 5.64% về Pass@1 và AvgPassRatio, tương ứng, điều này chứng minh hiệu quả thực tế của µFiX trong kịch bản như vậy. Ngoài ra, chất lượng test case có thể ảnh hưởng đến hiệu quả của µFiX. Chúng tôi phân tích thủ công một số trường hợp mà µFiX woT hoạt động kém hơn µFiX và thấy rằng test case được tạo ra bởi LLM chỉ bao gồm một nhánh được đề cập trong đặc tả hoặc thậm chí chứa lỗi, dẫn đến hiệu quả hơi tệ hơn. Trong tương lai, chúng tôi sẽ thiết kế các phương pháp tạo test tốt hơn, chẳng hạn như kết hợp coverage test để hướng dẫn việc tạo test dựa trên LLM, để nâng cao thêm hiệu quả của µFiX. Bên cạnh đó, tài nguyên có thể bị hạn chế trong thực tế. Trong các kịch bản như vậy, việc sử dụng ưu tiên test để chọn test case chất lượng cao hơn có thể giúp đảm bảo hiệu quả của µFiX. Chúng tôi coi việc khám phá đầy hứa hẹn này là công việc tương lai của chúng tôi.

C. So sánh với Các Kỹ thuật dựa trên Agent

Gần đây, một số kỹ thuật dựa trên agent tận dụng tương tác giữa các agent đã được đề xuất để nâng cao hiệu quả của LLM trong phát triển phần mềm. Chúng thực sự trực giao với µFiX ở mức độ lớn, và chúng tôi có thể sử dụng µFiX để nâng cao từng agent để tạo mã tốt hơn. Tuy nhiên, chúng tôi vẫn điều tra 4 kỹ thuật dựa trên agent tiên tiến (MetaGPT [53], Self-collaboration [10], ChatDev [54], và AgentCoder [55]) để so sánh. Chúng chia sẻ cùng insight cấp cao, mô phỏng quy trình phát triển phần mềm bằng cách gán vai trò cho một số agent LLM để cộng tác trong việc tạo mã. Sự khác biệt giữa chúng chủ yếu nằm ở việc chúng gán các vai trò khác nhau cho agent LLM và thiết kế các chiến lược khác nhau để giải quyết các nhiệm vụ tương ứng. Do không gian hạn chế, chi tiết hơn về các kỹ thuật này có thể được tìm thấy trong các bài báo tương ứng của chúng.

Chúng tôi sử dụng ChatGPT làm đại diện trong thí nghiệm này do chi phí đánh giá. Lưu ý rằng chúng tôi sử dụng số lần lặp mặc định cho MetaGPT, Self-collaboration, ChatDev, và AgentCoder trong thí nghiệm này, tương ứng là 3, 4, 10, và 5, và tất cả đều lớn hơn một lần lặp duy nhất của µFiX. Mặc dù sự so sánh này có thể không thuận lợi cho µFiX, µFiX vẫn vượt trội so với bốn kỹ thuật dựa trên agent bằng 19.23% ∼37.20% và 13.48% ∼60.33% về Pass@1 và AvgPassRatio, tương ứng, trung bình trên tất cả sáu benchmark. Do giới hạn không gian, chúng tôi đặt kết quả chi tiết trên trang dự án của chúng tôi [51]. Kiểm định Wilcoxon Signed-Rank [48] ở mức ý nghĩa 0.05 xác nhận rằng tất cả p-value đều nhỏ hơn 1.38e-3, chứng minh tính ưu việt có ý nghĩa thống kê của µFiX so với tất cả các kỹ thuật dựa trên agent được nghiên cứu. Đặc biệt, như được chứng minh trong Phần V-C, hiệu quả của µFiX có thể được nâng cao thêm với các lần lặp bổ sung (tức là tăng N và M). Trong số bốn kỹ thuật dựa trên agent, AgentCoder hiệu quả và hiệu suất nhất nhưng vẫn mất trung bình 211.75 giây và 63.77K token cho mỗi nhiệm vụ lập trình, tiêu thụ 830.77% thời gian hơn và 1753.78% token hơn so với µFiX. Nhìn chung, µFiX thể hiện tính ưu việt đáng kể so với các kỹ thuật dựa trên agent này.

D. Tính đúng đắn của Cơ chế Kiểm tra của chúng tôi

Đạt được tính tối ưu trong phân tích ngữ nghĩa thường là không thể quyết định được. Nhiều kỹ thuật hiện tại, ví dụ như các giải pháp dựa trên tìm kiếm trong kỹ thuật phần mềm, được biết là gặp phải tối ưu cục bộ [56]. Bản chất không thể giải thích được của LLM làm trầm trọng thêm khó khăn này. Do đó, không tồn tại một thủ tục quyết định để kiểm tra xem LLM có hiểu chính xác một đặc tả hay không, làm cho các cơ chế kiểm tra đáng tin cậy cho sự hiểu biết của LLM trở nên khó khăn. Những hạn chế tương tự như vậy là chung cho tất cả phương pháp dựa trên deep learning. Để giảm bớt thách thức này, trong µFiX, chúng tôi sử dụng một cơ chế kiểm tra trên nhiệm vụ tạo mã hạ lưu. Cụ thể, chúng tôi kiểm tra tính chính xác của mã được tạo ra trong giai đoạn prompting dựa trên phản hồi thông qua việc thực thi thực tế của test case vốn có được bao gồm trong đặc tả. Đó là, việc cải thiện hiểu biết được chứng minh bởi hiệu quả tốt hơn trong việc tạo mã hạ lưu. Nghiên cứu của chúng tôi thực sự xác nhận rằng cơ chế này giúp cải thiện hiệu quả của việc tạo mã, chứng minh thực nghiệm rằng việc tận dụng test case hiện có trong đặc tả có thể giảm bớt ảo giác của LLM.

E. Khám phá Kết hợp LLM trong µFiX

Chúng tôi khám phá hiệu quả của việc sử dụng các LLM khác nhau cho việc tinh chỉnh hiểu biết và tạo mã thông qua thí nghiệm sơ bộ. Do đặc tính cố hữu, khả năng lý luận của DeepSeek-Coder yếu hơn so với ChatGPT. Khi chúng tôi sử dụng ChatGPT cho việc tinh chỉnh hiểu biết và DeepSeek-Coder cho việc tạo mã, cải thiện là 1.55% và 1.01% về Pass@1 và AvgPassRatio, tương ứng, trung bình trên HumanEval, HumanEval+, và HumanEval-ET, so với việc sử dụng DeepSeek-Coder cho cả hai. Điều này chỉ ra tiềm năng của µFiX với các LLM phù hợp hơn trong các khía cạnh tương ứng, có thể được coi là công việc tương lai của chúng tôi.

VII. THREAT TO VALIDITY

Threat đầu tiên nằm ở tính tổng quát của kết quả thí nghiệm. Để giảm thiểu threat này, chúng tôi đã chọn lựa toàn diện các benchmark, chỉ số, baseline, và LLM. Theo các nghiên cứu trước đây [13], [14], [39], [40], chúng tôi đã chọn sáu benchmark được sử dụng rộng rãi trong việc tạo mã và sử dụng hai chỉ số để đánh giá tính chính xác của mã. Bên cạnh đó, chúng tôi sử dụng Pass@2, Pass@3, và CodeBLEU [43] để đo lường hiệu suất tạo mã (mặc dù CodeBLEU gặp phải một số hạn chế [14], [39], [57]), và đặt kết quả trên trang chủ của chúng tôi [51] do giới hạn không gian. Các kết luận nhất quán. Chúng tôi cũng đã chọn 9 kỹ thuật prompting điển hình hoặc tiên tiến (cũng như 6 kỹ thuật kết hợp) để so sánh và tiến hành đánh giá toàn diện trên hai LLM tiên tiến (tức là ChatGPT và DeepSeek-Coder). Trong tương lai, chúng tôi sẽ đánh giá µFiX để cải thiện hiệu suất tạo mã của LLM trên các benchmark toàn diện hơn.

Threat thứ hai nằm ở tính ngẫu nhiên liên quan đến LLM. Một mặt, nghiên cứu quy mô lớn và kết luận nhất quán của chúng tôi trên tất cả các chủ đề có thể giúp giảm threat này. Mặt khác, chúng tôi lặp lại thí nghiệm so sánh µFiX với baseline hiệu quả nhất (SCoT + Self-repair) trên ba benchmark (HumanEval, HumanEval+, và HumanEval-ET) làm đại diện ba lần do chi phí đánh giá khổng lồ. Độ lệch chuẩn cho SCoT + Self-repair và µFiX chỉ là 0.021 và 0.009 cho Pass@1, chứng minh tính mạnh mẽ của kết luận của chúng tôi ở mức độ lớn. Điều này tiếp tục giảm threat. Do giới hạn không gian, chúng tôi đặt kết quả chi tiết trên trang dự án của chúng tôi [51].

Threat thứ ba nằm ở thiết kế prompt trong µFiX. Chúng tôi không điều chỉnh đặc biệt định dạng mô tả bằng ngôn ngữ tự nhiên của prompt, và do đó không thể đảm bảo tính tối ưu của chúng. Chúng tôi thiết kế một số prompt tương tự thông qua paraphrase (ví dụ: thay thế từ đồng nghĩa và chuyển đổi câu chủ động-bị động), điều này không ảnh hưởng nhiều đến hiệu quả của µFiX. Chúng tôi sẽ điều tra tính mạnh mẽ của µFiX một cách có hệ thống trong tương lai. Hơn nữa, cấu trúc prompting trong µFiX chủ yếu nhấn mạnh đầu vào và đầu ra của test case, là các yếu tố test case thiết yếu bất kể đặc tả, cũng chứng minh tính tổng quát.

VIII. CÔNG TRÌNH LIÊN QUAN

Các kỹ thuật prompting đã được chứng minh hiệu quả để cải thiện hiệu suất tạo mã của LLM theo cách plug-and-play [9]–[13]. Nói chung, chúng có thể được chia thành hai loại chính: kỹ thuật prompting gợi ý tư duy và kỹ thuật prompting dựa trên phản hồi. Loại đầu nhằm thúc đẩy LLM tạo ra các bước lý luận trung gian để tạo mã chính xác hơn. Chúng tôi đã giới thiệu và so sánh các kỹ thuật CoT [9], Self-planning [13], và SCoT [14] trong Phần IV. Bên cạnh đó, KPC [58] là một kỹ thuật prompting dựa trên kiến thức, phân tách việc tạo mã thành các bước lý luận trung gian và sử dụng kiến thức chi tiết được trích xuất từ tài liệu API để tạo mã (đặc biệt trong mã xử lý ngoại lệ). Loại sau của các kỹ thuật prompting sử dụng thông báo lỗi được tạo ra từ việc thực thi test để cho phép LLM sửa mã được tạo ra không chính xác, chẳng hạn như SED [59], CodeRL [60], Self-Debugging [15], Self-Edit [16], và Self-repair [17]. Do chi phí đánh giá, chúng tôi không chọn tất cả các kỹ thuật prompting này trong nghiên cứu của mình mà chỉ chọn một số kỹ thuật điển hình hoặc tiên tiến làm baseline.

Khác với các kỹ thuật hiện tại, µFiX là kỹ thuật đầu tiên khám phá sự tương tác của cả hai loại bằng cách thiết kế cả prompting gợi ý tư duy tinh vi và prompting dựa trên phản hồi. Cốt lõi của việc cải thiện hiệu suất tạo mã của LLM là sửa chữa hiểu nhầm đặc tả của LLM trong mỗi giai đoạn.

IX. KẾT LUẬN

Trong công trình này, chúng tôi đề xuất một kỹ thuật prompting mới µFiX để cải thiện hiệu suất tạo mã của LLM. Khác với các kỹ thuật prompting hiện tại, µFiX thiết kế cả prompting gợi ý tư duy tinh vi và prompting dựa trên phản hồi, và khám phá sự tương tác của chúng. Chiến lược prompting gợi ý tư duy của chúng tôi trong µFiX khai thác phân tích test case và quá trình sửa chữa hiểu nhầm để thu được hiểu biết đặc tả chính xác hơn. Sau đó, chiến lược prompting dựa trên phản hồi của chúng tôi tiếp tục sửa chữa hiểu biết để giảm khoảng cách giữa hiểu biết được tinh chỉnh được cung cấp (từ giai đoạn đầu) và hiểu biết thực tế được LLM sử dụng ngầm định cho việc tạo mã. Chúng tôi đã tiến hành nghiên cứu rộng rãi trên hai LLM tiên tiến với sáu benchmark được sử dụng rộng rãi, chứng minh tính ưu việt của µFiX so với các kỹ thuật prompting tiên tiến.

LỜI CẢM ƠN

Công trình này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Chính của Quốc gia Trung Quốc (Số hiệu 2024YFB4506300), và Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số hiệu 62322208, 12411530122).

TÀI LIỆU THAM KHẢO

[1] OpenAI, "Chatgpt: Optimizing language models for dialogue." https://openai.com/blog/chatgpt, 2022.
[2] D. AI, "Deepseek coder: Let the code write itself." https://github.com/deepseek-ai/DeepSeek-Coder, 2024.
[3] Y. Ma, Y. Yu, S. Li, Y. Jiang, Y. Guo, Y. Zhang, Y. Xie, and X. Liao, "Bridging code semantic and llms: Semantic chain-of-thought prompting for code generation," arXiv preprint arXiv:2310.10698, 2023.
[4] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song et al., "Measuring coding challenge competence with apps," arXiv preprint arXiv:2105.09938, 2021.
[5] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy, "Challenges and applications of large language models," arXiv preprint arXiv:2307.10169, 2023.
[6] Z. Tian, J. Chen, and Z. Jin, "Code difference guided adversarial example generation for deep code models," in 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2023, pp. 850–862.
[7] Z. Tian, J. Chen, and X. Zhang, "On-the-fly improving performance of deep code models via input denoising," in 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2023, pp. 560–572.
[8] Z. Tian, H. Shu, D. Wang, X. Cao, Y. Kamei, and J. Chen, "Large language models for equivalent mutant detection: How far are we?" in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2024, pp. 1733–1745.
[9] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824–24 837, 2022.
[10] Y. Dong, X. Jiang, Z. Jin, and G. Li, "Self-collaboration code generation via chatgpt," arXiv preprint arXiv:2304.07590, 2023.
[11] C. Liu, X. Bao, H. Zhang, N. Zhang, H. Hu, X. Zhang, and M. Yan, "Improving chatgpt prompt for code generation," arXiv preprint arXiv:2305.08360, 2023.
[12] N. Nashid, M. Sintaha, and A. Mesbah, "Retrieval-based prompt selection for code-related few-shot learning," in Proceedings of the 45th International Conference on Software Engineering (ICSE'23), 2023.
[13] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li, "Self-planning code generation with large language model," arXiv preprint arXiv:2303.06689, 2023.
