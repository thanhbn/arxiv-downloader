VẼ PHÁC THẢO PROMPT CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN

Luca Beurer-Kellner, Mark Niklas Müller, Marc Fischer, Martin Vechev
Khoa Khoa học Máy tính
ETH Zurich, Thụy Sĩ
{luca.beurer-kellner, mark.mueller, marc.fischer, martin.vechev}@inf.ethz.ch

TÓM TẮT

Nhiều chiến lược prompting gần đây cho các mô hình ngôn ngữ lớn (LLM) truy vấn mô hình nhiều lần tuần tự – đầu tiên để tạo ra kết quả trung gian và sau đó là câu trả lời cuối cùng. Tuy nhiên, khi sử dụng các phương pháp này, cả bộ giải mã và mô hình đều không biết về các prompt tiếp theo có thể có, dẫn đến các phản hồi trung gian bị ngắt kết nối và không mong muốn dài dòng. Trong công trình này, chúng tôi giải quyết vấn đề này bằng cách đề xuất việc vẽ phác thảo prompt, một mô hình prompting mới trong đó một LLM không chỉ phản hồi bằng cách hoàn thành một prompt, mà bằng cách dự đoán giá trị cho nhiều biến trong một template. Theo cách này, việc vẽ phác thảo cấp cho người dùng nhiều quyền kiểm soát hơn đối với quá trình tạo, ví dụ, bằng cách cung cấp một khung suy luận thông qua các hướng dẫn trung gian, dẫn đến kết quả tổng thể tốt hơn. Ý tưởng chính cho phép vẽ phác thảo với các mô hình tự hồi quy hiện có là điều chỉnh thủ tục giải mã để cũng ghi điểm các hướng dẫn tiếp theo trong quá trình tạo văn bản, do đó tối ưu hóa khả năng template tổng thể trong suy luận. Các thí nghiệm của chúng tôi cho thấy rằng trong một thiết lập zero-shot, việc vẽ phác thảo prompt vượt trội hơn các sơ đồ prompting tuần tự hiện có như hỏi trực tiếp hoặc chain-of-thought trên 7 trong số 8 nhiệm vụ đánh giá LLM, bao gồm theo dõi trạng thái, suy luận số học, và trả lời câu hỏi chung. Để tạo điều kiện sử dụng trong tương lai, chúng tôi phát hành một số phác thảo chung, nhưng hiệu quả có thể áp dụng cho nhiều nhiệm vụ, và một thư viện mã nguồn mở gọi là dclib, cung cấp năng lượng cho các bộ giải mã nhận biết phác thảo của chúng tôi.

1 GIỚI THIỆU

Trong khi các chiến lược prompting sớm cho các mô hình ngôn ngữ lớn (LLM) tập trung vào các cụm từ kích hoạt đơn giản để gợi ra các phản hồi mong muốn (Kojima et al., 2022), công việc gần đây hơn xem xét các tương tác LLM đàm thoại (Ouyang et al., 2022), nhiều phần, và được hướng dẫn bằng template, nơi một mô hình được truy vấn nhiều lần theo cách bị hạn chế, dựa trên một template hoặc ngữ pháp. Điều này cung cấp quyền kiểm soát đối với suy luận LLM bằng cách điền vào một template của các bước được xác định trước (Beurer-Kellner et al., 2023; Lundberg and Ribeiro), cho phép giao tiếp với các hệ thống phần mềm tự động, và cho phép tạo mã đáng tin cậy về mặt cú pháp (Poesia et al., 2022).

Thách thức chính: Giải mã với các Ràng buộc Cấu trúc (Cứng) Chúng tôi xem xét một ứng dụng đơn giản, được minh họa trong Hình 1. Mục tiêu là tạo ra một danh sách các mục, thỏa mãn hai yêu cầu cứng: (1) kết quả phải là một danh sách có dấu gạch ngang của chính xác bốn mục và (2) mục thứ hai phải là Frisbee. Để đảm bảo rằng những yêu cầu này được thỏa mãn, chỉ prompting và fine-tuning thôi là không đủ, vì các LLM không bị ràng buộc vẫn vốn dĩ ngẫu nhiên, ngay cả với các hướng dẫn, minh chứng, hoặc huấn luyện tốt Arora et al. (2023); Zhao et al. (2021). Để giải quyết vấn đề này, suy luận được hướng dẫn bằng template xây dựng một template từ các ràng buộc (cứng), để lại nhiều lỗ cho LLM điền trong quá trình tạo (phía trên bên phải, Hình 1). Thật không may, chiến lược ngây thơ của việc gọi một mô hình không bị ràng buộc (Hình 1, trái) cho mỗi placeholder thường thất bại, vì mô hình chạy tiếp, vượt ra ngoài template, tạo ra nhiều mục cho mỗi placeholder. Một giải pháp thực tế là suy luận dừng-và-đi (giữa): Bằng cách cung cấp template từng phần, từng mục một, và thực thi các điều kiện dừng cho mỗi lời gọi, chúng ta có thể buộc đầu ra tổng thể tuân theo template. Trong khi phương pháp này hiệu quả cho việc định dạng đầu ra (Beurer-Kellner et al., 2023; Lundberg and Ribeiro), mô hình vẫn không biết về template tổng thể khi giải mã từng placeholder, dẫn đến các chiến lược suy luận không tối ưu. Ví dụ, trong Hình 1, dừng-và-đi tạo ra Frisbee làm mục đầu tiên, dẫn đến sự lặp lại của từ này, điều mà về mặt khác sẽ không có khả năng xảy ra dưới phân phối của mô hình. Ví dụ này đặt ra hai câu hỏi quan trọng: (1) Chúng ta có thể cải thiện suy luận dừng-và-đi ngây thơ bằng cách dự đoán template tổng thể trong quá trình tạo không? Và, (2) tác động chung của hình thức tạo bằng template này đối với hiệu suất mô hình tổng thể là gì, tức là, nó làm suy yếu hay cải thiện khả năng suy luận của mô hình?

Công việc này: Vẽ phác thảo Prompt Để trả lời những câu hỏi này, chúng tôi trình bày vẽ phác thảo prompt, một khung mới cho suy luận LLM được hướng dẫn bằng template. Sự khác biệt kỹ thuật chính của việc vẽ phác thảo so với các kỹ thuật trước đây là chúng tôi diễn đạt toàn bộ template như một vấn đề giải mã chuỗi phân đoạn, thay vì nhiều lời gọi mô hình bị cô lập. Điều này, (1) về mặt lý thuyết neo suy luận dừng-và-đi tiêu chuẩn như một trường hợp đặc biệt và (2) cho phép chúng tôi tổng quát hóa và triển khai các thủ tục giải mã mới, nhận biết phác thảo dựa trên tìm kiếm beam, tối ưu hóa template từ đầu đến cuối. Hình 1 so sánh giải mã nhận biết phác thảo (phải) với suy luận không bị ràng buộc (trái) và dừng-và-đi (giữa). Việc vẽ phác thảo cho phép chúng tôi tuân theo template prompt được cung cấp, đồng thời cũng tối ưu hóa nhiều biến placeholder cùng nhau, trong trường hợp này, tránh lặp lại Frisbee. Chúng tôi thực hiện một đánh giá thí nghiệm rộng rãi, cho thấy rằng việc vẽ phác thảo vượt trội hơn các phương pháp prompting không có template như chain-of-thought trên 7/8 nhiệm vụ suy luận LLM, chứng minh tính hiệu quả của suy luận được hướng dẫn bằng template trong suy luận chung. Trong các thí nghiệm của chúng tôi, việc vẽ phác thảo cho phép chúng tôi thực thi một cách nhất quán các chiến lược suy luận trên tất cả các nhiệm vụ, cho phép một hình thức lập trình LLM được kiểm soát hơn vượt ra ngoài prompting đơn giản. Đối với 5/8 nhiệm vụ, chúng tôi thậm chí quan sát thấy những cải thiện đáng kể so với templating dừng-và-đi đơn giản, chứng minh rằng giải mã nhận biết phác thảo và tối ưu hóa chung của nhiều biến là những thành phần quan trọng của suy luận LLM được hướng dẫn bằng template hiệu quả.

Đóng góp chính Những đóng góp cốt lõi của chúng tôi là:
• Một khung vẽ phác thảo prompt, diễn đạt suy luận LLM nhiều bước và được hướng dẫn bằng template như một vấn đề giải mã chuỗi phân đoạn.
• Hai thủ tục giải mã nhận biết phác thảo mới, chuyển giao một số hiểu biết từ giải mã chuỗi bị ràng buộc sang suy luận được hướng dẫn bằng template chung.
• Một bộ sưu tập các phác thảo prompt chung sẵn sàng sử dụng hoạt động tốt với một số nhiệm vụ suy luận LLM khó và có thể được điều chỉnh dễ dàng.
• Một đánh giá rộng rãi về việc vẽ phác thảo, bao gồm so sánh với suy luận không có template và dừng-và-đi, cũng như so sánh của một số chiến lược giải mã (nhận biết phác thảo).

Ngoài ra, chúng tôi xuất bản một thư viện mã nguồn mở dclib, được bao gồm trong tài liệu bổ sung (xem Phụ lục D), cho phép triển khai các thủ tục giải mã nhận biết phác thảo trên các mô hình OpenAI, transformers, và llama.cpp, do đó tạo điều kiện cho nghiên cứu trong tương lai và công việc mã nguồn mở.

2 KIẾN THỨC NỀN TẢNG

Trước tiên chúng tôi cung cấp kiến thức nền có liên quan về prompting và giải mã, trước khi thảo luận về vẽ phác thảo prompt.

Giải mã Hầu hết các mô hình ngôn ngữ gần đây chỉ hoạt động từ trái sang phải, tức là, chúng dự đoán một phân phối xác suất p(yt|y<t,x) trên token tiếp theo yt cho một chuỗi đầu vào x=⟨x1, x2, ..., xn⟩ và các token được dự đoán trước đó y<t=⟨y1, y2, ..., yt⟩. Do đó, một nhiệm vụ cốt lõi là chuyển đổi hoặc giải mã một đầu ra mô hình y⋆ tối đa hóa một số hàm tính điểm:

y⋆= arg max y∈Y score(y,x). (1)

Một lựa chọn phổ biến cho hàm tính điểm này là xác suất posterior hoặc xác suất chung được gán cho chuỗi được giải mã bởi mô hình ngôn ngữ. Điều này dẫn đến cái gọi là giải pháp posterior cực đại (MAP):

yMAP:= arg max y∈Y p(y|x) = arg max y∈Y ΠN t=1 p(yt|y<t,x) = arg max y∈Y ΣN t=1 log p(yt|y<t,x) (2)

Tuy nhiên, việc giải quyết giải mã MAP một cách chính xác nói chung là không thể thực hiện được, vì nó yêu cầu tất cả các xác suất có điều kiện p(yt|y<t,x) trên một không gian tìm kiếm lớn theo cấp số nhân được đánh giá. Để giải quyết vấn đề này, một loạt các chiến lược giải mã đã được giới thiệu, nhằm tìm các giải pháp xấp xỉ. Để thảo luận về chúng, việc tưởng tượng Y như một cây với prompt hoặc tiền tố x tại gốc và con của một nút tương ứng với các tiếp theo có thể, tất cả được ghi điểm bởi score(y<t,x) là hữu ích.

Giải mã ARGMAX tương ứng với tìm kiếm theo chiều sâu trước của cây giải mã của chúng ta kết thúc khi giải pháp đầu tiên được tìm thấy. Về mặt hoạt động, ở mỗi bước giải mã, chúng ta mở rộng giả thuyết y<(t−1) của mình bằng cách chọn token tiếp theo yt để tối đa hóa score(yt|y<(t−1),x):

yARGMAX := NM t=1 arg max yt∈Y p(yt|y<t,x) (3)

trong đó ⊕ biểu thị phép nối. Giải mã ARGMAX hiệu quả, nhưng cũng sẽ bỏ qua nhiều giả thuyết thay thế do bản chất tham lam của nó.

Tìm kiếm Beam tương ứng với tìm kiếm theo chiều rộng trước trong cây giải mã nơi chiều rộng (ở mỗi độ sâu cây) được giới hạn ở độ rộng beam n. Về mặt hoạt động, trước tiên chúng ta xác định n tiếp theo tốt nhất của tất cả n giả thuyết của chúng ta và sau đó giữ lại n cái tốt nhất trong tất cả những tiếp theo n2 này. Điều này mang lại các giải pháp chất lượng cao với chi phí tính toán vừa phải, làm cho Tìm kiếm Beam phổ biến trên một loạt rộng các nhiệm vụ. Thú vị là, các giải pháp thu được như vậy thường vượt trội hơn các giải mã chính xác (hoặc độ rộng beam rất lớn) trong các nhiệm vụ downstream (Holtzman et al., 2020). Meister et al. (2020) đề xuất rằng điều này là do tìm kiếm beam tạo ra một regularization hướng tới mật độ thông tin đồng đều, được ưa thích trong lời nói của con người.

Tìm kiếm Beam Lưới (Hokamp and Liu, 2017) mở rộng tìm kiếm beam để tạo điều kiện cho giải mã ràng buộc, tức là, chuyển đổi một phản hồi sao cho nó chứa các chuỗi nhất định hoặc thỏa mãn các ràng buộc. Vì các chuỗi tuân theo các ràng buộc như vậy thường đạt được điểm số thấp hơn nhiều so với các dự đoán mô hình tự nhiên, chúng sẽ không bao giờ được bao gồm khi sử dụng tìm kiếm beam vanilla. Tìm kiếm beam lưới giải quyết vấn đề này bằng cách giới thiệu các pool so sánh riêng biệt cho các giả thuyết thỏa mãn số lượng ràng buộc khác nhau. Để tránh sự gia tăng tuyến tính trong độ rộng beam và do đó chi phí tính toán trong số lượng ràng buộc, Post and Vilar (2018) giới thiệu một sơ đồ phân bổ beam động giữ độ rộng beam tổng cộng không đổi và gán các slot trên beam này tùy thuộc vào số lượng ràng buộc được thỏa mãn.

Chuẩn hóa Độ dài (Wu et al., 2016) thường được sử dụng để so sánh các chuỗi có độ dài khác nhau, để bù đắp cho việc tổng các logprob âm bổ sung. Chúng ta có thể đơn giản cân bằng hàm tính điểm của mình với một thuật ngữ chuẩn hóa độ dài, được tham số hóa bởi β∈R≥0 và α∈[0,1]:

w=(β+ 1)α (β+|y|)α, (4)

trong đó β= 0 và α= 1 khôi phục trung bình và α= 0 không chuẩn hóa.

3 VẼ PHÁC THẢO PROMPT

Cốt lõi của việc vẽ phác thảo prompt là suy luận LLM được hướng dẫn bằng template, tức là, luân phiên đầu ra mô hình với các token trung gian được dẫn xuất từ template. Điều này khác với các phương pháp prompting tuần tự như chain-of-thought hoặc answer-only, nơi đầu tiên, mô hình tiêu thụ một đầu vào như một câu hỏi hoặc hướng dẫn và sau đó tạo ra một câu trả lời theo cách không bị ràng buộc. Một cách chính thức hơn, chúng tôi xem xét một phác thảo S là một template có dạng S:= "< p1> [v2]. . .<pk−2> [vk−1] <pk>" trong đó, pi là các chuỗi token xác định, được chỉ định bởi template, và vi là các biến được hoàn thành bởi mô hình. Định nghĩa này nắm bắt các hình thức prompting hiện có, nơi ví dụ answer-only (AO) có thể được viết như SAO:= "<Q> A: [ANSWER]" và prompting chain-of-thought (CoT) như SCoT:= "<Q> A: Let's think step by step. [COT].", trong đó <Q> tương ứng với một câu hỏi và biến COT chứa suy luận mô hình cũng như câu trả lời cuối cùng.

Phác thảo Biến Đơn vs. Đa Chúng tôi xem xét SAO và SCoT như các phác thảo tuần tự, biến đơn, vì biến được đặt ở cuối template. Do đó, mô hình trước tiên tiêu thụ tất cả thông tin được cung cấp như một câu hỏi và hướng dẫn suy luận trước khi tạo ra câu trả lời. Ngược lại, với các phác thảo tổng quát hơn, giá trị cho nhiều biến có thể được tạo ra, và các hướng dẫn trung gian xác định có thể được chèn trong quá trình tạo. Các ví dụ hiện có của các vấn đề đa biến bao gồm các hệ thống đàm thoại như ChatGPT, prompting agentic như ReAct (Yao et al., 2022a), lập trình mô hình ngôn ngữ (Beurer-Kellner et al., 2023), và các cascade mô hình ngôn ngữ (Dohan et al., 2022).

Giải mã Phác thảo Tự hồi quy Việc vẽ phác thảo mở rộng phạm vi các chiến lược giải mã vượt ra ngoài chỉ tạo tuần tự. Tuy nhiên, hầu hết các mô hình ngôn ngữ vẫn là các dự đoán token tiếp theo đơn giản, tức là, cho một prompt x, chúng tạo ra một chuỗi token y tự hồi quy, tức là, một token một lúc, chỉ có điều kiện trên các token được tạo trước đó:

p(y|x) = |y|Y i=1 p(yi|x, y<i) (5)

Để căn chỉnh điều này với việc vẽ phác thảo, chúng tôi chia chuỗi được tạo y={y1, . . . , yn}, bao gồm cả các phần xác định và biến, thành k chunk liên tiếp Cy={c1, . . . , ck} có độ dài n1, . . . , nk tương ứng, tức là, Cy= {{y1, . . . , yn1}, . . . ,{yn(k−1)+1, . . . , ynk}}. Mỗi chunk trong Cy sau đó được liên kết với một phần prompt xác định pi hoặc một biến được dự đoán bởi mô hình vi. Xác suất chung tổng thể của tất cả các chunk sau đó được định nghĩa là

p(c1, . . . , ck) = kY j=1 njY i=nj−1+1 p(yi|y<i) (6)

Quan trọng, chúng tôi dẫn xuất các giá trị của tất cả các chunk từ một chuỗi token đơn y, có thể được dự đoán tuần tự bằng một mô hình tự hồi quy. Một chuỗi được phân vùng chunk và biến sau đó có thể được tận dụng bởi các thuật toán giải mã để có được các phản hồi chất lượng cao hơn hoặc tiêm các cụm từ xác định trong quá trình tạo. Thách thức chính của phương pháp này là chiến lược chunking, tức là, một cách để chia một chuỗi token được tạo y thành các chunk có độ dài không biết trước để xác định biến nào chúng nên được liên kết với.

Chunking với Cụm từ Dừng Giống như trong suy luận dừng-và-đi, việc vẽ phác thảo dựa vào việc sử dụng các cụm từ dừng theo biến (SP). SP được chỉ định như một phần của phác thảo prompt và chấm dứt việc tạo chunk hiện tại i khi xuất hiện. Điều này cho phép chúng ta chunk chuỗi đầu ra y, gán các chuỗi con kết quả cho các biến vi, và giữ mô hình khỏi chạy tiếp mà không tôn trọng template phác thảo. Trong trường hợp không có cụm từ dừng được chỉ định nào xuất hiện trước khi mô hình dự đoán token end-of-sequence được chỉ định của nó, chúng ta không chấm dứt toàn bộ quá trình tạo, mà chỉ chấm dứt việc giải mã chunk hiện tại, trừ khi không còn chunk nào.

Chunk Xác định và Bị ràng buộc Để tiêm các cụm từ xác định trong quá trình tạo, chúng ta buộc một chuỗi được xác định trước pi được giải mã, đồng thời vẫn đánh giá khả năng p(ci|c<i) của nó. Hơn nữa, chúng tôi xem xét các biến bị ràng buộc như một trường hợp đặc biệt của các biến không xác định, có giá trị được dự đoán bởi mô hình, nhưng chỉ có thể được chọn từ một tập hợp hạn chế các chuỗi (ví dụ, chỉ số, khớp với một biểu thức chính quy, v.v.). Để triển khai các biến bị ràng buộc, chúng tôi dựa vào ngôn ngữ truy vấn LMQL cho LLM (Beurer-Kellner et al., 2023). Điều này cho phép chúng tôi che tất cả các token sẽ không thỏa mãn một ràng buộc nhất định trong quá trình tạo, sao cho giá trị kết quả của một số biến bị hạn chế ci được đảm bảo thỏa mãn ràng buộc.

Ví dụ Chúng tôi hiển thị hai template phác thảo ví dụ trong Hình 2. Trong ví dụ Khung Suy luận, chúng tôi hướng dẫn quá trình suy luận của mô hình bằng cách chèn các cụm từ xác định như "On the one hand", "On the other hand", hoặc "In conclusion" giữa các bước suy luận được tạo. Trong ví dụ Suy luận Xen kẽ, chúng tôi cung cấp cho mô hình định nghĩa vấn đề của chúng ta, ví dụ câu bằng câu như các chunk Qi, prompting cho các kết quả trung gian sau mỗi câu. Khi mô tả vấn đề đầy đủ đã được cung cấp cho mô hình, chúng tôi tạo ra kết luận tổng thể và câu trả lời.

3.1 GIẢI MÃ NHẬN BIẾT PHÁC THẢO

Việc vẽ phác thảo cho phép chúng tôi biểu thị suy luận LLM được hướng dẫn bằng template như một vấn đề giải mã chuỗi phân đoạn dài. Với giải mã ARGMAX tham lam và mô hình tự hồi quy chỉ có điều kiện trên các token được tạo trước đó, điều này khôi phục suy luận dừng-và-đi. Như đã thảo luận trong Phần 1, tuy nhiên, hình thức giải mã tuần tự này không tính đến các phần sắp tới của template. Đồng thời, chúng ta hoạt động một cách tham lam nên sau khi một chunk xác định đã được chèn, chúng ta không thể thay đổi hồi tố các giá trị biến được tạo trước đó.

Để giải quyết điều này, chúng tôi tận dụng hiểu biết xác suất về việc vẽ phác thảo và đề xuất một lớp mới các thủ tục giải mã mà, trái ngược với các bộ giải mã cấp token truyền thống, hoạt động ở cấp độ template để hướng dẫn quá trình giải mã từ đầu đến cuối. Cụ thể, chúng tôi thử nghiệm với hai điều chỉnh bộ giải mã mới, cụ thể là: (1) Tìm kiếm Beam Cấp Biến Phân cấp (VAR) và (2) Tìm kiếm Beam Dựa trên Lưới (BEAM VAR). Tiếp theo, chúng tôi thảo luận về việc triển khai các phương pháp này chi tiết hơn.

VAR: Tìm kiếm Beam Cấp Biến dựa trên ý tưởng áp dụng tìm kiếm beam ở cấp độ các biến placeholder được giải mã. Điều này có nghĩa là thay vì mở rộng mỗi giả thuyết hoạt động bằng n token tiếp theo có khả năng nhất, chúng ta mở rộng nó bằng n giá trị được lấy mẫu cho biến hiện tại được giải mã. Bắt đầu với một chuỗi token rỗng, chúng ta giải mã biến theo biến. Khi ở biến vi, chúng ta có nhiều nhất n giả thuyết mà các biến v<i đã được chọn. Đối với mỗi cái trong số chúng, chúng ta sau đó tạo ra n đề xuất cho biến vi, do đó cung cấp cho chúng ta n2 giả thuyết trên các biến v≤i. Trong số này, chúng ta sau đó chọn n cái có khả năng nhất theo điểm số mô hình và chuyển sang biến tiếp theo. Các chunk xác định được xử lý bằng cách nối chúng vào tập hợp các giả thuyết hoạt động cùng một lúc. Quá trình này được lặp lại cho đến khi tất cả các biến đã được giải mã. Xem Phụ lục A, để có triển khai pseudo-code của VAR.

BEAM VAR: Tìm kiếm Beam Lưới Biến dựa trên ý tưởng rằng số lượng biến được giải mã là một thước đo quan trọng của tiến độ giải mã và do đó nên được xem xét khi so sánh điểm số của các chuỗi khác nhau trong tìm kiếm beam cấp token, để quyết định cái nào cần khám phá thêm. Điều này đặc biệt quan trọng khi có các chunk xác định, mà theo bản chất của chúng, thường có khả năng thấp hơn dưới phân phối mô hình so với các biến không xác định và do đó sẽ không bao giờ được bao gồm trong một giả thuyết được giải mã. Để làm điều này, chúng tôi điều chỉnh phương pháp phân bổ beam động của Post and Vilar (2018) cho thiết lập vẽ phác thảo và đề xuất Tìm kiếm Beam Lưới Biến (BEAM VAR): Chúng tôi phân vùng độ rộng beam của mình thành các pool riêng biệt tùy thuộc vào biến hiện tại được giải mã vi và chỉ so sánh điểm số cho mỗi pool. Để quyết định bao nhiêu slot để phân bổ cho mỗi pool và do đó biến, chúng tôi chia độ rộng beam cho số lượng biến hiện tại được giải mã duy nhất và phân bổ phần còn lại cho pool có nhiều biến được giải mã nhất, gán lại các slot không sử dụng cho các pool giải mã các biến sau, để đảm bảo tiến độ ở cấp độ template. Một triển khai pseudo-code của BEAM VAR có thể được tìm thấy trong Phụ lục A.

4 ĐÁNH GIÁ THÍ NGHIỆM

Chúng tôi tập trung đánh giá của mình vào các câu hỏi sau: (1) Suy luận được hướng dẫn bằng template và việc vẽ phác thảo có hiệu quả trong việc cải thiện hiệu suất của LLM trên các nhiệm vụ suy luận không? (2) Các bộ giải mã nhận biết phác thảo có thể vượt trội hơn các bộ giải mã hiện có trong và ngoài thiết lập vẽ phác thảo không? Và (3), loại nhiệm vụ nào hưởng lợi nhiều nhất từ việc vẽ phác thảo? Để trả lời những câu hỏi này, chúng tôi so sánh hiệu suất mô hình với suy luận tuần tự không có template trên một loạt rộng các benchmark suy luận khác nhau cho LLM (Phần 4.1) và cũng thử nghiệm với các ứng dụng mới được kích hoạt bởi việc vẽ phác thảo prompt (Phần 4.2).

Mô hình Chúng tôi sử dụng mô hình text-davinci-003 InstructGPT của OpenAI (175B tham số; Ouyang et al. (2022)) và Llama-2 Chat (13B tham số; Llama-2 trong phần sau; Touvron et al. (2023)) để đánh giá. Trong khi text-davinci-003 rõ ràng là mô hình có khả năng hơn, chúng tôi thấy rằng Llama-2 cung cấp một điểm so sánh thú vị cho khả năng áp dụng của việc vẽ phác thảo cho các mô hình nhỏ hơn, mở hơn. Chúng tôi cũng thử nghiệm với mô hình text-curie-001 nhỏ hơn của OpenAI, nhưng nhìn chung, thấy rằng khả năng tuân theo hướng dẫn của nó không đủ để hỗ trợ việc vẽ phác thảo (nghiên cứu so sánh trong Phụ lục C.2).

Baseline Như một baseline, chúng tôi so sánh việc vẽ phác thảo với các công thức zero-shot không có template của answer-only (AO) và chain-of-thought (CoT), sử dụng zero-shot CoT (Kojima et al., 2022) cho cái sau. Các ví dụ về tất cả prompt/phác thảo được sử dụng được đưa ra trong Phụ lục E. Trong quá trình tạo, không có minh chứng nhiệm vụ nào được cung cấp và mô hình được prompt với các hướng dẫn đơn giản chỉ. Điều này làm nổi bật một lợi ích cốt lõi của việc vẽ phác thảo: khả năng hướng dẫn mô hình một cách chính xác trong quá trình tạo mà không cần minh chứng cụ thể. Tuy nhiên, chúng tôi cũng bao gồm một so sánh với prompting few-shot trong Phụ lục C.1, thường trực giao với việc vẽ phác thảo.

Tập dữ liệu và Phác thảo Chúng tôi đánh giá trên tổng cộng 8 nhiệm vụ suy luận LLM. Đối với mỗi nhiệm vụ, chúng tôi áp dụng một trong hai template phác thảo chung: Đối với suy luận số học và logic, hiểu ngày tháng, và trả lời câu hỏi chung, chúng tôi dựa vào một hình thức phác thảo của chain-of-thought, như được hiển thị trong Hình 3. Đối với theo dõi trạng thái và suy luận hình dạng ma trận, chúng tôi sử dụng một phác thảo suy luận xen kẽ, như được hiển thị trong Hình 2, chia mô tả nhiệm vụ thành các câu và xen kẽ chúng với các bước suy luận của mô hình. Để có mô tả chi tiết về các nhiệm vụ và phác thảo, chúng tôi tham khảo Phụ lục E.

Tính toán và Kích thước Tập dữ liệu Tổng chi phí của các thí nghiệm OpenAI của chúng tôi là khoảng $4,000 USD trong việc sử dụng API. Để giới hạn những chi phí này cụ thể cho các thí nghiệm OpenAI của chúng tôi, chúng tôi chỉ đánh giá 100 mẫu ngẫu nhiên đồng đều cho mỗi cấu hình nhiệm vụ-bộ giải mã, với các giới hạn tin cậy được báo cáo trong Phụ lục C.3. Đối với Llama-2, mặt khác, chúng tôi chạy tất cả các thí nghiệm của mình trên 1000 mẫu cho mỗi nhiệm vụ (hoặc tập dữ liệu đầy đủ), sử dụng một GPU NVIDIA H100 đơn với bộ nhớ 80GB.

Cấu hình Bộ giải mã Như một baseline cho các thủ tục giải mã nhận biết phác thảo của chúng tôi, chúng tôi so sánh với ARGMAX và tìm kiếm beam truyền thống (BEAM), được áp dụng cho từng biến phác thảo riêng lẻ. Dựa trên điều này, chúng tôi kiểm tra lợi ích của việc vẽ phác thảo có và không có các bộ giải mã nhận biết phác thảo VAR và BEAM VAR của chúng tôi. Đối với BEAM, VAR, và BEAM VAR, chúng tôi sử dụng độ rộng beam n= 2 và dựa vào tính điểm chuẩn hóa độ dài phù hợp với công việc trước đó (Wu et al., 2016), sử dụng β= 0 và α= 0.7.

4.1 ĐỘ CHÍNH XÁC NHIỆM VỤ

Trong Bảng 1 và 2, chúng tôi báo cáo các kết quả chính của mình về hiệu suất nhiệm vụ với text-davinci-003 và Llama-2, tương ứng. Chỉ xem xét giải mã ARGMAX, chúng tôi liên tục quan sát hiệu suất được cải thiện hoặc duy trì với việc vẽ phác thảo, so với CoT hoặc AO tuần tự (7 trong số 8 được cải thiện cho text-davinci-003, 6 trong số 8 với Llama-2). Điều này cho thấy rằng việc vẽ phác thảo ARGMAX đơn giản đã có thể hiệu quả (cải thiện tới 4% và 8% điểm cho text-davinci-003 và Llama-2 tương ứng). Kiểm tra thủ công tiết lộ rằng việc vẽ phác thảo luôn dẫn đến suy luận có cấu trúc rõ ràng, trong khi với CoT mô hình đưa ra lựa chọn dường như ngẫu nhiên về hình thức suy luận được áp dụng cho mỗi mẫu (văn bản đơn giản, danh sách các bước, v.v.), làm suy giảm độ chính xác nhiệm vụ (xem Phụ lục E để có ví dụ chi tiết).

Llama-2 phần lớn xác nhận kết quả của chúng tôi cho text-davinci-003. Hai ngoại lệ là nhiệm vụ hình dạng ma trận và tập dữ liệu AQuA Ling et al. (2017). Đối với cả hai, Llama-2 thể hiện hiệu suất rất tệ trên tất cả các chiến lược giải mã và prompting, cho thấy rằng mô hình có khả năng không thể thực hiện các nhiệm vụ này cả. Chúng tôi quy điều này cho sự khác biệt về kích thước mô hình khi so sánh với OpenAI. text-davinci-003 có 175 tỷ tham số, trong khi biến thể Llama-2 chỉ có 13 tỷ tham số. Như được hiển thị bởi Kojima et al. (2022), kích thước mô hình liên quan trực tiếp đến sự gia tăng khả năng suy luận chung.

Bộ giải mã Kết hợp các phác thảo đơn giản với giải mã nhận biết phác thảo, chúng tôi quan sát những cải thiện hiệu suất mạnh mẽ hơn nữa lên tới 10% điểm, ví dụ, cho BEAM VAR so với prompting tuần tự với ARGMAX hoặc BEAM trên các tập dữ liệu trả lời câu hỏi AQuA (Ling et al., 2017) và StrategyQA (Geva et al., 2021) với text-davinci-003. So sánh VAR và BEAM VAR, chúng tôi quan sát VAR hoạt động đặc biệt tốt trên các nhiệm vụ dựa vào suy luận xen kẽ trong khi BEAM VAR hiệu quả hơn trong các thiết lập khác. Đối với Llama-2, chúng tôi quan sát các hiệu ứng tương tự, ví dụ, BEAM VAR cải thiện hiệu suất trên Date Understanding và GSM8K gần 7% điểm, so với CoT không có template và ARGMAX đơn giản.

Đối với text-davinci-003, chúng tôi cũng quan sát những cải thiện hiệu suất đáng chú ý lên tới 6% điểm, khi sử dụng các bộ giải mã nhận biết phác thảo của chúng tôi kết hợp với sơ đồ prompting Zero-Shot CoT đã được thiết lập (Kojima et al., 2022) (cf. Bảng 1). Điều này là bởi vì Zero-Shot CoT đã là một sơ đồ prompting hai phần, mà tự nhiên hưởng lợi từ các bộ giải mã nhận biết phác thảo của chúng tôi, cho phép chúng tối ưu hóa quá trình suy luận (biến đầu tiên) và câu trả lời cuối cùng (biến thứ hai) cùng nhau.

4.2 CÁC ỨNG DỤNG MỚI ĐƯỢC KÍCH HOẠT BỞI VẼ PHÁC THẢO PROMPT

Ngoài hiệu suất suy luận, việc vẽ phác thảo cũng cho phép các ứng dụng mới, mà suy luận tuần tự không có template hoặc thất bại hoàn toàn hoặc kém hiệu quả và đáng tin cậy hơn nhiều. Chúng tôi làm nổi bật nhiều kịch bản ở đây (sắp xếp lại nguyên nhân, sudoku, môi trường tương tác) và mở rộng về chúng trong Phụ lục B (tạo JSON và duyệt đồ thị).

Sắp xếp lại Nguyên nhân Đầu tiên, chúng tôi điều tra khả năng tham chiếu về phía trước với các bộ giải mã nhận biết phác thảo của chúng tôi. Cụ thể hơn, chúng tôi kiểm tra liệu các bộ giải mã nhận biết phác thảo có cho phép mô hình dự đoán thông tin trong tương lai ở một mức độ nào đó không. Để làm điều này, chúng tôi điều chỉnh tập dữ liệu Information Essentiality hiện có (Srivastava et al., 2022), bằng cách sắp xếp lại nó theo template được hiển thị trong Hình 4. Mô hình phải xác định tính cần thiết của hai tuyên bố <S1> và <S2>, đối với một câu hỏi đã cho <Q>. Tuy nhiên, trong prompt được sắp xếp lại của chúng tôi, biến kết quả IS_NEEDED1 được giải mã trước khi <Q> được hiển thị. Đối với nhiệm vụ tùy chỉnh này (cf. Bảng 1), chúng tôi thực sự quan sát rằng ARGMAX không có khả năng tạo ra bất kỳ kết quả có ý nghĩa nào (độ chính xác 0.01), trong khi, BEAM VAR và VAR đạt được độ chính xác cải thiện lần lượt là 0.25 và 0.06, bằng cách khám phá một không gian giả thuyết rộng hơn.

Sudoku Chúng tôi tiếp tục kiểm tra khả năng của một mô hình để giải quyết các câu đố 3×3 sudoku đơn giản: LLM được giao nhiệm vụ hoàn thành một lưới từng phần với các số duy nhất trong 1−9. Tương tự như trước, nhiệm vụ này yêu cầu tham chiếu về phía trước để hiệu quả chọn các số đúng. Như được hiển thị trong Bảng 3, trong số 10 câu đố với 1−6 chỗ trống, giải mã ARGMAX tuần tự chỉ có thể giải quyết một. Điều này được mong đợi, vì giải mã tham lam không cho phép dự đoán bất kỳ thông tin trong tương lai nào (tức là số cố định), trước khi chọn những số trước đó. Ngược lại, BEAM VAR và VAR giải quyết lần lượt 6/10 và 7/10 câu đố, một lần nữa chứng minh rằng chúng khám phá một không gian giả thuyết rộng hơn. Một giải pháp thay thế tiềm năng là sắp xếp lại template, cho phép text-davinci-003 đạt được độ chính xác hoàn hảo với ARGMAX, nhưng sắp xếp lại không phải lúc nào cũng là một lựa chọn với các câu đố nhiều bước phức tạp hơn.

Môi trường Tương tác Các bộ giải mã nhận biết phác thảo có thể tính đến hiệu ứng của các tiếp theo được tạo ra bởi template trong quá trình tạo văn bản. Nếu chúng ta chọn những tiếp theo này một cách động dựa trên đầu ra mô hình trước đó, chúng ta có thể hiệu quả tận dụng chúng để khám phá các môi trường tương tác (Driess et al., 2023). Để làm điều này, chúng tôi triển khai một duyệt đồ thị được hướng dẫn bởi LLM đơn giản, nơi một agent LLM duyệt một dungeon, bắt đầu trong một phòng được chọn ngẫu nhiên, với mục tiêu tìm lối thoát. Chúng tôi tạo ra 10 dungeon ngẫu nhiên với 8−10 phòng mỗi cái, nơi tuyến đường thoát ngắn nhất trung bình cách 2.3 bước. Tại mỗi nút, mô hình được hỏi về phòng/nút tiếp theo để duyệt tới. Như được hiển thị trong Bảng 3, ARGMAX chủ yếu tìm thấy lối thoát, nhưng thường yêu cầu nhiều bước hơn (trung bình 3.77) so với VAR/BEAM VAR. Đặc biệt BEAM VAR luôn tìm thấy lối thoát và hầu như luôn thông qua tuyến đường ngắn nhất (∼2.4 bước yêu cầu). Đối với text-curie-001, chúng tôi quan sát hiệu suất tương tự với BEAM VAR, trong khi ARGMAX thường không có khả năng tìm lối thoát trong giới hạn 10 bước (chỉ 5/10 thành công). Chúng tôi tham khảo Phụ lục B.2 để biết thêm chi tiết về công thức phác thảo và môi trường tương tác.

4.3 THẢO LUẬN

Đánh giá của chúng tôi cho thấy rằng việc vẽ phác thảo và, theo mở rộng, suy luận LLM được hướng dẫn bằng template nói chung, có thể cải thiện đáng kể khả năng suy luận của mô hình. Ở đây, chúng tôi thảo luận ngắn gọn về những hạn chế và các cân nhắc khác liên quan đến các khía cạnh thiết kế, tính toán, và khả năng áp dụng.

Thiết kế và Lặp lại Phác thảo Trong khi vẫn nhạy cảm với cách diễn đạt, việc vẽ phác thảo prompt thực sự cung cấp nhiều quyền kiểm soát hơn đối với hành vi mô hình chính xác, do đó giải quyết một số khó khăn của thiết kế prompt truyền thống (Reynolds and McDonell, 2021; Arora et al., 2023; Zhao et al., 2021). Tuy nhiên, việc vẽ phác thảo cũng không phải là một viên đạn bạc: Quan trọng nhất, chúng tôi thấy rằng một phác thảo hiệu quả không được quá hạn chế để không làm suy giảm hiệu suất mô hình. Tuy nhiên, như được chứng minh bởi kết quả của chúng tôi, ngay cả các phác thảo đơn giản đã có thể hiệu quả trong việc cải thiện khả năng suy luận. Cuối cùng, giống như các prompt không có template, các phác thảo vẫn yêu cầu phát triển và điều chỉnh lặp đi lặp lại để đạt được hiệu suất tối ưu trên một nhiệm vụ đã cho. Quan trọng hơn, tuy nhiên, chúng cung cấp những lợi ích như cải thiện quyền kiểm soát, định dạng đầu ra được đảm bảo, và giảm hướng dẫn định dạng văn bản tự do, cần thiết khác.

Khả năng Áp dụng Trong khi thiết kế phác thảo vẫn yêu cầu một số nỗ lực, chúng tôi thấy rằng nhiều nhiệm vụ trong đánh giá của chúng tôi có thể được giải quyết với một tập hợp nhỏ các phác thảo chung. Ví dụ, chúng tôi thấy rằng một hình thức phác thảo của chain-of-thought (Wei et al., 2022a) (xem Hình 3) đã hiệu quả cho một loạt rộng các nhiệm vụ, bao gồm suy luận số học và trả lời câu hỏi chung. Để áp dụng trực tiếp, chúng tôi cũng xuất bản các phác thảo được sử dụng trong đánh giá của chúng tôi, có thể được điều chỉnh hoặc sử dụng như hiện tại bởi các practitioner.

Chi phí Tính toán của Giải mã Nhận biết Phác thảo Các bộ giải mã nhận biết phác thảo tự nhiên phát sinh chi phí tính toán so với tìm kiếm tham lam đơn giản. Trong khi BEAM VAR yêu cầu nhiều tính toán như tìm kiếm beam thông thường, VAR yêu cầu một yếu tố bổ sung của độ rộng beam n nhiều giả thuyết hơn được theo dõi song song. Tương tự như tìm kiếm beam thông thường, đây là một đánh đổi được biết đến rõ: các bộ giải mã phân nhánh đắt hơn nhưng vẫn được sử dụng rộng rãi, đặc biệt khi hiệu suất và tính đa dạng được cải thiện là quan trọng.

5 CÔNG VIỆC LIÊN QUAN

Prompting Các công việc gần đây đã đề xuất nhiều kỹ thuật prompting khác nhau bao gồm prompting chain-of-thought (Wei et al., 2022a;b), trả lời câu hỏi tương tác (Yao et al., 2022b), self-consistency (Wang et al., 2022), và ThinkSum (Ozturkler et al., 2022). Những kỹ thuật lập trình prompt này (Reynolds and McDonell, 2021; Zhou et al., 2022), nhằm tận dụng khả năng suy luận chung của LLM để giải quyết các nhiệm vụ đa dạng. Để cho phép triển khai hiệu quả của các kỹ thuật prompting phức tạp như vậy, các hệ thống lập trình tập trung vào LM gần đây đã được giới thiệu: PromptChainer (Wu et al., 2022), PromptSource (Bach et al., 2022), và LMQL (Beurer-Kellner et al., 2023) cung cấp môi trường phát triển cho tương tác LM. Chúng tôi xây dựng trên LMQL, vì nó hỗ trợ các ràng buộc biến và luồng điều khiển trong prompt, cho phép biểu diễn hiệu quả của phác thảo. Cuối cùng, các cascade mô hình ngôn ngữ (Dohan et al., 2022) xem việc truy vấn LM như lập trình xác suất trên nhiều biến, do đó ngầm giả định một thiết lập vẽ phác thảo và mở ra những quan điểm thú vị cho các bộ giải mã tiên tiến hơn trong tương lai. Trái ngược với việc vẽ phác thảo prompt, tuy nhiên, các công việc hiện có kết hợp nhiều lời gọi LLM theo cách bị ngắt kết nối, và, quan trọng, không xem xét khả năng tổng thể của chuỗi kết quả.

Giải mã Mô hình Ngôn ngữ Hầu hết các kỹ thuật giải mã hoặc nhằm khôi phục gần đúng giải pháp posterior cực đại dưới phân phối mô hình hoặc lấy mẫu từ nó với mục đích tăng tính đa dạng. Ngoài việc lấy mẫu trực tiếp từ phân phối mô hình, Nucleus Sampling (Holtzman et al., 2020) cắt bỏ đuôi của phân phối và Locally Typical Sampling (Meister et al., 2022) xem xét một tập con mang lại các chuỗi mật độ thông tin đồng đều. Trong khi ARGMAX có thể được xem như một tìm kiếm tốt nhất trước của cây giải mã với độ rộng tối đa là 1, Tìm kiếm Beam có thể được xem như một tìm kiếm rộng trước với độ rộng bị ràng buộc ở k (thường là 5) quỹ đạo. Best First Beam Search (Meister et al., 2020) kết hợp hai ý tưởng, luôn khám phá chuỗi có điểm số lớn nhất trong khi duy trì giới hạn độ rộng, để tăng hiệu quả. Best-k Search (Xu et al., 2022a) bỏ hạn chế độ rộng và luôn khám phá k chuỗi có điểm số cao nhất. Lattice decoding (Xu et al., 2022b) cho phép tái kết hợp các quỹ đạo tương tự, dẫn đến các giải pháp đa dạng hơn. Diverse Beam Search (Vijayakumar et al., 2016) bao gồm một mục tiêu đa dạng trực tiếp trong hàm tính điểm của Tìm kiếm Beam. Để cải thiện hiệu suất trên các vấn đề giải mã ràng buộc, Grid Beam Search (Hokamp and Liu, 2017) tạo các beam riêng biệt cho các chuỗi thỏa mãn số lượng ràng buộc khác nhau. Post and Vilar (2018) đề xuất Dynamic Beam Allocation để thay vào đó phân vùng một độ rộng beam cố định thành các pool tùy thuộc vào số lượng ràng buộc được thỏa mãn, với Hu et al. (2019) giới thiệu một triển khai vector hóa.

6 KẾT LUẬN

Chúng tôi đã trình bày việc vẽ phác thảo prompt, một khung mới cho suy luận LLM được hướng dẫn bằng template diễn đạt việc tạo bằng template như một vấn đề giải mã chuỗi phân đoạn. Quan điểm này mở khóa các thủ tục giải mã nhận biết phác thảo mới tối ưu hóa cho khả năng template tổng thể và không chỉ tạo văn bản tuần tự. Các thí nghiệm của chúng tôi cho thấy rằng việc vẽ phác thảo vượt trội hơn templating ngây thơ cũng như prompting tuần tự như chain-of-thought trên 7 trong số 8 nhiệm vụ suy luận LLM khó, cải thiện độ chính xác nhiệm vụ lên tới 10% điểm. Nhìn về phía trước, chúng tôi cũng cho thấy cách việc vẽ phác thảo cho phép các ứng dụng mới như định dạng đầu ra đáng tin cậy, tham chiếu về phía trước trong suy luận, và duyệt đồ thị được hướng dẫn bởi LLM, truyền cảm hứng cho công việc trong tương lai theo hướng này.
