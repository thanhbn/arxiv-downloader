# 2308.07411.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2308.07411.pdf
# Kích thước file: 282396 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Khám Phá Giao Điểm Giữa Các Mô Hình Ngôn Ngữ Lớn và Mô Hình Hóa Dựa Trên Tác Nhân Thông Qua Kỹ Thuật Thiết Kế Prompt
Edward Junprung
Trường Thông Tin
Đại học California, Berkeley
ejunprung@berkeley.edu
Tóm Tắt
Biên giới cuối cùng cho mô phỏng là biểu diễn chính xác các hệ thống xã hội phức tạp trong thế giới thực. Mặc dù mô hình hóa dựa trên tác nhân (ABM) tìm cách nghiên cứu hành vi và tương tác của các tác nhân trong một hệ thống lớn hơn, nhưng nó không thể nắm bắt một cách trung thực toàn bộ sự phức tạp của hành vi do con người thúc đẩy. Các mô hình ngôn ngữ lớn (LLM), như ChatGPT, đã xuất hiện như một giải pháp tiềm năng cho nút thắt này bằng cách cho phép các nhà nghiên cứu khám phá các tương tác do con người thúc đẩy theo những cách không thể tưởng tượng được trước đây. Nghiên cứu của chúng tôi điều tra các mô phỏng tương tác con người sử dụng LLM. Thông qua kỹ thuật thiết kế prompt, lấy cảm hứng từ Park et al. (2023), chúng tôi trình bày hai mô phỏng về các đại diện đáng tin cậy của hành vi con người: một cuộc đàm phán hai tác nhân và một trò chơi giải đố giết người sáu tác nhân.

1 Giới Thiệu
Sự xuất hiện của các mô hình ngôn ngữ lớn (LLM) đã mở ra cánh cửa cho các mô phỏng thực tế về hành vi con người (Horton, 2023; Aher et al., 2023; Griffin et al., 2023). Trước khi giới thiệu LLM, các nhà nghiên cứu đã dựa vào một mô hình được gọi là Mô Hình Hóa Dựa Trên Tác Nhân (ABM) để mô phỏng hành vi tập thể của con người trong các hệ thống xã hội. Trong ABM, các tác nhân cá nhân với các quy tắc và quy trình ra quyết định cụ thể tương tác với nhau, tạo ra những hành vi nổi lên ở cấp độ hệ thống. Mặc dù là một mô hình mạnh mẽ, nó bị giới hạn ở các quy trình cấp cao như dịch tễ học hoặc các quy trình công nghiệp và không thể được sử dụng để mô phỏng chính xác ảnh hưởng của các hành động cá nhân của con người.

Lĩnh vực học tăng cường lấy cảm hứng từ ABM như ý tưởng về môi trường mô phỏng để khám phá hành vi tác nhân. Tương tự, chúng tôi nhằm mục đích kết hợp ABM với LLM để nâng cao hiểu biết của chúng ta về hành vi con người. Điều này sẽ cho phép các nhà nghiên cứu khám phá và phân tích các tình huống khác nhau do con người thúc đẩy với độ chính xác và tinh tế được cải thiện.

Bài báo này đóng góp những điều sau:
• Các ví dụ về mô phỏng do LLM thúc đẩy, cho phép các nhà nghiên cứu khám phá các kết quả tiềm năng bằng cách điều chỉnh nhân cách cho mỗi tác nhân LLM¹.
• Phân loại các mô phỏng do LLM thúc đẩy thành ba loại: một-với-một, một-với-nhiều và nhiều-với-nhiều (Bi et al., 2019).
• Thảo luận về các hạn chế trong việc xây dựng các mô phỏng thực tế của con người quy mô lớn, cụ thể là nút thắt của cửa sổ ngữ cảnh (tức là tối đa 4.096 token đầu vào).

2 Phương Pháp
2.1 Mô Hình
Chúng tôi tận dụng mô hình gpt-3.5-turbo của OpenAI để trình diễn các tương tác giữa các tác nhân LLM. Phiên bản cụ thể này của GPT có thể truy cập thông qua OpenAI API và đã được tinh chỉnh cho đối화, làm cho nó phù hợp cho trường hợp sử dụng cụ thể của chúng tôi.

Chúng tôi cũng đã đánh giá mô hình OPT của Meta AI (Zhang et al., 2022) cũng như các phiên bản mã nguồn mở của mô hình GPT của OpenAI thu được từ Hugging Face. Thật không may, những mô hình này có xu hướng lan man hoặc lạc đề ngay cả khi các cuộc trò chuyện mang tính xác định. Kết quả là, chúng không phù hợp để trình diễn các tương tác con người đáng tin cậy. Tính trôi chảy trong đối thoại của gpt-3.5-turbo dường như xuất phát từ việc tinh chỉnh bằng học tăng cường từ phản hồi của con người (Ouyang et al., 2022).

Hơn nữa, chúng tôi đã thử nghiệm với Llama-2-7b-chat-hf của Meta AI (Touvron et al., 2023) và thấy hiệu suất của nó khá hứa hẹn. Tuy nhiên, do số lượng tham số lớn, nó mất hơn một

¹Tác nhân LLM đề cập đến các tác nhân được hỗ trợ bởi Mô Hình Ngôn Ngữ Lớn (LLM) được sử dụng trong mô phỏng để khám phá các kết quả tiềm năng.

--- TRANG 2 ---
phút để tạo ra phản hồi cho mỗi prompt sử dụng máy tính cục bộ của chúng tôi, trong khi OpenAI API chỉ mất vài mili giây. Điều này làm cho Llama-2-7b-chat-hf không thực tế cho việc thử nghiệm nhanh chóng.

2.2 Kỹ Thuật Thiết Kế Prompt
Để tạo điều kiện cho các cuộc trò chuyện tự động, chúng tôi lập trình chuyển các phản hồi qua lại giữa các tác nhân LLM. Cơ chế này được mô tả chi tiết dưới đây.

2.2.1 Đầu Vào
Chúng tôi sử dụng API Chat Completions của OpenAI với mô hình gpt-3.5-turbo, yêu cầu ba loại đầu vào:

1. System thiết lập nhân cách của Assistant (tức là tác nhân LLM). Điều này tương tự như việc xác định tính cách độc đáo của một tác nhân. Nhân cách mà bạn xác định quyết định cách một tác nhân hành xử khi được đưa ra một prompt.

2. Assistant đề cập đến tác nhân LLM được định nghĩa trong System. Nó thể hiện nhân cách được chỉ định và đưa ra các phản hồi tương ứng.

3. User đề cập đến prompt được nhập bởi một người dùng (tức là con người) nhưng trong trường hợp của chúng tôi, người dùng được định nghĩa là một tác nhân LLM tự động khác. Assistant phản hồi với User dựa trên nhân cách của nó và ngữ cảnh được cung cấp trong cuộc trò chuyện.

2.2.2 Cơ Chế
Hình 1 minh họa cơ chế prompting được sử dụng để mô phỏng các tương tác tự động giữa các tác nhân LLM. Trong cách tiếp cận này, phản hồi tiếp theo của mỗi tương tác được điều kiện hóa dựa trên lịch sử cuộc trò chuyện tích lũy cho đến thời điểm đó. Việc bao gồm ngữ cảnh này rất quan trọng để duy trì tính mạch lạc và liên quan trong suốt cuộc trò chuyện, ngăn chặn các tác nhân LLM lệch khỏi chủ đề. Tuy nhiên, việc bao gồm ngữ cảnh làm tăng độ dài của prompt với mỗi tương tác. Sự tăng trưởng liên tục này trong độ dài prompt áp đặt một giới hạn, vì các mô phỏng của chúng tôi chỉ có thể chạy cho đến khi chúng đạt đến kích thước đầu vào prompt tối đa là 4.096 token (Brown et al., 2020). Do đó, thời gian và độ phức tạp của các mô phỏng của chúng tôi bị hạn chế bởi nút thắt này.

[Hình 1: Đối thoại tự động với cơ chế prompting qua lại. Mỗi hộp đại diện cho một prompt được chuyển đến các tác nhân LLM theo kiểu vòng tròn.]

3 Kết Quả
3.1 Mô Phỏng Một-với-Một: Mặc Cả Thẻ Pokémon
Chúng tôi khởi tạo một mô phỏng hai tác nhân được phân loại là "một-với-một", trong đó một tác nhân đàm phán một cách đối nghịch với tác nhân khác. Đầu tiên, chúng tôi định nghĩa System cho mỗi tác nhân. Để rõ ràng và biểu đạt tốt hơn, từ nay về sau chúng tôi sẽ gọi System là "nhân cách" để truyền đạt chính xác hơn mục đích của nó.

Định Nghĩa Nhân Cách
Người bán: Bạn là một thương gia thẻ Pokémon tại một hội nghị Pokémon. Bán một thẻ holographic Charizard với giá ít nhất 20 đô la nhưng mặc cả để có giá cao nhất có thể. Hãy ngắn gọn nhưng giải thích lý do của bạn.

Người mua: Bạn đang ở một hội nghị Pokémon và bạn quan tâm đến việc mua một thẻ holographic Charizard. Đàm phán để có giá thấp nhất có thể. Hãy thô lỗ và ngắn gọn nhưng giải thích lý do của bạn.

Nhân cách quyết định hành vi của tác nhân. Các cụm từ được làm nổi bật màu vàng mô tả mục tiêu của mỗi tác nhân, trong khi các cụm từ được làm nổi bật màu cam định nghĩa tính cách của họ. Các nhân cách tác nhân được viết tự do, có nghĩa là thứ tự các câu không quan trọng.

Trong tình huống này, tác nhân người bán tìm cách bán với giá ít nhất 20 đô la, với sự ưu tiên cho giá cao hơn. Ngược lại, tác nhân người mua nhằm mục đích có được giá thấp nhất có thể và sẽ hành xử thô lỗ để đạt được mục tiêu này. Quỹ đạo của cuộc trò chuyện và cuối cùng là kết quả của mô phỏng (tức là giá bán của thẻ Pokémon) được quyết định bởi nhân cách mà chúng tôi định nghĩa cho mỗi tác nhân.

Tiếp theo, chúng tôi bắt đầu mô phỏng bằng cách khởi tạo cuộc trò chuyện thủ công.

Khởi Tạo Cuộc Trò Chuyện
Người mua: Xin chào, bạn có thẻ holographic Charizard không?

Từ đây về sau, mỗi tác nhân LLM phản hồi một cách tự động. Cơ chế của tương tác qua lại này như được mô tả trong phần Phương Pháp.

Tương Tác Tự Động
Người bán: Có, tôi có thẻ holographic Charizard. Bạn sẵn sàng trả bao nhiêu cho nó?

Người mua: Nghe này, tôi không ở đây để lãng phí thời gian hay tiền bạc. Tôi muốn giá thấp nhất có thể cho thẻ holographic Charizard đó. Vậy hãy cho tôi giá thấp nhất tuyệt đối của bạn. Không có trò chơi, không có mánh khóe, chỉ là giá rẻ nhất mà bạn sẵn sàng bán.

Người bán: Tôi hiểu mong muốn của bạn về giá thấp, nhưng thẻ holographic Charizard này là một vật phẩm sưu tập được săn lùng nhiều. Do tính hiếm và sự phổ biến của nó, giá thấp nhất tôi có thể đưa ra là 50 đô la.

Người mua: Hmm, 50 đô la, phải không? Có vẻ hơi cao cho một mảnh bìa cứng. Tôi mong đợi tìm thấy nó với giá hợp lý hơn, đặc biệt là xem xét nó chỉ là một thẻ từ trò chơi trẻ em. Ý tôi là, nghiêm túc mà, ai sẽ trả nhiều tiền đến vậy cho một nhãn dán được tôn vinh? Tôi sẽ nói với bạn như thế này, tôi sẵn sàng đưa ra 15 đô la cho nó, và đó là sự hào phóng.

Tương Tác Tự Động
Người bán: Tôi thừa nhận quan điểm của bạn, nhưng các thẻ holographic Charizard có giá trị đáng kể trong cộng đồng thẻ Pokemon. Tính khan hiếm và sự mong muốn của chúng góp phần vào mức giá cao hơn. Xem xét điều này, tôi có thể giảm xuống còn 35 đô la cho thẻ holographic Charizard.

Người mua: 35 đô la? Bạn có đang cố cướp tôi không? Vẫn còn quá cao cho một thẻ ngớ ngẩn. Tôi không mua vào cái hype này. Nghe này, tôi sẽ đưa bạn 25 đô la, và đó là đề nghị cuối cùng tuyệt đối của tôi.

Người bán: Tôi hiểu sự do dự và mong muốn có giá thấp hơn của bạn. Tuy nhiên, nhu cầu về thẻ holographic Charizard vẫn cao, và giá trị của chúng đã tăng đều đặn theo thời gian. Xem xét những yếu tố này, tôi có thể chấp nhận đề nghị 25 đô la của bạn như một sự thỏa hiệp cuối cùng.

Người mua: Cuối cùng, chúng ta cũng đến được đâu đó. 25 đô la vẫn nhiều hơn tôi muốn trả, nhưng tôi cho rằng nó tốt hơn là không có gì. Được rồi, chúng ta có thỏa thuận. Chỉ cần đưa tôi chiếc thẻ đắt đỏ đó để chúng ta có thể kết thúc chuyện này.

Trong tương tác này, cả hai tác nhân LLM đều đồng ý về giá 25 đô la, như được làm nổi bật màu vàng. Kết quả này báo hiệu chiến thắng cho người bán, vì giá mục tiêu được đặt trong nhân cách là 20 đô la.

Một quan sát thú vị là người bán đã sử dụng chiến lược thổi phồng giá bán (50 đô la) ở đầu cuộc đàm phán, dẫn đến việc đảm bảo giá bán cuối cùng cao hơn. Hành vi nổi lên này không được định nghĩa rõ ràng ở bất cứ đâu trong mô phỏng, làm nổi bật khả năng của các tác nhân LLM trong việc đưa ra các chiến lược mới một cách tự động. Với quan sát này, chúng tôi khám phá các kết quả khác nhau bằng cách viết lại các nhân cách để thể hiện các mục tiêu khác nhau. Bảng 1 minh họa cách điều chỉnh các mục tiêu trong nhân cách của mỗi tác nhân dẫn đến một tập hợp kết quả đa dạng.

--- TRANG 3 ---
| Mục Tiêu Người Bán | Mục Tiêu Người Mua | Kết Quả | Token Prompt Cuối |
|---|---|---|---|
| Bán trên 20 đô la | Đàm phán giá thấp nhất | Bán được 25 đô la | 522 |
| Bán khoảng 20 đô la | Đàm phán giá thấp nhất | Bán được 17 đô la | 369 |
| Bán trên 20 đô la | Mua dưới 20 đô la | Không có thỏa thuận | 472 |

Bảng 1: Các prompt đầy đủ được sử dụng cho mỗi tình huống được cung cấp trong Phụ lục A.2.

Cuối cùng, hãy xem xét tổng số token, đại diện cho độ dài prompt cuối cùng khi kết thúc mô phỏng. Nhớ rằng độ dài prompt tăng với số lượng tương tác. Trong mô phỏng đơn giản này, số token cuối cùng dao động từ 300 đến 600 token đầu vào², vẫn nằm trong giới hạn 4.096 token.

3.2 Mô Phỏng Một-với-Nhiều: Giải Đố Giết Người
Trong mô phỏng tiếp theo này, chúng tôi xây dựng một mô phỏng sáu tác nhân được phân loại là "một-với-nhiều", trong đó một tác nhân LLM hợp tác và giao tiếp với năm tác nhân LLM khác để giải một vụ án giết người. Tương tự như mô phỏng 2 tác nhân trước đó, chúng tôi định nghĩa nhân cách cho mỗi tác nhân. Ba loại nhân cách được thiết lập: thuyền trưởng, hành khách và kẻ giết người.

Định Nghĩa Nhân Cách Cho Mỗi Loại Tác Nhân
Thuyền trưởng: Bạn là thuyền trưởng. Ai đó đã bị giết trên tàu của bạn. Thẩm vấn nhóm để tìm ra ai khớp với báo cáo nhân chứng. Một nhân chứng đã báo cáo rằng kẻ giết người được nhìn thấy mặc váy, chạy từ cabin. Ngoài ra, một số nhân chứng nghe thấy kẻ giết người nói về làm vườn. Cũng nghe nói rằng kẻ giết người là một sinh viên. Hãy ngắn gọn trong phản hồi của bạn, đừng tiết lộ báo cáo nhân chứng, và đặt câu hỏi cho cả nhóm.

Hành khách: Tên bạn là Bob, bạn là một người hài hước,

²Token prompt cuối cùng có thể thay đổi do siêu tham số temperature.

Định Nghĩa Nhân Cách Cho Mỗi Loại Tác Nhân
và bạn làm việc như một giảng viên Khoa học Dữ liệu. Trong thời gian rảnh, bạn thích dạy học. Một sự thật ngẫu nhiên là bạn có gốc Ba Lan. Cho chuyến du thuyền này, bạn đã đóng gói một chiếc áo có cổ và phần yêu thích của bạn trên tàu là phòng nghỉ. Ai đó đã bị giết trên du thuyền và thuyền trưởng muốn bạn giúp xác định kẻ giết người. Hãy giúp xác định kẻ giết người. Vui lòng ngắn gọn trong phản hồi của bạn.

Kẻ giết người: Tên bạn là Nancy, bạn là một người vui vẻ, và bạn làm việc như một Nhà khoa học Dữ liệu. Trong thời gian rảnh, bạn thích làm vườn. Một sự thật ngẫu nhiên là bạn là một sinh viên. Cho chuyến du thuyền này, bạn đã đóng gói một chiếc váy và phần yêu thích của bạn trên tàu là cabin. Ai đó đã bị giết trên du thuyền và thuyền trưởng muốn bạn giúp xác định kẻ giết người. Tuy nhiên, bạn là kẻ giết người. Đánh lạc hướng và nói dối mọi người để tránh bị bắt. Vui lòng ngắn gọn trong phản hồi của bạn.

Trên đây là các ví dụ về nhân cách tác nhân. Văn bản được làm nổi bật màu vàng phác thảo các mục tiêu được thể hiện bởi mỗi loại tác nhân trong ba loại. Văn bản được làm nổi bật màu xanh lá cây trình bày các sự kiện có thể liên kết tác nhân trở lại với vụ giết người, trong khi văn bản được làm nổi bật màu cam đại diện cho tính cách của tác nhân. Các nhân cách cho mỗi tác nhân được tạo ra một cách động³. Tổng cộng, sáu nhân cách tác nhân được khởi tạo: một thuyền trưởng và năm hành khách, một trong số họ được chọn ngẫu nhiên làm kẻ giết người. Chúng tôi bắt đầu mô phỏng bằng cách khởi tạo cuộc trò chuyện. Tất cả các tương tác tác nhân tiếp theo được thực hiện tự động.

³Mã để tái tạo mô phỏng này được liên kết trong Phụ lục A.1

--- TRANG 4 ---
[Hình 2: Minh họa này mô tả các tương tác giữa các tác nhân, trong đó mỗi phản hồi từ thuyền trưởng được điều kiện hóa dựa trên luồng bộ nhớ.]

Đối với mỗi tương tác, tác nhân thuyền trưởng đặt cùng một câu hỏi cho mỗi tác nhân hành khách theo kiểu vòng tròn. Với hơn hai tác nhân tương tác đồng thời, việc chia sẻ ngữ cảnh giữa các tác nhân trở thành một thách thức đáng kể. Để giải quyết điều này, khái niệm luồng bộ nhớ được giới thiệu (Park et al., 2023). Luồng bộ nhớ được tạo ra bằng cách nối các phản hồi từ mỗi tác nhân hành khách thành một chuỗi, giúp bảo tồn ngữ cảnh đang diễn ra của cuộc trò chuyện. Luồng bộ nhớ này phục vụ như một nhật ký, ghi lại tất cả các phản hồi từ các tác nhân hành khách đến câu hỏi của thuyền trưởng.

Một khi tất cả các tác nhân đã hoàn thành việc phản hồi, luồng bộ nhớ hoàn chỉnh trở thành câu trả lời cho câu hỏi của tác nhân thuyền trưởng. Sau đó, phản hồi tiếp theo của thuyền trưởng được điều kiện hóa dựa trên luồng bộ nhớ. Ngữ cảnh này cho phép thuyền trưởng tạo ra các phản hồi có liên quan, bao gồm bất kỳ câu hỏi theo dõi bổ sung nào. Nếu không có cơ chế này, các phản hồi của thuyền trưởng thường lệch khỏi chủ đề dự định mà chúng tôi đã khám phá thông qua thử nghiệm. Dưới đây là một ví dụ về cơ chế này trong hành động. Lưu ý rằng văn bản được làm nổi bật màu vàng ghi nhãn phản hồi của mỗi tác nhân hành khách.

Tương Tác Tự Động
Thuyền trưởng: Hãy giới thiệu bản thân.

Luồng Bộ Nhớ: Bob nói: Xin chào! Tôi là Bob, giảng viên Khoa học Dữ liệu thân thiện trong khu phố của bạn.

Tương Tác Tự Động
Siêng năng dạy người khác điều hướng thế giới dữ liệu trong khi ném ra vài câu đùa. Nancy nói: Xin chào mọi người! Tên tôi là Nancy, và tôi là một Nhà khoa học Dữ liệu. Tôi cũng là một sinh viên, điều này khiến tôi luôn tỉnh táo. Ngoài công việc và học tập, tôi có niềm đam mê làm vườn và dành thời gian trong thiên nhiên. Max nói: Xin chào, tên tôi là Max và tôi là một Nhà nghiên cứu Y khoa. Tôi nói tiếng Đức và thích đọc sách trong thời gian rảnh. Tôi đã đóng gói một chiếc áo blouse cho chuyến du thuyền này và phần yêu thích của tôi trên tàu là boong tàu. Chris nói: Xin chào mọi người, tên tôi là Chris. Tôi là một sinh viên thích chơi trò chơi điện tử và tôi luôn đeo chiếc kính đáng tin cậy của mình. Susan nói: Xin chào, tên tôi là Susan. Tôi là một kỹ sư phần mềm về nghề nghiệp và tôi thích du lịch trong thời gian rảnh. Tôi có sở thích với Tesla và là một người dè dặt.

Thuyền trưởng: Cảm ơn mọi người. Hãy tập trung vào các chi tiết xung quanh vụ giết người. Có ai thấy ai đó mặc váy chạy từ cabin không?

Luồng Bộ Nhớ: Bob nói: Tôi xin lỗi, nhưng không ai báo cáo việc nhìn thấy ai đó mặc váy chạy từ cabin. Nancy nói: Ôi, thật khủng khiếp khi nghe về vụ giết người. Tôi hy vọng chúng ta có thể đi đến tận cùng của vấn đề này. Về câu hỏi của bạn, tôi chưa thấy ai mặc váy chạy từ cabin. Có lẽ ai đó khác có thể đã chú ý đến điều gì đó? Max nói: Không, tôi chưa được nhìn thấy mặc váy trong chuyến du thuyền này. Chris nói: Không, tôi chưa được nhìn thấy mặc váy trong chuyến du thuyền này. Susan nói: Không, tôi chưa được nhìn thấy mặc váy trong chuyến du thuyền này.

Thuyền trưởng: Cảm ơn tất cả các phản hồi của các bạn. Thêm một câu hỏi nữa: Có ai nghe thấy ai đó nói về làm vườn không?

--- TRANG 5 ---
Chúng tôi kết thúc mô phỏng bằng cách yêu cầu thuyền trưởng đoán kẻ giết người, người được xác định chính xác dựa trên báo cáo nhân chứng. Mô phỏng này yêu cầu tổng cộng 1.734 token⁴, chiếm gần một nửa giới hạn token đầu vào tối đa của gpt-3.5-turbo, mặc dù mô phỏng khá đơn giản.

Đặt Câu Hỏi Cuối Cùng
Con người: Ai trong số các nghi phạm có vẻ đáng ngờ nhất?

Thuyền trưởng: Dựa trên thông tin được cung cấp, Nancy có vẻ đáng ngờ nhất. Cô ấy đã đề cập đến làm vườn, và xác nhận là một sinh viên.

3.3 Mô Phỏng Nhiều-với-Nhiều
Chúng tôi đặc trưng hóa các mô phỏng nhiều-với-nhiều như một sự kết hợp của các tình huống một-với-một (tức là tác nhân LLM trò chuyện với tác nhân LLM khác) và một-với-nhiều (tức là tác nhân LLM trò chuyện với một nhóm tác nhân LLM). Loại mô phỏng này có liên quan trong các tình huống thế giới thực, chẳng hạn như mô hình hóa sự lan truyền tin giả, nơi tin đồn có thể được chia sẻ giữa các cá nhân hoặc nhóm cá nhân.

Trong bài báo này, chúng tôi không khám phá các mô phỏng nhiều-với-nhiều do sự phức tạp của việc triển khai và các hạn chế liên quan đến giới hạn token đầu vào tối đa của gpt-3.5-turbo của OpenAI. Tuy nhiên, chúng tôi thừa nhận rằng các tình huống thế giới thực thường liên quan đến các tương tác giữa các nhóm cá nhân, làm cho chúng có giá trị để mô phỏng. Để có một ví dụ minh họa về mô phỏng nhiều-với-nhiều với LLM, chúng tôi tham khảo Park et al. (2023).

4 Công Trình Liên Quan
Bài báo này được lấy cảm hứng từ Park et al. (2023) và công trình tại Viện Stanford cho Trí tuệ Nhân tạo Lấy Con người Làm Trung tâm (HAI). Theo hiểu biết tốt nhất của chúng tôi, Generative Agents: Interactive Simulacra of Human Behavior là bài báo nghiên cứu đầu tiên cố gắng mô hình hóa các tương tác xã hội phức tạp, do con người thúc đẩy bằng cách sử dụng LLM. Chúng tôi mượn nhiều từ công trình này và thường tái tạo kết quả của họ, mặc dù ở mức độ cao.

Hơn nữa, chúng tôi quan tâm đến khái niệm kỹ thuật thiết kế prompt và khả năng đáng chú ý của nó trong việc tận dụng ngữ cảnh để tạo ra các phản hồi có liên quan.

⁴Lưu ý rằng token prompt cuối cùng có thể thay đổi do siêu tham số temperature.

Vì cơ chế này phản ánh chặt chẽ tương tác con người trong thế giới thực, nó là một thành phần quan trọng trong việc xây dựng các mô phỏng của chúng tôi. Liu et al. (2023) đo lường thực nghiệm hiệu quả của cơ chế này, phát hiện rằng LLM thành thạo nhất trong việc truy xuất ngữ cảnh từ đầu và cuối của prompt. Ngoài ra, Wei et al. (2023) chứng minh rằng việc xây dựng các prompt few-shot dưới dạng chuỗi suy nghĩ tăng cường khả năng của LLM trong việc tham gia vào lý luận phức tạp. Những hiểu biết này đã hướng dẫn thiết kế các mô phỏng của chúng tôi.

Ngoài cách tiếp cận chỉ suy luận để xây dựng mô phỏng với LLM, đổi mới đáng kể đang diễn ra ở phía đào tạo. Mô hình Học Tăng cường Phản hồi Con người (RLHF), như được tiên phong bởi Ouyang et al. (2022), dường như rất cần thiết để tinh chỉnh LLM, đặc biệt là trong các trường hợp sử dụng dựa trên đối thoại như của chúng tôi. Từ các thí nghiệm của chúng tôi, LLM không có RLHF sẽ lan man hoặc lạc đề, làm cho việc trình diễn hành vi con người đáng tin cậy trở nên không thể.

Cuối cùng, nghiên cứu đáng kể đã được thực hiện về việc đào tạo LLM để học cách truy xuất thông tin vượt ra ngoài những gì được lưu trữ ngầm trong các tham số của mạng neural cơ bản. Guu et al. (2020) đề xuất một cơ chế để đào tạo LLM thực hiện lý luận trên một kho kiến thức lớn một cách tức thì trong quá trình suy luận. Quá trình này tương tự như một con người nội hóa những gì họ đã học ở trường, có thể chứng minh quan trọng để xử lý các cửa sổ đầu vào tối đa 4.096 token và việc truy xuất ký ức dài hạn trong các mô phỏng của chúng tôi.

5 Kết Luận
Mặc dù các mô phỏng còn thô sơ, nghiên cứu này làm nổi bật tiềm năng xây dựng các sân chơi kỹ thuật số quy mô lớn để đánh giá các hành vi do con người thúc đẩy trong thế giới thực bằng cách sử dụng LLM. Về mặt độ tin cậy, Park et al. (2023) định lượng độ tin cậy của các mô phỏng do LLM thúc đẩy và phát hiện rằng các tác nhân LLM có thể tạo ra hành vi thậm chí còn đáng tin cậy hơn điều kiện con người, như được đánh giá bởi các đánh giá viên con người từ crowdsource.

Chúng tôi kết thúc bằng cách làm nổi bật hai thách thức trong việc tạo ra các mô phỏng quy mô lớn được hỗ trợ bởi LLM.

• Giới hạn token prompt tối đa 4.096 của gpt-3.5-turbo đã hạn chế độ phức tạp của các mô phỏng của chúng tôi. Các cửa sổ ngữ cảnh lớn hơn⁵

⁵Tại thời điểm viết bài này, gpt-4 cho phép 8.192 token prompt nhưng chúng tôi không thể truy cập được.

--- TRANG 6 ---
có thể cho phép các mô phỏng quy mô lớn liên quan đến hàng nghìn tác nhân LLM và các mô phỏng chạy dài kéo dài nhiều năm thời gian mô phỏng.

• Việc truy xuất thông tin có liên quan từ các cửa sổ ngữ cảnh lớn vẫn là một nhiệm vụ phức tạp (Liu et al., 2023). Các giải pháp có thể có thể liên quan đến phương pháp heuristic, tóm tắt lịch sử cuộc trò chuyện, hoặc việc kết hợp cơ chế attention để chú ý đến luồng bộ nhớ.

Tài Liệu Tham Khảo
Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using large language models to simulate multiple humans and replicate human subject studies.

Tianchi Bi, Hao Xiong, Zhongjun He, Hua Wu, and Haifeng Wang. 2019. Multi-agent learning for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 856–865, Hong Kong, China. Association for Computational Linguistics.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.

Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly Mai, Maria Vau, Matthew Caldwell, and Augustine Mavor-Parker. 2023. Large language models respond to influence like humans.

Kelvin Guu, Kenton Lee, Zora Tung, and Panupong Pasupatand Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training.

John J. Horton. 2023. Large language models as simulated economic agents: What can we learn from homo silicus?

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.

Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, and Robert Stojnic Sergey Edunov Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.

A Phụ Lục
A.1 Mã Mô Phỏng
Để truy cập mã được sử dụng trong bài báo này, vui lòng truy cập https://github.com/ejunprung/llm-agents. Kho lưu trữ này chứa mã nguồn đầy đủ cho các mô phỏng sau:

• Mô Phỏng Một-với-Một: Mặc Cả Thẻ Pokémon
• Mô Phỏng Một-với-Nhiều: Giải Đố Giết Người

--- TRANG 7 ---
A.2 Nhân Cách Tác Nhân Cho Mô Phỏng Đàm Phán
Trong tình huống 1, chúng tôi hướng dẫn tác nhân người bán mặc cả để có giá cao nhất có thể. Ràng buộc này khuyến khích người bán theo đuổi các chiến thuật đảm bảo giá cao hơn.

Tình Huống 1: Bán Trên 20 Đô La
Người bán: Bạn là một thương gia thẻ Pokémon tại một hội nghị Pokémon. Bán một thẻ holographic Charizard với giá ít nhất 20 đô la nhưng mặc cả để có giá cao nhất có thể. Hãy ngắn gọn nhưng giải thích lý do của bạn.

Người mua: Bạn đang ở một hội nghị Pokémon và bạn quan tâm đến việc mua một thẻ holographic Charizard. Đàm phán để có giá thấp nhất có thể. Hãy thô lỗ và ngắn gọn nhưng giải thích lý do của bạn.

Trong tình huống 2, chúng tôi cung cấp cho người bán sự linh hoạt để đàm phán cao hơn hoặc thấp hơn. Do đó, người bán quyết định ở mức giá thấp hơn 20 đô la để chốt thỏa thuận. Lưu ý rằng chúng tôi không hướng dẫn người mua phải thô lỗ trong trường hợp này. Chúng tôi phát hiện rằng một người mua thô lỗ sẽ dẫn đến việc người bán đàm phán để có giá cao hơn 20 đô la.

Tình Huống 2: Bán Dưới 20 Đô La
Người bán: Bạn là một thương gia thẻ Pokémon tại một hội nghị Pokémon. Đàm phán để bán một thẻ holographic Charizard với giá khoảng 20 đô la. Hãy ngắn gọn nhưng giải thích lý do của bạn.

Người mua: Bạn đang ở một hội nghị Pokémon và bạn quan tâm đến việc mua một thẻ holographic Charizard. Đàm phán để có giá thấp nhất có thể. Hãy ngắn gọn nhưng giải thích lý do của bạn.

Trong tình huống 3, chúng tôi áp đặt các hạn chế ngăn cản thỏa thuận. Người bán chỉ sẵn sàng bán với giá 20 đô la trở lên, trong khi người mua chỉ sẵn sàng mua với giá dưới 20 đô la. Hậu quả là không có thỏa thuận nào đạt được.

Tình Huống 3: Không Có Thỏa Thuận
Người bán: Bạn là một thương gia thẻ Pokémon tại một hội nghị Pokémon. Bán một thẻ holographic Charizard với giá ít nhất 20 đô la bất kể điều gì. Hãy ngắn gọn nhưng giải thích lý do của bạn.

Người mua: Bạn đang ở một hội nghị Pokémon và bạn quan tâm đến việc mua một thẻ holographic Charizard. Bạn sẽ không trả hơn 19 đô la. Hãy ngắn gọn nhưng giải thích lý do của bạn.
