# Tác động của Mã hóa Vị trí đến Khả năng Tổng quát hóa Chiều dài trong Transformers

Amirhossein Kazemnejad1, Inkit Padhi2
Karthikeyan Natesan Ramamurthy2,Payel Das2,Siva Reddy1,3,4
1Mila, McGill University;2IBM Research;
3Facebook CIFAR AI Chair;4ServiceNow Research
{amirhossein.kazemnejad,siva.reddy}@mila.quebec
inkpad@ibm.com ,{knatesa,daspa}@us.ibm.com

## Tóm tắt

Tổng quát hóa chiều dài, khả năng tổng quát từ kích thước ngữ cảnh huấn luyện nhỏ sang kích thước lớn hơn, là một thách thức quan trọng trong việc phát triển các mô hình ngôn ngữ dựa trên Transformer. Mã hóa vị trí (PE) đã được xác định là yếu tố chính ảnh hưởng đến tổng quát hóa chiều dài, nhưng tác động chính xác của các sơ đồ PE khác nhau đối với việc ngoại suy trong các tác vụ downstream vẫn chưa rõ ràng. Trong bài báo này, chúng tôi tiến hành một nghiên cứu thực nghiệm có hệ thống so sánh hiệu suất tổng quát hóa chiều dài của các Transformer chỉ giải mã với năm phương pháp mã hóa vị trí khác nhau bao gồm Embedding Vị trí Tuyệt đối (APE), PE Tương đối của T5, ALiBi, và Rotary, ngoài ra còn có Transformers không có mã hóa vị trí (NoPE). Đánh giá của chúng tôi bao gồm một loạt các tác vụ lý luận và toán học. Các phát hiện của chúng tôi cho thấy rằng các phương pháp mã hóa vị trí được sử dụng phổ biến nhất, như ALiBi, Rotary, và APE, không phù hợp cho tổng quát hóa chiều dài trong các tác vụ downstream. Quan trọng hơn, NoPE vượt trội hơn các phương pháp mã hóa vị trí rõ ràng khác trong khi không yêu cầu tính toán bổ sung. Chúng tôi chứng minh về mặt lý thuyết rằng NoPE có thể biểu diễn cả PE tuyệt đối và tương đối, nhưng khi được huấn luyện với SGD, nó chủ yếu giống với các mẫu attention PE Tương đối của T5. Cuối cùng, chúng tôi phát hiện rằng scratchpad không phải lúc nào cũng hữu ích để giải quyết tổng quát hóa chiều dài và định dạng của nó có tác động lớn đến hiệu suất của mô hình. Nhìn chung, công trình của chúng tôi gợi ý rằng mã hóa vị trí rõ ràng không cần thiết cho các Transformer chỉ giải mã để tổng quát tốt đến các chuỗi dài hơn.

## 1 Giới thiệu

Khả năng tổng quát từ kích thước ngữ cảnh huấn luyện nhỏ hơn sang kích thước lớn hơn, thường được gọi là tổng quát hóa chiều dài, là một thách thức lớn đối với các mô hình ngôn ngữ dựa trên Transformer (Vaswani et al., 2017; Deletang et al., 2023; Zhang et al., 2023). Ngay cả với các Transformer lớn hơn, vấn đề này vẫn tồn tại (Brown et al., 2020; Furrer et al., 2020). Với kích thước ngữ cảnh lớn hơn, một mô hình có thể hưởng lợi từ nhiều ví dụ học trong ngữ cảnh hơn, số lượng bước lý luận cao hơn, hoặc tạo văn bản dài hơn. Tuy nhiên, huấn luyện một Transformer với kích thước ngữ cảnh lớn hơn có thể cực kỳ chậm và tốn bộ nhớ. Điều này thậm chí còn rõ rệt hơn trong mô hình gần đây của việc tinh chỉnh mô hình trên các tập dữ liệu theo hướng dẫn (Wei et al., 2022a; Chung et al., 2022; Ouyang et al., 2022). Không chỉ không khả thi khi huấn luyện mô hình trên tất cả các độ dài ngữ cảnh có thể, mà số lượng ví dụ huấn luyện cũng giảm mạnh khi độ dài chuỗi tăng, đòi hỏi mô hình phải tổng quát từ các ví dụ huấn luyện có độ dài hữu hạn và ngắn hơn. Trong công trình này, chúng tôi tập trung vào tác động của mã hóa vị trí đối với tổng quát hóa chiều dài trong các Transformer "chỉ giải mã" trên các tác vụ khác nhau được huấn luyện từ đầu. Hình 1 tóm tắt phát hiện của chúng tôi rằng việc không sử dụng mã hóa vị trí tốt hơn việc sử dụng mã hóa vị trí rõ ràng.

Mã hóa vị trí (PE) dường như là một yếu tố chính trong tổng quát hóa chiều dài của Transformers vì mô hình phải mã hóa có hệ thống các token ở tất cả các vị trí có thể. Kiến trúc Transformer gốc (Vaswani et al., 2017) sử dụng các hàm tuần hoàn không tham số để biểu diễn embedding vị trí tuyệt đối (APE) một cách có hệ thống, nhưng các nghiên cứu tiếp theo đã chỉ ra rằng những hàm này không đủ cho tổng quát hóa chiều dài (Ontanon et al., 2022). Niềm tin phổ biến là các PE tương đối (Shaw et al., 2018; Raffel et al., 2020) hiệu quả hơn trong tổng quát hóa chiều dài so với các biến thể APE (Ontanon et al., 2022; Csordás et al., 2021). Tuy nhiên, Press et al. (2022) đã chỉ ra rằng ngay cả các Transformer với PE tương đối, như Rotary (Su et al., 2021), cũng có thể kém trong tổng quát hóa chiều dài. Nhưng việc đánh giá PE thường dựa vào perplexity mô hình hóa ngôn ngữ như một chỉ số chính (Haviv et al., 2022; Press et al., 2022) mà không phải lúc nào cũng phù hợp với hiệu suất trên các tác vụ downstream (Tay et al., 2022). Điều này đặt ra những câu hỏi quan trọng: ảnh hưởng chính xác của mã hóa vị trí đối với tổng quát hóa chiều dài trong các tác vụ downstream là gì? Hơn nữa, bằng chứng thực nghiệm sớm cho thấy rằng các Transformer chỉ giải mã mà không có thông tin vị trí rõ ràng (Tsai et al., 2019; Haviv et al., 2022) có thể hoạt động tốt như các PE hiện có trong các cài đặt phân phối trong, nhưng tác động của nó đối với tổng quát hóa chiều dài và hiệu suất downstream vẫn chưa rõ ràng.

Gần đây, việc yêu cầu các mô hình phát ra các bước tính toán trung gian vào một scratchpad, còn được gọi là chuỗi suy nghĩ, đã được áp dụng để cải thiện ngoại suy chiều dài trong Transformers (Nye et al., 2021; Wei et al., 2022b). Những kỹ thuật này độc lập với kiến trúc và có thể được sử dụng với bất kỳ phương pháp PE nào. Tuy nhiên, vẫn còn là một câu hỏi mở liệu những kỹ thuật này, ít nhất là về mặt tổng quát hóa chiều dài, có làm cho việc lựa chọn PE trở nên không liên quan, đặc biệt là do hiệu suất mô hình rất nhạy cảm với định dạng scratchpad (Bueno et al., 2022; Akyurek and Akyurek, 2022).

Để trả lời những câu hỏi này, chúng tôi tiến hành một nghiên cứu có hệ thống về tổng quát hóa chiều dài của các Transformer chỉ giải mã, được phổ biến bởi họ mô hình GPT (Radford et al., 2019), với các sơ đồ mã hóa vị trí được sử dụng phổ biến nhất, cả có và không có scratchpad. Cụ thể, chúng tôi đánh giá APE (Vaswani et al., 2017), PE Tương đối của T5 (Raffel et al., 2020), ALiBi (Press et al., 2022), Rotary (Su et al., 2021) và không có mã hóa vị trí (NoPE) trên một loạt các tác vụ lý luận và toán học. Kết quả của chúng tôi cho thấy rằng:

• Hầu hết các phương pháp mã hóa vị trí được sử dụng phổ biến, bao gồm ALiBi, Rotary, và APE, không phù hợp cho tổng quát hóa chiều dài trong các tác vụ downstream và bị vượt trội bởi PE Tương đối của T5.

• Các Transformer không có mã hóa vị trí (NoPE) vượt trội hơn tất cả các sơ đồ mã hóa vị trí rõ ràng. Chúng đạt được điều này mà không tính toán các thuật ngữ bổ sung trong cơ chế attention (trái ngược với các PE rõ ràng).

• Chúng tôi chỉ ra rằng NoPE có khả năng biểu diễn cả PE tuyệt đối và tương đối về mặt lý thuyết. Nhưng về mặt thực nghiệm, nó gần với sơ đồ mã hóa tương đối tương tự như PE Tương đối của T5.

• Scratchpad không phải lúc nào cũng hữu ích cho tổng quát hóa chiều dài và định dạng của nó có tác động lớn đến hiệu suất. Các phân phối attention cho thấy rằng NoPE và PE Tương đối của T5 khuyến khích chú ý đến cả vị trí tầm xa và tầm gần, ALiBi đến các vị trí gần đây, và Rotary và APE đến không có vị trí cụ thể nào.

## 2 Bối cảnh: Mã hóa Vị trí trong Transformers

Transformers, trái ngược với các mô hình tuần tự như RNN, là các kiến trúc song song sử dụng mã hóa vị trí để giúp mã hóa thứ tự từ. Các lựa chọn phổ biến nhất cho mã hóa vị trí là hoặc tuyệt đối, trong đó mỗi vị trí tuyệt đối (ví dụ: 1, 2, 3, ...) được biểu diễn trực tiếp, hoặc tương đối, trong đó khoảng cách giữa các token được sử dụng như thông tin vị trí. Trong phần này, chúng tôi xem xét ngắn gọn các phương pháp mã hóa phổ biến được sử dụng trong Transformers (Tham khảo Phụ lục B để biết thêm chi tiết chính thức).

Embedding Vị trí Tuyệt đối (APE), nhúng mỗi vị trí tuyệt đối i vào vector vị trí pi và cộng word embeddings với pi tương ứng của chúng trước khi đưa chúng vào mô hình. Biến thể không tham số của APE sử dụng các hàm tuần hoàn như sin và cosine để tạo embeddings cho bất kỳ vị trí i nào (Vaswani et al., 2017). Mặt khác, một phiên bản học được của APE, được sử dụng trong GPT3 (Brown et al., 2020) và OPT (Zhang et al., 2022), huấn luyện các position embeddings cùng với các tham số mô hình, và nó không thể tạo ra position embedding cho các vị trí chưa thấy, vì vậy cửa sổ ngữ cảnh được đặt ở một độ dài cố định.

Bias Tương đối của T5, đầu tiên ánh xạ khoảng cách tương đối (i−j) giữa các token ở vị trí i và j thành một giá trị bias vô hướng b=f(i−j), trong đó f là một bảng tra cứu. Bias tương đối b (được học trong quá trình huấn luyện) sau đó được cộng vào tích vô hướng của query và key trong cơ chế self-attention. Bảng tra cứu ánh xạ các khoảng cách lớn hơn một ngưỡng thành cùng một tham số để cho phép tổng quát hóa đến các khoảng cách chưa thấy.

Rotary, được sử dụng trong PaLM (Chowdhery et al., 2022) và LLaMA (Touvron et al., 2023), xoay các biểu diễn query và key với một góc tỉ lệ với vị trí tuyệt đối của chúng trước khi áp dụng attention tích vô hướng. Kết quả của việc xoay này, tích vô hướng attention sẽ chỉ phụ thuộc vào khoảng cách tương đối giữa các token, có hiệu quả làm cho nó trở thành một mã hóa vị trí tương đối (Su et al., 2021).

ALiBi, được sử dụng trong BLOOM (Scao et al., 2022a), tương tự như Bias Tương đối của T5 nhưng thay vào đó trừ một bias vô hướng từ điểm attention. Bias này tăng tuyến tính với khoảng cách giữa các token query và key. Điều này, có hiệu quả, tạo ra một sự ưu tiên đối với các token gần đây (bias gần đây).

Lưu ý rằng các Transformer chỉ encoder, như BERT, trở thành các mô hình bag-of-words khi không có mã hóa vị trí. Tuy nhiên, các Transformer chỉ decoder với causal attention mask không bất biến hoán vị và có thể mô hình hóa các chuỗi ngay cả khi không có thông tin vị trí rõ ràng (Tsai et al., 2019). Nhưng không rõ liệu những mô hình này có mã hóa thông tin vị trí một cách ngầm định hay tổng quát đến các độ dài chưa thấy. Chúng tôi làm sáng tỏ điều này trong Phần 5.

## 3 Đánh giá Mô hình

Thiết lập Tổng quát hóa Chiều dài Theo Anil et al. (2022), chúng tôi tập trung vào các tác vụ thuật toán như sao chép, cộng, v.v. Đối với mỗi tác vụ, chúng tôi huấn luyện trên một số lượng hữu hạn các ví dụ có độ dài tối đa một mức nhất định và kiểm tra chúng trên cả độ dài đã thấy và chưa thấy trong quá trình suy luận. Chúng tôi trình bày những vấn đề này như các tác vụ sequence-to-sequence, trong đó chuỗi đầu vào là instance của vấn đề và chuỗi đầu ra là giải pháp. Chính thức, cho D={(xi,yi)} biểu thị một tập dữ liệu của tác vụ như vậy trong đó xi là đầu vào và yi là chuỗi đầu ra. Đối với mỗi tác vụ, một hàm λ:D →N có thể được định nghĩa trả về bucket độ dài của một instance tác vụ d∈ D. Điều này có thể là số lượng token hoặc bất kỳ khái niệm chung nào về độ dài/độ sâu của lý luận. Sử dụng hàm này và một ngưỡng L, chúng tôi sử dụng các mẫu trong đó λ≤L để học tác vụ và các mẫu trong đó λ > L để đánh giá tổng quát hóa. Hiệu suất trên mỗi instance được báo cáo như độ chính xác exact-match của câu trả lời với ground truth.

Kiến trúc Chúng tôi sử dụng một kiến trúc Transformer chỉ decoder thông thường làm cơ sở cho tất cả các thí nghiệm và xem xét các phương pháp khác nhau để mã hóa vị trí: Embedding Vị trí Tuyệt đối (APE), ALiBi, Rotary và Bias Tương đối của T5. Chúng tôi cũng xem xét việc loại bỏ mã hóa vị trí (NoPE) để hiểu rõ hơn vai trò của nó trong tổng quát hóa chiều dài. Lưu ý rằng chúng tôi sử dụng APE với các hàm sinusoidal (Vaswani et al., 2017) vì biến thể học được không thể tạo ra embeddings cho các vị trí chưa thấy. Do không có LM dựa trên Transformer được huấn luyện công khai với các PE nêu trên trên cùng dữ liệu pretraining, chúng tôi chọn huấn luyện các mô hình của mình từ đầu cho mỗi tác vụ trên dữ liệu huấn luyện của nó với mục tiêu autoregressive language modeling logpθ(y|x) =∑T t=1logpθ(yt|x,y1:t−1). Chúng tôi sử dụng cùng siêu tham số cho tất cả PE và sử dụng cấu hình kích thước mô hình "base", phổ biến trong thư viện HuggingFace (Wolf et al., 2020), dẫn đến ∼107M trọng số có thể huấn luyện (Danh sách tất cả siêu tham số trong Phụ lục D.2).

Các Tác vụ Nghiên cứu của chúng tôi về tổng quát hóa chiều dài tập trung vào các tác vụ downstream. Cụ thể, chúng tôi đánh giá các mô hình trên ba danh mục (Bảng 1) của các tác vụ tổng hợp đã được sử dụng rộng rãi trong tài liệu để điều tra tổng quát hóa chiều dài: (1) Các tác vụ nguyên thủy như Sao chép và Đảo ngược (Ontanon et al., 2022), (2) Các tác vụ toán học và lý luận như Cộng (Nye et al., 2021), Đánh giá Đa thức, Sắp xếp, Tổng (Saxton et al., 2019), Parity (Anil et al., 2022), LEGO (Zhang et al., 2023) và (3) Các tập dữ liệu tổng quát hóa chiều dài cổ điển như SCAN (Lake and Baroni, 2018) và PCFG (Hupkes et al., 2020). Những tác vụ này cung cấp cho chúng tôi quyền kiểm soát hoàn toàn đối với phân phối train-test, đồng thời cũng yêu cầu các kỹ năng lý luận và tính thành phần, phục vụ như các khối xây dựng cơ bản cho các tác vụ phức tạp hơn. Đối với hai danh mục đầu tiên, chúng tôi tạo ra các tập dữ liệu tương ứng. Cụ thể, trước tiên chúng tôi lấy mẫu độ dài của instance tác vụ từ phân phối đều U(1, L), và sau đó, theo quy trình tạo của tác vụ, chúng tôi lấy mẫu các chuỗi đầu vào và đầu ra. Đối với tập kiểm tra, chúng tôi thực hiện cùng quy trình nhưng lấy mẫu độ dài từ U(1,2L) để bao gồm cả độ dài đã thấy và chưa thấy. Trong suốt bài báo, trừ khi có ghi chú khác, chúng tôi sử dụng L= 20. Đối với danh mục thứ ba của các tác vụ, chúng tôi sử dụng các phân chia tổng quát hóa chiều dài từ các tập dữ liệu tương ứng. Bảng 1 cung cấp một ví dụ về mỗi tác vụ (Thêm ví dụ trong Phụ lục D.1).

Chúng tôi báo cáo kết quả đánh giá thực nghiệm của mình trên mười tác vụ và ba seeds per cặp dataset-PE.

## 4 Tác động của Mã hóa Vị trí là gì?

Trong phần này, chúng tôi cung cấp kết quả so sánh của các mã hóa vị trí trong tổng quát hóa chiều dài. Để cung cấp cái nhìn toàn diện, theo Liang et al. (2022), chúng tôi báo cáo xếp hạng trung bình của các mô hình khác nhau trong Hình 1 và 2 khi so sánh với nhau cho tất cả các tác vụ và kịch bản. Hơn nữa, chúng tôi trình bày độ chính xác của các mô hình được đánh giá trên các ví dụ có độ dài khác nhau trong Hình 3. (Kết quả chi tiết cho mỗi tác vụ và kịch bản có thể được tìm thấy trong Phụ lục E).

Đầu tiên, chúng tôi quan sát thấy rằng trong hầu hết các tác vụ, các mô hình đạt được độ chính xác hoàn hảo hoặc gần hoàn hảo (Hình 3) trên các độ dài I.I.D., điều này chỉ ra rằng các mô hình không có vấn đề gì trong việc fitting dữ liệu huấn luyện. Tuy nhiên, sự khác biệt giữa các phương pháp mã hóa vị trí trở nên rõ ràng hơn khi chúng tôi đánh giá trên các độ dài lớn hơn so với những gì đã thấy trong quá trình huấn luyện. Trong hầu hết các kịch bản ngoại suy, Bias Tương đối của T5 vượt trội hơn các mã hóa vị trí rõ ràng khác. ALiBi đặt mình ở giữa, trong khi APE và Rotary cho thấy hiệu suất tổng quát hóa kém.

Mặc dù Rotary thường được coi là một phương pháp mã hóa tương đối (Ontanon et al., 2022), kết quả của chúng tôi cho thấy rằng nó hoạt động giống APE hơn là các sơ đồ tương đối khác. Hơn nữa, ALiBi, bất chấp lời hứa của nó về tổng quát hóa chiều dài, hoạt động kém hơn so với Bias Tương đối của T5 trong hầu hết các trường hợp. Kết quả này phù hợp với Taylor et al. (2022) người đã không tìm thấy cải thiện đáng kể từ ALiBi.

Đáng ngạc nhiên, mô hình NoPE, chỉ là một Transformer chỉ decoder mà không có bất kỳ mã hóa vị trí nào, hoạt động ngang bằng hoặc thậm chí tốt hơn PE rõ ràng hoạt động tốt nhất, Bias Tương đối của T5. NoPE đạt được cùng mức độ tổng quát hóa mà không có bất kỳ overhead tính toán nào vì nó không tính toán bất kỳ thuật ngữ bổ sung nào trong cơ chế attention. Thuộc tính này có tác động trực tiếp đến thời gian chạy và memory footprint của mô hình. Ví dụ, Press et al. (2022) báo cáo rằng tính toán bổ sung phát sinh bởi Bias Tương đối của T5 có thể làm cho thời gian huấn luyện và suy luận của mô hình chậm hơn gần hai lần so với Transformer với APE.

## 5 NoPE Biểu diễn Vị trí như thế nào?

Hiệu suất đáng ngạc nhiên của mô hình NoPE gợi ý rằng nó nắm bắt được thông tin vị trí hữu ích cũng có thể tổng quát hóa. Nhưng, cách nó làm điều đó là câu hỏi chính. Trong hai phần tiếp theo, chúng tôi cung cấp phân tích lý thuyết và thực nghiệm hướng tới việc trả lời câu hỏi này.

### 5.1 NoPE có thể biểu diễn cả PE tuyệt đối và tương đối về mặt lý thuyết

Cho fθ là một mô hình Transformer chỉ decoder NoPE, trong đó θ biểu thị các tham số mô hình. fθ xử lý chuỗi đầu vào x= [<bos> , x1, . . . , x T] bằng cách áp dụng một chuỗi các layer. Lưu ý rằng vì fθ không có bất kỳ PE nào, đầu vào x không được tăng cường với thông tin vị trí (ví dụ: [1,2, . . . , T ]). Mỗi layer l, bao gồm các đầu self-attention và một sub-layer feed-forward, đọc trạng thái ẩn trước đó H(l−1) và tạo ra trạng thái ẩn tại layer l: Hl. Mỗi đầu được tham số hóa bởi một query WQ, key WK, value WV, và output WO matrices, trong đó WQ,WK,WV∈Rh×d và WO∈Rd×h. d và h là kích thước trạng thái ẩn của mô hình và chiều attention, tương ứng. W1,W2∈Rd×k.d là các ma trận trọng số của sub-layer feed-forward.

**Định lý 1 (Mã hóa Tuyệt đối)** Cho x là một chuỗi đầu vào có độ dài T+ 1 đến mô hình. Khi đó, layer đầu tiên của fθ có thể khôi phục các vị trí tuyệt đối [1, . . . , T + 1] trong trạng thái ẩn H(1). Nghĩa là, tồn tại WQ,WK,WV,WO,W1, và W2 sao cho các hoạt động self-attention và feedforward trong layer đầu tiên tính toán các vị trí tuyệt đối và ghi nó vào trạng thái ẩn tiếp theo.

Chúng tôi tham khảo Phụ lục C.1 để biết chứng minh đầy đủ của Định lý 1. Định lý này cho thấy rằng stochastic gradient descent (SGD) có thể học để khôi phục các vị trí tuyệt đối trong các Transformer NoPE. Tiếp theo, chúng tôi chứng minh cách PE tương đối có thể được thực hiện trong các layer tiếp theo:

**Định lý 2 (Mã hóa Tương đối)** Giả sử rằng trạng thái ẩn H(1) chứa thông tin vị trí tuyệt đối, như đã nêu trong Định lý 1, và giả sử rằng nó không bị ghi đè bởi bất kỳ layer tiếp theo nào. Khi đó, self-attention trong tất cả các layer tiếp theo có thể thực hiện một mã hóa vị trí tương đối: tồn tại một tham số hóa của fθ sao cho, với l≥2, tích vô hướng attention giữa query qn và key km tại vị trí n và m có thể được biểu diễn như:

⟨qn,km⟩=fcnt(q,k) +frel(n−m) (1)

trong đó fcnt là một hàm của nội dung của chúng, và frel là một hàm của khoảng cách tương đối của chúng.

Phụ lục C.2 cung cấp chứng minh đầy đủ của Định lý 2. Kết quả lý thuyết của chúng tôi gợi ý rằng SGD có thể chọn giữa mã hóa tương đối và tuyệt đối trong các Transformer NoPE. Nhưng, cơ chế nào SGD học trong thực tế thì không rõ ràng. Chúng tôi tiếp theo điều tra câu hỏi này một cách thực nghiệm.

### 5.2 NoPE học sử dụng PE tương đối trong thực tế

Để khám phá các cơ chế mà NoPE sử dụng trong thực tế, chúng tôi tiến hành một phân tích định lượng bằng cách so sánh mẫu attention của nó với các mô hình được huấn luyện với các kỹ thuật mã hóa vị trí khác nhau. Giả thuyết là nếu NoPE sử dụng một thuật toán tương tự như các PE khác, thì các mẫu attention của những mô hình này sẽ khá giống nhau.

Để làm điều này, chúng tôi đưa cùng một đầu vào cho cả hai mô hình và, tại layer l, chúng tôi tính toán khoảng cách tối thiểu giữa phân phối attention của bất kỳ đầu nào trong mô hình đầu tiên và bất kỳ đầu nào trong mô hình thứ hai. Chính thức, cho Pt=p(k|qt) là một phân phối xác suất được tạo bởi một đầu self-attention causal cho query tại vị trí t, trên các key k∈[k1, . . .kt] trong một layer transformer nhất định. Trên một chuỗi có độ dài T, chúng tôi định nghĩa sự tương đồng giữa hai đầu P và Q là DAT(P,Q) = 1/T ∑T t=1DJSD(Pt||Qt) là trung bình Jensen–Shannon divergence (JSD) giữa hai đầu trên tất cả các vị trí. Đối với khoảng cách của hai mô hình A và B tại layer l, chúng tôi lấy khoảng cách tối thiểu giữa tất cả các cặp đầu attention trong layer tương ứng:

D(l)(A, B) = min(P,Q)∈Al×Bl DAT(P,Q) (2)

trong đó Al và Bl là các đầu attention trong layer l của mô hình A và B tương ứng. Chúng tôi đo lường thực nghiệm khoảng cách giữa NoPE và các sơ đồ mã hóa vị trí khác sau khi huấn luyện. Cụ thể, chúng tôi lấy mẫu các ví dụ từ mỗi bucket độ dài và đưa chúng (sự nối của đầu vào và đầu ra vàng) để tính toán các bản đồ attention và khoảng cách sử dụng Phương trình (2). Chúng tôi cũng xem xét khoảng cách giữa các seed khác nhau của NoPE như một baseline. Hình 4 cho thấy khoảng cách per layer cho bốn layer đầu tiên. (các layer sau cho thấy xu hướng tương tự Hình F.7). Chúng tôi thấy rằng các mẫu attention của NoPE giống nhất với T5's Relative PE, và ít giống nhất với APE và Rotary. Cùng xu hướng có thể được quan sát trên tất cả các layer và bucket độ dài, và thậm chí khi tính trung bình trên tất cả các layer. Những kết quả này có thể gợi ý rằng một mô hình Transformer mà không có mã hóa vị trí, được huấn luyện với stochastic gradient descent học cách biểu diễn vị trí theo cách tương tự như T5's Relative PE, là một sơ đồ mã hóa vị trí tương đối.

## 6 Scratchpad có làm cho Lựa chọn Mã hóa Vị trí trở nên Không liên quan?

Trong prompting scratchpad/CoT, mô hình tạo ra các tính toán trung gian cần thiết để đạt được câu trả lời cuối cùng như các phần rõ ràng của đầu ra. Những cơ chế như vậy, có hiệu quả, cung cấp một sự phân tách và lưu trữ trực tiếp cho các giá trị trung gian, đã được chỉ ra là cải thiện tổng quát hóa chiều dài của Transformers ngay cả ở quy mô nhỏ (Nye et al., 2021). Vì scratchpad chỉ sửa đổi đầu vào và đầu ra của mô hình (không phải kiến trúc), không rõ ràng và chưa được khám phá cách các lựa chọn kiến trúc như mã hóa vị trí ảnh hưởng đến tổng quát hóa chiều dài khi có mặt scratchpad. Để trả lời câu hỏi này, chúng tôi huấn luyện tất cả PE với và không có scratchpad trên nhóm tác vụ toán học và lý luận, và so sánh hiệu suất của chúng.

Hơn nữa, quyết định về cách biểu diễn các tính toán trung gian trong scratchpad, tức là định dạng scratchpad, là một lựa chọn thiết kế quan trọng có tác động không nhỏ đến hiệu suất của mô hình (Bueno et al., 2022).

Để tính đến những điều đó, chúng tôi xem xét năm thành phần trong mỗi bước của scratchpad: <input>, <computation>, <output>, <variable_update>, và <remaining_input> (Hình 5). Trong các thí nghiệm của chúng tôi, chúng tôi tạo ra các biến thể khác nhau của định dạng scratchpad bằng cách bật hoặc tắt mỗi thành phần, điều này cho phép chúng tôi nghiên cứu có hệ thống tác động của chúng. Hình 6 tóm tắt kết quả của chúng tôi. Tương tự như những nhận xét được đưa ra bởi (Nye et al., 2021; Anil et al., 2022), chúng tôi quan sát thấy rằng trên tất cả PE và bất kể định dạng, scratchpad chỉ có lợi cho tác vụ cộng. Ngoài ra, các phát hiện của chúng tôi chỉ ra rằng việc có một mã hóa vị trí với tổng quát hóa chiều dài mạnh mẽ là quan trọng vì scratchpad/CoT một mình có thể không tăng cường tổng quát hóa.

### 6.1 Phần nào của chuỗi được chú ý đến?

Định dạng scratchpad thường được sử dụng (Nye et al., 2021), tương tự như Hình 5, chứa thông tin dư thừa. Một ví dụ như vậy là sự lặp lại của phần còn lại của một đầu vào (R) trong mỗi bước của scratchpad. Nhưng, attention có thể chú ý đến thông tin này trực tiếp từ đầu vào chính. Vì vậy, vẫn không rõ ràng phần cụ thể nào của scratchpad mà các PE khác nhau dựa vào để giải quyết tác vụ.

Để giải quyết câu hỏi này, chúng tôi lấy các mô hình được huấn luyện với Định dạng đầy đủ trên phép cộng, trường hợp mà scratchpad hữu ích trên tất cả PE, và kiểm tra các attention của chúng. Cụ thể, đối với các token trong chuỗi đầu ra, chúng tôi tính toán khoảng cách d giữa query hiện tại qt và key được chú ý kn là (t−n+1) và sau đó chuẩn hóa nó dựa trên độ dài của chuỗi tại bước hiện tại. Giá trị chuẩn hóa được ký hiệu là d̄.

Hình 7 mô tả phân phối của d̄. Các giá trị của d̄ gần 0 chỉ ra attention đến các token gần vị trí hiện tại (ví dụ: bước scratchpad hiện tại), trong khi các giá trị gần 1 biểu thị attention đến các token xa (ví dụ: đầu vào). NoPE và T5's Relative PE giống nhau và thể hiện một phân phối bimodal, phản ánh cả attention tầm ngắn và tầm xa. Ngược lại, ALiBi (do bias gần đây của nó) mạnh mẽ ưu tiên attention tầm ngắn. Rotary, mặt khác, tạo ra một phân phối giống APE, được phân phối đều hơn. Đáng chú ý, NoPE và T5's RPE là các PE hoạt động tốt nhất trong thiết lập này, điều này gợi ý phân phối bimodal là tối ưu hơn.

## 7 Thảo luận

Các practitioners phải đưa ra những lựa chọn quan trọng về các nét tinh tế của kiến trúc Transformer như mã hóa vị trí trước khi thực hiện quá trình pretraining tốn kém. Trong đánh giá I.I.D của PE, chúng tôi chứng minh hiệu suất tương tự trên các PE khác nhau, phù hợp với các quan sát của Haviv et al. (2022) và Scao et al. (2022b), điều này làm cho việc lựa chọn mã hóa vị trí tối ưu trở nên thách thức.

Trong bài báo của chúng tôi, chúng tôi sử dụng tổng quát hóa chiều dài trong các tác vụ downstream như một phương tiện để đánh giá tính biểu đạt của các mã hóa vị trí. Thiết lập của chúng tôi, trái ngược với đánh giá I.I.D., tiết lộ một sự phân biệt rõ ràng giữa các phương pháp mã hóa vị trí. Chúng tôi thấy rằng NoPE vượt trội hơn các PE rõ ràng, và trong các PE rõ ràng, các phương pháp được sử dụng phổ biến tụt hậu so với T5's Relative PE. Thực tế, việc phát hành gần đây của các LLM (Touvron et al., 2023; Chowdhery et al., 2022) gợi ý một sự chuyển dịch hướng tới việc áp dụng Rotary như một sự thay thế cho APE trong kiến trúc Transformer. Tuy nhiên, kết quả của chúng tôi trong Phần 4 rõ ràng chứng minh rằng Rotary chỉ vượt trội APE một chút trong tổng quát hóa chiều dài. Hơn nữa, nó thể hiện hành vi tương tự như APE, như được chỉ ra trong Phần 6.1, cho thấy khả năng dễ bị ảnh hưởng bởi những hạn chế tương tự.

Những bất lợi của PE rõ ràng so với NoPE trong ngoại suy chiều dài góp phần vào bằng chứng ngày càng tăng rằng mã hóa vị trí đặt ra thách thức cho Transformers (Sinha et al., 2022; Luo et al., 2021). Kết quả thực nghiệm và phân tích lý thuyết của chúng tôi gợi ý rằng việc loại bỏ mã hóa vị trí có triển vọng như một sửa đổi cho kiến trúc Transformer chỉ decoder được sử dụng rộng rãi.

**Mở rộng lên mô hình 1B** Để nghiên cứu hành vi của position embeddings ở quy mô lớn, chúng tôi đã huấn luyện ba biến thể 1B sau khi nộp bài – ALiBi, Rotary và NoPE – với độ dài ngữ cảnh 1024 token trên một tập con của tập huấn luyện StarCoder (Li et al., 2023). Để biết thêm chi tiết, tham khảo Phụ lục F. Kết quả của chúng tôi về language modeling cho thấy rằng ở I.I.D tất cả các biến thể có perplexity tương tự, nhưng ở tổng quát hóa chiều dài, Rotary không thể tổng quát khi perplexity của nó bùng nổ. NoPE và ALiBi tổng quát tương tự đến kích thước ngữ cảnh lớn hơn lên đến gần gấp đôi kích thước ngữ cảnh huấn luyện của chúng, và đối với các ngữ cảnh lớn hơn ALiBi tương đối ổn định hơn NoPE (xem thảo luận về perplexity vs. hiệu suất downstream). Khám phá sơ bộ về fine-tuning các mô hình pretrained, trên các tập dữ liệu trong Phần 3, dẫn đến hiệu suất giống hệt nhau giữa các biến thể PE vì kích thước ngữ cảnh huấn luyện của các mô hình 1.3B lớn hơn nhiều so với độ dài instance trong các tập dữ liệu của chúng tôi. Một đánh giá downstream toàn diện của những mô hình này vẫn là chủ đề cho nghiên cứu tương lai.

**Perplexity vs. hiệu suất Downstream** Do các ràng buộc nhận thức của con người (Gibson, 1998; Kiyono et al., 2021), dữ liệu language modeling có thể bao gồm các phụ thuộc tầm ngắn. Sự kết hợp của cấu trúc tự nhiên này (có thể có nhiều trong các corpus dựa trên internet) với Recency Bias vốn có trong các mã hóa vị trí như ALiBi có thể mô tả một đại diện không thực tế về hiệu suất tổng quát hóa chiều dài của các mô hình. Thực tế, Chi et al. (2023) gần đây đã chứng minh rằng hiệu suất tổng quát hóa chiều dài của ALiBi có thể được sao chép bằng cách sử dụng một attention mask cửa sổ, trong đó các token vượt quá kích thước cửa sổ w bị che. Thú vị, chúng tôi cũng quan sát thấy rằng T5's Relative PE, có thể được coi là phiên bản có thể huấn luyện của ALiBi, học cách chú ý đến cả phụ thuộc tầm xa và tầm gần (Hình F.3). Điều này phù hợp với quan sát của Tay et al. (2022) và nhấn mạnh tầm quan trọng của các thiết lập đánh giá trên các tác vụ downstream so với việc chỉ dựa vào perplexity.

## 8 Công trình Liên quan

**Thất bại Tổng quát hóa Chiều dài Trong Transformers** Vấn đề tổng quát hóa chiều dài đã là một chủ đề quan tâm trong nghiên cứu các mô hình chuỗi neural từ lâu (Graves et al., 2016; Kaiser and Sutskever, 2016; Lake and Baroni, 2018; Hupkes et al., 2020; Yehudai et al., 2021). Transformers, là các mô hình chuỗi state-of-the-art, cũng không phải là ngoại lệ. Một nhóm nghiên cứu đã chỉ ra thất bại tổng quát hóa của các Transformer thông thường với APE trên các tập dữ liệu cụ thể như PCFG (Hupkes et al., 2020), LEGO (Zhang et al., 2023), hoặc CLUTRR (Sinha et al., 2019; Gontier et al., 2020). Vấn đề tổng quát hóa chiều dài đã được báo cáo ngay cả trong các Transformer pretrained như T5 (Furrer et al., 2020) và LaMDA (Anil et al., 2022). Csordás et al. (2021) và Ontanon et al. (2022) nghiên cứu tác động của mã hóa vị trí đối với tổng quát hóa chiều dài nhưng chủ yếu tập trung vào việc chỉ ra PE tương đối vượt trội hơn APE. Press et al. (2022), mặt khác, đề xuất một phương pháp mã hóa mới, ALiBi, và chứng minh rằng nó vượt trội hơn các PE phổ biến trong ngoại suy nhưng chỉ trong bối cảnh human language modeling. Liên quan nhất là nghiên cứu gần đây của Deletang et al. (2023) về tổng quát hóa chiều dài trong các mô hình chuỗi neural khác nhau (bao gồm RNN và Stacked-RNN) cho các tác vụ từ phân cấp Chomsky. Tuy nhiên, họ không tập trung vào sự khác biệt giữa mã hóa vị trí hoặc trên các mô hình autoregressive. Không giống như những nghiên cứu này, công trình của chúng tôi so sánh rộng rãi tổng quát hóa chiều dài trong các PE phổ biến cho một loạt các tác vụ, cụ thể tập trung vào các mô hình autoregressive, đại diện cho nhiều LLM đương đại.

**Mã hóa Vị trí** Một thành phần cốt lõi của Transformers là cơ chế mã hóa vị trí, giúp mô hình biểu diễn thứ tự của chuỗi đầu vào. Cơ chế self-attention trong encoder của Transformers là order-invariant và yêu cầu PE để tránh trở thành một mô hình bag-of-word. Nhiều phương pháp đã được đề xuất cho mục đích này. Ban đầu, Vaswani et al. (2017) giới thiệu các hàm sinusoidal mã hóa vị trí tuyệt đối (một biến thể học được được phổ biến bởi Devlin et al. (2019)). Phương pháp tương đối để mã hóa thông tin vị trí được giới thiệu thêm bởi Shaw et al. (2018), điều này đã tạo ra một số LM pretrained với mã hóa tương đối như TransformerXL (Dai et al., 2019) và T5 (Raffel et al., 2020) hoạt động tốt trong tổng quát hóa chiều dài. Gần đây hơn, Su et al. (2021) lấy khái niệm các hàm sinusoidal và gợi ý một cách mới để mã hóa thông tin vị trí bằng cách xoay các biểu diễn ẩn trước khi áp dụng self-attention. Phương pháp này, được gọi là Rotary, đã trở thành lựa chọn phổ biến trong các LLM gần đây. Press et al. (2022) đơn giản hóa mã hóa Tương đối của T5 và giới thiệu một biến thể hiệu quả hơn gọi là ALiBi, trong khi giữ hiệu suất ngoại suy tương tự hoặc cải thiện. Các Transformer chỉ decoder, do causal attention mask của chúng, không phải là order-agnostic và có thể hoạt động mà không có thông tin vị trí rõ ràng. Điều này đã được quan sát sớm bởi Shen et al. (2018) và sau đó được giải thích bởi Tsai et al. (2019). Quan sát rằng các Transformer mà không có mã hóa vị trí có thể hoạt động ngang bằng với PE rõ ràng đã được thực hiện trong các lĩnh vực khác nhau như dịch máy (Yang et al., 2019), language modelling (Irie et al., 2019; Haviv et al., 2022), và thậm chí các lĩnh vực khác như thị giác hoặc giọng nói (Likhomanenko et al., 2021). Trong công trình của chúng tôi, không chỉ chúng tôi chứng minh rằng các Transformer có thể hoạt động mà không có thông tin vị trí rõ ràng, mà chúng tôi còn trình bày một thiết lập quan trọng nơi chúng vượt trội hơn các PE rõ ràng. Hơn nữa, chúng tôi chỉ ra về mặt lý thuyết cách chúng có khả năng học cả mã hóa tuyệt đối và tương đối.

## 9 Kết luận

Chúng tôi đã nghiên cứu tính mạnh mẽ của các mã hóa vị trí khác nhau, trong các Transformer chỉ decoder, trong tổng quát hóa chiều dài trên các tác vụ toán học và lý luận downstream khác nhau. Nghiên cứu thực nghiệm rộng rãi của chúng tôi cho thấy hiệu quả của NoPE, và chứng minh thêm rằng các PE rõ ràng được sử dụng rộng rãi không phù hợp cho tổng quát hóa chiều dài. Chúng tôi cũng chứng minh rằng NoPE có thể học ngầm cả vị trí tuyệt đối và tương đối, nhưng sử dụng cái sau trong thực tế. Cuối cùng, chúng tôi thấy hiệu quả của scratchpad phụ thuộc vào tác vụ, và không phải là một giải pháp đáng tin cậy cho tổng quát hóa chiều dài.

## Hạn chế

Công trình của chúng tôi chủ yếu tập trung vào mã hóa vị trí như một lựa chọn thiết kế trong kiến trúc decoder Transformers. Chúng tôi không thể nghiên cứu cách pretraining quy mô lớn ảnh hưởng đến các PE khác nhau vì không có các mô hình ngôn ngữ lớn được huấn luyện công khai với các PE khác nhau trong các điều kiện tương tự. Chúng tôi để lại điều này cho công việc tương lai do ngân sách tính toán hạn chế của chúng tôi.

## Lời cảm ơn

Chúng tôi biết ơn các nhà đánh giá ẩn danh, Nicolas Chapados, và Omer Levy vì những gợi ý và thảo luận vô giá của họ. Chương trình tài trợ Mila-IBM đã cung cấp tài trợ cho dự án này. SR ghi nhận sự hỗ trợ được cung cấp bởi chương trình NSERC Discovery Grant và chương trình Facebook CIFAR AI Chair. Nghiên cứu này được hỗ trợ một phần bởi các tài nguyên tính toán được cung cấp bởi Mila, Digital Research Alliance of Canada và ServiceNow.
