# 2311.03033.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/prompt/2311.03033.pdf
# File size: 242705 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2311.03033v1  [cs.LG]  6 Nov 2023Beyond Words: A Mathematical Framework for Interpreting
Large Language Models
Javier González and Aditya V. Nori
Microsoft Research
Abstract
Large language models (LLMs) are powerful AI tools that can g enerate and comprehend
natural language text and other complex information. Howev er, the ﬁeld lacks a mathemat-
ical framework to systematically describe, compare and improve LLMs. We pr opose Hexa
framework that clariﬁes key terms and concepts in LLM resear ch, such as hallucinations, align-
ment, self-veriﬁcation and chain-of-thought reasoning. T heHexframework oﬀers a precise and
consistent way to characterize LLMs, identify their streng ths and weaknesses, and integrate
new ﬁndings. Using Hex, we diﬀerentiate chain-of-thought reasoning from chain-o f-thought
prompting and establish the conditions under which they are equivalent. This distinction clari-
ﬁes the basic assumptions behind chain-of-thought prompti ng and its implications for methods
that use it, such as self-veriﬁcation and prompt programmin g.
Our goal is to provide a formal framework for LLMs that can hel p both researchers and
practitioners explore new possibilities for generative AI . We do not claim to have a deﬁnitive
solution, but rather a tool for opening up new research avenu es. We argue that our formal
deﬁnitions and results are crucial for advancing the discus sion on how to build generative
AI systems that are safe, reliable, fair and robust, especia lly in domains like healthcare and
software engineering.
1 Introduction
Large language models (LLMs) are powerful tools that can gen erate text responses to various input
prompts, such as queries or commands. To interact with an LLM , a human user provides a prompt
and receives a response from the model, which is based on its i nternal algorithms. The user then
evaluates the response, and decides whether it answers the p rompt satisfactorily. This process may
be repeated several times until the user reaches a desired ou tcome (or gives up).
State-of-the-art LLMs, such as PaLM [ CND+22], GPT-4 [ Ope23 ] or LLaMA [ TLI+23], can
exhibit both remarkable and baﬄing behaviours, depending o n the task. Peter Lee captures this
paradox by saying that they are both " smarter and dumber than you " at math, statistics and
logic [ Sch23 ]. What causes this discrepancy? Research has primarily foc used on testing LLMs on
diﬀerent tasks so the user can gain insights about their beha viour. For GPT-4, a large collection of
behaviours have been studied on several tasks [ BCE+23]. Benchmarking eﬀorts have been conducted
in various domains such as causal discovery [ KNST23 ], summarization [ ZLD+23] and reasoning
[SPH+23]. These research directions are reasonable, given that mos t LLMs are black-boxes that we
cannot easily inspect, interpret or explain.
1

--- PAGE 2 ---
In this paper, we oﬀer a new perspective on interpreting LLMs and present a mathematical
framework that formalises and generalises what is already k nown about LLMs. To advance LLM
research, we pursue two objectives: First, we deﬁne key LLM c oncepts such as alignment [ SJH+23],
hallucinations [ JLF+23] and chain-of-thought reasoning [ WWS+23] using our mathematical frame-
work. Second, we provide a reasoning tool that facilitates a common understanding of existing LLM
research and form a basis for exploring new questions and cha llenges. Our framework, called Hex,
is based on commutative diagrams that relate diﬀerent level s of abstraction and computation in
LLMs. The framework itself does not aim to provide all the ans wers but it represents a framework
that can be used by researchers in LLMs to share, position and reason about their ﬁndings.
We show how Hexcan capture the essence of LLMs as abstract execution machin es that use
natural language as an interface, and how it can reveal the as sumptions and implications of various
methods that use LLMs, such as self-veriﬁcation and prompt p rogramming. We also show how Hex
can formalize and evaluate the output of LLMs using distance metrics and compatibility relations.
Our goal is to provide a formal framework for LLMs that can hel p both researchers and prac-
titioners explore new possibilities for generative AI. We d o not claim to have a deﬁnitive solution,
but rather a tool for opening up new research avenues. We argu e that our formal deﬁnitions and
results are crucial for advancing the discussion on how to bu ild generative AI systems that are safe,
reliable, fair and robust, especially in domains like healt hcare and software engineering.
The rest of the paper is organized as follows. Section 2introduces the Hexframework and its
basic deﬁnitions. Section 3shows how the Hexframework can formalize common LLM concepts,
such as hallucination, alignment, and chain-of-thought re asoning. Section 4discusses the conditions
of when chain-of-thought prompting elicits chain-of-thou ght reasoning, and their broader implica-
tions for LLM methods and how they are used in practice. Secti on5concludes the paper and
outlines future work.
2 The mathematical framework
LLMs are abstract execution machines that use natural langu age as an interface. Given an input
prompt that captures the user’s intention, an LLM produces a n output answer in the form of
text that is presented to the user. The prompt aims to solve a p roblem, such as a mathematical
operation, a summarization, or the generation of a certain t ype of text. The prompt always expresses
anintention to execute some operations based on some provided informati on. The LLM, which
operates as a black-box, uses the internal memory acquired a t training time to provide an answer
by sampling from some probability distribution conditione d on the input prompt1.
This process has become extremely powerful in the latest gen eration of LLMs, creating a new set
of terms for AI research and practice. Alignment, hallucina tions, chain-of-thought reasoning, self-
veriﬁcation and prompt programming are some of the concepts that AI researchers and practitioners
use routinely – our work allows us to formally deﬁne these con cepts using a uniﬁed mathematical
framework (Section 3). We introduce the deﬁnitions needed for our framework next .
Concrete state: A concrete state is a function that assigns values from a conc rete domain to
a (ﬁnite) set of variables. We use Vto denote the set of variables, and Cto denote the
concrete domain. We write σ:V → C to indicate that σis a concrete state. For example,
suppose V={x,y}andC=Z, the set of integers. Then, one possible concrete state is
1This is computed by choosing the next word that best ﬁts the pr evious text, and repeating the process for each
word.
2

--- PAGE 3 ---
σ={x/mapsto→1,y/mapsto→2}, which means that σ(x) = 1 andσ(y) = 2. The set of all concrete states
overVandCis denoted by 2V→C.
Problem: Letq: 2V→C→2V→Cbe a query, and let σbe a concrete state. Informally, the query
qexpresses what a user wishes to compute over a state σ. For example, qmight select some
variables, apply some operations, or ﬁlter some conditions . A problem is deﬁned by the
query-state pair (q,σ).
Prompt: A prompt is a natural language expression of a problem (q,σ)that an LLM can solve2.
A prompt (q,σ)consists of two strings: a query string qthat represents the query qin words,
and a state string σthat describes the concrete state σin words. We denote by Tthe set of
all strings.
LLM: Given a prompt (q,σ)as input, an LLM Minterprets this to produce a natural language or
string output.
Abstract state: A state string σhas a meaning that can be captured by a vector, which is an
abstract state ˆσor an embedding. An LLM works with these vectors to manipulat e state
strings. We denote by Athe set of all abstract states.
Abstraction map: An abstraction map αis a function that transforms a concrete state σinto
an abstract state ˆσ. The function αconsists of two sub-functions: (i) αc, which converts the
concrete state σto a state string ¯σ(this would be part of the prompt design process) and
(ii)αa, which encodes the text state ¯σinto an abstract state ˆσ(this is the LLM encoder). In
other words, α(σ) = (αa◦αc)(σ).
Concretization map: a concretization map γconcretizes an abstract state ˆσby producing a
concrete state σ. The function γconsists of two steps: (i) γa, which decodes the abstract
stateˆσinto a state string ¯σusing an LLM (the LLM decoder), and (ii) γc, which interprets
the state string ¯σas a description of the concrete output state (the human inte rpretation of
the LLM output). In other words, γ(ˆσ) = (γc◦γa)(ˆσ).
Figure 1shows the Hexdiagram3for an LLM Mand a problem (q,σ), whereqis a query and
σis a concrete state. The corners of the diagram are states and the vertical and horizontal edges
connect diﬀerent levels of abstraction. The vertical edges are based on the abstraction maps αc
andαa, and the concretization maps γaandγc. The horizontal edges are based on the functionals
ΛqandΛˆq, which compute the query qor its abstract counterpart ˆqon the corresponding states.
These functionals are deﬁned as follows:
Λq(σ)def=q(σ)applies the query qto the concrete state σ.
Λˆq(ˆσ)def= ˆq(ˆσ)computes the abstract query ˆq(this is the LLM M’s interpretation of the query
stringq) on the abstract state ˆσ=α(σ).
AHexdiagram represents diﬀerent ways or paths to solve a problem (q,σi). Each path starts
from an initial concrete state σiand ends at a ﬁnal concrete state σi+1. The diagram is commutative
if and only if every path leads to a compatible σi+1. In other words, if and only if Λq(σi)≡
2Both elements can be mixed in some prompts. Although our theo ry is general, we separate them to simplify our
exposition.
3A commutative diagram with six edges.
3

--- PAGE 4 ---
ˆσi∈ A ˆσi+1∈ A
σi∈ T σi+1∈ T
σi∈2V→Cσi+1∈2V→Cαcαa γa
γc
ΛqΛˆq
Figure 1: The Hexdiagram for a problem (q,σi).
(γc◦γa◦Λˆq◦αa◦αc)(σi), where≡is a suitable compatibility relation. Section 3.1gives some
examples of compatibility relations, such as equality.
The path starting with the state string σiand ending in the state string σi+1traces the execution
of the LLM M. The abstract states ˆσiandˆσi+1and the function Λˆqare unobservable when the
LLM is a blackbox.
2.1Hexexamples
In this section, we show how the Hexelements work together via two examples. One example is
an arithmetic problem that needs logical thinking, and the o ther is a code generation problem that
needs both logic and some creativity.
2.1.1 Simple arithmetic problem
Problem : Solve for z=x+ywherex= 12,y= 13.
TheHexinstance for this problem is deﬁned as follows.
• Concrete state: The set of variables V={x,y,z}, wherex,yandzare the variables of
the problem. The concrete domain Cis the set of integers Z. The concrete initial state
σi={x/mapsto→12,y/mapsto→13,z/mapsto→⊥,}, where⊥represents an undeﬁned value.
• Problem: The query q=λσ . σ(x)+σ(y), and the problem is the pair (q,σi).
• The functional Λq(σi)computes the concrete output state σi+1.
•σi+1={x/mapsto→12,y/mapsto→13,z/mapsto→25}. This is the correct answer that the user expects from the
LLM, in other words, the ground truth.
• Abstraction map: The function αcmaps the concrete input state σito a state string σi
which is encoded within the text prompt (shown below). The LL M then interprets the state
stringσi(represented by the substring "x = 12, y = 13" ) and produces ˆσi=αc(σi)which
corresponds to an unobservable internal LLM state.
4

--- PAGE 5 ---
• The functional Λˆqis an unknown, functional that computes the abstract query ˆq, speciﬁed by
the query string q="Solve for z = x+y" of the prompt shown below.
Simple arithmetic problem
Prompt :
final_answer = GPT4("Solve for z = x+y, where x = 12, y = 13.")
Answer:
The value of z is 25.
• The LLM output for Prompt is "The value of z is 25. ", and this is represented by the
state string σi+1.
• Concretization map: The concretization function γcinterprets the text state σi+1as a concrete
state{x/mapsto→12,y/mapsto→13,z/mapsto→25}which is equal to the ground truth concrete state σi+1. This
relies on the LLM computing the correct answer, which means t hat the corresponding Hex
diagram is commutative (and consequently, there is alignme nt).
This example has a unique and precise solution, so we can easi ly check that the Hexdiagram
commutes. However, as we will see later, this is not always th e case.
2.1.2 Sketching a unicorn
Problem : Write TikZ code to draw a unicorn.
TheHexinstance for this problem is deﬁned as follows.
• Concrete state: We use a set of variables V={x,body,legs,head,tail,horn,m }to represent
the information needed to draw an animal using TikZ, which is a language for creating graphics
in LaTeX. The variable xholds the name of the animal we want to draw (string value), su ch
as unicorn, and the variables body,legs,head,tail,hornhold the TikZ code for drawing each
part of the animal, such as a circle, a line, or a curve. The var iablemis a slack variable that
can be used to store any additional information or code that i s not captured by the other
variables. We start with an initial state σi, wherexis assigned the value "unicorn" and all
the other variables are assigned the undeﬁned value ⊥, meaning that we do not have any code
for drawing the unicorn yet.
• Problem: The query qis a TikZ code generator that takes σias input draws a diagram of
σi(x), and the problem is the pair (q,σi).
• The functional Λq(σi)produces the concrete output state σi+1. In particular, σi+1={x/mapsto→
"unicorn" , body/mapsto→code for body , legs/mapsto→code for legs , head/mapsto→code for head ,
tail/mapsto→code for tail , horn/mapsto→code for horn , m/mapsto→⊥}. This is the expected answer for
the user, or the ground truth, which we deﬁne as any code that s ketches the unicorn. However,
there are many possible ways to write such code. In Section 3.2, we explain how we measure
the quality of the LLM’s answers.
5

--- PAGE 6 ---
• Abstraction map: We use the function αcto turn the concrete input state σiinto a state string
σithat we can include in the text prompt (shown below). The stat e string is σi="unicorn"
that the LLM has to interpret. The LLM then produces an intern al stateˆσi=αc(σi)that we
cannot observe.
• We also have an unknown functional Λˆqthat takes the string q="draw in Tikz" from the
prompt and computes the abstract query ˆq.
Prompt :
answer = GPT4("You are a programmer that uses TikZ to generat e diagrams:
draw a unicorn.")
Answer :
\begin{tikzpicture}
% Body
\draw[fill=gray!50] (0, 0) circle (1cm);
\draw[fill=gray!50] (1.5, 0) circle (1cm);
\draw (1, 0) ellipse (0.7cm and 1.5cm);
% Legs
\draw (0.4, -1) -- (0.2, -2.5);
\draw (0.6, -1) -- (0.8, -2.5);
\draw (1.9, -1) -- (1.7, -2.5);
\draw (2.1, -1) -- (2.3, -2.5);
% Head
\draw[fill=gray!50] (2.7, 1.4) circle (0.7cm);
\draw (2.3, 1.5) -- (2.1, 1.2);
% Tail
\draw (0.0, 0.7) to[out=120, in=30] (-1, 1.2);
% Horn
\draw[thick] (2.7, 2.05) -- (2.9, 3);
\end{tikzpicture}
Result after compiling the generated code:
• The LLM output for Prompt is "begin{tikzpicture}... end{tikzpicture} ", and this
is the string state σi+1.
6

--- PAGE 7 ---
• Concretization map: The concretization function γcinterprets the text state σi+1as a concrete
state. In this example, the text state is a piece of code, so we can compile it to see what it
does. The compiled code indeed draws a shape that resembles a n unicorn.
Unlike the arithmetic example, this problem has multiple so lutions and the relation that makes
theHexdiagram commute depends on the scenario. For example, diﬀer ent human evaluations
may have diﬀerent standards for how much the ﬁgure resembles an unicorn. Others may care more
about the clarity of the code.
3 LLM concepts with Hex
This section shows how the Hexframework can formalize common LLM concepts.
3.1 Hallucination and misalignment
The LLM literature often uses the terms hallucination andmisalignment to describe two common
issues with these models. Hallucination means that LLMs pro duce wrong or irrelevant responses
to prompts, but make them sound reasonable or coherent [ JLF+23]. This can be dangerous when
LLMs are used in critical domains where accuracy and safety a re important [ MLC+23]. Alignment
means that LLMs act in accordance with their human users’ int entions [ SJH+23]. LLMs that are
misaligned act diﬀerently from what their users want. This c an also cause harm, such as giving
wrong answers, generating biased outputs or discriminatin g results [ WWLS23 ]. Alignment involves
tuning LLMs to encourage desired behaviors and discourage u ndesired ones.
We use the Hexdiagram to discuss the terms hallucination and misalignmen t, which are generic
terms in the literature. Given a problem (q,σ), we also need a compatibility relation ≡to compare
the equivalence of two concrete states. Hallucination and m isalignment occur when the Hexdiagram
for(q,σ)does not commute, meaning that the paths from σto the output concrete state do
not produce equivalent states under ≡. Formally, this means that the compatibility relationship
(γ◦Λˆq◦α)(σ)≡Λq(σ), where(γ◦Λˆq◦α)(σ)is the answer generated by the LLM, does not hold.
We want to detect hallucinations or misalignments by compar ing states, so we need a suitable
notion of equivalence ( ≡) that captures the kind of errors we are interested in. For ex ample, consider
the simple arithmetic problem in Section 2.1.1, where the correct answer is {x/mapsto→12,y/mapsto→13,z/mapsto→
25,w/mapsto→⊥}. If the LLM outputs " The sum of x and y is 29 ", this corresponds to the state
{x/mapsto→12,y/mapsto→13,z/mapsto→29,w/mapsto→⊥} under the concretisation function γ. This state is a hallucination,
because it does not match the correct answer. One possible wa y to deﬁne an equivalence relation
is to compare the values of variables in two concrete states σ1andσ2. We say that σ1andσ2are
equivalent, if they have the same value for every variable in V. Formally, (σ1≡σ2)⇐⇒ ∀x∈
V. σ1(x) =σ2(x). According to this deﬁnition, the only state equivalent to {x/mapsto→12,y/mapsto→13,z/mapsto→
25,w/mapsto→⊥} is itself.
However, suppose the LLM outputs " The sum of x and y is a tiger ". This answer is wrong
and also misaligned, because it contains a nonsensical outp ut that is not an integer. In this case,
the only way for the two states to be aligned is if every variab le in the scope has an integer value.
That is,(σ1≡σ2)⇐⇒ ∀x∈ V.typeof(σ1(x)) =typeof(σ2(x)) =int.
To evaluate how well an LLM performs a task, we need to measure the extent of its hallucinations
or misalignments. How can we do that? We discuss this in the ne xt section.
7

--- PAGE 8 ---
3.2 Evaluating the output of an LLM
Given a problem (q,σ), the output of an LLM Mis deﬁned by the state string σ= (γa◦Λˆq◦α)(σ).
Applying the γcfunction to σresults in the concrete state γc(σ). The compatibility relationship
introduced in Section 3.1is crucial for determining whether the Hexdiagram for (q,σ)is commu-
tative. If it is not, a natural question to ask is how much that LLM’s answer deviates from the one
that would make the Hexdiagram commutative.
We can evaluate how well an LLM solves a problem by comparing i ts output state, σ, with a
reference state that represents the correct answer, Λq(σ). To do this, we need a distance metric
∆ : 2V→C×2V→C→R+that quantiﬁes how diﬀerent two states are. The choice of ∆depends on
the problem scenario, but it must satisfy a consistency cond ition: the Hexdiagram must commute,
meaning that the concrete state derived from the LLM’s outpu t,γc(σ), must have zero distance
from the reference ground truth state, Λq(σ).
For example, suppose we are solving the problem of adding 12 + 13. If the LLM’s output is σ
and we apply γcto it, we get γc(σ) = (12,13,19)T(this is the vectorized state). This is the concrete
state that represents the LLM’s answer. However, the correc t answer is 25, which we can get by
applying Λqto the input σ. This gives us the reference ground truth state Λq(σ) = (12,13,25)T.
To measure how far the LLM’s answer is from the correct answer , if we use the L1-norm as the
distance function, we get ∆(γc(σ),Λq(σ)) =|19−25|= 6. This means that the LLM’s answer is oﬀ
by 6 units. The Hexdiagram does not commute in this case, because γc(σ)/ne}ationslash= Λq(σ).
Some problems need complex distance methods to measure how c lose the diagram is to being
commutative. This is hard when there are many valid solution s for the ground truth Λq(σ), such
as in code generation where diﬀerent solutions are acceptab le. The user of the LLM needs to deﬁne
these metrics to capture the right properties for these prob lems.
4 Chain-of-Thought
Chain-of-Thought (CoT) [ WWS+23] is a method for improving the ability of LLMs to perform
various reasoning tasks, such as answering questions, solv ing math problems, or generating expla-
nations. The idea is that some problems are too complex or abs tract for the LLM to solve directly,
and it can beneﬁt from breaking them down into simpler or more concrete sub-problems. To do
this, the method involves prompting the LLM to generate inte rmediate steps that lead to the ﬁnal
answer, and using these steps as additional context or feedb ack for the next step.
For example, imagine drawing a hexagon with a circle inside i t, and a square inside the circle.
The steps ( CoT reasoning ) are:
1. Draw a hexagon.
2. Draw the circle inside the hexagon.
3. Draw a square inside the circle.
The CoT reasoning method can help the model to avoid errors, g aps, or inconsistencies in its
reasoning, and to produce more coherent and transparent out puts.
To formalize CoT reasoning, we use the Hexframework. Consider a problem (q,σi), whereqis
a query and σiis a concrete input state. We can split this problem into two s ub problems (q1,σ1)
and(q2,σ2), such that the query transformation Λqis the same as applying Λq1ﬁrst and then Λq2,
withσi=σ1andσ2= Λq1(σ1), other words, Λq(σi) = (Λ q2◦Λq1)(σi).
8

--- PAGE 9 ---
ˆσ1 ˆσ2 ˆσ3 ˆσ4
σ1 σ2 σ3 σ4
σi σ2 σ2 σi+1Λq1Λˆq1
Λq2Λˆq2
Figure 2: Chain-of-thought reasoning.
ˆσ1 ˆσ2
σ1 σ2
σi σ2 σi+1Λq1Λˆq1.q2
Λq2
Figure 3: Chain-of-thought prompting.
Figure 2illustrates the Hexdiagram for CoT reasoning, which combines the Hexdiagrams for
two sub-problems: (q1,σ1)and(q2,σ2), whereσi=σ1.
Lemma 1. Suppose we have a problem (q,σi)that can be solved by splitting it into two sub-problems
(q1,σ1)and(q2,σ2), such that Λ(σi) = (Λ q2◦Λq1)(σi). Suppose also that Figure 2shows how to
use the CoT reasoning method to solve (q,σi). Then the CoT reasoning is valid if and only if every
possible way of transforming σiintoΛq(σi)in the diagram gives the same result, i.e., the diagram
commutes.
Proof. Follows from that fact that commutativity implies no halluc ination or misalignment.
To elicit CoT reasoning, CoT prompting embeds it in the LLM’s prompt [ WWS+23]. This
method is convenient because it avoids extra calls to the LLM , which can be costly. Figure 3
illustrates the Hexdiagram for CoT prompting – in particular, we split the probl em(q,σi)into
two sub problems (q1,σ1)and(q2,σ2), such that Λq= Λq1◦Λq2– in other words, CoT reasoning
holds for the concrete problem. Let (q1,_)and(q2,_)be the prompts for the problems (q1,σ1)and
(q2,σ2)respectively. Then we concatenate the query strings of the s ubproblem prompts to form the
CoT prompt for the original problem: (q1.q2,σi). We denote the abstract query corresponding to
the CoT prompt by ˆq1.q2, and the LLM function that takes the CoT prompt as input and pr oduces
the text state σ2as the output by Λˆq1.q2. Then the following holds.
9

--- PAGE 10 ---
Lemma 2. Suppose we have a problem (q,σi)that can be solved by splitting it into two sub-problems
(q1,σ1)and(q2,σ2), such that Λ(σi) = (Λ q2◦Λq1)(σi). Suppose also that Figure 3shows how to
use the CoT prompting method to solve (q,σi). Then CoT prompting is valid if and only if every
possible way of transforming σiintoΛq(σi)in the diagram gives the same result, i.e., the diagram
commutes.
Proof. Follows from that fact that commutativity implies no halluc ination or misalignment.
Corollary 3. CoT prompting and CoT reasoning produce the same output for a problem(q,σ)if
and only if both of these conditions are true:
•CoT reasoning holds for the problem (q,σ). That is, Λq= Λq1◦Λq2.
•TheHexdiagrams for CoT prompting and CoT reasoning are both commut ative.
CoT prompting and CoT reasoning are equivalent only under th e conditions given by Corollary 3.
However, many systems (including engineering APIs) rely on CoT prompting without verifying
this. This can lead to incorrect outcomes, as we will illustr ate. In general, CoT prompting is not a
suﬃcient for enabling CoT reasoning!
The following example shows a zero-shot prompting task for G PT-4: generate TikZ code to draw
a circle inside a hexagon, and a square inside the circle. How ever, GPT-4’s solution is incorrect. It
draws a circle inside a hexagon and a square inside the circle , as shown below.
Zero-shot prompting with GPT-4
Problem : Draw a circle inside an hexagon and a square inside the circl e using TikZ.
Prompt:
final_answer = GPT4("You are a programmer that uses TikZ to g enerate diagrams.
Draw a circle inside an hexagon and a square inside the circle .")
Result after compiling the generated code:
With CoT prompting, or trying to solve the problem step by ste p in the same prompt, GPT-4 draws
a square inside a circle and the square inside another circle , as shown below. Unfortunately, this is
still incorrect and in fact, worse than the zero-shot instan ce, which did not draw an incorrect shape
(the square instead of a hexagon).
10

--- PAGE 11 ---
CoT prompting with GPT-4
Prompt:
final_answer = GPT4("You are a programmer that uses TikZ to g enerate diagrams.
First, draw a hexagon. Then, draw a circle inside the hexagon . Then, draw
a square inside the circle.")
Result after compiling the generated code:
Finally, with CoT reasoning, GPT-4 draws the correct ﬁgure.
CoT reasoning with GPT-4:
Prompt1:
answer1 = GPT4("You are a programmer that uses TikZ to genera te diagrams.
Draw an hexagon.")
Prompt2:
answer2 =GPT4("You are a programmer that uses TikZ to genera te diagrams.
A hexagon can be drawn as {answer1}. Draw a circle inside the h exagon.")
Prompt3
final_answer =GPT4("You are a programmer that uses TikZ to g enerate
diagrams. An hexagon can be drawn as {answer1}. A circle insi de the
hexagon can be drawn as {answer2}. Draw a square inside the ci rcle.")
Result after compiling the generated code:
Corollary 3has important consequences on familiar concepts in the LLM s pace. These include
self-veriﬁcation and prompt engineering, which we will dis cuss next.
11

--- PAGE 12 ---
ˆσ1 ˆσ2 ˆσ3 ˆσ4
σ1 σ2 σ3 σ4
σi σ2 σ2 σi+1Λq1Λˆq1
Λq2Λˆq2
Figure 4: Self-veriﬁcation.
4.1 Self-veriﬁcation
Self-veriﬁcation [ GSC+23,WZX+23] is the ability of LLMs to assess the validity of their own out put.
In problems solved with self-veriﬁcation, the same LLM play s two diﬀerent roles: 1) it is a generator
of solutions, and 2) a discriminator of potentially wrong an swers. But how can an LLM that has
produced a wrong answer check itself? We explain this using t heHexframework.
Self veriﬁcation involves two sub-problems in one. First, a problem (q1,σ1)that generates an
output state σ2= Λq1(σ1). Second, a veriﬁcation problem (q2,σ2)that takes σ2as input and
produces a state σ3that indicates whether σ2is correct or not. This process can be represented
by composing the Hexdiagrams for the two sub-problems (q1,σ1)and(q2,σ2). This is analogous
to the CoT reasoning. Through Hex, there is a theoretical justiﬁcation for the empirical evid ence
in the literature that self-veriﬁcation improves accuracy : a commutative diagram, like the one in
Figure 4, where the two sub-diagrams are the generation and veriﬁcat ion steps.
Self-veriﬁcation with generation and veriﬁcation in the sa me prompt
Prompt:
answer = GPT4("You are an expert in political history. Give m e a list of
names of 20 politicians that were born in the city of Chicago. After
generating the list of names, create another list with the ci ties where they
were born.")
Answer:
List of Politicians born in Chicago:
1. Hillary Clinton 2. Nancy Pelosi 3. Carol Moseley Braun 4. R ahm Emanuel
5. Dan Rostenkowski 6. Harold Washington 7. Bobby Rush 8. Rod Blagojevich
9. Timuel Black 10. John W. E. Thomas
Their respective birth cities (all in Chicago):
1. Chicago, Illinois 2. Chicago, Illinois 3. Chicago, Illin ois 4. Chicago,
Illinois 5. Chicago, Illinois 6. Chicago, Illinois 7. Chica go, Illinois
8. Chicago, Illinois 9. Chicago, Illinois 10. Chicago, Illi nois
12

--- PAGE 13 ---
The simple example above illustrates this. We combine the ge neration and veriﬁcation steps
in the same prompt. In the generation step, the model produce s a list of 10 names, but it has no
way of asserting if they were all born in Chicago. In fact, Nan cy Pelosi, Bobby Rush and Timuel
Black were born elsewhere. However, when we split the task in to two steps – generating the list
and verifying it – we see that the model can identify its own mi stakes.
On the other hand, as seen below, splitting the problem into s everal LLM calls, rather than
one uniﬁed prompt, helps in this example too. This is just an i llustration, and the self-veriﬁcation
literature actually uses separate prompts for generating a nd verifying answers.
Self-veriﬁcation with veriﬁcation in a separate prompt
Prompt:
answer = GPT4("You are an expert in political history. Give m e a list with
the cities where these politicians were born: 1. Hillary Cli nton 2. Nancy
Pelosi 3. Carol Moseley Braun 4. Rahm Emanuel 5. Dan Rostenko wski
6. Harold Washington 7. Bobby Rush 8. Rod Blagojevich 9. Timu el Black
10. John W. E. Thomas.")
Answer:
1. Hillary Clinton - Born in Chicago, Illinois, United State s
2. Nancy Pelosi - Born in Baltimore, Maryland, United States
3. Carol Moseley Braun - Born in Chicago, Illinois, United St ates
4. Rahm Emanuel - Born in Chicago, Illinois, United States
5. Dan Rostenkowski - Born in Chicago, Illinois, United Stat es
6. Harold Washington - Born in Chicago, Illinois, United Sta tes
7. Bobby Rush - Born in Albany, Georgia, United States
8. Rod Blagojevich - Born in Chicago, Illinois, United State s
9. Timuel Black - Born in Birmingham, Alabama, United States
10. John W. E. Thomas - Born in Springfield, Illinois, United States.
Generating and verifying answers may involve multiple sub- problems, each of which could ben-
eﬁt from CoT reasoning. For instance, [ WZX+23] used CoT prompting to generate questions for
veriﬁcation, but we argue that CoT reasoning could also help to decompose the generation and
veriﬁcation processes further, when they require multiple steps. Some empirical evidence supports
this idea. The recent chain-of-veriﬁcation work [ DKX+23] compared the performance of LLMs
when they used the same prompt for both planning and executin g the veriﬁcation, or when they
used separate prompts for each step. The results showed that the latter approach, which follows
CoT reasoning principles, outperformed the former, which f ollows CoT prompting principles.
4.2 Prompt programming
The text structure of a prompt can aﬀect how large language mo dels (LLMs) process and respond
to it. We need a method to create and evaluate prompts that mat ch our goals and expectations for
LLMs, and that suit their text-processing capabilities. Th is is what prompt programming is about:
a systematic way to design and test prompts for LLMs [ RM21 ].
A clear and concise prompt has two basic components: a query a nd a state. The query tells
the model what the user wants to do, such as summarize a text or caption an image. The state
13

--- PAGE 14 ---
provides the model with the data or information it needs to do it, such as the text or the image.
This separation makes it easier to design prompts that can ad apt to diﬀerent scenarios.
The OpenAI API in Azure [ azu23 ] (and other platforms where GPT-4 is available) provides a
natural way to separate the query and the state in the prompt. The API lets users structure the
prompt into three sections: "system" ,"user" , and"assistant" . These sections correspond to the
key elements of the Hexdiagram in Figure 1:
• The"system" section deﬁnes the role of the LLM. It is the query component o f the prompt,
¯q, that expresses the question qthat we want to answer.
• The"user" section provides the speciﬁcs of the question. It is the stat e component of the
prompt, ¯σi, that represents the textual version of the initial state σi.
• The"assistant" section allows the user to write the state ¯σi+1explicitly. This is useful for
few-shot learning approaches, where multiple pairs of prom pts and responses are given to the
LLMs before the ﬁnal question is asked.
The following is an example of using the Python API to write a zero-shot prompt for adding
two numbers. This is the same example as in Section 2.1.
Zero-shot learning
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=[
{"role": "system", "content": "you are calculator that can compute
the sum of x and y."},
{"role": "user", "content": " x=12 and y =13."}])
LLM output: "The sum of x and y is 12 + 13 = 25."
The same API also supports few-shot learning, which require s some examples of questions and
answers that are related to the query as shown below.
14

--- PAGE 15 ---
Few-shot learning
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=[
{"role": "system", "content": "Assistant is an intelligen t chatbot
designed to help users answer history and political questio ns. "},
{"role": "user", "content": "Who was the first president of the
United States?"},
{"role": "assistant", "content": "George Washington."},
{"role": "user", "content": "What is the capital of France? "},
{"role": "assistant", "content": "The capital of France is Paris."},
{"role": "user", "content": "Who wrote the novel Pride and P rejudice?"},
{"role": "assistant", "content": "Jane Austen."},
{"role": "user", "content": "Who is the current Prime Minis ter of
the United Kingdom?"}])
LLM output: "The current Prime Minister of the United Kingdo m is
Rishi Sunak."
This API lets us create CoT prompts by providing the assistan t with step-by-step solutions of the
examples as its input.
Chain-of-thought promptinga
import openai
...
response = openai.ChatCompletion.create(
engine="gpt-4",
messages=
{"role": "system", "content": "Assistant is a travel plann er "},
{"role": "user", "content": "Which is a faster way to get hom e?
Option 1: Take a 10 minutes bus, then a 40 minute bus, and
finally a 10 minute train. Option 2: Take a 90 minutes train,
then a 45 minute bike ride, and finally a 10 minute bus."},
{"role": "assistant", "content": "Option 1 will take 10+40 +10 = 60
minutes. Option 2 will take 90+45+10=145 minutes. Since
Option 1 takes 60 minutes and Option 2 takes 145 minutes,
Option 1 is faster."},
{"role": "user", "content": "Which is a faster way to get to w ork?
Option 1: Take a 1000 minute bus, then a half hour train, and
finally a 10 minute bike ride. Option 2: Take an 800 minute bus ,
then an hour train, and finally a 30 minute bike ride."}])
aExample adapted from https://learnprompting.org/docs/i ntermediate/chain_of_thought
15

--- PAGE 16 ---
LLM output: "Option 1 will take 1000+30+10 = 1040 minutes. Op tion 2
will take 800+60+30 = 890 minutes. Since Option 2 takes 890 mi nutes
and Option 1 takes 1040 minutes, Option 2 is faster."
This API gives the user a lot of ﬂexibility to structure the pr oblem and assign diﬀerent roles to
the parts of the prompt. However, it does not show how the serv ice combines these components
internally. As Section 2explains, CoT reasoning involves strong assumptions and mu ltiple calls to
the LLM (one for each step), unlike CoT prompting that uses a s ingle prompt. This enables more
diverse and eﬀective ways to use and deploy models like GPT-4 with CoT reasoning ideas. This is
especially helpful in domains like healthcare, where a doma in expert needs to deﬁne and execute
sub-tasks of a problem. We hope that this work contributes to advancing these directions.
5 Discussion
LLMs are widely used in AI for various tasks, applications an d services, but they also pose challenges
and risks, such as generating inaccurate or misleading outp uts, or failing to align with human values
and expectations. We presented a mathematical framework to analyse, compare and improve LLMs
in a systematic way. Our framework deﬁnes hallucination and alignment, and reveals the factors
that aﬀect our interactions with LLMs. We show that a common m ethod of prompting LLMs,
which splits a query into sub-queries, does not ensure sound reasoning. This ﬁnding has important
implications for building reliable and robust AI systems wi th LLMs. The key idea is how to
guarantee the self-consistency of the model, or how to impro ve both models and prompts to ensure
factually correct answers that lead to commutative diagram s. Depending on the problem, this may
require more detailed instructions in a CoT prompting fashi on, or multiple calls to the model. We
do not provide a recipe for each case, but we characterize the se issues and highlight the diﬀerences.
Our mathematical framework reveals some promising directi ons for further research on LLMs. A
key challenge in AI is to ensure that LLMs learn abstract repr esentations from data that respect some
invariances or general principles. On that note, the Hexdiagram shows how its components relate
to various design principles or choices for LLMs. For exampl e, prompt engineering aims to ﬁnd good
abstraction mappings αcthat make the model’s input and output more consistent. On th e other
hand,γcdetermines how the model produces answers. In the current co nversational use of LLMs,
this involves learning from human feedback (RLHF). We can al so ﬁne-tune the representations for
the internal state ˆσiand the mapping Λˆqfor speciﬁc applications. We hope that our work helps
address these challenges and accelerate progress in AI.
Acknowledgements
We thank Javier Alvaraz-Valle, Shruthi Bannur and Ted Meeds for their valuable feedback, which
helped us improve our work. We also thank Peter Lee, Sriram Ra jamani and Desney Tan for
inspiring conversations that sparked some of our ideas.
References
[azu23] Azure OpenAI API. https://learn.microsoft.com/en-us/azure/ai-services /openai ,
2023.
16

--- PAGE 17 ---
[BCE+23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, J ohn A. Gehrke, Eric Horvitz,
Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lund berg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of A rtiﬁcial General Intel-
ligence: Early experiments with GPT-4. ArXiv , abs/2303.12712, 2023.
[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maar ten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchen ko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodk umar Prabhakaran,
Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duk e, Anselm Levskaya,
Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Ga rcía, Vedant Misra,
Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, Da vid Luan, Hyeontaek
Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, Davi d Dohan, Shivani Agrawal,
Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayan a Pillai, Marie Pel-
lat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksand r Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan F irat, Michele Catasta,
Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeﬀ De an, Slav Petrov, and Noah
Fiedel. Palm: Scaling language modeling with pathways. ArXiv , abs/2204.02311, 2022.
[DKX+23] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli
Celikyilmaz, and Jason Weston. Chain-of-veriﬁcation redu ces hallucination in large
language models, 2023.
[GSC+23] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann , Michel Galley, Jian-
feng Gao, and Hoifung Poon. Self-veriﬁcation improves few- shot clinical information
extraction, 2023.
[JLF+23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Y an Xu, Etsuko Ishii, Ye Jin
Bang, Andrea Madotto, and Pascale Fung. Survey of hallucina tion in natural language
generation. ACM Computing Surveys , 55(12):1–38, mar 2023.
[KNST23] Emre Kiciman, Robert Ness, Amit Sharma, and Chenha o Tan. Causal reasoning and
large language models: Opening a new frontier for causality .CoRR , abs/2305.00050,
2023.
[MLC+23] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hos seini, Mark Johnson,
and Mark Steedman. Sources of hallucination by large langua ge models on inference
tasks, 2023.
[Ope23] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023.
[RM21] Laria Reynolds and Kyle McDonell. Prompt programmin g for large language models:
Beyond the few-shot paradigm. CoRR , abs/2102.07350, 2021.
[Sch23] Charlotte Schubert. Gpt-4 and medicine: Microsoft research head peter lee on the ai
chatbot’s potential and limitations. GeekWire , October 2023.
[SJH+23] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilo ng Dong, Zisha Guo, Xinwei
Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv
preprint arXiv:2309.15025 , 2023.
17

--- PAGE 18 ---
[SPH+23] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pr anav Tadepalli, Paula Vidas,
Alexander Kranias, John J. Nay, Kshitij Gupta, and Aran Koma tsuzaki. Arb: Advanced
reasoning benchmark for large language models, 2023.
[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier M artinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hamb ro, Faisal Azhar, Aure-
lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaum e Lample. Llama: Open
and eﬃcient foundation language models, 02 2023.
[WWLS23] Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shash ua. Fundamental limitations
of alignment in large language models. CoRR , abs/2304.11082, 2023.
[WWS+23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in
large language models, 2023.
[WZX+23] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang L iu, and Jun Zhao. Large
language models are better reasoners with self-veriﬁcatio n, 2023.
[ZLD+23] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, K athleen McKeown, and
Tatsunori Hashimoto. Benchmarking large language models f or news summarization.
ArXiv , abs/2301.13848, 2023.
18
