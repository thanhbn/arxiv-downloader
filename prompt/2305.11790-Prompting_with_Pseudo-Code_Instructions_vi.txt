# 2305.11790.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/prompt/2305.11790.pdf
# Kích thước file: 474538 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Prompting với Hướng dẫn Pseudo-Code
Mayank Mishra∗, Prince Kumar∗, Riyaz Bhat,
Rudra Murthy V , Danish Contractor, Srikanth Tamilselvam
IBM Research AI
{mayank.mishra1,prince.kumar12, riyaz.bhat,danish.contractor}@ibm.com,
{rmurthyv,srikanth.tamilselvam}@in.ibm.com

Tóm tắt
Prompting với hướng dẫn ngôn ngữ tự nhiên gần đây đã nổi lên như một phương pháp phổ biến để khai thác khả năng của các mô hình ngôn ngữ lớn (LLM). Do tính mơ hồ vốn có trong ngôn ngữ tự nhiên, việc xem xét các lợi thế có thể có của prompting với các phong cách prompt ít mơ hồ hơn, như pseudo-code, là điều trực quan.

Trong bài báo này, chúng tôi khám phá xem liệu prompting thông qua hướng dẫn pseudo-code có giúp cải thiện hiệu suất của các mô hình ngôn ngữ được huấn luyện trước hay không. Chúng tôi tự tạo một bộ dữ liệu¹ gồm các prompt pseudo-code cho 132 nhiệm vụ khác nhau trải rộng các tác vụ phân loại, QA và sinh ngôn ngữ, được lấy từ bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b). Sử dụng các prompt này cùng với các đối tác của chúng bằng ngôn ngữ tự nhiên, chúng tôi nghiên cứu hiệu suất của chúng trên hai họ LLM - BLOOM (Scao et al., 2023), CodeGen (Nijkamp et al., 2023). Các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng hướng dẫn pseudo-code dẫn đến kết quả tốt hơn, với mức tăng trung bình (tuyệt đối) là 7-16 điểm trong điểm F1 cho các tác vụ phân loại và một cải thiện (tương đối) là 12-38% trong điểm ROUGE-L tổng hợp trên tất cả các tác vụ. Chúng tôi bao gồm các nghiên cứu ablation chi tiết cho thấy rằng các comment code, docstring và các gợi ý cấu trúc được mã hóa trong pseudo-code đều góp phần vào việc cải thiện hiệu suất. Theo hiểu biết của chúng tôi, công trình của chúng tôi là đầu tiên chứng minh cách các prompt pseudo-code có thể hữu ích trong việc cải thiện hiệu suất của các LM được huấn luyện trước.

1 Giới thiệu
Prompting với hướng dẫn ngôn ngữ tự nhiên gần đây đã nổi lên như một phương pháp phổ biến để khai thác khả năng của các mô hình ngôn ngữ lớn. Ngoài fine-tuning, các mô hình thường được fine-tune sử dụng hướng dẫn trên một bộ sưu tập lớn các bộ dữ liệu

∗Đóng góp ngang nhau
¹Code và bộ dữ liệu có sẵn tại https://github.com/mayank31398/pseudo-code-instructions

Listing 1 Một ví dụ về hướng dẫn pseudo-code cho nhiệm vụ từ Wang et al. (2022b). Một mô hình thành công được kỳ vọng sử dụng các hướng dẫn pseudo-code được cung cấp và đưa ra phản hồi cho một nhóm các instance đánh giá.

1def generate_sentiment(sentence: str) -> str:
2 """Đối với câu đã cho, nhiệm vụ là dự đoán 
3 sentiment. Đối với sentiment tích cực 
4 trả về "positive" ngược lại trả về 
5 "negative".
6
7 Parameters:
8 sentence (str): câu đầu vào
9 Returns:
10 str: sentiment của đầu vào
11 """
12
13 # dự đoán sentiment
14 ifsentiment_is_positive(sentence):
15 return "positive"
16 else :
17 return "negative"
18
19 >>> generate_sentiment(
20 "that has a charmingly bourbon air."
21 )

để giúp cải thiện khả năng của LM trong việc tuân theo hướng dẫn và hiệu suất trên các nhiệm vụ chưa từng thấy (Wei et al., 2022a; Wang et al., 2022b).

Tuy nhiên, các hướng dẫn ngôn ngữ tự nhiên có thể mơ hồ và không được chỉ định đầy đủ, và do đó có nhiều cách diễn giải - việc bao gồm các hướng dẫn chi tiết có thể không luôn có lợi, vì nó có thể làm tăng thêm độ phức tạp của việc lý luận cho các mô hình. Điều này đã dẫn đến sự phát triển của lĩnh vực 'prompt-engineering' nơi các chiến lược prompting chuyên biệt được phát triển cho các domain và loại nhiệm vụ khác nhau (Zhao et al., 2021; Reynolds và McDonell, 2021; Arora et al., 2023; Liu et al., 2023; Zamfirescu-Pereira et al., 2023). Ngoài ra, các chiến lược prompting tại thời điểm suy luận cụ thể hỗ trợ lý luận đa bước cũng đã được tìm thấy là hữu ích - ví dụ: việc bao gồm lý luận chain-of-thought trong các thiết lập few-shot dẫn đến

arXiv:2305.11790v3  [cs.CL]  19 Oct 2023

--- TRANG 2 ---
hiệu suất được cải thiện so với các prompt tiêu chuẩn (Wei et al., 2022b), prompt khét tiếng "Let's think step-by-step" để tăng cường hiệu suất 0-shot (Kojima et al., 2022).

Algorithm 1 Attention Block
1:function TRANSFORMERS _ATTENTION _BLOCK (Q,K,V)
2: Input: Q,K, và V: các ma trận đầu vào.
3: Output: Đầu ra của attention block.
4: scores ←Q·KT
5: attention _weights ←softmax (scores )
6: weighted _values ←attention _weights ·V
7: output ←Pn
i=1weighted _values i
8: return output
9:end function

Do tính mơ hồ vốn có trong ngôn ngữ tự nhiên, việc xem xét các lợi thế của prompting với các phong cách prompt ít mơ hồ hơn, như việc sử dụng pseudo-code, là điều trực quan. Pseudo-code là một tập hợp các cấu trúc giống code không chính thức, có xu hướng dễ hiểu đối với con người nhưng không nhất thiết phải có thể compile/thực thi được. Chúng thường được sử dụng để thể hiện các ý tưởng, quy trình và luồng phức tạp - ví dụ, Algorithm 1 thể hiện một phiên bản tóm tắt về những gì xảy ra trong một Multi-Head Attention block (Vaswani et al., 2017) bằng pseudo-code. Có thể nói, việc thể hiện những ý tưởng tương tự bằng ngôn ngữ tự nhiên có thể dẫn đến sự mơ hồ và có lẽ sẽ yêu cầu văn bản chi tiết để rõ ràng, điều này làm tăng thêm độ phức tạp.

Dựa trên những thành công gần đây trong các tác vụ NLP được thực hiện bởi các mô hình code (Madaan et al., 2022; Zhang et al., 2023a,b), nghiên cứu này nhằm mục đích kiểm tra hiệu quả của việc sử dụng hướng dẫn pseudo-code để prompting như một phương tiện nâng cao hiệu suất mô hình. Nghiên cứu này được thúc đẩy bởi giả thuyết rằng việc sử dụng pseudo-code làm prompt có thể mang lại lợi thế tự nhiên cho các mô hình trong các tác vụ NLP, nhờ vào việc thể hiện các ý tưởng một cách ngắn gọn và rõ ràng hơn trong pseudo-code. Để kiểm tra giả thuyết rằng prompting các mô hình ngôn ngữ lớn với pseudo-code thay vì dữ liệu ngôn ngữ tự nhiên có thể hữu ích, chúng tôi đã tạo ra các prompt pseudo-code² cho 132 nhiệm vụ khác nhau trải rộng 28 loại nhiệm vụ riêng biệt, được lấy từ bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b) (xem Listing 1 làm ví dụ). Sử dụng các prompt này cùng với các đối tác của chúng từ ngôn ngữ tự nhiên, chúng tôi nghiên cứu hiệu suất của chúng trên hai họ LLM: BLOOM (Scao et al., 2023) và CodeGen (Nijkamp et al., 2023). Cả hai họ LLM đều được huấn luyện trên dữ liệu ngôn ngữ tự nhiên cũng như code.

²Các hướng dẫn pseudo-code cho mỗi nhiệm vụ này được tạo bởi các tác giả của bài báo này.

Chúng tôi so sánh hiệu suất của cả hai phong cách prompt trên các tác vụ phân loại, tác vụ QA, cũng như một hỗn hợp các tác vụ sinh ngôn ngữ khác. Các thí nghiệm của chúng tôi chỉ ra rằng prompting với hướng dẫn pseudo-code thực sự có giúp ích, và chúng dẫn đến một mức tăng tuyệt đối là 7-16 điểm trong điểm F1 trên các tác vụ phân loại, và cải thiện tương đối 12-38% trong điểm ROUGE-L tổng hợp trên tất cả các tác vụ.

Đóng góp: Tóm lại, bài báo của chúng tôi đưa ra các đóng góp sau: (i) Chúng tôi phát hành một bộ dữ liệu gồm 132 prompt pseudo-code trải rộng 28 loại nhiệm vụ khác nhau; (ii) Thông qua một loạt các thí nghiệm chi tiết trên hai họ LLM có thể truy cập công khai, chúng tôi chứng minh cách prompting với hướng dẫn pseudo-code dẫn đến một cải thiện đáng kể về hiệu suất so với prompting với hướng dẫn ngôn ngữ tự nhiên; (iii) Chúng tôi bao gồm các nghiên cứu ablation chi tiết cho thấy rằng các comment code, docstring và các gợi ý cấu trúc được mã hóa trong pseudo-code đều góp phần vào việc cải thiện hiệu suất.

Theo hiểu biết của chúng tôi, công trình của chúng tôi là đầu tiên chứng minh cách các prompt pseudo-code³ có thể hữu ích trong việc cải thiện hiệu suất của các LM được huấn luyện trước. Những phát hiện của chúng tôi không chỉ nhấn mạnh tầm quan trọng của việc tận dụng pseudo-code để prompting mà còn làm sáng tỏ các yếu tố cụ thể trong pseudo-code góp phần vào những cải thiện được quan sát.

2 Công trình liên quan
Fine-tuning các mô hình ngôn ngữ lớn trên các bộ dữ liệu hướng dẫn có thể nâng cao hiệu suất của chúng và thậm chí khả năng tổng quát hóa của chúng cho các nhiệm vụ chưa từng thấy (Wei et al., 2021; Chung et al., 2022). Nhiều khía cạnh của instruction fine-tuning như số lượng nhiệm vụ, kích thước mô hình và fine-tuning trên dữ liệu chain-of-thought đã được tìm thấy là hữu ích (Chung et al., 2022). Do đó, những nỗ lực đáng kể đã được đầu tư vào việc tạo ra các bộ dữ liệu hướng dẫn thủ công, cũng như sử dụng các mô hình sinh hiện có để huấn luyện và đánh giá các mô hình ngôn ngữ (Mishra et al., 2021; Bach et al., 2022; Wang et al., 2022b,a). Các hướng dẫn có sẵn trong các bộ dữ liệu instruction tuning chủ yếu bằng ngôn ngữ tự nhiên, nhưng đã được áp dụng cho cả các nhiệm vụ ngôn ngữ tự nhiên và các nhiệm vụ lập trình. Nhưng các lựa chọn thay thế cho hướng dẫn ngôn ngữ tự nhiên như code ngôn ngữ lập trình, pseudo-code, ký hiệu (MacCartney và Manning, 2007) v.v. chưa được khám phá kỹ lưỡng ngay cả đối với các nhiệm vụ lập trình. So với ngôn ngữ tự nhiên, code hoặc pseudo-code có ít tính mơ hồ hơn do bản chất vốn có của việc sử dụng các function hoặc bước góp phần vào việc hoàn thành một nhiệm vụ. Điều này làm cho chúng trở thành một lựa chọn tự nhiên để chỉ định hướng dẫn. Gần đây, một số công trình (MarvinAI; Madaan et al., 2022; Zhang et al., 2023a,b) đã khám phá code và pseudo-code như các đầu vào. Không giống như công trình đương thời của Zhang et al. (2023a), chúng tôi thấy rằng hướng dẫn pseudo-code thực sự cung cấp hiệu suất tốt hơn so với hướng dẫn NL trên nhiều loại nhiệm vụ khác nhau.

³Trong phần còn lại của bài báo, chúng tôi sử dụng các từ 'pseudo-code' và 'code' có thể hoán đổi cho nhau khi đề cập đến prompt.

--- TRANG 3 ---
3 Bộ dữ liệu
Bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b) bao gồm 1,616 nhiệm vụ NLP đa dạng, và mỗi nhiệm vụ chứa hướng dẫn nhiệm vụ, ví dụ tích cực/tiêu cực và các instance. Chúng tôi lấy mẫu một hỗn hợp gồm 132 nhiệm vụ không yêu cầu khả năng đa ngôn ngữ và viết lại hướng dẫn cho một tập con của bộ dữ liệu này sử dụng các cấu trúc Python. Lưu ý rằng chúng tôi chỉ mượn các cấu trúc Python để thể hiện các prompt của chúng tôi bằng pseudo-code và các prompt của chúng tôi không tạo ra code Python có thể thực thi được. Hơn nữa, chúng tôi không bao gồm bất kỳ bước/hướng dẫn bổ sung nào không có trong hướng dẫn ngôn ngữ tự nhiên gốc.

Tất cả các hướng dẫn nhiệm vụ tuân theo schema được mô tả trong Listing 1. Schema bao gồm các yếu tố sau.

Function Prototype: Điều này định nghĩa prototype của function pseudo-code chính. Các tên function mang tính mô tả và tóm tắt nhiệm vụ cần thực hiện. Chúng cũng bao gồm tất cả các biến được truyền như đầu vào cùng với kiểu dữ liệu và kiểu trả về của chúng. Chúng tôi tuân theo hướng dẫn phong cách PEP 8⁴ để viết pseudo-code và sử dụng các prototype có kiểu mạnh. Chúng tôi tránh khai báo các biến toàn cục bất cứ khi nào có thể và truyền chúng như đối số cho một method. Trong phạm vi có thể, chúng tôi cũng tránh sử dụng các class và enumeration. Dòng số 1 trong Listing 1 cung cấp một ví dụ function prototype cho một nhiệm vụ phân loại sentiment.

DocString: Docstring cung cấp hướng dẫn chi tiết về nhiệm vụ cần thực hiện bằng ngôn ngữ tự nhiên. Thường thì đây là một phiên bản diễn giải của hướng dẫn ngôn ngữ tự nhiên gốc. Docstring kết thúc bằng một danh sách các tham số (với kiểu của chúng) được truyền và kiểu trả về từ

⁴https://peps.python.org/pep-0008/

function. Một ví dụ docstring cho nhiệm vụ phân loại sentiment được trình bày trong các dòng số 2 đến 12 trong Listing 1.

Function Definition: Điều này bao gồm phần lớn hướng dẫn pseudo-code mô tả cách giải quyết nhiệm vụ cụ thể. Trong phạm vi có thể, các định nghĩa function không bỏ sót bất kỳ thông tin nào có trong docstring. Pseudo-code trong định nghĩa function được viết như các function con. Các function con này thường không được định nghĩa và thường sử dụng các tên, đối số và biến mang tính mô tả. Chúng tôi bao gồm các comment nội tuyến chỉ ra những gì được thực hiện bởi function con và vai trò của các đối số nếu cần. Chúng tôi đôi khi cũng định nghĩa các function con thứ cấp nếu nó yêu cầu chi tiết bổ sung hoặc nếu tên function mô tả có thể không đủ để chỉ định mục tiêu của function con. Chúng tôi giả định tính khả dụng của các function helper cơ bản như concat_str, search v.v., và không bao gồm bất kỳ câu lệnh import nào.

Các dòng số 13 đến 16 trình bày định nghĩa function cho nhiệm vụ phân loại sentiment. Function gọi function con sentiment_is_positive để kiểm tra xem sentiment của câu đã cho có tích cực hay không. Function này không được định nghĩa rõ ràng trong hướng dẫn.

Pre-processor: Vì các hướng dẫn pseudo-code mong đợi đầu vào như các đối số, chúng tôi cần phân tích các đầu vào được cung cấp trong bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b) (cung cấp các đầu vào được định dạng trước). Đối với mỗi hướng dẫn pseudo-code, chúng tôi cũng bao gồm một pre-processor python có thể thực thi được sử dụng để phân tích đầu vào.

3.1 Thống kê bộ dữ liệu
Chúng tôi tạo hướng dẫn cho 132 nhiệm vụ có hướng dẫn và các cặp đầu vào/đầu ra bằng tiếng Anh. Chúng tôi nhóm các nhiệm vụ thành ba lớp: Nhiệm vụ Phân loại (Bảng 1), nhiệm vụ QA (Bảng 2) và các nhiệm vụ sinh ngôn ngữ khác (Bảng 3). Những nhiệm vụ này bao gồm tổng cộng 28 danh mục khác nhau và trải rộng 72 bộ dữ liệu duy nhất. Đối với mỗi nhiệm vụ, chúng tôi lấy mẫu 1000 instance để đánh giá.

4 Đánh giá
Để nghiên cứu xem liệu đặc tả hướng dẫn thông qua pseudo-code có dẫn đến hiệu suất được cải thiện so với

--- TRANG 4 ---
hướng dẫn NL tiếng Anh cơ sở hay không, chúng tôi chọn thử nghiệm với các mô hình BLOOM (Scao et al., 2023), CodeGen (Nijkamp et al., 2023). Lựa chọn mô hình của chúng tôi được thúc đẩy bởi thực tế là các mô hình này chưa được instruction-fine-tune trên bộ dữ liệu Natural Instructions. Ngoài ra, cả hai đều được huấn luyện trên dữ liệu code và ngôn ngữ tự nhiên.

[Bảng 1-3 với danh sách các tác vụ và bộ dữ liệu]

Các mô hình BLOOM được huấn luyện trên corpus ROOTS (Laurençon et al., 2022) bao gồm 46 ngôn ngữ tự nhiên và 13 ngôn ngữ lập trình. Mặt khác, các mô hình CodeGen được huấn luyện trên corpus Pile (Gao et al., 2020), các bộ dữ liệu BigQuery và BigPython có sẵn công khai của Google (Nijkamp et al., 2023). Các mô hình BLOOM đã được huấn luyện trên một hỗn hợp ngôn ngữ tự nhiên và code đồng thời. Đối với các mô hình CodeGen mà chúng tôi sử dụng, chúng ban đầu được huấn luyện trên ngôn ngữ tự nhiên và sau đó nhận được

huấn luyện bổ sung tập trung cụ thể vào code Python.

Lựa chọn mô hình của chúng tôi cho phép chúng tôi thiết lập một môi trường có kiểm soát nơi chúng tôi có thể nghiên cứu tác động của prompting bằng ngôn ngữ tự nhiên và pseudo-code.

--- TRANG 5 ---
Hầu hết các mô hình instruction-tuned gần đây đã thấy bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b) dưới một hình thức nào đó (Longpre et al., 2023) hoặc chúng không có tokenizer sẽ xử lý cú pháp code một cách có ý nghĩa (Raffel et al., 2020), và do đó không thể được sử dụng trong nghiên cứu của chúng tôi. Bằng cách nghiên cứu thực nghiệm hiệu suất của các mô hình trên những prompt này, chúng tôi hy vọng sẽ cung cấp thông tin cho các công trình tương lai về việc huấn luyện một mô hình instruction-tuned sử dụng hướng dẫn pseudo-code.

4.1 Cấu hình mô hình
Đối với tất cả các thí nghiệm được thực hiện trong bài báo này, chúng tôi sử dụng các mô hình BLOOM-3B, BLOOM 7B (Scao et al., 2023), CodeGen-mono 2B và CodeGen-mono 6B (Nijkamp et al., 2023). Việc suy luận được thực hiện sử dụng các GPU A100 80 GB. Để tăng tốc suy luận của tất cả các mô hình, chúng tôi sử dụng DeepSpeed-Inference (Aminabadi et al., 2022) trong fp16, dẫn đến cải thiện throughput suy luận trung bình khoảng 1.7x, so với suy luận HuggingFace tiêu chuẩn (Wolf et al., 2020).

Chúng tôi sử dụng greedy decoding cho tất cả các thí nghiệm của chúng tôi để tái tạo được và hạn chế đầu ra được sinh ra trong 100 token. Ngay cả đối với các tác vụ phân loại, chúng tôi sinh ra các nhãn lớp sử dụng auto-regressive decoding thay vì chọn nhãn lớp có perplexity thấp nhất. Điều này được thực hiện vì không phải tất cả các nhãn lớp đều có thể được ánh xạ tới một token duy nhất cho tất cả các nhiệm vụ. Kỹ thuật đánh giá hiệu suất của các tác vụ phân loại này thường được sử dụng khi sử dụng các LLM đóng, như những LLM đằng sau API (ví dụ: GPT4 của OpenAI (OpenAI, 2023), PaLM của Google (Chowdhery et al., 2022) v.v.).

4.2 Metric
Chúng tôi áp dụng các metric khác nhau cho mỗi danh mục nhiệm vụ: chúng tôi đo hiệu suất của các tác vụ phân loại sử dụng điểm F1 micro, macro và weighted, và đối với các tác vụ QA và sinh ngôn ngữ, chúng tôi sử dụng metric ROUGE-L. Chúng tôi báo cáo ROUGE-L, Exact Match (EM) và ANLS - Average Normalized Levenshtein Similarity (Biten et al., 2019) cho tất cả các nhiệm vụ.

4.3 Xử lý hậu đầu ra
Vì các mô hình mà chúng tôi thử nghiệm chưa được fine-tune để tuân theo hướng dẫn, chúng có xu hướng sinh ra văn bản dư thừa sau đầu ra cho nhiệm vụ đã cho. Do đó, chúng tôi xử lý hậu các đầu ra để đảm bảo các mô hình không bị phạt trong đánh giá của chúng tôi do sinh quá nhiều. Chúng tôi xử lý hậu

tất cả các đầu ra bằng cách cắt bớt theo ký tự newline '\n'. Hơn nữa, đầu ra được xử lý hậu bổ sung, bao gồm loại bỏ dấu câu và chuyển thành chữ thường.

4.4 Kết quả
Thông qua các thí nghiệm của chúng tôi, chúng tôi nhằm trả lời các câu hỏi sau: (i) Sự khác biệt về hiệu suất giữa prompting các mô hình ngôn ngữ và code được huấn luyện trước với prompt pseudo-code so với prompt ngôn ngữ tự nhiên là gì? (ii) Việc tăng kích thước mô hình ảnh hưởng như thế nào đến hiệu quả của prompt pseudo-code? (iii) Prompting có cấu trúc, như việc sử dụng tên function, docstring, comment nội tuyến và đối số, tác động đến hiệu suất trên các nhiệm vụ ở mức độ nào?

4.4.1 Prompting với Pseudo-code
Bảng 4 so sánh hiệu suất của prompting với pseudo-code (được gọi là hướng dẫn code) và hướng dẫn ngôn ngữ tự nhiên trong thiết lập 0-shot. Kết quả đã được nhóm theo họ mô hình và kích thước.

Như có thể thấy, đối với tất cả các họ mô hình và kích thước, prompting với pseudo-code dẫn đến một cải thiện đáng kể về hiệu suất. Hiệu suất trên các tác vụ phân loại đặc biệt đáng chú ý, ví dụ, các mức tăng trên weighted F1 dao động từ 7-16 điểm F1 (tuyệt đối). Hơn nữa, cải thiện hiệu suất tương đối trên tất cả các tác vụ khác, được đo bằng ROUGE-L, dao động từ 12-38%. Hiệu suất tổng thể được đo bằng ROUGE-L, ANLS và Exact Match cũng báo cáo các xu hướng tương tự.

So sánh CodeGen vs BLOOM Mặc dù hầu hết các nhiệm vụ không phải là nhiệm vụ code, CodeGen, một mô hình được thiết kế cho các ứng dụng code, vượt trội hơn các mô hình BLOOM, ngay cả khi sử dụng hướng dẫn ngôn ngữ tự nhiên (xem metric cho 'All Tasks'). Hành vi tương tự đã được báo cáo một cách giai thoại (Fu và Khot, 2022; Madaan et al., 2022), nhưng có thể chưa được điều tra sử dụng nhiều nhiệm vụ như được trình bày trong bài báo này. Tuy nhiên, lưu ý rằng việc sử dụng prompt pseudo-code trong các mô hình code dẫn đến hiệu suất tốt hơn so với bất kỳ cấu hình prompt-mô hình nào khác.

Hiệu suất trên các tác vụ QA Thú vị là, chúng tôi thấy rằng trên các tác vụ QA, hiệu suất của hướng dẫn pseudo-code tốt hơn hướng dẫn ngôn ngữ tự nhiên khi sử dụng mô hình CodeGen. Tuy nhiên, điều này không đúng khi sử dụng BLOOM.

--- TRANG 6 ---
[Bảng 4: Hiệu suất của các mô hình khi được prompt sử dụng hướng dẫn pseudo-code và hướng dẫn ngôn ngữ tự nhiên trong thiết lập 0-shot]

[Bảng 5: Hiệu suất 0-shot của các mô hình CodeGen 6B và BLOOM 7B trên các tác vụ QA từ bộ dữ liệu của chúng tôi]

Chúng tôi đã điều tra điều này sâu hơn và quan sát thấy rằng đối với hầu hết các tác vụ QA, các hướng dẫn bằng pseudo-code không chi tiết hơn hoặc dễ hiểu hơn đáng kể so với hướng dẫn ngôn ngữ tự nhiên. Ví dụ, hướng dẫn pseudo-code cho việc sinh câu trả lời từ bộ dữ liệu SQuAD chỉ đơn giản chứa câu lệnh sau trong định nghĩa function của nó: return get_answer_from_passage(passage, question) và phản ánh các chi tiết có trong hướng dẫn tự nhiên.

Chúng tôi đã phân tích thêm kết quả trên các danh mục tác vụ QA và thấy rằng hướng dẫn pseudo-code luôn giúp ích với các tác vụ câu hỏi trắc nghiệm (MCQ) (xem Bảng 5 để so sánh giữa CodeGen 6B và BLOOM 7B). Chúng tôi tin rằng điều này là bởi vì, việc hiểu hướng dẫn trong các tác vụ như vậy có thể phức tạp hơn. Để minh họa, hướng dẫn trong các tác vụ MCQ thường bao gồm chi tiết về cách câu trả lời được mong đợi - ví dụ: "chọn tùy chọn đúng A, B, C", "Chọn Tùy chọn 1 - Giá trị 1, Tùy chọn 2 - Giá trị 2". Tùy thuộc vào hướng dẫn, các mô hình có thể được yêu cầu trả về tùy chọn, giá trị hoặc cả hai, điều này thêm một mức độ phức tạp vào hướng dẫn so với các loại QA khác.

Sự khác biệt về hiệu suất giữa CodeGen và BLOOM trên các tác vụ QA (xem Bảng 5), có thể được quy cho thực tế rằng cấu trúc từ prompt code có thể được tận dụng tốt hơn bởi các mô hình code vì các ngôn ngữ lập trình và các khía cạnh của cú pháp code (cấu trúc) có khả năng được biểu diễn tốt hơn trong một mô hình code như CodeGen. Điều này đưa chúng ta đến câu hỏi tiếp theo - Đóng góp của cấu trúc có thể có trong prompt là gì?

4.4.2 Đóng góp của Cấu trúc trong prompt
Các lý do đằng sau việc cải thiện hiệu suất khi sử dụng prompt pseudo-code có thể là sự kết hợp của các yếu tố, bao gồm việc sử dụng các tên function mô tả truyền đạt mục đích của function (như get_answer(question)), một mô hình có thể sử dụng hiệu quả thông tin có cấu trúc, và một prompt có cấu trúc cho một nhiệm vụ có thể được hưởng lợi thêm từ các ví dụ few-shot.

Do đó, chúng tôi thử nghiệm với các phong cách prompting có cấu trúc khác nhau và báo cáo kết quả của chúng trong Bảng 6. Chúng tôi nghiên cứu hiệu suất của CodeGen và

--- TRANG 7 ---
[Bảng 6: Nghiên cứu về prompt có cấu trúc: Hiệu suất của các mô hình khi được prompt sử dụng hướng dẫn pseudo-code 0-shot, khai báo function trong thiết lập 0-shot và 2-shot cũng như prompting 2-shot với tên function 'chung' và việc sử dụng chỉ các ví dụ]

BLOOM với năm loại prompt: (i) Hướng dẫn Pseudo-code, (ii) Prompt sử dụng khai báo function (chỉ khai báo tên function), (iii) một prompt có cấu trúc chỉ bao gồm các ví dụ nhiệm vụ trong thiết lập 2-shot sử dụng tên function mô tả nhiệm vụ (iv) một prompt có cấu trúc chỉ bao gồm các ví dụ nhiệm vụ trong thiết lập 2-shot sử dụng tên function chung - 'func' (v) sử dụng các ví dụ Ngôn ngữ Tự nhiên (không có hướng dẫn) trong thiết lập 2-shot. Chi tiết về mỗi prompt đã được bao gồm trong Phụ lục.

Chúng tôi có ba quan sát quan trọng từ Bảng 6. Thứ nhất, hướng dẫn code trong thiết lập 0-shot liên tục mang lại hiệu suất tổng thể tốt nhất so với các prompt có cấu trúc khác. Thứ hai, trung bình, mô hình CodeGen liên tục vượt trội hơn BLOOM trên tất cả các nhiệm vụ. Cuối cùng, các tác vụ QA trong bộ dữ liệu của chúng tôi, tương đối dễ thể hiện bằng hướng dẫn ngôn ngữ tự nhiên, cũng được hưởng lợi từ prompt có cấu trúc, đặc biệt khi được prompt với các ví dụ.

Có thể suy ra từ những quan sát này rằng các mức tăng hiệu suất từ việc sử dụng prompt pseudo-code có khả năng do hướng dẫn nhiệm vụ rõ ràng hơn, chứ không chỉ là việc khai thác các mẫu thừa từ in-context learning. Những phát hiện này củng cố kết quả từ thí nghiệm trước, cho thấy rằng các mô hình code có khả năng khai thác prompt có cấu trúc tốt hơn. Trong trường hợp các tác vụ QA trong bộ dữ liệu của chúng tôi, đáng chú ý rằng vì hướng dẫn pseudo-code không chi tiết như vậy, ngay cả việc sử dụng một prompt có cấu trúc đơn giản hơn với các ví dụ có thể nâng cao đáng kể hiệu suất so với prompt ngôn ngữ tự nhiên.

4.4.3 Tác động của tài liệu pseudo-code
Trong phần này, chúng tôi nghiên cứu đóng góp của các comment và docstring có trong hướng dẫn pseudo-code của chúng tôi đối với việc cải thiện hiệu suất. Trước tiên, chúng tôi nghiên cứu hiệu suất của prompt pseudo-code có và không có việc sử dụng docstring và comment code.

Như có thể thấy trong Bảng 7, việc bao gồm comment cũng như docstring trong prompt hướng dẫn pseudo-code giúp cải thiện hiệu suất. Điều này chỉ ra rằng không chỉ cấu trúc của prompt được khai thác bởi mô hình, các mô hình cũng dựa vào văn bản trợ giúp bổ sung có trong tài liệu. Do đó, chúng tôi cũng điều tra xem liệu việc sử dụng những yếu tố này từ pseudo-code có thể cũng có lợi cho prompt hướng dẫn ngôn ngữ tự nhiên hay không.

Nửa dưới của bảng 7 nghiên cứu hiệu suất của prompt ngôn ngữ tự nhiên có và không có việc sử dụng comment pseudo-code và docstring. Chúng tôi

--- TRANG 8 ---
[Bảng 7: Ablation: Thiết lập Zero-Shot. Việc sử dụng comment code và docstring giúp cải thiện hiệu suất của prompt ngôn ngữ tự nhiên]

thấy rằng hiệu suất của hướng dẫn ngôn ngữ tự nhiên cũng được cải thiện bằng việc bao gồm comment và docstring cho mỗi họ mô hình và cấu hình. Chúng tôi đưa ra giả thuyết rằng các mức tăng có thể được quy cho một hình thức lý luận từng bước được rút ra từ comment pseudo-code đặc biệt trong các nhiệm vụ phức tạp.

4.5 Tóm tắt các phát hiện
Bây giờ chúng tôi tóm tắt các phát hiện của chúng tôi để dễ tham khảo.

Tác động của Phong cách Prompting: Từ Bảng 4, chúng tôi quan sát thấy rằng prompting 0-shot các mô hình được huấn luyện trước với prompt pseudo-code dẫn đến hiệu suất tốt hơn so với prompt ngôn ngữ tự nhiên. Điều này đúng cho cả mô hình code và mô hình ngôn ngữ. Các mức tăng rõ rệt hơn đối với các mô hình code.

Tác động của Cấu trúc trong prompt: Prompt pseudo-code bao gồm nhiều yếu tố như khai báo function, docstring, comment v.v. Từ Bảng 6, chúng tôi thấy rằng trong khi thông tin từ khai báo function và tên function chỉ thị nhiệm vụ có giúp ích, việc sử dụng prompt pseudo-code hoàn chính là hữu ích nhất.

Hơn nữa, từ Bảng 7, chúng tôi thấy rằng hướng dẫn pseudo-code vẫn hoạt động tốt hơn bất kỳ prompt nào được tạo với hướng dẫn ngôn ngữ tự nhiên, ngay cả khi docstring và comment từ pseudo-code được bao gồm trong hướng dẫn ngôn ngữ tự nhiên. Điều này cho thấy các mức tăng từ prompting bằng pseudo-code không chỉ do comment và docstring (có thể giúp củng cố hướng dẫn nhiệm vụ), mà còn do hướng dẫn rõ ràng hơn trong pseudo-code.

Tác động của Kích thước Mô hình: Từ Bảng 4, chúng tôi thấy rằng trong thiết lập 0-shot, với việc tăng quy mô, hiệu suất của hướng dẫn pseudo-code được cải thiện cho cả hai họ mô hình. Tuy nhiên, khi sử dụng hướng dẫn ngôn ngữ tự nhiên, điều này không đúng. Chúng tôi đưa ra giả thuyết rằng, vì không có mô hình nào trong số này được instruction-tune, các quy mô lớn hơn làm trầm trọng thêm xu hướng của các mô hình được primed cho việc hoàn thành ngôn ngữ.

Code vs. Mô hình Ngôn ngữ Tự nhiên: Chúng tôi thấy rằng các mô hình code phù hợp hơn để khai thác prompt pseudo-code so với các mô hình ngôn ngữ. Như có thể thấy từ Bảng 4 (xem metric cho 'All Tasks'), việc sử dụng hướng dẫn ngôn ngữ tự nhiên trên CodeGen dẫn đến hiệu suất tốt hơn so với việc sử dụng chúng trên BLOOM.

5 Kết luận và Công trình Tương lai
Trong bài báo này, chúng tôi trình bày công trình của chúng tôi về prompting với hướng dẫn pseudo-code. Chúng tôi tạo ra một bộ sưu tập hướng dẫn pseudo-code bao gồm 132 nhiệm vụ NLP từ bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b). Chúng tôi đánh giá hiệu suất của các họ mô hình sau - CodeGen và BLOOM ở các kích thước mô hình khác nhau và thấy rằng prompting tất cả các mô hình với hướng dẫn pseudo-code dẫn đến mức tăng đáng kể so với prompting với hướng dẫn NL. Công trình của chúng tôi mở ra nhiều hướng nghiên cứu tương lai. Thật thú vị khi quan sát rằng hướng dẫn pseudo-code không chỉ giúp ích khi được sử dụng với các mô hình code, chúng cũng hoạt động tốt hơn trên các mô hình được thiết kế cho các nhiệm vụ ngôn ngữ tự nhiên. Ngoài ra, thực tế rằng các mô hình code

--- TRANG 9 ---
được sử dụng trong các thí nghiệm của chúng tôi hoạt động tốt hơn các mô hình NL, ngay cả khi được prompt với hướng dẫn ngôn ngữ tự nhiên, cho thấy rằng có thể hữu ích khi khám phá instruction tuning của các mô hình code thay vì các mô hình NL thuần túy cho các ứng dụng NL. Dựa trên các phát hiện của bài báo này, có thể cũng hữu ích khi xem xét tác động của instruction fine-tuning với hướng dẫn pseudo-code thay vì hướng dẫn NL.

Một khía cạnh khác đáng nghiên cứu là cách chain-of-thought truyền thống có thể so sánh với prompt pseudo-code - lý luận được kích hoạt bởi hướng dẫn pseudo-code sẽ so sánh như thế nào với lý luận chain-of-thought có và không có fine-tuning? Hơn nữa, hướng dẫn pseudo-code có thể không chỉ được sử dụng như đầu vào trực tiếp cho một mô hình, mà chúng cũng có thể được sử dụng để tạo ra các phản hồi trung gian mà một mô hình cần sinh ra trước khi trả về một phản hồi.

Hạn chế
Kết quả của chúng tôi đã được báo cáo trên hai họ mô hình - CodeGen và BLOOM ở quy mô 2-7B tham số. Vẫn còn phải xem liệu các phát hiện của chúng tôi có giữ được ở các kích thước mô hình lớn hơn hay không. Có thể rằng lý luận tốt hơn được kích hoạt bởi các kích thước mô hình lớn hơn có thể giảm lợi ích của prompting với hướng dẫn pseudo-code nhưng chúng tôi chưa điều tra điều này trong công trình của chúng tôi. Ngoài ra, công trình của chúng tôi không bao gồm bất kỳ nhiệm vụ NLP đa ngôn ngữ nào - BLOOM được huấn luyện cụ thể để có thể hỗ trợ nhiều ngôn ngữ và có thể lựa chọn thiết kế mô hình này có thể đóng một vai trò trong các phát hiện của chúng tôi khi chúng tôi so sánh các mô hình code (CodeGen) và NL (BLOOM) với nhau. Hơn nữa, cả hai mô hình đều được huấn luyện trên các bộ dữ liệu khác nhau và điều này cũng ảnh hưởng đến khả năng lý luận nội tại của các mô hình này. Cuối cùng, và quan trọng, việc sử dụng pseudo-code để prompting LLM bị hạn chế bởi kỳ vọng rằng nó yêu cầu chuyên môn kỹ thuật để viết chúng, do đó giảm việc sử dụng rộng rãi của chúng.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài với các nghiên cứu liên quan]

--- TRANG 10-16 ---
[Tiếp tục danh sách tài liệu tham khảo và phụ lục chi tiết với các ví dụ prompt, bảng kết quả bổ sung và phân tích chi tiết]

A Phụ lục
A.1 Kết quả trên Các LLM Khác nhau
Chúng tôi cũng thực hiện thí nghiệm sử dụng mô hình Falcon-7B (Almazrouei et al., 2023). Kết quả được trình bày trong Bảng 8.

A.2 Xác thực Pseudo-Code
Để đảm bảo rằng các hướng dẫn pseudo-code tuân theo các hướng dẫn được cung cấp, chúng tôi chạy một bài kiểm tra tự động. Code kiểm tra gọi function preprocess được định nghĩa cho mỗi ví dụ từ bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022b) cho nhiệm vụ đó. Các giá trị được trả về từ function preprocess được so sánh với các đối số trong function prototype. Bất kỳ sự không khớp nào trong kiểu dữ liệu hoặc số lượng đối số đều dẫn đến lỗi. Người tạo hướng dẫn được đưa ra phản hồi để sửa các lỗi.

[Phần còn lại của phụ lục bao gồm các ví dụ chi tiết về các phong cách prompt khác nhau, bao gồm prompt pseudo-code, prompt ngôn ngữ tự nhiên, và các biến thể khác nhau được sử dụng trong nghiên cứu]
