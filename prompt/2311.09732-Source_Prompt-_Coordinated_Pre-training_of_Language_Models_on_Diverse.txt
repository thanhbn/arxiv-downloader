# 2311.09732.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/prompt/2311.09732.pdf
# File size: 169958 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Source Prompt: Coordinated Pre-training of Language Models on Diverse
Corpora from Multiple Sources
Yipei Xu, Dakuan Lu, Jiaqing Liang, Xintao Wang, Yipeng Geng, Yingsi Xin, Hengkui Wu, Ken
Chen, ruiji zhang, Yanghua Xiao
Fudan University, SuperSymmetry Technologies
Abstract
Pre-trained language models (PLMs) have established the
new paradigm in the field of NLP. For more powerful PLMs,
one of the most popular and successful way is to continu-
ously scale up sizes of the models and the pre-training cor-
pora. These large corpora are generally obtained by converg-
ing smaller ones from multiple sources, they are thus grow-
ing increasingly diverse. However, the side-effects of these
colossal converged corpora remain understudied. In this pa-
per, we identify the disadvantage of heterogeneous corpora
from multiple sources for pre-training PLMs. Towards coor-
dinated pre-training on diverse corpora, we further propose
source prompts (SP), which explicitly prompt the model of
the data source at the pre-training and fine-tuning stages. Re-
sults of extensive experiments demonstrate that PLMs pre-
trained with SP on diverse corpora gain significant improve-
ment in various downstream tasks.
Introduction
Recently, pre-trained language models (PLMs) have
markedly enhanced state-of-the-art performance in natural
language processing (NLP). They introduced a new ap-
proach using pre-training followed by fine-tuning. Specif-
ically, these models glean extensive linguistic knowledge
from unsupervised pre-training on vast corpora. To enhance
the capabilities of PLMs, the most effective practice has
been found to involve developing larger models pre-trained
on enormous, varied corpora (Raffel et al. 2019; Brown et al.
2020; Wu et al. 2021).
Corpus expansion is thus crucial for pre-training large
PLMs. This is typically achieved by combining several cor-
pora (Gao et al. 2020; Devlin et al. 2018; Yang, Uy, and
Huang 2020) from various sources, including large Inter-
net corpora collected using common crawlers (Raffel et al.
2019; Xue et al. 2020; Yuan et al. 2021; Wu et al. 2021).
Consequently, many diverse corpora are used in training the
PLM, ensuring adaptability for numerous downstream tasks.
Table 1 presents a selection of popular general or domain-
specific PLMs, each pre-trained from varied corpora.
Increasing corpus size by integrating more heterogeneous
corpora does not always enhance PLMs’ performance. For
some downstream tasks or datasets, pre-training on unre-
lated sub-corpora may be harmful. Table 2 illustrates this. A
T5 model pre-trained on a combined Wikipedia and TorontoBooks Corpus (TBC) (Zhu et al. 2015), totalling 20 GB,
achieves a SuperGLUE score of 73.24. However, the same
model pre-trained on the much larger but more heteroge-
neous 745 GB C4 (Raffel et al. 2019) corpus gets a lower
score of 71.36. The C4 corpus is larger and more varied, but
its quality is worse than the Wikipedia and TBC corpuses.
Therefore, the varied distribution of large corpora challenges
the performance of large PLMs in some benchmarks.
To further enhance the performance of large pre-trained
language models (PLMs), a critical element is effectively
coordinating high data source diversity corpora. Numerous
studies (Wang et al. 2018b; Silva et al. 2018; Aharoni and
Goldberg 2020; Iter and Grangier 2021) emphasize the im-
portance of data diversity in machine learning tasks. These
studies propose extracting training examples that are closely
similar to the downstream task to enhance performance.
However, data resampling for pre-training corpora is im-
practical due to two main reasons: a). Data resampling ren-
ders unselected data unused during pre-training, thereby di-
minishing the utilization of corpora data. b). As PLMs are
designed to handle a wide range of downstream tasks, re-
sampling for specific tasks damages the generality of PLMs.
c). The proportion allocation of pre-training data for large-s-
cale models is key and challenging, usually relying on prior
experience. Data resampling will have an impact on the orig-
inal data distribution.
We introduce the concept of source prompt (SP) to im-
prove the performance of PLMs that are trained from di-
verse, huge, and imbalanced corpora. Instead of using care-
ful resampling strategies or probing good corpus propor-
tions, we propose that corpus source serves as a clear indi-
cator of the heterogeneous data distribution of extensive cor-
pora. The source prompt is used in conjunction with the in-
put sequence to indicate the source of the data, implemented
during both pre-training and downstream tasks. For instance,
when pre-training a model such as BERT on Wikipedia
and BookCorpus concurrently, we can insert the prompts
’[WIKI]’ and ’[BOOK]’ before the input from Wikipedia
and BookCorpus respectively. For a given downstream task
dataset, we assign one of the pre-training corpus sources and
insert the SP corresponding. This source can be either man-
ually or automatically assigned. Our method warrants em-
phasis due to four principal advantages:
1.Generalizability. It can be directly implemented on pre-arXiv:2311.09732v1  [cs.CL]  16 Nov 2023

--- PAGE 2 ---
PLM Corpus Sources
BERT Corpus of BERT BooksCorpus (TBC) and English Wikipedia
GPT2-chinese (Zhao et al. 2019) CLUECorpussSmall News, Social networking sites, Chinese Wikipedia and Reviews
GPT-3 Corpora of GPT-3 Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia
CPM-2 (Zhang et al. 2021) WuDaoCorpora Encyclopedia, Novels, QA, Scientific Literature, E-book, News, and Reviews.
BioBERT (Lee et al. 2020) Biomedical corpora PubMed abstracts and PMC full-text articles
FinBERT (Yang, Uy, and Huang 2020) Financial corpora Corporate Reports, Earnings Call Transcripts, Analyst Reports
BBT-FinT51BBT-FinCorpus Corporate Reports, Analyst Reports, Stock Bar Forum and Financial News
Table 1: Pre-training corpora and corresponding diverse sources of several typical PLMs.
Corpus Size GLUE SuperGLUE
Wiki + TBC 20GB 83.65 73.24
C4 745GB 83.28 71.36
Table 2: Comparison between T5 pre-trained on different
corpus. While C4 significantly outstrip Wiki+TBC in terms
of scale and diversity, T5 pre-trained on C4 performs worse
than the latter.
trained corpora. Its operation is entirely dependant on the
sources of pre-training data, not necessitating any consid-
eration of downstream task features.
2.Data Utilization. It ensures that all corpora can be fully
utilized, eliminating the requirement for tricky data re-
sampling.
3.Simplicity. Our methodology doesn’t need preprocess-
ing and additional modules, conserving significant com-
putational resources.
4.Applicability. Our approach and the pre-trained models
can be effortlessly integrated into existing frameworks,
foregoing any alternations to the model structure and
training procedures.
Our experiments reveal that our approach yields superior
PLMs post pre-training on varied corpora. These models sig-
nificantly outperform on diverse downstream tasks.
Our contributions are summarized as follows:
• We propose the Source Prompt mechanism, designed to
leverage source diversity for enhanced performance of
PLMs in downstream tasks.
• We realize the Source Prompt concept across three
ubiquitous Transformer architectures: Encoder-Only,
Encoder-Decoder, and Decoder-Only.
• We perform experiments on multiple pre-training cor-
pora and various downstream datasets to verify the ef-
fectiveness of SP. The results show that PLMs pre-trained
with SP on different corpora achieve significant improve-
ments in different settings.
Related Work
In this section, we introduce two lines of related work, in-
cluding efforts on data diversity in single-task settings, and
prompt-tuning methods for PLM-based multi-task learning.
Data Diversity in Single-Task Settings
Data Selection Data selection is frequently employed
to address issues arising from data diversity in specifictasks (Silva et al. 2018; Wang et al. 2018b; Aharoni and
Goldberg 2020). This technique involves choosing training
samples based on their resemblance to the validation set or
a reliable in-domain dataset.
For instance, Aharoni and Goldberg (2020) employs
distance-based retrieval using sentence embeddings pro-
duced by PLMs to select in-domain data. van der Wees,
Bisazza, and Monz (2017) presents dynamic data selection
to enhance neural machine translation (NMT) and intro-
duces gradual fine-tuning, which surpasses traditional static
data selection. Wang et al. (2018b) extends techniques to as-
sess and select data for domain NMT and adapts them for
denoising NMT training, using reliable data and an online
data selection-based denoising curriculum.
However, while data selection can address the challenges
of data diversity in specific tasks, it’s not appropriate for SD
during pre-training. This is because it presupposes knowl-
edge of the evaluation set for downstream tasks, yet it’s un-
certain which tasks will be targeted by researchers using the
pre-trained PLMs in the subsequent stages.
Ensemble Learning EnsLM (Duan et al. 2021) proposes
an auto-encoding topic model to cluster data, and adopts en-
semble learning with weight modulation module to fit differ-
ent data clusters. Although it improves cross-domain perfor-
mance, its data clustering step and weight modulation mod-
ule are time-consuming and inconvenient to be adopted by
PLMs.
Domain Diversity One common type of data diversity is
domain diversity. Prior approaches on multi-domain data
generally assume that domain labels are well defined and
assigned to each sample (Wright and Augenstein 2020; Du
et al. 2020; Jiang et al. 2020). To improve the adaptability
of the NMT model to various domains, Jiang et al. (2020)
propose a novel multi-domain NMT model using individ-
ual modules for each domain, on which they apply word-
level, adaptive and layer-wise domain mixing. In order to
solve the problem that BERT cannot adapt to domain fea-
tures for cross domain transfer in cross domain emotion
classification tasks, (Du et al. 2020) design a post-training
procedure, which contains the target domain masked lan-
guage model task and a novel similar domain distinguish-
ing task for pre-training. Although above methods work well
in specific downstream domain tasks, they are not suitable
for pre-training with source diversity for the following rea-
sons. First, they generally only design for the multi-domain
dataset of a certain task or some specific tasks, rather than
the scenario of language model pre-training. Second, these

--- PAGE 3 ---
pre-defined domain labels are not always accurate or even
available (Aharoni and Goldberg 2020), especially for the
wild datasets, in which data come from different sources,
such as internet news, product reviews, and daily conversa-
tions (Duan et al. 2021).
Multi-task Learning with Prompts
Prompts have been widely used to better exploit PLMs in
downstream tasks under various settings. They can be ap-
plied during pre-training, fine-tuning and downstream prob-
ing. Brown et al. (2020) shows that using demonstration
examples or instructions as prompts can make GPT-3 ac-
complish several tasks under few-shot or zero-shot settings.
Sanh et al. (2021) suggests that colossal pre-training corpora
contain various task-related data, and appropriate prompting
helps PLMs recall such data in downstream applications. In
other words, prompts serve as a bridge between pre-training
corpora and downstream tasks.
Therefore, many recent efforts have considered using
prompts for multi-task learning. Prefix-tuning (Li and Liang
2021) freezes PLMs and tunes only task-specific prompts for
downstream tasks. T0 (Sanh et al. 2021) fine-tunes PLMs
on a big prompted dataset covering a wide range of tasks,
attaining strong zero-shot performance on several tasks.
PPT (Gu et al. 2021) generalizes three types of pre-training
tasks to learn prompts on big unlabeled datasets, and trans-
fer these prompts to zero-shot task datasets via initializa-
tion. UL2 (Tay et al. 2022) propose Mixture-of-Denoisers
(MoD), a pre-training objective that combines diverse pre-
training paradigms together, using a specific prefix for dif-
ferent pre-training methods. They further introduce a notion
of mode switching, wherein downstream fine-tuning is asso-
ciated with specific pre-training schemes. UL2 outperform-
ing T5 and GPT-like models across multiple diverse setups.
The task diversity scenario is close to SD in pre-training.
The above methods enable PLMs to remember heteroge-
neous knowledge from various tasks and retrieve related
knowledge in downstream tasks by task-specific prompts.
Inspired by these efforts, we hence prop source prompts to
solve the SD problem in pre-training corpora.
Method
In this section, we describe how source prompts can be ap-
plied to enhance language model pre-training with source di-
versity. First, we describe how the source prompt (SP) is im-
plemented in the pre-training stage. Then, we describe how
the pre-training model with SP is fine-tuned in the down-
stream task stage.
Preliminary
Pretraining Language Models (PLMs) have resulted in a
substantial performance enhancement in Natural Language
Processing (NLP) tasks. This improvement is primarily
achieved through the use of expansive neural networks pre-
trained on an extensive corpus of unlabeled data with a self-
supervised objective. Contemporary PLMs mainly leverage
the transformer architecture (Vaswani et al. 2017). There are
two principal self-supervised objectives: Masked LanguageModel (MLM) and Causal Language Model (CLM). The
MLM approach involves masking certain tokens within a
sentence and predicting them via classification, while CLM
predicts the subsequent token based on the token sequence
of preceding ones.
In our research, we introduce our method to three different
models: encoder-only, encoder-decoder, and decoder-only.
The encoder-only and encoder-decoder models are signifi-
cant components of MLMs, while the decoder-only model
is the core element of CLMs.
Pre-Training with Source Prompt
Towards more powerful PLMs, a common and successful
practice is to build more gigantic models and pre-train them
on more diverse and colossal corpora. A diverse corpus is
mainly obtained by converging small ones from multiple
sources. Therefore, the corpus Ccontains multiple subsets
from different sources:
C=C1∪ C2∪...∪ Cm, (1)
where each Ciis a sub-corpus from a specific source, and m
is the number of subsets.
For example, the corpora of BERT is composed of m= 2
datasets, Wikipedia and BookCorpus:
CBERT =Wikipedia ∪BookCorpus. (2)
A corpus C(or sub-corpus Ci) is a set of npieces of texts:
C={t1, t2, ..., t n}. (3)
We assign each sub-corpus Cia name Niindicating its
source. Then, for every tj∈ Ci, we define Cias the source
corpus (orsource ) oftjandNias its source name . A sim-
ple yet effective way of naming these sources is to use their
abbreviation, namely abbreviation SP , such as ‘[WIKI]’
for Wikipedia and ‘[BOOK]’ for BookCorpus. In fact, our
method is highly robust to different naming methods , and
it is even feasible to use meaningless letters to denote the
sources instead, such as A, B and C.
SP Pre-training on Encoder-Only or Encoder-Decoder
Models We place a source prompt at the start of each
text during pre-training, thereby informing the PLMs of the
text’s origin. For every tjinCi, its SP corresponds to the cor-
pus name Ni. We separate the source prompt and the orig-
inal input using a distinct delimiter. Therefore, the source-
prompted input ˆtjbecomes:
ˆtj= [Ni;−;tj], (4)
where [·;·]denotes string concatenation. For instance, con-
sidering the following text extracted from a news corpus:
“Spokesman for the British Prime Minister Johnson: we
hope to make significant amendments to the Northern Ire-
land agreement. We believe it is feasible within the agree-
ment’s framework. ”
We prefix this text with its source name, News :
“News−Spokesman for the British Prime Minister John-
son: we hope to make significant amendments to the North-
ern Ireland agreement. We believe it is feasible within the
agreement’s framework ”

--- PAGE 4 ---
<SP> Bank Indonesia expects GDP will be less than 4.9%News Books Wiki Paper Review
Source Original Text
Pre-training
Fine-tuning  & InferenceEncoder -Only & Encoder -Decoder 
Models
Decoder -Only Models
ManualSP<s>[MASK] Bank Indonesia expects [MASK] will be less than 4.9%
<s><News> Bank Indonesia expects GDP will be less than 4.9% <News><News> GDP
<News> Bank Indonesia expects GDP will be less than 4.9% <News> </s>Input:
Output:
Input:
Output:
AutoSP
Bank Indonesia expects GDP will be less than 4.9% [MASK]
Bank Indonesia expects GDP will be less than 4.9% <News>
Output<News>
<s>Bank Indonesia expects GDP will be less than 4.9%
<News>ORMask word prediction with the 
SP enhanced PLM
Next word prediction with the 
SP enhanced PLM Any down -stream task with 
the same SP enhanced PLM Figure 1: For encoder-only or encoder-decoder models, we employ Masked Source Prediction task, training it with Masked
Language Modeling. For decoder-only models, we utilize Next Token Prediction in pre-training. During fine-tuning and in-
ference, SP can be manually added for data from known sources (manual SP), or automatically appended MSP’s or NTP’s
prediction of the closest source (auto SP) for data from unknown sources. we can also fine-tune and inference without any SP.
Subsequently, this source-prompted text is subjected to
the pre-training phase of the PLMs. Therefore, the PLMs
can predict masked words using the SP’s assistance, en-
abling them to learn language styles dependent on varying
sources. Simultaneously, the tokens’ representations in the
SP are optimised, leading to an incremented source-specific
knowledge for both pre-training and downstream applica-
tions. Furthermore, we introduce the masked source predic-
tion (MSP), an innovative pre-training objective for multi-
source pre-training. In this process, the source prompts are
randomly masked with a certain probability, requiring the
PLMs to predict the masked source based on the contexts.
This task compels PLMs to learn-source related characteris-
tics. It can be easily amalgamated with the MLM objective
and incurs minimal additional costs.
SP Pre-training on Decoder-Only Models The decoder-
only model, lacking a bidirectional attention mechanism,
cannot apply MLM. We propose two types of SPs for such
models:
1.SPPositioning the SP at the start of the text allows the
SP to assist in predicting the text’s content.
2.Post SP Placing the SP at the text’s end enables source
prediction based on the text’s content.
We use the next-token-prediction method for training these
models.
It’s necessary to emphasize that during the pre-training
phase of any architectures, SP is incorporated at random,and a special token is interspersed between the SP and the
original text.
Fine-tuning with Source Prompts
Applying PLMs with SP to a specific downstream dataset
involves assigning the most relevant pre-training source
prompt to the dataset. Certain datasets derive from specific
sources or domains closely related to one of the pre-training
sources, for example, a news classification dataset. There-
fore, a manual selection of an SP for such datasets is fea-
sible. Conversely, other datasets might have less relevance
to pre-training sources or may include samples from various
sources. In these cases, we suggest two methodologies for
assigning source prompts to downstream datasets:
Manual SP Often, we compile downstream datasets from
a single source, detailed in its description. Given this sce-
nario, we can manually assign it an appropriate pre-training
source, if available. For instance, when pre-training PLMs
on Wikipedia and BookCorpus, similar to BERT, and fine-
tuning models on the QNLI (Wang et al. 2018a) dataset (a
dataset precisely derived from Wikipedia), we can directly
employ the SP [WIKI] for it. Following SP selection, we
insert the SP before the texts of the downstream datasets,
aligning with the pre-training process.
Auto SP Alternatively, downstream dataset sources could
be unidentified, mixed, or significantly divergent from the
pre-training sources. For these situations, we suggest the
Auto SP methodology, as demonstrated in Figure 1. The

--- PAGE 5 ---
Auto SP method leverages our MSP objective during pre-
training, enabling MLMs to self-predict the most suitable
sources. Specifically, we initially input the concatenation
[[MASK] ;−;t]of the source mask and the original input t.
We then ask PLMs to predict each sample’s text source, in-
cluding both training and testing data. Or predict the source
with Next Token Prediction for CLMs. Finally, we replace
the masked source (MASK) with the predicted source, using
these substituted samples for fine-tuning and prediction.
No SP Additionally, it is possible to use the pre-trained
model conventionally without any SP. Firstly, the applica-
tion of SP during pre-training enables the model to compre-
hend the source corpus effectively. Secondly, the optional
random addition of SP in the settings imparts a degree of
generalizability to the model, irrespective of the presence of
a SP. Consequently, even in the absence of SP implemen-
tation in downstream tasks, comparable performance levels
can be achieved as with SP.
Experiments
In this section, we evaluate the effectiveness of our source
prompt method under different settings. In general, PLMs
pre-trained with our method on diverse corpora achieve sig-
nificant improvements.
We begin by outlining the consistent settings employed
across all experiments. Subsequently, we undertake the fol-
lowing experimental tasks: a). Verification of the effective-
ness of our proposed method. b). Comparison of the impacts
of diverse naming policies for pre-training corpus sources,
demonstrating the robustness of SP to these varying policies.
c). Analysis of different masking probabilities for SP, attest-
ing to the effectiveness of MSP. d). Evaluation of the effects
of various SP assignment methods for downstream datasets,
revealing that auto SP is the most efficacious approach.
e). Confirmation of the generalizability of our methodol-
ogy across different model architectures, pre-training cor-
pora and downstream benchmarks. f). Investigation of the
effects of SP methods on domain-specific or general corpora
for decoder-only architecture large language models.
General Settings
Corpus We consider three diverse corpora of multi-
ple sources for pre-training, including BBT-FinCorpus2,
CLUECorpusSmall (Xu, Zhang, and Dong 2020) and Wu-
DaoCorpora (Yuan et al. 2021), which represent typical
domain-specific corpora or general corpora, respectively.
Detailed information of curpua is shown in Appendix.
Benchmark We use BBT-FinCUGE and CLUE (Xu et al.
2020) as our evaluation benchmarks. BBT-FinCUGE is a
Chinese financial evaluation benchmark consisting of five
understanding tasks and three generation tasks. The un-
derstanding tasks include event subject extraction, emo-
tion recognition, news classification, negative news and sub-
ject recognition, and relationship extraction. The genera-
tion tasks include causal QA, event QA and news sum-
mary. CLUE is a general Chinese NLP evaluation bench-
2https://github.com/ssymmetry/BBT-FinCUGE-Applicationmark consisting of nine comprehension tasks, including se-
mantic similarity, text classification, reading comprehension
and other tasks. For all evaluation benchmarks, we fine-tune
PLMs for evaluation, and take the average score on the test
set as the main comparison basis.
Implementation We delineate the specifics of our pre-
training and fine-tuning stages in this section.
Our selected foundational architectures encompass two
conventional Masked Language Models (MLMs), BERT
and T5, as well as one of the most recent Causal Language
Models (CLMs), OpenLLaMA-3b. BERT exemplifies the
Encoder-Only Model, T5 embodies the Encoder-Decoder
Model, whereas OpenLLaMA-3b signifies the Decoder-
Only Model. Our implementation of these models is heavily
reliant upon Hugging Face Transformers. The configuration
parameters for BERT-base and T5-base align with their orig-
inal implementations. For OpenLLaMA-3b, we incorporate
an expanded vocabulary to encompass Chinese tokens, as
suggested by previous research.
We execute pre-training for the three models on ei-
ther domain-specific corpora or general corpora, and sub-
sequently assess their performance on benchmarks such as
BBT-FinCUGE or CLUE. A comprehensive depiction of
the implementation is provided in the Appendix. To ensure
the reliability of our findings, all experiments are conducted
thrice and the average results are reported.
Overall Effectiveness of SP
To verify the basic effectiveness of SP method under the
pre-training and fine-tuning framework, we pre-train T5
on BBT-FinCorpus and fine-tune it on the BBT-FinCUGE
benchmark. We set up four experimental groups: group A,
which does not use SP in both stages, group B, which uses
SP only in the pre-training stage, and groups C and D, which
use SP in both stages and adopt manual SP and auto SP re-
spectively in the fine-tuning stage.
Table 3 shows the results, from which we make the fol-
lowing observations: (1) With source prompts, PLMs pre-
trained on diverse corpora achieve significantly better per-
formance on nearly all datasets. Their average scores (74.67-
75.88) have significant advantages over group A (71.76),
which fully proves the effectiveness of the SP method, es-
pecially with nearly no additional computing cost. (2) Intro-
ducing SP in the fine-tuning phase further improves model
performance, as shown by comparing group B with groups
C, D, and E.
Robustness of Source Naming Policies
As mentioned in Method Section, we do not have a deter-
ministic method to name the source of each corpus. A rel-
atively simple way is to use the manual abbreviation of the
corpus source. However, we show with experiments that our
method is robust to different source naming policies, namely
the specific tokens used to represent the sources. That is to
say, the effectiveness of SP originates from identification of
sources, instead of textual information in their names.
Specifically, we design two additional experiments for
comparison. The first is meaningless alphabet SP, that is, we

--- PAGE 6 ---
Setting Pre-train Fine-tune FinCQA FinESE FinFE FinNA FinNL FinNSP FinQA FinRE Avg.
A w/o SP w/o SP 67.81 78.84 79.85 42.37 87.28 89.13 74.75 54.08 71.76
B Abbreviation SP w/o SP 77.14 78.89 78.26 46.15 87.75 90.56 81.90 56.68 74.67
C Abbreviation SP Manual SP 77.75 79.25 78.96 46.47 87.82 90.56 81.76 57.19 74.97
D Abbreviation SP Auto SP 76.99 78.89 79.75 56.64 87.93 90.56 81.04 56.12 75.99
E Abbreviation SP Random SP 77.31 79.84 79.35 50.63 87.76 93.86 83.32 54.99 75.88
Table 3: Results of the T5 model pre-trained with BBT-FinCorpus and fine-tuned with BBT-FinCUGE. In general, T5 with SP
performs significantly better than T5 without SP. As shown in rows C, D, and E, auto SP outperforms the other assignment
method for downstream datasets.
Setting SP mask prob. FinCQA FinESE FinFE FinNA FinNL FinNSP FinQA FinRE Avg.
A 0 76.10 78.90 78.56 45.63 87.66 89.80 80.54 55.23 74.05
B 0.15 76.99 78.89 79.75 56.64 87.93 90.56 81.04 56.12 75.99
C 0.3 77.18 79.20 79.75 56.64 88.05 90.98 81.42 56.35 76.19
Table 4: Experiments with pre-trained T5 with or without masked source prediction (MSP) objective. The results show the
effectiveness of MSP.
Setting Pre-train Fine-tune FinESE FinFE FinNL FinNSP FinRE Avg.
A w/o SP w/o SP 38.44 77.22 83.77 68.19 44.44 62.42
B Abbreviation SP w/o SP 37.37 77.17 83.06 68.20 45.71 62.30
C Abbreviation SP Manual SP 58.45 77.82 83.76 67.62 49.14 67.36
D Abbreviation SP Auto SP 57.34 77.77 82.72 69.82 50.50 67.53
E Alphabet SP Manual SP 56.21 77.47 83.60 68.19 45.70 66.23
F Alphabet SP Auto SP 59.37 77.07 83.22 70.18 45.15 67.00
G Misplaced SP Manual SP 57.49 77.57 84.24 69.84 49.02 67.63
H Misplaced SP Auto SP 57.77 77.73 84.10 69.65 47.77 67.32
Table 5: Experiments of BERT in financial domain. It is pre-trained on BBT-FinCorpus and fine-tuned on BBT-FinCUGE. As
shown in row C to H, SP is robust to different source naming policies in the pre-training stage.
replace the specific names of the corpus sources with mean-
ingless letters like A, B, C, etc. Thus, the model can only
obtain the source identification of corpora, without specific
textual information of each source. The second is misplaced
SP, that is, we deliberately confuse the names of the corpus
sources (for example, set the SP of all news corpora as “com-
ments”). In all settings, we control the number of prompt
tokens of different sources to be equal.
Table 5 shows the results on BERT. The results demon-
strate that using alphabet SP and misplaced SP hardly de-
crease the performance, compared with abbreviation SP,
which is still far beyond the baseline without SP. These re-
sults suggest that SP is robust to the different name policies
of corpus. Hence, SP is effective because of identification
of sources, instead of textual information in their names.
Therefore, in real applications, the sources can be named at
will, which exert little influence on the performance.
Effectiveness of Mask Source Prediction
We study the effectiveness of the mask source prediction
(MSP) objective by pre-training models with varying mask
probability of SP and comparing their performance on the
benchmark.
We consider three values {0, 0.15, 0.3 }of the mask prob-
ability P. Setting P= 0 means that the models are pre-
trained without the MSP objective. We use abbreviation SPfor pre-training and manual SP for fine-tuning. As shown in
the table 4, the performance with P= 0 (without MSP)
is significantly under that of others, and the performance
withP= 0.3is slightly better than that with P= 0.15.
These validate the effectiveness of our MSP objective, and
suggest that higher SP masking probability encourage the
model to better distinguish texts from different sources and
learn source related features.
SP Assignment for Downstream Tasks
As described, we propose two methods to assign SP in
downstream datasets: manual SP and auto SP. This exper-
iment compares the effects of different SP assignment meth-
ods. We also set up a control group with a randomly sampled
SP from the pre-training sources for each sample, called ran-
dom SP. Table 3 shows that auto SP outperforms manual
SP (rows C and D). We attribute this to the fact that auto
SP leverages the source related information learned by the
model, and adaptively applies the most suitable SP for each
sample, while manual SP is only assigned at the dataset-
level. This conclusion is further validated by rows C and D
in Table 5 and Table 6.
Generalizability of SP
In order to verify the generalization of SP method, we
mainly conduct experiments from two dimensions, includ-

--- PAGE 7 ---
Setting Pre-Train Fine-Tune AFQMC CSL IFLYTEK OMNLI TNEWS WSC Avg.
A w/o SP w/o SP 69.45 77.53 57.57 75.88 55.31 63.71 66.57
B Abbreviation SP w/o SP 69.11 77.36 58.06 75.64 55.20 63.81 66.53
C Abbreviation SP Auto SP 70.92 78.13 57.82 75.59 55.44 63.81 66.95
D Abbreviation SP Manual SP 70.92 79.36 58.06 76.32 55.51 63.81 67.33
Table 6: Experiments of BERT in the general domain. It is pre-trained on the CLUECorpusSmall and fine-tuned on the CLUE
benchmark. The results prove the effectiveness of SP in the general domain, and show the generalizability of SP.
Setting Pre-train Fine-tune FinCQA FinESE FinFE FinNA FinNL FinNSP FinQA FinRE Avg.
A w/o SP w/o SP 75.4 84.7 79.8 48.7 88.0 94.8 79.9 55.6 75.8
B Abbreviation SP w/o SP 75.7 84.3 80.1 49.7 87.8 96.0 80.6 56.6 76.4
C Alphabet SP w/o SP 75.6 86.1 78.4 49.5 87.7 95.4 81.5 55.3 76.2
D Post Abbreviation SP w/o SP 77.3 85.2 80.0 49.9 88.3 95.6 81.0 55.5 76.6
Table 7: Results of the OpenLLaMA-3b model pre-trained with BBT-FinCorpus and fine-tuned with BBT-FinCUGE. All pre-
trained models are fine-tuned without SP. As shown in rows B, C, and D, all SP implenmentation can improve model generally,
and Post Abbreviation SP achieves the best.
Setting Pre-Train Fine-Tune AFQMC CSL IFLYTEK OCNLI TNEWS WSC Avg.
A w/o SP w/o SP 71.95 84.93 60.38 77.67 47.59 68.57 68.57
B Abbreviation SP w/o SP 72.86 85.57 62.77 77.37 59.68 64.8 70.5
C Post Abbreviation SP w/o SP 73.22 85.67 61.19 77.07 60.38 62.41 69.9
Table 8: Experiments of OpenLLaMA-3b in the general domain. It is pre-trained on the CLUECorpusSmall and Wudao Corpus,
and fine-tuned on the CLUE benchmark without SP. The results prove the effectiveness of SP in the general domain with scaled
model and training tokens.
ing different model architectures and different corpus do-
mains. Specifically, in order to verify the generalization of
SP to different model architectures, we applied SP methods
to BERT and T5 models respectively. In order to verify gen-
eralization of SP to different domains, we conducted exper-
iments in both the financial domain and the general domain.
The corpus and benchmark used are BBT-FinCorpus and
BBT-FinCUGE in the financial domain, and are CLUECor-
pusSmall and CLUE in the general domain.
Table 3 and Table 5 show that our models achieve signifi-
cant improvement under both model architectures. Besides,
it is shown in Table 5 and Table 6 that our model has the
expected effect in both domains. Therefore, it can be con-
cluded that our method is general enough to be widely ap-
plied to various model architectures and corpus domains.
SP on Decoder-Only LLMs
We conducted systematic experiments on the OpenLLaMA-
3b model to examine the influence of SP on decoder-only
LLM. In order to explore the effects of the two SP methods,
we conduct experiments on two SP separately. Table 7 and
Table 8 depict the experimental outcomes of OpenLLaMA-
3b in the financial and general domains, respectively. All
SP settings achieved scores beyond the preset baseline in
both experiments, implying the enhancement in model per-
formance during the pre-training process due to the incor-
poration of SP. Moreover, all downstream fine-tuning pro-
cedures were executed without SP, thus highlighting the ro-
bustness of the SP.Conclusion
In this paper, we first identify the side-effects of increased
corpus diversity for pre-training PLMs. To overcome this
problem, we propose source prompt (SP), an easy, efficient
and effective approach to promote coordinated pre-training
on such corpora, which is a prompt added before inputs of
PLMs to identify their source. Furthermore, we thoroughly
study different naming polices of pre-training SP and differ-
ent strategy to assign SP to downstream application, as well
as proposing a novel pre-training objective named masked
source prediction. Results of extensive experiments validate
the effectiveness, robustness and generalizability of SP, as
well as the benefits of MSP.
Limitations
First of all, ”source” is a relatively abstract concept. For
common crawl based corpora such as C4, their source in-
formation is largely unusable because their data is crawled
from millions of web pages. Therefore, our current method
is limited to the scenario where a certain number of small
corpora are merged together to form a large corpus. Second,
due to the scale of computing power we can obtain, the pa-
rameter scale and the amount of training tokens of PLMs we
use are quite limited. It remains to be studied whether our
method works for the large-scale PLMs. Last but not least,
it remains to be explored in-depth what the effectiveness of
introducing SP originates from. In the future, we will con-
tinue to study the effect of SP on large-scale PLMs, and in-
vestigate the effectiveness of SP in other NLP tasks such as
cross-domain sentiment analysis.

--- PAGE 8 ---
References
Aharoni, R.; and Goldberg, Y . 2020. Unsupervised Domain
Clusters in Pretrained Language Models. In Proceedings
of the 58th Annual Meeting of the Association for Com-
putational Linguistics , 7747–7763. Online: Association for
Computational Linguistics.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems , 33: 1877–
1901.
Dao, T. 2023. FlashAttention-2: Faster Attention with Better
Parallelism and Work Partitioning.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Du, C.; Sun, H.; Wang, J.; Qi, Q.; and Liao, J. 2020. Ad-
versarial and Domain-Aware BERT for Cross-Domain Sen-
timent Analysis. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics , 4019–
4028. Online: Association for Computational Linguistics.
Duan, Z.; Zhang, H.; Wang, C.; Wang, Z.; Chen, B.; and
Zhou, M. 2021. EnsLM: Ensemble Language Model for
Data Diversity by Semantic Clustering. In Proceedings of
the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1: Long Pa-
pers) , 2954–2967. Online: Association for Computational
Linguistics.
Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.;
Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; et al.
2020. The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Gu, Y .; Han, X.; Liu, Z.; and Huang, M. 2021. PPT:
Pre-trained Prompt Tuning for Few-shot Learning. ArXiv ,
abs/2109.04332.
Iter, D.; and Grangier, D. 2021. The Trade-offs of Domain
Adaptation for Neural Language Models. arXiv preprint
arXiv:2109.10274 .
Jiang, H.; Liang, C.; Wang, C.; and Zhao, T. 2020. Multi-
Domain Neural Machine Translation with Word-Level
Adaptive Layer-wise Domain Mixing. In Proceedings of the
58th Annual Meeting of the Association for Computational
Linguistics , 1823–1834. Online: Association for Computa-
tional Linguistics.
Kalamkar, D.; Mudigere, D.; Mellempudi, N.; Das, D.;
Banerjee, K.; Avancha, S.; V ooturi, D. T.; Jammalamadaka,
N.; Huang, J.; Yuen, H.; et al. 2019. A study of BFLOAT16
for deep learning training. arXiv preprint arXiv:1905.12322 .
Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and
Kang, J. 2020. BioBERT: a pre-trained biomedical language
representation model for biomedical text mining. Bioinfor-
matics , 36(4): 1234–1240.
Li, X. L.; and Liang, P. 2021. Prefix-tuning: Optimiz-
ing continuous prompts for generation. arXiv preprint
arXiv:2101.00190 .Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer. arXiv e-prints .
Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y . 2020.
Deepspeed: System optimizations enable training deep
learning models with over 100 billion parameters. In Pro-
ceedings of the 26th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining , 3505–3506.
Sanh, V .; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.;
Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.;
et al. 2021. Multitask prompted training enables zero-shot
task generalization. arXiv preprint arXiv:2110.08207 .
Silva, C. C.; Liu, C.-H.; Poncelas, A.; and Way, A. 2018.
Extracting In-domain Training Corpora for Neural Machine
Translation Using Data Selection Methods. In Proceedings
of the Third Conference on Machine Translation: Research
Papers , 224–231. Brussels, Belgium: Association for Com-
putational Linguistics.
Tay, Y .; Dehghani, M.; Tran, V . Q.; Garcia, X.; Bahri, D.;
Schuster, T.; Zheng, H. S.; Houlsby, N.; and Metzler, D.
2022. Unifying Language Learning Paradigms. arXiv
preprint arXiv:2205.05131 .
van der Wees, M.; Bisazza, A.; and Monz, C. 2017. Dy-
namic Data Selection for Neural Machine Translation. In
Proceedings of the 2017 Conference on Empirical Methods
in Natural Language Processing , 1400–1410. Copenhagen,
Denmark: Association for Computational Linguistics.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2018a. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 .
Wang, W.; Watanabe, T.; Hughes, M.; Nakagawa, T.; and
Chelba, C. 2018b. Denoising Neural Machine Translation
Training with Trusted Data and Online Data Selection. In
Proceedings of the Third Conference on Machine Transla-
tion: Research Papers , 133–143. Brussels, Belgium: Asso-
ciation for Computational Linguistics.
Wright, D.; and Augenstein, I. 2020. Transformer Based
Multi-Source Domain Adaptation. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , 7963–7974. Online: Associa-
tion for Computational Linguistics.
Wu, S.; Zhao, X.; Yu, T.; Zhang, R.; Shen, C.; Liu, H.; Li,
F.; Zhu, H.; Luo, J.; Xu, L.; et al. 2021. Yuan 1.0: Large-
scale pre-trained language model in zero-shot and few-shot
learning. arXiv preprint arXiv:2110.04725 .
Xu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;
Sun, K.; Yu, D.; Yu, C.; et al. 2020. CLUE: A Chinese lan-
guage understanding evaluation benchmark. arXiv preprint
arXiv:2004.05986 .

--- PAGE 9 ---
Xu, L.; Zhang, X.; and Dong, Q. 2020. CLUECorpus2020:
A Large-scale Chinese Corpus for Pre-training Language
Model. ArXiv , abs/2003.01355.
Xue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.;
Siddhant, A.; Barua, A.; and Raffel, C. 2020. mT5: A
massively multilingual pre-trained text-to-text transformer.
arXiv preprint arXiv:2010.11934 .
Yang, Y .; Uy, M. C. S.; and Huang, A. 2020. Finbert: A pre-
trained language model for financial communications. arXiv
preprint arXiv:2006.08097 .
Yuan, S.; Zhao, H.; Du, Z.; Ding, M.; Liu, X.; Cen, Y .; Zou,
X.; Yang, Z.; and Tang, J. 2021. Wudaocorpora: A super
large-scale chinese corpora for pre-training language mod-
els.AI Open , 2: 65–68.
Zhang, Z.; Gu, Y .; Han, X.; Chen, S.; Xiao, C.; Sun, Z.;
Yao, Y .; Qi, F.; Guan, J.; Ke, P.; et al. 2021. Cpm-2: Large-
scale cost-effective pre-trained language models. AI Open ,
2: 216–224.
Zhao, Z.; Chen, H.; Zhang, J.; Zhao, X.; Liu, T.; Lu, W.;
Chen, X.; Deng, H.; Ju, Q.; and Du, X. 2019. UER: An
Open-Source Toolkit for Pre-training Models. EMNLP-
IJCNLP 2019 , 241.
Zhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Aligning books and
movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE in-
ternational conference on computer vision , 19–27.
Appendix
Corpora Details
BBT-FinCorpus represents a substantial Chinese financial
corpus comprising approximately 200GB of text files. This
corpus incorporates various sources such as company an-
nouncements, research reports from securities companies
and investment banks, discussion forums focused on stock
bars and applicable forums such as the Snowball forum, as
well as multiple financial news fetched from several web-
sites. As demonstrated in Table 9, these five distinct sources
propose a challenge for models attempting to learn due to
the differentiation in their styles.
CLUECorpusSmall signifies a general Chinese corpus
possessing about 14GB of text files. The corpus is composed
of four diverse sources, each demonstrating a significant dif-
ference in their styles.
WudaoCorpora is a representation of a general Chinese
corpus that contains approximately 220GB of text files. The
corpus is made up of 25 data types where each type indicates
a source from which the data has been selected.
A comprehensive list of the data sources for these three
corpora is exhibited in Table 10.
Implementation Details
DeepSpeed (Rasley et al. 2020) is employed to ex-
pedite the training process. Notably, we adopt the
BFLOAT16 (Kalamkar et al. 2019) semi-precision format
and gradient partitioning, as implemented in DeepSpeed.For OpenLLaMA-3b’s pre-training, to accelerate the com-
putational process, we utilize flash-attention (Dao 2023).
BERT’s pre-training was executed using a batch size of
128, sequence length of 512, a learning rate of 5e-5, and a to-
tal of 100,000 steps. This took approximately 24 hours using
8 NVIDIA A100 GPUs. For evaluation, BBT-FinCUGE’s
three generation tasks were omitted and a simple fully-
connected layer served as the output head for each task,
based on BERT’s last hidden state.
T5’s training strategy on BBT-FinCorpus adhered to
UER (Zhao et al. 2019)’s two-stage setting. In the initial
stage, the model was trained using a sequence length of 128,
a batch size of 512, and a total of 1,000,000 steps. During
the second stage, with a sequence length of 512, a learning
rate of 1e-4, a batch size of 128, and 250,000 total steps,
the model was trained within 60 hours using 8 NVIDIA
A100 GPUs. For general domain, due to the scale limita-
tion of CLUECorpusSmall, the training strategy is to train
100,000 total steps on it under the same settings. Evaluations
modeled all tasks as text-to-text. Unnecessary variables were
avoided by not using task prompts; the model was trained to
output the relationship between head entities and tail enti-
ties in text, based on input sentences, head entities, and tail
entities.
OpenLLaMA-3b was trained using batch sizes of 512, a
sequence length of 1024, learning rates of 5e-5, and 20,000
total steps, taking around 70 hours with 8 NVIDIA A800
GPUs for both general domain and finicial domain. Owing
to the scale limitation of CLUECorpusSmall, WudaoCor-
pora was included in the general domain pre-training. The
exact numbers of training tokens are presented in table 11.
For evaluation, input and target were concatenated to model
tasks as text-to-text. During loss computation, only the MSE
loss on the target token positions was considered.
For Fine-tuning, we train with batch size of 64 and learn-
ing rate of 5e-5 on the T5 and Bert models. While on
OpenLLaMA-3b, we train it with batch size of 192 and
learning rate of 1e-5. We use the same parameters for Fine-
tuning on BBTFinCUGE and CLUE3. To ensure sufficient
fine-tuning, we test on the testset with the model that scores
the highest on the validation set after 8 training rounds on
the downstream tasks.
3https://github.com/CLUEbenchmark/CLUE

--- PAGE 10 ---
Corpus Example Description
Company Announcement The 2021 annual equity distribution plan of Changying Technology
Co., Ltd.Announcements of listed companies, formal
Research Report After the epidemic, macro-economy will recover after the trough... Research reports about companies, industry and economy
Guba BBS I hope medical stocks will improve tomorrow, buy in! Discussion of shareholders, colloquial and emotional
Snowball BBS I think Bitcoin will rise due to the release of the US dollar. Discussion of shareholders, colloquial and rational
Financial News Musk responded that Tesla stopped receiving orders for ... Financial news, diverse and relatively formal
Table 9: Sources, examples and descriptions of various sub-corpora in the BBT-FinCorpus.
Corpus Sources
WudaoCorporaDouban Topic
Blog
Nurturing Common Sense
Medical Question and Answering
Science and Technology
Introduction to Xiaohongshu
Agriculture
Encyclopedia
Entertainment
Information
Economy
Baijiahao Article
Culture
News
Sociaty
Experience
Travel
Real Estate
Education
International
Games
Sports
Cars
Law
Popular Science Articles
CLUECorpusSmallcomments
news
webtext
wikizh
BBT-FinCorpusCompany Announcement
Research Reports
Guba BBS
Snowball BBS
Financial News
Table 10: The detailed list of data source from WudaoCor-
pora, CLUECorpusSmall and BBT-FinCorpus
Corpus Tokens
WudaoCorpora 6 Billion
CLUECorpusSmall 4 Billion
Table 11: The number of training tokens from WudaoCor-
pora and CLUECorpusSmall. The number of tokens from
full CLUECorpusSmall is 4 billion.
