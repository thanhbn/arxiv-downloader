# 2309.13192.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/backpropagation/2309.13192.pdf
# Kích thước file: 798167 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
HƯỚNG TỚI AI XANH TRONG FINE-TUNING CÁC MÔ HÌNH NGÔN NGỮ LỚN THÔNG QUA ADAPTIVE BACKPROPAGATION
Kai Huang†, Hanyun Yin§, Heng Huang‡& Wei Gao†
University of Pittsburgh†, University of Maryland, College Park‡
University of Science and Technology of China§
k.huang@pitt.edu ,ykissgoodbye@gmail.com ,heng@umd.edu ,weigao@pitt.edu
TÓM TẮT
Fine-tuning là thiết yếu để điều chỉnh các mô hình ngôn ngữ lớn đã được pre-train cho các ứng dụng downstream. Với việc tăng độ phổ biến của các ứng dụng được hỗ trợ bởi LLM, fine-tuning đã được thực hiện một cách chuyên sâu trên toàn thế giới, tạo ra một lượng lớn chi phí tính toán tương ứng với carbon footprint lớn và tác động môi trường. Việc giảm thiểu tác động môi trường như vậy có mối tương quan trực tiếp với việc giảm FLOPs của fine-tuning. Các phương án fine-tuning hiện tại tập trung vào việc tiết kiệm bộ nhớ hoặc giảm overhead của việc tính toán cập nhật trọng số, nhưng không thể đạt được sự giảm FLOPs đủ lớn do bỏ qua chi phí training trong backpropagation. Để giải quyết hạn chế này, trong bài báo này chúng tôi trình bày GreenTrainer, một kỹ thuật mới tối thiểu hóa FLOPs của fine-tuning LLM thông qua adaptive backpropagation, lựa chọn tập hợp phù hợp nhất các tensor LLM cho fine-tuning dựa trên tầm quan trọng và chi phí backpropagation của chúng trong training. Kết quả thí nghiệm cho thấy GreenTrainer có thể tiết kiệm tới 64% training FLOPs so với full fine-tuning, mà không có bất kỳ mất mát độ chính xác đáng chú ý nào. So với các phương án hiện có như Prefix Tuning và LoRA, GreenTrainer có thể đạt được cải thiện độ chính xác mô hình lên tới 4%, với sự giảm FLOPs tương đương.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLMs) được sử dụng như các công cụ nền tảng trong AI tạo sinh. Để được sử dụng trong các ứng dụng downstream, một LLM đã được pre-train cần được fine-tune sử dụng dữ liệu ứng dụng cụ thể (Devlin et al., 2018). Một cách trực quan, fine-tuning ít tốn kém về mặt tính toán hơn pre-training do lượng dữ liệu training nhỏ hơn, nhưng nó có thể dẫn đến tiêu thụ năng lượng và carbon footprint cao đáng kể khi được thực hiện chuyên sâu trên toàn thế giới. Được hỗ trợ bởi việc dân chủ hóa các LLM mã nguồn mở (Candel et al., 2023) và các API tiện lợi để vận hành các LLM này (Ott et al., 2019; Wolf et al., 2019), ngay cả những cá nhân không chuyên môn cũng có thể dễ dàng fine-tune LLM để cải thiện hiệu suất mô hình hoặc cá nhân hóa (Scialom et al., 2022; Wang and Gao, 2023). Ví dụ, khi một mô hình LLaMA-13B (Touvron et al., 2023) được fine-tune bởi 10k người dùng sử dụng GPU A100-80GB, việc fine-tuning như vậy tiêu thụ nhiều hơn 6.9× giờ GPU so với pre-training một mô hình GPT-3 (Brown et al., 2020) với 175B tham số. Lượng năng lượng tiêu thụ bởi việc fine-tuning như vậy có thể so sánh với lượng năng lượng tiêu thụ bởi một số quốc gia kém phát triển, và lượng carbon footprint tương đương với 1000× lượng carbon được tạo ra bởi một chuyến bay New York-San Francisco (aii, 2023).

Việc giảm thiểu tác động môi trường như vậy hướng tới Green AI có mối tương quan trực tiếp với việc giảm số lượng phép toán dấu phẩy động (FLOPs) của fine-tuning, đại diện cho lượng phép toán tính toán và do đó tiêu thụ năng lượng trong training (Schwartz et al., 2020; Huang et al., 2023a). Tuy nhiên, hầu hết các kỹ thuật hiện có để tối ưu hóa fine-tuning LLM bị giới hạn trong việc giảm tiêu thụ bộ nhớ thay vì FLOPs (Malladi et al., 2023; Liao et al., 2023). Một số phương pháp khác giảm FLOPs bằng cách chỉ fine-tune một số loại tham số mô hình nhất định như bias (Zaken et al., 2021), LayerNorm và trọng số lớp output (Lu et al., 2021), nhưng chúng làm suy giảm khả năng biểu đạt của mô hình và chỉ áp dụng được cho các tác vụ học tập đơn giản không tạo sinh. Thay vào đó, các nhà nghiên cứu đề xuất giữ nguyên các tham số mô hình gốc bị đóng băng nhưng tiêm thêm các tham số trainable vào input (Lester et al., 2021; Liu et al., 2022) hoặc các lớp internal (Li and Liang, 2021; Hu et al., 2023; Huang et al., 2023b). Các phương pháp dựa trên LoRA gần đây (Hu et al., 2021; Zhang et al., 2023) tiếp tục giảm overhead của việc tính toán cập nhật trọng số cho các tham số được tiêm này thông qua xấp xỉ low-rank. Các phương pháp này có thể tối thiểu hóa mất mát độ chính xác của mô hình trên các tác vụ tạo sinh. Tuy nhiên, chúng vẫn cần tính toán các gradient activation qua toàn bộ mô hình và do đó việc giảm FLOPs của chúng bị hạn chế, bởi vì các phép tính cập nhật trọng số chỉ chiếm 25%-33% tổng training FLOPs.

1arXiv:2309.13192v2  [cs.LG]  29 Feb 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bên cạnh việc tính toán cập nhật trọng số, FLOPs trong training cũng được tạo ra trong i) forward propagation và ii) backward propagation của activation gradients. Vì forward propagation hoàn chỉnh là thiết yếu để tính toán training loss, chúng tôi hình dung rằng chìa khóa để giảm FLOPs hơn nữa là tính đến chi phí backpropagation của activation gradients, chiếm >33% tổng training FLOPs, và chọn lọc chỉ liên quan đến các cấu trúc mô hình phù hợp nhất trong backpropagation. Tuy nhiên, thách thức chính là việc training chọn lọc có thể mang lại mất mát độ chính xác mô hình. Chúng tôi tối thiểu hóa mất mát độ chính xác bằng cách điều chỉnh việc lựa chọn như vậy trong backpropagation theo một mục tiêu linh hoạt về giảm FLOPs, được xác định bởi carbon footprint trong cung cấp năng lượng. Ví dụ, khi carbon footprint như vậy thấp do việc chèn năng lượng tái tạo, việc sử dụng mục tiêu giảm FLOPs thấp hơn có thể liên quan đến nhiều cấu trúc mô hình hơn trong training và giữ lại độ chính xác training. Carbon footprint cao, thay vào đó, dẫn đến mục tiêu giảm FLOPs cao hơn để chấp nhận Green AI tốt hơn.

epoch 1 epoch 3backprop trained frozen model params
epoch 1 epoch 2 epoch 3
Hình 1: GreenTrainer chọn lọc thích ứng phần phù hợp nhất của mô hình LLM cho fine-tuning

Trong bài báo này, chúng tôi trình bày GreenTrainer, một kỹ thuật mới thực hiện adaptive backpropagation để fine-tuning LLM hiệu quả với mất mát độ chính xác tối thiểu. Như thể hiện trong Hình 1, với một mục tiêu giảm FLOPs, GreenTrainer chọn lọc thích ứng tập hợp các tensor mạng neural (NN) trainable trong mỗi epoch, dựa trên đánh giá tầm quan trọng của các tensor trong training. Việc đánh giá tầm quan trọng như vậy là khó khăn vì các tensor NN không liên kết trực tiếp với các biến dữ liệu input hoặc các feature trung gian, và hầu hết các kỹ thuật attribution (Sundararajan et al., 2017; Hesse et al., 2021) đánh giá tầm quan trọng feature không áp dụng được. Các metric tầm quan trọng phổ biến, bao gồm SNIP (Lee et al., 2018) và Fisher (Liu et al., 2021), chủ yếu được sử dụng trong NN pruning để định lượng tầm quan trọng của trọng số mô hình tại các giá trị hiện tại của chúng, nhưng chúng không thể định lượng tầm quan trọng của việc cập nhật trọng số trên một tensor để giảm training loss. Các metric cổ điển dựa trên đóng góp độ chính xác chính xác (Lin et al., 2022), độ lớn của cập nhật trọng số (Li et al., 2016), hoặc nhiễu loạn ngẫu nhiên (Breiman, 2001), mặt khác, hoặc không chính xác hoặc tốn kém về mặt tính toán đối với LLM. Thay vào đó, phương pháp của chúng tôi áp dụng một lý luận tương tự với các metric attribution và pruning hiện có, và định lượng đóng góp của mỗi tensor update vào training loss thông qua khai triển Taylor bậc nhất trên training loss. Theo cách này, chúng tôi đảm bảo rằng các tensor được chọn có thể đóng góp tối đa vào việc giảm training loss.

Một thách thức khác là làm thế nào để profile chính xác training FLOPs. Do sự phụ thuộc lẫn nhau giữa các tensor, tổng FLOPs của chúng trong training không bằng tổng FLOPs cá nhân của chúng. Sự phụ thuộc lẫn nhau như vậy được xác định bởi các đặc tính backpropagation của các toán tử NN trong mỗi tensor, nhưng các mô hình FLOPs hiện có không thể liên kết các toán tử NN với các tensor dựa trên luồng tính toán của backpropagation. Một số công trình hiện có (Kwon et al., 2023) chỉ kết hợp forward FLOPs theo lớp vào việc lựa chọn tensor, nhưng bỏ qua sự phụ thuộc tính toán giữa các lớp trong backpropagation. Để giải quyết thách thức này, chúng tôi mô hình hóa chặt chẽ các dependency cross-tensor trong việc profiling backpropagation FLOPs của chúng. Dựa trên mô hình này, chúng tôi phát triển một thuật toán dynamic programming (DP) để tìm ra việc lựa chọn tensor gần tối ưu từ một số lượng khả năng mũ (ví dụ, 2^515 cho 515 tensor trong mô hình OPT-2.7B (Zhang et al., 2022)). Do đó, GreenTrainer có thể đảm bảo rằng mục tiêu giảm FLOPs đã cho có thể được đáp ứng trong hầu hết các trường hợp.

Chúng tôi đánh giá GreenTrainer với ba LLM mã nguồn mở, cụ thể là OPT (Zhang et al., 2022), BLOOMZ (Muennighoff et al., 2022) và FLAN-T5 (Chung et al., 2022), trên các dataset text generation bao gồm SciTLDR (Cachola et al., 2020) và DialogSum (Chen et al., 2021). Kết quả của chúng tôi cho thấy GreenTrainer có thể tiết kiệm tới 64% training FLOPs so với full LLM fine-tuning, mà không có bất kỳ mất mát độ chính xác đáng chú ý nào. Trong một số trường hợp, GreenTrainer thậm chí có thể cải thiện độ chính xác mô hình so với full fine-tuning, bằng cách loại bỏ redundancy và overfitting của mô hình. So với các kỹ thuật hiện có như Prefix Tuning (Li and Liang, 2021) và LoRA (Hu et al., 2021), GreenTrainer cải thiện độ chính xác mô hình 4% với cùng lượng giảm FLOPs, và cũng cung cấp cho người dùng sự linh hoạt để cân bằng giữa độ chính xác training và chi phí tùy thuộc vào nhu cầu của Green AI.

2 BỐI CẢNH & ĐỘNG LỰC

2.1 KIẾN TRÚC TRANSFORMER CHO TEXT GENERATION

Các LLM hiện tại được xếp chồng bởi các transformer block (Vaswani et al., 2017), mỗi block chứa một lớp Multi-Head Attention (MHA), LayerNorms (Ba et al., 2016), và một Feed-Forward Network (FFN).

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Cho một chuỗi input X∈R^(n×d) với n token, MHA chiếu các token vào không gian (Q, K, V) h lần, sử dụng h bộ projector trainable (W_Q^(i), W_K^(i), W_V^(i))_{i=1,...,h}. Mỗi projection f_i: R^(n×d) → R^(n×d_h) được định nghĩa là Q_i, K_i, V_i = XW_Q^(i), XW_K^(i), XW_V^(i). Output (Q_i, K_i, V_i) sau đó thực hiện các cơ chế attention để tạo ra O_i bằng cách trọng số V_i với các attention score giữa Q_i và K_i. Output cuối cùng của MHA được thu được bằng cách nối các O_i, theo sau bởi một linear projection g: R^(n×d) → R^(n×d) với một projector trainable W_o:

O_i = Softmax(Q_iK_i^T/√(d/h))V_i, MHA_out = Concat(O_1, O_2, ..., O_h)W_o. (1)

Để cải thiện hiệu quả training, LLM áp dụng phương pháp teacher-forcing (Lamb et al., 2016) để tạo toàn bộ chuỗi output token trong một lần forward pass. Cụ thể, causal mask được áp dụng cho attention score của MHA, để mỗi output token có thể được dự đoán từ các label token ở các vị trí trước đó. Với kỹ thuật này, khi được fine-tune, LLM có thể được train theo cách tiêu chuẩn như bất kỳ mô hình feed-forward nào.

2.2 NHU CẦU CHO ADAPTIVE BACKPROPAGATION

Khi được fine-tune cho một tác vụ downstream, LLM thường bị over-parameterized, vì chỉ một phần của kiến thức thế giới mà chúng học được từ pre-training là hữu ích cho tác vụ đích. Trong những trường hợp này, việc chỉ liên quan một số cấu trúc con của mô hình vào fine-tuning có thể có ít tác động đến độ chính xác mô hình, nhưng giảm đáng kể lượng tính toán.

[THIS IS TABLE: Table 1 showing fine-tuning results for different substructures of OPT-2.7B and FLAN-T5-3B LLMs on DialogSum dataset, with FLOPs and accuracy metrics]

Công trình hiện có đã thực hiện các nỗ lực với việc lựa chọn cố định một số component NN, như 2 lớp cuối, decoder prefix (Li and Liang, 2021), và linear projector (W_Q, W_V) (Hu et al., 2021), trong fine-tuning. Tuy nhiên, do sự phụ thuộc lẫn nhau của các tham số NN (Jin et al., 2020), việc lựa chọn cố định như vậy sẽ làm suy giảm đáng kể độ chính xác mô hình. Như thể hiện trong Bảng 1, việc fine-tune riêng lẻ 2 lớp cuối hoặc decoder prefix dẫn đến giảm độ chính xác lên tới 10%. Lý do là các cấu trúc con NN gần kề có sự phụ thuộc lẫn nhau với các lựa chọn cố định bị loại trừ khỏi fine-tuning, và do đó trở nên không nhất quán với những cấu trúc con được chọn. Việc tăng mật độ lựa chọn, như bao gồm tất cả linear projector (W_Q, W_V), có thể giảm thiểu mất mát độ chính xác mô hình, nhưng có thể tiết kiệm nhiều nhất 33% FLOPs do backpropagating activation gradient qua các transformer block. Các phương pháp ngây thơ của việc lựa chọn động, như mở rộng phần trainable từ lớp cuối, có hạn chế tương tự.

Sự thiếu sót của các phương pháp hiện có này thúc đẩy chúng tôi thực thi việc lựa chọn linh hoạt và thích ứng hơn các cấu trúc con LLM trong backpropagation. Trong GreenTrainer, chúng tôi phát triển một metric tầm quan trọng tensor kết hợp các dependency tham số để đánh giá việc fine-tune mỗi tensor đóng góp như thế nào vào độ chính xác mô hình đã train tại runtime. Kiến thức về tầm quan trọng tensor như vậy, sau đó, cho phép chúng tôi đạt được sự giảm FLOPs mong muốn trong khi tối đa hóa độ chính xác mô hình.

[THIS IS FIGURE: Figure 2 showing backpropagation of a 4-layer dense NN with arrows indicating forward and backward paths]

2.3 MÔ HÌNH FLOPS CỦA BACKPROPAGATION

Thiết kế của GreenTrainer dựa vào việc tính toán đúng đắn backpropagation FLOPs của các cấu trúc con mô hình được chọn, có thể được phân tách thành hai phần sử dụng chain rule. Ví dụ, như

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

thể hiện trong Hình 2, khi training một NN dense 4 lớp không có bias, mỗi lớp tính toán i) dy_i là gradient của loss L w.r.t activation y_i, và ii) dw_i là loss gradient w.r.t weight W_i, sao cho

dy_i = ∂L/∂y_i = ∂L/∂y_{i+1} W_i^T = dy_{i+1} W_i^T, dw_i = ∂L/∂W_i = y_i^T ∂L/∂y_{i+1} = y_i^T dy_{i+1}, (2)

và lượng FLOPs tương ứng để tính dy_i và dw_i là t_{dy_i} và t_{dw_i}, tương ứng. (dy_i, dw_i) có thể được tính từ (dy_{i+1}, dw_{i+1}). Đặc biệt, ngay cả khi một lớp không được chọn trong fine-tuning, nó vẫn cần tính toán và truyền error gradient (dy_i) đến các lớp downstream. Do đó, lượng tính toán trong backpropagation không chỉ phụ thuộc vào các lớp được chọn, mà còn phụ thuộc vào một số lớp không được chọn. Ví dụ, nếu chỉ Layer 2 là trainable, tổng FLOPs cho backpropagation sẽ được quyết định bởi chi phí tính dw_2, dy_3 và dy_4. Do tính tổng quát của chain rule, lý luận như vậy về tính toán FLOPs cũng áp dụng được cho các loại lớp NN khác.

Dựa trên lý luận này, chúng ta có thể xây dựng các mô hình FLOPs cho các cấu trúc con LLM. Mô hình cấp lớp là coarse-grained và có thể dẫn đến việc lựa chọn tensor không chính xác. Một số tham số quan trọng có thể không được chọn do các tham số không quan trọng khác trong cùng lớp. Trong GreenTrainer, chúng tôi sử dụng granularity cấp tensor cho việc lựa chọn như vậy, có thể được hỗ trợ tốt bởi các thư viện NN tensorized (ví dụ, TensorFlow (Abadi, 2016) và PyTorch (Paszke et al., 2019)). Việc lựa chọn cấp weight, mặc dù fine-grained hơn, quá tốn kém về mặt tính toán do yêu cầu indexing fine-grained.

3 PHƯƠNG PHÁP

Để giảm FLOPs của LLM fine-tuning, một công thức bài toán trực quan là tối thiểu hóa FLOPs trong khi đạt được độ chính xác mô hình mong muốn. Tuy nhiên, khó có thể xác định một mục tiêu độ chính xác phù hợp trước, vì một số mục tiêu độ chính xác có thể yêu cầu training rất chuyên sâu và độ chính xác mà chúng ta có thể đạt được với ngân sách FLOPs của chúng ta không thể được ước tính trước khi training. Thay vào đó, chúng tôi tối đa hóa việc giảm training loss trong khi đạt được sự giảm FLOPs mong muốn:

max Δloss(m) s.t. T_{selective}(m) ≤ ρT_{full}, (3)

trong đó m là một vector nhị phân cần giải để lựa chọn tensor. m tham số hóa cả việc giảm loss (Δloss) và FLOPs training per-batch (T_{selective}), và T_{selective} bị ràng buộc trong một tỷ lệ được người dùng chỉ định (ρ) của FLOPs fine-tuning toàn bộ mô hình (T_{full}). Ví dụ, ρ = 0.5 có nghĩa là FLOPs của fine-tuning phải nhiều nhất là 50% của việc fine-tuning toàn bộ mô hình. Trong thực tế, giá trị của ρ có thể được preset hoặc điều chỉnh tại runtime trong bất kỳ giai đoạn nào của training.

Để xác định đóng góp của mỗi tensor trong fine-tuning, chúng tôi mô hình hóa Δloss(m) như tầm quan trọng tổng hợp của các tensor được chọn, và tính toán FLOPs được tạo ra bởi các tensor được chọn sử dụng mô hình FLOPs của backpropagation trong Mục 2.3. Với mô hình này, Phương trình (3) có thể được viết lại như:

max Δloss(m) s.t. T_{fp} + m · t_{dw} + σ(m) · t_{dy} ≤ ρT_{full}, (4)

trong đó T_{fp} chỉ ra FLOPs per-batch của forward pass, và mỗi cặp biến trong (t_{dy}, t_{dw}) đại diện cho FLOPs của việc tính (dy, dw) cho tensor tương ứng, tương ứng. Với một selector nhị phân m, σ(m) kết hợp tất cả các tensor dọc theo backward pass góp phần vào FLOPs của fine-tuning, bằng cách liên quan đến việc truyền error gradient (dy). Ví dụ, nếu m = [0,0,1,0,1,0,0], tất cả các tensor ở các lớp sâu hơn so với các tensor được chọn được liên quan đến việc truyền error gradient, và do đó σ(m) = [0,0,1,1,1,1,1].

Để làm cụ thể công thức này và giải m, GreenTrainer bao gồm ba component chính: (i) Tensor FLOPs Profiling, tính toán FLOPs của tất cả NN tensor (tức là t_{dy} và t_{dw}) trước training; (ii) Tensor Importance Evaluation, định lượng đóng góp của việc cập nhật mỗi NN tensor vào chất lượng training tại runtime; (iii) Tensor Selector, làm cụ thể bài toán lựa chọn tensor sử dụng FLOPs và tầm quan trọng của tensor, và cung cấp giải pháp thông qua dynamic programming tại runtime.

3.1 TENSOR FLOPS PROFILING

Các profiler NN tiêu chuẩn, như Torch Profiler (Paszke et al., 2019), có thể đo FLOPs thực thi của các toán tử NN cá nhân như matrix multiplication và convolution. Tuy nhiên, nó không thể được liên kết trực tiếp với các tensor NN tham gia vào các phép toán này. Khi một tập hợp tensor được train, training FLOPs của backpropagation không bằng tổng FLOPs cá nhân của chúng.

Để giải quyết hạn chế này, phương pháp của chúng tôi bao gồm hai bước. Đầu tiên, chúng tôi chuyển đổi cấu trúc NN dựa trên lớp của LLM thành một computing graph cấp tensor, giữ lại thứ tự thực thi của tất cả

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS FIGURE: Figure 3 showing a workflow diagram of tensor FLOPs profiling with input embedding, projector components, and tensor-level graph]

việc liên quan của các tensor trong training. Sau đó, chúng tôi trích xuất các backpropagation operator liên quan của mỗi tensor, và suy ra FLOPs của mỗi tensor i trong backpropagation (t_{dy_i} và t_{dw_i}) bằng cách matching và tổng hợp FLOPs của các NN operator này. Ví dụ trong Hình 3, việc training mỗi linear projector (Q, K và V) trong một lớp MHA phải được thực thi sau khi training bias tensor tương ứng của nó. Việc training mỗi linear projector, sau đó, sẽ liên quan đến hai matrix multiplication operator, có FLOPs trong backpropagation sẽ được tổng hợp. Chúng tôi phân loại các quy tắc matching và tổng hợp như vậy theo loại lớp LLM nơi các tensor được đặt, như mô tả bên dưới. Một ví dụ cụ thể về tensor FLOPs profiling như vậy trên mô hình OPT-2.7B được cung cấp trong Phụ lục A.3.

**Input & output embedding layers.** Lớp input embedding chứa một embedding tensor trainable ánh xạ mỗi raw token thành một biểu diễn dense. Với activation gradient dy_{i+1} từ các lớp upstream, việc suy ra update của tensor này chỉ liên quan đến variable assignment, và chúng ta có thể an toàn coi t_{dw_i} ≈ 0 cho bất kỳ tensor i nào. Nếu một raw token được ánh xạ đến vector thứ k trong embedding tensor trong forward pass, thì trong backpropagation, dy_{i+1} từ upstream sẽ chỉ được gán cho hàng thứ k của dw_i, sao cho dw_i[s] = dy_{i+1} nếu s = k, ngược lại dw_i[s] = 0. Vì lớp input không truyền activation gradient, chúng ta cũng có thể kết luận rằng t_{dy} của nó là 0.

Ngược lại, lớp output embedding chiếu mỗi token trở lại không gian xác suất. Một cách trực quan, (t_{dy}, t_{dw}) của nó có thể được suy ra theo cách tương tự như chúng ta đã làm với dense layer trong Phương trình (2). Tuy nhiên, trong hầu hết LLM, lớp output embedding chia sẻ cùng một tensor trainable với lớp input embedding. Điều này ngụ ý rằng nếu output embedding có thể train được, thì input embedding cũng sẽ được liên quan đến training. Do đó, tất cả t_{dy} từ output của LLM, lên đến lớp input embedding, phải được tích lũy vào t_{dy} của output embedding tensor, trong khi t_{dw} của nó vẫn không thay đổi.

**Multi-Head Attention (MHA) layer.** Một lớp MHA chứa nhiều linear projector như các tensor trainable, và FLOPs của chúng trong training có thể được suy ra theo cách tương tự như chúng ta đã làm với dense layer trong Phương trình (2). Một số LLM (ví dụ, OPT) cũng bao gồm bias như một loại tensor trainable khác sau projection như vậy. Trong trường hợp này, dựa trên chain rule, backpropagation của bias được tính như dy_i = dy_{i+1} và dw_i = 1^T dy_{i+1}, chỉ ra rằng t_{dy} cho bias là 0 vì dy_i được truyền giống hệt từ dy_{i+1}. t_{dw} của bias có thể được suy ra như FLOPs của việc cộng các element trong dy_{i+1} dọc theo mọi feature channel. Cơ chế attention trong Phương trình (1) được backpropagated trước các projector. Nếu bất kỳ projector nào trong số này được liên quan đến training, FLOPs backpropagation của attention cũng phải được tính toán, và chúng tôi tích lũy FLOPs như vậy vào t_{dy} của tensor projector tương ứng (W_V).

**LayerNorm.** Với một token, LayerNorm đầu tiên chuẩn hóa các feature của nó và sử dụng hai tensor trainable γ và β để element-wise nhân và cộng với token, tương ứng. Các phép toán nhân và cộng tương tự như những phép toán trong dense layer, và do đó FLOPs của nó có thể được tính theo cách tương tự. Tuy nhiên, backpropagation FLOPs của normalization operator phải được tích lũy vào t_{dy} của tensor trước đó. Nếu bất kỳ tensor nào trong các lớp trước được train, FLOPs của việc truyền normalization operator cũng phải được bao gồm trong FLOPs của lớp hiện tại.

**Feed-Forward Network (FFN).** Trong FFN, có một activation function phi tuyến giữa hai dense layer. Theo cùng phương pháp tính FLOPs của LayerNorm, chúng tôi tích lũy FLOPs của việc truyền qua activation function này vào t_{dy} của bias tensor trong dense layer đầu tiên.

3.2 TENSOR IMPORTANCE EVALUATION

Tầm quan trọng của một tensor trong training có thể được ước tính như tổng tầm quan trọng của tất cả trọng số của nó. Trong training, vì trọng số mô hình được cập nhật lặp đi lặp lại để tối thiểu hóa training loss, một phương pháp trực quan để đánh giá tầm quan trọng của một weight update trong một iteration cho trước là hoàn tác update này và kiểm tra training loss tăng trở lại như thế nào dưới dạng ΔL = L(w) - L(w + Δw), để một giá trị ΔL cao hơn có nghĩa là update này quan trọng hơn và weight nên được chọn.

Tuy nhiên, việc tính ΔL cho mọi weight là tốn kém. Thay vào đó, chúng tôi ước tính tầm quan trọng của tất cả weight trong một lần bằng cách làm mượt phép toán hoàn tác được mô tả ở trên và tính loss gradient đối với các update tương ứng với tất cả weight. Để c ∈ [0,1]^M biểu thị phép toán hoàn tác cho tất cả M weight, chúng ta có thể tính loss gradient như

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

-∂L(w + c ⊙ Δw)/∂c = -Δw ⊙ ∂L(u)/∂u|_{u=w+c⊙Δw}, (5)

trong đó ⊙ biểu thị phép nhân element-wise. Khi c = 0, Phương trình (5) trở thành một vector tầm quan trọng trên tất cả weight. Vì loss gradient được tham số hóa bởi tất cả weight, tầm quan trọng weight được tính theo cách này ngầm định kết hợp tác động của các dependency weight. Tầm quan trọng của tensor k sau đó được tính như

I_k = -∑_i Δw_i^{(k)} ∂L/∂w_i^{(k)}. (6)

Trong một số trường hợp, khi quá trình training gặp phải divergence, các giá trị của gradient và tầm quan trọng tensor được tính trong Phương trình (6) có thể rất lớn, cuối cùng dẫn đến overflow khi sử dụng các giá trị tầm quan trọng này để quyết định việc lựa chọn tensor trong Phương trình (4). Để giải quyết vấn đề này, chúng ta có thể tiếp tục scale tất cả tầm quan trọng tensor theo biên độ tối đa để cải thiện tính ổn định số.

[THIS IS FIGURE: Figure 4 showing two parts: (a) Subproblem definition with a table structure and (b) Finding recurrence relations with diagrams showing different cases]

3.3 TENSOR SELECTION

Vì Phương trình (4) là một bài toán integer programming phi tuyến và do đó NP-hard, trong GreenTrainer chúng tôi tìm kiếm một giải pháp xấp xỉ sử dụng dynamic programming (DP). Chúng tôi phân tách toàn bộ bài toán thành các subproblem bị ràng buộc bởi các độ sâu khác nhau của backpropagation. Các subproblem này có thể được giải tuần tự từ cái có độ sâu nhỏ nhất, bằng cách sử dụng các mối quan hệ recurrence của chúng.

**Định nghĩa subproblem.** Như thể hiện trong Hình 4(a), chúng tôi định nghĩa mỗi subproblem P[k, t] là tối đa hóa tầm quan trọng tích lũy của các tensor được chọn khi 1) việc lựa chọn nằm trong top k tensor¹ và 2) backpropagation FLOPs nhiều nhất là t. DP bắt đầu bằng việc giải subproblem nhỏ nhất P[k = 1, t = 1] và dần dần giải các subproblem lớn hơn dựa trên kết quả của các subproblem nhỏ hơn và mối quan hệ recurrence của các subproblem này, cho đến khi bài toán đích P[N, T_{full}] được giải.

**Mối quan hệ recurrence của các subproblem.** Mối quan hệ recurrence giữa subproblem P[k, t] và P[k-1, t] phụ thuộc vào việc liệu chúng ta có tiếp tục chọn top tensor k từ giải pháp của P[k-1, t] hay không, như thể hiện trong Hình 4(b). **Trường hợp 1:** Nếu k không được chọn, P[k, t] sẽ quay về P[k-1, t], vì tầm quan trọng của các tensor được chọn sẽ không được tăng thêm. **Trường hợp 2:** Nếu k được chọn, thì FLOPs của nó sẽ được bao gồm vào giải pháp của P[k, t], bất kể tensor nào khác được chọn. FLOPs liên quan đến tensor k bao gồm 1) FLOPs để cập nhật tensor k và 2) FLOPs để truyền activation gradient từ tensor được chọn gần nhất k_c, như tensor k-3 như thể hiện trong Hình 4(b), đến tensor k. Điều này ngụ ý rằng P[k, t] quay về một subproblem đã được giải trước đó P[k-k_c, t-Δt], trong đó

Δt = t_{dw_k} + ∑_{j=k_c}^{k-1} t_{dy_j}. (7)

Vì k_c không được biết trước, chúng tôi backtrace các subproblem đã được giải trước đó và khám phá tất cả khả năng của k_c bằng cách giảm độ sâu backpropagation từ k, và giải pháp tối ưu cho P[k, t] là cái có tầm quan trọng tích lũy cao nhất của các tensor được chọn. Dựa trên mối quan hệ recurrence này, chúng ta có thể giải tất cả subproblem bằng cách duyệt không gian subproblem. Độ phức tạp thời gian của việc giải mỗi subproblem là O(N), và độ phức tạp thời gian tổng thể của DP là O(N²T_{full}).

4 THÍ NGHIỆM

Trong đánh giá của chúng tôi, chúng tôi bao gồm các LLM decoder-only bao gồm OPT (Zhang et al., 2022) và BLOOMZ (Muennighoff et al., 2022), và một LLM encoder-decoder, cụ thể là FLAN-T5 (Chung et al., 2022),

¹Chúng tôi coi tensor gần nhất với NN output là topmost.

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

với kích thước LLM từ 350M đến 6.7B. Các thí nghiệm của chúng tôi chủ yếu được thực hiện sử dụng hai dataset sau của abstractive summarization:

• **SciTLDR** (Cachola et al., 2020) là một dataset của 5.4K text summary trên 3.2K paper. Nó chứa cả TLDR được tác giả viết và được chuyên gia suy ra, trong đó loại sau được thu thập bởi một giao thức annotation tạo ra summary chất lượng cao với gánh nặng annotation thấp.

• **DialogSum** (Chen et al., 2021) là một dialogue summarization dataset của 13,460 dialogue với summary và topic được gán nhãn thủ công. Nó đã được chứng minh là thách thức hơn các dataset summarization khác, như SAMSum (Gliwa et al., 2019) và CNN/Daily (Nallapati et al., 2016) ở quy mô tương tự.

Chúng tôi cũng thực hiện các tác vụ generative QA trên dataset WebQuestion (Berant et al., 2013) và PIQA (Bisk et al., 2020) trong Phụ lục A.4. Tuy nhiên, chúng tôi không xem xét các tác vụ không tạo sinh như phân loại tình cảm, phân loại entailment và extractive QA, vì các tác vụ này quá dễ đối với LLM và việc test chúng với LLM sẽ dẫn đến cải thiện hiệu suất phóng đại so với baseline.

Đối với OPT và BLOOMZ, chúng tôi tuân theo cấu trúc prompt giống GPT2 (Radford et al., 2019), "[source seq.] TL;DR:", cho các tác vụ summarization để preprocess dữ liệu input. Đối với FLAN-T5, chúng tôi áp dụng cấu trúc prompt "summarize: [source seq.]" được sử dụng trong pre-training T5 gốc. Chúng tôi truncate các source sequence để độ dài của mọi preprocessed input sequence nằm trong 512 token. Trên test data, chúng tôi sử dụng beam search size là 4, và đặt số lượng token được tạo tối đa là 64 cho SciTLDR và 128 cho DialogSum. Chúng tôi so sánh GreenTrainer (GT) với các baseline sau:

• **Full Fine-Tuning (Full FT)** fine-tune tất cả tham số LLM và nên đạt được độ chính xác tốt nhất của mô hình đã train một cách trực quan.

• **Fine-Tuning Top2 (FT-Top2)** chỉ fine-tune hai lớp cuối, thường là embedding layer và một LayerNorm. Input và output embedding layer được tied cho OPT và BLOOMZ, nhưng không tied cho FLAN-T5. Baseline ngây thơ này chỉ fine-tune phần nhỏ nhất của tham số LLM và được sử dụng để xác định liệu dataset có trivial đối với LLM hay không.

• **Prefix Tuning (Prefix-T)** (Li and Liang, 2021) chèn các prefix trainable vào input sequence của mỗi transformer block trong khi đóng băng các tham số mô hình. Đối với LLM encoder-decoder, các prefix trainable chỉ được chèn vào các decoder block.

• **LoRA** (Hu et al., 2021) hiện là phương pháp phổ biến nhất để fine-tuning LLM hiệu quả. Nó sử dụng matrix decomposition low-rank để giảm training cost. Chúng tôi áp dụng LoRA cho cả query và value projector, như đề xuất trong (Hu et al., 2021).

Trong tất cả thí nghiệm, chúng tôi sử dụng batch size là 4 và fine-tune mô hình trong 5 epoch. Chúng tôi sử dụng optimizer AdamW (Loshchilov and Hutter, 2017) với learning rate 2×10⁻⁵ với linear schedule và weight decay là 10⁻². Chúng tôi sử dụng ROUGE score (%R1/R2/RL) (Lin, 2004) như metric độ chính xác, và đo cả Peta-FLOPs (PFLOPs) và wall-clock time như training cost trong mỗi lần chạy. Chúng tôi đo end-to-end cost của training, bao gồm computing cost trong forward và backward pass, và computing cost của tensor importance evaluation và tensor selection sử dụng DP.

4.1 TRAINING COST & ACCURACY

Chúng tôi đầu tiên đánh giá training cost và accuracy của GreenTrainer (GT). Như thể hiện trong Bảng 2, đối với mô hình OPT-2.7B, GT-0.5 có thể đạt được yêu cầu giảm 50% FLOPs với mất mát độ chính xác nhiều nhất 2%, và GT-0.7 thậm chí có thể đạt được ROUGE score cao hơn 0.2%-3% so với Full FT. Chúng tôi đưa ra giả thuyết rằng điều này là vì GT chỉ fine-tune các tensor quan trọng nhất và do đó giảm thiểu overfitting có thể có trong Full FT. Tham số trainable không đủ cũng có thể dẫn đến underfitting, vì FT-Top2 có ROUGE score thấp hơn đáng kể. Tương tự, so với LoRA và Prefix Tuning, GT-0.7 đạt được độ chính xác cao hơn ít nhất 2% với cùng lượng training FLOPs.

Tương tự, đối với BLOOMZ-3B, GT-0.5 có thể tiết kiệm 50% training FLOPs và wall-clock time với mất mát độ chính xác <2%. So với Full FT, GT-0.7 đạt được cùng ROUGE score trên SciTLDR, và cao hơn 4%-10% trên DialogSum. Với cùng lượng training FLOPs, GT-0.7 có ROUGE score cao hơn 0.4%-1.4% so với LoRA. Lưu ý rằng cả hai dataset đều không trivial đối với mô hình BLOOMZ, vì baseline ngây thơ (FT-Top2) vẫn thể hiện mất mát độ chính xác cao.

Đối với mô hình FLAN-T5-3B, FT-Top2 đạt được chất lượng fine-tuning tương tự Full FT với FLOPs thấp hơn, chỉ ra rằng dataset SciTLDR là trivial đối với FLAN-T5. Trong trường hợp này, GT-0.34 có thể đạt được cùng FLOPs và ROUGE score bằng cách chọn một phần nhỏ tensor. Mặt khác, FT-Top2 mất độ chính xác đáng kể trên DialogSum, nhưng GT-0.4 giảm 54% training FLOPs và 43% wall-clock time mà không có mất mát độ chính xác đáng chú ý. GT-0.4 cũng vượt trội LoRA 1% trên ROUGE score và giảm thêm 11% FLOPs. So với Prefix tuning, GT-0.34 đạt được ROUGE score cao hơn 2%-5%, trong khi giảm cùng lượng training FLOPs.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Table 2 showing comparison of training cost & accuracy in LLM fine-tuning across different models (OPT-2.7B, BLOOMZ-3B, FLAN-T5-3B) and methods, with metrics for PFLOPs, Time, and R1/R2/RL scores on SciTLDR and DialogSum datasets]

4.2 TÁC ĐỘNG CỦA MỤC TIÊU GIẢM FLOPS

Để hiểu rõ hơn về cách GreenTrainer hoạt động với các mục tiêu giảm FLOPs khác nhau, chúng tôi thay đổi giá trị của ρ giữa 0.36 và 0.8, và so sánh GreenTrainer với LoRA trên mô hình OPT-2.7B. Như thể hiện trong Bảng 3, trên dataset SciTLDR, khi yêu cầu giảm FLOPs cao và tương ứng với giá trị ρ ≤ 0.4, GreenTrainer vượt trội LoRA bằng cách đạt được ROUGE score cao hơn 2% và tiết kiệm thêm 25% FLOPs và wall-clock time. Mặt khác, khi giá trị ρ tăng lên 0.6, GreenTrainer vượt trội Full FT trên ROUGE score 0.5% và vượt trội LoRA 5.2%, nhưng tiết kiệm 40% training FLOPs và 39% wall-clock time so với Full FT. Kết quả tương tự cũng được quan sát trên dataset DialogSum. Tóm lại, với các mục tiêu giảm FLOPs khác nhau, GreenTrainer luôn có thể cung cấp sự cân bằng tốt hơn giữa độ chính xác training và chi phí, so với các baseline SOTA.

[THIS IS TABLE: Table 3 showing impact of different FLOPs reduction objectives on OPT-2.7B model, comparing various methods across different ρ values]

Các kết quả này cũng chứng minh rằng GreenTrainer cung cấp tính linh hoạt lớn trong fine-tuning LLM giữa độ chính xác training và chi phí, bằng cách điều chỉnh giá trị ρ. Người dùng có thể chọn đặt giá trị ρ thấp (≤ 0.4) để tối đa hóa việc giảm FLOPs (>60%) với mất mát độ chính xác mô hình vừa phải (3%-4% trên hai dataset chúng tôi sử dụng). Hoặc, họ có thể sử dụng giá trị ρ cao (≥ 0.6) để có cùng mức độ giảm FLOPs như LoRA, nhưng đảm bảo mất mát độ chính xác mô hình tối thiểu hoặc thậm chí cải thiện độ chính xác mô hình nhỏ. Chúng tôi tin rằng tính linh hoạt như vậy có tầm quan trọng thực tế khi fine-tune LLM cho các tác vụ downstream với các yêu cầu và ràng buộc green AI khác nhau.

4.3 HIỆU QUẢ CỦA CÁC METRIC TENSOR IMPORTANCE

Chất lượng fine-tuning của GreenTrainer dựa trên việc đánh giá đúng đắn tầm quan trọng tensor. Chúng tôi so sánh metric của chúng tôi (∆w ∂L/∂w) với metric dựa trên magnitude (∆w) (Lee et al., 2020) và metric chỉ gradient (∂L/∂w) (Aji and Heafield, 2017), sử dụng mô hình OPT-2.7B với ρ = 0.7. Như thể hiện

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Table 4 showing efficacy of Tensor Importance Metrics for OPT-2.7B model, with rows for different methods and columns for SciTLDR and DialogSum datasets showing PFLOPs, Time, and R1/R2/RL scores]

trong Bảng 4, với cùng mục tiêu giảm FLOPs, việc sử dụng metric của chúng tôi (∆w ∂L/∂w) để đánh giá tầm quan trọng tensor đạt được độ chính xác mô hình cao nhất và vượt trội Full FT 1%-3% trên ROUGE score. Điều này là vì các metric dựa trên magnitude bỏ qua các dependency của weight update. Các metric chỉ gradient chỉ chứa thông tin hướng về tầm quan trọng tensor nhưng không thể phản ánh cường độ của tầm quan trọng. Các đo lường tầm quan trọng không chính xác sẽ dẫn đến việc lựa chọn các tensor trainable không phù hợp.

4.4 TÁC ĐỘNG CỦA KÍCH THƯỚC LLM

Một loại LLM có thể chứa nhiều biến thể với kích thước khác nhau. Để nghiên cứu hiệu suất của GreenTrainer với các kích thước LLM khác nhau, chúng tôi thực hiện fine-tuning sử dụng các mô hình OPT với kích thước từ 350M đến 6.7B. Như thể hiện trong Bảng 5, ngay cả trên các mô hình nhỏ (OPT-350M), GT-0.5 có thể tiết kiệm thêm 17%-21% training FLOPs so với LoRA, trong khi đạt được độ chính xác cao hơn 2%-4% (trên SciTDR) hoặc cùng độ chính xác (trên DialogSum). Khi kích thước mô hình tăng lên 2.7B, GT-0.5 vượt trội LoRA và GT-0.7 vượt trội Full FT trên dataset SciTLDR. Trên DialogSum, GT-0.7 hoạt động tương tự so với LoRA. Đối với mô hình OPT-6.7B², GT-0.4 có thể tiết kiệm thêm 27% training FLOPs so với LoRA trên SciTLDR, trong khi đạt được cùng độ chính xác mô hình, và lợi thế tương tự cũng có thể được quan sát khi so sánh GT-0.5 và GT-0.7 với LoRA. Nói chung, lợi thế hiệu suất của GreenTrainer áp dụng rộng rãi cho LLM với các kích thước khác nhau.

[THIS IS TABLE: Table 5 showing impact of LLM model size across different parameter counts (OPT-350M to OPT-6.7B) and methods, with performance metrics on SciTLDR and DialogSum datasets]

5 KẾT LUẬN

Trong bài báo này, chúng tôi trình bày GreenTrainer, một kỹ thuật mới để fine-tuning LLM cho phép lựa chọn hiệu quả các tham số trainable thông qua adaptive backpropagation, để đảm bảo chất lượng training cao trong khi tối thiểu hóa chi phí tính toán. GreenTrainer tiết kiệm tới 64% training FLOPs so với full fine-tuning mà không có mất mát độ chính xác đáng chú ý. So với các kỹ thuật hiện có như Prefix Tuning và LoRA, GreenTrainer cải thiện độ chính xác lên tới 4% với cùng sự giảm FLOPs.

²Đối với OPT-6.7B, Full FT và GT-0.7 với DialogSum có vấn đề out-of-memory trên GPU chúng tôi sử dụng.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN

Chúng tôi muốn cảm ơn các reviewer ẩn danh và area chair về các nhận xét và phản hồi của họ. Công trình này được hỗ trợ một phần bởi National Science Foundation (NSF) dưới số grant IIS-2205360, CCF-2217003 và CCF-2215042.

TÀI LIỆU THAM KHẢO

2023 AI index report. https://aiindex.stanford.edu/report/ , 2023.

M. Abadi. Tensorflow: learning functions at scale. In Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming , pages 1–1, 2016.

A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021 , 2017.

J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.

J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013.

Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages 7432–7439, 2020.

L. Breiman. Random forests. Machine learning , 45:5–32, 2001.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

I. Cachola, K. Lo, A. Cohan, and D. S. Weld. Tldr: Extreme summarization of scientific documents. arXiv preprint arXiv:2004.15011 , 2020.

A. Candel, J. McKinney, P. Singer, P. Pfeiffer, M. Jeblick, P. Prabhu, J. Gambera, M. Landry, S. Bansal, R. Chesler, et al. h2ogpt: Democratizing large language models. arXiv preprint arXiv:2306.08161 , 2023.

Y. Chen, Y. Liu, L. Chen, and Y. Zhang. Dialogsum: A real-life scenario dialogue summarization dataset. arXiv preprint arXiv:2105.06762 , 2021.

H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

B. Gliwa, I. Mochol, M. Biesek, and A. Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237 , 2019.

R. Hesse, S. Schaub-Meyer, and S. Roth. Fast axiomatic attribution for neural networks. Advances in Neural Information Processing Systems , 34:19513–19524, 2021.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.

Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, and S. Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 , 2023.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

K. Huang, B. Yang, and W. Gao. Elastictrainer: Speeding up on-device training with runtime elastic tensor selection. In Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services , pages 56–69, 2023a.

K. Huang, B. Yang, and W. Gao. Modality plug-and-play: Elastic modality adaptation in multimodal llms for embodied ai. arXiv preprint arXiv:2312.07886 , 2023b.

G. Jin, X. Yi, L. Zhang, L. Zhang, S. Schewe, and X. Huang. How does weight correlation affect generalisation ability of deep neural networks? Advances in Neural Information Processing Systems , 33:21346–21356, 2020.

Y. D. Kwon, R. Li, S. I. Venieris, J. Chauhan, N. D. Lane, and C. Mascolo. Tinytrain: Deep neural network training at the extreme edge. arXiv preprint arXiv:2307.09988 , 2023.

A. M. Lamb, A. G. ALIAS PARTH GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. Advances in neural information processing systems , 29, 2016.

J. Lee, S. Park, S. Mo, S. Ahn, and J. Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611 , 2020.

N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.

B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.

H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 , 2016.

X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 , 2021.

B. Liao, S. Tan, and C. Monz. Make your pre-trained model reversible: From parameter to memory efficient fine-tuning. arXiv preprint arXiv:2306.00477 , 2023.

C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013 .

J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, C. Gan, and S. Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems , 35:22941–22954, 2022.

L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang, Y. Chen, W. Yang, Q. Liao, and W. Zhang. Group fisher pruning for practical network compression. In International Conference on Machine Learning , pages 7021–7032. PMLR, 2021.

X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 61–68, 2022.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.

K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247 , 1, 2021.

S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora. Fine-tuning language models with just forward passes. arXiv preprint arXiv:2305.17333 , 2023.

N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.

R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 , 2016.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 , 2019.

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.

R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green ai. Communications of the ACM , 63(12):54–63, 2020.

T. Scialom, T. Chakrabarty, and S. Muresan. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 6107–6122, 2022.

M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International conference on machine learning , pages 3319–3328. PMLR, 2017.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.

H. Wang and W. Gao. Tackling the unlimited staleness in federated learning with intertwined data and device heterogeneities. arXiv preprint arXiv:2309.13536 , 2023.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.

E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.

Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512 , 2023.

S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

A PHỤ LỤC

A.1 GIẢM VIỆC SỬ DỤNG BỘ NHỚ CỦA TENSOR IMPORTANCE EVALUATION

Phương pháp của chúng tôi để đánh giá tầm quan trọng của các tensor NN trong Mục 3.2 yêu cầu cache tất cả trọng số mô hình trước đó và các gradient hiện tại, để tính Phương trình (6). Tuy nhiên, việc làm như vậy tăng đáng kể việc tiêu thụ bộ nhớ GPU, đặc biệt đối với các LLM hiện đại với hàng tỷ trọng số mô hình. Để giảm việc sử dụng bộ nhớ GPU như vậy, chúng tôi quan sát rằng công thức bài toán của chúng tôi trong Phương trình (4) sẽ ngăn các tensor trong các lớp đầu được chọn để training, do chi phí cao của việc truyền activation gradient trong backpropagation. Do đó, chúng ta có thể loại trừ an toàn các tensor này khỏi phần trainable của LLM fine-tuning và tiết kiệm một lượng đáng kể bộ nhớ GPU.

Cụ thể hơn, backpropagation trong tensor importance evaluation có thể được dừng sớm tại một tensor k nào đó, sao cho

∑_{i=k-1,...,N} t_{dy_i} < ρT_{full} ≤ ∑_{i=k,...,N} t_{dy_i}, (8)

tức là, FLOPs tích lũy của tất cả tensor từ 1 đến k vừa vượt quá mục tiêu giảm FLOPs của chúng ta. Như thể hiện trong Bảng 6, bằng cách áp dụng phương pháp early stopping như vậy, chúng ta có thể tiết kiệm bộ nhớ GPU tỷ lệ thuận với giá trị ρ, vì giá trị ρ nhỏ hơn dẫn đến k nhỏ hơn và backpropagation do đó có thể được dừng sớm hơn. Ví dụ, khi ρ = 50%, 25% bộ nhớ GPU có thể được tiết kiệm, và việc tiết kiệm như vậy có thể tăng thêm lên 50% khi ρ = 34%.

[THIS IS TABLE: Table 6 showing GPU memory consumption (in GigaBytes) for tensor importance evaluation across different models and early-stop values]

A.2 GIẢM CHI PHÍ TÍNH TOÁN CỦA DYNAMIC PROGRAMMING CHO TENSOR SELECTION

Trong phương pháp dynamic programming (DP) đề xuất của chúng tôi để lựa chọn tensor trong Mục 3.3, do khối lượng FLOPs cao trong LLM fine-tuning, giá trị của T_{full} có thể rất lớn. Để giảm chi phí tính toán của DP, chúng ta có thể giảm không gian subproblem bằng cách bỏ qua hai loại subproblem: 1) **invalid ones**, có ràng buộc FLOPs t vượt quá ràng buộc mong muốn (ρT_{full}); 2) **redundant ones**, có FLOPs để truyền activation gradient đến độ sâu được phép tối đa (k) vượt quá t. Thí nghiệm sơ bộ của chúng tôi cho thấy rằng, việc làm như vậy trên một mô hình OPT với ρ_{bp} = 50% có thể giảm số lượng subproblem đi 5.5× mà không ảnh hưởng đến tính tối ưu của training.

[THIS IS TABLE: Table 7 showing impact of DP resolution T_q on fine-tuning various LLM models with different metrics]

Bên cạnh đó, để tiếp tục giảm số lượng subproblem, chúng tôi scale FLOPs của tensor (t_{dw}, t_{dy}) bằng cách nhân với một hệ số Z:

\tilde{t}_{dw} = ⌊t_{dw} · Z⌋, \tilde{t}_{dy} = ⌊t_{dy} · Z⌋, (9)

trong đó Z = T_q/T_{full} giảm backpropagation FLOPs xuống độ phân giải T_q < T_{full}. Độ phức tạp thời gian tổng thể của DP sau đó được giảm xuống O(N^2T_q). Mặt khác, độ phân giải giảm như vậy có thể tăng sự mơ hồ trong DP và ảnh hưởng đến chất lượng training. Để điều tra sự cân bằng như vậy giữa chất lượng training và chi phí, chúng tôi thực hiện thí nghiệm sơ bộ trên nhiều LLM. Kết quả

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

trong Bảng 7 cho thấy rằng, đối với cả mô hình OPT-2.7B và BLOOMZ-3B, việc đặt T_q = 1e3 giảm overhead DP xuống <1% mà không ảnh hưởng đến chất lượng training. Tương tự, đối với FLAN-T5-3B, việc chọn T_q = 1e2 có thể giữ lại chất lượng training tốt với overhead không đáng kể. Mặt khác, khi T_q quá nhỏ, giải pháp của DP có thể không chính xác và do đó dẫn đến việc giảm FLOPs training không hiệu quả.

[THIS IS FIGURE: Figure 5 showing an example of tensor FLOPs profiling in OPT-2.7B model with tensor-level representation and backpropagation formulas]

A.3 MỘT VÍ DỤ VỀ TENSOR FLOPS PROFILING TRONG MÔ HÌNH OPT-2.7B

Để tạo điều kiện hiểu rõ hơn, chúng tôi tiếp tục thể hiện một ví dụ trong Hình 5 về cách chúng tôi profile các tensor trong mô hình OPT-2.7B trong các thí nghiệm của chúng tôi. Đầu tiên, chúng tôi chuyển đổi computing graph của LLM, được triển khai trong Python code, thành một biểu diễn chỉ tensor. Các tensor được sắp xếp dựa trên thứ tự thực thi của chúng trong forward pass, tương tự như layer-level graph trong Hình 3. Sau đó chúng tôi tính FLOPs của mỗi tensor (t_{dy}, t_{dy}) dựa trên các công thức backpropagation được thảo luận trong Mục 3.1. Các tính toán như vậy về cơ bản là đếm các phép nhân và được cộng trong các công thức của chúng.

[THIS IS TABLE: Table 8 showing OPT-2.7B performance on PIQA dataset]
[THIS IS TABLE: Table 9 showing OPT-2.7B performance on WebQuestion dataset]

A.4 HIỆU SUẤT TRÊN CÁC TÁC VỤ GENERATIVE QUESTION-ANSWERING

Để đánh giá hiệu suất của GreenTrainer trên các tác vụ khác tốt hơn, chúng tôi cũng thực hiện thí nghiệm bằng cách sử dụng mô hình OPT-2.7B trên dataset WebQuestions và PIQA cho các tác vụ generative QA. Dataset WebQuestions chứa 6,642 cặp QA sử dụng Freebase làm knowledge base. Dataset PIQA tập trung vào multi-choice QA về kiến thức vật lý với 21k cặp QA. Chúng tôi áp dụng định dạng prompt " question: {q}</s>answer: {a}</s> " cho WebQuestions và " goal: {q}</s>sol1: {sol1}</s>sol2: {sol2}</s>label: {a}</s> " cho PIQA, trong đó </s> là EOS token cho các mô hình OPT. Các siêu tham số cho training giống như những cái được mô tả trong Mục 4. Chúng tôi đánh giá sentence-level accuracy yêu cầu câu trả lời được tạo phải khớp chính xác với ground truth. Lưu ý rằng đối với PIQA, các token được tạo vẫn được dự đoán từ toàn bộ từ điển của OPT embedding thay vì từ hai lựa chọn: thứ nhất hoặc thứ hai. Như thể hiện trong Bảng 8 và Bảng 9, trên cả hai dataset, GreenTrainer (GT) đạt được độ chính xác và hiệu quả thời gian tốt hơn đáng kể so với LoRA.

Đặc biệt, kết quả trên dataset PIQA nói chung thấp hơn những kết quả được báo cáo trong Brown et al. (2020). Lý do cho khoảng cách độ chính xác này là cách chúng tôi sử dụng mô hình OPT để tạo câu trả lời thách thức hơn so với setup trong Brown et al. (2020). Theo Mục 2.4 trong Brown et al. (2020), nó công thức hóa tác vụ PIQA như một tác vụ multi-choice QA trong đó câu trả lời được rút ra từ một tập candidate nhỏ và được định nghĩa trước (ví dụ, ["0", "1"]), bằng cách so sánh probability score chỉ trên các candidate token. Ngược lại, chúng tôi nghiêm túc đưa bài toán về open-ended generation, trong đó tập candidate không được biết. Trong trường hợp đó, việc tạo câu trả lời đúng có thể khó khăn hơn, vì mô hình có thể tạo ra câu trả lời hoàn toàn không liên quan và tăng cơ hội mắc lỗi.

A.5 TÁC ĐỘNG CỦA TẦN SUẤT TENSOR IMPORTANCE EVALUATION

Thiết kế của GreenTrainer, theo mặc định, đánh giá tầm quan trọng của các tensor và chọn tập hợp các tensor trainable dựa trên tầm quan trọng như vậy ở đầu mỗi training epoch. Sử dụng phương pháp kỹ thuật được mô tả trong Mục 3.1, tensor importance evaluation như vậy rất nhẹ, và kết quả thí nghiệm của chúng tôi cho thấy overhead của importance evaluation chỉ 0.2% trên dataset SciTLDR và 0.01% trên dataset DialogSum, đối với toàn bộ chi phí fine-tuning.

Mặt khác, trong một số trường hợp nhất định, tầm quan trọng tensor, được tính từ các thay đổi gradient mô hình, có thể thể hiện sự khác biệt không đáng kể trong một epoch. Trong những trường hợp này, thiết kế linh hoạt của GreenTrainer sẽ cho phép chúng ta tăng thích ứng tần suất tensor importance evaluation và tensor selection dựa trên DP tương ứng. Để chứng minh tác động của việc đánh giá tầm quan trọng tensor thường xuyên hơn như vậy, chúng tôi thực hiện thí nghiệm bổ sung sử dụng mô hình OPT-2.7B trên dataset WebQuestions và tác vụ generative QA, như thể hiện trong Bảng 10.

[THIS IS TABLE: Table 10 showing impact of tensor importance evaluation frequency]

Kết quả cho thấy rằng: (1) Tensor importance evaluation thường xuyên hơn chỉ mang lại cải thiện rất nhỏ về độ chính xác tác vụ. Xem xét tính ngẫu nhiên trong các trial training khác nhau, chúng tôi tin rằng cải thiện độ chính xác như vậy không đáng kể, và độ chính xác thậm chí có thể giảm 1% khi tần suất evaluation rất cao (mỗi 200 iteration). Chúng tôi tin rằng điều này là do tích lũy lỗi tensor importance evaluation và tensor selection, xuất phát từ xấp xỉ bậc nhất trong metric tầm quan trọng tensor và giải pháp xấp xỉ trong DP. Một lý do có thể khác là tầm quan trọng tensor được tính trên training dataset, và tensor importance evaluation quá thường xuyên có thể làm cho quá trình training overfit với training dataset. (2) Chi phí training tăng đều với tần suất tensor importance evaluation. Khi khoảng cách evaluation giảm từ 945 iteration xuống 200 iteration, thời gian training tăng 28%.

Tóm lại, việc thực hiện tensor importance evaluation thường xuyên hơn trong mỗi epoch mang lại ít cải thiện về độ chính xác tác vụ nhưng tăng đáng kể chi phí training. Chúng tôi tin rằng tầm quan trọng tensor được đánh giá một lần trong mỗi epoch sẽ đủ chính xác cho việc lựa chọn phù hợp các tensor trainable.

A.6 SỰ CẦN THIẾT CỦA DYNAMIC TENSOR SELECTION

Nếu LLM fine-tuning sử dụng một training dataset cố định, có thể việc sử dụng tensor selection cố định được quyết định ở giai đoạn ban đầu của training có thể không dẫn đến mất mát độ chính xác mô hình đáng kể, so với runtime tensor selection. Tuy nhiên, trong các tình huống LLM fine-tuning thực tế, giả định này thường không đúng do hai lý do sau. Đầu tiên, trong rất nhiều tình huống LLM fine-tuning, như online learning và model personalization, mô hình được retrain liên tục sử dụng dữ liệu online, được tạo liên tục tại runtime với các phân phối dữ liệu biến đổi. Các phân phối dữ liệu biến đổi như vậy chắc chắn sẽ dẫn đến tầm quan trọng khác nhau của các tensor qua quá trình training và do đó yêu cầu runtime tensor selection. Các tình huống online LLM fine-tuning như vậy gần đây trở nên phổ biến hơn, đặc biệt với khả năng triển khai LLM trên thiết bị di động cá nhân của người dùng như smartphone. Thứ hai, ngay cả đối với một training dataset cố định, cũng có thể tầm quan trọng của một số tensor có thể thay đổi khi training tiến triển. Trong những trường hợp này, dynamic tensor selection có thể cải thiện độ chính xác mô hình đã train. Để xác minh điều này, chúng tôi thực hiện thí nghiệm bổ sung sử dụng mô hình OPT-2.7B trên dataset WebQuestions và tác vụ generative QA. Như thể hiện trong Bảng 11, dynamic tensor selection có thể đóng góp không đáng kể vào việc cải thiện độ chính xác tác vụ, với sự tăng training cost không đáng kể.

[THIS IS TABLE: Table 11 showing different strategies of tensor selection]

Lưu ý rằng, cải thiện độ chính xác mô hình như vậy sẽ phụ thuộc vào dataset và mô hình cụ thể được sử dụng, nhưng các kết quả thí nghiệm trên đã chứng minh sự cần thiết của runtime tensor selection. Ngoài ra, kết quả thí nghiệm của chúng tôi cũng cho thấy rằng tensor importance evaluation và selection như vậy thực sự tạo ra overhead tính toán bổ sung rất ít.

--- TRANG 16 ---
