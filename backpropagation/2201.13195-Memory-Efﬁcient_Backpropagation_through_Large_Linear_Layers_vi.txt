# 2201.13195.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/backpropagation/2201.13195.pdf
# Kích thước file: 501563 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Lan Truyền Ngược Tiết Kiệm Bộ Nhớ thông qua Các Lớp Tuyến Tính Lớn
Daniel Bershatsky1Aleksandr Mikhalev1Alexandr Katrutsa1Julia Gusak1Daniil Merkulov1
Ivan Oseledets1 2

Tóm tắt
Trong các mạng nơ-ron hiện đại như Transformers, các lớp tuyến tính yêu cầu bộ nhớ đáng kể để lưu trữ các kích hoạt trong quá trình lan truyền ngược. Nghiên cứu này đề xuất một phương pháp giảm bộ nhớ để thực hiện lan truyền ngược thông qua các lớp tuyến tính. Vì gradient của các lớp tuyến tính được tính bằng phép nhân ma trận, chúng tôi xem xét các phương pháp nhân ma trận ngẫu nhiên và chứng minh rằng chúng yêu cầu ít bộ nhớ hơn với việc giảm vừa phải độ chính xác kiểm tra. Ngoài ra, chúng tôi điều tra phương sai của ước lượng gradient được tạo ra bởi phép nhân ma trận ngẫu nhiên. Chúng tôi so sánh phương sai này với phương sai đến từ ước lượng gradient dựa trên batch mẫu. Chúng tôi chứng minh lợi ích của phương pháp đề xuất trên việc tinh chỉnh mô hình RoBERTa được huấn luyện trước trên các tác vụ GLUE.

1. Giới thiệu
Những tiến bộ gần đây trong việc giải quyết các tác vụ NLP dựa trên kiến trúc Transformer (Vaswani et al., 2017), nơi tồn tại hai nút thắt bộ nhớ trong công thức gốc. Nút thắt đầu tiên là lớp attention và thứ hai là các lớp tuyến tính với ma trận tham số lớn. Các vấn đề của việc vận hành với lớp attention trong thực tế được giải quyết với sự hỗ trợ của việc làm thưa ma trận attention (Child et al., 2019; Zaheer et al., 2020). Một thách thức tương tự trong việc vận hành với các ma trận dày đặc lớn của tham số trong các lớp tuyến tính vẫn chưa được thảo luận.

Vì việc lan truyền gradient thông qua lớp tuyến tính về bản chất là tính toán tích ma trận nhân ma trận, chúng tôi xem xét các sơ đồ ngẫu nhiên hóa để xấp xỉ gradient mục tiêu và đồng thời yêu cầu ít bộ nhớ hơn. Có những kỹ thuật nổi tiếng để tính toán phép nhân ma trận xấp xỉ trong tài liệu (Drineas et al., 2006). Tuy nhiên, thông thường những kỹ thuật này được xem xét từ góc độ thời gian chạy hơn là tiêu thụ bộ nhớ. Bài báo (Adelman et al., 2021) đề xuất xấp xỉ quá trình lan truyền ngược thông qua các lớp tuyến tính bằng phép nhân ma trận ngẫu nhiên và tập trung vào thời gian huấn luyện và độ chính xác kiểm tra của mô hình cuối cùng. Tuy nhiên, phương pháp này có yêu cầu bộ nhớ giống như phương pháp tiêu chuẩn. Trong công trình hiện tại, chúng tôi đề xuất một lý luận thuật toán và lý thuyết về lớp tuyến tính tiết kiệm bộ nhớ dựa trên phép nhân ma trận ngẫu nhiên. Phương pháp đề xuất yêu cầu ít dữ liệu hơn đáng kể để lưu trữ cho việc tính toán gradient xấp xỉ của hàm mất mát đối với trọng số.

Chúng tôi xác nhận việc giảm bộ nhớ và phân tích khả năng suy giảm hội tụ có thể xảy ra bằng cách thực hiện thí nghiệm trên việc tinh chỉnh mô hình RoBERTa được huấn luyện trước (Liu et al., 2019) trên các tác vụ GLUE (Wang et al., 2018). Đánh giá thực nghiệm của phương pháp được xem xét chứng minh rằng việc giảm bộ nhớ không dẫn đến giảm đáng kể độ chính xác kiểm tra. Đối với một số bộ dữ liệu, chúng tôi đã quan sát thấy rằng ngay cả việc giảm 90% bộ nhớ cũng dẫn đến giảm vừa phải độ chính xác kiểm tra, và đôi khi nhiễu bổ sung thậm chí còn có lợi cho việc tổng quát hóa.

Những đóng góp chính của bài báo này như sau.
• Thuật toán lan truyền gradient ngẫu nhiên tiết kiệm bộ nhớ thông qua các lớp tuyến tính lớn.
• Phân tích lý thuyết về phương sai gradient được tạo ra bởi các tính toán ngẫu nhiên hỗ trợ.
• Phân tích thực nghiệm về sự đánh đổi giữa hiệu quả bộ nhớ và giảm độ chính xác kiểm tra cho một số bộ dữ liệu.
• Thí nghiệm được thực hiện trong việc tinh chỉnh mô hình RoBERTa được huấn luyện trước trên các tác vụ GLUE.

2. Phương pháp
Khối xây dựng chính của mạng nơ-ron vẫn là lớp tuyến tính. Nó đòi hỏi rất nhiều bộ nhớ và tài nguyên tính toán chủ yếu do phép nhân các ma trận có kích thước đáng kể. Trong phần này, chúng tôi trình bày cách phép nhân ma trận ngẫu nhiên giảm nhẹ những vấn đề này. Trước tiên, chúng tôi trình bày sự sửa đổi của chúng tôi đối với lớp kết nối đầy đủ. Sau đó, chúng tôi xem xét phương pháp phổ biến để huấn luyện mạng nơ-ron và cụ thể là ước lượng gradient ngẫu nhiên. Sau đó, chúng tôi thảo luận về mối tương tác của các nguồn phương sai khác nhau và cung cấp một số đảm bảo lý thuyết. Cuối cùng, chúng tôi đưa ra ước lượng về bộ nhớ và độ phức tạp số học.

2.1. Quá trình Lan truyền Ngược Ngẫu nhiên cho Lớp Tuyến tính
Một lớp tuyến tính được định nghĩa bởi trọng số W∈R^{N_out×N_in} và bias b∈R^{N_in}. Nó chỉ thực hiện phép biến đổi affine của một batch đầu vào X∈R^{B×N_in}:
X̂ = XW^T + 1_B b^T                                    (1)

Gradient của hàm mất mát đối với đầu vào lớp có thể được biểu diễn như sau
∂L/∂X = (∂L/∂X̂)W                                     (2)

và gradient của hàm mất mát đối với trọng số lớp là
∂L/∂W = (∂L/∂X̂)^T X, ∂L/∂b = (∂L/∂X̂)^T 1_B         (3)

Phân tích tiêu thụ bộ nhớ. Trong cài đặt tiêu chuẩn, tensor đầu vào X được lưu trữ hoàn toàn cho đến khi gradient đối với W được tính toán. Như trong (Adelman et al., 2021), chúng tôi đề xuất thay thế phép nhân ma trận trong (3) bằng phiên bản lấy mẫu ngẫu nhiên tương đương, nhưng với một khác biệt chính: mục tiêu của chúng tôi không phải là tăng tốc tính toán, mà là tiết kiệm bộ nhớ trong giai đoạn huấn luyện. Cụ thể (xem, ví dụ, (Drineas et al., 2006)) chúng ta có
∂L/∂W = E_S[(∂L/∂X̂)^T SS^T X] = E_S[(∂L/∂X̂)^T S X_proj]    (4)

trong đó X_proj = S^T X ∈ R^{B_proj×N_in} được tính toán trong quá trình truyền thuận và được lưu trữ thay vì X (xem Thuật toán 1). Để điều này có thể thực hiện được, ma trận S phải độc lập với Y = ∂L/∂X̂. Trong (Adelman et al., 2021), việc xây dựng S yêu cầu kiến thức về chuẩn của các hàng của Y, vì vậy chúng ta không thể tính trước XS.

Yêu cầu duy nhất cho ma trận ngẫu nhiên S∈R^{B×B_proj} là nó phải thỏa mãn
E[SS^T] = I_{B×B}

trong đó I_{B×B} là ma trận đơn vị B×B. Lưu ý, mặc dù S được cần thiết trong quá trình lan truyền ngược (nó phải giống như trong quá trình truyền thuận), nó không được lưu trữ một cách rõ ràng mà được tái tạo từ hạt giống ngẫu nhiên được lưu trữ. Chúng tôi sẽ gọi thuật toán nhân ma trận xấp xỉ được sử dụng trong (4) là Nhân Ma trận Ngẫu nhiên (RMM).

Các phân phối ngẫu nhiên khác nhau có thể được sử dụng để tạo ma trận S. Trong nghiên cứu này, chúng tôi xem xét ma trận ngẫu nhiên Gaussian,
S = (1/√B_proj)P                                      (5)

trong đó các phần tử của P là các biến ngẫu nhiên Gaussian i.i.d với trung bình bằng không và phương sai đơn vị. Chúng tôi cũng đã thử nghiệm các biến thể khác như ma trận Subsampled Orthonormal with Random Signs (SORS) (Iwen et al., 2021). Chúng đi kèm với tích ma trận-vector nhanh nhưng độ giảm chính xác cao hơn, vì vậy chúng tôi để dành cho các nghiên cứu tương lai và không báo cáo ở đây.

2.2. Ước lượng Gradient Ngẫu nhiên
Chúng ta có một tính toán ngẫu nhiên của gradient; điều này cần chính xác đến mức nào? Trong các tác vụ tiêu chuẩn, phép xấp xỉ nên xấp xỉ mục tiêu thực sự chính xác, tức là với độ chính xác tương đối cao. Lỗi nhân ma trận ngẫu nhiên giảm như O(B_proj^{-0.5}) (các ước lượng chính xác sẽ được mô tả trong Phần 2.3), vì vậy có vẻ như đây không phải là ý tưởng tốt. Tuy nhiên, trong khuôn khổ của gradient descent ngẫu nhiên (SGD), chúng ta đã có một ước lượng nhiễu của gradient được tạo ra bởi việc lấy mẫu của bộ dữ liệu, tức là phép xấp xỉ này có một số phương sai. Do đó, việc yêu cầu rằng phương sai được tạo ra bởi phép xấp xỉ ngẫu nhiên có cùng bậc với phương sai được tạo ra bởi ước lượng ngẫu nhiên của gradient là tự nhiên. Hơn nữa, phương sai tổng cao hơn của ước lượng gradient không nhất thiết có nghĩa là sự hội tụ của tối ưu hóa tổng thể có thể tệ hơn, vì nhiễu có thể có lợi. Để ước lượng hiệu ứng của RMM lên phương sai, chúng ta cần có một ước lượng nhất định về phương sai của ước lượng gradient.

Giả sử chúng ta có bài toán tối ưu hóa sau đây dưới dạng tối thiểu hóa trung bình mẫu hữu hạn:
f(θ) = (1/N)∑_{i=1}^N f_i(θ) → min_{θ∈R^p}                (6)

Phương pháp thông thường để giải quyết bài toán như vậy bao gồm các phương pháp bậc nhất ngẫu nhiên, trong đó thay vì tính toán gradient đầy đủ ∇f(θ) = (1/N)∑_{i=1}^N ∇f_i(θ), người ta có thể sử dụng phép xấp xỉ ngẫu nhiên của vector này
g(θ) = (1/n)∑_{j=1}^n ∇f_{i_j}(θ) → min_{θ∈R^p}              (7)

trong đó I = {i_1,...,i_j,...,i_n} được lấy mẫu đồng nhất từ tập chỉ số gốc {1,...,N}. Số n là kích thước batch. Để thuận tiện, chúng ta có thể xử lý tính ngẫu nhiên này bằng cách xem xét vector gradient ngẫu nhiên g(θ) = g như sau.

g = (1/n)∑_{i=1}^N ∇f_i(θ)ξ_i

trong đó ξ_i = {1, nếu i ∈ I; 0, ngược lại}                    (8)

Ước lượng trong (8) có thể được xem như một trung bình thực nghiệm của biến ngẫu nhiên vector. Do đó, chúng ta cũng có thể xây dựng một ước lượng thực nghiệm của phương sai của biến ngẫu nhiên này, và sử dụng nó như một hướng dẫn cho phương sai của mô hình RMM. Chúng tôi sẽ thực hiện điều này cụ thể cho lớp tuyến tính, vì trong trường hợp này có thể thu được các công thức rất đơn giản và trực quan.

2.3. Phương sai của Ước lượng Gradient Ngẫu nhiên
Với nền tảng được đưa ra trong Phần 2.2, chúng ta có thể thảo luận đóng góp lý thuyết chính của chúng tôi. Người ta có thể theo dõi các suy dẫn chi tiết trong Phụ lục A. Quan sát đầu tiên mà chúng tôi thực hiện là gradient chính xác được tính toán cho một batch nhất định có thể được xem như một ước lượng trung bình thực nghiệm của một biến ngẫu nhiên, tức là nó có một lượng nhiễu nhất định. Phép xấp xỉ ngẫu nhiên đưa thêm nhiễu vào bức tranh, có thể nhỏ hơn nhiễu từ kích thước mẫu hữu hạn (trong trường hợp này chúng ta mong đợi sự hội tụ vẫn giữ nguyên) hoặc lớn hơn. Trong trường hợp sau, hiệu ứng của nhiễu bổ sung đôi khi có thể đóng vai trò của bộ điều chỉnh.

Lý thuyết về sự hội tụ và tổng quát hóa của SGD đang phát triển nhanh chóng, xem ví dụ (Keskar et al., 2019; Jastrzebski et al., 2017; Hoffer et al., 2017; Cheng et al., 2020; Li et al., 2021). Trong một số cài đặt, việc tổng quát hóa thậm chí có thể được cải thiện bằng cách tiêm thêm nhiễu (Hoffer et al., 2017; Cheng et al., 2020; Li et al., 2021).

Lợi ích của nhiễu trong SGD được hiểu khá rõ, tuy nhiên chúng tôi không biết về bất kỳ ước lượng thực tế nào của nhiễu này. Bổ đề sau đây cho thấy cách nó có thể được thực hiện bằng cách sử dụng một ước lượng thống kê rất tiêu chuẩn của phương sai.

Bổ đề 2.1 (Phương sai hậu nghiệm của SGD). Cho X∈R^{B×N} và Y∈R^{B×M} là đầu vào của lớp tuyến tính trong quá trình truyền thuận và đầu vào của nó trong quá trình lan truyền ngược (B ở đây là kích thước batch). Khi đó, chúng ta có thể ước lượng phương sai của nhiễu được tạo ra bởi việc lựa chọn ngẫu nhiên các mẫu như
D²_SGD(X,Y) = (B/(B-1))∑_{k=1}^B ||x_k||²||y_k||² - ||X^T Y||²_F/(B-1)  (9)

trong đó x_k = X^T e_k, y_k = Y^T e_k, k = 1,...,B, tức là x_k và y_k lần lượt là các cột của X^T và Y^T.

Ý nghĩa của ước lượng (9) rất đơn giản. Trong số hạng đầu tiên chúng ta có chuẩn của các gradient từng mẫu, và số hạng cuối cùng là chuẩn được tỷ lệ của gradient cho toàn bộ batch. Nếu số hạng sau nhỏ, nhưng chuẩn của các gradient từng mẫu lớn, thì chúng ta có phương sai cao của SGD (xem Hình 2). Một cách trực quan, Bổ đề 2.1 có thể được xem như một tổng quát hóa của phương sai mẫu trong ước lượng gradient ngẫu nhiên (để có suy dẫn đầy đủ xem Phụ lục A.1).

Bổ đề 2.2 (Phương sai tiên nghiệm của RMM). Cho X∈R^{B×N} và Y∈R^{B×M}, thì phương sai của phép nhân ma trận ngẫu nhiên thông qua ma trận S∈R^{B×B_proj} với các phần tử i.i.d tuân theo phân phối chuẩn N(0,B_proj^{-0.5}) được định nghĩa như
D²(X,Y) = E_S[||X^T SS^T Y - X^T Y||²_F]                    (10)

có thể được đánh giá như sau
D²_RMM(X,Y) = (||X||²_F ||Y||²_F - ||X^T Y||²_F)/B_proj      (11)

Chứng minh có thể tìm thấy trong Phụ lục A.2.

Định lý 2.3 (Cận trên của phương sai). Trong điều kiện của Bổ đề 2.1 và Bổ đề 2.2, phương sai trong mẫu D_SGD và phương sai D_RMM được tạo ra bởi lấy mẫu phụ ngẫu nhiên được liên kết với bất đẳng thức sau
(B_proj/(B-1)) × (D²_RMM(X,Y))/(D²_SGD(X,Y)) ≤ α + 1       (12)

trong đó
α = ||X^T Y||²_F/(||X||²_F ||Y||²_F) ∈ [0,1]                (13)

Chứng minh có thể tìm thấy trong Phụ lục A.3. Đáng chú ý rằng tham số α thực sự có thể bằng không trong trường hợp X^T Y = 0 dẫn đến biến thiên không bị chặn. Hãy giả sử ví dụ đơn giản sau với B = 2:

X = [1 0; ε 0], Y = [1 0; -ε 0], X^T Y = 0                   (14)

với một số tham số ε > 0. Vậy, các biến thiên ước lượng là:
(B-1)D²_SGD(X,Y) = 4                                        (15)

và
B_proj D²_RMM(X,Y) = 2 + ε² + ε²                           (16)

Do đó, tỷ số của chúng có thể là bất kỳ số lớn tùy ý nào, và phương sai "mẫu" của SGD có thể nhỏ hơn nhiều so với phương sai được đưa ra bởi RMM. Tuy nhiên, trong thực tế, chúng tôi không quan sát thấy những trường hợp như vậy. Một lời giải thích tự nhiên là đối với các minima của hàm mất mát tổng quát hóa tốt, chuẩn của Y cũng sẽ nhỏ (chuẩn của X có thể được làm cho bị chặn bằng, ví dụ, batch normalization) vì gradient đối với hầu như mọi mẫu sẽ nhỏ.

2.4. Dấu chân Bộ nhớ và Độ phức tạp Số học
Phép nhân ma trận tổng quát (matmul) AB tốn O(nml) phép toán dấu phẩy động cho A∈R^{n×m} và B∈R^{m×l}. Không cần thêm không gian nào ngoài việc lưu trữ ma trận kết quả. Ước lượng chi phí được tóm tắt trong Bảng 1.

2.4.1. YÊU CẦU BỘ NHỚ
Cài đặt mặc định của lớp kết nối đầy đủ lưu trữ tensor đầu vào X, được sử dụng cho cả quá trình truyền thuận và lan truyền ngược. Nó yêu cầu O(BN_in) bộ nhớ bổ sung ngoài tensor trọng số. Sự sửa đổi của chúng tôi đối với lớp kết nối đầy đủ lưu trữ tensor đầu vào nén thay thế, yêu cầu O(B_proj N_in) bộ nhớ. Xin lưu ý rằng các ma trận ngẫu nhiên được tái tạo khi cần thiết từ một bộ tạo số ngẫu nhiên giả nhất định, tức là hạt giống ngẫu nhiên với tiêu thụ bộ nhớ O(1). Nói cách khác, phương pháp của chúng tôi giảm dấu chân bộ nhớ bằng 1/(B/B_proj - 1) lần cho các tensor đầu vào của tất cả các lớp tuyến tính.

2.4.2. ĐỘ PHỨC TẠP TÍNH TOÁN
Cho B biểu thị chiều batch và N_in và N_out lần lượt là kích thước đầu vào và đầu ra của một lớp tuyến tính. Ngoài ra, cho tỷ lệ nén α ∈ (0,1] và chiều batch nén B_proj = αB.

Theo Thuật toán 1, quá trình truyền thuận của lớp tuyến tính yêu cầu O(BN_in N_out) phép toán để tính toán đầu ra X̂ và O(BB_proj N_in) phép toán để thu được đầu vào nén X_proj cho quá trình lan truyền ngược.

Độ phức tạp số học của quá trình lan truyền ngược cơ sở, dựa trên đầu vào không nén X, là O(BN_in N_out) phép toán dấu phẩy động. Mặt khác, phương pháp của chúng tôi cho quá trình lan truyền ngược yêu cầu nhân gradient đầu ra với ma trận ngẫu nhiên tái tạo S và ước lượng gradient đối với trọng số dẫn đến O(BB_proj N_out + B_proj N_in N_out) phép toán.

Tổng độ phức tạp tiệm cận của một chu kỳ truyền thuận-ngược đơn là O(BN_in N_out) cho cài đặt cơ sở và O(B_proj N_out(B + N_in)) cho phương pháp của chúng tôi.

Giả sử N = N_in ≈ N_out thì độ phức tạp tổng thể trở thành O(BN²) và O(αBN(αB + N)), tương ứng. Trong các tình huống thực tế của các mô hình Transformer lớn với N ≫ B, chúng ta kết luận thành O(αB²N) phép toán trong đó tỷ lệ nén được hợp nhất thành một hệ số nhân không đổi. Sự sửa đổi matmul ngẫu nhiên của lớp tuyến tính có tiệm cận tệ hơn về mặt kích thước batch nhưng việc chọn tỷ lệ nén α đủ nhỏ làm giảm thời gian tính toán đáng kể và làm cho phương pháp của chúng tôi hấp dẫn trong thực tế.

3. Thí nghiệm
Trong phần này, chúng tôi đánh giá hiệu suất của sự sửa đổi đề xuất của các lớp tuyến tính bằng cách so sánh nó với cài đặt mặc định. Tất cả các phép nhân ma trận ngẫu nhiên được triển khai với PyTorch (Paszke et al., 2019) trong Python (xem tài liệu bổ sung cho cài đặt tham khảo). Chúng tôi sử dụng mô hình RoBERTa-base được huấn luyện trước từ HuggingFace's Transformers (Wolf et al., 2020). Việc tinh chỉnh mô hình trên các tác vụ GLUE được thực hiện trong cài đặt GPU đơn với NVIDIA Tesla V100 SXM2 16 GB. Chúng tôi sử dụng cùng cài đặt huấn luyện và siêu tham số mô hình cho mô hình RoBERTa có trong Fairseq (Ott et al., 2019).

Chúng tôi viết lại cài đặt lớp kết nối đầy đủ trong PyTorch với sự sửa đổi cho quá trình truyền thuận và lưu cache quá trình lan truyền ngược đầu vào nén S^T X và trạng thái PRNG G giữa các lần truyền. Cài đặt của chúng tôi cho phép kiểm soát tỷ lệ nén α (chiều của phép chiếu ngẫu nhiên tỷ lệ với kích thước batch) hoặc cố định số chiều B_proj. Trong cả hai chế độ, chúng tôi có thể kẹp B_proj trong một khoảng mong muốn. Để rõ ràng, chúng tôi tuân thủ việc chỉ định α thay vì cố định giá trị chính xác của B_proj để nén đồng nhất trên tất cả các lớp trong mô hình.

3.1. Hiệu suất trên Benchmark GLUE
Trong những thí nghiệm này, chúng tôi đo sự suy giảm hiệu suất trong việc tinh chỉnh mô hình RoBERTa cơ sở trên benchmark GLUE tùy thuộc vào tỷ lệ nén (xem Bảng 2). Lớp dày đặc ngẫu nhiên thể hiện sự suy giảm vừa phải của các chỉ số đánh giá. Nén trong 5-10 lần dẫn đến giảm không đáng kể hiệu suất cho hầu hết các tác vụ GLUE.

3.2. Hiệu quả Bộ nhớ
Mặc dù lớp kết nối đầy đủ là phổ biến cho kiến trúc Transformer và nó chiếm phần lớn tổng mức sử dụng bộ nhớ trong thời gian huấn luyện, vẫn có những người tiêu thụ bộ nhớ đáng kể khác. Vì vậy, chúng tôi đo giảm dấu chân bộ nhớ thực tế liên quan đến tỷ lệ nén (xem Bảng 3). Trong cài đặt thí nghiệm này, chúng tôi huấn luyện RoBERTa trên các tác vụ GLUE với tỷ lệ nén thay đổi và kích thước batch B. Quan sát quan trọng là nén trong 5-10 lần cắt giảm bộ nhớ runtime tổng thể 10-20%.

Ngoài ra, chúng tôi thực hiện thí nghiệm để xác nhận việc sử dụng bộ nhớ trong cài đặt của chúng tôi với việc thay đổi kích thước batch B. Theo Phần 2.4.1, chúng tôi chỉ tiết kiệm O(B_proj N_in) bộ nhớ cho quá trình lan truyền ngược. Vì vậy, việc mở rộng gần như tuyến tính của việc sử dụng bộ nhớ cho các tỷ lệ nén khác nhau khi kích thước batch tăng xác nhận tính đúng đắn của cài đặt (xem Hình 3).

3.3. Ước lượng Phương sai Thực nghiệm
Trong phần này, chúng tôi khám phá hành vi ước lượng phương sai một cách thực nghiệm (xem Phần 2.3). Chúng tôi sử dụng cài đặt thí nghiệm chung nơi các lớp tuyến tính với quá trình lan truyền ngược ngẫu nhiên được sử dụng. Chúng tôi chọn một lớp kết nối đầy đủ và ước lượng các biến thiên (9) và (11) trong quá trình huấn luyện (xem Hình 4).

Hành vi của các ước lượng phương sai thú vị về mặt riêng: phương sai tăng chậm theo số bước, trong khi như chúng ta đã thấy, chuẩn của gradient (số hạng X^T Y) rất nhỏ. Điều này có nghĩa là toàn bộ động lực học được điều khiển bởi các số hạng nhiễu, tức là các tham số trải qua một quá trình khuếch tán. Hành vi tương đối của D²_SGD và D²_RMM cũng tương tự và hội tụ đến một hằng số nhất định. Đối với các lớp khác, bức tranh rất tương tự. Người ta có thể tìm thấy các thí nghiệm bổ sung trong Phụ lục B.1.

3.4. Đường cong Học tập
Trong phần phụ này, chúng tôi nghiên cứu thực nghiệm ảnh hưởng của lớp kết nối đầy đủ ngẫu nhiên đối với việc huấn luyện. Cụ thể, chúng tôi khám phá hành vi của mất mát cross-entropy trên tập huấn luyện và tập đánh giá tùy thuộc vào tỷ lệ nén α. Chúng tôi thấy rằng đường cong mất mát thay đổi một cách mượt mà khi tỷ lệ nén giảm (xem Hình 5). Giảm tỷ lệ nén dẫn đến tăng mất mát huấn luyện và làm phẳng mất mát đánh giá. Tuy nhiên, điểm overfitting gần như giống nhau.

3.5. So sánh các MatMul Ngẫu nhiên
Để giảm chi phí tính toán, chúng tôi xem xét nhiều cài đặt nhân ma trận ngẫu nhiên khác nhau. Trong số các cài đặt matmul mà chúng tôi xem xét, có việc lấy mẫu ma trận ngẫu nhiên S từ phân phối Gaussian hoặc Rademacher và áp dụng Biến đổi Fourier rời rạc (DFT) hoặc Biến đổi Cosine rời rạc (DCT). So với các phương pháp khác, DCT và DFT có lợi thế tính toán lý thuyết do cấu trúc đều đặn của chúng. Các matmul dựa trên DFT và DCT cho phép thực hiện phép nhân với ma trận ngẫu nhiên S trong O(BN log B) phép toán thay vì O(B²N). Tất cả các lựa chọn thay thế yêu cầu cùng không gian bộ nhớ.

Trong trường hợp matmul ngẫu nhiên Gaussian, chúng tôi lấy mẫu các phần tử i.i.d của ma trận S từ phân phối chuẩn N(0, B_proj^{-0.5}). Điều tương tự đúng cho trường hợp phân phối Rademacher có hàm khối lượng xác suất sau:
P(n) = 1/2, n = ±1

Sự khác biệt duy nhất là chúng ta phải ép buộc điều kiện không thiên vị E[SS^T] = I với chuẩn hóa thích hợp.

Chúng tôi thấy rằng các biến thể matmul khác nhau thể hiện sự suy giảm hiệu suất nhất quán ở mức độ vừa phải khi tỷ lệ nén giảm (xem Bảng 4). Tuy nhiên, thời gian huấn luyện thay đổi giữa các lựa chọn thay thế cho thấy rằng cài đặt ngây thơ ở mức cao trong PyTorch không đủ tốt và cần có các tối ưu hóa mức thấp.

3.6. Thời gian Tính toán
Để làm cho nghiên cứu thực nghiệm toàn diện nhất có thể, chúng tôi điều tra hiệu quả tính toán một cách thực nghiệm mặc dù đây không phải là mục tiêu chính của chúng tôi. Chúng tôi sử dụng cài đặt thí nghiệm tiêu chuẩn và đo số mẫu được xử lý mỗi giây (throughput) trong thời gian huấn luyện (xem Hình 6). Như đã đề cập trong Phần 2.4.2, việc ngẫu nhiên hóa lớp tuyến tính có độ phức tạp tính toán tệ hơn về mặt kích thước batch B. Tuy nhiên, có tỷ lệ nén đủ nhỏ sao cho lớp dày đặc ngẫu nhiên trở nên hiệu quả về mặt tính toán. Hơn nữa, chúng tôi thấy một cách thực nghiệm rằng việc ngẫu nhiên hóa của chúng tôi nhanh hơn nếu α ≤ 0.1.

4. Các Nghiên cứu Liên quan
Một nghiên cứu gần với chúng tôi là (Adelman et al., 2021), nơi một biến thể khác của phép nhân ngẫu nhiên được sử dụng để tăng tốc quá trình lan truyền ngược. Mục tiêu của chúng tôi là giảm bộ nhớ và chúng tôi cũng cung cấp phân tích lý thuyết về phương sai, điều này làm sáng tỏ hiệu ứng của các tính toán gradient xấp xỉ. Trong (Oktay et al., 2020), khái niệm vi phân tự động ngẫu nhiên đã được đề xuất.

Phép nhân ma trận ngẫu nhiên có lịch sử lâu dài, bắt đầu từ (Freivalds, 1977) nơi xác minh xác suất của phép nhân ma trận đã được đề xuất. Trong (Drineas et al., 2006), thuật toán ngẫu nhiên dựa trên điểm số đã được đề xuất và phân tích. Các thuật toán cải thiện cho phép nhân ma trận đã được đề xuất trong (Boutsidis & Gittens, 2013), nơi các thuật toán nhanh khác nhau đã được nghiên cứu cho ma trận lấy mẫu dựa trên kết quả của (Tropp, 2011) cho các biến đổi trực giao được lấy mẫu phụ.

Một hướng nghiên cứu khác tập trung vào các thuật toán khác để xấp xỉ phép nhân ma trận, để nêu một vài bài báo liên quan (Pagh, 2013) nơi các hàm băm đã được sử dụng và trong (Blalock & Guttag, 2021) các hàm băm được học từ dữ liệu. Đánh giá xuất sắc cho đại số tuyến tính xác suất có thể được tìm thấy trong (Martinsson & Tropp, 2020).

5. Kết luận và Nghiên cứu Tương lai
Chúng tôi đề xuất một sự thay thế drop-in cho lớp tuyến tính trong mạng nơ-ron sâu với phép toán ngược ngẫu nhiên giảm lượng bộ nhớ cần thiết để lưu trữ trong quá trình lan truyền ngược. Thuật toán dựa trên phép nhân ma trận ngẫu nhiên. Chúng tôi cung cấp các cận lý thuyết về phương sai bổ sung được đưa ra bởi việc ngẫu nhiên hóa so với nhiễu vốn có trong SGD, cung cấp các cận cho nhiễu này và các ước lượng có thể tính toán. Trong việc tinh chỉnh mô hình dựa trên Transformer trên các tác vụ GLUE khác nhau, chúng tôi cho thấy rằng chúng ta có được sự giảm bộ nhớ đỉnh trong khi duy trì độ chính xác của mô hình.

Có một số hướng mà chúng tôi muốn nghiên cứu trong nghiên cứu tương lai. Trước tiên, chúng tôi muốn có được các cài đặt ổn định và mạnh mẽ của phép nhân ma trận ngẫu nhiên với các ma trận S cho phép tích ma trận-vector nhanh. Subsampled Orthogonal with Random Signs dường như là một lựa chọn tốt, nhưng phương sai của các ước lượng như vậy trong thí nghiệm của chúng tôi khá lớn, vì vậy B_proj phải được chọn lớn hơn. Do đó, chúng tôi muốn nghiên cứu các lựa chọn khác. Đối với các mô hình dựa trên Transformer, số hàng trong X thực sự là tích của kích thước batch và độ dài chuỗi, tức là nó khá lớn; tuy nhiên, chúng tôi không sử dụng cấu trúc này. Một lựa chọn là sử dụng các ma trận lấy mẫu tensor-product để giảm độ phức tạp của phép nhân với S.

Hướng thứ hai là nghiên cứu sâu hơn về ước lượng phương sai và mối liên hệ của nó với lịch trình tỷ lệ học. Với một ước lượng tốt về phương sai, người ta có thể cố gắng biện minh lý thuyết cho một lịch trình tỷ lệ học cụ thể để duy trì một mức độ nhiễu nhất định trong huấn luyện.

6. Lời cảm ơn
Nghiên cứu được hỗ trợ bởi Trung tâm Phân tích dưới Chính phủ LB Nga (thỏa thuận trợ cấp 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).
