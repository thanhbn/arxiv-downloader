# 2309.13192.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/backpropagation/2309.13192.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 798167 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
HÆ¯á»šNG Tá»šI AI XANH TRONG TINH CHá»ˆNH CÃC MÃ” HÃŒNH NGÃ”N NGá»® Lá»šN THÃ”NG QUA LAN TRUYá»€N NGÆ¯á»¢C THÃCH NGHI
Kai Huangâ€ , Hanyun YinÂ§, Heng Huangâ€¡& Wei Gaoâ€ 
University of Pittsburghâ€ , University of Maryland, College Parkâ€¡
University of Science and Technology of ChinaÂ§
k.huang@pitt.edu ,ykissgoodbye@gmail.com ,heng@umd.edu ,weigao@pitt.edu
TÃ“M Táº®T
Tinh chá»‰nh lÃ  Ä‘iá»u cáº§n thiáº¿t Ä‘á»ƒ thÃ­ch á»©ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i cÃ¡c á»©ng dá»¥ng downstream. Vá»›i sá»± phá»• biáº¿n ngÃ y cÃ ng tÄƒng cá»§a cÃ¡c á»©ng dá»¥ng há»— trá»£ LLM, tinh chá»‰nh Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n má»™t cÃ¡ch tÃ­ch cá»±c trÃªn toÃ n tháº¿ giá»›i, phÃ¡t sinh má»™t lÆ°á»£ng chi phÃ­ tÃ­nh toÃ¡n khá»•ng lá»“ tÆ°Æ¡ng á»©ng vá»›i dáº¥u chÃ¢n carbon lá»›n vÃ  tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng. Giáº£m thiá»ƒu tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng nhÆ° váº­y cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n viá»‡c giáº£m FLOPs tinh chá»‰nh. CÃ¡c phÆ°Æ¡ng Ã¡n tinh chá»‰nh hiá»‡n cÃ³ táº­p trung vÃ o viá»‡c tiáº¿t kiá»‡m bá»™ nhá»› hoáº·c giáº£m chi phÃ­ tÃ­nh toÃ¡n cáº­p nháº­t trá»ng sá»‘, nhÆ°ng khÃ´ng thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m FLOPs Ä‘á»§ do há» bá» qua chi phÃ­ huáº¥n luyá»‡n trong lan truyá»n ngÆ°á»£c. Äá»ƒ giáº£i quyáº¿t háº¡n cháº¿ nÃ y, trong bÃ i bÃ¡o nÃ y chÃºng tÃ´i trÃ¬nh bÃ y GreenTrainer, má»™t ká»¹ thuáº­t má»›i nháº±m tá»‘i thiá»ƒu hÃ³a FLOPs cá»§a tinh chá»‰nh LLM thÃ´ng qua lan truyá»n ngÆ°á»£c thÃ­ch nghi, chá»n lá»c má»™t cÃ¡ch thÃ­ch nghi táº­p há»£p cÃ¡c tensor LLM phÃ¹ há»£p nháº¥t cho tinh chá»‰nh dá»±a trÃªn táº§m quan trá»ng vÃ  chi phÃ­ lan truyá»n ngÆ°á»£c cá»§a chÃºng trong huáº¥n luyá»‡n. Káº¿t quáº£ thÃ­ nghiá»‡m cho tháº¥y GreenTrainer cÃ³ thá»ƒ tiáº¿t kiá»‡m lÃªn Ä‘áº¿n 64% FLOPs huáº¥n luyá»‡n so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§, mÃ  khÃ´ng cÃ³ báº¥t ká»³ máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng chÃº Ã½ nÃ o. So vá»›i cÃ¡c phÆ°Æ¡ng Ã¡n hiá»‡n cÃ³ nhÆ° Prefix Tuning vÃ  LoRA, GreenTrainer cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh lÃªn Ä‘áº¿n 4%, vá»›i viá»‡c giáº£m FLOPs tÆ°Æ¡ng Ä‘Æ°Æ¡ng.

1 GIá»šI THIá»†U
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° cÃ¡c cÃ´ng cá»¥ ná»n táº£ng trong AI sinh táº¡o. Äá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c á»©ng dá»¥ng downstream, má»™t LLM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cáº§n Ä‘Æ°á»£c tinh chá»‰nh sá»­ dá»¥ng dá»¯ liá»‡u á»©ng dá»¥ng cá»¥ thá»ƒ (Devlin et al., 2018). Má»™t cÃ¡ch trá»±c quan, tinh chá»‰nh Ã­t tá»‘n kÃ©m tÃ­nh toÃ¡n hÆ¡n huáº¥n luyá»‡n trÆ°á»›c do lÆ°á»£ng dá»¯ liá»‡u huáº¥n luyá»‡n nhá» hÆ¡n, nhÆ°ng nÃ³ cÃ³ thá»ƒ dáº«n Ä‘áº¿n tiÃªu thá»¥ nÄƒng lÆ°á»£ng vÃ  dáº¥u chÃ¢n carbon cao Ä‘Ã¡ng ká»ƒ khi Ä‘Æ°á»£c thá»±c hiá»‡n tÃ­ch cá»±c trÃªn toÃ n tháº¿ giá»›i. ÄÆ°á»£c há»— trá»£ bá»Ÿi viá»‡c dÃ¢n chá»§ hÃ³a cÃ¡c LLM mÃ£ nguá»“n má»Ÿ (Candel et al., 2023) vÃ  cÃ¡c API thuáº­n tiá»‡n Ä‘á»ƒ váº­n hÃ nh nhá»¯ng LLM nÃ y (Ott et al., 2019; Wolf et al., 2019), ngay cáº£ cÃ¡c cÃ¡ nhÃ¢n khÃ´ng chuyÃªn cÅ©ng cÃ³ thá»ƒ dá»… dÃ ng tinh chá»‰nh LLMs Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t mÃ´ hÃ¬nh hoáº·c cÃ¡ nhÃ¢n hÃ³a (Scialom et al., 2022; Wang and Gao, 2023). VÃ­ dá»¥, khi má»™t mÃ´ hÃ¬nh LLaMA-13B (Touvron et al., 2023) Ä‘Æ°á»£c tinh chá»‰nh bá»Ÿi 10k ngÆ°á»i dÃ¹ng sá»­ dá»¥ng GPU A100-80GB, viá»‡c tinh chá»‰nh nhÆ° váº­y tiÃªu thá»¥ 6.9 Ã— nhiá»u giá» GPU hÆ¡n viá»‡c huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh GPT-3 (Brown et al., 2020) vá»›i 175B tham sá»‘. LÆ°á»£ng nÄƒng lÆ°á»£ng tiÃªu thá»¥ bá»Ÿi viá»‡c tinh chá»‰nh nhÆ° váº­y cÃ³ thá»ƒ so sÃ¡nh vá»›i lÆ°á»£ng tiÃªu thá»¥ cá»§a má»™t sá»‘ quá»‘c gia kÃ©m phÃ¡t triá»ƒn, vÃ  lÆ°á»£ng dáº¥u chÃ¢n carbon tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i 1000 Ã— lÆ°á»£ng Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t chuyáº¿n bay New York-San Francisco (aii, 2023).

Giáº£m thiá»ƒu tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng nhÆ° váº­y hÆ°á»›ng tá»›i AI Xanh cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n viá»‡c giáº£m sá»‘ lÆ°á»£ng phÃ©p toÃ¡n dáº¥u pháº©y Ä‘á»™ng (FLOPs) cá»§a tinh chá»‰nh, Ä‘áº¡i diá»‡n cho lÆ°á»£ng phÃ©p toÃ¡n tÃ­nh toÃ¡n vÃ  do Ä‘Ã³ tiÃªu thá»¥ nÄƒng lÆ°á»£ng trong huáº¥n luyá»‡n (Schwartz et al., 2020; Huang et al., 2023a). Tuy nhiÃªn, háº§u háº¿t cÃ¡c ká»¹ thuáº­t hiá»‡n cÃ³ Ä‘á»ƒ tá»‘i Æ°u hÃ³a tinh chá»‰nh LLM bá»‹ háº¡n cháº¿ trong viá»‡c giáº£m tiÃªu thá»¥ bá»™ nhá»› hÆ¡n lÃ  FLOPs (Malladi et al., 2023; Liao et al., 2023). Má»™t sá»‘ phÆ°Æ¡ng phÃ¡p khÃ¡c giáº£m FLOPs báº±ng cÃ¡ch chá»‰ tinh chá»‰nh má»™t sá»‘ loáº¡i tham sá»‘ mÃ´ hÃ¬nh nháº¥t Ä‘á»‹nh nhÆ° bias (Zaken et al., 2021), LayerNorm vÃ  trá»ng sá»‘ lá»›p Ä‘áº§u ra (Lu et al., 2021), nhÆ°ng chÃºng lÃ m suy giáº£m kháº£ nÄƒng biá»ƒu Ä‘áº¡t cá»§a mÃ´ hÃ¬nh vÃ  chá»‰ Ã¡p dá»¥ng Ä‘Æ°á»£c cho cÃ¡c tÃ¡c vá»¥ há»c táº­p khÃ´ng sinh táº¡o Ä‘Æ¡n giáº£n. Thay vÃ o Ä‘Ã³, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ Ä‘á» xuáº¥t giá»¯ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh gá»‘c Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng nhÆ°ng tiÃªm cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n bá»• sung vÃ o Ä‘áº§u vÃ o (Lester et al., 2021; Liu et al., 2022) hoáº·c cÃ¡c lá»›p ná»™i bá»™ (Li and Liang, 2021; Hu et al., 2023; Huang et al., 2023b). CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn LoRA gáº§n Ä‘Ã¢y (Hu et al., 2021; Zhang et al., 2023) cÃ²n giáº£m thÃªm chi phÃ­ tÃ­nh toÃ¡n cáº­p nháº­t trá»ng sá»‘ cho cÃ¡c tham sá»‘ Ä‘Æ°á»£c tiÃªm nÃ y thÃ´ng qua xáº¥p xá»‰ rank tháº¥p. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ tá»‘i thiá»ƒu hÃ³a máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c tÃ¡c vá»¥ sinh táº¡o. Tuy nhiÃªn, chÃºng váº«n cáº§n tÃ­nh toÃ¡n gradient kÃ­ch hoáº¡t qua toÃ n bá»™ mÃ´ hÃ¬nh vÃ  do Ä‘Ã³ viá»‡c giáº£m FLOPs cá»§a chÃºng bá»‹ háº¡n cháº¿, bá»Ÿi vÃ¬ cÃ¡c phÃ©p tÃ­nh cáº­p nháº­t trá»ng sá»‘ chá»‰ chiáº¿m 25%-33% tá»•ng FLOPs huáº¥n luyá»‡n.

1arXiv:2309.13192v2  [cs.LG]  29 Feb 2024

--- TRANG 2 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
BÃªn cáº¡nh viá»‡c tÃ­nh toÃ¡n cáº­p nháº­t trá»ng sá»‘, FLOPs trong huáº¥n luyá»‡n cÅ©ng Ä‘Æ°á»£c táº¡o ra trong i) lan truyá»n thuáº­n vÃ  ii) lan truyá»n ngÆ°á»£c cá»§a gradient kÃ­ch hoáº¡t. VÃ¬ lan truyá»n thuáº­n hoÃ n chá»‰nh lÃ  cáº§n thiáº¿t Ä‘á»ƒ tÃ­nh toÃ¡n máº¥t mÃ¡t huáº¥n luyá»‡n, chÃºng tÃ´i hÃ¬nh dung ráº±ng chÃ¬a khÃ³a Ä‘á»ƒ giáº£m FLOPs hÆ¡n ná»¯a lÃ  tÃ­nh Ä‘áº¿n chi phÃ­ lan truyá»n ngÆ°á»£c cá»§a gradient kÃ­ch hoáº¡t, chiáº¿m >33% tá»•ng FLOPs huáº¥n luyá»‡n, vÃ  cÃ³ chá»n lá»c chá»‰ liÃªn quan Ä‘áº¿n cÃ¡c cáº¥u trÃºc mÃ´ hÃ¬nh phÃ¹ há»£p nháº¥t trong lan truyá»n ngÆ°á»£c. Tuy nhiÃªn, thÃ¡ch thá»©c chÃ­nh lÃ  huáº¥n luyá»‡n cÃ³ chá»n lá»c cÃ³ thá»ƒ gÃ¢y ra máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. ChÃºng tÃ´i tá»‘i thiá»ƒu hÃ³a máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c báº±ng cÃ¡ch thÃ­ch á»©ng viá»‡c lá»±a chá»n nhÆ° váº­y trong lan truyá»n ngÆ°á»£c vá»›i má»™t má»¥c tiÃªu linh hoáº¡t vá» giáº£m FLOPs, Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi dáº¥u chÃ¢n carbon trong cung cáº¥p nÄƒng lÆ°á»£ng. VÃ­ dá»¥, khi dáº¥u chÃ¢n carbon nhÆ° váº­y tháº¥p do viá»‡c chÃ¨n nÄƒng lÆ°á»£ng tÃ¡i táº¡o, sá»­ dá»¥ng má»¥c tiÃªu giáº£m FLOPs tháº¥p hÆ¡n cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n nhiá»u cáº¥u trÃºc mÃ´ hÃ¬nh hÆ¡n trong huáº¥n luyá»‡n vÃ  giá»¯ láº¡i Ä‘á»™ chÃ­nh xÃ¡c huáº¥n luyá»‡n. Dáº¥u chÃ¢n carbon cao, thay vÃ o Ä‘Ã³, dáº«n Ä‘áº¿n má»¥c tiÃªu giáº£m FLOPs cao hÆ¡n Ä‘á»ƒ cháº¥p nháº­n AI Xanh tá»‘t hÆ¡n.

epoch 1 epoch 3backprop trained frozen model params
epoch 1 epoch 2 epoch 3
HÃ¬nh 1: GreenTrainer thÃ­ch nghi lá»±a chá»n pháº§n phÃ¹ há»£p nháº¥t cá»§a mÃ´ hÃ¬nh LLM Ä‘á»ƒ tinh chá»‰nh

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y GreenTrainer, má»™t ká»¹ thuáº­t má»›i thá»±c hiá»‡n lan truyá»n ngÆ°á»£c thÃ­ch nghi cho tinh chá»‰nh LLM hiá»‡u quáº£ vá»›i máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c tá»‘i thiá»ƒu. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 1, vá»›i má»™t má»¥c tiÃªu giáº£m FLOPs, GreenTrainer thÃ­ch nghi lá»±a chá»n táº­p há»£p cÃ¡c tensor máº¡ng nÆ¡-ron (NN) cÃ³ thá»ƒ huáº¥n luyá»‡n trong má»—i epoch, dá»±a trÃªn Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a cÃ¡c tensor trong huáº¥n luyá»‡n. Viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng nhÆ° váº­y lÃ  khÃ³ khÄƒn vÃ¬ cÃ¡c tensor NN khÃ´ng liÃªn káº¿t trá»±c tiáº¿p vá»›i cÃ¡c biáº¿n dá»¯ liá»‡u Ä‘áº§u vÃ o hoáº·c cÃ¡c Ä‘áº·c trÆ°ng trung gian, vÃ  háº§u háº¿t cÃ¡c ká»¹ thuáº­t phÃ¢n bá»• (Sundararajan et al., 2017; Hesse et al., 2021) Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng Ä‘áº·c trÆ°ng khÃ´ng Ã¡p dá»¥ng Ä‘Æ°á»£c. CÃ¡c chá»‰ sá»‘ táº§m quan trá»ng phá»• biáº¿n, bao gá»“m SNIP (Lee et al., 2018) vÃ  Fisher (Liu et al., 2021), chá»§ yáº¿u Ä‘Æ°á»£c sá»­ dá»¥ng trong cáº¯t tá»‰a NN Ä‘á»ƒ Ä‘á»‹nh lÆ°á»£ng táº§m quan trá»ng cá»§a trá»ng sá»‘ mÃ´ hÃ¬nh táº¡i giÃ¡ trá»‹ hiá»‡n táº¡i cá»§a chÃºng, nhÆ°ng chÃºng khÃ´ng thá»ƒ Ä‘á»‹nh lÆ°á»£ng táº§m quan trá»ng cá»§a viá»‡c cáº­p nháº­t trá»ng sá»‘ trÃªn má»™t tensor Ä‘á»ƒ giáº£m máº¥t mÃ¡t huáº¥n luyá»‡n. CÃ¡c chá»‰ sá»‘ cá»• Ä‘iá»ƒn dá»±a trÃªn Ä‘Ã³ng gÃ³p Ä‘á»™ chÃ­nh xÃ¡c chÃ­nh xÃ¡c (Lin et al., 2022), Ä‘á»™ lá»›n cá»§a cÃ¡c cáº­p nháº­t trá»ng sá»‘ (Li et al., 2016), hoáº·c nhiá»…u loáº¡n ngáº«u nhiÃªn (Breiman, 2001), máº·t khÃ¡c, hoáº·c khÃ´ng chÃ­nh xÃ¡c hoáº·c tá»‘n kÃ©m tÃ­nh toÃ¡n cho LLMs. Thay vÃ o Ä‘Ã³, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ã¡p dá»¥ng má»™t lÃ½ luáº­n tÆ°Æ¡ng tá»± vá»›i cÃ¡c chá»‰ sá»‘ phÃ¢n bá»• vÃ  cáº¯t tá»‰a hiá»‡n cÃ³, vÃ  Ä‘á»‹nh lÆ°á»£ng Ä‘Ã³ng gÃ³p cá»§a má»—i cáº­p nháº­t tensor vÃ o máº¥t mÃ¡t huáº¥n luyá»‡n thÃ´ng qua khai triá»ƒn Taylor báº­c nháº¥t trÃªn máº¥t mÃ¡t huáº¥n luyá»‡n. Báº±ng cÃ¡ch nÃ y, chÃºng tÃ´i Ä‘áº£m báº£o ráº±ng cÃ¡c tensor Ä‘Æ°á»£c chá»n cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p tá»‘i Ä‘a vÃ o viá»‡c giáº£m máº¥t mÃ¡t huáº¥n luyá»‡n.

Má»™t thÃ¡ch thá»©c khÃ¡c lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ chÃ­nh xÃ¡c láº­p há»“ sÆ¡ FLOPs huáº¥n luyá»‡n. Do sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c tensor, tá»•ng FLOPs cá»§a chÃºng trong huáº¥n luyá»‡n khÃ´ng báº±ng tá»•ng FLOPs riÃªng láº» cá»§a chÃºng. Sá»± phá»¥ thuá»™c láº«n nhau nhÆ° váº­y Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi cÃ¡c Ä‘áº·c tÃ­nh lan truyá»n ngÆ°á»£c cá»§a cÃ¡c toÃ¡n tá»­ NN trong má»—i tensor, nhÆ°ng cÃ¡c mÃ´ hÃ¬nh FLOPs hiá»‡n cÃ³ khÃ´ng thá»ƒ liÃªn káº¿t cÃ¡c toÃ¡n tá»­ NN vá»›i cÃ¡c tensor dá»±a trÃªn luá»“ng tÃ­nh toÃ¡n cá»§a lan truyá»n ngÆ°á»£c. Má»™t sá»‘ cÃ´ng trÃ¬nh hiá»‡n cÃ³ (Kwon et al., 2023) chá»‰ káº¿t há»£p FLOPs thuáº­n theo lá»›p vÃ o viá»‡c lá»±a chá»n tensor, nhÆ°ng bá» qua sá»± phá»¥ thuá»™c tÃ­nh toÃ¡n giá»¯a cÃ¡c lá»›p trong lan truyá»n ngÆ°á»£c. Äá»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y, chÃºng tÃ´i mÃ´ hÃ¬nh nghiÃªm ngáº·t cÃ¡c phá»¥ thuá»™c chÃ©o tensor trong viá»‡c láº­p há»“ sÆ¡ FLOPs lan truyá»n ngÆ°á»£c cá»§a chÃºng. Dá»±a trÃªn mÃ´ hÃ¬nh nÃ y, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t thuáº­t toÃ¡n quy hoáº¡ch Ä‘á»™ng (DP) Ä‘á»ƒ tÃ¬m viá»‡c lá»±a chá»n tensor gáº§n tá»‘i Æ°u tá»« má»™t sá»‘ lÆ°á»£ng kháº£ nÄƒng theo cáº¥p sá»‘ nhÃ¢n (vÃ­ dá»¥, 2515 cho 515 tensor trong mÃ´ hÃ¬nh OPT-2.7B (Zhang et al., 2022)). Do Ä‘Ã³, GreenTrainer cÃ³ thá»ƒ Ä‘áº£m báº£o ráº±ng má»¥c tiÃªu giáº£m FLOPs nháº¥t Ä‘á»‹nh cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡p á»©ng trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ GreenTrainer vá»›i ba LLM mÃ£ nguá»“n má»Ÿ, cá»¥ thá»ƒ lÃ  OPT (Zhang et al., 2022), BLOOMZ (Muennighoff et al., 2022) vÃ  FLAN-T5 (Chung et al., 2022), trÃªn cÃ¡c bá»™ dá»¯ liá»‡u sinh vÄƒn báº£n bao gá»“m SciTLDR (Cachola et al., 2020) vÃ  DialogSum (Chen et al., 2021). Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y GreenTrainer cÃ³ thá»ƒ tiáº¿t kiá»‡m lÃªn Ä‘áº¿n 64% FLOPs huáº¥n luyá»‡n so vá»›i tinh chá»‰nh LLM Ä‘áº§y Ä‘á»§, mÃ  khÃ´ng cÃ³ báº¥t ká»³ máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng chÃº Ã½ nÃ o. Trong má»™t sá»‘ trÆ°á»ng há»£p, GreenTrainer tháº­m chÃ­ cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§, báº±ng cÃ¡ch loáº¡i bá» sá»± dÆ° thá»«a mÃ´ hÃ¬nh vÃ  overfitting. So vá»›i cÃ¡c ká»¹ thuáº­t hiá»‡n cÃ³ nhÆ° Prefix Tuning (Li and Liang, 2021) vÃ  LoRA (Hu et al., 2021), GreenTrainer cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh 4% vá»›i cÃ¹ng lÆ°á»£ng giáº£m FLOPs, vÃ  cÅ©ng cung cáº¥p cho ngÆ°á»i dÃ¹ng sá»± linh hoáº¡t Ä‘á»ƒ cÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c huáº¥n luyá»‡n vÃ  chi phÃ­ tÃ¹y thuá»™c vÃ o nhu cáº§u cá»§a AI Xanh.

2 KIáº¾N THá»¨C Ná»€N Táº¢NG & Äá»˜NG Lá»°C

2.1 KIáº¾N TRÃšC TRANSFORMER CHO SINH VÄ‚N Báº¢N
CÃ¡c LLM hiá»‡n táº¡i Ä‘Æ°á»£c xáº¿p chá»“ng bá»Ÿi cÃ¡c khá»‘i transformer (Vaswani et al., 2017), má»—i khá»‘i chá»©a má»™t lá»›p Multi-Head Attention (MHA), LayerNorms (Ba et al., 2016), vÃ  má»™t Feed-Forward Network (FFN).

2

--- TRANG 3 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
Cho má»™t chuá»—i Ä‘áº§u vÃ o XâˆˆRnÃ—d vá»›i n token, MHA chiáº¿u cÃ¡c token vÃ o khÃ´ng gian (Q, K, V) h láº§n, sá»­ dá»¥ng h bá»™ chiáº¿u cÃ³ thá»ƒ huáº¥n luyá»‡n (W(i)
Q, W(i)
K, W(i)
V)i=1,...,h. Má»—i phÃ©p chiáº¿u fi:
RnÃ—dâ†’RnÃ—d
h Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  Qi, Ki, Vi=XW(i)
Q, XW(i)
K, XW(i)
V. Äáº§u ra (Qi, Ki, Vi) sau Ä‘Ã³ thá»±c hiá»‡n cÆ¡ cháº¿ attention Ä‘á»ƒ táº¡o ra Oi báº±ng cÃ¡ch trá»ng sá»‘ Vi vá»›i Ä‘iá»ƒm attention giá»¯a Qi vÃ  Ki. Äáº§u ra cuá»‘i cÃ¹ng cá»§a MHA Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng cÃ¡ch ná»‘i tá»«ng Oi, theo sau bá»Ÿi má»™t phÃ©p chiáº¿u tuyáº¿n tÃ­nh g:RnÃ—dâ†’RnÃ—d vá»›i má»™t bá»™ chiáº¿u cÃ³ thá»ƒ huáº¥n luyá»‡n Wo:

Oi= Softmax
QiKâŠ¤
i/p
d/h
Vi, MHA out= Concat( O1, O2, ..., O h)Wo. (1)

Äá»ƒ cáº£i thiá»‡n hiá»‡u quáº£ huáº¥n luyá»‡n, LLMs Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p teacher-forcing (Lamb et al., 2016) Ä‘á»ƒ sinh ra toÃ n bá»™ chuá»—i token Ä‘áº§u ra trong má»™t láº§n truyá»n thuáº­n duy nháº¥t. Cá»¥ thá»ƒ, cÃ¡c máº·t náº¡ nhÃ¢n quáº£ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o Ä‘iá»ƒm attention cá»§a MHA, Ä‘á»ƒ má»—i token Ä‘áº§u ra cÃ³ thá»ƒ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n tá»« cÃ¡c token nhÃ£n á»Ÿ cÃ¡c vá»‹ trÃ­ trÆ°á»›c Ä‘Ã³. Vá»›i ká»¹ thuáº­t nÃ y, khi Ä‘Æ°á»£c tinh chá»‰nh, LLMs cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n theo cÃ¡ch tiÃªu chuáº©n nhÆ° báº¥t ká»³ mÃ´ hÃ¬nh feed-forward nÃ o.

2.2 NHU Cáº¦U Vá»€ LAN TRUYá»€N NGÆ¯á»¢C THÃCH NGHI
Khi Ä‘Æ°á»£c tinh chá»‰nh cho má»™t tÃ¡c vá»¥ downstream, LLMs thÆ°á»ng Ä‘Æ°á»£c tham sá»‘ hÃ³a quÃ¡ má»©c, vÃ¬ chá»‰ má»™t pháº§n kiáº¿n thá»©c tháº¿ giá»›i mÃ  chÃºng há»c Ä‘Æ°á»£c tá»« huáº¥n luyá»‡n trÆ°á»›c lÃ  há»¯u Ã­ch cho tÃ¡c vá»¥ má»¥c tiÃªu. Trong nhá»¯ng trÆ°á»ng há»£p nÃ y, chá»‰ liÃªn quan má»™t sá»‘ cáº¥u trÃºc con cá»§a mÃ´ hÃ¬nh vÃ o tinh chá»‰nh cÃ³ thá»ƒ cÃ³ Ã­t tÃ¡c Ä‘á»™ng Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh, nhÆ°ng giáº£m Ä‘Ã¡ng ká»ƒ lÆ°á»£ng tÃ­nh toÃ¡n.

[THIS IS TABLE:
Báº£ng 1 showing training different substructures of OPT-2.7B and FLAN-T5-3B LLMs on DialogSum dataset with columns for Trainable substructure, OPT-2.7B metrics (FLOPs and Acc.), and FLAN-T5-3B metrics (FLOPs and Acc.)]

CÃ¡c nghiÃªn cá»©u hiá»‡n cÃ³ Ä‘Ã£ thá»­ nghiá»‡m vá»›i cÃ¡c lá»±a chá»n cá»‘ Ä‘á»‹nh cá»§a má»™t sá»‘ thÃ nh pháº§n NN, nhÆ° 2 lá»›p cuá»‘i, tiá»n tá»‘ bá»™ giáº£i mÃ£ (Li and Liang, 2021), vÃ  cÃ¡c bá»™ chiáº¿u tuyáº¿n tÃ­nh (WQ, WV)(Hu et al., 2021), trong tinh chá»‰nh. Tuy nhiÃªn, do cÃ¡c phá»¥ thuá»™c láº«n nhau cá»§a cÃ¡c tham sá»‘ NN (Jin et al., 2020), nhá»¯ng lá»±a chá»n cá»‘ Ä‘á»‹nh nhÆ° váº­y sáº½ lÃ m suy giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1, chá»‰ tinh chá»‰nh 2 lá»›p cuá»‘i hoáº·c tiá»n tá»‘ bá»™ giáº£i mÃ£ dáº«n Ä‘áº¿n giáº£m Ä‘á»™ chÃ­nh xÃ¡c lÃªn Ä‘áº¿n 10%. LÃ½ do lÃ  cÃ¡c cáº¥u trÃºc con NN gáº§n Ä‘Ã³ cÃ³ phá»¥ thuá»™c láº«n nhau vá»›i cÃ¡c lá»±a chá»n cá»‘ Ä‘á»‹nh bá»‹ loáº¡i trá»« khá»i tinh chá»‰nh, vÃ  do Ä‘Ã³ trá»Ÿ nÃªn khÃ´ng nháº¥t quÃ¡n vá»›i nhá»¯ng cáº¥u trÃºc con Ä‘Æ°á»£c chá»n. TÄƒng máº­t Ä‘á»™ lá»±a chá»n, nhÆ° bao gá»“m táº¥t cáº£ cÃ¡c bá»™ chiáº¿u tuyáº¿n tÃ­nh (WQ, WV), cÃ³ thá»ƒ giáº£m thiá»ƒu máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh, nhÆ°ng chá»‰ cÃ³ thá»ƒ tiáº¿t kiá»‡m tá»‘i Ä‘a 33% FLOPs do lan truyá»n ngÆ°á»£c gradient kÃ­ch hoáº¡t qua cÃ¡c khá»‘i transformer. CÃ¡c phÆ°Æ¡ng phÃ¡p ngÃ¢y thÆ¡ cá»§a lá»±a chá»n Ä‘á»™ng, nhÆ° má»Ÿ rá»™ng pháº§n cÃ³ thá»ƒ huáº¥n luyá»‡n tá»« lá»›p cuá»‘i, cÃ³ háº¡n cháº¿ tÆ°Æ¡ng tá»±.

Sá»± thiáº¿u sÃ³t cá»§a nhá»¯ng phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i thá»±c thi viá»‡c lá»±a chá»n cÃ¡c cáº¥u trÃºc con LLM linh hoáº¡t vÃ  thÃ­ch nghi hÆ¡n trong lan truyá»n ngÆ°á»£c. Trong GreenTrainer, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t chá»‰ sá»‘ táº§m quan trá»ng tensor káº¿t há»£p cÃ¡c phá»¥ thuá»™c tham sá»‘ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡ch tinh chá»‰nh má»—i tensor Ä‘Ã³ng gÃ³p vÃ o Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n táº¡i thá»i gian cháº¡y. Kiáº¿n thá»©c vá» táº§m quan trá»ng tensor nhÆ° váº­y, sau Ä‘Ã³, cho phÃ©p chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m FLOPs mong muá»‘n trong khi tá»‘i Ä‘a hÃ³a Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh.

Lá»›p 1
Lá»›p 2
Lá»›p 3
Lá»›p 4Dá»± Ä‘oÃ¡n Dá»¯ liá»‡u Ä‘áº§u vÃ o
NhÃ£n
thuáº­n
ngÆ°á»£cdy4 dy3 dy2
dw1 dw2 dw3 dw4y2 y1 y3 y4

HÃ¬nh 2: Lan truyá»n ngÆ°á»£c cá»§a má»™t NN dÃ y Ä‘áº·c 4 lá»›p

2.3 MÃ” HÃŒNH FLOPS Cá»¦A LAN TRUYá»€N NGÆ¯á»¢C
Thiáº¿t káº¿ cá»§a GreenTrainer dá»±a vÃ o viá»‡c tÃ­nh toÃ¡n chÃ­nh xÃ¡c FLOPs lan truyá»n ngÆ°á»£c cá»§a cÃ¡c cáº¥u trÃºc con mÃ´ hÃ¬nh Ä‘Æ°á»£c chá»n, cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ¡ch thÃ nh hai pháº§n sá»­ dá»¥ng quy táº¯c dÃ¢y chuyá»n. VÃ­ dá»¥, nhÆ°

3

--- TRANG 4 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2, khi huáº¥n luyá»‡n má»™t NN dÃ y Ä‘áº·c 4 lá»›p khÃ´ng cÃ³ bias, má»—i lá»›p tÃ­nh toÃ¡n i) dyi lÃ  gradient cá»§a máº¥t mÃ¡t L Ä‘á»‘i vá»›i kÃ­ch hoáº¡t yi, vÃ  ii) dwi lÃ  gradient máº¥t mÃ¡t Ä‘á»‘i vá»›i trá»ng sá»‘ Wi, sao cho

dyi=âˆ‚L
âˆ‚yi=âˆ‚L
âˆ‚yi+1WâŠ¤
i= dy i+1WâŠ¤
i, dwi=âˆ‚L
âˆ‚Wi=yâŠ¤
iâˆ‚L
âˆ‚yi+1=yâŠ¤
idyi+1, (2)

vÃ  lÆ°á»£ng FLOPs tÆ°Æ¡ng á»©ng Ä‘á»ƒ tÃ­nh toÃ¡n dyi vÃ  dwi lÃ  tdyi vÃ  tdwi, tÆ°Æ¡ng á»©ng.
(dyi,dwi) cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« (dyi+1,dwi+1). Cá»¥ thá»ƒ, ngay cáº£ khi má»™t lá»›p khÃ´ng Ä‘Æ°á»£c chá»n trong tinh chá»‰nh, nÃ³ váº«n cáº§n tÃ­nh toÃ¡n vÃ  truyá»n gradient lá»—i (dyi) Ä‘áº¿n cÃ¡c lá»›p downstream. Do Ä‘Ã³, lÆ°á»£ng tÃ­nh toÃ¡n trong lan truyá»n ngÆ°á»£c khÃ´ng chá»‰ phá»¥ thuá»™c vÃ o cÃ¡c lá»›p Ä‘Æ°á»£c chá»n, mÃ  cÃ²n phá»¥ thuá»™c vÃ o má»™t sá»‘ lá»›p khÃ´ng Ä‘Æ°á»£c chá»n. VÃ­ dá»¥, náº¿u chá»‰ Lá»›p 2 cÃ³ thá»ƒ huáº¥n luyá»‡n, tá»•ng FLOPs cho lan truyá»n ngÆ°á»£c sáº½ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi chi phÃ­ tÃ­nh toÃ¡n dw2, dy3 vÃ  dy4. Do tÃ­nh tá»•ng quÃ¡t cá»§a quy táº¯c dÃ¢y chuyá»n, lÃ½ luáº­n nhÆ° váº­y vá» tÃ­nh toÃ¡n FLOPs cÅ©ng Ã¡p dá»¥ng Ä‘Æ°á»£c cho cÃ¡c loáº¡i lá»›p NN khÃ¡c.

Dá»±a trÃªn lÃ½ luáº­n nÃ y, chÃºng tÃ´i cÃ³ thá»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh FLOPs cho cÃ¡c cáº¥u trÃºc con LLM. MÃ´ hÃ¬nh cáº¥p lá»›p thÃ´ sÆ¡ vÃ  cÃ³ thá»ƒ dáº«n Ä‘áº¿n viá»‡c lá»±a chá»n tensor khÃ´ng chÃ­nh xÃ¡c. Má»™t sá»‘ tham sá»‘ quan trá»ng cÃ³ thá»ƒ khÃ´ng Ä‘Æ°á»£c chá»n do nhá»¯ng tham sá»‘ khÃ´ng quan trá»ng khÃ¡c trong cÃ¹ng lá»›p. Trong GreenTrainer, chÃºng tÃ´i sá»­ dá»¥ng Ä‘á»™ chi tiáº¿t cáº¥p tensor cho viá»‡c lá»±a chá»n nhÆ° váº­y, cÃ³ thá»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t bá»Ÿi cÃ¡c thÆ° viá»‡n NN tensorized (vÃ­ dá»¥, TensorFlow (Abadi, 2016) vÃ  PyTorch (Paszke et al., 2019)). Lá»±a chá»n cáº¥p trá»ng sá»‘, máº·c dÃ¹ chi tiáº¿t hÆ¡n, quÃ¡ tá»‘n kÃ©m tÃ­nh toÃ¡n do yÃªu cáº§u láº­p chá»‰ má»¥c chi tiáº¿t.

3 PHÆ¯Æ NG PHÃP
Äá»ƒ giáº£m FLOPs cá»§a tinh chá»‰nh LLM, má»™t cÃ´ng thá»©c váº¥n Ä‘á» trá»±c quan lÃ  tá»‘i thiá»ƒu hÃ³a FLOPs trong khi Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh mong muá»‘n. Tuy nhiÃªn, khÃ³ xÃ¡c Ä‘á»‹nh má»™t má»¥c tiÃªu Ä‘á»™ chÃ­nh xÃ¡c phÃ¹ há»£p trÆ°á»›c, vÃ¬ má»™t sá»‘ má»¥c tiÃªu Ä‘á»™ chÃ­nh xÃ¡c cÃ³ thá»ƒ yÃªu cáº§u huáº¥n luyá»‡n ráº¥t tÃ­ch cá»±c vÃ  Ä‘á»™ chÃ­nh xÃ¡c mÃ  chÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c vá»›i ngÃ¢n sÃ¡ch FLOPs cá»§a chÃºng ta khÃ´ng thá»ƒ Ä‘Æ°á»£c Æ°á»›c tÃ­nh trÆ°á»›c khi huáº¥n luyá»‡n. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i tá»‘i Ä‘a hÃ³a viá»‡c giáº£m máº¥t mÃ¡t huáº¥n luyá»‡n trong khi Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m FLOPs mong muá»‘n:

max âˆ† loss(m)s.t.Tselective (m)â‰¤ÏTfull, (3)

trong Ä‘Ã³ m lÃ  má»™t vector nhá»‹ phÃ¢n cáº§n giáº£i quyáº¿t cho viá»‡c lá»±a chá»n tensor. m tham sá»‘ hÃ³a cáº£ viá»‡c giáº£m máº¥t mÃ¡t (âˆ†loss) vÃ  FLOPs trÃªn má»—i batch cá»§a huáº¥n luyá»‡n (Tselective), vÃ  Tselective bá»‹ háº¡n cháº¿ trong má»™t tá»· lá»‡ do ngÆ°á»i dÃ¹ng chá»‰ Ä‘á»‹nh (Ï) cá»§a FLOPs cá»§a tinh chá»‰nh toÃ n bá»™ mÃ´ hÃ¬nh (Tfull). VÃ­ dá»¥, Ï= 0.5 cÃ³ nghÄ©a lÃ  FLOPs cá»§a tinh chá»‰nh pháº£i nhiá»u nháº¥t 50% cá»§a viá»‡c tinh chá»‰nh toÃ n bá»™ mÃ´ hÃ¬nh.

Trong thá»±c táº¿, giÃ¡ trá»‹ cá»§a Ï cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c hoáº·c Ä‘iá»u chá»‰nh táº¡i thá»i gian cháº¡y trong báº¥t ká»³ giai Ä‘oáº¡n nÃ o cá»§a huáº¥n luyá»‡n.

Äá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Ã³ng gÃ³p cá»§a má»—i tensor trong tinh chá»‰nh, chÃºng tÃ´i mÃ´ hÃ¬nh âˆ†loss(m) nhÆ° táº§m quan trá»ng tá»•ng há»£p cá»§a cÃ¡c tensor Ä‘Æ°á»£c chá»n, vÃ  tÃ­nh toÃ¡n FLOPs phÃ¡t sinh bá»Ÿi cÃ¡c tensor Ä‘Æ°á»£c chá»n sá»­ dá»¥ng mÃ´ hÃ¬nh FLOPs cá»§a lan truyá»n ngÆ°á»£c trong Pháº§n 2.3. Vá»›i mÃ´ hÃ¬nh nÃ y, Eq. (3) cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i lÃ :

max âˆ† loss(m)s.t.Tfp+mÂ·tdw+Ïƒ(m)Â·tdyâ‰¤ÏTfull, (4)

trong Ä‘Ã³ Tfp biá»ƒu thá»‹ FLOPs trÃªn má»—i batch cá»§a láº§n truyá»n thuáº­n, vÃ  má»—i cáº·p biáº¿n trong (tdy,tdw) Ä‘áº¡i diá»‡n cho FLOPs cá»§a viá»‡c tÃ­nh toÃ¡n (dy,dw) cho tensor tÆ°Æ¡ng á»©ng, tÆ°Æ¡ng á»©ng. Cho má»™t bá»™ chá»n nhá»‹ phÃ¢n m, Ïƒ(m) káº¿t há»£p táº¥t cáº£ cÃ¡c tensor dá»c theo láº§n truyá»n ngÆ°á»£c Ä‘Ã³ng gÃ³p vÃ o FLOPs cá»§a tinh chá»‰nh, báº±ng cÃ¡ch tham gia vÃ o viá»‡c truyá»n gradient lá»—i (dy). VÃ­ dá»¥, náº¿u m= [0,0,1,0,1,0,0], táº¥t cáº£ cÃ¡c tensor á»Ÿ cÃ¡c lá»›p sÃ¢u hÆ¡n so vá»›i cÃ¡c tensor Ä‘Æ°á»£c chá»n Ä‘á»u tham gia vÃ o viá»‡c truyá»n gradient lá»—i, vÃ  do Ä‘Ã³ Ïƒ(m) = [0,0,1,1,1,1,1].

Äá»ƒ cÄƒn cá»© cÃ´ng thá»©c nÃ y vÃ  giáº£i quyáº¿t m, GreenTrainer bao gá»“m ba thÃ nh pháº§n chÃ­nh: (i) Láº­p há»“ sÆ¡ FLOPs Tensor, tÃ­nh toÃ¡n FLOPs cá»§a táº¥t cáº£ cÃ¡c tensor NN (tá»©c lÃ , tdy vÃ  tdw) trÆ°á»›c khi huáº¥n luyá»‡n; (ii) ÄÃ¡nh giÃ¡ Táº§m quan trá»ng Tensor, Ä‘á»‹nh lÆ°á»£ng Ä‘Ã³ng gÃ³p cá»§a viá»‡c cáº­p nháº­t má»—i tensor NN vÃ o cháº¥t lÆ°á»£ng huáº¥n luyá»‡n táº¡i thá»i gian cháº¡y; (iii) Bá»™ chá»n Tensor, cÄƒn cá»© váº¥n Ä‘á» lá»±a chá»n tensor sá»­ dá»¥ng FLOPs vÃ  táº§m quan trá»ng cá»§a cÃ¡c tensor, vÃ  cung cáº¥p cÃ¡c giáº£i phÃ¡p thÃ´ng qua quy hoáº¡ch Ä‘á»™ng táº¡i thá»i gian cháº¡y.

3.1 Láº¬P Há»’ SÆ  FLOPS TENSOR
CÃ¡c bá»™ láº­p há»“ sÆ¡ NN tiÃªu chuáº©n, nhÆ° Torch Profiler (Paszke et al., 2019), cÃ³ thá»ƒ Ä‘o FLOPs thá»±c thi cá»§a cÃ¡c toÃ¡n tá»­ NN riÃªng láº» nhÆ° nhÃ¢n ma tráº­n vÃ  tÃ­ch cháº­p. Tuy nhiÃªn, nÃ³ khÃ´ng thá»ƒ Ä‘Æ°á»£c liÃªn káº¿t trá»±c tiáº¿p vá»›i cÃ¡c tensor NN tham gia vÃ o nhá»¯ng phÃ©p toÃ¡n nÃ y. Khi má»™t táº­p há»£p tensor Ä‘Æ°á»£c huáº¥n luyá»‡n, FLOPs huáº¥n luyá»‡n cá»§a lan truyá»n ngÆ°á»£c khÃ´ng báº±ng tá»•ng FLOPs riÃªng láº» cá»§a cÃ¡c tensor.

Äá»ƒ giáº£i quyáº¿t háº¡n cháº¿ nÃ y, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i bao gá»“m hai bÆ°á»›c. Äáº§u tiÃªn, chÃºng tÃ´i chuyá»ƒn Ä‘á»•i cáº¥u trÃºc NN dá»±a trÃªn lá»›p cá»§a LLMs thÃ nh má»™t Ä‘á»“ thá»‹ tÃ­nh toÃ¡n cáº¥p tensor, giá»¯ láº¡i thá»© tá»± thá»±c thi cá»§a táº¥t cáº£

4

--- TRANG 5 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

Input Embed. 
TensorProjector Q Projector K Projector V Bias BiasOutput Embed. 
Tensor
0,0Tensors in MHA layer in 1st block
Tensor -level 
Graph:
Tensor FLOPs 
ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘,ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘:ğ‘‡ğ‘‡,ğ‘‡ğ‘‡ 0,ğ‘‡ğ‘‡ğ‘‡ ğ‘‡ğ‘‡,ğ‘‡ğ‘‡ 0,ğ‘‡ğ‘‡ğ‘‡ ğ‘‡ğ‘‡+ğ‘‡ğ‘‡att,ğ‘‡ğ‘‡ âˆ‘ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘,ğ‘‡ğ‘‡ğ‘‡ğ‘‡variable 
assign.Related 
Backprop Ops:matmul_1
matmul_2matmul_3
matmul_4add_1 add_2matmul_5
matmul_6matmul _{N-1}
matmul _{N}
Match & Agg. 
FLOPs
â¹â¶
â·
â¸Backprop direction

HÃ¬nh 3: Má»™t quy trÃ¬nh máº«u cá»§a láº­p há»“ sÆ¡ FLOPs tensor

cÃ¡c tensor tham gia vÃ o huáº¥n luyá»‡n. Sau Ä‘Ã³, chÃºng tÃ´i trÃ­ch xuáº¥t cÃ¡c toÃ¡n tá»­ lan truyá»n ngÆ°á»£c liÃªn quan cá»§a má»—i tensor, vÃ  dáº«n xuáº¥t FLOPs cá»§a má»—i tensor i trong lan truyá»n ngÆ°á»£c (tdyi vÃ  tdwi) báº±ng cÃ¡ch khá»›p vÃ  tá»•ng há»£p FLOPs cá»§a nhá»¯ng toÃ¡n tá»­ NN nÃ y. VÃ­ dá»¥ trong HÃ¬nh 3, viá»‡c huáº¥n luyá»‡n má»—i bá»™ chiáº¿u tuyáº¿n tÃ­nh (Q, K vÃ  V) trong má»™t lá»›p MHA pháº£i Ä‘Æ°á»£c thá»±c thi sau khi huáº¥n luyá»‡n tensor bias tÆ°Æ¡ng á»©ng cá»§a nÃ³. Viá»‡c huáº¥n luyá»‡n má»—i bá»™ chiáº¿u tuyáº¿n tÃ­nh, sau Ä‘Ã³, sáº½ liÃªn quan Ä‘áº¿n hai toÃ¡n tá»­ nhÃ¢n ma tráº­n, cÃ³ FLOPs trong lan truyá»n ngÆ°á»£c sáº½ Ä‘Æ°á»£c tá»•ng há»£p. ChÃºng tÃ´i phÃ¢n loáº¡i nhá»¯ng quy táº¯c khá»›p vÃ  tá»•ng há»£p nhÆ° váº­y theo loáº¡i lá»›p LLM nÆ¡i cÃ¡c tensor Ä‘Æ°á»£c Ä‘áº·t, nhÆ° Ä‘Æ°á»£c mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y. Má»™t vÃ­ dá»¥ cá»¥ thá»ƒ vá» viá»‡c láº­p há»“ sÆ¡ FLOPs tensor nhÆ° váº­y trÃªn mÃ´ hÃ¬nh OPT-2.7B Ä‘Æ°á»£c cung cáº¥p trong Phá»¥ lá»¥c A.3.

CÃ¡c lá»›p embedding Ä‘áº§u vÃ o & Ä‘áº§u ra. Lá»›p embedding Ä‘áº§u vÃ o chá»©a má»™t tensor embedding cÃ³ thá»ƒ huáº¥n luyá»‡n Ã¡nh xáº¡ má»—i token thÃ´ thÃ nh má»™t biá»ƒu diá»…n dÃ y Ä‘áº·c. Cho gradient kÃ­ch hoáº¡t dyi+1 tá»« cÃ¡c lá»›p upstream, viá»‡c dáº«n xuáº¥t cáº­p nháº­t cá»§a tensor nÃ y chá»‰ liÃªn quan Ä‘áº¿n gÃ¡n biáº¿n, vÃ  chÃºng ta cÃ³ thá»ƒ an toÃ n xem tdwiâ‰ˆ0 cho báº¥t ká»³ tensor i nÃ o. Náº¿u má»™t token thÃ´ Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n vector thá»© k trong tensor embedding trong láº§n truyá»n thuáº­n, thÃ¬ trong lan truyá»n ngÆ°á»£c, dyi+1 tá»« upstream sáº½ chá»‰ Ä‘Æ°á»£c gÃ¡n cho hÃ ng thá»© k cá»§a dwi, sao cho dwi[s] = dyi+1 náº¿u s=k, ngÆ°á»£c láº¡i dwi[s] = 0. VÃ¬ lá»›p Ä‘áº§u vÃ o khÃ´ng truyá»n gradient kÃ­ch hoáº¡t, chÃºng ta cÅ©ng cÃ³ thá»ƒ káº¿t luáº­n ráº±ng tdy cá»§a nÃ³ lÃ  0.

NgÆ°á»£c láº¡i, lá»›p embedding Ä‘áº§u ra chiáº¿u má»—i token trá»Ÿ láº¡i khÃ´ng gian xÃ¡c suáº¥t. Má»™t cÃ¡ch trá»±c quan, (tdy, tdw) cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c dáº«n xuáº¥t theo cÃ¹ng cÃ¡ch nhÆ° chÃºng ta Ä‘Ã£ lÃ m cho lá»›p dÃ y Ä‘áº·c trong Eq. (2). Tuy nhiÃªn, trong háº§u háº¿t LLMs, lá»›p embedding Ä‘áº§u ra chia sáº» cÃ¹ng tensor cÃ³ thá»ƒ huáº¥n luyá»‡n vá»›i lá»›p embedding Ä‘áº§u vÃ o. Äiá»u nÃ y ngá»¥ Ã½ ráº±ng náº¿u embedding Ä‘áº§u ra cÃ³ thá»ƒ huáº¥n luyá»‡n, thÃ¬ embedding Ä‘áº§u vÃ o cÅ©ng sáº½ Ä‘Æ°á»£c liÃªn quan Ä‘áº¿n huáº¥n luyá»‡n. Do Ä‘Ã³, táº¥t cáº£ tdy tá»« Ä‘áº§u ra cá»§a LLM, lÃªn Ä‘áº¿n lá»›p embedding Ä‘áº§u vÃ o, pháº£i Ä‘Æ°á»£c tÃ­ch lÅ©y vÃ o tdy cá»§a tensor embedding Ä‘áº§u ra, trong khi tdw cá»§a nÃ³ khÃ´ng thay Ä‘á»•i.

Lá»›p Multi-Head Attention (MHA). Má»™t lá»›p MHA chá»©a nhiá»u bá»™ chiáº¿u tuyáº¿n tÃ­nh nhÆ° cÃ¡c tensor cÃ³ thá»ƒ huáº¥n luyá»‡n, vÃ  FLOPs cá»§a chÃºng trong huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c dáº«n xuáº¥t theo cÃ¹ng cÃ¡ch nhÆ° chÃºng ta Ä‘Ã£ lÃ m vá»›i lá»›p dÃ y Ä‘áº·c trong Eq. (2). Má»™t sá»‘ LLMs (vÃ­ dá»¥, OPT) cÅ©ng bao gá»“m bias nhÆ° má»™t loáº¡i tensor cÃ³ thá»ƒ huáº¥n luyá»‡n khÃ¡c sau phÃ©p chiáº¿u nhÆ° váº­y. Trong trÆ°á»ng há»£p nÃ y, dá»±a trÃªn quy táº¯c dÃ¢y chuyá»n, lan truyá»n ngÆ°á»£c cá»§a bias Ä‘Æ°á»£c tÃ­nh toÃ¡n lÃ  dyi= dyi+1 vÃ  dwi= 1âŠ¤dyi+1, cho tháº¥y ráº±ng tdy cho bias lÃ  0 vÃ¬ dyi Ä‘Æ°á»£c truyá»n giá»‘ng há»‡t tá»« dyi+1. tdw cá»§a bias cÃ³ thá»ƒ Ä‘Æ°á»£c dáº«n xuáº¥t lÃ  FLOPs cá»§a viá»‡c cá»™ng cÃ¡c pháº§n tá»­ trong dyi+1 dá»c theo má»—i kÃªnh Ä‘áº·c trÆ°ng. CÆ¡ cháº¿ attention trong Eq. (1) Ä‘Æ°á»£c lan truyá»n ngÆ°á»£c trÆ°á»›c cÃ¡c bá»™ chiáº¿u. Náº¿u báº¥t ká»³ bá»™ chiáº¿u nÃ o trong sá»‘ nÃ y Ä‘Æ°á»£c liÃªn quan Ä‘áº¿n huáº¥n luyá»‡n, FLOPs lan truyá»n ngÆ°á»£c cá»§a attention cÅ©ng pháº£i Ä‘Æ°á»£c tÃ­nh toÃ¡n, vÃ  chÃºng tÃ´i tÃ­ch lÅ©y FLOPs nhÆ° váº­y vÃ o tdy cá»§a tensor bá»™ chiáº¿u tÆ°Æ¡ng á»©ng (WV).

LayerNorm. Cho má»™t token, LayerNorm Ä‘áº§u tiÃªn chuáº©n hÃ³a cÃ¡c Ä‘áº·c trÆ°ng cá»§a nÃ³ vÃ  sá»­ dá»¥ng hai tensor cÃ³ thá»ƒ huáº¥n luyá»‡n Î³ vÃ  Î² Ä‘á»ƒ nhÃ¢n theo pháº§n tá»­ vÃ  cá»™ng vÃ o token, tÆ°Æ¡ng á»©ng. CÃ¡c phÃ©p toÃ¡n nhÃ¢n vÃ  cá»™ng tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng phÃ©p toÃ¡n trong lá»›p dÃ y Ä‘áº·c, vÃ  do Ä‘Ã³ FLOPs cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n theo cÃ¡ch tÆ°Æ¡ng tá»±. Tuy nhiÃªn, FLOPs lan truyá»n ngÆ°á»£c cá»§a cÃ¡c toÃ¡n tá»­ chuáº©n hÃ³a pháº£i Ä‘Æ°á»£c tÃ­ch lÅ©y vÃ o tdy cá»§a tensor trÆ°á»›c Ä‘Ã³. Náº¿u báº¥t ká»³ tensor nÃ o trong cÃ¡c lá»›p trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c huáº¥n luyá»‡n, FLOPs cá»§a viá»‡c truyá»n qua cÃ¡c toÃ¡n tá»­ chuáº©n hÃ³a cÅ©ng pháº£i Ä‘Æ°á»£c bao gá»“m trong FLOPs cá»§a lá»›p hiá»‡n táº¡i.

Feed-Forward Network (FFN). Trong FFN, cÃ³ má»™t hÃ m kÃ­ch hoáº¡t phi tuyáº¿n giá»¯a hai lá»›p dÃ y Ä‘áº·c. Theo cÃ¹ng phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n FLOPs cá»§a LayerNorm, chÃºng tÃ´i tÃ­ch lÅ©y FLOPs cá»§a viá»‡c truyá»n qua hÃ m kÃ­ch hoáº¡t nÃ y vÃ o tdy cá»§a tensor bias trong lá»›p dÃ y Ä‘áº·c Ä‘áº§u tiÃªn.

3.2 ÄÃNH GIÃ Táº¦M QUAN TRá»ŒNG TENSOR
Táº§m quan trá»ng cá»§a má»™t tensor trong huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c Æ°á»›c tÃ­nh lÃ  tá»•ng táº§m quan trá»ng cá»§a táº¥t cáº£ cÃ¡c trá»ng sá»‘ cá»§a nÃ³. Trong huáº¥n luyá»‡n, vÃ¬ cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº­p nháº­t láº·p Ä‘i láº·p láº¡i Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a máº¥t mÃ¡t huáº¥n luyá»‡n, má»™t phÆ°Æ¡ng phÃ¡p trá»±c quan Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a viá»‡c cáº­p nháº­t trá»ng sá»‘ trong má»™t láº§n láº·p nháº¥t Ä‘á»‹nh lÃ  hoÃ n tÃ¡c cáº­p nháº­t nÃ y vÃ  kiá»ƒm tra máº¥t mÃ¡t huáº¥n luyá»‡n tÄƒng lÃªn nhÆ° tháº¿ nÃ o lÃ  âˆ†L=L(w)âˆ’L(w+ âˆ†w), Ä‘á»ƒ má»™t giÃ¡ trá»‹ âˆ†L cao hÆ¡n cÃ³ nghÄ©a lÃ  cáº­p nháº­t nÃ y quan trá»ng hÆ¡n vÃ  trá»ng sá»‘ nÃªn Ä‘Æ°á»£c chá»n.

Tuy nhiÃªn, viá»‡c tÃ­nh toÃ¡n âˆ†L cho má»—i trá»ng sá»‘ lÃ  tá»‘n kÃ©m. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i Æ°á»›c tÃ­nh táº§m quan trá»ng cá»§a táº¥t cáº£ trá»ng sá»‘ trong má»™t láº§n báº±ng cÃ¡ch lÃ m má»‹n phÃ©p toÃ¡n hoÃ n tÃ¡c Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ trÃªn vÃ  tÃ­nh toÃ¡n gradient máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c cáº­p nháº­t tÆ°Æ¡ng á»©ng vá»›i táº¥t cáº£ cÃ¡c trá»ng sá»‘. Äá»ƒ câˆˆ[0,1]M biá»ƒu thá»‹ phÃ©p toÃ¡n hoÃ n tÃ¡c cho táº¥t cáº£ M trá»ng sá»‘, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n gradient máº¥t mÃ¡t lÃ 

âˆ’âˆ‚L(w+câŠ™âˆ†w)
âˆ‚c=âˆ’âˆ†wâŠ™âˆ‚L(u)
âˆ‚u
u=w+câŠ™âˆ†w, (5)

trong Ä‘Ã³ âŠ™ biá»ƒu thá»‹ phÃ©p nhÃ¢n theo pháº§n tá»­. Khi c=0, Eq. (5) trá»Ÿ thÃ nh má»™t vector táº§m quan trá»ng trÃªn táº¥t cáº£ trá»ng sá»‘. VÃ¬ gradient máº¥t mÃ¡t Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi táº¥t cáº£ trá»ng sá»‘, táº§m quan trá»ng trá»ng sá»‘ Ä‘Æ°á»£c tÃ­nh toÃ¡n theo cÃ¡ch nÃ y ngáº§m káº¿t há»£p tÃ¡c Ä‘á»™ng cá»§a cÃ¡c phá»¥ thuá»™c trá»ng sá»‘. Táº§m quan trá»ng cá»§a tensor k sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh toÃ¡n lÃ 

Ik=âˆ’X
iâˆ†w(k)
iâˆ‚L/âˆ‚w(k)
i. (6)

Trong má»™t sá»‘ trÆ°á»ng há»£p, khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n gáº·p pháº£i sá»± phÃ¢n ká»³, giÃ¡ trá»‹ cá»§a gradient vÃ  táº§m quan trá»ng tensor Ä‘Æ°á»£c tÃ­nh toÃ¡n trong Eq. (6) cÃ³ thá»ƒ ráº¥t lá»›n, cuá»‘i cÃ¹ng dáº«n Ä‘áº¿n trÃ n sá»‘ khi sá»­ dá»¥ng nhá»¯ng giÃ¡ trá»‹ táº§m quan trá»ng nÃ y Ä‘á»ƒ quyáº¿t Ä‘á»‹nh lá»±a chá»n tensor trong Eq. (4). Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng ta cÃ³ thá»ƒ thÃªm tá»· lá»‡ táº¥t cáº£ táº§m quan trá»ng tensor theo biÃªn Ä‘á»™ tá»‘i Ä‘a Ä‘á»ƒ cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh sá»‘.

1 ğ‘‡ğ‘‡ğ‘ğ‘ğ‘ğ‘ğ‘¡ğ‘¡â€¦ â€¦
1â€¦
ğ‘˜ğ‘˜
ğ‘ğ‘â€¦ P[ k, t ]Subproblem 
Table  (ğ‘ğ‘Ã—ğ‘‡ğ‘‡ğ‘ğ‘ğ‘ğ‘)
backprop depth â‰¤ kbackprop FLOPs  â‰¤ t
k 1 k-11â€¦â€¦1â€¦â€¦1â€¦

(a) Äá»‹nh nghÄ©a bÃ i toÃ¡n con

k-1 k-2 k-3 1 kP[k-1, t]
P[k, t] = ?khÃ´ng
Ä‘Æ°á»£c chá»n
k-1 k-2 k-3 1 kP[k-1, t]
P[k, t] = ?Ä‘Æ°á»£c chá»n
khÃ´ng Ä‘Æ°á»£c chá»n Ä‘Æ°á»£c chá»nTrÆ°á»ng há»£p 1:
TrÆ°á»ng há»£p 2: (b) TÃ¬m quan há»‡ truy há»“i

HÃ¬nh 4: Giáº£i quyáº¿t váº¥n Ä‘á» lá»±a chá»n tensor sá»­ dá»¥ng DP

3.3 Lá»°A CHá»ŒN TENSOR
VÃ¬ Eq. (4) lÃ  má»™t váº¥n Ä‘á» láº­p trÃ¬nh sá»‘ nguyÃªn phi tuyáº¿n vÃ  do Ä‘Ã³ NP-hard, trong GreenTrainer chÃºng tÃ´i tÃ¬m kiáº¿m má»™t giáº£i phÃ¡p xáº¥p xá»‰ sá»­ dá»¥ng quy hoáº¡ch Ä‘á»™ng (DP). ChÃºng tÃ´i phÃ¢n tÃ¡ch toÃ n bá»™ váº¥n Ä‘á» thÃ nh cÃ¡c bÃ i toÃ¡n con bá»‹ háº¡n cháº¿ bá»Ÿi cÃ¡c Ä‘á»™ sÃ¢u khÃ¡c nhau cá»§a lan truyá»n ngÆ°á»£c. Nhá»¯ng bÃ i toÃ¡n con nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t tuáº§n tá»± tá»« bÃ i toÃ¡n con cÃ³ Ä‘á»™ sÃ¢u nhá» nháº¥t, báº±ng cÃ¡ch sá»­ dá»¥ng quan há»‡ truy há»“i cá»§a chÃºng.

Äá»‹nh nghÄ©a bÃ i toÃ¡n con. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4(a), chÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»—i bÃ i toÃ¡n con P[k, t] lÃ  tá»‘i Ä‘a hÃ³a táº§m quan trá»ng tÃ­ch lÅ©y cá»§a cÃ¡c tensor Ä‘Æ°á»£c chá»n khi 1) lá»±a chá»n náº±m trong k tensor hÃ ng Ä‘áº§u1 vÃ  2) FLOPs lan truyá»n ngÆ°á»£c nhiá»u nháº¥t lÃ  t. DP báº¯t Ä‘áº§u báº±ng cÃ¡ch giáº£i quyáº¿t bÃ i toÃ¡n con nhá» nháº¥t P[k= 1, t=1] vÃ  dáº§n dáº§n giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n con lá»›n hÆ¡n dá»±a trÃªn káº¿t quáº£ cá»§a cÃ¡c bÃ i toÃ¡n con nhá» hÆ¡n vÃ  quan há»‡ truy há»“i cá»§a nhá»¯ng bÃ i toÃ¡n con nÃ y, cho Ä‘áº¿n khi váº¥n Ä‘á» má»¥c tiÃªu P[N, Tfull] Ä‘Æ°á»£c giáº£i quyáº¿t.

Quan há»‡ truy há»“i cá»§a cÃ¡c bÃ i toÃ¡n con. Quan há»‡ truy há»“i giá»¯a bÃ i toÃ¡n con P[k, t] vÃ  P[kâˆ’1, t] phá»¥ thuá»™c vÃ o viá»‡c chÃºng ta cÃ³ thÃªm chá»n tensor hÃ ng Ä‘áº§u k tá»« giáº£i phÃ¡p cá»§a P[kâˆ’1, t] hay khÃ´ng, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4(b). TrÆ°á»ng há»£p 1: Náº¿u k khÃ´ng Ä‘Æ°á»£c chá»n, P[k, t] sáº½ trá»Ÿ vá» P[kâˆ’1, t], vÃ¬ táº§m quan trá»ng cá»§a cÃ¡c tensor Ä‘Æ°á»£c chá»n sáº½ khÃ´ng Ä‘Æ°á»£c tÄƒng thÃªm. TrÆ°á»ng há»£p 2: Náº¿u k Ä‘Æ°á»£c chá»n, thÃ¬ FLOPs cá»§a nÃ³ sáº½ Ä‘Æ°á»£c bao gá»“m vÃ o giáº£i phÃ¡p cá»§a P[k, t], báº¥t ká»ƒ nhá»¯ng tensor nÃ o khÃ¡c Ä‘Æ°á»£c chá»n. FLOPs liÃªn quan Ä‘áº¿n tensor k bao gá»“m 1) FLOPs Ä‘á»ƒ cáº­p nháº­t tensor k vÃ  2) FLOPs Ä‘á»ƒ truyá»n gradient kÃ­ch hoáº¡t tá»« tensor Ä‘Æ°á»£c chá»n gáº§n nháº¥t kc, nhÆ° tensor kâˆ’3 nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4(b), Ä‘áº¿n tensor k. Äiá»u nÃ y ngá»¥ Ã½ ráº±ng P[k, t] trá»Ÿ vá» má»™t bÃ i toÃ¡n con Ä‘Ã£ Ä‘Æ°á»£c giáº£i quyáº¿t trÆ°á»›c Ä‘Ã³ P[kâˆ’kc, tâˆ’âˆ†t], trong Ä‘Ã³

âˆ†t=tdwk+Xkâˆ’1
j=kctdyj. (7)

VÃ¬ kc khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c, chÃºng tÃ´i truy ngÆ°á»£c cÃ¡c bÃ i toÃ¡n con Ä‘Ã£ Ä‘Æ°á»£c giáº£i quyáº¿t trÆ°á»›c Ä‘Ã³ vÃ  khÃ¡m phÃ¡ táº¥t cáº£ cÃ¡c kháº£ nÄƒng cá»§a kc báº±ng cÃ¡ch giáº£m Ä‘á»™ sÃ¢u lan truyá»n ngÆ°á»£c tá»« k, vÃ  giáº£i phÃ¡p tá»‘i Æ°u cho P[k, t] lÃ  giáº£i phÃ¡p cÃ³ táº§m quan trá»ng tÃ­ch lÅ©y cao nháº¥t cá»§a cÃ¡c tensor Ä‘Æ°á»£c chá»n. Dá»±a trÃªn quan há»‡ truy há»“i nÃ y, chÃºng ta cÃ³ thá»ƒ giáº£i quyáº¿t táº¥t cáº£ cÃ¡c bÃ i toÃ¡n con báº±ng cÃ¡ch duyá»‡t khÃ´ng gian bÃ i toÃ¡n con. Äá»™ phá»©c táº¡p thá»i gian cá»§a viá»‡c giáº£i quyáº¿t má»—i bÃ i toÃ¡n con lÃ  O(N), vÃ  Ä‘á»™ phá»©c táº¡p thá»i gian tá»•ng thá»ƒ cá»§a DP lÃ  O(N2Tfull).

4 THÃ NGHIá»†M
Trong Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i, chÃºng tÃ´i bao gá»“m cÃ¡c LLM chá»‰ giáº£i mÃ£ bao gá»“m OPT (Zhang et al., 2022) vÃ  BLOOMZ (Muennighoff et al., 2022), vÃ  má»™t LLM mÃ£ hÃ³a-giáº£i mÃ£, cá»¥ thá»ƒ lÃ  FLAN-T5 (Chung et al., 2022), vá»›i kÃ­ch thÆ°á»›c LLM tá»« 350M Ä‘áº¿n 6.7B. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i chá»§ yáº¿u Ä‘Æ°á»£c tiáº¿n hÃ nh sá»­ dá»¥ng hai bá»™ dá»¯ liá»‡u tÃ³m táº¯t trá»«u tÆ°á»£ng sau:

â€¢SciTLDR (Cachola et al., 2020) lÃ  má»™t bá»™ dá»¯ liá»‡u gá»“m 5.4K tÃ³m táº¯t vÄƒn báº£n vá» 3.2K bÃ i bÃ¡o. NÃ³ chá»©a cáº£ TLDRs do tÃ¡c giáº£ viáº¿t vÃ  do chuyÃªn gia dáº«n xuáº¥t, trong Ä‘Ã³ loáº¡i sau Ä‘Æ°á»£c thu tháº­p bá»Ÿi má»™t giao thá»©c chÃº thÃ­ch táº¡o ra cÃ¡c tÃ³m táº¯t cháº¥t lÆ°á»£ng cao vá»›i gÃ¡nh náº·ng chÃº thÃ­ch tháº¥p.

â€¢DialogSum (Chen et al., 2021) lÃ  má»™t bá»™ dá»¯ liá»‡u tÃ³m táº¯t Ä‘á»‘i thoáº¡i gá»“m 13,460 Ä‘á»‘i thoáº¡i vá»›i cÃ¡c tÃ³m táº¯t vÃ  chá»§ Ä‘á» Ä‘Æ°á»£c gáº¯n nhÃ£n thá»§ cÃ´ng. NÃ³ Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  thÃ¡ch thá»©c hÆ¡n cÃ¡c bá»™ dá»¯ liá»‡u tÃ³m táº¯t khÃ¡c, nhÆ° SAMSum (Gliwa et al., 2019) vÃ  CNN/Daily (Nallapati et al., 2016) á»Ÿ quy mÃ´ tÆ°Æ¡ng tá»±.

ChÃºng tÃ´i cÅ©ng thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ QA sinh táº¡o trÃªn cÃ¡c bá»™ dá»¯ liá»‡u WebQuestion (Berant et al., 2013) vÃ  PIQA (Bisk et al., 2020) trong Phá»¥ lá»¥c A.4. Tuy nhiÃªn, chÃºng tÃ´i khÃ´ng xem xÃ©t cÃ¡c tÃ¡c vá»¥ khÃ´ng sinh táº¡o nhÆ° phÃ¢n loáº¡i cáº£m xÃºc, phÃ¢n loáº¡i há»‡ quáº£ vÃ  QA trÃ­ch xuáº¥t, vÃ¬ nhá»¯ng tÃ¡c vá»¥ nÃ y quÃ¡ dá»… dÃ ng cho LLMs vÃ  viá»‡c kiá»ƒm tra chÃºng vá»›i LLMs sáº½ dáº«n Ä‘áº¿n káº¿t quáº£ hiá»‡u suáº¥t Ä‘Æ°á»£c phÃ³ng Ä‘áº¡i so vá»›i baseline.

Äá»‘i vá»›i OPT vÃ  BLOOMZ, chÃºng tÃ´i tuÃ¢n theo cáº¥u trÃºc prompt giá»‘ng GPT2 (Radford et al., 2019), "[source seq.] TL;DR:", cho cÃ¡c tÃ¡c vá»¥ tÃ³m táº¯t Ä‘á»ƒ tiá»n xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº§u vÃ o. Äá»‘i vá»›i FLAN-T5, chÃºng tÃ´i Ã¡p dá»¥ng cáº¥u trÃºc prompt "summarize: [source seq.]" Ä‘Æ°á»£c sá»­ dá»¥ng trong huáº¥n luyá»‡n trÆ°á»›c T5 gá»‘c. ChÃºng tÃ´i cáº¯t ngáº¯n cÃ¡c chuá»—i nguá»“n Ä‘á»ƒ Ä‘á»™ dÃ i cá»§a má»—i chuá»—i Ä‘áº§u vÃ o Ä‘Æ°á»£c tiá»n xá»­ lÃ½ náº±m trong 512 token. TrÃªn dá»¯ liá»‡u kiá»ƒm tra, chÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c tÃ¬m kiáº¿m chÃ¹m lÃ  4, vÃ  Ä‘áº·t sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c sinh tá»‘i Ä‘a lÃ  64 cho SciTLDR vÃ  128 cho DialogSum. ChÃºng tÃ´i so sÃ¡nh GreenTrainer (GT) vá»›i cÃ¡c baseline sau:

â€¢Full Fine-Tuning (Full FT) tinh chá»‰nh táº¥t cáº£ cÃ¡c tham sá»‘ LLM vÃ  sáº½ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n má»™t cÃ¡ch trá»±c quan.

â€¢Fine-Tuning Top2 (FT-Top2) chá»‰ tinh chá»‰nh hai lá»›p cuá»‘i, thÆ°á»ng lÃ  lá»›p embedding vÃ  má»™t LayerNorm. CÃ¡c lá»›p embedding Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra Ä‘Æ°á»£c liÃªn káº¿t cho OPT vÃ  BLOOMZ, nhÆ°ng khÃ´ng Ä‘Æ°á»£c liÃªn káº¿t cho FLAN-T5. Baseline ngÃ¢y thÆ¡ nÃ y chá»‰ tinh chá»‰nh pháº§n nhá» nháº¥t cá»§a cÃ¡c tham sá»‘ LLM vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh liá»‡u bá»™ dá»¯ liá»‡u cÃ³ táº§m thÆ°á»ng Ä‘á»‘i vá»›i LLM hay khÃ´ng.

â€¢Prefix Tuning (Prefix-T) (Li and Liang, 2021) chÃ¨n cÃ¡c tiá»n tá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ o chuá»—i Ä‘áº§u vÃ o cá»§a má»—i khá»‘i transformer trong khi Ä‘Ã³ng bÄƒng cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. Äá»‘i vá»›i LLMs mÃ£ hÃ³a-giáº£i mÃ£, cÃ¡c tiá»n tá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n chá»‰ Ä‘Æ°á»£c chÃ¨n vÃ o cÃ¡c khá»‘i giáº£i mÃ£.

â€¢LoRA (Hu et al., 2021) hiá»‡n táº¡i lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t cho tinh chá»‰nh LLM hiá»‡u quáº£. NÃ³ sá»­ dá»¥ng phÃ¢n tÃ¡ch ma tráº­n rank tháº¥p Ä‘á»ƒ giáº£m chi phÃ­ huáº¥n luyá»‡n. ChÃºng tÃ´i Ã¡p dá»¥ng LoRA vÃ o cáº£ bá»™ chiáº¿u truy váº¥n vÃ  giÃ¡ trá»‹, nhÆ° Ä‘Æ°á»£c Ä‘á» xuáº¥t trong (Hu et al., 2021).

Trong táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m, chÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c batch lÃ  4 vÃ  tinh chá»‰nh mÃ´ hÃ¬nh trong 5 epoch. ChÃºng tÃ´i sá»­ dá»¥ng bá»™ tá»‘i Æ°u AdamW (Loshchilov and Hutter, 2017) vá»›i tá»‘c Ä‘á»™ há»c 2Ã—10âˆ’5 vá»›i lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh vÃ  suy giáº£m trá»ng sá»‘ 10âˆ’2. ChÃºng tÃ´i sá»­ dá»¥ng Ä‘iá»ƒm ROUGE (%R1/R2/RL) (Lin, 2004) lÃ m chá»‰ sá»‘ Ä‘á»™ chÃ­nh xÃ¡c, vÃ  Ä‘o cáº£ Peta-FLOPs (PFLOPs) vÃ  thá»i gian wall-clock lÃ m chi phÃ­ huáº¥n luyá»‡n trong má»—i láº§n cháº¡y. ChÃºng tÃ´i Ä‘o chi phÃ­ end-to-end cá»§a huáº¥n luyá»‡n, bao gá»“m chi phÃ­ tÃ­nh toÃ¡n trong cÃ¡c láº§n truyá»n thuáº­n vÃ  ngÆ°á»£c, vÃ  chi phÃ­ tÃ­nh toÃ¡n cá»§a Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor vÃ  lá»±a chá»n tensor sá»­ dá»¥ng DP.

4.1 CHI PHÃ HUáº¤N LUYá»†N & Äá»˜ CHÃNH XÃC
ChÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘Ã¡nh giÃ¡ chi phÃ­ huáº¥n luyá»‡n vÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a GreenTrainer (GT). NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2, Ä‘á»‘i vá»›i mÃ´ hÃ¬nh OPT-2.7B, GT-0.5 cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c yÃªu cáº§u giáº£m 50% FLOPs vá»›i máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c nhiá»u nháº¥t 2%, vÃ  GT-0.7 tháº­m chÃ­ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE cao hÆ¡n 0.2%-3% so vá»›i Full FT. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng Ä‘iá»u nÃ y lÃ  do GT chá»‰ tinh chá»‰nh cÃ¡c tensor quan trá»ng nháº¥t vÃ  do Ä‘Ã³ giáº£m thiá»ƒu overfitting cÃ³ thá»ƒ cÃ³ trong Full FT. CÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n khÃ´ng Ä‘á»§ cÅ©ng cÃ³ thá»ƒ dáº«n Ä‘áº¿n underfitting, nhÆ° FT-Top2 cÃ³ Ä‘iá»ƒm ROUGE tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ. TÆ°Æ¡ng tá»±, so vá»›i LoRA vÃ  Prefix Tuning, GT-0.7 Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n Ã­t nháº¥t 2% vá»›i cÃ¹ng lÆ°á»£ng FLOPs huáº¥n luyá»‡n.

TÆ°Æ¡ng tá»±, Ä‘á»‘i vá»›i BLOOMZ-3B, GT-0.5 cÃ³ thá»ƒ tiáº¿t kiá»‡m 50% FLOPs huáº¥n luyá»‡n vÃ  thá»i gian wall-clock vá»›i <2% máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c. So vá»›i Full FT, GT-0.7 Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE tÆ°Æ¡ng tá»± trÃªn SciTLDR, vÃ  cao hÆ¡n 4%-10% trÃªn DialogSum. Vá»›i cÃ¹ng lÆ°á»£ng FLOPs huáº¥n luyá»‡n, GT-0.7 cÃ³ Ä‘iá»ƒm ROUGE cao hÆ¡n 0.4%-1.4% so vá»›i LoRA. LÆ°u Ã½ ráº±ng cáº£ hai bá»™ dá»¯ liá»‡u Ä‘á»u khÃ´ng táº§m thÆ°á»ng Ä‘á»‘i vá»›i mÃ´ hÃ¬nh BLOOMZ, vÃ¬ baseline ngÃ¢y thÆ¡ (FT-Top2) váº«n thá»ƒ hiá»‡n máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c cao.

Äá»‘i vá»›i mÃ´ hÃ¬nh FLAN-T5-3B, FT-Top2 Ä‘áº¡t Ä‘Æ°á»£c cháº¥t lÆ°á»£ng tinh chá»‰nh tÆ°Æ¡ng tá»± vá»›i Full FT vá»›i FLOPs tháº¥p hÆ¡n, cho tháº¥y ráº±ng bá»™ dá»¯ liá»‡u SciTLDR lÃ  táº§m thÆ°á»ng Ä‘á»‘i vá»›i FLAN-T5. Trong trÆ°á»ng há»£p nÃ y, GT-0.34 cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cÃ¹ng FLOPs vÃ  Ä‘iá»ƒm ROUGE báº±ng cÃ¡ch chá»n má»™t pháº§n nhá» tensor. Máº·t khÃ¡c, FT-Top2 máº¥t Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng ká»ƒ trÃªn DialogSum, nhÆ°ng GT-0.4 giáº£m 54% FLOPs huáº¥n luyá»‡n vÃ  43% thá»i gian wall-clock mÃ  khÃ´ng cÃ³ máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng chÃº Ã½. GT-0.4 cÅ©ng vÆ°á»£t trá»™i hÆ¡n LoRA 1% vá» Ä‘iá»ƒm ROUGE vÃ  giáº£m 11% FLOPs hÆ¡n. So vá»›i Prefix tuning, GT-0.34 Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE cao hÆ¡n 2%-5%, trong khi giáº£m cÃ¹ng lÆ°á»£ng FLOPs huáº¥n luyá»‡n.

7

--- TRANG 8 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

# MÃ´ hÃ¬nh
& PhÆ°Æ¡ngphÃ¡p	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
OPT-2.7B
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
FT-Top2	29.0 (31% â†“)	0.61 (34% â†“)	9.1/4.0/7.6	181.6 (31% â†“)	3.8 (31% â†“)	20.8/7.9/17.5
Prefix-T	27.9 (33% â†“)	0.58 (37% â†“)	7.6/0.4/6.1	174.7 (33% â†“)	3.7 (33% â†“)	13.4/3.3/10.9
LoRA	27.9 (33% â†“)	0.59 (36% â†“)	28.2/12.1/21.0	174.7 (33% â†“)	3.6 (35% â†“)	23.8/9.5/18.8
GT-0.5	20.8 (50% â†“)	0.46 (50% â†“)	30.5/13.1/25.2	130.1 (50% â†“)	2.7 (51% â†“)	21.4/8.2/17.6
GT-0.7	29.2 (30% â†“)	0.68 (26% â†“)	33.1/15.2/27.6	182.7 (30% â†“)	4.0 (27% â†“)	26.8/11.0/21.6

BLOOMZ-3B
Full FT	47.2	1.0	28.3/12.1/22.5	294.8	6.5	26.1/10.6/21.0
FT-Top2	36.5 (23% â†“)	0.75 (25% â†“)	23.7/8.8/18.8	227.9 (23% â†“)	4.6 (29% â†“)	22.1/8.5/17.8
Prefix-T	31.5 (33% â†“)	0.68 (34% â†“)	6.5/2.2/5.5	196.5 (33% â†“)	4.2 (35% â†“)	29.6/9.4/24.9
LoRA	31.5 (33% â†“)	0.69 (33% â†“)	27.4/11.7/21.8	196.5 (33% â†“)	4.3 (34% â†“)	35.4/14.3/28.6
GT-0.5	23.4 (51% â†“)	0.51 (50% â†“)	26.7/10.7/21.2	146.4 (50% â†“)	3.1 (52% â†“)	24.9/9.5/20.0
GT-0.7	32.3 (32% â†“)	0.74 (28% â†“)	28.0/12.2/22.4	204.7 (31% â†“)	4.3 (34% â†“)	36.8/14.7/29.4

FLAN-T5-3B
Full FT	21.7	0.64	37.1/18.5/31.7	135.7	4.0	46.5/20.8/38.5
FT-Top2	7.3 (66% â†“)	0.21 (67% â†“)	36.5/18.4/31.5	46.1 (66% â†“)	1.4 (65% â†“)	39.2/16.7/32.9
Prefix-T	8.0 (63% â†“)	0.23 (64% â†“)	36.0/18.2/31.0	55.3 (60% â†“)	1.7 (57% â†“)	37.6/16.4/32.1
LoRA	14.4 (33% â†“)	0.41 (36% â†“)	36.6/18.5/31.5	90.5 (33% â†“)	2.5 (38% â†“)	44.7/19.8/37.1
GT-0.34	7.5 (65% â†“)	0.23 (64% â†“)	36.4/18.4/31.7	53.5 (61% â†“)	1.4 (65% â†“)	42.7/18.3/35.1
GT-0.4	10.0 (54% â†“)	0.38 (41% â†“)	36.7/18.5/31.5	62.5 (54% â†“)	2.3 (43% â†“)	46.0/20.7/38.1
GT-0.5	12.4 (43% â†“)	0.44 (31% â†“)	36.3/17.7/30.9	77.6 (43% â†“)	2.6 (35% â†“)	46.2/20.7/38.1

Báº£ng 2: So sÃ¡nh chi phÃ­ huáº¥n luyá»‡n & Ä‘á»™ chÃ­nh xÃ¡c trong tinh chá»‰nh LLM. GreenTrainer vá»›i má»¥c tiÃªu Ï giáº£m FLOPs Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  GT-Ï.

4.2 TÃC Äá»˜NG Cá»¦A Má»¤C TIÃŠU GIáº¢M FLOPS
Äá»ƒ hiá»ƒu rÃµ hÆ¡n cÃ¡ch GreenTrainer hoáº¡t Ä‘á»™ng vá»›i cÃ¡c má»¥c tiÃªu giáº£m FLOPs khÃ¡c nhau, chÃºng tÃ´i thay Ä‘á»•i giÃ¡ trá»‹ cá»§a Ï giá»¯a 0.36 vÃ  0.8, vÃ  so sÃ¡nh GreenTrainer vá»›i LoRA trÃªn mÃ´ hÃ¬nh OPT-2.7B. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 3, trÃªn bá»™ dá»¯ liá»‡u SciTLDR, khi yÃªu cáº§u giáº£m FLOPs cao vÃ  tÆ°Æ¡ng á»©ng vá»›i giÃ¡ trá»‹ Ïâ‰¤0.4, GreenTrainer vÆ°á»£t trá»™i hÆ¡n LoRA báº±ng cÃ¡ch Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE cao hÆ¡n 2% vÃ  tiáº¿t kiá»‡m 25% FLOPs vÃ  thá»i gian wall-clock hÆ¡n. Máº·t khÃ¡c, khi giÃ¡ trá»‹ Ï tÄƒng lÃªn 0.6, GreenTrainer vÆ°á»£t trá»™i hÆ¡n Full FT vá» Ä‘iá»ƒm ROUGE 0.5% vÃ  vÆ°á»£t trá»™i hÆ¡n LoRA 5.2%, nhÆ°ng tiáº¿t kiá»‡m 40% FLOPs huáº¥n luyá»‡n vÃ  39% thá»i gian wall-clock so vá»›i Full FT. Káº¿t quáº£ tÆ°Æ¡ng tá»± cÅ©ng Ä‘Æ°á»£c quan sÃ¡t trÃªn bá»™ dá»¯ liá»‡u DialogSum. TÃ³m láº¡i, vá»›i cÃ¡c má»¥c tiÃªu giáº£m FLOPs khÃ¡c nhau, GreenTrainer luÃ´n cÃ³ thá»ƒ cung cáº¥p sá»± cÃ¢n báº±ng tá»‘t hÆ¡n giá»¯a Ä‘á»™ chÃ­nh xÃ¡c huáº¥n luyá»‡n vÃ  chi phÃ­, so vá»›i cÃ¡c baseline SOTA.

PhÆ°Æ¡ng phÃ¡p	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
LoRA	27.9 (33% â†“)	0.59 (36% â†“)	28.2/12.1/21.0	174.7 (33% â†“)	3.6 (35% â†“)	23.8/9.5/18.8
GT-0.36	14.9 (64% â†“)	0.32 (65% â†“)	4.1/1.7/3.6	92.9 (65% â†“)	1.9 (65% â†“)	15.7/5.0/13.8
GT-0.4	16.6 (60% â†“)	0.36 (61% â†“)	28.6/11.6/23.5	103.4 (61% â†“)	2.2 (60% â†“)	17.9/6.3/15.4
GT-0.5	20.8 (50% â†“)	0.46 (50% â†“)	30.5/13.1/25.2	130.1 (50% â†“)	2.7 (51% â†“)	21.4/8.2/17.6
GT-0.6	25.0 (40% â†“)	0.56 (39% â†“)	33.4/15.3/27.8	156.6 (40% â†“)	3.3 (40% â†“)	24.0/9.7/19.2
GT-0.7	29.2 (30% â†“)	0.68 (26% â†“)	33.1/15.2/27.6	182.7 (30% â†“)	4.0 (27% â†“)	26.8/11.0/21.6
GT-0.8	33.4 (20% â†“)	0.77 (16% â†“)	33.1/15.5/27.6	209.6 (20% â†“)	4.4 (20% â†“)	23.9/9.9/19.1

Báº£ng 3: TÃ¡c Ä‘á»™ng cá»§a cÃ¡c má»¥c tiÃªu giáº£m FLOPs khÃ¡c nhau trÃªn mÃ´ hÃ¬nh OPT-2.7B

Nhá»¯ng káº¿t quáº£ nÃ y cÅ©ng chá»©ng minh ráº±ng GreenTrainer cung cáº¥p sá»± linh hoáº¡t tuyá»‡t vá»i trong tinh chá»‰nh LLM giá»¯a Ä‘á»™ chÃ­nh xÃ¡c huáº¥n luyá»‡n vÃ  chi phÃ­, báº±ng cÃ¡ch Ä‘iá»u chá»‰nh giÃ¡ trá»‹ cá»§a Ï. NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ chá»n Ä‘áº·t giÃ¡ trá»‹ Ï tháº¥p (â‰¤0.4) Ä‘á»ƒ tá»‘i Ä‘a hÃ³a viá»‡c giáº£m FLOPs (>60%) vá»›i máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh vá»«a pháº£i (3%-4% trÃªn hai bá»™ dá»¯ liá»‡u chÃºng tÃ´i sá»­ dá»¥ng). NgoÃ i ra, há» cÃ³ thá»ƒ sá»­ dá»¥ng giÃ¡ trá»‹ Ï cao (â‰¥0.6) Ä‘á»ƒ cÃ³ cÃ¹ng má»©c Ä‘á»™ giáº£m FLOPs nhÆ° cá»§a LoRA, nhÆ°ng Ä‘áº£m báº£o máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh tá»‘i thiá»ƒu hoáº·c tháº­m chÃ­ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh nhá». ChÃºng tÃ´i tin ráº±ng sá»± linh hoáº¡t nhÆ° váº­y cÃ³ táº§m quan trá»ng thá»±c táº¿ khi tinh chá»‰nh LLMs cho cÃ¡c tÃ¡c vá»¥ downstream vá»›i cÃ¡c yÃªu cáº§u vÃ  rÃ ng buá»™c AI xanh khÃ¡c nhau.

4.3 HIá»†U QUáº¢ Cá»¦A CÃC CHá»ˆ Sá» Táº¦M QUAN TRá»ŒNG TENSOR
Cháº¥t lÆ°á»£ng tinh chá»‰nh cá»§a GreenTrainer dá»±a trÃªn viá»‡c Ä‘Ã¡nh giÃ¡ phÃ¹ há»£p táº§m quan trá»ng tensor. ChÃºng tÃ´i so sÃ¡nh chá»‰ sá»‘ cá»§a chÃºng tÃ´i (âˆ†wâˆ‚Lâˆ‚w) vá»›i chá»‰ sá»‘ dá»±a trÃªn Ä‘á»™ lá»›n (âˆ†w) (Lee et al., 2020) vÃ  chá»‰ sá»‘ chá»‰ gradient (âˆ‚Lâˆ‚w) (Aji and Heafield, 2017), sá»­ dá»¥ng mÃ´ hÃ¬nh OPT-2.7B vá»›i Ï=0.7. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹

8

--- TRANG 9 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

PhÆ°Æ¡ng phÃ¡p	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
GT-0.7 (âˆ†w)	29.4 (30% â†“)	0.68 (26% â†“)	32.7/15.2/27.2	183.8 (30% â†“)	4.0 (27% â†“)	24.9/10.2/19.7
GT-0.7 (âˆ‚Lâˆ‚w)	29.4 (30% â†“)	0.67 (27% â†“)	32.8/15.1/27.2	184.0 (30% â†“)	4.0 (27% â†“)	25.0/10.2/20.0
GT-0.7 (âˆ†wâˆ‚Lâˆ‚w)	29.2 (30% â†“)	0.68 (26% â†“)	33.1/15.2/27.6	182.7 (30% â†“)	4.0 (27% â†“)	26.8/11.0/21.6

Báº£ng 4: Hiá»‡u quáº£ cá»§a CÃ¡c Chá»‰ sá»‘ Táº§m quan trá»ng Tensor (OPT-2.7B)

trong Báº£ng 4, vá»›i cÃ¹ng má»¥c tiÃªu giáº£m FLOPs, sá»­ dá»¥ng chá»‰ sá»‘ cá»§a chÃºng tÃ´i (âˆ†wâˆ‚Lâˆ‚w) cho Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh cao nháº¥t vÃ  vÆ°á»£t trá»™i hÆ¡n Full FT 1%-3% vá» Ä‘iá»ƒm ROUGE. Äiá»u nÃ y lÃ  do cÃ¡c chá»‰ sá»‘ dá»±a trÃªn Ä‘á»™ lá»›n bá» qua cÃ¡c phá»¥ thuá»™c cá»§a cáº­p nháº­t trá»ng sá»‘. CÃ¡c chá»‰ sá»‘ chá»‰ gradient chá»‰ chá»©a thÃ´ng tin hÆ°á»›ng vá» táº§m quan trá»ng tensor nhÆ°ng khÃ´ng thá»ƒ pháº£n Ã¡nh cÆ°á»ng Ä‘á»™ cá»§a táº§m quan trá»ng. CÃ¡c phÃ©p Ä‘o táº§m quan trá»ng khÃ´ng chÃ­nh xÃ¡c sáº½ dáº«n Ä‘áº¿n viá»‡c lá»±a chá»n tensor cÃ³ thá»ƒ huáº¥n luyá»‡n khÃ´ng phÃ¹ há»£p.

4.4 TÃC Äá»˜NG Cá»¦A KÃCH THÆ¯á»šC LLM
Má»™t loáº¡i LLM cÃ³ thá»ƒ chá»©a nhiá»u biáº¿n thá»ƒ vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau. Äá»ƒ nghiÃªn cá»©u hiá»‡u suáº¥t cá»§a GreenTrainer vá»›i cÃ¡c kÃ­ch thÆ°á»›c LLM khÃ¡c nhau, chÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n tinh chá»‰nh sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh OPT vá»›i kÃ­ch thÆ°á»›c tá»« 350M Ä‘áº¿n 6.7B. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 5, ngay cáº£ trÃªn cÃ¡c mÃ´ hÃ¬nh nhá» (OPT-350M), GT-0.5 cÃ³ thá»ƒ tiáº¿t kiá»‡m 17%-21% FLOPs huáº¥n luyá»‡n hÆ¡n so vá»›i LoRA, trong khi Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n 2%-4% (trÃªn SciTDR) hoáº·c cÃ¹ng Ä‘á»™ chÃ­nh xÃ¡c (trÃªn DialogSum). Khi kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÄƒng lÃªn 2.7B, GT-0.5 vÆ°á»£t trá»™i hÆ¡n LoRA vÃ  GT-0.7 vÆ°á»£t trá»™i hÆ¡n Full FT trÃªn bá»™ dá»¯ liá»‡u SciTLDR. TrÃªn DialogSum, GT-0.7 hoáº¡t Ä‘á»™ng tÆ°Æ¡ng tá»± so vá»›i LoRA. Äá»‘i vá»›i mÃ´ hÃ¬nh OPT-6.7B2, GT-0.4 cÃ³ thá»ƒ tiáº¿t kiá»‡m 27% FLOPs huáº¥n luyá»‡n hÆ¡n so vá»›i LoRA trÃªn SciTLDR, trong khi Ä‘áº¡t Ä‘Æ°á»£c cÃ¹ng Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh, vÃ  nhá»¯ng lá»£i tháº¿ tÆ°Æ¡ng tá»± cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c quan sÃ¡t khi so sÃ¡nh GT-0.5 vÃ  GT-0.7 vá»›i LoRA. NÃ³i chung, lá»£i tháº¿ hiá»‡u suáº¥t cá»§a GreenTrainer Ã¡p dá»¥ng rá»™ng rÃ£i cho cÃ¡c LLM vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau.

# Params
& PhÆ°Æ¡ng phÃ¡p	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
OPT-350M
Full FT	5.4	0.15	30.9/13.9/25.7	33.8	0.92	23.2/9.0/18.5
LoRA	3.6 (33% â†“)	0.10 (33% â†“)	25.9/10.8/20.3	22.5 (33% â†“)	0.65 (29% â†“)	21.5/7.7/17.3
GT-0.4	2.1 (61% â†“)	0.06 (60% â†“)	27.7/12.2/23.4	13.3 (61% â†“)	0.36 (61% â†“)	17.3/5.8/14.6
GT-0.5	2.7 (50% â†“)	0.08 (47% â†“)	29.9/13.2/24.9	16.7 (51% â†“)	0.45 (51% â†“)	21.3/7.8/17.3
GT-0.7	3.8 (30% â†“)	0.12 (20% â†“)	30.6/13.5/25.0	23.6 (30% â†“)	0.66 (28% â†“)	24.2/9.3/19.3

OPT-1.3B
Full FT	20.8	0.46	32.1/14.3/26.4	130.8	2.9	25.4/10.3/20.2
LoRA	13.9 (33% â†“)	0.31 (33% â†“)	28.1/11.9/22.0	87.2 (33% â†“)	1.9 (34% â†“)	24.6/9.9/19.4
GT-0.4	8.2 (61% â†“)	0.18 (61% â†“)	28.9/11.9/23.8	51.4 (61% â†“)	1.1 (62% â†“)	16.9/5.7/14.6
GT-0.5	10.3 (50% â†“)	0.23 (50% â†“)	30.0/12.7/24.5	64.2 (51% â†“)	1.4 (51% â†“)	20.1/7.4/16.7
GT-0.7	14.5 (30% â†“)	0.34 (26% â†“)	31.2/14.2/25.8	90.8 (30% â†“)	2.0 (31% â†“)	24.4/9.7/19.4

OPT-2.7B
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
LoRA	27.9 (33% â†“)	0.59 (36% â†“)	28.2/12.1/21.0	174.7 (33% â†“)	3.6 (35% â†“)	23.8/9.5/18.8
GT-0.4	16.6 (60% â†“)	0.36 (61% â†“)	28.6/11.6/23.5	103.4 (61% â†“)	2.2 (60% â†“)	17.9/6.3/15.4
GT-0.5	20.8 (50% â†“)	0.46 (50% â†“)	30.5/13.1/25.2	130.1 (50% â†“)	2.7 (51% â†“)	21.4/8.2/17.6
GT-0.7	29.2(30% â†“)	0.68 (26% â†“)	33.1/15.2/27.6	182.7 (30% â†“)	4.0 (27% â†“)	26.8/11.0/21.6

OPT-6.7B
Full FT	103.9	5.44	32.9/14.9/27.5	649.9	-	-
LoRA	69.3 (33% â†“)	1.3	28.4/12.3/22.7	433.3 (33% â†“)	8.1	24.9/10.2/19.4
GT-0.4	41.2 (60% â†“)	0.9	28.9/11.8/23.4	257.9 (60% â†“)	5.2	19.7/7.0/16.3
GT-0.5	50.8 (51% â†“)	1.1	30.1/13.0/24.8	331.4 (49% â†“)	6.7	21.8/8.5/17.3
GT-0.7	74.8 (28% â†“)	1.4	33.1/15.3/27.7	-	-	-

Báº£ng 5: TÃ¡c Ä‘á»™ng cá»§a kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh LLM

5 Káº¾T LUáº¬N
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y GreenTrainer, má»™t ká»¹ thuáº­t má»›i cho tinh chá»‰nh LLM cho phÃ©p lá»±a chá»n hiá»‡u quáº£ cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n thÃ´ng qua lan truyá»n ngÆ°á»£c thÃ­ch nghi, Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng huáº¥n luyá»‡n cao trong khi tá»‘i thiá»ƒu hÃ³a chi phÃ­ tÃ­nh toÃ¡n. GreenTrainer tiáº¿t kiá»‡m lÃªn Ä‘áº¿n 64% FLOPs huáº¥n luyá»‡n so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ mÃ  khÃ´ng cÃ³ máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡ng chÃº Ã½. So vá»›i ká»¹ thuáº­t hiá»‡n cÃ³ nhÆ° Prefix Tuning vÃ  LoRA, GreenTrainer cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c lÃªn Ä‘áº¿n 4% vá»›i cÃ¹ng má»©c giáº£m FLOPs.

2Äá»‘i vá»›i OPT-6.7B, Full FT vÃ  GT-0.7 vá»›i DialogSum cÃ³ váº¥n Ä‘á» háº¿t bá»™ nhá»› trÃªn cÃ¡c GPU chÃºng tÃ´i sá»­ dá»¥ng.

9

--- TRANG 10 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

Lá»œI Cáº¢M Æ N
ChÃºng tÃ´i muá»‘n cáº£m Æ¡n cÃ¡c ngÆ°á»i Ä‘Ã¡nh giÃ¡ áº©n danh vÃ  chá»§ tá»‹ch khu vá»±c vÃ¬ cÃ¡c bÃ¬nh luáº­n vÃ  pháº£n há»“i cá»§a há». CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi National Science Foundation (NSF) dÆ°á»›i sá»‘ tÃ i trá»£ IIS-2205360, CCF-2217003 vÃ  CCF-2215042.

TÃ€I LIá»†U THAM KHáº¢O
BÃ¡o cÃ¡o chá»‰ sá»‘ AI 2023. https://aiindex.stanford.edu/report/ , 2023.

M. Abadi. Tensorflow: learning functions at scale. In Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming , pages 1â€“1, 2016.

A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021 , 2017.

J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.

J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533â€“1544, 2013.

Y . Bisk, R. Zellers, J. Gao, Y . Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages 7432â€“7439, 2020.

L. Breiman. Random forests. Machine learning , 45:5â€“32, 2001.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877â€“1901, 2020.

I. Cachola, K. Lo, A. Cohan, and D. S. Weld. Tldr: Extreme summarization of scientific documents. arXiv preprint arXiv:2004.15011 , 2020.

A. Candel, J. McKinney, P. Singer, P. Pfeiffer, M. Jeblick, P. Prabhu, J. Gambera, M. Landry, S. Bansal, R. Chesler, et al. h2ogpt: Democratizing large language models. arXiv preprint arXiv:2306.08161 , 2023.

Y . Chen, Y . Liu, L. Chen, and Y . Zhang. Dialogsum: A real-life scenario dialogue summarization dataset. arXiv preprint arXiv:2105.06762 , 2021.

H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

B. Gliwa, I. Mochol, M. Biesek, and A. Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237 , 2019.

R. Hesse, S. Schaub-Meyer, and S. Roth. Fast axiomatic attribution for neural networks. Advances in Neural Information Processing Systems , 34:19513â€“19524, 2021.

E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.

Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, and S. Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 , 2023.

10

--- TRANG 11 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

K. Huang, B. Yang, and W. Gao. Elastictrainer: Speeding up on-device training with runtime elastic tensor selection. In Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services , pages 56â€“69, 2023a.

K. Huang, B. Yang, and W. Gao. Modality plug-and-play: Elastic modality adaptation in multimodal llms for embodied ai. arXiv preprint arXiv:2312.07886 , 2023b.

G. Jin, X. Yi, L. Zhang, L. Zhang, S. Schewe, and X. Huang. How does weight correlation affect generalisation ability of deep neural networks? Advances in Neural Information Processing Systems , 33:21346â€“21356, 2020.

Y . D. Kwon, R. Li, S. I. Venieris, J. Chauhan, N. D. Lane, and C. Mascolo. Tinytrain: Deep neural network training at the extreme edge. arXiv preprint arXiv:2307.09988 , 2023.

A. M. Lamb, A. G. ALIAS PARTH GOYAL, Y . Zhang, S. Zhang, A. C. Courville, and Y . Ben-gio. Professor forcing: A new algorithm for training recurrent networks. Advances in neural information processing systems , 29, 2016.

J. Lee, S. Park, S. Mo, S. Ahn, and J. Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611 , 2020.

N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.

B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.

H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 , 2016.

X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 , 2021.

B. Liao, S. Tan, and C. Monz. Make your pre-trained model reversible: From parameter to memory efficient fine-tuning. arXiv preprint arXiv:2306.00477 , 2023.

C.-Y . Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74â€“81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013 .

J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, C. Gan, and S. Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems , 35:22941â€“22954, 2022.

L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang, Y . Chen, W. Yang, Q. Liao, and W. Zhang. Group fisher pruning for practical network compression. In International Conference on Machine Learning , pages 7021â€“7032. PMLR, 2021.

X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, and J. Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 61â€“68, 2022.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.

K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247 , 1, 2021.

S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora. Fine-tuning language models with just forward passes. arXiv preprint arXiv:2305.17333 , 2023.

N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.

R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 , 2016.

11

--- TRANG 12 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 , 2019.

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.

R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green ai. Communications of the ACM , 63 (12):54â€“63, 2020.

T. Scialom, T. Chakrabarty, and S. Muresan. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 6107â€“6122, 2022.

M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International conference on machine learning , pages 3319â€“3328. PMLR, 2017.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.

H. Wang and W. Gao. Tackling the unlimited staleness in federated learning with intertwined data and device heterogeneities. arXiv preprint arXiv:2309.13536 , 2023.

T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.

E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.

Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and T. Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512 , 2023.

S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

12

--- TRANG 13 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

A PHá»¤ Lá»¤C

A.1 GIáº¢M VIá»†C Sá»¬ Dá»¤NG Bá»˜ NHá»š Cá»¦A ÄÃNH GIÃ Táº¦M QUAN TRá»ŒNG TENSOR

PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a cÃ¡c tensor NN trong Pháº§n 3.2 yÃªu cáº§u lÆ°u trá»¯ táº¥t cáº£ cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³ vÃ  gradient hiá»‡n táº¡i, Ä‘á»ƒ tÃ­nh toÃ¡n Eq. (6). Tuy nhiÃªn, viá»‡c lÃ m nhÆ° váº­y lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ tiÃªu thá»¥ bá»™ nhá»› GPU, Ä‘áº·c biá»‡t lÃ  Ä‘á»‘i vá»›i cÃ¡c LLM hiá»‡n Ä‘áº¡i vá»›i hÃ ng tá»· trá»ng sá»‘ mÃ´ hÃ¬nh. Äá»ƒ giáº£m viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU nhÆ° váº­y, chÃºng tÃ´i quan sÃ¡t ráº±ng cÃ´ng thá»©c váº¥n Ä‘á» cá»§a chÃºng tÃ´i trong Eq. (4) sáº½ ngÄƒn cáº£n cÃ¡c tensor trong cÃ¡c lá»›p Ä‘áº§u Ä‘Æ°á»£c chá»n Ä‘á»ƒ huáº¥n luyá»‡n, do chi phÃ­ cao cá»§a viá»‡c truyá»n gradient kÃ­ch hoáº¡t cá»§a chÃºng trong lan truyá»n ngÆ°á»£c. Do Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ an toÃ n loáº¡i trá»« nhá»¯ng tensor nÃ y khá»i pháº§n cÃ³ thá»ƒ huáº¥n luyá»‡n cá»§a tinh chá»‰nh LLM vÃ  tiáº¿t kiá»‡m má»™t lÆ°á»£ng bá»™ nhá»› GPU Ä‘Ã¡ng ká»ƒ.

Cá»¥ thá»ƒ hÆ¡n, lan truyá»n ngÆ°á»£c trong Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor cÃ³ thá»ƒ Ä‘Æ°á»£c dá»«ng sá»›m táº¡i má»™t tensor k nháº¥t Ä‘á»‹nh, sao cho

X
i=kâˆ’1,...,Ntdyi< ÏT fullâ‰¤X
i=k,...,Ntdyi, (8)

tá»©c lÃ , FLOPs tÃ­ch lÅ©y cá»§a táº¥t cáº£ cÃ¡c tensor tá»« 1 Ä‘áº¿n k vá»«a vÆ°á»£t quÃ¡ má»¥c tiÃªu giáº£m FLOPs cá»§a chÃºng ta. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 6, báº±ng cÃ¡ch Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p dá»«ng sá»›m nhÆ° váº­y, chÃºng ta cÃ³ thá»ƒ tiáº¿t kiá»‡m bá»™ nhá»› GPU theo tá»· lá»‡ vá»›i giÃ¡ trá»‹ cá»§a Ï, vÃ¬ giÃ¡ trá»‹ Ï nhá» hÆ¡n dáº«n Ä‘áº¿n k nhá» hÆ¡n vÃ  do Ä‘Ã³ lan truyá»n ngÆ°á»£c cÃ³ thá»ƒ Ä‘Æ°á»£c dá»«ng sá»›m hÆ¡n. VÃ­ dá»¥, khi Ï=50%, 25% bá»™ nhá»› GPU cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿t kiá»‡m, vÃ  viá»‡c tiáº¿t kiá»‡m nhÆ° váº­y cÃ³ thá»ƒ tÄƒng thÃªm lÃªn 50% khi Ï=34%.

MÃ´ hÃ¬nh	ÄÃ¡nh giÃ¡ Ä‘áº§y Ä‘á»§	Dá»«ng sá»›m Ï= 34%	Dá»«ng sá»›m Ï= 40%	Dá»«ng sá»›m Ï= 50%	Dá»«ng sá»›m Ï= 60%
OPT-2.7B	10.8	5.5	6.5	8.1	9.7
FLAN-T5-3B	12.0	6.1	7.2	9.0	10.8

Báº£ng 6: TiÃªu thá»¥ bá»™ nhá»› GPU (tÃ­nh báº±ng GigaBytes) cá»§a Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor

A.2 GIáº¢M CHI PHÃ TÃNH TOÃN Cá»¦A QUY HOáº CH Äá»˜NG CHO Lá»°A CHá»ŒN TENSOR

Trong phÆ°Æ¡ng phÃ¡p quy hoáº¡ch Ä‘á»™ng (DP) Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cho lá»±a chá»n tensor trong Pháº§n 3.3, do khá»‘i lÆ°á»£ng FLOPs cao trong tinh chá»‰nh LLM, giÃ¡ trá»‹ cá»§a Tfull cÃ³ thá»ƒ ráº¥t lá»›n. Äá»ƒ giáº£m chi phÃ­ tÃ­nh toÃ¡n cá»§a DP, chÃºng ta cÃ³ thá»ƒ giáº£m khÃ´ng gian bÃ i toÃ¡n con báº±ng cÃ¡ch bá» qua hai loáº¡i bÃ i toÃ¡n con: 1) nhá»¯ng bÃ i toÃ¡n khÃ´ng há»£p lá»‡, cÃ³ rÃ ng buá»™c FLOPs t vÆ°á»£t quÃ¡ rÃ ng buá»™c mong muá»‘n (ÏTfull); 2) nhá»¯ng bÃ i toÃ¡n dÆ° thá»«a, cÃ³ FLOPs Ä‘á»ƒ truyá»n gradient kÃ­ch hoáº¡t Ä‘áº¿n Ä‘á»™ sÃ¢u tá»‘i Ä‘a Ä‘Æ°á»£c phÃ©p (k) vÆ°á»£t quÃ¡ t. ThÃ­ nghiá»‡m sÆ¡ bá»™ cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng, viá»‡c lÃ m nhÆ° váº­y trÃªn mÃ´ hÃ¬nh OPT vá»›i Ïbp= 50% cÃ³ thá»ƒ giáº£m sá»‘ lÆ°á»£ng bÃ i toÃ¡n con 5.5 Ã— mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n tÃ­nh tá»‘i Æ°u cá»§a huáº¥n luyá»‡n.

MÃ´ hÃ¬nh	Tq= 1e1	Tq= 1e2	Tq= 1e3	Tq= 1e4	Tq= 1e5
OPT-2.7B	0.02/64.1/32.0	0.04/47.6/30.1	0.64/49.8/30.7	7.5/50.0/30.9	76.5/50.0/30.9
BLOOMZ-3B	0.0001/33.3/9.30	0.007/45.7/25.2	0.21/49.5/27.2	2.3/49.8/27.1	25.3/50.0/27.1
FLAN-T5-3B	0.04/64.9/36.5	0.25/57.1/36.5	3.5/55.3/36.7	41.8/51.8/36.7	449/50.0/36.7

Báº£ng 7: TÃ¡c Ä‘á»™ng cá»§a Ä‘á»™ phÃ¢n giáº£i DP Tq Ä‘áº¿n tinh chá»‰nh LLM OPT-2.7B, BLOOMZ-3B, vÃ  FLAN-T5-3B, trÃªn bá»™ dá»¯ liá»‡u SciTLDR vá»›i Ï= 50%. Má»—i bá»™ ba [a/b/c] trÃ¬nh bÃ y a) tá»· lá»‡ pháº§n trÄƒm thá»i gian wall-clock phÃ¡t sinh bá»Ÿi DP so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§, b) tá»· lá»‡ pháº§n trÄƒm FLOPs sau khi giáº£m so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§, vÃ  c) Ä‘iá»ƒm ROUGE-1 kiá»ƒm tra, tÆ°Æ¡ng á»©ng.

BÃªn cáº¡nh Ä‘Ã³, Ä‘á»ƒ giáº£m thÃªm sá»‘ lÆ°á»£ng bÃ i toÃ¡n con, chÃºng tÃ´i chia tá»· lá»‡ FLOPs cá»§a cÃ¡c tensor (tdw, tdy) báº±ng cÃ¡ch nhÃ¢n vá»›i há»‡ sá»‘ Z:

ftdw=âŒŠtdwÂ·ZâŒ‹,ftdy=âŒŠtdyÂ·ZâŒ‹, (9)

trong Ä‘Ã³ Z=Tq/Tfull giáº£m FLOPs backpropagation xuá»‘ng Ä‘á»™ phÃ¢n giáº£i Tq< Tfull. Äá»™ phá»©c táº¡p thá»i gian tá»•ng thá»ƒ cá»§a DP sau Ä‘Ã³ Ä‘Æ°á»£c giáº£m xuá»‘ng O(N2Tq). Máº·t khÃ¡c, Ä‘á»™ phÃ¢n giáº£i giáº£m nhÆ° váº­y cÃ³ thá»ƒ tÄƒng tÃ­nh mÆ¡ há»“ trong DP vÃ  áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng huáº¥n luyá»‡n. Äá»ƒ Ä‘iá»u tra sá»± cÃ¢n báº±ng nhÆ° váº­y giá»¯a cháº¥t lÆ°á»£ng huáº¥n luyá»‡n vÃ  chi phÃ­, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m sÆ¡ bá»™ trÃªn nhiá»u LLM. Káº¿t quáº£

13

--- TRANG 14 ---
ÄÆ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
trong Báº£ng 7 cho tháº¥y ráº±ng, Ä‘á»‘i vá»›i cáº£ mÃ´ hÃ¬nh OPT-2.7B vÃ  BLOOMZ-3B, viá»‡c Ä‘áº·t Tq= 1e3 giáº£m chi phÃ­ DP xuá»‘ng <1% mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng huáº¥n luyá»‡n. TÆ°Æ¡ng tá»±, Ä‘á»‘i vá»›i FLAN-T5-3B, viá»‡c chá»n Tq= 1e2 cÃ³ thá»ƒ giá»¯ láº¡i cháº¥t lÆ°á»£ng huáº¥n luyá»‡n tá»‘t vá»›i chi phÃ­ khÃ´ng Ä‘Ã¡ng ká»ƒ. Máº·t khÃ¡c, khi Tq quÃ¡ nhá», giáº£i phÃ¡p cá»§a DP cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c vÃ  do Ä‘Ã³ dáº«n Ä‘áº¿n viá»‡c giáº£m FLOPs huáº¥n luyá»‡n khÃ´ng hiá»‡u quáº£.

â€¦
query_states  = self.q_proj (hidden_states) * self.scaling
key_states = self._shape(self.k_proj (hidden_states ), -1, bsz)
value_states  = self._shape (self.v_proj( hidden_states), -1, bsz)
â€¦Má»™t pháº§n cá»§a Ä‘á»“ thá»‹ tÃ­nh toÃ¡n cá»§a LLM 
Ä‘Æ°á»£c biá»ƒu diá»…n trong mÃ£ python
Biá»ƒu diá»…n chá»‰ tensor dá»±a trÃªn thá»© tá»± thá»±c thi cá»§a tensor
Kernel
2560x2560Bias 
2560x 1Kernel
2560x2560Bias
2560x1Kernel
2560x2560Bias
2560x1q_proj k_proj v_proj
TÃ­nh toÃ¡n vÃ  khá»›p 
(ğ’•ğ’•ğ’…ğ’…ğ’…ğ’…,ğ’•ğ’•ğ’…ğ’…ğ’…ğ’…) cho má»—i tensor â€¦ â€¦batch_size = 4
token_num  = 512
token_dim  = 2560
ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘=4Ã—512Ã—2560 Ã—2560
ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘=4Ã—512Ã—2560 Ã—2560ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘=0
ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘=4Ã—512Ã—2560Kernel: Bias:ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘,ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘,ğ‘¡ğ‘¡ğ‘‘ğ‘‘ğ‘‘ğ‘‘
â€¦
Kernel:
 Bias:
 â€¦ CÃ´ng thá»©c lan truyá»n ngÆ°á»£c 
cho cÃ¡c loáº¡i tensor khÃ¡c nhauâ€¦

HÃ¬nh 5: Má»™t vÃ­ dá»¥ vá» láº­p há»“ sÆ¡ FLOPs tensor trong mÃ´ hÃ¬nh OPT-2.7B

A.3 Má»˜T VÃ Dá»¤ Vá»€ Láº¬P Há»’ SÆ  FLOPS TENSOR TRONG MÃ” HÃŒNH OPT-2.7B
Äá»ƒ táº¡o Ä‘iá»u kiá»‡n hiá»ƒu biáº¿t tá»‘t hÆ¡n, chÃºng tÃ´i tiáº¿p tá»¥c hiá»ƒn thá»‹ má»™t vÃ­ dá»¥ trong HÃ¬nh 5 vá» cÃ¡ch chÃºng tÃ´i láº­p há»“ sÆ¡ cÃ¡c tensor trong mÃ´ hÃ¬nh OPT-2.7B trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i. Äáº§u tiÃªn, chÃºng tÃ´i chuyá»ƒn Ä‘á»•i Ä‘á»“ thá»‹ tÃ­nh toÃ¡n cá»§a LLM, Ä‘Æ°á»£c triá»ƒn khai trong mÃ£ Python, thÃ nh biá»ƒu diá»…n chá»‰ tensor. CÃ¡c tensor Ä‘Æ°á»£c sáº¯p xáº¿p dá»±a trÃªn thá»© tá»± thá»±c thi cá»§a chÃºng trong láº§n truyá»n thuáº­n, tÆ°Æ¡ng tá»± nhÆ° Ä‘á»“ thá»‹ cáº¥p lá»›p trong HÃ¬nh 3. Sau Ä‘Ã³ chÃºng tÃ´i tÃ­nh toÃ¡n FLOPs cá»§a má»—i tensor (tdy,tdy) dá»±a trÃªn cÃ¡c cÃ´ng thá»©c lan truyá»n ngÆ°á»£c Ä‘Æ°á»£c tháº£o luáº­n trong Pháº§n 3.1. Nhá»¯ng phÃ©p tÃ­nh nhÆ° váº­y vá» cÆ¡ báº£n lÃ  Ä‘áº¿m cÃ¡c phÃ©p nhÃ¢n vÃ  Ä‘Æ°á»£c cá»™ng vÃ o trong cÃ´ng thá»©c cá»§a chÃºng.

PhÆ°Æ¡ng phÃ¡p	Äá»™ chÃ­nh xÃ¡c (%)	PFLOPs	Time (h)
LoRA	49.5	174.0	6.27
GT-0.5	59.2	130.5	4.69

Báº£ng 8: OPT-2.7B trÃªn bá»™ dá»¯ liá»‡u PIQA

PhÆ°Æ¡ng phÃ¡p	Äá»™ chÃ­nh xÃ¡c (%)	PFLOPs	Time (h)
LoRA	19.6	16.0	0.55
GT-0.5	28.7	12.0	0.50
GT-0.6	29.5	14.0	0.61

Báº£ng 9: OPT-2.7B trÃªn bá»™ dá»¯ liá»‡u WebQuestion

A.4 HIá»†U SUáº¤T TRÃŠN CÃC TÃC Vá»¤ Há»I ÄÃP SINH Táº O
Äá»ƒ Ä‘Ã¡nh giÃ¡ tá»‘t hÆ¡n hiá»‡u suáº¥t cá»§a GreenTrainer trÃªn cÃ¡c tÃ¡c vá»¥ khÃ¡c, chÃºng tÃ´i cÅ©ng Ä‘Ã£ tiáº¿n hÃ nh thÃ­ nghiá»‡m báº±ng cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh OPT-2.7B trÃªn cÃ¡c bá»™ dá»¯ liá»‡u WebQuestions vÃ  PIQA cho cÃ¡c tÃ¡c vá»¥ QA sinh táº¡o. Bá»™ dá»¯ liá»‡u WebQuestions chá»©a 6,642 cáº·p QA sá»­ dá»¥ng Freebase lÃ m cÆ¡ sá»Ÿ kiáº¿n thá»©c. Bá»™ dá»¯ liá»‡u PIQA táº­p trung vÃ o QA Ä‘a lá»±a chá»n vá» kiáº¿n thá»©c váº­t lÃ½ vá»›i 21k cáº·p QA. ChÃºng tÃ´i Ã¡p dá»¥ng Ä‘á»‹nh dáº¡ng prompt " question: {q}</s>answer: {a}</s> " cho WebQuestions vÃ  " goal: {q}</s>sol1: {sol1}</s>sol2: {sol2}</s>label: {a}</s> " cho PIQA, trong Ä‘Ã³ </s> lÃ  token EOS cho mÃ´ hÃ¬nh OPT. CÃ¡c siÃªu tham sá»‘ cho huáº¥n luyá»‡n giá»‘ng nhÆ° nhá»¯ng tham sá»‘ Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 4. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cáº¥p cÃ¢u yÃªu cáº§u cÃ¢u tráº£ lá»i Ä‘Æ°á»£c sinh ra pháº£i khá»›p chÃ­nh xÃ¡c vá»›i ground truth. LÆ°u Ã½ ráº±ng Ä‘á»‘i vá»›i PIQA, cÃ¡c token Ä‘Æ°á»£c sinh ra váº«n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n tá»« toÃ n bá»™ tá»« Ä‘iá»ƒn cá»§a embeddings OPT thay vÃ¬ tá»« hai lá»±a chá»n: thá»© nháº¥t hoáº·c thá»© hai. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 8 vÃ  Báº£ng 9, trÃªn cáº£ hai bá»™ dá»¯ liá»‡u, GreenTrainer (GT) Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ thá»i gian tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i LoRA.

Cá»¥ thá»ƒ, káº¿t quáº£ trÃªn bá»™ dá»¯ liá»‡u PIQA nÃ³i chung tháº¥p hÆ¡n nhá»¯ng káº¿t quáº£ Ä‘Æ°á»£c bÃ¡o cÃ¡o trong Brown et al. (2020). LÃ½ do cho khoáº£ng cÃ¡ch Ä‘á»™ chÃ­nh xÃ¡c nÃ y lÃ  cÃ¡ch chÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh OPT Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i thÃ¡ch thá»©c hÆ¡n so vá»›i thiáº¿t láº­p trong Brown et al. (2020). Theo Pháº§n 2.4 trong Brown et al. (2020), nÃ³ cÃ´ng thá»©c hÃ³a tÃ¡c vá»¥ PIQA nhÆ° má»™t tÃ¡c vá»¥ QA Ä‘a lá»±a chá»n trong Ä‘Ã³ cÃ¢u tráº£ lá»i Ä‘Æ°á»£c rÃºt ra tá»« má»™t táº­p há»£p á»©ng viÃªn nhá» vÃ  Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c (vÃ­ dá»¥, ["0", "1"]), báº±ng cÃ¡ch so sÃ¡nh Ä‘iá»ƒm xÃ¡c suáº¥t chá»‰ trÃªn cÃ¡c token á»©ng viÃªn. NgÆ°á»£c láº¡i, chÃºng tÃ´i nghiÃªm ngáº·t Ä‘Æ°a váº¥n Ä‘á» vÃ o sinh táº¡o má»Ÿ, trong Ä‘Ã³ táº­p há»£p á»©ng viÃªn khÃ´ng Ä‘Æ°á»£c biáº¿t. Trong trÆ°á»ng há»£p Ä‘Ã³, viá»‡c sinh ra cÃ¢u tráº£ lá»i Ä‘Ãºng cÃ³ thá»ƒ khÃ³ khÄƒn hÆ¡n, vÃ¬ mÃ´ hÃ¬nh cÃ³ thá»ƒ sinh ra cÃ¢u tráº£ lá»i hoÃ n toÃ n khÃ´ng liÃªn quan vÃ  tÄƒng cÆ¡ há»™i máº¯c lá»—i.

A.5 TÃC Äá»˜NG Cá»¦A Táº¦N SUáº¤T ÄÃNH GIÃ Táº¦M QUAN TRá»ŒNG TENSOR
Thiáº¿t káº¿ cá»§a GreenTrainer, theo máº·c Ä‘á»‹nh, Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a cÃ¡c tensor vÃ  chá»n táº­p há»£p cÃ¡c tensor cÃ³ thá»ƒ huáº¥n luyá»‡n dá»±a trÃªn táº§m quan trá»ng nhÆ° váº­y á»Ÿ Ä‘áº§u má»—i epoch huáº¥n luyá»‡n. Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p ká»¹ thuáº­t Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 3.1, viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor nhÆ° váº­y ráº¥t nháº¹, vÃ  káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng chi phÃ­ cá»§a viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng chá»‰ lÃ  0.2% trÃªn bá»™ dá»¯ liá»‡u SciTLDR vÃ  0.01% trÃªn bá»™ dá»¯ liá»‡u DialogSum, Ä‘á»‘i vá»›i toÃ n bá»™ chi phÃ­ tinh chá»‰nh.

Máº·t khÃ¡c, trong má»™t sá»‘ trÆ°á»ng há»£p nháº¥t Ä‘á»‹nh, táº§m quan trá»ng tensor, Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cÃ¡c thay Ä‘á»•i gradient mÃ´ hÃ¬nh, cÃ³ thá»ƒ thá»ƒ hiá»‡n sá»± khÃ¡c biá»‡t khÃ´ng thá»ƒ bá» qua trong má»™t epoch. Trong nhá»¯ng trÆ°á»ng há»£p nÃ y, thiáº¿t káº¿ linh hoáº¡t cá»§a GreenTrainer sáº½ cho phÃ©p chÃºng ta thÃ­ch nghi tÄƒng táº§n suáº¥t Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor vÃ  lá»±a chá»n tensor dá»±a trÃªn DP tÆ°Æ¡ng á»©ng. Äá»ƒ chá»©ng minh tÃ¡c Ä‘á»™ng cá»§a viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor vÃ  lá»±a chá»n tensor dá»±a trÃªn DP thÆ°á»ng xuyÃªn hÆ¡n nhÆ° váº­y, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh thÃªm thÃ­ nghiá»‡m sá»­ dá»¥ng mÃ´ hÃ¬nh OPT-2.7B trÃªn bá»™ dá»¯ liá»‡u WebQuestions vÃ  tÃ¡c vá»¥ QA sinh táº¡o, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 10.

Táº§n suáº¥t Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor	Äá»™ chÃ­nh xÃ¡c (%)	Time (h)
Má»—i 945 láº§n láº·p (má»™t láº§n má»—i epoch)	28.4	0.50
Má»—i 600 láº§n láº·p	28.5	0.54
Má»—i 400 láº§n láº·p	28.2	0.56
Má»—i 200 láº§n láº·p	27.5	0.64

Báº£ng 10: TÃ¡c Ä‘á»™ng cá»§a táº§n suáº¥t Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor

Káº¿t quáº£ cho tháº¥y ráº±ng: (1) Viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor thÆ°á»ng xuyÃªn hÆ¡n chá»‰ mang láº¡i sá»± cáº£i thiá»‡n ráº¥t nhá» vá» Ä‘á»™ chÃ­nh xÃ¡c tÃ¡c vá»¥. Xem xÃ©t tÃ­nh ngáº«u nhiÃªn trong cÃ¡c láº§n thá»­ huáº¥n luyá»‡n khÃ¡c nhau, chÃºng tÃ´i tin ráº±ng sá»± cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c nhÆ° váº­y lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ, vÃ  Ä‘á»™ chÃ­nh xÃ¡c tháº­m chÃ­ cÃ³ thá»ƒ giáº£m 1% khi táº§n suáº¥t Ä‘Ã¡nh giÃ¡ ráº¥t cao (má»—i 200 láº§n láº·p). ChÃºng tÃ´i tin ráº±ng Ä‘iá»u nÃ y lÃ  do sá»± tÃ­ch lÅ©y cá»§a cÃ¡c lá»—i Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor vÃ  lá»±a chá»n tensor, báº¯t nguá»“n tá»« xáº¥p xá»‰ báº­c nháº¥t trong chá»‰ sá»‘ táº§m quan trá»ng tensor vÃ  giáº£i phÃ¡p xáº¥p xá»‰ trong DP. Má»™t lÃ½ do kháº£ dÄ© khÃ¡c lÃ  táº§m quan trá»ng tensor Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n, vÃ  viá»‡c Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor quÃ¡ thÆ°á»ng xuyÃªn cÃ³ thá»ƒ khiáº¿n quÃ¡ trÃ¬nh huáº¥n luyá»‡n overfit vá»›i bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n. (2) Chi phÃ­ huáº¥n luyá»‡n tÄƒng Ä‘á»u vá»›i táº§n suáº¥t Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor. Khi khoáº£ng cÃ¡ch Ä‘Ã¡nh giÃ¡ giáº£m tá»« 945 láº§n láº·p xuá»‘ng 200 láº§n láº·p, thá»i gian huáº¥n luyá»‡n tÄƒng 28%.

TÃ³m láº¡i, viá»‡c thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng tensor thÆ°á»ng xuyÃªn hÆ¡n trong má»—i epoch mang láº¡i Ã­t cáº£i thiá»‡n vá» Ä‘á»™ chÃ­nh xÃ¡c tÃ¡c vá»¥ nhÆ°ng tÄƒng Ä‘Ã¡ng ká»ƒ chi phÃ­ huáº¥n luyá»‡n. ChÃºng tÃ´i tin ráº±ng táº§m quan trá»ng tensor Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ má»™t láº§n trong má»—i epoch sáº½ Ä‘á»§ chÃ­nh xÃ¡c cho viá»‡c lá»±a chá»n tensor cÃ³ thá»ƒ huáº¥n luyá»‡n phÃ¹ há»£p.

A.6 Sá»° Cáº¦N THIáº¾T Cá»¦A Lá»°A CHá»ŒN TENSOR Äá»˜NG
Náº¿u tinh chá»‰nh LLM sá»­ dá»¥ng má»™t bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n cá»‘ Ä‘á»‹nh, cÃ³ thá»ƒ viá»‡c sá»­ dá»¥ng lá»±a chá»n tensor cá»‘ Ä‘á»‹nh Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh á»Ÿ giai Ä‘oáº¡n ban Ä‘áº§u cá»§a huáº¥n luyá»‡n cÃ³ thá»ƒ khÃ´ng dáº«n Ä‘áº¿n sá»± sá»¥t giáº£m Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh Ä‘Ã¡ng ká»ƒ, so vá»›i lá»±a chá»n tensor táº¡i thá»i gian cháº¡y. Tuy nhiÃªn, trong cÃ¡c tÃ¬nh huá»‘ng tinh chá»‰nh LLM thá»±c táº¿, giáº£ Ä‘á»‹nh nÃ y thÆ°á»ng khÃ´ng Ä‘Ãºng do hai lÃ½ do sau. Äáº§u tiÃªn, trong ráº¥t nhiá»u tÃ¬nh huá»‘ng tinh chá»‰nh LLM, nhÆ° há»c trá»±c tuyáº¿n vÃ  cÃ¡ nhÃ¢n hÃ³a mÃ´ hÃ¬nh, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i liÃªn tá»¥c sá»­ dá»¥ng dá»¯ liá»‡u trá»±c tuyáº¿n, Ä‘Æ°á»£c táº¡o ra liÃªn tá»¥c táº¡i thá»i gian cháº¡y vá»›i cÃ¡c phÃ¢n phá»‘i dá»¯ liá»‡u biáº¿n Ä‘á»•i. Nhá»¯ng phÃ¢n phá»‘i dá»¯ liá»‡u biáº¿n Ä‘á»•i nhÆ° váº­y cháº¯c cháº¯n sáº½ dáº«n Ä‘áº¿n táº§m quan trá»ng khÃ¡c nhau cá»§a cÃ¡c tensor thÃ´ng qua quy trÃ¬nh huáº¥n luyá»‡n vÃ  do Ä‘Ã³ yÃªu cáº§u lá»±a chá»n tensor táº¡i thá»i gian cháº¡y. Nhá»¯ng tÃ¬nh huá»‘ng tinh chá»‰nh LLM trá»±c tuyáº¿n nhÆ° váº­y gáº§n Ä‘Ã¢y trá»Ÿ nÃªn ngÃ y cÃ ng phá»• biáº¿n, Ä‘áº·c biá»‡t vá»›i kháº£ nÄƒng triá»ƒn khai LLMs lÃªn cÃ¡c thiáº¿t bá»‹ di Ä‘á»™ng cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng nhÆ° Ä‘iá»‡n thoáº¡i thÃ´ng minh. Thá»© hai, ngay cáº£ Ä‘á»‘i vá»›i má»™t bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n cá»‘ Ä‘á»‹nh, cÅ©ng cÃ³ thá»ƒ táº§m quan trá»ng cá»§a má»™t sá»‘ tensor cÃ³ thá»ƒ thay Ä‘á»•i khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n tiáº¿n triá»ƒn. Trong nhá»¯ng trÆ°á»ng há»£p nÃ y, lá»±a chá»n tensor Ä‘á»™ng cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n. Äá»ƒ xÃ¡c minh Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh thÃªm thÃ­ nghiá»‡m sá»­ dá»¥ng mÃ´ hÃ¬nh OPT-2.7B trÃªn bá»™ dá»¯ liá»‡u WebQuestions vÃ  tÃ¡c vá»¥ QA sinh táº¡o. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 11, lá»±a chá»n tensor Ä‘á»™ng cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p khÃ´ng thá»ƒ bá» qua vÃ o viá»‡c cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c tÃ¡c vá»¥, vá»›i sá»± tÄƒng khÃ´ng Ä‘Ã¡ng ká»ƒ cá»§a chi phÃ­ huáº¥n luyá»‡n.

Chiáº¿n lÆ°á»£c	Äá»™ chÃ­nh xÃ¡c (%)	Time (h)
Lá»±a chá»n tensor cá»‘ Ä‘á»‹nh chá»‰ trong epoch Ä‘áº§u tiÃªn cá»§a huáº¥n luyá»‡n	27.4	0.49
Lá»±a chá»n tensor Ä‘á»™ng, má»™t láº§n trong má»—i epoch	28.4	0.50
Lá»±a chá»n tensor thÆ°á»ng xuyÃªn hÆ¡n (5 láº§n trong má»—i epoch)	27.5	0.64

Báº£ng 11: CÃ¡c chiáº¿n lÆ°á»£c lá»±a chá»n tensor khÃ¡c nhau

LÆ°u Ã½ ráº±ng, sá»± cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y sáº½ phá»¥ thuá»™c vÃ o bá»™ dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh cá»¥ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng, nhÆ°ng nhá»¯ng káº¿t quáº£ thÃ­ nghiá»‡m trÃªn Ä‘Ã£ chá»©ng minh sá»± cáº§n thiáº¿t cá»§a lá»±a chá»n tensor táº¡i thá»i gian cháº¡y. NgoÃ i ra, káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cÅ©ng cho tháº¥y ráº±ng viá»‡c Ä‘Ã¡nh giÃ¡ vÃ  lá»±a chá»n táº§m quan trá»ng tensor nhÆ° váº­y thá»±c sá»± phÃ¡t sinh ráº¥t Ã­t chi phÃ­ tÃ­nh toÃ¡n bá»• sung.

16
