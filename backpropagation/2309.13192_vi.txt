# 2309.13192.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/backpropagation/2309.13192.pdf
# Kích thước tệp: 798167 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
HƯỚNG TỚI AI XANH TRONG TINH CHỈNH CÁC MÔ HÌNH NGÔN NGỮ LỚN THÔNG QUA LAN TRUYỀN NGƯỢC THÍCH NGHI
Kai Huang†, Hanyun Yin§, Heng Huang‡& Wei Gao†
University of Pittsburgh†, University of Maryland, College Park‡
University of Science and Technology of China§
k.huang@pitt.edu ,ykissgoodbye@gmail.com ,heng@umd.edu ,weigao@pitt.edu
TÓM TẮT
Tinh chỉnh là điều cần thiết để thích ứng các mô hình ngôn ngữ lớn đã được huấn luyện trước với các ứng dụng downstream. Với sự phổ biến ngày càng tăng của các ứng dụng hỗ trợ LLM, tinh chỉnh đã được thực hiện một cách tích cực trên toàn thế giới, phát sinh một lượng chi phí tính toán khổng lồ tương ứng với dấu chân carbon lớn và tác động môi trường. Giảm thiểu tác động môi trường như vậy có liên quan trực tiếp đến việc giảm FLOPs tinh chỉnh. Các phương án tinh chỉnh hiện có tập trung vào việc tiết kiệm bộ nhớ hoặc giảm chi phí tính toán cập nhật trọng số, nhưng không thể đạt được sự giảm FLOPs đủ do họ bỏ qua chi phí huấn luyện trong lan truyền ngược. Để giải quyết hạn chế này, trong bài báo này chúng tôi trình bày GreenTrainer, một kỹ thuật mới nhằm tối thiểu hóa FLOPs của tinh chỉnh LLM thông qua lan truyền ngược thích nghi, chọn lọc một cách thích nghi tập hợp các tensor LLM phù hợp nhất cho tinh chỉnh dựa trên tầm quan trọng và chi phí lan truyền ngược của chúng trong huấn luyện. Kết quả thí nghiệm cho thấy GreenTrainer có thể tiết kiệm lên đến 64% FLOPs huấn luyện so với tinh chỉnh đầy đủ, mà không có bất kỳ mất mát độ chính xác đáng chú ý nào. So với các phương án hiện có như Prefix Tuning và LoRA, GreenTrainer có thể đạt được sự cải thiện độ chính xác mô hình lên đến 4%, với việc giảm FLOPs tương đương.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLMs) được sử dụng như các công cụ nền tảng trong AI sinh tạo. Để được sử dụng trong các ứng dụng downstream, một LLM đã được huấn luyện trước cần được tinh chỉnh sử dụng dữ liệu ứng dụng cụ thể (Devlin et al., 2018). Một cách trực quan, tinh chỉnh ít tốn kém tính toán hơn huấn luyện trước do lượng dữ liệu huấn luyện nhỏ hơn, nhưng nó có thể dẫn đến tiêu thụ năng lượng và dấu chân carbon cao đáng kể khi được thực hiện tích cực trên toàn thế giới. Được hỗ trợ bởi việc dân chủ hóa các LLM mã nguồn mở (Candel et al., 2023) và các API thuận tiện để vận hành những LLM này (Ott et al., 2019; Wolf et al., 2019), ngay cả các cá nhân không chuyên cũng có thể dễ dàng tinh chỉnh LLMs để tăng cường hiệu suất mô hình hoặc cá nhân hóa (Scialom et al., 2022; Wang and Gao, 2023). Ví dụ, khi một mô hình LLaMA-13B (Touvron et al., 2023) được tinh chỉnh bởi 10k người dùng sử dụng GPU A100-80GB, việc tinh chỉnh như vậy tiêu thụ 6.9 × nhiều giờ GPU hơn việc huấn luyện trước một mô hình GPT-3 (Brown et al., 2020) với 175B tham số. Lượng năng lượng tiêu thụ bởi việc tinh chỉnh như vậy có thể so sánh với lượng tiêu thụ của một số quốc gia kém phát triển, và lượng dấu chân carbon tương đương với 1000 × lượng được tạo ra bởi một chuyến bay New York-San Francisco (aii, 2023).

Giảm thiểu tác động môi trường như vậy hướng tới AI Xanh có liên quan trực tiếp đến việc giảm số lượng phép toán dấu phẩy động (FLOPs) của tinh chỉnh, đại diện cho lượng phép toán tính toán và do đó tiêu thụ năng lượng trong huấn luyện (Schwartz et al., 2020; Huang et al., 2023a). Tuy nhiên, hầu hết các kỹ thuật hiện có để tối ưu hóa tinh chỉnh LLM bị hạn chế trong việc giảm tiêu thụ bộ nhớ hơn là FLOPs (Malladi et al., 2023; Liao et al., 2023). Một số phương pháp khác giảm FLOPs bằng cách chỉ tinh chỉnh một số loại tham số mô hình nhất định như bias (Zaken et al., 2021), LayerNorm và trọng số lớp đầu ra (Lu et al., 2021), nhưng chúng làm suy giảm khả năng biểu đạt của mô hình và chỉ áp dụng được cho các tác vụ học tập không sinh tạo đơn giản. Thay vào đó, các nhà nghiên cứu đã đề xuất giữ các tham số mô hình gốc được đóng băng nhưng tiêm các tham số có thể huấn luyện bổ sung vào đầu vào (Lester et al., 2021; Liu et al., 2022) hoặc các lớp nội bộ (Li and Liang, 2021; Hu et al., 2023; Huang et al., 2023b). Các phương pháp dựa trên LoRA gần đây (Hu et al., 2021; Zhang et al., 2023) còn giảm thêm chi phí tính toán cập nhật trọng số cho các tham số được tiêm này thông qua xấp xỉ rank thấp. Những phương pháp này có thể tối thiểu hóa mất mát độ chính xác của mô hình trên các tác vụ sinh tạo. Tuy nhiên, chúng vẫn cần tính toán gradient kích hoạt qua toàn bộ mô hình và do đó việc giảm FLOPs của chúng bị hạn chế, bởi vì các phép tính cập nhật trọng số chỉ chiếm 25%-33% tổng FLOPs huấn luyện.

1arXiv:2309.13192v2  [cs.LG]  29 Feb 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bên cạnh việc tính toán cập nhật trọng số, FLOPs trong huấn luyện cũng được tạo ra trong i) lan truyền thuận và ii) lan truyền ngược của gradient kích hoạt. Vì lan truyền thuận hoàn chỉnh là cần thiết để tính toán mất mát huấn luyện, chúng tôi hình dung rằng chìa khóa để giảm FLOPs hơn nữa là tính đến chi phí lan truyền ngược của gradient kích hoạt, chiếm >33% tổng FLOPs huấn luyện, và có chọn lọc chỉ liên quan đến các cấu trúc mô hình phù hợp nhất trong lan truyền ngược. Tuy nhiên, thách thức chính là huấn luyện có chọn lọc có thể gây ra mất mát độ chính xác mô hình. Chúng tôi tối thiểu hóa mất mát độ chính xác bằng cách thích ứng việc lựa chọn như vậy trong lan truyền ngược với một mục tiêu linh hoạt về giảm FLOPs, được xác định bởi dấu chân carbon trong cung cấp năng lượng. Ví dụ, khi dấu chân carbon như vậy thấp do việc chèn năng lượng tái tạo, sử dụng mục tiêu giảm FLOPs thấp hơn có thể liên quan đến nhiều cấu trúc mô hình hơn trong huấn luyện và giữ lại độ chính xác huấn luyện. Dấu chân carbon cao, thay vào đó, dẫn đến mục tiêu giảm FLOPs cao hơn để chấp nhận AI Xanh tốt hơn.

epoch 1 epoch 3backprop trained frozen model params
epoch 1 epoch 2 epoch 3
Hình 1: GreenTrainer thích nghi lựa chọn phần phù hợp nhất của mô hình LLM để tinh chỉnh

Trong bài báo này, chúng tôi trình bày GreenTrainer, một kỹ thuật mới thực hiện lan truyền ngược thích nghi cho tinh chỉnh LLM hiệu quả với mất mát độ chính xác tối thiểu. Như được hiển thị trong Hình 1, với một mục tiêu giảm FLOPs, GreenTrainer thích nghi lựa chọn tập hợp các tensor mạng nơ-ron (NN) có thể huấn luyện trong mỗi epoch, dựa trên đánh giá tầm quan trọng của các tensor trong huấn luyện. Việc đánh giá tầm quan trọng như vậy là khó khăn vì các tensor NN không liên kết trực tiếp với các biến dữ liệu đầu vào hoặc các đặc trưng trung gian, và hầu hết các kỹ thuật phân bổ (Sundararajan et al., 2017; Hesse et al., 2021) đánh giá tầm quan trọng đặc trưng không áp dụng được. Các chỉ số tầm quan trọng phổ biến, bao gồm SNIP (Lee et al., 2018) và Fisher (Liu et al., 2021), chủ yếu được sử dụng trong cắt tỉa NN để định lượng tầm quan trọng của trọng số mô hình tại giá trị hiện tại của chúng, nhưng chúng không thể định lượng tầm quan trọng của việc cập nhật trọng số trên một tensor để giảm mất mát huấn luyện. Các chỉ số cổ điển dựa trên đóng góp độ chính xác chính xác (Lin et al., 2022), độ lớn của các cập nhật trọng số (Li et al., 2016), hoặc nhiễu loạn ngẫu nhiên (Breiman, 2001), mặt khác, hoặc không chính xác hoặc tốn kém tính toán cho LLMs. Thay vào đó, phương pháp của chúng tôi áp dụng một lý luận tương tự với các chỉ số phân bổ và cắt tỉa hiện có, và định lượng đóng góp của mỗi cập nhật tensor vào mất mát huấn luyện thông qua khai triển Taylor bậc nhất trên mất mát huấn luyện. Bằng cách này, chúng tôi đảm bảo rằng các tensor được chọn có thể đóng góp tối đa vào việc giảm mất mát huấn luyện.

Một thách thức khác là làm thế nào để chính xác lập hồ sơ FLOPs huấn luyện. Do sự phụ thuộc lẫn nhau giữa các tensor, tổng FLOPs của chúng trong huấn luyện không bằng tổng FLOPs riêng lẻ của chúng. Sự phụ thuộc lẫn nhau như vậy được xác định bởi các đặc tính lan truyền ngược của các toán tử NN trong mỗi tensor, nhưng các mô hình FLOPs hiện có không thể liên kết các toán tử NN với các tensor dựa trên luồng tính toán của lan truyền ngược. Một số công trình hiện có (Kwon et al., 2023) chỉ kết hợp FLOPs thuận theo lớp vào việc lựa chọn tensor, nhưng bỏ qua sự phụ thuộc tính toán giữa các lớp trong lan truyền ngược. Để giải quyết thách thức này, chúng tôi mô hình nghiêm ngặt các phụ thuộc chéo tensor trong việc lập hồ sơ FLOPs lan truyền ngược của chúng. Dựa trên mô hình này, chúng tôi phát triển một thuật toán quy hoạch động (DP) để tìm việc lựa chọn tensor gần tối ưu từ một số lượng khả năng theo cấp số nhân (ví dụ, 2515 cho 515 tensor trong mô hình OPT-2.7B (Zhang et al., 2022)). Do đó, GreenTrainer có thể đảm bảo rằng mục tiêu giảm FLOPs nhất định có thể được đáp ứng trong hầu hết các trường hợp.

Chúng tôi đánh giá GreenTrainer với ba LLM mã nguồn mở, cụ thể là OPT (Zhang et al., 2022), BLOOMZ (Muennighoff et al., 2022) và FLAN-T5 (Chung et al., 2022), trên các bộ dữ liệu sinh văn bản bao gồm SciTLDR (Cachola et al., 2020) và DialogSum (Chen et al., 2021). Kết quả của chúng tôi cho thấy GreenTrainer có thể tiết kiệm lên đến 64% FLOPs huấn luyện so với tinh chỉnh LLM đầy đủ, mà không có bất kỳ mất mát độ chính xác đáng chú ý nào. Trong một số trường hợp, GreenTrainer thậm chí có thể cải thiện độ chính xác mô hình so với tinh chỉnh đầy đủ, bằng cách loại bỏ sự dư thừa mô hình và overfitting. So với các kỹ thuật hiện có như Prefix Tuning (Li and Liang, 2021) và LoRA (Hu et al., 2021), GreenTrainer cải thiện độ chính xác mô hình 4% với cùng lượng giảm FLOPs, và cũng cung cấp cho người dùng sự linh hoạt để cân bằng giữa độ chính xác huấn luyện và chi phí tùy thuộc vào nhu cầu của AI Xanh.

2 KIẾN THỨC NỀN TẢNG & ĐỘNG LỰC

2.1 KIẾN TRÚC TRANSFORMER CHO SINH VĂN BẢN
Các LLM hiện tại được xếp chồng bởi các khối transformer (Vaswani et al., 2017), mỗi khối chứa một lớp Multi-Head Attention (MHA), LayerNorms (Ba et al., 2016), và một Feed-Forward Network (FFN).

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Cho một chuỗi đầu vào X∈Rn×d với n token, MHA chiếu các token vào không gian (Q, K, V) h lần, sử dụng h bộ chiếu có thể huấn luyện (W(i)
Q, W(i)
K, W(i)
V)i=1,...,h. Mỗi phép chiếu fi:
Rn×d→Rn×d
h được định nghĩa là Qi, Ki, Vi=XW(i)
Q, XW(i)
K, XW(i)
V. Đầu ra (Qi, Ki, Vi) sau đó thực hiện cơ chế attention để tạo ra Oi bằng cách trọng số Vi với điểm attention giữa Qi và Ki. Đầu ra cuối cùng của MHA được thu được bằng cách nối từng Oi, theo sau bởi một phép chiếu tuyến tính g:Rn×d→Rn×d với một bộ chiếu có thể huấn luyện Wo:

Oi= Softmax
QiK⊤
i/p
d/h
Vi, MHA out= Concat( O1, O2, ..., O h)Wo. (1)

Để cải thiện hiệu quả huấn luyện, LLMs áp dụng phương pháp teacher-forcing (Lamb et al., 2016) để sinh ra toàn bộ chuỗi token đầu ra trong một lần truyền thuận duy nhất. Cụ thể, các mặt nạ nhân quả được áp dụng vào điểm attention của MHA, để mỗi token đầu ra có thể được dự đoán từ các token nhãn ở các vị trí trước đó. Với kỹ thuật này, khi được tinh chỉnh, LLMs có thể được huấn luyện theo cách tiêu chuẩn như bất kỳ mô hình feed-forward nào.

2.2 NHU CẦU VỀ LAN TRUYỀN NGƯỢC THÍCH NGHI
Khi được tinh chỉnh cho một tác vụ downstream, LLMs thường được tham số hóa quá mức, vì chỉ một phần kiến thức thế giới mà chúng học được từ huấn luyện trước là hữu ích cho tác vụ mục tiêu. Trong những trường hợp này, chỉ liên quan một số cấu trúc con của mô hình vào tinh chỉnh có thể có ít tác động đến độ chính xác mô hình, nhưng giảm đáng kể lượng tính toán.

[THIS IS TABLE:
Bảng 1 showing training different substructures of OPT-2.7B and FLAN-T5-3B LLMs on DialogSum dataset with columns for Trainable substructure, OPT-2.7B metrics (FLOPs and Acc.), and FLAN-T5-3B metrics (FLOPs and Acc.)]

Các nghiên cứu hiện có đã thử nghiệm với các lựa chọn cố định của một số thành phần NN, như 2 lớp cuối, tiền tố bộ giải mã (Li and Liang, 2021), và các bộ chiếu tuyến tính (WQ, WV)(Hu et al., 2021), trong tinh chỉnh. Tuy nhiên, do các phụ thuộc lẫn nhau của các tham số NN (Jin et al., 2020), những lựa chọn cố định như vậy sẽ làm suy giảm đáng kể độ chính xác mô hình. Như được hiển thị trong Bảng 1, chỉ tinh chỉnh 2 lớp cuối hoặc tiền tố bộ giải mã dẫn đến giảm độ chính xác lên đến 10%. Lý do là các cấu trúc con NN gần đó có phụ thuộc lẫn nhau với các lựa chọn cố định bị loại trừ khỏi tinh chỉnh, và do đó trở nên không nhất quán với những cấu trúc con được chọn. Tăng mật độ lựa chọn, như bao gồm tất cả các bộ chiếu tuyến tính (WQ, WV), có thể giảm thiểu mất mát độ chính xác mô hình, nhưng chỉ có thể tiết kiệm tối đa 33% FLOPs do lan truyền ngược gradient kích hoạt qua các khối transformer. Các phương pháp ngây thơ của lựa chọn động, như mở rộng phần có thể huấn luyện từ lớp cuối, có hạn chế tương tự.

Sự thiếu sót của những phương pháp hiện có này thúc đẩy chúng tôi thực thi việc lựa chọn các cấu trúc con LLM linh hoạt và thích nghi hơn trong lan truyền ngược. Trong GreenTrainer, chúng tôi phát triển một chỉ số tầm quan trọng tensor kết hợp các phụ thuộc tham số để đánh giá cách tinh chỉnh mỗi tensor đóng góp vào độ chính xác của mô hình được huấn luyện tại thời gian chạy. Kiến thức về tầm quan trọng tensor như vậy, sau đó, cho phép chúng tôi đạt được sự giảm FLOPs mong muốn trong khi tối đa hóa độ chính xác mô hình.

Lớp 1
Lớp 2
Lớp 3
Lớp 4Dự đoán Dữ liệu đầu vào
Nhãn
thuận
ngượcdy4 dy3 dy2
dw1 dw2 dw3 dw4y2 y1 y3 y4

Hình 2: Lan truyền ngược của một NN dày đặc 4 lớp

2.3 MÔ HÌNH FLOPS CỦA LAN TRUYỀN NGƯỢC
Thiết kế của GreenTrainer dựa vào việc tính toán chính xác FLOPs lan truyền ngược của các cấu trúc con mô hình được chọn, có thể được phân tách thành hai phần sử dụng quy tắc dây chuyền. Ví dụ, như

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
được hiển thị trong Hình 2, khi huấn luyện một NN dày đặc 4 lớp không có bias, mỗi lớp tính toán i) dyi là gradient của mất mát L đối với kích hoạt yi, và ii) dwi là gradient mất mát đối với trọng số Wi, sao cho

dyi=∂L
∂yi=∂L
∂yi+1W⊤
i= dy i+1W⊤
i, dwi=∂L
∂Wi=y⊤
i∂L
∂yi+1=y⊤
idyi+1, (2)

và lượng FLOPs tương ứng để tính toán dyi và dwi là tdyi và tdwi, tương ứng.
(dyi,dwi) có thể được tính toán từ (dyi+1,dwi+1). Cụ thể, ngay cả khi một lớp không được chọn trong tinh chỉnh, nó vẫn cần tính toán và truyền gradient lỗi (dyi) đến các lớp downstream. Do đó, lượng tính toán trong lan truyền ngược không chỉ phụ thuộc vào các lớp được chọn, mà còn phụ thuộc vào một số lớp không được chọn. Ví dụ, nếu chỉ Lớp 2 có thể huấn luyện, tổng FLOPs cho lan truyền ngược sẽ được quyết định bởi chi phí tính toán dw2, dy3 và dy4. Do tính tổng quát của quy tắc dây chuyền, lý luận như vậy về tính toán FLOPs cũng áp dụng được cho các loại lớp NN khác.

Dựa trên lý luận này, chúng tôi có thể xây dựng các mô hình FLOPs cho các cấu trúc con LLM. Mô hình cấp lớp thô sơ và có thể dẫn đến việc lựa chọn tensor không chính xác. Một số tham số quan trọng có thể không được chọn do những tham số không quan trọng khác trong cùng lớp. Trong GreenTrainer, chúng tôi sử dụng độ chi tiết cấp tensor cho việc lựa chọn như vậy, có thể được hỗ trợ tốt bởi các thư viện NN tensorized (ví dụ, TensorFlow (Abadi, 2016) và PyTorch (Paszke et al., 2019)). Lựa chọn cấp trọng số, mặc dù chi tiết hơn, quá tốn kém tính toán do yêu cầu lập chỉ mục chi tiết.

3 PHƯƠNG PHÁP
Để giảm FLOPs của tinh chỉnh LLM, một công thức vấn đề trực quan là tối thiểu hóa FLOPs trong khi đạt được độ chính xác mô hình mong muốn. Tuy nhiên, khó xác định một mục tiêu độ chính xác phù hợp trước, vì một số mục tiêu độ chính xác có thể yêu cầu huấn luyện rất tích cực và độ chính xác mà chúng ta có thể đạt được với ngân sách FLOPs của chúng ta không thể được ước tính trước khi huấn luyện. Thay vào đó, chúng tôi tối đa hóa việc giảm mất mát huấn luyện trong khi đạt được sự giảm FLOPs mong muốn:

max ∆ loss(m)s.t.Tselective (m)≤ρTfull, (3)

trong đó m là một vector nhị phân cần giải quyết cho việc lựa chọn tensor. m tham số hóa cả việc giảm mất mát (∆loss) và FLOPs trên mỗi batch của huấn luyện (Tselective), và Tselective bị hạn chế trong một tỷ lệ do người dùng chỉ định (ρ) của FLOPs của tinh chỉnh toàn bộ mô hình (Tfull). Ví dụ, ρ= 0.5 có nghĩa là FLOPs của tinh chỉnh phải nhiều nhất 50% của việc tinh chỉnh toàn bộ mô hình.

Trong thực tế, giá trị của ρ có thể được đặt trước hoặc điều chỉnh tại thời gian chạy trong bất kỳ giai đoạn nào của huấn luyện.

Để xác định đóng góp của mỗi tensor trong tinh chỉnh, chúng tôi mô hình ∆loss(m) như tầm quan trọng tổng hợp của các tensor được chọn, và tính toán FLOPs phát sinh bởi các tensor được chọn sử dụng mô hình FLOPs của lan truyền ngược trong Phần 2.3. Với mô hình này, Eq. (3) có thể được viết lại là:

max ∆ loss(m)s.t.Tfp+m·tdw+σ(m)·tdy≤ρTfull, (4)

trong đó Tfp biểu thị FLOPs trên mỗi batch của lần truyền thuận, và mỗi cặp biến trong (tdy,tdw) đại diện cho FLOPs của việc tính toán (dy,dw) cho tensor tương ứng, tương ứng. Cho một bộ chọn nhị phân m, σ(m) kết hợp tất cả các tensor dọc theo lần truyền ngược đóng góp vào FLOPs của tinh chỉnh, bằng cách tham gia vào việc truyền gradient lỗi (dy). Ví dụ, nếu m= [0,0,1,0,1,0,0], tất cả các tensor ở các lớp sâu hơn so với các tensor được chọn đều tham gia vào việc truyền gradient lỗi, và do đó σ(m) = [0,0,1,1,1,1,1].

Để căn cứ công thức này và giải quyết m, GreenTrainer bao gồm ba thành phần chính: (i) Lập hồ sơ FLOPs Tensor, tính toán FLOPs của tất cả các tensor NN (tức là, tdy và tdw) trước khi huấn luyện; (ii) Đánh giá Tầm quan trọng Tensor, định lượng đóng góp của việc cập nhật mỗi tensor NN vào chất lượng huấn luyện tại thời gian chạy; (iii) Bộ chọn Tensor, căn cứ vấn đề lựa chọn tensor sử dụng FLOPs và tầm quan trọng của các tensor, và cung cấp các giải pháp thông qua quy hoạch động tại thời gian chạy.

3.1 LẬP HỒ SƠ FLOPS TENSOR
Các bộ lập hồ sơ NN tiêu chuẩn, như Torch Profiler (Paszke et al., 2019), có thể đo FLOPs thực thi của các toán tử NN riêng lẻ như nhân ma trận và tích chập. Tuy nhiên, nó không thể được liên kết trực tiếp với các tensor NN tham gia vào những phép toán này. Khi một tập hợp tensor được huấn luyện, FLOPs huấn luyện của lan truyền ngược không bằng tổng FLOPs riêng lẻ của các tensor.

Để giải quyết hạn chế này, phương pháp của chúng tôi bao gồm hai bước. Đầu tiên, chúng tôi chuyển đổi cấu trúc NN dựa trên lớp của LLMs thành một đồ thị tính toán cấp tensor, giữ lại thứ tự thực thi của tất cả

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Input Embed. 
TensorProjector Q Projector K Projector V Bias BiasOutput Embed. 
Tensor
0,0Tensors in MHA layer in 1st block
Tensor -level 
Graph:
Tensor FLOPs 
𝑡𝑡𝑑𝑑𝑑𝑑,𝑡𝑡𝑑𝑑𝑑𝑑:𝑇𝑇,𝑇𝑇 0,𝑇𝑇𝑇 𝑇𝑇,𝑇𝑇 0,𝑇𝑇𝑇 𝑇𝑇+𝑇𝑇att,𝑇𝑇 ∑𝑡𝑡𝑑𝑑𝑑𝑑,𝑇𝑇𝑇𝑇variable 
assign.Related 
Backprop Ops:matmul_1
matmul_2matmul_3
matmul_4add_1 add_2matmul_5
matmul_6matmul _{N-1}
matmul _{N}
Match & Agg. 
FLOPs
❹❶
❷
❸Backprop direction

Hình 3: Một quy trình mẫu của lập hồ sơ FLOPs tensor

các tensor tham gia vào huấn luyện. Sau đó, chúng tôi trích xuất các toán tử lan truyền ngược liên quan của mỗi tensor, và dẫn xuất FLOPs của mỗi tensor i trong lan truyền ngược (tdyi và tdwi) bằng cách khớp và tổng hợp FLOPs của những toán tử NN này. Ví dụ trong Hình 3, việc huấn luyện mỗi bộ chiếu tuyến tính (Q, K và V) trong một lớp MHA phải được thực thi sau khi huấn luyện tensor bias tương ứng của nó. Việc huấn luyện mỗi bộ chiếu tuyến tính, sau đó, sẽ liên quan đến hai toán tử nhân ma trận, có FLOPs trong lan truyền ngược sẽ được tổng hợp. Chúng tôi phân loại những quy tắc khớp và tổng hợp như vậy theo loại lớp LLM nơi các tensor được đặt, như được mô tả dưới đây. Một ví dụ cụ thể về việc lập hồ sơ FLOPs tensor như vậy trên mô hình OPT-2.7B được cung cấp trong Phụ lục A.3.

Các lớp embedding đầu vào & đầu ra. Lớp embedding đầu vào chứa một tensor embedding có thể huấn luyện ánh xạ mỗi token thô thành một biểu diễn dày đặc. Cho gradient kích hoạt dyi+1 từ các lớp upstream, việc dẫn xuất cập nhật của tensor này chỉ liên quan đến gán biến, và chúng ta có thể an toàn xem tdwi≈0 cho bất kỳ tensor i nào. Nếu một token thô được ánh xạ đến vector thứ k trong tensor embedding trong lần truyền thuận, thì trong lan truyền ngược, dyi+1 từ upstream sẽ chỉ được gán cho hàng thứ k của dwi, sao cho dwi[s] = dyi+1 nếu s=k, ngược lại dwi[s] = 0. Vì lớp đầu vào không truyền gradient kích hoạt, chúng ta cũng có thể kết luận rằng tdy của nó là 0.

Ngược lại, lớp embedding đầu ra chiếu mỗi token trở lại không gian xác suất. Một cách trực quan, (tdy, tdw) của nó có thể được dẫn xuất theo cùng cách như chúng ta đã làm cho lớp dày đặc trong Eq. (2). Tuy nhiên, trong hầu hết LLMs, lớp embedding đầu ra chia sẻ cùng tensor có thể huấn luyện với lớp embedding đầu vào. Điều này ngụ ý rằng nếu embedding đầu ra có thể huấn luyện, thì embedding đầu vào cũng sẽ được liên quan đến huấn luyện. Do đó, tất cả tdy từ đầu ra của LLM, lên đến lớp embedding đầu vào, phải được tích lũy vào tdy của tensor embedding đầu ra, trong khi tdw của nó không thay đổi.

Lớp Multi-Head Attention (MHA). Một lớp MHA chứa nhiều bộ chiếu tuyến tính như các tensor có thể huấn luyện, và FLOPs của chúng trong huấn luyện có thể được dẫn xuất theo cùng cách như chúng ta đã làm với lớp dày đặc trong Eq. (2). Một số LLMs (ví dụ, OPT) cũng bao gồm bias như một loại tensor có thể huấn luyện khác sau phép chiếu như vậy. Trong trường hợp này, dựa trên quy tắc dây chuyền, lan truyền ngược của bias được tính toán là dyi= dyi+1 và dwi= 1⊤dyi+1, cho thấy rằng tdy cho bias là 0 vì dyi được truyền giống hệt từ dyi+1. tdw của bias có thể được dẫn xuất là FLOPs của việc cộng các phần tử trong dyi+1 dọc theo mỗi kênh đặc trưng. Cơ chế attention trong Eq. (1) được lan truyền ngược trước các bộ chiếu. Nếu bất kỳ bộ chiếu nào trong số này được liên quan đến huấn luyện, FLOPs lan truyền ngược của attention cũng phải được tính toán, và chúng tôi tích lũy FLOPs như vậy vào tdy của tensor bộ chiếu tương ứng (WV).

LayerNorm. Cho một token, LayerNorm đầu tiên chuẩn hóa các đặc trưng của nó và sử dụng hai tensor có thể huấn luyện γ và β để nhân theo phần tử và cộng vào token, tương ứng. Các phép toán nhân và cộng tương tự như những phép toán trong lớp dày đặc, và do đó FLOPs của nó có thể được tính toán theo cách tương tự. Tuy nhiên, FLOPs lan truyền ngược của các toán tử chuẩn hóa phải được tích lũy vào tdy của tensor trước đó. Nếu bất kỳ tensor nào trong các lớp trước đó được huấn luyện, FLOPs của việc truyền qua các toán tử chuẩn hóa cũng phải được bao gồm trong FLOPs của lớp hiện tại.

Feed-Forward Network (FFN). Trong FFN, có một hàm kích hoạt phi tuyến giữa hai lớp dày đặc. Theo cùng phương pháp tính toán FLOPs của LayerNorm, chúng tôi tích lũy FLOPs của việc truyền qua hàm kích hoạt này vào tdy của tensor bias trong lớp dày đặc đầu tiên.

3.2 ĐÁNH GIÁ TẦM QUAN TRỌNG TENSOR
Tầm quan trọng của một tensor trong huấn luyện có thể được ước tính là tổng tầm quan trọng của tất cả các trọng số của nó. Trong huấn luyện, vì các trọng số mô hình được cập nhật lặp đi lặp lại để tối thiểu hóa mất mát huấn luyện, một phương pháp trực quan để đánh giá tầm quan trọng của việc cập nhật trọng số trong một lần lặp nhất định là hoàn tác cập nhật này và kiểm tra mất mát huấn luyện tăng lên như thế nào là ∆L=L(w)−L(w+ ∆w), để một giá trị ∆L cao hơn có nghĩa là cập nhật này quan trọng hơn và trọng số nên được chọn.

Tuy nhiên, việc tính toán ∆L cho mỗi trọng số là tốn kém. Thay vào đó, chúng tôi ước tính tầm quan trọng của tất cả trọng số trong một lần bằng cách làm mịn phép toán hoàn tác được mô tả ở trên và tính toán gradient mất mát đối với các cập nhật tương ứng với tất cả các trọng số. Để c∈[0,1]M biểu thị phép toán hoàn tác cho tất cả M trọng số, chúng ta có thể tính toán gradient mất mát là

−∂L(w+c⊙∆w)
∂c=−∆w⊙∂L(u)
∂u
u=w+c⊙∆w, (5)

trong đó ⊙ biểu thị phép nhân theo phần tử. Khi c=0, Eq. (5) trở thành một vector tầm quan trọng trên tất cả trọng số. Vì gradient mất mát được tham số hóa bởi tất cả trọng số, tầm quan trọng trọng số được tính toán theo cách này ngầm kết hợp tác động của các phụ thuộc trọng số. Tầm quan trọng của tensor k sau đó được tính toán là

Ik=−X
i∆w(k)
i∂L/∂w(k)
i. (6)

Trong một số trường hợp, khi quá trình huấn luyện gặp phải sự phân kỳ, giá trị của gradient và tầm quan trọng tensor được tính toán trong Eq. (6) có thể rất lớn, cuối cùng dẫn đến tràn số khi sử dụng những giá trị tầm quan trọng này để quyết định lựa chọn tensor trong Eq. (4). Để giải quyết vấn đề này, chúng ta có thể thêm tỷ lệ tất cả tầm quan trọng tensor theo biên độ tối đa để cải thiện tính ổn định số.

1 𝑇𝑇𝑏𝑏𝑏𝑏𝑡𝑡… …
1…
𝑘𝑘
𝑁𝑁… P[ k, t ]Subproblem 
Table  (𝑁𝑁×𝑇𝑇𝑏𝑏𝑏𝑏)
backprop depth ≤ kbackprop FLOPs  ≤ t
k 1 k-11……1……1…

(a) Định nghĩa bài toán con

k-1 k-2 k-3 1 kP[k-1, t]
P[k, t] = ?không
được chọn
k-1 k-2 k-3 1 kP[k-1, t]
P[k, t] = ?được chọn
không được chọn được chọnTrường hợp 1:
Trường hợp 2: (b) Tìm quan hệ truy hồi

Hình 4: Giải quyết vấn đề lựa chọn tensor sử dụng DP

3.3 LỰA CHỌN TENSOR
Vì Eq. (4) là một vấn đề lập trình số nguyên phi tuyến và do đó NP-hard, trong GreenTrainer chúng tôi tìm kiếm một giải pháp xấp xỉ sử dụng quy hoạch động (DP). Chúng tôi phân tách toàn bộ vấn đề thành các bài toán con bị hạn chế bởi các độ sâu khác nhau của lan truyền ngược. Những bài toán con này có thể được giải quyết tuần tự từ bài toán con có độ sâu nhỏ nhất, bằng cách sử dụng quan hệ truy hồi của chúng.

Định nghĩa bài toán con. Như được hiển thị trong Hình 4(a), chúng tôi định nghĩa mỗi bài toán con P[k, t] là tối đa hóa tầm quan trọng tích lũy của các tensor được chọn khi 1) lựa chọn nằm trong k tensor hàng đầu1 và 2) FLOPs lan truyền ngược nhiều nhất là t. DP bắt đầu bằng cách giải quyết bài toán con nhỏ nhất P[k= 1, t=1] và dần dần giải quyết các bài toán con lớn hơn dựa trên kết quả của các bài toán con nhỏ hơn và quan hệ truy hồi của những bài toán con này, cho đến khi vấn đề mục tiêu P[N, Tfull] được giải quyết.

Quan hệ truy hồi của các bài toán con. Quan hệ truy hồi giữa bài toán con P[k, t] và P[k−1, t] phụ thuộc vào việc chúng ta có thêm chọn tensor hàng đầu k từ giải pháp của P[k−1, t] hay không, như được hiển thị trong Hình 4(b). Trường hợp 1: Nếu k không được chọn, P[k, t] sẽ trở về P[k−1, t], vì tầm quan trọng của các tensor được chọn sẽ không được tăng thêm. Trường hợp 2: Nếu k được chọn, thì FLOPs của nó sẽ được bao gồm vào giải pháp của P[k, t], bất kể những tensor nào khác được chọn. FLOPs liên quan đến tensor k bao gồm 1) FLOPs để cập nhật tensor k và 2) FLOPs để truyền gradient kích hoạt từ tensor được chọn gần nhất kc, như tensor k−3 như được hiển thị trong Hình 4(b), đến tensor k. Điều này ngụ ý rằng P[k, t] trở về một bài toán con đã được giải quyết trước đó P[k−kc, t−∆t], trong đó

∆t=tdwk+Xk−1
j=kctdyj. (7)

Vì kc không được biết trước, chúng tôi truy ngược các bài toán con đã được giải quyết trước đó và khám phá tất cả các khả năng của kc bằng cách giảm độ sâu lan truyền ngược từ k, và giải pháp tối ưu cho P[k, t] là giải pháp có tầm quan trọng tích lũy cao nhất của các tensor được chọn. Dựa trên quan hệ truy hồi này, chúng ta có thể giải quyết tất cả các bài toán con bằng cách duyệt không gian bài toán con. Độ phức tạp thời gian của việc giải quyết mỗi bài toán con là O(N), và độ phức tạp thời gian tổng thể của DP là O(N2Tfull).

4 THÍ NGHIỆM
Trong đánh giá của chúng tôi, chúng tôi bao gồm các LLM chỉ giải mã bao gồm OPT (Zhang et al., 2022) và BLOOMZ (Muennighoff et al., 2022), và một LLM mã hóa-giải mã, cụ thể là FLAN-T5 (Chung et al., 2022), với kích thước LLM từ 350M đến 6.7B. Các thí nghiệm của chúng tôi chủ yếu được tiến hành sử dụng hai bộ dữ liệu tóm tắt trừu tượng sau:

•SciTLDR (Cachola et al., 2020) là một bộ dữ liệu gồm 5.4K tóm tắt văn bản về 3.2K bài báo. Nó chứa cả TLDRs do tác giả viết và do chuyên gia dẫn xuất, trong đó loại sau được thu thập bởi một giao thức chú thích tạo ra các tóm tắt chất lượng cao với gánh nặng chú thích thấp.

•DialogSum (Chen et al., 2021) là một bộ dữ liệu tóm tắt đối thoại gồm 13,460 đối thoại với các tóm tắt và chủ đề được gắn nhãn thủ công. Nó đã được chứng minh là thách thức hơn các bộ dữ liệu tóm tắt khác, như SAMSum (Gliwa et al., 2019) và CNN/Daily (Nallapati et al., 2016) ở quy mô tương tự.

Chúng tôi cũng thực hiện các tác vụ QA sinh tạo trên các bộ dữ liệu WebQuestion (Berant et al., 2013) và PIQA (Bisk et al., 2020) trong Phụ lục A.4. Tuy nhiên, chúng tôi không xem xét các tác vụ không sinh tạo như phân loại cảm xúc, phân loại hệ quả và QA trích xuất, vì những tác vụ này quá dễ dàng cho LLMs và việc kiểm tra chúng với LLMs sẽ dẫn đến kết quả hiệu suất được phóng đại so với baseline.

Đối với OPT và BLOOMZ, chúng tôi tuân theo cấu trúc prompt giống GPT2 (Radford et al., 2019), "[source seq.] TL;DR:", cho các tác vụ tóm tắt để tiền xử lý dữ liệu đầu vào. Đối với FLAN-T5, chúng tôi áp dụng cấu trúc prompt "summarize: [source seq.]" được sử dụng trong huấn luyện trước T5 gốc. Chúng tôi cắt ngắn các chuỗi nguồn để độ dài của mỗi chuỗi đầu vào được tiền xử lý nằm trong 512 token. Trên dữ liệu kiểm tra, chúng tôi sử dụng kích thước tìm kiếm chùm là 4, và đặt số lượng token được sinh tối đa là 64 cho SciTLDR và 128 cho DialogSum. Chúng tôi so sánh GreenTrainer (GT) với các baseline sau:

•Full Fine-Tuning (Full FT) tinh chỉnh tất cả các tham số LLM và sẽ đạt được độ chính xác tốt nhất của mô hình được huấn luyện một cách trực quan.

•Fine-Tuning Top2 (FT-Top2) chỉ tinh chỉnh hai lớp cuối, thường là lớp embedding và một LayerNorm. Các lớp embedding đầu vào và đầu ra được liên kết cho OPT và BLOOMZ, nhưng không được liên kết cho FLAN-T5. Baseline ngây thơ này chỉ tinh chỉnh phần nhỏ nhất của các tham số LLM và được sử dụng để xác định liệu bộ dữ liệu có tầm thường đối với LLM hay không.

•Prefix Tuning (Prefix-T) (Li and Liang, 2021) chèn các tiền tố có thể huấn luyện vào chuỗi đầu vào của mỗi khối transformer trong khi đóng băng các tham số mô hình. Đối với LLMs mã hóa-giải mã, các tiền tố có thể huấn luyện chỉ được chèn vào các khối giải mã.

•LoRA (Hu et al., 2021) hiện tại là phương pháp phổ biến nhất cho tinh chỉnh LLM hiệu quả. Nó sử dụng phân tách ma trận rank thấp để giảm chi phí huấn luyện. Chúng tôi áp dụng LoRA vào cả bộ chiếu truy vấn và giá trị, như được đề xuất trong (Hu et al., 2021).

Trong tất cả các thí nghiệm, chúng tôi sử dụng kích thước batch là 4 và tinh chỉnh mô hình trong 5 epoch. Chúng tôi sử dụng bộ tối ưu AdamW (Loshchilov and Hutter, 2017) với tốc độ học 2×10−5 với lịch trình tuyến tính và suy giảm trọng số 10−2. Chúng tôi sử dụng điểm ROUGE (%R1/R2/RL) (Lin, 2004) làm chỉ số độ chính xác, và đo cả Peta-FLOPs (PFLOPs) và thời gian wall-clock làm chi phí huấn luyện trong mỗi lần chạy. Chúng tôi đo chi phí end-to-end của huấn luyện, bao gồm chi phí tính toán trong các lần truyền thuận và ngược, và chi phí tính toán của đánh giá tầm quan trọng tensor và lựa chọn tensor sử dụng DP.

4.1 CHI PHÍ HUẤN LUYỆN & ĐỘ CHÍNH XÁC
Chúng tôi đầu tiên đánh giá chi phí huấn luyện và độ chính xác của GreenTrainer (GT). Như được hiển thị trong Bảng 2, đối với mô hình OPT-2.7B, GT-0.5 có thể đạt được yêu cầu giảm 50% FLOPs với mất mát độ chính xác nhiều nhất 2%, và GT-0.7 thậm chí có thể đạt được điểm ROUGE cao hơn 0.2%-3% so với Full FT. Chúng tôi đưa ra giả thuyết rằng điều này là do GT chỉ tinh chỉnh các tensor quan trọng nhất và do đó giảm thiểu overfitting có thể có trong Full FT. Các tham số có thể huấn luyện không đủ cũng có thể dẫn đến underfitting, như FT-Top2 có điểm ROUGE thấp hơn đáng kể. Tương tự, so với LoRA và Prefix Tuning, GT-0.7 đạt được độ chính xác cao hơn ít nhất 2% với cùng lượng FLOPs huấn luyện.

Tương tự, đối với BLOOMZ-3B, GT-0.5 có thể tiết kiệm 50% FLOPs huấn luyện và thời gian wall-clock với <2% mất mát độ chính xác. So với Full FT, GT-0.7 đạt được điểm ROUGE tương tự trên SciTLDR, và cao hơn 4%-10% trên DialogSum. Với cùng lượng FLOPs huấn luyện, GT-0.7 có điểm ROUGE cao hơn 0.4%-1.4% so với LoRA. Lưu ý rằng cả hai bộ dữ liệu đều không tầm thường đối với mô hình BLOOMZ, vì baseline ngây thơ (FT-Top2) vẫn thể hiện mất mát độ chính xác cao.

Đối với mô hình FLAN-T5-3B, FT-Top2 đạt được chất lượng tinh chỉnh tương tự với Full FT với FLOPs thấp hơn, cho thấy rằng bộ dữ liệu SciTLDR là tầm thường đối với FLAN-T5. Trong trường hợp này, GT-0.34 có thể đạt được cùng FLOPs và điểm ROUGE bằng cách chọn một phần nhỏ tensor. Mặt khác, FT-Top2 mất độ chính xác đáng kể trên DialogSum, nhưng GT-0.4 giảm 54% FLOPs huấn luyện và 43% thời gian wall-clock mà không có mất mát độ chính xác đáng chú ý. GT-0.4 cũng vượt trội hơn LoRA 1% về điểm ROUGE và giảm 11% FLOPs hơn. So với Prefix tuning, GT-0.34 đạt được điểm ROUGE cao hơn 2%-5%, trong khi giảm cùng lượng FLOPs huấn luyện.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

# Mô hình
& Phươngpháp	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
OPT-2.7B
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
FT-Top2	29.0 (31% ↓)	0.61 (34% ↓)	9.1/4.0/7.6	181.6 (31% ↓)	3.8 (31% ↓)	20.8/7.9/17.5
Prefix-T	27.9 (33% ↓)	0.58 (37% ↓)	7.6/0.4/6.1	174.7 (33% ↓)	3.7 (33% ↓)	13.4/3.3/10.9
LoRA	27.9 (33% ↓)	0.59 (36% ↓)	28.2/12.1/21.0	174.7 (33% ↓)	3.6 (35% ↓)	23.8/9.5/18.8
GT-0.5	20.8 (50% ↓)	0.46 (50% ↓)	30.5/13.1/25.2	130.1 (50% ↓)	2.7 (51% ↓)	21.4/8.2/17.6
GT-0.7	29.2 (30% ↓)	0.68 (26% ↓)	33.1/15.2/27.6	182.7 (30% ↓)	4.0 (27% ↓)	26.8/11.0/21.6

BLOOMZ-3B
Full FT	47.2	1.0	28.3/12.1/22.5	294.8	6.5	26.1/10.6/21.0
FT-Top2	36.5 (23% ↓)	0.75 (25% ↓)	23.7/8.8/18.8	227.9 (23% ↓)	4.6 (29% ↓)	22.1/8.5/17.8
Prefix-T	31.5 (33% ↓)	0.68 (34% ↓)	6.5/2.2/5.5	196.5 (33% ↓)	4.2 (35% ↓)	29.6/9.4/24.9
LoRA	31.5 (33% ↓)	0.69 (33% ↓)	27.4/11.7/21.8	196.5 (33% ↓)	4.3 (34% ↓)	35.4/14.3/28.6
GT-0.5	23.4 (51% ↓)	0.51 (50% ↓)	26.7/10.7/21.2	146.4 (50% ↓)	3.1 (52% ↓)	24.9/9.5/20.0
GT-0.7	32.3 (32% ↓)	0.74 (28% ↓)	28.0/12.2/22.4	204.7 (31% ↓)	4.3 (34% ↓)	36.8/14.7/29.4

FLAN-T5-3B
Full FT	21.7	0.64	37.1/18.5/31.7	135.7	4.0	46.5/20.8/38.5
FT-Top2	7.3 (66% ↓)	0.21 (67% ↓)	36.5/18.4/31.5	46.1 (66% ↓)	1.4 (65% ↓)	39.2/16.7/32.9
Prefix-T	8.0 (63% ↓)	0.23 (64% ↓)	36.0/18.2/31.0	55.3 (60% ↓)	1.7 (57% ↓)	37.6/16.4/32.1
LoRA	14.4 (33% ↓)	0.41 (36% ↓)	36.6/18.5/31.5	90.5 (33% ↓)	2.5 (38% ↓)	44.7/19.8/37.1
GT-0.34	7.5 (65% ↓)	0.23 (64% ↓)	36.4/18.4/31.7	53.5 (61% ↓)	1.4 (65% ↓)	42.7/18.3/35.1
GT-0.4	10.0 (54% ↓)	0.38 (41% ↓)	36.7/18.5/31.5	62.5 (54% ↓)	2.3 (43% ↓)	46.0/20.7/38.1
GT-0.5	12.4 (43% ↓)	0.44 (31% ↓)	36.3/17.7/30.9	77.6 (43% ↓)	2.6 (35% ↓)	46.2/20.7/38.1

Bảng 2: So sánh chi phí huấn luyện & độ chính xác trong tinh chỉnh LLM. GreenTrainer với mục tiêu ρ giảm FLOPs được ký hiệu là GT-ρ.

4.2 TÁC ĐỘNG CỦA MỤC TIÊU GIẢM FLOPS
Để hiểu rõ hơn cách GreenTrainer hoạt động với các mục tiêu giảm FLOPs khác nhau, chúng tôi thay đổi giá trị của ρ giữa 0.36 và 0.8, và so sánh GreenTrainer với LoRA trên mô hình OPT-2.7B. Như được hiển thị trong Bảng 3, trên bộ dữ liệu SciTLDR, khi yêu cầu giảm FLOPs cao và tương ứng với giá trị ρ≤0.4, GreenTrainer vượt trội hơn LoRA bằng cách đạt được điểm ROUGE cao hơn 2% và tiết kiệm 25% FLOPs và thời gian wall-clock hơn. Mặt khác, khi giá trị ρ tăng lên 0.6, GreenTrainer vượt trội hơn Full FT về điểm ROUGE 0.5% và vượt trội hơn LoRA 5.2%, nhưng tiết kiệm 40% FLOPs huấn luyện và 39% thời gian wall-clock so với Full FT. Kết quả tương tự cũng được quan sát trên bộ dữ liệu DialogSum. Tóm lại, với các mục tiêu giảm FLOPs khác nhau, GreenTrainer luôn có thể cung cấp sự cân bằng tốt hơn giữa độ chính xác huấn luyện và chi phí, so với các baseline SOTA.

Phương pháp	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
LoRA	27.9 (33% ↓)	0.59 (36% ↓)	28.2/12.1/21.0	174.7 (33% ↓)	3.6 (35% ↓)	23.8/9.5/18.8
GT-0.36	14.9 (64% ↓)	0.32 (65% ↓)	4.1/1.7/3.6	92.9 (65% ↓)	1.9 (65% ↓)	15.7/5.0/13.8
GT-0.4	16.6 (60% ↓)	0.36 (61% ↓)	28.6/11.6/23.5	103.4 (61% ↓)	2.2 (60% ↓)	17.9/6.3/15.4
GT-0.5	20.8 (50% ↓)	0.46 (50% ↓)	30.5/13.1/25.2	130.1 (50% ↓)	2.7 (51% ↓)	21.4/8.2/17.6
GT-0.6	25.0 (40% ↓)	0.56 (39% ↓)	33.4/15.3/27.8	156.6 (40% ↓)	3.3 (40% ↓)	24.0/9.7/19.2
GT-0.7	29.2 (30% ↓)	0.68 (26% ↓)	33.1/15.2/27.6	182.7 (30% ↓)	4.0 (27% ↓)	26.8/11.0/21.6
GT-0.8	33.4 (20% ↓)	0.77 (16% ↓)	33.1/15.5/27.6	209.6 (20% ↓)	4.4 (20% ↓)	23.9/9.9/19.1

Bảng 3: Tác động của các mục tiêu giảm FLOPs khác nhau trên mô hình OPT-2.7B

Những kết quả này cũng chứng minh rằng GreenTrainer cung cấp sự linh hoạt tuyệt vời trong tinh chỉnh LLM giữa độ chính xác huấn luyện và chi phí, bằng cách điều chỉnh giá trị của ρ. Người dùng có thể chọn đặt giá trị ρ thấp (≤0.4) để tối đa hóa việc giảm FLOPs (>60%) với mất mát độ chính xác mô hình vừa phải (3%-4% trên hai bộ dữ liệu chúng tôi sử dụng). Ngoài ra, họ có thể sử dụng giá trị ρ cao (≥0.6) để có cùng mức độ giảm FLOPs như của LoRA, nhưng đảm bảo mất mát độ chính xác mô hình tối thiểu hoặc thậm chí cải thiện độ chính xác mô hình nhỏ. Chúng tôi tin rằng sự linh hoạt như vậy có tầm quan trọng thực tế khi tinh chỉnh LLMs cho các tác vụ downstream với các yêu cầu và ràng buộc AI xanh khác nhau.

4.3 HIỆU QUẢ CỦA CÁC CHỈ SỐ TẦM QUAN TRỌNG TENSOR
Chất lượng tinh chỉnh của GreenTrainer dựa trên việc đánh giá phù hợp tầm quan trọng tensor. Chúng tôi so sánh chỉ số của chúng tôi (∆w∂L∂w) với chỉ số dựa trên độ lớn (∆w) (Lee et al., 2020) và chỉ số chỉ gradient (∂L∂w) (Aji and Heafield, 2017), sử dụng mô hình OPT-2.7B với ρ=0.7. Như được hiển thị

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Phương pháp	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
GT-0.7 (∆w)	29.4 (30% ↓)	0.68 (26% ↓)	32.7/15.2/27.2	183.8 (30% ↓)	4.0 (27% ↓)	24.9/10.2/19.7
GT-0.7 (∂L∂w)	29.4 (30% ↓)	0.67 (27% ↓)	32.8/15.1/27.2	184.0 (30% ↓)	4.0 (27% ↓)	25.0/10.2/20.0
GT-0.7 (∆w∂L∂w)	29.2 (30% ↓)	0.68 (26% ↓)	33.1/15.2/27.6	182.7 (30% ↓)	4.0 (27% ↓)	26.8/11.0/21.6

Bảng 4: Hiệu quả của Các Chỉ số Tầm quan trọng Tensor (OPT-2.7B)

trong Bảng 4, với cùng mục tiêu giảm FLOPs, sử dụng chỉ số của chúng tôi (∆w∂L∂w) cho đánh giá tầm quan trọng tensor đạt được độ chính xác mô hình cao nhất và vượt trội hơn Full FT 1%-3% về điểm ROUGE. Điều này là do các chỉ số dựa trên độ lớn bỏ qua các phụ thuộc của cập nhật trọng số. Các chỉ số chỉ gradient chỉ chứa thông tin hướng về tầm quan trọng tensor nhưng không thể phản ánh cường độ của tầm quan trọng. Các phép đo tầm quan trọng không chính xác sẽ dẫn đến việc lựa chọn tensor có thể huấn luyện không phù hợp.

4.4 TÁC ĐỘNG CỦA KÍCH THƯỚC LLM
Một loại LLM có thể chứa nhiều biến thể với kích thước khác nhau. Để nghiên cứu hiệu suất của GreenTrainer với các kích thước LLM khác nhau, chúng tôi đã thực hiện tinh chỉnh sử dụng các mô hình OPT với kích thước từ 350M đến 6.7B. Như được hiển thị trong Bảng 5, ngay cả trên các mô hình nhỏ (OPT-350M), GT-0.5 có thể tiết kiệm 17%-21% FLOPs huấn luyện hơn so với LoRA, trong khi đạt được độ chính xác cao hơn 2%-4% (trên SciTDR) hoặc cùng độ chính xác (trên DialogSum). Khi kích thước mô hình tăng lên 2.7B, GT-0.5 vượt trội hơn LoRA và GT-0.7 vượt trội hơn Full FT trên bộ dữ liệu SciTLDR. Trên DialogSum, GT-0.7 hoạt động tương tự so với LoRA. Đối với mô hình OPT-6.7B2, GT-0.4 có thể tiết kiệm 27% FLOPs huấn luyện hơn so với LoRA trên SciTLDR, trong khi đạt được cùng độ chính xác mô hình, và những lợi thế tương tự cũng có thể được quan sát khi so sánh GT-0.5 và GT-0.7 với LoRA. Nói chung, lợi thế hiệu suất của GreenTrainer áp dụng rộng rãi cho các LLM với kích thước khác nhau.

# Params
& Phương pháp	SciTLDR	DialogSum
PFLOPs	Time (h)	R1/R2/RL	PFLOPs	Time (h)	R1/R2/RL
OPT-350M
Full FT	5.4	0.15	30.9/13.9/25.7	33.8	0.92	23.2/9.0/18.5
LoRA	3.6 (33% ↓)	0.10 (33% ↓)	25.9/10.8/20.3	22.5 (33% ↓)	0.65 (29% ↓)	21.5/7.7/17.3
GT-0.4	2.1 (61% ↓)	0.06 (60% ↓)	27.7/12.2/23.4	13.3 (61% ↓)	0.36 (61% ↓)	17.3/5.8/14.6
GT-0.5	2.7 (50% ↓)	0.08 (47% ↓)	29.9/13.2/24.9	16.7 (51% ↓)	0.45 (51% ↓)	21.3/7.8/17.3
GT-0.7	3.8 (30% ↓)	0.12 (20% ↓)	30.6/13.5/25.0	23.6 (30% ↓)	0.66 (28% ↓)	24.2/9.3/19.3

OPT-1.3B
Full FT	20.8	0.46	32.1/14.3/26.4	130.8	2.9	25.4/10.3/20.2
LoRA	13.9 (33% ↓)	0.31 (33% ↓)	28.1/11.9/22.0	87.2 (33% ↓)	1.9 (34% ↓)	24.6/9.9/19.4
GT-0.4	8.2 (61% ↓)	0.18 (61% ↓)	28.9/11.9/23.8	51.4 (61% ↓)	1.1 (62% ↓)	16.9/5.7/14.6
GT-0.5	10.3 (50% ↓)	0.23 (50% ↓)	30.0/12.7/24.5	64.2 (51% ↓)	1.4 (51% ↓)	20.1/7.4/16.7
GT-0.7	14.5 (30% ↓)	0.34 (26% ↓)	31.2/14.2/25.8	90.8 (30% ↓)	2.0 (31% ↓)	24.4/9.7/19.4

OPT-2.7B
Full FT	41.8	0.92	32.9/14.9/27.1	262.0	5.5	23.6/9.5/18.8
LoRA	27.9 (33% ↓)	0.59 (36% ↓)	28.2/12.1/21.0	174.7 (33% ↓)	3.6 (35% ↓)	23.8/9.5/18.8
GT-0.4	16.6 (60% ↓)	0.36 (61% ↓)	28.6/11.6/23.5	103.4 (61% ↓)	2.2 (60% ↓)	17.9/6.3/15.4
GT-0.5	20.8 (50% ↓)	0.46 (50% ↓)	30.5/13.1/25.2	130.1 (50% ↓)	2.7 (51% ↓)	21.4/8.2/17.6
GT-0.7	29.2(30% ↓)	0.68 (26% ↓)	33.1/15.2/27.6	182.7 (30% ↓)	4.0 (27% ↓)	26.8/11.0/21.6

OPT-6.7B
Full FT	103.9	5.44	32.9/14.9/27.5	649.9	-	-
LoRA	69.3 (33% ↓)	1.3	28.4/12.3/22.7	433.3 (33% ↓)	8.1	24.9/10.2/19.4
GT-0.4	41.2 (60% ↓)	0.9	28.9/11.8/23.4	257.9 (60% ↓)	5.2	19.7/7.0/16.3
GT-0.5	50.8 (51% ↓)	1.1	30.1/13.0/24.8	331.4 (49% ↓)	6.7	21.8/8.5/17.3
GT-0.7	74.8 (28% ↓)	1.4	33.1/15.3/27.7	-	-	-

Bảng 5: Tác động của kích thước mô hình LLM

5 KẾT LUẬN
Trong bài báo này, chúng tôi trình bày GreenTrainer, một kỹ thuật mới cho tinh chỉnh LLM cho phép lựa chọn hiệu quả các tham số có thể huấn luyện thông qua lan truyền ngược thích nghi, để đảm bảo chất lượng huấn luyện cao trong khi tối thiểu hóa chi phí tính toán. GreenTrainer tiết kiệm lên đến 64% FLOPs huấn luyện so với tinh chỉnh đầy đủ mà không có mất mát độ chính xác đáng chú ý. So với kỹ thuật hiện có như Prefix Tuning và LoRA, GreenTrainer cải thiện độ chính xác lên đến 4% với cùng mức giảm FLOPs.

2Đối với OPT-6.7B, Full FT và GT-0.7 với DialogSum có vấn đề hết bộ nhớ trên các GPU chúng tôi sử dụng.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN
Chúng tôi muốn cảm ơn các người đánh giá ẩn danh và chủ tịch khu vực vì các bình luận và phản hồi của họ. Công trình này được hỗ trợ một phần bởi National Science Foundation (NSF) dưới số tài trợ IIS-2205360, CCF-2217003 và CCF-2215042.

TÀI LIỆU THAM KHẢO
Báo cáo chỉ số AI 2023. https://aiindex.stanford.edu/report/ , 2023.

M. Abadi. Tensorflow: learning functions at scale. In Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming , pages 1–1, 2016.

A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021 , 2017.

J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.

J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013.

Y . Bisk, R. Zellers, J. Gao, Y . Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages 7432–7439, 2020.

L. Breiman. Random forests. Machine learning , 45:5–32, 2001.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

I. Cachola, K. Lo, A. Cohan, and D. S. Weld. Tldr: Extreme summarization of scientific documents. arXiv preprint arXiv:2004.15011 , 2020.

A. Candel, J. McKinney, P. Singer, P. Pfeiffer, M. Jeblick, P. Prabhu, J. Gambera, M. Landry, S. Bansal, R. Chesler, et al. h2ogpt: Democratizing large language models. arXiv preprint arXiv:2306.08161 , 2023.

Y . Chen, Y . Liu, L. Chen, and Y . Zhang. Dialogsum: A real-life scenario dialogue summarization dataset. arXiv preprint arXiv:2105.06762 , 2021.

H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

B. Gliwa, I. Mochol, M. Biesek, and A. Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237 , 2019.

R. Hesse, S. Schaub-Meyer, and S. Roth. Fast axiomatic attribution for neural networks. Advances in Neural Information Processing Systems , 34:19513–19524, 2021.

E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.

Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, and S. Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 , 2023.

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

K. Huang, B. Yang, and W. Gao. Elastictrainer: Speeding up on-device training with runtime elastic tensor selection. In Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services , pages 56–69, 2023a.

K. Huang, B. Yang, and W. Gao. Modality plug-and-play: Elastic modality adaptation in multimodal llms for embodied ai. arXiv preprint arXiv:2312.07886 , 2023b.

G. Jin, X. Yi, L. Zhang, L. Zhang, S. Schewe, and X. Huang. How does weight correlation affect generalisation ability of deep neural networks? Advances in Neural Information Processing Systems , 33:21346–21356, 2020.

Y . D. Kwon, R. Li, S. I. Venieris, J. Chauhan, N. D. Lane, and C. Mascolo. Tinytrain: Deep neural network training at the extreme edge. arXiv preprint arXiv:2307.09988 , 2023.

A. M. Lamb, A. G. ALIAS PARTH GOYAL, Y . Zhang, S. Zhang, A. C. Courville, and Y . Ben-gio. Professor forcing: A new algorithm for training recurrent networks. Advances in neural information processing systems , 29, 2016.

J. Lee, S. Park, S. Mo, S. Ahn, and J. Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611 , 2020.

N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.

B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.

H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 , 2016.

X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 , 2021.

B. Liao, S. Tan, and C. Monz. Make your pre-trained model reversible: From parameter to memory efficient fine-tuning. arXiv preprint arXiv:2306.00477 , 2023.

C.-Y . Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013 .

J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, C. Gan, and S. Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems , 35:22941–22954, 2022.

L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang, Y . Chen, W. Yang, Q. Liao, and W. Zhang. Group fisher pruning for practical network compression. In International Conference on Machine Learning , pages 7021–7032. PMLR, 2021.

X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, and J. Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 61–68, 2022.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.

K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247 , 1, 2021.

S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora. Fine-tuning language models with just forward passes. arXiv preprint arXiv:2305.17333 , 2023.

N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.

R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 , 2016.

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 , 2019.

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.

R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green ai. Communications of the ACM , 63 (12):54–63, 2020.

T. Scialom, T. Chakrabarty, and S. Muresan. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 6107–6122, 2022.

M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International conference on machine learning , pages 3319–3328. PMLR, 2017.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.

H. Wang and W. Gao. Tackling the unlimited staleness in federated learning with intertwined data and device heterogeneities. arXiv preprint arXiv:2309.13536 , 2023.

T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.

E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.

Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and T. Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512 , 2023.

S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

A PHỤ LỤC

A.1 GIẢM VIỆC SỬ DỤNG BỘ NHỚ CỦA ĐÁNH GIÁ TẦM QUAN TRỌNG TENSOR

Phương pháp của chúng tôi để đánh giá tầm quan trọng của các tensor NN trong Phần 3.2 yêu cầu lưu trữ tất cả các trọng số mô hình trước đó và gradient hiện tại, để tính toán Eq. (6). Tuy nhiên, việc làm như vậy làm tăng đáng kể tiêu thụ bộ nhớ GPU, đặc biệt là đối với các LLM hiện đại với hàng tỷ trọng số mô hình. Để giảm việc sử dụng bộ nhớ GPU như vậy, chúng tôi quan sát rằng công thức vấn đề của chúng tôi trong Eq. (4) sẽ ngăn cản các tensor trong các lớp đầu được chọn để huấn luyện, do chi phí cao của việc truyền gradient kích hoạt của chúng trong lan truyền ngược. Do đó, chúng ta có thể an toàn loại trừ những tensor này khỏi phần có thể huấn luyện của tinh chỉnh LLM và tiết kiệm một lượng bộ nhớ GPU đáng kể.

Cụ thể hơn, lan truyền ngược trong đánh giá tầm quan trọng tensor có thể được dừng sớm tại một tensor k nhất định, sao cho

X
i=k−1,...,Ntdyi< ρT full≤X
i=k,...,Ntdyi, (8)

tức là, FLOPs tích lũy của tất cả các tensor từ 1 đến k vừa vượt quá mục tiêu giảm FLOPs của chúng ta. Như được hiển thị trong Bảng 6, bằng cách áp dụng phương pháp dừng sớm như vậy, chúng ta có thể tiết kiệm bộ nhớ GPU theo tỷ lệ với giá trị của ρ, vì giá trị ρ nhỏ hơn dẫn đến k nhỏ hơn và do đó lan truyền ngược có thể được dừng sớm hơn. Ví dụ, khi ρ=50%, 25% bộ nhớ GPU có thể được tiết kiệm, và việc tiết kiệm như vậy có thể tăng thêm lên 50% khi ρ=34%.

Mô hình	Đánh giá đầy đủ	Dừng sớm ρ= 34%	Dừng sớm ρ= 40%	Dừng sớm ρ= 50%	Dừng sớm ρ= 60%
OPT-2.7B	10.8	5.5	6.5	8.1	9.7
FLAN-T5-3B	12.0	6.1	7.2	9.0	10.8

Bảng 6: Tiêu thụ bộ nhớ GPU (tính bằng GigaBytes) của đánh giá tầm quan trọng tensor

A.2 GIẢM CHI PHÍ TÍNH TOÁN CỦA QUY HOẠCH ĐỘNG CHO LỰA CHỌN TENSOR

Trong phương pháp quy hoạch động (DP) được đề xuất của chúng tôi cho lựa chọn tensor trong Phần 3.3, do khối lượng FLOPs cao trong tinh chỉnh LLM, giá trị của Tfull có thể rất lớn. Để giảm chi phí tính toán của DP, chúng ta có thể giảm không gian bài toán con bằng cách bỏ qua hai loại bài toán con: 1) những bài toán không hợp lệ, có ràng buộc FLOPs t vượt quá ràng buộc mong muốn (ρTfull); 2) những bài toán dư thừa, có FLOPs để truyền gradient kích hoạt đến độ sâu tối đa được phép (k) vượt quá t. Thí nghiệm sơ bộ của chúng tôi cho thấy rằng, việc làm như vậy trên mô hình OPT với ρbp= 50% có thể giảm số lượng bài toán con 5.5 × mà không ảnh hưởng đến tính tối ưu của huấn luyện.

Mô hình	Tq= 1e1	Tq= 1e2	Tq= 1e3	Tq= 1e4	Tq= 1e5
OPT-2.7B	0.02/64.1/32.0	0.04/47.6/30.1	0.64/49.8/30.7	7.5/50.0/30.9	76.5/50.0/30.9
BLOOMZ-3B	0.0001/33.3/9.30	0.007/45.7/25.2	0.21/49.5/27.2	2.3/49.8/27.1	25.3/50.0/27.1
FLAN-T5-3B	0.04/64.9/36.5	0.25/57.1/36.5	3.5/55.3/36.7	41.8/51.8/36.7	449/50.0/36.7

Bảng 7: Tác động của độ phân giải DP Tq đến tinh chỉnh LLM OPT-2.7B, BLOOMZ-3B, và FLAN-T5-3B, trên bộ dữ liệu SciTLDR với ρ= 50%. Mỗi bộ ba [a/b/c] trình bày a) tỷ lệ phần trăm thời gian wall-clock phát sinh bởi DP so với tinh chỉnh đầy đủ, b) tỷ lệ phần trăm FLOPs sau khi giảm so với tinh chỉnh đầy đủ, và c) điểm ROUGE-1 kiểm tra, tương ứng.

Bên cạnh đó, để giảm thêm số lượng bài toán con, chúng tôi chia tỷ lệ FLOPs của các tensor (tdw, tdy) bằng cách nhân với hệ số Z:

ftdw=⌊tdw·Z⌋,ftdy=⌊tdy·Z⌋, (9)

trong đó Z=Tq/Tfull giảm FLOPs backpropagation xuống độ phân giải Tq< Tfull. Độ phức tạp thời gian tổng thể của DP sau đó được giảm xuống O(N2Tq). Mặt khác, độ phân giải giảm như vậy có thể tăng tính mơ hồ trong DP và ảnh hưởng đến chất lượng huấn luyện. Để điều tra sự cân bằng như vậy giữa chất lượng huấn luyện và chi phí, chúng tôi đã tiến hành các thí nghiệm sơ bộ trên nhiều LLM. Kết quả

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
trong Bảng 7 cho thấy rằng, đối với cả mô hình OPT-2.7B và BLOOMZ-3B, việc đặt Tq= 1e3 giảm chi phí DP xuống <1% mà không ảnh hưởng đến chất lượng huấn luyện. Tương tự, đối với FLAN-T5-3B, việc chọn Tq= 1e2 có thể giữ lại chất lượng huấn luyện tốt với chi phí không đáng kể. Mặt khác, khi Tq quá nhỏ, giải pháp của DP có thể không chính xác và do đó dẫn đến việc giảm FLOPs huấn luyện không hiệu quả.

…
query_states  = self.q_proj (hidden_states) * self.scaling
key_states = self._shape(self.k_proj (hidden_states ), -1, bsz)
value_states  = self._shape (self.v_proj( hidden_states), -1, bsz)
…Một phần của đồ thị tính toán của LLM 
được biểu diễn trong mã python
Biểu diễn chỉ tensor dựa trên thứ tự thực thi của tensor
Kernel
2560x2560Bias 
2560x 1Kernel
2560x2560Bias
2560x1Kernel
2560x2560Bias
2560x1q_proj k_proj v_proj
Tính toán và khớp 
(𝒕𝒕𝒅𝒅𝒅𝒅,𝒕𝒕𝒅𝒅𝒅𝒅) cho mỗi tensor … …batch_size = 4
token_num  = 512
token_dim  = 2560
𝑡𝑡𝑑𝑑𝑑𝑑=4×512×2560 ×2560
𝑡𝑡𝑑𝑑𝑑𝑑=4×512×2560 ×2560𝑡𝑡𝑑𝑑𝑑𝑑=0
𝑡𝑡𝑑𝑑𝑑𝑑=4×512×2560Kernel: Bias:𝑡𝑡𝑑𝑑𝑑𝑑,𝑡𝑡𝑑𝑑𝑑𝑑 𝑡𝑡𝑑𝑑𝑑𝑑,𝑡𝑡𝑑𝑑𝑑𝑑
…
Kernel:
 Bias:
 … Công thức lan truyền ngược 
cho các loại tensor khác nhau…

Hình 5: Một ví dụ về lập hồ sơ FLOPs tensor trong mô hình OPT-2.7B

A.3 MỘT VÍ DỤ VỀ LẬP HỒ SƠ FLOPS TENSOR TRONG MÔ HÌNH OPT-2.7B
Để tạo điều kiện hiểu biết tốt hơn, chúng tôi tiếp tục hiển thị một ví dụ trong Hình 5 về cách chúng tôi lập hồ sơ các tensor trong mô hình OPT-2.7B trong các thí nghiệm của chúng tôi. Đầu tiên, chúng tôi chuyển đổi đồ thị tính toán của LLM, được triển khai trong mã Python, thành biểu diễn chỉ tensor. Các tensor được sắp xếp dựa trên thứ tự thực thi của chúng trong lần truyền thuận, tương tự như đồ thị cấp lớp trong Hình 3. Sau đó chúng tôi tính toán FLOPs của mỗi tensor (tdy,tdy) dựa trên các công thức lan truyền ngược được thảo luận trong Phần 3.1. Những phép tính như vậy về cơ bản là đếm các phép nhân và được cộng vào trong công thức của chúng.

Phương pháp	Độ chính xác (%)	PFLOPs	Time (h)
LoRA	49.5	174.0	6.27
GT-0.5	59.2	130.5	4.69

Bảng 8: OPT-2.7B trên bộ dữ liệu PIQA

Phương pháp	Độ chính xác (%)	PFLOPs	Time (h)
LoRA	19.6	16.0	0.55
GT-0.5	28.7	12.0	0.50
GT-0.6	29.5	14.0	0.61

Bảng 9: OPT-2.7B trên bộ dữ liệu WebQuestion

A.4 HIỆU SUẤT TRÊN CÁC TÁC VỤ HỎI ĐÁP SINH TẠO
Để đánh giá tốt hơn hiệu suất của GreenTrainer trên các tác vụ khác, chúng tôi cũng đã tiến hành thí nghiệm bằng cách sử dụng mô hình OPT-2.7B trên các bộ dữ liệu WebQuestions và PIQA cho các tác vụ QA sinh tạo. Bộ dữ liệu WebQuestions chứa 6,642 cặp QA sử dụng Freebase làm cơ sở kiến thức. Bộ dữ liệu PIQA tập trung vào QA đa lựa chọn về kiến thức vật lý với 21k cặp QA. Chúng tôi áp dụng định dạng prompt " question: {q}</s>answer: {a}</s> " cho WebQuestions và " goal: {q}</s>sol1: {sol1}</s>sol2: {sol2}</s>label: {a}</s> " cho PIQA, trong đó </s> là token EOS cho mô hình OPT. Các siêu tham số cho huấn luyện giống như những tham số được mô tả trong Phần 4. Chúng tôi đánh giá độ chính xác cấp câu yêu cầu câu trả lời được sinh ra phải khớp chính xác với ground truth. Lưu ý rằng đối với PIQA, các token được sinh ra vẫn được dự đoán từ toàn bộ từ điển của embeddings OPT thay vì từ hai lựa chọn: thứ nhất hoặc thứ hai. Như được hiển thị trong Bảng 8 và Bảng 9, trên cả hai bộ dữ liệu, GreenTrainer (GT) đạt được độ chính xác và hiệu quả thời gian tốt hơn đáng kể so với LoRA.

Cụ thể, kết quả trên bộ dữ liệu PIQA nói chung thấp hơn những kết quả được báo cáo trong Brown et al. (2020). Lý do cho khoảng cách độ chính xác này là cách chúng tôi sử dụng mô hình OPT để sinh ra câu trả lời thách thức hơn so với thiết lập trong Brown et al. (2020). Theo Phần 2.4 trong Brown et al. (2020), nó công thức hóa tác vụ PIQA như một tác vụ QA đa lựa chọn trong đó câu trả lời được rút ra từ một tập hợp ứng viên nhỏ và được xác định trước (ví dụ, ["0", "1"]), bằng cách so sánh điểm xác suất chỉ trên các token ứng viên. Ngược lại, chúng tôi nghiêm ngặt đưa vấn đề vào sinh tạo mở, trong đó tập hợp ứng viên không được biết. Trong trường hợp đó, việc sinh ra câu trả lời đúng có thể khó khăn hơn, vì mô hình có thể sinh ra câu trả lời hoàn toàn không liên quan và tăng cơ hội mắc lỗi.

A.5 TÁC ĐỘNG CỦA TẦN SUẤT ĐÁNH GIÁ TẦM QUAN TRỌNG TENSOR
Thiết kế của GreenTrainer, theo mặc định, đánh giá tầm quan trọng của các tensor và chọn tập hợp các tensor có thể huấn luyện dựa trên tầm quan trọng như vậy ở đầu mỗi epoch huấn luyện. Sử dụng phương pháp kỹ thuật được mô tả trong Phần 3.1, việc đánh giá tầm quan trọng tensor như vậy rất nhẹ, và kết quả thí nghiệm của chúng tôi cho thấy rằng chi phí của việc đánh giá tầm quan trọng chỉ là 0.2% trên bộ dữ liệu SciTLDR và 0.01% trên bộ dữ liệu DialogSum, đối với toàn bộ chi phí tinh chỉnh.

Mặt khác, trong một số trường hợp nhất định, tầm quan trọng tensor, được tính toán từ các thay đổi gradient mô hình, có thể thể hiện sự khác biệt không thể bỏ qua trong một epoch. Trong những trường hợp này, thiết kế linh hoạt của GreenTrainer sẽ cho phép chúng ta thích nghi tăng tần suất đánh giá tầm quan trọng tensor và lựa chọn tensor dựa trên DP tương ứng. Để chứng minh tác động của việc đánh giá tầm quan trọng tensor và lựa chọn tensor dựa trên DP thường xuyên hơn như vậy, chúng tôi đã tiến hành thêm thí nghiệm sử dụng mô hình OPT-2.7B trên bộ dữ liệu WebQuestions và tác vụ QA sinh tạo, như được hiển thị trong Bảng 10.

Tần suất đánh giá tầm quan trọng tensor	Độ chính xác (%)	Time (h)
Mỗi 945 lần lặp (một lần mỗi epoch)	28.4	0.50
Mỗi 600 lần lặp	28.5	0.54
Mỗi 400 lần lặp	28.2	0.56
Mỗi 200 lần lặp	27.5	0.64

Bảng 10: Tác động của tần suất đánh giá tầm quan trọng tensor

Kết quả cho thấy rằng: (1) Việc đánh giá tầm quan trọng tensor thường xuyên hơn chỉ mang lại sự cải thiện rất nhỏ về độ chính xác tác vụ. Xem xét tính ngẫu nhiên trong các lần thử huấn luyện khác nhau, chúng tôi tin rằng sự cải thiện độ chính xác như vậy là không đáng kể, và độ chính xác thậm chí có thể giảm 1% khi tần suất đánh giá rất cao (mỗi 200 lần lặp). Chúng tôi tin rằng điều này là do sự tích lũy của các lỗi đánh giá tầm quan trọng tensor và lựa chọn tensor, bắt nguồn từ xấp xỉ bậc nhất trong chỉ số tầm quan trọng tensor và giải pháp xấp xỉ trong DP. Một lý do khả dĩ khác là tầm quan trọng tensor được tính toán trên bộ dữ liệu huấn luyện, và việc đánh giá tầm quan trọng tensor quá thường xuyên có thể khiến quá trình huấn luyện overfit với bộ dữ liệu huấn luyện. (2) Chi phí huấn luyện tăng đều với tần suất đánh giá tầm quan trọng tensor. Khi khoảng cách đánh giá giảm từ 945 lần lặp xuống 200 lần lặp, thời gian huấn luyện tăng 28%.

Tóm lại, việc thực hiện đánh giá tầm quan trọng tensor thường xuyên hơn trong mỗi epoch mang lại ít cải thiện về độ chính xác tác vụ nhưng tăng đáng kể chi phí huấn luyện. Chúng tôi tin rằng tầm quan trọng tensor được đánh giá một lần trong mỗi epoch sẽ đủ chính xác cho việc lựa chọn tensor có thể huấn luyện phù hợp.

A.6 SỰ CẦN THIẾT CỦA LỰA CHỌN TENSOR ĐỘNG
Nếu tinh chỉnh LLM sử dụng một bộ dữ liệu huấn luyện cố định, có thể việc sử dụng lựa chọn tensor cố định được quyết định ở giai đoạn ban đầu của huấn luyện có thể không dẫn đến sự sụt giảm độ chính xác mô hình đáng kể, so với lựa chọn tensor tại thời gian chạy. Tuy nhiên, trong các tình huống tinh chỉnh LLM thực tế, giả định này thường không đúng do hai lý do sau. Đầu tiên, trong rất nhiều tình huống tinh chỉnh LLM, như học trực tuyến và cá nhân hóa mô hình, mô hình được huấn luyện lại liên tục sử dụng dữ liệu trực tuyến, được tạo ra liên tục tại thời gian chạy với các phân phối dữ liệu biến đổi. Những phân phối dữ liệu biến đổi như vậy chắc chắn sẽ dẫn đến tầm quan trọng khác nhau của các tensor thông qua quy trình huấn luyện và do đó yêu cầu lựa chọn tensor tại thời gian chạy. Những tình huống tinh chỉnh LLM trực tuyến như vậy gần đây trở nên ngày càng phổ biến, đặc biệt với khả năng triển khai LLMs lên các thiết bị di động cá nhân của người dùng như điện thoại thông minh. Thứ hai, ngay cả đối với một bộ dữ liệu huấn luyện cố định, cũng có thể tầm quan trọng của một số tensor có thể thay đổi khi quá trình huấn luyện tiến triển. Trong những trường hợp này, lựa chọn tensor động có thể cải thiện độ chính xác mô hình được huấn luyện. Để xác minh điều này, chúng tôi đã tiến hành thêm thí nghiệm sử dụng mô hình OPT-2.7B trên bộ dữ liệu WebQuestions và tác vụ QA sinh tạo. Như được hiển thị trong Bảng 11, lựa chọn tensor động có thể đóng góp không thể bỏ qua vào việc cải thiện độ chính xác tác vụ, với sự tăng không đáng kể của chi phí huấn luyện.

Chiến lược	Độ chính xác (%)	Time (h)
Lựa chọn tensor cố định chỉ trong epoch đầu tiên của huấn luyện	27.4	0.49
Lựa chọn tensor động, một lần trong mỗi epoch	28.4	0.50
Lựa chọn tensor thường xuyên hơn (5 lần trong mỗi epoch)	27.5	0.64

Bảng 11: Các chiến lược lựa chọn tensor khác nhau

Lưu ý rằng, sự cải thiện độ chính xác mô hình như vậy sẽ phụ thuộc vào bộ dữ liệu và mô hình cụ thể được sử dụng, nhưng những kết quả thí nghiệm trên đã chứng minh sự cần thiết của lựa chọn tensor tại thời gian chạy. Ngoài ra, kết quả thí nghiệm của chúng tôi cũng cho thấy rằng việc đánh giá và lựa chọn tầm quan trọng tensor như vậy thực sự phát sinh rất ít chi phí tính toán bổ sung.

16
